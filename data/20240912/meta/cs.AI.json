[
    {
        "title": "\"My Grade is Wrong!\": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays",
        "authors": "Shengxin HongChang CaiSixuan DuHaiyue FengSiyuan LiuXiuyi Fan",
        "links": "http://arxiv.org/abs/2409.07453v1",
        "entry_id": "http://arxiv.org/abs/2409.07453v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07453v1",
        "summary": "Interactive feedback, where feedback flows in both directions between teacher\nand student, is more effective than traditional one-way feedback. However, it\nis often too time-consuming for widespread use in educational practice. While\nLarge Language Models (LLMs) have potential for automating feedback, they\nstruggle with reasoning and interaction in an interactive setting. This paper\nintroduces CAELF, a Contestable AI Empowered LLM Framework for automating\ninteractive feedback. CAELF allows students to query, challenge, and clarify\ntheir feedback by integrating a multi-agent system with computational\nargumentation. Essays are first assessed by multiple Teaching-Assistant Agents\n(TA Agents), and then a Teacher Agent aggregates the evaluations through formal\nreasoning to generate feedback and grades. Students can further engage with the\nfeedback to refine their understanding. A case study on 500 critical thinking\nessays with user studies demonstrates that CAELF significantly improves\ninteractive feedback, enhancing the reasoning and interaction capabilities of\nLLMs. This approach offers a promising solution to overcoming the time and\nresource barriers that have limited the adoption of interactive feedback in\neducational settings.",
        "updated": "2024-09-11 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07453v1"
    },
    {
        "title": "Introducing Perturb-ability Score (PS) to Enhance Robustness Against Evasion Adversarial Attacks on ML-NIDS",
        "authors": "Mohamed elShehabyAshraf Matrawy",
        "links": "http://arxiv.org/abs/2409.07448v1",
        "entry_id": "http://arxiv.org/abs/2409.07448v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07448v1",
        "summary": "This paper proposes a novel Perturb-ability Score (PS) that can be used to\nidentify Network Intrusion Detection Systems (NIDS) features that can be easily\nmanipulated by attackers in the problem-space. We demonstrate that using PS to\nselect only non-perturb-able features for ML-based NIDS maintains detection\nperformance while enhancing robustness against adversarial attacks.",
        "updated": "2024-09-11 17:52:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07448v1"
    },
    {
        "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories",
        "authors": "Ben BoginKejuan YangShashank GuptaKyle RichardsonErin BransomPeter ClarkAshish SabharwalTushar Khot",
        "links": "http://arxiv.org/abs/2409.07440v1",
        "entry_id": "http://arxiv.org/abs/2409.07440v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07440v1",
        "summary": "Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress.",
        "updated": "2024-09-11 17:37:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07440v1"
    },
    {
        "title": "Synthetic continued pretraining",
        "authors": "Zitong YangNeil BandShuangping LiEmmanuel CandèsTatsunori Hashimoto",
        "links": "http://arxiv.org/abs/2409.07431v1",
        "entry_id": "http://arxiv.org/abs/2409.07431v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07431v1",
        "summary": "Pretraining on large-scale, unstructured internet text has enabled language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient -- to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining using EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.",
        "updated": "2024-09-11 17:21:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07431v1"
    },
    {
        "title": "Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise Recommendation",
        "authors": "Luo JiGao LiuMingyang YinHongxia YangJingren Zhou",
        "links": "http://arxiv.org/abs/2409.07416v1",
        "entry_id": "http://arxiv.org/abs/2409.07416v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07416v1",
        "summary": "Modern listwise recommendation systems need to consider both long-term user\nperceptions and short-term interest shifts. Reinforcement learning can be\napplied on recommendation to study such a problem but is also subject to large\nsearch space, sparse user feedback and long interactive latency. Motivated by\nrecent progress in hierarchical reinforcement learning, we propose a novel\nframework called mccHRL to provide different levels of temporal abstraction on\nlistwise recommendation. Within the hierarchical framework, the high-level\nagent studies the evolution of user perception, while the low-level agent\nproduces the item selection policy by modeling the process as a sequential\ndecision-making problem. We argue that such framework has a well-defined\ndecomposition of the outra-session context and the intra-session context, which\nare encoded by the high-level and low-level agents, respectively. To verify\nthis argument, we implement both a simulator-based environment and an\nindustrial dataset-based experiment. Results observe significant performance\nimprovement by our method, compared with several well-known baselines. Data and\ncodes have been made public.",
        "updated": "2024-09-11 17:01:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07416v1"
    }
]