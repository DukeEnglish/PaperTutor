[
    {
        "title": "Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered Stereo Pairs",
        "authors": "Sadra SafadoustFabio TosiFatma GüneyMatteo Poggi",
        "links": "http://arxiv.org/abs/2409.07456v1",
        "entry_id": "http://arxiv.org/abs/2409.07456v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07456v1",
        "summary": "3D Gaussian Splatting (GS) significantly struggles to accurately represent\nthe underlying 3D scene geometry, resulting in inaccuracies and floating\nartifacts when rendering depth maps. In this paper, we address this limitation,\nundertaking a comprehensive analysis of the integration of depth priors\nthroughout the optimization process of Gaussian primitives, and present a novel\nstrategy for this purpose. This latter dynamically exploits depth cues from a\nreadily available stereo network, processing virtual stereo pairs rendered by\nthe GS model itself during training and achieving consistent self-improvement\nof the scene representation. Experimental results on three popular datasets,\nbreaking ground as the first to assess depth accuracy for these models,\nvalidate our findings.",
        "updated": "2024-09-11 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07456v1"
    },
    {
        "title": "DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for Text-to-3D Generation",
        "authors": "Haibo YangYang ChenYingwei PanTing YaoZhineng ChenZuxuan WuYu-Gang JiangTao Mei",
        "links": "http://arxiv.org/abs/2409.07454v1",
        "entry_id": "http://arxiv.org/abs/2409.07454v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07454v1",
        "summary": "Learning radiance fields (NeRF) with powerful 2D diffusion models has\ngarnered popularity for text-to-3D generation. Nevertheless, the implicit 3D\nrepresentations of NeRF lack explicit modeling of meshes and textures over\nsurfaces, and such surface-undefined way may suffer from the issues, e.g.,\nnoisy surfaces with ambiguous texture details or cross-view inconsistency. To\nalleviate this, we present DreamMesh, a novel text-to-3D architecture that\npivots on well-defined surfaces (triangle meshes) to generate high-fidelity\nexplicit 3D model. Technically, DreamMesh capitalizes on a distinctive\ncoarse-to-fine scheme. In the coarse stage, the mesh is first deformed by\ntext-guided Jacobians and then DreamMesh textures the mesh with an interlaced\nuse of 2D diffusion models in a tuning free manner from multiple viewpoints. In\nthe fine stage, DreamMesh jointly manipulates the mesh and refines the texture\nmap, leading to high-quality triangle meshes with high-fidelity textured\nmaterials. Extensive experiments demonstrate that DreamMesh significantly\noutperforms state-of-the-art text-to-3D methods in faithfully generating 3D\ncontent with richer textual details and enhanced geometry. Our project page is\navailable at https://dreammesh.github.io.",
        "updated": "2024-09-11 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07454v1"
    },
    {
        "title": "Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models",
        "authors": "Haibo YangYang ChenYingwei PanTing YaoZhineng ChenChong-Wah NgoTao Mei",
        "links": "http://arxiv.org/abs/2409.07452v1",
        "entry_id": "http://arxiv.org/abs/2409.07452v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07452v1",
        "summary": "Despite having tremendous progress in image-to-3D generation, existing\nmethods still struggle to produce multi-view consistent images with\nhigh-resolution textures in detail, especially in the paradigm of 2D diffusion\nthat lacks 3D awareness. In this work, we present High-resolution Image-to-3D\nmodel (Hi3D), a new video diffusion based paradigm that redefines a single\nimage to multi-view images as 3D-aware sequential image generation (i.e.,\norbital video generation). This methodology delves into the underlying temporal\nconsistency knowledge in video diffusion model that generalizes well to\ngeometry consistency across multiple views in 3D generation. Technically, Hi3D\nfirst empowers the pre-trained video diffusion model with 3D-aware prior\n(camera pose condition), yielding multi-view images with low-resolution texture\ndetails. A 3D-aware video-to-video refiner is learnt to further scale up the\nmulti-view images with high-resolution texture details. Such high-resolution\nmulti-view images are further augmented with novel views through 3D Gaussian\nSplatting, which are finally leveraged to obtain high-fidelity meshes via 3D\nreconstruction. Extensive experiments on both novel view synthesis and single\nview reconstruction demonstrate that our Hi3D manages to produce superior\nmulti-view consistency images with highly-detailed textures. Source code and\ndata are available at \\url{https://github.com/yanghb22-fdu/Hi3D-Official}.",
        "updated": "2024-09-11 17:58:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07452v1"
    },
    {
        "title": "FreeEnhance: Tuning-Free Image Enhancement via Content-Consistent Noising-and-Denoising Process",
        "authors": "Yang LuoYiheng ZhangZhaofan QiuTing YaoZhineng ChenYu-Gang JiangTao Mei",
        "links": "http://arxiv.org/abs/2409.07451v1",
        "entry_id": "http://arxiv.org/abs/2409.07451v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07451v1",
        "summary": "The emergence of text-to-image generation models has led to the recognition\nthat image enhancement, performed as post-processing, would significantly\nimprove the visual quality of the generated images. Exploring diffusion models\nto enhance the generated images nevertheless is not trivial and necessitates to\ndelicately enrich plentiful details while preserving the visual appearance of\nkey content in the original image. In this paper, we propose a novel framework,\nnamely FreeEnhance, for content-consistent image enhancement using the\noff-the-shelf image diffusion models. Technically, FreeEnhance is a two-stage\nprocess that firstly adds random noise to the input image and then capitalizes\non a pre-trained image diffusion model (i.e., Latent Diffusion Models) to\ndenoise and enhance the image details. In the noising stage, FreeEnhance is\ndevised to add lighter noise to the region with higher frequency to preserve\nthe high-frequent patterns (e.g., edge, corner) in the original image. In the\ndenoising stage, we present three target properties as constraints to\nregularize the predicted noise, enhancing images with high acutance and high\nvisual quality. Extensive experiments conducted on the HPDv2 dataset\ndemonstrate that our FreeEnhance outperforms the state-of-the-art image\nenhancement models in terms of quantitative metrics and human preference. More\nremarkably, FreeEnhance also shows higher human preference compared to the\ncommercial image enhancement solution of Magnific AI.",
        "updated": "2024-09-11 17:58:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07451v1"
    },
    {
        "title": "VMAS: Video-to-Music Generation via Semantic Alignment in Web Music Videos",
        "authors": "Yan-Bo LinYu TianLinjie YangGedas BertasiusHeng Wang",
        "links": "http://arxiv.org/abs/2409.07450v1",
        "entry_id": "http://arxiv.org/abs/2409.07450v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07450v1",
        "summary": "We present a framework for learning to generate background music from video\ninputs. Unlike existing works that rely on symbolic musical annotations, which\nare limited in quantity and diversity, our method leverages large-scale web\nvideos accompanied by background music. This enables our model to learn to\ngenerate realistic and diverse music. To accomplish this goal, we develop a\ngenerative video-music Transformer with a novel semantic video-music alignment\nscheme. Our model uses a joint autoregressive and contrastive learning\nobjective, which encourages the generation of music aligned with high-level\nvideo content. We also introduce a novel video-beat alignment scheme to match\nthe generated music beats with the low-level motions in the video. Lastly, to\ncapture fine-grained visual cues in a video needed for realistic background\nmusic generation, we introduce a new temporal video encoder architecture,\nallowing us to efficiently process videos consisting of many densely sampled\nframes. We train our framework on our newly curated DISCO-MV dataset,\nconsisting of 2.2M video-music samples, which is orders of magnitude larger\nthan any prior datasets used for video music generation. Our method outperforms\nexisting approaches on the DISCO-MV and MusicCaps datasets according to various\nmusic generation evaluation metrics, including human evaluation. Results are\navailable at https://genjib.github.io/project_page/VMAs/index.html",
        "updated": "2024-09-11 17:56:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07450v1"
    }
]