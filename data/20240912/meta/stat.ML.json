[
    {
        "title": "Asymptotics of Stochastic Gradient Descent with Dropout Regularization in Linear Models",
        "authors": "Jiaqi LiJohannes Schmidt-HieberWei Biao Wu",
        "links": "http://arxiv.org/abs/2409.07434v1",
        "entry_id": "http://arxiv.org/abs/2409.07434v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07434v1",
        "summary": "This paper proposes an asymptotic theory for online inference of the\nstochastic gradient descent (SGD) iterates with dropout regularization in\nlinear regression. Specifically, we establish the geometric-moment contraction\n(GMC) for constant step-size SGD dropout iterates to show the existence of a\nunique stationary distribution of the dropout recursive function. By the GMC\nproperty, we provide quenched central limit theorems (CLT) for the difference\nbetween dropout and $\\ell^2$-regularized iterates, regardless of\ninitialization. The CLT for the difference between the Ruppert-Polyak averaged\nSGD (ASGD) with dropout and $\\ell^2$-regularized iterates is also presented.\nBased on these asymptotic normality results, we further introduce an online\nestimator for the long-run covariance matrix of ASGD dropout to facilitate\ninference in a recursive manner with efficiency in computational time and\nmemory. The numerical experiments demonstrate that for sufficiently large\nsamples, the proposed confidence intervals for ASGD with dropout nearly achieve\nthe nominal coverage probability.",
        "updated": "2024-09-11 17:28:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07434v1"
    },
    {
        "title": "Synthetic continued pretraining",
        "authors": "Zitong YangNeil BandShuangping LiEmmanuel CandèsTatsunori Hashimoto",
        "links": "http://arxiv.org/abs/2409.07431v1",
        "entry_id": "http://arxiv.org/abs/2409.07431v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07431v1",
        "summary": "Pretraining on large-scale, unstructured internet text has enabled language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient -- to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining using EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.",
        "updated": "2024-09-11 17:21:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07431v1"
    },
    {
        "title": "Manifold Learning via Foliations and Knowledge Transfer",
        "authors": "E. TronE. Fioresi",
        "links": "http://arxiv.org/abs/2409.07412v1",
        "entry_id": "http://arxiv.org/abs/2409.07412v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07412v1",
        "summary": "Understanding how real data is distributed in high dimensional spaces is the\nkey to many tasks in machine learning. We want to provide a natural geometric\nstructure on the space of data employing a deep ReLU neural network trained as\na classifier. Through the data information matrix (DIM), a variation of the\nFisher information matrix, the model will discern a singular foliation\nstructure on the space of data. We show that the singular points of such\nfoliation are contained in a measure zero set, and that a local regular\nfoliation exists almost everywhere. Experiments show that the data is\ncorrelated with leaves of such foliation. Moreover we show the potential of our\napproach for knowledge transfer by analyzing the spectrum of the DIM to measure\ndistances between datasets.",
        "updated": "2024-09-11 16:53:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07412v1"
    },
    {
        "title": "Convergence of continuous-time stochastic gradient descent with applications to linear deep neural networks",
        "authors": "Gabor LugosiEulalia Nualart",
        "links": "http://arxiv.org/abs/2409.07401v1",
        "entry_id": "http://arxiv.org/abs/2409.07401v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07401v1",
        "summary": "We study a continuous-time approximation of the stochastic gradient descent\nprocess for minimizing the expected loss in learning problems. The main results\nestablish general sufficient conditions for the convergence, extending the\nresults of Chatterjee (2022) established for (nonstochastic) gradient descent.\nWe show how the main result can be applied to the case of overparametrized\nlinear neural network training.",
        "updated": "2024-09-11 16:40:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07401v1"
    },
    {
        "title": "A Scalable Algorithm for Active Learning",
        "authors": "Youguang ChenZheyu WenGeorge Biros",
        "links": "http://arxiv.org/abs/2409.07392v1",
        "entry_id": "http://arxiv.org/abs/2409.07392v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07392v1",
        "summary": "FIRAL is a recently proposed deterministic active learning algorithm for\nmulticlass classification using logistic regression. It was shown to outperform\nthe state-of-the-art in terms of accuracy and robustness and comes with\ntheoretical performance guarantees. However, its scalability suffers when\ndealing with datasets featuring a large number of points $n$, dimensions $d$,\nand classes $c$, due to its $\\mathcal{O}(c^2d^2+nc^2d)$ storage and\n$\\mathcal{O}(c^3(nd^2 + bd^3 + bn))$ computational complexity where $b$ is the\nnumber of points to select in active learning. To address these challenges, we\npropose an approximate algorithm with storage requirements reduced to\n$\\mathcal{O}(n(d+c) + cd^2)$ and a computational complexity of\n$\\mathcal{O}(bncd^2)$. Additionally, we present a parallel implementation on\nGPUs. We demonstrate the accuracy and scalability of our approach using MNIST,\nCIFAR-10, Caltech101, and ImageNet. The accuracy tests reveal no deterioration\nin accuracy compared to FIRAL. We report strong and weak scaling tests on up to\n12 GPUs, for three million point synthetic dataset.",
        "updated": "2024-09-11 16:34:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07392v1"
    }
]