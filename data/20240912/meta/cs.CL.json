[
    {
        "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories",
        "authors": "Ben BoginKejuan YangShashank GuptaKyle RichardsonErin BransomPeter ClarkAshish SabharwalTushar Khot",
        "links": "http://arxiv.org/abs/2409.07440v1",
        "entry_id": "http://arxiv.org/abs/2409.07440v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07440v1",
        "summary": "Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress.",
        "updated": "2024-09-11 17:37:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07440v1"
    },
    {
        "title": "A Suite for Acoustic Language Model Evaluation",
        "authors": "Gallil MaimonAmit RothYossi Adi",
        "links": "http://arxiv.org/abs/2409.07437v1",
        "entry_id": "http://arxiv.org/abs/2409.07437v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07437v1",
        "summary": "Speech language models have recently demonstrated great potential as\nuniversal speech processing systems. Such models have the ability to model the\nrich acoustic information existing in audio signals, beyond spoken content,\nsuch as emotion, background noise, etc. Despite this, evaluation benchmarks\nwhich evaluate awareness to a wide range of acoustic aspects, are lacking. To\nhelp bridge this gap, we introduce SALMon, a novel evaluation suite\nencompassing background noise, emotion, speaker identity and room impulse\nresponse. The proposed benchmarks both evaluate the consistency of the\ninspected element and how much it matches the spoken text. We follow a\nmodelling based approach, measuring whether a model gives correct samples\nhigher scores than incorrect ones. This approach makes the benchmark fast to\ncompute even for large models. We evaluated several speech language models on\nSALMon, thus highlighting the strengths and weaknesses of each evaluated\nmethod. Code and data are publicly available at\nhttps://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .",
        "updated": "2024-09-11 17:34:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07437v1"
    },
    {
        "title": "Synthetic continued pretraining",
        "authors": "Zitong YangNeil BandShuangping LiEmmanuel CandèsTatsunori Hashimoto",
        "links": "http://arxiv.org/abs/2409.07431v1",
        "entry_id": "http://arxiv.org/abs/2409.07431v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07431v1",
        "summary": "Pretraining on large-scale, unstructured internet text has enabled language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient -- to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining using EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.",
        "updated": "2024-09-11 17:21:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07431v1"
    },
    {
        "title": "Agent Workflow Memory",
        "authors": "Zora Zhiruo WangJiayuan MaoDaniel FriedGraham Neubig",
        "links": "http://arxiv.org/abs/2409.07429v1",
        "entry_id": "http://arxiv.org/abs/2409.07429v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07429v1",
        "summary": "Despite the potential of language model-based agents to solve real-world\ntasks such as web navigation, current methods still struggle with long-horizon\ntasks with complex action trajectories. In contrast, humans can flexibly solve\ncomplex tasks by learning reusable task workflows from past experiences and\nusing them to guide future actions. To build agents that can similarly benefit\nfrom this process, we introduce Agent Workflow Memory (AWM), a method for\ninducing commonly reused routines, i.e., workflows, and selectively providing\nworkflows to the agent to guide subsequent generations. AWM flexibly applies to\nboth offline and online scenarios, where agents induce workflows from training\nexamples beforehand or from test queries on the fly. We experiment on two major\nweb navigation benchmarks -- Mind2Web and WebArena -- that collectively cover\n1000+ tasks from 200+ domains across travel, shopping, and social media, among\nothers. AWM substantially improves the baseline results by 24.6% and 51.1%\nrelative success rate on Mind2Web and WebArena while reducing the number of\nsteps taken to solve WebArena tasks successfully. Furthermore, online AWM\nrobustly generalizes in cross-task, website, and domain evaluations, surpassing\nbaselines from 8.9 to 14.0 absolute points as train-test task distribution gaps\nwiden.",
        "updated": "2024-09-11 17:21:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07429v1"
    },
    {
        "title": "Towards Fairer Health Recommendations: finding informative unbiased samples via Word Sense Disambiguation",
        "authors": "Gavin ButtsPegah EmdadJethro LeeShannon SongChiman SalavatiWillmar Sosa DiazShiri Dori-HacohenFabricio Murai",
        "links": "http://arxiv.org/abs/2409.07424v1",
        "entry_id": "http://arxiv.org/abs/2409.07424v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07424v1",
        "summary": "There have been growing concerns around high-stake applications that rely on\nmodels trained with biased data, which consequently produce biased predictions,\noften harming the most vulnerable. In particular, biased medical data could\ncause health-related applications and recommender systems to create outputs\nthat jeopardize patient care and widen disparities in health outcomes. A recent\nframework titled Fairness via AI posits that, instead of attempting to correct\nmodel biases, researchers must focus on their root causes by using AI to debias\ndata. Inspired by this framework, we tackle bias detection in medical curricula\nusing NLP models, including LLMs, and evaluate them on a gold standard dataset\ncontaining 4,105 excerpts annotated by medical experts for bias from a large\ncorpus. We build on previous work by coauthors which augments the set of\nnegative samples with non-annotated text containing social identifier terms.\nHowever, some of these terms, especially those related to race and ethnicity,\ncan carry different meanings (e.g., \"white matter of spinal cord\"). To address\nthis issue, we propose the use of Word Sense Disambiguation models to refine\ndataset quality by removing irrelevant sentences. We then evaluate fine-tuned\nvariations of BERT models as well as GPT models with zero- and few-shot\nprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for\nbias detection, while fine-tuned BERT models generally perform well across all\nevaluated metrics.",
        "updated": "2024-09-11 17:10:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07424v1"
    }
]