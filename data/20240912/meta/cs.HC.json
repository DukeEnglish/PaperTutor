[
    {
        "title": "\"My Grade is Wrong!\": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays",
        "authors": "Shengxin HongChang CaiSixuan DuHaiyue FengSiyuan LiuXiuyi Fan",
        "links": "http://arxiv.org/abs/2409.07453v1",
        "entry_id": "http://arxiv.org/abs/2409.07453v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07453v1",
        "summary": "Interactive feedback, where feedback flows in both directions between teacher\nand student, is more effective than traditional one-way feedback. However, it\nis often too time-consuming for widespread use in educational practice. While\nLarge Language Models (LLMs) have potential for automating feedback, they\nstruggle with reasoning and interaction in an interactive setting. This paper\nintroduces CAELF, a Contestable AI Empowered LLM Framework for automating\ninteractive feedback. CAELF allows students to query, challenge, and clarify\ntheir feedback by integrating a multi-agent system with computational\nargumentation. Essays are first assessed by multiple Teaching-Assistant Agents\n(TA Agents), and then a Teacher Agent aggregates the evaluations through formal\nreasoning to generate feedback and grades. Students can further engage with the\nfeedback to refine their understanding. A case study on 500 critical thinking\nessays with user studies demonstrates that CAELF significantly improves\ninteractive feedback, enhancing the reasoning and interaction capabilities of\nLLMs. This approach offers a promising solution to overcoming the time and\nresource barriers that have limited the adoption of interactive feedback in\neducational settings.",
        "updated": "2024-09-11 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07453v1"
    },
    {
        "title": "Echoes of Privacy: Uncovering the Profiling Practices of Voice Assistants",
        "authors": "Tina KhezresmaeilzadehElaine ZhuKiersten GriecoDaniel J. DuboisKonstantinos PsounisDavid Choffnes",
        "links": "http://arxiv.org/abs/2409.07444v1",
        "entry_id": "http://arxiv.org/abs/2409.07444v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07444v1",
        "summary": "Many companies, including Google, Amazon, and Apple, offer voice assistants\nas a convenient solution for answering general voice queries and accessing\ntheir services. These voice assistants have gained popularity and can be easily\naccessed through various smart devices such as smartphones, smart speakers,\nsmartwatches, and an increasing array of other devices. However, this\nconvenience comes with potential privacy risks. For instance, while companies\nvaguely mention in their privacy policies that they may use voice interactions\nfor user profiling, it remains unclear to what extent this profiling occurs and\nwhether voice interactions pose greater privacy risks compared to other\ninteraction modalities.\n  In this paper, we conduct 1171 experiments involving a total of 24530 queries\nwith different personas and interaction modalities over the course of 20 months\nto characterize how the three most popular voice assistants profile their\nusers. We analyze factors such as the labels assigned to users, their accuracy,\nthe time taken to assign these labels, differences between voice and web\ninteractions, and the effectiveness of profiling remediation tools offered by\neach voice assistant. Our findings reveal that profiling can happen without\ninteraction, can be incorrect and inconsistent at times, may take several days\nto weeks for changes to occur, and can be influenced by the interaction\nmodality.",
        "updated": "2024-09-11 17:44:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07444v1"
    },
    {
        "title": "Trust Dynamics in Human-Autonomy Interaction: Uncover Associations between Trust Dynamics and Personal Characteristics",
        "authors": "Hyesun ChungX. Jessie Yang",
        "links": "http://arxiv.org/abs/2409.07406v1",
        "entry_id": "http://arxiv.org/abs/2409.07406v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07406v1",
        "summary": "While personal characteristics influence people's snapshot trust towards\nautonomous systems, their relationships with trust dynamics remain poorly\nunderstood. We conducted a human-subject experiment with 130 participants\nperforming a simulated surveillance task aided by an automated threat detector.\nA comprehensive pre-experimental survey collected data on participants'\npersonal characteristics across 12 constructs and 28 dimensions. Based on data\ncollected in the experiment, we clustered participants' trust dynamics into\nthree types and assessed differences among the three clusters in terms of\npersonal characteristics, behaviors, performance, and post-experiment ratings.\nParticipants were clustered into three groups, namely Bayesian decision makers,\ndisbelievers, and oscillators. Results showed that the clusters differ\nsignificantly in seven personal characteristics: masculinity, positive affect,\nextraversion, neuroticism, intellect, performance expectancy, and high\nexpectations. The disbelievers tend to have high neuroticism and low\nperformance expectancy. The oscillators tend to have higher scores in\nmasculinity, positive affect, extraversion and intellect. We also found\nsignificant differences in the behaviors and post-experiment ratings among the\nthree groups. The disbelievers are the least likely to blindly follow the\nrecommendations made by the automated threat detector. Based on the significant\npersonal characteristics, we developed a decision tree model to predict cluster\ntypes with an accuracy of 70%.",
        "updated": "2024-09-11 16:49:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07406v1"
    },
    {
        "title": "Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring System via Language Model Coordination",
        "authors": "Daniel Zhang-LiZheyuan ZhangJifan YuJoy Lim Jia YinShangqing TuLinlu GongHaohua WangZhiyuan LiuHuiqin LiuLei HouJuanzi Li",
        "links": "http://arxiv.org/abs/2409.07372v1",
        "entry_id": "http://arxiv.org/abs/2409.07372v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07372v1",
        "summary": "The vast pre-existing slides serve as rich and important materials to carry\nlecture knowledge. However, effectively leveraging lecture slides to serve\nstudents is difficult due to the multi-modal nature of slide content and the\nheterogeneous teaching actions. We study the problem of discovering effective\ndesigns that convert a slide into an interactive lecture. We develop\nSlide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring\nsystem that can (1) effectively convert an input lecture slide into a\nstructured teaching agenda consisting of a set of heterogeneous teaching\nactions; (2) create and manage an interactive lecture that generates responsive\ninteractions catering to student learning demands while regulating the\ninteractions to follow teaching actions. Slide2Lecture contains a complete\npipeline for learners to obtain an interactive classroom experience to learn\nthe slide. For teachers and developers, Slide2Lecture enables customization to\ncater to personalized demands. The evaluation rated by annotators and students\nshows that Slide2Lecture is effective in outperforming the remaining\nimplementation. Slide2Lecture's online deployment has made more than 200K\ninteraction with students in the 3K lecture sessions. We open source\nSlide2Lecture's implementation in\nhttps://anonymous.4open.science/r/slide2lecture-4210/.",
        "updated": "2024-09-11 16:03:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07372v1"
    },
    {
        "title": "Visual Compositional Data Analytics for Spatial Transcriptomics",
        "authors": "David HägeleYuxuan TangDaniel Weiskopf",
        "links": "http://arxiv.org/abs/2409.07306v1",
        "entry_id": "http://arxiv.org/abs/2409.07306v1",
        "pdf_url": "http://arxiv.org/pdf/2409.07306v1",
        "summary": "For the Bio+Med-Vis Challenge 2024, we propose a visual analytics system as a\nredesign for the scatter pie chart visualization of cell type proportions of\nspatial transcriptomics data. Our design uses three linked views: a view of the\nhistological image of the tissue, a stacked bar chart showing cell type\nproportions of the spots, and a scatter plot showing a dimensionality reduction\nof the multivariate proportions. Furthermore, we apply a compositional data\nanalysis framework, the Aitchison geometry, to the proportions for\ndimensionality reduction and $k$-means clustering. Leveraging brushing and\nlinking, the system allows one to explore and uncover patterns in the cell type\nmixtures and relate them to their spatial locations on the cellular tissue.\nThis redesign shifts the pattern recognition workload from the human visual\nsystem to computational methods commonly used in visual analytics. We provide\nthe code and setup instructions of our visual analytics system on GitHub\n(https://github.com/UniStuttgart-VISUS/va-for-spatial-transcriptomics).",
        "updated": "2024-09-11 14:36:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.07306v1"
    }
]