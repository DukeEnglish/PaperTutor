[
    {
        "title": "Establishing Rigorous and Cost-effective Clinical Trials for Artificial Intelligence Models",
        "authors": "Wanling GaoYunyou HuangDandan CuiZhuoming YuWenjing LiuXiaoshuang LiangJiahui ZhaoJiyue XieHao LiLi MaNing YeYumiao KangDingfeng LuoPeng PanWei HuangZhongmou LiuJizhong HuGangyuan ZhaoChongrong JiangFan HuangTianyi WeiSuqin TangBingjie XiaZhifei ZhangJianfeng Zhan",
        "links": "http://arxiv.org/abs/2407.08554v1",
        "entry_id": "http://arxiv.org/abs/2407.08554v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08554v1",
        "summary": "A profound gap persists between artificial intelligence (AI) and clinical\npractice in medicine, primarily due to the lack of rigorous and cost-effective\nevaluation methodologies. State-of-the-art and state-of-the-practice AI model\nevaluations are limited to laboratory studies on medical datasets or direct\nclinical trials with no or solely patient-centered controls. Moreover, the\ncrucial role of clinicians in collaborating with AI, pivotal for determining\nits impact on clinical practice, is often overlooked. For the first time, we\nemphasize the critical necessity for rigorous and cost-effective evaluation\nmethodologies for AI models in clinical practice, featuring\npatient/clinician-centered (dual-centered) AI randomized controlled trials\n(DC-AI RCTs) and virtual clinician-based in-silico trials (VC-MedAI) as an\neffective proxy for DC-AI RCTs. Leveraging 7500 diagnosis records from\ntwo-phase inaugural DC-AI RCTs across 14 medical centers with 125 clinicians,\nour results demonstrate the necessity of DC-AI RCTs and the effectiveness of\nVC-MedAI. Notably, VC-MedAI performs comparably to human clinicians,\nreplicating insights and conclusions from prospective DC-AI RCTs. We envision\nDC-AI RCTs and VC-MedAI as pivotal advancements, presenting innovative and\ntransformative evaluation methodologies for AI models in clinical practice,\noffering a preclinical-like setting mirroring conventional medicine, and\nreshaping development paradigms in a cost-effective and fast-iterative manner.\nChinese Clinical Trial Registration: ChiCTR2400086816.",
        "updated": "2024-07-11 14:37:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08554v1"
    },
    {
        "title": "DIDUP: Dynamic Iterative Development for UI Prototyping",
        "authors": "Jenny MaKarthik SreedharVivian LiuSitong WangPedro Alejandro PerezLydia B. Chilton",
        "links": "http://arxiv.org/abs/2407.08474v1",
        "entry_id": "http://arxiv.org/abs/2407.08474v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08474v1",
        "summary": "Large language models (LLMs) are remarkably good at writing code. A\nparticularly valuable case of human-LLM collaboration is code-based UI\nprototyping, a method for creating interactive prototypes that allows users to\nview and fully engage with a user interface. We conduct a formative study of\nGPT Pilot, a leading LLM-generated code-prototyping system, and find that its\ninflexibility towards change once development has started leads to weaknesses\nin failure prevention and dynamic planning; it closely resembles the linear\nworkflow of the waterfall model. We introduce DIDUP, a system for code-based UI\nprototyping that follows an iterative spiral model, which takes changes and\niterations that come up during the development process into account. We propose\nthree novel mechanisms for LLM-generated code-prototyping systems: (1) adaptive\nplanning, where plans should be dynamic and reflect changes during\nimplementation, (2) code injection, where the system should write a minimal\namount of code and inject it instead of rewriting code so users have a better\nmental model of the code evolution, and (3) lightweight state management, a\nsimplified version of source control so users can quickly revert to different\nworking states. Together, this enables users to rapidly develop and iterate on\nprototypes.",
        "updated": "2024-07-11 13:10:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08474v1"
    },
    {
        "title": "Towards a Quality Approach to Hierarchical Color Maps",
        "authors": "Tobias MertzJörn Kohlhammer",
        "links": "http://arxiv.org/abs/2407.08287v1",
        "entry_id": "http://arxiv.org/abs/2407.08287v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08287v1",
        "summary": "To improve the perception of hierarchical structures in data sets, several\ncolor map generation algorithms have been proposed to take this structure into\naccount. But the design of hierarchical color maps elicits different\nrequirements to those of color maps for tabular data. Within this paper, we\nmake an initial effort to put design rules from the color map literature into\nthe context of hierarchical color maps. We investigate the impact of several\ndesign decisions and provide recommendations for various analysis scenarios.\nThus, we lay the foundation for objective quality criteria to evaluate\nhierarchical color maps.",
        "updated": "2024-07-11 08:32:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08287v1"
    },
    {
        "title": "Leveraging LLMs to Predict Affective States via Smartphone Sensor Features",
        "authors": "Tianyi ZhangSongyan TengHong JiaSimon D'Alfonso",
        "links": "http://arxiv.org/abs/2407.08240v1",
        "entry_id": "http://arxiv.org/abs/2407.08240v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08240v1",
        "summary": "As mental health issues for young adults present a pressing public health\nconcern, daily digital mood monitoring for early detection has become an\nimportant prospect. An active research area, digital phenotyping, involves\ncollecting and analysing data from personal digital devices such as smartphones\n(usage and sensors) and wearables to infer behaviours and mental health. Whilst\nthis data is standardly analysed using statistical and machine learning\napproaches, the emergence of large language models (LLMs) offers a new approach\nto make sense of smartphone sensing data. Despite their effectiveness across\nvarious domains, LLMs remain relatively unexplored in digital mental health,\nparticularly in integrating mobile sensor data. Our study aims to bridge this\ngap by employing LLMs to predict affect outcomes based on smartphone sensing\ndata from university students. We demonstrate the efficacy of zero-shot and\nfew-shot embedding LLMs in inferring general wellbeing. Our findings reveal\nthat LLMs can make promising predictions of affect measures using solely\nsmartphone sensing data. This research sheds light on the potential of LLMs for\naffective state prediction, emphasizing the intricate link between smartphone\nbehavioral patterns and affective states. To our knowledge, this is the first\nwork to leverage LLMs for affective state prediction and digital phenotyping\ntasks.",
        "updated": "2024-07-11 07:37:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08240v1"
    },
    {
        "title": "Generating Contextually-Relevant Navigation Instructions for Blind and Low Vision People",
        "authors": "Zain MerchantAbrar AnwarEmily WangSouti ChattopadhyayJesse Thomason",
        "links": "http://arxiv.org/abs/2407.08219v1",
        "entry_id": "http://arxiv.org/abs/2407.08219v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08219v1",
        "summary": "Navigating unfamiliar environments presents significant challenges for blind\nand low-vision (BLV) individuals. In this work, we construct a dataset of\nimages and goals across different scenarios such as searching through kitchens\nor navigating outdoors. We then investigate how grounded instruction generation\nmethods can provide contextually-relevant navigational guidance to users in\nthese instances. Through a sighted user study, we demonstrate that large\npretrained language models can produce correct and useful instructions\nperceived as beneficial for BLV users. We also conduct a survey and interview\nwith 4 BLV users and observe useful insights on preferences for different\ninstructions based on the scenario.",
        "updated": "2024-07-11 06:40:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08219v1"
    }
]