[
    {
        "title": "Video Diffusion Alignment via Reward Gradients",
        "authors": "Mihir PrabhudesaiRussell MendoncaZheyang QinKaterina FragkiadakiDeepak Pathak",
        "links": "http://arxiv.org/abs/2407.08737v1",
        "entry_id": "http://arxiv.org/abs/2407.08737v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08737v1",
        "summary": "We have made significant progress towards building foundational video\ndiffusion models. As these models are trained using large-scale unsupervised\ndata, it has become crucial to adapt these models to specific downstream tasks.\nAdapting these models via supervised fine-tuning requires collecting target\ndatasets of videos, which is challenging and tedious. In this work, we utilize\npre-trained reward models that are learned via preferences on top of powerful\nvision discriminative models to adapt video diffusion models. These models\ncontain dense gradient information with respect to generated RGB pixels, which\nis critical to efficient learning in complex search spaces, such as videos. We\nshow that backpropagating gradients from these reward models to a video\ndiffusion model can allow for compute and sample efficient alignment of the\nvideo diffusion model. We show results across a variety of reward models and\nvideo diffusion models, demonstrating that our approach can learn much more\nefficiently in terms of reward queries and computation than prior gradient-free\napproaches. Our code, model weights,and more visualization are available at\nhttps://vader-vid.github.io.",
        "updated": "2024-07-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08737v1"
    },
    {
        "title": "Transformer Circuit Faithfulness Metrics are not Robust",
        "authors": "Joseph MillerBilal ChughtaiWilliam Saunders",
        "links": "http://arxiv.org/abs/2407.08734v1",
        "entry_id": "http://arxiv.org/abs/2407.08734v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08734v1",
        "summary": "Mechanistic interpretability work attempts to reverse engineer the learned\nalgorithms present inside neural networks. One focus of this work has been to\ndiscover 'circuits' -- subgraphs of the full model that explain behaviour on\nspecific tasks. But how do we measure the performance of such circuits? Prior\nwork has attempted to measure circuit 'faithfulness' -- the degree to which the\ncircuit replicates the performance of the full model. In this work, we survey\nmany considerations for designing experiments that measure circuit faithfulness\nby ablating portions of the model's computation. Concerningly, we find existing\nmethods are highly sensitive to seemingly insignificant changes in the ablation\nmethodology. We conclude that existing circuit faithfulness scores reflect both\nthe methodological choices of researchers as well as the actual components of\nthe circuit - the task a circuit is required to perform depends on the ablation\nused to test it. The ultimate goal of mechanistic interpretability work is to\nunderstand neural networks, so we emphasize the need for more clarity in the\nprecise claims being made about circuits. We open source a library at\nhttps://github.com/UFO-101/auto-circuit that includes highly efficient\nimplementations of a wide range of ablation methodologies and circuit discovery\nalgorithms.",
        "updated": "2024-07-11 17:59:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08734v1"
    },
    {
        "title": "BiEquiFormer: Bi-Equivariant Representations for Global Point Cloud Registration",
        "authors": "Stefanos PertigkiozoglouEvangelos ChatzipantazisKostas Daniilidis",
        "links": "http://arxiv.org/abs/2407.08729v1",
        "entry_id": "http://arxiv.org/abs/2407.08729v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08729v1",
        "summary": "The goal of this paper is to address the problem of \\textit{global} point\ncloud registration (PCR) i.e., finding the optimal alignment between point\nclouds irrespective of the initial poses of the scans. This problem is\nnotoriously challenging for classical optimization methods due to computational\nconstraints. First, we show that state-of-the-art deep learning methods suffer\nfrom huge performance degradation when the point clouds are arbitrarily placed\nin space. We propose that \\textit{equivariant deep learning} should be utilized\nfor solving this task and we characterize the specific type of bi-equivariance\nof PCR. Then, we design BiEquiformer a novel and scalable\n\\textit{bi-equivariant} pipeline i.e. equivariant to the independent\ntransformations of the input point clouds. While a naive approach would process\nthe point clouds independently we design expressive bi-equivariant layers that\nfuse the information from both point clouds. This allows us to extract\nhigh-quality superpoint correspondences and in turn, robust point-cloud\nregistration. Extensive comparisons against state-of-the-art methods show that\nour method achieves comparable performance in the canonical setting and\nsuperior performance in the robust setting in both the 3DMatch and the\nchallenging low-overlap 3DLoMatch dataset.",
        "updated": "2024-07-11 17:58:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08729v1"
    },
    {
        "title": "Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms",
        "authors": "Rayna AndreevaBenjamin DupuisRik SarkarTolga BirdalUmut Şimşekli",
        "links": "http://arxiv.org/abs/2407.08723v1",
        "entry_id": "http://arxiv.org/abs/2407.08723v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08723v1",
        "summary": "We present a novel set of rigorous and computationally efficient\ntopology-based complexity notions that exhibit a strong correlation with the\ngeneralization gap in modern deep neural networks (DNNs). DNNs show remarkable\ngeneralization properties, yet the source of these capabilities remains\nelusive, defying the established statistical learning theory. Recent studies\nhave revealed that properties of training trajectories can be indicative of\ngeneralization. Building on this insight, state-of-the-art methods have\nleveraged the topology of these trajectories, particularly their fractal\ndimension, to quantify generalization. Most existing works compute this\nquantity by assuming continuous- or infinite-time training dynamics,\ncomplicating the development of practical estimators capable of accurately\npredicting generalization without access to test data. In this paper, we\nrespect the discrete-time nature of training trajectories and investigate the\nunderlying topological quantities that can be amenable to topological data\nanalysis tools. This leads to a new family of reliable topological complexity\nmeasures that provably bound the generalization error, eliminating the need for\nrestrictive geometric assumptions. These measures are computationally friendly,\nenabling us to propose simple yet effective algorithms for computing\ngeneralization indices. Moreover, our flexible framework can be extended to\ndifferent domains, tasks, and architectures. Our experimental results\ndemonstrate that our new complexity measures correlate highly with\ngeneralization error in industry-standards architectures such as transformers\nand deep graph networks. Our approach consistently outperforms existing\ntopological bounds across a wide range of datasets, models, and optimizers,\nhighlighting the practical relevance and effectiveness of our complexity\nmeasures.",
        "updated": "2024-07-11 17:56:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08723v1"
    },
    {
        "title": "Unifying 3D Representation and Control of Diverse Robots with a Single Camera",
        "authors": "Sizhe Lester LiAnnan ZhangBoyuan ChenHanna MatusikChao LiuDaniela RusVincent Sitzmann",
        "links": "http://arxiv.org/abs/2407.08722v1",
        "entry_id": "http://arxiv.org/abs/2407.08722v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08722v1",
        "summary": "Mirroring the complex structures and diverse functions of natural organisms\nis a long-standing challenge in robotics. Modern fabrication techniques have\ndramatically expanded feasible hardware, yet deploying these systems requires\ncontrol software to translate desired motions into actuator commands. While\nconventional robots can easily be modeled as rigid links connected via joints,\nit remains an open challenge to model and control bio-inspired robots that are\noften multi-material or soft, lack sensing capabilities, and may change their\nmaterial properties with use. Here, we introduce Neural Jacobian Fields, an\narchitecture that autonomously learns to model and control robots from vision\nalone. Our approach makes no assumptions about the robot's materials,\nactuation, or sensing, requires only a single camera for control, and learns to\ncontrol the robot without expert intervention by observing the execution of\nrandom commands. We demonstrate our method on a diverse set of robot\nmanipulators, varying in actuation, materials, fabrication, and cost. Our\napproach achieves accurate closed-loop control and recovers the causal dynamic\nstructure of each robot. By enabling robot control with a generic camera as the\nonly sensor, we anticipate our work will dramatically broaden the design space\nof robotic systems and serve as a starting point for lowering the barrier to\nrobotic automation.",
        "updated": "2024-07-11 17:55:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08722v1"
    }
]