[
    {
        "title": "Transformer Circuit Faithfulness Metrics are not Robust",
        "authors": "Joseph MillerBilal ChughtaiWilliam Saunders",
        "links": "http://arxiv.org/abs/2407.08734v1",
        "entry_id": "http://arxiv.org/abs/2407.08734v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08734v1",
        "summary": "Mechanistic interpretability work attempts to reverse engineer the learned\nalgorithms present inside neural networks. One focus of this work has been to\ndiscover 'circuits' -- subgraphs of the full model that explain behaviour on\nspecific tasks. But how do we measure the performance of such circuits? Prior\nwork has attempted to measure circuit 'faithfulness' -- the degree to which the\ncircuit replicates the performance of the full model. In this work, we survey\nmany considerations for designing experiments that measure circuit faithfulness\nby ablating portions of the model's computation. Concerningly, we find existing\nmethods are highly sensitive to seemingly insignificant changes in the ablation\nmethodology. We conclude that existing circuit faithfulness scores reflect both\nthe methodological choices of researchers as well as the actual components of\nthe circuit - the task a circuit is required to perform depends on the ablation\nused to test it. The ultimate goal of mechanistic interpretability work is to\nunderstand neural networks, so we emphasize the need for more clarity in the\nprecise claims being made about circuits. We open source a library at\nhttps://github.com/UFO-101/auto-circuit that includes highly efficient\nimplementations of a wide range of ablation methodologies and circuit discovery\nalgorithms.",
        "updated": "2024-07-11 17:59:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08734v1"
    },
    {
        "title": "Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist",
        "authors": "Zihao ZhouShudong LiuMaizhen NingWei LiuJindong WangDerek F. WongXiaowei HuangQiufeng WangKaizhu Huang",
        "links": "http://arxiv.org/abs/2407.08733v1",
        "entry_id": "http://arxiv.org/abs/2407.08733v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08733v1",
        "summary": "Exceptional mathematical reasoning ability is one of the key features that\ndemonstrate the power of large language models (LLMs). How to comprehensively\ndefine and evaluate the mathematical abilities of LLMs, and even reflect the\nuser experience in real-world scenarios, has emerged as a critical issue.\nCurrent benchmarks predominantly concentrate on problem-solving capabilities,\nwhich presents a substantial risk of model overfitting and fails to accurately\nrepresent genuine mathematical reasoning abilities. In this paper, we argue\nthat if a model really understands a problem, it should be robustly and readily\napplied across a diverse array of tasks. Motivated by this, we introduce\nMATHCHECK, a well-designed checklist for testing task generalization and\nreasoning robustness, as well as an automatic tool to generate checklists\nefficiently. MATHCHECK includes multiple mathematical reasoning tasks and\nrobustness test types to facilitate a comprehensive evaluation of both\nmathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we\ndevelop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual\nreasoning and multi-modal reasoning capabilities, respectively, serving as\nupgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K.\nWe adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs,\nassessing their comprehensive mathematical reasoning abilities. Our results\ndemonstrate that while frontier LLMs like GPT-4o continue to excel in various\nabilities on the checklist, many other model families exhibit a significant\ndecline. Further experiments indicate that, compared to traditional math\nbenchmarks, MATHCHECK better reflects true mathematical abilities and\nrepresents mathematical intelligence more linearly, thereby supporting our\ndesign. On our MATHCHECK, we can easily conduct detailed behavior analysis to\ndeeply investigate models.",
        "updated": "2024-07-11 17:58:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08733v1"
    },
    {
        "title": "A Taxonomy for Data Contamination in Large Language Models",
        "authors": "Medha PalavalliAmanda BertschMatthew R. Gormley",
        "links": "http://arxiv.org/abs/2407.08716v1",
        "entry_id": "http://arxiv.org/abs/2407.08716v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08716v1",
        "summary": "Large language models pretrained on extensive web corpora demonstrate\nremarkable performance across a wide range of downstream tasks. However, a\ngrowing concern is data contamination, where evaluation datasets may be\ncontained in the pretraining corpus, inflating model performance.\nDecontamination, the process of detecting and removing such data, is a\npotential solution; yet these contaminants may originate from altered versions\nof the test set, evading detection during decontamination. How different types\nof contamination impact the performance of language models on downstream tasks\nis not fully understood. We present a taxonomy that categorizes the various\ntypes of contamination encountered by LLMs during the pretraining phase and\nidentify which types pose the highest risk. We analyze the impact of\ncontamination on two key NLP tasks -- summarization and question answering --\nrevealing how different types of contamination influence task performance\nduring evaluation.",
        "updated": "2024-07-11 17:50:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08716v1"
    },
    {
        "title": "GTA: A Benchmark for General Tool Agents",
        "authors": "Jize WangZerun MaYining LiSongyang ZhangCailian ChenKai ChenXinyi Le",
        "links": "http://arxiv.org/abs/2407.08713v1",
        "entry_id": "http://arxiv.org/abs/2407.08713v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08713v1",
        "summary": "Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.",
        "updated": "2024-07-11 17:50:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08713v1"
    },
    {
        "title": "Uncertainty Estimation of Large Language Models in Medical Question Answering",
        "authors": "Jiaxin WuYizhou YuHong-Yu Zhou",
        "links": "http://arxiv.org/abs/2407.08662v1",
        "entry_id": "http://arxiv.org/abs/2407.08662v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08662v1",
        "summary": "Large Language Models (LLMs) show promise for natural language generation in\nhealthcare, but risk hallucinating factually incorrect information. Deploying\nLLMs for medical question answering necessitates reliable uncertainty\nestimation (UE) methods to detect hallucinations. In this work, we benchmark\npopular UE methods with different model sizes on medical question-answering\ndatasets. Our results show that current approaches generally perform poorly in\nthis domain, highlighting the challenge of UE for medical applications. We also\nobserve that larger models tend to yield better results, suggesting a\ncorrelation between model size and the reliability of UE. To address these\nchallenges, we propose Two-phase Verification, a probability-free Uncertainty\nEstimation approach. First, an LLM generates a step-by-step explanation\nalongside its initial answer, followed by formulating verification questions to\ncheck the factual claims in the explanation. The model then answers these\nquestions twice: first independently, and then referencing the explanation.\nInconsistencies between the two sets of answers measure the uncertainty in the\noriginal response. We evaluate our approach on three biomedical\nquestion-answering datasets using Llama 2 Chat models and compare it against\nthe benchmarked baseline methods. The results show that our Two-phase\nVerification method achieves the best overall accuracy and stability across\nvarious datasets and model sizes, and its performance scales as the model size\nincreases.",
        "updated": "2024-07-11 16:51:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08662v1"
    }
]