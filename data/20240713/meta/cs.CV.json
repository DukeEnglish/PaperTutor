[
    {
        "title": "MAVIS: Mathematical Visual Instruction Tuning",
        "authors": "Renrui ZhangXinyu WeiDongzhi JiangYichi ZhangZiyu GuoChengzhuo TongJiaming LiuAojun ZhouBin WeiShanghang ZhangPeng GaoHongsheng Li",
        "links": "http://arxiv.org/abs/2407.08739v1",
        "entry_id": "http://arxiv.org/abs/2407.08739v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08739v1",
        "summary": "Multi-modal Large Language Models (MLLMs) have recently emerged as a\nsignificant focus in academia and industry. Despite their proficiency in\ngeneral multi-modal scenarios, the mathematical problem-solving capabilities in\nvisual contexts remain insufficiently explored. We identify three key areas\nwithin MLLMs that need to be improved: visual encoding of math diagrams,\ndiagram-language alignment, and mathematical reasoning skills. This draws forth\nan urgent demand for large-scale, high-quality data and training pipelines in\nvisual mathematics. In this paper, we propose MAVIS, the first MAthematical\nVISual instruction tuning paradigm for MLLMs, involving a series of\nmathematical visual datasets and specialized MLLMs. Targeting the three issues,\nMAVIS contains three progressive training stages from scratch. First, we curate\nMAVIS-Caption, consisting of 558K diagram-caption pairs, to fine-tune a\nmath-specific vision encoder (CLIP-Math) through contrastive learning, tailored\nfor improved diagram visual encoding. Second, we utilize MAVIS-Caption to align\nthe CLIP-Math with a large language model (LLM) by a projection layer,\nenhancing vision-language alignment in mathematical domains. Third, we\nintroduce MAVIS-Instruct, including 900K meticulously collected and annotated\nvisual math problems, which is adopted to finally instruct-tune the MLLM for\nrobust mathematical reasoning skills. In MAVIS-Instruct, we incorporate\ncomplete chain-of-thought (CoT) rationales for each problem, and minimize\ntextual redundancy, thereby concentrating the model towards the visual\nelements. Data and Models are released at https://github.com/ZrrSkywalker/MAVIS",
        "updated": "2024-07-11 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08739v1"
    },
    {
        "title": "Video Diffusion Alignment via Reward Gradients",
        "authors": "Mihir PrabhudesaiRussell MendoncaZheyang QinKaterina FragkiadakiDeepak Pathak",
        "links": "http://arxiv.org/abs/2407.08737v1",
        "entry_id": "http://arxiv.org/abs/2407.08737v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08737v1",
        "summary": "We have made significant progress towards building foundational video\ndiffusion models. As these models are trained using large-scale unsupervised\ndata, it has become crucial to adapt these models to specific downstream tasks.\nAdapting these models via supervised fine-tuning requires collecting target\ndatasets of videos, which is challenging and tedious. In this work, we utilize\npre-trained reward models that are learned via preferences on top of powerful\nvision discriminative models to adapt video diffusion models. These models\ncontain dense gradient information with respect to generated RGB pixels, which\nis critical to efficient learning in complex search spaces, such as videos. We\nshow that backpropagating gradients from these reward models to a video\ndiffusion model can allow for compute and sample efficient alignment of the\nvideo diffusion model. We show results across a variety of reward models and\nvideo diffusion models, demonstrating that our approach can learn much more\nefficiently in terms of reward queries and computation than prior gradient-free\napproaches. Our code, model weights,and more visualization are available at\nhttps://vader-vid.github.io.",
        "updated": "2024-07-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08737v1"
    },
    {
        "title": "BiEquiFormer: Bi-Equivariant Representations for Global Point Cloud Registration",
        "authors": "Stefanos PertigkiozoglouEvangelos ChatzipantazisKostas Daniilidis",
        "links": "http://arxiv.org/abs/2407.08729v1",
        "entry_id": "http://arxiv.org/abs/2407.08729v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08729v1",
        "summary": "The goal of this paper is to address the problem of \\textit{global} point\ncloud registration (PCR) i.e., finding the optimal alignment between point\nclouds irrespective of the initial poses of the scans. This problem is\nnotoriously challenging for classical optimization methods due to computational\nconstraints. First, we show that state-of-the-art deep learning methods suffer\nfrom huge performance degradation when the point clouds are arbitrarily placed\nin space. We propose that \\textit{equivariant deep learning} should be utilized\nfor solving this task and we characterize the specific type of bi-equivariance\nof PCR. Then, we design BiEquiformer a novel and scalable\n\\textit{bi-equivariant} pipeline i.e. equivariant to the independent\ntransformations of the input point clouds. While a naive approach would process\nthe point clouds independently we design expressive bi-equivariant layers that\nfuse the information from both point clouds. This allows us to extract\nhigh-quality superpoint correspondences and in turn, robust point-cloud\nregistration. Extensive comparisons against state-of-the-art methods show that\nour method achieves comparable performance in the canonical setting and\nsuperior performance in the robust setting in both the 3DMatch and the\nchallenging low-overlap 3DLoMatch dataset.",
        "updated": "2024-07-11 17:58:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08729v1"
    },
    {
        "title": "Map It Anywhere (MIA): Empowering Bird's Eye View Mapping using Large-scale Public Data",
        "authors": "Cherie HoJiaye ZouOmar AlamaSai Mitheran Jagadesh KumarBenjamin ChiangTaneesh GuptaChen WangNikhil KeethaKatia SycaraSebastian Scherer",
        "links": "http://arxiv.org/abs/2407.08726v1",
        "entry_id": "http://arxiv.org/abs/2407.08726v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08726v1",
        "summary": "Top-down Bird's Eye View (BEV) maps are a popular representation for ground\nrobot navigation due to their richness and flexibility for downstream tasks.\nWhile recent methods have shown promise for predicting BEV maps from\nFirst-Person View (FPV) images, their generalizability is limited to small\nregions captured by current autonomous vehicle-based datasets. In this context,\nwe show that a more scalable approach towards generalizable map prediction can\nbe enabled by using two large-scale crowd-sourced mapping platforms, Mapillary\nfor FPV images and OpenStreetMap for BEV semantic maps. We introduce Map It\nAnywhere (MIA), a data engine that enables seamless curation and modeling of\nlabeled map prediction data from existing open-source map platforms. Using our\nMIA data engine, we display the ease of automatically collecting a dataset of\n1.2 million pairs of FPV images & BEV maps encompassing diverse geographies,\nlandscapes, environmental factors, camera models & capture scenarios. We\nfurther train a simple camera model-agnostic model on this data for BEV map\nprediction. Extensive evaluations using established benchmarks and our dataset\nshow that the data curated by MIA enables effective pretraining for\ngeneralizable BEV map prediction, with zero-shot performance far exceeding\nbaselines trained on existing datasets by 35%. Our analysis highlights the\npromise of using large-scale public maps for developing & testing generalizable\nBEV perception, paving the way for more robust autonomous navigation.",
        "updated": "2024-07-11 17:57:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08726v1"
    },
    {
        "title": "MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces",
        "authors": "Wayne WuHonglin HeYiran WangChenda DuanJack HeZhizheng LiuQuanyi LiBolei Zhou",
        "links": "http://arxiv.org/abs/2407.08725v1",
        "entry_id": "http://arxiv.org/abs/2407.08725v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08725v1",
        "summary": "Public urban spaces like streetscapes and plazas serve residents and\naccommodate social life in all its vibrant variations. Recent advances in\nRobotics and Embodied AI make public urban spaces no longer exclusive to\nhumans. Food delivery bots and electric wheelchairs have started sharing\nsidewalks with pedestrians, while diverse robot dogs and humanoids have\nrecently emerged in the street. Ensuring the generalizability and safety of\nthese forthcoming mobile machines is crucial when navigating through the\nbustling streets in urban spaces. In this work, we present MetaUrban, a\ncompositional simulation platform for Embodied AI research in urban spaces.\nMetaUrban can construct an infinite number of interactive urban scenes from\ncompositional elements, covering a vast array of ground plans, object\nplacements, pedestrians, vulnerable road users, and other mobile agents'\nappearances and dynamics. We design point navigation and social navigation\ntasks as the pilot study using MetaUrban for embodied AI research and establish\nvarious baselines of Reinforcement Learning and Imitation Learning. Experiments\ndemonstrate that the compositional nature of the simulated environments can\nsubstantially improve the generalizability and safety of the trained mobile\nagents. MetaUrban will be made publicly available to provide more research\nopportunities and foster safe and trustworthy embodied AI in urban spaces.",
        "updated": "2024-07-11 17:56:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08725v1"
    }
]