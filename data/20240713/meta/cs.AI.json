[
    {
        "title": "Video Diffusion Alignment via Reward Gradients",
        "authors": "Mihir PrabhudesaiRussell MendoncaZheyang QinKaterina FragkiadakiDeepak Pathak",
        "links": "http://arxiv.org/abs/2407.08737v1",
        "entry_id": "http://arxiv.org/abs/2407.08737v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08737v1",
        "summary": "We have made significant progress towards building foundational video\ndiffusion models. As these models are trained using large-scale unsupervised\ndata, it has become crucial to adapt these models to specific downstream tasks.\nAdapting these models via supervised fine-tuning requires collecting target\ndatasets of videos, which is challenging and tedious. In this work, we utilize\npre-trained reward models that are learned via preferences on top of powerful\nvision discriminative models to adapt video diffusion models. These models\ncontain dense gradient information with respect to generated RGB pixels, which\nis critical to efficient learning in complex search spaces, such as videos. We\nshow that backpropagating gradients from these reward models to a video\ndiffusion model can allow for compute and sample efficient alignment of the\nvideo diffusion model. We show results across a variety of reward models and\nvideo diffusion models, demonstrating that our approach can learn much more\nefficiently in terms of reward queries and computation than prior gradient-free\napproaches. Our code, model weights,and more visualization are available at\nhttps://vader-vid.github.io.",
        "updated": "2024-07-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08737v1"
    },
    {
        "title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
        "authors": "Rohan SinhaAmine ElhafsiChristopher AgiaMatthew FoutterEdward SchmerlingMarco Pavone",
        "links": "http://arxiv.org/abs/2407.08735v1",
        "entry_id": "http://arxiv.org/abs/2407.08735v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08735v1",
        "summary": "Foundation models, e.g., large language models (LLMs), trained on\ninternet-scale data possess zero-shot generalization capabilities that make\nthem a promising technology towards detecting and mitigating\nout-of-distribution failure modes of robotic systems. Fully realizing this\npromise, however, poses two challenges: (i) mitigating the considerable\ncomputational expense of these models such that they may be applied online, and\n(ii) incorporating their judgement regarding potential anomalies into a safe\ncontrol framework. In this work, we present a two-stage reasoning framework:\nFirst is a fast binary anomaly classifier that analyzes observations in an LLM\nembedding space, which may then trigger a slower fallback selection stage that\nutilizes the reasoning capabilities of generative LLMs. These stages correspond\nto branch points in a model predictive control strategy that maintains the\njoint feasibility of continuing along various fallback plans to account for the\nslow reasoner's latency as soon as an anomaly is detected, thus ensuring\nsafety. We show that our fast anomaly classifier outperforms autoregressive\nreasoning with state-of-the-art GPT models, even when instantiated with\nrelatively small language models. This enables our runtime monitor to improve\nthe trustworthiness of dynamic robotic systems, such as quadrotors or\nautonomous vehicles, under resource and time constraints. Videos illustrating\nour approach in both simulation and real-world experiments are available on\nthis project page: https://sites.google.com/view/aesop-llm.",
        "updated": "2024-07-11 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08735v1"
    },
    {
        "title": "Transformer Circuit Faithfulness Metrics are not Robust",
        "authors": "Joseph MillerBilal ChughtaiWilliam Saunders",
        "links": "http://arxiv.org/abs/2407.08734v1",
        "entry_id": "http://arxiv.org/abs/2407.08734v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08734v1",
        "summary": "Mechanistic interpretability work attempts to reverse engineer the learned\nalgorithms present inside neural networks. One focus of this work has been to\ndiscover 'circuits' -- subgraphs of the full model that explain behaviour on\nspecific tasks. But how do we measure the performance of such circuits? Prior\nwork has attempted to measure circuit 'faithfulness' -- the degree to which the\ncircuit replicates the performance of the full model. In this work, we survey\nmany considerations for designing experiments that measure circuit faithfulness\nby ablating portions of the model's computation. Concerningly, we find existing\nmethods are highly sensitive to seemingly insignificant changes in the ablation\nmethodology. We conclude that existing circuit faithfulness scores reflect both\nthe methodological choices of researchers as well as the actual components of\nthe circuit - the task a circuit is required to perform depends on the ablation\nused to test it. The ultimate goal of mechanistic interpretability work is to\nunderstand neural networks, so we emphasize the need for more clarity in the\nprecise claims being made about circuits. We open source a library at\nhttps://github.com/UFO-101/auto-circuit that includes highly efficient\nimplementations of a wide range of ablation methodologies and circuit discovery\nalgorithms.",
        "updated": "2024-07-11 17:59:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08734v1"
    },
    {
        "title": "MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces",
        "authors": "Wayne WuHonglin HeYiran WangChenda DuanJack HeZhizheng LiuQuanyi LiBolei Zhou",
        "links": "http://arxiv.org/abs/2407.08725v1",
        "entry_id": "http://arxiv.org/abs/2407.08725v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08725v1",
        "summary": "Public urban spaces like streetscapes and plazas serve residents and\naccommodate social life in all its vibrant variations. Recent advances in\nRobotics and Embodied AI make public urban spaces no longer exclusive to\nhumans. Food delivery bots and electric wheelchairs have started sharing\nsidewalks with pedestrians, while diverse robot dogs and humanoids have\nrecently emerged in the street. Ensuring the generalizability and safety of\nthese forthcoming mobile machines is crucial when navigating through the\nbustling streets in urban spaces. In this work, we present MetaUrban, a\ncompositional simulation platform for Embodied AI research in urban spaces.\nMetaUrban can construct an infinite number of interactive urban scenes from\ncompositional elements, covering a vast array of ground plans, object\nplacements, pedestrians, vulnerable road users, and other mobile agents'\nappearances and dynamics. We design point navigation and social navigation\ntasks as the pilot study using MetaUrban for embodied AI research and establish\nvarious baselines of Reinforcement Learning and Imitation Learning. Experiments\ndemonstrate that the compositional nature of the simulated environments can\nsubstantially improve the generalizability and safety of the trained mobile\nagents. MetaUrban will be made publicly available to provide more research\nopportunities and foster safe and trustworthy embodied AI in urban spaces.",
        "updated": "2024-07-11 17:56:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08725v1"
    },
    {
        "title": "WhisperNetV2: SlowFast Siamese Network For Lip-Based Biometrics",
        "authors": "Abdollah ZakeriHamid HassanpourMohammad Hossein KhosraviAmir Masoud Nourollah",
        "links": "http://arxiv.org/abs/2407.08717v1",
        "entry_id": "http://arxiv.org/abs/2407.08717v1",
        "pdf_url": "http://arxiv.org/pdf/2407.08717v1",
        "summary": "Lip-based biometric authentication (LBBA) has attracted many researchers\nduring the last decade. The lip is specifically interesting for biometric\nresearchers because it is a twin biometric with the potential to function both\nas a physiological and a behavioral trait. Although much valuable research was\nconducted on LBBA, none of them considered the different emotions of the client\nduring the video acquisition step of LBBA, which can potentially affect the\nclient's facial expressions and speech tempo. We proposed a novel network\nstructure called WhisperNetV2, which extends our previously proposed network\ncalled WhisperNet. Our proposed network leverages a deep Siamese structure with\ntriplet loss having three identical SlowFast networks as embedding networks.\nThe SlowFast network is an excellent candidate for our task since the fast\npathway extracts motion-related features (behavioral lip movements) with a high\nframe rate and low channel capacity. The slow pathway extracts visual features\n(physiological lip appearance) with a low frame rate and high channel capacity.\nUsing an open-set protocol, we trained our network using the CREMA-D dataset\nand acquired an Equal Error Rate (EER) of 0.005 on the test set. Considering\nthat the acquired EER is less than most similar LBBA methods, our method can be\nconsidered as a state-of-the-art LBBA method.",
        "updated": "2024-07-11 17:51:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.08717v1"
    }
]