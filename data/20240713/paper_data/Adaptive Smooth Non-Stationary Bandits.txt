Adaptive Smooth Non-Stationary Bandits
Joe Suk
Columbia University
joe.suk@columbia.edu
Abstract
We study a K-armed non-stationary bandit model where rewards change smoothly, as captured by
Hölder class assumptions on rewards as functions of time. Such smooth changes are parametrized by a
Hölderexponentβ andcoefficient λ. Whilevarioussub-casesof thisgeneral modelhavebeen studiedin
isolation, we first establish the minimax dynamic regret rate generally for all K,β,λ.
Next,weshowthisoptimaldynamicregretcanbeattainedadaptively,withoutknowledgeofβ,λ. To
contrast,evenwithparameterknowledge, upperboundswereonlypreviouslyknownforlimited regimes
β 1 and β = 2 (Slivkins, 2014; Krishnamurthy and Gopalan, 2021; Manegueu et al., 2021; Jia et al.,
≤
2023). Thus, our work resolves open questions raised by thesedisparate threadsof theliterature.
We also study the problem of attaining faster gap-dependent regret rates in non-stationary bandits.
Whilesuchratesarelongknowntobeimpossibleingeneral(GarivierandMoulines,2011),weshowthat
environmentsadmitting asafe arm (SukandKpotufe, 2022) allow for muchfaster ratesthan theworst-
case scaling with √T. Whileprevious worksin thisdirection focused on attaining theusual logarithmic
regret bounds, as summed over stationary periods, our new gap-dependent rates reveal new optimistic
regimes of non-stationarity where even the logarithmic bounds are pessimistic. We show our new gap-
dependentrateistightandthatitsachievability(i.e.,asmadepossiblebyasafearm)hasasurprisingly
simple and clean characterization within thesmooth Hölder class model.
1 Introduction
In the multi-armed bandit (MAB) problem, an agent sequentially chooses actions, from a set of K arms,
basedonpartialanduncertainfeedbackintheformof(bounded)rewardsY (a)forpastactionsa [K](see
t
∈
Bubeck and Cesa-Bianchi, 2012; Slivkins, 2019; Lattimore and Szepesvári, 2020, for general surveys). The
goal is to maximize the cumulative reward.
We consider the non-stationaryvariant of the problem, where rewards are obliviously adversarial. In partic-
ular, we consider a smooth rewardmodel of non-stationary MAB where only mild Hölder class assumptions
are made on changes in rewards over time. In fact, this model captures any finite-horizon bandit problem
(e.g., via a polynomial interpolation). Additionally, the degree of smoothness (as measured by the Hölder
exponent or coefficient of the associated Hölder class) can be considered a more fine-grained measure of
non-stationarity in comparison to conventional measures appearing in other works on non-stationary MAB.
Indeed,the ratesinthis modelsmoothlyinterpolatebetweenthe moreparametric√LT ratesseeninswitch-
ing bandits (Garivier and Moulines, 2011), with L switches in rewards over horizon T, and the V1/3T2/3
rates in terms of the total variation measure V quantifying magnitude of total changes in rewards (Besbes
et al., 2019).
The smooth model has been previously studied in bits and pieces. Most previous works (Slivkins, 2014;
Wei and Srivatsva, 2018;Komiyama et al., 2021;Krishnamurthy and Gopalan, 2021)focused on the case of
non-stationary rewards which are Lipschitz in time, which is also called slowly varying bandits. Recently,
Manegueu et al. (2021) studied the more general Hölder continuous rewards with Hölder exponent β 1
≤
(i.e., the non-differentiable regime), while Jia et al. (2023) studied differentiable Hölder reward functions.
The known (dynamic1) regret upper bounds are scant in these works (see Table 1), even when assuming
1asmeasuredtoatime-varyingsequence ofbestarms.
1
4202
luJ
11
]LM.tats[
1v45680.7042:viXraknowledge of the smoothness. Even more challenging,it remainedopen whether one could achieve adaptive
regret upper bounds without knowing the smoothness. This work resolves these questions and thus unifies
these disparate threads in the literature.
Ourresultisalsosomewhatsurprisingsincepriorapproaches(KrishnamurthyandGopalan,2021;Manegueu
etal.,2021;Jiaetal.,2023)moreorlessreliedonconfidenceintervalsonthemagnitudeofchangeinrewards,
whose design requires knowledge of the smoothness. In non-parametric statistics, it’s long been known
that it’s impossible to design confidence intervals adaptive to unknown smoothness (Low, 1997), ruling out
approaches of this kind.
1.1 Further Discussion on Related Works
Smooth Non-Stationary Bandits. To our knowledge, Slivkins (2014) is the first work to study the
slowly varying (i.e., Lipschitz rewards in time) bandit problem. Given a bound δ on the drift in rewards
between rounds, their Corollary 13 attains δ1/3 T dynamic regret via a reduction to Lipschitz contextual
. ·
banditswithdeterministiccontextX =t. Otherworksalsostudiedtheslowlyvaryingsettinggettingδ1/3 T
t
·
or δ1/4 T regret (Combes and Proutiere, 2014; Levine et al., 2017; Wei and Srivatsva, 2018; Seznec et al.,
·
2019;Trovòetal.,2020;Komiyamaetal.,2021;Ghoshetal.,2022). Someofthementionedworksonlyused
thedriftparameterδ asameasureofnon-stationaritywithinmorestructuredbanditproblems. Importantly,
all of the above works’ procedures rely on knowledge of δ. Recently, Krishnamurthy and Gopalan (2021)
showed the δ1/3 T rate is minimax for the class of slowly-varying problems with drift parameter δ.
·
Manegueuet al. (2021)studied a more generalHölder continuous model where rewards-in-timehave Hölder
exponent β (0,1], and established a regret upper bound with a procedure which requires knowledge of β.
∈
Jia et al. (2023) is the first work to study reward functions which are differentiable in time. They derive a
dynamicregretlowerboundandshowmatchingregretupperboundsforonceandtwicedifferentiablereward
functions. Once again, all mentioned regret upper bounds crucially rely on knowledge of the smoothness.
Switching and Other Non-Stationary Bandits. Switching bandits was first considered in the adver-
sarial setting by Auer et al. (2002), where a version of EXP3 was shown to attain optimal dynamic regret
√LT when tuned with knowledge of the number L of switches. Later works showed similar guarantees in
this problem for procedures inspired by stochastic bandit algorithms (Kocsis and Szepesvári, 2006; Yu and
Mannor, 2009; Garivier and Moulines, 2011; Mellor and Shapiro, 2013; Liu et al., 2018; Cao et al., 2019).
Recently, Auer et al. (2018, 2019); Chen et al. (2019) established the first adaptive and optimal dynamic
regret guarantees, without requiring knowledge of L. Other non-stationarity measures, such as the afore-
mentioned total variation, or more nuanced counts than L were recently studied (Suk and Kpotufe, 2022;
Abbasi-Yadkoriet al., 2023).
Online Learning with Drift. There’s also a related thread of works on online learning with drift where
the δ1/3 rateappears(HelmboldandLong,1991;Bartlett,1992;HelmboldandLong,1994;BarveandLong,
1997; Long, 1998; Mohri and Muñoz Medina, 2012; Hanneke and Yang, 2019; Mazzetto and Upfal, 2023).
Non-parametric Contextual Bandits. Hölder class assumptions appear broadly in non-parametric
statistics (Györfi et al., 2002; Tsybakov, 2009). In particular, Hölder smooth models also naturally ap-
pear in the contextual bandit problem (Woodroofe, 1979; Sarkar, 1991; Yang et al., 2002; Lu et al., 2009;
Rigollet and Zeevi, 2010; Perchet and Rigollet, 2013; Slivkins, 2014; Qian and Yang, 2016a,b; Reeve et al.,
2018; Guan and Jiang, 2018; Gur et al., 2022; Krishnamurthy et al., 2019; Hu et al., 2020; Arya and Yang,
2020;SukandKpotufe,2021;Caietal.,2024;SukandKpotufe,2023;Blanchardetal.,2023). Asmentioned
earlier,thesmoothnon-stationarybanditisinfactaspecialcaseofthe(stationary)smoothcontextualbandit
.
problem when taking the context X =t.
t
Interestingly,Guretal.(2022)showthatonecannotingeneralrate-optimallyadapttounknownsmoothness
for this problem. As such, adaptive guarantees for this setting are typically made using a self-similarity
2assumption (Qian and Yang, 2016b; Gur et al., 2022; Cai et al., 2024). However, these results concern
.
randomi.i.d. contexts. To contrast,ourresultsforthe X =t casearefully adaptiveto smoothnesswithout
t
requiring self-similarity.
1.2 Contributions
Our contributions are as follows:
(a) We show a dynamic regret lower bound for all Hölder classes of reward functions. New in this work,
we give a sharp characterization of the optimal dependence on the number of arms K and Hölder
coefficientλ, whichis notconsideredinthe lowerbounds ofpriorworks(KrishnamurthyandGopalan,
2021; Jia et al., 2023).
(b) We next show the META algorithm of Suk and Kpotufe (2022), which attains a dynamic regret bound
in terms of so-called significant switches in best arm, in fact attains the optimal regret for all Hölder
classes without any parameter knowledge.
(c) As a secondary contribution, we study gap-dependent rates for non-stationary bandits. For environ-
mentswithnosignificantswitch,weproposeanewgap-dependentratebasedontheideaofasignificant
shift oracle which plays arms until they incur large dynamic regret. We show that this gap-dependent
rate recovers a more pessimistic restarting oracle gap-dependent rate targeted by prior related works
(Mukherjee and Maillard, 2019; Seznec et al., 2020; Krishnamurthy and Gopalan, 2021). We show
our new rate is achievable without any parameter knowledge by a randomized elimination algorithm
inspiredbySukandKpotufe(2022). Importantly,thisshowsthat,solongasnosignificantshiftoccurs,
one can achieve much faster gap-dependent rates than previously thought possible.
(d) Relating this back to the smooth non-stationary bandit, we give a simple and sharp characterization,
in terms of the maximum Hölder coefficient, of which smooth bandit models admit these fast gap-
dependent regret rates.
Works on Smooth Bandits Adaptive? Parameters Dynamic Regret Upper Bound
β+1 1 β
Manegueu et al. (2021) No β (0,1] T2β+1λ2β+1K2β+1
∈
Slivkins (2014)
No β =1 Tλ31K31
Krishnamurthy and Gopalan (2021)
β+1 1
Jia et al. (2023) No β =1,2,K =2 T2β+1λ2β+1
This Work β+1 1 β
Yes β >0 T2·β+1λ2·β+1K2·β+1
(matching upper & lower bounds)
Table 1: A summary comparison of our dynamic regret bounds with those of prior works.
2 Problem Setup
2.1 Preliminaries and Notation
We assume an oblivious adversary decides a sequence of distributions on the rewards of K arms in [K].
Arm a at round t has random reward Y (a) [0,1] with mean µ (a). A (possibly randomized) algorithm π
t t
∈
selects ateach round t some arm π [K] and observesrewardY (π ). The goalis to minimize the dynamic
t t t
∈
regret, i.e., the expected regret to the best arm at each round. This is defined as:
T T
.
R(π,T)= max µ (a) E µ (π ) .
t t t
a∈[K] − " #
t=1 t=1
X X
3We’ll use R (π,T) to denote the expected regret under an environment .
E
E
.
In this paper, we rely heavily on analyzing the gaps in mean rewards between arms. Thus, let δ (a′,a) =
t
µ (a′) µ (a) denote the relative gap of arms a to a′ at round t. Define the absolute gap of arm a as
t .− t
δ t(a)=max a′∈[K]δ t(a′,a),correspondingtotheinstantaneousdynamicregretofplayingaatroundt. Then,
the dynamic regret can be written as E[δ (π )].
t∈[T] t t
Notation. Throughout this paper, inPtheorem statements we’ll use C ,C ,... to denote universal constants
0 1
free of K,T,β,λ, µ (a) . In proofs, universal constants c ,c ,... will be used.
t t∈[T],a∈[K] 0 1
{ }
2.2 Smooth Non-Stationary Bandits
We first recall the definition of a Hölder class of functions (Tsybakov, 2009,Definition 1.2).
Definition 1 (Hölder Class Function). For β,λ > 0, we say a function f : [0,1] R is (β,λ)-Hölder if f
. →
is m= β -times differentiable and
⌊ ⌋
x,x′ [0,1]: f(m)(x) f(m)(x′) λ x x′ β−m.
∀ ∈ | − |≤ ·| − |
.
By convention, we let the zero-th derivative be f(0)(x) = f(x). We call λ the Hölder coefficient whose
value may be taken as sup x6=x′ |f(m |) x( −x) x− ′|f β( −m m)(x′)|.
Next, we say a bandit environment is Hölder class if the absolute gaps, as functions of normalizedtime, are
(β,λ)-Hölder in the sense above.
Definition 2 (Hölder Gap Environments). We say a bandit environment is (β,λ)-Hölder if, for every arm
a [K], there exists a (β,λ)-Hölder function f such that the gap function (in time) is realized by f, i.e.
∈
δ (a) = f(t/T) for all t [T]. We’ll use Σ(β,λ) to denote the class of bandit environments which are
t
∈
(β,λ)-Hölder over T rounds.
We note that, unlike in the aforementioned prior works on smooth non-stationary bandits (Slivkins, 2014;
KrishnamurthyandGopalan,2021;Manegueuet al.,2021;Jiaet al.,2023),ourmodel only relies oncharac-
terizing the smoothness of the absolute gapfunctions δ (a) in time t, and not onthe rewardfunctions µ (a).
t t
In particular, changes in rewards can be arbitrarily rough and changes in rewards µ (a) which do not
t
change the gaps δ (a) do not enter into our regret rates.
t
3 Dynamic Regret Lower Bound
WefirstcharacterizetheminimaxregretrateovertheclassofproblemsinΣ(β,λ). Forcomparison,Jiaetal.
(2023, Theorem 3.4) already established a lower bound for integer smoothness β Z , K = 2 arms, and
≥1
∈
fixed Hölder coefficient λ = 1. Our main novelty here is to show a more comprehensive lower bound which
captures sharp dependence on all of T,K,λ.
Theorem 3. (Proof in Appendix A) Fix β,λ>0, K 2, and T N. For any algorithm π, there exists an
≥ ∈
environment Σ(β,λ) such that the regret is lower bounded by
E ∈
R E(π,T) Ω(min √KT +T2β ·β+ +1 1 λ2·β1 +1 K2·ββ +1,T ).
≥ { · · }
Note that if the gap functions t δ t() are C∞ smooth in time, the above rate of T2β β+ +1 1 λ2β1 +1 for (β,λ)-
7→ · ·
Hölder gaps becomes T1/2 as β . Thus, the rate of Theorem 3 interpolates the stationary regret rate
→ ∞
√T and the T2/3 regret seen in slowly-varyingβ =1 bandits (Krishnamurthy and Gopalan, 2021).
Remark1. Forβ =1(i.e., theslowly-varying setting),theaboveratebecomesT2/3λ1/3K1/3 =T (λ/T)1/3
· ·
K1/3 which is the rate seen in Slivkins (2014) for drift parameter δ =λ/T.
44 Dynamic Regret Upper Bound
As alluded in Subsection 1.2, our main dynamic regret upper bound is achieved by the META algorithm of
Suk and Kpotufe (2022) which adapts to so-called significant shifts in best arm. The key idea behind this
result is that a significant shift encodes large variation with respect to any (β,λ)-Hölder environment, thus
allowing us to recover the rate of Theorem 3. We now recall the notion of a significant shift.
First, we say arm a incurs significant regret2 on interval3 [s ,s ] if:
1 2
s2
δ (a) K (s s +1), (1)
t 2 1
≥ · −
t X=s1
p
orintuitivelyifitincurslargedynamicregret. Ontheotherhand,if (1)holdsfornointervalinatimeperiod,
then arm a incurs little regretover that period and is safe to play. Thus, a significant shift is recorded only
when there is no safe arm left to play. The following recursive definition captures this.
Definition 4. Let τ = 1. Then, recursively for i 0, the (i + 1)-th significant shift is recorded at
0
≥
time τ , which denotes the earliest time τ (τ ,T] such that for every arm a [K], there exists rounds
i+1 i
∈ ∈
s <s ,[s ,s ] [τ ,τ], such that arm a has significant regret (1) on [s ,s ].
1 2 1 2 i 1 2
⊆
We will refer to intervals [τ ,τ ),i 0, as significant phases. The unknown number of such phases (by
time T) is denoted L˜+1, wi heri+ eb1 y [τ≥ ,τ ), for τ =. T +1, denotes the last phase.
L˜ L˜+1 L˜+1
Then, a significant shift oracle, which roughly plays arms until they’re unsafe in each significant phase and
then restarts at each significant shift, attains a regret bound of (Suk and Kpotufe, 2022,Proposition 1)
L˜
K (τ τ ). (2)
i+1 i
· −
i=0
Xp
ThemainresultofSukandKpotufe(2022)istomatchtheaboverateuptologtermswithoutanyknowledge
of non-stationarity. Their META algorithm estimates when the significant shifts τ occur using importance-
i
weighted estimates of the gaps and then restarts an elimination procedure upon detecting an empirical
versionofasignificantshift. Theinnerworkingsofthealgorithmarebeyondthescopeofthisdiscussionand
surprisingly irrelevant for our result here; the interested reader is deferred to Section 4 of Suk and Kpotufe
(2022).
Ourmainresultisthat,independentofthealgorithmandforanyenvironment,theregretrate(2)inherently
captures the minimax rate for smooth non-stationary bandits.
Theorem 5 (ProofinAppendix B). Consider any (β,λ)-Hölder environment over T rounds and let τ L˜
{ i }i=0
be the significant shifts of the environment as in Definition 4. Then, we have:
L˜
K (τ i+1 τ i) C 0 β+1 √KT +T2β β+ +1 1 λ2β1 +1 K2ββ +1 .
· − ≤ · ·
Xi=0 p p (cid:16) (cid:17)
An immediate corollary is that the META algorithm can match the lower bound of Theorem 3 up to log
terms.
Corollary 6. By Theorem 1 of Suk and Kpotufe (2022), the META algorithm has an expected regret upper
bound:
R(π,T) C 1log(K)log2(T) β+1 √KT +T2β β+ +1 1 λ2β1 +1 K2ββ +1 .
≤ · ·
2OurdefinitionisslightlydifferentfromthatofSukpandKpot(cid:16)ufe(2022); allmentionedresultshold(cid:17)foreithernotions.
3Fromhereon,we’llconflate theintervals [a,b],[a,b)fora,b∈Nwiththenatural numbers containedwithin.
5Remark 2. Note that any non-stationary bandit environment over T rounds can be captured by a (β,λ)-
Hölderenvironmentforanyβ >0using,e.g.,aLagrangeinterpolationofthefinitedata (t/T,µ (a)) .
t t∈[T],a∈[K]
{ }
As T2β β+ +1 1 λ2β1 +1 K2ββ +1 √KT as β , this seems to suggest we can recover a stationary √KT regret
· · → →∞
rate for any non-stationary environment, which is seemingly a contradiction. However, taking β will
→ ∞
make the bound of Theorem 5 vacuous as there is a constant dependence of √β+1 on β. This suggests
perhaps the √β dependence is unavoidable, and it’s curious if such a dependence can be tightened.
5 Gap-Dependent Dynamic Regret Bounds
We next turn to the task of studying gap-dependent regret rates for non-stationary bandits. A first idea
is to characterize the rate as that achieved by a restarting oracle, or an oracle procedure which restarts a
stationaryprocedureateachchangepoint. Inother words,the gap-dependentdynamic regretrateis defined
as the sum over stationary periods of the stationary gap-dependent rates:
L
log(T)
, (3)
δ (a)
ℓ
Xℓ=1a:δXℓ(a)>0
where L is the number of stationary phases, the ℓ-th of which has gap profile δ (a) . Unfortunately,
ℓ a∈[K]
{ }
it’s long been known in the switching bandit literature that, in the worst case, (3) cannot be attained
simultaneouslyfor differentvaluesofL (GarivierandMoulines,2011;LattimoreandSzepesvári,2020). The
reason for such a hardness is intuitively because of the additional exploration required to detect unknown
changes, which forces √T regret.
Thus,anaturalquestionremains: underwhatconditionscantherate(3)beachieved? Yet,beforeanswering
this, an even more basic question is glaringly unaddressed. Earlier (Section 1), we discussed alternative
measures of non-stationarity (Besbes et al., 2019; Suk and Kpotufe, 2022; Abbasi-Yadkori et al., 2023),
calling into question whether (3) is even a sensible notion. For instance, (3) must scale with the number of
stationary periods L which can be as large as T even while the total variation remains small (Besbes et al.,
2019)orwhiletherearenochangesinbestarmorsignificantshifts (SukandKpotufe,2022;Abbasi-Yadkori
et al., 2023). Thus, it remains to be seen if there is a better gap-dependent rate, which is invariant of
irrelevant non-stationarity, which can also be achieved adaptively without knowledge of non-stationarity.
In our next contribution, we give answers to both these questions in terms of the significant shift oracle,
introduced in Section 4. Recalling such an oracle roughly plays arms until they incur significant regret (1),
andrestartsateachsignificantshift, we’llseethatacarefulregretanalysisofthisoraclegivesrisetoafaster
rate than (3) which is achievable adaptively in so-called safe environments (Definition 10).
Before getting into this, we summarize some of previous results in these directions.
5.1 Related Work on Gap-Dependent Regret
Tostart,wegiveanaccountofsomeworkswhichaimtoachievetherestartingoraclerate(3)understructured
non-stationarity:
• Mukherjee and Maillard (2019) show a bound similar to (3) (albeit with a multiplicative factor which
further depends on the difficulty of changes in gaps) under several assumptions on the changes: i.e.,
rewardsofallarmschangesimultaneously,andchangesarewell-separatedin time andlargeenoughin
magnitude so as to allow for fast-enough detection.
• Seznec et al. (2020) achieve the rate (3) in (restless) rotting bandits where rewards decrease in time.
• Bessonetal.(2022)studystructurednon-stationaritywherechangesaresufficientlydelayedintimeto
allowfordetection; they showa√LKT regretboundonnon-stationaryinstanceswhere the minimum
gap is Ω(1) and speculate, based on experimental findings, that their procedure could achieve faster
logarithmic regret (as in (3)) on some problem instances.
6To contrast, rather than directly making assumptions about the nature of changes, we show (Theorem 12)
therestartingoraclerate(3)canbeattainedunderanynon-stationaritysolongasasafearmremainsintact
(whichdrivesthenotionofsafe environment;cf. Definition10). Inparticular,changesofanykind(violating
the structural assumptions listed above) are allowed in a safe environment. In general, however,we caution
that our safe environment assumption is incomparable to the assumptions on changes made above.
On the other hand, we achieve rates much faster than (3) on safe environments. Notably, our new rate is
freeofirrelevantnon-stationarity(suchasscalingwiththe rawnumberLofchanges). The onlyotherresult,
to our knowledge, which studies faster rates of this kind is Krishnamurthy and Gopalan (2021). For K =2
armedbandits,theygiveanalternativegap-dependentrateintermsofaso-calleddetectablegapprofilewhich
quantifies what size aggregate gap is detectable over time (regardless of non-stationarity). However, while
their proposed regret rate is logarithmic in the best case, it could scale like √T even in safe environments.
Furthermore, the only procedure in said work achieving the detectable gap profile rate requires knowledge
of non-stationarity.
5.2 Refined Regret Analysis of the Significant Shift Oracle
FromthediscussionofAppendix AofSukandKpotufe (2022),itisalreadyevidentthatthe significantshift
oracle, which has oracle knowledge of when arms incur significant regret (1), can attain safe regret of order
√KT on each significant phase. Here, we argue that, on a single significant phase, a tighter gap-dependent
regret rate can be attained. To do so, we first set up some notation.
Notation 7. Let be be the σ-algebra generated by the random reward variables Y (a) and
t s s≤t,a∈[K]
H { }
exogenous time-varying randomness π , as used by an algorithm π.
s s≤t
{ }
We’lluset ,...,t todenoteorderedstoppingtimeswithrespecttothefiltration anduse , ,
1 K t t∈[T] 1 T
{H } S ··· S
to denote random subsets of [K] which are adapted to this filtration.
. .
Definition 8. Let t =1 and let =[K]. Then, we’ll recursively define t and for t>t as follows:
0 1 i t i−1
S S
a stopping time t > t is called an eviction time w.r.t. initial time t if = = =
i i−1 i−1 Sti−1 Sti−2
···
Sti−1
and
s2
δ (a)
s2
log(T)
s
a ,[s ,s ] [1,t 1]: C . (4)
∀
∈Sti−1 1 2
⊆
i
− ≤
2
v
s X=s1 |Ss | u us X=s1 |Ss |
t
We call (t ,t ,...,t ) a sequence of eviction times with associated safe armsets .
1 2 K 1 2 T
S ⊇S ⊇···⊇S
Remark 3. Definition 8 can be seen as a generalization of Definition 4. The eviction time t serves as a
i
more refined version of the first round when an arm becomes unsafe in the sense of (1) in Definition 4. The
only major difference is that (4) more carefully involves the variance of estimating each arm’s reward while
uniformly exploring actions in the safe armsets (as the significant shift oracle does). This modification is
t
S
crucial for capturing the exact dependence on the number of arms when comparing to the restarting oracle
rate (Theorem 9) and avoiding an extraneous log(K) factor in the analysis of Suk and Kpotufe (2022).
Then, given Definition 8, we propose the following new gap-dependent rate
.
K ti−1
R( t , ,π)= E [δ (a)], (5)
{
i }i∈[K] {St }t∈[T] a∼Unif{St} t
Xi=1t= Xti−1
Plainlyspeaking,(5)capturestheregretofthesignificantshiftoracle,orspecificallyaneliminationprocedure
which tracks the safe armsets and uniformly explores at round t. Note that (5) is a random quantity
t t
S S
as the t , may depend on the random rewards and exogenous randomness of some algorithm π. Despite
i t
S
this randomness, we’ll next show that for any valid t , satisfying Definition 8, the rate of (5) recovers (3)
i t
S
and is achievable adaptively in safe environments (Definition 10).
75.3 Properties of New Gap-Dependent Regret Rate
Proofs are deferred to Appendix C.
Theorem 9 (Recovering Restarting Oracle Rate). Let t , be a sequence of eviction times
i i∈[K] t t∈[T]
{ } {S }
and safe armsets per Definition 8. Then, for any environment with L stationary phases with the ℓ-th phase
having gap profile δ (a) , we have for any algorithm/randomness π :
ℓ a∈[K] t t∈[T]
{ } { }
L
log(T)
R( t , ,π) C2 .
{ i }i∈[K] {St }t∈[T] ≤ 2 δ (a)
ℓ
Xℓ=1a:δXℓ(a)>0
We next show (5) recoversthe usual √KT regretbound in safe environments. We first define such a notion.
Definition 10. A bandit environment over T rounds is called safe if, for any shrinking sequence of armsets
, there exists a safe arm a♯ such that
1 T
G ⊇···⊇G
s2 δ (a♯) s2 log(T)
t
[s ,s ] [1,T]: C . (6)
1 2 3
∀ ⊆ ≤ v
t X=s1 |Gt | u ut X=s1 |Gt |
t
In such an environment, any eviction times t K and safe armsets T can be assumed WLOG to
{ i }i=1 {Si }t=1
satisfy a♯ and t =T +1, as the safe arm a♯ always satisfies (4).
T K
∈S
Remark 4. A significant shift cannot occur in a safe environment. Indeed, taking [K] in (6) gives us
t
s2 δ (a♯)<C K log(T) (s s +1) which is a generalization of the reversG al o≡ f (1).
t=s1 t 3 · · 2 − 1
TPheorem 11 (Rep covering √KT Rate). We have, for any safe environment with eviction times and safe
armsets t , , and algorithm π :
i i∈[K] t t∈[T] t t∈[T]
{ } {S } { }
R( t , ,π) C KTlog(T).
i i∈[K] t t∈[T] 2
{ } {S } ≤
p
5.4 Elimination Achieves Gap-Dependent Regret Rate in Safe Environments
As alreadyhinted up to this point, we posit thata randomized variantofelimination (similar to that ofSuk
and Kpotufe, 2022) in fact attains the rate of (5) in safe environments. We view our contribution here as
not algorithmic, but rather a tighter regret analysis compared to Lemma 3 of Suk and Kpotufe (2022).
RandomizedSuccessiveElimination. Wepresent(Algorithm1)aslightlydifferentversionofAlgorithm
2 of Suk and Kpotufe (2022). First, we estimate the relative gap δ (a′,a) via4:
t
δˆ(a′,a)=.
Y (a′) 111 π =a′ Y (a) 111 π =a . (7)
t t t t t
· { }− · { }
Then, atroundt, we’lleliminate armsfromanactive armset when anempiricalanalogueof (4)basedon
t
A
δˆ(a′,a) and holds.
t t
A
Theorem 12. Given any safe bandit environment over T rounds, letting π be Algorithm 1, we have w.p. at
least 1 1/T2, for some algorithm-dependent eviction times and safe armsets t , :
i i∈[K] t t∈[T]
− { } {S }
T
δ (π ) C log(T)+R( t , ,π) .
t t 4 i i∈[K] t t∈[T]
≤ · { } {S }
t=1
X (cid:0) (cid:1)
4Thisestimatorisslightlydifferentthantheimportance-weightedoneusedinSukandKpotufe(2022);werequirethisform
tomimictheformof(4).
8Algorithm 1: Randomized Successive Elimination
1 Initialize: [K].
t
A ←
2 for t=1,2,...,T do
3 Play a random arm a selected with probability 1/ .
t t
∈A |A |
4 Evict bad arms:
5 a [K]: round t t s.t. max t δˆ (a′,a)>C t log(T) holds .
At ←At \
(cid:26)
∈ ∃ 0 ≤ a′∈At s=t0 s 5
q
s=t0 |As|
(cid:27)
P P
Putting the previous results together, we conclude that elimination not only attains the gap-dependent
restarting oracle rate L log(T) but further attains a much faster rate (5) which is free of
ℓ=1 a:δℓ(a)>0 δℓ(a)
irrelevant non-stationarity. In particular:
P P
• There is no dependence in (5) on L, the number of changes in rewards or even the number S of best
arm switches. We can in fact have S,L=Ω(T) while R( t , ,π) is small.
i i∈[K] t t∈[T]
{ } {S }
• As (5) only depends on the gaps, it is completely free of any changes in mean rewards which preserve
the gaps (i.e., rewards of arms changing together)
On the other hand, as mentioned earlier, it’s known in switching bandits that the restarting oracle rate (3)
cannot be achieved adaptively for unknown L. However, this does not contradict our findings because the
constructedhardenvironment(e.g. LattimoreandSzepesvári,2020,Theorem31.2)isnotsafe(Definition10).
Thus, similar to Suk and Kpotufe (2022), we find that the notion of significant shift (which decides the
safeness of an environment) characterizes difficult non-stationarity in a new sense. So long as such a shift
does not occur, we can attain the faster rate (5).
5.5 Lower Bound for Gap-Dependent Regret Rate
We next give a sense in which our new gap-dependentregretrate (5) is the best achievable rate. We do this
by showing that the minimax regret rate over the class of all non-stationary environments with bounded
R( t , ,π) R is Ω(R).
i i∈[K] t t∈[T]
{ } {S } ≤
Remark 5 (Log Factor not included in Lower Bound). We note the log(T) factor in (4) of Definition 8
was only included for the sake of showing the regret upper bounds established up to this point. Going forward,
we’ll ignore the log(T) factor when we refer to (4).
Theorem 13. Let t K be an arbitrary set of rounds such that t t + 1 K for all i [K]
{ i }i=1 . . i+1 − i ≥ ∈
with the convention that t = 1 and t = t = T + 1. Fix a positive real number R such that
0 K−1 K
R K−1 (t t ) (K+1 i). Let E be the class of environments such that (a) t ,...,t are
≤ i=1 i − i−1 · − 1 K
valid deterministic eviction times with C = (K 2)1/2 in (4) for some shrinking sequence of safe armsets
P p 2 −
and (b) such that:
1 T
S ⊇···⊇S
K ti−1
E [δ (a)] R.
a∼Unif{St} t
≤
Xi=1t= Xti−1
Then, for any algorithm π, we have:
supR (π,T) Ω(R).
E
E∈E ≥
6 Achievability of Gap-Dependent Rate in terms of Smoothness
We’veseenthattheachievabilityofournewgap-dependentrate(5)hingesonwhetheranenvironmentissafe
(Definition10),orroughlywhetherasignificantshiftoccurs. Inthesmoothbanditmodel,asafeenvironment
9.
is cleanly characterizedvia the “maximumHölder coefficient” Letf (x)=δ (a) be the normalized-in-time
a x·T
gap function for arm a. First we define this “maximum Hölder coefficient”.
.
λ = sup sup f(n)(x).
n a
a∈[K]x∈[0,1]
Then, it turns out an environment is safe if max λ K/T, while max λ > K/T allows for unsafe
n n n n
≤
environments. Thus, K/T isthecriticalvaluemarkingaphasetransitionintheachievabledynamicregret
p p
rates. Our final result, whose proof mostly re-packages earlier results, describes this phase transition.
p
Theorem 14 (Proof in Appendix C.5). We have:
(i) For any β,λ>0, any Hölder class Σ(β,λ) environment with max λ K/T is safe.
n=0,...,⌊β⌋ n
≤
(ii) For any n N, the minimax regret over the class of non-stationary environmentspwith λ λ for real
n
∈ ≤
λ> K/T is Ω(√KT).
p
References
Yasin Abbasi-Yadkori, András György, and Nevena Lazić. A new look at dynamic regret for non-
stationary stochastic bandits. Journal of Machine Learning Research, 24(288):1–37, 2023. URL
https://jmlr.org/papers/volume24/22-0387/22-0387.pdf.
SakshiArya and Yuhong Yang. Randomized allocationwith nonparametric estimationfor contextualmulti-
armed bandits with delayed rewards. Statistics & Probability Letters, 164:108818,2020. ISSN 0167-7152.
URL https://arxiv.org/pdf/1902.00819.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochas-
tic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. URL
http://rob.schapire.net/papers/AuerCeFrSc01.pdf.
Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best arm with an unknown num-
ber of distribution changes. 14th European Workshop on Reinforcement Learning (EWRL), 2018. URL
https://ewrl.wordpress.com/wp-content/uploads/2018/09/ewrl_14_2018_paper_28.pdf.
Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an un-
known number of distribution changes. Conference on Learning Theory, pages 138–158, 2019. URL
https://proceedings.mlr.press/v99/auer19a/auer19a.pdf.
Peter L Bartlett. Learning with a slowly changing distribution. In Proceedings of
the fifth annual workshop on Computational Learning Theory (COLT), 1992. URL
https://dl.acm.org/doi/pdf/10.1145/130385.130412.
Rakesh D Barve and Philip M Long. On the complexity of learning from drift-
ing distributions. Information and Computation, 138(2):170–193, 1997. URL
https://www.phillong.info/publications/distdrift.pdf.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Optimal exploration-exploitation in a multi-armed-
bandit problem with non-stationary rewards. Stochastic Systems, 9(4):319–337, 2019. URL
https://pubsonline.informs.org/doi/epdf/10.1287/stsy.2019.0033.
LilianBesson,EmilieKaufmann,Odalric-AmbrymMaillard,andJulienSeznec. Efficientchange-pointdetec-
tion for tackling piecewise-stationary bandits. Journal of Machine Learning Research, 23(77):1–40, 2022.
URL https://jmlr.org/papers/volume23/20-1384/20-1384.pdf.
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Con-
textual bandit algorithms with supervised learning guarantees. AISTATS, 2011. URL
https://proceedings.mlr.press/v15/beygelzimer11a/beygelzimer11a.pdf.
10MoiseBlanchard,SteveHanneke,andPatrickJaillet.Adversarialrewardsinuniversallearningforcontextual
bandits. arXiv preprint arXiv:2302.07186, 2023. URL https://arxiv.org/pdf/2302.07186.
Sébastien Bubeck and Nicoló Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. Foundations and Trends in Machine Learning, 5(1), 2012. URL
https://arxiv.org/pdf/1204.5721.pdf.
Changxiao Cai, T. Tony Cai, and Hongzhe Li. Transfer learning for contextual multi-armed bandits. The
Annals of Statistics, 52(1):207 – 232, 2024. URL https://doi.org/10.1214/23-AOS2341.
Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie. Nearly optimal adaptive pro-
cedure with change detection for piecewise-stationary bandit. Proceedings of the 22nd
International Conference on Artificial Intelligence and Statistics (AISTATS), 2019. URL
https://proceedings.mlr.press/v89/cao19a/cao19a.pdf.
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary con-
textual bandits: efficient, optimal, and parameter-free. In 32nd Annual Conference on Learning Theory,
2019. URL https://proceedings.mlr.press/v99/chen19b/chen19b.pdf.
Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and opti-
mal algorithms. In Proceedings of the 31st International Conference on International Confer-
ence on Machine Learning - Volume 32, ICML’14, page I–521–I–529. JMLR.org, 2014. URL
https://proceedings.mlr.press/v32/combes14.pdf.
AurélienGarivierandEric Moulines. Onupper-confidencebound policies for switching bandit problems. In
Proceedings of the 22nd International Conference on Algorithmic Learning Theory, pages 174–188. ALT
2011, Springer, 2011. URL https://hal.science/hal-00281392/document.
AvishekGhosh,Abishek Sankararaman,KannanRamchandran,TaraJavidi,andAryaMazumdar. Compet-
ing bandits in non-stationary matching markets. IEEE Transactions on Information Theory, 2022. URL
https://ieeexplore.ieee.org/document/10387415.
Melody Y Guan and Heinrich Jiang. Nonparametric stochastic contextual bandits. AAAI, 2018. URL
https://cdn.aaai.org/ojs/11749/11749-13-15277-1-2-20201228.pdf.
YonatanGur,AhmadrezaMomeni,andStefanWager. Smoothness-adaptivecontextualbandits. Operations
Research, 70(6):3198–3216,2022. URL https://arxiv.org/pdf/1910.09714.
LászlóGyörfi,MichaelKohler,AdamKrzyżak,andHarroWalk. A Distribution-FreeTheory of Nonparamet-
ric Regression. Springer, New York, 2002. URL https://link.springer.com/book/10.1007/b97848.
SteveHannekeandLiuYang. Statisticallearningundernonstationarymixingprocesses.InKamalikaChaud-
huri and Masashi Sugiyama,editors, Proceedings of the Twenty-Second International Conference on Arti-
ficial Intelligence and Statistics,volume89ofProceedings of Machine Learning Research,pages1678–1686.
PMLR, 16–18 Apr 2019. URL http://proceedings.mlr.press/v89/hanneke19a/hanneke19a.pdf.
David P Helmbold and Philip M Long. Tracking drifting concepts using random examples. In Pro-
ceedings of the fourth annual workshop on Computational Learning Theory (COLT), 1991. URL
https://dl.acm.org/doi/10.5555/114836.114839.
David P Helmbold and Philip M Long. Tracking drifting concepts by
minimizing disagreements. Machine learning, 14(1):27–45, 1994. URL
https://link.springer.com/content/pdf/10.1007/BF00993161.pdf.
Yichun Hu, Nathan Kallus, and Xiaojie Mao. Smooth contextual bandits: Bridging the para-
metric and non-differentiable regret regimes. Conference on Learning Theory, 2020. URL
https://arxiv.org/pdf/1909.02553.
S.Jia,QianXie,NathanKallus,andP.Frazier. Smoothnon-stationarybandits. InInternationalConference
on Machine Learning, 2023. URL https://dl.acm.org/doi/10.5555/3618408.3619017.
11Levente Kocsis and Csaba Szepesvári. Discounted ucb. 2nd PASCAL Challenges Workshop, 2006. URL
https://www.lri.fr/~sebag/Slides/Venice/Kocsis.pdf.
JunpeiKomiyama,EdouardFouché,andJunyaHonda. Finite-time analysisofgloballynonstationarymulti-
armedbandits. arXivpreprint: arXiv:2107.11419, 2021. URLhttps://arxiv.org/pdf/2107.11419.pdf.
AkshayKrishnamurthy,JohnLangford,AleksandrsSlivkins, andChicheng Zhang. Contextualbandits with
continuousactions: Smoothing,zooming,andadapting.InProceedingsoftheThirty-SecondConferenceon
LearningTheory,volume99ofProceedings of Machine LearningResearch,pages2025–2027.PMLR,25–28
Jun 2019. URL http://proceedings.mlr.press/v99/krishnamurthy19a/krishnamurthy19a.pdf.
Ramakrishnan Krishnamurthy and Aditya Gopalan. On slowly-varying non-stationary bandits. arXiv
preprint: arXiv:2110.12916, 2021. URL https://arxiv.org/pdf/2110.12916.pdf.
Tor Lattimore and Csaba Szepesvári. Bandit Algoritms. Cambridge University Press, 2020. URL
https://tor-lattimore.com/downloads/book/book.pdf.
Nir Levine, Koby Crammer, and Shie Mannor. Rotting bandits. NIPS, 2017. URL
https://proceedings.neurips.cc/paper_files/paper/2017/file/97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf.
Fang Liu, Joohyun Lee, and Ness Shroff. A change-detection based framework for piecewise-stationary
multi-armed bandit problem. Proceedings of the AAAI Conference on Artificial Intelligence, 2018. URL
https://dl.acm.org/doi/pdf/10.5555/3504035.3504482.
Philip M Long. The complexity of learning according to two models of a drifting en-
vironment. In Conference on Computational Learning Theory (COLT), 1998. URL
https://dl.acm.org/doi/pdf/10.1145/279943.279968.
Mark G. Low. On nonparametric confidence intervals. The Annals of Statistics, 25(6):2547 – 2554, 1997.
URL https://doi.org/10.1214/aos/1030741084.
TylerLu, DávidPál,andMartinPál. Showingrelevantadsviacontextmulti-armedbandits. InProceedings
of AISTATS, 2009. URL https://david.palenica.com/papers/clicks/lipschitz-clicks.pdf.
AnneGaelManegueu,AlexandraCarpentier,andYiYu. Generalizednon-stationarybandits. arXivpreprint:
arXiv:2102.00725, 2021. URL https://arxiv.org/pdf/2102.00725.pdf.
Alessio Mazzetto and Eli Upfal. An adaptive algorithm for learning with unknown distribution
drift. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL
https://openreview.net/forum?id=exiXmAfuDK.
Joseph Mellor and Jonathan Shapiro. Thompson sampling in switching environments with bayesian on-
line change detection. In Carlos M. Carvalho and Pradeep Ravikumar, editors, Proceedings of the Six-
teenth International Conference on Artificial Intelligence and Statistics, volume 31 of Proceedings of Ma-
chine Learning Research, pages 442–450, Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR. URL
https://proceedings.mlr.press/v31/mellor13a.html.
Mehryar Mohri and Andres Muñoz Medina. New analysis and algorithm for learning with drift-
ing distributions. In International Conference on Algorithmic Learning Theory (ALT), 2012. URL
https://arxiv.org/pdf/1205.4343.
SubhojyotiMukherjee andOdalric-AmbrymMaillard. Distribution-dependentand time-uniformbounds for
piecewise i.i.d bandits. Reinforcement Learning for Real Life (RL4RealLife) Workshop in the 36th Inter-
national Conference on Mearning Learning, 2019. URL https://openreview.net/pdf?id=Ske_J_SrjE.
Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. The Annals of
Statistics, 41(2):693–721,2013. URL https://arxiv.org/pdf/1110.6084.
12Wei Qian and Yuhong Yang. Kernel estimation and model combination in a bandit prob-
lem with covariates. Journal of Machine Learning Research, 17(149):1–37, 2016a. URL
https://jmlr.org/papers/volume17/13-210/13-210.pdf.
Wei Qian and Yuhong Yang. Randomized allocation with arm elimination in a bandit
problem with covariates. Electronic Journal of Statistics, 10(1):242 – 270, 2016b. URL
https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-10/issue-1/Randomized-alloca
Henry Reeve, Joe Mellor, and Gavin Brown. The k-nearest neighbour ucb algorithm for multi-
armed bandits with covariates. In Proceedings of Algorithmic Learning Theory, volume 83
of Proceedings of Machine Learning Research, pages 725–752. PMLR, 07–09 Apr 2018. URL
https://proceedings.mlr.press/v83/reeve18a/reeve18a.pdf.
Phillipe Rigollet and Assaf Zeevi. Nonparametric bandits with covariates. COLT, 2010. URL
https://arxiv.org/pdf/1003.1630.
Jyotirmoy Sarkar. One-armed bandit problems with covariates. The Annals of Statistics, pages 1978–2002,
1991.URLhttps://projecteuclid.org/journals/annals-of-statistics/volume-19/issue-4/One-Armed-Bandit-P
IgalSason and Sergio Verdú. f -divergenceinequalities. IEEE Transactions on Information Theory, 62(11):
5973–6006,2016. URL https://arxiv.org/pdf/1508.00335.
Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko.
Rotting bandits are no harder than stochastic ones. Proceedings of the Twenty-Second In-
ternational Conference on Artificial Intelligence and Statistics, pages 2564–2572, 2019. URL
https://proceedings.mlr.press/v89/seznec19a/seznec19a.pdf.
Julien Seznec, Pierre Menard, Alessandro Lazaric, and Michal Valko. A single algorithm for both restless
andrestedrottingbandits. Proceedings of the 22nd International Conference on Artificial Intelligence and
Statistics (AISTATS), 2020. URL https://proceedings.mlr.press/v108/seznec20a/seznec20a.pdf.
Aleksandrs Slivkins. Contextual bandits with similarity information. The Journal of Machine Learning
Research, 15(1):2533–2568,2014. URL https://arxiv.org/pdf/0907.3986.
Aleksandrs Slivkins. Introduction to multi-armed bandits. Foundations and Trends® in Ma-
chine Learning, 12(1-2):1–286, 2019. ISSN 1935-8237. doi: 10.1561/2200000068. URL
https://arxiv.org/pdf/1904.07272.pdf.
Joe Suk and Samory Kpotufe. Self-tuning bandits over unknown covariate-shifts.
International Conference on Algorithmic Learning Theory (ALT), 2021. URL
https://proceedings.mlr.press/v132/suk21a/suk21a.pdf.
Joe Suk and Samory Kpotufe. Tracking most significant arm switches in bandits. Conference on Learning
Theory (COLT), 2022. URL https://proceedings.mlr.press/v178/suk22a/suk22a.pdf.
Joe Suk and Samory Kpotufe. Tracking most significant shifts in nonparametric contex-
tual bandits. Advances in Neural Information Processing Systems (NeurIPS), 2023. URL
https://arxiv.org/pdf/2307.05341.pdf.
Francesco Trovò, Stefano Paladino, Marcello Restelli, and Nicola Gatti. Sliding-window thomp-
son sampling for non-stationary settings. J. Artif. Intell. Res., 68:311–364, 2020. URL
https://www.jair.org/index.php/jair/article/view/11407.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer series in statistics. Springer,
2009. ISBN 978-0-387-79051-0. URL https://doi.org/10.1007/b13794.
Lai Wei and Vaihbav Srivatsva. On abruptly-changing and slowly-varying multi-
armed bandit problems. Annual American Control Conference (ACC), 2018. URL
https://ieeexplore.ieee.org/document/8431265.
13Michael Woodroofe. A one-armed bandit problem with a concomitant variable.
Journal of the American Statistical Association, 74(368):799–806, 1979. URL
https://dept.stat.lsa.umich.edu/~michaelw/PPRS/1979jasa.pdf.
Yuhong Yang, Dan Zhu, et al. Randomized allocation with nonparametric estimation for a multi-
armed bandit problem with covariates. The Annals of Statistics, 30(1):100–121, 2002. URL
https://projecteuclid.org/journals/annals-of-statistics/volume-30/issue-1/Randomized-Allocation-with-
Jia Yuan Yu and Shie Mannor. Piecewise-stationary bandit problems with side observations. Proceed-
ings of the 26th Annual international Conference on Machine Learning, pages 1177–1184, 2009. URL
http://www.machinelearning.org/archive/icml2009/papers/367.pdf.
14A Proof of Dynamic Regret Lower Bound (Theorem 3)
Overview of Argument. The construction will rely on bump reward functions which also appear in the
classical minimax lower bounds for integrated risk of nonparametric regression of (β,L)-Hölder functions
(e.g., Section 2.5 of Tsybakov, 2009). This will be combined with a Le Cam’s method style of argument for
establishing a regret lower bound for stationary bandits (Lattimore and Szepesvári, 2020, Theorem 15.2),
which we’ll modify to work for segments of mild non-stationarity.
Preliminaries. First, getting some trivial cases out of the way, let’s assume that K T/4 or else we can
≤
just trivially show a lower bound of order K using a stationary construction (which is always (β,λ)-Hölder
for any β,λ>0).
Let’s also assume that
√KT T2β β+ +1 1 λ2β1 +1 K2ββ +1 K/T λ,
≤ · · ⇐⇒ ≤
or else again we can appeal to the well-known √KT stationary lowper bound.
Now,letλ˜ =. (2−(2β+1) (T/K)β) λ. LetM =. T2β1 +1 K− 2β1 +1 λ˜ 2β2 +1 . Now,sinceλ˜ (T/K)β 2−(2β+1),
· ∧ · · ≤ ·
we have that M T/4 . Next, we argue thalt WLOG M divides T.mIf this is not the case, then we can
≤ ⌈ ⌉ .
replace the horizon T with T = M T/M T, which is a multiple of M, and show the lower bound for
0
·⌊ ⌋ ≤
T which suffices for the end result since T T M T/2.
0 0
≥ − ≥
At a high level, we’ll construct M instances of a randomly selected (β,λ)-Hölder environment of length
T/M. We’ll then argue that the concatenationof any such realizedM environments is itself a (β,λ)-Hölder
environment over T rounds.
Intuitively, over each period of length T/M, the constructed sub-environment will be nearly stationary and
ensurearegretlowerboundoforder K (T/M). Then,summingovertheM sub-environmentsandtaking
·
a random prior over choice of instances, we get a dynamic regret lower bound of order
p
√TKM T2β β+ +1 1 λ˜ 2β1 +1 K2ββ +1.
≥ · ·
If λ˜ = λ, we are done. If λ˜ < λ, then T2β β+ +1 1λ2β1 +1K2ββ +1 c 0T so that it suffices to show a linear regret
lower bound. Since λ˜ < λ = λ˜ (T/K)β, plugging this≥ into the above RHS indeed gives us said linear
⇒ ∝
regret lower bound.
We proceed by first defining the sub-environment over T/M rounds.
Defining Bump Function Mean Rewards. First, define the function ϕ:[0,1] R as:
≥0
→
. λ˜ hβ x h/2
ϕ(x)= · Φ − ,
2 · h
(cid:18) (cid:19)
.
where h=1/M is a bandwidth and Φ is the C∞ bump function
. 1
Φ(u)=exp 111 u 1 .
−1 u2 · {k k≤ }
(cid:18) − (cid:19)
.
Now, consider an assignment of M best arms a=(a ,...,a ). For an arm a [K], we define the function
1 M
∈
ϕ a,a,i(x) as
. ϕ(x) a=a i
ϕ a,a,i(x)= .
( ϕ(x) a=a i
− 6
Then, for assignment a, the reward function of arm a will be defined as:
M
. 1
µ t,a(a)= + ϕ a,a,i(t/T).
2
i=1
X
15The above are valid bounded reward functions in [0,1] since by the definition of λ˜:
λ˜ hβ 1
· .
2 ≤ 2
Next,weclaimthatforanyarmassignmenta,theinducedbanditenvironmentis(β,λ)-Hölder(Definition2).
First,sinceϕis(β,λ/2)-Hölder(assertion(a)ofTsybakov,2009,Section2.5),therewardfunctiont µ t,a(a)
7→
for eacharma is (β,λ/2)-Hölderbeing a sumof (β,λ/2)-Hölderfunctions with disjointsupports. Then, the
gap functions t max a′µ t,a(a′) µ t,a(a) are (β,λ)-Hölder as a difference of two (β,λ/2)-Hölder functions.
7→ −
In fact, we note the induced environments would also be (β,λ)-Hölder if we had defined ϕ a,a,i as:
. 0 a=a i
ϕ˜ a,a,i(x)= .
( ϕ(x) a=a i
− 6
In what follows, we’ll make use of environments which use both formulas ϕ a,a,i(x) and ϕ˜ a,a,i(x). All reward
random variables Y (a) will be Bernoulli’s and we’ll only specify the means µ (a).
t t
.
Lower Bound for Sub-Environment. Letting n = T/M, we will show a regret lower bound over a
sub-environment of n rounds where there’s a fixed optimal arm. In particular, we claim that, over a sub-
environmentof n rounds, the gapof any suboptimal armwill be Ω λ˜ (n/T)β overa subdomain of length
·
Ω(n). This will be enough to sum up regret lower bounds over M d(cid:16)ifferent sub(cid:17)-environments.
Going into more detail, for x [h/8,7h/8],observe that
∈
λ˜ hβ 1 λ˜ hβ
ϕ(x) · exp · . (8)
≥ 2 · −1 3 2 !≥ 10
− 8
(cid:0) (cid:1)
Now, consider a sub-environment over n rounds (which we’ll for ease momentarily parametrize via [n])
1
E
on which arm 1 is optimal with:
. 1 0 a=1
t [n]:µ (a)= + .
t
∀ ∈ 2 ( ϕ(t/T) a=1
− 6
For any algorithm π, there must exist an arm a=1 for which the arm-pull count N (a)=. n 111 π =a
6 n t=1 { t }
satisfies E [N (a)] n since K E [N (a)]=n. Now, consider the environment on which arm a
E1 n ≤ K−1 a=2 E1 n Ea P
is instead optimal with reward function:
P
. 1
t [n]:µ (a)= +ϕ(t/T).
t
∀ ∈ 2
The rewardfunctions of all arms other than a in are defined identically to that of .
a 1
E E
Then,if N (1) n/2inenvironment , thenbypigeonholeatleastn/4roundsofthe roundsin [n/8,7n/8]
n 1
mustconsistof≤
suboptimalarmpulls
pE ayingaper-roundregretofatleast∆=. λ˜·hβ
by(8). Similarly,under
10
environment , if arm 1 is pulled more than n/2 times, then at least n/4 of the rounds in [n/8,7n/8]must
a
E
consist of pulls of arm 1 which forces a regret of at least ∆. Thus, we lower bound the total regret over n
rounds in and by:
1 a
E E
n∆
R (π,n) P (N (1) n/2)
E1
≥ 4 ·
E1 n
≤
n∆
R (π,n) P (N (1)>n/2).
Ea
≥ 4 ·
Ea n
Then, combining the above two displays with the Bretagnolle-Huber inequality (Lemma 20), we have:
n∆ n∆
R (π,n)+R (π,n) (P (N (1) n/2)+P (N (1)>n/2)) exp( KL( , )), (9)
E1 Ea
≥ 4
E1 n
≤
Ea n
≥ 8 −
P1 Pa
16where , are the respective induced distributions on the history of observations and decisions over the
1 a
P P
n rounds. We next decompose this KL divergence using chain rule:
n
1 t 1 t
KL( , )= E [111 π =a ] KL Ber ϕ ,Ber +ϕ .
P1 Pa P1
{
t
} · 2 − T 2 T
t=1 (cid:18) (cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19)
X
.
Next, we bound the KL between Bernoulli’s. Let ∆ =ϕ(t/T). Then:
t
1 t 1 t 1 1/2+∆ 1 1/2 ∆
t t
KL Ber ϕ ,Ber +ϕ = +∆ log + ∆ log − .
t t
2 − T 2 T 2 1/2 ∆ 2 − 1/2+∆
(cid:18) (cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19) (cid:18) (cid:19) (cid:18) − t(cid:19) (cid:18) (cid:19) (cid:18) t(cid:19)
Elementary calculations show the above RHS expression is at most 10∆2 for ∆ 1/4, which holds for all
t [n] since by the definition of λ˜ and since Φ(x) 1 for all x: t t ≤
∈ ≤
λ˜ hβ 1
t [n]:ϕ(t/T) · .
∀ ∈ ≤ 2 ≤ 4
Recalling that ∆
=. λ˜·hβ
, the above can also be re-phrased as ∆ 5∆ for t [n]. Thus, we obtain a KL
10 t ≤ ∈
bound of:
n K
KL( , ) E [N (a)] 250∆2 250 ∆2 2.5,
P1 Pa
≤
E1 n
· ≤ K 1 · · ≤ K 1 ·
− −
where the last inequality follows from ∆ 1 K (which holds from the definition of M). Noting that
≤ 10 n
q
K 1
exp 2.5
−K 1 · ≥ 100
(cid:18) − (cid:19)
for all K 2, the sub-environment lower bound of order Ω(n λ˜ (n/T)β) is concluded by combining the
≥ · ·
above display with (9).
Concatenating Different Sub-Environments. We first claim that the pair of environments and
1
E
can be analogously constructed for every length n interval of rounds i M +1,...,(i + 1) M for
a
E { · · }
i [T/M 1]. First, note that the expected dynamic regret can be written as:
∈ −
T T/M−1 (i+1)·M
R(π,T)=E δ (π ) = E E δ (π ) ,
t t t t i·M
" #   |H 
t=1 i=0 t=i·M+1
X X X
  
where is the filtration of history of observations and decisions up to round t. Now, for i [T/M 1],
t
H ∈ −
there must exist an arm a = 1 whose conditional arm-pull count E (i+1)·M 111 π =a n .
6 t=i·M+1 { t }|Hi·M ≤ K−1
T reh ge rn et,c bo yn Ωdi (t nion λ˜alo (nn
/H
Ti )·M
β), .wecandesignenvironments
E1
and
Ea
asbhe Pforeandlowerboundthecoinditional
· ·
Puttingeverythingtogether,wehavethereexistsanassignmenta=(a ,...,a )ofarmsandaconcatenated
1 M
banditenvironmentconsistingofbump functionrewardsasdefinedearlierforwhichthe totalregretislower
bounded by:
M n λ˜ n β c 1min T21 β+ +β 1 λ˜ 2β1 +1 K2ββ +1,T λ˜ T21 β+ +β 1 λ˜ 2β1 +1 K2ββ +1,
· · · T ≥ { · · · }∝ · ·
(cid:16) (cid:17)
wherethelastequalityholdsfromthedefinitionofλ˜ andtheearlierassumptionsofK T/4and K/T λ
≤ ≤
(which were made to rule out trivial cases).
p
17B Proof of Dynamic Regret Upper Bound (Theorem 5)
Overview of Argument. The main idea is that on each significant phase [τ ,τ ,), any arm a which is
i i+1
at one point optimal, i.e. δ (a) = 0 for some t [τ ,τ ), must have a gap function t δ (a) with large
t i i+1 t
∈ 7→
variationwithinthephasesincethe gapmustalsoatsomepointbe largebecauseofournotionofsignificant
regret(seeFact16). Moregenerally,ifthe gapfunctionis β timesdifferentiable,thenwecanfindanorder
⌊ ⌋
β criticalpointofthe gapfunction across β differentphasesusing Rolle’sTheorem. Using the definition
⌊ ⌋ ⌊ ⌋
of Hölder function (Definition 1), we can then bound the derivatives of the gap functions using this critical
point. Such bounds can in turn be plugged into an order- β Taylor approximation of the gap function.
⌊ ⌋
Ultimately, these calculations allow us to relate the phase length τ τ to the smoothness parameters
i+1 i
−
β,λ. The key claim is that each phase must be roughly at least length T2β2 +β 1λ− 2β2 +1K2β1 +1 which gives the
desired regret bound.
The actualproofwill requirea bit morecareas the optimalarmcanchangeeveryroundwithin a phaseand
some phases may be too short to have sufficient variation.
Getting into the proof, we first establish two key facts about significant phases which will be crucial later
on.
Fact 15. Each significant phase [τ ,τ ) with τ =T +1 is length at least K.
i i+1 i+1
6
Proof. This is true by our notion of significant regret (1) since we must have for some arm a and interval
[s ,s ] [τ ,τ ):
1 2 i i+1
⊆
s2
s s +1 δ (a) K (s s +1) = s s +1 K.
2 1 t 2 1 2 1
− ≥ ≥ · − ⇒ − ≥
t X=s1
p
Thus, τ τ s s +1 K. (cid:4)
i+1 i 2 1
− ≥ − ≥
Fact 16. For each significant phase [τ ,τ ) with τ =T +1, we must have for each arm a [K], there
i i+1 i+1
6 ∈
exists a round t [τ ,τ ] such that:
i i+1
∈
K
δ (a) .
t
≥sτ
i+1
τ i+1
−
Proof. By Definition 4, we have for each arm a [K], there exists [s ,s ] [τ ,τ ] such that:
1 2 i i+1
∈ ⊆
s2 s2
K
δ (a) K (s s +1) .
t 2 1
≥ · − ≥ sτ
i+1
τ i+1
t X=s1
p
t X=s1 −
The conclusion follows. (cid:4)
Now, if T K, then the desired regret rate is vacuous so we are done. Suppose T >K.
≤
.
Let m= β . We next decompose the dynamic regret bound in terms of significant phases according to the
⌊ ⌋
length of the significant phase:
L˜
K (τ τ )=√KT + K (τ τ )+ K (τ τ ),
i+1 i i+1 i i+1 i
· − · − · −
Xi=0
p
i:τi+1−τXi>(m+1)·K
p
i:τi+1−τXi≤(m+1)·K
p
Our goal will be to show each of the two sums on the RHS above are of order
√m+1 √KT +T2β β+ +1 1 λ2β1 +1 K2ββ +1 .
· ·
(cid:16) (cid:17)
Note that, going forward in the proof, we’ll constrain our attention to significant phases [τ ,τ ) such that
i i+1
τ =T +1 as the √KT term on the above RHS accounts for this regret contributed by this final phase.
i+1
6
18Bounding Regret over Long Phases. We first bound the regret over long significant phases [τ ,τ )
i i+1
such that τ τ > (m+1) K. By the pigeonhole principle, there must exist an arm a [K] which is
i+1 i
− · ∈
optimal(i.e.,a argmax µ (a))foratleast(m+1)differentroundsin[τ ,τ ). Fixsuchanarma. By
∈ a∈[K] t i i+1
Definition 1, there exists a (β,λ)-Hölder interpolating function F : [0,1] [0,1] such that F(t/T) = δ (a).
t
→
By the definition of a, there exists at least (m+1) different rounds t [τ ,τ ) such that F(t/T)=0. By
i i+1
∈
Rolle’s Theorem, this means there exists a point x [τ /T,τ /T) such that F(m)(x )=0.
0 i i+1 0
∈
Then, by the definition of a (β,λ)-Hölder function (Definition 1), note that
β−m
τ τ τ τ
x i , i+1 : F(m)(x) = F(m)(x) F(m)(x ) λ i+1 − i , (10)
0
∀ ∈ T T | | | − |≤ · T
h i (cid:18) (cid:19)
First, suppose m = 0. Then, we have by Fact 16 that F(x) = δ (a) K for some x
x·T ≥ τi+1−τi+1 ∈
[τ i/T,τ i+1/T]. Combining this with our above display, there exists x [τ i/T,τ i+q1/T] such that for m=0:
∈
β−m
K τ τ +1
F(x)= F(m)(x) λ i+1 − i .
sτ
i+1
−τ i+1 ≤ | |≤ ·
(cid:18)
T
(cid:19)
Wewillnextargue,usingaTaylorapproximation,thattheaboveinequalitiesalsoessentiallyholdform 1.
≥
If m 1, then by the Mean Value Theorem and (10):
≥
τ τ
x i , i+1 : F(m−1)(x) sup F(m)(x′) sup y z
∀ ∈ T T | |≤ x′ | | y,z∈[τi,τi+1]| − |
h i T T
β−m
τ τ τ τ
i+1 i i+1 i
λ − − .
≤ · T T
(cid:18) (cid:19) (cid:18) (cid:19)
Then, by induction and repeatedly applying the Mean Value Theorem, we have:
τ τ β−k
k 0,...,m 1 : F(k)(x) λ i+1 − i . (11)
∀ ∈{ − } | |≤ · T
(cid:18) (cid:19)
Then,takinganorder-(m 1)TaylorexpansionwithLagrangeremainderofF aboutarootx [τ /T,τ /T)
1 i i+1
− ∈
(which we already argued exists since a is optimal at some round in [τ ,τ )), we have there exists
i i+1
ξ [τ /T,τ /T) such that:
i i+1
∈
τ τ
i i+1
x , : F(x) = F(x) F(x ) (F(x )=0)
1 1
∀ ∈ T T | | | − |
(cid:20) (cid:19)
m−1 F(k)(x ) F(m)(ξ)
| 1 | x x k+ | | x x m (Taylor’s Theorem)
1 1
≤ k! ·| − | m! ·| − |
k=1
X
β
τ τ
i+1 i
(e 1) λ − (from (11)).
≤ − · · T
(cid:18) (cid:19)
Now, as before, there must exist an x [τ /T,τ /T) such that (using the above display):
i i+1
∈
K τ τ +1 β
i+1 i
F(x) (e 1) λ − .
sτ
i+1
−τ i+1 ≤ ≤ − · ·
(cid:18)
T
(cid:19)
Rearranging,the above implies
τ i+1 τ i+1 (e 1)− 2β2 +1 T2β2 +β 1 λ− 2β2 +1 K2β1 +1.
− ≥ − · · ·
Now, letting M be the total number of long significant phases, we must have
2T T +L˜ τ i+1 τ i+1 M (e 1)− 2β2 +1 T2β2 +β 1 λ− 2β2 +1 K2β1 +1.
≥ ≥ − ≥ · − · · ·
i<L˜:τi+1−Xτi>(m+1)·K
19Thus,
2T
2
M (e 1)2β+1 .
≤ − T2β2 +β
1
λ− 2β2
+1
K2β1
+1
· ·
Then, we have by Jensen’s inequality:
K (τ τ ) √K T M
i+1 i
· − ≤ · ·
i<L˜:τi+1−Xτi>(m+1)·Kp
2T
1
(e 1)2β+1 K T
≤ − ·s · · (cid:18)T2β2 +β
1
λ− 2β2
+1
K2β1
+1(cid:19)
· ·
=√2(e 1)2β1 +1 T2β β+ +1 1 λ2β1 +1 K2ββ +1.
− · · ·
Bounding Regret over Short Phases. We next analyze the short significant phases [τ ,τ ) where
i i+1
τ τ (m+1) K. The difficulty here is that we cannot directly apply the same argument as we did
i+1 i
− ≤ ·
for long phases since there may not exist m+1 different rounds where an arm is optimal within the phase.
To get around this, we’ll concatenate different short phases together and construct pseudo phases where we
can apply the argument as we made for long phases.
Definition 17. Let n be the smallest significant shift τ belonging to a short significant phase [τ ,τ ).
0 i i i+1
Then, recursively definen to bethesmallest significant shift τ >n corresponding to ashort significant
j+1 i+1 j
phase [τ ,τ ) such that
i i+1
[n ,τ ) [τ ,τ ),
j i+1 i i+1
⊆
i<L˜:τi+1−[τi≤(m+1)·K
andsuchthat τ n (m+1) K. Ifnosuch significant shift τ exists, let n bethelargest significant
i+1 j i+1 j+1
− ≥ ·
shift τ such that
i+1
[n ,τ ) [τ ,τ ).
j i+1 i i+1
⊆
i<L˜:τi+1−[τi≤(m+1)·K
We call [n ,n ) a pseudo phase. The sequence n ,n ,... induces a partition of short phases:
j j+1 0 1
[τ ,τ )= [n ,n ).
i i+1 j j+1
i<L˜:τi+1−Gτi≤(m+1)·K Gj
Call a pseudo phase [n ,n ) filled if n n (m+1) K, and unfilled otherwise.
j j+1 j+1 j
− ≥ ·
Intuitively, a filled pseudo phase is sufficiently long and will be of similar length to a long phase.
We’ll now further decompose the dynamic regret over short phases using Jensen’s inequality and the fact
that each pseudo phase [n ,n ) can contain at most (m+1) short phases [τ ,τ ) since all phases are
j j+1 i i+1
length at least K (Fact 15):
K (τ τ ) K (n n ) (m+1).
i+1 i j+1 j
· − ≤ · − ·
i<L˜:τi+1−Xτi≤(m+1)·Kp Xj q
Next,wefurtherdecomposetheaboveRHSsumoverpseudophasesintosumsoverfilledandunfilledpseudo
phases. Thus, using Jensen again, it suffices to bound
(m+1) K T J + (m+1) K T J , (12)
1 2
· · · · · ·
p p
where J and J are respectively the number of filled and unfilled pseudo phases. We’ll first bound J .
1 2 1
20Bounding the Number of Filled Pseudo Phases. Thiswillproceedsimilarlytotheargumentforlong
phases. Fix a filled pseudo phase [n ,n ) which has n n (m+1) K. Then, by the pigeonhole
j j+1 j+1 j
− ≥ ·
principle, there must existan arma [K] whichis optimal in atleast (m+1) different rounds in [n ,n ).
j j+1
∈
Then, using the same arguments as before except replacing the long phase [τ ,τ ) with the pseudo phase
i i+1
[n ,n ), we conclude that there exists x [n /T,n /T) for which:
j j+1 j j+1
∈
β
K n n +1
j+1 j
δ (a) (e 1) λ − .
x·T
sn
j+1
−n
j
+1 ≤ ≤ − · ·
(cid:18)
T
(cid:19)
Rearranging,we get
n j+1 n j +1 (e 1)− 2β2 +1 T2β2 +β 1 λ− 2β2 +1 K2β1 +1.
− ≥ − · · ·
Then, via similar arguments to before, the number of filled pseudo phases is at most
2T
2
J
1
≤(e −1)2β+1
T2β2β
+1
λ− 2β2
+1
K2β1
+1.
· ·
Plugging this into (12) gives the desired regret bound for √KTJ .
1
Bounding the Number of Unfilled Pseudo Phases. Since unfilled pseudo phases are not of suffi-
cient length n n < (m+1) K, further care is required to make use of Rolle’s Theorem. The key
j+1 j
− ·
workaroundis thateachunfilled pseudophase canbe extended intoanintervaloflengthatleast(m+1) K
·
without overcounting rounds, essentially because the unfilled pseudo phases are well-separated in time by
Definition 17.
First, handling an edge case, suppose there are no long phases [τ ,τ ) with τ τ (m+1) K and no
i i+1 i+1 i
− ≥ ·
filled pseudo phases [n ,n ) with n n (m+1) K. By Definition 17, this means there is just one
j j+1 j+1 j
− ≥ ·
pseudo phase [n ,n ) which subsumes all the significant phases [τ ,τ ) with τ =T +1. Thus, in this
j j+1 i i+1 i+1
6
case, J =1 and we are done.
2
Now,suppose there are atleasttwounfilled pseudo phases. Then, by Definition 17,twoconsecutive unfilled
pseudo phases must be separated by at least one long phase [τ ,τ ). Let I ,I ,... be the unfilled pseudo
i i+1 1 2
phases ordered by start times. Then, each I
j
= [n j′,n j′+1) has a posterior long phase [τ i,τ i+1) such that
τ i =n j′+1 and τ i+1 n j′ τ i+1 τ i (m+1) K.
− ≥ − ≥ ·
Then, applying the same chain of reasoning as before to the interval [n j′,τ i+1), we conclude there exists an
arm a [K] and x [n j′/T,τ i+1/T) for which:
∈ ∈
K τ i+1 n j′ +1 β
δ (a) (e 1) λ − =
x·T
sτ i+1 −n j′ +1 ≤ ≤ − · · (cid:18) T (cid:19) ⇒
τ i+1 n j′ +1 (e 1)− 2β2 +1 T2β2 +β 1 λ− 2β2 +1 K2β1 +1.
− ≥ − · · ·
Now, for each unfilled pseudo phase I
j
= [n j′,n j′+1,), let I
j
be the extension to the posterior long phase
[n j′,τ i+1) per our previous discussion. Then, since the I
j
are mutually disjoint, we have the number of
unfilled pseudo phasesJ is atmostone greaterthanthe number ofextended intervalsI . Thus, via similar
2 j
arguments to before:
2T
2
J
2
≤1+(e −1)2β+1
T2β2β
+1
λ− 2β2
+1
K2β1
+1.
· ·
As before, plugging this into (12) give the desired regret bound for √KTJ .
2
C Proofs of Results about Gap-Dependent Regret (Section 5)
21C.1 Proof of Theorem 9
.
Fix ℓ [L] and let P = [s ,e ] denote the ℓ-th (stationary) phase. Fix also an arm a [K] and suppose
ℓ ℓ ℓ
∈ ∈
WLOG that arm a is the a-th armto be evicted (i.e., at round t ) in the sense of (4) of Definition 8. Let I
a ℓ
be the indices in [a] such that [t ,t 1] intersects phase P . Next, we have the regretcontribution to our
i−1 i ℓ
−
formula (5) of arm a in phase P is:
ℓ
δ (a) [t ,t 1] P δ (a)
ℓ 111 δ (a)>0 = | i−1 i − ∩ ℓ |· ℓ 111 δ (a)>0
ℓ ℓ
· { } · { }
t t
i X∈Iℓt∈[ti−1X,ti−1]∩Pℓ |S | i X∈Iℓ |S |
1
δ (a) 111 δ (a)>0 .
ℓ ℓ
≤ · { }
t
t∈Pℓ∩X[1,ta−1]|S |
.
Next, we apply (4) of Definition 8 for [s ,s ]=P [1,t 1] and δ (a)>0:
1 2 ℓ a ℓ
∩ −
δ (a)
1 C2log(T) t∈Pℓ∩[1,ta−1]|St |−1
=
C 22log(T)
.
ℓ ≤ 2 δ (a) −1 δ (a)
t∈Pℓ∩X[1,ta−1]|St
|
ℓ Pt∈Pℓ∩[1,ta−1]|St
|
ℓ
P
Plugging the above RHS bound into our earlier calculations, and then summing over arms a and phases ℓ
gives us the desired regret bound.
C.2 Proof of Theorem 11
.
As noted in Definition 8, in a safe environment we may WLOG take t =T +1 assume there’s a safe arm
. K
a♯ T . By convention, let = . Then, we have:
∈∩t=1St ST+1
∅
K ti−1 K ti−1
δ (a)
E [δ (a)]= 111 a t (t =T +1)
a∼Unif{St} t
{
∈Sti−1 \Sti} K
t
Xi=1t= Xti−1 Xi=1aX∈[K] Xt=1 |S |
K ti−1
log(T)
C 111 a (from (4))
≤
2
{
∈Sti−1 \Sti}v
Xi=1aX∈[K] u uXt=1 |St |
t
K ti−1
log(T)
C K (Jensen’s inequality)
≤
2v
u i=1 t=1 |St |
u XX
t
K ti
log(T)
C K (from )
≤
2v |Sti−1| St ⊇St+1
u
u
Xi=1 t= Xti−1 |Sti−1|
t
C KTlog(T).
2
≤
p
C.3 Proof of Theorem 12
First, similar to Proposition 3 of Suk and Kpotufe (2022), using Freedman’s inequality, we establish a
concentration error bound on our estimates δˆ(a′,a) (7).
t
Proposition 18. Let be the event that for all rounds s <s and all arms a,a′ [K]:
1 2
E ∈
s2 s2
log(T) log(T)
δˆ(a′,a) E δˆ(a′,a) 10(e 1) + max , (13)
t t t−1
(cid:12) (cid:12) (cid:12)t X=s1 − h |H i(cid:12) (cid:12) (cid:12)≤ − v u us X=s1 |As | s∈[s1,s2] |As | 
(cid:12) (cid:12) t 
(cid:12) T (cid:12) T δ2(a)
δ (π ) E[δ (π ) ] 10(e 1) log(T) t +log(T) . (14)
(cid:12) (cid:12)Xt=1 t t − t t |Ht−1 (cid:12) (cid:12)≤ − v u
u
Xt=1a X∈At |At | 
(cid:12) (cid:12) (cid:12) (cid:12) t 
(cid:12) (cid:12)
22where recall T is the filtration generated by π ,Y (π ) T . Then, occurs w.p. at least 1 1/T2.
{Ht }t=1 { t t t }t=1 E −
Proof. Both (13) and (14) follow from Freedman’s inequality (Beygelzimer et al., 2011, Theorem 1). (cid:4)
Now, note that since the active armset at round t is measurable:
t t−1
A H
δ (a′,a)
a′,a :E δˆ(a′,a) = t
t t t−1
∀ ∈A |H
t
h i |A |
δ (a)
E[δ (π ) ]= t .
t t t−1
|H
t
a X∈At |A |
.
Leta be the i-tharmtobeevictedfromtheactiveset. Lettˆ =1andlettˆ,...,tˆ be theorderedeviction
i 0 1 K
times ofarmsa ,...,a ,orelse lettˆ =T +1ifarma is neverevicted. Using(14)andAM-GMinequality,
1 K i i
we have:
T T δ (a) T δ2(a)
δ (π ) t +10(e 1) log(T) t +log(T)
t t ≤ − v 
Xt=1 Xt=1a X∈At |At | u
u
Xt=1a X∈At |At |
T t 
δ (a)
t
c log(T)+
2
≤ t !
Xt=1a X∈At |A |
K tˆ i−1
=c log(T)+ E [δ (a)] .
2  a∼Unif{At} t 
Xi=1t=Xtˆ i−1
 
This gives the desired regretbound so long as we can argue that tˆ are valid eviction times with safe
i i∈[K]
{ }
armsets .
t t∈[T]
{A }
First, we claim that, on event , the safe arm a♯ cannot be evicted from . Suppose a♯ is evicted from the
t
active set using the eviction cE riterion (Line 5) over subinterval [s ,s ]. A Then, since δˆ(a′,a) [ 1,1], we
1 2 t
∈ −
have:
s2 1 s2 δˆ (a′,a♯) s2 log(T) s2 1
max s C = C2log(T).
s X=s1 |As |
≥a′∈[K]
s X=s1 |As |
≥ 5 v
u us X=s1 |As |
⇒
s X=s1 |As |
≥ 5
In particular, this means for C >1 in Line 5: we wtill have
5
s2
log(T) log(T)
max .
v
u us X=s1 |As |
≥s∈[s1,s2]
|As |
t
Combining the above with (13), we see that:
s2 s2
log(T)
δ (a♯)>c ,
s 3
v
s X=s1 u us X=s1 |As |
t
which violates the definition of the safe arm (Definition 10) for C = 1 for C sufficiently large. Thus, we
3 5
conclude that a♯ T .
∈∩t=1At
We next decompose the (weighted) dynamic regret of any arm a over subinterval [s ,s ] via:
∈Atˆ i−1 1 2
s2 δ (a) s2 δ (a♯) s2 δ (a♯,a)
s s s
= + .
s s s
s X=s1 |A | s X=s1 |A | s X=s1 |A |
The firstboundisorder s2 log(T) viathe definitionofa♯ andthe secondsumis alsothe sameorderby
s=s1 |As|
our concentration estimaqte (13) and eviction criterion (Line 5). This establishes (4) for appropriately large
P
C (in terms of C ). Thus, tˆ are valid eviction times w.r.t. safe armsets T .
2 5 { i }i∈[K] {At }t=1
23C.4 Proof of Theorem 13
In a similar fashion to the proof of Theorem 3, we lower bound the regret iteratively by first designing the
hard environment for [t ,t ), then for [t ,t ), and so on. The following theorem serves as a base template
0 1 1 2
that we we can repeat on each period [t ,t ).
i i+1
Theorem 19. Fix a positive integer T, number of arms K [2,T] N and a real number ∆ [0, K/T].
∈ ∩ ∈
Let E′ be the class of all environments such that T E [δ (a)] R, and such that T is a valid
t=1 a∼Unif{[K]} t ≤ p
eviction time w.r.t. initial time 1 and threshold C =1 in (4). Then, for any algorithm π, we have:
2
P
1
sup R (π,T) R.
E∈E′ E ≥ 32 e25/12 ·
·
Proof. Asinthe proofofTheorem3,we’llfollowaLeCam’smethodstyleofargumentforshowingminimax
lower bounds in stationary bandits (e.g. Lattimore and Szepesvári, 2020, Theorem 15.2). We will refine the
argumentto show a more structured lower bound of order R over the class of problems with gap-dependent
rate at most R.
. .
Consider an environment where µ (1)= 1 and µ (a)= 1 R K for all arms a=1. Note that the
E1 t 2 t 2 − 4T · K−1 6
bound R √TK and T K ensures µ (a) [0,1]. (cid:16) (cid:17)
t
≤ ≥ ∈
One can verify this environment has the right gap-dependent rate and so belongs to the class ′.
E
T
R K
E [δ (a)]= · R.
a∼Unif{[K]} t 4(K 1) ≤
t=1 −
X
.
Now, by pigeonhole principle, there must exist an arm a = 1 for which the arm-pull count N (a) =
T
6
T 111 π = a satisfies E [N (a)] T . Consider an alternative environment whose mean rewards
t=1 { t } E0 T ≤ K.−1 . Ea
are identical to those of except µ (a)= 1 +∆. For ∆= R , this alternative environment also belongs to
tP
he class E′ since for K
E1
2:
t 2 8T
≥
T
R K K 1 ∆ T
E [δ (a)]= · +T ∆ − + · <R.
a∼Unif{[K]} t 4(K 1) · · K K
t=1 (cid:18) − (cid:19) (cid:18) (cid:19)
X
Next, weobservethe followingregretlowerbounds depending onwhether the totalarm-pullcountN (1)of
T
arm 1 is larger than T/2:
T R K
R (π,T) · P (N (1) T/2)
E1
≥ 2 · 4 T (K 1) ·
E1 T
≤
(cid:18) · · − (cid:19)
T
R (π,T) ∆ P (N (1)>T/2).
Ea
≥ 2 · ·
Ea T
By Bretagnolle-Huber inequality (Lemma 20), the above regret lower bounds give us:
R
R (π,T)+R (π,T) (P (N (1) T/2)+P (N (1)>T/2))
E1 Ea
≥ 16
E1 T
≤
Ea T
R
exp( KL( , )),
1 a
≥ 32 − E E
where we use KL( , ) to denote the KL divergence between the induced distributions on decisions and
1 a
E E
observations over T rounds in environments and .
1 a
E E
Next, we upper bound the KL between induced distributions which can be decomposed using chain rule
(Lattimore and Szepesvári, 2020,Lemma 15.1):
1 R K 1
KL( , )=E [N (a)] KL Ber ,Ber +∆ .
E1 Ea E1 T
· 2 − 4T · K 1 2
(cid:18) (cid:18) (cid:18) − (cid:19)(cid:19) (cid:18) (cid:19)(cid:19)
24By reverse Pinsker’s inequality for Bernoulli random variables (Sason and Verdú, 2016, Remark 33),
2
1 R K 1 4 R K
KL Ber ,Ber +∆ +∆ .
2 − 4T · K 1 2 ≤ 1 ∆ · 4T · K 1
(cid:18) (cid:18) (cid:18) − (cid:19)(cid:19) (cid:18) (cid:19)(cid:19) 2 − (cid:18) (cid:18) − (cid:19) (cid:19)
Now, since ∆ = R 1 K 1, we have the above RHS is upper bounded by 25 (R/T)2. Now, since R
8T ≤ 8 T ≤ 8 6 ·
is at most √TK and E q[N (a)] T , we have:
E1 T ≤ K−1
2
R 25T R R
R (π,T)+R (π,T) exp e−25/12
E1 Ea
≥ 32 · −6(K −1) · (cid:18)T
(cid:19)
!≥ 32 ·
It’s left to verify that round T is a valid eviction time for both environments and , or that (4) holds
1 a
E E
for [K]. Indeed, for , we have for any a [K]:
t 1
S ≡ E ∈
K 1
s2
δ (a)
s2
R K 1 s s +1
R √TK 2√TK − = t · 2 − 1 ,
≤ ≤ K ⇒ K ≤ 4T (K 1) · K ≤ K
(cid:18) (cid:19) s X=s1 s X=s1(cid:18) · − (cid:19) r
for any [s ,s ] [1,T]. A similar calculation applies for environment . (cid:4)
1 2 a
⊆ E
Now, equipped with this base lower bound, to prove Theorem 13, we concatenate the above construction
K 2times(eachtimeremovinganarmfromthearmset)toestablishalowerboundoforderRforanysetof
−
rounds t K byconcatenatingenvironmentsforK 2differentevictiontimes(wehavet =t =T+1
{ i }i=1 − K−1 K
since we need two arms to force a lower bound in the last segment).
First, note though that if t K are valid eviction times, then by summing (4) over arms a and periods
{ i }i=1
[s ,s ]=[t ,t 1]:
1 2 i−1 i
−
K−1 ti−1 K−1
E [δ (a)] (t t ) (K+1 i).
a∼Unif{St} t
≤
i+1
−
i
· −
Xi=1 t= Xti−1 Xi=1
p
Next, we claim we can find a partition of R = K−1R such that R (t t ) (K+1 i) for all
. i=1 i i ≤ i − i−1 · −
i [K 1]: specifically, let R =min (t 1) K,R and recursively define
∈ − 1 { 1 − P· } p
p i−1
.
R =min (t t ) (K+1 i),R R .
i  i − i−1 · − − j
p Xj=1 
By virtue of R
≤
K i=− 11 (t
i
−t i−1) ·(K+1 −i), we claim there must exist an index i for which R
i
=
R i−1R , in which case all subsequent R ,...,R are zero and K−1R = R. If such an index
− j=1 j P p i+1 K−1 i=1 i
did notexist, then we’dhave K−1 (t t ) (K+1 i)<R by consideringindex i=K 1, whichis
P i=1 i − i−1 · − P −
a contradiction.
P p
Next, note that Theorem 19 can be applied over each period [t ,t 1] with a different armset of size
i−1 i
−
Sti
K+1 i to obtaina lowerbound oforderR . Letting =[K],eachsubsequentarmset will be defined
−
i S1 Sti
to randomly exclude an arm in .
Sti−1
We next claim that our concatenated environments lie in the class . First, note that by design:
E
K−1 ti−1 K−1 K−1
E [δ (a)] R =R (t t ) (K+1 i).
a∼Unif{St} t
≤
i
≤
i
−
i−1
· −
Xi=1 t= Xti−1 Xi=1 Xi=1
p
Next, we claim that t K are valid eviction times. This follows in a similar fashion to the proof of
{ i }i=1
Theorem 19. Consider a generic subinterval [s ,s ] [1,t 1] and break it up according to the periods
1. 2
⊆
i
−
[t ,t ] which intersect it. Now, let [s ,s ] = [s ,s ] [t ,t 1]. Then, by Jensen’s inequality, we
j−1 j 1,j 2,j 1 2 j−1 j
∩ −
have since j K 2.
≤ −
1 K+1 j (s s ) s s
− 2,j − 1,j (K 2)1/2 2,j − 1,j .
j:[tj−1,tj−X1]∩[s1,s2]6=∅
2s t
j
−t
j−1
!· K+1 −j ≤ − · sj:[tj−1,tj−X1]∩[s1,s2]6=∅K+1 −j
25C.5 Proof of Theorem 14
.
We first show (i). Let a argmin f (1/T) be an optimal arm at round 1. Let m = β . Then, by
∈ a∈[K] a ⌊ ⌋
Taylor’s Theorem with Lagrange remainder, we have for all x [0,1], there exists ξ [0,1] such that:
∈ ∈
f (x)=f (x) f (1/T)
a a a
−
m−1 (k) (m)
f (x) f (ξ)
| a | x 1/T k+ | a | x 1/T m
≤ k! ·| − | m! ·| − |
k=1
X
m x 1/T k
λ | − |
max
≤ k!
k=1
X
K
(ex−1/T 1) .
≤ − · T
r
Now, this means arm a must be safe in the sense of (6) for suitable constant C since the gap at any round
3
cannot exceed K/T. This shows (i).
Next, we showp(ii). Fix a real number λ> K/T consider the class E of environments with
sup p sup f(n)(x) λ.
| a |≤
a∈[K]x∈[0,1]
Now, one can verify that, for (β′,λ′) = (n,λ), the constructed bump function reward environments in the
proofof Theorem3 for Hölder class Σ(β′,λ′) satisfy the aboveinequality andhence lie in the classE. Thus,
for any algorithm π, the minimax regret over class E is
supR E(π,T) Ω(T2n n+ +1 1 K2nn +1 λ2n1 +1) Ω(√KT),
E∈E ≥ · · ≥
where the last inequality follows from λ> K/T.
p
D Auxilliary Lemmas
Lemma 20 (Bretagnolle-Huber Inequality; Theorem 14.2 of Lattimore and Szepesvári (2020)). Let P and
Q be probability measures on the same measurable space (Ω, ), and let A be an arbitrary event. Then,
F ∈F
1
P(A)+Q(Ac) exp( KL(P,Q)),
≥ 2 −
where Ac =Ω A is the complement of A.
\
26