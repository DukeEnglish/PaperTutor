PublishedasaconferencepaperatCOLM2024.
Transformer Circuit Faithfulness Metrics Are Not Robust
JosephMiller∗ BilalChughtai WilliamSaunders
FARAI Independent Independent
Abstract
Mechanisticinterpretabilityworkattemptstoreverseengineerthelearned
algorithmspresentinsideneuralnetworks. Onefocusofthisworkhasbeen
todiscover‘circuits’–subgraphsofthefullmodelthatexplainbehaviour
onspecifictasks. Buthowdowemeasuretheperformanceofsuchcircuits?
Prior work has attempted to measure circuit ‘faithfulness’ – the degree
towhichthecircuitreplicatestheperformanceofthefullmodel. Inthis
work,wesurveymanyconsiderationsfordesigningexperimentsthatmea-
surecircuitfaithfulnessbyablatingportionsofthemodel’scomputation.
Concerningly,wefindexistingmethodsarehighlysensitivetoseemingly
insignificantchangesintheablationmethodology. Weconcludethatex-
istingcircuitfaithfulnessscoresreflectboththemethodologicalchoicesof
researchersaswellastheactualcomponentsofthecircuit-thetaskacircuit
isrequiredtoperformdependsontheablationusedtotestit. Theultimate
goalofmechanisticinterpretabilityworkistounderstandneuralnetworks,
soweemphasizetheneedformoreclarityinthepreciseclaimsbeingmade
about circuits. We open source a library at this https URL that includes
highlyefficientimplementationsofawiderangeofablationmethodologies
andcircuitdiscoveryalgorithms.
1 Introduction
Mechanistic interpretability (MI) is a form of post-hoc interpretability that attempts to
reverse engineer neural networks to provide faithful low-level explanations of model
behaviour(Olahetal.,2020). Onefocusofinterpretabilityworkontransformerlanguage
modelsisidentifying‘circuits’–subgraphsoftheentiremodel’scomputationalgraphthat
areprimarilyresponsibleforthemodel’soutputonsometask(Wangetal.,2023);where
a task is specific type of problem that a language model has to solve to output correct
next-tokenpredictions(ie. sentencesthatrequireaspecificalgorithmtocompletecorrectly).
Akeymetricusedbymechanisticinterpretability(MI)researcherstoquantifythequalityof
a‘circuit’forsometaskisit’sfaithfulness–thatis,thedegreetowhichthecircuitcaptures
the performance of the entire model (Zhang & Nanda, 2024). In this work, we study
varioussmallandreasonableseemingvariationsonmethodologiesformeasuringcircuit
faithfulness and find that such variations often lead to significantly different faithfulness
scores. Faithfulness is typically measured by performing a targeted, circuit-dependent
ablationtothemodel,andobservingtheeffectofthisonsomemetricofthemodel’soutput.
InthecontextofMI,anablationreferstoatypeofinterventionmadeontheactivations
ofamodelduringitsforwardpasswiththeintendedpurposeof‘deleting’somecausal
pathway(s),therebyisolatingthecausaleffectofthecircuit.
Inthiswork,weseektoanswerthequestions: Whatdocircuitfaithfulnessmetricsactually
show? To what extent are they a useful test of the circuit and to what extent are they a
reflectionoftheexperimentalmethodology?
WebeginbyreviewingthewaysinwhichMIresearchersmayvarytheirablationmethodol-
ogy(Section3),providingadetailedreviewofmethodsforablatingtransformercircuits.
Next,wetestthesevariationsonexistingcircuitsdiscoveredbyMIresearchers(Section4).
Weprovidedetailedcasestudiesofthe‘IndirectObjectIdentification’circuitbyWangetal.
∗Correspondencetojosephmiller101@gmail.com
1
4202
luJ
11
]GL.sc[
1v43780.7042:viXraPublishedasaconferencepaperatCOLM2024.
(2023),the‘Docstring’circuitbyHeimersheim&Janiak(2023)andthe‘SportsPlayers’circuit
byNandaetal.(2023b). Wethengoontostudy‘optimalcircuits’(Section5)inthecontext
ofautomatedcircuitdiscovery(Conmyetal.,2023)–anemergingparadigmthataimsto
discovercircuitsalgorithmically,withouthumaninput.
We conclude with recommendations for MI researchers (Section 6). We additionally re-
leaseAutoCircuit,alibrarycontainingefficientimplementationsofthecircuit-discovery
andcircuit-evaluationtechniquesusedinthispaper,thatissignificantlyfasterthanprior
implementationswetested(seeAppendixAformoredetails).
2 RelatedWork
Circuit Analysis. Circuit analysis is a form of post-hoc interpretability focused on un-
derstandingthefullend-to-endlearnedalgorithmresponsibleforsomespecifiednarrow
behaviour. Acircuitisasubgraphofthefullcomputationalgraphofthemodelthat(is
allegedto)implementsomeprecisebehavior. Circuitshavebeenstudiedinvisionmodels
(Cammarataetal.,2021;Olahetal.,2020)andintoytransformermodels(Nandaetal.,2023a;
Chughtaietal.,2023). Morerecently,thecircuitanalysisparadigmhasachievedsuccessin
interpretingtransformerlanguagemodelstoo,withanumberofpapersdiscoveringcircuits
implementing human understandable algorithms through ablation studies (Wang et al.,
2023;Heimersheim&Janiak,2023;Hannaetal.,2023). Toacceleratesuchstudies,recent
workhasattemptedtoautomatetheprocessofdiscoveringcircuits(Conmyetal.,2023;
Syed&Rager,2023;Kramaretal.,2024),particularlyinlargelanguagemodels,ascircuits
havehistoricallyrequiredalargeamountofresearcher-efforttouncover. Priorworkhas
suggestedthatidealcircuitsexistontheParetofrontieroffaithfulness,completenessand
simplicity(descriptionlength),astheentirenetworkistriviallyoptimalforthefirsttwo
criteria(Sharkey,2024).
Activation Patching. Zhang & Nanda (2024) recommend best practices in Activation
Patching(aformofablation,definedinSection3.1)formeasuringcircuitfaithfulnessina
similarworktoours. Theycomparesinglelayervs. multi-layerablation,ResampleAblation
vs. NoiseAblationandlogitdifferencevsprobabilitymetricswhenNodePatching. We
studyalargersetofvariationsinablationmethodologyinthiswork,enumeratingseveral
morechoicesinmethodologyandarguingthatdifferentoptimalcircuitsaredefinedinpart
bydifferentablationmethodologies,ratherthanprescribingasinglecorrectapproachto
ablation.
FaithfulexplanationsinNLP.Weareinterestedinexplainingmodelbehaviorinaway
that reflects the underlying reasoning process of the model, a criteria often referred to
as faithfulness. In this work we measure faithfulness by studying the fidelity of ablated
models-thesimilarityoftheablatedoutputtotheoutputsofthefullmodel(Alishahietal.,
2019;Guidottietal.,2018;Agarwaletal.,2024). AsarguedbyJacovi&Goldberg(2020),
faithfulnessshouldbeviewedasacontinuum. Anyinterpretationisanapproximationthat
willnecessarilyfailtocapturesomeaspectsoftheunderlyingbehavior.
Mechanisticinterpretability(MI)attemptstoreverseengineertrainedmachinelearning
modelstoproducefaithfulhumanunderstandableexplanationsofmodelpredictionsvia
analysis of the low level features and algorithms implemented by the network. Circuit
analysisisjustoneimportantdirectioninthisthemeofwork. Besidescircuitanalysis,MI
morebroadlyseekstounderstandthecorrectframetointerpretneuralnetworkcomputation
(Elhageetal.,2021;Brickenetal.,2023;Cunninghametal.,2023)andtounderstandthe
learnedfeaturesofmodels(Lietal.,2023;Tiggesetal.,2023;Gurnee&Tegmark,2024;Bills
etal.,2023). MIhasalsoinspiredworkinsteeringmodeloutputsthroughrepresentation
engineering(Turneretal.,2023;Lietal.,2024;Rimskyetal.,2024).
3 MeasuringFaithfulness
Wefollowpreviousworks(Wangetal.,2023;Heimersheim&Janiak,2023;Hannaetal.,
2023)indefiningfaithfulnessofcircuitsastheextenttowhichtheyencapsulatethefull
2PublishedasaconferencepaperatCOLM2024.
Choice Granularity Component Value Tokenpositions Direction Set
Examples Heads,MLPs Node Resample/Patch Alltokens AblateClean Circuit
Q,K,V,MLPs Edge Zero Specifictokens RestoreClean Complement
Heads,MLPNeurons Branch Mean
Sparsefeatures Noise
Table1: Thesix-tuplethatdefinesablationmethodologyfortransformercircuits.
Work Granularity Component Value Tokenpositions Direction Set
Vigetal.(2020)
(GenderBias) Heads,Neurons Node Resample(clean) Alltokens ResampleClean Circuit
Mengetal.(2022)
(ROME) Layers Node Resample(clean) Specifictokens ResampleClean Circuit
Wangetal.(2023) Node(evaluation)/
(IOI) Heads Path(discovery) Mean Specifictokens AblateClean Complement
Conmyetal.(2023)
(ACDC) Heads,MLPs Edge Resample(corrupt) Alltokens AblateClean Complement
Heimersheim&Janiak(2023) Alltokens(evaluation)/
(Docstring) Heads Node Resample(clean) Specifictokens(discovery) ResampleClean Circuit
Hannaetal.(2023)
(Greaterthan) Heads,MLPs Path Resample(corrupt) Alltokens AblateClean Complement
Nandaetal.(2023b)
(SportsPlayers) Heads,MLPs Path Resample(corrupt) Specifictokens AblateClean Complement
Table2: Summaryofthepatchingmethodologiesusedbysevenpreviousworks. Notethat
eachmethodologydiffersfromalloftheothersinatleastoneaspect.
model’scomputationofaparticulartask. Theseworksmeasurefaithfulnessbyablatingthe
componentsofthecomputationalgraphthatarenotinthecircuitandobservingthechange
inoutputofthemodel.
However,evenwithinthisframework,thereareseveralimportantfurtherchoiceswhen
designingexperiments,whichwereviewinthissectionandsummariseinTable1. Wealso
provideasummaryoftheapproachestakenbypreviousworksinTable2.
3.1 AblationMethodology
InthecontextofMI,anablationreferstoatypeofinterventionmadeontheactivations
ofamodelduringitsforwardpasswiththeintendedpurposeof‘deleting’precisecausal
pathways. In the language of casual inference, we denote the ablation of all activations
outsideacircuitConamodel Mas:
F(x) = M(x |do(a = a˜)), a ∈/ C (1)
Wherexistheinputtothemodel,aisaninternalactivationofthemodelanda˜istheablated
valueofa. Theablationmethodologydeterminesthetypesofactivationsandvaluesthata
anda˜canbe(eg. whetheraisaneuronnodeactivationoranedgebetweenattentionheads).
Intuitively, deleting important subcomponents for some task should damage task per-
formance, and conversely deleting unimportant sub-components should preserve task
performance. Assuch,ablationshavearisenasacommonlyusedtoolforlocalizingmodel
behaviourtospecificinternalmodelcomponents. Ablationsmaybeusedbothtofindand
evaluatemechanisticexplanationsofmodelbehavior.
The concept of ablation overlaps with a related technique, activation patching, in which
activations are modified during a model’s forward pass to some cached values from a
differentinput. ‘Corrupted’inputsareinputswhicharesimilartothe‘clean’distribution
beingstudied,butwhichhavecrucialdifferencesthatdrasticallychangetheoutput. For
example,atypical‘corrupt’promptcouldretainthestructureofa‘clean’prompt,while
switchingapropernoun,suchthatthecorrectnexttokenpredictionischanged. Inthis
workweconsideractivationpatchingtobeaspecifictypeofablation,andusetheterm
ResampleAblationinterchangeably. Butwenotethatingeneral,‘patching’meansediting
activationstosomeothervalue,insteadof‘deleting’them,asablationtypicallyconnotes.
3PublishedasaconferencepaperatCOLM2024.
Input
Attn 0
Input
Attn 0
Attn 1 MLP Attn 0 Input Attn 0
Input MLP Output Input MLP Output
Input +  +  Output
Attn 1 Input Attn 1
Attn 1
Input
(a)NodePatching(oftencalled
ActivationPatching)replaces (b)EdgePatchingreplacesthe
theoutputofsomecomponent activationsofasingleedgein (c)The‘treeified’formulation
to the residual stream in the thefactorizedviewofatrans- ofatransformerseparatesev-
unfactorizedtransformer. former. erypathfrominputtooutput.
Figure1: Thefactorizedand‘treeified’formulationsoftransformerssuggestmorespecific
ablationsthanablatingwholenodes.
Intheremainderofthissection,wereviewtherangeofablationtechniquesthatexistin
theliterature,specificallyastheyrelatetoevaluatingcircuits. Thereexistseveralimportant
experimentaldesignchoiceswhenevaluatingtransformercircuitsviaablations. Theseare
(1)thegranularityofthecomputationalgraphusedtorepresentthemodel,(2)whattypeof
componentinthegraphisablated,(3)whattypeofactivationvalueisusedtoablatethe
component,(4)whichtokenpositionsareablated,(5)theablationdirection(whetherthe
ablationdestroysorrestoresthesignal)and(6)thesetofcomponentsablated(thecircuit
orthecomplementofthecircuit). Acircuit-basedablationmethodologycanthereforebe
specifiedasasix-tuple, andpriorworkhasusedmanydifferentcombinations(Table2).
Inthispaperwearguethatexistingevaluationsofcircuitsaresensitivetoeachofthese
variables.
3.1.1 CircuitGranularity
InthisworkwestudycircuitsspecifiedatthelevelofattentionheadsandMLPs1. Wealso
separatetheinputofeachattentionheadintotheQ,KandVinputs,butweomitthisfrom
ourdiagramsforvisualsimplicity. Thisisthemostcommongranularityformechanistic
circuitanalysis(Conmyetal.,2023;Wangetal.,2023;Heimersheim&Janiak,2023;Hanna
etal.,2023;Nandaetal.,2023b)),butpreviousworkshavealsostudiedcircuitsspecified
attheleveloflayers(Mengetal.,2022),neurons(Vigetal.,2020),subspaces(Geigeretal.,
2023)andsparse”features”(Marksetal.,2024).
3.1.2 AblationComponentType(andAssociatedModelViews)
Transformers can be described as computational graphs in several different, equivalent
ways. Wecanchoosetowritethegraphasaresidualnetwork(Figure7a)ora‘factorized’
networkinwhichallnodesareconnectedviaanedgetoallpriornodes(Figure7b)(Elhage
etal.,2021). Orwecanwritedowna‘treeified’networkthatseparatesallpathsfrominput
tooutput(Figure8a). Allformulationsareequivalentbutthe‘factorized’viewallowsusto
isolateinteractionsbetweenindividualcomponentsandthe‘treeified’viewallowsusto
isolatechainsofinteractionsfrominputtooutput.
Thecomponenttypedefinesthetypeofinterventionmade: wedetailthreepossibilities,
withincreasinggranularity. Themoregranularapproachesaregenerallymoredifficultto
implementandmorecomputationallyexpensive.
(1)Nodes. Wemayinterveneonanode(inthestandard,residualview)duringtheforward
pass,replacingitsactivationwithsomeothervalue(Figure1a). Thisistheleastspecific
formofablation. Sincealldownstreamnodes‘see’thechangetherearealargenumberof
causalpathwaysaffectedbytheablation,whichmayresultinunintendedside-effects. This
1SeeThickstun(2024)forabriefoverviewofthetransformerarchitecture.
4PublishedasaconferencepaperatCOLM2024.
Attn 0
Attn 0
Input MLP Output
Input MLP Output
Attn 1
Attn 1
(Table4,Row3)EdgePatchingalltheedges
(Table4,Row2)EdgePatchingalltheedges not in a circuit with corrupt activations en-
inacircuitwithcleanactivationsallowsin- suresthatinformationfromthecleaninput
formationfromthecleaninputtoflowalong onlyflowsthroughedgesincludedinthecir-
pathsnotincludedinthecircuit. cuit.
Figure2: Twoapproachestotestingacircuitthatbothmeasurefaithfulnessasthesimilarity
oftheoutputtothefullmodel.
typeofablationisalsoknownas(vanilla)activationpatching(Vigetal.,2020)whenwe
ablatewithacachedactivationfromanotherinput.
(2)Edges. Usingthefactorizedviewofatransformer,wemayinterveneonanedgebetween
twocomponents(Figure1b). Thisismorespecificthanablatingnodes,asonlythespecified
destinationnodereceivestheablatedactivationofthesourcenode,soasmallernumberof
causalpathwaysareaffected.
(3)Branches. Theprevioustwoablationscanbeappliedtoindividualnodesoredges,orto
acollectionofnodesandedges. Branchablationsontheotherhandcanonlybeapplied
to paths from input to output (Figure 1c). The causal effect of individual paths through
themodelisisolatedby‘treeifying’thefactorizedmodel. Thisapproachwasintroduced
byChanetal.(2022)(formalizedbyGoldowsky-Dilletal.(2023))andisakeycomponent
ofarigorouscircuitevaluationapproachknownasCausalScrubbing. However,because
thenumberofpathsinthetreeifiedmodelisexponentialinthenumberoflayersofthe
modelthisapproachtocircuitevaluationisoftenintractableinpractice. Weomittreeified
experimentsinthiswork.
3.1.3 AblationValue
Whenperformingacausalinterventiononsomeactivation,wemaychoosewhatvaluewe
patchin. ThesimplestchoiceistoZeroAblate,byreplacingtheactivationwithavectorof
zeros(Olssonetal.,2022;Cammarataetal.,2021). Priorworkhasnotedhoweverthatthe
zeropointisarbitrary(Wangetal.,2023). ThenextsimplestistoapplyGaussianNoise
(GN)tothetokenembeddingsofthecleaninputtoobtaincorruptedactivations(Meng
etal.,2022). Bothoftheseapproachescantakethemodelsignificantlyoutofdistribution
(Zhang&Nanda,2024),producingnoisyoutputs(Wangetal.,2023).
TwomoreprincipledapproachesareResampleAblation(takeanactivationfromsome
othercorruptedinput)(Vigetal.,2020;Mengetal.,2022),andMeanAblation(replacewith
themeanactivationofanodefromsomedistribution)(Wangetal.,2023). Thesetwoablation
typeshavethedesirablepropertyofkeepingthemodelclosertoitsusualdistributionof
activations. Importantly,theydonotdeleteallinformationpresentinacomponent. Instead,
theydeleteinformationthatvariesacrossthedistribution, whilepreservinginformation
thatisconstantacrossit,allowingustoisolatepreciselanguagetasks,whileignoring,say,
genericgrammarprocessing. WhenMeanAblating,wehaveanadditionalchoiceinthesize
ofthemeanablationdataset(seeSection4.1). WefocusonMeanandResampleAblationsin
thiswork.
5PublishedasaconferencepaperatCOLM2024.
3.1.4 TokenPositions
Circuitsinautoregressivetransformersonanarrowdistributionaresometimesdefinedin
termsofcomponentsandtokenpositions. Whenthesetokenpositionsarespecified,wecan
choosetoeitherablatealltokenpositions,oronlythetokenpositionsnotinthespecified
set(Wangetal.,2023). Wecanmodifyequation(1)to
F(x) = M(x |do(a = a˜ )), a ∈/ C
i i i
wherea istheactivationaattokenpositioni.
i
3.1.5 AblationDirectionandTestingCircuits
Ablationtypicallyreferstoinstanceswherewerunthemodelonacleaninputandchange
activationstodestroytheinputsignal(Wangetal.,2023;Conmyetal.,2023;Hannaetal.,
2023;Nandaetal.,2023b). However, wecanalsorunthemodelonacorruptinputand
ResampleAblate(orPatch)inactivationsfromthecleaninput(Mengetal.,2022;Heimer-
sheim&Janiak,2023). Separately,whenevaluatingcircuits,wecanchoosetoeitherablate
allthecomponentsofthecircuitorwecanablateallthecomponentsnotinthecircuit(the
complement).
Thecombinationofthesechoicesdeterminesthetargetofourfaithfulnessmetric:
ModelInput Direction Set FaithfulnessTarget
Clean AblateClean Circuit DestroyPerformance
Corrupt RestoreClean Circuit RestorePerformance
Clean AblateClean Complement MaintainPerformance
Corrupt RestoreClean Complement MaintainInefficacy
Table3: Thefourmethodologiesfordirectionalpatchingforcircuitevaluation.
Figure2comparesthesecondandthirdrowsofthetable,whichbothmeasurefaithfulness
asthesimilarityoftheablationtothefullmodel. WenotethatResampleAblatingclean
activationsforthecircuitcomponentswhilepassingacorruptinputallowsthesignalfrom
thecleaninputtoflowthroughedgesnotincludedinthecircuit. Whereasablatingwith
corruptactivationsonthecomplementofthecircuitwithacleaninputensuresthatthe
signalfromtheinputonlyflowsthroughthecircuit.
3.2 Metric
Onefurtherconsiderationinadditiontotheablationmethodologyisthemetricusedto
evaluate the effect of the ablation. We also argue that the choice of metric is important.
Therearemanychoicesusedintheliterature,includingKLDivergence(Conmyetal.,2023),
top-kaccuracyHeimersheim&Janiak(2023)andtask-specificbenchmarks(Hannaetal.,
2023). Inthisworkwewillfocusonthemetricsusedbytherespectiveauthorsofthecircuits
thatwestudy,butnotethesechoicesarealsoingeneralfree.
4 FaithfulnessMetricsareSensitivetoAblationMethodology
Inthissection,weempiricallydemonstratethatevaluationsofagivencircuit’sfaithfulness
arehighlysensitivetotheexperimentalchoicesoutlinedinSection3madeatevaluation
time. Wefurtherarguethatthissensitivityisimportant,andmayresultinpractitioners
findingfundamentallydifferentalgorithms.
WeprovideacasestudyhereontheIndirectObjectIdentification(IOI)circuitidentified
byWangetal.(2023),asthisisthemoststudiedlanguagemodelcircuitintheliterature
(Conmyetal.,2023;Makelovetal.,2023;Zhang&Nanda,2024),butfindsimilarresults
6PublishedasaconferencepaperatCOLM2024.
forotherknownlanguagemodelcircuitsinAppendixD.TheIOIcircuitisspecifiedasan
edge-levelcircuit,butWangetal.(2023)evaluateitsfaithfulnessviaanode-wiseablation
methodology. Webeginbytestingthecircuitusingedge-levelablation.
TheIOIcircuit. TheIOIcircuitisamanually-identifiedsubgraphofGPT-2thatisintended
toperformtheIOItask,whichisdefinedbytheIOIdistribution. TheIOIcleandistribution
consistsof15sentencetemplateswhichinvolvetwopeopleinteracting,structuredsuchthat
thenextwordtobepredictedistheindirectobjectA.Eachtemplatecanbefilledwithnames
intheorderABBAorBABA,wherethefinalAisthepredictedtoken. Forexample: "When John
and Mary went to the store, John bought flowers for ". Thecorruptdistribution
(also called the ABC distribution) fills the same templates with names in the order ABC
whereA,BandCarethreedifferentnamessampledindependentlyofthecorresponding
cleanprompt(weonlyneedtospecifythreenamesbecausewearenotdefiningacorrect
completion,unlikewithABBAandBABA).Forexample: "When Gary and Nora went to the
store, Naomi bought flowers for ".
MeasuringIOICircuitFaithfulness. Wangetal.(2023)definethemetricofcircuitfaithful-
nesstobelogitdifferencerecovered2. Thelogitdifferenceiscomputedbetweenthecorrect
answerAandincorrectanswer(theothernameintheprompt)Bbothwhenthefullmodelis
runasnormalandwhenthespecifiednodesareablated. Then,thepercentageofthefull
model’slogitdifferencewhichisrecoveredbytheablatedmodeliscalculated.
F(x) −F(x)
correct incorrect ×100
M(x) −M(x)
correct incorrect
WhereF(x) denotesthelogitofthecorrectanswertokenonF(x)(andothertermsare
correct
definedsimilarly). Alogitdifferencerecoveredof100%meansthecircuitoutputhasthe
samelogitdifferenceasthefullmodel. Anegativevaluemeansthatthecircuitoutputsthe
corruptlogitaslargerthanthecleanlogitandavalueover100%meansthecircuitoutput
hasagreaterlogitdifferencethanthefullmodel. Weadoptthisdefinitionoffaithfulnessfor
theremainderofthissection.
Wangetal.(2023)testthefaithfulnessoftheircircuitbypassinginacleaninputandNode
Ablatingthecomplementofthecircuit. Theydistinguishbetweentokenpositions–thatis,
theyablatenodesinthecircuitatalltokenpositionsexceptthosespecifiedbythecircuit.
TheyuseaMeanAblation,wherethemeanvalueiscomputedforeachtokenpositionover
theABCdistribution,usingaroundsevenexamplespertemplate.
4.1 VarianceBetweenAblationMethodologies
Wenowshowcircuitfaithfulnessissensitivetothesechoices. Firstwecomparethefaithful-
nessmetricwhenwechangetheablationcomponentfromnodestoedges-weablatethe
complementofthesetofedgesspecifiedbythecircuitinsteadofthecomplementoftheset
ofnodesinthecircuit. AsshowninFigure3,ablatingattheedgelevelreturnssubstantially
higherpercentages.
Figure3alsoevaluatestheeffectofablationvalue. Wereruntheaboveexperimentusing
ResampleAblationsfromtheABCdistribution,andfindthatthisresultsinasystematically
lowerfaithfulnessascomparedwithmeanablations(staticallysignificantonat-testwith
p =1e−5forNodeAblationbutnotEdgeAblation). Finally,westudytheeffectofablating
at every token position, instead of only those specified by the circuit. This consistently
resultsinlowerfaithfulnessscores. Itisconcerningthattheedge-levelcircuitwithspecific
tokenpositionshasamedianscorewellover100%,asthisbestrepresentsthehypothesisof
Wangetal.(2023).
Next, wediscusssensitivityofthefaithfulnessmetrictoboththecleandistributionand
intricaciesofthemetriccalculation. Fortheseexperiments,weperformnode-levelMean
Ablationsonthecomplementofthecircuit,splitbytokenposition,similarlytoWangetal.
2Wangetal.(2023)usedifferentmetricsthroughoutthepaper.Herewearereferringtothemetric
usedtotesttheoverallfaithfulnessofthecircuitinSection4oftheirpaper.
7PublishedasaconferencepaperatCOLM2024.
Nodes/Specific Toks Nodes/All Toks Edges/Specific Toks Edges/All Toks
600
400
d
e
er 200
v
o Perfect Faithfulness
c
e
R 0
ff
Di
git −200
o
L
−400
−600
Resample Mean (ABC) Resample Mean (ABC) Resample Mean (ABC) Resample Mean (ABC)
Figure3: TheIOIfaithfulnessmetricissensitiveto(1)ablatingedges/nodes,(2)thetype
of ablation used – we test Resample Ablations and Mean Ablations (over a dataset of
100ABCprompts,whichdiffersfromWangetal.(2023))and(3)whetherwedistinguish
betweentokenpositionsinthecircuit. TheoriginalIOIworkevaluatedatspecifictoken
positionswithMeanNodeAblationsandobtainedalogitdifferencerecoveryof87%. Other
methodologiesgivingfaithfulnessscoresabove100%orbelow0%wouldhavegiventhe
authorssignificantlylessconfidenceabouttheIOIcircuit,andmayhaveledthemtoinclude
differentedges.
(2023). AsshowninthelefttwochartsofFigure4,faithfulnessissystematicallygreater
for the prompts of form BABA than prompts of form ABBA. We also find that faithfulness
monotonicallyincreaseswiththesizeoftheABCdataset(usedforcomputingtheMean
Ablation).
FinallywenotethatWangetal.(2023)computethelogitdifferencerecoveredbyfirstfinding
themeanlogitdifferenceforthefullmodelandtheablatedmodeloverallprompts,and
thencomputingthepercentage(Figure4,farleft).
E[F(x) −F(x) ]
correct incorrect ×100
E[M(x) −M(x) ]
correct incorrect
Ifinsteadwecomputethepercentdifferenceforeachpromptandthentakethemean,we
returnsubstantiallyhigherpercentages(Figure4,middleleft).
(cid:20) F(x) −F(x) (cid:21)
E correct incorrect ×100
M(x) −M(x)
correct incorrect
These are significant and important changes in evaluation. If the researchers had used
a different methodology, they may have discovered a different circuit and, therefore, a
differentunderlyingalgorithm. Thisisimportantsinceitsuggeststhatthealgorithmthe
circuitisrequiredtoperformdependsontheablationmethodology. Weexpandonthis
pointinSection5.
4.2 VarianceBetweenIndividualDatapoints
Even for a fixed ablation methodology and metric, there is significant variation in the
measuredfaithfulnessbetweenindividualpromptsinthedistribution.
We show this for the IOI circuit in the figures above, with results for other circuits in
AppendixD.ThegraphsontherightofFigure4showalargerangeoffaithfulnessscores
attained when we ablate the complement of the nodes in the IOI circuit. Note that the
graphs do not show the full range of datapoints and there are several extreme outliers
8PublishedasaconferencepaperatCOLM2024.
[Average Logit Diff] % Average [Logit Diff %] ABBA Predictions BABA Predictions
500 500
ed130 130 ABBA
ver 120 120 400 400 BABA
eco 110 110 300 300 Average
e R 100 Perfect Faithfulness 100 200 200
c
en 90 90 100 100
er Reported Faithfulness
Diff 80 80 0 0
git 70 70 −100 −100
Lo 60 60
−200 −200
2 5 10 2 51002 2 5 10 2 51002 2 5 10 2 51002 2 5 10 2 51002
ABC Dataset Size
Figure4: (Left)TheIOIcircuitissensitivetothesizeofABCdatasetusedformeanablation.
ThelogitdifferencerecoveredisconsistentlyhigherforpromptsoftheBABAformat. (Left
andMiddleLeft)Theorderofcomputingtheaverageandpercentageaffectsthefaithfulness
metric. Wangetal.(2023)use[AverageLogitDiff]%,givinglowerscoresthanAverage
[LogitDiff%]. (MiddleRightandRight)Thereisalargerangeoflogitdifferencerecovered,
the boxplots show the interquartile range. According to this faithfulness measurement
methodology, The IOI circuit implements the IOI task faithfully on average, but not for
manysingledatapoints.
with a logit difference recovered in the tens of thousands of percent. The inter-quartile
range (IQR) is also large, stretching up to 50% across the dataset. This is concerning:
whilethecircuitmatchesthebehavioronaverage,itdoesnotmatchitformanyexamples.
Anotherpropertyofidealcircuitsdescribingbehaviouronsometaskisthattheirfaithfulness
varianceshouldbelowoverthetaskinputdistribution. Otherwise, thecircuitisatleast
partiallyoptimizedtobalanceoutextremelyhigh(significantly>100%)andextremelylow
faithfulness scores (<0%). This variance consideration is importantly missing from the
mechanisticexplanationsofhowGPT-2implementstheIOItaskprovidedbyWangetal.
(2023). WeencourageMIresearcherstoevaluatetaskperformanceinboththeaveragecase
andworstcase.
5 OptimalCircuitsAreDefinedByPromptsandAblation
Methodologies
Weshowedintheprevioussectionthatmeasurementdetailscangreatlychangethefaith-
fulnessscoreofanexperiment. However,onemightaskifthisdifferencematters. Inthis
sectionwediscusstheconsequencesofsuchsensitivityforcircuitdiscovery.
Ifacircuitisspecifiedasasetofedges,itshouldbetestedusingedgeablationsandifitis
specifiedwithtokenpositionsthenitshouldbetestedwithtoken-specificablation. Butin
otheraspectsthereoftenisn’taclearlycorrectmethodology. Sohowshouldwethinkabout
thedifferenceinfaithfulnessbetweendifferentmethodologies? Westudythisquestionin
smalltoymodels,wherewehaveaccesstothe‘groundtruth’circuit. Weconcludethatthe
optimalcircuitforsomedistributioncannotbedefinedunlesswealsospecifytheablation
methodologyandmetricthatweareusingtomeasureit.
Tracrmodels(Lindneretal.,2023)aretinytransformersthatarecompiledinsteadoftrained.
Sincethegroundtruthalgorithmisbothsimpleandknown,theyprovideanexcellentsetup
fortestingcircuitdiscoveryalgorithms. RASPprograms(Rush&Weiss,2023)arecompiled
intotheweightsofatransformerthatimplementstheprogramexactly. FollowingConmy
etal.(2023),westudytwoTracrmodels,ReverseandX-Proportion.
TheX-Proportionmodelperformsthetaskofoutputtingateachtokenpositionthepropor-
tionofpreviouscharactersthatare‘x’s. Themodelhastwolayers,withoneheadineach
attentionlayer. ThefirstattentionlayerandthesecondMLParenotused,soweneedonly
considertheedgesbetweentheInput,MLP 0,Attn 1.0andOutput.
Conmyetal. considertheedgefromInputtoAttn 1.0tobepartofthegroundtruthcircuit
(Figure11). InspectingtheRASPprogram,weseethattheonlyinformationinthisedge’s
9PublishedasaconferencepaperatCOLM2024.
Reverse (Theirs) X-Proportion (Theirs) Reverse (Ours) X-Proportion (Ours)
e 1 ACDC
at 0.8 SP
R
e HISP
v 0.6
ositi
0.4
P
ue 0.2
Tr
0
0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1
False Positive Rate
Figure5:ROCCurvesmeasuringtheoverlapbetweenautomaticallydiscoveredcircuitsand
thetwodifferent“groundtruth”circuits,fortwoTracrtasks. Whenwematchtheablation
methodologyofthegroundtruthwiththeablationmethodologyofthecircuitdiscovery
algorithms,wecanachieveperfectcircuitrecoverywithallthreemethods.
activation that is used by the model is the positional encoding of the tokens. However,
thisdoesnotvarybetweendifferentinputs,soifourablationmethodologyusesResample
Ablationsthenthisedgeneednotbeincludedinthecircuit,asablatingitwillnotchangethis
positionalinformation. However,ifweinsteaduseZeroAblations,thenthisinformation
willbedestroyed,sotheedgemustbeincludedinthecircuit.
Conmyetal. testthreeautomaticcircuitdiscoveryalgorithmsonthistask. Allthreeal-
gorithmsuse(orapproximate)ResampleAblationstodiscovercircuits. Thefirstmethod,
ACDC,traversesthemodelinreversetopologicalorder,ablatingeachedgeinturn. Subnet-
workProbing(SP)learnsamaskparameterforeachnode,viagradientdescent,attempting
tomaximizethenumberofnodesablated,whileminimizingtheKLdivergencefromthe
originalmodel. Lastly,HeadImportanceScoring(HISP),usesafirstorder,gradient-based
approximationofNodeAblationtoassignattributionscorestoeachnode. Wetesteach
circuitdiscoverymethodbysweepingoverarangeofimportancethresholdstoobtainan
ordering of circuits of increasing size. Following Conmy etal. we then plotpessimistic
receiveroperatingcharacteristic(ROC)curves(Figure5)andcomparetheareaundercurves.
SPandHISP,use(orapproximate)NodeAblations,whileACDCusesEdgeAblations.3In
ourexperimentsweadjusttheimplementationofbothSPandHISPtouse(orapproximate)
EdgeAblations;SPlearnsmaskparametersthatablateeachedgeandHISPassignsattri-
butionscoresforeachedgebyapproximatingEdgePatching. Weprovideacomparison
betweenEdgeandNode-basedcircuitdiscoverymethodsinAppendixE.
Conmyetal. consideredtheedgesthatwouldberequiredwithZeroAblationstobethe
correctcircuits. Therefore,thealgorithmsfailtofullyrecoverthe“groundtruth”. When
weinsteadconsidertheedgesthatarerequiredwithResampleAblationstobethecorrect
circuit,allthreealgorithmsperfectlyrecoverthe“groundtruth”(Figure5).
This case study illustrates that the optimal circuit with respect to only a set of prompts
is undefined. The ablation partly determines the task. In this case, we must decide - is
determiningthepositionalencodingpartofthetask? Ifsothenthezeroablationcircuit
shouldbeconsideredthe‘ground-truth’,ifnotthentheresampleablationcircuitshouldbe.
3ToconvertthepredictionsofSPandHISPtoedge-basedcircuits,Conmyetal.includealledges
whichconnecttwonodesofsufficientimportance.Withthisimplementationitmaybeimpossiblefor
SPandHISPtocorrectlyorderedges.Forexample,therecanbetwonodeswhicharebothindividually
important,butwheretheedgeconnectingthemisunimportant.
10PublishedasaconferencepaperatCOLM2024.
6 Conclusion
Inthisworkweshowexistingtransformercircuitevaluationsarehighlysensitivetosmall
changes in the ablation methodology and the metrics used to quantify faithfulness. We
further show that the optimality of a circuit cannot be defined with respect to a set of
promptswithoutapreciseevaluationmethodology
Ifacircuitisspecifiedasasetofedges,itshouldbetestedusingedgeablations. Andifit
isspecifiedatachosensetoftokenpositionsitshouldbetestedwiththese. Butinother
aspects there often isn’t a clearly correct methodology. Do you want your IOI circuit to
includethemechanismthatdecidesitneedstooutputaname? Thenusezeroablations.
Ordoyouwanttofindthecircuitthat,giventhecontextofoutputtinganame,completes
the IOI task? Then use mean ablations. The task cannot be separated from the ablation
methodology.
Ourworkhassignificantconsequencesforcircuitdiscoverywork,particularlyautomated
circuitdiscoveryalgorithmsthataimtooptimizethesefaithfulnessscores. Itsuggeststhat
assessingthequalityofautomatedmethodsbymeasuringtheoverlapwithsome‘ground
truth’ can be misleading, if the ground truth was discovered using a different ablation
methodology.
We recommend that researchers precisely describe their experimental procedure when
reportingevaluationsofcircuits.Theyshouldconsiderwhichtaskexactlytheyareexpecting
theircircuittoperform.
7 Acknowledgments
ThankstoArthurConmyforhisgenerousassistanceinunderstandingandreproducing
hisworkonAutomaticCircuitDiscoveryandhisinsightfulcomments. ThankstoAdam
Gleave,LawrenceChan,ClementNeo,AlexCloud,DavidBau,StevenBills,SamMarks,
Adria` Garriga-AlonsoandouranonymousreviewersatCOLM2024fortheirinvaluable
feedbackandsuggestions. ThankstoBryceWoodworthforhishelpandencouragement.
References
ChiragAgarwal,SreeHarshaTanneru,andHimabinduLakkaraju. Faithfulnessvs.plau-
sibility: Onthe(un)reliabilityofexplanationsfromlargelanguagemodels,2024. URL
https://arxiv.org/abs/2402.04614.
AfraAlishahi,GrzegorzChrupała,andTalLinzen. Analyzingandinterpretingneuralnet-
worksfornlp: Areportonthefirstblackboxnlpworkshop. NaturalLanguageEngineering,
25(4):543–557,2019. doi: 10.1017/S135132491900024X.
StellaBiderman,HaileySchoelkopf,QuentinAnthony,HerbieBradley,KyleO’Brien,Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,Edward
Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for
analyzinglargelanguagemodelsacrosstrainingandscaling,2023.
Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh,
Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can ex-
plain neurons in language models. https://openaipublic.blob.core.windows.net/
neuron-explainer/paper/index.html,2023.
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Con-
erly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan
Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-
Dodds,AlexTamkin,KarinaNguyen,BraydenMcLean,JosiahEBurke,TristanHume,
Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: De-
composinglanguagemodelswithdictionarylearning. TransformerCircuitsThread,2023.
https://transformer-circuits.pub/2023/monosemantic-features/index.html.
11PublishedasaconferencepaperatCOLM2024.
Nick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, and
Chris Olah. Curve circuits. Distill, 2021. doi: 10.23915/distill.00024.006.
https://distill.pub/2020/circuits/curve-circuits.
LawrenceChan,Adria` Garriga-Alonso,NicholasGoldowsky-Dill,RyanGreenblatt,Jenny
Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal
scrubbing: a method for rigorously testing interpretability hypotheses [redwood
research], 2022. URL https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/
causal-scrubbing-a-method-for-rigorously-testing.
BilalChughtai,LawrenceChan,andNeelNanda. Atoymodelofuniversality: Reverse
engineeringhownetworkslearngroupoperations,2023.
ArthurConmy,AugustineN.Mavor-Parker,AengusLynch,StefanHeimersheim,andAdria`
Garriga-Alonso. Towardsautomatedcircuitdiscoveryformechanisticinterpretability. In
Thirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
HoagyCunningham,AidanEwart,LoganRiggs,RobertHuben,andLeeSharkey. Sparse
autoencodersfindhighlyinterpretablefeaturesinlanguagemodels,2023.
NelsonElhage,NeelNanda,CatherineOlsson,TomHenighan,NicholasJoseph,BenMann,
AmandaAskell,YuntaoBai,AnnaChen,TomConerly,NovaDasSarma,DawnDrain,
DeepGanguli,ZacHatfield-Dodds,DannyHernandez,AndyJones,JacksonKernion,
LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,JackClark,JaredKaplan,Sam
McCandlish,andChrisOlah. Amathematicalframeworkfortransformercircuits. Trans-
formerCircuitsThread,2021. URLhttps://transformer-circuits.pub/2021/framework/
index.html.
AtticusGeiger,ZhengxuanWu,ChristopherPotts,ThomasF.Icard,andNoahD.Good-
man. Findingalignmentsbetweeninterpretablecausalvariablesanddistributedneural
representations. ArXiv,abs/2303.02536,2023. URLhttps://api.semanticscholar.org/
CorpusID:257365438.
NicholasGoldowsky-Dill, ChrisMacLeod, LucasSato, andAryamanArora. Localizing
modelbehaviorwithpathpatching,2023.
Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti,
and Dino Pedreschi. A survey of methods for explaining black box models. ACM
Comput. Surv., 51(5), aug 2018. ISSN 0360-0300. doi: 10.1145/3236009. URL https:
//doi.org/10.1145/3236009.
WesGurneeandMaxTegmark. Languagemodelsrepresentspaceandtime,2024.
MichaelHanna,OllieLiu,andAlexandreVariengien. Howdoesgpt-2computegreater-
than?: Interpretingmathematicalabilitiesinapre-trainedlanguagemodel,2023.
Stefan Heimersheim and Jett Janiak. A circuit for Python
docstrings in a 4-layer attention-only transformer, 2023.
URL https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/
a-circuit-for-python-docstrings-in-a-4-layer-attention-only.
AlonJacoviandYoavGoldberg. TowardsfaithfullyinterpretableNLPsystems:Howshould
wedefineandevaluatefaithfulness? InDanJurafsky,JoyceChai,NatalieSchluter,andJoel
Tetreault(eds.),Proceedingsofthe58thAnnualMeetingoftheAssociationforComputational
Linguistics,pp.4198–4205,Online,July2020.AssociationforComputationalLinguistics.
doi: 10.18653/v1/2020.acl-main.386. URL https://aclanthology.org/2020.acl-main.
386.
JanosKramar,TomLieberum,RohinShah,andNeelNanda. Atp*: Anefficientandscalable
methodforlocalizingllmbehaviourtocomponents,2024.
KennethLi,AspenK.Hopkins,DavidBau,FernandaVie´gas,HanspeterPfister,andMartin
Wattenberg. Emergentworldrepresentations: Exploringasequencemodeltrainedona
synthetictask,2023.
12PublishedasaconferencepaperatCOLM2024.
Maximilian Li, Xander Davies, and Max Nadeau. Circuit breaking: Removing model
behaviorswithtargetedablation,2024.
David Lindner, Ja´nos Kramar, Matthew Rahtz, Thomas McGrath, and Vladimir Miku-
lik. Tracr: Compiled transformers as a laboratory for interpretability. arXiv preprint
arXiv:2301.05062,2023.
AleksandarMakelov,GeorgLange,andNeelNanda. Isthisthesubspaceyouarelooking
for? aninterpretabilityillusionforsubspaceactivationpatching,2023.
Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron
Mueller. Sparsefeaturecircuits: Discoveringandeditinginterpretablecausalgraphsin
language models. Computing Research Repository, arXiv:2403.19647, 2024. URL https:
//arxiv.org/abs/2403.19647.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing
factualassociationsinGPT. AdvancesinNeuralInformationProcessingSystems,36,2022.
Neel Nanda and Joseph Bloom. Transformerlens. https://github.com/
TransformerLensOrg/TransformerLens,2022.
NeelNanda,LawrenceChan,TomLieberum,JessSmith,andJacobSteinhardt. Progress
measuresforgrokkingviamechanisticinterpretability,2023a.
Neel Nanda, Senthooran Rajamanoharan, Janos Kramar, and Rohin Shah. Fact
finding: Attempting to reverse-engineer factual recall on the neuron level,
Dec 2023b. URL https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/
fact-finding-attempting-to-reverse-engineer-factual-recall.
ChrisOlah,NickCammarata,LudwigSchubert,GabrielGoh,MichaelPetrov,andShan
Carter. Zoomin: Anintroductiontocircuits. Distill,2020. doi: 10.23915/distill.00024.001.
https://distill.pub/2020/circuits/zoom-in.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn
Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy
Jones,JacksonKernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,Jack
Clark,JaredKaplan,SamMcCandlish,andChrisOlah. In-contextlearningandinduction
heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-
context-learning-and-induction-heads/index.html.
NinaRimsky,NickGabrieli,JulianSchulz,MegTong,EvanHubinger,andAlexanderMatt
Turner. Steeringllama2viacontrastiveactivationaddition,2024.
Alexander Rush and Gail Weiss. Thinking like transformers. In ICLR Blogposts 2023,
2023. URL https://iclr-blogposts.github.io/2023/blog/2023/raspy/. https://iclr-
blogposts.github.io/2023/blog/2023/raspy/.
Lee Sharkey. Sparsify: A mechanistic interpretability research agenda,
2024. URL https://www.alignmentforum.org/posts/64MizJXzyvrYpeKqm/
sparsify-a-mechanistic-interpretability-research-agenda. Accessed: 2024-06-
28.
AaquibSyedandCanRager. Attributionpatchingoutperformsautomatedcircuitdiscovery,
92023.
JohnThickstun. Thetransformermodelinequations,2024. URLhttps://johnthickstun.
com/docs/transformers.pdf. Accessed: 2024-06-28.
CurtTigges,OskarJohnHollinsworth,AtticusGeiger,andNeelNanda. Linearrepresenta-
tionsofsentimentinlargelanguagemodels,2023.
AlexanderMattTurner,LisaThiergart,DavidUdell,GavinLeech,UlisseMini,andMonte
MacDiarmid. Activationaddition: Steeringlanguagemodelswithoutoptimization,2023.
13PublishedasaconferencepaperatCOLM2024.
JesseVig,SebastianGehrmann,YonatanBelinkov,SharonQian,DanielNevo,YaronSinger,
andStuartShieber. Investigatinggenderbiasinlanguagemodelsusingcausalmediation
analysis. InAdvancesinneuralinformationprocessingsystems,volume33,pp.12388–12401,
2020.
KevinRoWang,AlexandreVariengien,ArthurConmy,BuckShlegeris,andJacobSteinhardt.
Interpretability in the wild: a circuit for indirect object identification in GPT-2 small.
In The Eleventh International Conference on Learning Representations, 2023. URL https:
//openreview.net/forum?id=NpsVSN6o4ul.
FredZhangandNeelNanda. Towardsbestpracticesofactivationpatchinginlanguage
models: Metricsandmethods,2024.
14PublishedasaconferencepaperatCOLM2024.
A AutoCircuitLibrary
WereleaseAutoCircuit,aPythonlibrarywithahighlyefficientimplementationofEdge
Patchingandvariouscircuitdiscoveryalgorithms,withsupportforTransformerLensmod-
els(Nanda&Bloom,2022). ItsupportsMean,ZeroandResampleAblations. Seeourblog
postformoredetailonourfastimplementation.
WetesttheperformanceofourimplementationbyrunningtheACDC(Conmyetal.,2023)
circuitdiscoveryalgorithm,whichiterativelypatcheseveryedgeinthemodel. Wecompare
the performance of AutoCircuit’s implementation to the official ACDC implementation
(whichiscurrentlythemostpopularlibraryforpatchinglargenumbersofactivations). We
runACDCusingbothlibrariesatarangeofthresholdsforatiny2-layermodelwithonly
0.5millionparametersandmeasurethetimetakentoexecuteonasingleGPU.
100
9 AutoCircuit (ours)
78 ACDC
6
5
s) 4
d
on 3
c
e
s
e
( 2
m
Ti
n
utio 10
9
ec 8
x 7
E 6
5
4
3
0 0.2 0.4 0.6 0.8 1
Proportion of Model Edges Included in Circuit
Figure6: TimetoexecutetheACDCalgorithmovertheentirenetworkforasmall2layer
transformer. AutoCircuit(ours)ifsignificantlyfasterthantheofficialAutomaticCircuit
Discoverycodebase.
DifferentnumbersofedgesareincludedatdifferentthresholdsintheACDCalgorithm.
Note that ACDC and AutoCircuit count the number of edges differently (AutoCircuit
doesn’tinclude’DirectComputation’or’Placeholder’edges)sowecomparetheproportion
of edges included (the underlying computation graphs are equivalent). Figure 6 shows
thatourimplementationissignificantlyfasterandthenumberofedgesincludedgreatly
affects the performance of the official ACDC implementation, but it doesn’t change the
performanceofourimplementation.
15PublishedasaconferencepaperatCOLM2024.
B FurtherDetailsonAblationMethodology
Attn 0
Attn 0
Attn 1 MLP Input MLP Output
Input +  +  Output Attn 1
(a)Thecanonicalformulationofatransformer. (b) The “factorized” formulation of a trans-
Everycomponentreadsinputfromtheresid- formerviewseverycomponentastakingin-
ualstreambackbone. putfromeverypreviouscomponent.
Figure7: Twoequivalentformulationsofthetransformerarchitecture. Weillustrateonly
onelayer,butthisextendstriviallytomanylayers.
Input
Input
Attn 0
Input Attn 0
Input MLP Output Attn 0
Attn 0
Attn 0
Input Attn 1 Token0 MLP Output0
Token1 MLP Output1
Token2 MLP Output2
Attn 1
Input Attn 1
Attn 1
Attn 1
(a) The “treeified” formulation of a trans-
formerseparateseverypathfrominputtoout- (b) We can consider each token position to
put. haveaseparatesetofedges.
Figure 8: (Left)“Treefied” transformers suggest another type of ablation. (Right) Distin-
guishingbetweendifferenttokenpositionsinEdgePatching.
16PublishedasaconferencepaperatCOLM2024.
C SummaryofTasksStudied
Faithfulness
Name Model ExampleCleanPrompt ExampleCorruptPrompt CorrectAnswer IncorrectAnswer Metric
Tracr Tracr 0,0.5,0.333, Meansquared
X-Proportion X-Proportion y,x,z,x,w z,w,w,y,x 0.5,0.4 0,0,0,0,0.2 error
Tracr Tracr
Reverse Reverse 1,0,2,2,2 1,0,0,1,2 2,2,2,0,1 2,1,0,0,1 KLDivergence
Indirect Then,ScottandJeremywent Then,MichaelandAnderson Logit
Object GPT-2 tothehospital.Jeremygavea wenttothehospital.Rachel ”Scott” ”Jeremy” Difference
Identification snackto gaveasnackto Recovered
deferror(self,create,option, deferror(self,create,option, ”size”,”output”,
file,run,client,project): output,host,label,project): ”host”,”label”,
4Layer ”””landemploymentcamp ”””landemploymentcamp ”first”,”text”, Correct
Docstring Attention ”client” ”request”,”user”, Prediction
Only :paramfile:proteinauthor :paramfirst:proteinauthor ”file”,”run”, Proportion
:paramrun:forestdegree :paramtext:forestdegree ”create”,”option”,
:param :param ”project”
Fact:TigerWoodsplaysthe Fact:TigerWoodsplaysthe
SportsPlayers Pythia2.8B sportofgolf\nFact:Phil sportofgolf\nFact:BabeRuth ”football” ” ”b ba as sk ebet ab lla ”ll”, T Lo op giS tport
Simmsplaysthesportof playsthesportof
Table4: Thetaskswestudy,whichpreviousworkshavefoundcircuitsfor,andthemetrics
usedbypreviousworkstomeasuretheirfaithfulness.
17PublishedasaconferencepaperatCOLM2024.
D FurtherStudyofFaithfulnessMetrics
Inthissection,weprovidefurtheranalysisdemonstratingfaithfulnessmetricsarebrittle,
ontwoothercircuitsfromtheexistingliterature.
D.1 Docstring
TheDocstringTask. TheDocstringtask(Heimersheim&Janiak,2023)isasimpletaskthat
testsa4layer,attention-onlymodel’sabilitytocompleteaspecificpartofastandardPython
docstring(seeTable4foranexample). Allpromptsfollowaverysimilarformat,withthe
onlydifferencebeingthenamesofthevariablesinthefunction. Thecorruptdistribution
followstheexactsameformat,usingadisjointsetofvariablenames.
MeasuringDocstringCircuitFaithfulness. Heimersheim&Janiak(2023)testtheircircuit
usingasimilarmethodologytotheonewhichWangetal.(2023)usedtotesttheIOIcircuit.
Theyablateallnodesinthecomplementoftheircircuit. However,unlikeWangetal.(2023)
theyuseaResampleAblation(alsoknowninthiscontextasActivationPatching),andthey
donotdistinguishdifferenttokenpositions. Themetricthattheyuseforfaithfulnessisthe
percentofhighestlogitoutputsthatarethecorrectansweroversomesetofprompts.
80 Resample Tokenwise Mean Clean And Corrupt
Resample Tokenwise Mean Clean And Corrupt
70 1
%) 60 Reported Faithfulness
ect
Answer ( 345 000 Full Model Probability 00 .. 68
Corr 20 wer 0.4
s
10 An
0.2
0
Nodes [E Rd eg pes
o
rN teod de ]s (E td og kee ns s( )tokensN )odes Edges Nodes (E td og kee ns s( )tokens) 0
NodesE dgesN odesE (d tg oe kesF n(u stl o )l kM eo nd se )l NodesE dgesN odesE (d tg oe kesF n(u stl o )l kM eo nd se )l
(a) The faithfulness of the Docstring circuit
accordingtothecorrectanswerpercentmetric (b) The faithfulness of the Docstring circuit
usedbyHeimersheimetal. Heimersheim& asmeasuredbytheprobabilityofthecorrect
Janiak(2023)issensitivetothetypeofablation answerishighlyvariablebetweenindividual
usedtomeasurethecircuit. prompts.
Figure9: FaithfulnessmetricsfortheDocstringcircuitwhenablatingeverynodeoredge
notinthecircuit,atalltokenpositionsandattokenpositionsspecifiedbyHeimersheim&
Janiak(2023).
InFigure9,wetestthefaithfulnessoftheDocstringcircuitwithvariousablationmethod-
ologies. Wecompare: (1)distinguishingbetweendifferenttokenpositions(Heimersheim
&Janiakspecifytheircircuitwithtokenpositions,eventhoughtheydonotusethisinfor-
mationintheirfaithfulnessevaluations),(2)ablatingattheedge-levelandnode-level(they
alsospecifyedges,eventhoughtheyevaluateonlywithnodes),(3)ablatingwithResample
andMeanAblationsand(4)twodifferentfaithfulnessmetrics: correctanswerpercentage
andanswerprobability.
Wemeasurevarioussignificantchangesinfaithfulnessinresponsetotheseadjustments.
Most importantly, Edge Ablations perform significantly better using a Mean Ablation
insteadofaResampleAblation. HadHeimersheim&Janiak(2023)performededge-level
ResampleAblationsinsteadofnode-wiseResampleAblations,theymayhavetrustedtheir
circuitsignificantlyless(andiftheyhadusededge-levelMeanAblations,theymayhave
trusteditmore).
18PublishedasaconferencepaperatCOLM2024.
Distinguishingbytokenpositionalsohadalargeeffectonfaithfulnessscoresforbothnode-
wiseandedge-wiseablations. Theselowscoressuggestthecircuitisinfactperforming
significantcomputationontokenpositionsoutsideofthecircuitspecifiedbyHeimersheim
&Janiak(2023).
Whenwemeasuretheprobabilityofthecorrectanswerwefindthat, similartoIOI,the
variance between individual prompts is high. This is important for reasons outlined in
Section4.
D.2 SportsPlayers
Resample Mean
100 100 Resample Mean
1 1
80 80
cy 0.8 0.8
a 60 60 y
Accur
40 40
obabilit
00 .. 46 00 .. 46
20 20 Pr
0.2 0.2
0 0
Ablated C Mi orc du ei ltFull Model Ablated C Mi orc du ei ltFull Model 0
Ablated C Mi orc
du ei
ltFull
Mode0
l
Ablated C Mi orc
du ei
ltFull
Model
(a)Thepercentageofpromptsforwhichthe
correctsporthasthehighestoutputlogitwith (b)Theoutputprobabilityofthecorrectsport
MeanandResampleAblations. withMeanandResampleAblations.
Figure10: ThefaithfulnessoftheSportsPlayerscircuitisreducedwhenusingResample
Ablations.
TheSportsPlayersTask. TheSportsPlayerstask(Nandaetal.,2023b)isasimpletaskthat
teststhePythia-2.8bmodel’s(Bidermanetal.,2023)abilitytorecallthesportsoffamous
football,baseballandbasketballplayers. SeeTable4foranexample. Allpromptsfollowa
verysimilarformat,withtheonlydifferencebeingthenameofthesportsplayerinquestion.
Thecorruptdistributionfollowstheexactsameformat,witheachclean/corruptpairhaving
twoplayersofdifferentsports.
MeasuringSportsPlayersCircuitFaithfulness. InFigure10,wetestthefaithfulnessof
theedge-levelsportsplayerscircuit,distinguishingtokenpositionswhile(1)ablatingthe
complement with both Resample and Mean Ablations and (2) calculating two different
faithfulnessmetrics: correctanswerpercentage(consideringonlythethreepossiblesports,
followingNandaetal.(2023b))andanswerprobability.
WefindadramaticdifferenceincorrectanswerpercentagebetweenResampleandMean
Ablation. Thiscaseisalittledifferentbecausetheauthors’aimwasn’ttofindthefullcircuit
but to identify the place in the model where factual recall occurs, so this result doesn’t
negatetheirhypothesis.
Notethatrandomguessingwouldachieve33%accuracyasthereare3possiblesports,and
thisisroughlywhatweseewhenMeanAblatingthewholemodel. ButResampleAblating
addssignalfromthecorruptprompt,whichisalwaysadifferentsport,explainingthe0%
accuracyscorefortheAblatedModelandtheCircuit.
19PublishedasaconferencepaperatCOLM2024.
D.3 FurtherDetailontheX-ProportionTracrGroundTruthCircuits
Attn 1.0 Attn 1.0
Input MLP 0 Output Input MLP 0 Output
(theirs) The “ground truth” circuit for the (ours)The“groundtruth”circuitfortheTracr
TracrX-ProportiontaskusingZeroAblations. X-ProportiontaskusingResampleAblations.
Figure11: FortheTracrX-Proportioncircuit,theedgefromInputtoAttn 1.0isonlyused
totransferthepositionalencoding,soitisnotrequiredwhenusingResampleAblations,
sincethesepreserveinformationthatisconstantbetweenthecleanandcorruptdistribution.
Thisillustratestheprinciplethatoptimalcircuitscannotbedefinedwithoutanablation
methodology. (NodesAttn 0.0andMLP 1arenotshownastheyarenotusedinthismodel.)
20PublishedasaconferencepaperatCOLM2024.
E Edge-Basedvs. Node-BasedCircuitDiscoveryMethods
Reverse X-Proportion
1 1
0.8 0.8 N
o
d
0.6 0.6 e
S
0.4 0.4 e a
r
c
0.2 0.2 h
e
t
a
R 0 0
e 0 0.5 1 0 0.5 1
v
ti
si
o
P
e 1 1
u
Tr
0.8 0.8 E
d
g
0.6 0.6 e
S
0.4 0.4 e a
r
c
0.2 0.2 h
0 0
0 0.5 1 0 0.5 1
False Positive Rate
ACDC SP HISP
Figure12: ROCCurvesforEdge-BasedandNode-Basedcircuitdiscoverymethods,using
theResampleAblationedgesasthegroundtruth(ours).
InSection5,weadaptedtheSubnetworkProbing(SP)andHeadImportanceScoring(HISP)
circuitdiscoverymethodstouse(orapproximate)EdgeAblation. ACDC(Conmyetal.,
2023)alreadyusesEdgeAblations,butwecansimilarlyadaptACDCtouseNodeAblations.
WecomparetheperformanceoftheNodePatchingversionsofACDC,SPandHISPtothe
EdgePatchingversions,fortheResampleAblationbased“groundtruth”circuitintroduced
inSection5(FigureE).
21PublishedasaconferencepaperatCOLM2024.
F ClarifyingNomenclature
SomeauthorshaveuseddifferenttermsforsomeoftheconceptsintroducedinSection3.For
instance,ActivationpatchinghaspreviouslyalsobeencalledCausalTracingorInterchange
Intervention. In the remainder of this section, we summarise how our nomenclature
relatestotheterminologyusedbyRedwoodResearchintheirseriesofearlymechanistic
interpretabilitytransformer-circuitspapers. Chronologically,theseareWangetal.(2023);
Chanetal.(2022);Goldowsky-Dilletal.(2023).
Wefirstdiscussthefinal,mostcomprehensivework(Chanetal.,2022),whichwereferto
asCausalScrubbing. CausalScrubbingisaverygeneralapproachforevaluatingcircuits
togetherwithexplanationsoftheroleofnodeswithinthecircuit. Itgenericallycomprises
performingspecificbranch-basedResampleAblationsonthetreeifiedmodelonboththe
circuit and its complement. Causal Scrubbing randomly replaces activations with those
thatyourhypothesispredictswillnotchangethemodeloutput. Forinstance,ifweclaim
thatagivennodedetectswhethertheinputiseven,CausalScrubbingcouldpatchinan
activationfromadifferenteveninput,andexpectstheoutputnottochange. Ingeneral,
CausalScrubbingpermitsanarbitrarynumberofpossiblecounterfactualinputs.
Goldowsky-Dilletal.(2023)simplifythissetup,droppingthestrictrequirementofrequiring
anexplanationforeachnode. Thisreducesthehypothesisclasstothenowstandardcircuit
discoveryproblem;doessomepathmatterfortaskperformanceornot?
FinallyWangetal.(2023)performafurthersimplifiedversionofpathpatchingtodiscover
theIOIcircuit. ThisisequivalenttoEdgeResampleAblationinourterminologybutwhich
they call Path Patching. They patch paths one at a time, to establish which edges are
importantfortaskperformance. Importantly,Wangetal.(2023)reasonthattheIOItask
shouldbeanattention-onlytask,asitonlycomprisesmovinginformationbetweentokens.
Assuch,theytakenodestoonlybeattentionheads,withMLPsconsideredtobepartof
thedirectpathbetweennodes. Thisapproachofone-hoppathpatchingisextendedand
automatedbyConmyetal.(2023).
22