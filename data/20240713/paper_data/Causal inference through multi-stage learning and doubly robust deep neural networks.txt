Causal inference through multi-stage learning and
doubly robust deep neural networks
Yuqian Zhang∗ Jelena Bradic†
Abstract
Deepneuralnetworks(DNNs)havedemonstratedremarkableempiricalperformancein
large-scale supervised learning problems, particularly in scenarios where both the sample
size n and the dimension of covariates p are large. This study delves into the application
of DNNs across a wide spectrum of intricate causal inference tasks, where direct estima-
tion falls short and necessitates multi-stage learning. Examples include estimating the
conditional average treatment effect and dynamic treatment effect. In this framework,
DNNs are constructed sequentially, with subsequent stages building upon preceding ones.
To mitigate the impact of estimation errors from early stages on subsequent ones, we
integrate DNNs in a doubly robust manner. In contrast to previous research, our study
offers theoretical assurances regarding the effectiveness of DNNs in settings where the di-
mensionalitypexpandswiththesamplesize. Thesefindingsaresignificantindependently
and extend to degenerate single-stage learning problems.
1 Introduction
Innumerousbiomedical, economic, andpoliticalstudies, wegrapplewiththechallengesposed
by modern large-scale data. On one hand, the substantial sample size allows us to surpass
the limitations of traditional linear or parametric models, facilitating the derivation of more
accurate and robust conclusions through non-parametric methods. However, on the other
hand,modernbigdataoftenharborscopiousamountsofredundantinformation,exacerbating
what is commonly referred to as the “curse of dimensionality.” This phenomenon frequently
renders traditional non-parametric statistical methods impractical. Navigating the task of
derivingreasonablecausalrelationshipsfromthisextensivedatasettoinformreal-lifedecision-
making is a prevalent yet formidable challenge.
Neural networks are arguably the most popular machine learning methods in large-scale
industry applications these days. Unlike traditional non-parametric regression methods, such
askernelregressionandsmoothingsplines, deepneuralnetworks(DNNs)pairedwithrectified
linear units (ReLU) activation functions [Nair and Hinton, 2010] and stochastic optimization
techniques [Kingma and Ba, 2014] deliver outstanding empirical performance in large-scale
complex estimation problems where both the sample size and the covariates’ dimension are
relatively large.
∗Institute of Statistics and Big Data, Renmin University of China
†Department of Mathematics and Halicioglu Data Science Institute, University of California, San Diego,
E-mail: jbradic@ucsd.edu
1
4202
luJ
11
]LM.tats[
1v06580.7042:viXraWhiletheuse ofneuralnetworks insimpleregression problems alreadycomeswith certain
theoretical guarantees, in many scenarios, we are no longer merely satisfied with predicting
the future based on existing data. Instead, we aim to understand the causal relationships
between variables to make informed decisions. For causal inference problems, one of the main
differences from traditional regression and prediction problems lies in the potential outcome
framework, where we can only observe the potential outcome corresponding to the treatment
that the individual has been assigned to. In other words, we essentially face a missing data
problem. As a result, the parameters or functions we are interested in cannot be directly
estimated by performing regression methods on observable samples, as some of the related
variables are missing.
Double machine learning (DML) [Chernozhukov et al., 2017], also known as doubly robust
or augmented inverse probability weighting, provides a potent framework for such problems.
When our object of interest is a finite-dimensional parameter θ, we typically seek a repre-
sentation θ = E{ψ(Z;η0)}, where Z denotes the observable variables, η0 represents a set
of nuisance functions, and ψ is a (non-centered) doubly robust score function satisfying the
Neyman orthogonality condition
∂ E{ψ(Z;η0)}(η−η0) = 0, (1.1)
η
where ∂ denotes the Gateaux derivative operator with respect to η. To estimate θ, it suffices
η
to estimate the nuisance functions η0 and take the empirical average of the score functions
after plugging in the nuisance estimates. In contrast to other methods, including inverse
probability weighting (IPW) and G-computation, DML only requires a subset of the nuisance
functions to be correctly specified for achieving consistent estimates of the final parameters
of interest. Additionally, when all nuisance functions can be consistently estimated, DML
typically leads to faster convergence rates. Examples of such finite-dimensional parameters
include the regression coefficient in a partially linear regression model, the average treatment
effect (ATE), the average treatment effect for the treated (ATTE), and the local average
treatment effects (LATE); refer to Chernozhukov et al. [2017] for more details.
In more intricate causal inference scenarios, our target of estimation is a function θ(x) de-
fined as the conditional expectation of unobservable variables, such as the conditional average
treatment effect (CATE) in Example 2 below. When aiming to estimate such a conditional
mean function, we look for representations of the form
θ(x) = E{ψ(Z;η0) | X = x}, (1.2)
whereη0 representsnuisancefunctionsandψ isascorefunction. Giventhatthefunctionθ(x)
residesinasignificantlymorecomplexspace, estimatingsuchafunctionpresentsgreaterchal-
lenges than estimating a one-dimensional parameter, such as the ATE. Rather than simply
taking the empirical average over the estimated score functions ψ(Z ;η), specific regression
i (cid:98)
methods are required to estimate the conditional mean function, utilizing the first-stage es-
timates η. In this study, we focus specifically on the utilization of DNNs. Estimating θ(x)
(cid:98)
necessitates the construction of two DNNs in a sequential manner: one for related nuisance
functions and another for the final target of interest, with the second DNN being constructed
based on the first one. We refer to such a problem as a two-stage learning problem.
While there are typically multiple choices for the score function ψ that fulfill the repre-
sentation (1.2), we seek a score function designed to satisfy the following generalized Neyman
2orthogonality condition:
∂ E{ψ(Z;η0) | X = x}(η−η0) = 0, ∀x ∈ X, (1.3)
η
where X is the support of X. Note that the usual Neyman orthogonality condition (1.1)
can be viewed as a special case of the generalized version (1.3) when ψ(Z;η) ⊥⊥ X and θ(x)
degenerates to a constant function.
Similar to the usual Neyman orthogonality condition (1.1), the generalized version (1.3)
is beneficial in that it leads to faster convergence rates and is suitable to use when the
target parameter is infinite dimensional. Denote Y# = ψ(Z;η0) and Y(cid:98) = ψ(Z;η (cid:98)) and as the
true score and its plug-in estimate. As illustrated in Lemmas 4.1 and 5.1 below, the first-
stage estimation error impacts the second learning stage through the deviation Y(cid:98) −Y# =
∆ +∆ , where ∆ and ∆ depend linearly and quadratically on the first-stage estimation
1 2 1 2
error,respectively. Whenthecondition(1.3)isfulfilled,∆ exhibitsaconditionalmeanofzero
1
and can be regarded as additional noise in the second learning stage, exerting no influence
on the convergence rate for the second-stage estimation. This observation is underscored
in our Theorems 3.1-3.3 for DNNs. Consequently, only the smaller second-order term ∆
2
contributes to the second-stage error. It becomes evident that employing score functions that
satisfy condition (1.3) is advantageous in mitigating the impact of estimation errors from
preceding learning stages on subsequent ones.
In even more complex scenarios, a conditional mean function θ(x) that necessitates a
two-stage learning estimation serves merely as a nuisance function in obtaining estimates for
final parameters of interest. Examples include the estimation of the dynamic treatment ef-
fect (DTE) and the controlled direct effect, as illustrated in Examples 3 and 4 below. Once
an estimate of θ(x) is obtained, an additional doubly robust estimation step is necessary to
estimate the final parameter of interest. Under such circumstances, a three-stage learning
approach is required: first, construct DNN estimates for the nuisance functions that are di-
rectlyestimableusingtheobservedsamples; then, constructDNNestimatesfortheremaining
nuisance functions based on the results from the first stage; and lastly, obtain an estimate
for the final parameter of interest utilizing all the nuisance estimates. This framework can be
extended to multi-stage learning, involving more than three learning stages. For the sake of
readability and simplicity, we only present results for two- and three-stage learning.
It is worth noting that the issues of confounding and model misspecification are two main
challenges that hinder accurate causal conclusions from observational studies. To ensure the
ignorability conditions (e.g., Assumptions 1 and 5), researchers typically need to collect a
sufficient amount of covariates to ensure that most of the confounding variables are included,
even though many of the collected covariates might be redundant. Consequently, to reduce
the impact of unmeasured confounders, it is typically inevitable to face the challenge of high
dimensionality. Traditional non-parametric regression methods, such as kernel regression and
splines, often struggle with the “curse of dimensionality,” as mentioned above. Meanwhile,
althoughregularizedparametricregressionmethodshave beendevelopedandserveasnatural
choices in high dimensions, their usage may lead to significant bias when the parametric
models deviate from the truth. As a result, it is crucial to develop methods that are suitable
for high-dimensional covariates while still maintaining model robustness.
Neural networks stand out as a non-parametric approach that exhibits exceptional em-
pirical performance when both the sample size n and covariate dimension p are relatively
large. Initial research on neural networks focused mainly on shallow structures with smooth
3activation functions and dates back to the 1980s; see, e.g., Cybenko [1989], Hornik [1991],
Hornik et al. [1989]. However, until recently, there have been few advances in the statistical
theory. Bauer and Kohler [2019], Kohler et al. [2022] recently explored the non-parametric
regressionproblemforsmoothandsparsecompositefunctions. Whenthedimensionpisfixed,
their network exhibits an L error rate bounded by n−2β/(2β+q) (accompanied by logarithmic
2
terms), whereβ signifiesthesmoothnesslevel, andq denotesthesparsitylevel. However, such
theoretical findings did not extend to the high-dimensional case with a growing p. Addition-
ally, due to the challenges of numerical optimization and suboptimal empirical performance,
shallow networks with smooth activation functions have become marginalized in recent times.
Recent advancements in stochastic gradient descent have significantly advanced networks
based on deep structures and non-smooth Rectified Linear Unit (ReLU) activation functions,
achievingnotableempiricalsuccessinlarge-scalereal-worldapplications. Yarotsky[2017]first
established upper bounds for the approximation error of DNNs with ReLU activation func-
tions. Subsequently, Schmidt-Hieber [2020a] explored sparse ReLU DNNs, while Kohler and
Langer [2021] extended this work to fully connected networks with very deep or wide struc-
tures. They focused on the composition structure as in Bauer and Kohler [2019], achieving
an L error of approximately n−2β/(2β+q) for a conditional mean function with sparsity level q
2
and smoothness level β. However, these results hold only if the dimension p remains fixed. As
noted in Schmidt-Hieber [2020b], “since the input dimension p in deep learning applications
is typically extremely large, a possible future direction would be to analyze neural networks
with high-dimensional p → ∞ and compare the rates to other nonparametric procedures.”
In this work, we provide an initial attempt considering a growing dimension with the sample
size.
Further research, including Chen et al. [2022], Jiao et al. [2023], Liu et al. [2021], Nakada
andImaizumi[2020],Schmidt-Hieber[2019],hasinvestigatedanotherformofsparsestructure,
assuming the covariates X are supported on a low-dimensional manifold. This assumption is
particularly suitable for data types where the information of X can be represented in a low-
dimensional space, such as image and voice data. For the causal effect estimation problem
addressed in this paper, sparsity conditions on the conditional distributions of the type P
Y|X
seem more appropriate and have been extensively studied in the existing literature under
parametric models; see, e.g., Athey et al. [2018], Avagyan and Vansteelandt [2021], Bradic
et al. [2024], Farrell [2015], Smucler et al. [2019], Tan [2020], Zhang et al. [2021]. Therefore,
we adhere to sparsity structures on P and provide an initial attempt that allows for a
Y|X
growing dimension.
In the realm of causal setups, Farrell et al. [2021] also considered a fixed dimension p and
investigated the DNNs’ application to ATE estimation problems. Given that all the nuisance
functions involved can be directly identified through observable variables, employing DNNs
with the observed data alone is sufficient. In our work, we extend theoretical guarantees for
the application of DNNs to address more intricate causal problems, such as the estimation
of CATE and DTE. These problems involve constructing nested DNNs in a doubly robust
manner – the DNNs constructed at subsequent stages depends on previous ones, along with
the usage of doubly robust score functions satisfying the condition (1.3). We conduct a
comprehensive analysis for the doubly robust nested DNNs. Importantly, unlike the existing
works, our analysis accommodates a growing number of the covariate dimension p.
We commence by presenting general theories for doubly robust nested DNNs; refer to
Theorems 3.1-3.3. Even in the degenerate non-nested non-parametric regression problem
4wheresingle-stagelearningissufficient, tothebestofourknowledge, thesearethefirstresults
for DNNs allowing the covariates’ dimension to grow with the sample size while imposing a
sparse structure on the conditional distribution P (instead of P ). Additionally, unlike
Y|X X
Schmidt-Hieber [2020a], we do not require additional constraints to bound the weights of
the DNNs, simplifying the optimization procedure. Moreover, Theorem 3.3 considers the
user-friendly multilayer perceptrons (i.e., fully connected networks), and our theory does not
depend on an optimal structure among a collection of sparse networks. In contrast, finding
the optimal structure is an NP-hard problem and is practically unattainable. To sum up,
our considered networks are more computationally feasible. While Farrell et al. [2021] also
exploredtheperformanceofmultilayerperceptronsunderthedegeneratenon-nestedscenario,
ourTheorem3.3establishesafasterconvergenceratewhenthetrueconditionalmeanfunction
exhibits approximate sparsity and is applicable when the dimension p grows; see Remark 2
for further details.
1.1 Organization
InSection2,weintroducegeneralmulti-stagelearningproblemsandprovidespecificexamples
within the realm of causal inference. Section 3 lays the foundation with a comprehensive
theory for doubly robust nested DNNs, crucial for analyzing DNN-based multi-stage learning.
Sections 4 and 5 explore the estimation of CATE and DTE using DNNs, exemplifying the
two-stage and three-stage learning problems introduced in Section 2. Further discussion is
provided in Section 6.
1.2 Notation
For any a,b ∈ R, we denote ⌈a⌉ be the smallest integer that is larger or equal to a, a∧b :=
min(a,b) and a ∨ b := max(a,b). For sequences a ,b ≥ 0, a ≪ b denotes a = O(b );
n n n n n n
a ≫ b denotes b = O(a ); a ≍ b denotes both a = O(b ) and b = O(a ) holds. For
n n n n n n n n n n
anysetA,denote|A|asthenumberofelementsinA. Foranysub-sampleS′ ⊆ S,defineES′(·)
as the expectation taken over S′ and VarS′(·) as the corresponding variance. For any random
vectorZ,defineP (·)andE (·)astheprobabilitymeasureandtheexpectationtakenoverthe
Z Z
joint distribution of Z, respectively. For any random variable X, define ∥X∥ := inf{α > 0 :
∞
P(|X| > α) = 0} as the essential supremum and ∥X∥ := inf{α > 0 : P (|X| > α) = 0}
∞,PZ Z
as the essential supremum defined through the probability measure P .
Z
2 Multi-stage learning
In traditional point estimation and supervised learning problems, the parameter of interest
can be directly identified as the (conditional) expectation of observable variables. This in-
cludes, for instance, the population mean θ = E(Y) with observable Y and the conditional
expectation θ(x) = E(Y | X = x) with observable (X,Y). After collecting identical and inde-
pendent (i.i.d.) samples, it suffices to take the empirical average or perform certain regression
methods to estimate the parameter of interest. We refer to such problems as single-stage
learning.
In this work, we focus on more complex multi-stage learning problems where additional
learning stages are required to identify and estimate the parameter of interest.
52.1 Two-stage learning
In complex scenarios, such as causal setups, some of the underlying variables suffer from
missingness. Consequently, the parameter of interest cannot be directly identified through
the observable variables within a single step. We illustrate the average treatment effect
(ATE) estimation problem below, which is a degenerate special case of the two-stage learning
problem considered in this work.
Example 1 (Average treatment effect). Let Z = (S,T,Y) represent the observable variables,
where S ∈ Rp denotes the covariates, T ∈ {0,1} is a binary treatment indicator, and Y ∈ R is
the observed outcome. Consider the potential outcome framework and suppose the existence
ofpotentialoutcomesY(1),Y(0) ∈ R. Foreachindividual, onlyoneofthepotentialoutcomes
is observed with Y = Y(T); the counterfactual one is missing. The average treatment effect
(ATE)isdefinedasθ := E{Y(1)−Y(0)}. SincethedifferenceY(1)−Y(0)isunobservable,
ATE
we cannot directly estimate the ATE through the empirical average of the sample potential
outcomes’differences. Instead,weneedtorepresenttheATEparameterinatwo-stagefashion.
Undertheignorability(alsoknownasnounmeasuredconfounding)condition{Y(1),Y(0)} ⊥⊥
T | S, the ATE can be identified as:
(cid:26) T{Y −µ0(1,S)} (1−T){Y −µ0(0,S)}(cid:27)
θ = E µ0(1,S)+ −µ0(0,S)− , (2.1)
ATE π0(S) 1−π0(S)
where π0(s) := P(T = 1 | S = s) and µ0(t,s) = E(Y | S = s,T = t) for each t ∈ {0,1}.
Example 1 illustrates a scenario where the final parameter of interest θ can be identi-
fied through a representation θ = E{ψ(Z;η0)} and all the nuisance functions η0 = (π0,µ0)
can be identified as the conditional expectations through the observable variables. Such
problems have been extensively studied in the existing literature; for recent advances in high-
dimensional or non-parametric problems, refer to Bradic et al. [2019], Chernozhukov et al.
[2017], Farrell [2015], Farrell et al. [2021], Smucler et al. [2019], Tan [2020]. The ATE es-
timation problem in Example 1 is relatively simple, as the final parameter of interest is
finite-dimensional, and the related nuisance functions are easy to estimate. In this work, we
further consider more complex causal inference problems and study the applications of neural
networks under more complicated situations.
Example 2 (Conditional average treatment effect). Consider the same setup as in Example 1.
The conditional average treatment effect (CATE) is defined as θ (s) := E{Y(1)−Y(0) |
CATE
S = s}. Since Y(1)−Y(0) is unobservable, we cannot directly employ regression methods
based on the observable variables. Under the same conditions as in Example 1, one way to
identify the CATE is through the following doubly robust representation:
θ (s) = E(Y# | S = s), where
CATE
T{Y −µ0(1,S)} (1−T){Y −µ0(0,S)}
Y# = µ0(1,S)+ −µ0(0,S)− . (2.2)
π0(S) 1−π0(S)
In Example 2, our ultimate object of interest is a function that can be represented as
θ(x) = E{ψ (Z;η0) | X = x}, where X is a sub-vector of Z, ψ is a given score function, and
2 2
η0 = (η0,...,η0)denotethenuisancefunctions,withw beingapositiveinteger. Furthermore,
1 w
the nuisance functions can be identified through the form η (xj) = E{ψ (Z) | Xj = xj} for
j 1,j
6each j ≤ w, with Xj being a sub-vector of Z and ψ being a given score function. In
1,j
general, to estimate any function θ(x) that can be identified through the above two-stage
representations, it suffices to perform the following two-stage learning based on i.i.d. samples
(Z )N ∼ P :
i i=1 Z
1. Regressing ψ (Z ) on Xj for each j ≤ w and obtain η = (η ,...,η ).
1,j i i (cid:98) (cid:98)1 (cid:98)w
2. Regressing ψ 2(Z i,η (cid:98)) on X
i
and obtain θ(cid:98)(x).
Onecanalsoperformsamplesplitting(orcross-fitting)techniquestoreducethebiasresulting
from the nuisance estimation, as seen in Chernozhukov et al. [2017].
Although both the ATE and CATE estimation problems introduced in Examples 1 and
2 above lie within the scenario of two-stage learning, estimating the CATE is harder than
the ATE. This is because the CATE is an infinite-dimensional parameter (unless assuming
certain parametric models) and hence lies within a much more complex space than the one-
dimensional ATE. On the other hand, the ATE estimation problem is a degenerate situation
of the two-stage learning, as the second learning stage degenerates to a constant function
estimation problem and can be easily estimated through taking the empirical average.
While designing the first-stage score functions ψ is natural, as the nuisance functions
1,j
canbedirectlyidentifiedthroughtheobservablevariables,thechoiceofthesecond-stagescore
functionψ isusuallynon-unique. Inthiswork,wespecificallyfocusonthedoublyrobustscore
2
functions that satisfy the generalized Neyman orthogonality condition (1.3). Such doubly
robust score functions are helpful to obtain more accurate estimates for the final parameter
of interest θ(x). For instance, although the CATE can be readily identified through the
difference of conditional means, θ (s) = µ0(1,s)−µ0(0,s), relying on this straightforward
CATE
identification may yield suboptimal results when the conditional mean functions µ0(1,s) and
µ0(0,s) are significantly more complex than the CATE itself. The estimation of CATE based
on the doubly robust representation (2.2), known as the DR-learner and initially proposed
by Van der Laan [2006], yields superior convergence results, especially when the propensity
score function π0(s) can be accurately estimated.
Recently, Kennedy [2023] explored the use of linear smoothers for the CATE estimation
problem, which is suitable only for classical low-dimensional settings. Foster and Syrgkanis
[2023]proposedageneralframeworkallowingforflexibleregressionmethods, includingabrief
introduction to results based on DNNs. However, they simply assumed that the CATE lies
within the class of DNNs and provided a slower convergence rate along with less robust re-
sults; see detailed comparisons in Remark 3. Notably, these existing works only addressed
two-stage learning problems. In this work, we aim to offer a comprehensive analysis of em-
ploying DNNs in multi-stage learning problems. Section 4 will focus on DNN-based CATE
estimation, serving as an illustration for two-stage learning problems, though the method’s
applicability extends to broader scenarios, including other two-stage learning problems and
problems requiring three or more stages.
2.2 Three-stage learning
In the following, we further consider situations where estimating the ultimate objects of
interest requires more than two stages of learning.
7Example 3 (Dynamic treatment effect with two exposures). Consider a dynamic setup with
two exposures of treatment assignments. Let Z = (S ,T ,S ,T ,Y) be the observable vari-
1 1 2 2
ables, where S
1
∈ Rd1 and S
2
∈ Rd2 denote the covariates evaluated before the first and
second exposures, T ,T ∈ {0,1} are binary treatment indicators assigned at the first and
1 2
second exposures, and Y ∈ R is the observed outcome at the last stage. Let T := (T ,T )
1 2
and Y(t) ∈ R be the (unobservable) potential outcome for any t = (t ,t ) ∈ {0,1}2. As-
1 2
sume the consistency Y = Y(T ,T ) and the sequential ignorability Y(t) ⊥⊥ T | S and
1 2 1 1
Y(c) ⊥⊥ T | (S ,T = t ), where S := (S⊤,S⊤)⊤. For any a,a′ ∈ {0,1}2, the dynamic
2 2 1 1 2 1 2
treatment effect (DTE) between paths a and a′, is defined as θ := E{Y(a)−Y(a′)}. In
DTE
the following, we introduce the identification of E{Y(1,1)}, which can be generalized to the
identification of E{Y(t)} for any t ∈ {0,1}2:
(cid:20) T {ν0(S )−µ0(S )} T T {Y −ν0(S )}(cid:21)
E{Y(1,1)} = E µ0(S )+ 1 2 1 + 1 2 2 , (2.3)
1 π0(S ) π0(S )ρ0(S )
1 1 2
where π0(s ) := P(T = 1 | S = s ), ρ0(s ) := P(T = 1 | S = s ,T = 1), µ0(s ) :=
1 1 1 1 2 2 2 2 1 1
E{Y(1,1) | S = s ,T = 1}, and ν0(s ) := E{Y(1,1) | S = s ,T = T = 1} are the
1 1 1 2 2 2 1 2
nuisance functions. Note that three of the nuisance functions, π0, ρ0, and ν0 can be directly
identified through observable variables. However, the remaining nuisance function µ0, rep-
resenting the conditional potential outcome given all the information at and before the first
treatmentexposure, cannotbedirectlyidentifiedasY(1,1)isnotobservablewithinthegroup
ofT = 1. Toidentifythefunctionµ0, wecanfurtherconsideradoublyrobustrepresentation:
1
(cid:16) (cid:17) T {Y −ν0(S )}
µ0(s ) = E Y# | S = s ,T = 1 , where Y# = ν0(S )+ 2 2 . (2.4)
1 1 1 1 2
ρ0(S )
2
Generally speaking, Example 3 illustrates a situation where the parameter of interest can
be represented as θ = E{ψ (Z,η0,η0)}, where ψ is a score function, η0 = (η0 ,...,η0 )
3 1 2 3 1 1,1 1,w1
are the directly-identifiable nuisance functions, and η0 = (η0 ,...,η0 ) are the remaining
2 2,1 2,w2
nuisance functions, with w ,w being positive integers. The nuisance functions η0 can be
1 2 2
further identified as η0 (x2,j) = E{ψ (Z,η0) | X2,j = x2,j} for each j ≤ w , where ψ is
2,j 2,j 1 2 2,j
the score function for η0 and X2,j is a sub-vector of Z. Lastly, η0 can be directly identified
2,j 1
as η0 (x1,j) = E{ψ (Z) | X1,j = x1,j} for each j ≤ w , where ψ is the score function for
1,j 1,j 1 1,j
η0 and X1,j is a sub-vector of Z. Based on the above three-stage representations, we can
1,j
estimate θ through a three-stage learning process:
1. Regressing ψ (Z ) on X1,j for each j ≤ w and obtain η = (η ,...,η ).
1,j i i 1 (cid:98)1 (cid:98)1,1 (cid:98)1,w1
2. Regressing ψ (Z ,η ) on X2,j for each j ≤ w and obtain η = (η ,...,η ).
2,j i (cid:98)1 i 2 (cid:98)2 (cid:98)2,1 (cid:98)2,w2
3. Taking the empirical average over ψ 3(Z i,η (cid:98)10,η (cid:98)20) and obtain θ(cid:98).
AlthoughtheultimateobjectofinterestconsideredinExample3isalsoaone-dimensional
parameter, its identification is more complicated than the ATE in Example 1 – as discussed
above, the identification of related nuisance functions is no longer a trivial problem. As a
result, an additional learning stage is required to estimate the DTE parameter.
In Example 3, we consider doubly robust score functions ψ and ψ satisfying the gen-
2,j 3
eralized Neyman orthogonality condition (1.3). Although both E{Y(t)} and µ0 can be
8identified through other representations, e.g., the G-formulas E{Y(t)} = E{µ0(S )} and
c 1
µ0(s ) = E{ν0(S ) | S = s ,T = c }, the doubly robust representations (2.3) and (2.4)
c 1 c 2 1 1 1 1
enhance robustness. This strategy has been introduced by Bradic et al. [2024], D´ıaz et al.
[2023], Luedtke et al. [2017], Rotnitzky et al. [2017]. Among these works, D´ıaz et al. [2023]
provided results for the final DTE estimate, assuming certain conditions on the nuisance
estimates without analyzing the nuisance estimation. However, it is worth noting that the
nuisance estimation problem is arguably one of the most challenging aspects of the problem
– the nuisance functions are estimated in a sequential manner, and understanding how the
estimation errors at previous stages affect the subsequent ones and the final DTE needs thor-
ough study. Luedtke et al. [2017] considered Donsker class of nuisance estimates, primarily
suitable for low-dimensional parametric models. Rotnitzky et al. [2017] explored linear oper-
ators, effective only in low dimensions with data-independent representations. Bradic et al.
[2024] extended the study to include Lasso-type nuisance estimates in high dimensions but
focused exclusively on parametric working models. Additionally, Zhang et al. [2021] proposed
a sequential model doubly robust approach that provides valid inference allowing for certain
model misspecification in high dimension, but still requiring some of the parametric models
to be correctly specified. Notably, all existing works either addressed low-dimensional situa-
tions or relied on parametric models, with none accommodating both high-dimensional and
non-parametric models. In Section 5, we will introduce a DNN-based DTE estimator and es-
tablishtheoreticalguarantees,onlynecessitatingsmoothnessconditionswhileaccommodating
diverging dimensions.
Below, we further illustrate a causal mediation analysis problem, which is analogous to
Example 3, considering the mediator as the second-stage treatment variable.
Example 4 (Controlled direct effect). Consider the causal mediation analysis. Let Z =
(S 1,T,S 2,M,Y), where S
1
∈ Rd1 denotes the baseline covariates and S
2
∈ Rd2 represents
the covariates evaluated subsequent to the treatment assignment but before the mediation.
Denote S := (S⊤,S⊤)⊤. Let M ∈ M be a discrete mediator with support M ⊆ R and
2 1 2
Y ∈ R be the observed outcome variable. Denote M(t) and Y(t,m) be the potential me-
diator and outcome, respectively, for any t ∈ {0,1} and m ∈ M. The observed mediator
and outcomes are M = M(T) and Y = Y(T,M(T)). Assume the ignorability conditions
Y(t,m) ⊥⊥ T | S and Y(t,m) ⊥⊥ M | (S ,T = t). Then, for any m ∈ M, the controlled
1 2
direct effect, θ (m) := E{Y(1,m)−Y(0,m)} := θ −θ , can be identified through the
CDE 1,m 0,m
following representation: for each t ∈ {0,1},
(cid:34) (cid:35)
1 {ν0 (S )−µ0 (S )} 1 {Y −ν0 (S )}
θ = E µ0 (S )+ T=t t,m 2 t,m 1 + T=t,M=m t,m 2 ,
1,m t,m 1 π0(S ) π0(S )ρ0 (S )
t 1 t 1 t,m 2
where π0(s ) := P(T = t | S = s ), ρ0 (s ) := P(M = m | S = s ,T = t), µ0 (s ) :=
t 1 1 1 t,m 2 2 2 t,m 1
E{Y(t,m) | S = s ,T = t}, and ν0 (s ) := E(Y | S = s ,T = t,M = m). Similarly as in
1 1 t,m 2 2 2
Example 3, the nuisance function µ0 can be identified as
t,m
(cid:34) (cid:35)
1 {Y −ν0 (S )}
µ0 (s ) = E ν0 (S )+ M=m t,m 2 | S = s ,T = t .
t,m 1 t,m 2 ρ0 (S ) 1 1
t,m 2
Examples2and4illustratesituationswherethreelearningstagesarerequired. Infact,the
framework can be extended to more general situations with more than three learning stages.
9This includes, for instance, the estimation of dynamic treatment effects (DTE) with more
than two time exposures; see, for example, Section 5 of Bradic et al. [2024]. Similar results
can be extended under such general cases, though additional proofs, similar but cumbersome,
would be required.
The multi-stage learning problems described above necessitate the sequential application
of regression methods. In this paper, our particular focus is on employing state-of-the-art
DNNs, known for their exceptional empirical performance. To rigorously examine the multi-
stage learning problem, we begin by presenting results for the doubly robust nested DNNs,
recognizingthatDNNsconstructedatsubsequentlearningstagesdependonthoseconstructed
at preceding stages.
3 Doubly robust nested DNNs
In this section, we introduce the construction of DNNs and present general theoretical results
for DNNs based on nested structure.
3.1 Construction of DNNs
We begin by introducing the construction of DNNs based on rectified linear unit (ReLU)
activation functions. A neural network is a function f(x) that maps Rp (cid:55)→ R. The network
is constructed with an input layer comprising p input units, L hidden layers, and an output
unit. Ineachhiddenlayerl ∈ {1,...,L}, thereareH(l) hiddenunitsdenotedasz(l) ,...,z(l) .
1 H(l)
The output unit is represented as f(x) = z(L+1) with H(L+1) = 1. Initially, we consider a
1
general class of feedforward networks.
For any 1 ≤ l ≤ L+1 and 1 ≤ k ≤ H(l), a hidden or output unit z(l) is connected with
k
certain units, possibly belonging to any of the preceding layers. We denote E(l) ⊆ {(l′,k′) :
k
0 ≤ l′ ≤ l−1,1 ≤ k′ ≤ H(l′)} as the collection of pairs (l′,k′) such that z(l′) is connected with
k′
z(l) . For simplicity, we denote the input units as z(0) = x ,...,z(0) = x with H(0) = p. The
k 1 1 p p
hidden and output units are then constructed as follows:
 
z(l) = σ(l) (cid:88) w(l,l′) z(l′) +b(l), ∀l ∈ {1,...,L+1}, k ∈ {1,...,H(l)}, (3.1)
k  k,k′ k′ k 
(l′,k′)∈E(l)
k
(l,l′) (l)
wherew andb aretheweightandintercept(bias)parameters. Foreachl ≤ L, weutilize
k,k′ k
the ReLU activation function σ(l)(x) = x∨0; whereas, the output layer employs the identity
activation function σ(L+1)(x) = x. Note that we allow connected edges between units from
non-neighboring layers.
Let F = F (L,(H(l)) ,(E(l) ) ) be the collection of neural networks
DNN DNN l≤L+1 k l≤L+1,k≤H(l)
defined through (3.1) with a user-chosen network architecture. Additionally, we focus on
two specific classes of network architectures. Denote F (L,U,W) as the collection of
DNN
feedforward neural networks with L hidden layers, U computation units, and W weights. In
otherwords, itistheunionofF (L,(H(l)) ,(E(l) ) )satisfying(cid:80)L+1H(l) =
DNN l≤L+1 k l≤L+1,k≤H(l) l=1
(cid:80) (l)
U and |E | = W; see an example in Figure 1a. Additionally, we denote
l≤L+1,k≤H(l) k
10(a) A network belonging to F (2,6,10) (b) The multilayer perceptron F (2,3)
DNN MLP
Figure1: Illustrationsofgeneralfeedforwardnetworks(left)andmultilayerperceptrons(right)
F (L,H) as multilayer perceptrons consisting of L hidden layers with H units in each
MLP
hidden layer. In this architecture, the units are connected to all units from neighboring layers
and remain unconnected with units from non-neighboring layers; see an example in Figure
1b.
3.2 DNNs through nested doubly robust regression
Let S1 := (X ,R ,Y#)n be independent and identically distributed (i.i.d.) samples, where
i i i i=1
X ∈ Rp is the covariate vector, R ∈ {0,1} is a sub-group indicator, and Y# ∈ R denotes
i i i
the outcome variable of interest. The indicator R is introduced to allow the established
i
results to be easily applied to problems involving regression within a specific subgroup. If the
target population is the entire population, one can simply set R ≡ 1. Consider situations
i
where Y i# is possibly non-observable and let Y(cid:98)i = (cid:98)h(X i;S2) be an estimate of Y i#, where
S2 ⊥⊥ S1 is another set of random samples. Let (X,R,Y#) be an independent copy of S1 and
Y(cid:98) =(cid:98)h(X;S2). Define the nested DNN estimate f(cid:98)as:
n
(cid:88) (cid:110) (cid:111)2
f(cid:98)∈ arg min n−1 R
i
Y(cid:98)i−f(X i) . (3.2)
f∈FDNN:∥f∥∞≤2M
i=1
This estimate is for f0(x) := E(Y# | X = x,R = 1) based on the user-chosen architecture
F . Here, M is an arbitrarily large constant independent of the sample size n. The bound
DNN
∥f∥ ≤ 2M enforces uniform boundedness, constituting the weakest form of constraint;
∞
see, for example, Fan et al. [2020], Farrell et al. [2021], Schmidt-Hieber [2020a]. Define the
approximation error ϵ as:
n
ϵ := inf ∥f −f0∥ . (3.3)
n ∞
f∈FDNN:∥f∥∞≤2M
Now, we present the following theorem characterizing the convergence results of the nested
DNN f(cid:98).
11Theorem 3.1. Let Y(cid:98) − Y# = ∆
1
+ ∆
2
with some ∆
1
= ∆ 1(Z;S2) and ∆
2
= ∆ 2(Z;S2)
satisfying
E (∆ | X,R = 1) = 0 almost surely and E (∆2 | R = 1) = O (e2). (3.4)
Z 1 Z 2 p n
Assume X ⊆ [−1,1]p, ∥RY#∥ ≤ M, ∥R∆ ∥ = O (1). Let n ≫ WLlogW, where L
∞ 1 ∞,PZ p
and W denote the numbers of hidden layers and weights, respectively. Then, as n → ∞,
(cid:18) (cid:19)
(cid:104) (cid:105) WLlogW logn
E
X
R{f(cid:98)(X)−f0(X)}2 = O
p n
+e2 n+ϵ2
n
. (3.5)
Remark 1 (Nested doubly robust regression and generalized Neyman orthogonality). In the
following, we explain how Theorem 3.1 applies to doubly robust regression problems based on
representations of the forms (1.2)-(1.3). Let R ≡ 1, Y# = ψ(Z;η0), and Y(cid:98) = ψ(Z;η (cid:98)), where
the nuisance estimates η = η(S2) are obtained from the training samples S2. Using Taylor’s
(cid:98) (cid:98)
theorem, we can decompose the first-stage error as Y(cid:98) −Y# = ψ(Z;η (cid:98))−ψ(Z;η0) = ∆ 1+∆ 2.
Here, ∆ = ∂ ψ(Z;η0)(η−η0)representsthelinearapproximationofthefirst-stageerror, and
1 η (cid:98)
∆ isaremainderterm. Whenthescorefunctionψ satisfiesgeneralizedNeymanorthogonality
2
(1.3), the linear approximation has a zero conditional mean, i.e., E (∆ | X) = 0, where the
Z 1
expectation is taken only over a new observation Z independent of S2.
The nested DNN (3.2) yields a convergence rate (3.5) while estimating the conditional
mean function f0(x) = θ(x) = E{ψ(Z;η0) | X = x}. It is essential to note that the conver-
gence rate (3.5) does not depend on the linear approximation part of the first-stage error, ∆ ;
1
it only involves the remainder term ∆ (characterized by e2), which is potentially small.
2 n
Theorem 3.1 provides general estimation results for nested DNNs, where the network is
based on an arbitrary user-chosen architecture F . The final estimation rate (3.5) involves
DNN
the approximation error ϵ based on the chosen architecture F , which necessitates further
n DNN
studyandcontrol. Forsmoothfunctions, Theorem1ofYarotsky[2017]offersanupperbound
for the approximation error over the class F (L,U,W) when L,U,W are sufficiently large.
DNN
For any β,q ≥ 0, we define the q-dimensional H¨older ball with smoothness β as:
(cid:26) (cid:27)
Wβ,∞([−1,1]q) := f : max |Dαf(x)| ≤ 1,∀x ∈ [−1,1]q , (3.6)
|α|≤β
where α = (α ,...,α ), |α| = (cid:80)q α , and Dαf is the weak derivative.
1 q j=1 j
Consider the following “optimal” network:
n
(cid:88)(cid:110) (cid:111)2
f(cid:98)opt ∈ arg min n−1 Y(cid:98)i−f(X i) , (3.7)
f∈FDNN(L,U,W):∥f∥∞≤2M
i=1
whichisthenestedDNNestimatebasedontheoptimalarchitectureoveracollectionofarchi-
tectures with a given triple (L,U,W). The following theorem characterizes the convergence
rate of f(cid:98)opt with optimal choices of parameters (L,U,W).
Theorem 3.2. Let the assumptions in Theorem 3.1 hold. Assume that f0(x) can be approx-
imated by a sparse smooth function f0(x ) : Rq (cid:55)→ R that
Q Q
∥f0(X)−f0(X )∥ ≤ r , (3.8)
Q Q ∞ n
12where x := (x ) , X := (X ) , |Q| = q ≥ 1 is a constant independent of n,
Q j j∈Q Q j j∈Q
r = o(1) is a non-negative sequence and f0 ∈ Wβ,∞([−1,1]q) with some constant smooth-
N Q
ness parameter β ≥ 0. Set ϵ¯ := n−β/(2β+q)log4β/(2β+q)n, let L, U, and W satisfy L ≥
n
−q/β −q/β
c{log(1/ϵ¯ )+1}, U,W ≥ cϵ¯ {log(1/ϵ¯ )+1}withL ≍ lognandU ≍ W ≍ ϵ¯ log(1/ϵ¯ ) ≍
n n n n n
nq/(2β+q)log1−4q/(2β+q)n, where c > 0 is a constant. Then, as n → ∞,
E(cid:104) R{f(cid:98)opt(X)−f0(X)}2(cid:105) = O p(cid:16) n− 2β2 +β q log2β8 +β q n+r n2 +e2 n(cid:17) . (3.9)
Consider a degenerate single-stage learning where Y(cid:98)i = Y i# and e
n
= 0. Moreover, as-
sume that the function f0 is precisely sparse and smooth with r = 0. In this case, the
0
convergence rate (3.9) solely involves n−2β/(2β+q)log8β/(2β+q)n. This rate is nearly optimal
for non-parametric estimation when the “active” covariate set Q is known. Notably, the con-
ditionsandoutcomesinTheorem3.2dependsolelyonthesparsitylevelq andareunrelatedto
the original dimension p. This is because the class of DNNs F (L,U,W) optimized in (3.7)
DNN
encompasses “oracle” structures with redundant input units unconnected to any subsequent
layers.
Although the statistical error of f(cid:98)opt is nearly minimax optimal, determining the optimal
sparse architecture from the collection F (L,U,W) is computationally intractable and re-
DNN
quires extensive tuning of architectures, a problem that is NP-hard. Pruning methods are
sometimes used to ensure network sparsity in practice, but they lack guarantees for reaching
the optimal architecture within an acceptable timeframe and are criticized for high compu-
tational costs; see, e.g., Evci et al. [2019], Liu et al. [2018]. Therefore, we consider a simpler
yet widely used class of functions, the multilayer perceptrons F (L,H), which consists of
MLP
only one architecture for a given pair (L,H); see an illustration in Figure 1b. Let f(cid:98) be the
MLP
nested DNN estimate based on multilayer perceptrons:
n
(cid:88) (cid:110) (cid:111)2
f(cid:98)
MLP
∈ arg min n−1 R
i
Y(cid:98)i−f(X i) . (3.10)
f∈FMLP(L,H):∥f∥∞≤2M
i=1
The following theorem provides the convergence rate for f(cid:98) .
MLP
Theorem 3.3. Let the assumptions in Theorem 3.1 hold. Assume that f0(x) can be approx-
imated by a smooth function f0(x ) : Rq (cid:55)→ R with (3.8) holds and f0 ∈ Wβ,∞([−1,1]q) with
Q Q Q
some constants q ≥ 1 and β ≥ 0.
Case (a): p = O(nq/(2β+2q)log(3β−q)/(β+q)n). Set ϵ˜ := n−β/(2β+2q)log4β/(β+q)n. Let L
n
−q/β
and H satisfy L ≥ c{log(1/ϵ˜ )+1}, H ≥ ⌈cϵ˜ {log(1/ϵ˜ )+1}⌉(L+1) with L ≍ logn and
n n n
H ≍ nq/(2β+2q)log(2β−2q)/(β+q)n, where c > 0 is a constant. Then, as n → ∞,
E(cid:104) R{f(cid:98) MLP(X)−f0(X)}2(cid:105) = O p(cid:16) n− β+β q logβ8 +β q n+r n2 +e2 n(cid:17) . (3.11)
Case (b): p ≫ nq/(2β+2q)log(3β−q)/(β+q)n and p = o(n/log5n). Set ϵ˜ := (p/n)β/(2β+q) ·
n
log5β/(2β+q)n. Let L and H satisfy L ≥ c{log(1/ϵ˜ )+1}, H ≥ ⌈cϵ˜−q/β {log(1/ϵ˜ )+1}⌉(L+1)
n n n
with L ≍ logn and H ≍ (n/p)q/(2β+q)log(4β−3q)/(2β+q)n, where c > 0 is a constant. Then, as
n → ∞,
(cid:104) (cid:105) (cid:16) 2β 10β (cid:17)
E R{f(cid:98) MLP(X)−f0(X)}2 = O
p
(p/n)2β+q log2β+q n+r n2 +e2
n
. (3.12)
13Remark 2 (Convergence rate). The convergence rates of multilayer perceptrons, (3.11) or
(3.12), are slower than DNNs based on optimal structures, (3.9). This discrepancy arises
because the architectures of multilayer perceptrons include unnecessary edges that do not
contribute to approximation power but increase estimation error due to added structural
complexity. However, the multilayer perceptrons are much more computationally attractive,
as the class F (L,H) contains only one network structure for any fixed pair of hyperpa-
MLP
rameters (L,H). Conversely, finding the optimal structure over a class F (L,U,W) with a
DNN
giventriple(L,U,W),oroveracollectionofsparsenetworks[Fanetal.,2020,Schmidt-Hieber,
2020a], is an NP-hard problem and computationally infeasible, especially for large-scale prob-
lems.
As demonstrated in Theorem 3.3, the convergence rate of nested multilayer perceptrons
consists of three components: (1) the statistical error, which takes different analytical forms
in (3.11) and (3.12) depending on the covariates’ dimension, (2) the sparse-approximation
error r2, and (3) the first-stage error e2. Under Case (a) above, the statistical error is
n n
n−β/(β+q)log8β/(β+q)n, which is faster than the usual minimax non-parametric estimation
rate n−2β/(2β+p) as long as q < p/2. In existing work, Farrell et al. [2021] considered the
degenerate single-stage learning problem, and their Theorem 1 provided a convergence rate
O (n−β/(β+p)log8n) for multilayer perceptrons when p is fixed. Comparing with their work,
p
apartfromthenestedstructurewehaveaddressedandaslightimprovementonthelogarithmic
terms, we provide a faster convergence rate as long as q < p.
It is essential to highlight that conducting non-parametric regressions in high dimensions
poses a significant challenge due to the “curse of dimensionality.” Nonetheless, we demon-
strate the consistency of multilayer perceptrons under growing dimension p → ∞ as n → ∞,
providedthatp = o(n/log5n). Ourfindingsoffertheoreticalinsightsintothepracticalsuccess
of DNNs in high dimensions. To the best of our knowledge, this is the first result providing
theoretical guarantees for multilayer perceptrons allowing for a diverging dimension without
assuming any sparse structure on the marginal distribution P , even in single-stage learning
X
problems. Instead, we consider sparse structures on the conditional distribution P , which
Y|X
are more suitable for the causal inference problems addressed in this work. Moreover, our
results do not rely on any type of beta-min conditions (i.e., the minimum non-zero signal is
large enough), which are essential for achieving variable selection consistency and are often
violated in practice.
We would like to emphasize that the result in Theorem 3.3 is also of independent interest
in that it provides a user-friendly general theory towards the convergence rate of multilayer
perceptronsbasedonnestedstructures. Thisresultisalsousefulwhileestablishingtheoretical
guarantees for the usage of DNNs in other contexts where sequential regression are needed,
such as reinforcement learning and the estimation of optimal dynamic treatment regimes.
Additionally, although we only considered the square loss above, the established results can
also be extended to other loss functions, such as the logistic loss.
4 Heterogenous treatment effect estimation using DNNs
Inthissection, weemployDNNstoestimatetheconditionalaveragetreatmenteffect(CATE)
introduced in Example 2.
144.1 The doubly robust DNN estimate of CATE
Let S = (Z )N = (S ,T ,Y )N be i.i.d. samples, and let Z be an independent copy of S with
i i=1 i i i i=1
S ∈ Rd, T ∈ {0,1}, and Y ∈ R representing the covariate vector, treatment indicator, and
the observed outcome, respectively. Using the potential outcome framework, let Y(t) be the
potential outcome an individual would have received when exposed to treatment t ∈ {0,1}.
Our goal is to estimate the CATE function θ (s) := E{Y(1)−Y(0) | S = s} for any given
CATE
s ∈ Rd. Estimating the CATE is valuable in the presence of heterogeneity and is crucial for
the development of precision medicine.
Denote π0(s) := P(T = 1 | S = s) as the propensity score function and µ0(t,s) := E(Y |
S = s,T = j) as the outcome regression function for each treatment group t ∈ {0,1}. Assume
the standard identification conditions [Imbens and Rubin, 2015, Tsiatis, 2006].
Assumption 1 (Identification conditions). Let the following conditions hold: (a) Ignorability:
{Y(1),Y(0)} ⊥⊥ T | S; (b) Consistency: Y = Y(T); (c) Positivity/Overlap: ∥1/π0∥ ,∥1/(1−
∞
π0)∥ ≤ M with some constant M > 0.
∞
Algorithm 1 The doubly robust DNN estimator for the CATE
Require: Observations S = (Z )N = (S ,T ,Y )N and a pair (L,H).
i i=1 i i i i=1
1: SplitSintotwoequal-sizedparts(S 1,S 2),indexedbyI 1 andI 2,respectively. Letn = N/2
be an integer for the sake of simplicity.
2: Construct nuisance estimates π (cid:98)(2)(s) = π (cid:98)(s,S 2) and µ (cid:98)(2)(t,s) = µ (cid:98)(t,s,S 2) for each t ∈
{0,1} based on training samples S using arbitrary machine learning methods.
2
3: For any i ∈ I 1, define the doubly robust outcomes
T {Y −µ(2)(1,S )} (1−T ){Y −µ(2)(0,S )}
Y(cid:98)i = µ (cid:98)(2)(1,S i)+ i i π(2(cid:98)
)(S )
i −µ (cid:98)(2)(0,S i)− i 1−i π(2)(cid:98)
(S )
i . (4.1)
(cid:98) i (cid:98) i
4: Construct a DNN estimate of the CATE based on training samples (X i,Y(cid:98)i) i∈I1:
(cid:88)(cid:110) (cid:111)2
θ(cid:98) C1
ATE
∈ arg min n−1 Y(cid:98)i−f(X i) .
f∈FMLP(L,H):∥f∥∞≤2M
i∈I1
5: Exchange subsamples S 1 and S 2, repeat Steps 2-4 above and obtain another estimate
θ(cid:98)2 .
CATE
6: return The doubly robust DNN estimator for the CATE, θ(cid:98) = (θ(cid:98)1 +θ(cid:98)2 )/2.
CATE CATE CATE
UnderAssumption1,theCATEcanbeidentifiedthroughthedoublyrobustrepresentation
(2.2). Letπandµbeestimatesofthenuisancefunctionsπ0 andµ,respectively. Here,weallow
(cid:98) (cid:98)
flexible nuisance estimation methods in the first learning stage, including, for example, DNNs
and Lasso estimators. Then, we employ the DNN in the second learning stage to estimate the
CATE, using the outcome variables (4.1) constructed based on the first-stage estimates and
a doubly robust score function. The detailed construction based on a cross-fitting technique
is introduced in Algorithm 1.
4.2 Theoretical properties
We assume the following conditions.
15Assumption 2 (Estimation errors). Let δ ≥ 0 be a sequence satisfying
N
(cid:104) (cid:105)
max E 1 (cid:8) µ(t,S)−µ0(t,S)(cid:9)2(cid:8) π(S)−π0(S)(cid:9)2 = O (δ4 ). (4.2)
Z T=t (cid:98) (cid:98) p N
t∈{0,1}
Assumption 3 (Boundedness). Let S ⊆ [−1,1]d1 and ∥Y∥
∞
≤ M. Additionally, let ∥µ (cid:98)∥ ∞,PZ,
∥1/π∥ , ∥1/(1−π)∥ = O (1).
(cid:98) ∞,PZ (cid:98) ∞,PZ p
Assumption 4(Smoothness). Assumethatθ (s)canbeapproximatedbyasmoothfunction
CATE
θ Q(s Q) : Rq θ (cid:55)→ R that ∥θ CATE(S)−θ Q(S Q)∥ ∞ ≤ r N, where s Q := (s j) j∈Q, S Q := (S j) j∈Q,
|Q| = q ≥ 1 is a constant, r = o(1), and µ lies in the H¨older ball with some constant
θ N Q
smoothness parameter β
θ
≥ 0 that µ
Q
∈ Wβ θ,∞([−1,1]q θ), (3.6).
Assumption 2 assumes a product rate condition on the estimation errors of nuisance func-
tions µ0 and π0, with the product rate δ appearing in the estimation error of the CATE
N
in Theorem 4.2. The boundedness conditions for S and Y in Assumption 3 are relatively
standard in the non-parametric regression literature; see, for instance, Farrell et al. [2021].
The uniform lower bounds on the propensity score estimates π and 1−π are also commonly
(cid:98) (cid:98)
seen in the double machine learning literature, such as Chernozhukov et al. [2017]. These
conditions, together with the uniform upper bound for µ, are easily satisfied, for example,
(cid:98)
by the DNNs as in Example 5 below. Assumption 4 is a generalization of the usual H¨older
smoothness condition; we only require µ , a sparse approximation of the CATE, to lie in
Q
the H¨older ball. A related but different condition has been considered by Fan et al. [2020],
Schmidt-Hieber [2020a], where the true conditional mean function is assumed to be a compo-
sition of exactly sparse smooth functions. While we require the sparsity level q > 0 to be a
constant independent of N for non-parametric regression, the total dimension d is allowed to
grow with the sample size.
In the following lemma, we first characterize the deviation between Y(cid:98) and Y#, which is
the error originated from the first learning stage.
Lemma 4.1. Let Assumptions 1, 2, and 3 hold. Let Y(cid:98) be an independent copy of (4.1) and
Y# is defined as in (2.2). Then,
Y(cid:98) −Y# = ∆ 1+∆ 2,
with some ∆ ,∆ ∈ R satisfying
1 2
E (∆ | S) = 0 almost surely and E (∆2) = O (δ4 ). (4.3)
Z 1 Z 2 p N
Lemma4.1establishesaconnectionbetweentheCATEestimationproblemandthenested
DNN results in Section 3.2, enabling the application of Theorem 3.3. According to Lemma
4.1, the first-stage error Y(cid:98) −Y# comprises two components: (a) a first-order estimation error
term ∆ , defined as the summation of (B.1)-(B.4) in the Supplementary Material, depending
1
linearly on the first-stage estimation errors; (b) a second-order term ∆ , defined as the sum-
2
mationof (B.5)-(B.6), dependingquadratically onthefirst-stageestimationerrors. Benefiting
from the doubly robust score (2.2) that satisfies the generalized Neyman orthogonality condi-
tion (1.3), the first-order error term has a conditional mean zero, as shown in (4.3). Hence, as
demonstrated in Theorem 3.3, the first-order error does not affect the convergence rate of the
doubly robust nested DNN estimation, and only the smaller second-order error contributes to
the final convergence rate. These results are summarized in the theorem below.
16Theorem 4.2. Let Assumptions 1, 2, 3 and 4 hold, d = o(N/log5N). Choose L and H as
in Theorem 3.3 with p = d, q = q , and β = β . Then, as N → ∞,
θ θ
(cid:20) (cid:21)
E
Z
(cid:110)
θ(cid:98) CATE(S)−θ
CATE(S)(cid:111)2
= O p(cid:0) w N,d(q θ,β θ)+r N2 +δ N4 (cid:1) , where (4.4)
− 2β 8β 2β 10β
w N,p(q,β) := N 2β+2q logβ+q N +(p/N)2β+q log2β+q N for any p,q,β ≥ 0. (4.5)
Remark 3 (Consistency and robustness). Recently, Foster and Syrgkanis [2023] proposed
a general framework for CATE estimation and briefly explored the utilization of DNNs
as an illustrative example. However, their approach necessitates the target function θ(x)
to precisely align with the class of DNNs, therefore reducing the problem to a paramet-
ric one, as the approximation error (or misspecification error) is assumed to be exactly
zero. Moreover, they presented a slower convergence rate involving an additional term
max E [1 {µ(t,S)−µ0(t,S)}4]+E [{π(S)−π0(S)}4], compared to our established
t∈{0,1} Z T=t (cid:98) Z (cid:98)
rate(4.4). Consequently, besidestheunrealisticassumptionofzeroapproximationerror, their
method requires consistent estimation of both nuisance models µ0 and π0 to attain a consis-
tent estimate for the CATE. In contrast, we only necessitate that the true CATE function
can be approximated by a sparse smooth function with r = o(1), and the product of the
N
first-stage error satisfies δ = o(1). The latter condition holds as long as either µ0 or π0 is
N
consistently estimated, not necessarily both – our results provide enhanced model robustness.
Toourknowledge,Theorem4.2marksthefirstconsistentresultforCATEthataccommodates
both a fully non-parametric model and a growing number of dimension, without relying on
any variable selection techniques.
4.3 Examples
In the following, we provide specific examples for the nuisance estimates (µ,π) and discuss
(cid:98) (cid:98)
the resulting convergence rates for CATE estimation. For the sake of simplicity, let θ be
CATE
exactly sparse, i.e., r defined in Assumption 4 is exactly zero.
N
Example 5 (First-stage learning through DNNs). Let both µ0(0,·) and µ0(1,·) be exactly
sparse functions with a sparsity level of q and H¨older smoothness level of β . Additionally,
µ µ
let π0 be an exactly sparse function with a sparsity level of q and H¨older smoothness level
π
of β . Consider DNN estimates: for each k ∈ {1,2},
π
(cid:88)
µ(k)(t,·) ∈ arg min n−1 1 {Y −f(S )}2 ∀t ∈ {0,1},
(cid:98) Ti=t i i
f∈FMLP(Lµ,t,Hµ,t):∥f∥∞≤2M
i∈Ik
(cid:88)
f(cid:98) π(k)(·) ∈ arg min n−1 (−T if(S i)+log[1+exp{f(S i)}]),
f∈FMLP(Lπ,Hπ):∥f∥∞≤2M
i∈Ik
and π (cid:98)(k) = ϕ(f(cid:98)π(k) ), with ϕ(u) = exp(u)/{1 + exp(u)} for any u ∈ R denoting the logistic
function. By Theorem 3.3, when the tuning parameters are chosen appropriately,
(cid:104) (cid:105)
max E 1 {µ(k)(t,S)−µ0(t,S)}2 = O (w (q ,β )).
T=t (cid:98) p N,d µ µ
t∈{0,1}
Additionally, repeating the proof of Theorem 3.3 using the logistic loss, we also have
(cid:104) (cid:105)
E {π(k)(S)−π0(S)}2 = O (w (q ,β )).
(cid:98) p N,d π π
17By construction, ∥µ(k)−µ0∥ ,∥π(k)−π0∥ = O(1). Therefore, the product term δ , (4.2),
(cid:98) ∞ (cid:98) ∞ N
can be chosen as δ4 = O(w (q ,β )∧w (q ,β )), leading to the following result for the
N N,d µ µ N,d π π
CATE estimation:
(cid:20) (cid:21)
(cid:110) (cid:111)2
E
Z
θ(cid:98) CATE(S)−θ CATE(S) = O p(w N,d(q θ,β θ)+w N,d(q µ,β µ)∧w N,d(q π,β π)). (4.6)
Therefore, even if both outcome regression models µ0(0,·) and µ0(1,·) are relatively complex
(e.g.,denseandnon-smooth),wecanstillobtainaconsistentestimatefortheCATEaslongas
the CATE itself and the propensity score are sparse and smooth. On the other hand, directly
estimating the CATE as the difference between µ(1,·) and µ(0,·) is generally inconsistent.
(cid:98) (cid:98)
This observation underscores the enhanced robustness and accuracy achieved through the use
of doubly robust regression in the second learning stage.
Example 6 (First-stage learning through Lasso). Consider Lasso and logistic Lasso estimates.
For any s ∈ Rd, k ∈ {1,2}, and t ∈ {0,1}, let µ (cid:98)(k)(t,s) = s⊤β(cid:98) t(k) and π (cid:98)(k)(s) = ϕ(s⊤β(cid:98)π(k) ),
where
β(cid:98) t(k) ∈ arg βm ∈i Rn dn−1 (cid:88) 1 Ti=t(cid:16) Y i−S⊤
i
β(cid:17)2 +λ t∥β∥
1
∀t ∈ {0,1},
i∈Ik
(cid:88) (cid:104) (cid:105)
β(cid:98) π(k) ∈ arg min n−1 −T iS⊤
i
β+log{1+exp(S⊤
i
β)} +λ π∥β∥ 1,
β∈Rd
i∈Ik
with tuning parameters λ ,λ ≥ 0. Let β∗, β∗, and β∗ be the best population linear/logistic
t π 0 1 π
slopes, and suppose that ∥β∗∥ ≤ s for each t ∈ {0,1} and ∥β∗∥ ≤ s , where ∥β∥ = |{j ≤
t 0 µ π 0 π 0
d : β ̸= 0}| for any β ∈ Rd. Additionally, assume that the linear and logistic approximation
j
errors satisfy E[1 {µ0(t,S)−S⊤β∗}2] ≤ e2 and E[{π0(S)−ϕ(S⊤β∗)}2] ≤ e2. Then, by
T=t t µ π π
standard high-dimensional statistics theory [Wainwright, 2019], we have
(cid:18) (cid:19)
(cid:104) (cid:105) s logd
max E 1 {µ(k)(t,S)−µ0(t,S)}2 = O µ +e2 ,
t∈{0,1} T=t (cid:98) p N µ
(cid:18) (cid:19)
(cid:104) (cid:105) s logd
E {π(k)(S)−π0(S)}2 = O π +e2 .
(cid:98) p N π
For sub-Gaussian covariates, we can choose the product term δ , (4.2), as δ4 = (s logd/N+
N N µ
e2)(s logd/N +e2). Therefore,
µ π π
(cid:20) (cid:21) (cid:18) (cid:18) (cid:19)(cid:18) (cid:19)(cid:19)
(cid:110) (cid:111)2 s logd s logd
E
Z
θ(cid:98) CATE(S)−θ CATE(S) = O
p
w N,d(q θ,β θ)+ µ
N
+e2
µ
π
N
+e2
π
.
Consistency of the CATE estimate requires either e = o(1) or e = o(1), not necessarily
µ π
both. When one of the nuisance models is correctly parametrized, i.e., e = 0 or e = 0, and
µ π
the other one is dense and non-smooth, the convergence rate above is faster than (or at least
the same as) the CATE estimator using DNN nuisance estimates, which has a convergence
rate (4.6). However, the approach in Example 5 is more robust as it does not require any of
the parametric working models to well approximate the true models.
185 Dynamic treatment effect estimation using DNNs
In this section, we consider the estimation of the dynamic treatment effect (DTE) introduced
in Example 3.
5.1 The sequential doubly robust estimate of DTE using DNNs
Algorithm 2 The doubly robust DNN estimator for µ0
Require: Observations S′ := (Z ) = (S ,T ,S ,T ,Y ) , the treatment path of in-
i i∈I′ 1i 1i 2i 2i i i∈I′
terest t = (t ,t ) = (1,1), the training indices I′ ⊆ {1,...,N}, and a pair (L,H).
1 2
1: Split S′ into 2 equal-sized parts (S′,S′), indexed by (I′,I′), with n := |I′|/2 assuming to
1 2 1 2
be an integer.
2: Construct ρ (cid:98)(2) using the subset of S′ 2 with the treatment path t 1 = 1. ▷ Propensity for
time two
3: Construct ν (cid:98)(2) using the subset of S′
2
with the treatment path t = (1,1). ▷ Outcome for
time two
4: Construct the doubly robust outcomes for the samples S′:
1
T {Y (1,1)−ν(2)(S )}
Y(cid:98)i := ν (cid:98)(2)(S 2i)+ 2i i (cid:98) 2i . (5.1)
ρ(2)(S )
(cid:98) 2i
5: Construct a DNN estimate of the CATE based on training samples (X i,Y(cid:98)i) i∈I′:
1
(cid:88) (cid:110) (cid:111)2
µ (cid:98)1 ∈ arg min n−1 T
1i
Y(cid:98)i−f(S 1i) .
f∈FMLP(L,H):∥f∥∞≤2M
i∈I′
1
6: Exchange subsamples S′
1
and S′ 2, repeat Steps 2-4 above and obtain another estimate µ (cid:98)2.
7: return The doubly robust DNN estimator for µ0, µ (cid:98)= (µ (cid:98)1+µ (cid:98)2)/2.
Let S = (Z )N = (S ,T ,S ,T ,Y )N be i.i.d. samples, and let Z = (S ,T ,S ,T ,Y)
i i=1 1i 1i 2i 2i i i=1 1 1 2 2
be an independent copy of S with S
t
∈ Rdt and T
t
∈ {0,1} representing the covariate vector
and treatment indicator at the t-th exposure, respectively, for each t ∈ {1,2}. Define S :=
2
(S⊤,S⊤)⊤ and d¯:= d +d . Consider the potential outcome framework, and let Y(t) denote
1 2 1 2
the potential outcome an individual would have received when exposed to treatment path
t = (t ,t ) ∈ {0,1}2, while the observed outcome is Y ∈ R. The dynamic treatment effect
1 2
(DTE) between two treatment paths a,a′ ∈ {0,1}2 is defined as θ := E{Y(a)−Y(a′)},
DTE
which evaluates the average difference in the treatment effect between two paths over the
entire population. In the following, we focus on the estimation of θ := E{Y(1,1)} since the
same procedure can be extended to the estimation of the counterfactual mean E{Y(t)} for
any t ∈ {0,1}2, and the DTE can be estimated as the difference between two counterfactual
means.
We first define the relevant nuisance functions necessary to identify the parameter of
interest. At the first exposure, denote µ0(s ) := E{Y(1,1) | S = s ,T = 1} and π0(s ) :=
1 1 1 1 1
P(T
1
= 1 | S
1
= s 1) for any s ∈ Rd1 as the outcome regression and propensity score functions,
respectively. Similarly, at the second exposure, define ν0(s ) := E{Y(1,1) | S = s ,T =
2 2 2 1
19T = 1} and ρ0(s ) := P(T = 1 | S = s ,T = 1) for any s ∈ Rd¯ . Assume the following
2 2 2 2 2 1 2
identification conditions, which are standard in the dynamic causal inference literature; see,
e.g., Murphy [2003], Robins [2000].
Assumption 5 (Identification conditions). Let the following conditions hold: (a) Sequential
ignorability: Y(1,1) ⊥⊥ T | S and Y(1,1) ⊥⊥ T | (S ,T = 1); (b) Consistency: Y =
1 1 2 2 1
Y(T ,T ); (c) Positivity/Overlap: ∥1/π0∥ ,∥1/ρ0∥ ≤ M with some constant M > 0.
1 2 ∞ ∞
UnderAssumption5,θ = E{Y(1,1)}canbeidentifiedthroughthedoublyrobustrepresen-
tation(2.3). Additionally,notethatthepropensityscorefunctionsπ0 andρ0 aredirectlyiden-
tifiable by observable variables based on their definitions. Furthermore, we can identify the
outcome regression function at the second exposure as ν0(s ) = E(Y | S = s ,T = T = 1)
2 2 2 1 2
under Assumption 5. Therefore, the nuisance functions π, ρ, and ν can be directly estimated
using the observed samples. Let π, ρ, and ν be the estimates at the first learning stage.
(cid:98) (cid:98) (cid:98)
Similar to Section 4, we also allow for flexible choices for such nuisance estimates.
Algorithm 3 The sequential double machine learning estimator for θ
Require: Observations S = (Z )N = (S ,T ,S ,T ,Y )N and the treatment path of
i i=1 1i 1i 2i 2i i i=1
interest t = (t ,t ) = (1,1).
1 2
1: ForanyfixedintegerK ≥ 2, splitthesampleSintoK equal-sizedparts(S k)K
k=1
randomly,
indexed by I . Define S := (Z ) .
k −k i i∈I\I
k
2: for k ∈ {1,··· ,K} do
3: Let I be a subset of indices of I −k with the treatment path t = (1,1).
4: Let I 1 be a subset of indices of I −k with the treatment path t 1 = 1.
5: Construct π (cid:98)−k using I −k samples. ▷ Propensity for time one
6: Construct ρ (cid:98)−k using I 1 samples. ▷ Propensity for time two
7: Construct ν (cid:98)−k using I samples. ▷ Outcome for time two
8: Construct µ (cid:98)−k as in Algorithm 2 using I 1 samples. ▷ Outcome for time one
9: Construct θ(cid:98)(k) = |I k|−1(cid:80) i∈I ψ(Z i;η (cid:98)−k), where η (cid:98)−k = (ρ (cid:98)−k,ν (cid:98)−k,π (cid:98)−k,µ (cid:98)−k) and
k
T {ν(S )−µ(S )} T T {Y −ν(S )}
1 2 1 1 2 2
ψ(Z;η) := µ(S )+ + for any η = (ρ,ν,π,µ).
1
π(S ) π(S )ρ(S )
1 1 2
10: end for
11: return The sequential double machine learning estimator for θ, θ(cid:98)= K−1(cid:80)K θ(cid:98)(k).
k=1
The remaining nuisance function µ is not directly identifiable by observable variables, as
the potential outcome Y (1,1) is unobservable among the group T = 1. We consider an
i 1i
additional doubly robust representation for µ0, (2.4), and employ the DNN using doubly
robust outcomes (5.1) at the second learning stage. Lastly, we perform another doubly robust
estimation based on the representation (2.3) at the third learning stage. The procedure is
implemented using a cross-fitting technique; see details in Algorithms 2 and 3.
5.2 Theoretical properties
In this section, we initially examine the theoretical properties of the proposed DNN estimate
for µ0 using the results from Section 3. Subsequently, we present general theories and char-
20acterize the asymptotic behavior of the estimator for θ. To establish the desired results, we
first assume the following conditions.
Assumption 6 (Estimation errors). Let a ,c ,d ,δ ≥ 0 be sequences satisfying
N N N N
(cid:104) (cid:105)
E T T (cid:8) ν(S )−ν0(S )(cid:9)2 = O (a2 ), (5.2)
Z 1 2 (cid:98) 2 2 p N
E (cid:2) {π(S )−π0(S )}2(cid:3) = O (c2 ), (5.3)
Z (cid:98) 1 1 p N
(cid:104) (cid:105)
E T (cid:8) ρ(S )−ρ0(S )(cid:9)2 = O (d2 ), (5.4)
Z 1 (cid:98) 2 2 p N
(cid:104) (cid:105)
E T T (cid:8) ν(S )−ν0(S )(cid:9)2(cid:8) ρ(S )−ρ0(S )(cid:9)2 = O (δ4 ). (5.5)
Z 1 2 (cid:98) 2 2 (cid:98) 2 2 p N
Assumption 7(Boundedness). LetS
1
⊆ [−1,1]d1 and∥Y∥
∞
≤ M. Additionally,let∥ν (cid:98)∥
∞,PZ
=
O (1), ∥1/ρ∥ = O (1) and ∥1/π∥ = O (1).
p (cid:98) ∞,PZ p (cid:98) ∞,PZ p
Assumption 8 (Smoothness). Assume that µ0(s ) can be approximated by a smooth function
1
µ Q(s 1,Q) : Rqµ (cid:55)→ R that ∥µ0(S 1) − µ Q(S 1,Q)∥
∞
≤ r N, where s
1,Q
:= (s 1j) j∈Q, S
1,Q
:=
(S ) , |Q| = q ≥ 1 is a constant, r = o(1), and µ lies in the H¨older ball with some
1j j∈Q µ N Q
constant smoothness parameter β
µ
≥ 0 that µ
Q
∈ Wβµ,∞([−1,1]qµ), (3.6).
Assumptions 6-8 are analogous to Assumptions 2-4 under the dynamic setup. In Assump-
tion 6, in addition to the product rate condition (5.5), which has an analogous version in
Assumption 2, we also denote the convergence rates of the nuisance estimates ν, π, and ρ as
(cid:98) (cid:98) (cid:98)
a , c , and d , respectively. Note that we do not require a ,c ,d = o(1); that is, the
N N N N N N
nuisance models are possibly misspecified.
The following lemma characterizes the error from the doubly robust outcomes when esti-
mating µ0 in the second learning stage.
Lemma 5.1. Let Assumptions 5, 6, and 7 hold. Let Y(cid:98) be an independent copy of (5.1) and
Y# is defined as in (2.4). Then,
Y(cid:98) −Y# = ∆ 1+∆ 2,
with some ∆ ,∆ ∈ R satisfying
1 2
E (∆ | S ,T = 1) = 0 almost surely and E (∆2 | T = 1) = O (δ4 ).
Z 1 1 1 Z 2 1 p N
Similar to Lemma 4.1, the first-order estimation error ∆ has a conditional mean of zero
1
and therefore does not contribute to the convergence rate of µ; only the smaller second-order
(cid:98)
error ∆ contributes. These results are characterized in the theorem below, leveraging the
2
general nested DNN results in Theorem 3.3.
Theorem 5.2. Let Assumptions 5, 6, 7, and 8 hold, d = o(n/log5n). Choose L and H as
1
in Theorem 3.3 with p = d , q = q , and β = β . Then, as N → ∞,
1 µ µ
E (cid:2) T {µ(S )−µ0(S )}2(cid:3) = O (cid:0)¯b2 +δ4 (cid:1) and ¯b2 := w (q ,β )+r2 , (5.6)
Z 1 (cid:98) 1 1 p N N N N,d1 µ µ N
where the function w is defined in (4.5).
N,p
21According to Theorem 5.2, µ is consistent as long as ¯b = o(1) and δ = o(1). The
(cid:98) N N
former holds when µ0 can be well approximated by a sparse and smooth function, and the
latter occurs as long as either ν0 or ρ0 can be consistently estimated.
The following theorem establishes the consistency and asymptotic normality of the final
sequential double machine learning estimator θ(cid:98).
Theorem 5.3. Let Assumptions 5, 6, 7, and 8 hold, d = o(N/log5N). Choose L and H as
1
in Theorem 3.3 with p = d , q = q , and β = β . Define ¯b as in (5.6). Then, as N → ∞,
1 µ µ N
(cid:16) (cid:17)
θ(cid:98)−θ = O
p
a Nd
N
+¯b Nc
N
+δ N2 c
N
+N−1/2 . (5.7)
Additionally, assume a ,c ,d ,r ,δ = o(1),
N N N N N
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
a d = o N−1/2 , ¯b c = o N−1/2 , and δ2 c = o N−1/2 . (5.8)
N N N N N N
Define σ (cid:98)2 := N−1(cid:80)K k=1(cid:80)
i∈I
{ψ(Z i;η (cid:98)−k)−θ(cid:98)}2 and let σ2 := Var{ψ(Z;η0)} > c with some
k
constant c > 0. Then, as N → ∞, in distribution,
√
Nσ−1(θ(cid:98)−θ) → N(0,1) and σ (cid:98)2 = σ2{1+o p(1)}. (5.9)
Remark 4 (Consistency and asymptotic normality). As far as we are aware, Theorem 5.3
also represents the first consistent and asymptotically normal result for DTE estimation that
allows for non-parametric models with a growing number of dimension.
We first discuss when the DTE estimator is consistent. Under Assumption 7, we can
chooseδ N2 = O(a N∧d N). Therefore, togetherwith(5.7), θ(cid:98)isconsistentwhenallthefollowing
conditionshold: (a)a = o(1)ord = o(1),i.e.,eitherν0 orρ0 canbeconsistentlyestimated,
N N
(b) ¯b = o(1) or c = o(1), i.e., either µ0 can be consistently estimated under an oracle case
N N
with observable Y# or π0 can be consistently estimated.
As for asymptotic normality, we require the product rate conditions a d +¯b c =
N N N N
o(N−1/2). Since the estimation of µ0 is based on doubly robust outcomes (5.1), which involve
nuisance estimates ν and ρ, its estimation error also depends on the estimation error of such
(cid:98) (cid:98)
nuisancefunctions,asdemonstratedinTheorem5.2. Asaresult,wealsorequireanadditional
product rate condition δ2 c = o(N−1/2), where δ2 itself, defined in (5.5), is also a product
N N N
rate capturing the estimation error at the first learning stage. As δ2 = O(a ∧ d ), a
N N N
sufficient condition for the last product rate condition to hold is either a c = o(N−1/2) or
N N
c d = o(N−1/2). When a tighter upper bound is obtained for δ , such a requirement can
N N N
be further relaxed, as seen in Example 8 below.
5.3 Examples
In the following, we provide specific examples for the nuisance estimates (π,ρ,ν) and discuss
(cid:98) (cid:98) (cid:98)
the estimation and inference results for the DTE. Similar to Section 4.3, assume that µ0 is
exactly sparse for the sake of simplicity, i.e., r defined in Assumption 8 is exactly zero.
N
Example 7 (First-stagelearningthroughDNNs). ConsiderDNNestimatesatthefirstlearning
stage. Let π0, ρ0, and ν0 be exactly sparse functions with sparsity levels of q , q , and q and
π ρ ν
22H¨older smoothness levels of β , β , and β , respectively. Construct the nuisance estimates
π ρ ν
π , ρ , and ν in Steps 5-7 of Algorithm 3 as follows:
(cid:98)−k (cid:98)−k (cid:98)−k
(cid:88)
f(cid:98) −π
k
∈ arg min |I −k|−1 (−T 1if(S 1i)+log[1+exp{f(S 1i)}]),
f∈FMLP(Lπ,Hπ):∥f∥∞≤2M
i∈I
−k
f(cid:98) −ρ
k
∈ arg min |I −k|−1 (cid:88) T 1i(cid:0) −T 2if(S 2i)+log[1+exp{f(S 2i)}](cid:1) ,
f∈FMLP(Lρ,Hρ):∥f∥∞≤2M
i∈I
−k
ν ∈ arg min n−1 (cid:88) T T (cid:8) Y −f(S )(cid:9)2 ,
(cid:98)−k 1i 2i i 2i
f∈FMLP(Lν,Hν):∥f∥∞≤2M
i∈Ik
with π
(cid:98)−k
= ϕ(f(cid:98) −π k), ρ
(cid:98)−k
= ϕ(f(cid:98) −ρ k), and ϕ(u) = exp(u)/{1+exp(u)} for any u ∈ R. Construct
nuisance estimates ρ(2) and ν(2) of Algorithm 2 analogously using different training samples.
(cid:98) (cid:98)
Similar to Example 5, the convergence rates (5.2)-(5.4) can be chosen as a2 = w (q ,β ),
N N,d¯ ν ν
c2 = w (q ,β ), and d2 = w (q ,β ), where the function w is defined in (4.5). Since
N N,d1 π π N N,d¯ ρ ρ N,p
δ2 = O(a ∧d ), (5.7) leads to a consistency rate
N N N
(cid:16) (cid:17)
θ(cid:98)−θ = O
p
N−1/2+w N1/ ,2 d¯(q ν,β ν)w N1/ ,2 d¯(q ρ,β ρ)
(cid:16)(cid:110) (cid:111) (cid:17)
1/2 1/2 1/2 1/2
+O w (q ,β )+w (q ,β )∧w (q ,β ) w (q ,β ) .
p N,d1 µ µ N,d¯ ν ν N,d¯ ρ ρ N,d1 π π
The asymptotic normality (5.9) requires a d +w1/2 (q ,β )c +δ2 c = o(N−1/2). For
N N N,d1 µ µ N N N
the sake of simplicity, consider the case with d 1 ≪ N2βµq +µ 2qµ ∧N2βπq +π 2qπ and d¯≪ N2βνq +ν 2qν ∧
qρ
N2βρ+2qρ. Then, (5.9) holds when (a) β µβ
π
> q µq π, (b) β νβ
ρ
> q νq ρ, and (c) β νβ
π
> q νq π.
Example 8 (First-stage learning through Lasso and DNNs). In the following, we explore
scenarios where a significant number of covariates are collected prior to the second exposure,
specifically focusing on cases where d¯= d +d ≫ N ≫ d . In such instances, where the total
1 2 1
dimension substantially exceeds the sample size, DNNs become less suitable for estimating
the nuisance functions associated with the second exposure, namely ν0 and ρ0. Instead, we
consider linear and logistic regression with ℓ -regularization. For any s ∈
Rd¯
and k ≤ K, let
1 2
ν (cid:98)−k(s 2) = s⊤ 2β(cid:98) ν−k and ρ (cid:98)−k(s 2) = ϕ(s⊤ 2β(cid:98) ρ−k), where
β(cid:98) ν−k ∈ arg min n−1 (cid:88) T 1iT 2i(cid:16) Y i−S⊤ 2iβ(cid:17)2 +λ ν∥β∥ 1,
β∈Rd
i∈I
−k
β(cid:98) ρ−k ∈ arg min n−1 (cid:88) T 1i(cid:104) −T 2iS⊤ 2iβ+log{1+exp(S⊤ 2iβ)}(cid:105) +λ ρ∥β∥ 1,
β∈Rd
i∈I
−k
with tuning parameters λ ,λ ≥ 0. Let β∗ and β∗ represent the best population lin-
ν ρ ν ρ
ear/logisticslopes, withsparsitylevels∥β∗∥ ≤ s and∥β∗∥ ≤ s , respectively. Additionally,
ν 0 ν ρ 0 ρ
let the linear and logistic approximation errors satisfy E[T T {ν0(S )−S⊤ β∗}2] ≤ e2 and
1 2 2 2 ν ν
E[T {ρ0(S )−ϕ(S⊤ β∗)}2] ≤ e2. Then, we can set a , d , and δ , defined in (5.2), (5.4),
1 2 2 ρ ρ N N N
and (5.5), as follows: a2 = s logd¯/N +e2, d2 = s logd¯/N +e2, and δ2 = a d .
N ν ν N ρ ν N N N
Considering that the dimension of covariates at the first exposure d is relatively small
1
compared to N, we employ DNNs to estimate π0 (as well as µ0 in the second learning stage).
As discussed in Example 7, if π0 is sparse and a smooth function with sparsity level q and
π
23H¨older smoothness level β , we can set c , defined in (5.3), as c2 = w (q ,β ). According
π N N N,d1 π π
to (5.7), we have the following rate of estimation for the final parameter of interest:
(cid:32)(cid:32)(cid:114) (cid:33)(cid:32)(cid:114) (cid:33)(cid:33)
s logd¯ s logd¯
ν ρ
θ(cid:98)−θ = O
p
+e
ν
+e
ρ
N N
(cid:16) (cid:17)
+O w1/2 (q ,β )w1/2 (q ,β )+N−1/2 .
p N,d1 µ µ N,d1 π π
Additionally, let e = e = 0, then the asymptotic normality (5.9) is ensured when s s =
ν ρ ν ρ
o(N/log2d¯) and w (q ,β )w (q ,β ) = o(N−1). The latter condition holds, for in-
N,d1 µ µ N,d1 π π
qµ qπ qµ qπ
stance, when (a) d
1
≪ N2βµ+2qµ ∧N2βπ+2qπ and β µβ
π
> q µq π, or (b) N2βµ+2qµ ∨N2βπ+2qπ ≪
4βµβπ−qµqπ
−c
d
1
≪ N8βµβπ+2βµqπ+2βπqµ with any constant c > 0.
6 Discusssion
Traditional non-parametric regression methods have demonstrated effectiveness in addressing
low-dimensional problems, offering minimax optimal convergence rates under smooth func-
tional classes. However, they encounter significant challenges when confronted with large
covariate dimensions p, known as the “curse of dimensionality.” While deep neural networks
(DNNs) are frequently employed in such scenarios, their empirical success often lacks theo-
retical assurance. This research aims to bridge this gap by providing convergence rates for
DNNs as p grows with the sample size, assuming a sparse structure on the conditional mean
function. Unliketraditionalnon-parametricmethods, DNNshavethecapabilitytoadaptively
learnthesparsestructureoftheregressionfunctionbylearningrepresentationsfromthedata.
This approach shows promise for addressing the challenges posed by high-dimensional data,
particularly for causal inference problems where a large number of covariates are typically
collected to reduce confounding bias in observational studies.
We explore the application of DNNs in intricate causal inference problems necessitating
multi-stage learning. While our theoretical framework is broad, we specifically demonstrate
its effectiveness in estimating the conditional average treatment effect (CATE) and dynamic
treatment effect (DTE), representing two- and three-stage learning problems, respectively.
These results extend to general multi-stage learning problems, even when more than three
learning stages are involved, albeit necessitating additional, similarly detailed proofs.
Our theoretical contributions primarily center on user-friendly multilayer perceptrons,
known for their straightforward structures and minimal tuning demands. Future research
avenues may explore more sophisticated network architectures to further enhance overall
estimation accuracy while maintaining manageable computational complexity.
References
Susan Athey, Guido W Imbens, and Stefan Wager. Approximate residual balancing: debiased
inference of average treatment effects in high dimensions. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 80(4):597–623, 2018.
24Vahe Avagyan and Stijn Vansteelandt. High-dimensional inference for the average treatment
effect under model misspecification using penalized bias-reduced double-robust estimation.
Biostatistics & Epidemiology, pages 1–18, 2021.
Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities.
The Annals of Statistics, 33(4):1497–1537, 2005.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-
dimension and pseudodimension bounds for piecewise linear neural networks. The Journal
of Machine Learning Research, 20(1):2285–2301, 2019.
Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimen-
sionality in nonparametric regression. The Annals of Statistics, 47(4):2261, 2019.
Jelena Bradic, Stefan Wager, and Yinchu Zhu. Sparsity double robust inference of average
treatment effects. arXiv preprint arXiv:1905.00744, 2019.
Jelena Bradic, Weijie Ji, and Yuqian Zhang. High-dimensional inference for dynamic treat-
ment effects. The Annals of Statistics, 52(2):415–440, 2024.
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on
low-dimensionalmanifoldsusingdeeprelunetworks: Functionapproximationandstatistical
recovery. Information and Inference: A Journal of the IMA, 11(4):1203–1253, 2022.
VictorChernozhukov, DenisChetverikov, MertDemirer, EstherDuflo, ChristianHansen, and
WhitneyNewey. Double/debiased/neymanmachinelearningoftreatmenteffects. American
Economic Review, 107(5):261–265, 2017.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of
Control, Signals and Systems, 2(4):303–314, 1989.
Iv´an D´ıaz, Nicholas Williams, Katherine L Hoffman, and Edward J Schenck. Nonparametric
causal effects based on longitudinal modified treatment policies. Journal of the American
Statistical Association, 118(542):846–857, 2023.
UtkuEvci, FabianPedregosa, AidanGomez, andErichElsen. Thedifficultyoftrainingsparse
neural networks. arXiv preprint arXiv:1906.10732, 2019.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep
q-learning. In Learning for Dynamics and Control, pages 486–489. PMLR, 2020.
Max H Farrell. Robust inference on average treatment effects with possibly more covariates
than observations. Journal of Econometrics, 189(1):1–23, 2015.
Max H Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and
inference. Econometrica, 89(1):181–213, 2021.
DylanJFosterandVasilisSyrgkanis. Orthogonalstatisticallearning. TheAnnalsofStatistics,
51(3):879–908, 2023.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Net-
works, 4(2):251–257, 1991.
25Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural Networks, 2(5):359–366, 1989.
Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical
sciences. Cambridge University Press, 2015.
Yuling Jiao, Guohao Shen, Yuanyuan Lin, and Jian Huang. Deep nonparametric regression
on approximate manifolds: Nonasymptotic error bounds with polynomial prefactors. The
Annals of Statistics, 51(2):691–716, 2023.
Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal
effects. Electronic Journal of Statistics, 17(2):3008–3049, 2023.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
Michael Kohler and Sophie Langer. On the rate of convergence of fully connected deep neural
network regression estimates. The Annals of Statistics, 49(4):2231–2249, 2021.
Michael Kohler, Adam Krzyz˙ak, and Sophie Langer. Estimation of a function of low local
dimensionality by deep neural networks. IEEE transactions on information theory, 68(6):
4032–4042, 2022.
Hao Liu, Minshuo Chen, Tuo Zhao, and Wenjing Liao. Besov function approximation and
binary classification on low-dimensional manifolds using convolutional residual networks.
In International Conference on Machine Learning, pages 6770–6780. PMLR, 2021.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the
value of network pruning. arXiv preprint arXiv:1810.05270, 2018.
Alexander R Luedtke, Oleg Sofrygin, Mark J van der Laan, and Marco Carone. Sequential
double robustness in right-censored longitudinal models. arXiv preprint arXiv:1705.02459,
2017.
Susan A Murphy. Optimal dynamic treatment regimes. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 65(2):331–355, 2003.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann ma-
chines. In Icml, 2010.
Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and generalization of deep
neural network with intrinsic dimensionality. The Journal of Machine Learning Research,
21(174):1–38, 2020.
James M Robins. Marginal structural models versus structural nested models as tools for
causal inference. In Statistical Models in Epidemiology, the Environment, and Clinical
Trials, pages 95–133. Springer, 2000.
Andrea Rotnitzky, James Robins, and Lucia Babino. On the multiply robust estimation of
the mean of the g-functional. arXiv preprint arXiv:1705.08582, 2017.
26JohannesSchmidt-Hieber. Deeprelunetworkapproximationoffunctionsonamanifold. arXiv
preprint arXiv:1908.00695, 2019.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu
activation function. The Annals of Statistics, 48(4):1875–1897, 2020a.
Johannes Schmidt-Hieber. Rejoinder:“nonparametric regression using deep neural networks
with relu activation function”. The Annals of Statistics, 48(4):1916–1921, 2020b.
Ezequiel Smucler, Andrea Rotnitzky, and James M Robins. A unifying approach for doubly-
robustl regularizedestimationofcausalcontrasts. arXiv preprint arXiv:1904.03737, 2019.
1
Zhiqiang Tan. Model-assisted inference for treatment effects using regularized calibrated
estimation with high-dimensional data. Annals of Statistics, 48(2):811–837, 2020.
Anastasios A Tsiatis. Semiparametric theory and missing data. 2006.
MarkJVanderLaan. Statisticalinferenceforvariableimportance. The International Journal
of Biostatistics, 2(1), 2006.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
Cambridge University Press, 2019.
DmitryYarotsky. Errorboundsforapproximationswithdeeprelunetworks. Neural Networks,
94:103–114, 2017.
Yuqian Zhang, Jelena Bradic, and Weijie Ji. Dynamic treatment effects: high-dimensional
inference under model misspecification. arXiv preprint arXiv:2111.06818, 2021.
27SUPPLEMENT TO “CAUSAL INFERENCE THROUGH MULTI-STAGE
LEARNING AND DOUBLY ROBUST DEEP NEURAL NETWORKS”
This supplementary document provides the proofs for the theoretical results presented in
themaindocument,includingTheorems3.1-5.3,alongwithauxiliarylemmasandtheirproofs.
All results and notation are consistent with those in the main text unless stated otherwise.
To simplify the exposition, we begin by listing some shorthand notations used throughout
the supplementary document. For any sequence f , define E (f ) :=
n−1(cid:80)n
f . For any
i n i i=1 i
functional class F, define R F := sup
n−1(cid:80)n
δ f(Z ), where δ ∈ {−1,1} with equal
n f∈F i=1 i i i
probability 1/2. We denote E (R F) as the empirical Rademacher complexity, where the
δ n
expecation is taken conditional on (Z )n ; the Rademacher complexity, E(R F), is defined
i i=1 n
through taking the expectation over both (Z )n and (δ )n .
i i=1 i i=1
A Proof of the results in Section 3.2
A.1 Auxiliary lemmas
Define ∆ := ∆ (Z ;S2), ∆ := ∆ (Z ;S2) and
1i 1 i 2i 2 i
f¯∈ arg min ∥f −f0∥ . (A.1)
∞
f∈FDNN:∥f∥∞≤2M
Lemma S.1. Let the assumptions in Theorem 3.1 hold. Then,
(cid:104) (cid:105)
E
n
R i{f(cid:98)(X i)−f0(X i)}2
(cid:114)
(cid:104) (cid:105)
≤ E n(cid:2) R i{f0(X i)−f¯(X i)}2(cid:3) +2 E n(R i∆2 2i)E
n
R i{f(cid:98)(X i)−f¯(X i)}2
(cid:104) (cid:105)
+2(E n−E Z) R i{f(cid:98)(X i)−f¯(X i)}{Y i#−f0(X i)+∆ 1i} (A.2)
and
(cid:104) (cid:105)
E
X
R{f(cid:98)(X)−f0(X)}2 ≤ E(cid:2) R{f0(X)−f¯(X)}2(cid:3) +W 1+W 2,
where
(cid:104) (cid:105)
W
1
:= (E n−E Z) R i{f(cid:98)(X i)−f¯(X i)}{2Y i#−f(cid:98)(X i)−f¯(X i)+2∆ 1i} , (A.3)
(cid:114)
(cid:104) (cid:105)
W
2
:= 2 E n(R i∆2 2i)E
n
R i{f(cid:98)(X i)−f¯(X i)}2 . (A.4)
Lemma S.2. For any t > 0, consider any r > 0 that satisfies
C1 : r2 ≥ 24ME(R F ), (A.5)
n r
32M2t
C2 : r2 ≥ , (A.6)
n
where
F := {g = R(f −f¯) : f ∈ F ,∥Rf∥ ≤ 2M,E{g2(X)} ≤ r2}. (A.7)
r DNN ∞
28Then, with probability at least 1−e−t,
sup E {g2(X )} ≤ 4r2. (A.8)
n i
g∈Fr
Lemma S.3. Let the assumptions in Theorem 3.1 hold. Then, for any t > 0, there exists
some M ≥ M ≥ 0 such that
t
P(Ec) < t−1, where E = E (t) := {∥R∆ ∥ ≤ M }; (A.9)
1 1 1 1 ∞,PZ t
P(Ec) < t−1, where E = E (t) := (cid:8) E (R ∆2 ) ≤ M2e2(cid:9) . (A.10)
2 2 2 n i 2i t n
Lemma S.4. Let the assumptions in Theorem 3.1 hold and n > max{(2eM)2,CWLlogW},
where C > 0 is some constant. Let r = r > 0 satisfies r ≥ 1/n, Conditions C1-C2 defined
0 0
in (A.5)-(A.6), as well as
(cid:104) (cid:105)
C3 : r2 ≥ E
X
R i{f(cid:98)(X)−f¯(X)}2 . (A.11)
Then, for any t > 0, on the event E ∩E , with probability at least 1−3e−t,
1 2
(cid:40) (cid:32)(cid:114) (cid:114) (cid:33) (cid:41)
(cid:104) (cid:105) WLlogW logn t t
E
X
R{f(cid:98)(X)−f¯(X)}2 ≤ c
t
r
0 n
+
n
+e
n
+
n
+ϵ2
n
,
where c > 0 is a constant and ϵ is defined as (3.3).
t n
Lemma S.5. Define
r := inf{r > 0 : 24ME(R F ) < s2,∀s ≥ r}. (A.12)
∗ n s
Then, with some constant K > 0,
(cid:114)
WLlogW logn
r ≤ K . (A.13)
∗
n
A.2 Proof of the auxiliary lemmas
Proof of Lemma S.1. Observe that
(cid:104) (cid:105)
E
n
R i{f(cid:98)(X i)−f0(X i)}2
(cid:104) (cid:105) (cid:104) (cid:105)
= E
n
R i{f(cid:98)(X i)−Y i#}2 −E
n
R i{f(cid:98)(X i)−Y i#}2−R i{f(cid:98)(X i)−f0(X i)}2
(cid:104) (cid:105) (cid:104) (cid:105)
= E
n
R i{f(cid:98)(X i)−Y i#}2 −E
Z
R{f(cid:98)(X)−Y#}2−R{f(cid:98)(X)−f0(X)}2
(cid:104) (cid:105)
−(E n−E Z) R i{f(cid:98)(X i)−Y i#}2−R i{f(cid:98)(X i)−f0(X i)}2 , (A.14)
where
(cid:104) (cid:105)
E
Z
R{f(cid:98)(X)−Y#}2−R{f(cid:98)(X)−f0(X)}2
(cid:104) (cid:105) (cid:104) (cid:105)
= E
Z
R{Y#−f(X)}2 +2E
Z
R{f(cid:98)(X)−f0(X)}{f0(X)−Y#} . (A.15)
29By definition, f0(X) = E(Y# | X,R = 1). Hence, using the tower rule, we have
(cid:104) (cid:105)
E
Z
R{f(cid:98)(X)−f0(X)}{f0(X)−Y#}
(cid:16) (cid:104) (cid:105) (cid:17)
= E
Z
E
Z
{f(cid:98)(X)−f0(X)}{f0(X)−Y#} | X,R = 1 P(R = 1 | X) = 0.
Together with (A.14) and (A.15), we have
(cid:104) (cid:105)
E
n
R i{f(cid:98)(X i)−f0(X i)}2
(cid:104) (cid:105) (cid:104) (cid:105)
= E
n
R i{f(cid:98)(X i)−Y i#}2 −E
Z
R{Y#−f(X)}2
(cid:104) (cid:105)
−(E n−E Z) R i{f(cid:98)(X i)−Y i#}2−R i{f(cid:98)(X i)−f0(X i)}2 . (A.16)
In addition,
(cid:104) (cid:105)
E
n
R i{f(cid:98)(X i)−Y i#}2
(cid:110) (cid:111) (cid:104) (cid:105) (cid:104) (cid:105)
= E
n
R i(Y(cid:98)i−Y i#)2 +E
n
R i{Y(cid:98)i−f(cid:98)(X i)}2 −2E
n
R i(Y(cid:98)i−Y i#){Y(cid:98)i−f(cid:98)(X i)}
(i) (cid:110) (cid:111) (cid:104) (cid:105) (cid:104) (cid:105)
≤ E
n
R i(Y(cid:98)i−Y i#)2 +E
n
R i{Y(cid:98)i−f¯(X i)}2 −2E
n
R i(Y(cid:98)i−Y i#){Y(cid:98)i−f(cid:98)(X i)}
(cid:104) (cid:105) (cid:104) (cid:105)
= 2E
n
R i(Y(cid:98)i−Y i#){f(cid:98)(X i)−f¯(X i)} +E
Z
R{Y#−f¯(X)}2
(cid:104) (cid:105)
+(E −E ) R {Y#−f¯(X )}2 , (A.17)
n Z i i i
where (i) holds by the constructions of f(cid:98)and f¯; see (3.2) and (A.1). Since f0(X) = E(Y# |
X,R = 1), we have
(cid:104) (cid:105)
E R{Y#−f0(X)}{f0(X)−f¯(X)}
Z
(cid:16) (cid:104) (cid:105) (cid:17)
= E E {Y#−f0(X)}{f0(X)−f¯(X)} | X,R = 1 P(R = 1 | X) = 0,
Z Z
and hence
(cid:104) (cid:105) (cid:104) (cid:105)
E R {Y#−f¯(X)}2 = E R{Y#−f0(X)}2 +E (cid:2) R {f0(X)−f¯(X)}2(cid:3) .
Z i Z Z i
Together with (A.16) and (A.17), we have
(cid:104) (cid:105)
E
n
R i{f(cid:98)(X i)−f0(X i)}2
(cid:16) (cid:104) (cid:105)(cid:17)
= (E n−E Z) R
i
{Y i#−f¯(X i)}2−{f(cid:98)(X i)−Y i#}2+{f(cid:98)(X i)−f0(X i)}2
(cid:104) (cid:105)
+E Z(cid:2) R i{f0(X)−f¯(X)}2(cid:3) +2E
n
R i(Y(cid:98)i−Y i#){f(cid:98)(X i)−f¯(X i)}
(cid:104) (cid:105)
= 2(E n−E Z) R i{f(cid:98)(X i)−f¯(X i)}{Y i#−f0(X i)}
(cid:104) (cid:105)
+E n(cid:2) R i{f0(X i)−f¯(X i)}2(cid:3) +2E
n
R i(Y(cid:98)i−Y i#){f(cid:98)(X i)−f¯(X i)} . (A.18)
30Here,
(cid:104) (cid:105)
E
n
R i(Y(cid:98)i−Y i#){f(cid:98)(X i)−f¯(X i)}
(cid:104) (cid:105) (cid:104) (cid:105)
= E
n
R i∆ 1i{f(cid:98)(X i)−f¯(X i)} +E
n
R i∆ 2i{f(cid:98)(X i)−f¯(X i)}
(cid:104) (cid:105)
≤ (E n−E Z) R i∆ 1i{f(cid:98)(X i)−f¯(X i)}
(cid:114)
(cid:113) (cid:104) (cid:105)
+ E n(R i∆2 2i) E
n
R i{f(cid:98)(X i)−f¯(X i)}2 , (A.19)
by the Cauchy-Schwarz inequality and the fact that
(cid:104) (cid:105) (cid:104) (cid:105)
E
Z
R∆ 1{f(cid:98)(X)−f¯(X)} = E
Z
{f(cid:98)(X)−f¯(X)}E Z(∆
1
| X,R = 1)P(R = 1 | X) = 0.
Combining (A.18) and (A.19), we conclude that (A.2) holds. Moreover,
(cid:104) (cid:105)
E
X
R i{f(cid:98)(X)−f0(X)}2
(cid:104) (cid:105) (cid:104) (cid:105)
= E
n
R i{f(cid:98)(X i)−f0(X i)}2 −(E n−E X) R i{f(cid:98)(X i)−f0(X i)}2
(cid:114)
(cid:113) (cid:104) (cid:105)
≤ E(cid:2) R{f0(X)−f¯(X)}2(cid:3) +2 E n(R i∆2 2i) E
n
R i{f(cid:98)(X i)−f¯(X i)}2
(cid:104) (cid:105)
+(E n−E Z) 2R i{f(cid:98)(X i)−f¯(X i)}{Y i#−f0(X i)+∆ 1i}
(cid:104) (cid:105)
+(E n−E Z) R i{f0(X i)−f¯(X i)}2−R i{f(cid:98)(X i)−f0(X i)}2
(cid:114)
(cid:113) (cid:104) (cid:105)
= E(cid:2) R{f0(X)−f¯(X)}2(cid:3) +2 E n(R i∆2 2i) E
n
R i{f(cid:98)(X i)−f¯(X i)}2
(cid:104) (cid:105)
+(E n−E Z) R i{f(cid:98)(X i)−f¯(X i)}{2Y i#−f(cid:98)(X i)−f¯(X i)+2∆ 1i} .
Proof of Lemma S.2. For any g ∈ F , there exists some f ∈ F and ∥Rf∥ ≤ 2M such
r DNN ∞
that g = R(f −f¯). Let h (g,X) := g2(X). Then,
1
|h (g,X)| = R(cid:8) f(X)−f¯(X)(cid:9)2 ≤ 16M2,
1
(cid:104) (cid:105) (cid:104) (cid:105)
Var{h (g,Z)} ≤ E R(cid:8) f(X)−f¯(X)(cid:9)4 ≤ 16M2E R(cid:8) f(X)−f¯(X)(cid:9)2 ≤ 16M2r2.
1
By Theorem 2.1 of Bartlett et al. [2005], with probability at least 1−e−t,
(cid:114)
2t 64M2t
sup(E −E){h (g,X )} ≤ 3ER {h (g,X ) : g ∈ F }+4Mr + .
n 1 i n 1 i r
n 3n
f∈Fr
Hence, with probability at least 1−e−t,
sup E {h (g,X )} ≤ sup(E −E){h (g,X )}+ sup E{h (g,X )}
n 1 i n 1 i 1 i
f∈Fr f∈Fr f∈Fr
(cid:114)
2t 64M2t
≤ 3E[R {h (g,X ) : g ∈ F }]+4Mr + +r2. (A.20)
n 1 i r
n 3n
31For any g ,g ∈ F ,
1 2 r
|h (g ,X)−h (g ,X)| = |g (X)+g (X)||g (X)−g (X)| ≤ 4M |g (X)−g (X)|.
1 1 1 2 1 2 1 2 1 2
By Lemma 2 of Farrell et al. [2021],
E [R {h (g,X ) : g ∈ F }] ≤ 8ME (R F ).
δ n 1 i r δ n r
By the law of total expectation and together with (A.20), with probability at least 1−e−t,
(cid:114)
2t 64M2t
sup E {h (g,X )} ≤ 24ME(R F )+4Mr + +r2 ≤ 4r2,
n 1 i n r
n 3n
f∈Fr
when Conditions C1 and C2 hold. That is, (A.8) holds with probability at least 1−e−t.
Proof of Lemma S.3. Firstly, (A.9) holds since ∥R∆ ∥ = O (1). In addition, we have
1 ∞,PZ p
E {E (R ∆2 )} = E (R∆2) = O (e2).
S1 n i 2i Z 2 p n
By Markov’s inequality,
E (R ∆2 ) = O (e2),
n i 2i p n
and hence (A.10) holds.
Proof of Lemma S.4. Recall that W is defined as (A.4). On the event E , (A.10), by Lemma
2 2
S.2, W satisfies
2
(cid:113) (cid:114)
W ≤ 2 E (R ∆2 ) sup E {g2(X )} ≤ 2M e r , (A.21)
2 n i 2i n i t n 0
g∈Fr0
with probability at least 1−e−t. Now, let us consider W defined as (A.3). For any g ∈ F ,
1 r0
there exists some f ∈ F and ∥Rf∥ ≤ 2M such that g = R(f −f¯), where F and f¯are
DNN ∞ r0
defined through (A.7) and (A.1), respectively. Let
h (g,X) :=g(X){2Y#−2f¯(X)+2∆ −g(X)}
2 1
=R{f(X)−f¯(X)}{2Y#−f(X)−f¯(X)+2∆ }.
1
Then,
W ≤ sup (E −E ){h (g,Z )}. (A.22)
1 n Z 2 i
g∈Fr0
Note that ∥Rf∥ ,∥Rf¯∥ ,∥RY#∥ ≤ 2M. On the event E , (A.9), we have
∞ ∞ ∞ 1
(cid:16) (cid:17)
|h (g,X)| ≤ (∥Rf∥ +∥Rf¯∥ ) 2∥RY#∥ +2∥R∆ ∥ +∥Rf∥ +∥Rf¯∥
2 ∞ ∞ ∞ 1 ∞,PZ ∞ ∞
≤ 4M(2M +2M +4M) ≤ 32M2,
t t
32and
(cid:104) (cid:105)
Var{h (g,X)} ≤ E R{f(X)−f¯(X)}2{2Y#−f(X)−f¯(X)+2∆ }2
2 Z 1
≤
(cid:16)
2∥RY#∥ +2∥R∆ ∥ +∥Rf∥ +∥Rf¯∥
(cid:17)2
E(cid:2) R{f(X)−f¯(X)}2(cid:3)
∞ 1 ∞,PZ ∞ ∞
≤ (2M +2M +4M)2r2 ≤ 64M2r2.
t 0 t 0
By Theorem 2.1 of Bartlett et al. [2005], on the event E , with probability at least 1−2e−t,
1
(cid:114)
2t 736M2t
sup (E −E ){h (g,X )} ≤ 6E R {h (g,X ) : g ∈ F }+8M r + t . (A.23)
n Z 2 i δ n 2 i r0 t 0
n 3n
g∈Fr0
For any g ,g ∈ F , there exists f ,f ∈ F and ∥Rf ∥ ,∥Rf ∥ ≤ 2M such that
1 2 r0 1 2 DNN 1 ∞ 2 ∞
g (X) = R{f (X)−f¯(X)} and g (X) = R{f (X)−f¯(X)}. Hence,
1 1 2 2
|h (g ,Z)−h (g ,Z)| = |g (X)−g (X)||2Y#−2f¯(X)+2∆ −g (X)−g (X)|
2 1 2 2 1 2 1 1 2
= R|f (X)−f (X)||2Y#+2∆ −f (X)−f (X)|
1 2 1 1 2
(cid:16) (cid:17)
≤ R|f (X)−f (X)| 2∥RY#∥ +2∥R∆ ∥ +∥Rf ∥ +∥Rf ∥
1 2 ∞ 1 ∞,PZ 1 ∞ 2 ∞
≤ 8M R|f (X)−f (X)|.
t 1 2
By Lemma 2 of Farrell et al. [2021],
E R {h (g,X ) : g ∈ F } ≤ 16M E (R F ). (A.24)
δ n 2 i r0 t δ n r0
For any r > 0, define
F(cid:101)r := {g : g(Z) = R{f(X)−f¯(X)},f ∈ F DNN,∥Rf∥
∞
≤ 2M,E n{g2(Z)} ≤ r2}.
Then, by Lemma S.2, F
r0
⊆ F(cid:101)2r0. Together with (A.22), (A.23) and (A.24), on the event E 1,
with probability at least 1−2e−t,
(cid:114)
2t 736M2t
W ≤ 96M E (R F )+8M r + t
1 t δ n r0 t 0
n 3n
(cid:114)
2t 736M2t
≤ 96M tE δ(R nF(cid:101)2r0)+8M tr
0
n
+ 3nt . (A.25)
By Lemmas 3 and 4 of Farrell et al. [2021] and repeating the proof of Section A.2.2 theirin,
when n > Pdim(F ),
DNN
(cid:115)
(cid:18) (cid:19)
Pdim(F ) 2eM 3
DNN
E δ(R nF(cid:101)2r0) ≤ 32r
0
n
log
r
+
2
logn
0
(cid:114)
Pdim(F )
DNN
≤ 64r logn, (A.26)
0
n
whenever r ≥ 1/n and n ≥ (2eM)2. Here, Pdim(F ) denotes the pseudo-dimension of the
0 DNN
network F . By Theorem 7 of Bartlett et al. [2019], with some constant C > 0,
DNN
Pdim(F ) ≤ CWLlogW,
DNN
33and we can see that n > Pdim(F ) holds since n > CWLlogW. Together with (A.25)
DNN
and (A.26), on the event E , with probability at least 1−2e−t,
1
(cid:32) (cid:114) (cid:114) (cid:33)
CWLlogW logn 2t 736M2t
W ≤ 8M r 768 + + t . (A.27)
1 t 0
n n 3n
Combining with Lemma S.1, (A.21), (A.27) and the fact that E(cid:2) {f0(X)−f¯(X)}2(cid:3) ≤ ϵ2, we
n
have
(cid:104) (cid:105) (cid:104) (cid:105)
E
X
R{f(cid:98)(X)−f¯(X)}2 ≤ 2E
X
R{f(cid:98)(X)−f0(X)}2 +2E(cid:2) R{f0(X)−f¯(X)}2(cid:3)
≤ 4E{f0(X)−f¯(X)}2+2W +2W
1 2
(cid:40) (cid:32)(cid:114) (cid:114) (cid:33) (cid:41)
WLlogW logn t t
≤ c r + +e + +ϵ2 ,
t 0 n n n n n
on the event E ∩E , with probability at least 1−3e−t and some constant c > 0.
1 2 t
Proof of Lemma S.5. By the definition (A.12), we know that r = 2r satisfies Condition C1,
∗
(A.5). Define
A := {E {g2(Z )} ≤ 16r2, ∀g ∈ F }.
n i ∗ 2r∗
Case 1: r2 ≥ 8M2logn/n. Then, r = 2r satisfies Condition C2, (A.6), with t = logn.
∗ ∗
By Lemma S.2,
P(A) ≥ 1−n−1. (A.28)
By the definition (A.12), there exists some constant v ∈ (1/2,1] such that
(vr )2 ≤ 24ME(R F ) ≤ 24ME(R F )
∗ n vr∗ n 2r∗
(cid:110) (cid:111)
≤ 24ME E δ(R nF(cid:101)4r∗)1 A+4M(1−1 A)
(cid:114)
(i) CWLlogW logn 96M2
≤ 1536Mr +
∗
n n
(cid:114)
(ii) CWLlogW logn 48Mr
∗
≤ 1536Mr + √ ,
∗
n 2nlogn
where (i) holds by Lemmas 3 and 4 of Farrell et al. [2021] and Theorem 7 of Bartlett et al.
[2019], as well as (A.28); (ii) holds since r2 ≥ 8M2logn/n. Hence, (A.13) holds with some
∗
constant K > 0.
Case 2: r2 ≤ 8M2logn/n. We also have (A.13) holds with some constant K > 0.
∗
A.3 Proof of the main results
Proof of Theorem 3.1. Define
(cid:114) (cid:32)(cid:114) (cid:33)
WLlogW logn logn
r¯:= (K +4c ) +4c +e
t t n
n n
(cid:115)
(cid:18) (cid:19)
logn 1
+ (32M2+2c ) +ϵ2 + . (A.29)
t n n n
34By Lemma S.5, r¯ > r . For any s ≥ r¯ > r , by the construction (A.12), r = s satisfies
∗ ∗
Condition C1, (A.5). In addition, r = s ≥ r¯ also satisfies r ≥ 1/n, as well as Condition C2,
(A.6), with t = logn.
In the following, we apply Lemma S.4 repeatedly. Choose l ≥ 1 be the smallest ingeter
larger than log (16M2n2). Then,
2
l ≤ 1+log (16M2n) = 5+2log M +2log n.
2 2 2
Additionally, we have
(cid:104) (cid:105)
2lr¯2 ≥ 16M2n2r¯2 ≥ 16M2 ≥ E
X
R{f(cid:98)(X)−f¯(X)}2 , (A.30)
since ∥Rf(cid:98)∥ ∞,∥Rf¯∥
∞
≤ 2M. Now, for each j ∈ {1,2,...,l}, we show that, on the event
E ∩E ,
1 2
(cid:104) (cid:105)
E
X
R{f(cid:98)(X)−f¯(X)}2 ≤ 2j−1r¯2 with probability at least 1−3n−1,
if we have
(cid:104) (cid:105)
E
X
R{f(cid:98)(X)−f¯(X)}2 ≤ 2jr¯2. (A.31)
Starting with j = l, as shown in (A.30), we have (A.31) holds. For any j ∈ {1,2,...,l},
assume that (A.31) holds. That is, r = 2jr¯satisfies Condition C3, (A.11). Since 2jr¯> r¯, we
know that r = 2jr¯ also satisfies r ≥ 1/n and Conditions C1, C2 with t = logn. By Lemma
S.4, on the event E ∩E , with probability at least 1−3n−1,
1 2
(cid:104) (cid:105)
E
X
R{f(cid:98)(X)−f¯(X)}2
(cid:32)(cid:114) (cid:114) (cid:33)
WLlogW logn logn c logn
≤ c 2jr¯ + +e + t +c ϵ2
t n n n n t n
(i) 2j−1r¯2 r¯2 2j−1r¯2 2j−1r¯2
≤ + ≤ + = 2j−1r¯2,
2 2 2 2
where(i)holdsbytheconstruction(A.29). Applyingtheabovestrategyrepeatedlyfromj = l
to j = 1, we conclude that, on the event E ∩E ,
1 2
(cid:104) (cid:105)
E
X
R{f(cid:98)(X)−f¯(X)}2 ≤ r¯2,
with probability at least
3l 3log (16M2n2) 12+6log M +6log n
1− ≥ 1− 2 = 1− 2 2 = 1−o(1).
n n n
Additionally, since P(E ∩E ) < 1−2/t and recall the definition of r¯, (A.29), we conclude
1 2
that
(cid:18) (cid:19)
(cid:104) (cid:105) WLlogW logn
E
X
R{f(cid:98)(X)−f¯(X)}2 = O
p n
+e2 n+ϵ2
n
.
35Therefore,
(cid:104) (cid:105) (cid:104) (cid:105)
E
X
R{f(cid:98)(X)−f0(X)}2 ≤ 2E
X
R{f(cid:98)(X)−f¯(X)}2 +2E(cid:2) R{f0(X)−f¯(X)}2(cid:3)
(cid:18) (cid:19)
WLlogW logn
= O +e2 +ϵ2 .
p n n n
Proof of Theorem 3.2. Define the following approximation error
ϵ := inf ∥f −f0∥ .
n,opt ∞
f∈FDNN(L,U,W):∥f∥∞≤2M
Repeating the proof of Theorem 3.1, we have
(cid:18) (cid:19)
(cid:104) (cid:105) WLlogW logn
E R{f(cid:98)opt(X)−f0(X)}2 = O
p n
+e2 n+ϵ2
n,opt
= O p(cid:16) n− 2β2 +β q log2β8 +β q n+e2 n+ϵ2 n,opt(cid:17) , (A.32)
q 1− 4q − β 4β
since L ≍ logn and W ≍ n2β+q log 2β+q n. Set ϵ¯ n = n 2β+q log2β+q n. Then, the chosen
−q/β
L,U,W satisfyL ≥ c{log(1/ϵ¯ )+1}andU,W ≥ cϵ¯ {log(1/ϵ¯ )+1},wherecistheconstant
n n n
defined in Theorem 1 of Yarotsky [2017]. Define Fq (L,U,W) as the DNNs defined through
DNN
(3.1) with q input units, L hidden layers, U computation units and W weights. Then, by
Theorem 1 of Yarotsky [2017], we have
inf ∥f(X )−f0(X )∥ ≤ ϵ¯ .
Q Q Q ∞ n
f∈Fq (L,U,W)
DNN
Together with the assumption that ∥f0(X)−f0(X )∥ ≤ r , we have
Q Q ∞ n
inf ∥f(X )−f0(X)∥ ≤ ϵ¯ +r .
Q ∞ n n
f∈Fq (L,U,W)
DNN
For any f ∈ Fq (L,U,W), we can construct a function g ∈ F (L,U,W) such that, apart
DNN DNN
from containing additional p−q unconnected input units, it shares the same architecture and
parameters as f. The constructed g ∈ F (L,U,W) satisfies f(X ) = g(X). Hence,
DNN Q
inf ∥f(X)−f0(X)∥ ≤ inf ∥f(X )−f0(X)∥
∞ Q ∞
f∈FDNN(L,U,W) f∈F Dq NN(L,U,W)
≤ ϵ¯ +r = o(1).
n n
Since ∥Y#∥ ≤ M, we have ∥f0∥ ≤ M, and hence
∞ ∞
inf ∥f −f0∥ ≥ M > ϵ¯ +r ,
∞ n n
f∈FDNN(L,U,W):∥f∥∞>2M
for large enough N. Therefore, we have
ϵ = inf ∥f −f0∥
n,opt ∞
f∈FDNN(L,U,W):∥f∥∞≤2M
= inf ∥f −f0∥ ≤ ϵ¯ +r .
∞ n n
f∈FDNN(L,U,W)
36− β 4β
Together with (A.32) and the choice that ϵ¯ n = n 2β+q log2β+q n, we conclude that
E(cid:104) R{f(cid:98)opt(X)−f0(X)}2(cid:105) = O p(cid:16) n− 2β2 +β q log2β8 +β q n+r n2 +e2 n(cid:17) .
Proof of Theorem 3.3. Define the following approximation error
ϵ := inf ∥f −f0∥ .
n,MLP ∞
f∈FMLP(L,H):∥f∥∞≤2M
NotethatthemultilayerperceptronsF (L,H)consistofW¯ = (L−1)H2+H(p+1)weights.
MLP
q 3β−q q 2β−2q
Case (a): p = O(n2β+2q log β+q n). By choosing L ≍ logn and H ≍ n2β+2q log β+q n,
we have W¯ ≍ nβ+q q log5β β− +q3q n + pn2β+q 2q log2β β− +q2q n ≍ nβ+q q log5β β− +q3q n = o(n) since p =
q 3β−q
O(n2β+2q log β+q n) and β ≥ 1. Repeating the proof of Theorem 3.1, we have
(cid:104) (cid:105) (cid:18) W¯ LlogW¯ logn (cid:19)
E R{f(cid:98) MLP(X)−f0(X)}2 = O
p n
+e2 n+ϵ2
n,MLP
= O p(cid:16) n− β+β q logβ8 +β q n+e2 n+ϵ2 n,MLP(cid:17) . (A.33)
By Lemma 1 of Farrell et al. [2021],
F (L,U,W) ⊆ F (L,WL+U).
DNN MLP
− β 4β
Set ϵ˜ n = n 2β+2q logβ+q n. Then, we have ϵ˜ n = o(1) since β ≥ 1. Choose U = W =
−q/β
⌈cϵ˜ {log(1/ϵ˜ ) + 1}⌉, where c is the constant defined in Theorem 1 of Yarotsky [2017].
n n
−q/β
Since H ≥ ⌈cϵ˜ {log(1/ϵ˜ )+1}⌉(L+1) = WL+U, we have
n n
F (L,U,W) ⊆ F (L,WL+U) ⊆ F (L,H). (A.34)
DNN MLP MLP
−q/β
Since L ≥ c{log(1/ϵ˜ ) + 1} and U,W ≥ cϵ˜ {log(1/ϵ˜ ) + 1}, by Theorem 1 of Yarotsky
n n n
[2017], we have
inf ∥f(X )−f0(X )∥ ≤ ϵ˜ .
Q Q Q ∞ n
f∈Fq (L,U,W)
DNN
Together with the assumption that ∥f0(X)−f0(X )∥ ≤ r , we have
Q Q ∞ n
inf ∥f(X )−f0(X)∥ ≤ ϵ˜ +r .
Q ∞ n n
f∈Fq (L,U,W)
DNN
For any f ∈ Fq (L,U,W), as demonstrated in the proof of Theorem 3.2, there exists some
DNN
g ∈ F (L,U,W) that satisfies f(X ) = g(X). Hence,
DNN Q
inf ∥f(X)−f0(X)∥ ≤ inf ∥f(X )−f0(X)∥
∞ Q ∞
f∈FDNN(L,U,W) f∈F Dq NN(L,U,W)
≤ ϵ˜ +r = o(1).
n n
37Together with (A.34), we have
inf ∥f −f0∥ ≤ inf ∥f −f0∥
∞ ∞
f∈FMLP(L,H) f∈FMLP(L,WL+U)
≤ inf ∥f −f0∥ ≤ ϵ˜ +r .
∞ n n
f∈FDNN(L,U,W)
Since ∥f0∥ ≤ M, we know that
∞
inf ∥f −f0∥ ≥ M > ϵ˜ +r ,
∞ n n
f∈FMLP(L,H):∥f∥∞>2M
for large enough N. Therefore, we have
ϵ = inf ∥f −f0∥
n,MLP ∞
f∈FMLP(L,H):∥f∥∞≤2M
= inf ∥f −f0∥ ≤ ϵ˜ +r .
∞ n n
f∈FMLP(L,H)
− β 4β
Together with (A.33) and the choice that ϵ˜ n = n 2β+2q logβ+q n, we conclude that
E(cid:104) {f(cid:98) MLP(X)−f0(X)}2(cid:105) = O p(cid:16) n− β+β q logβ8 +β q n+r n2 +e2 n(cid:17) .
q 3β−q
Case (b): p ≫ n2β+2q log β+q n and p = o(n/log5n). By choosing L ≍ logn and
H ≍ (n/p)2βq +q log4 2β β− +3 qq n, we have W¯ ≍ (n/p)2β2 +q q log10 2β β− +q5q n + n2βq +qp2β2 +β q log4 2β β− +3 qq n ≍
q 5β−3q q 3β−q
nβ+q log β+q n = o(n)sincep ≫ n2β+2q log β+q nandβ ≥ 1. RepeatingtheproofofTheorem
3.1, we have
(cid:104) (cid:105) (cid:18) W¯ LlogW¯ logn (cid:19)
E R{f(cid:98) MLP(X)−f0(X)}2 = O
p n
+e2 n+ϵ2
n,MLP
(cid:16) 2β 10β (cid:17)
= O
p
(p/n)2β+q log2β+q n+e2 n+ϵ2
n,MLP
. (A.35)
β 5β
Set ϵ˜
n
= (p/n)2β+q log2β+q n. Then, we have ϵ˜
n
= o(1) since p = o(n/log5n). Repeating the
proof of Case (a), we also have
ϵ ≤ ϵ˜ +r .
n,MLP n n
β 5β
Together with (A.35) and the choice that ϵ˜
n
= (p/n)2β+q log2β+q n, we conclude that
(cid:104) (cid:105) (cid:16) 2β 10β (cid:17)
E R{f(cid:98) MLP(X)−f0(X)}2 = O
p
(p/n)2β+q log2β+q n+r n2 +e2
n
.
B Proof of the results in Section 4
In this section, we denote
T{Y −µ(2)(1,S)} (1−T){Y −µ(2)(0,S)}
Y(cid:98) := µ (cid:98)(2)(1,S)+ π(2(cid:98)
)(S)
−µ (cid:98)(2)(0,S)− 1−π(2)(cid:98)
(S)
,
(cid:98) (cid:98)
T{Y −µ0(1,S)} (1−T){Y −µ0(0,S)}
Y# := µ0(1,S)+ −µ0(0,S)− .
π0(S) 1−π0(S)
38Then, we have the representation
Y(cid:98) −Y# = ∆ 1+∆ 2,
where ∆ = ∆ +∆ +∆ +∆ , ∆ = ∆ +∆ , and
1 1,1 1,2 1,3 1,4 2 2,1 2,2
(cid:26) (cid:27)
T
∆ := 1− {µ(2)(1,S)−µ0(1,S)}, (B.1)
1,1 π0(S) (cid:98)
(cid:26) (cid:27)
T T
∆ := − {Y(1)−µ0(1,S)}, (B.2)
1,2 π(2)(S) π0(S)
(cid:98)
(cid:26) (cid:27)
1−T
∆ := − 1− {µ(2)(0,S)−µ0(0,S)}, (B.3)
1,3 1−π0(S) (cid:98)
(cid:26) (cid:27)
1−T 1−T
∆ := − − {Y(0)−µ0(0,S)}, (B.4)
1,4 1−π(2)(S) 1−π0(S)
(cid:98)
(cid:26) (cid:27)
T T
∆ := − {µ(2)(1,S)−µ0(1,S)}, (B.5)
2,1 π(2)(S) π0(S) (cid:98)
(cid:98)
(cid:26) (cid:27)
1−T 1−T
∆ := − − {µ(2)(0,S)−µ0(0,S)}. (B.6)
2,2 1−π(2)(S) 1−π0(S) (cid:98)
(cid:98)
Proof of Lemma 4.1. Under Assumption 1,
(cid:20)(cid:26) (cid:27) (cid:21)
T
E (∆ | S) = E 1− {µ(2)(1,S)−µ0(1,S)} | S = 0, (B.7)
Z 1,1 Z π0(S) (cid:98)
(cid:20)(cid:26) (cid:27) (cid:21)
T T
E (∆ | S) = E − {Y(1)−µ0(1,S)} | S
Z 1,2 Z π(2)(S) π0(S)
(cid:98)
(cid:26) (cid:27)
T T
= E − | S E {Y(1)−µ0(1,S) | S} = 0. (B.8)
Z π(2)(S) π0(S) Z
(cid:98)
Similarly, we also have E (∆ | S) = 0 and E (∆ | S) = 0. Therefore, we conclude that
Z 1,3 Z 1,4
E (∆ | S) = 0 almost surely.
Z 1
Additionally,
(cid:20) (cid:21)
(cid:110) (cid:111)2(cid:110) (cid:111)2
E (∆2 ) ≤ ∥1/π0∥2 ∥1/π(2)∥2 E T π(2)(S)−π0(S) µ(2)(1,S)−µ0(1,S)
Z 2,1 ∞ (cid:98) ∞,PZ (cid:98) (cid:98)
= O (δ4 ),
p N
E (∆2 ) ≤ ∥1/(1−π0)∥2 ∥1/(1−π(2))∥2
Z 2,2 ∞ (cid:98) ∞,PZ
(cid:20) (cid:21)
(cid:110) (cid:111)2(cid:110) (cid:111)2
·E (1−T) π(2)(S)−π0(S) µ(2)(0,S)−µ0(0,S)
(cid:98) (cid:98)
= O (δ4 ),
p N
under Assumptions 1, 2, and 3. Note that ∆ ∆ = 0, we conclude that
2,1 2,2
E (∆2) = E (∆2 )+E (∆2 ) = O (δ4 ).
Z 2 Z 2,1 Z 2,2 p N
39Proof of Theorem 4.2. We first show that
θ (s) = E(Y# | S = s), ∀s ∈ Rd.
CATE
Under Assumption 1,
(cid:20) T{Y −µ0(1,S)} (1−T){Y −µ0(0,S)} (cid:21)
E(Y# | S) = θ (S)+E − | S
CATE π0(S) 1−π0(S)
E(T | S)E{Y(1)−µ0(1,S) | S} E(1−T | S)E{Y(0)−µ0(0,S) | S}
= θ (S)+ −
CATE π0(S) 1−π0(S)
= θ (S).
CATE
Since ∥Y∥ ≤ M under Assumption 3, the conditional mean function also satisfies ∥µ0∥ ≤
∞ ∞
M. Hence, under Assumptions 1 and 3,
∥Y#∥ ∞ = (cid:13) (cid:13) (cid:13) (cid:13)µ0(1,S)+ T{Y − π0µ (S0( )1,S)} −µ0(0,S)− (1−T 1){ −Y π− 0(µ S0 )(0,S)}(cid:13) (cid:13) (cid:13)
(cid:13)
∞
≤ 2∥µ0∥ +(∥1/π0∥ +∥1/(1−π0)∥ )(cid:0) ∥Y∥ +∥µ0∥ (cid:1) = O(1),
∞ ∞ ∞ ∞ ∞
(cid:13)(cid:26) (cid:27) (cid:13)
∥∆ 1,1∥ ∞,PZ =
(cid:13)
(cid:13)
(cid:13)
1−
π0T
(S)
{µ
(cid:98)(2)(1,S)−µ0(1,S)}(cid:13)
(cid:13)
(cid:13)
∞,PZ
≤ (1+∥1/π0∥ )(∥µ(2)∥ +∥µ0∥ ) = O (1),
∞ (cid:98) ∞,PZ ∞ p
(cid:13)(cid:26) (cid:27) (cid:13)
∥∆ 1,2∥ ∞,PZ =
(cid:13)
(cid:13)
(cid:13)
π(2T
)(S)
−
π0T
(S)
{Y(1)−µ0(1,S)}(cid:13)
(cid:13)
(cid:13)
(cid:98) ∞,PZ
≤ (∥1/π(2)∥ +∥1/π0∥ )(∥Y∥ +∥µ0∥ ) = O (1),
(cid:98) ∞,PZ ∞ ∞ ∞ p
(cid:13) (cid:26) (cid:27) (cid:13)
∥∆ 1,3∥ ∞,PZ =
(cid:13)
(cid:13) (cid:13)− 1−
1−1− π0T
(S)
{µ
(cid:98)(2)(0,S)−µ0(0,S)}(cid:13)
(cid:13)
(cid:13)
∞,PZ
≤ {1+∥1/(1−π0)∥ }(∥µ(2)∥ +∥µ0∥ ) = O (1),
∞ (cid:98) ∞,PZ ∞ p
(cid:13) (cid:26) (cid:27) (cid:13)
∥∆ 1,4∥ ∞,PZ =
(cid:13)
(cid:13) (cid:13)−
1−1 π− (2T
)(S)
−
1−1− π0T
(S)
{Y(0)−µ0(0,S)}(cid:13)
(cid:13)
(cid:13)
(cid:98) ∞,PZ
≤ (∥1/(1−π(2))∥ +∥1/(1−π0)∥ )(∥Y∥ +∥µ0∥ ) = O (1).
(cid:98) ∞,PZ ∞ ∞ ∞ p
It follows that
∥∆ ∥ = O (1).
1 ∞,PZ p
By Theorem 3.3 (with R ≡ 1) and Lemma 4.1, as N → ∞,
(cid:104) (cid:105)
E
Z
{θ(cid:98) C1 ATE(S)−θ CATE(S)}2
(cid:18) (cid:19)
= O
p
N− 2βθ2 +βθ 2qθ logβθ8 +βθ qθ N +(d/N)2β2 θβ +θ qθ log2β1 θ0 +βθ qθ N +r N2 +δ N4 .
Repeating the same procedure, we also have the same consistency rate for θ(cid:98)2 . Therefore,
CATE
θ(cid:98) = (θ(cid:98)1 +θ(cid:98)2 )/2 satisfies (4.4).
CATE CATE CATE
40C Proof of the results in Section 5
C.1 Auxiliary lemmas
Lemma S.1. Let Assumptions 5, 6 and 7 hold. In addition, assume
(cid:104) (cid:105)
E T (cid:8) µ(S )−µ0(S )(cid:9)2 = O (b2 ) (C.1)
1 (cid:98) 1 1 p N
with some sequence b ≥ 0. Then, as N → ∞,
N
(cid:16) (cid:17)
θ(cid:98)−θ = O
p
a Nd
N
+b Nc
N
+N−1/2 .
Lemma S.2. Let Assumptions 5, 6 and 7 hold. In addition, assume (C.1) holds with some
sequence b ≥ 0, a d +b c = o(N−1/2) and a ,b ,c ,d = o(1). Then, as N → ∞,
N N N N N N N N N
√
Nσ−1(θ(cid:98)−θ) → N(0,1)
in distribution, provided that σ2 = Var{ψ(Z;η0)} > c > 0.
C.2 Proof of the auxiliary lemmas
Proof of Lemma S.1. By Lemma S.8 of Bradic et al. [2024],
θ = E{ψ(Z ;η0)}.
i
Hence, we have
(cid:88)
θ(cid:98)(k)−θ = |I k|−1 ψ(Z i;η (cid:98)−k)−θ = W k,1+W k,2, (C.2)
i∈I
k
where
(cid:88)
W = |I |−1 ψ(Z ;η0)−E{ψ(Z ;η0)}, (C.3)
k,1 k i i
i∈I
k
W = |I
|−1(cid:88)(cid:8)
ψ(Z ;η )−ψ(Z
;η0)(cid:9)
. (C.4)
k,2 k i (cid:98)−k i
i∈I
k
By Lemma S.10 of Bradic et al. [2024],
(cid:16) (cid:17)
W = O N−1/2 . (C.5)
k,1 p
41In addition, since ψ(Z ;η )−ψ(Z ;η0), i ∈ I , are independent and identically distributed
i (cid:98)−k i k
conditional on S ,
−k
ES k(W k,2) = ES
k(cid:8)
ψ(Z i;η (cid:98)−k)−ψ(Z
i;η0)(cid:9)
(cid:20) T T {Y −ν (S )} T T {Y −ν0(S )} T {ν (S )−µ (S )}(cid:21)
= ES
1i 2i i (cid:98)−k 2i
−
1i 2i i 2i
+
1i (cid:98)−k 2i (cid:98)−k 1i
k π (S )ρ (S ) π0(S )ρ0(S ) π (S )
(cid:98)−k 1i (cid:98)−k 2i 1i 2i (cid:98)−k 1i
(cid:20) T {ν0(S )−µ0(S )} (cid:21)
+ES
k
µ (cid:98)−k(S 1i)− 1i π2 0i
(S )
1i −µ0(S 1i)
1i
( =i)
ES
(cid:20) T 1iρ0(S 2i){ν0(S 2i)−ν (cid:98)−k(S 2i)}
+
T 1i{ν (cid:98)−k(S 2i)−µ (cid:98)−k(S 1i)}(cid:21)
k π (S )ρ (S ) π (S )
(cid:98)−k 1i (cid:98)−k 2i (cid:98)−k 1i
(cid:20) T {ν0(S )−µ0(S )} (cid:21)
+ES
k
µ (cid:98)−k(S 1i)− 1i π2 0i
(S )
1i −µ0(S 1i)
1i
(ii)
= R +R +R ,
k,1 k,2 k,3
where (i) holds by the tower rule under Assumption 5; (ii) holds through rearranging and
R ,R ,R are defined as
k,1 k,2 k,3
(cid:20)(cid:26) (cid:27) (cid:21)
R
k,1
:= ES
k
1−
π
T (1 Si
)
(cid:8) µ (cid:98)−k(S 1i)−µ0(S 1i)(cid:9) ,
(cid:98)−k 1i
R
k,2
:= ES k(cid:20)
π
T (1 Si
)
(cid:26) 1− ρρ0( (S S2i) )(cid:27) (cid:8) ν (cid:98)−k(S 2i)−ν0(S 2i)(cid:9)(cid:21) ,
(cid:98)−k 1i (cid:98)−k 2i
(cid:20)(cid:26) (cid:27) (cid:21)
R
k,3
:= ES
k π
T (1 Si
)
− π0T (S1i
)
(cid:8) ν0(S 2i)−µ0(S 1i)(cid:9) .
(cid:98)−k 1i 1i
By the tower rule, we have
(cid:20) (cid:21)
R
k,1
= ES
k π
(ST )1 πi
0(S )
(cid:8) π (cid:98)−k(S 1i)−π0(S 1i)(cid:9)(cid:8) µ (cid:98)−k(S 1i)−µ0(S 1i)(cid:9)
(cid:98)−k 1i 1i
( ≤i) M2(cid:16)
ES k(cid:8) π (cid:98)−k(S 1i)−π0(S 1i)(cid:9)2 ES
k(cid:104)
T 1i(cid:8) µ (cid:98)−k(S 1i)−µ0(S
1i)(cid:9)2(cid:105)(cid:17)1/2
= O (b c ),
p N N
where (i) holds by the Cauchy-Schwarz inequality and under Assumption 5. Similarly, by the
tower rule,
(cid:20) (cid:21)
R
k,2
= ES
k π (S
)ρT 1iT (S2i
)ρ0(S )
(cid:8) ρ (cid:98)−k(S 2i)−ρ0(S 2i)(cid:9)(cid:8) ν (cid:98)−k(S 2i)−ν0(S 2i)(cid:9)
(cid:98)−k 1i (cid:98)−k 2i 2i
( ≤i) M3(cid:16)
ES
k(cid:104)
T 1i(cid:8) ρ (cid:98)−k(S 2i)−ρ0(S
2i)(cid:9)2(cid:105)
ES
k(cid:104)
T 1iT 2i(cid:8) ν (cid:98)−k(S 2i)−ν0(S
2i)(cid:9)2(cid:105)(cid:17)1/2
= O (a d ),
p N N
42where (i) holds by the Cauchy-Schwarz inequality and under Assumption 5. In addition, we
notice that
E{ν0(S ) | S ,T = 1} ( =i) E[E{Y(1,1) | S ,T = T = 1} | S ,T = 1]
2 1 1 2 1 2 1 1
(ii) (iii)
= E[E{Y(1,1) | S ,T = 1} | S ,T = 1] = E{Y(1,1) | S ,T = 1}
2 1 1 1 1 1
( =iv) E{Y(1,1) | S } ( =v) µ0(S ),
1 1
where (i) and (v) hold by the definitions of ν0(·) and µ0(·); (ii) and (iv) holds under Assump-
tion 5; (iii) holds by the tower rule. Therefore, using the tower rule, we also have
(cid:18) (cid:20)(cid:26) (cid:27) (cid:21) (cid:19)
R
k,3
= ES
k
ES
k π
1
(S )
− π0(1
S )
(cid:8) ν0(S 2i)−µ0(S 1i)(cid:9) | S 1i,T
1i
= 1 π0(S 1i)
(cid:98)−k 1i 1i
(cid:20)(cid:26) (cid:27) (cid:21)
= ES
k π
1
(S )
− π0(1
S )
(cid:8) µ0(S 1i)−µ0(S 1i)(cid:9) π0(S 1i) = 0.
(cid:98)−k 1i 1i
To sum up, we have
ES k(W k,2) = R k,1+R k,2+R
k,3
= O p(a Nd
N
+b Nc N). (C.6)
On the other hand, we note that
∥ψ(Z ;η )∥
i (cid:98)−k ∞,PS
k
(cid:13) (cid:13)
=
(cid:13) (cid:13)T 1iT 2i{Y i−ν (cid:98)−k(S 2i)}
+
T 1i{ν (cid:98)−k(S 2i)−µ (cid:98)−k(S 1i)}
+µ (cid:98)−k(S
1i)(cid:13)
(cid:13) = O p(1),
(cid:13) π (S )ρ (S ) π (S ) (cid:13)
(cid:98)−k 1i (cid:98)−k 2i (cid:98)−k 1i ∞,PS
k
and
∥ψ(Z i;η0)∥ ∞ = (cid:12) (cid:12) (cid:12) (cid:12)T 1i πT 2 0i ({ SY i )− ρ0ν (0 S(S )2i)} + T 1i{ν0(S π2 0i () S− )µ0(S 1i)} +µ0(S 1i)(cid:12) (cid:12) (cid:12)
(cid:12)
1i 2i 1i
= O(1) (C.7)
under Assumptions 5 and 7. Hence,
VarS k(W k,2) = |I k|−1VarS k(cid:8) ψ(Z i;η (cid:98)−k)−ψ(Z i;η0)(cid:9)
(cid:18) (cid:19)
≤ |I k|−1ES k(cid:8) ψ(Z i;η (cid:98)−k)−ψ(Z i;η0)(cid:9)2 = O p(|I k|−1) = O
p
N1 .
By Chebyshev’s inequality,
(cid:16) (cid:17)
W = O a d +b c +N−1/2 ,
k,2 p N N N N
and hence
(cid:16) (cid:17)
θ(cid:98)(k)−θ = ∆
1,k
+∆
2,k
= O
p
a Nd
N
+b Nc
N
+N−1/2 .
Finally, for fixed K > 0, we conclude that
K
(cid:88) (cid:16) (cid:17)
θ(cid:98)−θ = K−1 (θ(cid:98)(k)−θ) = O
p
a Nd
N
+b Nc
N
+N−1/2 .
k=1
43Proof of Lemma S.2. Observe that
7
(cid:88)
ψ(Z ;η )−ψ(Z ;η0) = R¯ ,
i (cid:98)−k i k,j,i
j=1
where
T T Y {ρ (S )−ρ0(S )}
R¯ := − 1i 2i i (cid:98)−k 2i 2i ,
k,1,i
π (S )ρ (S )ρ0(S )
(cid:98)−k 1i (cid:98)−k 2i 2i
T T Y {π (S )−π0(S )}
R¯ := − 1i 2i i (cid:98)−k 1i 1i ,
k,2,i
ρ0(S )π (S )π0(S )
2i (cid:98)−k 1i 2i
(cid:26) T (cid:27) T {ν (S )−ν0(S )}
R¯ := 1− 2i 1i (cid:98)−k 2i 2i ,
k,3,i
ρ (S ) π (S )
(cid:98)−k 2i (cid:98)−k 1i
(cid:26) T (cid:27) T ν0(S ){π (S )−π0(S )}
R¯ := − 1− 2i 1i 2i (cid:98)−k 1i 1i ,
k,4,i ρ (S ) π (S )π0(S )
(cid:98)−k 2i (cid:98)−k 1i 1i
T T ν0(S ){ρ (S )−ρ0(S )}
R¯ := 1i 2i 2i (cid:98)−k 2i 2i ,
k,5,i
π0(S )ρ0(S )ρ (S )
1i 2i (cid:98)−k 2i
(cid:26) (cid:27)
T
R¯ := 1− 1i {µ (S )−µ0(S )},
k,6,i (cid:98)−k 1i 1i
π (S )
(cid:98)−k 1i
T µ0(S ){π (S )−π0(S )}
R¯ := 1i 1i (cid:98)−k 1i 1i .
k,7,i π0(S )π (S )
1i (cid:98)−k 1i
By the tower rule,
E (cid:2) T T {ν (S )−ν0(S )}2(cid:3) = E (cid:2) T ρ0(S ){ν (S )−ν0(S )}2(cid:3)
S k 1i 2i (cid:98)−k 2i 2i S k 1i 2i (cid:98)−k 2i 2i
≥ M−1E (cid:2) T {ν (S )−ν0(S )}2(cid:3) ,
S k 1i (cid:98)−k 2i 2i
under Assumption 5. Similarly, we also have
E (cid:2) T {µ (S )−µ0(S )}2(cid:3) = E (cid:2) T π0(S ){µ (S )−µ0(S )}2(cid:3)
S k 1i (cid:98)−k 1i 1i S k 1i 1i (cid:98)−k 1i 1i
≥ M−1E (cid:2) {µ (S )−µ0(S )}2(cid:3) .
S k (cid:98)−k 1i 1i
Therefore,
E (cid:2) T {ν (S )−ν0(S )}2(cid:3) ≤ ME (cid:2) T T {ν (S )−ν0(S )}2(cid:3) = O (a2 ),
S k 1i (cid:98)−k 2i 2i S k 1i 2i (cid:98)−k 2i 2i p N
E (cid:2) {µ (S )−µ0(S )}2(cid:3) ≤ ME (cid:2) T {µ (S )−µ0(S )}2(cid:3) = O (b2 ).
S k (cid:98)−k 1i 1i S k 1i (cid:98)−k 1i 1i p N
When (C.1) holds, under Assumptions 5, 6 and 7, we have
ES k(R¯ k2 ,1,i) = O p(cid:0) ES k(cid:2) T 1i{ρ (cid:98)−k(S 2i)−ρ0(S 2i)}2(cid:3)(cid:1) = O p(d2 N),
ES k(R¯ k2 ,2,i) = O p(cid:0) ES k(cid:2) {π (cid:98)−k(S 1i)−π0(S 1i)}2(cid:3)(cid:1) = O p(c2 N),
ES k(R¯ k2 ,3,i) = O p(cid:0) ES k(cid:2) T 1i{ν (cid:98)−k(S 2i)−ν0(S 2i)}2(cid:3)(cid:1) = O p(a2 N),
ES k(R¯ k2 ,4,i) = O p(cid:0) ES k(cid:2) {π (cid:98)−k(S 1i)−π0(S 1i)}2(cid:3)(cid:1) = O p(c2 N),
ES k(R¯ k2 ,5,i) = O p(cid:0) ES k(cid:2) T 1i{ρ (cid:98)−k(S 2i)−ρ0(S 2i)}2(cid:3)(cid:1) = O p(d2 N),
ES k(R¯ k2 ,6,i) = O p(cid:0) ES k(cid:2) {µ (cid:98)−k(S 1i)−µ0(S 1i)}2(cid:3)(cid:1) = O p(b2 N),
ES k(R¯ k2 ,7,i) = O p(cid:0) ES k(cid:2) {π (cid:98)−k(S 1i)−π0(S 1i)}2(cid:3)(cid:1) = O p(c2 N).
44Recall the representation (C.2), now we have
VarS k(W k,2) ≤ |I k|−1ES k(cid:8) ψ(Z i;η (cid:98)−k)−ψ(Z i;η0)(cid:9)2
(cid:88)7 (cid:18) a2 +b2 +c2 +d2 (cid:19)
≤ 7|I k|−1 ES k(R¯ k2 ,j,i) = O
p
N N
N
N N . (C.8)
j=1
Together with (C.6), by Chebyshev’s inequality,
(cid:18) (cid:19)
a +b +c +d (cid:16) (cid:17)
W = O a d +b c + N N√ N N = o N−1/2
k,2 p N N N N p
N
when a d +b c = o (N−1/2) and a +b +c +d = o (1). Hence,
N N N N p N N N N p
K K
(cid:88) (cid:88)
θ(cid:98)−θ = K−1 (θ(cid:98)(k)−θ) = K−1 (W k,1+W k,2)
k=1 k=1
N
(cid:88) (cid:16) (cid:17)
= N−1 ψ(Z ;η0)−θ+o N−1/2 .
i p
i=1
Since (C.7) holds and σ > c > 0, the Lyapunov’s condition
E|ψ(Z;η0)−θ|2+δ
= o(1)
Nδ/2σ2+δ
holds with any δ > 0. By Lyapunov’s central limit theorem,
√ (cid:40) N (cid:41)
(cid:88)
Nσ−1 N−1 ψ(Z ;η0)−θ → N(0,1),
i
i=1
and hence
√ √ (cid:40) N (cid:41)
(cid:88)
Nσ−1(θ(cid:98)−θ) = Nσ−1 N−1 ψ(Z i;η0)−θ +o p(1) → N(0,1).
i=1
C.3 Proof of the main results
In the following, we denote
T {Y(1,1)−ν0(S )}
Y# := ν0(S )+ 2 2 ,
2
ρ0(S )
2
T {Y(1,1)−ν(2)(S )}
Y(cid:98) := ν (cid:98)(2)(S 2)+ 2 (cid:98) 2 .
ρ(2)(S )
(cid:98) 2
Consider the following representation:
Y(cid:98) −Y# = ∆ 1+∆ 2,
45where ∆ = ∆ +∆ and
1 1,1 1,2
(cid:26) (cid:27)
T
∆ := 1− 2 {ν(2)(S )−ν0(S )},
1,1 (cid:98) 2 2
ρ0(S )
2
(cid:26) (cid:27)
T T
∆ := 2 − 2 {Y(1,1)−ν0(S )},
1,2 2
ρ(2)(S ) ρ0(S )
(cid:98) 2 2
(cid:26) (cid:27)
T T
∆ := 2 − 2 {ν(2)(S )−ν0(S )}.
2 (cid:98) 2 2
ρ(2)(S ) ρ0(S )
(cid:98) 2 2
Proof of Lemma 5.1. By the law of total expectation,
E (T ∆ | S )
Z 1 1,1 1
(cid:18) (cid:20) (cid:26) (cid:27) (cid:21) (cid:19)
T
= E E T 1− 2 {ν(2)(S )−ν0(S )} | S ,T | S
Z Z 1 (cid:98) 2 2 2 1 1
ρ0(S )
2
(cid:18) (cid:20)(cid:26) (cid:27) (cid:21) (cid:19)
T
= E E 1− 2 | S ,T = 1 {ν(2)(S )−ν0(S )}E (T | S ) | S
Z Z 2 1 (cid:98) 2 2 Z 1 2 1
ρ0(S )
2
(cid:20)(cid:26) ρ0(S )(cid:27) (cid:21)
= E 1− 2 {ν(2)(S )−ν0(S )}E (T | S ) | S = 0. (C.9)
Z (cid:98) 2 2 Z 1 2 1
ρ0(S )
2
Similarly, let ξ := Y(1,1)−ν0(S ), then
2
E (T ∆ | S )
Z 1 1,2 1
(cid:18) (cid:20) (cid:26) (cid:27) (cid:21) (cid:19)
T T
= E E T 2 − 2 {Y(1,1)−ν0(S )} | S ,T | S
Z Z 1 2 2 1 1
ρ(2)(S ) ρ0(S )
(cid:98) 2 2
(cid:18)(cid:26) (cid:27) (cid:19)
1 1
= E − {ν0(S )−ν0(S )}E (T T | S ) | S = 0. (C.10)
Z 2 2 Z 1 2 2 1
ρ(2)(S ) ρ0(S )
(cid:98) 2 2
By (C.9) and (C.10),
E (T ∆ | S ) = E (T ∆ | S )+E (T ∆ | S ) = 0.
Z 1 1 1 Z 1 1,1 1 Z 1 1,2 1
Note that
E (T ∆ | S ) = E (T ∆ | S ,T = 1)P(T = 1 | S )
Z 1 1 1 Z 1 1 1 1 1 1
+E (T ∆ | S ,T = 0)P(T = 0 | S )
Z 1 1 1 1 1 1
= E (T ∆ | S ,T = 1)π(S )
Z 1 1 1 1 1
and ∥1/π0∥ ≤ M under Assumption 5, hence
∞
E (∆ | S ,T = 1) = 0 almost surely.
Z 1 1 1
Additionally,
E (T ∆2)
Z 1 2
(cid:20) (cid:21)
(cid:110) (cid:111)2(cid:110) (cid:111)2
≤ ∥1/ρ0∥2 ∥1/ρ(2)∥2 E T T ρ(2)(S )−ρ0(S ) ν(2)(S )−ν0(S )
∞ (cid:98) ∞,PZ 1 2 (cid:98) 2 2 (cid:98) 2 2
= O (δ4 ),
p N
46under Assumptions 5, 6, and 7. Note that
E (T ∆2) = E (T ∆2 | T = 1)P(T = 1)+E (T ∆2 | T = 0)P(T = 0)
Z 1 2 Z 1 2 1 1 Z 1 2 1 1
= E (T ∆2 | T = 1)P(T = 1)
Z 1 2 1 1
and P(T = 1) = E{π(S )} ≥ M−1 > 0 under Assumption 5, hence
1 1
E (T ∆2 | T = 1) = O (δ4 ).
Z 1 2 1 p N
Proof of Theorem 5.2. We first show that
µ0(s ) = E(Y# | S = s ,T = 1), ∀s ∈ Rd1.
1 1 1 1 1
Indeed,
(cid:20) T {Y(1,1)−ν0(S )} (cid:21)
E(Y# | S ,T = 1) = E ν0(S )+ 2 2 | S ,T = 1
1 1 2 1 1
ρ0(S )
2
( =i) E(cid:18) E(cid:20) ν0(S )+ T 2{Y(1,1)−ν0(S 2)} | S ,T = 1(cid:21) | S ,T = 1(cid:19)
2 2 1 1 1
ρ0(S )
2
( =ii) E(cid:20) ν0(S )+ E(T 2 | S 2,T 1 = 1)E{Y(1,1)−ν0(S 2) | S 2,T 1 = 1} | S ,T = 1(cid:21)
2 1 1
ρ0(S )
2
(i =ii) µ0(S ),
1
where(i)holdsbythelawoftotalexpectation; (ii)holdsunderAssumption5; (iii)holdssince
ν0(S ) = E{Y(1,1) | S ,T = 1} and E{ν0(S ) | S ,T = 1} = E{E[Y(1,1) | S ,T = 1} |
2 2 1 2 1 1 2 1
S ,T = 1] = µ0(S ).
1 1 1
In addition, observe that
∥T 1Y#∥ ∞ ( =i) (cid:13) (cid:13) (cid:13)T 1ν0(S 2)+ T 1T 2{Y −ν0(S 2)}(cid:13) (cid:13) (cid:13)
(cid:13) ρ0(S ) (cid:13)
2 ∞
≤ ∥ν0∥ +∥1/ρ0∥ (cid:0) ∥Y∥ +∥ν0∥ (cid:1) ( =ii) O(1),
∞ ∞ ∞ ∞
where (i) holds under Assumption 5; (ii) holds under Assumptions 5 and 7. Additionally,
(cid:13)(cid:26) (cid:27)
∥T 1∆ 1∥ ∞,PZ ( =i)(cid:13) (cid:13)
(cid:13)
T 1− ρT 0(1 ST 2
)
{ν (cid:98)(2)(S 2)−ν0(S 2)}
2
(cid:26) (cid:27) (cid:13)
+ T 1T 2 − T 1T 2 {Y −ν0(S 2)}(cid:13) (cid:13)
ρ(2)(S ) ρ0(S ) (cid:13)
(cid:98) 2 2 ∞,PZ
≤ (1+∥1/ρ0∥ )(∥ν(2)∥ +∥ν0∥ )
∞ (cid:98) ∞,PZ ∞
+(∥1/ρ(2)∥ +∥1/ρ0∥ )(cid:0) ∥Y∥ +∥ν0∥ (cid:1)
(cid:98) ∞,PZ ∞ ∞ ∞
(ii)
= O (1),
p
where (i) holds under Assumption 5; (ii) holds under Assumptions 5, 7, and the fact that
∥ν0∥ ≤ M as ∥Y∥ ≤ M.
∞ ∞
47Together with Lemma 5.1, we have verified the required conditions to apply Theorem 3.3.
Hence, it follows that
(cid:18) (cid:19)
E(cid:2) T 1{µ (cid:98)1(S 1)−µ0(S 1)}2(cid:3) = O p n− βµβ +µ qµ log8n+d 1n− 22 ββ µµ ++ 2q qµ µ log5n+r N2 +δ N4 .
Repeating the same procedure above, we also have the same consistency rate for µ2(S ).
(cid:98) 1
Therefore, the average µ satisfies (5.6).
(cid:98)
Proof of Theorem 5.3. By Theorem 5.2 and note that n ≍ N, we know that (C.1) is satisfied
with b2 =¯b2 +δ4 . By Lemma S.1, we have
N N N
(cid:16) (cid:17) (cid:16) (cid:17)
θ(cid:98)−θ = O
p
a Nd
N
+b Nc
N
+N−1/2 = O
p
a Nd
N
+¯b Nc
N
+δ N2 c
N
+N−1/2 .
Now,weadditionallyassumethata ,c ,d ,r ,δ = o(1),a d +¯b c +δ2 c = o(N−1/2)
N N N N N N N N N N N
and σ2 > c > 0. Then, we also have b = o(1) and b c = o(N−1/2). Hence, Lemma S.2
N N N
implies that, as N → ∞, in distribution,
(cid:16) (cid:17) √
θ(cid:98)−θ = O
p
N−1/2 and Nσ−1(θ(cid:98)−θ) → N(0,1). (C.11)
We now show that σ2 = σ2{1+o (1)}. Firstly, by (C.7), we have σ2 = Var{ψ(Z;η0)} =
(cid:98) p
O(1). Together with the assumption that σ2 > c > 0, we have σ ≍ 1. By (C.8), we have
ES k(cid:8) ψ(Z i;η (cid:98)−k)−ψ(Z i;η0)(cid:9)2 = O p(a2
N
+b2
N
+c2
N
+d2 N) = o p(1).
Hence,
 
ES k|I k|−1(cid:88) {ψ(Z i;η (cid:98)−k)−ψ(Z i;η0)}2  = ES k(cid:8) ψ(Z i;η (cid:98)−k)−ψ(Z i;η0)(cid:9)2 = o p(1).
i∈I
k
By Markov’s inequality,
(cid:88)
|I |−1 {ψ(Z ;η )−ψ(Z ;η0)}2 = o (1).
k i (cid:98)−k i p
i∈I
k
ByLemmaS.14ofBradicetal.[2024], togetherwith(C.11)andnotethatE{ψ(Z;η0)−θ}4 =
O(1) using the fact (C.7), we conclude that
σ2 = σ2{1+o (1)}.
(cid:98) p
48