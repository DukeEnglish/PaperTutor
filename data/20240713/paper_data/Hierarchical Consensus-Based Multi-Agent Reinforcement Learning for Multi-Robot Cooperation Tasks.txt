Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for
Multi-Robot Cooperation Tasks
Pu Feng, Junkang Liang, Size Wang, Xin Yu, Rongye Shi, and Wenjun Wu
Abstract—In multi-agent reinforcement learning (MARL),
theCentralizedTrainingwithDecentralizedExecution(CTDE)
framework is pivotal but struggles due to a gap: global state
guidance in training versus reliance on local observations
in execution, lacking global signals. Inspired by human so-
cietal consensus mechanisms, we introduce the Hierarchical
Consensus-based Multi-Agent Reinforcement Learning (HC-
MARL) framework to address this limitation. HC-MARL Cooperation
employscontrastivelearningtofosteraglobalconsensusamong
agents,enablingcooperativebehaviorwithoutdirectcommuni- Environment State Local Observation
cation.Thisapproachenablesagentstoformaglobalconsensus
Fig.1:Therelationshipbetweentheenvironmentalstateand from local observations, using it as an additional piece of
informationtoguidecollaborativeactionsduringexecution.To localobservationsintheCTDEframework.Despitediffering
cater to the dynamic requirements of various tasks, consensus local observations, they all correspond to the same environ-
is divided into multiple layers, encompassing both short-term mental state at each timestep, providing diverse perspectives
and long-term considerations. Short-term observations prompt
of a unified global state. In traditional CTDE approaches,
the creation of an immediate, low-layer consensus, while long-
agents rely solely on these local observations for decision-
term observations contribute to the formation of a strategic,
high-layer consensus. This process is further refined through making during execution.
an adaptive attention mechanism that dynamically adjusts the
influenceofeachconsensuslayer.Thismechanismoptimizesthe mation sharing and increased bandwidth requirements. The
balance between immediate reactions and strategic planning, second method leverages intrinsic rewards to create leader-
tailoring it to the specific demands of the task at hand.
follower dynamics [7], yet this approach is constrained by
Extensive experiments and real-world applications in multi-
itstask-specificeffectivenessandencountersdifficultieswith
robotsystemsshowcaseourframework’ssuperiorperformance,
marking significant advancements over baselines. broad applicability and generalization. The third approach,
employingmeanfieldtheory[8],offersapromisingdirection
I. INTRODUCTION
butoftenstrugglestoeffectivelyhandlecomplextasks.Given
Multi-Agent Reinforcement Learning (MARL) is garner- existing methods’ limitations, sociology’s research [9], [10]
ing increasing attention for its capability to tackle complex on consensus team agents interacting to align on shared
tasks [1]. Tasks involving distributed multi-robots [2] often values offers potential solutions for CTDE’s challenges with
require several agents to collaborate based on their local ob- partial observations.
servationstoaccomplishagivenobjective.Thisrequirement Inspired by consensus mechanisms in multi-agent sys-
aligns with the commonly adopted MARL framework of tems [11], we introduce the Hierarchical Consensus-based
Centralized Training with Decentralized Execution (CTDE), Multi-Agent Reinforcement Learning (HC-MARL) frame-
as exemplified by methods such as MADDPG (Multi-Agent work, designed to facilitate substantive multi-agent collabo-
DeepDeterministicPolicyGradient)[3]andMAPPO(Multi- ration in settings characterized by local observations and the
Agent Proximal Policy Optimization) [4]. These approaches absenceofdirectcommunication.AsshowninFig.1,despite
utilize global information during the training phase through differing local observations, agents all correspond to the
the critic, while the actor relies solely on individual obser- same environmental state at each timestep, merely offering
vations during execution. This setup leads to a significant diverseperspectivesofaunifiedglobalstate.Adaptingideas
challenge: agents lack a consensus during task execution, from contrastive learning [12], we first map local obser-
hindering their collective performance for cooperation [5]. vations into discrete latent spaces as forms of invariances
In addressing this issue, three primary methodolo- using the consensus builder. We define these invariances
gies have been advanced. The first involves strategies as global consensus. This global consensus is then treated
for communication-based multi-agent reinforcement learn- as an additional piece of local observation information fed
ing [6], which are faced with challenges in selective infor- into the actor network. Notably, utilizing the consensus only
requires an agent’s local observations, aligning with the
*This work was supported in part by the National Key R&D Program
CTDE framework’s prerequisite for partial observability.
of China (2022ZD0116401) and the National Natural Science Foundation
of China (62306023). All authors are with Beihang University, Xueyuan The initial findings, however, highlighted a limitation:
Road No.37, Beijing 100191, China. Emails: {fengpu, liangjunkang, size-
relyingexclusivelyonobservationsfromasingletimestepto
wang,nlsdeyuxin,shirongye,wwj09315}@buaa.edu.cn.WenjunWuisthe
correspondingauthor. establish a single-layer consensus falls short in fully captur-
4202
luJ
11
]IA.sc[
1v46180.7042:viXraingthenuancesofsequentialtasks.Forexample,asdepicted
obs
in Fig. 2, within the CTDE paradigm, the agents’ execution
obsrange
information is confined to local observations. Incorporating Neighbor obswith static
static environmental states, or short-term consensus, supple- state information
Agent
ments execution by integrating additional static information obswith dynamic
state information
from other agents, such as their positions and orientations,
Environment Information
not initially available. To effectively incorporate dynamic for execution
attributes, like other agents’ velocities, introducing dynamic
Fig. 2: Importance of Dynamic State Information: The di-
stateinformationthroughlong-termconsensusbecomescru-
agram illustrates agents as green triangles and neighbors
cial. To address this, we propose the hierarchical consensus
as blue triangles. The orientation of agents is indicated
mechanism, where consensus based solely on single-step
by the vertical position of the triangles, and their motion
observations is defined as low-layer consensus, focusing on
direction is shown by the arrows. The left side displays the
short-term optimality. High-layer consensus, on the other
environmental state, while the right side shows information
hand, aggregates local observations from multiple timesteps,
usableinexecutionwithinCTDE.Staticenvironmentalinfor-
forming a long-term group consensus that considers the
mation provides position and orientation, whereas dynamic
broader strategic trends or states of multiple agents. Lastly,
information additionally offers speed data.
weemployanattentionmechanismtodynamicallyweighthe
influence of each consensus layer for collaborative needs,
(MARL) [4], [20], [21] has become a focal point within the
ensuring that the system can adaptively prioritize either
third category, emphasizing the utilization of consensus for
short-termorlong-termconsiderationsbasedontheevolving
enhancing agent cooperation in complex environments.
context of the task. This hierarchical consensus serves as
additional local observation during the execution of actions,
B. Contrastive Learning
providingagentswithcontext-richgroupbehavioralinsights.
Recentstudieshaveincreasinglyleveragedself-supervised
Our HC-MARL framework can be seamlessly integrated
learning [22] to enhance model capabilities for downstream
as components of almost all MARL algorithms. The contri-
tasks.Amongthese,contrastivelearning[23]standsoutasa
butions of this paper are summarized as follows:
particularlytractableapproach.Itoperatesontheprincipleof
• Leveraged contrastive learning to construct global con-
minimizing the distance between augmented versions of the
sensus from local observations, enhancing cooperative
samesamplewhilemaximizingthedistancebetweendistinct
action execution in multi-agent reinforcement learning.
ones,asnotedbyWang[24].Thismethodeffectivelyboosts
• Introduced the HC-MARL framework with a hierarchi-
themodel’sunderstandingofequivalentdatarepresentations.
calconsensusmechanism,featuringbothshort-termand
However, the collection of positive and negative samples
long-term consensuses.
in contrastive learning presents challenges that significantly
• Implemented an adaptive attention mechanism that dy-
impactitseffectiveness.Inreinforcementlearning(RL)[25],
namically tunes the influence of each consensus layer,
samples are collected through ongoing interactions with the
optimizing the balance between immediate responses
environment,highlightingthepotentialbenefitsofintegrating
andstrategicplanningaccordingtothespecificdemands
contrastive learning with RL. Recent research efforts [26],
of the task.
havesuccessfullyemployedcontrastivelearningforrepresen-
• Demonstrated the superior performance of our frame-
tationlearningpriortoreinforcementlearning,achievingun-
work over baselines through extensive evaluations in
paralleleddataefficiencyinpixel-basedRLtasks.Moreover,
both simulated tasks and real-world robot experiments.
contrastive learning has been utilized to formulate reward
functions within RL systems [27].
II. RELATEDWORK
A. Consensus in Multi-Agent System C. Contrastive Learning for MARL
Consensusmeanstheinteractionbetweengroupsofagents Despite the progress in single-agent RL contexts, the
in a team to reach an agreement on a common value or exploration of contrastive learning in MARL remains com-
state [13], [14]. Consensus in Multi-Agent Systems has paratively underdeveloped. Lin [28] proposed incorporating
garnered extensive research interest, primarily focusing on contrastive learning outcomes as a loss function in the
achieving shared agreement among agents. Research in this MARL training process, applying it to multi-agent path
area spans three main categories: first, studies inspired by planningtasks.Xu[29]introducedusingcontrastivelearning
biologicalmechanismsandwildlifecollectivebehaviors[15], to articulate the observational differences among agents.
[16]; second, theoretical explorations using models like the Further, Liu [30] promoted the development of a common
graph theory for foundational insights into consensus [17], languagebymaximizingthemutualinformationbetweenthe
[18]; and third, practical applications, including the devel- messages of given trajectories in a contrastive manner. Our
opment of consensus models and protocols [19], with a HC-MARLmethodemployscontrastivelearningtoestablish
keenfocusonconvergence,equilibrium,andimplementation amulti-layerglobalconsensus.Tothebestofourknowledge,
challenges. Notably, Multi-Agent Reinforcement Learning thisisthefirststudyutilizingcontrastivelearningtocreateastructured global consensus for collaborative agent behavior For the actors, which operate based on their local obser-
under local observations. vations during decentralized execution, the policy gradient
is adjusted to reflect their dependence on local observations
III. PRELIMINARIES
o. The policy gradient for optimizing each agent’s policy π,
A. Problem Formulation considering local observations, is derived as follows:
The multi-robot cooperation task can be formulated as a
decentralized partially observable Markov decision process ∇ J(π)=E [∇ logπ(o,a|θ)Q (s,a)] (2)
θ o,a∼ρπ θ ϕ
(Dec-POMDP) [31], defined as (I,S,A,T,R,O,Z,γ). The
index set (I = {1,..,N}) represents the set of agents. C. Contrastive Learning
S is the global state space. Note that each agent is only
The Knowledge Distillation with No Labels (DINO) [32]
capable of partial observation of the environment s ∈ S,
method offers a solution as a form of self-supervised con-
and the individual observation o ∈O comes from the local
i trastive learning that leverages a teacher-student network
observation function o = Z (s) : S → O. Each agent i
i i architecture. In this framework, for a given sample u, a new
chooses its action according to its policy a i ∼ π i(·|o i). sample u′ is generated through data augmentation. Both u
The joint action space A consists of the union of all agent’s and u′ are then fed into the student and teacher networks,
actionspace(cid:83)N
A .Wedefinethestatetransitionfunction
i=1 i respectively,producingclassificationdistributionsP S(u)and
T : S × A → S and the discount factor γ ∈ (0,1). All P (u′). In the absence of true labels, the teacher network’s
T
agents share the same joint reward function R(s,a). The
outputservesaspseudo-labels,andthestudentnetworkaims
agents aim to maximize the expected joint return, defined as
tooptimizebyminimizingthecross-entropylossbetweenits
E [(cid:80)∞ γtR(s ,a )]
π t=0 t t output and these pseudo-labels.The cross-entropy loss used
for optimization can be formalized as:
B. Centralized Training with Decentralized Execution
(CTDE)
(cid:88)
L =− P (u′) logP (u) (3)
CL T c S c
To address the problems under decentralized partially
c
observable Markov decision processes (Dec-POMDPs), the
where c indexes over the classes, P (u′) is the pseudo-
Centralized Training with Decentralized Execution (CTDE) T c
labelprobabilityforclasscproducedbytheteachernetwork
framework emerges as a critical approach. CTDE delineates
for augmented sample u′, and P (u) is the probability
amethodologywherethetrainingphaseiscentralized,allow- S c
produced by the student network for the original sample u.
ingagentstoaccessglobalinformationandlearncoordinated
The teacher and student networks share the same archi-
strategies. In contrast, during execution, each agent operates
tecture, with the teacher’s parameters being an exponential
independently based on its local observations, aligning with
moving average (EMA) of the student’s parameters. This
the decentralized nature of many real-world applications.
arrangementfacilitatesacontinuousrefinementofthestudent
Although value decomposition methods and policy-based
network’s learning through guidance from a slowly evolving
multi-agent methods differ significantly in structure, they
version of itself, represented by the teacher network.
bothadheretotheCTDEprinciplesandencounterchallenges
In multi-agent reinforcement learning scenarios, local ob-
inprovidingunifiedguidanceduringdecentralizedexecution
servations made by different agents can be considered as
in fully cooperative tasks. To illustrate the optimization
diverse augmented samples of the same global state. The
process within the CTDE paradigm, we focus on the most
global consensus, therefore, corresponds to the classification
commonlyusedmethod,MAPPO.MAPPOisanactor-critic
outputfromtheteacher-studentnetworkframework.Bycon-
method based on the CTDE paradigm. Each agent learns a
policy π in an on-policy manner. MAPPO consists of a cen- structing this consensus metric, our aim is to guide agents,
operating under local observations, towards forming global
tralized critic and several independent actors corresponding
cooperation.
toeachagent.Duringthecentralizedtrainingphase,thecritic
utilizes global state information to estimate the joint action-
IV. METHODS
value Q. The critic is trained by minimizing the Temporal
Difference (TD) error as follows: In this section, we introduce Hierarchical Consensus-
based Multi-Agent Reinforcement Learning (HC-MARL), a
novel framework that dynamically guides agents towards
(cid:104) (cid:105)
LCritic(ϕ)=E (s,a,r,s′) (Q ϕ(s,a)−(r+γQ ϕ′(s′,a′)))2 cooperative execution under partial observations through a
(1) hierarchical consensus mechanism.
where Q (s,a) represents the critic’s current estimate of
ϕ
A. Consensus Builder
the joint action-value for the global state s and actions a,
parameterizedbyϕ;r istheimmediaterewardreceivedafter In multi-agent reinforcement learning, agents execute ac-
taking action a in state s; s′ is the next state, and a′ is the tionsbasedonlocalobservations,leadingtoalackofglobal
action taken in the next state as per the current policy; γ is information guidance during execution. This section focuses
the discount factor; and ϕ′ refers to the parameters of the on building an effective consensus within the Centralized
target critic used for bootstrapping. Training with Decentralized Execution (CTDE) framework.Asdiscussedinaprevioussection,theDINOframeworkpro- 1-st layer Consensus
cesses a sample and its data-augmented equivalent through Teacher
1
a teacher-student network. We treat an agent’s observation
EMA
𝑃𝑃𝑇𝑇(𝑥𝑥𝑗𝑗1
)
o as the sample, and observations from other agents as the 1 𝑎𝑎1 1
𝑥𝑥𝑗𝑗={oj} 𝐿𝐿S
equivalent samples. In other words, the observation function 𝑃𝑃𝑆𝑆(𝑥𝑥𝑖𝑖1
)
Student
oZ bsis erc vo an tis oid ne sre id sa an ugau mg em nte en dta it nio ton eo ap ce hra ati go en n, t’w sh oe br se et rh ve atg iolo nb oal
. 1·
𝑥𝑥𝑖𝑖1 ={o𝑖𝑖𝑎𝑎1
}
… 1 M Au ttl eti n-h tie oa nd
m-thlayer Consensus
These observations o correspond to the same global state s. Teacher
m
Drawing inspiration from human patterns of situational 𝑚𝑚
𝑃𝑃𝑇𝑇(𝑥𝑥𝑗𝑗) 𝑎𝑎𝑎𝑎𝑎𝑎
a stw anar de in ne gss o— f tw heh ie rre eni vn id roiv ni mdu ea nl ts fo roft men lod ce ar liv ce uea s,b sr uo ca hd au snd de isr- - Trajectory history 𝑥𝑥𝑗𝑗𝑚𝑚 ={oj𝑎𝑎1 …o𝑗𝑗𝑎𝑎𝑚𝑚 }EMA 𝐿𝐿𝑚𝑚 S 𝑃𝑃𝑆𝑆(𝑥𝑥𝑖𝑖𝑚𝑚 )
𝑐𝑐𝑖𝑖
Attention-weighted
cerninggeneralcardinaldirectionsinacitywithoutknowing Consensus
precise coordinates—we propose a model that leverages 𝑥𝑥𝑖𝑖𝑚𝑚 ={o𝑖𝑖𝑎𝑎1 …o𝑖𝑖𝑎𝑎𝑚𝑚 } Student𝑚𝑚
discrete categories for consensus in multi-agent systems. Fig. 3: An overview of the Hierarchical Consensus Mech-
This approach simulates how agents might infer a macro anism. xm i and xm j represent different local observations
classification of the current state from limited, local infor- from the same environmental state for the m-th layer, which
mation. Specifically, we define consensus in terms of K are used to derive a global consensus classification through
distinctclasses,enablingtheconsensusmoduletocategorize theteacher-studentnetwork.Consensusfromdifferentlayers
an agent’s local observations into a unified class k, which is aggregated into an attention-weighted consensus through
subsequently acts as the global consensus for guiding the multi-head attention.
agents’ actions.
this issue, this section introduces a hierarchical consensus
For each agent i in a set of n agents, the classification
mechanism, divided into short-term consensus and long-
distribution resulting from local observations is denoted by
term consensus. Short-term consensus considers only the
P (o ) for the student network and P (o ) for the teacher
S i T i
currenttimestep’sstate,whilelong-termconsensustakesinto
network. The consensus among agents is evaluated by pair-
accountinformationacrossmultipletimesteps,incorporating
wise comparison, optimizing the Consensus Builder through
a longer-term utilization of historical state information. This
minimizing the sum of cross-entropy loss:
is dynamically leveraged through an attention mechanism
(cid:88)n (cid:88)n (cid:88) that weighs the importance of short-term and long-term
L (θ)=− P (o ) logP (o ) (4)
S T j k S i k consensus. For instance, in scenarios requiring immediate
i=1j=1 k collision avoidance, agents prioritize short-term consensus.
where i and j are the indices of the agents. P (o ) Conversely, in collaborative search tasks, agents rely more
T j k
and P (o ) denote the probability of category k in the on long-term consensus to allocate search areas efficiently.
S i k
distribution output by the teacher and student networks, We expand the foundational consensus builder into a
respectively. The consensus c for each agent is obtained as Hierarchical Consensus Mechanism, distinguishing between
follows: short-term and long-term consensus. Short-term consen-
sus leverages observations or consensus from a single
c =argmaxP (o ) k (5) timestep. For long-term consensus, we introduce xm =
i S i c i
k {ot1,ot2,ot3,...,otm}, a set representing the agent i’s ob-
Thisprocedureidentifiestheconsensusclassc foragenti i i i i
i servations at various timesteps within the trajectory history,
byselectingtheclassk withthemaximumprobabilityinthe
wheremindicatestheinclusionofmhistoricalobservations.
classificationdistributionP (o ),asgeneratedbythestudent
S i It’s crucial to note that t , t , t , and so forth, denote
1 2 3
network for observation o .
i distinct timesteps, which are not required to be consecutive.
Such a method underscores the agents’ collaborative push
This approach proves especially beneficial in scenarios with
towards a harmonized environmental perception, thereby
brief training intervals, where successive states may exhibit
enabling a collective consensus derived from individual
minimal differences. By considering observations at spaced
observations.Byemployingacross-entropylossfunction,the
intervals, we capture more pronounced changes over time,
framework promotes similarity in probability distributions
thereby gaining insights into significant state transitions.
for observations by different agents, even in disparate local
As we transition from utilizing single-timestep observa-
contexts, thereby fostering a unified understanding of the
tions to aggregating multi-timestep observations for con-
global state.
trastivelearning,theoptimizationcriterionforthem-thlayer
B. Hierarchical Consensus Mechanism studentnetworkiscorrespondinglyrevised.Theupdatedloss
function is defined as follows:
Through the consensus builder, we have obtained a global
consensus among agents based on their local observations. (cid:88)n (cid:88)n (cid:88)
Lm(θ)=− P (xm) logP (xm) (6)
However, as shown in Fig. 2, obtaining a complete and S T j k S i k
effective global consensus from a single moment’s local ob- i=1j=1 k
servationsischallenginginpracticalapplications.Toaddress Therefore,theconsensusforthem-thlayer,cm,isredefinedtions. We integrate this consensus as an augmented obser-
𝑜𝑜𝑖𝑖 vation input within the multi-agent reinforcement learning
Hierarchical
𝑥𝑥𝑖𝑖𝑚𝑚 C Bon us ie ldn es rus 𝑐𝑐𝑖𝑖𝑎𝑎𝑎𝑎𝑎𝑎 framework. It’s pivotal to emphasize that while information
Policy Network from other agents is leveraged during the training phase of
Hierarchical
Agent
𝑥𝑥j𝑚𝑚 C Bon us ie ldn es rus 𝑐𝑐𝑗𝑗𝑎𝑎𝑎𝑎𝑎𝑎 the student network, the action execution phase exclusively
relies on an agent’s local observations. This design principle
Hierarchical
Consensus Critic Network ensures that our HC-MARL approach is compatible with a
Environment Builder
Local Observation Consensus Class widerangeofMARLalgorithms,adheringtotheCentralized
Global state s Training with Decentralized Execution (CTDE) paradigm.
Fig.4:OverviewoftheHC-MARLframework.Sequentially, Taking MAPPO as an example, by incorporating consen-
from left to right: Agents initially acquire local observations susinformationintotheobservations,theupdateforthecritic
from the environment. These observations are subsequently network becomes:
processedby thehierarchicalconsensusbuilder, yieldingthe
current consensus class. This derived consensus, denoted as (cid:104)(cid:16)
catt,enrichestheagents’observationalorstatedata.Itisthen LCritic(ϕ)= E (s,a,r,s′) Q ϕ(s,catt,a)
i
incorporated into both policy and critic networks, thereby (cid:17)2(cid:105)
−(r+γQ (s′,catt′ ,a′)) (9)
steering agent actions in alignment with the collectively ϕ′
determined global consensus.
where catt and catt′ represent the attention-weighted
for each agent i as follows: consensus information at the current and next time steps,
respectively. The actor network update is formulated as
cm i =argmaxP S(xm i ) k (7) follows:
k
Given the multiple layers of consensus achieved through ∇ ψJ(π)=E o,catt,a∼ρπ(cid:2) ∇ θlogπ(o,catt,a|ψ)Q ϕ(s,catt,a)(cid:3)
the hierarchical structure, it becomes crucial to evaluate (10)
and weigh these layers differently across various scenarios. These formulations illustrate how consensus information,
This differentiation acknowledges that the importance of specifically the attention-weighted consensus catt, is seam-
short-term and long-term consensus can vary significantly lessly integrated into the MARL process, enhancing the
depending on the context. To address this, we incorporate learning mechanism by providing a more informed perspec-
an attention mechanism that treats the consensus from each tive on the environment.
layerasinput,asdepictedinFig.3.Thisapproachenablesus
V. EXPERIMENTANDRESULTS
to integrate these varied consensus inputs into a multi-head
attention framework, effectively allowing for the dynamic Weconductedbothsimulationsandhardwareexperiments
weighting of each layer’s consensus. to validate the efficacy of our proposed HC-MARL.
To formalize this approach, we consider the output con-
A. Environment Settings
sensusfromeachlayer,cm,asinputtoamulti-headattention
i
mechanism. This mechanism aims to dynamically weigh
the consensus from different layers according to the current
scenario’s specific requirements, resulting in a contextually
weighted combination. The formalization is given by:
Unspecified
Predator Target
ca itt =MultiHead(Q(cm i ),K(cm i ),V(cm i )) (8) Prey
In this equation, ca itt represents the attention-weighted (a) Predator-Prey (b) Rendezvous (c) Navigation
consensusforagenti.ThefunctionsQ,K,andV correspond
Fig. 8: The simulated tasks considered in the experiments.
tothequery,key,andvaluefunctions,respectively,whichare
applied to the consensus inputs. These functions facilitate We constructed three cooperative multi-agent tasks within
the mapping of consensus from each layer into a space the Webots simulation [33], including Predator-Prey, Ren-
that allows for the evaluation of the layers’ relevance. The dezvous [34], and Navigation, as shown in Fig. 8. We
MultiHead attention mechanism aggregates these mapped implemented our HC-MARL in these three tasks and com-
representations, allocating weights based on their assessed pared it against two main-stream MARL baselines Multi-
importance to the current decision-making context. Agent Proximal Policy Optimization (MAPPO) [4] and its
variant Heterogeneous-Agent Proximal Policy Optimization
C. HC-MARL Framework
(HAPPO) [35]. Note that our HC-MARL framework can
AsillustratedinFig.4, thehierarchicalconsensusmecha- seamlessly integrate with various MARL algorithms. To
nism enables us to derive the attention-weighted consensus, ensure fair comparisons with these variants of MAPPO, we
catt. This consensus serves as the agent’s inferred under- constructedourHC-MARLframeworkbasedontheMAPPO
i
standing of the global state, derived from partial observa- architecture in this experiment.H MC A- PM PA ORL 700 1500
400 HAPPO
600
1250
500
300 1000
400
750
200 300
200 500
100 100 250
0 HC-MARL 0 HC-MARL
0 MAPPO MAPPO
−100 HAPPO HAPPO
−250
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Steps 1e6 Steps 1e6 Steps 1e6
(a) 3 Predators - 1 Prey (b) 5 Predators - 1 Prey (c) 10 Predators - 1 Prey
Fig. 5: Learning curves of the HC-MARL, MAPPO, HAPPO on the Predator-Prey task. Each experiment was executed 5
times with different random seeds.
0 0 0 HC-MARL
MAPPO
HAPPO
−50 −50 −50
−100 −100 −100
−150 −150 −150
−200 −200
−200 HC-MARL HC-MARL
MAPPO MAPPO
HAPPO HAPPO
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Steps 1e7 Steps 1e7 Steps 1e7
(a) 3 Agents (b) 5 Agents (c) 10 Agents
Fig. 6: Learning curves of the HC-MARL, MAPPO, HAPPO on the Rendezvous task. Each experiment was executed 5
times with different random seeds.
1200 2000 HC-MARL
MAPPO
600 1000 1750 HAPPO
800 1500
400 600 1250
1000
400
200 750
200
500
0 HC-MARL 0 HC-MARL 250
MAPPO MAPPO
HAPPO HAPPO 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Steps 1e6 Steps 1e6 Steps 1e6
(a) 3 Agents (b) 5 Agents (c) 10 Agents
Fig.7:LearningcurvesoftheHC-MARL,MAPPO,HAPPOontheNavigationtask.Eachexperimentwasexecuted5times
with different random seeds.
B. Main Results of predators was set to 3, 5, and 10, respectively, with the
number of prey fixed at one. The prey’s escape trajectory
This section describes and analyzes the experimental re-
was randomly generated during both the training and testing
sults in three tasks. The performance of each algorithm was
phases.Fig.5a,5b,and5crespectivelyshowcasethelearning
evaluated with five different random seeds. The learning
curvesunderthesettingsofthree,five,andtenpredators.The
curves, in terms of episode reward under varying numbers
resultsdemonstratethatourwork,HC-MARL,surpassesthe
of agents, are presented in Fig. 5, 6, and 7. In addition
baseline algorithms in terms of episode reward convergence
to episode rewards, Table I compares the differences in
and convergence speed. Furthermore, Table I reveals that
algorithmperformancethroughthenumberofstepsrequired
HC-MARLrequiressignificantlyfewerstepstocompletethe
to complete the tasks after training. These results demon-
task post-training compared to the baseline algorithms, indi-
strate that our work, HC-MARL, achieved varying degrees
cating that our method accomplishes tasks more efficiently
of advantage over all baseline algorithms.
and enhances the performance of the algorithm.
Predator-Prey task. In this scenario, predators must
pursue and catch the prey through movement. The number Rendezvous task. In the Rendezvous task, where no
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipEs ap ge gc ri efi gc ateta fr rg oe mt p tho ein irts raa nr de omass ii ng in tie ad l, loa cg ae tn iots nsau ot non to hm eo mu asl py
.
−− 75 .. 50 kkkk====14816 Convergence Reward −− 75 .. 50 mmmm====13510 Convergence Reward
−10.0 −10.0
Thenumberofagentswassetto3,5,and10.FromFig.6,it −12.5 −12.5
can be observed that HC-MARL outperforms the baselines −15.0 −15.0
in terms of convergence episode rewards and convergence −17.5 −17.5
−20.0 −20.0
speed. The performance advantage of HC-MARL over the
−22.5 −22.5
baselinesbecomesmorepronouncedwithanincreasingnum- −25.0 3 Number of 5Agents 10 −25.0 3 Number of 5Agents 10
ber of agents. Analyzing this phenomenon, it is evident that (a) Influence of k (b) Influence of m
as the number of agents increases, and thus the complexity
Fig.9:AblationstudyonHC-MARLintheRendezvoustask
of the task escalates, the hierarchical consensus mechanism
contributes more significantly to enhancing the algorithm’s
which can be extended to GPS or other visual positioning
performance. Table I also demonstrates that HC-MARL
methods.
surpasses the baseline algorithms in terms of the number
WeconductedexperimentsonPredator-Prey,Rendezvous,
of steps required to complete the task.
and Navigation tasks. In the Predator-Prey scenario, the
Navigation task. In the navigationtask, agents are asked
HC-MARL algorithm required 16% fewer steps to capture
to navigate through two obstacles and reach the target point
the prey than MAPPO and 19% fewer than HAPPO. For
while avoiding collisions with other agents and obstacles.
the Rendezvous task, agents using HC-MARL completed
Fig.7demonstratesthatourHC-MARLmethodsignificantly
the gathering objective with 10% fewer steps compared to
improvesepisoderewardsacrosstaskswithvaryingnumbers
MAPPO and 15% fewer than HAPPO. In the Navigation
of agents. Specifically, HC-MARL’s episode rewards are
task, HC-MARL agents reached their target destinations
approximately20%higherthanthoseofHAPOandMAPPO
with 30% less distance traveled than MAPPO and 34%
in tasks with three agents, and about 35% higher in tasks
less than HAPPO, without any collisions with obstacles.
with ten agents. Furthermore, Table I indicates that at ten
Taking the Navigation task as an example, Fig. 10 displays
agents, the improvement in the number of steps required
four representative scenes captured during both simulated
to complete the obstacle navigation task is even more sub-
and real-world experiments. For a visual representation,
stantial. HC-MARL requires only 700 steps to complete the
supplementary videos can be found in the Appendix.
task, representing a reduction of 30% and 40% compared to
HAPO and MAPPO, respectively.
C. Ablation Study
In the HC-MARL framework, we employ a hierarchical
consensus mechanism to derive short-term and long-term
consensus. To assess the efficacy of both the consensus
mechanism and the hierarchical approach, we conducted (a) Start (b) Nearly Colliding
ablation studies varying the number of consensus categories
and consensus layers.
Initially, we investigated the effect of global consensus
categories k on the Rendezvous task, testing k values of 1,
4,8,and16acrossagentcountsof3,5,and10.Fig.9ashows
scenarios with k >1 yield higher convergence rewards than
those with k = 1, highlighting the consensus mechanism’s (c) Collision avoidance (d) Arrived
benefit. Optimal rewards for 3 and 5 agents occurred at k = Fig. 10: Navigation Task Demonstrations. Left is the real-
4; for 10 agents, k = 8 was most effective. This indicates world environment, and Right is the Webots simulation.
that simpler scenarios benefit from fewer categories, while
VI. CONCLUSION
moreagentsnecessitatemorecategoriesforoptimaltraining.
Additionally,theimpactofconsensuslayersmontraining We introduced the Hierarchical Consensus-Based Multi-
rewards was examined for m = 1 (no hierarchy), 3, 5, and Agent Reinforcement Learning (HC-MARL) framework, a
10, depicted in Fig. 9b. Optimal rewards were achieved at novel approach that employs hierarchical consensus to facil-
m = 5, suggesting that increasing consensus layers up to itate cooperative execution among agents based on local ob-
a point enhances task performance. However, beyond this servations. Recognizing that each agent’s local observations
optimal level, training efficiency declined, due to increased aresubsetsofaconsistentglobalstate,ourframeworklever-
training complexity and instability with additional layers. ages contrastive learning from these observations to achieve
a global consensus, which then serves as additional local
D. Real World Experiments
observations for the agents. By implementing a hierarchical
We validated the real-world applicability of HC-MARL mechanism, we construct short-term and long-term consen-
by conducting experiments on E-puck swarm. We utilized sus to cater to the dynamic requirements of various tasks.
the NOKOV motion capture system for indoor positioning, This process is further refined through an adaptive attention
draweR draweRTABLE I: Number of steps required to complete the Task for HC-MARL and baselines (after training) across three tasks.
Error bars indicate the standard error of the mean.
Steps Predator-Prey Rendezvous Navigation
Agents MAPPO HAPPO HC-MARL(ours) MAPPO HAPPO HC-MARL(ours) MAPPO HAPPO HC-MARL(ours)
3 720±60 740±50 580±45 575±25 585±35 550±25 635±55 630±70 520±40
5 550±65 640±60 510±55 640±35 645±40 610±40 710±60 680±70 590±65
10 520±60 530±60 450±55 670±35 695±45 620±45 960±60 890±75 700±65
mechanism that dynamically adjusts the influence of each [14] R.Olfati-SaberandR.M.Murray,“Consensusproblemsinnetworks
consensus layer. Extensive experiments demonstrate that the of agents with switching topology and time-delays,” IEEE Transac-
tionsonautomaticcontrol,vol.49,no.9,pp.1520–1533,2004.
HC-MARL method significantly enhances the performance
[15] R. Olfati-Saber, “Flocking for multi-agent dynamic systems: Algo-
of multi-robot cooperation tasks. rithmsandtheory,”IEEETransactionsonautomaticcontrol,vol.51,
no.3,pp.401–420,2006.
VII. ACKNOWLEDGMENT [16] T.Vicsek,A.Cziro´k,E.Ben-Jacob,I.Cohen,andO.Shochet,“Novel
typeofphasetransitioninasystemofself-drivenparticles,”Physical
We gratefully acknowledge the support of the National reviewletters,vol.75,no.6,p.1226,1995.
Key R&D Program of China (2022ZD0116401) and the [17] Y.LiandC.Tan,“Asurveyoftheconsensusformulti-agentsystems,”
Systems Science & Control Engineering, vol. 7, no. 1, pp. 468–482,
National Natural Science Foundation of China (Grant No.
2019.
62306023). [18] W.RenandR.W.Beard,“Consensusseekinginmultiagentsystems
under dynamically changing interaction topologies,” IEEE Transac-
REFERENCES
tionsonautomaticcontrol,vol.50,no.5,pp.655–661,2005.
[19] X. Yu, W. Wu, P. Feng, and Y. Tian, “Swarm inverse reinforcement
[1] C.Berner,G.Brockman,B.Chan,V.Cheung,P.Dkbiak,C.Dennison,
learningforbiologicalsystems,”in2021IEEEInternationalConfer-
D.Farhi,Q.Fischer,S.Hashme,C.Hesse,etal.,“Dota2withlarge ence on Bioinformatics and Biomedicine (BIBM). IEEE, 2021, pp.
scaledeepreinforcementlearning,”arXivpreprintarXiv:1912.06680, 274–279.
2019. [20] Y.Gao,W.Wang,andN.Yu,“Consensusmulti-agentreinforcement
[2] A.Yahya,A.Li,M.Kalakrishnan,Y.Chebotar,andS.Levine,“Col- learning for volt-var control in power distribution networks,” IEEE
lective robot reinforcement learning with distributed asynchronous TransactionsonSmartGrid,vol.12,no.4,pp.3594–3604,2021.
guided policy search,” in 2017 IEEE/RSJ International Conference [21] Y.ZhangandM.M.Zavlanos,“Cooperativemulti-agentreinforcement
onIntelligentRobotsandSystems(IROS). IEEE,2017,pp.79–86. learning with partial observations,” IEEE Transactions on Automatic
[3] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mor- Control,2023.
datch, “Multi-agent actor-critic for mixed cooperative-competitive [22] A.Jaiswal,A.R.Babu,M.Z.Zadeh,D.Banerjee,andF.Makedon,“A
environments,” Advances in neural information processing systems, survey on contrastive self-supervised learning,” Technologies, vol. 9,
vol.30,2017. no.1,p.2,2020.
[4] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and [23] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola,
Y.Wu,“Thesurprisingeffectivenessofppoincooperativemulti-agent A. Maschinot, C. Liu, and D. Krishnan, “Supervised contrastive
games,”AdvancesinNeuralInformationProcessingSystems,vol.35, learning,”Advancesinneuralinformationprocessingsystems,vol.33,
pp.24611–24624,2022. pp.18661–18673,2020.
[5] M.Wen,J.Kuba,R.Lin,W.Zhang,Y.Wen,J.Wang,andY.Yang, [24] T. Wang and P. Isola, “Understanding contrastive representation
“Multi-agentreinforcementlearningisasequencemodelingproblem,” learning through alignment and uniformity on the hypersphere,” in
Advances in Neural Information Processing Systems, vol. 35, pp. International conference on machine learning. PMLR, 2020, pp.
16509–16521,2022. 9929–9939.
[6] J. Sheng, X. Wang, B. Jin, J. Yan, W. Li, T.-H. Chang, J. Wang, [25] K.Arulkumaran,M.P.Deisenroth,M.Brundage,andA.A.Bharath,
and H. Zha, “Learning structured communication for multi-agent “Deepreinforcementlearning:Abriefsurvey,”IEEESignalProcess-
reinforcementlearning,”AutonomousAgentsandMulti-AgentSystems, ingMagazine,vol.34,no.6,pp.26–38,2017.
vol.36,no.2,p.50,2022. [26] M.Laskin,A.Srinivas,andP.Abbeel,“Curl:Contrastiveunsupervised
[7] G.WenandB.Li,“Optimizedleader-followerconsensuscontrolusing representations for reinforcement learning,” in International confer-
reinforcementlearningforaclassofsecond-ordernonlinearmultiagent enceonmachinelearning. PMLR,2020,pp.5639–5650.
systems,” IEEE Transactions on Systems, Man, and Cybernetics: [27] D. Dwibedi, J. Tompson, C. Lynch, and P. Sermanet, “Learning ac-
Systems,vol.52,no.9,pp.5546–5555,2021. tionablerepresentationsfromvisualobservations,”in2018IEEE/RSJ
[8] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean international conference on intelligent robots and systems (IROS).
fieldmulti-agentreinforcementlearning,”inInternationalconference IEEE,2018,pp.1577–1584.
onmachinelearning. PMLR,2018,pp.5571–5580. [28] L.Chen,Y.Wang,Y.Mo,Z.Miao,H.Wang,M.Feng,andS.Wang,
[9] J.Cook,N.Oreskes,P.T.Doran,W.R.Anderegg,B.Verheggen,E.W. “Multiagent path finding using deep reinforcement learning coupled
Maibach,J.S.Carlton,S.Lewandowsky,A.G.Skuce,S.A.Green, withhotsupervisioncontrastiveloss,”IEEETransactionsonIndustrial
et al., “Consensus on consensus: a synthesis of consensus estimates Electronics,vol.70,no.7,pp.7032–7040,2022.
on human-caused global warming,” Environmental research letters, [29] Z. Xu, B. Zhang, D. Li, Z. Zhang, G. Zhou, H. Chen, and G. Fan,
vol.11,no.4,p.048002,2016. “Consensuslearningforcooperativemulti-agentreinforcementlearn-
[10] S. Suzuki, R. Adachi, S. Dunne, P. Bossaerts, and J. P. O’Doherty, ing,”inProceedingsoftheAAAIConferenceonArtificialIntelligence,
“Neural mechanisms underlying human consensus decision-making,” vol.37,no.10,2023,pp.11726–11734.
Neuron,vol.86,no.2,pp.591–602,2015. [30] Y. Liu, Q. Yan, and A. Alahi, “Social nce: Contrastive learning
[11] J.Qin,Q.Ma,Y.Shi,andL.Wang,“Recentadvancesinconsensusof of socially-aware motion representations,” in Proceedings of the
multi-agentsystems:Abriefsurvey,”IEEETransactionsonIndustrial IEEE/CVF International Conference on Computer Vision, 2021, pp.
Electronics,vol.64,no.6,pp.4972–4983,2016. 15118–15129.
[12] P. H. Le-Khac, G. Healy, and A. F. Smeaton, “Contrastive represen- [31] F.A.Oliehoek,C.Amato,etal.,Aconciseintroductiontodecentral-
tation learning: A framework and review,” Ieee Access, vol. 8, pp. izedPOMDPs. Springer,2016,vol.1.
193907–193934,2020. [32] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski,
[13] A.AmirkhaniandA.H.Barshooi,“Consensusinmulti-agentsystems: and A. Joulin, “Emerging properties in self-supervised vision trans-
areview,”ArtificialIntelligenceReview,vol.55,no.5,pp.3897–3935, formers,” in Proceedings of the IEEE/CVF international conference
2022. oncomputervision,2021,pp.9650–9660.[33] O.Michel,“Cyberboticsltd.webots™:professionalmobilerobotsim-
ulation,” International Journal of Advanced Robotic Systems, vol. 1,
no.1,p.5,2004.
[34] M. Hu¨ttenrauch, A. Sˇosˇic´, and G. Neumann, “Deep reinforcement
learningforswarmsystems,”JournalofMachineLearningResearch,
vol.20,no.54,pp.1–31,2019.
[35] Y. Zhong, J. G. Kuba, X. Feng, S. Hu, J. Ji, and Y. Yang,
“Heterogeneous-agentreinforcementlearning,”2023.