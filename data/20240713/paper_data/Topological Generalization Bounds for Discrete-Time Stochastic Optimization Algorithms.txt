Topological Generalization Bounds for Discrete-Time
Stochastic Optimization Algorithms
Rayna Andreeva∗ r.andreeva@sms.ed.ac.uk
School of Informatics, University of Edinburgh, UK
Benjamin Dupuis∗ benjamin.dupuis@inria.fr
INRIA - D´epartement d’Informatique de l’Ecole Normale Sup´erieure / PSL Research Univ, France
Rik Sarkar rsarkar@inf.ed.ac.uk
School of Informatics, University of Edinburgh, UK
Tolga Birdal† tbirdal@imperial.ac.uk
Deparment of Computing, Imperial College London, UK
Umut S¸im¸sekli† umut.simsekli@inria.fr
INRIA - D´epartement d’Informatique de l’Ecole Normale Sup´erieure / PSL Research Univ, France
Abstract
Wepresentanovelsetofrigorousandcomputationallyefficienttopology-basedcomplexity
notions that exhibit a strong correlation with the generalization gap in modern deep
neural networks (DNNs). DNNs show remarkable generalization properties, yet the
source of these capabilities remains elusive, defying the established statistical learning
theory. Recent studies have revealed that properties of training trajectories can be
indicative of generalization. Building on this insight, state-of-the-art methods have
leveragedthetopologyofthesetrajectories,particularlytheirfractaldimension,toquantify
generalization. Most existing works compute this quantity by assuming continuous- or
infinite-time training dynamics, complicating the development of practical estimators
capable of accurately predicting generalization without access to test data. In this
paper, we respect the discrete-time nature of training trajectories and investigate the
underlying topological quantities that can be amenable to topological data analysis tools.
This leads to a new family of reliable topological complexity measures that provably
boundthegeneralizationerror, eliminatingtheneedforrestrictivegeometricassumptions.
These measures are computationally friendly, enabling us to propose simple yet effective
algorithms for computing generalization indices. Moreover, our flexible framework can
be extended to different domains, tasks, and architectures. Our experimental results
demonstrate that our new complexity measures correlate highly with generalization error
in industry-standards architectures such as transformers and deep graph networks. Our
approach consistently outperforms existing topological bounds across a wide range of
datasets, models, and optimizers, highlighting the practical relevance and effectiveness of
our complexity measures.
∗Authorscontributedequally.
†Authorscontributedequally.
1
4202
luJ
11
]GL.sc[
1v32780.7042:viXra1 Introduction
Generalization, a hallmark of model efficacy, is one of the most fundamental attributes for certifying
any machine learning model. Modern deep neural networks (DNN) display remarkable generalization
abilities that defy the current wisdom of machine learning (ML) theory [86, 87]. The notion can be
formalized through the risk minimization problem, which consists of minimizing the function:
(w):=E [ℓ(w,z)], (1)
R
z∼µz
where z := denotes the data, distributed according to a probability distribution µ on the
z
∈Z X ×Y
data space . In practice, as µ is unknown, ML algorithms focus on minimizing the empirical risk,
z
Z
n
1 (cid:88)
(cid:98)S(w)= ℓ(w,z i), (2)
R n
i=1
where S := (z ,...,z ) µ⊗n are independent samples from µ . In many applications, the
1 n ∼ z z
minimization of (2) is achieved by discrete stochastic optimization algorithms, such as stochastic
gradient descent (SGD) or the ADAM [40] method. Such algorithms generate a sequence of iterates
in Rd, denoted := w , which depends on the data S, the initialization w Rd, and
WS { k }k≥0 0 ∈
some additional randomness U, e.g., the random batch indices in SGD. The generalization error
characterizing the model’s performance is then defined as:
G S(w k):= (w k) (cid:98)S(w k). (3)
R −R
The empirical risk (2) typically has numerous local minima, which raises the question of how to
characterize their generalization properties. Recently, training trajectories (cf., Figure 1a) have been
shown to be paramount to answer this question [85, 27]. Indeed, these trajectories can quantify the
quality of a local minimum in a compact way, because they depend simultaneously on the algorithm,
the hyperparameters, and the data, which is crucial for obtaining satisfactory bounds [29]. A wide
family of trajectory-dependent bounds has been developed [85, 27, 50, 4, 36]. For instance, several
results on stochastic gradient Langevin dynamics [57, 64, 49], continuous Langevin dynamics [57]
and SGD [59] take into account the impact of the whole trajectory on the generalization error.
Parallel to these developments, several studies have brought to light the empirical links between
topological properties of DNNs and their generalization performance [58, 52, 66, 70, 84], hereby
making new connections with topological data analysis (TDA) tools [2]. These studies focus on the
structural changes across the different layers of the network [51] or on the final trained network
[66, 70, 84], and are almost exclusively empirical. This partially inspired a new class of trajectory-
dependent bounds focusing on topological properties of the trajectories. In particular, recent studies
[78, 20, 35, 21, 9, 3] have proposed to relate the generalization error to various kinds of intrinsic
fractal dimensions [25, 53] that characterize the learning trajectory. Informally, these bounds provide
the guarantee that with probability at least 1 ζ, we have:1
−
(cid:114)
dim( )+IT+log(1/ζ)
sup G (w)≲ WS , (4)
S
n
w∈WS
where dim( ) denotes various equivalent fractal dimensions, in particular the persistent homology
S
W
dimension(PH-dim)[9,20]andthemagnitudedimension[3]. ThetermITisaninformation-theoretic
quantity that takes different forms among different studies. Despite providing rigorous links between
1Weuse≲ininformalstatementstoindicatethatabsoluteconstantsand/orsmalltermsaremissing.
21.4 0.8
1.2
0.6
1.0
0.8 0.4
0.6 0.2
000 ... 024 (cid:30) ρ (cid:30)S· ·,(cid:30) (cid:30)η, ,η η== =101 1−0 06− −6
4
−0 0. .0 2 d Ei 1m (P oH ur[ s8 )] PMag(√n)(ours)
0.2 ρS,η=10−4
−
0.6 0.8 1.0 1.2 1.4 1.6
dim-1
(a) MDS Trajectory Embeddings (b) Distance Matrices of Trajectories (c) Our Topological Generalization Measures
Figure 1: We devise a novel class of complexity measures that capture the topological properties of
discrete training trajectories. These generalization bounds correlate highly with the test performance
for a variety of deep networks, data domains and datasets. Figure shows different trajectories (a)
embedded using multi-dimensional scaling based on the distance-matrices (b) computed using either
the Euclidean distance ( ) between weights as in [9] or via the loss-induced pseudo-metric (ρ ) as
2 S
∥·∥
in [20]. (c) plots the average granulated Kendall coefficients for two of our generalization measures
(E and PMag(√n)) in comparison to the state-of-the-art persistent homology dimensions [9, 20]
α
for a range of models, datasets, and domains, revealing significant gains and practical relevance.
the topology of the trajectory and generalization, these bounds have major drawbacks. First and
foremost, as noted in [75, 76, 12], fractal-trajectory bounds, such as Equation (4), do not apply
to discrete-time algorithms. This creates a discrepancy between these theoretical results and the
TDA-inspired methods to numerically evaluate them on commonly used discrete algorithms [9, 20, 3].
Additionally, existing bounds rely on very intricate geometric assumptions, such as Ahlfors-regularity
[78, 35] or geometric stability [20], that are not realistic in a practical, discrete setting.
Previous attempts were made to address this discretization issue. Specifically, under the
assumption that the training dynamics possess a stationary measure µ∞ for T (T is the
w|S → ∞
number of iterations), it was shown in [12] that with probability 1 ζ over S µ⊗n and w µ∞ :
− ∼ z ∼ w|S
(cid:114)
dim(µ )+IT+log(1/ζ)
G (w)≲ w|S , (5)
S
n
where dim(µ ) corresponds to the fractal dimension of the measure µ (see [67] for formal
w|S w
definitions). Whilethiswasanimportantstep,thisboundonlybecomespracticallyrelevantwhenthe
numberofiterationsgrowstoinfinity,whichisneverattainedinreal-lifeexperiments. Otherattempts
make use of so-called finite fractal dimensions [71] or fine properties of the Markov transition kernels
associated with the dynamics [35]. However, these studies also rely on impractical assumptions and
involve intricate quantities which make them not amenable to numerical evaluation.
Despite the theoretical limitations of existing topology-dependent generalization bounds, TDA-
inspired tools have been developed to numerically estimate the proposed intrinsic dimensions in
practical settings. Two particular methods have emerged and successfully demonstrate correlation
with the generalization error, based on persistent homology [9, 20] (PH-dim) and metric space
magnitude [3] (magnitude dimension); these two dimensions are equivalent for compact metric spaces
[3]. Becauseofthelimitationsdiscussedabove,existingtheoriesdonotaccountfortheseexperiments,
conducted with finite-time discrete algorithms. Moreover, existing empirical studies [9, 20, 3, 78]
only consider very simple models and small (image) datasets. Because of their lack of theoretical
foundations, it is not clear whether they could be extended to more practical setups.
Contributions. In this paper, we investigate the building blocks of PH and magnitude dimensions,
in order to propose new topology-inspired generalization bounds that rigorously apply to widely
3
2-mid
ΨlladneKdetalunargegarevA
2(cid:31)01·R(cid:31)A-
FT ICiV
TiV 01RAFIC TiaC 01RAFIC TiaC 001RAFIC niwS 01RAFIC niwS 001RAFIC NCGdetaG TSINM egaShparG TSINMused discrete-time stochastic optimization algorithms, and experimentally test our new topological
complexities2 on practically relevant DNN architectures. Our detailed contributions are as follows:
• We start by establishing the first theoretical links between generalization and a new kind of
computationally thrifty topological complexity measure, the α-weighted lifetime sums [73, 74].
• We propose and elaborate on another novel topological complexity, positive magnitude (PMag), a
slightly modified version of magnitude [46, 55]. We rigorously link PMag with the generalization
error, by relying on a new proof technique. Overall, our generalization bounds, rooted in TDA,
admit the following generic form:
(cid:114)
(Topological complexity)+IT+log(1/ζ)
sup G (w)≲ .
S
n
w∈WS
• Wethenprovideaflexiblecomputationalimplementationbasedupondissimilaritymeasuresbetween
neural nets (Figure 1b), which enables quantifying generalization across different architectures and
models, without the need for domain or problem-specific analysis as done in [39, 7].
• Unlike existing trajectory-based studies [9, 20] operating on small models, our experimental
evaluation is extensive. We consider several vision transformers [19] and graph neural networks
(GNN)[30]trainedonmultipledatasetsspanningregularandirregulardatadomains(cf. Figure1c).
Our findings robustly demonstrate that the novel topological generalization measures we introduce
exhibit a strong correlation with test performance across diverse architectures, hyperparameters,
and data modalities actually used in practice.
All the proofs of the main results are presented in the appendix, along with addi-
tional experiments. We will make our entire implementation publicly available under:
https://github.com/rorondre/TDAGeneralization.
2 Technical Background
Our generalization indicators will be based upon α-weighted lifetime sums and magnitude, capturing
different topological features, as we shortly dicsuss below. Let (X,ρ) be a finite pseudometric space.
α-weighted lifetime sums. Persistent homology (PH) is an important concept in the analysis of
geometric complexes [10]. We focus on the persistent homology of degree 0 (PH0). Informally, it
consists in tracking the “connected components” of a finite set at different scales. We provide in
Sections A.3 and A.4 an overview of these notions. For simplicity, we present here an equivalent
formulation of the α-weighted lifetime sums based on minimum spanning trees (MST) [42, 73].
A tree over X is a connected acyclic undirected graph (a set of edges) whose vertices are the
points in X. Given an edge e linking the points a and b, we define its cost as e :=ρ(a,b). An MST
on X is a tree minimizing the total cost (cid:80) e. The α-weighted lifeti| m| e sums Eρ are then
T e∈T | | α
written as:
(cid:88)
α 0, Eρ(X):= eα.
∀ ≥ α e∈T | |
The celebrated persistent homology dimension (PH-dim) [1], of a compact pseudometric space (A,ρ)
is then defined as dimρ (A)=inf C >0, Y X finite, E (Y) C . the PH-dim has been
PH α≥0 {∃ ∀ ⊂ α ≤ }
proven to be related to generalization error for different pseudometrics ρ [9, 20].
2Ourterm“topologicalcomplexity”shouldnotbeconfusedwiththehomonymtopologicalinvariant.
4Magnitude. Magnitude is a recently introduced topological invariant [46] which encodes many
importantinvariantsfromgeometricmeasuretheoryandintegralgeometry[46,55,56]. Magnitudecan
beinterpretedastheeffectivenumberofdistinctpointsinaspace[46]. Fors>0,wedefineaweighting
of the modified space (X,sρ) as a map β : X R, such that a X, (cid:80) e−sρ(a,b)β(b) = 1.
→ ∀ ∈ b∈X
Given such a weighting β, the magnitude function of (X,sρ) is defined as
(cid:88)
Magρ(sX):= β(a). (6)
a∈X
The parameter s>0 should be interpreted as a “scale” through which we look at the set (X,ρ). We
presentinAppendixA.5additionalpropertiesofthisfunction. Notethatmagnitudeisusuallydefined
in metric spaces; we show in Appendix B.2 that we can seamlessly extend it to the pseudometric
setting. Magnitude can also be extended to (infinite) compact spaces [46, 55] and, as for PH,
an intrinsic dimension, the magnitude dimension, can be defined from magnitude as dimρ (A)=
Mag
lim logMag(sA). Itisknownthatdimρ anddimρ coincideforcompactmetricspaces[56,73,3].
s→∞ log(s) PH Mag
As a result, dimρ has also been proposed as a topological generalization indicator [3].
Mag
Total mutual information. Priorintrinsicdimension-basedstudiesreliedon“mixing”assumptions
([78, Assumption H5], [9, Assumption H1], [76, 12]) or various mutual information terms [35, 20] to
take into account the statistical dependence between the data and the training trajectory. Recently,
a new framework was proposed in [21] to unify these approaches by proving data-dependent uniform
generalizationboundsusingsimplerandsmallerinformation-theoretic(IT)terms. Byleveragingthese
methods, we derive new generalization bounds involving the same IT terms for all our introduced
topological complexities. More precisely, they take the form of a total mutual information between
the data S and the training trajectory . This term is denoted I (S, ) and measures the
S ∞ S
W W
dependence between S and . We refer to Appendix A.1 and [35, 82] for exact definitions.
W
3 Main Theoretical Results
We now introduce our learning-theoretic setup (Section 3.1) before delving into our main theoretical
results in Sections 3.2 and 3.3.
3.1 Mathematical setup
Random trajectories. The primary goal of our theory is to prove uniform generalization bounds
overthetrainingtrajectory w , k 0 . Wearemostlyinterestedinthebehaviornearlocalminima
k
of (cid:98)S. Tothisend, weobser{ vethet≥ raje} ctorybetweeniterationsτ andT, whereτ Nisthenumber
R ∈
of iterations before reaching (near) a local minimum and T τ is the total number of iterations.
≥
Therefore, we consider the set := w , τ i T , which we call the random trajectory. Note
τ→T i
W { ≤ ≤ }
that is a set, i.e., it does not contain any information about the time-dependence. Moreover,
τ→T
W
our setup allows the random times τ and T to depend on the data S through the choice of a stopping
criterion as opposed to being fixed predetermined times.
General Lipschitz conditions. The topological quantities described in Section 2, as well as the
intrinsic dimensions introduced in prior works [78, 9, 3, 20, 21], require a notion of distance between
parameters(inRd)tobecomputed. Inthecaseoffractal-basedgeneralizationbounds,twocaseshave
already been considered: the Euclidean distance [78] and the data-dependent pseudometric defined in
[20]. In our work, we emphasize that both examples are particular cases of a more general family of
pseudometrics on the parameter space Rd. In order to fully characterize this family of pseudometrics,
we define the data-dependent map L :Rd Rn by L (w)=(ℓ(w,z ),...,ℓ(w,z )). To fit into
S S 1 n
−→
our framework, a pseudometric must satisfy the following general Lipschitz condition.
5Definition 3.1 ((q,L,ρ)-Lipschitz continuity). For any pseudo-metric ρ on Rd and q 1, we will
say that ℓ is (q,L,ρ)-Lipschitz in w when w,w′ Rd, L (w) L (w′) Ln1/qρ(w≥ ,w′).
∀ ∈ ∥ S − S ∥q ≤
A wide variety of distances have been proposed to compare the weights of two DNNs [18]. The
above condition restricts our analysis to a family of pseudometrics containing the following examples.
Example 3.2 (Data-dependent pseudometrics). For any p 1, we define the pseudometrics
≥
ρ(p) (w,w′):=n−1/p L (w) L (w′) . The case ρ(1) corresponds to the “data-dependent pseudo-
S ∥ S − S ∥p S
(1)
metric” used in [20]; we will denote it ρ :=ρ .
S S
Example 3.3 (Euclidean distance). If ℓ(w,z) is L-Lipschitz continuous in w, i.e., ℓ(w,z) ℓ(w′,z)
| − |≤
L w w′ for all z, then ℓ is (p,L, )-Lipschitz continuous for every p 1.
∥ − ∥ ∥·∥2 ≥
Assumptions. Given an (q,L,ρ)-Lipschitz continuous (pseudo-)metric, our approach relies only on
a single assumption of a bounded loss function. For the case of the pseudometric ρ (Example 3.2),
S
this assumption is already made in [20, 21].
Assumption 1. We assume that the loss ℓ is bounded in [0,B], with B >0 a constant.
The boundedness of ℓ is classically assumed in the fractal / TDA literature [20, 35, 21]. In [20],
it is shown that the proposed theory seems to be experimentally valid even for unbounded losses.
Our experimental findings suggest that this observation also applies to our work.
3.2 Persistent homology related generalization bounds
Incontrasttoallexistingfractaldimension-basedbounds[78,9,12,20],weproposenewgeneralization
bounds that apply to practical discrete stochastic optimizers with a finite number of iterations. To
this end, our key idea involves replacing the intrinsic dimension with intermediary quantities that are
used to compute them numerically. Following [9, 3], this points us towards the two quantities, E
α
and Mag, defined in Section 2. We are now ready to state the first generalization bound in terms of
the α-weighted lifetime sums, where we denote Eρ for Eρ( ).
α α Wτ→T
Theorem 3.4. Let ρ be a pseudometric on Rd. Supposes that Assumption 1 holds and that ℓ is
(q,L,ρ)-Lipschitz, for q 1. Then, for all α [0,1], with probability at least 1 ζ, we have:
≥ ∈ −
(cid:118)
(cid:117) (cid:16) √ (cid:17)
(cid:117)2logEρ +αlog 8L n (cid:114)
(cid:116) α B 2B I ∞(S, τ→T)+log(1/ζ)
sup G (w ) 2B + +3B W .
S i
≤ n √n 2n
τ≤i≤T
The term I (S, ) is the total mutual information (MI) term that is defined in Sections
∞ τ→T
W
2 and A.1. It measures the statistical dependence between the random set and the data
τ→T
W
S µ⊗n. Such MI terms appear in previous works related to fractal-based generalization bounds
∼ z
[78, 12, 20, 35]. Our proof technique, presented in Appendix B.5, makes use of a recently introduced
PAC-Bayesian framework for random sets [21] to introduce this MI term. It is also shown in [21]
that the MI term I (S, ) is tighter than those appearing in the aforementioned works.
∞ τ→T
W
We highlight the fact that Theorem 3.4 is fundamentally different from the persistent homology
dimension (PH-dim) based bounds studied in [9, 20]. Indeed, while the growth of E for increasing
α
finite subsets of the trajectory are used in [9] to estimate the PH-dim, it does not provide any formal
linkbetweenthegeneralizationerrorandthevalueofE . Therefore, theabovetheoremcouldnotbe
α
cast as a corollary of these previous studies. Another important characteristic of the above theorem
(as well as the results of Section 3.3) is to be non-asymptotic, i.e., it is true for every n N∗. This is
∈
an improvement over the fractal dimensions-based bounds presented in [78, 9, 20, 21].
614
Euclidean distance E
1-pseudometric S 12 PMag( n)
103 01-pseudometric
10
102 8
6
101 4
2
100 0
100 101 102 103
0 20 40 60 80 100
Mag( n) Proportion of the data (%)
(a) Comparison of Mag and PMag. (b) Relative variation of E and Mag.
1
Figure 2: Left: Comparison of Mag and PMag (for s=√n), for different (pseudo)metrics (ViT on
CIFAR10). Right: relative variation of the quantities E ( ) and Mag(√n ), with respect
α τ→T τ→T
W W
(1)
to the proportion of the data used to estimated ρ (ViT on CIFAR10).
S
3.3 Positive magnitude (PMag) and related generalization bounds
Recent preliminary experimental results displayed a correlation between the generalization error
of DNNs and magnitude [3]. To provide a theoretical justification for this behavior, it would be
tempting to mimic the proof of Theorem 3.4 and build on existing covering arguments. However,
while lower bounds of magnitude in terms of covering numbers have been derived in [56], they appear
to be impractical in our case. Another possibility would be to use the magnitude dimension bounds
of [3]. Yet, this could not apply to our finite and discrete setting where the dimension is 0. Hence, we
identify a new quantity, closely related to magnitude, while being more relevant to learning theory.
With the notations of Section 2, we fix a finite metric space (X,ρ) and a weighting β :X R of
s
−→
(X,sρ), where s>0 is a “scale” parameter. We define the positive magnitude as
(cid:88)
s>0, PMagρ(sX):= β (a) , (7)
s +
∀ a∈X
where x := max(x,0) denotes the positive part of x. To avoid harming the readability of the
+
paper, we refer to Appendix B.3 for the extension of PMag to the pseudometric case. Based on a
new theoretical approach, we prove that the positive magnitude can be used to upper bound the
generalization error (see the proof in Appendix B.7). This leads to the following theorem:
Theorem 3.5. Let ρ be a pseudometric such that ( ,λρ) admits a positive magnitude (according to
W
Definition B.5) for every λ>0. We assume that ℓ is (q,L,ρ)-Lipschitz continuous with q 1. Then,
≥
for any s>0, we have with probability at least 1 ζ that
−
(cid:114)
2 B2 I (S, )+log(1/ζ)
sup G (w ) logPMagρ(Ls )+s +3B ∞ Wτ→T .
S i τ→T
≤ s W n 2n
τ≤i≤T
The IT term (I ) in the above result is the same as in Theorem 3.4. Given a fixed (finite)
∞
set and a big enough s, we establish Mag(s )=PMag(s ). Moreover, we present in Figure
W W W
2a an empirical comparison of Mag and PMag, showing a small and almost monotonic relation
between both quantities. Therefore, Theorem 3.5 may be seen as the first theoretical justification of
the empirical relationship between magnitude and the generalization error observed in [3].
Anaturalchoiceforthescaleswouldbes √n,ensuringaconvergencerateinn−1/2. However,
≈
our empirical evaluations (see Section 5, in particular, Table 1) revealed that small values of s (we
7
)n
(gaMP
)%(
noitairav
evitaleRTable 1: Correlation coefficients associated with the different topological complexities.
Model-dataset ViT-CIFAR10 Swin-CIFAR100 GraphSage-MNIST GatedGCN-MNIST
Compl.-Metric ψlr ψbs Ψ τ ψlr ψbs Ψ τ ψlr ψbs Ψ τ ψlr ψbs Ψ τ
dim PH√-ρS [20] 0.93 -0.67 0.13 0.61 0.69 -0.47 0.11 0.50 -0.28 -0.26 -0.27 -0.35 0.15 0.07 0.11 -0.06
Mag( n)-ρS 0.68 0.62 0.65 0.64 0.56 0.47 0.51 0.53 0.69 0.71 0.70 0.79 0.85 0.97 0.91 0.88
Mag(0.0 √1)-ρS 0.41 0.58 0.50 0.47 0.31 0.47 0.39 0.33 0.24 0.10 0.17 0.36 0.35 0.35 0.35 0.49
PMag( n)-ρS 0.91 0.67 0.79 0.85 0.69 0.47 0.58 0.62 0.59 0.46 0.53 0.59 0.73 0.97 0.85 0.84
PMag(0.01)-ρS 0.86 0.40 0.50 0.80 0.71 0.58 0.64 0.68 0.24 0.10 0.17 0.36 0.35 0.35 0.35 0.49
Eα-ρS 0.95 0.67 0.81 0.86 0.69 0.47 0.58 0.62 0.67 0.74 0.70 0.77 0.48 0.97 0.72 0.74
dim -∥·∥ [9] 0.93 -0.67 0.13 0.61 0.69 -0.47 0.34 0.51 0.32 0.81 0.56 0.51 -0.12 0.70 0.29 0.33
PH√ 2
Mag( n)-∥·∥ [3] 0.95 -0.59 0.13 0.73 0.71 -0.57 0.07 0.53 0.75 0.77 0.76 0.61 0.77 0.76 0.77 0.52
2
Mag(0.01)-∥·∥ [3] 0.95 -0.60 0.17 0.72 0.69 -0.44 0.12 0.53 0.75 0.74 0.74 0.60 0.77 0.42 0.60 0.47
√ 2
PMag( n)-∥·∥ 0.95 -0.59 0.18 0.73 0.71 -0.57 0.07 0.53 0.75 0.74 0.74 0.60 0.77 0.93 0.85 0.54
2
PMag(0.01)-∥·∥ 0.55 0.71 0.63 0.58 0.64 0.51 0.58 0.46 0.75 -0.05 0.35 0.51 0.60 -0.47 0.06 0.26
2
Eα-∥·∥
2
0.95 -0.31 0.32 0.76 0.63 0.75 0.74 0.74 0.75 0.74 0.74 0.60 0.77 0.93 0.84 0.54
dim -01[20] 0.95 -0.20 0.37 0.72 0.64 0.04 0.34 0.51 0.0 -0.13 -0.07 0.0 0.14 0.00 0.07 0.00
PH√
Mag( n)-01 0.95 0.67 0.81 0.88 0.69 0.47 0.58 0.62 0.64 0.68 0.66 0.75 0.78 0.85 0.82 0.82
Mag(0.01)-01 0.84 0.33 0.59 0.75 0.61 0.27 0.44 0.50 0.13 0.11 0.12 0.26 0.10 0.10 0.10 0.25
√
PMag( n)-01 0.95 0.64 0.80 0.89 0.69 0.47 0.58 0.62 0.63 0.65 0.64 0.74 0.76 0.83 0.79 0.80
PMag(0.01)-01 0.84 0.36 0.60 0.76 0.65 0.49 0.57 0.54 0.13 0.11 0.12 0.26 0.10 0.10 0.10 0.25
Eα-01 0.95 0.67 0.81 0.87 0.69 0.47 0.58 0.61 0.63 0.68 0.66 0.74 0.78 0.85 0.82 0.82
typically use s=10−2) can also provide good correlation with the generalization error. This could be
explained by the fact that PMag(s ) 1 as s 0, i.e., the bound may not diverge when s 0.
W → → →
For our topological complexities to be computationally efficient, we focus our experiments on fixed
values of s (in
(cid:8) √n,10−2(cid:9)
). We will omit the trajectory and denote Mag(s) and PMag(s).
4 Computational Considerations
We now detail the numerical estimation of the topological complexities mentioned above.
Computation of E . WecomputeE byusingthegiotto-phlibrary[65,6]. Thissetupisinspired
α α
by PH frameworks used in [9, 20]. This technique uses the equivalent formulation of E in terms of
α
PH (see Appendix A.3 for details). Theorem 3.4, and its proof (presented in Appendix B.6) suggest
that the relevant value of α is 1; similar to [9], this is what we used in our experiments.
Computation of Mag and PMag. Differentmethodsexisttoevaluatemagnitude[47]. Weusethe
Krylov approximation method [72], which is based on pre-conditioned conjugate gradient iteration,
implemented in the Python library krypy.linsys.Cg to solve for the magnitude weights. We then
sum over the weights to compute Mag, and sum over the positive weights to obtain PMag.
Distance matrix estimation.. Given a finite set (i.e., a trajectory) Rd, the calculation of
W ⊂
our topological complexities requires computing the distance matrix D
ρ
:=(ρ(w,w′)) w,w′∈W. For
large DNNs, this may become challenging. Depending on ρ, we propose the following solutions.
• Case 1: If ρ is the Euclidean distance on Rd, for large DNNs (in our case for the transformer
experiments)storingthewholetrajectoryischallenging. Instead, weusesparserandomprojections
inspired by the Johnson-Lindenstrauss lemma [83] to map the trajectories onto a lower-dimensional
subspace. We use the scikit-learn [63] impleementation so that, with high probability, the
relative variation of the distance matrices is at most 5%, see Appendix A.7 for details.
• Case 2: If ρ is of the form ρ(q) as in Example 3.2, then the computation of D requires the
S ρ
evaluation of the model on the entire dataset at each iteration, which becomes intractable for large
(1)
DNNs. In [20, Figure 3], the authors show that the PH-dim based on the pseudometric ρ =ρ
S S
is very robust to a random subsampling of a training dataset, i.e. when ρ is replaced by ρ with
S B
8B S and B /S 1. Figure 2b shows that E and positive magnitude are also robust to this
α
⊆ | | | |≪
subsampling. We mainly used B /S =10%. We refer the reader to Appendix C.2 for details.
| | | |
Generalization error. Our theory, like many trajectory-based studies [78, 9, 20, 3] predicts upper
bounds on the worst-case generalization error over the trajectory . Yet, experiments in
τ→T
W
previous works mainly reported the error at the last iteration. To estimate the worst-case error
in a computationally feasible way, we periodically evaluated the test risk between times τ and T
(with a period of 100 iterations) and reported (worst test risk - final train risk) as the error.
This is consistent as we start the trajectory from a weight w already in a local minimum of
τ→T τ
W
the empirical risk. Our main conclusions are still valid if the final generalization gap is used. This
observation, which is to the best of our knowledge new, is briefly discussed in Appendix D.1.
5 Empirical Analysis
Setup. Given a DNN and a dataset, we start from a pre-trained weight vector w , yielding high
τ
training accuracy on classification tasks. By varying the learning rate (η) and the batch size (b),
we define a grid of 6 6 hyperparameters. For each pair (η,b), we compute the training trajectory
×
for 5 103 iterations. Unless specified, we use the ADAM optimizer [40]. Based on the set
τ→T
W ×
, we estimate distance matrices as described in Section 4. For the sake of clarity, we focus on 3
τ→T
W
relevantpseudometrics: (i)theEuclideandistance asin[9],(ii)thedata-dependentpseudometric
∥·∥2
ρ , used in [20, 3], and (iii) the 01-loss distance. For (ii), ρ is computed based on the surrogate
S S
loss used in training (e.g., the cross-entropy loss), while the reported generalization error is always
based on accuracy gap (01-loss), which is of interest in most applications (see Section 4). For the last
one (iii) ρ is defined as in Example 3.2, but with ℓ being the 01-loss; we call it 01-pseudometric and
denote it by 01 in the tables. This last setup matches exactly our theoretical requirements.
In terms of DNN architectures, we focus on practically relevant models, while previous studies
mainly considered small networks [9, 35, 20, 76]. We examine two different families of architectures.
The first family consists of vision transformers (ViT [80], CaiT [81], Swin [48], see Table 2), each
evaluated on both the CIFAR10 [44] and CIFAR100 [43] datasets. Moreover, we also tested our
theory on graph neural networks (GNN) architectures, namely GatedGCN [11] and GraphSage [32]
trained on the Super-pixel MNIST dataset [22]. To the best of our knowledge, this is the first time
these kinds of topological complexities have been evaluated on transformers and GNNs. We ran the
experiments on 18 NVIDIA 2080Ti (11 GB) GPUs.
Granulated Kendall’s coefficients.. We assess the correlation between our complexities and the
generalization error by using the granulated Kendall’s coefficients (GKC) [37]. While the classical
Kendall’s coefficients (KC) [38] (denoted τ) measures the correlation between two quantities, it
may fail to capture their causal relationship. Instead, one “granulated” coefficient is defined in [37]
for each hyperparameter (i.e., ψ for η and ψ for b); it measures the correlation when only
LR BS
this hyperparameter is varying. In Table 1, we report τ, ψ and ψ , and the averaged GKC,
LR BS
Ψ:=(ψ +ψ )/2, for several models, datasets and topological complexities. In Figures 4a and
LR BS
4b, we represent our topological complexities in the plane (ψ ,ψ ); the red square indicates the
BS LR
region of best correlation (the coefficients are in [ 1,1], their sign is the sign of the correlation).
−
5.1 Analysis
As explained above, we focus our main experiments on the quantities E , Mag(√n), PMag(√n),
1
Mag(10−2) and PMag(10−2), each computed for the 3 pseudometrics discussed above ( , ρ ,
∥·∥2 S
01). In the interest of comparison, we also compute the PH-dim (proposed in [9] for the and in
∥·∥2
[20] for ρ ), which is thus tested for the first time on transformers and GNNs.
S
92.2×100
Batch size
103
Batch size
103 103
2×100 8 8 102
16 16
1.8×100 32 103 32
64 64 1.6×100 1 22 58
6
104 1 22 58
6
104 101 Batch 8size 104
1.4×100 16
102 100 3 62
4
1.2×100 128
256
105 105 105
6 8 10 6 8 10 6 8 10
Generalization gap Generalization gap Generalization gap
Figure 3: ρ -based complexity measures vs. generalization gap for a ViT trained on CIFAR10:
S
dim (left), PMag(√n) (middle), and E (right).
PH 1
Performance on vision transformers. We see in Table 1 and Figure 3 that our proposed
topological complexities consistently outperform the PH dimensions across several vision transformer
models and datasets. This suggests that PH-dim, previously tested only on small architectures, is
less scalable to industry-standards models with more parameters. Figure 4a, including all (model,
dataset) pairs for the pseudometric ρ , reveals important observations. First, we notice that the
S
GKC of our topological complexities are both positive and close to 1, indicating that they are indeed
good measures of generalization. We note that for most models and datasets, dim has a small or
PH
negative ψ , indicating that it has less ability to explain generalization for varying batch-sizes. As
BS
it was observed in [20] for PH-dim, our complexities computed from the pseudometric ρ correlate
S
very well with the generalization gap while this gap is based on the 01 loss.
Performance on GNNs. An important aspect of our framework is the ability to seamlessly
encapsulate different data domains. In particular, the possibility of using different pseudometrics can
helpdefinetopologicalcomplexitiesthatnaturallytakeintoaccounttheinternalsymmetriesofGNNs,
without any model-specific analysis [39, 7]. The results of Table 1 and Figure 4a confirm that our
proposed topological complexities outperform PH-dim and correlate strongly with the generalization
error for GNNs. Additionally, it may be observed that Mag(√n) performs significantly well for
GNNs,andinparticularbetterthanPMag(√n). Thispointsustowardstheideathatfurthertheory
would be desirable to formally relate magnitude to the generalization error in that case3.
Comparison of the topological complexities. In Table 1 and Figures 3 and 4a, it can be
seen that E and PMag(√n) perform equally well for the image and graph experiments across
1
multiple datasets, models, and data domains. We see in Table 1 that most topological complexities
perform better with data-dependent metrics (i.e., ρ and 01) than with the Euclidean distance,
S
for transformer-based experiments. This extends results obtained for PH-dim in [20], for smaller
architectures. However, the poor performance of Euclidean-based complexities may also be partially
caused by the projections applied to the Euclidean distance matrices to make them memory-wise
computable (see Section 4). This is a remaining limitation of our algorithms. On the other hand, the
01 and ρ data-dependent pseudometrics seem to yield similar performance in all experiments.
S
Ablations. In Figure 4b, we reveal that changing the optimizer has little effect on the observed
correlation (for the same model and dataset). Interestingly, we note that the PH-dim, computed with
pseudometric ρ and obtained from the SGD trajectories, exhibits high GKCs. This observation
S
agrees with the results in [20]. Figure 3 further displays the typical behavior of several topological
complexities for ViT and CIFAR10. In addition to the correlation of our proposed complexities being
stronger than for the PH-dim, we observe that E and PMag(√n) seem to better correlate with the
α
generalization gap for small learning rates. Finally, it is consistently observed in Table 1 and Figures
4a and 4b that using a relatively high value of the (positive) magnitude scale (s=√n) yields better
3Weshallunderlinethat,whileMagwiththeEuclideandistancewasempiricallyproposedasacomplexitymeasure
in[3],atheoreticaljustificationforMagresultsinTable1isstillmissingformoderatevaluesofs.
10
mid
hp
etar
gninraeL
edutingam
evitisop
etar
gninraeL ahpla
E
etar
gninraeL1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25 Optimizers - metric
Model - dataset SGD- S
0.00 ViT - CIFAR10 0.00 SGD-Eucl.
ViT - CIFAR100 SGD-01
0.25 CaiT - CIFAR10 0.25 ADAM- S
CaiT - CIFAR100 Topological quantities ADAM-Eucl. Topological quantities
0.50 Swin - CIFAR10 E1 0.50 ADAM-01 E1
0.75 Swin - CIFAR100 PMag( n) 0.75 RMSprop- S PMag( n)
GatedGCN - MNIST PMag(0.01) RMSprop-Eucl. PMag(0.01)
1.00 GraphSage - MNIST dimPH 1.00 RMSprop-01 dimPH
1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
bs bs
(a) ρ(1) pseudometric (b) Comparison of optimizers for ViT on CIFAR10.
S
Figure 4: Granulated Kendall coefficients for several models, datasets and topological quantities.
Note that our framework is directly applicable to graph networks.
correlations than small values (s=10−2). However, both cases still provide satisfying correlation,
comforting the robustness of magnitude as a generalization indicator.
Due to limited space, we present all the correlation coefficient of one transformer model ViT for
CIFAR10andSwinforCIFAR100inTable1asillustrativeexamplesforeachdataset. Theremaining
results appear in the Appendix, Tables 4, 6, 3 and 5, and they all follow a similar trend. Further
empirical results and illustrations of this behavior are provided in Appendix D.
On topological complexity & generalization. Contrary to the claims in [79], which are based
on the limited experimental settings of [20, 9], our findings on an extensive set of evaluations on
modern architectures decisively demonstrate that topological complexity measures serve as robust
proxies for generalization. While fractal dimension shows some correlation with generalization, our
rigorous discrete-time topological measures exhibit a significantly stronger correlation, evidenced by
the high granulated coefficients. This promising outcome underscores the need for future research in
exploring deeper connections between network dynamics, topology, and generalization.
6 Conclusion
Inthispaper,weprovednovelgeneralizationboundsbasedonseveraltopologicalcomplexitiescoming
from TDA, namely α-weighted lifetime sums and a new variant of metric space magnitude, which we
called positive magnitude. Compared to previous studies, we require fewer assumptions and operate
in a discrete setting in which our proposed quantities are fully computable. Our algorithms are
flexible enough to be seamlessly integrated with diverse data domains and tasks. These advantages
of our framework allowed us to create a computationally cheap experimental setup, as close as
possibletothetheoreticalsetup. Wethusprovidedacomprehensivesuiteofexperimentswithseveral
industry-relevant architectures across vision transformers and graph neural networks, which have not
been explored yet in this literature. We show that our proposed topological complexities correlate
well with the generalization error, outperforming the previously studied intrinsic dimensions.
Limitations & future work. The main limitation of our theory is the lack of understanding of
the IT terms, while they are still smaller than most prior works. Moreover, a better understanding
of the behavior of positive magnitude for small values of the scale factor s would be a necessary
improvement. Regardingourexperiments,arefinementoftheestimationtechniquesofthetopological
complexities would be beneficial. Despite experimenting with practically relevant architectures, our
future works also include scaling up our empirical analysis to include larger models and datasets, in
particular large language models, which are still beyond the scope of this study.
11
rl rlBroader impact. Certifying generalization is key for safe and trusted AI systems, hence we believe
that our study may have a positive societal impact on the general use of deep networks.
Acknowledgments
R.A. is supported by the United Kingdom Research and Innovation (grant EP/S02431X/1), UKRI
Centre for Doctoral Training in Biomedical AI at the University of Edinburgh, School of Informatics.
U.S¸. is partially supported by the French government under management of Agence Nationale
de la Recherche as part of the “Investissements d’avenir” program, reference ANR-19-P3IA-0001
(PRAIRIE 3IA Institute). B.D. and U.S¸. are partially supported by the European Research Council
Starting Grant DYNASTY – 101039676. T.B. is partially supported by the Royal Society Research
Grant RG R1 241402.
\ \
References
[1] Henry Adams, Manuchehr Aminian, Elin Farnell, Michael Kirby, Joshua Mirth, Rachel Neville,
Chris Peterson, and Clayton Shonkwiler. A fractal dimension for measures via persistent
homology. In Topological Data Analysis: The Abel Symposium 2018, pages 1–31. Springer, 2020.
4, 22
[2] Henry Adams and Michael Moy. Topology applied to machine learning: From global to local.
Frontiers in Artificial Intelligence, 4:668302, 2021. 2
[3] Rayna Andreeva, Katharina Limbeck, Bastian Rieck, and Rik Sarkar. Metric space magnitude
and generalisation in neural networks. In Proceedings of 2nd Annual Workshop on Topology,
Algebra, and Geometry in Machine Learning (TAG-ML), volume 221 of Proceedings of Machine
Learning Research, pages 242–253. PMLR, 2023. 2, 3, 5, 6, 7, 8, 9, 10, 18, 28, 41, 43, 45, 47, 49
[4] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-Grained Analysis
of Optimization and Generalization for Overparameterized Two-Layer Neural Networks, May
2019. 2
[5] Peter Bartlett and Shahar Mendelson. Rademacher and Gaussian Complexities: Risk Bounds
and Structural Results. Journal of Machine Learning Research, 2002. 18
[6] Ulrich Bauer. Ripser: Efficient computation of Vietoris-Rips persistence barcodes. Journal of
Applied and Computational Topology, 5(3):391–423, September 2021. 8
[7] Arash Behboodi, Gabriele Cesa, and Taco S Cohen. A pac-bayesian generalization bound for
equivariant networks. Advances in Neural Information Processing Systems, 35:5654–5668, 2022.
4, 10
[8] SubhrajitBhattacharya,RobertGhrist,andVijayKumar. Persistenthomologyforpathplanning
in uncertain environments. IEEE Transactions on Robotics, 31(3):578–590, 2015. 19
[9] Tolga Birdal, Aaron Lou, Leonidas J Guibas, and Umut Simsekli. Intrinsic dimension, persistent
homology and generalization in neural networks. Advances in Neural Information Processing
Systems, 34:6776–6789, 2021. 2, 3, 4, 5, 6, 8, 9, 11, 18, 19, 22, 28, 38, 39, 41, 42, 43, 45, 47, 49
12[10] Jean-Daniel Boissonat, Fr´ed´eric Chazal, and Mariette Yvinec. Geometrical and Topological
Inference. Cambridge Texts in Applied Mathematics. Cambridge University Press, 2018. 4, 19,
21, 22
[11] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint
arXiv:1711.07553, 2017. 9, 35, 36
[12] Alexander Camuto, George Deligiannidis, Murat A Erdogdu, Mert Gurbuzbalaban, Umut
Simsekli, and Lingjiong Zhu. Fractal structure and generalization properties of stochastic
optimization algorithms. Advances in neural information processing systems, 34:18774–18788,
2021. 3, 5, 6, 18, 24, 28
[13] Gunnar Carlsson. Topological pattern recognition for point cloud data*. Acta Numerica,
23:289–368, May 2014. 19
[14] Fr´ed´ericChazalandBertrandMichel. Anintroductiontotopologicaldataanalysis: fundamental
and practical aspects for data scientists. Frontiers in artificial intelligence, 4:108, 2021. 19
[15] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to
Algorithms. The MIT Press, 2nd edition, 2001. 22
[16] Ciprian A Corneanu, Meysam Madadi, Sergio Escalera, and Aleix M Martinez. What does it
mean to learn in deep networks? and, how does one detect adversarial attacks? In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4757–4766,
2019. 19
[17] Vin De Silva and Robert Ghrist. Coverage in sensor networks via persistent homology. Algebraic
& Geometric Topology, 7(1):339–358, 2007. 19
[18] Claire Donnat and Susan Holmes. Tracking network dynamics: a survey of distances and
similarity metrics, 2018. 6
[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale, June 2021. 4
[20] Benjamin Dupuis, George Deligiannidis, and Umut Simsekli. Generalization bounds using
data-dependent fractal dimensions. In International Conference on Machine Learning, pages
8922–8968. PMLR, 2023. 2, 3, 4, 5, 6, 8, 9, 10, 11, 18, 19, 22, 24, 25, 28, 38, 39, 41, 42, 43, 45,
47, 49
[21] Benjamin Dupuis, Paul Viallard, George Deligiannidis, and Umut Simsekli. Uniform generaliza-
tion bounds on data-dependent hypothesis sets via pac-bayesian theory on random sets, 2024. 2,
5, 6, 18, 24, 25, 28, 29
[22] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio,
and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning
Research, 24(43):1–48, 2023. 9, 36, 39
[23] Herbert Edelsbrunner and John Harer. Computational Topology - an Introduction Semantic
|
Scholar. American Mathematical Society, 2010. 19
13[24] Kevin Emmett, Benjamin Schweinhart, and Raul Rabadan. Multiscale topology of chromatin
folding. In 9th EAI International Conference on Bio-inspired Information and Communications
Technologies (formerly BIONETICS), pages 177–180, 2016. 19
[25] Kenneth Falconer. Fractal Geometry - Mathematical Foundations and Applications - Third
Edition. Wiley, 2014. 2, 22, 24, 30
[26] Carlos Fernandez-Granda. Lecture 5; random projections, 2016. 24
[27] Jingwen Fu, Zhizheng Zhang, Dacheng Yin, Yan Lu, and Nanning Zheng. Learning Trajectories
are Generalization Indicators, October 2023. 2
[28] Hanan Gani, Muzammal Naseer, and Mohammad Yaqub. How to train vision transformer on
small-scale datasets? arXiv preprint arXiv:2210.07240, 2022. 35
[29] Michael Gastpar, Ido Nachum, Jonathan Shafer, and Thomas Weinberger. Fantastic generaliza-
tion measures are nowhere to be found, 2023. 2
[30] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In
Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pages 729–734 vol. 2, 2005. 4
[31] Mustafa Hajij, Ghada Zamzmi, Theodore Papamarkou, Nina Miolane, Aldo Guzm´an-S´aenz,
Karthikeyan Natesan Ramamurthy, Tolga Birdal, Tamal K Dey, Soham Mukherjee, Shreyas N
Samaga, et al. Topological deep learning: Going beyond graph data. arXiv preprint
arXiv:2206.00606, 2022. 24
[32] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. Advances in neural information processing systems, 30, 2017. 9, 35
[33] Allen Hatcher. Algebraic Topology. Cambridge University Press, 2002. 19
[34] Yasuaki Hiraoka, Takenobu Nakamura, Akihiko Hirata, Emerson G Escolar, Kaname Matsue,
and Yasumasa Nishiura. Hierarchical structures of amorphous solids characterized by persistent
homology. Proceedings of the National Academy of Sciences, 113(26):7035–7040, 2016. 19
[35] Liam Hodgkinson, Umut S¸im¸sekli, Rajiv Khanna, and Michael W. Mahoney. Generaliza-
tion Bounds using Lower Tail Exponents in Stochastic Optimizers. Proceedings of the 39th
International Conference on Machine Learning, July 2022. 2, 3, 5, 6, 9, 18, 24
[36] Ahmed Imtiaz Humayun, Randall Balestriero, and Richard Baraniuk. Training Dynamics of
Deep Network Linear Regions. https://arxiv.org/abs/2310.12977v1, October 2023. 2
[37] YidingJiang, BehnamNeyshabur, HosseinMobahi, DilipKrishnan, andSamyBengio. Fantastic
Generalization Measures and Where to Find Them. ICLR 2020, December 2019. 9, 39
[38] Maurice G. Kendall. A new reasure of rank correlation. Biometrika, 1938. 9
[39] Bobak T Kiani, Thien Le, Hannah Lawrence, Stefanie Jegelka, and Melanie Weber. On the
hardness of learning under symmetries. arXiv preprint arXiv:2401.01869, 2024. 4, 10
[40] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. 2, 9
[41] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907, 2016. 35
14[42] Gady Kozma, Zvi Lotker, and Gideon Stupp. The minimal spanning tree and the upper box
dimension. Proceedings of the American Mathematical Society, 134(4):1183–1187, 2006. 4, 22, 28
[43] Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009. 9
[44] Alex Krizhevsky, Vinod Nair, and Geoffrey E. Hinton. The cifar-10 dataset, 2014. 9
[45] Gregory Leibon, Scott Pauls, Daniel Rockmore, and Robert Savell. Topological structures in the
equities market network. Proceedings of the National Academy of Sciences, 105(52):20589–20594,
2008. 19
[46] Tom Leinster. The magnitude of metric spaces. Documenta Mathematica, 18:857–905, 2013. 4,
5, 23
[47] Katharina Limbeck, Rayna Andreeva, Rik Sarkar, and Bastian Rieck. Metric space magnitude
for evaluating the diversity of latent representations, 2024. 8
[48] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedings of
the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 9, 35, 39
[49] Xuanyuan Luo, Luo Bei, and Jian Li. Generalization Bounds for Gradient Methods via Discrete
and Continuous Prior, October 2022. 2
[50] Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the Generalization Benefit of
Normalization Layers: Sharpness Reduction, January 2023. 2
[51] German Magai. Deep neural networks architectures from the perspective of manifold learning.
In 2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence
(PRAI), pages 1021–1031. IEEE, 2023. 2
[52] GermanMagaiandAntonAyzenberg. Topologyandgeometryofdatamanifoldindeeplearning.
arXiv preprint arXiv:2204.08624, 2022. 2
[53] Pertti Mattila. Geometry of Sets and Measures in Euclidean Spaces. Cambridge University
Press, 1999. 2, 24
[54] Pertti Mattila, Manuel Moran, and Jose-manual Rey. Dimension of a measure. Studia mathe-
matica 142 (3), 2000. 22
[55] Mark W Meckes. Positive definite metric spaces. Positivity, 17(3):733–757, 2013. 4, 5, 23, 28, 34
[56] Mark W Meckes. Magnitude, diversity, capacities, and dimensions of metric spaces. Potential
Analysis, 42(2):549–572, 2015. 5, 7, 23, 34
[57] Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization Bounds of SGLD for
Non-convex Learning: Two Theoretical Viewpoints. In Proceedings of the 31st Conference On
Learning Theory. arXiv, July 2017. 2
[58] Gregory Naitzat, Andrey Zhitnikov, and Lek-Heng Lim. Topology of deep neural networks.
Journal of Machine Learning Research, 21(184):1–40, 2020. 2
[59] Gergely Neu, Gintare Karolina Dziugaite, Mahdi Haghifam, and Daniel M. Roy. Information-
Theoretic Generalization Bounds for Stochastic Gradient Descent, August 2021. 2
15[60] Monica Nicolau, Arnold J Levine, and Gunnar Carlsson. Topology based data analysis identifies
a subgroup of breast cancers with a unique mutational profile and excellent survival. Proceedings
of the National Academy of Sciences, 108(17):7265–7270, 2011. 19
[61] Nina Otter, Mason A Porter, Ulrike Tillmann, Peter Grindrod, and Heather A Harrington. A
roadmap for the computation of persistent homology. EPJ Data Science, 6:1–38, 2017. 19
[62] Theodore Papamarkou, Tolga Birdal, Michael Bronstein, Gunnar Carlsson, Justin Curry, Yue
Gao,MustafaHajij,RolandKwitt,PietroLio`,PaoloDiLorenzo,etal.Positionpaper: Challenges
and opportunities in topological deep learning. arXiv preprint arXiv:2402.08871, 2024. 24, 25
[63] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011. 8, 24
[64] Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization Error Bounds for Noisy, Iterative
Algorithms. 2018 IEEE International Symposium on Information Theory (ISIT), January 2018.
2
[65] Juli´an Burella P´erez, Sydney Hauke, Umberto Lupo, Matteo Caorsi, and Alberto Dassatti.
Giotto-ph: A Python Library for High-Performance Computation of Persistent Homology of
Vietoris-Rips Filtrations, August 2021. 8
[66] David P´erez-Fern´andez, Asier Guti´errez-Fandin˜o, Jordi Armengol-Estap´e, and Marta Villegas.
Characterizing and measuring the similarity of neural networks with persistent homology. arXiv
preprint arXiv:2101.07752, 2021. 2, 19
[67] Yakov B Pesin. Dimension theory in dynamical systems: contemporary views and applications.
University of Chicago Press, 2008. 3
[68] MaithraRaghu, ThomasUnterthiner, SimonKornblith, ChiyuanZhang, andAlexeyDosovitskiy.
Do vision transformers see like convolutional neural networks? Advances in neural information
processing systems, 34:12116–12128, 2021. 35
[69] Patrick Rebeschini. Algorithmic fundations of learning, 2020. 24
[70] Bastian Rieck, Matteo Togninalli, Christian Bock, Michael Moor, Max Horn, Thomas Gumbsch,
and Karsten Borgwardt. Neural persistence: A complexity measure for deep neural networks
using algebraic topology. arXiv preprint arXiv:1812.09764, 2018. 2, 19
[71] Sarav Sachs, Umut S¸im¸sekli, and Tim van Erven. Generalization Guarantees via Algorithm-
dependent Rademacher Complexity - preprint. COLT 2023, 2023. 3
[72] Shilan Salim. The q-spread dimension and the maximum diversity of square grid metric spaces.
PhD thesis, University of Sheffield, 2021. 8
[73] Benjamin Schweinhart. Fractal dimension and the persistent homology of random geometric
complexes. Advances in Mathematics, 372:107291, 2020. 4, 5, 22, 28
[74] Benjamin Schweinhart. Persistent homology and the upper box dimension. Discrete & Compu-
tational Geometry, 65(2):331–364, 2021. 4
[75] Milad Sefidgaran, Amin Gohari, Ga¨el Richard, and Umut S¸im¸sekli. Rate-Distortion Theoretic
Generalization Bounds for Stochastic Learning Algorithms, June 2022. 3
16[76] Milad Sefidgaran and Abdellatif Zaidi. Data-dependent Generalization Bounds via Variable-Size
Compressibility, January 2024. 3, 5, 9
[77] Shai Shalev-Schwartz and Shai Ben-David. Understanding Machine Learning - From Theory to
Algorithms. Cambridge University Press, 2014. 18, 24, 29
[78] Umut Simsekli, Ozan Sener, George Deligiannidis, and Murat A Erdogdu. Hausdorff dimension,
heavy tails, and generalization in neural networks. Advances in Neural Information Processing
Systems, 33:5138–5151, 2020. 2, 3, 5, 6, 9, 24, 28
[79] Charlie Tan, In´es Garc´ıa-Redondo, Qiquan Wang, Michael M Bronstein, and Anthea Monod.
On the limitations of fractal dimension as a measure of generalization. arXiv preprint
arXiv:2406.02234, 2024. 11
[80] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herv´e J´egou. Training data-efficient image transformers & distillation through attention. In
International conference on machine learning, pages 10347–10357. PMLR, 2021. 9, 35, 39
[81] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv´e J´egou.
Goingdeeperwithimagetransformers. InProceedingsoftheIEEE/CVFinternationalconference
on computer vision, pages 32–42, 2021. 9, 35, 39
[82] Tim van Erven and Peter Harremo¨es. R´enyi Divergence and Kullback-Leibler Divergence. IEEE
Transactions on Information Theory, 60(7):3797–3820, July 2014. 5, 18
[83] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data
Science. Number47inCambridgeSeriesinStatisticalandProbabilisticMathematics.Cambridge
University Press, 2020. 8, 24
[84] Satoru Watanabe and Hayato Yamana. Topological measurement of deep neural networks using
persistent homology. Annals of Mathematics and Artificial Intelligence, 90(1):75–92, 2022. 2
[85] Jing Xu, Jiaye Teng, Yang Yuan, and Andrew Yao. Towards Data-Algorithm Dependent
Generalization: A Case Study on Overparameterized Linear Regression. Advances in Neural
Information Processing Systems, 36:79698–79733, December 2023. 2
[86] ChiyuanZhang,SamyBengio,MoritzHardt,BenjaminRecht,andOriolVinyals. Understanding
deep learning requires rethinking generalization. ICLR 2017, February 2017. 2
[87] ChiyuanZhang,SamyBengio,MoritzHardt,BenjaminRecht,andOriolVinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
115, February 2021. 2
[88] Afra Zomorodian. Topological data analysis. Advances in applied and computational topology,
70:1–39, 2012. 19
17Appendix
We now provide additional technical details and proofs that are omitted from the paper, followed by
experimental evidence complementing our main paper. We organize the appendix as follows:
• AppendixApresentsadditionaltechnicalbackgroundrelatedtoinformationtheory,Rademacher
complexity, and the various topological quantities that appear in our work.
• In Appendix B, we present the omitted proofs of all our theoretical results, as well as a few
additional theoretical contributions.
• In Appendix C, we show the experimental details needed to reproduce our experiments.
• Finally, Appendix D is dedicated to additional empirical results.
A Additional technical background
A.1 Information-theoretic quantities
The following definition is a precise definition of the total mutual information term that appears
in our main theoretical results. The reader may consult [82, 35, 21] for further information on this
notion.
Definition A.1 (Total mutual information). Let X and Y be two random elements defined on a
probability space (Ω, ,P) (note that the codomains of X and Y may be distinct). We define the
F
total mutual information between X and Y by the following formula:
(cid:18) P (A) (cid:19)
X,Y
I (X,Y)=log sup .
∞ P P (A)
A X Y
⊗
Such a term has already been used in the fractal-based generalization literature [35, 21]. Other
works used intricate variants of this total mutual information term [20, 9, 3, 12]. We stress the fact
that our proposed bounds are simpler.
A.2 Rademacher complexity
Rademacher complexity [5, 77] is a central tool in learning theory. As part of our theory uses this
notion, we now provide its definition and introduce some notation.
Definition A.2 (Rademacher complexity on a hypothesis set). Let us fix a dataset S n, a set
Rd and ϵ=(ϵ ,...,ϵ ) some iid Rademacher random variable.4 Whenever it is defin∈ edZ , we will
1 n
W ⊂
call Rademacher complexity of ℓ over the following quantity:
W
(cid:34) n (cid:35)
1 (cid:88)
Rad(ℓ, ,S):= E sup ϵ ℓ(w,z ) .
ϵ i i
W n
w∈W
i=1
Rademacher complexity has already been used in [20, Theorem 3.4] to relate the generalization
error to the so-called data-dependent fractal dimension. Part of our theory is based on a recent
extension of such arguments in the data-dependent setting [21].
4ARademacherrandomvariableisdefinedbyP(ϵi=1)=P(ϵi=−1)=1/2.
18A.3 Persistent homology
Thegoalofthisshortsubsectionistopresentafewnotionsofpersistenthomology,whichisnecessary
for a better understanding of our contributions.
Persistent homology [23, 13, 10] is an important subfield of TDA, capable of providing myriad of
newinsightsforanalysingdatabyextractingmeaningfultopologicalfeatures. Ithasdemonstratedits
usefulness in a very diverse set of applications from biology [60, 24], to materials science [34], finance
[45], robotics [8], sensor networks [17] and a lot more [61]. The types of datasets which are amenable
to this kind of analysis are finite metric spaces (known as point-cloud datasets), images, networks
and also level-sets of functions. More recently, several studies have brought to light empirical links
between persistent homology and DNNs [70, 16, 66]. In particular, recent studies have related the
worst-case generalization error to several concept of intrinsic dimensions defined through persistent
homology [9, 20]. As mentioned in the introduction, our goal is to extend these last studies to more
practical settings.
In general, persistent homology is defined for any degree k N (denoted PHk). Intuitively, PHk
∈
keeps track of the number of “holes of dimension k” in a set when looked at different scales. However,
in our work and as in [9, 20], we only use PH0, whose presentation is simpler. In this section, to
avoid harming the readability of the paper, we only present a high-level introduction to PH0 that is
sufficient to understand our work. The interested reader may consult [10, 14, 88] for a more in-depth
introduction to persistent homology.
We first start by introducing briefly homology, which is a classical concept in algebraic topology.
We only introduce the most essential concepts for understanding persistent homology. For a more
detailed introduction, please consult [33].
Definition A.3. A simplicial complex is a set K of finite sets closed under the subset relation: if
σ K and τ σ, then τ K.
∈ ⊂ ∈
In the above definition, σ is a simplex (plural simplices) and τ is a face of σ, its coface.
Definition A.4. An abstract simplicial complex is a finite collection of simplices where a face of
K
any simplex σ is also a simplex in .
∈K K
Definition A.5. A simplicial k-chain is the formal sum of k-simplices,
N
(cid:88)
=r σ , (8)
i i
i=1
where each r R, where R is a fixed commutative ring with additive identity 0 and multiplicative
i
∈
identity 1, and σ .
i
∈K
is the set of simplicial k-chains with addition over R, which is an R-module. Then, the set
k
K
of all k-simplices of the complex is a set of generators for . For each generator σ, the boundary
k
K K
of σ is the sum of all (k 1)-faces of σ.
−
Definition A.6. The boundary of a k-simplex σ =(x ,...,x ) is the (k 1)-chain
0 k
−
k
(cid:88)
∂ (σ)= ( 1)i(x ,...,xˆ,...,x ), (9)
k 0 i k
−
i=0
where (x ,...,xˆ,...,x ) is the (k 1)-simplex spanned by all vertices without x .
0 i k i
−
19It is common that the coefficients for homology are considered to be restricted to Z , which is
2
the field with 2 elements, 0 and 1, where 1+1=0. However, the theory extends to homoogy with
coefficeints in any field (and since every field is a ring, the definitions in terms of rings are more
general).
Definition A.7. A chain complex is a sequence of abelian groups A with homomorphisms (called
k
boundary maps) ∂ :A A , such that ∂ ∂ =0 for all k.
k k k−1 k−1 k
→ ◦
We should note that when considering coefficients in Z , a k-chain can be seen as a finite
2
collection of k-simplices.
Introduce topological invariants: simplicial homology groups and Betti numbers.
Definition A.8 (Simplicial Homology group). The n-th (simplicial) homology group of a finite
simplicial complex is
K
H =ker∂ /im∂ , (10)
n n n+1
where ker and im are the kernel and image respectively of the boundary operator.
In order to define the simplicial complexes of use in TDA, we need to first understand what a
nerve is.
Definition A.9 (Nerve). A simplicial complex associated to a collection of sets is called a nerve.
The sets are the vertices of the complex, and a simplex belongs to a complex iff its vertices have a
non-empty intersection, Nrv= α S A= .
A∈α
{ ⊆ |∩ ̸ ∅}
Definition A.10 (Cˇech complex). The Cˇech complex of X for radius r is Cˇech (X)=Nrv B(x,r)
r
{ |
x X , where B(x,r) is the closed ball of radius r 0, centered at x.
∈ } ≥
In other words, the Cˇech complex is the nerve of the ball neighbourhoods of a set of points
X Rn. The Cˇech complex faithfully captures the topology of the space, but it is not computed in
⊆
practice due to its high computational cost. Instead, a different complex called Vietoris-Rips (VR) is
used due to ease of construction for higher dimensions. It can be shown that the VR complex is not
always homotopy equivalent to the Cˇech complex, and therefore it can be seen as an approximation.
We first need to introduce the notion of a clique complex to explain what the VR is.
Definition A.11 (Cliquecomplex). Theclique complex foragraphG=(V,E)consistsofallcliques
of G, which are all simplices α V for which E contains all edges of α.
⊆
Now we have explicitly states all the necessary components in order to define the main complex
used in TDA, the Vietoris-Rips complex.
Definition A.12 (Vietoris-Rips complex). The Vietoris-Rips complex of X for radius r is the clique
complex of the 1-skeleton of the Cˇech complex of X and r, Rips (X)= α X u v 2r for
r { ∈ ||| − ||≤ }
all u,v α.
∈
Now that we have defined the most important complex in TDA, we proceed to explain how
we can derive important topological information at multiple scales by introducing the concept of a
filtration.
Definition A.13. Given a simplicial complex , a filtration is a totally ordered set of subcomplexes
K
i of , indexed by nonnegative integers, such that for i j, i j.
K K ≤ K ⊆K
20Definition A.14 (Filtered simplicial complex). A simplicial complex, , together with a filtration
(function f : R such that f(σ) f(τ) whenever σ is a face of τ).K The sublevel set at a value
r R is f−1K ( → ,r], which is a sub≤ xomplex of . Let r < r < < r be the values of the
0 1 m
∈ −∞ K ···
simplices, and =f−1( ,r ], then we call the sublevel set filtration of f.
i i 0 1 m
K −∞ K ⊆K ⊆···⊆K
Whenyoustartwithasimplicialcomplex andyoufilteritaccordingtoafiltrationf,itisclear
K
that the homology of evolves as the radius r increases. For example, new connected components
r
K
can be formed, loops can appear or disapper, cavities can form. What persistent homology does, and
where the importance of the filtering comes in is that now we have the tools to track the topological
changes associated with the different stages of the filtering process, and to associate a lifetime to
them (track when a topological feature has first appeared and at which stage of the filtration it will
disappear). This essential topological information is recorded in a set of intervals known as barcodes,
which can be represented as a multiset of points in R2, where the coordinates correspond to the birth
and death points of each interval.
A.3.1 Persistent homology of degree 0 (alternative approach)
For the rest of the this section, we only focus on homology in dimension 0, and provide an alternative
and perhaps easier to understand interpretation. Please note that the following definition is a
simplified and non-standard (though equivalent) definition of PH0.
Definition A.15 (Persistent homology of degree 0 (PH0)). Let (X,ρ) be a finite metric space and
N its cardinality. For each time5 t 0, we construct an undirected graph G , whose edges are given
t
≥
by:
x,y X, x,y G ρ(x,y) δ.
t
∀ ∈ { }∈ ⇐⇒ ≤
Thereexistsafinitesetoftimes0<t <t <+ suchthatthenumberofconnectedcomponents
1 k
··· ∞
in G changes compared to G for t<t . Let c be the number of connected components in G . By
ti t i i ti
convention we set c =N and t =0 and define n :=c c . PH0 is then defined as the following
0 0 i i i−1
−
multiset (the notation denotes multisets):
{{·}}
 
 
PH0 := t ,...,t ,t ,...,t ,...,t .
1 1 2 k k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
n1 times nk times
Remark A.16 (Vietoris-Rips filtration). The above is a simplified high-level definition of PH0. More
formally, the construction of the family of graphs G corresponds to the construction of the so-called
t
Vietoris-Rips filtration of X, of which we only kept the simplices of dimension 1, see [10] for more
details.
We now use PH0 to give the definitions of the quantities of interest in our work. The following is
adefinitionofthequantityE alreadymentionedinSection2,butseenthroughthelensofpersistent
α
homology. As it will be explained in Appendix A.4, these definitions are equivalent.
5Weusethetermtimeforthescalart,asitisclassicallydoneinthestudyofpersistenthomology. Notethatthis
hasnothingtodowiththenumberofiterationsappearingintherestofthepaper.
21Definition A.17 (α-weighted lifetime sums). With the same notations as in Definition A.15, we
define the α-weighted lifetime sums as:
(cid:88)
α 0, Eρ(X):= tα.
∀ ≥ α
t∈PH0
Remark A.18 (“birth” and “death” times). PHk is usually defined as a multiset of birth and death
times, tracking the appearance and disappearance of “holes of dimension k” during the construction
of the Vietoris-Rips filtration of X. In the particular case of PH0, all birth times are 0 and the times
that we constructed correspond to the death times.
We end this section by giving the definition of the PH dimension, which has been shown to be
theoretically and empirically related to the generalization error of neural networks in prior works
[9, 20].
Definition A.19 (Persistenthomologydimensionofdegree0). Givenacompactmetricspace(X,ρ),
we define the PH dimension of degree 0 by:
dimρ (X):=inf α 0, C >0, A X finite, E (A) C .
PH { ≥ ∃ ∀ ⊆ α ≤ }
It has been shown in [42, 73] that for any compact metric space, the PH dimension defined
above is equal to the celebrated upper box-counting dimension [25, 54].
A.4 Minimum spanning tree
The persistent homology dimension used in existing generalization bounds [9, 20] is closely related to
another notion of intrinsic dimension, called minimum spanning tree (MST) dimension [42], in the
sense that the PH and MST dimensions of bounded metric spaces are identical. The link between
persistent homology and MST is even deeper than the equality between the induced dimensions, as
noted by [73]. In this section, we define quantities related to MSTs which will play an important role
in our proofs.
Inthissectionletusfixafinitemetricspace(X,ρ). Letusfirstspecifyournotationsfortrees. A
tree on X is a connected undirected graph. We represent by its set of edges, which are denoted
T T
a b (or equivalently b a as the graph is undirected). For an edge e of the form a b, we define
→ → →
its length by e =ρ(a,b).
| |
Definition A.20 (Minimum spanning tree). Let us define the cost of a tree by the sum of the length
of its edges, i.e.,
(cid:88)
EMST( ):= e.
1 T | |
e∈T
An MST of X is defined as a tree with minimal cost. A consequence of the greedy algorithm to find
such an MST [15] is that an MST is also minimal for any of the following costs:
T
(cid:88)
EMST( ):= eα,
α T | |
e∈T
with α 0.
≥
Our interest in this notion comes from several results that are summed up in the following
theorem. The reader can refer to [1, 73, 10] for more details.
22Theorem A.21 (Link between MST and persistent homology of degree 0). There is a bijection
between the two following multisets:
• The multiset of the lifetimes in the persistent homology of degree 0 of the Vietoris-Rips complex
of X.
• The multiset of the length of the edges of an MST of X.
Therefore, if we fix some α 0, the weighted α-sum associated to the persistent homology of degree 0
≥
of the Vietoris-Rips complex of X is equal to the cost E of an MST of X, ie:
α
EMST( )=E (X).
α T α
In all the following, we will use the notation E to denote both quantities.
α
A.5 Magnitude
Let us restate formally a few standard definitions of magnitude, weighting, and positive definite
metric spaces. We refer the reader to [46, 55, 56] for more details. In this section, we fix a finite
metric space (X,ρ). Some of the presented concepts will be later extended to pseudometric spaces in
Appendix B.2.
As before, the similarity matrix [46] of X is defined by M(a,b)=e−ρ(a,b), for a,b X. We now
∈
define weightings and magnitude of X, according to [46, Section 2.1].
Definition A.22 (Weighting and magnitude). A weighting of X is a function β :X R such that
−→
(cid:88)
a X, e−ρ(a,b)β(b)=1.
∀ ∈
b∈X
If such a weighting exists, the magnitude of X is defined by:
(cid:88)
Mag(X):= β(b).
b∈X
It is easily seen that this definition is independent of the choice of weighting β. When a weighting
exists, we say that X “has magnitude”.
Basedonsuchadefinition,itisnaturaltoinquire,whethersuchaweightingexists. Thisquestion
has been studied by several authors [46, 55, 56]. This question appears to be related to the notion of
positive definite space, which we now define, according to [46].
Definition A.23 (Positivedefinitespace). X ispositivedefiniteifthesimilaritymatrixM ispositive
definite.
It is clear that positive definite spaces have magnitude. More interestingly, we have the following
result, which ensures that most metric spaces considered in this study are positive definite.
Theorem A.24 ([46, 55]). Let p [1,2] and d 1, every finite subset of (Rd, ) is positive
∈ ≥ ∥·∥p
definite.
23A.6 Covering and packing numbers
In this section, we fix a compact pseudometric space (X,ρ) and give definitions of covering and
packingnumbers. Thesequantitieshavelongbeenofprimaryinterestinlearningtheory,inparticular
through the classical covering arguments for Rademacher complexity [77, 69]. More recently, limits of
covering arguments have been leveraged by several authors to derive uniform generalization bounds
in terms of fractal dimensions [78, 35, 12, 20, 21], which we aim to improve in this study.
For x X and r > 0, we denote the closed ball centered at x and or radius r by B¯ (x) :=
r
∈
y X ,ρ(x,y) r . We can now define covering and packing.
{ ∈ ≤ }
Definition A.25 (Covering number). Let δ >0, the covering number Nρ(X) is the cardinality of a
δ
minimal set of points N such that:
(cid:91)
X B¯ (x).
δ
⊆
x∈N
Remark A.26. There exist several conventions for the definition of such numbers [25, 53, 83], all of
which are equivalent up to absolute constants and in particular induce the same fractal dimensions
on X (see [25]).
Definition A.27 (Packing number). Let δ >0, the covering number Nρ(X) is the cardinality of a
δ
maximal set of disjoint closed balls with centers in X.
A.7 About Johnson-Lindenstrauss lemma
In our implementation of Euclidean-based topological quantities, we use sparse random projections
to project the weight vectors from Rd to a lower dimensional subspace. This is necessary because of
memory constraints. Indeed, storing the full trajectory Rd (in our experiments T τ =
τ→T
W ⊂ −
5 103) can become intractable for large models.
×
(cid:16) (cid:17)
Given a finite set of points Rd and ϵ > 0. Let N log|W| , Johnson-Lindenstrauss
W ⊂ ≥ O ϵ2
lemma [83, 26] ensures the existence of a linear map P :Rd RN such that:
−→
w,w′ , (1 ϵ) w w′ 2 Pw Pw′ 2 (1+ϵ) w w′ 2 .
∀ ∈W − ∥ − ∥ ≤∥ − ∥ ≤ ∥ − ∥
Inpractice,thelinearmapssuggestedbythisresultcanbeobtainedthroughsubgaussianrandom
projections [83, Section 9.3].
Inourwork,asthepurposeofJohnson-Lindenstraussembeddingsismainlymemoryoptimization,
we have to rely on sparse random projections. We use the implementation provided in scikit-learn
[63]. More precisely, we used a relative variation ϵ of 5%.
Finally, it should be noted that these projection techniques were only used for the vision
transformer experiments, as the GNNs that we used have a small enough number of parameters to
avoid the use of random projections.
A.8 A note on the connection to Topological Deep Learning
Topologicaldeeplearning(TDL)isarapidlyevolvingfieldthatusestopologicalfeaturestounderstand
and design deep learning models [62, 31]. Our topological complexity measures can be seen as a
24direction towards addressing the Open Problem 7 mentioned in [62] concerning the discovery of
topological properties of internal representations that are linked to generalization.
B Omitted proofs of the theoretical results
In this section, we present the proofs of our main theoretical contributions. We divide our proofs
into two groups of subsections:
• Sections B.1,B.2 and B.3 focus on the extension (in a very natural way) of the quantities
appearing in our bounds in pseudometric spaces. The main outcome of this analysis is the
definition of positive magnitude in the pseudometric case. Note that Appendix B.1 is not a
contribution of this paper. We placed it in this section to improve the readability of the paper.
• In sections B.4, B.5, B.6 and B.7, we present the proof of our main theoretical results.
Before, provingourmainresults, wedefinethenotionofmetric identification, whichwillbeused
in several of the following subsections. This is the same setting that was used in [20] to naturally
extend the persistent homology dimension to pseudometric spaces.
Definition B.1 (Metric identification). Let (X,ρ) be a pseudometric space. We can define an
equivalencerelationonX bya b ρ(a,b)=0. Theassociatedquotientspace,whichisdenoted
∼ ⇐⇒
X/∼ is a metric space for the naturally induced metric, which we still denote ρ.6 We will also use the
canonical projection,
π :X X/∼.
−→
These notations will be used throughout the text.
B.1 Persistent homology and MST in pseudometric spaces
In this short subsection, we first restate results proven in [20], regarding persistent homology in
pseudometric spaces. The main result is the following proposition, which has been proven inside the
proof of [20, Lemma B.9].
Proposition B.2 ([21]). Let (X,ρ) be a finite pseudometric space and α 0, then we have:
≥
E α(X)=E α(X/∼)
where the pseudometric ρ (and its metric identification) have been omitted from the notation.
Based on Theorem A.21, the above result is also true when E represents the cost of a MST of
α
X.
B.2 Magnitude in pseudometric spaces
In this section, we fix (X,ρ) a finite pseudometric space. We denote by X/∼ its metric identification
and by π :X X/∼ the canonical projection.
−→
6Indeed,ifa∼b,thenwehave∀c∈X, ρ(a,c)=ρ(b,c).
25We directly extend Definition A.22 to the pseudometric case. In order for this definition to make
sense in our context, we first need to verify that it provides a well-posed definition of magnitude.
This follows from the following lemma.
Lemma B.3. We assume that the finite pseudometric space (X,ρ) has magnitude. Then magnitude
is independent of the choice of weighting.
Proof. The proof is straightforward and identical to the metric case. Let β,β′ be two weightings, we
have:
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
β(a)= e−ρ(a,b)β′(b)β(a)= β′(b) e−ρ(a,b)β(a)= β′(b).
a∈X a∈Xb∈X b∈X a∈X b∈X
In the following theorem, we show that magnitude is invariant through metric identification.
Theorem B.4 (Invariance of magnitude through metric identification). X has magnitude if and
only if X/∼ has magnitude, in which case we have:
Mag(X)=Mag(X/∼).
Proof. We decompose X into equivalence classes as:
(cid:97) (cid:97)
X = a¯=: a¯,
i
a¯∈X/∼ i∈I
where (cid:96) denotes disjoint union and the points (a ) XI represent each equivalence class. We
i i∈I
∈
denote by a¯ the equivalence class of a X.
∈
Let β :X R be any function. We have:
−→
(cid:88) (cid:88) (cid:88)
a X, e−ρ(a,b)β(b)= e−ρ(a¯,a¯i) β(b). (11)
∀ ∈
b∈X i∈I b∈a¯i
= : If X has magnitude, then we take β to be a weighting of X, we define:
⇒
(cid:88)
a¯ X/∼, β¯(a¯):= β(b).
∀ ∈
b∈a¯
By Equation (11), β¯ is a weighting of X/∼.
=: if β¯ is a weighting of X/∼, then we define:
⇐
1
a X, β(a):= β¯(a¯),
∀ ∈ a¯
| |
where a¯ denotes the cardinality of a¯. By Equation (11), β is a weighting of X.
| |
B.3 Definition of positive magnitude in the pseudometric case
Let us extend our new notion of positive magnitude in finite pseudometric spaces. This is a rather
complicated task. Indeed we need to ensure that the positive magnitude is independent of the choice
26of weighting, which is not true in general. For this reason, we restrict our definition to pseudometric
spaces whose metric identification is positive definite and we choose one particular weighting.
DefinitionB.5(Positivemagnitudeinfinitepseudometricspaces). Let(X,ρ)beafinitepseudometric
space whose metric identification X/∼ is positive definite. Let β¯:X/∼ R be a weighting of X/∼,
−→
then we define the positive magnitude of X, denoted PMag, by:
(cid:88)
PMag(X)= β¯(x¯) ,
+
x¯∈X/∼
wherex :=max(x,0)denotesthepositivepartofx. WewillsaythatX admitsapositivemagnitude
+
if its metric identification X/∼ is positive definite.
Note that X/∼ admits a unique weighting because it is positive definite. However, X still admits
several weightings in general. The above definition ensures that the definition of positive magnitude
is independent of any choice of weighting. For the need of our proofs, we will need to introduce
weightings in pseudometric spaces, whose sums of positive parts yield the positive magnitude. This
is possible by using the following definition, which corresponds to a “good” choice of weighting in
finite pseudometric spaces.
Definition B.6 (Canonical weighting). Let (X,ρ) be a finite pseudometric space whose metric
identification X/∼ is positive definite. Let β¯ : X/∼ R be a weighting of X/∼, we define the
canonical weighting β0 :X R on X by: −→
−→
1
a X, β0(a):= β¯(π(a)),
∀ ∈ π(a)
| |
where π :X X/∼ is the canonical surjection.
−→
The following lemma is then obvious but crucial to some of our theoretical results.
Lemma B.7. With the notation of the previous definition, we have:
(cid:88)
PMag(X)= β0(x) .
+
x∈X
The next proposition is a consequence of Theorem A.24, it shows that the pseudometrics
considered in practice in our work (and in our experiments) admit a positive magnitude.
Proposition B.8. Let p [1,2] and S n, then every finite subset of (Rd,ρ(p) ) admits a positive
∈ ∈Z S
magnitude, and therefore it also has a canonical weighting.
Proof. Let := w ,...,w be a finite set in Rd. We have
1 N
W { }
L (w) L (w′) =n1/pρ(p) (w,w′).
∥ S − S ∥p S
Therefore, if we denote by w¯ the equivalence class of w in the metric identification, it is clear that
w¯ =w¯′ L (w)=L (w′). Hence, the map φ :=n−1/pL naturally extends to an isometry
S S S S
⇐⇒
between metric spaces:
W/∼ ∼ φ S( ) Rn.
−→ W fin⊂ite
ByTheoremA.24, thefinitesetφ S( )ispositivedefinite, henceitisalsothecaseofW/∼. Therefore
W
admits a positive magnitude by definition.
W
27B.4 Warm-up: covering bounds
The following is deduced from the transcription of the results of [21] to our setting. It is the starting
point of our persistent homology-based analysis.
Theorem B.9. Let ρ be a pseudometric on Rd. Suppose that Assumption 1 holds and that ℓ is
(q,L,ρ)-Lipschitz, for q 1. Then, for all δ >0, with probability at least 1 ζ over µ⊗n µ⊗∞,
≥ − z ⊗ u
(cid:114) 2logNρ( ) (cid:114) I (S, )+log(1/ζ)
sup G (w ) 2Lδ+2B δ Wτ→T +3B ∞ Wτ→T .
S i
≤ n 2n
τ≤i≤T
The proof of this theorem will be given in the next subsection. Before discussing this proof, a
few remarks are in order.
Covering bounds, such as B.9 have been used in [78, 12, 9, 20] to introduce fractal dimensions
(more precisely through the notion of upper box-counting dimension) into the generalization bounds.
This is done via the following definition of the aforementioned upper box-counting dimension:
dimρ (X):=limsuplogN δρ(X)
.
B log(1/δ)
δ→0
By using a similar procedure, we see that our framework could be used to introduce intrinsic
dimensions associated to a wide range of pseudometrics, as soon as they satisfy a (q,L,ρ)-Lipschitz
continuity assumption.
However, arguments based on these intrinsic dimensions only make sense in the limit T ,
→∞
which makes little sense in practical settings. To address this issue, we take inspiration from two
other notions that are equal to the upper box-counting dimension (and therefore lay the ground of
the numerical approximation of this dimension), namely the PH-dimension [42, 73, 9, 20] and the
magnitudedimension[55,3]. Ourapproachistoreplacetheintrinsicdimensionsbythe“intermediary
quantities” used to define them. This leads to the results presented in the next two subsection.
B.5 Proof of Theorem B.9
BeforegoingtotheproofofTheoremB.9,wespecifyourtheoreticalsetup,whichistheoneintroduced
in [21]. In this section, we prove our results in the case T < + . However, note that one could
∞
consider T =+ without much technical difficulties.
∞
The setup is the following: let (F(Rd), ) denote the set of all finite subsets of Rd, endowed
T
with a σ-algebra .
T
We consider the following probability distribution on F(Rd):
(cid:90)
A , π(A):= ρ (A)dµ⊗n(S). (12)
∀ ∈T S z
Zn
As it is discussed in [21, Section 5.4], we make the following technical measure-theoretic
assumption.
Assumption 2. The probability measure µ⊗n is a strictly positive Borel measure. Moreover, for
z
every A , the map S ρ (A) is continuous.
S
∈T (cid:55)→
The following example highlights the fact this is a very mild assumption.
28Example B.10. If the data space is countable and the data distribution µ has no null mass, then
z
Z
the above assumption is automatically satisfied with respect to the discrete topology.
Theorem B.9. Let ρ be a pseudometric on Rd. Suppose that Assumption 1 holds and that ℓ is
(q,L,ρ)-Lipschitz, for q 1. Then, for all δ >0, with probability at least 1 ζ over µ⊗n µ⊗∞,
≥ − z ⊗ u
(cid:114) 2logNρ( ) (cid:114) I (S, )+log(1/ζ)
sup G (w ) 2Lδ+2B δ Wτ→T +3B ∞ Wτ→T .
S i
≤ n 2n
τ≤i≤T
Proof. Letusfixsomeζ (0,1). FirstnotethatthankstoAssumption2,wehavethatρ isabsolutely
S
∈
continuous with respect to π, µ⊗n-almost surely. Therefore, we can introduce its Radon-Nykodym
z
derivative, denoted by dρ /dπ.
S
Thanks to the above notation, we can apply the data-dependent Rademacher complexity bound
of [21, Theorem 10] to obtain that with probability at least 1 ζ, we have, for any λ>0:
−
(cid:16) (cid:17) 1 (cid:18) dρ (cid:19) 9B2
S
sup (w i) (cid:98)S(w i) 2Rad(ℓ, τ→T,S)+ ( τ→T)+log(1/ζ) +λ ,
R −R ≤ W λ dπ W 8n
τ≤i≤T
with Rad(ℓ, ,S) a Rademacher complexity term, defined by:
τ→T
W
(cid:34) n (cid:35)
1 (cid:88)
Rad(ℓ, ,S):=E sup ϵ ℓ(w,z ) ,
τ→T ϵ i i
W n
w∈Wτ→T
i=1
where ϵ:=(ϵ ,...,ϵ ) is a vector of independent centered Bernoulli random variables.
1 n
By [21, Lemma 16], we have almost surely that:
dρ
S
( ) I ( ,S).
τ→T ∞ τ→T
dπ W ≤ W
Therefore, by optimizing the choice of the parameter λ in the above equation, we have that:
(cid:114)
(cid:16) (cid:17) I (S, )+log(1/ζ)
∞ τ→T
sup (w i) (cid:98)S(w i) 2Rad(ℓ, τ→T,S)+3B W . (13)
R −R ≤ W 2n
τ≤i≤T
WenowperformacoveringargumentverysimilartoclassicalcoveringargumentsforRademacher
complexity[77]. Let us fix some δ >0 and introduce (x 1,...,x
N
δρ(Wτ→T)) the centers of a minimal
δ-covering of for pseudometric ρ. For any w , there exists j such that ρ(w,x ) δ.
τ→T τ→T j
W ∈W ≤
Therefore we have:
n n n
1 (cid:88) 1 (cid:88) 1 (cid:88)
sup ϵ ℓ(w,z ) sup ϵ ℓ(x ,z )+ ϵ (ℓ(w,z ) ℓ(x ,z ))
i i i j i i i j i
w∈Wτ→T
n
i=1
≤
1≤j≤N
δρ(Wτ→T)n
i=1
n
i=1
−
n n
1 (cid:88) 1 (cid:88)
sup ϵ ℓ(x ,z )+ ℓ(w,z ) ℓ(x ,z )
i j i i j i
≤
1≤j≤N
δρ(Wτ→T)n
i=1
n i=1| − |
n
1 (cid:88)
sup ϵ ℓ(x ,z )+n−1/q L (w) L (x ) ,
≤
1≤j≤N
δρ(Wτ→T)n
i=1
i j i ∥ S − S j ∥q
where the last line comes from H¨older’s inequality.
29We can now apply Massart’s lemma on the first term and the (q,L,ρ)-Lipschitz continuity of ℓ
on the second term, this gives us:
(cid:114)
2logNρ( )
Rad(ℓ, ,S) Lδ+B δ Wτ→T ,
τ→T
W ≤ n
which concludes the proof.
B.6 Persistent homology bounds
We now present the proofs of our persistent homology-based bounds, ie, the results of Section 3.2.
The following lemma is a pseudometric version of a classical result of fractal geometry [25].
Lemma B.11 (Covering and packing in pseudometric spaces). Let (X,ρ) be a pseudometric space,
(cid:8) (cid:9)
δ >0, and x ,...,x a maximal δ-packing of X for pseudometric ρ. Then we have:
1 Pδ(X)
Nρ (X) Pρ(X).
2δ ≤ δ
Proof. Let us fix δ >0 and let (x 1,...,x Pρ(X)) be centers of a maximal packing of X with closed
δ
δ-balls. Let us assume that:
(cid:91)
X B¯ (x )= ,
2δ i
\ ̸ ∅
1≤i≤Pρ(X)
δ
sothatwecantakesomex belongingtotheabovenon-emptyset. Nowletusfixi 1,...,Pρ(X)
and w B¯ (x ). By the tr0 iangle inequality and the definition of w and x , we hav∈ e:{ δ }
δ i 0
∈
ρ(x ,x ) ρ(x ,w)+ρ(w,x ).
0 i 0 i
≤
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
>2δ ≤δ
Therefore, we have ρ(x ,w)>δ, and hence B¯ (x ) B¯ (x ), so that we construct a bigger δ-packing,
0 δ i δ 0
∩
by adding x
0
to (x 1,...,x Pρ(X)), which is absurd.
δ
Therefore, we have X (cid:83) B¯ (x )= , hence the result.
\ 1≤i≤P δρ(X) 2δ i ∅
The next lemma asserts that E is increasing (with respect to the inclusion of sets), if and only
α
if α 1. This is the reason why we require α [0,1] in Theorem 3.4.
≤ ∈
Lemma B.12. Let (X,ρ) be a finite pseudometric space, α [0,1] and δ >0. Then we have:
∈
1
Eρ(X) Pρ(X)δα
α ≥ 2 δ
Proof. In all the following, we fix α [0,1] and δ >0. We also denote P :=Pρ(X). Without loss of
∈ δ
generality, we can assume P 2.
≥
We fix an MST of X, represented by a set of edges denoted x y, with x,y X2 (note that
T → ∈
we identify x y and y x). It is a classical result that there are X 1 edges. For an edge e of
→ → | |−
the form a b, we denote its length by e :=ρ(a,b).
→ | |
30For a,b X, with a = b, we denote by a b the shortest path between a and b. More
∈ ̸ { → }
precisely, we represent it as a list of edges, denoted a=a a a =b, for some K. When
0 1 K
→ ···→
the context is clear, we identify a b to the set of its edges a b.
{ → } →
Let us introduce (x ,...,x ) a maximal δ-packing of X by closed.
1 P
For every i 1,...,P , as is connected, there exists y X such that y / B¯ (x ) and y is
i i δ i i
the only point in∈ t{ he path } x T y that does not belong to th∈ e ball B¯ (x ). ∈
i i δ i
{ → }
For each i, we denote e the only edge in x y to which y belongs, i.e. e is of the form
i i i i i
z y , with z B¯ (x ). By construction, tho{ se e→ dges} e are the only ones that can be shared by
i i i δ i i
→ ∈
several paths x y .
i i
{ → }
Let us introduce the following set of indices:
I := i 1,...,P , j =i, e / x y , K := 1,...,P I.
i j j
{ ∈{ } ∀ ̸ ∈{ → }} { }\
Let us consider i K. Let us assume that we have j,j′ 1,...,P such that e x y
i j j
and e i x j′ y j′
.∈
If we denote e i as z i y i, we
have∈ th{
at z i
B}¯
δ(x i), by
defi∈ ni{ tion→
of
y}
i.
Therefo∈ re,{ by d→ efinit} ion of y , we have z = y→ (because B¯ (x ) B¯ ∈ (x ) = ). We have similarly
j i j δ i δ j
z i =y j′ and thus y j =y j′. By definition of y j and y j′ we also ha∩ ve y i B¯ δ(x∅ j) B¯ δ(x j′), which is
∈ ∩
absurd, by definition of packing. We conclude the following:
k K, !j =i, e x y .
i j j
∀ ∈ ∃ ̸ ∈{ → }
For k K, we denote the corresponding j by φ(k).
∈
By definition of K, it is clear that φ(k) K. Moreover, as y =z B¯ (x ), this implies that
φ(i) i δ i
∈ ∈
φ2(i)=i. Therefore, we have constructed an involution,
φ:K K,
−→
such that k K, φ(k)=k. This implies that the cardinality of K is even and that we can write
(cid:96)∀ ∈ ̸
K =K K , with:
1 2
K = K , φ(K )=K .
1 2 1 2
| | | |
The outcome of this construction is that we now have disjoint paths given by the (x y ) and
i i i∈I
→
the (x x ) . Therefore, we get the following lower bound on E (X).
k
→
φ(k) k∈K1 α
(cid:88) (cid:88) (cid:88) (cid:88)
E (X) eα+ eα.
α
≥ | | | |
i∈I e∈{xi→yi} k∈K1e∈ xk→xφ(k)
{ }
As α [0,1], we have that:
∈
 α  α
(cid:88) (cid:88) (cid:88) (cid:88)
E α(X) ≥  |e | +   |e |  .
i∈I e∈{xi→yi} k∈K1 e∈ xk→xφ(k)
{ }
By the triangle inequality, and by definition of packing, we have:
(cid:88) (cid:88) 1
E (X) δα+ δα =δα(I + K ) Pρ(X)δα,
α ≥ | | | 1 | ≥ 2 δ
i∈I k∈K1
31which concludes the proof.
Theorem 3.4. Let ρ be a pseudometric on Rd. Supposes that Assumption 1 holds and that ℓ is
(q,L,ρ)-Lipschitz, for q 1. Then, for all α [0,1], with probability at least 1 ζ, we have:
≥ ∈ −
(cid:118)
(cid:117) (cid:16) √ (cid:17)
(cid:117)2logEρ +αlog 8L n (cid:114)
(cid:116) α B 2B I ∞(S, τ→T)+log(1/ζ)
sup G (w ) 2B + +3B W .
S i
≤ n √n 2n
τ≤i≤T
Proof. For better clarity, we assume T < + . Let us fix some ζ (0,1), δ > 0, and α 0. By
∞ ∈ ≥
Theorem B.9, we have, with probability at least 1 ζ:
−
(cid:16) (cid:17) (cid:114) 2logNρ( ) (cid:114) I (S, )+log(1/ζ)
sup (w i) (cid:98)S(w i) 2Lδ+2B δ Wτ→T +3B ∞ Wτ→T .
R −R ≤ n 2n
τ≤i≤T
We now bound the covering number appearing in the above equation. By Lemma B.12, we have:
Eρ( ) 2−α−1Pρ ( )δα.
α Wτ→T ≥ δ/2 Wτ→T
Moreover, by Lemma B.11, we have:
Eρ( ) 2−α−1Nρ( )δα.
α Wτ→T ≥ δ Wτ→T
We now combine this with our generalization bound by choosing the value:
B
δ := ,
L√n
and we get that with probability at least 1 ζ, we have:
−
(cid:118)
(cid:117) (cid:16) √ (cid:17)
(cid:16) (cid:17) 2B (cid:117) (cid:116)2log(2E αρ( Wτ→T))+αlog 2L Bn
sup (w i) (cid:98)S(w i) +2B
R −R ≤ √n n
τ≤i≤T
(cid:114)
I (S, )+log(1/ζ)
∞ τ→T
+3B W ,
2n
leading to the desired result.
B.7 Proof of the magnitude-based generalization bounds
Lemma B.13. Let Rd be a finite set and ϵ := (ϵ ,...,ϵ ) and ρ a pseudometric such that
1 n
W ⊂
( ,λρ) admits a positive magnitude (according to Definition B.5) for every λ>0. We assume that
W
ℓ is (L,q,ρ)-Lipschitz continuous with q [1,2]. Then, for any λ>0, we have:
∈
(cid:34) (cid:40) n (cid:41)(cid:35)
E
ϵ
exp λ sup (cid:88) ϵ iℓ(w,z i) eλ2 2B n2 PMag((Lλ) ).
n ≤ W
w∈W
i=1
where PMag is the positive magnitude, see Appendix B.3
32Proof. We first remark that, by H¨older’s inequality and the (L,q,ρ)-Lipschitz condition, we have:
w,w′ , ρ (w,w′) n−1/q L (w) L (w′) Lρ(w,w′).
∀ ∈W S ≤ ∥ S − S ∥q ≤
Let us fix some λ > 0. As ( ,λρ) admits a positive magnitude, we can introduce a canonical
weighting β : R. By deW finition of a weighting, we have
W −→
(cid:88)
a , e−λρ(a,b)β(b)=1.
∀ ∈W
b∈W
n
Moreover, for any ϵ 1,1 , we introduce:
∈{− }
n
(cid:88)
a :=argmax ϵ ℓ(a,z ).
ϵ a∈W i i
i=1
With those notations, we can compute:
(cid:88)
1 e−λρ(aϵ,b)β (b)
+
≤
b∈W
(cid:88) e− LλρS(aϵ,b)β +(b)
≤
b∈W
(cid:40) n (cid:41)
(cid:88) λ (cid:88)
= exp ℓ(a ,z ) ℓ(b,z ) β (b)
ϵ i i +
−Ln | − |
b∈W i=1
(cid:40) n (cid:41)
(cid:88) λ (cid:88)
exp ϵ (ℓ(a ,z ) ℓ(b,z )) β (b)
i ϵ i i +
≤ −Ln −
b∈W i=1
(cid:40) n (cid:41) (cid:40) n (cid:41)
λ (cid:88) (cid:88) λ (cid:88)
=exp ϵ ℓ(a ,z ) exp ϵ ℓ(b,z ) β (b).
i ϵ i i i +
−Ln Ln
i=1 b∈W i=1
Therefore, by dividing by the first term on the right-hand side and using the independence of the ϵ ,
i
we deduce that:
(cid:34) (cid:40) n (cid:41)(cid:35) (cid:34) (cid:40) n (cid:41) (cid:35)
λ (cid:88) (cid:88) λ (cid:88)
E exp sup ϵ ℓ(w,z ) E exp ϵ ℓ(b,z ) β (b)
ϵ i i ϵ i i +
Ln ≤ Ln
w∈W
i=1 b∈W i=1
n
= (cid:88) (cid:89) E ϵ(cid:104) eLλ nϵiℓ(b,zi)(cid:105) β +(b).
b∈Wi=1
By Hoeffding’s lemma, we have:
(cid:34) (cid:40) n (cid:41)(cid:35)
E
ϵ
exp λ sup (cid:88) ϵ iℓ(w,z i) eλ 2n2B L22 (cid:88) β +(b)
Ln ≤
w∈W
i=1 b∈W
λ2B2
=e2nL2PMag(λ ).
W
The result follows by the change of variable λ=ΛL.
Theorem 3.5. Let ρ be a pseudometric such that ( ,λρ) admits a positive magnitude (according to
W
Definition B.5) for every λ>0. We assume that ℓ is (q,L,ρ)-Lipschitz continuous with q 1. Then,
≥
33for any s>0, we have with probability at least 1 ζ that
−
(cid:114)
2 B2 I (S, )+log(1/ζ)
sup G (w ) logPMagρ(Ls )+s +3B ∞ Wτ→T .
S i τ→T
≤ s W n 2n
τ≤i≤T
Proof. The beginning of the proof is completely similar to the proof of B.9 up to Equation (13).
More precisely, we have that with probability at least 1 ζ:
−
(cid:114)
(cid:16) (cid:17) I (S, )+log(1/ζ)
∞ τ→T
sup (w i) (cid:98)S(w i) 2Rad(ℓ, τ→T,S)+3B W .
R −R ≤ W 2n
τ≤i≤T
By Jensen’s inequality, we have, for all λ>0:
(cid:34) (cid:40) n (cid:41)(cid:35)
1 λ (cid:88)
Rad(ℓ, ,S) logE exp sup ϵ ℓ(w,z ) .
τ→T ϵ i i
W ≤ λ n
w∈Wτ→T
i=1
Therefore, we can apply Lemma B.13 to write that, for all s>0:
B2 1
Rad(ℓ, ,S) s + logPMag(Ls ).
τ→T τ→T
W ≤ 2n s W
We deduce that for all s>0, we have with probability at least 1 ζ that:
−
(cid:114)
(cid:16) (cid:17) B2 2 I (S, )+log(1/ζ)
sup (w i) (cid:98)S(w i) s + logPMag(Ls τ→T)+ ∞ Wτ→T .
R −R ≤ n s W 2n
τ≤i≤T
Remark B.14 (Link between magnitude and positive magnitude). Let RM be a finite set (for
W ⊂
some M), of cardinality N, and ρ a metric on . If we denote the similarity matrix, for a given
W
value of s>0, by M (a,b)=e−ρ(a,b), then it is clear that:
s
M I .
s N
s−→→∞
Moreover, by continuity of the inverse, this implies that the weighting associated to s > 0, i.e.
β : R, satisfy:
s
W →
a , β (a) 1.
s
∀ ∈W s−→→∞
From this, we first deduce that, for s , we have Magρ(s ) N. Moreover, by continuity of
→∞ W →
the inverse, this means that, up to a certain s, the weighting (β (a)) only has positive elements.
s a∈W
Therefore, this implies that, for s big enough, one has Magρ(s )=PMagρ(s ).
W W
Thanks to our definitions for positive magnitude in pseudometric spaces, given in Appendix B.3,
this observation extends to the pseudometric case.
Remark B.15 (Extension to infinite sets). There exist extensions of the definition of magnitude
beyond finite sets [55, 56]. More specifically, weightings are then represented by measures on the set.
It is clear from the above proofs that we can extend the positive magnitude in this setting and that
the proof would follow similar lines. Therefore, our theory provides upper bounds of Rademacher
complexity in terms of positive magnitude in more general cases than the one we use in this work.
34Table 2: Architecture details for the vision transformers (taken from [28]). WS refers to Window
Size.
Model Dataset Depth Patch Size Token Dim Heads MLP-ratio WS #Params
ViT [80] CIFAR10 9 4 192 12 2 - 2697610
ViT [80] CIFAR100 9 4 192 12 2 - 2714980
Swin [48] CIFAR10 [2,4,6] 4 96 [3,6,12] 2 4 7048612
Swin [48] CIFAR100 [2,4,6] 4 96 [3,6,12] 2 4 7083262
CaiT [81] CIFAR10 24 4 192 4 2 - 8053450
CaiT [81] CIFAR100 24 4 192 4 2 - 8070820
C Additional Experimental Details
In this section, we give additional details regarding the models, datasets, and hyperparameters used
in our experiments.
C.1 Experimental setting
C.1.1 Vision Transformers Architecture and implementation details
The design of the ViT has been modified to accommodate for the small datasets as per [68]. Our
implementation is based on the [28], which is based on the timm library with the architecture
parameters presented in Table 2. The implementation of Swin is based on the Swin-Transformer
libarary and the implementation of CaiT is predominantly based on the timm library with some
modifications. The full version can be found in the supplementary code.
Instead of training from scratch, which is extremely time-consuming, we used the pre-trained
weights available from the GitHub repository of the paper [28], we further fintetuned them for 100
epochs on the dataset CIFAR10 or CIFAR100 to achieve the optimum performance reported in
the paper [28]. Then we verified that the finetuned weights achieved 100% training performance,
and then they were the starting point of our computational framework. We ran the transformer
experiments on 18 NVIDIA 2080Ti GPUs, and the graph experiments on 18 Intel Xeon Silver 4114
CPUs.
C.1.2 GNN Architecture and implementation details
We will briefly talk about the details of GraphSage [32] and GatedGCN [11], prior works we use in
our experiments. GraphSage [32] is an improvement over the GCN (Graph ConvNets) model [41]
and it incorporates each node’s own features from the previous layer in an explicit way by the update
equation:
hl+1 =ReLU(UlConcat(hl,Mean hl)),
i i j∈Ni j
35where N is the neighbourhood of node i, hl is the feature vector and Ul Rd×2d. We use the
i i ∈
graph-pooling version of GraphSage, with the following update equation:
hl+1 =ReLU(UlConcat(hl,Max ReLU(Vlhl))),
i i j∈Ni j
where Vl Rd×d. GatedGCN (Gated Graph ConvNet) [11] uses the following update equation:
∈
(cid:88)
hl+1 =hl +ReLU(BN(Ulhl + el Vlhl)),
i i i ij ⊙ i
j∈Ni
where Ul,Vl Rd×d, is the Hadamard product, and the edge gates el have the following
∈ ⊙ ij
definitions:
σ(eˆl )
el = ij ,
ij (cid:80) σ(eˆl )+ϵ
j′∈Ni ij
eˆl =eˆl−1+ReLU(BN(Alhl−1+Blhl−1+Cleˆl−1)),
ij ij i i ij
where σ is the sigmid funciton, ϵ is a small constant for numerical stability, Al,Bl,Cl Rd×d, and
∈
BN stands for Batch Normalization.
We used the code provided by [22], which relies on the dgl library implementation of GraphSage
and GatedGCN. We trained GraphSage and GatedGCN until 100% training accuracy, following the
setup in [22]. All experiments were ran on 18 Intel Xeon Silver 4114 CPUs. Each experiment (one
fixed batch size and learning rate) was run on a single CPU and 18 experiments were run on the
server at any given time (on different CPUs).
C.2 Hyperparameter details
Hyperparameters shared among experiments.. For the Vision Transformers experiments,
we varied the learning rate range [10−5,10−3], and batch size in the range [8,256]. For the graph
experiments, [10−6,10−4], and batch size in the range [8,256]. For all experiments, we used 0.1
proportion of the training data for the computation of the pseudo matrix, apart from CaiT and Swin
on CIFAR100, where we used 0.09 proportion of the training data due to memory constraints. All
experiments use a 6 6 grid of hyperparameters which is specified as follows.
×
ViT on CIFAR10. We selected 6 values for the learning rate in the range [10−5,10−3], and the
batch size between [8,256], and data proportion for the computation of the pseudo-distance (ρ ) of
S
10% (see Section 4).
ViT on CIFAR100. We selected 6 values for the learning rate in the range [10−5,10−3], and the
batch size between [8,256], and data proportion for the computation of the pseudo-distance (ρ ) of
S
10% (see Section 4).
CaiT on CIFAR10. We selected 6 values for the learning rate in the range [10−5,10−3], batch size
between [8,256], and data proportion for the computation of the pseudo-distance (ρ ) of 10% (see
S
Section 4).
36CaiT on CIFAR100. We selected 6 values for the learning rate in the range [10−5,10−3], batch
size between [8,256], and data proportion for the computation of the pseudo-distance (ρ ) of 9%
S
(see Section 4).
Swin on CIFAR10. We selected 6 values for the learning rate in the range [10−5,10−3], batch size
between [8,256], and data proportion for the computation of the pseudo-distance (ρ ) of 10% (see
S
Section 4).
Swin on CIFAR100. We selected 6 values for the learning rate in the range [10−5,10−3], batch size
between [8,256], and data proportion for the computation of the pseudo-distance (ρ ) of 9% (see
S
Section 4).
GatedGCN. We selected 6 values for the learning rate in the range [10−6,10−4], the batch size
between [8,256] and data proportion for the computation of the pseudo-distance (ρ ) of 10% (see
S
Section 4). We note that for due to time constraints, the experiments with batch sizes of 8 and 256
for the Euclidean metric were not complete.
GraphSage. We selected 6 values for the learning rate in the range [10−6,10−4], the batch size
between [8,256], and data proportion for the computation of the pseudo-distance (ρ ) of 10% (see
S
Section 4).
ViT on CIFAR10 (Adam). We selected 6 values for the learning rate in the range [10−5,10−3],
and the batch size between [8,256], and data proportion for the computation of the pseudo-distance
(ρ ) of 10% (see Section 4).
S
ViT on CIFAR10 (SGD). We selected 6 values for the learning rate in the range [5 10−3,10−1],
×
and the batch size between [8,256], and data proportion for the computation of the pseudo-distance
(ρ ) of 10% (see Section 4).
S
ViT on CIFAR10 (RMSprop). Weselected6valuesforthelearningrateintherange[10−6,10−3],
and the batch size between [8,512], and data proportion for the computation of the pseudo-distance
(ρ ) of 10% (see Section 4).
S
D Additional experimental results
In this section, we present additional empirical results, in addition to what was already presented in
the main part of this document. We divide this section into three parts. First, we quickly explore in
Appendix D.1 the consequence of our choice of estimation technique of the worst-case generalization
error. In Appendix D.2 we report additional experiments based on vision transformers and in
Appendix D.3 we include additional illustration of the GNN experiments.
D.1 About the final accuracy gap and the worst accuracy gap
Our main theoretical results, presented in Section 3, apply to the worst-case generalization error
(cid:16) (cid:17)
over the trajectory, i.e. on the quantity sup
τ≤k≤T
R(w k) −R(cid:98)S(w k) . However, computing this
quantity over the whole trajectory may be extremely expensive as it requires evaluating the model
on the whole dataset at each iteration (this is a similar problem to the one encountered for the
computation of the data-dependent distance matrices, discussed in Section 4). Previous studies
on worst-case TDA-inspired generalization bounds circumvented this issue by reporting the final
accuracy gap as the “generalization error” in their experiments (as it is the case in our work, most
existing experiments consist of classification tasks).
37In our work, we argue that the true worst-case generalization error may however have a different
behavior than the final accuracy gap. In order to estimate this quantity in a computationally
friendly way, we used the following procedure: we periodically estimated the test accuracy during the
training, computed its minimum value acc and substracted it from the final train accuracy
test-worst
(acc train-final) to obtain the “generalization gap” G(cid:98)S reported in our main experiments, i.e.,
G(cid:98)S :=acc
train-final
acc test-worst.
−
Note that in addition to being a good proxy to the true error appearing in our theory, the above
quantity could be of independent experimental interest.
Inordertoassessthatourmainconclusionsremainvalidifthefinalaccuracygapisusedinstead
of G(cid:98)S, we present here a few additional experiments using the final accuracy gap as a generalization
measure (it is denoted Accuracy gap in the figures.) In the case of a ViT on CIFAR10, this is shown
in Figure 5 and Figure 6. We observe that our proposed topological complexities also correlate very
well with the final accuracy gap, and outperform the previously proposed PH dimensions [9, 20].
In addition to these findings, we make two additional new observations. First, the Ph dim, while
outperformed by our proposed metric, has better granulated Kendall’s coefficients when compared
to the final accuracy gap than the worst generalization error (Ψ goes from 0.20 to 0.36). This may
explain why we observed poor performance of PH-dim in Figure 4a. Second, we observe that the
correlation seems to be slightly less good with the final accuracy gap, especially for high learning
rates, which seems to be similar behavior to what was reported in [20].
103 103 103
2.4×100 Batch size
102 2.2×100 8
104 103 Batch 8size 104 1.2 8× ×1 10 00 0 1 4 98 2 7 104 101 Batch 8size 1 4 98 2 7 1.6×100 2 52 12 2 100 1 48 2 105 102 2 52 12 2 105 1.4×100 105
97 1.2×100
222
512
101 106 106 100 106
4 6 8 4 6 8 4 6 8
Accuracy gap Accuracy gap Accuracy gap
√
(a) E (b) PMag( n) (c) dim
α PH
Figure 5: ViT on CIFAR10 with ρ -pseudometric, using the final accuracy gap as a generalization
S
measure.
103 103 103
102 2.4×100 Batch size
2.2×100 8
101 103 2×100 18
100 104 104 1.8×100 4 92 7 104 Batch size 102 Batch size 1.6×100 222
101 8 8 512
102
1 48
2
105
101
1 48
2
105 1.4×100 105
97 97 1.2×100
103 2 52 12
2 106 100
2 52 12
2 106 100 106
4 6 8 4 6 8 4 6 8
Accuracy gap Accuracy gap Accuracy gap
√
(a) E (b) PMag( n) (c) dim
α PH
Figure 6: ViT on CIFAR10 with 01-pseudometric, using the final accuracy gap as a generalization
measure.
38
ahpla E
ahpla
E
etar gninraeL
etar gninraeL
edutingam evitisop
edutingam
evitisop
etar gninraeL
etar gninraeL
mid hp
mid
hp
etar gninraeL
etar gninraeLD.2 Vision Transformers - additional experiments
We compare the performance of the different metrics by using the granulated Kendall’s coefficients
introduced in [37]. The experiments presented here use 3 different Vision Transformers (ViT [80],
CaiT [81], Swin [48]) on CIFAR10 and CIFAR100. As a baseline, we use the dim introduced in [9]
PH
and the data-dependent dimension with the pseudometric dim from [20].
PH
Here we present the full results on each dataset and model. They can be found in Table 4
for CaiT and CIFAR10, 6 for Swin and CIFAR10, 3 for ViT and CIFAR100 and 5 for CaiT and
CIFAR100. The plots from each experiment for every computed quantity can be found in (the
remaining 3 quantities for ViT and CIFAR10).
103 103 103
Batch size 2.2 Batch size
102 8 2 8
16 16
103 32 1.8 32
64 64
101 Batch 8size 104 1 22 58
6
104 1.6 1 22 58
6
104
16 1.4
100 3 62
4
102
128 1.2
256
105 105 105
6 8 10 6 8 10 6 8 10
Generalization gap Generalization gap Generalization gap
Batch size
103 103
1.01 Batch size
103
8 8
16 1.008 16
103 32 32 64 101 1.006 64
1 22 58
6
104 Batch 8size 104 1 22 58
6
104
102 16 1.004
32
64 1.002
128
101 6 8 10 105 6 8 12 056 105 100 6 8 10 105
Generalization gap Generalization gap Generalization gap
Figure 7: ViT on CIFAR10 with ρ
S
D.3 Graph Neural Networks – Additional Experiments
In Table 1, we already presented the correlation coefficients for all quantities for the GNN models
considered in our study (GraphSage, GatedGCN) [22] (we have selected the models which achieve
100%trainingaccuracy))andGraph-MNIST.Wecanobserveanicecorrelation,outperforingdim-PH
in most experiments. As it was observed for the transformer-based experiments, the correlation
seems to be better for the data-dependent-metrics. This is an important fact, as no sparse random
projection was used to compute the Euclidean distance matrices in the GNN experiments (it was not
necessary as these models have less parameters than the tramsformers considered above). This shows
that the fact the data-dependent pseudometrics outperform the Euclidean distance also happens in
the absence of these projections. It also shows that all quantities seem to yield better correlations in
the absence of random projections, at least in the GNN expsriments.
The corresponding plots for GatedGCN can be seen in Figure 28 with the pseudometric, Figure
29 for the Euclidean and 30 for 01. The plots for GraphSage are reported in Figure 25, Figure 26
and Figure 27.
We can observe a strong correlation on these figures, outperforing dim-PH in most cases. As
it was observed for the transformer-based experiments, the correlation seems to be better for the
39
ahpla
E
edutingam
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid
hp
etar
gninraeL
etar
gninraeL103 103 103
1.17×100
103
4×103 1.16×100
Batch 8size 104 3×103 Batch 8size 104 1.15×100 Batch 8size 104
102 16 16 1.14×100 16
32 32 32
64 2×103 64 1.13×100 64
128 128 128
256 256 1.12×100 256
105 105 105
6 8 10 6 8 10 6 8 10
Generalization gap Generalization gap Generalization gap
103 103 1.6×100 103
Batch size
1.5×100 8
4×103 6 16
1.4×100 32
64
3×103 Batch 8size 104 4 Batch 8size 104 1.3×100 1 22 58
6
104
16 3 16 1.2×100
32 32
2×103 64 2 64 1.1×100
128 128
256 256
105 105 100 105
6 8 10 6 8 10 6 8 10
Generalization gap Generalization gap Generalization gap
Figure 8: ViT on CIFAR10 with
2
∥·∥
102
103 103
2.4×100
103
2.2×100
101 103 2×100
1.8×100
100
Batch size 104 102 Batch size 104 1.6×100 Batch size 104
8 8 8 101 16 16 1.4×100 16
32 32 32
102 64 101 64 1.2×100 64
128 128 128
256 256 256
103 105 105 100 105
6 8 10 6 8 10 6 8 10
Generalization gap Generalization gap Generalization gap
103 103 103
Batch 8size 1.004×100 Batch 8size
103 16 16
3 62 4 101 1.003×100 3 62 4
102 1 22 58 6 104 Batch 8size 104 1.002×100 1 22 58 6 104
16
101 3 62
4 1.001×100
128
100
6 8 10
105 100
6 8
12 056 105 100
6 8 10
105
Generalization gap Generalization gap Generalization gap
Figure 9: ViT on CIFAR10 with 01-pseudometric
data-dependent-metrics. This is an important fact, as no sparse random projection was used to
compute the Euclidean distance matrices in the GNN experiments7. This shows that data-dependent
pseudometricsoutperformtheEuclideandistancealsointheabsenceoftheseprojections. Inaddition,
all quantities seem to yield better correlations in the absence of random projections, at least in the
GNN expsriments.
Interestingly, afewfailurecasescanbeseenontheseplots. Indeed, Mag(0.01)andPMag(0.01)
seem to be almost constant and near 1. This indicates that the scale choice s=0.01 was not suited
7A sparse random projection was not necessary as these models have less parameters than the tramsformers
consideredabove
40
edutingam
ahpla
E
edutingam
ahpla
E
edutingam
evitisop
llams
edutingam
evitisop
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid
hp
llams
edutingam
mid
hp
etar
gninraeL
etar
gninraeL
etar
gninraeL
etar
gninraeLTable 3: Correlation coefficients for all quantities for ViT model and CIFAR100 dataset. The
corresponding plots are presented in Figures 10, Figure 11 and Figure 12.
Metric Complexity ψlr ψbs Ψ τ
E 0.78 0.71 0.74 0.70
α√
Mag( n) 0.78 0.71 0.74 0.72
Mag(0.01) 0.15 0.11 0.13 0.17
ρ √
S PMag( n) 0.78 0.71 0.74 0.72
PMag(0.01) 0.60 0.62 0.61 0.56
dim [20] 0.77 -0.71 0.03 0.36
PH
E 0.77 0.51 0.64 0.67
α
Mag(0.01) [3] 0.77 -0.69 0.04 0.50
√
Mag( n) 0.77 -0.45 0.16 0.54
∥·∥
2 PMag(0.01) 0.82 0.53 0.68 0.66
√
PMag( n) 0.78 -0.45 0.16 0.54
dim [9] 0.77 -0.71 0.03 0.37
PH
E 0.77 0.71 0.74 0.70
α√
Mag( n) 0.77 0.71 0.74 0.71
Mag(0.01) 0.68 0.51 0.59 0.59
01 √
PMag( n) 0.77 0.71 0.74 0.70
PMag(0.01) 0.72 0.71 0.71 0.63
dim 0.73 0.02 0.37 0.57
PH
103 103 1.8×100 103
Batch size
1.7×100 8
102
1.6×100 1 36
2
103 1.5×100 64 Batch 8size 104 Batch 8size 104
1.4×100
1 22 58
6
104
16 16
101 32 32 1.3×100
64 64
1 22 58 6 102 1 22 58 6 1.2×100
105 105 105
20 25 30 20 25 30 20 25 30
Generalization gap Generalization gap Generalization gap
103 103 1.025×100 103
1.02×100
101
103 1.015×100
Batch size 104 Batch size 104 Batch size 104
8 8 8
16 16 1.01×100 16
32 32 32
102
6 14
28
6 14
28
1.005×100 6 14
28
256 256 256
105 105 100 105
20 25 30 20 25 30 20 25 30
Generalization gap Generalization gap Generalization gap
Figure 10: ViT on CIFAR100 with ρ
S
for these experiments; this behavior was already reflected in Table 1 through very low Kendall’s
coefficients, indicating the absence of meaningful correlation. However, Mag(√n) and PMag(√n)
provide significantly better correlation, which supports our main claims, as s=√n has been argued
in Section 3.3 to be a particulary relevant choice of scale factor.
Note finally that the PH-dim plots for the 01-pseudometric failed to produce numbers in these
graphs experiments (this is why they are either missing or look irrelevant). As before, we gave away
this fact in Table 1 by imposing our granulated Kendall’s coefficients implementation to return zeros
41
ahpla
E
edutingam
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid
hp
etar
gninraeL
etar
gninraeL103 103 103
Batch size
103 1.175×100 8
4×103 1.17×100 1 36
2
1.165×100 64 Batch 8size 104 3×103 Batch 8size 104 1.16×100 1 22 58 6 104 102 16 16
32 32 1.155×100
64 64
128 2×103 128 1.15×100
256 105 256 105 1.145×100 105
20 25 30 20 25 30 20 25 30
Generalization gap Generalization gap Generalization gap
103 103 1.6×100 103
4×100 1.5×100
4×103 1.4×100
3×100
3×103 Batch 8size 104 Batch 8size 104 1.3×100 Batch 8size 104
16 16 1.2×100 16
32 2×100 32 32
64 64 1.1×100 64
2×103 128 128 128
256 256 256
105 105 100 105
20 25 30 20 25 30 20 25 30
Generalization gap Generalization gap Generalization gap
Figure 11: ViT on CIFAR100 with
2
∥·∥
103 103 2.2×100 103
Batch size
102 2×100 8
101
103 1.8×100 1 36
2
1.6×100 64 100 Batch 8size 104 102 Batch 8size 104 1 22 58 6 104 16 16 1.4×100
101 32 32
64 64 1.2×100
128 101 128
102 256 256
105 105 105
20 25 30 20 25 30 20 25 30
Generalization gap Generalization gap Generalization gap
103 102 103 103
1.008×100
103
1.006×100
102 Batch 8size 104 101 Batch 8size 104 1.004×100 Batch 8size 104
16 16 16
32 32 32
101
64 64
1.002×100
64
128 128 128
100
256
105
100 256
105
100 256
105
20 25 30 20 25 30 20 25 30
Generalization gap Generalization gap Generalization gap
Figure 12: ViT on CIFAR100 with 01-pseudometric
in the absence of correlation, hence the small numbers observed in this case. That being said, this
behavior shouldnot be seen as anissue. Indeed, PH-dim with 01-pseudometricconsists(in theory) in
estimating the dimension of a subset of a discrete hypercube, which is always 0. The reason we still
reported PH-dim for this pseudometric is for consistence and to test the implementation of [9, 20] in
this non-standard setting; it is however not theoretically grounded.
42
edutingam
ahpla E
edutingam
ahpla E
edutingam
evitisop
llams
edutingam
evitisop
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid hp
mid hp
llams
edutingam
etar
gninraeL
etar
gninraeL
etar
gninraeL
etar
gninraeLTable 4: Correlation coefficients for all quantities for CaiT model and CIFAR10 dataset. The
corresponding plots can be seen in Figures 13, 14 and 15.
Metric Complexity ψlr ψbs Ψ τ
E 0.91 0.33 0.62 0.78
α√
Mag( n) 0.91 0.33 0.62 0.75
Mag(0.01) 0.75 0.29 0.52 0.69
ρ √
S PMag( n) 0.91 0.33 0.62 0.75
PMag(0.01) 0.87 0.38 0.62 0.75
dim [20] 0.91 -0.19 0.36 0.75
PH
E 0.91 0.38 0.64 0.85
α√
Mag( n) 0.89 -0.42 0.23 0.73
Mag(0.01) [3] 0.91 -0.15 0.37 0.77
∥·∥ √
2 PMag( n) 0.89 -0.42 0.23 0.73
PMag(0.01) 0.53 0.26 0.4 0.48
dim [9] 0.91 -0.31 0.30 0.67
PH
E 0.91 0.33 0.62 0.84
α√
Mag( n) 0.91 0.33 0.62 0.77
Mag(0.01) 0.86 0.33 0.60 0.76
01 √
PMag( n) 0.91 0.33 0.62 0.79
PMag(0.01) 0.88 0.44 0.66 0.71
dim 0.91 -0.13 0.39 0.78
PH
103 103
2.4×100
103
2.2×100
102 2×100
103 1.8×100
101
Batch 8size 104 Batch 8size 104 1.6×100 Batch 8size 104
16 16 16
32 32 1.4×100 32
64 64 64
100 128 102 128 1.2×100 128
256 256 256
105 105 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
103 110033 103
1.025×100 Batch size
8
1.02×100 16 32 103 64
Batch 8size 104 110011 BBaatctchh 8s 8sizizee 110044 1.015×100 1 22 58
6
104
16 1166 1.01×100
32 3322
102 64 6644 1.005×100
128 112288
256 225566
105 110055 100 105
6 8 10 12 66 88 1100 1122 6 8 10 12
Generalization gap GGeenneeraralilzizaatitoionn g gaapp Generalization gap
Figure 13: CaiT on CIFAR10 with ρ -pseudometric.
S
43
ahpla
E
edutingam
edutingam
evitisop
llllaammss
eedduuttiinnggaamm
eevviittiissoopp
etar
gninraeL
llams
edutingam
mid
hp
etar
gninraeL
etar
gninraeL103 103 103
5×103
1.17×100
103 1.16×100
4×103
Batch 8size 104 Batch 8size 104 1.15×100 Batch 8size 104
16 16 1.14×100 16
102 32 32 32
6 14
28
3×103 6 14
28
1.13×100 6 14
28
256 256 256
105 105 1.12×100 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
5×103
103 103 2.2×100
Batch size
103
2×100 8
6×100 1.8×100 1 36 2
4×103 1.6×100 64
Batch 8size 104 4×100 Batch 8size 104
1.4×100
1 22 58
6
104
16 16
32 3×100 32
3×103 64 64 1.2×100
128 128
256 2×100 256
105 105 100 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
Figure 14: CaiT on CIFAR10 with distance.
2
∥·∥
103 103 103
102 2.6×100
2.4×100
101 103 2. 22 ×× 11 00 00
100 Batch 8size 104 102 Batch 8size 104 11 .. 68 ×× 11 00 00 Batch 8size 104 16 16 16
101 3 62 4 3 62 4 1.4×100 3 62 4
128 101 128 1.2×100 128
256 256 256
102 105 105 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
103 103 103
1.004×100 Batch size
8
103 16
101 1.003×100 3 62 4
102 Batch 8size 104 Batch 8size 104 1.002×100 1 22 58 6 104
16 16
32 32
101 64 64 1.001×100
128 128
256 100 256 100
105 105 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
Figure 15: CaiT on CIFAR10 with 01-pseudometric.
44
edutingam
ahpla E
edutingam
ahpla
E
edutingam
evitisop
llams
edutingam
evitisop
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid
hp
llams
edutingam
mid hp
etar
gninraeL
etar
gninraeL
etar
gninraeL
etar
gninraeLTable 5: Correlation coefficients for all quantities for CaiT model and CIFAR100 dataset. The
corresponding plots can be seen in 16, 17 and 18
Metric Complexity ψlr ψbs Ψ τ
E 0.67 0.13 0.40 0.54
α√
Mag( n) 0.67 0.13 0.40 0.52
Mag(0.01) 0.47 -0.18 0.14 0.36
ρ √
S PMag( n) 0.67 0.13 0.40 0.53
PMag(0.01) 0.76 0.53 0.64 0.71
dim [20] 0.67 -0.13 0.27 0.56
PH
E 0.67 0.40 0.53 0.64
α√
Mag( n) 0.68 0.33 0.50 0.65
Mag(0.01) [3] 0.66 -0.33 0.17 0.54
∥·∥ √
2 PMag( n) 0.68 0.33 0.50 0.65
PMag(0.01) 0.62 0.09 0.36 0.43
dim [9] 0.64 -0.09 0.28 0.50
PH
E 0.67 0.13 0.40 0.52
α√
Mag( n) 0.67 0.13 0.40 0.57
Mag(0.01) 0.61 0.18 0.40 0.43
01 √
PMag( n) 0.67 0.11 0.39 0.53
PMag(0.01) 0.65 0.41 0.53 0.48
01 loss 0.58 0.07 0.32 0.57
103 103 1.9×100 103
Batch size
1.8×100
8
102 1.7×100 16
103 1.6×100 3 62
4
101
Batch 8size 104 Batch 8size 104 11 .. 45 ×× 11 00 00 1 22 58
6
104
16 16
32 32 1.3×100
64 102 64
100 128 128 1.2×100
256 256
105 105 1.1×100 105
20 25 30 35 20 25 30 35 20 25 30 35
Generalization gap Generalization gap Generalization gap
103 103 1.0175×100 103
1.015×100
103 1.0125×100
Batch size 104 101 Batch size 104 1.01×100 Batch size 104
8 8 1.0075×100 8
16 16 16
102 32 32 1.005×100 32
64 64 64
128 128 1.0025×100 128
256 256 256
105 105 100 105
20 25 30 35 20 25 30 35 20 25 30 35
Generalization gap Generalization gap Generalization gap
Figure 16: CaiT on CIFAR100 with ρ -pseudometric.
S
45
ahpla
E
edutingam
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid
hp
etar
gninraeL
etar
gninraeL103 103 103
5×103
1.175×100
103 1.17×100
4×103 1.165×100
Batch 8size 104 Batch 8size 104 1.16×100 Batch 8size 104
16 16 1.155×100 16
102 32 32 1.15×100 32
64 3×103 64 64
128 128 1.145×100 128
256 256 256
105 105 1.14×100 105
20 25 30 35 20 25 30 35 20 25 30 35
Generalization gap Generalization gap Generalization gap
5×103
103 103 2.2×100 103
6×100 2×100
1.8×100
4×103 4×100 1.6×100
Batch
8
1s 6ize 104
3×100
Batch
8
1s 6ize 104
1.4×100
Batch
8
1s 6ize 104
32 32 32
3×103 64 64 1.2×100 64
128 2×100 128 128
256 256 256
105 105 100 105
20 25 30 35 20 25 30 35 20 25 30 35
Generalization gap Generalization gap Generalization gap
Figure 17: CaiT on CIFAR100 with .
2
∥·∥
103 103 2.2×100 103
102 Batch size
2×100 8
101 103 1.8×100 1 36 2
64
100 Batch 8size 104 102 Batch 8size 104 1.6×100 1 22 58 6 104 16 16 1.4×100
32 32
101 64 64
128 101 128 1.2×100
256 256
102 105 105 105
20 25 30 35 20 25 30 35 20 25 30 35
Generalization gap Generalization gap Generalization gap
103 103 103
1.004×100
103
1.003×100
102 Batch 8size 104
101
Batch 8size 104
1.002×100
Batch 8size 104
16 16 16 101 3 62 4 3 62 4 1.001×100 3 62 4
128 128 128
22 056
25 30 35
105 100 22 056
25 30 35
105 100 22 056
25 30 35
105
Generalization gap Generalization gap Generalization gap
Figure 18: CaiT on CIFAR100 with 01-pseudometric.
46
edutingam
ahpla E
edutingam
ahpla
E
edutingam
evitisop
llams
edutingam
evitisop
edutingam
evitisop
llams
edutingam
evitisop
llamsedutingam
mid
hp
mid hp
llams
edutingam
etar
gninraeL
etar
gninraeL
etar
gninraeL
etar
gninraeLTable6: CorrelationcoefficientsforallquantitiesforSwinmodelandCIFAR10. Thecorresponding
plots are in Figure 19, 20 and 21.
Metric Complexity ψlr ψbs Ψ τ
E 0.97 0.58 0.77 0.86
α√
Mag( n) 0.97 0.57 0.77 0.84
Mag(0.01) 0.87 0.58 0.72 0.75
ρ √
S PMag( n) 0.98 0.55 0.77 0.87
PMag(0.01) 0.76 0.20 0.48 0.65
dim [20] 0.97 -0.57 0.19 0.67
PH
E 0.97 -0.04 0.46 0.84
α√
Mag( n) 0.97 -0.43 0.27 0.77
Mag(0.01) [3] 0.98 -0.22 0.38 0.80
∥·∥ √
2 PMag( n) 0.98 -0.43 0.27 0.77
PMag(0.01) 0.51 0.53 0.52 0.47
dim [9] 0.95 -0.57 0.18 0.69
PH
E 0.97 0.58 0.77 0.84
α√
Mag( n) 0.97 0.58 0.77 0.86
Mag(0.01) 0.94 0.48 0.71 0.79
01 √
PMag( n) 0.98 0.58 0.78 0.87
PMag(0.01) 0.92 0.42 0.67 0.78
dim 0.93 -0.28 0.32 0.69
PH
103 103
2.2×100
103
2×100
102
103 1.8×100
101 Batch size 104 Batch size 104 1.6×100 Batch size 104
8 8 8 16 16 1.4×100 16
100 3 62 4 102 3 62 4 3 62 4
128 128 1.2×100 128
256 256 256
105 105 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
103 103 103
1.03×100 Batch size
8
1.025×100 16
103 32 101 1.02×100 64
Batch 8size 104 Batch 8size 104 1.015×100 1 22 58 6 104
102
16 16
32 32 1.01×100
64 64 1.005×100
128 128
101 256
105
100 256
105
100
105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
Figure 19: Swin on CIFAR10 with ρ -pseudometric.
S
47
ahpla
E
edutingam
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid
hp
etar
gninraeL
etar
gninraeL103 103 103
5×103 1.17×100
103 1.16×100
4×103
1.15×100
Batch 8size 104 Batch 8size 104
1.14×100
Batch 8size 104
16 16 16
102 32 3×103 32 32
64 64 1.13×100 64
128 128 128
256 256 1.12×100 256
105 105 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
103 103 103
5×103 2×100 Batch size
8
1.8×100 16
4×103 6×100 1.6×100 3 62 4
Batch 8size 104 4×100 Batch 8size 104 1.4×100 1 22 58 6 104
16 3×100 16 3×103 3 62
4
3 62
4
1.2×100
128 2×100 128
256 256
105 105 100 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
Figure 20: Swin on CIFAR10 with .
2
∥·∥
102 103 103 2.4×100 103
2.2×100
101 103 2×100
1.8×100
100 Batch 8size 104 102 Batch 8size 104 1.6×100 Batch 8size 104
101 16 16 1.4×100 16
32 32 32
102 6 14 28 101 6 14 28 1.2×100 6 14 28
256 256 256
105 105 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
103 103 103
1.004×100 Batch size
8
103 16
1.003×100 32 101 64
102 Batch 8size 104 Batch 8size 104 1.002×100 1 22 58 6 104
16 16
101 3 62
4
3 62
4
1.001×100
128 128
100 256 105 100 256 105 100 105
6 8 10 12 6 8 10 12 6 8 10 12
Generalization gap Generalization gap Generalization gap
Figure 21: Swin on CIFAR10 with 01-pseudometric.
48
edutingam
ahpla
E
edutingam
ahpla
E
edutingam
evitisop
llams
edutingam
evitisop
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid
hp
mid
hp
llams
edutingam
etar
gninraeL
etar
gninraeL
etar
gninraeL
etar
gninraeLTable 7: Correlation coefficients for all quantities for Swin model and CIFAR100. See Figures 22,
23 and 24 for the corresponding plots.
Metric Complexity ψlr ψbs Ψ τ
E 0.69 0.47 0.58 0.62
α√
Mag( n) 0.56 0.47 0.51 0.51
Mag(0.01) 0.31 0.47 0.39 0.33
ρ √
S PMag( n) 0.69 0.47 0.58 0.63
PMag(0.01) 0.71 0.58 0.64 0.68
dim [20] 0.69 -0.47 0.11 0.50
PH
E 0.69 0.22 0.46 0.63
α√
Mag( n) 0.71 -0.57 0.07 0.53
Mag(0.01) [3] 0.69 -0.44 0.12 0.53
∥·∥ √
2 PMag( n) 0.71 -0.57 0.07 0.53
PMag(0.01) 0.64 0.51 0.58 0.46
dim [9] 0.69 -0.47 0.11 0.45
PH
E 0.69 0.47 0.58 0.61
α√
Mag( n) 0.69 0.47 0.58 0.62
Mag(0.01) 0.61 0.27 0.44 0.50
01 √
PMag( n) 0.69 0.47 0.58 0.62
PMag(0.01) 0.65 0.49 0.57 0.54
dim 0.64 0.04 0.34 0.51
PH
103 103
1.9×100 Batch size
103
1.8×100 8
102 1.7×100 16
103 1.6×100 32
64
101 Batch 8size 104 Batch 8size 104 11 .. 45 ×× 11 00 00 1 22 58
6
104
16 16
100 3 62 4 102 3 62 4 11 .. 23 ×× 11 00 00
128 128
256 256 1.1×100
105 105 105
22.5 25.0 27.5 30.0 32.5 22.5 25.0 27.5 30.0 32.5 22.5 25.0 27.5 30.0 32.5
Generalization gap Generalization gap Generalization gap
103 103 103
1.014×100
1.012×100
103 Batch size
1.01×100 8 Batch size 104 101 Batch size 104 1.008×100 1 36 2 104
8 8
102 16 16 1.006×100 64
128 32 32 1.004×100 256
64 64
128 128 1.002×100
101 256
105
256
105
100
105
22.5 25.0 27.5 30.0 32.5 22.5 25.0 27.5 30.0 32.5 22.5 25.0 27.5 30.0 32.5
Generalization gap Generalization gap Generalization gap
Figure 22: Swin on CIFAR100 with ρ -pseudometric.
S
49
ahpla
E
edutingam
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid
hp
etar
gninraeL
etar gninraeL103 103 103
5×103 1.17×100
1.165×100
103
1.16×100
4×103
Batch 8size 104 Batch 8size 104 1. 11 .5 15 5× ×1 10 00 0 Batch 8size 104 102 1 36 2 1 36 2 1.145×100 1 36 2
64 3×103 64 1.14×100 64
128 128 128
256 256 1.135×100 256
22.5 25.0 27.5 30.0 32.5
105
22.5 25.0 27.5 30.0 32.5
105
22.5 25.0 27.5 30.0 32.5
105
Generalization gap Generalization gap Generalization gap
5×103 103 103 2×100 103
6×100
1.8×100
Batch size 4×103 4×100 1.6×100 8
Batch 8 1s 6ize 104 3×100 Batch 8 1s 6ize 104 1.4×100 1 3 66 2 4 104
32 32 128
3×103 6 14 28 2×100 6 14 28 1.2×100 256
256 256
22.5 25.0 27.5 30.0 32.5 105 22.5 25.0 27.5 30.0 32.5 105 100 22.5 25.0 27.5 30.0 32.5 105
Generalization gap Generalization gap Generalization gap
Figure 23: Swin on CIFAR100 with .
2
∥·∥
103 103 103
102 2×100 Batch size
8
101 103 1.8×100 16
32
1.6×100 64
100 Batch 8size 104 102 Batch 8size 104
1.4×100
1 22 58
6
104
101 16 16
32 32
64 101 64 1.2×100
102 128 128
256 256
105 105 105
22.5 25.0 27.5 30.0 32.5 22.5 25.0 27.5 30.0 32.5 22.5 25.0 27.5 30.0 32.5
Generalization gap Generalization gap Generalization gap
103 103 1.004×100 103
1.0035×100
103 1.003×100 Batch size
101 1.0025×100 8 16 102 Batch size 104 Batch size 104 1.002×100 32 104
8 16 8 16 1.0015×100 6 14 28 101 32 32 1.001×100 256
64 64
128 128 1.0005×100
100 256 105 100 256 105 100 105
22.5 25.0 27.5 30.0 32.5 22.5 25.0 27.5 30.0 32.5 22.5 25.0 27.5 30.0 32.5
Generalization gap Generalization gap Generalization gap
Figure 24: Swin on CIFAR100 with 01.
50
edutingam
ahpla
E
edutingam
ahpla E
edutingam
evitisop
llams
edutingam
evitisop
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid hp
mid
hp
llams
edutingam
etar
gninraeL
etar
gninraeL
etar
gninraeL
etar gninraeL104 104 104
101 Batch size
6×102 104 8
16
32
100 4×102 64 Batch 8size 105 Batch 8size 105 103 1 22 58 6 105 16 3×102 16
101 32 32
64 64
128 2×102 128 102
256 256
106 106 106
2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2
Generalization gap Generalization gap Generalization gap
104 104 104
Batch size 102 Batch size 1.005×100 Batch size
8 8 8
102 16 16 1.004×100 16 32 32 32
64 64 64
128 128 1.003×100 128 256 105 101 256 105 256 105
101 1.002×100
1.001×100
100 106 100 106 100 106
2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2
Generalization gap Generalization gap Generalization gap
Figure 25: GraphSage on MNIST with ρ -pseudometric.
S
104 104 104
1.13×100
101 103
1.12×100
Batch size 105 Batch size 105 1.11×100 Batch size 105
8 8 8 100 1 36 2 102 1 36 2 1.1×100 1 36 2
64 64 64
128 128 1.09×100 128
256 256 256
106 106 1.08×100 106
2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2
Generalization gap Generalization gap Generalization gap
104 104 104
6×100
1.01×100
103
4×100 1.008×100
Batch size 105
3×100
Batch size 105 1.006×100 Batch size 105
102 8 8 8
16 2×100 16 1.004×100 16
32 32 32
64 64 1.002×100 64
128 128 128
101 256 106 100 256 106 100 256 106
2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2
Generalization gap Generalization gap Generalization gap
Figure 26: GraphSage on MNIST with .
2
∥·∥
51
ahpla E
edutingam
ahpla
E
edutingam
llams
edutingam
evitisop
edutingam
evitisop
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
mid
hp
llams
edutingam
mid hp
etar
gninraeL
etar
gninraeL
etar
gninraeL
etar
gninraeL104 103 104 1.1×104
Batch size Batch size
100 8
16
8
16
11 .. 00 575 ×× 1010 44
3 62 4 102 3 62 4 1.025×104
101 1 22 58
6
105 Batch 8size 105 102 1 22 58
6
104
101 16 9.75×105
102 32 9.5×105
64
128 9.25×105
103 106 100 256 106 9×105
2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2 2.8 3.0 3.2
Generalization gap Generalization gap Generalization gap
104 104 104
102 Batch size Batch size Batch size
8 8 8
16 16 16
32 32 32 64 64 64
101 1 22 58 6 105 1 22 58 6 105 1 22 58 6 105
100
106
100
106
100
106
2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2 2.6 2.8 3.0 3.2
Generalization gap Generalization gap Generalization gap
Figure 27: GraphSage on MNIST with 01.
104 104 104
Batch size Batchsize Batch size
101 8 16 8 16 103 8 16
32 32 32
64 64 64 1 22 58 6 105 1 22 58 6 105 1 22 58 6 105
100
102
102
2.8 3.0 3.2
106
2.8 3.0 3.2
106
2.8 3.0 3.2
106
Generalization gap Generalization gap Generalization gap
Batch size 104 Batchsize 104 1.004×100 Batchsize 104
8 8 1.0035×100 8
102 1 36 2 1 36 2 1.003×100 1 36 2 64 64 1.0025×100 64
1 22 58 6 105 101 1 22 58 6 105 1.002×100 1 22 58 6 105
101
1.0015×100
1.001×100
1.0005×100
2.8 3.0 3.2 106 100 2.8 3.0 3.2 106 100 2.8 3.0 3.2 106
Generalization gap Generalization gap Generalization gap
Figure 28: GatedGCN on MNIST with ρ -pseudometric.
S
52
ahpla
E
edutingam
ahpla E
edutingam
edutingam
evitisop
llams
edutingam
evitisop
llams
edutingam
evitisop
edutingam
evitisop
llams
edutingam
mid
hp
llams
edutingam
mid hp
etar
gninraeL
etar
gninraeL
etar
gninraeL
etar
gninraeL104 104 104
1.14×100
101 103 1.135×100
1.13×100
105 105 105
1.125×100 Batch size Batch size Batch size
100 16 102 16 1.12×100 16
32 32 32
64 64 1.115×100 64
128 128 128
106 106 106
2.8 2.9 3.0 3.1 2.8 2.9 3.0 3.1 2.8 2.9 3.0 3.1
Generalization gap Generalization gap Generalization gap
104 104 104
Batch size 1.01×100 Batch size
16 16
103 4×100 32 1.008×100 32 64 64
3×100 128 128
1.006×100 105 105 105
102 Batch size 2×100 1.004×100
16
32 1.002×100
64
101 128 106 100 106 100 106
2.8 2.9 3.0 3.1 2.8 2.9 3.0 3.1 2.8 2.9 3.0 3.1
Generalization gap Generalization gap Generalization gap
Figure 29: GatedGCN on MNIST with .
2
∥·∥
104 103 104 104
Batch size Batch size
8
102
8
100
16 16
32 102 32
101 64 64
1 22 58
6
105 Batch 8size 105
101
1 22 58
6
105
102 101 16
32
64
103 128
100 256 100
106 106 106
2.8 3.0 3.2 2.8 3.0 3.2 2.8 3.0 3.2
Generalization gap Generalization gap Generalization gap
104 104
Batch size Batch size
1.00002×100 8 1.00002×100 8
16 16
32 32
1.00002×100 64 1.00002×100 64
128 128
1.00001×100 256
105
1.00001×100 256
105
1.00001×100 1.00001×100
100 100
106 106
2.8 3.0 3.2 2.8 3.0 3.2
Generalization gap Generalization gap
Figure 30: GatedGCN on MNIST with 01.
53
ahpla
E
edutingam
ahpla
E
llams
edutingam
evitisop
llams
edutingam
evitisop
edutingam
evitisop
edutingam
evitisop
llams
edutingam
mid
hp
llams
edutingam
edutingam
etar
gninraeL
etar
gninraeL
etar
gninraeL
etar
gninraeL