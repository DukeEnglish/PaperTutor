Unifying3DRepresentationandControlofDiverseRobotswithaSingle
Camera
Sizhe Lester Li‚àó, Annan Zhang, Boyuan Chen, Hanna Matusik, Chao Liu, Daniela Rus,
andVincentSitzmann‚àó
Mirroringthecomplexstructuresanddiversefunctionsofnaturalorganismsisalong-standingchallengeinrobotics[1,2,3,
4].Modernfabricationtechniqueshavedramaticallyexpandedfeasiblehardware[5,6,7,8],yetdeployingthesesystems
requirescontrolsoftwaretotranslatedesiredmotionsintoactuatorcommands.Whileconventionalrobotscaneasilybe
modeledasrigidlinksconnectedviajoints,itremainsanopenchallengetomodelandcontrolbio-inspiredrobotsthatare
oftenmulti-materialorsoft,lacksensingcapabilities,andmaychangetheirmaterialpropertieswithuse[9,10,11,12].Here,
weintroduceNeuralJacobianFields,anarchitecturethatautonomouslylearnstomodelandcontrolrobotsfromvision
alone.Ourapproachmakesnoassumptionsabouttherobot‚Äôsmaterials,actuation,orsensing,requiresonlyasinglecamera
forcontrol,andlearnstocontroltherobotwithoutexpertinterventionbyobservingtheexecutionofrandomcommands.
Wedemonstrateourmethodonadiversesetofrobotmanipulators,varyinginactuation,materials,fabrication,andcost.
Ourapproachachievesaccurateclosed-loopcontrolandrecoversthecausaldynamicstructureofeachrobot.Byenabling
robotcontrolwithagenericcameraastheonlysensor,weanticipateourworkwilldramaticallybroadenthedesignspace
ofroboticsystemsandserveasastartingpointforloweringthebarriertoroboticautomation.Additionalmaterialscanbe
foundontheprojectwebsite1.
1 INTRODUCTION
Modernmanufacturingtechniquespromiseanewgenerationofroboticsystemsinspiredbythediverse
mechanismsseeninnature.Whileconventionalsystemsareprecisionengineeredfromrigidpartsconnected
atdiscretejoints,bio-inspiredrobotsgenerallycombinesoft,compliantmaterialsandrigidparts,andoften
foregoconventionalmotor-drivenactuationforpneumaticandmuscle-likeactuators[9].Recentworkhas
demonstratedthatsuchhybridsoft-rigidsystemscanalreadyoutperformconventionalcounterpartsincertain
environmentswhereadaptationtochangingcircumstances[13,14]orsafetyinco-workingwithhumansis
key[15].Further,thesesystemsareamenabletomassproduction,somerequiringnohumanassembly[6],
andmaythusdramaticallylowerthecostandbarrierstoroboticautomation[16].However,deploymentof
bio-inspiredhardwareishinderedbyourcapabilityofmodelingthesesystems:Anyroboticsystemneedsto
bepairedwithamodelthatcanaccuratelypredictthemotionofkeycomponents,suchastheend-effector,
underallpossiblecommandsatalltimes.
Conventional robots were designed to make their modeling and control easy. They are usually con-
structedfromprecision-machinedpartsfabricatedoutofhigh-stiffnessmaterialswithYoung‚Äôsmoduliinthe
109‚àí1012Parange[9].Connectedbylow-tolerancejoints,theserigidrobotsareadequatelymodeledasa
kinematicchainconsistingofidealizedrigidlinks.Accuratesensorsineveryjointthenallowafaithful3D
reconstructionoftherobotduringdeployment.Withthisinplace,anexpertcanreliablymodelthemotionof
therobotunderallpossiblemotorcommands,anddesigncontrolalgorithmstoexecutedesiredmotions.
Incontrast,thebodiesofsoftandbio-inspiredrobotsaredifficulttomodel.Theyaretypicallymadeout
ofmaterialsthatmatchthestiffnessofsoftbiologicalmaterialssuchastissue,muscles,ortendons[9,5].
Thesematerialsundergolargedeformationsduringactuationandexhibittime-dependenteffectssuchas
‚àóCorrespondingauthors:sizheli@mit.edu,sitzmann@mit.edu.AllauthorsarewiththeComputerScienceandArtificialIntelligenceLaboratory
(CSAIL),MassachusettsInstituteofTechnology,32VassarSt,Cambridge,MA02139,USA.
1https://sizhe-li.github.io/publication/neural_jacobian_field
1
4202
luJ
11
]OR.sc[
1v22780.7042:viXraS.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
a b Gradient-based Iteration 1 (0.01s)
Robot Command Predicted Motion Optimization (12hz)
Error
(0.01s) Iteration 50 (0.50s)
(0.50s)
Time (s)
DDeessiirreedd MMoottiioonn ((22DD//33DD))
Input Image
xx Command Execution
Neural 3D NNeeuurraall JJaaccoobbiiaann FFiieelldd
Reconstruction
Fig.1. ControllingrobotsfromvisionviatheNeuralJacobianField.a,ReconstructionoftheNeuralJacobianField
andmotionprediction.Fromasingleimage,amachinelearningmodelinfersa3Drepresentationoftherobotinthescene,
theNeuralJacobianField.Itencodestherobot‚Äôsgeometryandkinematics,allowingustopredictthe3Dmotionsofrobot
surfacepointsunderallpossiblecommands.Colorationindicatesthesensitivityofthatpointtoindividualcommand
channels.b,Closed-loopcontrolfromvision.Givendesiredmotiontrajectoriesinpixelspaceorin3D,weusetheNeural
JacobianFieldtooptimizefortherobotcommandthatwouldgeneratetheprescribedmotionataninteractivespeedof
approximately12Hz.Executingtherobotcommandintherealworldconfirmsthatthedesiredmotionsareachieved.
viscoelasticityandgradualweakeningthroughrepeatedloadingandunloading.Partialdifferentialequations
thatgovernthebehaviorofsoftmaterialsderivedfromcontinuummechanicsandlargedeformationtheoryare
costlytosolve,especiallyforcontrolandreal-timeapplications.Modelorderreductionmethods,geometrical
approximationmethods,andrigiddiscretizationmethodsrelyheavilyonsimplifyingassumptionsaboutthe
specificsystemanddonotuniversallygeneralize[12,10,11].Priorworkhasleveragedmachinelearning[17,
18,19,20,21]andmarker-basedvisualservoing[22,23,24,25]toovercomethesechallenges,butrequire
extensiveexpert-guidedcustomizationtobeappliedtoaparticularrobotarchitecture.Further,high-precision
motioncapturesystems[26,27,28]arecostly,bulky,andrequireacontrolledsettingfordeployment.Recent
workexploresneuralscenerepresentationsofrobotmorphology[29],butassumespreciseembeddedsensors
unavailableinsoftandbio-inspiredrobots,andrelieson3Dmotion-captureforfine-grainedcontrol.Whatis
requiredisageneral-purposecontrolmethodthatisagnostictothefabrication,actuation,embeddedsensors,
material,andmorphologyoftheroboticsystem.
The work in this article is inspired by human perception. Controlling robots with just a video game
controller,humanscanlearntopickandplaceobjectswithinminutes[30].Theonlysensorswerequireare
oureyes:Fromvisionalone,welearntoreconstructtherobot‚Äôs3Dconfigurationandtopredictitsmotionas
afunctionofthecontrolinputswegenerate.
Inthisarticle,weintroduceNeuralJacobianFields,amachine-learningapproachthatcancontrolrobots
fromasinglevideocamerastream.Wetrainourframeworkusing2-3hoursofmulti-viewvideooftherobot
executingrandomlygeneratedcommandscapturedbytwelveconsumer-gradeRGB-Dvideocameras.No
humanannotationorexpertcustomizationisnecessarytolearntocontrolanewrobot.Aftertraining,our
methodcancontroltherobottoexecutedesiredmotionsusingonlyasinglevideocamera.Relyingonvision
astheonlysensor,NeuralJacobianFieldsdonotmakeassumptionsaboutthekinematics,dynamics,material,
2Unifying3DRepresentationandControlofDiverseRobotswithaSingleCamera
actuation,orsensingcapabilitiesoftherobot.Ourmethodisuniquelyenabledbyrecentadvancementsin
computervision,neuralscenerepresentation,motiontracking,anddifferentiablerendering[31,32,33].
WeevaluateNeuralJacobianFieldonawiderangeofroboticmanipulationsystems,specificallya3D
printedhybridsoft-rigidpneumatichand[34],acompliantwrist-likeroboticplatformmadeoutofhanded
shearingauxetics(HSA)[35],acommerciallyavailableAllegrohandwith16degreesoffreedom[36],anda
low-costeducationalrobotarm[37].Acrossallthesesystems,weshowthatourmethodreliablylearnsto
reconstructtheir3Dconfigurationandpredicttheirmotionatalltimes.
Ourmethodunshacklesthehardwaredesignofrobotsfromourabilitytomanuallymodelthem,whichin
thepasthasdictatedprecisionmanufacturing,costlymaterials,extensivesensingcapabilities,andrelianceon
conventional,rigidbuildingblocks.Ourmethodthushasthepotentialtodramaticallybroadenthedesign
spaceofrobotsthatcanbedeployedinpracticeaswellasloweringthecostandbarrierstoadoptingrobotic
automationbyenablingprecisioncontroloflow-costrobots.
2 THENEURALJACOBIANFIELD
Ourframeworkcomprisestwokeycomponents:(1)adeep-learning-basedstateestimationmodelthatinfersa
3Drepresentationoftherobotthatencodesbothits3Dgeometryaswellasitsdifferentialkinematics‚Äîhowany
pointin3Dwillmoveunderanypossiblerobotcommand‚Äîfromonlyasinglevideostream.(2)Aclosed-loop
inversedynamicscontrollerthattranslatesdesiredmotionsspecifiedinthe2Dinputtorobotcommandsat
interactivespeeds.AschematicoverviewofthedeployedsystemisshowninFigure1.
ThestateestimationmodelisadeeplearningarchitecturethatmapsasingleimageIoftherobottoa3D
neuralscenerepresentation.This3Drepresentationmapsany3Dcoordinatetofeaturesthatdescribethe
robot‚Äôsgeometricandkinematicpropertiesatthat3Dcoordinate[38,39,33].Specifically,wereconstructboth
aneuralradiancefield[38]thatencodestherobot‚Äôs3Dshapeandappearanceatevery3Dcoordinate,aswell
asanovelNeuralJacobianFieldthatmapseachpointin3Dtoalinearoperatorthatexpressesthatpoint‚Äôs3D
motionasafunctionofrobotactuatorcommands.
Theneuralradiancefieldmapsa3Dcoordinatetoitsdensityandradiance.Thisservesasarepresentation
ofthegeometryoftherobot,asthedensityvalueateverypointinthe3Dsceneencodestheregionsof3D
spaceoccupiedbytherobot.
TheNeuralJacobianFieldencodeshowany3Dcoordinatewillmoveasafunctionofanypossibleactuator
command.Itservesasarepresentationofthedifferentialkinematicsoftherobot.Itgeneralizestheconventional
systemJacobianinrobotics.Traditionally,expertsmodelarobotbydesigningadynamicalsystemthathas
stateq ‚àà Rùëö ,inputcommandu ‚àà Rùëõ ,anddynamicsq+ = f(q,u) whereq+ denotesthestateofthenext
timestep.ThesystemJacobianJ(q,u) = ùúïf ùúï(q u,u) isthematrixthatrelatesthechangeofcommandutothe
changeofstateq,whicharisesfromthelinearizationoff aroundthenominalpoint(q¬Ø,u¬Ø),asùõøq=J| q¬Ø,u¬Øùõøu.
Thisapproachreliesonexpertstodesignthesystem‚Äôsstateencodingqanddynamicsfonacase-by-casebasis.
Whilethisisfeasibleforconventionalrobots,itischallengingforhybridsoft-rigid,insufficientlysensorized,
andunder-actuatedsystems,orsystemswithsignificantbacklashduetoimprecisemanufacturing.
OurNeuralJacobianFieldinsteaddirectlymapsany3DpointxtoitscorrespondingsystemJacobian.
Insteadofconditioningonanexpert-designedstaterepresentationq,theJacobianFieldisreconstructed
directlyfromtheinputimageIviadeeplearning[40].Specifically,J(x,I) = ùúïùúï ux describeshowachangeof
actuatorstateùõøurelatestothechangeof3Dmotionatcoordinatex,viaùõøx = J(x,I)ùõøu.Thisallowsusto
3S.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
denselypredictthe3Dmotionofanypointinspaceusingùõøx=J(x,I)ùõøu.Figure1aillustratesthe3DJacobian
fieldreconstructedbyourmethodtopredictthemotionofall3Dpointsbelongingtoasoftpneumatichand.
ToreconstructNeuralJacobianandRadianceFields,werelyonaneuralsingle-view-to-3Dmodule[40].By
reconstructingbothrobotgeometryandkinematicsdirectlyfromacameraobservation,ourstateestimation
modelisagnostictothesensorsembeddedintherobot.Insteadofexpert-modelingtherelationshipbetween
motorandsensorreadingsandthe3Dgeometryandkinematicsoftherobot,oursystemdirectlylearnsto
regressthisrelationshipfromdata.
Ourstateestimationmodelistrainedself-supervisedusingvideostreamsfrom12RGB-Dcamerasthat
observetherobotexecutingrandomcommandsfromdifferentperspectives.Weprovideadetailedillustration
ofthetrainingprocessinExtendedDataFigure5.Foreachcamerastream,weextract2Dmotionusingoptical
flowandpoint-trackingmethods.Ateverytrainingstep,weselectoneofthe12camerasasinputforour
reconstructionmethod.Fromthissingleinputimage,wereconstructtheNeuralJacobianandRadianceFields
thatencodetherobot‚Äôs3Dgeometryandappearance.Givenarobotcommand,weusetheJacobianField
topredicttheresulting3Dmotionfield.Weusevolumerendering[38]torenderthe3Dmotionfieldto2D
opticalflowofoneoftheother12camerasandcomparewiththeobservedopticalflow.Thisproceduretrains
theJacobianFieldtopredictrobotmotionaccurately.Wefurthervolumerendertheradiancefieldfromoneof
the12camerasandcomparetheRGBanddepthoutputswiththecapturedRGB-Dimages,whichtrainsour
modeltoreconstructaccurate3Dgeometry.
Aftertraining,ourframeworkenablesclosed-loopcontroltoexecutedesiredmotionsontherobotsystem
withasinglecamera.Givenasetofdesired2Dmotionvectorsvspecifiedonrobotpixellocations,weusea
gradient-basedoptimizer(Fig.1b)tosolvefortherobotcommandùõøuatinteractivespeeds.
The Jacobian Field can be directly integrated into a differential inverse kinematics controller. In our
experiments,wecreatereference2Dmotiontrajectoriesbyrecordingvideoswhereweteleoperatetherobot
toexecuteadesiredphysicaltaskandextracting2Dpointtracks[41]fromeachvideothatcapturestherobot‚Äôs
motioninimagespace.Thecontrollergeneratesactuatorcommandsthattranslateto2Dpointtrajectories
thatfollowthereferencetrajectory.Thesamecameraisusedtoprovideavideostreamforcontrol.Giventhe
currentvideoframe,weusethepointtracker‚Äôsfeaturestofindthemostsimilarpointinthereferencevideo
foreachpointinthecurrentvideo.Thecontrollerthenfirstcontrolstherobottoreplicatetheconfiguration
observedintheinitialframeofthereferencetrajectory.Thedesired2Dmotioniscomputedasthedifference
betweenthetargetandcurrent2Dlocations.ThecontrollerthenusestheJacobianFieldtosearchfortherobot
commandùõøuthatcouldgeneratethedesiredmotions.Thecontrollerexecutesthecommandontherobot,
repeatsthesearch,andadvancestothenextstepinthereferencetrajectoryiftheL2normofthedesired
motionsisunderaspecifiedthreshold.Thecontrolprocessterminatesifthecontrollerhasreachedthefinal
referencestepunderthespecifiedthreshold.WereferthereaderstotheExtendedDataSection5fordetails
onourmethod.
3 RESULTS
Weshowthenewcapabilitiesenabledbyourframeworkbycontrollingroboticsystemsthatcoverdiverse
materialtypes,varyingkinematiccomplexity,anddifferentpricepoints.Insummary,wecontrola$300,
3D-printedhybridsoft-rigidpneumatichandmountedonaconventionalrobotarm,asoftparallelmanipulator
madefromhandedshearingauxetics,a16-degrees-of-freedomrigidAllegrohand,andamanuallyassembled
DIYrobotarmwith3D-printedparts,low-costmotors,andsignificantbacklash[44](SupplementaryVideo1).
4Unifying3DRepresentationandControlofDiverseRobotswithaSingleCamera
a Pred. Dep.True Dep. Pred. Jac. Pred. Dep.True Dep. Pred. Jac. b Prediction Arm movement
Truth
Index Finger
Little Finger Thumb
Observation
Pred. Dep.True Dep. Pred. Jac.
c Pred. Dep.True Dep. Pred. Jac. Observation d First Finger Thumb
Little Finger
Middle Finger
Pred. Dep.True Dep. Pred. Jac. Pred. Dep. True Dep. Pred. Jac.
e Pred. Dep.True Dep. Pred. Jac. Pred. Dep.True Dep. Pred. Jac. f C. Clockwise Rot. Clockwise Rot.
SE SW SE SW
NE NW NE NW
North bend South bend
SE SW SE SW
Observation
NE NW NE NW
Pred. Dep.True Dep. Pred. Jac.
g Pred. Dep.True Dep. Pred. Jac. Pred. Dep.True Dep. Pred. Jac. h Motor 4 Motor 3
Motor 6 Motor 2
Observation
Pred. Dep.True Dep. Pred. Jac.
Fig.2. Reconstructionofrobotgeometryandkinematicsfromasingleimage.(a,c,e,g),Visualizationofthe
reconstructedJacobianandRadianceFields(center)andcomparisonofreconstructedandmeasuredgeometry(sides)from
asingleinputimage.Colorizationindicatesthemotionsensitivityofthe3Dpointtodifferentactuatorcommandchannels,
meaningthatoursystemsuccessfullylearnscorrespondencebetweenrobot3Dpartsandcommandchannelswithout
humanannotations.WeshowdepthpredictionsnexttomeasurementsofRGB-Dcameras,demonstratingtheaccuracyof
the3Dreconstructionacrossallsystems.(b,d,f,h),3DmotionspredictedviatheJacobianField.Wedisplaythemotions
predictedviatheNeuralJacobianField(solidcircle)forvariousmotorcommandsnexttoreferencemotionsreconstructed
fromvideostreamsviapointtracking(dottedcircle).Reconstructedmotionsarequalitativelyaccurateacrossallrobotic
systems.Althoughwemanuallycolor-codecommandchannels,ourframeworkassociatescommandchannelswith3D
motionswithoutsupervision.
5S.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
a Desired Motion Reference Traj.
Completed Motion
d
b
e
f
c
Fig.3. Closed-loopcontrolofdiverserobotsfromvision.a,Wecontrola3D-printedsoft-rigidpneumatichandto
completeagrasp(top)andexecutefingermotionunderthepresenceofoccluders(bottom).b,Ourmethodprecisely
controlseveryfingeroftheAllegrohandtocloseandformafist.c,Weuseoursystemtocontrolcomplexrotationaland
bendingmotionsonthewrist-like,softhandedshearingauxeticsplatform.d,Wecontrolasystemthatmountsasoft-rigid
pneumatichandonaconventionalUR5robotarmtoaccomplishatoolgraspandapushingaction.e,Weshowtheprocess
ofassemblingalow-cost,3D-printedrobotarmthatisdifficulttomodelandnotequippedwithanysensors[42,43,44].f,
Ourmethodisrobustagainstbacklashandjerkymotionsofthelow-costmotors,successfullyenablingtherobotarmto
drawletters‚ÄúM‚Äù,‚ÄúI‚Äù,and‚ÄúT‚Äùintheair.
AsshowninFigure2,foreachofthesechallengingroboticsystems,NeuralJacobianFieldsucceedsat
reconstructinganaccurate3Drepresentationoftherespectiverobotfromjustasingleimage.Weassigna
uniquecolortotheinfluenceofeachchanneloftheùëÅ-dimensionalmotorcommandandvisualizetheJacobian
Field.WefindthatourJacobianFieldlearnsthecausalkinematicstructureofeachrobot,identifyingwhich
commandchannelisresponsibleforactuatingwhichpartoftherobotin3Dspace.Thiscapabilityarises
fullyself-supervised,withoutanyannotationorsupervisionthatwouldmatchmotorswithrobotparts.We
6Unifying3DRepresentationandControlofDiverseRobotswithaSingleCamera
a b c Reference Traj. Jacobian Field
Baseline position
40 24.97 mm 11.72 mm
30
Baseline position
20
10 7.303 mm
0
0 5 10 15 20 25 30 35
Time (s)
d e
1 2 3 4
25
60 140 1 2 3 4
20 120
100
15 40 80
10 60 20 40
5 0 1. .7 62 71 8 d me mg 20 7.879 mm 5.009 mm 3.001 mm 5.467 mm
0
0 0
0 10 20 30 40 50 60 70
0 2 4 Tim6 e (s) 8 10 12 Time (s)
Fig.4. Quantitativeanalysisandresiliencytest.a,WemodifythedynamicsoftheHSAplatform.Weattacharodto
theplatformandappend350gcalibrationweightsatacontrolledlocation,whichleadstheplatformtotiltinitsresting
position.b,OurframeworkenablestheHSAsystemwithchangeddynamicstocompletetherotationmotion(top).We
reportthedistance-from-goalovertime(bottom).c,Usingabird‚Äôs-eyeview,weoverlaythecompleted3Dtrajectoryon
topofthestartingconfigurationoftheHSAplatform.Wecomparetheexecutiontrajectoryofourapproachwiththe
referencetrajectory.Thisvisualizationconfirmsthatourmethodisabletocounteractthephysicaleffectsoftheweight
andstabilizethemotiontrajectorytowardthetargetpath.d,Weshowdistance-from-goalontheAllegroHand,which
decreasesovertimeasweexecutethemotionplan.Wemeasuredistance-from-goalusingbothjointerrorsindegreeand
finger-tippositionsinmillimeters.e,Usingasquaredrawingtask,onthetop,wevisualizethereferencetrajectoryinwhite
andthecompletedtrajectoryinrainbow.Onthebottom,weplotdistance-from-goalovertimeonthePoppyrobotarmin
fourtrajectorysegments.
furtherdemonstratequalitatively(Fig.2)andquantitatively(ExtendedDataTable2)that,givenavarietyof
motorcommands,the3DJacobianfieldinferredfromasingleimagesuccessfullypredictsthemotionsof3D
pointsontherobot.Wequalitativelyfindthat3Dmotionspredictedbyourframeworkgivenrobotcommands
highlyagreewiththegroundtruthreferencemotions.(Fig.2).Quantitatively,acrossroboticsystems,our
methodreconstructshigh-qualitygeometryandkinematicsfromjustasingleRGBinputview.Themeandepth
predictionerroristhelargestforthepneumatichand(6.519mm),andsmallestfortheHSAplatform(1.109
mm).Thepneumatichandisactuatedthroughtranslucenttubingthatcouldposechallengestogeometry
prediction.Forflowprediction,ourframeworkachievesmeanpredictionerrorsof1.305pixelsand1.150
pixelsontheAllegrohandandpneumatichand,respectively.ThePoppyrobotarmhasthelargestmeanflow
predictionerrorof6.797pixels,asthehardwareconstantlyexperiencesbacklashduetolow-qualitymotors.
7
)ged(
rorrE
tnioJ
)mm(
ecnatsiD
rekraM
)mm(
rorrE
piT regniF
)mm(
ecnatsiDS.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
OurmethodisabletomodeltherotationalandbendingmechanismsoftheHSAplatformandachievesa
meanflowpredictionerrorof3.597pixels.
Wenextevaluatetheperformanceofourmethodforclosed-loopcontrol.FortheAllegrohand,weprescribe
thecontrollerwitha2Dtrajectorythattracksadesiredpose(Fig.3b).Uponcompletionofthetrajectory,
wequantifyerrorusingthebuilt-in,high-precisionper-jointsensorsandthehand‚Äôsprecise3Dforward
kinematicsmodel.Purelyfromvision,oursystemcontrolstheAllegrohandtocloseandopeneveryfinger
fully,achievingerrorsoflessthan3¬∞perjointandlessthan4mmforeachfingertip(Fig.4d,ExtendedData
Table1,ExtendedDataFigure7).
WedemonstrateontheHSAplatformthatoursystemcansuccessfullycontrolrobotsunderdramatically
changeddynamicswithoutretraining.WeintentionallydisturbtheHSAplatformbyattachingcalibration
weightswithatotalmassof350gtoawoodenrod,whichwegluetothetopoftheHSAplatform.The
weightsexertaverticalforceandatorqueontheplatformtop,whichmakesittiltvisiblyinitsrestingposition
(Fig.4a).Furthermore,therodandtheweightsconstituteavisualdisturbance.WeusetheOptiTrackmotion
capturesystem[26](<0.2mmofmeasurementerror)andattachmarkersonthesurfaceoftheHSAplatform
toquantifythepositionerrorsingoalposetracking.Wefindthatourvision-basedframeworkiscapableof
controllingtherobottocompletecomplexrotationalmotionsandreachthetargetconfiguration,achievingan
errorof7.303mm,effectivelyovercomingexternalperturbationonthesystem‚Äôsdynamics(Fig.4b,c,Extended
DataTable1).
Forthe3D-printedpoppyrobotarm,wedesigntargettrajectoriesdemandingtherobottodrawasquare
andtheletters‚ÄúMIT‚Äùintheair.Thesemotionsequencesareout-of-distributionanddonotexistinourtraining
data.WeattachOptiTrackmarkersontheend-effectoroftherobotarmtomeasure3Dpositionerrors.Our
frameworkachievesanaverageerroroflessthan6mminthegoalposetrackingtask(Fig.4e,ExtendedData
Table1).
Overall,ourframeworkenablesprecisecontrolofdiverseroboticsystems,includingbothconventional,
rigidsystemsaswellas3D-printed,hybrid-materialsystems,withoutanyexpertmodeling,intervention,or
otherper-robotspecializationofthealgorithm.Figure3demonstrateshowoursystemcontrolsthediverse
roboticplatformstowardsexecutingavarietyofskills.Thesystemachievessmoothtrajectoriesandsucceeds
atcontrollingthepneumatichandmountedontheUR5robottopickupatoolfromaglassanduseitto
pushanapple.OntheAllegroHand,oursystemformsafist.OntheHSAplatform,itexecutesavarietyof
extensionandrotationcommands.Finally,ourmethodisabletocontrolthelow-costpoppyrobotarmtotrace
theletters‚ÄúMIT‚Äù.Tosumup,acrossadiversesetofrobots,oursystemcancontrolthesesystemstoperforma
varietyoflong-termskillswithoutanyexpertmodelingorcustomization.
4 DISCUSSION
Wehavepresentedavision-baseddeeplearningapproachthatlearnstocontrolrobotsfromvisionalone,
withoutanyassumptionsontherobot‚Äôsmaterials,actuation,orembeddedsensors.Acrosschallengingrobotic
platforms,rangingfromconventionalrigidsystemstohybridsoft-rigid,3D-printed,compliant,andlow-cost
educationalrobots,ourframeworksucceedsatestimatingtheir3Dconfigurationfromvisionalone,discovers
theirkinematicstructurewithoutexpertintervention,andexecutesdesiredmotiontrajectorieswithhigh
precisionusingasingleRGBcamera.Forthefirsttime,oursystemenablesmodelingandcontrolof3D-printed,
compliantsystemswithoutanyhumanmodelingandundersignificantchangesintheirdynamics,replacinga
8Unifying3DRepresentationandControlofDiverseRobotswithaSingleCamera
month-longexpertmodelingprocessthatneverthelesscannotaccountforchangesinmaterial,dynamics,or
manufacturingtolerances.
Ourframeworkallowsustocontrolawiderangeofrobotsfromvisionalone.Forthistobefeasible,itis
criticalthatthedifferentialkinematicsoftherobotcanbeinferredfromvisionalone.Someapplicationsof
interestmayviolatethisassumption.Forinstance,whenobservingmobileleggedrobotsfromanexternal
camera,thecameramaynotobservewhetheragivenlegistouchingthegroundornot,andthus,cannot
determinethemotionoftherobotasafunctionofthatleg‚Äôsactuation.Similarly,fordexterousmanipulation,
sensingcontactswithanobjectiscritical.Conditioningthedeep-learningbasedinferencemethodforNeural
JacobianFieldsonadditionalsensorssuchastactileones[45,46,47]couldeffectivelyaddressthislimitation.
Thoughattesttime,onlyasinglecameraisnecessarytocontrolarobot,NeuralJacobianFieldscurrently
requiresmulti-viewvideoattrainingtime.
Ourmethoddramaticallybroadensthedesignspaceofrobotsbydecouplingtheirhardwarefromtheir
modelingandcontrol.Weanticipatethatourmethodwillenablethedeploymentofbio-inspired,hybrid
soft-rigidrobotsthatwerepreviouslypracticallyimpossibletomodelandcontrol.Ourmethodfurtherhasthe
potentialtoloweringthebarrier-to-entrytoroboticautomationbyenablingthecontrolofmass-producible,
low-costrobotsthatlacktheprecisionandsensingcapabilitiestobecontrolledwithconventionalmethods.
5 METHODS
First,wewillgiveageneraloverviewofthedataoursystemingests.Then,wewillprovidedetailsonneural
3Dreconstructionandscenerepresentation,aswellastheirtrainingdetailsviadifferentiablerendering.Next,
wewilldescribemathematicalinsightsfromourJacobianFieldparameterization.Lastly,wewilldescribethe
roboticsystemsusedinourpaper,includingdetailsontheirmodeling,control,sensing,actuation,morphology,
material,fabrication,andcosts.
5.1 DatasetCollection
Ourmethodisfullyself-supervisedanddoesnotrequireanymanualdataannotation.Weillustratethedata
collectionprocessinExtendedDataFigure5a.Wecapturemultiplevideostreamsoftherobotexecuting
randomactions.Specifically,wesetup12consumer-gradecamerasthatobservetherobotfrom12different
perspectives.
Weobtainintrinsicsdirectlyfromthecameras.Wecalibratecameraposesusing3cmApriltags[48].We
denotethevectorofmotorsetpointsasu.Wefirstmanuallyselectasaferangeforeachofthecommandlines.
Tocreateasingledatasample,werandomlyselectfromauniformdistributionùë¢,executethecommand,and
waitforittosettletoasteadystate.Wethencaptureimageswithall12cameras,anddenotethetimestepasùë°.
Wethenuniformlysampleachangeinthemotorcommandsùõøuùë°.Thenextstepcommanduùë°+1=uùë° +ùõøuùë°
isthenexecutedontherobot.Weagaincaptureimageswithall12cameras,anddenotethisastimestepùë°+1.
Thisleadstoamultiviewimagedataset,{(Iùë°0,...,Iùë°11)}ùëá
ùë°=0,wherethesuperscriptdenotesthecameraindexand
ùë° denotesthetimestep.Whileourmethoddoesnotstrictlydependonit,leveragingRGB-Dcamerasthat
capturedepthinadditiontocoloracceleratestrainingduetoadditionalgeometrysupervision.Hence,weuse
IntelRealSenseD415RGB-Dcamerasforalloftheexperimentsinthispaper.Finally,weextract2Dmotion
informationfromthisdatasetviaanoff-the-shelfopticalflowmethod,RAFT[49],whichtakesasinputtwo
ùëñ
consecutivevideoframesofoneofthecamerasandcomputesopticalflowvùë° betweenanimagecapturedat
timeùë° andùë°+1bycameraùëñ,causedbythemotorcommandùõøuùë°.Foreachtimestepùë°,ourtrainingdatasetis
9S.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
thusatupleofthefollowingform:
({(Iùë°0,dùë°0,vùë°0,P0,K0),...,(Iùë°11,dùë°11,vùë°11,P11,K11)},ùõøuùë°), (1)
i.e.,RGBimagesIùëñ ùë°,depthdùëñ ùë°,opticalflowvùëñ ùë°,posePùëñ ,andintrinsicsKùëñ fortheùëñ-thoutof12cameras,aswell
asthechangeinrobotcommandùõøuùë°.
5.2 Neural3DReconstructionandNeuralSceneRepresentation
Givenasingleimage,weleveragedeeplearningtoreconstructboththeproposedJacobianFieldaswellasa
NeuralRadianceField.BoththeJacobianFieldandtheNeuralRadianceFieldarefunctionsthatmapa3D
coordinatextoeitherthesystemJacobianortheradianceandoccupancy.
WefollowpixelNeRF[40]toreconstructboththeserepresentations.GivenanimageI ‚àà RH√óW√ó3with
heightHandwidthW,wefirstextracta2DfeaturevolumeW‚ààRH/ùëù√óW/ùëù√óùëõ whereùëùindicatesdownsampling
resultingfromconvolutionswithstridelargerthan1.SupposewewanttopredicttheJacobianJ,radiancec,
anddensityùúéata3Dcoordinatex.Wefirstprojectthat3Dcoordinateontotheimageplaneusingtheknown
cameracalibrationasùúã(x).Wethensamplethefeaturevolumeattheresultingpixelcoordinateusingbilinear
interpolationW(ùúã(x)).WefinallypredicttheJacobianJ,radiancec,anddensityùúéusingafullyconnected
neuralnetworkFC:
(J,c,ùúé)=FC(W(ùúã(x)),ùõæ(x)), (2)
whereùõæ(x)denotessine-cosinepositionalencodingofxwithsixexponentiallyincreasingfrequencies[38].
5.3 TrainingviaDifferentiableRendering
WeillustratethetrainingloopofoursysteminExtendedDataFigure5b.Ineachforwardpass,wesamplea
randomtimestepùë° anditscorrespondingtrainingtupleasdescribedinEquation1.Wethenrandomlypick
twoofthetwelvecamerasanddesignateoneasthesourcecameraandoneasthetargetcamera.Thekeyidea
ofourtrainingloopistopredictboththeimageaswellastheopticalflowobservedbythetargetcameragiven
theinputviewIinputandtherobotactionùõøuùë°.Bothimageandopticalflowofthetargetviewaregenerated
fromtheradianceandJacobianfieldsviavolumerendering[38].Ourfollowingdiscussioncloselyfollowsthat
ofpixelNeRF[40].
Wefirstparameterizetheraysthatgothrougheachpixelcenterasr(ùë†)=o+ùë†e,withthecameraorigin
o‚ààR3andtherayunitdirectionvectore‚ààR3.WethenusevolumerenderingtopredictRGBÀÜIanddepthdÀÜ
images:
‚à´ ùë°ùëì
ÀÜI(r)= ùëá(ùë°)ùúé(ùë°)c(ùë°)ùëëùë° (3)
ùë°ùëõ
‚à´ ùë°ùëì
dÀÜ(r)= ùëá(ùë°)ùúé(ùë°)ùë°ùëëùë° (4)
ùë°ùëõ
whereùëá(ùë°)
=exp(‚àí‚à´ùë°
ùúé(ùë†)ùëëùë†)accountsforocclusionviaalpha-compositing,thatis,pointsclosertothe
ùë°ùëõ
camerawithanonzerodensityùúéwilloccludethosepointsbehindthem.Foreachrayrofthetargetcamera,
wethendenselysample3Dpointsbetweennearùë° ùëõandfarùë°
ùëì
depthbounds.Foreach3Dpointùëü(ùë°) ‚ààR3,we
obtainitsdensityùúé andcolorc,andJacobianJfromEquation2.ThenotationI(r)selectsthepixelinthe
imageIthatcorrespondstotherayr.
10Unifying3DRepresentationandControlofDiverseRobotswithaSingleCamera
PredictedopticalflowvÀÜ(r)isalsocomputedviavolumerendering.Forevery3Dpointalongaray,weuse
theJacobianquantitytoadvectthe3Draysampleviar(ùë°)+J(ùë°)ùõøu.Then,weapplyalphacompositingto
bothoriginal3DraysamplesandtheiradvectedcounterpartstoobtainxÀÜ(r),xÀÜ+(r) ‚ààR3
‚à´ ùë°ùëì
xÀÜ(r)= ùëá(ùë°)ùúé(ùë°)r(ùë°)ùëëùë° (5)
ùë°ùëõ
‚à´ ùë°ùëì
xÀÜ+(r)= ùëá(ùë°)ùúé(ùë°)(r(ùë°)+J(ùë°)ùõøu)ùëëùë° (6)
ùë°ùëõ
Finally,toobtainvÀÜ(r),weprojectxÀÜ(r),xÀÜ+(r)tothe2Dimagecoordinateusingcameraintrinsicandextrinsic
parametersandcomputethepositionaldifference
vÀÜ(r)=xÀÜ+(r) image‚àíxÀÜ(r) image (7)
5.3.1 SupervisingtheRobotGeometryviaRGB-DRenderings. TopredicttheRGBanddepthimagescaptured
bythetargetcamera,werelyontheradiancefieldcomponents,colorfieldcanddensityfieldùúé,inEquation2.
ThepredictionsfortheRGBimageanddepthimageobservedbythetargetcameraareobtainedbyalpha-
compositingtheRGBcolorsandsampledepthsforeachpixelaccordingtoEquation4.Foreachtargetimage
withitscorrespondingposeP,wecomputelosses
L RGB= ‚àëÔ∏Å(cid:13) (cid:13)IÀÜ(r)‚àíI(r)(cid:13) (cid:13)2 2, (8)
r‚ààR
‚àëÔ∏Å(cid:13) (cid:13)2
L = (cid:13)dÀÜ(r)‚àíd(r)(cid:13) , (9)
depth (cid:13) (cid:13)
2
r‚ààR
whereRisthesetofallraysinthebatch.Minimizingtheselossestrainsourmodeltorecoverthecorrect
densityvaluesandthus,therobotgeometry.Notethatthedepthlossisoptionalandneuralradiancefieldsare
generallytrainedwithoutit[38,40,50],butasconsumer-gradeRGB-Dcamerasarereadilyavailable,werely
onthisadditionalsignal.
5.3.2 SupervisingtheJacobianFieldbyPredicting2DMotion. Wecomputea2Dmotionlossusingground
truthmotiontrackstosupervisetheJacobianField.
‚àëÔ∏Å
L motion= ‚à•vÀÜ(r)‚àív(r)‚à•2 2. (10)
r‚ààR
MinimizingthislosstrainsourmodeltopredictthecorrectsystemJacobianateach3Dpoint.
5.4 JacobianFieldDetails
OurJacobianfieldisadense,spatial3DgeneralizationoftheconventionalsystemJacobianinthecontextof
dynamicalsystems.Inthissection,wemathematicallydescribethemotivationsandinsightsofourparameter-
ization.WefirstderivetheconventionalsystemJacobian.Consideradynamicalsystemwithstateq‚ààRùëö
,
inputcommandu‚ààRùëõ
,anddynamicsf
:Rùëö√óRùëõ ‚Ü¶‚ÜíRùëö
.Uponreachingasteadystate,thestateofthenext
timestepq+,isgivenby
q+=f(q,u). (11)
Locallinearizationoff aroundthenominalpoint(q¬Ø,u¬Ø)yields
q+=f(q¬Ø,u¬Ø)+
ùúïf(q,u)(cid:12)
(cid:12) (cid:12) ùõøu. (12)
ùúïu (cid:12) q¬Ø,u¬Ø
11S.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
Here,J(q,u)= ùúïf ùúï(q u,u) isknownasthesystemJacobian,thematrixthatrelatesachangeofcommandutothe
changeofstateq.
Conventionally,modelingaroboticsysteminvolvesexpertsdesigningastatevectorqthatcompletely
definestherobotstate,andthenembeddingsensorstomeasureeachofthesestatevariables.Forexample,the
piece-wise-rigidmorphologyofconventionalroboticsystemsmeansthatthesetofalljointanglesisafull
statedescription,andtheseareconventionallymeasuredbyanangularsensorineachjoint.However,these
designdecisionsarechallengingforsoftandhybridsoft-rigidsystems.First,insteadofdiscretejoints,large
partsoftherobotmightdeform.Embeddingsensorstomeasurethecontinuousstateofadeformablesystem
isdifficult,bothbecausethereisnocanonicalchoiceforsensorsuniversallycompatiblewithdifferentrobots
andbecausetheirplacementandinstallationarechallenging.Next,designingthestateitselfischallenging-
incontrasttoapiece-wiserigidrobot,wherethestatevectorcanbeafinite-dimensionalconcatenationof
jointangles,thestateofadeformablerobotisinfinite-dimensionalduetocontinuousdeformations.
OurJacobianFieldsolvesthesechallenges.First,thecombinationofJacobianandNeuralRadianceFieldsis
acompleterepresentationoftherobotstate-itencodesthepositionofevery3Dpointoftherobot,aswellas
itskinematics,i.e.,howthat3Dpointwouldmoveunderanypossibleaction.Thisabsolvesusfromtheneed
tomanuallymodelarobotstateq.Second,wenotethatformanyroboticsystems,itispossibletoinfertheir
3Dconfigurationfromvisionalone.Evenifpartsoftherobotareoccluded,weareoftenstillabletoinfer
their3Dpositionfromthevisiblepartsoftherobot-justlikeobservingthebackofahumanarmallowsusto
inferwhattheoccludedsidewilllooklike.Inthiswork,weinferthestatecompletelyfromasinglecamera,
butitisstraightforwardtoaddadditionalcamerastoachievebettercoverageoftherobot[40].
WenowderivetheconnectionoftheJacobianFieldandtheper-camera2Dopticalflowweuseforits
supervision.RearrangingEquation12yields
q+‚àíf(q¬Ø,u¬Ø)=
ùúïf(q,u)(cid:12)
(cid:12) (cid:12) ùõøu. (13)
ùúïu (cid:12) q¬Ø,u¬Ø
Inpractice,ournominalpointrepresentsasteadystate,asonecanwaitfortherobotcommandtosettle.Then,
f(q¬Ø,u¬Ø)isapproximatelyq¬Ø.Weconsolidateùõøq=q+‚àíq¬Øtoexpressthechangeinrobotstateùõøqasafunctionof
theSystemJacobian
ùõøq=
ùúïf(q,u)(cid:12)
(cid:12) (cid:12) ùõøu. (14)
ùúïu (cid:12) q¬Ø,u¬Ø
Wedefinethedense3Dpositionofeveryrobotpointasthestateoftherobot.Consequently,thechangein
robotstateùõøqcanbeunderstoodasthe3Dvelocityfieldthatmovesthesepointsaccordingtotheactionùõøu.
The3Dvelocityfieldùõøqcanbemeasuredas2Dpixelmotionsvùëñ
acrossallcameraviewsusingoff-the-shelf
opticalflowandpoint-trackingmethods.Givenatrainingdatasample(vùëñ,ùõøu,Kùëñ,Pùëñ),ourneuralJacobian
fieldJ(x,ùêº)associatesthetwosignals(ùõøq,ùõøu)viavùëñ =render(J(X,I),ùõøu,Kùëñ,Pùëñ)usingEquation7,whereX
(cid:98)
representssamplesof3Dcoordinatesfromtheneuralfield.
Tosumup,ourJacobianFieldleveragesvisualmotionmeasurementsasalearningsignalandcanbetrained
self-supervisedpurelybyobservingrobotmotionunderrandomactionswithmulti-viewcameras.Itdirectly
relatesthechangeinrobotstateùõøq,definedasthe3Dmotionfieldthatadvectsevery3Dpointoftherobot
accordingtotheactionùõøu,tothemotionin2D-pixelspaceobservedbymultiplecameras.Thisprovidesa
signalforlearningwhichpartof3Dspaceissensitivetoaparticularcommandoftheroboticsystem,and
enablescontrolbyspecifyingthedesiredmotionofanyrobotpointin2Dor3D.
12Unifying3DRepresentationandControlofDiverseRobotswithaSingleCamera
5.5 DomainRandomizationDetails
Weapplydomainrandomizationtechniques[51,52,53]totheinputimagesduringtraining.Thisprocess
trainsourframeworktoberobustagainstocclusion,backgroundchanges,andothervisualdistributionshifts.
Foreveryinputframecollectedinourvideodataset,weapplyamotionthresholdtothepointtracksand
usetheSegmentAnythingframework[54]toobtainabinarymaskcorrespondingtotherobot.Weapply
backgrounddomainrandomizationasdescribedandimplementedin[53].Weapplyforegrounddomain
randomizationbyoverlayingnaturalimagesrandomlysampledfromthecocodataset[55]onourtraining
videoframes.Beforeoverlaying,werandomlycropthecocoimagesandresizethemtobesmallerthanthe
datasetvideoframesize.Wehighlightthatthetargetimagesusedtocreatesupervisionarenotaugmented,
andaretheundisturbedoriginalobservationsI,d,v.Thistrainstheneuralsingle-image-to-3Dmoduletobe
robustagainstbackgroundchangesandpartialocclusion.AsempiricallyobservedinExtendedDataFigure8
andquantitativelytestedinExtendedDataFigure6,ourframework‚ÄôsdepthandJacobianpredictionsare
robustagainstvisualperturbations.
5.6 RobotSystems
5.6.1 PneumaticHand. Basedonadesignfirstintroducedin[34],thepneumaticallyactuatedsoftrobot
hand is 3D printed in one piece using vision-controlled jetting [6]. This printing technique is based on
inkjetdepositionandenablesthecombinationofsoftandrigidmaterials.ThefingersarebasedonPneuNet
actuators[56],andthepalmfeaturesarigidcoresurroundedbysoftelastomerskin.Thehanddoesnot
requiremanualassemblyafterprinting,andthetotalfabricationcostisaround$300.Thehandisdrivenbya
15-channelproportionalvalveterminal(MPA-FB-VI,Festo).Eachchannelcanbeindividuallycontrolledto
adjustthepressureineachofthe15degreesoffreedom(DoF)ofthehand.Forourexperiments,weemploy
twoversionsofthishand.Oneismarkedbyapenwithbluecrossesandoperatedstandinguprightinthe
workspace.Theotherisunmarkedandmountedtothetoolflangeofanindustrialrobotarm(UR5,Universal
RobotsA/S).Weusethetwoshoulderjointsofthearmtomovethehandhorizontallyandverticallyinthe
workspace.
5.6.2 AllegroHand. TheAllegrohand(WonikRoboticsCo.Ltd.)isacommerciallyavailable,16-DoFan-
thropomorphicrobothandwithfourfingers[36].Eachofthefingersisactuatedbyfourservosandhassoft
siliconepaddingatthefingertips.Theservosprovidejointpositionfeedbackandcanbetorque-controlled
viacurrentcontrol.TheAllegrohandispopularamongresearchersacrossacademiaandindustry,butalso
expensive,withapricetagupwardsof$15,000[57,58,59,60].
5.6.3 HSAPlatform. TheHSAPlatformisaservo-driven,4-DoFsoftroboticplatformbasedoncompliant
actuatorsmadeoutofhandedshearingauxetics(HSAs).Introducedin[35],HSAsaremetamaterialswith
patternsthatcoupleextensionwithshearing.Tiledonthesurfaceofacylinder,HSAsenablelinearactuators
thatextendwhentwisted.Theseactuatorsare3Dprintedfrompolyurethaneresins(FPU50,CarbonInc.)
viadigitallightprojection,accordingto[61].Fouractuatorsarearrangedina2√ó2configuration,joined
rigidlytogetheratthetop,andeachdrivenbyaservomotoratthebottom.Theresultingroboticplatform
hasthreerotationalDoFsthatmimicthemovementsofahumanwristandonetranslationalDoFthatallows
ittoextendandcontract.TheHSAplatformcostsabout$1,200tomake,withthefourservos(Dynamixel
MX-28,ROBOTIS)at$260eachdominatingthecost.Eventhoughamodelthatrelatesservopositionfeedback
13S.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
toplatformposecanbelearned,therelationshipbreaksdownwhenthecompliantHSAplatformdeforms
inresponsetoexternalforces[62].HSA-basedrobotscanbesensorizedviafluidicinnervation[63]and
embeddinginternalcameras[64].However,eachofthesemethodscomesatthepriceofsignificantlyincreased
fabricationcomplexity.
5.6.4 PoppyErgoJr. TheErgoJrisanultra-low-cost6-DoFrobotarmforeducationalpurposesandpartof
theDIYopen-sourceroboticsplatformcalledthePoppyProject[65,37].TheErgoJrisdrivenbysixlow-cost
servos(DynamixelXL-320,ROBOTIS)andcanbeassembledin30-45minutes[66].Apartfromtheservosand
theelectronics,itspartsare3Dprintedonaconsumer-gradedesktopFDMprinter(P1S,BambuLab).The
ErgoJrcostus$270intotal(ExtendedDataTable3),whichcanbereducedfurtheriftheservocontrolboard
wascustom-madeinsteadofboughtoff-the-shelf.TheErgoJrisnotequippedwithanysensorsotherthanthe
servoencoder.Pastresearch[42,43,44]describesthisrobotasdifficulttomodelduetoitslowmanufacturing
qualityandsignificantbacklashinitskinematicchain.
6 DATAAVAILABILITY
Alldataneededtoevaluatetheconclusionsinthepaperarepresentinthepaperortheextendeddata.The
trainingdatawillbepubliclyreleased.
7 CODEAVAILABILITY
ThefullsourcecodefortrainingtheNeuralJacobianFields,deployingthemontherobots,andreproducing
theresultswillbemadeavailableonGitHubathttps://github.com/sizhe-li/neural-jacobian-field
8 SUPPLEMENTARYINFORMATION
SupplementaryVideo1.Summaryoftheresults.3DreconstructionofJacobianFieldsforallrobots,3D
motionpredictionsforallrobots,visuomotorcontrolofallrobotsusingtheirJacobianFields.
9 ACKNOWLEDGEMENTS
TheauthorsthankHyungJuTerrySuhforhiswritingsuggestions(systemdynamics)andTaoChenand
PulkitAgrawalfortheirhardwaresupportontheAllegrohand.V.S.acknowledgessupportfromtheSolomon
BuchsbaumResearchFundthroughMIT‚ÄôsResearchSuppportCommittee.S.L.L.wassupportedthrough
anMITPresidentialFellowship.A.Z.,H.M.,C.L.,andD.R.acknowledgesupportfromtheNationalScience
FoundationEFRIgrant1830901andtheGwangjuInstituteofScienceandTechnology.
10 AUTHORCONTRIBUTIONS
AcontributiontothemainideasandmethodsoftheworkwasmadebyS.L.L.(neuraljacobianfield,applications
tosoftrobots)andV.S.(neuraljacobianfield,applicationstosoftrobots).Acontributiontothehardware
setupwasmadebyA.Z.(HSAplatform,Poppyrobotarm,camerasetup),C.L.(pneumatichand,FESTO
system),B.C.(Poppyrobotarm,camerasetup),S.L.L.(AllegroHand,camerasetup),H.M.(pneumatichand).
AcontributiontotheformalizationofmodelingandcontrolchallengesinsoftrobotswasmadebyA.Z.,
C.L.,D.R.AcontributiontoexperimentdesignwasmadebyA.Z.(robustnessstudies,pneumatichand),C.L.
(pneumatichand),S.L.L.(allexperiments),V.S.(allexperiments).Acontributiontotheimplementationand
experimentationoftheNeuralJacobianFieldwasmadebyS.L.L.Acontributiontothedataanalysisand
14Unifying3DRepresentationandControlofDiverseRobotswithaSingleCamera
interpretationwasmadebyS.L.L.(analysis,interpretation),A.Z.(interpretation)andV.S(interpretation)A
contributiontocomputeresources,funding,hardware,andfacilitieswasmadebyV.S.(computehardware,
funding,facilities)andD.R.(robothardware,funding,facilities).S.L.L.,A.Z.,B.C.,C.L.,V.S.draftedthework.
S.L.L.,A.Z.,V.S.substantivelyrevisedthework.Allauthorsapprovedthefinaldraftofthemanuscript.
11 COMPETINGINTERESTS
Theauthorsdeclarenocompetinginterests.
REFERENCES
[1] Guang-ZhongYangetal.2018.Thegrandchallengesofsciencerobotics.Sciencerobotics,3,14,eaar7650.
[2] SangbaeKim,CeciliaLaschi,andBarryTrimmer.2013.Softrobotics:abioinspiredevolutioninrobotics.Trendsinbiotechnology,31,5,
287‚Äì294.
[3] DeepakTrivedi,ChristopherDRahn,WilliamMKier,andIanDWalker.2008.Softrobotics:biologicalinspiration,stateoftheart,
andfutureresearch.Appliedbionicsandbiomechanics,5,3,99‚Äì117.
[4] RolfPfeifer,MaxLungarella,andFumiyaIida.2007.Self-organization,embodiment,andbiologicallyinspiredrobotics.science,318,
5853,1088‚Äì1093.
[5] CarmelMajidi.2019.Soft-matterengineeringforsoftrobotics.AdvancedMaterialsTechnologies,4,2,1800477.
[6] ThomasJKBuchneretal.2023.Vision-controlledjettingforcompositesystemsandrobots.Nature,623,7987,522‚Äì530.
[7] MichaelAndrewMcEvoyandNikolausCorrell.2015.Materialsthatcouplesensing,actuation,computation,andcommunication.
Science,347,6228,1261689.
[8] AlexanderLEvenchik,AlexanderQKane,EunBiOh,andRyanLTruby.2023.Electricallycontrollablematerialsforsoft,bioinspired
machines.AnnualReviewofMaterialsResearch,53,225‚Äì251.
[9] DanielaRusandMichaelTTolley.2015.Design,fabricationandcontrolofsoftrobots.Nature,521,7553,467‚Äì475.
[10] CostanzaArmanini,Fr√©d√©ricBoyer,AnupTeejoMathew,ChristianDuriez,andFedericoRenda.2023.Softrobotsmodeling:a
structuredoverview.IEEETransactionsonRobotics,39,3,1728‚Äì1748.
[11] JueWangandAlexChortos.2022.Controlstrategiesforsoftrobotsystems.AdvancedIntelligentSystems,4,5,2100165.
[12] CosimoDellaSantina,ChristianDuriez,andDanielaRus.2023.Model-basedcontrolofsoftrobots:asurveyofthestateoftheartand
openchallenges.IEEEControlSystemsMagazine,43,3,30‚Äì65.
[13] CeciliaLaschi,BarbaraMazzolai,andMatteoCianchetti.2016.Softrobotics:technologiesandsystemspushingtheboundariesof
robotabilities.Sciencerobotics,1,1,eaah3690.
[14] GillAPrattandMatthewMWilliamson.1995.Serieselasticactuators.InProceedings1995IEEE/RSJinternationalconferenceon
intelligentrobotsandsystems.Humanrobotinteractionandcooperativerobots.Vol.1.IEEE,399‚Äì406.
[15] MatthiasAlthoff,AndreaGiusti,StefanBLiu,andAaronPereira.2019.Effortlesscreationofsaferobotsfrommodulesthrough
self-programmingandself-verification.ScienceRobotics,4,31,eaaw1924.
[16] ShuguangLi,DanielMVogt,DanielaRus,andRobertJWood.2017.Fluid-drivenorigami-inspiredartificialmuscles.Proceedingsofthe
NationalacademyofSciences,114,50,13132‚Äì13137.
[17] KeeneChin,TessHellebrekers,andCarmelMajidi.2020.Machinelearningforsoftroboticsensingandcontrol.AdvancedIntelligent
Systems,2,6,1900171.
[18] DaekyumKimetal.2021.Reviewofmachinelearningmethodsinsoftrobotics.Plosone,16,2,e0246102.
[19] ThomasGeorgeThuruthel,EgidioFalotico,LuciaBeccai,andFumiyaIida.2021.Machinelearningtechniquesforsoftrobots.Frontiers
inRoboticsandAI,8,205.
[20] RyanLTruby,CosimoDellaSantina,andDanielaRus.2020.Distributedproprioceptionof3dconfigurationinsoft,sensorizedrobots
viadeeplearning.IEEERoboticsandAutomationLetters,5,2,3299‚Äì3306.
[21] ThomasGeorgeThuruthel,BenjaminShih,CeciliaLaschi,andMichaelThomasTolley.2019.Softrobotperceptionusingembedded
softsensorsandrecurrentneuralnetworks.ScienceRobotics,4,26,eaav1488.
[22] RomainLagneau,AlexandreKrupa,andMaudMarchal.2020.Activedeformationthroughvisualservoingofsoftobjects.In2020ieee
internationalconferenceonroboticsandautomation(icra).IEEE,8978‚Äì8984.
[23] MichaelCYipandDavidBCamarillo.2014.Model-lessfeedbackcontrolofcontinuummanipulatorsinconstrainedenvironments.
IEEETransactionsonRobotics,30,4,880‚Äì889.
[24] AliAlBeladi,EvanRipperger,SethHutchinson,andGirishKrishnan.2022.Hybrideye-in-hand/eye-to-handimagebasedvisual
servoingforsoftcontinuumarms.IEEERoboticsandAutomationLetters,7,4,11298‚Äì11305.
[25] JamesMBern,YannickSchnider,PolBanzet,NitishKumar,andStelianCoros.2020.Softrobotcontrolwithalearneddifferentiable
model.In20203rdIEEEInternationalConferenceonSoftRobotics(RoboSoft).IEEE,417‚Äì423.
[26] [n.d.]Optitrackmotioncapturesystem.Accessed:2024-06-20.().https://optitrack.com.
15S.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
[27] [n.d.]Vicon:award-winningmotioncapturesystems.Accessed:2024-06-20.().https://www.vicon.com.
[28] [n.d.]Qualisysmotioncapturesystems.Accessed:2024-06-20.().https://www.qualisys.com.
[29] BoyuanChen,RobertKwiatkowski,CarlVondrick,andHodLipson.2022.Fullybodyvisualself-modelingofrobotmorphologies.
ScienceRobotics,7,68,eabn1944.
[30] DylanPLosey,HongJunJeon,MengxiLi,KrishnanSrinivasan,AjayMandlekar,AnimeshGarg,JeannetteBohg,andDorsaSadigh.
2022.Learninglatentactionstocontrolassistiverobots.Autonomousrobots,46,1,115‚Äì147.
[31] XingyiZhou,VladlenKoltun,andPhilippKr√§henb√ºhl.2020.Trackingobjectsaspoints.ProceedingsoftheEuropeanConferenceon
ComputerVision(ECCV).
[32] AyushTewarietal.2022.Advancesinneuralrendering.InComputerGraphicsForum.WileyOnlineLibrary,703‚Äì735.
[33] YihengXieetal.2022.Neuralfieldsinvisualcomputingandbeyond.InComputerGraphicsForum.WileyOnlineLibrary,641‚Äì676.
[34] HannaMatusik,ChaoLiu,andDanielaRus.2023.Directly3dprinted,pneumaticallyactuatedmulti-materialrobotichand.arXiv
preprintarXiv:2310.16280.
[35] JeffreyIanLipton,RobertMacCurdy,ZacharyManchester,LillianChin,DanielCellucci,andDanielaRus.2018.Handednessin
shearingauxeticscreatesrigidandcompliantstructures.Science,360,6389,632‚Äì635.
[36] [n.d.]Allegrohand,wonikrobotics.Accessed:2024-06-17.().http://wonikrobotics.com/robot-hand.
[37] MatthieuLapeyre.2015.Poppy:open-source,3Dprintedandfully-modularroboticplatformforscience,artandeducation.Ph.D.
Dissertation.Universit√©deBordeaux.
[38] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,andRenNg.2020.NeRF:Representing
scenesasneuralradiancefieldsforviewsynthesis.InProceedingsoftheEuropeanConferenceonComputerVision(ECCV),405‚Äì421.
[39] VincentSitzmann,JulienN.P.Martel,AlexanderW.Bergman,DavidB.Lindell,andGordonWetzstein.2020.Implicitneuralrepresen-
tationswithperiodicactivationfunctions.InAdvancesinNeuralInformationProcessingSystems(NeurIPS).
[40] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.2021.pixelNeRF:Neuralradiancefieldsfromoneorfewimages.In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR).
[41] CarlDoersch,YiYang,MelVecerik,DilaraGokay,AnkushGupta,YusufAytar,JoaoCarreira,andAndrewZisserman.2023.Tapir:
trackinganypointwithper-frameinitializationandtemporalrefinement.InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,10061‚Äì10072.
[42] EricaSalvato,GianfrancoFenu,EricMedvet,andFeliceAndreaPellegrino.2021.Characterizationofmodelingerrorsaffecting
performancesofaroboticsdeepreinforcementlearningcontrollerinasim-to-realtransfer.In202144thInternationalConventionon
Information,CommunicationandElectronicTechnology(MIPRO).IEEE,1154‚Äì1159.
[43] MaximeChevalier-Boisvert,GuillaumeAlain,FlorianGolemo,andDerekNowrouzezahrai.2019.Robo-planet:learningtopokeina
day.arXivpreprintarXiv:1911.03594.
[44] FlorianGolemo,AdrienAliTaiga,AaronCourville,andPierre-YvesOudeyer.2018.Sim-to-realtransferwithneural-augmentedrobot
simulation.InConferenceonRobotLearning.PMLR,817‚Äì828.
[45] CarolinaHiguera,SiyuanDong,ByronBoots,andMustafaMukadam.2023.Neuralcontactfields:trackingextrinsiccontactwith
tactilesensing.In2023IEEEInternationalConferenceonRoboticsandAutomation(ICRA).IEEE,12576‚Äì12582.
[46] ShaohongZhong,AlessandroAlbini,OiwiParkerJones,PerlaMaiolino,andIngmarPosner.2023.Touchinganerf:leveragingneural
radiancefieldsfortactilesensorydatageneration.InConferenceonRobotLearning.PMLR,1618‚Äì1628.
[47] YimingDou,FengyuYang,YiLiu,AntonioLoquercio,andAndrewOwens.2024.Tactile-augmentedradiancefields.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,26529‚Äì26539.
[48] EdwinOlson.2011.Apriltag:arobustandflexiblevisualfiducialsystem.In2011IEEEinternationalconferenceonroboticsandautomation.
IEEE,3400‚Äì3407.
[49] ZacharyTeedandJiaDeng.2020.RAFT:Recurrentall-pairsfieldtransformsforopticalflow.InProceedingsoftheEuropeanConference
onComputerVision(ECCV).
[50] AyushTewari,TianweiYin,GeorgeCazenavette,SemonRezchikov,JoshuaBTenenbaum,Fr√©doDurand,WilliamTFreeman,and
VincentSitzmann.2023.Diffusionwithforwardmodels:solvingstochasticinverseproblemswithoutdirectsupervision.Advancesin
NeuralInformationProcessingSystems(NeurIPS).
[51] JoshTobin,RachelFong,AlexRay,JonasSchneider,WojciechZaremba,andPieterAbbeel.2017.Domainrandomizationfortransferring
deepneuralnetworksfromsimulationtotherealworld.In2017IEEE/RSJinternationalconferenceonintelligentrobotsandsystems
(IROS).IEEE,23‚Äì30.
[52] XueBinPeng,MarcinAndrychowicz,WojciechZaremba,andPieterAbbeel.2018.Sim-to-realtransferofroboticcontrolwith
dynamicsrandomization.In2018IEEEinternationalconferenceonroboticsandautomation(ICRA).IEEE,3803‚Äì3810.
[53] PeterRFlorence,LucasManuelli,andRussTedrake.2018.Denseobjectnets:learningdensevisualobjectdescriptorsbyandfor
roboticmanipulation.arXivpreprintarXiv:1806.08756.
[54] AlexanderKirillovetal.2023.Segmentanything.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,4015‚Äì
4026.
16Unifying3DRepresentationandControlofDiverseRobotswithaSingleCamera
[55] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDoll√°r,andCLawrenceZitnick.2014.
Microsoftcoco:commonobjectsincontext.InComputerVision‚ÄìECCV2014:13thEuropeanConference,Zurich,Switzerland,September
6-12,2014,Proceedings,PartV13.Springer,740‚Äì755.
[56] FilipIlievski,AaronDMazzeo,RobertFShepherd,XinChen,andGeorgeMcClellandWhitesides.2011.Softroboticsforchemists.
AngewandteChemieInternationalEditiion.
[57] MikeLambetaetal.2020.Digit:anoveldesignforalow-costcompacthigh-resolutiontactilesensorwithapplicationtoin-hand
manipulation.IEEERoboticsandAutomationLetters,5,3,3838‚Äì3845.
[58] KennethShaw,AnanyeAgarwal,andDeepakPathak.2023.Leaphand:low-cost,efficient,andanthropomorphichandforrobot
learning.arXivpreprintarXiv:2309.06440.
[59] SatoshiFunabashi,TomokiIsobe,ShunOgasa,TetsuyaOgata,AlexanderSchmitz,TitoPradhonoTomo,andShigekiSugano.2020.
Stablein-graspmanipulationwithalow-costrobothandbyusing3-axistactilesensorswithacnn.In2020IEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems(IROS).IEEE,9166‚Äì9173.
[60] HenryZhu,AbhishekGupta,AravindRajeswaran,SergeyLevine,andVikashKumar.2019.Dexterousmanipulationwithdeep
reinforcementlearning:efficient,general,andlow-cost.In2019InternationalConferenceonRoboticsandAutomation(ICRA).IEEE,
3651‚Äì3657.
[61] RyanLTruby,LillianChin,andDanielaRus.2021.Arecipeforelectrically-drivensoftrobotsvia3dprintedhandedshearingauxetics.
IEEERoboticsandAutomationLetters,6,2,795‚Äì802.
[62] AnnanZhang,Tsun-HsuanWang,RyanLTruby,LillianChin,andDanielaRus.2023.Machinelearningbestpracticesforsoftrobot
proprioception.In2023IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS).IEEE,2564‚Äì2571.
[63] RyanLTruby,LillianChin,AnnanZhang,andDanielaRus.2022.Fluidicinnervationsensorizesstructuresfromasinglebuildmaterial.
Scienceadvances,8,31,eabq4385.
[64] AnnanZhang,RyanLTruby,LillianChin,ShuguangLi,andDanielaRus.2022.Vision-basedsensingforelectrically-drivensoft
actuators.IEEERoboticsandAutomationLetters,7,4,11509‚Äì11516.
[65] [n.d.]Poppyproject-ergojr.Accessed:2024-06-17.().https://www.poppy-project.org/en/robots/poppy-ergo-jr/.
[66] [n.d.]Assemblepoppyergojr,documentationofthepoppyplatform.Accessed:2024-06-17.().https://docs.poppy-project.org/en/asse
mbly-guides/ergo-jr/index.html.
17S.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
A EXTENDEDDATA
aData Collection Multi-view Observations
Current observation
Random control inputs
u Next observation
d
n
a
m
m Time t
o
C
Optical flow
bSingle-Image 3D Reconstruction
Supervision using RGB
Jacobian Field 3D Motion Field
and optical flow targets
Volume
Rendering
Neural 3D
Reconstruction
Radiance Field
Fig.5. Overviewofdatasetcollection,training,andinferenceprocesses.a,Ourdatacollectionprocesssamples
randomcontrolcommandstobeexecutedontherobot.Usingasetupof12RGB-Dcameras,werecordmulti-viewcaptures
beforeeachcommandisexecuted,andaftereachcommandhassettledtothesteadystate.b,ourmethodfirstconducts
neural3DreconstructionthattakesasingleRGBimageobservationasinputandoutputstheJacobianfieldandRadiance
field.Givenarobotcommand,wecomputethe3DmotionfieldusingtheJacobianfield.Ourframeworkcanbetrained
withfullself-supervisionbyrenderingthemotionfieldintoopticalflowimagesandtheradiancefieldintoRGB-Dimages.
a b Mask ratio: 0.1 Mask ratio: 0.3
100
80
60
Mask ratio: 0.5 Mask ratio: 0.7
40
20
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Mask Ratio
Fig.6. Sensitivityanalysisofdepthprediction.a,Weevaluatethedepthreconstructionqualityunderout-of-distribution
scenarios.Givenacollectionoftestingimages,weperturbtheseimageswithblackpatchessampledatrandomlocations
thatachieveincreasinglylargermaskratios.Weplotthemeandeptherroroverthemaskratioandvisualizethestandard
deviationastheerrorbar.b,Overlayingblackpatchesonanexampleinputimagewithincreasinglylargermaskratios.
18
)mm(
rorrE
htpeDUnifying3DRepresentationandControlofDiverseRobotswithaSingleCamera
Desired Motion
a b
pneuamtic
pick+place
c d
e f
Fig.7. Additionalevaluationonvisuomotorcontrol.(a,b),Ourframeworkcontrolsa3D-printedpneumatichandto
grasponanobjectandcompleteapick-and-placetask.(c,d),OurapproachcontrolstheAllegrohandtocloseandopen
eachfingerfully.(e,f),Weintentionallyperturbthescenestocreateocclusionandbackgroundchanges.Wefindthat
NeuralJacobianFieldisrobustagainstout-of-distributionscenariosandsuccessfullycontrolsthehandtoachievedetailed
motions.
a.AllegroHand b.HSAPlatform c.PoppyArm d.PneumaticHand
Angleerror Locationerror Markererrorinmillimeter Markererror Pressureerror
indegree inmillimeter w/oweight w/weight rotational translational inmillimeter inmillibar
Mean 2.568 3.667 5.354 7.784 7.911 3.372 5.339 5.025
Std 1.595 3.297 2.343 0.481 0.398 0.616 1.735 2.161
Table1. Quantitativeevaluationonvisuomotorcontrol.a-d,Wemeasurethedistancebetweenthefinalstateachieved
byourmethodandthegroundtruthdesiredstate.a,Jointerrorsindegreeanglemeasurementandfinger-tiperrorsin
positionalmeasurement.b,Motioncapturemarkerlocationerrorbetweenthefinalstateachievedbyourmethodandthe
groundtruthrecordedinthereferencetrajectory.c,Similartob.,wereporterrorscomputedbasedonthelocationsofthe
motioncapturemarkers.d,Pressureerrorsmeasuredinmillibarfromthe15-channelproportionalvalveterminal.
AllegroHand HandedShearingAuxetics PoppyToyRobotArm PneumaticHand
Deptherror(mm) Flowerror(pix.) Deptherror(mm) FlowError(pix.) Deptherror(mm) Flowerror(pix.) Deptherror(mm) Flowerror(pix.)
Mean 5.033 1.305 1.109 3.597 1.206 6.797 6.519 1.150
Std 3.080 1.504 0.576 2.779 0.619 7.287 2.028 1.520
Table2. Quantitativeevaluationon3Dreconstruction.Usingtestingimagesfromallcameraviewpoints,wemeasure
depthpredictionerrorsinmillimetersandopticalflowpredictionerrorsona640√ó480imagegrid.Thedeptherrorsare
computedastheL2distancebetweenthegroundtruthdepthvaluemeasuredbytheIntelRealsenseD415RGB-Dcameras
andthepredicteddepthvalueaveragedoverallpixellocationsintheimage.TheopticalflowerrorsarecomputedastheL2
distancebetweenthegroundtruthmeasuredbythepointtracker[41]andthepredictionaveragedoverallpixellocations
intheimage.
19S.L.Li,A.Zhang,B.Chen,H.Matusik,C.Liu,D.Rus,V.Sitzmann
RGB Input Depth Pred. Depth Pred. Jacobian Pred. Jacobian Pred.
(Input View) (Novel View) (Input View) (Novel View)
a
b
c
Fig.8. Qualitativeresultsonrobustnessagainstout-of-distributionscenarios.a,Weperturbthevisualscenesby
placingadozenobjectsaroundthepneumatichandmountedontherobotarm.Consequently,theinputobservationis
highlyout-of-distributionfromthetrainingdata.WevisualizethepredictionsofdepthandJacobianatboththeinput
andnovelviewpoints.Wefindthatourmethodretainshigh-qualitypredictions.(b,c),Weplacedadozenobjectsaround
thepneumatichandtocreateocclusion.Forpresentationclarity,weoverlaytheJacobianpredictiononRGBimagesto
highlighttheshapesofthehandwithmasking.
Part Quantity UnitCost Total
ROBOTISDyanmixelXL-320servos 6 $26.90 $161.40
PixlPoppyErgoJrservocontrolboard 1 $32.47 $32.47
ROBOTISRS-10rivets 1 $6.60 $6.60
ROBOTISrivettool 1 $1.10 $1.10
RaspberiPi3ModelB+board 1 $48.99 $48.99
Gigastone8GBmicroSDcard 1 $3.30 $3.30
PwrON7.5V2AACDCpowersupply 1 $9.99 $9.99
Vabogu-CAT8ethernetcable 1 $3.99 $3.99
BambuLabPLAfilamentfor3Dprintedparts 108g $20perkg $2.16
Screws,nuts,standoffs(estimate) $2.00
GrandTotal $272.00
Table3. BillofmaterialsforPoppyErgoJrarm.
20