Map It Anywhere (MIA): Empowering Bird’s Eye View
Mapping using Large-scale Public Data
mapitanywhere.github.io
CherieHo1∗ JiayeZou1∗ OmarAlama1∗ SaiMitheranJagadeshKumar1
BenjaminChiang1 TaneeshGupta1 ChenWang2 NikhilKeetha1
KatiaSycara1 SebastianScherer1
1CarnegieMellonUniversity 2UniversityatBuffalo
Noisy World-Scale FPVs Towards Anywhere Map Prediction
Quality Semantic
FPVs BEVs
Mapper TIIM SkyEye
MIA
Data
Engine
Raw World-Scale Map
Figure 1: OurMapItAnywhere(MIA)dataengineempowersgeneralizableBird’sEyeView(BEV)map
predictionfromFirst-PersonView(FPV)images. Left: MIAenablesseamlessautomaticcurationofquality
FPV&semanticBEVmapdatafromcrowd-sourcedplatforms,Mapillary&OpenStreetMap.Right:Bothas
atoolfortraining&benchmarking,MIAenablesresearchtowardsanywheremapprediction.Asimplemodel
(Mapper)trainedondatafromMIAbettergeneralizesonbothheld-outcities(MIA-OOD)&existingbenchmarks,
whilestate-of-the-artbaselinestrainedonconventionalautonomousvehicledatasetsstruggle.
Abstract
Top-downBird’sEyeView(BEV)mapsareapopularrepresentationforground
robotnavigationduetotheirrichnessandflexibilityfordownstreamtasks. While
recentmethodshaveshownpromiseforpredictingBEVmapsfromFirst-Person
View(FPV)images,theirgeneralizabilityislimitedtosmallregionscapturedby
currentautonomousvehicle-baseddatasets. Inthiscontext,weshowthatamore
scalableapproachtowardsgeneralizablemappredictioncanbeenabledbyusing
two large-scale crowd-sourced mapping platforms, Mapillary for FPV images
and OpenStreetMap for BEV semantic maps. We introduce Map It Anywhere
(MIA),adataenginethatenablesseamlesscurationandmodelingoflabeledmap
prediction data from existing open-source map platforms. Using our MIA data
engine, we display the ease of automatically collecting a dataset of 1.2 million
pairsofFPVimages&BEVmapsencompassingdiversegeographies,landscapes,
environmental factors, camera models & capture scenarios. We further train
a simple camera model-agnostic model on this data for BEV map prediction.
Extensive evaluations using established benchmarks and our dataset show that
thedatacuratedbyMIAenableseffectivepretrainingforgeneralizableBEVmap
prediction,withzero-shotperformancefarexceedingbaselinestrainedonexisting
datasetsby35%. Ouranalysishighlightsthepromiseofusinglarge-scalepublic
mapsfordeveloping&testinggeneralizableBEVperception,pavingthewayfor
morerobustautonomousnavigation.
∗Equalcontribution.
Preprint.
4202
luJ
11
]VC.sc[
1v62780.7042:viXra1 Introduction
Bird’sEyeView(BEV)mapsareanimportantperceptualrepresentationforgroundrobots. Across
variousapplicationssuchasautonomousdriving[3,13,19,29],offroadnavigation[1,17,24,33],
andlocalization[17,30],priorworkhasdemonstratedthesimplicityofreasoningthroughtheBEV
perspective,makingitalsoattractiveformanydownstreamtaskssuchaspathplanning[14,26]. To
enablewidedeploymentofBEVmapsasaperceptualrepresentation,thereisarequirementfora
generalmappredictionbuildingblockthatperformsrobustlyacrossdifferentdomainsandsupports
effectiveadaptationtospecifictasks/environments.
DespitetremendousadvancementsinpredictingBEVmapsfromFirstPersonView(FPV)images[13,
23, 29], we find that achieving good out-of-the-box predictions across diverse scenarios remains
challenging. Thisshortcomingmainlystemsfromthecurrenttraining&testingparadigmonlimited-
scaledatasetscollectedusingautonomousvehicle(AV)platforms[4,5,20,32,36]. Whilethese
benchmarkshavemassivelypropelledthefield,theyareprincipallylimitedincapturinglarge-scale
diversityduetothetimeandcostassociatedwithmanuallabeling,thelimiteddeploymentrange,and
finally,theuseofspecificsensorconfigurationsoncurrentAVstacks.
Webelievethatacomplementarytraining&testingparadigmisnecessarytoassessthegeneralizability
&robustnessofBEVmapping,pavingthewayforanywheredeployment. Hence,startingfromfirst
principles,weformulatethekeyrequirementsforgeneralizableBEVmappingas: (a)beingableto
providetop-downinformationofkeynavigationclasses,(b)abilitytobeusedbydifferentagentsand
acrossdifferentoperatingregimes,forexample,sidewalkpredictionismorecriticalforautonomous
wheelchairs,(c)performreasonablyout-of-the-boxinunseenlocationssupportingquickadaptation,
and(d)easilyadaptabletodifferenthardwareconfigurationssuchascameramodels.
Inthiscontext,weexplorethequestionof“Howcanonecollectadatasettoempowergeneralizable
BEVmapping?” Specifically,tosupportresearchongeneralizableBEVmapping,suchadataset
needsto(a)containdiversegeographies,terraintypes,timeofday,andseasons,(b)capturescenarios
beyondon-the-roaddriving,(c)supportvariouscameramodels,and(d)consistsofwell-distributed
classesandlabelsforsupportingnavigation.
Toconstructsuchadataset,ourkeyinsightistoleveragetwodisjoint,crowd-sourced,andworld-scale
publicmappingplatforms: MapillaryforFirst-PersonView(FPV)imagesandOpenStreetMapfor
Bird’sEyeView(BEV)semanticmaps. Bothopen-sourceplatformsprovidethetoolsnecessaryto
associatecrowd-sourcedFPVimageswithsemanticrastermapsusedforeverydayhumannavigation.
WeintroduceMIA,adataenginewhichtapsintothepotentialofthesemappingplatformstoenable
seamlesscurationandmodelingoflabeleddataforgeneralizableBEVmapprediction. Specifically,
ourdataengineenablesanevergrowingBEVdatasetandbenchmark,whichexhibitsworld-scale
diversityandsupportsresearchonbothuniversal&environment-specificdeployment.
Inthispaper,toshowcasethepotentialofourdataengine,wemakethefollowingkeycontributions:
1. Weopen-sourceourMIAdataengineforsupportingautomationcurationofpairedworld-scale
FPV&BEVdata,whichcanbereadilyusedforBEVsemanticmappredictionresearch.
2. UsingourMIAdataengine,wereleaseadatasetcontaining1.2millionhighqualityFPVimage
andBEVmappairscovering470km2, therebyfacilitatingfuturemappredictionresearchon
generalizabilityandrobustness.
3. Weshowthattrainingasimplecameraintrinsics-agnosticmodelwithourreleaseddatasetsresults
insuperiorzero-shotperformanceoverexistingstate-of-the-artbaselinesonkeystaticclasses,
suchasroadsandsidewalks.
4. Throughanalysisofcurrentperformanceinurbanandruraldomainsofourbenchmark,weshow
thatsignificantresearchremainstoenablegeneralizableBEVmapprediction.
Overall,MIAestablishesadiverseevergrowingdataset&benchmarkformappredictionresearchand
showcaseshowcommoditypublicmapscanempowergeneralizableBEVperceptiontasks(Fig.1).
2 RelatedWork
Bird’sEyeViewMapPrediction: BEVmappredictioninvolvespredictingtop-downsemantic
mapsfromvarioussensorymodalitiestofacilitatedownstreamrobotictasks. Someworksrelysolely
2Table1: StatisticsshowcasingthebroaderscaleofMIAincomparisontopriorBEVDatasets.
Taxonomy: U:Urban, S:Suburban, R:Rural, O:Offroad, BN:Boston, SP:Singapore. "-": Attributesare
notavailable."*":MGLisaBEVlocalizationdatasetanddoesnotprovidesemanticBEVmapssuitablefor
mapprediction."#BEVAnnotatedFrames":ReadilyavailableBEVdata."AutomaticCuration":Nohuman
interventioninthecollectionandannotationofthedataset.
km2 #BEVAnnotated #Camera Domaintypes CapturePlatform Automatic
Dataset Locations
covered Frames Models Curation
U S R O Car Bik Ped
Argoverse[5] 2inUS 1.6[32] 22K 2 ✓ X X X ✓ X X X
Argoverse2[36] 6inUS - ∼108K[38] 2 ✓ X X X ✓ X X X
KITTI-360-BEV[13] 1inDE 5.3 83K 2 X ✓ ✓ X ✓ X X X
NuScenes[4] BN,SP 5.6 40K 2 ✓ ✓ X X ✓ X X X
Waymo[32] 3inUS 76[32] 230K 2 ✓ ✓ X X ✓ X X X
MGL*[30] 12inUS/EU - 760K 4 ✓ ✓ ✓ X ✓ X X X
MIA(Ours) 6inUS 470 1.2M 17 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
onLiDAR[18],othersonmulti-viewcameras[27],andsomeonboth[21,41]. Theseapproaches
relyonmodalitiesthatareexpensiveanddifficulttocalibrate. Recently,agrowingnumberofworks
usemonocularcameras[12,13,22,23,28,29],astheyareattractivefortheireaseofdeployment,
reducedcost,andhigherscalability. However,alltheseworksstillrelyoncurrentautonomousdriving
datasetsforlabels,whichlimitsthescalabilityofdatacollection. Toaddressthislimitation,SkyEye
[13]usesmoreavailablefront-viewsemanticstobuildmappredictorswithoutexplicitBEVmaps.
However,thismethodreliesongroundtruthFPVsemanticmasks,whicharecostlytoannotateand
scale. Incontrast,MIAleveragestworeadilyavailableworld-scaledatabasestoprovidediverseand
accuratesupervision,avoidingthehighcostofequipmentandmanuallabor.
AnotherlineofrelatedworkisthetaskofmatchingFPVimageswithBEVmapsthatcanbesatellite-
based[15,40],planimetric[30,37],ormulti-modal[31]. Theyoftenemploytechniquestopredicta
BEVfeaturemapgivenanFPVimage. Whileusefulforretrievalorlocalization,thesefeaturemaps
cannotbenefitdownstreamtaskswithoutcomplexlearneddecoders,unlikepredictedsemanticBEV
maps,whichdownstreamalgorithmslikepathplanningcanreadilyconsume.
DatasetsforBEVMapPrediction: ExistingBEVmappredictiondatasetsareoftenderivedfrom
multi-modalautonomousdrivingdatasets[4,5,12,13,32,36]thattargetvarioustasks,including
BEVprediction. ThesepioneeringworksdependonmanuallycollecteddatafromcostlyLiDARs,
hencerequiringcarefulcalibrationwithcamerasetupstoensureaccuratecorrespondencebetween
FPV&BEVdata. TheBEVisgeneratedbyaccumulatingsemanticallylabeledLiDARpointclouds
&thensplattingthemtoBEV,allowingthemtocapturedynamicandstaticclasses. However,these
approaches are principally limited in both scale and diversity due to their high cost, hindering
modelgeneralizability. Incontrast,MIA usesdataavailableoncrowd-sourcedplatformsandcanthus
obtainFPV-BEVpairsglobally,achievingbroaderdiversity&scaleasshowninTable1.
Crowd-sourcedDatasetsforLearningGeometricTasks: Crowd-sourcedplatformsenableopen-
sourcecontributorstouploaddiversein-the-wilddata,significantlyempoweringgeneralizabilityin
geometriclearningtasks. OnesuchnotableplatformisMapillary[7], whichhostsover2billion
(and growing) crowd-sourced street-level images from various locations worldwide, captured by
different cameras across all seasons and times. Mapillary has been notably used for tasks such
as depth estimation [2], lifelong place recognition [35], and visual localization [16, 30]. Most
related to our work, OrienterNet [30] addresses visual localization within a large top-down map,
curatingalarge-scalelocalizationdatasetprovidedbycrowd-sourcedplatforms,Mapillary[7]for
FPVimagesandOpenStreetMap[8](OSM)forlargeBEVmaps. However,theOrienterNetpipeline
forrasterizingmapsisnotsuitableforBEVpredictionasitrenderslargemaps,mimicsOSMstyle,
andincludesgraphicalelements/labelsirrelevanttothemappredictiontask. MIAfurtherrefinesthe
OrienterNetpipelinebyenablingautomaticcurationandcollectionofsemanticmaps, alignment
of BEV renders with satellite images, and inference of missing OSM sidewalk geometries, thus
providingrichsemanticmapsthatarereadyforBEVmapprediction.
3 MIADataEngine
To construct a dataset for generalizable BEV mapping, we develop a scalable data engine that
generateshigh-quality,diverseFPV-BEVpairswithrichsemanticlabels. Thisprocess,summarized
inFig.2anddetailedbelow,followsthecriteriadiscussedinSection1.
3Figure2: OverviewofhowtheMIAdataengineenablesautomaticcurationofFPV&BEVdata.
Givennamesofcitiesasinputfromtheleft,thetoprowshowsFPVprocessing,whilethebottomrow
depictsBEVprocessing. Bothpipelinesconvergeontheright,producingFPV,BEV,andposetuples.
3.1 FirstPersonView(FPV)Retrieval
Mapillary: ForFPVretrieval,weleverageMapillary[7],amassivepublicdatabase,licensedunder
CCBY-SA,withover2billioncrowdsourcedimages. Theimagesspanvariousweatherandlighting
conditionscollectedusingdiversecameramodelsandfocallengths. Furthermore,imagesaretaken
bypedestrians,vehicles,bicyclists,etc. Thisdiversityenablesthecollectionofmoredynamicand
difficultscenarioscriticalforanywheremapprediction. However,thismassivepoolofdataisnot
readilyamenabletodeeplearningasitcontainsmanynoisy,distorted,andincorrectlyregistered
instances. Thus far, few works, such as [30], have leveraged such data providing an impressive
retrieval and undistortion pipeline. However, the work relied on careful and manual curation of
limited camera models and scenarios. Such approaches are not scalable and cannot leverage the
powerfulquantityanddiversityofMapillary. Hence, wefurtherrefinetheOrienterNet[30]data
curationframeworkanddevelopafullyautomatedcurationpipelinethatcanharnessthefullpotential
oftheextensiveMapillarydatabase. Wedescribethepipelineinthefollowingsections.
FPVPipeline: AsdemonstratedinFig.2,theFPVpipelinestartsbymanuallyinputtingalistof
locations of interest, which can be as simple as inputting the name of the location or as specific
asspecifyingtheGPSbounds. Thepipelinethenconvertstheseboundstoalistofzoom-14tiles
andusesthepubliclyexposedMapillaryAPIstoqueryforanyimageinstancewithinthesetiles.
Theretrievedinstancesarethengeospatiallyfilteredtoensuretheyliewithintheexactboundaries
of interest. Given the retrieved image IDs, we use another Mapillary endpoint to retrieve image
metadata,whichincludescoordinatesrectifiedthroughstructurefrommotion,camerainformation,
poses,andtimestamps,amongstotherdetailsthatweuseforfilteringinthesubsequentstage.
WedevelopthefilteringpipelinebyobservinghundredsofFPV&BEVpairsandidentifyingthe
correlationsbetweengood-qualityFPVsandtheircorrespondingmetadata. Thecriteriaweused
includedarecencyfilter,acameramodelfilterspanning19cameramodelswithgoodRGBquality,a
location/anglediscrepancyfilterthatcomputesthedifferencebetweenStructurefromMotion(SfM)
computedandrecordedposesasaproxyformeasuringthequalityofthegeo-registration, anda
cameratypefilterthatonlyincludesperspectiveandfisheye. Topromotespatialdiversityoversheer
quantity,wefilteroutimageswithina4-meterradiusofanotherimagefromthesamesequence. After
filtering,weretrievetheRGBimagesfromMapillaryandfeedthemthroughanundistortionpipeline
adaptedfrom[30].Theundistortioniscriticalforfisheyeimagestoensuretheirpixel-alignedfeatures
canbecorrectlyliftedintoBEVspace. Usingthispipeline,wecanretrievehigh-qualityimagesfrom
anywhereintheworld,tappingintothepoweroftheMapillaryplatform.
3.2 BirdsEyeView(BEV)Retrieval
OpenStreetMap(OSM):ForBEVretrieval,weleverageOSM[8],aglobalcrowd-sourcedmapping
platformopen-sourcedunderOpenDataCommonsOpenDatabaseLicense(ODbL).OSMprovides
4richvectorizedannotationsforstreets,sidewalks,buildings,etc. However,OSMdatacannotbeeasily
usedformappredictionas(a)OSMoftendoesnotencodecriticalsidewalkgeometry,(b)structured
formatslikeOSMmapsprovedifficulttotrainon,(c)off-the-shelfrenderingpipelinestargethuman
consumptionoftenencodinginformationirrelevantforBEVprediction(suchastextuallabels)and
donotcareaboutpixelaligningmapswithsatelliteimagery,therebyencodinginaccurateroadwidths
inmanyinstances. Recognizingtheneedtocreateourownrasterizationpipeline,webuildontop
oftheMIT-licensedMapMachineproject[34]andstudyhundredsofsatellite/mappairstoachieve
rasterizationthatismorepixel-alignedwithsatelliteimagery. Wefurthercarefullymapthehundreds
ofelementsinOSMtoahandfulofinformativedominantsemanticlabels.
FPV Satellite Default Ours FPV Satellite Default Ours
Road Parking Sidewalk Crossing Building Terrain Robot
Figure3: ComparisonofdefaultMapMachine-stylerenderingwiththeMIA-style. Thefigure
showsourrenderingremovesirrelevantinformation,clusterskeysemanticcategories,alignsbetter
withsatelliteandisabletoprovidemoreaccuratesidewalkgeometrycorrectly. Satelliteimageryis
notpartoftheMIAdataengineandwasobtainedfrom[11]onlyfortuningmaprendering.
BEVGenerationPipeline: BEVretrievalstartsafterthefilteringstageinFPVretrievalasillustrated
inFig.2. GivencoordinatesintheWorldGeodeticSystem(WGS-84)frameforeachimage,we
projecteachpointontoaCartesianUTMcoordinateframe,estimatedseparatelyforeachcity/location.
Next,wecalculateanego-centricboundingboxofsize(α+β+δ)2ataresolutionofρmetersper
pixel. Here,αrepresentstherequestedimagedimension,β =α− α isthepaddingaddedto
cos(π)
4
accommodaterotationswithoutintroducingemptyspace,andδisthepaddingaddedtoavoidmissing
anyOSMelementsthatmaynotfallwithintheoriginalbox. ToadheretotheOSMAPI,weproject
boxesbacktoWGS-84coordinatesbeforeretrievingOSMdataforeveryimage. Wethenutilizeour
versionofMapMachine(enhancedtoinfermissingsidewalksfromOSM)withacarefullytuned,
satellite-alignedmapstyletorenderthedataintoSVGformat. Next,werotatetherenderedimageso
thattherobotislooking‘up’intheBEVimageplane,aligningitwiththeforwarddirectionofthe
FPVplane. Finally,werasterizetheSVGintoasemanticmaskcontainingsixstaticclasses(Road,
Parking,Sidewalk,Crossing,Building&Terrain),asshowninFig.3,toproducethefinalBEV.
4 EmpoweringMapPredictionwiththeMIA DataEngine
4.1 SamplingtheMIADataset
We show the utility of the MIA data engine by sampling six different urban-centered locations,
extendingtothesuburbs. Weselectedhighlypopulatedcities-NewYork,Chicago,Houston,andLos
Angeles-tocollectchallengingscenarioswithdiverseanddensetraffic. Additionally,weincluded
PittsburghandSanFranciscofortheiruniquetopologies. ForBEVretrieval,wesetα=224,δ =50,
andρ=0.5,resultinginwhatwebelieveisthelargestpublicBEVpredictiondataset,comprising
approximately1.2millionFPV-BEVpairs,asshowninTable1. Toillustratethediversityofthedata,
weadaptthecoveragemetricproposedby[32]inwhicheachimageinstancecoversaradiusof150
metersarounditspose. Foroursampleddataset,wecalculatethecoverageataradiusof112meters
consistentwithourchosenαvalue. AsshowninTable1,oursampleddatasetcovers 470km2,far
surpassingallexistingBEVpredictiondatasetsby6×. Thishighlightstheimmensepotentialofour
scalableMIAdataenginetoproducelargequantitiesofannotatedFPV-BEVpairscoveringextensive
geographiesandwithvaryingcameramodelsandfocallengths,ashighlightedinFig.4.
Tofurtherbenchmarkthegeneralizationcapabilityofmappredictionmodelsinmoreextremesettings,
wefurthersampleasmall(∼ 1.1K)rural/remotedataset, whichwedenoteasMIA-Rural. This
5Time of Day Vehicle Domain
Season / Weather Pedestrian Domain
Figure4: SamplesfromtheMIAdataset: Highlightingdiversityintimeofday,seasons,weather
andcapturescenariosfromvehicles&pedestrians.
datasethasadistributionverydifferentfromtheurban-centeredsamples. Weselectedlocationswith
distinctvisualappearances,namelyWillow(Alaska),Ely(Nevada),andOwenSprings(Australia).
Topushtheboundariesofgeneralizationtesting,wedisabledthecameramodelfilterforthistestset,
therebyincorporatingavarietyofchallengingcameramodelsintothisextremedataset.
4.2 Mapper: Trainingacameraintrinsics-agnosticbaselinemodel
Wetrainamodel,Mapper,withMIAtovalidatetheneedforsuchalarge-scaleanddiversedataset
bytestingitsgeneralizationcapability. LeveragingthediversityoftheMapillarydatasetrequiresa
modelarchitecturecapableofhandlingvariousimagecharacteristics,suchasfocallengthsandimage
size. Additionally,followingOrienterNet[30],itisreasonabletoassumethattherobotorphonehas
orientationinformation(IMU),andweaimtoincorporatethisinformationintoourmappredictions.
Our goal is to learn a model that takes in a monocular image I to predict a gravity-aligned BEV
semanticmapY. Formally,givenanimageI ∈ R3×H×W,itsintrinsicmatrixC ∈ R3×3,andits
extrinsicmatrixE∈R3×4,weseektoproduceamulti-labelbinarysemanticmapingravity-aligned
frame Y ∈ RX×Z×K where K is the number of semantic classes. To achieve this, we build on
OrienterNet[30],designedfortop-downmaplocalization,asthefront-endarchitecture. Thischoice
accommodatesdifferentcameracharacteristicsandposeinformationfromIMUs,therebyleveraging
theorientationdatafromOSMandMapillary. WeaddadecoderheadontheBEVfeatures(obtained
post2D-to-3DliftingofFPVfeatures)topredictasemanticmap,therebymaintainingsimplicityin
themodelarchitecture. Furthermore,toimprovegeneralizationcapability,wereplacetheResNet
encoderwiththeDINOv2encoder[25]. Fortraining,weresizeandpadimagestoa512x512square,
applyingweightedDiceandBinaryCrossEntropyLosstotheBEVpixelswithintheimagefrustum.
We use DINOv2 ViT-B/14 [25] with registers [9] as the image encoder. We further augment the
datasetwithbrightness,contrast,andcolorjittering. Wetrainwithabatchsizeof128for15epochs,
which takes approximately 4 hours using 4 NVIDIA-H100 GPUs. The supplementary material
providesmoredetailsonthemodelandtraining,alongwithafigureofthemodelpipeline.
5 ExperimentalSetup
Toevaluategeneralizability,wetestourMappermodelandbaselinesonmultipledatasets,including
ourdiverseMIAdatasetandconventionalmappredictiondatasets.
ConventionalDatasets: WeevaluateourmodelonBEVmapsegmentationbenchmarks: NuScenes
[4]andKITTI360-BEV[13],todemonstratethegeneralizabilityofamodeltrainedonMIAwhen
appliedtoestablisheddatasets. WefollowtheBEVgenerationprocedurein[28]forNuScenes. Both
datasetsarecollectedfromon-roadvehiclesandpresentchallengessuchasocclusions, different
timesofday,andvaryingweatherconditions. Weadheretotheconventional(geographicallynon-
overlapping)datasetsplits: Roddicketal.’s[28]splitforNuScenesandGosalaetal.’ssplit[13]
forKITTI360-BEV.Sincewefocusonstaticclassmapprediction,weexcludedynamicelements
fromthedatasetlabelsbeforetraining. ForNuScenes,weuseonlythestaticmaplayers,andfor
KITTI360-BEV,weremovedynamicobjectlabels.Intheexperiments,weusethefront-facingcamera
6datatoevaluatemonocularcameraBEVmapprediction. Classmappingsbetweenthedatasetsare
providedinTable5oftheappendix.
MIADataset: WeutilizetheMIAdatasettoassessperformanceindiverseurbanandruralsettingsnot
capturedbypreviousdatasets,includingheld-outenvironments. Specifically,wesplitourdatasetinto
twosettings: MIA-ID(NewYork,LosAngeles,SanFrancisco,Chicago)representsin-distribution
urbanareas,MIA-OOD(Houston,Pittsburgh)teststhegeneralizationabilityinheld-outurbansettings.
Furthermore,weuseMIA-Ruraltoprovideamorechallengingout-of-distributionevaluation. For
eachlocation, wegeneratean80%train/10%validation/10%testsplit, ensuringthesplitsare
geographicallydisparateasillustratedinFig.8oftheappendix.
Metrics: Weadheretostandardconventions[13,29]tocalculatetheIntersection-over-Union(IoU)
score using binarized predictions with a threshold of 0.5. The IoU score is computed over the
observable area as defined by the visibility mask, based on LiDAR observations or the visibility
frustum. SincethereisnoLiDARsensingintheMIAdataset,wegenerateaheuristic-basedvisibility
maskbyraycastingfromtherobot’sposition,ending4pixelsintoabuilding. Tobeconsistentwith
SkyEyeevaluations[13],inKITTI360-BEV,wealsoincludeoccludedareaswithinimagefrustumfor
IoUcalculation. Foralldatasets,weperformedcomparisonsovera50mx50mareawitharesolution
of0.5m/pixel. Consistentwithpriorwork[13,29],wereportthemacro-meanIoUoverallclasses. In
addition,forafaircomparisonacrossdifferentmethodsanddatasets,wereportthemacro-meanIoU
overthetwoclassescommoninalldatasets,road&sidewalk.
Baselines: Wecompareourresultswithpreviouslypublishedmethodsthatfocusonthemonocular
single-camerasetting, specificallyTranslatingImagesintoMaps(TIIM)[29], whichwastrained
andtestedonNuScenes[4],andSkyEye,whichwastestedonKITTI360-BEV[13]. Whilenewer
methodshavebeenproposed[6,39],TIIMisthemostrecentmethodwithavailablecode,toour
knowledge. Wefollowthetrainingprotocolsdescribedintherespectivepapersandcode,withslight
modificationstotrainforstaticclasses.Forevaluation,imagesareprocessedtomeettherequirements
ofeachmethod. Totestthebaselinemodelsondatasetstheywerenottrainedon,wefollowGosala
etal. [13]andresizetheimagetomatchthefocallengthofthemodel’strainingdataset. Moredetails
onbaselineimplementationareprovidedinsupplementarymaterialandappendix.
6 Results&Discussion
We firstly evaluate Mapper zero-shot against the baselines. Next, we test MIA’s effectiveness for
pre-trainingbyfinetuningMapperonlimiteddatafromanexistingdataset. Finally,westresstestall
modelsinextremeout-of-distributionscenariostohighlightfutureopportunitiesenabledbyMIA.
6.1 MIAcangomore"anywhere"out-of-the-box
Table 2 demonstrates the generalizability of Mapper, trained with the MIA-ID dataset, over both
zero-shot&fully-supervisedbaselines,inparticularontheNuScenesandMIA-OODenvironments.
Fig.5visuallycomparesthemodelpredictions,whereMapperprovidesmorerealisticpredictions
acrossthedatasetscomparedtothezero-shotbaseline,whichoftenfailsduetothedistributionchange
causedbyunseenlocation,differentcameramodels,orsevereweatherconditions. Whencomparing
average IoU of Road and Sidewalk, Mapper achieves superior zero-shot result in NuScenes val
[4]andMIA-OOD,withimprovementsof33%and144%,respectively. Notably,inNuScenesand
MIA-OOD,Mapperperformscomparablytofullysupervisedmethods(trainedwithin-domaindata)
inroadandsidewalkclasses. WhileMapperperformsconsistentlywithTIIM(trainedonNuScenes)
whentestedonKITTI360-BEV[13],Mapperprovidesmorerealisticpredictionsoverall,especially
in non-road regions where TIIM tends to overpredict roads. However, due to the limitations of
KITTI360-BEV,wheremuchofthemapisunlabeledandIoUisonlymeasuredinlabeledregions,as
illustratedinFig.6,itischallengingtoperformeffectivebenchmarkingandcomparisons.
6.2 Pre-trainingonMIAenableseffectivefine-tuningwithlimiteddata
We also test if the MIA dataset can provide effective pretraining for new map prediction tasks.
Specifically,wefinetuneMapperwith10%and1%ofNuScenesdata. Forcomparison,weusethe
samedatasplittotrainTIIM[29]. Tofine-tuneMapper,wemapthenewtrainingdatasetclassesto
MIAclassesasdescribedinTable5ofappendix. Detailsontraining,fine-tuninganddatasplitsare
7Figure5: Mapperconsistentlyprovidesmoreprecise&realisticzero-shotpredictionsacrossall
thedatasets. Notably,Mapper,empoweredbyMIAdata,canproducezero-shotpredictionswhich
arecomparabletothefully-supervisedbaselineswhichhavebeentrainedonin-domaindata.
Figure6: LackofcompletelabelsinKITTI360-BEV[20]dilutesqualityofbenchmarking. For
example,whileMapperpredictssidewalkandroadreasonablyinthisframe,thelackofsidewalk
labelsinthegroundtruthresultsinamisrepresentativeIoU.Meanwhile,TIIM’s[29]roadIoUis
artificiallyhigher,despitetheincorrectroadpredictionontheleft.
availableintheappendixandsupplementarymaterial. Table2dshowsthat Mappercanbeeffectively
fine-tunedonspecificenvironmentswithlimitednewmappredictiondata. Notably,theexperiment
suggeststhatMIAprovideseffectivepretrainingformapprediction,asfine-tuningthepre-trained
MapperyieldsimprovedresultscomparedtotrainingTIIMsolelyonthedatasubset.
6.3 MIAprovideschallengingsettingsforfutureworkonanywheremapprediction
Totestmodelgeneralizability,wefurthercurateMIA-Rural,whichisfarfromthetrainingdistri-
bution. Fig.7showsanexamplepredictionsfromhighwayimageswhereallmodels,includingour
proposedMapper,failtogeneralize. Quantitatively,ontheentiretyofMIA-Rural,theaverageIoU
betweenroadandsidewalks(Avg. R,S)is21.04forMapper,20.55forTIIM[29]and18.62for
SkyEye[13]. Thisfurtherillustratestheresearchneedforananywheremappredictiondataset.
7 Conclusion
Inthiswork,weproposeMIA,adatacurationpipelineaimedatempoweringanywhereBEVmap
predictionfromFPVimages. WereleasealargeMIAdatasetobtainedthroughthedataenginegiving
the research community access to 1.2M FPV-BEV pairs to accelerate anywhere map prediction
research. Resultsfromtrainingonthedatasetshowimpressivegeneralizationperformanceacross
conventionalmappredictiondatasets,whileontheotherhand,provideschallengingtestcasesforthe
researchcommunity. Ourapproachdepartsfromthetraditionalandexpensiveautonomousvehicle
8Figure7: ChallengingscenariosminedusingtheMIAdataengine: Wecuratehighwayimagesfar
fromurbanenvironmentstostresstestmodels. Thisshowcasesourabilitytoextractchallengingand
high-impacttestscenarioswherecurrentmodels,includingMapper,donotperformwell.
Table2: Benchmarkingacrossallthedatasetsinbothzero-shot&finetuningsetups.
Methods In-Domain Road Crossing Sidewalk Carpark Avg. Avg.R,S Methods In-Domain Road Sidewalk Building Terrain Avg. Avg.R,S
TIIM[29] ✓ 68.63 29.41 27.03 7.70 33.19 47.83 SkyEye[13] ✓ 76.59 40.21 32.47 44.22 48.37 58.40
SkyEye[13] × 52.57 0.00 15.47 0.00 17.01 34.02 TIIM[29] × 67.18 10.41 0.0 0.0 19.40 38.80
Mapper × 64.22 0.06 27.71 0.04 23.01 45.97 Mapper × 58.92 11.99 25.08 0.60 24.15 35.46
(a)Zero-ShotNuScenes[4] (b)Zero-ShotKITTI360-BEV[13]
Methods Data% Drivable Crossing Walkway Carpark Mean
Methods In-Domain Road Crossing Sidewalks Building Parking Terrain Avg. Avg.R,S Mapper 0% 64.22 0.06 27.71 0.04 23.01
Mapper+OOD ✓ 58.28 0.05 30.75 13.80 13.79 6.70 20.56 44.52 TIIM[29] 1% 57.80 3.11 12.66 0.10 18.42
TIIM[29] × 32.74 0.00 1.47 0.00 0.00 0.00 5.70 17.11 Mapper 1% 70.83 0.25 26.99 0.08 24.54
SkyEye[13] × 33.09 0.00 1.18 5.70 0.00 2.67 7.11 17.14 TIIM[29] 10% 61.12 23.69 14.52 1.53 25.22
Mapper × 54.65 0.06 27.55 15.78 2.66 1.83 17.09 41.10 Mapper 10% 75.06 12.41 19.84 0.00 26.83
TIIM[29] 100% 68.63 29.41 27.03 7.70 33.19
(c)Zero-ShotMIA-OOD
(d)FinetuningMapperonNuScenes[4]data
datacollectionandlabelingparadigm,towardsautomaticcurationofreadily-availablecrowd-sourced
data. Webelievethisworkseedsthefirststeptowardsanywheremapprediction.
7.1 Limitations,Biases,SocialImpact
Whileweshowpromisinggeneralizationperformanceonconventionaldatasets,wenotethatlabel
noiseinherentlyexists,toahigherdegreethanmanuallycollecteddata,incrowd-sourceddata,in
both pose correspondence and in BEV map labeling. Such noise is common across large-scale
automaticallyscraped/curatedbenchmarkssuchasImageNet[10]. Moreover,ourapproachdoes
notcapturedynamicclassesastheydonotexistinstaticmaps. However,weseeourapproachas
indispensableforscaleanddiversityyetcomplementarytoconventionallyobtaineddatasets. While
werecognizethatoursampleddatasetisbiasedtowardslocationsintheUS,ourMIAdataengineis
directlyapplicabletootherworldwidelocations.
NegativeSocietalImpact: Ourworkreliesheavilyoncrowd-sourceddata,whichplacestheburden
ofdatacollectiononopen-sourcecontributors. Additionally,whiletheFPVimagesandmetadata
fromMapillaryaredesensitized,thereremainsapotentialriskofreconstructingprivateinformation.
AcknowledgmentsandDisclosureofFunding
TheworkisfundedbyNationalInstituteofAdvancedIndustrialScienceandTechnology(AIST),
ArmyResearchLab(ARL)awardsW911NF2320007andW911NF1820218andW911NF20S0005,
DefenceScienceandTechnologyAgency(DSTA).OmarAlamaispartiallyfundedbyKingAbdulaziz
University. ThisworkusedBridges-2atPSCthroughallocationcis220039pfromtheAdvanced
Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program which is
supportedbyNSFgrants#2138259,#2138286,#2138307,#2137603,and#213296.
WethankMihirSharmaforhisworkonrelatedresearchcurationandexplorationintopotentialexten-
sionsinvolvingterrainheightextraction. WethankMononitoGoswamiforinvaluablediscussionson
paperorganization,framing,experimentaldesign,andprojectfeedback. Additionally,weappreciate
DavidFan,SimonStepputtis,andYaqiXiefortheirinsightfulfeedbackthroughouttheproject.
9References
[1] Shubhra Aich, Wenshan Wang, Parv Maheshwari, Matthew Sivaprakasam, Samuel Triest,
CherieHo,JasonMGregory,JohnGRogersIII,andSebastianScherer. Deepbayesianfuture
fusionforself-supervised,high-resolution,off-roadmapping. arXivpreprintarXiv:2403.11876,
2024. 2
[2] ManuelLópezAntequera,PauGargallo,MarkusHofinger,SamuelRotaBulò,YubinKuang,
andPeterKontschieder. Mapillaryplanet-scaledepthdataset. InComputerVision–ECCV
2020: 16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartII,
page589–604,Berlin,Heidelberg,2020.Springer-Verlag. ISBN978-3-030-58535-8. doi: 10.
1007/978-3-030-58536-5_35.URLhttps://doi.org/10.1007/978-3-030-58536-5_35.
3
[3] MaximilianBernhard,NiklasStrauß,andMatthiasSchubert. Mapformer: Boostingchange
detectionbyusingpre-changeinformation. March2023. doi: 10.48550/ARXIV.2303.17859. 2
[4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,
Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal
datasetforautonomousdriving,2020. 2,3,6,7,9
[5] Ming-FangChang,JohnWLambert,PatsornSangkloy,JagjeetSingh,SlawomirBak,Andrew
Hartnett, DeWang, PeterCarr, SimonLucey, DevaRamanan, andJamesHays. Argoverse:
3dtrackingandforecastingwithrichmaps. InConferenceonComputerVisionandPattern
Recognition(CVPR),2019. 2,3
[6] YongquanChen,WeimingFan,WenliZheng,RuiHuang,andJiahuiYu. Predictingbird’s-eye-
viewsemanticrepresentationsusingcorrelatedcontextlearning. IEEERoboticsandAutomation
Letters,9(5):4718–4725,2024. doi: 10.1109/LRA.2024.3384078. 7
[7] Mapillarycontributors. Mapillary. https://www.mapillary.com/,2013. 3,4,2
[8] OpenStreetMap contributors. Planet dump retrieved from https://planet.osm.org. https:
//www.openstreetmap.org,2017. 3,4,2
[9] TimothéeDarcet,MaximeOquab,JulienMairal,andPiotrBojanowski. Visiontransformers
needregisters,2023. 6,5
[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale
HierarchicalImageDatabase. InCVPR09,2009. 9
[11] NoelGorelick,MattHancher,MikeDixon,SimonIlyushchenko,DavidThau,andRebecca
Moore. Googleearthengine: Planetary-scalegeospatialanalysisforeveryone. RemoteSensing
ofEnvironment,2017. doi: 10.1016/j.rse.2017.06.031. URLhttps://doi.org/10.1016/j.
rse.2017.06.031. 5
[12] NikhilGosalaandAbhinavValada. Bird’s-eye-viewpanopticsegmentationusingmonocular
frontal view images. IEEE Robotics and Automation Letters, 7(2):1968–1975, April 2022.
ISSN2377-3774. doi: 10.1109/lra.2022.3142418. 3
[13] Nikhil Gosala, Kürsat Petek, Paulo L. J. Drews-Jr, Wolfram Burgard, and Abhinav Valada.
Skyeye: Self-supervised bird’s-eye-view semantic mapping using monocular frontal view
images. In2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),
pages14901–14910.IEEE,2023. doi: 10.1109/CVPR52729.2023.01431. 2,3,6,7,8,9,1
[14] Cherie Ho, Jay Patrikar, Rogerio Bonatti, and Sebastian Scherer. Adaptive safety margin
estimation for safe real-time replanning under time-varying disturbance. arXiv preprint
arXiv:2110.03119,2021. 2
[15] Sixing Hu, Mengdan Feng, Rang Ho Man Nguyen, and Gim Hee Lee. Cvm-net: Cross-
viewmatchingnetworkforimage-basedground-to-aerialgeo-localization. 2018IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 7258–7267, 2018. URL
https://api.semanticscholar.org/CorpusID:52829753. 3
[16] AraJafarzadeh,ManuelLópezAntequera,PauGargallo,YubinKuang,CarlToft,FredrikKahl,
andTorstenSattler. Crowddriven: Anewchallengingdatasetforoutdoorvisuallocalization.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),pages
9845–9855,October2021. 3
10[17] Lihong Jin, Wei Dong, and Michael Kaess. Bevrender: Vision-based cross-view vehicle
registration in off-road gnss-denied environment, 2024. URL https://arxiv.org/abs/
2405.09001. 2
[18] AlexHLang,SourabhVora,HolgerCaesar,LubingZhou,JiongYang,andOscarBeijbom.
Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages12697–12705,2019.
3
[19] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and
JifengDai. Bevformer: Learningbird’s-eye-viewrepresentationfrommulti-cameraimagesvia
spatiotemporaltransformers,2022. 2
[20] YiyiLiao,JunXie,andAndreasGeiger. KITTI-360: Anoveldatasetandbenchmarksforurban
sceneunderstandingin2dand3d. PatternAnalysisandMachineIntelligence(PAMI),2022. 2,
8,3
[21] ZhijianLiu,HaotianTang,AlexanderAmini,XinyuYang,HuiziMao,DanielaLRus,andSong
Han. Bevfusion: Multi-taskmulti-sensorfusionwithunifiedbird’s-eyeviewrepresentation.
In2023IEEEinternationalconferenceonroboticsandautomation(ICRA),pages2774–2781.
IEEE,2023. 3
[22] ChenyangLu,MarinusJacobusGerardusvandeMolengraft,andGijsDubbelman. Monocular
semanticoccupancygridmappingwithconvolutionalvariationalencoder-decodernetworks.
IEEE Robotics and Automation Letters, 4(2):445–452, April 2018. ISSN 2377-3774. doi:
10.1109/lra.2019.2891028. 3
[23] KaustubhMani,SwapnilDaga,ShubhikaGarg,SaiShankarNarasimhan,MadhavaKrishna,
andKrishnaMurthyJatavallabhula. Monolayout: Amodalscenelayoutfromasingleimage. In
ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision,pages
1689–1697,2020. 2,3
[24] XiangyunMeng,NathanHatch,AlexanderLambert,AnqiLi,NolanWagener,MatthewSchmit-
tle,JoonHoLee,WentaoYuan,ZoeyChen,SamuelDeng,GregOkopal,DieterFox,Byron
Boots,andAmirrezaShaban. TerrainNet: VisualModelingofComplexTerrainforHigh-speed,
Off-roadNavigation. InRobotics: ScienceandSystems(R:SS),2023. 2
[25] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,
PierreFernandez, DanielHaziza, FranciscoMassa, AlaaeldinEl-Nouby, MahmoudAssran,
NicolasBallas,WojciechGaluba,RussellHowes,Po-YaoHuang,Shang-WenLi,IshanMisra,
MichaelRabbat,VasuSharma,GabrielSynnaeve,HuXu,HervéJegou,JulienMairal,Patrick
Labatut,ArmandJoulin,andPiotrBojanowski. Dinov2:Learningrobustvisualfeatureswithout
supervision,2023. 6,5
[26] MichaelWOtte,ScottGRichardson,JaneMulligan,andGregoryGrudic.Localpathplanningin
imagespaceforautonomousrobotnavigationinunstructuredenvironments. In2007IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems,pages2819–2826.IEEE,2007. 2
[27] JonahPhilionandSanjaFidler. Lift,splat,shoot: Encodingimagesfromarbitrarycamerarigs
byimplicitlyunprojectingto3d. InComputerVision–ECCV2020: 16thEuropeanConference,
Glasgow,UK,August23–28,2020,Proceedings,PartXIV16,pages194–210.Springer,2020.
3
[28] ThomasRoddickandRobertoCipolla. Predictingsemanticmaprepresentationsfromimages
usingpyramidoccupancynetworks. arXiv,2020. doi: 10.48550/ARXIV.2003.13402. 3,6
[29] AvishkarSaha,OscarMendez,ChrisRussell,andRichardBowden. Translatingimagesinto
maps. In2022Internationalconferenceonroboticsandautomation(ICRA),pages9200–9206.
IEEE,2022. 2,3,7,8,9,1
[30] Paul-EdouardSarlin,DanielDeTone,Tsun-YiYang,ArmenAvetisyan,JulianStraub,Tomasz
Malisiewicz,SamuelRotaBulo,RichardNewcombe,PeterKontschieder,andVasileiosBalntas.
OrienterNet: VisualLocalizationin2DPublicMapswithNeuralMatching. InCVPR,2023. 2,
3,4,6,5
[31] Paul-Edouard Sarlin, Eduard Trulls, Marc Pollefeys, Jan Hosang, and Simon Lynen. Snap:
Self-supervisedneuralmapsforvisualpositioningandsemanticunderstanding. Advancesin
11NeuralInformationProcessingSystems,36,2024. 3
[32] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul
Tsui,JamesGuo,YinZhou,YuningChai,BenjaminCaine,VijayVasudevan,WeiHan,Jiquan
Ngiam, HangZhao, AlekseiTimofeev, ScottEttinger, MaximKrivokon, AmyGao, Aditya
Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in
perceptionforautonomousdriving: Waymoopendataset. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),June2020. 2,3,5
[33] SamuelTriest,MateoGuamanCastro,ParvMaheshwari,MatthewSivaprakasam,Wenshan
Wang,andSebastianScherer. Learningrisk-awarecostmapsviainversereinforcementlearning
foroff-roadnavigation. In2023IEEEInternationalConferenceonRoboticsandAutomation
(ICRA),pages924–930.IEEE,2023. 2
[34] SergeyVartanov. Mapmachine. https://github.com/enzet/map-machine,2022. 5,3
[35] FrederikWarburg,SørenHauberg,ManuelLópez-Antequera,PauGargallo,YubinKuang,and
JavierCivera. Mapillarystreet-levelsequences: Adatasetforlifelongplacerecognition. In
ComputerVisionandPatternRecognition(CVPR),June2020. 3
[36] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh
Khandelwal,BowenPan,RatneshKumar,AndrewHartnett,JhonyKaesemodelPontes,Deva
Ramanan,PeterCarr,andJamesHays. Argoverse2: Nextgenerationdatasetsforself-driving
perceptionandforecasting. InProceedingsoftheNeuralInformationProcessingSystemsTrack
onDatasetsandBenchmarks(NeurIPSDatasetsandBenchmarks2021),2021. 2,3
[37] XindiWu,KwunFungLau,FrancescoFerroni,AljošaOšep,andDevaRamanan. Pix2map:
Cross-modalretrievalforinferringstreetmapsfromimages. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages17514–17523,2023. 3
[38] ZhixinZhang,YiyuanZhang,XiaohanDing,FushengJin,andXiangyuYue. Onlinevectorized
hdmapconstructionusinggeometry. arXivpreprintarXiv:2312.03341,2023. 3
[39] TianhaoZhao,YongcanChen,YuWu,TianyangLiu,BoDu,PeilunXiao,ShiQiu,Hongda
Yang,GuozhenLi,YiYang,andYutianLin. Improvingbird’seyeviewsemanticsegmentation
bytaskdecomposition. arXiv,2024. doi: 10.48550/ARXIV.2404.01925. 7
[40] SijieZhu,TaojiannanYang,andChenChen. Vigor: Cross-viewimagegeo-localizationbeyond
one-to-oneretrieval. 2021IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages5316–5325,2020. URLhttps://api.semanticscholar.org/CorpusID:
227151840. 3
[41] XiyueZhu,VlasZyrianov,ZhijianLiu,andShenlongWang. Mapprior: Bird’s-eyeviewmap
layout estimation with generative models. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pages8228–8239,2023. 3
12A Appendix
A.1 FilteringPipelineYields
Table3showstheyieldofdatasamplesfromthedifferentstagesoftheMIAcurationpipeline. Table4
presentsastatisticsbreakdownonthelocations.
Table3: TheFPVFilteringpipelinestagefortheMIAdataset,covering6cities,startsbyfiltering
imagesbasedoncityboundariesandrecency(imagesafter2017). Itthenselects17cameramodels
andfiltersimagesbasedonlocationandanglediscrepanciesbetweentheSfMrectifiedposeand
recordedpose(keepingthosewithlessthan20°and3mdiscrepancies). Finally,aspatialsparsity
filterremovesimageswithina4mradiusinasequence.
CurationStage Boundaries Recency CameraModel AngleDiscrip LocDiscrip Spatial
#Images 15.93M 12.21M 3.049M 2.606M 1.353M 1.204M
%Images 100.00% 76.67% 19.15% 16.36% 8.50% 7.56%
Table 4: FPV numbers for the MIA Dataset as the curated data moves from the beginning of the
pipeline (top of table) down to the end. PT: Pittsburgh, NY: New York, CG: Chicago, LA: Los
Angeles,SF:SanFrancisco,HS:Houston.
Stage PT NY CG LA SF HS ALL
Boundaries 914.1K 3.161M 1.422M 4.150M 2.825M 3451961 15.93M
Recency 867.0K 2.999M 1.281M 4.055M 2.100M 908.5K 12.21M
CameraModel 31.0K 270.5K 478.5K 1.917M 294.9K 57.3K 3.049M
AngleDiscrip 25.7K 177.6K 433.6K 1.758M 156.9K 54.0K 2.606M
LocDiscrip 18.0K 89.8K 196.6K 958.6K 59.2K 31.1K 1.353M
Spatial 15.9K 79.2K 162.4K 879.4K 37.7K 29.4K 1.204M
A.2 DatasetSplitVisualization
MIAdatasetissplitintotrain,validation,andtestpartitionsbasedonthelocation. Weensurethatthe
samplesinthethreepartitionsaregeographicallynon-overlapping,asshowninFig.8.
A.3 Intra-DatasetClassMappings
As different dataset captures different map elements, we map the classes from NuScenes and
KITTI360-BEVcarefullytoMIA,asshowninTable5.
Table5: Intra-DatasetClassMappingforZero-ShotExperiments
KITTI360-BEV
Mapper NuScenes[29]
[13]
Road Drivable Road
Crossing Crossing N/A
Sidewalks Walkway Sidewalk
Building N/A Building
Parking Carpark N/A
Terrain N/A Terrain
A.4 Hyperparameters
Wereleaseourpipelinehyperparametersinthissection.WeuseTable6inourdataengine;Table7for
pretrainingMapperusingMIAdataset;andTable8,Table9forNuScenesandKITTI-360fine-tuning.
1Figure8:MIA Datasetsplitvisualization. Bluefortraining,greenforvalidation,andredfortesting.
Weensurethatthesplitsaregeographicallynon-overlappingforamorerobustevaluation.
Table6: DataCurationHyperparameters
Parameter Value
α 224
δ 50
ρ 0.5
RecencyFilter >2017
LocationDiscrepancy 3m
AngleDiscrepancy 20°
SparsityFilter 4m
CameraModels "hdr-as200v","iphone11pro","iphone11","iphone12",
"gopromax","iphone12pro","lm-v405",
"iphone11promax","hdr-as300","iphone13",
"fdr-x1000v","sm-g970u","sm-g930v",
"iphone13promax","iphone13pro","iphone12promax",
"fdr-x3000"
A.5 DatasetPrivacyandConsent
OurFirst-Person-Viewdatasource,Mapillary[7],employsmeasurestoensureprivacybyblurring
facesandlicenseplates,therebyremovingpersonallyidentifiableinformation. Detailedinformation
ontheirprivacypolicyisavailablehere. WhenuserscontributeimagestoMapillary,theseimagesare
sharedundertheCC-BY-SAlicense. Furtherdetailsonthislicensingcanbefoundhere.
OurBird’sEyeMap(BEV)datasource,OpenStreetMap[8]providesguidancetolimitmapping
privateinformation. Detailscanbefoundhere.
MorelicensinginformationcanbefoundinTable10.
2Table7: PretrainingHyperparameters
Data Model
Parameter Value Parameter Value
ResizeImage 512 BackboneModel DINOv2ViT-B/14w/
registers
BatchSize 128 LatentDimension 128
GravityAlignment Yes DropoutRate 0.2
PadToSquare Yes LearningRate 1.00E-03
RectifyPitch Yes LRScheduler CosineAnnealingLR
Scenes Chicago,NewYork,Los Losses DiceLoss+BinaryCross
Angeles,SanFrancisco EntropyLoss
Augmentations Brightness,Contrast, LossMask Frustum
Saturation,RandomFlip,
Hue
ClassWeights [1.00351229,4.34782609,
1.00110121,1.03124678,
6.69792364,7.55857899]
Table8: Finetuning(NuScenes)Hyperparameters
Data Model
Parameter Value Parameter Value
ResizeImage 512 BackboneModel DINOv2ViT-B/14w/
registers
BatchSize 128 LatentDimension 128
GravityAlignment Yes DropoutRate 0
PadToSquare Yes LearningRate 1.00E-04
RectifyPitch Yes LRScheduler CosineAnnealingLR
Scenes All(OnlyFrontCamera) Losses DiceLoss+BinaryCross
EntropyLoss
Augmentations Brightness,Contrast, LossMask Frustum
Saturation,RandomFlip,
Hue
ClassWeights [1.00060036,1.85908161,
1.0249052,2.57267816]
Table9: Finetuning(KITTI360-BEV)Hyperparameters
Data Model
Parameter Value Parameter Value
TargetFocalLength 256 BackboneModel DINOv2ViT-B/14w/
registers
BatchSize 32 LatentDimension 128
GravityAlignment No DropoutRate 0.1
PadToSquare Yes LearningRate 1.00E-04
RectifyPitch Yes LRScheduler CosineAnnealingLR
Scenes All(OnlyFrontCamera) Losses DiceLoss+BinaryCross
EntropyLoss
Augmentations Brightness,Contrast, LossMask Frustum+Visibility
Saturation,RandomFlip,
Hue
ClassWeights [2.5539,3.8968,1.9405,
5.6612]
Table10: Licensesforallprojectsanddatasetsusedthroughoutthiswork. "*"denotesworksused
forthedevelopmentofMIA."†"denotesworksusedforevaluationpurposesonly.
Projects License
Mapillary*[7] CCBY-SA4.0
OpenStreetMap*[8] ODbL
MapMachine*[34] MITLicense
Nuscenes†[4] CCBY-NC-SA4.0
KITTI-360†[20] CCBY-NC-SA3.0
KITTI-360-BEV†[13] NonCommercialUseOnly
OrienterNet*[30] CC-BY-SA4.0
3B AdditionalMIAdatasamples
AdditionaldatasamplesfromMIAdatasetandassociatedMapillarymetadataareshowninFigure9.
Figure9: AdditionalMIAdatasamplesandassociatedmetadata
C MapperModelArchitecture
Figure10: MapperArchitecture
WedesignasimplemodelarchitectureMapperthatcanleveragethediversecharacteristicsofthe
MIAdataset. Asdiscussedinthemainpaper,ourmodelbuildsonOrienterNet[30]asthefront-end
architecture. ThischoiceismadebecauseOrienterNetaccommodatesdifferentcameracharacteristics
(e.g. focallengthsandimagesize)andposesinformation,therebyleveragingthefullinformation
availablefromMapillary.
Toincorporateposeinformation,weadoptOrienterNet’sapproachofgravity-aligningtheimage,
ensuringthateachcolumnalignswithagravity-alignedverticalplanein3D[30]. Gravityalignment
isessentialformatchingwithOpenStreetMapdata,asmanyMapillaryimagesarecapturedfrom
vehiclesonslopes,orfrombicyclesandhand-heldcameraswithvaryingtilt. Aftergravityalignment,
theimageisresizedandpaddedto512x512pixels.
4Theprocessed,gravity-alignedimageisthenpassedthroughanimageencodertoobtainfirst-person
view(FPV)features. UnlikeOrienterNet[30],wereplaceitsResNetencoderwithaDINOv2ViT-
B/14[25]withregisters[9]astheimageencoderforimprovedgeneralizability. TheFPVfeatures
arethenfedintoalinearlayertoestimatepixel-wisescoresforoneof32scale(depthnormalized
byfocallength)bins. Thesescalescoresarethenmappedtometricdepthscoresbasedonthefocal
length. ThedepthscoresareusedtoprojectFPVfeaturestogenerateabird’seyeviewfeaturemap
throughpolarandCartesianprojections. ToadaptOrienterNet[30]tothemappredictiontask,we
addadecoderheadtotheBEVfeaturestopredictthesemanticmap.
5