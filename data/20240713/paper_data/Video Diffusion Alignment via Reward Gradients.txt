Video Diffusion Alignment via Reward Gradients
MihirPrabhudesai∗ RussellMendonca∗ ZheyangQin∗
KaterinaFragkiadaki DeepakPathak
CarnegieMellonUniversity
“A shark playing chess.” “A raccoon drumming on bongos under a starry night sky.”
“A child painting in an art class, using “A fairy tends to enchanted, glowing
watercolors and a brush on paper.” flowers.”
“A snow princess stands on the balcony of her ice castle, her hair “A joyful dog playing in the snow, leaving paw prints and
adorned with delicate snowflakes, overlooking her serene realm.” trying to catch snowflakes on its nose.”
Figure 1: Generations from video diffusion models after adaptation with VADER using reward
functionsforaestheticsandtext-imagealignment. Morevisualizationresultsareavailableathttps:
//vader-vid.github.io
Abstract
Wehavemadesignificantprogresstowardsbuildingfoundationalvideodiffusion
models. Asthesemodelsaretrainedusinglarge-scaleunsuperviseddata,ithas
becomecrucialtoadaptthesemodelstospecificdownstreamtasks. Adaptingthese
models via supervised fine-tuning requires collecting target datasets of videos,
which is challenging and tedious. In this work, we utilize pre-trained reward
modelsthatarelearnedviapreferencesontopofpowerfulvisiondiscriminative
models to adapt video diffusion models. These models contain dense gradient
information with respect to generated RGB pixels, which is critical to efficient
learningincomplexsearchspaces,suchasvideos. Weshowthatbackpropagating
gradients from these reward models to a video diffusion model can allow for
compute and sample efficient alignment. We show results across a variety of
rewardmodelsandvideodiffusionmodels,demonstratingthatourapproachcan
learnmuchmoreefficientlyintermsofrewardqueriesandcomputationthanprior
gradient-freeapproaches. Ourcode,modelweights,andmorevisualizationare
availableathttps://vader-vid.github.io.
∗EqualContribution
Preprint.Underreview.
4202
luJ
11
]VC.sc[
1v73780.7042:viXra1 Introduction
Wewouldliketobuildsystemscapableofgeneratingvideosforawidearrayofapplications,ranging
frommovieproduction,creativestory-boarding,on-demandentertainment,AR/VRcontentgenera-
tion,andplanningforrobotics. Themostcommoncurrentapproachinvolvestrainingfoundational
videodiffusionmodelsonextensiveweb-scaledatasets. However,thisstrategy,whilecrucial,mainly
producesvideosthatresembletypicalonlinecontent,featuringdullcolors,suboptimalcameraangles,
andinadequatealignmentbetweentextandvideocontent.
Contrast this with the needs of an animator who wishes to bring a storyboard to life based on a
scriptandafewpreliminarysketches. Suchcreatorsarelookingforoutputthatnotonlyadheres
closelytotheprovidedtextbutalsomaintainstemporalconsistencyandshowcasesdesirablecamera
perspectives. Relyingongeneral-purposegenerativemodelsmaynotsufficetomeetthesespecific
requirements. Thisdiscrepancystemsfromthefactthatlarge-scalediffusionmodelsaregenerally
trainedonabroadspectrumofinternetvideos,whichdoesnotguaranteetheirefficacyforparticular
applications. Trainingthesemodelstomaximizelikelihoodacrossavastdatasetdoesnotnecessarily
translatetohigh-qualityperformanceforspecializedtasks. Moreover,theinternetisamixedbag
whenitcomestocontentquality,andmodelstrainedtomaximizelikelihoodmightinadvertently
replicatelower-qualityaspectsofthedata. Thisleadsustothequestion: Howcanwetailordiffusion
modelstoproducevideosthatexcelintask-specificobjectives,ensuringtheyarewell-alignedwith
thedesiredoutcomes?
Theconventionalapproachtoaligninggenerativemodelsinthelanguageandimagedomainsbegins
withsupervisedfine-tuning[22,4]. Thisinvolvescollectingatargetdatasetthatcontainsexpected
behaviors,followedbyfine-tuningthegenerativemodelonthisdataset. Applyingthisstrategyto
videogeneration,however,presentsasignificantlygreaterchallenge. Itrequiresobtainingadataset
oftargetvideos,ataskthatisnotonlymorecostlyandlaboriousthansimilarendeavorsinlanguage
orimagedomains,butalsosignificantlymorecomplex. Furthermore,evenifwewereabletocollect
avideotargetdataset,theprocesswouldhavetoberepeatedforeverynewvideotask,makingit
prohibitivelyexpensive. Isthereadifferentsourceofsignalwecanuseforaligningvideodiffusion,
insteadoftryingtocollectatargetdatasetofdesiredvideos?
Rewardmodelsplayacrucialrole[24,32,17]inaligningimageandtextgenerations. Thesemodels
aregenerallybuiltontopofpowerfulimageortextdiscriminativemodelssuchasCLIPorBERT
[21,1,29]. Tousethemasrewardmodels,peopleeitherfine-tunethemviasmallamountsofhuman
preferencesdata[24]orusethemdirectlywithoutanyfine-tuning;forinstance,CLIPcanbeusedto
improveimage-textalignmentorobjectdetectorscanbeusedtoremoveoraddobjectsintheimages
[20].
This begs the question, how should reward models be used to adapt the generation pipeline of
diffusionmodels? Therearetwobroadcategoriesofapproaches,thosethatutilizerewardgradients
[20,6,33],andothersthatusetherewardonlyasascalarfeedbackandinsteadrelyonestimated
policygradients[2,18]. Ithasbeenpreviouslyfoundthatutilizingtherewardgradientdirectlyto
updatethemodelcanbemuchmoreefficientintermsofthenumberofrewardqueries,sincethe
rewardgradientcontainsrichinformationofhowtherewardfunctionisaffectedbythediffusion
generation[20,6]. However,intext-to-imagegenerationspace,rewardgradient-freeapproachesare
stilldominant[23],sincethesemethodscanbeeasilytrainedwithin24hoursandtheefficiencygains
ofleveragingrewardgradientsarenotsignificant.
Inthiswork,wefindthatasweincreasethedimensionalityofgenerationi.etransitionfromimageto
video,thegapbetweentherewardgradientandpolicygradientbasedapproachesincreases. Thisis
becauseoftheadditionalamountandincreasedspecificityoffeedbackthatisbackpropagatedtothe
model. Forrewardgradientbasedapproaches,thefeedbackgradientslinearlyscalewithrespectto
thegeneratedresolution,asityieldsdistinctscalarfeedbackforeachspatialandtemporaldimension.
In contrast, policy gradient methods receive a single scalar feedback for the entire video output.
WetestthishypothesisinFigure4,wherewefindthatthegapbetweenrewardgradientandpolicy
gradientapproachesincreasesasweincreasethegeneratedvideoresolution. Webelievethismakesit
crucialtobackpropagaterewardgradientinformationforvideodiffusionalignment.
WeproposeVADER,anapproachtoadaptfoundationalvideodiffusionmodelsusingthegradients
ofrewardmodels. VADERalignsvariousvideodiffusionmodelsusingabroadrangeofpre-trained
visionmodels. Specifically,weshowresultsofaligningtext-to-video(VideoCrafter,OpenSora,and
2ModelScope)andimage-to-video(StableVideoDiffusion)diffusionmodels,whileusingreward
modelsthatweretrainedontaskssuchasimageaesthetics,image-textalignment,objectdetection,
video-action-classification,andvideomaskedautoencoding. Further,wesuggestvarioustricksto
improvememoryusagewhichallowustotrainVADERonasingleGPUwith16GBofVRAM.
WeincludequalitativevisualizationsthatshowVADERsignificantlyimprovesuponthebasemodel
generationsacrossvarioustasks. WealsoshowthatVADERachievesmuchhigherperformancethan
alternativealignmentmethodsthatdonotutilizerewardgradients,suchasDPOorDDPO.Finally,
weshowthatalignmentusingVADERcaneasilygeneralizetopromptsthatwerenotseenduring
training. Ourcodeisavailableathttps://vader-vid.github.io.
2 RelatedWork
Denoisingdiffusionmodels[26,11]havemadesignificantprogressingenerativecapabilitiesacross
variousmodalitiessuchasimages, videosand3Dshapes[10,12,19]. Thesemodelsaretrained
usinglarge-scaleunsupervisedorweaklysuperviseddatasets. Thisformoftrainingresultsinthem
havingcapabilitiesthatareverygeneral;however,mostenduse-casesofthesemodelshavespecific
requirements,suchashigh-fidelitygeneration[24]orbettertextalignment[32].
Tobesuitablefortheseuse-cases,modelsareoftenfine-tunedusinglikelihood[3,4]orreward-based
objectives[2,20,6,33,18,7,9]. Likelihoodobjectivesareoftendifficulttoscale,astheyrequire
accesstothepreferredbehaviourdatasets. Rewardorpreferencebaseddatasetsontheotherhand
aremucheasiertocollectastheyrequireahumantosimplyprovidepreferenceorrewardforthe
data generated by the generative model. Further, widely available pre-trained vision models can
alsobeusedasrewardmodels,thusmakingitmucheasiertodorewardfine-tuning[2,20]. The
standardapproachforrewardorpreferencebasedfine-tuningistodoreinforcementlearningvia
policygradients[2,30]. Forinstance, theworkof[18]doesreward-weightedlikelihoodandthe
workof[2]appliesPPO[25]. Recentworksof[20,6],findthatinsteadofusingpolicygradients,
directlybackpropagatinggradientsfromtherewardmodeltodiffusionprocesshelpssignificantly
withsampleefficiency.
Arecentmethod,DPO[22,30],doesnottrainanexplicitrewardmodelbutinsteaddirectlyoptimizes
onthehumanpreferencedata.Whilethismakesthepipelinemuchsimpler,itdoesn’tsolvethesample
inefficiencyissueofpolicygradientmethods,asitstillbackpropagatesasinglescalarfeedbackfor
theentirevideooutput.
While we have made significant progress in aligning image diffusion models, this has remained
challenging for video diffusion models [3, 31]. In this work, we take up this challenging task.
Wefindthatnaivelyapplyingpriortechniquesofimagealignment[20,6]tovideodiffusioncan
result in significant memory overheads. Further, we demonstrate how widely available image or
videodiscriminativemodelscanbeusedtoalignvideodiffusionmodels. Concurrenttoourwork,
InstructVideo[34]alsoalignsvideodiffusionmodelsviahumanpreference;however,thismethod
requiresaccesstoadatasetofvideos. Suchadatasetisdifficulttoobtainforeachdifferenttask,and
becomesdifficulttoscaleespeciallytolargenumbersoftasks. Inthiswork,weshowthatonecan
easilyalignvideodiffusionmodelsusingpre-trainedrewardmodelswhilenotassumingaccessto
anyvideodataset.
3 Background
Diffusionmodelshaveemergedasapowerfulparadigminthefieldofgenerativemodeling. These
modelsoperatebymodelingadatadistributionthroughasequentialprocessofaddingandremoving
noise.
Theforwarddiffusionprocesstransformsadatasamplexintoacompletelynoisedstateoveraseries
ofstepsT. Thisprocessisdefinedbythefollowingequation:
√ √
x = α¯ x+ 1−α¯ ϵ, ϵ∼N(0,1), (1)
t t t
3whereϵrepresentsnoisedrawnfromastandardGaussiandistribution. Here,α¯
=(cid:81)t
α denotes
t i=1 i
thecumulativeproductofα =1−β ,whichindicatestheproportionoftheoriginaldata’ssignal
i i
retainedateachtimestept.
The reverse diffusion process reconstructs the original data sample from its noised version by
progressively denoising it through a learned model. This model is represented by ϵ (x ;t) and
θ t
estimatesthenoiseϵaddedateachtimestept.
Diffusionmodelscaneasilybeextendedforconditionalgeneration. Thisisachievedbyaddingcas
aninputtothedenoisingmodel:
1 (cid:88) √ √
L (θ;D′)= ||ϵ ( α¯ xi+ 1−α¯ ϵ,ci,t)−ϵ||2, (2)
diff |D′| θ t t
xi,ci∈D′
whereD′denotesadatasetconsistingofimage-conditiongpairs. Thislossfunctionminimizesthe
distancebetweentheestimatednoiseandtheactualnoise,andalignswiththevariationallowerbound
forlogp(x|c).
Tosamplefromthelearneddistributionp (x|c),onestartswithanoisesamplex ∼N(0,1)and
θ T
iterativelyappliesthereversediffusionprocess:
(cid:18) (cid:19)
1 β
x = √ x − √ t ϵ (x ,t,c) +σ z, z∼N(0,1), (3)
t−1 α t 1−α¯ θ t t
t t
Theaboveformulationcapturestheessenceofdiffusionmodels,whichhighlightstheirabilityto
generatestructureddatafromrandomnoise.
4 VADER:VideoDiffusionviaRewardGradients
Reverse Diffusion Send Gradients
Reward
Model Loss
∇θLoss
x T x t x t−1 x 0
Figure2: VADERalignsvariouspre-trainedvideodiffusionmodelsbybackpropagatinggradients
fromtherewardmodel,toefficientlyadapttospecifictasks.
Wepresentourapproachforadaptingvideodiffusionmodelstoperformaspecifictaskspecifiedvia
arewardfunctionR(.).
Givenavideodiffusionmodelp (.),datasetofcontextsD ,andarewardfunctionR(.),weseekto
θ c
maximizethefollowingobjective:
J(θ)=E [R(x ,c)]
c∼Dc,x0∼pθ(x0|c) 0
Tolearnefficiently,bothintermsofthenumberofrewardqueriesandcomputetime,weseekto
utilizethegradientstructureoftherewardfunction,withrespecttotheweightsθofthediffusion
model. Thisisapplicabletoallrewardfunctionsthataredifferentiableinnature. Wecomputethe
gradient∇ R(x ,c)ofthesedifferentiablerewards,anduseittoupdatethediffusionmodelweights
θ 0
θ. Thegradientisgivenby:
T
∇ R(x
,c)=(cid:88)∂R(x 0,c)
·
∂x
t.
θ 0 ∂x ∂θ
t
t=0
4VADER is flexible in terms of the denoising
schedule, we demonstrate results with DDIM
Algorithm1VADER
[27] and EDM solver [14]. To prevent over-
Require: DiffusionModelweightsθ optimization,weutilizetruncatedbackpropaga-
Require: RewardfunctionR(.) tion[28,20,6],wherethegradientisbackprop-
Require: DenoisingSchedulerf agatedonlyforKsteps,whereK<T,whereT
(eg-DDIM[27],EDM[14]) isthetotaldiffusiontimesteps. Usingasmaller
Require: GradientcutoffstepK valueofKalsoreducesthememoryburdenof
1: whiletrainingdo havingtobackpropagategradients,makingtrain-
2: fort=T,..1do ingmorefeasible. Weprovidethepseudocode
3: pred=ϵ θ(x t,c,t) ofthefulltrainingprocessinAlgorithm1. Next,
4: ift>Kthen wediscussthetypeofrewardfunctionswecon-
5: pred=stop_grad(pred) siderforaligningvideomodels.
6: endif
RewardModels: Consideradiffusionmodel
7: x t−1=f.step(pred,t,x t)
that takes conditioning vector c as input and
8: endfor
generatesavideox oflengthN,consistingof
9: g =∇ θR(x 0,c) 0
aseriesofimagesi ,foreachtimestepkfrom0
10: θ ←θ−η∗g k
toN. Thentheobjectivefunctionwemaximize
11: endwhile
isasfollows:
J =E [R([i ,i ...i ...i ],c)]
θ c,i0:N 0 1 k N−1
Weuseabroadrangeofrewardfunctionsforaligningvideodiffusionmodels. Belowwelistdown
thedistincttypesofrewardfunctionsweconsider.
Image-Text Similarity Reward - The generations from the diffusion model correspond to the text
providedbytheuserasinput. Toensurethatthevideoisalignedwiththetextprovided,wecandefine
arewardthatmeasuresthesimilaritybetweenthegeneratedvideoandtheprovidedtext. Totake
advantageofpopular,large-scaleimage-textmodelssuchasCLIP[21],wecantakethefollowing
approach. Fortheentirevideotobewellaligned,eachoftheindividualframesofthevideolikely
needtohavehighsimilaritywiththecontextc. Givenanimage-contextsimilaritymodelg ,we
img
have:
(cid:88) (cid:88)
R([i ,i ...i ...i ],c)= R(i ,c)= g (i ,c)
0 1 k N−1 k img k
k k
Then,wehaveJ =E [g (i ,c)],usinglinearityofexpectationasintheimage-alignment
θ k∈[0,N] img k
case. WeconductexperimentsusingtheHPSv2[32]andPickScore[16]rewardmodelsforimage-
textalignment. Astheaboveobjectiveonlysitsonindividualimages,itcouldpotentiallyresultina
collapse,wherethepredictedimagesaretheexactsameortemporallyincoherent. However,wedon’t
findthistohappenempirically,wethinktheinitialpre-trainingsufficientlyregularizesthefine-tuning
processtopreventsuchcases.
Video-TextSimilarityReward-Insteadofusingperimagesimilaritymodelg ,itcouldbebeneficial
img
to evaluate the similarity between the whole video and the text. This would allow the model to
generatevideoswherecertainframesdeviatefromthecontext, allowingforricher, morediverse
expressivegenerations. Thisalsoallowsgeneratingvideoswithmoremotionandmovement,which
is better captured by multiple frames. Given a video-text similarity model g we have J =
vid θ
E[g ([i ,i ...i ...i ],c)]. In our experiments, we use a VideoMAE[29] fine-tuned on action
vid 0 1 k N−1
classification,asg ,whichcanclassifyaninputvideointooneofasetofactiontextdescriptions.
vid
Weprovidethetargetclasstextasinputtothetext-to-videodiffusionmodel,andusethepredicted
probabilityofthegroundtruthclassfromVideoMAEasthereward.
ImageGenerationObjective-Whiletextsimilarityisastrongsignaltooptimize,someusecases
mightbebetteraddressedbyrewardmodelsthatonlysitonthegeneratedimage.Thereisaprevalence
ofpowerfulimage-baseddiscriminativemodelssuchasObjectDetectorsandDepthPredictors.These
modelsutilizetheimageasinputtoproducevarioususefulmetricsoftheimage,whichcanbeused
asareward. Thegeneratedvideoislikelytobebetteralignedwiththetaskiftherewardobtained
oneachofthegeneratedframesishigh. Hencewedefinetherewardinthiscasetobethemean
(cid:80)
oftherewardsevaluatedoneachoftheindividualframes,i.eR([i ,i ...i ...i ],c)= R(i ).
0 1 k N−1 k k
Notethatgiventhegeneratedframes,thisisindependentofthetextinputc. Hencewehave,J =
θ
5E [R(i )] = E [M (i )] via linearity of expectation, where M is a discriminative
k∈[0,N] k k∈[0,N] ϕ k ϕ
modelthattakesanimageasinputtoproduceametric,thatcanbeusedtodefineareward. Weuse
theAestheticRewardmodel[24]andObjectDetector[8]rewardmodelforourexperiments.
VideoGenerationObjective-Withaccesstoanexternalmodelthattakesinmultipleimageframes,
wecandirectlyoptimizefordesiredqualitiesofthegeneratedvideo. GivenavideometricmodelN ,
ϕ
thecorrespondingrewardisJ =E[N ([i ,i ,..i ...i ])].
θ ϕ 0 1 k N−1
VideoCrafter VADER (Ours)
“The raccoon is wearing a red coat and holding a snowball.”
“The fox is wearing a red hat and playing with leaves.”
“A dog playing a slide guitar on a porch during a gentle rainstorm.”
“A strong lion and a graceful lioness resting together in the shade of a big tree on a wide grassland.”
“A peaceful deer eating grass in a thick forest, with sunlight filtering through the trees.”
“A dog strumming an acoustic guitar by a lakeside campfire under the stars.”
Figure 3: Text-to-video generation results for VideoCrafter and VADER. We show results for
VideoCrafterText-to-VideomodelontheleftandresultsforVADERontheright, whereweuse
VideoCrafterasourbasemodel. TherewardmodelsappliedareacombinationofHPSV2.1and
Aestheticmodelinthefirstthreerows,andPickScoreinthelastthreerows. Thevideosinthethird
andlastrowsaregeneratedbasedonpromptsthatarenotencounteredduringtraining.
Long-horizonconsistentgeneration:Inourexperiments,weadoptthisformulationtoenableafeature
thatisquitechallengingformanyopen-sourcevideodiffusionmodels-thatofgeneratingclipsthat
arelongerinlength. Forthistask,weuseStableVideoDiffusion[3],whichisanimage-to-video
diffusion model. We increase the context length of Stable Video Diffusion by 3x by making it
autoregressive. Specifically,wepassthelastgeneratedframebythemodelasinputforgenerating
thenextvideosequence. However,wefindthistonotworkwell,asthemodelwasnevertrained
overitsowngenerationsthusresultinginadistributionshift. Inordertoimprovethegenerations,we
useavideometricmodelN (V-JEPA[1])thatgivenasetofframes,producesascoreabouthow
ϕ
predictivetheframesarefromoneanother. Weapplythismodelontheautoregressivegenerations,
6toencouragethesetoremainconsistentwiththeearlierframes. Trainingthemodelinthismanner
allowsustomakethevideoclipstemporallyandspatiallycoherent.
ReducingMemoryOverhead: Trainingvideo
diffusionmodelsisverymemoryintensive,as
theamountofmemorylinearlyscaleswithre-
specttothenumberofgeneratedframes. While  5 H Z D U G  * U D G L H Q W
     
VADER significantly improves the sample ef-  3 R O L F \  * U D G L H Q W
ficiency of fine-tuning these models, it comes        % D V H
at the cost of increased memory. This is be-      
causethedifferentiablerewardiscomputedon
     
thegeneratedframe,whichisaresultofsequen-
tialde-noisingsteps.      
(i)StandardTricks: Toreducethememoryus-
     
ageweuseLoRA[13]thatonlyupdatesasubset
ofthemodelparameters,furtherweusemixed
  [   [   [    [    [    [
precision that stores non-trainable parameters
 5 H V R O X W L R Q
infp16. Toreducememoryusageduringback-
propagationweusegradientcheckpointingand Figure4: Rewardgradientvspolicygradientap-
forthelonghorizontasks,offloadthestorageof proachesasoutputresolutionincreases: Wetrain
thebackwardcomputationgraphfromtheGPU DDPOandVADER,withincreasingresolutionof
memorytotheCPUmemory. thegeneratedvideo. Intheabovecurve,wereport
(ii)TruncatedBackprop: Additionally,Inour the reward achieved after 100 steps of optimiza-
experiments we only backpropagate through tion,wefindthatastheresolutionofthegeneration
the diffusion model for one timestep, instead increases,theperformancegapbetweenbothap-
ofbackpropagatingthroughmultipletimesteps proachessignificantlyincreases.
[20], and have found this approach to obtain
competitive results while requiring much less
memory.
(iii)SubsamplingFrames: Sinceallthevideodiffusionmodelsweconsiderarelatentdiffusion
models,wefurtherreducememoryusagebynotdecodingalltheframestoRGBpixels. Instead,we
randomlysubsampletheframesandonlydecodeandapplylossonthesubsampledones.
Weconductourexperimentson2A6000GPUS(48GBVRAM),andourmodeltakesanaverageof
12hourstotrain. However,ourcodebasesupportstrainingonasingleGPUwith16GBVRAM.
Before VADER (Ours)
“A book and a cup of tea on a blanket in a sunflower field.”
“A book and a cup of hot chocolate on a windowsill with a snowy view.”
“A book and a cup of coffee on a rustic wooden table in a cabin.”
Figure5: Objectremoval(removebook)usingVADER.Theleftcolumndisplaysresultsfromthe
basemodel(VideoCrafter),whiletherightcolumnshowsresultsfromVADERafterfine-tuningthe
basemodeltonotdisplaybooksbyusinganobjectdetectorasarewardmodel. Ascanbeseen,
VADEReffectivelyremovesbookandreplacesitwithsomeotherobject.
7
 G U D Z H 55 Results
In this work, we focus on fine-tuning various conditional video diffusion models, including
VideoCrafter [5] , Open-Sora [35] , Stable Video Diffusion [3] and ModelScope [31], through
acomprehensivesetofrewardmodelstailoredforimagesandvideos. TheseincludetheAesthetic
modelforimages[24],HPSv2[32]andPickScore[16]forimage-textalignment,YOLOS[8]for
objectremoval,VideoMAEforactionclassification[29],andV-JEPA[1]self-supervisedlossfor
temporalconsistency. Ourexperimentsaimtoanswerthefollowingquestions:
• HowdoesVADERcompareagainstgradient-freetechniquessuchasDDPOorDPOregard-
ingsampleefficiencyandcomputationaldemand?
• Towhatextentcanthemodelgeneralizetopromptsthatarenotseenduringtraining?
• Howdothefine-tunedmodelscompareagainstoneanother,asjudgedbyhumanevaluators?
• HowdoesVADERperformacrossavarietyofimageandvideorewardmodels?
ThisevaluationframeworkassessestheeffectivenessofVADERincreatinghigh-quality,aligned
videocontentfromarangeofinputconditioning.
Baselines. WecompareVADERagainstthefollowingmethods:
• VideoCrafter[5],Open-Sora1.2[35],andModelScope[31]arecurrentstate-of-the-art
(publicly available) text-to-video diffusion models. We serve them as base models for
fine-tuningandcomparisoninourexperimentsintext-to-videospace.
• StableVideoDiffusion [3]isthecurrentstate-of-art(publiclyavailable)image-to-video
diffusionmodel. Inallourexperimentsinimage-to-videospace,weusetheirbasemodel
forfine-tuningandcomparison.
• DDPO [2] is a recent image diffusion alignment method that uses policy gradients to
adaptdiffusionmodelweights. Specifically,itappliesPPOalgorithm[25]tothediffusion
denoisingprocess. Weextendtheircodeforadaptingvideodiffusionmodels.
• Diffusion-DPO [30] extends the recent development of Direct Preference Optimization
(DPO)[22]intheLLMspacetoimagediffusionmodels. Theyshowthatdirectlymodeling
the likelihood using the preference data can alleviate the need for a reward model. We
extendtheirimplementationtoaligningvideodiffusionmodels,whereweusethereward
modeltoobtaintherequiredpreferencedata.
Rewardmodels. Weusethefollowingrewardmodelstofine-tunethevideodiffusionmodel.
• AestheticRewardModel: WeusetheLAIONaestheticpredictorV2[24],whichtakesan
imageasinputandoutputsitsaestheticscoreintherangeof1-10. Themodelistrained
ontopofCLIPimageembeddings,forwhichitusesadatasetof176,000imageratings
providedbyhumansrangingfrom1to10,whereimagesratedas10areclassifiedasart
pieces.
• HumanPreferenceRewardModels: WeuseHPSv2[32]andPickScore[16],whichtake
asinputanimage-textpairandpredicthumanpreferenceforthegeneratedimage. HPSv2is
trainedbyfine-tuningCLIPmodelwithavastdatasetthatincludes798,090instancesof
humanpreferencerankingsamong433,760imagepairs,whilePickScore[16]istrainedby
fine-tuningCLIPmodelwith584,000examplesofhumanpreferences. Thesedatasetsare
amongthemostextensiveinthefield,offeringasolidfoundationforenhancingimage-text
alignment.
• ObjectRemoval: WedesignarewardmodelbasedonYOLOS[8],aVisionTransformer
basedobjectdetectionmodeltrainedon118,000annotatedimages. Ourrewardisoneminus
theconfidencescoreofthetargetobjectcategory,fromwhichvideomodelslearnstoremove
thetargetobjectcategoryfromthevideo.
• VideoActionClassification: Whiletheaboverewardmodelssitonindividualimages,we
employarewardmodelthattakesinthewholevideoasinput. Thiscanhelpwithgetting
gradientsforthetemporalaspectofvideogeneration. Specifically,weconsiderVideoMAE
8[29],whichisfine-tunedforthetaskofactionclassificationonKineticsdataset[15]. Our
rewardistheprobabilitypredictedbytheactionclassifierforthedesiredbehavior.
• TemporalConsistencyviaV-JEPA:Whileactionclassificationmodelsarelimitedtoa
fixedsetofactionlabels,hereweconsideramoregeneralrewardfunction. Specifically,we
useself-supervisedmaskedpredictionobjectiveasarewardfunctiontoimprovetemporal
consistency. Specifically,weuseV-JEPA[1]asourrewardmodel,wheretherewardisthe
negativeofthemaskedautoencodinglossintheV-JEPAfeaturespace. Notethatweemploy
exactlythesamelossobjectivethatV-JEPAusesintheirtrainingprocedure.
Before VADER (Ours)
Open-Sora
“a man in a trendy suit taking a selfie in a city square, surrounded by modern buildings and a fountain.”
“Abear enjoying a slice of cake at a picnic.”
ModelScope
“A shark riding a bike.”
“A bear playing chess.”
Figure6: AligningOpen-Sora1.2andModelScopewithVADER.Theleftcolumnshowsresults
fromthebasemodels,whileresultsfromVADERaredemonstratedontheright. Thefirsttworows
useOpen-Soraasthebasemodel,andthelasttworowsuseModelScope. Therewardmodelsapplied
arePickScoreinthefirstrow,HPSv2.1inthesecondrow,HPSv2inthethirdrow,andtheAesthetic
rewardmodelinthelastrow.
Prompts. Weconsiderthefollowingsetofpromptdatasetsforrewardfine-tuningoftext-to-video
andimage-to-videodiffusionmodels.
• ActivityPrompts(Text): WeconsidertheactivitypromptsfromtheDDPO[2]. Each
promptisstructuredas"a(n)[animal][activity],"usingacollectionof45familiaranimals.
Theactivityforeachpromptisselectedfromatrioofoptions: "ridingabike","playing
chess",and"washingdishes".
• HPSv2 Action Prompts (Text): Here we filter out 50 prompts from a set of prompts
introducedintheHPSv2datasetfortext-imagealignment. Wefilterpromptssuchthatthey
containactionormotioninformationinthem.
• ChatGPTCreatedPrompts(Text): WepromptChatGPTtogeneratesomevividand
creativelydesignedtextdescriptionsforvariousscenarios,suchasbooksplacedbesidecups,
animalsdressedinclothing,andanimalsplayingmusicalinstruments.
• ImageNetDogCategory(Image): Forimage-to-videodiffusionmodel,weconsiderthe
imagesintheLabradorretrieverandMaltesecategoryofImageNetasoursetofprompts.
9• StableDiffusionImages(Image): Hereweconsiderall25imagesfromStableDiffusion
onlinedemowebpage.
5.1 SampleandComputationalEfficiency
Trainingoflarge-scalevideodiffusionmodelsisdonebyasmallsetofentitieswithaccesstoalarge
amountofcomputing;however,fine-tuningofthesemodelsisdonebyalargesetofentitieswith
accesstoasmallamountofcomputing. Thus,itbecomesimperativetohavefine-tuningapproaches
thatboostbothsampleandcomputationalefficiency.
Inthissection,wecompareVADER’ssampleandcomputationalefficiencywithotherreinforcement
learningapproachessuchasDDPOandDPO.InFigure7,wevisualizetherewardcurvesduring
training,wherethex-axisintheupperhalfofthefigureisthenumberofrewardqueriesandtheone
inthebottomhalfistheGPU-hours. Ascanbeseen,VADERissignificantlymoreefficientinterms
ofsampleandcomputationthanDDPOorDPO.Thisismainlyduetothefactthatwesenddense
gradientsfromtherewardmodeltothediffusionweights,whilethebaselinesonlybackpropagate
scalarfeedback.
Aesthetics Text Alignment Action Prediction
0.32
6.5 0.8
0.30
6.0
0.6
5.5 0.28
0.4
5.0
0.26
4.5 0.2
0 500 1000 1500 0 500 1000 0 1000 2000 3000
Reward Queries Reward Queries Reward Queries
0.32
6.5 0.8
0.30
6.0
0.6
5.5 0.28
5.0 0.4
4.5 0.26 0.2
0 2 4 6 8 0 5 10 15 20 0 10 20 30 40
Compute (GPU-hours) Compute (GPU-hours) Compute (GPU-hours)
VADER (Ours) DPO DDPO
Figure7: Trainingefficiencycomparison: Top: SampleefficiencycomparisonwithDPOandDDPO.
Bottom: ComputationalefficiencycomparisonwithDPOandDDPO.ItcanbeseenthatVADER
startsconvergingwithinatmost12GPU-hoursoftraining,whileDPOorDDPOdonotshowmuch
improvement.
5.2 GeneralizationAbility
Aes(T2V) HPS(T2V) ActP Aes(I2V)
Method
Train. Test. Train. Test. Train. Train. Test.
Base 4.61 4.49 0.25 0.24 0.14 4.91 4.96
DDPO 4.63 4.52 0.24 0.23 0.21 N/A N/A
DPO 4.71 4.41 0.25 0.24 0.23 N/A N/A
Ours 7.31 7.12 0.33 0.32 0.79 7.83 7.64
Table1: RewardonPromptsintrain&test. Wesplitthepromptsintotrainandtestsets,suchthat
thepromptsinthetestsetdonothaveanyoverlapwiththeonesfortraining. WefindthatVADER
achievesthebestonbothmetrics.
10
erocS
citehtseA
erocS
citehtseA
erocS
2vSPH
erocS
2vSPH
ycaruccA
ssalC
ycaruccA
ssalCA desired property of fine-tuning is generalization, i.e. the model fine-tuned on a limited set of
promptshastheabilitytogeneralizetounseenprompts. Inthissection,weextensivelyevaluatethis
propertyacrossmultiplerewardmodelsandbaselines. Whiletrainingtext-to-video(T2V)models,
weuseHPSv2ActionPromptsinourtrainingset,whereasweuseActivityPromptsinourtestset.
WeuseLabradordogcategoryinourtrainingsetfortrainingimage-to-video(I2V)models,while
Maltesecategoryformsourtestset. Table1showcasesVADER’sgeneralizationability.
5.3 HumanEvaluation
Wecarriedoutastudytoevaluatehumanpreferences
via Amazon Mechanical Turk. The test consisted
Method Fidelity TextAlign
ofaside-by-sidecomparisonbetweenVADERand
ModelScope. Totesthowwellthevideossampled ModelScope 21.0% 39.0%
fromboththemodelsalignedwiththeirtextprompts, VADER(Ours) 79.0% 61.0%
weshowedparticipantstwovideosgeneratedbyboth
Table2: HumanEvaluationresultsforHPS
VADERandabaselinemethod,askingthemtoiden-
reward model, where the task is image-text
tifywhichvideobettermatchedthegiventext. For
alignment.
evaluating video quality, we asked participants to
compare two videos generated in response to the
sameprompt,onefromVADERandonefromabaseline,anddecidewhichvideo’squalityseemed
higher. Wegathered100responsesforeachcomparison. Theresults,illustratedinTable2,showa
preferenceforVADERoverthebaselinemethods.
5.4 QualitativeVisualization
Inthissection,wevisualizethegeneratedvideosforVADERandtherespectivebaseline. Weconduct
extensivevisualizationsacrossalltheconsideredrewardfunctionsonvariousbasemodels.
HPSRewardModel: InFigure3,wevisualizetheresultsbeforeandafterfine-tuningVideoCrafter
usingbothHPSv2.1andAestheticrewardfunctiontogetherinthetopthreerows. Beforefine-tuning,
theraccoondoesnotholdasnowball,andthefoxwearsnohat,whichisnotalignedwiththetext
description; however,thevideosgeneratedfromVADERdoesnotresultintheseinconsistencies.
Further,VADERsuccessfullygeneralizestounseenpromptsasshowninthethirdrowofFigure3,
wherethedog’spawislesslikeahumanhandthanthevideoontheleft. Similarimprovementscan
beobservedinvideosgeneratedfromOpen-SoraV1.2andModelScopeasshowninthesecondand
thirdrowsofFigure6.
AestheticRewardModel: InFigure3,inthetopthreerowswevisualizetheresultsbeforeand
afterfine-tuningModelScopeusingacombinationofAestheticrewardfunctionandHPSv2.1model.
Also,wefine-tuneModelScopeviaAestheticRewardfunctionanddemonstrateitsgeneratedvideo
inthelastrowinFigure6. WeobservethatAestheticfine-tuningmakesthegeneratedvideosmore
artistic.
PickScore Model: In the bottom three rows of Figure 3, we showcase videos generated by
PickScorefine-tunedVideoCrafter. VADERshowsimprovedtext-videoalignmentthanthebase
model. In the last row, we test both models using a prompt that is not seen during training time.
Further,videogeneratedfromPickScorefine-tunedOpen-SoraisdisplayedinthefirstrowofFigure
6.
ObjectRemoval: Figure5displaysthevideosgeneratedbyVideoCrafterafterfine-tuningusing
theYOLOS-basedobjectionremovalrewardfunction. Inthisexample,booksarethetargetobjects
forremoval. Thesevideosdemonstratethesuccessfulreplacementofbookswithalternativeobjects,
likeablanketorbread.
VideoActionClassification: InFigure8,wevisualizethevideogenerationofModelScopeand
VADER.Inthiscase,wefine-tuneVADERusingtheactionclassificationobjective,fortheaction
specifiedintheprompt. Fortheprompt,"Apersoneatingdonuts",wefindthatVADERmakesthe
humanfacemoreevidentalongwithaddingsprinklestothedonut. Earliergenerationsareoften
misclassifiedasbakingcookies,whichisadifferentactionclassinthekineticsdataset. Theaddition
11ModelScope VADER (Ours)
“A person playing
ofcolorsandsprinklestothedonutmakesitmoredistinguishablefromcookiesleadingtoahigher
Piano”
reward.
“A person eating
Donuts”
Figure8: Videoactionclassifiersasrewardmodel. WeuseVideoMAEactionclassificationmodel
asarewardfunctiontofine-tuneModelScope’sText-to-VideoModel. Weseethatafterfine-tuning,
VADERgeneratesvideosthatcorrespondbettertotheactions.
V-JEPA reward model: In Figure 9, we show results for increasing the length of the video
generated by Stable Video Diffusion (SVD). For generating long-range videos on SVD, we use
autoregressiveinference,wherethelastframegeneratedbySVDisgivenasconditioninginputfor
generatingthenextsetofimages. Weperformthreestepsofinference,thusexpandingthecontext
lengthofSVD bythreetimes. However, asone canseeinthe imagesborderedinred, afterone
stepofinference,SVDstartsaccumulatingerrorsinitspredictions. Thisresultsindeformingthe
teddybear,oraffectingtherocketinmotion. VADERusesV-JEPAobjectiveofmaskedencodingto
enforceself-consistencyinthegeneratedvideo. Thismanagestoresolvethetemporalandspatial
discrepancyinthegenerationsasshowninFigure9.
Stable Video
Diffusion
VADER
(Ours)
Stable Video
Diffusion
VADER
(Ours)
Figure 9: Improving temporal and spatial consistency of Stable Video Diffusion (SVD) Image-
to-Video Model. Given the leftmost frame as input, we use autoregressive inference to generate
3*Nframesinthefuture,whereNisthecontextlengthofSVD.However,thissuffersfromerror
accumulation,resultingincorruptedframes,ashighlightedintheredborder. WefindthatVADER
canimprovethespatio-temporalconsistencyofSVDbyusingV-JEPA’smaskedencodinglossasits
rewardfunction.
6 Conclusion
WepresentedVADER,whichisasampleandcomputeefficientframeworkforfine-tuningpre-trained
video diffusion models via reward gradients. We utilized various reward functions evaluated on
imagesorvideostofine-tunethevideodiffusionmodel. Wefurthershowcasedthatourframeworkis
agnostictoconditioningandcanworkonbothtext-to-videoandimage-to-videodiffusionmodels.
Wehopeourworkcreatesmoreinteresttowardsadaptingvideodiffusionmodels.
12References
[1] Bardes,A.,Garrido,Q.,Ponce,J.,Chen,X.,Rabbat,M.,LeCun,Y.,Assran,M.,Ballas,N.:
V-jepa: Latentvideopredictionforvisualrepresentationlearning(2023)
[2] Black, K., Janner, M., Du, Y., Kostrikov, I., Levine, S.: Training diffusion models with
reinforcementlearning.arXivpreprintarXiv:2305.13301(2023)
[3] Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y.,
English,Z.,Voleti,V.,Letts,A.,etal.: Stablevideodiffusion: Scalinglatentvideodiffusion
modelstolargedatasets.arXivpreprintarXiv:2311.15127(2023)
[4] Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image editing
instructions.In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.pp.18392–18402(2023)
[5] Chen,H.,Zhang,Y.,Cun,X.,Xia,M.,Wang,X.,Weng,C.,Shan,Y.: Videocrafter2: Overcom-
ingdatalimitationsforhigh-qualityvideodiffusionmodels(2024)
[6] Clark,K.,Vicol,P.,Swersky,K.,Fleet,D.J.: Directlyfine-tuningdiffusionmodelsondifferen-
tiablerewards.arXivpreprintarXiv:2309.17400(2023)
[7] Dong,H.,Xiong,W.,Goyal,D.,Zhang,Y.,Chow,W.,Pan,R.,Diao,S.,Zhang,J.,Shum,K.,
Zhang,T.: Raft: Rewardrankedfinetuningforgenerativefoundationmodelalignment(2023)
[8] Fang,Y.,Liao,B.,Wang,X.,Fang,J.,Qi,J.,Wu,R.,Niu,J.,Liu,W.: Youonlylookatone
sequence: Rethinkingtransformerinvisionthroughobjectdetection.CoRRabs/2106.00666
(2021),https://arxiv.org/abs/2106.00666
[9] Feng,W.,He,X.,Fu,T.J.,Jampani,V.,Akula,A.,Narayana,P.,Basu,S.,Wang,X.E.,Wang,
W.Y.: Training-freestructureddiffusionguidanceforcompositionaltext-to-imagesynthesis
(2023)
[10] Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P., Poole, B.,
Norouzi,M.,Fleet,D.J.,Salimans,T.: Imagenvideo: Highdefinitionvideogenerationwith
diffusionmodels(2022)
[11] Ho,J.,Jain,A.,Abbeel,P.: Denoisingdiffusionprobabilisticmodels.CoRRabs/2006.11239
(2020),https://arxiv.org/abs/2006.11239
[12] Ho,J.,Salimans,T.,Gritsenko,A.,Chan,W.,Norouzi,M.,Fleet,D.J.: Videodiffusionmodels
(2022)
[13] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora:
Low-rankadaptationoflargelanguagemodels(2021)
[14] Karras, T., Aittala, M., Aila, T., Laine, S.: Elucidating the design space of diffusion-based
generative models. Advances in Neural Information Processing Systems 35, 26565–26577
(2022)
[15] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F.,
Green,T.,Back,T.,Natsev,P.,etal.: Thekineticshumanactionvideodataset.arXivpreprint
arXiv:1705.06950(2017)
[16] Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., Levy, O.: Pick-a-pic: An open
datasetofuserpreferencesfortext-to-imagegeneration(2023)
[17] Lambert,N.,Pyatkin,V.,Morrison,J.,Miranda,L.,Lin,B.Y.,Chandu,K.,Dziri,N.,Kumar,
S.,Zick,T.,Choi,Y.,etal.: Rewardbench: Evaluatingrewardmodelsforlanguagemodeling.
arXivpreprintarXiv:2403.13787(2024)
[18] Lee,K.,Liu,H.,Ryu,M.,Watkins,O.,Du,Y.,Boutilier,C.,Abbeel,P.,Ghavamzadeh,M.,Gu,
S.S.: Aligningtext-to-imagemodelsusinghumanfeedback(2023)
13[19] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3:
Zero-shotoneimageto3dobject.In: ProceedingsoftheIEEE/CVFInternationalConference
onComputerVision.pp.9298–9309(2023)
[20] Prabhudesai, M., Goyal, A., Pathak, D., Fragkiadaki, K.: Aligning text-to-image diffusion
modelswithrewardbackpropagation.arXivpreprintarXiv:2310.03739(2023)
[21] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,
A.,Mishkin,P.,Clark,J.,etal.: Learningtransferablevisualmodelsfromnaturallanguage
supervision.In: Internationalconferenceonmachinelearning.pp.8748–8763.PMLR(2021)
[22] Rafailov,R.,Sharma,A.,Mitchell,E.,Manning,C.D.,Ermon,S.,Finn,C.: Directpreference
optimization: Yourlanguagemodelissecretlyarewardmodel.AdvancesinNeuralInformation
ProcessingSystems36(2024)
[23] Sauer,A.,Boesel,F.,Dockhorn,T.,Blattmann,A.,Esser,P.,Rombach,R.: Fasthigh-resolution
imagesynthesiswithlatentadversarialdiffusiondistillation.arXivpreprintarXiv:2403.12015
(2024)
[24] Schuhmann, C.: Laoin aesthetic predictor (2022), https://laion.ai/blog/
laion-aesthetics/
[25] Schulman,J.,Wolski,F.,Dhariwal,P.,Radford,A.,Klimov,O.: Proximalpolicyoptimization
algorithms.arXivpreprintarXiv:1707.06347(2017)
[26] Sohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,Ganguli,S.: Deepunsupervisedlearning
usingnonequilibriumthermodynamics(2015)
[27] Song,J.,Meng,C.,Ermon,S.: Denoisingdiffusionimplicitmodels(2022)
[28] Tallec, C., Ollivier, Y.: Unbiasing truncated backpropagation through time. arXiv preprint
arXiv:1705.08209(2017)
[29] Tong,Z.,Song,Y.,Wang,J.,Wang,L.: Videomae: Maskedautoencodersaredata-efficient
learners for self-supervised video pre-training. Advances in neural information processing
systems35,10078–10093(2022)
[30] Wallace,B.,Dang,M.,Rafailov,R.,Zhou,L.,Lou,A.,Purushwalkam,S.,Ermon,S.,Xiong,
C.,Joty,S.,Naik,N.: Diffusionmodelalignmentusingdirectpreferenceoptimization.arXiv
preprintarXiv:2311.12908(2023)
[31] Wang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X., Zhang, S.: Modelscope text-to-video
technicalreport.arXivpreprintarXiv:2308.06571(2023)
[32] Wu,X.,Hao,Y.,Sun,K.,Chen,Y.,Zhu,F.,Zhao,R.,Li,H.: Humanpreferencescorev2: A
solidbenchmarkforevaluatinghumanpreferencesoftext-to-imagesynthesis.arXivpreprint
arXiv:2306.09341(2023)
[33] Xu,J.,Liu,X.,Wu,Y.,Tong,Y.,Li,Q.,Ding,M.,Tang,J.,Dong,Y.: Imagereward: Learning
andevaluatinghumanpreferencesfortext-to-imagegeneration(2023)
[34] Yuan,H.,Zhang,S.,Wang,X.,Wei,Y.,Feng,T.,Pan,Y.,Zhang,Y.,Liu,Z.,Albanie,S.,Ni,
D.: Instructvideo: Instructing video diffusion models with human feedback. arXiv preprint
arXiv:2312.12490(2023)
[35] Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., You, Y.: Open-
sora: Democratizingefficientvideoproductionforall(March2024),https://github.com/
hpcaitech/Open-Sora
14