Uncertainty Estimation of Large Language Models in Medical
Question Answering
JiaxinWu
TheUniversityofHongKong
lisa24@connect.hku.hk
YizhouYu
TheUniversityofHongKong
yizhouy@acm.org
Hong-YuZhou
TheUniversityofHongKong&HarvardMedicalSchool
whuzhouhongyu@gmail.com
Abstract
LargeLanguageModels(LLMs)showpromisefornaturallanguagegener-
ationinhealthcare,butriskhallucinatingfactuallyincorrectinformation.
DeployingLLMsformedicalquestionansweringnecessitatesreliableun-
certaintyestimation(UE)methodstodetecthallucinations. Inthiswork,
webenchmarkpopularUEmethodswithdifferentmodelsizesonmedical
question-answeringdatasets. Ourresultsshowthatcurrentapproaches
generallyperformpoorlyinthisdomain,highlightingthechallengeofUE
formedicalapplications. Wealsoobservethatlargermodelstendtoyield
betterresults,suggestingacorrelationbetweenmodelsizeandthereliabil-
ityofUE.Toaddressthesechallenges,weproposeTwo-phaseVerification,a
probability-freeUncertaintyEstimationapproach. First,anLLMgenerates
astep-by-stepexplanationalongsideitsinitialanswer,followedbyformu-
latingverificationquestionstocheckthefactualclaimsintheexplanation.
Themodelthenanswersthesequestionstwice: firstindependently,and
thenreferencingtheexplanation. Inconsistenciesbetweenthetwosetsof
answersmeasuretheuncertaintyintheoriginalresponse. Weevaluateour
approachonthreebiomedicalquestion-answeringdatasetsusingLlama2
Chatmodelsandcompareitagainstthebenchmarkedbaselinemethods.
TheresultsshowthatourTwo-phaseVerificationmethodachievesthebest
overallaccuracyandstabilityacrossvariousdatasetsandmodelsizes,and
itsperformancescalesasthemodelsizeincreases.
1 Introduction
LargeLanguageModels(LLMs),suchasGPT-4andLlama2,havedemonstratedconsid-
erablepotentialingeneratinghuman-liketextacrossabroadspectrumoffields,without
additionaldomain-specifictraining. Theircapabilitiescanbeharnessedtoprovideassis-
tanceinthehealthcaresectorforawiderangeofapplications,includingbutnotlimited
todiseasediagnosis,clinicaldecision-making,andpatientcommunication(Cascellaetal.,
2023). Despitethepotential,thedeploymentofLLMsfaceschallenges. Aprevalentconcern
isthetendencyofLLMsto‘hallucinate’,atermusedtodescribecircumstanceswherethe
modelgeneratesplausibleyetincorrectinformation,particularlywhentheyarenotable
toprovideanaccurateresponse(Jietal.,2023). Inhigh-riskscenariossuchashealthcare,
wheredecisionscanhavedirectimpactonhumanlives,ensuringthereliabilityofLLMs
becomescritical. Thisunderscorestheneedforeffectiveapproachestoaccuratelyestimate
theuncertaintyofgeneratedresponsesanddetectinstancesofhallucination.
4202
luJ
11
]LC.sc[
1v26680.7042:viXraInmedicalsettings,existingmethodsforquantifyinguncertainty,includingentropy-based
methods (Kadavath et al., 2022; Kuhn et al., 2023) and fact-checking (Guo et al., 2022;
Shusteretal.,2021),havedemonstratedcertainlimitations. Entropy-basedmethodsoperate
ontheassumptionthatamodel,whenconfidentinitsanswer,generatesadistributionof
responseswithasmallentropy. Onthecontrary,ifthemodelisunsure,itmighthallucinate
andproduceadiverserangeofresponses,thusincreasingtheentropy (Kadavathetal.,
2022). However,withinthecomplexityofthemedicaldomain,themodelcanoftenfabricate
untruthfulinformationwithahighlevelofconfidence. Thisresultsinamisleadinglylow
entropy, which fails to accurately represent the uncertainty embedded in the generated
response. Fact-checking,anothercommonapproachforuncertaintyestimation,validates
thegeneratedresponsesbycomparingthemwithrelevanttruthretrievedfromanexternal
knowledgedatabase. However,thismethodencounterslimitationsduetothescarcityof
comprehensiveandprofessionalmedicalknowledgebases.
In this report, we benchmark several popular methods using different model sizes and
datasetstoestablishacomparativeunderstandingoftheirperformance. Thesebenchmarks
revealthechallengesofuncertaintyestimationinmedicalquestion-answering. Wealsopro-
poseTwo-phaseVerification,aprobability-freeapproachbasedontheChain-of-Verification
(CoVe)concept (Dhuliawalaetal.,2023). Thisapproachoperatesindependentlyoftoken-
levelprobabilitiesandthuscanbeappliedtoblack-boxmodels.First,themodelgeneratesan
explanationalongsideitsinitialanswer. Next,itformulatesverificationquestionstargeting
theexplanation,towhichitprovidesindependentanswers. Two-phaseVerificationrefines
CoVe’s inconsistency check process by prompting the model to answer the verification
questionsagain,usingthestatementinquestionasareference. Theinconsistenciesbetween
thetwosetsofanswersserveasameasureofuncertaintyintheanswer. Theworkflowsof
CoVeandTwo-phaseVerificationarevisualizedinFigures1aand1b,respectively.
(a)Chain-of-Verification(CoVe)methodforUncertaintyEstimation
(b)Two-phaseVerificationmethodforUncertaintyEstimation
Figure1: ComparisonofCoVeandTwo-phaseVerificationMethods
2 RelatedWork
2.1 Entropy-basedmethods
LargeLanguageModelsgenerateoutputonatoken-by-tokenbasisbasedonthesequence
sofar. Tokenprobabilitiesareadirectmeasureofamodel’sconfidenceinitsnext-tokenpre-
diction. Xiao&Wang(2021)explorethelinkbetweenhallucinationinconditionallanguagegenerationtasksandpredictiveuncertainty,whichisquantifiedusingentropymeasuresof
thetokenprobabilitydistributions. Theyfindthathigherlevelsofpredictiveuncertainty,
especiallyepistemicuncertaintyoriginatingfromthemodel’sknowledgegaps,correlate
withanincreasedpropensityforhallucinatoryoutputs. Duanetal.(2023)addressesthe
challengeoftoken-levelgenerativeinequalitybyamethodtermedShiftingAttentionto
Relevance(SAR),whichreassignsattentiontomoresemanticallyrelevantcomponentswhen
estimatinguncertainty. Malinin&Gales(2021)introduceinformation-theoreticuncertainty
measuresatboththetokenandsequencelevelsandproposeanovelmetric,reversemu-
tualinformation,forstructureduncertaintyassessment,utilizingensemblemethodslike
Monte-CarloDropoutandDeepEnsembles. Kuhnetal.(2023)proposesemanticentropy,
whichmeasuresuncertaintyovermeaningsratherthanjustsequencesofwords. Theirunsu-
pervisedmethodclusterssemanticallyequivalentgenerationsandcalculatesthepredictive
entropy of theresulting probability distribution overthese clusters. A concurrent work
of Wang et al. (2024) calibrates entropy-based uncertainty at word and sequence levels
accordingtotheirsemanticrelevance,aimingtoaddressthegenerativeinequalitychallenge.
2.2 Self-assessmentmethods
LLMspossesstheinherentpotentialtoreflectontheiroutputs;however,theself-evaluation
maynotberobustasLLMsareinclinedtofindtheirowncontentcredible(Kadavathetal.,
2022). To enhance the calibration and confidence estimation of LLMs, researchers have
developedtechniquesincludingfine-tuningandprompting. Linetal.(2022)finetuneGPT-3
bysupervisedlearningtoexpressitsuncertaintyinnaturallanguage. Theirexperiment
demonstratesthatGPT-3canbetrainedtoprovideanswersalongwithacorresponding
confidencelevel. Similarly, Kadavathetal.(2022)investigatetheself-awarenessofLLMs
by training them to estimate the likelihood that their generated responses are correct.
Theirresearchrevealsthattheeffectivenessofself-evaluationimproveswithmodelsize
and few-shot prompting. Additionally, the process benefits from presenting the model
withvariousanswersamplesbeforeaskingittoassessthevalidityofasingleproposed
response. Kojima et al. (2023) explore the zero-shot capabilities of LLMs, finding that
chain-of-thoughtpromptingbooststhereasoningabilitiesofLLMs,especiallyinarithmetic
tasks. ByinstructingLLMstogenerateintermediatestepsexplicitlybeforeansweringthe
questions, a simple prompt templateprovidesperformance gain. Manakulet al. (2023)
introduceazero-resourcemethodologyforLLMstoself-checkhallucinatedgenerations
based on the hypothesis that hallucinations tend to diverge. It operates by generating
multipleresponsestoapromptandthenassessingthefactualconsistencybetweenthese
responses.
2.3 Externaltools
Sinceknowledgegapsareacommoncauseofhallucinations,externalknowledgeretrieval
isoftenutilizedtomitigatehallucinationsandproducemorefaithfulgenerations (Heetal.,
2022; Shuster et al., 2021). For example, Chern et al. (2023) collect external evidence to
validatethefactualityofclaimsextractedfromtheLLMoutput. Whilepromptingstrategies
enhanceLLMperformanceincertaintasks,plausibleexplanationsareoftenprovidedeven
whenthefinalansweriswrong (Kojimaetal.,2023).Toovercomethislimitation, Chenetal.
(2023)proposeamulti-turnconversationframeworktointegratepromptingandexternal
toolsincludingcalculatorsandsearchengines,whichreducesthemistakesmadebyLLMs
andenhancestheaccuracyincomplexreasoningtasks.
3 Methodology
Inthissection,weelaborateonourapproachtoestimatinggenerationuncertainty,which
leverages the idea from the Chain-of-Verification (CoVe) framework. Our approach is
inspiredbythefoundationalworkof Dhuliawalaetal.(2023)andextendsitbyintegrating
ameasureforconfidencelevelbasedondiscoveredinconsistencies. Theprimarygoalisto
identifytheoccurrenceofpossiblehallucinationsbyincorporatingarobust,unsupervised
verificationmechanismthatoperatesindependentlyofthemodel’sinitialoutputs.3.1 Generatestep-by-stepexplanation
Foreachquestion,theLLMisrequiredtogenerateadefinitiveanswerfollowedbyastep-by-
stepexplanation. Weperformtheexperimentontwotypesofquestions: thosethatrequirea
ternaryresponse(affirmative,negative,oruncertain)andthosethatpresentmultiple-choice
options. Thedefinitiveanswerwillbeintheformof”yes,””maybe,”or”no”forthefirst
typeofquestions,oraselectionfromthemultiple-choiceoptionsforthesecondtype. This
isfollowedbygeneratingadetailedstep-by-stepexplanationforthechosenanswer,which
iscriticalforthesubsequentverificationchain. Thestep-by-stepbreakdownconvertsthe
model’sreasoningintodiscreteunitsthatcanbeindependentlyverifiedfortruthfulness
andconsistency,therebyenablinganestimationoftheoverallconfidenceintheresponse.
3.2 Planverification
Upongeneratingtheinitialanswerandstep-by-stepexplanation,themodelproceedstofor-
mulateasetofverificationquestions,witheachonetargetingasinglestepintheexplanation.
Thesequestionsarepurposefullydesignedtochallengetheaccuracyofparticularfactual
claimswithintheindividualstepsoftheexplanation. Theobjectiveofthesequestionsisto
verifythetruthfulnessofeachassertionwithoutnecessitatingsupplementaryknowledgeor
additionalcontextfortheirresolution. Forexample,inresponsetothestatement”Ringed
sideroblastsareacharacteristicfeatureofironoverload,particularlyinthebonemarrow”,apotential
verificationquestioncouldbe“Whatconditionareringedsideroblaststypicallyindicativeof?”
Thisquestiondirectlytargetsthefactualclaimmadewithinthestatementandisstructured
toelicitaresponsethateitherconfirmsorrefutestheaccuracyoftheoriginalstatement.
Whilethemodeliscapableofformulatingreasonableverificationquestionsonazero-shot
instruction,theincorporationofafew-shotpromptsignificantlyrefinesthisprocedureby
enhancingtheefficacyoftheverificationquestions. Afew-shotpromptpresentsthemodel
withasetofcarefullycuratedexemplarypairsofstatementsandcorrespondingverification
questions. Theseexamplesserveasatemplate,showcasingthestructureandpurposeofa
well-craftedverificationquestion. Consequently,thisfew-shotpromptempowersthemodel
toformulatequestionsthatarenotonlyrelevantbutalsoincisiveintheirabilitytodiscern
andtestthevalidityoffactualassertions.
3.3 Executeverification
Giventheverificationquestions,inthenextstep,themodelexecutestheverificationpro-
ceduretoself-checkwhethertheexplanationisaccurate. Weexaminedseveraldifferent
approachesforverificationinourexperiment.
3.3.1 Stepverification
Asabasefortheverificationprocedure,weusethemodeltodirectlyassessthetruthfulness
andtheconsistencyrelatedtothepreviousstepsofeachsentenceintheexplanationwithout
utilizingtheverificationquestions. Foreachsentence,themodelispromptedtodetermine
itstruthfulnessbasedonthepriorsentencesintheexplanation,classifyingitastrueorfalse.
Thisservesasabaselinemeasurementofthemodel’sabilitytoself-validateitscontent.
Thisdirectapproachassumesthatthelanguagemodelisintrinsicallycapableofrecogniz-
ingfactualinformation. Itprovidesastraightforwardvalidationmechanismwithoutthe
additionallayerofcomplexityintroducedbyverificationquestions.
3.3.2 CoVe
Inthisapproach,theLLManswerstheverificationquestionsindependentlytoavoidthe
influenceoftheinitialoutput. Next,theindependentanswerwillbecheckedagainstthe
original statement being examined for consistency. This is performed by providing the
modelwithboththeanswerandthestatementandaskingittodecideiftheyareconsistent
ornot.Theassumptionforthisapproachisthatthemodelislesslikelytorepeatanyhallucinations
presentintheinitialexplanationwhenansweringtheverificationquestionsindependently
withoutanycontext. Iftheindependentresponsealignswiththeexplanation,thecorre-
spondingstatementhasalowerpossibilityofbeingahallucination. Onthecontrary,an
inconsistencybetweenthetwoindicatesapotentialerrororhallucinationintheexplanation,
makingtheinitialanswerlessplausible.
3.3.3 Two-phaseverification
Inthismoresophisticatedapproach, themodelispromptedtoanswereachverification
questiontwice. First,themodelanswerstheverificationquestionindependently,asinthe
previous approach. Next, the model is given the statement to be verified as the context
and prompted to answer the verification question again. To evaluate whether the two
answersareconsistent,weadoptamethodforcheckingsemanticequivalencywhichuses
a Deberta-large model (He et al., 2021) for a bidirectional entailment check (Kuhn et al.,
2023). Thisprocessinvolvesappendingaspecialtokenbetweentheanswersandevaluating
whether each answer can be inferred from the other, with equivalence determined by
mutual”entailment”classificationsbythemodel. AnexampleoftheTwo-phaseVerification
procedureisillustratedinFigure2.
Therationaleforintegratingthesecondverificationquestionansweringsteprespondsto
twosignificantchallengesencounteredduringtheconsistencycheckinCoVe:
1. Ambiguity in Consistency Checks: The instructions for a consistency check can
themselves be ambiguous due to the different interpretations of “consistency”.
Additionally, themodelmayfixateonsuperficiallinguisticpatternsratherthan
theunderlyingfactualcontent. Variousphrasingsconveyingthesamemeaning
maynotberecognizedasconsistentbythemodel,leadingtofalsejudgmentsin
identifyingconsistency.
2. RelevanceandInformationDiscrepancies: Theindependentanswergeneratedby
themodelcouldintroduceadditionalinformationthatisnotstrictlyrelevanttothe
initialexplanation,oritcouldomitcrucialdetails,makingitdifficulttoaccurately
assessconsistency. Theanswermightbefactuallycorrectinitselfbutstillnotalign
perfectlywiththeexplanationduetodifferencesinscopeordetaillevel.
3.4 Uncertaintyquantification
Followingthecompletionoftheverificationsteps,wetranslatethefindingsintoameasur-
ableindicatorofuncertainty.Thisinvolvescountingthestatementsidentifiedasinconsistent
intheverificationphase,relativetotheoverallnumberofstatementsintheprovidedex-
planation. Toexpressthisquantitatively,wecomputetheUncertaintyLevel(UL)usingthe
formulabelow:
NumberofInconsistentStatements
UL =
TotalNumberofStatementsinExplanation
4 Experiment
4.1 Experimentalsetting
Models We conduct the experiment on Llama 2 Chat (Touvron et al., 2023), which is a
collectionofopen-sourcechatmodelsfine-tunedforoptimizeddialogueusecases. Llama2
Chat(7b)andLlama2Chat(13b)wereexaminedinourexperiment.
DatasetsWeconsiderthreebiomedicalquestion-answering(QA)datasets: PubMedQA(Jin
etal.,2019),MedQA(Jinetal.,2021)andMedMCQA(Paletal.,2022). PubMedQAisa
biomedicalresearchQAdatasetdesignedtoanswerquestionswithayes/no/maybeformat.
Eachquestioncomeswithacontextextractedfromthecorrespondingabstractofaresearch
paperandchallengesmodelstoreasonoverquantitativebiomedicalcontent. Theexpert-
annotatedquestionswereutilizedinourexperiment. MedQAisafree-formmultiple-choiceFigure2: IllustrationofTwo-phaseVerificationprocesswithanexamplequestion
QAdatasetcollectedfromquestionsinprofessionalmedicalboardexaminations,suchas
theUnitedStatesMedicalLicensingExamination(USMLE).MedMCQAisalarge-scale
multiple-choiceQAdatasetderivedfromreal-worldmedicalentranceexamquestions. Itis
designedtotestavarietyofreasoningabilitiesacrossawiderangeofmedicalsubjectsand
topics.
BaselinesWeconsider4baselinemethodsinourexperiments,includingLexicalSimilarity
(LS)(Fomichevaetal.,2020),SemanticEntropy(SE)(Kuhnetal.,2023),PredictiveEntropy
(PE)(Kadavathetal.,2022)andLength-normalizedEntropy(LE)(Malinin&Gales,2021).
LexicalSimilarityamongasetofgeneratedtextsisquantifiedbycomputingtheaverage
ROUGE-L score, and a higher similarity score indicates that the model is more certain
in its responses. Semantic Entropy addresses the difficulty of semantic equivalence in
the uncertainty estimation of free-form LLMs by clustering generations with the same
semanticmeaningsandcalculatingcluster-wiseentropy. PredictiveEntropyisestimatedby
averagingthesumofnegativelogprobabilitiesofeachtokeninthesampledanswersfora
givenquestion. Length-normalizedEntropydividesthesumofnegativelogprobabilitiesTable1:AUROCresultsforvariousuncertaintyestimationmethodsacrossmultipledatasets
and model sizes. Methods include Lexical Similarity (LS), Semantic Entropy (SE), Pre-
dictiveEntropy(PE),Length-normalizedEntropy(LE),StepVerification(Step),Chain-of-
Verification(CoVe)andTwo-phaseVerification(Two-phase). Resultsareshownfortwo
modelsizes: Llama2Chat(7b)andLlama2Chat(13b),evaluatedonPubMedQA,MedQA,
andMedMCQAdatasets. ThehighestAUROCscoreforeachmodel-datasetcombination
and the overall best results for averages and standard deviations (SDs) across datasets
are highlighted in bold. For entropy-based methods, 5 answers are generated for each
question,andthetemperatureissetto0.5,whichoptimizedSemanticEntropy(SE)and
Length-normalizedEntropy(LE)(Kuhnetal.,2023).
LS1 SE2 PE3 LE4 Step CoVe5 Two-phase(Ours)
Llama2Chat(7b)
PubMedQA 0.5277 0.6320 0.6322 0.6028 0.6288 0.6866 0.6132
MedQA 0.4871 0.5154 0.5189 0.5224 0.4170 0.4861 0.5553
MedMCQA 0.3837 0.4676 0.5028 0.6013 0.5178 0.5509 0.5304
Average 0.4662 0.5383 0.5513 0.5755 0.5212 0.5745 0.5663
SD 0.0742 0.0846 0.0705 0.0460 0.1059 0.1023 0.0425
Llama2Chat(13b)
PubMedQA 0.5551 0.5689 0.5681 0.4503 0.5085 0.5352 0.5906
MedQA 0.4860 0.4898 0.4010 0.5077 0.5934 0.5408 0.6460
MedMCQA 0.5142 0.5247 0.5708 0.5933 0.4895 0.6026 0.5793
Average 0.5184 0.5278 0.5133 0.5171 0.5305 0.5595 0.6053
SD 0.0347 0.0396 0.0973 0.0720 0.0553 0.0374 0.0357
Overallaverage 0.4923 0.5331 0.5323 0.5463 0.5258 0.5670 0.5858
OverallSD 0.0592 0.0593 0.0788 0.0628 0.0758 0.0694 0.0411
ofasequencebyitslength,handlingtheissueofdisproportionatecontributiontothetotal
entropyduetovariablesentencelength.
MetricsFollowingKuhnetal.(2023),weevaluatetheperformanceofouruncertaintyesti-
mationapproachusingtheareaunderthereceiveroperatingcharacteristiccurve(AUROC)
asourmetric. Themetricmeasurestheprobabilitythatarandomlychosencorrectanswer
hasaloweruncertaintylevelcomparedtoarandomlychosenincorrectanswer.
4.2 Results
WecompareTwo-phaseVerificationwithseveralbaselinemethodsonthreemedicaldatasets
usingtwoLlama2Chatmodels. TheresultsaresummarizedinTable1andFigure3.
LexicalSimilarity(LS),whichassessesuncertaintybasedontheoverlapamongsamplere-
sponses,showsthelowestoverallaverageAUROC.Thissuggeststhatlexicalresemblances
are insufficient indicators of certainty in the generated text, where semantic meaning is
crucial. SemanticEntropy(SE)andPredictiveEntropy(PE)demonstratemoderateimprove-
ments over LS. The two methods achieve similar AUROC scores, as they both estimate
uncertaintyfromtheentropyofsampleresponses. SEhasslightlybetteroverallresultsthan
PE,indicatingthatsemanticclusteringisaneffectivestrategyinentropy-basedmethods.
Length-normalizedEntropy(LE)achievesthehighestaverageAUROCfortheLlama2Chat
1LexicalSimilarity(Fomichevaetal.,2020)
2SemanticEntropy(Kuhnetal.,2023)
3PredictiveEntropy(Kadavathetal.,2022)
4Length-normalizedEntropy(Malinin&Gales,2021)
5Chain-of-Verification(Dhuliawalaetal.,2023)Figure3: PerformancecomparisonofUEmethodsondifferentmodelsizes
(7b)model,suggestingthatnormalizingentropybyanswerlengthprovidesamorereliable
uncertaintysignalforsmallermodels. However,LE’sperformancedoesnotconsistently
holdacrosslargermodelsizesoralldatasets,indicatingthatwhilelengthnormalizationis
beneficial,itisnotacomprehensivesolutionforUE.
StepVerification,asthemoststraightforwardverificationmethodthatreliessolelyonthe
model’sself-validationability,doesnotshowaperformanceimprovementcomparedto
otherbaselinemethods. Interestingly,forthe7bmodel,theperformanceofStepVerification
appearstobecorrelatedwiththeaccuracyofthemodel’sanswersoneachdataset. Pub-
MedQA,whichhasthehighestansweraccuracy(0.65),alsoshowsthebestStepVerification
result,whileMedQAandMedMCQA,withloweraccuracies(0.2991and0.3429,respec-
tively),havepoorerStepVerificationperformance. Thisobservationsuggeststhatsmaller
modelsmayexhibitoverconfidenceintheirgeneratedanswersandstruggletoidentifytheir
ownmistakesduringself-verification.Thislimitationhighlightsthenecessityofintroducing
averificationchaintohelpthemodelrecognizehallucinationsinitsoutputs.
CoVeslightlyoutperformsTwo-phaseVerificationinsomecasesbuthasahighstandard
deviation,particularlywithsmallermodelsizes. Thismaystemfromthevariabilityinthe
qualityofindependentlygeneratedanswers,assmallermodelsmightproducelessfaithful
responses. Further research could explore ways to improve the reliability of answering
verificationquestions,potentiallybyintegratingexternalknowledgebases.
Ingeneral,Two-phaseVerificationdemonstratesthebestoverallperformance,achievingthe
highestAUROCinhalfofthemodel-datasetcombinationsandthehighestaverageAUROC
acrossallexperiments. Italsoexhibitsstableperformancewiththelowestoverallstandard
deviation, unlike other methods such as CoVe, which show performance fluctuations
in certain scenarios. Moreover, the scalability of Two-phase Verification is particularly
noteworthywhencomparingtheAUROCresultsforLlama2Chat(7b)andLlama2Chat
(13b). Whilemostmethodsshowonlymodestimprovementsor,insomecases,adecreasein
performancewiththelargermodelsize,Two-phaseVerificationnotonlyimprovesbutalso
doessoatahigherratethanitscounterparts. Thesecharacteristicssuggestthemethod’s
potentialtoprovidereliableuncertaintyestimationacrossvariousdatasetsandtoscalewith
largermodelsizes.
5 Discussion
5.1 UncertaintyEstimationinmedicalQA
UncertaintyEstimationisofparamountimportanceinthemedicaldomain,whereuntruth-
fulinformationcanleadtosevereconsequences. InmedicalapplicationsutilizingLLMs,
suchasAImedicalchatbots,itiscrucialtoassessthetrustworthinessofmodeloutputsto
ensurepatientsafety. Inthecaseswherethemodelislesscertaininitspredictions,theusershouldbealertedandadvisedtoseekfurtherverificationorexpertopinionbeforefollowing
themodel’ssuggestions.
ThefindingsofourempiricalstudycontributetotheliteratureonUEofLLMs,particularly
inthecontextofmedicalquestionanswering,whichhasbeenlessstudied.Previousresearch
hasprimarilyfocusedonUEfromastatisticalperspective,hypothesizingthatthemodel
intrinsicallyknowswhenitisuncertainaboutananswer,leadingtohighervariabilityin
itsoutputs. However,professionalmedicalknowledgeisoftenunderrepresentedinthe
trainingdata,whichcanleadtothemodelgeneratingresponsesconfidentlyevenwhen
itishallucinating. Asaresult,answersmayexhibitlowentropy,falselysuggestinghigh
certainty.
Our Two-phase Verification method mitigates these issues by independently verifying
responses, thus providing an effective measure of a model’s certainty without needing
token-levelprobabilities. Thisisespeciallyusefulforblack-boxmodels,wherearchitectural
detailsareinaccessible. Moreover,thescalabilityoftheTwo-phaseVerificationmethodisa
criticalaspectforfutureapplications. Aslarge-scalemodelscontinuetoevolve,theability
tomaintainandevenenhanceperformancewithincreasedmodelsizeisessential.
TheconceptofChain-of-Verification(CoVe)hasbeenpreviouslyproposedtoreducehallucina-
tionsandthenself-correctthemtogeneratemorefactualstatements(Dhuliawalaetal.,2023).
However,tothebestofourknowledge,thisconcepthasnotbeenexploredforuncertainty
estimation. Ourworkdemonstratestheeffectivenessofintegratingaverificationchainfor
uncertaintyestimationinmedicalquestion-answering, openingupnewpossibilitiesfor
futureresearchinthisdirection.
5.2 Limitationsandfuturework
VerificationquestiongenerationAcriticalstageofTwo-phaseVerificationistogenerate
verificationquestionsthateffectivelychallengetheinitialexplanation. Asexplanationpara-
graphsaregeneratedwithlinguisticcoherence,sentencesoftenusepronounsorreferences
that rely on previous sentences. When verification questions are derived from discrete
sentences,themodelmaymissessentialcontext. Thus,theverificationquestionsmightnot
alwaysincisivelyinterrogatethekeyinformationpresented. Althoughfew-shotprompts
aidquestionformulation,theycaninadvertentlyinhibittheLLM’screativity,confiningit
tothepatternsseenintheseexamples. Infuturework,itwillbeessentialtoenhancethe
generationofverificationquestionstobemorecontext-awareandadaptable.
DomainknowledgeconstraintsAnotherconstraintforTwo-phaseVerificationistheknowl-
edgecapacityofthelanguagemodel,whichdirectlyaffectsthequalityoftheanswersto
verificationquestions. Llama2Chat,asageneral-purposelanguagemodel,possessesonlya
broadunderstandingofmedicalknowledge,lackingthedepthrequiredforspecializedareas.
Toimprovethemodel’sresponsestoverificationquestions,weintegratedenseretrieval
techniquestosourcerelevantinformationfromexternaldatabaseslikeWikipedia. However,
thismethodfallsshortastheretrievedresultsfrequentlyhavelowrelevancescorestothe
verificationqueriesandfailtoprovidethenecessaryknowledge. Futureimprovements
shouldfocusonretrievingrelevantinformationfromprofessionalmedicaldatasets,suchas
researchpapers,medicaltextbooks,andexpert-curatedknowledgebases. Byleveraging
thesedomain-specificresources,themodelcangeneratemoreaccurateandreliableindepen-
dentanswerstoverificationquestions,enablingmoreeffectivedetectionofhallucinations
anduncertaintiesinmedicalexplanations.
6 Conclusion
Inthispaper, weconductanempiricalstudyontheUncertaintyEstimationofLLMsin
medical question-answering tasks. We find Uncertainty Estimation challenging in the
medicaldomain,withexistingmethodsperformingpoorly,especiallywithsmallermodel
sizes. Toaddressthischallenge,weproposeTwo-phaseVerification,anovelapproachthat
integratestheconceptofCoVetoassessthereliabilityoflanguagemodeloutputs. Weshow
that the model is capable of detecting its own hallucinations by answering verificationquestions independently and cross-checking against the answers referencing its initial
reasoning. Overall,ourTwo-phaseVerificationmethoddemonstratessuperiorperformance
overbaselinemethodsandisreliableacrossvariousmodelsizesanddatasetsettings.
References
MarcoCascella,JonathanMontomoli,ValentinaBellini,andElenaBignami. Evaluatingthe
feasibilityofchatgptinhealthcare: ananalysisofmultipleclinicalandresearchscenarios.
JournalofMedicalSystems,47(1):33,2023.
Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, and Ji-Rong
Wen. Chatcot: Tool-augmentedchain-of-thoughtreasoningonchat-basedlargelanguage
models,2023.
I-ChunChern,SteffiChern,ShiqiChen,WeizheYuan,KehuaFeng,ChuntingZhou,Junxian
He,GrahamNeubig,andPengfeiLiu. Factool: Factualitydetectioningenerativeai–a
toolaugmentedframeworkformulti-taskandmulti-domainscenarios,2023.
ShehzaadDhuliawala,MojtabaKomeili,JingXu,RobertaRaileanu,XianLi,AsliCeliky-
ilmaz,andJasonWeston. Chain-of-verificationreduceshallucinationinlargelanguage
models,2023.
JinhaoDuan,HaoCheng,ShiqiWang,AlexZavalny,ChenanWang,RenjingXu,Bhavya
Kailkhura,andKaidiXu. Shiftingattentiontorelevance: Towardstheuncertaintyestima-
tionoflargelanguagemodels,2023.
MarinaFomicheva,ShuoSun,LisaYankovskaya,Fre´de´ricBlain,FranciscoGuzma´n,Mark
Fishel,NikolaosAletras,VishravChaudhary,andLuciaSpecia. Unsupervisedquality
estimationforneuralmachinetranslation. TransactionsoftheAssociationforComputational
Linguistics,8:539–555,2020.doi:10.1162/tacl a 00330.URLhttps://aclanthology.org/
2020.tacl-1.35.
ZhijiangGuo,MichaelSchlichtkrull,andAndreasVlachos. Asurveyonautomatedfact-
checking,2022.
HangfengHe,HongmingZhang,andDanRoth. Rethinkingwithretrieval: Faithfullarge
languagemodelinference,2022.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-
enhancedbertwithdisentangledattention,2021.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin
Bang,AndreaMadotto,andPascaleFung. Surveyofhallucinationinnaturallanguage
generation. ACMComputingSurveys,55(12):1–38,mar2023. doi: 10.1145/3571730. URL
https://doi.org/10.1145%2F3571730.
DiJin,EileenPan,NassimOufattole,Wei-HungWeng,HanyiFang,andPeterSzolovits.
What disease does this patient have? a large-scale open domain question answering
datasetfrommedicalexams. AppliedSciences,11(14):6421,2021.
QiaoJin,BhuwanDhingra,ZhengpingLiu,WilliamCohen,andXinghuaLu. PubMedQA:
Adatasetforbiomedicalresearchquestionanswering. InKentaroInui,JingJiang,Vin-
cent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pp. 2567–2577, Hong Kong, China, November
2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL
https://aclanthology.org/D19-1259.
SauravKadavath,TomConerly,AmandaAskell,TomHenighan,DawnDrain,EthanPerez,
NicholasSchiefer,ZacHatfield-Dodds,NovaDasSarma,EliTran-Johnson,ScottJohnston,
Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai,
SamBowman,StanislavFort,DeepGanguli,DannyHernandez,JoshJacobson,JacksonKernion,ShaunaKravec,LianeLovitt,KamalNdousse,CatherineOlsson,SamRinger,
DarioAmodei,TomBrown,JackClark,NicholasJoseph,BenMann,SamMcCandlish,
ChrisOlah,andJaredKaplan. Languagemodels(mostly)knowwhattheyknow,2022.
TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa.
Largelanguagemodelsarezero-shotreasoners,2023.
LorenzKuhn,YarinGal,andSebastianFarquhar. Semanticuncertainty: Linguisticinvari-
ancesforuncertaintyestimationinnaturallanguagegeneration,2023.
StephanieLin,JacobHilton,andOwainEvans. Teachingmodelstoexpresstheiruncertainty
inwords,2022.
Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured
prediction,2021.
PotsaweeManakul,AdianLiusie,andMarkJ.F.Gales. Selfcheckgpt: Zero-resourceblack-
boxhallucinationdetectionforgenerativelargelanguagemodels,2023.
Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A
large-scalemulti-subjectmulti-choicedatasetformedicaldomainquestionanswering.
In Gerardo Flores, George H Chen, Tom Pollard, Joyce C Ho, and Tristan Naumann
(eds.), Proceedings of the Conference on Health, Inference, and Learning, volume 174 of
Proceedings of Machine Learning Research, pp. 248–260. PMLR, 07–08 Apr 2022. URL
https://proceedings.mlr.press/v174/pal22a.html.
KurtShuster,SpencerPoff,MoyaChen,DouweKiela,andJasonWeston. Retrievalaugmen-
tationreduceshallucinationinconversation,2021.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes,JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
ViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,DianaLiskovich,YinghaiLu,Yuning
Mao,XavierMartinet,TodorMihaylov,PushkarMishra,IgorMolybog,YixinNie,Andrew
Poulton, JeremyReizenstein, RashiRungta, KalyanSaladi, AlanSchelten, RuanSilva,
EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,RossTaylor,
AdinaWilliams, Jian XiangKuan, Puxin Xu, Zheng Yan, IliyanZarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
SergeyEdunov,andThomasScialom. Llama2: Openfoundationandfine-tunedchat
models,2023.
ZhiyuanWang,JinhaoDuan,ChenxiYuan,QingyuChen,TianlongChen,HuaxiuYao,Yue
Zhang, Ren Wang, Kaidi Xu, and Xiaoshuang Shi. Word-sequence entropy: Towards
uncertaintyestimationinfree-formmedicalquestionansweringapplicationsandbeyond,
2024.
YijunXiaoandWilliamYangWang. Onhallucinationandpredictiveuncertaintyincondi-
tionallanguagegeneration,2021.