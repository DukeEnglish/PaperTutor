1
WhisperNetV2: SlowFast Siamese Network For
Lip-Based Biometrics
Abdollah Zakeri1, Hamid Hassanpour1, Mohammad Hossein Khosravi2, Amir Masoud Nourollah1
1Faculty of Computer Engineering, Shahrood University of Technology
2Faculty of Electrical and Computer Engineering, University of Birjand
Abstract—Lip-based biometric authentication (LBBA) has at- gait recognition, signature recognition, speaker identification,
tracted many researchers during the last decade. The lip is handwriting
specifically interesting for biometric researchers because it is
identification, and keystroke dynamics. Using lips as a bio-
a twin biometric with the potential to function both as a
metrictraithasattractedmuchattentioninthelastdecadedue
physiological and a behavioral trait. Although much valuable
research was conducted on LBBA, none of them considered totheirpotentialtobeemployedbothasaphysiologicalanda
the different emotions of the client during the video acquisition behavioraltraitfordifferenttaskssuchaspersonidentification
step of LBBA, which can potentially affect the client’s facial and verification. Visual features like the color and geometry
expressions and speech tempo. We proposed a novel network
of the lips can be considered a physiological trait, and the
structure called WhisperNetV2, which extends our previously
dynamics of the lips while speaking is a behavioral trait.
proposed network called WhisperNet. Our proposed network
leverages a deep Siamese structure with triplet loss having Although two people might utter the exact same words and
three identical SlowFast networks as embedding networks. The conveythesameconcept,thedynamicsoftheirlipswillnotbe
SlowFast network is an excellent candidate for our task since the same. As a result, the combination of lips’ visual features
the fastpathway extracts motion-related features(behavioral lip
and the dynamics of the lips while uttering a phrase provides
movements) with a high frame rate and low channel capacity.
enough uniqueness for a secure biometric system. Numerous
The slow pathway extracts visual features (physiological lip
appearance) with a low frame rate and high channel capacity. valuable research articles proposed using the audio modality
Using an open-set protocol, we trained our network using the along with the video of the lips, or even as a single trait
CREMA-D dataset and acquired an Equal Error Rate (EER) of to provide the discrimination power required for a biometric
0.005 on the test set. Considering that the acquired EER is less
system.However,thereareseveraldownsidestoincorporating
thanmostsimilarLBBAmethods,ourmethodcanbeconsidered
the audio modality like the detrimental effect of noisy audio
as a state-of-the-art LBBA method.
on the overall accuracy of the biometric system. Additionally,
Index Terms—Biometrics, Lip Authentication, Lip-Based Bio-
incorporation of audio modality decreases the applicability
metrics, LBBA, Siamese, SlowFast
of the biometric system in places in which silence is a
requirement like in a library. Furthermore, speech-impaired
I. INTRODUCTION
people cannot use these systems which is a major draw-
Biometric methods for person verification and identifica- back. Due to the abovementioned reasons, the audio modality
tion have attracted much attention among researchers in the reduces the biometric authentication system’s applicability;
last decade, thanks to the numerous advantages they have therefore, we use a visual-only approach in our proposed
compared to old authentication methods like passwords and method. Lip-based biometric authentication (LBBA) method
Personal Identity Numbers (PINs). Unlike the old methods is a very straightforward and practical way of authentication
mentioned above, biometric traits cannot be forgotten or since it does not need any special equipment for the video
transferredtoanotherperson.Furthermore,aclient’sbiometric acquisition, which can be done using the front-facing camera
data cannot be stolen, and they are pretty hard to replicate. of a regular smartphone. Furthermore, this method can be
Considering the aforementioned merits, biometric methods implemented in a very light and efficient way to be used
provide people authentication systems with more security. on mobile devices without any significant processing power.
Although some solutions like two-factor authentication have LBBA has attracted many researchers because of its merits
been proposed to improve the security of passwords, these andadvantagesoverotherbiometricmethods.Biometrictraits
methods are less resistant to spoofing attacks compared to necessitate uniqueness for each person, and lips are proven to
biometric-based authentication systems. provide this uniqueness [6], [7].
Biometric methods are divided into two main categories, Human lip functions both as a physiological and a behav-
namely physiological and behavioral traits. The former is ioral trait, considering that every person might utter the same
defined as measurable and unique aspects of the human body words uniquely [8], [9]. LBBA is more robust and secure
which can serve as an identity, like face [1], fingerprint [2], thanotherphysiological-onlybiometricsystems.Incorporating
palm print [3], hand geometry [4] and iris [5], while the latter lip movements as a behavioral trait makes spoofing attacks
is defined as any behavioral pattern that is done in a unique almost impossible since the attackers must model a person’s
manner by individuals and hence, can be used as a means talking habits, which is a highly complex task and requires
of identification. Common methods using behavioral traits are an abundance of data. Furthermore, the video acquisition
4202
luJ
11
]VC.sc[
1v71780.7042:viXra2
process for LBBA does not require any special sensors or works, are presented in Section 6.
instruments and can be effortlessly done using a video cam-
era. This feature makes LBBA more applicable than other
II. LITERATUREREVIEW
biometric methods requiring specific means of measurement.
Additionally, LBBA can be combined with other biometric The first step of pre-processing for an LBBA system is
methods such as face to increase the security and robustness to localize the lip region using image segmentation. Lip
of the biometric system. Moreover, LBBA is more hygienic segmentation methods are primarily classified into contour-
than other biometric methods like fingerprint which require based [11] and clustering-based techniques [12], [13]. An
the client to touch the sensor. Most visual biometric systems unsupervisedsegmentationmethodwasproposedbyChanthat
encounter the challenge of variations in clients’ appearance used Gaussian Mixture Models (GMMs) for lip localization
over time. In a facial verification system, for instance, the [11]. A Fuzzy C-Means (FCM) algorithm was used by Fu
growth of facial hair in men and the variation of makeup in et al. in [12] for lip segmentation. Furthermore, Lu and Liu
womencansometimesincreasetheerrorratesofthebiometric [14] used the Localized Active Contour Model (LACM) and
system. To solve this issue, the visual biometric systems two initial contours for lip segmentation. The task of feature
must be trained using a large enough dataset containing a extractionfromlipimagesisthenextstepinanLBBAsystem.
fair percentage of the possible variations in the data that Earlier research articles used low-level hand-crafted features
the system might face in practical applications. Although likecolor,shape[15],texture,andgeometry[16]andmid-level
somephysiologicalbiometricmethodslikefingerprintandiris featureextractionmethodslikePrincipalComponentAnalysis
recognition do not suffer from this issue, the LBBA method (PCA) and Linear Discriminant Analysis (LDA) to extract
willfacethischallengegiventhattheclientsmayhaveseveral visualfeaturesfromtheliparea.Behavioralfeaturescontained
emotional states at the time of authentication, which will temporaldataandwereextractedusingmodelssuchasHidden
potentiallyaffecttheirlipmovementsduringanutterance.This Markov Model (HMM) and GMM [15]. With the rise of deep
changeinthelipdynamicswilldecreasethesimilaritybetween learning techniques, Convolutional Neural Networks (CNNs)
the video acquired for enrollment and the video acquired for replaced the old-fashioned hand-crafted feature extraction
authentication.Consequently,thisissuewillincreasetheFalse techniquesforvisualfeatureextraction.Thetemporaldata(lip
Rejection Rate (FRR) of the biometric system. One possible movement among video frames) was modeled using RNNs
solution for decreasing the FRR of the system would be like LSTM or GRU since the movements of the lips can be
to decrease the threshold so that the system is less strict. modeled as sequential data. In [15], authors defined an Active
Nevertheless, there is a tradeoff between False Acceptance Shape Model (ASM) based on lip boundary and intensity
Rate (FAR) and FRR, and changing the threshold in favor of parameters. Furthermore, they used HMM and GMM for
decreasingtheFRRwouldincreasetheFAR.Anothersolution speakeridentificationandachieved97.9%accuracyonasmall
is to train the system so that it becomes inattentive to these dataset containing only 12 clients. Wark et al. [17] used lip
minor changes and attends only to the static characteristics. contour profiles and a multi-stream HMM for identification
In the domain of LBBA, the previous solution is translated and acquired 80% accuracy on the XM2VTS dataset. Lai et
as training the system so that it becomes invariant to all the al.[18]proposedusingSparseCoding(SC)tocharacterizethe
emotions that the client might have during the authentication movements in the lip region followed by a max pooling on a
video acquisition. This method is only possible if we have spatio-temporal hierarchical structure in order to produce the
the required data to train the biometric system. In this paper, final features. They tested their method on a private dataset
we extend our previous work, called WhisperNet [10], by of 40 clients and achieved a Half Total Error Rate (HTER) of
improving the proposed network architecture by substituting 0.46%. Wright and Stewart [19] trained a Siamese network
the embedding network with the SlowFast architecture which using the LipNet structure containing STCNN and biGRU
captures both physiological and behavioural features of lip layers as the embedding network for the task of speaker
videos, resulting in better performance and higher accuracy. verification under a closed-set Lausanne protocol (all subjects
The main contributions of this paper to the field are as are enrolled during training) and achieved 1.03% EER on the
follows: (i). Addressing the challenges that visual biomet- XM2VTS dataset. Their next article [20] tested their architec-
ric systems face due to different emotions that the clients ture under a more restrictive open-set protocol (new subjects
might have during authentication video/image acquisition and are enrolled during validation and test) and achieved an EER
proposing a possible solution. (ii). Improving the network of 1.65%. Kuang et al.[21] introduced LipAuth, leveraging
architecture proposed in our previous WhisperNet research in unique spatial-temporal features of human lips, achieving a
order to acquire both more efficient performance and higher 99.24% accuracy for user authentication and demonstrating
accuracy. (iii). Introducing a state-of-the-art architecture for robustness against video replay and mimic attacks. Similarly,
LBBA which can be adapted to other domains such as face Koch and Grbic´ [22] addressed vulnerabilities in existing
verification/identification. In the following sections of this LBBA methods by introducing the GRID-CCP dataset and
paper, we go through the previous related works in Section training a siamese neural network with 3D convolutions and
2, introduce the dataset and the pre-processing methods in recurrent neural network layers, achieving a False Acceptance
Section 3, introduce the proposed network architecture in Rate (FAR) of 3.2% and a False Rejection Rate (FRR) of
Section 4, and test results and experiments are presented in 3.8%. An overview of existing LBBA methods and datasets
Section 5. The conclusion, followed by suggestions for future are provided in tables I and II accordingly.3
TABLEI
ANOVERVIEWOFTHEEXISTINGLBBAMETHODS
Year ProposedBy Method Dataset #Clients Performance
1996 Luettinetal.[15] ActiveShapeModel(ASM)+HMMGMM Private 12 97.9%Acc
2000 Warketal.[17] Lipcontourprofiles+multi-streamHMM M2VTS[23] 37 80%Acc
2002 Brounetal.[16] Geometricfeatures+polynomial-basedmodel XM2VTS[24] 295 6.3%HTER
2012 Chanetal.[25] LOCP-TOP XM2VTS 295 0.36%HTER
Bakryand AVLetters[27] 10 42.82%Acc
2013 Nonlinearmapping+KernelPartialLeastSquares
Elgammal[26] OuluVS[28] 52 62.34%Acc
2014 Liuand Multi-boostedHMMs Private 46 3.91%EER
Cheung[29]
2016 Laietal.[18] Jointspatio-temporalsparse Private 40 0.46%HTER
codingandhierarchicalpooling
2019 Wrightand Siamese(STCNN+BiGRU) XM2VTS 295 1.03%EER
Stewart[19]
2020 Wrightand Siamese(STCNN+BiGRU) XM2VTS 295 1.65%EER
Stewart[20]
2021 Zakeriand Siamese(STCNN+BiGRU) CREMA-D[30] 88 95.41%Acc
Hassanpour[10]
2024 KochandGrbic[22] Siamese(3DCNN+RNN) GRID-CCP - 3.2% FAR, 3.8%
FRR
2024 Kuangetal.[21] LipAuth CustomDataset 50 99.24% Acc
TABLEII
ANOVERVIEWOFTHEEXISTINGLBBADATASETS
Year ProposedBy Name Resolution #Clients
1999 Messeretal.[24] XM2VTS 720×576 295
2002 Pattersonetal.[31] CUAVE 64×32 36
2005 Foxetal.[32] VALID 720×576 106
2005 Peeretal.[33] CVLFaceDatabase 640×480 114
2015 Aninaetal.[28] OuluVS2 44×30 52
2016 Bakshietal.[34] Nitrlipv1 180×180 109
2017 Ramanetal.[35] NITRLipV2(MobioLip) 72×72 20
III. DATASETANDPRE-PROCESSING by different emotions. An overview of the datasets applicable
in LBBA is listed in table II.
Numerousdatasetsareavailablethatcanbeusedtotrainan
LBBA network [24], [28], [31], [32], [33], [34], [35]. To the Out of the 88 people in our dataset, 66 were assigned to
bestoftheauthors’knowledge,noneofthemincludeemotion- the training set, 11 to the validation set, and 11 to the test set.
related data concerning the subjects’ facial expressions or Sincetheclientsinthetestsetarenotenrolledduringtraining,
variationsinutteranceandspeechtempoduetodifferentemo- we used an open-set protocol [30]. Since the dataset included
tional states. Hence, we used the CERMA-D dataset [30] to full-facevideos,weneededtocropeachframeofeveryvideo
train our network, which contains 7,442 videos of 91 subjects tothelipregionsothatthenetworkwouldnottakeadvantage
uttering 12 phrases in 6 different emotional states, including of any data other than lips. We used an open-source Python
neutral,happy,sad,anger,disgust,andfear.Nevertheless,there library called Face Recognition [36] to perform this task. We
were data shortages for three clients, so we excluded them extracted the lip landmarks from every frame of each video,
fromthedataset,andtherefore,wewereleftwith6,336videos and then using these landmarks, we selected a bounding box
and 88 clients. Although the size of our dataset is smaller containingonlythelipsandcroppedthewholeframeintothat
than other datasets like XM2VTS [24], our dataset contains boundingbox.Aftercropping,weresizedeachcroppedimage
emotional data that provides various utterances of the same to the size of the smallest lip image (30×18).
phrasebythesamesubjectbutwithdifferentfacialexpressions
and speech tempos. Considering that the network is trained Furthermore, the bounding boxes were all chosen in a way
assuming that the different utterances of the same phrase thattheywouldhavethesameaspectratioinordertominimize
by the same person are equivalent, the network gradually the interpolations during resizing. The complete algorithm for
becomes invariant to the small visual changes that are caused pre-processing of the database is presented in Algorithm 1.4
Algorithm 1 Lip Landmarks Extraction and Bounding Box Adjustment
1: for video in Dataset do
2: for frame in video do
3: Landmarks←{(x,y) i} i=1,...,24 ▷ Extract frame’s lip landmarks using FaceRecognition Library
4: x min ←min{x|(x,y) i ∈Landmarks}
5: x max ←max{x|(x,y) i ∈Landmarks}
6: y min ←min{y |(x,y) i ∈Landmarks}
7: y max ←max{y |(x,y) i ∈Landmarks}
8: BoundingBoxTopLeft←(x min,y min)
9: BoundingBoxBottomRight←(x max,y max)
10: BoundingBoxWidth←x max−x min
11: BoundingBoxHeight←y max−y min
12: BoundingBoxAspectRatio← BoundingBoxWidth
BoundingBoxHeight
13: if BoundingBoxAspectRatio<AR then ▷ AR is the aspect ratio with the highest absolute frequency, which is
5/3 in our case.
14: BoundingBoxWidth←BoundingBoxHeight×AR
15: else
16:
BoundingBoxHeight←BoundingBoxWidth×(cid:0) 1 (cid:1)
AR
17: end if
18: NewFrame← Crop the current frame according to the calculated bounding box and then resize to 30x18
19: end for
20: end for
Lip-based Biometric Authentication
Visual Authentication Model
Client ID
Client’s true embedding
Clients DB
Yes
Lip Segmentation
Match
Similarity > Threshold?
Cropped lip video Reject
No
Authentication video
Embedding
network Embedded input
Fig.1. Flowchartofourproposedmethodforpre-processingandvisualspeakerauthentication
IV. PROPOSEDMETHOD a static threshold, the person is successfully authenticated.
Aside from the pre-processing and data preparations, which
There are two common steps in biometric authentication are specific to our method, LBBA has the same two steps. A
systems. The first step is called enrollment and includes gen- flowchart of our proposed LBBA method is demonstrated in
erating the corresponding embedding for each client based on Fig. 1.
their biometric features and saving this embedding along with The embedding is a network mapping the input data to
a unique ID in the clients’ database. The next step is authen- latent space, and its output is called the embedding or the
tication, in which the system obtains biometric information encoding. In order to train this network, we used a Siamese
from the client and generates the corresponding embedding architecture with the triplet loss function [37], [38]. The
basedonthisobtainedinformation.Next,thesystemcompares Siamese architecture consists of three identical branches that
this newly generated embedding with the one stored in the take triplets of input videos and generate the corresponding
database, and if the two embeddings’ similarity surpasses embeddings for each item of the triplet tuple. The structure5
slowandfastpathways(β¡1).Tosumitup,boththeslowand
fast pathways are identical CNNs, but the fast pathway has a
lowtemporalstride(hightemporalresolution)andlowchannel
capacity. In contrast, the slow pathway has a high temporal
stride(lowtemporalresolution)andhighchannelcapacity.As
mentioned before, the lip can be used as a twin biometric,
considering its physiological and behavioral features. This
capability of the lips makes the SlowFast networks an ideal
candidate for the task of LBBA since, on the one hand,
the fast pathway extracts motion-related features from the lip
video,whicharethebehavioraltraitsrelatedtolipmovements
during an utterance. These motion-related data do not need
Fig.2. StructureofaSiameseNetwork
high channel capacity, making the fast pathway an excellent
candidateforbehavioralfeatureextraction.Ontheotherhand,
of the Siamese network is presented in Fig. 2. Each triplet the physiological features are static visual features that do not
includesthreevideos,namelytheanchor(A),thepositive(P), change much during the video. Hence, the extraction of these
andthenegative(N).Theanchorandpositivearetwovideosof features does not need high temporal resolution, but its high
thesamepersonutteringthesamephrase.Thenegativecanbe channel capacity would be beneficial, and this is the exact
any video that does not fit into the definition of positive. The description of the slow pathway. At the final layer of the
correspondingembeddingsforA,P,andNaregeneratedusing SlowFast architecture, the outputs of the two pathways are
three identical embedding networks. Using the triplet loss concatenated, forming our final embedding. This SlowFast
function,thenetworkgraduallylearnstogenerateembeddings embedding network uses physiological and behavioral lip
so that the distance between A and P embeddings is less than features to generate the final embedding vector. Additionally,
the distance between A and N embeddings. The triplet loss is several lateral connections between the two pathways fuse the
defined as follows: informationofthetwopathwaysbeforethefinalconcatenation
layer,whichmakeseachpathwayawareoftherepresentations
learned by the other. However, the authors of [39] found
L(A,P,N)=max(D(A,P)−D(A,N)+α,0) (1) similar results while using bidirectional and unidirectional
lateral connections, and therefore, in the implementation of
Thelossfunctionensuresaminimummarginwiththevalue
this network, these lateral connections are unidirectional con-
of α between any of the anchor-positive and the anchor-
nections from the fast pathway to the slow pathway. The
negative distances. The distance function D can be any dis-
resultofeach“stage”ofthefastpathwayisconcatenatedwith
tance measure like cosine or Euclidian distance. We chose
the result of the previous stage before being entered to the
cosine similarity as the distance function and the value of 0.7
next stage of the slow pathway. According to [39], since the
for the margin parameter. The cost function is calculated as
two pathways have different temporal dimensions, the lateral
follows:
connections must perform a transformation to match them.
m One of the crucial tasks in training Siamese networks with
(cid:88)
S = L(A(i),P(i),N(i)) (2) tripletlossistheselectionoftriplets.WrightandStewart[20]
i=1 classified the triplets into three categories:
(m denotes the total number of samples in the dataset) (i) Easytripletsaretripletsinwhichthedistancebetweenthe
anchor and positive embeddings is less than the distance
We used SlowFast architecture (Fig. 3) as the embedding between anchor and negative:
network [39]. This network was initially proposed for video
D(A,P)+α<D(A,N) (3)
recognitiontasks.Itpassesthesamevideointotwoprocessing
pathways: the slow pathway and the fast pathway. The fast (ii) Semi-hard triplets are triplets in which the positive is
pathway processes the input video with a high Frame Per closer to the anchor than the negative but still generate a
Second (FPS) (low temporal stride) to extract motion-related positive loss since the negative stays within the margin:
data from consecutive frames. The slow pathway processes
D(A,P)<D(A,N)<D(A,P)+α (4)
the input video with low FPS (high temporal stride) to
extract static visual features. Although the fast pathway has (iii) Hard triplets are triplets in which the distance between
a higher temporal resolution, it has lower channel capacity the negative and anchor is less than the distance between
than the slow pathway. There are two main hyperparameters the positive and the anchor:
for SlowFast networks, namely α and β. The α parameter
D(A,P)≥D(A,N) (5)
represents the temporal stride ratio between the slow and fast
pathways. For example, if the temporal strides of the fast and Usingonlyhardtripletsstopsthenetworkfromconverging,
slow pathways are 2 and 16 accordingly, then α=8, which is and using only easy triplets will not contribute much to
the typical value for this parameter according to the literature decreasing the loss value. So we selected triplets randomly
[39]Theβ parameterrepresentsthechannelratiobetweenthe from all the possible train triplets:6
Slow-Fast Network Architecture
Slow Pathway
T
T C
T C H ,W
Low Temporal Resolution C T
(High temporal stride) C
αT αT
αT
βC
High Temporal Resolution βC βC Result Feature Vector
(Low temporal stride)
Fast Pathway
Fig.3. Slow-FastNetworkArchitecture
Loss/Validation chart
Train Validation 50 per. Mov. Avg. (Train)
1.2
1
0.8
0.6
0.4
0.2
0
0 500 1000 1500 2000 2500 3000 3500 4000
Iteration
Fig.4. Traininglosscurveforourproposednetwork.Duetofluctuationsinthelossvalues,themovingaverageforlossvaluesisplotted.
utterthesamephraseintheanchorandthepositivevideos,all
(66×12×6)× 5 × other utterances of the remaining phrases done by the same
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) client are considered negative.
Anchor Positive
 
There are two challenging sets of triplets that are highly
  (6) beneficial for the embedding network. The first are triplets in
(65×12×6)+(1×11×6)=112,764,960
  which the negative video belongs to the same client as the
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
OtherPeople, SamePerson, positive and the anchor, but the uttered phrase is different.
allPhrases OtherPhrases
(cid:124) (cid:123)(cid:122) (cid:125) While training on these triplets, the slow pathways of the
Negative embedding networks produce almost the same output for all
Since the client should not be authenticated if they do not three inputs. As a result, the network should only lean on the
ssoL7
output of the fast pathway (behavioral features) to decrease
Average Test EER Across
the loss value, which improves the fast pathway.
Different Training Populations
The other set of challenging triplets are triplets in which
the negative video has the same phrase as the anchor and the
positivebututteredbyotherclients.Incontrastwiththeformer
0.5
set of triplets, the fast pathways of embedding networks will
0.4
generate similar embeddings. The network must use only the
slow pathway (physiological features) to decrease the triplet 0.3
loss value since the difference between physiological features 0.2
of anchor and negative is more than the difference between
0.1
their behavioral features. Consequently, the slow pathway of
the embedding network will be improved by using these 0
triplets. 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70
Training population
V. EXPERIMENTSANDRESULTS
All training and evaluations were performed on a linux-
based system with 2×Nvidia A100 GPU’s, 512 GB DDR4 Fig.5. Traininglosscurveforourproposednetwork.Duetofluctuationsin
thelossvalues,themovingaverageforlossvaluesisplotted.
RAM, and an AMD EPYC 7H12 64-core processor. With the
aforementionedconfiguration,asingleinferencestepincluding
I/O tasks, preprocessing, and feeding the input videos to the
Test results on different sizes of
embeddingnetworktogetthefinalresult,takes≈2.37seconds
(1.43sforreadingpairofvideos,0.42sforpreprocessing,0.52 training dataset
for generating embeddings and comparison).
We trained our network for 3800 iterations, however the Min Max Average
criterion for ending training (I¡Threshold) was met before
0.8
2000 iterations. In each iteration, we sampled a random batch
0.6
(batchsize=512)fromthetrainingdatasetandduetoalarge
numberoftrainingsamples,therewasnooverfittingduringthe 0.4
training session. Training/Validation loss values are plotted in 0.2
Fig. 4. 0
There are three standard metrics for the evaluation of 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70
biometric verification systems: Training Population
1) False Rejection Rate (FRR): Percentage of the times
a biometric verification system denies access to an
Fig.6. Traininglosscurveforourproposednetwork.Duetofluctuationsin
authorized client. This error is known as Type I error
thelossvalues,themovingaverageforlossvaluesisplotted.
in statistical terms.
# of denied authentic requests
FRR= (7) be 3,112,560, including hard, semi-hard, and simple triplets.
# of all requests
These triplets were sampled into batches of size 512, and
2) False Acceptance Rate (FAR):Percentageofthetimes
for each triplet, the cosine similarities for anchor positive
a biometric verification system grants access to an im-
and anchor negative pairs were calculated. For every value of
poster. This error is known as Type II error in statistical
similaritythresholdbetween0and1,withaprecisionof0.001,
terms.
we calculated the FAR and FRR values. At the threshold of
0.983, the FAR and FRR values were equal and had the value
# of granted imposter requests
FAR= (8) of 0.005, which would be the EER of our proposed method.
# of all requests
Additionally, to assess the performance of the network with
3) Crossover Error Rate (CER): Point where the FAR different training populations, we sampled random subsets of
and FRR are equal. This metric describes the overall trainingpeopleandgeneratedthetrainingtripletsaccordingly.
performance of the biometric verification system. For comparison purposes, all trained networks were evaluated
As we witnessed in our experiments, there is a tradeoff using a fixed test set (size=11 people). The results suggested
betweenFARandFRRwhilesettingthethresholdvalueforthe that the overall performance of the network improves with
biometric system. In a perfect biometric verification system, increasing the size of the training set. For each training
both FAR and FRR would be 0% for all of the possible population size, the network was trained for 20 sessions and
threshold values, and as a result, the EER would equal 0%. the training and validation datasets were shuffled before each
To test our proposed method, we selected 11 unseen clients session.TheAverageEERvaluesforeachtrainingpopulation
and generated the corresponding triplets for them. According are demonstrated in Fig. 5, and the loss values for the same
to a formula similar to (6), the number of test triplets would experiments are plotted in Fig. 6.
REE
eulaV
ssoL
493.0
2540.0 470.0 8150.0 2630.0 570.0
2380.0
290.0 6320.0 9920.0 7120.0 8210.0 4700.0
500.08
examinetheconceptsoftruepositives(TP)andfalsepositives
WhisperNetV2
(FP) as they relate to the results. True positives represent
C. Wright and D. Stewart
instances where our biometric authentication system correctly
C.H. Chan et al.
identified a legitimate user, thus providing a measure of
WhisperNet
the system’s accuracy and security. Conversely, false posi-
1
0.95 tives signify cases where the system erroneously accepted
0.9
an imposter as a genuine user, which can lead to potential
0.85
0.8 security vulnerabilities and usability issues. Comparing these
0.75
0.7 metricswiththosefrompreviousstudies,ourproposedmethod
0.65 demonstratesaremarkableadvantage.Thelowerfalsepositive
0.6
0.55 rate in our model, evident in the considerably lower Equal
0.5 Error Rate (EER) of 0.005, underscores its heightened se-
0.45
0.4 curity, as it effectively minimizes the risk of unauthorized
0.35 access. In contrast, previous methods, such as Wright and
0.3
0.25 Stewart’s [20] with an EER of 0.136 and Chan et al.’s
0.2
[25] with an EER of 0.059, exhibited higher false positive
0.15
0.1 rates,indicatingahighersusceptibilitytoimposteracceptance.
0.05
These findings emphasize the significance of our model’s
0
enhanced security and its potential for real-world applications
whereminimizingfalsepositivesisparamount.Inourresearch
False Positive Rate
paper, our latest model demonstrates notable improvements
over its predecessor, achieving a reduction of 0.033 in the
EqualErrorRate(EER)whilesimultaneouslyenhancingcom-
Fig.7. Traininglosscurveforourproposednetwork.Duetofluctuationsin
putational efficiency. The architecture of our previous em-
thelossvalues,themovingaverageforlossvaluesisplotted.
bedding network consisted of Spatio-Temporal Convolutional
Neural Networks (STCNNs) followed by Gated Recurrent
We evaluated the performance of our newly proposed Units (GRUs), totaling 76,749,124 trainable parameters. In
method alongside two established approaches (Wright and contrast,ourcurrentproposedembeddingnetworkemploysthe
Stewart [20], and Chan et al. [25]), as well as our previous morestreamlinedSlowFastnetworkwith52,314,144trainable
model,ontheCREMA-Ddataset.Allmethodsweresubjected parameters, resulting in accelerated training and inference
to the same training criteria, parameters, and pre-processing processes. Moreover, our previous method relied on a list of
steps, with only differences in input video sizes to match 24 lip landmark points to generate embeddings, making the
each method’s requirements. The results demonstrated in Fig. network susceptible to errors. These errors in lip landmark
7clearlyindicatethatourproposedmethodexcelsintermsof extraction had a cascading effect on the embedding network,
accuracy,withanexceptionallylowEqualErrorRate(EER)of compromising both the overall performance and robustness of
0.005.Incontrast,WrightandStewart[20]andChanetal.[25] the model. To address this issue, our present study eliminates
yielded higher EER values of 0.136 and 0.059, respectively. the lip landmark extraction step, effectively resolving this
Our previous model, represented by an EER of 0.038 in the source of error.
ROC curve, performs better than the other two methods. One
crucial aspect to consider is the complexity of the CREMA-D
VI. CONCLUSIONANDFUTUREWORKS
dataset,whichfeaturesutterancesofthesamephraseexpressed
with varying emotions. This variability introduces complexity This study presents a deep Siamese network for LBBA
in facial expressions and speech tempo, which demands a using the SlowFast network as the embedding network. The
certain level of model sophistication to effectively capture. SlowFast architecture has a fast pathway with low temporal
Models lacking this complexity tend to be highly biased and, strideandlowchannelcapacity,whichextractsbehavioralfea-
as evident in the results, exhibit degraded performance. In tures, and a slow pathway with high temporal stride and high
essence, the superior performance of our proposed method channelcapacityextractingphysiologicalfeaturesfromthelip
highlightstheimportanceoffindingtherightbalancebetween authenticationvideo.WeacquiredanEERof0.005onthetest
security and usability. Achieving low EER values signifies a set containing 11 unseen clients. Our model improves upon
robust security stance, while simultaneously ensuring that the the performance, robustness, and efficiency of its predecessor
model can handle the dataset’s inherent complexity enhances by decreasing both EER and number of trainable parameters.
usability. Our model strikes this balance effectively, outper- Although the propoed method outperforms similar LBBA
forming both previous iterations and competing methods in methods and the acquired results seems promising for real-
thischallengingbiometricauthenticationtaskontheCREMA- world applications, some challenges have not been mentioned
Ddataset.Thisunderscoresthesignificanceofcomprehensive in this paper, like the performance of the proposed method
model design when considering real-world applications that under various lighting conditions and different resolutions.
involve complex data. Another challenge in real-world applications would be the
In the context of our research findings, it’s crucial to capacity of the network. Several factors directly impact the
etaR
evitisoP
eurT
0 50.0 1.0 51.0 2.0 52.0 3.0 53.0 4.0 54.0 5.0 55.0 6.0 56.0 7.0 57.0 8.0 58.0 9.0 59.0 19
capacity of our proposed network, namely the length of the [19] C. Wright and D. Stewart, “One-Shot-Learning for Visual Lip-Based
uttered phrase in enrollment and authentication videos and Biometric Authentication,” in Advances in Visual Computing, ser.
Lecture Notes in Computer Science, G. Bebis, R. Boyle, B. Parvin,
the size of the embedding vector, which is the output of our
D.Koracin,D.Ushizima,S.Chai,S.Sueda,X.Lin,A.Lu,D.Thalmann,
SlowFast embedding network. Our current dataset has certain C. Wang, and P. Xu, Eds. Cham: Springer International Publishing,
limitations, and performing evaluations on larger publicly 2019,pp.405–417.
[20] C.WrightandD.W.Stewart,“Understandingvisuallip-basedbiometric
available datasets would make our method more reliable and
authentication for mobile devices,” EURASIP Journal on Information
robust. We plan to create a more extensive dataset for the Security,vol.2020,no.1,p.3,Mar.2020.
LBBAtaskinthenearfuturetofacilitateresearchinthisfield [21] L. Kuang, F. Zeng, D. Liu, H. Cao, H. Jiang, and J. Liu, “Lipauth:
Securingsmartphoneuserauthenticationwithlipmotionpatterns,”IEEE
and provide a benchmark for the fair comparison of different
InternetofThingsJournal,vol.11,no.1,pp.1096–1109,2024.
LBBA methods. [22] B. Koch and R. Grbic´, “One-shot lip-based biometric authentication:
Extending behavioral features with authentication phrase information,”
ImageandVisionComputing,vol.142,p.104900,2024.
REFERENCES [23] S.PigeonandL.Vandendorpe,“TheM2VTSmultimodalfacedatabase,”
inAudio-andVideo-basedBiometricPersonAuthentication,ser.Lecture
[1] J. Zhao, S. Yan, and J. Feng, “Towards Age-Invariant Face Recogni- NotesinComputerScience,J.Bigu¨n,G.Chollet,andG.Borgefors,Eds.
tion,”IEEETransactionsonPatternAnalysisandMachineIntelligence, Berlin,Heidelberg:Springer,1997,pp.403–409.
vol.44,no.1,pp.474–487,Jan.2022. [24] K. Messer, J. Matas, J. Kittler, K. Jonsson, J. Luettin, and G. Maˆıtre,
[2] A.Jain,L.Hong,andR.Bolle,“On-linefingerprintverification,”IEEE “XM2VTSDB: The Extended M2VTS Database,” in Second Interna-
Transactions on Pattern Analysis and Machine Intelligence, vol. 19, tionalConferenceonAudio-andVideo-basedBiometricPersonAuthen-
no.4,pp.302–314,Apr.1997. tication(AVBPA’99),Apr.2000.
[3] D. Zhang, W.-K. Kong, J. You, and M. Wong, “Online palmprint [25] C.H.Chan,B.Goswami,J.Kittler,andW.Christmas,“LocalOrdinal
identification,” IEEE Transactions on Pattern Analysis and Machine ContrastPatternHistogramsforSpatiotemporal,Lip-BasedSpeakerAu-
Intelligence,vol.25,no.9,pp.1041–1050,Sep.2003. thentication,”IEEETransactionsonInformationForensicsandSecurity,
[4] R. Sanchez-Reillo, C. Sanchez-Avila, and A. Gonzalez-Marcos, “Bio- vol.7,no.2,pp.602–612,Apr.2012.
metric identification through hand geometry measurements,” IEEE [26] A. Bakry and A. Elgammal, “MKPLS: Manifold Kernel Partial Least
Transactions on Pattern Analysis and Machine Intelligence, vol. 22, Squares for Lipreading and Speaker Identification,” in 2013 IEEE
no.10,pp.1168–1171,Oct.2000. Conference on Computer Vision and Pattern Recognition, Jun. 2013,
[5] L. Ma, T. Tan, Y. Wang, and D. Zhang, “Personal identification based pp.684–691.
on iris texture analysis,” IEEE Transactions on Pattern Analysis and [27] I.Matthews,T.Cootes,J.Bangham,S.Cox,andR.Harvey,“Extraction
MachineIntelligence,vol.25,no.12,pp.1519–1533,Dec.2003. ofvisualfeaturesforlipreading,”IEEETransactionsonPatternAnalysis
[6] S. K. Bandyopadhyay, S. Arunkumar, and S. Bhattacharjee, “Feature andMachineIntelligence,vol.24,no.2,pp.198–213,Feb.2002.
ExtractionofHumanLipPrints,”Dec.2013. [28] I. Anina, Z. Zhou, G. Zhao, and M. Pietika¨inen, “OuluVS2: A multi-
[7] Y. Tsuchihashi, “Studies on personal identification by means of lip viewaudiovisualdatabasefornon-rigidmouthmotionanalysis,”in2015
prints,”ForensicScience,vol.3,pp.233–248,Jan.1974. 11thIEEEInternationalConferenceandWorkshopsonAutomaticFace
[8] J.MasonandJ.D.Brand,“Theroleofdynamicsinvisualspeechbio- andGestureRecognition(FG),vol.1,May2015,pp.1–5.
metrics,”in2002IEEEInternationalConferenceonAcoustics,Speech, [29] X. Liu and Y.-m. Cheung, “Learning Multi-Boosted HMMs for Lip-
andSignalProcessing,vol.4,May2002,pp.IV–4076–IV–4079. PasswordBasedSpeakerVerification,”IEEETransactionsonInforma-
[9] D.P.Chowdhury,R.Kumari,S.Bakshi,M.N.Sahoo,andA.Das,“Lip tionForensicsandSecurity,vol.9,no.2,pp.233–246,Feb.2014.
asbiometricandbeyond:asurvey,”MultimediaToolsandApplications, [30] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, and
vol.81,no.3,pp.3831–3865,Jan.2022. R.Verma,“CREMA-D:Crowd-SourcedEmotionalMultimodalActors
[10] A.ZakeriandH.Hassanpour,“WhisperNet:DeepSiameseNetworkFor Dataset,”IEEETransactionsonAffectiveComputing,vol.5,no.4,pp.
Emotion and Speech Tempo Invariant Visual-Only Lip-Based Biomet- 377–390,Oct.2014.
ric,” in 2021 7th International Conference on Signal Processing and [31] E. Patterson, S. Gurbuz, Z. Tufekci, and J. Gowdy, “CUAVE: A
IntelligentSystems(ICSPIS),Dec.2021,pp.1–5. new audio-visual database for multimodal human-computer interface
[11] M. Chan, “Automatic lip model extraction for constrained contour- research,”in2002IEEEInternationalConferenceonAcoustics,Speech,
basedtracking,”inProceedings1999InternationalConferenceonImage andSignalProcessing,vol.2,May2002,pp.II–2017–II–2020.
Processing(Cat.99CH36348),vol.2,Oct.1999,pp.848–851vol.2. [32] N.A.Fox,B.A.O’Mullane,andR.B.Reilly,“VALID:ANewPractical
[12] J.-W. Fu, S.-L. Wang, and X. Lin, “Robust Lip Region Segmenta- Audio-VisualDatabase,andComparativeResults,”inAudio-andVideo-
tion Based on Competitive FCM Clustering,” in 2016 International BasedBiometricPersonAuthentication,ser.LectureNotesinComputer
ConferenceonDigitalImageComputing:TechniquesandApplications Science,T.Kanade,A.Jain,andN.K.Ratha,Eds. Berlin,Heidelberg:
(DICTA),Nov.2016,pp.1–8. Springer,2005,pp.777–786.
[13] S.Wang,S.Leung,andW.Lau,“Lipsegmentationbyfuzzyclustering [33] P.Peer,“ComputerVisionLaboratoryFacedatabase,”Jan.2005.
incorporatingwithshapefunction,”in2002IEEEInternationalConfer- [34] S. Bakshi, R. Raman, and P. K. Sa, “NITRLipV1: a constrained lip
ence on Acoustics, Speech, and Signal Processing, vol. 1, May 2002, databasecapturedinvisiblespectrum,”ACMSIGBioinformaticsRecord,
pp.I–1077–I–1080. vol.6,no.1,p.2:1,Apr.2016.
[14] Y. Lu and Q. Liu, “Lip segmentation using automatic selected initial [35] R.Raman,P.K.Sa,B.Majhi,andS.Bakshi,“Acquisitionandcorpus
contours based on localized active contour model,” EURASIP Journal descriptionofaconstrainedlipdatabasecapturedfromhandhelddevices:
onImageandVideoProcessing,vol.2018,no.1,p.7,Feb.2018. NITRLipV2(MobioLip),”ACMSIGBioinformaticsRecord,vol.7,no.1,
[15] J.Luettin,N.Thacker,andS.Beet,“Speakeridentificationbylipread- p.2:1,Feb.2017.
ing,” in Proceeding of Fourth International Conference on Spoken [36] A.Geitgey,“MachineLearningisFun!Part4:ModernFaceRecognition
LanguageProcessing.ICSLP’96,vol.1,Oct.1996,pp.62–65vol.1. withDeepLearning,”Sep.2020.
[16] C.C.Broun,X.Zhang,R.M.Mersereau,andM.Clements,“Automatic [37] J. Bromley, I. Guyon, Y. LeCun, E. Sa¨ckinger, and R. Shah, “Signa-
speechreading with application to speaker verification,” in 2002 IEEE ture Verification using a ”Siamese” Time Delay Neural Network,” in
InternationalConferenceonAcoustics,Speech,andSignalProcessing, AdvancesinNeuralInformationProcessingSystems,vol.6. Morgan-
vol.1,May2002,pp.I–685–I–688. Kaufmann,1993.
[17] T.Wark,S.Sridharan,andV.Chandran,“Theuseoftemporalspeechand [38] S. Chopra, R. Hadsell, and Y. LeCun, “Learning a similarity metric
lip information for multi-modal speaker identification via multi-stream discriminatively, with application to face verification,” in 2005 IEEE
HMMs,”in2000IEEEInternationalConferenceonAcoustics,Speech, Computer Society Conference on Computer Vision and Pattern Recog-
andSignalProcessing.Proceedings(Cat.No.00CH37100),vol.4,Jun. nition(CVPR’05),vol.1,Jun.2005,pp.539–546vol.1.
2000,pp.2389–2392vol.4. [39] C.Feichtenhofer,H.Fan,J.Malik,andK.He,“SlowFastNetworksfor
[18] J.-Y. Lai, S.-L. Wang, A. W.-C. Liew, and X.-J. Shi, “Visual speaker Video Recognition,” in 2019 IEEE/CVF International Conference on
identification and authentication by joint spatiotemporal sparse coding ComputerVision(ICCV),Oct.2019,pp.6201–6210.
andhierarchicalpooling,”InformationSciences,vol.373,pp.219–232,
Dec.2016.