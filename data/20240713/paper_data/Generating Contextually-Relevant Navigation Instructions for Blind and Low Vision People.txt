Generating Contextually-Relevant Navigation Instructions
for Blind and Low Vision People
Zain Merchant1 Abrar Anwar1 Emily Wang1 Souti Chattopadhyay1 Jesse Thomason1
Abstract—Navigating unfamiliar environments presents sig-
nificant challenges for blind and low-vision (BLV) individuals.
In this work, we construct a dataset of images and goals
across different scenarios such as searching through kitchens
or navigating outdoors. We then investigate how grounded in-
structiongenerationmethodscanprovidecontextually-relevant
navigational guidance to users in these instances. Through
a sighted user study, we demonstrate that large pretrained
language models can produce correct and useful instructions
perceivedasbeneficialforBLVusers.Wealsoconductasurvey
and interview with 4 BLV users and observe useful insights on
Fig. 1: We formulate the problem of providing contextually-
preferences for different instructions based on the scenario.
relevant navigational instructions to blind and low vision
I. INTRODUCTIONANDBACKGROUND (BLV) people as a grounded instruction generation task,
Nearly 253 million people struggle with visual impair- which we then evaluate with sighted and BLV participants
ment worldwide, where 36 million of these individuals are in a user study.
blind [1]. Dealing with the complexities of daily life poses
significantchallengesfortheseindividuals,particularlywhen
exploring unfamiliar environments. Traditional aids such existing systems gather different kinds of information from
as canes and guide dogs are vital in facilitating mobility the environment, using basic object detection [9] often com-
and independence. However, these tools have limitations in bined with auditory output [10] based on templates [11] to
conveyingtherichvisualinformationthatsightedindividuals generatedescriptionsorsimplylistobjects[12].Whilethese
rely on for navigation and object recognition. approachesoffervaluableassistance,theyoftenoverlookthe
Therehasbeenrecentgrowthinusingvision-and-language importanceofcontextandrelevanceindeliveringinstructions
models as visual assistants that can interactively commu- to BLV users [10], [13].
nicate with a user to provide feedback [2]. Additionally, Descriptions are subjective and depend on the user’s con-
after interviews with blind and low vision individuals, prior text[10],[15],andoverlygenericorunnecessaryinformation
work has noted a critical issue with the use of guide dogs: is often not preferred by users [13], [16], [17]. Noting these
the communication from the user to the dog is unimodal. flaws with prior work and considering recent work on the
However, there may be questions users want to ask, such as importance of context for BLV participants [10], [13], we
“Isitsafetocrossthestreet?”[3].Ifresearchersarebuilding propose to augment such systems with LLMs and VLMs
robotic guide dogs, the authors posit that any robotic guide and understand whether these methods are able to generate
dogshouldhandlecomplexinteractionswithausertoanswer contextually-relevant instructions.
these types of questions. As the companies and the research
II. PROBLEMSETTING
community begin to integrate large language models into
these robot systems, it is important to understand the role We want to generate instructions that are useful and goal-
of a language model’s contextual understanding capabilities aware for BLV users, as shown in Figure 1. We frame this
in providing personalized, informative feedback tailored to problem as a grounded instruction generation task, where a
a user’s specific goals and surroundings. In this work, we modelS(w 0,...,w N|g,o)usesanegocentricimagetogenerate
focusonnavigationassistanceandinvestigatetheusefulness an instruction w=[w 0,...,w N] that describes a route for an
and contextual relevance of generated instructions for navi- BLVusertoreachagoalggivenanimageobservationo.The
gationalassistanceusinglargelanguagemodels(LLMs)and goal g is semantic task context from the user like “How do
vision-and-language models (VLMs). I get to the building’s entrance?” The amount of objects and
Existing work in the field of blind and low vision (BLV) semantic information that can be conveyed to a user based
navigation assistance has primarily focused on lower-level on an image is often innumerable; however, only a subset
navigation tasks such as obstacle avoidance [4]–[8]. Many of this information is useful to the user to accomplish their
goal. In this work, we are interested in how the environment
1 Zain Merchant, Abrar Anwar, Emily Wang, Souti Chattopadhyay, and goal impact the usefulness of the generated instruction.
and Jesse Thomason are with the Thomas Lord Department of Computer
VizWiz [14] is a collection of photographs captured by
Science,UniversityofSouthernCalifornia,LosAngeles,CA,USA
Contact:zsmercha@usc.edu BLV participants and serves as a basis for formulating tasks
4202
luJ
11
]LC.sc[
1v91280.7042:viXraFig. 2: Left: We select 48 images from indoor and outdoor environments in VizWiz [14] and annotate them with navigation
goals. Middle: We design three instruction generation methods, described further in Section III. Right: These generated
instructions are then evaluated in a user study with sighted and BLV participants.
related to object detection and visual question-answering. LLM-generated Instructions. Text-only LLMs have
Since VizWiz does not consider navigation instruction good commonsense reasoning abilities given text input;
generation, we created our own dataset by selecting 48 however, unlike VLMs, they cannot take images as input. In
images from VizWiz that were relevant to navigation from ordertogenerategroundednavigationinstructions,wetakea
fourdifferentenvironments:offices,kitchens,generalindoor, Socraticmodel[19]approach,whereweprovidetheoutputs
andgeneraloutdoorsettings.Wethenassignedeachimagea of various off-the-shelf models as additional information for
goal,suchas“WhereistheTVremote”,whichinnavigation the LLM. We use off-the-shelf object detection [18], depth
tasks,maynotbevisibleinasingleframeduetotheobject’s estimation [20], image captioning, and optical character
size or placement. recognition. These inputs are formatted into a prompt with
Inoursetting,weareconsideringonlyasingleimage,but thegoalandafewin-contextexamples.WeuseGPT4[21]as
in practical applications, the image alone may not contain theLLMandgetcontextuallyrelevant,groundedinstructions
the answer to the goal. For example, in the case of ”Where as the output.
is the TV remote”, the remote may not be visible in the VLM-generated Instructions. VLMs are able to take
image and may require reasoning about potential locations images as input, so rather than a Socratic models approach,
rather than an exact answer. To capture these kinds of we use GPT-4 Vision [21] with the image as an input and
problems, we construct a split such that 20% of our dataset prompt the model to generate an instruction.
are Hard examples which require a model to reason more
IV. STUDYDESIGN
extensively on how to provide instructions for the goal. The
complementary split is referred to as the Easy split. Each We conducted two IRB-approved human subject studies
of the images was annotated by sighted volunteers with with sighted and BLV participants.
instructions that would solve the goal.
A. Sighted User Study
III. GROUNDEDINSTRUCTIONGENERATION In addition to the study with BLV participants, we be-
lieve that sighted participants can play an important role
We compare methods for generating navigation instruc- in validating the correctness of the generated instructions
tions from single images paired with semantic task context. along with gaining an understanding of what they believe
Humaninstructions.Asapointofreferenceformachine- is useful. Sighted participants were asked to take a survey
generated instructions, we sent selected image-goal pairs to to rate instructions given an image and a goal. They rated
four human annotators tasked with generating instructions the instructions in terms of Correctness and Usefulness on
for BLV users. a 1 to 7 Likert scale based on the following definition. We
Template Instructions. Past work [11] extracted the ob- define Correctness as how accurate the instruction is with
ject of interest from the user’s goal query. For example, for respect to directions and objects in the image. This metric
the sentence “where is the textbook”, this baseline extracts also verifies the instruction generation methods generate
the object “textbook”, then detects where the object is using accurate instructions. We define Usefulness as how useful
the OWL-ViT open-vocabulary object detector [18]. This the instruction would be to a BLV user to help achieve
method then localizes the object into nine predefined areas their goal, considering its relevance and safety. We recruited
(top left, center, bottom right, etc.). If a user asks about the eightsightedparticipants,whoeachviewedsiximagesacross
location of a microwave, the system can concisely respond: four methods. We acknowledge the Usefulness scores from
“The microwave is at the top left.” sightedusersarenotlikelythesameasthepreferencesfromaCorrectness Usefulness 7 office
indoor
Templated 3.85±1.95 1.96±1.05
6 kitchen
LLM-based 4.73±1.61 4.24±1.38
outdoor
VLM-based 4.75±1.78 4.52±1.70
5
Human 5.46±1.46 4.73±1.83
4
TABLE I: Sighted user ratings for Correctness and Use-
fulness. Human-annotated instructions are more accurate 3
compared to the other methods, but the LLM- and VLM-
2
generated instructions were rated similarly useful.
1
EasySplit HardSplit Templated LLM-based VLM-based Human
Method
Templated 2.11±1.09 1.40±0.70 Fig. 3: Sighted participant Usefulness ratings over the
LLM-based 4.39±1.75 5.00±1.49
generated instructions for 48 image-goal pairs across four
VLM-based 4.20±1.30 4.40±1.71
Human 4.58±1.80 5.30±1.95 methods separated by environment. VLM-based instructions
had similar ratings across environments to humans. The
TABLE II: Usefulness scores for the Easy and Hard splits
LLM-based model was rated slightly less useful.
from the 48 image-goal pairs. Interestingly, the Easy split
was rated lower than the Hard split.
environments. We find that the VLM and human-generated
instructions have similar usefulness score distributions com-
BLV participant; however, since it is difficult to recruit BLV
pared to the LLM-generated and templated instructions. We
participants at scale, we were interested in also collecting
alsoobservethatinstructionsgeneratedforofficeandgeneral
these Usefulness scores.
indoor environments are rated more useful than kitchen
B. BLV User Study and outdoor instructions. This trend could be because the
instruction generation methods have to reason about more
Due to the low incidence of the BLV population, we
complex scenes, or users having different expectations in
recruited three blind and one low-vision participant. Similar
these scenes.
to the sighted user study, each image is rated by a BLV
participant for Usefulness. We do not collect Correctness B. BLV Survey Results
ratings for the BLV participants since many participants
Thesighteduserresultsindicatethatthegeneratedinstruc-
would not be able to compare the instruction to the scene
tions are correct and show trends about the role of context.
itself due to the level of their visual impairment. After the
Though these results provide insights about the generated
survey, we conducted a semi-structured interview. Survey
instructions, we focus on the quantitative and qualitative
questions were aimed to elicit their thoughts on navigating
results from our BLV user study.
different environments, including social spaces, and their
BLVparticipantsratemethodsaslessusefulcompared
thoughts about the kinds of methods they experienced.
to sighted participants. As shown in Table III, we find the
V. RESULTS LLM- and VLM-generated instructions were rated slightly
lowertothehuman.Unsurprisingly,thetemplateinstructions
A. Sighted Survey Results
were consistently not useful. Due to the small size of our
With our sighted user survey, we find that users find BLV user study, we will focus primarily on the qualitative
the generated instructions correct and useful, which shows semi-structured interview in the next section.
promise for these methods to be tested with BLV users.
C. BLV Qualitative Interview
Generated instructions are similarly useful to human
annotated instructions. Table I shows the aggregated cor- Different environments change preferences on the
rectness and usefulness scores given by sighted users across kindsofinstructions.Participantsindicatedthatinconfined
methods. Users found human-generated instructions more spaces such as a kitchen, they have a preference for less
accurate than LLM- and VLM-generated instructions. In broad instructions, whereas outdoor, open spaces can be
contrast, the difference between usefulness ratings between morebroad(e.g.“walkforwarduntilyoureachthecorner”).
human, LLM-, and VLM-generated instructions was much Thelow-visionparticipantnotedthatlightingandaudiocues
smaller. The difference in usefulness ratings between the could provide a means for useful guidance.
VLM and LLM could be explained by harder-to-answer Generative methods rely on visual cues. Participants
instancesbenefitingfromtheinputoftheentireimage,while noted that some responses relied on visual cues such as “it’s
easy-to-answer instances can be solved more directly with near the big sign” which is not useful. Their suggestion was
object detectors, as supported by Table II. tomakethesystemmorespecificandtofocusonintegrating
Users find different amounts of usefulness of instruc- morespatialawarenessasthoseinstructionsaremostuseful.
tions depending on the environment. Figure 3 shows box Participants preferred specific directions (e.g. “take a few
plotsoftheusefulnessscoresofsightedusersacrossdifferent steps to your right”, “10 degrees to the right”) over vague
gnitaR
ssenlufesUMethod Usefulness [3] H. Hwang, H.-T. Jung, N. A. Giudice, J. Biswas, S. I. Lee, and
D.Kim,“Towardsroboticcompanions:Understandinghandler-guide
Templated 2.00±1.29
doginteractionsforinformedguidedogrobotdesign,”Conferenceon
LLM-based 3.97±1.78
HumanFactorsinComputingSystems(CHI),2024.
VLM-based 4.45±1.73
[4] S.RealandA.Araujo,“Navigationsystemsfortheblindandvisually
Human 4.03±1.80
impaired:Pastwork,challenges,andopenproblems,”Sensors,2019.
[Online].Available:https://www.mdpi.com/1424-8220/19/15/3404
TABLE III: Usefulness ratings from our BLV participants.
[5] A. Cassinelli, C. Reynolds, and M. Ishikawa, “Augmenting spatial
The VLM-based instructions were rated as more useful than awareness with haptic radar,” in IEEE International Symposium on
all of instructions. WearableComputers,2006.
[6] R. N. Kandalan and K. Namuduri, “Techniques for constructing
indoornavigationsystemsforthevisuallyimpaired:Areview,”IEEE
TransactionsonHuman-MachineSystems,2020.
ones (e.g. “the table is in the center”). However, they [7] Y.Lin,K.Wang,W.Yi,andS.Lian,“Deeplearningbasedwearable
assistive system for visually impaired people,” in ICCV Workshops,
noted that specificity is not always useful. For example,
Oct2019.
knowinghowmanyobjectsareonatablemightbetoomuch [8] Z. Bauer, A. Dominguez, E. Cruz, F. Gomez-Donoso, S. Orts-
information to be given at once and could be asked as a Escolano,andM.Cazorla,“Enhancingperceptionforthevisuallyim-
pairedwithdeeplearningtechniquesandlow-costwearablesensors,”
separate question.
PatternRecognitionLetters,2020.
What makes a useful navigation assistant? Several [9] M.Leo,A.Furnari,G.G.Medioni,M.Trivedi,andG.M.Farinella,
participants indicated that an ideal system would tell them “Deep learning for assistive computer vision,” in ECCV Workshops,
September2018.
where things are laid out in relation to other things that they
[10] H. Walle, C. De Runz, B. Serres, and G. Venturini, “A survey on
can reason about. For example, “the sponge is in the bottom recent advances in ai and vision-based methods for helping and
right of the basin” is helpful, but “the bench is to the right guiding visually impaired people,” Applied Sciences, 2022. [Online].
Available:https://www.mdpi.com/2076-3417/12/5/2308
ofthesign”isnot.Incontrasttopriorworkandsystemslike
[11] K. Thakoor, N. Mante, C. Zhang, C. Siagian, J. Weiland, L. Itti,
GoogleMaps,participantsnotedfrustrationwithinstructions and G. Medioni, “A system for assisting the visually impaired in
that stated a precise number of feet to walk, especially since localization and grasp of desired objects,” in ECCV Workshops,
L.Agapito,M.M.Bronstein,andC.Rother,Eds.,2014.
these systems cannot tell the user when they have reached
[12] S.Malek,F.Melgani,M.L.Mekhalfi,andY.Bazi,“Real-timeindoor
that distance. Instructions like “walk until you reach the scene description for the visually impaired using autoencoder fusion
corner” would resolve this issue. Thus, leveraging the re- strategies with visible cameras,” Sensors, 2017. [Online]. Available:
https://www.mdpi.com/1424-8220/17/11/2641
lationship between the goals with one’s surroundings can be
[13] K. M. P. Hoogsteen, S. Szpiro, G. Kreiman, and E. Peli, “Beyond
helpful.OneparticipantfoundtheLM-generatedinstructions the cane: Describing urban scenes to blind people for mobility
tobewordyorcondescending,motivatinginvestigationsinto tasks,” ACM Trans. Access. Comput., 2022. [Online]. Available:
https://doi.org/10.1145/3522757
preferences in how these models communicate information.
[14] D.Gurari,Q.Li,A.J.Stangl,A.Guo,C.Lin,K.Grauman,J.Luo,and
J. P. Bigham, “Vizwiz grand challenge: Answering visual questions
VI. CONCLUSION,ETHICS,ANDLIMITATIONS
fromblindpeople,”inCVPR,2018.
LLM and VLM-based methods for grounded instruction [15] E. Kreiss, C. Bennett, S. Hooshmand, E. Zelikman, M. R. Morris,
andC.Potts,“Contextmattersforimagedescriptionsforaccessibility:
generation show great promise in integrating with assis-
Challengesforreferencelessevaluationmetrics,”EMNLP,2022.
tive technologies. However, a significant challenge associ- [16] M. K. Scheuerman, W. Easley, A. Abdolrahmani, A. Hurst, and
ated with using these models is their tendency to produce S. Branham, “Learning the language: The importance of studying
writtendirectionsindesigningnavigationaltechnologiesfortheblind,”
hallucinations or inaccurate generations. Poorly generated
inCHIExtendedAbstractsonHumanFactorsinComputingSystems,
instructionscanleadtoconfusionandputusersinpotentially 2017.[Online].Available:https://doi.org/10.1145/3027063.3053260
hazardous situations. [17] M. A. Williams, C. Galbraith, S. K. Kane, and A. Hurst, “”just let
thecanehitit”:howtheblindandsightedseenavigationdifferently,”
We also recognize the reference instructions written by
in ACM SIGACCESS Conference on Computers & Accessibility, ser.
sighted annotators may not be tailored to how a BLV user ASSETS,2014.[Online].Available:https://doi.org/10.1145/2661334.
may want to be given instructions, as the annotators were 2661380
[18] M.Minderer,A.Gritsenko,A.Stone,M.Neumann,D.Weissenborn,
not expertly trained to communicate with BLV users.
A.Dosovitskiy,A.Mahendran,A.Arnab,M.Dehghani,Z.Shenetal.,
LLMs and VLMs are also susceptible to biases present “Simpleopen-vocabularyobjectdetection,”inECCV,2022.
in their training data [22]. It is important to ensure these [19] A.Zeng,M.Attarian,B.Ichter,K.Choromanski,A.Wong,S.Welker,
F.Tombari,A.Purohit,M.Ryoo,V.Sindhwani,J.Lee,V.Vanhoucke,
technologies are trained on diverse data sets that accurately
and P. Florence, “Socratic models: Composing zero-shot multimodal
represent the variety of cultures and environments that may reasoningwithlanguage,”2022.
beencountered so thattheseassistivetechnologies canserve [20] D. Kim, W. Ga, P. Ahn, D. Joo, S. Chun, and J. Kim, “Global-
local path networks for monocular depth estimation with vertical
users in an equitable manner. By emphasizing the role of
cutdepth,” CoRR, vol. abs/2201.07436, 2022. [Online]. Available:
context in the generation of instructions for BLV users, we https://arxiv.org/abs/2201.07436
hopeourworkcaninitiateacommunitydiscussiononhowto [21] J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman,
D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4
handle the many possible scenarios a user could experience.
technicalreport,”arXivpreprintarXiv:2303.08774,2023.
[22] T. Srinivasan and Y. Bisk, “Worst of both worlds: Biases compound
REFERENCES in pre-trained vision-and-language models,” NAACL Workshop on
GenderBiasinNaturalLanguageProcessing,2021.
[1] P. Ackland, S. Resnikoff, and R. Bourne, “World blindness and
visualimpairment:despitemanysuccesses,theproblemisgrowing,”
Communityeyehealth,2017.
[2] BeMyEyes,“Bemyeyes,”https://www.bemyeyes.com/.APPENDIX 2.Youcanimaginethatthecameraisattachedtotheblind
persons head, so the starting point will be the perspective of
A. LLM Prompt
the camera.
I’m going to give you some information about an image. 3. If you don’t see the object, note that you don’t see it,
I will then give you a goal. Using the information about the but succinctly explain where it could be.
image, I’d like you to give 2-3 sentences telling me how I 4. Responses should be around 2 sentences.
can achieve my goal and navigating me. This will be used 5. The person using this system might not know much
so that a blind person can navigate. aboutcomputers,souseperson-centriclanguage.Ratherthan
Rules: saying ”the detected object”, say ”I see a...”
1. Please give me instructions like left and right and not 7. Do not discuss how many steps it should take to reach
coordinates. So never use pixel positional information. an object, but instead talk about directions. For example, do
2.Youcanimaginethatthecameraisattachedtotheblind not say ”take 3 steps forward to reach the tv remote” and
persons head, so the starting point will be the perspective of instead say ”the tv remote is in front of you” Example: The
the camera. fridge is slightly to your left and forward. Please be careful
3. If you don’t see the object, note that you don’t see it, to not bump into the table and chairs as you move.
but succinctly explain where it could be.
4. Responses should be around 2 sentences. C. Semi-structured Interview Questions:
5. The person using this system might not know much
Thesearethefollowingquestionswhichweusedtoguide
aboutcomputers,souseperson-centriclanguage.Ratherthan
our semi-structured interviews, which are in several broad
saying ”the detected object”, say ”I see a”
categories.
6. The objects are printed based on confidence scores. So
1) Streets, homes, grocery stores, etc
if an object is printed multiple times, just use the first one’s
coordinates to help come up with your directions. • How did you feel about the responses in the
context of different environments?
7. Do not discuss how many steps it should take to reach
an object, but instead talk about directions. For example, do – Streets
not say ”take 3 steps forward to reach the tv remote” and – Grocery stores
instead say ”the tv remote is in front of you” – Homes
Here’s an example: The image size is 1500 x 1000 • What types of concerns did you have when nav-
Image caption: a kitchen with a table, chairs, and a igating the different environments? [Nudge about
refrigerator safety, completeness, moving components]
Objects: • Were there moments the responses didn’t address
Detected dining table at location (758, 767) with size 718 your concerns?
x 262 and depth 1 meters • Can you share some examples?
Detected chair at location (529, 739) with size 169 x 229 • What aspects or characteristics of the responses
and depth 1 meters were helpful?
Detected refrigerator at location (150, 523) with size 295 2) Familiar vs unfamiliar environments
x 659 and depth 1 meters
• What information helps you the most when navi-
Goal: Where is the fridge
gating unfamiliar environments?
Response: The fridge is slightly to your left and forward.
• For example, what kinds of objects such as stairs
Please be careful to not bump into the table and chairs as
would you find helpful to know when navigating?
you move.
• How much detail about these objects would be
The image size is {}
good to know?
Image caption: {}
3) Navigating in social spaces
Objects:
• What challenges do you face when navigating
Detected {} at location {} with size {} and depth {}
social spaces like restaurants or events?
meters
• What kind of information will help you navigate?
Text in image: {}
4) Interactions with Others
Goal: {}
• How do you prefer others to assist you when
B. VLM Prompt: navigating in social or public spaces, if at all?
I’d like you to give 2-3 sentences telling me how I can • Have you experienced any difficulties or discom-
fort when seeking assistance from strangers, ac-
achieve my goal and navigating me. This will be used so
quaintances, or online services such as Be My
that a blind person can navigate.
Eyes while navigating?
Rules:
1. Please give me instructions like left and right and not 5) Relevant information in assisted navigation
coordinates. So never use pixel positional information. • Role of feedback for metric navigation• In what situations do you prefer to navigate in-
dependently, and when would you like further
assistance?
• Consider you are washing dishes, you might have
direct information where a system might know
where an object is, but sometimes it doesn’t. For
example,ifyouwanttofindasponge,woulditbe
helpfulforthesystemtotellyouaboutinformation
such as where the dishes or sink is?
6) Current Technology/Tools
• What technologies or tools do you currently use
for navigation?
• Are there any specific features or functionalities
you wish existing navigation tools had to better
assist you?
• Have you encountered any barriers or challenges
in using navigation technology or tools?
7) Emotional Wellbeing
• Have you ever felt overwhelmed or stressed by
navigation tasks? What would help you feel less
overwhelmed or stressed?
• Would reassurance help at all?
• What support or resources would you find helpful
in managing navigation-related challenges?
8) Future
• Whatimprovementsdoyouhopetoseeinnaviga-
tion assistance for the blind or visually impaired
community?
9) Evaluation of system/generated responses
• You experienced a few different methods we had
designed. What would your ideal system that gen-
erates instructions look like to you?
• Think back to a helpful example. What did you
likeaboutthosemethodsandwhatdidn’tyoulike?
• Think back to an unhelpful example...
• How useful would an ideal system like this be to
you?
• Doyouthinksystemssuchasthiswouldhelpyou
gain independence? Feels wrong to just ask them
this directly.
• Were there any other challenges/issues about the
methods that you faced?
10) General Questions
• Do you have any recommendations as to adjust-
ments that would enhance the experience?
• Is there anything else you would like to share?
• Are there any additional concerns or aspects that
you feel as though should be addressed?