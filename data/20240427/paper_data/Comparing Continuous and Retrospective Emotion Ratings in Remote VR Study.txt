Comparing Continuous and Retrospective Emotion
Ratings in Remote VR Study
Maximilian Warsinke Tanja Kojic´ Maurizio Vergari
Quality and Usability Lab Quality and Usability Lab Quality and Usability Lab
Technische Universita¨t Berlin Technische Universita¨t Berlin Technische Universita¨t Berlin
Berlin, Germany Berlin, Germany Berlin, Germany
warsinke@tu-berlin.de tanja.kojic@tu-berlin.de maurizio.vergari@tu-berlin.de
Robert Spang Jan-Niklas Voigt-Antons Sebastian Mo¨ller
Quality and Usability Lab Immersive Reality Lab Quality and Usability Lab
Technische Universita¨t Berlin Hochschule Hamm-Lippstadt Technische Universita¨t Berlin and DFKI
Berlin, Germany Lippstadt, Germany Berlin, Germany
spang@tu-berlin.de jan-niklas.voigt-antons@hshl.de sebastian.moeller@tu-berlin.de
Abstract—This study investigates the feasibility of remote skewing the results [3]. This “observer effect” could be even
virtualreality(VR)studiesconductedathomeusingVRheadsets more noticeable in VR, where the presence of other people
and video conferencing by deploying an experiment on emotion
can affect sensation, immersion, and overall experience.
ratings. 20 participants used head-mounted displays to immerse
Therefore, remote testing allows for VR trials with partici-
themselves in 360° videos selected to evoke emotional responses.
The research compares continuous ratings using a graphical pants in their homes or chosen locations, while connecting to
interface to retrospective questionnaires on a digitized Likert researchersthroughwebvideostreaming.Furthermore,remote
Scaleformeasuringarousalandvalence,bothbasedontheself- testingallowsonetointeractwithparticipantsinenvironments
assessment manikin (SAM). It was hypothesized that the two in which they are most comfortable, such as their native con-
different rating methods would lead to significantly different
text [4]. This scenario is useful when researching real-world
values for both valence and arousal. The goal was to investigate
whether continuous ratings during the experience would better VR applications, such as remote work collaboration tools,
reflect users’ emotions compared to the post-questionnaire by virtualschools,andtherapeuticinterventions,andinvestigating
mitigating biases such as the peak-end rule. The results show various research problems, from usability and user experience
significant differences with moderate to strong effect sizes for
to cognitive and emotional reactions in VR.
valence and no significant differences for arousal with low
Some efforts have already been reported, such as the re-
to moderate effect sizes. This indicates the need for further
investigation of the methods used to assess emotion ratings in search and investigation of the potential benefits of VR and
VR studies. Overall, this study is an example of a remotely extended reality (XR) technology in remote settings. These
conducted VR experiment, offering insights into methods for include investigating the use of data collection capabilities
emotion elicitation in VR by varying the timing and interface
built into head-mounted displays (HMDs), such as hand and
of the rating.
gaze tracking, and the benefits of mobility and repeatability
Index Terms—Human-Computer Interaction, Extended Re-
ality, Head-mounted Displays, Immersive Experience, Virtual provided by XR in experimental settings [5]. Despite their
Reality, Emotional Responses, User Experience usefulness in other domains, the VR research community is
careful to embrace remote engagement methods [6].
I. INTRODUCTION To begin such research based on remote testing, an ex-
VR technology has received substantial attention from aca- periment that would normally take place in a laboratory
demics over the last several years [1]. Many of these research was conducted. The experiment investigates the evaluation of
activities have typically been carried out inside a controlled emotionsafterandduringviewingof360°videosinVRasan
laboratory environment, with researchers and participants lo- approachtogetclosertotheuser’s“real”emotions.Thevalues
cated near one another [2]. The appearance of the COVID-19 of valence and arousal according to the circumplex model
epidemic has highlighted the importance of investigating and of effect [7] were chosen as the self-report measurements.
putting into practice various distant recruiting tactics. In this study, two rating methods were developed that differ
WhileconductingVRstudiesinthelaboratoryhasprovided in interface and timing, both based on the SAM rating scale
usefulinsightsintouserbehavior,preferences,andusabilityof [8].Thecontinuousratingmethodrequiresparticipantstorate
VRapps,itisnotwithoutlimits.Onefactortoconsideristhe their emotions multiple times on a 2D interface while 360°
possibility of a bias introduced by the testing environment. In videosarerunninginthebackground.Theretrospectiverating
the laboratory, the physical presence of researchers, modera- method is a digitized Likert scale that is incorporated as a
tors,andobserversmayimpactparticipants’emotions,thereby floating interface into the VR environment and is shown after
©2024IEEE.Personaluseofthismaterialispermitted.PermissionfromIEEEmustbeobtainedforallotheruses,inanycurrentorfuturemedia,including
reprinting/republishingthismaterialforadvertisingorpromotionalpurposes,creatingnewcollectiveworks,forresaleorredistributiontoserversorlists,or
reuseofanycopyrightedcomponentofthisworkinotherworks.DOIandlinktooriginalpublicationwillbeaddedassoonastheyareavailable.
4202
rpA
52
]CH.sc[
1v78461.4042:viXrathevideoends.Thefollowinghypotheseswereformulatedfor include electroencephalography (EEG) [22], skin conductance
this experiment: capturing [23], and eye tracking [24]. Novel studies have
used head movement during experiences to successfully pre-
Hypothesis1(H ):Thereisastatisticallysignificantdiffer- dict emotional states [25], [26]. Subjective emotion ratings
1
ence in valence ratings between participants’ ratings from the are commonly reported as valence and arousal following
continuous and retrospective rating methods when watching the circumplex model of effect [27]. To gather data, post-
360° videos in VR. experience questionnaires such as SAM have been employed
Hypothesis2(H ):Thereisastatisticallysignificantdiffer- [28]. This self-assessment questionnaire uses symbolic depic-
2
ence in arousal ratings between participants’ ratings from the tions of emotions combined with a Likert scale for appraisal.
continuous and retrospective rating methods when watching This led to the further development of graphical rating in-
360° videos in VR. terfaces like the “EmojiGrid” [29] developed by Toet et al.
which was later used for emotion elicitation of 360° videos
in VR [30]. Continuous ratings during the experience have
II. RELATEDWORK been explored to observe a post-experience effect on emotion
A. Remote VR-studies estimationwithoutshowingsignificantdifferences[31].When
working with self-reports, psychological biases must be taken
VR devices’ popularity in private households has increased
into account, such as the peak-end rule, which describes an
in the last couple of years with ownership rates reaching 12%
overestimation of an effect when a situation is evaluated
intheUnitedStatesbytheyear2022[9].Researcherssuggest
retrospectively [32].
conducting remote VR studies, reporting reliable results com-
pared to results from the lab [10]. Using participants-owned III. METHODS
HMDs could increase the sample sizes of experiments with
A. Experimental Design
thedisadvantageofdrawingfromamorehomogeneousgroup,
The experimental design was used to fit a previous study
consistingofproficientVRusers[11].Alternatively,delivering
[31]. Therefore, a within-subject design was chosen such that
VR setups to the homes can foster diverse participants with
every participant gave ratings for both rating methods, elim-
the risk of insufficient technical knowledge to set up the VR
inating the influence of individual variability. The sequence
devices [12]. Mottelson et al. reported a diverse and high
of the rating method was alternated for every participant to
number of participants at lower costs in two case studies on
counterbalance the possible sequence effects. As a question-
unsupervised remote experiments [13]. A supervised remote
naire,theself-assessmentmanikin(SAM)waschosentoassess
studywithascheduledZoomcallhasbeenexploredbyMathis
emotions to ensure comparability with previous research.
et al. who describe the potential to complement conventional
During the experiment, the answers to the rating scales were
experiments done in the laboratory [14].
saved on the device’s storage and could later be extracted for
B. Inducing Emotion in VR
analysis.
Usingvirtualrealityexperiencestoinducehumanemotions
B. Participants
is a widely used approach in emotion research and human-
computer interaction (HCI) [15], [16]. Common practices A total of 21 participants between the ages of 20 and 29
utilize360°videos,3Denvironments[17],andVRgames[18] took part in the experiment, including 18 men, 2 women and
as stimuli to evoke emotional reactions in experiments. 1non-binaryperson.Duetoincompleteratings,datafromone
Whilethree-dimensionalVRenvironmentsappeartoinduce participantwereexcludedfromthedataset,resultingin20par-
a higher feeling of presence [1], 360° videos seem to be an ticipants for the analysis. All the remaining participants were
efficient alternative to use as stimuli for experiments, because students, apart from one who worked as a Python developer.
of higher online availability and fewer development efforts Their age ranged from 20 to 29. Among the 7 participants
compared to 3D content [19]. Additionally, while current who self-reported having vision impairments, 6 wore glasses
advancements in 3D modeling allow for a realistic depiction and one wore lenses. Regarding the experience with the VR
ofvariousenvironments,creatinghumancharactersoranimals system, the distribution was as follows: 9 participants used a
is more challenging, causing unintended responses such as VR system 1 to 2 times, 6 participants used it 3 to 5 times, 1
an uncanny valley effect [20]. By contrast, 360° videos can participant used it 6 to 9 times, and 4 participants used it 10
offer a more authentic experience by capturing real humans or more times. The mean Affinity for Technology Interaction
and animals in real-world scenes using 360° cameras. For Score (ATI) [33] of the participants across all items was 4.76
this purpose, a database of 360° videos has been collected (SD =0.50).
and annotated [21]. The database features full spherical view
C. Stimuli
footage of diverse situations that can be explored naturally
The 360° clips used in the experiment were selected from
using an HMD.
theStanfordpublicdatabase1 thatconsistof73videosintotal
Established methods for assessing emotions include quanti-
tative and qualitative approaches that use physiological mea-
1https://vhil.stanford.edu/public-database-360-videos-corresponding-
surements and questionnaires. Physiological measurements ratings-arousal-and-valenceTABLEI Participants were then allowed to start the experiment in a
LISTOFALLVIDEOSWITHTHEIRIDFROMLIETAL.[21]. practicemodeinwhichtheycouldgetfamiliarwiththeHMD
and its controls. Additionally, an explanation of the SAM and
ID Name Slice Quadrant
68 Jailbreak360 2:39–3:39 LVHA the concepts of valence and arousal were given. During the
20 WarKnowsnoNation1 4:44–5:44 LVHA main part of the experiment, the participants watched and
20 WarKnowsnoNation2 3:14–4:14 LVHA
rated4randomlyselected360°videos(1fromeachquadrant).
21 ZombieApocalypseHorror 0:40–1:40 LVHA
63 NasaRocketLaunch 3:15–4:15 HVHA The ratings were either given retrospectively or continuously
50 Puppies360 0:04–1:04 HVHA as predetermined for each participant. Subsequently, another
64 SurroundedbyElephants 0:30–1:30 HVHA
4 randomly selected videos (1 from each quadrant) were
69 WalktheTightRope 0:27–1:27 HVHA
chosen for the alternative rating method. Once participants
8 Happyland360 2:00–3:00 LVLA
18 TheDisplaced1 2:23–3:23 LVLA watched 8 videos, they were asked to complete the final
18 TheDisplaced2 3:35–4:35 LVLA questionnaire online, which asked for a subjective evaluation
16 SolitaryConfinement 0:00–1:00 LVLA
of the rating methods and gave participants the opportunity to
43 AlicetheBaby 0:00–1:00 HVLA
26 GettinglickedbyaCow 0:00–1:00 HVLA leave feedback. After debriefing, the Zoom call was closed.
22 GreatOceanRoad 0:00–1:00 HVLA
32 MalaekahanaSunrise 1:20–2:20 HVLA
E. Rating Methods
with their corresponding valence and arousal values. Video 1) Retrospective: In the retrospective rating method, the
materials are mostly publicly available on popular websites,
participants first watched the video entirely. They were then
such as YouTube [21]. The clips used were selected based on
confronted with a post-stimulus interface, where they could
theirbaselinevalues.Toensureequaldistributionofemotions,
rate by pointing and selecting with the controller and con-
4clipswereusedfromeachquadrantofthecircumplexmodel
firming it with a submit button (Fig. 1). Valence and arousal
of affect [7].
ratings were collected successively using a digitized version
of the SAM, including the 9-point Likert scale and the five
• LVHA (Low valence, high arousal) corresponding pictures [8].
• HVHA (High valence, high arousal) 2) Continuous: For the continuous rating method, a two-
• LVLA (Low valence, low arousal) dimensional grid was overlaid onto the 360° videos (Fig.
• HVLA (High valence, low arousal) 2), where participants could place their ratings using the
Meta Quest controller. The axes on the grid represent valence
Each video was shortened to a duration of 60 seconds to pre- and arousal and include the pictograms of the original SAM
vent differences in results arising from varying lengths. This questionnaire.Ratingsweregivenaftertheplayofanauditory
consistency is particularly crucial for retrospective ratings, in cue that occurred every 10 seconds, resulting in 5 ratings for
which longer video clips may lead to variations in the review the 60-second duration of the video. When rating, the video
of emotions. Due to a lack of video clips from the quadrant continued to play in the background with no interruption,
with low valence and high arousal (LVHA), two clips were which resulted in non-mandatory voting. If a participant did
taken from the same source video, depicting different scenes not provide a rating after the cue, the data point was skipped.
from the video. The same procedure was repeated for low- The ratings of each video were then averaged to get a single
valence and low-arousal quadrants (LVLA). The complete list value for both valence and arousal.
ofvideoclipsusedisprovidedinTableIwiththeirrespective
quadrants.
D. Procedure
The experiment was conducted and supervised in a remote
setting and approved by the local ethics committee of the
Technical University of Berlin. Participants received the VR-
Setup with the pre-installed experiment application via parcel
delivery and then showed up to a scheduled appointment to
meet on Zoom with the instructor. The study was conducted
using a Meta Quest HMD. The procedure was explained, and
theparticipantsdigitallysignedaconsentform.Afterward,the
participantsweregivenanonlinequestionnaireondemograph-
ics and 9 items from the Affinity for Technology Interaction
(ATI) questionnaire. The participants were then requested to
cast from the Meta Quest to their computer and share their Fig.1. RetrospectiveratinginterfaceforvalencewithadigitizedLikertscale
screen so that the instructor could retrace the application. basedontheSAM.IV. RESULTS
A. Correlation to Baseline Values
The mean retrospective arousal and mean retrospective va-
lencevaluesforeachvideowerecalculatedtoassessPearson’s
correlation coefficients with the baseline values from Li et
al. [21]. The analysis revealed a strongly positive correlation
(r = 0.816,p < 0.001) between retrospective valence ratings
and baseline valence values. A moderately to strong positive
correlation (r = 0.668,p = 0.003) was observed between
retrospective arousal ratings and baseline arousal values.
B. Valence, Continuous versus Retrospective
Fig.2. Continuous2D-overlayduringthevideobasedontheSAMwiththe In group LVHA, the valence was lower in the retrospective
x-axisdenotingvalenceandthey-axisdenotingarousal.
rating than in the continuous rating. A Wilcoxon signed-rank
test showed no significant difference (z = 1.71,p = 0.086),
F. Statistical Testing with a small positive effect size (r =0.27). In group HVHA,
the valence was higher in the retrospective rating than in
The values from the continuous feedback interface were the continuous rating. A Wilcoxon signed-rank test revealed
on a 2-dimensional scale with valence and arousal values a significant difference (z = −2.61,p = 0.009), with a
from −50 to 50. These values were converted using simple moderate negative effect size (r = −0.41). In group LVLA,
linear scaling to a 1 to 9 scale to match the values from the the valence was lower in the retrospective rating than in
retrospective SAM scale (i.e., −50 corresponding to 1 and the continuous rating. A Paired t-test revealed a significant
50 corresponding to 9), thus making a comparison possible. difference (t=3.84,p=0.001), with a strong positive effect
With 20 participants watching 8 video clips each, a total of size (d = 1.11). In group HVLA, the valence was higher
160 ratings were collected for both arousal (A) and valence in the retrospective rating than in the continuous rating. A
(V). These ratings are equally distributed between both rating Wilcoxon signed-rank test revealed a significant difference
methods, retrospective (V , A ), and continuous (V , (z =−2.17,p=0.030), with a moderate negative effect size
Retro Retro Cont
A ), leaving 4 groups of values, each with 80 ratings. (r =−0.34). The statistical tests for valence are displayed in
Cont
For statistical testing, these ratings were divided into groups Table II. All valence ratings from participants are visualized,
according to the 4 quadrants (LVHA, HVHA, LVLA, HVLA) grouped by quadrant and rating method in Fig. 3a.
to maintain the influence of the baseline values of the stimuli
C. Arousal, Continuous versus Retrospective
while ensuring statistical power. This results in a sample size
In group LVHA, the arousal was lower in the retrospective
of 20 data points for each group, which were compared to
rating than in the continuous rating. A Wilcoxon signed-rank
assess the differences in rating methods.
test showed no significant difference (z = 1.20,p = 0.232),
Eighttestprocedureswereconductedtocomparethemeans
with a small positive effect size (r =0.18). In group HVHA,
between the rating methods, all following a consistent pro-
the arousal was lower in the retrospective rating than in
tocol. The preferred statistical test was the paired t-test,
the continuous rating. A Paired t-test revealed no significant
which relies on the assumptions of homogeneity and normal
difference (t = 0.87,p = 0.395), with a small positive effect
distribution. Homogeneity was assessed using Levene’s test,
size (d=0.24). In group LVLA, the arousal was lower in the
and normal distribution was evaluated using the Shapiro-Wilk
retrospective rating than in the continuous rating. A Paired t-
test. If at least one of these assumptions was violated, a non-
test revealed no significant difference (t = 1.38,p = 0.184),
parametrictest,specificallytheWilcoxonsigned-ranktest,was
with a small to moderate positive effect size (d = 0.30).
employed;otherwise,thepairedt-testwasconsideredsuitable.
In group HVLA, the arousal was lower in the retrospective
TheeffectsizeswereevaluatedusingCohen’sdforpairswith
ratingthaninthecontinuousrating.APairedt-testrevealedno
a normal distribution and the Wilcoxon effect size r for non-
significant difference (t = 1.47,p = 0.158), with a moderate
normally distributed pairs.
positive effect size (d=0.45). The statistical tests for arousal
Tocomparetheratingdatawiththebaselinevalues,instead aredisplayedinTableIII.Allarousalratingsfromparticipants
of grouping the values by quadrant, the rating means for are visualized, grouped by quadrant and rating method in Fig.
each video were calculated. To increase comparability, for 3b.
this analysis, only the retrospective scores were considered,
D. Evaluation of Rating Methods
astheywereassessedinasimilarway,namelyretrospectively
and using a 9-point Likert scale. Spearman’s rank correlation When asked to evaluate the different rating methods on
coefficients or Pearson’s correlation coefficients were selected a scale from 1 to 5, with 1 representing the lowest rating,
depending on whether the assumptions of normal distribution the retrospective rating received an average rating of 3.8
were met. (SD = 0.523), compared to the continuous rating method,TABLEII
STATISTICALTESTSCOMPARINGVALENCEFORRETROSPECTIVERATINGANDCONTINUOUSRATING
Group MeanV Cont MeanVRetro SDV Cont SDVRetro TestUsed TestStatistic p-value EffectSize
LVHA 4.64 4.00 0.81 1.81 Wilcoxonsigned-rank 1.71 0.086 0.27
HVHA 5.01 6.25 0.44 1.77 Wilcoxonsigned-rank −2.61 0.009* −0.41
LVLA 4.71 3.52 0.94 1.19 Pairedt-test 3.84 0.001* 1.11
HVLA 5.63 6.80 1.09 1.94 Wilcoxonsigned-rank −2.17 0.030* −0.34
TABLEIII
STATISTICALTESTSCOMPARINGAROUSALFORRETROSPECTIVERATINGANDCONTINUOUSRATING
Group MeanA Cont MeanARetro SDA Cont SDARetro TestUsed TestStatistic p-value EffectSize
LVHA 6.19 5.80 1.50 1.99 Wilcoxonsigned-rank 1.20 0.232 0.18
HVHA 5.69 5.20 1.72 2.31 Pairedt-test 0.87 0.395 0.24
LVLA 4.62 4.15 1.21 1.81 Pairedt-test 1.38 0.184 0.30
HVLA 4.81 3.90 1.99 2.02 Pairedt-test 1.47 0.158 0.45
(a) Valence (b) Arousal
Fig.3. Meanratingsgroupedbyquadrantandratingmethod(whiskersrepresentingthe95%confidenceinterval).
which received an average rating of 3.35 (SD = 1.089). A B. Valence, Continuous versus Retrospective
Wilcoxon signed-rank test showed no significant difference
Apart from the LVHA quadrant, all the means between
(z =27.0,p=0.097) between these evaluations.
valence in continuous and retrospective were significantly
V. DISCUSSION
different, with moderate to strong effect sizes. With three
A. Correlation to Baseline Values out of the four significant differences, there is evidence to
Pearson’s correlation coefficients show a strong correlation rejectthenullhypothesisforH 1,indicatingthatthetworating
between valence ratings and a medium to strong correlation methods produce divergent results for emotion ratings. As a
with arousal ratings, which indicates a valid emotional re- general trend, the values for the continuous method are more
sponse to the video stimuli. A large difference between the stronglycenteredaroundthemeanvalue,andtheretrospective
raised values and the baseline values may be due to the values are more scattered.
video clips, which were shortened for this experiment to a One reason could be the interface that was used: a 9-point
consistent duration of one minute. This can lead to varying Likertscale(Fig.1),versusa2Dplane(Fig.2).Thedifference
emotional responses depending on the scene chosen from intheratinginterfaceseemstobevalidandcouldcausethese
the original video. Furthermore, it should be noted that the deviations but seems more unlikely when looking at the data
sample size was decreased by only using values from the collected for arousal ratings. Because the same method was
retrospectiveratingmethodandbycalculatingmeansforeach used for both emotion dimensions, a similar tendency should
video instead of grouping by quadrant. Apart from these be observable. In the absence of this effect, the spread cannot
factors, the divergence could also be attributed to the remote be attributed solely to the graphical rating interface.
setting, with the challenges of remote moderation, as one The other aspect is the moment of rating, retrospective
cannot adjust, show, or help participants in person. Due to rating, and post-condition, compared to continuous ratings
these limitations, the correlation results must be viewed as an during the video that were then averaged. Interestingly, the
initial evaluation to have a glimpse into comparability with a effect sizes are negative for high valence videos (HVLA,
lab experiment. HVHA), indicating that the mean valence was rated higher inretrospect. The opposite effect is observable for low valence valence is more pronounced, corresponding to the original
videos (LVLA, LVHA), where the effect size is positive, valenceofthestimuli.Thisiscongruentwiththeassumptions
indicatingthatthemeanvalenceratingwaslowerinretrospect. of the peak-end rule that an emotion rating can be overes-
This result supports the effect of an overestimation of valence timated if surveyed in retrospect. However, this cannot be
in the post-questionnaires, similar to what the peak-end rules solely attributed to a psychological effect; the lesser spread
predict. However, this dynamic occurs naturally when the of continuous ratings can be caused by other differences
values of the continuous ratings are more balanced towards in the rating method (e.g., interface, data processing, and
the mean. interruptions). Unlike valence, the arousal ratings showed no
significant differences, but a slight decrease in the means in
C. Arousal, Continuous versus Retrospective
retrospect, regardless of quadrant. Consequently, insufficient
For arousal ratings, no significant differences are found in evidencesupportsH .Thedifferenttestresultsforvalenceand
2
any of the four quadrants, despite the differences in rating arousal reduce the possibility of influencing factors stemming
interfaceandratingtime.Consequently,thenullhypothesisfor fromtheratingmethodsthemselvesandsuggestthattheremay
H cannot be rejected. The effect sizes are low to moderate be different dynamics in place for arousal and valence.
2
positive, indicating a slightly higher rating for the continuous Whether the retrospective or continuous rating method
method. While this difference in means is not significant, is closer to the “real” emotions of participants cannot be
a higher excitement while watching the video would be determined. The subjective evaluation of the rating methods
reasonable,especiallywhenconsideringthatparticipantswere showed no significant difference, but participants tended to
alerted to give a rating every 10 seconds. An overestimation prefer the retrospective method. However, it was found that
effect cannot be observed for arousal, possibly because the rating methods can produce different outcomes for valence
arousal values are not centered around the mean other than and arousal. Further, the experiment illustrates the feasibility
for valence. This could indicate that post-experience biases of conducting remote studies by providing HMDs for partic-
have different dynamics for arousal. ipants and instruction over video conferencing. In summary,
our study provides valuable insights into the complexities of
VI. CONCLUSION
assessingemotionsinremoteVRexperiences,highlightingthe
This study explored the dynamics of remote testing by importanceofchoiceinratingmethodsandacknowledgingthe
conducting an experiment on emotion elicitation. Different unique challenges posed by remote settings.
rating methods have been employed to gather valence and
arousal ratings to investigate the variability in self-reported
measurements. A moderate to strong correlation of emotion A. Limitations and Future Work
ratings between the baseline values and experimental data
was observed. Although this is not proof of the feasibility Some limitations of this study should be considered when
of remote studies, it ensures that the 360° videos have a interpreting its results. First, the impact of participants being
comparable emotional effect on participants. The differences in their homes instead of in a laboratory has not yet been
can be attributed to the small sample size, shortened stimuli, explored and cannot be investigated using this experimental
and difficulties in remote moderation, such as the inability to design. However, replication of this study in a laboratory
ensure optimal adjustment of the HMD. Additionally, the re- environmentcouldprovidedetailsontheinfluencingfactorsof
motesettingitselfmayhaveanimpactontheUX.Specifically, theremotesetting,suchastheeffectofthehomeenvironment
the emotions experienced by participants may vary depending on emotional reactions. The sample size of 20 participants,
on whether they are located in a lab environment or their mostly male students, had limited generalizability. Future
private home, potentially affecting the arousal and valence studies should include larger and more diverse groups of
ratings during the experiment. While conducting a remote participants to eliminate the necessity of grouping the video
study proved to be a valuable method to continue research stimuli by quadrant and possible gender bias. With enough
during times of pandemic, it was accompanied by additional equally distributed ratings for each video clip, a more precise
considerations. In particular, the shipment of VR devices to comparisonwouldbepossible.The60-secondvideoclipsused
the participant’s home may not be feasible for studies with in this study cannot be regarded as equal to the original video
highparticipantcounts.Nonetheless,theexperimentprocedure source, which leads to limited comparability. Future studies
itselfandremotemoderationcouldbeexecutedwithoutmajor investigating the impact of different rating methods should
problems. This suggests that remote studies with user-owned introduce more conditions to isolate factors such as the inter-
devicescouldbeanefficientapproachunlessthestudyrequires face and timing for more meaningful insights. For continuous
participants who are inexperienced with VR. ratingmethods,differentapproachestodataprocessingshould
When comparing the emotion ratings from the continuous be explored. Instead of averaging all values, more elaborate
and retrospective methods, different results were observed for calculations could lead to better results. Therefore, by further
valenceandarousal.Thereisasignificantdifferenceinvalence exploring these future paths, VR research may progress and
between the rating methods for the groups HVLA, HVHA better comprehend the remote testing of VR experiences and
and LVLA, supporting H . In the retrospective ratings, the subjective emotion elicitation.
1REFERENCES ser. CHI ’21. Association for Computing Machinery, 2021. [Online].
Available:https://doi.org/10.1145/3411764.3445588
[1] M.Boukhris,A.Paljic,andD.Lafon-Pham,“360°versus3DEnviron- [18] J.-N. Voigt-Antons, R. Spang, T. Kojic´, L. Meier, M. Vergari, and
ments in VR Headsets for an Exploration Task,” in ICAT-EGVE 2017 S. Mo¨ller, “Don’t worry be happy - using virtual environments to
- International Conference on Artificial Reality and Telexistence and induce emotional states measured by subjective scales and heart rate
Eurographics Symposium on Virtual Environments, R. W. Lindeman, parameters,”in2021IEEEVirtualRealityand3DUserInterfaces(VR),
G.Bruder,andD.Iwai,Eds. TheEurographicsAssociation,2017. 2021,pp.679–686.
[2] P.Kourtesis,D.Korre,S.Collina,L.A.Doumas,andS.E.MacPherson, [19] E.Brivio,S.Serino,E.NegroCousa,A.Zini,G.Riva,andG.DeLeo,
“Guidelines for the development of immersive virtual reality software “Virtualrealityand360°panoramatechnology:amediacomparisonto
for cognitive neuroscience and neuropsychology: the development of study changes in sense of presence, anxiety, and positive emotions,”
virtual reality everyday assessment lab (vr-eal), a neuropsychological Virtual Reality, vol. 25, no. 2, pp. 303–311, Jun. 2021. [Online].
testbatteryinimmersivevirtualreality,”FrontiersinComputerScience, Available:https://doi.org/10.1007/s10055-020-00453-7
vol.1,p.12,2020. [20] M.Mori,K.F.MacDorman,andN.Kageki,“Theuncannyvalley[from
[3] S.Marsh,Userresearch:apracticalguidetodesigningbetterproducts the field],” IEEE Robotics & automation magazine, vol. 19, no. 2, pp.
andservices. KoganPagePublishers,2018. 98–100,2012.
[4] D. Saffo, S. Di Bartolomeo, C. Yildirim, and C. Dunne, “Remote [21] B. J. Li, J. N. Bailenson, A. Pines, W. J. Greenleaf, and L. M.
and collaborative virtual reality experiments via social vr platforms,” Williams, “A Public Database of Immersive VR Videos with
in Proceedings of the 2021 CHI Conference on Human Factors in Corresponding Ratings of Arousal, Valence, and Correlations between
ComputingSystems,2021,pp.1–15. HeadMovementsandSelfReportMeasures,”FrontiersinPsychology,
[5] J.Ratcliffe,F.Soave,N.Bryan-Kinns,L.Tokarchuk,andI.Farkhatdi- vol. 8, 2017. [Online]. Available: https://www.frontiersin.org/articles/
nov,“Extendedreality(xr)remoteresearch:asurveyofdrawbacksand 10.3389/fpsyg.2017.02116
opportunities,”inProceedingsofthe2021CHIConferenceonHuman [22] A. Pinilla, J.-N. Voigt-Antons, J. Garcia, W. Raffe, and S. Moller,
FactorsinComputingSystems,2021,pp.1–13. “Real-time affect detection in virtual reality: a technique based on a
[6] J. Preece, “Citizen science: New research challenges for human– three-dimensionalmodelofaffectandeegsignals,”FrontiersinVirtual
computer interaction,” International Journal of Human-Computer In- Reality,vol.3,Jan.2023.
teraction,vol.32,no.8,pp.585–612,2016. [23] Q. Guimard, F. Robert, C. Bauce, A. Ducreux, L. Sassatelli,
[7] J. Posner, J. A. Russell, and B. S. Peterson, “The circumplex model H.-Y. Wu, M. Winckler, and A. Gros, “Pem360: A dataset of
of affect: An integrative approach to affective neuroscience, cognitive 360° videos with continuous physiological measurements, subjective
development,andpsychopathology,”Developmentandpsychopathology, emotional ratings and motion traces,” in Proceedings of the 13th
vol.17,no.3,pp.715–734,2005. ACM Multimedia Systems Conference, ser. MMSys ’22. Association
[8] M. M. Bradley and P. J. Lang, “Measuring emotion: The self- for Computing Machinery, 2022, p. 252–258. [Online]. Available:
assessmentmanikinandthesemanticdifferential,”JournalofBehavior https://doi.org/10.1145/3524273.3532895
Therapy and Experimental Psychiatry, vol. 25, no. 1, pp. 49–59, [24] L. Tabbaa, R. Searle, S. M. Bafti, M. M. Hossain, J. Intarasisrisawat,
Mar. 1994. [Online]. Available: https://www.sciencedirect.com/science/ M.Glancy,andC.S.Ang,“Vreed:Virtualrealityemotionrecognition
article/pii/0005791694900639 dataset using eye tracking & physiological measures,” Proc. ACM
[9] Deloitte, “Technology, media, and telecommunications predictions Interact. Mob. Wearable Ubiquitous Technol., vol. 5, no. 4, dec 2022.
2023,”Online,2022. [Online].Available:https://doi.org/10.1145/3495002
[10] A. Mottelson and K. Hornbæk, “Virtual reality studies outside [25] T.Xue,A.E.Ali,G.Ding,andP.Cesar,“Investigatingtherelationship
the laboratory,” in Proceedings of the 23rd ACM Symposium betweenmomentaryemotionself-reportsandheadandeyemovements
on Virtual Reality Software and Technology, ser. VRST ’17. in hmd-based 360° vr video watching,” in Extended Abstracts of the
Association for Computing Machinery, 2017. [Online]. Available: 2021 CHI Conference on Human Factors in Computing Systems, ser.
https://doi.org/10.1145/3139131.3139141 CHI EA ’21. Association for Computing Machinery, 2021. [Online].
[11] D. Saffo, S. Di Bartolomeo, C. Yildirim, and C. Dunne, “Remote Available:https://doi.org/10.1145/3411763.3451627
and collaborative virtual reality experiments via social vr platforms,” [26] Q. Guimard and L. Sassatelli, “Effects of emotions on head motion
in Proceedings of the 2021 CHI Conference on Human Factors predictabilityin360°videos,”inProceedingsofthe14thInternational
in Computing Systems, ser. CHI ’21. Association for Computing Workshop on Immersive Mixed and Virtual Environment Systems, ser.
Machinery, 2021. [Online]. Available: https://doi.org/10.1145/3411764. MMVE ’22. Association for Computing Machinery, 2022, p. 37–43.
3445426 [Online].Available:https://doi.org/10.1145/3534086.3534335
[12] R. Rivu, H. Bayerl, P. Knierim, and F. Alt, “‘can you set it up on [27] J. A. Russell, “A circumplex model of affect.” Journal of Personality
yourown?’–investigatingusers’abilitytoparticipateinremote-based andSocialPsychology,vol.39,p.1161–1178,1980.
virtual reality studies,” in Proceedings of the 21st International [28] W.Zhang,L.Shu,X.Xu,andD.Liao,“Affectivevirtualrealitysystem
Conference on Mobile and Ubiquitous Multimedia, ser. MUM ’22. (avrs):Designandratingsofaffectivevrscenes,”in2017International
Association for Computing Machinery, 2022, p. 121–127. [Online]. ConferenceonVirtualRealityandVisualization(ICVRV),2017,pp.311–
Available:https://doi.org/10.1145/3568444.3568462 314.
[13] A.Mottelson,G.B.Petersen,K.Lilija,andG.Makransky,“Conducting [29] A.Toet,D.Kaneko,S.Ushiama,S.Hoving,I.Kruijf,A.-M.Brouwer,
unsupervised virtual reality user studies online,” Frontiers in Virtual V.Kallen,andJ.Erp,“Emojigrid:A2dpictorialscalefortheassessment
Reality, vol. 2, 2021. [Online]. Available: https://www.frontiersin.org/ offoodelicitedemotions,”FrontiersinPsychology,vol.9,p.2396,Nov.
articles/10.3389/frvir.2021.681482 2018.
[14] F. Mathis, J. O’Hagan, K. Vaniea, and M. Khamis, “Stay [30] A. Toet, F. Heijn, A.-M. Brouwer, T. Mioch, and J. B. F. van Erp,
home! conducting remote usability evaluations of novel real-world “An immersive self-report tool for the affective appraisal of 360° vr
authentication systems using virtual reality,” in Proceedings of the videos,” Frontiers in Virtual Reality, vol. 1, 2020. [Online]. Available:
2022 International Conference on Advanced Visual Interfaces, ser. https://www.frontiersin.org/articles/10.3389/frvir.2020.552587
AVI 2022. Association for Computing Machinery, 2022. [Online]. [31] J.-N. Voigt-Antons, E. Lehtonen, A. P. Palacios, D. Ali, T. Kojic, and
Available:https://doi.org/10.1145/3531073.3531087 S. Mo¨ller, “Comparing Emotional States Induced by 360° Videos Via
[15] R. Somarathna, T. Bednarz, and G. Mohammadi, “Virtual reality for Head-MountedDisplayandComputerScreen,”in2020TwelfthInterna-
emotion elicitation – a review,” IEEE Transactions on Affective Com- tionalConferenceonQualityofMultimediaExperience(QoMEX),May
puting,pp.1–21,2022. 2020,pp.1–6,iSSN:2472-7814.
[16] T. Luong, A. Lecuyer, N. Martin, and F. Argelaguet, “A survey on [32] B.L.FredricksonandD.Kahneman,“Durationneglectinretrospective
affective and cognitive vr,” IEEE Transactions on Visualization and evaluations of affective episodes.” Journal of Personality and Social
ComputerGraphics,vol.28,no.12,pp.5154–5171,2022. Psychology,vol.65,no.1,p.45–55,1993.
[17] C.Jicol,C.H.Wan,B.Doling,C.H.Illingworth,J.Yoon,C.Headey, [33] T.Franke,C.Attig,andD.Wessel,“Apersonalresourcefortechnology
C. Lutteroth, M. J. Proulx, K. Petrini, and E. O’Neill, “Effects of interaction: development and validation of the affinity for technology
emotion and agency on presence in virtual reality,” in Proceedings of interaction (ati) scale,” International Journal of Human–Computer In-
the 2021 CHI Conference on Human Factors in Computing Systems, teraction,vol.35,no.6,pp.456–467,2019.