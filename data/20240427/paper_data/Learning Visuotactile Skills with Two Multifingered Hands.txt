Learning Visuotactile Skills with Two Multifingered Hands
Toru Lin, Yu Zhang∗, Qiyang Li∗, Haozhi Qi∗, Brent Yi, Sergey Levine, and Jitendra Malik
(cid:8)(cid:14)(cid:10)(cid:7)(cid:9)(cid:13)(cid:6)(cid:5)(cid:11)(cid:12)(cid:7)(cid:4)(cid:1)(cid:11)(cid:3)(cid:1)(cid:0)(cid:11)(cid:2) (cid:8)2(cid:10)(cid:7)3(cid:18)(cid:2)(cid:11)(cid:3)(cid:7)7(cid:3)(cid:14)(cid:6)*(cid:7)+(cid:18)(cid:19)(cid:5)(cid:3)(cid:18)(cid:23)
(cid:29)(cid:21)(cid:0)(cid:3)%P(cid:1)(cid:0)(cid:11)(cid:2)(cid:7)M7KPR(cid:7)+(cid:14)(cid:12)(cid:11)(cid:3)(cid:14)
(cid:29)(cid:2)(cid:18)(cid:7)h(cid:3)(cid:0)(cid:6)(cid:5)(cid:7)M7KPR(cid:7)+(cid:14)(cid:12)(cid:11)(cid:3)(cid:14)(cid:6)
(cid:8)!(cid:10)(cid:7)(cid:29)(cid:21)(cid:25)(cid:12)2(cid:7)+(cid:18)(cid:19)(cid:5)(cid:3)(cid:18)9
3(cid:25)(cid:23)(cid:23)(cid:7)M(cid:0)(cid:20)(cid:21)(cid:5)(cid:7) 3(cid:25)(cid:23)(cid:23)(cid:7)ƒ(cid:11)(cid:129)(cid:5)(cid:7) 3(cid:25)(cid:23)(cid:23)(cid:7)R(cid:18)(cid:2)(cid:19)(cid:7)
R(cid:11)}(cid:5)(cid:11)(cid:3)(cid:18)(cid:25)(cid:6)(cid:7)(cid:15)(cid:14)(cid:19)%z
(cid:2)(cid:0)(cid:5)(cid:21)(cid:7)(cid:29)(cid:18)(cid:25)!(cid:21)(cid:7)(cid:9)(cid:11)(cid:19)(cid:6)(cid:0)(cid:19)(cid:20)
(cid:8)%(cid:10)(cid:7)(cid:24)(cid:0)(cid:6)(cid:25)(cid:18)(cid:5)(cid:14)!(cid:5)(cid:0)(cid:23)(cid:11)(cid:7)(cid:9)(cid:30)(cid:0)(cid:23)(cid:23)(cid:6)(cid:7)(cid:2)(cid:0)(cid:5)(cid:21)(cid:7)(cid:29)(cid:2)(cid:18)(cid:7)$(cid:25)(cid:23)(cid:5)(cid:0)(cid:27)(cid:19)(cid:20)(cid:11)(cid:3)(cid:11)%(cid:7)(cid:15)(cid:14)(cid:19)%(cid:6)
(cid:15)(cid:14)(cid:19)%(cid:18)(cid:1)(cid:11)(cid:3) (cid:9)(cid:5)(cid:14)!(cid:30) 3(cid:18)(cid:25)(cid:3) (cid:9)(cid:11)(cid:3)(cid:1)(cid:11)
Fig.1.Anoverviewofoursystemsetupandlearnedvisuotactileskillsonfourtasks.(a)Ourhardwareandteleoperationsystemsetup.
The hardware consists of two UR5 robot arms, each equipped with a Psyonic Ability Hand. Visual observations are obtained via two
wrist-mountedandonethird-viewRGB-Dcameras.Tactileobservationscomefromthemultifingeredhands,witheachfingertipequipped
with six touch sensors. We utilize the Meta Quest 2 platform for teleoperation. (b) We use the grip buttons of the Quest controllers to
command power grasp of the non-thumb fingers. (c) We use thumbsticks to control the 2-DoF joint positions of the thumbs. (d) Four
policieslearnedfromvisuotactiledatacollectedbyourhands-armsteleoperationsystem(HATO).Thesepoliciescanaccomplishavariety
of complex bimanual tasks: handing over a slippery object, stacking a block tower, pouring from a wine bottle, and serving steak.
Abstract—Aiming to replicate human-like dexterity, percep- the effects of dataset size, sensing modality, and visual input
tualexperiences,andmotionpatterns,weexplorelearningfrom preprocessingonpolicylearning.Ourresultsmarkapromising
human demonstrations using a bimanual system with multifin- step forward in bimanual multifingered manipulation from
gered hands and visuotactile data. Two significant challenges visuotactiledata.Videos,code,anddatasetscanbefoundhere.
exist: the lack of an affordable and accessible teleoperation
systemsuitableforadual-armsetupwithmultifingeredhands,
andthescarcityofmultifingeredhandhardwareequippedwith
I. INTRODUCTION
touch sensing. To tackle the first challenge, we develop HATO,
Achievinghuman-leveldexterityisalong-termgoalinthe
a low-cost hands-arms teleoperation system that leverages off-
field of robotic manipulation. To this end, we explore the
the-shelf electronics, complemented with a software suite that
enables efficient data collection; the comprehensive software novel integration of a bimanual system with multifingered
suite also supports multimodal data processing, scalable policy hands,visuotactiledatamodalities,andlearningfromhuman
learning, and smooth policy deployment. To tackle the latter demonstrations, in hopes of mimicking the complexities of
challenge, we introduce a novel hardware adaptation by re-
human anatomy, sensory experiences, and behavior patterns.
purposing two prosthetic hands equipped with touch sensors
forresearch.Usingvisuotactiledatacollectedfromoursystem, Most existing bimanual systems opt for parallel-jaw grip-
we learn skills to complete long-horizon, high-precision tasks pers [1], [2], [3] as the end effectors, due to the high
which are difficult to achieve without multifingered dexterity maintenance costs and limited availability of more advanced
and touch feedback. Furthermore, we empirically investigate
alternatives [4], [5], [6]. However, this choice greatly con-
strains the range of motions that can be achieved compared
*Equalcontribution.
to multifingered hands [7], limiting abilities in adaptive
AllauthorsarewithUniversityofCalifornia,Berkeley.Correspondence
totoru@berkeley.edu grasping, in-hand manipulation, dexterous handovers, tool
4202
rpA
52
]OR.sc[
1v32861.4042:viXra(a) Slippery Handover (b) Tower Block Stacking
(c) Wine Pouring (d) Steak Serving
Fig. 2. Illustration of learned skills on four different tasks. Our learned policies complete long-horizon and high-precision tasks
that require bimanual dexterity. (a) Slippery Handover tests the basic coordination skill between two hands. (b) Tower Block Stacking
demonstratestheadvantageofhavingalargecontactareabecauseoftheflatpalm,aswellastheabilitytomaintainastableposeduring
movements.(c)WinePouringrequirestheabilitytograsplargerobjectswiththecenterofmasschangingduringmanipulation.(d)Steak
Serving requires the ability to use tools and maintain grasp stability when moving the objects on the tool.
use, and bilateral coordination for complex tasks. demonstrations is sufficient for learning effective bimanual
We assemble a bimanual system with multifingered hands dexterous policies. Additionally, we confirm the vital role
tolearndexterousskillsthroughvisuotactiledemonstrations. of wrist-mounted cameras in boosting policy performance,
We confront two major challenges: the absence of an af- while noting that depth information does not markedly
fordable and accessible teleoperation system suitable for a benefit the learning process.
dual-arm setup with multifingered hands, and the scarcity of Inthespiritoffosteringfurtherresearchandcollaboration,
multifingered hand hardware equipped with touch sensing. we will open-source all our hardware and software systems
Our main contributions include: (1) a novel hardware adap- andmaketheteleoperationdatasetwehavecollectedpublicly
tation that repurposes prosthetic hands equipped with rich available to facilitate evaluation and replication of our work.
tactilesensingforresearchuse;(2)HATO,alow-costhands-
II. RELATEDWORK
arms teleoperation system built with commercially available
virtualreality(VR)hardware,featuringnovelmappingsfrom Bimanual hands-arms manipulation. Prior bimanual ma-
teleoperatormotiontorobotcontrol;(3)acomprehensiveand nipulation systems usually use parallel-jaw grippers [1], [8],
versatile software suite for reliable and efficient data collec- [9], [10] for their simplicity and durability. Early work uses
tion, multimodal data processing, scalable policy learning, classical control for dexterous manipulation [11], [12], [13],
and smooth policy deployment. but these approaches are largely task-specific and require
We design four complex tasks that examine our system’s expert knowledge of system dynamics and task structures.
capability in achieving intricate skills like two-hand coordi- Reinforcement learning can alleviate these issues, but they
nation, manipulation of bulky objects, and sophisticated tool tend to have high sample complexity and are only tractable
use. From only 30 minutes to 2 hours of teleoperation data in simulation [14], [15]. Although there has been progress
collected using HATO (including around 5 to 10 minutes in transferring the policies from simulation to the real-world
of practice time), we are able to obtain dexterous bimanual for a single hand [16], [17], [18], [19] or two hands [20],
manipulation policies with visuotactile observations that can [21], the policies usually suffer from the sim-to-real gap.
adeptly complete all tasks through pure end-to-end learning. Imitation learning. Another promising approach to achieve
Our system demonstrates natural and human-like skills and human-level dexterity is learning from demonstrations [22],
showcases unprecedented dexterity. [23]. More recently, researchers have utilized deep neural
We also perform a thorough ablation study on the effects networks to obtain better representations and policies [24].
of dataset size, sensing modality, and visual input prepro- While bimanual manipulation has also been studied in this
cessingonpolicylearning.Mostnotably,wefindthatvision context [1], [2], [3], [25], these studies only demonstrate
and touch significantly enhance learning efficiency, policy systems with parallel-jaw grippers. Wang et al. [26] is
success rate, and policy robustness. Without touch or vision, concurrent work that showcases bimanual dexterous manip-
thepoliciesarenotabletoconsistentlysucceedorsometimes ulation skills. Unlike [26], which utilizes only vision and
completely fail, highlighting the importance of high-quality proprioceptionasinputs,ourworkadditionallyutilizestactile
touchsensingforenablinghuman-leveldexterity.Ourexper- input and shows that touch sensing is critical for the reliable
iments also reveal that a dataset comprising a few hundred completion of many challenging tasks considered.Fingertip Tactile Sensors Sensor Layout Robot hands. We attach two Psyonic Ability Hands to the
UR5e arms as end effectors. These hands were originally
5
2 designed for prosthetic use [37]; we repurpose them for
0 1
research by designing custom printed circuit boards (PCBs)
4
3 that simplify electrical wiring by integrating communication
interfaceswithpowerdistribution.Eachhandhasfivefingers,
and each finger has six actuated DoFs (one DoF per finger,
Tactile Inputs: two for the thumb). Each actuated DoF of the hand is a
revolute joint that has upper and lower limits similar to
human finger limits (see the leftmost and rightmost columns
ofFigure1(b)).Forthefournon-thumbfingers,themetacar-
pophalangeal (MCP) joints serve as the actuating DoF; the
Fig. 3. Fingertip Tactile Sensor Layout. There are six tactile MCP joints connect to the proximal interphalangeal (PIP)
sensors on each of the fingertips. Each tactile sensor provides a
joints via a four-bar linkage mechanism, contributing to an
continuous value proportional to the sensed pressure.
additional underactuated DoF on each finger. Each fingertip
also comes with six touch sensors (see Figure 3).
Bimanual teleoperation. Having access to a diverse set of
high-quality demonstrations is crucial for learning a high-
quality policy. Existing teleoperation systems are largely B. Teleoperation Setup
restricted to the use of parallel-jaw grippers [1], [2], [27],
OurteleoperationsystemleveragestheMetaQuest2plat-
[28] or single-handed scenarios [29], [30], [31]. In addition,
form. It comes with a VR headset and a pair of controllers,
the hand teleoperation systems heavily rely on retargeting to
each designated for one hand. The Quest combines visual
solve morphology differences, thus introducing unavoidable
tracking with Inertial Measurement Unit (IMU) sensors to
latency and are not intuitive to use. In contrast, our system
capture the spatial orientation and movements of the con-
teleoperates a robot hand by separating finger control into
trollers. Each controller is also outfitted with a range of in-
thumbcontrolandpowergraspcontrol,providingasmoother
teractive buttons, including thumbsticks, trigger buttons, and
user experience.
gripbuttons.UsingaVRapplicationlikeoculus reader[38],
Learning visuotactile skills. Our system also features rich one can stream data related to the controllers’ poses and
visuotactile sensory data. Integrated visuotactile sensing has button states in real-time. Our main contribution is the
been used for numerous applications such as grasping [32], developmentofasoftwaresuitethatprovidesflexibleoptions
in-hand manipulation [33], object shape reconstruction [34], for translating movements detected by the Quest controllers
[35], and cloth manipulation [36]. However, previous bi- to precise control commands for a bimanual multifingered
manual manipulation systems are either not equipped with robotic system. We highlight a simple yet effective mapping
dexterous hands or lack rich sensory signals. To the best from controller buttons to multifingered hand movements,
of our knowledge, our work is the first at the intersection which enables a smooth and intuitive user experience.
of bimanual dexterous manipulation and imitation learning
Arm control. We first read the pose measurements from the
from visuotactile inputs.
Quest controller, then transform the pose to a desired end-
III. HATO:HANDS-ARMSTELE-OPERATION effector(EEF)poseoftherobot’scoordinatesystem.Weuse
inverse kinematics (IK) to solve for the joint positions given
We develop HATO, a novel teleoperation system for
the desired EEF pose, and send the joint position command
bimanual multifingered hands. Our system is easy to set up
to the UR5e arm. In the case of IK solving failure, we use
andintuitivetouse,enablingefficientcollectionofbimanual
the last commanded joint positions. Aside from this control
dexterous manipulation data. An overview of our system is
implementation, our software suite also contains two other
shown in Figure 1. For teleoperation of each hand-arm pair,
implementations. The second implementation uses the first-
HATOmapsaMetaQuest2virtualreality(VR)controller’s
order approximation of the IK solution which linearly maps
pose to the end-effector pose of the robot arm, and the
the delta pose between the desired EEF pose and the current
controller’sgripbuttonandthumbsticktothehand’sjointpo-
EEF pose of the arm to the delta positions of the joints. The
sitions. The HATO software suite includes a data collection
third implementation directly sends the end-effector position
pipeline that records and processes data from all available
to the arm and the IK is done onboard. For the rest of the
sensing modalities (vision, touch, and proprioception). We
paper, we use the first implementation for both teleoperation
provide details below on the hardware setup, teleoperation
and policy execution.
pipeline, and data infrastructure.
Hand control. We map the controller’s grip button to the
A. Robot Setup
joint positions of the four non-thumb fingers (4 DoF), and
Robotarms.WeusetwoUR5erobotarmsforourmanipula- map the thumbstick readings to joint positions of the thumb
tion system. The UR5e is an industrial arm with six degrees (2 DoF). Readings from the grip button and thumbstick
of freedom (DoF). Each DoF is a revolute joint that has a of each controller are re-normalized based on the joint
working range of [−2π,2π]. ranges of their corresponding hand DoF, and sent to the
......Small Contact Area Object Slips Shaky Hold Object Falls
Unstable Grasp Points Object Slips Lack of Support Fail to Balance Object
Fig. 4. Common failures from parallel-jaw gripper teleoperation. When the object has a slippery and rigid surface or is larger than
the gripper, grasping with a parallel-jaw gripper requires very accurate planning, which is difficult to achieve during teleoperation. This
difficulty leads to various failure modes, including slipping objects, unstable holds, and unstable grasps. In contrast, multifingered hands
provide additional redundancy and contact areas to maintain a grasp.
Ability Hand as position control targets. Specifically, press- and axis-angle).
ing/releasingthegripbuttoncontrolsflexion/extensionofthe Vision.WeobtainRGB-Dimagedatafromthreecamerasat
non-thumbfingers(Figure1(b)),andthe2-Dpositionsofthe eachtimestep;boththeRGBanddepthimagesarestreamed
thumbstick control the thumb joints’ flexion/extension and ataresolutionof480×640.Weresizeallimagesto240×320
abduction/adduction (Figure 1 (c)). The hand’s power grasp before feeding them into the network.
isthereforeacontinuousmovementproportionaltotheoper- Touch.EachfingeroftheAbilityHandhassixtouchsensors
ator’spressingforceonthegripbutton,allowingfine-grained attached (Figure 3), contributing to a total of sixty touch
controlwhengraspingsoftordeformableobjects.Whilesuch sensorreadingsfrombothhandsh∈R60.Eachtouchsensor
mappingsacrificestheabilitytoperformsophisticatedfinger- reading is a continuous number whose range changes de-
gaiting,itprovidesanintuitiveuserinterfaceandisstillable pendingonmanufacturingtolerances.Thereadingstypically
to complete various grasps for complex tasks [39]. lie in the range of [200,400] (ADC values) when there is no
Pause-and-adjust. We follow a conventional design where contact event, and above 1000 on contact.
we use the controller’s trigger button to start and break a Control actions. For each arm, the control action is the
continuous arm control sequence, allowing the teleoperator desiredjointpositionforeachofthesixrevolutejointsofthe
to pause during a teleoperation trial and adjust their posture. arm, where each value in the vector represents the angle of
This design greatly helps with tasks that are close to the the revolute joint (in [−2π,2π]). For each hand, the control
teleoperator’s physical limit, e.g., those that require a large action is the desired joint position for each of the six finger
reach of the teleoperator’s arm. joints (independent of the arm control implementation).
Such mapping also makes HATO flexible enough to con- Data normalization. All values are scaled linearly per
trol potentially any robot arms with EEF control and hands dimension to be between −1 and 1, with the minimum
with an independent anthropomorphic thumb and the ability value mapped to −1 and the maximum value mapped to 1.
to perform power grasps. The minimum and maximum values are obtained from the
training data, except for the joint position reading and the
C. Data Collection and Preprocessing for Policy Learning
control action of the hand. We use a minimum value of 0
We collect multimodal data from both hands and arms by
andmaximumvaluesof[110,110,110,110,90,120]foreach
running HATO data collection pipeline at 10Hz. The data
of the six finger joint positions. For depth and RGB images,
include the proprioceptive states of both the UR5e arms and
we use their raw values: [0,255] for the RGB images and
the Ability Hands, the RGB-D images from three RealSense
[0,65535] for the depth images.
depth cameras (two mounted on the wrist of each hand, one
mounted at a stationary “head-view” position), the touch
IV. LEARNINGVISUOTACTILESKILLSWITHHATO
sensor readings from the Ability Hands, and the control With visuotactile demonstration data collected from
commands given to the UR5e arms and the Ability Hands. HATO, we can learn a variety of bimanual dexterous skills
Proprioception. Our proprioceptive state data at each time for complex tasks. In this section, we describe how we train
step includes the current joint positions of both arms, the diffusion policies [40] using the collected data. Importantly,
current finger positions of the hands, and the current end- we also propose a novel asynchronous inference algorithm,
effector pose (represented as a concatenation of translation which is the key to our fast and smooth policy deployment.Slippery Handover Block Stacking Wine Pouring Steak Serving
5.06
0.39 0.15
0.17
0.32 0.30 0.16 0.10
3.22
0.25 0.15 0.14 0.07 0.08
1.93
1.78
Proprioception Only No Vision No Touch Ours (Proprioception + Visuotactile)
Fig. 5. How vision and touch affect the policy performance across four challenging tasks. Across all tasks, vision is crucial for the
policy to achieve low prediction error. For Block Stacking and Steak Serving, removing the touch input does not significantly influence
the prediction error, but as we show in Table II and III, touch is also crucial for successfully completing these two tasks.
A. Learning [46]withalearningrateof0.0001,weightdecayof0.00001,
and a batch size of 128. Following [40], we maintain an
Our system learns bimanual dexterous skills from demon-
exponential weighted average of the model weights and use
stration data by treating action prediction as a conditional
it during evaluation/deployment.
generation problem. Following [40], with an observation
horizon of 1, we predict the action sequence of length
B. Deployment
16 from the current input observations using a denoising
Atdeploymenttime,weuseanasynchronoussetupwhere
diffusion probabilistic model (DDPM) [41]. Each input ob-
thediffusionmodelpredictionandtherobotexecutionrunin
servation is a collection of observation data from multiple
parallel. In particular, we use a remote inference server that
modalities: proprioception, vision, and touch. Each action
keeps track of the most recent observation and the timestep
is a 24-dimensional vector that specifies the desired joint
to which the observation corresponds. The local process
positions for the two arms and the two hands. We choose
sends the new observation to the remote inference server
to use only a single observation input as opposed to a short
ateverycontrolstep.Theinferenceservercontinuouslyruns
horizon of observations (as done in [40]) as we have found
the diffusion model on the latest observation and produces
that using one single observation is sufficient for the policy
the action sequence prediction. Then, it sends the action
to perform well and is much faster to train.
sequence prediction (with the corresponding timesteps) to
Proprioception. We use end-effector poses as the proprio-
the local process, where it computes the average of the
ception observations and do not include the arm joint posi-
predictions over multiple timesteps (similar to the temporal
tions. This is because the UR5 arms do not have redundant
ensemble in [1]). Note that this is different from how
joints, and the joint positions can vary very unpredictably
deployment is done in [40], where they do not use a
near singularity during teleoperation, posing bigger learning
temporal ensemble. We have found that the inclusion of
challenges.Theproprioceptionispassedthroughatwo-layer
action aggregation greatly improves motion smoothness. For
network with ReLU activation, a hidden size of 256, and an
inference, we use 15 diffusion steps.
output feature size of 64.
Touch. The touch signal is also passed through a two-layer V. EXPERIMENTS
network in the same way as proprioception.
We consider four challenging real-world tasks (Figure 2)
Vision. For image and depth observations from the three to study the bimanual dexterity enabled by our system. We
cameras, we follow the prior work on diffusion policy [40] validate the effectiveness of our system setup, data col-
to use the ResNet-18 architecture [42] and replace all the lection pipeline, policy learning, and deployment pipelines
BatchNorm [43] in the network with GroupNorm [44]. The by demonstrating teleoperation capabilities and showcasing
fully connected layer’s output size is adjusted to be 32. We learnedskillsthatsuccessfullycompletethesetasks.Wealso
do not share network weights across camera inputs. conduct an empirical investigation on how policy perfor-
Diffusion architecture. All the encoded image, depth, tac- mance is influenced by data size and sensing modalities.
tile, and proprioceptive observations are then concatenated Slippery Handover. Handing over objects is a motor skill
as the input to a diffusion model (CNN-based in [40]). We commonly required for a wide range of daily activities; it is
also use the same noise schedule (square cosine schedule) also commonly used as a bimanual manipulation task [47].
and the same number of diffusion steps (100) for training. Each task episode is initialized with the slippery object
Output action. The diffusion output from the model is the resting on a box. One hand needs to pick up the object and
normalized 6 DoF absolute desired joint positions of each hand it over to the other hand. The episode succeeds when
UR5e arm, and the 6 DoF normalized (0 to 1) desired joint the other hand holds the object stably and moves away from
positions of each Ability hand. the first hand by a distance of more than 10 centimeters.
Optimization details. We use the AdamW optimizer [45], As we will demonstrate, parallel grippers can face more
)01x(
ESM
noitcASlippery Handover Block Stacking Wine Pouring Steak Serving
Task Handover Stacking Pouring Serving
0.86 0.13
0.27
Pickup 10 / 10 10 / 10 10 / 10 10 / 10 3.46
0.22 0.09
Task Success 10 / 10 10 / 10 9 / 10 5 / 10 0.08
0.43 2.42 0.08
TABLEI.Successrateoneachofthefourchallengingbimanual 0.34 0.25 0.14 1.78
manipulationtasks.ForSlipperyHandoverandWinePouring,we 0.12 1.45
useonlyimageobservationandproprioceptivestateaswefindthese 25 50 75 100 25 50 75 100 50 100 200300 50 100 200300
twoinputsaresufficienttoachieveanalmost100%successrate.For Number of Demonstrations
Block Stacking and Steak Serving, we use image, proprioception,
and touch as inputs. The pickup success is an intermediate metric Fig. 6. How does the demonstration dataset size affect policy
thatmeasureshowoftenthehandssuccessfullypickupbothobjects. prediction error? Across all tasks, having more demonstration
trajectoriesconsistentlyleadstolowerpredictionloss.Inparticular,
the policy performance saturates for Block Stacking at 75 demon-
strations, Wine Pouring at 200 demonstrations, and Steak Serving
challenges when handling objects with slippery surfaces
at 100 demonstrations.
comparedtomultifingeredhands.Theanthropomorphichand
morphology greatly mitigates the slippery challenge due to
A. Capabilities from Teleoperation
thelargercontactareaandadditionalsupportthatitprovides
to objects. We qualitatively investigate whether having multifingered
hands as end-effectors allows for better manipulation capa-
Tower Block Stacking. Manipulation of bulky objects is
bilities than parallel-jaw grippers by comparing their perfor-
another common skill required for many everyday tasks.
mances on the four manipulation tasks above. In particular,
Motivated by manual labor jobs in construction sites where
totestthemanipulationcapabilityoftheparallel-jawgripper,
workers need to use two hands to move bricks, we design
we keep the rest of the system the same while replacing the
a tower block stacking task. Each task episode begins with
AbilityhandwiththeRobotiqgripperandmappingthesame
two piles of large blocks on the table, one consisting of two
gripbuttonontheQuestcontrollertothegripper’sopen/close
blocks (red and blue) and the other a single yellow block.
control. With multifingered hand end effectors, previously
Therobotneedstomoveupthepileoftwoblocksandstack
inexperienced teleoperators are able to collect hundreds of
itontopoftheyellowblock.Asuccessfulepisodeismarked
high-qualitydemonstrationswithinafewhours.Ontheother
when the two moved blocks stay stably on the yellow block
hand, with parallel-jaw grippers, the teleoperation suffers
after being released from the robot hands.
from a variety of failure modes, including slipping objects,
Wine Pouring. At task initialization, a large wine bottle and
shakyholds,unstablegrasping,andunstablebalancing.Some
asmallcupareplacedonastoolontopofatable.Therobot
common task failures are shown in Figure 4.
needstouseonehandtograspthewinebottle,usetheother
hand to grasp the cup, perform a pouring motion from the B. Capabilities from Learning
bottle to the cup, and put both the bottle and the cup back.
WevalidatetheeffectivenessofHATOasadatacollection
The wine bottle is filled with transparent beads to simulate
pipeline by demonstrating successful policies trained from
liquid. Since the center of mass quickly changes during the
HATO-collected datasets. In particular, we record the task
pouring process, we hypothesize that a power grasp with a
success rate of learned policies using 10 deployment trials.
multifingered hand can greatly reduce the difficulty.
In addition to the success rate for the full task, we also
Steak Serving. Inspired by cooking activities, we design a recordhowmanytimeseachpolicysuccessfullypicksupthe
long-horizontaskthatrequiresprehensilegraspsandintricate object(s) (e.g., bottle and cup for pouring, pan and spatula
force feedback control loops for humans. The goal of this for steak serving, two blocks for stacking, and banana for
task is to serve a piece of cooked steak onto a plate. At task handover) as the partial task completion rate. As shown in
initialization,threeobjectsareplacedonthetable:acooking Table I, our policy is able to pick up the object(s) with a
pan with a piece of steak inside, a spatula for serving, and 100% success rate across all tasks. Our policy is also able
a ceramic plate. The robot is expected to use one hand to to complete three of the four tasks (handover, stacking, and
hold the pan and the other hand to grasp the spatula. It then pouring) consistently with near 100% success rate. The last
needs to insert the spatula under the bottom of the steak and task, steak serving, is much more difficult to learn due to its
lift it up. The task is completed when the robot successfully long task horizon and the demand for high-precision control
holds the spatula with the steak and serves it onto the plate. (e.g., balancing the steak on a spatula held by a hand).
Teleoperation data collection details. For Slippery Han- Despite such difficulty, our policy is still able to achieve
dover and Tower Block Stacking, we collect 100 demonstra- around a 50% success rate.
tions, each demonstration lasting around 6 seconds and 20
C. Learning Efficiency
seconds respectively. For Wine Pouring and Steak Serving,
we collect 300 demonstrations, each demonstration lasting We study the efficiency of our learning method by em-
around 25 seconds and 40 seconds respectively. Before the pirically evaluating the correlation between the number of
data collection for each task, we ask the human teleoperator demonstrations and policy performance. We use the mean
to practice for 5 to 10 minutes. squared error between predicted actions and ground truth
)01x(
ESM
noitcAModality DefaultInit. RareInit. Modality Pickup Success Modality Pickup Success
Ours 10/10 10/10 Ours 10/10 5/10 Ours 10/10 10/10
w.o.Touch 10/10 4/10 w.o.Touch 10/10 0/10 w.Depth 10/10 1/10
w.o.Vision 10/10 0/10 w.o.Vision 0/10 0/10 OnlyWrist 0/10 1/10
EEFOnly 0/10 0/10 Only3rdView 0/10 0/10
TABLE II. Success rate on Tower Block
Stacking task. We use two task initializa- TABLEIII.SuccessrateonSteakServing TABLEIV.SuccessrateonSteakServing
tion schemes (as illustrated in Figure 7). taskusingpoliciestrainedwithdifferent task with different camera configura-
The default initialization (Default Init.) is sensing modalities. The pickup success is tions. The pickup success is an interme-
morefrequentlyencounteredinthedemon- an intermediate metric that measures how diate metric that measures how often the
stration dataset compared to the other ini- often the hands successfully pick up both handssuccessfullypickupbothobjects.We
tialization (Rare Init.). Without visual or objects. The policy cannot finish the task find having depth does not improve the
touchfeedback,thepolicystrugglestosuc- without all three sensing modalities and performanceandhavingallthecamerasare
ceed from Rare Init. cannot pick up the object without touch. necessary to finish this challenging task.
actions (ActionMSE) on a held-out test set as the evaluation better metric would be to estimate the log-likelihood of
metric. The evaluation dataset consists of 10 demonstration the data under the diffusion policy output distribution, but
trajectories for Slippery Handover and Block Stacking, and that has been notoriously difficult to estimate [48]. In our
20 demonstration trajectories for Wine Pouring and Steak experience,wefoundthattheActionMSEwasabletomostly
Serving. The results are shown in Figure 6. As we expect, inform us which policy is expected to perform well.
the prediction error decreases as the training dataset size
increases.ItisworthhighlightingthattheActionMSEmetric Vision and touch improve policy robustness. To further
roughly saturates for Block Stacking, Steak Serving, and understand how the sensing modality affects the robustness
Wine Pouring at around 75, 100, and 200 demonstrations of our policy, we experiment with a less common scene
each, respectively. For the other tasks, Slippery Handover, initialization for the Tower Block Stacking. Specifically, we
it is possible that more demonstrations may further improve rotate the blocks in random directions. Figure 7 shows a
policy robustness. comparisonbetweenthedefaultinitializationandtherareini-
tialization.Thissceneinitializationislessencounteredinthe
D. Importance of Vision and Touch
demonstrationdataset,andtheperturbedblockconfiguration
Our policy takes three types of sensing modalities as makesthetwo-blockpilehardertobepickedupbytworobot
inputs: proprioception, vision, and touch. In this section, hands.InTableII,weshowacomparisonofthesuccessrate
we quantitatively investigate how the visuotactile sensing forthisrareinitializationandthedefaultinitializationacross
modalities affect policy learning and performance. threedifferentsensingconfigurations.Whiletouchandvision
Vision and touch are crucial for learning. Figure 5 shows are not needed for the default scene initialization (they all
the ActionMSE for policies trained with different sensing achieve 100% success rate), for the rare scene initialization,
modalities. For all four tasks, the policy trained with no thepolicytrainedwithouttouchcanonlysucceed4/10times,
vision has a much higher prediction error than the policy and the policy trained without vision cannot succeed at all.
trainedwithallthreesensingmodalities.Foralltasksexcept This suggests that vision and touch sensing modalities allow
for Steak Serving, the policy trained with no touch has the policy to be more robust to rare scenarios.
a higher prediction error than the policy trained with all
three sensing modalities. Policies trained with neither vision Wrist camera vs third-view camera. We study the effect
nor touch consistently have the highest ActionMSE. As we of different camera positions. The results are shown in
will show in the following section, such a correlation also Figure8.Predictionerrorwithonlythewrist-viewcamerais
translates to the policy success rate. consistently lower than that with only the third-view camera
Vision and touch improve policy success rate. On the across the Slippery Handover, Block Stacking, and Steak
Steak Serving task, we evaluate the success rate of the Serving tasks; the two errors are comparable on the Wine
policies trained with different sensing modalities and report Pouringtask.Wehypothesizethatthisisbecausewrist-view
the results in Table III. Without vision, the policy fails at cameras contain richer information on task-relevant object
the first task stage, i.e., properly picking up the objects states, due to the less occluded object view and more spatial
(0/10 success rate). Without touch, the policy is able to hints via induced perspectives during arm movement.
accomplish the first task stage (i.e., pick up the pan and the
spatula) but fails to transfer the steak over to the plate. It is Use of depth. We examine the prediction errors of the
worthhighlightingthateventhoughtheActionMSEmetricis policies trained with and without depth information. The
similarforthepoliciestrainedwithorwithouttouch(0.07vs results are shown in Figure 8. Across all four tasks, adding
0.08), these policies have vastly different success rates: 0/10 depth does not provide a marked benefit in terms of the
(without touch) vs. 5/10 (with touch). We believe that this ActionMSE metric, sometimes even hurting performance
is because ActionMSE cannot fully capture how well the (e.g., Wine Pouring). We hypothesize that the noisy depth
diffusion policy fits the dataset distribution. A potentially readings cause more harm than good for learning.Common Initialization Rare Initialization (Rotated Block)
computational cluster provided by the Berkeley Research
Compute program.
REFERENCES
[1] T.Z.Zhao,V.Kumar,S.Levine,andC.Finn,“Learningfine-grained
bimanualmanipulationwithlow-costhardware,”inRSS,2023.
[2] Z. Fu, T. Z. Zhao, and C. Finn, “Mobile ALOHA: Learning bi-
Fig. 7. Two scene initializations for the Tower Block manualmobilemanipulationwithlow-costwhole-bodyteleoperation,”
arXiv:2401.02117,2024.
Stackingtask.Thedefaultinitializationisshownontheleftand
[3] C.Chi,Z.Xu,C.Pan,E.Cousineau,B.Burchfiel,S.Feng,R.Tedrake,
therareinitializationisshownontheright.Intherareinitialization,
and S. Song, “Universal manipulation interface: In-the-wild robot
the perturbed block configuration makes the two-block pile harder teachingwithoutin-the-wildrobots,”arXiv:2402.10329,2024.
topickupbytworobothandsandmoredifficulttostackstablyon [4] https://www.allegrohand.com/.
another block. [5] https://schunk.com/us/en/gripping-systems/special-gripper/svh/c/
PGR3161.
[6] https://www.shadowrobot.com/dexterous-hand-series/.
Slippery Handover Block Stacking Wine Pouring Steak Serving
0.36 [7] A. Bicchi and V. Kumar, “Robotic grasping and contact: A review,”
0.36 0.17 2.29 0.27 inICRA,2000.
0.25 [8] F. Caccavale, P. Chiacchio, A. Marino, and L. Villani, “Six-DOF
impedance control of dual-arm cooperative manipulators,” Transac-
0.15
0.27 0.14 tionsonMechatronics,2008. 0.25 1.72 1.78 [9] N.Sarkar,X.Yun,andV.R.Kumar,“Dynamiccontrolof3-Drolling
contactsintwo-armmanipulation,”inICRA,1993.
[10] R. Platt, A. H. Fagg, and R. A. Grupen, “Manipulation Gaits: Se-
3rd View Only Ours with Depth Ours quencesofGraspControlTasks,”inICRA,2004.
[11] C. Ott, O. Eiberger, W. Friedl, B. Bauml, U. Hillenbrand, C. Borst,
Fig. 8. How does each type of image observation affect policy
A.Albu-Schaffer,B.Brunner,H.Hirschmuller,S.Kielhofer,R.Koni-
prediction error? Aside from the vision modality, all policies are
etschke, M. Suppa, T. Wimbock, F. Zacharias, and G. Hirzinger, “A
trained with proprioception and touch. The policies trained with humanoidtwo-armsystemfordexterousmanipulation,”inHumanoids,
only the third-view camera (3rd View Only) have higher prediction 2006.
errors, except for the Wine Pouring task. Policies with all three [12] J. Steffen, C. Elbrechter, R. Haschke, and H. Ritter, “Bio-inspired
cameras that additionally include depth images (Ours with Depth) motion strategies for a bimanual manipulation task,” in Humanoids,
haveasimilarpredictionerrortothepoliciestrainedwithoutdepth 2010.
(Ours), except on Wine Pouring. [13] N.Vahrenkamp,M.Przybylski,T.Asfour,andR.Dillmann,“Biman-
ualgraspplanning,”inHumanoids,2011.
[14] Y. Chen, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. McAleer,
VI. DISCUSSIONS H. Dong, S.-C. Zhu, and Y. Yang, “Towards human-level bimanual
dexterous manipulation with reinforcement learning,” in NeurIPS,
In this work, we share novel engineering insights that en- 2022.
able high-performance policy learning from human demon- [15] K. Zakka, P. Wu, L. Smith, N. Gileadi, T. Howell, X. B. Peng,
S.Singh,Y.Tassa,P.Florence,A.Zeng,andP.Abbeel,“RoboPianist:
strations using a bimanual system with multifingered hands
Dexterouspianoplayingwithdeepreinforcementlearning,”inCoRL,
andvisuotactilesensing,andshowcasethedexterousmanip- 2023.
ulationcapabilitiesachievedbyoursystem.Inparticular,we [16] H.Qi,A.Kumar,R.Calandra,Y.Ma,andJ.Malik,“In-handobject
rotationviarapidmotoradaptation,”inCoRL,2022.
show that visuotactile sensing is the key for our policies
[17] T. Chen, M. Tippur, S. Wu, V. Kumar, E. Adelson, and P. Agrawal,
to complete complex and long-horizon tasks consistently “Visual dexterity: In-hand dexterous manipulation from depth,” Sci-
and robustly. Our low-cost teleoperation system opens up enceRobotics,2023.
a number of avenues for future research. For example, [18] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Jo´zefowicz,
B.McGrew,J.Pachocki,A.Petron,M.Plappert,G.Powell,A.Ray,
equipping our teleoperation system with haptic feedback
J.Schneider,S.Sidor,J.Tobin,P.Welinder,L.Weng,andW.Zaremba,
(e.g., attached to tactile sensing) could greatly improve the “Learningdexterousin-handmanipulation,”IJRR,2019.
userexperienceandconsequentlythequalityofteleoperation [19] A.Handa,A.Allshire,V.Makoviychuk,A.Petrenko,R.Singh,J.Liu,
D. Makoviichuk, K. Van Wyk, A. Zhurkevich, B. Sundaralingam,
data.Furthermore,ourpolicyislearnedentirelyfromscratch
Y.Narang,J.-F.Lafleche,D.Fox,andG.State,“Dextreme:Transfer
with no pre-training, making it susceptible to appearance of agile in-hand manipulation from simulation to reality,” in ICRA,
changesinthescene.Trainingmorerobustandgeneralizable 2023.
[20] B. Huang, Y. Chen, T. Wang, Y. Qin, Y. Yang, N. Atanasov, and
policies is also an interesting future research direction.
X.Wang,“Dynamichandover:Throwandcatchwithbimanualhands,”
inCoRL,2023.
ACKNOWLEDGMENT [21] T.Lin,Z.-H.Yin,H.Qi,P.Abbeel,andJ.Malik,“Twistinglidsoff
withtwohands,”arXiv:2403.02338,2024.
We thank Jesse Cornman from PSYONIC for help with
[22] A. Billard, S. Calinon, R. Dillmann, and S. Schaal, “Survey: Robot
setting up the Ability Hands, and Philipp Wu for help with programming by demonstration,” Springer handbook of robotics,
settinguptheUR5erobotarmsandGELLO.TLissupported 2008.
[23] A.Hussein,M.M.Gaber,E.Elyan,andC.Jayne,“Imitationlearning:
by fellowships from the National Science Foundation and
Asurveyoflearningmethods,”ACMComputingSurveys,2017.
UC Berkeley. QL is supported by ONR under N00014- [24] H.Ravichandar,A.S.Polydoros,S.Chernova,andA.Billard,“Recent
20-1-2383, and NSF IIS-2150826. HQ is supported by the advances in robot learning from demonstration,” Annual review of
control,robotics,andautonomoussystems,2020.
DARPAMachineCommonSenseandONRMURIN00014-
[25] J.Grannen,Y.Wu,B.Vu,andD.Sadigh,“StabilizetoAct:Learning
21-1-2801.ThisresearchwasalsopartlysupportedbySavio tocoordinateforbimanualmanipulation,”inCoRL,2023.
)01x(
ESM
noitcA[26] C. Wang, H. Shi, W. Wang, R. Zhang, L. Fei-Fei, and C. K. Liu,
“DexCap: Scalable and portable mocap data collection system for
dexterousmanipulation,”arXiv:2403.07788,2024.
[27] H.Fang,H.-S.Fang,Y.Wang,J.Ren,J.Chen,R.Zhang,W.Wang,
andC.Lu,“Low-costexoskeletonsforlearningwhole-armmanipula-
tioninthewild,”inICRA,2023.
[28] M. Seo, S. Han, K. Sim, S. H. Bang, C. Gonzalez, L. Sentis, and
Y. Zhu, “Deep imitation learning for humanoid loco-manipulation
throughhumanteleoperation,”inHumanoids,2023.
[29] S. P. Arunachalam, I. Gu¨zey, S. Chintala, and L. Pinto, “Holo-Dex:
Teachingdexteritywithimmersivemixedreality,”inICRA,2023.
[30] Y. Qin, H. Su, and X. Wang, “From one hand to multiple hands:
Imitation learning for dexterous manipulation from single-camera
teleoperation,”RA-L,2022.
[31] A.Sivakumar,K.Shaw,andD.Pathak,“Robotictelekinesis:Learning
a robotic hand imitator by watching humans on youtube,” in RSS,
2022.
[32] R.Calandra,A.Owens,D.Jayaraman,W.Yuan,J.Lin,J.Malik,E.H.
Adelson,andS.Levine,“Morethanafeeling:Learningtograspand
regraspusingvisionandtouch,”RA-L,2018.
[33] H.Qi,B.Yi,S.Suresh,M.Lambeta,Y.Ma,R.Calandra,andJ.Malik,
“General in-hand object rotation with vision and touch,” in CoRL,
2023.
[34] E.Smith,D.Meger,L.Pineda,R.Calandra,J.Malik,A.RomeroSo-
riano,andM.Drozdzal,“Active3dshapereconstructionfromvision
andtouch,”inNeurIPS,2021.
[35] S. Suresh, H. Qi, T. Wu, T. Fan, L. Pineda, M. Lambeta, J. Malik,
M.Kalakrishnan,R.Calandra,M.Kaess,J.Ortiz,andM.Mukadam,
“Neural feels with neural fields: Visuo-tactile perception for in-hand
manipulation,”arXiv:2312.13469,2023.
[36] N.Sunil,S.Wang,Y.She,E.Adelson,andA.R.Garcia,“Visuotactile
affordancesforclothmanipulationwithlocalcontrol,”inCoRL,2022.
[37] A. Akhtar, J. A. Austin, J. M. Cornman, D. M. Bala, and Z. Wang,
“Systemandmethodforanadvancedprosthetichand,”Mar2021.
[38] https://github.com/rail-berkeley/oculusreader.
[39] T.Feix,J.Romero,H.-B.Schmiedmayer,A.M.Dollar,andD.Kragic,
“Thegrasptaxonomyofhumangrasptypes,”Transactionsonhuman-
machinesystems,2015.
[40] C.Chi,S.Feng,Y.Du,Z.Xu,E.Cousineau,B.Burchfiel,andS.Song,
“Diffusionpolicy:Visuomotorpolicylearningviaactiondiffusion,”in
RSS,2023.
[41] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic
models,”inNeurIPS,2020.
[42] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
imagerecognition,”inCVPR,2016.
[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
networktrainingbyreducinginternalcovariateshift,”inICML,2015.
[44] Y.WuandK.He,“Groupnormalization,”inECCV,2018.
[45] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,”inICLR,2015.
[46] I.LoshchilovandF.Hutter,“Decoupledweightdecayregularization,”
arXivpreprintarXiv:1711.05101,2017.
[47] Y.Li,C.Pan,H.Xu,X.Wang,andY.Wu,“Efficientbimanualhan-
doverandrearrangementviasymmetry-awareactor-criticlearning,”in
ICRA,2023.
[48] Y.Song,C.Durkan,I.Murray,andS.Ermon,“Maximumlikelihood
trainingofscore-baseddiffusionmodels,”inNeurIPS,2021.