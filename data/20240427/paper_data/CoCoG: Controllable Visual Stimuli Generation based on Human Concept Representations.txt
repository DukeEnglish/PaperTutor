CoCoG: Controllable Visual Stimuli Generation based on Human Concept
Representations
ChenWei†,1,2, JiachenZou†,1, DietmarHeinke2 and QuanyingLiu∗,1
1SouthernUniversityofScienceandTechnology,Shenzhen,China
2UniversityofBirmingham,Birmingham,UnitedKingdom
{weic3,zoujc2022}@mail.sustech.edu.cn,d.g.heinke@bham.ac.uk,liuqy@sustech.edu.cn
Abstract
A central question for cognitive science is to un-
derstandhowhumansprocessvisualobjects,i.e,to
uncoverhumanlow-dimensionalconceptrepresen-
tation space from high-dimensional visual stimuli.
Generatingvisualstimuliwithcontrollingconcepts
is the key. However, there are currently no gen-
erative models in AI to solve this problem. Here,
we present the Concept based Controllable Gen-
eration (CoCoG) framework. CoCoG consists of
two components, a simple yet efficient AI agent
Figure1:Motivationsofourwork.
for extracting interpretable concept and predicting
human decision-making in visual similarity judg-
ment tasks, and a conditional generation model
Hebartetal.,2020;RoadsandLove,2023],suchasthesim-
for generating visual stimuli given the concepts.
ilarityjudgmenttask,inwhichvisualstimuliofspecificcon-
We quantify the performance of CoCoG from two
ceptsarepresentedtotheparticipantsandthentheirdecision-
aspects, the human behavior prediction accuracy
makingbehaviorsarerecorded. However,understandinghu-
and the controllable generation ability. The ex-
manconceptrepresentationthroughthesecognitivetaskshas
periments with CoCoG indicate that 1) the reli-
twomaindifficulties. Ontheonehand,traininganAIagent
ableconceptembeddingsinCoCoGallowstopre-
to uncover interpretable concept representations by predict-
dict human behavior with 64.07% accuracy in the
inghumandecision-makingbehaviorrequiressubstantialhu-
THINGS-similarity dataset; 2) CoCoG can gen-
mandecision-makingdataundermassivevisualobjects. On
erate diverse objects through the control of con-
theotherhand,tounderstandthecausalrelationshipbetween
cepts; 3) CoCoG can manipulate human similar-
conceptrepresentationandbehavior,itisnecessarytomanip-
ityjudgmentbehaviorbyinterveningkeyconcepts.
ulatetheconcepts,preservingallotherlow-levelfeatures,to
CoCoG offers visual objects with controlling con-
generatevisualobjects. ThisisunexploredterritoryforAI.It
cepts to advance our understanding of causality in
posesanewtechnicalchallengeforimagegeneration,namely
humancognition. ThecodeofCoCoGisavailable
controllablevisualobjectgenerationbasedonconceptrepre-
athttps://github.com/ncclab-sustech/CoCoG.
sentation.
Controllable generation models have made great progress
1 Introduction in AI community, for example, conditional generation mod-
elsbasedonGAN[Goodfellowetal.,2020;Taoetal.,2022]
Humans receive abundant visual stimulation from the natu-
and diffusion [Song et al., 2020; Ho et al., 2020; Ho and
ral world. Unlike computer vision models that aim at ob-
Salimans, 2022]. These conditional generation models have
jectrecognitiontasks,humansaimtosurviveincomplexna-
been applied in many fields, including text-to-image/video
ture,whichrequiresunderstandingabstractconceptsofthese
generation [Rombach et al., 2022; Ramesh et al., ], image
visual objects, such as functionality, toxicity, and danger.
inverse problem [Kawar et al., 2022; Chung et al., 2022;
To explore such concept representation in humans, scien-
Mengetal.,2021]andbiomedicalimaging[Songetal.,2021;
tistshaveproposedaseriesofvisualstimuli-baseddecision- O¨zbey et al., 2023]. The conditions for controllable im-
makingtasks[MurphyandMedin,1985;Medinetal.,1993;
agegenerationcancomefrommultipleaspects,suchastext,
sketches,edgemaps,segmentationmaps,depthmaps[Rom-
†Equalcontribution.
∗Correspondingauthor. bach et al., 2022; Ramesh et al., ; Meng et al., 2021;
ThispaperhasbeenacceptedbyIJCAI2024. Zhang et al., 2023; Yu et al., 2023; Bansal et al., 2023].
4202
rpA
52
]CN.oib-q[
1v28461.4042:viXraHowever, these conditions do not include human subjective 2 Method
feelings nor human feedback, resulting in generated images
TheCoCoGmethodcomprisestwoparts: aconceptencoder
misaligned with human needs. For instance, generated im-
andatwo-stageconceptdecoder,asshowninFigure2.
ages by recommendation system may not meet human pref-
erence. To align the generated images with human needs, 2.1 Conceptencoderforembedding
some pioneer works brought human feedback (e.g., the hu- low-dimensionalconcepts
manvisualpreferencescoreobtainedoffline [Wuetal.,2023;
ThefirststepinCoCcoGistotrainaconceptencodertolearn
Kirstain et al., 2023], the human-in-the-loop visual com-
theconceptembeddingsofvisualobjects(Figure2a). Given
parison decision [von Ru¨tte et al., 2023; Fan et al., 2023;
adatasetofvisualobjectsX, foreachvisualobjectxinthe
Tangetal.,2023])intoconditionalgenerativemodels. Nev-
dataset,wefirstinputitintotheCLIPimageencoderf toex-
ertheless, these works have not consider prior knowledge of
tracttheCLIPembeddingh ∈ RD. Then,theCLIPembed-
cognitive science, such as the factors that have the greatest
dingisinputintoalearnableconceptprojectorg,therebygen-
impactonhumandecision-making,thatis,concepts,ascon-
eratingtheconceptembeddingofthevisualobjectc∈Rd:
trolvariablesforimagegeneration.
A large number of human research suggest that similar- CLIPembedding: h=f(x),
(1)
ityjudgmenttasksareaneffectiveexperimentalparadigmfor conceptembedding: c=g(h),
revealing human concept representations [Roads and Love,
wheretheeachdimensioninconceptembeddingcrepresents
2023; Hebart et al., 2020]. In these tasks, human subjects
aninterpretable concept, and the corresponding activation of
are asked to compare different visual stimuli and make de-
thisdimensionindicatestheactivationstrengthoftheconcept
cisions based on their similarity. AI models, such as [Peter-
inthevisualobject. Inshort,wehavec=g(f(x)).
sonetal.,2018;Marjiehetal.,2022b;Marjiehetal.,2022a;
To train this model, we used the triplet odd-one-out sim-
Muttenthaleretal., 2022a;Jhaetal., 2023;Fuetal., 2023],
ilarity judgment task in the THINGS dataset [Hebart et al.,
are proposed to predict the subjects’ decisions, under an as-
2023]. Inthistask,participantsareaskedtoviewthreevisual
sumptionthathumanperceptionofvisualobjectscanbeen-
objectsandconsiderthesimilaritybetweeneachpairofvisual
codedintoalow-dimensionalmentalrepresentation,namely
objectstodeterminethesimilarpairandselecttheremaining
concept embedding. The distance between visual objects in
oneastheodd-one-out.
this concept space reflects the distance of visual objects in
Similar to previous works [Zheng et al., 2018; Hebart et
thehuman’smind. Humandecision-makingbehaviorcanbe
al., 2020; Muttenthaler et al., 2022b], we use the dot prod-
explainedbydifferentdimensionsofconceptrepresentation.
uctsimilarityasthesimilaritymeasurementfunctionbetween
Thus, aligning AI models with humans in terms of concept
concept embeddings (i.e., S =< c ,c >) and use cross-
representation would naturally align their outputs as well. ij i j
entropy based on pairwise similarities to predict human de-
Also, compared with existing controllable image generation
cisions. Therefore, for the concept embeddings c ,c ,c of
methods, a controllable generation model based on concept i j k
threevisualobjectsx ,x ,x ,wehave
representationwouldcontrolhumandecision-makingbehav- i j k
iormoreeffectively. p(y)=CrossEntropy(S ,S ,S ), (2)
jk ik ij
In this study, we propose a Concept based Controllable
where p(y) is the probability distribution of the triplet vi-
Generation(CoCoG)framework.CoCoGutilizeconceptem-
sualstimuli(x ,x ,x )beingtheodd-one-out,withthehigh-
beddingsasconditionsfortheimagegenerationmodel,bridg- i j k
estprobabilitychoicebeingthemodel’spredictedbehavioral
ingcognitivescienceandAI.CoCoGcomprisestwoparts: a
outcome. By comparing the model’s predicted behavioral
conceptencoderforlearningconceptembeddingviapredict-
outcomeswiththerecordedbehavioraloutcomesfromhuman
inghumanbehaviorsandaconceptdecoderwhichemploysa
participants,wecalculatethelossandperformbackpropaga-
conditionaldiffusionmodelformappingconceptembedding
tiontotraintheconceptprojectorg. Inthespecificprocess,
tovisualstimulithroughatwo-stagegenerationstrategy.
we added an L regularization to c to ensure the sparseness
CoCoGhasthreemaincontributions: 1
oflow-dimensionalconceptembedding.Weverifieddifferent
• CoCoG’sconceptencodercanpredicthumanvisualsim- trainingstrategies(seeAppendix).
ilarity decision-making behaviors with higher accuracy
thanthestate-of-the-art(SOTA)model(i.e.,VICE[Mut- 2.2 Two-stageconceptdecoderforcontrollable
tenthaler et al., 2022b]), uncovering a reliable, inter- visualstimuligeneration
pretableconceptspaceofhumans(Figure3). After training the concept encoder, we can obtain the data
triplet (x ,h ,c ) for each image in the dataset. Based on
• CoCoG’sconceptdecodercangeneratevisualstimuliby i i i
thesedata,wethentrainaconceptdecodertogeneratevisual
controlling the concept embedding. The generated vi-
objectsbycontrollinghumanconceptualrepresentations.
sualstimulihavediversityandhighconsistencywiththe
In our model, the concept embedding c completely deter-
target concept embeddings (Figure 4&5). The genera-
mines the distribution of the human decision y, and the h
tioncanbeguidedwithtextprompts(Figure6&7).
completely determines the distribution of c. Therefore, the
• CoCoG can manipulate the human similarity decision- joint distributionof the humandecisions p(x,h,c,y) can be
makingbehaviorinahighlycontrollablewaybycontrol- formulatedas:
lingtheconceptsofgeneratedvisualstimuli(Figure8).
p(x,h,c,y)=p(y)p(c|y)p(h|c)p(x|h). (3)Figure2:TheframeworkofCoCoG.(a)Theconceptencoderforlearningconceptembeddingsusingasimilarityjudgmentbehaviordataset.
VisualobjectsareprocessedthroughtheCLIPimageencodertoobtainCLIPimageembeddings,andthenpassedthroughalearnableconcept
projector to obtain concept embeddings. Then, we can predict similarity judgment behaviors by compute similarity with others. (b) The
stageIoftheconceptdecoder,thepriordiffusionfordeterminingtheconceptembeddingbasedonourdesiredjudgmentbehavior(e.g.,here
modifyingtheconcept“colorful”). Then,wetrainadiffusionmodelconditionedontheconceptembeddingtogeneratethecorresponding
CLIP embedding. (c) Stage II of the concept decoder, the CLIP guided generation. It uses the CLIP embedding as a condition to guide
thepre-trainedimagediffusiongenerationmodeltogenerateVAElatent,whicharethenprocessedthroughtheVAEdecodertoproducethe
generatedvisualobject.
Next, we present each step of p(c|y),p(h|c),p(x|h) re- Prior Diffusion model. We employ the classifier-free guid-
spectively. p(c|y)meansfindingtheappropriateconceptem- ance method to train this conditional generative diffusion
beddingcgiventhebehavioraloutcomey. Thisstepisdeter- model. ThespecificformulascanbefoundintheAppendix.
minedaccordingtospecificcontrolobjectives,whichwewill
discussinchapter4.2. Forp(c|y)andp(x|h),wedecompose
Stage II - CLIP guidance generation After obtaining
theconceptdecoderintotwostages, thePriorDiffusionand
the CLIP embedding h in Stage I, we model a generator
the CLIP guided generation, respectively, which execute the
p(x|h) to sample the visual object x conditional to h (Fig-
processesofgeneratingCLIPembeddinghfromtheconcept
ure 2c). In this study, we use the pre-trained SDXL and
embedding c and generating visual object x from the CLIP
IP-Adapter models[Podell et al., 2023; Sauer et al., 2023;
embeddingh.
Yeetal., 2023]. Speicfically, SDXLservesasthebackbone
StageI-Priordiffusion InspiredbyDALL-E2[Ramesh of the image diffusion generation model. Through the dual
et al., ], we train a diffusion model conditioned on the con- cross-attentionmodulesoftheIP-Adapter,weinputtheCLIP
cept embedding c to learn the distribution of CLIP embed- embedding h as a condition, thereby guiding the denoising
dingsp(h|c)(Figure2b). TheCLIPembeddinghobtainedin processoftheU-Net. SimilartoStageI,wehavethemodel
thisstageservesasthepriorforthenextstage. Weconstruct ϵ (z ,t,h),wherez arethenoisylatentsofSDXL’sVAE.
SD t t
alightweightU-Net: ϵ (h ,t,c),whereh representsthe Details can be found in the Appendix. Since we freeze the
prior t t
noisyCLIPembeddinginthediffusiontimestept.Weextract pre-trainedmodelswithoutanymodification,wecansimply
theCLIPembeddingsandtheconceptembeddingsfromIm- usetheexistingfunctionalitiesofthepre-trainedmodelsand
ageNetandusetheobtainedtrainingpairs(h ,c )totrainthe combinethemwithconceptembeddingguidance.
i idings helps us better utilize existing pre-trained conditional
generative models. Existing models can already use CLIP
embeddings as a conditional input for the generative model,
allowing us to simply adopt a two-stage generation strategy.
WeonlyneedtotrainaPriorDiffusionmodel,whichsignif-
icantly reduces the computational cost of training and infer-
ence.
3 ModelValidation
3.1 Conceptencodercanpredictandexplain
humanbehaviors
Wefirstvalidatedtheconceptencoderfromtwoaspects: the
prediction accuracy of human behaviors (Figure 3a) and the
interpretabilityofthelearnedlwo-dimensionalconcepts(Fig-
ure 3b&c). We used the THINGS Odd-one-out dataset as
the similarity judgment behavior dataset to train our con-
ceptencodingmodel. Wetestedvariousconfigurationsofthe
concept encoding model (which will be detailed in the Ap-
pendix). Figure 3a shows the results of the optimal model
configuration. Weused42-dimensionalconceptembeddings
for comparison with previous SOTA model (VICE) and our
experiments have found that more than 42 dimensions only
bringsmallimprovements. Intermsofbehavioralprediction,
our best-performing model achieved an accuracy of 64.07%
on the THINGS Odd-one-out dataset, surpassing the previ-
ous best model VICE’s accuracy of 63.27%. We also com-
paredthesimilaritypredictionsofvisualobjectsbetweenour
model and VICE, and the Pearson correlation coefficient of
theirpredictionsreached0.94. Thisindicatesthatourmodel
canaccuratelypredicthumansimilarityjudgmentbehaviors.
Onemajormeritofourconceptembeddingisitsgoodin-
terpretability. Figure3bshowsthelow-dimensionalconcept
embeddings for the two example visual objects. It is obvi-
ousthattheconceptembeddingsencodedbyCoCoGwellde-
scribe the visual objects, and the activation of concepts in a
visualobjectexhibitsafavorablepropertyofsparsity,which
Figure3:Theperformanceoftheconceptencoderinpredictingand isinlinewithhumanintuition(seeAppendix). Weusedthe
explaining human behavior. (a) Our model’s prediction accuracy
CLIP-Dissectmodeltoautomaticallychoosewordsfromthe
forsimilarityjudgmentbehavioris64.07%,exceedingtheprevious
lexicontodescribetheconceptsineachofthe42dimensions.
SOTAmodelVICE’s63.27%(bluedashedline),withonlyslightly
Note that the relationship between these words and the con-
lower than the noise ceiling (gray dashed line)The Pearson corre-
ceptsisnotabsoluteandisonlyforexamples.Figure3shows
lationcoefficientbetweenthesimilarityofvisualobjectspredicted
byourmodelandbyVICEis0.94;(b)Examplevisualobjectsand the visual objects with the highest activation in the first two
theirconceptembeddings, withdashedlinesrepresentingthe90th conceptualdimensions(i.e.,hometoolsandbakedfood).Im-
percentile of activated concepts; (c) Example visual objects with portantly,wefindthatthevisualobjectsthatsignificantlyac-
significant activation on the concept Home tools and Baked food, tivate these dimensions are highly consistent with these two
respectively. concepts. This indicates that our model can effectively en-
code the conceptual embeddings of visual objects, and the
encodedconceptshavegoodinterpretability.
CLIP embedding as an intermediate variable In both
processes from visual objects to concept embeddings and
3.2 Conceptdecodercangeneratevisualobjects
from concept embeddings back to visual objects, we use
consistentwithconceptembedding
CLIP embeddings as an intermediate variable. This is for
twopurposes: 1)Fortheconceptencoder,CLIPembeddings Inthissection,wevalidatethegenerativeeffectivenessofthe
aresufficientlylow-dimensionalandcontainkeyinformation concept decoder. Specific training parameters are shown in
of images. Previous study has shown that using CLIP em- theAppendix. Figure4showsvisualobjectsgeneratedunder
beddings along with simple linear probing can well predict the guidance of concept embeddings. These visual objects
humanbehaviorinsimilarityjudgmenttasks[Muttenthaleret generatedfromthesameconceptembeddingarewell-aligned
al., 2022a]. 2)Fortheconceptdecoder, usingCLIPembed- withtheconceptembeddingandhavegooddiversity,demon-Figure 4: The visual objects generated by controlling the concept Figure6:Visualobjectsgeneratedwiththesameconceptembedding
embeddings. combinedwithdifferenttextprompts.Thefirstimageinthefirstrow
isgeneratedwithoutusinganytextprompt,whileotherimagesare
generated by using different animal species as text prompts. The
images in the second row are generated by using different artistic
strating that our diffusion model can conditionally generate stylesastextprompts.
visualobjectsconsistentwiththeconceptembeddings.
We quantify the similarity between the concept embed-
4 CoCoGforstudyingcounterfactual
dings of the generated visual objects and the target concept
explanationsofhumanbehaviors
embeddingsaccordingtoEq.2. Thesimilaritybetweengen-
eratedvisualobjectsandtargetconceptembeddingsissignif-
icantly higher than that between two random visual objects According to the role of concept embedding, manipulating
(Figure 5a). Additionally, by adjusting the guidance scale, theconceptembeddingcandirectlyinfluencehumansimilar-
wecancontroltheguidingstrengthofthetargetconceptem- ityjudgmentbehavior. Conversely,doesthechangeinvisual
beddings, thereby controlling the similarity and diversity of objectsthatdonotaffecttheconceptembeddinghavenoim-
thegeneratedvisualobjects(specificmetricscalculationcan pact on human behavior? CoCoG is an excellent tool to ex-
befoundintheAppendix). Astheguidancescaleincreases, plorethiscounterfactualquestion.
the similarity between the visual objects and the target con-
ceptembeddingsincreaseswhilethediversitydecreases(Fig-
ure5b). 4.1 Flexiblecontrollingofgeneratedobjectswith
textprompts
As shown in 6, we used the same concept embedding com-
bined with different text prompts to generate visual objects.
Regardless of the changes in the category and style of the
visual objects, the images consistently retained the charac-
teristics of the concept embedding. Note that we fixed the
random seed to highlight the differences in text prompts. In
theory, these completely different visual objects would lead
tosimilarjudgmentbehaviorsinsimilarityjudgmentexperi-
ments.
Further, we show visual objects generated using the same
text prompt combined with different concept embeddings
Figure5:Measurementsoftheperformanceofgeneratedvisualob-
(Figure 7). The images in the upper half were used to ex-
jects. (a)Thesimilaritybetweenrandomvisualobjectsandthetar-
tract concept embeddings, which were then combined with
getconceptembeddings(blue),andthesimilaritybetweengenerated
text prompts to generate the new images in the lower half.
visualobjectsandthetargetconceptembeddings(orange);(b)The
similaritybetweenvisualobjectsandtargetconceptembeddingsas Thegeneratedimagesreflecttheproperty”teddybear”after
theguidancescalechanges(blue),andthediversityofvisualobjects adding the text prompt, with preserving the concept embed-
astheguidancescalechanges(orange). dings. Therefore,contrarytotheconcept-editingexperiment,
modifying text prompts do not lead to changes in judgment
behaviorbecausetheconceptembeddingispreserved.5 RelatedWorks
5.1 ConceptEmbeddings
Numerous computational models have been developed
to model embeddings encoding in similarity judgment
tasks [Roads and Love, 2023; Hebart et al., 2020]. These
methods aim to study concept embeddings through similar-
ityjudgmenttasks,eitherbyoptimizingconceptembeddings
for each object [Zheng et al., 2018; Roads and Love, 2021;
Hebart et al., 2020; Muttenthaler et al., 2022b] or by using
DNNstopredicthumansimilarityjudgmentbehavior[Peter-
sonetal.,2018;Marjiehetal.,2022b;Marjiehetal.,2022a;
Muttenthaleretal., 2022a;Jhaetal., 2023;Fuetal., 2023].
Previous methods, assuming sparsity, continuity, and pos-
itivity of concept embeddings, learn low-dimensional con-
cept embeddings for each object through probabilistic mod-
els. These embeddings have good interpretability but can-
not generalize to new objects. On the other hand, DNN-
basedmethodsconstructconceptembeddingsfromneuralac-
tivationsbyreducingthedimensionalityofhigh-dimensional
DNNlatentrepresentations[Jhaetal.,2023],aligningDNN-
based similarity judgments with human judgments [Mut-
Figure 7: Visual objects generated with different concept embed-
tenthaler et al., 2023], or using multimodal inputs to im-
dingscombinedwiththesametextprompt.Theimagesintheupper
andlowerhalvescorrespondtoeachother. Theimagesintheup-
provesimilarityjudgmentpredictions[Marjiehetal.,2022b;
perhalfwereusedtoextractconceptembeddings,whichwerethen Marjiehetal.,2022a]. Thisapproachcanutilizestate-of-the-
combined with the text prompt ‘a teddy bear’ to generate the new artDNNmodelstoformembeddingsthatnaturallygeneralize
imagesinthelowerhalf. tonewobjects. Toenhancebehavioralexperiments,Roadset
al. utilizedactivelearningcoupledwithatrialselectionstrat-
egy for efficient concept embedding inference [Roads and
4.2 Manipulatingthesimilarityjudgment Love, 2021]. Similarly, DreamSim leveraging Stable Diffu-
decisionsbyinterveningthekeyconcepts sion, generates synthetic data within specified categories to
create a Perceptual dataset, aimed at studying Human per-
In this experiment, we designed a simple scenario to show ceptualjudgments[Fuetal.,2023].
how CoCoG can manipulate similarity judgment behavior.
Suppose participants need to perform a Two alternative 5.2 ConditionalDiffusionModels
forcedchoiceexperiment,i.e.,theyneedtochoosefromRef- Recently, conditional generative models based on diffusion
erence1&2whichismoresimilartotheQuery. Inthissce- models have seen significant development. The classifier-
nario,wecandirectlyincreaseordecreasecertainconceptsto free guidance method has demonstrated strong controllable
maketheQuerymoresimilartooneofthereferences. Con- generative capabilities through supervised learning [Ho and
sidering that we use dot product similarity, we can simply Salimans, 2022]. Thesemethodsdominatethetasksoftext-
choosetheconceptthatissignificantintheReferencebutnot to-image generation. From the widely used Stable Diffu-
in the Query as the key concept. The results in Figure 8, sion[Rombachetal.,2022;Podelletal.,2023;Saueretal.,
showthatthepreferenceoftheQueryforReference1&2is 2023] to subsequent methods like ControlNet [Zhang et al.,
manipulated by modifying two and three concepts, with the 2023]andIP-Adapter[Yeetal.,2023],theyhaveaddedmore
conceptsshiftingtotwodifferentdirections. Itisevidentthat controllable conditions to conditional generation, such as
thegeneratedvisualobjectsbyCoCoGcanchangesmoothly edgemaps,segmentationmaps,depthmaps[Rombachetal.,
butsignificantlywiththemanipulationofconcepts. 2022;Rameshetal.,;Mengetal.,2021;Zhangetal.,2023;
Thismethodprovidesuswithaneffectivetoolforanalyz- Yu et al., 2023; Bansal et al., 2023]. Additionally, condi-
ingthecausalmechanismsofconceptsinsimilarityjudgment tionalgenerativemodelsbasedonhumanfeedbackandpref-
tasks.Forexample,intheexperiment,wecandirectlymanip- erenceshavealsoshownpotential. WorkslikeFABRICand
ulatetheconceptsofinterestandobservesubsequentbehav- DPOKusereal-timesimilaritytaskstoguidemodelstogen-
ior. Researcherscanactivelychoosetoexploretheimpactof erateimagesmeetinghumanneeds,provingthathumanfeed-
certainconceptsonbehaviorandanalyzethecausalrelation- backcanprovidemorenuancedcontroloverconditionalgen-
shipbetweenchangesinconceptsandjudgmentbehavior.By erative models [von Ru¨tte et al., 2023; Fan et al., 2023;
focusing on informative concepts and precisely controlling Tang et al., 2023]. Methods like Pick-a-pic and Human
the activation values of concepts, this method is expected to Preference Score generate images aligned with human aes-
improvetheefficiencyofdatacollectionandsignificantlyre- thetics [Kirstain et al., 2023; Wu et al., 2023]. These ad-
duce the number of experimental trials needed (rather than vancementsdemonstratethat,incomparisontostandardtext-
usingmillionsoftrials[Hebartetal.,2023]). to-imageframeworks,conditionalgenerativemodelspossessFigure8: Manipulatingsimilarityjudgmentbehaviorbykeyconceptsintervention. (upper)Thevisualobjectsgeneratedbyusing“many
colors”and“plant”askeyconcepts.(bottom)Thevisualobjectsgeneratedbyusing“groundanimals”,“cottonclothing”,and“babytoys”as
keyconcepts.
significant potential for more closely meeting human needs Future directions In the future, we will extend the
andpreferences. paradigmofhumanvisualcognitionresearchtothestudyof
AI representational spaces, which would help align AI with
6 Discussion humansandprovidenewinsightsforunderstandingAI’scog-
nition. Also, we recommend to bring optimal experimental
WeproposedtheCoCoGmodel,capableofpredictinghuman design [Rainforth et al., 2023; Roads and Love, 2021] into
visualsimilarityjudgmentbehaviorandlearninghumancon- humanexperiments. Itwilllargelyimprovetheefficiencyof
ceptualembeddingsforvisualobjects. Itcanalsoefficiently human behavioral data collection, facilitate model learning,
and controllably generate visual objects in line with human optimizingexperimentalparadigmswithbroaderapplications
cognition (Fig 3), manipulating human similarity judgment incognitivescienceandAI.
behavior and studying causal mechanisms in human cogni-
tion(Fig8).
EthicalStatement
Contributions to AI Our approach bridges generative
models and human visual cognition. Through the concept The human behavioral data in this study is from THINGS
encodingmodel,wealignDNNswithhumanvisualconcept public dataset. No animal or human experiments are in-
representations, simulating human processing and responses volved.
to visual objects more precisely, with potential to enhance
AI’s visual understanding capabilities; through controllable
Acknowledgments
diffusiongenerationbasedonconceptembeddings,wemake
conditionalgenerativemodelsmorecloselylinkedtohuman
This work is supported by the National Key R&D Pro-
cognition and can manipulate human behavior through gen-
gram of China (2021YFF1200804), Shenzhen Science and
eratedstimuli,promisingtoimprovecontrolandsafetyinAI-
Technology Innovation Committee (20200925155957004,
humaninteractions.
KCXFZ20201221173400001, SGDX2020110309280100),
Contributions to cognitive science Our approach signifi- GuangdongProvincialClimbingPlan(pdjh2024c21310).
cantlyexpandsresearchonhumanvisualcognitionincogni-
tivescience. Withtheconceptencodingmodel,weachieved
Appendix
interpretableconceptencodingforvisualobjects(Figure3b);
withcontrollablediffusiongenerationbasedonconceptem-
A Conceptencoder
beddings,wecangeneratearichvarietyofnaturalstimulito
controlhumansimilarityjudgmentbehaviors(Figure[Roads
A.1 Objectivefunctions
andLove,2021;Fuetal.,2023]).BycombiningadvancedAI
models with cognitive science research methods, we greatly Our concept encoder is designed to learn human concept
enhance the efficiency and breadth of visual cognition re- representations of visual objects by fitting human similarity
search. Additionally, this method may reveal causal mech- judgment behavior. In line with prior studies [Muttenthaler
anismsinhumanvisualcognition,offeringnewperspectives etal., 2022a], weemploytwostrategiestotraintheconcept
forunderstandinghumancognitiveprocesses. encoder.Fittinghumanbehavior GivenanimagesimilarityS and Table1: Comparisonofdifferentnetworksarchitecturesforbehav-
atriplet{c ,c ,c }withc=g (f(x))andS =⟨c ,c ⟩,we ioralprediction.Theaccuracyinthetestsetforeachmodelislisted.
i j k θ ij i j
CoCoGvariantsoftheconceptencoderarecomparedtoabaseline
usethesoftmaxofobjectsimilaritytomodeltheprobability
(Chance)andanupperbound(noiseceiling). EmbandBehmean
that{a,b}∈{i,j,k}isthemostsimilarpair:
differentobjectivefunctions.
exp(S )
p({a,b}|{i,j,k}):= ab Behavioralprediction
exp(S )+exp(S )+exp(S )
ij ik jk Chance* 33.33%
(4)
Ourmodelaimstomaximizethelog-likelihoodoftheodd- LinProj+Emb 61.90%
one-outjudgments. Divergingfrompreviouswork,weintro- MLP+Emb 63.02%
duceanl regularizationtermintheconceptctomaintainthe
1 LinProj+Beh 62.53%
sparsityofconceptembeddings:
MLP+Beh 64.07%
n noiseceiling* 67.22%
1 (cid:88)
argmin − logp({a,b}|{i,j,k})+λ∥c∥ (5)
n
θ s=1
Fitting VICE embeddings To facilitate comparisons with
current methodologies, our model is also capable of fit-
ting existing concept embeddings. As an example, we use
VICE[Muttenthaleretal.,2022b]embeddings:
m
(cid:88)
argmin ∥c¯ −g (f(x ))∥+λ∥c ∥ (6)
m θ m m
θ
i=1
where c¯ represents the VICE dimensions for image m,
m
and c = g (f(x )) denotes the concept embeddings pre-
m θ m
dictedbyourmodelforimagem.
A.2 Modelcomparison
In Table 1, we compare various network architectures in
terms of their ability to predict behavioral outcomes. As
shown, we have evaluated different combinations of linear
projection (LinProj) and multi-layer perceptron (MLP) with
embedding-based(Emb)andbehavior-based(Beh)objective
functions. The results are benchmarked against a baseline
Figure9:Modelgeneralizationperformancewithinadataset,across
(Chance)andanupperbound(noiseceiling)toprovidecon- datasets,andacrosstasks.
textfortheirperformance.
From these comparisons, we can observe that the MLP
models, especially when combined with behavior-based ob- the VICE embeddings, showcasing Pearson correlation co-
jectives, tend to offer superior performance, as indicated by efficients of 0.94 and 0.92 for the Novel Triplets and Novel
their higher accuracy in the test set. This suggests that the Objects experiments, respectively. These results underscore
MLP architecture, particularly when aligned with behavior- therobustnessofourmodelinhandlingwithin-datasetvaria-
oriented training, is more effective in capturing the nuances tions.
of human conceptual representations. But for comparison Whenfocusingonthecross-datasetscenariowiththeFood
withVICE,weuselinearprojectionmbedding-basedobjec- Dataset [Carrington et al., 2024], our model continues to
tivefunctionsforsubsequentconceptdecoder. maintainhighperformancestandards.Here,itachievesapre-
dictionaccuracyof72.41%onthevalidationsetaftertraining
A.3 Additionalresultsformodelvalidation on the respective train set. The object similarity calculated
Generalization across objects, datasets, and tasks Fig- byourmodelinthisnovelfooddatasetexhibitsasignificant
ure 9 demonstrates our model’s performance within and Pearson correlation with VICE embeddings, indicated by a
across datasets, as well as its adaptability to different task coefficient of 0.81. This outcome demonstrates the model’s
paradigms. InthecontextoftheTHINGSdataset[Hebartet capacitytoadaptandperformeffectivelyacrossdifferentcon-
al., 2023], our model exhibits strong generalization capabil- tentdomains.
ities, as evidenced by its accuracy in predicting novel odd- Additionally, we explored the cross-task generalization
one-out decisions in both the validation set (Novel Triplets) ability of our model through the Multi-arrangement task
andasubsetofobjects(NovelObjects),achievingaccuracies paradigm [King et al., 2019]. When trained on this new
of 64.07% and 59.55%, respectively. The object similarity taskparadigm, theobjectsimilaritycalculatedbyourmodel
computedbyourmodel’sembeddingscorrelateshighlywith showedanotablecorrelationwiththeexperimentaldata, in-dicated by a Pearson coefficient of 0.62. The alignment of
model-predictedandexperimentallymeasuredrepresentation
distance matrices, particularly concerning object classifica-
tion,furthervalidatesourmodel’sefficacy.
Figure11:Modelforconceptdescriptionselection.
B Conceptdecoder
In this section, we provide a concise overview of the con-
ditional diffusion model framework used in our concept de-
coder, following the presentation of continuous-time diffu-
sionmodelsin[Songetal.,2020;Karrasetal.,2022].
Figure 10: Concept prediction by concept encoder. (a) Predicted
accuraciesforeachconceptofVICE;(b)imageswithhighactivation Diffusion models Diffusion Models (DMs) engage in a
inselectedconcepts. generative process by transforming high-variance Gaussian
noise into structured data representations. This transforma-
tion is achieved by gradually reducing noise levels across a
Concept encoder performance Figure 10 showcases the
sequenceofsteps.Specifically,webeginwithahigh-variance
proficiency of our concept encoder in extracting and repre-
Gaussiannoisex ∼N(0,σ2 )andsystematicallydenoise
senting various concepts. Our analysis reveals a significant M max
it through a series of steps to obtain x ∼ p(x ;t), where
negativecorrelationbetweentheimportanceofaconceptand t t
σ < σ and σ = σ . For a well-calibrated DM, and
its predictability, quantified by Pearson’s r. Specifically, the t t+1 M max
withσ = 0,thefinalx alignswiththeoriginaldatadistri-
correlationcoefficientforthisrelationshipstandsat-0.75(p 0 0
bution.
= 1.3e-8), suggesting that concepts deemed more important
tendtobemorepredictable,whichalignswithourinitialex- Sampling process The sampling in DMs is implemented
pectations. For this evaluation, we utilized concept embed- by numerically simulating a Probability Flow ordinary dif-
dingsfrompreviousresearchasabenchmarkforcomparison. ferentialequation(ODE)orastochasticdifferentialequation
Complementingthisquantitativeanalysis,thevisualrepre- (SDE).TheODEisrepresentedas:
sentationsprovidedinthefigureillustrateourmodel’seffec-
dx=−σ˙(t)σ(t)∇ logp(x;t)dt, (8)
x
tivenessinactivatingmultipleconceptualdimensions. These
exampleimagesclearlydemonstratethemodel’snuancedca- where∇ xlogp(x;t)isthescorefunction,andσ(t)isapre-
pabilitytorecognizeandencodeadiverserangeofconcepts, definedschedulewithitstimederivativeσ˙(t). TheSDEvari-
therebyreaffirmingitsapplicabilityandeffectivenessincom- antincludesaLangevindiffusioncomponentandisexpressed
plexconceptualunderstandingtasks. as:
dx=−σ˙(t)σ(t)∇ logp(x;t)dt
A.4 Descriptionofconcepts x
Inspired by the CLIP-Dissect model [Oikarinen and Weng,
−β(t)σ2(t)∇ xlogp(x;t)dt (9)
(cid:112)
2022],weadoptamutualinformationmaximizationmethod + 2β(t)σ(t)dω ,
t
to select descriptions for each concept dimension. As de-
wheredω isthestandardWienerprocess.
picted in Figure 11, we compiled a descriptive word dataset t
T containing23,153words. ForagivenconceptC ,wecal- Training of DMs The core of DM training is to learn
k
culate the mutual information between the descriptions and a model s θ(x;t) for the score function. This is typically
themaximumactivationvisualobjectsB k oftheconceptdi- achievedthroughdenoisingscorematching(DSM),whereϵ θ
mension,selectingwordsthatyieldthehighestmutualinfor- is a learnable denoiser. The training process can be formu-
mation. We employ the SoftWPMI equation, as utilized by latedas:
CLIP-Dissect: E (cid:2) ∥ϵ (x +n ;t,c)−x ∥2(cid:3) ,
(x0,c)∼pdata(x0,c),(nt,t)∼p(nt,t) θ 0 t 0 2
(10)
sim(t m,q k;P)=logE[p(t m|B k)]−λlogp(t m) (7) wheren tisGaussiannoisewithvarianceσ t2,andcrepresents
acondition.Index LanguageDescription
1 hometools,metaltools,handtools,metallictools,tools
2 bakedfood,preparedfood,food,madedish,entrees
3 plant,greenplants,greenplantsandherbs,plants,commoninoutdoor
4 groundanimals,smallanimals,landanimals,mammal,animaltype
5 householdfurniture,furniture,maincomponentofroom,affordability,livingfurniture
6 cottonclothing,clothing,clothes,thingstowear,unisex
7 decorative,gaudiness,decoration,intricacy,jewelry
8 outdoorobjects,buildingmaterials,ground,foundation,protruding
9 pointytools,handtools,sharptools,metaltools,metallictools
10 wood,wood-colored,madeofwood,furniture,buildingmaterials
Table2:Fivelanguagedescriptionsfoundbythemodelthatbestmatchthefirsttenconcepts.
B.1 StageI-priordiffusion this purpose, we employ the integration of pre-trained mod-
Thetrainingofthepriordiffusionstageisacriticalstepinour els, namely SDXL and IP-Adapter [Podell et al., 2023;
conceptdecoder,employingtheclassifier-freeguidancetech- Ye et al., 2023], to achieve efficient and high-quality image
niqueinconjunctionwithdatapairsofCLIPembeddingsand generation.
conceptembeddings(h ,c ). Drawingonprinciplesfromad- The backbone of our image generation process is the
i i
vancedgenerativemodels,ourpriordiffusionprocessiscon- SDXLmodel,knownforitsrobustnessintext-to-imagediffu-
ditionedontheconceptembeddingctoeffectivelylearnthe sionprocesses.ByincorporatingtheIP-Adapter,whichintro-
distribution of CLIP embeddings p(h|c). The CLIP embed- ducesdualcross-attentionmodules,theCLIPembeddinghis
dinghobtainedinthisstageservesasthepriorforthesubse- effectively used as a conditional input, thereby guiding the
quent stage. Our model architecture includes a lightweight denoising process in the U-Net architecture. The combined
U-Net, denoted as ϵ prior(h t,t,c), where h t represents the model for this process is denoted as ϵ SD(z t,t,h), where z t
noisy CLIP embedding at the diffusion time step t. Train- representsthenoisylatentsfromSDXL’sVariationalAutoen-
ingpairsconsistingofCLIPandconceptembeddingsareex- coder(VAE).
tractedfromtheImageNetdataset,whichhaveover1million
Advantages of pre-Trained model integration Utilizing
images. ThesepairsarethenusedtotrainthePriorDiffusion
these pre-trained models allows us to leverage their inher-
model. Themodelisproficientlytrainedusingtheclassifier-
entcapabilitieswithouttheneedforadditionalmodifications.
free guidance method, which ensures a balance between fi-
Thisapproachnotonlysimplifiestheimplementationprocess
delity to the conditioning signal and the generative diversity
but also ensures the retention of the quality attributes inher-
oftheoutputs.
ent in these models. The integration of concept embedding
Classifier-free guidance technique The Classifier-Free guidancewiththesepre-trainedmodelsfacilitatesthegenera-
Guidance method plays a pivotal role in directing the itera- tionofvisualobjectsthatarecloselyalignedwiththeconcept
tive sampling of a Diffusion Model (DM) in response to a informationencodedinh.
givencondition,specificallyaconceptc. Thistechniqueop-
SDXL-turbo for enhanced speed To further enhance the
erates by harmonizing the outputs from both a conditional
efficiency of our model, we also explore the use of SDXL-
model and an unconditional model. The formulation of the
Turbo [Sauer et al., 2023], a distilled version of SDXL, op-
combinedmodel,ϵw (h;t,c),isgivenas:
prior timized for real-time synthesis. This model is particularly
ϵw (h;t,c)=(1+w)ϵ (h;t,c)−wϵ (h;t), (11) advantageous in scenarios where rapid generation of high-
prior prior prior
fidelityimagesisrequired.
wherew ≥0indicatestheguidancescale. Thisapproachen-
IP-Adapter TheIP-Adapter,withitsrelativelylightweight
ablessimultaneoustrainingoftheunconditionalmodelalong-
architecture,hasdemonstrateditseffectivenessinaddingim-
sidetheconditionalmodelwithinasinglenetworkarchitec-
agepromptcapabilitytopre-trainedtext-to-imagemodels.Its
ture. It is accomplished by intermittently replacing the con-
ability to work in tandem with text prompts for multimodal
ceptcwithanullvector, asperEquation(3), atafixedpro-
image generation broadens the scope of applications for our
portion of the time, like 10%. The chief application of this
conceptdecoder.
method is to refine the quality of the samples generated by
DMs,balancingitagainstthediversityoftheoutputs.
B.3 PerformanceMetrics
B.2 StageII-CLIPguidancegeneration
Similarity We define similarity using the dot product as
In Stage II of our concept decoder, the CLIP embedding h mentioned in the Objective Functions section. Specifically,
obtained from the prior diffusion training serves as the ba- thesimilarityS betweenthetargetconceptembeddingand
ij
sis for generating visual objects x conditioned on h. For theconceptembeddingextractedfromthegeneratedimageiscalculatedas: SherjilOzair,AaronCourville,andYoshuaBengio. Gen-
S =⟨c ,c ⟩ (12) erative adversarial networks. Communications of the
ij i j
ACM,63(11):139–144,2020.
wherec andc representtherespectiveconceptembeddings.
i j
[Hebartetal.,2020] Martin N Hebart, Charles Y Zheng,
Diversity Inspired by works in the literature [von Ru¨tte et
FranciscoPereira,andChrisIBaker. Revealingthemulti-
al., 2023; Corso et al., 2023], we describe image diversity
dimensionalmentalrepresentationsofnaturalobjectsun-
for a single concept embedding using In-batch CLIP simi-
derlyinghumansimilarityjudgements. Naturehumanbe-
larity. We use this approach to assess the variety of images
haviour,4(11):1173–1185,2020.
generatedfromthesameconceptembedding. In-batchCLIP
similarity quantifies how different the generated images are [Hebartetal.,2023] Martin N Hebart, Oliver Contier, Lina
fromoneanotherwithinthesamebatch,reflectingthediver- Teichmann, Adam H Rockter, Charles Y Zheng, Alexis
sityofoutputsforagivenconceptembedding. TheIn-batch Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and
ImageDiversitydforabatchofimagesx ,...,x isdefined Chris I Baker. Things-data, a multimodal collection of
1 n
asfollows: large-scaledatasetsforinvestigatingobjectrepresentations
inhumanbrainandbehavior. Elife,12:e82580,2023.
n i−1
d(x ,...,x )=1− 2 (cid:88)(cid:88) CLIP(x ,x ) (13) [HoandSalimans,2022] Jonathan Ho and Tim Salimans.
1 n n(n−1) i j Classifier-free diffusion guidance. arXiv preprint
i=2j=1
arXiv:2207.12598,2022.
where n(n−1) isthenumberofelementsintheuppertriangu- [Hoetal.,2020] JonathanHo,AjayJain,andPieterAbbeel.
2
larcosinesimilaritymatrix. Denoising diffusion probabilistic models. Advances in
neural information processing systems, 33:6840–6851,
References 2020.
[Bansaletal.,2023] Arpit Bansal, Hong-Min Chu, Avi [Jhaetal.,2023] Aditi Jha, Joshua C Peterson, and
Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Thomas L Griffiths. Extracting low-dimensional psy-
Jonas Geiping, and Tom Goldstein. Universal guidance chological representations from convolutional neural
for diffusion models. In Proceedings of the IEEE/CVF networks. Cognitivescience,47(1):e13226,2023.
ConferenceonComputerVisionandPatternRecognition, [Karrasetal.,2022] Tero Karras, Miika Aittala, Timo Aila,
pages843–852,2023. and Samuli Laine. Elucidating the design space of
[Carringtonetal.,2024] MadelineCarrington, AlexanderG diffusion-based generative models. Advances in Neural
Liu, Caroline Candy, Alex Martin, and Jason A Avery. InformationProcessingSystems,35:26565–26577,2022.
Naturalistic food categories are driven by subjective es- [Kawaretal.,2022] Bahjat Kawar, Michael Elad, Stefano
timates rather than objective measures of food qualities. Ermon, and Jiaming Song. Denoising diffusion restora-
FoodQualityandPreference,113:105073,2024. tionmodels. AdvancesinNeuralInformationProcessing
[Chungetal.,2022] Hyungjin Chung, Byeongsu Sim, Do- Systems,35:23593–23606,2022.
hoonRyu,andJongChulYe. Improvingdiffusionmodels [Kingetal.,2019] Marcie L King, Iris IA Groen, Adam
forinverseproblemsusingmanifoldconstraints.Advances Steel, Dwight J Kravitz, and Chris I Baker. Similarity
in Neural Information Processing Systems, 35:25683– judgments and cortical visual responses reflect different
25696,2022. propertiesofobjectandscenecategoriesinnaturalisticim-
[Corsoetal.,2023] Gabriele Corso, Yilun Xu, Valentin ages. NeuroImage,197:368–382,2019.
DeBortoli,ReginaBarzilay,andTommiJaakkola.Particle [Kirstainetal.,2023] Yuval Kirstain, Adam Polyak, Uriel
guidance:non-iiddiversesamplingwithdiffusionmodels. Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.
InNeurIPS2023WorkshoponDeepLearningandInverse Pick-a-pic:Anopendatasetofuserpreferencesfortext-to-
Problems,2023. imagegeneration.arXivpreprintarXiv:2305.01569,2023.
[Fanetal.,2023] YingFan,OliviaWatkins,YuqingDu,Hao [Marjiehetal.,2022a] Raja Marjieh, Ilia Sucholutsky,
Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Theodore R Sumers, Nori Jacoby, and Thomas L Grif-
Mohammad Ghavamzadeh, Kangwook Lee, and Kimin fiths. Predicting human similarity judgments using large
Lee.Dpok:Reinforcementlearningforfine-tuningtext-to- languagemodels. arXivpreprintarXiv:2202.04728,2022.
imagediffusionmodels.arXivpreprintarXiv:2305.16381,
[Marjiehetal.,2022b] Raja Marjieh, Pol Van Rijn, Ilia Su-
2023.
cholutsky, Theodore Sumers, Harin Lee, Thomas L Grif-
[Fuetal.,2023] StephanieFu,NetanelTamir,ShobhitaSun- fiths,andNoriJacoby. Wordsareallyouneed? language
daram,LucyChai,RichardZhang,TaliDekel,andPhillip as an approximation for human similarity judgments. In
Isola. Dreamsim: Learning new dimensions of human The Eleventh International Conference on Learning Rep-
visual similarity using synthetic data. arXiv preprint resentations,2022.
arXiv:2306.09344,2023.
[Medinetal.,1993] DouglasLMedin,RobertLGoldstone,
[Goodfellowetal.,2020] Ian Goodfellow, Jean Pouget- andDedreGentner. Respectsforsimilarity. Psychological
Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, review,100(2):254,1993.[Mengetal.,2021] Chenlin Meng, Yutong He, Yang Song, [RoadsandLove,2023] BrettDRoadsandBradleyCLove.
Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Er- Modelingsimilarityandpsychologicalspace. AnnualRe-
mon. Sdedit: Guided image synthesis and editing with viewofPsychology,75,2023.
stochasticdifferentialequations. InInternationalConfer-
[Rombachetal.,2022] Robin Rombach, Andreas
enceonLearningRepresentations,2021.
Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn
[MurphyandMedin,1985] Gregory L Murphy and Dou- Ommer. High-resolution image synthesis with latent
glas L Medin. The role of theories in conceptual coher- diffusion models. In Proceedings of the IEEE/CVF
ence. Psychologicalreview,92(3):289,1985. conference on computer vision and pattern recognition,
[Muttenthaleretal.,2022a] Lukas Muttenthaler, Jonas Dip- pages10684–10695,2022.
pel,LorenzLinhardt,RobertAVandermeulen,andSimon
[Saueretal.,2023] Axel Sauer, Dominik Lorenz, Andreas
Kornblith. Human alignment of neural network repre-
Blattmann, and Robin Rombach. Adversarial diffusion
sentations. In The Eleventh International Conference on
distillation. arXivpreprintarXiv:2311.17042,2023.
LearningRepresentations,2022.
[Songetal.,2020] Yang Song, Jascha Sohl-Dickstein,
[Muttenthaleretal.,2022b] Lukas Muttenthaler, Charles Y
Diederik P Kingma, Abhishek Kumar, Stefano Ermon,
Zheng, Patrick McClure, Robert A Vandermeulen, Mar-
and Ben Poole. Score-based generative modeling
tinNHebart,andFranciscoPereira. Vice: Variationalin-
throughstochasticdifferentialequations. InInternational
terpretable concept embeddings. Advances in Neural In-
ConferenceonLearningRepresentations,2020.
formationProcessingSystems,35:33661–33675,2022.
[Muttenthaleretal.,2023] Lukas Muttenthaler, Lorenz Lin- [Songetal.,2021] Yang Song, Liyue Shen, Lei Xing, and
hardt, Jonas Dippel, Robert A Vandermeulen, Katherine Stefano Ermon. Solving inverse problems in medical
Hermann,AndrewKLampinen,andSimonKornblith.Im- imaging with score-based generative models. In Interna-
provingneuralnetworkrepresentationsusinghumansimi-
tionalConferenceonLearningRepresentations,2021.
larityjudgments. arXivpreprintarXiv:2306.04507,2023. [Tangetal.,2023] Zhiwei Tang, Dmitry Rybin, and Tsung-
[OikarinenandWeng,2022] Tuomas Oikarinen and Tsui- Hui Chang. Zeroth-order optimization meets human
WeiWeng. Clip-dissect: Automaticdescriptionofneuron feedback: Provable learning via ranking oracles. arXiv
representations in deep visionnetworks. In The Eleventh preprintarXiv:2303.03751,2023.
International Conference on Learning Representations,
[Taoetal.,2022] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan
2022.
Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A
[O¨zbeyetal.,2023] Muzaffer O¨zbey, Onat Dalmaz, simple and effective baseline for text-to-image synthe-
Salman UH Dar, Hasan A Bedel, S¸aban O¨zturk, Alper sis. In Proceedings of the IEEE/CVF Conference on
Gu¨ngo¨r, and Tolga C¸ukur. Unsupervised medical image Computer Vision and Pattern Recognition, pages 16515–
translation with adversarial diffusion models. IEEE 16525,2022.
TransactionsonMedicalImaging,2023.
[vonRu¨tteetal.,2023] DimitrivonRu¨tte,ElisabettaFedele,
[Petersonetal.,2018] Joshua C Peterson, Joshua T Abbott, JonathanThomm,andLukasWolf. Fabric: Personalizing
and Thomas L Griffiths. Evaluating (and improving) the diffusion models with iterative feedback. arXiv preprint
correspondence between deep neural networks and hu- arXiv:2307.10159,2023.
manrepresentations.Cognitivescience,42(8):2648–2669,
2018. [Wuetal.,2023] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui
Zhao, and Hongsheng Li. Human preference score: Bet-
[Podelletal.,2023] Dustin Podell, Zion English, Kyle
teraligningtext-to-imagemodelswithhumanpreference.
Lacey, AndreasBlattmann, TimDockhorn, JonasMu¨ller,
InProceedingsoftheIEEE/CVFInternationalConference
Joe Penna, and Robin Rombach. Sdxl: Improving la-
onComputerVision,pages2096–2105,2023.
tentdiffusionmodelsforhigh-resolutionimagesynthesis.
arXivpreprintarXiv:2307.01952,2023. [Yeetal.,2023] HuYe,JunZhang,SiboLiu,XiaoHan,and
Wei Yang. Ip-adapter: Text compatible image prompt
[Rainforthetal.,2023] TomRainforth,AdamFoster,DesiR
adapterfortext-to-imagediffusionmodels. arXivpreprint
Ivanova, and Freddie Bickford Smith. Modern bayesian
arXiv:2308.06721,2023.
experimental design. arXiv preprint arXiv:2302.14545,
2023. [Yuetal.,2023] Jiwen Yu, Yinhuai Wang, Chen Zhao,
[Rameshetal.,] Aditya Ramesh, Prafulla Dhariwal, Alex BernardGhanem,andJianZhang.Freedom:Training-free
Nichol, Casey Chu, and Mark Chen. Hierarchical text- energy-guidedconditionaldiffusionmodel.arXivpreprint
conditionalimagegenerationwithcliplatents. arXiv:2303.09833,2023.
[RoadsandLove,2021] BrettDRoadsandBradleyCLove. [Zhangetal.,2023] Lvmin Zhang, Anyi Rao, and Maneesh
Enrichingimagenetwithhumansimilarityjudgmentsand Agrawala.Addingconditionalcontroltotext-to-imagedif-
psychologicalembeddings. InProceedingsoftheieee/cvf fusionmodels. InProceedingsoftheIEEE/CVFInterna-
conference on computer vision and pattern recognition, tionalConferenceonComputerVision,pages3836–3847,
pages3547–3557,2021. 2023.[Zhengetal.,2018] Charles Y Zheng, Francisco Pereira,
Chris I Baker, and Martin N Hebart. Revealing inter-
pretable object representations from human behavior. In
International Conference on Learning Representations,
2018.