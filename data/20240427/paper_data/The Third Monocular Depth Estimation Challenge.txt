The Third Monocular Depth Estimation Challenge
JaimeSpencer1 FabioTosi2 MatteoPoggi2 RipudamanSinghArora3 ChrisRussell4
SimonHadfield5 RichardBowden5 GuangYuanZhou6 ZhengXinLi7 QiangRao6
YiPingBao6 XiaoLiu6 DohyeongKim8 JinseongKim8 MyunghyunKim8
MykolaLavreniuk9 RuiLi10 QingMao10 JiangWu10 YuZhu10 JinqiuSun10
YanningZhang10 SurajPatni11 AradhyeAgarwal11 ChetanArora11 PihaiSun12
KuiJiang12 GangWu12 JianLiu12 XianmingLiu12 JunjunJiang12 XidanZhang13
JianingWei13 FangjunWang13 ZhimingTan13 JiabaoWang14 AlbertLuginov15
MuhammadShahzad15 SeyedHosseini16 AleksanderTrajcevski16 JamesH.Elder16
Abstract Foryears,mostproposedapproacheshavebeentailored
to training and testing in a single, defined domain – e.g.,
This paper discusses the results of the third edition of automotiveenvironments[33]orindoorsettings[64]–of-
the Monocular Depth Estimation Challenge (MDEC). The ten ignoring their ability to generalize to unseen environ-
challenge focuses on zero-shot generalization to the chal- ments. Purposely, the Monocular Depth Estimation Chal-
lenging SYNS-Patches dataset, featuring complex scenes lenge(MDEC)inthelastyearshasencouragedthecommu-
in natural and indoor settings. As with the previous edi- nitytodelveintothisaspect,byproposinganewbenchmark
tion, methods can use any form of supervision, i.e. su- for evaluating MDE models on a set of complex environ-
pervised or self-supervised. The challenge received a to- ments, comprising natural, agricultural, urban, and indoor
talof19submissionsoutperformingthebaselineonthetest settings. Thedatasetcomeswithavalidationandatesting
set: 10amongthemsubmittedareportdescribingtheirap- split, without any possibility of training/fine-tuning over it
proach, highlightingadiffuseduseoffoundationalmodels thusforcingthemodelstogeneralize.
such as Depth Anything at the core of their method. The WhilethefirsteditionofMDEC[90]focusedonbench-
challengewinnersdrasticallyimproved3DF-Scoreperfor- marking self-supervised approaches, the second [91] addi-
mance,from17.51%to23.72%. tionally opened the doors to supervised methods. During
the former, the participants outperformed the baseline [30,
92] in all image-based metrics (AbsRel, MAE, RMSE),
1.Introduction
but could not improve pointcloud reconstructions [65] (F-
Monocular depth estimation (MDE) aims at predicting the Score). Thelatter,instead,broughtnewmethodscapableof
distance from the camera to the points of the scene de- outperforming the baseline on both aspects, establishing a
picted by the pixels in the captured image. It is a highly new State-of-the-Art (SotA). The third edition of MDEC,
ill-posed problem due to the absence of geometric priors detailed in this paper, ran in conjunction with CVPR2024,
usuallyavailablefrommultipleimages. Nonetheless,deep followingthesuccessesofthesecondonebyallowingsub-
learninghasrapidlyadvancedthisfieldandmadeitareal- missions of methods exploiting any form of supervision,
ity,enablingresultsfarbeyondimagination. e.g.supervised,self-supervised,ormulti-task.
Following previous editions, the challenge was built
1Independent 2University of Bologna 3Blue River
around SYNS-Patches [1, 92]. This dataset was chosen
Technology 4OxfordInternetInstitute 5UniversityofSurrey
6ByteDance 7University of Chinese Academy of Science becauseofthevariegateddiversityofenvironmentsitcon-
8RGA Inc. 9Space Research Institute NASU-SSAU, Kyiv, tains, including urban, residential, industrial, agricultural,
Ukraine 10NorthwesternPolytechnicalUniversity,Xi’an 11Indian natural, and indoor scenes. Furthermore, SYNS-Patches
Institute of Technology, Delhi 12Harbin Institute of Technology contains dense high-quality LiDAR ground-truth, which is
13Fujitsu 14GuangXi University 15University of Reading verychallengingtoobtaininoutdoorsettings. Thisallows
16YorkUniversity
1
4202
rpA
52
]VC.sc[
1v13861.4042:viXraforabenchmarkthataccuratelyreflectstherealcapabilities estimates [48, 73, 112], motion masks [11, 22, 37, 98],
ofeachmodel,potentiallyfreefrombiases. optical flow [59, 81, 118], or via the minimum recon-
Whilethesecondeditioncounted8teamsoutperforming struction loss [35]. Finally, several architectural innova-
theSotAbaselineineitherpointcloud-orimage-basedmet- tions, including 3D (un)packing blocks [38], position en-
rics, this year 19 submissions achieved this goal. Among coding[36],transformer-basedencoders[2,127],sub-pixel
these, 10 submitted a report introducing their approach, 7 convolutions [71], progressive skip connections [60], and
ofwhoseoutperformedthewinningteamofthesecondedi- self-attention decoders [46, 110, 129], allowed further im-
tion. Thisdemonstratestheincreasinginterest–andefforts provements. Amongthem, lightweightmodelstailoredfor
–inMDEC. real-timeapplicationswithmemoryandruntimeconstraints
In the remainder of the paper, we will provide an havealsobeendeveloped[4,19,43,68,69,72,108].
overview of each submission, analyze their results on
Generalizationand“In-the-Wild”MDE.Estimatingdepthin
SYNS-Patches,anddiscusspotentialfuturedevelopments.
thewildreferstothechallengingtaskofdevelopingmeth-
ods that can generalize to a wide range of unknown set-
2.RelatedWork
tings[14,15]. Earlyworksinthisareafocusedonpredict-
ingrelative(ordinal)depth[14,15]. Nonetheless, thelim-
SupervisedMDE.Earlymonoculardepthestimation(MDE)
ited suitability of relative depth in many downstream con-
effortsutilizedsupervisedlearning,leveraginggroundtruth
textshasdrivenresearcherstoexploreaffine-invariantdepth
depth labels. Eigen et al. [26] proposed a pioneering
estimation[53,113]. Intheaffine-invariantsetting,depthis
end-to-endconvolutionalneuralnetwork(CNN)forMDE,
estimateduptoanunknownglobaloffsetandscale,offering
featuring a scale-invariant loss and a coarse-to-fine ar-
a compromise between ordinal and metric representations.
chitecture. Subsequent advancements incorporated struc-
Researchers have employed various strategies to achieve
turedpredictionmodelssuchasConditionalRandomFields
generalization,includingleveragingannotationsfromlarge
(CRFs) [54, 120] and regression forests [82]. Deeper net-
datasets to train monocular depth models [79, 80, 111],
work architectures [80, 109], multi-scale fusion [63], and
including internet photo collections [53, 113], as well
transformer-based encoders [8, 16, 79] further enhanced
as from automotive LiDAR [33, 38, 42], RGB-D/Kinect
performance. Alternatively, certain methods framed depth
sensors [17, 64, 95], structure-from-motion reconstruc-
estimationasaclassificationproblem[6,7,28,51]. Novel
tions [52, 53], optical flow/disparity estimation [80, 109],
loss functions were also introduced, including gradient-
and crowd-sourced annotations [14]. However, the vary-
basedregression[53,104], theberHuloss[50], anordinal
ingaccuracyoftheseannotationsmayhaveimpactedmodel
relationshiploss[14],andscale/shiftinvariance[80].
performance, and acquiring new data sources remains a
Self-Supervised MDE. To overcome the dependence on
challenge,motivatingtheexplorationofself-supervisedap-
costly ground truth annotations, self-supervised methods
proaches [116, 125]. For instance, KBR(++) [93, 94]
were developed. Garg et al. [30], for the first time, pro-
leverage large-scale self-supervision from curated internet
posed an algorithm based on view synthesis and photo-
videos. The transition from CNNs to vision transform-
metric consistency across stereo image pairs, the impor-
ers has further boosted performance in this domain, as
tance of which for was extensively analyzed by Poggi et
demonstratedbyDPT(MiDaSv3)[79]andOmnidata[25].
al. [74]. Godard et al. [34] introduced Monodepth, which
Furthermore, a few works like Metric3D [114] and Ze-
incorporated differentiable bilinear interpolation [44], vir-
roDepth [40] revisited the depth estimation by explicitly
tual stereo prediction, and a SSIM+L reconstruction loss.
1
feedingcameraintrinsicsasadditionalinput. Anotablere-
Zhou et al. [130] presented SfM-Learner, which required
cent trend involves training generative models, especially
only monocular video supervision by replacing the known
diffusion models [29, 41, 88] for monocular depth estima-
stereo transform with a pose estimation network. Follow-
tion[24,45,47,84,85].
ing the groundwork laid by these frameworks, subsequent
efforts focused on refining the depth estimation accuracy AdverseWeatherandTransparent/SpecularSurfaces. Exist-
byintegratingfeature-basedreconstructions[89,119,124], ing monocular depth estimation networks have struggled
semanticsegmentation[122],adversariallosses[3],proxy- under adverse weather conditions. Approaches have ad-
depth representations [5, 18, 48, 70, 83, 97, 107], trinoc- dressed low visibility [89], employed day-night branches
ular supervision [75] and other constraints [9, 61, 103]. usingGANs[100,126],utilizedadditionalsensors[31],or
Other works focused on improving depth estimates at ob- faced trade-offs [101]. Recently, md4all [32] enabled ro-
jectboundaries[96,99]. Moreover,attentionhasalsobeen bust performance across conditions without compromising
giventochallengingcasesinvolvingdynamicscenariosdur- ideal setting performance. Furthermore, estimating depth
ingthetrainingphase, whichposedifficultiesinproviding for transparent or mirror (ToM) surfaces posed a unique
accurate supervision signals for such networks. This has challenge[121,123].Costanzinoetal.[21]istheonlywork
been addressed, for example, by incorporating uncertainty dedicatedtothis,introducingnoveldatasets[77,78]. Their
2Outdoor-Urban Outdoor-Natural Outdoor-Agriculture Indoor
0.06 0.20
0.07 0.06
0.06 0.05
0.05
0.15
0.05 0.04
0.04
0.04
0.03 0.03 0.10
0.03
0.02 0.02
0.02 0.05
0.01 0.01 0.01
0.00 0.00 0.00 0.00
0 20 40 60 80 100120 0 20 40 60 80 100120 0 20 40 60 80 100120 0 20 40 60 80 100120
Figure 1. SYNS-Patches Properties. Top: Distribution of images per category in the validation split and the test split respectively.
Bottom: Depth distribution per scene type – indoor scenes are limited to 20m, while outdoor scenes reach up to 120m; natural and
Agriculturescenescontainalargerpercentageoflong-rangedepths(20-80m),whileurbanscenesfocusonthemid-range(20-40m).
approach relied on segmentation maps or pre-trained net- ture research directions. In this paper, we report results
works,generatingpseudo-labelsbyinpaintingToMobjects onlyforsubmissionsthatoutperformedthebaselineinany
and processing them with a pre-trained depth model [80], pointcloud-/image-basedmetricontheOveralldataset.
enabling fine-tuning of existing networks to handle ToM Dataset. The challenge takes place based on the
surfaces. SYNS-Patchesdataset[1,92],chosenduetothediversityof
scenesandenvironments. Abreakdownofimagespercat-
3.TheMonocularDepthEstimationChallenge
egoryandsomerepresentativeexamplesareshowninFig-
ure1andFigure2. SYNS-Patchesalsoprovidesextremely
The third edition of the Monocular Depth Estimation
high-quality dense ground-truth LiDAR, with an average
Challenge1 was organized on CodaLab [67] as part of a
coverage of 78.20% (including sky regions). Given such
CVPR2024workshop. Thedevelopmentphaselastedfour
dense ground-truth, depth boundaries were obtained using
weeks, using the SYNS-Patches validation split. During
Canny edge-detection on the log-depth maps, allowing us
thisphase,theleaderboardwaspublicbuttheusernamesof
to compute additional fine-grained metrics for these chal-
the participants were anonymized. Each participant could
lenging regions. As outlined in [91, 92], the images were
seetheresultsachievedbytheirownsubmission.
manuallycheckedtoremovedynamicobjectartifacts.
The final phase of the challenge was open for three
Evaluation. Participants were asked to provide the up-to-
weeks. At this stage, the leaderboard was completely pri-
scaledisparitypredictionforeachdatasetimage. Theeval-
vate,disallowingparticipantstoseetheirownscores. This
uationserverbilinearlyupsampledthepredictionstothetar-
choicewasmadetoencouragetheevaluationonthevalida-
getresolutionandinvertedthemintodepthmaps. Although
tionsplitratherthanthetestsplitand,togetherwiththefact
self-supervised methods trained with stereo pairs and su-
thatallground-truthdepthswerewithheld, severelyavoid-
pervised methods using LiDAR or RGB-D data should be
ing any possibility of overfitting over the test set by con-
capableofpredictingmetricdepth,inordertoensurecom-
ductingrepeatedevaluationsonit.
parisons are as fair as possible, the evaluation aligned any
Following the second edition [91], any form of super-
predictions with the ground-truth using the median depth.
vision was allowed, in order to provide a more compre-
Wesetamaximumdepththresholdof100meters.
hensive overview of the monocular depth estimation field
as a whole. This makes it possible to better study the Metrics. Followingthefirstandsecondeditionsofthechal-
lenge [90, 91], we use a mixture of image-/pointcloud-
gapbetweendifferenttechniquesandidentifypossible,fu-
/edge-based metrics. Image-based metrics are the most
1 https://codalab.lisn.upsaclay.fr/competitions/17161
common (MAE, RMSE, AbsRel) and are computed using
3Figure2. SYNS-PatchesDataset. Weshowsamplesfromdiversescenes,includingcomplexurban,natural,andindoorspaces. High-
qualityground-truthdepthcoversabout78.20%oftheimage,fromwhichdepthboundariesarecomputedasCannyedgesinlogspace.
pixel-wisecomparisonsbetweenthepredictedandground- on1.5Mlabeledimagesand62M+unlabeledimages.
truth depth map. Pointcloud-based metrics [65] (F-Score, Supervision. The model is fine-tuned in a supervised man-
IoU, Chamfer distance) instead bring the evaluation in the ner,withproxylabelsderivedfromstereoimages.Thefinal
3D domain, evaluating the reconstructed pointclouds as a lossfunctionintegratestheSILogloss,SSILloss,Gradient
whole. Among these, we select reconstruction F-Score as loss,andRandomProposalNormalization(RPNL)loss.
the leaderboard ranking metric. Finally, edge-based met- Training. The network was fine-tuned on the CityScapes
rics are computed only at depth boundary pixels. This dataset[20],resizingtheinputto384×768resolution,while
includes image-/pointcloud-based metrics and edge accu- keeping proxy labels at 1024 × ×2048 resolution. Ran-
racy/completionmetricsfromIBims-1[49]. dom flipping is used to augment data, the batch size is set
to 16 and the learning rate to 0.000161. The fine-tuning
4.ChallengeSubmissions is carried out to predict metric depth and early stops at 4
epochs,astrategicchoicetopreventoverfittingandensure
Wenowhighlightthetechnicaldetailsforeachsubmission,
themodel’srobustnesstonewdata.
asprovidedbytheauthorsthemselves. Eachsubmissionis
labeled based on the supervision used, including ground- Team3: RGA-Robot–†S
truth (D), proxy ground-truth (D*), DepthAnything [111]
D.Kim figure317@rgarobot.com
pretraining(†)andmonocular(M)orstereo(S)photometric
J.Kim jsk24@rgarobot.com
supportframes.Teamsarenumberedaccordingtorankings.
M.Kim wiseman218@rgarobot.com
Network. It uses the Depth Anything [111] pre-trained
Baseline–S
model to estimate relative depth, accompanied by an aux-
J.Spencer j.spencermartin@surrey.ac.uk iliarynetworktoconvertitintometricdepth. Thislatteris
C.Russell chris.russell@oii.ox.ac.uk NAFNet[13],processingthefinalfeaturemapsandrelative
S.Hadfield s.hadfield@surrey.ac.uk depthmappredictedbytheformermodeltogetherwiththe
R.Bowden r.bowden@surrey.ac.uk inputimage.
Challengeorganizers’submissionfromthefirstedition. Supervision. Self-supervisedlosswithtwomainterms: im-
Network. ConvNeXt-B encoder [56] with a base Mon- age reconstruction loss and smoothness loss. The former
odepthdecoder[34,62]from[92]. integrates perceptual loss with photometric loss as used
Supervision. Self-supervised with a stereo photometric in monodepth2 [35], with the former using a pre-trained
loss[30]andedge-awaredisparitysmoothness[34]. VGG19 backbone [87], following a similar approach as in
Training. Trained for 30 epochs on Kitti Eigen-Zhou with ESRGAN[106].
animageresolutionof192×640. Training. ThetrainiscarriedoutonKittiEigen-Zhouwith
batch size 8 and learning rate 1e−4 for 4 epochs. Only
Team1: PICO-MR–†D*
NAFNet is trained, while the Depth Anything model re-
G.Zhou zhouguangyuan@bytedance.com mainsfrozen.
Z.Li lizhengxin17@mails.ucas.ac.cn
Team4: EVP++–†D
Q.Rao raoqiang@bytedance.com
Y.Bao baoyiping@bytedance.com M.Lavreniuk nick 93@ukr.net
X.Liu liuxiao@foxmail.com Network. The architecture is based on Depth Anything
Network. BasedonDepth-Anything[111]withaBEiT384- [111], incorporating a VIT-L encoder [23] for feature ex-
Lbackbone, startingfromtheauthors’weightspre-trained traction and the ZoeDepth metric bins module [8] as a de-
4coder. This module computes per-pixel depth bin centers, Supervision. Supervised training using the ground truth
whicharelinearlycombinedtoproducemetricdepth. depth with SILog loss as the loss function with variance
Supervision. Themodelsweretrainedinasupervisedman- focus(λ)0.85. Ground-truthdepthistransformedas 1 .
(1+x)
ner using ground-truth depth information obtained from Training. Trained on NYUv2 [64], KITTI [33], virtual
variousdatasets,employingtheSILoglossfunction. KITTI v2 [10] for 25 epochs, with one-cycle learning rate
Training. Themodelsweretrainedonbothindoorandout- (min: 3e−5, max: 5e−4) and batch size 32 on 8× A100
doordata,respectivelyontheNYUv2dataset[64]withan GPUs.
imagesizeof392×518,andonKITTI[33],VirtualKITTI
2 [10], and DIODE outdoor [102] with an image size of
Team9: HIT-AIIA–†D
518×1078.Thebatchsizewassetto16,thelearningrateto
P.Sun 23s136164@stu.hit.edu.cn
0.000161,andthemaximumdepthto10forindoorscenes. K.Jiang jiangkui@hit.edu.cn
Foroutdoorscenes,thebatchsizewassetto1,thelearning G.Wu gwu@hit.edu.cn
rateto0.00002,andthemaximumdepthto80.Bothmodels J.Liu hitcslj@hit.edu.cn
weretrainedfor5epochs. X.Liu csxm@hit.edu.cn
J.Jiang jiangjunjun@hit.edu.cn
Team6: 3DCreators–†D
Network. It involves the pre-trained Depth Anything en-
R.Li lirui.david@gmail.com coderandpre-trainedCLlPmodel. Thelatterisintroduced
Q.Mao maoqing@mail.nwpu.edu.cn to calculate the similarity between the keywords ‘indoor’
J.Wu 18392713997@mail.nwpu.edu.cn or‘outdoor’andfeaturesextractedfromtheinputimageto
Y.Zhu yuzhu@nwpu.edu.cn route it to two, different instances of Depth Anything spe-
J.Sun sunjinqiu@nwpu.edu.cn cializedonindoororoutdoorscenarios.
Y.Zhang ynzhang@nwpu.edu.cn Supervision. Two instances of Depth Anything are fine-
Network. An architecture made of two sub-networks. The tunedonground-truthlabels,respectivelyfromNYUv2and
firstmodelconsistsofapre-trainedViT-largebackbone[23] KITTIforindoorandoutdoorenvironments.
from Depth Anything [111] and a ZoeDepth decoder [8]. Training. The training resolution is 392×518 on NYUv2
ThesecondisMetric3D[115],whichusesConvNext-Large and 384 × 768 on KITTI. The batch size is 16 and both
[57]backboneandaLeResdecoder[117]. instancesaretrainedfor5epochs.
Supervision. Thefirstnetworkisfine-tunedwiththeKITTI
Team10: FRDC-SH–†D
datasetusingSILogloss. Thesecondnetworkusesthere-
leasedpre-trainedweightstrainedbyadiversecollectionof X.Zhang zhangxidan@fujitsu.com
datasetsasdetailedin[115]. J.Wei weijianing@fujitsu.com
Training. Thefirstnetworkisfine-tunedusingbatchsize16 F.Wang wangfangjun@fujitsu.com
for 5 epochs. At inference, test-time augmentation – i.e., Z.Tan zhmtan@fujistu.com
colorjitteringandhorizontalflipping–isusedtocombine Network. The depth network is the Depth Anything
thepredictionsbythetwomodels: thesameimageisaug- [111] pre-trained model – based on ZoeDepth [8] with a
mented10timesandprocessedbythetwomodels,thenthe DPT BEiT L384–andfurtherfine-tuned.
predictionsareaveraged. Supervision.Trainedonground-truthdepth,withSILogand
HyperbolicChamferDistancelosses.
Team7: visioniitd–D Training.Themodelisfine-tunedonNYU-v2[64],7Scenes
S.Patni suraj.patni@cse.iitd.ac.in [86],SUNRGBD[128],DIODE[102],KITTI[33],DDAD
A.Agarwal aradhye.agarwal.cs520@cse.iitd.ac.in [39],andArgoverse[12]–withoutanyresizingoftheimage
C.Arora chetan@cse.iitd.ac.in resolution–for20epochswithbatchsize32,alearningrate
Network. ThemodelisECoDepth[66],whichprovidesef- setto1.61e-04,anda0.01weightdecay.
fectiveconditioningfortheMDEtasktodiffusionmethods
Team15: hyc123–D
likestablediffusion. ItisbasedonaComprehensiveImage
Detail Embedding (CIDE) module which utilizes ViT em- J.Wang 601533944@qq.com
beddings of the image and subsequently transforms them Network. Swin encoder [55] with skip connections and a
to yield a semantic context vector. These embeddings are decoderwithchannel-wiseself-attentionmodules.
used to condition the pre-trained UNet backbone in Stable Supervision. Trainedwithgroundtruthdepths,usingaloss
Diffusion, which produces hierarchical feature maps from consistingofacombinationoftwoL1lossesandanSSIM
itsdecoder. Theseareresizedtoacommondimensionand loss,weightedaccordingly.
passed to the Upsampling decoder and depth regressor to Training. ThemodelwastrainedonKittiEigen-Zhousplit
producethefinaldepth. usingimagesofsize370×1224for100epochs.
5Team16: ReadingLS–†MD* metric. Accordingly,toensureafaircomparisonamongthe
methods, the submitted predictions are aligned to ground-
A.Luginov a.luginov@pgr.reading.ac.uk
truthdepthsaccordingtomediandepthscaling.
M.Shahzad m.shahzad2@reading.ac.uk
Network. ThedepthnetworkisSwiftDepth[58],acompact 5.1.QuantitativeResults
modelwithonly6.4Mparameters.
Supervision. Self-supervised monocular training with the Table1highlightstheresultsofthisthirdeditionofthechal-
minimum reconstruction loss [35], enhanced by offline lenge,withthetop-performingtechniques,orderedusingF-
knowledgedistillationfromalargeMDEmodel[111]. Score performance, achieving notable improvements over
Training. The model is trained in parallel on Kitti thebaselinemethod. Afirst,noteworthyobservationisthe
Eigen-Zhou and a selection of outdoor YouTube videos, widespread adoption of the Depth Anything model [111],
similarlytoKBR[93].Bothtrainingandpredictionareper- pre-trainedon62Mofimages,asthebackbonearchitecture
formedwiththeinputresolutionof192×640. Theteacher by the leading teams, including PICO-MR, RGA-Robot,
model[111]isnottrainedoneitherthesedatasetsorSYNS- EVP++, 3DCreators, HIT-AIIA, FRDC-SH, and Read-
Patches. ingLS,demonstratingitseffectivenessandversatility.
Specifically, Team PICO-MR, which secured the top
Team19: ElderLab–D position on the leaderboard, achieved an F-score of 23.72,
outperformingthebaselinemethodbyaremarkable72.9%.
S.Hosseini smhh@yorku.ca
This represents a significant improvement over the previ-
A.Trajcevski atrajcev@yorku.ca
ous state-of-the-art method, DJI&ZJU, which achieved an
J.H.Elder jelder@yorku.ca
F-scoreof17.51inthe“TheSecondMonocularDepthEs-
Network. An off-the-shelf semantic segmentation model
timationChallenge”[91]. Inparticular,Team PICO-MR’s
[105]isusedatfirsttosegmenttheimage. Then,thedepth
resultshowsa35.5%increaseinperformancecomparedto
of pixels on the ground plane is estimated by predicting
DJI&ZJU,highlightingtherapidprogressmadeinmonoc-
the camera angle from the height of the highest pixel on
ulardepthestimationwithinarelativelyshortperiod. This
theground. Then,depthispropagatedverticallyforpixels
improvementcanbealsoclearlyobservedintheothermet-
above the ground, while the Manhattan frame is estimated
ricsconsidered, bothaccuracyanderror–notably, achiev-
with [76] to identify both Manhattan and non-Manhattan
ing the second absolute results on F-Edges, MAE, and
segments in the image and propagate depth along them in
RMSE.Theirsuccesscanbeattributedtothefine-tuningof
3D space. Finally, the depth map is completed according
theDepthAnythingmodelontheCityscapesdatasetusing
to heat equations [27], with pixels for which depth has
acombinationofSILog,SSIL,Gradient,andRandomPro-
beenalreadyestimatedimposingforcingconditions, while
posalNormalizationlosses,aswellastheirstrategicchoice
semanticboundariesandtheimageframeimposereflection
of fine-tuning for a few epochs to prevent overfitting and
boundaryconditions.
ensurerobustnesstounseendata.
Supervision. Ground-truth depth is used for training three
Team RGA-Robot, in the third place, achieved an F-
kernelregressionmodels.
scoreof22.79,outperformingthebaselineby66.1%. Their
Training. Three simple statistical models are trained on
novel approach of augmenting the Depth Anything model,
CityScapes [20] and NYUv2 [64]: 1) A kernel regression
maintained frozen, with an auxiliary network, NAFNet, to
modeltoestimategroundelevationanglefromthevertical
convert relative depth predictions into metric depth, com-
image coordinate of the highest observed ground pixel.
bined with self-supervised loss terms, shows the effective-
The ground truth elevation angle is computed by fitting a
nessofthisapproachinenhancingdepthaccuracy. Interms
plane (constrained to have zero roll) to the ground truth
oftheF-Edgesmetric,thismethodachievesthebestresult.
ground plane coordinates; 2) A kernel regression model
to estimate the depth of ground pixels from their vertical Team EVP++, ranking fourth, achieved an F-score of
coordinate,conditionedonsemanticclass;3)mediandepth 20.87, surpassing the baseline by 52.1%. Their approach
of non-ground pixels in columns directly abutting the involvedtrainingtheDepthAnythingmodelonbothindoor
bottomoftheimageframe,conditionedonsemanticclass. andoutdoordatasets,adaptingimagesizes,batchsizes,and
learningratestoeachscenario,andhighlightingtheimpor-
tance of tailoring model parameters to the specific charac-
teristics of the target environment. This strategy notably
5.Results
improvestheresultsintermsofstandard2Derrormetrics,
Submitted methods were evaluated on the testing split of yieldingthelowestMAE,RMSE,andAbsRel.
SYNS-Patches[1,92]. Participantswereallowedtosubmit Several other teams also surpassed both the baseline
methods without any restriction on the supervision or the method and the previous state-of-the-art from the second
predictions by the model, which can be either relative or edition of the challenge. Team 3DCreators achieved an
6Table1.SYNS-PatchesResults.Weprovidemetricsacrossthewholetestsplitofthedataset.Top-performingentriesgenerallyleverage
thepre-trainedDepthAnything[111]model.Onlyafewmethodsuseself-supervisedlossesorproxydepthlabels.
Train Rank F↑ F-Edges↑ MAE↓ RMSE↓ AbsRel↓ Acc-Edges↓ Comp-Edges↓
PICO-MR †D* 1 23.72 11.01 3.78 6.61 21.24 3.90 4.45
Anonymous ? 2 23.25 10.78 3.87 6.70 21.70 3.59 9.86
RGA-Robot †S 3 22.79 11.52 5.21 9.23 28.86 4.15 0.90
EVP++ †D 4 20.87 10.92 3.71 6.53 19.02 2.88 6.77
Anonymous ? 5 20.77 9.96 4.33 7.83 27.80 3.45 13.25
3DCreators †D 6 20.42 10.19 4.41 7.89 23.94 3.61 5.80
visioniitd D 7 19.07 9.92 4.53 7.96 23.27 3.26 8.00
Anonymous ? 8 18.60 9.43 3.92 7.16 20.12 2.89 15.65
HIT-AIIA †D 9 17.83 9.14 4.11 7.73 21.23 2.95 17.81
FRDC-SH †D 10 17.81 9.75 5.04 8.92 24.01 3.16 14.16
Anonymous ? 11 17.57 9.13 4.28 8.36 23.35 3.18 20.66
Anonymous ? 12 16.91 9.07 4.14 7.35 22.05 3.24 18.52
Anonymous ? 13 16.71 9.25 5.48 11.05 34.20 2.57 18.04
Anonymous ? 14 16.45 8.89 5.29 10.53 33.67 2.60 18.73
hyc123 D 15 15.92 9.17 8.25 13.88 43.88 4.11 0.74
ReadingLS †MD* 16 14.81 8.14 5.01 8.94 29.39 3.28 30.28
Baseline S 17 13.72 7.76 5.56 9.72 32.04 3.97 21.63
Anonymous ? 18 13.71 7.55 5.49 9.44 30.74 3.61 18.36
Anonymous ? 19 11.90 8.08 6.33 10.89 30.46 2.99 33.63
ElderLab D 20 11.04 7.09 8.76 15.86 63.32 3.22 40.61
M=Monocular–S=Stereo–D*=ProxyDepth–D=Ground-truthDepth–†=Pre-trainedDepthAnythingmodel
F-score of 20.42, outperforming the baseline by 48.8% by top-performing teams, such as PICO-MR, RGA-Robot,
fine-tuningandcombiningpredictionsfromtheDepthAny- EVP++, and 3DCreators, is the adoption of the Depth
thing model and Metric3D. Team visioniitd follows sur- Anything model as a backbone architecture. While Depth
passingthebaselineusingECoDepth,whichconditionsSta- Anything represents the current state-of-the-art in monoc-
bleDiffusion’sUNetbackbonewithComprehensiveImage ular depth estimation, the qualitative results highlight that
DetailEmbeddings. Team HIT-AIIAand FRDC-SHalso there are still significant challenges in accurately estimat-
achievednotableimprovements,withF-scoreof17.83and ing depth, particularly for thin structures in complex out-
17.81, respectively, using specialized model instances and door scenes. This is evident in columns 2, 4, 5, and 6
fine-tuningondiversedatasets. of Figure 3, where objects like trees and branches are not
Finally, the remaining teams outperformed the base- well-recovered, despite the impressive quantitative perfor-
line either on the F-score or any of the other metrics, yet manceofthesemethodsasshowninTable1. Interestingly,
not surpassing the winner of the previous edition. Team Team visioniitd, which employs a novel approach called
hyc123, with an F-score of 15.92, outperformed the base- ECoDepth to condition Stable Diffusion’s UNet backbone
lineby16.0%usingaSwinencoderwithskipconnections with Comprehensive Image Detail Embeddings, demon-
and a decoder with channel-wise self-attention modules, stratesaremarkableabilitytoestimatedepthforthinstruc-
whileTeamReadingLSoutperformsthebaselinebydistill- tures. Yet, they are outperformed quantitatively by other
ing knowledge from Depth Anything to a lightweight net- methodologies,suggestingthatestimatingdepthinsmooth
work based on SwiftDepth, further improved using mini- regionsmaybemorechallengingthaninthinstructures.
malreconstructionlossduringtraining. Finally, Team El- Thequalitativeresultsalsorevealsomemethod-specific
der Lab employed an off-the-shelf semantic segmentation anomalies. For instance, hyc123 exhibits salt-and-pepper
model and estimated depth using techniques such as pre- noiseartifacts,while ElderLab’smethod,whichrankslast,
dicting camera angle, propagating depth along Manhattan generates overly smooth depth maps that lose important
and non-Manhattan segments, and completing the depth sceneobjects. Theseanomalieshighlighttheimportanceof
map using heat equations. They achieved an F-score of developingrobusttechniquesthatcanhandlediversescene
11.04, 19.5% lower than the baseline score of 13.72, yet characteristics. Grid-like artifacts are observed in the pre-
theyobtained3.22Acc-Edge,beatingthebaseline. dictions of top-performers PICO-MR and RGA-Robot,
particularly in regions where the network seems uncertain
5.2.QualitativeResults
about depthestimates. Thissuggests thatfurther improve-
Figure 3 provides qualitative results for the depth predic- ments in network architecture and training strategies may
tions of each submission. A notable trend among the benecessarytomitigatetheseartifacts.
7GT
PICO-MR
RGA-Robot
EVP++
3DCreators
visioniitd
HIT-AIIA
FRDC-SH
hyc123
ReadingLS
Baseline
ElderLab
Figure 3. SYNS-Patches Depth Visualization. Best viewed in color and zoomed in. Methods are ranked based on their F-Score in
Table1. Wecanappreciatehowthinstructuresstillrepresentoneofthehardestchallengestoanymethod,suchasbranchesandrailings,
for instance. Near depth discontinuities, most approaches tend to produce “halos”, interpolating between foreground and background
objectsandthusfailingtoperceivesharpboundaries.Nonetheless,mostmethodsexposehigherlevelofdetailcomparedtothebaseline.
Theindoorscenariointhelastcolumnshowsthestrong noveltechniques, improvedtrainingstrategies, anddiverse
performance of methods like PICO-MR, EVP++, HIT- datasetswillbecrucialforfurtheradvancingthisfield.
AIIA, and FRDC-SH in estimating scene structure. This
can be attributed to their use of large-scale pre-training,
6.Conclusions&FutureWork
fine-tuningondiversedatasets,andcarefullydesignedloss
functionsthatcapturebothglobalandlocaldepthcues. Thispaperhassummarizedtheresultsforthethirdedition
However,allmethodsstillexhibitover-smoothingissues of MDEC. Over the various editions of the challenge, we
atdepthdiscontinuities,manifestingashaloeffects. While haveseenadrasticimprovementinperformance,showcas-
they outperform the baseline in this regard, likely due to ing MDE – in particular real-world generalization – as an
theirsupervisedtrainingwithgroundtruthorproxylabels, excitingandactiveareaofresearch.
thereremainssignificantroomforimprovement. WiththeadventofthefirstfoundationalmodelsforMDE
A notable limitation across all methods is the inability duringthelastmonths,weobservedadiffuseduseofframe-
to effectively estimate depth for non-Lambertian surfaces, workssuchasDepthAnything[111]. Thisignitedamajor
such as glass or transparent objects. This is evident in the boost to the results submitted by the participants, with a
penultimaterightcolumnandthefirstcolumn,correspond- muchhigherimpactcomparedtothespecifickindofsuper-
ingtothewindshield.Theprimaryreasonforthislimitation visionchosenforthechallenge. Nonetheless,aswecanap-
is the lack of accurate supervision for such surfaces in the preciatefromthequalitativeresults,anymethodsstillstrug-
trainingdata,highlightingtheneedfornoveltechniquesand gletoaccuratelypredictfinestructuresanddiscontinuities,
datasetsthatexplicitlyaddressthischallenge. hintingthatthereisstillroomforimprovementdespitethe
massiveamountofdatausedtotrainDepthAnything.
In conclusion, the qualitative results provide valuable
insights into the current state of monocular depth esti- We hope MDE will continue to attract new researchers
mation methods. While the adoption of large-scale pre- and practitioners to this field and renew our invitation to
trainingandcarefullydesignedarchitectureshasledtosig- participateinfutureeditionsofthechallenge.
nificant improvements, challenges persist in accurately es- Acknowledgments. This work was partially funded
timatingdepthforthinstructures,smoothregions,andnon- by the EPSRC under grant agreements EP/S016317/1,
Lambertiansurfaces. Addressingtheselimitationsthrough EP/S016368/1,EP/S016260/1,EP/S035761/1.
8References Simplebaselinesforimagerestoration. InEuropeancon-
ferenceoncomputervision,pages17–33.Springer,2022.
[1] Wendy J Adams, James H Elder, Erich W Graf, Julian [14] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng.
Leyland,ArthurJLugtigheid,andAlexanderMuryy. The Single-image depth perception in the wild. Advances in
Southampton-YorkNaturalScenes(SYNS)dataset: Statis- neuralinformationprocessingsystems,29,2016.
tics of surface attitude. Scientific Reports, 6(1):35805, [15] WeifengChen,ShengyiQian,DavidFan,NoriyukiKojima,
2016. MaxHamilton,andJiaDeng.OASIS:Alarge-scaledataset
[2] AshutoshAgarwalandChetanArora. Attentionattention forsingleimage3dinthewild. InCVPR,2020.
everywhere: Monocular depth prediction with skip atten- [16] Zeyu Cheng, Yi Zhang, and Chengkai Tang. Swin-
tion. InProceedingsoftheIEEE/CVFWinterConference depth: Using transformers and multi-scale fusion for
on Applications of Computer Vision, pages 5861–5870, monocular-baseddepthestimation. IEEESensorsJournal,
2023. 21(23):26912–26920,2021.
[3] FilippoAleotti,FabioTosi,MatteoPoggi,andStefanoMat- [17] Jaehoon Cho, Dongbo Min, Youngjung Kim, and
toccia. Generative adversarial networks for unsupervised Kwanghoon Sohn. Diml/cvl rgb-d dataset: 2m rgb-d im-
monocular depth prediction. In Proceedings of the Euro- agesofnaturalindoorandoutdoorscenes. arXivpreprint
peanConferenceonComputerVision(ECCV)Workshops, arXiv:2110.11590,2021.
pages0–0,2018. [18] Hyesong Choi, Hunsang Lee, Sunkyung Kim, Sunok
[4] FilippoAleotti,GiulioZaccaroni,LucaBartolomei,Matteo Kim, Seungryong Kim, Kwanghoon Sohn, and Dongbo
Poggi,FabioTosi,andStefanoMattoccia.Real-timesingle Min. Adaptive confidence thresholding for monocular
imagedepthperceptioninthewildwithhandhelddevices. depthestimation. InProceedingsoftheIEEE/CVFInter-
Sensors,21(1):15,2020. national Conference on Computer Vision, pages 12808–
[5] Lorenzo Andraghetti, Panteleimon Myriokefalitakis, 12818,2021.
PierLuigiDovesi,BelenLuque,MatteoPoggi,Alessandro [19] Antonio Cipolletta, Valentino Peluso, Andrea Calimera,
Pieropan, and Stefano Mattoccia. Enhancing self- MatteoPoggi,FabioTosi,FilippoAleotti,andStefanoMat-
supervised monocular depth estimation with traditional toccia. Energy-quality scalable monocular depth estima-
visualodometry. In2019InternationalConferenceon3D tiononlow-powercpus. IEEEInternetofThingsJournal,
Vision(3DV),pages424–433.IEEE,2019. 9(1):25–36,2021.
[6] ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka. [20] MariusCordts,MohamedOmran,SebastianRamos,Timo
Adabins:Depthestimationusingadaptivebins.InProceed- Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
ingsoftheIEEE/CVFConferenceonComputerVisionand Franke, Stefan Roth, and Bernt Schiele. The cityscapes
PatternRecognition,pages4009–4018,2021. dataset for semantic urban scene understanding. In Proc.
[7] ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka. of the IEEE Conference on Computer Vision and Pattern
Localbins: Improving depth estimation by learning local Recognition(CVPR),2016.
distributions. In Computer Vision–ECCV 2022: 17th Eu- [21] Alex Costanzino, Pierluigi Zama Ramirez, Matteo Poggi,
ropeanConference,TelAviv,Israel,October23–27,2022, Fabio Tosi, Stefano Mattoccia, and Luigi Di Stefano.
Proceedings,PartI,pages480–496.Springer,2022. Learning depth estimation for transparent and mirror sur-
[8] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter faces. InTheIEEEInternationalConferenceonComputer
Wonka, and Matthias Mu¨ller. Zoedepth: Zero-shot trans- Vision,2023. ICCV.
ferbycombiningrelativeandmetricdepth. arXivpreprint [22] Qi Dai, Vaishakh Patil, Simon Hecker, Dengxin Dai, Luc
arXiv:2302.12288,2023. Van Gool, and Konrad Schindler. Self-supervised object
[9] JiawangBian,ZhichaoLi,NaiyanWang,HuangyingZhan, motionanddepthestimationfromvideo.InProceedingsof
Chunhua Shen, Ming-Ming Cheng, and Ian Reid. Unsu- theIEEE/CVFConferenceonComputerVisionandPattern
pervisedScale-consistentDepthandEgo-motionLearning Recognition(CVPR)Workshops,June2020.
fromMonocularVideo.InAdvancesinNeuralInformation [23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
ProcessingSystems,volume32,2019. Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
[10] Yohann Cabon, Naila Murray, and Martin Humenberger. Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Virtualkitti2,2020. SylvainGelly,JakobUszkoreit,andNeilHoulsby. Anim-
[11] VincentCasser,SoerenPirk,RezaMahjourian,andAnelia
ageisworth16x16words: Transformersforimagerecog-
Angelova. Depth prediction without the sensors: Lever-
nition at scale. In International Conference on Learning
agingstructureforunsupervisedlearningfrommonocular
Representations,2021.
videos. InProceedingsoftheAAAIconferenceonartificial [24] YiqunDuan,XiandaGuo,andZhengZhu.DiffusionDepth:
intelligence,volume33,pages8001–8008,2019.
Diffusiondenoisingapproachformonoculardepthestima-
[12] Ming-Fang Chang, John W Lambert, Patsorn Sangkloy,
tion. arXivpreprintarXiv:2303.05021,2023.
JagjeetSingh,SlawomirBak,AndrewHartnett,DeWang, [25] AinazEftekhar, AlexanderSax, JitendraMalik, andAmir
PeterCarr,SimonLucey,DevaRamanan,andJamesHays. Zamir. Omnidata: A scalable pipeline for making multi-
Argoverse: 3dtrackingandforecastingwithrichmaps. In taskmid-levelvisiondatasetsfrom3dscans.pages10786–
Conference on Computer Vision and Pattern Recognition
10796,2021.
(CVPR),2019. [26] David Eigen and Rob Fergus. Predicting Depth, Surface
[13] LiangyuChen,XiaojieChu,XiangyuZhang,andJianSun. NormalsandSemanticLabelswithaCommonMulti-scale
9ConvolutionalArchitecture.InInternationalConferenceon [40] VitorGuizilini,IgorVasiljevic,DianChen,RaresAmbrus,
, ,
ComputerVision,pages2650–2658,2015. andAdrienGaidon.Towardszero-shotscale-awaremonoc-
[27] JamesHElder. Areedgesincomplete? InternationalJour- ulardepthestimation. 2023.
nalofComputerVision,34(2):97–122,1999. [41] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif-
[28] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat- fusionprobabilisticmodels,2020.
manghelich, and Dacheng Tao. Deep ordinal regression [42] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou,
networkformonoculardepthestimation.InProceedingsof QichuanGeng,andRuigangYang. Theapolloscapeopen
theIEEEconferenceoncomputervisionandpatternrecog- dataset for autonomous driving and its application. IEEE
nition,pages2002–2011,2018. transactionsonpatternanalysisandmachineintelligence,
[29] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, 42(10):2702–2719,2019.
Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. [43] Lam Huynh, Phong Nguyen, Jiˇr´ı Matas, Esa Rahtu, and
Geowizard: Unleashing the diffusion priors for 3d ge- JanneHeikkila¨. Lightweightmonoculardepthwithanovel
ometry estimation from a single image. arXiv preprint neural architecture search method. In Proceedings of the
arXiv:2403.12013,2024. IEEE/CVFwinterconferenceonapplicationsofcomputer
[30] RaviGarg,VijayKumar,GustavoCarneiro,andIanReid. vision,pages3643–3653,2022.
UnsupervisedCNNforSingleViewDepthEstimation:Ge- [44] MaxJaderberg,KarenSimonyan,AndrewZisserman,and
ometry to the Rescue. In European Conference on Com- Koray Kavukcuoglu. Spatial Transformer Networks. In
puterVision,pages740–756,2016. Advances in Neural Information Processing Systems, vol-
[31] Stefano Gasperini, Patrick Koch, Vinzenz Dallabetta, ume28,2015.
Nassir Navab, Benjamin Busam, and Federico Tombari. [45] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui
R4dyn: Exploring radar for self-supervised monocular Liu,ZhaoqiangLiu,TongLu,ZhenguoLi,andPingLuo.
depthestimationofdynamicscenes. In2021International DDP:Diffusionmodelfordensevisualprediction. 2023.
Conference on 3D Vision (3DV), pages 751–760. IEEE, [46] Adrian Johnston and Gustavo Carneiro. Self-Supervised
2021. MonocularTrainedDepthEstimationUsingSelf-Attention
[32] Stefano Gasperini, Nils Morbitzer, HyunJun Jung, Nassir andDiscreteDisparityVolume.InConferenceonComputer
Navab,andFedericoTombari. Robustmonoculardepthes- VisionandPatternRecognition,pages4755–4764,2020.
timation under challenging conditions. In Proceedings of [47] BingxinKe,AntonObukhov,ShengyuHuang,NandoMet-
the IEEE/CVF International Conference on Computer Vi- zger, Rodrigo Caye Daudt, and Konrad Schindler. Re-
sion,2023. purposingdiffusion-basedimagegeneratorsformonocular
[33] AGeiger,PLenz,CStiller,andRUrtasun. Visionmeets depthestimation. InConferenceonComputerVisionand
robotics: The KITTI dataset. International Journal of PatternRecognition,2024. CVPR.
RoboticsResearch,32(11):1231–1237,2013. [48] MariaKlodtandAndreaVedaldi.SupervisingtheNewwith
[34] Clement Godard, Oisin Mac Aodha, and Gabriel J. Bros- theOld: LearningSFMfromSFM. InEuropeanConfer-
tow.UnsupervisedMonocularDepthEstimationwithLeft- enceonComputerVision,pages713–728,2018.
Right Consistency. Conference on Computer Vision and [49] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and
PatternRecognition,pages6602–6611,2017. Marco Ko¨rner. Evaluation of CNN-Based Single-Image
[35] ClementGodard,OisinMacAodha,MichaelFirman,and Depth Estimation Methods. In European Conference on
GabrielBrostow. DiggingIntoSelf-SupervisedMonocular ComputerVisionWorkshops,pages331–348,2018.
DepthEstimation. InternationalConferenceonComputer [50] IroLaina,ChristianRupprecht,VasileiosBelagiannis,Fed-
Vision,2019-Octob:3827–3837,2019. ericoTombari,andNassirNavab. Deeperdepthprediction
[36] JuanLuisGonzalezBelloandMunchurlKim.PLADE-Net: with fully convolutional residual networks. International
TowardsPixel-LevelAccuracyforSelf-SupervisedSingle- Conferenceon3DVision,pages239–248,2016.
View Depth Estimation with Neural Positional Encoding [51] RuiboLi, KeXian, ChunhuaShen, ZhiguoCao, HaoLu,
and Distilled Matting Loss. In Conference on Computer and Lingxiao Hang. Deep attention-based classification
VisionandPatternRecognition,pages6847–6856,2021. networkforrobustdepthprediction. InComputerVision–
[37] ArielGordon,HanhanLi,RicoJonschkowski,andAnelia ACCV2018: 14thAsianConferenceonComputerVision,
Angelova. Depth from videos in the wild: Unsupervised Perth,Australia,December2–6,2018,RevisedSelectedPa-
monoculardepthlearningfromunknowncameras. InPro- pers,PartIV14,pages663–678.Springer,2019.
ceedings of the IEEE/CVF International Conference on [52] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker,
ComputerVision,pages8977–8986,2019. Noah Snavely, Ce Liu, and William T Freeman. Man-
[38] Vitor Guizilini, Ambrus Ambrus, Sudeep Pillai, Allan nequinchallenge: Learning the depths of moving people
Raventos, and Adrien Gaidon. 3D packing for self- by watching frozen people. IEEE Transactions on Pat-
supervised monocular depth estimation. Conference on ternAnalysisandMachineIntelligence,43(12):4229–4241,
Computer Vision and Pattern Recognition, pages 2482– 2020.
2491,2020. [53] Zhengqi Li and Noah Snavely. Megadepth: Learning
[39] VitorGuizilini,RaresAmbrus,SudeepPillai,AllanRaven- single-viewdepthpredictionfrominternetphotos. InPro-
tos, and Adrien Gaidon. 3d packing for self-supervised ceedings of the IEEE conference on computer vision and
monoculardepthestimation. InIEEEConferenceonCom- patternrecognition,pages2041–2050,2018.
puterVisionandPatternRecognition(CVPR),2020. [54] FayaoLiu,ChunhuaShen,andGuoshengLin. Deepcon-
10volutionalneuralfieldsfordepthestimationfromasingle [67] Adrien Pavao, Isabelle Guyon, Anne-Catherine Letour-
image.InProceedingsoftheIEEEconferenceoncomputer nel, Xavier Baro´, Hugo Escalante, Sergio Escalera, Tyler
visionandpatternrecognition,pages5162–5170,2015. Thomas, and Zhen Xu. Codalab competitions: An open
[55] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng sourceplatformtoorganizescientificchallenges.Technical
Zhang,StephenLin,andBainingGuo. Swintransformer: report,2022.
Hierarchicalvisiontransformerusingshiftedwindows. In [68] Valentino Peluso, Antonio Cipolletta, Andrea Calimera,
ProceedingsoftheIEEE/CVFinternationalconferenceon MatteoPoggi,FabioTosi,FilippoAleotti,andStefanoMat-
computervision,pages10012–10022,2021. toccia.Monoculardepthperceptiononmicrocontrollersfor
[56] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht- edgeapplications. IEEETransactionsonCircuitsandSys-
enhofer, Trevor Darrell, and Saining Xie. A convnet for temsforVideoTechnology,32(3):1524–1536,2021.
the2020s. InProceedingsoftheIEEE/CVFConferenceon [69] Valentino Peluso, Antonio Cipolletta, Andrea Calimera,
Computer Vision and Pattern Recognition, pages 11976– MatteoPoggi,FabioTosi,andStefanoMattoccia.Enabling
11986,2022. energy-efficient unsupervised monocular depth estimation
[57] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht- on armv7-based platforms. In 2019 Design, Automation
enhofer,TrevorDarrell,andSainingXie.Aconvnetforthe &TestinEuropeConference&Exhibition(DATE),pages
2020s. 2022. 1703–1708.IEEE,2019.
[58] AlbertLuginovandIlyaMakarov.Swiftdepth:Anefficient [70] RuiPeng,RonggangWang,YawenLai,LuyangTang,and
hybridcnn-transformermodelforself-supervisedmonocu- Yangang Cai. Excavating the potential capacity of self-
lardepthestimationonmobiledevices. In2023IEEEIn- supervisedmonoculardepthestimation. InProceedingsof
ternational Symposium on Mixed and Augmented Reality the IEEE/CVF International Conference on Computer Vi-
Adjunct(ISMAR-Adjunct),pages642–647,2023. sion,pages15560–15569,2021.
[59] ChenxuLuo,ZhenhengYang,PengWang,YangWang,Wei [71] Sudeep Pillai, Rares¸ Ambrus¸, and Adrien Gaidon. Su-
Xu,RamNevatia,andAlanYuille. Everypixelcounts++: perDepth: Self-Supervised, Super-Resolved Monocular
Jointlearningofgeometryandmotionwith3dholisticun- DepthEstimation.InInternationalConferenceonRobotics
derstanding. IEEE Transactions on Pattern Analysis and andAutomation,pages9250–9256,2019.
MachineIntelligence,42(10):2624–2641,2020. [72] MatteoPoggi,FilippoAleotti,FabioTosi,andStefanoMat-
[60] Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong, toccia. Towards real-time unsupervised monocular depth
Lina Liu, Yong Liu, Xinxin Chen, and Yi Yuan. HR- estimationoncpu. In2018IEEE/RSJinternationalconfer-
Depth:HighResolutionSelf-SupervisedMonocularDepth enceonintelligentrobotsandsystems(IROS),pages5848–
Estimation. AAAI Conference on Artificial Intelligence, 5854.IEEE,2018.
35(3):2294–2301,2021. [73] MatteoPoggi,FilippoAleotti,FabioTosi,andStefanoMat-
[61] Reza Mahjourian, Martin Wicke, and Anelia Angelova. toccia. OntheUncertaintyofSelf-SupervisedMonocular
Unsupervised Learning of Depth and Ego-Motion from DepthEstimation. InConferenceonComputerVisionand
MonocularVideoUsing3DGeometricConstraints. Con- PatternRecognition,pages3224–3234,2020.
ferenceonComputerVisionandPatternRecognition,pages [74] MatteoPoggi, FabioTosi, KonstantinosBatsos, Philippos
5667–5675,2018. Mordohai, and Stefano Mattoccia. On the synergies be-
[62] NikolausMayer,EddyIlg,PhilipHausser,PhilippFischer, tweenmachinelearningandbinocularstereofordepthesti-
DanielCremers,AlexeyDosovitskiy,andThomasBrox. A mationfromimages: asurvey. IEEETransactionsonPat-
LargeDatasettoTrainConvolutionalNetworksforDispar- ternAnalysisandMachineIntelligence,44(9):5314–5334,
ity,OpticalFlow,andSceneFlowEstimation. Conference 2021.
onComputerVisionandPatternRecognition,pages4040– [75] MatteoPoggi,FabioTosi,andStefanoMattoccia.Learning
4048,2016. MonocularDepthEstimationwithUnsupervisedTrinocular
[63] SMahdiHMiangoleh,SebastianDille,LongMai,Sylvain Assumptions. In International Conference on 3D Vision,
Paris,andYagizAksoy. Boostingmonoculardepthestima- pages324–333,2018.
tionmodelstohigh-resolutionviacontent-adaptivemulti- [76] YimingQian, Srikumar Ramalingam, and JamesHElder.
resolutionmerging. InProceedingsoftheIEEE/CVFCon- Ls3d: Single-view gestalt 3d surface reconstruction from
ferenceonComputerVisionandPatternRecognition,pages manhattanlinesegments.InComputerVision–ACCV2018:
9685–9694,2021. 14th Asian Conference on Computer Vision, Perth, Aus-
[64] PushmeetKohliNathanSilberman,DerekHoiemandRob tralia,December2–6,2018,RevisedSelectedPapers,Part
Fergus. Indoor segmentation and support inference from IV14,pages399–416.Springer,2019.
rgbdimages. InECCV,2012. [77] PierluigiZamaRamirez,AlexCostanzino,FabioTosi,Mat-
[65] Evin Pinar O¨rnek, Shristi Mudgal, Johanna Wald, Yida teo Poggi, Samuele Salti, Stefano Mattoccia, and Luigi
Wang, NassirNavab, andFedericoTombari. From2Dto DiStefano. Booster: abenchmarkfordepthfromimages
3D: Re-thinking Benchmarking of Monocular Depth Pre- ofspecularandtransparentsurfaces.IEEETransactionson
diction. arXivpreprint,2022. PatternAnalysisandMachineIntelligence,2023.
[66] Suraj Patni, Aradhye Agarwal, and Chetan Arora. Swin [78] Pierluigi Zama Ramirez, Fabio Tosi, Matteo Poggi,
transformer: Hierarchicalvisiontransformerusingshifted Samuele Salti, Stefano Mattoccia, and Luigi Di Stefano.
windows. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn. Openchallengesindeepstereo:theboosterdataset.InPro-
(CVPR),2024. ceedingsoftheIEEE/CVFConferenceonComputerVision
11andPatternRecognition,pages21168–21178,2022. Thesecondmonoculardepthestimationchallenge. InPro-
[79] Rene´Ranftl,AlexeyBochkovskiy,andVladlenKoltun.Vi- ceedingsoftheIEEE/CVFConferenceonComputerVision
siontransformersfordenseprediction. InProceedingsof andPatternRecognition,pages3063–3075,2023.
the IEEE/CVF International Conference on Computer Vi- [92] JaimeSpencer,ChrisRussell,SimonHadfield,andRichard
sion(ICCV),pages12179–12188,October2021. Bowden. Deconstructingself-supervisedmonocularrecon-
[80] Rene´ Ranftl, Katrin Lasinger, David Hafner, Konrad struction:Thedesigndecisionsthatmatter.Transactionson
Schindler, and Vladlen Koltun. Towards robust monocu- MachineLearningResearch,2022. ReproducibilityCerti-
lar depth estimation: Mixing datasets for zero-shot cross- fication.
datasettransfer. IEEEtransactionsonpatternanalysisand [93] JaimeSpencer,ChrisRussell,SimonHadfield,andRichard
machineintelligence,2020. Bowden. Kick back & relax: Learning to reconstruct
[81] Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan the world by watching slowtv. In Proceedings of the
Kim,DeqingSun,JonasWulff,andMichaelJBlack.Com- IEEE/CVF International Conference on Computer Vision
petitivecollaboration:Jointunsupervisedlearningofdepth, (ICCV),pages15768–15779,October2023.
cameramotion, opticalflowandmotionsegmentation. In [94] JaimeSpencer,ChrisRussell,SimonHadfield,andRichard
ProceedingsoftheIEEE/CVFconferenceoncomputervi- Bowden. Kick back & relax++: Scaling beyond ground-
sionandpatternrecognition,pages12240–12249,2019. truth depth with slowtv & cribstv. arXiv preprint
[82] AnirbanRoyandSinisaTodorovic. Monoculardepthesti- arXiv:2403.01569,2024.
mationusingneuralregressionforest.InProceedingsofthe [95] J.Sturm,N.Engelhard,F.Endres,W.Burgard,andD.Cre-
IEEEconferenceoncomputervisionandpatternrecogni- mers. Abenchmarkfortheevaluationofrgb-dslamsys-
tion,pages5506–5514,2016. tems. InProc.oftheInternationalConferenceonIntelli-
[83] Rui, Stu¨cklerJo¨rg, CremersDanielYangNan, andWang. gentRobotSystems(IROS),Oct.2012.
Deep Virtual Stereo Odometry: Leveraging Deep Depth [96] LiorTalker,AviadCohen,ErezYosef,AlexandraDana,and
PredictionforMonocularDirectSparseOdometry. InEu- MichaelDinerstein. Mindtheedge: Refiningdepthedges
ropean Conference on Computer Vision, pages 835–852, insparsely-supervisedmonoculardepthestimation. 2024.
2018. CVPR.
[84] SaurabhSaxena,CharlesHerrmann,JunhwaHur,Abhishek [97] FabioTosi,FilippoAleotti,MatteoPoggi,andStefanoMat-
Kar,MohammadNorouzi,DeqingSun,andDavidJ.Fleet. toccia. Learningmonoculardepthestimationinfusingtra-
The surprising effectiveness of diffusion models for opti- ditionalstereoknowledge.ConferenceonComputerVision
cal flow and monocular depth estimation. arXiv preprint andPatternRecognition,2019-June:9791–9801,2019.
arXiv:2306.01923,2023. [98] FabioTosi,FilippoAleotti,PierluigiZamaRamirez,Mat-
[85] SaurabhSaxena,AbhishekKar,MohammadNorouzi,and teo Poggi, Samuele Salti, Luigi Di Stefano, and Stefano
DavidJFleet. Monoculardepthestimationusingdiffusion Mattoccia.Distilledsemanticsforcomprehensivesceneun-
models. arXivpreprintarXiv:2302.14816,2023. derstandingfromvideos. InProceedingsoftheIEEE/CVF
[86] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram conference on computer vision and pattern recognition,
Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene pages4654–4665,2020.
coordinate regression forests for camera relocalization in [99] FabioTosi,YiyiLiao,CarolinSchmitt,andAndreasGeiger.
rgb-d images. In Proceedings of the IEEE conference on Smd-nets: Stereo mixture density networks. In Proceed-
computervisionandpatternrecognition,pages2930–2937, ingsoftheIEEE/CVFconferenceoncomputervisionand
2013. patternrecognition,pages8942–8952,2021.
[87] KarenSimonyanandAndrewZisserman.Verydeepconvo- [100] Madhu Vankadari, Sourav Garg, Anima Majumder, Swa-
lutionalnetworksforlarge-scaleimagerecognition. arXiv gatKumar,andArdhenduBehera. Unsupervisedmonocu-
preprintarXiv:1409.1556,2014. lardepthestimationfornight-timeimagesusingadversar-
[88] Jiaming Song, Chenlin Meng, and Stefano Ermon. ialdomainfeatureadaptation. InComputerVision–ECCV
Denoising diffusion implicit models. arXiv preprint 2020: 16th European Conference, Glasgow, UK, August
arXiv:2010.02502,2020. 23–28,2020,Proceedings,PartXXVIII16,pages443–459.
[89] Jaime Spencer, Richard Bowden, and Simon Hadfield. Springer,2020.
DeFeat-Net: General monocular depth via simultaneous [101] MadhuVankadari,StuartGolodetz,SouravGarg,Sangyun
unsupervised representation learning. In Conference on Shin,AndrewMarkham,andNikiTrigoni. Whenthesun
Computer Vision and Pattern Recognition, pages 14390– goesdown: Repairingphotometriclossesforall-daydepth
14401,2020. estimation.InConferenceonRobotLearning,pages1992–
[90] Jaime Spencer, C Stella Qian, Chris Russell, Simon Had- 2003.PMLR,2023.
field, Erich Graf, Wendy Adams, Andrew J Schofield, [102] IgorVasiljevic,NickKolkin,ShanyiZhang,RuotianLuo,
James H Elder, Richard Bowden, Heng Cong, et al. The HaochenWang,FalconZ.Dai,AndreaF.Daniele,Moham-
monocular depth estimation challenge. In Proceedings of madrezaMostajabi,StevenBasart,MatthewR.Walter,and
theIEEE/CVFWinterConferenceonApplicationsofCom- GregoryShakhnarovich.DIODE:ADenseIndoorandOut-
puterVision,pages623–632,2023. doorDEpthDataset. CoRR,abs/1908.00463,2019.
[91] JaimeSpencer,CStellaQian,MichaelaTrescakova,Chris [103] ChaoyangWang,JoseMiguelBuenaposada,RuiZhu,and
Russell, SimonHadfield, ErichWGraf, WendyJAdams, SimonLucey.LearningDepthfromMonocularVideosUs-
Andrew J Schofield, James Elder, Richard Bowden, et al. ingDirectMethods. ConferenceonComputerVisionand
12PatternRecognition,pages2022–2030,2018. ingsoftheIEEE/CVFConferenceonComputerVisionand
[104] Chaoyang Wang, Simon Lucey, Federico Perazzi, and PatternRecognition,pages204–213,2021.
OliverWang. Webstereovideosupervisionfordepthpre- [117] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus,
dictionfromdynamicscenes. In2019InternationalCon- LongMai, SimonChen, andChunhuaShen. Learningto
ferenceon3DVision(3DV),pages348–357.IEEE,2019. recover3dsceneshapefromasingleimage. InProc.IEEE
[105] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Conf.Comp.Vis.Patt.Recogn.(CVPR),2021.
Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, [118] ZhichaoYinandJianpingShi.Geonet:Unsupervisedlearn-
HongshengLi,etal.Internimage:Exploringlarge-scalevi- ingofdensedepth,opticalflowandcamerapose. InPro-
sionfoundationmodelswithdeformableconvolutions. In ceedings of the IEEE conference on computer vision and
ProceedingsoftheIEEE/CVFConferenceonComputerVi- patternrecognition,pages1983–1992,2018.
sionandPatternRecognition,pages14408–14419,2023. [119] FisherYu,HaofengChen,XinWang,WenqiXian,Yingy-
[106] XintaoWang,KeYu,ShixiangWu,JinjinGu,YihaoLiu, ing Chen, Fangchen Liu, Vashisht Madhavan, and Trevor
ChaoDong,YuQiao,andChenChangeLoy. Esrgan: En- Darrell. Bdd100k: Adiversedrivingdatasetforheteroge-
hancedsuper-resolutiongenerativeadversarialnetworks.In neousmultitasklearning. InProceedingsoftheIEEE/CVF
ProceedingsoftheEuropeanConferenceonComputerVi- conference on computer vision and pattern recognition,
sion(ECCV)Workshops,September2018. pages2636–2645,2020.
[107] JamieWatson,MichaelFirman,GabrielBrostow,andDani- [120] WeihaoYuan,XiaodongGu,ZuozhuoDai,SiyuZhu,and
yar Turmukhambetov. Self-supervised monocular depth PingTan. Neuralwindowfully-connectedcrfsformonoc-
hints.InternationalConferenceonComputerVision,2019- ular depth estimation. In 2022 IEEE/CVF Conference on
Octob:2162–2171,2019. Computer Vision and Pattern Recognition (CVPR), pages
[108] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Kara- 3906–3915,2022.
man, andVivienneSze. Fastdepth: Fastmonoculardepth [121] Pierluigi Zama Ramirez, Tosi Fabio, Luigi Di Stefano,
estimation on embedded systems. In 2019 International Radu Timofte, Alex Costanzino, Matteo Poggi, Samuele
Conference on Robotics and Automation (ICRA), pages Salti,StefanoMattoccia,JunShi,DafengZhang,YongA,
6101–6108.IEEE,2019. YixiangJin,DingzheLi,ChaoLi,ZhiwenLiu,QiZhang,
[109] KeXian,ChunhuaShen,ZhiguoCao,HaoLu,YangXiao, YixingWang,andShiYin. NTIRE2023challengeonHR
RuiboLi,andZhenboLuo. Monocularrelativedepthper- depthfromimagesofspecularandtransparentsurfaces. In
ceptionwithwebstereodatasupervision. InProceedings ProceedingsoftheIEEE/CVFConferenceonComputerVi-
of the IEEE Conference on Computer Vision and Pattern sionandPatternRecognitionWorkshops,2023.
Recognition,pages311–320,2018. [122] Pierluigi Zama Ramirez, Matteo Poggi, Fabio Tosi, Ste-
[110] Jiaxing Yan, Hong Zhao, Penghui Bu, and YuSheng fano Mattoccia, and Luigi Di Stefano. Geometry meets
Jin. Channel-Wise Attention-Based Network for Self- semantics for semi-supervised monocular depth estima-
SupervisedMonocularDepthEstimation. InInternational tion. InComputerVision–ACCV2018:14thAsianConfer-
Conferenceon3DVision,pages464–473,2021. enceonComputerVision,Perth,Australia,December2–6,
[111] LiheYang,BingyiKang,ZilongHuang,XiaogangXu,Ji- 2018,RevisedSelectedPapers,PartIII14,pages298–313.
ashi Feng, and Hengshuang Zhao. Depth anything: Un- Springer,2019.
leashingthepoweroflarge-scaleunlabeleddata. InCVPR, [123] Pierluigi Zama Ramirez, Fabio Tosi, Luigi Di Stefano,
2024. Radu Timofte, Alex Costanzino, Matteo Poggi, et al.
[112] Nan Yang, Lukas von Stumberg, Rui Wang, and Daniel NTIRE 2024 challenge on HR depth from images of
Cremers. D3VO: Deep Depth, Deep Pose and Deep Un- specular and transparent surfaces. In Proceedings of the
certainty for Monocular Visual Odometry. In Conference IEEE/CVF Conference on Computer Vision and Pattern
onComputerVisionandPatternRecognition,pages1278– Recognition(CVPR)Workshops,2024.
1289,2020. [124] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasek-
[113] Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi era,KejieLi,HarshAgarwal,andIanM.Reid. Unsuper-
Tian,SongcenXu,ChangmingSun,andDouRenyin. Di- visedLearningofMonocularDepthEstimationandVisual
versedepth: Affine-invariantdepthpredictionusingdiverse OdometrywithDeepFeatureReconstruction. Conference
data. arXivpreprintarXiv:2002.00569,2020. on Computer Vision and Pattern Recognition, pages 340–
[114] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, 349,2018.
Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Met- [125] Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu,
ric3D:Towardszero-shotmetric3dpredictionfromasingle andChunhuaShen. Hierarchicalnormalizationforrobust
image. 2023. monoculardepthestimation. 35,2022.
[115] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, [126] Chaoqiang Zhao, Yang Tang, and Qiyu Sun. Unsuper-
Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Met- visedmonoculardepthestimationinhighlycomplexenvi-
ric3d: Towardszero-shotmetric3dpredictionfromasin- ronments. IEEETransactionsonEmergingTopicsinCom-
gleimage. InProceedingsoftheIEEE/CVFInternational putationalIntelligence,6(5):1237–1246,2022.
ConferenceonComputerVision,pages9043–9053,2023. [127] ChaoqiangZhao,YouminZhang,MatteoPoggi,FabioTosi,
[116] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, XiandaGuo,ZhengZhu,GuanHuang,YangTang,andSte-
LongMai, SimonChen, andChunhuaShen. Learningto fanoMattoccia.Monovit:Self-supervisedmonoculardepth
recover3dsceneshapefromasingleimage. InProceed- estimationwithavisiontransformer. InternationalConfer-
13enceon3DVision,2022.
[128] BoleiZhou,AgataLapedriza,JianxiongXiao,AntonioTor-
ralba, and Aude Oliva. Learning deep features for scene
recognitionusingplacesdatabase. Advancesinneuralin-
formationprocessingsystems,27,2014.
[129] Hang Zhou, David Greenwood, and Sarah Taylor. Self-
SupervisedMonocularDepthEstimationwithInternalFea-
tureFusion. InBritishMachineVisionConference,2021.
[130] Tinghui Zhou, Matthew Brown, Noah Snavely, and
DavidG.Lowe.UnsupervisedLearningofDepthandEgo-
MotionfromVideo. ConferenceonComputerVisionand
PatternRecognition,pages6612–6619,2017.
14