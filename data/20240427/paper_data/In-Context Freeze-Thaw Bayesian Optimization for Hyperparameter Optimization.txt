In-Context Freeze-Thaw Bayesian Optimization
for Hyperparameter Optimization
HerilalainaRakotoarison*1 StevenAdriaensen*1 NeeratyoyMallik*1
SamirGaribov1 EdwardBergman1 FrankHutter1
Abstract 1.Introduction
Hyperparameters are essential in deep learning (DL) to
achieve strong model performance. However, due to the
With the increasing computational costs asso- increasingcomplexityofthesemodels,itisbecomingmore
ciated with deep learning, automated hyperpa- challengingtofindpromisinghyperparametersettings,even
rameter optimization methods, strongly relying withthehelpofhyperparameteroptimization(HPO,Sec-
onblack-boxBayesianoptimization(BO),face tion3.1)tools(Feurer&Hutter,2019;Bischletal.,2023).
limitations. Freeze-thaw BO offers a promis- TraditionalHPOtechniquesusingBayesianOptimization
inggrey-boxalternative,strategicallyallocating (BO)areunsuitableformodernDLbecausetheytreatthe
scarce resources incrementally to different con- problemasablackbox,makingitcomputationallyexpen-
figurations. However, the frequent surrogate siverequiringafullmodeltrainingforeachevaluation.
model updates inherent to this approach pose
RecentresearchinHPOhasshiftedtowardsmulti-fidelity
challenges for existing methods, requiring re-
methods(Lietal.,2017;2020a;Falkneretal.,2018;Klein
training or fine-tuning their neural network sur-
etal.,2020;Lietal.,2020b;Awadetal.,2021),utilizing
rogatesonline,introducingoverhead,instability,
lowerfidelityproxies(e.g.,trainingforfewersteps,using
and hyper-hyperparameters. In this work, we
less data, smaller models) and only evaluating the most
propose FT-PFN, a novel surrogate for Freeze-
promisinghyperparametersettingsatthefullfidelity. While
thaw style BO. FT-PFN is a prior-data fitted
thesemethodshavepotential,theyoftenusecoarse-grained
network (PFN) that leverages the transformers’
fidelity spaces and rely on the rank correlation of perfor-
in-context learning ability to efficiently and re-
mancesacrossfidelities. Moreover,theydonotfullyutilize
liablydoBayesianlearningcurveextrapolation
the anytime nature of algorithms such as checkpointing,
inasingleforwardpass. Ourempiricalanalysis
continuation, and extrapolation. As a result, these meth-
acrossthreebenchmarksuitesshowsthatthepre-
odsstruggletoallocatecomputationalresourcesefficiently,
dictionsmadebyFT-PFNaremoreaccurateand
leadingtosuboptimalperformanceinmanyscenarios.
10-100timesfasterthanthoseofthedeepGaus-
sianprocessanddeepensemblesurrogatesused The freeze-thaw BO method (FT-BO, Section 3.2), orig-
in previous work. Furthermore, we show that, inally proposed by Swersky et al. (2014), is a promising
whencombinedwithournovelacquisitionmech- approachtoefficientlyallocatecomputationalresourcesby
anism(MFPI-random),theresultingin-context pausingandresumingtheevaluationofdifferenthyperpa-
freeze-thawBOmethod(ifBO),yieldsnewstate- rameter configurations. This method is an improvement
of-the-artperformanceinthesamethreefamilies overtraditionalgrey-boxmethodsasitdynamicallyman-
ofdeeplearningHPObenchmarksconsideredin agesresources,andrecentimplementations(Wistubaetal.,
priorwork. 2022; Kadra et al., 2023) hold the state-of-the-art in the
low-budgetregime(∼20fullfunctionevaluations). How-
ever,thesecontemporaryimplementationshavelimitations.
In particular, they rely on online learning to update the
*Equalcontribution 1DepartmentofComputerScience,Uni- surrogatemodelateachstepwhichcanleadhighcompu-
versity of Freiburg, Germany. Correspondence to: Heri- tationaloverhead,instability,andcomplexityinmanaging
lalaina Rakotoarison <rakotoah@cs.uni-freiburg.de>, Steven additionalhyper-hyperparameters. Moreover,theyalsosuf-
Adriaensen <adriaens@cs.uni-freiburg.de>, Neeratyoy Mallik ferfromstrongassumptionsaboutlearningcurves,which
<mallik@cs.uni-freiburg.de>.
maynotbeapplicableinallscenarios,resultinginoverly
Preprint.Underreview. confidentincorrectpredictions.
1
4202
rpA
52
]GL.sc[
1v59761.4042:viXraIn-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure1.Comparisonofpredictionatahorizonof50steps,giventhesamesetofhyperparametersandtheirlearningcurves.TheGround
·
truth curvesshowthereallearningcurveswithdots( )indicatingthepointsobservedastrainingsetorcontextforallthesurrogates.
ifBOusesFT-PFNasitssurrogate,whichrequiresnorefittingbutinsteadusesthetrainingdotsascontextforinferringtheposterior
predictivedistributionatstep50forthegivenhyperparameters.DPLandDyHPO,useDeepPowerLawsEnsemblesandDeepKernel
GaussianProcessrespectively,aretrainedonthetrainingsettillconvergenceandthenusedtoextrapolatethegivenHPs.Thebottomrow
plotsthelog-likelihoodofthepredictionofeachHPatstep50,withthestars(⋆)indicatingthetruevalueofthecurve.
Inthiswork,weleverageprior-datafittednetworks(Mu¨ller mark suites for HPO for deep learning (LCBench,
etal.,2022,PFNs)(Section3.3),aTransformer-basedmeta- Taskset,PD1).
learningapproachtoBayesianinference,toenhancefreeze-
thaw BO through in-context learning. Our PFN model
(FT-PFN)infersthetask-specificrelationshipbetweenhy- We will publicly release the code for the surrogate PFN
perparametersettingsandtheirlearningcurvesinasingle trainingandreproducingexperimentsfromthispaper.
forwardpass,eliminatingtheneedforonlinetrainingduring
the search. Figure 1 compares learning curves extrapola- 2.RelatedWork
tion, including uncertainty, by our model (FT-PFN) and
two baselines. Beyond demonstrating superior extrapola- Multi-fidelityhyperparameteroptimization useslow-
tionquality,ourmodeldirectlyaddresseskeychallengesin costapproximationsoftheobjectivefunction,forexample,
traditionalHPOmethods,loweringcomputationaloverhead byevaluatingonlyafewepochsofmodeltraining. Ano-
andstabilizingtheoptimizationprocess. Thecontributions tableapproachinthiscategoryisHyperband(Lietal.,2017),
ofthispaperareasfollows. whichiterativelyallocatesasimilarbudgettovariouscan-
didate hyperparameter settings and retains only the most
promising ones, to be evaluated at a higher budget. Hy-
• We propose FT-PFN (Section 4.1), a new surrogate
perbandwasextendedforefficientparallelization(Lietal.,
modelforfreeze-thawBO,replacingonlinelearning
2020a), Bayesian optimization (Falkner et al., 2018) and
within-contextlearningusingPFNs. Wetrainasingle
evolutionarysearch(Awadetal.,2021)andtoincorporate
instance of FT-PFN model exclusively on synthetic
userpriors(Malliketal.,2023). However,akeylimitation
data, generated from a curve prior (Section 4.1) de-
ofthesemulti-fidelityapproachesisthattheydonotallow
signedtomimicrealisticHPOlearningcurves.
the continuation of previously discarded runs in the light
ofnewevidence,resultinginawasteofcomputationalre-
• WeempiricallyshowthatFT-PFNoutperformsexist-
sources. Additionally,theireffectivenessheavilydepends
ing surrogates, at point prediction and posterior dis-
onagoodmanualchoiceoffidelitylevelsandastrongcor-
tributionapproximation,whilebeingoveranorderof
relationbetweentheranksofhyperparametersettingsatlow
magnitudefaster,anddespiteneverhavingbeentrained
andhighfidelities(e.g.,nocrossingsoflearningcurves).
onrealHPOdata(Section5.1).
A promising research direction to mitigate these limi-
• We combine FT-PFN with a novel acquisition func- tations involves predicting performance at higher fideli-
tion (MFPI-random, Section 4.2) and find that the ties. Domhan et al. (2015) addressed this by proposing
resultingin-contextfreeze-thawmethod(ifBO)yields a Bayesian learning curve extrapolation (LCE) method.
anewstate-of-the-artperformanceonthethreebench- Then, Klein et al. (2017) extended the latter approach to
2In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
jointly model learning curves and hyperparameter values 3.Preliminaries
withBayesianNeuralNetworks. Non-Bayesianversionsof
Wenowdiscusssomepreliminariesthatwebuildonmore
LCEhavealsobeenexploredbyChandrashekaran&Lane
formally,introducingournotationalongtheway.
(2017)and Gargianietal.(2019). Despitetherobustextrap-
olationcapabilitiesoftheseLCEmethods,fullyleveraging
3.1.HyperparameterOptimization(HPO)
theminguidingHPOremainsachallenge.
Consider an iterative machine training pipeline with con-
Freeze-ThawBayesianOptimization offersapromising figurable hyperparameters λ ∈ Λ. E.g., when training a
solutiontoaddressthelimitationsofstandardmulti-fidelity neuralnetworkusinggradientdescentwecouldconfigure
andLCE-basedHPO.Itsabilitytopauseandresumeopti- thelearningrate,weightdecay,dropoutrate,etc. Letf(λ,t)
mizationrunsenablestheefficientmanagementofcomputa- be some measure of downstream performance (e.g., vali-
tionalresourcesandensuressolidanytimeperformance. As dationloss)ofthemodelobtained,usinghyperparameter
aBOmethod,itincorporatesasurrogatemodeltopredict settingsλ,aftertiterationsoftraining,thatwewouldlike
performanceathigherfidelitiesandreliesontheacquisition tominimize. Further,assumethatthetrainingrunforany
functiontoguidethesearch. Thefreeze-thawconceptwas λ is limited to b max such iterations. HPO aims to find a
introducedbySwerskyetal.(2014),whousedanentropy
hyperparametersettingproducingthebestmodel,i.e.,λ∗:
search acquisition function and a Gaussian Process with (λ∗,·) ∈ argmin λ∈Λ,t∈N 0f(λ,t), within the limited total
exponentialdecaykernelformodelinglearningcurves. Wis- optimizationbudgetofB iterations. Notethatinmodern
tubaetal.(2022)recentlyproposedDyHPO,animproved deeplearningthebudgetavailableforHPOoftendoesnot
version that uses a learned deep kernel combined with a allowustoexecutemorethanafewfulltrainingruns.Inthis
multi-fidelity-basedExpectedImprovement(EI)acquisition. setting,thecruxofHPOliesinallocatingtheselimitedtrain-
Mostrecently,Kadraetal.(2023)introducedDPL,another ingresourcestothemostpromisinghyperparametersettings,
(cid:80)
surrogate model that utilizes a deep ensemble of power i.e.,tofindanallocation{b λ} λ∈Λwith λ∈Λb λ =Bthat
lawsandExpectedImprovementatthemaximumbudgetas minimizesmin λ∈Λ,1≤t≤bλf(λ,t).
acquisitionfunction. TheseFreeze-ThawBOmethodssub-
stantiallyimproveuponstandardmulti-fidelityapproaches, 3.2.Freeze-ThawBayesianOptimization(FT-BO)
showingparticularlystrongperformanceinlow-budgetset-
Asdiscussedabove,Swerskyetal.(2014)proposedtoad-
tings. Nevertheless,thesemethodsshareacommonchal-
dress this challenge by following a fine-grained dynamic
lenge: thenecessityforonlinetrainingofasurrogatemodel
schedulingapproach,allocatingresourcestoconfigurations
duringthesearch,whichcanbecomputationallyintensive
(andobservingtheirperformance)“onestepatatime”.Here,
andmaycauseoptimizationinstabilities.
one“step”,denotedas1 ,correspondstooneormoreitera-
b
tionsofmodeltraining(e.g.,oneepoch).Algorithm1shows
In-contextLearning(ICL) isanexcitingnewlearning thefreeze-thawBayesianoptimizationframework(FT-BO)
paradigmthatoffersapromisingalternativetoonlinelearn- whichusesitshistoryH,i.e.,thevariouspartiallearning
ing methods. A key advantage of ICL is that it does not curves observed thus far in the current partial allocation,
requireretraining/fine-tuningthemodelwithnewdata,but tofitadynamicBayesiansurrogatemodelMthatproba-
instead,thedataisfedtothemodelasacontextualprompt. bilisticallyextrapolatesthepartiallyseenperformanceof
ICLfirstgainedalotofinterestwiththeriseofTransformer- configurationλbeyondb (Algorithm1,line4). Following
λ
basedmodelslikeLargeLanguageModels(Radfordetal., theBOframework,itdecideswhichofthesetocontinue(or
2019,LLMs)andhassincealsobeenexploredasanend-to- startifb =0)usingadynamicacquisitionpolicyAtrad-
λ
endapproachtoblackboxHPO(Chenetal.,2022). ingoffexplorationandexploitation(Algorithm1,line5).
ThecrucialdifferencewithtraditionalblackboxBayesian
OptimizationliesinthatresourcesinFT-BOareallocated
Prior data fitted networks, proposed by Mu¨ller et al.
onestep,ratherthanonefulltrainingrun,atthetime. Note,
(2022),aretransformer-basedmodelsthataretrainedtodo
FT-BOassumestrainingtobepreemptive,i.e.,thatwecan
in-contextBayesianprediction.Theyhavebeensuccessfully
stop(freeze)amodeltraining,andcontinue(thaw)itlater;
usedasanin-contextclassifierfortabulardata(Hollmann
thisassumptionisreasonableinmodernDLwherecheck-
etal.,2023),anin-contextsurrogatemodelforblack-box
pointingiscommonpractice. Alsonote,thatblackboxBO
HPO(Mu¨lleretal.,2023),anin-contextmodelforBayesian
canberecoveredasaspecialcase,bysettingb ,themax-
learningcurveextrapolation(Adriaensenetal.,2023),and max
imumnumberofstepsallocatedtoanysingleconfiguration,
an in-context time-series forecaster (Dooley et al., 2023).
to one, and 1 to the maximum budget available for any
Ourapproachdrawsonandexpandsthesepriorworksto b
singletrainingrun.
create an efficient in-context surrogate model for freeze-
thawBO.
3In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Algorithm 1 Freeze-thaw Bayesian Optimization. Blue 4.In-ContextFreeze-ThawBO(ifBO)
commentsdetailifBOspecifics.
Inthissection, wedescribeifBO, thein-contextlearned
Input:Λ:configurationspace,
variant of the freeze-thaw framework that we propose as
f:measureofmodelperformancetobeminimized,
1 :iterationsofmodeltrainingperfreeze-thawstep, analternativefortheexistingonlinelearningimplementa-
b
b :maximalstepsforanyconfigurationλ∈Λ, tions (Wistuba et al., 2022; Kadra et al., 2023). As can
max
B:totalHPObudgetiniterationsofmodeltraining. be seen in Algorithm 1 (line 4), the critical difference
Components:
lies in the fact that we do not need to refit our surrogate
M:thedynamicsurrogatemodel(FT-PFN),
modelaftereveryallocationstep, butinsteadprovidethe
A:thedynamicacquisitionpolicy(MFPI-random,Alg.2)
Output:λ∈Λ,thatobtainedthebestperformanceobserved full history of performance observations H as input for
prediction. By skipping the online refitting stage, we re-
Procedure: HPO(Λ,f,1 b,b max,B): ducecomputationaloverhead,codecomplexity,andhyper-
hyperparameters. NotethatwithintheFT-BOframework
1: λ∼U(Λ) initialrandomsample
describedinSection3.2,ourmethodisfullycharacterized
2: H ←{(λ,1,f(λ,1 ))} evaluatef (trainλforonestep)
b
byourchoiceofdynamicsurrogatemodel(Section4.1)and
3: while|H|·1 <Bdo acquisitionpolicy(Section4.2).
b
4: TrainMonH FT-PFNrequiresnomodelfitting
5: λ←−A(Λ,M,H,b max) selectλtothaw 4.1.DynamicSurrogateModel(FT-PFN)
6: n ← max{z} n :stepsperformedusingλ
λ (λ,z,y)∈H λ
7: y←f(λ,(n λ+1)·1 b) evaluatef (thawλforonestep) We propose FT-PFN a prior-data fitted network (Mu¨ller
8: H ←H∪{(λ,n +1,y)}
λ etal.,2022,PFNs)trainedtobeusedasanin-contextdy-
9: endwhile
namicsurrogatemodelinthefreeze-thawframework. As
10: returnλ∗:(λ∗,·)∈argmin f(λ,z·1 ) described in Section 3.3, PFNs represent a general meta-
λ∈Λ,1≤z≤nλ b
learnedapproachtoBayesianprediction,characterizedby
thedatausedformeta-learning. Followingpreviousworks
using PFNs (Mu¨ller et al., 2022; Hollmann et al., 2023;
3.3.Prior-dataFittedNetworks(PFNs)
Mu¨lleretal.,2023;Adriaensenetal.,2023;Dooleyetal.,
Asbrieflydiscussedabove,PFNs(Mu¨lleretal.,2022)are 2023), we train the PFN only on synthetically generated
neuralnetworksq thataretrainedtodoBayesianprediction data,allowingustogeneratevirtuallyunlimiteddataand
θ
for supervised learning in a single forward pass. More giving us full control over any biases therein. We aim to
specifically,letD =D ∪{(x ,y )}beadataset trainFT-PFNonartificialdata(D)thatresemblesthereal
train test test
usedfortraining;thePFN’sparametersθareoptimizedto performancedataweobserveinthecontextofFT-BO,i.e.,
takeD andx asinputsandmakepredictionsthat acollectionoflearningcurvesoftrainingrunsforthesame
train test
approximatetheposteriorpredictivedistribution(PPD)of task,butusingdifferenthyperparametersettings,i.e.,
theoutputlabely :
test
D ={(λ ,z ),f(λ ,z ·1 )}N (2)
q (x ,D )≈P(y |x ,D ), i i i i b i=1with1≤zi≤bmax
θ test train test test train
Note that since 1 acts as a constant scaling fac-
inexpectationoverdatasetsDsampledfromapriorp(D) b
tor, and we will normalize z w.r.t. b , we con-
overdatasets. Attesttime,thePFNdoesnotupdateitspa- i max
sider{(λ ,t ),f(λ ,t )}N asdatasetwhendis-
rametersbygradientdescentgivenatrainingdatasetD train, i i i i i=1with0≤ti≤1
cussingoursurrogate.
butrathertakesD asacontextualinput,predictingthe
train
labelsofunseenexamplesthroughin-contextlearning. The
PFNispretrainedonceforaspecificpriorp(D)andused Priordesiderata: FromaBayesianperspective,wewant
in downstream Bayesian prediction tasks without further togeneratedatafromadistributionthatcapturesourprior
fine-tuning. Morespecifically,itistrainedtominimizethe beliefsontherelationshipbetweenhyperparametersλ,train-
cross-entropy for predicting the hold-out example's label ingtimet,andmodelperformancef(λ,t).Whileonecould
y test,givenx testandD train: designsuchpriorforaspecificHPOscenario,ourgoalhere
istoconstructagenericprior,resultinginanFT-PFNsur-
ℓ θ =E[(cid:57)logq θ(y test|x test,D train)] rogateforgeneralHPO.Tothisend,weleveragegeneral
(1)
with {(x ,y )}∪D ∼p(D) beliefs,e.g.,weexpectlearningcurvestobenoisy,butto
test test train
exhibitanimproving,convex,andconvergingtrend;curves
Mu¨lleretal.(2022)provedthatthistrainingprocedureco- onthesametaskareexpectedtohavesimilarstart,satura-
incides with minimizing the KL divergence between the tion,andconvergencepoints;andtrainingrunsusingsimilar
PFN'spredictionsandthetruePPD. hyperparametersettingstoproducesimilarlearningcurves.
4In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
andΨ itsthebasisfunctionspecificparameters. Here,we
k
adoptfourdifferentbasisfunctions(K =4),eachhaving
fourparameters,resultinginatotalof22(=|E|)parameters
dependingonλthroughp.Ourfourbasisfunctionssubsume
thepowerlawmodelusedbyKadraetal.(2023),allthree
basis functions used by Adriaensen et al. (2023), and 9
ofthe11basisfunctionsoriginallyproposedbyDomhan
et al. (2015).2 Furthermore, unlike those considered in
previous works, our basis functions can have a breaking
point(Caballeroetal.,2023)atwhichconvergencestagnates
orperformancediverges,resultinginamoreheterogeneous
Figure2.DiagramforthepriordatamodeldescribedinSection4.1
andrealisticmodel.
that was used to generate data for meta-training FT-PFN. On
theleft,wehavetherandomlyinitializedneuralnetworkpthat Furtherdetailsaboutmeta-trainingFT-PFNcanbefound
modelstherelationshipbetweenahyperparametersettingλand
in Appendix A, including the basis curves, their parame-
itslearningcurve(showninpink),whoseoutputparameterizes
ters, andillustrationsofsamplesfromourlearningcurve
a curve model fˆthat is a linear combination of K (=2 in this
prior (Appendix A.1); the specific procedure we use for
illustration)basisfunctions(showninredandblue)withadded
λ-specificGaussiannoisewithvarianceσ2. generatingourmeta-trainingdata(AppendixA.2);andthe
architectureandhyperparametersused(A.3).
Prior data model: Following, Klein et al. (2017) and 4.2.DynamicAcquisitionFunction(MFPI-random)
Kadra et al. (2023), we model the relationship between
trainingtimetandf(λ,t)byaparametriclearningcurve FollowingtheFreeze-ThawBayesianOptimizationframe-
modelfˆ(t;E),whoseparametersE aretheoutputofaneu- work(Section3.2),wecontinuetrainingtheconfiguration
λ∈Λthatmaximizessomeacquisitionfunction. Here,we
ralnetworkp(λ;θ)takingthehyperparametersettingsλas
assumeageneralmulti-fidelity(MF)acquisitionfunction
input(seeFigure2). However,unlikethesepreviousworks,
(AF)representedas,
we do not train the parameters θ of this neural network.
Instead, we randomly initialize the network, to represent MF-AF(λ,z|f ):Λ× Z →R, (4)
threshold
atask-specificrelationshipbetweenhyperparametersand
theirlearningcurves,whichwethenusetogeneratedata
whereAFisageneralacquisitionfunction,λ∈Λahyper-
for training FT-PFN. This can be viewed as generating parameterconfiguration, z ∈ Z thefidelity(e.g. number
samples from a Bayesian Neural Network (BNN) prior, oftrainingsteps),andf threshold afunctionoff best (bestob-
meta-trainingFT-PFNtoemulateLCNet-likeBNNinfer- served performance). For example, fixing z = b max and
ence through in-context learning. As for p, we adopt the f threshold =f best,recoversavanillaAFsuchasExpectedIm-
samearchitectureandinitializationprocedurefortheBNN provement(EI)orProbabilityofImprovement(PI)(Mockus
priorusedbyMu¨lleretal.(2023)inthecontextofBB-BO. etal.,1978).
Theonlydifferencehereisthatphas|E|outputs,onefor
Thequalityofuncertaintycalibrationinsurrogatepredic-
eachparameterofourcurvemodelfˆ,asopposedtoasingle
tionsdependsonthetask,surrogatetrainingsetquality,and
output (model performance after a full training run). As extrapolationlength(horizon). Meanwhile,theAFscoreis
forfˆ,followingDomhanetal.(2015),Kleinetal.(2017)
calibratedbyanimprovementthreshold. Inourwork,we
andAdriaensenetal.(2023),wemodellearningcurvesasa choosePIasthebaseAFasitislesslikelytodrivesearch
linearcombinationofK basisfunctionsf k,withadditive towardshighuncertaintyregionsalongthehyperparameter
Gaussiannoise,i.e., andfidelitysubspaces. Insteadofusingafixedthreshold
(cid:16) (cid:17) orafixedextrapolationhorizonforthisPIcalculation,we
fˆ(t,E)∼N fˆ (t,E),σ2 with
comb explorearangeofpossiblethresholdsandhorizonsbyran-
K domsamplingfromeitherfixedorproblem-specificbounds.
(cid:88)
f (t,E)=y +(y −y )· w f (t,Ψ ) Suchsamplingprocedureisundertakeneveryiterationand
comb 0 ∞ 0 k k k
isakintoanacquisitionselectionfromaportfolioofdiffer-
k=1
and p(λ;θ)=E =(σ,y ,w ,...,w ,Ψ ,...,Ψ ) entPIs.Incontrasttointegratingoverallthresholds(leading
∞ 1 K 1 K
to Expected Improvement, EI), this allows for greater di-
(3)
versificationintheexplorationprocess. Wepositthatthis
wherey isthemodelperformanceafter0trainingsteps,and hedgingwithaportfolioofAFsineachiterationbenefits
0
y thatatconvergence.1 w theweightofbasiscurvef , freeze-thaw setups where queries are more granular than
∞ k k
1Notethaty ∈/ E asweassumeittobeindependentofλ. 2Ourmodelexcludesthetwounboundedbasiscurves.
0
5In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
standardBO.Theresultisasimple,parameter-freeAFwith multipleshortercurvesareexplored. Additionaldetailson
abalancedexploration-exploitationtrade-off, the sampling strategy can be found in Appendix A.2. To
assessthequalityofthepredictions,weutilizetwometrics:
log-likelihood(log-score,thehigherthebetter),measuring
MFPI-random(λ,zrand|frand ):Λ× Z →R, (5) the approximation of the posterior distribution (∼ uncer-
threshold
taintycalibration),andmeansquarederror(thelowerthe
which evaluates to the predicted likelihood that a candi-
better), measuring the accuracy of point predictions. We
date configuration λ ∈ Λ after zrand more steps of train-
also report the runtime, accounting for fitting and infer-
ing obtains a model exceeding the frand performance
threshold enceofeachsurrogate. Theevaluationwasrunonasingle
threshold, where zrand ∼ U(1,b ) is the sampled ex-
max Intel Xeon 6242 CPU.
trapolation horizon, and frand = f · (1 − τ), with
threshold best
log (τ) ∼ U(−4,−1) as the sampled scaling for the
10 Resultsdiscussion: Table1presentsthelog-likelihood
threshold. Further details, as well as pseudo code (Algo-
andMSE(MeanSquaredError)foreachapproachrelative
rithm2)canbefoundinAppendixA.4.
to the context sample size. As expected, we observe an
increase in log-likelihood and a decrease in MSE as the
5.Experiments contextsizegetlarger. Notably,FT-PFNanditsNoHPs
variantsignificantlyoutperformDPLandDyHPOinterms
Inthissection,wecompareifBOtostate-of-the-artmulti-
oflog-likelihood. DPLinparticularhaslowlog-likelihood
fidelityfreeze-thawBayesianoptimizationmethods. Tothis
values,correspondingtoapooruncertaintyestimatesuch
end,wefirstassessFT-PFNintermsofthequalityandcost
asbeingoverlyconfidentinincorrectpredictions. Thismay
ofitsprediction(Section5.1).Then,weassessourapproach,
be due to the very low ensemble size (= 5) adopted by
whichcombinesFT-PFNwithourMFPI-randomacqui-
(Kadraetal.,2023)compoundedbytheirstrongpowerlaw
sition function, on HPO tasks (Section 5.2). Finally, we
assumption. Ontheotherhand,DyHPOstruggleswithlow
conductanablationstudyonacquisitionfunctionusedin
log-likelihoodduetoitsinabilitytoextrapolatebeyonda
ifBO(Section5.3).
singlestepeffectively. RegardingMSE,FT-PFNgenerally
Our main baselines are both other recent freeze-thaw ap- surpassesthebaselinesinLCBenchandPD1,performing
proaches: DyHPO(Wistubaetal.,2022)andDPL(Kadra comparablytoDPLonTaskset.
etal.,2023). Weusethepubliclyavailableimplementations
Beyondtheimpressivelog-likelihoodandMSEresults,our
forDyHPOandDPLwithdefaulthyperparametervalues.
approachalsoyieldsignificantspeedadvantagesoverthe
We conduct our experiments on three benchmarks: baseline methods. Importantly, FT-PFN maintains supe-
LCBench(Zimmeretal.,2021),PD1(Wangetal.,2021), riorityinqualityandspeedforinferenceswithmorethan
andTaskset(Metzetal.,2020). Thesebenchmarks,cov- 1000samplesasacontextwithoutnotbeingtrainedinthis
eringdifferentarchitectures(Transformers,CNNs,MLPs) regime. Dependingonthecontextsamplesize,ourmethod
andtasks(NLP,vision,tabulardata),arecommonlyused achievesspeedupsrangingfrom10×to100×fasterthan
in the HPO literature. A detailed overview of the tasks DPLandDyHPO.
includedineachbenchmarkispresentedinAppendixB.
5.2.AssessmentofHPOperformance
5.1.CostandQualityofPredictions
Inthissection,wepresentanextensiveempiricalcompari-
In this section, we compare the predictive capabilities of son,showingthemeritsofourmethodonHPOtasksacross
FT-PFNtothatofexistingsurrogatemodels,includingthe avarietyoftabularbenchmarks(seeAppendixB)inthe
deepGaussianprocessofDyHPO(Wistubaetal.,2022)and lowbudgetregime. AdditionallytoDPLandDyHPO, we
thedeepensembleofpowerlawsmodelofDPL(Kadraetal., include Hyperband, ASHA, Gaussian Process-based FT-
2023). WealsoconsideravariantofFT-PFNtrainedon BO(usingDyHPO’sone-stepEIacquisitionfunction)and
thesameprior,butnottakingthehyperparametersasinput uniformrandomsearch,asbaselines.Eachalgorithmisallo-
(referredtoas”noHPs”). Thisvariantbasesitspredictions catedatotalbudgetof1000stepsforeverytask,whichcor-
solelyonasetofpartiallyobservedlearningcurves. respondsto20fulltrainingsonLCBenchandTaskset
witheachtaskbeingrepeated10times,eachtimestarting
Evaluationprocedure: Fromagivenbenchmark,wesam- from a different random configuration (see Algorithm 1).
plebothasetofpartialcurves,whereeachcurvehasitsown Wereporttwocomplementarymetrics: Thenormalizedre-
setoftargetepochs. Theselectionprocessisstrategically gret, capturing performance differences, and the average
designed to encompass a wide range of scenarios, vary- rankofeachmethod,capturingtherelativeorder. Formally,
ing from depth-first approaches, which involve a smaller thenormalizedregretcorrespondstoa[0,1]normalization
numberoflongcurves,tobreadth-firstapproaches,where of the observed error w.r.t to the best (lowest) and worst
6In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Table1.ComparisonofFT-PFN,avariantofFT-PFNthatexcludeshyperparameters,DyHPOandDPLacrossthreebenchmarks.Values
representthemedianovertasksofthelog-likelihoodandmeansquarederror(MSE)aswellastheruntimeofpredictions.
LCBench PD1 Taskset Runtime(s)
#samples Method Log-likelihood MSE Log-likelihood MSE Log-likelihood MSE
DPL −14.577 0.007 −13.384 0.043 −26.011 0.005 17.686
400 DyHPO −0.481 0.042 −0.573 0.104 −0.465 0.009 16.860
FT-PFN(noHPs) 1.649 0.008 0.983 0.028 2.860 0.005 0.215
FT-PFN 1.876 0.005 0.925 0.030 2.934 0.004 0.225
DPL −13.291 0.007 −11.721 0.037 −21.779 0.005 33.480
800 DyHPO −0.426 0.031 −0.510 0.088 −0.419 0.008 64.809
FT-PFN(noHPs) 1.701 0.007 1.103 0.024 2.835 0.005 0.527
FT-PFN 2.044 0.004 1.072 0.025 2.975 0.004 0.541
DPL −11.983 0.007 −11.017 0.035 −20.350 0.004 41.956
1000 DyHPO −0.368 0.012 −0.457 0.071 −0.381 0.008 59.949
FT-PFN(noHPs) 1.763 0.007 1.120 0.024 2.877 0.005 0.687
FT-PFN 2.118 0.004 1.133 0.024 3.016 0.004 0.719
DPL −11.333 0.007 −10.353 0.033 −17.760 0.004 56.576
1400 DyHPO −0.361 0.011 −0.438 0.061 −0.374 0.008 112.168
FT-PFN(noHPs) 1.733 0.007 1.225 0.021 2.874 0.005 1.084
FT-PFN 2.137 0.003 1.201 0.022 3.042 0.004 1.130
DPL −9.182 0.007 −9.263 0.035 −13.712 0.004 73.435
1800 DyHPO −0.365 0.009 −0.437 0.058 −0.381 0.008 166.491
FT-PFN(noHPs) 1.753 0.006 1.251 0.019 2.858 0.005 1.635
FT-PFN 2.199 0.003 1.192 0.022 3.057 0.004 1.715
(highest)errorsrecordedbyallalgorithmsonthetask. budget b (DPL). We also include their PI counter-
max
partsPI(one step)andPI(max),aswellasvariants
of MFPI-random that only vary the prediction horizon,
Results discussion: Figure 3 presents the comparative PI(random horizon) with T = 0, or only vary the
resultsperbenchmarkfamily. Theresultsvalidatethesupe- threshold,PI(max, random-T)withh=b . Apart
max
riorityoffreeze-thawapproaches(ifBO,DyHPO,andDPL) fromthemethodscompared,theexperimentalsetupisiden-
comparedtostandardapproaches(Hyperband,ASHA,and ticaltothatinSection5.2.
randomsearch)forlow-budgetsettings. Mostnotably,these
resultsestablishthepromiseofifBO,whicheitheroutper-
Resultsdiscussion: Figure4showsthecomprehensive
forms(onLCBenchandTaskset)orcompetesclosely
resultsforourifBOvariantsforeachofthebenchmarks,
(onPD1)withDPLandDyHPO. ifBOisalsoconsistently
intermsofaverageranksandaveragenormalizedregrets,
the best on average rank across all benchmarks (see Ap-
aggregated across all tasks. Generally, we find that per-
pendixC.2,Figure7). AppendixE(Figures11-16)offersa
formance varies strongly between acquisitions, suggest-
closerlookattherawerrormetricsforeachalgorithmper
ing this choice is at least as important for HPO success
task. Thesedetailedresultscollectivelyconfirmtherobust-
as our surrogate’s superior predictive quality. In particu-
nessofifBOforHPOtasks,showingitsabilitytocompete
lar, we find that combinations with the EI-based acquisi-
ifnotoutperformthemostcompetitivebaselines.
tionsEI(one step)andEI(max)proposedinprior-
art, are amongst the worst-performing variants, both in
5.3.AblationonAcquisition
termsofrankandregret. Forexample,EI(max)failson
Inthissection,weevaluatehowifBOperformsincombi- LCBenchandPD1,whileEI(one step)failsonPD1
nation with other acquisition functions and aim to assess andTaskset. Curiously,thesetrendsdonotseemtoex-
to what extent our novel acquisition function described tendtoourbaselines,e.g.,DPLusingEI(max)performs
in Section 4.2 contributes to its HPO success. To this stronglyonLCBenchandDyHPOusingEI(one step)
end,weconsiderifBOvariantscombiningFT-PFNwith performsstronglyonPD1. Weconjecturethatthisfailure
andcomparingperformanceagainsttheEI-basedacquisi- isrelatedtothe(justified)lackofconfidenceFT-PFNhas
tions used in prior-art (Wistuba et al., 2022; Kadra et al., aboutitspredictions,asisevidentbythesuperiorlog-scores
2023), i.e., EI(one step) predicting one step in the in Table 1. As a result, the predicted posterior will be
future (DyHPO), and EI(max) predicting at the highest heavy-tailed,resultinginthehighEIvaluesforthosecon-
7In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure3.Comparisonofourmethodagainststate-of-the-artbaselinesonall3benchmarks.Firstrowshowsnormalizedregretaggregated
acrossmultipletasksineachbenchmark(SeeAppendixBforbenchmarkdetails,andtheresultspertaskcanbefoundinAppendixE).
Secondrowshowstheaverageranksofeachmethod.
Figure4.ResultsofanablationstudyoftheacquisitionfunctioninifBOoneachbenchmarkfamily.Firstrowshowsnormalizedregret
aggregatedacrossmultipletasksineachbenchmark(AppendixB).Secondrowshowstheaverageranksofeachmethod.
figurations our predictions are least confident for. While benefitsofrandomizingthehorizonandthethreshold.
this drives exploration, in the very low budget regime, it
caneasilyleadtoacatastrophicfailuretoexploit. Overall,
6.Conclusion
wefindthatwhilesomevariantsaresuccessfulatspecific
tasksinearlystagesoftheoptimization,noneexhibitthe In this paper, we proposed FT-PFN a novel surrogate
samerobustnessinperformanceacrossbenchmarks,making for freeze-thaw Bayesian optimization. We showed that
MFPI-randomtheclearwinner. thepointanduncertaintyestimatesproducedbyFT-PFN
throughin-contextlearningaresuperiortothoseobtained
We perform comparable ablation studies for DPL and
by fitting/training recently proposed deep Gaussian pro-
DyHPO, as detailed in Appendix C.3, to demonstrate the
cess (Wistuba et al., 2022) and deep power law ensem-
8In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
ble(Kadraetal.,2023)models,whilebeingoveranorder ImpactStatement
ofmagnitudefaster. Wepresentedthefirst-everempirical
Thispaperpresentsworkwhosegoalistoadvancethefield
comparisonofdifferentfreeze-thawimplementations. Our
ofhyperparameteroptimization(HPO)inmachinelearning.
resultsconfirmthesuperiorityoftheseHPOmethods,inthe
Therearemanypotentialsocietalconsequencesofmachine
low-budgetregime,andshowthatourin-contextlearning
learning, none which we feel must be specifically high-
approachiscompetitivewiththestate-of-the-art.
lightedhere. Wewould,however,liketohighlightthatour
Despiteourpromisingresults,weadmitthatattainingthe workmakesHPOmorerobustandefficient,andwillthus
sampleefficiencyrequiredtoscaleuptomoderndeeplearn- helpmakemachinelearningmorereliableandsustainable.
ing(e.g.,LMMpretraining)remainsachallengingendeavor.
Futureworkshouldattempttoextendourapproachtotake
Acknowledgements
advantageofadditionalsourcesofpriorinformation,e.g.,
todoin-contextmeta-learning,leveraginglearningcurves We thank Johannes Hog for his feedback on the earlier
onrelatedtasks(Ruhkopfetal.,2022);toincorporateuser versionofthepaper. WeacknowledgefundingbytheEu-
priors(Mu¨lleretal.,2023;Malliketal.,2023); andaddi- ropean Union (via ERC Consolidator Grant Deep Learn-
tionalinformationaboutthetrainingprocess(e.g.,gradient ing2.0,grantno.101045765),TAILOR,aprojectfunded
statistics). Analternativetoscalingupisscalinginparallel. byEUHorizon2020researchandinnovationprogramme
Here,weexpectourin-contextlearningapproachtoreduce under GA No 952215, the state of Baden-Wu¨rttemberg
theoverheadfurther,astheonlinelearning/refittingstage through bwHPC and the German Research Foundation
occursonthecriticalpath,whilein-contextlearningduring (DFG)throughgrantnumbersINST39/963-1FUGGand
prediction can easily be parallelized. Finally, the current 417962828. Views and opinions expressed are however
FT-PFNmodelhassomelimitations. First,itrequiresthe those of the author(s) only and do not necessarily reflect
performancemetricandeachhyperparametervaluetobe those of the European Union or the European Research
normalizedin[0,1];andsupportsupto10hyperparameters. Council. NeithertheEuropeanUnionnorthegrantingau-
Whilewebelievethatthisisreasonable,futureworkbuild- thoritycanbeheldresponsibleforthem.
ingsystemsshouldpushtheselimits,trainlargermodels,on
moredata,andexplorewaystoscaletolargercontextsizes.
Insummary,thereisalotleftunexplored,andwehopethat
therelativesimplicity,efficiency,andpublicavailabilityof
ourmethod,lowersthethresholdforfutureresearchinthis
direction. References
Adriaensen, S., Rakotoarison, H., Mu¨ller, S., and Hutter,
F. Efficientbayesianlearningcurveextrapolationusing
prior-data fitted networks. In Proceedings of the 37th
InternationalConferenceonAdvancesinNeuralInfor-
mationProcessingSystems(NeurIPS’23),2023.
Awad,N.,Mallik,N.,andHutter,F. DEHB:Evolutionary
hyberbandforscalable,robustandefficientHyperparam-
eterOptimization. InZhou,Z.(ed.),Proceedingsofthe
30thInternationalJointConferenceonArtificialIntelli-
gence(IJCAI’21),2021.
Bischl, B., Binder, M., Lang, M., Pielok, T., Richter,
J., Coors, S., Thomas, J., Ullmann, T., Becker, M.,
Boulesteix,A.,Deng,D.,andLindauer,M.Hyperparame-
teroptimization: Foundations,algorithms,bestpractices,
andopenchallenges. 2023.
Bojar,O.,Chatterjee,R.,Federmann,C.,Haddow,B.,Huck,
M., Hokamp, C., Koehn, P., Logacheva, V., Monz, C.,
Negri,M.,Post,M.,Scarton,C.,Specia,L.,andTurchi,
M. Findingsofthe2015workshoponstatisticalmachine
translation. InProceedingsoftheTenthWorkshoponSta-
9In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
tisticalMachineTranslation,Lisbon,Portugal,Septem- Gargiani, M., Klein, A., Falkner, S., andHutter, F. Prob-
ber2015. abilisticrolloutsforlearningcurveextrapolationacross
hyperparametersettings. In6thICMLWorkshoponAuto-
Caballero, E., Gupta, K., Rish, I., and Krueger, D. Bro- matedMachineLearning,2019.
kenneuralscalinglaws. InInternationalConferenceon
LearningRepresentations(ICLR’23),2023. Gijsbers,P.,LeDell,E.,Poirier,S.,Thomas,J.,Bischl,B.,
andVanschoren,J.Anopensourceautomlbenchmark.In
Chandrashekaran, A.andLane, I.R. Speedinguphyper- Eggensperger,K.,Feurer,M.,Hutter,F.,andVanschoren,
parameter optimization by extrapolation of learning J.(eds.),ICMLworkshoponAutomatedMachineLearn-
curves using previous builds. In Joint European Con- ing(AutoMLworkshop2019),2019.
ferenceonMachineLearningandKnowledgeDiscovery
inDatabases.Springer,2017. Gilmer, J. M., Dahl, G. E., and Nado, Z. init2winit: a
jaxcodebaseforinitialization,optimization,andtuning
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., research,2021.
andKoehn,P.Onebillionwordbenchmarkformeasuring
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
progressinstatisticallanguagemodeling. arXivpreprint
learning for image recognition. In Proceedings of the
arXiv:1312.3005,2013.
InternationalConferenceonComputerVisionandPattern
Chen,Y.,Song,X.,Lee,C.,Wang,Z.,Zhang,Q.,Dohan, Recognition(CVPR’16),2016.
D.,Kawakami,K.,Kochanski,G.,Doucet,A.,Ranzato,
Hollmann,N.,Mu¨ller,S.,Eggensperger,K.,andHutter,F.
M., Perel, S., anddeFreitas, N. Towardslearninguni-
TabPFN:Atransformerthatsolvessmalltabularclassifi-
versalhyperparameteroptimizerswithtransformers. In
cationproblemsinasecond. InInternationalConference
Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D.,
onLearningRepresentations(ICLR’23),2023.
Cho,K.,andOh,A.(eds.),Proceedingsofthe36thInter-
nationalConferenceonAdvancesinNeuralInformation
Kadra, A., Janowski, M., Wistuba, M., and Grabocka,
ProcessingSystems(NeurIPS’22),2022.
J. Deep power laws for hyperparameter optimization.
In Proceedings of the 37th International Conference
Deng,L. Themnistdatabaseofhandwrittendigitimages
onAdvancesinNeuralInformationProcessingSystems
formachinelearningresearch. IEEESignalProcessing
(NeurIPS’23),2023.
Magazine,29(6):141–142,2012.
Kingma, D., Salimans, T., and Welling, M. Variational
Domhan,T.,Springenberg,J.,andHutter,F. Speedingup
dropoutandthelocalreparameterizationtrick. InCortes,
automaticHyperparameterOptimizationofdeepneural
C.,Lawrence,N.,Lee,D.,Sugiyama,M.,andGarnett,R.
networksbyextrapolationoflearningcurves. InYang,
(eds.),Proceedingsofthe29thInternationalConference
Q.andWooldridge, M.(eds.), Proceedingsofthe24th
onAdvancesinNeuralInformationProcessingSystems
InternationalJointConferenceonArtificialIntelligence
(NeurIPS’15),2015.
(IJCAI’15),2015.
Klein,A.,Falkner,S.,Springenberg,J.,andHutter,F.Learn-
Dooley,S.,Khurana,G.S.,Mohapatra,C.,Naidu,S.,and
ingcurvepredictionwithBayesianneuralnetworks. In
White,C. ForecastPFN:Synthetically-trainedzero-shot
ProceedingsoftheInternationalConferenceonLearning
forecasting.InProceedingsofthe37thInternationalCon-
Representations(ICLR’17),2017.
ferenceonAdvancesinNeuralInformationProcessing
Systems(NeurIPS’23),2023. Klein,A.,Tiao,L.,Lienart,T.,Archambeau,C.,andSeeger,
M. Model-based Asynchronous Hyperparameter and
Falkner,S.,Klein,A.,andHutter,F. BOHB:Robustand NeuralArchitectureSearch. arXiv:2003.10865[cs.LG],
efficientHyperparameterOptimizationatscale. InDy,
2020.
J.andKrause, A.(eds.), Proceedingsofthe35thInter-
nationalConferenceonMachineLearning(ICML’18), Krizhevsky,A. Learningmultiplelayersoffeaturesfrom
volume80.ProceedingsofMachineLearningResearch, tiny images. Technical report, University of Toronto,
2018. 2009.
Feurer,M.andHutter,F. HyperparameterOptimization. In Lefaudeux, B., Massa, B., Liskovich, D., Xiong, W.,
Hutter,F.,Kotthoff,L.,andVanschoren,J.(eds.),Auto- Caggiano, W., Naren, S., Xu, M., Hu, J., Tintore, M.,
matedMachineLearning: Methods,Systems,Challenges, Zhang,S.,Labatut,P.,andHaziza,D. xformers: Amod-
chapter1,.Springer,2019. ularandhackabletransformermodellinglibrary,2022.
10In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Li, J., Liu, Y., Liu, J., andWang, W. Neuralarchitecture selectionusingimplicitfidelityinformation.Transactions
optimizationwithgraphVAE.arXiv:2006.10310[cs.LG], onMachineLearningResearch,2022.
2020a.
Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,
Li,L.,Jamieson,K.,DeSalvo,G.,Rostamizadeh,A.,and Ma,S.,Huang,Z.,Karpathy,A.,Khosla,A.,Bernstein,
Talwalkar, A. Hyperband: Bandit-based configuration M.,Berg,A.,andFei-Fei,L. Imagenetlargescalevisual
evaluation for Hyperparameter Optimization. In Pro- recognitionchallenge. 115(3):211–252,2015.
ceedings of the International Conference on Learning
Swersky, K., Snoek, J., and Adams, R. Freeze-thaw
Representations(ICLR’17),2017.
Bayesian optimization. arXiv:1406.3896 [stats.ML],
Li,L.,Jamieson,K.,Rostamizadeh,A.,Gonina,E.,Ben- 2014.
tzur,J.,Hardt,M.,Recht,B.,andTalwalkar,A. Asystem
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
formassivelyparallelhyperparametertuning. InDhillon,
L.,Gomez,A.,Kaiser,L.,andPolosukhin,I. Attentionis
I.,Papailiopoulos,D.,andSze,V.(eds.),Proceedingsof
allyouneed. InGuyon,I.,vonLuxburg,U.,Bengio,S.,
MachineLearningandSystems2,volume2,2020b.
Wallach,H.,Fergus,R.,Vishwanathan,S.,andGarnett,R.
Liao,Z.andCarneiro,G. Competitivemulti-scaleconvolu- (eds.),Proceedingsofthe31stInternationalConference
tion. arXivpreprintarXiv:1511.05635,2022. onAdvancesinNeuralInformationProcessingSystems
(NeurIPS’17).CurranAssociates,Inc.,2017.
Loshchilov, I. and Hutter, F. SGDR: Stochastic gradi-
ent descent with warm restarts. In Proceedings of the Wang, Z., Dahl, G., Swersky, K., Lee, C., Mariet, Z.,
InternationalConferenceonLearningRepresentations Nado,Z.,Gilmer,J.,Snoek,J.,andGhahramani,Z. Pre-
(ICLR’17),2017. trained Gaussian processes for Bayesian optimization.
arXiv:2207.03084v4[cs.LG],2021.
Mallik,N.,Bergman,E.,Hvarfner,C.,Stoll,D.,Janowski,
M., Lindauer, M., Nardi, L., and Hutter, F. Priorband: Wistuba, M., Kadra, A., and Grabocka, J. Dynamic and
Practicalhyperparameteroptimizationintheageofdeep efficientgray-boxhyperparameteroptimizationfordeep
learning. InThirty-seventhConferenceonNeuralInfor- learning. In Koyejo, S., Mohamed, S., Agarwal, A.,
mationProcessingSystems,2023. Belgrave,D.,Cho,K.,andOh,A.(eds.),Proceedingsof
the36thInternationalConferenceonAdvancesinNeural
Metz,L.,Maheswaranathan,N.,Sun,R.,Freeman,C.D.,
InformationProcessingSystems(NeurIPS’22),2022.
Poole,B.,andSohl-Dickstein,J. Usingathousandopti-
mizationtaskstolearnhyperparametersearchstrategies. Xiao, H., Rasul, K., and Vollgraf, R. Fashion-MNIST: a
arXiv:2002.11887[cs.LG],abs/,2020. novelimagedatasetforbenchmarkingMachineLearning
algorithms. arXiv:1708.07747v2[cs.LG],2017.
Mockus,J.,Tiesis,V.,andZilinskas,A. Theapplicationof
Bayesianmethodsforseekingtheextremum. Towards Zagoruyko,S.andKomodakis,N. Wideresidualnetworks.
GlobalOptimization,2(117-129),1978. InRichardC.Wilson,E.R.H.andSmith,W.A.P.(eds.),
Proceedingsofthe27thBritishMachineVisionConfer-
Mu¨ller, S., Hollmann, N., Arango, S., Grabocka, J., and
ence(BMVC).BMVAPress,2016.
Hutter,F. TransformerscandoBayesianinference. In
ProceedingsoftheInternationalConferenceonLearning Zimmer, L., Lindauer, M., and Hutter, F. Auto-Pytorch:
Representations(ICLR’22),2022. Multi-fidelitymetalearningforefficientandrobustAu-
toDL. 43:3079–3090,2021.
Mu¨ller,S.,Feurer,M.,Hollmann,N.,andHutter,F.Pfns4bo:
In-contextlearningforbayesianoptimization. InKrause,
A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
andScarlett, J.(eds.), Proceedingsofthe40thInterna-
tionalConferenceonMachineLearning(ICML’23),vol-
ume202ofProceedingsofMachineLearningResearch.
PMLR,2023.
Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,and
Sutskever,I.Languagemodelsareunsupervisedmultitask
learners. OpenAIblog,1(8):9,2019.
Ruhkopf, T., Mohan, A., Deng, D., Tornede, A., Hutter,
F., and Lindauer, M. Masif: Meta-learned algorithm
11In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Referencename Formulaf (x ,Ψ ) Priorp(Ψ )
k t k k
−1
pow
4
((ϵ 1α1 −1)∗ xsx at
t,1
+1)−α1 ln(α 1)∼N(1,1) log 10(x sat,k)∼N(0,1), ∀k
( xt )α2
exp
4
(ϵ 2) Xsat,2 ln(α 2)∼N(0,1) ln(ϵ)∼N(−5,1), ∀k
ilog ln(α3) ln(α −1)∼N(−4,1) y =y −ϵ·(y −Y ), ∀k
4 1 3 sat,k inf inf 0
ln((α 3ϵ3−α3) Xsx at t,3+α3)
hill 1 ln(α )∼N(0.5,0.25) 1−r ∼Exp(1), ∀k
4 ( xt )α4(1−1)+1 4 sat,k
Xsat,4 ϵ4
Table2.Theformulasforeachofthefourbasisfunctionsinourcurveprior.Notethateachofthemarenormalizedtostartat1,converge
to0,andpassthroughthesaturationpoint.
A.FurtherDetailsaboutifBO
A.1.TheLearningCurvePrior
Inthissection,wecontinuethediscussionofourlearningcurvepriordefinedbyEquation2:
K
fˆ(t,E)∼N(fˆ (t,E),σ2) with f (t,E)=y +(y −y )·(cid:88) w ∗f (t,Ψ )
comb comb 0 inf 0 k k k
k=1
(6)
K
(cid:91)
and p(λ;θ)=E ={σ,y }∪{w }K ∪ Ψ
inf k k=1 k
k=1
Eachofthebasiscurvestakestheform
(cid:40)
1 t ift≤xλ
f (t,Ψ )=f′(x ,Ψ ) with x = · sat,k (7)
k k k t k t b
max
r sλ at,k(t−x sat,k)+x
sat,k
ift>xλ
sat,k
wheref hasthefollowingfourparametersΨ :
k k
α Theskewofthecurve,determiningtheconvergenceratechangeovertime.
k
x , y Thepointatwhichmodelperformancesaturatesandtheconvergencerateissuddenlyreduced.
sat,k sat,k
r Thereducedconvergencerateaftersaturation,whichcanbenegative,modelingdivergence.
sat,k
andf′(x ,θ )isa[0,1]boundedmonotonicgrowthfunction. Theformulasforthesegrowthfunctions,alongsidethetarget
k t k
distributionsoftheirparameters,arelistedinTable2.
Finally,notethatgiventhesechoices,wehavef (t,E)∈[0,1]andwecliptheGaussiannoiseinfˆ(t,E)inthesame
comb
range. Asaconsequence,ifperformancedoesnotnaturallyfallinthisrange,itmustbenormalizedbeforepassingitto
FT-PFN. ExamplesofcollectionsofcurvesgeneratedusingthispriorcanbefoundinFigure5.
A.2.Meta-trainingDataGeneratingProcedure
A single meta-training example in our setting corresponds to a training set D and test set D , where
train test
b′
D ={(λ,t),f(λ,t)} λ correspondstothe(synthetic)partiallearningcurvesobservedthusfar(i.e.,theanalogof
train t=1,∀λ∈Λ
H attesttime)andD ⊆λ,t, f(λ,t)} theextrapolationtargetswewantFT-PFNtopredict. Tokeepthe
test λ∈Λ,bmax≥t>bλ
inputsizeofFT-PFNfixedwechoose|D |+|D |=N andvarythesizeof|D |∼U(0,B−1). Asb varies
train test train max
inpractice,wesampleitlog-uniformlyin[1,1000]. Notethatinthespecialcaseb =1,wetrainFT-PFNforBB-BO.
max
Λ = λ = 1N isoursyntheticconfigurationspacewithλ ∼ U(0,1)d, with|λ | = m ∼ U(0,M)thedimensionality
ii i i
ofourconfigurationspace. Wedetermineb bysamplingabagof|D |elementsfromΛproportionallytoweights
λ train
{w } thatfollowaDirichletdistributionwithlog (α) ∼ U(−4,−1)resultinginheterogeneousbudgetallocations
λ λ∈Λ 10
that vary from breadth-first to depth-first.3 We use the same weights to sample another bag of |D | determining the
test
3WeadoptthesamestrategytogeneratebenchmarktasksforourevaluationofpredictionqualitydescribedinSection5.1.
12In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure5.21i.i.d. samplesoftheFT-PFNprior, i.e., syntheticallygeneratedcollectionsoflearningcurvesforthesametaskusing
differenthyperparameterconfigurations.Intheseexamples,weconsider3hyperparametersthataremappedontothecolorofthecurves,
suchthatrunsusingsimilarhyperparameters,havesimilarlycoloredcurves.Weobservecorrelations,invaryingdegrees,betweencurves
onthesametask,especiallywithsimilarhyperparameterconfigurations.
13In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
numberofextrapolationtargetsforeachλ,whereeachtargetischosenU(b ,b ). Finally,togeneratethecorresponding
λ max
f(λ ,t ),wefirstinstantiatetherandomvariablesthataretask-specificbutdonotdependonλ,i.e.,y ∼Beta(1,2),and
i i 0
thearchitectureandweightsθoftheneuralnetworkp. Finally,wedeterminef(λ ,t )=fˆ(t ,p(λ;θ)).
i i i
Limitations: Withthesemodelingchoicescomesomelimitations. First,FT-PFNistrainedforHPObudgetsB ≤N =
1,000; requirestheperformancemetricf andeachhyperparametervaluetobenormalizedin[0,1]; andsupportsupto
M =10hyperparameters.
A.3.ArchitectureandHyperparameters
FollowingMu¨lleretal.(2022),weuseasequenceTransformer(Vaswanietal.,2017)forFT-PFNandtreateachtuple
(λ,t,f(λ,t))(fortrain)and(λ,t)(fortest)asaseparateposition/token. Wedonotusepositionalencodingsuchthatwe
are permutation invariant. FT-PFN outputs a discretized approximation of the PPD, each output corresponding to the
probabilitydensityofoneoftheequal-sizedbins. Wesetthenumberofbins/outputsto1,000. Forthetransformer,weuse6
layers,anembeddingsizeof512,fourheads,andahiddensizeof1,024,resultinginatotalof14.69Mparameters. Weusea
standardtrainingprocedureforallexperiments,minimizingthecross-entropylossfromEquation1on2.0Msynthethic
datasetsgeneratedasdescribedinSectionA.2,usingtheAdamoptimizer(Kingmaetal.,2015)(learningrate0.0001,batch
size25)withcosineannealing(Loshchilov&Hutter,2017)[LoshchilovandHutter,2017]withalinearwarmupoverthe
first25%epochsofthetraining. Trainingtookroughly8GPUhoursonanRTX2080GPUandthesameFT-PFNisusedin
allexperimentsdescribedinSection5,withoutanyretraining/fine-tuning.
A.4.Acquisitionfunction
Algorithm2describestheacquisitionloopinifBO. The
Algorithm2MFPI-random
two parameters in Equation 4 are chosen randomly, with
ProbabilityImprovement(PI)(Mockusetal.,1978)asthe
Input:configurationspaceΛ,
probabilisticsurrogateM,
baseacquisitionfunction(AF).IneachiterationofifBO
historyofobservationsH,
(L3-L9inAlg.1),Alg.2isinvokedonce. Ineachsuchrun, maximalstepsb
max
therandomhorizonzandthescaledthresholdofimprove-
mentf aresampledonce. Thisprocesscanbeseen Output:λ′ ∈Λ,nextquery
threshold
asinstantiatinganacquisitionfunctionfromaportfolioof
ProcedureMFPI-random(Λ,M,H,b ):
multi-fidelityPIs. ThechoiceofPI,themulti-fidelitycom- max
ponentofextrapolatinghyperparameters,andtherandom
f ←bestscoreseeninH
best
selectionofanacquisitionbehaviourlendsthenamingof z∼U(1,b ) randomhorizon
max
thisacquisitionfunction,MFPI-random. Foreachcandi- τ ∼U(−4,−1) randomthresholdscaling
dateevaluated(λˆ ∈ Λ),theposteriorisinferredgiventhe f threshold =f best·(1−10τ)
surrogate for a hyperparameter λˆ at a total horizon of its D←{}
forallλˆ∈Λdo
current observed length in unit steps (b ) + the sampled
λˆ y′ ←P(M(λˆ, b +z)≤f )
horizon (z). The candidate with the highest obtained PI λˆ threshold
D←D(cid:83) {(λˆ, y′)}
scoreisreturnedasthecandidatesolutiontoquerynextin
endfor
themainAlg.1loop. returnargmax {(λ,y)}
λ∈Λ (λ,y)∈D
B.Benchmarks
Below,weenumeratethesetofbenchmarkswehaveconsidered. Thesebenchmarkcoveravarietyofoptimizationscenarios,
including the model being optimized, the task for which it’s being trained on, and the training metric with which to
optimizehyperparameterswithrespectto. Notably,eachofthesebenchmarksaretabular,meaningthatthesetofpossible
configurationstosamplefromisfinite.
Thischoiceofbenchmarksislargelydictatedbyfollowingtheexistingbenchmarksusedinpriorwork,especiallythetwo
primarybaselineswithwhichwecompareto,DyHPOandDPL.
14In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Thesebenchmarkswereprovidedusingmf-prior-bench4.
• LCBench(Zimmeretal.,2021)[DyHPO,DPL]-Weuseall35tasksavailablewhichrepresentthe7integerandfloat
hyperparametersofdeeplearningmodelsfromAutoPyTorch. Eachtaskrepresentsthe1000possibleconfigurations,
trainedfor52epochsonadatasettakenfromtheAutoMLBenchmark(Gijsbersetal.,2019). Wedropthefirstepoch
assuggestedbytheoriginalauthors.
Table3. The7hyperparametersforallLCBenchtasks.
name type values info
batch size integer [16,512] log
learning rate continuous [0.0001,0.1] log
max dropout continuous [0.0,1.0]
max units integer [64,1024] log
momentum continuous [0.1,0.99]
num layers integer [1,5]
weight decay continuous [1e-05,0.1]
• Taskset(Metzetal.,2020)[DyHPO, DPL]Thissetbenchmarkprovides1000diversetaskonavarietyofdeep
learningmodelsonavarietyofdatasetsandtasks. Wechoosethesame12tasksasusedintheDyHPOexperimentation
whichconsistsofNLPtaskswithpurelynumericalhyperparameters,mostlyexistingonalogscale. Weadditionally
choosea4hyperparametervariantandan8hyperparametervariant,wherethe4hyperparametervariantisasuperset
oftheformer. Thisresultsin24totaltasksthatweusefortheTasksetbenchmark.
Oneexceptionthatneedstobeconsidredwiththissetofbenchmarksisthattheoptimizersmustoptimizeforisthe
model’slog-loss. Thismetrichasnoupperbound,whichcontraststoallotherbenchmarks,wheretheboundsofthe
metricareknowna-priori. WenotethatintheDyHPOevaluationsetup,theyremoveddivergingcurvesasabenchmark
preprocessingstep,essentiallyside-steppingtheissuethattheresponsefunctionforagivenconfigurationmaysreturn
nansorout-of-distributionvalues. Asourmethodrequiresboundedmetrics,wemaketheassumptionthatapractitioner
canprovideareasonableupperboundfortheloglossthatwillbeobserved. Byclamplingtothisupperbound,this
effectivelyshrinkstherangeofvaluesthatourmethodwillobserve. Asweareinasimulatedbenchmarksetup,we
mustsimulatethisa-prioriknowledge. Wetakethemedianvalueofatepoch0,correspondingtothemedianloglossof
randomlyinitializedconfigurationsthathavenotyettakenagradientstep. Anyobservedvaluethatisnanorgreater
willthenbeclampedtothisupperboundbeforebeingfedtotheoptimizer.
Table4. The4hyperparametersearchspaceforTaskset.
name type values info
beta1 continuous [0.0001,1.0] log
beta2 continuous [0.001,1.0] log
epsilon continuous [1e-12,1000.0] log
learning rate continuous [1e-09,10.0] log
• PD1(Wangetal.,2021)[DPL]ThesebenchmarkswereobtainedfromtheoutputgeneratedbyHyperBO (Wangetal.,
2021)usingthedatasetandtrainingsetupof(Gilmeretal.,2021). Wechooseavarietyoftasksincludingthetuningof
largevisionResNet(Zagoruyko&Komodakis,2016)modelsondatasetssuchasCIFAR-10,CIFAR-100(Krizhevsky,
2009)andSVHN(Liao&Carneiro,2022)imageclassificationdatasets,alongwithtrainingaResNet(Heetal.,2016)
on the ImageNet (Russakovsky et al., 2015) image classification dataset. We also include some natural language
processingtasks,notabletransformerstrainontheLM1B(Chelbaetal.,2013)statisticallanguagemodellingdataset,
theXFormer(Lefaudeuxetal.,2022)trainedontheWMT15German-English(Bojaretal.,2015)translationdataset
4https://github.com/automl/mf-prior-bench
15In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Table5. The8hyperparametersearchspaceforTaskset.
name type values info
beta1 continuous [0.0001,1.0] log
beta2 continuous [0.001,1.0] log
epsilon continuous [1e-12,1000.0] log
learning rate continuous [1e-09,10.0] log
exponential decay continuous [9e-07,0.0001] log
l1 continuous [1e-09,10.0] log
l2 continuous [1e-09,10.0] log
linear decay continuous [1e-08,0.0001] log
andalsoatransformertrainedtosequencepredictionforproteinmodellingontheuniref50dataset. Lastly,wealso
includeasimpleCNNtrainedontheMNIST(Deng,2012)andFashion-MNIST(Xiaoetal.,2017)datasets.
Notably,allofthesebenchmarkssharethesame4deeplearninghyperparametersgivenintable6.
Table6. The4hyperparametersforallPD1tasks.
name type values info
lr decay factor continuous [0.01,0.99]
lr initial continuous [1e-05,10.0] log
lr power continuous [0.1,2.0]
opt momentum continuous [1e-05,1.0] log
Eachbenchmarkrangesinthesizeoftheirlearningcurves,dependingonthetask,rangingfrom5to1414. Foreach
task,therearedifferentvariantbasedonapairofdatasetandbatchsize. Intotalweevaluateourmethodonthe16PD1
tasksbelow.
– WideResnet-TunedontheCIFAR10,CIFAR100datasets,eachwithaconstantbatchsizeof256and2048. Also
includedistheSVHNdatasetwithaconstantbatchsize256and1024.
– Resnet-TunedonImageNetwiththreeconstantbatchsizes,256,512,and1024.
– XFormer-Tunedwithabatchsizeof2048ontheLM1Bstatisticallanguagemodellingdataset.
– TransfomerLanguageModelling-TunedontheWMT15German-Englishdatasetwithabatchsizeof64.
– TransformerProteinModelling-Tunedontheuniref50datasetwithabatchsizeof128.
– SimpleCNN-TunedonMNISTandFashion-MNISTwithconstantbatchsizesof256and2048foreachofthem.
C.FurtherAblations
C.1.Effectivenessofmodelingcurvedivergence
AsdetailedinSectionA.1,ourcurveprioriscapabletomodellearningcurvewithdivergingbehavior. Thiscapabilityis
novelcomparedtotherelatedworks(Adriaensenetal.,2023;Kleinetal.,2017;Kadraetal.,2023;Domhanetal.,2015),
whicharerestrictedtomonotoniccurvesonly. InFigure6,weempiricallyshowthatmodelingdivergingcurvesyieldsa
bettersurrogatemodelintermsofbothextrapolationandHPO.
C.2.Pairwisecomparisonoffreeze-thawapproaches
Forafine-grainedassessmentoftheperformanceofifBO,wepresentapairwisecomparisonwiththemainfreeze-thaw
approachesincludingDPLandDyHPO. Thisistovisualizetherelativegainofperformancecomparedtoeachbaseline,
whichmayhavebeenhiddenfromFigure5.2. AsshowninFigure7,ourapproachdominatesconsistentlyDPLandDyHPO
after≈150stepsofHPOrun.
16In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure6.ComparisonoftherelativeranksoftheperformancegainedbymodelingdivergencesinICL-FT-PFN.Theplots,showingthe
averageranksacrossallthebenchmarks(LCBench,PD1,andTaskSet),confirmthemeritsofcapturingdivergingcurvesbothintermsof
thequalityofthepredictions(log-likelihood,left)andHPOperformances(regret,right).
Figure7.Comparisonofrelativerankswhenaggregatedoverallbenchmarkfamilies, showingstronganytimeperformanceinboth
pairwisecomparisonsandalsooverallamongfreeze-thawalgorithms.
C.3.Acquisitionfunctionablationofthebaselines
Inthissection,ourobjectiveistoexploretheimpactofincorporatingrandomizationintotheacquisitionfunctiononthe
baselinemethods(DPLandDyHPO). Forthispurpose,weassesseachbaselineacrossfourdistinctacquisitionfunctions
(Figure8),eachrepresentingadifferentconfigurationofEquation4. Thevariantsinclude: (ours),whereboththehorizon
andthethresholdforimprovementarerandomlyselected,similartotheapproachinifBO;(one-step),wherethehorizon
andthresholdforimprovementarechosenasinDyHPO;(atmax),wheretheselectioncriteriaforthehorizonandthreshold
followthemethodologyinDPL;and(randomhorizon),wherethehorizonisrandomlydetermined,andthethresholdisset
tothebestvalueobserved.
TheresultspresentedinFigure8confirmthattherandomizationtechniquemarkedlyenhancestheperformanceofmethods
capableofextendinglearningcurvesovermanysteps,suchasifBOandDPL. Furthermore,pleasenotethatonlythegreedy
one-stepacquisitionfunctioniseffectiveforDyHPO,giventhatitisspecificallydesignedforone-stepaheadpredictions.
C.4.Comparisonoffreeze-thawapproacheswithMFPI-random
In Figure 9, we present a comparison of all the freeze-thaw approaches—including ours, DPL, and DyHPO—when
employingouracquisitionfunction(MFPI-random). Despiteallmodelsutilizingthesameacquisitionfunction,ourmodel
significantlyoutperformsthoseofDPLandDyHPO. Thisclearlyindicatesthecrucialroleourpriorinachievingthefinal
performance.
17In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure8. Relativeablationoverthehorizonandthresholdparametersofamulti-fidelityAF(Equation4).
Figure9.Comparisonoffreeze-thawapproaches(ifBO,DPL,andDyHPO)whenusingouracquisition(MFPI-random)withtheir
specificsurrogatemodels.Theplotsrepresenttheaverageranksoverallbenchmarks(LCBench,PD1,andTaskset).Thisablationconfirms
thatournovelsurrogate(andnotonlyournovelacquisitionfunction)contributessignificantlytotheHPOperformanceofifBO.
D.Aggregateplotsovertime
Figure10plotsFigure3(bottom)butwiththex-axisascumulativewallclocktimefromtheevaluationcostsreturnedbythe
benchmarkforeachhyperparameterforeveryunitstep. TheoverallconclusionsremainoverourHPObudgetof1000steps.
ifBOisonaverageanytimebetterrankedthanthefreeze-thawHPObaselines.
LCBench PD1 TaskSet Aggregated
Figure10.Comparingrelativerankoverwallclocktime(ins)overdifferentbenchmarkfamiliesandtheaggregatedresultoverall.ifBO
isonaveragebetterthanthebaselines,DyHPOandDPL,exceptfortheTaskSetbenchmarkfamilywhereDPLstartsthebestbutifBO
improveswithmorebudget.Plotshavebeencutoffatacommon105secondsforfaircomparison.
E.Per-taskHPOPlots
InSection3.1,wepresentedHPOresultsoneachofthesethreebenchmarksinacomprehensiveform,averagingrankand
normalizedregretsacrosseverytaskinthesuite. Theseaveragesmayhide/besusceptibletooutliers. Forcompleteness,
Figures11-16provideregretplotsforeverytaskinthebenchmark,averagedacrossthe10seeds. Wefindthatourmethod
consistentlyperformsonpar,orbetterthanthebestpreviousbestHPOmethod,especiallyinlaterstagesofthesearch,
withoutnotableoutliers.
18In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure11. Per-taskHPOresultsonLCBench
19In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure12. Per-taskHPOresultsonLCBench(cont.)
20In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure13. Per-taskHPOresultsonLCBench(cont.)
21In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
22
Figure14. Per-taskHPOresultsonPD1In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure15. Per-taskHPOresultsonTaskset
23In-ContextFreeze-ThawBayesianOptimizationforHyperparameterOptimization
Figure16. Per-taskHPOresultsonTaskset(cont.)
24