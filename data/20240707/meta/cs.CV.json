[
    {
        "title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output",
        "authors": "Pan ZhangXiaoyi DongYuhang ZangYuhang CaoRui QianLin ChenQipeng GuoHaodong DuanBin WangLinke OuyangSongyang ZhangWenwei ZhangYining LiYang GaoPeng SunXinyue ZhangWei LiJingwen LiWenhai WangHang YanConghui HeXingcheng ZhangKai ChenJifeng DaiYu QiaoDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2407.03320v1",
        "entry_id": "http://arxiv.org/abs/2407.03320v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03320v1",
        "summary": "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.",
        "updated": "2024-07-03 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03320v1"
    },
    {
        "title": "BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations",
        "authors": "Zhantao YangRuili FengKeyu YanHuangji WangZhicai WangShangwen ZhuHan ZhangJie XiaoPingyu WuKai ZhuJixuan ChenChen-Wei XieChaojie MaoYue YangHongyang ZhangYu LiuFan Cheng",
        "links": "http://arxiv.org/abs/2407.03314v1",
        "entry_id": "http://arxiv.org/abs/2407.03314v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03314v1",
        "summary": "This paper presents Bag-of-Concept Graph (BACON) to gift models with limited\nlinguistic abilities to taste the privilege of Vision Language Models (VLMs)\nand boost downstream tasks such as detection, visual question answering (VQA),\nand image generation. Since the visual scenes in physical worlds are structured\nwith complex relations between objects, BACON breaks down annotations into\nbasic minimum elements and presents them in a graph structure. Element-wise\nstyle enables easy understanding, and structural composition liberates\ndifficult locating. Careful prompt design births the BACON captions with the\nhelp of public-available VLMs and segmentation methods. In this way, we gather\na dataset with 100K annotated images, which endow VLMs with remarkable\ncapabilities, such as accurately generating BACON, transforming prompts into\nBACON format, envisioning scenarios in the style of BACONr, and dynamically\nmodifying elements within BACON through interactive dialogue and more. Wide\nrepresentative experiments, including detection, VQA, and image generation\ntasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel\nin their current cutting-edge solutions.",
        "updated": "2024-07-03 17:55:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03314v1"
    },
    {
        "title": "HoloHisto: End-to-end Gigapixel WSI Segmentation with 4K Resolution Sequential Tokenization",
        "authors": "Yucheng TangYufan HeVishwesh NathPengfeig GuoRuining DengTianyuan YaoQuan LiuCan CuiMengmeng YinZiyue XuHolger RothDaguang XuHaichun YangYuankai Huo",
        "links": "http://arxiv.org/abs/2407.03307v1",
        "entry_id": "http://arxiv.org/abs/2407.03307v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03307v1",
        "summary": "In digital pathology, the traditional method for deep learning-based image\nsegmentation typically involves a two-stage process: initially segmenting\nhigh-resolution whole slide images (WSI) into smaller patches (e.g., 256x256,\n512x512, 1024x1024) and subsequently reconstructing them to their original\nscale. This method often struggles to capture the complex details and vast\nscope of WSIs. In this paper, we propose the holistic histopathology\n(HoloHisto) segmentation method to achieve end-to-end segmentation on gigapixel\nWSIs, whose maximum resolution is above 80,000$\\times$70,000 pixels. HoloHisto\nfundamentally shifts the paradigm of WSI segmentation to an end-to-end learning\nfashion with 1) a large (4K) resolution base patch for elevated visual\ninformation inclusion and efficient processing, and 2) a novel sequential\ntokenization mechanism to properly model the contextual relationships and\nefficiently model the rich information from the 4K input. To our best\nknowledge, HoloHisto presents the first holistic approach for gigapixel\nresolution WSI segmentation, supporting direct I/O of complete WSI and their\ncorresponding gigapixel masks. Under the HoloHisto platform, we unveil a random\n4K sampler that transcends ultra-high resolution, delivering 31 and 10 times\nmore pixels than standard 2D and 3D patches, respectively, for advancing\ncomputational capabilities. To facilitate efficient 4K resolution dense\nprediction, we leverage sequential tokenization, utilizing a pre-trained image\ntokenizer to group image features into a discrete token grid. To assess the\nperformance, our team curated a new kidney pathology image segmentation (KPIs)\ndataset with WSI-level glomeruli segmentation from whole mouse kidneys. From\nthe results, HoloHisto-4K delivers remarkable performance gains over previous\nstate-of-the-art models.",
        "updated": "2024-07-03 17:49:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03307v1"
    },
    {
        "title": "Smart City Surveillance Unveiling Indian Person Attributes in Real Time",
        "authors": "Shubham KaleShashank SharmaAbhilash Khuntia",
        "links": "http://arxiv.org/abs/2407.03305v1",
        "entry_id": "http://arxiv.org/abs/2407.03305v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03305v1",
        "summary": "This project focuses on creating a smart surveillance system for Indian\ncities that can identify and analyze people's attributes in real time. Using\nadvanced technologies like artificial intelligence and machine learning, the\nsystem can recognize attributes such as upper body color, what the person is\nwearing, accessories they are wearing, headgear, etc., and analyze behavior\nthrough cameras installed around the city.",
        "updated": "2024-07-03 17:47:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03305v1"
    },
    {
        "title": "DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents",
        "authors": "Yilun XuGabriele CorsoTommi JaakkolaArash VahdatKarsten Kreis",
        "links": "http://arxiv.org/abs/2407.03300v1",
        "entry_id": "http://arxiv.org/abs/2407.03300v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03300v1",
        "summary": "Diffusion models (DMs) have revolutionized generative learning. They utilize\na diffusion process to encode data into a simple Gaussian distribution.\nHowever, encoding a complex, potentially multimodal data distribution into a\nsingle continuous Gaussian distribution arguably represents an unnecessarily\nchallenging learning problem. We propose Discrete-Continuous Latent Variable\nDiffusion Models (DisCo-Diff) to simplify this task by introducing\ncomplementary discrete latent variables. We augment DMs with learnable discrete\nlatents, inferred with an encoder, and train DM and encoder end-to-end.\nDisCo-Diff does not rely on pre-trained networks, making the framework\nuniversally applicable. The discrete latents significantly simplify learning\nthe DM's complex noise-to-data mapping by reducing the curvature of the DM's\ngenerative ODE. An additional autoregressive transformer models the\ndistribution of the discrete latents, a simple step because DisCo-Diff requires\nonly few discrete variables with small codebooks. We validate DisCo-Diff on toy\ndata, several image synthesis tasks as well as molecular docking, and find that\nintroducing discrete latents consistently improves model performance. For\nexample, DisCo-Diff achieves state-of-the-art FID scores on class-conditioned\nImageNet-64/128 datasets with ODE sampler.",
        "updated": "2024-07-03 17:42:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03300v1"
    }
]