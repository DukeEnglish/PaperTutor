[
    {
        "title": "Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages",
        "authors": "Max ZuoFrancisco Piedrahita VelezXiaochen LiMichael L. LittmanStephen H. Bach",
        "links": "http://arxiv.org/abs/2407.03321v1",
        "entry_id": "http://arxiv.org/abs/2407.03321v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03321v1",
        "summary": "Many recent works have explored using language models for planning problems.\nOne line of research focuses on translating natural language descriptions of\nplanning tasks into structured planning languages, such as the planning domain\ndefinition language (PDDL). While this approach is promising, accurately\nmeasuring the quality of generated PDDL code continues to pose significant\nchallenges. First, generated PDDL code is typically evaluated using planning\nvalidators that check whether the problem can be solved with a planner. This\nmethod is insufficient because a language model might generate valid PDDL code\nthat does not align with the natural language description of the task. Second,\nexisting evaluation sets often have natural language descriptions of the\nplanning task that closely resemble the ground truth PDDL, reducing the\nchallenge of the task. To bridge this gap, we introduce \\benchmarkName, a\nbenchmark designed to evaluate language models' ability to generate PDDL code\nfrom natural language descriptions of planning tasks. We begin by creating a\nPDDL equivalence algorithm that rigorously evaluates the correctness of PDDL\ncode generated by language models by flexibly comparing it against a ground\ntruth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across\n13 different tasks, with varying levels of difficulty. Finally, we evaluate\nseveral API-access and open-weight language models that reveal this task's\ncomplexity. For example, $87.6\\%$ of the PDDL problem descriptions generated by\nGPT-4o are syntactically parseable, $82.2\\%$ are valid, solve-able problems,\nbut only $35.1\\%$ are semantically correct, highlighting the need for a more\nrigorous benchmark for this problem.",
        "updated": "2024-07-03 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03321v1"
    },
    {
        "title": "Value-Penalized Auxiliary Control from Examples for Learning without Rewards or Demonstrations",
        "authors": "Trevor AblettBryan ChanJayce Haoran WangJonathan Kelly",
        "links": "http://arxiv.org/abs/2407.03311v1",
        "entry_id": "http://arxiv.org/abs/2407.03311v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03311v1",
        "summary": "Learning from examples of success is an appealing approach to reinforcement\nlearning that eliminates many of the disadvantages of using hand-crafted reward\nfunctions or full expert-demonstration trajectories, both of which can be\ndifficult to acquire, biased, or suboptimal. However, learning from examples\nalone dramatically increases the exploration challenge, especially for complex\ntasks. This work introduces value-penalized auxiliary control from examples\n(VPACE); we significantly improve exploration in example-based control by\nadding scheduled auxiliary control and examples of auxiliary tasks.\nFurthermore, we identify a value-calibration problem, where policy value\nestimates can exceed their theoretical limits based on successful data. We\nresolve this problem, which is exacerbated by learning auxiliary tasks, through\nthe addition of an above-success-level value penalty. Across three simulated\nand one real robotic manipulation environment, and 21 different main tasks, we\nshow that our approach substantially improves learning efficiency. Videos,\ncode, and datasets are available at https://papers.starslab.ca/vpace.",
        "updated": "2024-07-03 17:54:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03311v1"
    },
    {
        "title": "Accelerated Proton Resonance Frequency-based Magnetic Resonance Thermometry by Optimized Deep Learning Method",
        "authors": "Sijie XuShenyan ZongChang-Sheng MeiGuofeng ShenYueran ZhaoHe Wang",
        "links": "http://arxiv.org/abs/2407.03308v1",
        "entry_id": "http://arxiv.org/abs/2407.03308v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03308v1",
        "summary": "Proton resonance frequency (PRF) based MR thermometry is essential for\nfocused ultrasound (FUS) thermal ablation therapies. This work aims to enhance\ntemporal resolution in dynamic MR temperature map reconstruction using an\nimproved deep learning method. The training-optimized methods and five\nclassical neural networks were applied on the 2-fold and 4-fold under-sampling\nk-space data to reconstruct the temperature maps. The enhanced training modules\nincluded offline/online data augmentations, knowledge distillation, and the\namplitude-phase decoupling loss function. The heating experiments were\nperformed by a FUS transducer on phantom and ex vivo tissues, respectively.\nThese data were manually under-sampled to imitate acceleration procedures and\ntrained in our method to get the reconstruction model. The additional dozen or\nso testing datasets were separately obtained for evaluating the real-time\nperformance and temperature accuracy. Acceleration factors of 1.9 and 3.7 were\nfound for 2 times and 4 times k-space under-sampling strategies and the\nResUNet-based deep learning reconstruction performed exceptionally well. In\n2-fold acceleration scenario, the RMSE of temperature map patches provided the\nvalues of 0.888 degree centigrade and 1.145 degree centigrade on phantom and ex\nvivo testing datasets. The DICE value of temperature areas enclosed by 43\ndegree centigrade isotherm was 0.809, and the Bland-Altman analysis showed a\nbias of -0.253 degree centigrade with the apart of plus or minus 2.16 degree\ncentigrade. In 4 times under-sampling case, these evaluating values decreased\nby approximately 10%. This study demonstrates that deep learning-based\nreconstruction can significantly enhance the accuracy and efficiency of MR\nthermometry for clinical FUS thermal therapies.",
        "updated": "2024-07-03 17:49:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03308v1"
    },
    {
        "title": "DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents",
        "authors": "Yilun XuGabriele CorsoTommi JaakkolaArash VahdatKarsten Kreis",
        "links": "http://arxiv.org/abs/2407.03300v1",
        "entry_id": "http://arxiv.org/abs/2407.03300v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03300v1",
        "summary": "Diffusion models (DMs) have revolutionized generative learning. They utilize\na diffusion process to encode data into a simple Gaussian distribution.\nHowever, encoding a complex, potentially multimodal data distribution into a\nsingle continuous Gaussian distribution arguably represents an unnecessarily\nchallenging learning problem. We propose Discrete-Continuous Latent Variable\nDiffusion Models (DisCo-Diff) to simplify this task by introducing\ncomplementary discrete latent variables. We augment DMs with learnable discrete\nlatents, inferred with an encoder, and train DM and encoder end-to-end.\nDisCo-Diff does not rely on pre-trained networks, making the framework\nuniversally applicable. The discrete latents significantly simplify learning\nthe DM's complex noise-to-data mapping by reducing the curvature of the DM's\ngenerative ODE. An additional autoregressive transformer models the\ndistribution of the discrete latents, a simple step because DisCo-Diff requires\nonly few discrete variables with small codebooks. We validate DisCo-Diff on toy\ndata, several image synthesis tasks as well as molecular docking, and find that\nintroducing discrete latents consistently improves model performance. For\nexample, DisCo-Diff achieves state-of-the-art FID scores on class-conditioned\nImageNet-64/128 datasets with ODE sampler.",
        "updated": "2024-07-03 17:42:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03300v1"
    },
    {
        "title": "Improved Noise Schedule for Diffusion Training",
        "authors": "Tiankai HangShuyang Gu",
        "links": "http://arxiv.org/abs/2407.03297v1",
        "entry_id": "http://arxiv.org/abs/2407.03297v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03297v1",
        "summary": "Diffusion models have emerged as the de facto choice for generating visual\nsignals. However, training a single model to predict noise across various\nlevels poses significant challenges, necessitating numerous iterations and\nincurring significant computational costs. Various approaches, such as loss\nweighting strategy design and architectural refinements, have been introduced\nto expedite convergence. In this study, we propose a novel approach to design\nthe noise schedule for enhancing the training of diffusion models. Our key\ninsight is that the importance sampling of the logarithm of the Signal-to-Noise\nratio (logSNR), theoretically equivalent to a modified noise schedule, is\nparticularly beneficial for training efficiency when increasing the sample\nfrequency around $\\log \\text{SNR}=0$. We empirically demonstrate the\nsuperiority of our noise schedule over the standard cosine schedule.\nFurthermore, we highlight the advantages of our noise schedule design on the\nImageNet benchmark, showing that the designed schedule consistently benefits\ndifferent prediction targets.",
        "updated": "2024-07-03 17:34:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03297v1"
    }
]