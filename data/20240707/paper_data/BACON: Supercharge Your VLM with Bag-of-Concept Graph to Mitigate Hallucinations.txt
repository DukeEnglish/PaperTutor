BACON: Supercharge Your VLM with Bag-of-Concept Graph
to Mitigate Hallucinations
ZhantaoYang1,2‚ãÜ,RuiliFeng2‚ãÜ‚ãÑ,KeyuYan2,HuangjiWang1,ZhicaiWang2
ShangwenZhu1,HanZhang1,2,JieXiao2,PingyuWu2,KaiZhu2,JixuanChen2
Chen-WeiXie2,ChaojieMao2,YueYang3,HongyangZhang4,YuLiu2,FanCheng1‚Ä†
1ShanghaiJiaoTongUniversity,2Alibabagroup
3UniversityofPennsylvania,4UniversityofWaterloo
https://ztyang23.github.io/bacon-page
Abstract Image BACON Part I: Overall description
Part1.1: Style - color painting with a dynamic
and dramatic composition.
Part1.2: Theme - historical event, specifically
This paper presents Bag-of-Concept Graph a depiction of a revolution or uprising.
Part1.3: Global description of background -
(BACON)togiftmodelswithlimitedlinguistic The painting is chaotic and filled with a
abilities to taste the privilege of Vision Lan- multitude of figures, suggesting a crowded and
tumultuous setting, ‚Ä¶
guage Models (VLMs) and reduce hallucina- Part1.4: Global description of foreground -
The painting is dominated by a central female
tionsinthedownstreamtaskssuchasdetection, figure, who stands out prominently against the
crowded and violent backdrop, ...
visualquestionanswering(VQA),andimage
Part III: Relationships
generation. Since the visual scenes in phys- Crowd Child
i t
a
pc i
n
roa ennl sos ew
t
nabo tte sir otl wd
tn
hs se eea
i
mnnre
to
io ns b btr j aaeu sc gc it ct rsu a, mr pe B hid nA siw
m
tC ri uO ut ch mN tuc eb ro
l
er em e .map Ek el se
ln
ex td msor
a
ee w
n
nla n
d
t-
- is
lying among surroundis
s
l ly eain dsg F ama oll ne gn
W
fig ou misr ae a
n
b1 ove is behind
w s
i
Cnt Ori gs u N.e ct cus
C
at r py a
a
tl l
r
ie
e
oc
f
noe
u
smn
l
wa p pb o irl ts oe hi ms t ti ho pe en tas hl dy i eb
e
lpseu irn
g
oad
n
ftee psr
b
us d
i
bt ria
t
lf
h
in fi csd c -i aun
t
vhlg t ae, il lo aBa c bn a
A
ld t e-- < <
A
-
-
-
h
iC
C
DW M
n
gh
a
a
eo a
e
s
,t
txm n
e
e
c
ha2
g
g
ra om>
io
on
lp
dr
r,>
p
t<
y
y
ii
nl, oP
1
2M e<
gn:
::a f a
:L
F
alr
<
a nt
Ti
oW
fg
v
3 lI
hri
a> >I
en
eo
g: g,
g
,
m
<
w
,O
r
‚Ä¶ SoaC ob
hun
mrj en>oe
a
dw ic
n
st d
' s
dL > rai e,s r< st
m
sM
es
d
a
a
n
ir
n1
e
> ar,
a wis he id
te
F
ti
oa
s
l tl
p
he on
es
rif
t
ii
i
gg
o
hu
n
tr
e
ie os
d
f2 p to Mos i tti h ao e n nle e 2fd t of stands behindi Ms h ao nld 1inB guil Fd li an gg
VLMsandsegmentationmethods. Inthisway, robe and appears to be leading the crowd, ‚Ä¶
- Color: white robe Man3
we gather a dataset with 100K annotated im-
ages,whichendowVLMswithremarkableca- Figure1: BACONrepresentationofanimage,including
pabilities, such as accurately generating BA- overalldescription,objectlist,andrelationships.
CON, transforming prompts into BACON for-
produce those detailed annotations, downstream
mat,envisioningscenariosinthestyleofBA-
CON, and dynamically modifying elements multi-modalityapplicationsmaynoteasilyprocess
withinBACONthroughinteractivedialogueand them‚Äîunderstandinglongandintricatecontextis
more. Wide representative experiments, in- still a privilege of VLMs and is far beyond the
cludingdetection,VQA,andimagegeneration ability of most popular multi-modality methods,
tasks,tellBACONasalifelinetoachieveprevi-
likethoseResNet-basedmodels(Heetal.,2016a;
ousout-of-reachtasksorexcelintheircurrent
Huangetal.,2017;Heetal.,2016b),segmentation
cutting-edgesolutions.
methods (Kirillov et al., 2023; Liu et al., 2023b;
Keetal.,2024),andContrastiveLanguage-Image
Pretrainingmethods(CLIPs)(Radfordetal.,2021;
1 Introduction
Lee et al., 2022; Li et al., 2021). Thus, popular
A single image can tell long stories, weaving multi-modalitymethodsusuallysufferfrommodel
tales as those told across myriad pages, akin hallucinations,wherepredictionsfromthesemeth-
to the masterful strokes seen in ‚ÄúThe Last Sup- odsareinconsistentwithinputprompts. Forexam-
per‚Äù(Pitre,2015)orthevibrantdynamismof‚ÄúLib- ple,off-the-shelfimagegenerativemodelssuchas
ertyLeadingthePeople‚Äù(Chaulagain,2018). Yet DALL-E3andSDXLcannotevencountapplescor-
emerging multi-modality tasks feed on describ- rectlyanddonotunderstandpositionrelationships
ing images with language, which is bound to be between objects, generating inconsistent images
lengthyandconvoluted. ThoughVisionLanguage withhuman-providedcaptions(seeFigure5).
Models (VLMs) (OpenAI, 2023) can now easily Challenges stem from two main issues: the
complexity of long phrases and the difficulty in
‚Ä†Correspondingauthor,‚ãÜEqualcontribution,‚ãÑProject
leader pinpointing specific information. Smaller mod-
1
4202
luJ
3
]VC.sc[
1v41330.7042:viXraBACON Obj1.Apple 1. ‚ë† Point question answering Target region
Description: The apple is of ‚ÄòFuji‚Äô‚Ä¶
bbox:
Color: red.
[320, 0, 440, 70]
Obj2.Apple Tray 1. üåã
Description: The apple tray holds ‚Ä¶
Question
Color: a mix of red and yellow. LLaVA
Obj3.Table 2. Large IOU What color is this table?
Description: The table provides ...
Color: brown. Answer Brown
In an exploding nebula, there is ‚ë° Image generation ‚ë¢ Open-vocabulary scene graph generation
an erupting volcano with a
cyberpunk style building to the
bottl oe mft o rif
g
t hh te
c
v oo rl nc ea rn oo f. tI hn
e
t h sce
e ne BACON
BACON
Apple in
front of DS isi pg ln
ay Board
is a green river flowing with Tray is placed on
SDXL
venom. There is a yellow duck supports
on the river, and to the left of is reaching towards serves as a
the river stands a crowned backdrop for
potato man. On the right bank
of the river, there is a miniature Person Wooden Structure
version of the village.
Figure2: SchematicdiagramofmultipleexemplarydownstreamtaskscanbenefitfromBACON. Specifically,
BACON can(1)enableVLMstocarryoutthepointquestionansweringtaskpreviouslybeyondtheirscope;(2)
assisttext-to-imagegenerativemodelssuchasSDXLincreatingintricateimageswithhigherprecisionasdemanded
byprompts;and(3)executeopen-vocabularyscenegraphgenerationtasksthatwerenotfeasibleforotherVLMs.
els like CLIPs and Segment Anything Models Inconclusion,thecontributionofthisworkcan
(SAMs)(Kirillovetal.,2023),constrainedbytheir be summarized as follows. 1) We introduce BA-
trainingdataandresources,grasponlyword-level CON, a rich and digestible caption method for
or phase-level language and struggle to train ex- boostingmulti-modalitytasks;2)Wecollectahigh-
tensiveattentionmechanismsfordetailedcontext quality BACON-Dataset using human labor and
analysis. Thus,acriticalquestionemerges: advanced VLM models; the dataset and BACON-
Captionertrainedonitwillbesoonopen-source;3)
ShallthemeticulousbreakdownofVLMannota-
Theproposedmethodboostswideandrepresenta-
tionsintoclearlydistinguishableandcomprehensi-
tivedownstreamtasksremarkably,demonstrating
bleelementssignificantlymitigatemodelhallucina-
significantpotentialinpromotingfutureresearch.
tionsandelevatetheperformanceofdownstream
tasks?
2 Bag-of-ConceptGraph
Luckily,theanswerisyes. Thispaperproposes
BACON‚ÄîaneatandefficientapproachforVLMs Inthissection,wefirstexplainthedesignofBag-of-
to mitigate hallucinations and boost downstream ConceptGraphinSection2.1,andthenintroduce
tasks. BACON breaks down annotations into ba- howtogeta BACONrepresentationfromanimage
sic elements (like objects, style of picture, rela- inSection2.2.
tionships, etc.) within an image, which can be
2.1 Designof BACON
easilyunderstoodbyvariousmethods. Afterthat,
In-Context Learning (ICL) (Brown et al., 2020) BACONaimstotackletheissuesfacedbysmaller
techniquesaredeployedtoorganizetheannotation downstreammodelsinunderstandinglongphrases
intoagraph,withnodesrepresentingfundamental andinlocatingspecificpiecesofinformation. For
visualelementsandedgesillustratingtheirconnec- the former, BACON deconstructs the image an-
tions. Illustrated in Figure 1, BACON composes notations into basic elements, ensuring that even
ofthreekeyparts: 1)anoveralldescriptioncap- smallerdownstreammodelscanfullycomprehend
turing the image‚Äôs overall style, theme, and key them. Subsequently, BACON employs a specific
features; 2) a detailed object list with labels and graphstructuretoamalgamatetheseelements,en-
descriptionsforeveryitemintheimage;3)there- suringeachelementappearsinadesignatedspot,
lationshipsbetweentheseobjects. Thestructureof allowingsmallerdownstreammodelstoqueryand
BACON allowsmodelstoflexiblyextractanduse retrievethemeasily. Specifically,therealworldcan
thedesiredpartstocomprehendcomplexinforma- oftenbeexplicitlyrepresentedusingascenegraph
tionandaccomplishtaskspreviouslybeyondtheir composedofobjectsandtheirrelationships(Miller,
reach(seedetailedexamplesasFigure2). 1995;Doddingtonetal.,2004;Krishnaetal.,2017;
2
renoitpaC
‚ùÑ‚ë†BACON Object list mmaann
Man Man Man Sofa SUV ‚Ä¶
1 2 3
Object description:
He is wearing a hat, ‚Ä¶ ‚ë°Grounding
DINO
The caption of this image is
He is wearing a hat, ‚Ä¶,
does it correct?
‚ë£CLIP üåã
‚ë¢LLaVA
GroundingD+ IC NL OIP+CLIP&LLaVA
(a) Detailed method of enhancing BACON (b) Ablation study
Figure3: (a)Detailedmethodforgraphgrounding. Themethodcontainsfoursteps: 1)ExtractingBACONfrom
imagesusingGPT-4VorBACON-Captioner;2)GettingcandidateregionsusingGroundingDINOgiventhename
of the object; 3) Using LLaVA to discard blatant incorrect regions; 4) Select the region whose image feature
matchesthetextfeatureofobjectdescriptionthemostbyCLIP.(b)Ablationstudyofmethodin(a),exploringthe
improvementofintroducingCLIPandLLaVA,wheretheexperimentisconductedonBACONbenchmark.
Luetal.,2016;Xuetal.,2017;Johnsonetal.,2015, basicelementsandthencombiningthemaccording
2018). Inspiredbythisinsight,weadoptthestruc- to a specific structure. Our approach is divided
tureofagraphtodeconstructandreassemblebasic intotwokeyparts. Firstly,wedesignareversible
elements. Whenselectingthesebasicelements,we transformationmethodthatconverts BACONintoa
have deliberately considered those that may ben- stringformatunderstandablebyVLMs. Illustrative
efit downstream tasks (listed inAppendix A.2.1). examplesareprovidedinAppendixA.2.2. Then,
Finally,wehaveobtained BACONasshowninFig- we employ the ICL technique to teach VLMs to
ure1. outputfollowingthestringformatwedesigned. We
Surprisingly, we discover that such a design findafewsimplifiedexamplessufficientandexe-
also significantly increases the number of cor- cutetheICLlearningprocessinoneconversational
rectobjectsincludedincaptions(seethehigher exchange. Inpractice,weuseGPT-4VastheVLM
recallscoreofBACONcomparedtoGPT-4VinSec- andprovidethefinalinstructioninAppendixA.2.2.
tion4.2.2). Webelievethisisduetothestructured Based on this approach, we develop the BACON
objectlistemphasizingthedistinctionofseparated dataset,detailedinSection3.
entities,urgingVLMstoseparatecomplexobjects BACON-Captioner. Obtaining BACON by GPT-
intomoredetailedcomponents. 4V is reliable but expensive. Therefore, we opt
to fine-tune a 13B LLaVA model on the BACON
2.2 Formulationof BACON
dataset to serve as a specialized captioner, with
Given an image I, we aim to induce a structural trainingdetailsinSectionA.4.1. Ourgoalforthis
representationG = (D,O,R,B),whereD isthe captioneristomatchGPT-4V‚Äôscapabilityingen-
textual description, O is the list of objects in the erating BACON. Wegather BACON outputsfrom
image, with R denotes their relationships and B BACON-Captioner and GPT-4V on a test set, an-
as their bounding box positions. In practice, we alyzing the categories each detects and the root
optimize the construction of G in two stages: (1) nounsandverbsoftheiroutputs. Theresultingdis-
GraphConstruction(Section2.3),whichutilizes tributions,displayedinFigure9),showahighde-
VLMs to generate the graph elements (D,O,R) greeofsimilaritybetweenthem. Furthermore,the
fromtheimage,and(2)GraphGrounding(Sec- precisionandrecallscorecalculatedbymanualan-
tion2.4)alignsthegraphelementswiththebound- notation(themetricsaredetailedasSection4.2.2)
ingboxes(B)extractedbyagroundingmodel. show BACON-Captionerachieve91%ofprecision
score and 90% of recall score of that of GPT-4V.
2.3 Graphconstruction Consequently,BACON-Captionerisaviablealter-
Deconstructing annotations. BACON assists nativetoGPT-4Vforproducing BACONandhelps
downstream models in understanding complex usextend BACONdataset.
textsbydecomposingtheannotationsofVLMsinto BeyondgeneratingBaconfromimages,wesur-
3
UOImprisinglyfindthatthetrainedcaptionerisalsoadept Table 1: Comparison of open-vocabulary object
atperformingadditionalusefultaskswithoutfine-
detection among BACON, Grounding DINO, open-
vocabularyobjectdetectionmodels,andgroundingcap-
tuning, such as interactively modifying the items
tion models on BACON benchmark. GD represents
ofBACON,transformingpromptsintoBACONfor-
Grounding DINO. We have calculated error bars for
mat,envisioningscenariosinthestyleofBACON.
modelsthatexhibitrandomness.
2.4 Graphgrounding Method AP50(‚Üë) Recall(‚Üë) mIOU(‚Üë)
OV-DQUO 4.7% 10.7% 66.5%
Spaceinformationisimportantforrepresentingan
DE-VIT 19.3% 23.8% 76.8%
image. While VLMs have some level of ground- GD 33.1¬±2.5% 20.2¬±0.1% 75.7¬±0.1%
ing,specializedmodelslikeGroundingDINO(Liu Next-Chat 29.1¬±0.1% 7.7¬±0.1% 67.1¬±0.0%
etal.,2023b)significantlyoutperformthem. There- Kosmos-2 34.2¬±4.8% 13.3¬±2.4% 76.1¬±0.4%
GLaMM 34.3% 19.8% 79.6%
fore, we did not pursue the option of VLMs in
BACON 37.7¬±0.9% 35.9¬±0.7% 79.9¬±0.1%
Section2.3deliveringlocationdetails,butrather,
we intend to leverage dedicated grounding mod-
els for this purpose. Fortunately, BACON‚Äôs struc-
ten, their tasks involve simple verifications like
tureprovidesalistofobjectsrequiredbyground-
checkingcolorsorrelationships. Furthermore,the
ingmodels,enablingthecombinationofadvanced
comprehensivenatureofBACONgenerallyspares
VLMsfordetailingandtop-tiergroundingmodels
annotatorsfromgeneratingnewinformation,allow-
forpreciselocalizationwithin BACON.
ing them to focus on editing or deleting existing
Although Grounding DINO provides accurate
content‚Äîtasksthataretypicallylesscomplex.
object positions, names alone fall short of distin-
guishing objects within the same category. Here, Testbenchmark. Despitetheexistenceofgraph
BACON‚Äôsdetailednodedescriptionscomeintoplay, datasetslikeVisual-Genome(VG)(Krishnaetal.,
allowing for precise region identification when 2017) and PSG (Yang et al., 2022), there is still
usedinconjunctionwithCLIP.Moreover,ween- nodatasetthatconcurrentlyoffersopen-vocabulary
hancegroundingaccuracybyfirstapplyingLLaVA capabilities,detailedobjectattributes,andacom-
to filter out incorrect bounding boxes before pro- prehensiveoveralldescription. Toaddressthisdefi-
ceedingwiththeCLIPstep. Weconductedanab- ciency,wedevelopthe BACONbenchmark.
lation study on the BACON benchmark (with de-
DespiteGPT-4V‚Äôsadvancedcapabilities,itmay
tails in Section 3), and the findings, presented in
miss objects in images. For the test benchmark,
Figure3(b),confirmthebenefitsofincorporating
aimingfortheutmostaccuracy,weutilizearigor-
CLIPandLLaVAintoourapproach. SeeFigure3
ous method that relies heavily on human annota-
(a)foranillustrationofthisprocess.
torsincontrasttothetrainingset. Thismethod,as
3 BACON dataset showninFigure11,startsbyusingSAM(Kirillov
et al., 2023) to separate each object in an image.
In this section, we introduce the BACON dataset Next,itappliesVLMstodescribeallobjects,list
proposedinthispaper. BACONdatasetiscomposed their attributes, and explain how these objects in-
oftwoparts,thetrainingsetandthetestbenchmark, teractwitheachother. Finally,giventheobjectlist,
whichsharedifferentcollectionmethods. VLMprovidestheoveralldescriptionoftheimage.
Trainingset. Forthetrainingset,weemploythe We leave the details in the appendix. Thanks to
method as detailed in Section 2.3 to collect 110k SAM‚Äôs excellent performance, it is rare to miss
BACON-image pairs. Then, we engage in a thor- objects in the image. The challenge comes with
ough manual re-annotation process to eliminate VLM‚Äôsdifficultyinrecognizingobjectsgiventheir
ambiguities and incorrectness (see details in Ap- masked images, which is still hard even for ad-
pendixA.3). Theendproductisarefineddataset vancedVLMslikeGPT-4V(OpenAI,2023). This
of100khigh-qualityimage-BACONpairs. Notably, difficulty leads to a lot of manual annotation to
BACON‚Äôsstructuregreatlystreamlinestheannota- correctmistakes. Thehighcostofmanualcorrec-
tionprocess. BybreakingdownGPT-4V‚Äôsoutput tionsisthereasonwhythismethodisnotapplied
into manageable sections, we can assign specific tocreateourtrainingset. Finally,weannotateda
segments to different annotators, which concen- testbenchmarkcontainingaround3kimages,27k
tratestheireffortsandenhancesproductivity. Of- objects,and148krelationships.
440 45
PointQA v7w random Table 3: Quantitative comparison of VQA task
between BACON and multiple VLM-based baselines,
*
* wheretheinputimageoftheQAmodelisreplacedby
itscaptiontoevaluatetheperformanceofthecaptioner.
Themetricistheaccuracyofansweringquestions(‚Üë).
Method NLVR2 OK-VQA VQAv1 VQAv2
LLaVA 56.3% 30.9% 50.0% 64.1%
ShareGPT-4V 57.5% 31.4% 50.7% 65.4%
0 0 Qwen-VL-max 56.8% 29.0% 46.0% 65.6%
GLa M MKos moN s-e 2xt-CB hA atCON GLa M MKos moN s-e 2xt-CB hA atCON BACON 59.1% 32.1% 52.6% 66.4%
(a)PointQAtask (b)PointingQAtask
Figure4: Quantitativecomparisonon(a)PointQA
4.1.1 Open-vocabularyobjectdetection
and(b)PointingQAbetweenBACONandbaselines.
GroundingDINOstrugglestodifferentiatebetween
Table 2: Comparison on open-vocabulary scene multipleobjectsofthesamecategoryduringopen-
graphgenerationtaskbetweenBACONandmultiple
vocabulary object detection (OVD) task (Exam-
baselinesonVGdatasetandBACONbenchmark. The
ples are shown in Appendix A.4.2). However,
numberofcorrectpredictionsisusedasthemetric(‚Üë).
thanks to the structural features of BACON, the
Method Visual-Genome BACONdataset approach described in Section 2.4 can be lever-
PSGFormer 1.45K 0.21K agedtoaddressthisissue,significantlyenhancing
PSGTR 0.33K 0.07K GroundingDINO‚ÄôsperformanceinOVDtask. We
IMP 3.16K 1.28K
comparetheperformancewithgroundingcaption
Gps-net 3.13K 1.16K
Motifs 3.05K 1.25K models,includingGLaMM(Rasheedetal.,2024),
VCTree 3.11K 1.24K
Kosmos-2 (Peng et al., 2023), Next-Chat (Zhang
BACON 11.45K 1.44K
etal.,2023),andtwooftheSOTAdedicatedOVD
models,OV-DQUO(Wangetal.,2024a)andDE-
4 Experiments
VIT (Zhang et al., 2024) as baselines on BACON
benchmark,withAP50,recallandmIOUusedas
In this section, we show that BACON can be ap-
pliedtohelpmultipledownstreamtasksbyflexibly
themetrics. ResultsinTable1showthat BACON
enables grounding models to perform OVD task
utilizing desired parts of information and the re-
andoutperformallevaluatedbaselines.
markablecapabilitiesof BACON-Captioner. First,
in Section 4.1, we show BACON can help a wide
4.1.2 Pointquestionanswering
rangeofdownstreamtasksbyflexiblyutilizingde-
PointQuestionAnswering(PointQA)(Manietal.,
siredpartsof BACON. Then,forcompleteness,we
2020)aimstoanswerregion-relatedquestionsac-
discuss some special cases in Section 4.2 where
cording to an input image. Inspired by the set-
BACONcanbedirectlyutilizedwithoutanyspecial
ting of zero-shot visual question answering (ZS-
operations. Next, in Section 4.3, we show some
VQA)(Guoetal.,2023),wereplacetheinputof
impressive capabilities of BACON-Captioner. Fi-
theQAmodelfromimagestotheircaptionsto
nally,inSection4.4,weshowBACONcanbeeasily
evaluatetheperformanceofcaptioners. Intuitively,
generalizedtovideodense-captioningtask.
if a fixed QA model can correctly answer more
questionsusingagivencaption,itsuggeststhatthe
4.1 Downstreamtasksbenefitingfrom BACON
captioncontainsmoreaccurateinformation,indi-
The adaptable nature of BACON‚Äôs structure en- catingasuperiorcaptioningperformance.
hances models‚Äô comprehension of complex text This task is extremely difficult for VLMs like
and empowers them to undertake tasks previ- LLaVAbecausetheycan‚Äôtmatchpartsofthecap-
ouslybeyondtheirreach. Weconductevaluations tion to image regions. But BACON aids LLaVA
acrossfivedownstreamtasks,includingobjectde- in finding relevant descriptions for a given area
tection (Section 4.1.1), point question answering (we‚ÄôvedetailedthemethodinAppendixA.4.3). We
(PointQA) (Section 4.1.2), Pointing question an- usegroundingcaptionmodels,includingGLaMM,
swering(PointingQA)(Section4.1.3),scenegraph Kosmos-2, andNext-Chat, asbaselines. Besides,
generation(SGG)(Section4.1.4),andimagegen- we apply LLaVA as the fixed QA model and uti-
eration(Section4.1.5). lize the QA accuracy as the metric. Results on
5
ycaruccA ycaruccATable 4: Accuracy in depicting objects (A o) and (seedetailsinAppendixA.4.5)andBACONbench-
relationships (A ) in images generated from text
r mark. To calculate the open-vocabulary metrics,
prompts,asevaluatedbyhuman. WecompareSDXL
weapplyCLIPsimilaritytodecidewhetherapre-
enhancedbyBACONwithSDXLandDALL-E3.
diction is correct. The number of correct predic-
tionsisusedastheevaluationmetric(asdetailed
Method A (‚Üë) A (‚Üë)
o r inAppendixA.4.5). TheresultsshowninTable2
SDXL 59.20¬±3.96% 41.45¬±3.46% demonstratethatBACONcanperformtheOV-SGG
DALL-E3 90.05¬±4.17% 71.60¬±3.39%
task,whichisout-of-reachforVLMsandsubstan-
BACON+SDXL 95.20¬±1.13% 76.65¬±0.92%
tiallyoutperformspurpose-builtSGGmodels.
LookTwice-QAdataset(Manietal.,2020)shown 4.1.5 Imagegeneration
in Figure 4 (a) indicate BACON can help VLMs
Advancedtext-to-imagemodelslikeSDXL(Podell
associate descriptions with regions to perform
et al., 2023) struggle to follow complex text
PointQAtasksandsurpassallbaselines.
prompts and accurately generate images. Fortu-
4.1.3 Pointingquestionanswering nately, BACON allows generative models to sep-
aratelygeneratethebackgroundanddifferentob-
PointingQuestionAnswering(PointingQA)(Zhu
jectsandthenmergethem,thussplittingthechal-
etal.,2016)asksmodelstochoosethecorrectre-
lengeintoseveraleasyparts(Detailedmethodcan
gionfromvariousoptionsbasedonagivenques-
befoundinAppendixA.4.6). Weshowtwoexam-
tion, using only the input image. Following the
ples in Figure 5 with more instances available in
settings in Section 4.1.2, we replace the input
AppendixA.4.6. Toassessthecorrelationbetween
fromimagestocorrespondingcaptionstoeval-
thetextpromptsandthegeneratedimages,wecon-
uate the performance of captioners. BACON can
duct a user study involving 10 human annotators
helpLLaVAwiththetask,whichisoriginallybe-
and 100 samples. They are tasked with counting
yond its capabilities (details in Appendix A.4.4).
thesignificantobjectsandrelationshipsmentioned
Again, we chose the grounding caption models,
inthetextpromptsandthoseaccuratelygenerated
includingGLaMM,Kosmos-2,andNext-Chat,as
intheimages. Utilizingthemanualannotation,we
baselinesandusedtheaccuracyofselectingregions
computetheaccuracymetricsforobjects(A )and
asthemetric. ResultsonVisual-7Wdataset(Zhu o
relationships(A ),withthedetailsinTable4. The
etal.,2016)inFigure4(b)showthatBACONcan r
results demonstrate that BACON significantly en-
help LLaVA find the correct region according to
hances SDXL‚Äôs ability to understand and follow
the requirement and significantly outperform the
complex prompts. Remarkably, it enables SDXL
baselines,whoseperformancesareevenworsethan
tosurpassDALL-E3infaithfullyreproducingthe
randomlyselecting.
detailsspecifiedinthetextdescriptions.
4.1.4 Scenegraphgeneration
The scene graph generation (SGG) task focuses
4.2 Tasksdirectlyusing BACON
on identifying (subject-predicate-object) triplets Inthissection,forcompleteness,wediscussdown-
in images. Traditional SGG models are trained stream tasks where BACON can be used directly
to classify items in the triplet within a finite set withoutspecialoperations.
ofcategories, whichmaynotbesuitableforreal-
4.2.1 Visualquestionanswering
worldapplications. However,theopen-vocabulary
(OV-SGG) setting, closer to the real situation, is FollowingthesettingofZS-VQAandSection4.1.2,
extremelyhardforthoseSGGmodels. Fortunately, we replace the input of VQA from images to
BACON-CaptionercanperformtheOV-SGGtask their captions to evaluate the performance of
without any fine-tuning as BACON provides the the captioner. We compare BACON-Captioner
relationships part. We compare the performance with multiple VLM-based captioners including
ofOV-SGGbetweenBACON-Captionerwithmul- LLaVA (Liu et al., 2023a), Qwen-VL-max (Bai
tiple specialized SGG approaches including Mo- etal.,2023),andthedistillationversiononcaption
tifs (Zellers et al., 2018), GPS-Net (Lin et al., taskofGPT-4V,ShareGPT-4V(Chenetal.,2023a)
2020), VCTree (Tang et al., 2019), PSGTR, PS- onmultiplecommonlyusedVQAdetasetsinclud-
GFormer (Yang et al., 2022), and IMP (Xu et al., ing NLVR2 (Suhr et al., 2018), VQAv1 (Antol
2017) on two datasets, VG (Krishna et al., 2017) etal.,2015),VQAv2(Goyaletal.,2017),andOK-
6Caption BACON + SDXL DALL-E 3 SDXL
At the center of the screen is a man wearing a black suit and a red tie, riding on the
back of a dinosaur. The dinosaur stands on the green grass, opening its mouth to
reveal its sharp teeth. In the bottom right corner of the screen, a photographer wearing
a black coat squats, focusing on capturing this unique scene with a camera. The mid
shot features two trees with sparse leaves. In the background, there are two colorful
hot air balloons flying in the sky. One is black, located in the upper right corner of the
screen, and the other is colorful, located slightly higher in the middle of the screen
The full moon hangs high on the left side of the sky, with stars dotted around it. The
background on the left side of the picture is a magnificent castle, standing on a distant
hill. In the bottom left corner of the photo, two people are standing hand in hand on a
rock by the river. Men are on the left, women are on the right, and the river winds
from their feet towards the distance. On the right side of the picture is a male angel
holding a trumpet and spreading his wings. Above the angel, there is a glowing flying
saucer suspended in the air.
Figure5: ComparativeexamplesofimagegenerationrevealthatBACONenhancesadvancedgenerativemodels
likeSDXL.SDXLandDALL-E3strugglewithcomplextextandfailtoproducecorrespondingimages.Remarkably,
BACONnotonlyelevatesSDXL‚Äôsimagequalitybutalsomarkedlyboostsitscomprehensionofintricateinstructions,
enablingittosurpassDALL-E3intermsofaccuratelygeneratingimagesaligningwithtextualdirectives.
Edit item:
Boy standing next to Girl 1 2. .R Ce hm ano gv ee g L iri lg th ot rb oe ba om t Boy standing next to Robot
is looking towards 3.Add hot air balloon is looking towards
located behind is aboveCloud located behind is aboveCloud
is above is above
Building is above is scattered in Building is above is scattered in
cuts across
is floating in
Light beam sky Hot air balloon sky
Generated image from
Original image BACON Generated image from BACON Modified BACON
Modified BACON
Figure6: AnexampleofinteractivelymodifyingBACONusingBACON-Captioner.
Table5: Precision&recallscorecalculatedbyman-
* ShareGPT4v
ualannotationbetweenBACON-Captionerandother
BACON
* GPT-4V VLM-basedcaptioners.
Method Precision Recall
* LLaVA
LLaVA 36.35¬±1.48% 59.15¬±4.74%
* Qwen-max ShareGPT-4V 23.20¬±3.82% 55.30¬±2.12%
Others
Qwen-VL-max 35.20¬±5.94% 57.50¬±1.98%
Figure7: Winrateofpairwisecomparisonsbetween GTP4v 21.5¬±0.71% 70.55¬±13.36%
BACON-CaptionerandotherVLM-basedcaptioners. BACON 56.23¬±4.23% 82.83¬±8.33%
VQA (Marino et al., 2019). For evaluation mea- tions(seedetailsinAppendixA.4.7)andthenask
surement,wefollowtheofficialevaluationmetrics annotatorstocountthenumberofobjectsintheim-
tomeasuretheperformance. AsshowninTable3, ageandthenumberofcorrectpredictionsintheex-
BACONoutperformsallbaselines. tractednouns. Then,theprecisionandrecallscore
can be calculated. In the user preference study,
4.2.2 Precision&recallanduserstudy annotatorsselecttheirpreferredannotationinpair-
wisecomparisons,ensuringstructuralaspectsare
ToassesstheperformanceoftheBACON-Captioner,
neutralized to prevent any biases. The outcomes,
weconductauserpreferenceuserstudyandexam-
asshowninFigure7andTable5,indicateBACON
inetheprecisionandrecallscorebymanualannota-
outperforms all comparisons in general, notably
tion. WecompareBACON-Captionerwithvarious
predicting more correct objects than multiple
VLM-basedcaptioners,includingLLaVA,Qwen-
popularVLMsevencontainingGPT-4V.
VL-max,ShareGPT-4V,andGPT-4V,byanalyzing
captionsproducedforarandomlysampledsetof
4.3 Additionalcapabilitiesofcaptioner
200imagesfromtheMSCOCOdataset(Linetal.,
2014). Weengage10humanannotatorsformanual BeyondobtainingBACONfromimages,thetrained
labeling. For the precision and recall scores, we captionerisalsoadeptatperformingadditionaluse-
firstextractallimportantnounsexistinginthecap- ful tasks, including interactively editing BACON,
7
renoitpaC
‚ùÑlNew item compared to last frame lChanged item compared to last frame lDeleted item compared to last frame
curtain curtain floor curtain candelabra(1) man(1) RReelalatitoinosnhisph iinpfo i gnrfaop hg:raph:
man1
man2
man1 E D pox e ssa tucm rr eip .p l te i o1 n: :m Tha en m(1 a) n 1 is covereC da wte itg ho ar y pa 1 tt: e rnL ediv sin hg ir t‚Ä¶. a nd heC sa tate ng do s r wy it h2 : a haF to are ng dr ao u rn ed laxed cu hart na g i n behind place to
n
et ah re lec ftandelabra(1)
Color: black vest and pants, white shoes. man(1) man(2)
candelabra1 candelabra1 candelabra2 E D sex e vesa rcm ar lip p cl ate i
n
o2 dn l:
e
:c
s
Ta thhn aed tc e aal ra n edb ler il ta a
.
( b ‚Ä¶1 ra )
I
ts t paC rn oa d vt s ie
d
tg eo so t ar h y e
w
l1 ae r: ft
m
I on gfa ltn ohi wem ma tot ae tn
h
, ef e ia mt mur ei dC n ig aa tat ee b ag a ro s er e ay .a n2 d: steF mo r te hg ar to hu on ld ds reflect stand on
Color: gold stem and base, white candles with yellow flames. floor candelabra(2)
floor
floor
floor curtain candelabra(1) man(1) candelabra(2) man(2) RReelalatitoinosnhisph iinpfo i gnrfaop hg:raph:
Example 1: man(1) Category 1: Living Category 2: Foreground curtain near candelabra(1)
Frame1 Frame2
D p
C E
D
s
Ceo xe
o
e
ovs es
sal
ltu
o
orc
cm
arr
rr
re li
i: :p
.p
p
c
b
lH gat
tel
oi
i
nai o os
l2 dc
d
n
n
l:kr
e
i
s
: :cg
v
s t
T
T
e
e
ah
t
ms
hh ht
n t
ae eh
da
a
tam
c
en
nan
ald
da rd
a
n
en
p
bdbs 1
la
aeu
r
i
ln
sti p
a
a
.s
et
p
b(s
‚Ä¶
,c o
1,
r
wo r
aw
)v It
h
e the
s
i
d
t
tr
pi
a
et
Ce ret nd oh
c
a ds
v
aew
h t
s
i
ne
db oi
t
dt
g
er oeh li
ss o
e
m t.a
sar
h
y
e
wp o
w
a f
l
i1
ae
tt h ht
r:
fie
t
m
s yIr
o
nn
e
h
f
ga
le a lltnd ot oh.
i
wwes
m
h
ma
fti lor
t
a
ae
t
n
t
m‚Ä¶
h
,
e
e.
f
s
e
a
i
.a
mn td
mu
rh
ei
dC
ne igs
aa
t tata
ee
n
b
agd
a
ros
s
e
r
ew
a
y
.ai t n2h
d
:a
s
th eFa mt
o
a
r te
hn
g
ad
r
t
oa
hu
or ne
ld
dla sx ed
fm loa on
r(1)hang b sse
tt
ah
a
ni nn ddd
n
n exe ta tgr oest cu ar ne
d
m eto la awn ba( r2 r ad)
(2)pair with
Frame4 Frame3 floor curtain candelabra(1) man(1) candelabra(2) man(2) RReelaltaiotinoshnisph inipfo ignrfaop hg:raph:
Example 1: man(1) Category 1: Living Category 2: Foreground curtain candelabra(1)
Description: The man1 is covered with a patterned shirt‚Ä¶. and he stands with a hat and a relaxed
candem laa bn r1 am 1an2 candelabra2 man1 man2
candelabra2
p C E
D
feo exo es tsalt u o acmr rrre ei: p.
p
ibH l ntel ii a bos 2c
l
nr a:ki
:
cg mv kMh e as st at hnh
n
oaa (
2
en 2n sdd h)
.
aps sau np shtp s oo , rr w tt e h hd i at C iet rh a se ah t nb eo dr gei s o im s. r w dyo rhf e1ih st :ei
s
s
e
sh dh La i ir it nv. t , iH
a
nr e ge dd ac rl ta kie s s,p ua e ind
t
d wh w ii C ts hh h a i ata te et
v
w gi en soa th i r s ai yts nc d2ole a : wf tt . ha iFr tm eo r p. e ag nr to su ‚Ä¶n .d
His
man(1)
stand on
man(2)
Color: dark suit, white dress shirt, black tie, black shoes. floor reflect candelabra(2)
floor
floor
floor curtain candelabra(1) man(1) candelabra(2) man(2) RReelalatitoinosnhisph iinpfo i gnrfaop hg:raph:
Example 1: man(1) Category 1: Living Category 2: Foreground curtain near candelabra(1)
D r Cele oas lx oc er rdi
:
p p bt o li aso ctun kr : ve eT . sHh te e
a
m nc dla a n ps1 p
a
e ni ds
t s
c h
,
o i wsv he h ir a te etd i
s
nw
h
oi ht eih s
s
.a l e
w
p f hta iat tt ere m r sn h.e ird
t
,s rh ei drt t‚Ä¶ ie,. aa nn dd whe
h
is tt ea wnd as
i
sw tci oth
a
ta
.
hat and a
man(1)
stand next to
man(2)
pair
with
Example 2: candelabra(1) Category 1: Inanimate Category 2: Foreground
Description: The candelabra stands on the stage, holding several candles that are lit. ‚Ä¶ It is made
of metal with a design that curves outward at the top
Color: gold stem and base, white candles with yellow flames. floor candelabra(2)
Figure8: AnexampleofBACONonvideocaptioning,whichincludesthreecomponents: anoveralldescription,
anobjectlist,andtheirrelationships,eachdynamicallyevolvingovertime. Withrespecttoapriorframe,updates
arecolor-coded: newelementsingreen,removedinred,alteredingold,andpersistentonesinblack. BACONthus
adeptlycapturesthetemporalchangesandsalientdetailsofeachvideoframe,whileitsstructurednaturepotentially
aidsindownstreammodelcomprehension.
transformingordinarypromptsintoBACONformat, Table6:ComparisonofplantaskbetweenBACONand
LayoutGPT(Fengetal.,2024)onbothMSCOCO(Lin
andplanningpositionsofobjectsin BACON. First,
etal.,2014)andBACONbenchmark.
asshowninFigure6,BACON-Captionerallowsus
tointeractivelyedittheBACON,andthusaffectthe Dataset Method Precision Recall mIOU
generationofimages.
LayoutGPT 70.12% 39.74% 4.07%
MSCOCO
Subsequentlyandremarkably,withoutanyfine- BACON 71.18% 41.84% 6.83%
tuning, BACON-Captioner can transform an ordi- Bacon LayoutGPT 50.79% 29.16% 9.12%
narypromptintoa BACON. Forshortprompts,it Dataset BACON 51.66% 47.06% 18.39%
canadddetailstocreateaBACON,andforlonger,
itcanorganizethegiveninformationintoaBACON.
tions for videos with the help of additional tech-
Moreover,the BACON-Captionercanarrangethe
niques that address the temporal dimension of
positionsofobjectsofthe BACON. Weprovideex-
videocontent. Theprincipalchallengeofadapting
amplesofbothexpandingandorganizingprompts
BACON for videos is distinguishing between ele-
in Appendix A.4.8. We quantitatively compare
mentsthatchangeovertimeandthosethatremain
BACON-Captioner‚Äôs capability of planning with
constant. To resolve this, we employ a tracking
LayoutGPT(Fengetal.,2024)ontheMSCOCO
method(Chengetal.,2023)toidentifyandmatch
dataset(Linetal.,2014)and BACONdatasets,us-
the same object across different frames. Further-
ingthemIOU,precisionandrecallmetrics(Feng
more, we use T5 (Raffel et al., 2020) as the text
etal.,2024)asdetailedinAppendixA.4.8. Results
encoder to compare descriptions of the same ob-
inTable6indicatethatBACON-Captionerperforms
ject or scene segment across frames. Parts of the
betterthanLayoutGPTonbothevaluateddatasets.
textwithhighsimilarityscoresareconsideredsta-
ble, while those with low similarity are deemed
4.4 BACONonvideocaptioning
to have changed. An example shown in Figure 8
While BACON is primarily developed for image demonstratesthisapproachefficientlycapturesthe
data, it can be extended to create structured cap- continuityandevolutionofvideocontent,provid-
8ingacoherentanddescriptivenarration. Additional JamesBetker,GabrielGoh,LiJing,TimBrooks,Jian-
examplesareavailableinAppendixA.4.9. fengWang,LinjieLi,LongOuyang,JuntangZhuang,
JoyceLee,YufeiGuo,etal.2023. Improvingimage
generationwithbettercaptions. ComputerScience.
5 Conclusion
https://cdn.openai.com/papers/dall-e-3.pdf.
Inthispaper,weintroduce BACON toaddressthe
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
difficultyofdownstreammodelsinunderstanding Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
complextextsbybreakingdowncomplexannota- Neelakantan,PranavShyam,GirishSastry,Amanda
Askell,etal.2020. Languagemodelsarefew-shot
tionsintobasicminimumelementsandpresenting
learners. Adv.NeuralInform.Process.Syst.
them in a graph structure. We propose a novel
methodforobtainingthestructuralrepresentation YashodaChaulagain.2018. Visualpositionandjuxta-
from images and constructing a dataset of 100k position: an analytical study of liberty leading the
peopleandmoon-womancutsthecircle. Tribhuvan
scale based on it. A captioner is trained on this
UniversityJournal.
dataset with multiple remarkably useful capabili-
ties. Extensive experiments demonstrate that our Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-
methodcaneffectivelyassistdownstreammodels ghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. 2023a. ShareGPT4V: Improving large multi-
inaccomplishingtaskstheypreviouslycouldnot
modalmodelswithbettercaptions. arXivpreprint
achieveorexcelintheircurrentcutting-edgesolu-
arXiv:2311.12793.
tions.
XiChen,LianghuaHuang,YuLiu,YujunShen,Deli
6 Limitations Zhao,andHengshuangZhao.2024. Anydoor: Zero-
shotobject-levelimagecustomization. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVi-
This paper introduces a method designed to as-
sionandPatternRecognition,pages6593‚Äì6602.
sist smaller models in comprehending complex
textsandtofacilitatetheirintegrationwithVLMs, ZheChen,JiannanWu,WenhaiWang,WeijieSu,Guo
achievingremarkableperformancesacrossmultiple Chen, Sen Xing, Zhong Muyan, Qinglong Zhang,
benchmarks. However,despitetheseachievements, Xizhou Zhu, Lewei Lu, et al. 2023b. InternVL:
Scaling up vision foundation models and aligning
ourapproachstillfacescertainlimitations. Firstly,
for generic visual-linguistic tasks. arXiv preprint
given the absence of a fully automated method
arXiv:2312.14238.
that guarantees reliable quality, our data collec-
tion process still necessitates human annotation HoKeiCheng,SeoungWugOh,BrianPrice,Alexan-
derSchwing,andJoon-YoungLee.2023. Tracking
involvement. Secondly, due to cost and resource
anythingwithdecoupledvideosegmentation. InInt.
constraints,thecaptioner‚Äôslocalizationcapabilities
Conf.Comput.Vis.
remaininsufficient,necessitatingthecombination
ofagroundingmodeltoobtainhigh-qualityposi- GeorgeRDoddington,AlexisMitchell,MarkAPrzy-
bocki,LanceARamshaw,StephanieMStrassel,and
tionalinformation.
RalphMWeischedel.2004. Theautomaticcontent
extraction(ace)program-tasks,data,andevaluation.
InLrec.
References
WeixiFeng,WanrongZhu,Tsu-juiFu,VarunJampani,
PeterAnderson,XiaodongHe,ChrisBuehler,Damien Arjun Akula, Xuehai He, Sugato Basu, Xin Eric
Teney,MarkJohnson,StephenGould,andLeiZhang. Wang, and William Yang Wang. 2024. Layoutgpt:
2018. Bottom-upandtop-downattentionforimage Compositionalvisualplanningandgenerationwith
captioningandvisualquestionanswering. InIEEE largelanguagemodels. Adv.NeuralInform.Process.
Conf.Comput.Vis.PatternRecog. Syst.
StanislawAntol,AishwaryaAgrawal,JiasenLu,Mar- AndreaFrome,GregSCorrado,JonShlens,SamyBen-
garetMitchell,DhruvBatra,CLawrenceZitnick,and gio, Jeff Dean, Marc‚ÄôAurelio Ranzato, and Tomas
DeviParikh.2015. VQA:Visualquestionanswering. Mikolov.2013. Devise: Adeepvisual-semanticem-
InInt.Conf.Comput.Vis. beddingmodel. Adv.NeuralInform.Process.Syst.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, HananGani,ShariqFarooqBhat,MuzammalNaseer,
SinanTan, PengWang, JunyangLin, ChangZhou, Salman Khan, and Peter Wonka. 2024. LLM
andJingrenZhou.2023. Qwen-VL:Afrontierlarge blueprint: Enabling text-to-image generation with
vision-languagemodelwithversatileabilities. arXiv complexanddetailedprompts. Int.Conf.Learn.Rep-
preprintarXiv:2308.12966. resent.
9YashGoyal,TejasKhot,DouglasSummers-Stay,Dhruv JanghyeonLee,JongsukKim,HyoungukShon,Bum-
Batra,andDeviParikh.2017. MakingtheVinVQA sooKim,SeungHwanKim,HonglakLee,andJunmo
matter: Elevatingtheroleofimageunderstandingin Kim. 2022. Uniclip: Unified framework for con-
visualquestionanswering. InIEEEConf.Comput. trastive language-image pre-training. Adv. Neural
Vis.PatternRecog. Inform.Process.Syst.
Jiaxian Guo, Junnan Li, Dongxu Li, Anthony YangguangLi,FengLiang,LichenZhao,YufengCui,
Meng Huat Tiong, Boyang Li, Dacheng Tao, and Wanli Ouyang, Jing Shao, Fengwei Yu, and Jun-
StevenHoi.2023. Fromimagestotextualprompts: jie Yan. 2021. Supervision exists everywhere: A
Zero-shot visual question answering with frozen dataefficientcontrastivelanguage-imagepre-training
largelanguagemodels. InIEEEConf.Comput.Vis. paradigm. arXivpreprintarXiv:2110.05208.
PatternRecog.
Tsung-YiLin,MichaelMaire,SergeBelongie,James
KaimingHe,XiangyuZhang,ShaoqingRen,andJian Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,
Sun.2016a. Deepresiduallearningforimagerecog- and C Lawrence Zitnick. 2014. Microsoft COCO:
nition. InIEEEConf.Comput.Vis.PatternRecog. Commonobjectsincontext. InEur.Conf.Comput.
Vis.
KaimingHe,XiangyuZhang,ShaoqingRen,andJian
Sun.2016b. Identitymappingsindeepresidualnet- XinLin,ChangxingDing,JinquanZeng,andDacheng
works. InEur.Conf.Comput.Vis. Tao. 2020. GPS-Net: Graph property sensing net-
workforscenegraphgeneration. InIEEEConf.Com-
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan put.Vis.PatternRecog.
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
and Weizhu Chen. 2021. Lora: Low-rank adap-
Lee.2023a. Visualinstructiontuning. Adv.Neural
tation of large language models. arXiv preprint
Inform.Process.Syst.
arXiv:2106.09685.
ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,Hao
Gao Huang, Zhuang Liu, Laurens Van Der Maaten,
Zhang,JieYang,ChunyuanLi,JianweiYang,Hang
andKilianQWeinberger.2017. Denselyconnected
Su, Jun Zhu, et al. 2023b. Grounding Dino: Mar-
convolutionalnetworks. InIEEEConf.Comput.Vis.
ryingDinowithgroundedpre-trainingforopen-set
PatternRecog.
objectdetection. arXivpreprintarXiv:2303.05499.
Justin Johnson, Agrim Gupta, and Li Fei-Fei. 2018.
Cewu Lu, Ranjay Krishna, Michael Bernstein, and
Imagegenerationfromscenegraphs. InIEEEConf.
LiFei-Fei.2016. Visualrelationshipdetectionwith
Comput.Vis.PatternRecog.
languagepriors. InEur.Conf.Comput.Vis.
JustinJohnson,RanjayKrishna,MichaelStark,Li-Jia
ArjunMani,NoblineYoo,WillHinthorn,andOlgaRus-
Li,DavidShamma,MichaelBernstein,andLiFei-
sakovsky.2020. Pointandask: Incorporatingpoint-
Fei. 2015. Image retrieval using scene graphs. In
ingintovisualquestionanswering. arXivpreprint
IEEEConf.Comput.Vis.PatternRecog.
arXiv:2011.13681.
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
JunhuaMao,JonathanHuang,AlexanderToshev,Oana
andTamaraBerg.2014. Referitgame: Referringto
Camburu,AlanLYuille,andKevinMurphy.2016.
objectsinphotographsofnaturalscenes. InProceed-
Generationandcomprehensionofunambiguousob-
ingsofthe2014conferenceonempiricalmethodsin
jectdescriptions. InIEEEConf.Comput.Vis.Pattern
naturallanguageprocessing(EMNLP),pages787‚Äì
Recog.
798.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi,
LeiKe,MingqiaoYe,MartinDanelljan,Yu-WingTai,
andRoozbehMottaghi.2019. OK-VQA:Avisual
Chi-Keung Tang, Fisher Yu, et al. 2024. Segment
question answering benchmark requiring external
anythinginhighquality. Adv.NeuralInform.Process.
knowledge. In IEEE Conf. Comput. Vis. Pattern
Syst.
Recog.
AlexanderKirillov,EricMintun,NikhilaRavi,Hanzi ChenlinMeng,YutongHe,YangSong,JiamingSong,
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, JiajunWu,Jun-YanZhu,andStefanoErmon.2021.
Spencer Whitehead, Alexander C Berg, Wan-Yen SDEdit: Guided image synthesis and editing with
Lo, etal.2023. Segmentanything. arXivpreprint stochastic differential equations. arXiv preprint
arXiv:2304.02643. arXiv:2108.01073.
RanjayKrishna,YukeZhu,OliverGroth,JustinJohn- GeorgeAMiller.1995. Wordnet: alexicaldatabasefor
son, Kenji Hata, Joshua Kravitz, Stephanie Chen, english. CommunicationsoftheACM.
YannisKalantidis,Li-JiaLi,DavidAShamma,etal.
2017. Visualgenome: Connectinglanguageandvi- OpenAI. 2023. GPT-4V(ision) technical work and
sion using crowdsourced dense image annotations. authors. https://openai.com/contributions/
Int.J.Comput.Vis. gpt-4v/.
10ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shao- JunjieWang,BinChen,BinKang,YulinLi,YiChiChen,
hanHuang,ShumingMa,QixiangYe,andFuruWei. WeizhiXian,andHuifengChang.2024a. Ov-dquo:
2023. Groundingmultimodallargelanguagemodels Open-vocabularydetrwithdenoisingtextquerytrain-
totheworld. InInt.Conf.Learn.Represent. ing and open-world unknown objects supervision.
arXivpreprintarXiv:2405.17913.
BrantPitre.2015. Jesusandthelastsupper.
ZhenyuWang,EnzeXie,AoxueLi,ZhongdaoWang,
Dustin Podell, Zion English, Kyle Lacey, Andreas XihuiLiu,andZhenguoLi.2024b. Divideandcon-
Blattmann,TimDockhorn,JonasM√ºller,JoePenna, quer: Language models can plan and self-correct
andRobinRombach.2023. SDXL:Improvinglatent for compositional text-to-image generation. arXiv
diffusionmodelsforhigh-resolutionimagesynthesis. preprintarXiv:2401.15688.
arXivpreprintarXiv:2307.01952.
DanfeiXu,YukeZhu,ChristopherBChoy,andLiFei-
Fei.2017. Scenegraphgenerationbyiterativemes-
AlecRadford,JongWookKim,ChrisHallacy,Aditya
sage passing. In IEEE Conf. Comput. Vis. Pattern
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
Recog.,pages5410‚Äì5419.
try, Amanda Askell, Pamela Mishkin, Jack Clark,
etal.2021. Learningtransferablevisualmodelsfrom
JingkangYang,YiZheAng,ZujinGuo,KaiyangZhou,
natural language supervision. In Int. Conf. Mach.
WayneZhang,andZiweiLiu.2022. Panopticscene
Learn.
graphgeneration. InEur.Conf.Comput.Vis.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine LingYang,ZhaochenYu,ChenlinMeng,MinkaiXu,
Lee,SharanNarang,MichaelMatena,YanqiZhou, StefanoErmon,andBinCui.2024. Masteringtext-to-
WeiLi,andPeterJLiu.2020. Exploringthelimits imagediffusion: Recaptioning,planning,andgener-
oftransferlearningwithaunifiedtext-to-texttrans- atingwithmultimodalLLM. Int.Conf.Mach.Learn.
former. J.Mach.Learn.Res.
RowanZellers,MarkYatskar,SamThomson,andYejin
HanoonaRasheed,MuhammadMaaz,SahalShaji,Ab- Choi.2018. Neuralmotifs:Scenegraphparsingwith
delrahmanShaker,SalmanKhan,HishamCholakkal, globalcontext. InIEEEConf.Comput.Vis.Pattern
Rao M Anwer, Erix Xing, Ming-Hsuan Yang, and Recog.,pages5831‚Äì5840.
FahadSKhan.2024. Glamm: Pixelgroundinglarge
multimodalmodel. IEEEConf.Comput.Vis.Pattern Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng,
Recog. Wei Ji, and Tat-Seng Chua. 2023. Next-chat: An
LLM for chat, detection and segmentation. arXiv
RobinRombach,AndreasBlattmann,DominikLorenz, preprintarXiv:2311.04498.
Patrick Esser, and Bj√∂rn Ommer. 2022. High-
XinyuZhang,YutingWang,andAbdeslamBoularias.
resolutionimagesynthesiswithlatentdiffusionmod-
2024. Detecteverythingwithfewexamples. arXiv
els. InIEEEConf.Comput.Vis.PatternRecog.
preprintarXiv:2309.12969.
VishnuSarukkai,LindenLi,ArdenMa,ChristopherR√©,
YukeZhu,OliverGroth,MichaelBernstein,andLiFei-
andKayvonFatahalian.2024. Collagediffusion. In
Fei. 2016. Visual7w: Grounded question answer-
IEEEWinterConf.Appl.Comput.Vis.,pages4208‚Äì
inginimages. InIEEEConf.Comput.Vis.Pattern
4217.
Recog.
Piyush Sharma, Nan Ding, Sebastian Goodman, and
RaduSoricut.2018. Conceptualcaptions: Acleaned,
hypernymed,imagealt-textdatasetforautomaticim-
agecaptioning. InProceedingsofthe56thAnnual
Meeting of the Association for Computational Lin-
guistics.
AlaneSuhr, StephanieZhou, AllyZhang, IrisZhang,
Huajun Bai, and Yoav Artzi. 2018. A corpus for
reasoningaboutnaturallanguagegroundedinpho-
tographs. arXivpreprintarXiv:1811.00491.
KaihuaTang,HanwangZhang,BaoyuanWu,Wenhan
Luo, andWeiLiu.2019. Learningtocomposedy-
namic tree structures for visual contexts. In IEEE
Conf.Comput.Vis.PatternRecog.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and
DumitruErhan.2015. Showandtell: Aneuralim-
agecaptiongenerator. InIEEEConf.Comput.Vis.
PatternRecog.
11A Appendix A.2 Supplementaryof BACON
In this section, we will delve into the details of
Theappendixisorganizedintofivedistinctsections.
the BACON method as a supplement to the main
Itbeginsbypresentingacomprehensiveoverview
text. We will organize this section following the
ofrelatedworkinAppendixA.1. Thisisfollowed
structureofthemaintext,includingadditionalin-
by an in-depth discussion of the methodology in
sightsintothedesignphilosophybehindBACONin
Appendix A.2. The next section, Appendix A.3,
AppendixA.2.1,aswellasshowcasingcomplete
focusesontheaspectsofhumanannotationwithin
examples. InAppendixA.2.2,wewillpresentthe
the dataset collection process for BACON. Ap-
detailsandexamplesofthegraphconstruction.
pendixA.4thenprovidesathoroughexplanationof
theexperimentalsetup,detailingthemetricsused A.2.1 Designof BACON
andthemethodologyforimplementing BACONin As mentioned in the main text, when breaking
solvingdownstreamtasks. Moreover,thissection complexannotationsdownintobasicelements,we
includesadditionalexperimentalfindings. specificallyconsideredinformationthatcouldbe
beneficialfordownstreamtasksasthesebasicele-
A.1 Relatedworks ments. Wethenlistedthesebasicelementsalong-
sidethepotentialdomainsthatmightbenefitfrom
ImagedescriptionTransformingimagesintotex-
them,asillustratedinSection6. Besides,wepro-
tual descriptions is a quintessential task, with
videseveralcompleteexamplesof BACON inFig-
many downstream applications relying on accu-
ures13and14,whichareomittedinthemainpaper
ratetext-basedrepresentationsofvisualdata(Rad-
tosavespace.
ford et al., 2021; Antol et al., 2015; Podell et al.,
2023;Betkeretal.,2023;Liuetal.,2023b;Frome A.2.2 Graphconstruction
et al., 2013). Traditional methods for image cap- In the main paper, we introduced the method of
tioning usually suffer the problem of lacking de- constructinggraphsusingVLMsbutomittedsome
tails (Anderson et al., 2018; Mao et al., 2016; details due to space constraints. The method en-
Kazemzadeh et al., 2014; Sharma et al., 2018; compasses two key steps. The first is to design a
Vinyalsetal.,2015). Recently,VLM-basedmodels string format along with a reversible conversion
including GPT-4V (OpenAI, 2023), LLaVA (Liu methodthatcantransform BACON intothestring
etal.,2023a),ShareGPT-4V(Chenetal.,2023a), formatunderstandablebyVLMs,whilealsoensur-
InternVL(Chenetal.,2023b),Qwen-VL(Baietal., ing that this string format can be converted back
2023)andsoon,candescribeimagesinverydetail to BACONwithoutloss. Thesecondstepinvolves
and output very long captions, which have been utilizingtheICLtechniquetoteachtheVLMsto
proved helpful in the downstream fields, such as describeimagesusingourdesignedstringformat.
imagegeneration(Betkeretal.,2023). Wewillnowintroducethesetwostepsindetail.
DecomposingdescriptionHowever,overlycom- BACON in string format We translate the graph
plex texts pose significant challenges to the com- structure of BACON into a string format inter-
prehensionabilitiesofdownstreammodels. Take pretable by VLMs (we use GPT-4V in practice),
thedomainoftext-to-imagegenerationasanexam- demarcatingvarioussectionswithuniquesymbols,
ple,whererepresentativemodelslikeSDXL(Rom- whereanexampleisshowninFigure15. Specifi-
bach et al., 2022) struggle to accurately generate cally, we label main titles with %% and subtitles
images based on detailed textual guidance. This with &&. When listing objects, we enclose ex-
hasspurredaplethoraofeffortsaimedatdecom- tradetailslikecategory, description, andcolorin
posing and reassembling texts to enhance mod- brackets (). Each detail is separated by a semi-
els‚Äô fidelity to intricate instructions (Yang et al., colon ";". We mark the name of an object with
2024;Wangetal.,2024b;Ganietal.,2024). There <>. During the description of relationships, we
are also works trying to describe the image in a use<>forshowingobjectsand[]forthepredicate.
graphstructure,includinggroundingcaptionmod- Additionally,weuse<>tohighlightimportantob-
els(Pengetal.,2023;Zhangetal.,2023;Rasheed jectswithintheobject,servingmultiplepurposes.
et al., 2024) and scene graph generation (SGG) Onesuchfunctionistopost-processtheGPT-4V
models (Xu et al., 2017; Zellers et al., 2018; Lin outputresults. Thisinvolvesremovingforeground
etal.,2020;Tangetal.,2019;Yangetal.,2022). information from the background description by
12Partofinformation Potentialareasofbenefit
Style Artclassification,Styletransfer,Aestheticevaluations
Theme Recommendationsystems,Thematicanalysis
Background Scenereconstruction,Virtualreality
Foreground Focusanalysis,Subjecttracking,Interactiveapplications
Objectname Objectdetection,Labeling,Databaseorganization,Language-basedimageretrieval
Objectcategory Semanticsegmentation,Hierarchicalclassification
Lifestatus BehaviorPrediction,human-computerinteraction,Livingidentification
Depthcues Depthestimation,Layeredgeneration,Compositing
Objectdescription Fine-grainedrecognition,regioncaptioning,regionmulti-modalunderstanding,Imagegeneration
Objectcolor Color-basedretrieval,Fashionindustryapplications,Imagecoloring
Relationships Sceneunderstanding,Actionrecognition
Table7: Listofdifferentimportantpartsofinformationandthepotentialfieldsthatmaybenefitfromthem. Depth
cuesrepresenttheforeground-backgroundinformationofobjects.
deleting sentences where the foreground objects Therefore,GPT-4Vsometimesgetsdetailswrong,
appear,orsimilarly,eliminatingbackgroundinfor- like missing special symbols, even when we use
mationfromtheforegrounddescription. Byusing generalexamples. That‚Äôswhyweneedtousespe-
thesespecialsymbolstoseparatedifferentsections, cific examples to make sure GPT-4V really gets
we can effortlessly organize the string format of the structure. Take numbering items in the same
BACON intoadictionaryusingregularexpressions. category,forinstance,weintroduceaspecificex-
Thismakesiteasyfordownstreamtaskstoextract ampleinlines14-15. Withoutthisexample,GPT-
variouspiecesofinformationwithoutanyhassle. 4V tends to forget to number the items correctly,
InstructionforGPT-4Vtoobtain BACONThen, eventhoughwe‚Äôvealreadyrequireditinlines13-
wediscussthemethodofapplyingICLtechnique 14. Also, we noticed GPT-4V does well with the
to obtain BACON via VLMs (we use GPT-4V in formatofthefirstsectionbutoftenslipsupwiththe
practice) in detail. As described in Section 2.3, secondandthirdparts,whichcomplicatesturning
wediscoverthatGPT-4Vdoesnotrequireexhaus- thedataintoadictionary. Byprovidingonlyone
tive examples to master the desired format. We clearexampleforthesesections,GPT-4Vismuch
simplyneedtoinsertafewimportantexamplesin more likely to produce the right structure. The
the right spots within the instruction, which then ICLtechniquehashelpedensurethatnearlyallof
playakeyrole. Youcanseethefinalinstructionin the110kdataentrieswe‚Äôvegatheredareformatted
Figure16,wherewe‚Äôvehighlightedthecriticalex- correctly and can be translated into a dictionary
amplesinorange. Amongtheexamplesused,some format.
arespecificandothersaremoregeneral. We‚Äôveob-
servedthatforstraightforwardstructuralelements,
A.3 Supplementaryof BACONdataset
generalexamplesarequiteeffective. Forinstance, Inthissection,webeginbyexaminingtheoutput
justafewlines,like‚Äôlines3-4‚Äôor‚Äôlines8-9‚Äô,can distribution of BACON-Captioner, as detailed in
adequatelyindicatetheuseofspecialsymbolsin Appendix A.3.1. Subsequently, we introduce the
a section, eliminating the need for a full-fledged detailsofhumanannotation,coveringboththecol-
example. Inlines21-22,wepresentageneralex- lection of training data and the establishment of
amplethatclearlydelineatesthestructureofeach testbenchmarks.
object, which significantly minimizes GPT-4V‚Äôs
errors. Tokeepobjectdetailseasytograsp,weuse A.3.1 Outputdistributionof
ageneralexamplelines23-24,whicharesufficient BACON-Captioner
for producing simple sentences. Regarding lines We show the analysis of the root words and cate-
27-28,ageneralexampleisenoughtoinstructGPT- goriesdetectedintheoutputsofBACON-Captioner,
4Vonthebasicpatternfordepictingrelationships. whichcanbeseeninFigure9. Theresultclearly
Lastly, a general example set out in line 29 aids showsthattheoutputpatternofBACON-Captioner
inpreventingGPT-4Vfromrepeatedlygenerating isveryclosetothatofGPT-4V.Notably,there‚Äôsa
two-wayrelationshippairs. 100%overlapinthetop100frequentnouns,99%
However,ourhighdemandsonthecontentand forverbs,and97%forcategoriesdetectedbyGPT-
structure are extremely hard even for GPT-4V. 4VandBACON-captioner. Thissimilarityconfirms
13Root nouns ‚Äì BACON-Captioner Detected categories ‚Äì BACON-Captioner Root verbs ‚Äì BACON-Captioner
Root nouns ‚Äì GPT-4V Detected categories ‚Äì GPT-4V Root verbs ‚Äì GPT-4V
Figure9: AnalyzingtherootwordsanddetectedcategoriesinBACON‚Äôsoutputontestset: Wecomparetheroot
wordsanddetectedcategoriesgeneratedbyBACON-CaptionerandGPT-4V,withcertainsectionsmagnifiedfor
clearervisualization. TheresultsrevealthattheoutputdistributionofBACONcloselyresemblesthatofGPT-4V.
that BACON-Captioner can effectively take over Table8: Completehyper-parametersoftrainingBA-
fromGPT-4Vingenerating BACON fromimages
CON-Captioner.
andextendour BACONdataset.
Hyper-parameter Value Hyper-parameter Value
A.3.2 Humanannotation
Lorarank 128 Learningrate 2√ó10‚àí4
Aswe‚ÄôvementionedinSections2.3and3,creating Epochs 3 Warmupratio 0.03
Batchsize 16 Maxlength 2048
theBACONdataset‚Äôstrainingandtestsetsinvolves
humanannotations.
Collecting training data. In the process of col-
toseparatelydetermineifeachsentenceiscorrect.
lectingtrainingdata, BACON significantlyreduces
They don‚Äôt have to add objects as Segment any-
the workload of annotation. It breaks down the
thing(SAM)(Kirillovetal.,2023)inthismethod
complexdescriptionsintobasicelements,formany
hasensuredthattherewillbenoomissions. Atthe
ofwhichannotatorssimplyneedtomakeastraight-
laststage,theyhavetodetermineifarelationshipis
forward judgment of right or wrong, a task that
correctandaddanimportantrelationshipomitted
is remarkably simple. For large pieces of infor-
byVLMs.
mationsuchasbackgroundorforegrounddescrip-
tions,annotatorsareaskedtoseparatelydetermine
ifeachsentenceiscorrectaccordingtotheimage.
Besides, the annotators are asked to add objects A.4 Supplementaryofexperiments
missed by GPT-4V. In this process, the structure
we designed for objects can help annotators sim- In this section, we provide supplementary expla-
plifythedescriptionprocess. Theyonlyneedtofill nationsfortheexperimentaldetailsomittedinthe
inthecorrespondinginformationaccordingtothe main text (Section 4), including the training de-
structure. tailsofBACON-Captioner,thespecificmannerin
Collectingtestbenchmark. Inthemethodofcol- which BACON aids downstream tasks, the exact
lecting the test set of BACON, annotators are in- calculation methods for metrics, and any special
volved in four parts. For the first part, they are processingappliedtothedatasets. Wewillorganize
expectedtocorrecttheresultreturnedbyVLMsto thissectionfollowingthestructureofthemaintext
recognizetheobjectnamegiventhemaskedimage. (Section4)tofacilitatereadersinquicklylocating
Inthesecondandthirdparts,annotatorsareasked thecorrespondingsectionforeachexperiment.
14Bounding box Question
bbox: [320, 0, 440, 70] What color is this table?
Obj1.Apple(1). Bbox: green
Category: Living. Foreground.
Description: The apple is of ‚ÄòFuji‚Äô‚Ä¶
BACON Color: red. üåã
Obj2.Apple Tray(4). Bbox: blue
Large
IO
C
D
Ca
e
ost loe cg
r ri
:o
p
ar ty
i
mo:
n
iI xn
:
a oTn
fh
i rem
e
a
da
p
t ae
p
n.
l
deF o
t yr
er ae
ly
lg
o
r
h
wo
o
.u ln dd
s
.
‚Ä¶
LLaVA
U Obj3.Table(2). Bbox: yellow
Category: Inanimate. Background.
Description: The table provides ...
Color: brown. Answer Brown
(a)PointQAtask
Candidates: Question
Which one contains a cut apple in it? Text
Question Encoder
Obj1.Apple(1).
BACON Description: The Obj1. Apple(1) Score: 0.614
apple is of ‚ÄòFuji‚Äô Obj2. Apple(2) Score: 0.915
with a red skin‚Ä¶
Obj2.Apple(2). ‚Ä¶(Other objects with score)
Description: The
apple is of ‚ÄòFuji‚Äô
with a red skin. It is Answer
cut‚Ä¶ Candidate 4
argmax
weight
Obj1. Apple(1) IOU: 0.293 ‚Ä¶(Other Candidates with weights)
ùëì
Obj2. Apple(2) IOU: 0.347 Candidate 4 Weight: 0. 7772
(1) (2) (3) (4)
(b)Po‚Ä¶ in(O tit nhe gr o Qbj Aects
t
w ai sth
k
IOUs) ùëì=‚àë!"#‚àà ‚àë!" !# "%& #‚Äô ‚àà( !ùëÜ "ùëê #ùëú %&‚Äôùëü (ùëí ùêº! ùëÇ" ùëà#‚àó !"ùêº #ùëÇùëà!"# ‚Ä¶(Other Candidates with weights)
Figure 10: An illustrative diagram depicting how BACON aids downstream models in executing PointQA
andPointingQAtasks. In(a)thePointQAtask,alistofobjectsandtheircorrespondingdescriptionsprovidedby
BACONareutilized. Thedescriptionoftheobjectwiththelargeoverlapwiththetargetregionisusedtorepresent
thedescriptionofthatregion;thisregionaldescriptionisthenfedintoaQAmodeltoanswerquestionsrelatedto
theregion. In(b)thePointingQAtask,objectdescriptionsprovidedby BACON areusedtocalculatesimilarity
scoreswiththeinputquestion,generatingscoresforeachobject. Basedontheoverlapbetweenobjectpositionsand
candidateregions,aweightedsumofallobjectscoresiscomputedtoassignscorestocandidateregions;theregion
withthehighestscoreisthenselectedastheprediction.
A.4.1 Detailsoftraining BACON-Captioner labels. For example, as illustrated in Figure 12,
GroundingDINOoutputsambiguouslabelssuch
BACON-Captioner is fine-tuned on a pre-trained
as‚Äúone‚Äù,"fourmenoneonegray","another".
13B LLaVA model using Low-Rank Adaptation
(LoRA)(Huetal.,2021)technique,wherethenum- The second issue, which is more severe, is
berofparametersofLoRAadapterisaround0.5B. GroundingDINO‚Äôsdifficultyindistinguishingbe-
WefollowthedefaultsettingsofLLaVA-lorafine- tween different individuals of the same category.
tuning, where the detailed hyper-parameters are AsshowninFigure12,althoughGroundingDINO
in Table 8. The captioner is trained on NVIDIA identifiesfourpeople,itischallengingtodetermine
A100GPUs,takingaround100GPUhours. We‚Äôve whichindividualisrepresentedbywhichbounding
donethisinhopesofmakingiteasierforothersto box with vague labels like "four men one". Note
reproduce BACON-Captioner. thatthe BACON benchmarkservesassuchacom-
plexbenchmark,incorporatingnumerousscenarios
A.4.2 Open-vocabularyobjectdetection thatmorecloselymirrorreal-lifesituationswhereit
isnecessarytodistinguishdifferentobjectswithin
Although Grounding DINO can carry out open-
thesameorsimilarcategories.
vocabularyobjectdetectiontask,itstillfacessome
issues. Thereareprimarilytwoproblems. First,the BenefitingfromBACON‚Äôspowerfulcapabilities,
corestepofGroundingDINOrequiresanounasin- GroundingDINOcanovercomethesetwoissues
puttolocatethepositionofthatnounintheimage. withtheaidofBACON. Forthefirstproblem, BA-
Moreover, it introduces methods to extract a se- CON inherently possesses the ability to identify
riesofnounsfromasentencedescription,enabling important objects in an image, allowing Ground-
ittoperformobjectdetectiontasks. However,the ingDINOtoreceivealistofobjectsfromBACON,
methodofextractingnounscansometimeserr,lead- resulting in a more accurate and comprehensive
ing to Grounding DINO producing some bizarre listofnouns. Regardingthesecondissue,asintro-
15Image Segmentation list Instruction
Please describe the part of an image ‚Ä¶
(2) List of objects
Name: Road Category1: Inanimate Category2: Background
Description: The road is a black asphalt surface with red edges. ‚Ä¶
Color: black paved surface, ‚Ä¶
Name: Motorcycle Category1: Inanimate Category2: Foreground
Description:The motorcycle is parked on the road, .‚Ä¶
Color: Green motorcycle, silver metal frame, ‚Ä¶
Combined GPT-4V ‚Ä¶(more object descriptions)
segmentation ‚Ä¶(more segment)
(1) Overall description
(3) Relationship Style: The style is‚Ä¶
Theme: The theme‚Ä¶
<Motorcycle> [is parked on] <road> Global description of background:
<People1> [is next to] <People2> The background of ‚Ä¶
<Motorcycle> [is next to] <Billboard> Global description of foreground: GPT-4V
‚Ä¶ The foreground is‚Ä¶
‚Ä¶(more) GPT-4V
BACON (3) Relationship (1) Overall description (2) List of objects
Figure 11: A detailed overview of the method used to collect the BACON benchmark, segmented into five
distinctsteps. 1)TheSAMmodelsegmentsallcomponentswithintheimage. 2)VLMsidentifythenamesof
objectsinthemaskedimageobtainedfromthefirststep. 3)Usingthenamesidentifiedinthesecondstep,VLMs
annotateeachobjectindetail. 4)VLMsgenerateanoveralldescriptionoftheimagebasedonthelistofobjects
derivedfromtheabovesteps. 5)imagescreatedbyrandomlypairingtwomaskedimagesfromthefirststeparefed
toVLMstoidentifytherelationshipbetweenthecombinedsegments. Itisimportanttonotethathumanannotation
isrequiredtocorrectandverifytheoutputsfromstepstwothroughfive.
ducedinSection2.4,byutilizingthelistofobjects prompt. VLMs struggle to complete this task be-
providedby BACON,alongwithdetaileddescrip- causetheyoftenlacktheabilitytoperceiveinput
tionsofeachobject,itispossibletopost-process locationinformation. However,since BACON de-
GroundingDINO‚Äôspredictions. Thisenablesthe composesimagedescriptionsintoaseriesofbasic
precisedistinctionofdifferentindividualswithin elements,eachwithitscorrespondinglocation,we
thesamecategorylabel. can leverage this feature to accomplish the task.
AsshowninFigure10,themethodisdividedinto
A.4.3 Pointquestionanswering
threesteps. First,wecalculatetheCLIPsimilarity
Methodofapplying BACON. Inourexperiment, betweeneachobject‚Äôsdescriptionandtheinputtex-
PointQA is designed to answer questions related tualprompt,obtainingscoresforeachobject. The
to image regions based on the description of the more relevant an object is to the text description,
image. MostdescriptionsprovidedbyVisualLan- thehigheritsscore. Secondly,wecalculatescores
guageModels(VLMs)cannotaccomplishthistask foreachcandidateregionbyweightingthesumof
as their descriptions lack positional information. objectscoresbasedontheoverlapbetweenthecan-
However, BACON providesboththepositionalin- didateregionandtheobject‚Äôslocation. Thegreater
formation of objects within the image and their theoverlapwiththecandidatearea,thelargerthe
correspondingdescriptions. Givenatargetarea,by proportionofthatobject‚Äôsscore. Inthethirdstep,
combiningdescriptionsofdifferentobjectsbased theregionwiththehighestscoreisselectedasthe
ontheirpositionalrelationships,onecancreatea answer.
description relevant to the location. Specifically,
as illustrated in Figure 10, we compute the Inter-
A.4.5 Open-vocabularyscenegraph
sectionOverUnion(IOU)betweenthetargetarea
generation
and the positions of all objects. By combining
the descriptions of objects with high overlap, we Visual Genome dataset. Visual Genome is an
obtain a description that is closely related to the open-vocabularydataset. However,sincemostcur-
target area. Then, we feed this description to the rent scene graph generation (SGG) models only
question-answeringmodeltoanswerthequestion. consideracertainnumberofcategories,researchers
oftentreatitasadatasetwithalimitedsetofcate-
A.4.4 Pointingquestionanswering gories. Specifically,theyusuallyidentifythemost
Method of applying BACON. The PointingQA frequent 70 or 150 noun classes, along with the
taskrequiresselectingthemostappropriateregion 50 most common predicates, to create a filtered
from a set of candidate areas based on a textual dataset. In our case, as we are working on an
16
SAMCaption:
Four men are working together, one is wearing
black clothing, one is in white, and one is in
gray. There is another one looking at his phone.
GroundingDINO
Figure12: AnexampleofGroundingDINOundertakinganopen-vocabularytask,whereitencountersissues
withambiguouslabelsandfaceschallengesindistinguishingbetweendifferentindividualswithinthesamecategory.
open-vocabularyscenegraphgeneration(OV-SGG) ate. Therefore, by breaking down complex texts
task,wetreattheVGdatasetasanopen-vocabulary into basic elements, BACON can significantly as-
dataset,retainingallofitsannotations. sistSDXLinsimplifyingcomplextasks. Specifi-
Evaluationmetrics. TraditionalSGGtasksoften cally,SDXLcanfirstcreatethebackground,then
userecall-relatedmetricstoevaluateperformance, sequentially generate each object, and finally as-
thatis,howmany(subject-predicate-object)triplets semble the different parts. Currently, there are
are predicted in an image. However, these met- manymethodsthatcanbeutilizedforimagestitch-
ricscannotbedirectlyappliedtoassesstheperfor- ing,suchasAnydoor(Chenetal.,2024),Collage
manceofopen-vocabularytasks. Thisisbecause Diffusion(Sarukkaietal.,2024),etc. Sometimes,
previousmetricsinvolveperformingclassification images can also be directly stitched together and
tasks within a set of fixed categories and using thenrefinedusingSDXLasthebasemodel,with
the confidence of those classifications to obtain SDEdit(Mengetal.,2021)forrefiningtheimages,
the top K predictions with the highest likelihood. but this typically requires the images to be rela-
However, in an open-vocabulary scenario, there tively simple. Aside from generating individual
are theoretically an infinite number of possibili- partsoftheimageandthenstitchingthemtogether,
ties,makingitimpracticaltocalculateandsortall anotherapproachistosequentiallyinpaint(Rom-
scores. Therefore,weusethenumberofcorrectly bach et al., 2022) objects onto the image using
predicted triplets in the dataset as the evaluation inpaintingmethods.
metric. Themoretripletsareaccuratelypredicted, Moreresults. WeprovidemoreexamplesinFig-
thebettertheperformance. ure19
Additionally, we use CLIP to determine the
A.4.7 Precision&recallanduserstudy
correctnessofpredictionsonanopen-vocabulary
Whencalculatingprecisionandrecall,itinvolves
dataset. Givenaprediction,itisconsideredcorrect
identifyingwhichobjectshavebeenpredictedby
aslongastheCLIPsimilarityofitssubject,predi-
differentcaptioners. Forothercaptioners,thiscan
cate,andobjecttothecorrespondinggroundtruth
be challenging because directly extracting nouns
exceeds a certain threshold (0.85 here), and the
wouldincludemanynounsthatcannotbeconsid-
IOU between the positions of the subject and ob-
eredobjects. Therefore,weutilizeVLMstoaccom-
jectwiththegroundtruthalsosurpassesathreshold
plishthistask. Specifically,weinputthemodel‚Äôs
(0.5here).
captions into the VLMs, requesting them to ex-
tract the important objects contained within. For
A.4.6 Imagegeneration
BACON-Captioner,thisprocessisstraightforward
Method of enhancing SDXL by BACON. Even
becauseBACONexplicitlyprovidesalistofobjects.
as one of the most renowned models for text-to-
Thisalsohighlightstheadvantagesof BACON.
imagegeneration,SDXLoftenstrugglestounder-
stand complex prompts and generate precise im- A.4.8 Additionalcapabilitiesofcaptioner
agesaccurately. ThisisprimarilybecauseSDXL Evaluationmetrics. Evaluatingtheperformance
employsCLIPfortextunderstanding,whichlim- of the planning task is a subject that hasn‚Äôt been
its its ability to comprehend the text. However, widely discussed. As one of the pioneers, Lay-
eachbasicelementwithinacomplexpromptisnot outGPT(Fengetal.,2024)collectedsomeimages
complicated for SDXL to understand and gener- from the COCO dataset (Lin et al., 2014), which
17have varying numbers of objects of the same cat- prompt to enrich the overall scene and converts
egoryandusedprecisionandrecallasevaluation it into the required format. Conversely, Example
metrics to assess whether the quantity of objects in Figure 18 involves a more complex input; in
plannedisaccurate. Inspiredbytheirapproach,we suchcases,thelikelihoodoftheBACON-Captioner
have slightly expanded the concepts of precision addingnewobjectsisreduced.
andrecall. Werandomlysample1000imagesfrom
A.4.9 BACONonvideocaptioning
COCOandusetheirofficialcaptionsasinputfor
Weprovidemoreexamples(asFigures20and21)
eitherLayoutGPTorBACON-Captioner. Then,we
asasupplementaryofthemainpaper.
apply precision and recall metrics to assess how
many of the objects predicted by different plan-
ningmethodsactuallyexistintheimages,andhow
manyobjectspresentintheimagesarepredicted.
It‚Äôs important to note that both the captioner
and LayoutGPT operate in an open-vocabulary
manner. Hence, we used CLIP to map the open-
vocabularypredictionstoCOCO‚Äôsfixedsetofcat-
egories. Specifically,foranopen-vocabularypre-
diction,wecomputeitssimilaritytoallcategories
inCOCO,treatingthesimilarityaslogits,andthen
use a softmax function to map it to a category in
COCO. If the softmax score for the most likely
category exceeds a threshold (0.9 here), we con-
sider the prediction to be correct; otherwise, it is
deemed incorrect. In BACON dataset, the situa-
tionisquitesimilar. Aslightdifferenceisthatthe
model‚Äôs predictions are mapped onto the list of
groundtruthobjectsforthecurrentimage,rather
thanafixedsetofcategories. Similarly,whenthe
softmaxscoreexceedsacertainthreshold,itiscon-
sidered a correct prediction. Given that BACON
benchmarkissignificantlymorechallengingthan
COCO,ifthethresholdissettoohigh,almostall
predictionswouldbeincorrect;hence,welowered
thethresholdto0.5.
Precisionandrecalldonottakeintoaccountthe
positioningoftheplanning. Thisisbecauseevaluat-
ingwhetherapositionisappropriateisasubjective
task,andsolongasitisreasonable,itshouldsuf-
fice. Nonetheless,sincethepositionaldistribution
intheoriginalimagesisassuredlyreasonable,we
can also use the positions in the original images
as a certain reference. Therefore, we calculated
themeanIntersectionOverUnion(mIOU)ofthe
positionsoftheobjectsintheplanningcompared
tothoseintheoriginalimages,andusedthisasan
evaluationmetric.
Examples. We provide two examples (as shown
in Figures 17 and 18) to demonstrate how BA-
CONtransformsastandardpromptintotheformat
weneed. ExampleinFigure17featuresasimple
prompt;inthisscenario,theBACON-Captionerof-
tenimaginesadditionalobjectsnotpresentinthe
18Captioner&Grounding
Overall description:
StyleÔºö'The image is a photograph with a realistic style.'
ThemeÔºö'The theme of the image is transportation, specifically a train traveling through a rural landscape.'
Background descriptionÔºö'The background of the image features a rural landscape with elements of nature and infrastructure. There is a
bridge with green metal railings crossing over the train tracks. Beyond the bridge, a fence made of wooden posts and rails encloses a field.
The field appears to be grassy with some patches of bare earth. The sky is overcast, with a pale, diffused light suggesting an overcast or
cloudy day.'
Foreground description: 'In the foreground, a train is captured in motion on the tracks. The train is painted in a blue and yellow color
scheme. The train has multiple carriages, and the windows reflect the surrounding environment. The tracks are made of steel rails with
wooden sleepers, and they run parallel to a grassy embankment on the left side of the image.'
Object list:
Train:
Category: inanimate, foreground
Description: 'The <train> ‚Äòs body is long and sleek, with <windows> lined along its side. The front <car> has a curved nose
with a destination <sign> and <headlights>. The <train> is composed of several <carriages> connected together.‚Äô
Color: blue and yellow
Position: [200, 160, 441, 367]
Track:
Category: inanimate, foreground
Description: 'The <track> consists of parallel <steel rails> supported by wooden <sleepers>. It stretches into the distance,
guiding the <train>.‚Äô
Color: rusty brown rails, brown sleepers
Position: [128, 112, 553, 425]
Bridge:
Category: inanimate, background
Description: 'The <bridge> spans over the <tracks> with a structure made of metal <beams> and <railings>. It appears
functional and unadorned.‚Äô
Color: green railings
Position: [54, 95, 271, 160]
Fence:
Category: inanimate, background
Description: 'The <fence> is constructed of wooden <posts> and <rails>, enclosing the <field> and providing a boundary.‚Äô
Color: natural wood tone
Position: [274, 137, 638, 184]
Field:
Category: inanimate, background
Description: 'The <field> is predominantly grass-covered, with some areas of bare <soil>. It is bordered by the <fence> and
<trees>.‚Äô
Color: green grass, brown soil
Position: [283, 161, 638, 421]
Tree:
Category: inanimate, background
Description: 'The <trees> have bare branches, indicating a lack of <leaves> which could suggest a seasonal change.‚Äô
Color: dark brown branches
Position: [207, 0, 404, 146]
Sky:
Category: inanimate, background
Description: 'The <sky> is overcast, with a uniform light grey color, suggesting cloudy weather.‚Äô
Color: light grey
Position: [1, 0, 636, 103]
Relationship:
<Train> [is traveling on] <track>
<Train> [is passing under] <bridge>
<Bridge> [spans over] <track>
<Fence> [encloses] <field>
<Field> [is bordered by] <tree>
<Field> [is bordered by] <Fence>
<Tree> [is standing in] <Field>
Figure13: AcompleteexampleofBACON.
19Captioner&Grounding
Overall description:
StyleÔºö'The image is a candid, action-oriented photograph.'
ThemeÔºö'The theme of the image is water sports, specifically surfing.'
Background descriptionÔºöThe background of the image is dominated by the ocean with its varying shades of blue and white due to the
churning waves and foam. The water appears dynamic and powerful, suggesting a strong surf. There is no clear horizon line, but the
waves occupy the majority of the background space, creating a sense of immersion in the ocean environment. The light seems natural,
likely from the sun, and it enhances the texture of the water and waves. The ocean is the only element in the background, emphasizing the
vastness and the solitary nature of the surfing activity.'
Foreground description: 'In the foreground, a man is captured in the midst of surfing a wave. He is wearing a cap, shorts, and is shirtless,
which indicates a warm climate or season. The man is in a semi-crouched position on a surfboard, navigating the wave with a paddle in
his left hand. His facial expression shows concentration and determination. The surfboard is mostly white with a design on its surface,
and it cuts through the water, leaving a trail of spray behind. The man's musculature and stance suggest that he is experienced and in
control of the surfboard. The water around the surfboard is turbulent, with foam and spray being generated by the movement ofthe
surfboard and the wave.'
Object list:
Man:
Category: living, foreground
Description: 'The <man>‚Äôs <torso> is bare, and he is wearing a <cap> on his <head>. His lower body is covered by <shorts>.
He is in a semi-crouched position on the <surfboard>, holding a <paddle> in his left hand. His facial expression shows
focus.‚Äô
Color: skin tone, green shorts, white cap
Position: [200, 201, 270, 288]
Surfboard:
Category: inanimate, foreground
Description: 'The <surfboard> is under the <man>, supporting him as he rides the <wave>. It has a design on its surface and
is cutting through the <water>.‚Äô
Color: predominantly white with a design
Position: [178, 285, 283, 326]
Paddle:
Category: inanimate, foreground
Description: 'The <paddle> is held by the <man> in his left hand, assisting him in navigating the <wave>.‚Äô
Color: black shaft, white blade
Position: [103, 171, 266, 271]
Wave:
Category: inanimate, background
Description: 'The <wave> is large and powerful, with <water> churning and creating <foam> and <spray> as it breaks.‚Äô
Color: shades of blue and white
Position: [2, 63, 638, 422]
Ocean:
Category: inanimate, background
Description: 'The <ocean> fills the background, characterized by its dynamic <waves> and <foam>.‚Äô
Color: various shades of blue
Position: [2, 2, 638, 424]
Relationship:
<Man> [is riding] <Surfboard>
<Man> [is holding] <Paddle>
<Surfboard> [is cutting through] <Wave>
<Wave> [is breaking around] <Man>
<Man> [is surfing on] <Ocean>
<Ocean> [is supporting] <Surfboard>
Figure14: AcompleteexampleofBACON.
20Instruction
GPT4v
%%Part1: Overall descrip2on%%
&&Part1.1: Style&&
The image is a photograph with a realis2c style.
&&Part1.2: Theme&&
The theme of the image is urban transporta2on during twilight.
&&Part1.3: Global descrip2on of background&&
The background of the image consists of an overcast <sky> with a gradient of blue tones, transi2oning from a deeper blue at the top to a <lighter>
hue near the horizon. A large, modern <structure> labeled "DOKK1" dominates the leM side, featuring an angular design with illuminated
<windows>. To the right is a tall <building> with numerous <windows>, some of which emit a warm glow. Further back, there are more <city
buildings> with varying architectural designs, including one with a greenish <glass fa√ßade>. The <ambient ligh2ng> suggests it is either dawn or
dusk, with <ar2ficial lights> beginning to have a pronounced effect on the scene.
&&Part1.4: Global descrip2on of foreground&&
The foreground shows a <city street scene> with mul2ple <lanes>. A <tram> is on the leM, labeled ‚ÄúL2Aarhus H" and displaying a <des2na2on sign>.
It is sta2oned at what appears to be a <tram stop>, with a <plaWorm> and a <railing>. The <street> is busy with <cars>, all with <headlights> on,
indica2ng low light condi2ons. The <vehicles> vary in size and shape, sugges2ng a mix of personal and commercial <traffic>. The <pavement> along
the <street> is wet, reflec2ng the lights of the <tram> and nearby <street lamps>. The overall <atmosphere> is one of a bustling <urban
environment> in the evening hours.
%%Part2: List of objects%%
<Sky> (Inanimate; background; The <sky> presents a gradient of <blue> shades and is sca^ered with <clouds>; Color informa2on: shades of <blue>.)
<Building 1> (Inanimate; background; The <building> has an angular <design> with <windows> that are illuminated from within; Color informa2on:
<black> and <yellow> lights.)
<Building 2> (Inanimate; background; This <building> is tall with many <windows>, some of which are lit, and has a cylindrical <shape>; Color
informa2on: <white> with <yellow> lit windows.)
<Building 3> (Inanimate; background; Visible behind <Building 1>, this <building> features a <glass fa√ßade> with a <greenish> hue; Color
informa2on: <green> glass and <gray> structure.)
<Tram> (Inanimate; foreground; The <tram> is sta2onary with a <front display> showing its <route> and a <design> that is sleek and modern; Color
informa2on: predominantly <white> with <black> and <blue> accents.)
<Street> (Inanimate; foreground/background; The <street> shows <lanes> with mul2ple <vehicles> and <wet> condi2ons reflec2ng <lights>; Color
informa2on: <dark gray> asphalt, <white> road markings.)
<Car 1> (Inanimate; foreground; A <car> with its <headlights> on, driving on the <street>, exhibi2ng a <sedan> body style; Color informa2on:
<black>.)
<Car 2> (Inanimate; foreground; Another <car> follows <Car 1>, also with <headlights> on, and appears to be a <hatchback>; Color informa2on:
<silver>.)
<Car 3> (Inanimate; foreground; This <car> is in the <lane> closest to the <camera>, showing a <compact> shape with <headlights> on; Color
informa2on: <dark blue>.)
<Lamppost> (Inanimate; background; A <lamppost> stands tall with a <light> at the top, illumina2ng the <area> below; Color informa2on: <black>
post, <white> light.)
%%Part3: Rela2onships%%
<Sky> [overarches] <Buildings>.
<Building 1> [is adjacent to] <Tram>.
<Building 2> [towers over] <Street>.
<Building 3> [is situated behind] <Building 1>.
<Tram> [is sta2oned at] <Street>.
<Tram> [reflects on] <Street>.
<Car 1> [drives on] <Street>.
<Car 2> [follows] <Car 1>.
<Car 3> [is closest to] <Camera>.
<Lamppost> [illuminates] <Street>.
Figure15: AnexampleofBACONinstringformatobtainedbyGPT-4V.
211 Hello, I would like to ask for your help in describing an image. Please note that I would like the description to be as
2 detailed as possible. Please strictly respond following my instructions and do not print any redundant words.
3 This description needs to include three parts. The title of each part should be ‚Äò%%Part1: Overall description%%‚Äô, ‚Äò%%Part2:
4 List of objects%%‚Äô, and ‚Äò%%Part3: Relationships%%‚Äò. All important nouns in your response have to be bounded by '<' and
5 '>‚Äô!
6 The first part is an overall description of the image. Your answer to this part should consist of three parts, one sentence to
7 describe the style of the image, one sentence to describe the theme of the image, and several sentences to describe the
8 image. The titles of these parts are '&&Part1.1: Style&&', '&&Part1.2: Theme&&', '&&Part1.3: Global description of
9 background&&', 'Part1.4: Global description of foreground&&'. The global description should be as detailed as possible
10 and at least 150 words in total. If there is text content in the image, you can also describe the text, which should be bound
11 by quotation marks. All important nouns in your response have to be bounded by '<' and '>‚Äô!
12 The second part is to list all the objects in the image, as many as possible, in order of importance. Note that any object
13 should not be a part of other objects. Note that the listed object should not be the plural. If there are multiple individuals
14 of the same category of objects, please list them separately. For example, if there are three apples in the picture, they
15 should be listed as 'Apple 1,' 'Apple 2,' and 'Apple 3.', respectively. Additionally, the objects should be classified into two
16 categories: living and inanimate objects. Living refers to creatures such as humans, cats, dogs, and plants, while other
17 lifeless objects belong to the category of inanimate objects. Finally, each object should have a very detailed description,
18 with more important objects receiving more detailed descriptions. Each description should be at least 30 words and the
19 important nouns in it have to be bounded by '<' and '>'. You should also identify whether this object belongs to the
20 foreground or background. You should additionally provide a sentence to describe the color information of the object.
21 Therefore, the format for listing each object should be 'Object Name (Category (Living/Inanimate);
22 foreground/background; Description; Color information)'. Specifically, the detailed description of an object should focus
23 on its part and its action. All descriptions should be in the forms of, object's + part + verb + object/adjective or object + is +
24 present participle. The description should be detailed as well as possible, and try to describe all parts of this object. You
25 should specifically notice if there is a sky, tree, sun, or other object in the background of the environment. All important
26 nouns in your response have to be bounded by '<' and '>‚Äô!
27 The third part is to describe the relationships between all the objects in pairs. Please list them one by one. Additionally,
28 please describe the relationship between object A and object B in the format of 'Object A' + 'Action' + 'Object B.' Please
29 don't print the same relation twice. For example, if there is ‚ÄúA relation B‚Äù, you shouldn't print 'B relation A' again. All
30 important nouns in your response have to be bounded by '<' and '>‚Äô!
31 I will provide you with an example of the last two parts of a description to show you the desired format. You should only
32 focus on the format of this example instead of the content of it. You should use the same format to respond.
33 "%%Part2: List of objects%%
34 <Woman> (Living; foreground; The <woman>'s <hair> is bundled in a <scarf>. Her <torso> is covered with a <black shirt>.
35 Her <lower body> is clad in <blue jeans>. Her <legs> move through the <water>. Her <right hand> holds a pair of <shoes>;
36 Color information: <black> shirt, <blue> jeans, <orange> scarf.)
37 <Water> (Inanimate; foreground/background; The <water> floods the <street>, reflecting the <sky> and <surrounding
38 objects>; Color information: <murky blue-grey>.)
39 <Building 1> (Inanimate; background; The <building> has a <fa√ßade> with <doors> and <windows>, showing signs of
40 <water damage>; Color information: <pale yellow>.)
41 <Building 2> (Inanimate; background; This <building> is similar to <Building 1> but with a <red> roof visible above the
42 <flood>; Color information: <light orange> walls, <red> roof.)
43 <Vehicle 1> (Inanimate; background; A <vehicle> is partially submerged, showing only the <roof> and <upper parts>; Color
44 information: <white>.)
45 <Vehicle 2> (Inanimate; background; Another <vehicle>, also partially submerged, with a <visible logo>; Color information:
46 <silver>.)
47 <Sky> (Inanimate; background; The <sky> is filled with <clouds>, implying recent or ongoing <precipitation>; Color
48 information: <gray>.)
49 %%Part3: Relationships%%
50 <Woman> [is walking through] <Water>.
51 <Woman> [is moving away from] <Camera>.
52 <Water> [reflects] <Sky>.
53 <Water> [surrounds] <Vehicles>.
54 <Buildings> [line] <Street>.
55 <Vehicle 1> [is submerged by] <Water>.
56 <Vehicle 2> [is submerged by] <Water>.
Figure16: TheinstructionforGPT-4VtoobtainBACONfromanimage. Wehighlightthepartsinvolvingspecific
examplesinorange.
22BACON-
A cat and a dog gather around a pizza on the highway.
Captioner
Overall description
Style:The image is a candid, real-life photograph with a humorous setup.
Theme: The theme of the image is an unexpected interaction between animals and human food.
Background description: The background of the image features a highway setting with a large, dark-colored rock
on the left side and a scrapped car on the right. The sky is overcast, with a dull, uniform gray tone suggesting an
overcast or cloudy day. The highway itself is made of asphalt and has visible lane markings. The rock and car
appear to be out of place on the road, indicating a possible scene of an accident or roadside debris. There are no
visible people or moving vehicles, which adds to the stillness of the scene.
Foreground: In the foreground, a cat and a dog are positioned on the road, closely inspecting a pizza that lies
between them. The cat, on the right, is leaning forward with its head down, sniffing or showing interest in the pizza.
The dog, on the left, mirrors the cat's posture, with its nose also directed towards the pizza. The pizza itself is large,
with visible toppings that include cheese and pepperoni. The animals' proximity to the pizza and their focused
attention on it create a humorous and unexpected scene, as it is unusual to see wild animals interacting in such a
human-like manner. The road surface is a faded gray, and the animals' fur appears well-defined against this
backdrop.
Object list
‚ë†Name: Cat; Category: Animal, foreground; Description: The cat's body is arched slightly as it leans over the
pizza. Its head is lowered towards the pizza, its eyes seem to be focused on the food, and the cat's fur is dark blue;
Color: dark blue; Position: [677, 178, 981, 361]
‚ë°Name: Dog; Category: Animal, foreground; Description: The dog is standing with its body angled towards the
pizza. Its head is also lowered to get a closer look at the pizza. The dog has fur that appears to be short and smooth;
Color: dark gray; Position: [45, 167, 375, 423]
‚ë£Name: Pizza; Category: Inanimate, foreground; Description: The pizza is round with a crust and several
toppings that include cheese and pepperoni. It is placed directly on the road surface; Color: golden crust, red
pepperoni, yellow cheese; Position: [428, 183, 632, 324]
‚ë§Name: Rock; Category: Inanimate, background; Description: The rock is large and irregularly shaped, with a
rough texture and a dark color that contrasts with the asphalt of the highway; Color: dark gray; Position: [281, 537,
488, 649]
‚ë•Name: Scrapped Car; Category: Inanimate, background; Description: The scrapped car is heavily damaged,
with twisted metal. It is positioned on its side, suggesting it was involved in a severe accident; Color: rusty and
metallic tones; Position: [584, 478, 852, 655]
Relationship
‚ë†< Dog > on the left side of < pizza > ‚ë°< Cat > on the right side of < pizza >
‚ë¢< Pizza > is lying on < highway > ‚ë£< Rock > is blocking < highway >
‚ë§< Rock > is adjacent to < Scrapped Car > ‚ë•< Scrapped Car > is blocking < highway>
Figure 17: An example of how BACON-Captioner transforms a brief prompt into the distinctive style of
BACON.
23Under the bright moon, there is an icy sky and snow. In the
BACON-
center of the scene, a bonfire blazes. To the left of the bonfire
Captioner
are an arctic fox and a polar bear, to the right is a penguin, and
above the bonfire flutters a small ice butterfly.
Overall description
Style:The image is a color photograph with a clear focus and a balanced composition.
Theme: The theme of the image is a serene, wintery scene featuring various arctic animals gathered around a warm bonfire.
Background description: The background of the image is mainly the bright moon in the upper part of the screen. Below the
sky, the horizon is lined with a layer of snow that blankets the ground, reflecting the light from the bonfire and creating a
contrast with the dark sky. The snow appears to be undisturbed except for the area immediately around the fire, where it is
melted.
Foreground: In the foreground, the central focus is the bonfire, with vibrant flames leaping upwards, casting a warm glow
on the surrounding snow. The fire's light illuminates the figures of the animals and the snow around them. To the left of the
fire, an arctic fox sits attentively, its fur appearing thick and white. To the right, a polar bear sits with a relaxed posture, its
white fur contrasting with the darker elements of the scene. In front of the fire, a penguin stands out with its black and white
coloring, with its back facing the camera. Above the fire, an ice butterfly with outstretched wings adds a whimsical touch to
the scene. The animals are positioned on a flat surface, likely a frozen body of water, as indicated by the presence of the ice
butterfly and the reflection of the fire on the snow.
Object list
‚ë†Name: Arctic Fox; Category: Animal, foreground; Description: The arctic fox's body is facing the camera, with its head
turned slightly to the side. Its fur is thick and white; Color: White; Position: [153, 235, 337, 419]
‚ë°Name: Polar Bear; Category: Animal, foreground; Description: The polar bear is sitting with its body oriented away
from the camera. Its fur is thick and white, and its head is turned to the side, giving a profile view; Color: White; Position:
[266, 264, 481, 627]
‚ë¢Name: Penguin; Category: Animal, foreground; Description: The penguin stands with its body turned away from the
camera, its head slightly tilted. It has black back and white belly; Color: Black back, white belly; Position: [471, 154, 614,
416]
‚ë£Name: Bonfire; Category: Inanimate, foreground; Description: The bonfire consists of flames leaping upwards from a
pile of wood. The fire is bright and orange, casting light on the surrounding snow; Color: Orange flames, brown and black
wood; Position: [307, 204, 614, 409]
‚ë§Name: Ice Butterfly; Category: Animal, foreground; Description: The ice butterfly is positioned above the bonfire, with
its wings spread wide. It appears to be made of ice and is translucent with a white hue; Color: Translucent with a white tint;
Position: [378, 768, 450, 896]
Relationship
‚ë†< Arctic Fox > on the left side of < Bonfire > ‚ë° < Arctic Fox > sits near < Polar Bear>
‚ë¢< Polar Bear > on the right side of < Arctic Fox > ‚ë£< Penguin > in front of < Bonfire >
‚ë§< Ice Butterfly > floats above < Bonfire > ‚ë•< Polar Bear > sits beside < Bonfire>
Figure18: AnexampleofhowBACON-Captionertransformsarelativelycomplexpromptintothedistinctive
styleofBACON.
24Caption BACON + SDXL DALL-E 3 SDXL
In the deep sea, an abandoned large ship full of
marine life sank to the bottom of the sea. There
are two blue balloons floating in front of the ship.
There is a dolphin swimming below the balloon.
There is a drifting bottle floating in the deep sea,
inside which is a sailboat
In a yoga studio, there is an artwork of a green
jade dragon, with a white cat lying on the right
side of the artwork. On the distant ground, against
the wall, there is a painting depicting war
In an abandoned factory building, sunlight filtered
in. A technologically advanced spaceship flies
over the factory building. Listening to a
motorcycle below the spaceship, there is a pink
guitar on the ground to the right of the motorcycle.
On a pink night, there was a pool in the center of
the lawn, and a purple sports car was floating on
the pool. There was a light bulb on the hood of the
sports car, and there was an orange goldfish in the
bulb. On the left side of the car is a small, colorful
robot
There is a small river in the forest, and there is a
stone bridge on the river. There is a golden
praying mantis on the bridge. There is a
mongoose standing by the riverbank, and to its
right lies a turtle
In an old-fashioned subway station, there is a
emerald green lion, a gray white wolf, and a
colorful paper crane standing together waiting for
the subway
Figure19: AdditionalexamplesofBACONonimagegeneration.
25lNew item compared to last frame lChanged item compared to last frame lDeleted item compared to last frame
car dog sky tree lawn text house child person bicycle sidewalk Relationship
Example 1: Dog Category 1: Living Category 2: Foreground
Description: The dog has a fluffy white coat and appears to be a small breed. It is looking up child play withdogsidewalk car nearhouse
towards the person and is attached to a leash.
C E Dxo esalo cm rr i: p
p
w l te ih o2it n:e
:
Bc To ia hct ey. c bl ie cycle'sC fra at meg
e
o isr y
su
1 p:
p
ortI in na gn pim era st oe
n. Its wheels
C ara et e ing co or ny
t
a2 c:
t
wiF tho r te hg er go ru an sd
s.
stand
on
stand on stand o=nstand onunder near
The handlebars are being gripped by person's hands. A backpack is attached to the bicycle's rear ride on
rack. text lawnperson bicycletree sky
Color: silver frame, black tires, blue backpack.
car dog sky tree lawn text house child person bicycle sidewalk Relationship
Example 1: Dog Category 1: Living Category 2: Foreground
Description:The dog has a fluffy white coat and appears to be a small breed. It is looking up child dogsidewalk car house
towards the person and is attached to a leash.
Color: white coat. near
Example 2: Sidewalk Category 1: Inanimate Category 2: Background
Description: The sidewalk is a concrete path that borders the lawn and leads towards the house. It
is typical of suburban streets. text lawnperson bicycletree sky
Color:gray.
car dog sky tree lawn text house child person bicycle sidewalk Relationship
Example 1: Dog Category 1: Living Category 2: Foreground
Description:The dog has a fluffy white coat and appears to be a small breed. It is looking up child dogsidewalk car house
towards the person and is attached to a leash.
C Exo alo mr: pw leh i 2te
:
Tco ea xt t.
Category 1: Inanimate Category 2: Foreground
under
Description:The text is superimposed on the image in the lower part, stating 'Thirteen years
earlier‚Äô. text lawnperson bicycletree sky
Color:white on a semi-transparent background.
Relationship
car dog sky tree lawn text house child person bicycle sidewalk
Example 1: Dog Category 1: Living Category 2: Foreground child dogsidewalk car house
Description:The dog has a fluffy white coat and appears to be a small breed. It is looking up
towards the person and is attached to a leash.
Color: white coat.
Example 2: Sky Category 1: Inanimate Category 2: Background
Description:The sky is clear and blue, indicating good weather and no visible clouds.
Color:blue. text lawnperson bicycletree sky
Note: Considering the limited drawing space and beauty of graph structure, we have performed some simplification for relationships that appear more than once.
Figure20: AnadditionalexampleofBACONonvideocaptioning.
26lNew item compared to last frame lChanged item compared to last frame lDeleted item compared to last frame
SSUUVV ttrreeee1 rrooaadd ffeennccee1 floflwoweerr1 red car Relationship
Example 1: SUV Category 1: Inanimate Category 2: Background road
Description: The vehicle has a prominent Mercedes-Benz emblem on the front grille, which is a person side walk lawn
t ah pr pe ae r- ep no ti n dt ae md ast ga er oe rn c wi erc al re .d. ‚Ä¶ The overall condition of the car looks well-maintained, with no
in
frontp oa frk onshade for
Color: silver body, black spare tire cover, black tinted windows. fence1 SUVin front oftres etreet lamp
Example 2: Flower1 Category 1: Living Category 2: Foreground grow on
Description: The flower1 in the picture is a red rose, bright red, with layered petals that appear to flower car house sky
be in full bloom. A large number of green leaves cluster around the flowers.
Color: red petals, green leaves.
SSUUVV trtreeee1 rrooaadd ffeennccee1 floflwowerer1skyskyhohuosuesepepresrosnonsidsiedwewalaklkstresteret elta lmampp lawn carcar Relationship
E Dx esa cm rip pl te
i
o1 n: :S TU heV
vehicle
hC asa ate pg ro or my
i
n1 e:
n t
MIn ea rcn eim dea st -e
Benz emblem
C ona t the eg o frr oy
n
t2 g: rilF leo
,
r weg hr ico hu n isd
a
person road nearside walk
lawn
three-pointed star encircled. ‚Ä¶ The overall condition of the car looks well-maintained, with no
drive
apparent damage or wear. stand inside
Color: silver body, black spare tire cover, black tinted windows. fence1 SUV treestreet lamp
Example 2: Flowers Category 1: Living Category 2: Background obscured by
Description: The flower in the picture is a red rose, bright red, with layered petals that appear to car
be in full bloom. A large number of green leaves cluster around the flowers
flower housestand belowsky
Color:red petals, green leaves.
SSUUVV trteree1e rrooaadd ffeennccee1 flfolowweerr1 sskkyy hhoouussee ppeerrssoonn ssiiddeewwaallkk ssttrreeeett llaammpp llaawwnn Relationship
Example 1: SUV Category 1: Inanimate Category 2: Foreground in front of
D
t ah
pre pes aec r-r epi nop
ti
nt di
t
ao
e
mdn a:
st
gT
a
eh
r
e
oe
rv
n
e
c
wh
i
eri
c
ac ll ree
.d
h
.
a ‚Ä¶s a
T
hp ero om vi en re an llt cM oe nr dc ie tid oe ns - oB
f
e tn hz
e
e cm arb l le om
ok
o
s
n
w
t eh le
l-
mfro an int tg ar inil ele d,
,
w wh iti hc h
n
ois a
person
road side w ia
n
flk
ront
la ofwn
Color:silver body, black spare tire cover, black tinted windows. fence1 SUV tree street lamp
Example 2: Road Category 1: Inanimate Category 2: Foreground behind
Description:A smooth and flat concrete road. The road condition is good, with no obvious cracks flower car
or severe wear. There are no visible markings or markings on the road. house sky
Color:gray
SUV tree1 road fence1 fence2 sky house person sidewalk street lamp lawn car Relationship
E Dx esa cm rip pl te i o1 n: :S TU heV vehicle hC asa ate pg ro or my i n1 e: nt MIn ea rcn eim dea st -e Benz emC ba let meg oo nr y th 2 e : fronF t o gr re ig llr eo ,u wn hd ich is a road side walk lawn
t ah pr pe ae r- ep no ti n dt ae md ast ga er oe rn c wi erc al re .d. ‚Ä¶ The overall condition of the car looks well-maintained, with no person park on
Color: silver body, black spare tire cover, black tinted windows. fence2 SUV tree street lamp
E Dx esa cm rip pl te
i
o2 n: :C Aa sr
m all, red
C caa
r
t weg ito
h
r ay
h
1 a:
tc
hbI an ca kn i dm ea sit ge
n. The
C vea ht ice lg eo ir
s
y
c
a2 p: tureB da c ink g mro ou tin od
n. It has flower
behindcar
pass house
black windows, silver wheels through sky
Color:red body, black windows, silver wheels.
Note: Considering the limited drawing space, some similar objects with different names are omitted here. For relationships that appear more than once, we have performed some simplification.
Figure21: AnadditionalexampleofBACONonvideocaptioning.
27