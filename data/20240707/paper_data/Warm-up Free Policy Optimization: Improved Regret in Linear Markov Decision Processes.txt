Warm-up Free Policy Optimization:
Improved Regret in Linear Markov Decision
Processes
AsafCassel AvivRosenberg
TelAvivUniversity GoogleResearch
acassel@mail.tau.ac.il avivros@google.com
Abstract
Policy Optimization (PO) methods are among the most popular Reinforcement
Learning(RL)algorithmsinpractice. Recently,Shermanetal.[2023a]proposed
aPO-basedalgorithmwithrate-optimalregretguaranteesunderthelinearMarkov
DecisionProcess(MDP)model. However,theiralgorithmreliesonacostlypure
exploration warm-up phase that is hard to implement in practice. This paper
eliminatesthisundesiredwarm-upphase,replacingitwithasimpleandefficient
contractionmechanism. OurPO algorithmachievesrate-optimalregretwith im-
proveddependenceontheotherparametersoftheproblem(horizonandfunction
approximation dimension) in two fundamental settings: adversarial losses with
full-informationfeedbackandstochasticlosseswithbanditfeedback.
1 Introduction
Policy Optimization(PO) is a widely used methodin ReinforcementLearning(RL) thatachieved
tremendous empirical success, with applications ranging from robotics and computer games
[Schulmanetal., 2015, 2017, Mnihetal., 2015, Haarnojaetal., 2018] to Large LanguageModels
(LLMs; Stiennonetal. [2020], Ouyangetal. [2022]). Theoreticalwork on policyoptimizational-
gorithms initially considered tabular Markov Decision Processes (MDPs; Even-Daretal. [2009],
Neuetal.[2010b],Shanietal.[2020],Luoetal.[2021]),wherethenumberofstatesisassumedto
befiniteandsmall. Inrecentyearsthetheorywasgeneralizedtoinfinitestatespacesunderfunction
approximation,specificallyunderlinearfunctionapproximationinthelinearMDPmodel[Luoetal.,
2021,Daietal.,2023,Shermanetal.,2023b,a,Liuetal.,2023].
Recently,Shermanetal.[2023a]presentedthefirstpolicyoptimizationalgorithmthatachievesrate-
optimalregretinlinearMDPs,i.e.,aregretboundofO(poly(H,d)√K),whereKisthenumberof
interactionepisodes,H isthehorizon,anddisthedimensionofthelinearfunctionapproximation.
However, their algorithm requires a pure explorationewarm-up phase to obtain an initial estimate
of the transition dynamics. To that end, they utilize the algorithm of Wagenmakeretal. [2022b]
for reward-free exploration which is not based on the policy optimization paradigm. Moreover,
althoughthisalgorithmiscomputationallyefficient,itreliesonintricateestimationtechniquesthat
arehardtoimplementinpracticeandunlikelytogeneralizebeyondlinearfunctionapproximation
(seediscussioninsection4).
Inthispaper,weproposeanovelcontractionmechanismtoavoidthiscostlywarm-upphase. Both
our contractionmechanism and the warm-upphase serve a similar purpose– ensuringthat the Q-
value estimates are bounded and yield “simple” policies. But, unlike the warm-up, our method
is integrateddirectlyinto thePO algorithm,implementedusing a simpleconditionaltruncationof
the Q-estimates, and only contributesa lower-orderterm to the final regretbound. Moreover,our
Preprint.Underreview.
4202
luJ
3
]GL.sc[
1v56030.7042:viXraapproachismuchmoreefficientinpracticesinceitdoesnotrelyonanyreward-freemethods,which
explorethestatespaceuniformlywithouttakingtherewardintoaccount.
Based on this contraction mechanism, we build a new policy optimization algorithm that is sim-
pler,morecomputationallyefficient,easiertoimplement,andmostimportantly,improvesuponthe
best-knownregretboundsforpolicyoptimizationin linear MDPs. Our regretboundholdsin two
fundamentalsettings:
1. Adversariallosseswithfull-informationfeedback,wherethelossfunctionchangesarbitrar-
ilybetweenepisodesandisrevealedtotheagententirelyattheendofeachepisode.
2. Stochasticlosseswithbanditfeedback,wherethelossfunctionineachepisodeissampled
i.i.dfromsomeunknownfixeddistributionandtheagentonlyobservesinstantaneouslosses
inthestate-actionpairsthatshevisits.
Inthesesettings,thebest-knownregretbound(byShermanetal.[2023b])wasO(√H7d4K). Our
algorithm,ContractedFeaturesPolicy Optimization(CFPO), achievesO(√H4d3K) regret, yield-
ing a √H3d improvement over any algorithm for the adversarial setting and meatching the value
iterationbasedapproachofJinetal.[2020b]inthestochasticsetting. Wee notethatthisisthebest
regretwecanhopeforwithoutmoresophisticatedvariancereductiontechniques[Azaretal.,2017,
ZanetteandBrunskill, 2019, Heetal., 2023, Zhangetal., 2024], thathavenotyetbeenappliedto
POalgorithmseveninthetabularsetting.1 Ignoringlogarithmicfactors,theregretofCFPOleavesa
gapofonly√HdfromtheΩ(√H3d2K)lowerboundforlinearMDPs[Zhouetal.,2021a].Finally,
ouranalysisreliesonanovelregretdecompositionthatusesanotionofcontracted(sub)MDPand
maybeofseparateinterest(seesection5).
1.1 Relatedwork
PolicyoptimizationintabularMDPs. TheregretanalysisofPOmethodsintabularMDPswasin-
troducedbyEven-Daretal.[2009],whichconsideredthecaseofknowntransitionsandadversarial
lossesunderfull-informationfeedback.Neuetal.[2010a,b]extendedtheiralgorithmstoadversarial
lossesunderbanditfeedback.Then,Shanietal.[2020]presentedthefirstPOalgorithmsforthecase
ofunknowntransitions(forbothstochasticandadversariallosses),andfinallyLuoetal.[2021]de-
visedaPOalgorithmwithrate-optimalregretforthechallengingcaseofunknowntransitionswith
adversarial losses under bandit feedback. Since then, PO was studied in more challenging cases,
e.g.,delayedfeedback[Lancewickietal.,2022,2023]andbest-of-both-worlds[Dannetal.,2023].
Other regret minimization methods in tabular MDPs. An alternative popular method for re-
gret minimization in tabular MDPs with adversarial losses is O-REPS [ZiminandNeu, 2013,
RosenbergandMansour, 2019a,b, Jinetal., 2020a], which optimizes over the global state-action
occupancy measures instead of locally over the policies in each state. However, this method is
hardto implementinpracticeanddoesnotgeneralizetothe functionapproximationsetting(with-
out restrictive assumptions). For stochastic losses, optimistic methods based on Value Iteration
(VI;Jakschetal.[2010],Azaretal.[2017],ZanetteandBrunskill[2019])andQ-learning[Jinetal.,
2018,Zhangetal.,2020]areknowntoguaranteeoptimalregret,whichhasnotbeenestablishedyet
foradversariallosses.
Policy optimization in linear MDPs. While Shermanetal. [2023a] established rate-optimalre-
gretforPOmethodsinlinearMDPswithstochasticlosses,mostoftherecentresearchfocusedon
the case of adversariallosses with banditfeedback[Luoetal., 2021, NeuandOlkhovskaya, 2021,
Daietal.,2023,Shermanetal.,2023b,Kongetal.,2023,Liuetal.,2023,ZhongandZhang,2023],
whererate-optimalityhasnotbeenachievedyet.
Other regret minimization methods in linear MDPs and other models for function approxi-
mation. UnlikeO-REPSmethodsthatdonotgeneralizetolinearfunctionapproximation,value-
basedmethods(operatingunderthestochasticlossassumption)arealsopopularinlinearMDPsand
1Wuetal.[2022]applyvariancereductiontechniquestogetbetterregretboundsinthetabularsetting,but
they use L 2-regularization instead of KL-regularization which does not align with practical PO algorithms
Schulmanetal.[2015,2017].
2havebeen shownto yieldoptimalregret[Jinetal., 2020a, Zanetteetal., 2020, Wagenmakeretal.,
2022a, Huetal., 2022, Heetal., 2023, Agarwaletal., 2023]. Anotherline ofworks[Ayoubetal.,
2020, Modietal., 2020, Caietal., 2020, Zhangetal., 2021, Zhouetal., 2021a,b, Heetal., 2022,
ZhouandGu,2022]studylinearmixtureMDPwhichisadifferentmodelthatisincomparablewith
linearMDP[Zhouetal.,2021b]. Finally,thereisarichlineofworksstudyingstatisticalproperties
ofRLwithmoregeneralfunctionapproximation[Munos,2005,Jiangetal.,2017,Dongetal.,2020,
Jinetal.,2021,Duetal.,2021],buttheseusuallydonotadmitcomputationallyefficientalgorithms.
2 Problem setup
Episodic Markov Decision Process (MDP). A finite-horizon episodic MDP is de-
fined by a tuple ( , ,x , ℓk K ,P,H) with , a set of states, , a set oM f actions,
X A 1 { }k=1 X A
H, decision horizon, x , an initial state (assumed to be fixed for simplicity),
1
P =(P ) ,P : ∈ ∆(X ), thetransitionprobabilities,and ℓk K , sequenceof loss
functionh sh su∈ c[H h] thah t ℓkX =× (A ℓk)→ X ,ℓk : [0,1], is a horizon d{ ep} enk= de1 nt immediate loss
h h∈[H] h X ×A→
functionfortakingactionaatstatexandhorizonhofepisodek. AsingleepisodekofanMDPis
asequence(xk,ak,ℓk(xk,ak)) ( [0,1])H suchthat
h h h h h h∈[H] ∈ X ×A×
Pr[xk =x′ xk =x,ak =a]=P (x′ x,a).
h+1 | h h h |
Forthelosses,weconsidertwosettings: stochasticandadversarial. Inthestochasticsetting,there
exists a fixed loss function ℓ = (ℓ ) ,ℓ : [0,1] such that ℓk is sampled i.i.d
h h∈[H] h
X × A →
fromadistributionwhoseexpectedvalueisdefinedbyℓ,i.e.,E ℓk(x,a) x,a = ℓ (x,a). Inthe
h | h
adversarialsetting,thelossfunctionsequence ℓk K ischosenbyanadaptiveadversary.
{ }k=1 (cid:2) (cid:3)
LinearMDP. AlinearMDPJinetal.[2020b]satisfiesallthepropertiesoftheaboveMDPbuthas
thefollowingadditionalstructuralassumptions.Thereisaknownfeaturemappingφ: A Rd
suchthatP (x′ x,a)=φ(x,a)T ψ (x′)whereψ : Rd areunknownparameteX rs.× Mor→ eover,
h h h
for all h [H],| k [K], there is an unknownvectoX r θ→ k Rd such that, in the adversarialcase,
ℓk(x,a) =∈ φ(x,a)T∈ θk,whileinthestochasticcase,θk =h θ∈ andℓ (x,a) = φ(x,a)T θ . Wemake
h h h h h h
thefollowingnormalizationassumptions,commonthroughouttheliterature:
1. φ(x,a) 1forallx X,a ;
k k≤ ∈ ∈A
2. θk √dforallh [H],k [K];
k hk≤ ∈ ∈
3. ψ ( ) = ψ (x) √dforallh [H];
k| h | X k k x∈X| h |k≤ ∈
P
where ψ (x) istheentry-wiseabsolutevalueofψ (x) Rd.Wefollowthestandardassumptionin
h h
| | ∈
theliteraturethattheactionspace isfinite.Inaddition,withoutlossofgenerality(seeCasseletal.
A
[2024]fordetails),wealsoassumethatthestatespace isfinite.
X
Policyandvalue. AstochasticMarkovpolicyπ = (π ) : [H] ∆( )isamapping
h h∈[H]
×X 7→ A
from a step and a state to a distribution over actions. Such a policy induces a distribution over
trajectories ι = (x ,a ) , i.e., sequencesof H state-action pairs. For f : ( )H R,
h h h∈[H]
X ×A →
whichmapstrajectoriestorealvalues,wedenotetheexpectationwithrespecttoιunderdynamics
P andpolicyπasE [f(ι)]. Similarly,wedenotetheprobabilityunderthisdistributionbyP [].
P,π P,π
·
WedenotetheclassofstochasticMarkovpoliciesasΠ . Foranypolicyπ Π ,horizonh [H]
M M
∈ ∈
andepisodek [K]wedefineitsloss-to-go,as
∈
H
V hk,π(x)=E
P,π
"
h′=hE[ℓk h′(x h′,a h′) |x h′,a h′] (cid:12)x
h
=x #,
X (cid:12)
(cid:12)
whichistheexpectedlossifonestartsfromstatex athorizonho(cid:12)fepisodekandfollowspolicy
∈X
π onwards. Notethattheinnerexpectationisonlyrelevantforstochasticlossesasitsargumentis
deterministicintheadversarialsetup. Theperformanceofapolicyinepisodek,alsoknownasits
value,ismeasuredbyitsexpectedcumulativelossVk,π(x ).
1 1
3Interaction protocol and regret. We consider a standard episodic regret minimization setting
where an algorithm performsK interactions with an MDP . For stochastic losses we consider
M
banditfeedback,wheretheagentobservesonlytheinstantaneouslossesalongitstrajectory,whilefor
adversariallossesweconsiderfull-informationfeedback,wheretheagentobservesthefulllossfunc-
tionℓkintheendofepisodek [K].Concretely,atthestartofeachinteraction/episodek [K],the
∈ ∈
agentspecifiesastochasticMarkovpolicyπk =(πk) . Subsequently,itobservesthetrajectory
h h∈[H]
ιksampledfromthedistributionP P,πk,and,eithertheindividualepisodelossesℓk h(xk h,ak h),h ∈[H]
inthecaseofbanditfeedback,ortheentirelossfunctionℓkinthecaseoffull-informationfeedback.
We measure the quality of any algorithm via its regret – the difference between the value of the
policiesπk generatedbythealgorithmandthatofthebestpolicyinhindsight,i.e.,
K K K
Regret=
Vk,πk
(x ) min Vk,π(x )=
Vk,πk
(x )
Vk,π⋆
(x ),
1 1 −π∈ΠM 1 1 1 1 − 1 1
k=1 k=1 k=1
X X X
wherethebestpolicyinhindsightisdenotedbyπ⋆ (knowntobeoptimalevenamongtheclassof
stochastichistory-dependentpolicies).
Notation. Throughoutthepaperφk =φ(xk,ak) Rd denotethestate-actionfeaturesathorizon
h h h ∈
hofepisodek. Inaddition, v = √vTAv. Hyper-parametersfollowthenotationsβ andη for
k kA z z
somez,andδ (0,1)denotesaconfidenceparameter. Finally,inthecontextofanalgorithm,
∈ ←
signsrefertocomputeoperationswhereas=signsdefineoperators,whichareevaluatedatspecific
pointsaspartofcomputeoperations.
3 The role ofvalueclipping
Beforepresentingourcontractiontechniqueandmainresults,wediscusstherolethatvalueclipping
playsinregretminimizationanditsapparentnecessityforlinearMDPs. Asastartingpoint,itisim-
portanttonotethat,whilecommonlyused[Azaretal.,2017,Luoetal.,2021],valueclippingisnot
strictlynecessaryintabularMDPs. Todemonstratethis,considerafairlystandardoptimisticValue
Iteration(VI)algorithmthatconstructssample-basedestimatesℓˆ,Pˆ witherrorbounds∆ℓ,∆P,de-
fines exploration bonuses b = (∆ℓ + H ∆P), and chooses a policy πˆ⋆ that is optimal in the
·
empiricalMDPwhosedynamicsarePˆ andlossesareℓˆ b. Thenitssingle-episoderegretmaybe
−
decomposedas
Vπˆ⋆
(x )
Vπ⋆
(x
)=Vπˆ⋆
(x )
Vˆπˆ⋆
(x
)+Vˆπˆ⋆
(x )
Vˆπ⋆
(x
)+Vˆπ⋆
(x )
Vπ⋆
(x ),
1 1 − 1 1 1 1 − 1 1 1 1 − 1 1 1 1 − 1 1
(i)−bias/costofoptimism (ii)−FTL/ERM (iii)−optimism
where Vˆ is the value under|the emp{irzical MDP}. N|ow, by de{fiznition of}πˆ⋆|, we have{zthat (ii)} 0.
≤
Next,usingastandardvaluedifferencelemma(lemma14inappendixB)wehavethat(i).band
(iii)=E ∆ℓ(x ,a ) b(x ,a )+ ∆P(x′ x ,a
)Vπ⋆
(x′) (1)
Pˆ,π⋆  h h − h h | h h h+1 
hX∈[H] x X′∈X
 
E ∆ℓ(x ,a )+H∆P(x ,a ) b(x ,a ) =0,
≤
Pˆ,π⋆

h h h h
−
h h

hX∈[H]
wheretheinequalityals ousedthatVπ⋆
[0,H].
Thefinalregretbound
isconcludedbysumming
h ∈
overk [K] and using a boundon harmonicsums. We note that a similar clipping-freemethod
∈
alsoworksfortabularPO(seeCasseletal.[2024]).
MovingontoLinearMDPs,onemightexpectasimilarapproachtowork. Unfortunately,thestan-
dard approachthat estimates the dynamicsbackup operatorsψ ,h [H] using regularizedleast-
h
∈
squares presents a significant challenge. This is because, unlike the tabular setting, the resulting
estimatePˆ ( x,a) = φ(x,a)T ψ ()(eq.(2))isnotguaranteedtoyieldavalidprobabilitydistri-
h h
· | ·
bution,i.e.,therecouldexistx ,a ,h [H]suchthat
∈X ∈A ∈
Pˆ ( x,a)b =c>1 and/or minPˆ (x′ x,a)<0.
k h ·| k1 x′∈X h |
4Pˆ isstillafinitesigned-measure,whichisenoughforthefirstequalityineq.(1)tohold. However,
sinceE couldcontainnegativeprobabilityterms,theinequalityineq.(1)doesnothold. These
Pˆ,π⋆
negativeprobabilitiesalsoseemtomakecalculatingπˆ⋆computationallyhard.Finally,theℓ norm
1
−
exceeding1maycauseterm(i)todependonH exponentially.Whilesomeoftheseissuescouldbe
mitigatedwithoutclipping,wearenotawareofamethodthatresolvesallsimultaneously.
TheuseofvalueclippingopensthepathforanalternativevaluedecompositionthatreplacesE
ineq.(1)withE P,π⋆ atthecostofalsoreplacingV hπ +⋆ 1withVˆ hπ +⋆ 1.Wethusneedthat |Vˆ hπ +⋆ 1|.HPˆ, fπ o⋆ r
theinequalityineq.(1)towork. Thisismadepossibleusingaclippingmechanismthatdecouples
thescaleofVˆπ⋆
fromthemagnitudeofthebonusesb, whichmaybemuchlargerwhentheerror
h+1
estimates∆ℓ,∆P arelarge.Thisistypicallyachievedbyaddingmax 0, totherecursiveformula
{ ·}
for the value function. A similar clippingapproachalso worksfor tabularPO and VI [Azaretal.,
2017,Luoetal.,2021],andevenforVIinlinearMDPs[Jinetal.,2020b].
However,thisisnotthecaseforPOinlinearMDPswhereShermanetal.[2023a]explainthatthis
typeofvalueclippingleadstoprohibitivecomplexityofthepolicyandvaluefunctionclasses,and
thussub-optimalregret. Theyovercomethisissueusingawarm-upbasedtruncationtechnique. In
whatfollows,wesuggestanalternativesolutionthatusesanovelnotionofcontractedfeaturesand
hasseveraladvantagesovertheirapproach(seediscussionattheendofsection4).
4 Algorithm and mainresult
We present Contracted Features Policy Optimization (CFPO; algorithm 1), a policy optimization
routineforregretminimizationinlinearMDPs. Thealgorithmoperatesinepochs,eachbeginning
whentheuncertaintyofthedynamicsestimationshrinksbyamultiplicativefactor,asexpressedby
the determinantof the covariancematrices Λk,h [H] (see line 13 for the definition of Λk and
h ∈ h
line4fortheepochchangecondition). Atthestartofeachepoche,weresetthepolicytoitsinitial
(uniform)state,anddefinethecontractedfeaturesφ¯ke,h [H](line6)bymultiplyingtheoriginal
h ∈
featureswithcoefficientsintherange[0,1],andthusshrinkingtheirdistancetotheorigin.Inspired
byideasfromZanetteetal.[2020],thesecoefficientsarechoseninverselyproportionaltothecurrent
uncertaintyoftheleastsquaresestimatorsineachstate-actionpair,essentiallydegeneratingtheMDP
inareasofhighuncertainty.Insideanepoch,atepisodek,wecomputetheestimatedrewardvector
θk (line14)andestimateddynamicsbackupoperatorsψk (eq.(2)). Then,weusetheseθk andψk
h h
to compute our Q-value estimates with the contracted features (eq. (3)), and run an online mirror
dbescent(OMD) updateover them (eq.(5)), i.e., runa pbolicy optimizationstep with respbectto tbhe
contractedempiricalMDP(moreonthisinsection5.1).
We note that the computational complexity of algorithm 1 is comparable to other algorithms for
regret minimization in linear MDPs, such as LSVI-UCB [Jinetal., 2020b]. The following is our
mainresultforalgorithm1(seethefullanalysisappendixA).
Theorem1. SupposethatwerunCFPO(algorithm1)withtheparametersdefinedintheorem9(in
appendixA).Then,withprobabilityatleast1 δ,wehave
−
Regret=O H4d3Klog(K)log(KH/δ)+ H5dKlog(K)log .
|A|
(cid:16)p p (cid:17)
Discussion. PolicyoptimizationalgorithmstypicallyentailrunningOMDoverestimatesQˆofthe
state-actionvaluefunctionQ,asineq.(5). Thecruxofthealgorithmisinobtainingsuchestimates
thatsatisfyanoptimisticconditionsimilartoeq.(1),whilealsokeepingthecomplexityofthepolicy
classbounded. AsdiscussedinShermanetal.[2023a],thelatterdependson
Qˆk′
(eq.(3))
k′∈[k] h
havingalowdimensionalrepresentationnearlyindependentofk. Althoughstandardunclippedesti-
P
matesadmitsucharepresentation,theylackotheressentialproperties(seediscussioninsection3).
On theotherhand, the standardclippingmethod, whichrestrictsthe valueto[0,H] betweeneach
backupoperation(see,e.g.,Jinetal.[2020b]),doesnotadmitthedesiredrepresentation.
Shermanetal. [2023a] overcame this issue by employing a warm-up phase based on a reward-
free pure exploration algorithm by Wagenmakeretal. [2022b] to obtain initial backup operators
ψ0,h [H] andsubsets ¯ ,h [H] suchthat: (i)foreveryx,a ¯ thebonuses(b
h ∈ Xh ⊆ X ∈ ∈ Xh ×A
insection3),whichareproportionaltotheestimationuncertaintyofthevaluebackupestimates,are
sbmall( 1);and(ii)forallpoliciesπ Π M,theprobabilityofreachinganyx,a /
h∈[H]
¯
h
≤ ∈ ∈∪ X ×A
5Algorithm1ContractedFeaturesPOforlinearMDPs
1: input: d,H,K, ,δ,β ,β ,η >0.
w b o
2: initialize:e A 1,Λ1 I,h [H].
←− h ← ∈
3: forepisodek =1,2,...,K do
4: ifk =1 or h [H], det(Λk) 2det(Λke) then
∃ ∈ h ≥ h
5: e e+1andk k.
e
← ←
6: φ¯k he(x,a)=φ(x,a) ·σ −β w kφ(x,a) k(Λk he)−1 +logK . {σ(z)=1/(1+exp( −z))}
7: πk(a x)=1/ fora(cid:16)llh [H],a ,x . (cid:17)
h | |A| ∈ ∈A ∈X
8: endif
9: Playπk andobservelosses(ℓk(xk,ak)) andtrajectoryιk =(xk,ak) .
h h h h∈[H] h h h∈[H]
10: Inthecaseoffull-informationfeedback:observeθk.
h
11: DefineVˆk (x)=0forallx .
H+1 ∈X
12: forh=H,...,1do
13: Λk+1 I+ φτ(φτ)T .
h ← τ∈[k] h h
(Λk)−1 φτℓτ(xτ,aτ), feedback=bandit
14: θk h P τ∈[k−1] h h h h
h ←(θ hk,
P
feedback=full.
15: ForanyV : R,x ,a define:
b X → ∈X ∈A
ψkV =(Λk)−1 φτV(xτ ), (2)
h h h h+1
τ∈[k−1]
X
Qbˆk h(x,a) =φ¯k he(x,a)T [θ hk+ψ hkVˆ hk +1] −β b kφ¯k he(x,a) k(Λk he)−1, (3)
Vˆk(x) = πk(a x)Qˆk(x,a), (4)
h h |b hb
a∈A
X
πk+1(a x) πk(a x)exp( η Qˆk(x,a)). (5)
h | ∝ h | − o h
16: endfor
17: endfor
issmall(.K−1/2). Toensurethattheoverallvalueestimatesremainbounded,theytruncate(zero
out)theQ-valueestimateofthesenearlyunreachablestate-actionpairs,anoperationthatallowsfor
alow-dimensionalrepresentationofthepolicies. Nonetheless,theirwarm-upapproachhasseveral
drawbacks.
• It runs for K = poly(d,H)√K episodes, contributing the leading term in their regret
0
guarantee;
• Itreliesonafirst-orderregretalgorithmbyWagenmakeretal.[2022a]thatisnotPO-based
andusesa computationallyhardvariance-awareCatoniestimatorforrobustmeanestima-
tion of the value backups, instead of the standard least-squares estimator. To maintain
computationalefficiency,theyuseanapproximateversionoftheestimator,losingafactor
of√dintheregret;
• Still,tothebestofourknowledge,eventheapproximateestimatormustbecomputedusing
binarysearchmethods,makingithardtoapplyinpracticalmethodsthattypicallyrelyon
gradient-basedcontinuousoptimizationtechniques;
• Itrunsseparatealgorithmsforeachhorizonh [H],usingonly1outofH samplesduring
∈
thewarm-upphase;
• Itisnotreward-aware,andthushastoexplorethespaceuniformlytoensurethattheuncer-
taintyissmallforallpolicies,whichcouldbehighlyprohibitiveinpractice.
OurfeaturecontractionapproachobtainsthedesiredboundedQ-valueestimatesandlow-complexity
policy class withoutrelying on a dedicated warm-upphase. Crucially, it only contributesa lower
ordertermofpoly(d,H)logK totheregretguarantee,thusimprovingtheoveralldependenceond
andH. Additionally,itusesallsamples,iseasytoimplement,andisreward-aware. Tounderstand
thebenefitofreward-awareness,consideranMDPwhereattheinitialstatetheagenthastwoactions,
6eachleadingtoa distinctMDP. Now,supposethatbothMDPs haveonlya single state andaction
forthefirstH/2stepswithoneMDPincurringalossof1inthesestepswhiletheotherincurring0
loss. NoticethatregardlessofthelastH/2steps, the0lossMDPwilloutperformthe1lossMDP.
Nonetheless,thereward-freewarm-up,whichdoesnotobservethelosses,willhavetofullyexplore
bothMDPs. Incontrast,ourreward-awareapproachwouldquicklystopexploringtheinferiorMDP,
leadingtobetterperformanceinpractice.
5 Analysis
Inthissection,weprovethemainclaimsofourresult. ForfulldetailsseeappendixA.Webeginby
introducingthemaintechnicaltoolforourcontractionmechanism–thecontractedMDP.
5.1 Contracted(sub)MDP
For anyMDP = ( , ,x , ℓk K ,P,H) andcontractioncoefficientsρ : [H]
[0,1]wedefineM acontrX actA ed(s1 ub{ )M} Dk= P1 ¯(ρ) = ( , ,x , ℓ¯k K ,P¯,H)wherea× sℓX¯k(× x,A a)→ =
ρ (x,a)ℓk(x,a)
[0,1]arethecontracteM dlossesandX P¯A (x′1 x{ ,a} )k ==1
ρ (x,a)P (x′
x,ah
) [0,1]
h h ∈ h | h h | ∈
are the contracted(sub) probabilitytransitions. Notice that the transitionsbeinga sub-probability
measureimpliesthat P (x′ x,a) 1ascomparedwithaprobabilitymeasurewherethis
holdswithequality. Forx a′∈ nyX Mh arkov| policy≤ π Π ,letV¯k,π(;ρ): R,h [H]betheloss-
to-go(or value)functPionsof thecontractedMD∈ P. IM n partich ular,· theseX ma→ y bedefi∈ nedbythe usual
backwardrecursion
V¯k,π(x;ρ)=E E[ℓ¯k(x,a) x,a]+ P¯ (x′ x,a)V¯k,π(x′;ρ) ,
h a∼π(·|x) " h | x′∈X h | h+1 #
X
with V¯k,π (x;ρ) = 0 for all x . The followingresultshows thatthe valueof anycontracted
H+1 ∈ X
MDPlowerboundsitsnon-contractedvariant.
Lemma2. Foranyρ:[H] [0,1],π Π ,h [H],k [K],andx wehavethat
M
×X ×A→ ∈ ∈ ∈ ∈X
V¯k,π(x;ρ) Vk,π(x).
h ≤ h
Proof. Theprooffollowsby backwardinductionon h [H +1]. Forthe base case h = H +1,
∈
bothvaluesare0andtheclaimholdstrivially.Nowsupposetheclaimholdsforh+1,thenwehave
thatforallx
∈X
V¯k,π(x;ρ)=E E[ℓ¯k(x,a) x,a]+ P¯ (x′ x,a)V¯k,π(x′;ρ)
h a∼π(·|x) " h | x′∈X h | h+1 #
X
E E[ℓk(x,a) x,a]+ P (x′ x,a)Vk,π(x′) =Vk,π(x). (cid:4)
≤ a∼π(·|x) " h | x′∈X h | h+1 # h
X
Next, for any epoch e [E], consider its contracted linear MDP (line 6 in algorithm 1) whose
∈
contractioncoefficients are ρk he(x,a) = σ −β w kφ(x,a) k(Λk he)−1 +logK . The following result
givesanupperboundontheperformanceg(cid:16)apbetweenthecontractedandno(cid:17)n-contractedvariants.
Lemma3. Foranye [E]andv Rdwehavethat
∈ ∈
(φ(x ,a ) φ¯ke(x ,a ))T v (4β2 φ(x ,a ) 2 +2K−1) φ(x ,a )T v .
h h − h h h ≤ wk h h k(Λk h)−1 h h
Proof. Wehavethat (cid:12) (cid:12)
(cid:12) (cid:12)
(φ(x h,a h) −φ¯k he(x h,a h))T v =σ(β w kφ(x h,a h) k(Λk he)−1 −logK) ·φ(x h,a h)T v
≤2(β w2 kφ(x h,a h) k2 (Λk he)−1 +K−1) φ(x h,a h)T v
(4β2 φ(x ,a ) 2 +2K−1)(cid:12)φ(x ,a )T v(cid:12),
≤ wk h h k(Λk h)−1 (cid:12) h h (cid:12)
wherethefirstrelationisbythepropertyofthesigmoid1 σ(x)=σ( x),th(cid:12)esecondisb(cid:12)yasimple
− − (cid:12) (cid:12)
algebricargumentthataquadraticfunctionboundsthesigmoid(lemma19inappendixB),andthe
lastrelationusesdet(Λk) 2det(Λke)byline4inalgorithm1(seelemma16inappendixB). (cid:4)
h (cid:22) h
7WenotethatthecorrespondingclaiminShermanetal.[2023a]showsthatforallπ Π
M
∈
E [(φ(x ,a ) φ¯ke(x ,a ))T v] ǫ φ(x ,a )T v , (6)
P,π h h − h h h ≤ h h
whereǫ K−1/2. Summingthisoverk [K]yieldsatermth(cid:12)atscalesas√(cid:12)K. Incontrast,weuse
(cid:12) (cid:12)
≈ ∈
astandardboundonellipticalpotentials(lemma15inappendixB)togetthat
(4β2 φ(xk,ak) 2 +2K−1).logK.
wk h h k(Λk h)−1
kX∈[K]
This implies that the cost of our contraction is significantly lower than the truncation of
Shermanetal.[2023a].Weachievethisreducedcostbyusingaquadratic(ratherthanlinear)bound
onthelogistic function. Thechallengein ourapproachis thattheaboveboundonlyholdsforthe
observedtrajectoriesratherthan forall policiesasin Shermanetal. [2023a]. In whatfollows, we
overcomethischallengeusinganovelregretdecomposition.
5.2 Regretbound
Foranyepoche [E], letK bethesetofepisodesthatitcontains,andletV¯k,π(x ;ρke)denote
∈ e 1 1
thevalueofitscontractedMDPasdefinedaboveandinline6ofalgorithm1. Weboundtheregret
as
Regret=
Vk,πk
(x )
Vk,π⋆
(x )
1 1 − 1 1
kX∈[K]
Vk,πk
(x )
V¯k,π⋆
(x ;ρke) (lemma2)
≤ 1 1 − 1 1
eX∈[E]k X∈Ke
= Vk,πk (x ) Vˆk(x )+ Vˆk(x ) V¯k,π⋆ (x ;ρke)
1 1 − 1 1 1 1 − 1 1
kX∈[K] eX∈[E]k X∈Ke
= Vk,πk (x ) Vˆk(x )
1 1 − 1 1
kX∈[K]
(i)−Bias/Costofoptimism
| {z }
+ E P¯ke,π⋆
"
Qˆk h(x h,a)(π hk(a |x h) −π h⋆(a |x h))
#
eX∈[E]hX∈[H] k X∈Kea X∈A
(ii)−OMDregret
+| E P¯ke,π⋆ Qˆk h(x h,{az h) −φ¯k he(x h,a h)T (θ hk+ψ hVˆ hk +1}) ,
e X∈[E]k X∈Keh X∈[H] h i
(iii)−Optimism
where the last relat|ion is by the extended value diff{ezrence lemma (see Shanietal. [20}20] and
lemma14inappendixB).ThisdecompositionisverysimilartothestandardoneforPOalgorithms,
butwiththecrucialdifferencethatterm(iii)dependsonthecontractedfeaturesφ¯ke(x ,a )instead
h h h
ofthetruefeaturesφ(x ,a ). Asaby-product,theexpectationinterms(ii)and(iii)istakenwith
h h
respecttothecontractedMDPinsteadofthetrueone.Thepurposeofthismodificationwillbemade
clearintheproofofoptimism(seelemma4).
Inwhatfollows,weboundeachtermdeterministically,conditionedonthefollowing“goodevent”:
E = k [K],h [H]: θk θk β ; (7)
1 ∀ ∈ ∈ k h− hkΛk h ≤ r
n o
E
2
= k ∈[K],h ∈[H]: k(ψ
h
−ψbhk)Vˆ hk
+1kΛk
h
≤β p, kQˆk
h+1k∞
≤2H . (8)
n o
E and E are error bounds on the loss and dynamics estimation, respectively. In the full feed-
1 2 b
back setting, E holds trivially with β = 0. In the bandit setting, it holds with high prob-
1 r
ability with β = O( dlog(KH/δ)) by well-established bounds for regularized least-squares
r
estimation [Abbasi-Yadkorietal., 2011]. Showing that E holds with high probability follows
2
p
8similarly to Shermanetal. [2023a], again using least-squares arguments but also using the con-
traction to ensure that Qˆk are bounded (see lemma 6 in appendix A for details), specifically
h
β =O(Hd log(KH/δ)). Theproofoftheorem1isconcludedbyboundingeachofthetermsin
p
theregretdecomposition,summingoverk [K]andusingastandardboundonellipticalpotentials
p ∈
(lemma 15 in appendixB). Term (ii) is boundedusing a standard Online Mirror Descent (OMD)
argument(lemma7inappendixA).
Optimismanditscost. Thefollowinglemmasboundterms(iii)and(i),respectively.
Lemma4(Optimism). Supposethateqs.(7)and(8)hold,then
Qˆk(x,a) φ¯ke(x,a)T (θk+ψ Vˆk ) 0 , h [H],k [K],x ,a .
h − h h h h+1 ≤ ∀ ∈ ∈ ∈X ∈A
Proof. Wehavethat
Qˆk(x,a) φ¯ke(x,a)T (θ +ψ Vˆk )=φ¯ke(x,a)T (θk θ +(ψk ψ )Vˆk )
h − h h h h+1 h h− h h− h h+1
−β b kφ¯k he b(x,a) k(Λk he)− b1
≤(β r+β p) kφ¯k he(x,a) kΛk h−1 −β b kφ¯k he(x,a) k(Λk he)−1
≤(β r+β p −β b) kφ¯k he(x,a) k(Λk he)−1 =0,
wherethefirstrelationisbydefinitionofQˆk(eq.(3)inalgorithm1),thesecondrelationisbyeqs.(7)
h
and(8)togetherwithCauchy-Schwarz,thethirdrelationfollowssinceΛke Λk andthelastoneis
h (cid:22) h
byourchoiceβ =β +β (seetheorem9inappendixAforhyper-parameterchoices). (cid:4)
b r p
NoticethatthestandardPOdecompositionwouldhaverequiredthatweboundthenon-contracted
expressionE P,π⋆[Qˆk h(x,a) −φ(x,a)T (θ hk+ψ hVˆ hk +1)].InShermanetal.[2023a]thegapbetween
thisargumentandthatoflemma4canbeboundedusingeq.(6). However,theequivalentargument
forourcontractionislemma3,whichisboundedonlyforπk andnotforanypolicyπ Π .
M
∈
Lemma5(Costofoptimism). Supposethateqs.(7)and(8)hold,thenforeveryk [K]
∈
V
1k,πk
(x 1) −Vˆ 1k(x 1) ≤3(β r+β p)E P,πk

kφ(x h,a h) k(Λk h)−1

hX∈[H]
 
+16Hβ w2E P,πk

kφ(x h,a h) k2 (Λk h)−1 +16H2K−1.
hX∈[H]
Proof. First,bylemma14inappendixB,avaluedifferencelemmabyShanietal.[2020],
V 1k,πk (x 1) −Vˆ 1k(x 1)=E P,πk  φ(x h,a h)T θ h+ψ hVˆ hk +1 −Qˆk k(x h,a h) .
hX∈[H] (cid:16) (cid:17)
Now,usinglemma3withv =θk+ψ  Vˆk wehavethat φ(x,a)T v 4H (byeq.(8)) andthus
h h h+1 | |≤
[φ(x ,a ) φ¯ke(x ,a )]T θ +ψ Vˆk 16Hβ2 φ(x ,a ) 2 +16H2K−1.
h h − h h h h h h+1 ≤ wk h h k(Λk h)−1
Wecanthusconcludetheproofu(cid:16)singstandarda(cid:17)rgumentstoshowthat
φ¯ke(x ,a )T θ +ψ Vˆk Qˆk(x ,a )
h h h h h h+1 − k h h
=φ¯k h(cid:16) e(x h,a h)T θ hk(cid:17) −θ hk+(ψ h −ψ hk)Vˆ hk +1 +β b kφ¯k he(x h,a h) k(Λk he)−1 (eq.(3))
≤(β r +β p) kφ¯k h(cid:16)e(x h,a bh) k(Λk h)−1 + bβ b kφ¯k he((cid:17) x h,a h) k(Λk he)−1
(Cauchy-Schwarz,eqs.(7)and(8))
3(β +β ) φ¯ke(x ,a ) (det(Λk) 2det(Λke),β =β +β )
≤ r p k h h h k(Λk h)−1 h ≤ h b r p
3(β +β ) φ(x ,a ) , (σ(x) [0,1], x R)
≤
r p
k
h h k(Λk h)−1
∈ ∀ ∈
asdesired. (cid:4)
9Acknowledgments and DisclosureofFunding
ThisprojecthasreceivedfundingfromtheEuropeanResearchCouncil(ERC)undertheEuropean
Union’sHorizon2020researchandinnovationprogram(grantagreementNo. 101078075). Views
andopinionsexpressedarehoweverthoseoftheauthor(s)onlyanddonotnecessarilyreflectthose
of the European Union or the European Research Council. Neither the European Union nor the
grantingauthoritycanbeheldresponsibleforthem. Thisworkreceivedadditionalsupportfromthe
IsraelScienceFoundation(ISF,grantnumber2549/19),theLenBlavatnikandtheBlavatnikFamily
Foundation,andtheIsraeliVATATdatasciencescholarship.
References
Y.Abbasi-Yadkori,D.Pál,andC.Szepesvári. Improvedalgorithmsforlinearstochasticbandits. In
AdvancesinNeuralInformationProcessingSystems,pages2312–2320,2011.
A. Agarwal, Y. Jin, andT. Zhang. Vo q l: Towardsoptimalregretin model-freerlwith nonlinear
functionapproximation. InTheThirtySixthAnnualConferenceonLearningTheory,pages987–
1063.PMLR,2023.
A.Ayoub,Z.Jia,C.Szepesvari,M.Wang,andL.Yang. Model-basedreinforcementlearningwith
value-targeted regression. In International Conference on Machine Learning, pages 463–474.
PMLR,2020.
M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In
InternationalConferenceonMachineLearning,pages263–272.PMLR,2017.
Q. Cai, Z. Yang, C. Jin, and Z. Wang. Provably efficient exploration in policy optimization. In
InternationalConferenceonMachineLearning,pages1283–1294.PMLR,2020.
A.Cassel,H.Luo,A.Rosenberg,andD.Sotnikov. Near-optimalregretinlinearmdpswithaggre-
gatebanditfeedback. arXivpreprintarXiv:2405.07637,2024.
A.Cohen,T.Koren,andY.Mansour. Learninglinear-quadraticregulatorsefficientlywithonly√T
regret. InInternationalConferenceonMachineLearning,pages1300–1309,2019.
Y.Dai,H.Luo,C.-Y.Wei,andJ.Zimmert. Refinedregretforadversarialmdpswithlinearfunction
approximation. In International Conference on Machine Learning, pages 6726–6759. PMLR,
2023.
C. Dann, C.-Y. Wei, and J. Zimmert. Best of both worlds policy optimization. In International
ConferenceonMachineLearning,pages6968–7008.PMLR,2023.
K.Dong,J.Peng,Y.Wang,andY.Zhou. Root-n-regretforlearninginmarkovdecisionprocesses
with function approximationand low bellman rank. In Conference on Learning Theory, pages
1554–1557.PMLR,2020.
S.Du,S.Kakade,J.Lee,S.Lovett,G.Mahajan,W.Sun,andR.Wang.Bilinearclasses:Astructural
frameworkforprovablegeneralizationinrl. InInternationalConferenceonMachineLearning,
pages2826–2836.PMLR,2021.
E. Even-Dar,S. M.Kakade, andY. Mansour. Onlinemarkovdecisionprocesses. Mathematicsof
OperationsResearch,34(3):726–736,2009.
T.Haarnoja,A.Zhou,P.Abbeel,andS.Levine.Softactor-critic:Off-policymaximumentropydeep
reinforcementlearningwithastochasticactor.InInternationalConferenceonMachineLearning,
pages1861–1870.PMLR,2018.
J. He, D. Zhou, and Q. Gu. Near-optimalpolicy optimizationalgorithmsfor learning adversarial
linearmixturemdps. InInternationalConferenceonArtificialIntelligenceandStatistics,pages
4259–4280.PMLR,2022.
J. He, H. Zhao, D. Zhou, and Q. Gu. Nearly minimax optimal reinforcementlearning for linear
markov decision processes. In International Conference on Machine Learning, pages 12790–
12822.PMLR,2023.
10P.Hu,Y.Chen,andL.Huang. Nearlyminimaxoptimalreinforcementlearningwithlinearfunction
approximation. In International Conference on Machine Learning, pages 8971–9019. PMLR,
2022.
T.Jaksch,R.Ortner,andP.Auer. Near-optimalregretboundsforreinforcementlearning. Journal
ofMachineLearningResearch,11:1563–1600,2010.
N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision
processes with low bellman rank are pac-learnable. In International Conference on Machine
Learning,pages1704–1713.PMLR,2017.
C. Jin, Z. Allen-Zhu,S. Bubeck, andM. I. Jordan. Isq-learningprovablyefficient? Advancesin
neuralinformationprocessingsystems,31,2018.
C.Jin,T.Jin,H.Luo,S.Sra,andT.Yu.Learningadversarialmarkovdecisionprocesseswithbandit
feedbackandunknowntransition.InInternationalConferenceonMachineLearning,pages4860–
4869.PMLR,2020a.
C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provablyefficientreinforcementlearningwith linear
functionapproximation. InConferenceonLearningTheory,pages2137–2143.PMLR,2020b.
C. Jin, Q. Liu, and S. Miryoosefi. Bellman eluder dimension: New rich classes of rl problems,
andsample-efficientalgorithms. Advancesinneuralinformationprocessingsystems,34:13406–
13418,2021.
F. Kong, X. Zhang, B. Wang, and S. Li. Improvedregret boundsfor linear adversarial mdps via
linearoptimization. arXivpreprintarXiv:2302.06834,2023.
T.Lancewicki,A.Rosenberg,andY.Mansour.Learningadversarialmarkovdecisionprocesseswith
delayedfeedback. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,
pages7281–7289,2022.
T. Lancewicki, A. Rosenberg, and D. Sotnikov. Delay-adaptedpolicy optimizationand improved
regret for adversarial MDP with delayed bandit feedback. In A. Krause, E. Brunskill, K. Cho,
B.Engelhardt,S.Sabato,andJ.Scarlett,editors,InternationalConferenceonMachineLearning,
ICML 2023,23-29July 2023,Honolulu, Hawaii, USA,volume 202of Proceedingsof Machine
LearningResearch,pages18482–18534.PMLR,2023.
H. Liu, C.-Y.Wei, andJ.Zimmert. Towardsoptimalregretinadversariallinearmdpswithbandit
feedback. arXivpreprintarXiv:2310.11550,2023.
H.Luo,C.-Y.Wei,andC.-W.Lee. Policyoptimizationinadversarialmdps: Improvedexploration
viadilatedbonuses. AdvancesinNeuralInformationProcessingSystems,34,2021.
V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. nature,518(7540):529–533,2015.
A. Modi, N. Jiang, A. Tewari, and S. Singh. Sample complexityof reinforcementlearning using
linearly combinedmodelensembles. In InternationalConference on ArtificialIntelligenceand
Statistics,pages2010–2020.PMLR,2020.
R.Munos.Errorboundsforapproximatevalueiteration.InProceedingsoftheNationalConference
on Artificial Intelligence, volume 20, page 1006. Menlo Park, CA; Cambridge, MA; London;
AAAIPress;MITPress;1999,2005.
G.NeuandJ.Olkhovskaya.Onlinelearninginmdpswithlinearfunctionapproximationandbandit
feedback. AdvancesinNeuralInformationProcessingSystems,34:10407–10417,2021.
G.Neu, A. György,C. Szepesvári,andA. Antos. Onlinemarkovdecisionprocessesunderbandit
feedback. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta,
editors, Advances in Neural Information Processing Systems 23: 24th Annual Conference on
NeuralInformationProcessingSystems2010.Proceedingsofameetingheld6-9December2010,
Vancouver,BritishColumbia,Canada,pages1804–1812.CurranAssociates,Inc.,2010a.
11G.Neu,A.György,C.Szepesvári,etal. Theonlineloop-freestochasticshortest-pathproblem. In
COLT,volume2010,pages231–243.Citeseer,2010b.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
K. Slama, A.Ray, etal. Traininglanguagemodelsto followinstructionswithhumanfeedback.
AdvancesinNeuralInformationProcessingSystems,35:27730–27744,2022.
A.RosenbergandY.Mansour. Onlinestochasticshortestpathwithbanditfeedbackandunknown
transition function. In Advances in Neural Information Processing Systems, pages 2209–2218,
2019a.
A.RosenbergandY.Mansour.Onlineconvexoptimizationinadversarialmarkovdecisionprocesses.
InInternationalConferenceonMachineLearning,pages5478–5486.PMLR,2019b.
A. Rosenberg, A. Cohen, Y. Mansour, and H. Kaplan. Near-optimalregret boundsfor stochastic
shortestpath.InInternationalConferenceonMachineLearning,pages8210–8219.PMLR,2020.
J.Schulman,S.Levine,P.Abbeel,M.Jordan,andP.Moritz. Trustregionpolicyoptimization. In
Internationalconferenceonmachinelearning,pages1889–1897.PMLR,2015.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXivpreprintarXiv:1707.06347,2017.
L. Shani, Y. Efroni, A. Rosenberg, and S. Mannor. Optimistic policy optimization with bandit
feedback. InInternationalConferenceonMachineLearning,pages8604–8613.PMLR,2020.
U. Sherman, A. Cohen, T. Koren, and Y. Mansour. Rate-optimal policy optimization for linear
markovdecisionprocesses. arXivpreprintarXiv:2308.14642,2023a.
U.Sherman,T.Koren,andY.Mansour. Improvedregretforefficientonlinereinforcementlearning
with linear function approximation. In International Conference on Machine Learning, pages
31117–31150.PMLR,2023b.
N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F.
Christiano. Learningtosummarizewithhumanfeedback. AdvancesinNeuralInformationPro-
cessingSystems,33:3008–3021,2020.
A.J.Wagenmaker,Y.Chen,M.Simchowitz,S.Du,andK.Jamieson. First-orderregretinreinforce-
mentlearningwithlinearfunctionapproximation:Arobustestimationapproach.InInternational
ConferenceonMachineLearning,pages22384–22429.PMLR,2022a.
A.J.Wagenmaker,Y.Chen,M.Simchowitz,S.Du,andK.Jamieson. Reward-freerlisnoharder
than reward-aware rl in linear markov decision processes. In InternationalConference on Ma-
chineLearning,pages22430–22456.PMLR,2022b.
T. Wu, Y. Yang, H. Zhong, L. Wang, S. Du, and J. Jiao. Nearlyoptimalpolicyoptimizationwith
stable at any time guarantee. In InternationalConference on Machine Learning, pages24243–
24265.PMLR,2022.
A. Zanette and E. Brunskill. Tighter problem-dependentregret bounds in reinforcementlearning
withoutdomainknowledgeusingvaluefunctionbounds.InInternationalConferenceonMachine
Learning,pages7304–7312.PMLR,2019.
A.Zanette,D.Brandfonbrener,E.Brunskill,M.Pirotta,andA.Lazaric. Frequentistregretbounds
forrandomizedleast-squaresvalueiteration.InInternationalConferenceonArtificialIntelligence
andStatistics,pages1954–1964.PMLR,2020.
Z. Zhang, Y. Zhou, and X. Ji. Almost optimal model-free reinforcement learningvia reference-
advantagedecomposition.AdvancesinNeuralInformationProcessingSystems,33:15198–15207,
2020.
Z.Zhang,J.Yang,X.Ji,andS.S.Du. Improvedvariance-awareconfidencesetsforlinearbandits
and linear mixture mdp. Advances in Neural Information Processing Systems, 34:4342–4355,
2021.
12Z.Zhang,J.D.Lee,Y.Chen,andS.S.Du.Horizon-freeregretforlinearmarkovdecisionprocesses.
arXivpreprintarXiv:2403.10738,2024.
H.ZhongandT.Zhang. Atheoreticalanalysisofoptimisticproximalpolicyoptimizationinlinear
markovdecisionprocesses. AdvancesinNeuralInformationProcessingSystems,36,2023.
D.ZhouandQ.Gu.Computationallyefficienthorizon-freereinforcementlearningforlinearmixture
mdps. Advancesinneuralinformationprocessingsystems,35:36337–36349,2022.
D. Zhou, Q. Gu, and C. Szepesvari. Nearly minimax optimal reinforcement learning for linear
mixturemarkovdecisionprocesses.InConferenceonLearningTheory,pages4532–4576.PMLR,
2021a.
D. Zhou, J. He, and Q. Gu. Provably efficient reinforcement learning for discounted mdps with
featuremapping.InInternationalConferenceonMachineLearning,pages12793–12802.PMLR,
2021b.
A.ZiminandG.Neu. Onlinelearninginepisodicmarkoviandecisionprocessesbyrelativeentropy
policysearch. In Advancesin NeuralInformationProcessing Systems26: 27thAnnualConfer-
enceonNeuralInformationProcessingSystems2013.ProceedingsofameetingheldDecember
5-8,2013,LakeTahoe,Nevada,UnitedStates,pages1583–1591,2013.
13A Analysis
Webeginbydefiningaso-called“goodevent”,followedbyoptimism,costofoptimism,andPolicy
Optimizationcost. Weconcludewiththeproofoftheorem9.
Goodevent. We definethe followinggoodeventE = 3 E , overwhichthe regretis deter-
g i=1 i
ministicallybounded:
T
E = k [K],h [H]: θk θk β ; (eq.(7))
1 ∀ ∈ ∈ k h− hkΛk h ≤ r
n o
E
2
= k ∈[K],h ∈[H]: k(ψ
h
−ψbhk)Vˆ hk
+1kΛk
h
≤β p, kQˆk
h+1k∞
≤β
Q
; (eq.(8))
n o
b 6
E
3
=

E P,πk[Y k]
≤
2Y k+4H(3(β r+β p)+4β Qβ w2)log δ. (9)
kX∈[K] kX∈[K] 
whereY = 3(β +β ) φ(x ,a ) +4β β2 φ(x ,a ) 2 .
k h∈[H] r p k h h k(Λk h)−1 Q wk h h k(Λk h)−1
Lemma6(GPoodevent). Considertheparametersettingoftheorem9. Ifη 1,β2 K/(32Hd)
o ≤ w ≤
thenPr[E ] 1 δ.
g
≥ −
ProofinappendixA.1.
Policy online mirror descent. We use standard online mirror descent arguments to bound the
localregretineachstate.
Lemma7(OMD). SupposethatthegoodeventE holds(eqs.(7),(8)and(9))andsetη 1/β ,
g o Q
≤
then
log
Qˆk(x,a)(π⋆(a x) πk(a x)) |A| +η β2 , e [E],h [H],x .
h h | − h | ≤ η o Q ∀ ∈ ∈ ∈X
o
k X∈Kea X∈A k X∈Ke
Proof. Noticethatthepolicyπk isresetatthebeginningofeveryepoch. Then,thelemmafollows
directlybylemma13withy (a)= Qˆk(x,a),x (a)=πk(a x)andnotingthat Qˆk(x,a) β
t − h t h | | h |≤ Q
byeq.(8). (cid:4)
Epochschedule. Thealgorithmoperatesinepochs. Atthebeginningofeachepoch,thepolicyis
resettobeuniformlyrandom. WedenotethetotalnumberofepochsbyE,thefirstepisodewithin
epoch e by k , and the set of episodes within epoch e by K . The following lemma bounds the
e e
numberofepochs.
Lemma8. ThenumberofepochsEisboundedby(3/2)dHlog(2K).
Proof. Let = e1,e2,... be the epochswherethe conditiondet(Λk) 2det(Λke) was trig-
Th { h h } h ≥ h
geredinline4ofalgorithm1. Thenwehavethat
2det(Λke−1)
,e
det(Λke) h ∈Th
h ≥(det(Λ hke−1) ,otherwise.
Unrollingthisrelation,wegetthat
det(ΛK) 2|Th|−1detI =2|Th|−1,
h ≥
andchangingsides,andtakingthelogarithmwegetthat
1+log det ΛK
|Th |≤ 2 h
1+dlog Λ(cid:0)K (cid:1) (det(A) A d)
≤ 2k hk ≤k k
K−1
1+dlog 1+ φk 2 (triangleinequality)
≤ 2 k hk !
k=1
X
1+dlog K ( φk 1)
≤ 2 k hk≤
(3/2)dlog2K.
≤
14Weconcludethat
E = (3/2)dHlog(2K). (cid:4)
h∈[H] h h
| ∪ T |≤ |T |≤
(cid:0) (cid:1)
hX∈[H]
Regretbound.
Theorem9. Supposethatwerunalgorithm1withparameters
3dHlog(2K)log
η = |A|,β =β +β ,β =4(β +β )logK,
o Kβ2 b r p w r p
s Q
whereβ = 2 2dlog(6KH/δ),β = 28Hd log(10K5H/δ),β = 2H. Thenwith probability
r p Q
atleast1 δweincurregretatmost
− p p
Regret 264 Kd3H4log(2K)log(10K5H/δ)+8 KdH5log(2K)log
≤ |A|
12K
p +64H2dmax β2,log log p
{ w |A|} δ
=O( Kd3H4log(K)log(KH/δ)+ KdH5log(K)log ).
|A|
p p
Proof. First,ifβ2 >K/(32Hd)orη 1/β then
w ≥ Q
Regret KH 32H2dmax β2,log log(2K),
≤ ≤ { w |A|}
and the proofis concluded. Otherwise, if β2 K/(32Hd)thensuppose thatthe goodeventE
w ≤ g
holds(eqs.(7),(8)and(9)). Bylemma6,thisholdswithprobabilityatleast1 δ. Foranyepoch
−
e [E],letK bethesetofepisodesthatitcontains,andletV¯k,π(x ;ρke)denotethevalueofits
∈ e 1 1
contractedMDPasdefinedinsection5.1andline6ofalgorithm1. Weboundtheregretas
Regret=
Vk,πk
(x )
Vk,π⋆
(x )
1 1 − 1 1
kX∈[K]
Vk,πk
(x )
V¯k,π⋆
(x ;ρke) (lemma2)
≤ 1 1 − 1 1
eX∈[E]k X∈Ke
= Vk,πk (x ) Vˆk(x )+ Vˆk(x ) V¯k,π⋆ (x ;ρke)
1 1 − 1 1 1 1 − 1 1
kX∈[K] eX∈[E]k X∈Ke
= Vk,πk (x ) Vˆk(x )
1 1 − 1 1
kX∈[K]
(i)−Bias/Costofoptimism
| {z }
+ E P¯ke,π⋆
"
Qˆk h(x h,a)(π hk(a |x h) −π h⋆(a |x h))
#
eX∈[E]hX∈[H] k X∈Kea X∈A
(ii)−OMDregret
+| E P¯ke,π⋆ Qˆk h(x h,{az h) −φ¯k he(x h,a h)T (θ hk+ψ hVˆ hk +1}) ,
eX∈[E]k X∈KehX∈[H] h i
(iii)−Optimism
| {z }
where the last relation is by the extended value difference lemma (see Shanietal. [2020] and
lemma14inappendixB).
15Forterm(i),weuselemma5asfollows.
(i)
≤
E P,πk

3(β r+β p) kφ(x h,a h) k(Λk h)−1 +8β Qβ w2 kφ(x h,a h) k2 (Λk h)−1 +8Hβ Q
kX∈[K] hX∈[H]
 
6
6(β +β ) φ(x ,a ) +16β β2 φ(x ,a ) 2 +20Hβ β2 log
≤  r p k h h k(Λk h)−1 Q wk h h k(Λk h)−1  Q w δ
kX∈[K] hX∈[H]
 (eq.(9),β w 120(β r+β p))
≥
6
6(β +β )H 2Kdlog(2K)+32β β2Hdlog(2K)+20Hβ β2 log (lemma15)
≤ r p Q w Q w δ
p 12K
6(β +β )H 2Kdlog(2K)+32Hdβ β2 log .
≤ r p Q w δ
p
Bylemmas7and8(withourchoiceofη )wehave
o
logA
(ii)
≤
E P¯ke,π⋆
" η o
+η o β Q2 #≤4Hβ Q KdHlog(2K)log |A|.
hX∈[H]eX∈[E] k X∈Ke
p
Bylemma4(iii) 0. Puttingallboundstogether,wegetthat
≤
12K
Regret 6(β +β )H 2Kdlog(2K)+32Hdβ β2 log +4Hβ KdHlog(2K)log
≤ r p Q w δ Q |A|
p p 12K
264 Kd3H4log(2K)log(10K5H/δ)+8 KdH5log(2K)log +64H2dβ2 log
≤ |A| w δ
=O( pKd3H4log(K)log(KH/δ)+ KdHp5log(K)log ). (cid:4)
|A|
p p
A.1 Proofsofgoodevent
We beginbydefiningfunctionclasses and propertiesnecessary forthe uniformconvergenceargu-
mentsoverthevaluefunctions. We thenproceedto definea proxygoodevent,whosehighproba-
bilityoccurrenceisstraightforwardtoprove.Wethenshowthattheproxyeventimpliesthedesired
goodevent.
Valueandpolicyclasses. WedefinethefollowingclassofrestrictedQ-functions:
(C ,C ,C )= Qˆ(, ;β,w,Λ, ) β [0,C ], w C ,(2K)−1I Λ I, Qˆ(, ;w,Λ, ) C ,
Q β w Q · · Z | ∈ β k k≤ w (cid:22) (cid:22) k · · Z k∞ ≤ Q
n o
wbhereQˆ(x,a;β,w,Λ) = [wT φ(x,a) β φ(x,a) ] σ( β φ(x,a) +logK). Next,wede-
− k kΛ · − w k kΛ
finethefollowingclassofsoft-maxpolicies:
Π(C ,C )= π( ;Qˆ) Qˆ (C ,C , ) ,
β w β w
·|· | ∈Q ∞
n o
whereπ(a x;Qˆ) = exp(Qˆ(x,a)) . Finally,wedefibnethefollowingclassofrestrictedvalue
| Pa′∈Aexp(Qˆ(x,a′))
functions:
(C ,C ,C )= Vˆ(;π,Qˆ) π Π(C K,C K,C ),Qˆ (C ,C ,C ) , (10)
β w Q β w Q β w Q
V · | ∈ ∈Q
n o
where Vˆb(x;π,Qˆ) = π(a x)Qˆ(x,a). The following lemma pbrovides the bound on the
a∈A |
coveringnumberofthevaluefunctionclassdefinedabove.
P
Lemma10. Foranyǫ,C >0,C ,C 1,wehave
w β Q
≥
log (C ,C ,C ) 6d2log(1+4(√192K3C C β )(KC +KC +√d)/ǫ),
ǫ β w Q Q β w β w
N V ≤
(cid:16) (cid:17)
where ǫistbhecoveringnumberofaclassinsupremumdistance.
N
16Proof. WebeginbyshowingthattheclassofQfunctionisLipschitzinitsparameters. Foreaseof
notation,denotey =φ(x,a). Then
Q(x,a;β,w,Λ) = y σ( β y +logK) 1 (σ() [0,1], y 1,Λ I)
k∇β k k kΛ· − w k kΛ ≤ · ∈ k k≤ (cid:22)
Qˆ(x,a;β,w,Λ) = y σ( β y +logK) 1 (σ() [0,1], y 1)
k∇θ k k · − w k kΛ k≤ · ∈ k k≤
Q(x,a;β,w,Λ) Q(x,a;β,w,Λ′)
| − |
β y y σ( β y +logK)
≤ |k kΛ−k kΛ′|· − w k kΛ
+β y σ( β y +logK) σ( β y +logK)
k kΛ′| − w k kΛ − − w k kΛ′ |
β (Λ1/2 (Λ′)1/2)y +ββ y (Λ1/2 (Λ′)1/2)y
≤ k − k
w
k
kΛ′
k − k
( ,σ()1-Lipschitz,σ [0,1])
k·k · ∈
2ββ Λ1/2 (Λ′)1/2 ( y 1,Λ I,β 1)
w w
≤ k − k k k≤ (cid:22) ≥
√2Kββ Λ Λ′ (lemma17,Λ,Λ′ (2K)−1I)
w
≤ k − k (cid:23)
√2Kββ Λ Λ′ . ( )
≤ w k − kF k·k≤k·kF
Wethushavethatforanysuchy
Q(x,a;β,w,Λ) Q(x,a;β′,w′,Λ′)
| − |
Q(x,a;β,w,Λ) Q(x,a;β′,w,Λ) + Q(x,a;β′,w,Λ) Q(x,a;β′,w′,Λ)
≤| − | | − |
+ Q(x,a;β′,w′,Λ) Q(x,a;β′,w′,Λ′)
| − |
β β′ + w w′ +√2Kββ Λ Λ′
≤| − | k − k w k − kF
3( w w′ 2+ β β′ 2+(√2Kββ )2 Λ Λ′ 2)
≤ k − k | − | w k − kF
q
max 3,√6Kββ ( w w′ 2+ β β′ 2+ Λ Λ′ 2)
≤ { w } k − k | − | k − kF
=max 3,√6Kββ q (β,w,Λ) (β′,w′,Λ′) ,
w
{ }k − k
where (β,w,Λ) is a vectorization of the parameters. Assuming that C 1, we conclude that
β
≥
(C ,C ,C )is√6KC β Lipschitzinsupremumnorm,i.e.,
β w Q β w
Q −
Qˆ(, ;β,w,Λ) Qˆ′(, ;β′,w′,Λ′) √6KC β (β,w,Λ) (β′,w′,Λ′) .
b k · · − · · k∞ ≤ β w k − k
Next, notice that our policy class Π(C K,C K) is a soft-max over the Q functions thus fitting
β w
Lemma12ofShermanetal.[2023a].Weconcludethatthepolicyclassis√24K3C β Lipschitz,
β w
−
inℓ norm,i.e.,
1
−
π( x;β,w,Λ) π( x;β′,w′,Λ′) √24K3C β (β,w,Λ) (β′,w′,Λ′) .
k ·| − ·| k1 ≤ β w k − k
Now,letV,V′ (C ,C ,C )andθ =(β ,w ,Λ ,β ,w ,Λ ),θ′ =(β′,w′,Λ′,β′,w′,Λ )
R2(1+d+d2)be∈ theV irreβ specw tiveQ parameters.W1 eha1 vet1 hat2 for2 allx2 1 1 1 2 2 2 ∈
∈X
b
V(x;π,Qˆ) V(x;π′,Qˆ′) V(x;π,Qˆ) V(x;π,Qˆ′) + V(x;π,Qˆ′) V(x;π′,Qˆ′) .
| − |≤| − | | − |
(i) (ii)
Forthefirstterm
| {z } | {z }
(i)= π(a x)(Qˆ(x,a;β ,w ,Λ ) Qˆ(x,a;β′,w′,Λ′))
(cid:12) | 2 2 2 − 2 2 2 (cid:12)
(cid:12)a X∈A (cid:12)
(cid:12) (cid:12)
(cid:12) π(a x) Qˆ(x,a;β ,w ,Λ ) Qˆ(x,a;β′,w′,Λ′) (cid:12) (triangleinequality)
≤(cid:12) | 2 2 2 − 2 2 2 (cid:12)
a X∈A (cid:12) (cid:12)
(cid:12) (cid:12)
√6KC β (cid:12)(β ,w ,Λ ) (β′,w′,Λ′) . (Qˆ is√6K(cid:12)C β -Lipschitz,Cauchy-Schwarz)
≤ β w k 2 2 2 − 2 2 2 k β w
Forthesecondterm
(ii)= Qˆ′(x,a)(π(a x) π′(a x)) C π( x) π( x)
(cid:12) | − | (cid:12)≤ Q k ·| − ·| k1
(cid:12)a X∈A (cid:12)
(cid:12) (cid:12)
(cid:12)√96K3C C β (β ,w ,Λ ) (β(cid:12)′,w′,Λ′) ,
≤(cid:12) Q β w k 1 1 1 − (cid:12)1 1 1 k
17wherethefirsttransitionusedthat Q C forallQ (C ,C ,C )andthesecondused
k k∞ ≤ Q ∈ Q β w Q
the Lipschitz property of the policy class shown above. Combining the terms and assuming that
C Q 1wegetthat b
≥
V(x;π,Qˆ) V(x;π′,Qˆ′) √96K3C C β (β ,w ,Λ ) (β′,w′,Λ′)
| − |≤ Q β w k 1 1 1 − 1 1 1 k
+√96K3C C β (β ,w ,Λ ) (β′,w′,Λ′)
Q β w k 2 2 2 − 2 2 2 k
√192K3C C β θ θ′ ,
Q β w
≤ k − k
implyingthat (C ,C ,C )is√192K3C C β Lipschitzinsupremumnorm. Finally,notice
β w Q Q β w
V −
that
b
θ β + β + w + w + Λ + Λ 2KC +2KC +2√d,
k k≤| 1 | | 2 | k 1 k k 2 k k 1 kF k 2 kF ≤ β w
andapplyinglemma24concludestheproof. (cid:4)
Proxygoodevent. WedefineaproxygoodeventE¯ =E E¯ E where
g 1 2 3
∩ ∩
E¯ = k [K],h [H],V (β +β ,2β K,β ): (ψ ψk)V β , (11)
2 ∈ ∈ ∈V r p Q Q,h+1 k h − h kΛk h ≤ p
n o
whereβ =2(H +1 h),hb [H +1]. Thenwehavethefollowinbgresult.
Q,h
− ∈
Lemma11(Proxygoodevent). Considertheparametersettingoflemma6. ThenPr[E¯ ] 1 δ.
g
≥ −
Proof. First, by lemma 21 and our choice of parameters, E (eq. (7)) holds with probability at
1
least 1 δ/3. Next, applying lemmas 10 and 22, we get that with probability at least 1 δ/3
− −
simultaneouslyforallk [K],h [H],V (β +β ,2β K,β )
r p Q Q,h+1
∈ ∈ ∈V
(ψ ψk)V b
k h − h kΛk h
4β Q,hb+1 dlog(2K)+2log(6H/δ)+12d2log(1+8K(√192K3C ββ w)(KC β +KC w+1))
≤
q
1
4β dlog(2K)+2log(6H/δ)+12d2log(1+2K(√192K3K/(32Hd))( K K/(32Hd)+2β K2+1))
Q Q
≤ 4
r
p
4β dlog(2K)+2log(6H/δ)+12d2log(7K9/2)
Q
≤
q
4β d 12log(10K5H/δ)
Q
≤
28Hdplog(10K5H/δ)
≤
=β ,
p p
implying E¯ (eq. (11)). Finally, notice that φk 1, thus 0 Y H(3(β +β )+
2
k
hk(Λk h)−1
≤ ≤
k
≤
r p
4β β2).Usinglemma20,aBernstein-typeinequalityformartingales,weconcludethatE (eq.(9))
Q w 3
holdswithprobabilityatleast1 δ/3. (cid:4)
−
Thegoodevent. Thefollowingresultsshowthattheproxygoodeventimpliesthegoodevent.
Lemma 12. SupposethatE¯ holds. Ifπk Π(K(β +β ),2β K2)forallh [H] thenQˆk
g h ∈ r p Q ∈ h ∈
(β +β ,2β K,β ),Vˆk (β +β ,2β K,β )forallh [H +1].
Q r p Q Q,h h ∈V r p Q Q,h ∈
Proof. Weshowthattheclaimholdsbybackwardinductiononh [H +1].
b b
∈
Base caseh = H +1: SinceVˆk = 0 itisalsoimpliedthatQˆk = 0. Becauseβ,w = 0
H+1 H+1 ∈
(β +β ,2β K,β = 0) we have that Qˆk (β +β ,2β K,β = 0), and
Q r p Q Q,H+1 H+1 ∈ Q r p Q Q,H+1
similarlyVk (β +β ,2β K,β =0).
H+1 ∈V r p Q Q,H+1
b b
b
18Inductionstep: Now,supposetheclaimholdsforh+1andweshowitalsoholdsforh. Wehave
that
|Qˆk h(x,a) |= |φ¯k he(x,a)T w hk −β b kφ¯k he(x,a) k(Λk he)−1
|
≤|φ¯k he(x,a)T (θ h+(θ hk −θ h)+(ψ hk −ψ h)Vˆ hk +,i 1+ψ hVˆ hk +,i 1) |+β b kφ¯k he(x,a) k(Λk he)−1
≤1+ kVˆ hk +,i 1k∞+ kφ¯ bk he(x,a) k(Λk heb)−1 kθ hk −θ h kΛk
h
+ k(ψ hk −ψ h)Vˆ hk +,i 1kΛk
h
+β b
(thriangleinequality,Cauchy-Schwarz,Λke iΛk)
b b h (cid:22) h
≤1+β Q,h+1+(β r+β p,h+β b) kφ¯k he(x,a) k(Λk he)−1
(inductionhypothesis,eqs.(7)and(11))
1+β +(β +β +β )max[y σ( β y+logK)] (φ¯ke definition)
≤ Q,h+1 r p,h b y≥0 · − w h
2logK
1+β + (β +β +β ) (lemma18)
Q,h+1 r p,h b
≤ β
w
2+β (β 2(β +β +β )logK)
Q,h+1 w r p,h b
≤ ≥
=β .
Q,h
Additionally, β = β +β , (Λke)−1 I, Λke 1+ φk 2K, thus (Λke)−1
b r p h (cid:22) k h k ≤ k∈[K]k hk ≤ h (cid:23)
(2K)−1I,and
P
wk = θk+ψkVˆk,i K+β K 2β K =C .
k hk k h h h+1k≤ Q ≤ Q w
We concludethatQˆk
h ∈
Q(β
r
+bβ p,2bβ QK,β Q,h). Sinceπ hk
∈
Π(K(β
r
+β p),2β QK2), wealso
concludethatVˆk (β +β ,2β K,β ),provingtheinductionstepandfinishingtheproof. (cid:4)
h ∈V r p Q Q,h
b
Lemma(restatementoflemma6). Considertheparametersettingoftheorem9. Ifη 1,β2
b o ≤ w ≤
K/(32Hd)thenPr[E ] 1 δ.
g
≥ −
Proof. SupposethatE¯ holds. Bylemma11,thisoccurswithprobabilityatleast1 δ. Weshow
g
thatE¯ impliesE ,thusconcludingtheproof.Noticethat −
g g
k−1
πk(ax) exp η Qˆk′ (x,a)
h | ∝ h !
k X′=ke
k−1
=exp σ( −β w kφ(x,a) k(Λk he)−1 +logK) ·"φ(x,a)T
k
X′=keηw hk −ηβ b(k −k e) kφ(x,a) k(Λk he)−1 #!.
Weshowbyinductiononk K thatπk Π(K(β +β ),2β K2)forallh [H]. Forthebase
∈ e h ∈ r p Q ∈
case,k =k ,πkareuniform,correspondingtow,β =0 Π(K(β +β ),2β K2). Now,suppose
the claim hoe ldsh for allk′ < k. Thenby lemma 12 we h∈ ave thatQˆr k′ p (βQ +β ,2β K,β )
h ∈ Q r p Q Q,h
for all k′ < k and h [H]. This implies that k−1 ηwk 2β K2 for all h [H], thus
∈ k k′=ke hk ≤ Q ∈
πk Π(K(β +β ),2β K2)forallh [H],concludingtheinductionstbep.
h ∈ r p Q ∈ P
Now, since πk Π(K(β +β ),2β K2) for all k [K],h [H], we can apply lemma 12 to
h ∈ r p Q ∈ ∈
getthatQˆk (β +β ,2β K,β ),Vˆk (β +β ,2β K,β )forallk [K],h [H].
UsingE¯ (h eq∈ .(Q 11))r wecp oncluQ dethaQ t, Eh (eh q.∈ (8)V )hor lds,thp uscoQ ncludQ in,h gtheproof.∈ ∈ (cid:4)
2 2
b b
19B Technical tools
B.1 OnlineMirrorDescent
We beginwitha standardregretboundforentropyregularizedonlinemirrordescent(hedge). See
[Shermanetal.,2023a,Lemma25].
Lemma13. Lety ,...,y RAbeanysequenceofvectors,andη >0suchthatηy (a) 1for
1 T t
∈ ≥−
allt [T],a [A]. Thenifx ∆ isgivenbyx (a)=1/A a,andfort 1:
t A 1
∈ ∈ ∈ ∀ ≥
x (a)e−ηyt(a)
t
x (a)= ,
t+1 x (a′)e−ηyt(a′)
a′∈[A] t
then, P
T T
logA
max y (a)(x (a) x(a)) +η x (a)y (a)2.
t t t t
x∈∆A − ≤ η
Xt=1aX∈[A] Xt=1aX∈[A]
B.2 Valuedifferencelemma
We use the following extended value difference lemma by Shanietal. [2020]. We note that the
lemma holds unchanged even for MDP-like structures where the transition kernel P is a sub-
stochastic transition kernel, i.e., one with non-negativevalues that sum to at most one (instead of
exactlyone).
Lemma14(ExtendedValuedifferenceLemma1inShanietal.[2020]). Let bea(sub)MDP,
π,πˆ Π betwopolicies,Qˆ : R,h [H]bearbitraryfunction,aM ndVˆ : R be
M h h
∈ X ×A→ ∈ X →
definedasVˆ(x)= πˆ (a x)Qˆ (x,a).Then
a∈A h | h
P
Vπ(x ) Vˆ (x )=E Qˆ (x ,a)(π(a x ) πˆ(a x ))
1 1 − 1 1 P,π  h h | h − | h 
hX∈[H]a X∈A
 
+E ℓ (x ,a )+ P(x′ x ,a )Vˆ (x′) Qˆ (x ,a ) .
P,π h h h h h h+1 h h h
 | − 
hX∈[H] x X′∈X
 
We note that, in the context of linear MDP ℓ (x ,a ) + P(x′ x ,a )Vˆ (x′) =
h h h x′∈X
|
h h h+1
φ(x ,a )T (θ +ψ Vˆ ).
h h h h h+1 P
B.3 Algebraiclemmas
Next,isawell-knownboundonharmonicsums[see,e.g.,Cohenetal.,2019,Lemma13]. Thisis
usedtoshowthattheoptimisticandtruelossesarecloseontherealizedpredictions.
Lemma 15. Letz Rd′ be a sequencesuchthat z 2 λ, anddefineV = λI + t−1z zT.
t ∈ k t k ≤ t s=1 s s
Then
P
T T
kz t kVt−1 ≤vT kz t k2 Vt−1
≤
2Td′log(T +1).
t=1 u t=1
X u X p
t
Next,weneedthefollowingwell-knownmatrixinequality.
Lemma16(Cohenetal.[2019],Lemma27). IfN M 0thenforanyvectorv
(cid:23) ≻
detN
v 2 v 2
k kN ≤ detMk kM
Next,weneedaboundontheLipschitzconstantofthespectralnormofasquare-rootmatrix.
20Lemma17. Foranyλ>0andmatricesΛ,Λ′ Rd×dsatisfyingΛ,Λ′ λI wehavethat
∈ (cid:23)
1
Λ1/2 Λ′1/2 Λ Λ′ .
k − k≤ 2√λk − k
Proof. LetµbeaneigenvalueofΛ1/2 Λ′1/2witheigenvectorx Rd. Thenwehavethat
− ∈
xT (Λ Λ′)x = xT (Λ1/2 Λ′1/2)Λ1/2x+xT Λ′1/2(Λ1/2 Λ′1/2)x
| − | | − − |
= µxT (Λ1/2+Λ′1/2)x.
| |
Next, notice that xT (Λ Λ′)x x 2 Λ Λ′ , and xT (Λ1/2 +Λ′1/2) 2√λ x 2. We thus
| − | ≤ k k k − k ≥ k k
thereforechangesidestogetthat
1
µ Λ Λ′ ,
| |≤ 2√λk − k
andsincewecantakeµ= Λ1/2 Λ′1/2 ,theproofisconcluded. (cid:4)
±k − k
Finally,weneedthefollowingboundsonthelogisticfunction.
Lemma18. ForanyK 1,β >0wehavethat
≥
2logK
max[y σ( βy+logK)]
y≥0 · − ≤ β
Proof. First,ify′ (2logK)/β thenusingσ(y) [0,1]wehavethat
≤ ∈
y′σ( βy′+logK) y′ (2logK)/β,
− ≤ ≤
asdesired.Now,ify′ (2logK)/β then
≥
y′ y′ 2
y′σ( βy′+logK) y′σ( βy′/2)= = ,
− ≤ − 1+eβy′/2 ≤ βy′/2 β
wherethefirstinequalityalsousedthatσ(y)isincreasingandthelastinequalityusedthat1+ey y
≥
forally 0. (cid:4)
≥
Lemma19. ForanyK 1,z 0wehavethatσ(z logK) 2(z2+K−1).
≥ ≥ − ≤
Proof. Recall the logistic function σ(z) = 1/(1+ e−x) and define the function g(z) = σ(z
−
logK) (z+K−1/2)2. Weshowthatg(z) 0forallz 0. First,noticethat
− ≤ ≥
g(0)=σ( logK) K−1 =(K+1)−1 K−1 0.
− − − ≤
Next,recallthatσ′(x)=σ(x)σ( x)andthus
−
g′(z)=σ(z logK)σ( z+logK) 2(z+K−1/2).
− − −
Examiningz =0wefurtherhavethat
g′(0)=σ( logK)σ(logK) 2K−1/2
− −
=(K+1)−1(1+K−1)−1 2K−1/2
−
2[(K +1)−1 K−1/2] 0,
≤ − ≤
wherethelasttwoinequalitiesusedK 1. Now,wehavethat
≥
g′′(z)=σ(z logK)σ( z+logK)2 σ(z logK)2σ( z+logK) 2 0,
− − − − − − ≤
wheretheinequalityissinceσ(z) [0,1]forallz R. Sinceg(0),g′(0) 0andg′′(z) 0for
∈ ∈ ≤ ≤
all z 0, we conclude that g(z) 0 for all z 0. The proof is concluded using the AM-GM
≥ ≤ ≥
inequality. (cid:4)
21B.4 Concentrationbounds
WegivethefollowingBernsteintypetailbound(seee.g.,[Rosenbergetal.,2020,LemmaD.4].
Lemma20. Let X beasequenceofrandomvariableswithexpectationadaptedtoafiltration
{ t }t≥1
. Supposethat0 X 1almostsurely. Thenwithprobabilityatleast1 δ
t t
F ≤ ≤ −
T T
2
E[X ] 2 X +4log
t t−1 t
|F ≤ δ
t=1 t=1
X X
Westatethewell-knownselfnormalizederrorboundsforregularizedleastsquaresestimationofthe
rewardsanddynamics(seee.g.,Abbasi-Yadkorietal.[2011],Jinetal.[2020b]).
Lemma21(rewarderrorbound). Letθk beasinline14ofalgorithm1. Withprobabilityatleast
h
1 δ,forallk 1,h [H]
− ≥ ∈
b
θ θk 2 2dlog(2KH/δ).
k
h
−
hkΛk
h ≤
Lemma 22 (dynamics error uniformbconvergep nce). Let ψk : RX Rd be the linear operator
definedineq.(2)insidealgorithm1.Forallh [H],let h RX bea→ setofmappingsV : R
h
∈ V ⊆ X →
suchthat kV
k∞ ≤
β andβ
≥
1. Withprobabilityatleastb1 −δ,forallh
∈
[H], V
∈ Vh+1
and
k 1
≥
(ψ ψk)V 4β dlog(K +1)+2log(H /δ),
k h − h kΛk h ≤ Nǫ
p
whereǫ ≤β√d/2K, Nǫ = b h∈[H]Nh,ǫ,and Nh,ǫistheǫ −coveringnumberof Vhwithrespectto
thesupremumdistance.
P
B.5 Coveringnumbers
The following results are (mostly) standard bounds on the covering number of several function
classes.
Lemma23. Foranyǫ > 0,theǫ-coveringoftheEuclideanballinRd withradiusR 0isupper
boundedby(1+2R/ǫ)d. ≥
Lemma24. Let = V(;θ): θ W denoteaclassoffunctionsV : R. Supposethat
V { · k k≤ } X →
anyV isL-Lipschitzwithrespecttoθandsupremumdistance,i.e.,
∈V
V(;θ ) V(;θ ) L θ θ , θ , θ W.
k · 1 − · 2 k∞ ≤ k 1 − 2 k k 1 k k 2 k≤
Let betheǫ coveringnumberof withrespecttothesupremumdistance.Then
ǫ
N − V
log dlog(1+2WL/ǫ)
ǫ
N ≤
Proof. Let Θ be an (ǫ/L)-coveringof the Euclidean ball in Rd with radius W. Define =
ǫ/L ǫ
V
V(;θ):θ Θ . Bylemma23wehavethatlog dlog(1+2WL/ǫ). We showthat
ǫ/L ǫ ǫ
{ · ∈ } |V | ≤ V
is an ǫ-coverof , thusconcludingthe proof. Let V andθ be its associated parameter. Let
ǫ
θ′ Θ betheV pointinthecovernearesttoθ andV′∈ V itsassociatedfunction. Thenwehave
ǫ/L
∈ ∈ V
that
V() V′() = V(;θ) V(;θ′) L θ θ′ L(ǫ/L)=ǫ. (cid:4)
k · − · k∞ k · − · k∞ ≤ k − k≤
22