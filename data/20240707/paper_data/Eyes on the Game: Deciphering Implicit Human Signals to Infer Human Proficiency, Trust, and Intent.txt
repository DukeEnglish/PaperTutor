Eyes on the Game: Deciphering Implicit Human Signals
to Infer Human Proficiency, Trust, and Intent
Nikhil Hulle1,*, Ste´phane Aroca-Ouellette1,*, Anthony J. Ries2, Jake Brawer1,
Katharina von der Wense1, Alessandro Roncone1
Abstract—Effective collaboration between humans and AIs Game Data:
game states, actions,
hingesontransparentcommunicationandalignmentofmental scores
models. However, explicit, verbal communication is not always
Eye Data:
feasible.Undersuchcircumstances,human-humanteamsoften L/R eye gaze
coordinates, pupil
depend on implicit, nonverbal cues to glean important infor-
diameter, openness
mation about their teammates such as intent and expertise,
Keyboard Data:
thereby bolstering team alignment and adaptability. Among
Key presses/releases
these implicit cues, two of the most salient and fundamental
are a human’s actions in the environment and their visual Mouse Data:
Mouse position / clicks
attention.Inthispaper,wepresentanovelmethodtocombine
eyegazedataandbehavioraldata,andevaluatetheirrespective Human data:
Likert question answers,
predictive power for human proficiency, trust, and intent. We demographic survey
first collect a dataset of paired eye gaze and gameplay data
in the fast-paced collaborative “Overcooked” environment. We Fig.1. Inthiswork,collectalargedatasetofpairedeyegazeandgameplay
thentrainmodelsonthisdatasettocomparehowthepredictive data in the collaborative game ”Overcooked.” Using this data, we train a
powers differ between gaze data, gameplay data, and their causaltransformerdemonstratingstate-of-the-artperformanceinitsability
combination. We additionally compare our method to prior topredictacollaborator’staskproficiency,trustinanautonomousteammate,
worksthataggregateeyegazedataanddemonstratehowthese andfutureintent.
aggregation methods can substantially reduce the predictive
ability of eye gaze. Our results indicate that, while eye gaze and 2) a persons’s visual attention, which provides fine-
data and gameplay data excel in different situations, a model grained,immediatesignalsabouttheirfocus[5].Whilethese
thatintegratesbothtypesconsistentlyoutperformsallbaselines. signals have been leveraged independently to model and
Thisworkpavesthewayfordevelopingintuitiveandresponsive
predict human behavior, few works have sought to combine
agents that can efficiently adapt to new teammates.
them. In this work, we hypothesize that by integrating these
streams a more nuanced and complete model of a teammate
I. INTRODUCTION
canbelearned.Itisworthnotingthatacquiringhigh-fidelity
With the continued advancement of artificial intelligence data of complex cooperative tasks in sufficient quantities for
(AI) and robotics, it has become increasingly important to deep learning models and comprehensive analysis is still a
develop autonomous agents that can effectively collaborate challenge in the field. This paper not only provides such a
with humans. One promising research direction focuses on datasetcollectedthroughalargeuserstudy,butalsoprovides
endowing agents with a theory of mind [1], involving the a state-of-the-art (SotA) framework in the form of a causal
development of mental models of teammates to improve transformer [6] and analysis comparing different implicit
adaptability[2].“Explicit” communication—whichisdirect, signalstopredictahuman’s:a)proficiencyatatask;b)trust
unambiguous, and oftentimes verbal—can help form such in an autonomous teammate; and c) future intents.
mental models [3]. However, in many real-world teaming
Prior work in human-robot interaction (HRI) and human-
scenarios, only “implicit” communication—which is indi-
computerinteraction(HCI)havedemonstratedthepredictive
rect, suggestive, and often non-verbal—may be possible.
power of implicit signals like eye gaze [7], [8], [9] and
This could be due to factors such as the need for rapid
behavioral data [10], [11], [12]. Despite these advances,
action execution or high levels of ambient noise. In these
existingworkstillhaslimitations.First,mostoftheseworks
scenarios, autonomous agents must rely on implicit signals
infer only a single data point about their human teammates
to understand their teammates. Two implicit signals have
rather than build a comprehensive model of their behavior.
beenidentifiedintheliteratureaspromisingoptionsforthese
Second, they are often applied to non-representative, turn-
scenarios: 1) a teammate’s behavior in the environment [4],
based environments where a single action can span several
whichinformsaboutintentandcananticipatefuturebehavior
secondsandtheautonomousagentislimitedcomparedtothe
human teammate. Third, when eye gaze data is employed,
*EqualContribution.
1Department of Computer Science, University of Colorado Boulder, it is often hand-crafted into a small set of features before
Boulder,CO,USA.Email:{firstname.lastname}@colorado.edu. being fed to predictive models, degrading a rich source
2AJRiswiththeU.S.ArmyCombatCapabilitiesDevelopmentCommand of information for cluing the agent into the users’ mental
(DEVCOM)ArmyResearchLaboratory,Aberdeen,MD,USAandwiththe
state. Finally, most of these papers do not publicly release
WarfighterEffectivenessResearchCenter,UnitedStatesAirForceAcademy,
ColoradoSprings,CO,USA.Email:{firstname.j.lastname.civ}@army.mil. their datasets, hindering replication, comparison, and further
4202
luJ
3
]CH.sc[
1v89230.7042:viXraresearch. prediction in handover tasks [23]. Progress has also been
In contrast, this work not only leverages eye-tracking and made on predicting actions from RGB images and optical
behavioral data in parallel to accurately predict multiple flow [12] or RGB images alone [24], as well as on breaking
latent human factors, but also performs a comprehensive downhumanmovementintogranular“movemes”toimprove
analysis of these inputs to determine the advantages of each behavioral predictions [25].
datatype.Wecollectthisdatainthefast-pacedcollaborative Eyegazeasapredictor: Eyegazestandsoutasasalient
“Overcooked” environment (cf. see Fig. 1 and [13]), which signal, providing rich insights into a person’s attention,
serves as an ideal testbed for human-AI teaming due to its information processing, and social interactions, enhancing
capacity to efficiently gather large amounts of behavioral human teaming [26]. It has been used to anticipate intent in
data in the form of intricate and coordinated gameplay at a robotic manipulation tasks [27], as a substitute for wake-
different levels of abstraction. We then leverage state-of- words for smart-speakers [28], predict train routes in a turn-
the-art deep learning models to predict multiple mental and taking train board game [29], and detect errors in robot
behavioral factors including the human’s intent (in the form behavior[30].Interestingly,thepredictivepowerofgazehas
of future attempted subtasks), their trust in the autonomous been also demonstrated on the other end of the human-robot
teammate, and their proficiency at the game. Additionally, dyad: a robot equipped with a human-like binocular system
we compare several methods to aggregate and represent eye andcorrespondinggazecontroller[31]improvesthehuman’s
gazedata,findingthatgazedataprovidessalientinformation ability to predict the robot’s intent [32].
faster than gameplay data, but that gameplay provides a Our work shares similarities to [33] and [14]. Both works
stronger signal as the task progresses. Combining the two use implicit signals, including eye gaze, to predict informa-
consistently matches or outperforms the individual signals. tionabouthumansinafast-pacedcollaborativeenvironment.
Our results also show that gaze aggregation across the [33] explores the use of gaze features, game data, survey
temporal dimension only minimally impacts results in our data,anddemographicstopredictusers’preferencesbetween
tasks, while the spatial aggregation method used in [14] early game assistance and late game assistance. However,
substantially worsens performance. unlike our study, this work does not compare the use of
Inconclusion,wepresentthefollowingcontributions:1)a gameplay data on its own to eye gaze data alone. [14]
time-seriesmodelthatcanbeconditionedonbothhumaneye uses eye gaze data to predict periods of human confusion
gaze and gameplay data for accurate predictions of behav- in the same environment we employ, however they do not
ioral intents, skill level and trust in the agent; 2) a thorough consider the use of gameplay in any form. Notably, both of
analysis comparing the predictive power of gameplay data, these works aggregate gaze data over both time and space.
eye gaze data, and the combination of the two, providing Our research differs by 1) thoroughly comparing gaze data
practical insights the contexts in which each type of data to gameplay data, 2) examining the effects of aggregating
is most effective; and, 3) a publicly released dataset of gaze data in multiple different ways, and 3) exploring the
gameplay data paired with eye gaze data in a fast-paced predictive power of implicit communication across the three
collaborative environment. We believe these insights derived differentdimensions of trust, proficiency, and intent.
will enable AI agents to better model human teammates,
III. METHOD
allowing faster and more specific adaptation to improve the
A. Data Collection
team fluency and capability. By equipping agents with the
ability to process implicit signals, we introduce new modes 1) Environment: Due to its highly flexibly nature and
ofunderstandingandexpandtheboundariesofhuman-agent its ability to capture a wide-range of human-agent teaming
interaction. behaviors, we focus our work on the collaborative cooking
game “Overcooked” [13]. “Overcooked” requires a team
II. RELATEDWORK
composed of a human and an AI-controlled chefs to cook
Implicit human signals, such as EEG signals [15], heart and serve as many soups as possible within a set time
rate [16], recent actions [10], body language cues [17], and limit. To achieve this, players must execute a series of tasks
eye gaze [5], have been studied as a means to improve the ranging from collecting onions to placing them in a pot
human-machine interaction [18]. and serving the finished soups. Successful service rewards
Humanbehaviorasapredictor: Humanbehavioroften the team with 20 points. At each timestep, each player can
containsinformativeactioncuesthathintatfutureintent.For choose one of the following base actions: up, down, left,
instance,reachingforadoorhandlesuggeststheintentionto right, interact with an object (to pick up, place, or serve
exit a room. Notably, past sequences of behavior have been items), or stay in place. “Overcooked” requires players to
used to improve human-robot collaboration on assembly coordinate both on high-level strategic decision and on their
tasks [19], [20], anticipate a human’s action in a herding underlyingmovements.Atthestrategiclevel,playersshould
game[4],enhancehumanperformanceinteleoperationtasks aim to minimize redundancy and inefficiency—for instance,
[21],andpredictadecisionmakinginsearchandrescue[22]. avoiding the situation where both players retrieve a dish
Other work has investigated how to predict an action based when only one soup is being prepared. On the movement
on an observed initial portion of it. Wearable devices have level, careful navigation is essential to prevent collisions
been employed to collect arm movement data and improve between players. This combination of strategic planningspecific layouts used, the set of different agents used, the
number and recruitment methods for participants, the data
collected during each trial, and how the data was processed.
4) Participants: In total, 83 participants were recruited
across both the United States Air Force Academy (USAFA)
andtheUniversityofColorado(CU)Boulderusingnewslet-
(a)AsymmetricAdvantages (b)Coord.Ring (c)CounterCircuit
ter announcements and an online recruiting software. Nine
Fig.2. Thethree“Overcooked”layoutsused.From[13].
participants were removed due to either technical difficulties
withthesystemorpooreyetrackingdataquality(>40%of
and movement precision makes “Overcooked” an especially
eye tracking data was missing on at least one trial), leaving
suitable platform for studying human-agent collaboration.
74participantsinthedataset,foratotalof1332totalrounds
Fig. 2 shows the three specific game layouts we use to
of play or 29.6 hours of recorded play time. The age of
gather data: 1) Coordination Ring, which requires agents
participants ranged from 18 to 52 with an average age of
to focus heavily on their movement to avoid collisions 2)
21.43.33participantsidentifiedasmale,39asfemale,1non-
Asymmetric Advantages where agents are fully separated
binary, and 1 preferred not to disclose. When asked about
and so cannot collide, but must instead focus on aligning
theirpreviousexperiencewithOvercookedonascaleof1to
theirhigh-levelstrategy,and3)CounterCircuitthatrequires
7 (1 being no experience), participants reported an average
both movement and strategic alignment. Following previous
of 1.45, indicating that the majority of our participants had
work[34],werantheexperimentsfor400in-gametimesteps
no or limited experience with the game. As our primary
at 5 FPS, which equates to 80 seconds of gameplay.
objective is to test predictive ability with unseen humans,
2) AI Agents: To capture a thorough and wide range of we randomly select 59 participants for our training set, 5
human behaviors, we collected data using three different participantsforourvalidationset,and10participantsforour
agents of varying ability. The first agent is a random agent, testset.Allparticipantswererequiredtohavenormalvision
which randomly selects one of the six base actions. This (20/40 or better) without contact lenses to ensure that the
represents a very low level of play and is intended to eyecapturesystemwouldbeeffective.Priortoparticipating,
createsituationswherethehumanmaybeconfusedaboutits volunteers signed an informed consent document approved
teammate leading to low trust. The second agent is a self- by the IRB at the Army Research Laboratory (ARL 23-079)
play (SP) agent that is trained using reinforcement learn- in accordance with the Declaration of Helsinki.
ing (RL)—specifically proximal policy optimization (PPO) 5) User Study: Participant were required to complete an
[35]—and, as the name suggest, is trained being teamed online demographic survey prior to their in person session.
with itself. This agent can be quite good at the game if the Upon arrival, participants signed a consent form, and
humanadaptstoitsplay-style,butitstrainingregimecauses were then positioned around 70cm away from a display
it to be very rigid in its behaviors. This agent is aimed to and attached eye-tracking device (Tobii ProSpectrum), at
createtrialswherethehumancanhavemoderatetrustintheir which point a five-point calibration was executed using the
teammate,butmuststillpayattentiontotheagent’sbehavior Tobii Eye Tracker Manager (2.6.0) Fig. 1. Subsequent to
to avoid frequent collisions and a lower final score. Lastly, calibration, the accuracy of eye tracking was confirmed via
we use a SotA HAHA agent [34] that has been shown to be real-timegazetracking,withmandatoryrecalibrationforany
a significantly more performant, trusted, and understandable validation point discrepancies exceeding 1.5◦.
teammate. This agent was included to elicit situations where Following this, a Lab Streaming Layer (LSL) stream was
the humans have a high-level of trust in their teammate. startedthatbroadcastedtheeyegazedata(including,butnot
3) Trial Design: The primary objective in the dataset limited to, the right and left eye x and y coordinates relative
creation was to collect a wide range of human behaviors to the display, as well as pupil dilation collected at 300Hz),
while performing a collaborative task from which we could thegamedata(including,butnotlimitedto,thegamestates,
analyze and compare the predictive ability of gameplay teamactions,thereward,andinstanceofcollisions),andthe
data and human eye gaze data. To this end, we ran an keyboard data and mouse data. All data was recorded in xdf
IRB-approved user study where we recorded participants files. See Fig. 1 for depiction of the setup. Between each
playing the collaborative cooking game Overcooked. After round, we additionally collected the participants answers on
completing consent forms, participants were required to fill fivestatementsadaptedfrom[36]usinga7-pointLikertscale
out a demographic survey, read instructions about the game, [37] ranging from strong disagreement to strong agreement.
and complete a short tutorial that required them to serve These statements pertained to team fluency, perceived role
a completed soup before moving on. Each participant then significance, trust in the agent, understanding of agent ac-
played 18 rounds of Overcooked, with each round being tions, and the agent’s cooperativeness.
played with one of three different agents on one of three
B. Data Processing
different layouts. This led to each participant playing the
ninedifferentlayout-agentcombinationstwiceduringthefull To enable information to be readily fed into neural net-
duration of the trial. In the next section, we provide a more works, we first clean and process collected data. We use
in depth description of the Overcooked environment and the [13]’s lossless state encoding function to encode the gamet
i-1
Game state Lossless encoding Game data
1 1 1 1 1
1 1 1 1
1 1 1 11 1 1
1 111 11 11 1 11 11 1
t i X X XX t t t .+ + +t 1 2 3Y Y YY t t t .+ + +t 1 2 3 1 11 1 111 11 1 111 1 11 11 1 111 1111 1111 11 111 1 1 1 1 1 0.21 Con dca at te anate 1 1 1 11 1 1 1 1 1 11 1 1 1 1 11 1 11 11 0 0. .0 41 1 11 011 0.0181 1 11 0 0. .2 311 01 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Ga gm aze e a dn ad t aeye
. . 0.010.080.30
0.40
. .
Eye gaze data
Xt+nYt+n
Eye Map eye gaze to Heatmap
coordinates state
t
i+1
Fig.3. Anoverviewoftheprocessingmethodtocreaterepresentationsofeyegazedata,gameplaydata,andenableacombinationofthetwoforasingle
timestep.Therepresentationsaredesignedtobeeasilyfedintomodernneuralnetworks.
states into a grid representation of shape height x width x from0(stronglydisagree)to6(stronglyagree)with3being
27,whereeachofthe27channelscontainsinformationabout neutral.Second,foreachagent-layoutpair,webinallscores
different in-game objects or players. For the eye gaze data, in tertiles and label rounds by the tertiles they scored in.
we first average the x and y pixel coordinates of the two Scores in the bottom tertile would be in bin 0 (beginners),
eyes. If the gaze data for a single eye is null for a given the middle tertile in bin 1 (intermediates), and the top tertile
sample, we use the data from the only valid single eye data. in bin 2 (experts). We use these score tertiles as a proxy
If the gaze data is null for both eyes for a given sample, we for human proficiency. Third, we calculate when the human
excludethatsample.Wethenmapthepixelcoordinatestothe player completes one of the eleven different subtasks by
corresponding tile in the game’s grid environment. During identifying each time they perform an interact action and
this process, we filter out all eye gaze samples where the inspecting the change in state. We then back label each
participant is not looking within the boundaries of the game timestep since the previous subtask completion with the
environment. Since the eye gaze data is sampled at roughly completed subtask. We use these subtask labels to predict
300Hz compared to the 5Hz (or FPS) of the gameplay data, a human’s future intents. We note here that for trust and
wehaveapproximately60eyegazedatapointspergameplay proficiency, there is a single label for a full 400 timestep
timestep. To enable the combination of gameplay data and round. For intent, there are many subtask labels in a single
eye gaze data, we create eye gaze heatmaps of the same round, and the duration of a subtask label is highly variable
shape as the underlying game grid, and populate the grid anddependentonwhichtaskisbeingperformed,thecurrent
withtheratioofgazesamplesthatfallwithintheboundaries layout, and the proficiency of the human.
of each tile. A visual representation of our method is shown Asthisdataiscollectedfromahumanstudyandnothand
in Fig. 3. To compare our method to the method used in curated, class distributions are not perfectly balanced. Re-
[14], we additionally map each eye gaze sample to a game portingaccuracyinthiscasecanoverstatetheperformance.
grid tile and classify the sample as the human looking at Due to this, we use an F1 score as our primary metric, as
their own agent, looking at their teammate, or looking at the F1 scores incorporate both precision and recall in its final
environment. For each game timestep, we calculate the ratio output.Weadditionallyincludeabaselinemodelthatalways
of samples in each of these three bins. predicts the majority class in all our results.
Afterprocessing,wecanreadilyusefiveinputrepresenta- C. Models
tionsforagivennumberoftimesteps.1)alosslessgamestate
We train two types of models to predict our labels from
encoding per game timestep (Game Data), 2) an eye gaze
theinputdata.Fortheinputdatatypesthatretaintime-series
heatmap per game timestep (Eye Gaze Data), 3) a combined
information—game data, eye gaze data, game data + eye
stateencodingandtheeyegazeheatmappertimestep(Game
gazedata—wefirstflattenthetimesteprepresentations,asin
Data + Eye Gaze Data), 4) the average heatmap across all
Fig. 3. We then apply a linear layer to encode them into a
twenty timesteps (Collapsed Eye Gaze), and 5) the average
tokenembeddingsizeandpassthefirst20timestepsthrough
ratio of eye gaze samples that map to the human’s agent,
atransformermodel[38].Tocapturethetemporaldependen-
teammate, environment across all timesteps (Gaze Object).
ciesinthedata,weemployacausaltransformerarchitecture
We use three different labels from our dataset. The first [6]. Specifically, we generate a causal attention mask that
are the humans levels of agreement on the likert question: ensures each output token can only attend to the previous
”I trusted the agent to do the right thing:”. This ranges tokens in the sequence. This masking mechanism is crucial(a)ProficiencyPrediction (b)TrustPrediction (c)IntentionPrediction
(d)ProficiencyCumulativeF1 (e)TrustCumulativeF1 (f)Legend
Fig.4. F1scoresovertimefordifferentimplicithumansignalspredictinghumanproficiency,trust,andfutureintentsstartingattimestep0ofeachtrial.
Thetoprowofgraphsshowstheper-timesteppredictionoutputtedbyourtransformermodelthatcanhandletime-seriesdata.Thebottomrowshowsthe
cumulativepredictionofallpasttimesteps.Dottedlinesrepresentmethodsthataggregateovertimeandusethefull20secondwindowfortheirprediction.
forpreventinginformationleakagefromfuturetimestepsand gameplay data at 5Hz. Additionally, they include keyboard
enablingthemodeltolearnmeaningfultemporalpatterns.To and mouse data that were not utilized in our analysis. In
preventoverfitting,techniqueslikedropoutandlayernormal- addition to the XDF files, the datset contains the results of
ization are applied in positional encodings and transformer the likert scale questions, which can be mapped to the XDF
layers. Each output token is fed into a linear layer to get files using anonymized user and trial ids.
the appropriate number of logits for the task at hand. We
use a cross-entropy loss between the logits and ground truth IV. EXPERIMENTALDESIGN
labels at every timesteps and the RAdam optimizer [39] to
Withthecollecteddata,wesetouttoanswerthefollowing
train the model. We use the same architecture parameters
three research questions. RQ1: How does the predictive
as the base model in [38]. We perform a grid search on
power of eye gaze data compare to the predictive power
learning rate: lr ∈ {1e − 5,3e − 5,1e − 4}, batch size:
of gameplay data and to the combination of both? Core
bs ∈ {32,64,128}, warmup steps: ws ∈ {500,1000,2000}
to our contributions is a thorough analysis of the predictive
and found lr =3e−5, bs=128, and ws=2000 provided
power of gaze data compared to gameplay data. To this end,
the best results.
we train a causal model per agent-layout combination on
For the two representations that aggregate over
the first 20 timesteps of each round for each of our three
timesteps—collapsed eye gaze and gaze object—we
prediction labels: trust, proficiency, and next subtask to be
average their representations over all 20 timesteps and then
completed.
feed the aggregated input into a three layer multi-layered
RQ2:Howdoesaggregatingeyegazedataalongspatial
perceptron with 128 hidden units. We use the same loss
and temporal dimensions effect its predictive power?
function and optimizer. We perform the same grid search
Recent work has often aggregated eye gaze data across
excluding warmup steps which are transformer specific and
different dimensions to simplify the input space [14], [33].
found lr =1e−4 and bs=128 provided the best results.
This immediately poses the question of if and by how much
these simplifying aggregation techniques are impacting the
D. Data Release
predictive power of eye gaze data. To test this, we compare
Ananonymizedversionofthecollecteddataandthecode
thepredictivepowerwhenusingthefulltimeserieseyegaze
usedtoprocessitcanbefoundonline1.Thedatasetcontains
data to two lossy methods. In the first method, we average
XDF files that include all eye gaze data at 300Hz and all
the heatmap across timesteps, which collapses the temporal
dimension of the data and that we name collapsed eye gaze.
1https://hiro-group.ronc.one/overcooked-eye-gaze-
In the second, inspired by the approach used in [14], we
datasethoststhedataset.https://github.com/HIRO-group
/HAHA/tree/EyeGazehoststhecodeusedtoprocessthisdata. collapses the spatial dimension and only looks at the ratiosof eye focus on the user themselves, the teammate, and the their first productive action. For more experienced players,
environment. We name this method gaze objects. this shift would occur around this threshold of timestep
RQ3: Does the predictive power of eye gaze and 3−4, whereas for less experienced players, it would occur
gameplay data differ between the start of a new task later, usually between timesteps 7 and 15, aligning with the
and during continuous execution? Lastly, we hypothesize results we see here. Similarly to the subtask prediction, as
that a human’s work flow may change between the start of a the players deviate away from the start state, the gaze data
newtaskandwhentheyhavebeenperformingthesametask lacks necessary game information and the models start to
for a while. If true, we expect to see a difference in game lose some of its predictive ability. However, as seen in the
play and gaze data patterns. To examine this, we compare bottom row, this can be significantly mitigated by using the
thepredictivepowerofeyegazeandgameplaydataonwhen cumulative probabilities of all predictions.
focusing on the first 20 timesteps of gameplay compared to Lastly,usingacombinationofeyegazedataandgameplay
focusing on timesteps 200 to 220. dataprovidesthebestofbothmodalities,requiringlittledata
to get a good performance, and continuing to improve with
V. RESULTS
more data. Unlike the ungrounded gaze only models, the
RQ1: Comparing eye gaze data to gameplay data. gaze here can be attributed to objects in the environment,
Fig. 4 depicts the predictive power of eye gaze data, game- and we see no drop in performance. In all cases, using both
play data, and their combination across multiple human modalities provides the best or tied for best performance.
mental and behavioral factors. We first focus on the intent, RQ2: The effect of gaze data representation. We next
or “next subtask” prediction, shown in Fig. 4 c). As this investigate different methods to represent eye gaze data.
particular analysis only considers the inital 20 time steps Specifically, we compare the full time series representation
of the game, almost all participants will retrieve an onion utilizedintheprevioussectiontothecollapsedgazeandgaze
as their first subtask, leading to a very high f1 score early object representations. We note the latter two approaches
on. However, whereas the models that use game data and use all timesteps in question for their prediction, and there-
the combination of game and gaze data maintain a high fore are only comparable to the final timestep prediction.
predictive ability, using ungrounded gaze data on its own In the cumulative F1 approach (bottom graphs), we see
leads to a drop in performance at later stages. This is likely that the time series approach matches or outperforms the
because the gaze data only provides information on where otherapproaches.However,thecollapsedeyegazeapproach
the human is looking, but without the game data, there is performs nearly as well using a simpler model. In contrast,
no information on what the human is looking at. While the the gaze object approach of [14], which collapses the spatial
model can memorize the location of fixed objects in the dimensionandonlyusesthefrequencyatwhichhumanslook
environmenttoprovideabetterthanrandomprediction,there at different objects, drastically reduces performance. These
isnowaytoknowwhereeitheragentissituatedortomodel results indicate that the spatial dimension of gaze data is
thedynamicchangestotheenvironment.Inasituationwhere more useful for predicting proficiency and trust compared to
the human looks at the location where a pot is, for example, the temporal dimension.
from ungrounded eye data alone it would be difficult to RQ3: Task time We now examine how predictive power
ascertain whether the human is going to drop an onion into changes as humans move from starting a new tasks to a
the pot, retrieve a completed soup from the pot, or perhaps phase of continuous execution of the task. Fig. 5 show
checkiftheirteammateisperformingeitheroftheseactions. the predictions curves when we start predicting at timestep
We next focus on the proficiency and trust predictions, 200. Compared to the previous results, there are two trends.
shown in Fig. 4 a) and b). For these, since the labels are First, gameplay data is now strongly predictive from the
identical for an entire round, we provide two versions of first observed timestep, indicating that the state space itself
the graph. The top row of graphs show the individual pre- containsasignificantamountofinformationaboutthequality
dictions at each separate timestep, whereas the bottom row of play up to that point. Second, we see that the per-
of graphs averages all the probabilities up to and including timestep predictionof the ungrounded gazemodel no longer
the current timestep. These results show a clear trend where has the spike in prediction accuracy at the beginning, but
the predictive power of gameplay data starts near majority rather consistently predicts trust and proficiency at a similar
class prediction, and consistently trends upwards. This is level to its predictions around timesteps 20. Notably, even
expected because the game’s initial state is the same in all withoutthisbeneficialearlybump,thecumulativeprediction
trials, and the model can refine its prediction as trajectories accuracy (bottom graphs) increases over time and achieves
deviate toward or away from optimal paths. Notably, game a substantially higher F1 score than any single timestep,
data achieves a high performance within 20 timesteps. indicatingthatevenwithnogrounding,repeatedmeasuresof
Eye gaze data alone provides a strong predictive signal eye gaze data contain a rich signals about human behavior.
very early on, spiking around timestep 3 and 4 in both the
VI. DISCUSSIONANDCONCLUSION
trustandproficiencypredictions.Aqualitativeanalysisofthe
eyegazeandbehavioratthestartofthegameshowedatrend Inthispaper,wecollectalargedatasetofhumangameplay
where the participants would first look at their teammate, and gaze data while collaborating with a variety of different
then switch their focus to their agent before performing agents, specifically a random agent, a self-play(SP) agentFig.5. F1scoresstartingattimestep200.RefertoFig.4forafulldescriptionofthefigure
and a HAHA [34], in the fast-paced simulated environment anddetermininganappropriatefrequencytousewhendelin-
of “Overcooked.” We then use this dataset to thoroughly eatingtimesteps.Additionally,onepotentiallimitationofthis
examinethepredictiveabilityofvariousimplicithumansig- study is that our data was drawn from a participant pool rel-
nals. We highlight the following findings: 1) Both eye gaze atively lacking in terms of age, ethnic and cultural diversity.
data and gameplay data provide strong predictive signals for Considering evidence for the culturally contingent nature of
human proficiency, their trust in an autonomous agent, and gaze patterns (e.g., [40]), future work should explore the
theirintent;2)eyegazedata,evenwhennotgroundedinthe cultural nuances of eye gaze as a communicative signal.
environment, provides its predictive power early on, and is This is particularly relevant in diverse and multicultural
superior to gameplay data at the start of tasks when humans settings where human-robot interactions may be influenced
aredecidinghowtoactandfewactionshavebeenperformed. by varying interpretations of gaze behavior.
Asmorehumanbehaviorisobserved,gameplaydatacatches
ACKNOWLEDGMENT
up and eventually surpasses ungrounded eye gaze data; 3)
Combining both eye gaze data and gameplay data provides This work was supported by the Army Research Labo-
the best overall predictive ability; 4) Caution should be ratory under Grants W911NF-21-2-02905 and W911NF-21-
applied when aggregating eye gaze data. If eye gaze data 2-0126 and by the Office of Naval Research under Grant
is to be aggregated, our findings support aggregation over N00014-22-1-2482. The authors thank Abdul Ariyo, Zoe
the temporal dimension as preferable over aggregation over Iwu, Anna Madison, and R.J. Long for their assistance.
thespatialdimension.Wenotethatwhileinourexperiments
temporal aggregation only had a minimal impact, certain REFERENCES
tasks or domains may be more sensitive to it. [1] N. Rabinowitz, F. Perbet, F. Song, C. Zhang, S. M. A. Eslami, and
M.Botvinick,“MachineTheoryofMind,”inProceedingsofthe35th
International Conference on Machine Learning, ser. Proceedings of
A. Future Work
Machine Learning Research, J. Dy and A. Krause, Eds., vol. 80.
PMLR,10–15Jul2018,pp.4218–4227.
Our findings underpin two key future research directions.
[2] A. Tabrez, M. B. Luebbers, and B. Hayes, “A Survey of Mental
First, we are interested in investigating the potential for en- Modeling Techniques in Human–Robot Teaming,” Current Robotics
hancing the adaptability and personalization of autonomous Reports.
[3] A.Roncone,O.Mangin,andB.Scassellati,“Transparentroleassign-
agents by conditioning them on the information collected
mentandtaskallocationinhumanrobotcollaboration,”in2017IEEE
about their human teammates. Second, we intend to apply InternationalConferenceonRoboticsandAutomation(ICRA),2017,
and extend these findings to real-world human-robot collab- pp.1014–1021.
[4] F.Auletta,R.Kallen,M.DiBernardo,andM.Richardson,“Predicting
oration. While we are confident that our general conclusions
andunderstandinghumanactiondecisionsduringskillfuljoint-action
will extend to these practical scenarios, a real-world domain using supervised machine learning and explainable-AI,” Scientific
raises number of interesting questions. These include how Reports,vol.13,032023.
[5] H. Admoni and B. Scassellati, “Social Eye Gaze in Human-Robot
to adapt the systems to account for the movement of the
Interaction:AReview,”JournalofHuman-RobotInteraction,vol.6,
human, how to classify the completion of a human action, p.25,032017.[6] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,andI.Sutskever, D.Fleet,T.Pajdla,B.Schiele,andT.Tuytelaars,Eds. Cham:Springer
“LanguageModelsareUnsupervisedMultitaskLearners,”2019. InternationalPublishing,2014,pp.689–704.
[7] Y.Zhang,A.Yadav,S.K.Hopko,andR.K.Mehta,“InGazeWeTrust: [26] O.Sˇpakov,H.Istance,K.-J.Ra¨iha¨,T.Viitanen,andH.Siirtola,“Eye
ComparingEyeTracking,Self-report,andPhysiologicalIndicatorsof gazeandheadgazeincollaborativegames,”062019,pp.1–9.
Dynamic Trust during HRI,” in Companion of the 2024 ACM/IEEE [27] H. Admoni and S. S. Srinivasa, “Predicting user intent through
InternationalConferenceonHuman-RobotInteraction,ser.HRI’24. eye gaze for shared autonomy,” in Proceedings of AAAI ’16 Fall
New York, NY, USA: Association for Computing Machinery, 2024, SymposiumonSharedAutonomyinResearchandPractice,November
p.1188–1193. 2016,pp.298–303.
[8] Y.LuandN.Sarter,“Modelingandinferringhumantrustinautoma- [28] D.McMillan,B.A.T.Brown,I.Kawaguchi,R.Jaber,J.S.Belenguer,
tionbasedonreal-timeeyetrackingdata,”ProceedingsoftheHuman and H. Kuzuoka, “Designing with Gaze: Tama - a Gaze Activated
Factors and Ergonomics Society Annual Meeting, vol. 64, pp. 344– Smart-Speaker,” Proc. ACM Hum. Comput. Interact., vol. 3, no.
348,122020. CSCW,pp.176:1–176:26,2019.
[9] Y.RazinandK.Feigh,“Learningtopredictintentfromgazeduring [29] R.Singh,T.Miller,J.Newn,L.Sonenberg,E.Velloso,andF.Vetere,
robotic hand-eye coordination,” in Proceedings of the Thirty-First “Combining planning with gaze for online human intention recogni-
AAAI Conference on Artificial Intelligence, ser. AAAI’17. AAAI tion,”072018.
Press,2017,p.4596–4602. [30] R. M. Aronson, “Gaze for Error Detection During Human-Robot
[10] CatharineL.R.McGhanandAliNasirandEllaM.Atkins,“Human SharedManipulation,”2010.
intent prediction using markov decision processes,” J. Aerosp. Inf. [31] A. Roncone, U. Pattacini, G. Metta, and L. Natale, “A Cartesian 6-
Syst.,vol.12,pp.393–397,2012. DoFGazeControllerforHumanoidRobots.”inRobotics:scienceand
[11] C.-M.Huang,S.Andrist,A.Sauppe´,andB.Mutlu,“Usinggazepat- systems,vol.2016,2016.
ternstopredicttaskintentincollaboration,”FrontiersinPsychology, [32] J.-D.Boucher,U.Pattacini,A.Lelong,G.Bailly,F.Elisei,S.Fagel,
vol.6,2015. P. F. Dominey, and J. Ventre-Dominey, “I reach faster when I see
[12] S.Li,L.Zhang,andX.Diao,“Deep-Learning-BasedHumanIntention youlook:gazeeffectsinhuman–humanandhuman–robotface-to-face
PredictionUsingRGBImagesandOpticalFlow,”JournalofIntelli- cooperation,”Frontiersinneurorobotics,vol.6,p.3,2012.
gent&RoboticSystems,vol.97,pp.95–107,2019. [33] K. Candon, J. Chen, Y. Kim, Z. Hsu, N. Tsoi, and M. Va´zquez,
[13] M.Carroll,R.Shah,M.K.Ho,T.Griffiths,S.Seshia,P.Abbeel,and “Nonverbal Human Signals Can Help Autonomous Agents Infer
A.Dragan,“OntheUtilityofLearningaboutHumansforHuman-AI Human Preferences for Their Behavior,” in Proceedings of the 2023
Coordination,”inAdvancesinNeuralInformationProcessingSystems, InternationalConferenceonAutonomousAgentsandMultiagentSys-
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche´-Buc, E. Fox, tems,ser.AAMAS’23. Richland,SC:InternationalFoundationfor
andR.Garnett,Eds.,vol.32. CurranAssociates,Inc.,2019. AutonomousAgentsandMultiagentSystems,2023,p.307–316.
[14] L. Wachowiak, P. Tisnikar, G. Canal, A. Coles, M. Leonetti, and [34] S. Aroca-Ouellette, M. Aroca-Ouellette, U. Biswas, K. Kann, and
O. Celiktutan, “Analysing Eye Gaze Patterns during Confusion and A. Roncone, “Hierarchical Reinforcement Learning for Ad Hoc
Errors in Human–Agent Collaborations,” in RO-MAN 2022 - 31st Teaming,” in Proceedings of the 2023 International Conference
IEEEInternationalConferenceonRobotandHumanInteractiveCom- on Autonomous Agents and Multiagent Systems, ser. AAMAS ’23.
munication,ser.RO-MAN2022-31stIEEEInternationalConference Richland, SC: International Foundation for Autonomous Agents and
onRobotandHumanInteractiveCommunication:Social,Asocial,and MultiagentSystems,2023,p.2337–2339.
AntisocialRobots,2022,pp.224–229. [35] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
[15] J.-S. Kang, U. Park, V. Gonuguntla, K. C. Veluvolu, and M. Lee, “Proximal Policy Optimization Algorithms,” ArXiv, vol.
“Humanimplicitintentrecognitionbasedonthephasesynchronyof abs/1707.06347,2017.
EEGsignals,”PatternRecognitionLetters,vol.66,pp.144–152,2015. [36] G. Hoffman, “Evaluating Fluency in Human–Robot Collaboration,”
IEEE Transactions on Human-Machine Systems, vol. 49, no. 3, pp.
[16] P.E.Tressoldi,M.Martinelli,E.Zaccaria,andS.Massaccesi,“Implicit
209–218,2019.
intuition:howheartratecancontributetopredictionoffutureevents,”
JournaloftheSocietyforPsychicalresearch,vol.73,no.894,p.1, [37] R. Likert, “A technique for the measurement of attitudes.” Archives
ofpsychology,1932.
2009.
[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
[17] S.Greipl,K.Bernecker,andM.Ninaus,“FacialandBodilyExpres-
Gomez,L.u.Kaiser,andI.Polosukhin,“AttentionisAllyouNeed,”in
sionsofEmotionalEngagement:HowDynamicMeasuresReflectthe
AdvancesinNeuralInformationProcessingSystems,I.Guyon,U.V.
Use of Game Elements and Subjective Experience of Emotions and
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
Effort,” Proc. ACM Hum.-Comput. Interact., vol. 5, no. CHI PLAY,
R.Garnett,Eds.,vol.30. CurranAssociates,Inc.,2017.
oct2021.
[39] L.Liu,H.Jiang,P.He,W.Chen,X.Liu,J.Gao,andJ.Han,“Onthe
[18] A.Schmidt,“ImplicitHumanComputerInteractionThroughContext,”
VarianceoftheAdaptiveLearningRateandBeyond,”inInternational
PersonalTechnologies,vol.4,071999.
ConferenceonLearningRepresentations,2020.
[19] L. Wang, R. Gao, J. Va´ncza, J.Kru¨ger, X. V.Wang, S. Makris, and
[40] X. Zhang, M. Dalmaso, L. Castelli, S. Fu, and G. Galfano, “Cross-
G. Chryssolouris, “Symbiotic human-robot collaborative assembly,”
cultural asymmetries in oculomotor interference elicited by gaze
CIRPannals,vol.68,no.2,pp.701–726,2019.
distractors belonging to Asian and White faces,” Scientific Reports,
[20] S. Mehak, J. D. Kelleher, M. Guilfoyle, and M. C. Leva, “Action
vol.11,no.1,p.20410,2021.
Recognition for Human–Robot Teaming: Exploring Mutual Perfor-
manceMonitoringPossibilities,”Machines,vol.12,no.1,2024.
[21] M. Ahmed, B. Lall, R. Kumar, and A. A. Kherani, “Towards Es-
timation of Human Intent in Assistive Robotic Teleoperation Using
KinaestheticandVisualFeedback,”inProceedingsoftheIEEE/CVF
International Conference on Computer Vision (ICCV) Workshops,
October2023,pp.1928–1934.
[22] M. Zhao, R. Simmons, and H. Admoni, “The role of adaptation in
collectivehuman–AIteaming,”TopicsinCognitiveScience,2022.
[23] W. Wang, R. Li, Y. Chen, and Y. Jia, “Human Intention Prediction
in Human-Robot Collaborative Tasks,” in Companion of the 2018
ACM/IEEE International Conference on Human-Robot Interaction,
ser. HRI ’18. New York, NY, USA: Association for Computing
Machinery,2018,p.279–280.
[24] B. Keshinro, Y. Seong, and S. Yi, “Deep Learning-based human
activityrecognitionusingRGBimagesinHuman-robotcollaboration,”
Proceedings of the Human Factors and Ergonomics Society Annual
Meeting,vol.66,no.1,pp.1548–1553,2022.
[25] T.Lan,T.-C.Chen,andS.Savarese,“AHierarchicalRepresentation
for Future Action Prediction,” in Computer Vision – ECCV 2014,