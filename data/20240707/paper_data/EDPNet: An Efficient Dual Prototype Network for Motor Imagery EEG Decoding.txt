ffi
EDPNet: An E cient Dual Prototype Network for Motor Imagery EEG
Decoding
CanHana,ChenLiua,CrystalCaia,JunWangb,∗,DahongQiana,∗
aSchoolofBiomedicalEngineering,ShanghaiJiaoTongUniversity,Shanghai,200240,China
bSchoolofComputerandComputingScience,HangZhouCityUniversity,Hangzhou,310015,China
Abstract
Motor imagery electroencephalograph (MI-EEG) decoding plays a crucial role in developing motor imagery brain-
computer interfaces (MI-BCIs). However, decoding intentions from MI remains challenging due to the inherent
complexity of EEG signals relative to the small-sample size. In this paper, we propose an Efficient Dual Prototype
Network(EDPNet)toenableaccurateandfastMIdecoding. EDPNetemploysalightweightadaptivespatial-spectral
fusionmodule,whichpromotesmoreefficientinformationfusionbetweenmultipleEEGelectrodes. Subsequently,a
parameter-freemulti-scalevariancepoolingmoduleextractsmorecomprehensivetemporalfeatures.Furthermore,we
introducedualprototypicallearningtooptimizethefeaturespacedistributionandtrainingprocess,therebyimproving
the model’s generalization ability on small-sample MI datasets. Our experimental results show that the EDPNet
outperforms state-of-the-art models with superior classification accuracy and kappa values (84.11% and 0.7881 for
dataset BCI competition IV 2a, 86.65% and 0.7330 for dataset BCI competition IV 2b). Additionally, we use the
BCIcompetitionIIIIVadatasetwithfewertrainingdatatofurthervalidatethegeneralizationabilityoftheproposed
EDPNet. Wealsoachievesuperiorperformancewith82.03%classificationaccuracy. Benefitingfromthelightweight
parametersandsuperiordecodingaccuracy,ourEDPNetshowsgreatpotentialforMI-BCIapplications. Thecodeis
publiclyavailableathttps://github.com/hancan16/EDPNet.
Keywords: prototypelearning,attentionmechanism,lightweight,brain-computerinterface,motorimagery
1. Introduction
Brain-computer interface (BCI) systems enable non-muscular communication between users and machines by
interpreting users’ neural activity patterns [1]. In BCI applications, Electroencephalography (EEG) has become in-
creasingly popular due to its non-invasive nature and cost-effectiveness. Motor Imagery (MI) [2] is the mental re-
hearsal of movement execution without any physical movement. When participants visualize moving parts of their
body,specificareasofthebrainexperienceenergychangesknownasevent-relateddesynchronization/synchronization
(ERD/ERS).ThesechangescanberecordedviaEEGandusedtodiscriminatemotorintent[3,4]. TheMI-basedBCI
hasgarneredsignificantattentionasitenablesthedecodingofusermotorintentionsfromEEGsignals. Ithasbeen
successfullyappliedinvariousfields,suchasstrokerehabilitation[5],wheelchaircontrol[6],andexoskeletonrobot
armcontrol[7].
Advancements in deep learning (DL) have significantly increased the accuracy of decoding EEG signals for
MI-based BCI applications [8, 9, 10, 11], yet several issues still hinder EEG-based models from reaching practi-
caluse[12]. WhendevelopingapracticalandaccurateEEG-MIdecodingalgorithm,severalkeychallengesneedto
betakenintoconsideration.
• Complex characteristics of EEG. EEG signals are contaminated with noises and artifacts, leading to a low
signal-to-noiseratio(SNR).Moreover,complexspatial-spectralcouplingcharacteristicsandhightemporalvari-
∗Correspondingauthor.
Emailaddresses:hancan@sjtu.edu.cn(CanHan),wangjun@hzcu.edu.cn(JunWang),dahong.qian@sjtu.edu.cn(DahongQian)
PreprintsubmittedtoElsevier July4,2024
4202
luJ
3
]CH.sc[
1v77130.7042:viXraabilityfurthercomplicatethedecodingofMI-EEGsignals[13]. Therefore, extractingdiscriminativefeatures
fromEEGsignalsischallengingyetcrucial.
• Limited data. EEG signals frequently encounter constraints due to a scarcity of training samples, caused
by several issues such as cumbersome calibration procedures, uncertainty in annotations due to variability in
participants’responsestoMItasks,anddataprivacyissues[14,15].Withoutamassiveamountoftrainingdata,
themodelmayoverfit. Thisposeschallengestothemodel’sgeneralizationabilityonnewtestdata.
• Computational cost. In practical BCI applications, computational resources are often limited. Therefore,
lightweightandfastmodelsaremoresuitableforpracticalscenarios[16].
Existing research primarily focuses on addressing one of the aforementioned challenges. Some research efforts
leverage advanced DL techniques, such as multi-branch designs [17, 18, 19], transformers [20, 21, 22, 23, 24], and
attention mechanisms [16, 25, 26, 27], to extract highly discriminative features from EEG data, thereby improving
EEG-MI decoding accuracy. However, these methods overlook the overfitting issue caused by limited training data
and tend to have high computational complexity. Other research efforts [28, 29, 30, 31] employ transfer learning
(TL) to mitigate the small-sample issue. Nevertheless, these TL methods still require a relatively large amount of
additionaldatafromothersubjectstoachievegoodperformance,whichmaynotbepracticalinreal-worldscenarios.
The Sinc-ShallowNet proposed by Borra et al. [32] offers advantages in lightweight design and interpretability, but
itsdecodingaccuracyisunsatisfactoryduetothelackofeffectivemechanismsforextractingdiscriminativefeatures.
Becauseofincompleteconsiderationandresolutionofthesechallenges,thereremainsagapbetweenexistingresearch
andpracticalapplications.
Considering all the above challenges, this paper aims to design a lightweight neural network architecture that
effectivelyextractshighlydiscriminativeandrobustfeaturesfromcomplexEEGsignalsforaccurateMIclassification,
even with limited training data. To achieve this goal, we propose an Efficient Dual Prototype Network (EDPNet),
inspiredbytherecognitionmechanismofthehumanbrain. Humanbrainscanestablishcognitiveunderstandingfrom
asmallamountoflearnabledataandeffectivelygeneralizeittonewdatabasedonmemoriesandtemplate/prototype
matching. TheEDPNetiscomposedoftwomaincomponents,i.e.,afeatureextractorandtheprototypesforallMI
classes. The feature extractor simulates the sensory system of humans for transforming original data into abstract
representations. Moreover, the prototypes for each class act as abstract memories of the corresponding class in our
brains. As in human recognition, the decision in EDPNet is made by matching the feature (abstract representation)
withprototypes(memories)ofeachclass.
ForthefeatureextractorcomponentofEDPNet,basedonERD/ERSpriorknowledge,wedesigntwonovelmod-
ules: AdaptiveSpatial-SpectralFusion(ASSF)moduleandMulti-scaleVariancePooling(MVP).TheASSFmodule
focuses on modeling the relationship between EEG electrode channels that reflect levels of brain activation [33].
Equippedwithalightweightattentionmechanism,theASSFmodulecanadaptivelyadjusttheweightsofeachEEG
channel according to its importance, thereby effectively extracting spatial-spectral features relevant to specific MI
tasks. Then,theMVPmodulecapturesmulti-scalelong-termtemporalfeaturesbasedonsignalvariancewhichrep-
resentstheEEGspectralpower[34]. TheMVPmodulehasnotrainableparametersandiscomputationallyefficient,
servingasasuperiormethodforextractingpowerfultemporalfeaturesinMI-EEGdecodingtasks. Thecombination
of the ASSF module and MVP module enables the feature extractor of EDPNet to extract discriminative spatial-
spectral-temporalfeaturesfromthecomplexEEGsignals.
Furthermore,toaddressthesmall-sampledilemmainEEG-MIdecoding,wedesignanewprototypelearning(PL)
approachtooptimizethedistributionofprototypesandfeatures,aimingtoobtainarobustfeaturespace. Tothebest
ofourknowledge,thisisthefirststudytoapplythePLtoMI-EEGdecoding. TheclassicPLmethod[35]employs
aprototypelosstopushfeaturevectorstowardscorrespondingprototypes,makingthefeatureswithinthesameclass
morecompact, whichisbeneficialforclassificationandmodelgeneralization. BasedontheclassicPLmethod, we
proposeaDualPrototypeLearning(DPL)approachforourEDPNettodecoupleinter-classseparationandintra-class
compactnessintrainingprocesses. TheDPLnotonlyenhancesintra-classcompactness,butalsoexplicitlyincreases
inter-classmargins. ComparedtotheclassicPL,ourDPLfurtherimprovesthemodel’sgeneralizationcapability.
Themajorcontributionsofthispapercanbesummarizedasfollows:
1. InspiredbyclinicalpriorknowledgeofEEG-MIandhumanbrainrecognitionmechanisms,weproposeahigh-
performance, lightweight, and interpretable MI-EEG decoding model EDPNet. The EDPNet simultaneously
2considersandovercomesthreemajorchallengesinMI-BCIs.
2. ToextracthighlydiscriminativefeaturesfromEEGsignals,wedesigntwonovelmodules,ASSFandMVP,for
the feature extractor of EDPNet. The ASSF module extracts effective spatial-spectral features, and the MVP
moduleextractspowerfulmulti-scaletemporalfeatures.
3. Toovercomethesmall-sampleissueofMItasks,weproposeanovelDPLapproachtooptimizethedistribution
offeaturesandprototypes,aimingtoobtainarobustfeaturespace. Thisenhancesthegeneralizationcapability
andclassificationperformanceofourEDPNet.
4. WeconductexperimentsonthreebenchmarkpublicdatasetstoevaluatethesuperiorityoftheproposedEDPNet
againststate-of-the-art(SOTA)MIdecodingmethods. Additionally,comprehensiveablationexperimentsand
visualanalysisdemonstratetheeffectivenessandinterpretabilityofeachmoduleintheproposedEDPNet.
2. RelatedWorks
2.1. DeepLearningbasedEEG-MIDecoding
Withrecentadvancementsindeeplearning,researchersareincreasinglyusingvariousdeeplearningarchitectures
todecodeEEGsignals. DeepConvNet[8]employedmultipleconvolutionallayerswithtemporalandspatialfeature
extraction kernels. Sakhavi et al. [9] utilized FBCSP for feature extraction, followed by CNN-based classification.
Lawhern et al. [10] introduced a compact network, EEGNet, employing depthwise and separable convolution for
spatial-temporalfeatureextraction. However,duetothelackofeffectivemechanismsforextractinghighlydiscrimi-
nativefeatures,theimprovementsofthesemethodsarelimited.
Attention mechanisms, which have recently gained significant recognition in various fields, have been success-
fully applied to MI-EEG decoding. TS-SEFFNet [25] combines a channel attention module based on the wavelet
packetsub-bandenergyratiowithatemporalattentionmechanism,followedbyafeaturefusionarchitecture. LMDA-
Net[16]combinesacustomchannelrecalibrationmodulewithafeaturechannelattentionmodulefromECA-Net[36].
Wimpff et al. [26] applied various attention mechanisms to the proposed BaseNet and made a very comprehensive
comparison of the different variations. M-FANet [27] uses a convolution with a small kernel size to extract local
spatialinformationandaSE[37]moduletoextractinformationfrommultipleperspectives. Thesemethodsmainly
applyattentionmechanismstodeepfeaturesextractedbytheneuralnetworkandimprovetheMIdecodingaccuracy
to some extent. Nonetheless, few studies have employed attention mechanisms to model the relationships between
EEGelectrodechannelsthatreflectlevelsofbrainactivation[33].
Besides,researcheffortshavealsobeendevotedtoextractingmoreeffectivetemporalfeaturesfromhightemporal
resolutionEEGsignals. Lately,transformermodelshavemadewavesinnaturallanguageandcomputervisiondueto
theinherentperceptionofglobaldependencies[38]. TransformersalsoemergedinMIdecodingandachievedgood
performance[20,21,22,23,24],byleveraginglong-termtemporalrelationships. ATCNet[22]usesself-attentionto
highlightthemostimportantinformationinEEGsignals. Conformer[23]stackstransformerblockstoextractlong-
term dependency features based on local temporal features extracted by CNN. However, transformer models have
highparametersandcomputationalcosts,makingthemhardtobeusedforreal-timeMIdecoding.
2.2. PrototypeLearning
PLsimulatesthewayhumanslearnbymemorizingtypicalexamplestounderstandandgeneralizetonewsitua-
tions. InPLmethods, asetofrepresentativesamples(prototypes)islearnedduringtraining, andduringtestingtest
samplesareassignedtotheclosestprototypetodeterminetheircategories. In[39],Snelletal. proposedtoapplythe
prototypeconceptforfew-shotlearning. However,thismethodlearnstheprototypesandthefeatureextractorsepa-
ratelyusingdiscriminativeloss. Yangetal.[35]introducedtheprototypemodelintotheDLparadigmanddesigned
different discriminative loss as well as generative loss. This significantly improves the performance and robustness
of DL models in classification tasks. Following the study [35], a substantial amount of research [40, 41, 42] has
been devoted to using PL to learn a compact feature space for addressing open-world recognition. Another line of
research[43,44]continuestoexplorethepotentialofPLinfew-shotlearning.
3×    1×    1×1   1×   2×   2 ×   2 ×    2 ∗  
3 3 3


×
+

Spatial-
Variance Concat&
LightConv Spectral PointwiseConv1D Split Pooling Flatten Dual Prototype
Attention
Learning
Spatial-Spectral
Adaptive Spatial-Spectral Fusion Multi-scale Variance Pooling
Embedding

Figure1:TheoverallframeworkoftheproposedEDPNet.CandTdenotethenumberofEEGchannelsandthenumberoftimepoints,respectively.
F1andF2denotethenumbersoftemporalfiltersandspatial-spectralfilters,respectively.Tirepresentstheoutputlengthofthevariancelayerwith
differentkernelsizes.TheSSE,ASSF,andMVPmakeupthefeatureextractor,whileDPLisusedfortrainingoptimizationandclassification.
3. Method
As shown in Figure 1, our proposed EDPNet consists of four modules. The Spatial-Spectral Embedding (SSE)
module,theAdaptiveSpatial-SpectralFusionmodule,andtheMulti-scaleVariancePoolingmoduleconstitutethefea-
tureextractorforextractinghighlydiscriminativefeatures. TheDualPrototypeLearningmoduleisusedtooptimize
thefeaturespaceandmakeclassificationdecisions.
3.1. EEGRepresentation
In this paper, we feed raw MI-EEG signals into the proposed model without any additional time-consuming
preprocessing. Given a set of m labeled MI trials S = {X,y}m , where X ∈ RC×T consists of C channels (EEG
i i i=1 i
electrodes)andT timepoints,y ∈ {1,...,n}isthecorrespondingclasslabel,andnisthetotalnumberofpredefined
i
classes for set S, our EDPNet model first maps a motor imagery trail X to the feature space Z and obtains z =
i i
f(X) ∈ Rd,where f isthefeatureextractor,asshowninFigure1. Then,theDPLmodulemapsthefeaturez toits
i i
correspondingclassy.
i
3.2. FeatureExtractorofEDPNet
3.2.1. Spatial-SpectralEmbedding
Since different MI classes may differ in their corresponding spectral-spatial sensorimotor rhythm (SMR) pat-
terns [10], most existing studies first extract multi-view spectral information from each EEG electrode channel to
formaspatial-spectralrepresentation. SomeworksfollowthepracticeofEEGNet[10]andusea2Dconvolutionto
extractspectralfeatures,whileothersfollowthepracticeofFBCNet[9]andusemultiplenarrow-banddigitalfilters
tomanuallyextractdifferentspectralfeatures. OurSSEmoduleissimilartotheformer,butusesa1Dconvolutionfor
end-to-endextractionofspectralfeatures.
Unlike these popular methods [16, 22, 23, 25, 26, 27] following EEGNet, we do not incorporate an additional
featuredimensiontocreatea2DrepresentationX ∈R1×C×T fortherawEEGsignalX ∈RC×T. Wedirectlytreatthe
i i
EEGelectrodedimensionCinX ∈RC×T asthefeaturedimensionanduse1Dconvolutiontoextractspectralfeatures.
i
Specifically,weintroducetheLightConv[45],adepthwiseconvolutionthatsharescertainoutputchannels,toactas
the 1D temporal convolution. LightConv first divides the input signal X ∈ RC×T into h groups along the channel
i
dimension. Therefore, each group hasC/h channels, and each channel within the same group shares convolutional
4weights. Theimplementationstepsareasfollows:
X =Reshape(X)∈R(C/h)×h×T (1)
h i
X
dw
=DWConv1D(X h,W)∈R(C/h)×(h∗F1)×T (2)
X
sse
=Reshape(X dw)∈RCF1×T, (3)
where DWConv1D denotes 1D depthwise convolution. Additionally, W ∈ R(h∗F1)×1×k is the learnable convolution
parameter,and X
sse
∈ RCF1×T correspondstotheresultantspatial-spectralembedding. ItisimportanttonotethatW
containsh∗F filters,aseachchannelusesF filterswithkernelsizektogeneratedifferentspectralcharacteristics.
1 1
Comparedtodepthwiseconvolution,LightConvreducesthenumberofparametersbyafactorofC/h.
Moreover, by setting different h values and arranging electrode channels, different brain regions can be easily
decoded by different temporal filters, which helps extract more comprehensive information. By setting h = C in
Eq.(1),theLightConvisequivalenttoadepthwiseconvolution,whereeachelectrodechannelusesdifferentfiltersto
extractspectralfeatures. Toreducethenumberofparametersandacceleratetraining,wesethas1inthispaper. All
electrodechannelsshareF temporalfilterstoproducespatial-spectralembeddingX ,asshowninFigure1.
1 sse
3.2.2. AdaptiveSpatial-SpectralFusion
An EEG equipment uses multiple electrodes distributed in different regions of the cerebral cortex to capture
neuronal activity in these brain regions. When performing different MI tasks, the EEG signal amplitudes recorded
bydifferentelectrodesmayincreaseordecreaseinspecificspectralbands[46]. ThisphenomenonisknownasERD
and ERS. Therefore, it is critical to emphasize the relationship between different spectral features among multiple
EEG electrodes. When extracting features in the spatial-spectral dimension, each channel should not be treated as
equal,butrather,focusshouldbeplacedonareasandspectralbandsrelevanttothespecificMItask. Consequently,
leveragingtheattentionmechanism,wedesignaSpatial-SpectralAttention(SSA)toextracteffectivespatial-spectral
informationforallelectrodes.
Global Channel
Context Normalization
Embedding
Gating
Adaptation
Figure2:Thestructureoftheproposedspatial-spectralattention.
Inspired by the gated channel transformation [47], our spatial-spectral attention consists of three parts: global
context embedding, channel normalization, and gating adaptation, as shown in Figure 2. For an input X ∈ RC×T,
globalcontextembeddingemploysamean-varoperationtoaggregatetemporalinformationfromeachchannel. This
involves calculating the variance within each 1-second window, followed by averaging them. Subsequently, α is
responsibleforcontrollingtheweightofeachchannel:
s=α·mean-var(X). (4)
5Then,weuseachannelnormalizationcomponenttomodelchannelrelations:
√ √
Cs Cs
sˆ= = , (5)
||s||
2
[((cid:80)C
s2)+ϵ]1
2
c=1
whereϵ isasmallconstanttoavoidtheproblemofderivationatthezeropoint. Thegatingweightandbias,γandβ
areresponsibleforadjustingthescaleoftheinputfeaturechannel-wise:
Attention=1+tanh(γsˆ+β) (6)
Xˆ = X·Attention. (7)
ThescaleofeachchannelofX
sse
∈RCF1×T outputbytheSSEmodulewillbeadjustedbythecorrespondingatten-
tionweight. Additionally, SSAleveragesglobaltemporalinformationtomodelchannelrelationshipsandmodulate
featuremapsonthechannel-wiselevel.Therefore,wecaneffectivelyfusetheweightedspatial-spectralfeaturesusing
asimple1Dpointwiseconvolution. AsshowninFigure1,thepointwiseconvolutionusesF filterstosimultaneously
2
fusespectralfeaturesofallelectrodestogetX
assf
∈RF2×T.
3.2.3. Multi-scaleVariancePooling
Itiscrucialtoacquirelong-termdependenciesandglobaltemporalinformationforEEGdecoding. Transformer-
based[22,20,21,23]modelscancaptureglobalinformationwellbyusingself-attentionmechanism,buttheyhavea
largenumberofparametersandhighcomputationalcomplexity. Infact,undertheconstrainedEEGtrainingdata,itis
difficultforthetransformer-basedmodeltoachieveoptimalperformanceasinthecomputervisionfield. Therefore,
itisnecessarytodesignanewmethodforEEGdecodingthatcanextractlong-termdependenciesinformation.
Metaformer [48] has proposed that using a simple pooling layer in place of self-attention in transformers can
also perform well. Therefore, we consider using a pooling layer with a large kernel to extract the global temporal
informationfromtheEEGsignals. Inspiredby[34]andconsideringthatvariousclassesofMIdifferintheirspectral
power(ERD/ERS),avarianceoperationwhichrepresentsthespectralpowerinthegiventimeseriesbecomesamore
suitable option for EEG temporal characterization. In order to achieve this, we first design a 1D variance pooling
layer, VarPool, whichismorecompatiblewiththeneuralnetworkarchitecture. Foratimeseriessignal x ∈ Rt, the
relationshipbetweenitsvarianceDxandmeanExisasfollows:
Dx= E(x−Ex)2
= E(x2)+(Ex)2−2∗(Ex)2
= E(x2)−(Ex)2. (8)
Therefore, for the EEG representation X ∈ RC×T, we can utilize average pooling to calculate variance pooling, as
Square AvgPool
AvgPool Square
Figure3:AnillustrationoftheproposedVarPoollayer.
6showninFigure3:
VarPool(X) =AvgPool(X2) −AvgPool(X)2 , (9)
k,s k,s k,s
wherekand srepresentthespecifiedwindowlengthandslidingstepsize. ForaninputX ∈ RC×T,theVarPoollayer
slidesalongthetimedimensiontocalculatethevariancewithineachwindowtoobtaintheoutputXˆ ∈RC×Tˆ :
(cid:36) T +2×padding−(k−1)−1 (cid:37)
Tˆ = +1 . (10)
s
Furthermore,toextractmulti-scalelong-termtemporalfeatures,wedesigntheMulti-scaleVariancePoolinglayer.
Specifically, the output X of the ASSF module is split into three groups along the channel dimension. VarPool
assf
layers with different large kernel sizes (i.e., 50, 100, and 200) are used for each group to extract temporal features.
Then,theoutputsofthethreegroupsareflattenedandconcatenatedtoobtainthefinalfeaturevectorz.Itisnoteworthy
i
thatourMVPmodulecontainsnotrainableparametersandiscomputationallyefficient.
Margin


(a) CEloss (b) PL
Margin


orce    
(c) OnewaytoimprovePL (d) DPL
Figure4:AnillustrationofthefeaturespacedistributionforCEloss,PL,andourproposedDPL.
3.3. DualPrototypeLearning
Inclassificationtasks, existingmethodsinputthefeaturevectorzintoamulti-layerperceptron(MLP)toobtain
classificationresults,andoptimizetheparametersoftheneuralnetworkusingCEloss.Recentstudies[21,11]indicate
thatCElossmaybelackingintheeffectivenessofreducingintra-classvariation,especiallywhenconsideringthenon-
stationarityofEEGsignals. AsshowninFigure4(a),CElossonlyoptimizessamplestowardsthedecisionboundary
ofthecorrespondingclass,resultinginaloosedistributionofsamplefeatures.WhenapplyingPLtoclassification,the
firststepistoassignaprototypetoeachclass. Aclassificationlossoptimizesthesamplefeaturestobeclosesttoits
correspondingprototypesforclassification. Additionally,aprototypelossisusedtofurtherpushthesamplefeatures
towardsitscorrespondingprototypes,whichcanincreaseintra-classcompactnessasshowninFigure4(b),whilealso
actingasaformofregularizationtopreventmodeloverfitting.
Although PL has been widely applied in the field of computer vision, its potential in EEG-MI decoding has
beenscarcelyresearched. PLmethodsfocusonutilizingprototypelosstoincreaseintra-classcompactness,thereby
implicitly enhancing inter-class distance to form a margin, as illustrated in Figure 4 (b). Benefitting from larger
7margins,thePLmethodsoutperformCElossinbothgeneralclassificationtasks[35,40]andfew-shotlearning[39].
Therefore,inthispaper,wearededicatedtofurtherincreasinginter-classmarginsbasedonthePLmethod,inorder
toenhancethemodel’sgeneralizationcapabilityinMIdecodingtaskswithsmallsamples. Anaturalideaistoextend
theclusteredfeaturesalongthedirectionoftheircorrespondingprototypes,asshowninFigure4(c).
WeachievethisgoalusingDualPrototypeLearning,ultimatelyobtainingalargerinter-classmarginasshownin
Figure4(d).
Specifically, wedeveloptwoprototypesforeachclass: theInter-classSeparationPrototype(ISP)andtheIntra-
class Compact Prototype (ICP), aiming to achieve inter-class separation and intra-class compactness, respectively.
BasedontheISPs,weutilizesoftmaxandCElosstoachieveinter-classseparation:
L S(s,z)=−
m1 (cid:88) i=m
1
log(cid:80)n
je =s 1yi e·z si
j·zi, (11)
where m is the number of training samples, n is the number of classes, z is the feature of the i-th sample, y is the
i i
corresponding label in range [1,n], s represents the ISPs, and s ∈ Rd is the ISP of class j. Minimizing L can
j S
facilitatetheseparationoffeaturesfromdifferentclasses,resultinginafeaturespacesimilartoFigure4(a).
Furthermore,weuseintra-classcompactnesslosstocompressthedistancebetweensamplesbelongingtothesame
classinthefeaturespace,whichisdefinedas:
(cid:88)m
L (c,z)= D(z,c ), (12)
C i yi
i=1
where c represents the ICPs, c ∈ Rd is the ICP of class y, and D is the distance function. To prevent training
yi i
oscillations and mitigate the influence of outlier samples, we usethe Huber loss L (z,c) with δ = 1 as the distance
δ
functionD(z,c),whichisdefinedas:

L
(z,c)=1 2(z−c)2 if |z−c|≤δ
. (13)
δ δ|z−c|− 1δ2 if |z−c|>δ
2
L canrepresentthecompactnessofeachclass’sfeaturevectors. ByminimizingL ,wecanincreasetheintra-class
C C
compactnesssothatfeaturesofthesameclassareclusteredtogetherlikeFigure4(b).
Previous PL methods use a single prototype for each class, as described in Figure 4 (b). In comparison, we
decouple inter-class separation and intra-class compactness by using ISPs and ICPs. On one hand, this decouple
enhancestherobustnessofthetrainingprocess. Ontheotherhand, itprovidestheconditionsforfurtherincreasing
the inter-class margins. Specifically, we apply an implicit force and an explicit force to the features to achieve the
featurespaceoptimizationfromFigure4(c)toFigure4(d).
• Implicitforce. Duetothesoftmax’sproperties[49],theL tendstoincreases ·z untilitconvergestoacon-
S yi i
stantvalueduringtraining. Thisproceduresimultaneouslydisplacesthefeaturevectorz anditscorresponding
i
ISP s awayfromtheoriginofthefeaturespaceuntilconvergence. Furthermore,ifweconstrainthenormof
yi
ISPstoasmallervalue,i.e.,∥s∥ ≤ S (weight-normalization),thefeaturevectorswillbepushedfurtheraway
i 2
fromtheorigin. Thisconstraintcanactasanimplicitforce.
• Explicitforce. Tocomplementtheimplicitforce,wedesignasimplelossfunction,L ,toincreasethenorms
EF
ofICPs:
L (c)=−∥c∥ . (14)
EF 2
By minimizing L , the norms of ICPs increase, thereby guiding the features to be pushed away from the
EF
origin.
Duringthetrainingphase,theoptimizationobjectiveoftheproposedDPLisasfollows:
minimize L (s,z)+λL (c,z)+αL (c)
S C EF
, (15)
subjectto ∥s∥ ≤S, ∀i=1,2,...n
i 2
8whereλandαarethetrade-offscalartobalancethethreelosses,andS issetto1. Duringthetestingphase,thetest
sampleX isclassifiedbycalculatingthedotproductbetweenitsfeaturevectorz andtheISPforeachclass:
i i
yˆ =argmax(z ·s ), j=1,2,...n, (16)
i i j
j
whereyˆ denotesthepredictedresult.
i
4. Experimentsandresults
4.1. EvaluationDatasets
To demonstrate the effectiveness of our EDPNet, we evaluate it on two public MI-EEG datasets, namely, BCI
CompetitionIV2a(BCIC-IV-2a)[50]andBCICompetitionIV2b(BCIC-IV-2b)[51]. Additionally,weusetheBCI
competition III IVa (BCIC-III-IVa) [52] with fewer training data to further validate the generalization ability of the
proposedmethod.
DatasetI.BCIC-IV-2aprovidedbyGrazUniversityofTechnologyconsistsofEEGdatafrom9subjects. There
werefourmotorimagerytasks,coveringtheimaginationofmovingthelefthand,righthand,bothfeet,andthetongue.
Twosessionsondifferentdayswerecollectedwith22Ag/AgClelectrodesatasamplingrateof250Hz. Onesession
contained 288 EEG trials, i.e., 72 trials per task. We use [2, 6] seconds of each trial and all 22 electrodes in the
experiments.
DatasetII.BCIC-IV-2bprovidedbyGrazUniversityofTechnologyconsistsofEEGdatafrom9subjects. There
weretwomotorimagerytasks,coveringtheimaginationofmovingleftandrighthand. Fivesessionswerecollected
withthreebipolarelectrodes(C3,Cz,andC4)atasamplingrateof250Hzandeachsessioncontained120trials. We
usethe[3,7]secondsofeachtrialintheexperiments.
DatasetIII.BCIC-III-IVa,recordedat100Hzusing118electrodes,contains280trialspersubjectandcomprises
twodistinctclasses: righthand,andfoot. Thisdatasetdistinguishesitselffromotherdatasetsthroughitsimbalanced
divisionintotrainingandtestingtrials. Thequantityoftrainingtrialsfluctuatesbetween28and224,varyingwiththe
subject(al: 224, aa: 168, av: 84, aw: 56, ay: 28), withtheresidualtrialsdesignatedfortesting. Eachtriallasts3.5
seconds. Toprecludeoverfittingbyreducingthenumberofdatapointspertrial,weselectthethreechannelsshared
(C3,Cz,andC4).
Asthecompetitionguidelines[51]forBCIC-IV-2aandBCIC-IV-2bdatasets,weapplyhold-outanalysistoevalu-
atetheperformanceofourEDPNetandallcomparisonmethods. Assuch,themodelistrainedandtestedcompletely
in different sessions. This evaluation method is more in line with practical application scenarios and can better test
thegeneralizationabilityofthemodel. FortheBCIC-III-IVadataset,wefollowitsofficialprotocoltofurthervalidate
theadvantagesofourmethodonsmall-sampletrainingdatasets.
4.2. ExperimentalSetups
4.2.1. ExperimentalDetails
Inthisstudy,weimplementourEDPNetusingthePyTorchlibrary,basedonPython3.10withanNvidiaGeforce
2080TiGPU.WeusetheAdamWoptimizerwithdefaultsettings(learningrate=0.001,weightdecay=0.01)totrain
thefeatureextractorofourEDPNet. Additionally,weuseanotherAdamoptimizertooptimizeISPsandICPs,witha
learningrateof0.001onDatasetsIandII,andalearningrateof0.01onthesmall-sampleDatasetIII.Moreover,for
thehyperparametersofthemodelinFigure1,onDatasetIandII,weempiricallysetthekernelsizeofLightConvas
75,F andF as9and48,andthekernelsizesofdifferentscalesoftheMVPlayeras50,100,and200. OnDataset
1 2
III,duetothedifferencesinsamplingrate,wesetthekernelsizeofLightConvas50,andthekernelsizesofdifferent
scalesoftheMVPlayeras50,100,and150.
To prevent overfitting and reduce the number of epochs needed to train the model, a two-stage training strategy
asin[8]isusedinthiswork. Specifically,duringthetrainingphase,thetrainingdataissplitintoatrainingsetanda
validationset. Inthefirststage,onlythetrainingsetisused,andthetrainingisstoppedifthereisnodecreaseinthe
validation set loss for N consecutive epochs or reach the maximum training epoch N . During the second training
e 1
stage,alltrainingdataareemployed,thencontinuetrainingfor N epochs. Duetothedifferentsizesofthedatasets
2
used,wesetN ,N ,andN tobe1000,200,and300respectivelyforDatasetI,and300,150,and200,respectively,
1 e 2
forDatasetII.ForDatasetIII,wesetthemtobe300,150,and150.
9Table1:ClassificationAccuracy(%)andKappaComparisonswithSOTAMethodsonDatasetI.
Methods A01 A02 A03 A04 A05 A06 A07 A08 A09 Average Std Kappa p-value
FBCSP[53] 76.00 56.50 81.25 61.00 55.00 45.52 82.75 81.25 70.75 67.75 12.89 0.5700 0.0020
EEGNet[10] 85.76 61.46 88.64 67.01 55.90 52.08 89.58 83.33 79.51 74.50 13.85 0.6600 0.0020
TS-SEFFNet[25] 82.29 49.79 87.57 71.74 70.83 63.75 82.92 81.53 81.94 75.17 11.32 0.6630 0.0020
LMDA-Net[16] 83.90 60.30 88.10 78.20 56.20 57.20 88.40 82.70 84.30 75.40 12.78 0.6700 0.0020
Basenet-SE[26] 81.60 52.08 90.28 73.96 76.39 62.85 86.81 80.56 79.51 76.00 11.22 0.6794 0.0020
M-FANet[27] 86.81 75.00 91.67 73.61 76.39 61.46 85.76 75.69 87.17 79.28 8.84 0.7259 0.0137
ATCNet[22] 86.81 68.40 92.01 73.61 78.82 62.15 86.46 87.15 83.33 79.86 9.37 0.7312 0.0020
Conformer[23] 87.85 54.86 86.46 76.04 58.33 59.72 89.58 83.33 81.25 75.27 13.06 0.6702 0.0020
FBMSNet[11] 87.85 66.32 92.36 76.74 72.57 62.15 80.21 86.46 87.85 79.17 9.91 0.7235 0.0020
EDPNet 89.58 71.88 93.06 82.64 81.25 70.14 89.93 89.24 89.24 84.11 7.83 0.7881 -
Bestperformancesarehighlightedinbold,whilethesecond-bestwithunderlined.
Table2:ClassificationAccuracy(%)andKappaComparisonswithSOTAMethodsonDatasetII.
Methods B01 B02 B03 B04 B05 B06 B07 B08 B09 Average Std Kappa p-value
FBCSP[53] 70.00 60.36 60.94 97.50 93.12 80.63 78.13 92.50 86.88 80.01 13.06 0.6000 0.0059
EEGNet[10] 71.50 58.65 81.12 96.25 86.23 77.88 85.12 91.10 80.15 80.89 10.43 0.6321 0.0020
TS-SEFFNet[25] 72.81 65.71 75.75 96.25 91.25 85.00 88.63 91.87 82.18 83.27 9.51 0.6637 0.0020
LMDA-Net[16] 75.80 63.20 65.20 97.30 94.30 84.50 82.40 92.90 87.00 82.51 12.40 0.6500 0.0039
Basenet-SE[26] 72.50 67.86 81.13 96.86 93.44 84.69 88.75 93.44 84.69 84.82 9.20 0.6918 0.0057
ATCNet[22] 72.50 67.64 80.31 95.94 96.06 88.12 86.88 89.69 90.94 85.34 9.38 0.7068 0.0645
Conformer[23] 74.56 57.00 62.50 97.01 92.36 83.44 85.00 93.44 87.19 81.39 13.16 0.6265 0.0086
FBMSNet[11] 71.30 55.20 80.55 97.15 95.00 84.66 85.23 91.10 87.13 83.04 12.25 0.6692 0.0039
EDPNet 77.50 68.93 82.81 96.88 96.25 87.50 89.06 93.44 87.50 86.65 8.58 0.7330 -
Bestperformancesarehighlightedinbold,whilethesecond-bestwithunderlined.
4.2.2. PerformanceMetrics
Intheexperiments,theclassificationaccuracy(ACC)andtheCohen’skappacoefficient(Kappa)areusedastwo
metricsforperformanceevaluation. ThemathematicalformulaofCohen’skappacoefficientisdefinedasfollows:
P −P
Kappa= 0 e, (17)
1−P
e
where p representstheclassificationaccuracyofthemodeland p representstheexpectedconsistencylevel. Never-
0 e
theless,aone-sidedWilcoxonsigned-ranktestisusedtoverifythesignificanceofimprovement.
4.3. OverallPerformanceComparison
WeconductextensiveexperimentsandcompareourmethodwithnumerousSOTAapproachesacrossthreepublic
datasets.Table1displaystheclassificationperformanceofallmethodsonDatasetI.OurEDPNetmethodachievesthe
highestaverageaccuracyof84.11%andthehighestaveragekappavalueof0.7881.Moreover,Ourmethodachievesa
levelofsignificancewithp<0.05comparedtoallbenchmarkmethods.Theresultsdemonstratethattheclassification
accuracy of our proposed EDPNet is not only 16.36% higher than the competition champion solution FBCSP (p <
0.01)butalsosignificantlysurpassestheclassicEEGNetby9.61%(p<0.01).Thefourlatestattention-basedmethods
allapplytheattentionmechanismtodeepfeaturedimensions.Incontrast,ourEDPNetutilizeslightweightattentionin
thespatial-spectraldimension.Consequently,ourapproachismoreinterpretableandhasanaccuracyof4.83%higher
thanthebestattention-basedmethodM-FANet.Comparedtotransformer-basedmethods,weutilizeanon-parametric
andcomputationallyefficientMVPmoduletoextractlong-termtemporalfeatures. Theresultsdemonstratethatour
method achieves accuracy improvements of 4.25% and 8.84% compared to ATCNet and Conformer, respectively.
10Table3:ClassificationAccuracy(%)andKappaComparisonswithSOTAMethodsonDatasetIII.
al aa av aw ay
Methods Average Kappa
224/256 168/112 84/196 56/224 28/252
EEG-Net[10] 100 68.75 58.16 79.46 51.59 71.59 0.4331
LMDA-Net[16] 100 70.13 61.33 78.94 52.11 72.50 0.4567
Basenet+SE[26] 100 79.46 64.80 75.89 64.68 76.97 0.5374
ATCnet[22] 100 75.00 61.22 79.02 53.57 73.76 0.4750
FBMSNet[11] 100 82.14 57.14 82.04 59.33 76.13 0.5270
EDPNet 100 88.39 70.41 83.48 67.86 82.03 0.6426
Bestperformancesarehighlightedinbold,whilethesecond-bestwithunderlined.
ComparedtoFBMSNet,ourEDPNetnotonlyincreasesintra-classcompactnessbutalsofurtherenlargesinter-class
margins,displayinganaccuracyimprovementof4.94%.
TheexperimentalresultsonDatasetIIarepresentedinTable2, whichshowssimilarresultstothoseonDataset
I. Our EDPNet also achieves the highest average accuracy of 86.65% with the smallest standard deviation and the
highest average kappa value of 0.7330. And our method demonstrates significant advantages compared to most of
the comparison methods. Notably, on Datasets I and II, our method achieves the best or second-best accuracy for
nearly all subjects. Particularly, on Dataset I, for subjects A02 and A06 where other methods do not perform well,
ourmethodachievesanaccuracyabove70%. Assuggestedin[11], aBCIsystemwith> 70%binaryclassification
accuracyisgenerallyconsideredtobeusableforhealthysubjectsandstrokepatients. Thisdemonstratesthepotential
ofourEDPNetforMI-basedBCIapplications.
Moreover, on the smaller training Dataset III, we reproduce several SOTA methods suitable for comparison on
this dataset. As shown in Table 3, the number of training samples for the 5 subjects in Dataset III decreases from
224 to 28. Our EDPNet achieves an average recognition accuracy of 82.03% and a kappa value of 0.6426, which
are5.06%and0.1052higherthanthesecond-bestmethod,respectively. Especiallyforthesubject”ay”withonly28
trainingsamples,ourmethodstillachievesarecognitionaccuracyof67.86%. Thisexperimentalresultdemonstrates
thesuperiorgeneralizationabilityofourmethodinsmall-sampleEEGdecodingtasks.
4.4. AblationStudy
ThesignificantimprovementofourEDPNetcanbeattributedtothreenoveldesigns:theAdaptive-Spatial-Spectral
fusion module, the Multi-scale Variance Pooling module, and the Dual Prototype Learning approach. To further
analyzetheimpactofthesethreemodulesonmodelperformance,weconductablationexperimentsonDatasetsIand
II.Fourmodels,namedModel1,Model2,Model3andModel4,areutilized,whichrepresentfourscenariosasfollows:
• Model1 The model is realized by removing the ASSF module and adopting a depthwise convolution used in
EEGNettofusetheinformationbetweenEEGelectrodes.
• Model2 This model is implemented by using a single kernel size (100) variance pooling layer, to verify the
importanceofmulti-scaletemporalinformation.
• Model3ThismodelremovestheDPLmoduleandusesCElosstooptimizeparameters.
• Model4ThismodelremovestheDPLmoduleandusesthePLmethod[40]tooptimizeparameters.
Figure 5 shows the ablation data of the accuracy for each subject on Dataset I. It can be clearly seen that the
ASSF module brings a 4.83% average accuracy improvement for Model1. This is because the ASSF module uses
attention mechanisms to highlight specific spectral features of EEG electrodes related to the specific MI task and
moreeffectivelyfusesspatial-spectralinformation. Similarly,theMVPcanbringanaverageaccuracyimprovement
of 2.98% for Model2, as it better adapts to changes in the length of MI-related activity segments between different
11EDPNet Model1 Model2 Model3 Model4
100
95
90
85
)
%
80
(
y
c a75
r
u
c70
c
A
65
60
55
50
A01 A02 A03 A04 A05 A06 A07 A08 A09 AVG
Subject
Figure5:TheaccuracycomparisonofeachsubjectinDatasetI.
EDPNet Model1 Model2 Model3 Model4
100
95
90
85
)
%
80
(
y
c a75
r
u
c70
c
A
65
60
55
50
B01 B02 B03 B04 B05 B06 B07 B08 B09 AVE
Subject
Figure6:TheaccuracycomparisonofeachsubjectinDatasetII.
subjects. Employing dual prototype learning yields the most substantial enhancements. Compared with Model3,
it not only results in an 8.26% average accuracy improvement, but also results in consistent improvement on each
subject. OurDPLapproach, whencomparedtoCEloss, notonlyincreasesintra-classcompactnessbutalsofurther
enlargesinter-classmargins.Thisgreatlyenhancesthemodel’sgeneralizationcapabilityandrecognitionperformance.
Moreover,ourEDPNethasimprovedtheaccuracyby4.98%comparedtoModel4. ThisdemonstratesthatourDPL
methodhaseffectivelyimprovedupontheclassicalPLmethods. AsimilarresultisalsoobservedonDatasetII.As
showninFigure6,onDatasetII,ourEDPNetachievesaccuracyimprovementsof2.00%,0.96%,2.82%,and0.85%
comparedtoModel1,Model2,Model3,andModel4,respectively.
12(a) DatasetI (b) DatasetII (c) DatasetIII
Figure7:TheaccuracyofEDPNetacrossvarioussettingsofλandαonthreedatasets.
4.5. ParameterSensitivity
OurEDPNetemploysacombinationofL , L andL asthefinallossfunction, asshowninEq.(15). While
s c EF
the L loss aims to minimize misclassification of subject movement intent, the L loss minimizes the sum of the
s c
embeddedspacedistanceofsamplesinaclasstoitscenter,makingthesamplesbelongingtothesameclasscompact
in the feature space. And L provides an explicit force, pushing the features away from the origin of the feature
EF
space to achieve larger inter-class margins. The λ and α are used to balance the impact of these three losses. To
evaluatetheinfluenceoftheλandα,anempiricalinvestigationcomparestheperformanceofEDPNetacrossvarious
settingsonallthreedatasets.
AsshowninFigure7,λ = 0.001isthemostsuitablevalueacrossallthreedatasets. Whenλisincreasedto0.01
ordecreasedto0.0001,thereisanoticeabledecreaseinaccuracyonDatasetsIandII.Whenλisfixedat0.001and
αvariesbetween0and0.001,theaccuraciesareconsistentlyhighandreachthebestperformanceatα=0.00001on
DatasetIandII.Itisworthnotingthatevenwhenα=0,theaccuraciesonDatasetsIandIIremainhigh.Thisindicates
that our DPL can automatically learn larger inter-class margins relying solely on the implicit force. In contrast, on
DatasetIII,thebestperformanceisachievedwhenαisincreasedto0.01. ThisimpliesthatincreasingtheL can
EF
furtherimproveclassificationaccuracyondatasetswithfewertrainingsamples.
Insummary,ourEDPNetisrelativelyrobusttothevaluesofthehyperparametersλandα. Whenλ=0.001andα
issettoasmallvalue(i.e.,α<0.001),EDPNetcanachievegoodperformance. Iftheamountoftrainingdataisvery
limited,furtherincreasingαcanbeconsidered.
5. FurtherAnalysis
5.1. EffectofAdaptiveSpatial-SpectralFusion
ThekeytoourASSFmoduleliesinusingtheSSAtomodeltherelationshipsbetweenEEGelectrodes.Thesignal
amplitudeofspecificspectralbandsofdifferentEEGelectrodesmayincreaseordecreasewhenperformingdifferent
MItasks(suchasimaginingthehandmovementandthefootmovement). OurSSAexploitsthisphenomenontohelp
the model focus on it for classification. SSA generates adaptive attention weights in the spatial-spectral dimension
basedontheinputEEGrepresentationstore-weighttheEEGrepresentations. Thisenablesthemodeltofocusmore
onspatial-spectralfeaturesrelevanttothecurrenttask.
To further verify the role of the SSA mechanism in imagining movements of different body parts, we use the
t-SNE[54]methodtovisualizetheattentionvectorsdefinedinEq.(6)whenperformingdifferenttasks. Asshownin
Figure8(a), therearedistinctdifferencesinthedistributionofattentionvectorswhenA03performsMIofthehand
andMIofothersonDatasetI.Similarly,whenthesubject”al”inDatasetIIIperformingMItasksofthehandandfoot,
thedistributionofattentionvectorsalsoshowsclearboundariesandclusters.Thisfullydemonstratestheeffectiveness
andinterpretabilityofourSSAmechanism.
13hand hand
others foot
(a) A03inDatasetI (b) ”al”inDatasetIII
Figure8: ThedistributionofattentionvectorswhenperformingdifferentMItasksontwodatasets. Allattentionvectorsaremappedtothe2D
spaceusingthet-SNEmethod.
5.2. EffectofMulti-scaleVariancePooling
OurMVPmoduleinnovativelyusesapoolinglayerwithalargekernelsizetoextractlong-termtemporalinforma-
tion. Moreover,utilizingthecrucialpriorknowledgeofspectralpowerinEEGsignals,wedesignavariancepooling
layer. ThisintegratestheEEGpriorintothearchitecturedesignoftheneuralnetwork. Mostimportantly,compared
to transformer-based models, our MVP module has no learnable parameters and is computationally efficient. The
ablation experiments in Figure 5 and Figure 6 demonstrate that using only a single kernel size (100) for variance
poolingachievesanaccuracyof81.13%and85.69%onDatasetsIandII,respectively. Thisresultissuperiortothe
comparisonmethodsinTable1andTable2.
Moreover,withinonetrial,thestartpointanddurationoftheactualMIperiodshowingtheappropriateERSand
ERD pattern can be different from trial to trial [20]. This phenomenon is more significant among trials between
different subjects. In order to adapt to these differences between trials and extract more discriminative temporal
information,wegrouptheEEGrepresentationsalongthechanneldimensionandusevariancepoolingwithdifferent
kernelsizestoextractmulti-scaletemporalinformation. AsshowninFigure9,asmallerkernelsizeof50performs
betteronsubjectsA02, A04, and A09, while alargerkernelsizeof200 performsbetteronsubjectsA05, A07, and
A08. Altogether, the MVP integrates information from different scales and achieves the best overall performance
acrossallsubjects.
Figure9:Comparisonoftheaccuracyofsingle-scalevariancepoolingandMVP.
14A01 A02 A03 A04 A05 A06 A07 A08 A09
Figure10:t-SNEvisualizationofthefeaturedistributionforeachsubjecttrainedonDatasetIusingCEloss(firstrow)andDPL(secondrow).The
pointswithdifferentcolorsdenotefeaturesfromdifferentclasses.
5.3. EffectofDualPrototypeLearning
Our EDPNet is the first to apply prototype learning methods to MI-EEG decoding. Furthermore, we decouple
inter-classseparationandintra-classcompactnessbyusingtwoprototypes,ISPandICP,foreachclass. Figure5and
Figure 6 demonstrate that our DPL method significantly improves the recognition accuracy of MI tasks compared
to CE loss. To further explain the effectiveness of DPL, we visualize the distribution of feature vectors z using the
t-SNE method. Figure 10 displays the feature distribution of all subjects on Dataset I under both CE loss and DPL
optimizationapproaches.ItisapparentthatourDPLmethodachievesgreaterintra-classcompactnessandlargerinter-
classmarginscomparedtoCEloss.Therefore,ourDPLsignificantlyenhancesthemodel’sgeneralizationability.This
alsointuitivelyexplainswhyourDPLachievesbetterclassificationaccuracy.
ToverifythatourDPLpushesthefeaturesfurtherawaycomparedtothePLmethod,therebyachievinglargerinter-
class margins, we visualize the feature norm distribution of our DPL method and the PL method. Figure 11 shows
the L2 norm distribution of deep features trained with DPL and PL on Dataset I. It can be clearly seen that across
allsubjects,thefeaturenormstrainedusingDPLarestatisticallylargerthanthosetrainedusingPL.Thisrealizesthe
featurespaceoptimizationprocessfromFigure4(b)toFigure4(d),demonstratingthesuperiorityofDPL.
5.4. ComputationalExpenses
As BCI systems typically operate in online or closed-loop mode on devices with limited computational re-
sources [26], it is crucial to examine the computational expense of new algorithms. Table 4 displays the prepro-
cessingmethods,theclassificationaccuracy,modelparameters,andFloatingPointOperations(FLOPs)forDatasetI.
LMDA-NetneedstouseEuclideanalignment(EA)toachievehighrecognitionperformance,butEAisnotsuitable
for real-time testing. FMBSNet and M-FANet require time-consuming multi-narrow-band band-pass filtering. In
comparison,ourEDPNetdoesnotrequireanypreprocessingstepsandachievesthehighestrecognitionaccuracywith
the lowest FLOPs. This suggests that our model optimally balances the accuracy and speed of MI-EEG decoding.
Table4: ComparisonswithSOTAMethodsintheComputationalExpensesandRecognitionPerformanceon
DatasetI.
Methods Preprocessing Acc(%) Kappa Parameters(k) FLOPs(M)
LDMA-Net[16] EA&BP 75.40 0.6700 3.71 50.38
FBMSNet[11] MBP 79.17 0.7235 16.23 99.95
M-FANet[27] MBP 79.28 0.7259 4.08 23.39
Conformer[23] BP 75.27 0.6702 789.57 63.86
ADCNet[22] nopreprocessing 79.86 0.7312 113.73 29.81
EDPNet nopreprocessing 84.11 0.7881 15.21 9.65
BP:band-passfiltering,MBP:multi-narrow-bandband-passfiltering.
15A01 A02 A03
25
DPL DPL DPL
20 PL 30 PL 25 PL
20
15 20 15
10
10
10
5 5
0 0 0
10 15 20 25 30 10 15 20 25 30 10 15 20 25
L2 Norm Value L2 Norm Value L2 Norm Value
A04 A05 A06
50 DPL 50 DPL 50 DPL
PL PL PL
40 40 40
30 30 30
20 20 20
10 10 10
0 0 0
10 15 20 25 30 10 20 30 40 10 20 30 40
L2 Norm Value L2 Norm Value L2 Norm Value
A07 A08 A09
50
30 DPL DPL 30 DPL
PL 40 PL PL
25
20 30 20
15 20
10 10
10
5
0 0 0
10 15 20 25 10 20 30 40 5 10 15 20 25
L2 Norm Value L2 Norm Value L2 Norm Value
Figure11:HistogramoftheL2normdistributionoffeaturesforeachsubjecttrainedonDatasetIusingPLandDPL,respectively.
Moreover,thenumberofparametersforourmodelismuchlessthantransformer-basedmethods,andisonparwith
lightweightCNNmethods.
5.5. LimitationandFutureWork
AlthoughtheproposedEDPNetaddressesmajorchallengesinEEG-MIdecodingandachievesexcellentperfor-
mance,therearesomelimitationsinourcurrentwork. First,incorporatingmorepriorknowledgeintoneuralnetwork
design is worth exploring, such as considering the mirror distribution of EEG electrodes and the functional parti-
tioning of the brain [24, 55]. Although the LightConv proposed in the SSE module could potentially leverage this
priorknowledge,wedidnotfurtherexplorethisaspectasitisnotthefocusofthiswork. Second,thepotentialofthe
brain-inspiredDPLframeworkremainstobefullyexplored.Bydecouplinginter-classseparationandintra-classcom-
pactness, we simply constrain the prototype and feature distribution to achieve superior performance. Nonetheless,
more effective and discriminative loss functions deserve further investigation. Finally, EDPNet has only undergone
offlinetestingonpublicdatasetsandhasnotyetbeenvalidatedinanonlineBCIenvironment. Inthefuture,wewill
continuetoenhanceEDPNetbasedontheseavenuestoachievegoodperformanceinonlineBCIapplications.
6. Conclusion
In this paper, we propose a lightweight and efficient dual prototype network for MI-EEG decoding. Based on
neurophysiological priors and EEG data characteristics, we design the ASSF module and MVP module to extract
16
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerFhigh discriminative features from EEG signals. The ASSF module utilizes a lightweight SSA mechanism to model
the relationship between EEG electrodes for the extraction of powerful spatial-spectral features. Then, the MVP
moduleisusedtocapturemulti-scalelong-termtemporalfeatures. Moreover,inspiredbytherecognitionmechanism
ofthehumanbrain,weproposeanovelDPLapproachtoexplicitlyincreaseintra-classcompactnessandinter-class
margins in the feature space. The DPL enhances the model’s generalization capability, thereby helping to alleviate
the limited sample issue. We conduct extensive experiments on three public datasets, and the results confirm that
our method surpasses other SOTA methods. The proposed EDPNet holds promising potential for MI-based BCI
applicationsduetoitsremarkableperformancecombinedwithlowcomputationalexpenses.
Declarationofcompetinginterest
The authors declare that they have no known competing financial interests or personal relationships that could
haveappearedtoinfluencetheworkreportedinthispaper.
Acknowledgments
ThisworkwaspartiallysupportedbyOYMotionTechnologies.
References
[1] X.Gao,D.Xu,M.Cheng,S.Gao,Abci-basedenvironmentalcontrollerforthemotion-disabled,IEEETransactionsonneuralsystemsand
rehabilitationengineering11(2)(2003)137–140.
[2] R.Abiri,S.Borhani,E.W.Sellers,Y.Jiang,X.Zhao,Acomprehensivereviewofeeg-basedbrain–computerinterfaceparadigms,Journalof
neuralengineering16(1)(2019)011001.
[3] K.Sakai,K.Goto,J.Tanabe,K.Amimoto,K.Kumai,H.Kamio,Y.Ikeda,Effectsofvisual-motorillusiononfunctionalconnectivityduring
motorimagery,ExperimentalBrainResearch239(7)(2021)2261–2271.
[4] P.D.E.Baniqued,E.C.Stanyer,M.Awais,A.Alazmani,A.E.Jackson,M.A.Mon-Williams,F.Mushtaq,R.J.Holt,Brain–computer
interfaceroboticsforhandrehabilitationafterstroke:Asystematicreview,Journalofneuroengineeringandrehabilitation18(2021)1–25.
[5] K.K.Ang,C.Guan,Eeg-basedstrategiestodetectmotorimageryforcontrolandrehabilitation,IEEETransactionsonNeuralSystemsand
RehabilitationEngineering25(4)(2016)392–401.
[6] J.Long,Y.Li,H.Wang,T.Yu,J.Pan,F.Li,Ahybridbraincomputerinterfacetocontrolthedirectionandspeedofasimulatedorreal
wheelchair,IEEETransactionsonNeuralSystemsandRehabilitationEngineering20(5)(2012)720–729.
[7] B.J.Edelman,J.Meng,D.Suma,C.Zurn,E.Nagarajan,B.S.Baxter,C.C.Cline,B.He,Noninvasiveneuroimagingenhancescontinuous
neuraltrackingforroboticdevicecontrol,Sciencerobotics4(31)(2019)eaaw6844.
[8] R.T.Schirrmeister,J.T.Springenberg,L.D.J.Fiederer,M.Glasstetter,K.Eggensperger,M.Tangermann,F.Hutter,W.Burgard,T.Ball,
Deeplearningwithconvolutionalneuralnetworksforeegdecodingandvisualization,Humanbrainmapping38(11)(2017)5391–5420.
[9] S.Sakhavi,C.Guan,S.Yan,Learningtemporalinformationforbrain-computerinterfaceusingconvolutionalneuralnetworks,IEEEtrans-
actionsonneuralnetworksandlearningsystems29(11)(2018)5619–5629.
[10] V.J.Lawhern,A.J.Solon,N.R.Waytowich,S.M.Gordon,C.P.Hung,B.J.Lance,Eegnet: acompactconvolutionalneuralnetworkfor
eeg-basedbrain–computerinterfaces,Journalofneuralengineering15(5)(2018)056013.
[11] K.Liu,M.Yang,Z.Yu,G.Wang,W.Wu,Fbmsnet: Afilter-bankmulti-scaleconvolutionalneuralnetworkforeeg-basedmotorimagery
decoding,IEEETransactionsonBiomedicalEngineering70(2)(2022)436–445.
[12] X.Gu,F.Deligianni,J.Han,X.Liu,W.Chen,G.-Z.Yang,B.Lo,Beyondsupervisedlearningforpervasivehealthcare,IEEEReviewsin
BiomedicalEngineering(2023).
[13] X.Zhang, L.Yao, X.Wang, J.Monaghan, D.Mcalpine, Y.Zhang, Asurveyondeeplearning-basednon-invasivebrainsignals: recent
advancesandnewfrontiers,Journalofneuralengineering18(3)(2021)031002.
[14] S.Li,H.Wu,L.Ding,D.Wu,Meta-learningforfastandprivacy-preservingsourceknowledgetransferofeeg-basedbcis,IEEEComputational
IntelligenceMagazine17(4)(2022)16–26.
[15] J.Han,X.Gu,G.-Z.Yang,B.Lo,Noise-factorizeddisentangledrepresentationlearningforgeneralizablemotorimageryeegclassification,
IEEEJournalofBiomedicalandHealthInformatics(2023).
[16] Z.Miao,M.Zhao,X.Zhang,D.Ming,Lmda-net:Alightweightmulti-dimensionalattentionnetworkforgeneraleeg-basedbrain-computer
interfacesandinterpretability,NeuroImage276(2023)120209.
[17] W.Tao,Z.Wang,C.M.Wong,Z.Jia,C.Li,X.Chen,C.P.Chen,F.Wan,Adfcnn: Attention-baseddual-scalefusionconvolutionalneural
networkformotorimagerybrain-computerinterface,IEEETransactionsonNeuralSystemsandRehabilitationEngineering(2023).
[18] X.Tang,C.Yang,X.Sun,M.Zou,H.Wang,Motorimageryeegdecodingbasedonmulti-scalehybridnetworksandfeatureenhancement,
IEEETransactionsonNeuralSystemsandRehabilitationEngineering31(2023)1208–1218.
[19] X.Liu,S.Xiong,X.Wang,T.Liang,H.Wang,X.Liu,Acompactmulti-branch1dconvolutionalneuralnetworkforeeg-basedmotorimagery
classification,BiomedicalSignalProcessingandControl81(2023)104456.
[20] P.Deny,S.Cheon,H.Son,K.W.Choi,Hierarchicaltransformerformotorimagery-basedbraincomputerinterface,IEEEJournalofBiomed-
icalandHealthInformatics(2023).
17[21] H.-J.Ahn, D.-H.Lee, J.-H.Jeong, S.-W.Lee, Multiscaleconvolutionaltransformerforeegclassificationofmentalimageryindifferent
modalities,IEEETransactionsonNeuralSystemsandRehabilitationEngineering31(2022)646–656.
[22] H.Altaheri,G.Muhammad,M.Alsulaiman,Physics-informedattentiontemporalconvolutionalnetworkforeeg-basedmotorimageryclas-
sification,IEEEtransactionsonindustrialinformatics19(2)(2022)2249–2258.
[23] Y.Song,Q.Zheng,B.Liu,X.Gao,Eegconformer: Convolutionaltransformerforeegdecodingandvisualization,IEEETransactionson
NeuralSystemsandRehabilitationEngineering31(2022)710–719.
[24] J.Zhang,K.Li,B.Yang,X.Han,Localandglobalconvolutionaltransformer-basedmotorimageryeegclassification,FrontiersinNeuro-
science17(2023)1219988.
[25] Y.Li, L.Guo, Y.Liu, J.Liu, F.Meng, Atemporal-spectral-basedsqueeze-and-excitationfeaturefusionnetworkformotorimageryeeg
decoding,IEEETransactionsonNeuralSystemsandRehabilitationEngineering29(2021)1534–1545.
[26] M.Wimpff,L.Gizzi,J.Zerfowski,B.Yang,Eegmotorimagerydecoding: Aframeworkforcomparativeanalysiswithchannelattention
mechanisms,JournalofNeuralEngineering21(3)(2024)036020.
[27] Y.Qin,B.Yang,S.Ke,P.Liu,F.Rong,X.Xia,M-fanet:Multi-featureattentionconvolutionalneuralnetworkformotorimagerydecoding,
IEEETransactionsonNeuralSystemsandRehabilitationEngineering(2024).
[28] K.Zhang,N.Robinson,S.-W.Lee,C.Guan,Adaptivetransferlearningforeegmotorimageryclassificationwithdeepconvolutionalneural
network,NeuralNetworks136(2021)1–10.
[29] S.Pe´rez-Velasco,E.Santamar´ıa-Va´zquez,V.Mart´ınez-Cagigal,D.Marcos-Mart´ınez,R.Hornero,Eegsym: Overcominginter-subjectvari-
abilityinmotorimagerybasedbciswithdeeplearning,IEEETransactionsonNeuralSystemsandRehabilitationEngineering30(2022)
1766–1775.
[30] H.W.Ng,C.Guan,Subject-independentmeta-learningframeworktowardsoptimaltrainingofeeg-basedclassifiers,NeuralNetworks172
(2024)106108.
[31] K.Yin, E.Y.Lim, S.-W.Lee, Gitgan: Generativeinter-subjecttransferforeegmotorimageryanalysis, PatternRecognition146(2024)
110015.
[32] D.Borra,S.Fantozzi,E.Magosso,Interpretableandlightweightconvolutionalneuralnetworkforeegdecoding: Applicationtomovement
executionandimagination,NeuralNetworks129(2020)55–74.
[33] T.Hanakawa,I.Immisch,K.Toma,M.A.Dimyan,P.VanGelderen,M.Hallett,Functionalpropertiesofbrainareasassociatedwithmotor
executionandimagery,Journalofneurophysiology89(2)(2003)989–1002.
[34] R.Mane,N.Robinson,A.P.Vinod,S.-W.Lee,C.Guan,Amulti-viewcnnwithnovelvariancelayerformotorimagerybraincomputer
interface,in:202042ndAnnualInternationalConferenceoftheIEEEEngineeringinMedicine&BiologySociety(EMBC),IEEE,2020,pp.
2950–2953.
[35] H.-M. Yang, X.-Y. Zhang, F. Yin, C.-L. Liu, Robust classification with convolutional prototype learning, in: Proceedings of the IEEE
conferenceoncomputervisionandpatternrecognition,2018,pp.3474–3482.
[36] Q.Wang,B.Wu,P.Zhu,P.Li,W.Zuo,Q.Hu,Eca-net:Efficientchannelattentionfordeepconvolutionalneuralnetworks,in:Proceedings
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,2020,pp.11534–11542.
[37] J.Hu,L.Shen,G.Sun,Squeeze-and-excitationnetworks,in:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
2018,pp.7132–7141.
[38] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,I.Polosukhin,Attentionisallyouneed,Advancesin
neuralinformationprocessingsystems30(2017).
[39] J.Snell,K.Swersky,R.Zemel,Prototypicalnetworksforfew-shotlearning,Advancesinneuralinformationprocessingsystems30(2017).
[40] H.-M.Yang,X.-Y.Zhang,F.Yin,Q.Yang,C.-L.Liu,Convolutionalprototypenetworkforopensetrecognition,IEEETransactionsonPattern
AnalysisandMachineIntelligence44(5)(2020)2358–2370.
[41] F.C.Borlino,S.Bucci,T.Tommasi,Contrastivelearningforcross-domainopenworldrecognition,in:2022IEEE/RSJInternationalConfer-
enceonIntelligentRobotsandSystems(IROS),IEEE,2022,pp.10133–10140.
[42] Z.Xia,P.Wang,G.Dong,H.Liu,Adversarialkineticprototypeframeworkforopensetrecognition,IEEETransactionsonNeuralNetworks
andLearningSystems(2023).
[43] B.Zhang,X.Li,Y.Ye,Z.Huang,L.Zhang,Prototypecompletionwithprimitiveknowledgeforfew-shotlearning,in: Proceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition,2021,pp.3754–3762.
[44] F.Zhou,P.Wang,L.Zhang,W.Wei,Y.Zhang,Revisitingprototypicalnetworkforcrossdomainfew-shotlearning,in: Proceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,2023,pp.20061–20070.
[45] F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, M. Auli, Pay less attention with lightweight and dynamic convolutions, arXiv preprint
arXiv:1901.10430(2019).
[46] H.Altaheri,G.Muhammad,M.Alsulaiman,S.U.Amin,G.A.Altuwaijri,W.Abdul,M.A.Bencherif,M.Faisal,Deeplearningtechniques
forclassificationofelectroencephalogram(eeg)motorimagery(mi)signals: Areview,NeuralComputingandApplications35(20)(2023)
14681–14722.
[47] Z.Yang,L.Zhu,Y.Wu,Y.Yang,Gatedchanneltransformationforvisualrecognition,in: ProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,2020,pp.11794–11803.
[48] W.Yu,C.Si,P.Zhou,M.Luo,Y.Zhou,J.Feng,S.Yan,X.Wang,Metaformerbaselinesforvision,IEEETransactionsonPatternAnalysis
andMachineIntelligence(2023).
[49] F.Wang,X.Xiang,J.Cheng,A.L.Yuille,Normface: L2hypersphereembeddingforfaceverification,in: Proceedingsofthe25thACM
internationalconferenceonMultimedia,2017,pp.1041–1049.
[50] M.Tangermann,K.-R.Mu¨ller,A.Aertsen,N.Birbaumer,C.Braun,C.Brunner,R.Leeb,C.Mehring,K.J.Miller,G.R.Mu¨ller-Putz,etal.,
Reviewofthebcicompetitioniv,Frontiersinneuroscience6(2012)55.
[51] K.K.Ang,Z.Y.Chin,C.Wang,C.Guan,H.Zhang,Filterbankcommonspatialpatternalgorithmonbcicompetitionivdatasets2aand2b,
Frontiersinneuroscience6(2012)21002.
[52] B.Blankertz,K.-R.Muller,D.J.Krusienski,G.Schalk,J.R.Wolpaw,A.Schlogl,G.Pfurtscheller,J.R.Millan,M.Schroder,N.Birbaumer,
18Thebcicompetitioniii: Validatingalternativeapproachestoactualbciproblems,IEEEtransactionsonneuralsystemsandrehabilitation
engineering14(2)(2006)153–159.
[53] K.K.Ang,Z.Y.Chin,H.Zhang,C.Guan,Filterbankcommonspatialpattern(fbcsp)inbrain-computerinterface,in:2008IEEEinternational
jointconferenceonneuralnetworks(IEEEworldcongressoncomputationalintelligence),IEEE,2008,pp.2390–2397.
[54] L.VanderMaaten,G.Hinton,Visualizingdatausingt-sne.,Journalofmachinelearningresearch9(11)(2008).
[55] J.Luo,Y.Wang,S.Xia,N.Lu,X.Ren,Z.Shi,X.Hei,Ashallowmirrortransformerforsubject-independentmotorimagerybci,Computers
inBiologyandMedicine164(2023)107254.
19