Accelerated Inference for Partially Observed Markov Processes
using Automatic Differentiation
K. Tan1, G. Hooker1 and E. L. Ionides2
1Department of Statistics, University of Pennsylvania
2Department of Statistics, University of Michigan
Draft compiled on July 4, 2024
Abstract
Automatic differentiation (AD) has driven recent advances in machine learning, including
deepneuralnetworksandHamiltonianMarkovChainMonteCarlomethods. Partiallyobserved
nonlinear stochastic dynamical systems have proved resistant to AD techniques because widely
used particle filter algorithms yield an estimated likelihood function that is discontinuous as
a function of the model parameters. We show how to embed two existing AD particle filter
methods in a theoretical framework that provides an extension to a new class of algorithms.
This new class permits a bias/variance tradeoff and hence a mean squared error substantially
lower than the existing algorithms. We develop likelihood maximization algorithms suited to
the Monte Carlo properties of the AD gradient estimate. Our algorithms require only a differ-
entiable simulator for the latent dynamic system; by contrast, most previous approaches to AD
likelihood maximization for particle filters require access to the system’s transition probabili-
ties. NumericalresultsindicatethatahybridalgorithmthatusesADtorefineacoarsesolution
from an iterated filtering algorithm show substantial improvement on current state-of-the-art
methods for a challenging scientific benchmark problem.
Many scientific models involve highly nonlinear stochastic dynamic systems possessing signifi-
cant random variation in both the process dynamics and the measurements. Commonly, the latent
system is modeled as a Markov process, giving rise to a partially observed Markov process (POMP)
model, also known as a hidden Markov models or a state space model. POMP models arise in fields
as diverse as automated control [1], epidemiology [2, 3], ecology [4] and finance [5, 6]. Despite their
ubiquity, estimation and inference within this broad class of models remains a challenging problem.
This article concerns the use automatic differentiation (AD) to construct improved algorithms for
inference on complex POMP models.
The particle filter, also known as sequential Monte Carlo, serves as the foundation for various
inference algorithms for POMP models. It provides an unbiased estimate of the likelihood function
[7], enabling Bayesian inference [8, 9] and likelihood-based inference [10, 11]. Likelihood-based
inference is statistically efficient [12] and is robust to a moderate amount of Monte Carlo error
[13, 14]. An attractive feature of basic particle filter algorithms, also known as boostrap filters, is
that they do not require evaluation of the transition density of the latent Markov process, enabling
an arbitrary model simulator to be plugged into the algorithm. This plug-and-play property is
useful for scientific applications [2]. Some plug-and-play methods depend on the construction
of low-dimensional summary statistics [15, 16], sacrificing statistical efficiency for computational
1
4202
luJ
3
]EM.tats[
1v58030.7042:viXraconvenience. Plug-and-play methods are often called likelihood-free [17], and it may be counter-
intuitivethatlikelihood-basedinferenceispossibleusinglikelihood-freemethods. However,iterated
filtering algorithms have shown that this is practical in various scientific investigations, such as
[18, 19, 20, 21, 22, 23]. Nevertheless, the Monte Carlo variability arising in iterated filtering
applications becomes increasingly problematic as the size of the data and the complexity of the
model increases. Algorithmic advances are needed to make plug-and-play likelihood maximization
forPOMPmodelsmorenumericallyefficient. Wedevelopplug-and-playADmethodswhichprovide
a large computational advantage over the methods used in these previous applications.
Recent advances in AD for particle filters [24, 25, 26, 27, 1] have drawn attention to AD as a
tool for inference in POMP models. However, existing approaches are either asymptotically biased
[24, 25], have high variance [28, 27], are computationally expensive [26, 29], or require access to
transition densities [28, 27, 1, 29].
Scibior and Wood [27] showed that the estimators derived by Poyiadjis et al. [28] can be
attained with standard AD software using an algorithmic procedure, called a stop-gradient, which
allows selected expressions to be evaluated but not differentiated. Unfortunately, [27] used their
estimator within an algorithm which does not have the plug-and-play property and which has high
Monte Carlo variability, while the stop-gradient procedure is introduced without mathematical
motivation. We use [27] and [28] as our starting point, while developing a new approach which
remedies the weaknesses of these papers.
Westartbypresentinganewconstructionofthegradientestimatorof[27]and[28]. Specifically,
we show how this estimator can be derived as the direct derivative of a suitably weighted particle
filter,which(forreasonswhichwillbediscussedlater)wecallaMeasurementOff-Parameter(MOP)
particle filter. In short, instead of directly differentiating through a basic particle filter, MOP uses
AD to differentiate through a smooth particle filter constructed using measurement density ratios
and a differentiable simulator. Our MOP algorithm differs from previous smooth particle filters
[30, 31] by possessing the plug-and-play property. Critically, we also avoid the high variance of [30]
by generalizing the MOP particle filter to add a discount factor, α ∈ [0,1]. This discount factor
enables MOP-α to interpolate between the biased gradient estimator of [24], when α = 0, and the
high-variance gradient estimate from [28, 27], when α = 1 (Theorem 1). The bias-variance tradeoff
induced by α suggests, both in theory (Theorem 5) and in practice (Figure 1), the use of α values
strictly between 0 and 1.
MOP-α avoids the issue of asymptotic bias [26] arising from having to drop terms arising from
resampling when differentiating through the standard particle filter as in [24] in a novel way. This
particle filter is smooth by construction, allowing us to bypass the seemingly incompatible para-
dox of differentiating through discrete Monte Carlo sampling by instead using AD to differentiate
throughaseriesofmeasurementdensityratiosandadifferentiablesimulator. Yet, MOP-αcannev-
ertheless provide strongly consistent particle and log-likelihood estimates (Theorem 3), and when
α = 1, consistent score estimates (Theorem 4).
We derive a linear convergence rate for stochastic gradient descent (SGD) using MOP-α in
the presence of strong convexity (Theorem 2). Critically, the estimator converges quickly within
a neighborhood of the maximum but struggles to reach this neighborhood in the presence of local
minima and saddle points. That behavior is complementary to that of iterated filtering algorithms
[10, 11] which provides a relatively fast and stable way to identify this neighborhood. We therefore
buildasimplebuteffectivehybridalgorithmcalledIteratedFilteringwithAutomaticDifferentiation
(IFAD) that warm-starts first-order or second-order gradient methods with a preliminary solution
2obtained from a few rounds of iterated filtering. Promising numerical results indicate that IFAD
beats IF2 (and by the numerical results of [11], also IF1 [10, 32] and the Liu-West filter [33]) on a
challenging problem in epidemiology, the Dhaka cholera model of [18].
These improvements also extend to Bayesian inference, as we show in Section 5 that we can
use the MOP-α gradient estimates within a No-U-Turn Sampler (NUTS) [34] in conjunction with
a nonparametric empirical Bayes-style prior estimated with IF2 to reduce the burn-in period of
particle MCMC [8] on the Dhaka cholera model of [18] from the 700,000 iterations in [35] to just
500. To the best of our knowledge, attaining rapid mixing on this challenging problem has not
previously been achievable.
1 Problem Setup
Consider an unobserved Markov process {X(t),t ≥ t }, with discrete-time observations Y ,...,Y
0 1 N
realized at values y∗,...,y∗ at times t ,...,t . The process is parameterized by an unknown pa-
1 N 1 N
rameter θ ∈ Θ ⊆ Rp, where the state X(t) take values in the state space X ⊆ Rd, the observations
Y take values in Y, and we write X := X(t ).
n n n
Wesupposethatthediscete-timelatentprocessmodelhasadensityf (x | x ;θ). The
Xn|Xn−1 n n−1
existence of this density is necessary to define the likelihood function, but plug-and-play algorithms
do not have access to evaluation of this density. Instead, they have access to a corresponding simu-
lator, which we write as process (x | x ;θ). We set f (y | x ;θ) to be the measurement
n n n−1 Yn|Xn n n
density, which we suppose can be evaluated. We write y∗ for the actual values of the observa-
n
tions that were observed. We call f (x |y∗ ;θ) the posterior distribution of states, and
X1:n|Y1:n 1:n 1:n
f (x |y∗ ;θ) the filtering distribution at time t . Superscripts xA of particles denote the
Xn|Y1:n n 1:n n n,j
ancestral trajectory of particle j at time n, a(·) is the ancestor function that maps a particle to its
parent, j (cid:55)→ a(j), and k denotes the resample indices drawn for each particle j.
j
The above densities are defined on a probability space (Ω,Σ,P) which is also assumed to enable
constructionofindependentreplicatesandallotherrandomvariablesdefinedinouralgorithms. For
the algorithmic interpretation of our theory, we identify the random number seed with an element
of the sample space ω ∈ Ω. The random seed determines the sequence of pseudo-random numbers
as generated by a computer, just as the outcome ω ∈ Ω generates the sequence of random variables.
2 Off-Parameter Particle Filters
Our approach is as follows. Instead of directly differentiating through a basic particle filter, we
instead use AD to differentiate through a series of measurement density ratios and a differentiable
simulator. This is done through the construction of a novel particle filtering algorithm called
Measurement Off-Parameter with discount factor α (MOP-α), defined by the pseudocode in Algo-
rithm 1. Note that the plug-and-play property forbids access to transition densities but permits
access to the density of the measurements conditional on the value of the latent process. The
simulator can be differentiated using AD as long as the underlying computer code for the simulator
is differentiable, and so MOP-α is applicable to a broad class of POMP models.
The MOP-α algorithm resamples the particles according to an arbitrary resampling rule that
depends on both the target parameter value and a baseline parameter value, and this explains
the name measurement off-parameter. This is intended as an analogy to off-policy learning in
reinforcement learning. Specifically, MOP-α evaluates the likelihood at some θ ∈ Θ, but instead
3resamplestheparticlesaccordingtotheindicesgeneratedbyavanillaparticlefilterrunatabaseline
parameter, ϕ ∈ Θ. This ensures that the resampling indices are invariant to θ when ω and ϕ are
fixed, bypassing the issue of Monte Carlo resampling. We show in Theorem 3 that MOP-α exactly
targets the filtering distribution when θ = ϕ or α = 1.
Wesupposethatthesimulatorisadifferentiablefunctionofθ foreveryfixedω, aconditionthat
requires the latent process to be a continuous random variable. This condition is also known as
the reparameterization trick [26]. Supposing also that the measurement model is differentiable in θ,
direct differentiation of MOP-α is available via AD. Theorem 1 shows that MOP-α is constructed
so this direct derivative obtains the score estimator of [28, 27] when α = 1 and that of [24] when
α = 0. Setting α < 1 adds bias but reduces variance, raising opportunities for a favorable tradeoff
as illustrated in theory (Theorem 5) and in practice (Figure 1).
Algorithm 1 MOP-α
Input: Number of particles J, timesteps N, measurement model f (y∗|x ,θ), simulator
Yn|Xn n n
process (x |x ;θ), evaluation parameter θ, behavior parameter ϕ, seed ω.
n n+1 n
First pass: Set θ = ϕ and fix ω, yielding XP,ϕ, XF,ϕ, gϕ .
n,j n,j n,j
Second pass: Fix ω, and filter at θ ̸= ϕ:
Initialize particles XF,θ ∼ f (·;θ), weights wF,θ = 1.
0,j X0 0,j
For n = 1,...,N:
Accumulate discounted weights, wP,θ = (cid:0) wF,θ (cid:1)α .
n,j n−1,j
Simulate process model, XP,θ ∼ process (cid:0) ·|XF,θ ;θ(cid:1) .
n,j n n−1,j
Measurement density, gθ = f (y∗|XP,θ;θ).
n,j Yn|Xn n n,j
Compute LB,θ,α = (cid:80)J gθ wP,θ(cid:14)(cid:80)J wP,θ.
n j=1 n,j n,j j=1 n,j
Conditional likelihood under ϕ, Lϕ = 1 (cid:80)J gϕ .
n J m=1 n,m
Select resampling indices k with P(cid:0) k = m(cid:1) ∝ gϕ .
1:J j n,m
Obtain resampled particles XF,θ = XP,θ .
n,j n,kj
Calculate resampled corrected weights wF,θ = wP,θ gθ (cid:14) gϕ .
n,j n,kj n,kj n,kj
Compute LA,θ,α = Lϕ ·(cid:80)J wF,θ(cid:14)(cid:80)J wP,θ.
n n j=1 n,j j=1 n,j
Return: likelihood estimate Lˆ(θ) = (cid:81)N LA,θ,α or Lˆ(θ) = (cid:81)N LB,θ,α, filtering distributions
n=1 n n=1 n
{(XF,θ,wF,θ)}N,J .
n,j n,j n,j=1
2.1 Algorithm Outline
Algorithm1constructstwocoupledsetsofparticles,oneunderϕ ∈ Θ,andanotherwiththeprocess
model at θ ∈ Θ but with the resampling indices constructed from the first pass, with the baseline
parameter, ϕ. The resampling indices, which we write as k ∼ Categorical(cid:0) gϕ ,...,gϕ (cid:1) , are a
j n,1 n,J
function of ϕ for any value of θ. For the categorical distribution, we use systematic resampling
[36, 37] which usually has superior performance to multinomial resampling. If θ and ϕ coincide,
one only needs one particle filter run at θ = ϕ, otherwise one needs two runs at the same seed
ω ∈ Ω.
Algorithm 1 reweights the conditional likelihoods by a correction factor accumulated over time
to account for the resampling under ϕ. Writing gθ = f (y∗|XP,θ;θ) for the measurement
n,j Yn|Xn n n,j
density and Lϕ = 1 (cid:80)J gϕ for the conditional likelihood estimate under ϕ, we obtain two
n J m=1 n,m
4suitably weighted estimates of the conditional likelihood under θ,
(cid:80)J gθ wP,θ (cid:80)J wF,θ
LB,θ,α = j=1 n,j n,j , LA,θ,α = Lϕ · j=1 n,j , (1)
n (cid:80)J wP,θ n n (cid:80)J wP,θ
j=1 n,j j=1 n,j
where the weights for each particle are updated recursively to correct for the cumulative error
incurred by the off-parameter resampling:
wP,θ = (wF,θ )α, wF,θ = wP,θ gθ (cid:14) gϕ , wF,θ = 1. (2)
n,j n−1,j n,j n,kj n,kj n,kj 0,j
The before-resampling conditional likelihood estimate LB,θ,α is preferable in practice, as it has
n
slightly lower variance than the after-resampling estimate LA,θ,α, but the latter is useful in deriving
n
properties of the MOP-α gradient estimate such as that in Theorem 1.
If α = 1 in Algorithm 1, the weights, wP,θ and wF,θ, accumulate as n increases. This leads
n,j n,j
to numerical instability unless N is small. For α < 1, the weights fom previous timesteps are
discounted, as in Equation (2). Heuristically, α controls a rate of exponential decay of the memory
that the filter at time t , and its resulting gradient estimate, has over the ancestral trajectories
n
prior to t . With α < 1, Algorithm 1 continues to target the filtering distribution and likelihood if
n
θ = ϕ. However, we expect bias for θ ̸= ϕ that shrinks as θ approaches ϕ. This lets us optimize a
bias-variance tradeoff for the MOP-α score estimate.
When α = 1, MOP-1 maintains complete memory of each particle’s ancestral trajectory, and
Theorem 1 shows that it recovers the consistent but high-variance gradient estimate from [28, 27].
At the other extreme, when α = 0, MOP-0 considers only single-step transition dynamics, recover-
ing the low-variance but asymptotically biased gradient estimator of [24]. The novel possibility of
0 < α < 1 is effective both in theory (Theorem 5) and in practice (Figure 1).
Our approach has similarities with [30], who construct a deterministic local approximation of
the likelihood at the k-th optimization step via saving the particles from a run at some θ ,
k−1
perform a reweighting with transition and measurement density ratios to evaluate the likelihood at
any sufficiently nearby θ, and then use a 0-th order optimizer such as optim in R to choose a θ that
k
maximizes the likelihood in that neighborhood. However, due to their use of transition densities in
the reweighting, their approach is not plug-and-play. The variance of their log likelihood estimate
is also roughly O(||θ−θ ||2N), where N is the horizon. Our approach bypasses these two issues.
k−1 2
The use of the reparameterization trick to consider particle paths dependent on θ (and not θ or
k−1
ϕ) allows us to retain the plug-and-play property. On the other hand, our use of AD allows us to
only evaluate the likelihood and score estimates when θ = ϕ. The use of AD bypasses the issue of
exponentially increasing variance, but nevertheless can result in problematic polynomial variance.
Our introduction of α avoids that difficulty.
2.2 MOP-α Encompasses the Estimators of [28, 27, 24]
[27] show that the estimate of [24] is the gradient of a vanilla particle filter when resampling terms
are dropped, and also recover the estimate of [28] through the use of a stop-gradient operation in
theADprocedure. Itturnsoutthatbothofthese, whenappliedonthebootstrapfilter, correspond
to special cases of MOP-α.
5Theorem 1 (MOP-0 and MOP-1 Functional Forms). Writing ∇ ℓˆα(θ) for the gradient estimate
θ
yielded by MOP-α when θ = ϕ and using the after-resampling conditional likelihood estimate so
that Lˆ(θ) = (cid:81)N LA,θ,α, when α = 0,
n=1 n
N J
∇ ℓˆ0(θ) =
1 (cid:88)(cid:88)
∇
log(cid:16)
f
(y∗|xF,θ;θ)(cid:17)
,
θ J θ Yn|Xn n n,j
n=1j=1
yielding the estimate of [24] for the bootstrap filter. When α = 1,
J
∇ ℓˆ1(θ) =
1 (cid:88)
∇ logf
(cid:16)
y∗
|xA,F,θ(cid:17)
,
θ J θ Y1:N|X1:N 1:N 1:n,j
j=1
yielding the estimator of [28, 27] for the bootstrap filter.
We defer the proof to Appendix A. The argument relies on a useful decomposition of the after-
resampling conditional likelihood estimate LA,θ,α that yields a telescoping product in the MOP-1
n
case. Repeated applications of the log-derivative identity that ∇ log(f(x)) = (∇ f(x))/f(x), and
x x
noting θ = ϕ implies that wP,θ evaluates to 1, yield the result.
n,j
This further illustrates how α dictates the memory of the gradient estimate. As [27] remarks,
the MOP-0 estimator of [24] only considers single-step quantities, and is “memoryless” beyond a
single step. This is in contrast to the case when α = 1, as that estimate, studied by [28], only
considers the surviving particles at time N and so fully tracks dependencies over time.
Figure 1: Illustration of the bias-variance tradeoff induced by the discounting hyperparameter α,
on the Dhaka cholera model of [18]. We display the MSE of score estimates for the trend in
transmission, evaluated at the MLE.
2.3 Summary of Theoretical Guarantees
The construction of MOP-α bypasses the issue of differentiating through a Monte Carlo algorithm
with discontinuous resampling by turning it into a problem of differentiating through a simulator
6and a series of measurement density ratios. We defer the theoretical analysis of MOP-α to Section
6, but first we highlight a few key results. MOP-α estimates the likelihood and conditional distri-
butions of latent variables, as one expects of a particle filter (Theorem 3). When differentiated,
we obtain the estimators of [28, 27, 24] as special cases (Theorem 1). In particular, MOP-1 is
consistent for the score (Theorem 4), has rates for its bias and variance under different choices of
α (Theorem 5) that illustrate the desirable bias-variance tradeoff observed empirically in Figure 1,
and enjoys a linear rate of convergence for gradient descent with the resulting gradient estimate
(Theorem 2).
3 Practical Maximum-Likelihood Estimation
If θ is evaluated at ϕ, the particles at θ and ϕ coincide. One then only needs to run one particle
filter at θ = ϕ, setting the particles at ϕ to be copies of the particles at θ where gradients don’t
propagate. This is done algorithmically through the stop gradient() function in JAX, providing
a mathematical justification for the use of the stop-gradient operation by [27].
3.1 Optimization
Thisstillleavesuswiththequestionofdesigninganeffectiveprocedureforlikelihoodmaximization
with MOP-α. We propose a simple algorithm we call Iterated Filtering with Automatic Differen-
tiation (IFAD) in Algorithm 2 that runs a few iterations of IF2 to warm-start an iterative (first or
second-order) method that uses the MOP-α gradient estimate. This leverages IF2’s quick empirical
convergence to a neighborhood of the MLE, overcoming the tendency of gradient methods to get
stuck in saddle points and local minima. Conversely, switching to gradient ascent with MOP-α
score estimates lets one bypass the difficulty that IF2 has with optimizing the last few units of
log-likelihood. Combining these two methods in this way lets us enjoy the best of both worlds.
The convergence of IF2 (and so the first stage of IFAD) to a neighborhood of the MLE happens
rapidly in practice. In the case of the Dhaka cholera model of [18], when an aggressive geometric
cooling multiplier of 0.95 and initial random walk standard deviation of 0.02 is used, initial conver-
gence happens within 40 iterations. Finding the MLE itself with IF2, however, takes much longer,
as one has to use a less aggressive cooling rate to do so. For example, [11] use 100 iterations with
the Dhaka cholera model of [18], while [38] use 200 with Model 1 in [39]. By simply requiring that
the first stage of IFAD get to a neighborhood of the MLE and not the MLE itself, we are able to
substantially reduce the number of IF2 iterations required.
3.2 Linear Convergence Rates
On the other hand, the second stage of IFAD enjoys a linear convergence rate under the usual
smoothness and strong convexity assumptions in the theory of convex optimization, as we show
below in Theorem 2.
Theorem 2 (Linear Convergence of IFAD). Consider the second stage of IFAD (Algorithm 2)
where one stops if ∥∇ ℓˆα(θ )∥ ≤ (1+σ)ϵ, where σ ≥ 4Γ , for some β ∈ (0,1). Assume −ℓ is
θ m (1−β)
strongly convex and smooth, γI ⪯ ∇2(−ℓ) ⪯ ΓI. Choose the learning rate η such that η ≤ c(1−β) .
θ 2Γ
Then, for sufficiently large α and J to ensure the score estimate is an ϵ-approximation and the
7minimum eigenvalue of H is greater than some c > 0 for all m with probability at least 1−δ, the
second stage of IFAD converges linearly to the MLE:
ℓ(θ∗)−ℓ(θ ) ≤
(cid:16)
1−ηβ
8γ(cid:17)
(cid:0) ℓ(θ∗)−ℓ(θ )(cid:1) .
m+1 m
9c
We borrow from tools in the field of randomized numerical linear algebra [40] to solve this
problem. The proof, which is similar to that of Theorem 6 in [40], is deferred to Appendix B. For
simplicity, we only prove this for MOP-1 and note that the general case follows as the required
ϵ-approximation can still be obtained for sufficiently large α. When α = 1, the required J is given
in the supplementary information in Lemmas 3 and 4. This result shows that particle estimates for
the gradient can enjoy the same linear convergence rate on well-conditioned problems as gradient
descent with access to the score. As , we note that it is possible.
We therefore see that the second stage of IFAD converges linearly to the MLE if (1) the log-
likelihood surface is γ-strongly convex in a neighborhood of the MLE and (2) the first stage of
IFAD successfully reaches a (high-probability) basin of attraction of the MLE. This happens fairly
often in practice, for example, when sufficient regularity conditions for local asymptotic normality
of the MLE hold. We conjecture that this applies to the entirety of IFAD, as IF2 converges very
quickly to a neighborhood of the MLE, and will explore this in future work.
Algorithm 2 IFAD
Input: Number of particles J, timesteps N, IF2 cooling schedule η , MOP-α discounting param-
m
eter α, θ , m = 0.
0
Run IF2 until initial ”convergence” under cooling schedule η , or for a fixed number or iterations,
m
to obtain {Θ ,j = 1,...,J}, set θ := 1 (cid:80)J Θ .
j m J j=1 j
While procedure not converged:
Run Algorithm 1 to obtain ℓˆ(θ ).
m
Obtain g(θ ) = ∇ (cid:0) −ℓˆ(θ )(cid:1) , H(θ ) s.t. λ (cid:0) H(θ )(cid:1) ≥ c.
m θm m m min m
Update θ := θ −η(H(θ ))−1g(θ ), m := m+1.
m+1 m m m
Return θˆ:= θ .
m
4 Application to a Cholera Transmission Model
The cholera transmission model that [18] developed for Dhaka, Bangladesh, has been used to
benchmark the performance of various POMP inference methods [11, 35, 41], and we employ it
here for the same purpose. This model categorizes individuals in a population as susceptible, S(t),
infected, I(t), and recovered, R(t) and so is called an SIR compartmental model. In this case,
the compartment R(t) is further subdivided into three recovered compartments R1(t), R2(t), R3(t)
denoting varying degrees of cholera immunity. We write P(t) for the total population, and M
n
for the cholera deaths in each month. As in [18, 11], the transition dynamics follow a series of
8Figure 2: A compartment flow diagram for the Dhaka cholera model from [18].
stochastic differential equations:
dS = (cid:0) kϵRk +δ(S −P)−λ(t)S(cid:1) dt+dP −(σSI/H)dB,
(cid:0) (cid:1)
dI = λ(t)S −(m+δ+γ)I dt+(σSI/H)dB,
dR1 = (cid:0) γI −(kϵ+δ)R1(cid:1) dt, ...
dRk = (cid:0) kϵRk−1−(kϵ+δ)Rk(cid:1) dt,
with Brownian motion B(t), cholera death rate m, recovery rate γ, mean immunity duration 1/ϵ,
standard deviation of the force of infection σ, and population death rate δ = 0.02. The force of
infection, λ , is modeled by splines (s )6
t j j=1
   
6 6
(cid:88) I (cid:88)
λ
t
= expβ trend(t−t 0)+ β js j(t) +exp ω js j(t),
P
j=1 j=1
where the coefficients (β )6 model seasonality in the force of infection, β models the trend
j j=1 trend
in the force of infection, and the ω represent seasonality of a non-human environmental reser-
j
voir of disease. The measurement model for observed monthly cholera deaths is given by Y ∼
n
N(M ,τ2M2), where M = γ(cid:82)tn I(s)ds is the modeled number of cholera deaths in that month.
n n n tn−1
4.1 Results
We tested IFAD against IF2 on a global search problem for the Dhaka cholera model. We re-
implemented IF2 to do so, but we compare our results with the results of [11] (labeled ”IF2 2015”).
Our re-implementation ourperforms that of [11], likely due to a better choice of algorithmic param-
eters. For each method, we performed 100 searches, initialized with 100 initial starting parameter
vectorsdrawnuniformlyfromthesamewideboundingboxusedin[11]. Wesummarizeourfindings
below.
IFAD Successfully Finds the MLE: Previously, an MLE at a log-likelihood of −3748.6 was
reported by [37]. This MLE was obtained with much computational effort, using many global and
local IF2 searches, and with the assistance of likelihood profiling. Meanwhile, [11] only achieve a
maximum log-likelihood of −3768.6, while the best log-likelihood found by [18] was only −3793.4.
Despite being initialized for a global search, IFAD manages to get much closer to the MLE over
the 100 searches than [11] and finds it up to 2.5 standard deviations of Monte Carlo error, as seen
in Table 1. On this problem, the sequence of local searches, refinement, and likelihood profiling
that was previously required for finding the MLE is not necessary with the IFAD algorithm. In
9Best Log-Likelihood Rank
IFAD-0.97 -3750.2 1
IFAD-0 -3752.2 2
IFAD-1 -3754.6 3
IF2 Warm Start -3757.3 4
IF2 -3758.2 5
IF2 2015 -3768.6 6
Table 1: Maximum log-likelihood found by IF2, IFAD, and MOP alone. IFAD performs the best
among all methods. Our implementation of IF2 outperforms that of [11], but still ultimately un-
derperforms IFAD. IFAD manages to find the MLE, matching the highest log-likelihood previously
found in the Dhaka cholera model implemented within the pomp package of [37].
other words, the improvement in numerical efficiency of IFAD over IF2 is so large that routine
application of IFAD (consisting of a collection of Monte Carlo replications from random starting
points) generates results outside the reach of a routine application of IF2. When interpreting
Table 1, bear in mind that by Wilks’ Theorem, any difference of over 1.92 log units has statistical
relevance when testing one parameter at the 0.05 significance level, and therefore potentially has
scientific value.
IFAD Outperforms Both IF2 and MOP Alone: While IF2 quickly approaches a neighbor-
hood of the MLE within only 40 iterations, performing IF2 alone ultimately fails to achieve the
last few log-likelihood units, as no IF2 search comes within 7 log-likelihood units of the MLE (as
seen in Figure 3). Conversely, when we tried gradient descent with MOP alone, we encountered
many failed searches. This is a difficult, nonconvex, and noisy problem with 18 parameters, and
the search gets stuck in local minima and saddle points, failing to approach the MLE.
IFAD,incomparison, approachestheMLEquicklyduetotheIF2warm-start(asseeninFigure
5) and also succeeds at refining the coarse solution found by the warm-start with MOP gradient
steps to find the MLE (as seen in Figures 3 and 4). IFAD therefore successfully combines the
best qualities of IFAD and MOP, outperforming either of them alone. Monte Carlo replication is
appropriateforIFAD,oranyMonteCarloalgorithmusedtosolveachallengingnumericalproblem,
but Figure 4 shows that IFAD can find higher likelihood values, more quickly and more reliably
than the previous state-of-the-art. In particular, Figure 4 shows that IFAD with α = 0.97 can
maximize the challenging likelihood of [18] using a modest number of iterations and Monte Carlo
replications, whereas previously it was necessary to carry out an extensive customized search or to
live with an incompletely maximized likelihood.
For the results presented here, we did not include the many common heuristics used in the
machine learning and optimization literature in the gradient descent stage. We used constant
learning rates of 0.01,0.05, and 0.2 for IFAD-1,0, and 0.97 respectively, and a constant cooling rate
of 0.95 for our IF2 implementation. While techniques such as annealing learning rates, gradient
normalization, andmomentumcouldfurtherimprovetheperformanceofIFADinothersimulations
we performed, we chose to report the results of the simplest implementation to serve as a baseline
for the method’s performance. The results for this basic implementation are already sufficient to
show the high potential of the approach.
10Figure3: ScatterplotsdepictingtheperformanceofIFADagainstthatofIF2. Left: Pairedsearches
from the same starting point. Controlling for initial starting point, tuning α allows IFAD-0.97 to
strictly improve on IF2, [28], and [24], on almost every iteration. Right: Q-Q plot of ranked IFAD
searches against ranked IF2 searches. It is clear that on average, IFAD performs best, and manages
to find the MLE while no IF2 search successfully gets within 7 log-likelihood units of it. The dotted
red line shows the true maximized log-likelihood.
5 Application to Bayesian Inference
Particle MCMC, as introduced by [8], is arguably the most popular method for full-information
plug-and-play Bayesian inference for POMP models. The plug-and-play property enables its use
in simulation-based Bayesian inference for complex scientific models where computing transition
densities is not feasible, such as in disease modeling. However, the particle Metropolis-Hastings al-
gorithmoftenexperiencesslowmixingandrequireslongburn-inperiods. Forinstance,[35]reported
needing 700,000 burn-in iterations for effective sampling in a cholera model. This computational
burden necessitated simplifying the scientific model to reduce computational costs, by adjusting
the length of each Euler timestep to be a month instead of a day.
We employ a NUTS sampler powered by a MOP-α, with a nonparametric empirical prior
initialized by the IF2 warm start from the previous section. To construct the prior, we perform a
kernel density estimate (KDE) on the parameter swarm from the warm start, and use the output
of that as an empirical prior. The KDE is performed to ensure some degree of mutual contiguity
betweenthedensities. Thisyieldsapriorthatisroughlyequivalenttoaparametercloudcenteredat
(cid:112)
some point in the neighborhood of the MLE, with a standard deviation of (0.02·0.9540)2·600 ≈
6.3% of the MLE. Using a NUTS sampler powered by MOP-α with this prior significantly lowers
the mixing time of the Markov chain to as little as 500 iterations, as seen in Figure 6.
For completeness, we note that without the empirical Bayes-style prior from IF2, NUTS with
the MOP-α estimate alone tended to diverge. In contrast, particle Metropolis-Hastings (with or
without the empirical Bayes prior from IF2) fails to effectively explore the parameter space. We
display the results of the above in Figures 7 and 8 in Appendix G. The result obtained in this
section, of a speedup in the mixing time of particle MCMC by over three orders of magnitude,
provides hope that particle Bayesian inference might finally be practical for the complex scientific
11Figure 4: Left: Raincloud plot depicting the performance of IFAD and IF2 where we plot the
results of the best run out of every ten runs, representing the common procedure of running a few
searches and choosing the best one. Right: Raincloud plot of all searches. IFAD outperforms
all other methods, and the gradient steps improve on the warm-start given by running 40 IF2
iterations. The dotted red line shows the true maximized log-likelihood.
models commonly encountered in areas like epidemiology.
6 Theoretical Analysis of MOP-α
Here,wewillshowthatMOP-αtargetsthefilteringdistributionandlikelihoodunderθ,isconsistent
when α = 1, and characterize rates for its bias and variance under different choices of α. To do so,
we require the following assumptions:
(A1) Continuity of the Likelihood. ℓ(θ) has more than two continuous derivatives in a neigh-
borhood {θ : ℓ(θ) > λ } for some λ < sup ℓ(φ).
1 1 φ
(A2) Bounded Process Model. There exist M,M¯ such that 0 < M ≤ f (x |x ;θ) ≤
Xn|Xn−1 n n−1
M¯ < ∞.
(A3) Bounded Measurement Model. There exist G,G¯ such that 0 < G ≤ f (y∗ | x ;θ) ≤
Yn|Xn n n
G¯ < ∞ and there exists G′(θ) with ∥∇ logf (y∗ | x ;θ)∥ ≤ G′(θ) < ∞.
θ Yn|Xn n n ∞
(A4) Bounded Gradient Estimates. There are functions G(θ),H(θ) : Θ → [0,∞) uniformly
bounded by G∗,H∗ < ∞, so the MOP-α gradient and Hessian estimates at θ = ϕ are almost
surely bounded by G(θ) and H(θ) for all α.
(A5) Differentiability of Density Ratios and Simulator. The measurement density,
f (y∗|x ;θ), and simulator have more than two continuous derivatives in θ.
Yn|Xn n n
The fact that the likelihood estimate yielded by MOP-α has more than two continuous deriva-
tives in θ follows from the construction of the likelihood estimate in equations 1 and 2, as well as
Assumptions (A3) and (A5).
6.1 MOP-α Targets the Filtering Distribution
We show here that MOP-α targets the filtering distribution under θ and is strongly consistent for
the likelihood under θ when α = 1 or θ = ϕ. Employing α < 1 lead to inconsistency when θ
12Figure 5: Optimization progress of IFAD and IF2. The dashed orange line depicts the median
warm-start given by running 40 IF2 iterations. While running 60 more iterations of IF2 improves
upon the median warm-start, doing so ultimately underperforms IFAD. We see that IFAD has
better tail control and successfully reaches the MLE. We use a dotted red line to display the MLE.
Figure 6: Convergence diagnostics for NUTS with the informative nonparametric empirical Bayes
prior from applying KDE to preliminary searches using IF2, with 4 chains. We display the results
for the trend parameter in the Dhaka cholera model of [18]. The NUTS sampler mixes quickly, and
the posterior estimates from each chain share roughly the same posterior mode.
13deviates from ϕ, but this bias disappears as θ approaches ϕ.
While the result is presented here as specific to MOP-α, we actually prove a more general
result in Appendix D. That is, we show a strong law of large numbers for triangular arrays of
particles with off-parameter resampling, where we resample the particles according to an arbitrary
resampling rule not necessarily in proportion to the targeted distribution of interest and employ
weightsthatencodethecumulativediscrepancybetweentheresamplingandthetargetdistribution
instead of equal weights.
Theorem 3 (MOP-α Targets the Filtering Distribution and Likelihood). When α = 1 or θ = ϕ,
MOP-α targets the filtering distribution under θ and is strongly consistent for the likelihood under
θ. That is, for π (θ) = f (x |y∗ ;θ) and any measurable and bounded functional h and for
n Xn|Y1:n n 1:n
Lˆ(θ) = (cid:81)N LA,θ,α or (cid:81)N LB,θ,α, it holds that
n=1 n n=1 n
(cid:80)J h(xF,θ)wF,θ
j=1 n,j n,j a →.s. E (cid:2) h(X )(cid:3) , Lˆ(θ) a →.s. L(θ).
(cid:80)J wF,θ πn(θ) n
j=1 n,j
Proof. We provide a proof sketch here, deferring most of the details, and discussion of the after-
resampling conditional likelihood estimate LA,θ,α, to Appendix D. When θ = ϕ, regardless of the
n
valueofα,theratiogθ /gϕ = 1,andthisreducestothevanillaparticlefilter. Whenα = 1andθ ̸=
n,j n,j
ϕ, suppose inductively that {(XF,θ ,wF,θ )}J targets f (x |y∗ ;θ). It can then
n−1,j n−1,j j=1 Xn−1|Y1:n−1 n−1 1:n−1
be shown that {(XP,θ,wP,θ)}J targets f (x |y∗ ;θ), that {(XP,θ,wP,θgθ )}J targets
n,j n,j j=1 Xn|Y1:n−1 n 1:n−1 n,j n,j n,j j=1
f (x |y∗ ;θ), and that weighting the particles by (XF,θ,wF,θ) = (XP,θ ,wP,θ gθ (cid:14) gϕ ),
Xn|Y1:n n 1:n n,j n,j n,kj n,kj n,kj n,kj
resampling the k with probabilities proportional to gϕ , also targets f (x |y∗ ;θ). If the
j n,j Xn|Y1:n n 1:n
likelihood is estimated with the before-resampling conditional likelihoods Lˆ(θ) = (cid:81)N LB,θ,α,
n=1 n
the strong consistency is a direct consequence of our earlier result that
{(cid:0) XP,θ,wP,θ(cid:1)
} targets
n,j n,j
f (x |y∗ ;θ).
Xn|Y1:n−1 n 1:n−1
6.2 MOP-1 Is Consistent for the Score
Despite showing that the MOP-1 gradient estimate yields the estimate of [28, 27] when applied to
the bootstrap filter, [28, 27] estimate the Fisher score by
J
1 (cid:88) ∇ logf (cid:16) xA,F,θ,y∗ ;θ(cid:17)
J θ X0:N,Y1:N 0:n,j 1:N
j=1
and not
J
1 (cid:88)
∇ logf
(cid:16)
y∗
|xA,F,θ;θ(cid:17)
.
J θ Y1:N|X1:N 1:N 1:n,j
j=1
It is therefore not immediately apparent that these two converge to the same thing. As such we
directly show the consistency of the MOP-1 gradient estimate below. We present an abbreviated
proof here, postponing the full argument to Appendix E.
Theorem 4 (Consistency of MOP-1 Gradient Estimate). The gradient estimate of MOP-α when
α = 1, θ = ϕ is strongly consistent for the score: ∇ ℓˆ1(θ) a →.s. ∇ ℓ(θ) as J → ∞.
θ J θ
14Proof. Fix ω ∈ Ω, and set ϕ = θ, where θ is the point at which we wish to evaluate the gradient.
The sequence (∇ θLˆ1 J(θ)(ω)) J∈N is uniformly bounded over all J by Assumption (A4). Again by
Assumption (A4), the second derivative of Lˆ1(θ)(ω)| is also uniformly bounded over all J by
J θ=θ′
H∗ for almost every ω ∈ Ω and every θ′ ∈ Θ. So (∇ θLˆ1 J(θ)(ω)) J∈N is uniformly Lipschitz, and
therefore uniformly equicontinuous for almost every ω ∈ Ω.
By Arzela-Ascoli, there is a uniformly convergent subsequence. But there is only one subse-
quential limit, as we can treat the gradient estimate at θ as a bounded functional of the particles
by Assumption (A4), allowing us to apply Theorem 3 to see that the sequence (∇ θLˆ1 J(θ)(ω)) J∈N
converges pointwise for θ = ϕ and almost every ω ∈ Ω. So the whole sequence must converge
uniformly to lim ∇ Lˆ1(θ)(ω).
J→∞ θ J
With uniform convergence for the derivatives established, we can swap the limit and derivative
and obtain, in conjunction with the strong consistency Lˆ1(θ)(ω) a →.s. L(θ) in Theorem 3, that for
J
almosteveryω ∈ Ω,lim ∇ Lˆ1(θ)(ω) = ∇ lim Lˆ1(θ)(ω) = ∇ L(θ).Theresultthenfollows
J→∞ θ J θ J→∞ J θ
by the continuous mapping theorem.
6.3 MOP-α Error, Bias and Variance
WenowprovidearesultshowingthatMOP-αhasadesirablebias-variancetradeoffwhen0 < α < 1.
This is achieved because it combines favorable properties from the low-variance but asymptotically
biased estimate of [24] and the high-variance but consistent estimate of [28]. As the bias itself is
difficult to analyze, we instead analyze the MSE and variance. The below result applies for any
α ∈ [0,1), but not α = 1, as the gradient estimate when α = 1 has no forgetting properties.
Theorem 5. When α ∈ (0,1) and θ = ϕ, define ψ(α) = (αk+αk+1−α)/(1−α). There exists an
ϵ > 0 depending on M¯,M,G¯,G as in [42] such that the MSE and variance of MOP-α are:
E(cid:13) (cid:13)∇ θℓ(θ)−∇ θℓˆα(θ)(cid:13) (cid:13)2
2
≲ minNpG′(θ)2(cid:16) k2J−1+(1−ϵ)⌊k/(clog(J))⌋+k+ψ(α)(cid:17) , (3)
k≤N
Var(cid:0) ∇ ℓˆα(θ)(cid:1) ≲
minNpG′(θ)2(cid:18) k2
+
αk N(cid:19)
. (4)
θ k≤N (1−α)2J 1−α
We defer the proof to Appendix F, but provide a brief outline here. The variance bound can
be reduced to the approximation error between MOP-α and a variant called MOP-(α,k) where
the discounted weights are truncated at lag k. The discount factor, α, ensures strong mixing.
The covariance of the derivative of MOP-(α,k) can be controlled with Davydov’s inequality and a
standard Lp error bound for the particle filter. The MSE bound considers the error between the
score and MOP-(1,k), and the error between MOP-(1,k) and MOP-α. The latter can be shown to
be O˜(cid:0) NpG′(θ)2(k +ψ(α))(cid:1) , and the former can be controlled with a result on the the forgetting
of the particle filter from [42] and the very same Lp error bound mentioned above.
Interpretation: Theorem5providestheoreticalunderstandingoftheresultsobservedinFigure1
that show the empirically favorable bias-variance tradeoff enjoyed by MOP-α. The first term in the
MSEboundinEquation3correspondstotheerroroftheparticleapproximation,thesecondmixing
error,andthethirdandfourththeerrorbetweentheMOP-(1,k)estimateandtheMOP-αestimate.
As α goes to 1, choosing k appropriately, the first, third and fourth term (corresponding to the
variance) increases, while the second term (corresponding to the bias) decreases. Likewise, the first
term in the variance bound in Equation 4 corresponds to the error of the particle approximation,
while the second corresponds to mixing error. As α goes to 1, the variance increases.
15Bias-VarianceTradeoff:
WeshowinAppendixFthatthevarianceofMOP-0isO˜(cid:0) NpG′(θ)2(cid:14) J(cid:1)
.
Previously, [28] established that the variance of MOP-1 is O˜(N4/J), ignoring factors of p and G′.
Combining these results with Theorem 5 shows that MOP-α interpolates between MOP-0 and
MOP-1, as N ≤ Nk2 ≤ N4, with a phase transition as soon as α < 1. The phase transition arises
as even though the particle filter has forgetting properties [42], the resulting derivative estimate of
[28] does not, and we require both for the variance reduction.
In contrast, as Nk2 ≤ N4 and α,k can be as large as desired to balance the impact of the last
three terms, MOP-α achieves a lower MSE than MOP-1 does. We also show that MOP-0 achieves
aMSEofO˜(cid:0) NpG′(θ)2(J−1+(1−ϵ)⌊1/clog(J)⌋)(cid:1)
, wherethesecondtermcorrespondstouncontrolled
mixing error. Unlike MOP-α, MOP-0 has no opportunity to tune α and k to reduce said mixing
error, leading to uncontrolled asymptotic bias.
7 Computational Efficiency
MOP-α and IFAD are fast algorithms, both in theory and practice. In line with the cheap gradient
principle of [43], getting a gradient estimate from MOP-α takes no more than 6 times that of the
runtimeoftheparticlefilter. Inoursimulations, MOP-αtook3.75timestheruntimeoftheparticle
filter. MOP-α and IFAD therefore share the same O(NJ) time complexity as the particle filter,
unlike the O(NJ2) complexity of [26] and Algorithm 2 in [28, 27].
Our implementation of the particle filter, simulator, and MOP-α in JAX [44] enabled us to
take advantage of just-in-time compilation and GPU acceleration, even with a simulator written
in Python. This led to a 16x speedup (379ms vs 6.29s on a Intel i9-13900K CPU and NVIDIA
RTX3090 GPU) over the CPU-only implementation of the particle filter (with a simulator written
in C++) in the pomp package of [37].
8 Discussion
If the simulator is discontinuous in θ, MOP-α no longer applies. We are working on a variation
of MOP-α applicable to this case. Specifically, differentiable transition density ratios can be used
instead of a differentiable simulator. The off-parameter treatment of the measurement model is
extendedtoanoff-parametersimulationforthedynamicprocess. Inthatcase,onerequiresaccessto
the transition densities, or at least their ratios. The discounting parameter, α, can be incorporated
asforMOP-α. ThisalgorithmmaybebettersuitedtolargediscretestatespacesthaneitherMOP-
α or the Baum-Welch algorithm. Additionally, our gradient estimate can be used for variational
inference to approximate the posterior distribution over latent states [24] and parameters.
Discounting the weights by some α ∈ [0,1] is not the only way to interpolate between the
estimators of [24] and [28]. As the proof of Theorem 5 implies, we can also truncate the weights
at a fixed lag, corresponding to the MOP-(1,k) estimate mentioned. The analysis is similar, with
comparable rates but with a slightly less convenient implementation.
Practical likelihood-based data analysis involves many likelihood optimizations [18, 19, 20, 21,
22, 23]. In particular, profile likelihood confidence intervals require a sequence of optimizations.
Additionally, a careful scientist should consider many model variations, to see whether the con-
clusions of the study are sensitive to alternative model specifications. Coding model variations is
relativelysimplewhenusingplug-and-playinferencemethodology. However, assessingthescientific
potential of these variations requires likelihood optimization. If many rounds of laborious searching
16arerequiredtoattainthemaximum(asarisesinpreviousmethods)thathaspracticalconsequences
for the rate at which scientists can evaluate hypotheses. We have demonstrated, for the first time,
a plug-and-play maximum likelihood approach which can directly maximize the likelihood for the
complex benchmark model of [18], requiring some Monte Carlo replication but not prolonged se-
quences of explorations. We anticipate that this will promote scientific advances in all domains
where complex POMP models arise.
Acknowledgments
This research was supported by National Science Foundation grant DMS-1761603. We thank
Nicolas Chopin for helpful communications regarding the strong law of large numbers for off-
parameterresampledparticlefilters. WealsothankArnaudDoucetforhelpfuldiscussionregarding
the manuscript.
References
[1] A. Singh, O. Makhlouf, M. Igl, J. Messias, A. Doucet, and S. Whiteson, “Particle-based score
estimation for state space model learning in autonomous driving,” arXiv, vol. 2212.06968,
2022.
[2] D. He, E. L. Ionides, and A. A. King, “Plug-and-play inference for disease dynamics: Measles
in large and small towns as a case study,” Journal of the Royal Society Interface, vol. 7,
pp. 271–283, 2010.
[3] T. Stocks, T. Britton, and M. H¨ohle, “Model selection and parameter estimation for dynamic
epidemic models via iterated filtering: Application to rotavirus in Germany,” Biostatistics,
vol. 21, pp. 400–416, 09 2018.
[4] J. Knape and P. de Valpine, “Fitting complex population models by combining particle filters
with markov chain monte carlo,” Ecology, vol. 93, no. 2, pp. 256–263, 2012.
[5] J. Kim and D. S. Stoffer, “Fitting stochastic volatility models in the presence of irregular
sampling via particle methods and the EM algorithm,” Journal of Time Series Analysis,
vol. 29, no. 5, pp. 811–833, 2008.
[6] C.Bret´o,“Onidiosyncraticstochasticityoffinancialleverageeffects,”StatisticsandProbability
Letters, vol. 91, pp. 20–26, 2014.
[7] P. Del Moral, Feynman-Kac Formulae: Genealogical and Interacting Particle Systems with
Applications. New York: Springer, 2004.
[8] C. Andrieu, A. Doucet, and R. Holenstein, “Particle Markov chain Monte Carlo methods,”
Journal of the Royal Statistical Society, Series B, vol. 72, pp. 269–342, 2010.
[9] N.Chopin, P.E.Jacob, andO.Papaspiliopoulos, “SMC2: Anefficientalgorithmforsequential
analysisofstatespacemodels,”Journal of the Royal Statistical Society, Series B,vol.75,no.3,
pp. 397–426, 2013.
17[10] E. L. Ionides, C. Bret´o, and A. A. King, “Inference for nonlinear dynamical systems,” Pro-
ceedings of the National Academy of Sciences of the USA, vol. 103, pp. 18438–18443, 2006.
[11] E. L. Ionides, D. Nguyen, Y. Atchad´e, S. Stoev, and A. A. King, “Inference for dynamic
and latent variable models via iterated, perturbed Bayes maps,” Proceedings of the National
Academy of Sciences of the USA, vol. 112, no. 3, pp. 719—-724, 2015.
[12] Y. Pawitan, In all likelihood: statistical modelling and inference using likelihood. Oxford
University Press, 2001.
[13] E. L. Ionides, C. Breto, J. Park, R. A. Smith, and A. A. King, “Monte Carlo profile confidence
intervals for dynamic systems,” Journal of the Royal Society Interface, vol. 14, pp. 1–10, 2017.
[14] N. Ning, E. L. Ionides, and Y. Ritov, “Scalable Monte Carlo inference and rescaled local
asymptotic normality,” Bernoulli, vol. 27, pp. 2532–2555, 2021.
[15] S. N. Wood, “Statistical inference for noisy nonlinear ecological dynamic systems,” Nature,
vol. 466, pp. 1102–1104, 2010.
[16] T. Toni, D. Welch, N. Strelkowa, A. Ipsen, and M. P. Stumpf, “Approximate Bayesian com-
putation scheme for parameter inference and model selection in dynamical systems,” Journal
of the Royal Society Interface, vol. 6, pp. 187–202, 2009.
[17] J. Owen, D. J. Wilkinson, and C. S. Gillespie, “Likelihood free inference for Markov processes:
A comparison,” Statistical Applications in Genetics and Molecular Biology, vol. 14, no. 2,
pp. 189–209, 2015.
[18] A. A. King, E. L. Ionides, M. Pascual, and M. J. Bouma, “Inapparent infections and cholera
dynamics,” Nature, vol. 454, pp. 877–880, 2008.
[19] I. M. Blake, R. Martin, A. Goel, N. Khetsuriani, J. Everts, C. Wolff, S. Wassilak, R. B. Ayl-
ward,andN.C.Grassly,“Theroleofolderchildrenandadultsinwildpoliovirustransmission,”
Proceedings of the National Academy of Sciences of the USA,vol.111,no.29,pp.10604–10609,
2014.
[20] M. Pons-Salort and N. C. Grassly, “Serotype-specific immunity explains the incidence of dis-
eases caused by human enteroviruses,” Science, vol. 361, no. 6404, pp. 800–803, 2018.
[21] R. Subramanian, Q. He, and M. Pascual, “Quantifying asymptomatic infection and transmis-
sion of COVID-19 in New York City using observed cases, serology, and testing capacity,”
Proceedings of the National Academy of Sciences of the USA, vol. 118, no. 9, 2021.
[22] S. J. Fox, M. Lachmann, M. Tec, R. Pasco, S. Woody, Z. Du, X. Wang, T. A. Ingle, E. Javan,
M.Dahan,K.Gaither, M.E.Escott, S.I.Adler, S.C.Johnston,J.G.Scott, andL.A.Meyers,
“Real-time pandemic surveillance using hospital admissions and mobility data,” Proceedings
of the National Academy of Sciences of the USA, vol. 119, no. 7, p. e2111870119, 2022.
[23] J.M.Drake, A.Handel, E´.Marty, E.B.O’Dea, T.O’Sullivan, G.Righi, andA.T.Tredennick,
“A data-driven semi-parametric model of SARS-CoV-2 transmission in the United States,”
PLOS Computational Biology, vol. 19, no. 11, p. e1011610, 2023.
18[24] C. Naesseth, S. Linderman, R. Ranganath, and D. Blei, “Variational sequential Monte Carlo,”
in Proceedings of the Twenty-First International Conference on Artificial Intelligence and
Statistics (A. Storkey and F. Perez-Cruz, eds.), vol. 84 of Proceedings of Machine Learning
Research, pp. 968–977, PMLR, 09–11 Apr 2018.
[25] R.Jonschkowski,D.Rastogi,andO.Brock,“Differentiableparticlefilters: End-to-endlearning
with algorithmic priors,” arXiv, vol. 1805.11122, 2018.
[26] A.Corenflos,J.Thornton,G.Deligiannidis,andA.Doucet,“Differentiableparticlefilteringvia
entropy-regularizedoptimaltransport,” inProceedings of the 38th International Conference on
Machine Learning (M. Meila and T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning
Research, pp. 2100–2111, PMLR, 18–24 Jul 2021.
[27] A.S´cibiorandF.Wood, “Differentiableparticlefilteringwithoutmodifyingtheforwardpass,”
arXiv, vol. 2106.10314, 2021.
[28] G.Poyiadjis,A.Doucet,andS.S.Singh,“Particleapproximationsofthescoreandobservedin-
formationmatrixinstatespacemodelswithapplicationtoparameterestimation,”Biometrika,
vol. 98, no. 1, pp. 65–80, 2011.
[29] X. Chen and Y. Li, “Normalising flow-based differentiable particle filters,” arXiv,
vol. 2403.01499, 2024.
[30] A. Svensson, F. Lindsten, and T. B. Sch¨on, “Learning nonlinear state-space models using
smooth particle-filter-based likelihood approximations,” IFAC–PapersOnLine, vol. 51, no. 15,
pp. 652–657, 2018.
[31] S. Malik and M. K. Pitt, “Particle filters for continuous likelihood evaluation and maximisa-
tion,” Journal of Econometrics, vol. 165, no. 2, pp. 190–209, 2011.
[32] E.L.Ionides,A.Bhadra,Y.Atchad´e,andA.A.King,“Iteratedfiltering,”Annals of Statistics,
vol. 39, pp. 1776–1802, 2011.
[33] J.LiuandM.West,“Combiningparameterandstateestimationinsimulation-basedfiltering,”
inSequentialMonte CarloMethods in Practice(A.Doucet,N.deFreitas,andN.Gordon,eds.),
pp. 197–224, New York: Springer, 2001.
[34] M. D. Homan and A. Gelman, “The No-U-turn sampler: Adaptively setting path lengths in
Hamiltonian Monte Carlo,” J. Mach. Learn. Res., vol. 15, p. 1593–1623, jan 2014.
[35] M.Fasiolo, N.Pya, andS.N.Wood, “Acomparisonofinferentialmethodsforhighlynonlinear
state space models in ecology and epidemiology,” Statistical Science, vol. 31, pp. 96–118, 02
2016.
[36] M. S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, “A tutorial on particle filters for
online nonlinear, non-Gaussian Bayesian tracking,” IEEE Transactions on Signal Processing,
vol. 50, pp. 174 – 188, 2002.
[37] A. A. King, D. Nguyen, and E. L. Ionides, “Statistical inference for partially observed Markov
processes via the R package pomp,” Journal of Statistical Software, vol. 69, pp. 1–43, 2016.
19[38] J. Wheeler, A. Rosengart, J. Zhuxun, K. H. E. Tan, N. Treutle, and E. L. Ionides, “Inform-
ing policy via dynamic models: Cholera in Haiti,” PLOS Computational Biology, vol. 20,
p. e1012032, 2024.
[39] E. C. Lee, D. L. Chao, J. C. Lemaitre, L. Matrajt, D. Pasetto, J. Perez-Saez, F. Finger,
A. Rinaldo, J. D. Sugimoto, M. E. Halloran, I. M. Longini, R. Ternier, K. Vissieres, A. S.
Azman, J. Lessler, and L. C. Ivers, “Achieving coordinated national immunity and cholera
eliminationinHaitithroughvaccination: Amodellingstudy,”The Lancet Global Health,vol.8,
no. 8, pp. e1081–e1089, 2020.
[40] F. Roosta-Khorasani and M. W. Mahoney, “Sub-sampled Newton methods I: Globally conver-
gent algorithms,” arXiv, vol. 1601.04737, 2016.
[41] N. Wycoff, J. W. Smith, A. S. Booth, and R. B. Gramacy, “Voronoi candidates for Bayesian
optimization,” 2024.
[42] J. Karjalainen, A. Lee, S. S. Singh, and M. Vihola, “On the forgetting of particle filters,”
arXiv, vol. 2309.08517, 2023.
[43] S. Kakade and J. D. Lee, “Provably correct automatic subdifferentiation for qualified pro-
grams,” arXiv, vol. 1809.08530, 2019.
[44] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula,
A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang, “JAX: composable transfor-
mations of Python+NumPy programs,” 2018.
[45] P. Del Moral and A. Doucet, “On a class of genealogical and interacting Metropolis models,”
in S´eminaire de Probabilit´es XXXVII (J. Az´ema, M. E´mery, M. Ledoux, and M. Yor, eds.),
pp. 415–446, Berlin: Springer, 2003.
[46] P. Del Moral and E. Rio, “Concentration inequalities for mean field particle models,” The
Annals of Applied Probability, vol. 21, jun 2011.
[47] N. Chopin and O. Papaspiliopoulos, An Introduction to Sequential Monte Carlo. Springer,
2020.
[48] N. Chopin, “Central limit theorem for sequential Monte Carlo methods and its application to
Bayesian inference,” Annals of Statistics, vol. 32, pp. 2385–2411, 2004.
A MOP-α Functional Forms
Theorem 1 follows immediately as a consequence of the following results, Lemmas 1 and 2.
Lemma 1. Write ∇ ℓˆα(θ) for the gradient estimate yielded by MOP-α when θ = ϕ. Consider the
θ
case where we use the after-resampling conditional likelihood estimate so that Lˆ(θ) = (cid:81)N LA,θ,α.
n=1 n
When α = 1,
J
∇ ℓˆ1(θ) =
1 (cid:88)
∇ logf
(cid:16)
y∗
|xA,F,θ(cid:17)
, (5)
θ J θ Y1:N|X1:N 1:N 1:n,j
j=1
yielding the estimator of [28, 27] with the bootstrap filter.
20Proof. Consider the case of MOP-α when α = 1 and θ = ϕ. We then have a nice telescoping
product property for the after-resampling likelihood estimate:
 
Lˆ1(θ) :=
(cid:89)N
LA n,θ,α =
(cid:89)N
Lϕ n·
(cid:80)(cid:80) JJ
j=1
ww
n
PF ,, ,jθ
θ
=
(cid:89)N
Lϕ n·
(cid:80)(cid:80) JJ
j=1
ww FnF ,θ,, jθ
= 
J1 (cid:88)J
w NF, ,θ
j(cid:89)N
Lϕ n, (6)
n=1 n=1 j=1 n,j n=1 j=1 n−1,j j=1 n=1
where the third equality follows from the choice of α = 1, and the fourth equality is the resulting
telescoping property. The log-derivative identity lets us decompose the score estimate as
(cid:16) (cid:17)
∇ ℓˆ1(θ) = ∇ θLˆ1(θ) = ∇ θ J1 (cid:80)J j=1w NF, ,θ j (cid:81)N n=1Lϕ n = 1 (cid:88)J ∇ wF,θ. (7)
θ Lˆ1(θ) (cid:81)N Lϕ J θ N,j
n=1 n j=1
From (7), we see that the derivative of the log-likelihood estimate is
J
∇ ℓˆ1(θ) := 1 (cid:88) ∇ wF,θ. (8)
θ J θ N,j
j=1
We proceed to decompose (8). First, observe that as α = 1,
wP,θ = wF,θ
g nθ
,j =
(cid:89)n g iA ,j,P,θ
, (9)
n,j n−1,j gϕ gA,P,ϕ
n,j i=1 i,j
whereweusethe(·)A superscripttodenotetheancestraltrajectoryofthej-thpredictionorfiltering
particle at timestep n. Note that this quantity is the cumulative product of measurement density
ratios over the ancestral trajectory of the j-th prediction particle at timestep n. We then use the
log-derivative identity again, yielding the following expression for the gradient of the log-weights
as the sum of the log measurement densities over the ancestral trajectory:
∇ wP,θ (cid:32) n gA,P,θ(cid:33)
θ n,j = ∇ logwP,θ = ∇ log (cid:89) i,j (10)
wP,θ θ n,j θ gA,P,ϕ
n,j i=1 i,j
n
= ∇
(cid:88)(cid:16)
loggA,P,θ
−loggA,P,ϕ(cid:17)
(11)
θ i,j i,j
i=1
n
= (cid:88) ∇ loggA,P,θ. (12)
θ i,j
i=1
This is equal to the gradient of the logarithm of the conditional density of the observed measure-
ments given the ancestral trajectory of the j-th prediction particle up to timestep n:
N (cid:32) N (cid:33)
∇ (cid:88) loggA,θ = ∇ log (cid:89) gA,P,θ (13)
θ n,j θ n,j
n=1 n=1
(cid:32) N (cid:33)
= ∇ log
(cid:89)
f
(cid:16) y∗|xA,P,θ(cid:17)
(14)
θ Yn|Xn n n,j
n=1
(cid:16) (cid:17)
= ∇ logf y∗ |xA,P,θ . (15)
θ Y1:N|X1:N 1:N 1:n,j
21MultiplyingbothsidesoftheexpressionbywP,θ yieldsanexpressionforthegradientoftheweights
N,j
at timestep N:
N
∇ wP,θ = wP,θ (cid:88) ∇ loggA,P,θ = wP,θ∇ logf (cid:16) y∗ |xA,P,θ(cid:17) . (16)
θ N,j N,j θ n,j N,j θ Y1:N|X1:N 1:N 1:n,j
n=1
Substituting the above identity into the log-likelihood decomposition obtained earlier in Equation
7 yields
J J J
∇ ℓˆ1(θ) := 1 (cid:88) ∇ wF,θ = 1 (cid:88) ∇ wP,θ = 1 (cid:88) wP,θ ∇ logf (cid:16) y∗ |xA,P,θ(cid:17) . (17)
θ J θ N,j J θ N,kj J N,kj θ Y1:N|X1:N 1:N 1:n,kj
j=1 j=1 j=1
Finally, observing that θ = ϕ implies wF,θ = 1, we obtain
N,j
J
∇ ℓˆ1(θ) :=
1 (cid:88)
∇ logf
(cid:16)
y∗
|xA,F,θ(cid:17)
. (18)
θ J θ Y1:N|X1:N 1:N 1:n,j
j=1
This yields the gradient estimators of [28, 27] when applied to the bootstrap filter.
Note that the variance of the MOP-α log-likelihood estimate scales poorly with N the moment
θ ̸= ϕ. This can be seen by observing that
J J J
∇ ℓˆ1(θ) := 1 (cid:88) ∇ wF,θ = 1 (cid:88) ∇ wP,θ = 1 (cid:88) wP,θ ∇ logf (cid:16) y∗ |xA,P,θ(cid:17) . (19)
θ J θ N,j J θ N,kj J N,kj θ Y1:N|X1:N 1:N 1:n,kj
j=1 j=1 j=1
When θ ̸= ϕ, we see that wP,θ = O(cN). When θ = ϕ, this is a special case of the [28] estimator,
N,kj
which has O(N4) variance by a property of functionals of the particle filter [45].
Lemma 2. Write ∇ ℓˆα(θ) for the gradient estimate yielded by MOP-α when θ = ϕ. Consider the
θ
case where we use the after-resampling conditional likelihood estimate so that Lˆ(θ) = (cid:81)N LA,θ,α.
n=1 n
When α = 0,
N J
∇ ℓˆ0(θ) =
1 (cid:88)(cid:88)
∇
log(cid:16)
f
(y∗|xF,θ;θ)(cid:17)
, (20)
θ J θ Yn|Xn n n,j
n=1j=1
yielding the estimate of [24] when applied to the bootstrap filter.
Proof. First, write
f (y∗|xP,θ)
s =
Yn|Xn n n,j
n,j f (y∗|xP,ϕ)
Yn|Xn n n,j
as shorthand for the measurement density ratios. Observe that, when α = 0,, the likelihood
22estimate becomes
N N (cid:80)J wF,θ
Lˆ0(θ) := (cid:89) LA,θ,α = (cid:89) Lϕ · j=1 n,j (21)
n n (cid:80)J wP,θ
n=1 n=1 j=1 n,j
N J
(cid:89) 1 (cid:88)
= Lϕ · s (22)
n J n,j
n=1 j=1
=
(cid:89)N
Lϕ ·
1 (cid:88)J f Yn|Xn(y n∗|xP n,, jθ)
. (23)
n J f (y∗|xP,ϕ)
n=1 j=1 Yn|Xn n n,j
We lose the nice telescoping property observed in the MOP-1 case, but this expression still yields
something useful. This is because its gradient when θ = ϕ is therefore
 
N J
(cid:88) 1 (cid:88)
∇ θℓˆ0(θ) := ∇ θlogLϕ nJ s n,j (24)
n=1 j=1
(cid:16) (cid:17)
N ∇ Lϕ1 (cid:80)J s
(cid:88) θ nJ j=1 n,j
= (25)
(cid:16) (cid:17)
Lϕ1 (cid:80)J s
n=1 nJ j=1 n,j
N (cid:80)J ∇ s
(cid:88) j=1 θ n,j
= (26)
(cid:80)J
s
n=1 j=1 n,j
=
(cid:88)N 1 (cid:88)J ∇ θf Yn|Xn(y n∗|xF n,, jθ;θ)
(27)
J f (y∗|xF,ϕ;ϕ)
n=1 j=1 Yn|Xn n n,j
N J
=
1 (cid:88)(cid:88)
∇
log(cid:16)
f
(y∗|xF,θ;θ)(cid:17)
, (28)
J θ Yn|Xn n n,j
n=1j=1
whereweusethelog-derivativetrickinthesecondequality,
observethat(cid:80)J
s = J whenθ = ϕ
j=1 n,j
in the fourth equality, and use the log-derivative trick again while noting that θ = ϕ in the fifth
equality. This yields the desired result.
B Optimization Convergence Analysis
The analysis in this section roughly follows the analysis in [40], except with the caveat that none
of the matrix concentration bounds they use apply here as the particles are dependent. We instead
use the concentration inequality from [46] to bound the gradient and Hessian estimates. In this
section, we fix ω ∈ Ω only within each filtering iteration, evaluate Algorithm 1 at θ = ϕ, and
analyze Algorithm 2 post-iterated filtering.
The convergence analysis in Theorem 2 is limited to the case where −ℓ is γ-strongly convex.
Though it is true that in a neighborhood of the optimum local asymptotic normality holds and the
log-likelihoodisstronglyconvexinthisneighborhood,inpracticelikelihoodsurfacesforPOMPsare
often highly nonconvex globally. The convergence to an optimum, local or global, must therefore
be sensitive to initialization.
23B.1 Bounding the Gradient
Lemma 3 (Concentration of Measure for Gradient Estimate). Consider the gradient estimate
obtained by MOP-1, which we know by Theorem 4 is consistent for the score, where θ = ϕ. For
∥∇ ℓˆ1(θ)−∇ ℓ(θ)∥ to be bounded by ϵ with probability 1−δ, we require
θ θ 2
√
(cid:26) (cid:18) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19) (cid:27)
r p 2p plog(2p/δ)
J > max 2G(θ) N 1+h−1 log ,8G(θ)2β2 , (29)
ϵ δ N ϵ2
where NG′(θ) ≤ G(θ) are defined in Assumptions (A3) and (A4), h(t) = 1(t−log(1+t)), and β
2 N
and r are two additional finite model-specific constants that do not depend on J, but do depend
N
on N and p, as defined in [46]. Equivalently, with probability at least 1−δ, it holds that
(cid:32) √ (cid:114) (cid:33)
r p 2plog(2p/δ)
∥∇ ℓˆ1(θ)−∇ ℓ(θ)∥ ≤ G(θ) N (1+h−1(log(2p/δ)))+ β . (30)
θ θ 2 N
J J
Remark: According to [46], under some regularity conditions, r and β are linear in the tra-
N N
jectory length N. This corresponds to the finding by [28] that the variance of the estimate is at
least quadratic in the trajectory length, and their remark that the result of [45] establishes that
the L error is bounded by O(N2J−1/2) (equivalently, the variance is bounded by O(N4J−1)) af-
p
ter accounting for the sum over timesteps. The MOP-1 variance upper bound is therefore in fact
O(N4), in contrast to the MOP-α, where α < 1, upper bound of O(N).
Proof. Wewillseektousetheconcentrationinequalityof[46]toboundthedeviationofthegradient
estimate from the gradient of the negative log-likelihood in the sup norm with a union bound. Fix
θ = ϕ. From the decomposition in the proof of Lemma 1, as wF,θ = 1 when θ = ϕ, we have that
N,j
J J N J N
∇ ℓˆ(θ) := 1 (cid:88) ∇ wF,θ = 1 (cid:88)(cid:88) wP,θ ∇ loggA,θ = 1 (cid:88)(cid:88) ∇ loggA,θ . (31)
θ J θ N,j J N,kj θ n,kj J θ n,kj
j=1 j=1n=1 j=1n=1
Define φi(xF,θ) := ∂ loggA,θ , which is a functional of the filtering particles xF,θ = xP,θ . These
n n,j ∂θi n,kj n,j n,kj
are bounded measurable functionals bounded by G′(θ) by Assumption (A3). Therefore, these have
(cid:16) (cid:17)
bounded oscillation, satisfying the requirement that osc ∂ φ (xP,θ) ≤ G′(θ). Note that [46] in
∂θi i n,j
fact assume osc(f) ≤ 1, so we simply scale their bound accordingly.
Now we apply the Hoeffding-type concentration inequality from Del Moral and Rio [46] and a
union bound over each φi(xF,θ), totaling N timesteps and p parameters, to find that
n n,j
(cid:13) (cid:13)
(cid:13) J (cid:13) (cid:32) (cid:114) (cid:33)
n=m 1,a ..x .,N(cid:13) (cid:13) (cid:13)J1 (cid:88) ∇ θlogg nA ,, kθ
j
−∇ θℓ n(θ)(cid:13) (cid:13)
(cid:13)
≤ G′(θ) r JN (1+h−1(t))+ 2 Jt β N (32)
(cid:13) j=1 (cid:13)
∞
with probability at least 1−2Npexp(−t). Although the above concentration inequality only con-
siders the error from the expectation under the filtering distribution, we invoke the consistency of
MOP-1 shown in Theorem 4 to establish that the expectation under the filtering distribution is in
24fact the score. It therefore holds that with the same probability, that when summing over N, as
NG′(θ) ≤ G(θ),
(cid:13) (cid:13)
(cid:13) J N (cid:13) (cid:32) (cid:114) (cid:33)
(cid:13) (cid:13) (cid:13)J1 (cid:88)(cid:88) ∇ θlogg nA ,, kθ
j
−∇ θℓ(θ)(cid:13) (cid:13)
(cid:13)
≤ G′(θ)N r JN (1+h−1(t))+ 2 Jt β N (33)
(cid:13) j=1n=1 (cid:13)
∞
(cid:32) (cid:114) (cid:33)
r 2t
≤ G(θ) N (1+h−1(t))+ β . (34)
N
J J
We split the δ failure probability among these 2Np terms, to find δ ≤ 2Npexp(−t), and therefore,
t ≤ log(2Np/δ), where h(t) = 1(t−log(1+t)). The two additional model-specific parameters are
2
β and r , which do not depend on J. The analogous bound for the 2-norm follows from scaling
t t √
the right-hand side by p, to require
(cid:32) √ (cid:114) (cid:33)
r p 2plog(2p/δ)
∥∇ ℓˆ(θ)−∇ ℓ(θ)∥ ≤ G(θ) N (1+h−1(log(2p/δ)))+ β . (35)
θ θ 2 N
J J
We therefore need
√
(cid:26) (cid:18) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19) (cid:27)
r p 2p plog(2p/δ)
J > max 2G(θ) N 1+h−1 log ,8G(θ)2β2 . (36)
ϵ δ N ϵ2
B.2 Bounding Hessian Estimates
Should one choose to use a second-order method involving a particle Hessian estimate, we provide
a guarantee for its positive-definiteness below.
Lemma 4 (Minimum Eigenvalue Bound for Hessian Estimate). Assume that the Hessian of the
negative log-likelihood H = (cid:80)J EH has a minimum eigenvalue 0 < γ < 1, and that Eλ (H ) =
j=1 j min j
γ′ > 0. If
(cid:26) 2r (1+h−1(t))+2c 2(2tβ2+c)2(cid:27) r (1+h−1(t)) √
J > max t , t ≥ t + 2tJβ /γ′+c/γ′ (37)
γ′ γ′2 γ′ t
then Hˆ(θ) is invertible and positive definite with minimum eigenvalue greater than or equal to
c ∈ (0,(cid:80)J Eλ (H )), with probability at least 1−exp(−t).
j=1 min j
Proof. Write Hˆ(θ) = Hˆ = (cid:80)J H for the estimate of the negative of the Hessian, where H is an
j=1 j j
element of the outer sum over the J particles.
Asthenegativelog-likelihoodisconvex,wewanttoboundtheminimumeigenvalueofHˆ(θ)from
below with high probability, so that all the eigenvalues of Hˆ(θ) are positive with high probability.
This ensures that the estimated Hessian is invertible and positive-definite.
Itisknownthattheminimumeigenvalueofasymmetricmatrixisconcave. Therefore,itsuffices
to show that the first inequality in the below expression
 
J J
(cid:88) (cid:88)
0 < λ min(H j) ≤ λ min H j = λ min(Hˆ) (38)
j=1 j=1
25holds with high probability. We apply the particle Hoeffding concentration inequality from [46] to
find that
J J (cid:114)
1 (cid:88) λ (H )−E λ (H ) = 1 (cid:88) λ (H )−γ′ ≥ −r t(cid:0) 1+h−1(t)(cid:1) − 2t β (39)
J
min j π˜t min j
J
min j
J J
t
j=1 j=1
J √
(cid:88) λ (H ) ≥ −r (cid:0) 1+h−1(t)(cid:1) − 2tJβ +Jγ′, (40)
min j t t
j=1
with probability at least 1−exp(−t). Here, h(t) = 1(t−log(1+t)). The two additional model-
2
specific parameters are β and r , which do not depend on J.
t t
We additionally require, for c ∈ (cid:0) 0,(cid:80)J Eλ (H )(cid:1) ,
j=1 min j
J √
(cid:88) λ (H ) ≥ −r (cid:0) 1+h−1(t)(cid:1) − 2tJβ +Jγ′ ≥ c, (41)
min j t t
j=1
√
Jγ′ ≥ c+r (cid:0) 1+h−1(t)(cid:1) + 2tJβ . (42)
t t
It is therefore sufficient to have
(cid:40) 2r (cid:0) 1+h−1(t)(cid:1) +2c 2(2tβ2+c)2(cid:41) r (cid:0) 1+h−1(t)(cid:1) √
J > max t , t ≥ t + 2tJβ /γ′+c/γ′ (43)
γ′ γ′2 γ′ t
for Hˆ(θ) to be invertible and positive definite with minimum eigenvalue greater than or equal to c
with probability at least 1−exp(−t).
B.3 Convergence Analysis of Theorem 2
Proof. In this analysis, we largely follow the proof of Theorem 6 in [40]. Define θ = θ +ηp ,
η m m
where p = −(H(θ ))−1g(θ ). As in Roosta-Khorasani and Mahoney [40], we want to show there
m m m
is some iteration-independent η˜> 0 such that the Armijo condition
f(θ +ηp ) ≤ f(θ )+ηβpT g(θ ), (44)
m m m m m
holds for any 0 < η < η˜ and some β ∈ (0,1). By an argument found in the beginning of the
proof of Theorem 6 in [40], we have that choosing J such that ∥∇ ℓˆ(θ ) − ∇ ℓ(θ )∥ ≤ ϵ and
θ m θ m
λ (H(θ )) ≥ c > 0 for each m, yields
min m
f(θ )−f(θ ) ≤ ηpT g(θ )+ϵη∥p ∥+η2Γ∥p ∥2/2, (45)
η m m m m m
with probability 1−δ/2. From now on, we assume that we are on the success event of this high-
probability statement. Consequently, we have
pT g(θ ) = −pT H(θ )p ≥ −c∥p ∥2, (46)
m m m m m m
and we can obtain a decrease in the objective. Substituting this into the previous expression,
f(θ )−f(θ ) ≤ −ηpT H(θ )p +ϵη∥p ∥+η2Γ∥p ∥2/2, (47)
η m m m m m m
26the Armijo condition becomes
−ηpT H(θ )p +ϵη∥p ∥+η2Γ∥p ∥2/2 ≤ ηβpT g(θ ) = −ηβpT H(θ )p (48)
m m m m m m m m m m
ϵ∥p ∥+ηΓ∥p ∥2/2 ≤ (1−β)pT H(θ )p (49)
m m m m m
ϵ+ηΓ∥p ∥/2 ≤ c(1−β)∥p ∥. (50)
m m
This holds and guarantees an iteration-independent lower bound if
c(1−β) c(1−β) c(1−β)
η ≤ , ϵ ≤ ∥g(θ )∥ ≤ ∥p ∥, (51)
m m
Γ 2Γ 2
which is given by our choice of η. Now, first note that
∥g(θ )∥−∥∇ f(θ )∥ ≤ ∥g(θ )−∇ f(θ )∥ ≤ ϵ =⇒ ∥∇ f(θ )∥ ≥ ∥g(θ )∥−ϵ (52)
m θ m m θ m θ m m
and
∥∇ f(θ )∥−∥g(θ )∥ ≤ ∥∇ f(θ )−g(θ )∥ ≤ ϵ =⇒ ∥g(θ )∥ ≥ ∥∇ f(θ )∥−ϵ. (53)
θ m m θ m m m θ m
There are now two cases. If the algorithm terminates and ∥g(θ )∥ ≤ σϵ, we can derive
m
∥∇ f(θ )∥ ≤ ∥g(θ )∥+ϵ = σϵ+ϵ = (σ+1)ϵ. (54)
θ m m
If the algorithm does not terminate, then ∥g(θ )∥ > σϵ. Notice that
m
ϵ ≥ ∥g(θ )−∇ f(θ )∥ ≥ ∥g(θ )∥−∥∇ f(θ )∥ (55)
m θ m m θ m
∥∇ f(θ )∥+ϵ ≥ ∥g(θ )∥ ≥ σϵ (56)
θ m m
∥∇ f(θ )∥ ≥ σϵ−ϵ = (σ−1)ϵ (57)
θ m
∥∇ f(θ )∥
θ m
≥ ϵ, (58)
σ−1
and now
∥∇ f(θ )∥
θ m
∥∇ f(θ )∥−ϵ ≥ ∥∇ f(θ )∥− (59)
θ m θ m
σ−1
(cid:18) (cid:19)
1
= 1− ∥∇ f(θ ∥ (60)
θ m
σ−1
σ−2
= ∥∇ f(θ )∥ (61)
θ m
σ−1
2
≥ ∥∇ f(θ )∥. (62)
θ m
3
Since ∥A−1∥ = 1/σ (A),
min
pT H(θ )p = (cid:0) −(H(θ ))−1g(θ )(cid:1)T H(θ )(cid:0) −(H(θ ))−1g(θ )(cid:1) (63)
m m m m m m m m
= g(θ )T(H(θ ))−1g(θ ) (64)
m m m
1
≥ ∥g(θ )∥2 (65)
m
c
1(cid:0) (cid:1)2
≥ ∥∇ f(θ )∥−ϵ (66)
θ m
c
4
≥ ∥∇ f(θ )∥2. (67)
θ m
9c
27From the assumption that f is γ-strongly convex, γI ⪯ ∇2−ℓ ⪯ ΓI, by an implication of γ-strong
θ
convexity we have
f(θ m)−f(θ∗) ≤ 1 (cid:13) (cid:13)∇ θf(θ m)(cid:13) (cid:13)2 , (68)
2γ
and we put together:
f(θ m)−f(θ∗) ≤ 21 γ(cid:13) (cid:13)∇ θf(θ m)(cid:13) (cid:13)2 ≤ 9 4c 21
γ
pT mH(θ m)p m, (69)
8 9γ c(cid:0) f(θ m)−f(θ∗)(cid:1) ≤ 94
c
(cid:13) (cid:13)∇ θf(θ m)(cid:13) (cid:13)2 ≤ pT mH(θ m)p m, (70)
9c
f(θ )−f(θ∗) ≤ pT H(θ )p , (71)
m 8γ m m m
−8 9γ c(cid:0) f(θ m)−f(θ∗)(cid:1) ≥ − 94
c
(cid:13) (cid:13)∇ θf(θ m)(cid:13) (cid:13)2 ≥ −pT mH(θ m)p m. (72)
From earlier, as the Armijo condition is fulfilled with our choice of η and ϵ,
f(θ )−f(θ ) ≤ −ηpT H(θ )p +ϵη∥p ∥+η2Γ∥p ∥2/2 (73)
m+1 m m m m m m
≤ −ηβpT H(θ )p (74)
m m m
≤ −ηβ
8γ (cid:0)
f(θ
)−f(θ∗)(cid:1)
. (75)
m
9c
Therefore,
f(θ )−f(θ∗) = f(θ )−f(θ )+f(θ )−f(θ∗) (76)
m+1 m+1 m m
≤ f(θ )−f(θ∗)−ηβ 8γ (cid:0) f(θ )−f(θ∗)(cid:1) (77)
m m
9c
=
(cid:16) 1−ηβ8γ(cid:17)
(cid:0)
f(θ
)−f(θ∗)(cid:1)
. (78)
m
9c
C Feynman-Kac Models and Monte Carlo Approximations
Inthissection,weintroducetheFeynman-Kacconventionof[7]thathassincebecomecommonplace
[42] for the analysis of the particle filter. The mathematical formalization and notation introduced
here will be adopted in the remainder of the analysis, in order to prove Theorems 3, 4, and 5.
Let (η )N ,(π )N ,(ρ )N be sequences of probability measures on the state space X. This is
n n=1 n n=1 n n=1
the sequence of prediction distributions f , filtering distributions f , and posterior
Xn|Y1:n−1 Xn|Y1:n
distributions f that we seek to approximate with the particle filter. For any measurable
X1:n|Y1:n
boundedfunctionalh,weadoptthefollowingfunctional-analyticnotation,borrowedfrom[7,47,42].
We choose our specific choice of notation and definitions to be in line with that of [42].
28Markov kernels and the process model: A Markov kernel M with source X and target X is
1 2
a map M : X ×B(X ) → [0,1] such that for every set A ∈ B(X ) and every point x ∈ X , the map
1 2 2 1
x (cid:55)→ M(x,A) is a measurable function of x, and the map A (cid:55)→ M(x,A) is a probability measure on
X . The quantity M(x,A) can be thought of as the probability of transitioning to the set A given
2
that we are at the point x. If this yields a density, this then corresponds to the process density
f conditional on x and integrated over A.
X2|X1
Markov kernels and measures: For any measure η, any Markov kernel M on X, any point
x ∈ X and any measurable subset A ⊆ X, let
(cid:90) (cid:90)
η(h) = hdη = h(x)η(dx), (79)
(cid:90)
(ηM)(A) = η(dx)M(x,A), (80)
(cid:90)
(Mh)(x) = M(x,dy)h(y). (81)
Compositions of Markov kernels: The composition of a Markov kernel M with another
1
Markov kernel M is another Markov kernel, given by
2
(cid:90)
(M M )(x,A) = M (x,dy)M (y,A). (82)
1 2 1 2
Total variation distance: The total variation distance between two measures µ and ν on X is
∥µ−ν∥ = sup |µ(ϕ)−ν(ϕ)| = sup |µ(h)−ν(h)|. (83)
TV
∥h∥∞≤1/2 osc(h)≤1
Dobrushin contraction: The Dobrushin contraction coefficient β of a Markov kernel M is
TV
given by
(cid:13) (cid:13) ∥µM −νM∥ TV
β TV(M) = sup (cid:13)M(x,·)−M(y,·)(cid:13)
TV
= sup
∥µ−ν∥
. (84)
x,y∈X µ,ν∈P,µ̸=ν TV
Potential functions and the measurement model: A potential function G : X → [0,∞) is a
non-negative function of an element of the state space x ∈ X. In our case, this corresponds to the
measurement model, and in our previous notation is written as g = f (y∗|xF ) = G (xF ),
n,j Yn|Xn n n,j n n,j
where in a slight abuse of notation we suppress the dependence on θ for notational simplicity. Note
that G (·) = f (y∗| · ) is the conditional density of the observed measurement at time n, where
n Yn|Xn n
we condition on the filtering particle xF as an element of the state space.
n,j
Feynman-Kac models: A Feynman-Kac model on X is a tuple (π ,(M )N ,(G )N ) of an
0 n n=1 n n=1
initial probability measure on the state space π , a sequence of transition kernels (M )N , and a
0 n n=1
sequence of potential functions (G )N . In the notation used in the main text, this corresponds to
n n=1
the starting distribution f , the sequence of transition densities f , and the measurement
X0 Xn|Xn−1
densities f . This induces a set of mappings from the set of probability measures on X to itself,
Yn|Xn
P(X) → P(X), as follows:
29• The update from the prediction to the filtering distributions is given by
G (x)·η (dx)
n n
π (dx) = Ψ (η )(dx) = . (85)
n n n
η (G )
n n
• The map from the prediction distribution at timestep n to timestep n+1 is given by
Φ (η ) = Ψ (η )M . (86)
n+1 n n n n+1
• The composition of maps between prediction distributions yields the map from the prediction
distribution at time k to the prediction distribution at time n where k ≤ n,
Φ = Φ ◦...◦Φ . (87)
k,n n k+1
The particle filter: The particle filter then yields a Monte Carlo approximation to the above
Feynman-Kac model, via a sequence of mixture Dirac measures. When one resamples at every
timestep, the prediction measure at timestep n is then given by
J
1 (cid:88)
ηJ = δ , (88)
n J xP n,j
j=1
and the filtering measure at timestep n is given by
(cid:80)J
g δ J
j=1 n,j xP 1 (cid:88)
πJ = n,j ≈ δ . (89)
n (cid:80)J g J xF n,j
j=1 n,j j=1
In a slight abuse of notation, we will identify xP ≡ ηJ, and xF ≡ πJ. As in [42], one can view
n,1:J n n,1:J n
this as an inhomogenous Markov process evolving on XJ. The corresponding Markov transition
kernel is then
  ⊗J
J
M n(xP n−1,1:J,·) = (cid:0) Φ n(cid:0) η nJ(cid:1)(cid:1)⊗J = Φ n J1 (cid:88) δ xP n,j , (90)
j=1
and the composition of Markov kernels on particles from timestep n to timestep n+k is written
M = M ◦...◦M . (91)
n,n+k n+k n
One may wonder why [42] require this process to evolve on XJ. This is because at every timestep
n, we in fact draw XP |{XP = xP } ∼ M (xP ,·) = η⊗JM for j = 1,...,J.
n,j n−1,1:J n−1,1:J n n−1,1:J 0 0,n
Forgetting of the particle filter: The above formalization yields a result from [42] on the
forgetting of the particle filter that we require for our analysis of the bias, variance, and error of
MOP-α. That is, [42] show that
β (M ) ≤ (1−ϵ)⌊k/(O(logJ))⌋, (92)
TV n,n+k
for some ϵ dependent on G¯,G,M¯,M in Assumptions (A3) and (A2). As a result, the mixing time
of the particle filter is only on the order of O(log(J)) timesteps.
Equippedwiththeaboveformalismsandresults, wearenowinapositiontoprovideguarantees
on the performance of MOP-α itself.
30D A Strong Law of Large Numbers for Triangular Arrays of Par-
ticles With Off-Parameter Resampling
Here, we prove a more general result, from which it will be clear that MOP-α targets the filtering
distribution. We will prove a strong law of large numbers for triangular arrays of particles with off-
parameterresampling, meaningthatweresampletheparticlesaccordingtoanarbitraryresampling
rule that is not necessarily in proportion to the target distribution of interest. Using weights that
encode the cumulative discrepancy between the resampling distribution and the target distribution
(insteadofresamplingtoequalweights,asinthebasicparticlefilter)providesasufficientcorrection
to ensure almost sure convergence. We now introduce the precise definition of an off-parameter
resampled particle filter.
Definition 1 (Off-Parameter Resampled Particle Filters). An off-parameter resampled particle
(cid:16) (cid:17)
filter is a Monte Carlo approximation to a Feynman-Kac model π ,(M )N ,(G )N , where we
0 n n=1 n n=1
inductively define given some xF , wF comprising the filtering measure approximation πJ:
n−1,j n−1,j n
1. The prediction particles at timestep n, xP , are drawn from XP ∼ πJM , and the prediction
n,j n,j n n
weights are given by wP = wF .
n,j n−1,j
2. The prediction measure at timestep n is given by
(cid:80)J wP δ
j=1 n,j xP
ηJ = n,j. (93)
n (cid:80)J wP
j=1 n,j
3. Theparticlesareresampledateverytimestepn, yieldingindicesk accordingtosomearbitrary
j
probabilities (p )J .
n,j j=1
4. the filtering measure at timestep n is given by
πJ =
(cid:80)J j=1δ xP n,jw nP ,jg n,j
≈
(cid:80)J j=1δ xP n,kjw nP ,kj g n,kj(cid:14) p n,kj
=
(cid:80)J j=1w nF ,jδ xF
n,j. (94)
n (cid:80)J wP g (cid:80)J wP g (cid:14) p (cid:80)J wF
j=1 n,j n,j j=1 n,kj n,kj n,kj j=1 n,j
5. The prediction weights at the next timestep are given by wP = wF = wP g (cid:14) p .
n+1,j n,j n,kj n,kj n,kj
To prove that the weight correction is sufficient for almost sure convergence, we first introduce
what it means for a triangular array of particles to target a given target distribution.
Definition 2 (Targeting). A pair of random vectors (X,W) drawn from some measure g targets
another measure π if for any measurable and bounded functional h,
(cid:2) (cid:3) (cid:2) (cid:3)
E h(X)·W = E h(X) . (95)
g π
A set of particles targeting π is a triangular array of pairs of random vectors (XJ,WJ),j =
j j
1,2,...,J such that for any measurable and bounded functional h,
(cid:80)J h(XJ)WJ
j=1 j j a.s.
→ E (h(X)) (96)
(cid:80)J WJ π
j=1 j
as J → ∞.
31[48] asserted without proof that common particle filter algorithms targets the filtering distri-
bution in this sense, while [47] proved a related result assuming bounded densities. We follow a
similar approach to [47], based on showing strong laws of large numbers for triangular arrays, not-
ing that triangular array strong laws do not hold without an additional regularity condition such
as boundedness in general.
In order to prove the consistency of our variation on the particle filter, we now present three
helper lemmas. The first follows from standard importance sampling arguments, the second from
integrating out the marginal, and the third from Bayes’ theorem. We state Lemma 5 assuming
multinomial resampling, which is convenient for the proof though other resampling strategies may
be preferable in practice.
Lemma 5 (Change of Weight Measure). Suppose that {(X˜J,UJ),j = 1,...,J} targets f . Now,
j j X
let {(YJ,VJ),j = 1,...,J} be a multinomial sample with indices k drawn from {(X˜J,UJ)} where
j j j j j
(X˜J,UJ) is represented, on average, proportional to πJJ times. Write
j j j
(YJ,VJ) = (cid:0) X˜J ,UJ(cid:14) πJ (cid:1) . (97)
j j kj kj kj
If the importance sampling weights U /π are bounded, then {(YJ,VJ),j = 1,...,J} targets f .
j j j j X
Proof. Note that as the YJ are a subsample from XJ, h can be a function of Y as well as it is one
j j
for X. We then expand
UJ
(cid:80) jh(Y jJ)V jJ
=
(cid:80) jh(X˜ kJ j) π kJk jj
. (98)
(cid:80) VJ UJ
j j (cid:80) kj
j πJ
kj
By hypothesis,
(cid:80) h(X˜J)UJ
j j j a →.s. E (cid:2) f(X)(cid:3) . (99)
(cid:80) UJ fX
j j
We want to show
UJ
(cid:80) jh(X kJ j) π kJk jj
−
(cid:80) jh(X˜ jJ)U jJ
a →.s.
0. (100)
UJ (cid:80) UJ
(cid:80) kj j j
j πJ
kj
For this, it is sufficient to show that
(cid:88) h(X˜J )U kJ j −(cid:88) h(X˜J)U jJ πJ a →.s. 0 (101)
kj πJ j πJ j
j kj j j
sinceanapplicationofthisresultwithh(x) = 1providesalmostsureconvergenceofthedenominator
UJ
in (100). Write g(X˜J) = h(X˜J) kj. We therefore need to show that
j j πJ
kj
(cid:88) ZJ := (cid:88)(cid:16) g(X˜J )−g(X˜J)πJ(cid:17) a →.s. 0. (102)
j kj j j
j j
32Because the functional h and importance sampling weights uJ /πJ are bounded, we have that
kj kj
E(cid:2) (ZJ)4(cid:3)
< ∞. We can then follow the argument of [47] from this point on, where noting that the
j
ZJ are conditionally independent given the X˜J and UJ,
j j j
 4(cid:12) 
(cid:88) (cid:12) (cid:104) (cid:105)
E  Z jJ  (cid:12) (cid:12)(X˜ jJ,U jJ)J j=1 = JE (Z 1J)4|(X˜ jJ,U jJ)J j=1
(cid:12)
j
+ 3J(J −1)(cid:16) E(cid:2) (ZJ)2(cid:12) (cid:12)(X˜J,UJ)J (cid:3)(cid:17) (103)
j j j j=1
≤ CJ2, (104)
for some C > 0. Taking expectations on both sides yields
 4
(cid:88)
E  Z jJ   ≤ CJ2 (105)
j
by the tower property. Now by Markov’s inequality,
(cid:12) (cid:12)   4
(cid:12) J (cid:12) J
P (cid:12) (cid:12) (cid:12)J1 (cid:88) Z jJ(cid:12) (cid:12)
(cid:12)
> ϵ ≤ ϵ41 J4E (cid:88) Z jJ   ≤ ϵ4C J2, (106)
(cid:12) j=1 (cid:12) j=1
and as these terms are summable we can apply Borel-Cantelli to conclude that these deviations
happen only finitely often for every ϵ > 0, giving us the almost-sure convergence for
(cid:88) h(XJ )
uJ
kj −(cid:88) h(XJ)
uJ
j πJ a →.s. 0. (107)
kj πJ j πJ j
j kj j j
Similarly, we also have that
(cid:88)
uJ
j πJ −(cid:88)
uJ
kj a →.s. 0, (108)
πJ j πJ
j j j kj
and the result is proved.
Remark: Note that Lemma 5 permits π to depend on {(X ,u )} as long as the resampling is
1:J j j
carried out independently of {(X ,u )}, conditional on π .
j j 1:J
Lemma 6 (Particle Marginals). Suppose that {(X˜J,UJ),j = 1,...,J} targets f . Also suppose
j j X
that Z˜J ∼ f (·|X˜J) where f is a conditional probability density function corresponding to a
j Z|X j Z|X
joint density f with marginal densities f and f . Then, if the UJ are bounded, {(Z˜J,UJ)}
X,Z X Z j j j
targets f .
Z
Proof. We want to show that, for any measurable bounded h,
(cid:80) h(Z˜J)UJ
j j j a →.s. E [h(Z)] = E (cid:2)E [h(Z)|X](cid:3) . (109)
(cid:80) UJ fZ fX f Z|X
j j
33By assumption, for any measurable and bounded functional g with domain X,
(cid:80) g(X˜J)UJ
j j j a →.s. E [g(X)]. (110)
(cid:80) UJ fX
j j
Let U¯J = JU jJ . Examine the numerator and denominator of the quantity
j (cid:80) UJ
j j
J−1(cid:80) h(Z˜J)UJ J−1(cid:80) h(Z˜J)U¯J
j j j j j j
= . (111)
J−1(cid:80) UJ J−1(cid:80) U¯J
j j j j
The denominator converges to 1 almost surely. The numerator, on the other hand, is
1 (cid:88)
h(Z˜J)U¯J, (112)
J j j
j
and by the same fourth moment argument to the above lemma, it converges almost surely to the
limit of its expectation,
   
1 (cid:88) 1 (cid:88) (cid:104) (cid:12) (cid:105)
Jl →im ∞E 
J
h(Z˜ jJ)U¯ jJ  = Jl →im ∞E 
J
E h(Z˜ jJ)U¯ jJ(cid:12) (cid:12)X˜ jJ,U¯ jJ  (113)
j j
 
1 (cid:88) (cid:104) (cid:12) (cid:105)
= Jl →im ∞E 
J
E h(Z)(cid:12) (cid:12)X = X˜ jJ U¯ jJ . (114)
j
Applying (110) with g(x) =
E(cid:2)
h(Z)|X =
x(cid:3)
, the average on the right hand side converges almost
surely to E(cid:8)E[h(Z)|X](cid:9) = E[h(Z)]. It remains to swap the limit and expectations. We can do so
with the bounded convergence theorem, and therefore obtain
1 (cid:88) h(Z˜J)U¯J a →.s. E [h(Z)]. (115)
J j j fZ
j
Lemma 7 (Particle Posteriors). Suppose that {(XJ,UJ),j = 1,...,J} targets f . Also suppose
j j X
that (X′J,U′J) = (cid:0) XJ,UJ f (z∗|XJ)(cid:1) . Then, if UJ f (z∗|XJ) and UJ f (z∗|XJ)(cid:14) f (z∗)
j j j j Z|X j j Z|X j j Z|X j Z
are bounded, {(X′J,U′J)} targets f (·|z∗).
j j X|Z
Proof. Again, we want to show that
(cid:80) h(XJ)·UJ ·f (z∗|XJ)
j
(cid:80)
Uj
J
·fj (Z z| ∗X
|XJ)
j a →.s. E
f
X|Z(cid:2) h(X)(cid:12) (cid:12)z∗(cid:3) . (116)
j j Z|X j
We already have that for any measurable bounded g,
(cid:80) g(XJ)UJ
j j j a →.s. E (cid:2) g(X)(cid:3) . (117)
(cid:80) UJ fX
j j
34Consider the following:
J−1(cid:80) h(XJ)f (z∗|XJ)UJ (cid:32) J−1(cid:80) f (z∗|XJ)UJ(cid:33)−1
j j Z|X j j j Z|X j j
× . (118)
J−1(cid:80) UJ J−1(cid:80) UJ
j j j j
WewillapplyEquation(117)tothenumeratorandthedenominatorintheratioaboveindividually.
The numerator converges to
J−1(cid:80) h(XJ)f (z∗|XJ)UJ
j j Z|X j j a →.s. E (cid:2) h(X)f (z∗|X)(cid:3) , (119)
J−1(cid:80) uJ fX Z|X
j j
while the reciprocal of the denominator converges to
J−1(cid:80) f (z∗|XJ)UJ
j Z|X j j a →.s. E (cid:2) f (z∗|X)(cid:3) = f (z∗). (120)
J−1(cid:80) UJ fX Z|X Z
j j
Now we take advantage of the identities
E (cid:2) h(X)f (z∗|X)(cid:3) (cid:20)h(X)f (z∗|X)(cid:21)
fX Z|X = E Z|X (121)
f (z∗) fX f (z∗)
Z Z
(cid:20) f (X|z∗)(cid:21)
= E h(X) X|Z = E (cid:2) h(X)|z∗(cid:3) , (122)
fX
f (X)
f
X|Z
X
to give the desired result.
Theorem 6 (Off-Parameter Resampled Particle Filters Target the Filtering Distribution). The
off-parameter resampled particle filter as outlined in Definition 1 targets the filtering distribution.
Proof. Recursively applying Lemmas 5, 6, and 7, we obtain that the off-parameter resampled
particle filter targets the posterior. Specifically, suppose inductively that (cid:8)(cid:0) XF ,wF (cid:1)(cid:9) tar-
n−1,j n−1,j
gets π . Then, Lemma 6 tells us that (cid:8)(cid:0) XP ,wP (cid:1)(cid:9) targets η . Lemma 7 tells us that
n−1 n,j n,j n
(cid:8)(cid:0) XP ,wP gθ (cid:1)(cid:9) therefore targets π . Lemma 5 guarantees that the resampling rule, given by
n,j n,j n,j n
(cid:0) XF ,wF (cid:1) = (cid:0) XP ,wP g (cid:14) p (cid:1) , (123)
n,j n,j n,kj n,kj n,kj n,kj
with resampling probabilities proportional to p , therefore also targets π .
n,j n
Proposition 1 (MOP-1TargetstheFilteringDistribution). When α = 1 or ϕ = θ, MOP-α targets
the filtering distribution.
gθ
Proof. When θ = ϕ, regardless of the value of α, the ratio n,j = 1, and this reduces to the vanilla
gϕ
n,j
particle filter estimate.
When α = 1, and θ ̸= ϕ, the proof is identical to that of 6. Recursively applying Lemmas 5,
6, and 7, we obtain that the MOP-1 filter targets the posterior. Specifically, suppose inductively
that (cid:8)(cid:0) XF,θ ,wF,θ (cid:1)(cid:9) is properly weighted for f (x |y∗ ;θ). Then, Lemma 6 tells
n−1,j n−1,j Xn−1|Y1:n−1 n−1 1:n−1
35us that (cid:8)(cid:0) XP,θ,wP,θ(cid:1)(cid:9) targets f (x |y∗ ;θ). Lemma 7 tells us that (cid:8)(cid:0) XP,θ,wP,θgθ (cid:1)(cid:9)
n,j n,j Xn|Y1:n−1 n 1:n−1 n,j n,j n,j
therefore targets f (x |y∗ ;θ). Lemma 5 guarantees that the resampling rule, given by
Xn|Y1:n n 1:n
(cid:0) XF,θ,wF,θ(cid:1) = (cid:0) XP,θ ,wP,θ gθ (cid:14) gϕ (cid:1) , (124)
n,j n,j n,kj n,kj n,kj n,kj
with resampling weights proportional to gϕ , therefore also targets f (x |y∗ ;θ).
n,j Xn|Y1:n n 1:n
This has addressed filtering, but not quite yet the likelihood evaluation. For this we use the
following lemma.
Lemma 8 (LikelihoodProperWeighting). f (y∗|y∗ ;θ) is consistently estimated by either
Yn|Y1:n−1 n 1n−1
the before-resampling estimate,
(cid:80)J gθ wP,θ
LB,θ = j=1 n,j n,j , (125)
n (cid:80)J wP,θ
j=1 n,j
or by the after-resampling estimate,
(cid:80)J wF,θ
LA,θ = Lϕ j=1 n,j . (126)
n n(cid:80)J wP,θ
j=1 n,j
where Lϕ is as defined in the various algorithms.
n
Here, (125) is a direct consequence of our earlier result that
{(cid:0) XP,θ,wP,θ(cid:1)
} targets
n,j n,j
f (x |y∗ ;θ). To see (126), we write the numerator of (5) as
Xn|Y1:n−1 n 1:n−1
Lϕ(cid:88)J (cid:34) g nθ
,j
wP,θ(cid:35) g nϕ
,j =
Lϕ(cid:88)J
wFC,θ
g nϕ
,j (127)
n gϕ n,j Lϕ n n,j Lϕ
j=1 n,j n j=1 n
gϕ
Using Lemma 5, we resample according to probabilities n,j to see this is properly estimated by
Lϕ
n
J
Lϕ(cid:88) wF,θ, (128)
n n,j
j=1
from which we obtain (126). Using Lemma 8, we obtain a likelihood estimate,
N (cid:32) (cid:80)J wF,θ(cid:33)
LA,θ = (cid:89) Lϕ j=1 n,j . (129)
n (cid:80)J wP,θ
n=1 j=1 n,j
Since wF,θ = wP,θ , this is a telescoping product. The remaining terms are (cid:80)J wP,θ = J on the
n,j n+1,j j=1 0,j
denominator and (cid:80)J wF,θ on the numerator. This derives the MOP likelihood estimates.
j=1 N,j
LB,θ should generally be preferred in practice, since there is no reason to include the extra
variability from resampling when calculating the conditional log likelihood, but it lacks the nice
telescoping product that lets us derive exact expressions for the gradient in Theorem 1.
36E Consistency of Off-Parameter Resampled Gradient Estimates
Wenowprovidearesultshowingthatundersufficientregularityconditions,onecaninterchangethe
orderofdifferentiationandexpectationoffunctionalsofoff-parameterresampledparticleestimates,
showing that if an off-parameter resampled particle estimate is consistent for some estimand, its
derivative is consistent for the derivative of the estimand as well.
One may wonder why this may be necessary, given that we have already shown strong consis-
tencyformeasurableboundedfunctionalsinTheorem6. Ifwerequirethegradienttobeboundedas
well, thenthegradientisalsoaboundedfunctional. Theansweristhatthestrongconsistencyisfor
expectationsunderthefilteringdistribution,soTheorem6onlyestablishesstrongconsistencyofthe
gradient of the estimate to its expectation under the filtering distribution, ∇ ηJ(h ) a →.s. η (∇ h ),
θ n θ n θ θ
which is not a-priori the gradient of the estimand ∇ η (h ). We require an interchange of the
θ n θ
derivative and expectation, which we show is possible when the particle estimates have two contin-
uous uniformly bounded derivatives over all J below.
Theorem 7 (Off-Parameter Particle Filters Yield Strongly Consistent Estimates of Derivatives of
Functionals). Let h : X → R be a measurable bounded functional of particles, where ηJ(h ) has
θ n θ
two continuous derivatives uniformly bounded over all J by H∗ for almost every ω ∈ Ω and θ ∈ Θ.
If it holds that ηJ(h ) a →.s. η(h ) = h∗, then we also have that ∇ ηJ(h ) a →.s. η (∇ h ) = ∇ η (h ) =
n θ θ θ θ n θ n θ θ θ n θ
∇ h∗.
θ θ
Proof. Fixω ∈ Ω. Thesequence(∇ θη nJ(h θ)(ω)) J∈N isuniformlyboundedoverallJ,byassumption.
The sequence is also uniformly equicontinuous. To see this, by assumption, the second derivative of
ηJ(h )(ω)| is also uniformly bounded over all J for almost every ω ∈ Ω and every θ′ ∈ Θ. A set
n θ θ=θ′
of functions with derivatives bounded by the same constant is uniformly Lipschitz, and therefore
uniformly equicontinuous. So the sequence (η nJ(h θ)ω)) J∈N is uniformly equicontinuous over θ for
almost every ω ∈ Ω. Explicitly, for almost every ω ∈ Ω and every ϵ > 0, there exists some δ(ω) > 0
such that for every ||θ−θ′|| < δ and every J ∈ N we have that
∞
(cid:12) (cid:12)|η nJ(h θ)(ω)−η nJ(h θ′)(ω)(cid:12) (cid:12)|
∞
< ϵ. (130)
Then, by Arzela-Ascoli, there exists a uniformly convergent subsequence. We claim that there is
onlyonesubsequentiallimit. Whenthegradientisbounded,wecantreatthegradientasabounded
functional. SobyTheorem6thesequence(η nJ(h θ)(ω)) J∈N convergespointwiseforθ = ϕandalmost
every ω ∈ Ω, and there is therefore only one subsequential limit. The sequence therefore converges
uniformly to its limit lim ηJ(h )(ω). Therefore, with uniform convergence for the derivatives
J→∞ n θ
established, we can swap the limit and derivative, and obtain that for almost every ω ∈ Ω,
lim ηJ(h )(ω) = ∇ lim ηJ(h )(ω). (131)
n θ θ n θ
J→∞ J→∞
Again from Theorem 6, we know that
ηJ(h )(ω) → η (h ) = h∗ (132)
n θ n θ θ
for almost every ω ∈ Ω. We then have that for almost every ω ∈ Ω,
lim ηJ(h )(ω) = ∇ η (h ) = ∇ h∗, (133)
n θ θ n θ θ θ
J→∞
as we wanted.
The proof of Theorem 4 is now merely a corollary, where we apply ηJ(h ) = Lˆ1(θ), η (h ) =
n θ J n θ
L(θ) = h∗, and then use the continuous mapping theorem.
θ
37F Bias-Variance Analysis
Inthissection, weprovevariousratesonthebias, variance, andMSEofMOP-α. First, wenotethe
following relation between the Dobrushin contraction coefficient and the alpha-mixing coefficients
in our context below.
Lemma 9. Setting X to be the particle collection at time m, and Y at time n, we have that the
alpha mixing coefficients,
(cid:90)
(cid:12) (cid:12)
(cid:12)f XY(x,y)−f X(x)f Y(y)(cid:12)dxdy < α, (134)
are bounded by the Dobrushin coefficient, i.e we have
(cid:90)
(cid:12) (cid:12)
(cid:12)f Y|X(y|x 1)−f Y|X(y|x 2)(cid:12)dy < α (135)
for all x , x .
1 2
Proof. We rewrite the alpha-mixing assertion as
(cid:90) (cid:90)
(cid:12) (cid:12)
(cid:12)f Y|X(y|x)−f Y(y)(cid:12)dyf X(x)dx. (136)
We claim that the Dobrushin coefficient implies
(cid:90)
(cid:12) (cid:12)
(cid:12)f Y|X(y|x 1)−f Y(y)(cid:12)dy < α (137)
for all x . This is shown as follows:
1
(cid:90) (cid:90) (cid:12)(cid:90) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:2) (cid:3) (cid:12)
(cid:12)f Y|X(y|x)−f Y(y)(cid:12)dy = (cid:12) f Y|X(y|x 1)−f Y|X(y|x) f X(x)dx(cid:12)dy (138)
(cid:12) (cid:12)
(cid:90) (cid:90)
(cid:12) (cid:12)
< (cid:12)f Y|X(y|x 1)−f Y|X(y|x)(cid:12)dyf X(x)dx (139)
(cid:90)
< αf (x)dx (140)
X
= α (141)
We then have the desired result.
F.1 Warm Up: MOP-0 Variance Bound
NotethatwemadeAssumptions(A2)and(A3)toleveragetheresultsfrom[42]ontheforgettingof
the particle filter. This is required to show error bounds on the gradient estimates that we provide,
namely that the error of the MOP-1 estimator, corresponding to the estimator of [28], is O(N4),
and that the variance of MOP-α for any α < 1 is O(N).
We can decompose the MOP-α estimator as follows. When θ = ϕ,
N N (cid:80)J wF,θ N N (cid:80)J wF,θ
Lˆ(θ) := (cid:89) LA,θ,α = (cid:89) Lϕ · j=1 n,j = (cid:89) LA,θ,α = (cid:89) Lϕ · j=1 n,j . (142)
n n (cid:80)J wP,θ n n (cid:80)J (wF,θ )α
n=1 n=1 j=1 n,j n=1 n=1 j=1 n−1,j
38Now, to illustrate the proof strategy for the general case in an easier context, we first analyze the
special case when α = 0. We now prove the variance bound for this case, presented below. To our
knowledge, this result for the variance of the gradient estimate of [24] is novel.
Theorem 8 (Variance of MOP-0 Gradient Estimate). The variance of the gradient estimate from
MOP-0, i.e. the algorithm of [24], is:
NpG′(θ)2
Var(∇ ℓˆ0(θ)) ≲ . (143)
θ
J
Proof. When α = 0, we have:
N N (cid:80)J wF,θ,0
Lˆ0(θ) := (cid:89) LA,θ,0 = (cid:89) Lϕ · j=1 n,j (144)
n n (cid:80)J wP,θ,0
n=1 n=1 j=1 n,j
=
(cid:89)N
Lϕ ·
1 (cid:88)J
s =
(cid:89)N
Lϕ ·
1 (cid:88)J f Yn|Xn(y n∗|xP n,, jθ)
. (145)
n J n,j n J f (y∗|xP,ϕ)
n=1 j=1 n=1 j=1 Yn|Xn n n,j
Similarly to the proof of Lemma 2, we define
f (y∗|xP,θ)
s =
Yn|Xn n n,j
, (146)
n,j f (y∗|xP,ϕ)
Yn|Xn n n,j
and from the exact same arguments conclude that its gradient when θ = ϕ is therefore
 
N J
(cid:88) 1 (cid:88)
∇ θℓˆ0(θ) := ∇ θlogLϕ nJ s n,j (147)
n=1 j=1
(cid:16) (cid:17)
N ∇ Lϕ1 (cid:80)J s
(cid:88) θ nJ j=1 n,j
= (148)
(cid:16) (cid:17)
Lϕ1 (cid:80)J s
n=1 nJ j=1 n,j
N (cid:80)J ∇ s
(cid:88) j=1 θ n,j
= (149)
(cid:80)J
s
n=1 j=1 n,j
=
(cid:88)N 1 (cid:88)J ∇ θf Yn|Xn(y n∗|xF n,, jθ;θ)
(150)
J f (y∗|xF,ϕ;ϕ)
n=1 j=1 Yn|Xn n n,j
N J
=
1 (cid:88)(cid:88)
∇
log(cid:16)
f
(y∗|xF,θ;θ)(cid:17)
, (151)
J θ Yn|Xn n n,j
n=1j=1
where we use the derivative of the logarithm, observe that
(cid:80)J
s = J when θ = ϕ, and use the
j=1 n,j
derivative of the logarithm where θ = ϕ again. We do this to establish that the gradient of the
log-likelihood estimate is given by the sum of terms over all N and J:
N J
∇ ℓˆ0(θ) =
1 (cid:88)(cid:88)
∇
log(cid:16)
f
(y∗|xF,θ;θ)(cid:17)
. (152)
θ J θ Yn|Xn n n,j
n=1j=1
39Therefore, for a given θ in the parameter vector θ,
i
 
(cid:18) (cid:19) N J
Var ∂∂ ℓˆ0(θ) = J1 2Var(cid:88)(cid:88) ∂∂ log(cid:16) f Yn|Xn(y n∗|xF n,, jθ;θ)(cid:17)  (153)
θi
n=1j=1
θi
 
N J
= J1
2
(cid:88) Var(cid:88) ∂∂ log(cid:16) f Yn|Xn(y n∗|xF n,, jθ;θ)(cid:17)  (154)
n=1 j=1
θi
 
J J
+2 (cid:88) Cov J1 (cid:88) ∂∂ log(cid:16) f Ym|Xm(y m∗ |xF m, ,θ j;θ)(cid:17) , J1 (cid:88) ∂∂ log(cid:16) f Yn|Xn(y n∗|xF n,, jθ;θ)(cid:17)  (155)
m<n j=1
θi
j=1
θi
N (cid:18) (cid:19) (cid:18) (cid:19)
(cid:88) ∂ (cid:88) ∂ ∂
= Var ℓˆ0(θ) +2 Cov ℓˆ0 (θ), ℓˆ0(θ) . (156)
∂ n ∂ m ∂ n
n=1
θi
m<n
θi θi
Here, we use Assumptions (A2) and (A3) that ensure strong mixing. We know from Theorem
3 of [42] that when M is the k-step Markov operator from timestep n and β (M) =
n,n+k TV
sup ∥M(x,·)−M(y,·)∥ = sup
∥µM−νM∥TV
is the Dobrushin contraction coefficient
x,y∈E TV µ,ν∈P,µ̸=ν ∥µ−ν∥TV
of a Markov operator,
β (M ) ≤ (1−ϵ)⌊k/(clog(J))⌋, (157)
TV n,n+k
i.e. the mixing time of the particle filter is O(log(J)), where ϵ and c depend on M¯,M,G¯,G in (A2)
and (A3).
By Lemma 9, the particle filter itself is strong mixing, with α-mixing coefficients a(k) ≤
(1 − ϵ)⌊k/(clog(J))⌋. Therefore, functions of particles are strongly mixing as well, with α-mixing
coefficients bounded by the original (to see this, observe that the σ-algebra of the function-
als is contained within the original σ-algebra). Therefore, by Davydov’s inequality, noting that
∂ ℓˆ0(θ) ≤ G′(θ) by Assumption (A3), and without loss of generality labeling m and n such that
∂ n
θi
E(cid:2) ( ∂ ℓˆ0 (θ))4(cid:3)1/4 ≤ E(cid:2) ( ∂ ℓˆ0(θ))4(cid:3)1/4 , we see that
∂ m ∂ n
θi θi
(cid:18)
∂ ∂
(cid:19)
(cid:34)
(cid:18)
∂
(cid:19)4(cid:35)1/4 (cid:34)
(cid:18)
∂
(cid:19)4(cid:35)1/4
Cov ℓˆ0 (θ), ℓˆ0(θ) ≤ a(n−m)1/2E ℓˆ0 (θ) E ℓˆ0(θ) (158)
∂ m ∂ n ∂ m ∂ n
θi θi θi θi
(cid:34)
(cid:18)
∂
(cid:19)4(cid:35)1/2
≤ a(n−m)1/2E ℓˆ0(θ) . (159)
∂ n
θi
To bound this, we use the fact that
 
(cid:34)
(cid:18)
∂
(cid:19)4(cid:35)
(cid:18)
∂
(cid:19)4
(cid:34)
(cid:18)
∂
(cid:19)2(cid:35)2 (cid:34)
(cid:18)
∂
(cid:19)2(cid:35)2
E
∂
ℓˆ0 n(θ) = E 
∂
ℓˆ0 n(θ) −E
∂
ℓˆ0 n(θ) +E
∂
ℓˆ0 n(θ) (160)
θi θi θi θi
alongside Lemma 2 of [42], which shows that
 
(cid:18) ∂ (cid:19)4
(cid:34)
(cid:18) ∂
(cid:19)2(cid:35)2
G′(θ)4
E 
∂
ℓˆ0 n(θ) −E
∂
ℓˆ0 n(θ)  ≲
J2
, (161)
θi θi
(cid:34) (cid:35)
(cid:18) ∂ (cid:20) ∂ (cid:21)(cid:19)2 G′(θ)2
E ℓˆ0(θ)−E ℓˆ0(θ) ≲ . (162)
∂ n ∂ n J
θi θi
40It follows that
(cid:34)
(cid:18) ∂
(cid:19)4(cid:35)1/2 (cid:115)
G′(θ)4 (cid:18) G′(θ)2(cid:19)2 G′(θ)2
E ℓˆ0(θ) ≲ + = , (163)
∂ n J2 J J
θi
and we conclude that
Cov(cid:18) ∂ ℓˆ0 (θ), ∂ ℓˆ0(θ)(cid:19) ≤ (1−ϵ)21⌊ c|n lo− g(m J| )⌋G′(θ)2 . (164)
∂ m ∂ n J
θi θi
Putting it all together, we see that
(cid:18) (cid:19) N (cid:18) (cid:19) (cid:18) (cid:19)
∂ (cid:88) ∂ (cid:88) ∂ ∂
Var ℓˆα(θ) = Var ℓˆ0(θ) +2 Cov ℓˆ0 (θ), ℓˆ0(θ) (165)
∂ ∂ n ∂ m ∂ n
θi
n=1
θi
m<n
θi θi
NG′(θ)2 N (cid:88)−1
1⌊ n
⌋G′(θ)2
≤ +2 (N −n)(1−ϵ)2 clog(J) (166)
J J
n=1
G′(θ)2 (cid:32) N (cid:88)−1
1⌊ n
⌋(cid:33)
≤ N +2 (N −n)(1−ϵ)2 clog(J) (167)
J
n=1
G′(θ)2N
≲ . (168)
J
It then follows that as θ ∈ Θ ⊆ Rp,
Var(cid:0) ∇ ℓˆα(θ)(cid:1) ≲
NpG′(θ)2
. (169)
θ
J
Remark: Note that the factor of N that pops up here is due to the use of the unnormalized
√
gradient. If one divides the gradient estimate by N before usage, the variance does not depend
√
on the horizon. If one divides by N, the error is O(1/ NJ).
NpG′(θ)2
The variance here of can be thought of as a lower bound for the variance of MOP-α.
J
We will later see that the variance of MOP-α contains this term, and an additional term that
corresponds to the memory of the MOP-α gradient estimate.
F.2 MOP-α Variance Bound
We are now in a position to tackle the MOP-α variance bound. Here, we analyze the case for
α ∈ (0,1). The proof strategy is as follows. Instead of analyzing the variance of the MOP-α
gradient estimate proper, we will analyze the variance of a modified estimator where we truncate
theweightsatk timesteps,sothisestimatoronlylooksatmostk timestepsbackwhileaccumulating
the importance weights. We write
 (cid:18) (cid:19)α(n−i)
(cid:80)J (cid:81)n g iA ,j,F,θ
(cid:88)N  j=1 i=n−k gA,F,ϕ 
sˆα(θ) := log i,j  (170)
k  (cid:18) (cid:19)α(n−i)
n=1 (cid:80)J (cid:81)n−1 g iA ,j,F,θ 
j=1 i=n−k gA,F,ϕ
i,j
41for the log-likelihood estimate from the k-truncated estimator with discount factor α. It is the sum
of components
J n
sˆα (θ) :=
1 (cid:88) (cid:88) (cid:16) αn−ilog(cid:16) gA,F,θ(cid:17)
−αn−i+1∇
log(cid:16) gA,F,θ(cid:17)(cid:17)
(171)
n,k J i,j θ i−1,j
j=1i=n−k
over timesteps N.
This enables us to establish strong mixing for the truncated estimator, in order to get a bound
on the variance that is O(NJ−1). We emphasize that this truncated estimator is only a theoretical
construct – we do not actually use the truncated estimator. The truncated estimator, as we note in
the discussion, possesses similar theoretical guarantees as the MOP-α estimator, but would require
the user to specify the number of timesteps to truncate at instead of a discount factor.
Coupledwithaboundensuringthatthek-truncatedestimatorprovidesanestimatethatisclose
to that of MOP-α proper, we get a bound on the variance of MOP-α that comprises of the variance
of the k-truncated estimator plus the error, for any k ≤ N. It then holds that the final variance
bound on MOP-α is given by the minimum over all k, and is never larger than O(ψ(α)+NJ−1)
for some function ψ increasing in α.
Theorem9(VarianceofMOP-αGradientEstimate). Whenα ∈ (0,1), thevarianceofthegradient
estimate from MOP-α is
Var(cid:0) ∇ ℓˆα(θ)(cid:1) ≲
min(cid:18) k2G′(θ)2Np
+
αk NpG′(θ)2(cid:19)
. (172)
θ k≤N (1−α)2J 1−α
Proof. Using the derivative of the logarithm and that wF,θ,α = wF,θ,α,k = 1 when θ = ϕ,
n,j n,j
(cid:13)    (cid:13)2
(cid:13) J J (cid:13)
(cid:13) (cid:13) (cid:13)∇ θlog(cid:88) w nF ,, jθ,α −∇ θlog(cid:88) w nF ,, jθ,α,k (cid:13) (cid:13)
(cid:13)
(173)
(cid:13) j=1 j=1 (cid:13)
2
(cid:13) (cid:13)∇ (cid:80)J wF,θ,α ∇ (cid:80)J wF,θ,α,k(cid:13) (cid:13)2
= (cid:13) θ j=1 n,j − θ j=1 n,j (cid:13) (174)
(cid:13)
(cid:13)
(cid:80)J wF,θ,α (cid:80)J wF,θ,α,k (cid:13)
(cid:13)
j=1 n,j j=1 n,j 2
(cid:13) (cid:13)2
(cid:13) J J (cid:13)
= (cid:13) (cid:13) (cid:13)J1 (cid:88) ∇ θw nF ,, jθ,α− J1 (cid:88) ∇ θw nF ,, jθ,α,k(cid:13) (cid:13)
(cid:13)
(175)
(cid:13) j=1 j=1 (cid:13)
2
(cid:13) (cid:13)2
(cid:13) (cid:13)1 (cid:88)J ∇ θw nF ,, jθ,α 1 (cid:88)J ∇ θw nF ,, jθ,α,k(cid:13) (cid:13)
= (cid:13) − (cid:13) (176)
(cid:13)J wF,θ,α J wF,θ,α,k (cid:13)
(cid:13) j=1 n,j j=1 n,j (cid:13)
2
(cid:13) (cid:13)2
(cid:13) J J (cid:13)
= (cid:13) (cid:13) (cid:13)J1 (cid:88) ∇ θlog(cid:16) w nF ,, jθ,α(cid:17) − J1 (cid:88) ∇ θlog(cid:16) w nF ,, jθ,α,k(cid:17)(cid:13) (cid:13)
(cid:13)
. (177)
(cid:13) j=1 j=1 (cid:13)
2
42This lets us bound the cumulative weight discrepancies by
(cid:13)    (cid:13)2
(cid:13) J J (cid:13)
(cid:13) (cid:13) (cid:13)∇ θlog(cid:88) w nF ,, jθ,α −∇ θlog(cid:88) w nF ,, jθ,α,k (cid:13) (cid:13)
(cid:13)
(178)
(cid:13) j=1 j=1 (cid:13)
2
(cid:13) (cid:13)2
(cid:13) J J (cid:13)
= (cid:13) (cid:13) (cid:13)J1 (cid:88) ∇ θlog(cid:16) w nF ,, jθ,α(cid:17) − J1 (cid:88) ∇ θlog(cid:16) w nF ,, jθ,α,k(cid:17)(cid:13) (cid:13)
(cid:13)
(179)
(cid:13) j=1 j=1 (cid:13)
2
(cid:13) (cid:13)2
(cid:13) J (cid:13)
= (cid:13) (cid:13) (cid:13)J1 (cid:88) ∇ θ(cid:16) log(cid:16) w nF ,, jθ,α(cid:17) −log(cid:16) w nF ,, jθ,α,k(cid:17)(cid:17)(cid:13) (cid:13)
(cid:13)
(180)
(cid:13) j=1 (cid:13)
2
=
(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)J1 (cid:88)J
∇
θ(cid:32) (cid:88)n α(n−i)log(cid:32) gg AiA ,j ,, FF ,, ϕθ(cid:33)
−
(cid:88)n α(n−i)log(cid:32) gg AiA ,j ,, FF ,, ϕθ(cid:33)(cid:33)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)2
(181)
(cid:13) j=1 i=1 i,j i=n−k i,j (cid:13)
2
=
(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)J1 (cid:88)J
∇
θ(cid:32)n (cid:88)−k α(n−i)log(cid:32) gg AiA ,j ,, FF ,, ϕθ(cid:33)(cid:33)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)2
(182)
(cid:13) j=1 i=1 i,j (cid:13)
2
J n−k
≤
1 (cid:88)(cid:88) α(n−i)(cid:13)
(cid:13)∇
log(cid:16) gA,F,θ(cid:17)(cid:13) (cid:13)2
(183)
J (cid:13) θ i,j (cid:13) 2
j=1 i=1
J n−k
1 (cid:88)(cid:88)
≤ α(n−i)pG′(θ)2 (184)
J
j=1 i=1
αk −αn
≤ pG′(θ)2 , (185)
1−α
where the second-last line follows from Assumption (A3). We can then bound ∥∇ ℓˆα(θ)−∇ sˆα(θ)∥
θ θ k
43as follows:
(cid:13) (cid:13)2
(cid:13)∇ ℓˆα(θ)−∇ sˆα(θ)(cid:13) (186)
(cid:13) θ θ k (cid:13)
2
(cid:13)  (cid:18) (cid:19)α(n−i)   (cid:18) (cid:19)α(n−i)(cid:13)2
(cid:13)
(cid:13)
(cid:80)J (cid:81)n g iA ,j,F,θ (cid:80)J (cid:81)n g iA ,j,F,θ (cid:13)
(cid:13)
(cid:13)(cid:88)N  j=1 i=1 gA,F,ϕ  (cid:88)N  j=1 i=n−k gA,F,ϕ (cid:13)
= (cid:13) ∇ log i,j − ∇ log i,j (cid:13)
(cid:13) θ  (cid:18) (cid:19)α(n−i) θ  (cid:18) (cid:19)α(n−i)(cid:13)
(cid:13) (cid:13)n=1 (cid:80)J (cid:81)n−1 g iA ,j,F,θ  n=1 (cid:80)J (cid:81)n−1 g iA ,j,F,θ (cid:13) (cid:13)
(cid:13) j=1 i=1 gA,F,ϕ j=1 i=n−k gA,F,ϕ (cid:13)
i,j i,j 2
=
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)(cid:88)N
∇
θ(cid:32) log (cid:88)J (cid:89)n (cid:32) gg AiA
,j
,, FF ,, ϕθ(cid:33)α(n−i) −log (cid:88)J n (cid:89)−1(cid:32) gg AiA
,j
,, FF ,, ϕθ(cid:33)α(n−i)

n=1 j=1i=1 i,j j=1 i=1 i,j
−log (cid:88)J (cid:89)n (cid:32) gg AiA
,j
,, FF ,, ϕθ(cid:33)α(n−i) +log (cid:88)J n (cid:89)−1 (cid:32) gg AiA
,j
,, FF ,, ϕθ(cid:33)α(n−i) (cid:33)(cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:13)2
(187)
j=1i=n−k i,j j=1i=n−k i,j 2
(cid:13) (cid:13) N (cid:32)  J   J   J   J  (cid:33)(cid:13) (cid:13)2
= (cid:13) (cid:13) (cid:13)(cid:88) ∇ θ log(cid:88) w nF ,, jθ,α −log(cid:88) w nF ,, jθ,α,k −log(cid:88) w nA −,F 1, ,θ j,α +log(cid:88) w nA −,F 1, ,θ j,α,k  (cid:13) (cid:13)
(cid:13)
(cid:13)n=1 j=1 j=1 j=1 j=1 (cid:13)
2
(188)
(cid:13)     (cid:13)
N (cid:13) J J (cid:13)
≤ (cid:88)(cid:13) (cid:13) (cid:13)∇ θlog(cid:88) w nF ,, jθ,α −log(cid:88) w nF ,, jθ,α,k (cid:13) (cid:13)
(cid:13)
n=1(cid:13) j=1 j=1 (cid:13)
∞
(cid:13)     (cid:13)2
N (cid:13) J J (cid:13)
+(cid:88)(cid:13) (cid:13) (cid:13)∇ θlog(cid:88) w nA −,F 1, ,θ j,α +log(cid:88) w nA −,F 1, ,θ j,α,k (cid:13) (cid:13)
(cid:13)
, (189)
n=1(cid:13) j=1 j=1 (cid:13)
2
and each of these two terms is bounded by αk NpG′(θ)2. So we now know that
1−α
(cid:13) (cid:13)2 2αk
(cid:13)∇ ℓˆα(θ)−∇ sˆα(θ)(cid:13) ≤ NpG′(θ)2, (190)
(cid:13) θ θ k (cid:13) 2 1−α
which is our desired error bound. Now, we bound the variance of ∇ sˆα(θ). Recall that we defined
θ k
 (cid:18) (cid:19)α(n−i)
(cid:80)J (cid:81)n g iA ,j,F,θ
(cid:88)N  j=1 i=n−k gA,F,ϕ 
∇ sˆα(θ) = ∇ log i,j  (191)
θ k θ  (cid:18) (cid:19)α(n−i)
n=1 (cid:80)J (cid:81)n−1 g iA ,j,F,θ 
j=1 i=n−k gA,F,ϕ
i,j
    
N J J
= (cid:88) ∇ θlog(cid:88) w nF ,, jθ,α,k −log(cid:88) w nA −,F 1, ,θ j,α,k . (192)
n=1 j=1 j=1
We can then decompose this expression with the derivative of the logarithm while noting that
44wF,θ,α,k = 1 whenever θ = ϕ, to see that
n,j
    
N J J
∇ θsˆα k(θ) = (cid:88) ∇ θlog(cid:88) w nF ,, jθ,α,k −log(cid:88) w nA −,F 1, ,θ j,α,k  (193)
n=1 j=1 j=1
N (cid:32)(cid:80)J ∇ wF,θ,α,k (cid:80)J ∇ wA,F,θ,α,k(cid:33)
(cid:88) j=1 θ n,j j=1 θ n−1,j
= − (194)
(cid:80)J wF,θ,α,k (cid:80)J wA,F,θ,α,k
n=1 j=1 n,j j=1 n−1,j
 
N J J
= (cid:88)  J1 (cid:88) ∇ θw nF ,, jθ,α,k − J1 (cid:88) ∇ θw nA −,F 1, ,θ j,α,k  (195)
n=1 j=1 j=1
N J
= 1 (cid:88)(cid:88) ∇ (cid:16) wF,θ,α,k −wA,F,θ,α,k(cid:17) . (196)
J θ n,j n−1,j
n=1j=1
It now follows that we need to bound the variance at a single timestep, namely
 
J
Var J1 (cid:88) ∇ θ(cid:16) w nF ,, jθ,α,k −w nA −,F 1, ,θ j,α,k(cid:17) . (197)
j=1
We use the derivative of the logarithm yet again to find, noting that wF,θ,α,k = 1 when θ = ϕ, that
n,j
∇ wF,θ,α,k n (cid:32) gA,F,θ(cid:33)
∇ wF,θ,α,k = θ n,j = ∇ log(wF,θ,α,k) = (cid:88) αn−i∇ log i,j (198)
θ n,j wF,θ,α,k θ n,j θ gA,F,ϕ
n,j i=n−k i,j
n
=
(cid:88)
αn−i∇
log(cid:16) gA,F,θ(cid:17)
. (199)
θ i,j
i=n−k
Then,
(cid:16) (cid:17)
Var ∇ (cid:0) wF,θ,α,k −wA,F,θ,α,k(cid:1) (200)
θ n,j n−1,j
(cid:32) (cid:32) n n−1 (cid:33)(cid:33)
= Var ∇
(cid:88)
αn−i∇
log(cid:16) gA,F,θ(cid:17)
−
(cid:88)
αn−i∇
log(cid:16) gA,F,θ(cid:17)
(201)
θ θ i,j θ i,j
i=n−k i=n−k−1
(cid:32) n (cid:33)
= Var
(cid:88)
∇
(cid:16) αn−ilog(cid:16) gA,F,θ(cid:17) −αn−i+1log(cid:16) gA,F,θ(cid:17)(cid:17)
. (202)
θ i,j i−1,j
i=n−k
Note that
J n
∇ sˆα (θ) :=
1 (cid:88) (cid:88)
∇
(cid:16) αn−ilog(cid:16) gA,F,θ(cid:17)
−αn−i+1∇
log(cid:16) gA,F,θ(cid:17)(cid:17)
(203)
θ n,k J θ i,j θ i−1,j
j=1i=n−k
is a function bounded by CG′(θ)(1−αk) =: CG′(θ)r in each coordinate for some constant C (by
1−α
Assumption (A3)) that depends only on k timesteps, and not n. Subsequently, we suppress vector
andmatrixnotationforthecoordinatesofθ anditsderivatives,andtheirvariancesandcovariances,
45with the variance of a vector interpreted as the sum of the variance of its components. We invoke
Lemma 2 of [42] to bound the L error of each of the k (from time n−k to time n) functionals
2 √
from its expectation under the posterior by Cr pG′(θ)J−1/2k. That is, we have that
√
Cr pG′(θ)k
E(cid:2) ∥∇ sˆα (θ)−E ∇ sˆα (θ)∥2(cid:3)1/2 ≤ √ . (204)
θ n,k π θ n,k 2
J
By the bias-variance decomposition, this in turn bounds the variance at a single timestep by
C2r2pG′(θ)2J−1k2. Concretely, we have that
 
Var
J1 (cid:88)J (cid:88)n
∇
θ(cid:16) αn−ilog(cid:16)
g iA
,j,F,θ(cid:17) −αn−i+1log(cid:16)
g iA −,F 1,,
jθ(cid:17)(cid:17)
 ≲
r2k2p2 JG′(θ)2
. (205)
j=1i=n−k
It now remains to bound the variance of ∇ sˆα(θ) by considering the covariance of each of the
θ k
N terms that comprise it. We decompose
 
N J n
Var(cid:0) ∇ θsˆα k(θ)(cid:1) = Var(cid:88) J1 (cid:88) (cid:88) ∇ θ(cid:16) αn−ilog(cid:16) g iA ,j,F,θ(cid:17) −αn−i+1log(cid:16) g iA −,F 1,, jθ(cid:17)(cid:17)  (206)
n=1 j=1i=n−k
N
= (cid:88) Var(cid:0) ∇ sˆα (θ)(cid:1) +2 (cid:88) Cov(cid:0) ∇ sˆα (θ),∇ sˆα (θ)(cid:1) (207)
θ n,k θ m,k θ n,k
n=1 m<n
≲
r2k2pG′(θ)2N
+2 (cid:88) Cov(cid:0) ∇ sˆα (θ),∇ sˆα (θ)(cid:1) . (208)
J θ m,k θ n,k
m<n
Similarly to the proof of the MOP-0 case, we use Assumptions (A2) and (A3) that ensure
strong mixing. We know from Theorem 3 of [42] that when M is the k-step Markov operator
n,n+k
from timestep n and β (M) = sup ∥M(x,·)−M(y,·)∥ = sup
∥µM−νM∥TV
is the
TV x,y∈E TV µ,ν∈P,µ̸=ν ∥µ−ν∥TV
Dobrushin contraction coefficient of a Markov operator,
β (M ) ≤ (1−ϵ)⌊k/(clog(J))⌋, (209)
TV n,n+k
i.e. the mixing time of the particle filter is O(log(J)), where ϵ and c depend on M¯,M,G¯,G in
(A2) and (A3). By Lemma 9, the particle filter itself is strongly mixing, with α-mixing coefficients
a(l) ≤ (1−ϵ)⌊l/(clog(J))⌋. Therefore,functionsofparticlesarestronglymixingaswell,withα-mixing
coefficients bounded by the original (to see this, observe that the σ-algebra of the functionals is
contained within the original σ-algebra).
Wenowderivetheα-mixingcoefficientsof(∇ sˆα (θ))N . Observethat∇ sˆα isstrongmixing
θ n,k n=1 θ n,k
at lag k+1, as all weights beyond k timesteps are truncated. We therefore have that the mixing
time of ∇ sˆα (θ) is O(1+k +log(J)), and that the α-mixing coefficients for (∇ sˆα (θ))N are
θ n,k θ n,k n=1
given by a(l) ≤ (1−ϵ)⌊l/(c(1+k+log(J)))⌋. Therefore, by Davydov’s inequality and Lemma 2 of [42],
and noting that ∇ sˆα (θ) ≤ CrG′(θ) by Assumption (A3), by a similar argument to the MOP-0
θ n,k
case, we find that
Cov(cid:0) ∇ θsˆα m,k(θ),∇ θsˆα n,k(θ)(cid:1) ≲ (1−ϵ)1 2⌊ c(1+| kn +− lm og|
(J))⌋r2pG J′(θ)2
. (210)
46Concretely, noting that ∇ sˆα (θ) ≤ CrG′(θ) by Assumption (A3), and, without loss of generality,
θ n,k
assuming E[(∇ sˆα (θ))4]1/4 ≤ E[(∇ sˆα (θ))4]1/4, we apply Davydov’s inequality to see that
θ m,k θ n,k
Cov(cid:0) ∇ sˆα (θ),∇ sˆα (θ)(cid:1) ≤ a(n−m)1/2E(cid:2) (∇ sˆα (θ))4(cid:3)1/4E(cid:2) (∇ sˆα (θ))4(cid:3)1/4 (211)
θ m,k θ n,k θ m,k θ n,k
≤ a(n−m)1/2E(cid:2) (∇ sˆα (θ))4(cid:3)1/2 . (212)
θ n,k
To bound this, we use the fact that
(cid:104) (cid:105)
E(cid:2) (∇ sˆα (θ))4(cid:3) = E (cid:0) ∇ sˆα (θ)−E(cid:2) ∇ sˆα (θ)](cid:1)4 +E(cid:2) (∇ sˆα (θ))2(cid:3)2 (213)
θ n,k θ n,k θ n,k θ n,k
alongside Lemma 2 of [42], which alongside the fact that
E(cid:104)
(cid:0) ∇ sˆα (θ)−E[∇ sˆα
(θ)](cid:1)4(cid:105)
≲
r4p2G′(θ)4
, (214)
θ n,k θ n,k J2
E(cid:104)
(cid:0) ∇ sˆα (θ)−E[∇ sˆα
(θ)](cid:1)2(cid:105)
≲
r2pG′(θ)2
, (215)
θ n,k θ n,k J
allows us to find that
(cid:115)
E(cid:104)
(cid:0) ∇ sˆα
(θ)(cid:1)4(cid:105)1/2
≲
r4p2G′(θ)4 +(cid:18) r2pG′(θ)2(cid:19)2
=
r2pG′(θ)2
, (216)
θ n,k J2 J J
and conclude that
Cov(cid:0) ∇ θsˆα m,k(θ),∇ θsˆα n,k(θ)(cid:1) ≤ (1−ϵ)21⌊ c(1+| kn +− lm og|
(J))⌋r2pG J′(θ)2
. (217)
Putting it all together, we see that
N
Var(cid:0) ∇ sˆ (θ)(cid:1) = (cid:88) Var(cid:0) ∇ sˆα (θ)(cid:1) +2 (cid:88) Cov(cid:0) ∇ sˆα (θ),∇ sˆα (θ)(cid:1) (218)
θ k θ n,k θ m,k θ n,k
n=1 m<n
Nr2pk2G′(θ)2 N (cid:88)−1
1⌊ n
⌋r2pG′(θ)2
≤ +2 (N −n)(1−ϵ)2 c(1+k+log(J)) (219)
J J
n=1
r2pk2G′(θ)2 (cid:32) N (cid:88)−1
1⌊ n
⌋(cid:33)
≤ N +2 (N −n)(1−ϵ)2 c(1+k+log(J)) (220)
J
n=1
k2r2pG′(θ)2N
≲ . (221)
J
We will now use this result to bound
Var(cid:0)
∇
sˆα(θ)(cid:1)
. For random variables X,Y where |X−Y|
θ k
is bounded almost surely by some M, we have that
X −EX −2M ≤ Y −EY ≤ X −EX +2M (222)
so,
(Y −EY)2 ≤ (X −EX −2M)2+(X −EX +2M)2. (223)
47Taking expectations, we get
Var(Y) ≤ Var(X)+8M2. (224)
It follows from this result and the result we proved earlier, namely that
(cid:13) (cid:13)∇ θℓˆα(θ)−∇ θsˆα k(θ)(cid:13) (cid:13)2
2
≤ 12 −αk αNpG′(θ)2, (225)
that the variance of MOP-α proper is bounded by, for any k ≤ N, that the variance of MOP-α
proper is bounded by
Var(cid:0) ∇ ℓˆα(θ)(cid:1) ≲
k2r2G′(θ)2Np
+
αk
NpG′(θ)2. (226)
θ
J 1−α
As the above holds for any k ≤ N,
Var(cid:0) ∇ ℓˆα(θ)(cid:1) ≲
min(cid:18) k2pG′(θ)2Np
+
αk NpG′(θ)2(cid:19)
. (227)
θ k≤N (1−α)2J 1−α
F.3 MOP-α MSE Bound
Theorem 10 (MSE of MOP-α). When α ∈ (0,1), the MSE of MOP-α is given by
E(cid:13) (cid:13)∇ θℓ(θ)−∇ θℓˆα(θ)(cid:13) (cid:13)2
2
≲ km ≤i NnNpG′(θ)2(cid:18) k J2 +(1−ϵ)⌊k/(clog(J))⌋+k+ αk + 1α −k+ α1−α(cid:19) . (228)
Proof. The broad idea is to decompose the MSE into three terms as in Equation 230. The first
term can be controlled by a mixing argument, the second term garners O(Nk) error, and the third
(cid:16) (cid:17)
term is O N . It is unclear whether the bias can be reduced further – the gradient is a sum of
1−α
N terms, and is therefore O(N) itself.
The first term is the problem term. We will first bound the ground truth of conditional scores
from particle approximation conditional on correct filtering distribution at time n−k, and then
bound that from the particle approximation resulting from an arbitrary filtering distribution at
time n−k. That is, where πˆ is the particle approximation at time n of the posterior π and we
n n
write ∇ sˆ1 (θ) for the truncated MOP-1 conditional score estimate given the correct
θ n,k|πˆ =π
n−k n−k
filtering distribution at time n−k, we can first observe that
(cid:13) (cid:13)2
N N N
(cid:13) (cid:13)∇ θℓ
θ
−∇ θℓˆα(θ)(cid:13) (cid:13)2
2
= (cid:13) (cid:13) (cid:13)(cid:88) ∇ θℓˆα n(θ)−(cid:88) ∇ θℓ n(θ)(cid:13) (cid:13)
(cid:13)
≤ (cid:88)(cid:13) (cid:13) (cid:13)∇ θℓˆα n(θ)−∇ θℓ n(θ)(cid:13) (cid:13) (cid:13)2 , (229)
(cid:13) (cid:13) 2
n=1 n=1 2 n=1
and decompose
N N
(cid:88)(cid:13) (cid:13)2 (cid:88)
(cid:13)∇ ℓˆα(θ)−∇ ℓ (θ)(cid:13) ≤ ∥∇ ℓ (θ)−∇ sˆ1 (θ)∥2 +
(cid:13) θ n θ n (cid:13) θ n θ n,k 2
2
n=1 n=1
N N
(cid:88) (cid:88)
∥∇ sˆ1 (θ)−∇ sˆα (θ)∥2+ ∥∇ sˆα (θ)−∇ ℓˆα(θ)∥2. (230)
θ n,k θ n,k 2 θ n,k θ n 2
n=1 n=1
48We bound the first term of Equation 230 , (cid:80)N ∥∇ ℓ (θ)−∇ sˆ1 (θ)∥2, by decomposing it into
n=1 θ n θ n,k 2
two terms,
N
(cid:88)
∥∇ ℓ (θ)−∇ sˆ1 (θ)∥2 ≤
θ n θ n,k 2
n=1
N N
(cid:88) (cid:88)
∥∇ ℓ (θ)−∇ sˆ1 (θ)∥2+ ∥∇ sˆ1 (θ)−∇ sˆ1 (θ)∥2. (231)
θ n θ n,k|πˆ =π 2 θ n,k|πˆ =π θ n,k 2
n−k n−k n−k n−k
n=1 n=1
The first term of Equation 230 is a particle approximation dependent on k timesteps, so by Lemma
2 of [42], this is bounded by
CpG′(θ)2k2
E∥∇ ℓ (θ)−∇ sˆ1 (θ)∥2 ≤ . (232)
θ n θ n,k|πˆ n−k=π n−k 2 J
Bounding the second term of Equation 230 amounts to bounding the difference between func-
tionals of two different particle measures that mix under the same Markov kernel. Here, we use
Assumptions (A2) and (A3) that ensure strong mixing. We know from Theorem 3 of [42] that
when M is the k-step Markov operator from timestep n and β (M) = sup ∥M(x,·)−
n,n+k TV x,y∈E
M(y,·)∥ = sup
∥µM−νM∥TV
is the Dobrushin contraction coefficient of a Markov oper-
TV µ,ν∈P,µ̸=ν ∥µ−ν∥TV
ator,
β (M ) ≤ (1−ϵ)⌊k/(clog(J))⌋, (233)
TV n,n+k
i.e. the mixing time of the particle filter is O(log(J)), where ϵ and c depend on M¯,M,G¯,G in (A2)
and (A3).
Then, we can bound E∥∇ sˆ1 (θ)−∇ sˆ1 (θ)∥2 by
θ n,k|πˆ =π θ n,k 2
n−k n−k
∥µM −νM ∥
sup n,n+k n,n+k TV = β (M ) ≤ (1−ϵ)⌊k/(clog(J))⌋, (234)
TV n,n+k
∥µ−ν∥
µ,ν∈P,µ̸=ν TV
implying that
E(cid:13) (cid:13)∇ θsˆ1
n,k|πˆ n−k=π
n−k(θ)−∇ θsˆ1 n,k(θ)(cid:13) (cid:13)2
2
(235)
≲ sup sup pG′(θ)2(cid:12) (cid:12)(µM n,n+k)(ψ)−(νM n,n+k)(ψ)(cid:12) (cid:12) (236)
µ,ν ∥ψ∥∞≤1/2
≤
suppG′(θ)2(cid:13)
(cid:13)µM n,n+k −νM
n,n+k(cid:13)
(cid:13) TV (237)
µ,ν
≤ pG′(θ)2(1−ϵ)⌊k/(clog(J))⌋∥πˆ −π ∥ (238)
n−k n−k TV
≤ pG′(θ)2(1−ϵ)⌊k/(clog(J))⌋ sup (cid:12) (cid:12)πˆ n−k(ψ)−π n−k(ψ)(cid:12) (cid:12) (239)
∥ψ∥∞≤1/2
≲ pG′(θ)2(1−ϵ)⌊k/(clog(J))⌋. (240)
Therefore, we have that
E(cid:13) (cid:13)∇ θℓ(θ)−∇ θsˆ1 k(θ)(cid:13) (cid:13)2
2
(241)
N N
≤ (cid:88) E(cid:13) (cid:13)∇ θℓ n(θ)−∇ θsˆ1
n,k|πˆ n−k=π
n−k(θ)(cid:13) (cid:13)2 2+(cid:88) E(cid:13) (cid:13)∇ θsˆ1
n,k|πˆ n−k=π
n−k(θ)−∇ θsˆ1 n,k(θ)(cid:13) (cid:13)2
2
(242)
n=1 n=1
CpG′(θ)2k2
≲ N +NpG′(θ)2(1−ϵ)⌊k/(clog(J))⌋. (243)
J
49Now that the first term, E∥∇ ℓ(θ)∥2, is taken care of, it remains to bound E∥∇ sˆ1(θ)−∇ sˆα(θ)∥2
θ 2 θ k θ k 2
and E∥∇ sˆα(θ)−∇ ℓˆα(θ)∥2. We see, by a similar argument to the variance bound, that
θ k θ 2
E(cid:13) (cid:13)∇ θsˆ1 k(θ)−∇ θsˆα k(θ)(cid:13) (cid:13)2
2
(cid:13) (cid:12)    (cid:12)
N (cid:13) (cid:32)(cid:12) J J (cid:12)
≤ (cid:88) E(cid:13) (cid:13) (cid:13)∇ θ (cid:12) (cid:12) (cid:12)log(cid:88) w nF ,, jθ,1,k −log(cid:88) w nF ,, jθ,α,k (cid:12) (cid:12) (cid:12)+
n=1 (cid:13) (cid:12) j=1 j=1 (cid:12)
(cid:12)
(cid:12)

J
 
J
(cid:12) (cid:12)(cid:33)(cid:13) (cid:13)2
(cid:12) (cid:12)log(cid:88) wA,F,θ,1,k +log(cid:88) wA,F,θ,α,k (cid:12) (cid:12) (cid:13) (cid:13) . (244)
(cid:12) n−1,j n−1,j (cid:12) (cid:13)
(cid:12) j=1 j=1 (cid:12) (cid:13)
2
Each of these terms can be bounded using the derivative of the logarithm and that wF,θ,1,k =
n,j
wF,θ,α,k = 1 when θ = ϕ,
n,j
(cid:13)    (cid:13)2
(cid:13) J J (cid:13)
(cid:13) (cid:13) (cid:13)∇ θlog(cid:88) w nF ,, jθ,1,k −∇ θlog(cid:88) w nF ,, jθ,α,k (cid:13) (cid:13)
(cid:13)
(245)
(cid:13) j=1 j=1 (cid:13)
2
J n
≤
1 (cid:88) (cid:88) (1−α(n−i))(cid:13)
(cid:13)∇
log(cid:16) gA,F,θ(cid:17)(cid:13) (cid:13)2
(246)
J (cid:13) θ i,j (cid:13) 2
j=1i=n−k
J n
1 (cid:88) (cid:88)
≤ (1−α(n−i))pG′(θ)2 (247)
J
j=1i=n−k
(cid:18) α(1−αk)(cid:19)
≤ pG′(θ)2 k− , (248)
1−α
where the second-last line follows from Assumption (A3). So,
E(cid:13) (cid:13)∇ θsˆ1 k(θ)−∇ θsˆα k(θ)(cid:13) (cid:13)
∞
≤
2NpG′(θ)2(cid:18)
k− α( 11 −−
ααk)(cid:19)
≤ 2NpG′(θ)2k. (249)
Now, we address the third term of Equation 230, E∥∇ sˆα(θ) − ∇ ℓˆα(θ)∥ . From the variance
θ k θ ∞
argument, we know that we can bound ∥∇ ℓˆα(θ)−∇ sˆα(θ)∥:
θ θ k
(cid:13) (cid:13)2
(cid:13)∇ ℓˆα(θ)−∇ sˆα(θ)(cid:13) (250)
(cid:13) θ θ k (cid:13)
2
(cid:13)  (cid:18) (cid:19)α(n−i)   (cid:18) (cid:19)α(n−i)(cid:13)2
(cid:13)
(cid:13)
(cid:80)J (cid:81)n g iA ,j,F,θ (cid:80)J (cid:81)n g iA ,j,F,θ (cid:13)
(cid:13)
(cid:13)(cid:88)N  j=1 i=1 gA,F,ϕ  (cid:88)N  j=1 i=n−k gA,F,ϕ (cid:13)
= (cid:13) ∇ log i,j − ∇ log i,j (cid:13) (251)
(cid:13) θ  (cid:18) (cid:19)α(n−i) θ  (cid:18) (cid:19)α(n−i)(cid:13)
(cid:13) (cid:13)n=1 (cid:80)J (cid:81)n−1 g iA ,j,F,θ  n=1 (cid:80)J (cid:81)n−1 g iA ,j,F,θ (cid:13) (cid:13)
(cid:13) j=1 i=1 gA,F,ϕ j=1 i=n−k gA,F,ϕ (cid:13)
i,j i,j 2
(cid:13)     (cid:13)2
N (cid:13) J J (cid:13)
≤ (cid:88)(cid:13) (cid:13) (cid:13)∇ θlog(cid:88) w nF ,, jθ,α −log(cid:88) w nF ,, jθ,α,k (cid:13) (cid:13)
(cid:13)
n=1(cid:13) j=1 j=1 (cid:13)
2
(cid:13)     (cid:13)2
N (cid:13) J J (cid:13)
+(cid:88)(cid:13) (cid:13) (cid:13)∇ θlog(cid:88) w nA −,F 1, ,θ j,α +log(cid:88) w nA −,F 1, ,θ j,α,k (cid:13) (cid:13)
(cid:13)
. (252)
n=1(cid:13) j=1 j=1 (cid:13)
2
50Using the derivative of the logarithm and that wF,θ,α = wF,θ,α,k = 1 when θ = ϕ,
n,j n,j
(cid:13)    (cid:13)2
(cid:13) J J (cid:13)
(cid:13) (cid:13) (cid:13)∇ θlog(cid:88) w nF ,, jθ,α −∇ θlog(cid:88) w nF ,, jθ,α,k (cid:13) (cid:13)
(cid:13)
(253)
(cid:13) j=1 j=1 (cid:13)
2
J n−k
≤
1 (cid:88)(cid:88) α(n−i)(cid:13)
(cid:13)∇
log(cid:16) gA,F,θ(cid:17)(cid:13) (cid:13)2
(254)
J (cid:13) θ i,j (cid:13) 2
j=1 i=1
J n−k
1 (cid:88)(cid:88)
≤ α(n−i)pG′(θ)2 (255)
J
j=1 i=1
αk −αn
≤ pG′(θ)2 , (256)
1−α
where the second-last line follows from Assumption (A3). Putting it together and taking expecta-
tions on both sides, we obtain
(cid:13) (cid:13)2 2αk
E(cid:13)∇ ℓˆα(θ)−∇ sˆα(θ)(cid:13) ≤ NpG′(θ)2, (257)
(cid:13) θ θ k (cid:13) 2 1−α
which is our desired error bound. Therefore, our decomposition yields the MSE bound
E(cid:13)
(cid:13)∇ θℓ(θ)−∇
θℓˆα(θ)(cid:13) (cid:13)2
2
(258)
≤ E(cid:13) (cid:13)∇ θℓ(θ)−∇ θsˆ1 k(θ)(cid:13) (cid:13)2 2+E(cid:13) (cid:13)∇ θsˆ1 k(θ)−∇ θsˆα k(θ)(cid:13) (cid:13)2 2+E(cid:13) (cid:13)∇ θsˆα k(θ)−∇ θℓˆα(θ)(cid:13) (cid:13)2
2
(259)
(cid:18) CpG′(θ)2k2
≲ min N +NpG′(θ)2(1−ϵ)⌊k/(clog(J))⌋
k≤N J
(cid:18) α(1−αk)(cid:19) αk (cid:19)
+2NpG′(θ)2 k− +2 NpG′(θ)2 (260)
1−α 1−α
(cid:18) k2 αk +αk+1−α(cid:19)
≲ minNpG′(θ)2 +(1−ϵ)⌊k/(clog(J))⌋+k+ . (261)
k≤N J 1−α
Corollary 1 (MSE of MOP-0). The MSE of MOP-0, i.e. the estimator of [24], is
(cid:18) (cid:19)
E(cid:13) (cid:13)∇ θℓ(θ)−∇ θℓˆ0(θ)(cid:13) (cid:13)2
2
≲ NpG′(θ)2 J1 +(1−ϵ)⌊1/(clog(J))⌋ . (262)
Proof. Observe that MOP-0 is equivalent to MOP-(1,1). Noting that k = 1 and that we only
need to bound the difference between ∇ ℓ(θ) and ∇ sˆ1(θ), repeating the above arguments almost
θ θ 1
verbatim allows us to obtain this result.
As before, we bound the first term, (cid:80)N ∥∇ ℓ (θ)−∇ sˆ1 (θ)∥2, by decomposing it into two
n=1 θ n θ n,1 2
terms,
N
(cid:88)(cid:13) (cid:13)∇ θℓ n(θ)−∇ θsˆ1 n,1(θ)(cid:13) (cid:13)2
2
≤
n=1
N N
(cid:88)(cid:13) (cid:13)∇ θℓ n(θ)−∇ θsˆ1 n,1|πˆn−1=πn−1(θ)(cid:13) (cid:13)2 2+(cid:88)(cid:13) (cid:13)∇ θsˆ1 n,1|πˆn−1=πn−1(θ)−∇ θsˆ1 n,1(θ)(cid:13) (cid:13)2 2. (263)
n=1 n=1
51The first term is a particle approximation dependent on 1 timesteps, so by Lemma 2 of [42], this is
bounded by
E(cid:13) (cid:13)∇ θℓ n(θ)−∇ θsˆ1 n,1|πˆn−1=πn−1(θ)(cid:13) (cid:13)2
2
≤ CpG J′(θ)2 . (264)
The second term amounts to bounding the difference between functionals of two different parti-
clemeasuresthatmixunderthesameMarkovkernel. Here,weuseAssumptions(A2)and(A3)that
ensure strong mixing. We know from Theorem 3 of [42]that when M is thek-step Markov op-
n,n+k
erator from timestep n and β (M) = sup ∥M(x,·)−M(y,·)∥ = sup
∥µM−νM∥TV
TV x,y∈E TV µ,ν∈P,µ̸=ν ∥µ−ν∥TV
is the Dobrushin contraction coefficient of a Markov operator,
β (M ) ≤ (1−ϵ)⌊k/(clog(J))⌋, (265)
TV n,n+k
i.e. the mixing time of the particle filter is O(log(J)), where ϵ and c depend on M¯,M,G¯,G in (A2)
and (A3).
Then, we can bound E∥∇ sˆ1 (θ)−∇ sˆ1 (θ)∥2 by
θ n,1|πˆn−1=πn−1 θ n,1 2
∥µM −νM ∥
sup n,n+1 n,n+1 TV = β (M ) ≤ (1−ϵ)⌊1/(clog(J))⌋, (266)
TV n,n+1
∥µ−ν∥
µ,ν∈P,µ̸=ν TV
implying that
E(cid:13) (cid:13)∇ θsˆ1 n,1|πˆn−1=πn−1(θ)−∇ θsˆ1 n,1(θ)(cid:13) (cid:13)2
2
(267)
≲ sup sup pG′(θ)2(cid:12) (cid:12)(µM n,n+1)(ψ)−(νM n,n+1)(ψ)(cid:12) (cid:12) (268)
µ,ν ∥ψ∥∞≤1/2
≤
suppG′(θ)2(cid:13)
(cid:13)µM n,n+1−νM
n,n+1(cid:13)
(cid:13)
TV
(269)
µ,ν
≤ pG′(θ)2(1−ϵ)⌊1/(clog(J))⌋∥πˆ −π ∥ (270)
n−1 n−1 TV
≤ pG′(θ)2(1−ϵ)⌊1/(clog(J))⌋ sup (cid:12) (cid:12)πˆ n−1(ψ)−π n−1(ψ)(cid:12) (cid:12) (271)
∥ψ∥∞≤1/2
≲ pG′(θ)2(1−ϵ)⌊1/(clog(J))⌋. (272)
Therefore, we have that
E(cid:13) (cid:13)∇ θℓ(θ)−∇ θsˆ1 1(θ)(cid:13) (cid:13)2
2
(273)
N N
≤ (cid:88) E(cid:13) (cid:13)∇ θℓ n(θ)−∇ θsˆ1 n,1|πˆn−1=πn−1(θ)(cid:13) (cid:13)2 2+(cid:88) E(cid:13) (cid:13)∇ θsˆ1 n,1|πˆn−1=πn−1(θ)−∇ θsˆ1 n,1(θ)(cid:13) (cid:13)2
2
(274)
n=1 n=1
CpG′(θ)2
≲ N +NpG′(θ)2(1−ϵ)⌊1/(clog(J))⌋. (275)
J
52G Figures for Bayesian Inference
Figure 7: Convergence diagnostics for the Metropolis-Hastings variant particle MCMC with the
random walk proposal and an informative empirical prior from IF2. Here, we display the results
for the trend parameter in the Dhaka cholera model of [18].
Figure 8: Convergence diagnostics for a No-U-Turn Sampler (NUTS) with uniform priors on a
compact set. Again, we display the results for the trend parameter in the Dhaka cholera model of
[18]. The NUTS sampler explores more of the posterior than the Metropolis-Hastings sampler, but
fails to converge quickly.
53Figure 9: Posterior estimates from NUTS powered by MOP-α with the informative nonparametric
empirical prior obtained from a kernel density estimator on the IF2 parameter cloud. The posterior
estimates from each chain are largely in agreement.
54