Can Watermarks Survive Translation?
On the Cross-lingual Consistency of Text Watermark
for Large Language Models
ZhiweiHe1* BinglinZhou1∗ HongkunHao1 AiweiLiu3
XingWang2 ZhaopengTu2 ZhuoshengZhang1 RuiWang1†
1ShanghaiJiaoTongUniversity 2TencentAILab 3TsinghuaUniversity
1
{zwhe.cs,zhoubinglin,zhangzs,wangrui12}@sjtu.edu.cn
2 3
{brightxwang,zptu}@tencent.com liuaw20@mails.tsinghua.edu.cn
Abstract Prompt Response (En)
The powerful the spread of fake
T
i
mde oex
n
dt etw
i lf
sya (t Le cr Lom
n
Ma ter sk
n
)i
t
tn opg prot re edc vuh ecn neo
td
mlo
b
ig syy usla eai .m
rg
Is
e
nt tlo
ha
int sa gg
su
tua
a
dn
g
yd
e
,
g
c
l
pe
a
a
on
p
n
se
a
g
er
b
u
a
i
a
rt
l
g
ii
i
e
sv
t
ke
i
m
s
e
o
,s
d e
so
l
uf
s
c h as
n
f
a
re
o
c
ew
r
a
gs
d
a
c
e
ra
h
m
dWn
e
i
ld
a
c eat
t
set
i
w
srh
n
r
me
g
i
oa
t
frkm
i
i
i
n
n
ts
g
hu
,
es e
weintroducetheconceptof“cross-lingualcon- language of the
content.
sistency”intextwatermarking,whichassesses
theabilityoftextwatermarkstomaintaintheir LLMs Translation System
effectivenessafterbeingtranslatedintoother
languages. Preliminaryempiricalresultsfrom
Response(Zh)
two LLMs and three watermarking methods
revealthatcurrenttextwatermarkingtechnolo- AW la gt oe rr im ta hr mk
s 虚假新闻的传播m
a和rk
在学术
gieslackconsistencywhentextsaretranslated 写作中的W作at弊er
⾏为，⽆论
intovariouslanguages. Basedonthisobserva- Watermark
内容是什么语⾔。
tion,weproposeaCross-lingualWatermarkRe-
movalAttack(CWRA)tobypasswatermarking
byfirstobtainingaresponsefromanLLMin Strong Watermark Strength Weak
apivotlanguage,whichisthentranslatedinto
Figure1: Illustrationofwatermarkdilutioninacross-
thetargetlanguage. CWRAcaneffectivelyre-
lingualenvironment. Bestviewedincolor.
movewatermarksbyreducingtheAreaUnder
theCurve(AUC)from0.95to0.67withoutper-
formanceloss. Furthermore, weanalyzetwo
embedsamessagewithinLLM-generatedcontent
keyfactorsthatcontributetothecross-lingual
consistencyintextwatermarkingandpropose thatisimperceptibletohumanreaders,butcanbe
adefensemethodthatincreasestheAUCfrom detectedalgorithmically. Bytrackinganddetecting
0.67to0.88underCWRA. text watermarks, it becomes possible to mitigate
the abuse of LLMs by tracing the origin of texts
1 Introduction
andascertainingtheirauthenticity.
Largelanguagemodels(LLMs)likeGPT-4(Ope- Therobustnessofwatermarkingalgorithms,i.e.,
nAI,2023)havedemonstratedremarkablecontent the ability to detect watermarked text even after
generation capabilities, producing texts that are it has been modified, is important. Recent works
hardtodistinguishfromhuman-writtenones. This haveshownstrongrobustnessundertextrewriting
progresshasledtoconcernsregardingthemisuse and copy-paste attacks (Liu et al., 2023b; Yang
ofLLMs,suchastherisksofgeneratingmislead- et al., 2023). However, these watermarking tech-
ing information, impersonating individuals, and niqueshavebeentestedsolelywithinmonolingual
compromisingacademicintegrity(ChenandShu, contexts. Inpracticalscenarios,watermarkedtexts
2023a,b). Asacountermeasure,textwatermarking might be translated, raising questions about the
technologyforLLMshasbeendeveloped,aiming efficacyoftextwatermarksacrosslanguages(see
at tagging and identifying the content produced Figure 1). For example, a malicious user could
by LLMs (Kirchenbauer et al., 2023a; Liu et al., useawatermarkedLLMtoproducefakenewsin
2023b,c). Generally,atextwatermarkingalgorithm English and then translate it into Chinese. Obvi-
ously, the deceptive impact persists regardless of
* EqualContribution.WorkdoneduringZhiwei’sintern-
thelanguage,butitisuncertainwhetherthewater-
shipatTencentAILab.
† CorrespondencetoRuiWang. mark would still be detectable after such a trans-
1
4202
beF
12
]LC.sc[
1v70041.2042:viXralation. Toexplorethisquestion,weintroducethe 2 Background
conceptofcross-lingualconsistencyintextwater-
2.1 LanguageModel
marking,aimingtocharacterizetheabilityoftext
watermarks to preserve their strength across lan- A language model (LM) M has a defined set of
guages. Our preliminary results on 2 LLMs × 3 tokens known as its vocabulary V. Given a se-
watermarksrevealthatcurrenttextwatermarking quence of tokens x1:n = (x1,x2,...,xn), which
technologieslackconsistencyacrosslanguages. werefertoastheprompt,themodelM computes
In light of this finding, we propose the Cross- theconditionalprobabilityofthenexttokenover
lingual Watermark Removal Attack (CWRA) to V as P M(xn+1|x1:n). Therefore, text generation
highlightthepracticalimplicationsarisingfromde- canbeachievedthroughanautoregressivedecod-
ficientcross-lingualconsistency. Whenperforming ing process, where M sequentially predicts one
CWRA,theattackerbeginsbytranslatingtheorigi- tokenatatime,formingaresponse. SuchanLM
nallanguagepromptintoapivotlanguage,whichis canbeparameterizedbyaneuralnetwork,suchas
fedtotheLLMtogeneratearesponseinthepivot Transformer(Vaswanietal.,2017),whichiscalled
language. Finally,theresponseistranslatedback neuralLM.Typically,aneuralLMcomputesavec-
intotheoriginallanguage. Inthisway,theattacker toroflogitszn+1 = M(x1:n) ∈ R|V| forthenext
obtainstheresponseintheoriginallanguageand tokenbasedonthecurrentsequencex 1:n viaaneu-
bypasses the watermark with the second transla- ral network. The probability of the next token is
tionstep. CWRAoutperformsre-writingattacks, thenobtainedbyapplyingthesoftmaxfunctionto
suchasre-translationandparaphrasing(Liuetal., theselogits: P M(xn+1|x1:n) = softmax(zn+1).
2023c),asitachievesthelowestAUC(reducingit
2.2 WatermarkingforLMs
from0.95to0.67)andthehighesttextquality.
Inthiswork,weconsiderthefollowingwatermark-
ToresistCWRA,weproposeadefensemethod
ingmethods. Allofthemembedthewatermarkby
thatimprovesthecross-lingualconsistencyofcur-
modifyinglogitsduringtextgenerationanddetect
rentLLMwatermarking. Ourmethodisbasedon
thepresenceofthewatermarkforanygiventext.
twocriticalfactors. Thefirstisthecross-lingual
KGW (Kirchenbauer et al., 2023a) sets the
semantic clustering of the vocabulary. Instead
groundworkforLMwatermarking. Ironingawa-
of treating each token in the vocabulary as the
termarkisdelineatedasthefollowingsteps:
smallest unit when ironing watermarks, as done
(1) computeahashofx1:n: hn+1 = H(x1:n),
byKGW(Kirchenbaueretal.,2023a),ourmethod
(2) seed a random number generator with hn+1
considersaclusteroftokensthatsharethesamese-
and randomly partitions V into two disjoint
manticsacrossdifferentlanguagesasthesmallest
lists: thegreenlistV andthered listV ,
unitofprocessing. Inthisway,thepost-translated g r
(3) adjust the logits zn+1 by adding a constant
tokenwillstillcarrythewatermarkasitwouldfall
biasδ (δ > 0)fortokensinthegreenlist:
inthesameclusterasbeforetranslation. Thesec-
ondiscross-lingualsemanticrobustvocabulary
∀i ∈ {1,2,...,|V|},
partition. InspiredbyLiuetal.(2023b),weensure
(cid:40)
zn+1+δ, ifv ∈ V , (1)
thatthepartitionofthevocabularyaresimilarfor z˜n+1 = i i g
semanticallysimilarcontextsindifferentlanguages. i zn+1, ifv ∈ V .
i i r
Despite its limitations, our approach elevates the
As a result, watermarked text will statistically
AUCfrom0.67to0.88undertheCWRA,paving
containmoregreentokens,anattributeunlikelyto
thewayforfutureresearch.
occurinhuman-writtentext. Whendetecting,one
Ourcontributionsaresummarizedasfollows:
canapplystep(1)and(2),andcalculatethez-score
• We reveal the deficiency of current text wa-
asthewatermarkstrengthofx:
termarking technologies in maintaining cross-
lingualconsistency(§3). s = (|x| −γ|V|)/(cid:112) |V|γ(1−γ), (2)
g
• Based on this finding, we propose CWRA that
successfully bypasses watermarks without de- where|x| g isthenumberofgreentokensinxand
gradingthetextquality(§4). γ =
|Vg|
. The presence of the watermark can be
|V|
• Basedonouranalysisofthetwokeyfactorsfor determinedbycomparingswithathreshold.
improvingcross-lingualconsistency,wepropose Unbiasedwatermark(UW)viewstheprocess
adefensemethodagainstCWRA(§5). of adjusting the logits as applying a ∆ function:
2z˜n+1 = zn+1+∆,anddesignsa∆functionthat PearsonCorrelationCoefficient(PCC) Weuse
satisfies: PCCtoassesslinearcorrelationbetweenS andSˆ:
(cid:104) (cid:105)
E P˜
M
= P M, (3) cov(S,Sˆ)
PCC(S,Sˆ) = , (5)
whereP˜
M
istheprobabilitydistributionofthenext σ Sσ Sˆ
tokenafterlogitsadjustment(Huetal.,2023). wherecov(S,Sˆ)isthecovarianceandσ andσ
S Sˆ
Semanticinvariantrobustwatermark(SIR)
arethestandarddeviations. APCCvaluecloseto
showstherobustnessunderre-translationandpara-
1suggestsconsistenttrendsinwatermarkstrengths
phrasing attack (Liu et al., 2023b). Its core idea
acrosslanguages.
istoassignsimilar∆forsemanticallysimilarpre-
fixes. Givenprefixsequencesxandy,SIRadopts Relative Error (RE) Unlike PCC, which cap-
an embedding model E to characterize their se- turesconsistencyintrends,REisusedtoassessthe
mantic similarity and trains a watermark model
magnitudeofdeviationbetweenS andSˆ:
thatyields∆withthemainobjective: (cid:34) |Sˆ−S|(cid:35)
RE(S,Sˆ) = E ×100%. (6)
L = |Sim(E(x),E(y))−Sim(∆(x),∆(y))|, |S|
(4)
AlowerREindicatesthatthewatermarkretains
where Sim(·,·) denotes similarity function. Fur-
strengthclosetoitsoriginalvalueaftertranslation,
thermore,∀i ∈ {1,2,...,|V|},∆ istrainedtobe
i
signifyinggreatcross-lingualconsistency. Toavoid
closeto+1or−1. Therefore,SIRcanbeseenas
theinstabilitycausedbyvaluesofS thatareclose
an improvement based on KGW, where ∆ > 0
i
to0,wefirstaggregatethedatabytextlengthand
indicating that v is a green token. The original
i replace the original values of S and Sˆ with their
implementationofSIRusesC-BERT(Chanchani
respective mean values within each group. We
andHuang,2023)astheembeddingmodel,which
also apply min-max normalization to ensure that
isEnglish-only. ToadoptSIRinthecross-lingual
allvaluesarenon-negative.
scenario,weuseamultilingualS-BERT(Reimers
andGurevych,2019)1instead.
3.2 ExperimentalSetup
3 Cross-lingualConsistencyofText Setup We sampled a subset of 1,000 prompts
from the UltraChat test set (Ding et al., 2023)2,
Watermark
andgeneratedresponsesfromtheLLMusingthe
In this section, we define the concept of cross- text watermarking methods described in § 2.2.
lingual consistency in text watermarking and an- The default decoding method was multinomial
swerthreeresearchquestions(RQ): sampling, and both the prompts and the LLM-
• RQ1: Towhatextentarecurrentwatermarking generatedresponseswereinEnglish. Toevaluate
algorithmsconsistentacrossdifferentlanguages? the cross-lingual consistency, these watermarked
• RQ2: Dowatermarksexhibitbetterconsistency responsesweretranslatedintofourlanguagesusing
betweensimilarlanguagesthanbetweendistant gpt-3.5-turbo-06133: Chinese (Zh), Japanese
languages? (Ja),French(Fr),andGerman(De). Notably,En-
glish shares greater similarities with French and
• RQ3: Doessemanticinvariantwatermark(SIR)
German, in contrast to its significant differences
exhibitbettercross-lingualconsistencythanoth-
fromChineseandJapanese.
ers(KGWandUW)?
Models FortheLLMs,weadopt:
3.1 Definition
• BAICHUAN-7B (Baichuan., 2023): an LLM
Wedefinecross-lingualconsistencyastheability
trainedon1.2trilliontokens. Itoffersbilingual
of a watermark, embedded in a text produced by
supportforbothChineseandEnglish.
anLLM,toretainitsstrengthafterthetextistrans-
latedintoanotherlanguage. Werepresenttheorigi- • LLAMA-2-7B-CHAT (Touvron et al., 2023):
nalstrengthofthewatermarkasarandomvariable, trained on 2 trillion tokens and only provides
denotedbyS (AppendixA.1),anditsstrengthaf- supportforEnglish.
ter translation as Sˆ. To quantitatively assess this
consistency,weemploythefollowingtwometrics.
2https://huggingface.co/datasets/
HuggingFaceH4/ultrachat_200k
1paraphrase-multilingual-mpnet-base-v2 3https://platform.openai.com/docs/models
3PCC↑ RE(%)↓
Method
En→Zh En→Ja En→Fr En→De Avg. En→Zh En→Ja En→Fr En→De Avg.
BAICHUAN-7B
KGW 0.108 -0.257 0.059 0.144 0.013 75.62 88.50 76.37 73.65 78.54
UW 0.190 0.087 0.166 0.183 0.156 97.57 98.82 97.22 97.89 97.88
SIR 0.283 0.380 0.348 0.234 0.311 84.16 68.28 76.07 93.41 80.41
LLAMA-2-7B-CHAT
KGW 0.056 0.177 0.276 0.080 0.147 85.57 79.55 86.58 92.54 86.06
UW 0.076 0.092 0.116 0.109 0.098 92.85 95.40 95.32 96.14 94.93
SIR -0.106 -0.159 0.146 0.323 0.051 69.52 92.80 59.76 68.57 72.48
Table1: Comparisonofcross-lingualconsistencybetweendifferenttextwatermarkingmethods(KGW,UW,and
SIR).Boldentriesdenotethebestresultamongthethreemethods.
En (Original language) En→Zh En→Ja En→Fr En→De
12 90 0.4
10 72 0.3
7 54 0.2
5
36 0.2
2
0 18 0.1
-2 0 0.0
25 275 525 25 275 525 25 275 525
Text Length Text Length Text Length
(a) KGW (b) UW (c) SIR
Figure2: Trendsofwatermarkstrengthswithtextlengthbeforeandaftertranslation. Thisistheaverageresultof
BAICHUAN-7BandLLAMA-2-7B-CHAT. Figure7displaysresultsforeachmodel. Giventhedistinctcalculations
forwatermarkstrengthsofthethreemethods,they-axisscalesvaryaccordingly.
3.3 Results ResultsforRQ3 Overall,SIRindeedexhibitssu-
periorcross-lingualconsistencycomparedtoKGW
Table1presentstheresults,andFigure2illustrates
andUW.Particularlywhenusing BAICHUAN-7B,
thetrendofwatermarkstrengthswithtextlength.
SIRachievesthebestPCCsforalltargetlanguages.
When using LLAMA-2-7B-CHAT, SIR still per-
ResultsforRQ1 Werevealanotabledeficiency
forms well in languages similar to English. This
in the cross-lingual consistency of current water-
finding highlights the importance of semantic in-
marking methods. Among all the settings, the
varianceinpreservingwatermarkstrengthacross
PCCsaregenerallylessthan0.3,andtheREsare
languages,whichwewillexploremorein§5. De-
predominantlyabove80%. Furthermore,Figure2
spite its superiority, SIR still presents a notable
visuallydemonstratesthatthewatermarkstrengths
reduction in watermark strength in cross-lingual
ofthethreemethodsexhibitasignificantdecrease
scenarios,asevidencedbyFigure2e.
after translation. These results suggest that cur-
rentwatermarkingalgorithmsstruggletomaintain
4 Cross-lingualWatermarkRemoval
effectivenessacrosslanguagetranslations.
Attack
ResultsforRQ2 OnlySIRexhibitssuchachar- In the previous section, we focus on scenarios
acteristic: when using LLAMA-2-7B-CHAT, its wheretheresponseofLLMistranslatedintoother
cross-lingualconsistencyperformsnotablybetter languages. However,anattackertypicallyexpects
amongsimilarlanguagescomparedtodistantones, a response from the LLM in the same language
especiallyintermsofPCC.Thiscanbeattributed as the prompt while removing watermarks. To
toitssemanticinvarianceandsharedwordsamong bridgethisgap,weintroducetheCross-lingualWa-
similarlanguages. However,thischaracteristicis termarkRemovalAttack(CWRA)inthissection,
notshownonBAICHUAN-7B,whichmightbere- constitutingacompleteattackprocessandposing
latedtotokenization. Incontrast, thispropertyis amoresignificantchallengetotextwatermarking
lessevidentinthecaseofKGWandUW. thanparaphrasingandre-translationattacks.
4
htgnertS
kramretaW
)2
.qE(
htgnertS
kramretaW
)61
.qE(
htgnertS
kramretaW
)71
.qE(Original Prompt (En) LLMs
Translation System
The powerful generative
capabilities of language
Response (En)
models pose risks, such as
the spread of fake news
Translation System Watermark Algorithms a cn hd
e
at th ie
n
gm Wi is nau tes are cm aaf dro ekr
m ic
writing, regardless of the
Pivot Prompt (Zh) Pivot Response (Zh)
language of the content.
语⾔模型强⼤的⽣成能⼒带来了⻛ 虚假新闻的传 W播
at和er在ma学rk
术写作中的
险，例如 作弊⾏为，⽆论内容是什么语⾔。 Weak Watermark Strength Strong
Figure3: AnexamplepipelineofCWRAwithEnglish(En)astheoriginallanguageandChinese(Zh)asthepivot
language. WhenperformingCWRA,theattackernotonlywantstoremovethewatermark,butalsogetsaresponse
intheoriginallanguagewithhighquality. ItscoreideaistowrapthequerytotheLLMintothepivotlanguage.
Figure3showstheprocessofCWRA.Instead CWRA vs Other Attack Methods CWRA
offeedingtheoriginalpromptintotheLLM,theat- demonstratesthemosteffectiveattackperformance,
tackerinitiatestheattackbytranslatingtheprompt significantly diminishing the AUC and the TPR.
intoapivotlanguagenamedthepivotprompt. The For one thing, existing watermarking techniques
LLMreceivesthepivotpromptandprovidesawa- are not designed for cross-lingual contexts, lead-
termarked response in the pivot language. The ing to weak cross-lingual consistency. For an-
attackerthentranslatesthepivotresponsebackinto other thing, strategies such as Re-translation and
the original language. This approach allows the Paraphraseareessentiallysemantic-preservingtext
attackertoobtaintheresponseintheoriginallan- rewriting. Such strategies tend to preserve some
guage. Due to the inherent challenges in main- n-grams from the original response, which may
taining cross-lingual consistency, the watermark stillbeidentifiablebythewatermarkdetectional-
wouldbeeffectivelyeliminatedduringthesecond gorithm. Incontrast,CWRAreducessuchn-grams
translationstep. duetolanguageswitching.
4.1 Setup SIRvsOtherWatermarkingMethods Under
theCWRA,SIRexhibitssuperiorrobustnesscom-
To assess the practicality of attack methods, we
paredtootherwatermarkingmethods. TheAUCs
consider two downstream tasks: text summariza-
forKGWandUWunderCWRAplummetto0.61
tion and question answering. We adopt Multi-
and 0.54, respectively, approaching the level of
News (Fabbri et al., 2019) and ELI5 (Fan et al.,
random guessing. In stark contrast, the AUC for
2019)as testsets, respectively. Bothdatasets are
theSIRmethodstandssignificantlyhigherat0.67,
inEnglishandrequirelongtextoutputwithanav-
aligning with our earlier observations regarding
erage output length of 198 tokens. We selected
cross-lingualconsistencyintheRQ3of§3.3.
500 samples for each test set that do not exceed
themaximumcontextlengthofthemodelandper- TextQuality AsshowninTable2,theseattack
formed zero-shot prompting on BAICHUAN-7B. methods not only preserve text quality, but also
For CWRA, we select Chinese as the pivot lan- bringslightimprovementsinmostcasesduetothe
guageandcomparethefollowingtwomethods: goodtranslatorandparaphraser. Amongthecom-
• Paraphrase: rephrasingtheresponseintodiffer- paredmethods,CWRAstandsoutforitssuperior
entwordingwhileretainingthesamemeaning. performance. Considering that the same transla-
• Re-translation: translatingtheresponseintothe torandparaphraserwereusedacrossallmethods,
pivotlanguageandbacktotheoriginallanguage. wespeculatethatthisisbecausethe BAICHUAN-
The paraphraser and translator used in all attack 7B modelusedinourexperimentsperformseven
methodsaregpt-3.5-turbo-0613toensurecon- betterinthepivotlanguage(Chinese)thaninthe
sistencyacrossthedifferentattackmethods. originallanguage(English). Thisfindingimplies
thatapotentialattackercouldstrategicallychoose
4.2 Results
apivotlanguageatwhichtheLLMexcelstoper-
Figure4exhibitsROCcurvesofthreewatermark- formCWRA,therebyachievingthebesttextqual-
ingmethodsunderdifferentattackmethods. itywhileremovingthewatermark.
5No attack Re-translation Paraphrase CWRA (Ours) Random
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
TPR TPR TPR
0.4 0.992 0.4 0.913 0.4 0.940
0.776 0.263 0.825
0.2 0.594 0.2 0.238 0.2 0.682
0.213 0.166 0.230
0.0 0.0 0.0
0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate (FPR) False Positive Rate (FPR) False Positive Rate (FPR)
(a) KGW (b) UW (c) SIR
Figure4:ROCcurvesforKGW,UW,andSIRundervariousattackmethods:Re-translation,ParaphraseandCWRA.
WealsopresentTPRvaluesatafixedFPRof0.1. Thisistheoverallresultoftextsummarizationandquestion
answering. Figure8andFigure9displayresultsforeachtask.
WM KGW UW SIR
Attack
ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L
TextSummarization
Noattack 14.24 2.68 12.99 13.65 1.68 12.38 13.34 1.79 12.43
Re-translation 14.11 2.43 12.89 13.89 1.77 12.63 13.63 1.98 12.61
Paraphrase 15.10 2.49 13.69 14.72 1.95 13.31 15.56 2.11 14.14
CWRA(Ours) 18.98 3.63 17.33 15.88 2.31 14.25 17.38 2.67 15.79
QuestionAnswering
Noattack 19.00 2.18 16.09 11.70 0.49 9.57 16.95 1.35 14.91
Re-translation 18.62 2.32 16.39 12.98 1.30 11.16 16.90 1.80 15.12
Paraphrase 18.45 2.24 16.47 14.38 1.37 13.07 17.17 1.79 15.54
CWRA(Ours) 18.23 2.56 16.27 15.20 1.88 13.45 17.47 2.22 15.53
Table2: Comparativeanalysisoftextqualityimpactedbydifferentwatermarkremovalattacks.
5 ImprovingCross-lingualConsistency 鸟], where “movies” and “电影” are semanti-
callyequivalent,asare“birds”and“鸟”.
Uptothispoint,wehaveobservedthechallenges
2. Amachinetranslator(MT)thentranslatesthe
associatedwithtextwatermarkingincross-lingual
entire sentence “I watch movies” into Chi-
scenarios. In this section, we first analyze two
nese: “我看电影”.
key factors essential for achieving cross-lingual
consistency. Basedonouranalysis,weproposea Thequestionofinterestis: whatconditionsmust
defensemethodagainstCWRA. thevocabularypartitionmeetssothatthetoken“电
影”,theChineseequivalentof“movies”,alsofalls
5.1 TwoKeyFactorsofCross-lingual
withinthegreenlist?
Consistency
Figure5(a)illustratesasuccessfulcase,where
KGW-basedwatermarkingmethodsfundamentally
twokeyfactorsexists:
dependonthepartitionofthevocabulary,i.e.,the
redandgreenlists,asdiscussedin§2.2. Therefore,
1. Cross-lingualsemanticclusteringofthevo-
cross-lingual consistency aims to ensure that the
cabulary: semanticallysimilartokensmustbe
greentokensinthewatermarkedtextwillstillbe
inthesamepartition,eithergreenorredlists.
recognized as green tokens after being translated
2. Cross-lingual semantic robust vocabulary
intootherlanguages.
partition: forsemanticallysimilarprefixesin
Withthisgoalinmind,westartouranalysiswith different languages: “I watch” and “我 看”,
asimpleEnglish-ChinesecaseinFigure5:
thepartitionsofthevocabularyarethesame.
1. Given“I watch”astheprefix,awatermarked
LMpredicts“movies”asthenexttoken. Due BothFigure5(b)andFigure5(c)satisfyonlyoneof
to watermarking, “movies” is a green token. thetwofactors,thusfailingtorecognize“电影”as
Allcandidatetokensare[movies,birds,电影, agreentokenandlosingcross-lingualconsistency.
6
)RPT(
etaR
evitisoP
eurT
)RPT(
etaR
evitisoP
eurT
)RPT(
etaR
evitisoP
eurTGreen List token before translation I=我 watch=看 movies=电影 birds=⻦
Red List token after translation
Dictionary
English Vocab partition based on I watch movies 电影 birds ⻦
Prefix English prefix
Chinese Vocab partition based on
我 看 movies 电影 birds ⻦
Prefix Chinese prefix Legend
(b) Factor 1 ✔ | Factor 2 ✘
I watch movies 电影 birds ⻦ I watch movies ⻦ birds 电影
我 看 movies 电影 birds ⻦ 我 看 movies ⻦ birds 电影
(a) Factor 1 ✔ | Factor 2 ✔ (c) Factor 1 ✘ | Factor 2 ✔
Figure5: CasesinEnglish-Chinesetomaintaincross-lingualconsistency: onlywhenboththecircle(⃝)andthe
triangle(△)symbolsareinthegreenlistcancross-lingualconsistencybeachieved. Factor1: semanticallysimilar
tokens should be in the same list (either red or green). In these cases, “movies” and “电影” are semantically
equivalent,asare“birds”and“鸟”. Factor2: thevocabularypartitionsforsemanticallysimilarprefixesshouldbe
thesame. Inthesecases,allprefixesaresemanticallyequivalent.
5.2 DefenseMethodagainstCWRA i.e.,ifv andv aretranslationsofeachother,they
i j
will fall into the same list. To obtain such a se-
WenowimprovetheSIRsothatitsatisfiesthetwo
mantic clustering C, we treat each token in V as
factorsdescribedabove. Asdiscussedin§2.2,SIR
anode,andaddanedge(v ,v )whenever(v ,v )
usesthe∆functiontorepresentvocabularyparti- i j i j
tion(∆ ∈ R|V|), where∆ > 0indicatingthatv corresponds to an entry in a bilingual dictionary.
i i
Therefore, C is all the connected components of
isagreentoken. BasedonEq.4,SIRhasalready
thisgraph.
optimizedforFactor2whenusingamultilingual
embeddingmodel. Forprefixesxandy,thesim- We name this method as X-SIR and evaluated
ilarity of their vocabulary partitions for the next itunderthesamesettingas§4. Wealsodetailits
tokenshouldbeclosetotheirsemanticsimilarity: limitationsin§8.
Sim(∆(x),∆(y)) ≈ Sim(E(x),E(y)), (7)
TextSumm. QuestionAns.
Method
whereE isamultilingualembeddingmodel. PCC↑ RE(%)↓ PCC↑ RE(%)↓
BasedonSIR,wefocusonFactor1,i.e.,cross-
SIR 0.431 66.18 0.321 71.42
lingualsemanticclusteringofthevocabulary. For-
X-SIR 0.554 43.49 0.507 34.98
mally, we define semantic clustering as a parti-
tionC ofthevocabularyV: C = {C 1,C 2,...,C |C|} Table3: Cross-lingualconsistenciesintermsofPCC
, where each cluster C consists of semantically andREunderCWRA.
i
equivalenttokens. Insteadofassigningbiasesfor
each token in V, we adapt the ∆ function so that
ityieldsbiasestoeachclusterinC,i.e.,∆ ∈ R|C|. Cross-lingualconsistency&ROCcurvers Ta-
Thus,theprocessofadjustingthelogitsshouldbe: ble 3 shows the cross-lingual consistency of
SIR and X-SIR when confronted CWRA. X-SIR
∀i ∈ {1,2,...,|V|},
(8) achievessignificantimprovementsintermsofPPC
z˜n+1 = zn+1+∆ ,
i i C(i) andREinbothtasks. Consequently,asdepictedin
Figure 6, X-SIR substantially enhances the AUC
where C(i) indicates the index of v ’s cluster
i
under CWRA, with an increase in TPRs by ~0.4.
within C. By doing so, if token v and v are se-
i j
Furthermore,X-SIRdeliversperformanceonpar
mantically equivalent, they will receive the same
withSIRintheabsenceofanyattacks. Thesefind-
biasonlogits:
ings validate the two key factors of cross-lingual
C(i) = C(j) =⇒ ∆ = ∆ , (9) consistencythatweidentifiedin§5.1.
C(i) C(j)
7X-SIR (No attack) X-SIR (CWRA) semanticinformationofthetext,whichshowsro-
SIR (No attack) SIR (CWRA)
1.0 1.0 bustnesstoparaphraseattacks. Liuetal.(2023a)
0.8 0.8 proposes the first unforgeable publicly verifiable
watermarkingalgorithmforlargelanguagemodels.
0.6 0.6
TPR TPR SemStamp (Houetal.,2023)isanothersemantic-
0.4 0.918 0.4 0.990
0.879 0.984 related watermarking method and it generate wa-
0.2 0.640 0.2 0.820 0.223 0.462 termarked text at sentence granularity instead of
0.0 0.0
token granularity. Tu et al. (2023) introduces
0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate (FPR) False Positive Rate (FPR) WaterBench, the first comprehensive benchmark
forLLMwatermarks. Weintroducethedetailsof
(a) Text summarization (b) Question answering
KGW,UWandSIRin§2.2.
Figure6: ROCcurvesofX-SIRandSIR.
6.2 WatermarkRobustness
Method ROUGE-1 ROUGE-2 ROUGE-L
TextSummarization A good watermarking method should be robust
SIR 13.34 1.79 12.43 to various watermarking removal attacks. How-
X-SIR 15.65 2.04 14.29 ever, current works on watermarking robustness
mainly focus on single-language attacks, such as
QuestionAnswering
paraphrase attacks. For example, Kirchenbauer
SIR 16.95 1.35 14.91
X-SIR 16.77 1.39 14.07 et al. (2023b) evaluates the robustness of KGW
against paraphrase attacks as well as copy-paste
Table4: EffectsofX-SIRandSIRontextquality. attacksandproposesadetecttricktoimprovethe
robustnesstocopy-pasteattacks. Zhaoetal.(2023)
employs a fixed green list to improve the robust-
Text quality As shown in Table 4, X-SIR
nessofKGWagainstparaphraseattacksandedit-
achievesbettertextqualitythanSIRintextsumma-
ing attacks. Chen et al. (2023) proposes a new
rization,andcomparableperformanceonquestion
paraphraserobustwatermarkingmethod“XMark”
answering, meaning the semantic partition of vo-
basedon“textredundancy”oftextwatermark.
cabularywillnotnegativelyaffecttextquality.
6 RelatedWork 7 Conclusion
6.1 LLMWatermarking
Thisworkaimstoinvestigatethecross-lingualcon-
Textwatermarkingaimstoembedawatermarkinto sistencyofwatermarkingmethodsforLLMs. We
atextanddetectthewatermarkforanygiventext. firstcharacterizeandevaluatethecross-lingualcon-
Currently, text watermark method can be classi- sistency of current watermarking techniques for
fied into two categories (Liu et al., 2023c): wa- LLMs,revealingthatcurrentwatermarkingmeth-
termarkingforexsitingtextandwatermarkingfor odsstruggletomaintaintheirwatermarkstrengths
generatedtext. Inthiswork,wefocusonthelatter, across different languages. Based on this obser-
whichismorechallengingandhasmorepractical vation, we propose the cross-lingual watermark
applications. removalattack(CWRA),whichsignificantlychal-
Thistypeofwatermarkmethodusuallycanbe lengeswatermarkrobustnessbyefficientlyelimi-
illustratedasthewatermarkironingprocess(mod- natingwatermarkswithoutcompromisingperfor-
ifying the logits of the LLM during text genera- mance. Throughtheanalysisoftwoprimaryfactors
tion)andwatermarkdetectionprocess(assessthe that influence cross-lingual consistency, we pro-
presenceofwatermarkbyacalculatedwatermark poseX-SIRasadefensestrategyagainstCWRA.
strength score). Kirchenbauer et al. (2023a) in- Despite its limitations, this approach greatly im-
troducesKGW,thefirstwatermarkingmethodfor proves the AUC and paves the way for future re-
LLMs. Hu et al. (2023) proposes UW without search. Overall,thisworkcompletesaclosedloop
affecting the output probability distribution com- inthestudyofcross-lingualconsistencyinwater-
paredtoKGW. Liuetal.(2023b)introducesSIR, marking,including: evaluation,attacking,analysis,
a watermarking method taking into account the anddefensing.
8
)RPT(
etaR
evitisoP
eurT
)RPT(
etaR
evitisoP
eurT8 Limitations multi-documentsummarizationdatasetandabstrac-
tivehierarchicalmodel. InProceedingsofthe57th
WhileX-SIRhasdemonstratedpromisingcapabil- AnnualMeetingoftheAssociationforComputational
ities in enhancing cross-lingual consistency and Linguistics,pages1074–1084,Florence,Italy.Asso-
ciationforComputationalLinguistics.
defendingagainstCWRA,italsofacescertainlim-
itations. A key limitation is its narrow scope of
AngelaFan,YacineJernite,EthanPerez,DavidGrang-
applicability. This limitation stems from the re- ier, Jason Weston, and Michael Auli. 2019. ELI5:
liance on an external bilingual dictionary for se- Long form question answering. In Proceedings of
the57thAnnualMeetingoftheAssociationforCom-
manticclusteringofthevocabulary,whichmeans
putationalLinguistics,pages3558–3567,Florence,
X-SIRfocusessolelyonwholewordsandneglects
Italy.AssociationforComputationalLinguistics.
smaller word units. Consequently, its effective-
ness is closely tied to the tokenization. X-SIR’s Abe Bohan Hou, Jingyu Zhang, Tianxing He,
YichenWang,Yung-SungChuang,HongweiWang,
performancemaybecompromisedifthetokenizer
Lingfeng Shen, Benjamin Van Durme, Daniel
favorsfiner-grainedtokensegmentation. Moreover,
Khashabi,andYuliaTsvetkov.2023. Semstamp: A
X-SIR could face difficulties in scenarios with a semanticwatermarkwithparaphrasticrobustnessfor
significant difference in word order between the textgeneration.
original and pivot languages. This aspect is cru-
ZhengmianHu,LichangChen,XidongWu,YihanWu,
cial, as attackers can exploit these differences in
HongyangZhang,andHengHuang.2023. Unbiased
any language pair to conduct CWRA. Therefore, watermarkforlargelanguagemodels.
X-SIR does not solve the issue of cross-lingual
John Kirchenbauer, Jonas Geiping, Yuxin Wen,
consistencybutsetsthestageforfutureresearch.
JonathanKatz,IanMiers,andTomGoldstein.2023a.
Awatermarkforlargelanguagemodels. InProceed-
ingsofthe40thInternationalConferenceonMachine
References Learning, volume 202 of Proceedings of Machine
LearningResearch,pages17061–17084.PMLR.
Baichuan.2023. Alarge-scale7bpretraininglanguage
modeldevelopedbybaichuan-inc. JohnKirchenbauer,JonasGeiping,YuxinWen,Manli
Shu,KhalidSaifullah,KezhiKong,KasunFernando,
Sachin Chanchani and Ruihong Huang. 2023. AniruddhaSaha,MicahGoldblum,andTomGold-
Composition-contrastive learning for sentence em- stein. 2023b. On the reliability of watermarks for
beddings. InProceedingsofthe61stAnnualMeeting largelanguagemodels.
of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 15836–15848, Aiwei Liu, Leyi Pan, Xuming Hu, Shu’ang Li, Lijie
Toronto, Canada. Association for Computational Wen, IrwinKing, andPhilipS.Yu.2023a. Anun-
Linguistics. forgeablepubliclyverifiablewatermarkforlargelan-
guagemodels.
CanyuChenandKaiShu.2023a. Canllm-generated
misinformation be detected? arXiv preprint Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and
arXiv:2309.13788. LijieWen.2023b. Asemanticinvariantrobustwater-
markforlargelanguagemodels.
CanyuChenandKaiShu.2023b. Combatingmisinfor-
mation in the age of llms: Opportunities and chal- AiweiLiu, LeyiPan, YijianLu, JingjingLi, Xuming
lenges. arXivpreprintarXiv:2311.05656. Hu,LijieWen,IrwinKing,andPhilipSYu.2023c.
A survey of text watermarking in the era of large
Liang Chen, Yatao Bian, Yang Deng, Shuaiyi Li, languagemodels. arXivpreprintarXiv:2312.07913.
BingzheWu,PeilinZhao,andKamfaiWong.2023.
X-mark: Towardslosslesswatermarkingthroughlex- OpenAI.2023. Gpt-4technicalreport.
icalredundancy.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, BERT:SentenceembeddingsusingSiameseBERT-
Shengding Hu, Zhiyuan Liu, Maosong Sun, and networks. InProceedingsofthe2019Conferenceon
BowenZhou.2023. Enhancingchatlanguagemod- EmpiricalMethodsinNaturalLanguageProcessing
els by scaling high-quality instructional conversa- andthe9thInternationalJointConferenceonNatu-
tions. In Proceedings of the 2023 Conference on ralLanguageProcessing(EMNLP-IJCNLP),pages
EmpiricalMethodsinNaturalLanguageProcessing, 3982–3992,HongKong,China.AssociationforCom-
pages3029–3051,Singapore.AssociationforCom- putationalLinguistics.
putationalLinguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
AlexanderFabbri,IreneLi,TianweiShe,SuyiLi,and bert, Amjad Almahairi, Yasmine Babaei, Nikolay
DragomirRadev.2019. Multi-news: Alarge-scale Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
9Bhosale, et al. 2023. Llama 2: Open founda- forKGW.KGWusesahashfunctionH tocompute
tion and fine-tuned chat models. arXiv preprint ahashofthepreviousk tokens. Inthiswork,we
arXiv:2307.09288.
follow the experiment settings of (Kirchenbauer
Shangqing Tu, Yuliang Sun, Yushi Bai, Jifan Yu, Lei etal.,2023a),usingtheminhashwithk = 4. The
Hou, and Juanzi Li. 2023. Waterbench: Towards proportionofgreentokenlistsV tothetotalword
g
holisticevaluationofwatermarksforlargelanguage listV issettoγ = 0.25. Theconstantbiasδ isset
models.
to2.0.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
A.3 UW
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser,andIlliaPolosukhin.2017. Attentionisall
Inthissection,weintroducethedetailwatermark-
youneed. InAdvancesinNeuralInformationPro-
ingironingprocessandwatermarkdetectionpro-
cessingSystems,volume30.CurranAssociates,Inc.
cessofUW.TheironingprocessofUWissimilar
XiYang, KejiangChen, WeimingZhang, ChangLiu, toKGW.ThedifferencebetweenUWandKGWis
Yuang Qi, Jie Zhang, Han Fang, and Nenghai Yu.
thewaytomodifythelogits:
2023. Watermarking text generated by black-box
(1) compute a hash of x1:n: hn+1 = H(x1:n),
languagemodels.
and use hn+1 as seed generating a random
Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu- numberp ∈ [0,1).
XiangWang.2023. Provablerobustwatermarking
(2) determinethetokentsatisfies:
forai-generatedtext.
(cid:34)t−1
A WatermarkMethodDetail (cid:88)
p ∈ P (xn+1|x1:n),
Mi
Inthissection,weprovidemoredetailsaboutthe i=1
t (cid:33)
watermarkingmethodsweuseinourexperiments. (cid:88)
P (xn+1|x1:n) (10)
Mi
A.1 WatermarkStrengthScore i=1
As we discussed in Section 6.1, we focus on the (3) set P (xn+1|x1:n) = 0 for i ̸= t and
Mi
watermarking methods for LLMs. And we illus- P (xn+1|x1:n) = 1.
Mt
tratedthatawatermarkingmethodcanbedivided ThenwegettheadjustedlogitsP˜ (xn+1|x1:n).
M
intotwoparts: watermarkironingprocessandwa-
Thedetectionprocesscalculatesamaximinvari-
termarkdetectionprocess. Inironingprocess,the
ant Log Likelihood Ratio (LLR) of the detected
watermarkisembeddedintothetextbymodifying
text to assess the watermark strength score. Log
the logits of the LLM during text generation. In
LikelihoodRatio(LLR)isdefinedas:
detection process, the watermark detector calcu-
lates the watermark strength score S to assess P˜ (xi|x1:i−1)
M
r = (11)
thepresenceofwatermark. S isascalarvalueto i P (xi|x1:i−1)
M
indicatethestrengthofthewatermarkinthetext.
Foranygiventext,wecancalculateitswatermark Thetotalscoreisdefinedas:
strengthscoreS basedondetectionprocessofthe
P˜ (xa+1:n|x1:a)
watermarking method. A higher S indicates that M
R = (12)
thetextismorelikelytocontainwatermark. Inthe P M(xa+1:n|x1:a)
opposite, a lower S indicates that the text is less
Wherex1:a ispromptandxa+1:n isthedetected
likelytocontainwatermark. Everywatermarking
text. Let
methodhasitsownwaytoironingthewatermark
andcalculatethewatermarkstrengthscoreS. We
P = P (xi|x1:i−1) (13)
provide the details of watermark ironing process i M
and watermark detection process for KGW, UW Q i = P˜ M(xi|x1:i−1) (14)
andSIRinthefollowingsections. R = (r (x ),r (x ),··· ,r (x )) (15)
i i 1 i 2 i |V|
A.2 KGW
Wherer (x )istheLLRoftokenx atpositioni.
i k k
InSection2.2,weintroducethewatermarkironing To avoid the limitation of the original LLR, (Hu
processandwatermarkdetectionprocessofKGW. etal.,2023)proposesamaximinvariantLLR.The
Hereweprovidethedetailsofexperimentsettings calculatingprocessofmaximinvariantLLRcanbe
10illustratedasfollows: model uses a four-layer fully connected residual
networkwithrectifiedlinearunitactivations,with
max min (cid:10) Q′,R (cid:11) ,
Ri Q′ i∈∆V,TV(Q′ i,Qi)≤d i i 1th 0e ,λhyp =erp 0a .1r .am Ae nt der wk e1 u= se2 A0 d, ak m2 o= pt1 im00 iz0 e, rλ w1 it=
h
2
s.t. ⟨P ,exp(R )⟩ ≤ 1 (16)
i i learningrate1e−5totrainthewatermarkmodel
T. The watermark strength parameter δ is set to
Where∆ isthesetofallprobabilitydistributions
V
4.0.
overthesymbolsetV,andTV isthetotalvariation
distance,disahyperparametertocontrolTV,and
B ResultofCross-lingualConsistency
⟨·,·⟩istheinnerproduct. UWutilizesthemaximin
variant LLR to calculate the watermark strength Inthissection,weprovidethedetailresultofour
score. cross-lingualconsistencyexperiment. Weperform
Intheexperiments,wefollowtheexperimentset- our experiment on three watermarking methods:
tingsoforiginalpaper,usingtheprevious5tokens KGW,UWandSIRandtwolargelanguagemod-
tocomputethehashandsetd = 0. els: BAICHUAN-7Band LLAMA-2-7B-CHAT. Fig-
ure7showstheresultofcross-lingualconsistency
A.4 SIR experiment.
In this section, we introduce the watermark iron- Allwatermarkingmethodsshowthatthewater-
ing process and watermark detection process of mark strength score of raw text is positively cor-
SIR.GivenalanguagemodelM,atextembedding relatedwiththetextlength. Aftertranslation,the
languagemodelE,atrainedwatermarkmodelT, watermark strength score significantly decreases.
previous generated text t = (cid:2) t0,··· ,tl−1(cid:3) . The The result indicates that the watermarking meth-
watermarkironingprocessofSIRcanbeillustrated ods we use in our experiments exhibit a lack of
asfollows: cross-lingualconsistency.
(1) Generatethenexttokenlogits: P (tl|t). C ResultofWatermarkRemovalAttack
M
(2) Generatethesemanticembeddingofthepre- Inthissection,weprovidethedetailresultoftext
vioustokens: e = E(t). watermarkremovalattackexperiment. Weperform
l
the experiment on three watermarking methods:
(3) Generate the watermark logit bias: ∆ =
KGW,UWandSIRandtwotasks: textsummariza-
T(e ) ∈ R|V|.
l tionandquestionanswering. Thelargelanguage
(4) Adjustthelogits: P˜ (tl|t) = P (tl|t)+δ× modelweuseintheexperimentisBAICHUAN-7B.
M M
Figure8showstheROCcurvesresultoftextsum-
∆.
marizationtaskundervariousattackmethods: Re-
Whereδisahyperparameter(asmallpositivenum- translation,ParaphraseandCWRAforKGW,UW
ber)tocontrolthestrengthofthewatermark. andSIR.WhileFigure9showstheROCcurvesre-
Thedetectionprocesscalculatesthemeanofthe sultofquestionansweringtaskundervariousattack
watermark bias of the detected text to assess the methods: Re-translation, Paraphrase and CWRA
watermarkstrengthscore. Giventhedetectedtext for KGW, UW and SIR. We also report the TPR
x = (cid:2) x1,··· ,xN(cid:3) . valuesatafixedFPRof0.1ineachsubfigure.
AlltheresultsshowthatCWRAcaneffectively
s =
(cid:80)N n=1∆ I(xn)(x1:n−1)
, (17)
removethewatermarkfromthewatermarkedtext
N which is more effective than Re-translation and
Paraphraseattack.
where I(xn) is the index of token xn within the
vocabulary V. ∆ (x1:n−1) is the watermark
I(xn)
biasoftokenxnatpositionn. Sincethewatermark
(cid:80)
biassatisfiestheunbiasedproperty: ∆ =
t∈V I(t)
0,theexpecteddetectionscoreforthetextwithout
watermarkis0. Thedetectionscoreofthetextwith
watermarkshouldover0.
Inthiswork,wefollowtheoriginexperimentset-
tingsforthewatermarkmodelT. Thewatermark
11En (Original language) En→Zh En→Ja En→Fr En→De
20 140 0.5
15 112 0.4
10 84 0.3
5 56 0.2
0 28 0.1
-5 0 0.0
25 275 525 25 275 525 25 275 525
Text Length Text Length Text Length
(a) KGW | BAICHUAN-7B (b) UW | BAICHUAN-7B (c) SIR | BAICHUAN-7B
8 60 0.2
6 48
0.1
5 36
0.1
3 24
0.0
2 12
0 0 -0.1
25 275 525 25 275 525 25 275 525
Text Length Text Length Text Length
(d) KGW | LLAMA-2-7B-CHAT (e) UW | LLAMA-2-7B-CHAT (f) SIR | LLAMA-2-7B-CHAT
Figure7: Trendsofwatermarkstrengthswithtextlengthbeforeandaftertranslation. Topthreesubfiguresshow
thetrendsofwatermarkstrengthswithtextlengthbeforeandaftertranslationforBAICHUAN-7B.Bottomthree
subfiguresshowthetrendsofwatermarkstrengthswithtextlengthbeforeandaftertranslationforLLAMA-2-7B-
CHAT.
No attack Re-translation Paraphrase CWRA (Ours) Random
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
TPR TPR TPR
0.4 0.980 0.4 0.932 0.4 0.879
0.663 0.318 0.709
0.2 0.480 0.2 0.281 0.2 0.512
0.270 0.183 0.223
0.0 0.0 0.0
0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate (FPR) False Positive Rate (FPR) False Positive Rate (FPR)
(a) KGW (b) UW (c) SIR
Figure8: ROCcurvesforKGW,UWandSIRundervariousattackmethods: Re-translation,ParaphraseandCWRA.
WealsopresentTPRvaluesatafixedFPRof0.1. Thisistheresultoftextsummarizationtask.
12
htgnertS
kramretaW
)RPT(
etaR
evitisoP
eurT
htgnertS
kramretaW
htgnertS
kramretaW
htgnertS
kramretaW
)RPT(
etaR
evitisoP
eurT
htgnertS
kramretaW
htgnertS
kramretaW
)RPT(
etaR
evitisoP
eurTNo attack Re-translation Paraphrase CWRA (Ours) Random
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
TPR TPR TPR
0.4 0.998 0.4 0.915 0.4 0.984
0.884 0.212 0.964
0.2 0.705 0.2 0.188 0.2 0.906
0.168 0.146 0.462
0.0 0.0 0.0
0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate (FPR) False Positive Rate (FPR) False Positive Rate (FPR)
(a) KGW (b) UW (c) SIR
Figure9: ROCcurvesforKGW,UWandSIRundervariousattackmethods: Re-translation,ParaphraseandCWRA.
WealsopresentTPRvaluesatafixedFPRof0.1. Thisistheresultofquestionansweringtask.
13
)RPT(
etaR
evitisoP
eurT
)RPT(
etaR
evitisoP
eurT
)RPT(
etaR
evitisoP
eurT