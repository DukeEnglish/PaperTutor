Geometry-Informed Neural Networks
ArtursBerzins∗1,2 AndreasRadler∗1 SebastianSanokowski1
SeppHochreiter1,3 JohannesBrandstetter1,3
∗Equalcontribution
1LITAILab,InstituteforMachineLearning,JKULinz,Austria
2SINTEF,Oslo,Norway
3NXAIGmbH,Linz,Austria
{berzins, radler, sanokowski, hochreit, brandstetter}@ml.jku.at
Abstract
Weintroducetheconceptofgeometry-informedneuralnetworks(GINNs),which
encompass(i)learningundergeometricconstraints,(ii)neuralfieldsasasuitable
representation,and(iii)generatingdiversesolutionstounder-determinedsystems
oftenencounteredingeometrictasks. Notably,theGINNformulationdoesnot
requiretrainingdata,andassuchcanbeconsideredgenerativemodelingdriven
purelybyconstraints. Weaddanexplicitdiversitylosstomitigatemodecollapse.
Weconsiderseveralconstraints,inparticular,theconnectednessofcomponents
whichweconverttoadifferentiablelossthroughMorsetheory. Experimentally,
wedemonstratetheefficacyoftheGINNlearningparadigmacrossarangeoftwo
andthree-dimensionalscenarioswithincreasinglevelsofcomplexity.
Figure1: TheGINNlearningparadigmappliedtoarealisticengineeringproblemofajetengine
liftingbracket,asdetailedinSection4.2. AgivensetofconstraintsontheshapeΩanditsboundary
∂ΩdefinesthefeasiblesetK. AGINNisaneuralnetworktrainedtorepresentaclose-to-feasible
shape.Ofteningeometry,thefeasiblesetisnon-singular,hencethegenerativeGINNoutputsmultiple
solutionswithenforceddiversity. Usingonlyconstraintsanddiversity,ourapproachtogenerative
modelingisentirelydata-free.
4202
beF
12
]GL.sc[
1v90041.2042:viXra1 Introduction
Neural networks (NNs) have demonstrated remarkable success in capturing complex func-
tions(Krizhevskyetal.,2012;Vaswanietal.,2017). Inparticular,neuralfields(NFs)(Xieetal.,
2022)excelinrepresentingcontinuousspatio-temporalsignalsinarbitraryinput/outputdimensions.
NFs offer detailed, smooth, and topologically flexible representations as closed level-sets, while
beingcompacttostore. Assuch,NFsfindapplicationinmodelingaudio,images,scenes,shapes,and
solutionsofdifferentialequations.
Incontrasttotraditionalsupervisedlearning,inmanyscientificandengineeringapplicationswhere
NFs apply, data is not abundant. On the other hand, these disciplines are equipped with formal
problemdescriptions,suchasobjectivesandconstraints.
This raises the question: Is it possible to train a neural field on objectives and constraints alone,
withoutrelyingonanydata? Anotableprecedentforthisalreadyexistsinphysics-informedneural
networks (PINNs) (Raissi et al., 2019) which are optimized to satisfy the physics and boundary
constraintsofaboundaryvalueproblem.
Inthiswork,wefirstdemonstrateananalogousapproachingeometry–geometry-informedneural
networks(GINNs)–forfindingshapesthatconformtospecifieddesignconstraints.Examplesinclude
prescribedinterfaces,availabledesignspace,materialbudget,and,mostnotably,connectedness. It
isaremarkablyimportantbinarytopologicalfeaturethatwetreatthroughcarefulconsiderationof
Morsetheory.
However,problemsingeometryareoftenunder-determinedandaresolvedbymultiplesolutions.
Everydayobjects,forinstance,takeonvariousforms,andinshapeandtopologyoptimization,several
close-to-optimaldesignsarepossible. Themultiplicityofsolutionsalsoarisesinotherfields,for
instance,forpartialdifferentialequations(PDEs)ofcertaintypesorwithunder-specifiedboundary
orinitialconditions.
Thisinherentnatureofunder-determinedproblemsmotivatesgenerativeGINNs: conditionalneural
fieldsthatrepresentdifferentgeometriesthatallsatisfygivenconstraints. Yet,akintogenerative
adversarialnetworks(Goodfellowetal.,2014),suchamodelmaysufferfrommodecollapse. To
addressthis,weproposeamechanismtoexplicitlyencouragediversity.
Figure 1 showcases the overall concept and the primary experimental result of our study. Given
geometricrequirementsfromarealisticengineeringscenario(prescribedinterface,availabledesign
space,andconnectedness),wetrainaGINNtogenerateasetofneuralfieldsrepresentingvaried
designsthatcloselyadheretotheserequirements.
Ourcontributionsaresummarizedasfollows: (i)weintroduce(generative)GINNsbuiltonthree
pillars–theory-informedlearning,neuralfields,andthegenerativeaspectnecessitatedbyunder-
determinedproblemsettings;(ii)weformulateadiversitylosstomitigatemodecollapseandconsider
severaldifferentiablegeometriclosses,notablyaddressingtheconnectedness;(iii)wedemonstrate
GINNsacross2Dand3Dengineeringusecases.
Neural Conditional Generative
NFs
fields INS modeling
Generative
GINN
GINN Boltzmann
PINN generator
Theory-informed
learning
Figure2: Generativegeometry-informedneuralnetworks(GINNs)lieattheintersectionofneural
fields,particularlyimplicitneuralshapes(INS),generativemodeling,andtheory-informedlearning.
22 Foundations
Westartbyrelatingtheconceptsoftheory-informedlearning,neuralfields,andgenerativemodeling–
allofwhichareimportantbuildingblocksforgenerativeGINNs.
2.1 Theory-informedlearning
Theory-informedlearninghasintroducedaparadigmshifttoscientificdiscoverybyusingscientific
knowledgetoremovephysicallyinconsistentsolutionsandreducingthevarianceofamodel(Karpatne
etal.,2017). Suchknowledgecanbeincludedinthemodelviaequations, logicrules, orhuman
feedback(Dashetal.,2022;Muralidharetal.,2018;VonRuedenetal.,2021). Geometricdeeplearn-
ing(Bronsteinetal.,2021)introducesaprincipledwaytocharacterizeproblemsbasedonsymmetry
andscaleseparationprinciples. Prominentexamplesincludeenforcinggroupequivariances(Cohen&
Welling,2016;Kondor&Trivedi,2018;Cohenetal.,2019)orphysicalconservationlaws(Cranmer
etal.,2020;Greydanusetal.,2019;Guptaetal.,2020;Hoedtetal.,2021).
Notably, mostworksoperateinthetypicaldeeplearningregime,i.e., withanabundanceofdata.
However,intheory-informedlearning,trainingondatacanbereplacedbytrainingwithobjectivesand
constraints. Moreformally,onesearchesforasolutionf minimizingtheobjectiveO(f)s.t. f ∈K,
whereKdefinesthefeasiblesetinwhichtheconstraintsaresatisfied. Forexample,inBoltzmann
generators (Noé et al., 2019), f is a probability function parameterized by a neural network to
approximateanintractabletargetdistribution. Anotherexampleiscombinatorialoptimizationwhere
f ∈{0,1}N isoftensampledfromaprobabilisticneuralnetwork(Belloetal.,2016;Bengioetal.,
2021;Sanokowskietal.,2023).
Aprominentexampleofneuraloptimizationisphysics-informedneuralnetworks(PINNs)(Raissi
etal.,2019), inwhichf isafunctionthatmustminimizetheviolationO ofapartialdifferential
equation(PDE),theinitialandboundaryconditions,and,optionally,somemeasurementdata. Since
PINNs can incorporate noisy data and are mesh-free, they hold the potential to overcome the
limitationsofclassicalmesh-basedsolversforhigh-dimensional,parametric,andinverseproblems.
ThishasmotivatedthestudyofthePINNarchitectures,losses,training,initialization,andsampling
schemes(Wangetal.,2023). WefurtherrefertothesurveyofKarniadakisetal.(2021). APINNis
typicallyrepresentedasaneuralfield(Xieetal.,2022).
2.2 Neuralfields
Aneuralfield(NF)(alsocoordinate-basedNN,implicitneuralrepresentation(INR))isaNN(typically
amultilayer-perceptron(MLP))representingafunctionf :x(cid:55)→ythatmapsaspatialand/ortemporal
coordinate x to a quantity y. Compared to discrete representations, NFs are significantly more
memory-efficientwhileprovidinghigherfidelity,aswellascontinuityandanalyticdifferentiability.
Theyhaveseenwidespreadsuccessinrepresentingandgeneratingavarietyofsignals,including
shapes(Parketal.,2019;Chen&Zhang,2019;Meschederetal.,2019),scenes(Mildenhalletal.,
2021),images(Karrasetal.,2021),audio, video(Sitzmannetal.,2020),andphysicalquantities
(Raissietal.,2019). Foramorecomprehensiveoverview,werefertoasurvey(Xieetal.,2022).
Implicit neural shapes (INSs) represent geometries through scalar fields, such as occupancy
(Mescheder et al., 2019; Chen & Zhang, 2019) or signed-distance (Park et al., 2019; Atzmon &
Lipman,2020). InadditiontothepropertiesofNFs,INSsalsoenjoytopologicalflexibilitysupporting
shapereconstructionandgeneration. Wepointoutthedifferencebetweenthesetwotrainingregimes.
Inthegenerativesetting,thetrainingissupervisedonthegroundtruthscalarfieldofeveryshape
(Parketal.,2019;Chen&Zhang,2019;Meschederetal.,2019). However,insurfacereconstruction,
i.e.,findingasmoothsurfacefromasetofpointsmeasuredfromasingleshape,nogroundtruthis
available(Atzmon&Lipman,2020)andtheproblemisill-defined (Bergeretal.,2016).
Regularization methodshavebeenproposedtocountertheill-posednessingeometryproblems.
Theseincludeleveragingground-truthnormals(Atzmon&Lipman,2021)andcurvatures(Novello
et al., 2022), minimal surface property (Atzmon & Lipman, 2021), and off-surface penalization
(Sitzmannetal.,2020). Acentraleffortistoachievethedistancefieldpropertyofthescalarfieldfor
whichmanyregularizationtermshavebeenproposed: eikonalloss(Groppetal.,2020),divergence
loss(Ben-Shabatetal.,2022),directionaldivergenceloss(Yangetal.,2023),level-setalignment
(Maetal.,2023),orclosestpointenergy(Marschneretal.,2023). Thedistancefieldpropertycan
3beexpressedasaPDEconstraintcalledeikonalequation|∇f(x)| = 1,establishingarelationof
regularizedINStoPINNs(Groppetal.,2020).
Inductivebias. Inadditiontoexplicitlossterms,thearchitecture,initialization,andoptimizercan
alsolimitorbiasthelearnedshapes. Forexample, typicalINSarelimitedtowatertightsurfaces
withoutboundariesorself-intersections(Chibaneetal.,2020;Palmeretal.,2022). ReLUnetworks
arelimitedtopiece-wiselinearsurfacesandtogetherwithgradientdescentarebiasedtowardlow
frequencies(Tanciketal.,2020). Fourier-featureencoding(Tanciketal.,2020)andsinusoidalactiva-
tionscanchangethebiastowardhigherfrequencies(Sitzmannetal.,2020). Similarly,initialization
techniquesareimportanttoconvergetowarddesirableoptima(Sitzmannetal.,2020;Atzmon&
Lipman,2020;Ben-Shabatetal.,2022;Wangetal.,2023).
2.3 Generativemodeling
Deepgenerativemodeling(Kingma&Welling,2013;Goodfellowetal.,2014;Rezende&Mohamed,
2015;Tomczak,2021)playsacentralroleinadvancingdeeplearning,havingenabledbreakthroughs
invariousfields,suchasnaturallanguageprocessing(Brownetal.,2020)andcomputervision(Ho
etal.,2020). MostrelatedtoourworkareconditionalNFsandtheirapplicabilitytodeepgenerative
design.
Conditionalneuralfields encodemultiplesignalssimultaneouslybyconditioningtheweightsof
theNFonalatentvariablez: f(x)=F(x;z)whereF isabasenetwork. Thedifferentchoicesof
theconditioningmechanismleadtoazooofarchitectures,includinginputconcatenation(Parketal.,
2019),hypernetworks(Haetal.,2017),modulation(Mehtaetal.,2021),orattention(Rebainetal.,
2022). Thesecanbeclassifiedintoglobalandlocalmechanisms,whichalsoestablishesaconnection
ofconditionedNFstooperatorlearning(Perdikaris,2023). FormoredetailwerefertoXieetal.
(2022);Rebainetal.(2022);Perdikaris(2023).
Generativedesign referstocomputationaldesignmethods,whichcanautomaticallyconductdesign
explorationunderconstraintsthataredefinedbydesigners(Jangetal.,2022). Itholdsthepotential
ofstreamlininginnovativedesignsolutions. Incontrasttogenerativemodeling,thegoalofgenerative
design is not to mimic existing data, but to generate novel designs. However, in contrast to text
andimagegeneration,datasetsarenotabundantinthesedomainsandoftencoverthedesignspace
sparsely. Nonetheless,deeplearninghasshownpromiseinmaterialdesign,shapesynthesis,and
topology optimization (Regenwetter et al., 2022). The latter searches for a material distribution
thatminimizesaspecifiedobjective,suchasmechanicalcompliance(Bendsoe&Sigmund,2003).
Sincethegeometryismostlyrepresentedimplicitlytoallowtopologicaltransitions,severalrecent
works investigate the use of NFs to parameterize the material density and perform optimization
usingclassical(Chandrasekhar&Suresh,2021)orPINNsolvers(Zehnderetal.,2021). Someof
theseworksattempttogeneralizeacrossprobleminstancesusingconditionalNFs(Zehnderetal.,
2021)ordiscreterepresentations(Cangetal.,2019;Jangetal.,2022). Formoredetailontopology
optimizationviamachinelearning,werefertothesurveyofShinetal.(2023).
3 Method
Consideranelementf insomespaceF. Inthiswork,wefocusonf beingafunctionrepresentinga
geometryoraPDEsolution. Letthesetofconstraints1C(f)=[c (f)]besatisfiedinthefeasibleset
i
K={f ∈F|C(f)=0}. SelectingtheconstraintsC ofageometricnaturelaysthefoundationfora
geometry-informedneuralnetworkorGINN,whichoutputsasolutionthatsatisfiestheconstraints:
f ∈ K. Section 3.1 first details how to find a single solution f that represents a shape. As we
detailinSection3.3,theGINNformulationisanalogoustoPINNs,butwithakeydifferencethat
geometricproblemsareoftenunder-determined. ThismotivatesagenerativeGINN whichoutputsa
setofdiversesolutionsS asaresultoftheformalobjectivemax δ(S)whereδcapturessome
S⊆K
intuitivenotionofdiversityofaset. Inthesecondpart(Section3.2),wethereforediscussfindingand
representingmultiplediversesolutionsS usingconditionalNFs.
4Setconstraintc(Ω) Functionconstraintc(f) Lossl(f)
Designregion Ω⊂E f(x)>0∀x∈/ E (cid:82) max(0,f(x))2dx
X\E
Interface ∂Ω⊃I f(x)=0∀x∈I (cid:82) f2(x)dx
I
Prescribednormal n(x)=n¯(x)∀x∈I ∇ f(x) =n¯(x)∀x∈I (cid:82) (cid:16) ∇ f(x) −n¯(x)(cid:17)2 dx
|f(x)| I |f(x)|
Meancurvature κ (x)=0∀x∈∂Ω div(cid:16) ∇ f(x) (cid:17) =0∀x∈∂Ω (cid:82) div2(cid:16) ∇ f(x) (cid:17) dx
H |f(x)| ∂Ω |f(x)|
Connectedness SeeSection3.1.1andAppendix
Table 1: Geometric constraints used in our experiments. The shape Ω and its boundary ∂Ω are
representedimplicitlybythe(sub-)levelsetofthefunctionf. Ifgiven,theshapemustbecontained
within the design region E ⊆ X and attach to the interface I ⊂ E with a potentially prescribed
normaln¯(x). ndenotestheoutward-facingnormalandκ isthemeancurvature,bothofwhichcan
H
becomputedfromf inaclosedform. MoreconstraintsarediscussedinTable3.
3.1 Geometry-informedneuralnetworks(GINNs)
Representation of a solution. Let f : X (cid:55)→ R be a continuous scalar function on the domain
X ⊂ Rn. The sign of f implicitly defines the shape Ω = {x∈X|f(x)⩽0} and its boundary
∂Ω = {x∈X|f(x)=0}. WeuseaNNtorepresenttheimplicitfunction,i.e. animplicitneural
shape,duetoitsmemoryefficiency,continuity,andanalyticdifferentiability. Nonetheless,GINNs
extendeasilytootherrepresentations,suchasparametrizations,aswedemonstrateexperimentallyin
Section4.1.
Sincethereareinfinitelymanyimplicitfunctionsrepresentingthesamegeometry,werequiref to
approximatethesigned-distancefunction(SDF)ofΩ. EvenifSDF-nessisfullysatisfied,onemust
becarefulwhenmakingstatementsaboutΩusingf,e.g. whencomputingdistancesbetweenshapes.
WedonotconsidertheSDF-nessoff asageometricconstraintsinceitcannotbeformulatedonthe
geometryΩitself. Nonetheless,intraining,theeikonallossistreatedanalogouslytothegeometric
losses,asdescribednext.
Constraintsonasolution. Theconditionf ∈ Kiseffectivelyahardconstraint. Werelaxeach
constraintc intoadifferentiablelossl :F (cid:55)→[0,∞)whichdescribestheconstraintviolation. With
i i
theweightsλ >0,thetotalconstrainviolationoff is
i
(cid:88)
L(f)= λ l (f). (1)
i i
i
Thisrelaxestheconstraintsatisfactionproblemf ∈Kintotheunconstrainedoptimizationproblem
min L(f). ThecharacteristicfeatureofGINNsisthattheconstraintsareofageometricnature. The
f
constraintsusedinourexperimentsarecollectedinTable1andmorearediscussedinTable3. By
representingthesetΩthroughthefunctionf,thegeometricconstraintsonΩ(Column2)canbe
translatedintofunctionalconstraintsonf (Column3). Thisinturnallowstoformulatedifferentiable
losses (Column 4). Some losses are trivial and several have been previously demonstrated as
regularizationtermsforINS(e.g. normals,curvatures). Intheremainderofthissub-section,wefocus
ondevelopingalossforconnectedness,whichholdsthekeytoapplyingGINNstomanyengineering
designproblems.
3.1.1 Connectedness
Motivation. Connectednessdenotesthestateofanobjectconsistingofasingleconnectedcomponent.
Moreformally,weconsiderΩconnected,ifforallpointsx ,x ∈Ωthereexistsacontinuousfunction
0 1
γ :[0,1](cid:55)→Ωsuchthatγ(0)=x andγ(1)=x . Connectednessisaubiquitousfeatureofmany
0 1
objects since it allows the propagation of forces (structural integrity, manufacturability), signals
(nervoussystems,electricnetworks),andotherresources(bloodvessels,streetnetworks). Inthe
contextofNNs,connectednessconstraintshavebeenpreviouslyformulatedfordiscrete(pixel/voxel)
segmentationmasksonstreetnetworks(Wangetal.,2020)andmedicalimages(Nadimpallietal.,
2023;Cloughetal.,2022). Tothebestofourknowledge,ithasnotbeenaddressedintheINScontext,
yetweargueitisanecessityforenablingGINNs.
1Foreaseofnotation,wetransforminequalityconstraintstoequalityconstraints.
5Figure3: Ourconnectednesslossbuildsuponthesurfacenetwork,inwhichintegralpaths(black)
connectcriticalpoints. Thekeyintuitionbehindourlossisthatconnectedcomponents(sub-level
setswithboundariesinred)startatminima(purple),terminateatmaxima(yellow),andconnectvia
saddlepoints(turquoise). Bypenalizingvaluesatspecificsaddlepoints,anupdate(right,blue)can
connectcomponents.
Sinceconnectednessandothertopologicalinvariantsarediscrete-valued,itisnon-trivialtoformulate
a differentiable loss. We propose a solution based on Morse theory. We present the necessary
backgroundandourmethodonlybrieflyinthemaintextandrefertoBiasottietal.(2008a,b);Rana
(2004)formorebackgroundandtoAppendix3.1.1formoredetailsonourapproach.
Background. Morsetheoryrelatesthetopologyofamanifoldtothecriticalpointsoffunctions
defined on that manifold. In essence, the topology of a sub-level set Ω(t) = {x∈D|f(x)⩽t}
changesonlywhentpassesthroughacriticalvalueoff. RootedinMorsetheoryisthesurface
network,whichisagraphwithverticesascriticalpointsandedgesasintegralpaths(seeFigure3).
Thisandrelatedgraphscompactlyrepresenttopologicalinformationandfindmanyapplicationsin
computervision, graphics, andgeometry(Biasottietal.,2008a;Rana,2004). However, existing
algorithmsconstructthemondiscreterepresentations.
First,weextendtheconstructionofasurfacenetworktoINSbyleveragingautomaticdifferentia-
tion. ThisisdetailedinAppendixB.1andillustratedinFigure11,butinbrief: Wefindcriticalpoints
andcomputetheirHessianmatricesH. Theireigenvaluesignscharacterizethetypesofcriticalpoints.
Next,weconstructtheintegralpathsthatstartateachsaddlepoint(wheretheyaretangentialtothe
eigenvectorsofH)andfollowthepositive/negativegradientoff untilreachinglocalmaxima/minima.
Weconvertthesepathstoedgestoobtainthesurfacenetwork. Theresultinggraphprovidesabasis
forcomputingmanytopologicalandgeometricalfeatures.
Second, we construct a differentiable connectedness loss by relaxing the inherently discrete
constraint. The key insight is that connected components of Ω are born at minima, destroyed at
maxima,andconnectedviasaddlepoints. Usinganaugmentededge-weightedgraphbuiltfromthe
surfacenetwork,wefirstidentifyandthenconnectdisconnectedcomponentsbypenalizingthevalue
off atcertainsaddlepoints. Inpractice,weneedtohandledeviationsfromthetheory(AppendixB)
whichleavesroomforimprovingthemethodfurther. WeapplythislossinSection4.1,includingour
primaryexperimentinFigure1andanexplicitablationinFigure4(c).
3.2 GenerativeGINNs
We proceed to extend the GINN framework to produce a set of diverse solutions, leading to the
conceptofgenerativeGINNs.
Representationofthesolutionset. ThegeneratorG(z) = f mapsalatentvariablez ∈ Z toa
solutionf. Thesolutionsetishencetheimageofthelatentsetunderthegenerator: S =Im (Z).
G
Furthermore, the generator transforms the input probability distribution p over Z to an output
Z
probabilitydistributionpoverS. Inpractice,thegeneratorisamodulatedbasenetworkproducinga
conditionalneuralfield: f(x)=F(x;z).
6Constraintsonthesolutionset. Byadoptingaprobabilisticview,weextendtheconstraintviolation
toitsexpectedvalue. ThisagainrelaxestherelationS ⊆Kintomin L(S):
S
(cid:90)
p(f)L(f)df = E [L(G(z))]=L(S). (2)
S
z∼pZ
Diversityofthesolutionset. ThelastmissingpiecetotrainingagenerativeGINNismakingS a
diversecollectionofsolutions. Inthetypicalsupervisedgenerativemodelingsetting,thediversityof
thegeneratorisinheritedfromthediversityofthetrainingdataset. Theviolationofthisisstudied
underphenomenalikemodecollapseinGANs(Cheetal.,2017). Explorationbeyondthetraining
data has been attempted by adding an explicit diversity loss, such as entropy (Noé et al., 2019),
Coulombrepulsion(Unterthineretal.,2018),determinantalpointprocesses(Chen&Ahmed,2020;
HeyraniNobarietal.,2021),pixeldifferenceandstructuraldissimilarity(Jangetal.,2022). Justas
thedata-freescenariorequiresustopushtheuseoftheorytothelimit,thesameholdsfordiversity.
Manyscientificdisciplinesrequiretomeasurethediversitiesofsetswhichhasresultedinarange
of definitions for diversity (Parreño et al., 2021; Enflo, 2022; Leinster & Cobbold, 2012). Most
start from a distance d : F2 (cid:55)→ [0,∞), which can be transformed into therelated similarity and
dissimilarity. Diversityδ : 2F (cid:55)→ [0,∞)isthenthecollectivedissimilarityofaset(Enflo,2022),
aggregatedinsomeway. Intheremainder,wedescribethesetwoaspects: thedistancedandthe
aggregationintothediversityδ.
Aggregation. AdoptingterminologyfromEnflo(2022),weusetheminimalaggregationmeasure2:
(cid:32) (cid:18) (cid:19)1/2(cid:33)2
(cid:88)
δ(S)= mind(f ,f ) . (3)
i j
j̸=i
i
Crucially,thismeasureisconcavepromotinguniformcoverageoftheavailablespace,asdepicted
inFigure13. Section4.2demonstratesthatthissufficestobuildagenerativeGINNandweleave
additionalconsiderationstofutureresearch,particularlytheexplorationofdiversitiesforsetswith
manifoldstructure.
Distance. AsimplechoiceformeasuringthedistancebetweentwofunctionsistheL2 function
(cid:113)
distance d (f ,f ) = (cid:82) (f (x)−f (x))2dx. While the L2 function distance may suffice in
2 i j X i j
manyapplications,recallthatforgeometryweultimatelywanttomeasurethedistancebetweenthe
geometries,nottheirimplicitfunctionrepresentations. Forexample,consideradiskandremoveits
centralpoint. Whilewewouldnotexpecttheirshapedistancetobesignificant,theL2 distanceof
theirSDFsis. ThisisbecauselocalchangesinthegeometrycancauseglobalchangesintheSDF.
Forthisreason,wemodifythedistance(derivationinAppendixD)toonlyconsidertheintegralon
theshapeboundaries∂Ω ,∂Ω whichpartiallyalleviatestheglobalityissue:
i j
(cid:115)
(cid:90) (cid:90)
d(f ,f )= f (x)2dx+ f (x)2dx. (4)
i j j i
∂Ωi ∂Ωj
Inpractice,wesampleboundarypointsandapproximatetheintegralswithMonteCarlo. Iff is
j
anSDFthen(cid:82) f (x)2dx=(cid:82) min ||x−x′||2dx(analogouslyforf )anddisclosely
∂Ωi j ∂Ωi x′∈∂Ωj 2 i
relatedtothechamferdistance(Nguyenetal.,2021).
Note: disnotapropermetricdistanceonfunctions,butrecallthatwecareaboutthegeometriesthey
represent. Usingappropriateboundarysamples,onemayalsodirectlycomputeageometricdistance,
e.g.,anypointclouddistance(Nguyenetal.,2021). However,thepropagationofthegradientsfrom
thegeometricboundarytothefunctionrequirestheconsiderationofboundarysensitivity(Berzins
etal.,2023),whichweleaveforfuturework.
To summarize, training a generative GINN corresponds to an unconstrained optimization
problemmin L(S)−λ δ(S),whereλ > 0controlsthepotentialtrade-offbetweenconstraint
S δ δ
violationL(S)anddiversityδ(S)onthesetS =Im (Z)ofgeneratedgeometries.
G
3.3 RelationtoPINNs
IthasbeenobservedthatthefittingofINSsisrelatedtoPINNs,e.g.,viatheeikonalequation(Gropp
etal.,2020),thePoissonproblem(Sellán&Jacobson,2023),andimplicitneuralrepresentations
2Theminimalaggregationmeasureappliestofinitesets,i.e.,abatch.
7(Sitzmannetal.,2020). WealsoobservethatmanybestpracticesforPINNs(Wangetal.,2023)
transfertoGINNs. However,thereareseveralnotabledifferences. InPINNs,constraintsprimarily
use differential and only occasionally integral or fractional operators (Karniadakis et al., 2021),
whereasGINNscommonlyrequireabroaderclassofconstraints:differential(e.g.curvature),integral
(e.g. volume),topological(e.g. connectedness),orgeometric(e.g. thickness). Secondly,thedesign
specificationmayrequireevenmorelosstermscomparedtoPINNs. Thirdly,andmostimportantly,
geometric problems are frequently under-determined, motivating the search for multiple diverse
solutions. Forcompleteness,inSection4.3,wegeneratediversesolutionstoanunder-determined
systemofPDEs,therebyintroducingtheconceptofgenerativePINNs.
4 Experiments
WeexperimentallydemonstratekeyaspectsofGINNs,step-by-stepbuildingtowardsarealistic3D
engineeringdesignusecase. Additionalexperimentdetails,includingnegativeresults,areprovided
inAppendixA.Unlessdiscussedotherwise,thelossesareusedasgiveninTable1. Weconclude
by demonstrating the analogous idea – a generative PINN – that outputs diverse solutions to an
under-determinedphysicsproblem.
4.1 GINNs
Plateau’s problem to demonstrate GINNs on a well-posed problem. Plateau’s problem is
to find the surface S with the minimal area given a prescribed boundary Γ (a closed curve in
X ⊂ R3). A minimal surface is known to have zero mean curvature κ everywhere. Minimal
H
surfaceshaveboundariesandmaycontainintersectionsandbranchpoints(Douglas,1931)which
cannotberepresentedimplicitly. Forsimplicity,weselectasuitableprobleminstance,notingthat
moreappropriategeometricrepresentationsexist(Wang&Chern,2021;Palmeretal.,2022). For
animplicitsurface,themeancurvaturecanbecomputedfromthegradientandtheHessianmatrix
(Goldman,2005). Altogether,werepresentthesurfaceasS =∂Ω∩X andthetwoconstraintsare:
Γ⊂S andκ (x)=0∀x∈S. TheresultinFigure4(a)agreeswiththeknownsolution.
H
Parabolicmirrortodemonstrateadifferentgeometricrepresentation. Althoughwemainly
focus on INS, the GINN framework trivially extends to other representations, such as explicit,
parametric, or discrete shapes. Here, the GINN learns the height function f : [−1,1] (cid:55)→ R of a
mirrorwiththesingleconstraintthatallthereflectedraysshouldintersectasinglepoint(0,1.5).
TheresultinFigure4(b)approximatestheknownsolution: aparabolicmirror. Thisisaverybasic
exampleofcaustics,aninverseprobleminoptics,whichwehopeinspiresfutureworkonanalogous
vision-informedneuralnetworksleveragingtherecentdevelopmentsinneuralrenderingtechniques.
Obstacletodemonstratetheconnectednessloss. ConsiderthedomainX =[−1,1]×[−0.5,0.5]
andthedesignregionthatisasmallerrectangulardomainwithacircularobstacleinthemiddle:
E =([−0.9,0.9]×[−0.4,0.4])\{x2+x2 ≤0.12}. Thereisaninterfaceconsistingoftwovertical
1 2
linesegmentsI = {(±0.9,x )|−0.4 ⩽ x ⩽ 0.4}withtheprescribedoutwardfacingnormals
2 2
n¯(±0.9,−0.4⩽x ⩽0.4)=(0,±1). WeuseanMLPwith4layers512neuronseachandsoftplus
2
(continuouslydifferentiableReLU)activation(moredetailsinAppendixA.3andTable2).Figure4(c)
depictsthisset-upandtwosolutionsobtainedbytrainingaGINNwithandwithoutaconnectedness
loss demonstrating its viability. Note, that this problem admits infinitely many solutions, so we
continuebyaddingdiversitytoproducetheresultsinFigure5.
4.2 GenerativeGINNs
Obstacletointroducediversity. Wecontinuewiththepreviouslydescribedobstacleproblemby
searchingforseveraldiversesolutions. Forthis,weneedtomakethemodelgenerativeandadda
diversityloss. Wesetasmalldiversityweightλ =5×10−5 andconditiontheneuralfieldusing
δ
inputconcatenation. Fortraining,weapproximatetheone-dimensionallatentsetZ =[−1,1]with
N = 16fixedequallyspacedsamples. Thisenablesthereuseofsomecalculationsacrossepochs
andresultsinawell-structuredlatentspace,illustratedthroughlatentspaceinterpolationinFigure5.
Atthetwoextremes,theshapesconnectaboveandbelowtheobstacle,respectively,drivenbythe
diversityloss. Withoutit,thesolutionsetcollapsestoasingleshape(Figure7).
Giventhesimplicityoftheshapes,anintermediateinterpolationstepmustviolateeithertheobstacle
8(a) (b) (c)
Figure 4: GINN solving different geometry problems. In (a), a GINN finds the unique surface
thatattachestotheprescribedboundarywhilehavingzeromeancurvatureeverywhere. In(b), a
GINNfindstheuniquesurfaceofamirrorthatcollectsreflectedraysintoasinglepoint. In(c),our
connectednesslossallowsaGINNtofindashapeconnectingthetwointerfaceswithintheallowed
designregion.
ortheconnectednessconstraint(asisthecasehere). Thismaybeexplainedbythelow-frequencybias
inherentinMLPswithsoftplus(smoothReLU)activations(Tanciketal.,2020). Whilemorecomplex
topologies allow to interpolate between the two discrete states without violating any constraints,
representingthisrequiresamoreexpressivemodel,suchasSIREN(Sitzmannetal.,2020),whichwe
applyinthefollowingmainexperiment.
JetenginebrackettodemonstrategenerativedesignwithGINNsonarealistic3Dengineering
designproblem. Theproblemspecificationdrawsinspirationfromanengineeringdesigncompeti-
tionhostedbyGeneralElectricandGrabCAD3.Thechallengewastodesignthelightestpossible
liftingbracketforajetenginesubjecttobothphysicalandgeometricalconstraints. Here,wefocus
onasubsetoftheseconstraints. Theshapemustfitinaprovideddesignspaceandattachtofive
cylindricalinterfaces: fourfixingboltsandaloadingpin. Inaddition,wepositconnectednesswhich
isatrivialrequirementforstructuralintegrity(Figure9(a)showsacounterexample).
Toaccommodatethemuchhighershapecomplexity,weuseaSIRENmodelwith5hiddenlayers
and 256 neurons each (more details in Appendix A.4 and Table 2). Figure 8 shows that we find
shapesthatmeettheconstraints. However,theyexhibitundulations(highsurfacewaviness)dueto
theinductivebiasofSIREN.Wefindthatcontrollingtheinitializationcancounteractthis,butalso
interfereswiththeconstraintsatisfaction(Figure9(b)).
To achieve smoother shapes, we additionally penalize surface strain away from the interfaces:
(cid:82) κ2(x)+κ2(x)dx, where κ and κ are the principal curvatures. The resulting shapes in
∂Ω\I 1 2 1 2
Figure1demonstratethatthegenerativeGINNcanproducesmoothanddiverseresultsthatclosely
satisfytheinterface,designspace,andconnectednessconstraints.
During training, we condition SIREN using input concatenation on N = 4 different fixed latent
codesspacedequallyinZ =[−0.1,0.1]. ThisisduetothehighexpressivityofSIREN,forwhich
wedonotachieveawell-structuredlatentspace,asdemonstratedbytheinterpolationinFigure 10.
Prospectiveapproachestoaddressthisincludealternativeconditioningmechanisms(Mehtaetal.,
2021),latentspaceregularization(Liuetal.,2022),aswellassimplyscalingupthetraining. Wealso
observethattrainingasinglegenerativeGINNonN latentcodestakesmuchlesstimethantraining
N individualGINNs(5h<16·1hforobstacleand26h<4·17hforliftingbracketasdetailedin
AppendixA)toachievesimilarquality. Thesamesub-linearscalinghasbeenobservedfortraining
latentconditionedPINNs(Taufik&Alkhalifah,2023). Thisindicatessomegeneralizationevenfor
thebracketproblemandencouragesfurtherscalingoftheexperiments. Altogether,wehopethese
resultsinspiremorefutureworkonapplyingGINNstogenerativedesign.
3https://grabcad.com/challenges/ge-jet-engine-bracket-challenge
9Figure5: ShapesproducedbyinterpolatingthelatentspaceofagenerativeGINNtrainedtoconnect
theleftandrightsidesofthedesignspace. Inaddition,weoptimizeforthediversityofthetraining
samples(16intotal,includingthefirstandlastshapesshownhere). Attheextremesofthelatent
space,thetwofoundshapesconnectaboveandbelowtheobstacle. Asaresult,duringlatentspace
interpolation,theshapesmustviolateeitherthedesignspaceortheconnectedness(here).
4.3 GenerativePINNs
Having developed a generative GINN that is capable of producing diverse solutions to an under-
determinedproblem,weaskifthisideageneralizestootherareas. Inphysics,problemsareoften
well-definedandhaveauniquesolution. However,casesexistwhereanon-particularPDEsolutionis
sufficientandtheexactinitialconditionsareirrelevant,suchastheanimationoffire,smoke,orwater.
WeconcludetheexperimentalsectionbydemonstratingtheentirelyanalogousgenerativePINN ona
reaction-diffusionsystem. SuchsystemswereintroducedbyTuring(1952)toexplainhowpatternsin
nature,suchasstripesandspots,canformasaresultofasimplephysicalprocessofreactionand
diffusionoftwosubstances. AcelebratedmodelofsuchasystemistheGray-Scottmodel(Pearson,
1993),whichproducesanextremevarietyofinterestingpatternsbychangingjusttwoparameters–
thefeed-rateαandthekill-rateβ –inthefollowingPDE:
∂u
=D ∆u−uv2+α(1−u),
∂t u
(5)
∂v
=D ∆v+uv2−(α+β)v.
∂t v
ThisPDEdescribestheconcentrationu,voftwosubstancesU,V undergoingthechemicalreaction
U +2V → 3V. The rate of this reaction is described by uv2, while the rate of adding U and
removingV iscontrolledbytheparametersαandβ. Crucially,bothsubstancesundergodiffusion
(controlledbythecoefficientsD ,D )whichproducesaninstabilityleadingtorichpatternsaround
u v
thebifurcationlineα=4(α+β)2.
Computationally,thesepatternsaretypicallyobtainedbyevolvingagiveninitialconditionu(x,t=
0) = u (x), v(x,t = 0) = v (x)onsomedomainwithperiodicboundaryconditions. Avariety
0 0
of numerical solvers can be applied, but previous PINN attempts fail without data (Giampaolo
et al., 2022). To demonstrate a generative PINN on a problem that admits multiple solutions,
we omit the initial condition and instead consider stationary solutions, which are known to exist
for some parameters α,β (McGough & Riley, 2004). We use the corresponding stationary PDE
(∂u/∂t=∂v/∂t=0)toformulatetheresiduallosses:
(cid:90)
L = (D ∆u−uv2+α(1−u))2dx,
u u
D (6)
(cid:90)
L = (D ∆v+uv2−(α+β)v)2dx.
v v
D
To avoid trivial (i.e. uniform) solutions, we encourage non-zero gradient with a loss term
−max(1,(cid:82) (∇u(x))2+(∇v(x))2dx). Similartothemorecomplex3Dgeometryexperiment,we
X
findthatarchitectureandinitializationarecritical. WeusetwoidenticalSIRENnetworksforeachof
thefieldsuandv. Theyhavetwohiddenlayersofwidths256and128(moredetailsinAppendix
10Figure6: AgenerativePINNproducingTuringpatternsthatmorphduringlatentspaceinterpolation.
Thisisaresultofsearchingfordiversesolutionstoanunder-determinedGray-Scottsystem.
A.5). Using the diffusion coefficients D = 1.2×10−5, D = 2D and the feed and kill-rates
v u v
α = 0.028,β = 0.057, the generative PINN produces diverse and smoothly changing pattern of
worms,illustratedinFigure6. Tothebestofourknowledge,thisisthefirstPINNthatproduces2D
Turingpatternsinadata-freesetting.
5 Conclusion
Wehaveintroducedgeometry-informedneuralnetworksdemonstratinggenerativemodelingdriven
solelybygeometricconstraintsanddiversityasaremedytomodecollapse. Wehaveconsidered
severaldifferentiablelosses,mostnotablyconnectedness,andhavestep-by-stepdemonstratedGINNs
experimentally,culminatinginarealistic3Dengineeringdesignusecase.
Limitationsandfuturework. GenerativeGINNscombineseveralknownandnovelcomponents,
eachofwhichwarrantsanin-depthstudyoftheoreticalandpracticalaspects. Itisworthexploring
severalalternativestotheshapedistanceanditsaggregation,architecture,andconditioningmecha-
nism,aswellasconnectedness,whosecurrentimplementationiscomputationallycostlyanddeviates
from the theoretical foundation due to the necessary robustness. Likewise, investigating a broad
rangeofconstraintsspanningandcombininggeometry,topology,physics,andotherdomainslike
visionpresentsaclearavenueforfutureinvestigation. AnobservedlimitationofGINNtrainingis
thesensitivitytohyperparametersincludingthebalancingofmanylosses. Replacingsomelosses
withhardconstraintscanhelpmitigatethis. Inadditiontoscalingupthetraining,webelievetackling
theseaspectscanhelptransferthesuccessofNNstopracticalapplicationsindesignsynthesisand
relatedtasks.
Acknowledgments
WesincerelythankGeorgMuntinghandOliverBarrowcloughfortheirfeedbackonthepaper.
TheELLISUnitLinz,theLITAILab,andtheInstituteforMachineLearningaresupportedbythe
FederalStateofUpperAustria. WethanktheprojectsMedicalCognitiveComputingCenter(MC3),
INCONTROL-RL(FFG-881064),PRIMAL(FFG-873979),S3AI(FFG-872172),EPILEPSIA(FFG-
892171), AIRI FG 9-N (FWF-36284, FWF-36235), AI4GreenHeatingGrids (FFG- 899943), IN-
TEGRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-
CL6-2021-CLIMATE-01-01). We thank Audi.JKU Deep Learning Center, TGW LOGISTICS
GROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline GmbH, Google,
ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA,
VerbundAG,SoftwareCompetenceCenterHagenbergGmbH,BorealisAG,TÜVAustria,Frauscher
Sensonic,TRUMPF,andtheNVIDIACorporation.
Arturs Berzins was supported by the European Union’s Horizon 2020 Research and Innovation
ProgrammeunderGrantAgreementnumber860843.
11References
Atzmon,M.andLipman,Y. SAL:Signagnosticlearningofshapesfromrawdata. InIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),June2020.
Atzmon,M.andLipman,Y. SALD:signagnosticlearningwithderivatives. In9thInternational
ConferenceonLearningRepresentations,ICLR2021,2021.
Bello,I.,Pham,H.,Le,Q.V.,Norouzi,M.,andBengio,S. Neuralcombinatorialoptimizationwith
reinforcementlearning. arXivpreprintarXiv:1611.09940,2016.
Ben-Shabat,Y.,HewaKoneputugodage,C.,andGould,S. DiGS:Divergenceguidedshapeimplicit
neuralrepresentationforunorientedpointclouds. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.19323–19332,2022.
Bendsoe,M.P.andSigmund,O. Topologyoptimization: theory,methods,andapplications. Springer
Science&BusinessMedia,2003.
Bengio,Y.,Lodi,A.,andProuvost,A. Machinelearningforcombinatorialoptimization: amethod-
ologicaltourd’horizon. EuropeanJournalofOperationalResearch,290(2):405–421,2021.
Berger,M.,Tagliasacchi,A.,Seversky,L.,Alliez,P.,Guennebaud,G.,Levine,J.,Sharf,A.,andSilva,
C. ASurveyofSurfaceReconstructionfromPointClouds. ComputerGraphicsForum,pp. 27,
2016.
Berzins,A.,Ibing,M.,andKobbelt,L. Neuralimplicitshapeeditingusingboundarysensitivity. In
TheEleventhInternationalConferenceonLearningRepresentations.OpenReview.net,2023.
Biasotti, S., De Floriani, L., Falcidieno, B., Frosini, P., Giorgi, D., Landi, C., Papaleo, L., and
Spagnuolo,M. Describingshapesbygeometrical-topologicalpropertiesofrealfunctions. ACM
Comput.Surv.,40(4),oct2008a. ISSN0360-0300.
Biasotti, S., Giorgi, D., Spagnuolo, M., and Falcidieno, B. Reeb graphs for shape analysis and
applications.TheoreticalComputerScience,392(1):5–22,2008b.ISSN0304-3975.Computational
AlgebraicGeometryandApplications.
Bronstein,M.M.,Bruna,J.,Cohen,T.,andVelicˇkovic´,P. Geometricdeeplearning: Grids,groups,
graphs,geodesics,andgauges. arXivpreprintarXiv:2104.13478,2021.
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,Dhariwal,P.,Neelakantan,A.,Shyam,
P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural
informationprocessingsystems,33:1877–1901,2020.
Cang,R.,Yao,H.,andRen,Y. One-shotgenerationofnear-optimaltopologythroughtheory-driven
machinelearning. Computer-AidedDesign,109:12–21,2019. ISSN0010-4485.
Chandrasekhar,A.andSuresh,K. TOuNN:Topologyoptimizationusingneuralnetworks. Structural
andMultidisciplinaryOptimization,63(3):1135–1149,Mar2021. ISSN1615-1488.
Che,T.,Li,Y.,Jacob,A.P.,Bengio,Y.,andLi,W. Moderegularizedgenerativeadversarialnetworks.
In5thInternationalConferenceonLearningRepresentations,ICLR2017,Toulon,France,April
24-26,2017,ConferenceTrackProceedings.OpenReview.net,2017.
Chen,W.andAhmed,F. PaDGAN:LearningtoGenerateHigh-QualityNovelDesigns. Journalof
MechanicalDesign,143(3):031703,112020. ISSN1050-0472.
Chen,Z.andZhang,H. Learningimplicitfieldsforgenerativeshapemodeling. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.5939–5948,2019.
Chibane,J.,Mir,A.,andPons-Moll,G. Neuralunsigneddistancefieldsforimplicitfunctionlearning.
InAdvancesinNeuralInformationProcessingSystems(NeurIPS),December2020.
Clough,J.R.,Byrne,N.,Oksuz,I.,Zimmer,V.A.,Schnabel,J.A.,andKing,A.P. Atopological
loss function for deep-learning based image segmentation using persistent homology. IEEE
TransactionsonPatternAnalysis&MachineIntelligence,44(12):8766–8778,dec2022. ISSN
1939-3539.
12Cohen,T.andWelling,M. Groupequivariantconvolutionalnetworks. InInternationalconference
onmachinelearning,pp.2990–2999.PMLR,2016.
Cohen,T.S.,Geiger,M.,andWeiler,M. AgeneraltheoryofequivariantCNNsonhomogeneous
spaces. Advancesinneuralinformationprocessingsystems,32,2019.
Cranmer, M., Greydanus, S., Hoyer, S., Battaglia, P., Spergel, D., andHo, S. Lagrangianneural
networks. arXivpreprintarXiv:2003.04630,2020.
Dash,T.,Chitlangia,S.,Ahuja,A.,andSrinivasan,A. Areviewofsometechniquesforinclusionof
domain-knowledgeintodeepneuralnetworks. ScientificReports,12(1):1040,2022.
Douglas,J. Solutionoftheproblemofplateau. TransactionsoftheAmericanMathematicalSociety,
33(1):263–321,1931. ISSN00029947.
Dugas,C.,Bengio,Y.,Bélisle,F.,Nadeau,C.,andGarcia,R. Incorporatingsecond-orderfunctional
knowledgeforbetteroptionpricing. InLeen,T.,Dietterich,T.,andTresp,V.(eds.),Advancesin
NeuralInformationProcessingSystems,volume13.MITPress,2000.
Enflo,K. Measuringone-dimensionaldiversity. Inquiry,0(0):1–34,2022.
Ester,M.,Kriegel,H.-P.,Sander,J.,andXu,X. Adensity-basedalgorithmfordiscoveringclusters
inlargespatialdatabaseswithnoise. InProceedingsoftheSecondInternationalConferenceon
KnowledgeDiscoveryandDataMining,KDD’96,pp.226–231.AAAIPress,1996.
Giampaolo, F., De Rosa, M., Qi, P., Izzo, S., and Cuomo, S. Physics-informed neural networks
approachfor1dand2dgray-scottsystems. AdvancedModelingandSimulationinEngineering
Sciences,9(1):5,May2022.
Goldman, R. Curvature formulas for implicit curves and surfaces. Computer Aided Geometric
Design,22(7):632–658,2005. ISSN0167-8396. GeometricModellingandDifferentialGeometry.
Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,Courville,A.,and
Bengio,Y. Generativeadversarialnets. Advancesinneuralinformationprocessingsystems,27,
2014.
Greydanus, S., Dzamba, M., andYosinski, J. Hamiltonianneuralnetworks. Advancesinneural
informationprocessingsystems,32,2019.
Gropp,A.,Yariv,L.,Haim,N.,Atzmon,M.,andLipman,Y. Implicitgeometricregularizationfor
learningshapes. InIII,H.D.andSingh,A.(eds.),ProceedingsofMachineLearningandSystems
2020,volume119ofProceedingsofMachineLearningResearch,pp.3569–3579.PMLR,13–18
Jul2020.
Gupta,J.K.,Menda,K.,Manchester,Z.,andKochenderfer,M. Structuredmechanicalmodelsfor
robotlearningandcontrol. InLearningforDynamicsandControl,pp.328–337.PMLR,2020.
Ha,D.,Dai,A.M.,andLe,Q.V. HyperNetworks. In5thInternationalConferenceonLearning
Representations,ICLR2017.OpenReview.net,2017.
HeyraniNobari,A.,Chen,W.,andAhmed,F. PcDGAN:Acontinuousconditionaldiversegenerative
adversarialnetworkforinversedesign. InProceedingsofthe27thACMSIGKDDConference
onKnowledgeDiscovery&DataMining,KDD’21,pp.606–616,NewYork,NY,USA,2021.
AssociationforComputingMachinery. ISBN9781450383325.
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural
informationprocessingsystems,33:6840–6851,2020.
Hoedt,P.-J.,Kratzert,F.,Klotz,D.,Halmich,C.,Holzleitner,M.,Nearing,G.S.,Hochreiter,S.,and
Klambauer,G. Mc-lstm: Mass-conservinglstm. InInternationalconferenceonmachinelearning,
pp.4275–4286.PMLR,2021.
Jang,S.,Yoo,S.,andKang,N. Generativedesignbyreinforcementlearning: Enhancingthediversity
oftopologyoptimizationdesigns. Computer-AidedDesign,146:103225,2022. ISSN0010-4485.
13Karniadakis,G.E.,Kevrekidis,I.G.,Lu,L.,Perdikaris,P.,Wang,S.,andYang,L. Physics-informed
machinelearning. NatureReviewsPhysics,3(6):422–440,June2021.
Karpatne,A.,Atluri,G.,Faghmous,J.H.,Steinbach,M.,Banerjee,A.,Ganguly,A.,Shekhar,S.,
Samatova,N.,andKumar,V. Theory-guideddatascience: Anewparadigmforscientificdiscovery
fromdata. IEEETransactionsonknowledgeanddataengineering,29(10):2318–2331,2017.
Karras, T., Aittala, M., Laine, S., Härkönen, E., Hellsten, J., Lehtinen, J., and Aila, T. Alias-
freegenerativeadversarialnetworks. InRanzato,M.,Beygelzimer,A.,Dauphin,Y.,Liang,P.,
andVaughan,J.W.(eds.),AdvancesinNeuralInformationProcessingSystems,volume34,pp.
852–863.CurranAssociates,Inc.,2021.
Kingma,D.P.andWelling,M. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,
2013.
Kondor,R.andTrivedi,S. Onthegeneralizationofequivarianceandconvolutioninneuralnetworks
totheactionofcompactgroups.InInternationalConferenceonMachineLearning,pp.2747–2755.
PMLR,2018.
Krizhevsky, A., Sutskever, I., andHinton, G.E. Imagenetclassificationwithdeepconvolutional
neuralnetworks. Advancesinneuralinformationprocessingsystems,25,2012.
Kurochkin,S.V. Neuralnetworkwithsmoothactivationfunctionsandwithoutbottlenecksisalmost
surelyamorsefunction.ComputationalMathematicsandMathematicalPhysics,61(7):1162–1168,
Jul2021.
Leinster,T.andCobbold,C.A. Measuringdiversity: theimportanceofspeciessimilarity. Ecology,
93(3):477–489,March2012.
Liu,H.-T.D.,Williams,F.,Jacobson,A.,Fidler,S.,andLitany,O. Learningsmoothneuralfunctions
vialipschitzregularization. InACMSIGGRAPH2022ConferenceProceedings,SIGGRAPH’22,
NewYork,NY,USA,2022.AssociationforComputingMachinery. ISBN9781450393379.
Ma,B.,Zhou,J.,Liu,Y.,andHan,Z. Towardsbettergradientconsistencyforneuralsigneddistance
functionsvialevelsetalignment. In2023IEEE/CVFConferenceonComputerVisionandPattern
Recognition (CVPR), pp. 17724–17734, Los Alamitos, CA, USA, jun 2023. IEEE Computer
Society.
Marschner,Z.,Sellán,S.,Liu,H.-T.D.,andJacobson,A. Constructivesolidgeometryonneural
signeddistancefields. InSIGGRAPHAsia2023ConferencePapers,SA’23,NewYork,NY,USA,
2023.AssociationforComputingMachinery. ISBN9798400703157.
McGough,J.S.andRiley,K. Patternformationinthegray–scottmodel. NonlinearAnalysis: Real
WorldApplications,5(1):105–121,2004. ISSN1468-1218.
Mehta,I.,Gharbi,M.,Barnes,C.,Shechtman,E.,Ramamoorthi,R.,andChandraker,M. Modulated
periodicactivationsforgeneralizablelocalfunctionalrepresentations. In2021IEEE/CVFInterna-
tionalConferenceonComputerVision(ICCV),pp.14194–14203,LosAlamitos,CA,USA,oct
2021.IEEEComputerSociety.
Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., and Geiger, A. Occupancy networks:
Learning3dreconstructioninfunctionspace. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.4460–4470,2019.
Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf:
Representingscenesasneuralradiancefieldsforviewsynthesis. CommunicationsoftheACM,65
(1):99–106,2021.
Muralidhar,N.,Islam,M.R.,Marwah,M.,Karpatne,A.,andRamakrishnan,N. Incorporatingprior
domainknowledgeintodeepneuralnetworks. In2018IEEEinternationalconferenceonbigdata
(bigdata),pp.36–45.IEEE,2018.
Nadimpalli, K. V., Chattopadhyay, A., and Rieck, B. A. Euler characteristic transform based
topologicallossforreconstructing3dimagesfromsingle2dslices. 2023IEEE/CVFConference
onComputerVisionandPatternRecognitionWorkshops(CVPRW),pp.571–579,2023.
14Nguyen, T., Pham, Q., Le, T., Pham, T., Ho, N., and Hua, B. Point-set distances for learning
representationsof3dpointclouds. In2021IEEE/CVFInternationalConferenceonComputer
Vision(ICCV),pp.10458–10467,LosAlamitos,CA,USA,oct2021.IEEEComputerSociety.
Novello,T.,Schardong,G.,Schirmer,L.,daSilva,V.,Lopes,H.,andVelho,L. Exploringdifferential
geometryinneuralimplicits. Computers&Graphics,108:49–60,2022.
Noé,F.,Olsson,S.,Köhler,J.,andWu,H. Boltzmanngenerators: Samplingequilibriumstatesof
many-bodysystemswithdeeplearning. Science,365(6457):eaaw1147,2019.
Palmer,D.,Smirnov,D.,Wang,S.,Chern,A.,andSolomon,J. DeepCurrents: Learningimplicit
representations of shapes with boundaries. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition(CVPR),2022.
Park,J.J.,Florence,P.,Straub,J.,Newcombe,R.,andLovegrove,S. DeepSDF:Learningcontinuous
signeddistancefunctionsforshaperepresentation. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.165–174,2019.
Parreño,F.,ÁlvarezValdés,R.,andMartí,R. Measuringdiversity.areviewandanempiricalanalysis.
EuropeanJournalofOperationalResearch,289(2):515–532,2021. ISSN0377-2217.
Pearson,J.E. Complexpatternsinasimplesystem. Science,261(5118):189–192,1993.
Perdikaris,P. Aunifyingframeworkforoperatorlearningvianeuralfields,Dec2023.
Raissi,M.,Perdikaris,P.,andKarniadakis,G.E. Physics-informedneuralnetworks: Adeeplearning
framework for solving forward and inverse problems involving nonlinear partial differential
equations. JournalofComputationalphysics,378:686–707,2019.
Rana,S.(ed.). Topologicaldatastructuresforsurfaces. JohnWiley&Sons,Chichester,England,
March2004.
Rebain,D.,Matthews,M.J.,Yi,K.M.,Sharma,G.,Lagun,D.,andTagliasacchi,A. Attentionbeats
concatenationforconditioningneuralfields. Trans.Mach.Learn.Res.,2023,2022.
Regenwetter,L.,Nobari,A.H.,andAhmed,F. DeepGenerativeModelsinEngineeringDesign: A
Review. JournalofMechanicalDesign,144(7):071704,032022. ISSN1050-0472.
Rezende, D. and Mohamed, S. Variational inference with normalizing flows. In International
conferenceonmachinelearning,pp.1530–1538.PMLR,2015.
Sanokowski,S.,Berghammer,W.,Hochreiter,S.,andLehner,S. Variationalannealingongraphsfor
combinatorialoptimization. arXivpreprintarXiv:2311.14156,2023.
Sellán, S. and Jacobson, A. Neural stochastic poisson surface reconstruction. In SIGGRAPH
Asia2023ConferencePapers,SA’23,NewYork,NY,USA,2023.AssociationforComputing
Machinery. ISBN9798400703157.
Shin,S.,Shin,D.,andKang,N. Topologyoptimizationviamachinelearninganddeeplearning: a
review. JournalofComputationalDesignandEngineering,10(4):1736–1766,072023. ISSN
2288-5048.
Sitzmann, V., Martel, J. N., Bergman, A. W., Lindell, D. B., and Wetzstein, G. Implicit neural
representationswithperiodicactivationfunctions. InProc.NeurIPS,2020.
Tancik,M.,Srinivasan,P.,Mildenhall,B.,Fridovich-Keil,S.,Raghavan,N.,Singhal,U.,Ramamoor-
thi,R.,Barron,J.,andNg,R. Fourierfeaturesletnetworkslearnhighfrequencyfunctionsinlow
dimensionaldomains. AdvancesinNeuralInformationProcessingSystems,33:7537–7547,2020.
Taufik,M.H.andAlkhalifah,T. LatentPINNs: Generativephysics-informedneuralnetworksviaa
latentrepresentationlearning. arXivpreprintarXiv:2305.07671,2023.
Tomczak,J.M. Whydeepgenerativemodeling? InDeepGenerativeModeling,pp.1–12.Springer,
2021.
15Turing, A.A.M. Thechemicalbasisofmorphogenesis. Philos.Trans.R.Soc.Lond., 237(641):
37–72,August1952.
Unterthiner,T.,Nessler,B.,Seward,C.,Klambauer,G.,Heusel,M.,Ramsauer,H.,andHochreiter,
S. CoulombGANs: provablyoptimalnashequilibriaviapotentialfields. In6thInternational
ConferenceonLearningRepresentations,ICLR2018,Vancouver,BC,Canada,April30-May3,
2018,ConferenceTrackProceedings.OpenReview.net,2018.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and
Polosukhin,I. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,30,
2017.
VonRueden,L.,Mayer,S.,Beckh,K.,Georgiev,B.,Giesselbach,S.,Heese,R.,Kirsch,B.,Pfrommer,
J., Pick, A., Ramamurthy, R., et al. Informed machine learning–a taxonomy and survey of
integratingpriorknowledgeintolearningsystems. IEEETransactionsonKnowledgeandData
Engineering,35(1):614–633,2021.
Wang,F.,Liu,H.,Samaras,D.,andChen,C. TopoGAN:Atopology-awaregenerativeadversarial
network. InProceedingsofEuropeanConferenceonComputerVision,2020.
Wang,S.andChern,A. Computingminimalsurfaceswithdifferentialforms. ACMTrans.Graph.,
40(4):113:1–113:14,August2021.
Wang,S.,Sankaran,S.,Wang,H.,andPerdikaris,P. Anexpert’sguidetotrainingphysics-informed
neuralnetworks,2023.
Xie,Y.,Takikawa,T.,Saito,S.,Litany,O.,Yan,S.,Khan,N.,Tombari,F.,Tompkin,J.,Sitzmann,V.,
andSridhar,S. Neuralfieldsinvisualcomputingandbeyond. ComputerGraphicsForum,2022.
ISSN1467-8659.
Yang,H.,Sun,Y.,Sundaramoorthi,G.,andYezzi,A. StEik: Stabilizingtheoptimizationofneural
signeddistancefunctionsandfinershaperepresentation. InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023.
Zehnder, J., Li, Y., Coros, S., and Thomaszewski, B. NTopo: Mesh-free topology optimization
usingimplicitneuralrepresentations. InRanzato,M.,Beygelzimer,A.,Dauphin,Y.,Liang,P.,
andVaughan,J.W.(eds.),AdvancesinNeuralInformationProcessingSystems,volume34,pp.
10368–10381.CurranAssociates,Inc.,2021.
16A ImplementationandExperimentalDetails
Wereportadditionaldetailsontheexperimentsandtheirimplementation. Werunalltheexperiments
onasingleGPU(oneofNVIDIARTX2080Ti,RTX3090,A40,orP40).
A.1 Plateau’sproblem
ThemodelisanMLPwith[3,256,256,256,1]neuronsperlayerandthetanhactivation. Wetrain
withAdam(defaultparameters)for10000epochswithalearningrateof10−3takingaroundthree
minutes. Thethreelosses(interface,meancurvature,andeikonal)areweightedequallybutmean
curvaturelossisintroducedonlyafter1000epochs. Tofacilitateahigherlevelofdetail,thecorner
pointsoftheprescribedinterfaceareweightedhigher.
A.2 Parabolicmirror
ThemodelisanMLPwith[2,40,40,1]neuronsperlayerandthetanhactivation. Wetrainwith
Adam(defaultparameters)for3000epochswithalearningrateof10−3takingaroundtenseconds.
A.3 Obstacle
Hyperparametertuning. Theobstacleexperimentservesasaproofofconceptforincludingand
balancingseverallosses,inparticulartheconnectednessloss. ThemodelisanMLPwithresidual
connections, as it was stable to train and was sufficiently expressive to solve the task. The most
important hyperparameters are summarized in Table 2. Leveraging the similarity to PINNs, we
followmanypracticalsuggestionsdiscussedinWangetal.(2023). Wefindthatagoodstrategyfor
lossbalancingistostartwiththelocallosses(i.e. interface,envelope,obstacle,normal)andthen
incorporategloballosses(firsttheeikonallossandthentheconnectednessandcurvaturelosses).
Ingeneral,weobservethatthegloballossweightsλshouldbekeptlowerthanthoseofthelocal
lossesinordernottodestroythelocalstructure. Byaddingonelossatatime,webinary-searchan
appropriateweightwhilepreservingtheoverallbalance. Toextendtothegenerativesettingsome
hyperparametersneedadjusting(usuallywithin1or2ordersofmagnitude).
Computationalcost. ThetotaltrainingtimeisaroundanhourfortheGINN(singleshape)and5
hoursforthegenerativeGINN(trainedon16shapes). Thebulkofthecomputationtime(oftenmore
than90%)istakenbytheconnectednessloss. Toalleviatethis,weonlyrecomputethecriticalpoints
every10epochsandusethepreviouspointsasawarmstart. Whilethisworkswellforthesoftplus
MLP,itdoesnotworkreliablyforSIRENnetworkssincethebehavioroftheircriticalpointsismore
spurious. Thisaspectpresentsanopportunityforfutureimprovement.
Ablation ofthediversitylossresultsinmodecollapseasshowninFigure7.
Figure7: GenerativeGINNtrainedonasimpleobstacleproblemwithoutdiversity. Incontrastto
Figure5,thegeneratorshowsmodecollapse.
17Hyperparameter Obstacle(2D) Jetenginebracket(3D)
Architecture MLPwithresidualconnections MLP(SIREN)
Layers [3,512,512,512,512,1] [4,256,256,256,256,256,1]
Activation softplus sine
ω offirstlayerforSIREN n/a 8.0
0
Learningrate 0.001 0.001
Learningrateschedule 0.5t/1000 0.5t/1000
Adambetas 0.9,0.999 0.9,0.999
Iterations 2000 5000
λ 1 1
interface
λ 1 10−1
envelope
λ 10−1 n/a
obstacle
λ 10−2 10−6
normal
λ 10−2 10−9
eikonal
λ 10−5 10−2
connectedness
λ 10−5to10−4 10−5to10−3
diversity
λ n/a 10−8to10−7
curvature
Table2: Hyperparametersforthegenerative2Dobstacleand3Djetenginebracketexperiments. The
inputisa2Dor3Dpointconcatenatedwitha1Dlatentvector. Forbothexperiments, theinitial
learningrateishalvedevery1000iterations. Interestingly,theSIRENnetworkoverallhadfewer
parameters,whilefittingamorecomplexshape.
A.4 Jetenginebracket
Thejetenginebracketisourmostcomplexexperiment. Comparedtotheobstacleexperiment,we
swap the architecture to the more expressive SIREN network and increase the sampling density
aroundtheinterfaces. Themostimportanthyperparametersforthejetenginebracketaresummarized
inTable2. Thetotaltrainingtimeisaround17hoursfortheGINN(singleshape)and26hoursfor
thegenerativeGINN(trainedon4shapes).
SIRENnetworks. EarlyexperimentsshowedthatanMLPwithresidualconnectionsandasoftplus
activationfunction(Dugasetal.,2000)wasnotexpressiveenoughtosatisfythegivenconstraints.
We,therefore,employaSIRENnetwork(Sitzmannetal.,2020). Asrecommended,wetuneω =8.0
0
of the first layer, while the values 6.5 and 10.0 produced shapes that were too smooth or wavy,
respectively.
Spatialresolution. Thecurseofdimensionalityimpliesthatwithhigherdimensions,exponentially
(inthenumberofdimensions)morepointsareneededtocoverthespaceequidistantly. Therefore,in
3D,substantiallymorepoints(andconsequentlymemoryandcompute)areneededthanin2D.In
ourexperiments,weobservethatalowspatialresolutionaroundtheinterfacespreventsthemodel
fromlearninghigh-frequencydetails,likelyduetoastochasticgradient. Increasedspatialresolution
resultsinabetterlearningsignalandthemodelpicksupthedetailseasier. Formemoryandcompute,
weincreasetheresolutionmuchmorearoundtheinterfacesandlesssoelsewhere.
Ablation. We observed several failure modes during the jet engine bracket experiments. First,
Figure9(a)showsanablationoftheconnectednessloss,whichleadstoaspuriousshape. Second,
weobservethattheSIRENnetworkishighlysensitivetotheω parameterofthefirstlayer. Figure
0
9(b)showsanexampleofω =6.5,whichledtoanoverlysmoothshapepoorlyfittingtheinterfaces.
0
Conversely,Figures8and10showresultswithratherhighsurfaceundulation. Third,ourmodeland
trainingfailtostructurethelatentspaceasevidencedbytheinterpolationinFigure10.
18Figure8: GenerativeGINNtrainedwithoutthesmoothnessloss,contrastingthesmoothershapesin
Figure1. Theseshapessatisfytheconstraintswellbutdisplayhighsurfaceundulation(waviness)
duetoSIREN’sbiastowardhigh-frequencies.
(a) (b)
Figure9: Ablationof(a)connectednessand(b)initializationscheme. (a)Ashapegeneratedbya
GINNusingSIRENtrainedwithouttheconnectednessloss. Theshapefitsthedesignspaceand
interfaces well, but it consists of many spurious disconnected components failing to connect the
prescribedboundaries. (b)AshapegeneratedbyaGINNusingSIRENwithapoorinitializationof
ω =6.5forthefirstlayer. Theshapeistoosmoothanddoesnotfittheinterfaceswell.
0
19Figure 10: Failure mode: lack of latent space structure. For each row, the left and right shapes
correspondtotwooutoffourfixedlatentcodesusedduringtraining.Themiddleshapesaregenerated
bylinearlyinterpolatingthesetwolatentcodes. Wehypothesizethefailuretostructurethelatent
spaceisacombinationoftheexpressivityofthemodelandinsufficienttraining.
A.5 Reaction-diffusion
WeusetwoidenticalSIRENnetworksforeachofthefieldsuandv. Theyhavetwohiddenlayers
ofwidths256and128. WeenforceperiodicboundaryconditionsontheunitdomainX = [0,1]2
throughtheencodingx (cid:55)→(sin2πx ,cos2πx )fori=1,2. Withthisencoding,weuseω =3.0
i i i 0
toinitializeSIREN.WealsofindthatthesameshapedFourier-featurenetwork(Tanciketal.,2020)
withanappropriateinitializationofσ =3worksequallywell.
We compute the gradients and the Laplacian using finite differences on a 64×64 grid, which is
randomly translated in each epoch. Automatic differentiation produces the same results for an
appropriateinitializationscheme,butfinitedifferencesareanorderofmagnitudefaster. Thetrained
fieldsu,vcanbesampledatanarbitrarilyhighresolutionwithoutdisplayinganyartifacts.
Weusethelossweightsλ = 1,λ = 10−4,andλ = 10−7. ThegenerativePINNsare
residual gradient δ
trainedwithAdamfor20000epochswitha10−3learningratetakingafewminutes.
B Connectedness
WedetailthealgorithmforcomputingtheconnectednesslossintroducedinSection3.1.1. Onahigh
level,wecomputeitintwosteps. First,weconstructthesurfacenetworkoftheINS,detailedin
SectionB.1. Second, wemodifythenetworkbyaugmentingitwithnewedgesandaddingedge
weights. Bytraversingthisgraph,wedeterminehowtopenalizethevaluesatcertainsaddlepointsto
createaconnectionbetweendisconnectedcomponents,detailedinSectionB.2.
B.1 Surfacenetwork
Morse theory. Let M be a smooth compact n-dimensional manifold without a boundary, and
f :M (cid:55)→Ratwicecontinuouslydifferentiablefunctiondefinedonit. LetH (p)denotetheHessian
f
matrix of f at p ∈ M. A critical point p ∈ M is non-degenerate if H (p) non-singular. For a
f
non-degeneratecriticalpointp,thenumberofnegativeeigenvaluesoftheHessianiscalledtheindex
ofp. f iscalledaMorsefunctionifallthecriticalpointsoff arenon-degenerate. f issometimes
calledasimpleMorsefunctionifallthecriticalpointsphavedifferentvaluesf(p). (Simple)Morse
functionsaredenseincontinuousfunctions. UndermildassumptionsmostNNsareMorsefunctions
(Kurochkin,2021).
Surfacenetworks areatypeofgraphusedinMorsetheorytocapturetopologicalpropertiesofa
sub-levelset. Theyoriginatedingeospatialapplicationstostudyelevationmapsf :X ⊂R2 (cid:55)→R
onbounded2Ddomains. Moreprecisely,asurfacenetworkisagraphwhoseverticesarethecritical
points of f connected by edges which represent integral paths. An integral path γ : R (cid:55)→ M is
20everywheretangenttothegradientvectorfield: ∂γ/∂s=∇f(γ(s))foralls∈R. Bothendsofan
integralpathlim γ(s)areatcriticalpointsoff. Thereexistclassicalalgorithmstofindsurface
s(cid:55)→±∞
networksongrids,meshes,orotherdiscreterepresentations(Rana,2004;Biasottietal.,2008b).
We extend the construction of the surface network to an INS represented by a NN f leveraging
automaticdifferentiationinthefollowingsteps(illustratedinFigure11).
1. Find critical points. Initialize a large number of points X ⊂ X, e.g. by random
or adaptive sampling. Minimize the norm of their gradients using gradient descent:
min (cid:80) ||∇f(x)||2. After reaching a stopping criterion, remove points outside of
X x∈X 2
thedomainandnon-convergedcandidatepointswhosegradientnormexceedssomethresh-
old. Clustertheremainingcandidatepoints. WeuseDBSCAN(Esteretal.,1996).
2. CharacterizecriticalpointsbycomputingtheeigenvaluesoftheirHessianmatricesH (x).
f
Minima have only positive eigenvalues, maxima only negative eigenvalues, and saddle
pointshaveatleastonepositiveandonenegativeeigenvalue.
3. Findintegralpaths. Fromeachsaddlepoint,start2dimX integralpaths,eachtangenttoa
Hessianmatrixeigenvectorwithapositive/negativeeigenvalue. Followthepositive/negative
gradientuntilreachingalocalmaximum/minimumorleavingthedomain.
4. ConstructsurfacenetworkasagraphG=(V,E),wherethesetofverticesV consistsof
thecriticalpointsfromstep1andthesetofedgesE fromstep3.
B.2 Connectednessloss
InMorsetheorycomponentsofthesub-levelsetappearatminima,disappearatmaxima,andconnect
throughsaddlepoints. MorsetheoryonlyassumesthatthefunctionisMorse,buton(approximate)
SDFs,saddlepointscanbeassociatedwiththemedialaxis.
Signeddistancefunction (SDF)f :X (cid:55)→RofashapeΩgivesthe(signed)distancefromthequery
pointxtotheclosestboundarypoint:
(cid:26) d(x,∂Ω) ifx∈Ωc(ifxisoutsidetheshape),
f(x)= (7)
−d(x,∂Ω) ifx∈Ω(ifxisinsidetheshape).
Apointx∈X belongstothemedialaxisifitsclosestboundarypointisnotunique. Thegradientof
anSDFobeystheeikonalequation||∇f(x)||=1everywhereexceptonthemedialaxiswherethe
gradientisnotdefined. Figure12depictsanSDFforashapewithtwoconnectedcomponents. In
INS,theSDFisapproximatedbyaNNwithparametersθ: f ≈f.
θ
Intuition. Figure12showsanexactSDFwithtwoconnectedcomponents(CCs)(inred)andserves
asanentrypointtopresentingtheconnectednesslossinmoredetail. Theshortestline(inblack)
betweenthetwoCCsintersectsthemedialaxisatx′. Atthisintersection,bothdirectionsalongthe
shortestlinearedescentdirectionsandtherestrictionoff tothemedialaxishasalocalminimum
(i.e.,hastwoascentdirections). Nonetheless,thispointx′ isnotapropersaddlepoint,sincethe
gradient∇f(x′)isnotwell-defined. However,wecanexpecttheapproximateSDFf ∈C2tohave
θ
asaddleatx′. ToconnecttwoCCsalongtheshortestpath,wecanconsiderthemedialaxis,i.e. the
saddlepointsoftheapproximateSDF.Therefore,webuildaconnectednesslossbypenalizingthe
valueoff atthesaddlepointsinacertainway.
Multiplesaddlepointsbetweentwoconnectedcomponents. Ingeneral,thereisnoreasonto
expectthereisauniquesaddlepointbetweentwoCCssoanyorallofthemultiplesaddlepointscan
beusedtoconnecttheCCs. Manyapproachesaregeneralizedbyapenaltyweightp foreachsaddle
i
pointx . E.g.,onesimplesolutionistopickthesaddlepointontheshortestpathbetweentheCCs
i
amountingtoaunitpenaltyvectorp. Anothersolutionistopenalizeallsaddlepointsbetweenthe
twoCCsequally. Wepickthepenaltyptobeinverselyproportionaltothedistancedbetweentwo
shapeboundaries,i.e. p∼ 1. ThisimpliesthattheshorterthedistancebetweentwoCCsviaasaddle
d
point,thehigheritspenaltyandthemoreincentivefortheshapetoconnectthere.
Shortestpathsusingdistanceweightededges. Weconstructthesurfacenetworkoff asexplained
θ
inSectionB.1. Wemodifythisgraphbyweightingtheedgeswiththedistancesbetweenthenodes.
WeweightheedgesthatconnectnodesofthesameCCwith0. Intotal, theweightedgraphG
w
allowsustofindtheshortestpathsbetweenpairsofCCsusinggraphtraversal.
21Figure11: Thefourstepsofconstructingthesurfacenetworkfromlefttoright. (1)Findthecritical
points by doing gradient descent to the minimum of the gradient norm of the input points. (2)
Characterizecriticalpointsviaanalyzingtheeigenvaluesofthepoints’Hessians. (3)Connectthe
saddlepointstotheadjacentcriticalpointsviagradientascent/descent. (4)Constructthesurface
networkgraphwithedgescorrespondingtotheascents/descentsfromthesaddlepoints.
22Figure12: Asigneddistancefunctionthatdescribesashape(inred)withtwoconnectedcomponents
(anellipseontherightandawavypentagonontheleft). Thecontourcolorsin-andoutsidetheshape
increaseaccordingtotheeikonalequation||∇f(x)||=1andaredescribedbythegraylevelsetsand
therightcolorbar. TheleftcolorbardescribestheSDFvaluesatthemedialaxiswhichisalineof
discontinuity,sinceateachpointofthemedialaxis,thedistancetobothcomponentsisequal. The
blacklinemarkstheshortestdistancebetweenthetwoconnectedcomponents. Thislinecrossesthe
medialaxisatthemedialaxispointwithminimumelevation. Thepointofintersectionisexactlyhalf
ofthedistancebetweenthetwocomponents.
Robustness. Thusfarweassumedthat(i)f isacloseapproximationofthetrueSDFf and(ii)that
θ
wefindtheexactsurfacenetworkoff . However,inpractice,theseassumptionsrarelyhold,sowe
θ
introducetwomodificationstoaidtherobustness.
RobustnesstoSDFapproximation. Theassumptionthatf isacloseapproximationofthetrue
θ
SDF is easily violated during the initial stages of training or when the shape undergoes certain
topologicalchanges. ForatrueSDF,theshortestpathbetweentwoCCscrossesthemedialaxisonly
once,soonewouldexpecttwoCCstoconnectviaasinglesaddlepoint. ForanapproximateSDF,
theshortestpathmightcontainmultiplesaddlepoints. However,thissimplycorrespondstomultiple
hopsinthegraphG whichdoesnotposeadditionalchallenges. Wechoosetopenalizeonlythose
w
saddlepointsthatareadjacenttotheshapesothattheshapegrowsoutward. Alternatively,onecould
penalizeallthesaddlepointsontheentireshortestpath. Whilethiscancausenewcomponentsto
emerge in-between the shapes, this and other options are viable choices that can be investigated
further.
Robustnesstosurfacenetworkapproximation. Sofar,wealsoassumethatweextracttheexact
surfacenetworkoff (independentofwhetheritisanexactorapproximateSDF).However,due
θ
tonumericallimitations,itmaynotcontainallcriticalpointsorthecorrectintegralpaths. Thiscan
causenotbeingabletoidentifyapathbetweenCCs. Intheextremecase,theerroneouslyconstructed
surfacenetworkmightbeentirelyempty,inwhichcasethereisnoremedy. Toimprovetherobustness
againstmildercases,weaugmentG withedgesbetweenallpairsofcriticalpointsthatareoutside
w
oftheshape. TheedgeweightsaresettotheEuclideandistancesbetweenthepoints,resultinginthe
augmentedweightedgraphG . Thisimprovesthelikelihoodthattherealwaysexistsatleastone
a
pathbetweenanytwoCCs.
Algorithm. Once we have computed the penalty weights, we normalize them for stability and
computetheloss. PuttingitalltogetherwearriveatAlgorithm1.
Limitations. AsmentionedinSection5andAppendixA.3,ourcurrentapproachiscomputationally
costly due to building the surface network and traversing the augmented graph in every epoch.
Whilewemanagetoupdateandreusethesestructuresinsomecases,doingthisreliablyrequires
further investigation. Furthermore, the requisite robustness of the practical implementation has
led to deviations from the theoretical foundations. Overall, there is a compelling motivation for
23Algorithm1Connectednessloss
Input: augmentedweightedsurfacenetworkG constructedfromf
a θ
Output: connectednesslossl
connectedness
1: foreachnodekdo
2: initializepenaltyp k =0
3: ifkisadjacenttoacomponentc l then
4: foreachpairofconnectedcomponents{c i,c j}do
5: computed ij asthelengthoftheshortestpathinG aconnectinganynodeinc iandany
nodeinc vianodek
j
6: addtopenaltyofkaccordingtothedistancep k =p k+ dij1 +ϵ
7: endfor
8: endif
9: endfor
10: foreachnodekdo
11: normalizethepenaltyp k = (cid:80)p lk pl
12: endfor
(cid:80)
13: computethelossl
connectedness
= kp kf(x k)
futureresearchtoaddressboththeoreticalandpracticalaspects,alongsideexploringincremental
adjustmentsorentirelynovelmethodologies.
C Geometricconstraints
InTable3,weprovideanon-exhaustivelistofmoreconstraintsrelevanttoGINNs.
Constraint Comment
Volume Non-trivialtocomputeanddifferentiateforlevel-setfunction(easierfordensity).
Area Non-trivialtocompute,buteasytodifferentiate.
Minimalfeaturesize Non-trivialtocompute,relevanttotopologyoptimizationandadditivemanufac-
turing.
Symmetry Typicalconstraintinengineeringdesign,suitableforencoding.
Tangential Computefromnormals,typicalconstraintinengineeringdesign.
Parallel Computefromnormals,typicalconstraintinengineeringdesign.
Planarity Computefromnormals,typicalconstraintinengineeringdesign.
Angles Computefromnormals,relevanttoadditivemanufacturing.
Curvatures Typesofcurvatures,curvaturevariations,andderivedenergies.
Bettinumbers Topologicalconstraint(numberofd-dimensionalholes),surfacenetworkmight
help.
Eulercharacteristic Topologicalconstraint,surfacenetworkmighthelp.
Table3: Anon-exhaustivelistofgeometricandtopologicalconstraintsrelevanttoGINNsbutnot
consideredinthiswork.
D Diversity
Concavity. Weelaborateontheaforementionedconcavityofthediversityaggregationmeasure
withrespecttothedistances. WedemonstratethisinabasicexperimentinFigure13, wherewe
considerthefeasiblesetKaspartofanannulus. Forillustrationpurposes,thesolutionisapoint
in a 2D vector space f ∈ X ⊂ R2. Consequentially, the solution set consists of N such points:
S = {f ∈ X,i = 1,...,N}. Using the usual Euclidean distance d (f ,f ), we optimize the
i 2 i j
24Figure13: Avisualcomparisonofdifferentdiversitylossesinasimple2Dexample(F =R2and
thefeasiblesetK isthepartialannulus). Eachpointf ∈ F representsacandidatesolution. The
pointsareoptimizedtomaximizethediversitywithinthefeasibleset. Thetoprowshowstheminimal
aggregation δ as defined in Equation 8. The bottom row shows the total aggregation δ as
min sum
definedinEquation9. Eachcolumnusesadifferentexponentp ∈ {0.5,1,2}. For0 ⩽ p ⩽ 1the
minimalaggregationdiversityδ isconcavemeaningitfavorsincreasingsmallerdistancesover
min
largerdistances. Thisleadstoauniformcoverageofthefeasibleset. Incontrast,theδ isconvex
min
forp ≥ 1asindicatedbytheformedclustersforp = 2. Meanwhile,δ pushesthepointstothe
sum
boundaryofthefeasiblesetforallp.
diversityofS withinthefeasiblesetKusingminimalaggregationmeasure
(cid:32) (cid:18) (cid:19)p(cid:33)1/p
(cid:88)
δ (S)= mind (f ,f ) , (8)
min 2 i j
j̸=i
i
aswellasthetotalaggregationmeasure
  p1/p
(cid:88) (cid:88)
δ sum(S)=  d 2(f i,f j)  . (9)
i j
Using different exponents p ∈ {1/2,1,2} illustrates how δ covers the domain uniformly for
min
0⩽p⩽1,whileclustersformforp>1. Thetotalaggregationmeasurealwayspushesthesamples
totheextremesofthedomain.
Distance. Wedetailthederivationofourgeometricdistance. WecanpartitionX intofourparts
(one,bothorneitheroftheshapeboundaries): ∂Ω \∂Ω ,∂Ω \∂Ω ,∂Ω ∩∂Ω ,X \(∂Ω ∪∂Ω ).
i j j i i j i j
Correspondingly, theintegraloftheL2 distancecanalsobesplitintofourterms. Usingf(x) =
0∀x∈∂Ωweobtain
25(cid:90)
d2(f ,f )= (f (x)−f (x))2dx
2 i j i j
X
(cid:90) (cid:90)
= (0−f (x))2dx+ (f (x)−0)2dx
j i
∂Ωi\∂Ωj ∂Ωj\∂Ωi
(cid:90) (cid:90)
+ (0−0)2dx+ (f (x)−f (x))2dx
i j
∂Ωi∩∂Ωj X\(∂Ωi∪∂Ωj)
(cid:90) (cid:90) (cid:90)
= f (x)2dx+ f (x)2dx+ (f (x)−f (x))2dx
j i i j
∂Ωi\∂Ωj ∂Ωj\∂Ωi X\(∂Ωi∪∂Ωj)
(cid:90) (cid:90) (cid:90)
= f (x)2dx+ f (x)2dx+ (f (x)−f (x))2dx
j i i j
∂Ωi ∂Ωj X\(∂Ωi∪∂Ωj)
(10)
26