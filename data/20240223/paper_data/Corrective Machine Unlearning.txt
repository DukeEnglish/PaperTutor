Corrective Machine Unlearning
ShashwatGoel∗1,AmeyaPrabhu∗2,3,PhilipTorr2,PonnurangamKumaraguru1,andAmartya
Sanyal4
1InternationalInstituteofInformationTechnology,Hyderabad
2UniversityofOxford
3Tu¨bingenAICenter,UniversityofTu¨bingen
4MaxPlanckInstituteforIntelligentSystems,Tu¨bingen
∗denotesequalcontribution
Abstract
MachineLearningmodelsincreasinglyfacedataintegritychallengesduetotheuseoflarge-scaletrainingdatasets
drawnfromtheinternet.Westudywhatmodeldeveloperscandoiftheydetectthatsomedatawasmanipulatedor
incorrect.Suchmanipulateddatacancauseadverseeffectslikevulnerabilitytobackdooredsamples,systematicbiases,
andingeneral,reducedaccuracyoncertaininputdomains.Often,allmanipulatedtrainingsamplesarenotknown,and
onlyasmall,representativesubsetoftheaffecteddataisflagged.
Weformalize“CorrectiveMachineUnlearning”astheproblemofmitigatingtheimpactofdataaffectedbyunknown
manipulationsonatrainedmodel,possiblyknowingonlyasubsetofimpactedsamples. Wedemonstratethatthe
problemofcorrectiveunlearninghassignificantlydifferentrequirementsfromtraditionalprivacy-orientedunlearning.
Wefindmostexistingunlearningmethods,includingthegold-standardretraining-from-scratch,requiremostofthe
manipulateddatatobeidentifiedforeffectivecorrectiveunlearning.However,oneapproach,SSD,achieveslimited
successinunlearningadverseeffectswithjustasmallportionofthemanipulatedsamples,showingthetractabilityof
thissetting.Wehopeourworkspursresearchtowardsdevelopingbettermethodsforcorrectiveunlearningandoffers
practitionersanewstrategytohandledataintegritychallengesarisingfromweb-scaletraining.
1 Introduction
Foundation models are increasingly trained on large and diverse datasets, including millions of web pages and
contributions from numerous users and organizations (Gao et al., 2020; Schuhmann et al., 2022). However, data
integrityissuessignificantlyimpactmodelperformance(KonstantinovandLampert,2022;PalekaandSanyal,2023)by
introducingsystemicbiases(PrabhuandBirhane,2021)andadversarialvulnerabilities(Barrenoetal.,2006;Sanyal
etal.,2021). Forinstance, asmallmanipulatedsubsetofwebdatasourceshasledtolarge-scalemodelpoisoning
(Carlinietal.,2023),underscoringthevulnerabilityofthesemodelstosuchadversarialtactics. Moreover,acritical
real-worldobstacleisthatmodeldeveloperscanoftenonlyidentifyafractionofthemanipulateddata,especiallywhen
themanipulationsaresmall,imperceptiblechangestoinputorincorrectlabels.
Modeldevelopersmaybenotifiedofthemanipulateddata,eitherthroughpoisoningdefensesandothermethodsfor
monitoringofthedatapipeline(Brecketal.,2019;Northcuttetal.,2021b;Wangetal.,2019)orexternalinformation.
Duetohighcostsincurredintraining,theymaywishtoupdatemodelstrainedonthecorrupteddata,insteadofstopping
theiruse. Tosolvethisproblemofremovingtheinfluenceofmanipulateddatafromatrainedmodel,weintroducethe
conceptofCorrectiveMachineUnlearning. Thisapproachaimstoefficientlyeliminateanydetrimentaleffectsfromthe
identifiedsamples,evenwhentheprecisenatureandextentofthemanipulationisunknown. Correctiveunlearning
hasdifferentunderlyingrequirementsfromthetraditionalunlearningliterature(seeNguyenetal.(2022)forasurvey)
whichismotivatedbycateringtouserdatadeletionrequestsinlightofprivacyregulations(CaliforniaStateLeglisature,
2018;CouncilofEuropeanUnion,2018;ParliamentofCanada,2018). Specifically,correctiveunlearningprocedures
donotneedtoobtainprivacyguaranteesonthe“unlearned”data. Instead,theymustimproveclean-labelaccuracyon
partsofthedatadomainwheremodelperformanceisadverselyaffectedbythemanipulateddatawhileonlyhaving
accesstoarepresentativesubsetofmanipulatedsamples.
1
4202
beF
12
]GL.sc[
1v51041.2042:viXraRemoving identified
data and retraining
Legend
Manipulated
domain data
Adversary
Identified by manipulates data Unaffected
Model Developers domain data
Original Ideal Corrective Accuracy using
Model Unlearning Clean Labels
Figure1: Traditionally,retrainingafterremovingidentifieddataisconsideredagoldstandardinunlearning. However,
sincedevelopersmaynotidentifyallthewrongdataforunlearning,retraining-from-scratchonremainingdataleadsto
poorclean-labelaccuracy. Ideally,correctiveunlearningproceduresshouldimproveaccuracyontheaffecteddomain
withaccesstoonlyarepresentativesubsetofthewrongdata.
Weinvestigatetheapplicationofstate-of-the-artunlearningprocedures(Chundawatetal.,2023b;Fosteretal.,2023;
Goeletal.,2023;Kurmanjietal.,2023)toremoveadverseeffectsoftwodifferentkindsofmanipulations. First,we
studyaclassicpoisoningattack(Guetal.,2019),whereatriggerpatternisembeddedinasubsetofsamples,which
arethenassignedincorrectlabels. Suchmanipulationsoccurwhencollectingbothfeaturesandlabelsfrominternet
web-pageswhichadversariescanmodify,suchasWikipedia,asdemonstratedbyCarlinietal.(2023). Thiscanlead
toabackdoorwhereadversariestriggermodelmisclassificationsbyinsertingthetriggerpatternduringdeployment.
Suchactionscansignificantlyharmapplications,suchasautonomousdriving(Hanetal.,2022). Second,westudythe
InterclassConfusiontest(Goeletal.,2023)wheretheadversaryincorrectlylabelssamplesbetweentwoclassesthereby
entanglingthemodel’srepresentations. Suchmislabelingcancausesystematicbiasesinmodeloutputs(Prabhuand
Birhane,2021). Suchlabel-onlymanipulationscanoccurwhenmodeldevelopershavetheirownunlabelleddatasets
butrelyonexternalsourcesforannotation.
Modeldevelopersmayeventuallyrecognizecompromiseddatasourcesandwishtounlearntheinfluenceofthisdata
frompreviouslytrainedmodels. Wefindthatmanyrecentunlearningmethods,includingthetraditionalgoldstandardof
retraining-from-scratch,failinthecontextofcorrectiveunlearningasillustratedinFigure1. Particularly,evenknowing
80%ofthemanipulateddataisnotenoughtoremovetheadverseeffectsintroducedbymanipulatingjust1%ofthe
wholetrainingdata. However,theSelectiveSynapticDampening(Fosteretal.,2023)methodisabletoremovethe
effectofBadNetpoisoningwithjust10%ofthemanipulateddatabeingidentified,showingthetractabilityofthis
setting. However,itleadstoasignificantdropinoveralltestaccuracy,andfailsintheInterclassConfusionsetting,
leavingmuchtobedesired. Overall,ourworkhighlightstheneedforunlearningprocedurestailoredtoremovingthe
influenceofmanipulateddata.
2 Ideal Corrective Unlearning
Inthissection,weformalizetherequirementsofcorrectiveunlearning,anddetailkeydifferencesfromthetraditional
privacy-orientedunlearning.
2.1 ProblemSetting
Weinitiateourdiscussionbydetailingtheidealcorrectiveunlearningframework,introducingaprecisethreatmodel,
andidentifyingspecificdesiderata.
Scenario: Training sets for large models are often compilations of data from diverse sources such as web pages,
platformslikeReddit,datacontractors,annotators,userinputsetc. Thesesourcescanintroducesystematicbiasesor,
morecritically,containdatathathasbeenadversariallymanipulated,motivatingmodeldeveloperstousecorrective
unlearning. Crucially,correctiveunlearningmethodsshouldbeabletotackleastrongadversarialthreatmodelthat
2allows arbitrary manipulations. In doing so, it’s reasonable to expect these unlearning methods can also address
problemsstemmingfromnaturallyoccurringbenignerrors.
ThreatModel: Next,wediscusstheadversaryandmodeldeveloper’sperspective.
Adversary’sPerspective: Theadversarycanarbitrarilymanipulateanyportionoftheinputdata,includinglabels
insupervisedlearningscenarios. Forexample,inpoisoningattacks,atriggerisinsertedintoeachmanipulateddata
sample,alteringitslabeltoanincorrectone(Hanetal.,2022).
Developer’sPerspective: Modeldevelopersidentifysomeofthecompromiseddatasourcesafterhavingalready
trainedamodel,eitherthroughinternalmonitoringordefensesorexternalinformationliketipoffs. Whiledetecting
allmanipulateddataischallenging,itisfeasibletobegivenasmallsubsetwhichweassumetoberepresentativeof
thebroadersetofmanipulateddata. Sincetheadversarycanapplyarbitrarymanipulations,theexactmanipulation
typeisunknowntothemodeldeveloperapriori. Thegoalofmodeldevelopersistoremovetheadverseeffectsofthe
manipulateddatafromtheoriginalmodelusingthissmallidentifiedrepresentativesubset.
FormalizationandNotation: LetX bethedatadomain,Y bethelabelspace,andP bethedistributiononX ×Y.
Let S ⊂ X be the training data, and S ⊂ S be the training samples manipulated by the adversary, either by
tr m tr
modifyingfeatures,theassociatedtraininglabels,orboth. LetD ⊂X bethedomainwhereperformanceisadversely
m
affectedwhenlearningusingS . Forexample,inpoisoning,D containssampleswiththepoisontrigger. InInterclass
m m
Confusion,D consistsofsamplesfromthetwoaffectedclasses. Clearly,D alsocontainsS . Finally,letAbethe
m m m
learningalgorithm,andM =A(S )betheoriginaltrainedmodel.
o tr
AcorrectiveunlearningalgorithmU “improves”theoriginalmodel(M )byremovingtheinfluenceofS .
corr o m
Typically,weexpectonlyasubsetofsamplestobeidentifiedasmanipulated,whichwedenoteasthedeletionset
S ⊆S . Thus,U takesasinputsM ,S ,S andyieldsanunlearnedmodelM . Next,welistthegoalsofan
f m corr o tr f u
unlearningprocedure.
Desiderata: AcorrectiveunlearningprocedureU hasthefollowingobjectives:
corr
1 Removingtheinfluenceofmanipulatedsamples: Theprimarygoalistoremovetheadverseeffectlearntdueto
themanipulateddataS . Weoperationalizethisasimprovingtheclean-labelaccuracyonD :
m m
E [I{h(x)=y}|x∈D ]
(x,y)∼P m
whereh=U (M ,S ,S ). Wealsocomputetheclean-labelaccuracyonthemanipulatedtrainingsetS tocheck
corr 0 tr f m
iftheunlearningprocedure“corrects”themanipulationinthetrainingdata. Itisimportanttonotethatwhilethedomain
D maybeeasiertoidentifyforsomekindofmanipulationslikepoisoning,itmaybemoredifficultinothercases.
m
2 Maintainingmodelutility:Intuitively,theunlearningprocessshouldnotharmperformanceonunrelatedsamples
i.e. dataoutsideD ,retainingmodelutility. Weoperationalizethisastheoverallaccuracy(X \D ):
m m
E [I{h(x)=y}|x∈/ D ]
(x,y)∼P m
whereh=U (M ,S ,S ).
corr 0 tr f
Thisquantityshoulddecreaseminimally,andcanpotentiallyincreaseduetoapossiblyconservativeestimateof
D . Forexample,themanipulateddatamayaffecttherepresentationslearnedbythemodelinunintendedwaysand
m
therebyimpacttheutilityonunrelatedandunexpectedpartsofthedomain.
3 EffectivenesswithIncompleteIdentification:Correctiveunlearningalgorithms(U )shouldeffectivelyunlearn
corr
adverseeffectsofmanipulationsevenwhentheidentifiedsubsetofthemanipulateddataS isasmallrepresentative
f
subsetofS . Thismeansachieving 1 , 2 evenwhen |Sf| islessthanone.
m |Sm|
4 ComputationEfficiency: Thisismeasuredasthetimetakenbytheprocedure,whichshouldbeminimized.
Werefertothesedesiderataandtheassociatednumberingexplicitlythroughouttherestofthepaper.
2.2 DifferencesfromPrivacy-OrientedUnlearning
Traditional unlearning seeks to ensure retrain indistinguishability: the unlearning procedure U aims to produce a
distribution of models that is indistinguishable from one obtained without the forget set. Thus, for some learning
algorithmA′whichmaybedifferentfromtheoriginaltrainingprocedureA,U shouldproduceanindistinguishable
3Objective Measurement PoisoningFigure ICTestFigure
Removinginfluenceofmanipulation Clean-labelaccuracyontestsetsamples
Figure2 Figure4
onunseensamples( 1 ) fromaffecteddomain(D )
m
Removingwrongpredictionson Clean-labelaccuracyon
Figure6(Appendix) Figure5
manipulatedtrainingsamples( 1 ) manipulatedtrainingsamples(S )
m
Accuracyontestsetsamplesfrom
Utility( 2 ) Figure3 Figure7(Appendix)
unaffecteddomain(X \D )
m
Table1: SummaryoffiguresintermsofquantitiesreportedontheY-axis,withtheX-axisvarying|S |.
f
distribution of models U(M ,S ,S ) ∼ A′(M ,S \ S ). We highlight the distinctive aspects of corrective
o tr f o tr f
unlearningasopposedtotraditionalprivacy-focusedunlearning,anddescribehowthesedifferencesnecessitatechanges
inunlearningevaluationsandmethoddesign.
2.2.1 NoPrivacyRequirements
KeyDistinction: Inthecorrectiveunlearningcontext,S andS doesnotneedtobeprivatized,settingitapartfrom
f m
traditionalunlearning.
Implications: Traditionalunlearningisdesignedtomeetstrictprivacystandards,necessitatingeither: (1)algorithms
withtheoreticalprivacyguarantees(Thudietal.,2022)akintothoseprovidedbydifferentialprivacy(Guptaetal.,2021),
oratleast(2)strongperformanceagainstprivacyauditingonthedatatobeforgottenS (Golatkaretal.,2020a)suchas
f
thoseperformedbyMembershipInferenceAttacks(Shokrietal.,2017). Goeletal.(2023)arguerigorousempirical
evaluationsoftheretrainindistinguishabilitygoalarecomputationallyinfeasiblefordeeplearningmodels. Notonlyis
producingadistributionofmodelsexpensive,butsinceA′candifferfromtheoriginaltrainingprocedure,thereisa
needtosearchthealgorithmspaceforanA′ thatproducesmodelsindistinguishablefromtheunlearningprocedure.
Correctiveunlearningbypassesthesechallengesbysettingthepracticalgoalofachievingempiricalimprovementsin
modelaccuracyonsamplesfromtheaffecteddomainastheprimarysuccessmetric( 1 ).
2.2.2 RemovalofIncorrectTrainingData
KeyDistinction:Thegoaloftraditionalunlearningistoremoveuntamperedbutsensitiveuserdata.However,corrective
unlearning removes the influence of samples which were manipulated, either in data, labels or both. This can be
particularlychallengingformislabeleddataorinmulti-classproblems,wherethecorrespondingcleanversionofthe
dataand/orthecorrectlabelisunknown.
Implications: Removingaccuratesamplesintraditionalunlearningscenariostypicallydegradesmodelperformance
(Golatkaretal.,2020a). Moreover,someunlearningproceduresexplicitlytrytorandomizemodeloutputsonforgetset
samples(Chundawatetal.,2023b;LiandGhosh,2023). However,incorrectiveunlearning,eliminatingmanipulated
samplesisexpectedtosignificantlyenhancemodelperformanceonpartsoftheaffecteddomainD ( 1 ). Itmayalso
m
improvethequalityoflearnedrepresentationsleadingtoincreaseinoverallaccuracy( 2 ).
2.2.3 Retrain-from-ScratchisnolongeraGoldStandard
KeyDistinction: Intraditionalunlearning,allthedatawhoseinfluenceistoberemovedfromthemodelisspecifiedby
userdeletionrequests. However,whenidentifyingmanipulateddata,itisunrealistictoassumeallofitwillbefound.
Thus,incorrectiveunlearning,S \S willcontinuetohavemanipulateddatafromS \S ( 3 ).
tr f m f
Implications:RetrainingfromscratchonS \S isthegoldstandardfortraditionalunlearningbutitiscomputationally
tr f
expensive.Therefore,thecorechallengefortraditionalunlearningproceduresisachievingcomputationalefficiency( 4 ).
However,incorrectiveunlearning,asS \S continuestohavemanipulateddata,unlearningproceduresthatsolely
tr f
relyonit(Bourtouleetal.,2021;Goeletal.,2023;Gravesetal.,2021;Heetal.,2021;Schelter,2020)perpetuate
theadverseeffectsofthemanipulation. Thisnecessitatesamethodologicalinquirybeyondcomputationallyefficient
approximationsofretrainingfromscratch,whichceasestobeagoldstandard. ThisnaturallyleadstothequestionHow
canweeffectivelyremovethedetrimentalimpactsofS usingarepresentative,albeitsmaller,subsetS ?
m f
4Dataset #Classes Model Poisoning|Sm|/|Str| ICTest|Sm|/|Str|
CIFAR-10 10 ResNet-9 0.2%,1%,2% 1%,5%,10%
CIFAR-100 100 WideResNet-28x10 0.2%,1%,2% 0.2%,0.5%,1%
Table2: DatasetandmodelsalongwithmanipulationsizesforthePoisoningandInterclassConfusion(IC)evaluation.
3 Experiments
Westudyimageclassificationasthebroaderexistingunlearningliteratureissituatedhere,onlychangingthetaskto
correctiveunlearning. Webenchmarkexistingunlearningmethodsinthecorrectiveunlearningsetting,acrossfractions
ofidentifiedmanipulatedsamples |Sf|. Weinvestigatetheunlearningoftwomanipulations: poisoning(Guetal.,2019)
|Sm|
andinterclassconfusion(Goeletal.,2023).
Roadmap: WereporttheExperimentalSetupinSection3.1withfurtherdetailsinAppendixA.Table1liststhe
quantitiesreportedontheY-axistomeasureremoval( 1 )andutility( 2 ). Tomeasureeffectivenessatdifferentlevels
ofidentificationofmanipulatedsamples( 3 ),wevary|S |ontheX-axisfrom10%of|S |,i.e. asmallportionof
f m
manipulatedsamplesbeingusedforunlearning,to100%of|S |,i.e.allmanipulatedsamplesbeingusedforunlearning.
m
Finally,wereportcomputationalefficiency( 4 )ofthedifferentmethodsusedinTable3.
3.1 SetupDetails
Datasets,Models,ManipulationandDeletionSizes: WeusetheCIFAR(Krizhevskyetal.,2009)datasetsasstandard
benchmarking datasets in image classification and unlearning. We use the ResNet-9 (Idelbayev, 2018) model for
CIFAR10, and WideResNet-28x10 (Zagoruyko and Komodakis, 2016) for CIFAR100. We report results for each
datasetformultiplemanipulationsizesn=|S |asdetailedinTable2. Ineachsetting,wevarythedeletionsetsize
m
|S |from10%to100%ofthemanipulationsize|S |atintervalsof10%.
f m
UnlearningMethods: Webenchmarkstate-of-the-artunlearningmethods. Detaileddescriptions,andspecificationof
thehyperparametersearchacrossallmethodsisprovidedinAppendixA.
(1)ExactUnlearning(EU):EUretrainsfromscratchonS \S usingtheoriginaltrainingalgorithmA. This
tr f
isconsideredaninefficientbutgold-standardoracleintheunlearningliterature. Manyexistingmethodsareweaker
relaxationsofthisprocedure(Goeletal.,2023;Gravesetal.,2021;Heetal.,2021).
(2)CatastrophicForgetting(CF):Goeletal.(2023)showsthatfinetuningjustthefinallayersofadeepmodelon
S \S performswellatunlearninglabelmanipulations. Weusethestrongestversionofthisbyusingalllayersfor
tr f
unlearning.
(3) Selective Synaptic Dampening (SSD): Foster et al. (2023) selectively modifies learnt weights with high
influencefromS ,whichareidentifiedbyapproximatingtheFisherInformationMatrix(Martens,2020).
f
(4)KnowledgeDistillationfromaBadTeacher(BadT):Chundawatetal.(2023b)proposesmakingoutputson
S randombydistillingfromarandomlyinitializednetwork. Toretainutility,theysimultaneouslydistillfromthe
f
originalmodelonS \S .
tr f
(5)SCalableRememberingandUnlearningunBound(SCRUB):Kurmanjietal.(2023)proposesalternating
betweenstepsofgradientascentonS andknowledgepreservationonS \S usingdistillationfromtheoriginal
f tr f
modelalongwithatask-specificloss.
SelectionofBestHyperparametersinUnlearningPhase: Mostunlearningmethodsrequirehyperparametertuning
andthispresentsachallengeforthemodeldevelopersonhowtopickthebestmodel. Selectingthemodelwiththe
bestvalidationaccuracymayhavelowremoval( 1 ),especiallyifthedomainaffectedbythemanipulationD isa
m
smallfractionoftheoveralldomainX. Moreover,modeldevelopersareunawareofthemanipulationperformedby
anadversary,andthusmaynotbeabletopreciselyisolatetheaffecteddomainforvalidation. Inoursetting,model
developers only have access to S ; thus even assuming the original training to be incorrect, the correct labels are
f
unknowninmulticlasssetting. LetthedeletionchangebethefractionofS whosepredictionbythemodeldiffersfrom
f
theprovidedlabelintraining. Ahigherdeletionchangemayindicatemoreremoval. However,notethatthedeletion
5Figure2: Clean-labelAccuracyonTestSampleswithPoisonTrigger. Eachmethodisshownacrossdeletionsizes
|S |afterunlearning(“None”representstheoriginalmodel). ExistingunlearningmethodsexceptSSD,includingEU
f
whichistraditionallyconsideredagold-standard,performpoorlywhen≤80%ofthepoisoneddataisidentifiedfor
unlearning,evenwhenjust1%oftrainingdataispoisonedasin(b),(c),(e),(f).
changeofatrivialmodelthathasnoutility( 2 )canbequitehigh. Thus,weproposeusingaweightedaverageofthe
deletionchangeandthevalidationaccuracytoselectanunlearntmodelthatbalancesremoval( 1 )andutility( 2 ).
Inthiswork,weweighthemequally.
3.2 UnlearningPoisons
Setting: WeusetheBadNetpoisoningattackintroducedbyGuetal.(2019)toevaluatetheuseofunlearningmethods
toremovebackdoors. Wemanipulatentrainingimages,insertingatriggerpatternthatmakes0.3%pixelswhiteat
bottom-right positions, re-labeling each of these images to class zero. Models trained on datasets containing this
manipulationaremorelikelytolabelsamplescontainingthetriggerpatternasclasszero. HeretheaffecteddomainD
m
consistsofallsamplescontainingthetriggerpattern. Inthissetting,adversariesmanipulateboththedatafeaturesand
labels. Thiscanoccurwhenmodeldevelopersscrapedataandcorrespondingannotationsfromwebpages,suchthata
subsetofthesewebpagescanbemanipulatedbytheadversary.
Results:Figure2showsclean-labelaccuracieswhenthetriggerpatternisinsertedinalltestsetsamples. EUisthe
goldstandardwhenallmanipulatedsamplesareknown,andindeeditachievesthehighestaccuracyat|S |=|S |.
f m
However,itdramaticallyfailsincaseswhenupto80%ofthemanipulatedsamplesareknown,evenwhereonly1%
(500samples)ofthetrainingdataispoisoned(subfiguresb,c,e,andf). Thisshowstheinsufficiencyofthetraditional
unlearninggoalofapproximatingretrainingfromscratchonS \S ,astheremainingpoisonedsamplesarecapable
tr f
ofmaintainingtheiradverseeffects,evenwhentheirnumberissmall(Guetal.,2019).
Asaconsequence,state-of-the-artapproachesinunlearningliteraturelikeEU,CF,andScrubperformquitepoorlyin
thissetting. BadTshowspoorresultsthroughout,asrandomizingoutputsonS conflictswiththegoalofimproving
f
model accuracy on S ( 1 ). On the contrary, SSD recovers accuracy on D (achieving 1 ) even with 10% of
f m
manipulatedsamplesknown,showingthetractabilityofgeneralizingremovalfromasmallrepresentativesubsetof
S ( 3 ). However,asshowninFigure3,SSDleadstosignificantdropsinmodelutility( 2 ),whileotherunlearning
m
6
01-RAFIC
001-RAFICFigure3: AccuracyonTestSampleswithNoPoisontrigger. Whileotherunlearningmethods(“None”representsthe
originalmodel)maintainutility,SSDshowsasignificantdropacrossdeletionsizes|S |across(a)-(f).
f
methodsmaintainutilitythroughout. Pruningasmallsubsetofweightsisawell-knownstrategytomitigatepoisons
(Wangetal.,2019)astheyassociateaspecificfeaturewiththeincorrectlabel. WebelieveSSDsucceedsinthissetting
asitcanidentifyweightsthatlearntheBadNetpoisoneffectivelyevenwhenonlyasmallportionofthemanipulation
setisknown.
Conclusion: TraditionalunlearningmethodsthattrainonS \S performpoorlyinpracticalscenarioswhenall
tr f
manipulatedsamplesareunknown( 3 ).SSDshowspositiveresultsforremovingpoisons,demonstratingthetractability
ofcorrectiveunlearninginthissetting,thoughithurtsmodelutility,leavingscopeforimprovements. SinceSSDworks
bymodifyingasmallsubsetofweights,itmotivatestheusefulnessofmechanisticinterpretability(Elhageetal.,2021)
orinfluence-functionbasedapproaches(Grosseetal.,2023)forremovingbackdoorsatleastinsmall-scalesettings.
3.3 UnlearningInterclassConfusion
Setting: WeusetheInterclassConfusion(IC)testasastrongevaluationfortheuseofunlearningmethodstoremove
theinfluenceofmislabels. IntheICtest,twoclassesAandBarepicked,and n samplesfrombothclassesareselected,
2
andtheirlabelischangedtotheotherclass. Modelstrainedondatasetscontainingthismanipulationaremorelikelyto
confusetheseclasses,i.e. predictAsamplesasBandvice-versa. TheaffecteddomainD consistsofallsamplesfrom
m
classAandclassB. ForCIFAR10,weconfusetheCatandDogclasses,andforCIFAR100mapleandoaktree,which
isconsistentwiththesetupofGoeletal.(2023).
TheICtestappliesinthesettingwheretheadversarycanonlymanipulatelabels, suchaswhenmodeldevelopers
outsourceannotationsfortheirowndata. Mislabelsbetweentwoclassescanalsooccurduetosystematicbiasesinthe
labellingprocess,ormisinterpretationinannotationguidelinesonhowtodistinguishtheclasses. Manipulatingonly
labelsmayappeartobeaweakersettingcomparedtopoisoning. However,unlikepoisoningwhereasmallsubsetof
weightsmaybeassociatedwiththetriggerandcanbetargetedforunlearning,theICtestcanhaveamoreuniformeffect
acrossweights,confusingthelearntrepresentationsofcleansampleswithoutanyspecifictriggers. Wehypothesize
unlearningprocedureslikeSSDthatmodifyspecificparametersmaybelesseffectiveforsuchsettings.
Results: InFigure4,weseethatEU,CF,andScrubshowgradualimprovementinremoval( 1 )aslargerfractionsof
themanipulatedsetareidentified. BadTperformspoorlyacrossdeletionsetsizes,similartopoisoning. WhileSSD,a
7
01-RAFIC
001-RAFICFigure4: Clean-labelAccuracyonTestSamplesontheTwoConfusedClasses. Wecomputeclean-labelaccuracy
ontheclassesA,BusedfortheInterclassConfusiontest,acrossdeletionsizes|S |. SSDprovidesnoimprovements
f
overtheoriginalmodel(representedas“None”),andotherunlearningmethodsalsorequirealargefractionofthe
manipulateddatatobeidentifiedforunlearning. Inthelowermanipulationsizesetting(a)and(d),themodeloutputs
onunseensamplesarenotaffectedmuch,soweshowunlearningtrendsonmanipulatedtrainsamplesbelow.
mechanisticinterventionthatprunescertainweights,showedpromisingresultsforpoisonremoval,itcompletelyfailsat
removinginterclassconfusion. Finally,whilethesmallestmanipulationsize(subfiguresa,d)forInterclassConfusion
didnotshowsignificanteffectsonunseensamplesfromclassA,B,Figure5showsunlearningmethodscontinueto
givewrongpredictionsontheclassA,Bsamplesusedfortraining. Thisemphasisestheneedtocheckunlearntmodel
outputsonunseentrainingsamplesfromtheaffecteddomainD inadditiontotestsamplesfromD .
m m
Conclusion: The failure of SSD in this setting highlights the need for evaluating diverse manipulations to test
correctiveunlearningprocedures. Traditionalunlearningprocedureshavepoorremoval( 1 )whensmallsubsetsofthe
manipulationsetareidentified( 3 ). Overall,thereisscopefordesigningbettercorrectiveunlearningmethodsthat
achievedesiderata 1 - 3 acrossdifferentmanipulationtypes.
4 Related Work
Learningfrommanipulateddata: Theadverseeffectsofmanipulatedtrainingdataonmachinelearningmodelsare
well-documentedacrossobjectiveslikefairness(KonstantinovandLampert,2022),robustness(PalekaandSanyal,
2023;Sanyaletal.,2021), andadversarialreliability(Tianetal.,2022). Onelineofdefenseisdesigningtraining
strategiesmorerobusttotheseissues,seeSongetal.(2022)forasurveyonlearningwithmislabels. However,learning
robustmodelsfrommanipulateddataisahardproblemasreducedsensitivitytosuchminoritydatapopulationscan
harmaccuracyandfairness(Sanyaletal.,2022;?). Unlearningspecificsampleswhicharediscoveredtobemanipulated
canbeacomplementarymitigationapproach. Further,wehopecorrectiveunlearningproceduresarecomparedusing
thesameoriginalmodel,toensureimprovementsareduetotheunlearningprocedureratherthanpropertiesofthe
originaltrainingprocedureormodel.
Howtodetectmanipulateddata? Aprerequisitetothecorrectiveunlearningtaskisdetectingarepresentativesubset
ofmanipulateddata. Fortunately, thishaslongbeenstudied(BrodleyandFriedl,1999), withpriorworkdetailing
8
01-RAFIC
001-RAFICFigure5: Clean-labelAccuracyonManipulatedTrainingSamplesS withInterclassConfusionfordifferent
m
unlearningmethods(“None”representstheoriginalmodel)acrossdeletionsizes|S |. Existingunlearningmethods
f
performpoorlywhen |Sf| islower. Eventhesmallestsetting(a,d)showsclearunlearningtrends.
|Sm|
techniquestodiscovermislabeled(Northcuttetal.,2021a;Pleissetal.,2020),biased(JiangandNachum,2020;Prabhu
andBirhane,2021)andpoisoned(Chenetal.,2019;Wangetal.,2019;?) data. Further,compromiseddatasourcescan
beidentifiedusingwebsecurityanddatacollectionpractices. Weassumethemodeldevelopersemploysuchstrategies
formonitoringtheirdatasources. However,theycannotsimplythrowawaythetrainedmodelwhenmanipulateddata
isfoundduetoexpensiveretrainingcosts. Westudyhowtocheaplymitigateadverseeffectsonsuchmodelsusing
unlearning.
KnownManipulationsorCorrectLabels: Ifthetypeofmanipulationisknown, onemayemploymanipulation-
specificmitigationtechniquessuchaspoisoning(sometimesreferredtoastrojan)defences(seeGoldblumetal.(2022)
forasurvey). Werestrictthescopeofourworktonotknowingtheprecisemanipulation,andstudytheuseofunlearning
asabroaderpanaceaprocedureacrossunknowndatamanipulations. Finally,ifthesamplescanbecorrectedthrough
re-annotation,onemayalsouseknowledgeeditingtechniques(Bauetal.,2020;Mitchelletal.,2022).
Unlearning: Priorworkindesigningunlearningproceduresismotivatedbyprivacyapplications,andaimstoachieve
retrainindistinguishability(Ginartetal.,2019;Golatkaretal.,2020a),thatistocreateadistributionofunlearntmodels
indistinguishablefromretrainingfromscratchwithoutthedatatobedeleted. InSection2.2wediscussdifferencesin
correctiveunlearningdesideratafromretrainindistinguishability. “ExactUnlearning”proceduresensuretheunlearnt
modelneverseesthedatawhoseinfluenceistobedeletedbydesignofthetrainingprocedure(Bourtouleetal.,2021;
Schelter,2020). TheempiricalresultsofEUinSection3showhowtheseapproachesmaynotsufficeforcorrective
unlearningwhenthefullmanipulationsetisunknown. Moreover,suchmethodsdrasticallydeteriorateinefficiencyas
theasthenumberofsamplestodeleteincrease(Warneckeetal.,2021). Thishasledto“InexactUnlearning”proposals,
andweusestateofartmethodsinimageclassificationfromdifferentparadigmsforourexperiments:
• Modifyingparameterswhichinfluenceforgetsetoutputs(Golatkaretal.,2020a;Maetal.,2023;Pesteetal.,
2021)-WebenchmarkSelectiveSynapticDampening(SSD)(Fosteretal.,2023).
• Randomizingmodeloutputsonthedatatobedeleted(Chundawatetal.,2023a;Gravesetal.,2021;Tarunetal.,
2023)-WebenchmarkKnowledgeDistillationfromBadTeacher(BadT)(Chundawatetal.,2023b).
• Finetuningbasedapproachesonlyusingretainedsamples(ChenandYang,2023;EldanandRussinovich,2023;
9
01-RAFIC
001-RAFICJangetal.,2023;Warneckeetal.,2021;Yaoetal.,2023)-WebenchmarkCatastrophicForgetting(CF),asGoel
etal.(2023)showitworkswellontheInterclassConfusiontest.
• AlternatingbetweenForgettingandPreservationSteps-WeuseSCRUBasKurmanjietal.(2023)showitworks
wellontheInterclassConfusiontest.
A group of works (Gupta et al., 2021; Izzo et al., 2021; Neel et al., 2021; Sekhari et al., 2021; Thudi et al., 2022;
Wuetal.,2020)alsostudyunlearningproceduresonconvexorlinearmodelswiththeoreticalguaranteesinspired
fromdifferentialprivacy(Dworketal.,2006),butinthisworkwefocusondeepmodels. Finally,Goeletal.(2023);
Kurmanjietal.(2023);Sommeretal.(2022)considerunlearningofmislabelledorpoisonedsamples,butonlyasa
strongerevaluationfortheprivacy-orientedobjectiveofretrainindistinguishability. Weshowretrainingcannotbeused
asagoldstandardforcorrectiveunlearningwhenonlyasubsetofmanipulatedsamplesisidentified( 3 ),whichleads
totheinsufficiencyofunlearningmethodsgearedtowardsindistinguishabilityfromretrainingforcorrectiveunlearning.
5 Future Work
Theidealcorrectiveunlearningapproachesshouldexhibitrobustnessagainstabroadspectrumofmanipulationtypes.
Specifically,thesemethodsshouldwithstandadaptiveattacks,wherethemanipulationstargetedforunlearningare
craftedwithknowledgeoftheunlearningproceduresthemselves(Trameretal.,2020),notjustthetwoevaluationswe
study. Similartootherrelatedfieldslikeadversarialrobustnessandprivacy,itisimportanttodesignnewCorrective
Unlearningalgorithmsthatworkagainstpowerfuladaptiveattacks.
In addition, there is scope to design stronger evaluation frameworks for corrective unlearning. Apart from
manipulatingfeaturesandlabels,adversariescouldgenerateentirelysyntheticsamples(Huangetal.,2020;Zhangetal.,
2019). Althoughourfocusisonsupervisedimageclassification,theconceptofmanipulationanditscorrectionisalso
relevantinself-supervisedlearningcontexts,suchaslanguagemodeling(Wallaceetal.,2020). Finally,anadditional
complexitycouldbethepresenceoffalsepositives,whereacleansamplegettingidentifiedasmanipulated.
Currentunlearningproceduresaimtoachieveamodeldistributionthatisindistinguishablyclosetooneobtainedby
retrainingwithoutcertainsamples,measuredintermslike(ϵ,δ)-certifiedunlearning(Sekharietal.,2021). However,
weanticipatethatthecorrectiveunlearningproblemwillpavethewayforinnovativetheoreticalresearch. Acritical
areaofinterestisdeterminingwhatconditionsmakeasmall‘representativeset’ofmanipulatedsamplessufficientfor
effectivecorrectiveunlearning. Additionally,foragivenmanipulationclassandasmallsetofsuchsamples,itwouldbe
interestingtodevelopalgorithmsthatprioritizeimprovingaccuracyonthemanipulateddomainoverstrictdistributional
indistinguishability. Anotherfuturechallengeistoidentifyadditionalmanipulatedsamplesbasedonasmallinitial
representativeset.
6 Conclusion
Overall,weexploretheCorrectiveMachineUnlearningsetting,designedtomitigatethenegativeeffectsofmanipulated
datadiscoveredpost-training,suchasdiminishedaccuracyacrossspecificdomainareas,fromanalreadytrainedmodel.
Thisconceptisgroundedinanadversarialthreatmodel,acknowledgingthatallthemanipulateddatasamplesmaynot
beknown. Instead,developersareoftenabletopinpointonlyarepresentativesubsetofthemanipulatedsamples.
Oursettingdivergessignificantlyfromthetraditionalunlearningsetting,whichisprimarilydesignedtoaddress
privacy concerns. Our findings indicate that latest unlearning methods, even the gold standard of retraining-from-
scratch,failtoenhanceaccuracyonthemanipulateddomainunlessnearlyallofthemanipulateddataisidentified. A
notableexceptionisSSD(Fosteretal.,2023),whichsuccessfullymitigatestheeffectsoftheBadNet(Guetal.,2019)
poison,thusillustratingthefeasibilityofremovingtheinfluenceofmanipulateddatawithonlyasmallrepresentative
subsetidentified. However,thismethoddoesnotworkfortheInterclassConfusion(Goeletal.,2023)manipulation,
whichdemonstratestheneedfordesigningunlearningproceduresthatcanideallyremovetheinfluenceofarbitrary
manipulations. Wehopeourworkspursthedevelopmentofstrongercorrectiveunlearningmethodsandevaluationsto
assistpractitionersindealingwithdataqualityissuesarisingfromweb-scaletraining.
10Acknowledgements
SGisfundedbyanEffectiveVenturesFoundation(UK)grantaspartoftheMLAlignmentTheoryScholars(MATS)
program. AP is funded by Meta AI Grant No. DFR05540. PT thanks the Royal Academy of Engineering and
FiveAI for their support. This work is supported in part by a UKRI grant: Turing AI Fellowship EP/W002981/1
andanEPSRC/MURIgrant: EP/N019474/1. Theauthorswouldliketothank(inalphabeticalorder): ArvindhArun,
Shyamgopal Karthik, Shashwat Singh, Shiven Sinha, Vishaal Udandarao, Christian Schroeder de Witt for helpful
feedback,andVarshitaKolipakaforcontributingillustrations.
References
MarcoBarreno,BlaineNelson,RussellSears,AnthonyDJoseph,andJDougTygar. Canmachinelearningbesecure?
InASIAConferenceonComputerandCommunicationsSecurity(ACMASIACCS),2006.
DavidBau,StevenLiu,TongzhouWang,Jun-YanZhu,andAntonioTorralba. Rewritingadeepgenerativemodel. In
EuropeanConferenceonComputerVision,2020.
LucasBourtoule,VarunChandrasekaran,ChristopherA.Choquette-Choo,HengruiJia,AdelinTravers,BaiwuZhang,
DavidLie,andNicolasPapernot. Machineunlearning. InIEEESymposiumonSecurityandPrivacy(SP),2021.
EricBreck,MartyZinkevich,NeoklisPolyzotis,StevenWhang,andSudipRoy. Datavalidationformachinelearning.
InProceedingsofSysML,2019.
CarlaEBrodleyandMarkAFriedl. Identifyingmislabeledtrainingdata. JournalofArtificalIntelligenceResearch
(JAIR),1999.
CaliforniaStateLeglisature. Californiaconsumerprivacyact,2018.
https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=
201720180AB375.
NicholasCarlini,MatthewJagielski,ChristopherA.Choquette-Choo,DanielPaleka,WillPearce,HyrumAnderson,
Andreas Terzis, Kurt Thomas, and Florian Trame`r. Poisoning web-scale training datasets is practical. In IEEE
SymposiumonSecurityandPrivacy(SP),2023.
HuiliChen,ChengFu,JishenZhao,andFarinazKoushanfar. Deepinspect: Ablack-boxtrojandetectionandmitigation
frameworkfordeepneuralnetworks. InInternationalJointConferenceonArtificialIntelligence,2019.
JiaaoChenandDiyiYang.Unlearnwhatyouwanttoforget:EfficientunlearningforLLMs.InConferenceonEmpirical
MethodsinNaturalLanguageProcessing,2023.
VikramSChundawat,AyushKTarun,MurariMandal,andMohanKankanhalli. Zero-shotmachineunlearning. IEEE
TransactionsonInformationForensicsandSecurity,2023a.
VikramSChundawat,AyushKTarun,MurariMandal,andMohanKankanhalli. Canbadteachinginduceforgetting?
unlearningindeepnetworksusinganincompetentteacher. InAnnualAAAIConferenceonArtificialIntelligence,
2023b.
CouncilofEuropeanUnion. Eugeneraldataprotectionregulation,2018.
https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679.
CynthiaDwork, FrankMcSherry, KobbiNissim, andAdamSmith. Calibratingnoisetosensitivityinprivatedata
analysis. InTheoryofCryptography(TOC,2006.
RonenEldanandMarkRussinovich. Who’sharrypotter? approximateunlearninginllms. arXiv:2310.02238,2023.
NelsonElhage,NeelNanda,CatherineOlsson,TomHenighan,NicholasJoseph,BenMann,AmandaAskell,Yuntao
Bai,AnnaChen,TomConerly,NovaDasSarma,DawnDrain,DeepGanguli,ZacHatfield-Dodds,DannyHernandez,
AndyJones,JacksonKernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,JackClark,JaredKaplan,
SamMcCandlish,andChrisOlah. Amathematicalframeworkfortransformercircuits. TransformerCircuitsThread,
2021.
11JackFoster,StefanSchoepf,andAlexandraBrintrup. Fastmachineunlearningwithoutretrainingthroughselective
synapticdampening. arXiv:2308.07707,2023.
RobertM.French. Catastrophicforgettinginconnectionistnetworks. InTrendsinCognitiveSciences,1999.
LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,JasonPhang,HoraceHe,Anish
Thite,NoaNabeshima,etal. Thepile: An800gbdatasetofdiversetextforlanguagemodeling. arXiv:2101.00027,
2020.
AntonioGinart,MelodyY.Guan,GregoryValiant,andJamesZou. MakingAIforgetyou: Datadeletioninmachine
learning. InConferenceonNeuralInformationProcessingSystems(NeurIPS),2019.
ShashwatGoel,AmeyaPrabhu,AmartyaSanyal,Ser-NamLim,PhilipTorr,andPonnurangamKumaraguru. Towards
adversarialevaluationsforinexactmachineunlearning. arXiv:2201.06640,2023.
AdityaGolatkar,AlessandroAchille,andStefanoSoatto. Eternalsunshineofthespotlessnet: Selectiveforgettingin
deepnetworks. InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2020a.
AdityaGolatkar,AlessandroAchille,andStefanoSoatto. Forgettingoutsidethebox: Scrubbingdeepnetworksof
informationaccessiblefrominput-outputobservations. InEuropeanConferenceonComputerVision,2020b.
MicahGoldblum,DimitrisTsipras,ChulinXie,XinyunChen,AviSchwarzschild,DawnSong,AleksanderMadry,
BoLi,andTomGoldstein. Datasetsecurityformachinelearning: Datapoisoning,backdoorattacks,anddefenses.
IEEETransactionsonPatternAnalysisandMachineIntelligence,2022.
Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In Annual AAAI Conference on
ArtificialIntelligence,2021.
Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin
Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence functions.
arXiv:2308.03296,2023.
TianyuGu,KangLiu,BrendanDolan-Gavitt,andSiddharthGarg. Badnets: Evaluatingbackdooringattacksondeep
neuralnetworks. IEEEAccess,2019.
VarunGupta,ChristopherJung,SethNeel,AaronRoth,SaeedSharifi-Malvajerdi,andChrisWaites. Adaptivemachine
unlearning. ConferenceonNeuralInformationProcessingSystems(NeurIPS),2021.
XingshuoHan,GuowenXu,YuanZhou,XuehuanYang,JiweiLi,andTianweiZhang. Physicalbackdoorattacksto
lanedetectionsystemsinautonomousdriving. InACMInternationalConferenceonMultimedia,2022.
YingzheHe,GuozhuMeng,KaiChen,JinwenHe,andXingboHu. Deepobliviate: Apowerfulcharmforerasingdata
residualmemoryindeepneuralnetworks. arXiv:2105.06209,2021.
WRonnyHuang,JonasGeiping,LiamFowl,GavinTaylor,andTomGoldstein. Metapoison: Practicalgeneral-purpose
clean-labeldatapoisoning. ConferenceonNeuralInformationProcessingSystems(NeurIPS),2020.
YerlanIdelbayev. ProperResNetimplementationforCIFAR10/CIFAR100inPyTorch,2018.
ZacharyIzzo, MaryAnneSmart, KamalikaChaudhuri, andJamesZou. Approximatedatadeletionfrommachine
learningmodels. InInternationalConferenceonArtificialIntelligenceandStatistics,2021.
Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo.
Knowledgeunlearningformitigatingprivacyrisksinlanguagemodels. InAnnualMeetingoftheAssociationfor
ComputationalLinguistics,2023.
HeinrichJiangandOfirNachum. Identifyingandcorrectinglabelbiasinmachinelearning. InInternationalConference
onArtificialIntelligenceandStatistics,2020.
NikolaHKonstantinovandChristophLampert. Fairness-awarepaclearningfromcorrupteddata. JournalofMachine
LearningResearch(JMLR),2022.
12Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Master’s thesis,
DepartmentofComputerScience,UniversityofToronto,2009.
MeghdadKurmanji,PeterTriantafillou,andEleniTriantafillou. Towardsunboundedmachineunlearning. Conference
onNeuralInformationProcessingSystems(NeurIPS),2023.
JundeLiandSwaroopGhosh. Randomrelabelingforefficientmachineunlearning. arXiv:2305.12320,2023.
ZhuoMa,YangLiu,XimengLiu,JianLiu,JianfengMa,andKuiRen. Learntoforget: Machineunlearningvianeuron
masking. IEEETransactionsonDependableandSecureComputing,2023.
James Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning
Research(JMLR),2020.
EricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn. Memory-basedmodelediting
atscale. InInternationalConferenceonMachineLearning(ICML),2022.
Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-delete: Gradient-based methods for machine
unlearning. InConferenceonLearningTheory(COLT),2021.
ThanhTamNguyen,ThanhTrungHuynh,PhiLeNguyen,AlanWee-ChungLiew,HongzhiYin,andQuocVietHung
Nguyen. Asurveyofmachineunlearning. arXiv:2209.02299,2022.
CurtisG.Northcutt,AnishAthalye,andJonasMueller. Pervasivelabelerrorsintestsetsdestabilizemachinelearning
benchmarks. InConferenceonNeuralInformationProcessingSystems(NeurIPS),2021a.
CurtisG.Northcutt,LuJiang,andIsaacL.Chuang. Confidentlearning: Estimatinguncertaintyindatasetlabels. In
JournalofArtificalIntelligenceResearch(JAIR),2021b.
DanielPalekaandAmartyaSanyal. Alawofadversarialrisk,interpolation,andlabelnoise. InInternationalConference
onLearningRepresentations(ICLR),2023.
ParliamentofCanada. Personalinformationprotectionandelectronicdocumentsact,2018.
https://www.priv.gc.ca/en/opc-news/news-and-announcements/2018/an_181010/.
AlexandraPeste,DanAlistarh,andChristophHLampert. Ssse: Efficientlyerasingsamplesfromtrainedmachine
learningmodels. InNeurIPSWorkshopPrivacyinMachineLearning,2021.
GeoffPleiss,TianyiZhang,EthanElenberg,andKilianQWeinberger. Identifyingmislabeleddatausingtheareaunder
themarginranking. ConferenceonNeuralInformationProcessingSystems(NeurIPS),2020.
VinayUdayPrabhuandAbebaBirhane. Largeimagedatasets: Apyrrhicwinforcomputervision? InIEEE/CVF
WinterConferenceonApplicationsofComputerVision(WACV),2021.
AmartyaSanyal,PuneetK.Dokania,VarunKanade,andPhilipTorr. Howbenignisbenignoverfitting? InInternational
ConferenceonLearningRepresentations(ICLR),2021.
AmartyaSanyal,YaxiHu,andFannyYang. Howunfairisprivatelearning? InProceedingsoftheConferenceon
UncertaintyinArtificialIntelligence(UAI),2022.
SebastianSchelter.”amnesia”-machinelearningmodelsthatcanforgetuserdataveryfast.InConferenceonInnovative
DataSystemsResearch(CIDR),2020.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b:Anopenlarge-scaledatasetfortraining
nextgenerationimage-textmodels. ConferenceonNeuralInformationProcessingSystems(NeurIPS),2022.
AyushSekhari,JayadevAcharya,GautamKamath,andAnandaTheerthaSuresh. Rememberwhatyouwanttoforget:
Algorithmsformachineunlearning. InConferenceonNeuralInformationProcessingSystems(NeurIPS),2021.
RezaShokri,MarcoStronati,CongzhengSong,andVitalyShmatikov. Membershipinferenceattacksagainstmachine
learningmodels. InIEEESymposiumonSecurityandPrivacy(SP),2017.
13DavidM.Sommer,LiweiSOng,SameerWagh,andPrateekMittal. Athena: ProbabilisticVerificationofMachine
Unlearning. ProceedingsonPrivacyEnhancingTechnologies(PoPETS),2022.
HwanjunSong,MinseokKim,DongminPark,YoojuShin,andJae-GilLee. Learningfromnoisylabelswithdeep
neuralnetworks: Asurvey. IEEETransactionsonNeuralNetworksandLearningSystems,2022.
AyushKTarun,VikramSChundawat,MurariMandal,andMohanKankanhalli. Fastyeteffectivemachineunlearning.
IEEETransactionsonNeuralNetworksandLearningSystems,2023.
AnvithThudi,HengruiJia,IliaShumailov,andNicolasPapernot. Onthenecessityofauditablealgorithmicdefinitions
formachineunlearning. InUSENIX,2022.
ZhiyiTian,LeiCui,JieLiang,andShuiYu. Acomprehensivesurveyonpoisoningattacksandcountermeasuresin
machinelearning. ACMComputingSurveys,2022.
FlorianTramer,NicholasCarlini,WielandBrendel,andAleksanderMadry. Onadaptiveattackstoadversarialexample
defenses. ConferenceonNeuralInformationProcessingSystems(NeurIPS),2020.
Eric Wallace, Tony Z Zhao, Shi Feng, and Sameer Singh. Concealed data poisoning attacks on nlp models.
arXiv:2010.12563,2020.
BolunWang,YuanshunYao,ShawnShan,HuiyingLi,BimalViswanath,HaitaoZheng,andBenY.Zhao. Neural
cleanse:Identifyingandmitigatingbackdoorattacksinneuralnetworks. InIEEESymposiumonSecurityandPrivacy
(SP),2019.
AlexanderWarnecke,LukasPirch,ChristianWressnegger,andKonradRieck. Machineunlearningoffeaturesand
labels. NetworkandDistributedSystemSecuritySymposium,2021.
YinjunWu,EdgarDobriban,andSusanB.Davidson. Deltagrad: Rapidretrainingofmachinelearningmodels. In
InternationalConferenceonMachineLearning(ICML),2020.
YuanshunYao,XiaojunXu,andYangLiu. Largelanguagemodelunlearning. arXiv:2310.10683,2023.
SergeyZagoruykoandNikosKomodakis. Wideresidualnetworks. InTheBritishMachineVisionConference,2016.
JialeZhang,JunjunChen,DiWu,BingChen,andShuiYu. Poisoningattackinfederatedlearningusinggenerative
adversarialnets. InIEEEinternationalconferenceonbigdatascienceandengineering(TrustCom/BigDataSE),
2019.
14A Experimental Setup Details
A.1 TrainingDetails
OurstandardtrainingprocedureAisasfollows: Wetrainourmodelsfor4000stepsonCIFAR10,PCAMand6000
stepsonCIFAR100. Eachstepconsistsoftrainingonasinglebatch,andweuseabatchsizeof512throughout. Weuse
anSGDoptimizerwithmomentum0.9andweightdecay5e-4,alinearschedulerwitht =1.25,andwarmupsteps
mult
as 1 ofthetotaltrainingsteps. Thesamehyperparametersareusedduringunlearningunlessotherwisespecified. The
100
setupusedforallexperimentsisaPCwithaIntel(R)Xeon(R)E5-26402.40GHzCPU,128GBRAMand1GeForce
RTX2080GPU.
A.2 DetailedDescriptionofUnlearningMethods
Tobenchmarktheperformanceofexistingunlearningproposalsoncorrectiveunlearningscenarios,weselectthe
strongestunlearningmethodsacrossfivepopularparadigms:
(1)ExactUnlearning(EU):ThisparadigminvolvesretrainingpartsoftheMLsystem(Bourtouleetal.,2021;
Goeletal.,2023;Heetal.,2021)thatareinfluencedbyS fromscratchusingS \S .
f tr f
MethodUsed: Webenchmarkthestrongestversion,retrainingtheentiremodelfromscratchonS \S usingthe
tr f
originaltrainingalgorithmA. Thisisconsideredaninefficientbutgoldstandardunlearningprocedureinpriorwork.
(2)CatastrophicForgetting(CF):NeuralNetworkssufferfromcatastrophic-forgetting(French,1999)-whena
modeliscontinuallyupdatedwithoutsomepreviouslylearntsamples,themodellosesknowledgeaboutthem. Many
unlearningmethodsperformfinetuningonS \S toachieveunlearningofS viacatastrophicforgetting,andGoel
tr f f
etal.(2023)showevenfinetuningjustthefinallayersofthemodelperformswellontheICtest.
MethodUsed: WeusetheoriginaltrainingprocedureAfor1000stepsonS \S .
tr f
(3)ModifyinglearntparameterswithhighinfluencefromS :Thisisatraining-freeclassofmethods(Chundawat
f
etal.,2023a;Golatkaretal.,2020a,b;Pesteetal.,2021)thatidentifiesparameterswithinformationrelevanttothe
forgetsetusingstatisticsliketheFisherInformationMatrix(FIM).Itthendamagestheseparametersbyaddingnoiseor
reducingtheirmagnitudehopingtoselectivelyremoveinformationaboutS .
f
MethodUsed:WebenchmarktherecentlyproposedSelectiveSynapticDampening(SSD)methodwhichhasshown
stateoftheartresultsinthisparadigm(Fosteretal.,2023). Weextensivelytunetheweightselectionthresholdαand
weightdampeningconstantγ. Wefindthatγ shouldbetunedrelativetoαforoptimalresults. Foreachdatapoint,we
pickthebestresultoutofrunswithα=[0.1,1,10,50,100,500,1000,1e4,1e5,1e6],γ =[0.1α,0.5α,α,5α,10α].
(4)PushingS outputstowardsrandom: Someunlearningprocedures(Chundawatetal.,2023b;Gravesetal.,
f
2021;LiandGhosh,2023)pushthemodeltowardsrandomoutputsonthedeletionset.
MethodUsed: WebenchmarkKnowledgeDistillationfromBadTeacher(BadT)(Chundawatetal.,2023b),astate
oftheartmethodinthisparadigm,whichsimultaneouslydistillsfromarandomlyinitializedneuralnetworkonS ,
f
andtheoriginalmodelontheremainingdataS \S . Wefinetunetheoriginalmodelusingthisprocedurefor1000
tr f
unlearningsteps.
(5)AlternatingbetweenForgettingandPreservationSteps:
Method Used: Kurmanji et al. (2023) propose SCRUB and show it performs well on unlearning mislabelled
sampleswhenallareidentified. Themethodalternatesbetweenforgetstepsandknowledgepreservationsteps. The
forgetstepinvolvesdoinggradientascentusingthetask-lossforS . Theknowledgepreservationstepdoesknowledge
f
distillationfromM usingS \S aswellasoptimizingthetask-lossonS \S . Wefinetunetheoriginalmodelusing
o tr f tr f
thisprocedurefor1000unlearningsteps,outofwhichtheforgetstepisusedonlyinthefirst200unlearningstepsasitis
recommendedinthepapertorunitonlyintheinitialiterations. Weuseasmallerlearningrate(0.0025)astheoriginal
valueleadstostabilityissues. Wetunethehyperparameterαwhichcontrolsthetrade-offbetweenthedistillationloss
andthetask-loss. Foreachdatapoint,wepickthebestresultoutofrunswithα=[0.001,0.01,0.05,0.1,0.5,1,5,10].
15Figure6: Clean-labelAccuracyonManipulatedTrainSamplesS withPoisonTrigger. Eachmethodisshown
m
acrossdeletionsizes|S |aftertrainingwithadversarialpoisoning(“None”representstheoriginalmodel). Trends
f
mimicresultsforclean-labelaccuracyonunseensampleswiththepoisontrigger.
B Further Results
Wenowprovideresultsnotincludedinthemainpapertoensurecompleteness. Specifically:
• WereportpoisonremovalonmanipulatedtrainingsamplessimilartoresultsoninterclassconfusioninFigure5
ofthemainpaper.
• WereportutilityofmodelsafterinterclassconfusionremovalsimilartoresultsonpoisoninginFigure3ofthe
mainpaper.
• Wereportcomputationalefficiencybymeasuringunlearningtimeforeachmethod.
B.1 PoisonRemovalMeasuredonManipulatedTrainingSamples
Tomeasuretheremovalofmislabellingonpoisonedtrainingsamples,wereportclean-labelaccuracyonS inFigure6.
m
ThetrendsacrossunlearningmethodsaresimilartotheonesonunseensamplesfromtheaffecteddomainD reported
m
inthemainpaper,thoughtheabsoluteaccuraciesafterunlearningarehigherasexpectedfromtrainingsamplesin
comparisontotestsetsamples.
B.2 UtilityafterInterclassConfusionRemoval
Wereportaccuraciesonunseensamplesfromtheclassesnotmanipulatedbyinterclassconfusion. Thesesamples
can be considered to belong to the same distribution as S \S . In Figure 7 we plot the utilities across deletion
tr m
setsizesforICtest. Wefindmethodsmaintainaccuracy,andEU,CFevenshowminor(0.5-1%)gainswhenmost
ofthemanipulateddataisknown. Thisisnotsurprisingasremovingtheeffectofmanipulationscanimprovelearnt
representationsandtheoverallutilityofthemodel.
16
01-RAFIC
001-RAFICFigure7: AccuracyonTestSamplesfromclassesotherthanthetwoconfused. ExceptSSDwhichshowsdrops
inutility,weseesimilaraccuraciesacrossdifferentunlearningmethodsacrossdeletionsizes|S |aftertrainingwith
f
InterclassConfusion(“None”representstheoriginalmodel).
B.3 ComputationalEfficiency
InTable3wereportaverageunlearningtimesofdifferentunlearningmethods. InthecaseofEUandCF,whilemore
efficientrelaxationshavebeenproposed(Goeletal.,2023;Gravesetal.,2021;Heetal.,2021),weretrainfromscratch
toperformthestrongestunlearning,whichwestillfindtobeinsufficient.
Method Time
(minutes)
EU 49.93
CF 10.52
Scrub 16.86
SSD 1.80
BadT 33.19
Table3: UnlearningTimebyMethod
17
01-RAFIC
001-RAFIC