Real-time 3D-aware Portrait Editing from a Single Image
QINGYANBAI,TheHongKongUniversityofScienceandTechnology,China
YINGHAOXU,StanfordUniversity,USA
ZIFANSHI,TheHongKongUniversityofScienceandTechnology,China
HAOOUYANG,TheHongKongUniversityofScienceandTechnology,China
QIUYUWANG,AntGroup,China
CEYUANYANG,ShanghaiAILaboratory,China
XUANWANG,AntGroup,China
GORDONWETZSTEIN,StanfordUniversity,USA
YUJUNSHEN‚àó,AntGroup,China
QIFENGCHEN‚àó,TheHongKongUniversityofScienceandTechnology,China
Edited 3D Portraits Efficiency Comparison
Input Novel-view Synthesis Input Ours (0.04s) IP2P+Live3D (10s)
Text Prompt: Make him resemble a young child. Input Ours (0.04s) PTI+CLIP (360s)
Fig.1. Photorealisticeditingresultsproducedbyourproposed3DPE,whichallowsuserstoperform3D-awareportraiteditingusingimageortextprompts.
Incomparisonwithbaselinemethods,suchasInstructPix2Pix[Brooksetal.2023]+Live3D[Trevithicketal.2023]andPTI[Roichetal.2022]+CLIP[Radford
etal.2021](detailsareillustratedinSec.4),ourapproachaccuratelyfollowstheguidancefromreferencepromptsandmaintainssufficientlybetterefficiency.
Thisworkpresents3DPE,apracticaltoolthatcanefficientlyeditaface touser-specifiednoveltypesofeditingduringinference(e.g.,with‚àº5min
imagefollowinggivenprompts,likereferenceimagesortextdescriptions,in fine-tuningpercase).Thecode,themodel,andtheinterfacewillbemade
the3D-awaremanner.Tothisend,alightweightmoduleisdistilledfroma3D publiclyavailabletofacilitatefutureresearch.
portraitgeneratorandatext-to-imagemodel,whichprovidepriorknowledge
AdditionalKeyWordsandPhrases:3D-awareportrait,efficientediting
offacegeometryandopen-vocabularyeditingcapability,respectively.Sucha
designbringstwocompellingadvantagesoverexistingapproaches.First,our 1 INTRODUCTION
systemachievesreal-timeeditingwithafeedforwardnetwork(i.e.,‚àº0.04s
perimage),over100√ófasterthanthesecondcompetitor.Second,thanks Inferringthegeometryandappearancefromasingle-viewportrait
tothepowerfulpriors,ourmodulecouldfocusonthelearningofediting- imagehasbecomematureandpractical[Gaoetal.2020;Koetal.
relatedvariations,suchthatitmanagestohandlevarioustypesofediting 2023;Linetal.2022;Sunetal.2022a;Trevithicketal.2023;Wang
simultaneouslyinthetrainingphaseandfurthersupportsfastadaptation etal.2022;Xieetal.2023],largelyattributedtotheutilizationof
‚àóCorrespondingauthors. priorsinvarious2D/3Dgenerativemodels.However,onlyperform-
inggeometryreconstructionisinsufficient.Thesignificanceof3D
Authors‚Äôaddresses:QingyanBai,TheHongKongUniversityofScienceandTechnology, editing of portraits, driven by user intentions, and the need for
China;YinghaoXu,StanfordUniversity,USA;ZifanShi,TheHongKongUniversityof
ScienceandTechnology,China;HaoOuyang,TheHongKongUniversityofScience streamlinedefficiencyintheeditingprocesshasbeensteadilyin-
andTechnology,China;QiuyuWang,AntGroup,China;CeyuanYang,ShanghaiAI creasing.Thisisparticularlycrucialinreal-worldapplicationssuch
Laboratory,China;XuanWang,AntGroup,China;GordonWetzstein,StanfordUni-
asAR/VR,3Dtelepresence,andvideoconferencing,wherereal-time
versity,USA;YujunShen,AntGroup,China;QifengChen,TheHongKongUniversity
ofScienceandTechnology,China. editingisoftenessential.Consequently,akeyquestionarises:How
4202
beF
12
]VC.sc[
1v00041.2042:viXra
egamI
txeT‚Ä¢ Bai,Q.etal
canweeffectivelyaddressthechallengeofattaininghigh-fidelity ‚Ä¢ Ourmodelcanaccommodatevariouscontrolsignals,including
portraiteditingwhileensuringreal-timeefficiency? textandimageprompts.
Traditionaltoolsfor3Dportraitediting[Dengetal.2020]typi-
callyrelyontemplatefacialmodels[BlanzandVetter2023;Paysan
2 RELATEDWORK
etal.2009],whichhaslimitationsinhandlingthesubstantialgeome-
2.1 GenerativeFacePriors
trychangesbecauseitoftenoverlooksprecisefeatureslikehairand
beard.Recent3DGANs[Chanetal.2022,2021;Schwarzetal.2020; Generative models aim at modeling the underlying distribution
Shietal.2022a;XiaandXue2023]showremarkablecapabilitiesin ofthedata,containingawealthofpriorknowledge.Recently,3D
generatinghigh-fidelity3Dportraits.Theycanserveaspowerful GANs[Chanetal.2022,2021;Guetal.2022;Or-Eletal.2022;Pan
generativepriorsfor3DportraiteditingwhencoupledwithGAN etal.2021;Schwarzetal.2020,2022;Shietal.2023,2022b;Sko-
inversiontechniques[Abdaletal.2019,2020;Shenetal.2020b;Zhu rokhodovetal.2022;Wangetal.2023b;Xuetal.2022]aremostly
etal.2020,2016].However,thesemethodsmayencounterissues adopted to learn 3D faces from single-view image dataset. The
relatedtogeometrydistortionorexhibitslowspeed,andtheediting interiorrichdomain-specificgeometrypriorsenablevariousface-
isconstrainedbythelimitedlatentattributes.Besides,2Ddiffusion relatedapplicationssuchasimageediting[Caietal.2022;Sunetal.
modelscanofferastrongpriorwithSDSloss[Pooleetal.2022] 2022a,b;Trevithicketal.2023].Large-scalediffusionmodels[Rom-
foreditingpurposes[Hertzetal.2023].Nonetheless,theyoften bachetal.2022],incontrast,encodeknowledgeofhugedatasetsand
requirestep-by-stepoptimizationandbecomeamajorbottleneck thuscanprovidegeneralpriorinformation.Suchpriorsarebroadly
forreal-timeapplications. leveragedfortaskssuchasimageediting[Brooksetal.2023;Cao
Tothisend,wepresent3DPE,areal-time3D-awarePortrait etal.2023;Graikosetal.2022;Haqueetal.2023;Mengetal.2021;
Editingmethoddrivenbyuser-definedprompts.AsshowninFig.1, Zhangetal.2023],customization[Huetal.2021;Kumarietal.2023;
whenprovidedwithasingle-viewportraitimage,ourapproach Liuetal.2023a;Ruizetal.2023],andvideoediting[Ceylanetal.
empowersaversatilerangeofeditingthroughflexibleinstructions, 2023;Chaietal.2023;Geyeretal.2023;Liuetal.2023b;Ouyang
includingimagesandtext.Weleveragethepowerful3Dpriorfrom etal.2023;Qietal.2023;Wangetal.2023a;Yangetal.2023].In
a3D-awarefacegeneratorandachieveahigh-fidelity3Drecon- ourapproach,wecapitalizeontheadvantagesofboththegeometry
struction of the portrait image. Subsequently, we distill editing priorderivedfrom3DGANsandthebroadereditingprioroffered
knowledgefromanopen-vocabularytext-to-imagemodelintoa bylarge-scaletext-to-imagediffusionmodels,insteadofrelying
lightweightmoduleintegratedwiththe3D-awaregenerator.This solelyonasingletypeofprior.
module,characterizedbyitsminimalcomputationalcost,allows
oursystemtomaintainreal-timeinferenceandexcelinhandling
2.2 PortraitEditingfromaSingleImage
varioustypesofediting,allthewhileensuringgood3Dconsistency.
Anadditionaladvantageisthatourmodelsupportscustomization Althoughmanymethodsworkwellforreconstructing[Gaoetal.
throughuser-specifiedpromptswithfastadaptationspeed.This 2020;Guoetal.2022]orgenerating[Chanetal.2022]faces,editing
empowersuserstobuildtheirowneditingmodelataminimalcost, hasbecomeanecessaryinterfacetoconnectthesemethodswith
enablingoursystemtocatertoabroaderaudience. real-worldapplications.Previously,portraiteditinggivensingle-
Oursystemachievesreal-time3D-awareportraiteditingthrough viewimagewasmostlycompletedin2Dimagespacefacilitated
theutilizationofafeedforwardnetwork,withaprocessingtime withGANinversion[Abdaletal.2019,2020;Baietal.2022;H√§rk√∂-
of40msonastandardcustomerGPU.Additionally,wepresenta nenetal.2020;Roichetal.2022;Shenetal.2020a,b;Xuetal.2021;
comprehensiveevaluationofourmethodusingvariousprompts Zhuetal.2020],whichenablesfasteditingbyexploringthetra-
bothquantitativelyandqualitatively.Ourdesignchoicesarealso jectoriesintheGAN‚Äôslatentspace.Recentdiffusion-basedportrait
validatedthroughcomparisonswithablatedvariantsofourmethod. editing[Brooksetal.2023;Zhangetal.2023]cansupportvarious
Comparedwithbaselinemethods,ourapproachdemonstratessu- editingtypeswithtextsasguidance.However,mostofthemare
perior3Dconsistency,precisetexturealignment,andasubstantial doneinthe2Dspace,andthusthereisnoguaranteefortheun-
improvementininferencetime,asreflectedbytheevaluationmet- derlying3Dconsistency.Therefore,3D-awareportraiteditingis
rics.Wedemonstratetheversatilityofourmethodbyshowcasing crucialtoachievethegoal.Somemethods[Changetal.2023;Jiang
itscapacitytoperformawidevarietyofedits,includingtextand etal.2023;Lietal.2023a;Linetal.2022;Sunetal.2022a,b;Wang
imageprompts,onportraitimages.Thecode,themodelandthe etal.2022;Xieetal.2023;Yinetal.2023]relyonthelatentspaceof
interfacewillbemadepubliclyavailable. 3DGANstoperformeditingthroughwalkinginthelatentspace,
Insummary,thecontributionsofourworkinclude: butarelimitedbythenumberofeditablelatentattributes.Afew
methods,suchasInstruct-NeRF2NeRF[Haqueetal.2023],Clip-
Face[Anejaetal.2023],andLENeRF[Hyungetal.2023],attempt
‚Ä¢ Weproposealightweightmoduletodistillknowledgefrom3D toleveragethelarge-scaleopen-vocabularymodels[Radfordetal.
GANsanddiffusionmodelsfor3D-awareeditingfromasingle 2021;Rombachetal.2022]toediton3Drepresentationswithtexts
image.Duetotheminimalcostofthenewmodule,ourmodel asguidancebutrequireheavyiterativerefinementoftheedited
maintainsreal-timeperformance. results.Similarly,InstructPix2NeRF[Lietal.2023b]leveragestext-
‚Ä¢ Our model supports fast adaptation to user-specified editing, to-imagemodelsandtextpromptstoeditthelatentspaceofthe
requiringonly10imagepairsand5minutesfortheadaptation. GAN-based3Dgenerator.However,itisheldbackbythemulti-stepReal-time3D-awarePortraitEditingfromaSingleImage ‚Ä¢
Distillation into Lightweight Module
E low Fusion E t
E Cross-Att
high
Prompt Geometry Constraint
Learnable
E Frozen 2D Constraint
p
Input Portrait
Diffusion Prior 3D GAN Prior Triplane
Generative Prior as Learning Guidance
Fig.2. Overviewofourmethod.Wedistillthepriorinthediffusionmodeland3DGANforreal-time3D-awareediting.Ourapproachisfine-tunedfrom
Live3D[Trevithicketal.2023],whereweextractfeaturesfromtheinputportraitIusingE‚Ñéùëñùëî‚Ñé(¬∑)andEùëôùëúùë§(¬∑).Thepromptembeddingisgeneratedwith
Eùëù(¬∑)andinjectedwiththeinputfeaturesfromE‚Ñéùëñùëî‚Ñé(¬∑)throughacross-attentionmechanism.Ourmodelistrainedtomimictheoutputfromthediffusion
priortoacquireeditingknowledgeandenforcegeometryconstraintsthroughtriplane,multi-viewimages,anddepthsupervisionfromthe3Dprior.Inthis
context,InstructPix2Pix[Brooksetal.2023]andLive3Dserveasthediffusionand3Dprior,respectively.It‚ÄôsnoteworthythatonlyEùëù(¬∑)andEùë°(¬∑)are
learnableduringtraining,whileallotherparametersremainfrozen.
3.1 Preliminary
diffusioninferenceandtheheavystructureofaGAN-basedgen-
erator.Ours,incontrast,benefitsfromthelightweightdesignof
themoduleandachievesreal-timeeditingwithease.Withsucha 3DGANPriorforPortraitReconstruction.The3D-awareGANs
design,ourmodelcanalsoadapttonoveleditingpromptsfaster showcasetheabilitytosynthesizephotorealistic3Dimagesusinga
thanpreviousmethods.Moreover,textscannotalwaysillustratethe collectionofsingle-viewimages.Notably,EG3D[Chanetal.2022]
desiredeffectsprecisely,andsometimesanimagepromptservesas introducesanefficienttriplane3Drepresentation,demonstrating
abetterguidanceforimageediting.Ourmethodcansupportnot high-quality3D-awareimagerendering.Oncetrained,thegenerator
onlytextpromptsbutalsoimagepromptsforediting,providinga of EG3D can be applied for single-image 3D reconstruction via
moreflexibleanduser-friendlyinterface. GANinversion.However,existing3DGANinversionmethodsoften
encountergeometrydistortionorexhibitslowinferencespeed.To
tacklethesechallenges,weutilizeLive3D[Trevithicketal.2023],a
state-of-the-artsingle-imageportrait3Dreconstructionmodelbuilt
3 METHOD uponEG3D,preservinggeometryqualitywhileensuringreal-time
performance.Live3Demploysatwo-branchencoder,E‚Ñéùëñùëî‚Ñé(¬∑)and
OurapproachtakesaportraitimageIanditscameraposec,obtained
Eùëôùëúùë§(¬∑),toextractdifferentresolutionfeaturesfromtheinputimage.
viathefaceposeestimator[Dengetal.2019b].Additionally,theref-
ItthenutilizesaViT-basedEùë°(¬∑)decoder[Dosovitskiyetal.2020]
erencedpromptsP(imagesortexts)serveastheeditinginstructions.
totransformthefusedencoderfeaturesintothetriplane,T:
Theoutcomeofourmodelisaneditedversionofthe3Dportraits
characterizedbyNeRFinaccordancewiththeprompts.Theedited T=Eùë°(E‚Ñéùëñùëî‚Ñé(I),Eùëôùëúùë§(I)) (1)
image is denoted as Iùëù. Within our framework, we achieve this
objectivebyleveragingtheknowledgeof3DGANsandanopen- ThistriplaneTissubsequentlyusedinconjunctionwiththevolume
vocabularytext-to-imagemodel,whichisdistilledintoalightweight renderingmoduleandupsamplerofEG3Dtogeneratephotorealistic
module.Followingdistillation,ourmethodenablesreal-time3Dpor- viewsynthesisgiventhecameraposec.Forclarification,weuse
traiteditingandefficientadaptationtouser-specifiedprompts. R(¬∑)todenotethisprocess.
InSec.3.1,weofferanillustrationof3DGANsanddiffusion DiffusionPriorforPortraitEditing.Onlyperforming3Dre-
models. The process of distilling these priors into a lightweight constructionisinsufficientasourgoalistoeditportraitimages.
moduleisoutlinedinSec.3.2.Finally,thedetailsformodeltraining Large-scalediffusionmodels[Rombachetal.2022],trainedonvast
andinference,alongwiththefastadaptationfornovelprompts,are text-imagepairs,cansynthesizerealisticphotoswithtextinput,
presentedinSec.3.3. offering a powerful editing prior. Despite their effectiveness for‚Ä¢ Bai,Q.etal
faceimages,theeditingprocessisoftenslowduetostep-by-step Input Rec Novel Views
optimizationormultipleinferencesindiffusionmodels.Moreover,
obtainingalargeamountofhigh-qualitypaireddatawithmulti-
viewconsistencyfromthediffusionmodelisextremelychallenging,
resultingintheapplicationof3D-awareeditingfromasingleimage
impractical.Therefore,ourgoalistodistilleditingknowledgefrom
thediffusionmodel,integrateitwiththe3DpriorinLive3Dintoa
lightweightmodule,andemployitforreal-timeportraitediting.
3.2 DistillingPriorsintoaLightweightModule
Oursystemaimstoperformreal-time3D-awareeditingforvari-
ouspromptsforthesingle-viewportrait.Thus,itneedspowerful
3Dknowledgeforgeometryreconstructionandeditingpriortohan-
dlingvariouscontrolsignals.Weleveragethestrengthsofboth3D
GANsanddiffusionmodels.Inthefollowing,weprovidedetailed Fig.3. DisentanglementinLive3Dfeatures.Weseparatelydisablethe
presentationsonhowtheknowledgeofthesetwotypesofmodels
featuresfromE‚Ñéùëñùëî‚Ñé(¬∑)andEùëôùëúùë§(¬∑)toinferthereconstructedimage.With-
isdistilledintoalightweightmodule.
outE‚Ñéùëñùëî‚Ñé(¬∑),theoutputretainsthecoarsestructurebutlosestheappear-
FeatureRepresentationinLive3D.WecarefullystudytheLive3D
ance.Conversely,whenEùëôùëúùë§(¬∑)isdeactivated,thereconstructedportraits
preservethetexture(suchastheblueandpurplereflectionontheglasses)
modelanddiscoverthat,asatwo-branchtriplane-based3Drecon-
butfailtocapturethegeometry.
structionmodel,thelow-resolutionandhigh-resolutionfeatures
fromtheLive3DencoderEùëôùëúùë§(¬∑) andE‚Ñéùëñùëî‚Ñé(¬∑) tendtolearnvari-
ouslevelsofinformationwithoutexplicitguidance.Weconduct servesasthequeryandEùëù(P)isemployedaskeyandvalue.The
astudybydisablingoneofthebranchesandinferringtherecon- updatedfeatureFùëéandthegeometryfeatureFùëîarethenfeedinto
structedimages.AsillustratedinFig.3,whenthehigh-resolution thedecoderEùë°(¬∑)toinferthetriplaneTp:
encoderE‚Ñéùëñùëî‚Ñé(¬∑)isdisabled,theinferenceimageretainsasimilar
structurebutlosesitsdetailedappearance.Conversely,whenthe
Tp=Eùë°(Fùëé,Fùëî). (3)
low-resolutionencoderEùëôùëúùë§(¬∑)isdisabled,thereconstructedpor- DistillingDiffusionand3DGANPrior.Unlikepreviousmethods
traitspreservesomeofthetexturefromtheinputbutstruggleto thatrequirelargeamountsof3Ddata,ourmodeloperateswithonly
capturethegeometry.Basedonthisanalysis,thefeaturesofthe 2Dpaireddatageneratedbythe2Deditingmodel[Brooksetal.2023]
twobranchesseparatelymodellow-frequencyandhigh-frequency duringtraining.Forvisualprompts,weperformcaptioning[Lietal.
information.Thisinsightmotivatesourmodeldesignintermsof 2022]onPandthenusethecaptionsastheinstructiontogenerate
howtodistillknowledgefromthediffusionmodeland3DGANs. aneditedimageIùëîùë° with2Ddiffusionmodel[Brooksetal.2023]
GeometryPredictionforInputsI.Inourframework,weusethe asthepseudolabel.Fortextprompts,wefollowasimilarprocess
promptsPtorefinetheinputportraitIintotheeditedimageIùëù. butskipthecaptioningstep.WiththepredictedtriplaneTp,we
TheIùëù inheritsasimilarstructuretotheinputI,providingcoarse canrendertheimageusingthepretrainedEG3Dmodelandthen
geometryorstructuralguidanceintheeditingprocess.Asdiscussed calculatethereconstructionlossasfollows:
intheabovesection,theencoderEùëôùëúùë§(¬∑) consistentlygenerates
low-frequencyfeaturestorepresentgeometrycues,makingitwell- L 2ùëë =‚Ñì I(Iùëîùë°,R(Tp,c)), (4)
suitedforpreservingstructuredinformationintheinputportrait. whereR(¬∑)istherenderingmoduleofEG3D,cisthecameraposeof
Asaresult,wefreezeEùëôùëúùë§(¬∑)andleverageittoproducestructure inputportrait,and‚Ñì 1(¬∑)isanimagereconstructionlosspenalizing
featuresFùëî =Eùëôùëúùë§(I)forcoarsegeometryprediction. thedifferencebetweenthegroundtruthIùëîùë° andtherenderingIùëù =
InjectingPromptsPasCondition.Incontrasttoinputimages, R(Tp,c)).It‚ÄôsimportanttonotethatL 2ùëë,usedtoreconstructthe
promptstypicallyoffermorehigh-frequencyandtextureinforma- Iùëîùë°,essentiallydistilltheknowledgeofeditingfromthediffusion
tion to guide and control the editing process. Accordingly, it is model.
appropriatetoincorporatethepromptsintothehigh-levelbranch WeobservethatL 2ùëë helpsthemodeltoreconstructwellonthe
E‚Ñéùëñùëî‚Ñé(¬∑) inLive3Dtoextractthehigh-frequencyfeatures.Toin- input camera view but suffers from geometry distortion during
corporateprompts,weutilizethepromptencoderEùëù(¬∑)tocreate novel-viewsynthesis.Tofullyexploitthe3DpropertiesofLive3D,
promptembeddingsthatarefusedintoourmodelthroughcross weproposetodistillthe3DknowledgeofLive3D.Specifically,we
attention,whichissimilartothestrategyinStableDiffusion[Rom- leveragethepretrainedLive3DtoinferthetriplaneTùëîùë°,andmulti-
bachetal.2022].Specifically,weaddacross-attentionlayerafter viewdepthsDùëîùë° andimagesIùëîùë° ofthepseudo-labelimageIùëîùë°:
E‚Ñéùëñùëî‚Ñé(¬∑)toobtainthefeatureupdatedwiththepromptembeddings:
Tùëîùë°,Dùëîùë° =G(Iùëîùë°,C), (5)
Fùëé =crossatt(E‚Ñéùëñùëî‚Ñé(I),Eùëù(P)), (2)
whereG(¬∑)representstheinferenceprocessofLive3D,whereC=
wheretheencoderEùëù(¬∑) isatransformer(MAE[Heetal.2022] {c1,..,cùëõ}isthecameraset,andùëõdenotesthenumberofcameras.
for image and CLIP [Radford et al. 2021] for text). The E‚Ñéùëñùëî‚Ñé(I) Wealsorenderthemulti-viewdepths Dùëù andimagesIùëù ofthe
lluF
hgihE
o/w
wolE
o/wReal-time3D-awarePortraitEditingfromaSingleImage ‚Ä¢
editedimageIùëù fromthetriplaneTùëù andthendefinetheobjective:
Table1. Quantitativecomparisons.Wecompareseveralbaselineson
the100imagesofFFHQdataset.It‚Äôsimportanttonotethatweexclude
L 3ùëë =‚Ñì I(Iùëù,Iùëîùë°)+‚Ñì T(Tùëù,Tùëîùë°)+‚Ñì D(Dùëù,Dùëîùë°), (6) ùê∂ùêøùêºùëÉùëü forPTI+CLIPandLive3D+CLIPsincethesemodelsutilizeCLIPfor
where‚Ñì I(¬∑),‚Ñì T(¬∑)and‚Ñì D(¬∑)isthereconstructionlosspenalizing o impt pim roi vz ea mtio en n. tO (cu or mm po ad re el de wxc ie tl hs Iin P23 PD +q Lu iva eli 3t Dy )an ind ia nc fh ei re ev ne cs ea sr pe em edar ,k aa cb hl ie e2 v5 in0 gx
thedifferenceinimage,triplane,anddepthbetweenIùëù andIùëîùë°.
real-timeperformance.
3.3 TrainingandInference
Method ùêºùê∑ùë° ‚Üë ùê∂ùêøùêºùëÉùëü ‚Üë 3D‚Üë Time‚Üì
PTI+CLIP 0.11 - 0.73 360s
Training.Duringthetrainingphase,wesampleatripletconsisting
Live3D+CLIP 0.63 - 0.72 30s
oftheinputportraitIalongwithitscameraposec,promptsP,and
IP2P+Live3D 0.52 0.59 0.75 10s
thepseudo-labelimageIùëîùë° generatedbythe2Ddiffusionmodel.
Ours 0.47 0.73 0.76 0.04s
WeleveragetheLive3Dmodelasapretrainedmodelandaddan
additionalpromptsencoderEùëù toextractpromptsembeddings.The
overalllearningobjectivecanbedescribedasfollows:
ùêø=ùúÜ 1ùêø 2ùê∑ +ùúÜ 2ùêø 3ùê∑, (7) forreconstructionandaugmentbothofthemwithCLIPloss[Rad-
fordetal.2021]forediting.Inthesecondcategory,weemployan
whereùúÜ 1andùúÜ 2arelossweights.Inoursetting,ùúÜ 1andùúÜ 2areboth instructivepixel-to-pixeleditingmethod[Brooksetal.2023]prior
setto1.0.Forthereconstructionloss,‚Ñì I(¬∑),‚Ñì I(¬∑)arecombinations toexecuting3DreconstructionviaLive3D.
ofL2lossandLPIPSloss[Zhangetal.2018],withlossweightsbeing
EvaluationCriteria.Weevaluateallmetricson100pairsofim-
1and2,respectively.‚Ñì D(¬∑)and‚Ñì T(¬∑)areL1 loss.Notably,during
agesprocessedfromFFHQ.Weconductacomprehensiveevaluation
training,onlyEùëù(¬∑),Eùë°(¬∑)arelearnable,whileothermodulesare
oftheeditingperformanceinthefollowingfouraspects:identity
frozen.ItallowsourmodeltoleverageLive3Dknowledgeasmuch
preservation,referencealignment,3Dconsistency,andinference
aspossibleandconvergeataveryfastspeed.
speed.1)Identitypreservation(IDt)aimstomeasurethepreser-
Inference.Forinference,userscanprovideasingleportraitimage
vationoftheoriginalidentitybycalculatingthecosinesimilarity
andchoosepromptsusedduringourtraining.Ourmodelisable
betweentheidentityfeatureoftheinputimageIandthatofthe
to generate the edited 3D NeRF along with photorealistic view
editedimageIùëù.WeuseArcFacemodel[Dengetal.2019a]toextract
synthesis.
identityfeatures.2)Referencealignment(CLIPr)targetsatassess-
CustomizedPromptsAdaptation.Toaccommodatecustomized
ingthealignmentoftheoutputeditingstylestothedesiredinput
stylesprovidedbyusers,weproposeamethodtoadaptourpre-
promptbycomputingthecosinesimilarityintheCLIP[Radford
trainedencodertonovelprompts.Weincreasethetuningefficiency
etal.2021]featurespace.3)3Dconsistency(3D)ontheeditedout-
byoptimizingonlyEùëù(¬∑)andthenormalizationlayersinEùë°(¬∑)with
putsismeasuredfollowingtheevaluationprotocolsestablishedby
thesamelearningobjectiveEq.7.Thismethodallowsustolimit
EG3D[Chanetal.2022].Thisinvolvescalculatingtheidentitysimi-
thetrainingdatatoonly10imagepairsandthelearningtimeto5
larityacrossmultipleviews.4)Inferencespeed(Time)ismeasured
minutesonasingleGPU.
onasingleNVIDIAA6000GPUwithanaverageof100samples.
4 EXPERIMENTS
4.2 Efficient3D-awarePortraitEditing
4.1 ExperimentalSetup
Wemakein-depthanalysisofourefficientportraiteditingsystem
TrainingSettings.Forrealfaces,weadopttheFFHQdataset[Kar- bothquantitativelyandqualitatively.Theresultsareincludedin
ras et al. 2019] at 512√ó512 resolution with camera parameters Tab.1andFig.4.Forreferencealignment,wedonotreportùê∂ùêøùêºùëÉ
ùëü
alignedbyEG3D[Chanetal.2022].Inordertoobtainthestylized formethodsthatleverageCLIPforoptimizationsinceitisevaluated
imagesaspseudolabelsandvisualprompts,weleverageInstruct- withCLIPaswell.Thestandoutfeatureofoursystemisitsefficiency.
Pix2Pix[Brooksetal.2023]toedittherealfaces.Specifically,given Ourmethodachievesaninferencespeedofmerely40ms,which
animageprompt,wefirstemployBLIP[Lietal.2022]forcaptioning improvesover100timescomparedtothefastestexistingbaselines,
andusetheobtainedtextastheinstructiontosynthesizeanedited whichrequirearound10seconds.Becauseoftheefficientknowledge
imagewithInstructPix2Pix.Wetotallyconductexperimentson20 distillation,ourapproachisalsogoodinpreservingtheidentity,
styles,andforeachstyle,wesynthesize1000images,resultingin adheremorecloselytothereferencepromptsandachievethebest
20,000imagepairsformodeltraining.Foreachstyle,weuse8tex- 3Dconsistency.AlthoughtheLive3D+CLIPachievethebestùêºùê∑
ùë°
tualpromptsfortrainingand5textualpromptsfortesting.Weadopt score,theeditedresultsarebasicallyunchangedcomparedtothe
alearningrateof5e-5andoptimizethemodelfor60kiterations input.WesuspectthatLive3Ddoesnothavealatentspaceandloses
withabatchsizeof32.Theentiretrainingprocedureiscompleted theeditingpriors,makingitchallengingfortheresultsofCLIP
overaperiodof40hoursutilizing8NVIDIAA100GPUs. optimizationtoalignwellwithprompts.ComparedwithPTI+CLIP
Baselines.Weconductcomparisonsagainsttwocategoriesofmeth- methods,ourmodelcanenableprecisealignmentwithinputand
odsthatachieveanalogousoutcomes:1)3Dconstructioncoupled prompts,whilethebaselinealwaysgenerateslow-fidelitytextures,
with3Dediting;2)2Deditingfollowedby3Dreconstruction.For andtheresultinggeometryhasmanyartifacts.Incontrasttothe2D
thefirstcategory,weimplementtwobaselineapproaches,wherein editingandsubsequent3Dreconstructionpipeline(IP2P+Live3D),
weleveragePTI[Roichetal.2022]orLive3D[Trevithicketal.2023] oursystemproducestexturesconsistentwiththepromptandaligns‚Ä¢ Bai,Q.etal
Recolor it to
a Gothic
color scheme.
Fig.4. Qualitativecomparisons.Wecomparetheresultsofseveralbaselineswithimagepromptsandtextprompts.Ineachcase,weincludetheedited
portraitsaswellastheirnovelviewrenderings.Ourmethodgenerateshigh-qualityeditedportraitswithbetter3Dqualityandalignmentwiththereferenced
prompts.
Novel Prompt Input 10s 1min 2min 5min Novel Views Novel Views
Fig.5. Novelpromptadaptation.Weshowtheintermediatetestingresultsat10s,1min,2minand5minduringadaptationwith10pairedtrainingimages.
betterwiththeinputstructure.Forexample,asthesecondsample orproducedusingtext-guidedimageeditingmodels.Theadaptation
inFig.4shows,ourmodelcanconsistentlyretainthehairstructure, processitselfisremarkablyfast,requiringonlyabout5minutesto
whileIP2P+Live3Dcannothandlethis.Thethirdsampleshowcases accomplishthelearningofthenovelprompts.InFig.5,wepresent
that IP2P + Live3D introduces texture artifacts on the face and theintermediatetestingresultsintheadaptationprocess.Ourmodel
cannotinheritthestyleofpromptsverywell. canquicklymasterthenovel-promptknowledgeinabout2minutes.
Withfurthertraining(e.g.,5minutes),theeditedresultsbecome
4.3 AdaptationtoCustomizedEditing
morestylized,demonstratingatrade-offbetweentheauthenticity
Withthetrainededitingnetwork,oursystemalreadysupportsa andthestylization.Uponcompletionoftheadaptation,thesystem
variety of 3D-aware editing styles. To expand the selection and allowsuserstoedittestinginputsinthesenewlylearnedstyles
betterconformtousers‚Äôpreferences,weofferanefficientmethod with a minimal inference time of 0.04s. This rapid performance
forfastadaptationtonewprompts.Userscanpersonalizetheediting indicatesthatthesystemiswell-optimizedforreal-timeapplications,
networkbyprovidingamodestsetof10referenceeditingpairs. providingaseamlessandefficientuserexperience.
Thesepairscanbeeitherhandpickedfromartist-createdexamples
tupnI
PILC+ITP
PILC+D3eviL
D3eviL+P2PI
sruO
Ôºâs063Ôºà
)s03(
)s01(
Ôºâs40.0ÔºàReal-time3D-awarePortraitEditingfromaSingleImage ‚Ä¢
Input Novel Views Input w/o L2d w/ L2d
(a) (b)
Fig.6. Qualitativecomparisonforablationson(a)thedistillationlossof3DGAN(L 3ùê∑)and(b)diffusionmodels(L 2ùê∑).
Table2. Ablationstudyonthenumberofdatapairsusedfornovel
promptadaptation.WereportLPIPSscoreafter0.1min,1min,2min,and
5minfine-tuningforevaluation.
#Pair/Time 0.1min 1min 2min 5min
2 0.6191 0.5431 0.5213 0.5212
5 0.6148 0.5324 0.5064 0.4971
10 0.6076 0.5242 0.4983 0.4935
20 0.5981 0.5525 0.4954 0.4895
50 0.5891 0.5718 0.4922 0.4797
around2minutes.Asthenumberofprovideddatapairsincreases,
themodelcanadaptfasteratthebeginning.
4.5 Applications
Oneimportantapplicationofourpipelineistoeditstreamingvideos,
requiringaccuratereconstructionandefficienteditingsimultane-
ously.Wedemonstratetheeffectivenessofourpipelineontalking
Fig.7. Videoeditingresultswith3DPE.Weusetextprompts"Bronze facevideos,asillustratedinFig.7.Ourmethodaccuratelyrecon-
statue"toedittheinputvideo.Ourmethodcanaccuratelyreconstructchal- structsfacialexpressions,whileachievinghigh-qualityeditingre-
lengingfacialexpressionsandachievehigh-qualitynovelviewrenderings. sults.Sinceourmethodis3D-aware,wecannaturallyviewvideos
fromnovelperspectives.Thedemovideoisavailableinthesupple-
4.4 AblationStudies
mentarymaterials.Tobetterexperienceourmethod,wedesignan
Weanalyzeourmodelandvalidateourdesignchoicebyablating interactivesystemthatallowsusersforreal-timeediting,whichis
thecomponents. showninFig.8.Usersarerequiredtoprovideaprompttoindicate
L 3ùê∑ forDistillationofLive3D.Wecompareourmodeltrained thedesiredstyle,andaninputimagethatistobeedited.Byclicking
withandwithout L 3ùê∑.AsshowninFig.6,weobservethatthe the‚Äòsubmit‚Äôbutton,thesystemwillautomaticallyapplyourmethod
modelwithoutL 3ùê∑ resultsinveryflatgeometry,andthesynthesis onthegivenimageandprompt,andoutputreal-timeeditedimage
ofthesideviewlosesthenormalstructurewithmanyartifacts.This aswellasnovel-vieweditedresults.
highlights the importance of the 3D knowledge from Live3D in
5 DISCUSSION
trainingourmodelwith2Dpaireddata,enablingtheinferenceof
reasonablefacegeometrywithoutrelyingonany3Ddatasources. LimitationsandFuturework.Despiteachievingstate-of-the-art
L 2ùê∑ forDistillationofDiffusionModels.Wealsostudythe performanceintermsofqualityandefficiency,oursystemexhibits
effectofL 2ùê∑.FromFig.6,weobservethatL 2ùê∑ iscriticalforour inconsistenciesinthedetailsfornovel-viewrendering.Thisoccurs
model, especially in preserving detailed texture. Without it, the becausetheEG3Dframeworkreliesonasuper-resolutionmodule.
editedimagetendstoloseappearanceinformationfromprompts Additionally,whenourmethodisappliedtovideoediting,itpresents
andexhibitsstructuralartifactsaroundthenose. flickeringartifactssinceourmodelisdesignedforper-frameediting.
AblationonNovelPromptAdaptations.Toinvestigatetheabil- Developingourmethodsforvideoeditingcouldbeaninteresting
ityofourmethodadaptedtonovelprompts,weconductanab- avenueoffutureresearch.
lationstudyonthenumberofnovel-promptdatapairsrequired Conclusion.Weintroduce3DPEforreal-time3D-awareportrait
fortraining.Wechoosethegoldenstatueasthenovelpromptfor editing.Bydistillingthepowerfulknowledgeofdiffusionmodels
demonstrationandperformadaptationwith2,5,10,20,or50pairs and3DGANsintoalightweightmodule,ourmodelsignificantly
ofnovel-promptdataonasingleA6000GPU.Weexperimentally reduceseditingtimewhileensuringquality.Additionally,ourmodel
findthatourmodelcanconvergeinabout5minutes,andtherefore, supportsfastadaptationtouser-specifiednovelprompts.Thead-
wereporttheperceptualdistancebetweenthemodeloutputand vantagesofourmethodempowerustogeneratephotorealistic3D
theprovidedgroundtruthat0.1min,1min,2min,and5minforcom- portraits,acapabilitycrucialforthevisualeffectsindustry,AR/VR
parison.AsshowninTab.2,ourmodelcanadapttothenewstylein systems,andteleconferencing,amongotherapplications.
tupnI
detidE
weiV
levoN
w/o
L3d
w/
L3d‚Ä¢ Bai,Q.etal
Fig.8. Theinteractiveinterfacethatallowsuserstocustomizetheirediting.
REFERENCES
2022.Efficientgeometry-aware3Dgenerativeadversarialnetworks.InIEEEConf.
RameenAbdal,YipengQin,andPeterWonka.2019.Image2StyleGAN:HowtoEmbed Comput.Vis.PatternRecog.
ImagesIntotheStyleGANLatentSpace?.InInt.Conf.Comput.Vis. EricRChan,MarcoMonteiro,PetrKellnhofer,JiajunWu,andGordonWetzstein.
RameenAbdal,YipengQin,andPeterWonka.2020.Image2StyleGAN++:HowtoEdit 2021.pi-gan:Periodicimplicitgenerativeadversarialnetworksfor3d-awareimage
theEmbeddedImages?.InIEEEConf.Comput.Vis.PatternRecog. synthesis.InIEEEConf.Comput.Vis.PatternRecog.
ShivangiAneja,JustusThies,AngelaDai,andMatthiasNie√üner.2023.Clipface:Text- SeunggyuChang,GihoonKim,andHayeonKim.2023.HairNeRF:Geometry-Aware
guidededitingoftextured3dmorphablemodels.InSIGGRAPH. ImageSynthesisforHairstyleTransfer.InInt.Conf.Comput.Vis.
QingyanBai,YinghaoXu,JiapengZhu,WeihaoXia,YujiuYang,andYujunShen.2022. JiankangDeng,JiaGuo,NiannanXue,andStefanosZafeiriou.2019a.Arcface:Additive
High-fidelityGANinversionwithpaddingspace.InEur.Conf.Comput.Vis. angularmarginlossfordeepfacerecognition.InIEEEConf.Comput.Vis.Pattern
VolkerBlanzandThomasVetter.2023. Amorphablemodelforthesynthesisof3D Recog.
faces.InSeminalGraphicsPapers:PushingtheBoundaries,Volume2. YuDeng,JiaolongYang,DongChen,FangWen,andXinTong.2020.Disentangledand
TimBrooks,AleksanderHolynski,andAlexeiAEfros.2023.Instructpix2pix:Learning controllablefaceimagegenerationvia3dimitative-contrastivelearning.InIEEE
tofollowimageeditinginstructions.InIEEEConf.Comput.Vis.PatternRecog. Conf.Comput.Vis.PatternRecog.
ShengquCai,AntonObukhov,DengxinDai,andLucVanGool.2022.Pix2NeRF:Unsu- YuDeng,JiaolongYang,SichengXu,DongChen,YundeJia,andXinTong.2019b.
pervisedConditionalp-GANforSingleImagetoNeuralRadianceFieldsTranslation. Accurate3dfacereconstructionwithweakly-supervisedlearning:Fromsingle
InIEEEConf.Comput.Vis.PatternRecog. imagetoimageset.InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiang patternrecognitionworkshops.
Zheng.2023.MasaCtrl:Tuning-FreeMutualSelf-AttentionControlforConsistent AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xiaohua
ImageSynthesisandEditing.arXivpreprintarXiv:2304.08465(2023). Zhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,
DuyguCeylan,Chun-HaoPHuang,andNiloyJMitra.2023.Pix2video:Videoediting SylvainGelly,etal.2020.Animageisworth16x16words:Transformersforimage
usingimagediffusion.InProceedingsoftheIEEE/CVFInternationalConferenceon recognitionatscale.arXivpreprintarXiv:2010.11929(2020).
ComputerVision.23206‚Äì23217. ChenGao,YichangShih,Wei-ShengLai,Chia-KaiLiang,andJia-BinHuang.2020.
WenhaoChai,XunGuo,GaoangWang,andYanLu.2023. Stablevideo:Text-driven Portraitneuralradiancefieldsfromasingleimage.arXivpreprintarXiv:2012.05903
consistency-awarediffusionvideoediting.InProceedingsoftheIEEE/CVFInterna- (2020).
tionalConferenceonComputerVision.23040‚Äì23050. MichalGeyer,OmerBar-Tal,ShaiBagon,andTaliDekel.2023.Tokenflow:Consistent
EricRChan,ConnorZLin,MatthewAChan,KokiNagano,BoxiaoPan,Shalini diffusionfeaturesforconsistentvideoediting. arXivpreprintarXiv:2307.10373
DeMello,OrazioGallo,LeonidasJGuibas,JonathanTremblay,SamehKhamis,etal. (2023).Real-time3D-awarePortraitEditingfromaSingleImage ‚Ä¢
AlexandrosGraikos,NikolayMalkin,NebojsaJojic,andDimitrisSamaras.2022.Diffu- DanielRoich,RonMokady,AmitHBermano,andDanielCohen-Or.2022. Pivotal
sionmodelsasplug-and-playpriors.Adv.NeuralInform.Process.Syst.(2022). tuningforlatent-basededitingofrealimages.ACMTrans.Graph.(2022).
JiataoGu,LingjieLiu,PengWang,andChristianTheobalt.2022.Stylenerf:Astyle- RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer.
based3d-awaregeneratorforhigh-resolutionimagesynthesis.InInt.Conf.Learn. 2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.InIEEEConf.
Represent. Comput.Vis.PatternRecog.
JiaGuo,JinkeYu,AlexandrosLattas,andJiankangDeng.2022.Perspectivereconstruc- NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfir
tionofhumanfacesbyjointmeshandlandmarkregression.InEur.Conf.Comput. Aberman.2023.Dreambooth:Finetuningtext-to-imagediffusionmodelsforsubject-
Vis. drivengeneration.InIEEEConf.Comput.Vis.PatternRecog.
AyaanHaque,MatthewTancik,AlexeiAEfros,AleksanderHolynski,andAngjoo KatjaSchwarz,YiyiLiao,MichaelNiemeyer,andAndreasGeiger.2020.Graf:Generative
Kanazawa.2023. Instruct-nerf2nerf:Editing3dsceneswithinstructions. arXiv radiancefieldsfor3d-awareimagesynthesis.InAdv.NeuralInform.Process.Syst.
preprintarXiv:2303.12789(2023). KatjaSchwarz,AxelSauer,MichaelNiemeyer,YiyiLiao,andAndreasGeiger.2022.
ErikH√§rk√∂nen,AaronHertzmann,JaakkoLehtinen,andSylvainParis.2020.Ganspace: Voxgraf:Fast3d-awareimagesynthesiswithsparsevoxelgrids.Adv.NeuralInform.
Discoveringinterpretablegancontrols.Adv.NeuralInform.Process.Syst.(2020). Process.Syst.(2022).
KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDoll√°r,andRossGirshick. YujunShen,JinjinGu,XiaoouTang,andBoleiZhou.2020a. Interpretingthelatent
2022.Maskedautoencodersarescalablevisionlearners.InIEEEConf.Comput.Vis. spaceofGANsforsemanticfaceediting.InIEEEConf.Comput.Vis.PatternRecog.
PatternRecog. YujunShen,CeyuanYang,XiaoouTang,andBoleiZhou.2020b. InterFaceGAN:In-
AmirHertz,KfirAberman,andDanielCohen-Or.2023.Deltadenoisingscore.InInt. terpretingtheDisentangledFaceRepresentationLearnedbyGANs. IEEETrans.
Conf.Comput.Vis. PatternAnal.Mach.Intell.(2020).
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang, Zifan Shi, Sida Peng, Yinghao Xu, Andreas Geiger, Yiyi Liao, and Yujun Shen.
LuWang,andWeizhuChen.2021.Lora:Low-rankadaptationoflargelanguage 2022a. Deepgenerativemodelson3drepresentations:Asurvey. arXivpreprint
models.arXivpreprintarXiv:2106.09685(2021). arXiv:2210.15663(2022).
JunhaHyung,SungwonHwang,DaejinKim,HyunjiLee,andJaegulChoo.2023.Local ZifanShi,YujunShen,YinghaoXu,SidaPeng,YiyiLiao,ShengGuo,QifengChen,
3DEditingvia3DDistillationofCLIPKnowledge.InIEEEConf.Comput.Vis.Pattern andDit-YanYeung.2023.Learning3d-awareimagesynthesiswithunknownpose
Recog. distribution.InIEEEConf.Comput.Vis.PatternRecog.
KaiwenJiang,Shu-YuChen,HongboFu,andLinGao.2023.NeRFFaceLighting:Implicit ZifanShi,YinghaoXu,YujunShen,DeliZhao,QifengChen,andDit-YanYeung.2022b.
andDisentangledFaceLightingRepresentationLeveragingGenerativePriorin Improving3d-awareimagesynthesiswithageometry-awarediscriminator.Adv.
NeuralRadianceFields.ACMTrans.Graph.(2023). NeuralInform.Process.Syst.(2022).
TeroKarras,SamuliLaine,andTimoAila.2019.Astyle-basedgeneratorarchitecture IvanSkorokhodov,SergeyTulyakov,YiqunWang,andPeterWonka.2022. Epigraf:
forgenerativeadversarialnetworks.InIEEEConf.Comput.Vis.PatternRecog. Rethinkingtrainingof3dgans.InAdv.NeuralInform.Process.Syst.
JaehoonKo,KyusunCho,DaewonChoi,KwangrokRyoo,andSeungryongKim.2023. JingxiangSun,XuanWang,YichunShi,LizhenWang,JueWang,andYebinLiu.2022a.
3dganinversionwithposeoptimization.InIEEEWinterConf.Appl.Comput.Vis. Ide-3d:Interactivedisentanglededitingforhigh-resolution3d-awareportraitsyn-
NupurKumari,BingliangZhang,RichardZhang,EliShechtman,andJun-YanZhu. thesis.ACMTrans.Graph.(2022).
2023.Multi-conceptcustomizationoftext-to-imagediffusion.InProceedingsofthe JingxiangSun,XuanWang,YongZhang,XiaoyuLi,QiZhang,YebinLiu,andJueWang.
IEEE/CVFConferenceonComputerVisionandPatternRecognition.1931‚Äì1941. 2022b. Fenerf:Faceeditinginneuralradiancefields.InIEEEConf.Comput.Vis.
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi.2022. Blip:Bootstrapping PatternRecog.
language-imagepre-trainingforunifiedvision-languageunderstandingandgenera- AlexTrevithick,MatthewChan,MichaelStengel,EricChan,ChaoLiu,ZhidingYu,
tion.InInt.Conf.Mach.Learn. SamehKhamis,ManmohanChandraker,RaviRamamoorthi,andKokiNagano.2023.
JianhuiLi,JianminLi,HaojiZhang,ShilongLiu,ZhengyiWang,ZihaoXiao,Kaiwen Real-timeradiancefieldsforsingle-imageportraitviewsynthesis. ACMTrans.
Zheng,andJunZhu.2023a. PREIM3D:3DConsistentPreciseImageAttribute Graph.(2023).
EditingfromaSingleImage.InIEEEConf.Comput.Vis.PatternRecog. QiuyuWang,ZifanShi,KechengZheng,YinghaoXu,SidaPeng,andYujunShen.
JianhuiLi,ShilongLiu,ZidongLiu,YikaiWang,KaiwenZheng,JinghuiXu,JianminLi, 2023b.BenchmarkingandAnalyzing3D-awareImageSynthesiswithaModularized
andJunZhu.2023b.InstructPix2NeRF:Instructed3DPortraitEditingfromaSingle Codebase.arXivpreprintarXiv:2306.12423(2023).
Image.arXivpreprintarXiv:2311.02826(2023). WenWang,YanJiang,KangyangXie,ZideLiu,HaoChen,YueCao,XinlongWang,and
ConnorZLin,DavidBLindell,EricRChan,andGordonWetzstein.2022. 3dgan ChunhuaShen.2023a.Zero-shotvideoeditingusingoff-the-shelfimagediffusion
inversionforcontrollableportraitimageanimation.arXivpreprintarXiv:2203.13441 models.arXivpreprintarXiv:2303.17599(2023).
(2022). YoujiaWang,TengXu,YiwenWu,MinzhangLi,WenzhengChen,LanXu,andJingyi
ShaotengLiu,YuechenZhang,WenboLi,ZheLin,andJiayaJia.2023b. Video-p2p: Yu.2022.NARRATE:ANormalAssistedFree-ViewPortraitStylizer.arXivpreprint
Videoeditingwithcross-attentioncontrol.arXivpreprintarXiv:2303.04761(2023). arXiv:2207.00974(2022).
ZhihengLiu,RuiliFeng,KaiZhu,YifeiZhang,KechengZheng,YuLiu,DeliZhao, WeihaoXiaandJing-HaoXue.2023.ASurveyonDeepGenerative3D-awareImage
JingrenZhou,andYangCao.2023a.Cones:Conceptneuronsindiffusionmodels Synthesis.Comput.Surveys(2023).
forcustomizedgeneration.arXivpreprintarXiv:2303.05125(2023). JiaxinXie,HaoOuyang,JingtanPiao,ChenyangLei,andQifengChen.2023. High-
ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,and fidelity3DGANInversionbyPseudo-multi-viewOptimization.InIEEEConf.Comput.
StefanoErmon.2021.Sdedit:Guidedimagesynthesisandeditingwithstochastic Vis.PatternRecog.
differentialequations.arXivpreprintarXiv:2108.01073(2021). YinghaoXu,SidaPeng,CeyuanYang,YujunShen,andBoleiZhou.2022.3D-aware
Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira ImageSynthesisviaLearningStructuralandTexturalRepresentations.InIEEEConf.
Kemelmacher-Shlizerman.2022. Stylesdf:High-resolution3d-consistentimage Comput.Vis.PatternRecog.
andgeometrygeneration.InIEEEConf.Comput.Vis.PatternRecog. YinghaoXu,YujunShen,JiapengZhu,CeyuanYang,andBoleiZhou.2021.Generative
HaoOuyang,QiuyuWang,YuxiXiao,QingyanBai,JuntaoZhang,KechengZheng, hierarchicalfeaturesfromsynthesizingimages.InIEEEConf.Comput.Vis.Pattern
XiaoweiZhou,QifengChen,andYujunShen.2023.Codef:Contentdeformation Recog.
fieldsfortemporallyconsistentvideoprocessing.arXivpreprintarXiv:2308.07926 ShuaiYang,YifanZhou,ZiweiLiu,andChenChangeLoy.2023. RerenderAVideo:
(2023). Zero-ShotText-GuidedVideo-to-VideoTranslation.arXivpreprintarXiv:2306.07954
XingangPan,XudongXu,ChenChangeLoy,ChristianTheobalt,andBoDai.2021. (2023).
Ashading-guidedgenerativeimplicitmodelforshape-accurate3d-awareimage FeiYin,YongZhang,XuanWang,TengfeiWang,XiaoyuLi,YuanGong,YanboFan,
synthesis.Adv.NeuralInform.Process.Syst.(2021). XiaodongCun,YingShan,CengizOztireli,etal.2023.3dganinversionwithfacial
PascalPaysan,ReinhardKnothe,BrianAmberg,SamiRomdhani,andThomasVetter. symmetryprior.InIEEEConf.Comput.Vis.PatternRecog.
2009.A3Dfacemodelforposeandilluminationinvariantfacerecognition.InIEEE LvminZhang,AnyiRao,andManeeshAgrawala.2023.Addingconditionalcontrolto
internationalconferenceonadvancedvideoandsignalbasedsurveillance. text-to-imagediffusionmodels.InInt.Conf.Comput.Vis.
BenPoole,AjayJain,JonathanTBarron,andBenMildenhall.2022. Dreamfusion: RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.2018.
Text-to-3dusing2ddiffusion.arXivpreprintarXiv:2209.14988(2022). Theunreasonableeffectivenessofdeepfeaturesasaperceptualmetric.InIEEEConf.
ChenyangQi,XiaodongCun,YongZhang,ChenyangLei,XintaoWang,YingShan, Comput.Vis.PatternRecog.
andQifengChen.2023.Fatezero:Fusingattentionsforzero-shottext-basedvideo JiapengZhu,YujunShen,DeliZhao,andBoleiZhou.2020.In-domainGANinversion
editing.arXivpreprintarXiv:2303.09535(2023). forrealimageediting.InEur.Conf.Comput.Vis.
AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,Sandhini Jun-YanZhu,PhilippKr√§henb√ºhl,EliShechtman,andAlexeiAEfros.2016.Generative
Agarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.2021. visualmanipulationonthenaturalimagemanifold.InEur.Conf.Comput.Vis.
Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInt.Conf.
Mach.Learn.‚Ä¢ Bai,Q.etal
Input Prompt Edited Novel Views
Add dark
Goth makeup
and clothes
Present it with
the solidity of a
bronze statue.
Fig.9. Additionalqualitativeresults.