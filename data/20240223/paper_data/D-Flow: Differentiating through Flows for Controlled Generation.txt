D-Flow: Differentiating through Flows for Controlled Generation
HeliBen-Hamu12* OmriPuny2 ItaiGat1 BrianKarrer1 UrielSinger1 YaronLipman12
Abstract
Tamingthegenerationoutcomeofstateoftheart
DiffusionandFlow-Matching(FM)modelswith-
out having to re-train a task-specific model un-
locksapowerfultoolforsolvinginverseproblems,
GT Distorted Ours α=96.62
conditionalgeneration,andcontrolledgeneration
ingeneral. InthisworkweintroduceD-Flow,a
simpleframeworkforcontrollingthegeneration
processbydifferentiatingthroughtheflow,opti-
mizingforthesource(noise)point. Wemotivate
GT Distorted Ours
this framework by our key observation stating
thatforDiffusion/FMmodelstrainedwithGaus-
Figure1: Free-forminpaintingwithalatentT2IFMmodel
sianprobabilitypaths,differentiatingthroughthe
(GroundtruthimageistakenfromtheMS-COCOvalidation
generationprocessprojectsgradientonthedata
set),conditionallygeneratedmoleculeandaudioinpainting
manifold,implicitlyinjectingthepriorintotheop-
usingD-Flow.
timizationprocess. Wevalidateourframeworkon
linearandnon-linearcontrolledgenerationprob- trainedmodel,addingadditionalguidance(Bar-Taletal.,
lems including: image and audio inverse prob- 2023;Yuetal.,2023). Theguidanceisusuallybuiltupon
lemsandconditionalmoleculegenerationreach- strongassumptionsonthegenerationprocessthatcanlead
ingstateoftheartperformanceacrossall. toerrorsinthegenerationandmostlylimitthemethodto
observationsthatarelinearinthetarget(Kawaretal.,2022;
Chungetal.,2022;Songetal.,2023a;Pokleetal.,2023);
1.Introduction
lastly,(iii)adoptavariationalperspective,framingthecon-
trolledgenerationasanoptimizationproblem(Graikosetal.,
Controlled generation from generative priors is of great
2023; Mardani et al., 2023; Wallace et al., 2023; Samuel
interestinmanydomains. Variousproblemssuchascondi-
etal.,2023b),requiringonlyadifferentiablecosttoenforce
tionalgeneration,inverseproblems,sampleeditingetc.,can
thecontrol. Thispaperbelongstothisthirdclass.
all be framed as a controlled generation problem. In this
workwefocusoncontrolledgenerationfromdiffusion/flow Thegoalofthispaperistointroduceaframeworkforadding
generativemodels(Song&Ermon,2019;Hoetal.,2020; controlled generation to a pre-trained Diffusion or Flow-
Lipmanetal.,2023)astheyarethecurrentstate-of-the-art Matching(FM)modelbasedondifferentiationthroughthe
generativeapproachesacrossdifferentdatamodalities. ODE sampling process. Our key observation is that for
Diffusion/FMmodelstrainedwithstandardGaussianproba-
Therearethreemainapproachesforcontrolledgeneration
bilitypaths,differentiatinganarbitrarylossL(x)through
fromdiffusion/flowmodels: (i)conditionaltraining,where
thegenerationprocessofxwithrespecttotheinitialpoint,
themodelreceivestheconditionasanadditionalinputdur-
x ,projectsthegradient∇ Lontothe“datamanifold”,i.e.,
ingtraining(Songetal.,2020;Dhariwal&Nichol,2021; 0 x
ontomajordatadirectionsatx,implicitlyinjectingavalu-
Ho & Salimans, 2022b), although performing very well
ableprior. Basedonthisobservationweadvocateasimple
thisapproachrequirestaskspecifictrainingofagenerative
generalalgorithmthatminimizesanarbitrarycostfunction
modelwhichincasesmaybeprohibitive;(ii)training-free
L(x),representingthedesiredcontrol,asafunctionofthe
approaches that modify the generation process of a pre-
sourcenoisepointx usedtogeneratex. Thatis,
0
*Work done while interning at Meta. 1Meta 2Weizmann
Institute of Science. Correspondence to: Heli Ben-Hamu min L(x). (1)
<heli.benhamu@weizmann.ac.il>. x0
DifferentiatingthroughageneratorofaGANoranormal-
1
4202
beF
12
]GL.sc[
1v71041.2042:viXraD-Flow:DifferentiatingthroughFlowsforControlledGeneration
izingflowwasprovengenerallyusefulforcontrolledgen- Algorithm1D-Flow.
eration(Boraetal.,2017;Asimetal.,2020;Whangetal.,
Require: costL,pre-trainedflowmodelu (x)
t
2021). Recently,(Wallaceetal.,2023;Samueletal.,2023b) Initializex(0) =x
have been suggesting to differentiate through a discrete 0 0
fori=1,...,N do
diffusion solver for the particular tasks of incorporating
x(i)(1)←solve(x(i),u )
classifier guidance and generating rare concepts. In this 0 t
x(i+1) ←optimize step(x(i),∇ L(x(i)(1)))
paperwegeneralizethisideaintwoways: (i)weconsider 0 0 x0
generalflowmodelstrainedwithGaussianprobabilitypaths, endfor
includingDiffusionandFlow-Matchingmodels;and(ii)we
returnxN(1)
demonstrate,boththeoreticallyandpractically,thatthein-
ductivebiasinjectedbydifferentiatingthroughtheflowis
applicabletoamuchwiderclassofproblemsmodeledby from time t = 0 to time t = 11 using a predetermined
generalcostfunctions. velocity field u : [0,1] × Rd → Rd. We denote by p
1
thedistributionanddensityfunctionofx(1)givenx(0)∼
Weexperimentwithourmethodonavarietyofsettingsand
p (x(0)).
applications: Inverseproblemsonimagesusingconditional 0
ImageNetandtext-2-image(T2I)generativepriors,condi-
tionalmoleculegenerationwithQM9unconditionalgenera- 3.Controlledgenerationviasourcepoint
tivepriors,andaudioinpaintingandsuper-resolutionwith optimization
unconditionalgenerativeprior. Inallapplicationwewere
Givenapre-trained(frozen)flowmodel,u (x),represented
able to achieve state of the art performance without care- t
byaneuralnetworkandsomecostfunctionL:Rd →R ,
fullytuningthealgorithmacrossdomainsandapplications. +
our goal is to find likely samples x that provide low cost
Onedrawbackofourmethodistherelativelongtimefor
L(x)andarelikelyundertheflowmodel’sdistributionp .
generation(usually5−15minutesonImageNet-128onan 1
Weadvocateageneralframeworkformulatingthisproblem
NVidiaV100GPU)comparedtosomebaselines,however
asthefollowingoptimizationproblem
themethod’ssimplicityanditssuperiorresultscanjustify
itsusageandadaptationinmanyusecases. Furthermore,
webelievethereisgreatroomforspeedimprovement. min L(x(1)) (3)
x0
Tosummarize,ourcontributionsare:
whereingeneralLcanalsoincorporatemultiplecostsin-
cludingpotentiallyaregularizationtermthatcandependon
• Weformulatethecontrolledgenerationproblemasa
x andu,
0
simplesourcepointoptimizationproblemusinggen-
L˜(x)=L(x)+R(x ,u). (4)
eralflowgenerativemodels. 0
• Weshowthatsourcepointoptimizationofflowstrained In this formulation, the sample x(1) is constrained to be
withGaussianprobabilitypathsinjectanimplicitbias a solution of the ODE 2 with initial boundary condition
exhibitingadata-manifoldprojectionbehaviourtothe x(0)=x ,wherex isthe(only)optimizedquantity,Lis
0 0
costfunction’sgradient. the desired cost function. Optimizing equation 3 is done
bycomputingthegradientsofthelossw.r.t.theoptimized
• Weempiricallyshowthegeneralityandtheeffective- variablex aslistedinAlgorithm1. WecallthismethodD-
0
nessoftheproposedapproachfordifferentdomains. Flow. Tobetterunderstandthegeneralityofthisframework
wenextconsiderseveralinstantiationsofequation3.
2.Preliminaries 3.1.Costfunctions
Flowmodels. Generativeflowmodels,includingContin- Reversedsampling. First,considerthesimplecasewhere
uous Normalizing Flows (CNFs) (Chen et al., 2018; Lip- L(x) = ∥x−y∥2. In this case, the solution of 3 will be
manetal.,2023)and(deterministicsamplingof)Diffusion the x 0 that its ODE trajectory reaches y at t = 1, i.e.,
Models (Song et al., 2020) generate samples x(1) ∈ Rd x(1)=y. Notethatsince(undersomemildassumptionson
by first sampling from some source (noise) distribution u t(x))equation2definesadiffeomorphismRd →Rd for
x(0) ∼ p 0(x 0)andthensolvinganOrdinaryDifferential anarbitraryy ∈Rdthereexistsauniquesolutionx 0 ∈Rd
Equation(ODE) toequation3.
1Inthispaperweusetheconventionoft=0correspondsto
x˙(t)=u (x(t)), (2)
t noise,andt=1todata.
2D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Distorted Initialx(1) step2 step4 step6 step8 step10 step12 GT
Figure2: Intermediatex(1)duringoptimization. Givenadistortedimageandrandomlyinitializedx definingtheinitial
0
x(1),ouroptimizationtravelsclosetothenaturalimagemanifoldpassingthroughin-distributionimagesonitswaytothe
GTsamplefromtheface-blurredImageNet-128validationset.
Inverseproblems. Inthiscasewehaveaccesstosome 3.3.Regularizations
knowncorruptionfunctionH :Rd →Rnandacorrupted
Theformulationinequation3allowsincludingdifferentreg-
sample
ularizationsR(equation4)discussednext. Maybethemost
y =H(x )+ϵ, (5)
∗
intriguing of these regularizations, and the main point of
whereϵ∼N(ϵ)isanoptionaladditivenoise,andthecost thispaper,istheimplicitregularization,i.e.,corresponding
functionisusually toR≡0,discussedlastinwhatfollows.
L(x)=∥H(x)−y∥2 (6)
Regularizing the target x(1). Maybe the most natural
wherethenormcanbesomearbitraryL p normorevena is incorporating the negative log likelihood (NLL) of the
general loss ℓ(H(x),y) comparing H(x) and y. Specific samplex(1),i.e.,R=−logp (x(1))inequation4. This
1
choicesofthecorruptionfunctionH canleadtocommon priorcanbeincorporatedbyaugmentingx(t)∈Rdwithan
applications: Imageinpaintingcorrespondstochoosingthe extracoordinatez ∈Randformulateequation3as
corruptionfunctionH tosub-sampleknownn<dpixels
outofdtotalpixels;Imagedeblurringcorrespondstotaking min L(x(1))−z(1) (9a)
H :Rd →Rdtobeablurringfunction,e.g.,aconvolution
x0
s.t. x˙(t)=u (x(t)), x(0)=x (9b)
withablurringkernel;Super-resolutioncorrespondstoH : t 0
Rd →Rd/k loweringthedimensionbyafactorofk. z˙(t)=−divu t(x(t)), z(0)=logp 0(x 0) (9c)
Indeed, solving the ODE sys-
Conditionalsampling. Anotherimportantapplicationis
tem defined by equations 9b
toguidethesamplingprocesstosatisfysomeconditioning
and 9c for times t ∈ [0,1]
y. InthiscasewecantakeL(x)toencourageaclassifieror
provides z(1) = logp (x(1)),
1
someenergyfunctiontoreachaparticularclassorenergyy.
see (Chen et al., 2018). How-
Forexample,ifF :Rd →Rissomefunctionandwewould
ever,asidefromtheextracom-
liketogenerateasamplefromacertainlevelsetc∈Rwe BPD=2.02 BPD=1.84
plexityintroducedbythediver-
canusetheloss gencetermintheODEinequa- Figure 3: BPD of two
L(x)=(F(x)−c)2. (7) tion 9c (see e.g., (Grathwohl imagesinanImageNet-
et al., 2018) for ways to deal 128model.
withthistypeofODE)itisnot
3.2.Initialization
clearwhetherlikelihoodisagoodpriorindeepgenerative
Theinitializationofx 0canhaveagreatimpactonthecon- modelsinhighdimensions(Nalisnicketal.,2019);InFig-
vergenceoftheoptimizationofequation3. Anaturalchoice ure3wecomparebits-per-dimension(BPD)ofatestimage
willbetoinitializex 0 withasamplefromthesourcedis- ofImageNet-128andaversionofthisimagewithamiddle
tributionp 0(x 0). Wefindthatforcaseswhenanobserved square masked with zeros providing a more likely image
signaly providesalotofinformationaboutthedesiredx accordingtoourImageNettrainedflowmodel.
onecanimprovetheconvergencespeedoftheoptimization.
Forexample,inlinearinverseproblemsonimages,where Regularizingthesourcex(0) = x . Anotheroptionis
0
theobservedyhasastrongprioronthestructureoftheim- toregularizethesourcepointx(0) = x . Thefirstchoice
0
age,itisbeneficialtoinitializex 0withablendofasample wouldagainbetoincorporatetheNLLofthenoisesample,
fromthesourcedistributionandthebackwardsolutionof i.e.,R=−logp (x ),whichforstandardnoisep (x )=
0 0 0 0
theODEfromt=1tot=0ofy: N(x |0,I)wouldreducetoR=c+ 1∥x ∥2,wherecisa
√ √ 0 2 0
constantindependentofx . Thishowever,wouldattractx
x = α·y(0)+ 1−α·z (8) 0 0
0
towardsthemostlikelyallzeromeanbutfarfrommostof
√
(cid:82)0
wherez ∼p (x )andy(0)=y+ u(t,y(t))dt. theprobabilitymassatnorm d.
0 0 1
3D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Following(Samueletal.,2023a)weinsteadprefertomake the solver used to parameterize x(1). To this end we uti-
surex staysintheareawheremostmassofp isconcen- lizethetorchdiffeqpackage(Chen,2018),providing
0 0
tratedandthereforeusetheχddistribution,whichisdefined awideclassofdifferentiableODEsolvers. Backpropagat-
astheprobabilitydistributionp(r)oftherandomvariable ingthroughthesolvercanbeexpensiveinmemoryandwe
r = ∥x ∥ where x ∼ N(x |0,I) is again the standard thereforeusegradientcheckpointingtoreducememorycon-
0 0 0
normaldistribution. TheNLLofrinthiscaseis sumptionatthecostofruntime. Inmostofourexperiments
we use the midpoint method with 6 function evaluations.
R=−logp(r)=c+(d−1)log∥x ∥−
∥x 0∥2
, (10)
Lastly, we need to choose the optimizer for the gradient
0 2 step. Sincetheoptimizationweperformisnotstochastic
wechoosetousetheLBFGSalgorithmwithlinesearchin
wherecisaconstantindependentofx .
0 allexperiments. Theruntimeoftheoptimizationdepends
ontheproblembuttypicallyrangesfrom5−15minutes
Implicitregularization. Maybethemostinterestingand
persample. Forlargetext-2-imageandtext-2-audiomodels
potentiallyusefulregularizationinourformulation(equa-
runtimesarehigherandcanreach30−40minutes.
tion3)comesfromthechoiceofoptimizingthecostL(x(1))
asafunctionofthesourcepointx(0) = x . Forstandard
0
4.Theory
diffusion/flowmodelsthataretrainedtozeroloss:
Optimizing the cost L(x(1)) with respect to x follows In this section we provide the theoretical support to the
0
the data distribution p (x ) by projecting the gradient implicitregularizationclaimmadeintheprevioussection.
1 1
∇ L(x(1))withthelocaldatacovariancematrix. First,werevisitthefamilyofAffineGaussianProbability
x(1)
Paths(AGPP)takingnoisetodatathatareusedtosupervise
This is intuitively illustrated
diffusion/flowmodels. Whendiffusion/flowmodelsreach
in Figure 4: while moving
zerolosstheyreproducetheseprobabilitypathsandwewill
in direction of the gradient
thereforeusethemtoanalyzetheimplicitbias. Second,we
∇ L(x(1)) generally moves
x(1) usethemethodofadjointdynamicstoprovideanexplicit
away from the data distribu-
formulaforthegradient∇ L(x(1))undertheAGPPas-
tion (in pink), differentiating
x0
sumption,andconsequentlyderivetheasymptoticchange
w.r.t. x(0) projects this gradi-
(velocity vector) in x(1). Lastly, we interpret this veloc-
ent onto high variance data di-
ityvectorofx(1)todemonstratewhyitispointinginthe
rectionsandconsequentlystay- Figure 4: Implicit
directionofthedatadistributionp (x).
ingclosetothedatadistribution. bias in differentiating 1
To exemplify this phenomena throughthesolver.
AffineGaussianProbabilityPaths. Diffusionandrecent
we show in Figure 2 optimiza-
flow based models use Affine Gaussian Probability Path
tion steps x(0)(1),x(2)(1),x(4)(1),... of a loss L(x) =
(AGPP)tosupervisetheirtraining. Inparticular,denoting
∥H(x)−H(x¯)∥2,whereH isalinearmatrixthatsubsam- p = N(0,σ2I) the Gaussian noise (source) distribution
0 0
plesa(random)subsetoftheimage’spixelsconsistingof
andp data(target)distribution,anAGPPisdefinedby
1
90%ofthetotalnumberofpixels,andx¯isatargetimage
(cid:90)
(differentfromtheinitialx(0)(1)). Thesamplingprocess
p (x)= p (x|x )p (x )dx , (11)
t t 1 1 1 1
hereisusinganImageNettrainedflowmodelwiththeclass
condition‘bulbul’. Ascanbeseeninthissequenceofim- where p (x|x ) = N(x|α x ,σ2I) is a Gaussian kernel
ages,theintermediatestepsoftheoptimizationstayclose t 1 t 1 t
andα ,σ : [0,1] → [0,1]arecalledthescheduler, satis-
t t
to the distribution and pass through different sub-species
fying α = 0, σ ≈ 0, and α = 1 = σ , consequently
0 1 1 0
ofthebulbulbird. Inthenextsectionweprovideaprecise
guaranteeingthatp interpolates(exactlyorapproximately)
t
mathematicalstatementsupportingthisclaimbutfornow
thesourceandtargetdistributionsattimest=0andt=1,
letusprovidesomeintuitiveexplanation.
respectively. The velocity field that generates this proba-
bilitypathandcoincidewiththevelocityfieldtrainedby
3.4.Practicalimplementation
diffusion/flowmodelsatzerolossis(Lipmanetal.,2023;
Shauletal.,2023)
ThepracticalimplementationofAlgorithm1requiresthree
algorithmicchoices. First,oneneedstodecidehowtoini- (cid:90)
u (x)= [a x+b x ]p (x |x)dx (12)
tialize x . In all experiment we either initialize x as a t t t 1 t 1 1
0 0
samplefromthesourcedistribution,i.e.,normalGaussian,
whereusingBayes’Theorem
orweuseavariancepreservingblendofanormalGaussian
with the backward solution from t = 1 to t = 0 of the p (x|x )p (x )
p (x |x)= t 1 1 1 , (13)
observedsignalwhenpossible. Second,weneedtochoose t 1 p (x)
t
4D-Flow:DifferentiatingthroughFlowsforControlledGeneration
and 5.RelatedWork
σ˙ σ˙
a = t, b =α˙ −α t. (14)
t σ t t tσ Inverse Problems. A new line of works alter the diffu-
t t
siongenerationprocessfortraining-freesolutionsofinverse
Differentiatingthroughthesolver. Whendiffusion/flow problems. Mostworkscanbeviewedasbuildingguidance
modelsareoptimizedtoazerolosstheyperfectlyreproduce strategies to the generation process of diffusion models.
the AGPP velocity field, i.e., equation 12 (Lipman et al., (Kawaretal.,2022)takesavariationalapproachderivinga
2023).Forthisvelocityfieldwefindaformulaforthecost’s solverforlinearinverseproblems. Similarly,(Chungetal.,
gradientwithrespecttox 0: 2022;Wangetal.,2022)modifythegenerationprocessby
Theorem4.1. ForAGPPvelocityfieldu (seeequation12) enforcingconsistencywiththeobservationseitherviacost
t
andx(t)definedviaequation2thedifferentialofx(1)asa functionsorprojections(Choietal.,2021;Wangetal.,2022;
functionofx is Lugmayretal.,2022).Otherapproachesguidethesampling
0
processwithderivativesthroughthediffusionmodelateach
(cid:20)(cid:90) 1 (cid:21)
D x(1)=σ exp γ Var(x |x(t))dt , (15) denoisingstep(Hoetal.,2022;Chungetal.,2023;Song
x0 1 t 1
0 etal.,2023a;Pokleetal.,2023). Arecentworkby(Rout
whereγ
t
= 1 2dd tsnr(t)andwedefinesnr(t)= α
σ
t22 t. e bt ya cl h., a2 in0 e2 d3) ape px lt ie cn ad tis onth se oi fd ee na cs of do er r-l da ete cn ot dd ei r.ff Su is mio in lam rto od oe uls
r
TheproofisgiveninAppendixA.Intheexactcasewhere approach(Mardanietal.,2023)performsoptimizationofa
(cid:82)1 reconstructionlosswithscorematchingregularization.
σ = 0wealsohave γ dt = ∞,neverthelessweshow
1 0 t
inAppendixAthatD x0x(1)iswelldefinedalsointhiscase. Conditional sampling. Conditional sampling from dif-
Now,thematrixD x0x(1)∈Rd×dissymmetricpositivedef- fusion models can be achieved by training an additional
initeandthematrix-vectorproductD x0x(1)vcorresponds noise-awareconditionpredictormodel(Songetal.,2020)
(cid:82)1
toiterativeapplicationsofthematrix
0
γ tVar(x 1|x(t))dt orbyincorporatingtheconditionintothetrainingprocess
to v. While a closed form expression to this integral is (Dhariwal&Nichol,2021;Ho&Salimans,2022a). These
unknown,itisaweightedsumofthecovariancematrices, approaches however require task specific training. Plug-
Var(x |x(t))=E [x −xˆ ][x −xˆ ]T , (16) and-playapproaches,ontheotherhand,utilizeapre-trained
1 pt(x1|x(t)) 1 1 1 1
unconditionalgenerativemodelasaprior. (Graikosetal.,
where xˆ = E x is the denoiser (Karras
et al.,
2021
2).
Thpte(x v1| ex c( tt o)) r-m1
atrix multiplication with
2023)performconstrainedgenerationviaoptimizationofa
reconstructiontermregularizedbythediffusionloss. (Liu
Var(x |x(t))v projects v on the major axes of the distri-
1
etal.,2023)seeksforoptimalcontroloptimizingthroughthe
butionofthedataconditionedonx(t),i.e.,x |x(t). Aswe
1
generationprocesstolearnguidingcontrols. Ourmethod
willsoonsee,D x(1)iskeytounderstandingtheimplicit
x0
formulatesasimilaroptimizationproblemlikeearlierworks
biasclaim.
ongenerativenormalizingflowmodels(Whangetal.,2021;
Cha´vez,2022). Wenotethatusinggradientsthroughthe
The dynamics of x(1). Consider an optimization step
solver for the case of discrete diffusion models was first
updatingtheoptimizedvariablex withagradientstep,i.e.,
0
xτ =x −τ∇ L(x(1)),wherethegradient∇ L(x(1)) usedby(Wallaceetal.,2023)forclassifierguidanceandby
0 0 x0 x0
(Samueletal.,2023b)togenerateraresamples.
canbenowcomputedwiththechainruleandequation18,
∇ L(x(1))=D x(1)∇ L(x(1)), (17)
x0 x0 x(1) 6.Experiments
andweusedthatD x(1)isasymmetricmatrix. Wecan
x0
nowask: Howthesamplex(1)ischanginginfinitesimally We test D-Flow on the tasks: linear inverse problems on
under this gradient step? Denote by Φ : Rd → Rd the images,inverseproblemswithlatentflowmodelsandcon-
maptakinginitialconditionsx tosolutionsofequation2 ditionalmoleculegeneration. Foralltheinverseproblems
0
att=1,i.e.,Φ(x )=x(1). Thevariationofx(1)is experiments,wheretheobservedsignalprovidesstructural
0
information,weuseablendinitializationtoouralgorithm
d (cid:12)
δx(1)= (cid:12) Φ(x −τ∇ L(x(1))) speedingupconvergenceandoftenimprovingperformance.
dτ(cid:12)
τ=0
0 x0
Furthermore,inmostexperimentswefindthatthereisno
=−[D x0x(1)]2∇ x(1)L(x(1)), needinaddinganexplicitregularizingtermintheoptimiza-
where the first equality is the definition of variation and tion. Theonlycaseswherewefoundregularizationhelpful
the second equality is using chain rule, equation 18, and wasinthenoisycaselinearinverseproblemsandmolecule
equation 17. Indeed, the dynamics of x(1) follows the generation. AdditionaldetailsareinAppendixB.
projectionofthegradient∇ L(x(1))withtheoperator
x(1)
D x(1)thatiterativelyappliesprojectionbythecovariance
x0
matrixVar(x |x(t))atdifferenttimest.
1
5D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Distorted GroundTruth Ours RED-Diff OT-ODE ΠGDM Distorted GroundTruth Ours RED-Diff OT-ODE ΠGDM
Figure5: QualitativecomparisonforlinearinverseproblemsonImageNet-128. GTsamplesfromImageNet-128validation.
Table1: Quantitativeevaluationoflinearinverseproblemsonface-blurredImageNet-128.
Inpainting-Center Super-ResolutionX2 Gaussiandeblur
Method FID↓ LPIPS↓ PSNR↑ SSIM↑ FID↓ LPIPS↓ PSNR↑ SSIM↑ FID↓ LPIPS↓ PSNR↑ SSIM↑
σy=0
ΠGDM(Songetal.,2023a) 5.73 0.096 36.89 0.908 6.01 0.104 34.31 0.911 4.27 0.066 37.61 0.961
OT-ODE(Pokleetal.,2023) 5.65 0.094 37.00 0.893 4.28 0.097 33.88 0.903 2.04 0.048 37.44 0.959
RED-Diff(Mardanietal.,2023) 5.40 0.068 38.91 0.928 3.05 0.091 33.74 0.900 1.62 0.055 35.18 0.937
Ours 4.14 0.072 37.67 0.922 2.50 0.069 34.88 0.924 2.37 0.035 39.47 0.976
σy=0.05
ΠGDM(Songetal.,2023a) 7.99 0.122 34.57 0.867 4.38 0.148 32.07 0.831 30.30 0.328 29.96 0.606
OT-ODE(Pokleetal.,2023) 6.25 0.119 35.01 0.882 4.61 0.149 32.59 0.862 4.84 0.175 31.94 0.821
RED-Diff(Mardanietal.,2023) 14.63 0.171 32.42 0.820 10.54 0.182 31.82 0.852 21.43 0.229 31.41 0.807
Ours 4.76 0.102 34.609 0.890 4.26 0.146 32.35 0.858 5.35 0.167 31.99 0.820
6.1.Linearinverseproblemsonimages face-blurredImageNetdatasetusedby(Pokleetal.,2023).
We compare our method to three recent state of the art
Wevalidateourmethodonstandardlinearinverseproblems
methods: ΠGDM (Song et al., 2023a), OT-ODE (Pokle
withaknowndegradationmodelonimages. Thetaskswe
et al., 2023) and RED-Diff (Mardani et al., 2023). We
consider are center-crop inpainting, super-resolution and
use the implementation of (Pokle et al., 2023) for all the
Gaussiandeblurringbothinthenoislessandnoisycase. In
baselines. Allmethods,includingours,areevaluatedwith
allcaseswestoptheoptimizationatataskdependenttarget
thesameCond-OTflow-matchingclassconditionedmodel
PSNR.ForthenoisycasewechoosethetargetPSNRtobe
trainedontheface-blurredImageNet-128unlessthereults
thePSNRcorrespondingtotheknownaddednoise.
we produced were inferior to the ones reported in (Pokle
Tasks. We follow the same settings as in (Pokle et al., etal.,2023). Inthatcase,weusethereportednumbersfrom
2023): (i) For center-crop inpainting, we use a 40 × 40 (Pokleetal.,2023).
centeredmask;(ii)forsuper-resolutionweusebicubicinter-
Results. AsshowninTable1,ourmethodshowsstrong
polationtodownsampletheimagesby×2;andlastly(iii)
performanceacrossalltasks,Figure5showssamplesfor
forGaussiandeblurweapplyaGaussianblurkernelofsize
eachtypeofdistortion. Forinpaintingandsuper-resolution
61×61withintensity1. Foreachtaskwereportresultsfor
ourmethodimprovesuponstateoftheartinmostmetrics.
thenoiselessandnoisy(Gaussiannoiseofσ =0.05,see
y Webelievethatourmethod’sabilitytoreachimageswith
equation5)cases. Furtherimplementationdetailscanbe
higherfidelitytothegroundtruthisattributedtothesource
foundintheAppendixB.1.
pointoptimization,which,differentlyfromguidedsampling
Metrics. Followingtheevaluationprotocolofpriorworks approachessuchas(Songetal.,2023a;Pokleetal.,2023),
(Chungetal.,2022;Kawaretal.,2022)wereportFre´chet iterativelycorrectthesamplingtrajectorytobettermatchthe
Inception Distance (FID) (Heusel et al., 2018), Learned observedsignal.WefurthernotethatcomparedtoRED-Diff,
Perceptual Image Patch Similarity (LPIPS) (Zhang et al., whichisalsoanoptimizationapproach,ourmethoddoesnot
2018), peak signal-to-noise ratio (PSNR), and structural struggleinthenoisycaseandachievesSOTAperformance.
similarityindex(SSIM). WeshowmoresamplesinFigures7,8.
Datasetsandbaselines. Weusetheface-blurredImageNet-
128 dataset and report our results on the 10k split of the
6D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Table2: Quantitativeevaluationofmusicgenerationwithlatentflowmodels.
Inpainting(10%) Inpainting(20%) Super-ResolutionX2 Super-ResolutionX4 Super-ResolutionX8
Method FAD↓ PSNR↑ FAD↓ PSNR↑ FAD↓ PSNR↑ FAD↓ PSNR↑ FAD↓ PSNR↑
In-domain
RED-Diff(Mardanietal.,2023) 0.75 31.19 0.78 29.99 0.93 35.27 1.63 33.51 1.73 29.12
Ours 0.22 31.02 0.49 29.57 0.22 44.51 0.50 42.64 1.01 36.50
MusicCaps
RED-Diff(Mardanietal.,2023) 3.59 32.81 3.72 30.39 3.07 37.13 3.51 34.99 3.97 30.49
Ours 1.19 31.78 1.31 31.08 1.25 38.93 1.42 35.83 2.09 32.20
Table3: Quantitativeevaluationoffree-forminpaintingon intext-conditionalmusicgeneration, achievingaFre´chet
MS-COCOwithT2Ilatentmodel. AudioDistance(FAD)scoreof3.13(Kilgouretal.,2018)
Inpainting-Free-Form on MusicCaps and FAD of 0.72 on in-domain data. The
Method FID↓ LPIPS↓ PSNR↑ SSIM↑ Clipscore↑
model is trained to generate ten-seconds samples. In the
RED-Diff(Mardanietal.,2023) 23.31 0.327 33.28 0.813 0.882
Ours 16.92 0.327 32.34 0.759 0.892 following,weevaluatetheperformanceofinpaintingand
super-resolutionusingourmethodandRED-Diffasbase-
6.2.Inverseproblemswithlatentflowmodels line,wereportFADandPSNRmetrics.
6.2.1.IMAGEINPAINTING Datasetsandbaselines.
Wedemonstratethecapabilityofourapproachfornon-linear Forevaluation,weusetheMusicCapsbenchmark,which
inverse problems by applying it to the task of free-form comprisesof5.5Kpairsofmusicandatextualdescription
inpaintingusingalatentT2IFMmodel. andaninternal(in-domain)evaluationsetof202samples,
similarto(Copetetal.,2023;Zivetal.,2024). Similarto
Metrics. Toquantitativelyassessourresultswereportstan-
prior work, we compute FAD metric using VGGish. We
dardmetricsusedinT2Igeneration: PSNR,FID(Heusel
compareourmethodtoRED-Diff(Mardanietal.,2023).
etal.,2018),andClipscore(Rameshetal.,2022).
Results. Table2studiesourmethodininapintingandsuper
Datasetsandbaselines. TheT2Imodelweusewastrained
resolutiontasks. Thisexperimentdemosntratestheability
onaproprietarydatasetof330mimage-textpairs. Itwas
ofourmethodtoworkinnon-linearsetup,wheretheflow
trainedonthelatentspaceofanautoencoderasin(Rombach
modelistrainedoveraneuralrepresentationandthecost
etal.,2022). ThearchitectureisbasedonGLIDE(Nichol
functionisevaluatedonthepost-decodedsignal(neuralrep-
etal.,2022)andusesaT5textencoder(Raffeletal.,2023).
resentationafterdecoding). Intheinpaintingtask,wecenter
Weevaluateonasubsetof1ksamplesfromthevalidation
crop the signal by 10% and 20%, i.e., for a ten-seconds
setoftheCOCOdataset(Linetal.,2015). Wecompareour
signal,wemaskouttwoandfoursecondsrespectively. In
methodtoRED-Diff(Mardanietal.,2023)asitisalsonot
thesuper-resolutiontaskweupscaleasignalbyfactorsof
limitedtolinearinverseproblemsliketheotherbaselines
two,four,andeight,i.e.,from4kHz,8kHz,16kHzto32kHz
weusedintheprevioussection. Wetesteddifferenthyper-
respectively. Overall,ourmethodimprovesuponthebase-
parametersforRED-Diffandreportresultswiththebest.
line. Specifically, in all experiments, our method obtain
Results. Table3 reports metricsfor the baselineand our thelowestFADmetric. Intheinpaintingtaskourmethod
method. The metrics indicate that while RED-Diff bet- obtainsaslightlylowerPSNRfromthebaseline.Audiosam-
termatchestheunmaskedareas,achievingsuperiorperfor- plesareattachedinasupplementraymaterial. Additional
mance for structural metrics (PSNR, SSIM) our method implementationdetailsappearinAppendixB.2.2.
produces more semantically plausible image completion
winninginperceptualmetrics.WedoobservethatRED-Diff 6.3.ConditionalmoleculegenerationonQM9
oftenproducesartifactsforthistask. Resultsarevisualized
In this experiment we illustrate the application of our
inFigure9.
methodforcontrollablemoleculegeneration,whichisof
practicalsignificanceinthefieldsofmaterialanddrugde-
6.2.2.AUDIOINPAINTINGANDSUPER-RESOLUTION
sign. The properties targeted for conditional generation
Weevaluateourmethodonthetasksofmusicinpainting (cinequation7)includepolarizabilityα,orbitalenergies
andsuper-resolution,utilizingalatentflow-matchingmusic ε ,ε andtheirgap∆ε,Dioplemomentµ,and
HOMO LUMO
generation model. For this, we used a trained Cond-OT heatcapacityC . Toassessthepropertiesofthemolecules
v
flow-matchingtextconditionedmodelwithatransformer generated,weusedapropertyclassifier(F inequation7)
architectureof325mparametersthatoperatesontopofEn- for each property. Those classifiers were trained follow-
Codecrepresentation(De´fossezetal.,2022). Themodel’s ingthemethodologyoutlinedin(Hoogeboometal.,2022).
performancealignswiththecurrentstate-of-the-artscores FurtherdetailsareinAppendixB.3.
7D-Flow:DifferentiatingthroughFlowsforControlledGeneration
52.22 63.60 72.63 79.36 81.33 88.88 95.13 102.40
Figure6: Qualitativevisualizationofcontrolledgeneratedmoleculesforvariouspolarizability(α)levels.
Table 4: Quantitative evaluation of conditional molecule as an empirical lower bound. It is important to note that
generation. ValuesreportedinthetableareMAE(over10K for each specific property of conditional generation, the
samples)formoleculepropertypredictions(lowerisbetter). baselinemethodsutilizedadistinctconditionalmodel,each
individuallytrainedforgeneratingthatparticularproperty
Property α ∆ε ε HOMO ε LUMO µ C v whileweusedasingleunconditionalmodel. Accordingto
Units Bohr2 meV meV meV D mca ol lK theconditionaltrainingprotocolfrom(Satorrasetal.,2022),
QM9∗ 0.10 64 39 36 0.043 0.040 thepropertyclassifieristrainedoverhalfoftheQM9train
EDM 2.76 655 356 584 1.111 1.101
set(50K)whiletheconditionaltrainingisdonewiththere-
EQUIFM 2.41 591 337 530 1.106 1.033
maininghalf. Forourunconditionalmodel,whichoperates
GEOLDM 2.37 587 340 522 1.108 1.025
Ours 1.38 340 179 330 0.299 0.784 without any conditional context, we utilized a pretrained
modelthatwastrainedontheentireQM9dataset.
Table5: StabilityandvalidityevaluationofD-flowoncon- Results. Table 4 demonstrates that our approach signifi-
ditionalmoleculegeneration(10Ksamples).
cantlyoutperformsallotherbaselinemethodsinthequality
Property α ∆ε εHOMO εLUMO µ Cv ofconditionalmoleculegeneration. Thissuperiorperfor-
MoleculeStability(%) 52.6 54.9 55.2 54.4 57.3 53.3 manceisattributedtoourdirectoptimizationofthecondi-
AtomStability(%) 94.7 95.0 95.1 95.0 95.3 94.8
tionalgeneration. Table5presentsthestabilityandvalidity
Validity(%) 78.7 79.5 80.6 81.0 82.0 80.0
Validity&Uniqueness(%) 77.3 77.9 80.0 79.8 80.6 78.6 metrics for our method. In comparison with conditional
EDM, which achieves an average molecular stability of
82.1%acrossdifferentproperties,ourmethodrevealsadis-
Metrics. Toassessconditionalgeneration,wecalculatethe parityinthequalityofthegeneratedmolecules.Thisquality
MeanAbsoluteError(MAE)betweenthepredictedproperty reductionislikelyaconsequenceofalteringthesampling
valueofthegeneratedmoleculebythepropertyclassifier pathfromstochastictodeterministic,aswellastheabsence
and the target property value. Additionally, we appraise ofstructuralinductivebiasintheoptimizationprocess. We
thequalityofthegeneratedmoleculesbyevaluatingatom presentadditionalevidenceforthefirstclaiminAppendix
stability (the percentage of atoms with correct valency), B.3. Figure6visualizethecontrolledgenerationfordiffer-
moleculestability(thepercentageofmoleculeswhereall entpolarizabilityαvalues;allmoleculesinthefigureare
atomsarestable),validity(asdefinedinRDKit(Landrum, validandstablewithaclassifiererrorlowerthan1.
2016)),andtheuniquenessofthegeneratedmolecules.
Dataset and baselines. The generative models used for 7.Discussion,limitationsandfuturework
thisexperimentaretrainedusingtheQM9dataset(Ramakr-
Wehavepresentedasimpleandgeneralframeworkforcon-
ishnan et al., 2014), a commonly used molecular dataset
trolledgenerationfrompre-traineddiffusion/flowmodels
containingsmallmoleculeswithupto29atoms. Themodel
anddemonstrateditsefficacyonawiderangeofproblems
we use as prior in these experiments is the unconditional
fromvariousdomainsanddatatypesrangingfromimages,
diffusionmodelformoleculegeneration,E(3)Equivariant
and audio to molecules. The main limitation of our ap-
DiffusionModel(EDM)(Hoogeboometal.,2022). EDM
proachisinitsrelativelylongruntimes(seeSection3.4,and
was trained as a noise prediction model (also known as
AppendixB)whichstemsfromtheneedtoback-propagate
ϵ-prediction)withpolynomialschedule. Toperformdeter-
throughmultiplecompositionsofthevelocityfield(equiv-
ministicsamplingforouroptimization, wetransformthe
alently,thediffusionmodel). Ourtheoreticalanalysisand
noisepredictiontovelocityfieldprediction(seeEquation
empiricalevidenceshowhoweverthatcomputinggradients
52)anduse50stepsmidpointODEsampler. Wecompare
through the ODE solution have a desirable implicit bias,
ourmethodtoseveralstateoftheartconditionalmodels:
producing state of the art results on common conditional
conditional EDM, Equivariant Flow-Matching (EQUIFM)
generation tasks. Consequently, an interesting future di-
(Songetal.,2023b),andGeometricLatentDiffusionModel
rection is to utilize the implicit bias but with potentially
(GEOLDM)(Xuetal.,2023)anequivariantflow-matching
cheapercomputationaloverhead,anddrawconnectionsto
model. Additionally,wereportthetestMAEofeachprop-
otherbiasesusedinothercontrolledgenerationparadigms.
ertyclassifier(denotedasQM9∗inTable4),whichserves
8D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Acknowledgments Evans,L.C. Anintroductiontomathematicaloptimalcon-
troltheory. LectureNotes,UniversityofCalifornia,De-
OPissupportedbyagrantfromIsraelCHEProgramfor
partmentofMathematics,Berkeley,3:15–40,2005.
DataScienceResearchCentersandtheMinervaStiftung.
Graikos,A.,Malkin,N.,Jojic,N.,andSamaras,D. Diffu-
References sionmodelsasplug-and-playpriors,2023.
Anderson,B.,Hy,T.-S.,andKondor,R. Cormorant: Co-
Grathwohl,W.,Chen,R.T.Q.,Bettencourt,J.,Sutskever,
variantmolecularneuralnetworks,2019.
I.,andDuvenaud,D. Ffjord: Free-formcontinuousdy-
namicsforscalablereversiblegenerativemodels,2018.
Asim,M.,Daniels,M.,Leong,O.,Ahmed,A.,andHand,
P. Invertible generative models for inverse problems:
Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,and
mitigatingrepresentationerroranddatasetbias,2020.
Hochreiter,S. Ganstrainedbyatwotime-scaleupdate
Bar-Tal, O., Yariv, L., Lipman, Y., and Dekel, T. Multi- ruleconvergetoalocalnashequilibrium,2018.
diffusion: Fusing diffusion paths for controlled image
generation. arXivpreprintarXiv:2302.08113,2023. Ho,J.andSalimans,T. Classifier-freediffusionguidance.
arXivpreprintarXiv:2207.12598,2022a.
Bora, A., Jalal, A., Price, E., and Dimakis, A. G. Com-
pressedsensingusinggenerativemodels,2017. Ho,J.andSalimans,T. Classifier-freediffusionguidance,
2022b.
Chen,R.T.,Rubanova,Y.,Bettencourt,J.,andDuvenaud,
D. Neuralordinarydifferentialequations. arXivpreprint
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
arXiv:1806.07366,2018.
bilisticmodels. arXivpreprintarXiv:2006.11239,2020.
Chen, R. T. Q. torchdiffeq, 2018. URL https://
Ho,J.,Salimans,T.,Gritsenko,A.,Chan,W.,Norouzi,M.,
github.com/rtqichen/torchdiffeq.
andFleet,D.J. Videodiffusionmodels,2022.
Choi,J.,Kim,S.,Jeong,Y.,Gwon,Y.,andYoon,S. Ilvr:
Conditioningmethodfordenoisingdiffusionprobabilistic Hoogeboom,E.,Satorras,V.G.,Vignac,C.,andWelling,
models,2021. M. Equivariantdiffusionformoleculegenerationin3d,
2022.
Chung, H., Sim, B., Ryu, D., and Ye, J. C. Improving
diffusion models for inverse problems using manifold Karras,T.,Aittala,M.,Aila,T.,andLaine,S. Elucidating
constraints,2022. the design space of diffusion-based generative models.
AdvancesinNeuralInformationProcessingSystems,35:
Chung, H., Kim, J., Mccann, M. T., Klasky, M. L., and
26565–26577,2022.
Ye,J.C. Diffusionposteriorsamplingforgeneralnoisy
inverseproblems,2023.
Kawar, B., Elad, M., Ermon, S., and Song, J. Denoising
Cha´vez,J.A.Generativeflowsasageneralpurposesolution diffusionrestorationmodels. InAdvancesinNeuralIn-
forinverseproblems,2022. formationProcessingSystems,2022.
Copet,J.,Kreuk,F.,Gat,I.,Remez,T.,Kant,D.,Synnaeve, Kilgour, K., Zuluaga, M., Roblek, D., and Sharifi, M.
G., Adi, Y., andDe´fossez, A. Simpleandcontrollable Fre´chetaudiodistance: Ametricforevaluatingmusicen-
musicgeneration. InNeurIPS,2023. hancementalgorithms. arXivpreprintarXiv:1812.08466,
2018.
Detlefsen,N.S.,Borovec,J.,Schock,J.,Jha,A.H.,Koker,
T., Liello, L. D., Stancl, D., Quan, C., Grechkin, M.,
Landrum, G. Rdkit: Open-source cheminformatics soft-
andFalcon,W. Torchmetrics-measuringreproducibility
ware. 2016. URLhttps://github.com/rdkit/
in pytorch. Journal of Open Source Software, 7(70):
rdkit/releases/tag/Release_2016_09_4.
4101, 2022. doi: 10.21105/joss.04101. URL https:
//doi.org/10.21105/joss.04101.
Lin,T.-Y.,Maire,M.,Belongie,S.,Bourdev,L.,Girshick,
R.,Hays,J.,Perona,P.,Ramanan,D.,Zitnick,C.L.,and
Dhariwal,P.andNichol,A. Diffusionmodelsbeatganson
Dolla´r,P. Microsoftcoco: Commonobjectsincontext,
imagesynthesis. arXivpreprintarXiv:2105.05233,2021.
2015.
De´fossez, A., Copet, J., Synnaeve, G., and Adi, Y.
Highfidelityneuralaudiocompression. arXivpreprint Lipman,Y.,Chen,R.T.Q.,Ben-Hamu,H.,Nickel,M.,and
arXiv:2210.13438,2022. Le,M. Flowmatchingforgenerativemodeling,2023.
9D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Liu,X.,Wu,L.,Zhang,S.,Gong,C.,Ping,W.,andLiu,Q. Shaul,N.,Chen,R.T.,Nickel,M.,Le,M.,andLipman,Y.
Flowgrad: Controllingtheoutputofgenerativeodeswith Onkineticoptimalprobabilitypathsforgenerativemod-
gradients. InProceedingsoftheIEEE/CVFConference els. InInternationalConferenceonMachineLearning,
onComputerVisionandPatternRecognition(CVPR),pp. pp.30883–30907.PMLR,2023.
24335–24344,June2023.
Song, J., Vahdat, A., Mardani, M., and Kautz, J.
Lugmayr,A.,Danelljan,M.,Romero,A.,Yu,F.,Timofte, Pseudoinverse-guideddiffusionmodelsforinverseprob-
R.,andGool,L.V. Repaint: Inpaintingusingdenoising lems. InInternationalConferenceonLearningRepresen-
diffusionprobabilisticmodels,2022. tations,2023a. URLhttps://openreview.net/
forum?id=9_gsMA8MRKQ.
Mardani,M.,Song,J.,Kautz,J.,andVahdat,A. Avaria-
tionalperspectiveonsolvinginverseproblemswithdiffu- Song,Y.andErmon,S. Generativemodelingbyestimat-
sionmodels,2023. ing gradients of the data distribution. arXiv preprint
arXiv:1907.05600,2019.
Nalisnick,E.,Matsukawa,A.,Teh,Y.W.,Gorur,D.,and
Lakshminarayanan,B. Dodeepgenerativemodelsknow Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er-
whattheydon’tknow?,2019. mon,S.,andPoole,B. Score-basedgenerativemodeling
throughstochasticdifferentialequations. arXivpreprint
Nichol,A.,Dhariwal,P.,Ramesh,A.,Shyam,P.,Mishkin,
arXiv:2011.13456,2020.
P.,McGrew,B.,Sutskever,I.,andChen,M. Glide: To-
wardsphotorealisticimagegenerationandeditingwith Song, Y., Gong, J., Xu, M., Cao, Z., Lan, Y., Ermon, S.,
text-guideddiffusionmodels,2022. Zhou,H.,andMa,W.-Y. Equivariantflowmatchingwith
hybridprobabilitytransport,2023b.
Pokle, A., Muckley, M.J., Chen, R.T.Q., andKarrer, B.
Training-freelinearimageinversionviaflows,2023. Wallace,B.,Gokul,A.,Ermon,S.,andNaik,N. End-to-end
diffusionlatentoptimizationimprovesclassifierguidance,
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
2023.
Matena,M.,Zhou,Y.,Li,W.,andLiu,P.J. Exploring
thelimitsoftransferlearningwithaunifiedtext-to-text Wang,Y.,Yu,J.,andZhang,J. Zero-shotimagerestoration
transformer,2023. usingdenoisingdiffusionnull-spacemodel,2022.
Ramakrishnan,R.,Dral,P.,Rupp,M.,andvonLilienfeld, Whang, J., Lei, Q., and Dimakis, A. G. Solving inverse
A. Quantumchemistrystructuresandpropertiesof134 problemswithaflow-basednoisemodel,2021.
kilomolecules. ScientificData,1,082014. doi:10.1038/
sdata.2014.22. Xu, M., Powers, A., Dror, R., Ermon, S., and Leskovec,
J. Geometric latent diffusion models for 3d molecule
Ramesh,A.,Dhariwal,P.,Nichol,A.,Chu,C.,andChen, generation,2023.
M. Hierarchicaltext-conditionalimagegenerationwith
cliplatents,2022. Yu,J.,Wang,Y.,Zhao,C.,Ghanem,B.,andZhang,J. Free-
dom: Training-freeenergy-guidedconditionaldiffusion
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and model,2023.
Ommer,B. High-resolutionimagesynthesiswithlatent
diffusionmodels,2022. Zhang,R.,Isola,P.,Efros,A.A.,Shechtman,E.,andWang,
O. Theunreasonableeffectivenessofdeepfeaturesasa
Rout, L., Raoof, N., Daras, G., Caramanis, C., Dimakis, perceptualmetric,2018.
A.G.,andShakkottai,S. Solvinglinearinverseproblems
provablyviaposteriorsamplingwithlatentdiffusionmod- Ziv,A.,Gat,I.,Lan,G.L.,Remez,T.,Kreuk,F.,De´fossez,
els,2023. A.,Copet,J.,Synnaeve,G.,andAdi,Y. Maskedaudio
generationusingasinglenon-autoregressivetransformer.
Samuel, D., Ben-Ari, R., Darshan, N., Maron, H., and
2024.
Chechik, G. Norm-guidedlatentspaceexplorationfor
text-to-imagegeneration,2023a.
Samuel, D., Ben-Ari, R., Raviv, S., Darshan, N., and
Chechik,G. Generatingimagesofrareconceptsusing
pre-traineddiffusionmodels,2023b.
Satorras,V.G.,Hoogeboom,E.,Fuchs,F.B.,Posner,I.,and
Welling,M. E(n)equivariantnormalizingflows,2022.
10D-Flow:DifferentiatingthroughFlowsforControlledGeneration
A.Proofsandtheorems
A.1.ProofofTheorem4.1
WerestateTheorem4.1here:
TheoremA.1. ForAGPPvelocityfieldu (seeequation12)andx(t)definedviaequation2thedifferentialofx(1)asa
t
functionofx is
0
(cid:20)(cid:90) 1 (cid:21)
D x(1)=σ exp γ Var(x |x(t))dt , (18)
x0 1 t 1
0
whereγ
t
= 1 2dd tsnr(t)andwedefinesnr(t)= α σ22 t.
t
Proof. Tocomputethedifferentialofx(1)w.r.ttheinitialpointx weutilizeadjointdynamics.
0
Letusdefinetheadjointp(t)=D x(1). Thedynamicsofp(t)aredefinedbythefollowingODE(Evans,2005):
x(t)
p˙(t)=−D u (x(t))Tp(t) (19)
x t
p(1)=D x(1)=I. (20)
x(1)
TocomputeD x(1)wesolve19fromtimet=1backtotimet=0. Then,
x0
p(0)=D x(1). (21)
x0
TheadjointODE19isalinearODEandtogetherwiththeinitialconditionequation20itssolutionisgivenby:
(cid:20)(cid:90) 1 (cid:21)
p(t)=exp D u (x(s))ds (22)
x s
t
Att=0,weget:
(cid:20)(cid:90) 1 (cid:21)
D x(1)=exp D u (x(s))ds (23)
x0 x s
0
WewillnowusethepropertiesofAGPPstofurtheranalyzethedifferentialofthevelocityfield,D u (x(t)),insidethe
x t
integral.
WerecallageneralaffineGaussianpathisdefinedby
p (x|x )=N(x|α x ,σ2I), conditionalprobabilitypath (24)
t 1 t 1 t
(cid:90)
p (x)= p (x|x )q(x )dx , marginalprobabilitypath (25)
t t 1 1 1
where(α ,σ )definetheschedulerandq isthedatasetprobabilitydensity. Thevelocityfieldsdefiningthesepathsare
t t
(Lipmanetal.,2023)
σ˙ σ˙
u (x|x )=a x+b x , a = t, b =α˙ −α t conditionalvelocityfield (26)
t 1 t t 1 t σ t t tσ
t t
(cid:90) p (x|x )q(x )
u (x)= u (x|x )p (x |x)dx , p (x |x)= t 1 1 marginalvelocityfield (27)
t t 1 t 1 1 t 1 p (x)
t
plugging26into27weget:
u (x)=a x+b xˆ (28)
t t t 1
wherexˆ =E x =xˆ (x,t),alsocommonlycalledthedenoiserandweabusenotationheresinceweevaluatethe
1 pt(x1|x) 1 1
denoiseratfixedxandtisouranalysis.
11D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Thedifferentialofthevelocityfieldisthen:
D u (x)=a I+b D xˆ (29)
x t t t x 1
WearenowleftwithfindingD xˆ :
x 1
(cid:90) (cid:90)
D xˆ =D x p (x |x)dx = x ∇ p (x |x)dx (30)
x 1 x 1 t 1 1 1 x t 1 1
First,sincep (x|x )isaGaussian:
t 1
α x −x
∇ p (x|x )= t 1 p (x|x ) (31)
x t 1 σ2 t 1
t
andplugginginto25,wehave:
(cid:90) α x −x
∇ p (x)= t 1 p (x|x )q(x )dx (32)
x t σ2 t 1 1 1
t
using27,weget:
α
∇ p (x |x)=p (x |x) t (x −xˆ ) (33)
x t 1 t 1 σ2 1 1
t
Finally,30takestheform:
(cid:90) α (cid:90) α α
D xˆ = tp (x |x)x (x −xˆ )Tdx = tp (x |x)(x −xˆ )(x −xˆ )Tdx = tVar(x |x) (34)
x 1 σ2 t 1 1 1 1 1 σ2 t 1 1 1 1 1 1 σ2 1
t t t
whereinthesecondequalitywesubtractedtheintegral σα 2txˆ 1(cid:82) p t(x 1|x)(x 1−xˆ 1)Tdx
1
=0.
t
Insertingbackinto23weget:
(cid:20)(cid:90) 1 α (cid:21)
D x(1)=exp a I+b tVar(x |x(t))dt . (35)
x0 t tσ2 1
0 t
Incasea isintegrableweget
t
(cid:20)(cid:90) 1 α (cid:21)
D x(1)=σ exp b tVar(x |x(t))dt , (36)
x0 1 tσ2 1
0 t
(cid:104) (cid:105)
(cid:82)1
whereexp a dt =σ ,and
0 t 1
α (cid:18) σ˙ (cid:19) α α˙ α σ2−α2σ˙ σ 1 d (cid:18) α2(cid:19) 1 d
γ =b t = α˙ −α t t = t t t t t t = t = snr(t) (37)
t tσ2 t tσ σ2 σ4 2dt σ2 2dt
t t t t t
concludingtheproof.
Nextweshowthattheintegralinequation18isdefinedalsoforσ =0.
1
LemmaA.2. ForaLipschitzfunctionf :Rd →Rwehavethat(cid:82) N(x|y,σ2I)f(x)dx=f(y)+O(σ).
Proof.
(cid:12)(cid:90) (cid:12) (cid:90)
(cid:12) (cid:12) N(x|y,σ2I)f(x)dx−f(y)(cid:12) (cid:12)≤ N(x|y,σ2I))|f(x)−f(y)|dx
(cid:12) (cid:12)
(cid:90)
= N(z|0,I)|f(σz+y)−f(y)|dz
(cid:90)
≤Kσ N(z|0,I)|z|dz
=O(σ),
12D-Flow:DifferentiatingthroughFlowsforControlledGeneration
whereinthefirstequalityweperformedachangeofvariablez = x−y,andinthesecondinequalityweusedthefactthatf
σ
isLipschitzwithconstantK >0.
UsingthisLemmaweprove(undertheassumptionthatp (x)anditsderivativesisLipschitz):
1
PropositionA.3. Thedenoiserasymptoticsatt→1is
x
xˆ = +O(σ ) (38)
1 α t
t
Proof. Firstwenotethatweassumeσ →0andα →1ast→1,
t t
N(x|α tx 1,σ t2I)=c tN
(cid:32)
x
1(cid:12)
(cid:12) (cid:12) (cid:12)αx
,(cid:18)
ασ
t(cid:19)2 I(cid:33)
, (39)
t t
wherec issomenormalizationconstantsuchthatc =1. Now,
t 1
(cid:90)
p (x)= N(x|α x ,σ2I)p (x )dx (40)
t t 1 t 1 1 1
=c
t(cid:90)
N
(cid:32)
x
1(cid:12) (cid:12)
(cid:12)
(cid:12)αx ,(cid:18) ασ t(cid:19)2 I(cid:33)
p 1(x 1)dx 1 (41)
t t
(cid:18) (cid:19)
x
=c p +O(σ ), (42)
t 1 α t
t
whereinsecondequalityweusedequation39andthelastequalityLemmaA.2.
(cid:90)
xˆ = x p (x |x)dx (43)
1 1 t 1 1
(cid:90) N(x|α x ,σ2I)p (x )
= x t 1 t 1 1 dx (44)
1 p (x) 1
t
(cid:18) (cid:12) (cid:16) (cid:17)2 (cid:19)
(cid:90)
c tN x 1(cid:12) (cid:12)αx t, ασt
t
I p 1(x 1)
= x dx (45)
1 p (x) 1
t
(cid:16) (cid:17)
c x p x +O(σ )
=
tαt 1 αt t
(46)
(cid:16) (cid:17)
c p x +O(σ )
t 1 αt t
x
= +O(σ ), (47)
α t
t
whereinthesecondequalityweusedthedefinitionofp (x |x),inthethirdequalityweusedequation39,andinthefourth
t 1
equalityweusedLemmaA.2.
NowwecanshowthatD u (x(t))isboundedast→1
x t
D u (x(t))=a I+b D xˆ (48)
x t t t x 1
(cid:18) (cid:19)
1
=a I+b I+O(σ ) (49)
t t a t
t
α˙
= tI+O(1), (50)
α
t
whereinthefirstequalityweusedequation29,inthesecondPropositionA.3(andthefactthatthederivativesofp are
1
Lipschitzforthederivationoftheasymptoticrule),andinthelastequalityequation26. FurthermoreD u (x(t))isbounded
x t
ast→0asbotha ,b arewelldefined. ThismeansthatD u (x(t))isintegrableover[0,1].
0 0 x t
13D-Flow:DifferentiatingthroughFlowsforControlledGeneration
A.2.Onflow-matching,denoisersandnoiseprediction
Considerageneralaffineconditionalprobabilitypathdefinedbythefollowingtransportmap:
x =σ x +α x
t t 0 t 1
wherex ∼p andx ∼p .
0 0 1 1
Fordifferentchoicesofσ ,α wecanparametrizeknowndiffusionandflow-matchingpaths. Thecorrespondingconditional
t t
vectorfieldonx is:
1
(cid:18) (cid:19)
σ˙ σ˙ σ˙ α
u (x|x )= t(x−α x )+α˙ x = tx− t t −α˙ x
t 1 σ t 1 t 1 σ σ t 1
t t t
andtheconditionalvectorfieldonx is:
0
(cid:18) (cid:19)
α˙ α˙ α˙ σ
u (x|x )=σ˙ x + t(x−σ x )= tx− t t −σ˙ x
t 0 t 0 α t 0 α α t 0
t t t
wheref˙= df.
dt
Considerthemarginalvelocityfield:
(cid:90) (cid:90)
u (x)= u (x|x )p (x |x)dx = u (x|x )p (x |x)dx
t t 1 t 1 1 t 0 t 0 0
Onecanexpressitintermsoftheoptimaldenoiserfunction,xˆ∗(x,t):
1
σ˙ (cid:90) (cid:18) σ˙ α (cid:19)(cid:90) σ˙ (cid:18) σ˙ α (cid:19)
u (x)= t xp (x |x)dx − t t −α˙ x p (x |x)dx = tx− t t −α˙ xˆ∗(x,t)
t σ t 1 1 σ t 1 t 1 1 σ σ t 1
t t t t
ForCond-OT:
xˆ∗(x,t)−x
u (x)= 1 (51)
t 1−t
Or,intermsoftheoptimalnoisepredictor,ϵ∗(x,t),likeinDDPM:
α˙ (cid:90) (cid:18) α˙ σ (cid:19)(cid:90) α˙ (cid:18) α˙ σ (cid:19)
u (x)= t xp (x |x)dx − t t −σ˙ x p (x |x)dx = tx− t t −σ˙ ϵ∗(x,t)
t α t 0 0 α t 0 t 0 0 α α t
t t t t
andforCond-OT:
x−ϵ∗(x,t)
u (x)= (52)
t t
B.Implementationdetails
B.1.Linearinverseproblemsonimages
OptimizationDetails. ForallexperimentsinthissectionweusedtheLBFGSoptimizerwith20inneriterationsforeach
optimizationstepwithlinesearch. StoppingcriterionwassetbyatargetPSNRvalue,varyingfordifferenttasks. Thelosses,
regularizations,initializationsandstoppingcriterionsofouralgorithmforthelinearinverseproblemsarelistedinTable6.
IntheTableχdregularizationcorrespondstoequation10andλdenotesthecoefficientsused.
14D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Table6: AlgorithmicchoicesfortheImageNet-128linearinverseproblemstasks.
Inpainting-Center Super-ResolutionX2 GaussianDeblur
σ =0 σ =0.05 σ =0 σ =0.05 σ =0 σ =0.05
y y y y y y
Loss −PSNR(Hx,y) −PSNR(Hx,y) −PSNR(H†Hx,H†y) −PSNR(Hx,y)
Regularization None χd,withλ=0.01 None χd,withλ=0.01 None χd,withλ=0.01
Initialization 0.1blend 0.1blend 0.1blend
TargetPSNR 45 32 55 32 55 32
Runtimes. Fornoiselesstasks: inpaintingcentercroptookonavarage10minutesperimage,superresolutiontook12.5
minutesperimageandGaussiandeblurringtook15.5minutesperimage. Forthenoisytasks: inpaintingcentercroptookon
avarage4minutesperimage,superresolutiontook2.5minuteperimageandGaussiandeblurringtook3.5minutesper
image. Experimentsranon32GBNVIDIAV100GPU.
MetricsarecomputedusingtheopensourceTorchMetricslibrary(Detlefsenetal.,2022).
RED-DiffBaseline. TousetheRED-DiffbaselinewithaFMcond-OTtrainedmodelwetransformthevelocityfieldto
epsilonpredictionaccordingto52. Wesearchedforworkingparametersandreportedresultsthatoutperformedtheresults
thatwereproducedby(Pokleetal.,2023)withanepsilonpredictionmodel,otherwisewekeptthenumberfrom(Pokle
etal.,2023).
B.2.Inpaintingwithlatentflowmodels
B.2.1.IMAGEINPAINTING
Optimizationdetails. InthisexperimentweusedtheLBFGSoptimizerwith20inneriterationsforeachoptimization
stepwithlinesearch. Stoppingcriterionwassetbyaruntimelimitof30minutes,butoptimizationusuallyconvergences
before. Thesolverusedwasmidpointwith6functionevaluationsandthelosswasnegativePSNRwithoutregularization.
Weinitializedthealgorithmwithabackwardblendwithα=0.25. TofacilitatethebackpropagationthroughalargeT2I
modelweusegradientcheckpointing.
The validation set of the COCO dataset, used for evaluation, was downloaded from
http://images.cocodataset.org/zips/val2017.zip.
RED-DiffBaseline. ToadaptRED-Difftoalatentspacediffusionmodel,letusrecallthelossusedinRED-Diff:
ℓ(µ)=∥y−f(µ)∥2+λ (sg[ϵ(x(t),t)−ϵ])Tµ (53)
t
where f can be any differentiable function. In latent diffusion/flow model for inverse problems we can model f as
f =H(decode(µ)),wheredecodeappliesthedecoderoftheautoencoderusedinthelatentdiffusion/flowmodelandH
isthecorruptionoperator. Weuselr=0.25,λ=0.25.
B.2.2.AUDIOINPAINTING
Optimizationdetails. WefollowthesamesetupdescribedinB.2.1. Differently,weuse10inneriterationsandstopafter
100globaliterations. Weinitializethealgorithmwithabackwardblendwithα=0.1.
RED-DiffBaseline. WefollowthesameadaptationdescribedaboveinB.2.1. Weuselr=0.05,λ=0.5.
B.3.ConditionalmoleculegenerationonQM9
Optimizationdetails. Inthissection,wedescribehowAlgorithm1waspracticallyappliedintheQM9experiment. We
initializedx ∈Rn×9fortheexperiment,wherenrepresentsthemolecule’satomcountand9thenumberofattributesper
0
atom,usingastandardGaussiandistribution. Toenhanceoptimizationprocessstability,weensuredx hadafeature-wise
0
meanofzeroandastandarddeviationofonebynormalizingitaftereveryoptimizationphase. Weemployedthemidpoint
method for the ode solver, with a total of 100 function evaluations. The optimization technique utilized was LBFGS,
configuredwith5optimizationstepsandalimitof5inneriterationsforeachstep. Thelearningratewassetto1. On
average,generatingasinglemoleculetookapproximately2.5minutesusingasingleNVIDIAQuadroRTX8000GPU.
Noisepredictionvsvelocityfieldprediction. Inthissection,weexaminetheimpactofchangingthestochasticsampler
15D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Table7: Comparisonofgeneratedmoleculesqualityusingdifferentsamplingpaths.
NFE MoleculeStability AtomStability Validity Validity&Uniqueness
SampleMethod
(#) (%) (%) (%) (%)
EDM(ϵ-prediction,stochastic) 1000 81.73 98.40 91.50 90.32
EDM(ϵ-prediction,stochastic) 100 78.22 98.00 90.26 89.00
EDM(VF-prediction,deterministic) 1000 65.90 97.03 83.70 83.00
EDM(VF-prediction,deterministic) 100 66.24 96.61 84.39 83.20
EDM(VF-prediction,deterministic)+optimization 100 54.83 95.00 80.3 79.03
oftheϵ-predictionunconditionaldiffusionmodeltoavelocityfield(VF)deterministicsampler. Specifically,Ourprimary
assertionisthatthischangeinthesamplerisresponsiblefortheobserveddeclineinthequalityofthemoleculeswegenerate.
Table7illustratesacomparisonofthequalityofmoleculesgeneratedusingthesamediffusionmodelbutwithdifferent
samplers. Theunconditionaldiffusionmodelfrom(Hoogeboometal.,2022)wastrainedwitha1000stepsdiscretediffusion.
As mentioned in the previous part we sampled with D-flow using 100 function evaluations. The table reveals that the
principalreasonforthereducedqualityisthealterationofthesampler,ratherthanthereducedstepcountortheoptimization
processitself. FutureresearchwillfocusonassessingourapproachwithmodelstrainedwithFM.
QM9. TheQM9dataset(Ramakrishnanetal.,2014),awidelyrecognizedcollection,encompassesmolecularcharacteristics
andatomicpositionsfor130Ksmallmolecules,eachcontainingnomorethan9heavyatoms(upto29atomswhenincluding
hydrogens). Thetrain/validation/testpartitionsusedareaccordingto(Andersonetal.,2019)andconsistsof100K/18K/13
samplesperpartition. Weprovideadditionaldetailsregardingthepropertiesusedintheexperiment:
• αPolarizabilty-Tendencyofamoleculetoacquireanelectricdipolemomentwhensubjectedtoanexternalelectric
field.
• ε -Highestoccupiedmolecularenergy.
HOMO
• ε -Lowestunoccupiedmolecularenergy.
LUMO
• ∆ε-ThedifferencebetweenHOMOandLUMO.
• µ-Dipolemoment.
• C -Heatcapacityat298.15K.
v
16D-Flow:DifferentiatingthroughFlowsforControlledGeneration
C.AdditionalResults
Distorted GroundTruth Ours RED-Diff OT-ODE ΠGDM
Figure7: QualitativecomparisonforlinearinverseproblemsonImageNet-128forthenoiselesscase. GTsamplescome
fromtheface-blurredImageNet-128validationset.
17D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Distorted GroundTruth Ours RED-Diff OT-ODE ΠGDM
Figure8: QualitativecomparisonforlinearinverseproblemsonImageNet-128forthenoisycase. GTsamplescomefrom
theface-blurredImageNet-128validationset.
18D-Flow:DifferentiatingthroughFlowsforControlledGeneration
Distorted GroundTruth Ours RED-Diff
Figure9: Qualitativecomparisonforfree-forminpaintingontheMS-COCOdatasetusingaT2IlatentFMmodel. GT
samplescomefromtheMS-COCOvalidationset.
19