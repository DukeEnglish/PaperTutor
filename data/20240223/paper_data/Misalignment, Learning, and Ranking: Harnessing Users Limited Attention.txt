Misalignment, Learning, and Ranking: Harnessing Users
Limited Attention
Arpit Agarwal∗ Rad Niazadeh
Columbia University University of Chicago
New York, NY 10027, USA Chicago, IL 60637, USA
aa4931@columbia.edu rad.niazadeh@chicagobooth.edu
Prathamesh Patil
University of Pennsylvania
Philadelphia, PA 19104, USA
pprath@seas.upenn.edu
Abstract
IndomainslikedigitalhealthorEdTech,recommendationsystemsoftenencounteracritical
challenge: users,drivenbyimpulsiveorshort-sightedtendencies,exhibitpreferencesthatstarkly
contrast with the platform’s forward-looking, long-term payoffs. This discrepancy complicates
the task of learning-to-rank items to maximize the platform’s total payoff, as it may lead to
insufficient exploration on higher payoff items due to misalignment between user preferences
and platform payoffs. Our paper addresses this challenge by leveraging the limited attention
spanof users,knownas positionbias. We considera simple model in whicha platformdisplays
a ranked list of items in an online fashion to a sequence of T arriving users. At each time, the
arrivinguserselectsanitembyfirstconsideringaprefixwindowofacertainsizeoftheseranked
items and then picking the highest preferred item in that window (and the platform observes
its payoff for this item as feedback). We study how to exploit the combinatorial structure of
our model to design online learning algorithms that learn and optimize the unknown collected
payoffs and obtain vanishing regret with respect to hindsight optimal benchmarks.
We first consider the bandit online learning problem with adversarial window sizes and
stochastic i.i.d. payoffs. We design an active-elimination-basedalgorithmthat achievesan opti-
mal instance-dependent regretboundof (log(T)),byshowingmatchingregretupperandlower
O
bounds. The key idea behind our result is using the combinatorial structure of the problem to
either obtain a large payoff from each item or to explore by getting a sample from that item.
To do so, our algorithm book-keeps a nested subset of items and picks the top item in each set
to obtain a partial ordering at each time. It then updates this nested structure based on the
number of samples received for each item and estimated confidence intervals.
Second,weconsiderthebanditonlinelearningsettingwithadversarialpayoffsandstochastic
i.i.d. window sizes. In this setting, we first consider the full-information problem of finding
the permutation that maximizes the expected payoff. By a novel combinatorial argument, we
characterize the polytope of admissible item selection probabilities by a permutation and show
it has a polynomial-size representation. Equipped with this succinct representation, we show
how standard algorithms for adversarial online linear optimization in the space of admissible
probabilities can be used to obtain polynomial-time online learning algorithm in the space of
permutations with (√T) regret with respect to the best ranking in-hindsight.
O
∗
The author is currently at FAIR,Meta. Work done while the authorwas at Columbia University.
1
4202
beF
12
]GL.sc[
1v31041.2042:viXra1 Introduction
Recommendation systems play a crucial role in various online platforms by suggesting relevant
content, tasks, or options to users based on their preferences, skills, or previous interactions. Due
to their effectiveness in helping platforms achieve a broad spectrum of goals, they are increasingly
being used in a wide range of domains. These encompass digital content services such as music or
videostreamingplatforms,crowd-sourcingapplicationssuchascampaigncrowdfunding,andseveral
other emerging applications, examples of which are course recommendations and peering tutors in
EdTech, suggesting exercise routines and dietary plans in wellness apps for digital health, and
recommending tasks to volunteers within the nonprofit sector. In almost all of these applications,
users face an abundanceof options; e.g. thereare several podcasts that could bepossiblysuggested
or several volunteer jobs to select from. However, users typically have limited time and also limited
attention from a psychological point of view to find their most preferred option. A critical aspect
of all of these systems is then deciding how to display these options, usually in the form of a ranked
vertical list, given the position bias of users towards higher ranked options.
To measure and optimize the effectiveness of the selected ranking, the recommendation system
typically assigns payoffs for receiving clicks on items in the list. However, in several practical
scenarios, there is a drastic misalignment between platform payoffs and user preferences, where
the latter is the driving force behind clicks. This misalignment often arises from multiple factors.
On one hand, online platforms may pursue objectives that are longer-term or non-conventional,
with complex attributions to immediate user engagement through clicks. On the other hand, users
themselves sometimes exhibit short-sighted, impulsive, or irrational behaviors, overlooking their
own long-term interests or genuine objectives.1 For instance, users of a fitness app might gravitate
toward less challenging workouts, whereas the platform prioritizes the user’s overall health impact.
Similarly, students using an EdTech app might favor shorter, more popular courses, while the
platform focuses on fostering the user’s long-term educational and learning objectives. In such
contexts, user preferences and behavior are typically predictable through machine learning models
(trainedwithalargeamountofdataonpastuserclicks)orduetotheaccessofuserstocoordination
devices such as public reviews.2 However, these misaligned payoffs are usually unknown to the
platform due to their complex nature and being intertwined with longer-term goals. They can
only be observed (or estimated) ex-post after the user interacts with the selected item — hence
the platform can only learn how to optimize the ranking through sequential experimentation and
receiving feedback about these item payoffs over time after user interactions.
The above sequential experimentation and decision-making task can be cast as an online learn-
ing problem over the space of permutations, where the platform selects a permutation over the
items (i.e., arms) upon the arrival of each user and then obtains partial feedback on the payoff of
the selected arm after user’s click. However, in contrast to conventional online learning settings
with partial feedback, e.g., stochastic multi-armed bandits [Lai et al., 1985, Auer et al., 2002a] or
adversarial multi-armed bandits [Auer et al., 2002b, Bubeck et al., 2012], where the platform dic-
tates which arm is selected, in recommendation systems the arms are being pulled by the users
themselves. The biggest hurdle of the online learner who tries to maximize the platform’s payoff
over time is then the misalignment between the user preferences and the platform’s payoffs, as the
1There are strong evidences in both psychology and behavioral economics literature that there are fundamental
differencesbetween userslong-term benefitsandimmediate preferences, duetotemporal discounting,impulsiveness,
and present bias in users; e.g. see Ainslie [1975], Frederick et al. [2002], O’Donoghueand Rabin [1999].
2Recommendation systems usually employ various methods to learn user preferences through both exploration
and exploitation, while incentivizing exploration by diversifying recommendations, adding a random element to the
ranking, or using various forms of discounts and incentives. Seesection 1.1 for more context and related literature.
2arm needed to be pulled for proper learning is not necessarily the user’s most preferred arm. In
fact, when theplatform does not have “dictatorship” power in the selection of the arms, misaligned
preferences of users can lead to insufficient exploration and therefore a lack of discovery of higher
payoff arms. This phenomenon shows itself in the regret of the online learner with respect to
the optimal solution in hindsight. The key challenge is to perform online learning, in particular
exploration and exploitation, given this misalignment to obtain small (or vanishing) average regret.
In short, in this paper, we show how to mitigate the above challenge by harnessing the fact
that users of the platform have limited attention. We study a simple learning-to-rank model
where platform payoffs are unknown and user preferences are known, and we assume that they
can be arbitrarily misaligned. Although in some applications the platform can try to incentivize
users to pull its desired arm (e.g., by monetary discounts), we take a different approach relying
on the structure of the problem, particularly the fact that the platform displays a sorted list
of the items and that users have limited attention (and hence position bias). We investigate
whether the structure of the problem makes it amenable to exploration without the need for extra
incentives, enabling theplatform to performlearning andoptimization without relyingon excessive
interventions. More formally, we ask the following research question:
Do non-dictator online learning algorithms exist for ranking and displaying items to limited-
attention users (under specific model primitives) that achieve the same asymptotic perfor-
mance guarantee (i.e., up to sub-linear regret) as the optimal solution in hindsight?
The combinatorial aspect of the problem imposes extra information-theoretic and computa-
tional challenges on the decision-maker as she tries to explore and exploit. Importantly, even
ignoring the misaligned preferences and platform payoffs, the action set of the decision maker,
that is, the space of all permutations, is combinatorial. Despite that, as our main result in this
work, we answer the above question in the affirmative by designing and analyzing polynomial-time
online learning algorithms with optimal (asymptotic) regret guarantees both under stochastic and
adversarial settings.
Model. More formally, we consider a finite-horizon sequential decision-making problem with par-
tial feedback. Each time, the platform decides on a permutation over a finite set of items to display
tothearrivinguser. Each item isassociated withatime-dependentpayoff, whichisunknowntothe
platform when deciding a permutation to display. Given a permutation, the arriving user selects
an item. Our users have ordinal preferences over items (although we use cardinal user utilities for
simplicity of notation) and have limited attention: At each time, the user selects his most preferred
item in a prefix of a specific size of the ranked list depending on his attention span, which we refer
to as the user’s attention window. Similar user behavior models have been studied in the literature
of product ranking [Ferreira et al., 2022, Asadpour et al., 2022, Derakhshan et al., 2022] or assort-
mentoptimization [Aouad et al.,2021,Aouad and Segev,2021]. Usershavepossiblyheterogeneous
preferences and attention spans across time.
In order to focus on studying the trade-off between the misalignment and learning, we make
a simplifying assumption that the platform exactly knows the user preferences or the ordering
over the user’s utility of selecting each item before decision-making at each time (or equivalently,
we consider a deterministic utility choice model). For example, these utilities might be known to
the platform through public/private signals such as user reviews, learned preferences, etc. 3 (We
3Inthissimplemodel,giventhewindowsize,theitemselectionisdeterministic—andstillourmodelbearsseveral
technical challenges. We leave studying the more complex model where the user’s selection is not deterministic and
is drawn from a known distribution (for example, when the user has a randomized utility choice model) for future
work.
3further extend our results to the setting where user preferences should be learned first through
collecting reviews and social learning. See Appendix C.1).
The platform’s goal is to adaptively select permutations given the preferences of the arriving
users to maximize her expected total payoff in T N rounds of decision-making. In line with both
∈
stochastic and adversarial bandit models in the literature, we study two basic, yet fundamental,
settings of our problem:
(i) Section 3: stochastic i.i.d. item payoffs drawn from a unknown distribution (and possibly
adversarial attention windows).
(ii) Section 4: adversarial item payoffs (and i.i.d. attention window sizes drawn from a known
distribution.)
It is essential to highlight that the algorithm does not know the user’s window size in the first
setting. Similarly, in the second setting, the algorithm does not know the exact realization of
window sizes but knows the underlying distribution. Given these primitives, our benchmark in
setting (i), denoted by OPT , is the clairvoyant optimal in-hindsight algorithm that knows
stochastic
themeanpayoffsofeachitemandtheadversarialchoicesofwindowsizes,andgiventhisinformation
finds the best-fixed mapping of the user utilities to permutations. In setting (ii), our benchmark
OPT is a similar clairvoyant optimal in-hindsight algorithm that, given the adversarial
adversarial
choices of payoffs and window size distribution, finds the best-fixed mapping of the user utilities to
permutations. For simplicity of technical exposition, we first restrict our attention to fixed utilities
over time in each of the Sections 3 and 4. For this particular case, our benchmarks correspond to
a fixed permutation; Later, we explain how our results can easily extend to the case with changing
(but known) utilities.
Main Technical Results. Wefirstconsiderthecaseofstochastic payoffs withadversarialwindow
sizes (setting (i)). As our main result in this setting, we provide a complete characterization of the
achievable instance-dependent (asymptotic) regret with respect to our benchmark OPT .
stochastic
Our regret characterization is two-fold:
(a)First,wepresentacomputationallyefficientonlinelearningalgorithmthatachievesaninstance-
dependent O(logT) regret bound. The form of our regret bound is tightly related to the combina-
torial nature of our problem, i.e., selecting permutations, and hence depends on the pairwise gaps
log(nT)
between the item payoffs. Specifically, it has the following form: O ,
i∈[2:n] ∆i−1i
where n is the number of items, and for any pair i,j
∈
[n] of (cid:16)it Pems, ∆
ij
is th(cid:17)e gap in mean
payoffs (the items are assumed to belabeled in decreasing order of mean payoffs). See Theorem 3.1
for more details on the above instance-dependent regret bound under changing utilities. See also
Theorem 3.2 for an improved instance-dependent regret bound with fixed utilities. Surprisingly,
our optimal regret algorithm achieving both of these bounds, Algorithm 1, is relatively simple and
interpretable.
(b)Wethencomplementtheaboveresultswithmatchinginformation-theoreticinstance-dependent
lower-bounds by showing that the bound in Theorem 3.2 is the best regret achievable under the
worst-case choice of payoff distributions and window sizes (even if the window sizes are known to
the algorithm upfront) when utilities are fixed. See Theorem 3.5. A corollary of this lower-bound
theorem is the tightness of the bound Theorem 3.1, in the particular case when utilities are fixed
and payoffs are in reverse order of utilities.
Switching to the setting with adversarial payoffs and stochastic window sizes (setting (ii)), we
studyadifferentstyleofalgorithms thattrytolearn andoptimize inthelower-dimensional spaceof
marginalselectionprobabilitiesofeachitem—insteadofoptimizinginthehigher-dimensionalspace
4of permutations. Our main result is a computationally efficient online learning algorithm, Algo-
rithm 5, that achieves the (information-theoretically) optimal adversarial regret bound of O(√nT)
with respect to the benchmark OPT — both when utilities are fixed or changing over
adversarial
time. Again our algorithm is surprisingly natural and heavily uses the combinatorial structure of
the problem in order to reduce the problem of learning permutations to the problem of learning
marginal selection probabilities through a polynomial-time reduction.
We also provide results for two extensions to our main setting that are based on practical
considerations: (1) unknownutilities, and (2) delayed payoff feedback (see AppendixC for details).
High-level Overview of Techniques. Algorithm 1 for the stochastic payoff setting builds upon
the idea of active-arm elimination for classical bandits, where one maintains a set of active items
which areplayed inaround-robinfashion untilwegather sufficientevidence indicating thesubopti-
mality of some item. At this point, the suboptimal item is permanently eliminated from the active
set. The regret incurred from playing any item can then be bounded by bounding the number of
times itis played beforesomesuperioritem can certify its suboptimality with confidence. However,
thereisanobviousbarriertousingthisideadirectlyinourmodel–wecanneitherforceaparticular
item to beselected nor can we prevent a particular item from being selected (i.e., eliminate), as the
selected item strongly depends upon the realized window length in that trial which is beyond our
control. Consequently, the number of selections of each item in our set can differ arbitrarily, mak-
ing the regret analysis challenging; all analyses for classical bandit algorithms implicitly depend on
controlling the number of times any sub-optimal item is played across the time horizon [Lai et al.,
1985, Auer et al., 2002a, Lattimore and Szepesv´ari, 2020, Slivkins et al., 2019].
Thekeyideabehindouralgorithmisthenotionofinversion,whichisanovelchargingargument
that allows us to directly bound the regret due to selecting sub-optimal items without first having
to control the number of selections. If the estimated means are a good approximation to the actual
means, and the confidence intervals are non-overlapping, then the permutation produced will be
optimal. If not, we will misposition any item only a finite number of times (incurring bounded
regret in the process). Breaking ties in favor of items selected fewer times is crucial to achieving
this guarantee. To hold to these primitives, our algorithm implicitly and recursively book-keeps
a nested subset of items based on the number of samples received for each item and estimated
confidence intervals.: It computes the top set (empirically unbeaten items) and puts the least
played item at the end of the partial permutation. It then recomputes the top set (since now,
the items that were only beaten by this item that was just added to the permutation have now
become top items). A combination of the mentioned charging argument and controlling confidence
bounds help with analyzing this algorithm. We elaborate on this intuition in Section 3.2.2, where
we analyze the regret achieved by our algorithm.
We take a slightly different approach to solving the problem with adversarial payoffs. Fixing
the particular user behavior we consider in our model, each permutation of the products induces a
specific vector of marginal selection probabilities on the items. The “right” choice of this marginal
selection probability vector can help the platform learn and optimize by doing exploration and
exploitation. However, not all marginal selection probabilities can be chosen, as the platform
only selects a permutation (possibly randomized). At first glance, the space of permutations is
exponential-size; hence, the space of admissible marginal selection probabilities might have had
an exponential-size representation. We ask two immediate questions: given this combinatorial
construct, can we induce certain distributions for item selections, e.g., distributions that allow the
platform to explore? What about inducing even a better distribution, that is, the output of an
online linear optimization algorithm over marginal selection probabilities?
First, as a warm-up result, we construct a O(n)-support distribution over permutations that
5induces a uniform distribution over marginal selection probabilities, i.e. each item is selected with
probability 1/n, under a special case when users are lazy. The laziness assumption requires that
likelihood of a shorter attention window size is higher for each user. Using this construction one
can use the ǫ-greedy algorithm in order to achieve a O(T2/3) regret bound.
Next, we study the polytope of admissible marginal selection probabilities. We use a combina-
torial recursive argument to find a closed-form and succinct, i.e., polynomial-size, representation
for this polytope. Ourrepresentation comes handin handwith a polynomial-time (exact) rounding
algorithm that, given a marginal selection probability matrix in our problem, decomposes that
into a convex combination of integral permutation. We then show how to use this algorithm in
a black-box fashion to reduce our problem to the bandit version of the adversarial online linear
optimization [Bubeck et al., 2012, Audibert et al., 2009, Auer et al., 1995]. See more details in
Section 4, in particular, Algorithm 3 for the decomposition and Algorithm 4 that helps with the
recursive step.
1.1 Further Related Work
Besides the rich lines of work on stochastic, and adversarial bandits, our work is connected to
several other streams of research:
Incentivizing exploration. The problem of utility-driven selection of items by users on a platform
that lacks dictatorship is also studied in theincentivizing exploration literature, wheretheaim is to
persuade rational users to pick arms that are aligned with the learning objectives of the platform
(both exploration and exploitation). This is typically achieved either through various forms of dis-
counts and incentives, e.g. Bayesian Incentive Compatible (BIC) banditalgorithms without money
[Kremer et al., 2014, Mansour et al., 2016, Immorlica et al., 2019, Mansour et al., 2020] and with
money [Frazier et al., 2014, Wang and Huang, 2018], or by considering explicit types of platform
intervention suchashidinginformationfromtheusers[Immorlica et al.,2020,Papanastasiou et al.,
2018]. While these approaches partially mitigate the issue of misaligned preferences, they usually
come at a loss in performance due to the platform’s interventions [Sellke and Slivkins, 2021] or rely
on occasionally strong game-theoretic assumptions that may not hold in practice — such as the
power of commitment from the side of the platform and assuming users know the recommender
mechanism, or restricting to BIC solution concept andassumingusers areBayesian with a common
prior. Moreover, theseworksimplicitly assumeaparticular formofmisalignmentbetween userutil-
ities (empirical mean rewards of items), and platform payoffs (true mean rewards of items). Our
work on the other hand aims to understand whether the structure of the online learning prob-
lem itself can be leveraged to nudge user behavior in a favorable direction without any external
intervention, even when the user utilities are arbitrarily misaligned with the platforms objectives.
Learning to rank. Online learning to rank is a core problem in information retrieval, machine
learning, and online platforms. Many provably efficient algorithms have been proposed for this
problem in specific click models, under different feedback structures, and with different objectives.
The canonical examples of click models are the well-studied Cascade Model [Kveton et al., 2015,
Zong et al., 2016], Dependent Click Model (with single or multiple clicks) [Katariya et al., 2016],
Position-Based Modelwithknownpositionbias[Lagr´ee et al.,2016],Window-basedConsider-then-
Choose users [Ferreira et al., 2022], and general click model [Zoghi et al., 2017, Lattimore et al.,
2018]. There are also several works on adversarial models for learning to rank. These settings
are the most similar to the stochastic position-based and document-based models, but with the
additional robustness bought by the adversarial framework. For example, Radlinski et al. [2008]
and the follow up work in Slivkins et al. [2013] where the learner’s objective is to maximize the
6probability that a user clicks on any item rather than rewarding multiple clicks. See the survey by
Hofmann et al. [2011] for more details.
There are several differences between the techniques used in LTR literature and the techniques
used in our work. Firstly, most algorithms in the LTR literature rely on learning the optimal
ranking between items, typically based on item click-through rates (CTRs). This is aligned with
the objective of maximizing the number of clicks or minimizing regret. In our setting, however,
the objective of learning a ranking (based on payoffs) might be misaligned with the objective of
regret minimization and might even be infeasible. Secondly, an important challenge in the design
of our algorithm is to bound the regret due to sub-optimal permutations resulting from errors in
estimating payoffs. How can we translate the confidence radius for payoffs of individual items into
bounds on regret incurred by the permutation? We overcome this challenge by using a charging
argument that allows us to charge the regret of the permutation to different items based on the
realized attention window. This allows us to balance the number of times an item is selected at a
given position with regret due to this item.
Product ranking in online platforms. Thereisalso arich literatureon productrankingoptimization
in the revenue management literature [Derakhshan et al., 2022, Abeliuk et al., 2016, Gallego et al.,
2020, Aouad and Segev, 2021, Asadpour et al., 2022, Sumida et al., 2021, Golrezaei et al., 2021],
which focus on maximizing revenue (i.e., product-weighted objectives), or a combination of revenue
and social welfare, or similar other economically justified objectives — in different contexts with
different applications and agendas.
Online combinatorial learning. Movingbeyondonlinelearningtoonlyrank,ourworkcontributesto
the rich literature on adversarial oracle efficient online with exact algorithms [Kalai and Vempala,
2005, Dud´ık et al., 2020] or approximation algorithms [Kakade et al., 2007, Niazadeh et al., 2022]
as the oracle. Our technical approach, especially in the adversarial setting, is also related to online
combinatorial optimization [Audibert et al., 2014].
Online contextual learning. In this paper, for simplicity, we consider item-dependent payoffs; how-
ever, in recommendation system applications where personalization is allowed, the platform can
also benefitfromuser-dependentpayoffs throughcontextual learning [Li et al.,2010,Li and Zhang,
2018,Besbes et al.,2021,Cohen et al.,2020,Leme and Schneider,2018,Liu et al.,2018,Gollapudi et al.,
2021]. Fitting our paper into this framework, one can think of our approach as a form of contex-
tual learning where user preferences are essentially the context and used by the platform to rank
the products; notably, this context changes the way an item is selected in our model, but not the
platform payoffs upon selection (and hence no cross-learning is happening nor required).4
2 Preliminaries
We consider a setting where a platform displays items to each arriving user over a finite time-
horizon T. Specifically, at each time t [T], the platform displays a permutation πt over n items to
∈
the user. The utility of the user for item i [n] at time t is denoted by ut. We assume the utilities
∈ i
at time t are known to the platform beforedisplaying πt. For the ease of technical exposition, when
needed, we focus on a setting where utilities are fixed over time and later show how our results
extend to the general setting of changing utilities. The user has ‘limited-attention’ in its selection–
it first selects a window of size wt [n] and then selects the highest utility item that falls in the
∈
window of size wt, i.e. the selected item yt = argmax u , where π(a) denotes the item in
a∈[wt] πt(a)
4Considering a fully contextual-model with context-dependent payoffs, and then trying to benefit from cross-
learning across theproducts, is an interesting avenuefor future research.
7position a in π. We assume that the realization of wt does not depend on the permutation πt that
is played at time t. The payoff of item i at time t is denoted by rt. We consider the setting of
i
stochastic payoffs in Section 3 and adversarial payoffs in Section 4. The consider a limited feedback
model for payoffs: the platform only observes the selected item yt and its payoff rt . Note that
yt
the platform does not observe the realized window wt. The goal of the platform is to minimize its
regret against a suitable benchmark. In the remainder of the paper we will study different settings
of payoffs and window realizations, and define regret benchmarks appropriately.
3 Stochastic Payoffs and Adversarial Attention Windows
Inthis section, we consider thesetting wheretheitem payoffs are stochastic, andour objective is to
becompetitive againstthebestsequenceofpermutationsover items,givenanarbitrary(potentially
adversarial and adaptive) sequence of item utility orderings and attention window realizations.
3.1 Model and Notations
We assume that for each item i [n], there is an underlying (stationary) Gaussian5 distribution
∈
(µ ,1) over payoffs. In any trial t [T], in the event that item i [n] is selected, we receive a
i
N ∈ ∈
payoff rt sampled i.i.d. from its payoff distribution. Note that an item i [n] is selected in trial t iff
i ∈
it is the highest utility item in the prefix of length wt, the realized attention window length in that
trial, in the permutation πt displayed in that trial. Consequently, the (pseudo) regret R(πt;wt)
incurred due to this selection is ∆ := µ µ , where yt = argmax ut is the item
y∗tyt y∗t − yt i:πt(i)≤wt i
selected from the displayed permutation πt, and yt = argmax ut is the item that would
∗ i:π∗t(i)≤wt i
have been selected from the best permutation πt in hindsight for that trial. The objective then is
∗
to minimize this cumulative (pseudo) regret over all trials t [T] over an unknown time horizon T:
∈
T
Regret(T) , R(πt;wt)
t=1
X
Wemakenoassumptionsonthesequenceofthesequenceofattentionwindowrealizations wt ,
t∈[T]
{ }
nor the sequence of item utility orderings ut except that the latter is observable prior to
{
i}t∈[T]
decision makingineach trial; inparticular, bothcan bechosen by anadaptive adversarydepending
on the past payoffs, as well as the entire execution history of our algorithm.
3.2 The Upper Bound
3.2.1 The Algorithm
We start by providing more details around our algorithm. As with most bandit algorithms, we
maintain two statistics per item i [n] – the cumulative payoff observed r , and the number of
i
∈
times it was selected N . These statistics are then used to compute an estimate of the mean payoff
i
µˆ = r /N , as well as a confidence interval c = Θ( logT/N ), which gives us high-probability
i i i i i
lower and upper bounds on the true mean payoff µ [µˆ c ,µˆ +c ] of that item. In each trial
i ∈p i − i i i
t [T], we use these estimates to greedily construct a permutation πt to display in the following
∈
manner: starting with a partial permutation which is initially empty, we identify a set S [n]
⊆
of empirically undominated items from the set of items A that are not already in the partial
5We assume Gaussianity with unit variance only for convenience. All our results can be extended quite straight-
forwardly to general sub-Gaussian distributions.
8permutation. These are items i A that are not beaten by any other item j A with confidence,
∈ ∈
i.e. µˆ +c > µˆ c . From this set S, we select the item i∗ S with the least number of selections
i i j j
− ∈
(breaking ties arbitrarily) and append it to the partial permutation constructed thus far, followed
by any other items that have lower utility than ut that are not already in the partial permutation.
i∗
The order of these lower utility items does not matter since they will never be selected under any
attention window realization (they are preceded by i∗, which is a higher utility item). We then
remove these items that have now been added to the partial permutation (i∗ and all items with
lower utility than u i∗) from the set A, and recurse, terminating when all items have been assigned
to a position in πt (or equivalently, A becomes empty). A formal description is in Algorithm 1,
with the construction of the permutation in any round being defined in Algorithm 2.
The following theorem then gives a high probability bound on the cumulative regret achieved
by our algorithm for the general case with changing utilities.
Theorem 3.1. Given items [n] with Gaussian payoff distributions (µ ,1) , let wt and
i i∈[n] t>0
{N } { }
ut beanyarbitrary sequence of attention window lengthsand utilityorderings, respectively,
{
i}i∈[n],t>0
possibly adversarial and adaptively chosen. Then with probability at least 1 δ, where 0 < δ 1 is
− ≤
any specified confidence parameter, the cumulative regret of Algorithm 1 over any time horizon T
is bounded by
T n
log(nT/δ)
R(πt;wt)= O ,
∆
i−1i !
t=1 i=2
X X
where for any pair i,j [n] of items, ∆ = µ µ is the gap in expected payoffs, and items are
ij i j
∈ −
assumed to be labeled in decreasing order of mean payoffs.
The above high probability regret bound also gives us an upper bound on the expected regret
incurred by our algorithm as a corollary (with expectation over the random payoffs observed by
our algorithm). We have that
T n n
log(nT/δ) log(nT/∆ )
E R(πt;wt) C (1 δ) +δ ∆ T = O max ,
max
≤ · − · ∆ · · ∆
! i−1i ! i−1i !
t=1 i=2 i=2
X X X
whereC istheconstantintheregretupperboundinTheorem3.2, and∆ := max µ µ is
max i,j∈[n] i j
−
the maximum (pseudo) regret that can be incurred in any trial. The final bound follows by setting
δ = (∆ T)−1, using the usual doubling trick for unknown time horizons.
max
In the special case that the item utilities are stationary, our algorithm achieves potentially an
even stronger regret upper bound, depending on the underlying structure of the utility parameters
of theinstance. Furthermore, as we shall see in the following section, this regret boundis also tight,
i.e. foranyfixedstationaryutility ordering,thereexistsasequenceofattention windowrealizations
where any algorithm must incur the regret we outline below. Before presenting this stronger regret
bound, we shall introduce some notation. Given an instance over [n] items, with utilities u
i i∈[n]
{ }
and mean payoffs µ , we say an item j [n] is dominated if there exists some item i [n]
i i∈[n]
{ } ∈ ∈
that beats item j both in terms of utility, as well as mean payoff, i.e. u > u and µ > µ . We
i j i j
then define D := j : i [n] where u > u and µ > µ to be the set of all dominated items,
i j i j
{ ∃ ∈ }
with S := [n] D := s ,...,s being the set of all remaining (undominated) items labeled in
1 n−|D|
\ { }
decreasing order of mean payoffs, i.e. µ > µ for i < j. Furthermore, for any undominated item
si sj
s i ∈ S, we define set D si := {j ∈ D : s i = argmax i′∈S:u i′>uj µ i′ } to be the set of items dominated
by s that are not already dominated by some other item i′ with a larger mean payoff than s . Note
i i
that the sets D are a disjoint partition of the dominated items D.
{
si}si∈S
9Algorithm 1 Algorithm for Stochastic Payoffs
1: Input: items [n], and confidence δ..
2: items i [n],set cumulative payoff r i 0, number of selections N i 0
∀ ∈ ← ←
3: for t= 1,... do
4: Observe utilities {ut i}i∈[n]
5: πt ←Find-Permutation([n],t,δ, {r i }i∈[n], {N i }i∈[n], {ut i}i∈[n])
6: Display πt; observe selection yt, payoff rt .
yt
7: Update r yt
←
r yt +r yt t, N yt
←
N yt +1.
Algorithm 2 Find-Permutation
1: Input: items [n], trial t, confidence δ, cumulative payoff r i i∈[n], number of selections
{ }
N , utilities u
i i∈[n] i i∈[n]
{ } { }
2: items i [n], set estimated mean payoff µˆ i r i/N i, confidence c i log(4nt2/δ)/N i.
∀ ∈ ← ←
3: Remaining items A [n], partial permutation π
← ← ∅ p
4: while A = do
6 ∅
5: S i A : j A, µˆ i+c i > µˆ j c j ⊲ Empirically undominated arms
← { ∈ ∀ ∈ − }
6: i∗
←
argmin i∈SN i ⊲ Least played empirically undominated arm
7: B i∗ j A: u j < u i∗ ⊲ Arms blocked by i∗
← { ∈ }
8: Update partial permutation π π+i∗+B i∗ ⊲ B i∗ ordered arbitrarily
←
9: A A (i∗ B i∗)
← \ ∪
10: Return: π
The following theorem then characterizes the regret achieved by Algorithm 1 for the special
case of stationary utilities.
Theorem 3.2. Given items [n] with stationary utilities u and Gaussian payoff distributions
i i∈[n]
{ }
(µ ,1) , let wt be any arbitrary sequence of attention window lengths, possibly adver-
i i∈[n] t>0
{N } { }
sarial and adaptively chosen. Then with probability at least 1 δ, where 0 < δ 1 is any specified
− ≤
confidence parameter, the cumulative regret of Algorithm 1 over any time horizon T is bounded by
T |S| |S|
log(nT/δ) log(nT/δ)
R(πt;wt)= O + .
 ∆ ∆ 
Xt=1 Xi=2
si−1si
Xi=1j X∈Dsi
sij
 
3.2.2 Regret Analysis
The proof of the regret upper bound, at its core, is a decomposition that charges the pseudo regret
incurred by displaying a permutation in a trial to a particular pair of items – one being the item
selected from the displayed permutation under the realized attention window in that trial, and
other being the item that would have been selected from an optimal permutation under the same
attention window realization. The challenge then is to show that the total amount of regret that
can be charged to any pair is then bounded (roughly by the inverse of the squared gap between
the mean payoffs of the items in that pair). The key to this charging argument is establishing the
structure of an optimal permutation, which we describe in the following lemma.
Lemma 3.3. Given an instance over [n] items with utilities u , and mean payoffs µ ,
i i∈[n] i i∈[n]
{ } { }
let S = s ,s ,...,s be the set of undominated items labeled in decreasing order of mean payoffs
1 2 k
{ }
10(or equivalently, in increasing order of utilities), i.e. for any s ,s S such that l < h, then
l h
∈
µ > µ (equivalently, u < u ). Then any permutation that maximizes the expected payoff for
sl sh sl sh
any attention window length 1 w n chosen independent of the realized payoffs of the arms,
≤ ≤
belongs to the family
Π∗ := (s ,B ,s ,B ,...,s ,B ) ,
{
1 s1 2 s2 k sk
}
where for any s S, B is a permutation of the set D .
s s
∈
Proof. We begin by boundingthe maximum expected payoff that can beobtained by any permuta-
tion for a fixed attention window length 1 w n. Given utilities u , for each item i [n],
i i∈[n]
≤ ≤ { } ∈
we define the set B := j [n] : u > u to be the set of items with utilities at most that of
i i j
{ ∈ }
item i. Now we claim that for any attention window of length w, the maximum expected payoff
of any permutation is at most max µ . This follows from the simple observation that
i∈[n]:|Bi|≥w−1 i
no item j with B < w 1 can be selected in a attention window of length w, regardless of it’s
j
| | −
position in the permutation, as the attention window must necessarily contain some higher utility
(more preferred) item j′ with u j′ > u j due to the pigeonhole principle. Given this observation, the
optimality of Π∗ follows immediately, as any permutation from this family achieves expected payoff
that matches this upper bound for all attention window lengths 1 w n.
≤ ≤
Before proceeding with the remainder of the proof, observe that the above described family of
optimal permutations contains the permutation that will be returned by the greedy Algorithm 2
when the estimated means of the arms respect the ordering induced by the true means, and the
confidence intervals are non-overlapping, i.e. µˆ ,c such that for all pairs of items i,j [n]
i i i∈[n]
{ } ∈
where µ > µ , µˆ c > µˆ +c .
i j i i j j
−
We now consider the following event of interest, which intuitively guarantees that across the
entire execution of our algorithm, the estimated means for all arms will not differ significantly
from their corresponding true means. Moreover, we show that this good event will occur with a
sufficiently high probability 1 δ, where δ is the given confidence parameter.
−
Lemma3.4. Givenitems[n]withutilities u , andGaussianpayoff distributions (µ ,1) ,
i i∈[n] i i∈[n]
{ } {N }
let wt be any sequence of attention window realizations, potentially chosen adversarially and
t>0
{ }
adaptively. Then given any confidence 0 < δ 1, for any item i [n] and trial t, we define the
≤ ∈
good event
rt log(4nt2/δ)
:= µ i ,
Ei,t
(cid:12)
i − N it (cid:12)≤ s N it
(cid:12) (cid:12)
where rt is the cumulative payoff receiv(cid:12)ed from (cid:12)item i until trial t, and Nt is the number of times
i (cid:12) (cid:12) i
item i was selected until trial t. Then the event := occurs with probability at least
i∈[n],t>0 i,t
E ∩ E
1 δ.
−
Proof. Consider any fixed item i [n]. For the purpose of analysis, we consider the following
∈
thought experiment to model the stochastic process that generates the payoff of item i - the payoffs
for this item are sampled independently from its underlying payoff distribution ahead of time and
are written on an infinite tape. If this item is selected in some trial (which will be a function
of the permutation πt played in that trial, as well as the attention window wt realized in that
trial), the algorithm simply reads the payoff from the next cell on the tape. Now observe that
for any fixed value N of the number of times item i is selected, the probability that the empirical
average deviates significantly from the true mean µ µˆ > log(4nt2/δ)/N is at most 2δ/(4nt2)
i i
| − |
by Hoeffding’s inequality (Theorem A.2). Furthermore, observethat this probability can befurther
p
upper bounded by δ/(2nN2), as N t for all t. By taking a union bound over all N > 0, and all
≤
11possible items i [n], we have that the probability that this event occurs for some item i [n]
∈ ∈
for some value of N is at most δ. Moreover, this bound holds independent of the realization of
the sequence of attention windows wt . Our claimed bound follows by observing that this is
t>0
{ }
exactly the probability of the event .
¬E
From this point on, we shall condition on the good event , following which the regret incurred
E
by our algorithm will be bounded deterministically. We shall focus on the case with stationary
utilities, as it is a more fine grained analysis. The proof of the general case follows from a cruder
analysis of the same.
Proof of Theorem 3.2. We begin by decomposing the cumulative regret incurred by our algorithm
into regret due to playing any item j [n]. For any fixed optimal permutation π Π∗, and
∈ ∈
any attention window of length w, let s(w) S be the highest utility undominated item with
∈
rank at most w in π, i.e. s(w) : π−1(s(w)) w. Observe that this item is uniquely defined
≤
by the family of permutations Π∗ and the attention window length w, and is independent of
the specific choice of permutation π Π∗. This is precisely the item that would be played by
∈
any optimal permutation when an attention window of size w realizes. Then we have that the
cumulative regret of our algorithm over any time horizon of length T is given by T R(πt;wt)=
t=1
T I yt = j (µ µ )
j∈[n] t=1 s(wt) − j P
We shall now bound the total regret incurred due to playing some fixed item j [n] when some
P P (cid:2) (cid:3) ∈
undominated item s S where µ > µ could have been played, i.e. trials where yt = j and
i
∈
si j
s(wt) = s . We refer to such an event as an inversion between s and j. Observe that a necessary
i i
condition for such an inversion to occur, is that item j is placed in an unblocked position (i.e. there
is no higher-utility item placed before j in πt) in addition to either one of (1) j being placed before
s in πt, or (2) j being placed after s but there is at least one more item with utility smaller than
i i
u that is placed after j in πt (i.e. item j could have been pushed out of the attention window
si
wt by placing this blocked item before j instead, forcing the selection of s under this attention
i
window realization). Observe that due to the nature of our algorithm 2 which after fixing the
position of any empirically undominated item in the partial permutation places all items that are
blocked by that item immediately after it, an inversion due to case (2) can never occur. The key
idea in our regret analysis is an argument that charges regret to inversions, which we show will
occur a bounded number of times as opposed to the number of selections which can be arbitrary.
For any pair of items s S,j [n], we claim that the number of inversions between s ,j over
i i
∈ ∈
any time horizon of length T is at most M := 4log(4nT2/δ)/∆2 . More precisely, we claim that
sij sij
an inversion between s ,j cannot occur when Nt M , where Nt is the number of times item
i j ≥ sij j
j has been selected until trial t, which is a tighter condition since Nt always upper bounds the
j
number of inversions between j and any other item s i′ until trial t. This follows by observing that
we receive a sample from the inferior item j every time an inversion occurs with any other superior
item s i′. Let t′ be the trial when N
j
first exceeds M sij, and let N st i′ be the number of times item
s has been selected until trial t′. We have the following two possibilities: either (1) Nt′ M
i si ≥ sij
due to which (by event ) it must be the case that µˆt ct > µˆt +ct for all subsequent trials
E si − si j j
t > t′, or (2) Nt′ < M , due to which (by event ) it must be the case that µˆt +ct > µˆt ct.
si sij E si si j − j
In either case, in all subsequent trials after t′, our algorithm will always place s ahead of j if j
i
is not already blocked, precluding the possibility of an inversion between s and j; in case (1), s
i i
will always empirically dominate j, due to which j cannot be added into the partial permutation
in an unblocked position until s has been placed first. In case (2), our algorithm prioritizes lower
i
selected items amongst items where no item beats another with confidence, due to which it will
always place s ahead of j if j is not already blocked in all subsequent rounds until eventually case
i
(1) occurs.
12Equipped with this bound on the number of inversions, we are now ready to prove our claimed
regret bound. For a cleaner analysis, for any item j [n], we defineM := 0, and∆ to besome
∈
s0j s0j
arbitrarily large positive numberapproaching infinity. We shall analyze theregret for undominated
and dominated items separately. For any undominated item s S, we have
j
∈
T T j−1
I yt = s (µ µ ) = I yt = s M <Nt M µ µ
j s(wt) − sj j ∧ si−1sj sj ≤ sisj s(wt) − sj
Xt=1
(cid:2) (cid:3)
Xt=1 Xi=1 h
(cid:0) (cid:1)
(cid:16) (cid:17)i
(cid:0) (cid:1)
j−1 T
I yt = s M <Nt M µ µ
≤ j ∧ si−1sj sj ≤ sisj si − sj
Xi=1 Xt=1 h
(cid:0) (cid:1)
(cid:16) (cid:17)i
(cid:0) (cid:1)
j−1 j−1
1 1
(M M ) ∆ = 4 ∆ log(4nT2/δ)
≤ sisj − si−1sj · sisj · sisj · · ∆2 − ∆2
i=1 i=1 sisj si−1sj!
X X
j−1
= 4 log(4nT2/δ) 1+ ∆ sisj 1 1
· · ∆ · ∆ − ∆
i=1 (cid:18) si−1sj(cid:19) (cid:18) sisj si−1sj(cid:19)
X
j−1
1 1 8
8 log(4nT2/δ) log(4nT2/δ),
≤ · · ∆ − ∆ ≤ ∆ ·
i=1 (cid:18) sisj si−1sj(cid:19) sj−1sj
X
where first inequality follows from our earlier condition on an inversion between items s ,j, which
i
gives us that µ < µ for all trials t where Nt > M , the penultimate inequality follows
s(wt) si j sij
from the fact that ∆ > ∆ for any i, and the last inequality follows from cascading the
si−1sj sisj
summation.
The regret analysis for a dominated item j D uses one additional observation: item j is
∈
permanently eliminated after N j = M sj∗j, where s j∗ = argmax i∈S:ui>uj µ i is the highest mean
undominated item that dominates item j. This is because after item j is selected M times, it
s j∗j
will never be placed in an unblocked position in any subsequent permutation; either j is already in
a blocked position, or s j∗ must precede j in the permutation, which will then block j. Therefore,
applying an identical calculus as before, we have that for any dominated item j D,
∈
T T j∗
I yt = j (µ µ )= I yt = j M < Nt M (µ µ )
s(wt) − j ∧ si−1j j ≤ sij s(wt) − j
t=1 t=1 i=1
X (cid:2) (cid:3) XX (cid:2)(cid:0) (cid:1) (cid:0) (cid:1)(cid:3)
j∗ T
I yt = j M < Nt M (µ µ )
≤ ∧ si−1j j ≤ sij si − j
i=1 t=1
XX (cid:2)(cid:0) (cid:1) (cid:0) (cid:1)(cid:3)
8
log(4nT2/δ).
≤ ∆ ·
s j∗j
Combiningtheaboveregretboundsforundominatedanddominateditemsgives ustheregretupper
bound claimed in Theorem 3.2.
The proof of Theorem 3.1 follows essentially identically, except that in our charging argument,
we cannot consider justpairs of items with one item in thepair beingundominated(since the set of
undominated items can change from round to round). Our bound M on the number of inversions
ij
13between any pair of items i,j with µ < µ still holds, which gives us that for any item j,
i j
T T j−1
I yt = j (µ µ )= I yt = j M < Nt M (µ µ )
s(wt) − j ∧ ij j ≤ i+1j s(wt) − j
t=1 t=1 i=1
X (cid:2) (cid:3) XX (cid:2)(cid:0) (cid:1) (cid:0) (cid:1)(cid:3)
j−1 T
I yt = j M < Nt M (µ µ )
≤ ∧ ij j ≤ i+1j i − j
i=1 t=1
XX (cid:2)(cid:0) (cid:1) (cid:0) (cid:1)(cid:3)
8
log(4nT2/δ).
≤ ∆ ·
j−1j
Summing up this quantity over all items j [n], j = 1 gives us the regret upper bound claimed in
∈ 6
Theorem 3.1.
3.3 The Lower Bound
In this section we will present an instance-dependent lower bound for our problem in the special
case of stationary utilities, showing that the upper bound given in Theorem 3.2 is tight.
We define an algorithm to be consistent if, for any p < 1, there exists a constant C such that
p
for sufficiently large T, E[R ( )] C Tp, uniformly over all instances of the problem.
T p
A ≤
Theorem3.5. Givenitems[n]withutilities u andGaussianpayoff distributions (µ ,1)
i i∈[n] i i∈[n]
{ } {N }
such that µ ’s are distinct, there exists a sequence of attention window realizations wt such
i t∈[T]
{ }
that the expected regret of any consistent algorithm over this instance is lower bounded as:
A
E[R T( ; wt t∈[T])] |S|−1 1 |S| 1
lim A { } = Ω + , (1)
T→∞ logT 
Xi=1
∆ sisi+1
Xi=1j X∈Dsi
∆ sij
 
where ∆ = µ µ , and the other quantities such as S,s ,D are defined according to Lemma 3.3.
ij i
−
j i si
Note that our lower bound matches the upper bound given in Theorem 3.2 in the case of
fixed utilities. This shows that our algorithm achieves instance-wise optimality in the case of fixed
utilities. We also have an immediate corollary of the above theorem.
Corollary 3.6. In the setting of Theorem 3.5, if the instance is such that u < u < < u and
1 2 n
···
µ > µ > > µ then
1 2 n
···
E[R T( ; wt t∈[T])] n−1 1
lim A { } = Ω .
T→∞ logT ∆ ii+1!
i=1
X
Note that this lower bound matches the upper bound in Theorem 3.1. Our construction uses
ideas developed for classical bandits (see Chapter 16 in Lattimore and Szepesv´ari [2020]), and we
defer a formal proof to Appendix B in the interest of space.
4 Adversarial Payoffs and Stochastic Windows
In this section we consider a setting where the payoffs can be arbitrary or adversarial. Specifically,
the environment generates the payoff vectors rt ahead of time. At time t, when the user
t∈[T]
{ }
selects an item yt [n], the corresponding payoff rt is revealed to the algorithm. We also assume
∈ yt
thatthewindowrealizations arestochastic. Specifically, ateach timet,theuserdrawswt according
14to Multinomial(q , ,q ) where q = 1 and q [0,1] is probability of realization of a
1 ··· n w∈[n] w w ∈
window of size w. For the ease of exposition, we first restrict our attention to the case with
P
fixed utilities. Then we show how our result easily extends to the general case. Because of that,
the platform’s goal is to minimize total (pseudo-)regret with respect to the optimal in-hindsight
permutation, defined as follows:
Regret(T) , max E[rt(π) rt(πt)].
π∈Sn−1 −
t∈[T]
X
4.1 Warm Up: Sub-optimal Regret by Inducing Exploration
In this section we consider a laziness constraint on the window probabilities (q , ,q ) and give
1 n
···
a regret upper bound of O(T2/3) under this constraint using the ǫ-greedy algorithm. This laziness
constraint requires that q q q and corresponds to a setting where the users are biased
1 2 n
≥ ··· ≥
towards smaller consideration set or window sizes.
We show that under this constraint it is possible to induce an uniform exploration over items
whichis requiredby theǫ-greedy algorithm in ordertoachieve aO(T2/3)regret. Formally, weshow
that it is possible to select a distribution over permutations such that when a ‘lazy’ user is shown
a random permutation drawn from this distribution the effective selection probability for any item
is 1. Intuitively, this constraint allows uniform exploration as it is possible to ‘incentivize’ a ‘lazy’
n
user to select the worst utility item with sufficient probability by placing it in the first position.
We first present discuss how to select a distribution over permutations that induced uniform
exploration using a combinatorial argument. For ease of exposition, let the items be indexed in
increasing order of utilities, i.e. u < u < < u . For each i [n], we define the permutation
1 2 n
··· ∈
π = (i,i 1, ,1,i+1,i+2, ,n). We consider distribution over these permutations where π
i i
has proba− bilit· y·· mass α > 0 wit· h·· n α = 1. We define
i i=1 i
α :=
Pi j− =1 1q j −(i −1)q i
, with α =
1
.
i n ·P( i j− =1 1q j) ·( i j=1q j) 1 nq 1
It is very easy to observe that Pα > 0 for Pall i [n] using the laziness assumption. We now
i
∈
establish that α n induces uniform item selection probabilities using an induction argument.
{ i }i=1
Since, item 1 is only selected in π w.p. q it is easy to observe that p = q α = 1/n. Now, let us
1 1 1 1 1
assume that p = 1/n holds for some i 1. We will show that p = 1/n. In general an item i
i−1 i
−
is not selected in permutations π , π , and is selected w.p. q in permutations π , π and
i+1 n i 1 i−1
··· ···
w.p. q in π . Hence, we get
1 1
i i−1 i i−2
q
i−1
p = α q + α q = α q +α q + α q
i i j j i i j i−1 i j i
· · · · · · q
i−1
j=1 j=1 j=1 j=1
X X X X
i i−1
1 q 1
i
= α q +α q +( α q ) = ,
i j i−1 i i−1 j
· · n − · q n
i−1
j=1 j=1
X X
where the third equality holds by the induction hypothesis and the last equality holds by plugging
the values of α and some algebra. Finally, the fact that α = 1 can easily be established by the
i i
fact that p = 1.
i i P
Given this distribution over permutations the algorithm is simple: at each round t, toss a
P
coin with probability of heads being O(T−1/3). If this coin turns up head then sample i
∼
Multinomial(α , ,α ) and play the permutation π . This induces a uniform distribution over
1 n i
···
15item selections. If the coin turns up tails, then play the optimal permutation with respect to the
estimated payoffs from previous trials. Using a standard argument for the ǫ-greedy algorithm one
can easily show that the total regret of exploration as well as exploitation is O(T2/3).
In this section we showed that theuniformdistributionis admissibleunderthe laziness assump-
tion. In the next section we give a characterization of the space of all admissible item distributions,
which will eventually lead to a O(√T) regret algorithm.
4.2 Admissible Selection Probabilities
In this section, we introduce the notion of “admissible” item selection probabilities, which at a
high level, are an alternate representation of the feasible set of actions that can be performed in
any trial in our model. Consider any fixed permutation π over items. Then observe that since the
realized attention window is an i.i.d. random variable drawn from the distribution q ∆ (where
n
∈
∆ is the n-dimensional probability simplex), the selected item is also an i.i.d. random variable
n
drawn from the distribution P(π)q ∆ , where P(π) 0,1 n×n is the “selection matrix” of π,
n
∈ ∈ { }
the entries P (π) of which indicate whether item i is selected in permutation π when the realized
i,w
attention window is of size w. Therefore, we can equivalently view a permutation as a categorical
distribution over items (more precisely, the one induced by the distribution over attention windows
on that permutation) and consequently, view the action of displaying a permutation as choosing
a distribution over items instead. The objective then is to be competitive with the best fixed
distribution over items in hindsight. However, the chosen distributions cannot be arbitrary – they
must correspond to a distribution induced by some permutation (or some convex combination of
them since an algorithm can also play “mixed strategies” in any trial). We refer to such feasible
categorical distributions as “admissible” selection probabilities. In this section, we show that this
set , despite having factorially many vertices, can be succinctly characterized by only a few
P
constraints.
In order to describe this set, we start by introducing some notation. In the remainder of
this section, we shall find it convenient to label items (without loss of generality) in increasing
order of utility, and use comparisons between items to refer to comparisons between their utilities
i.e. i,j [n], i < j u < u . As mentioned earlier, any permutation π can equivalently
i j
∀ ∈ ≡
be represented as a “selection” matrix P(π) 0,1 n×n, where the rows correspond to items,
∈ { }
and the columns correspond to the realized lengths of the windows. The entries of this matrix
P (π) = I π−1(i) w j [n] :π−1(j) w,i > j indicate whether an item i is selected when
i,w
≤ ∧∀ ∈ ≤
the realized attention window is of size w, which occurs iff item i has the highest utility (highest
(cid:2) (cid:3)
label) amongst the first w items in permutation π. Since the categorical distribution induced by
π can now be represented as P(π)q, characterizing the set of all admissible selection probabilities
reduces to characterizing the set of all valid selection matrices, i.e. selection matrices P = P(π)
that are realized by some permutation π, or a convex combination thereof. We claim that this set
16is as follows:
P
= P such that (2)
P { }
0 P 1 i,w [n] (C.1)
i,w
≤ ≤ ∀ ∈
n
P = 1 w [n] (C.2)
i,w
∀ ∈
i=1
X
P = 0 i,w [n] :i < w (C.3)
i,w
∀ ∈
n n
P i,w P i,w′ k [n 1], w,w′ [n]: w <w′ (C.4)
≤ ∀ ∈ − ∈
i=n−k i=n−k
X X
These constraints intuitively describe the following properties of item selection probabilities (more
generally induced by a convex combination of selection matrices):
(C.1)Theprobabilityofselectinganitemiunderarealizedwindowlengthwmustbenon-negative
and upper bounded by 1.
(C.2) Exactly one item must be selected for every window length w.
(C.3) Item i (i.e. ith lowest utility) cannot beselected in any window of length strictly larger than
i.
(C.4) The selection probability of some item from the set of the k highest utility items can only
increase with increasing window lengths (larger window lengths are increasingly favourable
to high utility items).
The easy direction, i.e. any selection matrix P(π) induced by some permutation π must belong
to this set, follows directly from the definition of P(π). Observe that
P (π) = I π−1(i) w j [n]:π−1(j) w,i > j ,
i,w
≤ ∧∀ ∈ ≤
(cid:2) (cid:3)
due to which it is a 0,1 matrix with exactly one 1 in every column, satisfying constraints (C.1)
{ }
and (C.2), the entries P (π) = 0 for any w > i since by the pigeonhole principle, there must exist
i,w
some item j > i with π−1(j) w satisfying constraint (C.3), and due to the laminar nature of
≤
the windows, i.e. for any w < w′, i [n] : π−1(i) w i [n] : π−1(i) w′ , the indicator
{ ∈ ≤ } ⊂ { ∈ ≤ }
can only “shift downwards” across columns, i.e. the labels of the items (utilities) selected under
increasing window lengths can only increase, satisfying constraint (C.4).
To prove the hard direction, i.e. any feasible matrix P (potentially fractional) can be
∈ P
decomposedintoaconvex combination ofselection matrices ofpermutations,wewillinfactprovide
a stronger constructive guarantee: there is an efficient algorithm that, given as input any feasible
matrix P, returns at most n2 many permutations along with their corresponding weights such that
the weighted average of their selection matrices produces P.
4.2.1 The Rounding Algorithm
Our roundingalgorithm for decomposing a fractional selection matrix P into a convex combination
of integral selection matrices of permutations is a recursive peeling algorithm. In each round, our
algorithm identifies an integral matrix supported on the non-zero coordinates of this fractional
matrix and peels off its maximal contribution from the fractional matrix. This integral matrix
is identified as follows: for each window length w [n], we identify the lowest utility item i
w
∈
17Algorithm 3 Round Fractional Selection Matrix (RFSM)
1: Input: Feasible selection probability matrix P .
∈ P
2: Decomposition into (weight, permutation) pairs R , residual probability mass p 1.
← ∅ ←
3: while P = 0 do
6
4: For every w
∈
[n], define i w
←
min {i
∈
[n]: P i,w > 0 }. Then m
←
min w∈[n]P iw,w.
5: Set integral selection matrix Pˆ 0,1 n×n, where Pˆ i,w = 1 if i= i w and 0 otherwise.
∈ { }
6: Set π Integral-Permutation(Pˆ).
←
7: Update decomposition R R (p m,π).
← ∪ ·
8: Update residual mass p p (1 m)
← · −
9: Update and scale residual selection probabilities P (1 m)−1(P m Pˆ).
← − − ·
10: Return: R
that has a non-zero probability P > 0 of being selected in this window. These items i then
iw,w w
define an integral matrix Pˆ, which has a 1 in coordinates (i ,w) for each window length (column)
w
w [n] and 0 everywhere else. As we will see later in the proof of correctness of this algorithm,
∈
this construction always produces a valid integral selection matrix, i.e. there is some permutation
π such that this integral matrix Pˆ = P(π) corresponds to the selection matrix of π. Moreover, this
permutation π can beefficiently recovered from Pˆ. Theweight m of this integral (selection) matrix
Pˆ is then its maximal contribution m = min P in P. This weighted integral matrix is then
w∈[n] iw,w
removed from P, following which the residual is rescaled to project it back into the feasible set ,
P
i.e. P is updated to be (1 m)−1(P m Pˆ). We then recurse on this rescaled residual matrix,
− − ·
stopping when the matrix becomes all 0. The depth of this recursion is bounded by n2, since in
each round, by maximally removing an integral selection matrix, we reduce the number of non-zero
entries in P by at least 1. As a consequence, the support of the resultant decomposition of P into
integral selection matrices/permutations is also bounded by n2. The formal rounding algorithm
is described in Algorithm 3, with the algorithm for reconstructing a permutation from an integral
selection matrix being described in Algorithm 4.
Algorithm 4 Integral Permutation
1: Input: Feasible integral selection probability matrix P 0,1 n×n P .
∈ { } ∧ ∈ P
2: Remaining items A [n], partial permutation π .
← ← ∅
3: for w = 1,...,n do
4: i∗ i :P i,w = 1 ⊲ i∗ is the item selected in a window of length w
←
5: if i∗ / A then ⊲ i∗ already appears earlier in π
∈
6: i∗ min i A
← { ∈ }
7: Update π π+i∗, A A i∗. ⊲ Assign item i∗ to the wth position in π
← ← \
8: Return: π
The following theorem describes the main result of this section.
Theorem 4.1. Any matrix P can be decomposed into a convex combination P = z p
∈ P r=1 r ·
P(π ) of selection matrices P(π ) of at most z n+1 permutations π , where z is
r r r∈[z] r r∈[z]
{ } − { } P
the number of non-zero entries in P. Moreover, there is an efficient algorithm for finding this
decomposition p ,π .
r r r∈[z]
{ }
Proof. We shall prove this claim via induction on z, the number of non-zero entries in matrix
P . To show the base case where z = n (observe that z n as every column sums to 1),
∈ P ≥
18we will show that any integral matrix P corresponds to a selection matrix P(π) of some
∈ P
permutation π, which can be reconstructed efficiently given P (Algorithm 4). In order to do so,
we shall find it useful to first understand the structure of any integral matrix P. For every item
i [n], we define the set W := w [n] : P = 1 , which corresponds to the window lengths
i i,w
∈ { ∈ }
under which this item would be selected. Note that this set can be empty for some items (the
items that never get selected). If on the other hand, we have that W > 1 for some item i [n],
i
| | ∈
it must be the case that W must contain consecutive values due to the fact that P is integral and
i
satisfies constraint (C.4). Furthermore, it must be the case that w W ,w i due to constraint
i
∀ ∈ ≤
(C.3), and W mustpartition [n]duetoconstraint (C.2). Given thesesets, wenow iteratively
i i∈[n]
{ }
construct a permutation π, starting with an empty permutation as follows: for each position w in
increasing order of positions, we identify the item i whose set W contains w. Observe that such
i
an item i must exist since the sets W partition [n]. If i has not already been placed in the
i i∈[n]
{ }
partial permutation π thus far, we assign i to position w in our permutation. Otherwise, we place
in position w, the lowest utility item i′ that has not yet been placed in the permutation so far. We
claim that this item i′ must be such that i′ < i and W i′ = . To see this, consider the top left
∅
sub-matrix P P P which musthave at least as many rows as columns since i w. Since
1,1 a,b i,w
≤ ≤ ≥
exactly w 1 items have been placed in π so far, which include all items i′′ with an entry 1 in some
−
in some column < w (we assign positions to items in increasing order of positions), and there are
exactly w entries with value 1 in this submatrix, the pigeonhole principle guarantees the existence
of at least one item i′ (which by definition satisfies i′ < i) with an all 0 row in this submatrix
(which gives us W i′ = ), that has not been placed in π so far. Given this construction, it is now
∅
straightforward to verify that this will produce a permutation π with P(π) = P, proving our base
case.
For the induction step, we shall prove our claim for any matrix P with z non-zero entries,
∈ P
assuming our claim holds for all n z′ < z. The proof of this effectively reduces to showing two
≤
things: (a) the integral matrix Pˆ extracted from P is contained in , and (b) the residual matrix
P
R = (1 m)−1(P mPˆ), if it is not an all 0 matrix, is also contained in , where quantities Pˆ and
− − P
m are defined in Algorithm 3, and discussed in its description. The former allows us to leverage
our base case – since the integral matrix Pˆ has exactly n non-zero entries, it corresponds to the
selection matrix P(π ) of some permutation π . The latter allows us to leverage our induction
0 0
hypothesis – since the residual matrix R must contain at least one fewer non-zero coordinate than
P (in particular, the non-zero entry P for which the minimum value m is achieved becomes a
iw,w
zero entry in R), it can be decomposed into a convex combination R =
z′
p P(π ) of selection
r=1 r · r
matrices P(π r) r∈[z′] of z′ (z 1) n+1 permutations π r r∈[z′]. We can combine these two
{ } ≤ − − { } P
properties to decompose P = m P(π )+
z′
(1 m) p P(π ) into a convex combination of
· 0 r=1 − · r · r
selection matrices of z′+1 z n+1 permutations, completing the induction step.
To show that Pˆ , o≤ bser− ve that it iP s a 0,1 matrix with exactly one 1 in each column,
∈ P { }
satisfying constraints (C.1) and (C.2), and since P , it must be that i w for each w [n]
w
∈ P ≥ ∈
satisfying constraint (C.3). To show constraint (C.4) is satisfied, we claim that for any w < w′, it
must be that i
w
i w′. To see this, assume for the sake of contradiction that this does not hold for
1so ,m we hp era ei ar sw,w n i′ =, i≤ i w.e P. ii ,ww′ =< 1i w , ww hh ie cr he gw iv< esw u′. s T thh ae tn ob n is =e ir wv Pe it ,wh ′at <we h n ia =v iwe P Pi,n i w=i fw oP ri w,w′ <≤ w1 ′− , P vii ow l′ a,w ti′ n< g
constraint (C.4) for matrix P, contradicting the assumption P .
We will nP ow show that the rescaled residualmaP trix R = (1 ∈ mPP )−1(P mPˆ) . We have that
− − ∈ P
for any coordinate [P mPˆ] , it is equal to P m if i = i and P otherwise. By definition of
i,w i,w w i,w
− −
m = min P ,wehavethat[P mPˆ] 0. Itmustalsobethecasethat[P mPˆ] 1 m,
w∈[n] iw,w
−
i,w
≥ −
i,w
≤ −
since for every i = i , P 1 P which follows by definition of i and the fact that P
6
w i,w
≤ −
iw,w w
19satisfies (C.2). Moreover, this also gives us that for any w, n [P mPˆ] = n P m =
i=1 − i,w i=1 i,w −
1 m. Therefore, R satisfies constraints (C.1) and (C.2). R also satisfies constraint (C.3) quite
− P P
straightforwardly as P satisfies constraint (C.3) and any 0 coordinate in P continues to remain a
0 coordinate in R. To show that R also satisfies constraint (C.4), consider any pair of columns
w < w′. The only case we need to consider is i
w
< n k i w′, since in all other cases, constraint
− ≤
(C.4) is satisfied straightforwardly; observe that for matrix P mPˆ, in the case that n k i ,
w
we have n i=n−k[P −mPˆ] i,w = n i=n−k[P −mPˆ] i,w′ = 1 −m− , and in the case that n −− k >≤ i w′,
we have Pn i=n−k[P −mPˆ] i,w = Pn i=n−kP i,w and n i=n−k[P −mPˆ] i,w′ = n i=n−kP i,w′, and since
P satisfie Ps constraint (C.4), it mu Pst be that n i=n−Pk[P −mPˆ]
i,w
≤
n i=n−kP[P −mPˆ] i,w′. Now if k
is such that i
w
< n −k
≤
i w′, we have n i=Pn−k[P −mPˆ]
i,w
= n i=Pn−kP
i,w
≤
1 −P
iw,w
≤
1 −m.
Furthermore, we have n i=n−k[P
−
mPˆ P] i,w′ = n i=n−kP i,w′ −Pm = 1
−
m. Therefore, we have
n i=n−k[P −mPˆ]
i,w
≤
1 P−m = n i=n−k[P −mPˆ P] i,w′. Therefore, R also satisfies constraint (C.4),
which proves R , completing the proof of our claim.
P ∈ P P
The consequence of Theorem 4.1 is that we can efficiently simulate any “admissible” selection
probabilities by randomly sampling a permutation π p ,π from the above decomposition.
r r r∈[n]
∼ { }
4.3 The Algorithm
We are now ready to present our algorithm that achieves O(√T) regret. Our algorithm is based
on online linear optimization with bandit feedback over the space of admissible probabilities. We
use the characterization of this space of admissible probabilities from the previous section in order
to decompose an induced probability distribution over items into a distribution over permutations.
We first define the problem of linear optimization with bandit feedback.
Definition 4.1 (Bandit Linear Optimization (BLO)). Let the time-horizon be T and action set of
the player be Rd. In each round t [T], the player chooses an action at and the adversary
C ⊆ ∈ ∈ C
simultaneously chooses a loss function ℓt Rd. The player then observes the loss (ℓt)⊤at. The
performance of the player is evaluated in t∈ erms of regret R , defined as R = E[ T (ℓt)⊤at]
T T t=1 −
min E[ T (ℓt)⊤a∗].
a∈C t=1 P
ThispProblemhasbeenwell-studiedinonlinelearningliterature[Bubeck et al.,2012,Hazan et al.,
2016], andthereareseveral BLOalgorithms thatachieve aregret ofO(√T). Inour resultweusethe
well-knownOnlineStochasticMirrorDescent(OSMD)algorithm. WeassumeuseOSMDinablack-box
manner assuming access to two functions OSMDAct(t) that outputs tehe action at at time t, and
OSMDFeed(t,ℓˆt) that takes as input an estimate of ℓˆt of the loss at time t. ∈ C
We first describe the reduction for the case of fixed utilities across time. Given the set of
utilities u n , the action space ∆ is the space of all admissible item selection probabilities,
{ i }i=1 C ⊆ n
i.e. = p ∆ : P with p = Pq where is defined in Equation 2. It is easy to see that
n
C { ∈ ∃ ∈ P } P
this set is closed and convex as it is the convex hull of item selection probabilities induced by the
permutations π . Also, the loss vector ℓt at time t is the defined as ℓt = rt. At each time
n−1
∈ S −
t [T] we call OSMDAct(t) which outputs a vector pt . Now, since we cannot directly select an
∈ ∈ C
item according to pt, we need to find a distribution over permutations such that playing a random
permutation according to this distribution induces item selection probabilities pt. In order to find
such a distribution we will utilize the decomposition algorithm from the previous section. This
algorithm requires an item selection matrix P as input. However, one can find a matrix P
∈ P ∈ P
such that pt = Pq in polynomial time by describing P using a set of linear constraints. We then
use the RFSM algorithm (Algorithm 3) over P to find a distribution over permutations. Next, we
D
20play πt and observe the payoff rt of the selected item. Note that yt is distributed according
∼ D yt
to pt.
We now need to provide unbiased estimates of the loss ℓt to OSMD. Let ℓˆt = rt/pt for i = y
i − i i t
and ℓˆt =0 for i = y . We then have that E[ℓˆt] = pt −r yt +(1 pt) 0 = ℓt
i 6 t i i · pt i − i · i
We present a pseudo-code of our algorithm in Algorithm 5.
Algorithm 5 Ranking for Limited-Attention Users using BLO
1: Input: items [n], space of admissible selection probabilities ∆ n
C ⊆
2: Initialize the OSMD algorithm over the action space
C
3: for t = 1,...,T do
4:
pt OSMDAct(t)
←
5: Find P such that pt = Pq
∈ P
6: RFSM(P) (Algorithm 3)
D ←
7: Sample πt and play πt
∼ D
8: Observe the payoff rt of the selected item yt
yt
9:
OSMDFeed(t,ℓˆt)
We now state Theorem 5.7 from Bubeck et al. [2012] that bounds the regret of OSMD.
Theorem 4.2 (OSMD). For any closed convex action set Rd, the OSMD algorithm with a
C ⊆
regularizer F : R + over a sequence of loss vectors ℓt T with access to loss estimates
ψ C → + ∪{ ∞} { }t=1
ℓˆt such that E[ℓˆt] =ℓt satisfies R 2√2Tn, for ψ(x) = x−2 and F (a) = n aiψ−1(s)ds.
T ≤ ψ i=1 0
Using the above theorem we directly get a regret upper bound for AlgorPithmR5:
T T T T
max E[ (rt)⊤p ] E[ (rt)⊤p ] = max E[ (rt)⊤p ] E[ (rt)⊤pt]
π πt π
π∈Sn−1 − π∈Sn−1 −
t=1 t=1 t=1 t=1
X X X X
T T
= max E[ (rt)⊤p] E[ (rt)⊤pt]
p∈Conv({pπ} π∈Sn−1)
t=1
−
t=1
X X
T T
= E[ (ℓt)⊤pt] minE[ (ℓt)⊤p] 2√2Tn,
− p∈C ≤
t=1 t=1
X X
where the first equality holds as πt is selected in a way that the induced item distribution is pt, the
second equality follows due to the well-known fact that any linear function is maximized at one of
the vertices of a convex constraint set, the third equality follows from the definition of ℓt.
Note that the regret achieved by this algorithm is tight because an Ω(√Tn) lower bound is
known for the multi-armed bandit problem which is a special case of our problem when q = 1.
1
Now, consider the case of changing utilities across time. Recall, that the observed set of the
utilities at time t is ut n . Without loss of generality, we will assume that ut 1,2, ,n
{ }i=1 i ∈ { ··· }
for all t [T] and i [n]. At time t, let σt : [n] [n] be a permutation that maps items to its
∈ ∈ →
utilities, i.e. σt(i) = j if ut = j. We now run the same algorithm as before except one change in the
i
computation of loss estimates: if ℓˆt = rt/pt if i = σt(y ) and ℓˆt = 0 otherwise. The same regret
i − i i t i
bound as before holds against the best fixed mapping from utilities to permutations by the simple
observation that losses are now associated with utility values rather than items.
215 Conclusion
In this paper,we studied arelatively general modelfor onlinelearning-to-rank for limited-attention
users whose preferences may not align with the goals of the platform. We considered two settings
for online learning with bandit feedback: stochastic item payoffs with adversarial attention window
lengths, and adversarial item payoffs with stochastic attention window lengths (with known distri-
bution). In both settings, we designed interpretable algorithms that achieve low regret against a
natural benchmark. In the stochastic payoff setting, our algorithm achieved an instance-dependent
O(logT) regret that we further showed was optimal via a matching regret lower bound. In the ad-
versarial payoff setting, our algorithm achieved a worst-case O(√T) regret, which was also optimal
due to known regret lower bounds for classical bandits.
References
Andr´es Abeliuk, Gerardo Berbeglia, Manuel Cebrian, and Pascal Van Hentenryck. Assortment
optimization under a multinomial logit model with position bias and social influence. 4OR, 14
(1):57–75, 2016.
George Ainslie. Specious reward: a behavioral theory of impulsiveness and impulse control. Psy-
chological bulletin, 82(4):463, 1975.
Ali Aouad and Danny Segev. Display optimization for vertically differentiated locations under
multinomial logit preferences. Management Science, 67(6):3519–3550, 2021.
Ali Aouad, Vivek Farias, and Retsef Levi. Assortment optimization under consider-then-choose
choice models. Management Science, 67(6):3368–3386, 2021.
Arash Asadpour, Rad Niazadeh, Amin Saberi, and Ali Shameli. Sequential submodular maximiza-
tion and applications to ranking an assortment of products. Operations Research, 2022.
Jean-Yves Audibert, S´ebastien Bubeck, et al. Minimax policies for adversarial and stochastic
bandits. In COLT, volume 7, pages 1–122, 2009.
Jean-Yves Audibert, S´ebastien Bubeck, and Ga´bor Lugosi. Regret in online combinatorial opti-
mization. Mathematics of Operations Research, 39(1):31–45, 2014.
PeterAuer,NicoloCesa-Bianchi,YoavFreund,andRobertESchapire. Gamblinginariggedcasino:
The adversarial multi-armed bandit problem. In Proceedings of IEEE 36th annual foundations
of computer science, pages 322–331. IEEE, 1995.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2):235–256, 2002a.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-
armed bandit problem. SIAM journal on computing, 32(1):48–77, 2002b.
Omar Besbes, Yuri Fonseca, and Ilan Lobel. Contextual inverse optimization: offline and online
learning. arXiv preprint arXiv:2106.14015, 2021.
S´ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.
22MaximeCCohen,IlanLobel,andRenatoPaesLeme. Feature-based dynamicpricing. Management
Science, 66(11):4921–4943, 2020.
Mahsa Derakhshan, Negin Golrezaei, Vahideh Manshadi, and Vahab Mirrokni. Product ranking
on online platforms. Management Science, 68(6):4024–4041, 2022.
Miroslav Dud´ık, Nika Haghtalab, Haipeng Luo, Robert E Schapire, Vasilis Syrgkanis, and Jen-
nifer Wortman Vaughan. Oracle-efficient online learning and auction design. Journal of the
ACM (JACM), 67(5):1–57, 2020.
Kris J Ferreira, Sunanda Parthasarathy, and Shreyas Sekar. Learning to rank an assortment of
products. Management Science, 68(3):1828–1848, 2022.
Peter Frazier, David Kempe, Jon Kleinberg, and Robert Kleinberg. Incentivizing exploration. In
Proceedings of the fifteenth ACM conference on Economics and computation, pages 5–22, 2014.
ShaneFrederick, George Loewenstein, andTedO’donoghue. Timediscountingandtime preference:
A critical review. Journal of economic literature, 40(2):351–401, 2002.
Guillermo Gallego, Anran Li, Van-Anh Truong, and Xinshang Wang. Approximation algorithms
for product framing and pricing. Operations Research, 68(1):134–160, 2020.
Sreenivas Gollapudi, Guru Guruganesh, Kostas Kollias, Pasin Manurangsi, Renato Leme, and Jon
Schneider. Contextual recommendations and low-regret cutting-plane algorithms. Advances in
Neural Information Processing Systems, 34:22498–22508, 2021.
Negin Golrezaei, Vahideh Manshadi, Jon Schneider, and Shreyas Sekar. Learning productrankings
robusttofakeusers.InProceedings ofthe22ndACMConference onEconomicsandComputation,
pages 560–561, 2021.
Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends in Opti-
mization, 2(3-4):157–325, 2016.
Katja Hofmann, Shimon Whiteson, and Maarten De Rijke. A probabilistic method for inferring
preferences from clicks. In Proceedings of the 20th ACM international conference on Information
and knowledge management, pages 249–258, 2011.
Nicole Immorlica, Jieming Mao, Aleksandrs Slivkins, and Zhiwei Steven Wu. Bayesian exploration
with heterogeneous agents. In The world wide web conference, pages 751–761, 2019.
Nicole Immorlica, Jieming Mao, Aleksandrs Slivkins, and Zhiwei Steven Wu. Incentivizing explo-
ration with selective data disclosure. In Proceedings of the 21st ACM Conference on Economics
and Computation, pages 647–648, 2020.
Pooria Joulani, Andras Gyorgy, and Csaba Szepesv´ari. Online learning under delayed feedback. In
International Conference on Machine Learning, pages 1453–1461. PMLR, 2013.
Sham M Kakade, Adam Tauman Kalai, and Katrina Ligett. Playing games with approximation
algorithms. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing,
pages 546–555, 2007.
Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of
Computer and System Sciences, 71(3):291–307, 2005.
23Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, and Zheng Wen. Dcm bandits: Learning to
rank with multiple clicks. In International Conference on Machine Learning, pages 1215–1224.
PMLR, 2016.
IlanKremer,Yishay Mansour,andMotty Perry. Implementingthe“wisdomofthecrowd”. Journal
of Political Economy, 122(5):988–1012, 2014.
Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning
to rank in the cascade model. In International conference on machine learning, pages 767–776.
PMLR, 2015.
PaulLagr´ee, ClaireVernade,andOlivierCappe. Multiple-playbanditsintheposition-basedmodel.
Advances in Neural Information Processing Systems, 29, 2016.
TzeLeungLai, HerbertRobbins,et al. Asymptotically efficient adaptive allocation rules. Advances
in applied mathematics, 6(1):4–22, 1985.
Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020.
Tor Lattimore, Branislav Kveton, Shuai Li, and Csaba Szepesvari. Toprank: A practical algorithm
for online stochastic ranking. Advances in Neural Information Processing Systems, 31, 2018.
Renato Paes Leme and Jon Schneider. Contextual search via intrinsic volumes. In 2018 IEEE 59th
Annual Symposium on Foundations of Computer Science (FOCS), pages 268–282. IEEE, 2018.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web, pages 661–670, 2010.
Shuai Li and Shengyu Zhang. Online clustering of contextual cascading bandits. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 32, 2018.
Weiwen Liu, Shuai Li, and Shengyu Zhang. Contextual dependent click bandit algorithm for web
recommendation. In Computing and Combinatorics: 24th International Conference, COCOON
2018, Qing Dao, China, July 2-4, 2018, Proceedings 24, pages 39–50. Springer, 2018.
Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. Bayesian explo-
ration: Incentivizing exploration in bayesian games. In Proceedings of the 2016 ACM Conference
on Economics and Computation, pages 661–661, 2016.
Yishay Mansour, Aleksandrs Slivkins, and Vasilis Syrgkanis. Bayesian incentive-compatible bandit
exploration. Operations Research, 68(4):1132–1161, 2020.
Rad Niazadeh, Negin Golrezaei, Joshua Wang, Fransisca Susan, and Ashwinkumar Badanidiyuru.
Online learning via offline greedy algorithms: Applications in market design and optimization.
Management Science, 2022.
Ted O’Donoghue and Matthew Rabin. Doing it now or later. American economic review, 89(1):
103–124, 1999.
Yiangos Papanastasiou, Kostas Bimpikis, and Nicos Savva. Crowdsourcing exploration. Manage-
ment Science, 64(4):1727–1746, 2018.
24Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with multi-
armed bandits. In Proceedings of the 25th international conference on Machine learning, pages
784–791, 2008.
Mark Sellke and Aleksandrs Slivkins. The price of incentivizing exploration: A characterization
via thompson sampling and sample complexity. In Proceedings of the 22nd ACM Conference on
Economics and Computation, pages 795–796, 2021.
Aleksandrs Slivkins, Filip Radlinski, and Sreenivas Gollapudi. Ranked bandits in metric spaces:
learning diverse rankings over large document collections. The Journal of Machine Learning
Research, 14(1):399–436, 2013.
AleksandrsSlivkinsetal. Introductiontomulti-armedbandits. Foundations and Trends inMachine
Learning, 12(1-2):1–286, 2019.
Mika Sumida, Guillermo Gallego, Paat Rusmevichientong, Huseyin Topaloglu, and James Davis.
Revenue-utility tradeoff in assortment optimization under the multinomial logit model with to-
tally unimodular constraints. Management Science, 67(5):2845–2869, 2021.
Siwei Wang and Longbo Huang. Multi-armed bandits with compensation. Advances in Neural
Information Processing Systems, 31, 2018.
Masrour Zoghi, Tomas Tunys, Mohammad Ghavamzadeh, Branislav Kveton, Csaba Szepesvari,
and Zheng Wen. Online learning to rank in stochastic click models. In International conference
on machine learning, pages 4199–4208. PMLR, 2017.
Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, and Branislav Kveton. Cascading
bandits for large-scale recommendation problems. arXiv preprint arXiv:1603.05359, 2016.
A Technical Facts and Lemmas
We first state this standard fact about the KL divergence of two Gaussian random variables.
Fact A.1. The KL divergence between two Gaussians KL( (µ,1), (µ′,1)) = (µ µ′)2.
N N −
We now state a classical inequality that is used in our lower bound construction.
Theorem A.1 (Bretagnolle-Huber inequality). Let P and Q be probability measures on the same
measurable space (Ω, ), and let A be an arbitrary event. Then,
F ∈ F
1
P(A)+Q(Ac) exp( KL(P Q)),
≥ 2 − ||
where Ac = Ω A is the complement of event A.
\
Theorem A.2 (Hoeffding’s inequality). Let X ,...,X be n i.i.d. σ-sub-Gaussian random vari-
1 n
ables with mean µ. Then for any t > 0, we have that
n
Pr X µ > t
2e−t2/(nσ2)
i
− ≤
(cid:12)i=1 (cid:12) !
(cid:12)X (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
25B Proof of Theorem 3.5
Theproofofourlowerboundusesideasfromlowerboundconstructionfortheclassicalmulti-armed
bandit problem (see Chapter 16 in Lattimore and Szepesv´ari [2020]).
We will utilize the following divergence lemma. in order to prove our bound.
Lemma B.1. Given time-horizon T, consider two instances of the problem that share item utilities
u but have different payoff distributions ν = (P , P ) and ν′ = (P′, ,P′). Fix an
{ i }i∈[n] 1 ··· n 1 ··· n
algorithm that knows the sequence of attention window realizations ahead of time. Let P = P
ν νA
A
and P ν′ = P ν′A be probability measures on realized sample paths of under ν, and under ν′,
A A
respectively.
E ν[N i(T)] ·KL(P
i
||
P i′) = KL(P
ν
||
P ν′),
i∈[n]
X
where N (T) denotes the number of samples of item i in T trials and KL is the Kullback-Leibler
i
divergence between two probability measures.
The proof of this lemma follows directly from Lemma 15.1 in Lattimore and Szepesv´ari [2020].
We are now ready to prove our lower bound.
Proof of Theorem 3.5. Given a sufficiently large time-horizon T that is divisible by n, let the at-
tention window realizations be defined as
T T T
wt := i for i [n] and t (i 1) +1,(i 1) +2,...,i .
∈ ∈ { − · n − · n · n}
Note that these attention window realizations are deterministic and are known by the algorithm
ahead of time. This can only improve the regret of the algorithm as it can simply choose to
ignore this information. Also, note that these attention window realizations are the same across
all instances and do not provide any information about the structure of the optimal permutation.
Hence, we will drop the dependence on wt’s in the remainder of the proof.
Let := ( u , (µ ,1) ) denote the given instance of the problem, and let ν =
k k∈[n] k k∈[n]
I { } {N }
( (µ ,1), , (µ ,1)) be the collection of payoff distributions. Recall that the family of optimal
1 n
N ··· N
permutations under denoted by Π∗( ) is (s ,D ,s ,D ,...,s ,D ). The proof of the lower
I I
1 s1 2 s2 k sk
bound contains S phases each corresponding one of the items in S. The i-th phase considers the
| |
time interval 1,...,T [T] where T is defined such that, if one plays some permutation from
i i
{ } ⊆
Π∗( ) then all the selected items belong to s , ,s upto time T . Specifically, since an item
1 i i
I { ··· }
from s , ,s if and only if w (D +1), we have that T = argmax t [T] : wt
{ 1 ··· i } ≤ i′∈[i] | s i′| i { ∈ ≤
(D +1) .
i′∈[i] | s i′| } P
We now consider phase i for some i [S ]. Fix an item j D s if i = S , otherwise
P ∈ | | ∈
si
∪{
i+1
} 6 | |
fix an item j D . Let T′ = T . We will modify the instance to create another instance ′
∈
si i
I I
which has the same set of utilities but different means (µ′, ,µ′ ). Let µ′ = µ for all k = j,
1 ··· n k k 6
and µ′ = µ + ∆ + ǫ for an arbitrary ǫ > 0 chosen such that µ′ = µ′ for any k = j. Let
j si sij j 6 k 6
ν′ = ( (µ′,1), , (µ′ ,1)). Firstly, using Fact A.1, we have that KL( (µ ,1) (µ′,1)) =
N 1 ··· N n N j ||N j
(∆ +ǫ)2. Moreover, it is easy to observe that KL( (µ ,1) (µ′ ,1)) = 0 for k = j. Then, using
sij N k ||N k 6
Lemma B.1 on ν and ν′, we have that KL(P
ν
||P ν′)
≤
E ν[N j(T′)] ·(∆
sij
+ǫ)2.
Using Theorem A.1, for any event A, we have that
1 1
P ν(A)+P ν(Ac)
≥
2exp( −KL(P
ν
||P ν′))
≥
2exp −E ν[N j(T′)] ·(∆
sij
+ǫ)2 . (3)
(cid:0) (cid:1)
26We will now choose an appropriate event A based on the number of times j is selected in each
instance.
Firstly, since j / s , ,s , it will not be selected by any permutation in Π∗( ) upto time
1 i
∈ { ··· } I
T′ by the definition of T′. Hence, if any other permutation π / Π∗( ) selects j upto time T′ then
∈ I
it will incur regret at least ∆ per selection. This is due to the fact that the expected payoff of
sij
Π∗( ) is at least µ for each time in [T′] while the expected payoff for π will be µ upon selecting
I
si j
j, and µ <µ .
j si
Observe that j / D for any i′ < i as otherwise its position in Π∗( ) would have been
∈ s i′ I
ahead of s . Now, consider the set of optimal permutations Π∗( ′) under ′. Since, µ′ > µ′ , j
i I I j si
acquires a higher position in Π∗( ′) than s , i.e. the position of j denoted by p will be at most
i j
I
(D +1). This implies that the attention window realizes to p exactly T/n times upto
i′∈[i−1] | s i′| j
time T′ as p (D +1). Now, using the fact that j is not dominated by any other item of
P j ≤ i′∈[i] | s i′|
mean higher than µ′ we get that Π∗( ′) will select j at least T/n times. Hence, if any permutation
P j I
π selects j lesser than T/n times when an attention window of size p realizes, then it will incur a
j
positive regret for selecting any other item with mean smaller than µ′. Also, note that π cannot
j
select any item with mean higher than µ′ when an attention window of size p realizes because
j j
Π∗( ′) could have done the same thing and strictly increase its payoff. Hence, if j is selected lesser
I
than T/n times by π then it incurs regret at least ∆ = min µ′ µ′ : i′ S such that µ′ < µ′
{ j − s i′ ∈ s i′ j}
for each such time. Note that ∆ > 0 by the definition of µ′.
j
We now define event A = N j(T′) > T/2n . Let R = E ν[R T′( )] and R′ = E ν′[R T′( )]. We
{ } A A
have
T
R+R′
≥ 2n
·(P ν(A)∆
sij
+P ν′(Ac)∆)
T
min ∆,∆ (P (A)+P (Ac))
≥ 2n · {
sij
}·
ν ν
T
min ∆,∆ exp E [N (T′)] (∆ +ǫ)2 ,
≥ 4n · {
sij
}· −
ν j
·
sij
(cid:0) (cid:1)
where the first inequality above follows from the fact selecting j for more than T/2n times in
I
incurs regret at least ∆ and selecting j for less than T/2n times incurs regret ∆, and the last
sij
inequality follows from Equation 3.
Rearranging and taking limit gives us
E ν[N j(T′)] 1 log Tm 4(i Rn{ +∆ Rs ′i )j n,∆}
lim lim
T→∞ logT ≥ (∆ sij +ǫ)2 T→∞ (cid:16) logT (cid:17)
1 log(R+R′) 1
= 1 lim = Ω ,
(∆
sij
+ǫ)2 ·
(cid:18)
−T→∞ logT
(cid:19)
∆2
sij!
where the first equality follows due to the fact that min ∆ ,∆ and n do not depend on T, and
{
sij
}
the last equality follows by the consistency of and the fact that ǫ can be made arbitrarily small.
A
This implies
∆ E [N (T′)] 1
lim
sij
·
νA j
= Ω ,
T→∞ logT ∆2 sij!
As noted previously, the regret contributed due to j being selected anytime in [T′] is at least ∆ .
sji
27Hence, we have that
E [R ( )] |S| ∆ E [N (T )] |S|−1 ∆ E [N (T )]
lim
ν T
A lim
sij
·
ν j i
+
sisi+1
·
ν si+1 i
T→∞ logT ≥ T→∞ logT logT
Xi=1j X∈Dsi Xi=1
|S|−1 |S|
1 1
= Ω + .
 ∆ ∆ 
Xi=1
sisi+1
Xi=1j X∈Dsi
sij
 
This proves the statement of our lower bound.
C Extensions Based on Practical Considerations
C.1 Unknown Utilities
Inthissection, weshalldiscusssimpleapproachesonemightadoptinpracticalscenariostoestimate
userpreferencesintheeventthattheyareunknowntotheplatformaheadoftime. Theseapproaches
can be combined with our more technical algorithms presented in Sections 3 and 4 as a two-phase
estimate-then-optimize overall algorithm, where there is a small burn-in period where we only aim
to learn the users preferences, following which we minimize regret once these preferences have been
learned with sufficient precision. A key assumption here is that the users’ preferences are time
invariant6, and that their attention windows are stochastic and lazy, i.e. shorter attention window
lengths are morefrequentthan longer ones (see Section 4.1). Supposingtheusers aredeterministic
intheselection behavior,i.e. theygreedilyselectthehighestutilityitemfromwithintheirattention
window, then the estimation problem becomes quite straightforward – one can simply simulate any
sortingalgorithmbyplacingthepairofitemswhoserelativeorderofpreferencesistobedetermined
in the top 2 positions in the permutation, and then playing that same fixed permutation until one
of these items is selected. This willnecessarily occur when an attention window of length 2realizes,
the probability of which is Ω(1/n) due to the laziness assumption. Therefore, the answer to any
pairwise comparison queried by the underlying sorting algorithm can be obtained in O(nlogT)
trials with polynomially large probability, and since most query-efficient sorting algorithms require
just O(nlogn) queries, this simulation can be performed in just O(n2lognlogT) trials with high
probability.
A more interesting selection behavior is the following, which captures the essence of “social-
learning”wheretheusersthemselves areinitially unawareof theirtruepreferences, butarelearning
throughthe selections of their peers. Specifically, let usassumethat thereare someunderlyingtrue
item utilities that are only approximately known to both the users as well as the platform through
confidence intervals, i.e. for each item, both the users and the platform know a range that contains
its true utility. The width of this range is assumed to be inversely proportional to some monotone
functionofthetotalnumberofselections ofthisitemthusfar. Anaturalchoiceofonesuchfunction
would be the square root, owing to concentration of subgaussians. A practical motivation behind
this modeling choice might be one where the users perceptions of the utilities themselves are being
shaped by the selection patterns of their peers; each selection results in a new unbiased estimate
of the underlying true utility which is visible as a public review or score, with the total number
of selections providing newly arriving users with “confidence” of its true utility in the form of an
interval of shrinking width around the mean of these utility estimates. When a newly arrived user
6moregenerally,changingveryslowlywithtimeaswecanre-runourestimationsubroutinetoupdatethepreference
from time to time.
28is presented with these approximate utilities, he fixes for each item a perceived utility, which is
a value chosen arbitrarily from the confidence interval of its true utility. Upon being presented a
permutation,hethenselectstheitemwithhighestperceivedutilitywithinhisattentionwindow,and
fortheselecteditem,addshis(unbiased)estimateofthetrueutilityoftheselecteditemtothepublic
review. Underthismodelingassumption, thefollowing approach can beusedto efficiently learn the
underlying true item utilities. More precisely, since all our algorithms only require knowledge of
the ordering of item utilities, assuming the aforementioned model for selection behavior, it suffices
to reach a state where the confidence intervals of item utilities are disjoint. In order to satisfy
this condition, simply positioning the least-selected item whose utility interval overlaps with at
least one other utility interval at the top of the displayed permutation suffices. Supposing the
minimum separation in utilities is ∆u := min u u , then it is straightforward to see
min i,j∈[n] | i − j |
that this approach would separate all utility intervals within O((∆u )−2n2logT) trials: due to
min
the lazy assumption, a window of length 1 realizes every Ω(nlogT) trials with polynomially large
probability, and when the number of selections of all items exceeds (∆u )−2, all utility intervals
min
are necessarily disjoint.
C.2 Delayed Feedback
In this section we provide an extension of our results for a setting where the payoff feedback is
delayed. This setting is motivated by applications where there might be delays in observing the
longer-term payoff of current actions. We consider a simple setting of delayed feedback where the
payoff for roundt is observed after a delay of τ , i.e., thepayoff for chosen item at timet is observed
t
at time t+τ . Note that the item selected by the user is revealed instantaneously to the algorithm,
t
but the payoff for the selected item might be available after a delay. We assume that τ is a integer
t
random variable that is bounded between 0 and τ .
max
[Joulani et al., 2013] gave a black-box reduction for classical multi-armed bandits that converts
any algorithm for the non-delayed setting to an algorithm for the delayed setting. In particular,
Joulani et al. [2013] showed that delayed feedback increased the regret in a multiplicative way for
adversarial bandit problems while in an additive way for stochastic bandit problems. Here, we
follow the black-box reduction approach from [Joulani et al., 2013] for handling delayed feedback.
We firstconsider thestochastic payoffs case from Section 3. We usethe QPM-D algorithm from
Joulani et al.[2013]whichisametaalgorithmthatsimulatesabasealgorithminadelayedfeedback
environment. Thisalgorithm maintains a FIFO queueQ for each arm i [K]. Itstores thepayoffs
i
∈
received for item i from previous rounds in the queue Q as soon as they become available. If the
i
base algorithm wants to play arm i and a payoff is available in the queue Q then it will provide
i
the base algorithm with this payoff. Otherwise it will put the base algorithm on hold, and keep
playing arm i until a payoff becomes available. Using the same black-box reduction given in QPM-
D gives us the following regret guarantee for our stochastic setting with delayed feedback assuming
that the attention window are stochastically generated under the lazy assumption in each round:
E[Regret(T)] E[RegretBASE(T)]+n τ where BASE is any algorithm for the stochastic
max
≤ ·
setting for our problem. One can also get a dependence on the expected delay in this setting with
some additional work.
We now move to the adversarial payoffs case from Section 4. We use the BOLD algorithm
from Joulani et al. [2013] which is also a meta algorithm that simulates a base adversarial bandit
algorithm in a delayed feedback environment. This algorithm runs multiple instances of the base
algorithm instead of a single instance. It spawns a new instance of the base algorithm and plays
an arm according to this instance if all the existing instances are waiting for feedback. Once the
feedback is available for an existing instance, it becomes active again and an arm can be played
29according to this instance. The algorithm bounds the number of spawned instances in terms
of the parameter of the delay distribution. Using the same black-box reduction given in BOLD
gives us the following regret guarantee for our adversarial payoffs setting with delayed feedback:
E[Regret(T)] (τ +1) E[RegretBASE(T/(τ +1))] where BASE is any algorithm for the
max max
≤ ·
adversarial payoffs setting from Section 4. Once again we can get a dependence on the expected
delay with some additional work similar to Joulani et al. [2013].
30