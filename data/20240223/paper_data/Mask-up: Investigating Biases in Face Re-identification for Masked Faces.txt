1
Mask-up: Investigating Biases in Face
Re-identification for Masked Faces
Siddharth D Jaiswal, Ankit Kr. Verma, Animesh Mukherjee
Abstract—AIbasedFaceRecognitionSystems(FRSs)arenow Definition 1: This involves detecting a target face, and
widely distributed and deployed as MLaaS solutions all over comparing it against a database (of size ≥ 1) to iden-
the world, moreso since the COVID-19 pandemic for tasks
tify or authenticate individuals. This is also known as face
ranging from validating individuals’ faces while buying SIM
similarity/comparison/matching in literature. 1-to-1 face re-
cards to surveillance of citizens. Extensive biases have been
reported against marginalized groups in these systems and have identification involves comparing a source image against a
ledtohighlydiscriminatoryoutcomes.Thepost-pandemicworld database that has only one image. This has been used by
has normalized wearing face masks but FRSs have not kept Uber since 2017 [4] for verifying driver identity. On the
up with the changing times. As a result, these systems are
other hand, 1-to-N face re-identification involves comparing a
susceptible to mask based face occlusion. In this study, we audit
source image against a database of size N (different for each
four commercial and nine open-source FRSs for the task of
face re-identification between different varieties of masked and FRS), thus being a more complex task.
unmasked images across five benchmark datasets (total 14,722 Use-cases for re-identification range from automation of
images). These simulate a realistic validation/surveillance task access to buildings [5], at vaccination centres [6], for voting
as deployed in all major countries around the world. Three of
booths [7], airports [8] to identifying persons of interest [9]
the commercial and five of the open-source FRSs are highly
and verifying the identity of convicted criminals [10]. Some
inaccurate; they further perpetuate biases against non-White
individuals, with the lowest accuracy being 0%. A survey for oftheseapplicationsarebenign,whileotherscanbedescribed
the same task with 85 human participants also results in a as morally questionable. FRSs are distributed by commercial
low accuracy of 40%. Thus a human-in-the-loop moderation private companies [11]–[14], government agencies [15] and
in the pipeline does not alleviate the concerns, as has been
open-source projects [16]–[18].
frequentlyhypothesizedinliterature.Ourlarge-scalestudyshows
Masked face recognition and the biases thereof: Multiple that developers, lawmakers and users of such services need to
rethinkthedesignprinciplesbehindFRSs,especiallyforthetask studies have proven the existence of biases in FRSs against
of face re-identification, taking cognizance of observed biases.1 individuals of certain ethnicities/genders [19]–[30]. In fact,
biases in face detection and downstream applications have
Index Terms—Face Recognition System, Bias, Fair classifica-
been observed to get exacerbated under adversarial condi-
tion
tions [31], [32], thus indicating the vulnerability of FRSs to
realistic perturbations in the input. Face re-identification, by
I. INTRODUCTION designattachesanidentitytoindividuals,butinrealityreduces
them to statistical metrics. While performances are evaluated
MultipleAIapplicationshavehadanuptickindevelopment in numbers, inaccuracies impact real lives. Individuals have
and deployment since the COVID-19 pandemic induced lock- previouslybeenwrongfullyarrested[33],[34],blockedoutof
down [1]. Automated face recognition is one such application Uber [4] and denied SIM cards [35] due to faulty FRSs.
that has observed increased deployment, partly due to democ- The COVID-19 pandemic normalized wearing of face
ratizing costs of compute, storage and availability of com- masks to prevent spread of infections [36]. As face masks,
mercial SaaS platforms like Amazon AWS [2] and Microsoft available in different types and colours, are a genuine source
Azure[3]and,partlyduetotheneedformultipleapplications of occlusion for FRSs they constitute a natural adversarial
like gender/age prediction and face re-identification. setting, thus hampering the accuracy of tasks like detection
Face recognition systems (FRS): Face Recognition Systems and re-identification. In fact, past literature [37] has shown
(FRSs) are designed to recognize faces of individuals in an thataccuracyofcamera-basedfacere-identificationmethodsis
image or live video feeds and then predict various features substantially hampered by face occlusion. Concerns have also
like gender, age, etc. or match against existing images in been reported about individuals acting in bad faith using face
somedatabase.Inthiswork,wefocusonfacere-identification, maskstoconcealtheirtrueidentitywhichcanbeaseriouscon-
defined as follows. cern for identity validation and security tasks [38]. To counter
this, organizations are improving their technology [15], [39],
S.D. Jaiswal and A. Mukherjee are with Indian Institute [40] but such attempts have been relatively few.
of Technology Kharagpur, Kharagpur 721302, India (e-mail:
In thisstudy, we audit FRSs forthe task ofre-identification
siddsjaiswal@kgpian.iitkgp.ac.in;animeshm@cse.iitkgp.ac.in).
A.K.VermaiswithStraummanGroup,Bangalore560047,India.(e-mail: in an occlusion setting based on face masks. While re-
ankit.verma5859@gmail.com) identificationitselfmaybeamorallyquestionabletask,biases
1This work has been submitted to the IEEE for possible publication.
in these platforms lead to unjust outcomes, specially against
Copyright may be transferred without notice, after which this version may
nolongerbeaccessible. disadvatanged, marginalized and vulnerable individuals who
4202
beF
12
]VC.sc[
1v17731.2042:viXra2
maylacksocietalsupporttoseekredressal,legalorotherwise. II. RELATEDWORK
Suchinadvertentconsequences,asweshallshowinthispaper,
We now give a brief overview of the prior literature ex-
are only aggravated in the presence of facial occlusion which
plaining how FRSs can significantly impact discriminated and
could potentially lead to accusing an innocent, acquitting a
vulnerablepopulations,howthesesystemshavealonghistory
perpetrator or denying services and opportunities.
of propagating biases and how audit studies, with adversarial
Human-assisted face recognition: Automated face detection inputs can be used to ‘stress-test’ these platforms.
and re-identification have almost reached, and in some cases, BiasesandauditsofFRSs:FRSsarenowubiquitousinterms
surpassedhumanlevelaccuracyonbenchmarktasks[41],[42] of both frequency of deployment and use cases. While there
but existing biases continue to propagate. This indicates a arevariousargumentsabouttheethicalimplicationsofdeploy-
lack of oversight on part of the model designers and dataset ment,amorepressingconcerniswithregardstobiasedperfor-
developers. Researchers and industry practitioners have used mance against certain racial and gender groups in the existing
human-assistedorhuman-in-the-loopfacerecognitionsystems FRSservices.Variousmediaarticleshaveextensivelycovered
to aid with standard detection and recognition tasks [43]– biases in face recognition ranging from tagging images of
[46] which have improved accuracy. While the intentions are Black individuals as primates [49], [50], to misclassifying US
honorable, scalability presents a challenge. An FRS software Congressmen as convicts [51]. Even open-source models are
can process 100-1000s of images in a matter of few minutes inclined to similar biases, although mitigating them is easier
but humans cannot operate at the same speed. On the other through re-training and other techniques, but the same is not
hand, some researchers [47] have proposed caution while trueforcommercialFRSsoftwarewhichareeffectivelyblack-
using humans-in-the-loop and called for more experimental boxmodelsaccessibleonlythroughAPIs,withnoinformation
studies before using these solutions in the wild as systematic about the architecture, weights, etc. made public. Thus, any
investigations have shown how humans may make biased evaluation of the platform/model performance can only be
decisions [48]. Thus, it is important to compare human per- donethroughexternalthirdpartyaudits[52].Aseminalstudy
formance along the dimensions of accuracy and bias before byBuolamwiniandGebru[19]firstauditedcommercialFRSs
employing them as part of the face recognition pipeline. for biases against intersectional groups. This was followed by
Research questions handled in this paper: In this work, we various academic studies [20], [22], [23], [31], [53]–[70] that
study the potential biases in face re-identification by auditing have audited existing commercial and open source FRSs for
four commercial and nine open-source FRSs on publicly biases. In this work, we study biases against intersectional
availablebenchmarkfacedatasetsthathavebeenadversarially groups, defined as follows.
mutated with face masks. We also look at the activation maps Definition 2: For the purpose of this paper, similar to [19],
fortheopen-sourceFRSs’tobetterunderstandthepredictions. [71], we define an intersectional group ig as a set
p1...pn
To complement our audit of AI based FRSs, we perform comprising the intersection of all the members of the groups
a survey amongst a group of human volunteers for the task g p1...pn where p 1...p n are marginal protected attributes like
of face re-identification. We evaluate them for both speed race, gender, etc. Formally,
andaccuracyandcomparetheperformanceagainstautomated
ig =g ∩g ∩···∩g
FRSs. We now state our research questions.
p1×p2×...pn p1 p2 pn
RQ1. Are FRS softwares robust to face mask occlusions for Thus, if g p1=race = {Black, White} and g p2=gender =
thetaskofre-identification?Aretheredifferencesinaccuracies {man, woman} then ig = {Black men, Black
againstspecificgenderorracialgroups?Throughthisquestion, women, White men, White women}. This definition is
we seek to understand whether these commercial and open- different from the standard sociological definition of
source FRSs introduce or exacerbate existing societal biases intersectionality2. Henceforth, we will use Definition 2,
for the task of face re-identification. unless otherwise stated.
With a growing focus on biased AI software and its im-
RQ2. Is FRS re-identification accuracy impacted by the type
plications on society, many private corporations have either
and color of the mask worn by an individual? Through this
stopped offering their FRS services [72], or are introducing
question we seek to understand if the shape, size and color of
new responsible AI measures [73] that limits the applications
the face mask occlusion has an impact on the accuracy and if
forwhichFRSscanbedeployed.Thisisbeingdonetoreduce
so,doesitincreaseordecreaseforcertaintypesofmasks.This
misuse of these services and contextualize their deployment
willgiveusaninsightintotherobustnessoftheseFRSmodels
to reduce and mitigate the biases therein. While this may
and may help in designing more robust and fairer models.
be helpful for various platforms and specific tasks like face
RQ3. Can the open-source FRS re-identification be explained
detection, the solutions cannot be generalized. Platforms like
through activation maps to better understand the models’
Amazon [11] & Uber [74] suggest that users should have
behaviour under the face mask based occlusion setting?
humans-in-the-loop to verify the outputs of face similarity,
RQ4. How do the human survey participants perform com-
on AWS Rekognition and Microsoft Azure Face respectively.
pared to the automated FRSs for the task of face re-
Non-expert human annotators are already known to be unre-
identification? Through this question we seek to compare
liable when it comes to identifying faces [75], [76] and thus
humansagainstAIforbothspeedandscalabilityand,thereby,
may introduce their self-selected biases in the pipeline [47],
judge whether a human-in-the-loop scheme is indeed a plau-
sible solution. 2https://www.oed.com/view/Entry/4298433
[48], thereby, causing more harm than good. This highlights oflowqualityand128×128pxresolutionwithsignificant
the fragile social aspect of these systems, especially against variations in the angle, lighting and pose. An example is
discriminated and vulnerable members of society. Hence, AI present in Figure 1 (row 1).
based FRSs need to be subjected to temporal audits continu- • Chicago Face Database (CFD-USA) [90]: Thisdataset
ously through various exploratory means to ensure that biases has 597 images of USA citizens from four races– Asian
are identified and mitigated. (A), Black (B), White (W) and Latinx (L; Latino in
Adversarialauditofmaskedfaces:Adversarialauditsimply the original dataset). Each image is high quality and
auditing of FRSs by supplying them with adversarial, noisy standardized for angle, lighting etc, with a resolution of
inputs to evaluate the change in performance of these sys- 2444×1718px. An example is present in Figure 1 (row
tems. There has been considerable research toward designing 2). This dataset has two other variants –
adversarial inputs that are capable of deceiving FRSs [77]– – CFD-MR [91]: This dataset has 88 unique images
[85]. Similar to these, adding occlusions like watermarks [86] (M:26,F:62)ofindividualswhoseparentsbelongto
or facemasks [87],[88] canalso beused to createadversarial different racial groups. The images are standardized
inputs that can then be used to either deceive FRSs (as in the similar to CFD-USA.
caseof[86])orimprovetheaccuracyofthesesystems[88].In – CFD-IND[92]:Thisdatasethas142uniqueimages
this paper, we use the software package MASKTHEFACE [88] (M: 90, F: 52) of individuals from various states in
to generate masked inputs from our initial datasets that are India. As with CFD-MR, the images are standard-
then passed to the FRSs as input. The inputs cannot be con- ized.
sidered out-of-distribution as face masks are now a common
All images in the CFD group are annotated for all
occurrence in the post-pandemic world. features.3. The gender and race labels are self-identified
Relation to existing work: Our work fits within the purview
by the individuals whose photos are part of the dataset
of adversarial audits, similar to Jaiswal et al. [31]. While the
for all Chicago Face Database images [90]–[92].
authors in [31] audit commercial FRSs for the task of face • FAIRFACE [93]:Wesample2100imagesfromthelarger
detectionusingvariousrealisticfilters,ourauditisdoneforthe
dataset of 108,501 images belonging to the YFCC-100M
taskoffacere-identificationofmaskedfaces,amorenaturally
Flickr dataset. This is the most diverse dataset that we
occurring occlusion.
use for our study with individuals belonging to seven
Compared to existing literature like Damer et al. [89], we racialgroups–White(W),Black(B),Latinx(L),Indian(I),
primarilydifferinthefollowingways–(i)Auditofalargerset Southeast Asian(Se), East Asian(Ea) and Middle East-
of both commercial and open-source FRSs, (ii) Our setup is ern(Mi). We sample equal number of images from each
morecomplex,realisticandtheresultsaremoregeneralizable. gender-race combination. Similar to CELEBSET, the im-
Oursurveyparticipantpoolismorediverseand7×larger,(iii) ages are of generally low quality and are highly diverse
We explicitly study intersectional biases in these platforms to intermsoflighting,viewingangleandpose.Eachimage
understand how their discriminatory performance may impact has a resolution of 224 × 224px and is annotated for
vulnerable populations and finally, (iv) The central finding in gender, age and race. An example image is present in
ourpaperisthatnotallsoftwaresperformequallywellandthe Figure 1 (row 3).
performance may vary depending on the input conditions like
Adversarially masking the input: We use a highly popular
the image quality, mask type and color, face angle, etc. Thus
open-source masking software package – MaskTheFace [88]
any organization that plans to deploy such web-API based
to apply face masks on the datasets to create our adversarial
FRS services (whether commercial or open-source) needs
inputs. This tool has been used previously to improve accu-
to conduct large-scale extensive audits for various expected
racy [94], [95] and fairness [96] of FRSs. In this study, we
input distributions (masked and unmasked faces), to deliver
choose three of the most common types of masks worn by
a consistent, equitable and just experience to all individuals,
civilians and healthcare workers [97] during the spread of
specially those belonging to discriminated and vulnerable
COVID-19 pandemic – surgical, N95 and cloth. We choose
groups.Ourstudycanserveasablueprintforsuchlarge-scale
two extra colors for the cloth masks based on the Monk
adversarial audits.
Skintone scale [98], proposed by Google– MONK 02 (light
skintone) and MONK 07 (dark skintone). These skintones
III. DATASETS represent an average light and dark skin tone thus making the
occlusionincrediblycomplexfortheFRSaseachoftheseget
Inthisstudy,weauditfivebenchmarkdatasets,allofwhich
blended with the corresponding skin colours. Thus the mouth
have diversity in terms of race, angle, lighting and picture
areaisoccludedwithoutmodifyingthefaceitself.Weperform
quality, but only two ground truth binary gender labels– male
& female. We note that gender is a spectrum and not binary
this step for an already low quality dataset – CELEBSET.
Examples of the outputs of the masking software are present
as presented in the datasets here, but due to the lack of other
in Figure 1 – surgical masks (col 2), N95 masks (col 3, row
labelsinthedatasetannotations,weareunabletoevaluatethe
2 & 3) and cloth masks (row 1, col 3 and, col 4).
biases for these.
The masking software is not flawless and thus fails to
• CELEBSET [20]: This dataset has 1600 images of 80 identify ≈4% faces in CELEBSET and ≈28% in FAIRFACE.
Hollywoodcelebrities(20peridentity)andisbalancedin
termsofgenderandrace–Black&White.Theimagesare 3https://chicagofaces.org/default/4
a threshold of 0.5 (less means successful re-identification).
Fortheopen-sourcemodels,werefertothedefaultthresholds
given in the DeepFace repository 6.
Audit on FRS platforms: We audit the platforms described
above for the tasks of 1-to-1 and 1-to-N face re-identification
on the five benchmark datasets and their adversarial masked
variants.
• 1-to-1 face re-identification: In this task, we compare
an image I with its masked variant M(I) where M() is
the output of the masking operation on any image. This
experiment is intended to verify the accuracy of FRSs
under the best case scenario where the only adversarial
component is the face mask that acts as an occlusion.
• 1-to-Nfacere-identification:Inthistask,weincreasethe
Fig. 1: Images from CELEBSET (row 1), CFD-USA (row 2) and complexity of the audit. We audit using the CELEBSET
FAIRFACE (row 3) datasets in their original format (leftmost column) dataset by comparing a single masked image against
and with the surgical, N-95 and cloth (multiple colors) masks and, the
multiple (4) unmasked images. Effectively, the input is
MonkSkinTonescale(lastrow).
a single masked image M(I ) of a person i and the
i
Thusweexperimenton–[(1541+597+88+142+1512)×3+ database has four images – two different images, I i′, I i′′
(1541×2)]=14,722.Themultiplier×3referstothesurgical, of the same person i and two images, I j, I k of different
N95 and cloth masks for all datasets and the multipler ×2 individuals j and k, but belonging to the same gender
referstotheMonk02andMonk07masksfortheCELEBSET and race. We consider re-identification successful if the
dataset. FRSreturnsaconfidenceabovethethresholdsmentioned
above for one/both (as applicable7) of I′ and I′′. Hence,
i i
the adversarial component here is the mask as well as
IV. METHODOLOGY
the different photos in the database. The dataset contains
Here, we describe the platforms which are audited, the
imagesof80uniqueindividuals.Astherearetwoground
design of the survey, demography of the survey participants, truthimages–I′ andI′′ inthedatabasecorrespondingto
and the methodology of our experiments. i i
the same identity, there can be a maximum 160 correct
pairs identified by the FRSs.
A. Platforms investigated
In this work, we audit thirteen FRS platforms; four of
B. Survey with human volunteers
these are commercial – Face++ [12] (FPP), Amazon AWS
Rekognition [11] (AWS), Microsoft Azure Face [13] (MSFT) The following is mentioned in the documentation for AWS
and FaceX [99] (FCX) and the other nine are open-source Rekognition (commercial FRS)– “If you plan to use Com-
(implementation from DeepFace4) – VGG-Face [100] (VGG), pareFaces to make a decision that impacts an individual’s
FaceNet[101](FNET)anditsvariation–FaceNet-512(FNET- rights, privacy, or access to services, we recommend that you
512), OpenFace [18] (OPFC), DeepFace [42] (DPFC), Deep- passtheresulttoahumanforreviewandfurthervalidationbe-
ID[102](DP-ID),ArcFace[103](ARC),DLib[104](DLIB), fore taking action.”8, but research has shown that non-domain
and SFace [105] (SFC). The commercial models provide expertsarethemselvesunreliableatmatchingindividuals[44],
various face recognition services through economically priced [75],[76].Moreover,humanshavetheirownbiaseswhichmay
web-based APIs but do not disclose any model or training be reflected in their decisions and may cause more harm than
dataset information, whereas the weights, model architecture, good [47], [48].
etc. are available for the open-source models. Ours is the To determine whether humans are indeed unreliable, and
first study that performs an in-depth audit of FaceX5 for any inaccurate when performing the task of face re-identification
face recognition task. Our current audit work deals with the under adversarial scenarios, we conduct a survey where
task of face re-identification or matching. For Face++, AWS participants are required to perform the task of 1-to-N re-
Rekognition and Azure Face, we use a confidence of 80% identification on the CELEBSET dataset (the only dataset that
to consider the re-identification as positive by taking a cue has 20 images for the same identity). We then compare these
from real-world deployments by police forces [106]. While responses against those by the FRSs to observe how humans
we understand that the platform documentations, in particular fare against AI in terms of accuracy and scalability. Next, we
AWS Rekognition, state that the confidence should be over
99%,keepingthethresholdat80%allowsustostudytheper- 6https://github.com/serengil/deepface/blob/master/deepface/modules/verific
ation.py
formance in real-world deployment scenarios. FaceX returns
7In our experiments, Face++ returns a single image, both Azure/AWS
Euclidean distances instead of confidence values; we consider
mostly return both images, & the open-source models return a confidence
forallimageswherefacesaredetected.
4https://github.com/serengil/deepface 8https://docs.aws.amazon.com/rekognition/latest/dg/faces-
5ThewebsiteandservicesareofflinesinceMarch2023. comparefaces.html5
Demographic Distribution
Gender M:51|F:34
Age 18-25:18|26-31:43|32-45:16|45+:8
Professionalbackground AC:10|UG:16|PG:21|CC:22|SE:12|OT:4
TABLE I: Demographics of the respondents who participated in our
survey. The abbreviations for professional background correspond to
– AC: academic researcher, UG: undergraduate student, PG: post-
graduate/PhD student, CC: corporate, SE: self-employed, OT: other.
Majority of the participants identify as male, are 26-31 years old and
areworkinginthecorporatesector.
Fig. 2: Screenshot of the experiment with no deadline scenario on
our survey website. Each webpage has one set of images, wherein the The participants are shown only 20 sets of images during
participant must answer for all images using a 5-point Likert-like scale each phase of the experiment to prevent cognitive fatigue. We
ontheirlikenesstothemaskedimage.Theexperimentwithdeadline,in
collect various demographic information from the participants
addition, has a timer on the top of the page that counts down from 2
minutes. but remove all PII information before processing the results.
In this and in any future results, we will be presenting only
the aggregated demographic information without any PII.
describe our experiment design, and participant demographics
Participant demographics: We shared our survey website
& characteristics.
withparticipantswithinourinstitutionaswellasontheAma-
Design of the survey: Our survey is designed to run in two
zonMechanicalTurk(MTurk)platformandreceivedresponses
parts on our in-house website (screenshot shown in Figure 2).
from a total of 85 legitimate respondents. The participants
Part 1: experiment with no deadline: The first part, hence-
withintheinstitutionwereselectedthroughsnowballsampling
forth referred to as experiment with no deadline, shows the
and those on Amazon MTurk were selected through random
participant 20 sets of images. Each set contains the masked
sampling.InTableI,wenotetheparticipantdemographics.A
photo M(I ) of an individual i and three unmasked photos –
i majority of the participants self-identify themselves as male,
a different photo I′ of the same individual i, and two photos
i areintheagegroupof26-31,andworkinthecorporatesector.
I and I of two different individuals j and k respectively.
j k Out of the 85 participants, 62 are from Amazon MTurk and
While the 1-to-N experiments on the FRSs have two photos
23arefromourinstitution.Whiletheinstitutionalparticipants
of the masked individual, the set that is shown to any survey
took part in the survey free-of-cost, the Amazon MTurk
participant contains only one of the two photos. Also, the
participants were paid $0.50 for successfully completing the
participants are not informed that only one of the unmasked
survey. All participants were recommended to complete the
images belongs to the person in the masked image. This
survey in 10 minutes.
ensures that they treat each unmasked image equivalently as
We chose two different participant pools to understand the
a potential match. To ensure gender and race uniformity in
difference between experts (our institutional participants, a
the experiments, the participant is shown 5 sets of images
majority of whom have taken a graduate level course on
fromeachintersectionalgroup–Whitemales,Whitefemales,
AI Ethics and thus were aware of the responsibility and
Black males and Black females. Each set is shown on a new
seriousness of the task at hand) and normal paid volunteers
webpage and the unmasked images are shown in a random
who may have other incentives to participate in the task. This
order. The participant is then asked to respond for each
was also reflected in the results (and follows from previous
of the unmasked images, how similar it is to the masked
research [44]) that we observed, as described in the next
image using a 5-point Likert-like scale. The graduations are –
section.
‘differentperson’,‘somewhatsimilar’,‘similar’,‘verysimilar’
and ‘same person’. We collect the responses and the time a
participant spends on each set, to evaluate the average time
spent by a participant when there is no pressure of deadline. V. RESULTS&OBSERVATIONS
Part 2: experiment with a deadline: The second part of the
survey, with a deadline, imposes an overall time restriction of In this section we present the results and associated obser-
2 minutes, within which a participant is shown a maximum vations for the face re-identification audits on all the FRSs
of 20 sets of images. The goal of this task is same as the along with a simulation of the 1-to-N face re-identification
earlier case but under the pressure of a deadline. This is often withhumanparticipants.Wefirstpresenttheresultsforoverall
the case for time-critical applications like surveillance/entry- accuracy and disparities in accuracies between the different
exit to buildings, etc. Here as well, we collect the participant intersectional groups for the simpler task of 1-to-1 face re-
responses and the time taken on each set. The two-part identification. This is followed by the results for the more
experiment allows us to study how the two parameters – time realistic task of 1-to-N face re-identification. Next we discuss
and accuracy, relate to each other. Although the survey has our take-aways from the Grad-CAM activation maps on the
been performed independently of the FRS audits, our survey open-source FRSs. Finally, we present the results for the 1-
attempts to capture the following pipeline – the FRS provides to-N face re-identification simulation with human volunteers.
its predictions for the given input image and the volunteer Here, we define disparity for each dataset as follows – the
verifies these outputs by evaluating the same set of images. difference between the maximum and minimum accuracy
Thus, the humans here are thought to be acting in a post-hoc amongst all intersectional/gender groups combined with the
manner and are used to aid the FRS decision-making process. different mask types.6
DATASET MASK FPP FCX MSFT AWS VGG FNET FNET-512 OPFC DPFC DP-ID DLIB ARC SFC
SRGCL 51.46 6.54 87.15 99.42 94.48 52.63 84.88 5.13 29.46 0.06 98.83 73.52 97.08
N-95 58.86 9.6 88.19 99.61 94.35 52.04 85.46 6.49 14.6 0.45 97.99 61.39 94.29
CLBST CLT-B 48.35 14.34 88.97 99.22 96.43 55.09 91.82 4.22 63.6 1.3 98.57 73.78 96.5
CLT-M2 31.47 8.76 82.61 99.09 96.69 58.79 90.91 2.92 60.8 11.16 98.96 72.29 95.26
CLT-M7 47.24 17.78 85.53 98.96 96.95 56.46 93.38 2.86 74.37 33.29 98.57 74.69 94.94
SRGCL 41.21 18.43 99.16 100 89.95 62.98 83.75 2.18 3.35 2.01 87.44 78.73 82.91
CFD-USA N-95 49.41 9.88 99.5 100 84.42 58.63 79.06 3.69 3.69 2.18 75.04 67.34 73.2
CLT-B 56.95 31.32 99.5 100 91.79 68.84 86.77 2.51 24.12 9.05 89.78 81.74 87.27
SRGCL 59.19 12.9 83.53 97.95 97.49 70.04 88.96 0.99 15.48 0.4 93.72 76.19 98.68
FFACE N-95 58.66 9.66 82.8 98.41 97.16 67.59 86.44 2.31 4.1 0.79 94.64 32.54 98.48
CLT-B 58.8 17.66 86.64 97.82 98.21 71.16 91.34 1.79 15.81 1.06 94.18 65.21 97.82
SRGCL 47.73 28.41 100 100 95.45 60.23 90.91 4.55 5.68 4.55 89.77 80.68 90.91
CFD-MR N-95 53.41 13.64 100 100 90.91 60.23 85.23 6.82 6.82 4.55 82.95 73.86 84.09
CLT-B 61.36 42.05 100 100 97.73 70.45 96.59 4.55 21.59 13.64 94.32 85.23 94.32
SRGCL 58.45 36.62 99.3 100 94.37 47.89 91.55 3.52 3.52 3.52 90.14 83.8 89.44
CFD-IND N-95 53.52 14.79 100 100 83.1 43.66 81.69 7.04 4.23 4.23 71.83 61.97 71.83
CLT-B 82.39 40.85 98.59 100 95.07 48.59 97.18 4.93 10.56 4.23 95.77 88.73 95.77
TABLE II: Overallaccuracyfor1-to-1re-identificationonthedifferentFRSplatforms,datasetsandmasktypes(andcolors).Thephrasesinthe
secondcolumnare differentmaskedinputs– surgical(SRGCL),N-95,cloth-blue color(CLT-B),cloth-MONK02 color (CLT-M2)andcloth-MONK
07 color (CLT-M7). Each cell stores the percentage of images that are correctly re-identified by an FRS for a given mask type in a particular
dataset.AWSandVGG-Facearethebestperformingcommercialandopen-sourceFRSsrespectively.Maxvaluesforeachdataset-FRScombination
areinboldandminvaluesareunderlined.
A. RQ1 & RQ2: 1-to-1 face re-identification rows12-17),weseethatamongstthecommercialFRSs,AWS
Rekognition and Azure Face are the most robust FRSs on
We now discuss the results for 1-to-1 face re-identification CFD-MR (100% accuracy for both) and CFD-IND (100%
on the thirteen FRS platforms for each dataset. The results accuracy for AWS). This is similar to the performance on
for overall accuracy are shown in Table II. We also present CFD-USA,anotherdatasetwithhighqualityimages.Coming
the disparities in accuracies among the intersectional groups totheopen-sourceFRSs,similartotheperformanceon CFD-
(combinationofgenderandrace)andbetweenthetwogender USA, VGG-Face is the best performing FRS and OpenFace
groups (for CFD-MR and CFD-IND) in Table III. is the worst.
Overall accuracy: We test for all the mask types for the Observations: We report three major observations from Ta-
CELEBSET dataset as it is the most balanced one in terms bleII–(i)AllFRSsexhibitvariableperformancebasedonthe
of gender and race, thus giving us a better overview into the typeandcolorofthemask.(ii)Onaverage,AWSRekognition
performance. For other datasets – CFD (USA/IND/MR) and is the best performing FRS, followed by VGG-Face. FaceX
FAIRFACE, we experiment with only surgical, N-95 and the andOpenFacearetheworstperformingFRSs.(iii)Someofthe
cloth mask, all of which are blue in color. open-sourceFRSsdespitenotbeingtrainedwithmaskedfaces,
(i) CELEBSET: In Table II (rows 1-5) we present the results perform consistently well for the given task, thus indicating
for the CELEBSET dataset. We see that AWS Rekognition that the face embeddings generated by these models may not
and FaceX are the best and the worst performing commercial need the entire face information to perform re-identification.
FRSs respectively, independent of the mask type being used.
Amongst open-source FRSs, on average DLib and OpenFace Disparity: In Table III, we take note of the disparity be-
are the best and the worst performing FRSs. Two out of the tween the intersectional groups. We choose four of the FRSs
four commercial FRSs report their lowest accuracies for the (two commercial and two open source) for paucity of space;
cloth mask of MONK 02 color, whereas six out of the nine however, the results are representative and hold across all
open-source FRSs report their lowest accuracies for the blue the platforms under investigation. Even if the overall re-
N-95 mask. identification accuracy is high, there may be differences be-
(ii) CFD-USA: In Table II (rows 6-8), for CFD-USA we tween the different intersectional groups.
see a similar performance for the commercial FRSs wherein (i) CELEBSET: For the CELEBSET dataset, we see a huge
AWSRekognitionisthebestperformingFRS(100%accuracy disparityof60%againstBlackmalesbyFace++andmorethan
on all mask types) and FaceX is the worst performing one. 40%againstBlackfemalesbyArcFace.Infact,allcommercial
Amongst the open-source FRSs, we see that VGG-Face is FRSs are biased against Black individuals.
the best performing model. Interestingly, the cloth mask of (ii) CFD-USA: For CFD-USA we see that three of the
blue color is the least adversarial occlusion for all but one FRSs are biased against Black males with ArcFace reporting
(OpenFace) FRSs on this dataset. This shows that the color a disparity of 87.33%. We also see disparate performance
and type of the mask can have a significant impact on the against Black males, based on the type of mask in Face++
accuracy, across datasets. and ArcFace (cloth mask vs. N95).
(iii) FAIRFACE: For the FAIRFACE dataset (Table II, rows 9- (iii) FAIRFACE:Thedisparityresultsonthe FAIRFACE dataset
11), the results show that the performance of the commercial is very similar to those obtained for the CFD-USA dataset.
FRSs is similar to CELEBSET but the best performing open- In fact, biases are reported against all people of color – Black
source FRS is now SFace. Seven FRSs perform best for andIndian.Thus,studyingdisparityvaluesallowsustobetter
the blue cloth mask. Similar to CELEBSET, three of the understandbiasesagainstmarginalizedandvulnerablegroups.
commercial and five of the open-source FRSs report their (iv) CFD-MR & CFD-IND:InthelasttworowsofTableIII
lowest accuracies on the N-95 mask. we look at the disparity amongst the two gender groups for
(iii) CFD-MR & CFD-IND:Forthesetwodatasets(TableII, different masked inputs. AWS reports no disparity, whereas7
DATA FPP AWS VGG ARC
CLBST 59.73%(WM/CLT-B–BM/CLT-M2) 3.77%(WM/N95,CLT-M7–BF/CLT-M7) 12.66%(BM/CLT-M7-WF/SRGCL) 40.81%(WM/CLT-M7-BF/SRGCL)
CFD-USA 56.99%(WM/CLT-B–BM/CLT-B) 0% 49.46%(AM,LF/SRGCL,LF/N95,AM,AF,LM/CLT-B–BM/N95) 87.33%(LM/SRGCL-BM/N95)
FFACE 37.66%(MiM/N95,CLT-B–BM/CLT-B) 6.25%(IM/SeF(all)–BM/CLT-B) 8.41%(WF/CLT-B–IM/N95) 66.78%(MeM/SRGCL–BF/N95)
CFD-MR 46.65%(Male/CLT-B–Female/SRGCL) 0% 8.07%(Female/CLT-B–Female/N95) 14.52%(Female/CLT-B–Female/N95)
CFD-IND 31.58%(Female/CLT-B–Male/N95) 0% 21.11%(Female/SRGCL,CLT-B–Male/N95) 53.33%(Female/SRGCL,CLT-B–Male/N95)
TABLE III: Disparity in accuracy for 1-to-1 re-identification on four FRS platforms (highest and lowest disparity for both commercial and
open-sourceFRS).Mostplatformsreporthighbiasesagainstindividualsofcolor,withthetypeofmaskplayingarole.
MASK FPP MSFT AWS VGG FNET FNET-512 OPFC DPFC DP-ID DLIB ARC SFC
SRGCL 5 11.81 92.31 74.38 21.88 33.13 1.88 7.5 0 75.63 45 69.38
N-95 10 13.48 91.67 75.63 18.13 38.13 1.25 1.88 0 73.13 34.38 66.25
CLT-B 2.5 12.59 88.37 77.5 21.88 43.75 0 23.13 0 78.75 53.75 68.75
CLT-M2 0 11.27 88.1 75.63 23.75 43.13 0 27.5 0 78.13 50 60.63
CLT-M7 0 13.99 89.47 78.13 21.88 49.38 0 40 1.25 78.13 49.38 61.88
TABLE IV: Overallaccuracyfor1-to-Nre-identificationonthedifferentFRSplatformsandmasktypes(andcolors)fortheCELEBSETdataset.
AWSisthemostrobustcommercialandDLIBisthemostrobustopen-sourceFRS.Maxvaluesandminvaluesineverycol.areboldandunderlined.
FPP MSFT VGG SFC
15.79%(BF/N95–All/CLT-M2,CLT-M7) 19.44%(BM/N95–BF/SRGCL,CLT-M2) 56.9%(WM/CLT-M2–WF/SRGCL) 45%(WM/SRGCL–BM/CLT-M7)
TABLE V: DisparityinaccuraciesacrossthedifferentintersectionalgroupsandmaskcombinationsfortheCELEBSETdatasetonfourdifferent
FRSplatformsforthetaskof1-to-Nfacere-id.AzureandVGG-Facereportthehighestdisparitiesat19.44%and56.9%amongstcommercialand
open-sourceFRSsrespectively.
Face++ reports higher accuracy for males in CFD-MR and color, with both Azure Face and Face++ reporting very low
opposite in CFD-IND. The two open-source FRSs report a accuracy. Amongst open-source FRSs, DLib has a maximum
similar behaviour with the highest accuracy for females and accuracy of 78.75% for the cloth mask of blue color and a
the lowest is always for the N-95 masked input. minimum accuracy of 73.13% for the N-95 masks. We note
Observations: From the above results, we can see three pat- that all commercial FRSs report the lowest accuracies for the
ternsemerging–(a)ArcFacehasthehighestdisparityamongst cloth masks of MONK02 color, but the open-source FRSs do
all FRSs, that too against Black individuals wearing non- not have a consistent trend.
cloth masks, irrespective of the dataset under consideration. We also analyze the disparities in the accuracy among
This indicates the extreme sensitivity of ArcFace toward such intersectional groups in Table V. We report representative
inputs. A similar observation exists for Face++ but for cloth results for only four FRSs due to paucity of space. Amongst
masks. (b) AWS is the most robust FRS for masked inputs, commercial FRSs, Microsoft Azure has the highest disparity,
reporting no disparity for high-quality images of CFD-USA againstBlackfemales–≈20%whileamongsttheopen-source
dataset. (c) The open-source models are more sensitive to N- FRSs, VGG-Face exhibits the highest disparity of ≈ 57%.
95 face masks. Deployment of these FRSs must account not Hence this experiment confirms that the color and type of the
only for gender and race but also for the type of mask that a mask plays a huge role in how well an FRS performs for the
person may be wearing. re-identification task. Three of the FRSs report biases against
Black individuals, for different mask types.
Observations: We see that (a) AWS and DLib are the most
B. RQ1 & RQ2: 1-to-N face re-identification with FRSs
robust commercial and open-source FRSs respectively in the
Here, we discuss the results for the more general and generalized scenario, but they are also disparate against dif-
realisticproblemof1-to-Nfacere-identificationwhereasingle ferent mask types. Hence, the mask type is an important
masked face is compared against multiple unmasked images obfuscatingfactorforfacere-identification,andFRSproviders
from a database. These experiments are performed on the need to reevaluate their systems in light of our observations.
FRSs as well as with human volunteers to simulate a human- (b) For commercial FRSs, the highest and the lowest accu-
in-the-loop approach to the re-identification task. Note that racies are reported for Black individuals, thus deviating from
these results are only computed for the CELEBSET dataset previous observations where Black individuals rarely reported
since it contains multiple images per identity annotated with high accuracies, but the open-source FRSs report the highest
the person name, gender and race. Since FaceX was the accuracies for White males, as expected.
worst performing platform for the task of 1-to-1 face re-
identification, with a maximum accuracy of only 18% for
C. RQ3: Grad-CAM explanation for open-source models
the CELEBSET dataset, we do not audit it for the more
challenging task of 1-to-N face re-identification. In Figure 3 we show the the Grad-CAM activation maps of
Table IV presents the results for overall accuracy on 1-to-N the face embeddings generated by the VGG-Face open-source
face re-identification for the 12 FRSs. We see that only AWS FRS on the CELEBSET dataset, for the unmasked standard
Rekognition(commercial),VGG-FaceandDLib(open-source) images and the surgical and N-95 images. On the left side,
perform well for this task – with accuracies above 70%. we see the images for the correct re-identification and on the
Amongst commercial FRSs, AWS has a maximum accuracy right side, we see the images where re-identification fails. On
of 92.31% on the surgical masked inputs and a minimum each side a single triplet corresponds to first the re-identified
accuracy of 88% on the cloth masked input of MONK 02 image, second the masked input image to the FRS and third8
Human-FRSCorrelation
Scenario
Face++ AzureFace AWS VGG-Face DLib
Nodeadline -0.0091 0.0726 0.0563 0.0497 0.0639
Deadline 0.0538 0.1044 0.0860 0.0422 0.0534
TABLE VI: Pearson correlation between human survey participants
and the individual FRSs for the No Deadline and Deadline scenario.
Maximumvaluesforagivenscenarioareinbold.
participantswas≈11,comparedto6bytheMTurkparticipant
pool. The median numbers for the two groups were 12 and 5
Fig. 3: Grad-CAM activation maps of CELEBSET images for the
respectively.
task of 1-to-N re-identification on the VGG-Face model with surgical
and N-95 masked inputs. The first set of images on the left are for Thus, as also observed in prior research [44], experts
correct re-identification and the images on the right are for incorrect perform better than control groups at face re-identification
re-identification. In every triplet, we can see that the mask shifts the
tasks, but the cost of training and maintaining such experts is
regionofinterest,leadingtoincorrectre-identificationforsomeimages.
higher.Moreover,thereisnoguaranteethattrainingprograms
will significantly improve accuracy of human volunteers [76].
the original unmasked version of the same input image. On a Performance across intersectional groups: Next, we look at
closer observation, we can make the following inferences. the accuracies for the intersectional groups to understand to
Observations: (i) For all images, we see that on applying a what extent the human participants are also biased compared
face mask, the region of interest shifts/changes – the different to the automated FRSs. We observe the best accuracy for
heatmap locations in the second (masked input) and the third White males at 42.6% and the worst for both White and
image (the unmasked version of the input) in every triplet Black females at 35.3%. Thus even though the disparity is
show this shift. (ii) For the correctly re-identified images, only 7%, the lower accuracies are still reported for women
irrespective of the mask, we notice that there are overlap- and Black individuals. Systemic changes are needed to first
ping regions of interest (area around the eyes) between the educate people and reduce/remove their biases, and only then
unmasked and the masked image. (iii) For the set of images can AI be expected to be less biased. This corroborates with
where re-identification fails, we see that the potential regions prior research that has indicated humans may introduce their
of overlap (for example, the nose bridge) are either covered own biases in such tasks [47]. Our institutional pool reported
by the face mask, or shifted away. a higher accuracy for all intersectional groups compared to
The above observations can be generalized for all other the MTurk pool– 66% compared to 34% for White males and
models over a large set of images, and for the cloth masks 42.6%comparedto32.6%forBlackfemales.Thus,bothpools
(including other colours). Hence, the Grad-CAM activation still have a bias against minority groups but, as expected, the
mapsallowustodevelopahypothesisonwhytheopen-source experts perform better than the average.
modelsmayoftenfailatre-identificationundertherealistic1-
Correlation between humans and FRSs: We compare the
to-Nsetting.Thiscanserveasafirstcluetomitigationofsuch
responses of the volunteers and the FRS softwares viz. all
biases in future models.
commercialones–Face++,AzureFaceandAWSRekognition
and the top performing open-source ones – VGG-Face and
D. RQ4: 1-to-N face re-identification with volunteers DLib, using Pearson correlation in Table VI (row 1). For
To determine whether humans are indeed unreliable and every source-target pair (source = masked input image, target
inaccurate when performing the task of face re-identification = unmasked database image), we consider a positive re-
withmaskedfaces,weconductasurveywhereparticipantsare identification by volunteers if a majority of the volunteers
required to perform the task of 1-to-N re-identification on the who saw that source image, correctly performed the re-
CELEBSET dataset.Wethencomparetheseresponsesagainst identification. The FRS also has a binary response – either
those by the FRSs to observe how humans fare compared to it successfully performs the re-identification or it fails. From
the AI in terms of accuracy and scalability. the table we notice that the correlation values are positive for
1) Experiments with no deadline: The first part of the all FRSs except Face++. While the absolute numerical values
survey is without deadline, i.e., the participants are asked of correlation are fairly low, we notice that humans have the
to iterate through 20 sets of images and match the masked highestcorrelationwithAzure.Thissimplymeansthathumans
photo with the unmasked photos they feel are most similar, andAzureFRSagreethemostoften–irrespectiveofwhether
without any ‘pressure’ of time deadline. We consider the they re-identify correctly or fail.
participant’sresponseassuccessfuliftheychooseeither‘very 2) Experiments with deadline: We now discuss the results
similar’or‘sameperson’forthecorrectunmaskedphoto.Each for the second phase of the survey where participants were
participant can correctly assess between 0 – 20 images. asked to perform the re-identification task for as many sets
Aggregate performance: The mean number of images cor- as possible within a deadline of two minutes. Here as well,
rectly re-identified by 85 participants is ≈8, with a standard we consider the participant’s response as successful if they
deviationof5.8andamedianof6,givinganaccuracyofonly choose either ‘very similar’ or ‘same person’ for the correct
40%. While this is higher than both Face++ and Azure Face, unmasked photo.
it is less than half of AWS Rekognition. The two participant Aggregate performance: The average number of sets at-
pools performed very differently from each other – the mean tempted by the 85 participants was ≈ 13 with a standard
number of images correctly identified by the 23 institutional deviation of 4.55. The mean number of images re-identified9
by the participants was 4, with a standard deviation of 3.7 prone to societal biases and their judgements are severely
and a median of 3, giving an even lower accuracy than the erroneous when they are subjected to function under the
experiments without deadline at 31%. Once again, the two pressure of a deadline.
participant pools perform differently from each other – the
meannumberofimagescorrectlyidentifiedbytheinstitutional REFERENCES
participantsis≈5,comparedto≈3bytheMTurkparticipant
[1] P. India, “Ai: An opportunity amidst a crisis,” https://www.pwc.in/a
pool. The median numbers for the two groups were 5 and ssets/pdfs/data-and-analytics/ai-an-opportunity-amidst-a-crisis.pdf,
2 respectively. This experiment shows that while humans 2020,accessed:2023-05-25.
[2] Amazon, “Amazon aws,” https://aws.amazon.com/, 2022, accessed:
are generally unreliable, their accuracy is further adversely
2023-04-01.
impacted if they are asked to perform the task under the [3] Microsoft, “Microsoft azure,” https://azure.microsoft.com/, 2022,
‘pressure’ of a time restriction. accessed:2023-04-01.
[4] S. Goled, “Everything wrong with uber’s facial recognition system,”
Accuracy across intersectional groups: The disparity in
https://analyticsindiamag.com/everything-wrong-with-ubers-facial-rec
accuracyforeachindividualintersectionalgroupintheexperi- ognition-system/,2021,accessed:2022-04-05.
mentswithdeadlineisinsubstantial,withthehighestaccuracy [5] R. K. Verma, P. Singh, C. R. Panigrahi, and B. Pati, “Iss: Intelligent
security system using facial recognition,” in Progress in Advanced
reported for Black males at 32.5% and the lowest for White
ComputingandIntelligentEngineering,2021,pp.96–101.
females at 28.6%. Thus, the disparity has now reduced to [6] P. Batavia, I. Gajera, S. Gandhi, P. Mody, and S. Korde, “Accessible
half of the value seen for the scenario without deadline. We self-care and automated indoor navigation for covid-19 vaccination
centre,”in20212ndGlobalConferenceforAdvancementinTechnology
posit that this is related to the time constraint imposed on the
(GCAT),2021,pp.1–10.
participants who are now equally likely to misidentify a given [7] K. Srikrishnaswetha, S. Kumar, and M. Rashid Mahmood, “A study
maskedface,irrespectiveofthegenderorrace.Comparingthe onsmartelectronicsvotingmachineusingfacerecognitionandaadhar
verificationwithiot,”inInnovationsinElectronicsandCommunication
accuracyforintersectionalgroupsbetweenthetwoparticipant
Engineering,2019,pp.87–95.
pools, we notice that while the institutional group reported an [8] L. J. Spreeuwers, A. J. Hendrikse, and K. Gerritsen, “Evaluation of
accuracyof48.6%forWhitemales,MTurkparticipantscould automaticfacerecognitionforautomaticbordercontrolonactualdata
recordedoftravellersatschipholairport,”in2012BIOSIG-Proceedings
only identify 24.4% of them correctly. For Black females,
of the International Conference of Biometrics Special Interest Group
the accuracy values were 38% and 28.5% respectively. Even (BIOSIG). IEEE,2012,pp.1–6.
though the bias against gender/racial groups is seemingly [9] L. Best-Rowden, H. Han, C. Otto, B. F. Klare, and A. K. Jain,
“Unconstrainedfacerecognition:Identifyingapersonofinterestfrom
reduced, it comes at the cost of reduced overall accuracy.
amediacollection,”IEEETransactionsonInformationForensicsand
Correlation between humans and FRSs: Here as well, we Security,pp.2144–2157,2014.
compare the responses between the volunteers and the FRS [10] N. A. Abdullah, M. J. Saidi, N. H. A. Rahman, C. C. Wen, and
I. R. A. Hamid, “Face recognition for criminal identification: An
softwares in Table VI (row 2). Unlike the scenario with no
implementationofprincipalcomponentanalysisforfacerecognition,”
deadline, we notice that human volunteers have a positive inAIPconferenceproceedings,2017.
correlation with all FRSs and with higher absolute values for [11] Amazon,“Amazonawsrekognition,”https://aws.amazon.com/rekogni
tion/faqs/,2022,accessed:2022-04-01.
the commercial FRSs and similar ones for the open-source
[12] Face++, “Face++ ai open platform,” https://www.faceplusplus.com/,
models. Here as well Azure has the highest correlation. 2022,accessed:2022-04-01.
[13] Microsoft, “Microsoft azure face,” https://azure.microsoft.com/en-in/
services/cognitive-services/face/,2022,accessed:2022-04-01.
VI. CONCLUSION [14] Clarifai,“Clarifai,”https://www.clarifai.com/models/ai-face-detection,
2022,accessed:2022-04-01.
In this paper, we have audited thirteen automated FRSs, [15] IANS,“’aiindefence’:Govtdevelopsfacerecognitionsystemtosee
through mask, disguise,” https://www.deccanherald.com/national/ai-i
four commercial ones viz. Face++, Azure Face, AWS Rekog-
n-defence-govt-develops-face-recognition-system-to-see-through-mas
nition and FaceX and, nine open-source ones viz. VGG- k-disguise-1137223.html,2022,accessed:2022-09-05.
Face, Facenet, Facenet-512, OpenFace, DeepFace, DeepID, [16] S. I. Serengil and A. Ozpinar, “Hyperextended lightface: A facial
attributeanalysisframework,”inICEET,2021.
ArcFace,DLibandSFace,forthetaskoffacere-identification
[17] J. Guo, J. Deng, A. Lattas, and S. Zafeiriou, “Sample and com-
between masked and unmasked face images curated from putation redistribution for efficient face detection,” arXiv preprint
multiple benchmark datasets. While the utility of face masks arXiv:2105.04714,2021.
[18] B. Amos, B. Ludwiczuk, and M. Satyanarayanan, “Openface: A
in preventing the spread of viruses is still up for debate, un-
general-purpose face recognition library with mobile applications,”
doubtedly they have been commonplace since COVID-19 and CMU-CS-16-118, CMU School of Computer Science, Tech. Rep.,
therefore there is an increasing threat of their malicious use 2016.
[19] J.BuolamwiniandT.Gebru,“Gendershades:Intersectionalaccuracy
to hide and spoof identity. Such malicious use coupled with
disparitiesincommercialgenderclassification,”inPMLRFAT*,2018.
the known biases of FRSs against marginalized groups [19]– [20] I.D.Raji,T.Gebru,M.Mitchell,J.Buolamwini,J.Lee,andE.Denton,
[28], [31] can pose severe societal challenges and become “Saving face: Investigating the ethical concerns of facial recognition
auditing,”inAAAI/ACMAIES,2020.
highly dangerous when FRSs are deployed by governments
[21] J. P. Robinson, G. Livitz, Y. Henon, C. Qin, Y. Fu, and S. Timoner,
and private corporations for surveillance and policing pur- “Face recognition: Too bias, or not too bias?” in Proceedings of the
poses. Our work demonstrates through a series of rigorous IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR)Workshops,2020.
experiments that such practices need to be implemented with
[22] R. Singh, A. Agarwal, M. Singh, S. Nagpal, and M. Vatsa, “On the
utmost caution and depends on various factors including the robustnessoffacerecognitionalgorithmsagainstattacksandbias,”in
type and color of occlusion, the software in use, and the AAAI,2020.
[23] S.Nagpal,M.Singh,R.Singh,andM.Vatsa,“Deeplearningforface
population exposed. Further, having a human-in-the-loop is
recognition: Pride or prejudiced?” arXiv preprint arXiv:1904.01219,
neither scalable nor much useful as humans are themselves 2019.10
[24] P. Grother, M. Ngan, and K. Hanaoka, Face recognition vendor test [46] H. in the Loop, “Humans in the loop,” https://humansintheloop.org/,
(fvrt): Part 3, demographic effects. National Institute of Standards 2023,accessed:2023-05-25.
andTechnologyGaithersburg,MD,2019. [47] F. Poursabzi-Sangdeh, S. Samadi, J. W. Vaughan, and H. Wallach,
[25] J.G.Cavazos,P.J.Phillips,C.D.Castillo,andA.J.O’Toole,“Accu- “A human in the loop is not enough: The need for human-subject
racycomparisonacrossfacerecognitionalgorithms:Whereareweon experiments in facial recognition,” in CHI Workshop on Human-
measuringracebias?”IEEEtransactionsonbiometrics,behavior,and CenteredApproachestoFairandResponsibleAI,vol.8,2020,p.3.
identityscience,vol.3,no.1,pp.101–111,2020. [48] J. J. Howard, L. R. Rabbitt, and Y. B. Sirotin, “Human-algorithm
[26] P. Drozdowski, C. Rathgeb, A. Dantcheva, N. Damer, and C. Busch, teaminginfacerecognition:Howalgorithmoutcomescognitivelybias
“Demographicbiasinbiometrics:Asurveyonanemergingchallenge,” humandecision-making,”Plosone,vol.15,no.8,p.e0237855,2020.
IEEE Transactions on Technology and Society, vol. 1, no. 2, pp. 89– [49] M. Zhang, “Google photos tags two african-americans as gorillas
103,2020. through facial recognition software,” https://www.forbes.com/sites
[27] R. Van Noorden, “The ethical questions that haunt facial-recognition /mzhang/2015/07/01/google-photos-tags-two-african-americans-as-g
research,”Nature,vol.587,no.7834,pp.354–359,2020. orillas-through-facial-recognition-software/?sh=7999eabd713d,2015,
[28] F. Bacchini and L. Lorusso, “Race, again: how face recognition accessed:2022-08-20.
technology reinforces racial discrimination,” Journal of information, [50] BBC, “Facebook apology as ai labels black men ’primates’,” https:
communicationandethicsinsociety,2019. //www.bbc.com/news/technology-58462511,2021,accessed:2022-08-
[29] K. Krishnapriya, V. Albiero, K. Vangara, M. C. King, and K. W. 20.
Bowyer, “Issues related to face recognition accuracy varying based [51] J. Snow, “Amazon’s face recognition falsely matched 28 members of
onraceandskintone,”IEEETransactionsonTechnologyandSociety, congress with mugshots,” https://www.aclu.org/news/privacy-technol
vol.1,no.1,pp.8–20,2020. ogy/amazons-face-recognition-falsely-matched-28, 2018, accessed:
[30] G.C.andF.J.,“Facial-recognitionsoftwaremighthavearacialbias 2021-04-01.
problem,” https://www.theatlantic.com/technology/archive/2016/04 [52] C. Sandvig, K. Hamilton, K. Karahalios, and C. Langbort, “Auditing
/the-underlying-bias-of-facial-recognition-systems/476991/, 2022, algorithms:Researchmethodsfordetectingdiscriminationoninternet
accessed:2022-08-01. platforms,”Dataanddiscrimination:convertingcriticalconcernsinto
[31] S. Jaiswal, K. Duggirala, A. Dash, and A. Mukherjee, “Two-face: productiveinquiry,2014.
Adversarial audit of commercial face recognition systems,” in AAAI [53] T. Sixta, J. Jacques Junior, P. Buch-Cardona, E. Vazquez, and S. Es-
ICWSM,vol.16,2022,pp.381–392. calera, “Fairface challenge at eccv 2020: Analyzing bias in face
[32] S.Dooley,G.Z.Wei,T.Goldstein,andJ.Dickerson,“Robustnessdis- recognition,” in European conference on computer vision. Springer,
paritiesinfacedetection,”AdvancesinNeuralInformationProcessing 2020.
Systems,vol.35,pp.38245–38259,2022.
[54] C.Hazirbas,J.Bitton,B.Dolhansky,J.Pan,A.Gordo,andC.C.Ferrer,
[33] K.Johnson,“Howwrongfularrestsbasedonaiderailed3men’slives,” “Towards measuring fairness in ai: the casual conversations dataset,”
https://www.wired.com/story/wrongful-arrests-ai-derailed-3-mens-liv IEEETransactionsonBiometrics,Behavior,andIdentityScience,2021.
es/,2022,accessed:2023-04-01.
[55] S. Engelmann, C. Ullstein, O. Papakyriakopoulos, and J. Grossklags,
[34] ——,“Facerecognitionsoftwareledtohisarrest.itwasdeadwrong,” “Whatpeoplethinkaishouldinferfromfaces,”in2022ACMConfer-
https://www.wired.com/story/face-recognition-software-led-to-his-arr enceonFairness,Accountability,andTransparency,2022.
est-it-was-dead-wrong/,2023,accessed:2023-04-04.
[56] S.Yucer,F.Tektas,N.AlMoubayed,andT.P.Breckon,“Measuring
[35] W.areplacementforyourlostSIMcard?Biometricsmightgetinyour
hiddenbiaswithinfacerecognitionviaracialphenotypes,”inWACV,
way,“Vallarisanzgiri,”https://www.medianama.com/2023/03/223-rep
2022.
lacement-sim-lost-car-astr-facial-biometrics/, 2023, accessed: 2023-
[57] S.Qian,V.H.Pham,T.Lutellier,Z.Hu,J.Kim,L.Tan,Y.Yu,J.Chen,
04-04.
andS.Shah,“Aremydeeplearningsystemsfair?anempiricalstudy
[36] R. Pal and U. Yadav, “Resurgence of covid-19 in india: time for
of fixed-seed training,” Advances in Neural Information Processing
introspection,”PostgraduateMedicalJournal,2022.
Systems,2021.
[37] M. Opitz, G. Waltner, G. Poier, H. Possegger, and H. Bischof, “Grid
[58] G. Jain and S. Parsheera, “Cinderella’s shoe won’t fit soundarya:
loss: Detecting occluded faces,” in Computer Vision – ECCV 2016,
An audit of facial processing tools on indian faces,” arXiv preprint
B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds. Springer
arXiv:2112.09326,2021.
InternationalPublishing,2016.
[59] M. V. Ferreira, A. Almeida, J. P. Canario, M. Souza, T. Nogueira,
[38] J. Vincent, “Face masks are breaking facial recognition algorithms,
and R. Rios, “Ethics of ai: Do the face detection models act with
says new government study,” https://www.theverge.com/2020/7/28/2
prejudice?”inBrazilianConferenceonIntelligentSystems,2021.
1344751/facial-recognition-face-masks-accuracy-nist-study, 2020,
[60] K.Kyriakou,P.Barlas,S.Kleanthous,andJ.Otterbacher,“Fairnessin
accessed:2022-04-05.
proprietaryimagetaggingalgorithms:Across-platformauditonpeople
[39] T.Simonite,“Howwellcanalgorithmsrecognizeyourmaskedface?”
images,”AAAIICWSM,2019.
https://www.wired.com/story/algorithms-recognize-masked-face/,
[61] I. D. Raji and G. Fried, “About face: A survey of facial recognition
2020,accessed:2023-04-04.
evaluation,”arXivpreprintarXiv:2102.00813,2021.
[40] W.Xu,W.Song,J.Liu,Y.Liu,X.Cui,Y.Zheng,J.Han,X.Wang,and
K.Ren,“Maskdoesnotmatter:Anti-spoofingfaceauthenticationusing [62] S.-g. Jung, J. An, H. Kwak, J. Salminen, and B. Jansen, “Assessing
mmwavewithouton-siteregistration,”ser.MobiCom’22. NewYork, theaccuracyoffourpopularfacerecognitiontoolsforinferringgender,
NY,USA:AssociationforComputingMachinery,2022,p.310–323. age,andrace,”AAAIICWSM,2018.
[41] A.J.O’Toole,P.JonathonPhillips,F.Jiang,J.Ayyad,N.Penard,and [63] P. Barlas, K. Kyriakou, S. Kleanthous, and J. Otterbacher, “Social
H.Abdi,“Facerecognitionalgorithmssurpasshumansmatchingfaces b(eye)as: Human and machine descriptions of people images,” AAAI
overchangesinillumination,”IEEETransactionsonPatternAnalysis ICWSM,2019.
andMachineIntelligence,pp.1642–1646,2007. [64] N. Srinivas, K. Ricanek, D. Michalski, D. S. Bolme, and M. King,
[42] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing “Face recognition algorithm bias: Performance differences on images
thegaptohuman-levelperformanceinfaceverification,”in2014IEEE ofchildrenandadults,”inCVPRWorkshops,2019.
Conference on Computer Vision and Pattern Recognition, 2014, pp. [65] A. Kortylewski, B. Egger, A. Schneider, T. Gerig, A. Morel-Forster,
1701–1708. and T. Vetter, “Empirically analyzing the effect of dataset biases on
[43] H.Wang,S.Gong,X.Zhu,andT.Xiang,“Human-in-the-loopperson deepfacerecognitionsystems,”inCVPRWorkshops,2018.
re-identification,” in Computer Vision–ECCV 2016: 14th European [66] J.Pahl,I.Rieger,A.Mo¨ller,T.Wittenberg,andU.Schmid,“Female,
Conference,Amsterdam,TheNetherlands,October11–14,2016,Pro- white,27?biasevaluationondataandalgorithmsforaffectrecognition
ceedings,PartIV14. Springer,2016,pp.405–422. infaces,”inFAccT,2022.
[44] P.J.Phillips,A.N.Yates,Y.Hu,C.A.Hahn,E.Noyes,K.Jackson, [67] E.Kim,D.Bryant,D.Srikanth,andA.Howard,“Agebiasinemotion
J. G. Cavazos, G. Jeckeln, R. Ranjan, S. Sankaranarayanan et al., detection: An analysis of facial emotion recognition performance on
“Face recognition accuracy of forensic examiners, superrecognizers, young,middle-aged,andolderadults,”inAIES,2021.
andfacerecognitionalgorithms,”ProceedingsoftheNationalAcademy [68] D.Raz,C.Bintz,V.Guetler,A.Tam,M.Katell,D.Dailey,B.Herman,
ofSciences,vol.115,no.24,pp.6171–6176,2018. P.Krafft,andM.Young,“Facemis-id:Aninteractivepedagogicaltool
[45] C.Butler,H.Oster,andJ.Togelius,“Human-in-the-loopaiforanalysis demonstratingdisparateaccuracyratesinfacialrecognition,”inAIES,
offreeresponsefacialexpressionlabelsets,”ser.IVA’20,2020. 2021.11
[69] B. F. Klare, M. J. Burge, J. C. Klontz, R. W. Vorder Bruegge, and [96] J.Yu,X.Hao,Z.Cui,P.He,andT.Liu,“Boostingfairnessformasked
A. K. Jain, “Face recognition performance: Role of demographic in- facerecognition,”inICCVWorkshops,2021.
formation,”IEEETransactionsonInformationForensicsandSecurity, [97] S. Das, S. Sarkar, A. Das, S. Das, P. Chakraborty, and J. Sarkar, “A
2012. comprehensivereviewofvariouscategoriesoffacemasksresistantto
[70] M. K. Scheuerman, J. M. Paul, and J. R. Brubaker, “How computers covid-19,”Clinicalepidemiologyandglobalhealth,2021.
see gender: An evaluation of gender classification in commercial [98] Google, “Improving skin tone evaluation in machine learning,” https:
facialanalysisservices,”ProceedingsoftheACMonHuman-Computer //skintone.google/,2022,accessed:2022-05-05.
Interaction,2019. [99] FaceX, “Facex.io,” https://facex.io/step-three-face-recognition.html,
[71] A. Ghosh, L. Genuit, and M. Reagan, “Characterizing intersectional 2022,accessed:2022-04-01.
groupfairnesswithworst-casecomparisons,”inAIDBEI,2021. [100] O. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,”
[72] J. Peters, “Ibm will no longer offer, develop, or research facial inBMVC2015-ProceedingsoftheBritishMachineVisionConference
recognitiontechnology,”https://www.theverge.com/2020/6/8/21284683 2015. BritishMachineVisionAssociation,2015.
/ibm-no-longer-general-purpose-facial-recognition-analysis-software, [101] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified
2020,accessed:2021-04-01. embeddingforfacerecognitionandclustering,”inProceedingsofthe
[73] S.Bird,“Responsibleaiinvestmentsandsafeguardsforfacialrecogni- IEEE conference on computer vision and pattern recognition, 2015,
tion,”https://azure.microsoft.com/en-us/blog/responsible-ai-investmen pp.815–823.
ts-and-safeguards-for-facial-recognition/,2022,accessed:2022-07-15. [102] Y. Sun, Y. Chen, X. Wang, and X. Tang, “Deep learning face rep-
[74] Uber,“Howdoesuberverifymyphoto?”https://help.uber.com/drivin resentation by joint identification-verification,” Advances in neural
g-and-delivering/article/how-does-uber-verify-my-photo?nodeId=aa informationprocessingsystems,vol.27,2014.
821486-c8d1-42b7-b784-2fc24eb85f93& csid=LTQcYY9DRweK7D [103] J.Deng,J.Guo,N.Xue,andS.Zafeiriou,“Arcface:Additiveangular
Yrls8WTg&state=2koveCnme0maAGJ 8NNJFZVyNiDVk3nuXrQAa marginlossfordeepfacerecognition,”inProceedingsoftheIEEE/CVF
UhwIo0%3D&effect=,2022,accessed:2022-04-01. conference on computer vision and pattern recognition, 2019, pp.
[75] D.White,A.M.Burton,R.I.Kemp,andR.Jenkins,“Crowdeffectsin 4690–4699.
unfamiliarfacematching,”Appliedcognitivepsychology,pp.769–777, [104] D. King, “Face recognition models,” https://github.com/davisking/dli
2013. b-models,2023,accessed:2023-08-01.
[76] A. Towler, R. I. Kemp, A. M. Burton, J. D. Dunn, T. Wayne, [105] F. Boutros, M. Huber, P. Siebke, T. Rieber, and N. Damer, “Sface:
R.Moreton,andD.White,“Doprofessionalfacialimagecomparison Privacy-friendly and accurate face recognition using synthetic data,”
trainingcourseswork?”PloSone,2019. in 2022 IEEE International Joint Conference on Biometrics (IJCB).
[77] D.Zeng,R.Veldhuis,andL.Spreeuwers,“Asurveyoffacerecognition IEEE,2022,pp.1–11.
techniquesunderocclusion,”IETBiometrics,2021. [106] I.F.Foundation,“Delhipolice’sclaimsthatfrtisaccuratewitha80%
match are 100% scary,” https://internetfreedom.in/delhi-polices-frt-u
[78] V. Chandrasekaran, C. Gao, B. Tang, K. Fawaz, S. Jha, and
S. Banerjee, “Face-off: Adversarial face obfuscation,” arXiv preprint se-is-80-accurate-and-100-scary/,2022,accessed:2022-08-20.
arXiv:2003.08861,2020.
[79] F.Vakhshiteh,A.Nickabadi,andR.Ramachandra,“Adversarialattacks
againstfacerecognition:Acomprehensivestudy,”IEEEAccess,2021.
[80] S.J.Oh,M.Fritz,andB.Schiele,“Adversarialimageperturbationfor
privacyprotectionagametheoryperspective,”inIEEEICCV,2017.
[81] M. D. Bloice, P. M. Roth, and A. Holzinger, “Biomedical image
augmentationusingAugmentor,”Bioinformatics,2019.
[82] A. Goel, A. Singh, A. Agarwal, M. Vatsa, and R. Singh, “Smartbox:
Benchmarkingadversarialdetectionandmitigationalgorithmsforface
recognition,”inIEEEBTAS,2018.
[83] G.Garofalo,V.Rimmer,D.Preuveneers,W.Joosenetal.,“Fishyfaces:
Craftingadversarialimagestopoisonfaceauthentication,”inUSENIX
WOOT,2018.
[84] A.J.BoseandP.Aarabi,“Adversarialattacksonfacedetectorsusing
neuralnetbasedconstrainedoptimization,”inIEEEMMSP,2018.
[85] F. V. Massoli, F. Carrara, G. Amato, and F. Falchi, “Detection of
face recognition adversarial attacks,” Computer Vision and Image
Understanding,2021.
[86] equalAIs, “equalais empowering humans by subverting machines,” ht
tps://equalais.media.mit.edu/,2021,accessed:2021-04-01.
[87] S.Mishra,P.Majumdar,R.Singh,andM.Vatsa,“Indianmaskedfaces
inthewilddataset,”inICIP,2021.
[88] A.AnwarandA.Raychowdhury,“Maskedfacerecognitionforsecure
authentication,”arXivpreprintarXiv:2008.11104,2020.
[89] N. Damer, F. Boutros, M. Su¨ßmilch, M. Fang, F. Kirchbuchner, and
A. Kuijper, “Masked face recognition: Human versus machine,” IET
Biometrics,2022.
[90] D.S.Ma,J.Correll,andB.Wittenbrink,“Thechicagofacedatabase:
A free stimulus set of faces and norming data,” Behavior Research
Methods,2015.
[91] D. S. Ma, J. Kantner, and B. Wittenbrink, “Chicago face database:
Multiracialexpansion,”BehaviorResearchMethods,2020.
[92] A.Lakshmi,B.Wittenbrink,J.Correll,andD.S.Ma,“Theindiaface
set:Internationalandculturalboundariesimpactfaceimpressionsand
perceptionsofcategorymembership,”FrontiersinPsychology,2021.
[93] K. Karkkainen and J. Joo, “Fairface: Face attribute dataset for bal-
ancedrace,gender,andageforbiasmeasurementandmitigation,”in
IEEE/CVFWACV,2021.
[94] D. Qi, K. Hu, W. Tan, Q. Yao, and J. Liu, “Balanced masked and
standard face recognition,” in ICCV Workshops, October 2021, pp.
1497–1502.
[95] H. Qiu, D. Gong, Z. Li, W. Liu, and D. Tao, “End2end occluded
face recognition by masking corrupted features,” IEEE Transactions
onPatternAnalysisandMachineIntelligence,2021.