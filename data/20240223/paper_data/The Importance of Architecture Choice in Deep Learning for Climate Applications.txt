The Importance of Architecture Choice in Deep Learning for Climate
Applications
SimonDra¨ger1 MaikeSonnewald1
Abstract
Machine Learning has become a pervasive tool
inclimatescienceapplications. However,current
models fail to address nonstationarity induced
byanthropogenicalterationsingreenhouseemis- Tropical Subpolar
sionsanddonotroutinelyquantifytheuncertainty
ofproposedprojections. Inthispaper,wemodel Figure1.Stommel box model with a fresh water forcing and a
theAtlanticMeridionalOverturningCirculation temperatureforcing.Notethatthisisthe“extendedvariant”where
(AMOC)whichisofmajorimportancetoclimate weobtainthestandardvariantbysettingF t ≡0.
inEuropeandtheUSEastCoastbytransporting
warmwatertotheseregions,andhasthepotential
1.Introduction
forabruptcollapse. Wecangeneratearbitrarily
extremeclimatescenariosthrougharbitrarytime
Climatechangehasasignificantimpactonourplanetand
scales which we then predict using neural net-
society,withmanyunknownvariables. Oneofthesevari-
works. Our analysis shows that the AMOC is
ables is the Atlantic Meridional Overturning Circulation
predictableusingneuralnetworksunderadiverse
(AMOC)whichtransportswarmwaterfromthetropicsto
setofclimatescenarios. Furtherexperimentsre-
theUSEastCoastandEurope. WithouttheAMOC,these
veal that MLPs and Deep Ensembles can learn
places would be uninhabitable due to ice age conditions,
thephysicsoftheAMOCinsteadofimitatingits
however, recent climate developments induce significant
progressionthroughautocorrelation. Withquanti-
nonstationarityintotheAMOCsystemwithlargelyuncer-
fieduncertainty,anintriguingpatternof“spikes”
tain outcomes. An interesting part of the AMOC is its
before critical points of collapse in the AMOC
tipping point behavior (Tziperman, 2022; Lenton et al.,
castsdoubtonpreviousanalysesthatpredictedan
2008). Atippingpoint(Lentonetal.,2008;VanNesetal.,
AMOCcollapsewithinthiscentury. Ourresults
2016)isdefinedasapointinasystemwhichuponasmall
show that Bayesian Neural Networks perform
perturbationleadstoadramaticchangewithinthesystem.
poorlycomparedtomoredensearchitecturesand
Thistippingpointbehaviorisespeciallyinterestingdueto
care should be taken when applying neural net-
anthropogenic climate change driving the AMOC in the
workstononstationaryscenariossuchasclimate
aforementionednonstationaryway,ultimatelyleadingto-
projections. Further, our results highlight that
wardatippingpoint(Tziperman,2022). Bynonstationary,
bigNNmodelsmighthavedifficultyinmodeling
wemeanforexampleanon-constantamplitudeofsinusoidal
globalEarthSystemdynamicsaccuratelyandbe
greenhouseemissions(stationary,ontheotherhand,would
successfullyappliedinnonstationaryclimatesce-
meantheemissionshavethesamemeanandstandarddevi-
narios due to the physics being challenging for
atinoovertime). Todealwiththechallengeofincreasingly
neuralnetworkstocapture.
nonstationaryAMOC(andclimate,byextension)behavior,
we wish to take advantage of the predictive abilities that
MachineLearning(ML)methodsprovide. Here,wepresent
newinsightintotheextentthatdifferentneuralnetworkar-
1Department of Computer Science, University of Cal- chitecturesareabletolearntheAMOC’sphysics,howthey
ifornia, Davis, USA. Correspondence to: Simon Dra¨ger workinnonstationaryclimatescenariosandhowuncertain
<sdraeger@ucdavis.edu>. theirpredictionsare.
With the inception of ML as a pervasive tool for predic-
1
4202
beF
12
]GL.sc[
1v97931.2042:viXraTheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
downorrapidchangeisstudiedusingconventionaltools
1040
within climate science, such as climate models (Menary
1030 etal.,2013;Swingedouwetal.,2013;Chengetal.,2013;
0
1020 S-8 Wenetal.,2016;Jacksonetal.,2023),butalsousingsim-
O
1010 E ρ pler statistical tools, such as stochastic process modeling
usingaStochasticDifferentialEquation(SDE)(Ditlevsen
1000
& Ditlevsen, 2023). The projections of these statistical
tools recently caused alarm, suggesting an AMOC shut-
40
30
0 20 1020 downcouldhappenassoonas2025(Ditlevsen&Ditlevsen,
40 0 T
S 2023). However,suchmethodsrelyonmakingverystrong
assumptions about the AMOC system. With neural net-
Figure2.ρEOS-80(T,S)plottedonT×S ⊂[0,40]2.ρEOS-80(T,S) worksbeinguniversalfunctionapproximators,weareless
isconcave,andthusnonlinear,inS. reliantonassumptionsandweseemuchpromisefortheuse
ofMachineLearning.
TheuseofMachineLearningisalreadyprevalentinweather
tive modeling, different questions of societal importance, forecasts(Pathaketal.,2022;Singhetal.,2019;Salman
including climate science and oceanography (see for ex- etal.,2015),aswellastopredictandexplaintheimpactof
ample(Sonnewaldetal.,2021;Jones,2017;Irrgangetal., globalheatingontheNorthAtlanticOcean(Sonnewald&
2021)),havebeenaddressed. Weidentifythreekeyopen Lguensat,2021). Morerecently,researchontheocean(Bire
questionsthatremainunaddressed: Firstly,itremainsun- etal.,2023)startedemployingtheFourierNeuralOperator
knownhowwellMLmethodssuchasneuralnetworksare (FNO;(Lietal.,2020))inanoveloceanemulatorsystem.
abletocapturetheunderlyingsystembehaviorofthecli- Previousefforts(Hazelegeretal.,2013;Mateietal.,2012;
matesystem.Inordertouseneuralnetworksconfidentlywe Mahajanetal.,2011)atpredictingtheAMOCarelimited
needtoknowwhatdeterminesatrainednetwork’sabilityto toshorttimescaleswithin10yearsintothefutureandare
estimateout-of-sampleclimatedatareliablyandwhetherits unable to simulate and quantify concrete climate change
skillcorrespondstotheunderlyingphysics.Ifanetperforms variablessuchasgreenhousegasemissions.
wellonlyduetospuriouscorrelationsintheinputdata,us-
MeasuredAMOCdataisnotabundantandonlyabout20
ingthenetforinferencecouldleadtodevastatingsocietal
years of contiguous AMOC data have been collected so
impactfromwronginference.Secondly,itisunclearhow
far(McCarthyetal.,2015). Thissignificantdrawbackof
confidentlyaneuralnetcanbeusedwiththenonstationary
usingrealworlddatatopredicttheAMOCislimitingac-
forcingimposedbyanthropogenicallyinducedgreenhouse
curatefutureprojectionsandusuallydoesnotincorporate
gases. As such, we are projecting systems, such as the
thedrivingvariablesoftheAMOC. Adifferentdownside
AMOC, with known instabilities into an unknown future
is that none of the previously mentioned works on ocean
byaskingneuralnetstobeskillfulout-of-sample,likepre-
modelingassessthequestionofwhethertheneuralnetwork
dictinga“tippingpoint”theypreviouslyhaveneverseen.
hascapturedtheunderlyingphysicalsystem,orifthenet
Thirdly, thereisnodoubtthatanyprediction, physics-or
isfitfordeploymentinnonstationaryapplications. Inour
ML-based,willhavesomenonzerouncertaintyassociated
work,wemodeltheNorthAtlanticOceanusingaphysical
with it. Thus, it is imperative to be able to quantify the
systemandsubsequentlyattempttopredictitsbehaviorus-
uncertaintyofaneuralnet’spredictionforittobesocially
ingneuralnetworks. Inaddition, wenotonlypredictbut
useful.
explainwhydifferentnetworkarchitecturespredictedthe
Thequestionofwhetheraneuralnethas“learned”physics system in the way they did, yielding an approach that is
andifitcanbeappliedout-of-samplewithloworquantified flexibleandcanuseanytimescale(months,years,1000sof
uncertaintyarefascinatingproblemswithingthefieldofML. years).
Hereweillustratehowskillfuldifferentarchitecturesareat
WeconsideramodeloftheAtlanticOceanconsistingoftwo
predictingtheAMOC,howwellneuralnetworksareableto
boxes; anorthernandasouthernbox. Thismodelisalso
capturetheunderlyingphysicsusingExplainableAI(XAI,
calledthe“Stommelboxmodel”(Stommel,1961)andis
describedfurtherbelow)andhowconfidentlytheyperform
canonicallyusedtomodeloceancirculationliketheAMOC.
undernonstationaryconditionsusinganensembleapproach.
IntheStommelboxmodel,eachboxisassignedavariable
Inparticular,weareinterestedinunderstandingtheintrinsic
temperature T and a salinity S where i 1,2 . The
differncesbetweenneuralnetstrainedonphysics-informed i i ∈ { }
AMOCisrepresentedasavariableqandF isanexternal
featuresvs.autoregressively,whichestablishedtheduality s
freshwaterforcingoriginatingfromanthropogenicclimate
understandingvs.imitatingtheAMOCsystem.
change. InFigure1,wedepictaschematicoftheStommel
Duetoitsimportance,theAMOCanditspotentialforashut-
2TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
ForcingSetupIdentifier F F ρ
s t
Linear —
1
F Stationarysinusoidal — ρlin
2
F
Nonstationarysinusoidal —
3
F
Linear Linear
4
F Stationarysinusoidal Stationarysinusoidal ρEOS-80
5
F
Nonstationarysinusoidal Nonstationarysinusoidal
6
F
Table1. Thesetofforcingsetupsthatweconsiderinourexperiments.
model with a fresh water and a temperature forcing, i.e., 2.2.Modelingq
externalwaterthatflowsintothesouthernboxaswellas
Nonlinearcase. Webeginwithρbeingnonlinear. First,
greenhouseemissionsalteringitstemperature. Welistall
letτ bethetimestepwhichweusetomodeltheevolution
parameters for the box model along with their values for
ofS andT throughtime(Tziperman,2022):
thispaperinAppendixA,Table2. i i
∂S q (S S ) F (τ)
At the heart of the Stommel box model lie the concepts 1 = | | 2 − 1 − s (2)
breakdownandrecovery.Breakdownreferstotheexceeding ∂τ V 1
ofapointinthemodelthatleadstoasuddencollapseofthe ∂S 2 = |q |(S 1 −S 2)+F s(τ) (3)
AMOC(Liuetal.,2017;Tziperman,2022). Similarly,the ∂τ V
2
inverseconceptrecoverydenotesthereturnoftheAMOC ∂T q (T T ) F (τ)
1 = | | 2 − 1 − t (4)
toapreviousstateinwhichithadnotcollapsed(Thomas ∂τ V
1
&Fedorov,2019). TheStommelboxmodelthusgivesus
∂T q (T T )+F (τ)
threestablestatesaneuralnetshouldbeabletomodeland 2 = | | 1 − 2 t (5)
∂τ V
arelativelysimpleattractorbasinthatemergesthroughthe 2
combinationofequationsdescribingtheboxmodel. This with V being the volume of the i-th box. F (τ), a fresh
i s
attractor basin and its underlying equations are what the waterforcing,andF (τ),atemperatureforcing,represent
t
neuralnetshould“learn”. Fromaneuralnetworkperspec- ice melting in the polar region or increased heat due to
tive,ensembleneuralnetsofferanintuitiveavenueinterms elevatedgreenhousegaslevels.Theseallowustoeffectively
ofexploringtheattractorbasinsinceitisknownthatthere simulate different climate scenarios. Note that F (τ) is
t
existthreeequallyimportantstablestatesintheStommel optional(fallbacktothestandardboxmodelvariant)and
boxmodel(Tziperman,2022). leavingitoutomitsEquations(4)and(5). Figure2showsa
plotofρEOS-80(T,S),demonstratingthatitisconcaveinS.
FortheexperimentsusingρEOS-80,weselecttheinitialS
2.ModelingtheAMOCusingPhysical i
suchthattheylieintheinterval[5,15]inordertointroduce
EquationsandNeuralNetworks
asmuchnonlinearityaspossibleintotheboxmodel,thereby
2.1.ImportanceofDensityanditsApproximation establishing a stark contrast between experiments with a
linearandnonlineardensity.
Thedensityρ ineachboxoftheStommelmodeldepends
i
onT andS andfundamentallyaffectsqviathedifference
i i Linearcase. Following(Tziperman,2022),wedefine
inρ betweenbothboxes(Tziperman,2022). Thereexist
i
twowaystomodelthewaterdensityinbox1and2. The ∆S =S S (6)
1 2
−
firstisalinearapproximation ∆T =T T (7)
1 2
−
which due to the linearity of differentiation leads to the
ρ(T,S)=ρ α(T T )+β(S S ) (1)
0 − − 0 − 0 formulationofthepartialdifferentialequationsthatmodel
theevolutionof∆S and∆T underforcingsF andF over
s t
whereρ isabaselinedensityvalueandT ,S arebaseline time,respectively(Tziperman,2022):
0 0 0
valuesfortemperatureandsalinity,respectively.αandβare
∂∆S q ∆S+F (τ)
thethermalexpansionandhalinecontractioncoefficients. = 2| | s (8)
∂τ − V
The second is the Equation of State (EOS-80; (Fofonoff,
∂∆T q ∆T +F (τ)
1985))forseawater,whichforareferencepressure(inthis = 2| | t (9)
∂τ − V
paper,atmosphericpressure)nonlinearlyapproximatessea-
waterdensity. Duetoitssizewedonotoutlineithere. whereV =V =V istheboxvolume.
1 2
3TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
BNN MLP DE
0 0 0
20
− 20 20
− −
0 500 1000 0 500 1000 0 500 1000
τ τ τ
0 0 0
20
− 20 20
− −
0 500 1000 0 500 1000 0 500 1000
τ τ τ
Ground Truth Prediction
Figure3.Predictiveperformancefortheconsideredarchitecturesusingphysics-informed(PI;firstrow)andautoregressive(AR;second
row)featuresunderF .
1
BNN MLP DE notinclude∆T asaninputfeatureduetothefactthatfor
25.8 27.7 27.66 F 0, ∆T τmax 0 τmax.
µ µ µ t ≡ { τ }τ=0 ≈{ }τ=0
Fs Fs Fs Now let f θ be a neural network with parameters θ. We
∆S 0 500 −25.8 ∆S 0 500 −27.7 ∆S 0 500 −27.66 considertwofundamentalapproaches;
τ τ τ
4.64 24.02 25.36
Approach(I): Physics-informed(PI),inthesensethatthe
inputstof arephysicalvariables:
θ
0 5 τ00 −4.64 0 5 τ00 −24.02 0 5 τ00 −25.36 f θ(∆S τ,∆T τ,F s(τ),F t(τ))=q τ (14)
and
Figure4.DeepLIFTattributionmapsfortheconsideredarchitec-
turesusingphysics-informed(PI;toprow)andautoregressive(AR;
f (S ,S ,T ,T ,F (τ),F (τ))=q . (15)
bottomrow)featuresunderF . θ 1,τ 2,τ 1,τ 2,τ s t τ
1
Approach(II): Autoregressive(AR),wherewediscardthe
Finally,q ismodeledasaproportionaldensitydifference
physicalvariablesandpredictqfromthelastωvaluesofq
betweenbox1and2:
intime:
(cid:40) k(β∆S α∆T), ρ=ρlin f θ(q τ−ω,...,q τ−1)=q τ. (16)
q =k(ρ ρ )= −
1 − 2 k(ρEOS-80 ρEOS-80), ρ=ρEOS-80
1 − 2
(10) 2.4.NeuralNetworkArchitectures
Weconsiderthreeneuralnetworkarchitecturesthatweuse
2.3.ObtainingSimulatedClimateDatawiththe
topredicttheAMOC:Multi-LayerPerceptron(MLP),Deep
StommelBoxModel
Ensemble(DE)andBayesianNeuralNetwork(BNN).Only
By solving Equations (8) and (9) or Equations (2) to (5), twooftheaforementionedarchitecturesareabletoquantify
respectively,wereceivesetsofvaluesorderedbytime: uncertainty;theDeepEnsemblebyaveragingandtheBNN
by MonteCarlo samplingand successive computationof
∆S τmax ∆T τmax (11)
{ τ }τ=0 { τ }τ=0 themeanandstandarddeviation(Jospinetal.,2022).BNNs
havepreviouslybeenusedforoceanapplications(Clare&
and
Piggott,2022;Clareetal.,2022;Yiketal.,2023;Rasouli
S τmax S τmax (12) etal.,2020;Bittigetal.,2018;Juanetal.,2023;Clare&Pig-
{ 1,τ }τ=0 { 2,τ }τ=0
T τmax T τmax (13) gott,2022),aswellasdifferenttasksusingthemforuncer-
{ 1,τ }τ=0 { 2,τ }τ=0 taintyquantification(Thodberg,1996;Springenbergetal.,
with F (τ) τmax and F (τ) τmax asadditionalsetsofval- 2016;Zhang&Garikipati,2021;Baoetal.,2020).DeepEn-
{ s }τ=0 { t }τ=0
uesovertime. Notethatforthestandardboxmodel,wedo semblesconsistoftrainingmultipleindependentdeepneural
4
µ1
τ,...,01
τ
−
−
µ1
τ,...,01
τ
−
−
)vS(q
)vS(q
µ1
τ,...,01
τ
−
−
)vS(q
)vS(q
)vS(q
)vS(qTheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
ForcingSetup Forcing Variables q
10
0 0
0
F1
10
−2 −20
− 0 100 0 100 0 100
τ(kiloyears) τ(kiloyears) τ(kiloyears)
10
0 0
0
F2
10
−2 −20
− 0 100 0 100 0 100
τ(kiloyears) τ(kiloyears) τ(kiloyears)
5 0 0
0
F3 −5 −2 −20
0 100 0 100 0 100
τ(kiloyears) τ(kiloyears) τ(kiloyears)
Figure5. Forcingtrajectory,physicalvariablesandAMOCforF ,F andF .
1 2 3
networkswhosepredictionsareaveragedduringinference. propagatinginternalcontributionscorescanleadtodifferent
Theyhavebeenusedextensivelyinclimatescienceappli- attributionmapsandtellussomethingabouttheinternaland
cationstogeneratemoreaccuratepredictions(Sonnewald externalattributionmechanicsofatrainedpredictor,respec-
etal.,2021;Choetal.,2022;Jahanbakhtetal.,2021;Qi tively. Thiscanalsobeseenasexaminingsensitivitytodata
etal.,2023). vs.theinternalnetworkparameters. Priorwork(Clareetal.,
2022) has examined this, linking different explainability
2.5.Explainability algorithmstoaleatoricvs.epistemicuncertainty. Aleatoric
uncertaintyreferstouncertaintythatisinherentinthedata
Currentworks(Lobelleetal.,2020;Ditlevsen&Ditlevsen,
whereasepistemicuncertaintydenotesuncertaintyinherent
2023) on predicting the AMOC focus on predicting the
inthemodelthatisbeingused(Clareetal.,2022). Forthe
AMOC’sprogressionovertimewithanemphasisonapoten-
autoregressivecases(usingSHAP)duetoaninputlengthof
tialtippingpoint. Theydonot,however,attempttoexplain
10,wedonotuseallfeaturesubsetsbutrathersamplethem.
whytheirpredictivemodelsoutputtheresultingpredictions.
Werepeatthissamplingprocess20timesandtakethemean
Thisisamajorrestrictionwithrespecttodeterminingwhy
foreachτ ineachattributionmap.
theAMOCisprojectedtoevolveinacertainwayandim-
pedesclimateactionability. Weovercomethisrestrictionby
generatingfeatureattributionsperτ,visualizingwhatinput
featurehadastrongcontributionforq τ. BNN MLP DE
We apply DeepLIFT (Shrikumar et al., 2017) and 0 0 0
SHAP (Lundberg & Lee, 2017) to each trained f in or-
θ 20
− 20 20
dertodeterminewhetherf θ learnedtheunderlyingsystem − −
dynamicsorsimplyimitatedthetimeseries {q
τ
}ττm =ax 0. For 0 50 τ0 1000 0 50 τ0 1000 0 50 τ0 1000
example,inforcingscenarioswiththeonlyforcingbeing
0 0 0
F , f should not make a prediction qˆ = f (...) such
s θ τ θ
t sh ma at lt lh .e Thco isrr ie ss bp eo cn ad ui sn eg sa at lt ir ni ib tu yt dio ifn fe| rA en(qˆ cτ e, s∆ ,inS d) u| cf eo dr b∆ yS thi es −20 −20 −20
0 500 1000 0 500 1000 0 500 1000
freshwaterforcing,drivetheAMOC. WechoseDeepLIFT τ τ τ
in combination with SHAP due to their relative recency Ground Truth Prediction
amongcomparablemethods(Selvarajuetal.,2016;Ribeiro
Figure6.Predictiveperformancefortheconsideredarchitectures
etal.,2016)andcontinuedimpactfulness(Lietal.,2021;
using physics-informed (PI; first row) and autoregressive (AR;
Sixtetal.,2020;Zhouetal.,2023;Cakirogluetal.,2024;
secondrow)featuresunderF .
2
Dewi et al., 2023). Besides this, we aim to use different
attributionperspectives;samplingfeaturesubsetsaswellas
5
)vS(q
)vS(q
)vS(COMA
)vS(COMA
)vS(COMA
)vS(q
)vS(q
)vS(q
)vS(qTheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
2.6.DetectingNetworkUnderstandingvs.Imitation 3.2.Results
Onequestionweaddressiswhetherf learnedorimitated Here,weonlyincludetheDeepLIFTattributionplotswhile
θ
thephysicsofq. Althoughthereexistsnoformalmethodof the SHAP plots are located in Appendix B due to space
identifyingf ’sunderstandingofphysics,wecaninfersome constraintsandtheirsimilaritytotheDeepLIFTattributions.
θ
physicalunderstandingandimitationbyidentifyingcases Theresultingground-truth-predictionplotsafter120epochs
inwhichaplausiblephysicalunderstandingwaslearnedvs. oftrainingareplottedinFigures3,6,10,13,15and18with
when spurious correlations were learned. Mainly, if ∆S DeepLIFTattributionheatmapsbeingplottedinFigures4,
orS receivehighabsoluteattributionscores,wecaninfer 8, 12, 14, 17and20. Averticaldashedlineindicatesthe
i
learnedphysicsaswellasanattributionpatternassigning train/test split (70% / 30%). We supply plots of the bias
S ascorewithpolaritya 1,1 andS ascorewith qˆ q for , , , inFigures7,11,16and19.
1 2 τ τ 2 3 5 6
∈ {− } − {F F F F }
polarity a. This would concur with natural AMOC dy- WeconductadditionaltuningexperimentsontheBNNin
−
namics;differencesinsalinitycausedifferencesindensity AppendixE.
whichthendriveq.
InFigures3and4,weplotthepredictionsandgroundtruth
withuncertaintybandsunder . Immediately,wenotice
1
MLP DE thattheBNNarchitecture resuF lts inhighlyuncertainpre-
0.5
dictionswithbothphysics-informed(PI)andautoregressive
0 (AR) features. This is not the case for the MLP and DE,
0.0
thelatterofwhichrecoversthebestapproximationforthe
5 validation data. In the physics-informed case, we notice
0.5 −
− 0 1000 0 1000 slight uncertainty where the MLP is producing divergent
τ τ predictions. Autoregressively, we notice a small spike in
0.5
the predicted AMOC (compare bottom row of Figure 3)
0.0 0 thatisdampenedusingaDE. Thisislikelyindicativeof
afailedtippingpointcapturesincethesespikesoccurjust
0.5
− 5 around the recovery of the only tipping point present in
−
0 1000 0 1000 thedata. Theattributionmapsunderphysics-informedfea-
τ τ turesarefairlysimilaruptoasignflipintheattributions,
Ground Truth Prediction which comes from different random initializations of the
models. Wheretherearephysicallyplausibleattributions
Figure7.Bias(qˆ −q )fortheMLPandDEarchitecturesunder
τ τ inthephysics-informedcase,theautoregressivediffersig-
F .
2
nificantly between the BNN and the dense architectures;
whilethedensearchitecturesprimarilyusethefeaturesub-
set τ 3,...,τ 1 ,theBNNappearstopickupspurious
3.Experiments { − − }
correlationseveninasimplelinearforcingscenario,indi-
3.1.ExperimentalDesign cating that it likely did not develop a plausible physical
understanding.
We aim to understand two perspectives: how does the
Concerningpredictiveperformanceunder (seeFigure6),
AMOCevolveaccordingtothephysicalrulesetproposed 2
F
wehavetheBNNfailingtoaccuratelyarticulateallofthetip-
inSection2.2vs.autoregressivelywhereweonlyusemea-
pingpointsunderstationarity(physics-informedandautore-
sured data that is sampled from an externally observable
gressively). Instead,itsmoothlyapproximatesbreakdown
physical process? On the macro level, we have the dif-
ferencebetweenalinearboxmodel(ρlin)andanonlinear andrecoveryphaseswithalargeamountofuncertainty.The
one(ρEOS-80)whichyieldsamoreaccuratebutdifficult-to- MLPandDEperformremarkablywellusingbothPIand
ARdataandhaveseeminglyequalperformance. However,
predictAMOCrepresentation.
fromFigure7itbecomesclearthattheDEperformsbetter
In Table 1, we list the combinations of forcings we use duetoasmallerandsmootherbiaswhichalsoreflectsinthe
in our experiments. For the standard box model we set attributions(Figure8);smoothertransitionsin∆S aswell
ρ = ρlin and include a linear and two sinusoidal forcings as τ 3,...,τ 1 .
withonenonstationaryforcing.Fortheextendedboxmodel, { − − }
Under , we notice an interesting failure to capture an
weconsiderthesameforcingsinconjunctionwithF and 3
t F
ρ=ρEOS-80.Allforcingswiththeircorresponding∆ S,T out-of-sample tipping point breakdown and its recovery
{ } (comparetoprowofFigure10)withPIdata. Interestingly,
and S ,T includingtheresultingAMOCareplottedin
i i
{ } this is consistent among all three architectures and thus
Figures5and9,respectively. Wetrainallmodelsfor120
raisesthequestionofwhetherasystemundernonstationary
epochsusingtheAdamoptimizer(Kingma&Ba,2014).
6
saiB
saiB
saiB
saiBTheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
BNN MLP DE
21.03 26.95 26.1
µ µ µ
Fs Fs Fs
∆S ∆S ∆S
0 500 −21.03 0 500 −26.95 0 500 −26.1
τ τ τ
7.44 22.72 20.92
0 5 τ00 −7.44 0 5 τ00 −22.72 0 5 τ00 −20.92
Figure8.DeepLIFTattributionmapsfortheconsideredarchitecturesusingphysics-informed(PI;toprow)andautoregressive(AR;
bottomrow)featuresunderF .
2
ForcingSetup Forcing Variables q
20
10
50
0 10
4
F 10
− 0 0
0 100 0 100 0 100
τ(kiloyears) τ(kiloyears) τ(kiloyears)
20
10
50
0 10
5
F 10
− 0 0
0 100 0 100 0 100
τ(kiloyears) τ(kiloyears) τ(kiloyears)
10 20
50
0 10
6
F 10
− 0 0
0 100 0 100 0 100
τ(kiloyears) τ(kiloyears) τ(kiloyears)
Figure9. Forcingtrajectory,physicalvariablesandAMOCforF ,F andF .
4 5 6
forcing is too complex for neural networks to represent. toprow)orisshifted(seeFigure13,bottomrow). Thisis
However,theDEallowsforaslightamountofuncertainty linkedtoalargeamountofspuriouslypickedupfeatures
inthisparticulartippingpoint. IntheARcase, theBNN (seeFigure14,firstrow)withPIandARdata. Inparticular,
generatesprofuselyuncertainoutputsduringtherecovery theMLPandDEapproximateqwellunder withasignif-
4
F
of a tipping point and displays a consistent positive shift icantlysmallernumberofattributedfeatures(seeFigure14),
intheAMOC. PerformancebetweentheMLPandDeep whichisalikelyindicationthatthesearchitectureslearned
Ensembleappearsequaluptoabias(seeFigure11)whichis thephysicsoftheAMOC.
smoothedwithPIdatabuttendstobelargerthantheMLP’s
withARdata. Curiously,theBNNleavesoutrangesofAR
features(compareFigure12,bottomrow)initsprediction
whichcouldindicatethatitsimplylearnedtoapproximate
a rough estimate of q while leaving out certain features.
Thishypothesisarisesduetothedensearchitectureshaving
mostoftheirattributionscoresdistributedamongthefeature
range τ 2,τ 1 ,whileattaininggoodgeneralization
{ − − }
abilities.
Inthemorecomplicated , whichrequiressignificantly
4
F
moreinputvariables,theBNNarchitecturetendstoproduce
aversionoftheAMOCthateitherdiverges(seeFigure13,
7
µ1
τ,...,01
τ
−
−
µ1
τ,...,01
τ
−
−
µ1
τ,...,01
τ
−
−
)vS(COMA
)vS(COMA
)vS(COMATheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
BNN MLP DE functionduringrecoveryperiods. Thisispairedwithhigh
0 0 0 uncertaintiesduringthetippingpointsforARdata(com-
pareFigure15). ItislikelythatthisstemsfromtheBNN
20
− −20 −20 pickingupadditionalspuriouscorrelations(seeFigure17,
0 500 1000 0 500 1000 0 500 1000 leftcolumn)whichthedensearchitecturesdonotpickup.
τ τ τ
Interestingly,itappearsthatsalinityisthemaindriverofthe
0 0 0 AMOCinhighlynonlinearandextremeclimatescenarios
(compareFigure17,row2and3). IncontrasttotheBNN
−20 −20 −20 architecture,theMLPandDEarchitecturesaregoodathan-
0 5 τ00 1000 0 5 τ00 1000 0 5 τ00 1000 dlingbothPIandARdata,withtheensemble’sbiastending
Ground Truth Prediction towardszero(seeFigure16). Withrespectto 6,aslightly
F
misaligned tipping point in the validation set is observed
Figure10.Predictiveperformancefortheconsideredarchitectures (see Figure 18, top row), although it is dampened by the
using physics-informed (PI; first row) and autoregressive (AR;
Ensemblearchitecture. WithARdata,theBNNappearsto
secondrow)featuresunderF .
3 havelearnedaverticallynegativelyshiftedversionofthe
AMOC(seeFigure18,bottomrow)thatisnotlearnedby
MLP DE
theotherarchitectures. Curiously,themisalignedtipping
0 10 pointisnotrevealedbytheattributionmaps(Figure20). It
is,however,shownthattheBNNobtainedcorrelationsfrom
0 anumberoffeatureswhichdoesnotseemtobenecessary
2 tolearnanaccuratephysicalunderstandingoftheAMOC.
−
0 1000 0 1000 A further observation is that the bias for both MLP and
τ τ
ensembleisthesameinthisforcingscenario.
0
10
BNN MLP DE
0
2 53.5 89.9 90.5
− µ µ µ
0
τ
Ground1 T0 r0 ut0
h
Predictio− n10 0
τ
1000 F
S
ST TF s 2
1 2
1t
0 50 τ0 −53.5
F
S
ST TF s 2
1 2
1t
0 50 τ0 −89.9
F
S
ST TF s 2
1 2
1t
0 50 τ0 −90.5
11.93 78 79.8
Figure11.Bias(qˆ −q )fortheMLPandDEarchitecturesunder τ τ
F . 3
MLP DE
0 5 τ00 −11.93 0 5 τ00 −78 0 5 τ00 −79.8
2.5
0 Figure17.DeepLIFTattributionmapsfortheconsideredarchitec-
turesusingphysics-informed(PI;toprow)andautoregressive(AR;
0.0
bottomrow)featuresunderF .
5
2.5 10
− 0 1000 − 0 1000
τ τ
We see that adjusting the prior standard deviation (Ap-
2.5
pendix E) of the BNN leads to a diverse set of outcomes
0
withrespecttopredictiveperformanceandexplainability.
0.0
Forexample,in (seeFigure27)withPIdata,noneofthe
1
F
10 valuesofσ canproduceveryaccuratepredictions. Patho-
0 1000 − 0 1000 logicalcasesinwhichthepredictionisnear-constantshow
τ τ
almost vanishing attributions (Figure 28 (e, f, o, p, q, r)).
Ground Truth Prediction
Interestingly,theamountoffailurecaseswithnear-constant
predictions when varying σ is equal across equal density
Figure16.Bias(qˆ −q )fortheMLPandDEarchitecturesunder
τ τ
F 5. approximations;in F1, F2 and F3,thereareexactlythree
such cases (compare Figures 27, 29 and 31) while in ,
4
F
and thereareexactlytwo(compareFigures33,35
5 6
F F
Under ,theBNNarchitecturesignificantlysimplifiesthe and37). Theseoccurforverylowvaluesofσinwhichthe
5
F
AMOC and tends to approximate it by a nearly constant BNNisunabletosampleappropriately.
8
)vS(q
)vS(q
saiB
saiB
saiB
saiB
)vS(q
)vS(q
saiB
saiB
saiB
saiB
)vS(q
)vS(q
µ1
τ,...,01
τ
−
−
µ1
τ,...,01
τ
−
−
µ1
τ,...,01
τ
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
BNN MLP DE
27.87 28.07 26.09
µ µ µ
Fs Fs Fs
∆S ∆S ∆S
27.87 28.07 26.09
0 500 − 0 500 − 0 500 −
τ τ τ
5.02 22.31 20.96
0 500 −5.02 0 500 −22.31 0 500 −20.96
τ τ τ
Figure12.DeepLIFTattributionmapsfortheconsideredarchitecturesusingphysics-informed(PI;toprow)andautoregressive(AR;
bottomrow)featuresunderF .
3
BNN MLP DE BNN MLP DE
53.8 65 63.5
µ µ µ
50
0 0 500 1000
50
0 0 500 1000
50
0 0 500 1000
F
S
ST TF s 2
1 2
1t
0 500 −53.8
F
S
ST TF s 2
1 2
1t
0 500 −65
F
S
ST TF s 2
1 2
1t
0 500 −63.5
τ τ τ τ τ τ
75 75 13.37 37.2 32.54
75
50 50 50
25 25 25
0 5 τ00 1000 0 5 τ00 1000 0 5 τ00 1000 0 5 τ00 −13.37 0 5 τ00 −37.2 0 5 τ00 −32.54
Ground Truth Prediction
Figure14.DeepLIFTattributionmapsfortheconsideredarchitec-
Figure13.Predictiveperformancefortheconsideredarchitectures turesusingphysics-informed(PI;toprow)andautoregressive(AR;
using physics-informed (PI; first row) and autoregressive (AR; bottomrow)featuresunderF 4.
secondrow)featuresunderF .
4
learntheAMOCphysics.Suchspuriouscorrelationsarenot
4.Conclusion&FutureWork seeninthedensearchitectures,leadingustotheconclusion
thattheselearnedasensiblerepresentationoftheAMOC’s
In this paper, we have addressed the question of whether physics.Apartfromthis,theBNNappearstoproduceacon-
neuralnetworksareabletolearntheunderlyingphysical sistentlynegativeshiftinthey-axisforautoregressivedata
system of the AMOC or entirely derive predictive skill whichisagainnotobservedintheMLPandDE. Ingeneral,
from “unphysical” or spurious correlations. In both the bothdensearchitectures(MLP&DE)generalizebetter,pos-
physics-informedandautoregressiveapproach,theBNNar- siblyduetothebetterattractorspaceoptimizationwhich
chitectureisunabletoapproximatetheAMOCsufficiently isseenespeciallyintheDE. Botharchitectures’abilityto
well(compareFigures3,6,10,13,15and18). Itisunlikely performwellout-of-sampleis,otherthanintheBNNs,an-
thattheseresultsmakeanabsolutestatementaboutthecapa- otherplausibleindicatorthattheyretainedAMOCphysics
bilitiesofBNNsbutrathertellusthattheyaremoresensitive during training. Importantly, due to the spiking behavior
tocarefulhyperparametertuningthanotherarchitectures. nearimminentbreakdownandrecoveryphasesinARdata,
AsshowninAppendixE,theBNNisabletocapturethe weraiseconcernaboutthecatastrophicpredictionsmade
AMOCbutonlyaftertuninganotherparameter. Especially usingthemethodologiesofDitlevsen&Ditlevsen(2023)
its imposed latent space regularization could be a crucial andemphasizethatourapproachmakesfewerassumptions
factorwithrespecttoitspredictiveperformance. Usingthe thatwouldbiasaneffectiveAMOCforecast. Inthefuture,
BNNarchitectureautoregressivelyleadstoitpickingupspu- weseetheneedforexperimentsonmoreforcingscenarios,
riouscorrelationswhichisunwantedandindicatesfailureto especiallythosethatrepresentcurrentgreenhousegasemis-
9
)vS(q
)vS(q
µ1
τ,...,01
τ
−
−
)vS(q
)vS(q
)vS(q
)vS(q
µ1
τ,...,01
τ
−
−
µ1
τ,...,01
τ
−
−
µ1
τ,...,01
τ
−
−
µ1
τ,...,01
τ
−
−
µ1
τ,...,01
τ
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
BNN MLP DE
100
50 50
50
0 0 0
0 500 1000 0 500 1000 0 500 1000
τ τ τ
100
75 75
MLP DE
50 50 50
2
25 25 0
0 500 1000 0 500 1000 0 500 1000
τ τ τ 5
Ground Truth Prediction 0 −
0 1000 0 1000
Figure15.Predictiveperformancefortheconsideredarchitectures τ τ
using physics-informed (PI; first row) and autoregressive (AR; 2
secondrow)featuresunderF .
5 0
BNN MLP DE 5
100 0 −
50 50 0 1000 0 1000
50
τ τ
0 0 0 Ground Truth Prediction
0 500 1000 0 500 1000 0 500 1000
τ τ τ
75
75 75 Figure19.Bias(qˆ τ−q τ)fortheMLPandDEarchitecturesunder
F . 6
50 50 50
25
25 25
0 500 1000 0 500 1000 0 500 1000
τ τ τ
Ground Truth Prediction
Figure18.Predictiveperformancefortheconsideredarchitectures
using physics-informed (PI; first row) and autoregressive (AR;
secondrow)featuresunderF .
6
sionsandfreshwaterforcingintheNorthAtlanticOcean.
Besidesthis,differentneuralnetworkarchitectureslikethe
FNOorGraphNeuralNetworks(GNNs;(Scarsellietal.,
2008))couldbeappliedtopredictq,however,withoutthe BNN MLP DE
readyabilitytoapplyXAI. 60.3 152 141.7
µ µ µ
Acknowledgements F S ST TF s 2 1 2 1t
0 500 −60.3
F S ST TF s 2 1 2 1t
0 500 −152
F S ST TF s 2 1 2 1t
0 500 −141.7
WethankMarianaClareandRedouaneLguensatfortheir τ τ τ
13.83 74.3 71
helpfulcomments.
0 5 τ00 −13.83 0 5 τ00 −74.3 0 5 τ00 −71
Figure20.DeepLIFTattributionmapsfortheconsideredarchitec-
turesusingphysics-informed(PI;toprow)andautoregressive(AR;
bottomrow)featuresunderF .
6
10
)vS(q
)vS(q
)vS(q
)vS(q
)vS(q
)vS(q
)vS(q
)vS(q
)vS(q
)vS(q
)vS(q
)vS(q
µ1
τ,...,01
τ
saiB
saiB
−
−
µ1
τ,...,01
τ
−
−
saiB
saiB
µ1
τ,...,01
τ
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
References Hazeleger,W.,Wouters,B.,vanOldenborgh,G.J.,Corti,
S., Palmer, T., Smith, D., Dunstone, N., Kro¨ger, J.,
Bao, Y., Velni, J. M., and Shahbakhti, M. Epistemic un-
Pohlmann,H.,andvonStorch,J.-S. Predictingmultiyear
certaintyquantificationinstate-spacelpvmodelidenti-
northatlanticoceanvariability. JournalofGeophysical
ficationusingbayesianneuralnetworks. IEEEControl
Research: Oceans,118(3):1087–1098,2013.
SystemsLetters,5(2):719–724,2020.
Irrgang, C., Boers, N., Sonnewald, M., Barnes, E. A.,
Bire, S., Lu¨tjens, B., Azizzadenesheli, K., Anandkumar,
Kadow,C.,Staneva,J.,andSaynisch-Wagner,J. Towards
A.,andHill,C.N. Oceanemulationwithfourierneural
neural earth system modelling by integrating artificial
operators: Doublegyre. AuthoreaPreprints,2023.
intelligence in earth system science. Nature Machine
Intelligence,3(8):667–674,2021.
Bittig, H. C., Steinhoff, T., Claustre, H., Fiedler, B.,
Williams, N. L., Sauze`de, R., Ko¨rtzinger, A., and Gat-
Jackson,L.C.,Hewitt,H.T.,Bruciaferri,D.,Calvert,D.,
tuso,J.-P. Analternativetostaticclimatologies: Robust
Graham,T.,Guiavarc’h,C.,Menary,M.B.,New,A.L.,
estimationofopenoceanco2variablesandnutrientcon-
Roberts,M.,andStorkey,D. Challengessimulatingthe
centrationsfromt,s,ando2datausingbayesianneural amocinclimatemodels. PhilosophicalTransactionsof
networks. FrontiersinMarineScience,5:328,2018. theRoyalSocietyA,381(2262):20220187,2023.
Cakiroglu, C., Demir, S., Ozdemir, M. H., Aylak, B. L., Jahanbakht,M.,Xiang,W.,andAzghadi,M.R. Seasurface
Sariisik,G.,andAbualigah,L. Data-driveninterpretable temperatureforecastingwithensembleofstackeddeep
ensemblelearningmethodsforthepredictionofwindtur- neuralnetworks. IEEEGeoscienceandRemoteSensing
binepowerincorporatingshapanalysis. ExpertSystems Letters,19:1–5,2021.
withApplications,237:121464,2024.
Jones, N. How machine learning could help to improve
Cheng,W.,Chiang,J.C.,andZhang,D.Atlanticmeridional climateforecasts. Nature,548(7668),2017.
overturningcirculation(amoc)incmip5models:Rcpand
Jospin, L. V., Laga, H., Boussaid, F., Buntine, W., and
historicalsimulations. JournalofClimate,26(18):7187–
Bennamoun,M. Hands-onbayesianneuralnetworks—a
7197,2013.
tutorial for deep learning users. IEEE Computational
Cho,D.,Yoo,C.,Son,B.,Im,J.,Yoon,D.,andCha,D.-H. IntelligenceMagazine,17(2):29–48,2022.
A novel ensemble learning for post-processing of nwp
Juan, N.P., Matutano, C., andValdecantos, V.N. Uncer-
model’snext-daymaximumairtemperatureforecastin
tainties in the application of artificial neural networks
summerusingdeeplearningandstatisticalapproaches.
inoceanengineering. OceanEngineering,284:115193,
WeatherandClimateExtremes,35:100410,2022.
2023.
Clare,M.C.andPiggott,M.D. Bayesianneuralnetworks
Kingma,D.P.andBa,J. Adam: Amethodforstochastic
for the probabilistic forecasting of wind direction and
optimization. arXivpreprintarXiv:1412.6980,2014.
speedusingoceandata. TrendsinRenewableEnergies
Offshore,pp.533–540,2022. Lenton,T.M.,Held,H.,Kriegler,E.,Hall,J.W.,Lucht,W.,
Rahmstorf,S.,andSchellnhuber,H.J. Tippingelements
Clare, M. C., Sonnewald, M., Lguensat, R., Deshayes,
intheearth’sclimatesystem.ProceedingsoftheNational
J., and Balaji, V. Explainable artificial intelligence for
AcademyofSciences,105(6):1786–1793,2008.
bayesianneuralnetworks:towardtrustworthypredictions
of ocean dynamics. Journal of Advances in Modeling Li, J., Zhang, C., Zhou, J. T., Fu, H., Xia, S., and Hu, Q.
EarthSystems,14(11):e2022MS003162,2022. Deep-lift: Deeplabel-specificfeaturelearningforimage
annotation. IEEE Transactions on Cybernetics, 52(8):
Dewi, C., CHEN, R.-C., Yu, H., and JIANG, X. Xai for 7732–7741,2021.
image captioning using shap. Journal of Information
Science&Engineering,39(4),2023. Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhat-
tacharya, K., Stuart, A., and Anandkumar, A. Fourier
Ditlevsen,P.andDitlevsen,S. Warningofaforthcoming neuraloperatorforparametricpartialdifferentialequa-
collapseoftheatlanticmeridionaloverturningcirculation. tions. arXivpreprintarXiv:2010.08895,2020.
NatureCommunications,14(1):1–12,2023.
Liu, W., Xie, S.-P., Liu, Z., and Zhu, J. Overlooked pos-
Fofonoff,N. Physicalpropertiesofseawater:Anewsalinity sibility of a collapsed atlantic meridional overturning
scaleandequationofstateforseawater. JournalofGeo- circulationinwarmingclimate. ScienceAdvances,3(1):
physicalResearch: Oceans,90(C2):3332–3342,1985. e1601666,2017.
11TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
Lobelle, D., Beaulieu, C., Livina, V., Sevellec, F., and Salman, A. G., Kanigoro, B., and Heryadi, Y. Weather
Frajka-Williams, E. Detectability of an amoc decline forecastingusingdeeplearningtechniques. In2015in-
incurrentandprojectedclimatechanges. Geophysical ternational conference on advanced computer science
ResearchLetters,47(20):e2020GL089974,2020. andinformationsystems(ICACSIS),pp.281–285.IEEE,
2015.
Lundberg,S.M.andLee,S.-I. Aunifiedapproachtointer-
pretingmodelpredictions. AdvancesinNeuralInforma- Scarselli,F.,Gori,M.,Tsoi,A.C.,Hagenbuchner,M.,and
tionProcessingSystems,30,2017. Monfardini,G. Thegraphneuralnetworkmodel. IEEE
TransactionsonNeuralNetworks,20(1):61–80,2008.
Mahajan,S.,Zhang,R.,Delworth,T.L.,Zhang,S.,Rosati,
A. J., and Chang, Y.-S. Predicting atlantic meridional
Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R.,
overturningcirculation(amoc)variationsusingsubsur-
Parikh,D.,andBatra,D. Grad-cam: Visualexplanations
faceandsurfacefingerprints. DeepSeaResearchPartII:
fromdeepnetworksviagradient-basedlocalization.arXiv
TopicalStudiesinOceanography,58(17-18):1895–1903,
preprintarXiv:1610.02391,2016.
2011.
Shrikumar, A., Greenside, P., and Kundaje, A. Learning
Matei, D., Baehr, J., Jungclaus, J. H., Haak, H., Mu¨ller,
importantfeaturesthroughpropagatingactivationdiffer-
W.A.,andMarotzke,J. Multiyearpredictionofmonthly
ences. InInternationalConferenceonMachineLearning,
meanatlanticmeridionaloverturningcirculationat26.5
pp.3145–3153.PMLR,2017.
n. Science,335(6064):76–79,2012.
Singh,N.,Chaturvedi,S.,andAkhter,S. Weatherforecast-
McCarthy, G. D., Smeed, D. A., Johns, W. E., Frajka-
ingusingmachinelearningalgorithm. In2019Interna-
Williams, E., Moat, B.I., Rayner, D., Baringer, M.O.,
tionalConferenceonSignalProcessingandCommunica-
Meinen,C.S.,Collins,J.,andBryden,H.L. Measuring
tion(ICSC),pp.171–174.IEEE,2019.
the atlantic meridional overturning circulation at 26 n.
ProgressinOceanography,130:91–111,2015.
Sixt,L.,Granz,M.,andLandgraf,T. Whenexplanations
Menary, M. B., Roberts, C. D., Palmer, M. D., Halloran, lie: Whymanymodifiedbpattributionsfail. InInterna-
P. R., Jackson, L., Wood, R. A., Mu¨ller, W. A., Matei, tionalConferenceonMachineLearning,pp.9046–9057.
D.,andLee,S.-K. Mechanismsofaerosol-forcedamoc PMLR,2020.
variability in a state of the art climate model. Journal
Sonnewald,M.andLguensat,R. Revealingtheimpactof
of Geophysical Research: Oceans, 118(4):2087–2096,
globalheatingonnorthatlanticcirculationusingtranspar-
2013.
entmachinelearning. JournalofAdvancesinModeling
Pathak, J., Subramanian, S., Harrington, P., Raja, S., EarthSystems,13(8):e2021MS002496,2021.
Chattopadhyay, A., Mardani, M., Kurth, T., Hall, D.,
Sonnewald,M.,Lguensat,R.,Jones,D.C.,Dueben,P.D.,
Li, Z., Azizzadenesheli, K., et al. FourCastNet: A
Brajard,J.,andBalaji,V. Bridgingobservations,theory
global data-driven high-resolution weather model us-
ing adaptive fourier neural operators. arXiv preprint and numerical simulation of the ocean using machine
arXiv:2202.11214,2022. learning. EnvironmentalResearchLetters,16(7):073008,
2021.
Qi, J., Zhang, L., Yin, B., Li, D., Xie, B., and Sun, G.
Advancingoceansubsurfacethermalstructureestimation Springenberg, J. T., Klein, A., Falkner, S., and Hutter, F.
inthepacificocean: Amulti-modelensemblemachine Bayesianoptimizationwithrobustbayesianneuralnet-
learningapproach.DynamicsofAtmospheresandOceans, works. Advancesinneuralinformationprocessingsys-
104:101403,2023. tems,29,2016.
Rasouli,K.,Nasri,B.R.,Soleymani,A.,Mahmood,T.H., Stommel, H. Thermohaline convection with two stable
Hori,M.,andHaghighi,A.T. Forecastofstreamflowsto regimesofflow. Tellus,13(2):224–230,1961.
thearcticoceanbyabayesianneuralnetworkmodelwith
snowcoverandclimateinputs. HydrologyResearch,51 Swingedouw,D.,Mignot,J.,Labetoulle,S.,Guilyardi,E.,
(3):541–561,2020. and Madec, G. Initialisation and predictability of the
amocoverthelast50yearsinaclimatemodel. Climate
Ribeiro, M. T., Singh, S., and Guestrin, C. ”why should Dynamics,40(9-10):2381–2399,2013.
itrustyou?”explainingthepredictionsofanyclassifier.
InProceedingsofthe22ndACMSIGKDDinternational Thodberg, H. H. A review of bayesian neural networks
conferenceonknowledgediscoveryanddatamining,pp. withanapplicationtonearinfraredspectroscopy. IEEE
1135–1144,2016. transactionsonNeuralNetworks,7(1):56–72,1996.
12TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
Thomas,M.D.andFedorov,A.V.Mechanismsandimpacts
of a partial amoc recovery under enhanced freshwater
forcing. GeophysicalResearchLetters,46(6):3308–3316,
2019.
Tziperman, E. Global Warming Science: A Quantitative
Introduction to Climate Change and its Consequences.
PrincetonUniversityPress,2022.
VanNes, E.H., Arani, B.M., Staal, A., vanderBolt, B.,
Flores,B.M.,Bathiany,S.,andScheffer,M.Whatdoyou
mean,’tippingpoint’? TrendsinEcology&Evolution,
31(12):902–904,2016.
Wen,N.,Frankignoul,C.,andGastineau,G. Activeamoc–
naocouplingintheipsl-cm5a-mrclimatemodel. Climate
Dynamics,47:2105–2119,2016.
Yik, W., Sonnewald, M., Clare, M. C., and Lguensat, R.
Southern ocean dynamics under climate change: New
knowledge through physics-guided machine learning.
arXivpreprintarXiv:2310.13916,2023.
Zhang, X. and Garikipati, K. Bayesian neural networks
forweaksolutionofpdeswithuncertaintyquantification.
arXivpreprintarXiv:2101.04879,2021.
Zhou, X., Liu, C., Zhai, L., Jia, Z., Guan, C., and Liu,
Y. Interpretableandrobustaiineegsystems: Asurvey.
arXivpreprintarXiv:2304.10755,2023.
13TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
A.BoxModelParameters
WeshowtheparameterconfigurationfortheStommelboxmodelinthispaper.
Parameter Value
S 35.0ppt
0
T 24.0℃
0
S 12.0ppt
1
S 20.0ppt
2
T 1.0℃
1
T 10.0℃
2
A(area) (5 107)m2
k × 1 1010
Depth 4×000m
α 0.2
β 0.8
Yearsofprediction 150,000
Table2. Parametersfortheboxmodelusedinouranalysis.
B.SHAPAttributions
WepresentadditionalattributionplotsgivenbytheSHAPalgorithm. TheseconcurnarrowlywiththeDeepLIFTattribution
plots.
BNN MLP DE
22.03 25 24.18
µ µ µ
F F F
s s s
∆S ∆S ∆S
22.03 25 24.18
0 500 − 0 500 − 0 500 −
τ τ τ
5.29 19.17 17.31
0 500 5.29 0 500 19.17 0 500 17.31
− − −
τ τ τ
Figure21.SHAPattributionmapsfortheconsideredarchitecturesusingphysics-informed(PI;toprow)andautoregressive(AR;bottom
row)featuresunderF .
1
14
µ
1
τ,...,01
τ
−
−
µ
1
τ,...,01
τ
−
−
µ
1
τ,...,01
τ
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
BNN MLP DE
20.6 27.15 25.81
µ µ µ
F F F
s s s
∆S ∆S ∆S
20.6 27.15 25.81
0 500 − 0 500 − 0 500 −
τ τ τ
9.41 21.43 22.08
0 500 −9.41 0 500 −21.43 0 500 −22.08
τ τ τ
Figure22.SHAPattributionmapsfortheconsideredarchitecturesusingphysics-informed(PI;toprow)andautoregressive(AR;bottom
row)featuresunderF .
2
BNN MLP DE
25.9 25.4 24.37
µ µ µ
F F F
s s s
∆S ∆S ∆S
25.9 25.4 24.37
0 500 − 0 500 − 0 500 −
τ τ τ
6.47 22.34 21.52
0 500 −6.47 0 500 −22.34 0 500 −21.52
τ τ τ
Figure23.SHAPattributionmapsfortheconsideredarchitecturesusingphysics-informed(PI;toprow)andautoregressive(AR;bottom
row)featuresunderF .
3
15
µ
1
τ,...,01
τ
µ
1
τ,...,01
τ
−
−
−
−
µ
1
τ,...,01
τ
µ
1
τ,...,01
τ
−
−
−
−
µ
1
τ,...,01
τ
µ
1
τ,...,01
τ
−
−
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
BNN MLP DE
49.71 61.7 61.3
µ µ µ
F F F
t t t
F F F
s s s
T T T
2 2 2
T T T
1 1 1
S S S
2 2 2
S S S
1 49.71 1 61.7 1 61.3
0 500 − 0 500 − 0 500 −
τ τ τ
9.94 21.44 19.58
0 500 −9.94 0 500 −21.44 0 500 −19.58
τ τ τ
Figure24.SHAPattributionmapsfortheconsideredarchitecturesusingphysics-informed(PI;toprow)andautoregressive(AR;bottom
row)featuresunderF .
4
BNN MLP DE
47.56 72.2 72.2
µ µ µ
F F F
t t t
F F F
s s s
T T T
2 2 2
T T T
1 1 1
S S S
2 2 2
S S S
1 47.56 1 72.2 1 72.2
0 500 − 0 500 − 0 500 −
τ τ τ
14.32 45.33 43.6
0 500 −14.32 0 500 −45.33 0 500 −43.6
τ τ τ
Figure25.SHAPattributionmapsfortheconsideredarchitecturesusingphysics-informed(PI;toprow)andautoregressive(AR;bottom
row)featuresunderF .
5
16
µ
1
τ,...,01
τ
µ
1
τ,...,01
τ
−
−
−
−
µ
1
τ,...,01
τ
µ
1
τ,...,01
τ
−
−
−
−
µ
1
τ,...,01
τ
µ
1
τ,...,01
τ
−
−
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
BNN MLP DE
54.6 80 77.8
µ µ µ
F F F
t t t
F F F
s s s
T T T
2 2 2
T T T
1 1 1
S S S
2 2 2
S S S
1 54.6 1 80 1 77.8
0 500 − 0 500 − 0 500 −
τ τ τ
9.67 38.5 38.5
9.67 38.5 38.5
0 500 − 0 500 − 0 500 −
τ τ τ
Figure26.SHAPattributionmapsfortheconsideredarchitecturesusingphysics-informed(PI;toprow)andautoregressive(AR;bottom
row)featuresunderF .
6
C.ρlin
InthisAppendix,weincludesupplementaryforcingscenariosusingρlin thatcomplement ,..., fromthemain
1 6
{F F }
experiments. WeincludeaRecurrentNeuralNetwork(RNN)asanadditionalarchitecture.
C.1.ExtendedBoxModel
C.1.1.PERFORMANCE
Scenario Architecture Prediction: PI Prediction: AR
40
20 20
BNN
0 0
0 500 1000 0 500 1000
τ τ
20 20
MLP
0 0
0 500 1000 0 500 1000
τ τ
17
µ
1
τ,...,01
τ
−
−
.niL
: F s
.niL
: F t
µ
1
τ,...,01
τ
−
−
)vS(
q
)vS(
q
)vS(
q
)vS(
q
µ
1
τ,...,01
τ
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
20
20
Deep
0 0
Ensemble
0 500 1000 0 500 1000
τ τ
20
10
RNN —
0
10
− 0 500 1000
τ
20
20
BNN
0 0
0 500 1000 0 500 1000
τ τ
20 20
MLP
0 0
0 500 1000 0 500 1000
τ τ
20 20
Deep
0 0
Ensemble
0 500 1000 0 500 1000
τ τ
20
10
RNN —
0
0 500 1000
τ
20
20
BNN
0 0
0 500 1000 0 500 1000
τ τ
18
)yranoitats(.niS
:
F
)yranoitatsnon(.niS
:
F
s
s
)yranoitats(.niS
:
F
)yranoitatsnon(.niS
:
F
t
t
)vS(
q
)vS(
q
)vS(
q
)vS(
q
)vS(
q
)vS(
q
)vS(q
)vS(
q
)vS(
q
)vS(
q
)vS(q
)vS(
qTheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
20 20
MLP
0 0
0 500 1000 0 500 1000
τ τ
20 20
Deep
0 0
Ensemble
0 500 1000 0 500 1000
τ τ
20
10
RNN —
0
0 500 1000
τ
Table3: PredictiveperformanceusingF ,F andρlin.
s t
C.1.2.EXPLAINABILITY
Scenario Architecture DeepLIFT: PI SHAP: PI DeepLIFT: AR SHAP: AR
14.6 11.28
µ 21.98 µ 16.83
Ft Ft
BNN Fs Fs
∆T 21.98 ∆T 16.83
− −
∆S ∆S
0 50 τ0 0 50 τ0 0 5 τ00 −14.6 0 5 τ00 −11.28
27.03 17.93
µ 23.31 µ 16.76
Ft Ft
MLP Fs Fs
∆T 23.31 ∆T 16.76
− −
∆S ∆S
0 50 τ0 0 50 τ0 0 5 τ00 −27.03 0 5 τ00 −17.93
19
.niL
: F s
.niL
: F t
)vS(
q
)vS(
q
)vS(
q
)vS(
q
)vS(q
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
28.81 18.74
µ 24.3 µ 17.02
Ft Ft
Deep En- Fs Fs
semble ∆T −24.3 ∆T −17.02
∆S ∆S
0 50 τ0 0 50 τ0 0 5 τ00 −28.81 0 5 τ00 −18.74
6.23 13.84
µ µ
RNN — — ττ τ τ τ τ τ τ τ τ −− − − − − − − − −101 2 3 4 5 6 7 8 9 0 500 −6.23 ττττττττττ −−−−−−−−−−10123456789 0 500 −13.84
τ τ
8.39 10.26
µ 19.01 µ 17.62
Ft Ft
BNN Fs Fs
∆T 19.01 ∆T 17.62
− −
∆S ∆S
0 50 τ0 0 50 τ0 0 5 τ00 −8.39 0 5 τ00 −10.26
24.85 16.33
µ 26.73 µ 20.9
Ft Ft
MLP Fs Fs
∆T −26.73 ∆T −20.9
∆S ∆S
0 50 τ0 0 50 τ0 0 5 τ00 −24.85 0 5 τ00 −16.33
26.73 20.9
µ 26.35 µ 20.3
Ft Ft
Deep En- Fs Fs
semble ∆T −26.35 ∆T −20.3
∆S ∆S
0 50 τ0 0 50 τ0 0 5 τ00 −26.73 0 5 τ00 −20.9
15.3 18.67
µ µ
RNN — — ττ τ τ τ τ τ τ τ τ −− − − − − − − − −101 2 3 4 5 6 7 8 9 0 500 −15.3 ττττττττττ −−−−−−−−−−10123456789 0 500 −18.67
τ τ
20
)yranoitats(.niS
:
F
s
)yranoitats(.niS
:
F
t
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
−
−
−
−
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
−
−
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
7.75 8.68
µ 19.83 µ 17.64
Ft Ft
BNN Fs Fs
∆T 19.83 ∆T 17.64
− −
∆S ∆S
0 50 τ0 0 50 τ0 0 500 −7.75 0 500 −8.68
τ τ
25.1 18.64
µ 23.43 µ 20.06
Ft Ft
MLP Fs Fs
∆T 23.43 ∆T 20.06
− −
∆S ∆S
0 50 τ0 0 50 τ0 0 5 τ00 −25.1 0 5 τ00 −18.64
22.74 16.72
µ 23.79 µ 19.87
Ft Ft
Deep En- Fs Fs
semble ∆T 23.79 ∆T 19.87
− −
∆S ∆S
0 50 τ0 0 50 τ0 0 5 τ00 −22.74 0 5 τ00 −16.72
4.32 18.43
µ µ
RNN — — ττ τ τ τ τ τ τ τ τ −− − − − − − − − −101 2 3 4 5 6 7 8 9 0 500 −4.32 ττττττττττ −−−−−−−−−−10123456789 0 500 −18.43
τ τ
Table4: AttributionmapsusingF ,F andρlin.
s t
D.ρEOS-80
InthisAppendix,weincludesupplementaryforcingscenariosusingρEOS-80 thatcomplement ,..., fromthemain
1 6
{F F }
experiments. WeincludeaRecurrentNeuralNetwork(RNN)asanadditionalarchitecture.
D.1.StandardBoxModel
D.1.1.FORCINGSCENARIOS
Scenario Forcing Variables q
21
)yranoitatsnon(.niS
:
F
s
)yranoitatsnon(.niS
:
F
t
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
−
−
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
10
20
0 10 50
F : Linear
s
10 0 0
− 0 100 0 100 0 100
τ (kiloyears) τ (kiloyears) τ (kiloyears)
10
20
0 10 50
F : Sinusoidal(stationary)
s
10 0 0
− 0 100 0 100 0 100
τ (kiloyears) τ (kiloyears) τ (kiloyears)
20
5
0 10 50
F : Sinusoidal(nonstationary)
s 5
−
0 0
0 100 0 100 0 100
τ (kiloyears) τ (kiloyears) τ (kiloyears)
Table5: Overviewofthescenariosandtheirvariablesusingonlyfreshwaterforcing.
D.1.2.PERFORMANCE
Scenario Architecture Prediction: PI Prediction: AR
100
100
50
BNN 50
0
0 500 1000 0 500 1000
τ τ
75
50
MLP 50
0
0 500 1000 0 500 1000
τ τ
75
50
Deep 50
Ensemble
0
0 500 1000 0 500 1000
τ τ
22
raeniL
:
F
s
)vS(
q
)vS(
q
)vS(
q
)vS(
q
)vS(
q
)vS(
q
)vS(COMA
)vS(COMA
)vS(COMATheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
80
60
RNN —
40
0 500 1000
τ
100
100
50
BNN
50
0
0 500 1000 0 500 1000
τ τ
0
75
MLP 50
20
−
0 500 1000 0 500 1000
τ τ
75
50
Deep 50
Ensemble
0
0 500 1000 0 500 1000
τ τ
80
60
RNN —
40
0 500 1000
τ
100 100
50
BNN 50
0
0 500 1000 0 500 1000
τ τ
75
50
MLP 50
0
0 500 1000 0 500 1000
τ τ
23
)yranoitats(ladiosuniS
:
F
)yranoitatsnon(ladiosuniS
:
F
s
s
)vS(
q
)vS(
q
)vS(
q
)vS(
q
)vS(
q
)vS(q
)vS(
q
)vS(
q
)vS(
q
)vS(q
)vS(
q
)vS(
qTheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
75
50
Deep 50
Ensemble
0
0 500 1000 0 500 1000
τ τ
80
60
RNN —
40
0 500 1000
τ
Table6: PredictiveperformanceusingF andρEOS-80.
s
D.1.3.EXPLAINABILITY
Scenario Architecture DeepLIFT: PI SHAP: PI DeepLIFT: AR SHAP: AR
58.4 66.8
µ µ 10.97 11.73
BNN
Fs Fs
S2 S2
S1
0 500
−58.4 S1
0 500
−66.8
0 500 −10.97 0 500 −11.73
τ τ τ τ
85 82.6
µ µ 35.04 18.1
MLP
Fs Fs
S2 S2
S1
0 500
−85 S1
0 500
−82.6
0 500 −35.04 0 500 −18.1
τ τ τ τ
97.8 86.4
µ µ 32.94 19.98
Deep En-
Fs Fs
semble
S2 S2
S1
0 500
−97.8 S1
0 500
−86.4
0 500 −32.94 0 500 −19.98
τ τ τ τ
24
raeniL
:
F
s
)vS(
q
)vS(
q
)vS(q
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
−
−
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
22.71 20.85
µ µ
RNN — —
ττττττττττ −−−−−−−−−−10123456789
0 500
−22.71
ττττττττττ −−−−−−−−−−10123456789
0 500
−20.85
τ τ
54.3 58
µ µ 17.3 15.44
BNN
Fs Fs
S2 S2
S1
0 500
−54.3 S1
0 500
−58
0 500 −17.3 0 500 −15.44
τ τ τ τ
116.6 88.1
µ µ 97.2 47.57
MLP
Fs Fs
S2 S2
S1
0 500
−116.6 S1
0 500
−88.1
0 500 −97.2 0 500 −47.57
τ τ τ τ
122.1 91.9
µ µ 87 45.74
Deep En- Fs Fs
semble
S2 S2
S1
0 50 τ0
−122.1 S1
0 50 τ0
−91.9
0 5 τ00 −87 0 5 τ00 −45.74
46.4 55.4
µ µ
τ 1 τ 1
RNN — — ττ τ τ τ τ τ τ τ −− − − − − − − − −102 3 4 5 6 7 8 9
0 500
−46.4
ττ τ τ τ τ τ τ τ −− − − − − − − − −102 3 4 5 6 7 8 9
0 500
−55.4
τ τ
57 59.2
µ µ 12.6 10.95
BNN
Fs Fs
S2 S2
S1
0 500
−57 S1
0 500
−59.2
0 500 −12.6 0 500 −10.95
τ τ τ τ
25
)yranoitats(ladiosuniS
:
F
)yranoitatsnon(ladiosuniS
:
F
s
s
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
−
−
−
−
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
−
−
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
141.7 91.7
75 39.5
µ µ
MLP Fs Fs
S2 S2
S1
0 50 τ0
−141.7 S1
0 50 τ0
−91.7
0 5 τ00 −75 0 5 τ00 −39.5
145.7 98.3
75.8 40.7
µ µ
Deep En-
Fs Fs
semble
S2 S2
S1
0 500
−145.7 S1
0 500
−98.3
0 500 −75.8 0 500 −40.7
τ τ τ τ
36.47 31.92
µ µ
RNN — —
ττττττττττ −−−−−−−−−−10123456789
0 500
−36.47
ττττττττττ −−−−−−−−−−10123456789
0 500
−31.92
τ τ
Table7: AttributionmapsusingF andρEOS-80.
s
E.BNNPriorStandardDeviation
WeconductadditionalexperimentsusingtheBNNarchitectureandvaryingvaluesofthepriorstandarddeviationσ. Weset
σ =1−2,1−3,1−4,1−5,1−6. Notethatthevalueinthemainexperimentswasσ =0.1.
26
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−
µ1
τ,...,01
τ
µ1
τ,...,01
τ
−
−
−
−TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
Ground Truth Prediction
(a)AR,σ=1−2 (b)AR,σ=1−3 (c)AR,σ=1−6
(d)AR,σ=1−5 (e)AR,σ=1−4 (f)PI,σ=1−2
(g)PI,σ=1−3 (h)PI,σ=1−6 (i)PI,σ=1−5
(j)PI,σ=1−4
Figure27. BNNpredictionsundervaryingσforF .
1
27TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
(a)SHAP,AR,σ=1−2 (b)DeepLIFT,AR,σ=1−2 (c)SHAP,AR,σ=1−3 (d)DeepLIFT,AR,σ=1−3
(f)DeepLIFT,AR,σ=1−6
(g)SHAP,AR,σ=1−5 (h)DeepLIFT,AR,σ=1−5
(e)SHAP,AR,σ=1−6
(k)SHAP,PI,σ=1−2 (l)DeepLIFT,PI,σ=1−2
(i)SHAP,AR,σ=1−4 (j)DeepLIFT,AR,σ=1−4
(m)SHAP,PI,σ=1−3 (n)DeepLIFT,PI,σ=1−3 (o)SHAP,PI,σ=1−6 (p)DeepLIFT,PI,σ=1−6
(q)SHAP,PI,σ=1−5 (r)DeepLIFT,PI,σ=1−5 (s)SHAP,PI,σ=1−4 (t)DeepLIFT,PI,σ=1−4
Figure28. BNNattributionsundervaryingσforF .
1
28TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
Ground Truth Prediction
(a)AR,σ=1−2 (b)AR,σ=1−3 (c)AR,σ=1−6
(d)AR,σ=1−5 (e)AR,σ=1−4 (f)PI,σ=1−2
(g)PI,σ=1−3 (h)PI,σ=1−6 (i)PI,σ=1−5
(j)PI,σ=1−4
Figure29. BNNpredictionsundervaryingσforF .
2
29TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
(a)SHAP,AR,σ=1−2 (b)DeepLIFT,AR,σ=1−2 (c)SHAP,AR,σ=1−3 (d)DeepLIFT,AR,σ=1−3
(g)SHAP,AR,σ=1−5 (h)DeepLIFT,AR,σ=1−5
(e)SHAP,AR,σ=1−6 (f)DeepLIFT,AR,σ=1−6
(k)SHAP,PI,σ=1−2 (l)DeepLIFT,PI,σ=1−2
(i)SHAP,AR,σ=1−4
(j)DeepLIFT,AR,σ=1−4
(m)SHAP,PI,σ=1−3 (n)DeepLIFT,PI,σ=1−3 (o)SHAP,PI,σ=1−6 (p)DeepLIFT,PI,σ=1−6
(q)SHAP,PI,σ=1−5 (r)DeepLIFT,PI,σ=1−5 (s)SHAP,PI,σ=1−4 (t)DeepLIFT,PI,σ=1−4
Figure30. BNNattributionsundervaryingσforF .
2
30TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
Ground Truth Prediction
(a)AR,σ=1−2 (b)AR,σ=1−3 (c)AR,σ=1−6
(d)AR,σ=1−5 (e)AR,σ=1−4 (f)PI,σ=1−2
(g)PI,σ=1−3 (h)PI,σ=1−6 (i)PI,σ=1−5
(j)PI,σ=1−4
Figure31. BNNpredictionsundervaryingσforF .
3
31TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
(a)SHAP,AR,σ=1−2 (b)DeepLIFT,AR,σ=1−2 (c)SHAP,AR,σ=1−3 (d)DeepLIFT,AR,σ=1−3
(e)SHAP,AR,σ=1−6 (f)DeepLIFT,AR,σ=1−6
(g)SHAP,AR,σ=1−5 (h)DeepLIFT,AR,σ=1−5
(k)SHAP,PI,σ=1−2 (l)DeepLIFT,PI,σ=1−2
(j)DeepLIFT,AR,σ=1−4
(i)SHAP,AR,σ=1−4
(m)SHAP,PI,σ=1−3 (n)DeepLIFT,PI,σ=1−3 (o)SHAP,PI,σ=1−6 (p)DeepLIFT,PI,σ=1−6
(q)SHAP,PI,σ=1−5 (r)DeepLIFT,PI,σ=1−5 (s)SHAP,PI,σ=1−4 (t)DeepLIFT,PI,σ=1−4
Figure32. BNNattributionsundervaryingσforF .
3
32TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
Ground Truth Prediction
(a)PI,σ=1−2 (b)PI,σ=1−3 (c)PI,σ=1−6
(d)PI,σ=1−5 (e)PI,σ=1−4 (f)AR,σ=1−2
(g)AR,σ=1−3 (h)AR,σ=1−6 (i)AR,σ=1−5
(j)AR,σ=1−4
Figure33. BNNpredictionsundervaryingσforF .
4
33TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
(a)SHAP,PI,σ=1−2 (c)SHAP,PI,σ=1−3
(b)DeepLIFT,PI,σ=1−2
(d)DeepLIFT,PI,σ=1−3
(e)SHAP,PI,σ=1−6
(f)DeepLIFT,PI,σ=1−6 (g)SHAP,PI,σ=1−5 (h)DeepLIFT,PI,σ=1−5
(i)SHAP,PI,σ=1−4
(j)DeepLIFT,PI,σ=1−4 (k)SHAP,AR,σ=1−2 (l)DeepLIFT,AR,σ=1−2
(m)SHAP,AR,σ=1−3 (n)DeepLIFT,AR,σ=1−3
(o)SHAP,AR,σ=1−6
(p)DeepLIFT,AR,σ=1−6
(q)SHAP,AR,σ=1−5 (r)DeepLIFT,AR,σ=1−5 (s)SHAP,AR,σ=1−4 (t)DeepLIFT,AR,σ=1−4
Figure34. BNNattributionsundervaryingσforF .
4
34TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
Ground Truth Prediction
(a)PI,σ=1−2 (b)PI,σ=1−3 (c)PI,σ=1−6
(d)PI,σ=1−5 (e)PI,σ=1−4 (f)AR,σ=1−2
(g)AR,σ=1−3 (h)AR,σ=1−6 (i)AR,σ=1−5
(j)AR,σ=1−4
Figure35. BNNpredictionsundervaryingσforF .
5
35TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
(a)SHAP,PI,σ=1−2
(b)DeepLIFT,PI,σ=1−2
(c)SHAP,PI,σ=1−3
(d)DeepLIFT,PI,σ=1−3
(e)SHAP,PI,σ=1−6
(f)DeepLIFT,PI,σ=1−6 (g)SHAP,PI,σ=1−5 (h)DeepLIFT,PI,σ=1−5
(i)SHAP,PI,σ=1−4 (j)DeepLIFT,PI,σ=1−4 (k)SHAP,AR,σ=1−2 (l)DeepLIFT,AR,σ=1−2
(m)SHAP,AR,σ=1−3 (n)DeepLIFT,AR,σ=1−3
(o)SHAP,AR,σ=1−6 (p)DeepLIFT,AR,σ=1−6
(q)SHAP,AR,σ=1−5 (r)DeepLIFT,AR,σ=1−5 (s)SHAP,AR,σ=1−4 (t)DeepLIFT,AR,σ=1−4
Figure36. BNNattributionsundervaryingσforF .
5
36TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
Ground Truth Prediction
(a)PI,σ=1−2 (b)PI,σ=1−3 (c)PI,σ=1−6
(d)PI,σ=1−5 (e)PI,σ=1−4 (f)AR,σ=1−2
(g)AR,σ=1−3 (h)AR,σ=1−6 (i)AR,σ=1−5
(j)AR,σ=1−4
Figure37. BNNpredictionsundervaryingσforF .
6
37TheImportanceofArchitectureChoiceinDeepLearningforClimateApplications
(a)SHAP,PI,σ=1−2 (b)DeepLIFT,PI,σ=1−2 (c)SHAP,PI,σ=1−3 (d)DeepLIFT,PI,σ=1−3
(e)SHAP,PI,σ=1−6
(f)DeepLIFT,PI,σ=1−6 (g)SHAP,PI,σ=1−5 (h)DeepLIFT,PI,σ=1−5
(i)SHAP,PI,σ=1−4
(j)DeepLIFT,PI,σ=1−4 (k)SHAP,AR,σ=1−2 (l)DeepLIFT,AR,σ=1−2
(m)SHAP,AR,σ=1−3
(n)DeepLIFT,AR,σ=1−3 (o)SHAP,AR,σ=1−6 (p)DeepLIFT,AR,σ=1−6
(q)SHAP,AR,σ=1−5 (r)DeepLIFT,AR,σ=1−5 (s)SHAP,AR,σ=1−4 (t)DeepLIFT,AR,σ=1−4
Figure38. BNNattributionsundervaryingσforF .
6
38