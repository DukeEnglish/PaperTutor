Dealing with unbounded gradients in stochastic saddle-point optimization
GergelyNeu1 NnekaOkolo1
Abstract sureboththattheoptimumremainsintherewhilekeeping
the optimization process reasonably efficient? In this pa-
We study the performance of stochastic first-
per,weproposeamethodthataddressesthisquestionand
order methods for finding saddle points of
providesasgoodguaranteesasthebestknownprojection-
convex-concave functions. A notorious chal-
based method, but withouthaving to committo a specific
lengefacedbysuchmethodsisthatthegradients
projectionradius.
can grow arbitrarily large during optimization,
whichmayresultininstabilityanddivergence.In It is well known that simply running gradient descent
thispaper,weproposeasimpleandeffectivereg- for both the minimizing and maximizing players can
ularization technique that stabilizes the iterates easily result in divergence, even when having access
and yields meaningful performance guarantees to exact gradients without noise (Goodfellow, 2016;
evenifthedomainandthegradientnoisescales Mertikopoulosetal., 2018). While theaverageofthe iter-
linearly with the size of the iterates (andis thus atesmayconvergeinsuchcases,theirrateofconvergence
potentiallyunbounded). Besidesprovidinga set is typically affected by the magnitude of the gradients,
of general results, we also apply our algorithm which grows larger and larger as the iterates themselves
to a specific problem in reinforcement learning, diverge, thus resulting in arbitrarily slow convergence of
whereitleadstoperformanceguaranteesforfind- the average. Numerous solutions have been proposed to
ing near-optimal policies in an average-reward this issue in the literature, most notably using some form
MDPwithoutpriorknowledgeofthebiasspan. ofgradientextrapolation(Korpelevich,1976;Popov,1980;
Gideletal.,2018;Mertikopoulosetal.,2018). Whenthese
methods have access to noiseless gradients and are run
1. Introduction on smooth objectives, these methods are remarkably sta-
ble: theycanbeshowntoconvergemonotonicallytowards
We studytheperformanceofstochasticoptimizationalgo- their limit. That said, convergence of such methods in
rithms for solving convex-concavesaddle-point problems the stochastic case is much less well-understood, unless
oftheformmin x max y f(x,y). Thealgorithmswe the iterates are projected to a compact set (Juditskyetal.,
∈X ∈Y
consideraimtoapproximatesaddlepointsviarunningtwo 2011; Gideletal., 2018), or the boundedness of the gra-
stochasticconvexoptimizationmethodsagainsteachother, dients ensured by other assumptions (Mishchenkoetal.,
oneaimingtominimizetheobjectivefunctionandtheother 2020;Loizouetal.,2021;Sadievetal.,2023). Indeed,un-
aimingtomaximize.Bothplayershaveaccesstonoisygra- less projections are employed, the iterates of one player
dientevaluationsatindividualpointsx t andy t ofthe pri- may grow large, which can result in large gradients ob-
malanddualdomains and ,andtypicallycomputetheir served by the opposite player, which in turn may result
X Y
updates via gradient-descent-like procedures. Due to the in largeiteratesforthe secondplayer—whicheffectsmay
complicatedinteractionbetweenthetwoconcurrentproce- eventuallycascadeandresultininstabilityanddivergence.
dures, it is notoriously difficult to ensure convergence of Our main contribution is proposing a stabilization tech-
thesemethodstowardsthedesiredsaddlepoints,andinfact niquethateliminatestheriskofdivergence.
even guaranteeing their stability is far from trivial. One
Forthesakeofexposition,letusconsiderthespecialcase
commonwaytomakesurethattheiteratesdonotdiverge
ofbilinearobjectives
isprojectingthemtoboundedsetsaroundtheinitialpoint.
Whilethisideadoesthejob,itgivesrisetoadilemma:how f(x,y)=xTMy+bTx cTy,
should one pick the size of these constraint sets to make −
and primal-dual gradient descent starting from the initial
1Universitat Pompeu Fabra, Barcelona, Spain. Correspon-
pointx =0andy =0asabaseline:
dence to: Gergely Neu <gergely.neu@gmail.com>, Nneka 1
Okolo<nnekamaureen.okolo@upf.edu>.
x =x η f(x ,y )=x ηg (t)
t+1 t x t t t x
− ∇ −
y =y η f(x ,y )=y ηg (t),
t+1 t y t t t y
− ∇ −
1
4202
beF
12
]GL.sc[
1v30931.2042:viXraDealingwithunboundedgradientsinstochasticsaddle-pointoptimization
wherewealsointroducedtheshorthandnotationsg (t) = erty of beinginitialization-dependent,in that theyguaran-
x
f(x ,y ) and g (t) = f(x ,y ). Using standard tee improvedperformancewhenwe picktheinitialpoints
x t t y y t t
∇ ∇
tools(thatwewillexplainindetailbelow),theaverageof (x ,y )closeto(x ,y ).
1 1 ∗ ∗
the first T iterates produced by the above procedure can
Our contributions are closely related to the work
beshowntosatisfythefollowingguaranteeontheduality
of Liu&Orabona (2022), who propose algorithms
gap:
that are guaranteed to achieve an initialization-
G(x ∗,y ∗)=f
1 T
x t,y ∗ f x ∗,
1 T
y t
depe kn xd ∗e −n xt
1
kc 2o +n kv ye ∗rg −e yn 1c ke
2
r ,ate wo hf ert ehe o (rd )er hG id( ex s∗, py o∗ ly) lo=
g-
T !− T ! O √T O ·
Xt=1 Xt=1 ari(cid:16)thmic factors of T a(cid:17)nd the solution norms. When
x 2+ y 2 η T ceompared to our main result, this boeund demonstrates an
k ∗ k2 k ∗ k2 + g (t) 2+ g (t) 2 .
≤ 2ηT 2T k x k2 k y k2 improved scaling with the initialization error. However,
Xt=1(cid:16) (cid:17) theirresultisprovedundertheconditionthatallgradients
Ifonecanensurethatthegradientsg x(t)andg y(t)remain remain bounded: kg x(t) k2
2 ≤
1 and kg y(t) k2
2 ≤
1.
boundedbyaconstantG > 0,onecansetη 1/(G√T) As per the above discussion, this is only possible in
and obtain a convergence rate of order
G( k∼ x∗ k2 2+ ky∗ k2 2)
.
general when projecting the iterates to a bounded sub-
√T set of the domain, which requires prior knowledge
However,noticethatthereisnowaytomakesurethatthe
of the norms x and y . When accounting for
∗ ∗
gradientsactuallyremainboundedwhileexecutingtheal- k k k k
this issue and adapting the method of Liu&Orabona
gorithm! Indeed, notice that xf(x t,y t) = My t + b, (2022) to our setting, their bounds end up scaling with
∇
which growslargeas y growslarge. A naturalidea is to
t (D +D )( x x + y y ), which is worse
projecttheiteratestoballsofrespectivesizesD x, D y >0, thax n the y scalk ing∗ − of o1 uk r2 bouk nd∗ .− Fur1 tk h2 ermore, the results
which guarantees that the iterates and thus the gradients
of Liu&Orabona (2022) are only proved for noiseless
remain bounded. However, convergence to the saddle
gradients(althoughwebelievethatthisspecificrestriction
point (x ∗,y ∗) is now only possible whenever the respec- may not be hard to address). On the positive side, this
tivenormssatisfy x ∗ D x and y ∗ D y,otherwise assumption allows their algorithm to achieve very fast
k k ≤ k k ≤
theoptimalsolutionisexcludedfromthefeasibleset. Un-
convergencewheninitialized veryclose tothe optimum—
fortunately, in many applications, it is impossible to pick
which is generally not possible in the stochastic case
theconstantsD xandD
y
appropriatelydueoflackofprior
we consider. Taking all this into account, we regard our
knowledge of the solution norms. We give an important guaranteesassignificantlydifferentfromtheirresults,and
exampleofsuchasituationattheendofthissection.
inmanysensesastrictimprovement.
In this paper, we propose a method that guaranteesupper
More broadly speaking, our work contributes to the
boundsonthedualitygapofthefollowingform(whenspe-
line of work on parameter-free optimization methods
cializedtothesettingdescribedabove):
that are able to adapt to problem complexity with-
out prior knowledge of the relevant problem parame-
x x 2+ y y 2+1
G(x ∗,y ∗)= k ∗ − 1 k2 k ∗ − 1 k2 , ters. In the context of online convex optimization
O √T ! (OCO), several effective parameter-free algorithms are
known to achieve guarantees scaling optimally with the
where the big-O notation hides some problem-dependent initialization error x x , without requiring prior
∗ 1
constantsrelatedtotheobjectivefunctionf (whichwillbe knowledge thereof (Sk tree− ter&k McMahan, 2012; Orabona,
madeexplicitin ourmain theorem). Notably,ourmethod 2013; vanderHoeven, 2019). (Cutkosky&Boahen,
requiresnopriorknowledgeofthenorms kx ∗ −x 1 k2 2 and 2016; Cutkosky, 2019; Mhammedi&Koolen, 2020) im-
y y 2 whatsoever,andinparticularperformsnopro- provetheseguaranteesbyprovidinginitialization-adaptive
k ∗ − 1 k2
jectionstomakesurethattheiteratesremainbounded,thus bounds for OCO in unconstrained domains without prior
addressingthechallengeoutlinedabove. Aswewillshow, knowledgeofboththesizeofthedomainorsub-gradients
this guarantee holds even when the gradients are subject of the loss. One would think that this would make their
to multiplicative noise that can scale with the magnitude method suitable for solving the problem we study in this
ofthe gradientitself. Ourmaintechnicaltoolisaugment- paper—however, their bound depends on the maximum
ing the objective with a well-chosen regularization term normof the observedsub-gradients,which is problematic
whichallowsustoeliminatetheterms kg x(t) k2 2+ kg y(t) k2
2
forthereasonswehavediscussedextensivelyabove.
appearing in the guarantee of standard primal-dual gra-
Unconstrained saddle-point problems have many
dient descent, and replace them with an upper bound
important applications. Perhaps the most well-
of the gradients of the objective evaluated at the initial
known such applications is in optimizing dual
point (x ,y ). These bounds have the appealing prop-
1 1
2Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
representations of convex functions (Bubecketal., Definition2.2. (subgradientandsubdifferential)Let
∗
2015, Section 5.2,Shalev-Shwartz&Singer, 2006; denote the dual space of . For a function h :
XR,
X X →
Wang&Abernethy, 2018; Wangetal., 2023). Our orig- g isasubgradientofhatx ifforallx ,
∗ ′
∈X ∈X ∈X
inal motivation during the development of this work
has been to develop primal-dual methods for solving h(x) h(x ′) g,x x ′ .
− ≤h − i
average-reward Markov decision processes (MDPs): this
Thesetofallsubgradientsofafunctionhatxiscalledthe
problem can be formulated as a linear program with
subdifferential,andisdenotedby∂h(x).
primalvariablesthatareofunknownscale. Inthesimpler
settingofdiscountedMarkovdecisionprocesses,previous
We recall that when h is convex and differentiable, then
work has provided efficient planning methods based on
h(x) ∂h(x) holds for all x , and additionally
saddle-point optimization (Wang, 2017; Jin&Sidford, ∇ ∈ ∈ X
∂h(x) = h(x) holdswheneverxis intheinteriorof
2020;Chengetal.,2020). Whileinthissimplesettingthe {∇ }
thedomain .
primalvariables(calledvaluefunctionsinthissetting)are X
Definition2.3. (strongconvexity)Forγ 0, a function
known to be uniformly bounded, this is not the case in
h :
Risγ-stronglyconvexwithresp≥
ecttothenorm
the more challenging average-reward setting we consider X →
ifandonlyifforallx,x dom(h),g ∂h(x):
here: in this case, the value functionscan have arbitrarily k·k ′ ∈ ∈
large norm depending on the program structure. As it γ
is well-known in the reinforcement-learning literature, h(x ′) −h(x) ≥hg,x ′ −x i+ 2 kx ′ −x k2 .
estimatingthisparameterisashardassolvingtheoriginal
problem, and learning optimal policies without its prior We consider the problem of finding (approximate) saddle
knowledge has been widely conjectured to be impossible points of convex-concavefunctions on the potentially un-
(Bartlett&Tewari,2009;Fruitetal.,2018b;a;Zhang&Ji, boundedconvexdomains Rm Rn:
X ×Y ⊆ ×
2019). Using our techniques developed in the present
paper, we make progress on this important problem inf supf(x,y), (1)
x y
by proposing a planning algorithm that is guaranteed ∈X ∈Y
to produce a near-optimal policy without having prior wheref : Risassumedtobeconvex-concavein
X ×Y →
knowledge of the scale of the value functions after a thesenseofDefinition2.1.Wefocusontheclassicstochas-
polynomialnumber of queries made to a simulator of the tic first-order oracle modelwhere algorithms can only ac-
environment. cessnoisyestimatesofthesubgradientsatindividualpoints
in . Specifically,wewillconsiderincrementalalgo-
Notations. For an integer T, we use [T] = 1,2, ,T. rithX m× s tY hat produce a sequence of iterates (x ,y )T by
We denote as 1 the vector with all one entries in R· m·· and runningtwoconcurrentonlinelearningmethodt sfot rt c= h1 oos-
represent the positive orthant as Rn. Let Rm and ing the two sequences x T and y T . The algo-
Bdf e refi: gn mX e ath n→ e dir iviR en rn gd e ei rf nf p ce r er oe odn futi c xa tb aal tse. xhx ′F i,o nxr d′ uiv+ ce = ec dto Pbr ys m i= fxX 1, :xx X⊆ i′ x →∈ ′i an RX d atw she e r msi et iqh num pe lnp aci yec ek roi ,n f ag l no dt sh s te e hs ese
{
aq lfu g(e o·n r{ , ic y te htt m)} { }t x T t= p=t i1 } c1T t k= a inn1 gda {{ i im s
y
tt s r }} et
T
tt fo == er 11 m re ti hdn ai tm to ai iz a me s st th h toe e
Dx(x kx ′)=f(x) −f(x ′) −h∇f(x ′),x −x ′ i. m roi un nim di tz ,e th{ e− tf w( ox t p, l· a) y} eT t= rs1 his avc eal ale cd ceth sse tm oa ax sp tl oa cy he ar s. tI in ce fia rc sh
t-
orderoraclethatprovidesthefollowingnoisyestimatesof
2. Preliminaries
a pair of subgradients g (t) ∂ f(x ,y ) and g (t)
x x t t y
∈ ∈
We nowformallydefineourproblemsetupandobjectives. ∂ y( f(x t,y t)),withthenoisyestimateswrittenas
− −
First,werecallsomestandarddefinitions.
g (t)=g (t)+ξ (t)
x x x
Definition 2.1. (convex-concave function) Let
Rm, Rn beconvexsets. Afunctionf : X ⊆R g y(t)=g y(t)+ξ y(t).
Y ⊆ X ×Y → e
is said to be convex-concaveif it is convexin the first ar- Here, ξ (t) Rm and ξ (t) Rn are zero-mean noise
gument and concave in the second. That is, f is convex- vectorsx gener∈ atede in rouny d t ∈ [T] from some unknown
c ho avn ecaveifforanyx,x ′ ∈X,y,y ′ ∈Y andλ ∈[0,1],we Udi ss it nri gbu thti eon ns o, ti an tid oe npe End [e xnt ]ly =of E∈ th [xeinteract ]io =nh xist to ory dF ent o− t1 e.
t t t t 1 t
expectations conditioned on the h| iF sto−ry of observations
f(λx+(1 −λ)x ′,y) ≤λf(x,y)+(1 −λ)f(x ′,y),
uptotheendoftimet, wecanwritetheaboveconditions
and as E t[g x(t)] = g x(t) and E t g y(t) = g y(t). Note that
when the objective is differentiable we can simply set
(cid:2) (cid:3)
f(x,λy+(1 λ)y ′) λf(x,y)+(1 λ)f(x,y ′). g x(t)=e xf(x t,y t)andg y(t)e= yf(x t,y t).
− ≥ − ∇ ∇
3Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
Inagoodpartofthiswork,wefocusontheimportantclass 3. Algorithmand mainresults
ofbilinearobjectivefunctionsthattakethefollowingform:
We now presentour algorithmic approachand provide its
f(x,y)=xTMy+bTx cTy. performance guarantees. For didactic purposes, we start
−
Here,M Rm n,b Rm andc Rn andthedomains with the special case of bilinear games and Euclidean
×
for the opt∈ imizationva∈ riablesare ∈ = Rm and = Rn. geometries, and then later provide an extension to sub-
X Y bilinear objectives and more general geometries in Sec-
This objective is clearly differentiable, and its gradients
tion3.2.
withrespecttox andy aregivenasg (t)=My +band
t t x t
g (t) = MTx c. Inthecontextofbilineargames,we
y t
− 3.1.Unconstrainedbilineargames
will consider a natural noise model where in each round
t, we have access to noisy versions of the matrices and As a gentle start, we first describe our approach for bilin-
vectors necessary for computing the gradients. Specifi- ear games as defined in Section 2 where the domains are
cally, we have M(t) = M +ξ M(t), b(t) = b+ξ b(t), =Rmand =Rn,anddistancesaremeasuredinterms
and c(t) = c+ξ c(t) where ξ M(t),ξ b(t),ξ c(t) are i.i.d, X of the EuclideY an distances in the respective spaces. For
zero-meanrandocmmatricesandvectorsgbeneratedfromun- thiscase,thecoreideaofourapproachistorunstochastic
know bn distributions. We then use these observations to gradientdescent/ascentto computethe iterates of the two
buildthefollowingestimatorsforthegradients: players. As discussed before, this proceduremay diverge
and produce large gradients when run on the original ob-
g (t)=M(t)y +b(t)
x t jective, unless the iterates are projected to a bounded set.
g (t)=M(t)Tx c(t). Our key idea is to replace the projection set with an ap-
ey c t −b
propriatelychosen regularizationterm addedto the objec-
This fits into the generic noise modeldefined earlier with
ξ (t) = ξ (t)ye +ξ (t)c andξ (t) =b ξ (t)Tx ξ (t). tive. Precisely, we introduce the regularization functions
Rx egardingM
the
mt agnitub
de of
they
noise,
wM
e will
mt − akec
the
Hx(x) = 21 kx −x
1
k2
2
and Hy(y) = 21 ky −y
1
k2 2, and
defineouralgorithmviathefollowingrecursiveupdates:
assumptionthatthereexistsconstantsL ,L andL such
M b c
thatE t kb t k2 ≤L b,E t kc t k2 2 ≤L c,and x = argmin x,g (t) +̺ (x)+ 1 x x 2
(cid:2) b (cid:3)E t M(t(cid:2) )y b 2 2 (cid:3) ≤L2 Mky k2 2 t+1 x ∈Rm (cid:26)h x i x Hx η x 1k − t k2 (cid:27)
E t h(cid:13) (cid:13)Mc(t)Tx(cid:13) (cid:13) k2 2i ≤L2 Mkx k2 2, y t+1= a yrg ∈Rm nin (cid:26)− ye,g y(t) +̺ y Hy(y)+ η y ky −y t k2 2 (cid:27).
(cid:10) (cid:11)
holdsforallx,yh(cid:13)
.
Noi
tethatthislatterassumption For each player, the upedate rules can be recognizedas an
(cid:13)c
∈X×Y
is satisfied whenever the operator norm of each M(t) is instanceofCompositeObjectiveMIrrorDescent(COMID,
upperboundedbyL withprobabilityone. Duchietal., 2010), andaccordinglywerefertothe result-
M
c ing algorithm as Composite Objective Gradient Descent-
Thisnoisemodelisoftenmorerealisticthansimplyassum-
Ascent(COGDA).Theupdatescanbewritteninclosedform
ing that ξ (t) and ξ (t) have uniformly bounded norms,
x y as
andismuchmorechallengingtoworkwith:notably,these
noise variablesscale with the iterates x t and y t, and may x = x t −η xg x(t) + ̺ xη xx 1
thusgrowuncontrollablyastheiteratesgrowlarge. Inpar- t+1 1+̺ η 1+̺ η
x x x x
(3)
tt oicu rela inr, foth rce en mo eis ne ti ls eap rr ne ic ni gse ply reo sf enth teis df io nrm Sei cn tio ou nr 4a .pplication
y
t+1
=
y t+η yge y(t)
+
̺ yη yy
1 .
1+̺ η 1+̺ η
y y y y
The final output of the algorithm will be denoted as e
Thisexpressionhasaclearintuitiveinterpretation: forthe
(x ,y ),andduetonoiseinthegradients,itsqualitywill
T T min-player,itisaconvexcombinationofthestandardSGD
bemeasuredin termsoftheexpecteddualitygap, defined
updatex η g (t)andtheinitialpointx ,withweights
withrespecttoacomparatorpoint(x ∗,y ∗)as
that
depet n−
d
ox
n
tx
he regularization
paramet1
er ̺ . Setting
x
E[G(x ∗,y ∗)]=E[f(x T,y ∗) −f(x ∗,y T)]. ̺
x
= 0 recov eers the standard SGD update and makes
the algorithm vulnerable to divergence issues. The over-
Here, it is typical to choose as comparator point a saddle
all method closely resembles the stabilized online mirror
pointoff thatsatisfiestheinequalities
descent methodof Fangetal. (2022), and we will accord-
f(x ∗,y) f(x ∗,y ∗) f(x,y ∗). (2) inglyrefertotheeffectofthenewlyintroducedregulariza-
≤ ≤
tiontermasstabilization.
forallx ,y . Ouranalysiswillallowprovingguar-
∈X ∈Y
anteesagainstarbitrarycomparatorpoints,whichisuseful AfterrunningtheaboveiterationsforT steps,thealgorithm
forcertainapplications(cf.Section4). outputs x = 1 T x and y = 1 T y . The
T T t=1 t T T t=1 t
P P
4Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
followingtheoremis our main resultregardingthe perfor- f(x,y)+ ̺x x x 2 ̺y y y 2 andrewritethe
2 k − 1 k2− 2 k − 1 k2
manceofthisalgorithm. expecteddualitygapas
Theorem3.1. Let̺ = 2η L2 and̺ = 2η L2 . Then,
y x M x y M T
thedualitygapachievedbyCOGDAsatisfiesthefollowing
E[G(x ;y )]
1
E f(reg)(x ,y ) f(reg)(x ,y )
boundagainstanycomparator(x ,y ) Rm Rn: ∗ ∗ ≤ T t ∗ − ∗ t
∗ ∗ ∈ × Xt=1 h i
E[G(x ∗;y ∗)] ≤ (cid:18)2η1 yT 1+η xL2 M (cid:19)ky ∗ −y 1 k2 2 + 2̺ Tx Xt=T 1E hkx ∗ −x 1 k2 2−kx t −x 1 k2 2
i
+ (cid:18)2η xT +η yL2 M (cid:19)kx ∗ −x 1 k2 2 + 2̺ Ty T E ky
∗
−y
1
k2 2−ky
t
−y
1
k2
2
.
+
η
y
T
E M(t)Tx c(t)
2 Xt=1 h i
T 1 − 2 Thefirstterminthisdecompositionthencanbefurtherwrit-
Xt=1 (cid:20)(cid:13) (cid:13) (cid:21) tenasthesumofregretsoftheminandthemaxplayers:
η T (cid:13) (cid:13)c b (cid:13) (cid:13)2
In particular,
settin+
g
xTx
1
Xt
==1E
0(cid:20) a(cid:13)
(cid:13)
(cid:13)nM dcy(t 1)y =1+ 0bb a(t n)
d(cid:13)
(cid:13)
(cid:13)2 η(cid:21)
x =
T1
Xt=T
1E hf(reg)(x t,y ∗) −f(reg)(x ∗,y t)
i
1/L M√T and η y = 1/L M√T, the duality gap is upper 1 T
boundedas = E f(reg)(x t,y ∗) f(reg)(x t,y t)
T −
E[G(x ∗,y ∗)]= OL2 M (cid:16)ky ∗ k2 2+ Lk Mx √∗ k T2 2 (cid:17)+L2 b +L2 c . +
T1Xt=1
T
h
E f(reg)(x t,y t) −f(reg)(x
∗,yi
t)
Xt=1 h i
 
Thesetermscanthenbecontrolledviathestandardregret
It is insightful to compare this bound side by side with
analysis of COMID due to Duchietal. (2010). In particu-
the one we would get by running primal-dual stochastic
lar,afewlinesofcalculations(alongthelinesoftheonline
gradient without regularization. By standard arguments
gradientdescentanalysisofZinkevich,2003)yieldthefol-
(see, e.g., Liu&Orabona, 2022; Abernethyetal., 2018;
lowingboundonthesumofthetworegrets:
Zinkevich,2003),thefollowingboundiseasytoprove:
x x 2 y y 2 1 T
E[G(x ∗;y ∗)] k ∗ − 1 k2 + k ∗ − 1 k2 E f(reg)(x t,y ∗) f(reg)(x ∗,y t)
≤ η T 2η T T −
x y
Xt=1 h i
+ η x T E M(t)y t+b(t) 2 ky ∗ −y 1 k2 2 + η y T E g (t) 2
+ 2 ηT y Xt= T1 E(cid:20)(cid:13) (cid:13) (cid:13) Mc (t)Tx t b c(t(cid:13) (cid:13) (cid:13) )2 (cid:21) 2 . ≤ + k2 xη ∗y −T x 1 k2 2 +2T ηXt x=1 T h E(cid:13) (cid:13)ey g ((cid:13) (cid:13) t2 )i 2
A major problem wit2 hT thXt i= s1 bou(cid:20) n(cid:13) (cid:13) (cid:13)dcis that it− fbeatu(cid:13) (cid:13) (cid:13)re2 s(cid:21) the Recallingtheformo2 fη x thT egradie2 nT tesXt t= im1 atoh rk se,x wenk o2 ti
ethat
squared stochastic gradient normsevaluated at x and y ,
t t
whicharegenerallyunbounded,whichmakesthisguaran- 2
E g (t) 2 =E M(t)y +b(t)
tee void of meaning without projecting the updates. Our k x k2 t 2
ownguaranteestatedabovereplacesthesegradientnorms h i (cid:20)(cid:13) (cid:13) (cid:21)
withthenormsofthegradientsevaluatedattheinitialpoint 2eE M(t)(y (cid:13) (cid:13)cy ) 2 +2bE (cid:13) (cid:13)M(t)y +b(t) 2
t 1 1
≤ − 2 2
x 1,y 1,whichisalwaysboundedirrespectiveofhowlarge (cid:20)(cid:13) (cid:13) (cid:21) (cid:20)(cid:13) (cid:13) (cid:21)
theactualiteratesx t,y tget. 2L2(cid:13) (cid:13)Ec y y 2 +(cid:13) (cid:13)2E M((cid:13) (cid:13)t)c y +b(t) b2 ,(cid:13) (cid:13)
≤ M k t − 1 k2 1 2
At first, it mayseem surprisingthat such an improvement h i (cid:20)(cid:13) (cid:13) (cid:21)
ispossibletoachievebysuchasimpleregularizationtrick.
andasimilarboundcanbeshow(cid:13) (cid:13)nfcorthenormbof(cid:13)
(cid:13)g (t)as
y
Toprovidesomeinsightabouthowregularizationhelpsus well. Putting the aboveboundstogetherand setting ̺
y
≥
achieveourgoal, we providethe briefproofsketch of the 2L Mη xand̺ x 2L Mη y givestheresult. e
≥
abovestatementhere.
As can be seen from the proof, the role of the additional
ProofsketchofTheorem3.1. Fix (x ,y ) Rm Rn. regularizationtermforthex-playeristoeliminatethegra-
∗ ∗
∈ ×
Asthefirststep, weintroducethenotationf(reg)(x,y) = dientnormsappearingin the regretboundof the y-player.
5Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
This effect kicks in once the regularization parameter ̺ inconvexanalysis,cf.Hiriart-Urruty&Lemare´chal,2001,
x
becomeslargeenough,so thatthecorrespondingnegative Section C.1). We will further suppose that the stochastic
term in the regretboundof the first player can overpower gradients themselves satisfy the following conditions for
the positive term appearing on the bound of the opposite someL>0:
player. Thesamestoryappliestothesecondplayer. Note
2
thatwhiletheregularizationpullstheiteratesclosertothe E g (t) L2 y y 2 +1 ,
initialpointx ,y ,itdoesnotexplicitlyguaranteethatthey t k x kx, ∗ ≤ k t − 1 kx, ∗ (4)
1 1 h i2 (cid:16) (cid:17)
remain uniformlyboundedat all times t, and in fact such E
t
g ey(t)
y,
≤L2 kx
t
−x
1
k2
y,
+1 .
claimseemsimpossibletoshowingeneralduetothenoise ∗ ∗
h(cid:13) (cid:13) i (cid:16) (cid:17)
in the gradientestimates. Remarkably,the analysisabove Supposing(cid:13)theatth(cid:13)econditionholdswithnormsrespectively
worksseamlesslyfornoisygradientestimates,eventhough centered at x and y is without loss of generality, and
1 1
thegradientnoisecangrowproportionallywiththesizeof in particular one can always verify l2 x 2 + 1
theiterates. L2 x x 2 +1 atthepriceofreplak cink gy l, ∗ byalarg≤ er
k − 1 ky, (cid:0) (cid:1)
factorLthatma∗ydependon x .
3.2.Sub-bilineargamesandgeneraldivergences (cid:0) (cid:1) k 1 ky, ∗
For this setting, our algorithm is an adaptation of
Aftersettingthestageintheprevioussection,wearenow composite-objective mirror descent (COMID, Duchietal.,
readytointroduceourmethodinitsfullgenerality.Specifi- 2010), which itself is an adaptation of the classic
cally,wearegoingtoconsiderasomewhatbroaderclassof mirror descent method of Nemirovski&Yudin (1983);
objective functions, and provide mirror-descent style per- Beck&Teboulle(2003),variantsofwhichhavebeenused
formance guarantees that measure distances in terms of broadly since the early days of numerical optimization
Bregman divergences. We are going to take inspiration (Rockafellar, 1976; Martinet, 1970; 1978). In particu-
fromTheorem3.1anditsproofwehavejustpresented: in lar, we introduce the additional regularization functions
short,theideaistoaddappropriateregularizationtermsto : R and : R defined respec-
x y
theobjectivethatwillcancelsomeotherwiselargepositive tH ively foX r ea→ ch x and yH as (Y x)→ = 1 x x 2 and
termsintheregretanalysesofthetwoplayers. Thechoice (y) = 1 y y 2 ,anH dx usethese2 ak sad− ditio1 k ny a, l∗ regu-
oftheregularizationtermswillbesomewhatmoreinvolved Hy 2k − 1 kx,
larizationtermsto calcu∗latethefollowingsequenceofup-
inthiscase,andwillrequiretakingthestructureoftheob-
datesineachroundt=1,2,...,T:
jectivefunctionintoaccount.
We will let ω : R and ω : R be two con- 1
vex
functionsx
,
toX be→
called the
dy istaY nce→
-generating func-
x t+1= argmin hx,g x(t) i+̺ x Hx(x)+
η
Dx(x kx t)
tions over and . We suppose that ω
x
is γ x-strongly x ∈X (cid:26) 1x (cid:27)
c iso γnv ye -sx trw onit ghX lyre cs op ne vcY t exto wth ite hn ro esrm peck t·k tox ka ·n kd y.si Wm eila wrl iy llt rh ea st pω ecy
-
y t+1= ar yg ∈m Yax
(cid:26)
(cid:10)y,eg y(t) (cid:11)−̺ y Hy(y) − η yDy(y ky t) (cid:27),
tivelydenotetheBregmandivergencesinducedbyω x and We refer to this algoriethmas Composite-ObjectiveMirror
ω
y
as x( )and y( ). Wewillassumethattheobjec- Descent-Ascent(COMIDA),andprovideourmainresultre-
D ·k· D ·k·
tivefunctionsatisfiesthefollowingcondition: gardingitsperformance.
Definition3.2. (sub-bilinearfunction)Aconvex-concave
Theorem 3.3. Suppose that f is sub-bilinear and the
functionf : R is said to be l-sub-bilinear for
stochasticgradientssatisfy the conditionsin Equation(4).
X ×Y →
s so um bge ral d> ien0 tsw gi xth ∈res ∂p xe fc (t xto ,yth )e ann dor gm ys ∈k·k ∂x yfan (xd ,k y·k )y s, ai tf isi ft ys L coe mtti pn ag r̺ ax to=
r
pη oy γ iL ny t2 s,an thd e̺ ey x= pecη tx γ eL x d2 d, ua an ld itx y∗ g, ay p∗ ob fea Cr Ob Mit Ira Dr Ay
thefollowingconditionsforallx,y:
satisfiesthefollowingbound:
g 2 l2 y 2 +1 ,
k kgx yk kx 2 y,, ∗ ≤≤ l2(cid:16) kk xk kx 2 y, ,∗ +1(cid:17) . E[G(x ∗;y ∗)] ≤ Dy( ηy y∗ Tky 1) + ̺ y ky ∗ − 2 y 1 k2 x, ∗
∗ ∗
Thisconditioneffectivelysta(cid:16) testhat,fora(cid:17)
fixedy(resp.x), +
Dx(x ∗ kx 1)
+
̺ x kx ∗ −x 1 k2 y,
∗
η T 2
theobjectivefunctionf(x,y)isLipschitzwithrespecttox x
η η
( (r re es sp p. .y kx)w kyi ,th )a
.
Pco un ts dt ia fn fet rt eh na tt lg yr ,o itw ms ea at nm so ts ht aa ts ffa bs et ha as vk ey sk lix k,
∗e
+L2 (cid:18)2γy
y
+ 2γx x(cid:19).
abilinearfu∗nctionasymptoticallyasoneapproachesinfin-
ityineachdirection,whichjustifiesthename“sub-bilinear” Themostimportantspecialcaseofoursettingiswhenthe
(mirroring the notion of “sublinearity” or “subadditivity” normsappearinginthestatementaredualtoeachother,and
6Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
in particular = and = , so that ω described thus: for k = 1,2, ,K steps, having ob-
k·kx k·ky, k·ky k·kx, x ···
isstronglyconvexwithres∗pectto andω∗ isstrongly served the current state s of the environment, the agent
k·ky, y k
convexwithrespectto . Thisisth∗ecaseforinstance takesactiona accordingtosomestochasticpolicyπ( s ).
k·kx, k ·| k
when = =Rm,ω = 1∗ 2 andω = 1 2 for In consequence of this action, the agent receives an im-
asymmX etricY positivedex finite2 mk· ak tA rixA y Rm ×2 mk .·k WA e−1 state mediate reward r k = r(s k,a k), and moves to the next
aspecializedversionofourstatementto∈ thissettingbelow. state s k+1 P( s k,a k), fromwherethe interactioncon-
∼ ·|
tinues. The performance of the policy π is measured in
Corollary 3.4. Suppose that f is sub-bilinear and the
terms of the long-term average reward (or gain) ρπ =
stochasticgradientssatisfy the conditionsin Equation(4),
limsup 1E K r(s ,a ) . Thegoaloftheop-
andsupposeadditionallythatω xisγ x-stronglyconvexwith K
→∞
K π k=1 k k
respectto k·ky, andω y isγ y-stronglyconvexwithrespect timal controlproblehm Pis to find an oiptimalpolicy π ∗ that
to . Set t∗ he parameters as ̺ = ηyL2 , ̺ = ηxL2 , achieves maximal average reward: π ∗ = argmax πρ(π).
k·kx,
∗
x γy y γx Weprovidemoredetailsontheexistenceconditionsofsuch
η x =η y = γ xγ y/L2T.Then,lettingx ∗,y ∗bearbitrary optimalpoliciesintheAppendix.
comparatorpoints,thedualitygapof COMIDAsatisfiesthe
p The Lagrangian associated with the optimal control prob-
followingbound:
lemiswrittenas
L( (x x )+ (y y )+1)
E[G(x ∗;y ∗)]= Dx ∗ k 1 Dy ∗ k 1 . (µ;v)= µ,r + v,PTµ ETµ .
O γ xγ yT ! L h i h − i
p Here, theprimalvariableµ ∆ SA isa probabilitydistri-
The proof simply follows from using the definition of butiononthestate-actionspa∈ cethatwewillrefertoasan
2stron (g
y
co ynv )e ax nit dy γto xupper xbo 2und γ 2y ky (x∗ − xy 1 )k .2 x, ∗ ≤ o fuc nc cu tp ioa nnc ty ham te wa esu wre illa rn ed fet rhe tod au sal av va∈ lueR fS uni cs tia onre .a Tl- hv eal su ae dd
-
Dy ∗ k 1 x k ∗ − 1 ky, ≤ Dy ∗ k 1
∗ dle point (µ ,v ) corresponds to the pair of the optimal
The above results enjoy the same initialization-dependent ∗ ∗
occupancymeasureµ andtheoptimalvaluefunctionv .
propertyastheoneswehaveestablishedearlierforbilinear ∗ ∗
Inmostproblemsofpracticalinterest,thescaleofthevalue
games,withtheupgradethattheresultnowholdsinterms
functionsisunknownapriori,andconsequentlythereisno
of general Bregman divergences and also slightly relaxes
tractablewayofcomingupwithaboundedset RSthat
theconditionsontheobjectivefunction. V ⊂
will include the optimal value function v . Without such
∗
priorknowledge,onehastosolvetheunconstrainedsaddle-
4. DAp ecp il si ic oa nti Pon rot co esA sv ee srageReward Markov ip noi on rt deo rp tt oim fii nza dti to hn eop pro tib mle am lpm oli in cyv —∈R wS hm ica hx µ is∈p∆ rS eA ciL se( lµ y; thv e)
subjectofourpaper.
Inthissection,weapplytechniquesfromtheprevioussec-
tionforcomputingnear-optimalpoliciesinaverage-reward We willemployaversionofourstochasticprimal-dualal-
MarkovDecisionProcesses(AMDPs).Asitiswell-known, gorithm to solve the above unconstrained problem. We
thistaskcanbeformulatedasalinearprogram(LP),which work in the well-studied setting of planningwith random-
inturncanbesolvedbyfindingasaddlepointoftheassoci- access models, where we are given a simulator (or gen-
atedLagrangian. Below,wewillonlydescribethesaddle- erative model) of the transition function P that we can
pointoptimizationproblemitselfandgivemorecontexton queryatanystate-actionpair(s,a)forani.i.d.samplefrom
theprobleminAppendixB.Forafulltechnicaldescription P( s,a). Wewillusethissimulatortobuildestimatorsof
·|
oftheLPformulationofoptimalcontrolinMDPs,werefer thegradients
toSection8.8intheclassictextbookofPuterman(1994).
(µ;v)=PTµ ETµ
v
We consider infinite-horizon AMDPs denoted as ∇ L −
(µ;v)=r+Pv Ev,
µ
( , ,r,P) where is a finite state space of cardi- ∇ L −
S A S
nality S, is a finite action space of cardinality A,
withtheirstochasticestimatorscalculatedforeachtas
A
r : [0,1]arewardfunctionandP : ∆
S
S×A→ S×A→
a ofts eto nc rh ea fs et ric totra thn esit ri eo wn am rdod ve el c. torFo rr ease RSo Af n wot ia thtio en n, trw iee
s
g v(t)=e s′
t
−e st
w{r it( hs, ra o)
w}
s(s P,a ()
s∈ ,S a)×
,A=,an Pd (th se ,atr )ansit ∆io Sn∈ m foa rtr (i sx ,aP )∈RSA
×S
.
g eµ(t)=
(s,a X)
∈S×A[r(s,a)+v t(s ′t) −v t(s)]e (s,a),
We also define t·he mat· r| ix E∈ RSA S wit∈ h S en× triA es e
E (s,a),s′ =I {s=s′ }. ∈ × su ′ts (in s,g ai ).i ∼.d. Psa (m ·|sp ,l aes )f( os rt, aa llt) (s∼ ,a)µ ∈t, Ss ′t ×∼ A.P T( h·| is st, ma at k), esal fs oo
r
The agent-environmentinteraction in this MDP setting is atotalofSA+1queriespergradientcomputation.
7Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
Since in our setting only v is unconstrained, it will be eachiterationusesSA+1queriestothegenerativemodel,
enoughtointroducethestabilizingregularizationforthese thismakesforatotalof B4S2A2log(SA) querycomplexity,
ε2
parameters. Withthat,ouralgorithmwillinitializev 1 = 0 whichissuboptimalintermsofitsdependenceonSA,but
andµ 1arbitrarily,andthenperformthefollowingsequence optimal in terms of ε. Most importantly, this guarantee
ofupdatesforallt=1,2,...,T: constitutes the first one we are aware of in the literature
thatdoesnotrequirepriorknowledgeoftheso-called“bias
1
v = argmin v,g (t) + v v 2+̺ v 2 , span”B.
t+1 v RS (cid:26)h v i 2η v k − t k2 v k k ∞(cid:27)
∈
1
µ = argmin µe,g (t) + (µ µ ) , 5. Discussion
t+1 − µ η DKL k t
µ ∈∆SA (cid:26) µ (cid:27)
(cid:10) (cid:11) Ourworkcontributesto therichliteratureonsaddle-point
where DKL(µ kµ ′) = e s,aµ(s,a)log µµ ′( (s s, ,a a)
)
is the rel- optimizationviaincrementalfirst-ordermethods,asubject
ative entropy (or Kullback–Leibler divergence) between studied at least since the works of Martinet(1970; 1978);
P
µ and µ. We refer to the resulting algorithm as Rockafellar (1976); Nemirovski&Yudin (1983). In the
′
COMIDA-MDP. lastfewyears,thistopichasenjoyeda massivecomeback
within the context of optimization for machine learning
The output of COMIDA-MDP is a policy π : ∆ ,
definedbyfirstcomputingtheaverageoftheT primS al→ iterateAs models, and in particular generative adversarial networks
(GANs, Goodfellowetal., 2014). The instability of stan-
µ = 1 T µ ,andthensetting
T T t=1 t dardgradientdescent/ascentmethodshasbeenpointedout
P µ (s,a) early on during this revival, which brought significant at-
π T(as)= T tention to a family of methodsknownextragradientmeth-
| µ (s,a)
a′
∈A
T ′
ods,firstproposedbyKorpelevich(1976)andfurtherdevel-
for all s,a. Then, adapPting a result from Chengetal. oped by Popov (1980); Nemirovski (2004); Juditskyetal.
(2020), we can relate the suboptimality of the output pol- (2011); Rakhlin&Sridharan (2013a;b). A wealth of re-
icy to the duality gap evaluated at a well-chosen pair of cent works have contributed to a better understanding of
comparatorpoints(µ ∗,vπT): these methods, and most notably established last-iterate
convergence of extragradient-type methods for a variety
ρπ∗ ρπT =G(µπ∗ ;vπT).
− of problem settings (Daskalakisetal., 2017; Gideletal.,
Notably,thesizeofthecomparatorpointvπT isunknowna 2018;Mertikopoulosetal.,2018;Mishchenkoetal.,2020).
The majority of these works assume access to either de-
priori,andadditionallyitdependsontheinteractionhistory
terministicgradientsorgradientswith uniformlybounded
whichwillnecessitatesomeextracareinouranalysis. We
noise and bounded domain. The assumption of bounded
onceagainrefertoAppendixBformoredetailsregarding
thechoiceofvπT andtheformalproofoftheaboveclaim. noisewasmorerecentlyliftedintheworksofLoizouetal.
(2021) and Sadievetal. (2023), but their assumptions on
Ourmainresultinthissectionisthefollowing. the noise and the objective function are ultimately incom-
Theorem 4.1. Let ̺ = 4η . Then, the output of patiblewithoursetting.
v µ
COMIDA-MDPsatisfiesthefollowingbound:
Weleaveseveralinterestingquestionsopenforfuturework.
µπ∗ µ The biggest of these questions is if the scaling with the
E hDµπ∗ −µπT,r Ei≤ D +K (cid:18)L (cid:0) ηη v1µ TT +(cid:13)
(cid:13)
41 η(cid:1)
µ
(cid:19)+ Eη µ h(cid:13)+ v4 πη Tv
(cid:13)2
2
i.
i p
s
c
bn
i
ar obi o
n
ut lvi
e
na te
u
dl wdi
n
sz t
h
ea io net ti
nk
ho Tx en
w
h∗
ele
e
e
o−r ar
rh
ro eax
n
mr
v
i1
nek
k
3gx
2
p
.1+ r∗
r
a
.io−
k
t
Weryx
k
t
i∗
o
tn1 h−k
o
ofuw2 2 uy
l
tl
l+
1
e
y
pk
d
r2
ok
ig
o. py
e
rT t∗
io
kmh−
f
ni is
ot
zhy wi es e1 lso tk eheb2 2 dev gnc i
fi
eooa ,rrn u
sm
its tb l
s
isy s,e ep
la
ti eo nm
o
ss
d
sf- -
In particular, if the output policy satisfies vπT(cid:13) (cid:13) B
≤ clearifsuchimprovementispossible,unlikeinthecaseof
for some B > 0 and µ is the uniform distributi∞on over
1 (cid:13) (cid:13) convex minimization problems where there exist efficient
SA, and tuning the parameters as η µ = (cid:13) log S(cid:13)(S TA) and algorithms that achieve such improved rates, at least up
η = SA/T,theboundbecomes q tologfactors(Streeter&McMahan,2012;Orabona,2013;
v
2014). Another interesting line of investigation is to find
p
E µπ∗ µπT,r = B4SAlog(SA) . outifitispossibletoextendourmethodologytogosignif-
− O r T ! icantlybeyondbilinearobjectives.
hD Ei
Weclosebyrecallingthatourapproachbearssomesignif-
Thus, the iteration complexity of COMIDA-MDP for find- icant similarity with the stabilized online mirror descent
ing an ε-optimal policy is of the order B4SAlog(SA). As method of Fangetal. (2022): their approachintroducesa
ε2
8Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
similarregularizationtermtoaddressissuesfacedbyOMD Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng,
inunconstrainedconvexminimizationproblems.Thisidea H. Training gans with optimism. arXiv preprint
was adapted to equilibrium finding in multiplayer games arXiv:1711.00141,2017.
by Hsiehetal. (2021), buttheir resultsare once againnot
Duchi,J.C.,Shalev-Shwartz,S.,Singer,Y.,andTewari,A.
comparable to ours even in two-player zero-sum games
Composite objective mirror descent. In Conference on
(e.g., they consider noiseless gradients and bounded deci-
LearningTheorz,volume10,pp.14–26,2010.
sion sets). We are curiousto see if this stabilization trick
canfindfurtherusesinthecontextofsaddle-pointoptimiza- Fang, H., Harvey, N. J., Portella, V. S., and Friedlander,
tionandgametheoryinthefuture. M. P. Online mirror descent and dual averaging: keep-
ing pace in the dynamiccase. The Journalof Machine
Acknowledgements LearningResearch,23(1):5271–5308,2022.
Fruit, R., Pirotta, M., and Lazaric, A. Near optimal
This project has received funding from the European Re-
exploration-exploitationin non-communicatingmarkov
search Council (ERC) under the European Union’s Hori-
decisionprocesses.AdvancesinNeuralInformationPro-
zon2020researchandinnovationprogramme(Grantagree-
cessingSystems,31,2018a.
mentNo.950180).
Fruit, R., Pirotta, M., Lazaric, A., and Ortner, R. Ef-
References ficientbias-span-constrainedexploration-exploitationin
reinforcementlearning. InInternationalConferenceon
Abernethy, J., Lai, K. A., Levy, K. Y., and Wang, J.-K.
MachineLearning,pp.1578–1586,2018b.
Faster rates for convex-concavegames. In Conference
OnLearningTheory,pp.1595–1625.PMLR,2018. Gidel, G., Berard, H., Vignoud, G., Vincent, P., and
Lacoste-Julien, S. A variational inequality perspec-
Bartlett, P. L. and Tewari, A. REGAL: A regularization tive on generativeadversarialnetworks. arXiv preprint
based algorithm for reinforcement learning in weakly arXiv:1802.10551,2018.
communicating MDPs. In Uncertainty in Artificial In-
Goodfellow,I. NIPS2016tutorial: Generativeadversarial
telligence,2009.
networks. arXivpreprintarXiv:1701.00160,2016.
Bauschke, H. H., Bolte, J., and Teboulle, M. A descent
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
lemma beyond lipschitz gradient continuity: first-order
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
methodsrevisitedandapplications. MathematicsofOp-
Y. Generativeadversarialnets. Advancesinneuralinfor-
erationsResearch,42(2):330–348,2017.
mationprocessingsystems,27,2014.
Beck, A. and Teboulle, M. Mirror descent and nonlinear Hiriart-Urruty,J.-B.andLemare´chal,C. Fundamentalsof
projectedsubgradientmethodsfor convexoptimization. ConvexAnalysis. Springer,2001.
OperationsResearchLetters,31(3):167–175,2003.
Hsieh, Y.-G., Antonakopoulos, K., and Mertikopoulos, P.
Bubeck, S. et al. Convex optimization: Algorithms and Adaptivelearning in continuousgames: Optimal regret
complexity. FoundationsandTrendsinMachineLearn- boundsandconvergencetonashequilibrium. InConfer-
ing,8(3-4):231–357,2015. enceonLearningTheory,pp.2388–2422.PMLR,2021.
Jin, Y. and Sidford, A. Efficiently solving mdps with
Cesa-Bianchi,N.andLugosi,G. Prediction,learning,and
stochastic mirror descent. In International Conference
games. Cambridgeuniversitypress,2006.
onMachineLearning,pp.4890–4900.PMLR,2020.
Cheng, C.-A., Combes, R. T., Boots, B., and Gordon, G.
Juditsky,A.,Nemirovski,A.,andTauvel,C. Solvingvaria-
Areductionfromreinforcementlearningtono-regreton-
tionalinequalitieswithstochasticmirror-proxalgorithm.
line learning. In InternationalConference on Artificial
StochasticSystems,1(1):17–58,2011.
IntelligenceandStatistics,pp.3514–3524.PMLR,2020.
Korpelevich,G. M. The extragradientmethodfor finding
Cutkosky,A.Artificialconstraintsandhintsforunbounded saddlepointsandotherproblems.Matecon,12:747–756,
onlinelearning. InConferenceonLearningTheory,pp. 1976.
874–894.PMLR,2019.
Liu, M. and Orabona, F. On the initialization for convex-
Cutkosky,A. andBoahen,K. A. Onlineconvexoptimiza- concave min-max problems. In International Confer-
tion with unconstraineddomainsand losses. Advances ence on Algorithmic Learning Theory, pp. 743–767.
inneuralinformationprocessingsystems,29,2016. PMLR,2022.
9Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
Loizou, N., Berard, H., Gidel, G., Mitliagkas, I., and Orabona, F. A modern introduction to online learning.
Lacoste-Julien, S. Stochastic gradient descent-ascent arXivpreprintarXiv:1912.13213,2019.
andconsensusoptimizationforsmoothgames: Conver-
Popov,L.D. Amodificationofthearrow-hurwiczmethod
gence analysis underexpected co-coercivity. Advances
for search of saddle points. Mathematical notes of the
in Neural Information Processing Systems, 34:19095–
AcademyofSciencesoftheUSSR,28:845–848,1980.
19108,2021.
Puterman, M. L. Markov Decision Processes: Discrete
Martinet, B. Re´gularisation d’ine´quations variationnelles
Stochastic Dynamic Programming. Wiley-Interscience,
par approximations successives. ESAIM: Mathemati-
April1994.
cal Modelling and Numerical Analysis - Mode´lisation
Mathe´matique et Analyse Nume´rique, 4(R3):154–158, Rakhlin, A. and Sridharan, K. Online learning with pre-
1970. dictablesequences. InConferenceonLearningTheory,
pp.993–1019.PMLR,2013a.
Martinet, B. Perturbation des me´thodes d’optimisation.
applications. ESAIM: Mathematical Modelling and Rakhlin, A. and Sridharan, K. On equivalence of martin-
Numerical Analysis - Mode´lisation Mathe´matique et galetailboundsanddeterministicregretinequalities. In
Analyse Nume´rique, 12(2):153–171, 1978. URL ConferenceonLearningTheory,pp.1704–1722.PMLR,
http://eudml.org/doc/193317. 2017.
Rakhlin,S.andSridharan,K. Optimization,learning,and
McMahan, B. and Abernethy, J. Minimax optimal algo-
gameswith predictablesequences. Advancesin Neural
rithmsforunconstrainedlinearoptimization. Advances
InformationProcessingSystems,26,2013b.
inNeuralInformationProcessingSystems,26,2013.
Rockafellar, R. T. MonotoneOperatorsand the Proximal
Mertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S.,
Point Algorithm. SIAM Journal on Control and Opti-
Chandrasekhar,V., and Piliouras, G. Optimistic mirror
mization,14(5):877–898,1976.
descentin saddle-pointproblems: Goingthe extra(gra-
dient) mile. In International Conference on Learning Sadiev,A.,Danilova,M.,Gorbunov,E.,Horva´th,S.,Gidel,
Representations,2018. G., Dvurechensky, P., Gasnikov, A., and Richta´rik, P.
High-probabilityboundsforstochasticoptimizationand
Mhammedi, Z. and Koolen, W. M. Lipschitz and
variationalinequalities: thecaseofunboundedvariance.
comparator-normadaptivityin onlinelearning. In Con-
arXivpreprintarXiv:2302.00999,2023.
ference on Learning Theory, pp. 2858–2887. PMLR,
2020. Shalev-Shwartz,S.andSinger,Y. Convexrepeatedgames
andfenchelduality. Advancesinneuralinformationpro-
Mishchenko, K., Kovalev, D., Shulgin, E., Richta´rik, P., cessingsystems,19,2006.
andMalitsky,Y. Revisitingstochasticextragradient. In
International Conference on Artificial Intelligence and Streeter, M. and McMahan, H. B. No-regret algorithms
Statistics,pp.4573–4582.PMLR,2020. for unconstrained online convex optimization. arXiv
preprintarXiv:1211.2260,2012.
Nemirovski, A. Prox-method with rate of convergenceo
vanderHoeven,D.User-specifiedlocaldifferentialprivacy
(1/t)forvariationalinequalitieswithlipschitzcontinuous
in unconstrainedadaptiveonlinelearning. Advancesin
monotoneoperatorsandsmoothconvex-concavesaddle
NeuralInformationProcessingSystems,32,2019.
point problems. SIAM Journal on Optimization, 15(1):
229–251,2004. Wang, J.-K. and Abernethy, J. D. Acceleration through
optimistic no-regretdynamics. Advances in Neural In-
Nemirovski, A. S. and Yudin, D. B. Problem complexity
formationProcessingSystems,31,2018.
andmethodefficiencyinoptimization. 1983.
Wang, J.-K., Abernethy,J., andLevy,K.Y. No-regretdy-
Orabona, F. Dimension-free exponentiated gradient. Ad- namicsinthefenchelgame: Aunifiedframeworkforal-
vances in Neural Information Processing Systems, 26, gorithmicconvexoptimization. MathematicalProgram-
2013. ming,pp.1–66,2023.
Orabona, F. Simultaneous model selection and optimiza- Wang, M. Primal-dual π learning: Sample complex-
tion through parameter-free stochastic learning. Ad- ity and sublinear run time for ergodic markov deci-
vances in Neural Information Processing Systems, 27, sion problems. CoRR, abs/1710.06100, 2017. URL
2014. http://arxiv.org/abs/1710.06100.
10Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
Xiao,L. Dualaveragingmethodforregularizedstochastic
learning and online optimization. Advances in Neural
InformationProcessingSystems,22,2009.
Zhang, Z. and Ji, X. Regret minimization for reinforce-
ment learning by evaluating the optimal bias function.
AdvancesinNeuralInformationProcessingSystems,32,
2019.
Zinkevich, M. Online convex programming and general-
izedinfinitesimalgradientascent. InProceedingsofthe
Twentieth International Conference on Machine Learn-
ing(ICML),2003.
11Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
A. Proofofresults inSection 3
Inthissection,weprovideadetailedproofofclaims,lemmasandtheoremsinSection3inthemaintext.
A.1.CompleteproofofTheorem3.1
Westartbyrewritingtheexpecteddualitygapevaluatedat(x ;y )asfollows:
∗ ∗
E[G(x ∗;y ∗)]=E[f(x T,y ∗) −f(x ∗,y T)]
T
1
= E[f(x ,y ) f(x ,y )]
t ∗ ∗ t
T −
t=1
X
T
1
= E f(reg)(x t,y ∗) f(reg)(x ∗,y t)
T −
Xt=1 h i
T
̺
+ y E y y 2 y y 2
2T k ∗ − 1 k2−k t − 1 k2
Xt=1 h i
T
̺
+ x E x x 2 x x 2 . (5)
2T k ∗ − 1 k2−k t − 1 k2
Xt=1 h i
To control the first set of terms in the above expression, we apply the regret analysis of COMIDA in Appendix A.2.1.
Precisely,with (x x)= 1 x x 2, (y y )= 1 y y 2and, (x)= 1 x x 2, (y)= 1 y y 2,
Dx k ′ 2k − ′ k2 Dy k ′ 2k − ′ k2 Hx 2k − 1 k2 Hy 2k − 1 k2
weget:
T
E f(reg)(x ,y ) f(reg)(x ,y )
t ∗ ∗ t
−
Xt=1 h i
T T
= E f(reg)(x t,y ∗) f(reg)(x t,y t) + E f(reg)(x t,y t) f(reg)(x ∗,y t)
− −
Xt=1 h i Xt=1 h i
ky ∗ −y 1 k2 2 + η y T E g (t) 2 + kx ∗ −x 1 k2 2 + η x T E g (t) 2 . (6)
≤ 2η 2 y 2 2η T 2 k x k2
y x
Xt=1 h(cid:13) (cid:13) i Xt=1 h i
To proceed,werecalltheassumptionswemadeonthe(cid:13)geradien(cid:13)testimatorsonthemaintext,nameelythattheinequalities
E M(t)y 2 L2 y 2 andE M(t)Tx 2 L2 x 2 holdforallx,y . Usingthisconditionallows
t 2 ≤ Mk k2 t k2 ≤ Mk k2 ∈ X ×Y
ustho (cid:13)boundth (cid:13)eigradientnormsas h(cid:13) i
(cid:13)c (cid:13) (cid:13)c
2
E g (t) 2 =E M(t)Tx c(t)
t y 2 t t − 2
h(cid:13) (cid:13)e (cid:13) (cid:13) i =E (cid:20)(cid:13) (cid:13) (cid:13)Mc (t)T(x bx (cid:13) (cid:13) (cid:13))+(cid:21) M(t)Tx c(t) 2
t t 1 1
− − 2
(cid:20)(cid:13) (cid:13) (cid:21)
2E (cid:13) (cid:13)c M(t)T(x x ) 2 c +2E Mb (t(cid:13) (cid:13))Tx c(t) 2
t t 1 t 1
≤ − 2 − 2
(cid:20)(cid:13) (cid:13) (cid:21) (cid:20)(cid:13) (cid:13) (cid:21)
2L2 (cid:13) (cid:13)xc x 2+2E (cid:13) (cid:13) M(t)Tx (cid:13) (cid:13)c c(t) 2 , b (cid:13) (cid:13)
≤ Mk t − 1 k2 t 1 − 2
(cid:20)(cid:13) (cid:13) (cid:21)
wherethethirdlineusesthetriangleinequalityandCauchy–Schwa(cid:13) (cid:13)rzc,andthesec bond(cid:13) (cid:13)followsfromsaidassumption.Like-
wise,wecanshow
2
E g (t) 2 2L2 y y 2+2E M(t)y +b(t) .
t k x k2 ≤ Mk t − 1 k2 t 1 2
h i (cid:20)(cid:13) (cid:13) (cid:21)
Therefore,bythetowerruleandm eonotonicityofexpectation, (cid:13) (cid:13)c b (cid:13)
(cid:13)
E g (t) 2 =E E g (t) 2 2L2 E x x 2 +2E M(t)Tx c(t) 2 ,
y 2 t y 2 ≤ M k t − 1 k2 1 − 2
h(cid:13) (cid:13)e (cid:13) (cid:13) i h h(cid:13) (cid:13)e (cid:13) (cid:13) ii 12h i (cid:20)(cid:13) (cid:13) (cid:13)c b (cid:13) (cid:13) (cid:13) (cid:21)Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
and
2
E g (t) 2 =E E g (t) 2 2L2 E y y 2 +2E M(t)y +b(t) .
k x k2 t k x k2 ≤ M k t − 1 k2 1 2
h i h h ii h i (cid:20)(cid:13) (cid:13) (cid:21)
Plugging these deriv eations into the boun ed of Equation Equation (6) and then com(cid:13) (cid:13)bcining the rbesul(cid:13) (cid:13)t with the bound of
EquationEquation(5),weobtain
T
1 ̺ η 2
E[G(x ;y )] + y y y 2+ y E M(t)Tx c(t)
∗ ∗ ≤ (cid:18)2η yT 2 (cid:19)k ∗ − 1 k2 T
Xt=1 (cid:20)(cid:13)
1 − (cid:13)2
(cid:21)
1 ̺ η T (cid:13) (cid:13)c b (cid:13) (cid:13) 2
+
(cid:18)2η xT
+ 2x (cid:19)kx ∗ −x 1 k2 2+ Tx Xt=1E (cid:20)(cid:13)M(t)y 1+b(t)
(cid:13)2
(cid:21)
T (cid:13)cT b (cid:13)
1 ̺ 1(cid:13) (cid:13) ̺
+ E y y 2 η L2 y + E x x 2 η L2 x .
T k t − 1 k2 x M − 2 T k t − 1 k2 y M − 2
Xt=1 h i(cid:16) (cid:17) Xt=1 h i(cid:16) (cid:17)
By setting ̺ = 2η L2 and ̺ = 2η L2 , we eliminate the last two termsin the boundaboveand arrive at the result
y x M x y M
statedinthetheorem:
T
1 η 2
E[G(x ∗;y ∗)] ≤ (cid:18)2η yT +η xL2 M (cid:19)ky ∗ −y 1 k2 2+ Ty Xt=1E (cid:20)(cid:13)M(t)Tx 1 −c(t) (cid:13)2
(cid:21)
1 η T (cid:13) (cid:13)c b (cid:13) (cid:13) 2
+ +η L2 x x 2+ x E M(t)y +b(t) .
(cid:18)2η xT y M (cid:19)k ∗ − 1 k2 T
Xt=1 (cid:20)(cid:13)
1 (cid:13)2
(cid:21)
(cid:13) (cid:13)c b (cid:13)
(cid:13)
A.2.ProofofTheorem3.3
Considertheexpecteddualitygapatarbitrarycomparatorpoints(x ,y ):
∗ ∗
E[G(x ;y )]=E[f(x ,y ) f(x ,y )].
∗ ∗ T ∗ − ∗ T
Bytheconvex-concavepropertyoff andstraightforwardderivations,wecanrewritetheabovegapintermsoftheregret
ofamin-maxoptimizationschemeandregularizationtermsas
E[G(x ;y )]=E[f(x ,y ) f(x ,y )]
∗ ∗ T ∗ − ∗ T
T
1
E[f(x ,y ) f(x ,y )]
t ∗ ∗ t
≤ T −
t=1
X
T T
1 1
= E[f(x ,y ) f(x ,y )]+ E[f(x ,y ) f(x ,y )]
t ∗ t t t t ∗ t
T − T −
t=1 t=1
X X
T T
1 1
= E f(reg)(x ,y ) f(reg)(x ,y ) + E f(reg)(x ,y ) f(reg)(x ,y )
t ∗ t t t t ∗ t
T − T −
Xt=1 h i Xt=1 h i
T T
̺ ̺
+ y E[ y(y ∗) y(y t)]+ x E[ x(x ∗) x(x t)], (7)
T H −H T H −H
t=1 t=1
X X
whereinthiscase,
̺ ̺
f(reg)(x,y)=f(x,y)+ x (x) y (y).
x y
2 H − 2 H
Therestoftheproofissplitintwoparts. First,wecontrolregularizedregretoftheminandmaxplayers,correspondingto
thefirsttwosumsappearingontheright-handsideoftheabovebound. Then,substitutingtheresultingboundbackinto
Equation(7),wetakeadvantageofthenegativeterms (x )and (y )appearingontherighthandsidetocancelout
x t y t
H H
somepotentiallylargetermsintheregretanalysis,arrivingataboundthatisrobusttolargeiterates.
13Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
A.2.1.REGRET ANALYSIS OFCOMIDA ON A REGULARIZED OBJECTIVE
This part of the proof is based on the regretanalysis of Composite Mirror Descent (COMID) for stochastic convexopti-
mization. Theproofisamore-or-lessstandardexercisein convexanalysis(appearing,e.g., asTheorem8 ofDuchietal.
(2010)),andweprovideitforcompletenessasLemmaD.1inAppendixD.Inthissection,wedirectlyapplytheimplied
guaranteeontheregretofCOMIDagainstafixedcomparatorinCorollaryD.2tocontroltheregretofeachplayer.
Forthemaxplayer,wedenotethelossinroundtasℓ (y) = f(x ,y)fory andwedefineitsregularizedlossas
t t
− ∈ Y
ℓ(reg)(y) = f(x ,y)+̺ (y). Then,thetotalexpectedregretofthemaxplayerontheregularizedobjectivecanbe
t − t y Hy
rewrittenas
T T
E f(reg)(x ,y ) f(reg)(x ,y ) = E ℓ(reg)(y ) ℓ(reg)(y ) .
t ∗ − t t t t − t ∗
Xt=1 h i Xt=1 h i
Notice that ℓ(reg)() is convexby the concave propertyof f(x , ). We will bound the regret using Corollary D.2, with
initial iteratet u =· y , gradientestimates g (t) = g (t) ant d· gradientsg (t) = g (t). Also, we will set = Rn,
1 1 u − y u − y U
ω =ω ,η =η and̺ =̺ . Withy fixedandindependentoftheiterates,thisgives
u y u y u y ∗
e e
T T
E f(reg)(x t,y ∗) −f(reg)(x t,y t)
≤
Dy(y η∗ ky 1) + 2η γy E g y(t) 2
y,
+̺ yE[ Hy(y 1)].
Xt=1 h i y y Xt=1 h(cid:13) (cid:13) ∗ i
Likewise,reusingpreviousnotationwedenotethelossoftheminplayerasinrou(cid:13) nedta(cid:13)
sℓ (x)=f(x,y ). Sincef(,y )
t t t
isconvex,andbyequivalenceoftheminimizationstepofCOMIDAtothatofCorollaryD.2whenu =x ,g (t)=g· (t),
1 1 u x
g (t) = g (t), = Rm, ω = ω , η = η and ̺ = ̺ , we can boundthe regretof the min player againsta fixed
u x u x u x u x
U
comparatorx asfollows:
∗ e e
T T
(x x ) η
E f(reg)(x t,y t) −f(reg)(x ∗,y t)
≤
Dx η∗ k 1 + 2γx E kg x(t) k2
x,
+̺ xE[ Hx(x 1)].
x x ∗
Xt=1 h i Xt=1 h i
Therefore,thetotalexpectedregretofCOMIDAontheregularizedobjectiveisbouendedaboveasfollows:
T T
E f(reg)(x t,y ∗) f(reg)(x t,y t) + E f(reg)(x t,y t) f(reg)(x ∗,y t)
− −
Xt=1 h i Xt=1 h i
T
Dy(y ∗ ky 1) + η y E g (t) 2 +̺ E[ (y )]
≤ η 2γ y y, y Hy 1
y y Xt=1 h(cid:13) (cid:13) ∗ i
(x x ) η
T(cid:13)e (cid:13)
+ Dx ∗ k 1 + x E g (t) 2 +̺ E[ (x )].
η 2γ k x kx, x Hx 1
x x ∗
Xt=1 h i
Thiscompletesthefirstpartoftheproof. e
A.2.2.ELIMINATINGTHEGRADIENT NORMS
Forthe secondpart, we make use ofourspecific definitionofthe regularizationfunction: (x) = 1 x x 2 and
(y) = 1 y y 2 . Inthiscase (x )= (y ) =0. Then,pluggingintheboundH sfx romApp2 ek ndi− xA.1 2k .1y, i∗ nthe
Hy 2k − 1 kx, Hx 1 Hy 1
expecteddualitygapwe∗have:
T T
E[G(x ∗;y ∗)]
≤
Dy( ηy ∗ Tky 1) + 2γη y
T
E g y(t) 2
y,
+ Dy( ηx ∗ Tkx 1) + 2γη x
T
E kg x(t) k2
x,
y y Xt=1 h(cid:13) (cid:13) ∗ i x x Xt=1 h ∗ i
̺ T (cid:13)e (cid:13) ̺ T e
+ 2Ty E ky ∗ −y 1 k2 x, ∗−ky t −y 1 k2 x,
∗
+ 2Tx E kx ∗ −x 1 k2 y, ∗−kx t −x 1 k2 y,
∗
. (8)
Xt=1 h i Xt=1 h i
To proceed,wemakecrucialuseofournoiseconditionstated asEquation(4)in themaintextso thatwecanboundthe
gradientnormsas
E g (t) 2 =E E g (t) 2 E L2 x x 2 +1 .
y 2 t y 2 ≤ k t − 1 ky,
∗
h(cid:13) (cid:13) i h h(cid:13) (cid:13) ii h (cid:16) (cid:17)i
(cid:13)e (cid:13) (cid:13)e (cid:13)14Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
Also,
E g (t) 2 =E E g (t) 2 E L2 y y 2 +1 .
k x k2 t k x k2 ≤ k t − 1 kx,
∗
h i h h ii h (cid:16) (cid:17)i
PluggingtheseintotheboundofEquation(8)gives
e e
(y y ) η ̺
E[G(x ∗;y ∗)] ≤ Dy η∗ Tk 1 + 2γy L2+ 2yE ky ∗ −y 1 k2 x,
y y ∗
h i
(x x ) η ̺
+ Dy η∗ Tk 1 + 2γx L2+ 2xE kx ∗ −x 1 k2 y,
x x ∗
h i
1 T η L2 ̺ 1 T η L2 ̺
+ E y y 2 x y + E x x 2 y x .
T k t − 1 kx, 2γ − 2 T k t − 1 ky, 2γ − 2
Xt=1 h ∗ i(cid:18) x (cid:19) Xt=1 h ∗ i(cid:18) y (cid:19)
Lastly,choosing̺ =
ηxL2
and̺ =
ηyL2
resultsintheboundstatedinthetheorem:
y γx x γy
E[G(x ∗;y ∗)] Dy(y ∗ ky 1) + η yL2 + ̺ y ky ∗ −y 1 k2 x, ∗
≤ η T 2γ 2
y y
+
Dx(x ∗ kx 1)
+
η xL2
+
̺ x kx ∗ −x 1 k2 y,
∗.
η T 2γ 2
x x
15Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
B. AnalysisfortheAverage-Reward MDP Setting
B.1.Problemsetup
First we briefly recall some general concepts related to average-reward MDPs (and refer the reader to Chapter 8 of
Puterman,1994foramoredetailedintroductionintothetopic). Considerinfinite-horizonAMDPsdenotedas( , ,r,P)
S A
where isafinitestatespaceofcardinalityS, isafiniteactionspaceofcardinalityA,r: [0,1]arewardmodel
andPS : ∆ astochastictransitionA model. Forease ofnotation,weoftenreferS to× thA er→ ewardvectorr RSA
S
with r(sS ,a× )A → entries,andthetransitionmatrixP RSA withP[s,a]=p( s,a) ∆ for(s,a) ∈ .
(s,a) ×S S
{ } ∈S×A ∈ ·| ∈ ∈S×A
In this work, we primarilyfocusonthe class of AMDPs where eachpolicy π hasa well-defineduniquestationarystate
distribution(orstate-occupancymeasure)νπ : [0,1],definedforeachsas
S →
K
1
νπ(s)= lim P[x =x π].
k
K K |
→∞ k=1
X
Thestationarydistributioncanbeseentosatisfythelinearsystemofequationsνπ(s)= p(ss,a)π(a s)νπ(s)
(s′,a′)
|
′ ′ ′
|
′ ′
foralls . Hence,thecorrespondingstationarystate-actiondistribution(orstate-actionoccupancymeasure)µπ(s,a)=
∈S P
π(as)νπ(s) for (s,a) is also unique, and we can write the average-rewardobjective as ρπ = µπ,r . This
| ∈ S ×A h i
compactrepresentationoftherewardcriterionandoccupancymeasureinspiresthelinearprogrammingapproachtooptimal
controlinMDPs,whereinweareinterestedinsolvingthelinearprogram
max µ,r
µ RSA h i
∈
subjectto ETµ=PTµ
(9)
µ,1 =1
h i
µ 0.
≥
In the above expressions, the operator E : RSA R is defined as (ETµ)(s) = µ(s,a) for s . This LP is
→ S a ∈ S
motivatedbythefactthatthesetofdistributionsµthatsatisfytheconstraintsexactlycorrespondstothesetofstationary
P
state-actiondistributionsthatcanbepotentiallyinducedbyastationarypolicyintheMDP.
We also definethevaluefunction(orbiasfunction)ofpolicyπ asvπ : R, takingthefollowingvalueineachstate
S →
s :
∈S
K
vπ(s)= lim E (r(s ,a ) ρπ) s =s
π k k 0
K " − (cid:12) #
→∞ k X=1 (cid:12)
(cid:12)
= π(as)[r(s,a) ρπ+ p( s(cid:12),a),vπ ]. (10)
| − h ·|(cid:12) i
a
X
Then, the value functionof an optimalpolicymaximizingρπ can be shown to be an optimalsolution of the dualof the
LP(9),writtenasfollows:
min ρ
ρ R,v (11)
∈ ∈V
subjectto Ev r+Pv 1ρ.
≥ −
FindinganoptimalsolutiontoeitheroftheLPscanbeequivalentlyphrasedassolvingthefollowingbilineargame:
min max (v;µ), (12)
v ∈Vµ ∈∆SAL
withtheLagrangianassociatedwiththeLPsisdefinedas
(v;µ)= µ,r + v,PTµ ETµ +ρ(1 µ,1 )
L h i h − i −h i
= µ,r + v,PTµ ETµ .
h i h − i
Thegradientsoftheaboveobjectivearerespectivelyexpressedas
(v;µ)=PTµ ETµ and (v;µ)=r+Pv Ev.
v µ
∇ L − ∇ L −
16Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
Nowinthecontextofplanning,itisassumedthatthetransitionmodelisunknown,hencethegradientscannotbecomputed
exactly. Rather,weassumeaccesstoanaccuratesimulatorwhichcanbequeriedatanystate-actionpair(s,a)
∈ S ×A
to obtaina samplenextstate s p( s,a). Indeed,with v ,µ determinedbythe endofroundt 1, we cancompute
′ t t
∼ ·| −
unbiasedestimatesinroundtas:
g v(t)=e s′
t
−e st
g eµ(t)= r(s,a)+v t(s′t(s,a)) −v t(s) e (s,a),
(s,a X)
∈S×A(cid:0) (cid:1)
e
usingi.i.dsamples(s ,a ) µ ,s p( s ,a ),alsos (s,a) p( s,a)forall(s,a) .
t t
∼
t ′t
∼ ·|
t t ′t
∼ ·| ∈S×A
Our aim is to find a near-optimal policy with a polynomial number of queries to the generative model, by running a
versionof gradientdescent-ascentonthe Lagrangian . In particular,we aimto derivea boundonthe suboptimalityof
L
the outputpolicy in termsof the optimization-errorguaranteethat we obtain by runningour algorithm. To achieve this,
a keyquantitytostudy isthe expectedgapofthe averagediterates(µ ,v ) ∆ againstarbitrarycomparators
T T ∈ SA ×V
(µ ,v ) ∆ denotedas
∗ ∗ SA
∈ ×V
E[G(µ ∗;v ∗)]=E[ L(µ ∗;v T) −L(µ T;v ∗)], (13)
where(µ ,v ) = 1 T µ , 1 T v andπ areasdescribedinthemaintext. Then,arelationshipbetweenthe
T T T t=1 t T t=1 t T
dualitygapandthep(cid:16)olic Pycanbeesta Pblishedb(cid:17)ychoosingthecomparatorsas(µ ∗,v ∗)=(µπ∗ ,vπT) ∆
SA
RS. Indeed,
∈ ×
asweshowinLemmaC.2(aresultadaptedfromChengetal.,2020),thetwoquantitiesunderthischoicecanberelatedas
E G(µπ∗ ;vπT) =E µπ∗ µπT,r . (14)
−
h i hD Ei
B.2.Methodology
InordertoapplystandardOMDtosolveEquation(12),previousLP-basedapproachestoplanninginfiniteAMDPs(Wang,
2017; Jin&Sidford, 2020) requiredthe domain to cover v , which requiresprior knowledgeof the propertiesof the
∗
V
MDP. To this end, they made the assumption that the value functions of all policies have bounded span seminorm: for
allpoliciesπ, thevaluefunctionvπ satisfies kvπ
ksp
= max svπ(s) −min s′vπ(s ′)
≤
B forsomeB > 0. We callthis
quantity the worst-case bias span. A simple way to make sure that the above assumption holds is to suppose that the
Markovchainsinducedbyeachpolicyπhaveboundedmixingtimet ,definedas
mix
t =max argmin max νT(Pπ)t νπ .
mix
π " t ≤1 (cid:26)ν ∈∆S
(cid:13)
− (cid:13)1 (cid:27)#
(cid:13) (cid:13)
Thisensuresthatthesupremumnormofthevalueofanypolicy(cid:13)isboundedabov(cid:13)ewith vπ 2t . Previousworks
mix
k k ≤
ofWang(2017);Jin&Sidford(2020)assumedthismixing-timeparametertobeknown,andd∞esignediterativealgorithms
thatrequireprojectionstotheset = v RS : v 2t . Sincethisparameteristypicallyunknownandishard
B mix
toestimate,thesealgorithmsarenV otfullysa∈ tisfactork y.k∞ ≤
(cid:8) (cid:9)
Weareinterestedinnear-optimalplanningingeneralAMDPsforwhichthestationarystatedistributioniswelldefinedand
biasspanispotentiallyunknown,andthuswehavetoset =RS. Sincetheprimalvariablesarenaturallyrestrictedtothe
V
simplexdomain,weonlyrequirethestabilizationtricktocontroltheactionsofthemin-playerin thebound. Hence,we
canboundthedualitygapagainstarbitrarycomparatorpoints(v ;µ )asfollows:
∗ ∗
E[G(v ∗;µ ∗)]=E[ L(v T;µ ∗) −L(v ∗;µ T)]
T
1
E[ (v t;µ ∗) (v ∗;µ t)]
≤ T L −L
t=1
X
T T
1 1
= E[ (v ;µ ) (v ;µ )]+ E[ (v ;µ ) (v ;µ )]
t ∗ t t t t ∗ t
T L −L T L −L
t=1 t=1
X X
T T
1 1
= E[ (v ;µ ) (v ;µ )]+ E (reg)(v ;µ ) (reg)(v ;µ )
t ∗ t t t t ∗ t
T L −L T L −L
Xt=1 Xt=1 h i
17Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
T
̺
+ v E[ v(v ∗) v(v t)], (15)
T H −H
t=1
X
wherewehavedefined (reg)(v;µ)= (v;µ)+̺ (v).
v v
L L H
Takingintoaccountthenew(unregularized)lossobjectiveofthemax-playerandrequiredprojectionstothesimplex,our
algorithmexecutesCOMIDtooptimizev andstandardOMD (whichissameasCOMIDAwith̺ = 0)forµ. Precisely,
µ
theupdatesarecalculatedbysolving
1
v = argmin v,g (t) +̺ v 2 + v v 2
t+1 v RS (cid:26)h v i v k k ∞ 2η v k − t k2 (cid:27)
∈
1
µ = argmin µe,g (t) + (µ µ ) ,
t+1 − µ η DKL k t
µ ∈∆SA (cid:26) µ (cid:27)
(cid:10) (cid:11)
usingthegradientestimatorsdescribedinthemaintext.eWepresentthecompletepseudocodeasAlgorithm1.
Algorithm1COMIDA-MDP
Input: Stepsizesη ,η ,Regularizationconstants̺ ,Initialpointsv ,µ .
v µ v 1 1
fort=1toT do
//MirrorDescent//
Sample(s ,a ) µ ,s p( s ,a )
t t
∼
t ′t
∼ ·|
t t
Computeg v(t)=e s′
t
−e st
Update
ev
t+1
= argmin
v ∈RS
hv,g v(t) i+̺
v
kv k2 ∞+ 2η1
v
kv −v
t
k2
2
n o
//MirrorAscent//
e
Samples p( s,a)forall(s,a)
′t
∼ ·| ∈S×A
Computeg (t)= [r(s,a)+v (s ) v (s)]e
µ (s,a) t ′t − t (s,a)
Update
P
eµ = argmin µ,g (t) + 1 (µ µ )
t+1 µ ∈∆SA − µ ηµDKL k t
n (cid:10) (cid:11) o
endfor
e
Return v = 1 T v ,µ = 1 T µ .
T T t=1 t T T t=1 t
P P
C. The proofofTheorem 4.1
Werestatetheresulthereforconvenienceofthereader.
TheoremC.1. Let̺ =η . Then,theoutputof COMIDA-MDPsatisfiesthefollowingbound:
v µ
E µπ∗ −µπT,r
≤
DKL ηµπ T∗ µ 1 +η µ+ η1
T
+4η
µ
E vπT 2
2
+4η
v
hD Ei (cid:0) µ (cid:13) (cid:13) (cid:1) (cid:18) v (cid:19) h(cid:13) (cid:13) i
(cid:13) (cid:13)
Westartbystatingausefulresult(whichwehavelearnedfromChengetal.,2020)thatconnectsthedualitygapwiththe
suboptimalityofthepolicyoutputbythealgorithm.
LemmaC.2. (cf. Proposition4ofChengetal.,2020)Thedualitygapat(µ ,v )satisfies
T T
G(µπ∗ ,vπT)= L(µπ∗ ;v T) −L(µ T;vπT)=ρ
∗
−ρπT.
Proof. FromEquation(13),recallthat
G(µπ∗ ,vπT)= L(µπ∗ ;v T) −L(µ T;vπT). (16)
18Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
BydefinitionoftheLagrangian,wecanwrite
(µπ∗
;v )=
µπ∗
,r + v
,PTµπ∗ ETµπ∗
=
µπ∗
,r ,
T T
L −
D E D E D E
sinceµπ∗ isavalidstationarydistributionthatsatisfiesPTµπ∗
=
ETµπ∗
. Ontheotherhand,usingthatµ ∆ and
T ∈ SA
rearrangingtermswehavethat:
(µ ;vπT)= µ ,r + vπT,PTµ ETµ +ρπT(1 µ ,1 )
L T h T i T − T −h T i
= µ T,r+P(cid:10)vπT −EvπT −ρπT1 (cid:11) +ρπT
=(cid:10) µ T(s,a ′)π T(a |s) r(s,a)(cid:11)+ p( ·|s,a),vπT −vπT(s) −ρπT +ρπT =ρπT,
Xs,a Xa′ (cid:16) (cid:10) (cid:11) (cid:17)
wherethelastequalityholdsbydefinitionofπ inthemaintextandthevaluefunctionsinEquation(10). Combiningboth
T
expressionsinEquation(16)givesthedesiredresult.
C.1.ProofofTheorem4.1
First,weprovethatthegradientnormsarebounded.Bydefinitionofthegradients,
E t kg v(t) k2 2 =E t e s′ t −e st 2 2 =E t 1 −2I {s′ t=st}+1 ≤2. (17)
h i h(cid:13) (cid:13) i (cid:2) (cid:3)
Also,usingthatr(s,a) [0,1]f eorany(s,a) (cid:13) , (cid:13)
∈ ∈S×A
g µ(t) 2
∞
≤(sm ,aa ,sx ′)|r(s,a)+v t(s ′) −v t(s) |2 ≤(1+2 kv t
k
∞)2 ≤2+8 kv t k2 ∞, (18)
(cid:13) (cid:13)
wherethelastinequ(cid:13)aelityis(cid:13)Cauchy–Schwarz.
Inwhatfollows,weletv
∗
= vπT,andderiveaboundonthedualitygapevaluatedatthiscomparatorpoint. Westartby
appealingtoLemmaC.2anddecomposingthedualitygapasfollows:
T
1
ρ∗ ρπT =E[G(v ∗;µ ∗)] E[ (v t,µ ∗) (v t,µ t)]
− ≤ T L −L
t=1
X
T
1
+ E (reg)(v ,µ ) (reg)(v ,µ ) (19)
t t ∗ t
T L −L
Xt=1 h i
T
̺
+ v E v 2 v 2 .
∗ t
T k k −k k
∞ ∞
Xt=1 h i
Bythestandardonlinemirrordescentanalysis,weobtainthefollowingupperboundonthefirsttermthatcorrespondsto
theregretoftheµ-player:
T T
(a)
E[ L(v t;µ ∗) −L(v t;µ t)]
≤
E µ ∗ −µ t,g µ(t)
t=1 t=1
X X (cid:2)(cid:10) (cid:11)(cid:3)
(b) DKL(µ ∗ kµ 1) + ηe µ T E g (t) 2 (20)
≤ η 2 µ
µ Xt=1 h(cid:13) (cid:13)∞ i
(c) (µ µ )
T (cid:13)e (cid:13)
DKL ∗ k 1 +η E 1+4 v 2
µ t
≤ η k k
µ ∞
Xt=1 h i
Here,wehaveused(a)Definition2.2, (b)CorollaryD.2with = ∆ , ℓ () = (v ; ), (u u) = (u u),
SA t t u ′ KL ′
U · −L · D k D k
̺ =0andu =µ ,aswellas(c)theboundonthegradientnormestablishedinEquation(18).
u 1 1
Asforthesecondtermthatcorrespondstotheregretofthev-player,theanalysisissomewhatmoreinvolved.Onechallenge
isthatthecomparatorpointv
∗
= vπt isdependentontheiterates. Thiswillbeaddressedattheendofthisanalysis. For
19Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
now, applying Lemma D.1 with the appropriateparametersincluding (u u) = 1 u u 2, (u) = (u) =
Du k ′ 2k − ′ k2 Hu Hv
u 2 ,u = v = 0sothat (u ) = 0,aswellasnotingthatthesquaredEuclideannormis1-stronglyconvexandis
1 1 u 1
k k H
thed∞ualnormofitselfgivesthefollowingbound:
T
E (reg)(v t;µ t) (reg)(v ∗;µ t)
L −L
Xt=1 h i
E[ (v v )] η T T
≤ Dv η ∗ k 1 + 2v E kg v(t) k2 2 + E[ hg v(t) −g v(t),v ∗ −v t i]
v
Xt=1 h i Xt=1
1 eT e
E[ (v v )]+2η T + E[ g (t) g (t),v v ].
≤ η Dv ∗ k 1 v h v − v ∗ − t i
v
t=1
X
e
ThelastinequalityfollowsfromusingthatE g (t) 2 2.
k v k2 ≤
h i
We are left with the problemof boundingthe laest term. We first observethatE T t=1hg v(t) −g v(t),v
t
i
= 0 dueto
the
Ft
−1-measurabilityof v t, so in factall thatremainsis boundingE T t=1hgh vP(t) −g v(t),v
∗
ie. This riequiressome
care because v
∗
= vπT dependson the entire sequence of iterates, andh Pthus we cannotmake direict use of the fact that
E [g (t) g (t)]=0. Wewilladdressthisissueviaareductiontoonlinelearninginspireedbythemartingaletailbounds
t v − v
ofRakhlin&Sridharan(2017).
Tothisend,eletusconstructanauxiliaryonlinelearninggamewhereineachroundt=1,2,...,T,thefollowingstepsare
repeatedbetweenanonlinelearneranditsenvironment:
• Theonlinelearnerchoosesafunctionv RS,
t
∈
• theenvironmentchoosesthecostfunctionc =g (t) g (t),
b t v − v
• theonlinelearnerincurscost c ,v andobservesc .
t t t
h i e
Byconstruction,wehavethat kc t k2 ≤b kc t k1 ≤ 2. Wewillstudythecasebelowwheretheonlinelearnerexecutesonline
gradient descent on the sequence c T , initialized at v = v and using stepsize η . By standard techniques (e.g.,
{ t }t=1 1 1 v
LemmaD.1with̺ =0and),theregretofthismethodcanbeboundedagainstanycomparatorv asfollows:
u ∗
b
T v v 2 η T v v 2
hc t,v t −v ∗ i≤ k ∗ 2− η 1 k + 2v kc t k2 2 ≤ k ∗ 2− η 1 k +2η vT.
v v
t=1 t=1
X X
b
Then,thetermweseektoboundcanbecontrolledasfollows:
T T T T
E
"
hg v(t) −g v(t),v ∗ i#= −E
"
hc t,v ∗ i#=E
"
hc t,v t −v ∗ i#−E
"
hc t,v t
i#
t=1 t=1 t=1 t=1
X X X X
e E v v 2 b b
∗ 1
k − k
+2η T,
≤ h 2η i v
v
whereinthelaststepweusedourregretboundstatedjustabove,andalsothatE T c ,v =0,whichholdsbecause
t=1h t t i
thesuminquestionisamartingale.Indeed,notethatforanyt,wehave h i
P
b
E [ c ,v ]= E [c ],v = 0,v =0,
t t t t t t t
h i h i h i
whichholdsduetothefactthatv waschosenbeforec wasrevealedtotheonlinelearner. Overall,thisproves
t t
b b b
T E v v 2
E b g (t) g (t),v v k ∗ − 1 k +2η T,
" h v − v ∗ − t i#≤ h 2η v i v
t=1
X
e
20Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
thusverifyingtheinequality
T E v v 2
E (reg)(v ;µ ) (reg)(v ;µ ) k ∗ − 1 k2 +4η T.
L t t −L ∗ t ≤ h η i v
v
Xt=1 h i
PuttingtheaboveinequalitytogetherwithEquations(19)and(20),wefinallyobtainthefollowingbound:
T E v v 2 T
E[G(v ∗;µ ∗)]
≤
DKL η(µ T∗ kµ 1) + η Tµ E 1+4 kv
t
k2 + hk ∗ η−
T
1 k2
i
+4η v+ ̺ Tv E kv
∗
k2 −kv
t
k2 .
µ ∞ v ∞ ∞
Xt=1 h i Xt=1 h i
Recalling the choice v = 0, choosing ̺ = 4η , and bounding v v we obtain the result claimed in the
1 v µ k ∗ k ≤ k ∗ k2
theorem. ∞
21Dealingwithunboundedgradientsinstochasticsaddle-pointoptimization
D. AuxiliaryLemmas
Lemma D.1. (cf.Theorem8ofDuchietal., 2010) Letℓ : Rbe convex,g (t) ∂ℓ (u )andg (t) besuchthat
E [g (t)] = g (t). Givenu , defineg˜ (1) Rmt andU th→ esequenceofvecu tors ∈ (u ,t g (t t)) T u via thefollowing
t u u 1 ∈ U u ∈ { t u }t=2
recursionfort [T]:
e
∈
1
e u = argmin u,g (t) +̺ (u)+ (u u ) . e (21)
t+1 h u i u Hu η Du k t
u ∈U (cid:26) u (cid:27)
Supposethedistance-generatingfunctionω uisγ u-st eronglyconvexwithrespectto k·ku. Foranyu
∗
∈U,
T
E ℓ(reg)(u ) ℓ(reg)(u )
t t − t ∗
Xt=1 h i
E[ (u u )] η T T
≤ Du η ∗ k 1 + 2γu E kg u(t) k2 u, +̺ uE[ Hu(u 1)]+ E[ hg u(t) −g u(t),u ∗ −u t i].
u u ∗
Xt=1 h i Xt=1
where e e
ℓ(reg)(u)=ℓ (u)+̺ (u). (22)
t t u Hu
Proof. Usingthedefinitionofℓ(reg),considertheregretintermsoftheregularizedloss:
t
ℓ(reg)(u ) ℓ(reg)(u )=ℓ (u ) ℓ (u )+̺ (u ) ̺ (u )
t t − t ∗ t t − t ∗ u Hu t − u Hu ∗
= ℓ (u ) ℓ (u )+̺ (u ) ̺ (u ) +̺ (u ) (u ) .
t t t ∗ u u t+1 u u ∗ u u t u t+1
− H − H H −H
(cid:16) (cid:17)
(cid:0) (cid:1)
Toproceed,weleth (t+1) ∂ (u ),sothatwecanusetheconvexityofℓ and toboundthefirstsetofterms
u u t+1 t u
∈ H H
as
ℓ (u ) ℓ (u )+̺ (u ) ̺ (u )
t t t ∗ u u t+1 u u ∗
− H − H
g (t),u u +̺ h (t+1),u u
u t ∗ u u t+1 ∗
≤h − i h − i
= g (t),u u +̺ h (t+1),u u + g (t) g (t),u u . (23)
h u t − ∗ i u h u t+1 − ∗ i h u − u t − ∗ i
Before we proceed to bound the first two terms, note that u in Equation (21) is a solution to a constrained convex
e t+1 e
optimizationproblem,andasaresultitsatisfiesthefollowingoptimalityconditionforanyu :
∈U
1
u u ,g (t)+̺ h (t+1)+ ( ω (u ) ω (u )) 0. (24)
− t+1 u u u η ∇ u t+1 −∇ u t ≥
(cid:28) u (cid:29)
Thus,weboundthefirsttwotermsonetheright-handsideoftheinequality(23)asfollows:
hg u(t),u t −u ∗ i+̺ u hh u(t+1),u t+1 −u ∗
i
= g (t)+̺ h (t+1),u u + g (t),u u
h u u u t+1 − ∗ i h u t − t+1 i
e
1
= eg u(t)+̺ uh u(t+1)+
η
( ∇ω u(u t+e1) −∇ω u(u t)),u t+1 −u ∗
(cid:28) u (cid:29)
1
+e
η
h∇ω u(u t+1) −∇ω u(u t),u
∗
−u
t+1
i+ hg u(t),u
t
−u
t+1
i
u
(a) 1
ω (u ) ω (u ),u u + ge(t),u u
≤ η h∇ u t+1 −∇ u t ∗ − t+1 i h u t − t+1 i
u
(b) 1 1
=
η
Du(u
∗
ku t) −Du(u
∗
ku t+1)
− η
Du(uet+1 ku t)+ hg u(t),u
t
−u
t+1
i
u u
(cid:16) (cid:17)
(c) 1 γ
(u u ) (u u ) u u u 2 +eg (t),u u
≤ η Du ∗ k t −Du ∗ k t+1 − 2η k t+1 − t ku h u t − t+1 i
u u
(cid:16) (cid:17)
1 γ η 1
(u u ) (u u ) + u sup ug (t),u e u 2
≤ η u (cid:16)Du ∗ k t −Du ∗ k t+1 (cid:17) η u u (cid:18)(cid:28)γ u u (cid:29)− 2k ku (cid:19)
22 eDealingwithunboundedgradientsinstochasticsaddle-pointoptimization
2
( =d) 1 (u u ) (u u ) + γ u η ug (t)
η Du ∗ k t −Du ∗ k t+1 2η γ u
u u (cid:13) u (cid:13)u,
(cid:16) (cid:17) (cid:13) (cid:13) ∗
= η1 Du(u
∗
ku t) −Du(u
∗
ku t+1) + 2η γu k(cid:13) (cid:13)g u(te) k2
u,
(cid:13) (cid:13).
u u ∗
(cid:16) (cid:17)
We have used (a) the optimality condition stated in Equation (24), (b) the seo-called three-points identity of Bregman
divergences(cf.Lemma4.1in(Beck&Teboulle,2003)),(c)thestrongconvexityof ( u )and(d)thefactthatforany
u t
D ·k
norm ,wehavesup u,g 1 u 2 = 1 g 2.
k·k u h i− 2k k 2k k
∗
n o
Thus,puttingtogetheralltheabovecalculations,wearriveatthefollowingbound:
ℓ t(u t) ℓ t(u ∗)+̺
u
u(u t+1) ̺
u
u(u ∗)
− H − H
g (t),u u +̺ h (t+1),u u + g (t) g (t),u u
≤h u t − ∗ i u h u t+1 − ∗ i h u − u t − ∗ i
1 η
(u u ) (u u ) + u g (t) 2 + g (t) g (t),u u .
≤ ηe Du ∗ k t −Du ∗ k t+1 2γ k u ku, h ue − u t − ∗ i
u u ∗
(cid:16) (cid:17)
Furthermore,plugginginthedefinitionofℓ(reg) weget e e
t
ℓ(reg)(u ) ℓ(reg)(u )=(ℓ (u ) ℓ (u )+̺ (u ) ̺ (u ))+̺ (u ) (u )
t t − t ∗ t t − t ∗ u Hu t+1 − u Hu ∗ u Hu t −Hu t+1
1 η (cid:16) (cid:17)
(u u ) (u u ) + u g (t) 2 + g (t) g (t),u u
≤ η Du ∗ k t −Du ∗ k t+1 2γ k u ku, h u − u t − ∗ i
u u ∗
(cid:16) (cid:17)
+̺ u u(u t) u(u t+1) . e e
H −H
(cid:16) (cid:17)
Hence, taking marginalexpectationson both sides, summing overt = 1, ,T steps, evaluatingthe telescoping terms
andupperboundingsomenegativetermsbyzero,wefinallyobtainthefoll· o· w· ingboundonthetotalregretofCOMIDon
theregularizedobjective:
T
E ℓ(reg)(u ) ℓ(reg)(u )
t t − t ∗
Xt=1 h i
E[ (u u )] η T T
≤ Du η ∗ k 1 + 2γu E kg u(t) k2 u, + E[ hg u(t) −g u(t),u t −u ∗ i]+̺ uE[ Hu(u 1)].
u u ∗
Xt=1 h i Xt=1
e e
Thiscompletestheproof.
CorollaryD.2. Supposethesequenceofvectors (x ,g (t)) T isasdescribedabove.Ifthecomparatorx isfixedand
{ t x }t=1 ∗
independentoftheiterates,thefollowinginequalityholds:
e
T T
(u u ) η
E ℓ t(reg)(u t) −ℓ t(reg)(u ∗)
≤
Du η∗ k 1 + 2γu E kg u(t) k2
u,
+̺ uE[ Hu(u 1)].
u u ∗
Xt=1 h i Xt=1 h i
e
Proof. Since (u ,y ) is -measurable, E [g (t)] = g (t) and u does not depend on the iterates, the term
t t Ft −1 t u u ∗
g (t) g (t),u u iszeroinexpectation.Precisely,
h u − u ∗ − t i
e
E[ g (t) g (t),u u ]=E[E [ g (t) g (t),u u ]]
e h u − u ∗ − t i t h u − u ∗ − t i
=E[ E [g (t) g (t)],u u ]=0
h t u − u ∗ − t i
e e
Thestatedresultfollowsfromthisobservation.
e
23