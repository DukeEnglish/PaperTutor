Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP
Guided Reinforcement Learning
AntoineChaffin EwaKijak VincentClaveau
IRISA,IMATAG IRISA IRISA,CNRS
Rennes,France Rennes,France Rennes,France
antoine.chaffin@irisa.fr ewa.kijak@irisa.fr vincent.claveau@irisa.fr
Abstract input image and other (similar) ones. For instance, ”one
person is standing” can be considered as a correct caption
Trainingimagecaptioningmodelsusingteacherforcing forseveralimagesshowingsomeone:itisacorrectsentence
results in very generic samples, whereas more distinctive that fundamentally describes such images, yet it is not de-
captions can be very useful in retrieval applications or to scribing specifically a given image more than another. In
produce alternative texts describing images for accessibil- contrast to generic ones, distinctive captions are more in-
ity.ReinforcementLearning(RL)allowstousecross-modal formativeanddescriptive. Thisisanexpectedpropertyfor
retrievalsimilarityscorebetweenthegeneratedcaptionand retrieval applications, by indexing images using an appro-
the input image as reward to guide the training, leading priatetextualrepresentation,ortoprovidefurtherdetailsto
tomoredistinctivecaptions. Recentstudiesshowthatpre- peoplewithvisionimpairment.
trainedcross-modalretrievalmodelscanbeusedtoprovide Captions in standard datasets [11, 13, 22, 23] only de-
this reward, completely eliminating the need for reference scribe the most salient objects in the image, that are of-
captions. However, we argue in this paper that Ground ten common to many images. Thus, captioning models
Truth (GT) captions can still be useful in this RL frame- trained to match Ground Truth (GT) captions tend to gen-
work. We propose a new image captioning model training erate overly generic captions, and often produce the exact
strategy that makes use of GT captions in different ways. samecaptionfordifferentimagesthatsharethesameglobal
Firstly,theycanbeusedtotrainasimpleMLPdiscrimina- semantics[3,5,6,8,27,28].Thereasonisthataneasyway
torthatservesasaregularizationtopreventrewardhacking tooptimizeusualimagecaptioningmetricsbasedonword
and ensures the fluency of generated captions, resulting in matchingistogeneratewordsthatarecommonacrosstrain-
a textual GAN setup extended for multimodal inputs. Sec- ingsamples,andnottogenerateveryspecificwordsthatare
ondly, they can serve as additional trajectories in the RL presentinveryfewcaptions.
strategy,resultinginateacherforcinglossweightedbythe The distinctiveness of a caption can be measured by a
similarityoftheGTtotheimage. Thisobjectiveactsasan cross-modal retriever: the generated text should allow to
additional learning signal grounded to the distribution of retrieve the target image among all others [5, 14]. A Lan-
theGTcaptions. Thirdly,theycanserveasstrongbaselines guageModel(LM)canthusbetrainedtogeneratetextsthat
whenaddedtothepoolofcaptionsusedtocomputethepro- optimize the retrieval score using Reinforcement Learning
posedcontrastiverewardtoreducethevarianceofgradient (RL)bylearningfromgeneratedsequencesthatyieldhigh
estimate. Experiments on MS-COCO demonstrate the in- score. Recently, advances in cross-modal retrieval mod-
terest of the proposed training strategy to produce highly els enabled the use of fixed pre-trained models such as
distinctivecaptionswhilemaintaininghighwritingquality. CLIP [19, 22] to guide the generator towards distinctive
captions [3, 34, 36]. Using a fixed cross-modal retriever
reducestheriskofthegeneratorandtheretrievercoopera-
tivelyconvergingtowardssomethingclosertoahashfunc-
1.Introduction tionratherthantonaturallanguage. However,sincethere-
triever has not been trained to evaluate the quality of the
Image captioning is the task of generating a description of inputtext,butonlyitsrelevancetotheimage,itmayassign
the semantics of an image in natural language. One major averyhighsimilarityscoretoill-formedsequences,andthe
challengeinthisdomainistogeneratedistinctivecaptions, LMwillultimatelyproducenon-readablecaptions. Aregu-
thatis,adescriptionthatallowstodistinguishbetweenthe larizationofthegeneratedsequencesisthusstillneededto
4202
beF
12
]LC.sc[
1v63931.2042:viXraFigure1.Examplesofimageswithanoverlygenericgroundtruthcaption,acaptiongeneratedbyamodelwithoutregularization(leading
torewardhacking),andthecaptiongeneratedbyourapproach(well-writtenanddistinctive).
avoiddriftingtoomuchfromthenaturallanguage. by optimizing weights θ of a neural network which out-
In this work, we propose a training method taking ad- puts a probability distribution over the dictionary for the
vantage of GT captions to optimize the trade-off between next token given the input ones, i.e p (x | x ) at
θ t 1:t−1
thedistinctivenessandthewritingqualityofgeneratedcap- a given time step t. As only the exact ground truth se-
tions, illustrated in Figure 1. The use of cross-modal re- quenceisguaranteedtoberightandthecorrectnessofeven
trieval models in RL frees from the need for target refer- verysmallvariationsofitisunknown,themodelistrained
encecaptions, becausethescoreoftheproducedsequence to predict the next token from the GT xgt given previous
is computed by its similarity with the image rather than GT tokens, by optimizing its probability through a cross-
comparing it to a reference sequence. However, we argue entropylossbetweenthetargettokenandtheoutputdistri-
thattheycanstillbeusefulinthissetup. First,theycanbe bution. This results in the Teacher Forcing (TF) loss [30]:
usedtotrainasimpleMLPtodistinguishbetweenrealand L(θ) = −(cid:80)T logp (xgt | xgt ). Abasicapproachto
t=1 θ t 1:t−1
generated samples. This discriminator can replace manu- trainanimagecaptioningmodelistotrainaLMtoproduce
ally defined regularization criteria from other approaches thecaptionxgtwhilebeingconditionedtotheinputimagei
that leverage pre-trained CLIP models. This results in a usingteacherforcing. Theimagecanbeseenasadditional
GAN[7]environment,wherethediscriminatorandthegen- previoustokensusedascontext,resultinginaverysimilar
eratorimprovetogether.Second,theycanbetreatedasgen- loss: L (xgt)=−(cid:80)T logp (xgt |xgt ,i).
θ t=1 θ t 1:t−1
eratedsequencesintheRLparadigm,resultinginateacher Imagecaptioning,whentrainedthroughTF,suffersfrom
forcing objective weighted by the similarity score of the thesameissuesasanytextgenerationtask. Firstly,theex-
caption to the image, thus promoting the most descriptive posurebias[20]inducedbythemismatchbetweenthetrain-
captionsamongthese.Thisallowstolearntogeneratemore ingandthegenerationprocess. Themodelisneverexposed
distinctivecaptionsusingonlyGTcaptions. Couplingthis toitsmistakesduringtrainingbutwillsufferfromerrorac-
objective with the more traditional RL one computed on cumulation at test time. Secondly, TF only considers one
samplesgeneratedbytheLMallowstoperformexploration targetsequence,whereasmanydifferentsequencescanrep-
whilehavingalearningsignalgroundedtothehumandis- resent the same semantic content and be valid targets. Fi-
tribution that shares the same objective. Finally, GT can nally,thelossisdefinedatthetokenlevel,whilethequality
furtherbeusedascandidatebaselinesinourproposedcon- ofasampleisdefinedatthefinishedsequencelevel.
trastiverewardthatusesthestrongestbaselineinabatchto
reducethevarianceofthegradientestimation.
ReinforcementLearning. Onewaytoovercomethelim-
Background on training distinctive image captioning
itationsofTFistodirectlyoptimizeasequencelevelevalu-
models is first introduced in Section 2. Then, we present
ationmetricthroughRL[20,33]. Thisobjectivecanbeany
ourproposedapproachbyintroducing1)theuseofthedis-
standard NLP metric such as BLEU [31] or ROUGE [18].
criminator,2)theuseofgroundtruthasadditionaltrajecto-
In the context of image captioning, the metric commonly
riesand3)thecontrastiverewardsinSection3. Finally,we
optimized [9, 12, 21, 26, 35] is CIDEr [25]. These met-
comparetheresultsoftheapproachtoamodeltrainedfol-
ricsarecomputedatthesequencelevelbycomparingsam-
lowing[3]andprovidesomeinsightsonourdifferentcon-
pled sequences to GT references. Thus, they are non-
tributionsthroughanablationstudyinSection4.
differentiableandareoptimizedthroughtheREINFORCE
algorithm [29]. REINFORCE estimates the gradient by
2.RelatedWork
samplingsequencesfromthemodel.TheLMisthentrained
TeacherForcing. Languagemodelingestimatestheprob- tooptimizetheloglikelihoodofthebestonesbyscalingthe
abilitydistributionofsequencesofsymbolsx ,x ,··· ,x associatedgradientbasedontheobtainedreward(e.g., the
1 2 T
(called tokens) taken from a vocabulary V, with variable CIDErscore). Forageneratorparameterizedbyθ,agener-
lengthsT.Givenatrainingsetoftexts,aLMcanbetrained atedsequence(Monte-Carlosample)xanditsrewardr(x),thegradientbecomes: ∇ L (x)=−r(x)∇ logp (x). thanwhatwouldbetractablebyevaluatingtheconditional
θ θ θ θ
A baseline b can be subtracted from the reward to re- probabilityofeverysequenceforthismodel.[3]useCLIP
ducethevarianceofthegradientestimate,aslongasitdoes score in the SCST framework to train the model and fine-
not depend on the sample x (so the expected gradient is tune its text encoder to detect grammatical mistakes in or-
the same): ∇ L (x) = −(r(x)−b)∇ logp (x). Self- der to prevent reward hacking. [36] improves over SCST
θ θ θ θ
CriticalSequenceTraining(SCST)[21]isthemostwidely by replacing the self-critical baseline with the CLIP simi-
usedmethodfortrainingimagecaptioningmodel. SCSTis larityofthegeneratedcaptiontoagroupofsimilarimages
anextensionofREINFORCEthatusesthemodelitselfasa andaddaCIDErrewardtopreventthemodelfromdiverg-
baselinetonormalizetherewards. Duringthetraining,the ing. These rewards only focuse on either text-to-image or
current model will be used to generate a sequence xˆ using image-to-text retrieval, whereas our approach considers a
test-time decoding method (e.g. Greedy Search (GS)) and whole batch of similar captions and images, thus consid-
usesitsrewardasabaseline(b=r(xˆ))forasequencegen- ering both directions. Moreover, rather than a fixed gram-
eratedusingabetterdecodingmethod,suchasBeamSearch mar network/CIDEr score, we use an evolving discrimina-
(BS).Themodelprobabilitiesofsamplesthatarebetterthan torwhichadaptstothegeneratorandpreventsemergingbe-
theactualmodelwillbeincreasedandtheonesofsamples haviors that are not observable at the sequence level (e.g.,
that are worse will be decreased. Hence, SCST optimizes lowdiversitythatcanonlybemeasuredfromasetofgen-
asequenceevaluationmetricasREINFORCE,butstrongly erated sequences). [8] identifies the limited vocabulary of
reduces the variance induced by sampling a full sequence a RL-trained model as a bottleneck for discriminativeness,
whilealsoavoidingtolearnacritic[15,24]thatestimates preventingthemodelfromusinglow-frequencywordsthat
theexpectedfuturerewardforagivensub-sequence. areneededtocorrectlydistinguishoneimagefromanother.
However, BLEU,ROUGEorCIDErmetricsarenotto- Thisvocabularycollapseappearsbecauseonlywordssam-
tallycorrelatedwithhumanjudgmentandoptimizingthem pled by the model obtain rewards, causing less frequent
directlymightleadtobiasedresultsratherthanhuman-like words to be less and less frequent [4]. [27] shows that
ones [16]. A less biased metric is the score of a discrimi- using teacher forcing during the RL training limits the vo-
nator trained to differentiate the distributions of generated cabulary collapse. Our proposed weighted teacher forcing
versus real texts. Guiding the generator towards a distri- combinestheTFandRLobjectives,byusinggroundtruth
bution that is indistinguishable from the real one would captionswhichcontainwordsthatmightnotbesampledby
result in a perfect generator. Generative Adversarial Net- themodelfortheRLobjective.
works (GANs) [7] allowed massive improvements in gen-
erativetaskssuchasimagesgeneration,thankstotheirca- 3.Method
pacity to approximate continuous data distribution. How-
TheproposedmethodextendstheRLparadigmthatusesthe
ever,fordiscretedatasuchastext,thegradientflowcannot
similarityscorebetweenageneratedcaptionanditsimage
be back-propagated from the discriminator to the genera-
fromthecross-modalretrieverCLIPasthereward.
tor,thereforetheproblemiscommonlycastasareinforce-
fI(i)andfT(x)arerespectivelytheCLIPembeddings
mentlearningproblemusingscoresofthediscriminatoras
ofimageiandtextsequencex. Therewardr(x)isatrade-
rewards [20, 33]. These scores serve as a learning signal
off between r (x), the similarity of fT(x) with fI(i),
whichisnotaffectedbytheexposurebiasanddoesnotrely sim
andr (x), aregularizationbasedonthewritingquality
onmanuallydesignedmetricsthatcanbebiasedtoevaluate regu
ofthesample,controlledbyaparameterα. Thegradientis
thequalityofasample.
thengivenby:
Distinctive image captioning. Contrastive Learning for ∇ L (x)=−r(x)∇ logp (x)
θ θ θ θ
Image Captioning [5] introduces the distinctiveness prop- (1)
with r(x)=αr (x)+(1−α)r (x)
erty of image captioning models. The generator is trained sim regu
using the log-ratio of probabilities of the model with re-
3.1.Overview
spect to a reference baseline model on positive and nega-
tivepairscreatedbyrandomlyswappingthepositivepairs. TheproposedlearningschemeisdepictedinFigure2. For
The goal of the model is to assign higher probabilities to eachimageiinabatch,associatedwithitsgroundtruthcap-
positivepairs(respectively,lowertonegativeones)thanthe tion xgt, similar images are mined and generated captions
reference model. Our approach, on the other hand, does xbs and xgs are sampled from the LM using respectively
notdirectlyworkonsequences probabilities, that areopti- beamsearchandgreedysearchdecoding. Allcaptionsand
mizedusingreinforcementlearningasaproxy. Leveraging imagesareprojectedintheCLIPembeddingspace. Those
adualencodermodel(CLIP)allowstocomputescoresfor representations then are used to compute both terms r
sim
everypairinthebatchandtoconsidermuchmorecouples and r of the reward (1). This reward is computed for
reguFigure2. Proposedcaptioningmodellearningoverview. Generatedandground-truthcaptions,aswellasinputandminedsimilarimages,
areprojectedintheCLIPembeddingspace.Thoserepresentationsareusedtocomputetherewardcomposedofadiscriminatorscore(Sec-
tion3.2)andaCLIP-basedbidirectionalcontrastivesimilarityscore(Section3.4),forbeamsearchandground-truthsamples(Section3.3)
(inblueintherewardcomputationbloc).
BS samples xbs, while GS samples xgs and mined images resentationscomputedbyCLIPeasilyachievesaveryhigh
serveasbaselinesinther computation. discriminationaccuracy.
sim
TheGTcaptionsxgtareleveraged:
(i) to train a simple discriminator D in the CLIP embed- TheprobabilityP (x)ofasequencextobefromahu-
D
ding space that discriminates between GT samples and mangivenbyDcanbeusedasr (x). Itisworthnoting
regu
the LM-generated BS ones, used as the regularization that,contrarytothegrammarheadof[3],thediscriminator
termr regu(Section3.2), is trained without fine-tuning the text encoder of the CLIP
(ii) as additional training samples using the RL objective model, preventing a mismatch between optimizing the re-
(Section3.3), trieval model used for training and the one used at test-
(iii) ascandidatebaselinesfortherewardr sim(Section3.4). time. Although being less discussed in the literature, the
discriminatorisalsousefultopreventtheLMfromdegener-
3.2.PreventingRewardHackingUsingaDiscrimi-
atinginanothersituation:awell-writtencaptionbutinsuffi-
nator
cientlyspecifictotheimage(Figure1). Suchcaptionswill
Duringtheexplorationofthespacebythepolicy(theLM), obtain negative rewards because CLIP scores them poorly,
bad sequences with high rewards might be produced, for butloweringtheirlikelihoodmayleadthemodeltounlearn
examplebyrepeatingimportantkeywordsforCLIP(reward the grammar and correct sequence structure. While posi-
hacking),orproducingout-of-domainsequencesthatobtain tiverewardsattractthemodeldirectlytowardthetrajectory,
near-randomrewards. Whenill-formedsequencesgethigh negative rewards push it away in an unknown direction. It
scores,themodellearnstoreproducethemmoreandmore, causes the model to produce random sequences that might
untilthemodelfullycollapsesonthisbaddistribution(see makethemodeltotallycollapse.Theweightofthediscrim-
Figure1). inatorintherewardshouldthusbelargeenoughtoprevent
Topreventlearningfromsuchill-formedsolutions,pre- bothunlearningwell-writtentextsandlearningfromreward
vious approaches [3, 34, 36] use different criteria to regu- hackingsamples.
larizethetraining: detectionofrepetitionsandgrammatical
errors,divergencefromthedistributionoftheoriginalLM, Compared to the grammar network of [3] or the CIDEr
orCIDErvalue. However, wearguethatallofthesecrite- scoreof[36], ourdiscriminatornotonlygivesinformation
riacanbeencapsulatedinasinglediscriminatorD trained atthesentencelevel,butmoregenerallyonthedistribution
todiscriminatebetweenGTsamplesandtheLM-generated ofthegeneratedtextsandcanadapttoemergentbehaviors
ones. AverysimpleMLPclassifiertakingasinputtherep- ofthelanguagemodelthatcouldtrickafixedmodel.3.3.Reward-WeightedTeacherForcing ward,usedassimilarityrewardr in(1):
sim
SincetheCLIPrepresentationsofthegroundtruthcaptions r bicont(x c)=r i2t(x c)+r t2i(x c)
a thre erc lo em vep ru agte ed thto emtra toin trt ah ie nd this ecr gi em ni en ra at to ivr, ew me odp ero l.p Ros ee int fo orf cu er-
-
=τ
log
efT(xc) τ·fI(ic)
ment learning scores trajectories (sequences of words in
(cid:80) x∈T\xcefT(x) τ·fI(ic)
contextoftextgeneration)andlearnsfromthosethatscored 
well. Goodsampledsequencesarethusrequiredsothatthe
efT(xc) τ·fI(ic)
+log  (2)
m allo od we slc toan finle da grn oofr do sm olt uh te iom n. s,W ith hi ale st ah gis ree ax tp vl ao rr ia at nio cn eap nro dc ce as ns (cid:80) i∈I\icefT(xc τ)·fI(i)
leadtodegeneratesolutions(rewardhacking).
Please note that the reward more precisely corresponds
to the definition of the decoupled contrastive loss of [32]
GT captions can be considered a great source of rela-
thatexcludesthepositivecouplefromthedenominator. T
tivelygoodsolutions.Wethusproposetousethesecaptions
iscomposedofxbs,xgs,andxgtforallimagesofthebatch
asadditionaltrajectoriesfortheRLloss. Iftherewardisdi-
(seeFigure2).Iiscomposedofalltheimagesofthebatch,
rectly derived from the ground truth as in reference-based
aswellassimilarimagesminedinthedatasetandaddedto
metrics(BLEU,ROUGE,CIDEr...),GTtrajectoriesalways
thebatchasadditionalimages,followingthesetupof[36].
obtaintheupper-boundvalueoftherewardmetric. Thisre-
Notethatalthoughatext-to-imagemodelasOFA[26]can
ward is the same for every GT sequence, resulting in the
beusedtogeneratenegativeimages,inpracticethismethod
standardteacherforcingobjectivewithalearningratemul-
istoocomputationallyexpensiveandmayleadtolesshard
tipliedbythisconstant. Inourcase, thecross-modalsimi-
negativesinCLIPspace. Theminingofsimilarimagesand
larityscorer associatedtotheGTisnotconstant(some
sim thecomputationofallimagerepresentationsfI(i)isdone
GT captions are closer to their images in the cross-modal
only once before the training, and thus does not bring any
space). The resulting loss is thus equivalent to the teacher
computational overhead during training. As the represen-
forcinglossweightedbytherewardr(xgt). Werefertothis
tations fT(xgt) and fT(xbs) of GT and BS captions are
objectiveasWeightedTeacherForcing(WTF).Sincethe
alreadycomputedforthediscriminator,computingthecon-
representations of ground truth captions are already com-
trastive reward is thus inexpensive since it mainly consists
putedtotrainthediscriminator,theirassociatedrewardsare
ofdotproducts.
obtainedthroughcheapdotproducts.
Unlikepreviousapproaches,ourcontrastiverewardcon-
sidersbothdirections(bynormalizingeitherbythesimilar-
Comparedtoteacherforcing,themodelstilllearnstore-
ityoftheimagewithallthecaptionsofthebatch-–image-
producehuman-writtensequencesbutfocusesmoreonthe
to-textrewardr —,orbythesimilarityofthecaptionwith
captionsthatarehighlydescriptiveoftheirimage,allowing i2t
alltheimagesinthebatch—text-to-imagerewardr —),
to produce distinctive captions. Moreover, since these tra- t2i
makingsurethatthecaptionisverydescriptiveoftheimage
jectoriesarewrittenbyhumans,itstronglyreducestherisk
andthisimageonly. Bothpartsoftherewardcanberewrit-
ofrewardhacking. Besideshelpingthemodeltostayclose
ten as the similarity of the couple (x ,i ) minus the Log-
tothehumandistribution,italsoenablestogetbacktoitif c c
SumExp(LSE)ofthesimilaritieswithinthebatch. Witha
the model reaches a pitfall (such as reward hacking or di-
smallenoughtemperatureparameterτ,LSEisanapproxi-
vergence),allowingthemodeltorecoverandstartlearning
mationofthemaxoperator:
again. ThisisaveryhandfulpropertysinceRLtrainingis
knowntobeunstable.
 
efT(xc) τ·fI(ic)
3.4. Beyond a Single Baseline: The Bidirectional
r i2t(x c)=τlog
(cid:80) x∈T\xcefT(t)·
τfI(ic)
 
ContrastiveReward =τlog(efT(xc) τ·fI(ic)
)−log(
(cid:88) efT(x) τ·fI(ic)
)
x∈T\xc
Previous studies use either only texts [3] or images [36]
≈fT(x )·fI(i )− max {fT(x)·fI(i )}
asthebaselineandthusonlyconsideronecross-modalre- c c c
x∈T\xc
search direction. Inspired by the contrastive loss used to
train CLIP, defined for a given couple (x ,i ), a tempera- This image-to-text reward (r ) can therefore be seen
c c i2t
ture parameter τ and a collection of negative texts T and as the standard SCST where the baseline b is the hardest
negativeimagesI,wederiveabidirectionalcontrastivere- negative: the most similar caption to the image i among
cnegativesamplesT \x . Thismotivatestheuseofthede- xgt andxbs generatedbytheoriginalLMpre-trainedwith
c
coupledcontrastiveloss: notexcludingx fromthedenom- TF.Indeed,thediscriminatorshouldbegoodenoughtocor-
c
inatorwouldleadtoarewardthatisalwaysnegativeorzero, rectly guide the LM at the beginning of the training. It is
evenwhenx isthemostsimilarcaptiontoi amongevery thentrainedthroughouttheLMtrainingprocess,atthebe-
c c
captioninthebatch(thegoalofthemodel).Pleasenotethat ginning of each iteration along with the generator, on the
itreducestothestandardSCSTrewardof[3]whenthemost batchsamples.
similarcaptioninthebatchisthecaptiongeneratedforthe
imageusingGS. Metrics Different metrics are used to compare different
Appliedtothetext-to-imagereward(r t2i),thisapproxi- propertiesofgeneratedsamples. TheRecall@kmetricus-
mationisalmostequivalenttotheG minformulationin[36] ing the fixed pre-trained CLIP model (R@k) evaluates the
that uses the similarity of the most similar image as the discriminativeness of the generated caption. This metric
baseline. The proposed bidirectional contrastive reward is reported for k ∈ {1,5,10} and for both text-to-image
thus seamlessly handles both cross-modal retrieval direc- andimage-to-textretrieval(contrarytopreviousapproaches
tions and selects the strongest baselines among a large thatonlyreportonecross-modalretrievaldirection). Next,
batch,ataverylowcost. Themeansimilarityinthebatch standardCOCOcaptioningmetricsthatevaluatethewriting
isclosertotherunningaverage,oftenusedasabaselinein quality are reported, including BLEU [31], ROUGE [18],
traditional RL. However, early experiments showed that it CIDEr [25], METEOR [2] and SPICE [1]. The Self-
is not a strong enough baseline to prevent the model from BLEU[37]metric,correspondingtotheBLEU[17]metric
diverging. Theproposedrewardresultsinamoreconserva- usingothergeneratedcaptionsasreference,isalsoreported.
tive learning, only letting the model learn from very good AhighSelf-BLEUindicatesahighoverlapbetweengener-
sequences. atedsamples,implyingalowdiversity.
Finally, applying (1) with the reward defined in (2) as
4.2.AblationStudy
r and the discriminator as r to both beam search
sim regu
generatedsequences(traditionalRL)andgroundtruthcap- Wetrainedthreevariantsoftheproposedsetup, thatlever-
tions (weighted teacher forcing) captions, we end up with ages a discriminator D and uses the bidirectional con-
thefollowinggradientforagivenimage: trastiverewardr bicont:
• WTF-RL uses both BS and GT trajectories, which
∇ L (xbs,xgt)=
θ θ corresponds to the policy gradient given by (3)
(cid:20) (cid:21) ∇ L (xbs,xgt),
− r(xbs)∇ logp (xbs)+r(xgt)∇ logp (xgt) (3) θ θ
θ θ θ θ • WTF uses only GT trajectories, with policy gradient
∇ L (xgt),
with r(x)=αr (x)+(1−α)p (x) θ θ
bicont D
• RLusesonlygeneratedBStrajectories,withpolicygra-
4.Experiments dient∇ θL θ(xbs).
Following previous studies [3, 36], we measure the re- The gain brought by the bidirectional reward is studied
trieval rate achieved using generated captions and a fixed byremovingthetext-to-imagerewardfromthecontrastive
retriever as well as their writing quality on the MS COCO reward(RL-Unidirectional). Thegradientpolicyisthen:
dataset[13]usingtheKarpathysplits[10]. Toevaluatethe
∇ L (xbs)=−(αr (xbs)+(1−α)p (xbs))logp (xbs).
contribution of each component of the proposed learning θ θ i2t D θ
scheme, we train different variants of the proposed setup
ThisrewardisverysimilartoSCSTwithabaselinecorre-
(Section4.2).
spondingtothecaptionthathasthehighestsimilaritywith
4.1.Setup theimageinthebatch, insteadofconsideringonlytheGS
sample as the usual SCST. These models are compared to
Training We use the state-of-the-art captioning model the training setup of [3], using the grammar network pro-
OFA [26] in its tiny version as LM. All the models are vided by the authors and the same weighting between the
trainedstartingfromthesamecheckpoint:tiny-OFAtrained grammarandCLIPscorereward(SCST-Grammar). To
usingTFfor2epochs,forwhichwealsoreporttheresults evaluate the benefits of the discriminator, we also train a
asbaseline(TF).Themodelsarethentrainedfor5epochs, model using only the GS caption as the baseline (SCST -
using a learning rate of 1e−6, α set to 0.94, and a batch Discriminator). Thislastsetupisthesameas[3](SCST-
sizeof20groundtruthcaption-imagepairs. Grammar), but using a discriminator rather than the gram-
marnetwork.
Discriminator Thediscriminatorisa3-layerMLPandis Although our results are not directly comparable to the
first pre-trained on the MS COCO train set to distinguish ones of previous approaches due to the difference in theT2IRETRIEVAL I2TRETRIEVAL WRITINGQUALITY DIVERSITY
R@1↑ R@5↑ R@10↑ R@1↑ R@5↑ R@10↑ B4↑ R-L↑ C↑ M↑ S↑ Self-BLEU↓
TF 17.14 39.06 51.14 23.98 49.72 61.94 32.73 55.43 109 27.19 20.69 70.49
WTF 20.52 44.58 57.66 29.32 56.72 69.08 32.9 55.57 110.2 27.46 21.26 61.45
WTF-RL 33.82 61.98 73.68 44.26 73.34 83.4 24.61 51.05 86.22 25.7 20.09 57.55
RL 35.24 62.9 75.3 46.68 75.28 84.66 21.59 49 76.06 25.01 19.21 58.01
RL-Unidirectional 31.52 58.34 71.04 45.86 74.4 83.4 21.45 48.14 78.53 24.75 19.83 62.3
SCST-Discriminator 34.72 62.46 74.22 51.38 79.08 87.54 16.54 44.62 46.21 24.31 18.46 68.88
SCST-Grammar 31.84 58.98 71.10 44.0 71.86 81.92 16.35 45.23 41.24 25.31 19.72 80.66
Table1. CaptioningresultsontheMSCOCOdataset(Karpathysplits). R@kcorrespondtotheretrievalrateatkusingthefixedCLIP
modeleitherusingtextqueries(T2I)orimagequeries(I2T).WritingqualitymetricsincludesBLEU@4(B4),ROUGE-L(R-L),CIDEr
(C),METEOR(M)andSPICE(S).TheSelf-BLEUmetricmeasuresthediversity.
.
backbonegenerativemodel,pleasenotethatSCST-Gram- from generating such sequences too often. As previously
marcorrespondstothesetupof[3]andthatther reward mentioned,thediscriminatorhasaglobalviewofgenerated
t2i
subsume the reward of [36]. This allows to contextualize texts. It can thus detect some bad behaviors that are not
theresultsw.r.tthecurrentstate-of-the-art. visible at the sequence level and adapt to these emergent
behaviors.
4.3.Results
TheresultsofthedifferentmodelsreportedinTable1give
informationabouttheimpactofourdifferentcontributions.
Bidirectional reward When using only a unidirectional
UseofGTastrajectoriesfortheRL Thefirstobservable image-to-text reward (RL - Unidirectional), the text-to-
finding is that the WTF objective alone improves retrieval image retrieval metrics significantly drop compared to the
metricsoverTFusingonlyGTcaptions,withoutdegrading proposed bidirectional reward (RL). This means that the
the writing quality of the model. This shows that learning generatedcaptionsaremoredescriptiveofotherimagesin
fromthemostdistinctivehuman-writtencaptionsallowsthe the dataset than the input image. This is because, during
LMtogeneratecaptionscontainingmoreimportantdetails the training, the model has not been trained to generate a
whilestayingclosetothedistributionoftheGTcaptions. It caption that is more descriptive of the input image than of
is thus a better objective than TF to couple with the tradi- a similar image. This illustrates that considering both re-
tionalRLone.UsingGTactsasanadditionalregularization trievaldirectionsintherewardisneededtoproduceacap-
andpreventsvocabularycollapse,whileallowingtorecover tionthatishighlydescriptiveofaspecificimageonly.
ifthemodelreachesapitfallduringRLtraining. Thecom-
binationofthetwoobjectives(WTF-RL)resultsinamodel
thatachievescompetitiveretrievalresultswhilemaintaining
highwritingquality. Strongerbaseline Finally,theimage-to-textrewardonly
using the GS baseline (SCST - Discriminator) results in
Discriminator Additionally, the computed representa- higher retrieval results but lower writing quality compared
tions of the GT allow to use a discriminator to replace the to using multiple textual baselines. Indeed, selecting the
grammar network of [3]. This single replacement (SCST strongest baseline in the batch (RL - Unidirectional) low-
- Grammar/Discriminator) allows to achieve significantly ersthesimilaritypartofthereward(r ), resulting, fora
sim
higherretrievalrateswithoutdegradingthewritingquality, fixed α, in a higher weight of the discriminator. Although
resulting in a better trade-off. Besides, it also yields sub- wecannotconcludefromtheseresultsthatusingastronger
stantially lower Self-BLEU scores. Indeed, CLIP assigns baselineyieldsabetterretrieval/writingqualitytrade-off,it
great rewards to some stereotypical information structures is expected to reduce the variance even more and prevent
suchas”inthebackground”or”intheforeground”. Since themodelfromcommittingtooearly. Thiscanhelpprevent
thesesequencesaregrammaticallycorrect,theyarenotpe- the exploitation of the reward model biases, especially in
nalized by the grammar network. The discriminator, how- theearlystageofthetraining,wheretheGSsamplescanbe
ever,adaptstotheLMandlearnsthattheyareausefulclue veryweak. Werecallthatthisdoesnotbringcomputational
to detect generated samples; so, it prevents the generator overhead.5.Conclusion WA,UnitedStates,July10-15,2022,pages517–527.Asso-
ciationforComputationalLinguistics,2022. 1,2,3,4,5,6,
Westudiedhowgroundtruthcaptionscanbeusedinare-
7
inforcement learning training that leverages a pre-trained
[4] Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri
cross-modal retrieval model in which they are no longer
Abend. On the weaknesses of reinforcement learning for
required. These captions can be used as additional trajec- neuralmachinetranslation. InInternationalConferenceon
toriesfortheRLobjective, resultinginaweightedteacher LearningRepresentations,2020. 3
forcing objective that allows to ground the exploration to [5] BoDaiandDahuaLin. Contrastivelearningforimagecap-
thehumandistribution.Thisadditionalregularizationcould tioning. InAdvancesinNeuralInformationProcessingSys-
showveryusefulinasetupwheretheretrievalmodelisnot tems30:AnnualConferenceonNeuralInformationProcess-
fixed,byforcingthemodeltouseoriginalcaptionsandtheir ing Systems 2017, December 4-9, 2017, Long Beach, CA,
vocabulary while sharing the objective of generating dis- USA,pages898–907,2017. 1,3
tinctive captions. It also allows the model to recover from [6] BoDai,SanjaFidler,RaquelUrtasun,andDahuaLin. To-
the inherent instabilities of RL training. They also can be wards diverse and natural image descriptions via a condi-
tionalGAN.InIEEEInternationalConferenceonComputer
usedtotrainadiscriminatorthatwillgroundtheexploration
Vision,ICCV2017,Venice,Italy,October22-29,2017,pages
made by the policy to the human distribution by favoring
2989–2998.IEEEComputerSociety,2017. 1
human-likegeneratedsamples. Thissignalservesasareg-
[7] IanJ.Goodfellow,JeanPouget-Abadie,MehdiMirza,Bing
ularization of the writing quality of the model, subsuming
Xu,DavidWarde-Farley,SherjilOzair,AaronC.Courville,
sequence-level criteria used in previous studies while pre-
and Yoshua Bengio. Generative adversarial nets. In Ad-
ventingtheemergenceofbadbehaviorsthatarenotobserv-
vances in Neural Information Processing Systems 27: An-
ableatthesequencelevel. nualConferenceonNeuralInformationProcessingSystems
Finally, we leverage the fact that dual encoder models 2014, December 8-13 2014, Montreal, Quebec, Canada,
can compute the score of every pair in the batch at a low pages2672–2680,2014. 2,3
cost,andweusethedefinitionofthedecoupledcontrastive [8] UkyoHonda,TaroWatanabe,andYujiMatsumoto. Switch-
losstoselectthestrongestbaselineinthebatch. Thiscon- ingtodiscriminativeimagecaptioningbyrelievingabottle-
trastivereward,inadditiontobeingverysimilartotheorig- neckofreinforcementlearning. InIEEE/CVFWinterCon-
inaltrainingsetupoftherewardmodel,canbeusedinboth ference on Applications of Computer Vision, WACV 2023,
cross-modalretrievaldirections,whichisimportanttobuild Waikoloa, HI, USA, January 2-7, 2023, pages 1124–1134.
IEEE,2023. 1,3
truly distinctive captions. Our findings pave the way for
studiesthattrytoalsoimprovetheCLIPmodeljointlywith [9] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,
Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up
the captioning model. Starting from a strong pre-trained
vision-language pre-training for image captioning. In Pro-
cross-modal retriever and strongly grounding the learning
ceedingsoftheIEEE/CVFConferenceonComputerVision
to ground truth captions might help to overcome the drift-
andPatternRecognition(CVPR),pages17980–17989,2022.
ing inherent of the collaboration between the two models.
2
To enable such extensions, the code of our approach is
[10] AndrejKarpathyandLiFei-Fei.Deepvisual-semanticalign-
madepubliclyavailable.
mentsforgeneratingimagedescriptions. IEEETrans.Pat-
ternAnal.Mach.Intell.,39(4):664–676,2017. 6
References
[11] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
[1] Peter Anderson, Basura Fernando, Mark Johnson, and KenjiHata,JoshuaKravitz,StephanieChen,YannisKalan-
StephenGould. SPICE:semanticpropositionalimagecap- tidis,Li-JiaLi,DavidA.Shamma,MichaelS.Bernstein,and
tionevaluation.InComputerVision-ECCV2016-14thEu- LiFei-Fei. Visualgenome:Connectinglanguageandvision
ropean Conference, Amsterdam, The Netherlands, October usingcrowdsourceddenseimageannotations. Int.J.Com-
11-14,2016,Proceedings,PartV,pages382–398.Springer, put.Vis.,123(1):32–73,2017. 1
2016. 6 [12] XiujunLi,XiYin,ChunyuanLi,PengchuanZhang,Xiaowei
[2] SatanjeevBanerjeeandAlonLavie.METEOR:anautomatic Hu,LeiZhang,LijuanWang,HoudongHu,LiDong,Furu
metricforMTevaluationwithimprovedcorrelationwithhu- Wei,YejinChoi,andJianfengGao.Oscar:Object-semantics
manjudgments.InProceedingsoftheWorkshoponIntrinsic alignedpre-trainingforvision-languagetasks. InComputer
andExtrinsicEvaluationMeasuresforMachineTranslation Vision-ECCV2020-16thEuropeanConference,Glasgow,
and/or Summarization@ACL 2005, Ann Arbor, Michigan, UK,August23-28,2020,Proceedings,PartXXX,pages121–
USA,June29,2005,pages65–72.AssociationforCompu- 137.Springer,2020. 2
tationalLinguistics,2005. 6 [13] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
[3] Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Der- Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and
noncourt,TrungBui,andMohitBansal. Fine-grainedimage C. Lawrence Zitnick. Microsoft COCO: common objects
captioning with CLIP reward. In Findings of the Associa- in context. In Computer Vision - ECCV 2014 - 13th Eu-
tion for Computational Linguistics: NAACL 2022, Seattle, ropean Conference, Zurich, Switzerland, September 6-12,2014,Proceedings,PartV,pages740–755.Springer,2014. [23] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
1,6 Soricut. Conceptual captions: A cleaned, hypernymed,
[14] Ruotian Luo, Brian L. Price, Scott Cohen, and Gregory image alt-text dataset for automatic image captioning. In
Shakhnarovich. Discriminability objective for training de- Proceedings of the 56th Annual Meeting of the Associa-
scriptivecaptions. In2018IEEEConferenceonComputer tion for Computational Linguistics, ACL 2018, Melbourne,
VisionandPatternRecognition,CVPR2018,SaltLakeCity, Australia, July 15-20, 2018, Volume 1: Long Papers,
UT,USA,June18-22,2018,pages6964–6974.ComputerVi- pages 2556–2565. Association for Computational Linguis-
sionFoundation/IEEEComputerSociety,2018. 1 tics,2018. 1
[15] VolodymyrMnih,Adria`Puigdome`nechBadia,MehdiMirza, [24] Richard S. Sutton and Andrew G. Barto. Reinforcement
AlexGraves,TimothyP.Lillicrap,TimHarley,DavidSilver, learning-anintroduction. MITPress,1998. 3
and Koray Kavukcuoglu. Asynchronous methods for deep [25] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi
reinforcementlearning. InProceedingsofthe33ndInterna- Parikh. Cider: Consensus-based image description evalu-
tional Conference on Machine Learning, ICML 2016, New ation. In IEEE Conference on Computer Vision and Pat-
York City, NY, USA, June 19-24, 2016, pages 1928–1937. ternRecognition,CVPR2015,Boston,MA,USA,June7-12,
JMLR.org,2016. 3 2015,pages4566–4575.IEEEComputerSociety,2015.2,6
[16] JekaterinaNovikova,OndrejDusek,AmandaCercasCurry, [26] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
and Verena Rieser. Why we need new evaluation metrics Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
forNLG.InProceedingsofthe2017ConferenceonEmpiri- Hongxia Yang. OFA: unifying architectures, tasks, and
calMethodsinNaturalLanguageProcessing,EMNLP2017, modalities through a simple sequence-to-sequence learning
Copenhagen,Denmark,September9-11,2017,pages2241– framework. InInternationalConferenceonMachineLearn-
2252.AssociationforComputationalLinguistics,2017. 3 ing, ICML 2022, 17-23 July 2022, Baltimore, Maryland,
[17] KishorePapineni,SalimRoukos,ToddWard,andWei-Jing USA,pages23318–23340.PMLR,2022. 2,5,6
Zhu. Bleu: a method for automatic evaluation of machine [27] QingzhongWangandAntoniB.Chan. Describinglikehu-
translation.InProceedingsofthe40thAnnualMeetingofthe mans:Ondiversityinimagecaptioning.InIEEEConference
AssociationforComputationalLinguistics,July6-12,2002, on Computer Vision and Pattern Recognition, CVPR 2019,
Philadelphia,PA,USA,pages311–318.ACL,2002. 6 LongBeach,CA,USA,June16-20,2019,pages4195–4203.
[18] RomainPaulus,CaimingXiong,andRichardSocher.Adeep ComputerVisionFoundation/IEEE,2019. 1,3
reinforcedmodelforabstractivesummarization. In6thIn- [28] Zhuhao Wang, Fei Wu, Weiming Lu, Jun Xiao, Xi Li, Zi-
ternationalConferenceonLearningRepresentations, ICLR tong Zhang, and Yueting Zhuang. Diverse image caption-
2018,Vancouver,BC,Canada,April30-May3,2018,Con- ingviagrouptalk. InProceedingsoftheTwenty-FifthInter-
ferenceTrackProceedings.OpenReview.net,2018. 2,6 national Joint Conference on Artificial Intelligence, IJCAI
[19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya 2016,NewYork,NY,USA,9-15July2016,pages2957–2964.
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, IJCAI/AAAIPress,2016. 1
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen [29] RonaldJ.Williams. Simplestatisticalgradient-followingal-
Krueger, and Ilya Sutskever. Learning transferable visual gorithms for connectionist reinforcement learning. Mach.
modelsfromnaturallanguagesupervision. InProceedings Learn.,8:229–256,1992. 2
ofthe38thInternationalConferenceonMachineLearning, [30] RonaldJ.WilliamsandDavidZipser. Alearningalgorithm
ICML 2021, 18-24 July 2021, Virtual Event, pages 8748– forcontinuallyrunningfullyrecurrentneuralnetworks.Neu-
8763.PMLR,2021. 1 ralComput.,1(2):270–280,1989. 2
[20] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and [31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le,
Wojciech Zaremba. Sequence level training with recurrent MohammadNorouzi, Wolfgang Macherey, MaximKrikun,
neuralnetworks. In4thInternationalConferenceonLearn- Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner,
ing Representations, ICLR 2016, San Juan, Puerto Rico, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz
May2-4,2016,ConferenceTrackProceedings,2016. 2,3 Kaiser,StephanGouws,YoshikiyoKato,TakuKudo,Hideto
[21] StevenJ.Rennie,EtienneMarcheret,YoussefMroueh,Jerret Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei
Ross,andVaibhavaGoel. Self-criticalsequencetrainingfor Wang,CliffYoung,JasonSmith,JasonRiesa,AlexRudnick,
image captioning. In2017 IEEEConference on Computer OriolVinyals, GregCorrado, MacduffHughes, andJeffrey
VisionandPatternRecognition,CVPR2017,Honolulu,HI, Dean. Google’sneuralmachinetranslationsystem: Bridg-
USA, July 21-26, 2017, pages 1179–1195. IEEE Computer ingthegapbetweenhumanandmachinetranslation. CoRR,
Society,2017. 2,3 abs/1609.08144,2016. 2,6
[22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, [32] Chun-HsiaoYeh,Cheng-YaoHong,Yen-ChiHsu,Tyng-Luh
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Liu, YubeiChen, andYannLeCun. Decoupledcontrastive
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- learning. InComputerVision-ECCV2022-17thEuropean
man, Patrick Schramowski, Srivatsa Kundurthy, Katherine Conference,TelAviv,Israel,October23-27,2022,Proceed-
Crowson,LudwigSchmidt,RobertKaczmarczyk,andJenia ings,PartXXVI,pages668–684.Springer,2022. 5
Jitsev. LAION-5B:anopenlarge-scaledatasetfortraining [33] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seq-
nextgenerationimage-textmodels. CoRR,abs/2210.08402, gan:Sequencegenerativeadversarialnetswithpolicygradi-
2022. 1 ent. InProceedingsoftheThirty-FirstAAAIConferenceonArtificial Intelligence, February 4-9, 2017, San Francisco,
California,USA,pages2852–2858.AAAIPress,2017. 2,3
[34] Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel,
JaeSungPark,XimingLu,PrithvirajAmmanabrolu,Rowan
Zellers,RonanLeBras,GunheeKim,andYejinChoi. Mul-
timodal knowledge alignment with reinforcement learning.
CoRR,abs/2205.12630,2022. 1,4
[35] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,
Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.
Vinvl: Revisiting visual representations in vision-language
models. InIEEEConferenceonComputerVisionandPat-
tern Recognition, CVPR 2021, virtual, June 19-25, 2021,
pages 5579–5588. Computer Vision Foundation / IEEE,
2021. 2
[36] YouyuanZhang,JiuniuWang,HaoWu,andWenjiaXu.Dis-
tinctiveimagecaptioningviaCLIPguidedgroupoptimiza-
tion.InComputerVision-ECCV2022Workshops-TelAviv,
Israel, October 23-27, 2022, Proceedings, Part IV, pages
223–238.Springer,2022. 1,3,4,5,6,7
[37] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan
Zhang, Jun Wang, and Yong Yu. Texygen: A benchmark-
ingplatformfortextgenerationmodels.InThe41stInterna-
tionalACMSIGIRConferenceonResearch&Development
inInformationRetrieval,SIGIR2018,AnnArbor,MI,USA,
July08-12,2018,pages1097–1100.ACM,2018. 6