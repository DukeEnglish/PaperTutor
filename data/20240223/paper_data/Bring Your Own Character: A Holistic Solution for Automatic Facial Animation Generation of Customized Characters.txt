Author’spreprintversion. ToappearintheproceedingsoftheIEEEVRConference.
Bring Your Own Character: A Holistic Solution for
Automatic Facial Animation Generation of Customized Characters
ZechenBai1,2* PengChen1,3 XiaolanPeng1 LuLiu1 HuiChen1,3† MikeZhengShou2† FengTian1,3
InstituteofSoftware,ChineseAcademyofSciences,China1
ShowLab,NationalUniversityofSingapore,Singapore2
UniversityofChineseAcademyofSciences,China3
ABSTRACT
Animatingvirtualcharactershasalwaysbeenafundamentalresearch Reference Video
probleminvirtualreality(VR).Facialanimationsplayacrucialrole
astheyeffectivelyconveyemotionsandattitudesofvirtualhumans.
However,creatingsuchfacialanimationscanbechallenging,ascur-
rentmethodsofteninvolveutilizationofexpensivemotioncapture
devicesorsignificantinvestmentsoftimeandeffortfromhuman
Bring Your
animatorsintuninganimationparameters.Inthispaper,wepropose
Own Character
aholisticsolutiontoautomaticallyanimatevirtualhumanfaces.In
oursolution,adeeplearningmodelwasfirsttrainedtoretargetthe
facial expression from input face images to virtual human faces
byestimatingtheblendshapecoefficients. Thismethodoffersthe
flexibilityofgeneratinganimationswithcharactersofdifferentap-
pearancesandblendshapetopologies. Second,apracticaltoolkit
wasdevelopedusingUnity3D,makingitcompatiblewiththemost Generated Animation
popularVRapplications.Thetoolkitacceptsbothimageandvideo
asinputtoanimatethetargetvirtualhumanfacesandenablesusers
tomanipulatetheanimationresults. Furthermore,inspiredbythe
spiritofHuman-in-the-loop(HITL),weleverageduserfeedbackto Figure1: Givenatargetfacialvideoasreference,bringyourown
furtherimprovetheperformanceofthemodelandtoolkit,thereby characterintooursolutionintegratedwithUnity3D,anditautomatically
increasingthecustomizationpropertiestosuituserpreferences.The generatesfacialanimationforthevirtualcharacter.
wholesolution,forwhichwewillmakethecodepublic,hasthe
potentialtoacceleratethegenerationoffacialanimationsforusein Currently,facialblendshapecoefficientsaremainlyobtainedfrom
VRapplications.https://github.com/showlab/BYOC
humanexpert. Throughadeepunderstandingofthemeaningof
eachblendshape,humananimatorsoftenneedtodetermineblend-
IndexTerms: VirtualHuman—FacialAnimation—Blendshape—
shapecoefficientsbasedontheirimaginationofthedesiredfacial
Human-in-the-loop
expression. Alternatively, theymayfine-tunethecoefficientsus-
ingatrial-and-errorapproach,referencingadesiredfacialexpres-
1 INTRODUCTION
sion.Bothwaysaretime-consumingandlabor-intensive.Recently,
Virtual reality (VR) technology is attracting increasing attention somecommercialsoftwares[1,2]providethefunctionofcomputing
withthepopularityofMetaverseinrecentyears. Whenbuilding blendshapecoefficientsbasedonvideooraudioinput.Theyusually
virtualcharacters,itisimportanttocreatevibrantandvividfacial projecttheinputsignaltoapre-definedblendshapespace,showing
animations, as they can intuitively convey emotion and feeling, promisingresult.However,inthewiderrangeofVRapplications,
playingacrucialroleinawiderangeofVRapplications[3,9,26], it is common to introduce customized virtual characters holding
suchasenhancingemotionallychallengingexperiencesinVRgames appearancesandtopologiesthataredifferentfromthepre-defined
[30–33].However,itisnon-trivialtocreatediversefacialanimations ones.Toourknowledge,thereisstillagapintheavailablesolutions
conveniently. Traditional methods usually use dedicated motion forcreatinganimationsforcustomizedvirtualcharacters.Toaddress
capturedevicestotrackthekeypointsofhumanactorsandreplicate this,ourpaperproposesacomprehensivesolutionthatautomatesthe
themonvirtualcharacters. Approachesinindustryofteninvolve generationoffacialanimationsspecificallytailoredtocustomized
creatingfacialanimationsbycombiningmusclemovementsusing virtualcharacterswithvaryingappearancesandblendshapetopolo-
coefficients.Inthismethod,afacialexpressionistypicallyregarded gies. Bydoingso,ourapproachaimstoalleviatetheworkloadof
asasuperpositionofthemovementofdifferentmusclesintheface. humananimatorsandexpandthescopeofuseandapplication.The
Asaconsequence,apracticalandefficientfacerepresentationcalled mainideaofoursolutionisillustratedinFig.1.
blendshapehasbeendevisedforcontrollingthefacialexpressions Wefirstpresentadeeplearningmodeltoretargetthereferencefa-
ofavatars.Eachblendshaperepresentsamodeledsingleactionunit cialexpressionimage.Thisisachievedbyestimatingtheblendshape
ofthefacewithrespecttotheneutralface. coefficientsthatareabletoreplicatethereferencefacialexpression
onthetargetvirtualcharacter.Themodelconsistsofabasemodel
*ProjectinitiatedatISCAS.TheauthornowaffiliatedwithNUS. andanadaptermodel. Thebasemodelisresponsibleforextract-
†Correspondingauthor inggeneric3Dfacialparametersfromthegivenreferenceimage.
Theadaptermodelaimstoadaptthegeneric3Dfacialparameters
to specific blendshape coefficients. Specially, the base model is
character-agnostic,i.e,oncepre-trained,itisapplicabletoallthe
characters.Theadaptermodelistopology-aware,i.e,itissensitive
1
4202
beF
12
]CH.sc[
1v42731.2042:viXraAuthor’spreprintversion.ToappearintheproceedingsoftheIEEEVRConference.
totheblendshapetopologyofthevirtualcharacter.Oncetrained,the fromreal-timevideoinput.Reference[28]proposedamethodthat
wholemodeliscompatibletovirtualcharacterswithdifferenttexture estimatestheemotionalexpressionfacialparametersofvirtualchar-
appearancesandthemodelcanalsobequicklyadaptedtoavirtual actersbasedoninputaudio.JALI[15]isasystemdedicateddesigned
characterwithanewblendshapetopologybysimplyfinetuningthe for expressive lip-synchronization animation. Some commercial
adapteronaautomaticallygenerateddataset. toolshavebeendevelopedtosupportblendshape-basedanimation
WethendevelopatoolkitinUnity3Dtoprovideauser-friendly creation.Notably,thefacialretargetingproductlikeARKit[2],Face-
interfacetotheproposedmodelwithadditionaldesignonincorpo- waresuite[16],and“LiveLinkFace”[1]havegainedpopularity
ratinguserfeedback. Thetoolkitacceptsvideoorimageasinput. intheindustryfortheirabilitytocomputeblendshapeweightsand
Forvideoinput,thetoolkitwilloutputavirtualhumananimation facilitatethereplicationoffacialexpressionsonavatars.
synchronizedwiththevideo.Forimageinput,thetoolkitgenerates Wearguethatpreviousworkssufferfromthefollowingdraw-
asmoothfacialanimationbytreatingthefacialexpressioninthe backs: 1)theyareoftencoupledwithspecificcharacterrigsand
imageasthepeakintensity. Thetoolkitprovidesuserinterfaceto blendshapetopology,prohibitingapplicationsoncustomizedvirtual
modifythegeneratedresults,suchastuningsomeunsatisfiedblend- characters;2)thecommercialsoftwareareusuallyexpensivetouse.
shape coefficient values or highlighting the keyframe-of-interest. Our work differ from them in our support of customized virtual
Besides,thehumanfeedbackwillbeutilizedtofurtherenhancethe characterscreatedbytheusers.Wewillopen-sourcethesolutionto
performanceofthetoolkitandtheinternalmodel.Instantfeedback benefitthecommunity.
oncertainkeyframesactsasuserpreferenceforthecurrentanima-
tion,whichcanbequicklyappliedtoalltheframes,accelerating 2.2 CharacterAutoCreation
theprocessofanimationgeneration. Finally,theeffectivenessof
Automaticcharactergenerationtechnologyisbasedon2Dimages
theproposeddeeplearningmethodforfacialanimationhasbeen
todrivethegenerationof3Dmodels,involvingestimatingspecific
evaluated,andthedevelopedtoolkithasalsoundergonetestingto
faceattributesparametersfromthegivenimage. Wolfetal.[39]
assessitsfunctionalusabilityanduserexperience.
firstproposedamethodcalledTiedOutputSynthesis(TOS)in2017,
Thisworkcontributesaholisticsolutionforautomaticfacialani-
whichutilizesadversarialtrainingtocreateparameterizedavatarsby
mationgenerationoncustomizedcharacters,whichconsistsofthree
selectingfacialattributesfromapre-definedlibraryoffacialattribute
parts:(1)Wepresentadeeplearning-basedmethodtoretargetthe
templates.In2019,Shietal.[36]proposedamethodcalledFace-to-
referencefacialexpressionbyestimatingblendshapecoefficients.
Parameter(F2P)thatestimatesasetofcontinuousfaceparameter
Thismethoddiffersfrompreviousworksintheflexibilityofgen-
valuesbasedon2Dfacialimages.Thefaceparameterestimationis
erating animations with characters of different appearances and
carriedoutthroughaniterativesearchprocess.Subsequently,Shiet
blendshapetopologies,broadeningthescopeofuse.(2)Wedevelop
al.proposedanupgradedversionofthemethod,Face-to-Parameter
atoolkitthatencapsulatestheproposeddeeplearningmethod.The
V2(F2Pv2)[37]intheir2020study. Thismethodrequiresonly
toolkitprovidesapracticaluserinterfaceforadjustmentonthefly
oneforwardneuralnetworkoperationtocompletethefaceparame-
andalsoleverageshuman-in-the-looptoenhancetheperformanceof
terestimation,greatlyimprovingefficiency.Thetaskofautomatic
boththetoolkitandtheinternalmodel.(3)Weevaluatetheeffective-
charactergenerationappearstobesimilartoblendshapecoefficients
nessoftheproposedmodelandtheusabilityofthetoolkit,which
estimation. Thesimilarityliesinthatbothoftheproblemaimto
canpotentiallyinspirefutureworkinthedomainofvirtualcharacter
manipulate3Dcharactersthatcloselyresembletheinputfacialim-
animations. Additionally,wewillmaketheentiresolution’scode
agesbyestimatingfacialparameters.However,charactergeneration
publiclyavailabletobenefitawiderrangeofusecases.
assumesthatthecharacterhasalargeparameterspace,including
bothfacialmovementsandattributes. Inthatcase,theestimated
2 RELATEDWORK
parametersincludeidentity,expression,pose,andeventextureand
2.1 Blendshape-basedAnimationCreation hairstyle. Incontract,inanimationcreation,peoplemainlyfocus
onthefacialmusclemovementsthataffecttheexpressionsorposes,
Animation creation in VR has been a topic of interest in both
otherthanidentityorhairstyle.Ourworklieinthelineofanimation
academia and industry recently [4–6]. One traditional approach
creation.Thisactuallyposesauniquetechnicalchallengethatwe
istheuseofmotioncapturedevices[27,35],allowingforcaptur-
needtodecoupleandextracttheexpressionandposeinformation
ingandreplicatingreal-worldmovementsontovirtualcharacters.
fromthefacialimages.
Anotherlineofresearchfocusesonparametertuning.Byadjusting
specificanimationparameters,suchasfacialmusclemovementsor
2.3 3DMMand3DFaceReconstruction
deformations,researchersaimtoachieveexpressivecharacterani-
mations.Blendshapes,alsoknownasmorphtargetsorshapekeys, 3Dfacemodelingandreconstructionisaclassicproblemincom-
isacommonlyusedtechniqueincomputeranimationformodeling puter graphics, aiming at recovering 3D facial information from
complexandsubtlefacialexpressionsandmovements[20,22,23]. 2Dfacialimages. Onerepresentativeworkinthisfieldisthe3D
For3Dvirtualhumanmodels,eachblendshaperepresentsasingle MorphableModel(3DMM)proposedbyBlanzetal.[7]in1999.
facialunit,suchaseyebrows,lips,jaw,etc.Bycreatingaseriesof ThisworkusesPrincipalComponentAnalysis(PCA)toparame-
blendshapesthatrepresentdifferentpositionsandshapesoftheface, terizethe3Dmeshoftheface, thenoptimizestheparametersof
animatorscancreateawiderangeofvividfacialexpressionsby PCAtofittheinput2Dfacialimage.Buildinguponthiswork,new
blendingtheseshapestogetherinvariouscombinations.Intuitively, researchhasemergedthataimstoenhancetherepresentationpower
moreblendshapesimplymoredetaileddepictionandcontroloffacial ofMorphableModelsintermsofshape,texture,andexpression,
expression,increasingtheupperboundofexpressiveness.Currently, suchasBFM09[19],FaceWarehouse[10],andFLAME[24]. 3D
mainstreamsoftware,suchasMAYA,Blender,andUnity3D,can facereconstructionbasedon3DMMcanbeessentiallyregarded
beutilizedforeditinganddesigningvirtualhumansbasedonblend- asaparameterfittingproblembetween2Dfacialimagesand3D
shapes.Insuchsoftware,blendshapeisimplementedastheoffset facial models. In deep learning era, this problem is extensively
vectorsonsubsetverticesofthemesh. studiedbydesigninganeuralnetworkmodeltopredict3Dfacial
Recently,therearesomeattemptsonautomaticallyestimating parameters based on 2D images [11,13,14,17]. In this process,
blendshapecoefficientsfromvarioussources,includingimage(or differentiablerendererisemployedtorender3Dfacebasedonthe
video)[1,2,38],audio[28],etc. HeadBox[38]providesafacial predicted3DMMparameters,thengeneratesupervisionsignalto
blendshapeanimationtoolkitspecificallyfortheMicrosoftRocket- traintheneuralnetwork. Thisseriesofmethodsshowpromising
boxLibrary,whichsupportsdrivingthevirtualcharacteranimation performancein3Dfacereconstructionbutstillcannotaddressthe
2Author’spreprintversion.ToappearintheproceedingsoftheIEEEVRConference.
blendshapeestimationproblem.Themainproblemisthatdifferen- reconstructedimagesusingacommonlyusedlandmarkdetection
tiablerendererusedinthethesemethodsisspecificallydesignedto model[21].Thelossfunctioniscomputedasfollows:
fit3DMMparameters.Itisoftennotavailableforcustomizedvirtual
c oh fa 3r Dac Mte Mrsi hn avp ero bd eu ec nti won id. eF lo yrt vu en ria fite el dy ., Wthe eu un tii lv ize ers 3a Dlit My Mand pare rali mab ei tl eit ry
s
L lan(I,I′)= N1 ∑N ||q′ n−qn|| 2, (6)
n=1
ofBFM09asaparametricrepresentationfor2Dfacialimagesin3D
whereNrepresentsthetotalnumberoflandmark,whichis68inour
space.Moreover,this3DMMrepresentationfacilitatestheexplicit
implementation.
decouplingandextractionofexpressionandposeparameters.
Inadditiontoemployingintuitiveimage-levellossfunctions,we
3 METHOD alsoincorporateperceptuallossfunctions. Currently,popularper-
ceptuallossfunctionstypicallyinvolveutilizingpre-traineddeep
3.1 Overview
neuralnetworkstoextractimagefeaturesandmeasurethediffer-
Inthissection,wepresentadeeplearning-basedmethodforestimat- encesbetweenthesefeatures.Consideringthatthealgorithminthis
ingblendshapecoefficientsbasedon2Dfacialimages.Themethod sectionprimarilyfocusesonfaces, alarge-scalepre-trainedface
overviewisillustratedinFig.2.Thetaskcanbeformallydescribed recognitionnetwork,FaceNet[34],isutilizedtoextractfacialimage
asfollows:givena2Dfacialimagerepresentingareferencefacial features.Theperceptualisasfollows:
expression,themethodestimatesasetofblendshapecoefficients
t hh ua mt ac nan mr oe dp er lo .d Tu hc ee pth roe pr oe sf ee dre mnc ee thf oa dci mal aie nx lp yr ce oss ni so in stson ofa tw3D omvi or dtu ea lsl
.
L per(I,I′)=1− |< |f(f I( )I |) |, ·|f |( fI (′ I) ′>
)||
(7)
First,abasemodelisemployedtoregressasetof3DMMfacial
where f(·)representsthefunction(neuralnetwork)forextracting
parametersbasedonthegivenfacialimage.This3DMMfacialpa-
imagefeatures,and<·,·>representsvectordotproduct.
rameterscanberegardedastherepresentationvectorofafacein3D
Thebasemodelprojectsthehumanfacefroma2Dimagetoa3D
space.Next,anadaptermodelisproposedtoadapttheexpression
space.Asagenericrepresentation,the3DMMparametershavethe
parametersof3DMMtothetargetblendshapecoefficients.Wefirst
capacitytoreproducealargeamountofhumanfacialexpressions.
introduceeachindividualmodelinSec.3.2andSec.3.3respectively.
Furthermore,the3DMMparametersexplicitlydecoupletheface
ThenwepresentthetrainingandtestingdetailsinSec.3.4.
intoidentity,expression,andpose,therebyaddressingtheissueof
3.2 BaseModel entangledfacialattributes.Thebasemodelisacharacter-agnostic
model in this method. Once pre-trained, it does not require any
The3DMMparametersincludeidentity,expression,andtexture,as
retrainingwhentransferringthemethodtoother3Dvirtualhuman
wellascameraparameterssuchasposeandlighting. Withinthe
modelswithnewtopologicalstructures.Inotherwords,thismodel
parameterspaceof3DMM,theshapeSandtextureTofafacecan
istotallyuniversalandreusable.
berepresentedusingthefollowingapproach:
3.3 Adapter
S=S(β,γ)=S+B idβ+Bexpγ, (1)
Asshowninstage2ofFig.2,thetaskoftheadaptermodeltakesthe
T=T(δ)=T+Btδ, (2) expressionparametersasinputandpredictstheblendshapecoeffi-
cients.Thismodelisalightweightneuralnetworkmodelconsisting
whereSrepresentstheaveragefaceshape,Trepresentstheaverage ofonlytwolinearfullyconnectedlayerswithanactivationfunction
facetexture,B id,Bexp,andBt representthePCAbasisvectorsfor betweenthem.Afterthefinallayer,aClampoperatorisappliedto
identity,expression,andtexture,respectively.Thesebasisvectorsis truncatetheoutputvalueswithintherangeof0to1,whichisthe
asetoforthogonalvectorsobtainedthroughprincipalcomponent validvaluerangeofblendshapecoefficients. Duringthetraining
analysis.Correspondingly,β,γ,andδ representtheparametersfor process,meansquarederror(MSE)isutilizedasthelossfunctionto
identity,expression,andtexture,respectively.Inourimplementation, optimizethismodel.Giventhe3DMMexpressionparametersγand
thepopular2009BaselFaceModel[29]isutilizedtoconstructS, theground-truthblendshapecoefficientlabelsα,thisstagecanbe
B id,T,andBt,whiletheexpressionbasisBexpisderivedfromthe formallydescribedas:
FaceWarehouse[10]dataset.
′
We employ a neural network model based on ResNet-50 for α =Adapter(γ), (8)
regressingthe3DMMparameters. Duringtraining,givenaRGB
imageI,theneuralnetworkmodelisutilizedtoregressthe3DMM
L=||α′−α|| 2, (9)
parametersandcameraparameters.Subsequently,theseparameters whereα′ representstheoutputblendshapecoefficientspredictedby
areemployedfordifferentiable3Dfacereconstructionandrendering, theadaptermodel.Theobjectiveofmodeltrainingistominimize
resultinginthereconstructedimageI′.Formally, thelossfunctionL.
Theadaptermodelisatopology-awaremodel. Itstrainingde-
β,γ,δ,p,l=ResNet(I), (3) pends on specific blendshape topology of the virtual human. In
practical applications, it’s inevitable to encounter virtual human
′
I =DifferentiableRendering(β,γ,δ,p,l), (4) modelsthatpossesdifferentappearances,differenttopologies.How-
ever,themodel’sgeneralizationabilityisnothindered.Oncetrained,
whereβ,γ,δ,p,lrepresenttheparametersofidentity,expression,
thismodelcanbeusedonthevirtualcharactersfamilywiththesame
texture,pose,andlightilluminationrespectively.
BasedonIandI′,thismethodemploysajointtrainingapproach blendshapetopology,eventheirtextureappearancesaredifferent.
Besides,themodelcanbeeasilyandquicklyadaptedtovirtualchar-
usingbothimage-levellossandperceptual-levellosstooptimizethe
acterswithnewblendshapetopology,thanksto(1)theautomatic
model.Theimage-levellosscomprisespixellossandlandmarkloss.
datagenerationandfinetuning(willbeintroducedinSec.3.4.2),(2)
Theformulationofthepixellossfunctionisexpressedasfollows:
themodel’ssimpleandlightweightarchitecture. Thisadaptation
L (I,I′)=||I′−I|| , (5) imposesnolimitationonblendshapetopology.Thisincludes,butis
photo 2
notlimitedto,theorder,quantity,content,andnamesofblendshapes.
whichmeasurestheper-pixeldifferencesbetweentheoriginalin- Inotherwords,userscanprovidetheirhome-bakedvirtualcharacter
putimageandthereconstructedimage. Thelandmarklossfunc- modelswitharbitraryblendshapetopologies,andourmethodand
tioninvolvesdetectingfaciallandmark{qn}fortheoriginaland toolkitcanbeutilizedontheircustomizedcharacters.
3Author’spreprintversion.ToappearintheproceedingsoftheIEEEVRConference.
Pixel loss
-
Landmark loss
-
Differentiable
Renderer
Train Base Model
β - identity
γ - expression
Blendshape
Base δ - texture Adapter α - blendshape Estimation loss
Model coefficients
||α’ - α||2
p - pose
Train Adapter l - light
Animating
Usage
Human Feedback
Figure2:Illustrationoftrainingandusageofthemodel.Thefirststagetrainsthebasemodelthatregresses3DMMfacialparameters.Thesecond
stagetrainstheadaptermodeltoestimatethetargetblendshapecoefficientswiththehelpofpre-trainedbasemodel.Theusagestageutilizesthe
pre-trainedbasemodelandadaptermodeltodrivevirtualhumantoreplicatefacialexpressionsandheadposeofthereferenceimage.
3.4 TrainingandTestingDetails For each virtual character (family) with a specific blendshape
topology, we constructed a dataset of 10K data samples (8K for
Fig.2showsthethreestagesinourmethod. Thefirsttwostages
training,1Kforvalidation,and1Kfortesting).Theadaptermodelis
introducethetrainingofthebasemodelandadaptermodelrespec-
trainedwiththegenerateddataset.Specifically,therenderedvirtual
tively.Thethirdstageshowstheusageofthemodelsaftertraining.
humanfaceimagesarefedintothepre-trainedbasemodeltoproduce
3.4.1 TrainingBaseModel 3DMMparameters. Then,theadaptermodeltakestheexpression
parametersfromthe3DMMparametersasinputandpredictsthe
TheupperpartofFig.2illustratesthetrainingofthebasemodel.
blendshape coefficients. The blendshape coefficients within the
We use publicly available real-world face datasets, including
datasetareusedasground-truthtosupervisethepredictedvalues.In
CelebA[25]andLFW[18],totrainthebasemodel. CelebAand
thisprocess,theparametersofthebasemodelarenolongertrained,
LFWarehigh-qualityfacedatasetswithdiverseanddistinctvaria-
onlytheparametersoftheadaptermodelareupdated.
tionsinfactorssuchasrace,age,gender,expression,andfaceshape.
In our empirical study, we found that when generating the
Inthedatapreparationstage,wefirstperformfacedetection,align-
character-dependentdataset,injectinghumanpriorknowledgewould
ment,andcroppingusingthemethodprovidedinDlib[21].Then,
helpstabilizethetraining.Specifically,withoutanyconstraint,the
allimagesareuniformlyresizedtotheresolutionof224×224.We
blendshapecoefficientsaretotallyrandomvaluesrangingfrom0
followthetrainingscheduleroftherelatedwork[13].Inshort,we
to1,whichmaycauseinvalidfacialexpressions.Forexample,itis
taketheImageNet[12]pre-trainedweightsastheinitialization,and
unlikelyforapersontomovetheirlipsintwooppositedirections
trainthebasemodelusingAdamoptimizerwithbatchsizeof5,
simultaneously. Consideringthis,weproposeanoptionalstepto
initiallearningrateof1e−4,and500Ktotaliterations.
increasethequalityofthegenerateddataset.Thiscanbeachieved
3.4.2 TrainingAdapterModel bydefiningasetofrulesthatexcludetheinvalidvaluecombinations.
Withthepresenceofsuchrule-basedconstraints,asetofreasonable
ThemiddlepartofFig.2showsthetrainingoftheadaptermodel.As
blendshapecoefficientvaluescanbeobtained.
wementioned,theadaptermodelisatopology-awaremodel.There-
fore,whentrainingtheadaptermodelonspecificvirtualcharacter,
3.4.3 Usage
ortransferringtovirtualcharacterswithnewblendshapetopology,
weproposeanautomaticdatasetgenerationprocedurebasedonthe ThelowerpartofFig.2showstheprocedureofusingthewhole
targetvirtualcharacter,i.e,character-dependentdataset. model.Thepre-trainedbasemodelandadaptermodelcanbeused
Therequireddatasetforatarget3Dvirtualcharactermodelin- asoff-the-shelfblackboxesinthisphase.Giventhereferencefacial
cludes:(1)randomlygeneratedblendshapecoefficients,(2)virtual expressionimage, wefirstinputittothepre-trainedbasemodel
humanfacialimagesrenderedbasedontheblendshapecoefficients. toextract3DMMparameters. Theexpressionparameterisused
Regardingthegenerationofblendshapecoefficients, eachblend- tofurtherpredicttheblendshapecoefficientsusingthepre-trained
shapecanbetreatedasachannelwithvaluesrangingfrom0to1.In adaptermodel. Theestimatedblendshapecoefficientsareusedto
thatcase,thetaskofcoefficientgenerationistorandomlygenerate animatethevirtualcharacter. Theposeparameterfromthebase
avalueforeachchannel.Forthegenerationofvirtualhumanfacial modelisalsoutilizedintheanimatingprocesstoincreasefidelity.
images,therenderingfunctioninthe3Dsoftware,suchasMAYA, Intheanimationgenerationprocess,apartfromthetwofrozen
Unity3D,isutilizedtorendervirtualhumanfaceimagesbasedon models,wealsointroducehumanfeedbacksignalstoenhancethe
theblendshapecoefficients.Inourimplementation,afront-facing animationqualityaswellasfurtherboosttheperformanceofthe
virtualcameraisusedtorenderthevirtualhumanfaceimages. mode.ThedetailswillbeintroducedinSec.4.
4Author’spreprintversion.ToappearintheproceedingsoftheIEEEVRConference.
HITL Online
Adjust Apply Export
Coefficients Preference Coefficients
α* - adjusted
blendshape
HITL Offline
🔥 Blendshape
Estimation loss α* - adjusted
Adapter blendshape
||α* - α||2
User Interface
Figure3:Theleftpartistheuserinterfaceoftheproposedtoolkit.TherightpartisthetwomodesofHuman-in-the-loop(HITL).Intheonline
mode,userscanadjustblendshapecoefficientsofseveralkey-framesandthenapplythepreferencetothewholevideo.Intheofflinemode,the
adjustedblendshapewillbecollectedasground-truthdatatofinetunetheadaptermodeloffline,furtherboostingtheperformanceofthemodel.
4 TOOLKIT Blendshape-levelinteraction Inthetoolkit,theblendshape
coefficientsareautomaticallyestimatedbythedeeplearningmodel.
4.1 HolisticDesign
Apartfromthat,wealsoprovidetheinterfacetoletusersadjustthe
We develop a Unity-based toolkit that automates the process of blendshapecoefficients.Forexample,whentheuserfindsthatthe
virtualhumanfacialanimationgenerationbasedon2Dimagesor intensityof‘eye-open’blendshapedoesnotreflecttheinputfacial
videos. Thistoolkitextractsinformationoffacialexpressionand expression, he/shecaneasilytunethevalueaccordingtohis/her
headposefromvideoframesbybasemodel,andpredictsblendshape need.Thisdesignoffertheflexibilitytocorrecttheerrorproduced
coefficientsforcustomizedvirtualcharactersbyadaptermodel.The bythemodelandenhancethequalityoftheanimations.
toolkitsimultaneouslydisplaysthegeneratedfacialanimationofthe Theabovemodificationswillbedisplayedinthesceneinreal-
virtualcharacterandtheoriginalvideoframesintheuserinterface, timeforreference.Bothframe-levelandblendshape-levelinterac-
allowing for comparison and adjustment for users. This toolkit tionsarereflectionofuserpreference.
implementsahuman-in-the-loopapproach,wherehumanfeedback
are involved into the animation generation process. It not only 4.3 Human-in-the-loop
injectsuserpreferenceforthecurrentvideoathand,butalsoboost
Human-in-the-loop(HITL)referstoacomputationalframeworkor
theperformanceoftheinternaldeeplearningmodel.
systemwherehumaninvolvementisintegratedintothedecision-
4.2 BasicFunctionalities makingorexecutionprocess. Itinvolvesacollaborationbetween
humansandmachines,wherehumansprovideinput,feedback,or
4.2.1 Auto-AnimationGeneration
supervisiontoimprovetheperformanceofautomatedalgorithms
WeutilizethedeeplearningmodeldescribedinSec.3inthetoolkit or systems. Apart from automatic generation, our toolkit places
toautomaticallygeneratefacialanimations.Thespecificgeneration emphasisonhumanfeedbackandpreferences,incorporatinghuman
processdependsonthetypeofinputdata.Forsingleimageinput,the adjustment of blendshape parameters as a crucial element in the
toolkitfirstestimatestheblendshapecoefficientsbasedontheinput HITLprocess, boostingperformancebeyondmodelfromSec.3.
image,thengeneratesfacialexpressionforthevirtualhuman.The TheHITLinthetoolkithasonlinemodeandofflinemode.
generatedexpressionisusedasthepeakintensityvalues.Afterthat,
Online mode AsmentionedinSec.4.2.2,thetoolkitallows
alinearinterpolationisperformedbetweenthenaturalexpression
userstomakeblendshape-leveladjustments. IntheHITLOnline
blendshapecoefficientvalues(zeros)andthepeakintensityvalues,
mode,thedifferencesbetweentheautoestimatedblendshapevalues
gradually transitioning from zero to the peak intensity, and then
andtheadjustedvaluesarestoredasuserpreferencesinthetoolkit.
backtozero.Thisprocessensuresasmoothandgradualtransitional
Subsequently,withasingleclick,userscanapplythesepreferences
animationthatpresentstheinputfacialexpression.
toallvideoframes.Thealgorithmbehindthisdesignissimpleyet
For video input, the toolkit first estimates the blendshape co-
effective.Weaveragealltheuseradjusteddifferencevaluesasauser
efficients of each frame in the video. It then sparsely samples
preferenceδ,thenapplytheδ tothecorrespondingblendshapesof
keyframesandgeneratesthefacialanimationforthevirtualhuman
alltheremainingframes.Therefore,afew-to-manyfunctionalityis
bylinearlyinterpolatingbetweenthesekeyframes. Therationale
achieved,i.e,youonlytunefewframestoinjectyourpreferenceto
ofsamplingwithintervalisthattheblendshapecoefficientsofcon-
thethewholeanimation.Theonlinemodesignificantlyreducesthe
tinuous video frames inevitably suffer from jitter problem. This
timeandcostforanimatorstoadjustanimations,therebyreducinga
sample-and-transitionstrategyensuresasmoothanimationbetween
substantialportionoftheirworkload.
theselectedkeyframes.
Offlinemode AfterseveralroundsofonlineHITLinteractions,
4.2.2 Userinteraction
thetoolkitaccumulatesasufficientamountofhumanfeedbackand
Frame-levelinteraction Whentheinputisavideo,thetoolkit adjustments for the blendshapes. These data can be regarded as
adoptsansparsesamplingstrategyduringinitialization.Bydefault, valuablehumanannotationandbefurtherexploitintheHITLOffline
we select one keyframe every 5 frames and generate animation. mode. Specifically,weutilizetheannotationdataassupervision
Additionally,weprovidetheoptionforuserstomanuallyselectideal signal to finetune the adapter model. Considering that this data
keyframes. Forexample,theusermaywanttohighlightthemost contains enough human preference, the resulted model not only
exaggeratefacialexpression,thentheneighborframescanbeall achievesbetterblendshapeestimationaccuracy,butalsoimproves
selected.Then,thetoolkitperformssegmentedlinearinterpolation theoverallalignmenttohumanpreference.
betweenallthekeyframes,includingtheinitializedkeyframesand Therearesomedifferencesbetweenthetwomodes. IntheOn-
theuser-selectedkeyframes,toachieveasmoothanimation. linemode,users’feedbackisspecificallyprovidedforthecurrent
5Author’spreprintversion.ToappearintheproceedingsoftheIEEEVRConference.
Table1:Experimentsondifferentsettingsoftheadaptermodel.We
animation. Theadjustmentcanbemadewithrelativelylesstime
useMeanAbsoluteError(MAE)forevaluationandcomparison.
andwillbedisplayedinreal-timeonthetoolkitinterface
Incontrast,theOfflinemodeinvolvesfinetuningthemodelwith Layers Hidden-dim MAE
thedatacollectedfromdifferentvideos. Therefore,comparedto Linear→ReLU→Linear 256 0.09
preferenceforspecificanimation,thisdataembedsmoregeneric Linear→ReLU→Linear 100 0.10
andcompletepersonalintentiontoalltheanimations. Linear→ReLU→Linear 384 0.09
Fromanotherperspective,thetwomodescanbenefiteachother. Linear→LeakyReLU→Linear 256 0.09
Ontheonehand,theOnlinemodehelpsaccumulatehumanfeedback Linear→ReLU→Linear→Clamp 256 0.08
datafortheOfflinemode.Ontheotherhand,theofflinefinetuned Linear→LeakyReLU→Linear→Clamp 256 0.07
modelwillbeusedtopredictblendshapecoefficientsasinitialization
for the the online mode. This essentially builds a data flywheel “AddKeyframe”button. Userscanmanuallyinsertkeyframes
effect with human-in-the-loop. It allows for iterative refinement thattheywishtoemphasizethroughtheutilizationofthisbutton.
byincorporatinghumanexpertiseandcontinuouslyimprovingthe “ExportResults”button.Byclickingthisbutton,theuserfeed-
performanceofthesystem.Consequently,thebetterthesystemis, backwillbeexportedtoa.jsonfile,whichreferstotheadjusted
thelesshumaninterventionisrequired. blendshapescoefficients.
4.4 UserInterface 5 EVALUATION
Figure3illustratestheuserinterfaceoftheToolkit. Itconsistsof 5.1 EvaluationoftheModel
fourareas,namelytheinputvideoarea,virtualhumanarea,user
The deep learning model, which aims to automatically generate
interactionarea,andframediagramarea.
animationfromimageorvideoinput,wasassessedfromtwoaspects:
Theinputvideoarea, locatedattheleftsideofthesceneina thequantitativeevaluationofmodelingaccuracyandthequalitative
rectangularregion,iscapableofdisplayingthecurrentvideoframe. results of facial animation. In this part, virtual characters with
The virtual human area, situated in the central region of the differenttextureappearancesandblendshapetopologiesweretested.
interface, showcases the facial expressions and head poses of a
virtual human. This area is designed to show the automatically 5.1.1 QuantitativeEvaluationofModelingAccuracy
generatedfacialanimationortheusers’adjustmentinrealtime.
Toevaluatethemodelingaccuracyoftheproposeddeeplearning
Theframediagramareaislocatedatthebottomoftheinterface
method,wereporttheblendshapeestimationaccuracyofseveral
within a dark gray box. It visualizes all animation frames with
designchoicesofourmethodagainstthegroundtruthblendshape
a scatter plot. The horizontal axis denotes the index number of
coefficients.Indetail,wehaveseveraldifferentsettingsoftheim-
frames,whiletheverticalaxisrepresentstheaveragevalueofall
plementationoftheadaptermodel.WecomputetheMeanAbsolute
blendshapecoefficientsattherespectiveframe. Bydefault,each
Error(MAE)betweentheresultsandthegroundtruth,insteadof
frameisdenotedasawhitedot.Keyframesarespeciallyrepresented
usingMSEasinthetrainingprocess.
by green dots. While red dots represents frames that have been
manuallyadjustedbytheuserandrecordedaspreferencesbythe MAE=||α′−α|| 1. (10)
toolkit.Theredverticallineservesasaprogressbar,corresponding
totheframethatdisplayedinthevirtualhumanarea. ThisisbecauseMSEhelpsstabilizethetrainingprocesswhileMAE
Theuserinteractionarea,positionedontherightsideoftheinter- givesamorestraightforwardsenseoferror.
face,comprisesasetofbuttonsdesignedtooffervariousfunctional- AsshowninTable1,thefirstthreeexperimentshavecompared
itiesforanimationgeneration,visualization,playback,adjustment, theeffectofhiddendimensionbetweenthetwolinearlayers.With
andresultexportation.Inthesubsequentsections,weelaborateon 256 as the baseline, results show that a smaller dimension may
thespecificsofeachbuttonandfield. limit the model’s performance, while no significant changes are
“Initialize” button. It executes the initialization procedure of observedwhenincreasingthehidden-dimto384. Then,wehave
thetoolkit,invokingboththebasemodelandtheadaptermodelto explored different network structures, including replacing ReLU
predictblendshapecoefficientsfrominputimagesorvideos. withLeakyReLU,appendingaClampoperatorattheend,andthe
The←→button. Itimplementsthefunctionofscrollingback combinationofthetwooperations.Wecanseefromthetablethat
andforthbetweenanimation(andinputvideo)frames. truncatingtheoutputbytheClampoperatorhasbenefitsinreducing
The<<>>button.Playallvideoframesandanimationframes theestimationerror.TheleasterrorisachievedbyusingLeakyReLU
continuouslyineitherforwardorbackwarddirection. andClampsimultaneously.Allthesettingscontainasimilaramount
“PlayAnim”button.Byclickingonthisbutton,thetoolkitwill oftrainableparameters,i.e,twofullyconnectedlayers.Theadapter
displaythegeneratedfacialanimationaswedescribedinSec.4.2.1. modelaredesignedtobelightweightforeasieradaptation.
“Target” input field. Within the designated region, users can
inputtheindexcorrespondingtothetargetblendshapetheywishto 5.1.2 QualitativeResultsofFacialAnimation
modifyinthecurrentframe. Forexample,index1correspondsto Toassessthefacialanimationqualityofthedeeplearningmethod,
thelefteyelid,index2correspondstotherighteyelid,andsoforth. weconductevaluationfromthefollowingthreeperspectives.
“Value”inputfield.Withinthedefinedregion,usersareableto -Foronespecificvirtualcharacter,wetakevarioustypesofinput
inputthespecificvalueoftheblendshapecoefficienttheyintendto togeneratefacialexpressions.
modify,fallingwithinthespecifiedrangeof0to1.Theinputvalue -Wetestvirtualcharacterswiththesameblendshapetopology
shouldbeintheformofafloating-pointnumber. butdifferenttextureappearances.
“Adjust Blendshape” button. Upon entering data in the des- -Wetestvirtualcharacterswiththesametextureappearancebut
ignated target and value fields, clicking this button will save the differentblendshapetopologies.
adjustedvalueinthetoolkit,registeringitastheuser’spreference. ThequalitativeresultsofanimationareshowninFig.4andFig.5.
“Apply Preference” button. After finetuning a representative Thedeeplearningmodeltakesthehumanfaceimageasinputand
selection of frames and blendshapes, activating this button will estimatesasetofblendshapecoefficients,whichareusedtorender
computeuserpreferenceandextendittoallframeswithinthevideo. virtualhumanfacialexpression.Foronespecificcharacterequipped
“ClearPreference”button.Thisfunctionerasesuserpreferences with50blendshapes,asshownintheupperpartofFig.4,weobserve
thatarenotintendedtobeappliedacrossallvideoframes. thatthegeneratedfacesarecapableofreplicatingthereferencefacial
6Author’spreprintversion.ToappearintheproceedingsoftheIEEEVRConference.
Table2:ComparisononMAEofVariousBlendshapeTopologies.
MLPArchitecture TheNumberofBlendshapes MAE
(64,256)→(256,25) 25 0.0805
(64,256)→(256,66) 66 0.1063
(64,256)→(256,113) 113 0.1076
However,the25-blendshapeand66-blendshapevirtualcharacters
cannotdepictthismovementduetotheirtopologylackingtheneces-
sarymotionunits.InthethirdrowofFig.5,thepersonintheinput
imageisdisplayingasmilingexpression.Blendshapesrepresenting
thissmilearepresentinallthreevirtualcharacterswithdifferent
topologies.Itcanbeobservedthatthe25-blendshapevirtualchar-
acterexhibitsthemostconvincingsmile,whilethe113-blendshape
characterdisplaystheleastconvincingexpression.
InTable2,wereportthequantitativeresultsofmodelsthatare
adaptedtodifferentvirtualcharacters. Inconcept,thegreaterthe
numberofblendshapes,themoredetailedthevirtualhumanmodel
willbe,resultinghigherqualityanimation.However,inTable2we
Figure4:Upperpart:facialanimationexamplesofonespecificvirtual observethatgreaternumberofblendshapeleadstohighererror,since
character.Lowerpart:facialanimationexamplesofvirtualcharacters itmakestheoptimizationmoredifficult.Actuallythisphenomenon
withdifferenttextureappearancesbutthesameblendshapetopology. alsoholdstrueforhumans,i.e,itisalsomoredifficultforhuman
Results generated by the deep learning model fully automatically, animatorstomanipulatealargenumberofblendshapes.
withoutheadpose,withouthuman-in-the-loopintervention. Theresultssuggestthatthereisatrade-offwhendeterminingthe
25 66 113
Input Images numberofblendshapesofvirtualcharacters.Ontheonehand,fewer
Blendshapes Blendshapes Blendshapes
blendshapesmakeadaptertrainingeasier,resultinginlowerinfer-
enceerrorandmoreprecisecontrol. However,fewerblendshapes
impliesthattheupperboundexpressivenessofthevirtualcharacter
isalsolimited. Ontheotherhand,moreblendshapesincreasethe
potentialexpressivenessofthevirtualcharacter,whilealsoposing
challengesinoptimizingtheadaptermodel.
Intermsofadaptationtime,withthesupportofNVIDIAGeForce
RTX3060GPUhardware,trainingforadapting3Dvirtualcharacters
withdifferenttopologicalstructurestakesonlyabout30minutes.
Comparedtohumananimatorsmanuallyadjustingblendshapestofit
virtualcharacterswithdifferenttopologicalstructures,thismethod
eliminatestheneedtoconsidertheorder,physicalmeanings,and
controlledfacialunitsofblendshapesinthevirtualcharacters.This
significantlyreducesthetimeandmanpowercostsassociatedwith
animationediting.
Figure5: Facialanimationexamplesofvirtualcharacterswiththe
5.2 EvaluationoftheToolkit
sametextureappearancebutdifferentblendshapetopologies. The
modelsareequippedwith25,66,and113blendshapes,respectively. Forthetoolkit,differentHITL(human-in-the-loop)modesforadjust-
Theresultsaregeneratedbythedeeplearningmodelfullyautomati- inganimationswereevaluatedontheirfunctionalusabilityfirstand
cally,withoutheadpose,withouthuman-in-the-loopintervention. thenapilotstudywasconductedtogatheropinionsandsuggestions
fromsomeend-usersregardingtheusingofthetoolkit.
expressionsandarerobusttovarioustypesofinput. Fordifferent
charactersequippedwith50blendshapes, asshowninthelower 5.2.1 UsabilityofdifferentHITLmodes
partofFig.4,themodelcanbedirectlyappliedtoawiderangeof Inthispart,weusethevirtualcharacterwith50blendshapes.Both
characterswiththesameblendshapetopology,evenwithdifferent facialanimationqualityandsatisfactionscoreunderthreedifferent
textureappearancesandidentities.Oneimportantadvantageofthe modes(Auto,HITLOnline,HITLOffline)werecompared.
proposedmodelisitsflexibilityintransferringtovirtualcharacters
Facial animation quality We provide the qualitative results
withvariousblendshapetopologies. Fig.5showsthatourmodel
generatedbythetoolkitinFig.6.Inthefirstgroup,wequalitatively
canbetransferredtocharacterswithdifferenttopologies.Thethree
comparetheresultsofthreedifferentmodesmentionedabove.Itcan
modelsareequippedwith25,66,and113blendshapes,respectively.
beobservedthattheanimationresultsgeneratedbyHITLarecloser
5.1.3 DiscussionontheEffectofBlendshapeTopology tothereferencevideothantheonesauto-generatedbythetoolkit.
However,thedifferencesbetweenthetwomodesoftheHITLare
ItcanbeobservedfromFig.5thattheexpressivenessofthegener-
subtle,whichindicatesthemodelcanbeofflinetunedintosimilar
atedresultisaffectedbythenumberofblendshapes.Inthefirstrow
performancewiththeonlinetuningone.InthebelowpartofFig.6,
ofFig.5,therighteyebrowofthepersonintheinputimageisraised,
weprovidemoreexamplesgeneratedbythetoolkitwithvarious
butthisexpressionisnotcapturedbythe25-blendshapevirtualchar-
inputidentitiesandformats,includingvideosandimages. These
acter.However,boththe66-blendshapeand113-blendshapevirtual
examplesdemonstratethatthetoolkitiscapableofreplicatingthe
charactersdepicttheraisedrighteyebrow.Thisdisparityarisesfrom
facialexpressionandheadposeoftheinput.
the fact that the 25-blendshape virtual character’s topology does
notsupportindividualeyebrowmovement. Inthesecondrowof Satisfactionscore Wealsocomparehumansatisfactionscores
Fig.5,thepersonintheinputimagehasanopenlowerjaw,amove- about the quality of virtual human animations generated by the
mentaccuratelyrepresentedbythe113-blendshapevirtualcharacter. toolkitunderthreedifferentmodes:(1)Auto.(2)HITLOnline.(3)
7Author’spreprintversion.ToappearintheproceedingsoftheIEEEVRConference.
Table3:Comparisononframe-wiseanimationgenerationtimecost
andsatisfactionscore.
Reference Video
Mode Score GenerationTime
Toolkit(Auto) 4.47±1.93 0.21±0.02s
Toolkit(HITLOnline) 7.07±1.95 0.84±0.12s
Toolkit (Auto) Toolkit(HITLOffline) 6.57±2.27 Training+0.21±0.02s
generationtimeinToolkit(HITLOnline)modeisstillrelatively
Toolkit+HITL (Online)
long. InToolkit(HITLOffline)mode,ontheotherhand,onlya
single,relativelylongtrainingperiodisrequired.Afterthistraining
time,animationscanbegeneratedatthespeedofToolkit(Auto)
Toolkit+HITL (Offline)
mode,andthequalityoftheanimationsisrelativelygood.
Insummary,theToolkit(HITLOnline)modeandToolkit(HITL
Offline)modehastheirownuniqueadvantagesandcomplement
Reference Video eachother.Toolkit(HITLOnline)showsthebestsatisfactionscore,
withthecostoflongergenerationtime.WhileToolkit(HITLOffline)
achievessatisfactionscorethatisveryclosetotheOnlinemode
Toolkit+HITL (Offline) inashortertime.Toolkit(HITLOnline)providesvaluablehuman
annotationtotunethemodelofToolkit(HITLOffline),whileToolkit
(HITLOffline)canreducethetimrequiredformanualadjustments
inToolkit(HITLOnline)thankstotheimprovedperformance.
Reference Images
5.2.2 Experienceofusingthetoolkit
Finally, weconductedanindependentpilotuserstudywherewe
Toolkit+HITL (Offline) invitedagroupofend-userstotryouttheproposedtoolkit. The
purposewastogathertheiropinionsandsuggestionsregardingthe
toolkit’susagetoidentifyanyareasforimprovement.
Figure6:Facialexpressionanimationsgeneratedbythetoolkit.The
virtualcharacterisabletoreplicatethethefacialexpressionandhead Participants 16participants(6male,9femaleand1non-binary
poseoftheinputimageorvideo. gender;ageM=24.6,SD=3.12)wererecruitedandcompletedthe
userstudy.Allofthemarecollegestudentswhohaveexperienceand
HITL Offline. 13 participants (8 males, 5 females), which were familiaritywith3DsoftwareapplicationssuchasMAYA,3DMAX,
collegestudentsaged18to25yearsoldwithvariousmajors,were andUnity3D.Thischoicewasmadeconsideringthattheyarepo-
askedtogiveasatisfactionscore(1−10)tothefacialanimation tential end-users of the toolkit and their experience with similar
images.Specifically,eachparticipantwasrequiredtocompletesix software may enable them to provide valuable and constructive
setsofratingtasks.Ineachsetofratingtask,theparticipantswere opinionsandsuggestionsfortheimprovementofthetoolkit.
shownonerealhumanfacialimageandthreedifferentanimation
Procedure Beforestarting,eachparticipantwasgivenanin-
imagesgeneratedinthreedifferentmodesaccordingtherealhuman
troductionaboutthestudyandwasrequiredtosignaconsentform.
image.Theywerethenaskedtorateeachanimationimagebasedon
Followingthis,theywereaskedtoprovidethedemographicinforma-
howvisuallyreasonableandcloseitappearedtotherealimage.A
tion(age,gender,occupation,3Dsoftwareexperience).Afterthat,
ratingscaleof1to10wasused,whereascoreof1indicatedthatthe
theywereinstructedtowatchaninstructionalvideo(6minutes)that
animationimagewascompletelydissimilartotherealimage,while
providedanoverviewofthetoolkit’sfunctionsanduserinterface.
ascoreof10indicatedthattheanimationimagecloselyreplicated
This step ensured that all participants had a basic understanding
therealimage.Itisimportanttonotethatparticipantswereunaware
ofhowtousethetoolkitbeforeproceedingfurther,similartothe
ofwhichanimationimagewasgeneratedusingwhichmode,and
approachtakenwithcommercialtoolsorsystems.
theplacementpositionofthethreedifferentanimationimageswas
Subsequently,participantswereaskedtofirstutilizethetoolkit
randomizedineachsettingofratingtask.
(Auto)andthentrythetoolkit(HITLonline)togivemanipulations.
Thesatisfactionscoreforaparticularmodeofthetoolkitwas
Whenusingthetoolkit,participantsweregiventhefreedomtoeither
calculatedbyaveragingthesixsetsofscoresgivenbyallthepar-
followtheexamples(4examplevideosand19images)providedin
ticipants. AsshowninTable3,thesatisfactionscoreishighestin
theinstructionalvideoorexplorethetoolkitindependentlybasedon
theToolkit(HITLOnline)modeandlowestintheToolkit(Auto)
theirownpreferences.Thisapproachallowedparticipantstohave
mode. This indicates a significant improvement in the animated
flexibilityintheirusageandensuredthattheirexperiencewiththe
effectsafterincorporatingthehuman-in-the-loop(HITL)mode,asit
toolkitwasalignedwiththeirindividualneedsandgoals.Participant
integrateshumanpreferences,aligningbetterwithusers’cognition
wasaskedtotryallthefunctionsprovidedbythetoolkitandgenerate
andsatisfactionlevelscomparedtotheoriginalanimations. The
animationfacesbyusingatleastonevideoandfiveimages.
satisfactionscoresintheToolkit(HITLOffline)modeareveryclose
Afterthetrialusingofthetoolkit,participantswereaskedtocom-
tothoseintheToolkit(HITLOnline)mode.Thissuggeststhatthe
plete:1)astandardSUSquestionnaire[8]measuringtheusability
mode that involves human feedback into the model training pro-
ofthetoolkit(1−5score),2)asemi-structuredinterviewtogather
cessoffline,namelytheToolkit(HITLOffline),generatesanimated
opinionsandsuggestionsaboutthetoolkit.Itrequiresapproximately
effectscomparabletothosegeneratedintheToolkit(HITLOnline).
40to60minutestocompletethestudyandeachparticipantwas
rewardedwith10USDfortheirparticipation.
Generation time In terms of generation time, as shown in
Table3,theanimationgenerationtimeisshortestinToolkit(Auto) Results WiththestandardSUSquestionnaire[8],participants
mode,whilethegenerationtimeinToolkit(HITLOnline)modeis findthatthetoolkitiswellintegrated(Mean=4.0,Std=0.79),easy
approximatelyfourtimeslongerthanthatinToolkit(Auto)mode. touse(Mean=4.4,Std=0.77)andarewillingtousethistoolkitin
Althoughthisrepresentsasignificantspeedimprovementcompared theirwork(Mean=4.1,Std=0.93). Theyfeelthattheycanhavea
tothemanualeditingofanimationsbyanimators, theanimation goodcommandofthetoolkitanduseitconfidentlyafterwatching
8Author’spreprintversion.ToappearintheproceedingsoftheIEEEVRConference.
theinstructionalvideoprovided(Mean=4.6,Std=1.00). TheSUS enough.3)ThetooliscurrentlylimitedtouseinUnityandcannot
resultsshowthatthetoolkitiseasytolearnanduse. beutilizedinsoftwaresuchasUnrealEngineorMAYA.
Participants’opinionsandsuggestionsaregatheredthroughthe Thesuggestionsthatparticipantsgivencanbesummarizedasfol-
semi-structuredinterview.Followingaresomeinterviewquestions lows:1)Thecurrentfacialanimationqualityisacceptable,butthere
alongwiththecorrespondinganswers. isstillroomforimprovement,particularlyintermsofenhancing
“How do you think about the facial animation facialdetails.2)Itwouldbebeneficialtosupportvirtualcharacters
generation? Feel free to discuss topics such as withbothfacialexpressionsandbodymovements,asthiswould
character design, animation quality, or any other enableabroaderrangeofapplications. 3)Itisrecommendedto
aspects you find interesting.” generatefacialanimationsusingmorenaturalstimuli,suchastexts,
“Theexpressionanimationautomaticallygeneratedbythistool audiosandfacialimagesorvideoswithartifacts.4)Thetoolkitis
cansimulatemostofthefacialexpressions,especiallyforfaceswith alsoexpectedtobedeployableonvariousplatforms.
relativelylargefacialexpressions.Theeyesandmouthneedtobe
improved,andthemicro-expressionsoftheface,suchastheface
6 LIMITATION
aroundthenosemightbealittlestiffandunnatural.” Our work has the following limitations. Firstly, there is still a
“Thegeneralfacialexpressionandposeretargetinglooksgreat, largespaceforimprovementonthequalityofgeneratedanimations,
buttherearesomeflawsondetailedregions,liketheeyes,mouth.” especiallywhencomparingtocommercialsoftwares.Forexample,
“Virtualexpressionsareclosertorealvideosorimagesandcan thetoolkitstrugglestoreplicatingverydetailedfacialexpressions.
meetbasicneeds. Butifyouwanttomeetmoreprofessionaland Thisapproachdoesnotaimtocompletelyreplacehumananimators
refined fields such as animation production, you may need more orcompetewithcommercialsoftware,buttoassisthumananimators
realisticexpressions.” when operating their personalized virtual human characters. In
“Thecharacterdesignmainlyleanstowardsacartoonstyle,with production,westillneedhumananimatorstosuperviseandimprove
supportforclearfacialexpressions. However, thereisroomfor thequalityoftheanimation.Andinfuturework,wewilloptimize
improvementingeneratingsubtlefacialexpressions,whichmight ouralgorithmsandtoolkittoenhancetheaccuracyandinference
requiremorecomplexandrealisticvirtualcharacters.” speedoftheanimations.
“Ifthetoolkitcouldshowcaseanimationsofvirtualcharacters’ Secondly,intheevaluationofourmethod,wemainlyfocuson
bodyposturesandmovements,itmightbemoreeasilyapplicable mostcommonhumanoidcharactersandassumethataclearinput
andwidelyadopted.” is given. This can cover a wide range of application scenarios.
Investigationondiversecharacterswouldbeafuturework,including
“How do you think about the HITL function
characterswithdifferentgenders,skintones,facialartifacts(like
for human adjusting? Have you made further
jewelry,tattoos,scars,blemishes),andnon-humancharacters.
modification on the automatically generated
Thirdly,theevaluationsectiondoesnotincludecomparisonswith
animation?”
othermethods.Themainreasonisthatrelatedworksareeitherclose-
“Thistoolkitcanroughlyreplicatethingsintherealworld. I
sourcedsoftwareordesignedforfixedvirtualcharacters,makingan
thinktheexpressionsgeneratedinsomeregionsaretooexaggerated,
apple-to-applecomparisonprohibitive.Oursolutionisnotcoupled
butwhenyoucanusetheadjustmenttooltoadjusttheparameters,
withspecificcharacters,enablingflexiblecustomization.Wehope
theexaggerationisnotunbearable.Thisisalreadygoodenoughto
thisworkservesasaninitialbaselineandinspiresfutureworks.
reducealotofworkload.”
Fourthly,theuserstudypresentedinthepaperisapilotstudy
“Imanuallyeditedtheanimationsgeneratedbythetoolkitbe-
demonstratingsomepropertiesoftheproposedapproach. Itpro-
causeIpreferredmyvirtualcharactertohaveafriendlyappearance.
videsapreliminaryassessmentofthethetoolkitandprovidessome
So, I adjusted the facial parameters controlling the smile on the
guidelinesforfuturedirection. Animprovedtoolkitwithamore
cheeksmanuallyandtweakedonlyafewparameters.Then,Icould
thoroughuserstudyisafuturework.
quicklydeploythesmilingexpressionacrossallframesusingthe
“ApplyPreference”feature. Thismethodsignificantlyreducedmy
7 CONCLUSION
workloadcomparedtoadjustingthesmileexpressionforeachframe
Inthispaper, weproposeaholisticsolutionforautomaticfacial
individually.However,itsprecisionstillneedsimprovement.”
animationgenerationofcustomizedcharactersregardlessoftheir
“Iprefermyvirtualcharacter’sanimationstobemoreprecise,so
blendshapetopologiesandtextureappearances.Thisisachievedby
Imanuallyadjustedtheeyesandeyebrowsofthecharactertokeep
estimatingtheblendshapecoefficientsoftheinputimageorvideo.
theminlinewiththepositionsinthevideo.”
Wefirstproposeadeeplearningmodeltoestimatetheblendshape
“What are the advantages and disadvantages of
coefficientsofthereferencefacialexpressioninthegivenimage.
this toolkit? Please feel free to provide any
Thenwedevelopatoolkitthatencapsulatesthedeeplearningmodel
suggestions for improving the toolkit.”
withuser-friendlyinterfacesandhuman-in-the-loopscheme. The
Theadvantagesofthetoolkitthatparticipantsacknowledgedcan
evaluationresultsofthedeeplearningmethodindicatethatthepro-
besummarizedasfollows: 1)It’seasyandalsoquicktogenerate
posedsolutionofferstheflexibilitytosupportcustomizedvirtual
anacceptablefacialanimationbasedonanimageorvideoinput.
charactermodels.Moreover,thedevelopedtoolkitenablesusersto
2)Itisfavorabletoallowuserstoadjustthegenerationwithwhich
generatefacialanimationsinaneasyandefficientmanner,resulting
theyarenotsatisfiedandalsomakemodificationstoalignwiththeir
inacceptableanimationquality.Wheninvolvinghumanfeedback,
preferences. 3)Thetoolcanshowcasevirtualhumanexpressions
i.e,human-in-the-loop,theperformanceofthesolutioncanbefur-
fromanyangle,acapabilitythat2Dvirtualcharacterscannotachieve.
therimproved.Wemakethecodepublictobenefittheanimatorsand
4)Thetooldoesnothaveanyspecificrequirementsforinputface
inspirefurtherstudyinthedomainofvirtualcharacteranimation.
data,makingitapplicabletovariouscharacters,whichmayserveas
abridgebetween2Dvideosand3Dmodels. ACKNOWLEDGEMENT
Thedisadvantagesthatparticipantsmentionedcanbesummarized
ThisworkwassupportedbytheNationalKeyR&DProgramof
asfollows:1)Thefacialanimationdetailsarenotalwaysaccurate
China(2022ZD0117900),theNationalNaturalScienceFoundation
incertainsituations.Forinstance,theremaybeslightdiscrepancy
ofChina(62332015,62302494),andtheOpenResearchFundof
inthepositioningoftheeyesandeyebrowscomparedtotheirreal
GuangxiKeyLabofHuman-machineInteractionandIntelligent
positions. 2)Thetoolsupportscharacterswithrelativelylimited
Decision(GXHIID2201).
facialdetailandthebuttonlayoutoftheinterfaceisnotintuitive
9Author’spreprintversion.ToappearintheproceedingsoftheIEEEVRConference.
REFERENCES [24] T.Li,T.Bolkart,M.J.Black,H.Li,andJ.Romero.Learningamodel
offacialshapeandexpressionfrom4Dscans.ACMTransactionson
[1] Livelinkfaceontheappstore.https://apps.apple.com/us/app/ Graphics,(Proc.SIGGRAPHAsia),36(6):194:1–194:17,2017.
live-link-face/id1495370836.Accessed:2023-09-27. [25] Z.Liu,P.Luo,X.Wang,andX.Tang.Large-scalecelebfacesattributes
[2] Apple.Trackingandvisualizingfaces.2022.
(celeba)dataset.RetrievedAugust,15(2018):11,2018.
[3] N.I.Badler,M.S.Palmer,andR.Bindiganavale.Animationcontrolfor [26] O.M.Machidon,M.Duguleana,andM.Carrozzino.Virtualhumans
real-timevirtualhumans.CommunicationsoftheACM,42(8):64–73, inculturalheritageictapplications: Areview. JournalofCultural
1999.
Heritage,33:249–260,2018.
[4] Z.Bai,N.Yao,L.Liu,H.Chen,andH.Wang.Asimpleapproachto [27] P.Nogueira.Motioncapturefundamentals.InDoctoralSymposiumin
animatingvirtualcharactersbyfacialexpressionsreenactment.In2023 InformaticsEngineering,vol.303,2011.
IEEEConferenceonVirtualRealityand3DUserInterfacesAbstracts [28] Y.Pan,R.Zhang,S.Cheng,S.Tan,Y.Ding,K.Mitchell,andX.Yang.
andWorkshops(VRW),pp.585–586.IEEE,2023. Emotionalvoicepuppetry. IEEETransactionsonVisualizationand
[5] Z.Bai,N.Yao,N.Mishra,H.Chen,H.Wang,andN.MagnenatThal-
ComputerGraphics,29(5):2527–2535,2023.
mann.Enhancingemotionalexperiencebybuildingemotionalvirtual [29] P.Paysan, R.Knothe, B.Amberg, S.Romdhani, andT.Vetter. A
charactersinvrvolleyballgames. ComputerAnimationandVirtual 3dfacemodelforposeandilluminationinvariantfacerecognition.
Worlds,32(3-4):e2008,2021. In2009sixthIEEEinternationalconferenceonadvancedvideoand
[6] Z.Bai,N.Yao,N.Mishra,H.Chen,H.Wang,andN.M.Thalmann.
signalbasedsurveillance,pp.296–301.Ieee,2009.
Playwithemotionalcharacters:Improvinguseremotionalexperience [30] X.Peng,J.Huang,A.Denisova,H.Chen,F.Tian,andH.Wang. A
by a data-driven approach in vr volleyball games. In 2021 IEEE paletteofdeepenedemotions:exploringemotionalchallengeinvirtual
ConferenceonVirtualRealityand3DUserInterfacesAbstractsand realitygames.InProceedingsofthe2020CHIconferenceonhuman
Workshops(VRW),pp.458–459.IEEE,2021. factorsincomputingsystems,pp.1–13,2020.
[7] V.BlanzandT.Vetter.Amorphablemodelforthesynthesisof3dfaces. [31] X.Peng,J.Huang,L.Li,C.Gao,H.Chen,F.Tian,andH.Wang.
InProceedingsofthe26thannualconferenceonComputergraphics Beyondhorrorandfear:Exploringplayerexperienceinvokedbyemo-
andinteractivetechniques,pp.187–194,1999. tionalchallengeinvrgames.InExtendedabstractsofthe2019CHI
[8] J.Brookeetal.Sus-aquickanddirtyusabilityscale.Usabilityevalua- conferenceonhumanfactorsincomputingsystems,pp.1–6,2019.
tioninindustry,189(194):4–7,1996. [32] X.Peng,C.Meng,X.Xie,J.Huang,H.Chen,andH.Wang.Detecting
[9] D.BurdenandM.Savin-Baden.Virtualhumans:Todayandtomorrow. challengefromphysiologicalsignals:Aprimarystudywithatypical
CRCPress,2019.
gamescenario.InCHIConferenceonHumanFactorsinComputing
[10] C.Cao,Y.Weng,S.Zhou,Y.Tong,andK.Zhou.Facewarehouse:A
SystemsExtendedAbstracts,pp.1–7,2022.
3dfacialexpressiondatabaseforvisualcomputing.IEEETransactions [33] X.Peng,X.Xie,J.Huang,C.Jiang,H.Wang,A.Denisova,H.Chen,
onVisualizationandComputerGraphics,20(3):413–425,2013. F.Tian,andH.Wang.Challengedetect:Investigatingthepotentialof
[11] B.Chaudhuri,N.Vesdapunt,L.Shapiro,andB.Wang.Personalized detectingin-gamechallengeexperiencefromphysiologicalmeasures.
facemodelingforimprovedfacereconstructionandmotionretargeting.
InProceedingsofthe2023CHIConferenceonHumanFactorsin
InComputerVision–ECCV2020:16thEuropeanConference,Glas- ComputingSystems,pp.1–29,2023.
gow,UK,August23–28,2020,Proceedings,PartV16,pp.142–160. [34] F.Schroff,D.Kalenichenko,andJ.Philbin.Facenet:Aunifiedembed-
Springer,2020.
dingforfacerecognitionandclustering.InProceedingsoftheIEEE
[12] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.Imagenet:
conferenceoncomputervisionandpatternrecognition,pp.815–823,
Alarge-scalehierarchicalimagedatabase.In2009IEEEconference 2015.
oncomputervisionandpatternrecognition,pp.248–255.Ieee,2009. [35] S.Sharma,S.Verma,M.Kumar,andL.Sharma.Useofmotioncapture
[13] Y.Deng,J.Yang,S.Xu,D.Chen,Y.Jia,andX.Tong.Accurate3dface in3danimation:motioncapturesystems,challenges,andrecenttrends.
reconstructionwithweakly-supervisedlearning:Fromsingleimageto
In2019internationalconferenceonmachinelearning,bigdata,cloud
imageset.InProceedingsoftheIEEE/CVFconferenceoncomputer andparallelcomputing(comitcon),pp.289–294.IEEE,2019.
visionandpatternrecognitionworkshops,pp.0–0,2019. [36] T.Shi,Y.Yuan,C.Fan,Z.Zou,Z.Shi,andY.Liu.Face-to-parameter
[14] P.Dou,S.K.Shah,andI.A.Kakadiaris.End-to-end3dfacereconstruc- translationforgamecharacterauto-creation. InProceedingsofthe
tionwithdeepneuralnetworks.InproceedingsoftheIEEEconference IEEE/CVFInternationalConferenceonComputerVision,pp.161–170,
oncomputervisionandpatternrecognition,pp.5908–5917,2017. 2019.
[15] P.Edwards,C.Landreth,E.Fiume,andK.Singh. Jali:ananimator- [37] T.Shi,Z.Zuo,Y.Yuan,andC.Fan.Fastandrobustface-to-parameter
centricvisememodelforexpressivelipsynchronization.ACMTrans- translationforgamecharacterauto-creation. InProceedingsofthe
actionsongraphics(TOG),35(4):1–11,2016. AAAIConferenceonArtificialIntelligence,vol.34,pp.1733–1740,
[16] Faceware.Introtofacewarestudio.2022. 2020.
[17] Y.Feng,F.Wu,X.Shao,Y.Wang,andX.Zhou.Joint3dfacerecon- [38] M.Volonte,E.Ofek,K.Jakubzak,S.Bruner,andM.Gonzalez-Franco.
structionanddensealignmentwithpositionmapregressionnetwork.In Headbox: Afacialblendshapeanimationtoolkitforthemicrosoft
ProceedingsoftheEuropeanconferenceoncomputervision(ECCV), rocketboxlibrary. In2022IEEEConferenceonVirtualRealityand
pp.534–551,2018.
3DUserInterfacesAbstractsandWorkshops(VRW),pp.39–42.IEEE,
[18] G.B.Huang,M.Mattar,T.Berg,andE.Learned-Miller.Labeledfaces 2022.
inthewild:Adatabaseforstudyingfacerecognitioninunconstrained [39] L.Wolf,Y.Taigman,andA.Polyak.Unsupervisedcreationofparame-
environments.InWorkshoponfacesin’Real-Life’Images:detection, terizedavatars.InProceedingsoftheIEEEInternationalConference
alignment,andrecognition,2008. onComputerVision,pp.1530–1538,2017.
[19] IEEE. A3DFaceModelforPoseandIlluminationInvariantFace
Recognition.Genova,Italy,2009.
[20] P.Joshi,W.C.Tien,M.Desbrun,andF.Pighin.Learningcontrolsfor
blendshapebasedrealisticfacialanimation.InACMSiggraph2006
Courses,pp.17–es.2006.
[21] D.E.King. Dlib-ml: Amachinelearningtoolkit. TheJournalof
MachineLearningResearch,10:1755–1758,2009.
[22] J.P.Lewis,K.Anjyo,T.Rhee,M.Zhang,F.H.Pighin,andZ.Deng.
Practiceandtheoryofblendshapefacialmodels.Eurographics(State
oftheArtReports),1(8):2,2014.
[23] J.P.LewisandK.-i.Anjyo.Directmanipulationblendshapes.IEEE
ComputerGraphicsandApplications,30(4):42–50,2010.
10