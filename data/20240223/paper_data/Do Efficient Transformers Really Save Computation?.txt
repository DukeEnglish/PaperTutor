Do Efficient Transformers Really Save Computation?
KaiYang1 JanAckermann2 ZhenyuHe1 GuhaoFeng1 BohangZhang1 YunzhenFeng3 QiweiYe4
DiHe1 LiweiWang1
Abstract andLLaMa,typicallyhavebillionsofparametersandare
trained on datasets containing trillions of tokens. Given
Astransformer-basedlanguagemodelsaretrained
thesubstantialcomputationaldemands,enhancingLLMs’
onincreasinglylargedatasetsandwithvastnum-
efficiencyhasbecomeapivotalresearchfocusinacademic
bers of parameters, finding more efficient alter-
andindustrialcontexts.
nativestothestandardTransformerhasbecome
veryvaluable. WhilemanyefficientTransformers The primary computational bottleneck in Transformers
andTransformeralternativeshavebeenproposed, arises from the self-attention module, whose complexity
noneprovidetheoreticalguaranteesthattheyarea scales quadratically with the sequence length. The cost
suitablereplacementforthestandardTransformer. becomesparticularlynoticeableintasksthatrequirelongse-
Thismakesitchallengingtoidentifywhentouse quencegeneration,suchascoherentstorygenerationorrea-
aspecificmodelandwhatdirectionstoprioritize soningwithChain-of-Thoughtprompts(Weietal.,2022b;
forfurtherinvestigation. Inthispaper,weaimto Kojima et al., 2022; Nye et al., 2022; Zhou et al., 2023).
understandthecapabilitiesandlimitationsofeffi- Given the practical needs, a large body of work seeks to
cientTransformers,specificallytheSparseTrans- developefficientTransformersthatcanreducethequadratic
formerandtheLinearTransformer. Wefocuson complexity of self-attention (Tay et al., 2022), typically
theirreasoningcapabilityasexhibitedbyChain- byimposingsparsityintoarchitecturaldesign(Childetal.,
of-Thought(CoT)promptsandfollowprevious 2019;Beltagyetal.,2020;Qiuetal.,2020;Kitaevetal.,
workstomodelthemasDynamicProgramming 2020; Vyas et al., 2020; Roy et al., 2021) or by employ-
(DP)problems. Ourresultsshowthatwhilethese inglow-rankorkernel-basedapproximationstoaccelerate
models are expressive enough to solve general thecomputation(Katharopoulosetal.,2020;Choromanski
DPtasks,contrarytoexpectations,theyrequire etal.,2021;Pengetal.,2021;Wangetal.,2020;Luoetal.,
a model size that scales with the problem size. 2021). However,thereisgenerallyalackofunderstanding
Nonetheless,weidentifyaclassofDPproblems aboutthecapabilitiesofefficientTransformer.
forwhichthesemodelscanbemoreefficientthan
Inthiswork,wetakeasteptowardstheoreticallyunderstand-
thestandardTransformer.Weconfirmourtheoret-
ingthecapabilityofefficientTransformers. Inparticular,
icalresultsthroughexperimentsonrepresentative
we focus on the models’ reasoning ability, a fundamen-
DPtasks,addingtotheunderstandingofefficient
tal aspect of human intelligence that plays a vital role in
Transformers’practicalstrengthsandweaknesses.
problem-solving,decision-making,andplanning. Inspired
byarecentstudyinFengetal.(2023),wemodelreason-
ingasadynamicprogramming(DP)processasitclosely
1.Introduction
resemblesthewayChain-of-Thoughtpromptsareexecuted.
The Transformer architecture, as introduced in the semi- Theoutputsequenceconsistsofanswerstoaseriesofinter-
nalworkofVaswanietal.(2017),hasdemonstratedare- mediatesteps,eachcorrespondingtosolvingasubproblem
markable performance in numerous applications ranging representedbyaDPstate. Fengetal.(2023)provedthat
fromnaturallanguageprocessingtocomputervisionand allreasoningproblemsfittingwithinthisframeworkcanbe
speech. Asignificantadvancementhasrecentlybeenmade, solvedbyastandardautoregressiveTransformerofacon-
byscalingupTransformerstobuildLargeLanguageMod- stantsize(irrelevanttotheproblemscale),thusachievinga
els (LLMs) (Brown et al., 2020; OpenAI, 2023; Touvron
computationalcomplexityofΘ(L2)whereListhelength
etal.,2023). TheseLLMs,exemplifiedbymodelslikeGPT oftheoutputsequence.
In our work, we focus on two representative (Tay et al.,
1Peking University 2ETH Zurich 3New York University
4BeijingAcademyofArtificialIntelligence. 2022)andsuccessful(Tayetal.,2020b;Brownetal.,2020)
variantsofefficientTransformers: theSparseTransformer
Preprint.WorkinProgress.
1
4202
beF
12
]GL.sc[
1v43931.2042:viXraDoEfficientTransformersReallySaveComputation?
Table1. ComplexityoftheTransformervariantsondifferenttasks.
Architecture GeneralReasoning Arithmetic Reasoning(localityassumption)
StandardTransformer Θ˜(L2) Θ˜(L2) Θ˜(L2)
√ √ √
SparseTransformer Θ˜(L2) Θ˜(L L) Θ˜(L L)ifm=O( L)
√
LinearTransformer Θ˜(L2) Ω˜(L L) Ω˜(mL)
(Childetal.,2019)andtheLinearTransformer(Katharopou- required hidden dimension increases as the problem size
los et al., 2020). (In the following we will refer to these growsinmostscenarios,whilethisisnotthecaseforthe
two as efficient Transformers.) Our analysis shows that standardTransformer. Moreover,thedependencybetween
botharchitecturespossessthenecessaryexpressivenessfor hiddendimensionandproblemscaleismorepronounced
allproblemswithinthisDPframeworkdespiteonlyscal- in LIS than in ED. These results validate our theory and
√
ingwithΘ(L L)andΘ(L). Althoughthispositiveresult offerpracticalinsightsintothestrengthsandweaknessesof
might lead one to believe that we can supplant the stan- efficientTransformers.
dardTransformerwithothersoflowercomplexity,thesitu-
Notations. We adopt the big-O notation throughout this
ationismorecomplicated: ourmainresulthighlightsthat
paper. Specifically,giventwofunctionsf,g :X →[0,∞)
bothSparseTransformerandLinearTransformerrequire
whereX canbeanyset,wewritef =O(g)ifthereexists
agrowingmodelsizewithrespecttotheproblemscaleL,
a constant c > 0 such that f(x) ≤ cg(x) for all x ∈ X.
in contrast to the constant size of standard Transformers.
We also write f = Ω(g) if g = O(f), and write f =
Specifically,undermildassumptions,weprovethatneither
Θ(g) if both f = O(g) and f = Ω(g) hold. Moreover,
architecturecangeneratetheDPsolutionunlessthehidden
√ given two functions f,g : Nd → [0,∞), we write f =
dimensionofthenetworklayersscalesasΩ˜( L).Thisscal- +
ing results in a total computational complexity of Ω˜(L2), O˜(g) if there exist constants c,k > 0 such that f(x) ≤
cg(x)logk(x ···x ) for all x ∈ Nd. The notations Ω˜(·)
matching the vanilla Transformer’s complexity. But this 1 d +
resultintroducesaparadox: whentacklinggeneralDPprob-
andΘ˜(·)canbesimilarlydefined.
lems,thetoutedefficiencyofthese“efficient”Transformers
appearstodissolve,renderingthemcomparablyefficientto 2.RelatedWork
standardTransformers.
TransformersandLargeLanguageModelshavereceivedsig-
The above findings about general DP problems raise the
nificantattentionduetotheirunprecedentedsuccessacross
question: For which problems are efficient Transformers
various domains. A considerable body of literature has
efficient? Toanswerthisquestion,westartbystudyinga
emergedtoestablishadeepertheoreticalunderstandingof
fundamentaltaskforreasoning: evaluatingarithmeticex-
theirstrengthsandconstraints.
pressions (Feng et al., 2023). Notably, we find that the
√
complexitylowerboundcanbeimprovedtoΩ˜(L L)for UniversalApproximation. Initially,thetheoreticalfocus
botharchitectures,andthelowerboundcanbeattainedfor wasonthecapacityofTransformerstoapproximatediverse
theSparseTransformerwithaconstanthiddendimension. functions. Yunetal.(2019)postulatedthatadequatelysized
Motivatedbythisfinding,wethenidentifyageneralcon- Transformerscanuniversallyapproximateanycontinuous
ditionthatunlockstheefficiencyofefficientTransformers, sequence-to-sequencefunctionswithincertainbounds. A
calledthelocalityassumption. Intuitively,thisassumption parallellineofworkfirstshowedthatTransformerswithinfi-
statesthateachstepinthereasoningprocessonlydepends niteprecisionareturing-complete(Pe´rezetal.,2019;2021)
on the outcome of recent m reasoning steps where m is andlaterWeietal.(2022a)establishedthatTransformers
farsmallerthanL,i.e. m=o(L). Underthisassumption, withfiniteprecisionareapproximatelyturing-complete. Re-
weshowthatthecomplexitylowerboundcanbeimproved cently,Albertietal.(2023)provedthatLinearTransformers
for sparse- and Linear Transformer. We summarize our are also universal approximators. Whereas these results
theoreticalresultsinTable1. approachexpressivenessbyprovingcomputationalcapacity,
wecomplementourexpressivenessresultswithcomplexity
Wecomplementourtheoreticalfindingswithanextensive
lowerboundsforpracticalsettings.
setofexperiments. FollowingFengetal.(2023),wefocus
ontheArithmetictaskandtwoadditionalDPproblems: the Formal Language Learning. Additionally, the Trans-
LongestIncreasingSubsequence(LIS)andtheEditDistance former’s expressivity has been studied in the context of
(ED).Notably,theEDtasksatisfiesthelocalityassumption, formallanguagelearning. Bhattamishraetal.(2020)con-
whereastheLIStaskdoesnot. Foreachtask,wesystemati- structedaTransformerthatdetectscounterlanguages,and
callyinvestigatehowvariationsintheproblemsize(i.e.,the Yaoetal.(2021)showhowtodetectDycklanguages. Liu
sequencelengthL)andthehiddendimensionoftheTrans- etal.(2022)showthatshallowTransformerscanlearnfi-
formermodelsimpactthemodels’performance. Empirical nitestateautomataandsimulatethemforanumberofsteps
evidenceconfirmsthat,forbothefficientTransformers,the that scale with the model size. Conversely, Hahn (2020)
2DoEfficientTransformersReallySaveComputation?
showsthattransformerscannotlearndistributionsoverlan- x(0) =Embed(s )+p ∈RD,whereEmbed(·)isthetoken
i i
guages. Otherworksuseclassicaltechniquesfromcircuit embeddinglayerandp alearnablepositionalembedding.
i
complexity(Furstetal.,1984)toprovethatTransformers Then,M Transformerblocksfollow,thel-thofwhichhas
cansimulateclassesofcircuits(Haoetal.,2022;Merrill thefollowingform:
etal.,2022;Merrill&Sabharwal,2023).
h(l) =x(l−1)+Attn(l)(x(l−1);{x(l−1) :j ∈[i]}), (1)
Measuring Complexity. Weiss et al. (2021) introduce a i i i j
programminglanguagethatmapstolearnableTransformer x(l) =h(l)+FFN(l)(h(l)) (2)
i i i
encodersandfacilitatestheanalysisofthecomplexityof
problemswithrespecttolayersandattentionheads. San- Here, Attn(l) and FFN(l) denote the multi-head self-
ford et al. (2023) introduce a sparse averaging task that
attention layer and the feed-forward network of the l-th
requiresrecurrentandfeed-forwardnetworkstobeoflinear
Transformerblock,respectively:
complexity, whereastheTransformeronlyneedstoscale
logarithmically. Theseworksaresimilartooursinthatwe H
establishconcreterelationshipsbetweenmodelcomplexity
Attn(l)(x,S)=(cid:88)(cid:16) W(l,h)(cid:17)⊤
·H(l,h)(x,S), (3)
O
andsolvabilityoftheposedproblems. Butourworkdeals h=1
with autoregressive efficient Transformers equipped with (cid:80) exp(cid:16) (W(l,h)z)⊤(W(l,h)x)(cid:17) W(l,h)z
Chain-of-Thought. H(l,h)(x,S)= z∈S K Q V ,
(cid:16) (cid:17)
(cid:80) exp (W(l,h)z)⊤(W(l,h)x)
In-contextlearning.Arecentapproachshowsitsin-context z∈S K Q
learningability(Gargetal.,2022;Brownetal.,2020). Fol- (4)
lowingthis,therearealsotheoreticalresultsthat(Daietal., FFN(l)(x)=W(l)σ(W(l)x), (5)
2023;VonOswaldetal.,2023;Akyu¨reketal.,2022)prove 2 1
itcanperformgradientdescent. Anotherlineofworkshows
in-context-learningviainductionheads(Elhageetal.,2021;
whereW Q(l,h),W K(l,h),W V(l,h),W O(l,h) ∈R⌈ HD⌉×D arethe
query, key, value, output matrices of the h-th head in the
Olsson et al., 2022). Similarly, Feng et al. (2023) show
thatautoregressivetransformerscanlearntoperformdy- l-th layer, respectively, and W 1(l),W 2(l) ∈ RD×D are
namicprogrammingwhenequippedwithChain-of-Thought. weight matrices in the FFN. The activation σ is chosen
WhileinthesamesettingasFengetal.,weinvestigateeffi- as GeLU (Hendrycks & Gimpel, 2016), following (Rad-
cientTransformersandpresentaproblemclassthatencour- fordetal.,2019;Devlinetal.,2019). Thecomputedem-
agesefficiency. beddingx(M) willbeusedtopredictthenexttokens ,
n n+1
whichisthenconcatenatedtotheinputtocontinuethese-
EfficientTransformer. Duetothehighcomplexityofthe
quence generation process. The process stops when an
attention layer, many more efficient methods have been
End-of-Sentencetokenisgenerated.
proposed. A first series of ideas exploit fixed attention
patterns(Childetal.,2019;Beltagyetal.,2020;Qiuetal., BasedonEquations(1)to(3)and(5),itiseasytoseethat
2020). Another line of work approximates the attention thecomputationalcomplexityofanautoregressiveTrans-
asalowrankmatrixorwithkernels(Katharopoulosetal., former is Θ(M(L2D+LD2)), where L is the sequence
2020; Wang et al., 2020; Choromanski et al., 2021) and length. ThisquadraticdependencyonLlimitstheapplica-
furtherworksdealwithlearnedpatterns(Kitaevetal.,2020; tionofTransformerstolongtext,inparticularforcomplex
Tay et al., 2020a; Roy et al., 2021). A last set of works reasoningtasks. Tobattlethis,researchershaveproposed
evencompletelymoveawayfromtransformers(Sunetal.; variousefficientTransformerstoreducethecomplexity. In
Gu&Dao,2023). Tworecentworksstudywhenstandard our work, we investigate the Sparse Transformer and the
attentioncanbeefficient(Alman&Song,2023)andhowto LinearTransformer. Below,wedescribethetwoarchitec-
approximatestandardattentioninlineartime(Kelesetal., tureswhicharestudiedinthispaper.
2023).Incontrasttotheirwork,wegivetheoreticalanalyses
Sparse Transformer. Unlike the standard Transformer
forexistingandpopularefficientTransformers.
whereeachtokenx(l) canattendtoallpreviouspositions
{x(l) :j ∈[i]}(seeEquation(1)),inaSparseTransformer
3.EfficientTransformers j
itonlyattendstoasubsetofprevioustokens{x(l) :j ∈I }.
j i
The autoregressive Transformer, also called the decoder- Inthispaper,westudyastandarddesignparadigmproposed
onlyTransformer(Radfordetal.,2019;Daietal.,2019),is inChildetal.(2019),whichemploysablock-wisepattern
asequence-to-sequenceneuralnetworkdefinedasfollows. asshowninthefollowing:
Givenaninputsequencesoflengthn, itfirsttransforms
eachinputtokens i (i ∈ [n])intoaD-dimensionalvector I i ={j :i−kB <j ≤i}∪{j :j−1modB ≥B−c}
(6)
3DoEfficientTransformersReallySaveComputation?
whereBiscalledtheblocksizeandk,careconstantinte- becharacterizedusingatransitionfunction:
√
gers. WhenB =Θ( L),theSparseTransformerachieves
√ (cid:0) (cid:1)
aminimalcomplexityofΘ(M(L LD+LD2)). Wenote dp(i)=f i,dp(h 1(i)),··· ,dp(h K(i)),s g1(i),··· ,s gJ(i)
thatGPT-3adoptedtheabovedesignparadigm(Brownetal., (9)
2020). where s is the input sequence, and f, g 1,··· ,g J,
h ,··· ,h arefunctionsthatdependsontheproblem. In
1 K
Linear Transformer. Another line of work proposed to
other words, the answer of each subproblem is fully de-
acceleratetheattentioncomputation(Equation(3))using
termined by the answers of a finite number of previous
kernel-basedapproximations. Arepresentativeapproachis
subproblemsplusafinitenumberofinputtokens. Based
theLinearTransformer(Katharopoulosetal.,2020),which
onEquation(9),wecansequentiallysolveallsubproblems
approximatesAttn(l)withthefollowingformula:
onebyone. Aftersolvingallsubproblems,thefinalanswer
can be computed by u(dp(i )), where i is the last DP
N N
stateanduisaproblem-dependentfunction. Bydefining
Attn(l)
(x,S)=(cid:88)H (cid:16) W(l,h)(cid:17)⊤
·H(l,h)(x,S), (7)
ourproblemsogenerally,wealsocoverCoTproblems. We
linear O linear assumethatthef,g,handuabovecanbeapproximatedby
h=1 anMLPwithGeLUactivationofconstantsize. Wealsoas-
(cid:80) ϕ(W(l,h)z)⊤ϕ(W(l,h)x)(W(l,h)z)
sumethatduringtheCoTgenerationprocess,thenextstate
H(l,h)(x,S)= z∈S K Q V
linear (cid:80) ϕ(W(l,h)z)⊤ϕ(W(l,h)x) canbeobtainedbyanMLPwheretheinputisthecurrent
z∈S K Q state. OnecanrefertoAppendixBforaformaldescription.
(8)
Wearguethattheseassumptionsaremildandthattheyhave
beenusedinpreviouswork(Fengetal.,2023).
where they choose ϕ(x) = elu(x) + 1. The above Inoursubsequentanalysis,withoutlossofgenerality,we
computation can be accelerated by rearranging the or- assumethateachinputelements j isaninteger,andeach
der of computation so that the intermediate results state i, DP value dp(i), and the final answer can all be
(cid:80) (W(l,h)z)ϕ(W(l,h)z)⊤ and (cid:80) ϕ(W(l,h)z)⊤ representedbyvectorsofintegerelements. Thedomainof
z∈S V K z∈S K
associated with different S can be jointly computed us- these integers can grow polynomially with respect to the
ingprefixsum,finallyyieldingacomplexityofΘ(MLD2) lengthL.
whichislinearinL.
Outputformat. FollowingFengetal.(2023),givenaDP
taskandaninputseuqences,anautoregressiveTransformer
4.ExpressivenessofEfficientTransformersin generatestheanswerwithallintermediatestepsinthefol-
ReasoningTasks lowingform:
Reasoningconstitutesafundamentalaspectofhumanintel- (s 1,0,0,0) ... (s n,0,0,0) |
ligenceandplaysavitalroleinproblem-solving,decision- (0,i ,dp(i ),0) ... (0,i ,dp(i ),0)
1 1 N N
making,andplanning. Recently,Transformer-basedLLMs
(0,0,0,u(dp(i ))) (10)
N
havedemonstratedremarkablereasoningabilities(OpenAI,
2023;Touvronetal.,2023). Thishassparkedaseriesof Here, the subsequence ending at the special token “|” is
studiesaimedattheoreticallyunderstandinghowpowerful the input to the Transformer, and the remainder will be
thesemodelsare. Inparticular,Fengetal.(2023)recently autoregressivelygenerated. Theoutputateachpositionis
revealed that autoregressive Transformers are capable of split into four parts that store the input, state, DP value,
solvingageneralclassofreasoningproblemsformalizedas and final answer, respectively. We denote by i ,··· ,i
1 N
DynamicProgramming(DP).Inthissection,weextendthis thesequenceofDPstatesrepresentingallsubproblemsin
findingbyinvestigatinghowthingschangewhenmovingto order. Weconsidertheregressionsettingwheretheoutput
varioustypesofefficientTransformers. ateachpositionissimplyobtainedfromtheembeddingof
thelastTransformerlayerbyprojectingeachdimensionto
4.1.Problemformulation thenearestinteger. Similarly,eachgeneratedoutputdirectly
serves as the input of the next position (without using a
Dynamicprogrammingdecomposesacomplexreasoning
tokenembeddinglayer).
problemintoasequenceofreasoningsteps,eachofwhich
correspondstoasubproblemandiscalledaDPstate.Differ- Log-precision Transformers. We adopt a realistic and
entsubproblemsdependoneachotherbecausetheycanbe widely-usedsettingwhereallinternalneuronsintheTrans-
efficientlysolvedbasedontheanswersofpreviouslysolved formercanonlystorefloating-pointnumberswithinafinite
subproblems. Formally, denoting by dp(i) the answer of O(logL) bit precision (Merrill & Sabharwal, 2023; Liu
subproblemi,thentherelationbetweensubproblemscan etal.,2023;Fengetal.,2023),andallbasicfloating-point
4DoEfficientTransformersReallySaveComputation?
computationsaretruncated,asimplementedonacomputer. two different input sequences s(1) and s(2) (of the same
Log-precisionimpliesthateachneuronhasalimitedcapac- length)andafixedbutarbitrarymodelthatsolvestheDP
ityforcomputationandinformationstorage. Nevertheless, problem, there is a state i such that dp(i) is different be-
theyremainpowerfulastheycanrepresentalargerangeof tweeninputs(1)ands(2).
values(i.e.,polynomialinthesequencelengthL),recover-
We remark that regularity is a weak assumption, which
ingimportantquantitieslikepositionalembedding.
onlystatesthatthereasoningprocess(notthefinalanswer)
Undertheaboveassumptions,Fengetal.(2023)provedthe shouldnotbeexactlythesamewhentheinputchanges. For
followingmainresultforthestandardtransformer: example,itexcludesthecasewherethewholeDPprocess
Theorem 4.1 (informal). Consider any DP problem de- doesnotdependonaspecificinputelements . Equipped
j
finedabovethatsatisfiestheassumptionsfromB.Forany withtheregularityassumption,wepresentacentralimpos-
integern > 0,thereexistsalog-precisionautoregressive sibilityresult:
TransformerwithaconstantdepthM, aconstanthidden Theorem4.4. ConsideranyregularDPproblemsatisfying
dimensionD,andaconstantnumberofattentionheadsH thesameconditionasinTheorem4.1. Assumethattheout-
(independentofn)thatcangeneratethecorrectoutputfor putsequencelengthLisproportionaltotheinputsequence
allinputssoflengthn. lengthn,i.e., L = Θ(n). Then,givenasufficientlylarge
n,forboth(log-precision)SparseTransformerwithblock
√
4.2.Mainresults sizeB =Θ( L)andLinearTransformer,amodelwitha
constantdepthM andaconstantnumberofattentionheads
Now,weinvestigatewhethertheefficientTransformersde-
H cangeneratethecorrectoutputforallinputssoflength
fined in Section 3 are as powerful as the standard Trans- √
nonlyifthehiddendimensionD =Ω˜( L).
formerinsolvingDPproblems. Inparticular,weestablish
a similar result to Theorem 4.1, which we present in the AspresentedinAppendixB.2, theproofofTheorem4.4
theorembelow: isbasedonthefollowingfinding: thereareinherentinfor-
Theorem 4.2. Consider any DP problem satisfying the mationbottlenecksinbothtypesofefficientTransformers.
sameconditionasinTheorem4.1. Givenanyintegern>0, Here,thebottleneckisasetofneuronswhosevaluescom-
letLbethelengthoftheoutputsequencewhentheinput pletelydetermineallensuingoutputsfromaspecificposi-
sequencelengthisn. Then,forboth(log-precision)Sparse tion. Due to the log-precision assumption, these neurons
√
TransformerwithblocksizeB =Θ( L)andLinearTrans- only store a limited amount of information. Hence it is
former, there is amodel with a constant depth M, a con- only possible to recover all subsequent outputs when the
√
stantnumberofattentionheadsH,andahiddendimension hiddendimensionisΩ˜( L)—otherwise,thePigeonhole
√
D = O( L)thatcangeneratethecorrectoutputforall principle will imply that there are two different input se-
inputssoflengthn. quencesthatsharethesamesetofneuronvalues,yielding
thecontradictionbyDefinition4.3.
TheproofofTheorem4.2isnon-trivialandisdeferredto
AppendixB.1.Intheproof,wegiveexplicitconstructionsof
5.WhenCanEfficientTransformersReally
parametersforsparse/linearattentionandFFNlayers,show-
ingthattheselayerscanimplementasetofbasicoperations SaveComputation?
presentedinAppendixA.Wethenusetheseoperationsas
In the previous section, we showed the surprising result
buildingblockstoformacompletemodelthatsolvesthe
thattheseefficientTransformersmaynotleadtoreduced
DPtask.
complexitycomparedtothestandardTransformersingen-
Theorem 4.2 suggests that replacing the standard self- eralreasoningtasks. However,oneshouldnothastilyjump
attentionwiththeseefficientvariantsdoesnotrestrictthe to the conclusion that these efficient Transformers are al-
model’sexpressivenessinreasoning. However, whenwe waysinefficient. Inthissection,wewilldiscussinwhich
comparethetotalcomplexityO(L2)ofthederivedmodels, situationsefficientTransformersareefficient.
wecanseethatitisthesameasforthestandardattention,
whichonlyneededd=O(1). 5.1.Amotivatingexample: evaluatingarithmetic
expressions
Is the increase in model size necessary? Theorem 4.2
only gives a complexity upper bound for Sparse/Linear Webeginbyinvestigatingalesscomplextaskproposedby
Transformers. Itremainstoshowwhethertheboundistight Fengetal.(2023),calledthearithmeticevaluation. Thetask
andwhatthelowerboundoftherequiredhiddendimension istoevaluateanarithmeticexpressionlike“2×(1+5)÷4”
is. Toanswerthisquestion, wewillfocusonarestricted andthecompleteoutputsequencelookslike“2×(1+5)÷
classofDPproblemswhichwecallregularDPproblems: 4 = 2×6÷4 = 12÷4 = 3”. Fengetal.(2023)proved
Definition4.3. ADPproblemiscalledregularifforany that a standard Transformer of a constant size can solve
5DoEfficientTransformersReallySaveComputation?
this task. Surprisingly, we find that a similar result also eachDPstateonlydependsonrecentmDPstates.Notethat
holdsforaconstant-sizeSparseTransformer,asshownin theassumptionm≥nisnecessarytoensurethatallinputs
thepropositionbelow: contribute to the answer of the DP problem. Below, we
Proposition5.1(informal). Foranyintegern,thereexists willdiscusshowtherequiredhiddendimensionofefficient
a log-precision Sparse Transformer with block size B = TransformerscanbereducedwhenmisfarsmallerthanL.
√
Θ( L), 5 layers, 5 attention heads per layer, a constant
WefirstconsidertheSparseTransformer, wherewehave
hiddendimensionthatcangeneratethecorrectoutputfor
thefollowingresult:
thearithmeticevaluationtaskforallexpressionsoflength
Proposition5.4. Consideranym-localityDPproblemsat-
nomorethann.
isfyingthesameconditionasinTheorem4.1. Givenanyin-
Owing to the constant dimension, the complexity of the tegern>0,letLbethelengthoftheoutputsequencewhen
√
arithmeticevaluationtaskcanbereducedtoO(L L)by the input sequence length is n. Then, there exists a (log-
usingSparseTransformers. WenextturntoLinearTrans- precision)SparseTransformerwithblocksizeB =Θ(m),
formers. While we do not give explicit constructions of aconstantdepthM,aconstantnumberofattentionheads
modelparametersthatcansolvethearithmetictask,onecan H,andaconstanthiddendimensionDthatcangenerate
stillderivealowerboundbyusingasimilaranalysisasin thecorrectoutputforallinputssoflengthn.
Theorem4.4:
As a result, the complexity of Sparse Transformer scales
Proposition 5.2 (informal). For any integer n, a log-
likeO˜(mL),whichisstrictlylessthanΘ˜(L2)whenmis
precisionLinearTransformerwithaconstantdepthM and
farsmallerthanL. WenextturntotheLinearTransformer,
a constant number of heads H can generate the correct
wherewehavethefollowinglowerbound:
output for the arithmetic evaluation task for all expres-
sionsoflengthnomorethannonlyifthehiddendimension Proposition5.5. Consideranym-localityregularDPprob-
√
D =Ω˜(4L). lem satisfying the same condition as in Theorem 4.1 and
assumethatm=Θ(n)wherenistheinputsequencelength.
Based on this result, the comple √xity lower bound of Lin- Then,alog-precisionLinearTransformerwithaconstant
earTransformersscaleslikeΩ˜(L L),which,interestingly, depthM andaconstantnumberofheadsH cangenerate
matchesthatofSparseTransformersandisalsostrictlyless the correct output for all inputs s of length n only if the
√
thanΩ˜(L2).Thisfindingnaturallyraisesthefollowingques- hiddendimensionD =Ω˜( m).
tion: WhydotheseefficientTransformersnolongerrequire
alargehiddendimensiononthearithmetictask? Wewill Theaboveresultimpliesthatthecomplexitylowerbound
answerthisquestioninthenextsubsection. ofLinearTransformer,whichisimposedbythebottleneck,
scaleslikeΩ˜(mL),whichisstrictlylessthanΘ˜(L2)when
5.2.Localityencouragesefficiency misfarsmallerthanL. However,weremarkthatitremains
achallengingopenquestionofwhethersuchacomplexity
Akeydifferenceofthearithmetictaskcomparedtogeneral lowerboundcanbematched.
DPisthatitsreasoningprocessexhibitsinherentstructures.
Tobespecific,theoutputsequenceofarithmeticcomputa-
6.Experiments
tioncanbepartitionedintoblocks(separatedbythesymbol
“=”),wherethecontentofeachblockdependssolelyonthe Intheprecedingsections,weconductedatheoreticalanal-
preceding one and is irrelevant to other historical blocks. ysistoassessthecapabilitiesofefficientTransformersfor
Thisparadigmisoftennamedasdatalocalityincomputer generalDPproblemsandproblemswithlocality. Thissec-
scienceliteratureandisalsocommoningeneralreasoning tionservestovalidatethosefindingsthroughcomprehensive
processesthatfollowtheso-calledChainofThoughtformat empiricalexperimentation. Inspiredbythereasoningevalu-
(Weietal.,2022b). Inlightofthis,weconsideraspecial ationin(Fengetal.,2023),weadoptasimilarexperimental
classofDPproblemsdubbedthem-localityDP,whichis
designusingcommonDPproblemswithChain-of-Thought
formallydefinedbelow: demonstrations. Wefocusonunderstandinghowtwokey
Definition 5.3 (m-locality DP). Consider a DP problem factors,problemsizeandtheembeddingdimensionofthe
with output sequence o ,··· ,o of the form (10) where Transformermodel,affectperformanceacrosstasks.
1 L
o ,··· ,o istheinputsequence. TheDPproblemissaid
1 n
to satisfy the m-locality condition for some m ≥ n, if 6.1.ExperimentalDesign
thereexistfunctionsf,h ,··· ,h suchthatforalli∈[L],
1 K
Tasks and datasets. We chose three well-known tasks –
o =f(o ,··· ,o ),wherei−m≤h (i)<ifor
i h1(i) hK(i) k
LIS,ED,andArithmetic–torepresentavarietyofprob-
k ∈[K].
lemsintheDPdomain. FortheLIStask,thegoalistofind
Inotherwords, them-localityconditionsimplysaysthat thelengthofthelongestincreasingsubsequenceofagiven
6DoEfficientTransformersReallySaveComputation?
Arithmetic
ED LIS
3 2 6 4 1 2 8 2 5 6 5 1 2 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4 100
87
72 83
221 210 223
355 342 352
80
506 502
505
3 2 6 4 1 2 8 2 5 6 5 1 2 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4
87
72 83 60
221 210 223
355 342 352
506 502 40
505
3 2 6 4 1 2 8 2 5 6 5 1 2 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4
87
72 83
20 221 210 223
355 342 352
506 502
505
0
Figure1.Acomparisonofaccuraciesacrossdifferenttasksandmodeltypes.Eachcolumncorrespondstoatask(Arithmetic,ED,LIS),
andeachrowtoamodel(StandardTransformer,EfficientTransformer,SparseTransformer).Withineachsubplot,thex-axisrepresentsthe
embeddingdimension,andthey-axisdenotestheproblemsize.Thecolorintensitiesindicatetheaccuracylevelachievedbytherespective
models. ThefiguredemonstratesthatefficientTransformersneedlargerhiddendimensionsandthatthisrequirementincreaseswith
problemsize.ItalsohighlightshowstandardTransformerscanhandletasksacrossalldifficultylevelswithfixedembeddingdimensions.
integer sequence. The ED task’s goal is to calculate the Model configurations. For the standard Transformer
minimumcostrequiredtoconvertonesequencetoanother model, we use the same configurations as used by (Feng
usingthreebasiceditoperations: insert,delete,andreplace. et al., 2023), albeit with varying embedding dimensions.
FortheArithmetictask,thegoalistocalculatethecorrect We employ sinusoidal positional embeddings and apply
result of an arithmetic expression consisting of numbers, Xavier initialization to all parameters. For the activation
addition, subtraction, multiplication, division, and brack- function, we chose the standard GeLU, and the embed-
ets. TheLIStaskisthemostgeneralDPproblemwithout dingdimensionsfortheEDandLIStasksspantherange
localityproperty,whileEDandArithmeticexhibithigher 32,64,128,256,512,1024 uniformly. As for the Arith-
locality. metic task, we exclude the 1024 embedding dimensions
asallthemodelshavealreadyperformedwellin512. The
Following previous work (Feng et al., 2023), we curate
FFNlayer’shiddendimensionisfourtimestheembedding
fivedatasetsforeachtask,withdifferentproblemsizesand
dimension. We use the same configurations for both the
increasingdifficulty. FortheLIStask,thedatasetsencom-
LinearandSparseTransformers. WithintheSparseTrans-
passsequencesoflengths{40,110,175,250},equatingto √
CoT lengths of {83,223,353,503}. For the ED task, the
former,theblocksizeBis2⌊log 2( L)⌋,withLrepresenting
the upper limit of CoT length. Every experiment has the
datasetsspanvaryingsequencesoflengths–specifically,of
globaltokencountcfixedat1foreveryblock.
averagedlengths6,12,16,and20,equatingtomaximum
CoTlengthsof{72,210,342,506}.FortheArithmetictask, Modeltrainingandinference. Inallexperiments,weem-
the dataset consists of sequences with operator numbers ploy the AdamW optimizer (Loshchilov & Hutter, 2017)
in {6,10,13,16}, equating to maximum CoT lengths of with the following hyperparameters: β = 0.9,β =
1 2
{87,221,355,505}. Eachtrainingdatasethas1Msamples, 0.999,lr = 10−4, and weightdecay = 0.01. To enhance
andeachcorrespondingtestingdatasethas0.1M. model generalization, we maintain a consistent dropout
7
dradnatS
raeniL
esrapS
ycaruccADoEfficientTransformersReallySaveComputation?
ED_local
Table2.MinimumGFLOPsofdifferentmodeltypestoachievean
accuracyabove90%onArithmeticTask. 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4
100
Method Length87 Length221 Length355 Length505 66
Standard 0.027 0.266 0.426 0.605 80
Linear 0.422 4.220 6.756 9.584 198
60
Sparse 0.106 1.055 1.690 2.399
326 40
20
rateof0.1. Ouroptimizationminimizesthenegativelog-
486
likelihoodlossforalltokensinboththeCoTstepsandthe 0
answers. Eachmodeldoes100trainingepochswithabatch 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4
sizeof512. Duringtheinferencestage,themodelgenerates
100
theentireCoTprocesstokenbytoken,usinggreedysearch 66
80
untilreachingtheEnd-of-Sentencetoken. Weevalu-
198
atethemodels’performanceusingtheaccuracyofthefinal 60
answer,whichisthelastoutputinthesequence. Werunall
326 40
experimentsonfourV100GPUs.
20
486
0
6.2.ExperimentalResults
Figure2.AccuraciesoftheSparseTransformerandLinearTrans-
Figure1showsourmainresults. Eachcolumnofthefigure formerontheED Localtaskwithvaryingproblemsizeonthey-
correspondstoaparticulartask,andeachrowtoadifferent andembeddingdimensiononthex-axis.Wecanobservethatboth
model. Withineachsubplot,thex-axisshowstheembed- modelsbenefitfromlocality.
dingdimension,andthey-axisindicatestheproblemsize.
experimentinwhichwemodifiedtheEDproblemtohavea
Thecolorintensitiesindicatethecorrespondingaccuracies.
higherlocality.WecallthisproblemED Localandshowthe
For almost all tasks and varying problem sizes, models resultsinFigure2. Wecanclearlyseethattheincreasedlo-
withsufficientlylargeembeddingdimensionscanachieve calityledtoareductioninnecessaryembeddingsize,which
nearly 100% accuracy, except for the LIS task with the backsupourtheoreticalfindings.
SparseTransformer.Nevertheless,forthistask,theaccuracy
stillincreasesastheembeddingdimensionincreases. This 7.Limitations&Conclusion
findingshowsthatefficientTransformerscanhandlethese
DPtaskswithadequateexpressiveness. Limitations. Althoughweshowourresultsforrepresenta-
tiveefficientTransformers,itdoesnotmeanthatourfindings
Whenwecomparethesubplotscolumn-wise,itisevident
directlytransfertoallmodelswithsimilardesigns. Further,
thatefficientTransformersgenerallyneedlargerhiddendi-
despiteourexperimentsindicatingthatLinearTransformers
mensions than standard Transformers. Moreover, within
alsobenefitfromlocality,itremainstoprovewhetherthe
eachsubplotofefficientTransformers,therequiredembed-
boundfortheLinearTransformercanbetightened.
dingdimensionincreasesastheproblemgrows. Incontrast,
standardTransformers,withfixedembeddingdimensions Conclusion. While the Sparse Transformer and Linear
of128or256,canhandletasksacrossalldifficultylevels. Transformer are expressive enough to solve general DP
These observations confirm our theoretical findings, sug- tasks,ourfindingsindicatethattheycannotsustaintheiref-
gesting that efficient Transformers are less efficient than ficiencyinthegeneralcase. Thiscontradictstheanticipated
previouslyperceived. InTable2,wefurthercomparethe efficiencygains,bringingtheirperformanceclosertothat
minimalnumberofFLOPsrequiredtoachieve90%accu- ofstandardTransformers,whichmaintainaconstantmodel
racyonArithmeticforallmodels.ThestandardTransformer size. TheparadoxicalnatureoftheseefficientTransformers
requiresthelowestnumberofflopsacrossalllengths. promptsacrucialquestion: underwhatconditionsdothese
architecturesbecomeefficient? Bydelvingintoarithmetic
Comparingthesubplotsrow-wise,thegrowthinrequired
expressionevaluationandintroducingthelocalityassump-
embeddingdimensionwiththemodelsizebecomesmore
tion, we identify scenarios where efficient Transformers
pronounced as the locality decreases. This suggests that
canbeefficient. Ourtheoreticalresultsfindempiricalsup-
efficientTransformersaremoreefficientforDPtaskswith
portthroughextensiveexperimentsontaskslikeArithmetic,
stronglocality,whichalignswithourpreviousresults.
LongestIncreasingSubsequence(LIS),andEditDistance
LocalityStudy. Althoughthepreviousresultsalreadyin- (ED).Theobserveddependencybetweenhiddendimension
dicate that problems with higher locality require higher andproblemscaleforefficientTransformers,incontrastto
embeddingdimensions,weconductedanothermoreexplicit thestabilityinstandardTransformers,validatesourtheory.
8
esrapS
raeniL
ycaruccA
ycaruccADoEfficientTransformersReallySaveComputation?
8.ImpactStatement Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.BERT:
Pre-training of deep bidirectional transformers for lan-
Thispaperpresentsworkwhosegoalistoadvancethefield
guageunderstanding. InProceedingsofthe2019Confer-
of Machine Learning. There are many potential societal
enceoftheNorthAmericanChapteroftheAssociationfor
consequences of our work, none which we feel must be
ComputationalLinguistics: HumanLanguageTechnolo-
specificallyhighlightedhere.
gies,Volume1(LongandShortPapers),pp.4171–4186.
AssociationforComputationalLinguistics,2019.
References
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph,
Akyu¨rek, E., Schuurmans, D., Andreas, J., Ma, T., and
N.,Mann,B.,Askell,A.,Bai,Y.,Chen,A.,Conerly,T.,
Zhou, D. What learning algorithm is in-context learn-
etal. Amathematicalframeworkfortransformercircuits.
ing? investigations with linear models. arXiv preprint
TransformerCircuitsThread,1,2021.
arXiv:2211.15661,2022.
Feng,G.,Gu,Y.,Zhang,B.,Ye,H.,He,D.,andWang,L.
Alberti,S.,Dern,N.,Thesing,L.,andKutyniok,G. Sum- Towardsrevealingthemysterybehindchainofthought:a
former: Universalapproximationforefficienttransform- theoreticalperspective. AdvancesinNeuralInformation
ers. InTopological,AlgebraicandGeometricLearning ProcessingSystems,2023.
Workshops2023,pp.72–86.PMLR,2023.
Furst,M.,Saxe,J.B.,andSipser,M.Parity,circuits,andthe
Alman, J. and Song, Z. Fast attention requires bounded polynomial-timehierarchy. Mathematicalsystemstheory,
entries. arXivpreprintarXiv:2302.13214,2023. 17(1):13–27,1984.
Beltagy,I.,Peters,M.E.,andCohan,A. Longformer: The Garg,S.,Tsipras,D.,Liang,P.,andValiant,G. Whatcan
long-documenttransformer. arXiv:2004.05150,2020. transformers learn in-context? a case study of simple
function classes. In Advances in Neural Information
Bhattamishra, S., Ahuja, K., and Goyal, N. On the abil- ProcessingSystems,2022.
ityandlimitationsoftransformerstorecognizeformal
languages. arXivpreprintarXiv:2009.11264,2020. Gu, A. and Dao, T. Mamba: Linear-time sequence
modeling with selective state spaces. arXiv preprint
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D., arXiv:2312.00752,2023.
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Hahn,M. Theoreticallimitationsofself-attentioninneural
Askell,A.,etal. Languagemodelsarefew-shotlearners.
sequence models. Transactions of the Association for
In Advances in neural information processing systems,
ComputationalLinguistics,8:156–171,2020.
volume33,pp.1877–1901,2020.
Hao,Y.,Angluin,D.,andFrank,R. Formallanguagerecog-
Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
nitionbyhardattentiontransformers: Perspectivesfrom
eratinglongsequenceswithsparsetransformers. arXiv
circuitcomplexity. TransactionsoftheAssociationfor
preprintarXiv:1904.10509,2019.
ComputationalLinguistics,10:800–810,2022.
Choromanski,K.M.,Likhosherstov,V.,Dohan,D.,Song,
Hendrycks,D.andGimpel,K. Gaussianerrorlinearunits
X.,Gane,A.,Sarlos,T.,Hawkins,P.,Davis,J.Q.,Mo-
(gelus). arXivpreprintarXiv:1606.08415,2016.
hiuddin,A.,Kaiser,L.,etal. Rethinkingattentionwith
performers. In International Conference on Learning
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Representations,2021.
Transformersarernns: Fastautoregressivetransformers
withlinearattention. InInternationalconferenceonma-
Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and
chinelearning,pp.5156–5165.PMLR,2020.
Wei,F. Whycangptlearnin-context? languagemodels
implicitlyperformgradientdescentasmeta-optimizers. Keles, F. D., Wijewardena, P. M., and Hegde, C. On the
InICLR2023WorkshoponMathematicalandEmpirical computationalcomplexityofself-attention. InInterna-
UnderstandingofFoundationModels,2023. tionalConferenceonAlgorithmicLearningTheory,pp.
597–619.PMLR,2023.
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and
Salakhutdinov, R. Transformer-xl: Attentive language Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The
models beyond a fixed-length context. arXiv preprint efficienttransformer. arXivpreprintarXiv:2001.04451,
arXiv:1901.02860,2019. 2020.
9DoEfficientTransformersReallySaveComputation?
Kojima,T.,Gu,S.S.,Reid,M.,Matsuo,Y.,andIwasawa, Pe´rez,J.,Barcelo´,P.,andMarinkovic,J. Attentionisturing
Y. Largelanguagemodelsarezero-shotreasoners. InAd- complete. TheJournalofMachineLearningResearch,
vancesinNeuralInformationProcessingSystems,2022. 22(1):3463–3497,2021.
Liu,B.,Ash,J.T.,Goel,S.,Krishnamurthy,A.,andZhang, Qiu,J.,Ma,H.,Levy,O.,Yih,W.-t.,Wang,S.,andTang,J.
C. Transformers learn shortcuts to automata. arXiv Blockwiseself-attentionforlongdocumentunderstand-
preprintarXiv:2210.10749,2022. ing. In Findings of the Association for Computational
Linguistics: EMNLP2020,pp.2555–2565,2020.
Liu,B.,Ash,J.T.,Goel,S.,Krishnamurthy,A.,andZhang,
C. Transformers learn shortcuts to automata. In The Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
Eleventh International Conference on Learning Repre- Sutskever,I.,etal. Languagemodelsareunsupervised
sentations,2023. multitasklearners. OpenAIblog,1(8):9,2019.
Loshchilov,I.andHutter,F. Fixingweightdecayregular- Roy,A.,Saffar,M.,Vaswani,A.,andGrangier,D. Efficient
izationinadam. 2017. content-based sparse attention with routing transform-
ers. TransactionsoftheAssociationforComputational
Luo, S., Li, S., Cai, T., He, D., Peng, D., Zheng, S., Ke, Linguistics,9:53–68,2021.
G., Wang, L., and Liu, T.-Y. Stable, fast and accurate:
Sanford,C.,Hsu,D.,andTelgarsky,M. Representational
Kernelized attention with relative positional encoding.
InAdvancesinNeuralInformationProcessingSystems, strengthsandlimitationsoftransformers. arXivpreprint
arXiv:2306.02896,2023.
volume34,pp.22795–22807,2021.
Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J.,
Merrill, W. and Sabharwal, A. The parallelism tradeoff:
Wang, J., and Wei, F. Retentive network: A successor
Limitationsoflog-precisiontransformers. Transactions
totransformerforlargelanguagemodels(2023). URL
oftheAssociationforComputationalLinguistics,2023.
http://arxiv.org/abs/2307.08621v1.
Merrill, W., Sabharwal, A., and Smith, N. A. Saturated
Tay, Y., Bahri, D., Yang, L., Metzler, D., andJuan, D.-C.
transformersareconstant-depththresholdcircuits. Trans-
Sparsesinkhornattention. InInternationalConference
actionsoftheAssociationforComputationalLinguistics,
onMachineLearning,pp.9438–9447.PMLR,2020a.
10:843–856,2022.
Tay,Y.,Dehghani,M.,Abnar,S.,Shen,Y.,Bahri,D.,Pham,
Nye,M.,Andreassen,A.J.,Gur-Ari,G.,Michalewski,H.,
P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long
Austin,J.,Bieber,D.,Dohan,D.,Lewkowycz,A.,Bosma,
range arena: A benchmark for efficient transformers.
M.,Luan,D.,Sutton,C.,andOdena,A. Showyourwork:
arXivpreprintarXiv:2011.04006,2020b.
Scratchpadsforintermediatecomputationwithlanguage
models. InDeepLearningforCodeWorkshop,2022.
Tay,Y.,Dehghani,M.,Bahri,D.,andMetzler,D. Efficient
transformers: Asurvey,2022.
Olsson,C.,Elhage,N.,Nanda,N.,Joseph,N.,DasSarma,
N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A.,Conerly,T.,Drain,D.,Ganguli,D.,Hatfield-Dodds, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Z.,Hernandez,D.,Johnston,S.,Jones,A.,Kernion,J., Bhosale,S.,etal. Llama2: Openfoundationandfine-
Lovitt,L.,Ndousse,K.,Amodei,D.,Brown,T.,Clark,J., tuned chat models. arXiv preprint arXiv:2307.09288,
Kaplan,J.,McCandlish,S.,andOlah,C.In-contextlearn- 2023.
ingandinductionheads. TransformerCircuitsThread,
2022. https://transformer-circuits.pub/2022/in-context- Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
learning-and-induction-heads/index.html. L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
tentionisallyouneed. Advancesinneuralinformation
OpenAI. Gpt-4 technical report. arXiv preprint processingsystems,30,2017.
arXiv:2303.08774,2023.
VonOswald,J.,Niklasson,E.,Randazzo,E.,Sacramento,
Peng,H.,Pappas,N.,Yogatama,D.,Schwartz,R.,Smith, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov,
N. A., and Kong, L. Random feature attention. arXiv M. Transformers learn in-context by gradient descent.
preprintarXiv:2103.02143,2021. In International Conference on Machine Learning, pp.
35151–35174.PMLR,2023.
Pe´rez, J., Marinkovic´, J., and Barcelo´, P. On the turing
completeness of modern neural network architectures. Vyas, A., Katharopoulos, A., and Fleuret, F. Fast trans-
arXivpreprintarXiv:1901.03429,2019. formerswithclusteredattention. InAdvancesinNeural
10DoEfficientTransformersReallySaveComputation?
InformationProcessingSystems,volume33,pp.21665–
21674,2020.
Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.
Linformer: Self-attentionwithlinearcomplexity. arXiv
preprintarXiv:2006.04768,2020.
Wei, C., Chen, Y., and Ma, T. Statistically meaningful
approximation: a case study on approximating turing
machineswithtransformers. AdvancesinNeuralInfor-
mationProcessingSystems,35:12071–12083,2022a.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,
Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought
prompting elicits reasoning in large language models.
AdvancesinNeuralInformationProcessingSystems,35:
24824–24837,2022b.
Weiss,G.,Goldberg,Y.,andYahav,E. Thinkingliketrans-
formers. InInternationalConferenceonMachineLearn-
ing,pp.11080–11090.PMLR,2021.
Yao,S.,Peng,B.,Papadimitriou,C.,andNarasimhan,K.
Self-attentionnetworkscanprocessboundedhierarchical
languages. InProceedingsofthe59thAnnualMeeting
oftheAssociationforComputationalLinguisticsandthe
11thInternationalJointConferenceonNaturalLanguage
Processing (Volume 1: Long Papers), pp. 3770–3785,
2021.
Yun, C., Bhojanapalli, S., Rawat, A.S., Reddi, S.J., and
Kumar, S. Are transformers universal approximators
of sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077,2019.
Zhou,D.,Scha¨rli,N.,Hou,L.,Wei,J.,Scales,N.,Wang,
X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V.,
andChi,E.H. Least-to-mostpromptingenablescomplex
reasoning in large language models. In The Eleventh
InternationalConferenceonLearningRepresentations,
2023.
11DoEfficientTransformersReallySaveComputation?
A.TechnicalLemmas
Inthissection,weproposesomelemmasabouttheexpressivepoweroftheMLP,thelineartransformer,andthesparse
transformer. DuetothesimilaritybetweentheefficientandthestandardTransformer,webasesomeideasandconstructions
onpreviouswork(Fengetal.,2023).
A.1.LemmasforMLP
Inthepreviouswork(Fengetal.,2023),theauthorsinvestigatedtheexpressivepoweroftheMLPwithGeLUactivation
function. Theyshowedthatatwo-layerMLPwithGeLUactivationandafixednumberoflog-precisionneuronscanperform
varioustaskssuchasmultiplication,lineartransformation,andselection. Buildingontheirwork,ourproofismoreconcise
andclear. Wewillrestatetherelevantlemmasbelow,andrefertotheappendicesofFengetal.(2023)fortheproofs.
LemmaA.1(FromFengetal.(2023)). Letf :R2 →Rbeatwo-layerMLPwithGeLUactivation,andthehiddendimension
is4. Then,foranyϵ>0andM >0,thereexistMLPparameterswithℓ normupperboundedbyO(poly(M,1/ϵ))such
∞
that|f(a,b)−ab|≤ϵholdsforalla,b∈[−M,M].
LemmaA.2(FromFengetal.(2023)). Letg :Rd1 →Rd2 beatwo-layerMLPwithReLUactivation,andallparameter
values are upper bounded by M. Then, for any ϵ > 0, there exists a two-layer MLP f of the same size with GeLU
activation and parameters upper bounded by O(poly(M,1/ϵ)) in the ℓ
∞
norm, such that for all x ∈ Rd1, we have
∥f(x)−g(x)∥ ≤ϵ.
∞
LemmaA.3(FromFengetal.(2023)). Letf : Rd1 → Rd2 beatwo-layerMLPwithGeLUactivation,andthehidden
dimensionis2d 2. LetW ∈ Rd2×d1 beanymatrixanddenoteM = max ij|W ij|. Then,foranyϵ > 0,thereexistMLP
parameterswithℓ ∞normboundedbyO(poly(M,1/ϵ)),suchthatforanyx∈Rd1,wehave∥f(x)−Wx∥
∞
≤ϵ.
LemmaA.4(FromFengetal.(2023)). Definetheselectionfunctiong :Rd×Rd×R→Rdasfollows:
(cid:26)
x ift≥0,
g(x,y,t)= (11)
y ift<0.
Letf :Rd×Rd×R→Rdbeatwo-layerMLPwithGeLUactivation,andthehiddendimensionis2d+2. Then,forany
ϵ>0,α>0,andM >0,thereexistMLPparameterswithℓ normboundedbyO(poly(M,1/α,1/ϵ)),suchthatforall
∞
x∈[−M,M]d,y ∈[−M,M]d,andt∈[−∞,−α]∪[α,+∞],wehave∥f(x,y,t)−g(x,y,t)∥ ≤ϵ.
∞
LemmaA.5. Forintegeri,defineS =(cid:2) i− 1,i+ 1(cid:3) . Letn∈ZandD =∪n S . Definethefunctiong :D →Rn as
i 3 3 i=0 i
follows: f(x) = m ifx ∈ S ,wherem hasitsfirstientries1andtherestentries0. Letf : R → Rn beatwo-layer
i i i
MLPwithGeLUactivation,andthehiddendimensionis2n. Then,foranyϵ>0,α>0,thereexistsMLPparameterswith
l normboundedbyO(poly(n,1/ϵ)),suchthatforallx∈D,wehave∥f(x)−g(x)∥ ≤ϵ.
∞ ∞
Proof. Noticethatthek-thentryofm is
j
(cid:20) (cid:18) (cid:19)(cid:21) (cid:20) (cid:18) (cid:19)(cid:21)
3 1
I[k ≤j]=ReLU −2k+2 j+ −ReLU −2k+2 j+ (12)
4 4
Thuseachentryofm canbeimplementedbyanMLPwithhiddendimension2. ByLemmaA.2,wecanperformboth
j
tasksusinganMLPwithGeLUactivation,withhiddendimension2n.
LemmaA.6. Forintegeri,defineS =(cid:2) i− 1,i+ 1(cid:3) . Letn∈ZandD =∪n S . Definethefunctiong :D →Rn as
i 3 3 i=0 i
follows: f(x)=n ifx∈S ,wheren hasitsfirsti−1entries0andtherestentries1. Letf :R→Rnbeatwo-layer
i i i
MLPwithGeLUactivation,andthehiddendimensionis2n. Then,foranyϵ>0,α>0,thereexistsMLPparameterswith
l normboundedbyO(poly(n,1/ϵ)),suchthatforallx∈D,wehave∥f(x)−g(x)∥ ≤ϵ.
∞ ∞
Proof. Similarlytothepreviousproof,thek-thentryofn is
j
(cid:20) (cid:18) (cid:19)(cid:21) (cid:20) (cid:18) (cid:19)(cid:21)
3 1
I[k ≥j]=ReLU 2k−2 j− −ReLU 2k−2 j− (13)
4 4
whichindicatesthateachentryofn canbeimplementedbyanMLPwithhiddendimension2. ByLemmaA.2,wecan
j
performbothtasksusinganMLPwithGeLUactivation,withhiddendimension2n.
12DoEfficientTransformersReallySaveComputation?
LemmaA.7. Forintegeri,defineS = (cid:2) i− 1,i+ 1(cid:3) . Givenpositiveintegernandintegeri ∈ [1,n2]. Thereexistsa
i 4 4
two-layerMLPf :R→R3withGeLUactivationandhiddendimensionO(n),suchthatforanyintegeri∈[1,n2]and
x ∈ S , f(x) = (⌈i/n⌉, ⌈i/n⌉−1, n−(⌈i/n⌉×n−i)). Here, n−(⌈i/n⌉×n−i) = imodnifimodn ̸= 0, and
i
n−(⌈i/n⌉×n−i)=nifimodn=0.
Proof. Sincei∈{1,2,··· ,n2},wecanget⌈i/n⌉∈{1,2,··· ,n}. Bythepropositionthatiisaninteger,wecanusean
MLPwithhiddendimension2nandReLUactivationtocalcualte⌈i/n⌉asfollowing:
n n (cid:20) (cid:18) (cid:19)(cid:21) (cid:20) (cid:18) (cid:19)(cid:21)
(cid:88) (cid:88) 3 1
⌈i/n⌉= I[i≤jn]= ReLU −2i+2 jn+ −ReLU −2i+2 jn+ (14)
4 4
j=1 j=1
Thuswecancalculate⌈i/n⌉,⌈i/n⌉−1,n−(⌈i/n⌉×n−i)taskbyanMLPwithReLUactivationwithhiddendimension
O(n). ByLemmaA.2,wecanfinishthesetasksbyanMLPwithGeLUactivationwithhiddendimensionO(n).
A.2.LemmasforLinearTransformer
LemmaA.8. TheattentionmoduleinLinearTransformer
(cid:80) ϕ(W x )⊤ϕ(W x )W x
Attn(x )= j≤i K j Q i V i (15)
i (cid:80) ϕ(W x )⊤ϕ(W x )
j≤i K j Q i
canbeimplementedasfollows:
s =0, z =0 (16)
0 0
s =s +ϕ(W x )(W x )⊤ (17)
i i−1 K i V i
z =z +ϕ(W x ) (18)
i i−1 K i
ϕ(W x )⊤s
Attn(x )⊤ = Q i i (19)
i ϕ(W x )⊤z
Q i i
SincethisimplementationisquitesimilartoRNNs,wecalleds,zinourimplementationhiddenstates.
Now,weintroducetheAGGoperationforLinearTransformer. Letmbeanintegerandletx ,x ,··· ,x beasequence
1 2 m2
ofvectorswherex =(xˆ ,e ,e ,1)∈[−M,M]d+m+1,xˆ ∈Rd,ande ,e areone-hotvectorsinRm,andM
i i ⌈i/m⌉ qi i ⌈i/m⌉ qi
isalargeconstant. LetS = {j ∈ Z : q m−(m−1) ≤ j ≤ min(q m,i)}. DefinetheAGGoperationasfollows: The
i i i
outputisasequenceofvectorsu ,··· ,u withu =mean xˆ . TheoutputisundefinedwhenS =∅.
1 m2 i j∈Si j i
LemmaA.9. Forany0 < ϵ ≤ M,thereexistsanlinearattentionlayerwithhiddendimensionO(max(d,m))andone
causalattentionheadthatcanapproximatetheAGGoperationdefinedabove. Specifically,supposetheattentionoutputare
o ,··· ,o ,thenwehave∥o −u ∥ ≤ϵforalli∈[m2]. Moreover,thel normofattentionparametersarebounded
1 m2 i i ∞ ∞
byO(poly(log(M),log(m),log(1/ϵ)).
Proof. Withoutlossofgenerosity,wecanassumed=m(otherwise,wecandosomepaddingwith0). Wewillprovidea
proofbasedontheimplementofLemmaA.8.
Weconstructthequery,keyandvaluevectorsasfollows:
• Query: q =µe −µ
i qi
• Key: k =µe −µ
i ⌈i/m⌉
• Value: v =xˆ
i i
whereµ>0isaconstantdefinedlater. ThiscanbeachievedbysettingappropriateW ,W ,W .
Q K V
Noticethatj ∈S ifandonlyifj ≤iand⌈j/m⌉=q . Bytheconstructionofq ,k ,v ,wecanget
i i i i i
(cid:40)
1+(m−1)exp(−µ), j ∈S
a :=q⊤k = i
ij i j 2exp(−µ)+(m−2)exp(−2µ), j ∈/ S
i
13DoEfficientTransformersReallySaveComputation?
Bytakingµ=ln2Mm3 ,whichisboundedbyO(poly(log(M),log(m),log(1/ϵ)),wehave
ϵ
(cid:88)
a ≤
(m2−|S i|)[2exp(−µ)+(m−2)exp(−2µ)]
ij (m2−|S |)[2exp(−µ)+(m−2)exp(−2µ)]+|S |[1+(m−1)exp(−µ)]
i i
j∈/Si
1
=
1+ |Si| · 1+(m−1)exp(−µ)
m2−|Si| 2exp(−µ)+(m−2)exp(−2µ)
m2−|S | 2exp(−µ)+(m−2)exp(−2µ)
≤ i ·
|S | 1+(m−1)exp(−µ)
i
ϵ
≤m2·mexp(−µ)=m3exp(−µ)=
2M
Similarly,forj ∈S ,wehave
i
(cid:12) (cid:12)
(cid:12) 1 (cid:12)
(cid:12) (cid:12)a ij − |S |(cid:12) (cid:12)
i
1 1+(m−1)exp(−µ)
≤ −
|S | (m2−|S |)[2exp(−µ)+(m−2)exp(−2µ)]+|S |[1+(m−1)exp(−µ)]
i i i
1 1
= −
|S i| (m2−|S i|)2exp 1(− +µ (m)+ −( 1m )− ex2 p) (e −xp µ( )−2µ) +|S i|
(m2−|S |)2exp(−µ)+(m−2)exp(−2µ)
i 1+(m−1)exp(−µ)
=
(cid:104) (cid:105)
|S | (m2−|S |)2exp(−µ)+(m−2)exp(−2µ) +|S |
i i 1+(m−1)exp(−µ) i
1
≤ (m2−|S |)·[2exp(−µ)+(m−2)exp(−2µ)]
|S | i
i
1 1 ϵ
≤ m2·mexp(−µ)= m3exp(−µ)=
|S | |S | 2M|S |
i i i
Wethusobtain
(cid:13) (cid:13)  
(cid:13) (cid:13) (cid:12) (cid:12)
(cid:13)(cid:88) 1 (cid:88) (cid:13) (cid:88) (cid:88) (cid:12) 1 (cid:12)
∥o i−u i∥ ∞ =(cid:13) (cid:13)
(cid:13) j
a ijxˆ j − |S i| j∈Sixˆ j(cid:13) (cid:13)
(cid:13)
∞
≤m jax∥xˆ j∥ ∞· j∈/Sia ij + j∈Si(cid:12) (cid:12)a ij − |S i|(cid:12) (cid:12)≤ϵ
whichconcludesourproof.
A.3.LemmasforSparseTransformer
ThepreviousworkbyFengetal.(2023)studiedtheexpressivepowerofthestandardattentionlayerandintroducedabasic
operation,COPY,thatcanbeimplementedbytheattentionlayer. Inthissection,weintroduceanupdatedversionofthe
COPYoperationforthesparsetransformeranddemonstratethatthesparsetransformermoduleiscapableofimplementing
thisoperation.
Firstly,wewillenumeratetheelementsandtheirrespectivenotationsthatwillbeemployedtodefinetheCOPYoperation.
• Asequenceofvectorsx ,x ,··· ,x ∈Rd+2,wherex =[v ,r ,imodB,⌈i⌉,1],x ∈Rd,r ∈R,B ∈Z.
1 2 n i i i B i i
• ThreematricesK,Q,V ∈Rd′×(d+2),where∥V∥ ≤1
∞
• TheattentionsetsS′foreachi,whereS′istheindicesofthetokensattendedbythei-thtoken.
i i
• Tworealnumberρ,δ >0
Denotingthatk =Kx ,q =Qx ,v =Vx ∈Rd′,wecandefinethematchingsetS fortheiasS ={j <i|q ·k <
i i i i i i i i i j
ρ}∩S′. ThentheoutputoftheCOPYoperationu ,u ,··· ,u isdefinedasu =v ,wherepos(i)=argmax r .
i 1 2 n i pos(i) j∈Si j
14DoEfficientTransformersReallySaveComputation?
Intuitively,theCOPYoperationcopiestheembeddingofthematchingtokenwiththehighestrating. Notethattheoutputof
theCOPYoperationcanbeanythingwhenS isempty.
i
Moreover,wemakethefollowingregularityassumption:
AssumptionA.10. Theinputsequencex ,x ,··· ,x andthematricesK,Q,V satisfythefollowingcondition:
1 2 n
• Foranyi,j ∈[n],either|q ·k |≤ρorq ·k ≤−δ
i j i j
• Foranyi,j ∈[n],eitheri=j or|r −r |≥δ
i j
• ∥V∥ ≤1
∞
AssumptionA.10ensuresthatonlyonetokenwiththehighestratingismatched,andthereisasubstantialdifferencebetween
thistokenandothers. Thesubsequentlemmademonstratesthatthesparsetransformermodulecaneffectivelyimplementthe
COPYoperation.
LemmaA.11(COPYOperationofStandardTransformer,FromFengetal.(2023)). GiventhatAssumptionA.10holds
withρ ≤ δ2 ,thereexistsastandardtransformermoduledefinedinEquation(6)withoneattentionlayer,blocksizeB,
8M
embeddingsizeO(d),andoneattentionheadthatcanapproximatetheCOPYoperationdefinedabove. Specifically,for
anysequenceofvectorsx ,x ,··· ,x ,denotethecorrespondingoutputoftheattentionlayeraso ,o ,··· ,o . Then,
1 2 n 1 2 n
wehave∥o −u ∥ ≤ ϵforalli ∈ [n]withS ̸= ∅. Furthermore,theℓ normofattentionparametersisboundedby
i i ∞ i ∞
O(poly(M,1/δ,log(n),log(1/ϵ))).
Now,wewillproveaspecialformoftheCOPYoperationfortheSparseTransformer,suchthatgivenanindexofatoken,
COPYtheembeddingofthetokenwiththegivenindex.
Lemma A.12 (COPY Operation of Sparse Transformer). Given that Assumption A.10 holds with ρ ≤ δ2 , there ex-
8M
ists a sparse transformer module defined in Equation (6) with three attention layer, block size B, embedding size
O(d), andoneattentionheadthatcanapproximatetheCOPYoperationdefinedabove. Specifically, foranysequence
of vectors x ,x ,··· ,x , denote the corresponding output of the attention layer as o ,o ,··· ,o . Then, we have
1 2 n 1 2 n
∥o − u ∥ ≤ ϵ for all i ∈ [n] with S ̸= ∅. Furthermore, the ℓ norm of attention parameters is bounded by
i i ∞ i ∞
O(poly(M,1/δ,log(n),log(1/ϵ))).
The proof for this version is an extension of the proof for a Lemma described in Feng et al. (2023). For the reader’s
convenience,weprovideaconciseproofasfollows.
Proof. Layer1. Inourformulation,theinputembeddingx hastheformx =(v ,p ,imodB,⌈i⌉,1),wherep
i i i target B target
isthetargetindexoftheCOPYoperation. TheprimaryobjectiveofthefirstlayeristoutilizeitsMLPtotransformtheinput
embedding. Thetargetoutputofthislayeris
(cid:40)
i 0 ,whenj ̸=imodB+1
[v ,v ,··· ,v ,p ,imodB,⌈ ⌉,1], where v =
i,1 i,2 i,B target B i,j v ,whenj =imodB+1
i
Notethatv isboundby[−M,M]. Therefore,wecanexpressv as
i i,j
(cid:16) (cid:0) (cid:1)(cid:17)
v =ReLU ReLU(v +M)−M · 1+·ReLU(j−imodB)+·ReLU(imodB−j)
i,j i
AccordingtoLemmaA.2,anMLPwithGeLUastheactivationfunctioncanaccomplishthistaskwitharbitrarilysmall
error. Therefore,wecanwritetheoutputofthislayerasx(1) =[v ,v ,··· ,v ,p ,imodB,⌈i⌉,1].
i i,1 i,2 i,B target B
Layer 2. The main task of the second layer is to sum over the embedding of previous B tokens. In this layer, the
attention head just pays uniform attention to the previous B tokens. Note that, in the first layer, we distribute the
embeddingsofdifferenttokensonthedifferentdimensionsoftheembedding. Therefore,theoutputoftheattentionlayeris
1 ·[v ,v ,··· ,v ,···v ,p ,imodB,⌈i⌉,1]. Then,byusingtheMLP
B (i−B+imodB+1) (i−B+imodB+2) i (i−B+imodB) target B
to multiply the number of tokens B, we can get the embeddings of all tokens. Moreover, in this layer, according to
LemmaA.7wecanusetheMLPtocalculatetheindexoftheblockofthetargettokenas⌈ptarget⌉. Theoutputofthislayeras
B
i p
x(2) =[v ,v ,··· ,v ,···v ,p ,imodB,⌈ ⌉,⌈ target⌉,1].
i (i−B+imodB+1) (i−B+imodB+2) i (i−B+imodB) target B B
15DoEfficientTransformersReallySaveComputation?
Layer3. ThemaintaskofthesecondlayeristoCOPYtheembeddingofthetokeninthecorrespondingblockandselect
thetargetembedding. Inthislayer,theattentionheadjustCOPYtheembeddingofthetokeninthecorrespondingblock.
AccordingtoLemmaA.11,theattentionlayercancompletethistaskwitharbitrarilysmallerrors. Finally,weusetheMLP
toselectthetargetembeddingfromthecopiedembeddingandobtainthefinaloutput.
B.ProofsofthetheoremsinSection4
Foreaseofreading,wereclaimsomeimportantassumptionshere,whicharenaturalandmild.
AssumptionB.1. Eachfunctionf,g,handudefinedinCoTforDPcanbeapproximatedbyconstantsizeMLPwith
GeLUactivation.
AssumptionB.2. ThestatetransitionfunctionF,wheretheinputsarethetokennumberandcurrentstate,andtheoutputs
arethenextstate,canbeapproximatedwithMLPandGeLUfunction.
InadditionwereplaceallresidualconnectionsinattentionlayersandMLPwithconcatenation,whichdoesn’tchangethe
expressivepowerofmodelarchitecture.
B.1.ProofoftheTheorem4.2
Inthissubsection,weprovideacompleteproofofTheorem4.2. Forclarity,werestatethetheoremforthelineartransformer
andsparsetransformerindividuallyandprovidetheirrespectiveproofs.
TheoremB.3. ConsideranyDPproblemsatisfyingthesameconditionasinTheorem4.1. Givenanyintegern>0,letL
bethelengthoftheoutputsequencewhentheinputsequencelengthisn. Then,thereisa(log-precision)lineartransformer
√
withaconstantdepthM,aconstantnumberofattentionheadsH,andahiddendimensionD =O( L)thatcangenerate
thecorrectoutputforallinputssoflengthn.
√
Proof. Denote W = ⌈ L⌉. In this formulation, the embeddings are x(0) = (einput,estate,edp,eanswer,k,1), where
k k k k k
einput,estate,edp,eanswer corresponds to input token, DP state, DP value, final result. The embeddings are 0 if it’s not
k k k k
defined. Weconstructthelayersasfollows.
Block1. AccordingtoAssumptionB.2,wecanuseconstantlayersofMLPstoobtainenextstatewithinputestate. Thiscanbe
k k
donebysettingzeroweightmatricesinattentionlayersanddiscardattentionoutputinlinearprojectionofMLP.Theoutput
ofthisblockisx(1) =(einput,estate,enextstate,edp,k,1).
k k k k k
Block2. ThesecondlayeroftheTransformerfinishesthefollowingtasks:
• Calculateh(enextstate),g(enextstate).
k k
• Calculatefstateastheindicatorvariableofestateisthelaststate.
k k
• Calculate(h(enextstate))2,(g(enextstate))2,(estate)2,whichareelement-wisesquareoperations,andk2.
k k k
ThefirsttaskcanbeimplementedbyseverallayersofMLPsbyAssumptionB.1. Toperformthesecondtask,wecancheck
whetherestate ̸=0andenextstate =0. ThelasttaskcanbedonewithanMLPusingLemmaA.1. Theoutputofthisblockis
k k
x(2) =(einput,estate,enextstate,edp,esep,h(enextstate),g(enextstate),
k k k k k k k k
(h(enextstate))2,(g(enextstate))2,(estate)2,fstate,k,k2,1).
k k k k
Block3. ThethirdblockoftheTransformerusesK+J headstoperformthefollowingtaskswhereK andJ aredefinedin
(9),referstothetransitioninDP:
• Getinputsofs ,··· ,s whereicorrespondstoenextstate.
g1(i) gJ(i) k
• GetDPvalueofdp(h (i)),··· ,dp(h (i))foricorrespondstoenextstate.
1 K k
• Calculateenextdp,i.e.,theDPvalueofthenextstate,enextstate.
k k
16DoEfficientTransformersReallySaveComputation?
Toperformthefirsttwotasks,weneedtofirstcalculatetheabsolutepositionoftheembeddingswewant. Thiscanbeimple-
mentedbyseveralMLPsbyAssumptionB.1andproblemsizen.Supposetheabsolutepositionsaregˆ ,··· ,gˆ ,hˆ ,··· ,hˆ .
1 J 1 K
Thenwecandothefollowingtasksfort=gˆ ,··· ,gˆ ,hˆ ,··· ,hˆ :
1 J 1 K
1. Calculate⌈t/W⌉,W −(⌈t/W⌉×W −t)byLemmaA.7.
2. Calculatee ∈RW fors=⌈t/W⌉,W −(⌈t/W⌉×W −t)byLemmaA.5,A.6andthefactthate =m ⊙n .
s i i i
Wealsocalculatee ∈RW fors=⌈k/W⌉,W −(⌈k/W⌉×W −k)byLemmaA.5,A.6. Thenwecanget
s
(I[W −(⌈k/W⌉×W −k)=1]·x(2),··· ,I[W −(⌈k/W⌉×W −k)=1]·x(2))
k k
bythemultiplicationbetweene andx(2)whichcanbeimplementedbyanMLPwithhiddendimension
W−(⌈k/W⌉×W−k) k
O(W). Then,wecangetx(2)fort=gˆ ,··· ,gˆ ,hˆ ,··· ,hˆ infollowingsteps:
t 1 J 1 K
1. Get
1
(x(2) ,··· ,x(2) )(i≥⌈t/W⌉×W)
W ⌈t/W⌉×W−(W−1) ⌈t/W⌉×W
or
1
(x(2) ,··· ,x(2),0,··· ,0)(i<⌈t/W⌉×W)
i−⌈t/W⌉×(W −1) ⌈t/W⌉×W−(W−1) i
fort=gˆ ,··· ,gˆ ,hˆ ,··· ,hˆ byusingLemmaA.9.
1 J 1 K
2. Get
(x(2) ,··· ,x(2) )(i≥⌈t/W⌉×W)
⌈t/W⌉×W−(W−1) ⌈t/W⌉×W
or
(x(2) ,··· ,x(2),0,··· ,0)(i<⌈t/W⌉×W)
⌈t/W⌉×W−(W−1) i
fort = gˆ ,··· ,gˆ ,hˆ ,··· ,hˆ byusingLemmaA.1formultiplicationfirst, anduseLemmaA.4toimplementa
1 J 1 K
conditionalselection.
3. Getx(2)fort=gˆ ,··· ,gˆ ,hˆ ,··· ,hˆ bymultiplicationbetweentheabovevectorande ,thensum
t 1 J 1 K W−(⌈t/W⌉×W−t)
upthem. Thenwecangets ,··· ,s ;dp(h(n)(i)),··· ,dp(h(n)(i))bylinearprojection.
g(n)(i) g(n)(i) 1 K
1 J
Ifthecorrespondingfieldisundefined,wecanmarkthemassomespecialvalue. ThiscanbedonebyselectionusingMLP
(LemmaA.4). AssumptionB.1indicatesthatwecancalculatethefunctionf (definedin(9))usinganMLP.Theoutputof
thislayeris
x(3) =(enextstate,edp,enextdp,n ,fstate,k,1).
k k k k k k
Block4. Thefourthblockoftheautoregressivetransformergeneratestheoutputbasedontheflagfstate. Wecalculateu(edp)
k k
togetthefinalresult. Iffstate =1,thenweselectu(edp)astheoutput;otherwise,weprepareandoutputtheDPresultfor
k k
thenextstate,i.e.,enextstateandenextdp. ThisisaconditionalselectionoperationandthuscanbeimplementedbyanMLP
k k
(LemmaA.4).
TheoremB.4. ConsideranyDPproblemsatisfyingthesameconditionasinTheorem4.1. Givenanyintegern>0,letL
bethelengthoftheoutputsequencewhentheinputsequencelengthisn. Then,thereisa(log-precision)sparseTransformer
√
withblocksizeB =Θ( L)withaconstantdepthM,aconstantnumberofattentionheadsH,andahiddendimension
√
D =O( L)thatcangeneratethecorrectoutputforallinputssoflengthn.
17DoEfficientTransformersReallySaveComputation?
Proof. Theconstructionofthefirsttwoblocksisthesameasthelineartransformer,andwewillgivetheconstructionofthe
thirdblockandthefourthblockforthesparsetransformer.
Block3. ThethirdblockoftheTransformerusesK+J headstoperformthefollowingtaskswhereK andJ aredefinedin
(9),referstothetransitioninDP:
• Getinputsofs ,··· ,s whereicorrespondstoenextstate.
g1(i) gJ(i) k
• GetDPvalueofdp(h (i)),··· ,dp(h (i))foricorrespondstoenextstate.
1 K k
• Calculateenextdp,i.e.,theDPvalueofthenextstate,enextstate.
k k
To perform the first two tasks, we need to first calculate the absolute position of the embeddings we want. This
can be implemented by several MLPs by Assumption B.1 and problem size n. Supposing the absolute positions are
gˆ ,··· ,gˆ ,hˆ ,··· ,hˆ ,wecanperformtheCOPYoperationbyLemmaA.12,andwecanobtaintheinputs ,··· ,s
1 J 1 K g1(i) gJ(i)
andtheDPvalueofdp(h (i)),··· ,dp(h (i))foricorrespondstoenextstate. WiththeinputsandDPvalue,accordingto
1 K k
AssumptionB.1,wecancalculatethefunctionf (definedin(9))usinganMLP.Theoutputofthislayeris
x(3) =(enextstate,edp,enextdp,n ,fstate,k,1).
k k k k k k
Block4. Thefourthblockofthesparsetransformergeneratestheoutputbasedontheflagfstate. Wecalculateu(edp)toget
k k
thefinalresult. Iffstate =1,thenweselectu(edp)astheoutput;otherwise,weprepareandoutputtheDPresultforthenext
k k
state,i.e.,enextstateandenextdp. ThisisaconditionalselectionoperationandthuscanbeimplementedbyanMLP(Lemma
k k
A.4).
B.2.TheproofofTheorem4.4
Inthissubsection,weprovideacompleteproofofTheorem4.2. Forclarity,werestatethetheoremforthelineartransformer
andsparsetransformerindividuallyandprovidetheirrespectivefullproofs.
TheoremB.5. ConsideranyregularDPproblemsatisfyingthesameconditionasinTheorem4.1. Assumethattheoutput
sequencelengthLisproportionaltotheinputsequencelengthn,i.e.,L = Θ(n). Then,givenasufficientlylargen,for
(log-precision)linearTransformer,amodelwithaconstantdepthM andaconstantnumberofattentionheadsH can
√
generatethecorrectoutputforallinputssoflengthnonlyifthehiddendimensionD =Ω˜( L).
Proof. ByLemmaA.8,wecanknowthehiddenstatescorrespondingtoeachLinearTransformerlayerandthelastinput
token (which is a fixed special token) determines the CoT output. Thus, the size of hidden states should be at least
Ω(n)=Ω(L). Bytheregularityassumption,weknowthatdifferentinputshouldcorrespondstodifferenthiddenstates.
√
ThisimpliesthatthehiddendimensionshouldbeatleastΩ((cid:112) L/logL)=Ω˜( L),concludingourproof.
TheoremB.6. ConsideranyregularDPproblemsatisfyingthesameconditionasinTheorem4.1. Assumethattheoutput
sequencelengthLisproportionaltotheinputsequencelengthn,i.e.,L = Θ(n). Then,givenasufficientlylargen,for
√
(log-precision)sparseTransformerwithblocksizeB =Θ( L),amodelwithaconstantdepthM andaconstantnumber
√
ofattentionheadsH cangeneratethecorrectoutputforallinputssoflengthnonlyifthehiddendimensionD =Ω˜( L).
√
Proof. AssumingD =o( L),wepresentaproofbycontradiction. Whenthemodelgeneratestheoutputsequence,the
lnL
modelattendstoatmostΘ(B)tokensintheinputsequence, whicharethelastctokensofeveryblockandthelastB
tokensoftheinputsequence. TheoverallmemorycostofallthehiddenembeddingsforthesetokensisΘ(BDlogn)bits.
AccordingtoEquation(6),giventhesamehiddenembeddingofthesetokensineverylayer,themodelwillexecutethe
samecomputationandproducetheidenticalsequence. Thus,themodelcanproduceamaximumofeΘ(BDlogn) =eo(L)
√
outputsequencetypesgiventhehiddendimensionD = o( L). However,foraninputsequenceoflengthL,thereare
lnL
correspondingeΘ(L) inputandoutputsequencetypes. Accordingtothepigeonholeprinciple,thisimpliestheexistence
oftwodistinctinputsequencesthatthemodelgeneratesthesameoutputsequence. Accordingtoregularityassumption,
there is an input sequence that the model generates an incorrect output sequence. Therefore, we have determined that
√ √
D =Ω( L)=Ω˜( L).
lnL
18DoEfficientTransformersReallySaveComputation?
C.ProofsofthetheoremsinSection5
Inthissection,wewillprovethetheoremsandpropositionsoutlinedinSection5. Firstly,wewillprovideapreciseand
formaldefinitionoftheproblemsandstatementsforeachtheoremandproposition. Subsequently,wewillofferathorough
proofforeachtheoremandproposition.
ThispaperutilizestheidenticalformulationofthearithmeticevaluationtaskandCoTsolutionpresentedbyFengetal.
(2023). ThearithmeticevaluationtaskisdenotedasArithmetic(n,p)andisdefinedonthefinitefieldmodulop,withthe
inputlengthnotexceedingn. TheCoTsolutioncanbeformallydefinedasfollows: foranyarithmeticexpression,there
mustexistahandle,whichreferstoapairofadjacentnumbersconnectedbyanoperatorthatcanbeevaluated. IntheCoT
method,wesolvetheleftmostvariablefirstineachstepandconnecttheequationsofeachstepusingtheequalsign.
C.1.ProofsofProposition5.2
Inthissection,wewillgiveaproofofProposition5.2.
TheoremC.1. Foranyintegern,alog-precisionlinearTransformerwithaconstantdepthM andaconstantnumberof
headsH cangeneratethecorrectoutputforthearithmeticevaluationtaskforallexpressionsoflengthnomorethannonly
√
ifthehiddendimensionD =Ω˜(4L).
Proof. NoticethatthegeneratedCoTsequencecanbedeterminedbytheresultofthefirststep. Thedifferentpossibilitiesof
resultsforthefirststepareatleastΩ(exp(n))sinceeacha OP a ··· arelegalwherea ∈F andOP ∈{+,−,×,÷}.
1 1 2 i p i
On the other hand, the hidden state corresponding to the input sequence determines the output CoT sequence. Thus,
√
the size of hidden state should be at least Ω(n) = Ω( L). This means that the hidden dimension should be at least
(cid:113)√ √
Ω( L/logL)=Ω˜(4L),whichendsourproof.
C.2.ProofsofProposition5.4
Inthissubsection,wewillproveProposition5.4,whichisacorollaryofthetheoremfromFengetal.(2023)suchthatthe
sparsetransformercangeneratethecorrectoutputfortheDPproblem.
TheoremC.2(FromFengetal.(2023)). ForanyDPproblem,anyintegern∈N,thereexistsanautoregressiveTransformer
withconstantdepthL,hiddendimensiondandattentionheadsH (independentofn),suchthattheanswergeneratedbythe
Transformeriscorrectforallinputsequencessoflengthnomorethann. Moreover,allparametervaluesareboundedby
O(poly(n)).
TheoremC.3. Consideranym-localityDPproblemsatisfyingthesameconditionasinTheorem4.1. Givenanyinteger
n>0,letLbethelengthoftheoutputsequencewhentheinputsequencelengthisn. Then,thereexistsa(log-precision)
sparse Transformer with block size B = Θ(m), a constant depth M, a constant number of attention heads H, and a
constanthiddendimensionDthatcangeneratethecorrectoutputforallinputssoflengthn.
Proof. Undertheassumptionoflocality,wecantreatthesparsetransformerasastandardtransformertosolvethem-locality
DPproblem. Whenthestandardtransformersolvesthem-localityDPproblem,theattentionheadwillonlyattendtothe
tokenswiththedistanceatmostm,andtheattentiontoothertokensis0. Thesparsetransformeraddsmaskstoothertokens,
andtherefore,isequivalenttothestandardtransformerform-localityDPproblem. AccordingtoTheoremC.3,thesparse
transformercansolvem-localityDPproblem.
C.3.ProofsofProposition5.5
Inthissection,wewillgiveaproofofProposition5.5,whichisacorollaryofTheorem4.4.
TheoremC.4. Consideranym-localityregularDPproblemsatisfyingthesameconditionasinTheorem4.1andassume
thatm=Θ(n)wherenistheinputsequencelength. Then,alog-precisionlinearTransformerwithaconstantdepthM
andaconstantnumberofheadsH cangeneratethecorrectoutputforallinputssoflengthnonlyifthehiddendimension
√
D =Ω˜( m).
Proof. SameastheargumentsusedintheproofofTheorem4.2,wecangetthesizeofhiddenstatesshouldbeatleast
Ω(n)=Ω(m).
TheregularityassumptionimpliesthatthehiddendimensionshouldbeatleastΩ((cid:112) m/logm)=Ω˜(√
m),
19DoEfficientTransformersReallySaveComputation?
concludingourproof.
20