Asymptotics of Learning with Deep Structured (Random) Features
DominikSchröder1*,DaniilDmitriev2*,HugoCui3*,andBrunoLoureiro4
1
DepartmentofMathematics,ETHZurich,8006Zürich,Switzerland
2DepartmentofMathematics,ETHZurichandETHAICenter,8092Zürich,Switzerland
3StatisticalPhysicsOfComputationlab.,InstituteofPhysics,ÉcolePolytechniqueFédéraledeLausanne(EPFL),
1015Lausanne,Switzerland
4Départementd’Informatique,ÉcoleNormaleSupérieure(ENS)-PSL&CNRS,F-75230Pariscedex05,France
dschroeder@ethz.ch,daniil.dmitriev@ai.ethz.ch,hugo.cui@epfl.ch,bruno.loureiro@di.ens.fr
*Maincontributions
February22,2024
Abstract
Foralargeclassoffeaturemapsweprovideatightasymptoticcharacterisationofthetesterrorassociatedwith
learningthereadoutlayer,inthehigh-dimensionallimitwheretheinputdimension,hiddenlayerwidths,andnumber
oftrainingsamplesareproportionallylarge.Thischaracterizationisformulatedintermsofthepopulationcovariance
ofthefeatures.OurworkispartiallymotivatedbytheproblemoflearningwithGaussianrainbowneuralnetworks,
namelydeepnon-linearfully-connectednetworkswithrandombutstructuredweights,whoserow-wisecovariancesare
furtherallowedtodependontheweightsofpreviouslayers.Forsuchnetworkswealsoderiveaclosed-formformulafor
thefeaturecovarianceintermsoftheweightmatrices.Wefurtherfindthatinsomecasesourresultscancapturefeature
mapslearnedbydeep,finite-widthneuralnetworkstrainedundergradientdescent.
1 Introduction
Deepneuralnetworksarethebackboneofmostsuccessfulmachinelearningalgorithmsinthepastdecade.Despitetheir
ubiquity,afirmtheoreticalunderstandingoftheverybasicmechanismbehindtheircapacitytoadapttodifferenttypes
ofdataandgeneraliseacrossdifferenttasksremains,toalargeextent,elusive. Forinstance,whatistherelationship
betweentheinductivebiasintroducedbythenetworkarchitectureandtherepresentationslearnedfromthedata,and
howdoesitcorrelatewithgeneralisation?Albeitthelackofacompletepicture,insightscanbefoundinrecentempirical
andtheoreticalworks.
Onthetheoreticalside,asubstantialfractionoftheliteraturehasfocusedonthestudyofdeepnetworksatinitialisa-
tion,motivatedbythelazytrainingregimeoflarge-widthnetworkswithstandardscaling.Besidesthemathematical
convenience,thestudyofrandomnetworksatinitialisationhaveproventobeavaluabletheoreticaltestbed–allowing
inparticulartocapturesomeempiricallyobservedbehaviour,suchasthedouble-decent[1]andbenignoverfitting[2]
phenomena.Assuch,proxysfornetworksatinitialisation,suchastheRandomFeatures(RF)model[3]havethusbeen
theobjectofconsiderabletheoreticalattention,withtheirlearningbeingasymptoticallycharacterizedinthetwo-layer
case[4–10]andthedeepcase[11–14]. Withtheexceptionof[6](limitedtotwo-layernetworks)and[14](limitedto
linearnetworks),alltheanalysesfornon-lineardeepRFsassumeunstructuredrandomweights.Insharpcontrast,the
weightsoftrainedneuralnetworksarefundamentallystructured-restrictingthescopeoftheseresultstonetworksat
initialization.
Indeed,anactiveresearchdirectionconsistsofempiricallyinvestigatinghowthestatisticsoftheweightsintrained
neuralnetworksencodethelearnedinformation,andhowthistranslatestopropertiesofthepredictor,suchasinductive
biases[15,16].Ofparticularrelevancetoourworkisarecentobservationby[17]thatarandom(butstructured)network
withtheweightssampledfromanensemblewithmatchingstatisticscanretainacomparableperformancetotheoriginal
trainedneuralnetworks. Inparticular,forsometasksitwasshownthatsecondorderstatisticssuffices–defininga
Gaussianrainbownetworkensemble.
OurgoalinthismanuscriptistoprovideanexactasymptoticcharacterizationofthepropertiesofGaussianrainbow
networks,i.e.deep,non-linearnetworkswithstructuredrandomweights.Ourmaincontributionsare:
1
4202
beF
12
]LM.tats[
1v99931.2042:viXra• We derive a tight asymptotic characterization of the test error achieved by performing ridge regression with
Lipschitz-continuousfeaturemaps,inthehigh-dimensionallimitwherethedimensionofthefeaturesandthe
numberofsamplesgrowatproportionalrate.ThisclassoffeaturemapsencompassesasaparticularcaseGaussian
rainbownetworkfeatures.
• Theasymptoticcharacterizationisformulatedintermsofthepopulationcovarianceofthefeatures.ForGaussian
rainbownetworks,weexplicitaclosed-formexpressionofthiscovariance,formulatedasintheunstructuredcase
[12]asasimplelinearrecursiondependingontheweightmatricesofeachlayer.Theseformulaeextendsimilar
resultsof[12,18]forindependentandunstructuredweightstothecaseofstructured–andpotentiallycorrelated–
weights.
• We empirically find that our theoretical characterization captures well the learning curves of some networks
trainedbygradientdescentinthelazyregime.
Relatedworks
Randomfeatures— Randomfeatures(Rfs)wereintroducedin[3]asacomputationallyefficientwayofapproximating
largekernelmatrices.Intheshallowcase,theasymptoticspectraldensityoftheconjugatekernelwasderivedin[19–21].
Thetesterrorwasontheotherhandcharacterizedin[9,10]forridgeregression,andextendedtogenericconvexlosses
by[4,6,8],andin[22–24]forotherpenalties.RFshavebeenstudiedasamodelfornetworksinthelazyregime,seee.g.
[25–28];
DeepRFs– Recentworkhaveaddressedtheproblemofextendingtheseresultstodeeperarchitectures.Inthecaseof
linearnetworks,asharpcharacterizationofthetesterrorisprovidedin[11]forthecaseofunstructuredweightsand[14]
inthecaseofstructuredweights.Fornon-linearRFs,[12]providesdeterministicequivalentsforthesamplecovariance
matrices,and[12,13]provideatightcharacterizationofthetesterror.Deeprandomnetworkshavebeenalsostudiedin
thecontextofGaussianprocessesby[29,30],Bayesianneuralnetworksin[11,31–35]andinferencein[36–40]. The
recentworkof[17]providesempiricalevidencethatforagiventrainedneuralnetwork,aresamplednetworkfroman
ensemblewithmatchingstatistics(rainbownetworks)mightachievecomparablegeneralizationperformance,thereby
partlybridgingthegapbetweenrandomnetworksandtrainednetworks.
2 Setting
Considerasupervisedlearningtaskwithtrainingdata(x ,y ) .Inthismanuscript,weareinterestedinstudyingthe
i i i∈[n]
statisticsoflinearpredictorsf θ(x) = √1 pθ⊤φ(x)foraclassoffixedfeaturemapsφ : Rd → Rp andweightsθ ∈ Rp
trainedviaempiricalriskminimization:
θˆ = min (cid:88) (y −f (x ))2+λ||θ||2. (1)
λ i θ i
θ∈Rp
i∈[n]
Ofparticularinterestisthegeneralizationerror:
(cid:16) (cid:17)2
E (θˆ )=E y−f (x) (2)
gen λ θˆ
λ
wheretheexpectationisoverafreshsamplefromthesamedistributionasthetrainingdata.Moreprecisely,ourresults
willholdunderthefollowingassumptions.
Assumption2.1(Labels). Weassumethatthelabelsy aregeneratedbyanotherfeaturemapφ : Rd →Rk as
i ∗
1
y = √ θ⊤φ (x )+ε , (3)
i ∗ ∗ i i
k
whereε∈Rnisanadditivenoisevector(independentofthecovariatesx )ofzeromeanandcovarianceΣ:=Eεε⊤,
i
andθ ∈Rk isadeterministicweightvector.
∗
Assumption2.2(Data&Features). Weassumethatthecovariatesx areindependentandcomefromadistribution
i
suchthat
(i) thefeaturemapsφ,φ arecenteredinthesenseEφ(x )=0,Eφ (x )=0,
∗ i ∗ i
2(ii) thefeaturecovariances
Ω:=Eφ(x )φ(x )⊤ ∈Rp×p, Ψ:=Eφ (x )φ (x )⊤ ∈Rk×k, Φ:=Eφ(x )φ (x )⊤ ∈Rp×k, (4)
i i ∗ i ∗ i i ∗ i
haveuniformlyboundedspectralnorm.
(iii) scalarLipschitzfunctionsofthefeaturematrices
X :=(φ(x ),...,φ(x ))∈Rp×n, Z :=(φ (x ),...,φ (x ))∈Rk×n (5)
1 n ∗ 1 ∗ n
areuniformlysub-Gaussian.
Assumption2.3(Proportionalregime). Thenumberofsamplesnandthefeaturedimensionsp,k arealllargeand
comparable,seeTheorem3.1later.
Remark2.4. WeformulatedAssumption2.2asajointassumptiononthecovariatesdistributionandthefeaturemaps.A
conceptuallysimplerbutlessgeneralconditionwouldbetoassumethat
(ii’) thecovariatesx areGaussianwithboundedcovarianceΩ :=Ex x⊤
i 0 i i
(iii’) thefeaturemapsφ,φ areLipschitz-continuous
∗
insteadofAssumptions(ii)and(iii).
Thesettingabovedefinesaquitebroadclassofproblems,andtheresultsthatfollowinSection3willholdunder
thesegenericassumptions.Themainclassoffeaturemapsweareinterestedinaredeepstructuredfeaturemodels.
Definition2.5(Deepstructuredfeaturemodel). ForanyL∈Nanddimensionsd,p ,...,p =p,letφ ,...,φ : R→
1 L 1 L
RbeLipschitz-continuousactivationfunctions|φ l(a)−φ l(b)|≲|a−b|appliedentrywise,andletW
1
∈Rp1×d,W
2
∈
Rp2×p1,...bedeterministicweightmatriceswithuniformlyboundedspectralnorms,∥W l∥≲1.Wethencall
φ(x):=φ (W φ (···W φ (W x))). (6)
L L L−1 2 1 1
adeepstructuredfeaturemodel.
Notethat eq.(6) definesa Lipschitz-continuousmap1 φ: Rd → Rp,φ : Rd → Rk andthereforeif bothφ,φ
∗ ∗
aredeepstructuredfeaturemodels(withdistinctparametersingeneral),thenAssumption2.2issatisfiedwhenever
thefeaturemapsφ,φ arecentered2withrespecttoGaussiancovariatesx .Ashintedintheintroductionwewillbe
∗ i
particularlyinterestedinonesub-classofDefinition2.5knownasGaussianrainbownetworks.
Definition2.6(Gaussianrainbowensemble). Borrowingtheterminologyof[17],wedefineafully-connected,L-layer
GaussianrainbownetworkasarandomvariantofDefinition2.5whereforeachℓthehidden-layerweightsW =Z C1/2
ℓ ℓ ℓ
arerandommatriceswithZ
ℓ
∈Rpℓ+1×pℓ havingzeromeanandi.i.d.variance1/pℓGaussianentriesandC
ℓ
∈Rpℓ×pℓ
beinguniformlyboundedcovariancematrices,whichweallowtodependonpreviouslayerweightsZ ,...,Z .
1 l−1
NotethatGaussianrainbownetworksabovecanbeseenasageneralizationofthedeeprandomfeaturesmodel
studiedin[12,13,41],withthecrucialdifferencethattheweightsarestructured.
Notations
For square matrices A ∈ Rn×n we denote the averaged trace by ⟨A⟩ := n−1TrA, and for rectangular matrices
A ∈ Rn×m we denote the Frobenius norm by ∥A∥2 := (cid:80) |a |2, and the operator norm by ∥A∥. For families of
F ij ij
non-negativerandomvariablesX(n),Y(n)wesaythatX isstochasticallydominated byY,andwriteX ≺Y,ifforall
ϵ,DitholdsthatP(X(n)≥nϵY(n))≤n−D fornsufficientlylarge.
1∥φ(Wx)−φ(Wx′)∥2=(cid:80) |φ(w⊤x)−φ(w⊤x′)|2≲(cid:80) |w⊤(x−x′)|2=∥W(x−x′)∥2≲∥x−x′∥2
2Itissufficentthate.g.ϕ lisodi d,andi xiiscenterei d. i i
33 Test error of Lipschitz feature models
UnderAssumptions2.1and2.2thegeneralizationerrorfromEq.(2)isgivenby
θ⊤Ψθ θ⊤ZX⊤GΩGXZ⊤θ n(cid:28) X⊤GΩGXΣ(cid:29) θ⊤Φ⊤GXZ⊤θ
E (λ)= ∗ ∗ + ∗ ∗ + −2 ∗ ∗, (7)
gen k kp2 p p kp
intermsoftheresolvent G=G(λ):=(XX⊤/p+λ)−1.
OurmainresultisarigorousasymptoticexpressionforEq.(7).Tothatenddefine,m(λ)tobetheuniquesolutionto
theequation
1 (cid:28) (cid:16) n (cid:17)−1(cid:29)
=λ+ Ω I+ m(λ)Ω , (8)
m(λ) p
anddefine
(cid:16) n (cid:17)−1
M(λ)= λ+ λm(λ)Ω (9)
p
whichisthedeterministicequivalent oftheresolvent,M(λ)≈G(λ),seeTheorem3.3later.Thefactthateq.(8)admitsa
uniquesolutionm(λ)>0whichiscontinuousinλfollowsdirectlyfromcontinuityandmonotonicity.Moreover,from
(cid:28) (cid:16) n (cid:17)−1(cid:29) (cid:26) rankΩ 1 (cid:27)
0≤ Ω I+ mΩ ≤min ⟨Ω⟩,
p n m
weobtainthebounds
(cid:40) (cid:41)
1 1− rankΩ 1
max , n ≤m(λ)≤ . (10)
λ+⟨Ω⟩ λ λ
Wealsoremarkthatm(λ)dependsonΩonlythroughitseigenvaluesω ,...,ω ,whileM(λ)dependsontheeigenvectors.
1 p
TheasymptoticexpressionEq.(12)forthegeneralizationerrorderivedbelowdependsontheeigenvaluesofΩ,the
overlapoftheeigenvectorsofΩwiththeeigenvectorsofΦ,andtheoverlapoftheeigenvectorsofΨ,Φwithθ .
∗
Theorem3.1. UnderAssumption2.1,Assumption2.2andAssumption2.3forfixedλ>0wehavetheasymptotics
(cid:18) (cid:19)
1
E (λ)=Ermt(λ)+O √ , (11)
gen gen n
intheproportionaln∼k ∼pregime,where
1 Ψ− nmλΦ(M +λM2)Φ⊤ (λm)2n⟨MΩMΩ⟩
Ermt(λ):= θ⊤ p θ +⟨Σ⟩ p . (12)
gen k ∗ 1− n(λm)2⟨ΩMΩM⟩ ∗ 1− n(λm)2⟨ΩMΩM⟩
p p
Inthegeneralcaseofcomparableparameterswehavetheasymptoticswithaworseerrorof
1 (cid:16) max{n,p,k}(cid:17)
1+ .
(cid:112)
min{n,p,k} min{n,p,k}
Remark3.2(Relationtopreviousresults). Wefocusonthemisspecifiedcaseasthispresentsthemainnoveltyofthepresent
work.InthewellspecifiedcaseZ =X ourmodelessentiallyreducestolinearregressionwithdatadistributionx=φ(x).
Therehasbeenextensiveresearchonthegeneralizationerroroflinearregression,seee.g.in[42–45]andthereferencestherein.
(a) WeconfirmConjecture1of[23]underassumption2.2.TheexpressionfortheerrorterminTheorem3.1matchesthe
expressionobtainedin[23]foraGaussiancovariatesteacher-studentmodel.
(b) Independentlyandconcurrentlytothecurrentwork[46](partiallyconfirmingaconjecturemadein[47])obtained
similarresultsunderdifferentassumptions.Mostimportantly[46]considersone-layerunstructuredrandomfeature
modelsandcomputestheempiricalgeneralizationerrorforadeterministicdataset,whileweconsidergeneralLipschitz
featuresofrandomdata,andcomputethegeneralizationerror.
(c) In the unstructured random feature model [10, 48] obtained an expression for the generalization error under the
assumptionthatthetargetmodelislinearorrotationallyinvariant.
ThenoveltyofTheorem3.1comparedtomanyofthepreviousworksis,besidesthelevelofgenerality,two-fold:
4(i) WeobtainadeterministicequivalentforthegeneralizationerrorinvolvingthepopulationcovarianceΦandthe
samplecovarianceXZ⊤inthegeneralmisspecifiedsetting.
(ii) Ourdeterministicequivalentisanisotropic,allowingtoevaluateEq.(7)forfixed targetsθ andstructurednoise
∗
covarianceΣ̸=I.
Someofthepreviousrigorousresultsonthegeneralizationerrorofridgeregressionhavebeenlimitedtothewell-specified
case,X =Z,sinceinthisparticularcasethesecondtermofEq.(7)canbesimplifiedto
XX⊤ XX⊤
GΩG =(1−λG)Ω(1−λG). (13)
p p
When computing deterministic equivalents for terms as GΩG, some previous results have relied on the “trick” of
differentiatingageneralizedresolventmatrixGr (λ,λ′):=(XX⊤/p+λ′Ω+λ)−1withrespecttoλ′.Ourapproachis
morerobustandnotlimitedtoexpressionswhichcanbewrittenascertainderivatives.
ToillustrateItem(ii),theconventionalapproachintheliteraturetoapproximatinge.g.thethirdtermontheright
handsideofEq.(7)inthecaseΣ=I wouldbetousethecyclicityofthetracetoobtain
1 1 XX⊤
TrX⊤GΩGX = TrG GΩ
p2 p p (14)
=⟨GΩ⟩−λ⟨G2Ω⟩.
ThenuponusingEq.(8)and⟨GΩ⟩≈⟨MΩ⟩,thefirsttermofEq.(14)canbeapproximatedby1/(λm(λ))−1,whilefor
thesecondtermitcanbearguedthatthisapproximationalsoholdsinderivativesensetoobtain
d d 1 λm′(λ)+m(λ)
⟨G2Ω⟩=− ⟨GΩ⟩≈− =
dλ dλλm(λ) (λm(λ))2
BydifferentiatingEq.(8),solvingform′andsimplifying,itcanbecheckedthatthisresultagreeswiththesecondterm
ofEq.(12)inthespecialcaseΣ=I. However,itisclearthatanyapproachwhichonlyreliesonscalar deterministic
equivalentsisinherentlylimitedinthetypeofexpressionswhichcanbeevaluated. Instead,ourapproachinvolving
anisotropicdeterministicequivalentshasnoinherentlimitationonthestructureoftheexpressionstobeevaluated.
AnalternativetoevaluatingrationalexpressionsofX,Z,commonlyusedinsimilarcontexts,isthetechniqueof
linearpencils[46,48]. TheideahereistorepresentrationalfunctionsofX,Z asblocksofinversesoflargerrandom
matriceswhichdependlinearlyX,Z.Thedownsideoflinearpencilsisthatevenforsimplerationalexpressionsthe
linearizationsbecomecomplicated,sometimesevenrequiringtheuseofcomputeralgebrasoftwarefortheanalysis3
Incomparisonwebelievethatourapproachismoredirectandflexible.
3.1 ProofofTheorem3.1
WepresenttheproofofTheorem3.1indetailsinAppendixA.ThemainstepsandingredientsfortheproofofTheorem3.1
consistofthefollowing:
Concentration:
AsafirststepweestablishconcentrationestimatesforLipschitzfunctionsofX,Z anditscolumns.Akeyaspectis
theconcentrationofquadraticformsinthecolumnsx :=φ(x )ofX:
i i
|x⊤Ax −Ex⊤Ax |=|x⊤Ax −TrΩA|≺∥A∥
i i i i i i F
which follows from the Hanson-Wright inequality [49]. The concentration step is very similar to analagous
considerationsinpreviousworks[50,51]butwepresentitforcompleteness.Themainpropertyusedextensively
inthesubsequentanalysisisthattracesofresolventswithdeterministicobservablesconcentrateas
⟨|A|2⟩1/2
|⟨A[G(λ)−EG(λ)]⟩|≺ . (15)
nλ3/2
AnisotropicMarchenko-PasturLaw:
AsasecondstepweproveananisotropicMarchenko-PasturlawfortheresolventG,oftheform:
3Forinstance[48]usedblockmatriceswithupto16×16blocksinordertoevaluatetheasymptotictesterror.
5Theorem3.3. ForarbitrarydeterministicmatricesAwehavethehigh-probabilitybound
⟨|A|2⟩
|⟨(G(λ)−M(λ)A⟩|≺ , (16)
nλ3
intheproportionaln∼pregime4.
Remark 3.4. Tracial Marchenko-Pastur laws (case A = I above) have a long history, going back to [52] in the
√
isotropiccaseΩ = I,[53]inthegeneralcasewithseparablecovariancex = Ωz and[54]underquadraticform
concentrationassumption.AnisotropicMarchenko-Pasturlawsundervariousconditionsandwithvaryingprecision
havebeenprovene.g.in[47,50,55,56].
FortheproofofTheorem3.3theresolventGq :=(X⊤X/p+λ)−1 ∈Rn×noftheGrammatrix X⊤X playsakey
role.Themaintoolusedinthissteparethecommonlyusedleave-one-outidentities,e.g.
Gx =λGq G x , G :=(cid:16)(cid:88)x jx⊤ j +λ(cid:17)−1 (17)
i ii −i i −i p
j̸=i
whichallowtodecoupletherandomnessduethei-thcolumnfromtheremainingrandomness.Suchidentitiesare
usedrepeatedlytoderivetheapproximation
EG≈(cid:16)n λ⟨EGq ⟩Ω+λ(cid:17)−1 (18)
p
inFrobeniusnorm, which, togetherwiththerelation1−λ⟨Gq ⟩ = p(cid:0) 1−λ⟨G⟩(cid:1)betweenthetracesofGand
n
Gq,yieldsaself-consistentequationfor⟨Gq ⟩. Thisself-consistentequationisanapproximateversionofEq.(8),
justifyingthedefinitionofm.Thestabilityoftheself-consistentequationthenimpliestheaveragedasymptotic
equivalent
|m−⟨EGq ⟩|≲ 1 . (19)
nλ2
andthereforebyEq.(18)finally
1
∥M −EG∥ ≲ , (20)
F n1/2λ3
whichtogetherwithEq.(15)impliesTheorem3.3.
Compared to most previous anisotropic deterministic equivalents as in [56] we measure the error of the ap-
proximationEq.(16)withrespecttotheFrobeniusoftheobservableA. Asinthecaseofunifiedlocallawsfor
Wignermatrices[57]thisidearenderstheseparatehandlingofquadraticformboundunnecessary,considerably
streamliningtheproof.ToillustratethedifferencenotethatspecializingAtoberank-oneA=xy⊤in
(cid:40)
∥A∥
|y⊤(G−M)x|=|Tr(G−M)A|≺
⟨|A|2⟩1/2
√
resultsinatrivialestimate∥x∥∥y∥inthecaseofthespectralnorm,andintheoptimalestimate∥x∥∥y∥/ pinthe
caseoftheFrobeniusnorm.
AnisotropicMulti-ResolventEquivalents
ThemainnoveltyofthecurrentworkliesinPropositionA.4whichasymptoticallyevaluatestheexpressions
ontheright-hand-sideofEq.(7). Akeypropertyofthedeterministicequivalentsisthattheapproximationis
not invariant under multiplication. E.g. for the last term in Eq. (7) we have the approximations G ≈ M and
1XZ⊤ = 1 (cid:80) x z⊤ ≈Φ,whilefortheproductthecorrectdeterministicequivalentis
n n i i
XZ⊤
G ≈λmMΦ, (21)
n
i.e.theisanadditionalfactorofmλ.Inthiscasetheadditionalfactorcanbeobtainedfromadirectapplicationofthe
leave-one-outidentityEq.(17)totheproductGXZ⊤,butthederivationofthemulti-resolventequivalentsrequires
n
4SeetheprecisestatementinthecomparableregimeinEq.(51)later
6moreinvolvedarguments.Whenexpandingthemulti-resolventexpression⟨GAGB⟩weobtainanapproximative
self-consistentequationoftheform
n
⟨GAGB⟩≈⟨MAMB⟩+ (mλ)2⟨MBMΩ⟩⟨GAGΩ⟩.
p
Usingastabilityanalysisthisyieldsadeterministicequivalentforthespecialform⟨GAGΩ⟩whichthencanbe
usedforthegeneralcase.ThesecondtermofEq.(7)requiresthemostcarefulyanalysisduetotheinterplayof
themulti-resolventexpressionandthedependencyamongZ,X.
4 Population covariance for rainbow networks
Theorem3.1characterizesthetesterrorforlearningusingLipschitzfeaturemapsasafunctionofthethreefeatures
population(cross-)covariancesΩ,Φ,Ψ.Fortheparticularcasewhereboththetargetandlearnerfeaturemapsaredrawn
fromtheGaussianrainbowensemblefromDefinition2.6,thesepopulationcovariancescanbeexpressedinclosed-form
intermsofcombinationsofproductsoftheweightsmatrices.Considertworainbownetworks
φ(x)=φ (W φ (...φ (W x)))
L L L−1 1 1 (22)
φ ∗(x)=ψ Lr(V Lrψ Lr −1(...ψ 1(V 1x)))
withdepthsL,Lr.Theapproachweintroducehereisintheorycapableofobtaininglinearorpolynomialapproximations
toΩ,Φ,Ψunderverygeneralassumptions.However,fordefinitenesswefocusonaclassofcorrelatedrainbownetworks
inwhich,forallk ̸= j,thek-throwofW isindependentfromthej-throwofW ,V asthisallowsforparticularly
ℓ ℓ ℓ
simpleexpressionsforthelinearizedcovariances5.Notethatweexplicitlyallowforweightstobecorrelatedacrosslayers.
Assumption4.1(Correlatedrainbownetworks). BysymmetryweassumewithoutlossofgeneralityL≤Lr.Furthermore,
weassumethat
(a) forℓ≤L≤Lralltheinternalwidthsp ofW ,V agree,
ℓ ℓ ℓ
(b) forallℓ≤Lr,thedimensionsscaleproportionally,i.e.n∼d∼p ,
ℓ
(c) forℓ≤L≤Lrtherowsw ,v ofW ,V aremean-zeroandi.i.d.with
ℓ ℓ ℓ ℓ
C :=p Ew w⊤, Cr :=p Ev v⊤, Cq :=p Ew v⊤,
ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ
(d) fortwo(possiblyidentical)rowsu,z,andforanymatrixA,quadraticformsadmitconcentration,w.h.p.6
u⊤Az−Tr(AEzu⊤)≲n−1/2, (23)
(e) forallℓ≤Lr,operatornormsof(cross-)covariancematricesadmituniformbounds
∥C ∥+∥Cr ∥+∥Cq ∥≲1. (24)
ℓ ℓ ℓ
UnderAssumption4.1thelinearizedpopulationcovariancescanbedefinedrecursivelyasfollows:
Definition4.2(Linearizedpopulationcovariances). DefinethesequenceofmatricesΩlin,Φlin,Ψlinbytherecursions
ℓ ℓ ℓ
Ωlin =(κ1)2W Ωlin W⊤+(κ∗)2I
ℓ ℓ ℓ ℓ−1 ℓ ℓ
Ψlin =(κ˜1)2V Ψlin V⊤+(κr∗)2I (25)
ℓ ℓ ℓ ℓ−1 ℓ ℓ
Φlin =κ1κ˜1W Φlin V⊤+(κq∗)2I,
ℓ ℓ ℓ ℓ ℓ−1 ℓ ℓ
withΩlin =Ψlin =Φlin =Ω theinputcovariance.Thecoefficients{κ1,κ˜1,κ∗,κ˜∗}aredefinedbytherecursion
0 0 0 0 ℓ ℓ ℓ ℓ
κ1 :=Eφ′(N ), κr1 :=Eψ′(Nr ) (26)
ℓ ℓ ℓ ℓ ℓ ℓ
5TheidentitymatricesinEq.(25)areadirectconsequenceofthisassumption.Incaseofweightmatriceswithvaryingrow-normsorcovariances
acrossrowstheresultingexpressionwouldbeconsiderablymorecomplicated.
6Thisconcentrationholdsinparticularwhenrowsu,zareLipschitzconcentratedwithconstantO(n−1/2),seeTheoremA.3.
7and
(cid:113)
κ∗ = E[φ (N )2]−r (κ1)2
ℓ ℓ ℓ ℓ ℓ
(cid:113)
κ˜∗ = E[ψ (Nr )2]−r˜(κ˜1)2 (27)
ℓ ℓ ℓ ℓ ℓ
(cid:113)
κq∗ = E[φ (N )ψ (Nr )]−rˇκ1κr1,
ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ
whereN ,Nr arejointlymean-zeroGaussianwithEN2 =r ,ENr 2 =rr,EN Nr =rq,with
ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ
r =Tr[C Ωlin ], rr =Tr[Cr Ψlin ], rq =Tr[Cq⊤Φlin ].
ℓ ℓ ℓ−1 ℓ ℓ ℓ−1 ℓ ℓ ℓ−1
Finally,forL˜ ≥ℓ≥L+1,define
Φlin =κr1Φlin WĂ⊤, (28)
ℓ ℓ ℓ−1 ℓ
withstillκr1,κr∗justasbefore,andΨlinwiththesamerecursion(25).
ℓ ℓ ℓ
Conjecture4.3. ThepopulationscovariancesΩ,Φ,ΨinvolvedinTheorem3.1canbeasymptoticallyapproximatedwith
thelastiteratesofthelinearrecursionsofDefinition4.2,i.e.
∥Ω−Ωlin∥ +∥Ψ−Ψlin∥ +∥Φ−Φlin∥ ≲1 (29)
L F L˜ F L˜ F
Note that the linearization from Definition 4.2 also provides good approximation to the population covariances
Ω ,Φ ,Ψ ofthepost-activationsatintermediatelayersℓ.Themethodweusetorigorouslyderivethelinearizations
ℓ ℓ ℓ
isintheoryapplicabletoanydepths,howevertheestimatesquicklybecometedious. Tokeepthepresentworkata
manageablelengthweprovidearigorousproofofconceptonlyforthesimplestmulti-layercase.
r
Theorem4.4. UnderAssumption4.1withL=1,L=2wehave
∥Ω −Ωlin∥ +∥Ψ −Ψlin∥ +∥Φ −Φlin∥ ≲1
1 1 F 1 1 F 1 1 F
∥Ψ −Ψlin∥ +∥Φ −Φlin∥ ≲1
2 2 F 2 2 F
withhighprobability.
Remark4.5(Comparison). Theapproachwetakehereissomewhatdifferentfrompreviousworks[12,41,58]on(multi-layer)
randomfeaturemodels.Inthesepreviousresults,thedeterministicequivalentfortheresolventwasobtainedusingprimarily
therandomnessoftheweights,resultinginrelativelystringentassumptions(Gaussianityandindependencebetweenlayers).
Thislayer-by-layerrecursiveapproachresultedinadeterministicequivalentfortheresolventwhichisconsistentwitha
samplecovariancematrixwithlinearizedpopulationcovariance.Herewetakethedirectapproachofconsideringfeature
models with arbitrary structured features, and then linearize the population covariances in a separate step for random
features.
4.1 ProofofTheorem4.4
WesketchthemaintoolsusedintheargumentandwereferthereadertoPropositionB.11andTheoremB.12forthe
formalproof. Intheproof,wecruciallyrelyonthetheoryofWienerchaosexpansionandStein’smethod(see[59]).
GaussianWienerchaosisageneralizationofHermitepolynomialexpansions,whichpreviouslyhavebeenusedfor
approximatelinearization[12,41]insimilarcontexts.ThebasicideaistodecomposerandomvariablesF =F(x)which
arefunctionsoftheGaussianrandomvectorx,intopairwiseuncorrelatedcomponents
(cid:88) (cid:16)EDpF(cid:17)
F =EF + I , (30)
p p!
p≥1
whereI isasocalledmultipleintegral(generalizingHermitepolynomials)andDpisthep-thMalliavinderivative.By
p
applyingthistotheone-layerquantitiesφ (w⊤x),ψ (u⊤x)weobtain,forinstance
1 1
Eφ (w⊤x)ψ (v⊤x)
1 1
=(cid:88) 1 Eφ(p)(w⊤x)Eψ(p)(u⊤x)⟨w,v⟩p, (31)
p! 1 1
p≥1
80.175 =0.0
=0.2
0.150
=0.5
=0.8
0.125
0.100
0.075
0.050
0.025
0.5 1.0 1.5 2.0
Figure 1: Test error for a target θ⊤tanh(W x), when learning with a four-layer Gaussian rainbow network with
∗ ∗
featuremapφ(x) = tanh(W tanh(W tanh(W x))). Allwidthweretakenequaltotheinputdimensiond,andthe
3 2 1
regularization employed is λ = 10−4. The student weights are correlated across layers, with W = W , and the
1 2
covarianceC 3ofW 3dependingonW 1asC
3
=(W 1W 1⊤+1/2I)−1.Target/studentcorrelationsarealsopresent,with
Cˇ
1
= 1/2I. ThecovariancesC 1,C 2,C˜
1
werefinallytakentohaveaspectrumwithpower-lawdecay, parametrized
byγ. AlldetailsareprovidedinApp. C.Solidlines: theoreticalpredictionofTheorem3.1, inconjunctionwiththe
closed-formexpressionforthefeaturespopulationcovarianceofDefinition4.2. Crosses: numericalsimulationsin
d=1000.Allexperimentalpointswereaveragedover20instances,witherrorbarsrepresentingonestandarddeviation.
Differentcolorsrepresentdifferentvaluesfortheparameterγ,withsmall(large)valuesindicatingslow(fast)covariance
eigenvaluedecay.
whichforindependentw,vwecantruncateafterp=1,givingrisetothelinearization.
Forthemulti-layercasewecombinethechaosexpansionwithStein’smethodinordertoprovequantitativecentral
limittheoremsofthetype
d (F,N)≲E|EF2−⟨DF,−DL−1F⟩| (32)
W
fortheWassersteindistanced ,where
W
F :=w⊤φ (Wx), N ∼N(0,EF2), (33)
1
andL−1isthepseudo-inverseofthegeneratoroftheOrnstein–Uhlenbecksemigroup.
4.2 DiscussionofTheorem4.4
Thepopulationcovariancesthusadmitsimpleapproximateclosed-formexpressionsaslinearcombinationsofproducts
ofrelevantweightmatrices.Theseexpressionsgeneralizesimilarlinearizationsintroducedin[12,13,18,41,58]forthe
caseofweightswhicharebothunstructuredandindependent,anditerativelybuilduponearlierresultsforthetwo-layer
casedevelopedin[4,6,7,9]. Infact,theexpressionsleveragedintheseworkscanberecoveredasaspecialcasefor
C =C˜ =I(isotropicweights)andCˇ =0(independence).Importantly,notethatpossiblecorrelationbetweenweights
ℓ ℓ ℓ
acrossdifferentlayersdonotenterinthereportedexpressions.Inpractice,wehaveobservedinallprobedsettingsthe
testerrorpredictedbyTheorem3.1,inconjunctionwiththelinearizationformulaeforthefeaturescovariance,tomatch
wellnumericalexperiments.
Figure 1illustratesasettingwheremanytypesofweightscorrelationsarepresent.Itrepresentsthelearningcurves
ofafour-layerGaussianrainbownetworkwithfeaturemaptanh(W tanh(W tanh(W x))),learningfromatwo-layer
3 2 1
targetθ ∗⊤tanh(WĂ x). Toillustrateourresult,weconsiderbothtarget/studentcorrelationsCq
1
=1/2I,andinter-layer
correlationsW = W . Wefurthermoretookthecovarianceofthethirdlayertodependontheweightsofthefirst
1 2
layer,C
3
=(W 1W 1⊤+1/2I)−1.Inordertohavestructuredweights,thecovariancesCr 1,C 1,C 2werechosentohave
apower-lawspectrum.AlldetailsontheexperimentaldetailsandparametersareexhaustivelyprovidedinAppendix
C.Notethatdespitethepresenceofsuchnon-trivialcorrelations,thetheoreticalpredictionofTheorem3.1usingthe
linearizedclosed-formformulaeofDef.4.2forthefeaturescovariances(solidlines)capturescompellinglythetesterror
evaluatedinnumericalexperiment(crosses).
Finally, wenotethatakinto[12], asaconsequenceofthesimplelinearrecursions, itfollowsthattheGaussian
rainbownetworkfeaturemapφsharesthesamesecondmoments,andthusbyTheorem3.1thesametesterror,asan
9
g0.400
0.375
0.350
0.325
0.300
0.275
0.250 at initialization
trained
0.225 ridge regression
0.0 0.5 1.0 1.5 2.0
Figure2: Crosses: Testerrorwhentrainingthereadoutlayeronlyofatanh-activatedthree-layerneuralnetworkat
initialization(green)andaftertraining(blue),usingthePytorchimplementationofthefull-batchAdam[61]optimizer,
over3000epochswithleraningrate10−4andn =1400samples,indimensiond=1000.(red):ridgeregression.The
0
dataissampledfromanisotropicGaussiandistribution.Inalltrainingprocedures,anℓ optimizationwasemployed,and
2
thestrengththereofoptimizedoverusingcross-validation.SolidlinesrepresentthetheoreticalpredictionofTheorem
3.1,usingthelinearizedformulaeofDefinition4.2forthefeaturespopulationcovariancesΩ,Ψ,Φ.Crossesrepresent
numericalexperiments.Eachsimulationpointisaveragedover10instances,witherrorbarsrepresentingonestandard
deviation.
equivalentlinearstochasticnetworkφlin =ψ ◦···◦ψ ,with
L 1
ψ (x)=κ1W x+κ∗ξ (34)
ℓ ℓ ℓ ℓ ℓ
whereξ ∼N(0,I)astochasticnoise.Thisequivalentviewpointhasprovenfruitfulinyieldinginsightsontheimplicit
ℓ
biasofRFs[12,60]andonthefundamentallimitationsofdeepnetworksintheproportionalregime[18].IntheSection5
we push this perspective further, by heuristically finding that the linearization and Theorem 3.1 can also describe
deterministicnetworkstrainedwithgradientdescentinthelazyregime.
5 Linearizing trained neural networks
The previous discussion addressed feature maps associated to random Gaussian networks. However, note that the
linearizationitselfonlyinvolvesproductsoftheweightsmatrices,andcoefficientdependingonweightcovariances
whichcanstraightforwardlybeestimatedtherefrom.Thelinearization4.2canthusbereadilyheuristicallyevaluated
forfeaturemapsassociatedtodeterministictrained finite-widthneuralnetworks. Aswediscusslaterinthissection,
theresultingpredictionforthetesterrorcaptureswellthelearningcurveswhenre-trainingthereadoutweightsofthe
networkinanumbersettings. Naturally,suchsettingscorrespondtolazylearningregimes[60],wherethenetwork
featuremapiseffectivelylinear,thuslittleexpressive.However,thesetrainedfeaturemap,albeitlinear,canstillencode
someinductivebias,asshownby[62]foronegradientstepintheshallowcase.Inthissection,webrieflyexplorethese
questionsforfullytraineddeepnetworks,throughthelensofourtheoreticalresults.
Fig.2contraststhetesterrorachievedbylinearregression(red),andregressiononthefeaturemapassociatedtoa
three-layerstudentatinitialization(green)andafter3000epochsofend-to-endtrainingusingfull-batchAdam[61]at
learningrate10−4andweightdecay10−3overn =1400trainingsamples(blue).Forallcurves,thereadoutweights
0
weretrainedusingridgeregression,withregularizationstrengthoptimizedoverusingcross-validation.Solidcurves
indicate the theoretical predictions of Thm. 3.1 leveraging the closed-form linearized formulae 4.2 for the features
covariance.Interestingly,evenforthedeterministictrainednetworkfeatures,theformulacapturesthelearningcurve
well.Thisobservationtemptinglysuggeststointerpretthefeaturemapφ(x)asthestochasticlinearmap
φg(x)=W x+C1/2ξ (35)
eff. eff.
whereW ∈ Rp×d isproportionaltotheproductofalltheweightmatricesandξ ∼ N(0,I)isastochasticnoise
eff.
10
gcoloredbythecovariance
L−1(cid:32) L (cid:33)2
C ≡(cid:88) κ∗ (cid:89) κ1 Wˆ ...Wˆ Wˆ⊤ ...Wˆ⊤+(κ∗)2I. (36)
eff. ℓ s L ℓ+1 ℓ+1 L L
ℓ=1 s=ℓ+1
We denoted {Wˆ } the trained weights. Note that the effective linear network (35) simply corresponds to the
ℓ 1≤ℓ≤L
compositionoftheequivalentstochasticlinearlayers(34).Averysimilarexpressionforthecovarianceoftheeffective
structurednoise(36)appearedin[12]fortherandomcasewithunstructuredanduntrainedrandomweights.Theeffective
linearmodel(35)affordsaconciseviewpointonadeepfinite-widthnon-linearnetworktrainedinthelazyregime.On
anintuitivelevel,duringtraining,thenetworkeffectivelytunesthetwomatricesW ,C whichparametrizethe
eff. eff.
effectivemodel(35).TheeffectiveweightsW controlsthe(linear)representationofthedata,whilethecolorednoise
eff.
C1/2ξin(35)canbelooselyinterpretedasinducinganeffectiveregularization.
eff.
Infact,despitethefactthatallthreefeaturemapsrepresentedinFig.2areeffectivelyjustlinearfeaturemaps,they
canstillencodeverydifferentbiases,yieldingdifferentphenomenology.Inparticular,remarkthatthetrainedfeature
map(blue)isoutperformedbymereridgeregression(red)atlargesamplecomplexities,despitetheformerhavingbeen
priorlytrainedonn additionalsamples–suggestingthetrainedweightsW ,C learnedsomeformofinductive
0 eff. eff.
biaswhichishelpfulatsmallandmoderatesamplecomplexities,butultimatelyharmfulforlargesamplecomplexities.
Acknowledgements
WethankFlorentinGuth,NathanaëlCuvelle-Magar,StéphaneMallat,LenkaZdeborováforfruitfuldiscussionsduringthe
courseofthisproject.WethanktheInstitutd’ÉtudesScientifiquesdeCargèseforthehospitality,wherediscussionsfor
thisprojectwereheldthroughoutthe2023“Statisticalphysics&machinelearningbacktogetheragain” programorganised
byVittorioErba,DamienBarbier,FlorentKrzakala,BLandLenkaZdeborová.BLacknowledgessupportfromtheChoose
France-CNRSAIRisingTalentsprogram.DDissupportedbyETHAICenterdoctoralfellowshipandETHFoundationsof
DataScienceinitiative.DSissupportedbySNSFAmbizioneGrantPZ00P2_209089.HCacknowledgessupportfrom
theERCundertheEuropeanUnion’sHorizon2020ResearchandInnovationProgramGrantAgreement714608-SMiLe.
References
[1] MikhailBelkin,SiyuanMa,SoumikMandal,MikhailBelkin,andDanielHsu. Reconcilingmodernmachine-learning
practiceandtheclassicalbias–variancetrade-off. Proc.Natl.Acad.Sci.U.S.A.,116(32):15849–15854,2019.
[2] PeterL.Bartlett,PhilipM.Long,GáborLugosi,andAlexanderTsigler. Benignoverfittinginlinearregression. Proc.
Natl.Acad.Sci.USA,117(48):30063–30070,2020.
[3] AliRahimiandBenjaminRecht. Randomfeaturesforlarge-scalekernelmachines. InJ.Platt,D.Koller,Y.Singer,
andS.Roweis,editors,AdvancesinNeuralInformationProcessingSystems,volume20.CurranAssociates,Inc.,2007.
[4] SebastianGoldt,BrunoLoureiro,GalenReeves,FlorentKrzakala,MarcMezard,andLenkaZdeborová.TheGaussian
equivalenceofgenerativemodelsforlearningwithshallowneuralnetworks. InProceedingsofthe2ndMathematical
andScientificMachineLearningConference,ProceedingsofMachineLearningResearch.145,pages426–471,2021.
[5] SebastianGoldt,MarcMézard,FlorentKrzakala,andLenkaZdeborová. ModelingtheInfluenceofDataStructure
onLearninginNeuralNetworks:TheHiddenManifoldModel. Phys.Rev.X,10(4),2020.
[6] Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborová. Generalisation error
in learning with random features and the hidden manifold model. In Hal Daumé III and Aarti Singh, editors,
Proceedingsofthe37thInternationalConferenceonMachineLearning,volume119ofProceedingsofMachineLearning
Research,pages3452–3462.PMLR,13–18Jul2020.
[7] HongHuandYueM.Lu. UniversalityLawsforHigh-DimensionalLearningwithRandomFeatures. IEEETrans.Inf.
Theory,2022.
[8] OussamaDhifallahandYueM.Lu.Apreciseperformanceanalysisoflearningwithrandomfeatures.arXiv:2008.11904,
2020.
[9] SongMeiandAndreaMontanari. Thegeneralizationerrorofrandomfeaturesregression:preciseasymptoticsand
thedoubledescentcurve. Comm.PureAppl.Math.,75(4):667–766,2022.
11[10] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random feature and kernel
methods:hypercontractivityandkernelmatrixconcentration. Appl.Comput.Harmon.Anal.,59:3–84,2022.
[11] JacobA.Zavatone-Veth,CengizPehlevan,andWilliamL.Tong. Contrastingrandomandlearnedfeaturesindeep
Bayesianlinearregression. Phys.Rev.E,105(6),2022.
[12] DominikSchröder,HugoCui,DaniilDmitriev,andBrunoLoureiro. Deterministicequivalentanderroruniversality
ofdeeprandomfeatureslearning. InAndreasKrause,EmmaBrunskill,KyunghyunCho,BarbaraEngelhardt,Sivan
Sabato,andJonathanScarlett,editors,Proceedingsofthe40thInternationalConferenceonMachineLearning,volume
202ofProceedingsofMachineLearningResearch,pages30285–30320.PMLR,23–29Jul2023.
[13] DavidBosch,AshkanPanahi,andBabakHassibi. Preciseasymptoticanalysisofdeeprandomfeaturemodels,2023.
[14] JacobAZavatone-VethandCengizPehlevan. Learningcurvesfordeepstructuredgaussianfeaturemodels. arXiv
preprintarXiv:2303.00564,2023.
[15] MatthiasThamm,MaxStaats,andBerndRosenow. Randommatrixanalysisofdeepneuralnetworkweightmatrices.
Phys.Rev.E,106(5):PaperNo.054124,15,2022.
[16] CharlesH.MartinandMichaelW.Mahoney. Implicitself-regularizationindeepneuralnetworks:Evidencefrom
randommatrixtheoryandimplicationsforlearning. JournalofMachineLearningResearch,22(165):1–73,2021.
[17] FlorentinGuth,BriceMénard,GasparRochette,andStéphaneMallat. Arainbowindeepnetworkblackboxes.
arXivpreprintarXiv:2305.18512,2023.
[18] HugoCui,FlorentKrzakala,andLenkaZdeborova. Bayes-optimallearningofdeeprandomnetworksofextensive-
width. InInternationalConferenceonMachineLearning,pages6468–6521.PMLR,2023.
[19] ZhenyuLiaoandRomainCouillet. Onthespectrumofrandomfeaturesmapsofhighdimensionaldata. InJennifer
DyandAndreasKrause,editors,Proceedingsofthe35thInternationalConferenceonMachineLearning,volume80of
ProceedingsofMachineLearningResearch,pages3063–3071.PMLR,10–15Jul2018.
[20] JeffreyPenningtonandPratikWorah. Nonlinearrandommatrixtheoryfordeeplearning. J.Stat.Mech.TheoryExp.,
(12):124005,14,2019.
[21] LucasBenigniandSandrinePéché. Eigenvaluedistributionofsomenonlinearmodelsofrandommatrices. Electron.
J.Probab.,26:PaperNo.150,37,2021.
[22] TengyuanLiangandPragyaSur.Aprecisehigh-dimensionalasymptotictheoryforboostingandminimum-ℓ1-norm
interpolatedclassifiers. Ann.Statist.,50(3):1669–1695,2022.
[23] BrunoLoureiro,CedricGerbelot,HugoCui,SebastianGoldt,FlorentKrzakala,MarcMézard,andLenkaZdeborová.
Learningcurvesofgenericfeaturesmapsforrealisticdatasetswithateacher-studentmodel. J.Stat.Mech.Theory
Exp.,2022(11):PaperNo.114001,78,2022.
[24] DavidBosch,AshkanPanahi,AycaÖzcelikkale,andDevdattDubhash. Doubledescentinrandomfeaturemodels:
Preciseasymptoticanalysisforgeneralconvexregularization. arXiv:2204.02678,2022.
[25] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy training of
two-layersneuralnetwork. InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett,
editors,AdvancesinNeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.
[26] BehroozGhorbani,SongMei,TheodorMisiakiewicz,andAndreaMontanari. Whendoneuralnetworksoutperform
kernelmethods? J.Stat.Mech.TheoryExp.,2021(12):PaperNo.124009,110,2021.
[27] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural
networks. InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett,editors,Advancesin
NeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.
[28] MariaRefinetti,SebastianGoldt,FlorentKrzakala,andLenkaZdeborová. Classifyinghigh-dimensionalgaussian
mixtures: Wherekernelmethodsfailandneuralnetworkssucceed. InMarinaMeilaandTongZhang,editors,
Proceedingsofthe38thInternationalConferenceonMachineLearning,volume139ofProceedingsofMachineLearning
Research,pages8936–8947.PMLR,18–24Jul2021.
12[29] JaehoonLee,YasamanBahri,RomanNovak,SamuelS.Schoenholz,JeffreyPennington,andJaschaSohl-Dickstein.
Deepneuralnetworksasgaussianprocesses. In6thInternationalConferenceonLearningRepresentations,ICLR2018,
Vancouver,BC,Canada,April30-May3,2018,ConferenceTrackProceedings.OpenReview.net,2018.
[30] AlexanderG.DeG.Matthews,JiriHron,MarkRowland,RichardE.Turner,andZoubinGhahramani. Gaussian
processbehaviourinwidedeepneuralnetworks. InInternationalConferenceonLearningRepresentations,2018.
[31] OmryCohen,OrMalka,andZoharRingel. Learningcurvesforoverparametrizeddeepneuralnetworks:Afield
theoryperspective. Phys.Rev.Res.,3:023034,Apr2021.
[32] GadiNaveh,OdedBenDavid,HaimSompolinsky,andZoharRingel. Predictingtheoutputsoffinitedeepneural
networkstrainedwithnoisygradients. Phys.Rev.E,104:064301,Dec2021.
[33] QianyiLiandHaimSompolinsky. Statisticalmechanicsofdeeplinearneuralnetworks:Thebackpropagatingkernel
renormalization. Phys.Rev.X,11:031059,2021.
[34] BorisHaninandMihaiNica. Finitedepthandwidthcorrectionstotheneuraltangentkernel. ArXiv,abs/1909.05989,
2019.
[35] R.Pacelli,S.Ariosto,M.Pastore,F.Ginelli,M.Gherardi,andP.Rotondo. Astatisticalmechanicsframeworkfor
bayesiandeepneuralnetworksbeyondtheinfinite-widthlimit. NatureMachineIntelligence,5(12):1497–1507,Dec
2023.
[36] AndreManoel,FlorentKrzakala,MarcMézard,andLenkaZdeborová. Multi-layergeneralizedlinearestimation. In
2017IEEEInternationalSymposiumonInformationTheory(ISIT),pages2098–2102,2017.
[37] Marylou Gabrié, Andre Manoel, Clément Luneau, jean barbier, Nicolas Macris, Florent Krzakala, and Lenka
Zdeborová. Entropy and mutual information in models of deep neural networks. In S. Bengio, H. Wallach,
H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,editors,AdvancesinNeuralInformationProcessing
Systems,volume31.CurranAssociates,Inc.,2018.
[38] BenjaminAubin,BrunoLoureiro,AntoineMaillard,FlorentKrzakala,andLenkaZdeborová. Thespikedmatrix
modelwithgenerativepriors. InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett,
editors,AdvancesinNeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.
[39] PaulHand,OscarLeong,andVladVoroninski. Phaseretrievalunderagenerativeprior. InS.Bengio,H.Wallach,
H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,editors,AdvancesinNeuralInformationProcessing
Systems,volume31.CurranAssociates,Inc.,2018.
[40] BenjaminAubin,BrunoLoureiro,AntoineBaker,FlorentKrzakala,andLenkaZdeborová. Exactasymptoticsfor
phaseretrievalandcompressedsensingwithrandomgenerativepriors. InJianfengLuandRachelWard,editors,
ProceedingsofTheFirstMathematicalandScientificMachineLearningConference, volume107ofProceedingsof
MachineLearningResearch,pages55–73.PMLR,20–24Jul2020.
[41] ZhouFanandZhichaoWang. Spectraoftheconjugatekernelandneuraltangentkernelforlinear-widthneural
networks. InProceedingsofthe34thInternationalConferenceonNeuralInformationProcessingSystems,NIPS’20,Red
Hook,NY,USA,2020.CurranAssociatesInc.
[42] FrancisBach. High-dimensionalanalysisofdoubledescentforlinearregressionwithrandomprojections. Preprint,
2023.
[43] EdgarDobribanandStefanWager. High-dimensionalasymptoticsofprediction:ridgeregressionandclassification.
Ann.Statist.,46(1):247–279,2018.
[44] PeterL.Bartlett,AndreaMontanari,andAlexanderRakhlin. Deeplearning:astatisticalviewpoint. Preprint,2021.
[45] ChenChengandAndreaMontanari. Dimensionfreeridgeregression. Preprint,2022.
[46] HugoLatourelle-VigeantandElliotPaquette. MatrixDysonequationforcorrelatedlinearizationsandtesterrorof
randomfeaturesregression. Preprint,2023.
[47] CosmeLouart,ZhenyuLiao,andRomainCouillet. Arandommatrixapproachtoneuralnetworks. Ann.Appl.
Probab.,28(2):1190–1248,2018.
13[48] BenAdlamandJeffreyPennington.TheNeuralTangentKernelinHighDimensions:TripleDescentandaMulti-Scale
TheoryofGeneralization. Preprint,2020.
[49] RadosławAdamczak. Anoteonthehanson-wrightinequalityforrandomvectorswithdependencies. Electron.
Commun.Prob.,20,2015.
[50] ClémentChouard. Quantitativedeterministicequivalentofsamplecovariancematriceswithageneraldependence
structure. arXiv:2211.13044,2022.
[51] CosmeLouart,ZhenyuLiao,andRomainCouillet. Arandommatrixapproachtoneuralnetworks. Ann.Appl.
Probab.,28(2):1190–1248,2018.
[52] VAMarčenkoandLAPastur. Distributionofeigenvaluesforsomesetsofrandommatrices. Mathematicsofthe
USSR-Sbornik,1(4):457,apr1967.
[53] J.W.Silverstein. Strongconvergenceoftheempiricaldistributionofeigenvaluesoflargedimensionalrandom
matrices. JournalofMultivariateAnalysis,55(2):331–339,1995.
[54] ZhidongBaiandWangZhou. Largesamplecovariancematriceswithoutindependencestructuresincolumns.
StatisticaSinica,18(2):425–442,2008.
[55] Francisco Rubio and Xavier Mestre. Spectral convergence for a general class of random matrices. Statistics &
ProbabilityLetters,81(5):592–602,2011.
[56] AnttiKnowlesandJunYin. Anisotropiclocallawsforrandommatrices. Probab.TheoryRelatedFields,169(1-2):257–
352,2017.
[57] GiorgioCipolloni,LászlóErdős,andDominikSchröder. Rank-uniformlocallawforWignermatrices. ForumMath.,
Sigma,10,2022.
[58] ClémentChouard.DeterministicequivalentoftheConjugateKernelmatrixassociatedtoArtificialNeuralNetworks.
Preprint,2023.
[59] Ivan Nourdin and Giovanni Peccati. Normal approximations with Malliavin calculus: from Stein’s method to
universality,volume192. CambridgeUniversityPress,2012.
[60] ArthurJacot,BerfinSimsek,FrancescoSpadaro,ClementHongler,andFranckGabriel. Implicitregularizationof
randomfeaturemodels. InHalDauméIIIandAartiSingh,editors,Proceedingsofthe37thInternationalConference
onMachineLearning,volume119ofProceedingsofMachineLearningResearch,pages4631–4640.PMLR,13–18Jul
2020.
[61] DiederikPKingmaandJimmyBa. Adam:Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,
2014.
[62] JimmyBa,MuratA.Erdogdu,TaijiSuzuki,ZhichaoWang,DennyWu,andGregYang.High-dimensionalasymptotics
offeaturelearning:Howonegradientstepimprovestherepresentation,2022.
[63] RadosławAdamczak. AnoteontheHanson-Wrightinequalityforrandomvectorswithdependencies. Electron.
Commun.Probab.,20:no.72,13,2015.
[64] RomanVershynin. Introductiontothenon-asymptoticanalysisofrandommatrices. Preprint,2010.
[65] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,Zeming
Lin,NataliaGimelshein,LucaAntiga,AlbanDesmaison,AndreasKopf,EdwardYang,ZacharyDeVito,Martin
Raison,AlykhanTejani,SasankChilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumithChintala. Pytorch:
Animperativestyle,high-performancedeeplearninglibrary. arXivpreprintarXiv:1912.01703,2019.
14A Anisotropic asymptotic equivalents
RecallfromAssumption2.2thatweassumethatthefeaturematricesX,Z areLipschitz-concentratedinthefollowing
sense(consideringthevectorsspaceofrectangularmatricesequippedwiththeFrobeniusnorm):
Definition A.1 (Lipschitz concentration). Wesay that arandom vector x in anormed vector space X is Lipschitz-
concentratedwithconstantµifthereexistsaconstantC suchthatforall1-Lipschitzfunctionsf: X → Ritholds
that
(cid:16) t2 (cid:17)
P(|f(x)−Ef(x)|≥t)≤Cexp − . (37)
Cµ2
A sufficent condition for Lipschitz concentration is that that the columns x = φ(x ) are Lipschitz functions of
i i √
Gaussian random vectors x of bounded covariance Ω := Ex x⊤, c.f. Remark 2.4. Indeed, let φr(g) := φ( Ω g)
i 0 i i 0
and consider standard Gaussian vectors g ,...,g . We recall that standard Gaussian random vectors are Lipschitz-
i n
concentratedwithaconstantwhichisindependentofthedimension:
TheoremA.2(Gaussianconcentration). LetgbearandomvectorwithindependentstandardGaussianentries.Thengis
Lipschitz-concentratedwithconstantµ=1.
ThereforewecanstacktheGaussianvectorsg ,...,g intog∈RnpandwriteX =X(g)=(φr(g ),...,φr(g )).
1 n 1 n
Then X is Lipschitz-concentrated with dimension-independent constant by Theorem A.2 since for any Lipschitz
f: Rp×n →Ritholdsthatg(cid:55)→f(X(g))isLipschitzdueto
(cid:88) (cid:88)
|f(X(g))−f(X(g′))|2 ≤∥X(g)−X(g)′∥2 = ∥φr(g )−φr(g′)∥2 ≲ ∥g −g′∥2 =∥g−g′∥2. (38)
F i i i i
i i
Resolventconcentration
ItwillbeusefultointroducealsotheresolventoftheassociatedGrammatrixX⊤X/pwhichisgivenby
Gq =(cid:16)X⊤X +λ(cid:17)−1 . (39)
p
Thetworesolventsarerelatedbytheidentity
X⊤GX = 1 X⊤(cid:16)XX⊤ +λ(cid:17)−1 X = X⊤X(cid:16)X⊤X +λ(cid:17)−1 =1−λGq . (40)
p p p p p
BothresolventsG,GqareLipschitz-continuouswithrespecttotheFrobeniusnormduetotheresovlentidentity
(cid:16)XX⊤ (cid:17)−1 (cid:16)YY⊤ (cid:17)−1 (cid:16)XX⊤ (cid:17)−1(Y −X)Y⊤+X(Y −X)⊤(cid:16)YY⊤ (cid:17)−1
+λ − +λ = +λ +λ (41)
p p p p p
andthebound
(cid:112) (cid:112)
∥GX∥≤ p∥G∥+pλ∥G2∥≤ 2p/λ, (42)
implying
µ (cid:16)XX⊤ (cid:17)−1 (cid:16)YY⊤ (cid:17)−1
∥G−G′∥ ≤2 ∥X−Y∥ , G:= +λ , G′ := +λ . (43)
F λ3/2p1/2 F p p
Thereforeweobtainthat
|⟨A(G−EG)⟩|≲
⟨|A|2⟩1/2
, |⟨A(Gq −EGq )⟩|≲
⟨|A|2⟩1/2
(44)
λ3/2p λ3/2p1/2n1/2
fromTheoremA.2,
1 2⟨|A|2⟩1/2
|⟨A(G−G′)⟩|≤ ∥A∥ ∥G−G′∥ ≤ ∥X−Y∥ . (45)
p F F λ3/2p F
andtheanalogousestimateforGq −Gq ′.Animportantspecialcaseofeq.(44)isAbeingrank-onewhichyields
|x⊤Gy−Ex⊤Gy|≲ ∥x∥∥y∥ , |x⊤Gq y−Ex⊤Gq y|≲ ∥x∥∥y∥ (46)
λ3/2p1/2 λ3/2p1/2
15Quadraticformandnormconcentration
TheotherimportantconcentrationresultneededintheproofofTheorem3.1istheconcentrationofquadraticforms,see
e.g.Theorem2.3in[63].
Theorem A.3. If x is a random vector of mean zero satisfying Lipschitz concentration with constant µ, and A is a
deterministicmatrix,then
|x⊤Ax−Ex⊤Ax|≲µ2∥A∥ . (47)
F
√
FinallyweneedsomeupperboundontheoperatornormofX/ pwhichcanbeobtainedstandardϵ-netarguments,
(cid:13) (cid:13) (cid:13)XX⊤ −Ω(cid:13) (cid:13) (cid:13)≺ p , (48)
(cid:13) n (cid:13) n
seee.g.Remark 5.40in[64].
Leave-one-outidentities
Definetheleave-one-outresolventG =(λ+p−1(cid:80) x x⊤)−1forwhichwehavetheidentity
−i j̸=i j j
G=G − 1 G −ix ix⊤ i G −i =G −λG −ix ix⊤ i G −iGq
−i p1+x⊤G x /p −i p ii
i −i i (49)
Gx =G x (cid:16) 1− 1 x⊤ i G −ix i (cid:17) = G −ix i =λGq G x
i −i i p1+x⊤G x /p 1+x⊤G x /p ii −i i
i −i i i −i i
wherethedenominatorscanbesimplifiedusing
− 1 = x⊤ i G −ix i −1= x⊤ i Gx i −1=−λ(Gq ) (50)
1+x⊤G x /p 1+x⊤G x /p p ii
i −i i i −i i
dueto(40).
AnisotropicMarchenko-PasturLaw
WearenowreadytoproveTheorem3.3,theanisotropicMarchenko-PasturLaw.InthecomparableregimefromTheo-
rem3.1wewillshowthat
⟨|A|2⟩1/2(cid:16) p n(cid:17)
|⟨[G(λ)−M(λ]A⟩|≺ 1+ + . (51)
pλ3 n p
ProofofTheorem3.3. FortheresolventGweobtaintheequation
I =
λ(cid:88)(cid:16) (EGq
)EG
Ω+E(Gq −EGq
)G x
x⊤(cid:17)
+λEG
p ii −i ii ii −i i i
i (52)
=EG(cid:16) λn ⟨EGq ⟩Ω+λ(cid:17)
+
λ(cid:88)(cid:16) ⟨EGq
⟩(EG
−EG)Ω+E(Gq −EGq
)G x
x⊤(cid:17)
p p −i ii ii −i i i
i
sothat
EG=(cid:16) λn ⟨EGq ⟩Ω+λ(cid:17)−1 +λ(cid:88)(cid:16) ⟨EGq ⟩(EG −EG)Ω+E(Gq −EGq )G x x⊤(cid:17)(cid:16) λn ⟨EGq ⟩Ω+λ(cid:17)−1 . (53)
p p −i ii ii −i i i p
i
Usingthebounds
1 1
∥G x x⊤−EG x x⊤∥ ≤∥G x x⊤−G Ω∥ +∥(G −E G )Ω∥ ≺ + , (54)
−i i i −i i i F −i i i −i F −i −i −i F λ p1/2λ3/2
∥EG −i−EG∥ F =λ|Gq ii|(cid:13) (cid:13) (cid:13) (cid:13)G −ix i px⊤ i G −i(cid:13) (cid:13) (cid:13) (cid:13) ≺ p1(cid:16) ∥G −iΩG −i∥ F +∥G −i(x ix⊤ i −Ω)G −i∥ F(cid:17) ≺ p1/1 2λ2 (55)
F
16and|Gq −EGq |≺ 1 fromEq.(46)wethusobtain
ii ii p1/2λ3/2
(cid:13) (cid:13)EG(λ)−M(λ,⟨EGq ⟩)(cid:13) (cid:13) ≺ n , M(λ,m):=(cid:16) λn mΩ+λ(cid:17)−1 . (56)
(cid:13) (cid:13) F p3/2λ3 p
NotethatwhileM(λ,⟨EGq ⟩)isadeterministicmatrix,itstilldependsontheexpectedtraceofGqexplicitly.However,
weclaimthat
|m−⟨EGq ⟩|≲ m , (57)
pλ3
proving
∥EG−M∥ ≲ 1 + n +∥M(λ,⟨EGq ⟩)−M(λ,m)∥ ≲ 1 (cid:16) 1+ n + p(cid:17) . (58)
F p1/2λ3/2 p3/2λ3 F p1/2λ3 p n
NowEq.(51)followsdirectlytogetherwiththeconcentrationestimateEq.(44).Finally,eq.(57)followsfrom
(cid:12) (cid:12)
q q (cid:12) 1 1 (cid:12) q (cid:12) q (cid:12) (cid:18) m (cid:19)
|m−⟨EG⟩|≤λm|⟨EG⟩|(cid:12) − (cid:12)=λm|⟨EG⟩|(cid:12)⟨ΩM(λ,m(λ))⟩−⟨ΩM(λ,⟨EG⟩)⟩(cid:12)+O
(cid:12) (cid:12)λm λ⟨EGq ⟩(cid:12)
(cid:12)
(cid:12) (cid:12) pλ2
(cid:18) (cid:19)
≤|m−⟨EGq ⟩|λ2m|⟨EGq ⟩|n ⟨ΩM(λ,m(λ))ΩM(λ,⟨EGq
⟩)⟩+O
m (59)
p pλ2
(cid:18) (cid:19)
q ⟨Ω⟩ m
≤|m−⟨EG⟩| +O
λ+⟨Ω⟩ pλ2
dueto
q p (cid:16) q (cid:17)
⟨ΩM(λ,⟨EG⟩)⟩= 1−λ⟨M(λ,⟨EG⟩)⟩
q
nλ⟨EG⟩
(cid:32) (cid:33) (cid:32) (cid:33) (60)
p (cid:16) (cid:17) 1 1 1
= 1−λ⟨EG⟩ +O = −1+O .
q q q q
nλ⟨EG⟩ pλ3⟨EG⟩ λ⟨EG⟩ pλ3⟨EG⟩
and
λ2m⟨EGq ⟩n ⟨ΩM(λ,m)ΩM(λ,⟨EGq ⟩)⟩≤λm⟨ΩM⟩=1−λm≤ ⟨Ω⟩ . (61)
p λ+⟨Ω⟩
Multi-ResolventDeterministicEquivalents
ThekeyforprovingTheorem3.1isextendingtheanisotropicMarchenko-Pasturtomutli-resolventexpressions,which
wesummarizeinthefollowingproposition.Forsimplicitywecarrythepreciseerrorterminthecomparableregimeonly
inthefirststatement,theotheronesbeingsimilar.
PropositionA.4.
1. ForanyA∈Rk×pwehave
1 λmn (cid:18) n (cid:16) n p(cid:17)(cid:19)
√ ⟨GXZ⊤A⟩= √ ⟨MΦA⟩+O 1+ + (62)
kp kp k1/2p3/2λ3 p n
2. ForanyA∈Rp×pwehavemoregenerally
⟨AMΩM⟩
(cid:18) ⟨|A|2⟩1/2(cid:19)
⟨AGΩG⟩= +O (63a)
1− n(mλ)2⟨ΩMΩM⟩ pλ7
p
whileforanyA,B ∈Rp×pwehave
n ⟨AMΩM⟩⟨ΩMBM⟩
(cid:18) ⟨|A|2⟩1/2∥B∥(cid:19)
⟨AGBG⟩=⟨AMBM⟩+ (mλ)2 +O (63b)
p 1− n(mλ)2⟨ΩMΩM⟩ pλ7
p
173. ForanyA∈Rp×pwehave
(cid:28) X⊤GΩGXA(cid:29) λ2m2⟨ΩMΩM⟩ (cid:18) ⟨|A|2⟩1/2(cid:19)
= ⟨A⟩+O (64)
p 1− n(mλ)2⟨ΩMΩM⟩ pλ7
p
4. Finally,foranyA∈Rp×pwehave
(cid:28) ZX⊤GΩGXZ⊤A(cid:29) =(mλ)2n(cid:68) A(cid:16)(cid:0) Ψ−2n pλmΦ⊤MΦ(cid:1) ⟨ΩMΩM⟩+ n pΦ⊤MΩMΦ(cid:17)(cid:69)
kp k 1− n(mλ)2⟨ΩMΩM⟩
p
(65)
(cid:18) ⟨|A|2⟩1/2(cid:19)
+O
pλ7
BeforeturningtotheproofofPropositionA.4,wedemonstratehowPropositionA.4impliesTheorem3.1.
ProofofTheorem3.1. ByapplyingPropositionA.4tothetermsofEq.(2)weobtain
φ⊤Ψφ φ⊤ZX⊤GΩGXZ⊤φ n(cid:28) X⊤GΩGXΣ(cid:29) φ⊤Φ⊤GXZ⊤φ
E = ∗ ∗ + ∗ ∗ + −2 ∗ ∗
gen k kp2 p p kp
1 (cid:18) n(cid:0) Ψ−2nλmΦ⊤MΦ(cid:1) ⟨ΩMΩM⟩+ nΦ⊤MΩMΦ n (cid:19)
= φ Ψ+(mλ)2 p p −2λm Φ⊤MΦ φ (66)
k ∗ p 1− n(mλ)2⟨ΩMΩM⟩ p ∗
p
(λm)2n⟨MΩMΩ⟩ (cid:18)
∥φ
∥2(cid:19)
+⟨Σ⟩ p +O ∗ .
1− n(λm)2⟨ΩMΩM⟩ p1/2λ7
p
ItremainstoshowthatthematrixinthebracketscanbesimplifiedtotheexpressioninTheorem3.1.Forthelasttermin
thenumeratorofthefractionweuse
n
mλ MΩM =M −λM2, (67)
p
sothatthebracket,aftersimplifying,becomes
Ψ−mλnΦ⊤(M +λM2)Φ
p , (68)
1− n(mλ)2⟨ΩMΩM⟩
p
justasclaimed.
ProofofPropositionA.4. WebeginwiththeproofofItem1. Firstnotethat⟨GXZ⊤A⟩isaLipschitzfunctionofthe
GaussianrandomnessdusedtoconstructX andZ.Indeed,denotingG,X,Z evaluatedatanotherrealizationofthe
GaussianrandomnessbyG′,X′,Z′wehave
⟨GXZ⊤A⟩−⟨G′X′(Z′)⊤A⟩=⟨(G−G′)XZ⊤A⟩+⟨G′(X−X′)Z⊤A⟩+⟨G′X′(Z−Z′)⊤A⟩
(cid:18) ∥X−X′∥ ∥X∥∥Z∥⟨|A|2⟩1/2 (∥X−X′∥ ∥Z∥+∥X∥∥Z−Z′∥ )⟨|A|2⟩1/2(cid:19)
=O F + F F ,
λ3/2p pλ
(69)
√ √
sothatonthehighprobabilityevent(recallEq.(48))that∥X∥≺ p,∥Z∥≺ kitfollowsthat⟨GXZ⊤A⟩isLipschitz
withconstant⟨|A|2⟩1/2/pλ3/2.Byestimatingthecomplementofthishighprobabilityeventtriviallywecanconclude
(cid:12) (cid:12) (cid:12)√1 ⟨GXZ⊤A⟩− √1 ⟨EGXZ⊤A⟩(cid:12) (cid:12) (cid:12)≺ ⟨|A|2⟩1/2 . (70)
(cid:12) kp kp (cid:12) pλ3/2
FortheexpectationwewriteoutXZ⊤anduseeq.(49)weobtain
√1 GXZ⊤ = √1 (cid:88) Gx z⊤ = √1 (cid:88) λGq G x z⊤. (71)
kp kp i i kp ii i i i
i i
18With
√λ E(cid:88) (Gq ) ⟨G x z⊤A⟩= √λ (cid:88)(cid:16) (EGq )⟨EG x z⊤A⟩+O(cid:18)(cid:113) VarGq (cid:113) Var⟨G x z⊤A⟩(cid:19)(cid:17)
kp ii i i i kp ii i i i ii i i i
i i
(cid:18) (cid:19)
= √λ (cid:88) (EGq )⟨EG ΦA⟩+O n (72)
kp ii i k1/2p3/2λ2
i
λmn (cid:18) n (cid:16) n p(cid:17)(cid:19)
= √ ⟨MΦA⟩+O 1+ +
kp k1/2p3/2λ3 p n
duetoEq.(58),VarGq ≲ 1 and
ii pλ3
1 ⟨|A|2⟩
Var⟨G x z⊤A⟩≲ E ∥AG ∥2 +Var ⟨G ΦA⟩≲ (73)
−i i i p2 −i −i F −i −i pλ3
byeq.(44),thisconcludestheproofofItem1.
WenowturntotheproofofItem2.FirstnotethatbyLipschitzconcentrationwehave
∥A∥⟨|B|2⟩1/2
|⟨AGBG−EAGBG⟩|≲ (74)
pλ5/2
dueto
∥A∥∥B∥
|⟨AGBG⟩−⟨AG′BG′⟩|≤|⟨A(G−G′)BG⟩|+|⟨AG′B(G−G′)⟩|≤2 F∥G−G′∥ (75)
pλ F
andeq.(43).
ItisusefultoexpandGaroundM asin
n q XX⊤ n q XX⊤ q n (cid:18) 1 (cid:19)
G=M +λMΩG ⟨m−G⟩−M G+λMΩG ⟨G⟩=M −M G+λ⟨G⟩MΩG +O MΩG
p p p p p pλ3
(76)
usingeq.(57)inthesecondstep.Consequentlyweobtain
XX⊤ nλ q (cid:18) ⟨|A|2⟩1/2⟨|B|2⟩1/2(cid:19)
⟨GAGB⟩=⟨MAGB⟩−⟨M GAGB⟩+ ⟨G⟩⟨MΩGAGB⟩+O
p p pλ6
=⟨MAMB⟩−
1(cid:88)
(⟨Mx
x⊤GAGB⟩−λGq
⟨MΩGAGB⟩)+O(cid:18) ∥B∥⟨|A|2⟩1/2(cid:19)
p i i ii pλ6
i (77)
=⟨MAMB⟩− λ(cid:88) Gq (⟨Mx x⊤G AG
B⟩−⟨MΩGAGB⟩)+O(cid:18) ∥B∥⟨|A|2⟩1/2(cid:19)
p ii i i −i −i pλ6
i
+ λ2 (cid:88) Gq2 x⊤ i G −iAG −ix ix⊤ i G −iBMx i,
p ii p p
i
usingeq.(49)inthethirdstep.Thesecondtermofeq.(77)canbeestimatedinexpectationusing
λ E(cid:88) Gq ⟨Mx x⊤G AG B⟩= λ(cid:88)(cid:16) (EGq )⟨EMΩG AG B⟩+O(cid:18)(cid:113) VarGq (cid:113) Var⟨Mx x⊤G AG B⟩(cid:17)(cid:19)
p ii i i −i −i p ii −i −i ii i i −i −i
i i
λ(cid:88) q
(cid:18) n∥B∥⟨|A|2⟩1/2(cid:19)
= (EG )⟨EMΩGAGB⟩+O
p ii p2λ4
i
λ (cid:88) q
(cid:18) n∥B∥⟨|A|2⟩1/2(cid:19)
= E G ⟨MΩGAGB⟩+O
p ii p2λ4
i
(78)
sinceVarGq ≲ 1 ,
ii pλ3
1 ∥B∥2⟨|A|2⟩ 1
Var⟨Mx x⊤G AG B⟩≲ E ∥G AG BM∥2 +Var ⟨MΩG AG B⟩≲ (1+ ). (79)
i i −i −i p2 −i −i −i F −i −i −i pλ6 pλ
19and
1
(cid:18) ∥B∥⟨|A|2⟩1/2(cid:19)
∥G−G ∥≲ , ⟨MΩG AG B⟩=⟨MΩGAGB⟩+O . (80)
i pλ2 −i −i pλ4
ForthelasttermofEq.(77)wehave
x⊤G AG x (cid:18) 1 (cid:19) ⟨|A|2⟩1/2
i −i −i i =⟨ΩG AG ⟩+O ∥G AG ∥ =⟨ΩGAG⟩+O( )
p −i −i p −i −i F p1/2λ2
(81)
x⊤G BMx (cid:18) 1 (cid:19) ⟨|B|2⟩1/2
i −i i =⟨ΩG BM⟩+O ∥G BM∥ =⟨ΩMBM⟩+O( ),
p −i p −i F p1/2λ2
sothatwith
EGq2 x⊤ i G −iAG −ix ix⊤ i G −iBMx i =(EGq2)(cid:16) Ex⊤ i G −iAG −ix i(cid:17)(cid:16) Ex⊤ i G −iBMx i(cid:17) +O(cid:18) ⟨|A|2⟩1/2⟨|B|2⟩1/2(cid:19)
ii p p ii p p pλ7
=(EGq2)⟨EΩGAG⟩⟨EΩGBM⟩+O(cid:18) ⟨|A|2⟩1/2⟨|B|2⟩1/2(cid:19)
ii pλ7
(82)
and
(cid:18) (cid:19)
1(cid:88) Gq2 = n m2+2n m⟨Gq −m⟩+ 1(cid:88) (Gq −m)2 = n m2+O 1 + n (83)
p ii p p p ii p pλ5 p2λ3
i i
wearriveat
λ2 E(cid:88) Gq2 x⊤ i G −iAG −ix ix⊤ i G −iBMx i = n (mλ)2⟨ΩMBM⟩⟨EΩGAG⟩+O(cid:18) ⟨|A|2⟩1/2⟨|B|2⟩1/2(cid:19) . (84)
p ii p p p pλ6
i
ChoosingB =Ωitfollowsthat
n
(cid:18) ⟨|A|2⟩1/2(cid:19)
⟨GAGΩ⟩(1− λ2m2⟨ΩMΩM⟩)=⟨MAMΩ⟩+O , (85)
p pλ6
sothatthefinalclaimItem2followsupondivision.
TurningtotheproofofItem3wefirstnotethatbyeq.(49)wehave
E(cid:16)X⊤GΩGX(cid:17)
=λ2EGq2⟨EΩG
ΩG
⟩+O(cid:18) 1(cid:113)
VarGq
2(cid:113)
Varx⊤G ΩG x
(cid:19)
p ii ii −i −i p ii i −i −i i
(86)
(cid:18) (cid:19)
=λ2EGq2 ⟨ΩMΩM⟩
+O
1
,
ii1− n(mλ)2⟨ΩMΩM⟩ pλ7
p
sothatbyaLipschitzconcentrationargumentasinEq.(74)weobtainforthediagonalpartA ofA=A +A that
d d o
(cid:28) X⊤GΩGX (cid:29) λ2m2⟨ΩMΩM⟩ (cid:18) ⟨|A |2⟩1/2(cid:19)
A = ⟨A ⟩+O d . (87)
p d 1− n(mλ)2⟨ΩMΩM⟩ d pλ7
p
Fortheoff-diagonalpartweuseeq.(49)twicetoobtain
q q
(cid:16)X⊤GΩGX(cid:17) λ2G G
= ii jjx⊤G ΩG x
p ij p i −i −j j
λ2Gq Gq λ4Gq 2Gq 2
= ii jjx⊤G ΩG x + ii jjx⊤G x x⊤G ΩG x x⊤G x (88)
p i −ij −ij j p3 i −ij j j −ij −ij i i −ij j
−
λ3Gq 2 iiGq
jjx⊤G ΩG x x⊤G x −
λ3Gq iiGq 2
jjx⊤G x x⊤G ΩG x .
p2 i −ij −ij i i −ij j p2 i −ij j j −ij −ij j
Thesecondtermcanbeestimatedtriviallybyp−3/2λ−4,whileforthefirst,thirdandfourthtermsthetrivialestimatesof
p−1/2λ−2,p−1λ−3andp−1/2λ−3donotsuffice.ForthoseweusetheexpectationanddecomposeGq =m+(Gq −m),
ii ii
Gq =m+(Gq −m)toobtain
jj jj
λ2Gq Gq λ2(Gq −m)(Gq
−m)
(cid:18)
1
(cid:19)
E ii jjx⊤G ΩG x =E ii jj x⊤G ΩG x =O (89)
p i −ij −ij j p i −ij −ij j λ2p3/2
20and
λ3Gq 2Gq λ3(Gq 2Gq −m3) (cid:18) 1 (cid:19)
E ii jjx⊤G ΩG x x⊤G x =E ii jj x⊤G ΩG x x⊤G x =O (90)
p2 i −ij −ij i i −ij j p2 i −ij −ij i i −ij j p3/2λ7/2
usingthat,say,x iscenteredandindependentofx ,G .Bycombiningtheseestimatesweobtain
j i −ij
E(cid:12) (cid:12) (cid:12)(cid:16)X⊤GΩGX(cid:17) (cid:12) (cid:12) (cid:12)=O(cid:18) 1 (cid:19) , (91)
(cid:12) p ij(cid:12) p3/2λ4
concludingtheproofofItem3.
Wenowturntotheproofof (4)whichfollowsasimilarstrategyastheproofofItem2. Firstwenotethatbya
LipschitzconcentrationargumentasinEq.(74)itissufficienttoapproximatetheexpectationofZX⊤GΩGXZ⊤.By
writingoutZX⊤andXZ⊤andusingeq.(49)twiceweobtain
1 1 (cid:88)
ZX⊤GΩGXZ⊤ = z x⊤GΩGx z⊤
kp kp i i j j
ij (92)
= 1 (cid:88) (λGq )2z x⊤G ΩG x z⊤+ 1 (cid:88) (λGq )(λGq )z x⊤G ΩG x z⊤.
kp ii i i −i −i i i kp ii jj i i −i −j j j
i i̸=j
ForthefirsttermofEq.(92)wehave
n n (cid:18) n (cid:113) (cid:113) (cid:19)
E⟨Az x⊤G ΩG x z⊤⟩= (Ez⊤Az )(Ex⊤G ΩG x )+O Varz⊤Az Varx⊤G ΩG x
kp i i −i −i i i k2p i i i −i −i i k2p i i i −i −i i
n
(cid:18) n⟨|A|2⟩1/2(cid:19)
= ⟨AΨ⟩E⟨ΩG ΩG ⟩+O
k −i −i p1/2k3/2λ2
n ⟨ΩMΩM⟩ (cid:18) n⟨|A|2⟩1/2 (cid:112) (cid:19)
= ⟨AΨ⟩ +O (1+ p/k)
k 1− n(mλ)2⟨ΩMΩM⟩ pkλ3
p
(93)
usingItem2intheultimatestep.ForthesecondtermintherighthandsideofEq.(92)weexpandbothG andG
−i −j
aroundG usingEq.(49)to
−ij
(cid:42) (cid:43)
⟨z x⊤G ΩG x z⊤A⟩≈ z
x⊤(cid:16)
G −λmG
x jx⊤
j G
(cid:17) Ω(cid:16)
G −mλG
x ix⊤
i G
(cid:17)
x z⊤A
i i −i −j j j i i −ij −ij p −ij −ij −ij p −ij j j
(cid:42) (cid:43)
=(cid:10) z x⊤G ΩG x z⊤A(cid:11) +(λm)2 z x⊤G x jx⊤ j G ΩG x ix⊤ i G x z⊤A
i i −ij −ij j j i i −ij p −ij −ij p −ij j j
−λm(cid:28)
z x⊤G ΩG
x ix⊤
i G x
z⊤A(cid:29) −λm(cid:42)
z x⊤G
x jx⊤
j G ΩG x
z⊤A(cid:43)
.
i i −ij −ij p −ij j j i i −ij p −ij −ij j j
(94)
Hereinthefirstlinewereplaced(Gq ) and(Gq ) bymwhichresultsinanerrortermnegligiblecomparedtothe
−i jj −j ii
othererrorterms.ThefirsttermofEq.(94)can,inexpectation,beapproximatedby
E(cid:10) z x⊤G ΩG x z⊤A(cid:11) =E⟨Φ⊤G ΩG ΦA⟩=
⟨Φ⊤MΩMΦA⟩ +O(cid:18) ⟨|A|2⟩1/2(cid:19)
, (95)
i i −ij −ij j j −ij −ij 1− n(mλ)2⟨ΩMΩM⟩ pλ7
p
usingItem2intheultimatestep.ThethirdtermofEq.(94)canbeapproximatedby
(cid:28) x x⊤ (cid:29)
λmE z x⊤G ΩG i i G x z⊤A
i i −ij −ij p −ij j j
1
= λm(x⊤G ΦAz )(x⊤G ΩG x )
kp i −ij i i −ij −ij i
 (cid:114) (cid:115)  (96)
(cid:16) x⊤G ΦAz x⊤G ΩG x (cid:17)
=λmE −ij ⟨Φ⊤G −ijΦA⟩⟨ΩG −ijΩG −ij⟩+O Var i i − kij i Var i i −ij p −ij i 
⟨Φ⊤MΦA⟩⟨ΩMΩM⟩ (cid:18) ⟨|A|2⟩(cid:16) (cid:114) p(cid:17)(cid:19)
=λm +O 1+
1− n(mλ)2⟨ΩMΩM⟩ λ7p k
p
21andthefourthtermisexactlythesamebysymmetry.Hereintheultimatestepweused
x⊤G ΩG x 1 1 x⊤G ΦAz ⟨|A|2⟩
Var i −ij −ij i ≲ ∥G ΩG ∥2 ≲ , Var i −ij i ≲ (97)
i p p2 −ij −ij F pλ2 i k λk
andEq.(58)andItem2.Finally,forthesecondtermofEq.(92)weusethesimplebound
(cid:42) (cid:43)
z x⊤G
x jx⊤
j G ΩG
x ix⊤
i G x z⊤A
i i −ij p −ij −ij p −ij j j
= 1 (x⊤G x )(x⊤G ΩG x )(x⊤G x )(z⊤Az ) (98)
kp2 i −ij j j −ij −ij i i −ij j j i
(cid:18) 1 (cid:19) (cid:18) ⟨|A|2⟩1/2 (cid:19)
=O ∥G ∥ ∥G ΩG ∥ ∥G ∥ ∥A∥ =O .
kp2 −ij F −ij −ij F −ij F F k1/2p1/2λ4
BycombiningalltheaboveestimatesweconcludetheproofofItem4.
B Linearization of population covariance
B.1 Technicalbackground
Inthissectionwestateseveraldefinitionsandpropositionsfrom[59],thatwillbeusedfurtherinourarguments.Let
x ∈ Rd be a mean-zero Gaussian vector with covariance ExxT = I. Let X = {X(v) := v⊤x, forv ∈ Rd} be a
collectionofjointlyGaussiancenteredrandomvariables.NotethatEX(g)X(h)=g⊤h.ThetheoryofWienerchaos,
whichwillbeintroducedshortly,canbeusedtostudyfunctionsontheprobabilityspace(Ω,F,P),whereFisgenerated
byX.Forourneeds,weonlystatetheresultsfortheexplicitconstructionofX,however,notethattheresultsfrom[59]
areaboutgeneralseparableHilbertspaces.
Following([59],Definition2.2.3),wewriteH todenotetheclosedlinearsubspaceofL2(Ω,F,P)generatedbythe
n
randomvariablesoftypeH (X(h)),h∈Rd =:H,∥h∥=1.WecallH ,thenthWienerchaos.
n n
DefinitionB.1. LetL2(Ω,H⊗˜p)bethespaceoffunctionsf :Rd×p →R,suchthatf issquare-integrableand
1 (cid:88)
f(a ,...,a )= f(a ,...,a ). (99)
1 p p! σ(1) σ(p)
σ∈Sp
Let S denote the set of all random variables of the form f(X(h ),...,X(h )), where f : Rm → R is a C∞-
1 m
function.
DefinitionB.2([59],Definition2.3.2). LetF ∈S andp≥1beaninteger. ThepthMalliavinderivativeofF (with
respecttoX)istheelementofL2(Ω,H⊗˜p),definedby
(cid:88)m ∂pf
DpF := (X(h ),...,X(h ))h ⊗...⊗h . (100)
∂x ...∂x 1 m i1 ip
i1,...,ip=1
i1 ip
PropositionB.3([59],Proposition2.3.7). Letϕ:Rm →Rbeacontinuouslydifferentiablefunctionwithboundedpartial
derivatives.SupposethatF =(F ,...,F )isarandomvectorwhosecomponentsarefunctionswithderivativesinLq(γ),
1 m
forsomeq ≥1.Then,derivativeofϕ(F)alsoliesinLq(γ)and
m
(cid:88) ∂ϕ
Dϕ(F)= (F)DF . (101)
∂x i
i
i=1
DefinitionB.4([59],Definition2.5.2). WedefineδpuastheuniqueelementofL2satisfying
E[Fδp(u)]=E[⟨DpF,u⟩ ].
H⊗p
DefinitionB.5([59],Definition2.7.1). Letp ≥ 1andf ∈ H⊗˜p. Thep-thmultipleintegraloff withrespecttoX is
definedbyI (f)=δp(f).
p
22PropositionB.6([59],Proposition2.7.5). Fixintegers1≤q ≤pandf ∈H⊗˜pandg ∈H⊗˜q.Wehave
EI (f)I (g)=δ p!⟨f,g⟩ (102)
p q pq H⊗p
TheoremB.7([59],Theorem2.7.7). Letf ∈Hbesuchthat∥f∥ =1.Then,foranyintegerp≥1,wehave
H
H (X(f))=I (f⊗p), (103)
p p
whereH isthep-thHermitepolynomial.
p
CorollaryB.8([59],Corollary2.7.8). EveryF ∈L2(Ω)canbeexpandedas
∞
(cid:88)
F =EF + I (f ), (104)
p p
p=1
forsomeuniquecollectionofkernelsf ∈H⊗˜p,p≥1.Moreover,ifF ∈C∞,thenforallp≥1,
p
1
f = EDpF. (105)
p p!
TheoremB.9([59],Theorem5.1.5). LetF ∈C∞beasquare-integrablefunction.LetEF =0andEF2 =σ2 >0and
N ∼N(0,σ2).Leth:R→RbeC2with∥h′′∥ <∞.Then,
∞
|Eh(N)−Eh(F)|≤ 1 ∥h′′∥ E(cid:2) |⟨DF,−DL−1F⟩−σ2|(cid:3) . (106)
2 ∞
Forourapplication,weneedthefollowingexpansion:forsmoothoddfunctionsf,andmatrixW ∈Rk×d,wecan
write
(cid:88)Ef(p)((WW⊤)1/2N)
f(Wx) =f(w⊤x)= ii I (w⊗p), (107)
i i p! p i
p≥1
wherew ∈Rdisthei-throwofW.Herewithoutlossofgeneralityweassumethatxhasi.i.d.entries,thegeneralcase
i √
ofcovarianceΩ thenfollowsuponredefiningW (cid:55)→W Ω .
0 1 1 0
LemmaB.10(Weakcorrelation). Letb ≥ 1beanfixedinteger. Leth ,h ,...,h beacollectionoffunctions. Then,if
0 1 b
⟨w ,w ⟩≲d−1/2fori̸=j,wehavethat
i j
(cid:34) b (cid:35) b
(cid:89) (cid:89)
E h (u⊤φ (W1x)) h (w⊤x) =Eh (u⊤φ (W1x)) Eh (w⊤x)+O(d−1/2). (108)
0 1 i i 0 1 i i
i=1 i=1
Proof. Thefactthatu ≲d−1/2andφ (w⊤x)≲1togetherwithperturbationanalysisimplythat
i 1
(cid:34) b (cid:35)    b 
(cid:89) (cid:88) (cid:89)
E h 0(u⊤φ 1(W1x)) h i(w i⊤x) =Eh 0 u kφ 1(w k⊤x) h i(w i⊤x)+O(d−1/2). (109)
i=1 k≥b+1 i=1
(cid:16) (cid:17)
LetA:=h (cid:80) u φ (w⊤x) andB :=(cid:81)b h (w⊤x). Notethatforanyp≥1,⟨EDpA,EDpB⟩constitutes
0 k≥b+1 k 1 k i=1 i i
ofterms⟨w ,w ⟩wherei̸=j.EachoftheseinnerproductsisoforderO(d−1/2)byourassumptions.Therefore,intotal,
i j
⟨EDpA,EDpB⟩=O(d−p/2).Thisimpliesthat
   b     (cid:34) b (cid:35)
(cid:88) (cid:89) (cid:88) (cid:89)
Eh 0 u kφ 1(w k⊤x) h i(w i⊤x)=Eh 0 u kφ 1(w k⊤x)E h i(w i⊤x) +O(d−1/2). (110)
k≥b+1 i=1 k≥b+1 i=1
Similarly,itfollowsthatE(cid:81)b h (w⊤x)=(cid:81)b Eh (w⊤x)+O(d−1/2)andfinally,usingperturbationanalysisagain,
i=1 i i i=1 i i
weconcludethat
(cid:34) b (cid:35) b
(cid:89) (cid:89)
E h (u⊤φ (W1x)) h (w⊤x) =Eh (u⊤φ (W1x)) Eh (w⊤x)+O(d−1/2) (111)
0 1 i i 0 1 i i
i=1 i=1
23B.2 Onelayerlinearization
Consider a mean-zero Gaussian random vector x ∈ Rd with covariance Exx⊤ = I, two weight matrices W ∈
Rk×d,V ∈Rk×dandtwosmoothoddfunctionsφ,ψappliedentrywisetoWx,Vx.WeassumethatrowsofW and
V satisfyAssumption4.1;inparticular,theyarei.i.d.mean-zerosamples(w ,v )∼(w,v).WedefineC :=C /k =
i i w 1
Eww⊤,C :=Cr /k =Evv⊤andC :=Cq /k =Ewv⊤.NotethatwealsohaveTrC ,TrC ,TrC ≲1.
v 1 wv 1 w v wv
LetN ,N bejointlyGaussianmean-zerorandomvariables,suchthat
w v
EN2 =TrC , EN2 =TrC , EN N =TrC . (112)
w w v v w v wv
Define
Φ =Eφ(Wx)ψ(Vx)⊤,
1 (113)
Φlin =(Eφ′(N ))(Eψ′(N ))WV⊤+[Eφ(N )ψ(N )−(Eφ′(N ))(Eψ′(N ))(EN N )]I.
1 w v w v w v w v
PropositionB.11. Wehavethat,withhighprobability,∥Φ −Φlin∥ =O(1).
1 1 F
Proof. UsingaWienerchaosexpansion(eq.(107)),wecanwrite
φ(Wx)
=(cid:88)Eφ(p)((WW⊤)1 ii/2N)
I (Wx) , ψ(Vx)
=(cid:88)Eψ(p)((VV⊤)1 jj/2N)
I (Vx) (114)
i p! p i j p! p j
p≥1 p≥1
whereN ∼N(0,1)andI (Wx),I (Vx)arerandomvectorswithcovariance
p q
EI (Wx)I (Vx)⊤ =p!δ (WV⊤)⊙p (115)
p q pq
withA⊙pdenotingthep-thentrywise(Hadamard)power.Thuswehavetheidentity
Eφ(Wx) ψ(Vx) =(cid:88) 1 (Eφ(p)((WW⊤)1/2N))(WV⊤)p(Eψ(p)((VV⊤)1/2N)). (116)
i j p! ii ij jj
p≥1
Fromconcentrationofquadraticformsassumptionforw,v,itfollowsthat
(WW⊤) =TrC +O(d−1/2), (VV⊤) =TrC +O(d−1/2), (117)
ii w jj v
(WV⊤) =δ TrC +O(d−1/2). (118)
ij ij wv
Fromperturbationanalysis,wecanwrite
Eφ(p)((WW⊤)1/2N)=Eφ(p)((cid:112)
TrC N)+O(d−1/2)=Eφ(p)(N )+O(d−1/2), (119)
ii w w
andsimilarlyEψ(p)((VV⊤)1/2N)=Eψ(p)(N )+O(d−1/2).
jj v
off-diagonalentries Here,forp≥2,wehavethat(WV⊤)p =O(d−p/2).Therefore,
ij
Eφ(Wx) ψ(Vx) =Eφ′(N )ψ′(N )(WV⊤) +O(d−1)=(Φlin) +O(d−1). (120)
i j w v ij 1 ij
diagonalentries Werewritetheinfinitesumas
Eφ(Wx) ψ(Vx) =(cid:88) 1 (Eφ(p)((WW⊤)1/2N))(WV⊤)p(Eψ(p)((VV⊤)1/2N))
i i p! ii ii ii
p≥1
√ √
=(cid:88)[Eφ(p)( TrC wN)][Eψ(p)( TrC vN)]
(TrC )p+O(d−1/2)
(121)
p! wv
p≥1
=Eφ(N )ψ(N )+O(d−1/2)=(Φlin) +O(d−1/2).
w v 1 ii
Summingupoverallentries,weconcludethat∥Φ −Φlin∥ =O(1).
1 1 F
NotethatincaseofindependentN ,N (i.e.,independentv,w)thesecondtermofΦlin vanishesandincaseof
v w 1
W =V,f ≡gthisreducesto
Φlin =(Eφ′(N ))2WW⊤+[Eφ(N )2−(Eφ′(N ))2TrC ]I. (122)
1 w w w w
24B.3 Twolayercase
Wenowconsiderthesimplest2-layerexample
φ (W2φ (W1x)), ψ(Vx) (123)
2 1
with general matrices W1,W2,V ∈ Rk×d and smooth odd functions φ ,φ ,ψ. We assume that rows of W1,W2
1 2
and V satisty Assumption 4.1; in particular, they are mean-zero i.i.d. samples (w1,w2,v ) ∼ (w1,w2,v). Define
i i i
C := C /k = Ew1(w1)⊤,C := C /k = Ew2(w2)⊤, C := Cr /k = Evv⊤ and C := Cq /k = Ew1v⊤.
1,w 1 2,w 2 v 1 wv 1
Define
Φ :=Eφ (W1x)ψ(Vx)⊤,
1 1
Φ :=Eφ (W2φ (W1x))ψ(Vx)⊤,
2 2 1
Φlin :=(Eφ′(N ))(Eψ′(N ))WV⊤+[Eφ (N )ψ(N )−(Eφ′(N ))(Eψ′(N ))(EN N )]I,
1 1 1,w 1,v 1 1,w 1,v 1 1,w 1,v 1,w 1,v
Φlin :=(Eφ′(N ))WΦlin,
2 2 2,w 1
Ω :=Eφ (W1x)φ (W1x)⊤,
1 1 1
Ωlin :=(Eφ′(N ))2W1(W1)⊤+[Eφ (N )2−(Eφ′(N ))2TrC ]I,
1 1 1,w 1 1,w 1 1,w 1
Ω :=Eφ (W2φ (W1x))φ (W2φ (W1x))⊤,
2 2 1 2 1
Ωlin :=(Eφ′(N ))2W2Ωlin(W2)⊤+[Eφ (N )2−(Eφ′(N ))2TrC ]I,
2 2 2,w 1 2 2,w 2 2,w 1
(124)
whereN ,N ,N arezero-meanjointlyGaussian.
1,w 2,w v
EN2 =Tr(C ) EN2 =Tr(C ) EN N =Tr(C ) EN2 =Tr(C Ωlin) (125)
1,w 1,w v v 1,w v wv 2,w 2,w 1
TheoremB.12. Wehavethat,withhighprobability,(i)∥Φ −Φlin∥ =O(1)and(ii)∥Ω −Ωlin∥ =O(1).
2 2 F 2 2 F
Wesplittheproofintofollowinglemmas:
LemmaB.13(DiagonalentriesofΩ ). ForrowvofmatrixW2,withhighprobability,
2
Eφ (v⊤φ (W1x))2 =Eφ (N )2+O(d−1/2). (126)
2 1 2 2,w
LemmaB.14(Off-diagonalentriesΩ ). Ifuandzareindependentrows,suchthatu ≲d−1/2andz ≲d−1/2,wehave
2 i i
withhighprobability,
Eφ (u⊤φ (Wx))ψ (z⊤ψ (Vx))
2 1 2 1 (127)
=Eφ′(u⊤φ (Wx))Eψ′(z⊤ψ (Vx))u⊤E(cid:2) φ (Wx)ψ (Vx)⊤(cid:3) z+O(d−1).
2 1 2 1 1 1
LemmaB.15(EntriesofΦ ). Forrowsu,vofmatricesW2,V respectively,withhighprobability
2
Eφ (u⊤φ (W1x))ψ(v⊤x)=E(cid:2) φ′(u⊤φ (W1x))]u⊤Eφ (W1x)ψ(v⊤x)+O(d−1). (128)
2 1 2 1 1
ProofofTheoremB.12. LemmaB.15impliesthat
∥Φ −E(cid:2) φ′(u⊤φ (Wx))]W2Φ ∥ =O(1). (129)
2 2 1 1 F
Notethat,sincefromPropositionB.11∥Φ −Φlin∥ =O(1)andsince∥W2∥=O(1),wehavethat
1 1 F
∥E(cid:2) φ′(u⊤φ (Wx))(cid:3) W2Φ −E(cid:2) φ′(u⊤φ (Wx))(cid:3) W2Φlin∥ =O(1), (130)
2 1 1 2 1 1 F
therefore,bytriangleinequality,
∥Φ −E(cid:2) φ′(u⊤φ (Wx))(cid:3) W2Φlin∥ =O(1). (131)
2 2 1 1 F
Finally,notethatbysimplechaosexpansion,E(cid:2) φ′(u⊤φ (Wx))(cid:3) =Eφ′(N ),therefore∥Φ −Φlin∥ =O(1).
2 1 2 2,w 2 2 F
Furthermore,LemmaB.13togetherwithLemmaB.14implythat
∥Ω −(E(cid:2) φ′(u⊤φ (Wx))(cid:3) )2W2Ω (W2)⊤−(Eφ (N )2−(E(cid:2) φ′(u⊤φ (Wx))(cid:3) )2)I∥ =O(1). (132)
2 2 1 1 2 2,w 2 1 F
Again,usingthefactthat∥Ω −Ωlin∥ =O(1),∥W2∥=O(1)andapproximatingEφ′(u⊤φ (Wx))withEφ′(N ),
1 1 F 2 1 2 2,w
weobtainthat∥Ω −Ωlin∥ =O(1).
2 2 F
25B.4 ProofofLemmaB.13
LetF =v⊤φ (W1x).OurgoalistocomputeEφ (v⊤φ (W1x))2 =Eφ (F)2.Forsimplicityweomitindicesinφ
1 2 1 2 1
andW1andwritejustφandW.Wecandecompose
F
=v⊤φ(Wx)=(cid:88)
I
(cid:18) EDpF(cid:19) =(cid:88)
I
(cid:32) (cid:88)v iEDpφ(w i⊤x)(cid:33) =(cid:88)
I
(cid:32) (cid:88)v iw i⊗pEφ(p)(w i⊤x)(cid:33)
. (133)
p p! p p! p p!
p p i p i
Letφp :=Eφ(p)(w⊤x).Weobtainthat
i i
(cid:32) (cid:33) (cid:32) (cid:33)
DF
=(cid:88)
pI
(cid:88)v iw i⊗pφp
i and −DL−1F
=(cid:88)
I
(cid:88)v iw i⊗qφq
i . (134)
p−1 p! q−1 q!
p≥1 i q≥1 i
LemmaB.16.
E|⟨DF,−DL−1F⟩−EF2|=O(d−1/2). (135)
Proof. Notethat
(cid:32) (cid:33)
I
(cid:88)v iw i⊗pφp
i
=(cid:88)v iφp iI p−1(w i⊗p−1)w
i, (136)
p−1 p! p!
i i
whichimpliesthat,forsomecoefficientsc ,
p,q
(cid:88) (cid:88)
⟨DF,−DL−1F⟩= c ⟨w ,w ⟩v v φpφqI (w⊗p−1)I (w⊗q−1). (137)
p,q i j i j i j p−1 i q−1 j
p,q≥1 i,j
Now,using([59],Theorem2.7.10),wecanrewrite
I (w⊗p−1)I (w⊗q−1)
p−1 i q−1 j
p∧q−1
= (cid:88) ⟨w ,w ⟩rc I (w⊗p−1−r⊗r w⊗q−1−r)
i j r,p,q p+q−2(r+1) i j
r=0 (138)
p+q−2
= (cid:88) c ⟨w ,w ⟩(p+q−2−s)/2I (cid:16) w⊗(p−q+s)⊗r w⊗(q−p+s)(cid:17) ,
r,p,q i j s i j
s=|p−q|
2divides(s−|p−q|)
andpluggingthisexpressionbackintoeq.(137),weget
⟨DF,−DL−1F⟩=(cid:88) (cid:88) c˜ (cid:88) ⟨w ,w ⟩(p+q−s)/2v v φpφqI (w⊗(s+p−q)/2⊗r w⊗(s+q−p)/2).
r,p,q i j i j i j s i j
s≥0 |p−q|≤s i,j
2divides(s−|p−q|)
p∧q≥1+(s−|p−q|)/2
(139)
Inthesumabove,terms=0correspondstoEF2.Letuscollectallthetermscorrespondingtosthmultipleintegral.
Notethatgivenconditionsons,p,q,wealwayshavethat(p+q−s)/2≥1,whichisthepoweroftheinnerproduct
⟨w ,w ⟩intheexpressionabove. Ifweintroducea:=(p+q−s)/2,thenforfixeds,sthmultipleintegralI canbe
i j s
rewrittenasfollows:
 
(cid:88)(cid:88)
I s ⟨w i,w j⟩av iv jT is j, (140)
a≥1 i,j
whereTs issomes-dimensionaltensor,whichisasumoftensorproductsofw andw ,alsocontainingcombinatorial
ij i j
terms,andproductsofexpectationsofderivativesoff.Then,wecanwrite
 2
(cid:88) (cid:88)(cid:88)
E(⟨DF,−DL−1F⟩−EF2)2 = EI s ⟨w i,w j⟩av iv jT is j . (141)
s≥1 a≥1 i,j
Observethat
 2
EI s(cid:88)(cid:88) ⟨w i,w j⟩av iv jT is j = (cid:88) (cid:88) ⟨w i,w j⟩a⟨w i′,w j′⟩a′ v iv jv i′v j′⟨T is j,T is ′j′⟩, (142)
a≥1 i,j a,a′≥1 i,j
i′,j′
26andnotethatforsomeconstantC (dependingoncombinatorialterms,andproductsofexpectationsofderivativesoff)
⟨Ts,Ts ⟩canbeupperboundedby
ij i′j′
⟨Ts,Ts ⟩≤C(⟨w ,w ⟩+⟨w ,w ⟩+⟨w ,w ⟩+⟨w ,w ⟩)s. (143)
ij i′j′ i i′ i j′ j i′ j i′
Now,weanalyzeeachtermofthesummandineq.(142)dependingons,a,a′,i,i′,j,j′.DefineN :=|{i,i′,j,j′}|,the
numberofdistinctindicesamongi,i′,j andj′.Notethatsinceentriesofvareoforderv ≲d−1/2,wegetthatintotal
i
v v v v contributeO(d−2).
i j i′ j′
CaseN =1 Here,sincethereareintotaldsuchterms,whichimmediatelyobtainO(d−1)upperbound.
CaseN =2 ThereareintotalO(d2)suchterms.Inthiscase,itmustbethateither(1)i̸=j or(2)i′ ̸=j′or(3)both
i=j,i′ =j′.Notethatinthefirsttwocases,wegetthat⟨w ,w ⟩a⟨w ,w ⟩a′ ≲d−1/2,whichtogetherwithboundon
i j i′ j′
v ’sgivesO(d2−2−1/2)contribution.Ifbothi=jandi′ =j′,thennecessarily⟨Ts,Ts ⟩=O(d−1/2)andwearriveat
i ij i′j′
thesameconclusion.
CaseN =3,min(a,a′,s)≥2 ThereareintotalO(d3)suchterms.WLOGassumethati=j.Sincemin(a,a′,s)≥2,
wegetthat⟨w ,w ⟩a′ =O(d−1)and⟨Ts,Ts ⟩=O(d−1),whichintotalgivesO(d−1)contribution.
i′ j′ ij i′j′
CaseN = 4 Here,thereareintotalO(d4)suchterms. Ifmin(a,a′) ≥ 2,thenittotalweobtainO(d4−2−2−1/2) =
O(d−1/2)contribution.Westartwiththecasea=a′ =1.Let
(cid:88)
X = ⟨w ,w ⟩⟨w ,w ⟩v v v v ⟨T1,T1 ⟩ with E X =0,
i j i′ j′ i j i′ j′ ij i′j′ W
i,j,i′,j′
alldistinct
(cid:88) (cid:88)
X2 = ⟨w ,w ⟩⟨w ,w ⟩⟨w ,w ⟩⟨w ,w ⟩v v v v v v v v ⟨Ts ,Ts ⟩⟨Ts ,Ts ⟩.
i1 j1 i′ 1 j 1′ i2 j2 i′ 2 j 2′ i1 j1 i′ 1 j 1′ i2 j2 i′ 2 j 2′ i1j1 i′ 1j 1′ i2j2 i′ 2j 2′
i a1 l, lj d1 i, si t′ 1in, cj t1′i a2 l, lj d2 i, si t′ 2in, cj t2′
(144)
Sincew isindependentfromw fora̸=b,andallrowsaremean-zero,weconcludethattheonlynon-zerocontribution
a b
comesfromtermswithpairingsbetweenindices.Therefore,wegetthatE X2 =O(d−3),whichimpliesthat,with
W
highprobability,X =O(d−1/2).
When a = 1 and a′ = 2, by similar computation one obtains that E X2 = O(d−2), and therefore, with high
W
probability,X =O(d−1/2).
CaseN =3,min(a,a′,s)=1 Thiscasecanbedonesimilarlytothepreviousones.
Overall,weobtainthat
E(⟨DF,−DL−1F⟩−EF2)2 =O(d−1/2), (145)
which,byTheoremB.9,usingh(x)=φ (x)2that
2
Eφ (u⊤φ (W1x))2 =Eφ (Z)2+O(d−1/2), (146)
2 1 2
whereEZ2 =u⊤Eφ (W1x)Eφ (W1x)⊤u.Byperturbationanalysis,weobtainthat
1 1
Eφ (u⊤φ (W1x))2 =Eφ (N )2+O(d−1/2) (147)
2 1 2 2,w
B.5 ProofofLemmaB.15
Werestatethelemma:
LemmaB.17.
Eφ (u⊤φ (Wx))ψ(v⊤x)=E(cid:2) φ′(u⊤φ (Wx))]u⊤Eφ (Wx)ψ(v⊤x)+O(d−1) (148)
2 1 2 1 1
27Ourgoalistocompute
Eφ (u⊤φ (Wx))ψ(v⊤x). (149)
2 1
LetF =φ (w⊤x)andF =(F ,...,F ),wherew ’saretherowsofW.Notethatwecanviewφ (u⊤φ (Wx))=ϕ(F),
i 1 i 1 n i 2 1
forϕ(F)=φ ((cid:80) u F ).RecallthatDpF =φ(p)(w⊤x)w⊗p.Then,wehavethat
2 k k k k k
(cid:88)
Dϕ(F)= φ′(u⊤φ (Wx))u DF . (150)
2 1 k k
k
FromLemmaB.10,itfollowsthat
EDϕ(F)=(cid:88) E(cid:2) φ′(u⊤φ (Wx))(cid:3) u EDF +O(d−1/2), (151)
2 1 k k
k
andweobtainthat
⟨EDψ(v⊤x),EDφ (u⊤φ (Wx))⟩=E(cid:2) φ′(u⊤φ (Wx))(cid:3)(cid:88) u ⟨EDψ(v⊤x),EDφ (w⊤x)⟩. (152)
2 1 2 1 k 1 i
k
Forthesecondderivative,weobtainthat
(cid:88) (cid:88)
D2ϕ(F)= φ′′(u⊤φ (Wx))u u′(DF )⊗(DF )+ φ′(u⊤φ (Wx))u D2F . (153)
2 1 k k k k′ 2 1 k k
k,k′ k
Overall,forEDpϕ(F),weobtain
EDpϕ(F)=(cid:88) E(cid:2) φ′ 2(u⊤φ 1(Wx))u kDpF k(cid:3) +ER (154)
k
whereRisthetermcontaininghigherderivativesofφ .Thisimpliesthat
2
(cid:88) (cid:104) (cid:105) (cid:104) (cid:105)
⟨EDpϕ(F),EDpψ(v⊤x)⟩= u E f′(u⊤φ (Wx))φ(p)(w⊤x) E ψ(p)(v⊤x) ⟨w ,v⟩p+⟨ER,EDpψ(v⊤x)⟩.
k 2 1 k k
k
(155)
FromLemmaB.10,wehavethat
E(cid:104) φ′(u⊤φ (Wx))φ(p)(w⊤x)(cid:105) =E(cid:2) φ′(u⊤φ (Wx))(cid:3) E(cid:104) φ(p)(w⊤x)(cid:105) +Q, (156)
2 1 k 2 1 k
whereQ=O(d−1/2).WenowshowthattermsinvolvingQhaveO(d−1)contributiontothefinalexpression:
LemmaB.18.
(cid:88)
u ⟨w ,v⟩p =O(d−1/2) (157)
k k
k
Proof. SincevcanbecorrelatedwithatmostonerowofW byourassumptions,WLOGweassumethatvisuncorrelated
withallw fork ≥2.Thenwecanwrite
k
(cid:88) (cid:88)
u ⟨w ,v⟩p =u ⟨w ,v⟩p+ u ⟨w ,v⟩p, (158)
k k 1 1 k k
k k≥2
wherethefirsttermisO(d−1/2)sinceu =O(d−1/2).Next,ifp≥2,wedirectlyobtainthatthesecondtermisO(d−1/2).
i
Forcasep=1,notesince(cid:80) u ⟨w ,v⟩isthesumofindependentmean-zerorandomvariables(intheW space),we
k≥2 k k
obtainthat(cid:80) u ⟨w ,v⟩=O(d−1/2).Altogether,wegetthat(cid:80) u ⟨w ,v⟩p =O(d−1/2)withhighprobability.
k k k k k k
LemmaB.18impliesthat
⟨EDpϕ(F),EDpψ(v⊤x)⟩
=(cid:88) u E(cid:2) φ′(u⊤φ (Wx))(cid:3) E(cid:104) φ(p)(w⊤x)(cid:105) E(cid:104) ψ(p)(v⊤x)(cid:105) ⟨w ,v⟩p+⟨ER,EDpψ(v⊤x)⟩+O(d−1)
k 2 1 k k (159)
k
=E(cid:2) φ′(u⊤φ (Wx))(cid:3)(cid:88) u ⟨EDpF ,EDpψ(v⊤x)⟩+⟨ER,EDpψ(v⊤x)⟩+O(d−1).
2 1 k k
k
28LemmaB.19.
⟨ER,EDpψ(v⊤x)⟩=O(d−1). (160)
Proof. Notethatwecanrewrite
|π|
(cid:88) (cid:88) (cid:89)(cid:16) (cid:17)
⟨ER,EDpψ(v⊤x)⟩= u ⟨v,w ⟩b(q) , (161)
iq iq
π⊢[p] i1,...,i|π|q=1
π̸={[p]}
whereforπ (partitionof[p])wedenoteb(q)asthesizeofqthblock. Letπ ̸= {[p]}besomepartition,s := |π|,and
letA :=(cid:80) (cid:81)s (cid:0) u ⟨v,w ⟩b(q)(cid:1).Ifallblocksofπareofsizeatleast2,thennaiveestimategivesintotalthe
π i1,...,is q=1 iq iq
contributionisO(d−1).Nowassumethatthereisablockofsize1.WLOGweassumethatviscorrelatedwithw and
1
b(1)=1.Ifi =1,thenletrbethenumberofindicesamongs−1remainingindices,suchthatcorrespondingindexis
1
summedoveri ≥2.Ifr ≤s−2,thenwearriveattheestimateO(dr−s/2−r/2)=O(d−1).Now,assumethatr =s−1.
k
Iftwoindicescoincide,thenwearriveattheestimateO(d(s−2)−s/2−(s−1)/2)=O(d−1).Therefore,theremainingcase
is
s
(cid:88) (cid:89)
B := u ⟨v,w ⟩ u ⟨v,w ⟩b(q). (162)
π 1 1 iq iq
i2̸=i3̸=...̸=is̸=1 q=2
If exists q > 1, such that b(q) ≥ 1, then from naive estimate we obtain bound O(ds−1−s/2−(s−2)/2−1) = O(d−1).
Therefore,wecanassumethatallblocksofπareofsize1.Byindependenceofrows,theW-expectationofB iszeroand
π
p
(cid:88) (cid:88) (cid:89)
E B2 = u2⟨v,w ⟩2 u u ⟨v,w ⟩⟨v,w ⟩. (163)
W π 1 1 iq i′ q iq i′ q
i2̸=i3̸=...̸=i|π|̸=1i′ 2̸=i′ 3̸=...̸=i′ |π|̸=1 q=2
Theonlynon-zerocontributionscomefrompairingsbetweeni,i′, whichcontributeO(d(p−1)−p−(p−1)) = O(d−1).
Thecasei ̸= 1followsbysimilarcalculations. Overall,weprovedthat⟨ER,EDpψ(v⊤x)⟩ = (cid:80) O(d−1) =
1 π̸={[p]}
O(d−1).
UsingLemmaB.19,wehavethat⟨EDpϕ(F),EDpψ(v⊤x)⟩=E(cid:2) φ′(u⊤φ (Wx))(cid:3)(cid:80) u ⟨EDpF ,EDpψ(v⊤x)⟩+
2 1 k k k
O(d−1),andthisimpliesthat
Eφ (u⊤φ (Wx))ψ(v⊤x)
2 1
(cid:88) 1
= ⟨EDpφ (u⊤φ (Wx)),EDpψ(v⊤x)⟩
p! 2 1
p≥1
=(cid:88) 1 (cid:88) u E(cid:2) φ′(u⊤φ (Wx))]⟨EDpφ (w⊤x),EDpψ(v⊤x)⟩+O(d−1) (164)
p! k 2 1 1 k
p≥1 k
=E(cid:2) φ′(u⊤φ (Wx))](cid:88) u Eφ (w⊤x)ψ(v⊤x)+O(d−1)
2 1 k 1 k
k
=E(cid:2) φ′(u⊤φ (Wx))]u⊤Eφ (Wx)ψ(v⊤x)+O(d−1).
2 1 1
Therefore,weobtainthatEφ (u⊤φ (Wx))ψ(v⊤x)=E(cid:2) φ′(u⊤φ (Wx))]u⊤Eφ (Wx)ψ(v⊤x)+O(d−1)
2 1 2 1 1
B.6 ProofofLemmaB.14
Forconvenience,werestatethelemma.
LemmaB.20. Ifuandzareindependent,wehave
Eφ (u⊤φ (Wx))ψ (z⊤ψ (Vx))=Eφ′(u⊤φ (Wx))Eψ′(z⊤ψ (Vx))u⊤E(cid:2) φ (Wx)ψ (Vx)⊤(cid:3) z+O(d−1)
2 1 2 1 2 1 2 1 1 1
(165)
Here,ourgoalistocomputeEφ (u⊤φ (Wx))ψ (z⊤ψ (Vx))withassumptionthatuandzareindependent.Since
2 1 2 1
EDpφ (u⊤φ (Wx))=(cid:80) u w⊗pE(cid:2) φ′(u⊤φ (Wx))φ (w⊤x)(cid:3),wecanwrite
2 1 k k k 2 1 1 k
⟨EDpφ (u⊤φ (Wx)),EDpψ (z⊤ψ (Vx))⟩
2 1 2 1
=(cid:88) ⟨w ,v ⟩pu z E(cid:2) φ′(u⊤φ (Wx))φ (w⊤x)(cid:3) E(cid:2) ψ′(z⊤ψ (Vx))ψ (v⊤x)(cid:3) +R, (166)
k j k j 2 1 1 k 2 1 1 j
k,j
29whereRisthetermcontaininghigherderivativesofφ ,ψ .Bycomputationssimilartopreviouscase,wecanshowthat
2 2
R=O(d−1).Also,byweakcorrelationbetweenthelayers(alsoseepreviouscase),wecanshowthat
⟨EDpφ (u⊤φ (Wx)),EDpψ (z⊤ψ (Vx))⟩
2 1 2 1
=(cid:88)
⟨w ,v ⟩pu z
E(cid:104)
φ′(u⊤φ
(Wx))φ(p)(w⊤x)(cid:105) E(cid:104)
ψ′(z⊤ψ
(Vx))ψ(p)(v⊤x)(cid:105)
+O(d−1)
k j k j 2 1 1 k 2 1 1 j
(167)
k,j
=(cid:88) ⟨w ,v ⟩pu z E(cid:2) φ′(u⊤φ (Wx))(cid:3) E(cid:104) φ(p)(w⊤x)(cid:105) E(cid:2) ψ′(z⊤ψ (Vx))(cid:3) E(cid:104) ψ(p)(v⊤x)(cid:105) +O(d−1)
k j k j 2 1 1 k 2 1 1 j
k,j
Afterrevertingchaosexpansion,weobtainthat
Eφ (u⊤φ (Wx))ψ (z⊤ψ (Vx))=Eφ′(u⊤φ (Wx))Eψ′(z⊤ψ (Vx))u⊤E(cid:2) φ (Wx)ψ (Vx)⊤(cid:3) z+O(d−1).
2 1 2 1 2 1 2 1 1 1
(168)
C Details on numerics
WeprovideinthisAppendixmoredetailsontheexperimentsreportedinFig.1andFig.2.
C.1 DetailsofFig.1
Target Weconsideratwo-layerstructuredRFteacher,withfeaturemap
φ (x)=tanh(W x) (169)
∗ ∗
wheretheweightW =Z C˜ 21 ∈Rd×dhascovariance
∗ ∗ 1
C˜ =diag({k−0.3} ). (170)
1 1≤k≤d
Student Weconsiderthetaskoflearningthistargetwithafour-layerRFstudent,withfeaturemap
φ(x)=tanhW (tanh(W tanh(W x))) (171)
3 2 1
where,inordertointroduceinter-layerandtarget/studentweightcorrelations,weconsideredW =W ,with
2 1
W
1
=1/2Z 1diag({k−γ/2} 1≤k≤d)+1/2W ∗, (172)
forγ ∈{0.0,0.2,0.5,0.8}.Inotherwords,thecovarianceC ofW ,W isasumoftwopowerlawswithdecayγ and
1 1 2
0−3.Finally,inordertointroduceanotherformofcorrelation,wechose
W =Z C1/2 (173)
3 3 3
wherethecovarianceC dependsonthepreviousweightsas
3
C
3
=(W 1W 1⊤+1/2I d)−1. (174)
C.2 DetailsonFig.2
InFig.2,weconsiderthetaskoflearningatargetcorrespondingtoastructuredthree-layerRF
φ (x)=θ⊤tanh(W∗sign(W∗x)), (175)
∗ ∗ 2 1
wheretheweightsW∗,W∗ ∈Rd×dhavecovariance
1 2
C =C =I +8vv⊤, (176)
1 2 d
Finally,thereadoutθ ∗hasi.i.dvariance1/dGaussianentries.Tolearnthistarget,weconsidertrainingthethree-layer
feed-forwardneuralnetwork
θ⊤tanh(W tanh(W x), (177)
2 1
withtrainableweightsW ,W ∈Rd×d.Weconsiderthetwo-steptrainingprocedure
1 2
301. Wefirsttrainthewholenetworkend-to-end,usingthPytorch[65]implementationoffull-batchAdam[61],
forT =3000epochs,withlearningrateη =10−3 andweightdecay10−5. ThetrainingisdoneforN =1400
trainingsamples,withfor1 ≤ µ ≤ ninputsxµ ∼ N(0,I ),andthecorrespondinglabelsθ⊤φ (xµ). Allthese
d ∗ ∗
experimentsweresetindimensiond=1000.
2. Attheendofthetraining,onefreezestheweightmatricestotheirtrainedvaluesWˆ ,Wˆ ,therebyobtainingthe
1 2
trainednetworkfeaturemap
φˆ(x)=tanh(Wˆ tanh(Wˆ x). (178)
2 1
Thereadoutweightsθ arethenre-trainedalone,accordingtotheERMdescribeinthemaintext,withafresh
trainingsetofn=αdsamples.Theridgeregularizationstrengthλisoptimizedoverusingcross-validationinthis
secondstep.
ThegeneralizationofthestudentnetworkattheendofthesetwostepsyieldsthebluecurveinFig.2.Weplotalongsidethe
performanceofthenetworkwithuntrainedfirstweights(green),i.e.whenthefirststepisskipped.Inthiscase,thefeature
mapinvolvestheweightsatinitialization,namelyGaussianrandommatrices,andcorrespondstoanunstructureddRF
[12,13].Finally,theredcurvecorrespondstotheperformanceofridgeregressiondirectlyontheinputsx,unprocessed
byanyfeaturemap.
31