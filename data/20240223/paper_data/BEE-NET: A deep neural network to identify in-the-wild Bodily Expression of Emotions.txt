1
BEE-NET: A deep neural network to identify
in-the-wild Bodily Expression of Emotions
Mohammad Mahdi Dehshibi Member, IEEE and David Masip Senior Member, IEEE
Abstract—Inthisstudy,weinvestigatehowenvironmentalfactors,specificallythescenesandobjectsinvolved,canaffecttheexpression
ofemotionsthroughbodylanguage.Tothisend,weintroduceanovelmulti-streamdeepconvolutionalneuralnetworknamedBEE-
NET. We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in
the learning process. Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution
ofbothavailableandanticipatednon-availablecontextualinformationinlatentspace.Importantly,ourfusionstrategyisdifferentiable,
allowingforend-to-endtrainingandcapturingofhiddenassociationsamongdatapointswithoutrequiringfurtherpost-processingor
regularisation. To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available
databasefortheAutomaticIdentificationofthein-the-wildBodilyExpressionofEmotions(AIBEE).Ourexperimentalresultsdemonstrate
thatourproposedapproachsurpassesthecurrentstate-of-the-artinAIBEEbyamarginof2.07%,achievinganEmotionalRecognition
Scoreof66.33%.
IndexTerms—End-to-end,Deeplearning,Bodylanguage,Emotionrecognition
✦
1 INTRODUCTION
H UMANS recognise and perceive the emotional expressions non-available places/objects. This scalable architecture is differ-
of others to use these critical cues for successful nonverbal entiable, allowing for end-to-end model training without addi-
communication. Equipping computers with the ability to recog- tionalpost-processingorregularisation.Weevaluatetheproposed
nise,perceive,process,andsimulatehumanaffectswillaidinthe method using the Body Language database (BoLD) [24], which
developmentofempatheticdevicesthatcanbeusedinmonitoring is by far the largest database available for AIBEE. Experimental
certain mental/physical disorders [3, 10], home-assistant devices, resultsindicatethattheproposedmethodoutperformsthestate-of-
publicsafety,ortheanalysisofsocialmediadata[8,13].Emotion the-art in identifying discrete and continuous in-the-wild bodily
perception has typically been studied by analysing facial expres- expressionsofemotions.
sions[29],bodyposturesandgestures[24,28],andphysiological The rest of this paper is organised as follows: Section 2
signs [2, 9, 11]. However, psychological studies have shown that surveys the previous studies. The proposed method is detailed
body language can convey individuals’ emotional state [1, 4], in Section 3. Architectural design and implementation details are
and the lack of a high-quality and diverse database with ground giveninSection4.Section5presentsexperimentsresults.Finally,
truth makes Automatic Identification of the in-the-wild Bodily Section6concludesthepaper.
ExpressionofEmotions(AIBEE)achallengingresearchtopic.
Several studies have assessed the variables that humans nat-
2 LITERATURE REVIEW
urally use from a young age to provide a more reliable measure
of an individual’s emotional state by fusing different modalities Detecting actions in videos is a critical step toward understand-
such as facial expressions, speech prosody, and context-related ing human behaviour. In this context, high-level reasoning is
data [20, 30]. Body language has recently been used to help required not only in the spatial dimension but also in the tem-
understand other people’s emotional states. According to Beck’s poral dimension. Simonyan and Zisserman [34] proposed one
cognitivedepressiontheory[5],peoplesufferingfromdepression of the most promising multi-task convolutional neural network
tendtoviewthemselvesinmostlynegativeanddarkenvironments. (CNN)architecturesforactionrecognition,integratingspatialand
Thisintuitionmotivatedustoproposeadeeplearningarchitecture temporal networks. Tran et al. [37] proposed using 3D CNNs
thatincorporatesmeta-informationabouttheenvironmentandthe for general action recognition with the separate convolution of
objectsinvolvedtoreinforceAIBEE. spatialandtemporalfilters,whichincreasedrecognitionaccuracy
Wehypothesisethattheenvironmentandtheobjectsinvolved in AVA benchmark [17]. Feichtenhofer et al. [14] suggested a
greatly influence our perception of the in-the-wild bodily expres- two-pathway network with a low frame rate path focused on
sions of emotions. Therefore, rather than isolating human bodies spatial information extraction and a high frame rate path focused
in video frames, we propose a multi-stream convolutional neural on motion encoding. Hussein et al. [18] developed a multi-scale
network (BEE-NET) that incorporates prior knowledge about the temporal convolution approach, employing various kernel sizes
joint probability of emotions and both available and anticipated anddilationratestocapturetemporaldependencies.
Ulutanetal.[38]proposedcombiningactorfeatureswitheach
Spatio-temporal region in the scene to generate attention maps
• Correspondingauthor:M.M.DehshibiisaffiliatedwiththeDepartmentof
ComputerScienceandEngineeringatUniversidadCarlosIIIdeMadrid between the actor and the context. Girdhar et al. [16] suggested
inLegane´s,Spain(e-mail:mohammad.dehshibi@yahoo.com). a Transformer-style architecture for weighting actors based on
Manuscript#TPAMIreceivedXYZ;revisedXYZ. contextual features. Wang and Gupta [41] suggested modelling
4202
beF
12
]VC.sc[
1v55931.2042:viXra2
avideocliptocombinewholeclipfeaturesandweightedproposal (see Fig. 1(a)). We assume that each image consists of three
featurescomputedbyagraphconvolutionalnetworkbasedonfea- components,namelyemotion,placeandobject.Emotiontagsare
turespacesimilaritiesandSpatio-temporaldistancesbetweeneach provided in D. To incorporate contextual information (place and
detection.Tomeietal.[36]proposeagraph-basedframeworkfor object)thatwasnotexplicitlyprovidedintheBoLDdatabase,we
learning high-level interactions between people and objects. The used the transfer learning strategy to incorporate pseudo-ground
Spatio-temporalrelationshipsarelearnedthroughself-attentionon truths into the learning model. Figures 1(b) and 1(c) show the
amulti-layergraphstructuretolinkentitiesfromconsecutiveclips provided place and object tags for the input image using the
forawiderangeofSpatio-temporaldependencies. Places-CNNscenedescriptorandtheYOLOobjectdetector[31],
Human action recognition in-the-wild has to deal with chal- which trained on the Places2 [45] and Microsoft COCO [23]
lenges such as different degrees of freedom, heterogeneity in databases,respectively.
people’sbehaviour,clutteredbackgrounds,andvariationsinsize,
pose,andcameraviewpoint[28,43].Furthermore,existingbench-
markdatabases,suchasAVA[17],lackedtagsforhumanaffects.
Therefore, several studies have focused on facial expressions
rather than body gestures to identify emotions that are less
subjective thanks to the introduction of the Facial Action Coding
System [12] and more flexible as a result of the face having
fewer degrees of freedom than the body [32]. Mollahosseini et
al. [27] proposed a six-layer CNN with two convolution layers
and four inception layers to classify facial expressions in still
images.PonsandMasip[29]proposedaCNNcommitteeinwhich
amulti-tasklearninglossfunctionintegratesthedetectoroffacial
actionunitsintotheemotionlearningmodeltosolvetheproblem
of learning multiple tasks with heterogeneously labelled data.
MicroexpressionswereconsideredbyXuetal.[42]toaddnuances
tofacialexpressionrecognition.Luvizonetal.[25]usedamulti-
task learning approach, and Li et al. [22] used a Spatio-temporal
graphCNNtoleverageposeknowledgeandimprovetheaccuracy (a)
ofbodilyexpressionrecognitionacrossmultiplemodalities.
Giventhatbodilyexpressionscanconveyemotionalstatesand,
insomecases,aretheonlymodalitythatcanbeusedtocorrectly
disambiguatethecorrespondingfacialexpression[1,4,8],recent
studies attempted to incorporate bodily expressions of emotions
into affective computing by introducing benchmark models and
databases such as the EMOTIC [20] and BoLD [24]. Kosti
(b) (c)
et al. [20] introduced the EMOTIC database, which included
information about valence, arousal, and dominance dimensions
Fig. 1. (a) A sample from the BoLD database that mainly represents
in addition to the six basic emotions. They used a two-stream happiness. (b) Top-10 place tags were obtained by applying Places-
CNN to extract features that represented the body expression CNNtrainedonthePlaces2databasetotheinput.(c)Objecttagswere
obtainedbyapplyingYOLOtrainedontheMicrosoftCOCOdatabaseto
and the scene description. Luo et al. [24] introduced the BoLD
theinput.
database and proposed a framework combining human identifi-
cation, pose estimation, and learning representation to recognise
bodily expressions of emotions. Kumar et al. [21] propose using
Mensinketal.[26]reportedthattheco-occurrenceofattributes
the BoLD database to train a noisy student network in which
in a model’s training phase could occur with high probability in
various face regions are processed independently using a multi-
thetestingphase.Integratingattributeco-occurrencealsohelpsto
levelattentionmechanismtoenhancefacialexpressionrecognition
diminishunwantedoutliersintroducedbydomainintegrationand
incrementally.
makes L a good approximation for the loss function [15, 44].
2
However,fine-tuningthepre-trainedmodelswithadatabasewhere
3 BEE-NET MULTI-STREAM ARCHITECTURE attributelabelsarenotexplicitlyprovidedisnotpossible.
We formulate the AIBEE as a regression problem in which the To address this problem, we propose a multi-stream convolu-
response Y ∈ Rdy is predicted given the input image X ∈ Rdx, tionalneuralnetwork(BEE-NET)inwhichthepoolinglayerand
andthelossismeasuredbythemeansquarederror(L ).Givena loss function drive the emotion learning process during training
2
databaseofnimagesD ={(x i,y i)}n i=1,withx
i
∈Rdx andy
i
∈ using a priori contextual information about the joint probability
Rdy,ourgoalistolearnaneuralnetworkH(x) = E[Y|X = x] of emotions and both available and anticipated non-available
thatminimisesthelossfunction. places/objects. We formulated a derivable pooling scheme based
In this study, we use BoLD database [24] in which the input on Bayesian theory to fuse the extracted uncertain information
imageislabelledbyd =29emotions.Wescaletheinputimage with the predicted image-based emotional states, allowing end-
y
X to the size of d = 224×224. The emotion tag Y consists to-end model training to capture the hidden correlation between
x
of 26 discrete emotions with values in the range of [0,1] and datawithoutadditionalpost-processingorregularisation.Fig.2(a)
three continuous emotions with values in the range of [1,10] illustratestheproposedarchitecture.3
(a) (b)
Fig.2.(a)BEE-NETarchitecturefortheidentificationofin-the-wildbodilyexpressionofemotions.PlaceandObjectstreamshaveashadeofgrey
inthisschematicpipelinetohighlightfrozenlayersatazerolearningrateduringthetrainingphase.(b)Thepseudo-colourplotoftheconditional
probability of the emotion tags given the place and object tags. The y-axis represents the probability of pseudo-ground-truth for the place and
objects(κ=365+80),whilethex-axisrepresents26discreteemotions.Notethatthedarkertheblue,thelowertheprobabilityvalues.
3.1 Networkarchitecture represents the probability of object categories. With the same
purpose as in the scene descriptor stream, we convolve zobject
The proposed architecture is composed of three main streams:
withatrainablefilterbankfobject ∈ R1×1×80×κ andadditwith
(i) the scene descriptor stream, which determines the probability
biasbobject ∈Rκ asinEq.2.
of place categories associated with the input image; (ii) the
objectdetectorstream,whichdetectsobjectsinvolvedintheinput yobject =zobject⊛fobject+bobject. (2)
image; and (iii) the emotion stream, which focuses on learning
the map between the input image and the emotions. The initial (iii) Emotion stream (Hemotion): This stream learns a re-
network stem (Hbase) is a feature encoder implemented with gression model that maps the output of the initial network stem
GoogLeNet [35] where the output of the Inception (4d) module F into emotion (yemotion ∈ R1×1×dy). This stream is mainly
(F ∈ RW×H×D) is fed into the rest of streams. W = 14, composed of the Inception (4e), (5a) and (5b), global average
H = 14 and D = 528 are the width, height, and number of pooling, dropout, and fully connected modules. Inspired by [19],
channelsofthefeaturemap,respectively. we formulate the regression task with a softmax likelihood in
(i)Scenedescriptor(Hplace):resemblesthearchitectureofthe Eq.3.Notethathereweintentionallydroptheemotionsuperscript
Places-CNNwiththeGoogLeNetbackbonetoprovidethepseudo- tosimplifymathematicalnotations.
ground-truthforplacetags.ThelayersincludetheInception(4e),
(cid:18) 1 (cid:19)
(5a) and (5b), global average pooling, dropout, fully connected p(y|H(F),σ)=softmax H(F) . (3)
and softmax modules. More formally, Hplace : F → zplace, σ2
wherezplace ∈R1×1×365 isavectorrepresentingtheprobability where σ is a learnable noise scalar that observes the uniformity
ofplacecategories.Toincorporatezplace intotheproposedprob- of a discrete distribution. In the maximum likelihood inference,
abilisticmodelandharmonisethedimensionalityofeachstream, we maximise the log-likelihood of the model. To obtain the log-
weconvolvezplacewithafilterbankandaddthebiasusingEq.1.
likelihoodofthemodel(Eq.3),wecanwritethecross-entropyloss
asEq.4,whereH (F)isthejth elementofthevectorH(F).
j
yplace =zplace⊛fplace+bplace. (1)
where ⊛ performs the convolution, κ is the number of output logp(y =i|H(F),σ) =
dimension, fplace ∈ R1×1×365×κ is the trainable filter, and
1≤i≤dy
bplace ∈Rκ isthebiasterm. 1 (cid:88)dy (cid:18) 1 (cid:19)
H (F)−log exp H (F) . (4)
(ii)Objectdetector(Hobject):providesthepseudo-ground-truth σ2 i σ2 j
j=1
for object tags. This stream is composed of three groups of
serially connected convolution, ReLU, and batch normalisation In the formulation of the cross-entropy loss, we assumed that
layers,inwhichtheentiretopmostfeaturemapisusedtopredict (cid:80)dy exp(cid:0) 1 H (F)(cid:1) ≈ (cid:16) (cid:80)dy exp(H (F))(cid:17) σ1 2 to simplify
j=1 σ2 j j=1 j
confidencesformultiplecategoriesofobjectsatasinglestage.In
the optimisation objective. This approximation becomes equality
Inception(4e),thefiltersizeissetto14×14tomatchthenumber
whenσ (cid:55)−→1.Thislossfunctionisdifferentiableandavoidsany
of channels in F. The second (5a) and third (5b) convolution
divisionbyzero.Italsopreventstheweightsoftheemotionstream
layers have the filter size of 7×7 to enable the model to detect
from converging to zero when the co-occurrence probability of
small objects. These layers are followed by the transform and
availableandnon-availablemeta-informationisincorporatedinto
output layers, respectively. The transform layer transforms raw
thearchitecture.
CNN output into the form required for object detections and is
followedbytheoutputlayer,whichdefinesandimplementsthe7
3.2 ProbabilisticPoolingforLateFusioninBEE-NET
anchorboxesandlossfunctionusedtotrainthedetector.Anchor
boxes extract the activations of the last convolutional layer and Due to the association between emotions and the diversity of
match predicted bounding boxes with the ground truth. More objects/places,itisnoteasytoaccuratelyestimatetheconditional
formally, Hobject : F → zobject, where zobject ∈ R1×1×80 probabilitiesofagivenimagexwithregardtoallattributelabels4
at the same time. To address this issue, we propose a multi-
stream architecture (BEE-NET), which includes place (yplace) Pr(B ∩¬A )
and object (yobject) auxiliary streams in addition to the emotion P i− ,j =Pr(B i|¬A j)= Pri (¬A )j
stream (yemotion). As shown in Fig. 2(a), the lower layers (F) j (9)
Pr(B )−(B ∩A ) p −C
are shared across streams, while the top layers are separated to = i i j = i i,j
1−Pr(A ) 1−p
focus on the attributes for different contextual information. To j j
fusetheextracteduncertaininformationfromtheplaceandobject We then use P+ and P− matrices to calculate the joint
streamswiththepredictedimage-basedemotionalstates,webuild
probability
Pˆ
as in Eq. 10, which is the output of the proposed
amatrix,asgiveninEq.5,toincorporatepriorknowledgeabout
poolingscheme.
thejointprobabilityofemotionsandplaces/objects,whichcanbe
considered of as softmax classifier outputs stacked into a 2×d y Pˆ =QP++(1−Q)P−. (10)
matrix P. Note that we substituted yplace, yobject, and yemotion
withwithA 1,A 2 andB,tosimplifymathematicalnotations. where 1 ∈ R1×dy is a full one vector. By adding the proposed
(cid:20) (cid:21) pooling scheme to the rest of the architecture, the assembled
Pr(B |A ) Pr(B |A ) ··· Pr(B |A )
P = 1 1 2 1 dy 1 . (5) network H can predict bodily expressions of emotion y˜ given
Pr(B |A ) Pr(B |A ) ··· Pr(B |A )
1 2 2 2 dy 2
theinputimagexbyEq.11.
where Pr(B |A ) is the conditional probability of B given the
i j i
featureofthejth, j ∈ {1,2}stream.TocalculatePr(B i|A j)in y˜=H(x)= 1 Pˆ. (11)
thetrainingset,wemustfirstdeterminethenumberofoccurrences λ
ofeachemotionlabelaswellasthenumberofco-occurrencesof where λ is used to regulate the prediction results of the streams.
this emotion label given place/object pseudo-labels. If N i is the In order to minimise the loss function (ℓ) in Eq. 12, we use the
number of occurrences of the ith emotion in the dataset, then stochastic gradient descent algorithm combined with the back-
p i = Pr(B i) = N ni is the probability of the ith label. Likewise, propagationstrategytoupdatethenetworkparameters.
if N is the co-occurrence number of the dataset’s ith and jth
i,j
labels, C i,j = Pr(B i ∩ A j) = N ni,j is a matrix, representing ℓ= 1 (cid:88)n ||y −y˜||2. (12)
the joint probability of B and A . Therefore, we can obtain the n i i
i j
i=1
conditionalprobabilityofoneattributegivenanother,asshownin
Eq. 6. Fig. 2(b) shows the pseudo-colour plot of P+ discovered
4 ARCHITECTURE DETAILS
ontheBoLDtrainingset,wherethedarkertheblue,thelowerthe
probabilityvalues. Experiments were performed on the NVIDIA RTX 2080 GPU
with 8 GB of memory in MATLAB 2020a using the Deep
Pr(B ∩A ) C
P+ =Pr(B |A )= i j = i,j. (6) Learning toolbox. The parameters for the proposed architecture
i,j i j Pr(A j) p j areasfollows:
wherep isahigh-orderposteriorprobabilityexpressingacorre- – Scene descriptor stream: In (Hplace), we set weights and
j
lation between the place (A ) and object (A ) streams given the parameters of layers to the identical ones in Places-CNN, which
1 2
ith emotion,ascalculatedbyEq.7. was trained with Places2 [45] database. To prevent layers of this
stream from being overfitted during the training of Hemotion
Pr(cid:0) Ai (cid:1) =Pr(cid:16) Ai |Ai,1 ,Ai,2 ,··· ,Ai,κ (cid:17) , on the BoLD database and force the stream to determine the
j=1 j=1 j=2 j=2 j=2 probabilityofplacecategories,wesetthelearningratestozero.
Pr(cid:0) Ai (cid:1) =Pr(cid:16) Ai |Ai,1 ,Ai,2 ,··· ,Ai,κ (cid:17) . (7) –Objectdetectorstream:InHobject,wereplacepoolinglayers
j=2 j=2 j=1 j=1 j=1 with 16 × 16 and 32 × 32 strides, respectively. We use 7
Because these high-order posterior probabilities cannot be anchor boxes (width, height) ∈ {(10, 13), (16, 30), (33, 23),
calculated precisely, we approximate p using local max-pooling (30, 61), (62, 45), (59, 119), (116, 90)} with an intersection
j
overstreamsandapplylocalmax-poolingoverP+ (seeEq.8). overunionthresholdof0.4.Wesetthenon-maximalsuppression
i,j
(NMS) threshold to 0.4 in order to keep the best bounding box.
Q ≈ max Pr(B |A )= max P+. (8) Since we use Microsoft COCO database [23], each anchor box
1×dy
j∈{1,2}
i j
j∈{1,2}
i,j
(x,y,w,h,s,c) has 85 properties, where (x,y,w,h) represents
The prediction result of the auxiliary streams, which are boundingboxproperties,sisthedetectionscoreandcisanarray
extracted from the input image using empirical knowledge about whose size is equal to the number of classes. Anchor boxes are
places/objects, has a significant impact on the accuracy of Eq. 8. onlyaddedtothefinalstream’slayerinwhicheachcellcontains
Becausetheseattributesarecorrelated,itisdifficultforthestreams 7×85=595elementsmaking8×(7×7)×392predictions.
to adequately separate them, resulting in a biased estimation of TosettheweightsandparametersofHobject,wefirstbuilta
the conditional probability vector using Eq. 8. In addition, the deep neural network that was the serial connection of Hbase and
columnarelementsofPr(B ∩A )areextremelysmall,resulting Hobject. We trained this network in a separate scenario with the
i j
in relatively small max-pooling values. On the other hand, the Microsoft COCO database [23]. Throughout the training, we use
unavailability of a meta-information in an input image may lead a batch size of 8, a momentum of 0.9, and a decay of 5×10−3.
to the availability of another meta-information or vice versa. For The learning rate is set to 10−2, 10−3 and 10−4 for the first 75
instance, it is impossible to imagine people playing soccer in epochs,thenext30epochs,andthefinal30epochs,respectively.
a pool full of water. For this reason, we also use conditional This policy is used to prevent the model from diverging due to
probability to measure the probability of the anticipated non- unstable gradients. After training, we transferred the weights and
availablemeta-information(P−)byEq.9. parametersofthelayersthathadthesamearchitectureasHobject5
(a) (b) (c)
Fig.3.Cumulativeprobabilityof(a)labelsinBoLDdatabase,(b)pseudo-tagprovidedbyapplyingplace-CNNtoBoLDdatabase,and(c)pseudo-tag
providedbyapplyingYOLOobjectdetectortoBoLDdatabase.
to the proposed architecture. Finally, we set the learning rates in Hobject streams that contribute the most to the calculation of
these layers to zero in order to prevent these layers from being Eq. 5–10. The trainable filter kernel also enables the architec-
overfittedduringthetrainingofHemotion ontheBoLDdatabase. ture to learn the correspondences of the feature maps for meta-
– Emotion stream: In Hemotion, we train the network using information to minimise the loss function in Eq 12. Another
stochasticgradientdescentwithmomentum0.9.Theinitiallearn- hyper-parameter is λ that we evaluate its impact by changing the
ing rate is set to 10−2, and the learning rate is decreased by a value from 0 to 0.5 with a 0.1 step. We trained the proposed
factor of 0.1 every 45 epochs. We set the maximum number of architecture as a function of (κ,λ) and plotted the emotion
training epochs to 90 and use a mini-batch with 8 observations recognition score (ERS) to find the best trade-off between these
at each iteration, where the training data is shuffled before each two hyper-parameters (see Fig. 4). We obtained an ERS value of
training epoch. The training parameters for the initial network 83.64atκ=56andλ=0.2andusedthesevaluesintherestof
stem(Hbase)aresettothesamevalues. theexperiments.
The proposed multi-stream architecture has two hyper-
parameters κ and λ that are used to align the meta-information
dimensionality in the calculation of yplace and yobject and to
regulate the prediction results of the streams in Eq. 11. To find
the appropriate value for κ, we apply the pre-trained YOLO
object detector and Places-CNN to the training set. By applying
Places-CNN, the input image (x ) maps into a 365-dimensional
i
vector(zplace)containingtheprobabilityofplacetags.However,
i
this map cannot be straightforwardly built for the YOLO object
detector.
The list of potential objects for each input image (x ) may
i
containadifferentnumberofelements.Also,thislistmaycontain
multiple instances from one object with different confidence
scores.Tounifythecodomaintowhichtheinputimageismapped,
we define a vector with 80 entries (zobject), each of which
i
corresponds to one object tag. We then assign the normalised
cumulative confidence score of the detected objects to the corre-
spondingentriesoftheobjectlist(zobject)andsettherestofthe
Fig. 4. The emotion recognition score for the proposed architecture
entries to zero. For example, 2 instances of ‘person’, 5 instances as a function of (κ,λ). The best trade-off between these two hyper-
parametersis(κ,λ)=(56,0.2),resultinginanERSvalueof83.64%.
of ‘tie’ and 1 instance of ‘remote’ were detected by the object
detectorinFig.1(c).Thenormalisedcumulativeconfidencescores
FromFig.4andtheanalysisofpredictionerrors,weinferred
of 0.19, 0.12 and 0.01 are assigned to the corresponding entries
that larger values of (κ,λ) not only increase computational
for these objects in the output list, and the rest of the entries are
complexitybutalsomaketheresult(y˜)moreinclinedtothemeta-
setto0. informationestimationofP+ andP−.
The maximum value of κ is equal to the minimum dimen-
sionality of Hplace and Hobject outputs, i.e., κ = 80. We
calculate the normalised sum of outputs for the place (zplace =
5 EXPERIMENTS
1 (cid:80)n zplace) and object (zobject = 1 (cid:80)n zobject) tags in 5.1 BodyLanguagedatabase
n i=1 i n i=1 i
thetrainingsettofindtheminimumvalueofκ.Thedistributionof We performed our experiments on the Body Language database
zplaceandzobjectareshowninFigures3(b)and3(c),respectively. (BoLD) [24], which contains by far the most data for AIBEE.
Byapplyingthethresholdof0.01tozplace andzobject,wefound The videos in BoLD were chosen from the publicly available
that27placeand14objecttagscanmeetthethreshold.Therefore, AVAdatabase[17],whichincludesacollectionofYouTubemovie
we set the minimum value of κ to 14 and test its impact by IDs. There are 9,876 video clips of humans expressing emotion,
changingthevaluefrom14to80withthestepof6. mainly through body gestures. The crowd-sourcing platform was
The parameter κ enables fplace and fobject filters in Eq. 1 employed to annotate the database with two widely accepted
and 2 to select a subset of relevant predictions of Hplace and emotionalcategorisations.6
Thereare26labelsforcategoricalemotions,including{Peace, detect 2D poses in still images to use in LMA and reported
Affection,Esteem,Anticipation,Engagement,Confidence,Happi- promisingresultsontheBoLDdatabaseforAIBEE.
ness,Pleasure,Excitement,Surprise,Sympathy,Doubt/confusion, We also compared the proposed architecture with two CNN
Disconnection, Fatigue, Embarrassment, Yearning, Disapproval, architectures that were considered as state-of-the-art in action
Aversion, Annoyance, Anger, Sensitivity, Sadness, Disquietment, recognition. To use the Temporal Segment Networks (TSN) [40]
Fear, Pain, Suffering}. The continuous emotional dimensions are inAIBEE,wespliteachvideointo2segments.Duringthetraining
Valence, Arousal, and Dominance. It should be noted that, while stage, one frame is randomly sampled for each segment, and the
the gathered videos are annotated based on body language, the classificationresultisaveragedoverallthesampledframes.Weset
movies with a close-up of the face rather than the entire or thelearningrateandbatchsizeto10−3and16,respectively.Other
partially-occludedbodyremainunlabelled. trainingrequirementsaresimilartotheoriginalversion.Thetwo-
streaminflated3DCNN[7]uses3DconvolutiontolearnSpatio-
temporal features in an end-to-end way. However, we replaced
5.2 Evaluationmetricsandexperimentalprotocols
3D convolution with 2D convolution to perform experiments on
We use the mean R2 score (Eq. 13) to evaluate the proposed still images that were randomly sampled from each video. In our
regressionmodel.TheR2 metriccalculatestheratioofexplained experiment, we set the learning rate and batch size to 10−2 and
variance(y)tomeasurehowwelltheunseensamplesarelikelyto
16, respectively. We preserved other training details, as stated in
bepredictedbythemodel’sindependentvariables. theoriginalversion.
R2(y,y˜)=1− (cid:80) (cid:80)n i n i= =1 1( (y yi i− −y˜ εi )) 22 , ε= n1 (cid:88) i=n 1y i. (13) T oh fe cop ne tr info ur om ua sn ec me oo tf ioB nE sE is-N RE 2T .TT inA ht eB hL e pE ete r1 cs et ns te at g. eTh me em trie ca sn om fmea As Pur ,e mm Re Ant
andmF1areusedtoreporttheperformanceofclassifyingdiscrete
wherey˜ i istheisthepredictedvalueofthei-thsample,andy i is emotions.ERSisusedtoreporttheemotionidentificationscore.Note
thecorrespondingtruevalue. that‘Chance’referstoarandommethodbasedonpriors.
Since we formulated the AIBEE as a regression problem,
we applied the max threshold to convert the predicted quantity Method ERS(%) Regression Classification(%)
mR2 mAP mRA mF1
of the regression model into discrete buckets for emotions. The
Chance 61.75 0 11.75 50 19.02
use of argmax lets us evaluate the efficiency of the proposed Luoetal.[24] 64.08 0.0947 17.48 62.59 27.32
regression model using Precision, Recall and F1, where F1 is a Wangetal.[40] 62.24 0.0760 14.02 57.65 22.55
weighted average of the precision and recall metrics. We report Carreiraetal.[7] 64.26 0.1007 17.33 61.2 27.01
BEE-NET 66.33 0.1493 23.18 71.56 35.01
the average precision (mAP), the mean area under the receiver’s
operating characteristic curve (mRA), and the mean of F1 for
Figure 5 provides comprehensive metric comparisons of all
theassessment.Foreaseofcomparison,theEmotionRecognition
methods of each categorical and dimensional emotion. From
Score(ERS)isalsodefinedinEq.14.
Table 1 and Fig. 5, it could be said that our hypothesis regard-
ing the influence of the environment and the object involved
(cid:18)mAP +mRA(cid:19)
in the disclosure of human emotions is valid. Moreover, it can
G ={R2,mAP,mRA}, ∆=R2+ ,
2 be seen in Fig. 5 that the {Engagement, Happiness, Pleasure,
∆−min(G ) Anticipation,Sadness}categoriescomprisethetop-5predictions.
ERS= i , 1≤i≤3.
Indeed,theproposedarchitecturecouldappropriatelyaddressthe
max(G )−min(G )
i i
bias problem (see Fig. 3(a)) towards {Engagement, Anticipation,
(14)
Confidence,Peace,Doubt}intheBoLDdatabase.IntheAblation
In the experiments, we did not use data augmentation tech- study, we will demonstrate that the formulation of the pooling
niquestopreventunrealisticchangesincolour,angleandposition scheme, where the probability of both available and anticipated
thatcouldalterourhypothesisabouttherelationshipbetweenthe non-available items being considered, could contribute to this
representation of emotions and the environment and the objects achievement.
involved.Wepartitionedthedatabaseintotrain,testandvalidation In the assessment of continuous emotions, all methods show
setscontaining60%,30%and10%ofsamples,respectively.The a greater performance of arousal regression than valence and
members of each set were chosen at random in such a way that dominance. However, compared to the subjective test reported
thedistributionofBoLDforeachsetwasobserved(seeFig.3(a)). in [24], humans showed better valence-recognition performance
than arousal. This distinction between human and model output
5.3 Experimentalresults indicatesthatdomainknowledgeandexperienceinothercontexts
allow humans to make better decisions in completely new situa-
Table1showstheperformanceoftheproposedarchitecturealong
tions.
withcompetitivemethods[7,24,40]toverifytheeffectivenessof
theBEE-NET.Following[24],weusedarandommethodbasedon
priors (referred to as ‘Chance’) as a basis for comparison. Laban 5.4 Ablationstudy
Movement Analysis (LMA) [39] was originally developed to In the ablation study, we conducted four sets of experiments to
characterize dance movements by a set of structural and physical better understand the efficacy of the BEE-NET for the AIBEE
characteristics through representing body, effort, form and space. task.Wethereforeexamine:
Becauseoftheproximityofbodylanguagetothisrepresentation, — Contribution of pre-trained model weights: In this ex-
Luo et al. [24] used LMA to identify the bodily expression of periment, instead of assigning random weights to the Hbase +
emotions. They used the method proposed by Cao et al. [6] to Hemotion networkfilters,weusedGoogLeNetfilters’weightthat7
Fig. 5. Classification performance for discrete emotions is reported based on the average precision (AP) in the [first row] and area under the
receiver’soperatingcharacteristiccurve(RA)inthe[secondrow].Theregressionperformanceforcontinuousemotionsisreportedonthebasisof
theR2scoreinthe[thirdrow].
was trained with ImageNet [35], Places2 and Microsoft COCO. It can be inferred from Table 3 that the impact of Hplace is
Wereducedthetrainingstepsto45iterationsandretainedallthe greaterthanthatofHobject.ThiseffectisalsoevidentinTable2,
otherparametersasdescribedinSection4.ThefindingsinTable2 where the use of pre-trained model filters with the Microsoft
showthattheinitialisationoffilterswithrandomweightsleadsto COCO database, among other databases, resulted in lower per-
amarginallybetterperformanceofAIBEE. formance. Furthermore, the frames were primarily sourced from
oldermovieswithintheBoLDdatabase,whichinherentlypossess
lower quality and smaller sizes. Therefore, locating the right and
TABLE2
Ablationstudyontheeffectofpre-trainedmodels. appropriate objects in the scene is met with an error that later
propagates to the architecture. For example, a “remote” object in
Regression Classification(%) Fig.1(c)wasdetectedbytheobjectdetector,whichisanincorrect
Initialweight ERS(%)
mR2 mAP mRA mF1 and unrelated object to the context. Moreover, considering the
Ablationstudy
intent of the BoLD database, ‘Person’ is the dominant object
Random 65.42 0.1241 21.37 69.82 32.72
ImageNet 65.09 0.1007 19.89 66.36 30.60 in all scenes. Therefore, the majority of zobject entries have a
Places2 64.59 0.1103 18.72 64.66 29.03 valueofzerothat,despiteapplyingthefobject kernel,thesparsity
MicrosoftCOCO 64.44 0.0997 18.36 64.02 28.53 propagates to yobject feature vector. In this way, the classifier
Proposedarchitecture
must deal with the sparse representation in the latent space that
BEE-NET 66.33 0.1493 23.18 71.56 35.01
reduces its performance. However, the influence of Hobject in
the proposed architecture is undeniable as its combination with
— Contribution of Hplace and Hobject: In two experiments,
Hplace could improve the state-of-the-art in identifying in-the-
weeliminatedHplace andHobject streamsandtrainedthealtered
wildbodilyexpressionsofemotionsby2.07%.
architecture (H ⊖ Hplace and H ⊖ Hobject) with BoLD data
to understand the effect of each stream. In this experiment, we — Contribution of face to AIBEE: To examine the impact of
retainedallotherparametersasdescribedinSection4,exceptfor the face on BEE-NET performance, we filled the face area with
κ. The value κ was respectively set to 100 and 40 during the blackpixelsinalldatabaseframes.Then,wetrainedtheproposed
operation of the Hplace and Hobject streams. The results of this architecture with new images, where the network parameters are
ablationstudyareshowninTable3. retainedasdescribedinSection4.Althoughmaskingthefacehad
little effect on ‘Person’ detection due to the presence of Hobject
stream, the ERS metric decreased to 62.35%. This remarkable
TABLE3
AblationstudyontheeffectofHplaceandHobjectstreams. decreaseemphasiseshowfacialexpressioncansupportthebodily
expressionofemotions.
Regression Classification(%) —Contributionoffusionstrategies:Incorporatingfusionstrate-
Architecture ERS(%)
mR2 mAP mRA mF1
gies into deep learning models that deal with multi-modal input
Ablationstudy
H⊖Hobject 64.91 0.0804 18.85 63.56 29.07 or extract features using multi-stream architectures can improve
H⊖Hplace 63.55 0.0589 15.23 56.48 23.99 accuracy and performance. These fusion strategies are typically
Proposedarchitecture categorised into early, intermediate, and late fusion categories.
BEE-NET 66.33 0.1493 23.18 71.56 35.01 This ablation study aims to compare the efficacy of the proposed8
probabilistic pooling-based late fusion strategy with other fusion
whereY˜ isthepredictedvalues,andp(y˜)istheprobabilityofY˜
strategies to demonstrate how leveraging meta-information can takingthevaluey˜.
outperform conventional fusion strategies. To do this, we have (cid:32) (cid:33)
P (y,y˜)
modified the proposed architecture in the following ways and MI= (cid:88) (cid:88) P (y,y˜)log (Y,Y˜) . (16)
presenttheresultsinTable4.
y˜∈Y˜ y∈Y
(Y,Y˜)
P Y(y)P Y˜(y˜)
where P is the joint probability density function of Y and
1) The early fusion strategy involves merging and process- (Y,Y˜)
ing all input data at the beginning of the neural network Y˜ ,andP Y andP Y˜ arethemarginalprobabilitydensityfunctions
before performing feature extraction. Although an early
ofY(i.e.,ground-truthvalues)andY˜
,respectively.
fusionstrategycanbeusefulforsimpletasks,itmaylead Thesefindingssupportourhypothesisthattheproposedprob-
tooverfitting.Inthisablationstudy,wecannotapplythe abilistic pooling-based late fusion strategy effectively utilises the
early fusion strategy as we feed the BEE-NET with uni- meta-information provided by the place and object streams to
modaldata. confidently identify bodily expressions of emotions. Our results
2) The intermediate fusion strategy in multi-stream deep also highlight that the intermediate fusion strategy tends to di-
models combines features from multiple streams at an lute individual modalities’ strengths and combine the individual
intermediatelayerbeforeperformingtheclassificationor models’ uncertainties. This can lead to an overall higher level of
regression. This fusion strategy usually uses concatena- uncertainty and inferior performance compared to our proposed
tion, element-wise addition, or element-wise multiplica- fusionstrategy.Thisobservationalignswiththeresultspresented
tion. In this ablation study, we replaced the proposed in Table 3, where we showed that if the object stream detects
probabilistic pooling-based fusion strategy with an in- incorrect, unrelated, and dominant objects concerning the nature
termediate fusion strategy in BEE-NET. Specifically, we of the database, it can increase the sparsity, noise, and outlier in
concatenated the output features of the place (Hplace) the latent space. These defects can be propagated throughout the
and object (Hobject) streams with the features of the model when the fusion strategy fails to leverage the correlation
initialnetworkstem(Hbase).Wethenpassedtheconcate- betweentheoutputsfromdifferentstreams.
natedfeaturesthroughafullyconnectedlayertoobtaina Finally, it is essential to note that the decision between late
fused feature vector. Finally, we fed the emotion stream and intermediate fusion strategies should be carefully evaluated
(Hemotion) with the fused features and continued the based on the specific requirements of the task, the characteristics
forwardpass. of the data, and the available computational resources. While
3) Thelatefusionstrategyinmulti-streamdeepneuralnet- late fusion strategies may offer advantages in specific scenarios,
worksinvolvescombiningtheoutputsofmultiplestreams intermediate fusion strategies can provide opportunities for more
atalaterstageinthenetworkarchitecture,typicallyafter efficientandeffectiveintegrationofmulti-modalinformation.Itis
theindividualstreamshavebeenprocessedbytheirown imperative to consider the unique contributions of each modality
set of layers. In this ablation study, we conducted two and the potential impact of noise when designing multi-modal
experiments to assess the proposed fusion strategy. In deeplearningarchitecturesforoptimalperformance.
the first experiment, we removed the probability of an-
ticipated non-available meta-information by eliminating
the P− and Pˆ terms from Eq. 10 and trained the BEE-
6 CONCLUSION
NET with Pˆ = QP+. In the second experiment, we Humans rely on emotional expressions to create meaningful
substituted the proposed fusion strategy with the one interpersonal relationships. To enable computers to recognise,
proposed by Kendall et al. [19] and proceeded with the perceive, interpret, and simulate emotions as humans do, they
forwardpass. must be equipped with the ability to understand and simulate
human affects. Recent research has attempted to integrate bod-
ily expressions of emotions into affective computing, as bodily
Table4revealsthattheintermediatefusionstrategyexhibitsa
expressions can convey emotional states and are sometimes the
marginal improvement over both late fusion strategies regarding
onlymodalitythatcanaccuratelydisambiguatethecorresponding
the evaluation metrics. However, intriguingly, the late fusion
facialexpression.
strategy from which the probability of anticipated non-available
The present study investigated how environmental and object
meta-informationisexcludedshowsreduceduncertaintyinterms
ofEntropy(seeEq.15)andMutualInformation(seeEq.16)1. factorsmayinfluencetheperceptionofin-the-wildbodilyexpres-
sions of emotions. We proposed a novel multi-stream convolu-
tional neural network (BEE-NET), which integrates pre-trained
(cid:88)
E=− p(y˜)log(p(y˜)), (15) place and object recognition networks to represent contextual
y˜∈Y˜ information. To incorporate this information, we formulated a
derivable pooling scheme based on Bayes’ theorem, which fuses
the extracted uncertain information with the predicted image-
1.Entropy quantifies the uncertainty of a single random variable, while
basedemotionalstates.Thisallowsforend-to-endmodeltraining
Mutual Information measures the shared information between two random
andtheacquisitionofaprioriinformationonthejointprobability
variables. In our experiment, we employ kernel density estimation with a
Gaussiankernel(i.e.,N(µ,σ)=N(0,1))toestimatetheprobabilitydensity of emotions and both available and anticipated non-available
functionsforpredictedandground-truthvalues.Subsequently,wederivethe places/objects, driving the emotion learning process during train-
joint and marginal probability density functions from the estimated ones. In
ourexperiment,wesetthebandwidthvaluehto1.06σn−1
5 [33]tominimise
ing.
Our experimental results, obtained using the Body Language
the mean integrated squared error, where n = 29 represents the number of
predictedvalues. Database (BoLD), the largest database available for identifying9
TABLE4
Ablationstudyresultsonfusionstrategies’contribution,measuredbymutualinformation(MI)andentropy(E)metrics.MIincreaseswithaccuracy
anddependence,whileEincreaseswithuncertaintyandrandomnessinmodelpredictions.
Regression Classification Uncertainly
Fusionmethod ERS(%) mR2 mAP mRA mF1 E↓ MI↑
Ablationstudy
Earlyfusion N/A N/A N/A N/A N/A N/A N/A
Intermediatefusion 64.50 0.1095 17.46 60.68 25.86 3.32 2.95
Latefusion(Pˆ=QP+) 63.42 0.1104 16.70 62.75 26.37 1.98 3.81
Latefusion(Kendalletal.[19]) 63.77 0.1036 17.14 62.70 26.92 2.03 3.66
Proposedarchitecture
BEE-NET 66.33 0.1493 23.18 71.56 35.01 1.25 4.51
in-the-wild bodily expressions of emotions, demonstrate that our [6] Z. Cao, G. Hidalgo, T. Simon, S. E. Wei, and Y. Sheikh.
proposed method outperforms the state-of-the-art in identifying OpenPose: Realtime Multi-Person 2D Pose Estimation Using
categorical (discrete) and continuous in-the-wild bodily expres- Part Affinity Fields. IEEE Trans. Pattern Anal. Mach. Intell.,
43(1):172–186,2021.
sions of emotions. Specifically, we validated our hypothesis that
[7] J.CarreiraandA.Zisserman. QuoVadis,ActionRecognition?
explicitly incorporating the co-occurrences of available and an-
ANewModelandtheKineticsDataset. InCVPR,pages4724–
ticipated non-available places/objects into the fusion strategy can 4733.IEEE,2017.
simplifyandguidethelearningprocess,removingtheneedforthe [8] M. M. Dehshibi, Bita Baiani, Gerard Pons, and David Masip.
networktoautomaticallydiscovertheimpactoftheserelationships A Deep Multimodal Learning Approach to Perceive Basic
NeedsofHumansFromInstagramProfile. IEEETrans.Affect.,
onthedecision.
14(2):944–956,2023.
Overall,ourproposedmethod,BEE-NET,providesanefficient
[9] M. M. Dehshibi and A. Bastanfard. A new algorithm for age
and effective approach to incorporating contextual information recognition from facial images. Signal Process, 90(8):2431–
intotheemotionrecognitionprocess,whichcanleadtoimproved 2444,2010.
performanceinreal-worldapplications. [10] M.M.Dehshibi,T.A.Olugbade,F.Diaz-deMaria,N.Bianchi-
Berthouze, and A. Tajadura-Jime´nez. Pain Level and Pain-
Related Behaviour Classification Using GRU-Based Sparsely-
ACKNOWLEDGEMENTS Connected RNNs. IEEE Journal of Selected Topics in Signal
Processing,17(3):677–688,2023.
Funding: This research was supported by a grant from the [11] M.M.DehshibiandJ.Shanbehzadeh. Cubicnormandkernel-
Spanish Ministry of Science, Innovation, and Universities basedbi-directionalPCA:towardage-awarefacialkinshipveri-
(RTI2018-095232-B-C22) and the NVIDIA Hardware grant fication. Vis.Comput.,35(1):23–40,2019.
[12] P.Ekman. Anargumentforbasicemotions. Cogn.Emot.,6(3-
program.ThisworkwasalsosupportedbytheEuropeanResearch
4):169–200,1992.
Council(ERC)throughtheHorizon2020researchandinnovation
[13] B.A.Erol,A.Majumdar,P.Benavidez,P.Rad,K.K.R.Choo,
program under grant agreement number 101002711. Mohammad and M. Jamshidi. Toward Artificial Emotional Intelligence for
MahdiDehshibireceivedpartialfundingfromthissource. Cooperative Social Human–Machine Interaction. IEEE Trans.
Comput.Soc.Syst.,7(1):234–246,2020.
AuthorContributions:MohammadMahdi¬Dehshibicontributed [14] C. Feichtenhofer, H. Fan, J. Malik, and K. He. SlowFast
Networks for Video Recognition. In ICCV, pages 6201–6210.
to the conception of the idea, defined the scope of the study,
IEEE,2019.
and conducted the experiments. All authors participated in the
[15] B.Gholami,P.Sahu,O.Rudovic,K.Bousmalis,andV.Pavlovic.
discussion of results, provided feedback on the manuscript, and UnsupervisedMulti-TargetDomainAdaptation:AnInformation
assistedinwritingandediting. Theoretic Approach. IEEE Trans. Image Process., 29:3993–
4002,2020.
[16] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman. Video
CompetingInterests:Theauthorsdeclarenocompetinginterests.
ActionTransformerNetwork. InCVPR,pages244–253.IEEE,
2019.
REFERENCES [17] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y.
Li, S. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar,
[1] L. Abramson, R. Petranker, I. Marom, and H. Aviezer. Social C. Schmid, and J. Malik. AVA: A Video Dataset of Spatio-
interaction context shapes emotion recognition through body TemporallyLocalizedAtomicVisualActions. InCVPR,pages
language,notfacialexpressions. Emotion,21(3),2020. 6047–6056.IEEE,2018.
[2] M.Aghaahmadi,M.M.Dehshibi,A.Bastanfard,andM.Fazlali. [18] N.Hussein,E.Gavves,andA.W.M.Smeulders. Timeception
Clusteringpersianvisemeusingphonemesubspacefordevelop- for Complex Action Recognition. In CVPR, pages 254–263.
ing visual speech application. Multim. Tools Appl., 65(3):521– IEEE,2019.
541,2013. [19] A.Kendall,Y.Gal,andR.Cipolla. Multi-taskLearningUsing
[3] Mona Ashtari-Majlan, Abbas Seifi, and Mohammad Mahdi Uncertainty to Weigh Losses for Scene Geometry and Seman-
Dehshibi. A Multi-Stream Convolutional Neural Network for tics. InCVPR,pages7482–7491.IEEE,2018.
ClassificationofProgressiveMCIinAlzheimer’sDiseaseUsing [20] R. Kosti, J. Alvarez, A. Recasens, and A. Lapedriza. Context
StructuralMRIImages.IEEEJournalofBiomedicalandHealth Based Emotion Recognition Using EMOTIC Dataset. IEEE
Informatics,26(8):3918–3926,2022. Trans.PatternAnal.Mach.Intell.,42(11):2755–2766,2020.
[4] H. Aviezer, Y. Trope, and A. Todorov. Body Cues, Not Facial [21] V. Kumar, S. Rao, and L. Yu. Noisy Student Training Using
Expressions, Discriminate Between Intense Positive and Nega- Body Language Dataset Improves Facial Expression Recogni-
tiveEmotions. Science,338(6111):1225–1229,2012. tion. InECCVWorkshops,pages756–773.Springer,2020.
[5] A.T.Beck. Depression:Clinical,experimental,andtheoretical [22] B. Li, X. Li, Z. Zhang, and F. Wu. Spatio-Temporal Graph
aspects. HoeberMedicalDivision,Harper&Row,1967.10
Routing for Skeleton-Based Action Recognition. In AAAI-19, Mohammad Mahdi Dehshibi (Member, IEEE)
volume33,pages8561–8568.AAAIPress,2019. receivedhisPhDinComputerSciencein2017.
[23] T. Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- He is currently a research scientist at Univer-
manan,P.Dolla´r,andC.L.Zitnick.MicrosoftCOCO:Common sidad Carlos III de Madrid, Spain. He is also
ObjectsinContext. InECCV,pages740–755.SpringerCham, an adjunct researcher at Universitat Oberta de
Catalunya(Spain)andtheUnconventionalCom-
2014.
puting Lab. at UWE (Bristol, UK). He has con-
[24] Y. Luo, J. Ye, R. B. Adams, J. Li, M. G. Newman, and J. Z.
tributed to more than 70 papers published in
Wang. ARBEE: Towards Automated Recognition of Bodily
peer-reviewedjournalsandconferenceproceed-
Expression of Emotion in the Wild. International Journal of
ings. He also serves as an associate editor of
ComputerVision,128(1):1–25,2020. the International Journal of Parallel, Emergent,
[25] D.C.Luvizon,D.Picard,andH.Tabia. 2D/3DPoseEstimation andDistributedSystems.HisresearchinterestsincludeDeepLearning,
and Action Recognition Using Multitask Deep Learning. In MedicalImageProcessing,HumanBehaviourAnalysis,Unconventional
CVPR,pages5137–5146.IEEE,2018. Computing,andAffectiveComputing.
[26] T. Mensink, E. Gavves, and C. G. M. Snoek. COSTA: Co-
Occurrence Statistics for Zero-Shot Classification. In CVPR,
David Masip (Senior Member, IEEE) received
pages2441–2448.IEEE,2014. his Ph.D. degree in Computer Vision in 2005
[27] A. Mollahosseini, D. Chan, and M. H. Mahoor. Going deeper (UniversitatAutonomadeBarcelona,Spain).He
infacialexpressionrecognitionusingdeepneuralnetworks. In was awarded the best thesis in Computer Sci-
WACV,pages1–10.IEEE,2016. ence. He is a Full Professor at the Computer
[28] F.Noroozi,D.Kaminska,C.Corneanu,T.Sapinski,S.Escalera, Science, Multimedia, and Telecommunications
andG.Anbarjafari. SurveyonEmotionalBodyGestureRecog- DepartmentatUniversitatObertadeCatalunya,
nition. IEEETrans.Affect.Comput.,12(2):505–523,2021. Spain, and the Director of the Doctoral School
[29] G.PonsandD.Masip. Multitask,multilabel,andmultidomain since 2015. He has published more than 70
learning with convolutional networks for emotion recognition. scientificpapersinrelevantjournalsandconfer-
ences. His research interests include Affective
IEEETrans.Cybern.,52(6):4764–4771,2022.
Computing,Oculomics,andRetinaImageAnalysis.
[30] S. Poria, E. Cambria, R. Bajpai, and A. Hussain. A review
of affective computing: From unimodal analysis to multimodal
fusion. Inf.Fusion,37:98–125,2017.
[31] J.Redmon,S.Divvala,R.Girshick,andA.Farhadi. YouOnly
Look Once: Unified, Real-Time Object Detection. In CVPR,
pages779–788.IEEE,2016.
[32] K. Schindler, L. Van Gool, and B. De Gelder. Recognizing
emotionsexpressedbybodypose:Abiologicallyinspiredneural
model. NeuralNetw,21(9):1238–1246,2008.
[33] Bernard W. Silverman. Density Estimation for Statistics and
DataAnalysis. Routledge,1998.
[34] K. Simonyan and A. Zisserman. Two-Stream Convolutional
Networks for Action Recognition in Videos. In NIPS, pages
568–576.MITPress,2014.
[35] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,
D.Erhan,V.Vanhoucke,andA.Rabinovich. Goingdeeperwith
convolutions. InCVPR,pages1–9.IEEE,2015.
[36] M. Tomei, L. Baraldi, S. Calderara, S. Bronzin, and R.
Cucchiara. Video action detection by learning graph-based
spatio-temporal interactions. Comput. Vis. Image Underst.,
206:103187,2021.
[37] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M.
Paluri. A Closer Look at Spatiotemporal Convolutions for
ActionRecognition. InCVPR,pages6450–6459.IEEE,2018.
[38] O. Ulutan, S. Rallapalli, M. Srivatsa, C. Torres, and B. S.
Manjunath.ActorConditionedAttentionMapsforVideoAction
Detection. InWACV,pages516–525.IEEE,2020.
[39] R.VonLaban. Choreutics. MacdonaldandEvans,1966.
[40] L. Wang, Y. Xiong, Z. Wang, Yu Q., D. Lin, X. Tang, and
L. Van Gool. Temporal Segment Networks: Towards Good
Practices for Deep Action Recognition. In ECCV, pages 20–
36.Springer,2016.
[41] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural
networks. InCVPR,pages7794–7803,2018.
[42] F.Xu,J.Zhang,andJ.Z.Wang. MicroexpressionIdentification
andCategorizationUsingaFacialDynamicsMap. IEEETrans.
Affect.Comput.,8(2):254–267,2017.
[43] S.K.Yadav,K.Tiwari,H.M.Pandey,andS.A.Akbar.Areview
ofmultimodalhumanactivityrecognitionwithspecialemphasis
on classification, applications, challenges and future directions.
Knowl.BasedSyst.,223:106970,2021.
[44] H. X. Yu, W. S. Zheng, A. Wu, X. Guo, S. Gong, and J. H.
Lai. Unsupervised Person Re-Identification by Soft Multilabel
Learning. InCVPR,pages2143–2152.IEEE,2019.
[45] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba.
Places: A 10 Million Image Database for Scene Recognition.
IEEETrans.PatternAnal.Mach.Intell.,40(6):1452–1464,2018.