[
    {
        "title": "Coercing LLMs to do and reveal (almost) anything",
        "authors": "Jonas GeipingAlex SteinManli ShuKhalid SaifullahYuxin WenTom Goldstein",
        "links": "http://arxiv.org/abs/2402.14020v1",
        "entry_id": "http://arxiv.org/abs/2402.14020v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14020v1",
        "summary": "It has recently been shown that adversarial attacks on large language models\n(LLMs) can \"jailbreak\" the model into making harmful statements. In this work,\nwe argue that the spectrum of adversarial attacks on LLMs is much larger than\nmerely jailbreaking. We provide a broad overview of possible attack surfaces\nand attack goals. Based on a series of concrete examples, we discuss,\ncategorize and systematize attacks that coerce varied unintended behaviors,\nsuch as misdirection, model control, denial-of-service, or data extraction.\n  We analyze these attacks in controlled experiments, and find that many of\nthem stem from the practice of pre-training LLMs with coding capabilities, as\nwell as the continued existence of strange \"glitch\" tokens in common LLM\nvocabularies that should be removed for security reasons.",
        "updated": "2024-02-21 18:59:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14020v1"
    },
    {
        "title": "D-Flow: Differentiating through Flows for Controlled Generation",
        "authors": "Heli Ben-HamuOmri PunyItai GatBrian KarrerUriel SingerYaron Lipman",
        "links": "http://arxiv.org/abs/2402.14017v1",
        "entry_id": "http://arxiv.org/abs/2402.14017v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14017v1",
        "summary": "Taming the generation outcome of state of the art Diffusion and Flow-Matching\n(FM) models without having to re-train a task-specific model unlocks a powerful\ntool for solving inverse problems, conditional generation, and controlled\ngeneration in general. In this work we introduce D-Flow, a simple framework for\ncontrolling the generation process by differentiating through the flow,\noptimizing for the source (noise) point. We motivate this framework by our key\nobservation stating that for Diffusion/FM models trained with Gaussian\nprobability paths, differentiating through the generation process projects\ngradient on the data manifold, implicitly injecting the prior into the\noptimization process. We validate our framework on linear and non-linear\ncontrolled generation problems including: image and audio inverse problems and\nconditional molecule generation reaching state of the art performance across\nall.",
        "updated": "2024-02-21 18:56:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14017v1"
    },
    {
        "title": "Corrective Machine Unlearning",
        "authors": "Shashwat GoelAmeya PrabhuPhilip TorrPonnurangam KumaraguruAmartya Sanyal",
        "links": "http://arxiv.org/abs/2402.14015v1",
        "entry_id": "http://arxiv.org/abs/2402.14015v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14015v1",
        "summary": "Machine Learning models increasingly face data integrity challenges due to\nthe use of large-scale training datasets drawn from the internet. We study what\nmodel developers can do if they detect that some data was manipulated or\nincorrect. Such manipulated data can cause adverse effects like vulnerability\nto backdoored samples, systematic biases, and in general, reduced accuracy on\ncertain input domains. Often, all manipulated training samples are not known,\nand only a small, representative subset of the affected data is flagged.\n  We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the\nimpact of data affected by unknown manipulations on a trained model, possibly\nknowing only a subset of impacted samples. We demonstrate that the problem of\ncorrective unlearning has significantly different requirements from traditional\nprivacy-oriented unlearning. We find most existing unlearning methods,\nincluding the gold-standard retraining-from-scratch, require most of the\nmanipulated data to be identified for effective corrective unlearning. However,\none approach, SSD, achieves limited success in unlearning adverse effects with\njust a small portion of the manipulated samples, showing the tractability of\nthis setting. We hope our work spurs research towards developing better methods\nfor corrective unlearning and offers practitioners a new strategy to handle\ndata integrity challenges arising from web-scale training.",
        "updated": "2024-02-21 18:54:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14015v1"
    },
    {
        "title": "Misalignment, Learning, and Ranking: Harnessing Users Limited Attention",
        "authors": "Arpit AgarwalRad NiazadehPrathamesh Patil",
        "links": "http://arxiv.org/abs/2402.14013v1",
        "entry_id": "http://arxiv.org/abs/2402.14013v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14013v1",
        "summary": "In digital health and EdTech, recommendation systems face a significant\nchallenge: users often choose impulsively, in ways that conflict with the\nplatform's long-term payoffs. This misalignment makes it difficult to\neffectively learn to rank items, as it may hinder exploration of items with\ngreater long-term payoffs. Our paper tackles this issue by utilizing users'\nlimited attention spans. We propose a model where a platform presents items\nwith unknown payoffs to the platform in a ranked list to $T$ users over time.\nEach user selects an item by first considering a prefix window of these ranked\nitems and then picking the highest preferred item in that window (and the\nplatform observes its payoff for this item). We study the design of online\nbandit algorithms that obtain vanishing regret against hindsight optimal\nbenchmarks.\n  We first consider adversarial window sizes and stochastic iid payoffs. We\ndesign an active-elimination-based algorithm that achieves an optimal\ninstance-dependent regret bound of $O(\\log(T))$, by showing matching regret\nupper and lower bounds. The key idea is using the combinatorial structure of\nthe problem to either obtain a large payoff from each item or to explore by\ngetting a sample from that item. This method systematically narrows down the\nitem choices to enhance learning efficiency and payoff.\n  Second, we consider adversarial payoffs and stochastic iid window sizes. We\nstart from the full-information problem of finding the permutation that\nmaximizes the expected payoff. By a novel combinatorial argument, we\ncharacterize the polytope of admissible item selection probabilities by a\npermutation and show it has a polynomial-size representation. Using this\nrepresentation, we show how standard algorithms for adversarial online linear\noptimization in the space of admissible probabilities can be used to obtain a\npolynomial-time algorithm with $O(\\sqrt{T})$ regret.",
        "updated": "2024-02-21 18:52:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14013v1"
    },
    {
        "title": "Chasing Convex Functions with Long-term Constraints",
        "authors": "Adam LechowiczNicolas ChristiansonBo SunNoman BashirMohammad HajiesmailiAdam WiermanPrashant Shenoy",
        "links": "http://arxiv.org/abs/2402.14012v1",
        "entry_id": "http://arxiv.org/abs/2402.14012v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14012v1",
        "summary": "We introduce and study a family of online metric problems with long-term\nconstraints. In these problems, an online player makes decisions $\\mathbf{x}_t$\nin a metric space $(X,d)$ to simultaneously minimize their hitting cost\n$f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric. Over the\ntime horizon $T$, the player must satisfy a long-term demand constraint\n$\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction\nof demand satisfied at time $t$. Such problems can find a wide array of\napplications to online resource allocation in sustainable energy and computing\nsystems. We devise optimal competitive and learning-augmented algorithms for\nspecific instantiations of these problems, and further show that our proposed\nalgorithms perform well in numerical experiments.",
        "updated": "2024-02-21 18:51:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14012v1"
    }
]