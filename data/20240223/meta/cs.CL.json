[
    {
        "title": "Coercing LLMs to do and reveal (almost) anything",
        "authors": "Jonas GeipingAlex SteinManli ShuKhalid SaifullahYuxin WenTom Goldstein",
        "links": "http://arxiv.org/abs/2402.14020v1",
        "entry_id": "http://arxiv.org/abs/2402.14020v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14020v1",
        "summary": "It has recently been shown that adversarial attacks on large language models\n(LLMs) can \"jailbreak\" the model into making harmful statements. In this work,\nwe argue that the spectrum of adversarial attacks on LLMs is much larger than\nmerely jailbreaking. We provide a broad overview of possible attack surfaces\nand attack goals. Based on a series of concrete examples, we discuss,\ncategorize and systematize attacks that coerce varied unintended behaviors,\nsuch as misdirection, model control, denial-of-service, or data extraction.\n  We analyze these attacks in controlled experiments, and find that many of\nthem stem from the practice of pre-training LLMs with coding capabilities, as\nwell as the continued existence of strange \"glitch\" tokens in common LLM\nvocabularies that should be removed for security reasons.",
        "updated": "2024-02-21 18:59:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14020v1"
    },
    {
        "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
        "authors": "Vyas RainaAdian LiusieMark Gales",
        "links": "http://arxiv.org/abs/2402.14016v1",
        "entry_id": "http://arxiv.org/abs/2402.14016v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14016v1",
        "summary": "Large Language Models (LLMs) are powerful zero-shot assessors and are\nincreasingly used in real-world situations such as for written exams or\nbenchmarking systems. Despite this, no existing work has analyzed the\nvulnerability of judge-LLMs against adversaries attempting to manipulate\noutputs. This work presents the first study on the adversarial robustness of\nassessment LLMs, where we search for short universal phrases that when appended\nto texts can deceive LLMs to provide high assessment scores. Experiments on\nSummEval and TopicalChat demonstrate that both LLM-scoring and pairwise\nLLM-comparative assessment are vulnerable to simple concatenation attacks,\nwhere in particular LLM-scoring is very susceptible and can yield maximum\nassessment scores irrespective of the input text quality. Interestingly, such\nattacks are transferable and phrases learned on smaller open-source LLMs can be\napplied to larger closed-source models, such as GPT3.5. This highlights the\npervasive nature of the adversarial vulnerabilities across different judge-LLM\nsizes, families and methods. Our findings raise significant concerns on the\nreliability of LLMs-as-a-judge methods, and underscore the importance of\naddressing vulnerabilities in LLM assessment methods before deployment in\nhigh-stakes real-world scenarios.",
        "updated": "2024-02-21 18:55:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14016v1"
    },
    {
        "title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
        "authors": "Chaoqun HeRenjie LuoYuzhuo BaiShengding HuZhen Leng ThaiJunhao ShenJinyi HuXu HanYujie HuangYuxiang ZhangJie LiuLei QiZhiyuan LiuMaosong Sun",
        "links": "http://arxiv.org/abs/2402.14008v1",
        "entry_id": "http://arxiv.org/abs/2402.14008v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14008v1",
        "summary": "Recent advancements have seen Large Language Models (LLMs) and Large\nMultimodal Models (LMMs) surpassing general human capabilities in various\ntasks, approaching the proficiency level of human experts across multiple\ndomains. With traditional benchmarks becoming less challenging for these\nmodels, new rigorous challenges are essential to gauge their advanced\nabilities. In this work, we present OlympiadBench, an Olympiad-level bilingual\nmultimodal scientific benchmark, featuring 8,952 problems from Olympiad-level\nmathematics and physics competitions, including the Chinese college entrance\nexam. Each problem is detailed with expert-level annotations for step-by-step\nreasoning. Evaluating top-tier models on OlympiadBench, we implement a\ncomprehensive assessment methodology to accurately evaluate model responses.\nNotably, the best-performing model, GPT-4V, attains an average score of 17.23%\non OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark\nrigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V\npoints out prevalent issues with hallucinations, knowledge omissions, and\nlogical fallacies. We hope that our challenging benchmark can serve as a\nvaluable resource for helping future AGI research endeavors.",
        "updated": "2024-02-21 18:49:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14008v1"
    },
    {
        "title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models",
        "authors": "Zhiwei HeBinglin ZhouHongkun HaoAiwei LiuXing WangZhaopeng TuZhuosheng ZhangRui Wang",
        "links": "http://arxiv.org/abs/2402.14007v1",
        "entry_id": "http://arxiv.org/abs/2402.14007v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14007v1",
        "summary": "Text watermarking technology aims to tag and identify content produced by\nlarge language models (LLMs) to prevent misuse. In this study, we introduce the\nconcept of ''cross-lingual consistency'' in text watermarking, which assesses\nthe ability of text watermarks to maintain their effectiveness after being\ntranslated into other languages. Preliminary empirical results from two LLMs\nand three watermarking methods reveal that current text watermarking\ntechnologies lack consistency when texts are translated into various languages.\nBased on this observation, we propose a Cross-lingual Watermark Removal Attack\n(CWRA) to bypass watermarking by first obtaining a response from an LLM in a\npivot language, which is then translated into the target language. CWRA can\neffectively remove watermarks by reducing the Area Under the Curve (AUC) from\n0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors\nthat contribute to the cross-lingual consistency in text watermarking and\npropose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.",
        "updated": "2024-02-21 18:48:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14007v1"
    },
    {
        "title": "Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models",
        "authors": "Aline Ioste",
        "links": "http://arxiv.org/abs/2402.14002v1",
        "entry_id": "http://arxiv.org/abs/2402.14002v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14002v1",
        "summary": "Large Language Models with transformer architecture have revolutionized the\ndomain of text generation, setting unprecedented benchmarks. Despite their\nimpressive capabilities, LLMs have been criticized for generating outcomes that\ndeviate from factual accuracy or display logical inconsistencies, phenomena\ncommonly referred to as hallucinations. This term, however, has often been\nmisapplied to any results deviating from the instructor's expectations, which\nthis paper defines as attention misdirection rather than true hallucinations.\nUnderstanding the distinction between hallucinations and attention misdirection\nbecomes increasingly relevant in business contexts, where the ramifications of\nsuch errors can significantly impact the value extraction from these inherently\npre-trained models. This paper highlights the best practices of the PGI,\nPersona, Grouping, and Intelligence, method, a strategic framework that\nachieved a remarkable error rate of only 3,15 percent across 4,000 responses\ngenerated by GPT in response to a real business challenge. It emphasizes that\nby equipping experimentation with knowledge, businesses can unlock\nopportunities for innovation through the use of these natively pre-trained\nmodels. This reinforces the notion that strategic application grounded in a\nskilled team can maximize the benefits of emergent technologies such as the\nLLMs.",
        "updated": "2024-02-21 18:40:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14002v1"
    }
]