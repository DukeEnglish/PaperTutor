[
    {
        "title": "Meditating in Live Stream: An Autoethnographic and Interview Study to Investigate Motivations, Interactions and Challenges",
        "authors": "Jingjin LiJiajing GuoGilly Leshed",
        "links": "http://dx.doi.org/10.1145/3637417",
        "entry_id": "http://arxiv.org/abs/2402.13992v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13992v1",
        "summary": "Mindfulness practice has many mental and physical well-being benefits. With\nthe increased popularity of live stream technologies and the impact of\nCOVID-19, many people have turned to live stream tools to participate in online\nmeditation sessions. To better understand the practices, challenges, and\nopportunities in live-stream meditation, we conducted a three-month\nautoethnographic study, during which two researchers participated in\nlive-stream meditation sessions as the audience. Then we conducted a follow-up\nsemi-structured interview study with 10 experienced live meditation teachers\nwho use different live-stream tools. We found that live meditation, although\nhaving a weaker social presence than in-person meditation, facilitates\nattendees in establishing a practice routine and connecting with other\nmeditators. Teachers use live streams to deliver the meditation practice to the\nworld which also enhances their practice and brand building. We identified the\nchallenges of using live-stream tools for meditation from the perspectives of\nboth audiences and teachers, and provided design recommendations to better\nutilize live meditation as a resource for mental wellbeing.",
        "updated": "2024-02-21 18:24:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13992v1"
    },
    {
        "title": "What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience",
        "authors": "Dian LeiYao HeJianyou Zeng",
        "links": "http://arxiv.org/abs/2402.13939v1",
        "entry_id": "http://arxiv.org/abs/2402.13939v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13939v1",
        "summary": "With the widespread application of artificial intelligence(AI), the\nexplainable AI (XAI) field has undergone a notable resurgence. In this\nbackground, the importance of user experience in XAI has become increasingly\nprominent. Simultaneously, the user interface (UI) serves as a crucial link\nbetween XAI and users. However, despite the existence of UI design principles\nfor XAI, there is a lack of prioritization based on their significance. This\nwill lead practitioners to have a vague understanding of different design\nprinciples, making it difficult to allocate design space reasonably and\nemphasize design focal points. This paper aims to prioritize four design\nprinciples, providing clear guidance for UI design in XAI. Initially, we\nconducted a lightweight summary to derive five user experience standards for\nnon-expert users in XAI. Subsequently, we developed four corresponding webpage\nprototypes for the four design principles. Nineteen participants then\ninteracted with these prototypes, providing ratings based on five user\nexperience standards, and We calculated the weights of the design principles.\nOur findings indicate that, for non-expert users, \"sensitivity\" is the optimal\nUI design principle (weight = 0.3296), followed by \"flexibility\" (weight =\n0.3014). Finally, we engage in further discussion and summarization of our\nresearch results, and present future works and limitations.",
        "updated": "2024-02-21 17:07:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13939v1"
    },
    {
        "title": "Mask-up: Investigating Biases in Face Re-identification for Masked Faces",
        "authors": "Siddharth D JaiswalAnkit Kr. VermaAnimesh Mukherjee",
        "links": "http://arxiv.org/abs/2402.13771v1",
        "entry_id": "http://arxiv.org/abs/2402.13771v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13771v1",
        "summary": "AI based Face Recognition Systems (FRSs) are now widely distributed and\ndeployed as MLaaS solutions all over the world, moreso since the COVID-19\npandemic for tasks ranging from validating individuals' faces while buying SIM\ncards to surveillance of citizens. Extensive biases have been reported against\nmarginalized groups in these systems and have led to highly discriminatory\noutcomes. The post-pandemic world has normalized wearing face masks but FRSs\nhave not kept up with the changing times. As a result, these systems are\nsusceptible to mask based face occlusion. In this study, we audit four\ncommercial and nine open-source FRSs for the task of face re-identification\nbetween different varieties of masked and unmasked images across five benchmark\ndatasets (total 14,722 images). These simulate a realistic\nvalidation/surveillance task as deployed in all major countries around the\nworld. Three of the commercial and five of the open-source FRSs are highly\ninaccurate; they further perpetuate biases against non-White individuals, with\nthe lowest accuracy being 0%. A survey for the same task with 85 human\nparticipants also results in a low accuracy of 40%. Thus a human-in-the-loop\nmoderation in the pipeline does not alleviate the concerns, as has been\nfrequently hypothesized in literature. Our large-scale study shows that\ndevelopers, lawmakers and users of such services need to rethink the design\nprinciples behind FRSs, especially for the task of face re-identification,\ntaking cognizance of observed biases.",
        "updated": "2024-02-21 12:48:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13771v1"
    },
    {
        "title": "Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters",
        "authors": "Zechen BaiPeng ChenXiaolan PengLu LiuHui ChenMike Zheng ShouFeng Tian",
        "links": "http://arxiv.org/abs/2402.13724v1",
        "entry_id": "http://arxiv.org/abs/2402.13724v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13724v1",
        "summary": "Animating virtual characters has always been a fundamental research problem\nin virtual reality (VR). Facial animations play a crucial role as they\neffectively convey emotions and attitudes of virtual humans. However, creating\nsuch facial animations can be challenging, as current methods often involve\nutilization of expensive motion capture devices or significant investments of\ntime and effort from human animators in tuning animation parameters. In this\npaper, we propose a holistic solution to automatically animate virtual human\nfaces. In our solution, a deep learning model was first trained to retarget the\nfacial expression from input face images to virtual human faces by estimating\nthe blendshape coefficients. This method offers the flexibility of generating\nanimations with characters of different appearances and blendshape topologies.\nSecond, a practical toolkit was developed using Unity 3D, making it compatible\nwith the most popular VR applications. The toolkit accepts both image and video\nas input to animate the target virtual human faces and enables users to\nmanipulate the animation results. Furthermore, inspired by the spirit of\nHuman-in-the-loop (HITL), we leveraged user feedback to further improve the\nperformance of the model and toolkit, thereby increasing the customization\nproperties to suit user preferences. The whole solution, for which we will make\nthe code public, has the potential to accelerate the generation of facial\nanimations for use in VR applications.",
        "updated": "2024-02-21 11:35:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13724v1"
    },
    {
        "title": "Exploring users' sense of safety in public using an Augmented Reality application",
        "authors": "Maurizio VergariTanja KojićNicole Stefanie BertgesFrancesco VonaSebastian MöllerJan-Niklas Voigt-Antons",
        "links": "http://dx.doi.org/10.1109/QoMEX58391.2023.10178675",
        "entry_id": "http://arxiv.org/abs/2402.13688v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13688v1",
        "summary": "Nowadays, Augmented Reality (AR) is available on almost all smartphones\ncreating some exciting interaction opportunities but also challenges. For\nexample, already after the famous AR app Pokemon GO was released in July 2016,\nnumerous accidents related to the use of the app were reported by users. At the\nsame time, the spread of AR can be noticed in the tourism industry, enabling\ntourists to explore their surroundings in new ways but also exposing them to\nsafety issues. This preliminary study explores users' sense of safety when\nmanipulating the amount and UI elements visualization parameters of Point of\nInterest (POI) markers in a developed AR application. The results show that the\namount of POI markers that are displayed is significant for participants' sense\nof safety. The influence of manipulating UI elements in terms of transparency,\ncolor, and size cannot be proven. Nevertheless, most tested people stated that\nmanipulating transparency and size somehow influences their sense of safety, so\na closer look at them should be taken in future studies.",
        "updated": "2024-02-21 10:45:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13688v1"
    }
]