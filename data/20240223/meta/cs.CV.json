[
    {
        "title": "Corrective Machine Unlearning",
        "authors": "Shashwat GoelAmeya PrabhuPhilip TorrPonnurangam KumaraguruAmartya Sanyal",
        "links": "http://arxiv.org/abs/2402.14015v1",
        "entry_id": "http://arxiv.org/abs/2402.14015v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14015v1",
        "summary": "Machine Learning models increasingly face data integrity challenges due to\nthe use of large-scale training datasets drawn from the internet. We study what\nmodel developers can do if they detect that some data was manipulated or\nincorrect. Such manipulated data can cause adverse effects like vulnerability\nto backdoored samples, systematic biases, and in general, reduced accuracy on\ncertain input domains. Often, all manipulated training samples are not known,\nand only a small, representative subset of the affected data is flagged.\n  We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the\nimpact of data affected by unknown manipulations on a trained model, possibly\nknowing only a subset of impacted samples. We demonstrate that the problem of\ncorrective unlearning has significantly different requirements from traditional\nprivacy-oriented unlearning. We find most existing unlearning methods,\nincluding the gold-standard retraining-from-scratch, require most of the\nmanipulated data to be identified for effective corrective unlearning. However,\none approach, SSD, achieves limited success in unlearning adverse effects with\njust a small portion of the manipulated samples, showing the tractability of\nthis setting. We hope our work spurs research towards developing better methods\nfor corrective unlearning and offers practitioners a new strategy to handle\ndata integrity challenges arising from web-scale training.",
        "updated": "2024-02-21 18:54:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14015v1"
    },
    {
        "title": "Geometry-Informed Neural Networks",
        "authors": "Arturs BerzinsAndreas RadlerSebastian SanokowskiSepp HochreiterJohannes Brandstetter",
        "links": "http://arxiv.org/abs/2402.14009v1",
        "entry_id": "http://arxiv.org/abs/2402.14009v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14009v1",
        "summary": "We introduce the concept of geometry-informed neural networks (GINNs), which\nencompass (i) learning under geometric constraints, (ii) neural fields as a\nsuitable representation, and (iii) generating diverse solutions to\nunder-determined systems often encountered in geometric tasks. Notably, the\nGINN formulation does not require training data, and as such can be considered\ngenerative modeling driven purely by constraints. We add an explicit diversity\nloss to mitigate mode collapse. We consider several constraints, in particular,\nthe connectedness of components which we convert to a differentiable loss\nthrough Morse theory. Experimentally, we demonstrate the efficacy of the GINN\nlearning paradigm across a range of two and three-dimensional scenarios with\nincreasing levels of complexity.",
        "updated": "2024-02-21 18:50:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14009v1"
    },
    {
        "title": "Real-time 3D-aware Portrait Editing from a Single Image",
        "authors": "Qingyan BaiYinghao XuZifan ShiHao OuyangQiuyu WangCeyuan YangXuan WangGordon WetzsteinYujun ShenQifeng Chen",
        "links": "http://arxiv.org/abs/2402.14000v1",
        "entry_id": "http://arxiv.org/abs/2402.14000v1",
        "pdf_url": "http://arxiv.org/pdf/2402.14000v1",
        "summary": "This work presents 3DPE, a practical tool that can efficiently edit a face\nimage following given prompts, like reference images or text descriptions, in\nthe 3D-aware manner. To this end, a lightweight module is distilled from a 3D\nportrait generator and a text-to-image model, which provide prior knowledge of\nface geometry and open-vocabulary editing capability, respectively. Such a\ndesign brings two compelling advantages over existing approaches. First, our\nsystem achieves real-time editing with a feedforward network (i.e., ~0.04s per\nimage), over 100x faster than the second competitor. Second, thanks to the\npowerful priors, our module could focus on the learning of editing-related\nvariations, such that it manages to handle various types of editing\nsimultaneously in the training phase and further supports fast adaptation to\nuser-specified novel types of editing during inference (e.g., with ~5min\nfine-tuning per case). The code, the model, and the interface will be made\npublicly available to facilitate future research.",
        "updated": "2024-02-21 18:36:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.14000v1"
    },
    {
        "title": "BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions",
        "authors": "Mohammad Mahdi DehshibiDavid Masip",
        "links": "http://arxiv.org/abs/2402.13955v1",
        "entry_id": "http://arxiv.org/abs/2402.13955v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13955v1",
        "summary": "In this study, we investigate how environmental factors, specifically the\nscenes and objects involved, can affect the expression of emotions through body\nlanguage. To this end, we introduce a novel multi-stream deep convolutional\nneural network named BEE-NET. We also propose a new late fusion strategy that\nincorporates meta-information on places and objects as prior knowledge in the\nlearning process. Our proposed probabilistic pooling model leverages this\ninformation to generate a joint probability distribution of both available and\nanticipated non-available contextual information in latent space. Importantly,\nour fusion strategy is differentiable, allowing for end-to-end training and\ncapturing of hidden associations among data points without requiring further\npost-processing or regularisation. To evaluate our deep model, we use the Body\nLanguage Database (BoLD), which is currently the largest available database for\nthe Automatic Identification of the in-the-wild Bodily Expression of Emotions\n(AIBEE). Our experimental results demonstrate that our proposed approach\nsurpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving\nan Emotional Recognition Score of 66.33%.",
        "updated": "2024-02-21 17:35:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13955v1"
    },
    {
        "title": "Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning",
        "authors": "Antoine ChaffinEwa KijakVincent Claveau",
        "links": "http://arxiv.org/abs/2402.13936v1",
        "entry_id": "http://arxiv.org/abs/2402.13936v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13936v1",
        "summary": "Training image captioning models using teacher forcing results in very\ngeneric samples, whereas more distinctive captions can be very useful in\nretrieval applications or to produce alternative texts describing images for\naccessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval\nsimilarity score between the generated caption and the input image as reward to\nguide the training, leading to more distinctive captions. Recent studies show\nthat pre-trained cross-modal retrieval models can be used to provide this\nreward, completely eliminating the need for reference captions. However, we\nargue in this paper that Ground Truth (GT) captions can still be useful in this\nRL framework. We propose a new image captioning model training strategy that\nmakes use of GT captions in different ways. Firstly, they can be used to train\na simple MLP discriminator that serves as a regularization to prevent reward\nhacking and ensures the fluency of generated captions, resulting in a textual\nGAN setup extended for multimodal inputs. Secondly, they can serve as\nadditional trajectories in the RL strategy, resulting in a teacher forcing loss\nweighted by the similarity of the GT to the image. This objective acts as an\nadditional learning signal grounded to the distribution of the GT captions.\nThirdly, they can serve as strong baselines when added to the pool of captions\nused to compute the proposed contrastive reward to reduce the variance of\ngradient estimate. Experiments on MS-COCO demonstrate the interest of the\nproposed training strategy to produce highly distinctive captions while\nmaintaining high writing quality.",
        "updated": "2024-02-21 17:05:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13936v1"
    }
]