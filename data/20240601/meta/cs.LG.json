[
    {
        "title": "Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image",
        "authors": "Kailu WuFangfu LiuZhihan CaiRunjie YanHanyang WangYating HuYueqi DuanKaisheng Ma",
        "links": "http://arxiv.org/abs/2405.20343v1",
        "entry_id": "http://arxiv.org/abs/2405.20343v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20343v1",
        "summary": "In this work, we introduce Unique3D, a novel image-to-3D framework for\nefficiently generating high-quality 3D meshes from single-view images,\nfeaturing state-of-the-art generation fidelity and strong generalizability.\nPrevious methods based on Score Distillation Sampling (SDS) can produce\ndiversified 3D results by distilling 3D knowledge from large 2D diffusion\nmodels, but they usually suffer from long per-case optimization time with\ninconsistent issues. Recent works address the problem and generate better 3D\nresults either by finetuning a multi-view diffusion model or training a fast\nfeed-forward model. However, they still lack intricate textures and complex\ngeometries due to inconsistency and limited generated resolution. To\nsimultaneously achieve high fidelity, consistency, and efficiency in single\nimage-to-3D, we propose a novel framework Unique3D that includes a multi-view\ndiffusion model with a corresponding normal diffusion model to generate\nmulti-view images with their normal maps, a multi-level upscale process to\nprogressively improve the resolution of generated orthographic multi-views, as\nwell as an instant and consistent mesh reconstruction algorithm called ISOMER,\nwhich fully integrates the color and geometric priors into mesh results.\nExtensive experiments demonstrate that our Unique3D significantly outperforms\nother image-to-3D baselines in terms of geometric and textural details.",
        "updated": "2024-05-30 17:59:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20343v1"
    },
    {
        "title": "From Zero to Hero: Cold-Start Anomaly Detection",
        "authors": "Tal ReissGeorge KourNaama ZwerdlingAteret Anaby-TavorYedid Hoshen",
        "links": "http://arxiv.org/abs/2405.20341v1",
        "entry_id": "http://arxiv.org/abs/2405.20341v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20341v1",
        "summary": "When first deploying an anomaly detection system, e.g., to detect\nout-of-scope queries in chatbots, there are no observed data, making\ndata-driven approaches ineffective. Zero-shot anomaly detection methods offer a\nsolution to such \"cold-start\" cases, but unfortunately they are often not\naccurate enough. This paper studies the realistic but underexplored cold-start\nsetting where an anomaly detection model is initialized using zero-shot\nguidance, but subsequently receives a small number of contaminated observations\n(namely, that may include anomalies). The goal is to make efficient use of both\nthe zero-shot guidance and the observations. We propose ColdFusion, a method\nthat effectively adapts the zero-shot anomaly detector to contaminated\nobservations. To support future development of this new setting, we propose an\nevaluation suite consisting of evaluation protocols and metrics.",
        "updated": "2024-05-30 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20341v1"
    },
    {
        "title": "CoSy: Evaluating Textual Explanations of Neurons",
        "authors": "Laura KopfPhiline Lou BommerAnna HedströmSebastian LapuschkinMarina M. -C. HöhneKirill Bykov",
        "links": "http://arxiv.org/abs/2405.20331v1",
        "entry_id": "http://arxiv.org/abs/2405.20331v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20331v1",
        "summary": "A crucial aspect of understanding the complex nature of Deep Neural Networks\n(DNNs) is the ability to explain learned concepts within their latent\nrepresentations. While various methods exist to connect neurons to textual\ndescriptions of human-understandable concepts, evaluating the quality of these\nexplanation methods presents a major challenge in the field due to a lack of\nunified, general-purpose quantitative evaluation. In this work, we introduce\nCoSy (Concept Synthesis) -- a novel, architecture-agnostic framework to\nevaluate the quality of textual explanations for latent neurons. Given textual\nexplanations, our proposed framework leverages a generative model conditioned\non textual input to create data points representing the textual explanation.\nThen, the neuron's response to these explanation data points is compared with\nthe response to control data points, providing a quality estimate of the given\nexplanation. We ensure the reliability of our proposed framework in a series of\nmeta-evaluation experiments and demonstrate practical value through insights\nfrom benchmarking various concept-based textual explanation methods for\nComputer Vision tasks, showing that tested explanation methods significantly\ndiffer in quality.",
        "updated": "2024-05-30 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20331v1"
    },
    {
        "title": "Don't drop your samples! Coherence-aware training benefits Conditional diffusion",
        "authors": "Nicolas DufourVictor BesnierVicky KalogeitonDavid Picard",
        "links": "http://arxiv.org/abs/2405.20324v1",
        "entry_id": "http://arxiv.org/abs/2405.20324v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20324v1",
        "summary": "Conditional diffusion models are powerful generative models that can leverage\nvarious types of conditional information, such as class labels, segmentation\nmasks, or text captions. However, in many real-world scenarios, conditional\ninformation may be noisy or unreliable due to human annotation errors or weak\nalignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a\nnovel method that integrates coherence in conditional information into\ndiffusion models, allowing them to learn from noisy annotations without\ndiscarding data. We assume that each data point has an associated coherence\nscore that reflects the quality of the conditional information. We then\ncondition the diffusion model on both the conditional information and the\ncoherence score. In this way, the model learns to ignore or discount the\nconditioning when the coherence is low. We show that CAD is theoretically sound\nand empirically effective on various conditional generation tasks. Moreover, we\nshow that leveraging coherence generates realistic and diverse samples that\nrespect conditional information better than models trained on cleaned datasets\nwhere samples with low coherence have been discarded.",
        "updated": "2024-05-30 17:57:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20324v1"
    },
    {
        "title": "Vision-based Manipulation from Single Human Video with Open-World Object Graphs",
        "authors": "Yifeng ZhuArisrei LimPeter StoneYuke Zhu",
        "links": "http://arxiv.org/abs/2405.20321v1",
        "entry_id": "http://arxiv.org/abs/2405.20321v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20321v1",
        "summary": "We present an object-centric approach to empower robots to learn vision-based\nmanipulation skills from human videos. We investigate the problem of imitating\nrobot manipulation from a single human video in the open-world setting, where a\nrobot must learn to manipulate novel objects from one video demonstration. We\nintroduce ORION, an algorithm that tackles the problem by extracting an\nobject-centric manipulation plan from a single RGB-D video and deriving a\npolicy that conditions on the extracted plan. Our method enables the robot to\nlearn from videos captured by daily mobile devices such as an iPad and\ngeneralize the policies to deployment environments with varying visual\nbackgrounds, camera angles, spatial layouts, and novel object instances. We\nsystematically evaluate our method on both short-horizon and long-horizon\ntasks, demonstrating the efficacy of ORION in learning from a single human\nvideo in the open world. Videos can be found in the project website\nhttps://ut-austin-rpl.github.io/ORION-release.",
        "updated": "2024-05-30 17:56:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20321v1"
    }
]