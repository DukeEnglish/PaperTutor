4DHands: Reconstructing Interactive Hands in 4D with Transformers
DixuanLin1,2,YuxiangZhang2,MengchengLi2,YebinLiu2,WeiJing3,
QiYan3,QianyingWang3*,HongwenZhang1*
1BeijingNormalUniversity,2TsinghuaUniversity,3Lenovo
Figure 1. The proposed method, 4DHands, can robustly recover interactive hand meshes and their relative movement from monocular
inputs.
Abstract tion. Morevideoresultscanbefoundontheprojectpage:
https://4dhands.github.io.
In this paper, we introduce 4DHands, a robust ap-
proachtorecoveringinteractivehandmeshesandtheirrel-
ativemovementfrommonocularinputs. Ourapproachad- 1.Introduction
dresses two major limitations of previous methods: lack-
Hand mesh recovery from monocular inputs is a fun-
ing a unified solution for handling various hand image in-
damental technology for numerous advanced applications
putsandneglectingthepositionalrelationshipoftwohands
likevirtual/augmentedreality,human-computerinteraction,
within images. To overcome these challenges, we develop
robotics,andembodiedAI.Despitetheremarkableprogress
a transformer-based architecture with novel tokenization
achievedbyexistingmethods[27,29,53]inthisfield,chal-
and feature fusion strategies. Specifically, we propose a
lenges remain in addressing generalization under in-the-
Relation-aware Two-Hand Tokenization (RAT) method to
wildscenarios,capturingrobusthandmotionswithtempo-
embedpositionalrelationinformationintothehandtokens.
ralstabilityovertime,andrecoveringplausibleinteractions
In this way, our network can handle both single-hand and
betweentwohands.
two-handinputsandexplicitlyleveragerelativehandposi-
With the developments in transformer architectures, re-
tions,facilitatingthereconstructionofintricatehandinter-
cent state-of-the-art approaches [39] to monocular hu-
actionsinreal-worldscenarios. Assuchtokenizationindi-
man/handmeshrecoveryhaveshiftedtowardinvestigating
catestherelativerelationshipoftwohands,italsosupports
the usage of simple and high-capacity models along with
more effective feature fusion. To this end, we further de-
large-scale in-the-wild data. Despite the improvement in
velopaSpatio-temporalInteractionReasoning(SIR)mod-
2D alignment, existing approaches still can not robustly
ule to fuse hand tokens in 4D with attention and decode
handle close interaction and reconstruct hand meshes in
them into 3D hand meshes and relative temporal move-
the 4D space with stable temporal movement. Moreover,
ments. The efficacy of our approach is validated on sev-
existing approaches typically handle two-hand and single-
eralbenchmarkdatasets. Theresultsonin-the-wildvideos
hand scenarios differently. As the two-hand status fre-
andreal-worldscenariosdemonstratethesuperiorperfor-
quently switches between close interaction and separation
mances of our approach for interactive hand reconstruc-
inreal-worldscenes,theinconsistentprocessingprocedure
*Correspondingauthors. inevitably leads to clear jittering artifacts and temporal in-
1
4202
yaM
03
]VC.sc[
1v03302.5042:viXrastability. gleframesorsequences.
In this paper, we identify the three major limitations in
previoussolutions: i)thelackofaunifiedsolutiontohan- 2.RelatedWork
dle hand image inputs in various situations, ii) neglecting
2.1.MonocularHandMeshRecovery
the positional relationship between the hands within im-
ages, and iii) unstable estimation of the temporal trans- Recent years have witnessed great progress in hand mesh
lation of hands especially during the status shift between recovery from monocular RGB input. Following the ad-
interaction and separation. To address these issues, we vances in monocular human mesh recovery [20, 23, 24,
proposed 4DHands, a robust solution to recover interac- 45, 55, 56], some researchers [1, 2, 37, 39, 58] use neu-
tivehandmeshesandtheirrelativemovementsin4Dfrom ralnetworkstoregressthedrivingparametersofhandtem-
monocularinputs.Thekeyinsightofoursolutionistounify plates such as MANO [41] to recover hand meshes, while
the feature learning of two hands in various scenarios and others prefer to directly estimate the vertex coordinates
fullyleveragetheirpositionalrelationship. Toachievethis, of hand meshes through GCN [4–6, 14, 25, 44] or trans-
4DHands adopts a transformer-based architecture and in- former[29,30].
cludesnoveltokenizationand4Dfeaturefusionstrategies. Recently, there have also been some efforts to simulta-
Specifically, we first propose a Relation-aware Two- neouslyrecoverbothtwohandsofhumans. However, two
Hand Tokenization (RAT) method to embed positional re- hands mesh recovery is much more challenging than one
lationinformationofeachhandintoitstokens. Suchatok- hand,duetolargerposedistributionandmoresevereocclu-
enizationmethodbringsthefollowingbenefitsthreefold. i) sion. Furthermore,comparedtothelarge-scalesingle-hand
RAT is not reliant on the co-occurrence of two hands and datasets [39] with rich capture scenarios and postures, ex-
can thus handle both single-hand and two-hand inputs in istingtwo-handdatasetsaremuchfewer,andmostofthem
a unified manner. ii) RAT takes the overlapped status of arecapturedinlimitedenvironments.
twohandsintoaccountandadjuststhetokensaccordingly, A straightforward method for two-hand mesh recovery
whichallowsthenetworktolearnamoreconcentrateddis- is to locate and crop each hand separately from the input
tributionofthehandtokensregardlessofthevariousscales image,andthenconvertthetaskintosingle-handscenarios
and positions. iii) RAT explicitly encodes the relative po- byprocessingtwocroppedhandimagesindividually. This
sitionsoftwohandsintothetokens, facilitatingtherecon- strategyiswildlyusedinfull-bodyposeestimationframe-
structionofcloseandintricateinteractioninreal-worldsce- works[8,12,19,35,42,57,59]. However,thisapproachis
narios. After obtaining the image tokens, we further de- stillbasedonsingle-handrecoverynetwork,whichusually
velopaSpatio-temporalInteractionReasoning(SIR)mod- performs poorly under close interaction situations. More-
ule to infer hand poses and the relative positions between over,therelativetranslationbetweentwohandsisambigu-
thetwohands. Thankstotheproposedtokenizationmethod ouswhenthetwohandsareprocessedseparately.
RAT, SIR can fuse both the spatial and temporal informa-
2.2.HandMeshRecoveryunderInteraction
tionmoreeffectivelyasthehandtokensindicatetherelative
relationshipoftwohands,therebyenhancingtherobustness
To better address two-hands pose estimation and mesh re-
of hand mesh and temporal movement recovery in the 4D
coveryincloseinteractionscenarios,someresearchers[21,
space.
27,28,32,34,53,54,63]usedoneimagecontainingboth
Insummary, themaincontributionsofthisworkcanbe hands as input. Zhang et al. [54] utilized 2d joints projec-
listedasfollows: tion attention and context-aware refinement module to im-
• We introduce a Relation-aware Two-Hand Tokenization prove the pose accuracy. Li et al. [27] utilize transformer
(RAT)inourtransformer. Byembeddingpositionalrela- module [9] to learn implicit attention between two hands.
tioninformationinthetokenizationprocess,RATenables Zuoetal.[63]proposeadata-basedprioroftwointeracting
a generic and informative process of hand inputs across hands. Yuetal.[53]constructedtheattentionrelationship
diversescenarios,regardlessofwhetherthetwohandsare betweenthetwohandsbasedon2Dhandcentermaps. Al-
interactingorseparated. though the above approaches have produced promising re-
• We propose a Spatio-temporal Interaction Reasoning sults, therearestillsomelimitations. Becausethesemeth-
(SIR) module to fuse 4D hand features. The collabora- odsuseasingleimagecontainingbothhandsasinput,when
tionofRATandSIRfurtherenhancesthestabilityofhand thetwohandsarefarapart,theareaproportionofhandsin
meshandtemporalmovementrecoveryinthe4Dspace. theinputimagewillbetoosmall,whichcanreducetheac-
• Our final solution, 4DHands, achieves robust 4D hand curacyofmeshrecovery.Furthermore,theirnetworkdesign
mesh recovery in real-world scenarios, serving as a ver- inherentlylackstheabilitytorecoversingle-handmeshand
satile solution to handle hand inputs in various forms, onlycanoperateundertwo-handscenarios.
whether the images contain single or two hands, in sin- Therefore, we propose a new network that is compati-
2blewithbothsingle-handrecoveryandtwo-handrecovery. bethelocationoftheleftrootjointlocationwhenwesub-
Wetakeoneortwocroppedhandimagesasinput,enabling tractthejointsofbothhandsfromtherighthand’s3Droot
utilize large-scale single-hand datasets to boost two-hand joint location. Note that our method can robustly handle
recoveryresults. Wealsotokenizethe2Drelativedistance bothsingle-handandtwo-handinputs. Whenthereisonly
and overlap between two cropped hand images by a pow- a single hand visible in the images, our method is simply
erfultransformer[9,48]module,resultinginaccuraterela- degradedtotheestimationofthemeshofonehand.
tiveroottranslationestimationandinteractinghandsmesh
3.2.Relation-awareTwo-HandTokenization
recovery.
2.3.TemporalHandMeshRecovery Existingapproachestomonocularhandmeshrecoverytyp-
ically take the cropped hand images as input, but neglect
Severalmethodshavebeenproposedforhandmeshrecov- the hand relationship in the images. Besides, these ap-
ery from temporal sequence data [7, 13, 17, 22, 31, 52]. proacheshandletwo-handandsingle-handscenariosindif-
Hasson et al. [17] leverage the optical flow of nearby ferentmanners,resultinginjitteringartifactswhenthetwo
frames to constrain the motion of hands and objects by handsoperateininteractionandseparately. Tohandlethese
photometric supervision. SeqHAND [52] incorporates the issues in our transformer-based architecture, we propose a
convolution-LSTMlayertocapturethesequentialrelation- Relation-aware Two-Hand Tokenization (RAT) method to
ship between consecutive hand poses. Deformer [13] pro- taketherelationoftwohandsintoconsiderationandhandle
poses a novel dynamic fusion module that explicitly de- inputs in various scenarios. Following recent state-of-the-
forms nearby frames with clear hand visibility for robust artpracticesin3Dhuman/handmeshrecovery[15,39,51],
handposeestimationfromtemporalinput. VIBE[22]and we utilize ViT [9] as the visual encoder to tokenize image
TCMR [7] utilize the recurrent neural network to produce patches. In contrast to these methods, we handle both the
temporallyconsistentmotion. single-hand and two-hand inputs in a unified manner and
However, these methods rely solely on sequence data. considerembeddingpositionalrelationinformationofeach
Our approach introduces a Spatio-Temporal Interaction handintoitstokensduringthetokenizationprocess.
Reasoning(SIR)module, enablingourmethodtonotonly Toachievethis,wefirstlocateeachhandfrominputsand
recoveraccuratetwo-handinteractingmeshesfromsingle- centeritintothecorrespondingleftandrighthandimages.
frameinputbutalsoproducesmoothandstable4Dhandre- This strategy allows our method to learn a more concen-
coveryfromoccludedandmotion-blurredtemporalinput. trateddistributionofthehandtokensregardlessofthevar-
iousscalesandrelativepositions. Moreover,bytokenizing
3.Method
theimagesoftwohandsindividually,ourmethodcanmain-
tainthestabilityofinputsacrossvariousinteractionscenar-
InthisSection,wepresentthetechnicaldetailsof4DHands.
ios,eventuallyenhancingtherobustnessofoursolution. To
AsillustratedinFig2,4DHandsisatransformer-basednet-
compensateforthelackofrelationinformationinthesepa-
work, which takes hand images as input and estimates the
ratedtokens,wefurtherconsiderenrichingthehandtokens
hand meshes, as well as their relative position. In the pro-
withrelativepositionalembeddinginformationsothatthey
posed transformer, the key components include Relation-
can be aware of the relative position with respect to each
aware Two-hand Tokenization (RAT) and Spatio-temporal
handandtheinputimages.
Interaction Reasoning (SIR). As will be discussed shortly,
Specifically,foreachinputimage,wefollowViT[9]to
thetokenizationandreasoningstrategiesmake4DHandsa
segmentitintoH×W imagepatchesandsubsequentlyfeed
genericsolutiontohandlehandimagesinvariousforms,re-
them into the ViT backbone, resulting in a H × W × C
gardlessoftheimageswithsingleortwohands,inasingle
vision token map. To enrich each token map with rela-
frameorsequences.
tionship information, we further calculate relation maps
3.1.ProblemFormulation {R ,R } and embed them into the final image to-
R2L L2R
kens, so that they can contain both the patch-wise visual
Givenasingle-frameorsequentialimageinputsI,ourgoal
knowledge and patch-to-patch relation information. Basi-
is to reconstruct the hand meshes and their relative posi-
cally, the relation maps are the composition of the relative
tion. We leverage the widely used parametric hand model
distancemapandtheoverlappingmap. Withoutlosinggen-
MANO[41]. MANOdefinesafunctionM(θ,β)thattakes
twoparametersposeθ ∈ R48 andshapeβ ∈ R10 asinput, eralization, we describe the right-hand relation map R R2L
returns3DhandmeshV ∈ R778×3 and3Djointkeypoints indetail asfollows. The left-hand relationmap R L2R can
V ∈R21×3. TheoutputsofourmodelareMANOparame- becalculatedsimilarly.
tersoftwohands: θ ,θ ,β ,β ,bywhichwecanregress
R L R L
meshes and joints of each hand, and a vector υ to repre- RelativeDistanceMap Therelativedistancemapiscal-
sent the joint root distance of two hands. We define υ to culated based on the hand position in the original images.
3Relation-Aware Tokenization Spatio-Temporal Reasoning
Relation
Token Map Left Token Map
MLP
Feature Spatio Temporal
Fusion Decoder Decoder
Right Token Map
ViT Vision
Token Map
Input Images
Figure2. Overviewofour4DHandsframework. 4DHandsisatransformer-basednetwork,whichtakesmonocularimagesasinputand
estimatestwo-handmesheswiththeirrelativepositions.
Let B = [c ,c ,s ,s ] be the bounding box of one hand, theminthelastdimensionandusinganMLPtomatchthe
x y x y
c ,c thex-coordinateandy-coordinateofthecenterposi- dimensionwiththeimagefeaturedimensionC:
x y
tion,ands ,s theboxscalelengthsonx-axisandy-axis.
x y
WeobtainapositionmapC ∈RH×W×2,representingposi- R R2L =MLP(D R2L⊕D L2R⊕O R2L), (4)
tionsofthecenterineachimagepatch.Forthepatchp(i,k)
atthei-thcolumnandthek-throw,itsvalueis where ⊕ denotes the concatenation operation. Finally, the
relation map acts as the relative positional embedding and
(2i−W)·s (2k−H)·s enhancestheoriginalViTtokensF byconcatenatingthem
C(p)=[c + x,c + y]. (1) R
x 2W y 2H astherelation-awaretokensF+.
R
In our method, for each right hand and left hand, we first 3.3.Spatio-temporalInteractionReasoning
obtain their position maps C ,C and then calculate a rel-
R L After obtaining the image tokens, we further leverage a
ative distance map D ∈ RH×W×2. The relative distance
Spatio-temporalInteractionReasoning(SIR)moduletoin-
tokenatp(i,k)oftherighthandisformulatedas
fer the hand poses and the relative position between two
hands. Specifically, we flatten and concatenate the tokens
C (p)−C (p)
D R2L(p)=ψ(τ · R
s
L ) (2) of two hands to a sequence M∗ ∈ R2HW×C. The token
sequence is further fedinto attention modules to fuse both
wheres=(s ,s )isthescaleofright-handboundingbox, thespatialandtemporalinformation.
x y
ψ is an activation function, τ is a hyper-parameter used to
adjusttheactivationdistance. Inthisway, therelativedis- Adaptive Spatial Fusion To enhance the adaptability of
tance map is activated when two hands are close. In our our model, we first employ a self-attention transformer to
experiments,weuseSigmoidastheactivationfunctionψ. proceedwiththetokensequenceM∗.Astherelation-aware
tokensareembeddedwithbothvisualandrelationfeatures,
OverlappingMap Toexplicitlyrepresenttheoverlapped the self-attention module can adjust its weight adaptively
relationoftwohands,wefurtherintroducetheoverlapping basedonvisualandrelativepositionalassociativityandmit-
map. Denoting Inside(p(i,k),B) as a function that indi- igatetheinterdependenceofbilateralhandinformation.For
cates whether the center of patch p is inside the bound- instance, when relation-aware tokens indicate that the two
ing box B, we formulate the the overlapping map O ∈ hands are far away from each other, the self-attention can
RH×W×1as mainly interact with the tokens of each hand individually.
The outputs from the self-attention module are further fed
(cid:40)
1, Inside(p,B ) into a transformer decoder, resulting in a global feature G
O (p)= L (3)
R2L correspondingtoeachhand.
−1, otherwise
Given the above relative distance map and overlapping Temporal Fusion When dealing with the sequential in-
map, the relation map is calculated by first concatenating puts, the global features of each frame are concatenated
4alongthetemporaldimensiontoformthetemporalfeature With ground truth 3-D joint keypoints and camera pa-
{G }T ,whichwillbefurtherproceededbyatemporalde- rameters,wecanget2-Dkeypoints˜j = π˜(J˜)ontheorigi-
t t=1
coder. By fusing the temporal information, our model can nalimage. Thepredicted2-Dkeypointsarecalculatedwith
predictthehandposesofintermediateframesmorerobustly predicted camera parameter π. The 2-D projection loss is
in handling disturbances caused by object occlusions and formulated
dynamicblurring. Notethatourmethodisalsocompatible
with single-frame inputs. In this case, we stack the global L =maxMSE(π(J),˜j) (8)
2D
features repetitively to obtain the feature {G } and feed it
t
intothetemporaldecoder.
3.4.3 Inter-HandLosses
Afterthefeaturefusionofbothspatialandtemporaldi-
mensions,wecanobtaintheintegratedfeaturesG∗ andG∗
R L
of the right and left hand, respectively. Finally, these fea- WeutilizeInter-Handlossesinordertooptimizethefinger
tures are proceeded by an MLP-based regression network touchingdetailsoftwohands. Tocalculatetherelativedis-
to estimate the MANO parameters of two hands and their tancebetweenrighthandkeypointsandlefthandkeypoints,
relativeposition. werelocateallkeypointstosetthepalmjointofrighthand
ontheoriginalcoordinate. Wedenoteϕ (i,k) ∈ R3 asthe
3.4.Losses j
distancevectorfromthei−thrighthandjointtothek−th
As our method predicts two hands in a disentangled way, lefthandjoint,andϕ v(i,k) ∈ R3 asthedistancevectorof
weintroducesingle-handlossesandinter-handlossestoim- vertices,definedinthesameway.
prove the accuracy of single-hand reconstruction and two- To supervise the interacting gesture, for ground truth
handinteraction. joints and predicted joints, we calculate the distance from
each right-hand joint to each left-hand joint, respectively.
Weformulatethejoint-relationlossas:
3.4.1 maxMSELoss
Nj Nj
L =(cid:88)(cid:88) ||ϕ˜ (i,k)−ϕ (i,k)||2 (9)
Following [13], we use the maxMSE Loss to adjust the jrel j j
weights of each joint to maximize the MSE loss. Given i=1k=1
jointslocationsP∗,P,themaxMSElossisformulatedas
where N = 21 represents the number of joint keypoints.
j
Forafinerreconstructionincontactofhands,wesupervise
(cid:80)N ||P −P∗ ||4
maxMSE(P,P∗)= i=1 i i . (5) thecloseverticesdistancesas:
(cid:80)N ||P −P∗ ||2
i=1 i i
L
=(cid:88)Nv (cid:88)Nv (cid:40) ||ϕ˜ v(i,k)−ϕ v(i,k)||2, if||ϕ˜ v(i,k)||2 ≤α
WithmaxMSEloss,thejointsorverticeswithlargeerrors, close 0, if||ϕ˜ (i,k)||2 >α
typically the vertices of fingertips, will be assigned larger i=1k=1 v
(10)
lossweights.
whereN =778isthenumberofvertices,andαisahyper-
v
parameter,representingthedistancethreshold. Wesetα =
3.4.2 Single-HandLosses 0.005inexperiments.
4.Experiments
Forsingle-handalignment,wetrainourmodelwithMANO
parameter losses, 3D losses, and 2D losses. For a single 4.1.Implementationdetails
handimage,ourmodelpredictsitsposeθ,shapeβandcam-
era parameter π. With ground truth MANO parameters θ˜ We implement our model based on PyTorch [38]. We use
andshapeβ˜,weoptimizetheMANOparametersusingMSE a pretrained Vision Transformer following a huge design
(ViT-h)asthebackbonetoencodeimagefeatures. Theres-
loss:
L =||θ˜−θ||2+||β˜−β||2 (6) olutionofinputimageis256×192andthesizeofthefea-
mano
ture map is H = 16,W = 12,C = 1280. We use trans-
With the pose and shape parameters, we can obtain 3-D former encoders and decoders in our feature fusion mod-
vertices V and 3-D joint keypoints J. With accurate 3-D ules. We train our model using AdamW optimizer with a
groundtruthV˜ andJ˜,wecansupervisetheoutput3-Dkey- linearlearningratedecayschedulefromaninitiallearning
pointswith3-DlossusingmaxMSE rate1×10−5. Thelengthofanimagesequenceissetto9
with a gap of 5 frames to sample images in a consecutive
L =maxMSE(V˜,V)+maxMSE(J˜,J) (7) videosequence.
3D
54.2.Datasets Position Error (MRRPE) to measure the accuracy of rela-
tive positions of two hands. Following the prior method
Weuseatrainingdatasetwithamixupofinteractinghand
[27], weperformrootjointalignmentandskeletonscaling
datasets and single-hand datasets. The interacting hand
when evaluating MPJPE. In addition, we use acceleration
datasets are used to optimize the hand relations in cases
errorAccel Etoevaluatethetemporalconsistency,follow-
thattwohandsareclose. Thesingle-handdatasetsareused
ing[22].
tooptimizetheaccuracyofsingle-handmeshpredictionin
caseswheretwohandsarefaraway. 4.4.QualitativeResults
Interhand2.6M[34]isthefirstpubliclyavailabledataset
Our qualitative comparisons are shown in Figure 3. We
ofsequencesofinteractinghandswithaccurateannotations,
demonstrate results of in-the-wild datasets ARCTIC and
which is widely used for training and testing in two-hand
RenderIH, which are unseen in the training phrase. We
reconstruction methods. InterHand2.6M in 30 fps is for
comparewithIntagHand[27]andACR[53]incameraper-
video-related research and the 5 fps version is a subset of
spectiveandanarbitraryperspectivetoshowmoredetails.
the30fpsversionwhichremovestheredundancyforsingle
We show more results on realistic complex cross-hand in-
imageresearch. Inourmethod,wemakeuseof30fpsand
teractingsamplesin4. Theresultsdemonstratethestability
5 fps interacting two-hand (IH) data with human and ma-
of4D-Handsinreal-worldscenarios.
chine (H+M) annotations and filter invalid data following
[27]. Weultimatelyutilize1,614Kimagesfortrainingand 4.5.Comparisonwithstate-of-the-artmethods
601Kimagesfortesting.
To demonstrate the versatility and stability of 4DHands
Re:InterHand[36]hasimageswithrealisticanddiverse
across various scenarios, we conducted comparative anal-
appearancesalongwithaccurateGT3Dinteractinghands.
yseswithstate-of-the-artmethodsinmultipledomains,in-
It has 738K video-based images in 3rd-person viewpoints.
cluding temporal two-hand evaluation, frame-based two-
Wetakethewholedatasetfortraining.
hand evaluation, temporal single hand evaluation and in-
DexYCB [3] contains sequences of single hands inter-
the-wilddatasetevaluation.
acting with objects. It consists of 1000 sequences with
Comparison with Temporal Two Hand Methods. In
582Kimages. Althoughit’snotatwo-handdataset,weuse
Table 1 we compare the proposed 4D-Hands with previ-
it as part of the training set and testing set to enhance our
ous temporal methods on Interhand2.6m 30fps. The re-
model’sperformanceinscenarioswithobjectocclusionand
sultsshowthatourmethodfaroutperformstheexistingtwo-
motionblur.
handedtemporalmodels, with45.20%(7.37mmvs.13.45
Static Single Hand Datasets. To enhance the perfor-
mm)improvementinMPJPEand24.05%(2.81mm/s2 vs.
manceinwildvideos,weusesingle-handdatasetsthatpro-
3.70mm/s2)improvementinAccel E.
vide 3D hand annotations for training. Following the pre-
Comparison with Frame-based Two Hand Methods.
vious work [39], we use the datasets with a combination
Tocomparewithstate-of-the-artframe-basedmethodsand
of FreiHAND [62], HO3D [16], MTC [50], RHD [61],
demonstrate the performance in single frame input scenar-
H2O3D [16], DEX YCB [3], COCO WholeBody [18],
ios, we conduct experiments at Interhand2.6m 5fps by ex-
Halpe[11]andMPIINZSL[43].
pandingsingleimagetosequences. Ascanbeobservedat
In-the-wildDatasets. Weusein-the-wilddatasetsHIC
Table2,eventhoughwedropthetemporalknowledge,our
[47],ARCTIC[10]andRenderIH[26]toevaluatetheper-
methodstillkeephighperformanceandoutperformsprevi-
formanceonvariousscenarios. HICprovidesdiversehand-
ous methods that were specially designed for single frame
handinteractingandobject-handinteractingsequencesand
input scenarios, achieving 7.49 mm, 7.72 mm and 24.58
contains3DGTmeshesofbothhands. ARCTICisamulti-
mminMPJPE,MPVPEandMRRPE.
view sequential dataset of two-hand interacting with daily
use objects, involving occlusions and detailed interactive
Table1.Quantitativecomparisonswithframe-basedandtemporal
actions. RenderIH provides realistically rendered hands
methodsonInterhand2.6m30fpsdataset.
with a variety of poses, textures, backgrounds, and illumi-
nations,whichcansimulatevariousscenariosinreality.For Methods AccelE↓ MPJPE↓ MPVPE↓
SeqHand[52] 6.69 18.20 19.46
allin-the-wilddatasets,weonlyusethemfortestingtoeval- Zhaoet.al[60] 3.70 13.45 13.91
uatetheperformanceonunseenscenarios. Ours 2.81 7.37 7.58
4.3.Metrics
Comparison with Temporal Single Hand Methods.
WeadoptMeanPerJointPositionError(MPJPE)andMean To further exploit the universality of our method, we con-
PerVertexPositionError(MPVPE)andAreaUnderCurve duct experiments on sequential single hand dataset Dex-
(AUC) scores to measure the accuracy of the pose and YCB. We manage our input by flipping an input image as
shape of the estimated meshes, and Mean Relative-Root the opposite hand, and using far away bounding boxes to
6Input
Image Ours IntagHand ACR
Figure 3. Qualitative Comparison. We compare 4DHands with state-of-the-art methods on in-the-wild datasets ARCTIC[10] and
RenderIH[26]. ARCTIC provides two-hand object interacting images, and RenderIH provides interactive hands under different light-
ingconditions.Wehavedemonstratedtheresultsfromboththecameraperspectiveandanarbitraryperspective.Theresultsshowthatour
methodoutperformsthestate-of-the-artmethodsincomplexreal-worldenvironments.
indicatethehandsarenotinteracting. Wereporttheresults ingsinglehandtemporalmethodsinallmetrics. Itisnote-
of Root-Aligned and Procrustes-aligned metrics. It can be worthythatourleadinRoot-Alignedmetricsisgreaterthan
observed in Table 3 that our method outperforms all exist- those in Procrustes-aligned metrics (+38.63% vs. +9.00%
7Figure4. QualitativeResults. Weshowourmodel’sresultsoncomplexcross-handinteractionsinrealisticscenariostodemonstrateits
performance.Foradetailedperception,viewsofmultipleperspectivesaredemonstrated.
Table 2. Performance comparisons on the Interhand2.6m 5fps Table3. Quantitativecomparisonswithothersinglehandtemoral
dataset.Wecomparewiththemostrecentmethods. reconstructionmethodsontheDex-YCBdataset.
Methods R-MAJPE↓ R-JAUC↑ P-MPJPE↓ P-JAUC↑
VIBE[22] 16.95 67.5 6.43 87.1
Methods MPJPE↓ MPVPE↓ MRRPE↓
TCMR[7] 16.03 70.1 6.28 87.5
InterWild[33] 11.12 13.01 29.29 Tuetal.[46] 19.67 62.5 7.27 85.5
Renet.al[40] 10.49 10.26 28.98 MeshGraphormer[30] 16.21 69.1 6.41 87.2
IntagHand[27] 8.79 9.03 - Deformer[13] 13.64 74.0 5.22 89.6
MEMAHand[49] 8.65 8.89 - Ours 8.37 83.4 4.75 90.4
ACR[53] 8.41 8.53 -
Zuoet.al[63] 8.34 8.51 -
Ours 7.49 7.72 24.58
and40.5%,respectively,whileourmethodexhibitsamore
modest reduction of 24.43%. This demonstrates that our
inMPJPE,+12.70%vs.+0.89%inJAUC),indicatingthat
methodhasgreaterstabilityonunseendata.
wecanpredictaccurateshapesandrotationangleswithout
theassistanceofgroundtruthalignment.
ComparisononIn-the-wildDataset. Wereportthere- Table4. PerformancecomparisonsontheHICdataset. Wecom-
sultsonHICdataset,whichcontainsin-the-wildcross-hand parewithIntaghand[27]andInterWild[33].
data, to compare the performance at in-the-wild scenarios.
We only use samples in which both hands are visible. In Methods MPJPE↓ MPVPE↓ MRRPE↓
4 we compare the results with IntagHand and InterWild, Intaghand[27] 20.38 21.56 73.04
a SOTA method specifically designed for two-hand recov- InterWild[33] 15.62 15.17 26.43
eryinthewild. ComparingtotheMPJPEresultsonInter- Ours 9.32 9.93 25.26
hand2.6m, which is employed during training, the perfor-
manceofIntagHandandInterWilddeterioratesby131.8%
84.6.AblationStudy
Input Image Full Model w/o RAT
ViTBaseline. WetrainedabaselineusingViTandatrans-
formerdecoderonfulltrainingset,cross-handfeatureinter-
actionisnotemployedonthebaseline. Weshowtheresults
in5. AlthoughViThashighperformanceindetectingsin-
glehand,theocclusioncausedbyhandsinteractionreduces
its predictive ability. Besides, without cross-hand knowl-
edge, it separately predict positions of two hand, causing
hugeerrorsinrelativeposition.
Effectiveness of Cross-hand Information. We exper-
imented in adding cross-hand information in two ways:
(1) Input holistic image including two hands(+Cross H).
(2) Input separate images, each image contains a single
hand(+Cross S).In both way weadd attention transformer Figure 5. Ablation study of RAT. Two views of predicted hand
module to integrate cross-hand information, the results meshesarevisualizedtodemonstratedetailsofrelativeposition.
show that the last way achieves better accuracy. The main ’FullModel’isourmodelwithallmodules.’w/oRAT’meansthat
thepositionembeddingisremoved.
reason is that by separating two hands as input, the visual
encoder focuses on extracting single hand features, allow-
ing the ViT to learn more hand details when learning on
largescaledatasets.
Input
EffectivenessofRelation-awareTokenization. Byre- Sequence
movingRAT(+CrossSw/oRAT),themodellostglobalpo-
sition information during inference, which means that the
modelcouldonlyinfertwo-handrelativepositionfromvi-
Full
sualinformation. InInterHand2.6m,thetwohandsarerel- Model
atively close and the 2D scales of each hand are similar.
Therefore, even without a global position information, the
approximate positions of both hands can be inferred from w/o
Temporal
the input image. We test cases when two hands are rela-
Fusion
tivelyfarawayandcaseswhen2Dscalesofeachhandare
different. Asshownin5,withoutRAT,themodelperforms
poorinpredictingtwo-handrelativeposition. Figure6. Ablationstudyonocclusioncase. ’FullModel’isour
model with all modules. ’w/o Temporal Fusion’ means that the
EffectivenessofTemporalFusion. Byaddingtemporal
temporalintegratingmoduleisremoved.
information interacting(+Cross S + Seq), our method can
make sequential consistent prediction therefore infer rea-
sonable poses encountering occlusions, and achieves im-
5.Conclusions
provementof0.32mminMPJPE.InFigure6wevisualize
the predicted mesh sequences in the case of severe occlu- In this paper, we presented 4DHands, a robust solution
sion. It shows that the left hand is entirely occluded, the for recovering interactive hand meshes and their rela-
modelwithouttemporalfusionmakeserroneousprediction. tive movements from monocular inputs. 4DHands is a
transformer-based method with novel tokenization and
Ourfullmodelpredictsaconsistentsequencewithassistant
feature fusion strategies. To this end, we introduced the
informationfromthepastandthefuture.
Relation-aware Two-Hand Tokenization (RAT) method,
which embeds positional relation information into hand
Table5. AblationstudiesoftheRATandSIRmodulesonInter- tokens. Thisallowsournetworktohandlebothsingle-hand
Hand2.6M. and two-hand inputs, leveraging their relative positions
to reconstruct detailed hand interactions in real-world
Methods MPJPE↓ MPVPE↓ MRRPE↓ scenarios. Moreover, this tokenization approach enhances
Baseline 10.94 11.35 137.53 feature fusion by indicating the relative positions of the
+CrossH 8.78 9.01 28.65
hands. To further this goal, we developed a Spatio-
+CrossS 7.69 7.92 24.79
temporal Interaction Reasoning (SIR) module that fuses
+CrossSw/oRAT 7.81 8.00 25.81
+CrossS+Seq(Ours) 7.37 7.58 24.48 hand tokens in 4D with attention, decoding them into 3D
hand meshes and their temporal movements. The effec-
tiveness of 4DHands is demonstrated through extensive
9evaluations on benchmark datasets, with results from [11] Hao-ShuFang,JiefengLi,HongyangTang,ChaoXu,Haoyi
in-the-wildvideosandreal-worldscenariosshowcasingits Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu. Alpha-
superior performance in interactive hand reconstruction. pose: Whole-body regional multi-person pose estimation
and tracking in real-time. IEEE Transactions on Pattern
AnalysisandMachineIntelligence,2022. 6
References
[12] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios
Tzionas,andMichaelJ.Black. Collaborativeregressionof
[1] SeungryulBaek,KwangInKim,andTae-KyunKim. Push-
expressivebodiesusingmoderation. InInternationalCon-
ingtheenvelopeforrgb-baseddense3dhandposeestima-
ferenceon3DVision(3DV),2021. 2
tionvianeuralrendering. InProceedingsoftheIEEE/CVF
[13] QichenFu, XingyuLiu, RanXu, JuanCarlosNiebles, and
Conference on Computer Vision and Pattern Recognition
Kris M Kitani. Deformer: Dynamic fusion transformer
(CVPR),2019. 2
for robust hand pose estimation. In Proceedings of the
[2] AdnaneBoukhayma,RodrigodeBem,andPhilipHSTorr.
IEEE/CVF International Conference on Computer Vision,
3dhandshapeandposefromimagesinthewild. InCVPR,
pages23600–23611,2023. 3,5,8
2019. 2
[14] LiuhaoGe, ZhouRen, YunchengLi, ZehaoXue, Yingying
[3] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov,
Wang, JianfeiCai, and JunsongYuan. 3D handshape and
AnkurHanda, JonathanTremblay, YashrajS.Narang, Karl
poseestimationfromasingleRGBimage. InCVPR,2019.
VanWyk,UmarIqbal,StanBirchfield,JanKautz,andDieter
2
Fox. DexYCB:Abenchmarkforcapturinghandgraspingof
objects. InIEEE/CVFConferenceonComputerVisionand [15] ShubhamGoel, GeorgiosPavlakos, JathushanRajasegaran,
PatternRecognition(CVPR),2021. 6 AngjooKanazawa,andJitendraMalik. Humansin4d: Re-
[4] XingyuChen,YufengLiu,ChongyangMa,JianlongChang, constructing and tracking humans with transformers. In
HuayanWang,TianChen,XiaoyanGuo,PengfeiWan,and ProceedingsoftheIEEE/CVFInternationalConferenceon
WenZheng.Camera-spacehandmeshrecoveryviasemantic ComputerVision,pages14783–14794,2023. 3
aggregationandadaptive2d-1dregistration. InProceedings [16] ShreyasHampali,MahdiRad,MarkusOberweger,andVin-
oftheIEEE/CVFConferenceonComputerVisionandPat- centLepetit.Honnotate:Amethodfor3dannotationofhand
ternRecognition(CVPR),2021. 2 and object poses. In Proceedings of the IEEE/CVF con-
[5] Xingyu Chen, Yufeng Liu, Dong Yajiao, Xiong Zhang, ference on computer vision and pattern recognition, pages
ChongyangMa, YanminXiong, YuanZhang, andXiaoyan 3196–3206,2020. 6
Guo. Mobrecon: Mobile-friendlyhandmeshreconstruction [17] Yana Hasson, Bugra Tekin, Federica Bogo, Ivan Laptev,
from monocular image. In Proceedings of the IEEE/CVF MarcPollefeys,andCordeliaSchmid.Leveragingphotomet-
Conference on Computer Vision and Pattern Recognition ricconsistencyovertimeforsparselysupervisedhand-object
(CVPR),2022. reconstruction. InCVPR,2020. 3
[6] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee.
[18] ShengJin,LuminXu,JinXu,CanWang,WentaoLiu,Chen
Pose2mesh:Graphconvolutionalnetworkfor3dhumanpose
Qian, Wanli Ouyang, and Ping Luo. Whole-body human
andmeshrecoveryfroma2dhumanpose.InEuropeanCon-
pose estimation in the wild. In Computer Vision–ECCV
ferenceonComputerVision(ECCV),2020. 2 2020:16thEuropeanConference,Glasgow,UK,August23–
[7] HongsukChoi,GyeongsikMoon,JuYongChang,andKy- 28,2020,Proceedings,PartIX16,pages196–214.Springer,
oungMuLee. Beyondstaticfeaturesfortemporallyconsis- 2020. 6
tent3dhumanposeandshapefromavideo. InProceedings
[19] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap-
oftheIEEE/CVFconferenceoncomputervisionandpattern
ture:A3Ddeformationmodelfortrackingfaces,hands,and
recognition,pages1964–1973,2021. 3,8
bodies. InCVPR,2018. 2
[8] VasileiosChoutas, GeorgiosPavlakos, TimoBolkart, Dim-
[20] Angjoo Kanazawa, Michael J Black, David W Jacobs, and
itriosTzionas,andMichaelJ.Black. Monocularexpressive
Jitendra Malik. End-to-end recovery of human shape and
bodyregressionthroughbody-drivenattention. InEuropean
pose. InCVPR,pages7122–7131,2018. 2
ConferenceonComputerVision(ECCV),2020. 2
[21] DongUkKim,KwangInKim,andSeungryulBaek.End-to-
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
enddetectionandposeestimationoftwointeractinghands.
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
InICCV,2021. 2
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans- [22] Muhammed Kocabas, Nikos Athanasiou, and Michael J
formers for image recognition at scale. arXiv preprint Black. Vibe: Video inference for human body pose and
arXiv:2010.11929,2020. 2,3 shape estimation. In Proceedings of the IEEE/CVF con-
[10] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed ference on computer vision and pattern recognition, pages
Kocabas, Manuel Kaufmann, Michael J Black, and Otmar 5253–5263,2020. 3,6,8
Hilliges. Arctic: A dataset for dexterous bimanual hand- [23] NikosKolotouros,GeorgiosPavlakos,MichaelJBlack,and
objectmanipulation. InProceedingsoftheIEEE/CVFCon- Kostas Daniilidis. Learning to reconstruct 3d human pose
ferenceonComputerVisionandPatternRecognition,pages and shape via model-fitting in the loop. In ICCV, pages
12943–12954,2023. 6,7 2252–2261,2019. 2
10[24] Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani- [37] JoonKyu Park, Yeonguk Oh, Gyeongsik Moon, Hongsuk
ilidis. Convolutional mesh regression for single-image hu- Choi,andKyoungMuLee. Handoccnet: Occlusion-robust
manshapereconstruction. InCVPR,2019. 2 3dhandmeshestimationnetwork. InConferenceonCom-
[25] DominikKulon,RizaAlpGu¨ler,IasonasKokkinos,Michael puterVisionandPatternRecognition(CVPR),2022. 2
Bronstein,andStefanosZafeiriou.Weakly-supervisedmesh- [38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
convolutional hand reconstruction in the wild. In CVPR, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
2020. 2 Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
[26] LijunLi,LinruiTian,XindiZhang,QiWang,BangZhang, imperativestyle,high-performancedeeplearninglibrary. In
LiefengBo, MengyuanLiu, andChenChen. Renderih: A NeurIPS,2019. 5
large-scalesyntheticdatasetfor3dinteractinghandposees- [39] GeorgiosPavlakos,DandanShan,IlijaRadosavovic,Angjoo
timation. In Proceedings of the IEEE/CVF International Kanazawa, David Fouhey, and Jitendra Malik. Recon-
ConferenceonComputerVision,pages20395–20405,2023. structing hands in 3d with transformers. arXiv preprint
6,7 arXiv:2312.05251,2023. 1,2,3,6
[27] MengchengLi, LiangAn, HongwenZhang, LianpengWu, [40] Pengfei Ren, Chao Wen, Xiaozheng Zheng, Zhou Xue,
Feng Chen, Tao Yu, and Yebin Liu. Interacting attention Haifeng Sun, Qi Qi, Jingyu Wang, and Jianxin Liao. De-
graphforsingleimagetwo-handreconstruction.InProceed- couplediterativerefinementframeworkforinteractinghands
ingsoftheIEEE/CVFConferenceonComputerVisionand reconstruction from a single rgb image. In Proceedings of
PatternRecognition,pages2761–2770,2022. 1,2,6,8 the IEEE/CVF International Conference on Computer Vi-
[28] Mengcheng Li, Hongwen Zhang, Yuxiang Zhang, Ruizhi sion,pages8014–8025,2023. 8
Shao,TaoYu,andYebinLiu.HHMR:Holistichandmeshre- [41] Javier Romero, Dimitrios Tzionas, and Michael J. Black.
coverybyenhancingthemultimodalcontrollabilityofgraph Embodied hands: Modeling and capturing hands and bod-
diffusionmodels.InProceedingsoftheIEEEConferenceon iestogether. InSIGGRAPHAsia,2017. 2,3
ComputerVisionandPatternRecognition,2024. 2 [42] YuRong,TakaakiShiratori,andHanbyulJoo. Frankmocap:
[29] KevinLin,LijuanWang,andZichengLiu. End-to-endhu- Fastmonocular3Dhandandbodymotioncapturebyregres-
man pose and mesh reconstruction with transformers. In sionandintegration. InICCVW,2021. 2
CVPR,2021. 1,2 [43] Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser
[30] Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh Sheikh.Handkeypointdetectioninsingleimagesusingmul-
graphormer. InProceedingsoftheIEEE/CVFinternational tiviewbootstrapping.InProceedingsoftheIEEEconference
conference on computer vision, pages 12939–12948, 2021. on Computer Vision and Pattern Recognition, pages 1145–
2,8 1153,2017. 6
[31] Shaowei Liu, Hanwen Jiang, Jiarui Xu, Sifei Liu, and Xi- [44] XiaoTang,TianyuWang,andChi-WingFu. Towardsaccu-
aolong Wang. Semi-supervised 3d hand-object poses es- ratealignmentinreal-time3Dhand-meshreconstruction. In
timation with interactions in time. In Proceedings of the ICCV,2021. 2
IEEE conference on computer vision and pattern recogni- [45] YatingTian,HongwenZhang,YebinLiu,andLiminWang.
tion,2021. 3 Recovering3Dhumanmeshfrommonocularimages:Asur-
[32] HaoMeng,ShengJin,WentaoLiu,ChenQian,Mengxiang vey. IEEEtransactionsonpatternanalysisandmachinein-
Lin,WanliOuyang,andPingLuo. 3Dinteractinghandpose telligence,2023. 2
estimation by hand de-occlusion and removal. In ECCV, [46] ZhigangTu,ZhishengHuang,YujinChen,DiKang,Linchao
pages380–397.Springer,2022. 2 Bao,BishengYang,andJunsongYuan. Consistent3dhand
[33] GyeongsikMoon. Bringinginputstoshareddomainsfor3d reconstruction in video via self-supervised learning. IEEE
interacting hands recovery in the wild. In Proceedings of TransactionsonPatternAnalysisandMachineIntelligence,
theIEEE/CVFConferenceonComputerVisionandPattern 2023. 8
Recognition,pages17028–17037,2023. 8 [47] DimitriosTzionas, LucaBallan, AbhilashSrikantha, Pablo
[34] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, Aponte,MarcPollefeys,andJuergenGall. Capturinghands
andKyoungMuLee. Interhand2.6m: Adatasetandbase- inactionusingdiscriminativesalientpointsandphysicssim-
line for 3d interacting hand pose estimation from a single ulation.InternationalJournalofComputerVision,118:172–
rgb image. In Computer Vision–ECCV 2020: 16th Euro- 193,2016. 6
pean Conference, Glasgow, UK, August 23–28, 2020, Pro- [48] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
ceedings,PartXX16,pages548–564.Springer,2020. 2,6 reit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Il-
[35] GyeongsikMoon,HongsukChoi,andKyoungMuLee. Ac- lia Polosukhin. Attention is all you need. In Proceedings
curate 3d hand pose estimation for whole-body 3d human ofthe31stInternationalConferenceonNeuralInformation
meshestimation. InComputerVisionandPatternRecogni- ProcessingSystems,page6000–6010,RedHook,NY,USA,
tionWorkshop(CVPRW),2022. 2 2017.CurranAssociatesInc. 3
[36] Gyeongsik Moon, Shunsuke Saito, Weipeng Xu, Rohan [49] CongyiWang,FeidaZhu,andShileiWen. Memahand: Ex-
Joshi,JuliaBuffalini,HarleyBellan,NicholasRosen,Jesse ploiting mesh-mano interaction for single image two-hand
Richardson, Mallorie Mize, Philippe De Bree, et al. A reconstruction.InProceedingsoftheIEEE/CVFConference
datasetofrelighted3dinteractinghands.AdvancesinNeural on Computer Vision and Pattern Recognition, pages 564–
InformationProcessingSystems,36,2024. 6 573,2023. 8
11[50] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocu- ings of the IEEE/CVF International Conference on Com-
lartotalcapture: Posingface, body, andhandsinthewild. puterVision,pages9054–9064,2023. 2,8
InProceedingsoftheIEEE/CVFconferenceoncomputervi-
sionandpatternrecognition,pages10965–10974,2019. 6
[51] YufeiXu,JingZhang,QimingZhang,andDachengTao.Vit-
pose: Simple vision transformer baselines for human pose
estimation.AdvancesinNeuralInformationProcessingSys-
tems,35:38571–38584,2022. 3
[52] John Yang, Hyung Jin Chang, Seungeui Lee, and Nojun
Kwak. Seqhand: Rgb-sequence-based 3d hand pose and
shape estimation. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings,PartXII16,pages122–139.Springer,2020. 3,
6
[53] Zhengdi Yu, Shaoli Huang, Chen Fang, Toby P Breckon,
and Jue Wang. Acr: Attention collaboration-based regres-
sorforarbitrarytwo-handreconstruction. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages12955–12964,2023. 1,2,6,8
[54] Baowen Zhang, Yangang Wang, Xiaoming Deng, Yinda
Zhang, PingTan, CuixiaMa, andHonganWang. Interact-
ingtwo-hand3dposeandshapereconstructionfromsingle
colorimage. InProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision,pages11354–11363,2021.2
[55] HongwenZhang,YatingTian,XinchiZhou,WanliOuyang,
YebinLiu,LiminWang,andZhenanSun. PyMAF:3Dhu-
manposeandshaperegressionwithpyramidalmeshalign-
mentfeedbackloop. InICCV,2021. 2
[56] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and
ZhenanSun.Learning3Dhumanshapeandposefromdense
bodyparts. TPAMI,44(5):2610–2627,2022. 2
[57] HongwenZhang,YatingTian,YuxiangZhang,Mengcheng
Li,LiangAn,ZhenanSun,andYebinLiu. PyMAF-X:To-
wardswell-alignedfull-bodymodelregressionfrommonoc-
ularimages. TPAMI,2023. 2
[58] XiongZhang,QiangLi,HongMo,WenboZhang,andWen
Zheng. End-to-endhandmeshrecoveryfrom amonocular
rgbimage. InICCV,2019. 2
[59] YuxiangZhang,ZheLi,LiangAn,MengchengLi,TaoYu,
andYebinLiu.Light-weightmulti-persontotalcaptureusing
sparsemulti-viewcameras. InICCV,2021. 2
[60] Weichao Zhao, Hezhen Hu, Wengang Zhou, Li Li, and
HouqiangLi. Exploitingspatial-temporalcontextforinter-
acting hand reconstruction on monocular rgb video. ACM
Transactions on Multimedia Computing, Communications
andApplications,20(6):1–18,2024. 6
[61] ChristianZimmermannandThomasBrox. Learningtoesti-
mate3dhandposefromsinglergbimages.InProceedingsof
theIEEEinternationalconferenceoncomputervision,pages
4903–4911,2017. 6
[62] ChristianZimmermann, DuyguCeylan, JimeiYang, Bryan
Russell, Max J. Argus, and Thomas Brox. Freihand: A
datasetformarkerlesscaptureofhandposeandshapefrom
singlergbimages. InICCV,2019. 6
[63] Binghui Zuo, Zimeng Zhao, Wenqian Sun, Wei Xie, Zhou
Xue,andYangangWang. Reconstructinginteractinghands
withinteractionpriorfrommonocularimages. InProceed-
12