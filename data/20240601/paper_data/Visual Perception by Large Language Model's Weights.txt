Visual Perception by Large Language Model‚Äôs Weights
FeipengMa1‚àó,HongweiXue1,3‚Ä†,GuangtingWang2,YizhouZhou2‚Ä°,FengyunRao2
ShilinYan4,YueyiZhang1‚Ä°,SiyingWu5,MikeZhengShou3,XiaoyanSun1,5‚Ä°
1UniversityofScienceandTechnologyofChina 2WeChat,TencentInc.
3ShowLab,NationalUniversityofSingapore 4FudanUniversity
5InstituteofArtificialIntelligence,HefeiComprehensiveNationalScienceCenter
{mafp,xuehongwei}@mail.ustc.edu.cn
harryizzhou@tencent.com,{zhyuey,sunxiaoyan}@ustc.edu.cn
Abstract
ExistingMultimodalLargeLanguageModels(MLLMs)followtheparadigmthat
perceivesvisualinformationbyaligningvisualfeatureswiththeinputspaceof
LargeLanguageModels(LLMs),andconcatenatingvisualtokenswithtexttokens
toformaunifiedsequenceinputforLLMs. Thesemethodsdemonstratepromising
resultsonvariousvision-languagetasksbutarelimitedbythehighcomputational
effortduetotheextendedinputsequenceresultingfromtheinvolvementofvisual
tokens. Inthispaper,insteadofinputspacealignment,weproposeanovelparame-
terspacealignmentparadigmthatrepresentsvisualinformationasmodelweights.
Foreachinputimage,weuseavisionencodertoextractvisualfeatures,convert
featuresintoperceptualweights,andmergetheperceptualweightswithLLM‚Äôs
weights. In this way, the input of LLM does not require visual tokens, which
reducesthelengthoftheinputsequenceandgreatlyimprovesefficiency. Following
thisparadigm, weproposeVLoRAwiththeperceptualweightsgenerator. The
perceptualweightsgeneratorisdesignedtoconvertvisualfeaturestoperceptual
weightswithlow-rankproperty,exhibitingaformsimilartoLoRA.Theexperi-
mentalresultsshowthatourVLoRAachievescomparableperformanceonvarious
benchmarksforMLLMs,whilesignificantlyreducingthecomputationalcostsfor
bothtrainingandinference. Thecodeandmodelswillbemadeopen-source.
1 Introduction
Largelanguagemodels(LLMs)[44,54,61]haveachievedpromisingperformanceonmostnatural
languagetasksandhaveshowngreatgeneralizationabilityinsolvingreal-worldproblems. Derived
fromLLMs,multimodallargelanguagemodels(MLLMs)[4,34,45,52,59,62]takeasteptoward
artificialgeneralintelligence(AGI)byperceivingvisualinformationfromtherealworld. Therefore,
thewayofperceivingvisualinformationisthekeytomovingfromLLMtoMLLM.
Toperceivevisualinformation,recentMLLMsfollowaninputspacealignmentparadigmthataligns
visualfeatureswiththeinputspaceofLLMandconcatenatesvisualtokenswithtexttokenstoform
a unified sequence as input for LLM. For instance, LLaVA [34] uses CLIP-ViT-L-14 [47] as the
visualencoderandintroducesalinearprojectortoalignthevisualtokenswiththeinputspaceof
LLM.Monkey[29]dividesinputimagesintouniformpatchesandequipsindividualadaptersforeach
patchtohandlehigh-resolutionimages. Recentwork[53]alsoidentifiesthevisualshortcomingsof
‚àóThisworkwasperformedwhileFeipengMaandHongweiXuewereinternsatWeChat,TencentInc.
‚Ä†ProjectLeader.
‚Ä°Correspondingauthors.
Preprint.Underreview.
4202
yaM
03
]VC.sc[
1v93302.5042:viXraCLIPforMLLMsas‚ÄúCLIP-blindpairs‚Äùandintegratesvisionself-supervisedlearningfeatureswith
MLLMtoaddressthisissue. DeepSeek-VL[39]andSphinx[30]alsoadopthybridvisionencoders.
Vary[55]identifiesthatafixedvisionvocabularylimitsthedenseandfine-grainedvisualperception
andintroducesanewvocabularytoaddressthisissue.
DespitetheseeffortstoadvanceMLLMinvisualperception,theparadigmofinputspacealignment
remainsunchanged,whichcanresultincomputationalinefficiencyforbothtrainingandinference.
ThecomputationalcostofMLLMisconcentratedontheattentionmechanismofLLM,whichisO(n2)
whenthelengthoftheinputsequenceisn. UsingViT-L-14asthevisionencoder,a224√ó224low-
resolutionimagecanresultin256visualtokens,andthelengthincreasesto576whentheresolution
slightlyraisesto336√ó336. Consideringhigh-resolutionimages,someworks[11,29,30,33]split
animageintomultiplesub-imagesforcapturingfine-grainedinformation,leadingtoasignificantly
higher number of visual tokens. For instance, Sphinx-2k [30] adopts 2,890 visual tokens, while
InternLM-Xcomposer2-4KHD[11]evenusesupto8,737visualtokens. Concatenatingsuchalong
sequenceofvisualtokenstotexttokensresultsinadramaticincreaseincomputationaloverheadfor
bothtrainingandinference. Specifically,currentMLLMsareusuallypre-trainedonweb-crawled
image-textpairs,whichusuallyhaveveryshorttexts,withanaveragewordcountof10.95forLAION-
2B[48]and8.99forLAION-COCO[1]. Asaresult,thenumberofvisualtokensduringthepre-
trainingstageisabout20to50timesthenumberoftexttokens,whichsuggeststhattheinvolvement
ofvisualtokensseriouslyaffectstheefficiencyofthepre-training. Someworks[9,22,25]employ
resamplerstoreducethenumberofvisualtokenstoafixedcountbutstillfollowtheinputspace
alignmentparadigmandintroduceextravisualtokensforLLMs.
Toaddressthisissue,weexploreanovelparameterspacealignmentparadigmwherevisualinfor-
mationisrepresentedasLLM‚Äôsweights. AsshowninFig.1,foraninputimage,weuseavision
encodertoextractvisualfeatures.Then,thevisualfeaturesareconvertedtoperceptualweights,which
representvisualinformationasmodelweights. Theperceptualweightscanbedirectlymergedwith
LLM‚Äôsweights. Thus,thevisualinformationismergedintoLLMintheformofweights,eliminating
theneedforvisualtokensintheLLM‚Äôsinputandsignificantlyimprovingefficiency. Buildingonthis
paradigm,weintroduceVLoRA,whichcontainstheperceptualweightsgenerator. Theperceptual
weightgeneratorisdesignedtoconvertvisualfeaturestoperceptualweights. LLMsusuallycontaina
largenumberofparameters,forfeasibilityandefficiency,perceptualweightsaredesignedwitha
low-rankproperty. ThusthegeneratedperceptualweightsaresimilartotheformofLoRAweights.
Ourcontributionsaresummarisedasfollows:
1. WeexploreanovelparadigmforMLLMsthatalignsvisualfeatureswiththeparameter
spaceofLLMs,whichhighlyimprovestheefficiencyofMLLMs
2. Basedonthisparadigm,weproposeVLoRAanddesigntheperceptualweightsgenerator
thatgenerateslow-rankperceptualweights.
3. Experimentalresultsdemonstratetheeffectivenessandefficiencyofourapproach.Weobtain
resultscomparabletothoseofstate-of-the-artMLLMsonvariousbenchmarks,including
MMBench,ScienceQA,HallusionBench,andMMMU.
2 RelatedWorks
MultimodalLargeLanguageModels. CurrentMLLMsaredevelopedfromLLMsbyaligning
visualfeaturesintotheinputspaceofLLMs. Manyeffortshavebeenmadetoexploreintroducing
visual perception capability for LLMs. LLaVA [34] connects the visual encoder of CLIP to the
Vicuna[61]withalinearprojector. Furtherresearchthatfollowsthisparadigmfocusesonimproving
MLLMsfromtheperspectiveofvisionencoderandprojectorDeepSeek-VL[39]useSigLip[58]
toextracthigh-levelsemanticfeaturesanduseSAM-B[20]toprocesslow-levelfeatures. Tonget
al.[53]findsthatvisuallydistinctimagescanbeencodedassimilarduetotheshortcomingofCLIP
andintegratesvisionself-supervisedlearningfeatureswithCLIPfeatures. Sphinx[30]ensembles
variousvisionbackbonesthathavedifferentarchitectures,pre-trainingparadigms,andinformation
granularities. TheseworksinputtheentirevisualtokenssequenceintotheLLM,whichcanleadto
ahighcomputationalcostduringtrainingandinference. Specifically,LLaVA[32]andDeepSeek-
VL [39] utilize 576 visual tokens, Sphinx-2k [30] employs 2,890 visual tokens, and InternLM-
XComposer2-4KHD[11]usesupto8,737tokens. Someworksconsideradoptingcross-attention
2LLM LLM Decoder Layer
Decoder Layer Decoder Layer ùëä%& ùëä&
ùëä!
Decoder Layer Decoder Layer ùëä" ùëä# ùëä$
Vision Encoder Decoder Layer Decoder Layer
Perceptual Text
... ... ... Weights Tokens
LLM Visual
Projector How to make How to make this dish? Weights Tokens
... this dish? The addition of two weight matrices
(a) Visual feature extractor (b) Input space alignment (c) VLoRA: Parameter space alignment
Figure1: Overviewoftheinputspacealignmentandtheparameterspacealignmentparadigms.
TheinputspacealignmentparadigmisaligningvisualfeatureswiththeinputspaceofLLMand
concatenatingvisualtokenswithtexttokensasinputforLLM.OurproposedVLoRAfollowsthe
parameterspacealignmentparadigmthatalignsvisualfeatureswiththeparametersofLLMand
mergesperceptualweightsgeneratedbytheperceptualweightsgeneratorwithLLM‚Äôsweights.
architecture as the projector to improve efficiency. MiniGPT4-v1 [62] and BLIP series [9, 25]
adoptQ-Formerastheprojector,whichreducesthelengthofvisualtokenstoafixednumberof64.
Qwen-VL[5]usesasingle-layercross-attentionmoduleincorporatedwith2Dabsolutepositional
encodingstoavoidthepotentiallossofpositionaldetails. However,theseimprovementsstillfollow
theparadigmofaligningvisualfeaturestotheinputspaceofLLM,introducingextracomputational
overheadonLLMinference. Differentfrompreviouswork,ourVLoRAalignsvisualfeatureswith
theparameterspaceofLLM.Thevisualinformationcanberepresentedasperceptualweightsin
LoRAformatandmergedintoLLM‚Äôsweightsduringinference.
Parameter-EfficientFine-Tuning. Parameter-efficientfine-tuning(PEFT)isakeytechniqueforfine-
tuninglargepre-trainedmodels,includingLLMsandMLLMs. PEFTmethodsfreezethebackbone
andonlyfine-tuneasmallnumberofparameters,whichcanbetypicallycategorizedintothreeclasses:
adapters[16,46,51,60],prefix-tuning[24,27,36],andLow-RankAdaption(LoRA)[10,17,35]. In
thefieldoflanguagemodels,Houlsbyetal.[16]designbottleneckadaptersandinserttwoadapters
intothetransformerlayers,oneaftertheattentionmoduleandoneafterthefeed-forwardnetwork.
Prefix-tuning[27]prependsasetoflearnableprefixvectorsatthequeryandkeyoftheself-attention
moduleforeverylayer. Prompt-tuningproposestoonlyprependlearnablevectorstotheinputprompt
withnointermediate-layerprefixes. LoRA[17]useslearnablelow-rankmatricestoapproximate
thebackbone‚Äôsweightupdates,andthelow-rankmatricescanbemergedwiththebackboneduring
inferencewithoutextrainferenceburden. Consideringthepre-trainingstage,currentMLLMsusually
freezetheunimodalbackbonesandprojectvisualtokensthroughalearnableprojector,thenprepend
visualtokensintotheinputsequenceofLLMs,whichcanbeseenasprefix-tuningmethods. Our
VLoRAisclosertothestyleofLoRA.Specifically,VLoRAgenerateslow-rankperceptualweights,
whichcanbeseenasageneratedvisualparametersmatrix‚àÜW ‚ààRh√ór multipliedwithalearnable
A
matrix‚àÜW ‚ààRr√óh. SimilartoLoRA,theperceptualweightscanbeinjectedintoLLMs‚Äôweights
B
withoutintroducingextrainferenceoverhead.
3 Method
3.1 Preliminaries
Inthissubsection,wereviewthedetailsofthedecoderblockinthecurrentLLM.AsshowninFig.2,
thedecoderblockofLLMcontainsaself-attentionmoduleandafeed-forwardnetwork.
Self-attention. As shown in Fig. 2 (b), the self-attention module contains four types of linear
layers: queryW ‚àà Rh√ód,keyW ‚àà Rh√ód,valueW ‚àà Rh√ód,andoutputW ‚àà Rh√óh. Here,
Q K V O
hrepresentsthedimensionofthehiddenstatesofLLM,anddrepresentsthedimensionofeach
attention head. For each input token x ‚àà Rh in the input sequence X = (x ,x ,...,x ), it is
i 1 2 N
multipliedbylinearlayersW ,W ,W ,obtainingXq =XW ,Xk =XW andXv =XW .
Q K V Q K V
3
... ...
sthgieW
lautpecreP
rotareneGùëæùë∂ Linear
Add & Norm
Concat
Feed-forward Multi-head
Scaled Dot-Product
Attention
Add & Norm ùëæùüè
Self-attention
ùëæùë∏ ùëæùë≤ ùëæùëΩ
ùëæùüê
(a) Decoder Block of LLM (b) MultiheadSelf-Attention (c) Feed-forward Network
Figure2: DetailsoftheLLMDecoderBlock. (a)illustratesthedetailsoftheLLMdecoderblock,
includingthemulti-headself-attentionmoduleandthefeed-forwardnetwork. (b)providesadetailed
viewofthemulti-headself-attentionmodule,whichincorporatesfourtypesofweights: W ,W ,
Q K
W ,andW . (c)depictsthefeed-forwardnetwork,whichconsistsoftheweightsW andW .
V O 1 2
Then,theattentionoperationisexecutedalongthesequencedimensionasfollows:
XqXkT
Attention(Xq,Xk,Xv)=softmax( ‚àö )Xv. (1)
d
Theself-attentionmechanismisperformedoneachhead,andtheoutputsfromdifferentheadsare
concatenatedandmultipliedbyoutputlinearlayerwithweightsW .
O
Feed-forwardNetwork. AsshowninFig.2(c),thefeed-forwardnetworkisanMLPwithtwofully
connectedlayersandanon-linearactivationfunction. Theformulationcanbewrittenasfollows:
FFN(x )=œï(x W )W , (2)
i i 1 2
wherex istheinputtoken,œïistheactivationfunction,andW andW aretheweightsoftwofully
i 1 2
connectedlayers. Tosummarize,thedecoderblockofLLMhasfivetypesofweights,includingW ,
Q
W ,W ,W fromtheself-attentionmodule,andW ,W fromthefeed-forwardnetwork.
K V O 1 2
3.2 VisualPerceptionbyLLM‚ÄôsWeights
PreviousMLLMsfollowtheparadigmofaligningthevisualfeatureswiththeinputspaceofLLM
andrequireadditionalvisualtokensasLLM‚Äôsinput,whichcanleadtocomputationalinefficiency.
Thisinefficiencybecomesmorepronouncedwhenencounteringhigh-resolutionormultipleimagesas
thenumberoftokensincreasesdrastically. Toaddressthisissue,weproposetoalignvisualfeatures
withLLM‚ÄôsparameterspacewithoutintroducingextratokensintoLLM‚Äôsinput.
Toachievethisgoal,werepresentthevisualinformationoftheinputimageasperceptualweightsand
integratethemintotheweightsofLLM.ThisapproachallowsLLMtoperceivevisualinformation
withoutintroducingextratokensintotheinput.AsmentionedinSect.3.1,LLM‚Äôsdecoderblockshave
fivetypesofweights. WeuseW ‚ààRh√óhtodenotetheweightmatrixofLLM.Foraninputimage
I, wefirstadoptavisionencoderf(¬∑)toextractthevisualfeaturesz = f(I), wherez ‚àà Rc√ódv,
c is the number of visual tokens, and d is the dimension of visual features. Then, we design a
v
perceptualweightsgeneratorg(¬∑)toconvertthevisualfeaturestoperceptualweights‚àÜW ‚ààRh√óh.
Itisworthnotingthat,giventhatwewantLLMtoperceivevisualinformationwhilepreservingits
languagecapabilities,‚àÜW isalow-rankmatrix,whichalsohelpstoreducethecomputationcostof
theperceptualweightsgenerator. Withthegeneratedperceptualweights‚àÜW,wecandirectlymerge
itintotheLLM‚Äôsweightsas:
WÀÜ =W +‚àÜW. (3)
ByintegratingtheweightstransferredfromthevisualfeaturesintotheLLM‚Äôsweights,thevisual
perception ability is naturally equipped. After merging the weights, no extra inference burden
willbeintroducedforLLM.ForanyweightsineachdecoderblockofLLM,wecangeneratethe
correspondingperceptualweightsandintegratethemintoLLM‚Äôsweights.
4Linear Linear
√ó =
Linear
Perceptual Queries ùëä!‚àà‚Ñù"√ó$ ùëä%‚àà‚Ñù$√ó" ‚àÜùëä‚àà‚Ñù"√ó"
Linear
‚àÜùëä
x N LoRA: "
Visual Perceptual ‚àÜùëä
Parameters Weights !
(b) Equivalent form to LoRA,
(a) Perceptual Weights Generator where ‚àÜùëä !is learnable
Figure3:PerceptualWeightsGenerator.Figure(a)illustratesthepipelineofourperceptualweights
generator. Wesetklearnableperceptualqueries,whichinteractwithimagefeaturesinN decoder
blocks,andobtainkvisualparameters. Then,asharedlinearlayerandkindependentlinearlayers
areusedtoconvertthesevisualparameterstoperceptualweights‚àÜW. Figure(b)demonstratesthat
ourapproachisformallyconsistentwithLoRA.
3.3 PerceptualWeightsGenerator
Toconvertvisualfeaturestoperceptualweights‚àÜW ‚ààRh√óh,weproposetheperceptualweights
generator. SinceeachlayerandeachtypeofweightinLLMfocusondifferentvisualinformation,
ourperceptualweightsgeneratorneedstobeabletogenerateweightscorrespondingtoeachofthe
LLMweightsflexibly.
InspiredbyDETR[6]andBLIP-2[25],wedesigntheperceptualweightsgeneratorasadecoder-
only architecture with cross-attention layers to generate ‚àÜW ‚àà Rh√óh. As shown in Fig. 3 (a),
the perceptual weights generator contains N blocks, each comprising a self-attention module, a
cross-attentionmodule,andafeed-forwardnetwork. Thehiddenstatesdimensionoftheperceptual
weights generator is h , where h ‚â™ h¬∑h. We set k learnable perceptual quires corresponding
p p
tothenumberofdecoderblockswherewewanttoinsertperceptualweights. Foreachblock,the
perceptualqueriesfirstpassthroughtheself-attentionmodule,theninteractwithvisualfeaturesinthe
cross-attentionmodule,andfinallygothroughafeed-forwardnetwork. AfterN blocks,weobtain
kfeaturesp
v
‚àà Rhp. Thefeaturesp
v
shouldbemappedtothetargetshapeofperceptualweights
‚àÜW ‚àà Rh√óh. However, duetoh ‚â™ h¬∑h, directlymappingthedimensionsofthep fromh
p v p
toh¬∑hwithalinearlayercanintroducealargenumberofparameters,dramaticallyreducingthe
feasibility. Therefore,weconsiderintroducingthelow-rankpropertyinthisprocess. Weadopta
sharedlinearlayerW
share
‚ààRhp√óh¬∑r tomapallfeaturesp
v
fromh ptoh¬∑rasfollows:
W =p W , (4)
v v share
whereristherankforperceptualweightsandW ‚ààRh¬∑r isvisualparameter.
v
AndwereshapetheoutputW ash√ór.Whenascendingtothetargetdimensionh√óh,kindependent
v
linearlayersW ‚ààRr√óhareusedforeachvisualparameterandobtainkperceptualweights‚àÜW,
s
thisprocesscanbeformulatedasfollows:
‚àÜW =W W . (5)
v s
SubstitutingEq.(5)intoEq.(3),weget:
WÀÜ =W +‚àÜW =W +W W . (6)
v s
Consideringthelow-rankpropertyofW andW ,wecanobservethatEq.(6)andLoRA[17]areof
v s
thesameform,whereW correspondsto‚àÜW andW correspondsto‚àÜW .AsillustratedinFig.3
v A s B
(b),ourperceptualweightsgeneratorcanbeseenas‚ÄúLoRAweightsgenerator‚Äùfromtheperspective
ofLoRA.Thisisbecauseitgenerates‚àÜW and‚àÜW forweightsofLLM.Ourperceptualweights
A B
generatorgeneratesonetypeofperceptualweightsforkdecoderblocksatatime. Forgenerating
multipletypesofweights,weemploymultipleperceptualweightsgenerators.
5
Encoder
Visual
Self-Attention Cross-Attention Feed-Forward Shared
Linear
...8% of LLaVA-v1.5's
Figure4:ComparisonofFLOPs.ThisfigureshowstheFLOPsofLLaVAandVLoRAwithdifferent
numbersofinputvisualtokens. TheleftsubplotillustratesthechangeinGFLOPs,therightsubplot
plotstheratioofGFLOPsforVLoRAtoLLaVA,andCdenotesthenumberoftexttokens.
3.4 AnalysisoftheComputationalCost
BynotintroducingadditionalvisualtokensintheinputoftheLLM,ourVLoRAachieveshigher
computational efficiency for both training and inference. We only consider the computational
cost of LLM, as the computational overhead of our perceptual weights generator is negligible
in comparison. We assume the LLM has d blocks and hidden states dimension of h, the input
text length is C, and the number of visual tokens is L. For convenience, we only consider the
computational cost of the self-attention module and feed-forward network in LLM. The FLOPs
of the self-attention module and the feed-forward network are 8Lh2 + 4L2h and 16Lh2. For
previous MLLMs that align visual features to the input space of LLM, the FLOPs of LLM are
24(L+C)dh2 +4(L+C)2dh. ForourVLoRA,theextracomputationalcostoccursinEq.(6),
where ‚àÜW is multiplied with ‚àÜW . Assuming that we generate perceptual weights for all 5
A B
types of weighs in k decoder blocks. During training, we do not merge the perceptual weights
withtheLLMweightsbutusethemasbranchesoftheLLMweights. Therefore, theFLOPsare
24Cdh2 +4C2dh+24krh2 +12Ckh2 +14Ckh. For inference, the perceptual weights can be
mergedintotheLLM,andtheFLOPsare24Cdh2+4C2dh+24krh2+12kh2.DetailsoftheFLOPs
calculationareintheAppendixA.Thereisasmallincreaseintheoverheadoftrainingcomparedto
inference,andwecomparebythetrainingFLOPs. InFig.4,wecomparetheFLOPsofLLaVAand
VLoRA.Ourapproachdoesnotintroduceadditionalcomputationasthenumberofvisualtokens
increases,andourFLOPsareonly8%ofLLaVA-v1.5‚Äôswhenthetextlengthis32.
4 Experiments
4.1 ImplementationDetails
ModelSettings. WeuseVicuna-7b-v1.5[61]asourfoundationalLLMandCLIP-ViT-L-14[47]as
visionencoder. Theperceptualweightsgeneratorisinitializedrandomly. Fortheperceptualweights
generator,wesetthehiddensizeh as512,andthenumberofblocksN as8.Therankrofperceptual
p
weightsis64. Thenumberofperceptualqueriesis8,whichmeansthatweinsertperceptualweights
‚àÜW onlyon8blocks,andintheimplementation,forVicuna-7b-v1.5with32blocks,weinsert‚àÜW
every4blocks.Forbettervisualperceptualability,weinsert‚àÜW forallfivetypesofweightsinLLM.
Itisworthnotingthatthelastklinearlayersoftheperceptualweightsgeneratorarezero-initializedas
theyareequivalenttothe‚àÜW ofLoRAweights,whichareinitializedaszerofortrainingstability.
B
Pre-trainingData. Duringpre-training,weuseimage-textpairstotrainourmodel. Specifically,we
useasubsetofCapsFusion-120M[56]with30millionimage-textpairs. CapsFusion-120Mrandomly
collectsimage-textpairsfromLAION-COCO[1],whichcontainsbothweb-crawledandsynthetic
captionsgeneratedbyBLIP[26]. Then,afine-tunedLLMisusedtointegratebothtypesofcaptions.
Pre-trainingConfiguration. WefreezetheweightsofLLMandvisualencoderinthepre-training
stage,makingonlytheperceptualweightsgeneratortrainable. WeusetheAdamW[38]optimizer
withalearningrateof5e-5,whichfollowsalinearwarm-upandthenacosinedecayschedule. The
6Table1: ComparisonsonsixMLLMbenchmarks,includingMMBench,MME,ScienceQA,Hallu-
sionBench,MMMU,andCCBench. vis. tok. denotesthenumberofvisualtokensinvolvedinthe
LLM.Boldednumbersindicatethebestresults,andunderlinednumbersarethesecond-bestresults.
GFLOPsdenotestheoverheadoftheLLMpartwhenthenumberofinputtexttokensis32.
Model Size #vis.tok. GFLOPs MMBench MME ScienceQA HallusionBench MMMU CCBench
InstructBLIP[9] 8B 32 827 36.0 1137.1 54.7 31.2 30.6 12.7
MiniGPT-4-v1[62] 7B 32 827 12.2 770.6 39.0 31.9 23.6 1.8
MiniGPT-4-v2[7] 7B 256 3754 24.3 708.4 54.1 30.0 25.0 1.4
Idefics-instruct[23] 9B 64 1362 48.2 942 51.6 27.3 18.4 7.8
OpenFlamingov2[3,4] 9B 64 1362 6.6 535 45.7 29.4 28.2 6.3
Qwen-VL[5] 9.6B 256 3754 38.2 334.1 57.7 29.9 29.6 6.1
Qwen-VL-Chat[5] 9.6B 256 3754 60.6 1467.8 65.5 36.8 37.0 41.2
LLaVA-v1.5[32] 7.2B 576 8027 64.3 1510.7 66.8 27.6 35.7 27.5
VLoRA 7.8B 0 619 63.4 1311.3 66.4 26.4 36.0 28.6
pre-trainingisconductedwithatotalbatchsizeof768for40,000iterations. Theinputimagesare
resizedtoaresolutionof336√ó336. Thepre-trainingstageuses24NVIDIAH800GPUsfor7hours.
Fine-tuning Data. For supervised fine-tuning, we adopt the same data as LLaVA-v1.5. Specifi-
cally, thesupervisedfine-tuningdataisconstructedwithVQAv2[13], GQA[18], OKVQA[42],
OCRVQA [43], A-OKVQA [49], TextCaps [50], RefCOCO [19, 41], Visual Genome [21],
ShareGPT[2],andLLaVA-Insturct[34],withatotalof665Kconversationdata.
Fine-tuningConfiguration. Duringthefine-tuningstage,wefreezethevisionencoderandupdate
theweightsoftheperceptualweightsgeneratorandLLM.Thelearningrateissetto5e-5andthe
learningratescheduleisthesameasinthepre-trainingstage. Theglobalbatchsizeis128. Wetrain
foroneepochon8NVIDIAH800GPUs,whichtakes2hours.
4.2 BenchmarksforEvaluation
MMBench&CCBench. MMBench[37]isacomprehensivemultimodalbenchmarkdesignedto
evaluatetheperformanceofMLLMs. Itincludesover3,000multiple-choicequestionscovering20
abilitycategories. Theevaluationisdividedintoperceptualandreasoningdimensionsandsubdivided
into 20 categories. CCBench [37], released by the MMBench team, is designed for evaluating
MLLMsinthedomainofChineseCulture.
MME.MME[12]alsomeasurestheadvancedMLLMsintermsofperceptionandcognition,witha
totalof14subtasks. TominimizetheinfluenceofpromptengineeringonMLLMs,theinstructionsof
MMEaredesignedassimplebinaryresponses: ‚Äúpleaseansweryesorno".
ScienceQA. ScienceQA [40] is constructed from elementary and high school science curricula.
QuestionsofScienceQAspanthreesubjects: naturalscience,languagescience,andsocialscience.
WeusesampleswithimagesfromthevalidationsettoevaluateMLLMs.
HallusionBench. HallusionBench[14]isdesignedforevaluatingimage-contextreasoning,including
346imagespairedwith1129questionscraftedbyhumanexperts. Unlikeotherbenchmarks[15,28,
31]thatfocusonobjecthallucinationswithlimitedtopicsandvisualinputtypes,HallusionBench
considersbothlanguagehallucinationsandvisualillusionsacrossadiverserangeoftopics.
MMMU. MMMU [57] collects 11.5K multimodal questions from college exams, quizzes, and
textbooks,coveringsixcoredisciplines,spanning30subjectsand183subfields,andcomprising
30heterogeneousimagetypes. MMMUismorechallengingthanexistingbenchmarksduetothe
demandforcollege-leveldomain-specificknowledge.
4.3 ComparisonwithState-of-the-arts
Tab.1comparesourVLoRAwithotherstate-of-the-artMLLMsonsixMLLMbenchmarks. The
resultsareobtainedfromOpenCompass[8]. UnlikeotherMLLMs,ourVLoRAdoesnotrequireany
visualtokensduringLLMinferenceandhasonly8%ofthecomputationaloverheadofLLaVA-v1.5
when the text length is 32. On most benchmarks, VLoRA outperforms InstructBLIP, MiniGPT-
4, Idefics-instruct, and OpenFlamingo v2. Compared with Qwen-VL-Chat pre-trained on 1.4B
image-textpairs,VLoRAhasahigherscoreof3.7onMMBenchand1.3onScienceQA.Compared
withLLaVA-v1.5,VLoRAcanachievecomparableperformanceonMMBench,ScienceQA,and
7Table2: ComparisontoLLaVA-v1.5withvarioussettingsonsixMLLMbenchmarks,including
MMBench,MME,ScienceQA,HallusionBench,MMMU,andCCBench. PTdatarepresentsthe
pre-trainingdata. vis. tok. denotesthenumberofvisualtokensinvolvedinLLM.
Model PTdata #vis.tok. MMBench MME ScienceQA HallusionBench MMMU CCBench
LLaVA-7b-v1.5 blip-558k 576 64.3 1510.7 66.8 27.6 35.7 27.5
LLaVA-7b-v1.5 CapsFus-30m 576 64.6 1470.0 67.7 27.4 33.8 25.3
LLaVA-7b-v1.5-QFormer CapsFus-30m 128 60.7 1241.5 67.3 26.7 33.8 25.3
VLoRA CapsFus-30m 0 63.4 1311.3 66.4 26.4 36.0 28.6
Table3: Theimpactofweightstypethatequippedperceptualweights. q,k,v,andodenotethequery,
key,value,andoutputweightsintheself-attentionmodule,respectively. mdenotestheweightsof
thefeed-forwardnetwork.
Weightstype MMBench MME ScienceQA HallusionBench MMMU CCBench
qkvom 63.4 1311.3 66.4 26.4 36.0 28.6
qkvm 59.6 1227.5 64.6 23.4 34.7 24.9
qkv 59.4 1267.9 65.8 23.2 33.9 28.8
qko 57.2 1240.5 64.0 23.4 34.6 24.9
qk 53.3 1169.8 65.0 23.5 36.7 21.8
HallusionBenchandevenbetterperformanceonMMMUandCCBench. However,theresultson
MMEfallshortofLLaVA-v1.5sinceourperceptualweightsgeneratorisrandomlyinitializedand
necessitatesmoreimage-textpairdataduringthepre-trainingstage. Toverifythis,inTab.2,we
reproduceLLaVA-v1.5byreplacingtheprojectorwitharandomlyinitializedQ-Formerandachieve
similarresultsonMME.OurVLoRAachievescomparableperformancetostate-of-the-artMLLMs
withoutintroducingvisualtokensasLLMinputs,drasticallyreducingcomputationaloverhead.
5 AblationStudy
Currently,theperformanceofMLLMsissignificantlyaffectedbythefoundationalLLMsandthe
trainingdata,includingpre-trainingdataandsupervisedfine-tuningdata. Toexploretheeffectiveness
of our proposed paradigm and model, we perform a fair comparison with LLaVA-v1.5 [34] by
adoptingthesamefoundationLLMandtrainingdatainthissection. Then,withthissetting,wealso
exploretheimpactofdifferentsettingsofeachcomponentonperformance.
5.1 ComparisonwithLLaVA-v1.5
ToensureafaircomparisonwithLLaVA-v1.5,wereproduceLLaVA-v1.5withthesamesettingas
ourVLoRA,includingthepre-trainingandsupervisedfine-tuningdata. Furthermore,toeliminate
theinfluenceofthedifferenceintheprojector,wereplacetheprojectofLLaVA-v1.5asarandomly
initializedQ-Former,whichhasthesamenumberofblocksandhiddensizeasourperceptualweights
generator. Thetrainingisconductedusingthesamepre-trainingandfine-tuningdataasVLoRA.
InTab.2,thesecondrowistheresultsofLLaVA-v1.5pre-trainingonCapsFus-30m. Withmore
pre-trainingdata,LLaVA-v1.5doesn‚ÄôtachievesignificantimprovementonMLLMbenchmarksbut
ratheradroponMME,HallusionBench,MMMU,andCCBench. OurVLoRAisstillcomparable
withtheLLaVA-v1.5trainingonthesamedata. ThethirdrowistheresultsofLLaVA-v1.5with
Q-Former,whichispre-trainedonCapsFus-30m. Wesetthenumberoflearnablequeriesas128,thus
thenumberofvisualtokensis128.ExceptforbeingslightlylowerinScienceQAandHallusionBench,
ourVLoRAissignificantlybetteronotherMLLMbenchmarks. Theseresultsdemonstratethatour
approachiscomparabletoorevenbetterthanLLaVA-v1.5withconsistentsettings.
5.2 Analysisofeachcomponent
TofurtheranalyzeVLoRA,weexploretheimpactofeachcomponent,includingthetypeofweights
that equipped perceptual weights, the rank of perceptual weights, and the number of blocks of
perceptualweightsgenerator.
8Table 4: The impact of perceptual weights‚Äô rank. The rank of the generated perceptual weights
indicatestheextentofvisualinformationcompression.
Rank MMBench MME ScienceQA HallusionBench MMMU CCBench
r =16 59.4 1212.7 67.1 22.9 39.3 24.5
r =32 60.7 1235.6 67.2 23.5 36.0 25.3
r =64 63.4 1311.3 66.4 26.4 36.0 28.6
r =128 61.0 1228.4 68.0 23.8 33.4 26.7
Table5: Theimpactofdifferentnumbersofblocksofperceptualweightsgenerator.
Blocks MMBench MME ScienceQA HallusionBench MMMU CCBench
N =4 60.7 1289.3 63.9 24.4 32.0 26.7
N =8 63.4 1311.3 66.4 26.4 36.0 28.6
N =12 61.3 1289.3 67.1 25.5 34.7 30.2
Thetypeofweightsthatequippedperceptualweights. AswementionedinSect.3.1,thereare
fivetypesofweightsinthedecoderblockofLLM,whicharequery,key,value,output,andmlp. We
exploretheimpactofinsertingperceptualweightsfordifferenttypesofLLMweights. Asshown
inTab.3,wecomparedifferentcombinations,includingqkvom,qkvm,qkv,qko,andqk. Themodel
thatequippedperceptualweightsforalltypesofweightscanachievethebestperformanceonmost
benchmarks. Wenoticethattheperformanceofqkvismuchbetterthanqk. Thissuggeststhatthe
valuematrixisessentialforvisualperceptionsincetheoutputofthevaluematrixwillbeweighted
andsummed,involvingtheresultsoftheself-attentionmodule.
The rank of perceptual weights. The rank of the generated perceptual weights represents the
degreeofvisualinformationcompression. Thesmallertherank,themorecompressedthevisual
information. Wecomparetheperformanceofrankrfrom16to128inTab.4. Whenther =16,the
visualinformationiscompressedseverelyinperceptualweights. However,LLMwithsuchlow-rank
perceptualweightscanstillperceivevisualinformation. Fromr =16tor =64,theperformance
onMMBench,MME,HallusionBench,andCCBenchimproveswithincreasingrank. Specifically,
thescoreofMMBenchincreasesfrom57.6to63.4,andthescoreofMMEincreasesfrom1163.8to
1311.3. Whentherankreaches128,VLoRA‚Äôsperformancedeclinesacrossthesebenchmarks. The
reasonmightbethatthevisualinformationbecomesredundant,andalargerankmayintroducenoise
intotheperceptualweights,whichhurtsLLM‚Äôscapability.
Thenumberofblocksofperceptualweightsgenerator. Toexploretheinfluenceoftheperceptual
weightsgenerator,weperformexperimentswithdifferentnumbersofblocksintheperceptualweights
generator. InTab.5,weobservethattheperformanceoftheweightsgeneratorwith8blocksisbetter
thanwith4blocks. However,whenitcomestoN =12,thescoresonScienceQAandCCBenchare
higherthanwith8blocks,butperformancedropsonotherbenchmarks. Thissuggeststhatwhilea
strongerperceptualweightsgeneratorcanachievebetterperformance,thereisnobenefittoincreasing
thenumberofblocksafterthethresholdisreached.
6 Conclusion
Inthispaper,insteadofaligningvisualfeatureswiththeinputspaceofLLM,weproposeVLoRA
to align visual features with the parameter space of LLM. By not introducing visual tokens into
LLM,ourVLoRAcanmakeLLMperceivevisualinformationwithoutextracomputationaloverhead.
Toconvertvisualfeaturesintoperceptualweights,weproposetheperceptualweightsgeneratorto
generatelow-rankperceptualweightsforanyweightsofLLM.Duetothelow-rankproperty,the
perceptualweightscanbeseenasLoRAweights,while‚àÜW isgeneratedand‚àÜW islearnable.We
A B
performcomprehensiveexperimentsonsixMLLMbenchmarks,andVLoRAcanachievecomparable
performancetoLLaVA-v1.5inmostbenchmarkswhileonlybringing10%computationalcostas
LLaVA‚Äôs. Intheablationstudy,wereproduceLLaVA-v1.5underthesamesettingsandshowthatour
methodcanachievebetterperformance.
97 Limitations
DespiteVLoRA‚Äôspromisingperformanceonvariousbenchmarks,itstillhassomelimitations. 1)
Representingimagesasmodelweightsisapreviouslyunexploredpractice,andtheextractedfeatures
fromexistingCLIPmodelsmaynotbesuitabletobeconvertedintomodelweights. Itisnecessaryto
exploreavisionencoderthatismoresuitableforthisparadigm. 2)Weuseoneperceptualweights
generatorforonetypeofweight,whichmayleadtoaninsufficientcorrelationbetweendifferent
typesofgeneratedperceptualweights. Itmaybebettertousethesameperceptualweightsgenerator
toproduceweightsforalltypesatonce.
References
[1] Laioncoco:600msyntheticcaptionsfromlaion2b-en. https://laion.ai/blog/laion-coco,2022.
[2] Sharegpt. https://sharegpt.com,2023.
[3] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelfor
few-shotlearning. Advancesinneuralinformationprocessingsystems,35:23716‚Äì23736,2022.
[4] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,KalyaniMarathe,
YonatanBitton,SamirGadre,ShioriSagawa,etal. Openflamingo:Anopen-sourceframeworkfortraining
largeautoregressivevision-languagemodels. arXivpreprintarXiv:2308.01390,2023.
[5] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl: Afrontierlargevision-languagemodelwithversatileabilities. arXivpreprint
arXiv:2308.12966,2023.
[6] NicolasCarion,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,AlexanderKirillov,andSergey
Zagoruyko. End-to-endobjectdetectionwithtransformers. InEuropeanconferenceoncomputervision,
pages213‚Äì229.Springer,2020.
[7] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,RaghuramanKrish-
namoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. Minigpt-v2:largelanguagemodel
asaunifiedinterfaceforvision-languagemulti-tasklearning. arXivpreprintarXiv:2310.09478,2023.
[8] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models.
https://github.com/open-compass/opencompass,2023.
[9] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleNFung,andStevenHoi. Instructblip:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. AdvancesinNeuralInformationProcessingSystems,36,2024.
[10] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,XilinWei,Songyang
Zhang,HaodongDuan,MaosongCao,WenweiZhang,YiningLi,HangYan,YangGao,XinyueZhang,
WeiLi,JingwenLi,KaiChen,ConghuiHe,XingchengZhang,YuQiao,DahuaLin,andJiaqiWang.
Internlm-xcomposer2:Masteringfree-formtext-imagecompositionandcomprehensioninvision-language
largemodel. arXivpreprintarXiv:2401.16420,2024.
[11] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,SongyangZhang,Haodong
Duan,WenweiZhang,YiningLi,etal. Internlm-xcomposer2-4khd:Apioneeringlargevision-language
modelhandlingresolutionsfrom336pixelsto4khd. arXivpreprintarXiv:2404.06512,2024.
[12] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,WeiLin,
JinruiYang,XiawuZheng,etal. Mme: Acomprehensiveevaluationbenchmarkformultimodallarge
languagemodels. arXivpreprintarXiv:2306.13394,2023.
[13] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. MakingtheVinVQA
matter: Elevating the role of image understanding in Visual Question Answering. In Conference on
ComputerVisionandPatternRecognition(CVPR),2017.
[14] TianruiGuan,FuxiaoLiu,XiyangWu,RuiqiXian,ZongxiaLi,XiaoyuLiu,XijunWang,LichangChen,
FurongHuang,YaserYacoob,DineshManocha,andTianyiZhou.Hallusionbench:Anadvanceddiagnostic
suiteforentangledlanguagehallucination&visualillusioninlargevision-languagemodels,2023.
10[15] AnishaGunjal,JihanYin,andErhanBas. Detectingandpreventinghallucinationsinlargevisionlanguage
models. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages18135‚Äì18143,
2024.
[16] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,AndreaGes-
mundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransferlearningfornlp. InInternational
conferenceonmachinelearning,pages2790‚Äì2799.PMLR,2019.
[17] EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,WeizhuChen,etal.
Lora:Low-rankadaptationoflargelanguagemodels. InICLR,2021.
[18] DrewAHudsonandChristopherDManning. Gqa: Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages6700‚Äì6709,2019.
[19] SaharKazemzadeh,VicenteOrdonez,MarkMatten,andTamaraBerg. Referitgame:Referringtoobjects
inphotographsofnaturalscenes. InProceedingsofthe2014conferenceonempiricalmethodsinnatural
languageprocessing(EMNLP),pages787‚Äì798,2014.
[20] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,
SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages4015‚Äì4026,2023.
[21] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,
YannisKalantidis,Li-JiaLi,DavidAShamma,etal. Visualgenome: Connectinglanguageandvision
usingcrowdsourceddenseimageannotations. Internationaljournalofcomputervision,123:32‚Äì73,2017.
[22] HugoLauren√ßon,L√©oTronchon,MatthieuCord,andVictorSanh. Whatmatterswhenbuildingvision-
languagemodels? arXivpreprintarXiv:2405.02246,2024.
[23] HugoLauren√ßon,LucileSaulnier,L√©oTronchon,StasBekman,AmanpreetSingh,AntonLozhkov,Thomas
Wang,SiddharthKaramcheti,AlexanderM.Rush,DouweKiela,MatthieuCord,andVictorSanh. Obelics:
Anopenweb-scalefiltereddatasetofinterleavedimage-textdocuments,2023.
[24] BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-efficientprompttuning.
InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages
3045‚Äì3059,2021.
[25] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. InInternationalconferenceonmachinelearning,
pages19730‚Äì19742.PMLR,2023.
[26] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration.InInternationalconferenceonmachinelearning,
pages12888‚Äì12900.PMLR,2022.
[27] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In
Proceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11th
InternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582‚Äì
4597,2021.
[28] YifanLi,YifanDu,KunZhou,JinpengWang,XinZhao,andJi-RongWen.Evaluatingobjecthallucination
inlargevision-languagemodels. InThe2023ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,2023.
[29] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,YuliangLiu,and
XiangBai. Monkey:Imageresolutionandtextlabelareimportantthingsforlargemulti-modalmodels. In
proceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,2024.
[30] ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,HanXiao,HanQiu,ChenLin,WenqiShao,
KeqinChen,etal. Sphinx: Thejointmixingofweights,tasks,andvisualembeddingsformulti-modal
largelanguagemodels. arXivpreprintarXiv:2311.07575,2023.
[31] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large
multi-modalmodelwithrobustinstructiontuning. arXivpreprintarXiv:2306.14565,2023.
[32] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXivpreprintarXiv:2310.03744,2023.
11[33] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-next:
Improvedreasoning,ocr,andworldknowledge,January2024.
[34] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,2023.
[35] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-
Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint
arXiv:2402.09353,2024.
[36] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,andJieTang.Gptunderstands,
too. AIOpen,2023.
[37] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhnag,WangboZhao,YikeYuan,JiaqiWang,
ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. Mmbench:Isyourmulti-modalmodelanall-around
player? arXiv:2307.06281,2023.
[38] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternationalConferenceon
LearningRepresentations,2018.
[39] HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,TongzhengRen,
ZhuoshuLi,YaofengSun,etal. Deepseek-vl:towardsreal-worldvision-languageunderstanding. arXiv
preprintarXiv:2403.05525,2024.
[40] PanLu,SwaroopMishra,TonyXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,Peter
Clark,andAshwinKalyan.Learntoexplain:Multimodalreasoningviathoughtchainsforsciencequestion
answering. InThe36thConferenceonNeuralInformationProcessingSystems(NeurIPS),2022.
[41] JunhuaMao, JonathanHuang, AlexanderToshev, OanaCamburu, AlanLYuille, andKevinMurphy.
Generationandcomprehensionofunambiguousobjectdescriptions.InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pages11‚Äì20,2016.
[42] KennethMarino,MohammadRastegari,AliFarhadi,andRoozbehMottaghi. Ok-vqa:Avisualquestion
answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on
computervisionandpatternrecognition,pages3195‚Äì3204,2019.
[43] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual
questionansweringbyreadingtextinimages. In2019internationalconferenceondocumentanalysisand
recognition(ICDAR),pages947‚Äì952.IEEE,2019.
[44] OpenAI. Gpt-4technicalreport,2023.
[45] OpenAI. Gpt-4v(ision)systemcard. 2023.
[46] JonasPfeiffer,AishwaryaKamath,AndreasR√ºckl√©,KyunghyunCho,andIrynaGurevych. Adapterfusion:
Non-destructivetaskcompositionfortransferlearning. arXivpreprintarXiv:2005.00247,2020.
[47] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconferenceonmachinelearning,pages8748‚Äì8763.PMLR,
2021.
[48] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b:Anopenlarge-scale
datasetfortrainingnextgenerationimage-textmodels. NeurIPS,35:25278‚Äì25294,2022.
[49] DustinSchwenk,ApoorvKhandelwal,ChristopherClark,KennethMarino,andRoozbehMottaghi. A-
okvqa:Abenchmarkforvisualquestionansweringusingworldknowledge. InEuropeanConferenceon
ComputerVision,pages146‚Äì162.Springer,2022.
[50] OleksiiSidorov,RonghangHu,MarcusRohrbach,andAmanpreetSingh. Textcaps:adatasetforimage
captioningwithreadingcomprehension. InComputerVision‚ÄìECCV2020:16thEuropeanConference,
Glasgow,UK,August23‚Äì28,2020,Proceedings,PartII16,pages742‚Äì758.Springer,2020.
[51] Yi-LinSung,JaeminCho,andMohitBansal. Vl-adapter:Parameter-efficienttransferlearningforvision-
and-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages5227‚Äì5237,2022.
[52] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
12[53] ShengbangTong,ZhuangLiu,YuexiangZhai,YiMa,YannLeCun,andSainingXie. Eyeswideshut?
exploringthevisualshortcomingsofmultimodalllms,2024.
[54] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timoth√©eLacroix,
BaptisteRozi√®re,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama:Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023.
[55] HaoranWei,LingyuKong,JinyueChen,LiangZhao,ZhengGe,JinrongYang,JianjianSun,Chunrui
Han,andXiangyuZhang. Vary:Scalingupthevisionvocabularyforlargevision-languagemodels. arXiv
preprintarXiv:2312.06109,2023.
[56] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, and Jingjing Liu.
Capsfusion:Rethinkingimage-textdataatscale. arXivpreprintarXiv:2310.20550,2023.
[57] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,Dongfu
Jiang,WeimingRen,YuxuanSun,etal. Mmmu:Amassivemulti-disciplinemultimodalunderstanding
andreasoningbenchmarkforexpertagi. arXivpreprintarXiv:2311.16502,2023.
[58] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguageimage
pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
11975‚Äì11986,2023.
[59] PanZhang,XiaoyiDongBinWang,YuhangCao,ChaoXu,LinkeOuyang,ZhiyuanZhao,Shuangrui
Ding,SongyangZhang,HaodongDuan,HangYan,etal. Internlm-xcomposer:Avision-languagelarge
modelforadvancedtext-imagecomprehensionandcomposition. arXivpreprintarXiv:2309.15112,2023.
[60] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
HongshengLi, andYuQiao. Llama-adapter: Efficientfine-tuningoflanguagemodelswithzero-init
attention. arXivpreprintarXiv:2303.16199,2023.
[61] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging
llm-as-a-judgewithmt-benchandchatbotarena. arXivpreprintarXiv:2306.05685,2023.
[62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. InICLR,2024.
13A AnalysisofVLoRAcomputationaloverhead
Inthissubsection,wegiveadetailedcalculationofthecomputationaloverheadofVLoRA.Similar
toSect.3.4,weassumetheLLMhasdblocksandhiddenstatesdimensionofh,theinputtextlength
isC,andthenumberofvisualtokensisL. Therefore,theFLOPsoftheself-attentionmoduleandthe
feed-forwardnetworkare8Lh2+4L2hand16Lh2.Sincevisualtokensarenotintroduced,thenLLM
hasacomputationaloverheadof24Cdh2+4C2dhfortexttokensequenceinput.Fortraining,weuse
perceptualweightsasbranchesofLLMweights. Theextracomputationcomesfromthreeparts: 1)
thematrixmultiplicationofthetwoperceptualweightswithFLOPsof24krh2. 2)Themultiplication
ofthetexttokenandtheperceptualweightswithFLOPsof12Ckh2. 3)Theoutputcomingoutofthe
perceptualweightsistobeaddedtotheoutputoftheLLMweightswithFLOPsof14Ckh.Therefore,
thetotalFLOPsofVLoRAduringtrainingis24Cdh2+4C2dh+24krh2+12Ckh2+14Ckh. For
inference,wemergetheperceptualweightswithLLM‚Äôsweights. Theextracomputationcomesfrom
twoparts: 1)thematrixmultiplicationofthetwoperceptualweightswithFLOPsof24krh2,which
isthesameastraining. 2)AddingperceptualweightstoLLMweightswithFLOPsof12kh2. The
totalFLOPsduringinferenceare24Cdh2+4C2dh+24krh2+12kh2.
B VisualizationResults
VLoRAcanachievepromisingresultsonvariousMLLMbenchmarks,butthesebenchmarksare
eithermultiplechoiceorjudgmental,andtodemonstrateVLoRA‚Äôscapabilitiesevenfurther,weshow
somereal-worldsamplesinFig.5. ThefirstfiguresuggeststhatourVLoRAcancounttheaccurate
numberofsteaksintheimage. ThesecondfigureshowsthatVLoRAhassufficientcommonsense.
Inthethirdfigure,VLoRAdemonstratestheabilitytoreasonandhavelongtextconversations.
Q: Why is the river golden
yellow?
A: The river appears golden
yellow due to the reflection of
the sunlight on the water. The
sunlight creates a warm and
vibrant glow on the surface of
the river, which is then
reflected onto the water's
surface. This phenomenon is
common in bodies of water,
especially during sunrise or
sunset, when the sunlight
Q: How many steaks are in the image? Q: Who is the director of this film? creates a beautiful and
A: There are two steaks in the image? A: James Cameron colorful atmosphere.
Figure5: VisualizationresultsofVLoRA.ThisfiguredemonstratesthecapabilitiesofourVLoRA
inreal-worldscenarios,includingaccuratecountingandcommonsensereasoning.
C BroaderImpacts
Ourproposednewparadigmsignificantlyimprovesthetrainingandinferenceefficiencyofmultimodal
largemodelsandreducesthecomputationaloverhead,which,intermsofresearch,canreducethe
resourcethresholdofmultimodallargemodelresearch,whichisconducivetotheactiveexploration
ofresearchersinrelatedfields,and,intermsofpracticalapplication,reducesthecostoflarge-scale
deploymentforuseandhelpstoreducetheconsumptionofresources.
14