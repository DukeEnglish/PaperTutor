Disentangling and mitigating the impact of task
similarity for continual learning
NaokiHiratani
DepartmentofNeuroscience
WashingtonUniversityinStLouis
StLouis,MO63110
hiratani@wustl.edu
Abstract
Continuallearningofpartiallysimilartasksposesachallengeforartificialneural
networks,astasksimilaritypresentsbothanopportunityforknowledgetransferand
ariskofinterferenceandcatastrophicforgetting. However,itremainsunclearhow
tasksimilarityininputfeaturesandreadoutpatternsinfluencesknowledgetransfer
andforgetting,aswellashowtheyinteractwithcommonalgorithmsforcontinual
learning. Here, wedevelopalinearteacher-studentmodelwithlatentstructure
andshowanalyticallythathighinputfeaturesimilaritycoupledwithlowreadout
similarityiscatastrophicforbothknowledgetransferandretention. Conversely,
theoppositescenarioisrelativelybenign. Ouranalysisfurtherrevealsthattask-
dependentactivitygatingimprovesknowledgeretentionattheexpenseoftransfer,
whiletask-dependentplasticitygatingdoesnotaffecteitherretentionortransfer
performance atthe over-parameterized limit. In contrast, weight regularization
basedontheFisherinformationmetricsignificantlyimprovesretention,regardless
oftasksimilarity,withoutcompromisingtransferperformance. Nevertheless,its
diagonalapproximationandregularizationintheEuclideanspacearemuchless
robust against task similarity. We demonstrate consistent results in a permuted
MNISTtaskwithlatentvariables. Overall,thisworkprovidesinsightsintowhen
continuallearningisdifficultandhowtomitigateit.
1 Introduction
Artificialneuralnetworkssurpasshumancapabilitiesinvariousdomains,yetstrugglewithcontinual
learning. These networks tend to forget previously learned tasks when trained sequentially—a
problem known as catastrophic forgetting [42, 15, 22, 31]. This phenomenon affects not only
supervised training of feedforward networks but also extends to recurrent neural networks [10],
reinforcementlearningtasks[29],andfine-tuningoflargelanguagemodels[39]. Manyalgorithms
formitigatingcatastrophicforgettinghavebeendevelopedpreviously,includingrehearsaltechniques
[46,47,57],weightregularization[29,35,65],andactivity-gatingmethods[14,52,40,54],among
others [55, 48, 61, 20]. However, these methods often hinder forward and backward knowledge
transfer[27,38,28],andthusitremainsunclearhowtoachieveknowledgetransferandretention
simultaneously.
Akeyfactorforcontinuallearningistasksimilarity. Iftwosubsequenttasksaresimilar,thereisa
potentialforaknowledgetransferfromonetasktoanother,buttheriskofinterferencealsobecomes
high [45, 27, 34, 11, 38]. The impact of task similarity on transfer and retention performance is
particularlycomplicatedbecausetwotaskscanbesimilarindifferentmanners[63,31]. Sometimes
familiarinputfeaturesneedtobeassociatedwithnoveloutputpatterns,butatothertimes,novel
inputfeaturesneedtobeassociatedwithfamiliaroutputpatterns. Previousworksobservedthatthese
Preprint.Underreview.
4202
yaM
03
]LM.tats[
1v63202.5042:viXratwoscenariosinfluencecontinuallearningdifferently[34],yettheimpactoftheinputandoutput
similarityonknowledgetransferandretentionhasnotbeenwellunderstood. Moreover,itremains
unknownhowthetasksimilarityinteractswithalgorithmsforcontinuallearningsuchasactivity
gatingorweightregularization.
Togaininsightintothesequestions,inthiswork,weinvestigatehowtransferandretentionperfor-
mancedependontasksimilarity,task-dependentgating,andweightregularizationinanalytically
tractableteacher-studentmodels.Teacher-studentmodelsaresimple,typicallylinear,neuralnetworks
in which the generative model of data is specified explicitly by the teacher network [17, 64, 3].
Thesemodelshaveprovidedtremendousinsightsintogeneralizationproperty[53,43,18,1],con-
vergencerate[59,56,37], andlearningdynamics[49,50,4,24]ofneuralnetworks, duetotheir
analyticaltractability. Severalworksalsostudiedcontinuallearningusingteacher-studentsettings
[2,34,26,11,19,36,12](seeRelatedworkssectionfordetails).
Wedevelopalinearteacher-studentmodelwithalow-dimensionallatentstructureandanalyzehow
the similarity of input features and readout patterns between tasks affect continual learning. We
showanalyticallythatacombinationoflowfeaturesimilarityandhighreadoutsimilarityisrelatively
benignforcontinuallearning,astheretentionperformanceremainshighandthetransferperformance
remains non-negative. However, the opposite, a combination of high feature similarity and low
readoutsimilarityisharmful. Inthisregime,bothtransferandretentionperformancebecomebelow
thechancelevelevenwhenthetwosubsequenttasksarepositivelycorrelated. Furthermore,transfer
performancedependsonthefeaturesimilaritynon-monotonically,suchthat,beyondacriticalpoint,
thehigherthefeaturesimilarityis,thelowerthetransferperformancebecomes.
Wefurtheranalyzehowcommonalgorithmsforcontinuallearning, activityandplasticitygating
[14,40,44],activitysparsification[55],andweightregularization[29,35,65],interactwithtask
similarityinourproblemsetting,derivingseveralnon-trivialconclusions. Activitygatingimproves
retentionatthecostoftransferwhenthegatinghighlysparsifiestheactivity,buthelpsbothtransferand
retentiononaverageiftheactivityiskeptrelativelydense. Plasticitygatingandactivitysparsification,
by contrast, do not influence either transfer or retention performance at the over-parameterized
limit. Lastly,weightregularizationintheFisherinformationmetrichelpsretentionwithoutaffecting
knowledgetransferandachievesperfectretentionregardlessoftasksimilarityinthepresenceof
low-dimensionallatent. However,itsdiagonalapproximationandtheregularizationintheEuclidean
metricaremuchlessrobustagainstbothtasksimilarityandregularizeramplitude.
Furthermore,wetestourkeypredictionsnumericallyinapermutedMNISTtaskwithalatentstructure.
Whentheinputpixelsarepermutedfromonetasktothenext,theretentionperformanceremainshigh.
However,whenthemappingfromthelatenttothetargetoutputischanged,boththeretentionand
transferperformancegobelowthechancelevel,aspredicted. Randomtask-dependentgatingofinput
andhiddenlayeractivityimprovesretentionatthecostofknowledgetransfer,butadaptivegating
mitigatesthistradeoff. Wealsoshowthatinafully-connectedfeedforwardnetwork,thereexistsan
efficientlayer-wiseapproximationoftheweightregularizationintheFisherinformationmetric,which
outperformsitsdiagonalapproximationandtheregularizationintheEuclideanmetric. Nevertheless,
theperformanceofthediagonalapproximationismuchclosertothelayer-wiseapproximationofthe
FisherinformationmetricthantotheEuclideanweightregularization.
Ourtheorythusrevealswhencontinuallearningisdifficult,andhowdifferentalgorithmsmitigate
thesechallengingsituations,providingabasicframeworkforanalyzingcontinuallearninginartificial
andbiologicalneuralnetworks.
2 Relatedworks
Previousworksoncontinuallearninginlinearteacher-studentmodelsfoundthatforgettingismost
prominentattheintermediatetasksimilarity[11,19,12]asobservedempirically[45]. However,
theseworksdidnotaddressthetradeoffbetweenforgettingandknowledgetransfer,andthesesimple
settingsdidnotdisentangletheeffectofthesimilarityininputfeatureandreadoutpattern. Forward
andbackwardtransferperformanceincontinuallearningwerealsoanalyzedinlinearanddeep-linear
networks[32,8],yetitsrelationshipwithcatastrophicforgettinghasnotbeenwellcharacterized. Lee
etal.[34]analyzeddynamicsofbothforgettingandforwardtransferinone-hiddenlayernonlinear
networkunderamulti-headcontinuallearningsetting. However,theiranalysisofreadoutsimilarity
2anditscomparisontofeaturesimilaritywereconductednumericallyanditdidnotaddresstheeffect
ofcommonheuristics,suchasgatingorweightregularization,either.
Bycomparison,weintroducealow-dimensionallatentstructureintoalinearteacher-studentmodel,
whichenablesustodecoupletheinfluenceoffeatureandreadoutsimilarityonknowledgetransfer
andretention. Moreover,thislow-dimensionalityassumptiononthelatentenablesustoevaluatethe
transferandretentionperformanceanalyticallyeveninthepresenceofgatingorweightregularization
intheFisherinformationmetric.
Continual learning has been studied from many theoretical frameworks beyond teacher-student
modeling, including neural tangent kernel [5, 9, 26], PAC learning [6, 58], and computational
complexity [30]. Learning of low-dimensional latent representation has also been studied in the
contextofmulti-tasklearning[41,60]. Inaddition,severalworksinvestigatedtheeffectofweight
regularizationoncontinuallearninginanalyticallytractablesettings[29,11,23].
3 Teacher-studentmodelwithlow-dimensionallatentvariables
Letusconsidertask-incrementalcontinuallearningofregressiontasks. Foranalyticaltractability,we
considerateacher-studentsettingwherethetargetoutputsaregeneratedbytheteachernetwork. We
definethestudentnetwork,whichlearnsthetask,asalinearprojectionfromapotentiallynonlinear
transformation of the input, y = Wψ(x), where x ∈ RNx is the input, y ∈ RNy is the output,
W ∈RNy×Nx isthetrainableweightmatrix,andψ(x):RNx →RNx isaninputtransformation.We
useψ(x)=xforthevanillaandweightregularizationmodels,ψ(x)=g⊙xforthetask-dependent
gatingmodel,andψ(x)=sgn(x)⊙max{0,|x|−h}forthesoft-thresholdingmodel,wheregand
haregatingandthresholdingvectors,respectively. Here,g⊙xisanelement-wisemultiplication,
andsgn(x)isafunctionthatreturnsthesignoftheinput. Throughoutthepaper,weusebold-italic
lettersforvectors,capital-italiclettersformatrices.
Wegeneratetheinputxandtargetoutputy∗oftheµ-thtaskusingalatentvariables∈RNs:
s←N (0,I ), x=A s, y∗ =B s, (1)
s µ µ
forµ=1,2,whereA
µ
∈RNx×Ns andB
µ
∈RNy×Ns aremixingmatriceshiddenfromthestudent
network,andI isthesizeN identitymatrix. Weintroducethislatentstructure,s,todecouplethe
s s
effectoffeatureandreadoutsimilarityonthetransferandretentionperformance. Below,wesetthe
latentspacetobelow-dimensionalcomparedtotheinputspace(i.e.,N ≪N ). Thisismotivated
s x
bythepresenceoflow-dimensionallatentstructureinmanymachinelearningdatasets[62,7]and
thetasksusedinneuroscienceexperiments[16,25],butalsoaidsanalyticaltractability.
Wegeneratethemixingmatricesforthefirsttask,A andB ,bysamplingelementsindependently
1 1
fromaGaussiandistributionwithmeanzeroandvariance 1 . ThesubsequenttaskmatricesA and
Ns 2
B arealsogeneratedrandomly,butwithelement-wisecorrelationwiththepreviousmatricesA
2 1
andB (seeAppendixAforthedetails). Wedenotetheelement-wisecorrelationbetweenA and
1 1
A byρ andreferitasfeaturecorrelation,becauseA andA specifytheinputfeatures. Similarly,
2 a 1 2
wedenotethecorrelationbetweenB andB ,thereadoutcorrelation,byρ . Fig. 1Aillustrates
1 2 b
thedifferencebetweenρ andρ . Whenρ =1andρ =0,thenewtaskrequiresthenetworkto
a b a b
associatefamiliarinputfeaturestonoveloutputs(middlerow). Bycontrast,whenρ =0andρ =1,
a b
thenetworkneedstoassociatenovelinputfeaturestofamiliarreadouts(bottomrow). Below,we
focusonthescenariowhentaskshavenon-negativecorrelationintermsofbothinputfeaturesand
readout(i.e.,0 ≤ ρ ,ρ ≤ 1). Thisisbecausewheneitherρ orρ isnegative(i.e.,inareversal
a b a b
learningsetting),thenetworkfailstoachieveknowledgetransferrathertrivially.
WemeasuretheperformanceofthestudentnetworkwithweightW ontheµ-thtaskbymean-squared
(cid:68) (cid:69)
errorϵ [W] ≡ 1 ∥B s−Wψ(A s)]∥2 ,where⟨·⟩ istheexpectationoverlatentvariables.
µ Ny µ µ
s
s
Thetransferandretentionperformancearedefinedby
∆ϵ ≡⟨ϵ [W ]−ϵ [W ]⟩ , ∆ϵ ≡⟨ϵ [W ]−ϵ [W ]⟩ . (2)
TF 2 o 2 1 A,B RT 1 o 1 2 A,B
Here, ⟨·⟩ is the expectation over randomly generated task matrices A ,A ,B and B under
A,B 1 2 1 2
agivenfeatureandreadoutcorrelationρ ,ρ . AsillustratedinFig. 1B,thetransferperformance
a b
∆ϵ measureshowmuchperformancethemodelachievesontask2bylearningtask1,whereasthe
TF
retentionperformance∆ϵ measureshowwellthemodelcanperformtask1afterlearningtask2
RT
3Figure1: Transferandretentionperformanceofthevanillamodel. (A)Schematicoftasksimilarity.
(B) Illustration of ∆ϵ and ∆ϵ . Red and blue lines represent the error on task 1 and task 2,
TF RT
respectively. Here,themodelwastrainedontask1for100iterationsandthentrainedontask2for
another100iterations. (C,D)Transferperformanceundervarioustasksimilarity. Pointsinpanel
Carenumericalresults(themeansandthestandarddeviationsovertenrandomseeds),whilesolid
linesareanalyticalresults(Eq. 4). (E-H)Retentionperformanceundervarioustasksimilarity. Panel
Hmagnifiesthe0.9≤ρ ≤1.0regionofpanelF,andthewhitedashedlineinpanelHrepresents
b
localminima/maxima.
(here,thetaskswitchoccursatthe100thiteration). Below,westudyhowknowledgetransferand
retention,thetwokeyobjectivesofcontinuallearning,dependontasksimilarity,andhowtooptimize
theperformancethroughgatingandweightregularization.
4 Impactoftasksimilarityonknowledgetransferandretention
Letusfirstinvestigatethevanillamodelwithoutgatingorweightregularization,toexaminehowtask
similarityaffectsknowledgetransferandretention. Givenψ(x) = x,attheinfinitesamplelimit,
learningbygradientdescentfollowsW˙ =−(WA −B )AT. Thefixedpointofthisdynamics,as
µ µ µ
detailedinAppendixB.1,is:
W =W (I−U UT)+B A+, (3)
µ µ−1 µ µ µ µ
whereW istheweightafterlearningoftheprevioustask,U isthesemi-orthogonalmatrixfrom
µ−1 µ
singularvaluedecomposition(SVD)ofA =U Λ VT,andA+isthepseudo-inverseofmatrixA .
µ µ µ µ µ µ
InsertingEq. 3intoEq. 2andtakingtheexpectationoverrandomlygeneratedtasksA ,B ,A ,B ,
1 1 2 2
thetransferandretentionperformancearewrittenas(seeAppendixB.1)
∆ϵ =ρ (2ρ −ρ ), ∆ϵ =1−ρ2(ρ2 −2ρ ρ +1). (4)
TF a b a RT a a a b
Recall that ρ is the feature similarity defined by the correlation between A and A , while ρ
a 1 2 b
is the readout similarity, representing the correlation between B and B . The derivation of the
1 2
aboveequationsrelieson(correlated)randomgenerationofA ,B ,A ,B andthelow-ranklatent
1 1 2 2
assumption: N ≪ N . These two equations, despite their simplicity, capture the transfer and
s x
retentionperformanceinnumericalsimulationswell(Figs. 1C,E,andG;seeAppendixE.1forthe
detailsofnumericalestimation). Furthermore,theyrevealasymmetricandnon-monotonicimpactof
thefeatureandreadoutsimilarityontheperformance.
Fig.1Ddepictstheknowledgetransferfromtask1totask2,∆ϵ ,undervarious(ρ ,ρ )conditions.
TF a b
Asexpected,∆ϵ =0whenthetwotasksareindependent(i.e.,(ρ ,ρ )=(0,0)),and∆ϵ =1
TF a b TF
whenthetasksareidentical(i.e.,(ρ ,ρ )=(1,1)). Atintermediatelevelsofsimilarity,thereisclear
a b
asymmetryintheinfluenceoffeatureandreadoutsimilaritiesontransferperformance;particularly,a
combinationofhighfeaturesimilarityandlowreadoutsimilarityleadstonegativetransfer,whilethe
oppositescenarioresultsinamodestpositivetransfer(lower-rightvsupper-leftofFig. 1D).
Notably, under a fixed readout similarity ρ , the knowledge transfer depends non-monotonically
b
on the feature similarity ρ . When ρ < ρ , the higher the feature similarity the better transfer
a a b
4Figure2: Randomtask-dependentactivitygatingmodel. (A)Knowledgetransferperformanceunder
ρ =1.0. Thegatinglevelαisdefinedasthefractionofactiveinputneurons(i.e.,α=Pr[g =1]).
a i
(B) The transfer performance under the optimal gating level α∗ = min{ρb,1}. (C) Retention
ρa
performanceunderρ =1.0. (D)Averagetransferandretentionperformanceoveruniformprioron
a
0≤ρ ,ρ ≤1. Horizontaldashedlinesaretheperformanceofthevanillamodel,whilesolidlines
a b
aretheperformanceoftherandomgatingmodel. Pointsarenumericalestimations.
becomes,asimpliedfromEq. 4. However,oncethefeaturesimilarityexceedsthereadoutsimilarity,
counter-intuitively,highfeaturesimilarityworsensthetransferperformance(Figs. 1CandD).Thisis
because,whenthefeaturesimilarityishigh,inputsarealignedwithlearnedfeatures,resultingina
largeoutputregardlessofreadoutsimilarity. Particularly,underalowreadoutsimilarity,performance
becomesbelowzerobecauseextractedfeaturesaremostlyprojectedtotheincorrectdirections.
Theretentionperformancealsoexhibitsasymmetricdependenceonfeatureandreadoutsimilarity.
Whenfeaturesimilarityρ islow,thenetworkbarelyforgetstheprevioustaskregardlessofreadout
a
similarity(Fig. 1Fleft). Bycontrast,whenthefeaturesimilarityishigh,theretentionperformance
caneitherbepositiveornegativedependingonthereadoutsimilarity. Moreover,inthehighreadout
similarityregime,theretentionperformanceisthelowestatanintermediatefeaturesimilarity(Figs.
1GandH).Thisisbecausehighretentionispossiblewheneitherinterferenceislow,orsimilarity
betweentwotasksishigh. Notethat,thislastpointonthenon-monotonicdependenceonfeature
similarityunderρ = 1hasbeeninvestigatedbothempirically[45]andanalytically[34,11,12].
b
Indeed,atρ =1,∆ϵ inEq. 4coincideswithEq. (5)in[12].
b RT
Therefore,theknowledgetransferandretentionperformancedependonfeatureandreadoutsimi-
laritiesinanasymmetricandnon-monotonicmanner. Specifically,acombinationofhighfeature
similarityandlowreadoutsimilarityisdetrimentaltocontinuallearning,resultinginnegativetransfer
andretentionperformance,eveninthepresenceofapositivecorrelationbetweenthetwotasksin
bothinputfeaturesandreadoutpatterns. Moreover,underafixedreadoutsimilarity,theknowledge
transferperformancedependsonthefeaturesimilarityinanon-monotonicmanner. Thus,thecon-
tinuallearninginthevanillamodelissensitivetotasksimilarityandlimitedinperformance. These
resultsmakeuswonderwhetherwecanmitigatethesensitivitytotasksimilarityandimprovethe
overalltransferandretentionperformancebymodifyingthelearningalgorithm. Tothisend,wenext
investigatehowtask-dependentgatingandweightregularizationmethods,twopopularstrategiesin
continuallearning,alleviateknowledgetransferandretention.
5 Task-dependentgating
5.1 Randomactivitygating
Onepopularmethodformitigatingforgettingincontinuallearningisactivitygating[14,52,40,20,
54]. Withgatingofinputactivity,thenetworkiswrittenasy =W(g µ⊙x),whereg
µ
∈{0,1}Nx
isabinarygatingvectorfortaskµ. Wefirstconsiderarandomgatingscenariowhereelementsofg
µ
arerandomlysampledfromaBernoullidistributionwithrateαwhichwedenoteasthegatinglevel.
Allunitsareactiveatα=1,whilenounitsareactiveatα→0limit.
From a parallel argument with the vanilla model, when N ≪ αN , the transfer and retention
s x
performanceareestimatedas(seeAppendixB.1):
∆ϵ =αρ (2ρ −αρ ), (5a)
TF a b a
∆ϵ =1−α2ρ2(α2ρ2 −2αρ ρ +1). (5b)
RT a a a b
5Figure3: Transferandretentionperformanceoftheadaptiveactivitygating(A,B),randomplasticity
gating (C), and input soft-thresholding (D) models. Dashed and solid lines in panels A and B
representtheperformanceoftherandomandadaptiveactivitygatingmodels,respectively.
Thus,randomgatingscalesthefeaturesimilarityfromρ toαρ . Thisscalinglowerstheknowledge
a a
transfer if ρ ≥ ρ because random gating reduces the fraction of input neurons active in two
b a
subsequent tasks (lime line in Fig. 2A). However, if ρ < ρ , gating with α ≥ ρb enhances the
b a ρa
transferbyreducingtheeffectivefeaturesimilarity(bluelinesinFig. 2A;alsocompareFig. 2Bwith
Fig. 1D).Theoptimalgatinglevelα∗forknowledgetransferismin{ρb,1},indicatingthattheinput
ρa
activitytypicallyneedstobedense. Bycontrast,theretentionperformanceisoptimizedatα→0
limitwheretasksdonotinterferewitheachothers(Fig. 2C).Atthislimit,wehave∆ϵ →1. Note
RT
that,inreality,αhastobenon-zerotooptimizetheretention(Eq. 5bholdsonlywhenα≫ Ns).
Nx
Theseresultsindicatethatrandomactivitygatingimprovestheretentionatthecostofforwardknowl-
edgetransfer,assuggestedpreviously[27,38,28]. Thistradeoffbetweenknowledgetransferand
retentionisespeciallycriticalwhenthenetworkdoesn’tknowthetasksimilarities. Supposethatthe
tasksimilarityisdistributeduniformlyover0≤ρ ,ρ ≤1.Then,theaveragetransferperformanceis
a b
maximizedatα= 3,whiletheaverageretentionperformancedecreasesmonotonicallyasafunction
4
ofα(Fig. 2D).Thus,amoderategating(α ≈ 3)benefitsbothtransferandretentiononaverage.
4
However,retentionperformanceinthisregimeissignificantlybelowone,implyingthatthenetwork
cannotreliablyachievehighretentionperformancewithoutsacrificingthetransferperformance.
5.2 Adaptiveactivitygating
Onewaytoovercomethetradeoffbetweentransferandretentionperformanceistoconsideradaptive
gating. Letusintroduceaprobetrialatthebeginningofthetask2,inwhichthemodeltestshow
wellitperformsifitusesthegatingfortask1,g ,fortask2aswell. Iftheerrorintheprobetrialis
1
small,themodelshouldkeepusingthesamegatingvectorstoachieveagoodknowledgetransfer.
Otherwise,itshouldresamplethegatingvectorforretainingtheknowledgeontask1. Usingthe
probeerrorϵ ,wesettheprobabilityofusingthesamegatingasρ =1−ϵ /ϵ . Thisadaptive
pb g pb o
gatingindeedimprovestheaveragetransferperformancecomparedtotherandomgating(solidvs
dashedlinesinFig. 3A)withonlyarelativelysmallreductionintheretentionperformance(Fig. 3B).
5.3 Plasticitygatingandinputsoft-thresholding
Previousworkshavealsoexploredothergatingmechanisms,suchasplasticitygating[44]andactivity
sparsification[55]. However,theirbenefitsovertheactivitygatingdiscussedabovehavenotbeen
fullyunderstood. Weimplementtask-dependentplasticitygatingintoourproblemsettingbymaking
onlythesynapticweightsprojectedfromasubsetofinputneuronsplastic. Unexpectedly,wefound
thatbothtransferandretentionperformanceareindependentofthefractionofplasticsynapsesinthe
N ≪N limit,potentiallyduetoover-parameterization(Fig. 3C;seeAppendixB.2fordetails).
s x
Similarly,whentheinputactivityissparsifiedusingsoft-thresholdingφ(x)=sgn(x)max{0,|x|−
√
h},withafixedthresholdh = 2erfc−1(α),thetransferandretentionperformanceremaininde-
pendentoftheresultantinputsparsityα(Fig. 3D;seeAppendixB.3forthedetails). Theseresults
implythatplasticitygatingandinputsparsificationarenoteffectiveinourmodel,whichoperatesin
anover-parameterizedregime(i.e.,N ≪N ),butdonotexcludetheirpotentialbenefitsformany
s x
continuallearningtasks.
6Figure4: PerformanceofweightregularizationinEuclideanmetric. (A,B)Transfer(A)andretention
(B)performance. Theamplitudeoftheweightregularizationscaleswith 1 −1. (C)Regularizer
γ
coefficientγ thatoptimizestheretentionperformance. (D)Averageperformanceoveruniformtask
similaritydistributionin0≤ρ ,ρ ≤1. Horizontaldashedlinesaretheperformanceofthevanilla
a b
model.
Figure5: WeightregularizationintheFisherinformationmetric. (A,B)Theretentionperformance
undervarioustask similaritiesandregularizercoefficients. (C,D)Average transferandretention
performanceundertheregularizationwiththeexactFisherinformationmetric(C)anditsdiagonal
approximation(D).
6 Weightregularization
6.1 WeightregularizationinEuclideanmetric
Anotherpopularmethodisweightregularization,whichkeepsthesynapticweightsclosetothose
learnedfromprevioustasks[29,35,65]. LetusfirstconsiderregularizationoftheEuclideandistance
betweenthecurrentweightW andtheweightlearnedintheprevioustaskW :
µ−1
ℓ µ = 1 2∥B µ−WA µ∥2 F + 2N Nx s( γ1 −1)∥W −W µ−1∥2 F . (6)
Here,weparameterizetheregularizeramplitudeby Nx(1 −1)usinganon-negativeparameterγ
Ns γ
forbrevity. γ =1correspondstozeroregularization,whileγ =0istheinfiniteregularizationlimit.
Under this parameterization, if N ≪ N , the transfer and retention performance follow simple
s x
expressionsasbelow(seeAppendixC.1):
∆ϵ =γρ (2ρ −γρ ), (7a)
TF a b a
∆ϵ =1−γ2ρ2(1−2γρ ρ +γ2ρ2)+2γρ (1−γ)(ρ −γρ )−(1−γ)2. (7b)
RT a a b a a b a
TheexpressionofthetransferperformanceisequivalenttoEq. 5aifwechangeγ toα. Thus,the
Euclideanweightregularizationwithamplitude Nx(1−1)ismathematicallyequivalentwithrandom
Ns γ
activitygatingwithsparsityγ,intermsofknowledgetransfer(compareFig. 4AwithFig. 2A).By
contrast,theexpressionoftheretentionperformancecontainstwoadditionaltermscomparedto5b.
Inparticular, thelastterm, (1−γ)2, indicatesthatstrongweightregularizationnotonlyprevent
forgetting, but also impairs task acquisition. Thus, the retention performance is optimized at an
intermediateregularizerstrength(Figs. 4BandC).
Becausestrongregularization(i.e.,smallγ)impairsbothretentionandtransfer,unliketherandom
activitygatingmodel,thereisn’tatradeoffbetweenknowledgetransferandretentionintermsof
theaverageperformanceover0≤ρ ,ρ ≤1(Fig. 4D).However,theretentionperformanceatthe
a b
optimalparametricregimeisrelativelylow(compareFig. 4DwithFig. 3B).
7Figure6: PermutedMNISTwithlatentvariables. (A,B)Transferandretentionperformanceofthe
vanillamodel. (C,D)Performanceofrandom(dashedlines)andadaptive(solidlines)activitygating
models. (E-H)PerformanceoftheweightregularizationintheEuclideanmetric,andapproximated
Fisherinformationmetrics(red: layer-wiseapproximation;orange: synapse-wise/diagonalapproxi-
mation). Here,linesarelinearinterpolationsanderrorbarsarestandarderrorsoverrandomseeds,
notstandarddeviations.
6.2 WeightregularizationinFisherinformationmetric
PreviousworksproposedthatthesynapticweightsshouldberegularizedintheFisherinformation
metrictoallowflexibilityinthenon-overlappingweightspace[29,65]. Applyingittoourmodel
setting,theregularizerterminEq.6insteadbecomes 1(1−1)∥(W −W )A ∥2 (seeAppendix
2 γ µ−1 µ−1 F
C.2). Ifρ ,γ <1andN ≪N ,optimizationoftheweightunderthisregularizationyields
a s x
W µ =W µ−1(cid:16) I− Nx(N 1−s ρ2 a)A µ(cid:2) AT µ −ρ aAT µ−1(cid:3)(cid:17) + Nx(N 1−s ρ2 a)B µ(cid:0) AT µ −ρ aAT µ−1(cid:1) , (8)
whereW istheweightaftertheprevioustasklearning. Notably,theweightbecomesindependent
µ−1
oftheregularizer’samplitudeγ.Furthermore,theretentionperformanceunderarbitrarytasksimilarity
(ρ a,ρ b)isderivedas∆ϵ RT = 1−O( Nx(N 1−s ρ2 a)). Thus, undertheFisherinformationmetric, the
retentionperformanceisperfectaslongastheconditionN ≪N (1−ρ2)issatisfied(Figs. 5A-C).
s x a
Intuitively,assumingover-parameterization(N ≪N ),thenetworkcanfreezetheweightchanges
s x
inalow-dimensionalsubspace,whilemaintainingsufficientplasticityfornewtasks,unlessthetwo
taskssharethesamefeature. Notably, ifthefirsttaskislearnedwithweightregularizationinan
orthogonaldirection,thetransferperformanceremainsthesamewiththevanillamodel(Fig. 5C).
Importantly,thisinvariancenolongerholdswhentheFisherinformationmetricisapproximatedby
itsdiagonalcomponent(compareFigs. 5Dvs5C),asisdoneintheelasticweightmethods[29].
Thisisbecausethediagonalapproximationmakesthemetricfull-rankevenwhenthetruemetric
hasalow-rankstructure. Thus,weightregularizationintheFisherinformationmetricrobustlyhelps
retentionwithoutharmingtransfer,butdiagonalapproximationattenuatesitsrobustness.
7 Numericalexperiments
Ourtheorysofarhasrevealedwhencontinuallearningisdifficultandwhenactivitygatingandweight
regularization can ameliorate the transfer and retention in a simple problem setting. Let us next
examinehowmuchtheseinsightsareapplicabletomorerealisticdatasetsandneuralarchitectures.
To this end, we consider the permuted MNIST dataset [33, 21], but with a latent structure (see
AppendixE.2). Foreachoutputlabel,weconstructedafour-dimensionallatentvariablesusingthe
binaryrepresentationofthecorrespondingdigit(e.g.,s =[1,0,0,1]T). Thetargetoutputwasthen
9
generatedbyarandomfixedprojectionofthelatentvariable: y∗ =B (s− 1). Wecontrolledthe
µ 2
readoutsimilaritybetweentasksusingtheelement-wisecorrelationbetweentwomatricesB and
1
B . Thefeaturesimilaritywascontrolledbypermutingasubsetofinputpixels,asintheprevious
2
permutedMNISTtasks. Weusedaone-hidden-layerfully-connectednetworkwithrectified-linear
nonlinearityatthehiddenlayer,andtrainedthenetworkusingstochasticgradientdescent.
8Aspredictedfromourtheory,bothtransferandretentionperformance,measuredbythetesterror,
exhibited asymmetric and non-monotonic dependence on the task similarity. When (ρ ,ρ ) =
a b
(1.0,0.0),bothtransferandretentionperformancewerebelowzero(lower-rightofFigs. 6AandBvs.
Figs. 1DandF).Bycontrast,themodelachievedhighretentionandzerotransferunder(ρ ,ρ )=
a b
(0.0,1.0)asexpected(upper-left). Notably,aroundρ =0.5,thetransferperformanceexhibiteda
b
non-monotonicdependenceonρ inaccordancewiththetheory. Theretentionperformancealso
a
exhibitednon-monotonicdependenceonρ underρ ≈1,butthedependencebecamemonotonic
a b
afteralongtraining(seeFigs. 9CandDinAppendix).
Random activity gating, implemented in both input and hidden layers, improved the retention
performance at the cost of low knowledge transfer as predicted (compare Fig. 6D and Fig. 3B).
Moreover, adaptivegatingbasedonaprobetrialimprovedthetransferperformanceunderalow
gatinglevel,especiallywhenthereadoutsimilarityishigh(solidvsdashedlinesinFigs. 6CandD).
In a deep neural network, weight regularization using the Fisher information metric is typically
computationallyexpensive. However,anefficientlayer-wiseapproximationexistsforfully-connected
networks(seeAppendixE.2). Thislayer-wiseapproximationenabledhighretentionperformance
acrossawiderangeofregularizationamplitudesandconsistentlyoutperformedweightregularization
intheEuclideanmetricundervarioustasksimilarities(redvs. graylinesinFigs. 6F-H).Nevertheless,
the retention performance under the diagonal approximation was closer to that of the layer-wise
approximationoftheFisherinformationmetricandwasslightlybetterwhenbothmethodsperformed
poorly(redvs. orangelines). Thisisbecausethesparsityofhiddenlayerweightsandactivitymakes
thediagonalcomponentssparse.
8 Discussion
Here,weanalyzedtheimpactoftasksimilarityoncontinuallearninginalinearteacher-studentmodel
withlow-dimensionallatentstructures. Weshowedthat,acombinationofhighfeaturesimilarity
andlowreadoutsimilarityleadtopooroutcomesinbothretentionandtransfer,unliketheopposite
combination. Wefurtherexploredhowcontinuallearningalgorithmssuchasgatingmechanisms,
activitysparsification,andweightregularizationinteractwithtasksimilarity. Resultsindicatethat
weightregularizationintheFisherinformationmetricsignificantlyaidsretentionregardlessoftask
similarity. NumericalexperimentsinapermutedMNISTtaskwithlatentsupportedthesefindings.
Ourfindingsontheinteractionbetweentasksimilarityandcontinuallearningalgorithmshaveseveral
implications. Firstly,whentasksareknowntohavehighfeaturesimilarityandlowreadoutsimilarity,
adaptiveactivitygatingandweightregularizationintheFisherinformationmetrichelpretention
withoutsacrificingtransfer(Figs. 3,5,6). Inthecontextofneuroscience,ourresultsindicatethat
simplenon-adaptiveactivityorplasticitygatingmechanismsmaynotbesufficientforgoodcontinual
learningperformance, especiallywhenfamiliarsensorystimulineedtobeassociatedwithnovel
motoractions. Indeed,apreviousstudyreportedcatastrophicforgettingamongratslearningtiming
estimationtasks[13]. Lastly,italsoimpliesthepotentialimportanceofstudyingwiderbenchmarks
beyondpermutedimagerecognitiontasksforcontinuallearning,becauseimagepermutationbelongs
toaclassofrelativelyharmlesstasksimilarityaccordingtoourtheory. Asasimpleextension,here
wedevelopedanimagerecognitiontaskwithalatentvariableandshowedthatavanilladeepnetwork
exhibitsmoreforgettingunderreadoutremappingthanunderinputpermutation(Figs. 6AandB).
Limitations Ourtheoreticalresultsfromateacher-studentmodelcomewithlimitationindirect
applicability. Thepresenceoflow-dimensionallatentgeneratingbothinputandtargetoutputisa
reasonable assumption for many real-world tasks [7, 25], but the linear projection assumption is
not. WhilewereplicatedmostkeyresultsinadeepnonlinearnetworksolvingpermutedMNIST
task,wefoundthatthediagonalapproximationofFisherinformationmetricperformsmuchbetter
thanthepredictionfromtheteacher-studentmodel. Thisispotentiallybecausesparseactivityatthe
hiddenlayereffectivelymakestheregularizationlow-rankevenunderthediagonalapproximation.
Moregenerally,thepresenceofhiddenlayersshouldenableadistinctiveadaptationforfeatureand
readoutsimilarity,whichisanimportantfuturedirection. Itisalsoimportanttoapplyourtheoretical
frameworkforanalyzingcontinuallearninginmorecomplicatedneuralarchitecturesanddatasets.
9AcknowledgmentsandDisclosureofFunding
TheauthorthanksLiuZiyinandZiyanLifordiscussion. Thisworkwaspartiallysupportedbythe
McDonnellCenterforSystemsNeuroscience.
References
[1] M.S.Advani,A.M.Saxe,andH.Sompolinsky. High-dimensionaldynamicsofgeneralizationerrorin
neuralnetworks. NeuralNetworks,132:428–446,2020.
[2] H.Asanuma,S.Takagi,Y.Nagano,Y.Yoshida,Y.Igarashi,andM.Okada. Statisticalmechanicalanalysis
ofcatastrophicforgettingincontinuallearningwithteacherandstudentnetworks. JournalofthePhysical
SocietyofJapan,90(10):104001,2021.
[3] Y.Bahri, J.Kadmon, J.Pennington, S.S.Schoenholz, J.Sohl-Dickstein, andS.Ganguli. Statistical
mechanicsofdeeplearning. AnnualReviewofCondensedMatterPhysics,11:501–528,2020.
[4] M.Baity-Jesi,L.Sagun,M.Geiger,S.Spigler,G.B.Arous,C.Cammarota,Y.LeCun,M.Wyart,and
G.Biroli. Comparingdynamics:Deepneuralnetworksversusglassysystems. InInternationalConference
onMachineLearning,pages314–323.PMLR,2018.
[5] M. A. Bennani, T. Doan, and M. Sugiyama. Generalisation guarantees for continual learning with
orthogonalgradientdescent. arXivpreprintarXiv:2006.11942,2020.
[6] X.Chen,C.Papadimitriou,andB.Peng. Memoryboundsforcontinuallearning. In2022IEEE63rd
AnnualSymposiumonFoundationsofComputerScience(FOCS),pages519–530.IEEE,2022.
[7] U.Cohen,S.Chung,D.D.Lee,andH.Sompolinsky. Separabilityandgeometryofobjectmanifoldsin
deepneuralnetworks. Naturecommunications,11(1):746,2020.
[8] O.DhifallahandY.M.Lu.Phasetransitionsintransferlearningforhigh-dimensionalperceptrons.Entropy,
23(4):400,2021.
[9] T.Doan,M.A.Bennani,B.Mazoure,G.Rabusseau,andP.Alquier. Atheoreticalanalysisofcatastrophic
forgetting through the ntk overlap matrix. In International Conference on Artificial Intelligence and
Statistics,pages1072–1080.PMLR,2021.
[10] B.Ehret,C.Henning,M.R.Cervera,A.Meulemans,J.VonOswald,andB.F.Grewe. Continuallearning
inrecurrentneuralnetworks. arXivpreprintarXiv:2006.12109,2020.
[11] I.Evron,E.Moroshko,R.Ward,N.Srebro,andD.Soudry. Howcatastrophiccancatastrophicforgetting
beinlinearregression? InConferenceonLearningTheory,pages4028–4079.PMLR,2022.
[12] I.Evron,D.Goldfarb,N.Weinberger,D.Soudry,andP.Hand. Thejointeffectoftasksimilarityand
overparameterizationoncatastrophicforgetting–ananalyticalmodel. arXivpreprintarXiv:2401.12617,
2024.
[13] R.FrenchandA.Ferrara. Modellingtimeperceptioninrats: evidenceforcatastrophicinterferencein
animallearning. In21stCongressofCognitiveSciences,1999.
[14] R.M.French. Usingsemi-distributedrepresentationstoovercomecatastrophicforgettinginconnectionist
networks. In Proceedings of the 13th annual cognitive science society conference, volume 1, pages
173–178,1991.
[15] R.M.French. Catastrophicforgettinginconnectionistnetworks. Trendsincognitivesciences, 3(4):
128–135,1999.
[16] P.Gao,E.Trautmann,B.Yu,G.Santhanam,S.Ryu,K.Shenoy,andS.Ganguli.Atheoryofmultineuronal
dimensionality,dynamicsandmeasurement. BioRxiv,page214262,2017.
[17] E.GardnerandB.Derrida. Threeunfinishedworksontheoptimalstoragecapacityofnetworks. Journal
ofPhysicsA:MathematicalandGeneral,22(12):1983,1989.
[18] B.Ghorbani,S.Mei,T.Misiakiewicz,andA.Montanari. Limitationsoflazytrainingoftwo-layersneural
network. AdvancesinNeuralInformationProcessingSystems,32,2019.
[19] D.GoldfarbandP.Hand.Analysisofcatastrophicforgettingforrandomorthogonaltransformationtasksin
theoverparameterizedregime. InInternationalConferenceonArtificialIntelligenceandStatistics,pages
2975–2993.PMLR,2023.
10[20] S.Golkar,M.Kagan,andK.Cho.Continuallearningvianeuralpruning.arXivpreprintarXiv:1903.04476,
2019.
[21] I.J.Goodfellow,M.Mirza,D.Xiao,A.Courville,andY.Bengio.Anempiricalinvestigationofcatastrophic
forgettingingradient-basedneuralnetworks. arXivpreprintarXiv:1312.6211,2013.
[22] R.Hadsell,D.Rao,A.A.Rusu,andR.Pascanu. Embracingchange:Continuallearningindeepneural
networks. Trendsincognitivesciences,24(12):1028–1040,2020.
[23] R.Heckel. Provablecontinuallearningviasketchedjacobianapproximations. InInternationalConference
onArtificialIntelligenceandStatistics,pages10448–10470.PMLR,2022.
[24] N.Hiratani,Y.Mehta,T.Lillicrap,andP.E.Latham. Onthestabilityandscalabilityofnodeperturbation
learning. AdvancesinNeuralInformationProcessingSystems,35:31929–31941,2022.
[25] M. Jazayeri and S. Ostojic. Interpreting neural computations by examining intrinsic and embedding
dimensionalityofneuralactivity. Currentopinioninneurobiology,70:113–120,2021.
[26] R.KarakidaandS.Akaho. Learningcurvesforcontinuallearninginneuralnetworks:Self-knowledge
transferandforgetting. InInternationalConferenceonLearningRepresentations,2021.
[27] Z.Ke,B.Liu,andX.Huang. Continuallearningofamixedsequenceofsimilaranddissimilartasks.
Advancesinneuralinformationprocessingsystems,33:18493–18504,2020.
[28] Z.Ke,B.Liu,W.Xiong,A.Celikyilmaz,andH.Li.Sub-networkdiscoveryandsoft-maskingforcontinual
learningofmixedtasks. arXivpreprintarXiv:2310.09436,2023.
[29] J.Kirkpatrick,R.Pascanu,N.Rabinowitz,J.Veness,G.Desjardins,A.A.Rusu,K.Milan,J.Quan,T.Ra-
malho,A.Grabska-Barwinska,etal. Overcomingcatastrophicforgettinginneuralnetworks. Proceedings
ofthenationalacademyofsciences,114(13):3521–3526,2017.
[30] J.Knoblauch,H.Husain,andT.Diethe. Optimalcontinuallearninghasperfectmemoryandisnp-hard. In
InternationalConferenceonMachineLearning,pages5327–5337.PMLR,2020.
[31] D. Kudithipudi, M. Aguilar-Simon, J. Babb, M. Bazhenov, D. Blackiston, J. Bongard, A. P. Brna,
S. Chakravarthi Raja, N. Cheney, J. Clune, et al. Biological underpinnings for lifelong learning ma-
chines. NatureMachineIntelligence,4(3):196–210,2022.
[32] A.K.LampinenandS.Ganguli. Ananalytictheoryofgeneralizationdynamicsandtransferlearningin
deeplinearnetworks. arXivpreprintarXiv:1809.10374,2018.
[33] Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner. Gradient-basedlearningappliedtodocumentrecognition.
ProceedingsoftheIEEE,86(11):2278–2324,1998.
[34] S.Lee,S.Goldt,andA.Saxe. Continuallearningintheteacher-studentsetup:Impactoftasksimilarity. In
InternationalConferenceonMachineLearning,pages6109–6119.PMLR,2021.
[35] S.-W.Lee,J.-H.Kim,J.Jun,J.-W.Ha,andB.-T.Zhang.Overcomingcatastrophicforgettingbyincremental
momentmatching. Advancesinneuralinformationprocessingsystems,30,2017.
[36] C.Li,Z.Huang,W.Zou,andH.Huang. Statisticalmechanicsofcontinuallearning:Variationalprinciple
andmean-fieldpotential. PhysicalReviewE,108(1):014309,2023.
[37] Y.Li,T.Ma,andH.R.Zhang. Learningover-parametrizedtwo-layerneuralnetworksbeyondntk. In
Conferenceonlearningtheory,pages2613–2682.PMLR,2020.
[38] S.Lin,L.Yang,D.Fan,andJ.Zhang.Beyondnot-forgetting:Continuallearningwithbackwardknowledge
transfer. AdvancesinNeuralInformationProcessingSystems,35:16165–16177,2022.
[39] Y.Luo,Z.Yang,F.Meng,Y.Li,J.Zhou,andY.Zhang. Anempiricalstudyofcatastrophicforgettingin
largelanguagemodelsduringcontinualfine-tuning. arXivpreprintarXiv:2308.08747,2023.
[40] N.Y.Masse,G.D.Grant,andD.J.Freedman. Alleviatingcatastrophicforgettingusingcontext-dependent
gatingandsynapticstabilization. ProceedingsoftheNationalAcademyofSciences,115(44):E10467–
E10475,2018.
[41] A.Maurer,M.Pontil,andB.Romera-Paredes. Thebenefitofmultitaskrepresentationlearning. Journalof
MachineLearningResearch,17(81):1–32,2016.
11[42] M.McCloskeyandN.J.Cohen. Catastrophicinterferenceinconnectionistnetworks: Thesequential
learningproblem. InPsychologyoflearningandmotivation,volume24,pages109–165.Elsevier,1989.
[43] A.NdirangoandT.Lee. Generalizationinmultitaskdeepneuralclassifiers:astatisticalphysicsapproach.
AdvancesinNeuralInformationProcessingSystems,32,2019.
[44] S.Özgün, A.-M.Rickmann, A.G.Roy, andC.Wachinger. Importancedrivencontinuallearningfor
segmentationacrossdomains. InMachineLearninginMedicalImaging:11thInternationalWorkshop,
MLMI2020,HeldinConjunctionwithMICCAI2020,Lima,Peru,October4,2020,Proceedings11,pages
423–433.Springer,2020.
[45] V.V.Ramasesh,E.Dyer,andM.Raghu. Anatomyofcatastrophicforgetting:Hiddenrepresentationsand
tasksemantics. arXivpreprintarXiv:2007.07400,2020.
[46] A.Robins. Catastrophicforgetting,rehearsalandpseudorehearsal. ConnectionScience,7(2):123–146,
1995.
[47] D.Rolnick,A.Ahuja,J.Schwarz,T.Lillicrap,andG.Wayne. Experiencereplayforcontinuallearning.
Advancesinneuralinformationprocessingsystems,32,2019.
[48] A.A.Rusu,N.C.Rabinowitz,G.Desjardins,H.Soyer,J.Kirkpatrick,K.Kavukcuoglu,R.Pascanu,and
R.Hadsell. Progressiveneuralnetworks. arXivpreprintarXiv:1606.04671,2016.
[49] D.SaadandS.A.Solla. On-linelearninginsoftcommitteemachines. PhysicalReviewE,52(4):4225,
1995.
[50] A.M.Saxe,J.L.McClelland,andS.Ganguli. Exactsolutionstothenonlineardynamicsoflearningin
deeplinearneuralnetworks. arXivpreprintarXiv:1312.6120,2013.
[51] A.M.Saxe,J.L.McClelland,andS.Ganguli. Amathematicaltheoryofsemanticdevelopmentindeep
neuralnetworks. ProceedingsoftheNationalAcademyofSciences,116(23):11537–11546,2019.
[52] J.Serra,D.Suris,M.Miron,andA.Karatzoglou. Overcomingcatastrophicforgettingwithhardattention
tothetask. InInternationalconferenceonmachinelearning,pages4548–4557.PMLR,2018.
[53] H.S.Seung,H.Sompolinsky,andN.Tishby. Statisticalmechanicsoflearningfromexamples. Physical
reviewA,45(8):6056,1992.
[54] E.Sezener,A.Grabska-Barwin´ska,D.Kostadinov,M.Beau,S.Krishnagopal,D.Budden,M.Hutter,
J.Veness,M.Botvinick,C.Clopath,etal. Arapidandefficientlearningruleforbiologicalneuralcircuits.
BioRxiv,pages2021–03,2021.
[55] R.K.Srivastava,J.Masci,S.Kazerounian,F.Gomez,andJ.Schmidhuber.Competetocompute.Advances
inneuralinformationprocessingsystems,26,2013.
[56] Y.Tian. Ananalyticalformulaofpopulationgradientfortwo-layeredrelunetworkanditsapplicationsin
convergenceandcriticalpointanalysis.InInternationalconferenceonmachinelearning,pages3404–3413.
PMLR,2017.
[57] G.M.VandeVen,H.T.Siegelmann,andA.S.Tolias. Brain-inspiredreplayforcontinuallearningwith
artificialneuralnetworks. Naturecommunications,11(1):4069,2020.
[58] L.Wang,X.Zhang,Q.Li,M.Zhang,H.Su,J.Zhu,andY.Zhong.Incorporatingneuro-inspiredadaptability
forcontinuallearninginartificialintelligence. NatureMachineIntelligence,5(12):1356–1368,2023.
[59] J.Werfel,X.Xie,andH.Seung. Learningcurvesforstochasticgradientdescentinlinearfeedforward
networks. Advancesinneuralinformationprocessingsystems,16,2003.
[60] Z.XuandA.Tewari. Onthestatisticalbenefitsofcurriculumlearning. InInternationalConferenceon
MachineLearning,pages24663–24682.PMLR,2022.
[61] J.Yoon,E.Yang,J.Lee,andS.J.Hwang. Lifelonglearningwithdynamicallyexpandablenetworks. arXiv
preprintarXiv:1708.01547,2017.
[62] X.Yu,T.Liu,X.Wang,andD.Tao. Oncompressingdeepmodelsbylowrankandsparsedecomposition.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages7370–7379,
2017.
12[63] A.R.Zamir,A.Sax,W.Shen,L.J.Guibas,J.Malik,andS.Savarese. Taskonomy:Disentanglingtask
transferlearning. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages3712–3722,2018.
[64] L.ZdeborováandF.Krzakala. Statisticalphysicsofinference:Thresholdsandalgorithms. Advancesin
Physics,65(5):453–552,2016.
[65] F.Zenke,B.Poole,andS.Ganguli. Continuallearningthroughsynapticintelligence. InInternational
conferenceonmachinelearning,pages3987–3995.PMLR,2017.
13A Problemsetting: Linearteacher-studentmodelwithalatentvariable
LetusconsideracontinuouslearningofN sesstasks. Wegeneratetheinputx∈RNx andthetarget
outputy∗ ∈RNy oftheµ-thtaskby
s←N (0,I ), x=A s, y∗ =B s, (9)
s µ µ
where s ∈ RNs is the latent variable, I
s
is the size N
s
identity matrix, and A
µ
∈ RNx×Ns and
B
µ
∈ RNy×Ns aremixingmatricesthatmapthelatentvariablesintotheinputspaceandoutput
space,respectively.Throughoutthepaper,thevectorxisdefinedasacolumnvector,anditstranspose
xT isarowvector.
Weconsidersequentiallearningofthesetaskswithaneuralnetworkdefinedbyy =Wψ(x),where
W ∈ RNy×Nx is the plastic weight. We set the function ψ : RNx → RNx to ψ(x) = x in the
vanillaandweightregularizationmodels,ψ(x) = g⊙xinactivityandplasticitygatingmodels,
andψ(x)=sgn(x)⊙max{0,|x|−h}inthesoft-thresholdingmodel. Here,g⊙xrepresentsan
element-wisemultiplicationoftwovectors,andsgn(x)isafunctionthatreturnsthesignoftheinput
x. Thegoalofthestudentnetwork,y =Wψ(x),istomimictheteachernetworkthatgeneratesboth
inputandtargetoutput[17,64,3]. Below,wefocusonlearningofN =2tasks. Toanalyzethe
sess
transferandretentionperformance,weintroducethefollowingtwoassumptionsontaskstructure.
AssumptionI:Randomtaskassumption WeconsiderrandomgenerationoftaskmatricesA ,B
µ µ
butwithcorrelationbetweensubsequenttasks. Wesampleelementsofthemixingmatricesforthe
firsttask,A andB ,fromaGaussiandistributionwithmeanzeroandvariance 1 independently.
1 1 Ns
Forthesubsequenttasks,wegeneratemixingmatricesA by
µ
(cid:40)
A (withprobabilityρ )
A = µ−1,ij a (10)
µ,ij
A(cid:101)µ,ij (otherwise)
whereA(cid:101)µisarandommatrixwhoseelementsaresampledindependentlyfromaGaussiandistribution
withmeanzeroandvariance 1 . WegenerateB inthesamemanner,butwithcorrelationρ .
Ns µ b
AssumptionII:Low-dimensionallatentassumption Thesecondassumptionistherelativelow
dimensionalityofthelatentspacewithrespecttotheinputspace: N ≪ N . Thisassumptionof
s x
over-parameterizationintheinputspacesimplifiestheanalysis(seeAppendixD).
Evaluationofthetransferandretentionperformance Weevaluatethetaskperformanceofa
studentmodelwithweightW ontaskµbymean-squarederror:
1 (cid:68) (cid:69)
ϵ [W]≡ ∥B s−Wψ(A s)]∥2 . (11)
µ N y µ µ s
Here,⟨·⟩ istheexpectationoverlatents∼N(0,I ).Usingϵ [W],thedegreeofforwardknowledge
s s µ
transferandretentionareevaluatedby
∆ϵTF ≡⟨ϵ [W ]−ϵ [W ]⟩ , (12a)
µ µ o µ µ−1 A,B
∆ϵRT ≡⟨ϵ [W ]−ϵ [W ]⟩ , (12b)
µ µ o µ µ+1 A,B
whereW istheweightaftertrainingontheµ-thtask. Notethat,theretentionperformanceisdefined
µ
bythedifferencewiththeinitialerrorϵ [W ]. Thismeansthat,ifthenetworkfailstolearnthetask
µ o
inthefirstplace(i.e.,ifϵ [W ] > 0),theretentionperformancebecomessub-optimalevenifthe
µ µ
networkdoesn’tforgetthelearnedtask. Weusedthisdefinitionforourproblemsettingbecausethe
networkisabletolearnthetaskperfectlyunlessstrongregularizationisaddedtothenetwork.
Learning procedures We consider the gradient descent learning from infinite samples at the
gradient flow limit and analyze the performance at the convergence. Below, we investigate how
∆ϵTF and∆ϵRT dependonthetasksimilarityandhyper-parametersundervariousalgorithmsfor
µ µ
continuallearning.
14B Task-dependentgatingmodel
B.1 Activitygating
Letusfirstconsidertask-dependentactivitygatingmodel. Thestudentnetwork(thenetworkthat
learnsthetask)isgivenby:
y =W[g ⊙x], (13)
µ
whereg isthegatingvectorthatdependsonthetaskidµ,butindependentofinputx. Notethat,by
µ
settingg =1,werecoverthevanillamodely =Wx.
µ
Theperformanceofthisnetworkontheµ-thtaskiswrittenas
1 (cid:68) (cid:69) 1
ϵ [W]≡ ∥B s−W[g ⊙(A s)]∥2 = ∥B −WD A ∥2 , (14)
µ N y µ µ µ s N y µ µ µ F
whereD ≡diag[g ]isadiagonalmatrixwhose(i,i)-thelementisg ,and∥·∥ istheFrobenius
µ µ µ,i F
norm.
Solutionofgradientdescentlearning Underthegradientdescentlearning,theweightdynamics
follows
∂ϵ [W] 2η
W˙ (t)=−η µ =− (WD A −B )(D A )T . (15)
∂W N µ µ µ µ µ
y
Fromsingularvaluedecomposition(SVD),D A isrewrittenas
µ µ
D A =U Σ VT, (16)
µ µ µ µ µ
whereU
µ
∈RNx×No andV
µ
∈RNs×No aresemi-orthonormalmatrices(i.e.,U µTU
µ
=V µTV
µ
=I o),
Σ
µ
∈RNo×No isapositivediagonalmatrix,N oisthenumberofnon-zerosingularvaluesofD µA µ.
Usingthisdecomposition,thegradientdescentdynamicsisrewrittenas
2η
W˙ (t)=− (WU Σ −B V )Σ UT. (17)
N µ µ µ µ µ µ
y
Thus,theweightmatrixW(t)atarbitrarytiswrittenas
W(t)=W(t=0)+Q(t)UT, (18)
µ
whereQ ∈ RNy×No isatime-dependentmatrix. Atthefixedpointofthislearningdynamics,we
have
(cid:0)(cid:2) W +QUT(cid:3) U Σ −B V (cid:1) Σ UT =O, (19)
µ−1 µ µ µ µ µ µ µ
whereOisthezeromatrixandW ≡W(t=0)istheweightafterlearningoftheprevioustask.
µ−1
BecauseΣ isapositivediagonalmatrix,theequationabovehasauniquesolution:
µ
Q=(B V −W U Σ )Σ−1, (20a)
µ µ µ−1 µ µ µ
W =W (cid:0) I −U UT(cid:1) +B (D A )+, (20b)
µ−1 x µ µ µ µ µ
where(D A )+ =V Σ−1UT isthepseudo-inverseofD A ,andI isthesizeN identitymatrix.
µ µ µ µ µ µ µ x x
FromEqs. 20band14,wehave
ϵ µ[W µ]= N1 (cid:13) (cid:13)B µ(cid:0) V µV µT −I s(cid:1)(cid:13) (cid:13)2
F
. (21)
y
Moreover,bysettingtheinitialweightW (theweightbeforethefirsttasks)tozero,wehave
o
1
ϵ [W ]= ∥B ∥2 , (22)
µ o N µ F
y
andW andW follow
1 2
W =B (D A )+, W =B (D A )+(I −U UT)+B (D A )+. (23)
1 1 1 1 2 1 1 1 x 2 2 2 2 2
15Theerrorintask2afterlearningoftask1isthuswrittenas
ϵ 2[W 1]= N1 (cid:13) (cid:13)B 2−B 1(D 1A 1)+D 2A 2(cid:13) (cid:13)2
F
. (24)
y
Similarly,theerrorontask1afterlearningoftask2is
ϵ 1[W 2]= N1 (cid:13) (cid:13)B 1(cid:0) I s−(D 1A 1)+(I x−U 2U 2T)D 1A 1(cid:1) −B 2(D 2A 2)+D 1A 1(cid:13) (cid:13)2
F
(25)
y
NotethattheresultssofardoesnotrelyonAssumptionsIandII,makingthemapplicabletoarbitrary
taskmatrices(A,B).
The weight matrix after learning depends on the pseudo-inverse, (DA)+ = VΣ−1UT, which is
typicallyacomplicatedfunctionoftheoriginalmatrixDA. However,whenαN ≫N ,itcanbe
x s
approximatedbyascaledtranspose, Ns (DA)T (seeEq. 101).
αNx
Randomandadaptiveactivitygating Intheadaptiveactivitygatingmodel,wegenerateinput
gatingunits{gµ}randomly,butwithcorrelationsbetweensubsequenttasks. Wesamplethegating
i
unitsforthefirsttaskwithg1 ←Bernoulli(α),whereαisthesparsityofthegatingunits. Thegating
i
unitsfortheµ+1task,{gµ+1},arethengeneratedby
i
(cid:26) gµ (withprobabilityρg )
gµ+1 = i µ+1 (26)
i Bernoulli(α) (otherwise)
Here,ρg isthecorrelationbetweenthegatingunitsfortheµ-thandµ+1-thtasks. Thegating
µ+1
correlationρg issettozerointhecaseofrandomtask-dependentgating, whereasρg isadjusted
µ µ
basedonthemodelperformancerightafteramodelswitchintheadaptivetask-dependentgating. To
thisend,weintroduceaprobetrialwherethemodelsolvesthenewtaskusingthegatingvectorfor
theoldtask. Theerrorontheprobetrialis
1
ϵ ≡ ∥B −W D A ∥2 . (27)
pb N 2 1 1 2 F
y
Wethensettheprobabilityofusingthesamegatingelementsby
(cid:110) (cid:111)
ρ =max 0,1− ϵpb (28)
g ϵo
whereϵ ≡ 1 ∥B ∥2 isthebaselineerroronthetask2. Itkeepsthegatingthesameifϵ = 0,
o Ny 2 F pb
whileitresamplesallgatingelementsincaseϵ ≥ϵ .
pb o
Transferperformance Theaveragetransferperformance∆ϵ fromthefirsttothesecondtasks
TF
is:
1 (cid:68) (cid:69) 1 (cid:28)(cid:13) (cid:13)2(cid:29)
∆ϵ = ∥B ∥2 − (cid:13)B −B (D A )+D A (cid:13)
TF N y 2 F N y (cid:13) 2 1 1 1 2 2(cid:13) F
= N1 (cid:68) 2tr(cid:2) B 2TB 1(D 1A 1)+D 2A 2(cid:3) −(cid:13) (cid:13)B 1(D 1A 1)+D 2A 2(cid:13) (cid:13)2 F(cid:69)
y
= 2 Nρ b (cid:10) tr(cid:2) (D 1A 1)+D 2A 2(cid:3)(cid:11) − N1 (cid:68)(cid:13) (cid:13)(D 1A 1)+D 2A 2(cid:13) (cid:13)2 F(cid:69) . (29)
s s
Inthelastline,wetooktheexpectationoverthecorrelatedrandommatricesB andB . Usingthe
1 2
approximation(DA)+ ≈ αN Ns x(DA)T (Eq. 101),andthentakingtheexpectationoverA 1,A 2,D 1,
andD ,thefirsttermbecomes
2
(cid:10) tr[(D 1A 1)+D 2A 2](cid:11) ≈ αN Ns
x
(cid:10) tr[AT 1D 1D 2A 2](cid:11)
= αN Ns
x
(cid:88)Ns (cid:88)Nx
(cid:68)
a( j1 i)g j(1)g j(2)a( j2
i)(cid:69)
=α˜ρ aN s, (30)
i=1j=1
wherea(1)representsthe(j,i)-thelementofA ,andα˜isdefinedby
ji 1
α˜ ≡ρ +(1−ρ )α. (31)
g g
16ThesecondtermofEq. 29follows
(cid:68)(cid:13) (cid:13)(D 1A 1)+D 2A 2(cid:13) (cid:13)2 F(cid:69) ≈(cid:16) αN Ns x(cid:17)2(cid:68)(cid:13) (cid:13)AT 1D 1D 2A 2(cid:13) (cid:13)2 F(cid:69)
=(cid:16)
Ns
(cid:17)2
(cid:88)Ns (cid:88)Nx
(cid:68) g(1)g(2)g(1)g(2)a(1)a(2)a(2)a(1)(cid:69)
αNx j j l l ji jk lk li
i,k=1j,l=1
=
1
(cid:88)Ns (cid:88)Nx
(cid:68)
g(1)g(2)g(1)g(2)[δ ρ2 +δ +δ δ
ρ2](cid:69)
α2N2 j j l l ik a jl ik jl a
x i,k=1j,l=1
=N s(cid:18) α˜2ρ2 a+ αα˜ NN xs (cid:16) 1+ Nρ2 a s(cid:17)(cid:19) . (32)
The third line follows from Isserlis’ theorem, from which we can decompose the higher-order
correlationasbelow:
(cid:68) (cid:69) (cid:68) (cid:69)(cid:68) (cid:69) (cid:68) (cid:69)(cid:68) (cid:69) (cid:68) (cid:69)(cid:68) (cid:69)
a(1)a(2)a(2)a(1) = a(1)a(2) a(2)a(1) + a(1)a(1) a(2)a(2) + a(1)a(2) a(2)a(1)
ji jk lk li ji jk lk li ji li jk lk ji lk jk li
= Nρ2 a 2δ ik+ N1 2δ jl+ Nρ2 a 2δ ikδ jl. (33)
s s s
Summinguptheequationsabove,at Ns →0limit,wehave
αNx
∆ϵ =α˜ρ (2ρ −α˜ρ ). (34)
TF a b a
Retention performance Let us next analyze the retention performance, ∆ϵ ≡
RT
⟨ϵ [W ]−ϵ [W ]⟩ ,whichcharacterizeshowwellthenetworkperformsthetask1afterlearning
1 o 1 2 A,B
task2. AtαN ≫N regime,⟨ϵ [W ]⟩ =1. InsertingEq. 23intoEq. 14,weget
x s 1 o A,B
ϵ 1[W 2]= N1 (cid:13) (cid:13)B 1−(cid:2) B 1(D 1A 1)+(I x−U 2U 2T)+B 2(D 2A 2)+(cid:3) D 1A 1(cid:13) (cid:13)2
F
y
= N1 (cid:13) (cid:13)B 1(D 1A 1)+U 2U 2TD 1A 1−B 2(D 2A 2)+D 1A 1(cid:13) (cid:13)2
F
. (35)
y
Thus,∆ϵ follows
RT
∆ϵ
RT
= N1 (cid:68) ∥B 1∥2 F(cid:69) − N1 (cid:68)(cid:13) (cid:13)B 1(D 1A 1)+U 2U 2TD 1A 1(cid:13) (cid:13)2 F(cid:69) − N1 (cid:68)(cid:13) (cid:13)B 2(D 2A 2)+D 1A 1(cid:13) (cid:13)2 F(cid:69)
y y y
+ 2 (cid:68) tr(cid:104) BTB (D A )+U UTD A (D A )T (cid:0) (D A )+(cid:1)T(cid:105)(cid:69)
N 2 1 1 1 2 2 1 1 1 1 2 2
y
=1− N1 (cid:68)(cid:13) (cid:13)(D 1A 1)+U 2U 2TD 1A 1(cid:13) (cid:13)2 F(cid:69) − N1 (cid:68)(cid:13) (cid:13)(D 2A 2)+D 1A 1(cid:13) (cid:13)2 F(cid:69)
s s
+ 2ρ b (cid:68) tr(cid:104) (D A )+U UTD A (D A )T (cid:0) (D A )+(cid:1)T(cid:105)(cid:69) . (36)
N 1 1 2 2 1 1 1 1 2 2
s
BecauseD A ATD =U Σ2UT,attheN ≪αN limit,U UT isapproximatedby(seeEq. 102)
2 2 2 2 2 2 2 s x 2 2
N
U UT ≈ s D A ATD . (37)
2 2 αN 2 2 2 2
x
17Figure7: Thegatingleveldependenceofthetransferandretentionperformance(A)Phasediagram
ofthegatingleveldependence. (B-D)Transferandretentionperformanceasafunctionofthegating
levelatarepresentativepointofeachphase.
Using this approximation and (DA)+ ≈ Ns (DA)T (Eq. 101), we can obtain an analytical
αNx
expressionforEq. 36. TakingtheexpectationoverDandA,thesecondtermofEq. 36becomes
N1 (cid:18) αN Ns (cid:19)4(cid:68)(cid:13) (cid:13)AT 1D 1D 2A 2AT 2D 2D 1A 1(cid:13) (cid:13)2 F(cid:69)
s x
=(cid:88) (cid:88) (cid:68) g(1)g(1)g(1)g(1)g(2)g(2)g(2)g(2)a(1)a(2)a(2)a(1)a(1)a(2)a(2)a(1)(cid:69)
j l n q j l n q ji jk lk lm nm np qp qi
ijklmnpq
=
1 (cid:88) (cid:88) (cid:68) g(1)g(1)g(1)g(1)g(2)g(2)g(2)g(2)(cid:69)
α4N N4 j l n q j l n q
s x ijklmnpq
×(cid:0) [δ δ δ +δ δ δ +δ δ δ ]ρ4 +[δ δ δ +δ δ δ +δ δ δ +δ δ δ ]ρ2(cid:1)
ik km mp ik lq mp km jn ip a ik km nq km mp jq mp pi jl pi ik ln a
(cid:16) (cid:17)
+O 1
αNx
(cid:16) (cid:17)
=α˜4ρ4 + Ns α˜3(4ρ2 +2ρ4)+O 1 , (38)
a αNx a a αNx
Here,weappliedIsserlis’theoremagain,thenretainedthetermsuptothenext-to-leadingorderwith
respectto Ns . Similarly,thethirdtermofEq. 36becomes
αNx
N1
s
(cid:18) αN Ns x(cid:19)2 (cid:10) tr(cid:2) AT 1D 1D 2A 2AT 2D 2D 1A 1(cid:3)(cid:11) =α˜2ρ2 a+ αα˜ NN xs (cid:16) 1+ Nρ2 a s(cid:17) . (39)
Lastly,thecross-termisevaluatedas
1
(cid:18)
N s
(cid:19)3
(cid:10) tr(cid:2) ATD D A ATD D A ATD D A (cid:3)(cid:11)
N αN 2 2 1 1 1 1 2 2 2 2 1 1
s x
= 1 (cid:88)(cid:88)(cid:68) g(1)g(1)g(1)g(2)g(2)g(2)(cid:69)(cid:0) δ δ ρ3 +δ δ ρ +δ δ ρ +δ δ ρ3(cid:1) +O(cid:16) 1 (cid:17)
α3N sN x3
ijk lmn
j l n j l n ik km a ik ln a im jl a mk jn a αNx
(cid:16) (cid:17)
=α˜3ρ3 a+ αN Ns xα˜2(2ρ a+ρ3 a)+O αN1
x
. (40)
Therefore,uptotheleadingorder,theretentionperformancefollows
∆ϵ RT =1−α˜2ρ2 a(cid:0) α˜2ρ2 a−2α˜ρ aρ b+1(cid:1) +O(cid:16) αN Ns x(cid:17) . (41)
Vanillamodel Inthevanillamodel,wehaveα˜ =1. Therefore,atN ≪N limit,fromEqs. 34
s x
and41,thetransferandretentionperformancefollow
∆ϵ =ρ (2ρ −ρ ), (42a)
TF a b a
∆ϵ =1−ρ2(ρ2 −2ρ ρ +1). (42b)
RT a a a b
Randomactivitygating Undertherandomtask-dependentactivitygating,thegatinglevelα˜ =α
isaconstant. IfN ≪αN ,thetransferandretentionperformancearewrittenby
s x
∆ϵ =αρ (2ρ −αρ ), (43a)
TF a b a
∆ϵ =1−α2ρ2(α2ρ2 −2αρ ρ +1). (43b)
RT a a a b
18Notethat,thisexpressiondoesnotholdatthesparselimitα< Ns.
Nx
Dependingon(ρ ,ρ )combination,thegatinglevelinfluencesthetransferandretentionperformance
a b
in three different manners (Fig. 7). When ρ > ρ , unless both ρ and ρ are large, there is a
b a a b
monotonic tradeoff between the transfer and retention performance (Fig. 7B). In the red region
√
whereρ
b
≥ 2ρ 3a + 3ρ1
a
orρ
b
≥ 2 32 ∧ρ
a
≥ √1 2,alargegatinglevelaboveathresholdimprovesthe
retentionperformance(Fig. 7C).Whenρ <ρ ,highgatinglevel,α> ρb,lowerbothtransferand
b a ρa
retentionperformance,thusthereisnobenefitofchoosinglargeα(Fig. 7D).
Adaptiveactivitygating Intheadaptivetask-dependentactivitygatingmodel,weintroduceda
probetrialatthebeginningofthetask2totestthemodelperformanceforthenewtask. Underan
(cid:110) (cid:111)
adaptivegatingwithρ =max 0,1− ϵpb ,theprobeerrorϵ follows:
g ϵo pb
(cid:28) (cid:29)
1
⟨ϵ ⟩ = ∥B −W D A ∥2
pb A,B,g N 2 1 1 2 F
y A,B,g
≈(cid:42)
N1
(cid:13)
(cid:13) (cid:13) (cid:13)B 2− αN Ns B 1(D 1A 1)TD 1A
2(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:43)
y x F A,B,g
(cid:18) (cid:19)
N
=1−2ρ ρ +ρ2 +O s . (44)
a b a αN
x
Thus,atN ≪αN region,ρ =max{0,ρ (2ρ −ρ )},andtheeffectivegatinglevelbecomes
s x g a b a
(cid:26)
α+(1−α)ρ (2ρ −ρ ) (if 2ρ ≥ρ )
α= a b a b a (45)
(cid:101) α (otherwise)
meaningthatifthetransferperformanceispositiveinthevanillamodel,thereshouldbeanoverlap
inthegating,butotherwise,thegatingvectorforthenexttaskshouldbesampledindependently.
Average error under a uniform prior on task similarity Gating influences the performance
differentlydependingonthefeatureandreadoutsimilarity. Thus,ideallythemodelshouldadjustthe
gatinglevelbasedonthetasksimilarity,butinmostpracticalsettings,themodeldoesn’tknowthe
similarityapriori. Therefore,itisimportanttoanalyzehowitperformsonaverageunderarandomly
chosentasksimilarity. Tothisend,herewemeasuretheaveragetransferandretentionperformance
assumingauniformprioron0≤ρ ,ρ ≤1.
a b
Inthevanillamodel,wehave
(cid:90) 1 (cid:90) 1 1
∆ϵ¯ = dρ dρ (ρ (2ρ −ρ ))= , (46a)
TF a b a b a 6
0 0
∆ϵ¯ =(cid:90) 1 dρ (cid:90) 1 dρ (cid:0) 1−ρ2(cid:2) ρ2 −2ρ ρ +1(cid:3)(cid:1) = 43 . (46b)
RT a b a a a b 60
0 0
RedandbluedashlinesinFigs. 2D,3C,3D,4D,5C,and5Drepresentthesebaselineperformanceof
thevanillamodel: ∆ϵ¯ = 1 and∆ϵ¯ = 43. Bycontrast,underarandomtask-dependentgating,
TF 6 RT 60
α α2 α2 α3 α4
∆ϵ¯ = − , ∆ϵ¯ =1− + − . (47)
TF 2 3 RT 3 4 5
SolidlinesinFig. 2DanddashedlinesinFig. 3Bplottheresultabove. Inthecaseofadaptivegating,
theexpressionoftheaverageperformancebecomescomplicated,yetstillnumericallytractable.
Notethattheaveragetransferperformanceisexpectedtobelowerthantheretentionperformance,
becauseforwardknowledgetransferrequiresahighsimilarity. Evenifweusetheoptimalgating
levelfortransfer,α∗(ρ ,ρ )=min(1, ρb),theaveragetransferperformancebecomes
a b ρa
(cid:90) 1 (cid:90) 1 1
dρ dρ α∗(ρ ,ρ )ρ (2ρ −α∗(ρ ,ρ )ρ )= . (48)
a b a b a b a b a 4
0 0
19B.2 Plasticitygating
An alternative strategy is to gate plasticity at certain synapses in a task-dependent manner while
keepingtheactivityintact. Weimplementthismethodbymakingsynapsesfromonlyasubsetof
inputneuronsplasticforeachtask. Givenalinearregressionmodely =Wx,thelearningdynamics
follows:
W˙ (t)=−η(WA −B )(D A )T, (49)
µ µ µ µ
whereD =diag(g )isadiagonalmatrixspecifyingwhichsynapsesaregated. Asintheactivity
µ µ
gating,g isagatingvectorwhoseelementstakeeitheroneorzeroinatask-dependentmanner. We
µ
sampleelementsofg fromaBernoullidistributionwithprobabilityαasbefore.
µ
LetusdenoteSVDofA byA =U Λ VT. Then,fromaparallelargumentwithEqs. 18and19,
µ µ µ µ µ
wehave
W =W +(B V Λ−1−W U )(UTD U )−1UTD . (50)
µ µ−1 µ µ µ µ−1 µ µ µ µ µ µ
Notably, unlike Eq. 20b, gating term D appears both inside and outside of the inverse term
µ
((UTD U )−1andUTD ). Thus,uptotheleadingorder,gatinglevelαdoesnotaffectthetransfer
µ u µ µ µ
andretentionperformance.
B.3 Inputsoft-thresholding
Input-dependentgatingisananothermethodproposedforefficientcontinuallearning. Inaregression
setting,itcorrespondstosoft-thresholdingoftheinputs. Letusintroduceasoft-thresholdingfunction
φ(x)by
φ(x)=sgn(x)max{0,|x|−h}, (51)
wheresgn(x)representsthesignofx. Thisnonlinearitymakestheanalysisdifficult,butifN ≫1
s
inadditiontoAssumptionsIandII,theerrorbecomestractable. NotethatN ≫1assumptionis
s
requiredonlyinthissubsection. Underthestudentnetworkdefinedby
yˆ=Wφ(x), (52)
themean-squarederrorfollows
ϵ[W]=
1 (cid:68) ∥Bs∥2(cid:69)
+
1 (cid:68) ∥Wφ(As)∥2(cid:69)
−
2 (cid:10) tr(cid:2) Bsφ(As)TWT(cid:3)(cid:11)
. (53)
N y s N y s N y s
Letusdenotem
≡(cid:80)Ns
A s ,thenwehave
i j=1 ij j
⟨m ⟩=0, (cid:10) m2(cid:11)
=(cid:88)Ns
A2 ≈1, ⟨s m ⟩=A . (54)
i i ij j i ij
j=1
Thus,s andm approximatelyfollowsajointGaussiandistribution:
j i
(cid:18) (cid:19) (cid:18)(cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
s 0 1 A
j ∼N , ji . (55)
m 0 A 1
i ji
DenotingA =ρforbrevity,wehave
ji
⟨s φ(m )⟩
j i
=(cid:90) ∞ ds(cid:32) (cid:90) −h dm(m+h)+(cid:90) ∞ dm(m−h)(cid:33) s exp(cid:18)
−
1 (cid:2) s2+m2−2ρsm(cid:3)(cid:19)
2π(cid:112) 1−ρ2 2(1−ρ2)
−∞ −∞ h
(cid:32) (cid:33)
(cid:90) ∞ (cid:90) −h (cid:90) ∞ s (cid:104) (cid:105)
= ds dm(m+h)+ dm(m−h) 1+ρsm+ ρ2 (s2−1)(m2−1)+O(ρ3)
2π 2
−∞ −∞ h
(cid:18) s2+m2(cid:19)
×exp −
2
(cid:104) (cid:105)
=erfc √h ρ+O(ρ3). (56)
2
20In the third line, we performed a Taylor expansion around ρ = 0. Similarly, the expectation of
φ(m )φ(m )oversiswrittenas
j i
(cid:32) (cid:33)(cid:32) (cid:33)
1 (cid:90) −h (cid:90) ∞ (cid:90) −h (cid:90) ∞
⟨φ(m )φ(m )⟩= dm(m+h)+ dm(m−h) dm′(m′+h)+ dm′(m′−h)
j i 2π
−∞ h −∞ h
×(cid:104) 1+ρ cmm′+ ρ 22 c(m2−1)(m′2−1)+O(ρ3 c)(cid:105) exp(cid:18) −m2+ 2m′2(cid:19)
(cid:16) (cid:104) (cid:105)(cid:17)2
= erfc √h
2
ρ c+O(ρ3 c), (57)
whereρ
=(cid:80)Ns
A A . Therefore,if|ρ|≪1and|ρ |≪1,theerrorϵ[W]iswrittenas
c k=1 ik jk c
ϵ[W]= 1 ∥B∥2 + α2 tr(cid:2) WAATWT(cid:3) − 2α tr[BATWT], (58)
N F N N
y y y
(cid:104) (cid:105)
where the input activity sparseness α follows α = erfc √h . Under this loss function, gradient
2
descentfromW =W convergesto
o
1
W =W (I−UUT)+ BA+, (59)
o α
whereA=UΛVT isthesingularvaluedecompositionofA. Therefore,fromW =O,theweights
o
aftertask1andtask2become
1 1
W
1
= αW(cid:102)1, W
2
= αW(cid:102)2, (60)
where
W(cid:102)1 ≡B 1A+ 1, W(cid:102)2 ≡B 1A+
1
(cid:2) I−U 2U 2T(cid:3) +B 2A+ 2. (61)
Thismeansthatforbothtasks(µ,ν =1,2)
1 1 (cid:104) (cid:105) 2
ϵ µ[W ν]=
N
∥B µ∥2
F
+
N
tr W(cid:102)νA µAT µW(cid:102) νT −
N
tr[B µAT µW(cid:102) νT], (62)
y y y
implyingthattheerrorisinvariantagainstinputsparsenessα. Thus,inourproblemsetting,input
sparsificationviasoft-thresholdingdoesn’tinfluenceknowledgetransferorretention. Italsoimplies
thatfromenergyefficiencyperspective,soft-thresholdingisbeneficialbecauseitreducesthenumber
ofactiveneuronswhilepreservingitslearningproficiency.
C Weightregularization
C.1 WeightregularizationinEuclideanmetric
LetusnextconsideraweightregularizationapproachbyintroducingL2regularizationwithrespect
totheweightlearnedintheprevioustask. Thelossfunctionℓ fortheµ-thtaskisthengivenby
µ
1 λ
ℓ = ∥B −WA ∥2 + ∥W −W ∥2 . (63)
µ 2 µ µ F 2 µ−1 F
Whenλ>0,thereexistsauniquesolution:
W =(B AT +λW )(A AT +λI )−1. (64)
µ µ µ−1 µ µ x
Thus,fromzero-initialization,W andW become
1 2
W =B AT (cid:0) A AT +λI (cid:1)−1 (65a)
1 1 1 1 1 x
W =(cid:0) B AT +λB AT[A AT +λI ]−1(cid:1) (A AT +λI )−1. (65b)
2 2 2 1 1 1 1 x 2 2 x
UnderN ≫N ,theinversetermisapproximatedby
x s
(cid:0) AAT +λI (cid:1)−1 = 1 I − 1
A(cid:18)
I + 1
ATA(cid:19)−1
AT
x λ x λ2 s λ
(cid:18) (cid:19)
1 N
≈ I − s AAT . (66)
λ x N +λN
x s
21Figure8: WeightregularizationinEuclideanmetric. (A,B)Optimalregularizercoefficientγ that
maximizesthetransferperformance(A),andthemaximumperformanceattheoptimalγ (B)under
various (ρ , ρ ) pairs. (C,D) Optimal regularizer coefficient for retention (C), and the resultant
a b
performance(D).PanelCisthesamewithFig. 4C(replicatedforcompleteness).
Inthefirstline,weusedWoodburymatrixidentity,andinthesecondline,weusedATA≈ N Nx sI s
(seeEq. 99). Letusintroducenormalizedregularizationamplitudeγ andγ˜by
N N
γ ≡ x , γ ≡ s . (67)
N +λN (cid:101) N +λN
x s x s
Undertheseapproximations,W andW simplifyto
1 2
W ≈ 1B AT (cid:0) 1−γA AT(cid:1) ≈γB AT, (68a)
1 λ 1 1 (cid:101) 1 1 (cid:101) 1 1
W ≈ 1 (cid:0) B AT +λγB AT(cid:1)(cid:0) I −γA AT(cid:1) ≈γ(cid:0) B AT +B AT(cid:1) −γ2B ATA AT. (68b)
2 λ 2 2 (cid:101) 1 1 x (cid:101) 2 2 (cid:101) 1 1 2 2 (cid:101) 1 1 2 2
Here,wefurtherappliedATA≈ N Nx sI s. Notethat,unlikeEq. 20b,theresultaboveholdsonlyunder
AssumptionsI&II.
Transferperformance Thetransferperformance∆ϵ follows
TF
∆ϵ
TF
= N1 (cid:68) ∥B 2∥2 F(cid:69) − N1 (cid:68)(cid:13) (cid:13)B 2−γ (cid:101)B 1AT 1A 2(cid:13) (cid:13)2 F(cid:69)
y y
= N2γ (cid:101) (cid:10) tr[B 2TB 1AT 1A 2](cid:11) − Nγ (cid:101)2 (cid:68)(cid:13) (cid:13)B 1AT 1A 2(cid:13) (cid:13)2 F(cid:69)
y y
= 2 Nγ (cid:101)ρ b (cid:10) tr[AT 1A 2](cid:11) − Nγ (cid:101)2 (cid:68)(cid:13) (cid:13)AT 1A 2(cid:13) (cid:13)2 F(cid:69)
s s
(cid:16) (cid:17)
=γρ a(2ρ b−γρ a)+O NN xs . (69)
Inthelastline,weusedEq. 33toestimate(cid:68)(cid:13) (cid:13)AT 1A 2(cid:13) (cid:13)2 F(cid:69) . Notably,theexpressionofthetransfer
performance, ∆ϵ , coincides with Eq. 5a under γ → α, indicating that in terms of transfer
TF
performance,theweightregularizationwithamplitude Nx (cid:0)1 −1(cid:1) isequivalenttorandomactivity
Ns α
gatingwithgatinglevelα. Figs. 8AandBshowtheoptimalregularizercoefficientγ thatmaximizes
transferperformanceandtheresultantperformance.
Retentionperformance Theaverageerrorontask1afterlearningtask2is
⟨ϵ [W ]⟩
1 2 A,B
(cid:28) (cid:29)
= N1 (cid:13) (cid:13)B 1−(cid:0) γ (cid:101)[B 1AT
1
+B 2AT 2]−γ (cid:101)2B 1AT 1A 2AT 2(cid:1) A 1(cid:13) (cid:13)2
F
y
= N1 (cid:68)(cid:13) (cid:13)B 1(I−γ (cid:101)AT 1A 1)(cid:13) (cid:13)2 F(cid:69) + N2γ (cid:101) (cid:10) tr(cid:2) (I−γ (cid:101)AT 1A 1)B 1T(γ (cid:101)B 1AT 1A 2−B 2)AT 2A 1(cid:3)(cid:11)
y y
+ Nγ (cid:101)2 (cid:68)(cid:13) (cid:13)(γ (cid:101)B 1AT 1A 2−B 2)AT 2A 1(cid:13) (cid:13)2 F(cid:69) . (70)
y
22TakingtheexpectationoverA 1,A 2,B 1,B 2,uptotheleadingorderwithrespectto NN xs,wehave
N1 (cid:68)(cid:13) (cid:13)B 1(I−γ (cid:101)AT 1A 1)(cid:13) (cid:13)2 F(cid:69) = N1 (cid:68) ∥B 1∥2
F
−2γ (cid:101)tr[B 1TB 1AT 1A 1]+γ (cid:101)2(cid:13) (cid:13)B 1AT 1A 1(cid:13) (cid:13)2 F(cid:69)
y y
=1− N2γ (cid:101) (cid:10) tr[AT 1A 1](cid:11) + Nγ (cid:101)2 (cid:68)(cid:13) (cid:13)AT 1A 1(cid:13) (cid:13)2 F(cid:69)
s s
(cid:16) (cid:17)
=(1−γ)2+O Ns . (71)
Nx
Similarly,wehave
γ (cid:101) (cid:10) tr(cid:2) (I−γATA )BT(γB ATA −B )ATA (cid:3)(cid:11)
N (cid:101) 1 1 1 (cid:101) 1 1 2 2 2 1
y
= γ (cid:101) (cid:10) tr(cid:2) (I−γATA )(γATA −ρ I)ATA (cid:3)(cid:11)
N (cid:101) 1 1 (cid:101) 1 2 b 2 1
s
(cid:16) (cid:17)
=ρ aγ(1−γ)(γρ a−ρ b)+O NN xs , (72)
and
N1 γ (cid:101)2(cid:68)(cid:13) (cid:13)(γ (cid:101)B 1AT 1A 2−B 2)AT 2A 1(cid:13) (cid:13)2 F(cid:69)
y
= γ (cid:101)2 (cid:10) tr[ATA ATA ]−2γρ tr[ATA ATA ATA ]+γ2tr[ATA ATA ATA ATA ](cid:11)
N 1 2 2 1 (cid:101) b 1 2 1 2 2 1 (cid:101) 1 2 2 1 1 2 2 1
s
=ρ2 aγ2(cid:0) 1−2ρ aρ bγ+ρ2 aγ2(cid:1) +O(cid:16) NN xs(cid:17) . (73)
Therefore,theerroriswrittenas
⟨ϵ [W ]⟩=(1−γ)2−2γρ (1−γ)(ρ −γρ )+γ2ρ2(1−2γρ ρ +γ2ρ2). (74)
1 2 a b a a a b a
Theoptimalγ foreachtasksimilarity(ρ ,ρ )andtheresultantretentionperformanceareplottedin
a b
Figs. 8CandD.
C.2 Elasticweightregularization
In the previous section, we regularized the change in the weight matrix in the Euclidean space.
However,previousworksindicatethattheweightshouldberegularizedusingtheFisherinformation
matrix(FIM)asthemetric[29,65]. Letusconstructaninferencemodelbyy =Wx+σξ,whereξ
isazeromeanGaussianrandomvariable. Giveninput-targetpairx,y,thelikelihoodofweightW is
(cid:18) (cid:19)
1
p(W|x,y)∝p(x,y|W)p(W)∝exp − ∥Wx−y∥2 . (75)
2σ2
Thus,(ij,kl)-thcomponentofFIMbecomes
(cid:28) ∂2 (cid:29) 1 (cid:42) (cid:88)(cid:88) (cid:43) 1 (cid:88)
(−logp(W|x,y)) = δ a a s s = δ a a ,
∂w w 2σ2 ik jm ln m n 2σ2 ik jn ln
ij kl s m n s n
(76)
andthequadraticweightregularizationintheFisherinformationmetricfollows
(cid:32) (cid:33)
(cid:88)(cid:88) δ (cid:88) a a (w −wo)(w −wo )=∥(W −W )A∥2 . (77)
ik jn ln ij ij kl kl o F
ij kl n
Therefore,thelossfunctionforthisweightregularizeriswrittenas
1 λ
ℓ = ∥B −WA ∥2 + ∥(W −W )A ∥2 . (78)
µ 2 µ µ F 2 µ−1 µ−1 F
Aswewillsee,thislossfunctionhasdifferentformsofsolutiondependingonwhetherρ < 1or
a
ρ =1. Wethusconsiderthesetwoconditionsseparatelybelow.
a
23C.2.1 Variablefeatures(ρ <1)
a
TakinggradientwithrespecttoW,wehave
∂ℓ
µ =−(B −WA )AT +λ(W −W )A AT , (79)
∂W µ µ µ µ−1 µ−1 µ−1
indicating that W is written as W = W
µ−1
+QA(cid:101)T where Q is a N
y
×2N
s
matrix and A(cid:101) ≡
[A ,A ]isanN ×2N matrix. Solving ∂ℓµ =0,weget
µ−1 µ x s ∂W
(cid:18) λAT A AT A (cid:19)(cid:18) AT (cid:19) (cid:18) AT (cid:19)
Q µ−1 µ−1 µ−1 µ µ−1 =(O B −W A ) µ−1 , (80)
λATA ATA AT µ µ−1 µ AT
µ µ−1 µ µ µ µ
Multiplyingbothsideswith(A A )fromtherightside,
µ−1 µ
(cid:18) λAT A AT A (cid:19)(cid:18) AT A AT A (cid:19)
Q µ−1 µ−1 µ−1 µ µ−1 µ−1 µ−1 µ
λATA ATA ATA ATA
µ µ−1 µ µ µ µ−1 µ µ
(cid:18) AT A AT A (cid:19)
=(O B −W A ) µ−1 µ−1 µ−1 µ (81)
µ µ−1 µ ATA ATA
µ µ−1 µ µ
Ifλ̸=0andρ <1,twosquarematricesintheaboveequationsarealmostsurelyinvertibleunder
a
assumptionsI&II.Therefore,wehave
(cid:18) λAT A AT A (cid:19)−1
Q=(O B −W A ) µ−1 µ−1 µ−1 µ (82)
µ µ−1 µ λATA ATA
µ µ−1 µ µ
UnderN ≫N ,fromEq. 99,theinversematrixisapproximatedby
x s
(cid:18) λAT µ−1A µ−1 AT µ−1A µ(cid:19)−1 ≈ N s (cid:18) λI s ρ aI s(cid:19)−1 = N s (cid:18) λ1I s −ρ λaI s(cid:19)
λAT µA µ−1 AT µA µ N x λρ aI s I s N x(1−ρ2 a) −ρ aI s I s
(83)
Therefore,W =W µ−1+QA(cid:101)T follows
(cid:18) (cid:19)
W =W I− N s A (cid:2) AT −ρ AT (cid:3) + N s B (cid:0) AT −ρ AT (cid:1) . (84)
µ−1 N (1−ρ2) µ µ a µ−1 N (1−ρ2) µ µ a µ−1
x a x a
Notably, the equation above doesn’t depend on the regularizer amplitude λ, except for λ ̸= 0
condition. Atλ→0limit,theinversematrixterminEq. 82effectivelybecomessingular,andthus
theequationabovenolongerholds.
Letussupposethefirsttaskislearnedwithoutanyweightregularization,orlearnedwithweight
regularization in the Fisher information metric imposed by an uncorrelated task. Then, we have
W
1
= NN xsB 1AT 1. Hence,thetransferperformancebecomesthesamewiththevanillamodel. The
weightafterthesecondtaskbecomes
(cid:18) (cid:19)
N N N
W = sB AT I− s A [AT −ρ AT] + s B (AT −ρ AT). (85)
2 N 1 1 N (1−ρ2) 2 2 a 1 N (1−ρ2) 2 2 a 1
x x a x a
Thus,theretentionperformanceunderW follows
2
1 (cid:68) (cid:69)
∆ϵ = ∥B ∥2
RT N 1 F
y
1 (cid:28)(cid:13) (cid:16) (cid:17) (cid:16) (cid:17) (cid:13)2(cid:29)
− N y (cid:13) (cid:13)B 1 I− NN xsAT 1A 1 + Nx(N 1−s ρ2 a) NN xsB 1AT 1A 2−B 2 (AT 2 −ρ aAT 1)A 1(cid:13) (cid:13) F .
(86)
TakingexpectationoverB andB ,∆ϵ isrewrittenas
1 2 RT
(cid:28)(cid:13) (cid:13)2(cid:29)
∆ϵ
RT
=1− N1
s
(cid:13) (cid:13)I− NN xsAT 1A 1(cid:13)
(cid:13)
F
− Nx(12
−ρ2
a)(cid:68) tr(cid:104) (I− NN xsAT 1AT 1)(cid:16) NN xsAT 1A 2−ρ bI(cid:17)(cid:0) AT
2
−ρ aAT 1(cid:1) A 1(cid:105)(cid:69)
− N1
s
(cid:16) Nx(N 1−s
ρ2
a)(cid:17)2(cid:28)(cid:13) (cid:13) (cid:13)NN xsAT 1A 2(AT
2
−ρ aAT 1)A 1(cid:13) (cid:13) (cid:13)2
F
+(cid:13) (cid:13)(AT
2
−ρ aAT 1)A 1(cid:13) (cid:13)2 F(cid:29)
+ 2 Nρ xb (cid:16) Nx(N 1−s
ρ2
a)(cid:17)2(cid:10) tr(cid:2) AT 1(A 2−ρ aA 1)AT 2A 1(AT
2
−ρ aAT 1)A 1(cid:3)(cid:11) . (87)
24Thesecondtermisrewrittenas
N1
s
(cid:28)(cid:13) (cid:13) (cid:13)I− NN xsAT 1A 1(cid:13) (cid:13) (cid:13)2 F(cid:29) =1− N2
x
(cid:10) tr[AT 1A 1](cid:11) + N1
s
(cid:16) NN xs(cid:17)2(cid:10) tr[AT 1A 1AT 1A 1](cid:11) = N Ns+ x1. (88)
Moreover,(AT −ρ AT)A termalsocancelsoutuptotheleadingordertermbecause
2 a 1 1
N1
s
(cid:16) Nx(N 1−s
ρ2
a)(cid:17)2(cid:68)(cid:13) (cid:13)(AT
2
−ρ aAT 1)A 1(cid:13) (cid:13)2 F(cid:69)
= N1
s
(cid:16)
Nx(N 1−s
ρ2
a)(cid:17)2(cid:88)(cid:68)
a( j1 i)(a( j2 k)−ρ aa( j1 k))(a( lk2)−ρ aa( lk1))a(
li1)(cid:69)
ijkl
= N1
s
(cid:16)
Nx(N 1−s
ρ2
a)(cid:17)2(cid:88)(cid:68)
a( j1 i)a(
li1)(cid:69)(cid:68)
(a( j2 k)−ρ aa( j1 k))(a( lk2)−ρ aa(
lk1))(cid:69)
ijkl
= N1
s
(cid:16) Nx(N 1−s
ρ2
a)(cid:17)2(cid:88) δ jl1 N− sρ 22 a = Nx(N 1−s
ρ2 a)
(89)
ijkl
Similarly,thetracetermsarealsocancelledoutuptotheleadingorderterm:
N1
x
(cid:16) Nx(N 1−s
ρ2
a)(cid:17)2(cid:10) tr[AT 1(AT
2
−ρ aA 1)AT 2A 1(AT
2
−ρ aAT 1)A 1](cid:11)
= N1
x
(cid:16) Nx(N 1−s
ρ2
a)(cid:17)2 (cid:88) (cid:68) a( j1 i)(a( j2 k)−ρ aa( j1 k))a( lk2)a( lm1)(a( n2 m) −ρ aa( n1 m))a( n1 i)(cid:69)
ijklmn
= N1
x
(cid:16) Nx(N 1−s
ρ2
a)(cid:17)2 (cid:88) (cid:68) (a( j2 k)−ρ aa( j1 k))(a( n2 m) −ρ aa( n1 m))(cid:69)(cid:68) a( j1 i)a( lk2)a( lm1)a( n1 i)(cid:69)
ijklmn
(cid:16) (cid:17)2 (cid:88)
= N1
x
Nx(N 1−s
ρ2 a)
N1 sδ jnδ km(1−ρ2 a) N1
s2
(ρ a+δ jlδ ik2ρ a)
ijklmn
(cid:16) (cid:17)
= Nsρa 1+ 2 , (90)
Nx(1−ρ2 a) NxNs
and
(cid:68) (cid:69)
Nx(11
−ρ2 a)
tr[(I− NN xsAT 1A 1)( NN xsAT 1A 2−ρ bI)(AT
2
−ρ aAT 1)A 2]
(cid:42)(cid:32) (cid:33)(cid:32) (cid:33) (cid:43)
= Nx(11 −ρ2 a)(cid:88) δ ij − NN xs (cid:88) a( m1 i)a( m1 j) NN xs (cid:88) a( n1 j)a( n2 k)−ρ bδ jk (cid:16) a( lk2)−ρ aa( lk1)(cid:17) a( li2) =0.
ijkl m n
(91)
(cid:16) (cid:17)
Thus,weget∆ϵ RT = 1−O Nx(N 1−s ρ2 a) . Therefore,ifN s ≪ (1−ρ2 a)N x,wehave∆ϵ RT ≈ 1
regardlessoftasksimilarityρ andρ .
a b
C.2.2 Fixedfeatures(ρ =1)
a
Whenρ =1,A =A =A. Thegradientfollows
a µ−1 µ
∂ℓ
µ =[−(B −WA)+λ(W −W )A]AT, (92)
∂W µ µ−1
andtheweightW iswrittenasW =W +QUT,whereU isdefinedbySVDofA:A=UΛVT.
µ−1
Solving ∂ℓµ =0,weget
∂W
(cid:18) (cid:19)
1 1
W =W I− UUT + B A+ (93)
µ−1 1+λ 1+λ µ
Thus,theweightaftertask1and2follow
(cid:18) (cid:19)
1 1 1 1
W = B A+, W = B A+ I− UUT + B A+. (94)
1 1+λ 1 2 1+λ 1 1+λ 1+λ 2
25Therefore,thetransferperformanceis
1 (cid:68) (cid:69) 1 (cid:68) (cid:69)
∆ϵ = ∥B ∥2 − ∥B −W A ∥2
TF N 2 N 2 1 2
y y
2ρ 1
= b − =γ (2ρ −γ ). (95)
1+λ (1+λ)2 f b f
Inthelastline,wedefinedγ byγ ≡ 1 . Similarly,theretentionperformanceiswrittenas
f f 1+λ
1 (cid:68) (cid:69) 1 (cid:68) (cid:69)
∆ϵ = ∥B ∥2 − ∥B −W A∥2
RT N 1 F N 1 2 F
y y
= N1 (cid:68) ∥B 1∥2 F(cid:69) − N1 (cid:42)(cid:13) (cid:13) (cid:13) (cid:13)(cid:18) 1− (1+λ λ)2(cid:19) B 1− 1+1 λB 2(cid:13) (cid:13) (cid:13) (cid:13)2(cid:43)
y y F
=2γ (1−γ )2−γ4+2γ (cid:2) (1−γ )2−γ2(cid:3) ρ . (96)
f f f f f f b
Ifthefirsttaskislearnedwithoutweightregularization,theretentionperformanceinsteadbecomes:
∆ϵ RT = N1 (cid:68) ∥B 1∥2 F(cid:69) − N1
(cid:42)(cid:13)
(cid:13) (cid:13) (cid:13)1+1 λ(B 1−B
2)(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:43)
=1−2γ f2(1−ρ b). (97)
y y F
D Propertiesofverytallzero-meanrandommatrices
Our simple analytical results rely on the assumption that matrix A ∈ RNx×Ns is very tall (i.e.,
N ≫N ),andeachelementofAissampledindependentlyfromanormaldistributionwithmean
x s
zeroandafinitevariance.
(cid:68) (cid:69)
GivenA ∼N(0, 1 ),wehave 1 ATA = 1 I . Theelement-wisedeviationfromthemean
ij Ns Nx
A
Ns s
isevaluatedas
(cid:42)(cid:18)(cid:104)
(cid:105)
(cid:19)2(cid:43)
1+δ
1 ATA− 1 I = ij, (98)
Nx Ns s ij N xN s2
A
(cid:28)(cid:13) (cid:13)2(cid:29) (cid:16) (cid:17)
implyingthat (cid:13) 1 ATA− 1 I (cid:13) = 1 1+ 1 . Therefore,atN ≫N limit, 1 ATA
(cid:13)Nx Ns s(cid:13)
F A
Nx Ns x s Nx
approximatelyfollows
1 ATA≈ 1 I . (99)
Nx Ns s
Eveninthepresenceofrandomgating, iftheeffectivematrixisstilltall(i.e., αN ≫ N ), sim-
x s
ilar approximations hold. Given a diagonal matrix D whose diagonal components are sampled
independentlyfromaBernoullidistributionwithrateα,atαN ≫N limit,wehave
x s
1 (DA)TDA≈ 1 I . (100)
αNx Ns s
Thisresultimpliesthat,
(DA)+ ≈ Ns (DA)T. (101)
αNx
Moreover,denotingtheSVDofDAbyDA=UΣVT,atαN ≫N limit,wehave
x s
UUT ≈ Ns DAATD. (102)
αNx
Thisisbecause,atαN ≫N limit,fromEq. 100,theeigenspectrumofmatrix 1 DA(DA)T is
x s αNx
concentratedatλ= 1 ,andthus,
Ns
Ns DAATD = Ns UΣ2UT ≈UUT. (103)
αNx αNx
26Figure9: PermutedMNISTwithalatentvariable. (A)Learningcurvesunder(ρ ,ρ )=(0.8,0.2).
a b
(B)Knowledgetransferundervarioustasksimilarities. (C)Comparisonoftheretentionperformance
measured after 10 epochs (black) and 100 epochs (gray) of training. (D) Retention performance
after100epochsoftrainingwithtask2. After100epochsoftraining,thesystemstillexhibitsstrong
asymmetricdependenceonthefeatureandreadoutsimilarities,butthenon-monotonicdependence
onthefeaturesimilarityunderρ ≈ 1regiondisappears. (E,F)Retentionperformanceunderthe
b
weightregularizationintheFisherinformationmetric(FIM)withalayer-wiseapproximation(panel
E)andadiagonalapproximation(panelF),under(ρ ,ρ ) = (0.5,0.5). UnlikeFigs. 6F-Hinthe
a b
mainfigure,weadjustedtheregularizeramplitudeoftwolayersindependently. AsinFig. 6,error
barsinpanelA-Crepresentstandarderrorsover10randomseeds.
E Numericalmethods
NumericalexperimentswereconductedinstandardlaboratoryGPUsandCPUs. Sourcecodesforall
numericalresultsaremadepubliclyavailableathttps://github.com/nhiratani/transfer_
retention_model.
E.1 Implementationoflinearteacher-studentmodels
Numerical results in Figs. 1-5 and 7-8 were implemented as below. We set the latent variable
dimensionalityN =30,theinputwidthN =3000,andtheoutputwidthN =10. Thestudent
s x y
weightW wasinitializedasthezeromatrix,andupdatedwiththefullgradientdescentwithlearning
rateη =0.001. WetrainedthenetworkwiththefirsttaskforN epochs,andthenretrainedit
epoch
withthesecondtaskforanotherN epochs. WeusedN =100forthevanillamodeland
epoch epoch
theactivitygatingmodels,butweusedN = 500forothermodels. Unlessstatedotherwise,
epoch
errorbarsrepresentstandarddeviationover10randomseeds. AverageperformancesinFigs. 2C,
3B-D,4D,and5CDwereestimatedbytakingaverageover100pairsof(ρ ,ρ ),sampleduniformly
a b
fromρ ,ρ ∈[0,1].
a b
Intheinputsoft-thresholdingmodel,weestimatedthegradientinasample-basedmannerbecause
theexactgradientisnottractableduetononlinearity. Weestimatedthegradientfrom10000random
samplesateachiterations,thenupdatedthemodelfor5000iterationswithlearningη =0.01.
E.2 ImplementationofpermutedMNISTwithlatent
Datageneration WeusedpermutedMNISTdataset[33,21],acommonbenchmarkforcontinual
learning,butwithadditionofthelatentspace. Weconstructedafourdimensionallatentspacesusing
thebinaryrepresentationofdigits,s =[0,0,0,0]T,s =[0,0,0,1]T,andsoon. Thetargetoutput
0 1
wasgeneratedbyaprojectionofthelatentvariablestoaten-dimensionalspace,y∗ =B(s− 11),
2
whereBisa10×4matrixgeneratedrandomly. TheelementsofmatrixB forthefirsttaskwere
1
sampledindependentlyfromaGaussiandistributionwithmeanzeroandvarianceone. Forthesecond
task, we introduced readout similarity by keeping some of the elements while resampling other
elements. Wedefinedthereadoutsimilarityρ bythefractionoftheelementskeptthesamebetween
b
twotasks.
27Featuresimilaritywasintroducedbypermutingtheinputpixelsasdonepreviously[21]. Weusedthe
vanillaMNISTimagesforthefirsttaskandpermutedsomeofthepixelsinthesecondtaskdepending
onthefeaturesimilarityρ .
a
Modelimplementation WeusedonehiddenlayerneuralnetworkwithReLUnon-linearityinthe
hiddenlayer: y = b +W ReLU[b +W x]. WesetthehiddenlayerwidthtoN = 1500. The
2 2 1 1 h
inputandoutputwidthsweresettoN =784andN =10. WeinitializedthefirstweightW and
x y 1
thebiasb bysamplingeachweightindependentlyfromaGaussiandistributionwithmeanzero
1
andvariance 1 . WeinitializedthesecondweightW andthebiasb inthesamemanner,butwith
N2 2 2
h
variance 1 . Here,wesettheamplitudeoftheinitialweightssmalltooperatethelearninginthe
N2
y
richregime[51]. Wesetthemini-batchsizeto300andthelearningratetoη = 0.01,andtrained
thenetworkfor100epochspertask. Theretentionperformancewasmeasuredafter10epochsof
training with task 2 except for Figs. 9 C and D. In the learning of task 2, the test error typically
dropssignificantlyinthefirst10epochsandshiftstogradualimprovementafterward(redlineinFig.
9A).Notably,theasymmetrictasksimilaritydependenceoftheretentionperformancewasobserved
consistentlybothafterashortandlongtrainingonthesecondtask(Figs. 6Band9D).
WeightregularizationintheFisherinformationmetric Fromthesameargumentmadeinsection
C.2,FIMofanoise-freemodelisapproximatedbytheHessianofthemeansquarederror. Given
y =b +W ϕ(b +W x)andℓ= 1∥y−y∗∥2,theHessianisestimatedas
2 2 1 1 2
∂2ℓ
=δ ϕ ϕ , (104a)
∂w(2)∂w(2) ik j l
ij kl
∂2ℓ
=w(2)ϕ′x ϕ +δ (y −y∗)ϕ′x , (104b)
∂w(2)∂w(1) ik k l j jk i i j l
ij kl
∂2ℓ =(cid:88) w(2)ϕ′x w(2)ϕ′x +δ (cid:88) (y −y∗ )w(2)ϕ′′x x . (104c)
∂w(1)∂w(1) mi i j mk k l ik m m mi i j l
ij kl m m
UndertheReLUactivation,thesecond-orderderivativeϕ′′iseffectivelynegligible,thustheweight
regularizationintheFisherinformationmetriciswrittenas
R[W ,W ;(x,y∗)]
1 2
=(cid:88) ∂2ℓ δw(2)δw(2)+2(cid:88) ∂2ℓ δw(2)δw(1)+ ∂2ℓ δw(1)δw(1)
∂w(2)∂w(2) ij kl ∂w(2)∂w(1) ij kl ∂w(1)∂w(1) ij kl
ijkl ij kl ijkl ij kl ij kl
=∥δW ϕ∥2+2tr(cid:2) δWTW ∇ϕδW xϕT(cid:3) +2tr(cid:2) δW ∇ϕδW xδyT(cid:3) +∥W ∇ϕδW x∥2,
2 2 2 1 2 1 2 1
(105)
whereδW ≡ W −Wo andδW ≡ W −Wo arethedeviationsfromthetargetweights,δy =
1 1 1 2 2 2
y−y∗,and∇ϕisadiagonalmatrixwherediagonalcomponentsarederivativeofϕ.Inthelayer-wise
approximation,weomittedthetrace-termsoriginatedfromthecrossterms,whichyields
1(cid:68) (cid:69) 1(cid:68) (cid:69)
R [W ,W ]= ∥δW ϕ∥2 + ∥W ∇ϕδW x∥2 , (106)
lw 1 2 2 2 2 2 1
wheretheexpectationistakenovertrainingdata(x,y∗)generatedfromtheprevioustask. Thus,the
derivativewithrespecttoδW andδW follows
1 2
∂R lw =(cid:10) ∇ϕWTW ∇ϕδW xxT(cid:11) ≈(cid:10) ∇ϕWTW ∇ϕ(cid:11) δW (cid:10) xxT(cid:11) , (107a)
∂(δW ) 2 2 1 2 2 1
1
∂R lw =δW (cid:10) ϕϕT(cid:11) . (107b)
∂(δW ) 2
2
Inthefirstline,wefurtherdecoupledtheexpectationsforcomputationaltractability. Inthenumerical
implementation,wescaledtheregularizationtermontheweightchanges∆W and∆W byZ =
(cid:13)
(cid:13)W 2W
2T(cid:13)
(cid:13)
2(cid:13) (cid:13)(cid:10) xxT(cid:11)(cid:13)
(cid:13) 2 and Z 2 =
(cid:13) (cid:13)(cid:10) ϕϕT(cid:11)(cid:13)
(cid:13) 2, respectively, where ∥·∥ 2 is
th1
e
spectral2
norm
1
and
theexpectationwascalculatedatthelastweightoftheprevioustask. Wedidnotimposeweight
regularizationonthebiasparametersb andb .
1 2
28Inthecaseofdiagonalapproximation,weusedthefollowingregularizer:
R [W ,W ]= 1 (cid:88)(cid:88)(cid:16) w(2)ϕ′x (cid:17)2(cid:16) δw(1)(cid:17)2 + 1 (cid:88) ϕ2(cid:16) δw(2)(cid:17)2 , (108)
diag 1 2 2Z ki i j ij 2Z j ij
1 2
ij k ij
(cid:68) (cid:69)
where the scaling factors Z and Z were defined by Z = max (cid:80) (w(2)ϕ′x )2 and Z =
1 2 1 ij k ki i j 2
max
(cid:10) ϕ2(cid:11)
.
j j
Toseethepotentialeffectofdifferentnormalizationmethodsusedforthelayer-wiseanddiagonal
approximations,inFigs. 9EandF,weindependentlychangedtheregularizeramplitudeforW and
1
W andmeasuredtheretentionperformanceunderthetwoapproximationmethods. Eveninthis
2
setting,thelayer-wiseapproximationoftheFisher-informationmetricrobustlyoutperformedthe
diagonalapproximation,whentheregulazieramplitudeforW wassufficientlylarge.
2
29