Randomized Exploration for Reinforcement Learning with
Multinomial Logistic Function Approximation
Wooseong Cho∗ wooseong cho@snu.ac.kr
Graduate School of Data Science
Seoul National University
Taehyun Hwang∗ th.hwang@snu.ac.kr
Graduate School of Data Science
Seoul National University
Joongkyu Lee jklee0717@snu.ac.kr
Graduate School of Data Science
Seoul National University
Min-hwan Oh† minoh@snu.ac.kr
Graduate School of Data Science
Seoul National University
Abstract
We study reinforcement learning with multinomial logistic (MNL) function approximation
wheretheunderlyingtransitionprobabilitykerneloftheMarkovdecisionprocesses (MDPs)
is parametrized by an unknown transition core with features of state and action. For the
finite horizon episodic setting with inhomogeneous state transitions, we propose provably
efficient algorithms with randomized exploration having frequentist regret guarantees. For
our first algorithm, RRL-MNL, we adapt optimistic sampling to ensure the optimism of
the estimated value function with sufficient frequency and establish that RRL-MNL is both
√
statistically and computationally efficient, achieving a O(cid:101)(κ−1d3 2H23 T) frequentist regret
bound with constant-time computational cost per episode. Here, d is the dimension of the
transitioncore,H isthehorizonlength,T isthetotalnumberofsteps,andκisaproblem-
dependent constant. Despite the simplicity and practicality of RRL-MNL, its regret bound
scales with κ−1, which is potentially large in the worst case. To improve the dependence
on κ−1, we propose ORRL-MNL, which estimates the value function using local gradient
information of the MNL transition model. We show that its frequentist regret bound is
√
O(cid:101)(d3 2H23 T+κ−1d2H2). Tothebestofourknowledge, thesearethefirstrandomizedRL
algorithms for the MNL transition model that achieve both computational and statistical
efficiency. Numerical experiments demonstrate the superior performance of the proposed
algorithms.
1. Introduction
Reinforcement learning (RL) is a sequential decision-making problem in which an agent
tries to maximize its expected cumulative reward by interacting with an unknown envi-
ronment over time. Despite significant empirical progress in RL algorithms for various
applications (Kober et al., 2013; Mnih et al., 2015; Silver et al., 2017, 2018; Fawzi et al.,
∗. Equal contribution
†. Corresponding author
1
4202
yaM
03
]LM.tats[
1v56102.5042:viXra2022), the theoretical understanding of RL algorithms had long been limited to tabular
methods (Jaksch et al., 2010; Osband and Roy, 2014; Azar et al., 2017; Zhang et al., 2020,
2021b), which explicitly enumerate the entire state and action spaces and learn the value
(or the policy) for each state and action. Recently, there has been an increasing body of
research in RL with function approximation to extend beyond the tabular problem setting.
In particular, linear function approximation has served as a foundational model (Jin et al.,
2020; Zanette et al., 2020; Du et al., 2020; Ayoub et al., 2020; Ishfaq et al., 2021). On
the other hand, the linear transition model assumption poses significant constraints: 1) the
output of the function must be within [0,1], and 2) the sum of the probabilities for all
possible next states must be exactly 1. These constraints make it challenging to apply RL
with linear function approximation to real-world applications (Hwang and Oh, 2023). To
overcome such challenges, there has been literature on RL with general function approxi-
mation (Du et al., 2021; Foster et al., 2021; Ishfaq et al., 2021; Jin et al., 2021; Agarwal
and Zhang, 2022a; Chen et al., 2023). Despite the guarantee of sample efficiency achieved
bytheiralgorithms, thisaccomplishmentmightbeimpededbycomputationalintractability
or the necessity to rely on stronger assumptions. As a result, the resulting methods may
not be as general or practical.
On the other hand, Hwang and Oh (2023) introduce specific non-linear parametric
MDPs called MNL-MDPs (Assumption 1) where the transition probability of MDPs is
given by an MNL model. They consider an upper confidence bound (UCB) approach to
balance exploration and exploitation. Since it is costly or even intractable to compute
UCB explicitly, randomized exploration methods such as Thompson Sampling (TS) are
widely studied in RL with linear function approximation as well as tabular MDPs. This
is because, in various decision-making problems ranging from multi-armed bandits to RL,
randomized exploration algorithms have been shown to perform better than UCB methods
in empirical evaluations (Chapelle and Li, 2011; Osband and Van Roy, 2017; Russo et al.,
2018; Kveton et al., 2020). Furthermore, randomized exploration can be easily integrated
with linear function approximation. This is because the value function in linear MDPs
can be linearly parameterized, allowing perturbations of the estimator to directly control
the perturbations of the value function. However, although there has been some literature
aiming to propose randomized algorithms for general function classes (Ishfaq et al., 2021;
Agarwal and Zhang, 2022a,b; Zhang, 2022), these methods do not discuss how to define the
posterior distribution supported by the given function class and how to draw the optimistic
sample from the posterior (Agarwal and Zhang, 2022a,b; Zhang, 2022), or they require
stronger assumptions on stochastic optimism (Ishfaq et al., 2021), which is one of the
most challenging elements in frequentist regret analysis. Thus, the design of a tractable
randomized exploration RL algorithm and the feasibility of frequentist regret analysis for
randomized exploration remain open challenges. Hence, the following question arises:
Can we design a provably efficient and tractable randomized algorithm for RL with MNL
function approximation?
We answer the above question by proposing the first randomized algorithm, RRL-MNL,
√
achieving
O(cid:101)(κ−1d3 2H3
2 T) frequentist regret with constant-time computational cost per
episode. RRL-MNL is not only the first algorithm with randomized exploration for MNL-
MDPs, but also, to the best of our knowledge, it provides the first frequentist regret analy-
2sis for a non-linear model-based algorithm with randomized exploration without assuming
stochastic optimism (Ishfaq et al., 2021).
While RRL-MNL is both computationally and statistically efficient, the current method
used to analyze the regret of MNL function approximation introduces a problem-dependent
constant κ (Assumption 4), which reflects the level of non-linearity of the MNL transition
model. This constant κ originates from the use of generalized linear models (GLMs) for
contextual bandit settings (Filippi et al., 2010; Li et al., 2017; Jun et al., 2017) and MNL
bandit settings (Oh and Iyengar, 2019; Chen et al., 2020; Oh and Iyengar, 2021). The
magnitude of the constant κ can be exponentially small with respect to the size of the
decision set, hence the regret bound scaling with κ−1 could be prohibitively large in the
worst case (Faury et al., 2020). However, the situation is quite different in RL, as in
the worst case, κ−1 can be much larger than in the case of bandits. To overcome the
prohibitive dependence on κ, algorithms based on new Bernstein-like inequalities and the
self-concordant-like property of the log-loss have been proposed for logistic bandits (Faury
et al., 2020; Abeille et al., 2021; Faury et al., 2022) and for MNL bandits (Perivier and
Goyal, 2022; Agrawal et al., 2023; Lee and Oh, 2024). As an extension of these works, the
following fundamental question remains open:
Is it possible for RL algorithms with MNL function approximation to have a sharper
dependence on the problem-dependent constant κ?
For the above question, we propose the second randomized algorithm referred to as
√
ORRL-MNL, which establishes a regret bound of O(cid:101)(d23 H3 2 T +κ−1d2H2) with constant-time
computational cost per episode. We summarize our main contributions as follows:
• WeproposecomputationallytractablerandomizedalgorithmsforRLwithMNLfunc-
tion approximation: RRL-MNL and ORRL-MNL. To the best of our knowledge, these are
the first randomized model-based RL algorithms with MNL function approximation
that achieve both computational and statistical efficiency.
√
• We establish that RRL-MNL enjoys O(cid:101)(κ−1d23 H3 2 T) frequentist regret bound with
constant-time computational cost per episode, where d is the dimension of the tran-
sition core, H is horizon length, T is the total number of rounds, and κ is a problem-
dependentconstant. WederivethestochasticoptimismofRRL-MNL,andtoourknowl-
edge,thisisthefirstfrequentistregretanalysisforanon-linearmodel-basedalgorithm
with randomized exploration without assuming stochastic optimism.
• To achieve a regret bound with improved dependence on κ, we introduce ORRL-MNL,
which constructs the optimistic randomized value functions by taking into account
the effects of the local gradient information for the MNL transition model at each
√
reachable state. We prove that ORRL-MNL enjoys an O(cid:101)(d3 2H3 2 T +κ−1d2H2) regret
with constant-time computational cost per episode, significantly improving the regret
of RRL-MNL without requiring prior knowledge of κ.
• We evaluate our algorithms on tabular MDPs and demonstrate the superior perfor-
mance of our proposed algorithms compared to the existing state-of-the-art MNL-
MDP algorithm (Hwang and Oh, 2023). The experiments provide evidence that our
proposed algorithms are both computationally and statistically efficient.
31.1 Related Work
Table 1: This table compares the problem settings, online update, performance of the
this paper with those of other methods in provable RL with function approximation. For
computation cost, we only keep the dependence on the number of episode K.
Algorithm Model-based Transitionmodel Reward Computationcost Regret
√
LSVI-UCB(Jinetal.,2020) ✗ Linear Linear O(K) O(cid:101)(d23H3 2√T)
OPT-RLSVI(Zanetteetal.,2020) ✗ Linear Linear O(K) O(cid:101)(d2H2 T)
√
LSVI-PHE(Ishfaqetal.,2021) ✗ Linear Linear O(K) O(cid:101)(d23H3 2√T)
UC-MatrixRL(YangandWang,2020) ✓ Linear Known O(K) O(cid:101)(d23H2 √T)
UCRL-VTR(Ayoubetal.,2020) ✓ Linearmixture Known O(K) O(cid:101)(dH3 2 √T)
UCRL-MNL(HwangandOh,2023) ✓ MNL Known O(K) O(cid:101)(κ−1dH3 2 √T)
RRL-MNL(thiswork) ✓ MNL Known O(1) O(cid:101)(κ−1d23H3 2 T)
ORRL-MNL(thiswork) ✓ MNL Known O(1) O(cid:101)(cid:16) d23H3 2√ T+κ−1d2H2(cid:17)
UCRL-MNL+(thiswork) ✓ MNL Known O(1) O(cid:101)(cid:16) dH3 2√ T+κ−1d2H2(cid:17)
RL with linear function approximation There has been a growing interest in
studies that extend beyond tabular MDPs and focus on function approximation methods
with provable guarantees (Jiang et al., 2017; Yang and Wang, 2019; Jin et al., 2020; Zanette
et al., 2020; Modi et al., 2020; Du et al., 2020; Cai et al., 2020; Ayoub et al., 2020; Wang
etal.,2020;Weiszetal.,2021;Heetal.,2021;Zhouetal.,2021a,b;Ishfaqetal.,2021;Hwang
andOh,2023). Inparticular,forminimizingregretinlinearMDPs,Jinetal.(2020)propose
an optimistic variant of the Least-Squares Value Iteration (LSVI) algorithm (Bradtke and
Barto, 1996; Osband et al., 2016) under the assumption that the transition model and
reward function of the MDPs are linear function of a d-dimensional feature mapping and
√
3 3
they guarantee O(cid:101)(d2H2 T) regret. Zanette et al. (2020) propose a randomized LSVI
algorithm that incorporates exploration by perturbing the least-square approximation of
√
the action-value function, and this algorithm guarantees O(cid:101)(d2H2 T) regret. Also, there
have been studies on model-based methods with function approximation in linear MDPs,
such as Yang and Wang (2020), which assume that the transition probability kernel is a
bilinearmodelparametrizedbyamatrixandproposeaUCB-basedalgorithmwithanupper
√
bound of O(cid:101)(d3 2H2 T) for reg √ret. He et al. (2023) propose an algorithm achieving nearly
minimaxoptimalregretO(cid:101)(dH T). Jiaetal.(2020)consideraspecifictypeofMDPscalled
linear mixture MDPs in which the transition probability kernel is a linear combination of
different basis kernels. This model encompasses various types of MDPs studied previously
in Modi et al. (2020); Yang and Wang (2020). For this model, Jia et al. (2020) propose a
UCB-based RL algorithm with value-targeted model parameter estimation that guarantees
√
3
an upper bound of O(cid:101)(dH2 T) for regret. The same linear mixture MDPs have been used
in other studies such as Ayoub et al. (2020); Zhou et al. (2021a,b). Specifically, in Zhou
et al. (2021a), a variant of the method proposed by Jia et al. (2020) is suggested and proved
√
that the algorit √hm guarantees an upper bound of O(cid:101)(dH T) regret with a matching lower
bound of Ω(dH T) for linear mixture MDPs. More recently, there are also works achieving
horizon-free regret bounds for linear mixture MDPs (Zhang et al., 2021a; Kim et al., 2022;
Zhou and Gu, 2022).
RL with non-linear function approximation Studies have been conducted on
extending function approximation beyond linear models. Ayoub et al. (2020); Wang et al.
4(2020);Ishfaqetal.(2021)provideupperboundforregretbasedoneluderdimension(Russo
and Van Roy, 2013). Also, there has been an effort to develop sample-efficient methods
with more “general” function approximation (Krishnamurthy et al., 2016; Jiang et al.,
2017; Dann et al., 2018; Du et al., 2019, 2021; Foster et al., 2021; Ishfaq et al., 2021; Jin
et al., 2021; Agarwal and Zhang, 2022a,b; Zhang, 2022; Chen et al., 2023) However, these
attempts may have been hindered by the difficulty of solving computationally intractable
problems (Krishnamurthy et al., 2016; Jiang et al., 2017; Dann et al., 2018; Du et al.,
2021; Foster et al., 2021; Jin et al., 2021; Chen et al., 2023), the necessity of relying on
stronger assumptions (Du et al., 2019; Ishfaq et al., 2021), or the lack of discussion on how
to define the posterior distribution supported by a given function class and how to draw the
optimistic sample from the posterior (Agarwal and Zhang, 2022a,b; Zhang, 2022). That is
why even after there exists a so-called “general function class”-based result, it is often the
casethattheresultsinspecificparametricmodelsarestillneeded. Despitethelargenumber
of studies on RL with linear function approximation, there is limited research on extending
beyond linear models to other parametric models. Wang et al. (2021) use generalized linear
function approximation, where the Bellman backup of any value function is assumed to
be a generalized linear function of feature mapping. Hwang and Oh (2023) discuss the
limitations of linear function approximation and propose a UCB-based algorithm for MNL
√
3
transition model in feature space achieving O(cid:101)(dH2 T).
Contextual bandits Faury et al. (2020) first provide a UCB-based algorithm with
κ-independent regret for binary logistic bandit and Abeille et al. (2021) present UCB & TS
basedalgorithmsachievingnearlyminimaxoptimalregretforthesamesetting. Fauryetal.
(2022) propose a jointly efficient UCB-based algorithm that achieve κ-independent regret
bound with O(logt) computation cost. In the context of MNL model, Oh and Iyengar
(2019) employ TS approach, while Oh and Iyengar (2021) incorporate a combination of
UCB exploration and online parameter updates for MNL bandits. Both of the methods
√
have O(κ−1 T) regret. Amani and Thrampoulidis (2021) propose an optimistic algorithm
with better dependence on κ. Agrawal et al. (2023) design a UCB-based algorithm with
√
O( T) regret bound without κ in its leading term, and Perivier and Goyal (2022) establish
(cid:112)
O( T/κ ) regret for the uniform reward setting. Zhang and Sugiyama (2023) develop
∗
jointly efficient UCB-based algorithm for non-uniform MNL bandit problem. Lee and
Oh (2024) propose nearly minimax optimal MNL bandit algorithm for both uniform and
non-uniform reward structures.
2. Problem Setting
WeconsidertheepisodicMarkovdecisionprocesses (MDPs)denotedbyM(S,A,H,{P}H ,r),
h=1
where S is the state space, A is the action space, H is the horizon length of each episode,
{P}H is the collection of probability distributions, and r is the reward function. Every
h=1
episodesstartfromtheinitialstates andforeverysteph ∈ [H] := {1,...,H}inanepisode,
1
the learning agent interacts with the environment represented as M. The agent observes
the state s ∈ S, chooses an action a ∈ A, receives a reward r(s ,a ) ∈ [0,1] and the next
h h h h
state s is given by the transition probability distribution P (·|s ,a ). Then this process
h+1 h h h
is repeated throughout the episode. A policy π : S×[H] → A is a function that determines
the action of the agent at state s , i.e., a = π(s ,h) := π (s ).
h h h h h
5We define the value function of the policy π, denoted by Vπ(s), as the expected sum
h
of rewards under the policy π until the end of the episode starting from s = s, i.e.,
h
(cid:34) H (cid:35)
(cid:88)
Vπ(s) = E r(s ,π (s )) | s = s . Similarly, we define the action-value function
h π h′ h′ h′ h
h′=h
Qπ(s,a) = r(s,a)+E (cid:2) Vπ (s′)(cid:3) . Wedefineanoptimalpolicyπ∗tobeapolicythat
h s′∼P h(·|s,a) h+1
achieves the highest possible value at every (s,h) ∈ S ×[H]. We denote the optimal value
function by V∗(s) = Vπ∗(s) and the optimal action-value function by Q∗(s,a) = Qπ∗(s,a).
h h h h
To simplify, we introduce the notation P V (s,a) = E [V (s′)]. Recall that
h h+1 s′∼P (·|s,a) h+1
h
the Bellman equations are,
Qπ(s,a) = r(s,a)+P Vπ (s,a), Q∗(s,a) = r(s,a)+P V∗ (s,a),
h h h+1 h h h+1
where Vπ (s) = V∗ (s) = 0 and V∗(s) = max Q∗(s,a) for all s ∈ S.
H+1 H+1 h a∈A h
The goal of the agent is to maximize the sum of rewards for K episodes. In other
words, the goal is to minimize the cumulative regret of the policy π over K episodes where
π = {πk}K is a collection of policies πk at k-th episode. The regret is defined as
k=1
K
Regret (K) := (cid:88) (V∗−Vπk )(sk)
π 1 1 1
k=1
where sk is the initial state at the k-th episode.
1
2.1 Multinomial Logistic Markov Decision Processes (MNL-MDPs)
EventhoughalotofprovableRLalgorithmsforlinearMDPsareproposed,thereisasimple
but fundamental problem with the linear transition model assumption on the linear MDPs.
In other words, the output of a linear function approximating the transition model must
be in [0,1] and the probability of all possible following states must sum to 1 exactly. Such
restrictive assumption can affect the regret performances of algorithm suggested under the
linearityassumption. Toresolvethesechallenges,HwangandOh(2023)proposeasettingof
a multinomial logistic Markov decision processes (MNL-MDPs), where the state transition
model is given by a multinomial logistic model. We introduce the formal definition for
MNL-MDP as follows:
Assumption 1 (MNL-MDPs (Hwang and Oh, 2023)). An MDP M(S,A,H,{P }H ,r)
h h=1
is an MNL-MDP with a feature map φ : S ×A×S → Rd, if for any h ∈ [H], there exists
θ∗ ∈ Rd, such that for any (s,a) ∈ S ×A and s′ ∈ S := {s′ ∈ S : P(s′ | s,a) ̸= 0}, the
h s,a
state transition kernel of s′ when an action a is taken at a state s is given by,
exp(φ(s,a,s′)⊤θ∗)
P (s′ | s,a) = h . (1)
h (cid:80) exp(φ(s,a,s)⊤θ∗)
s (cid:101)∈Ss,a (cid:101) h
We call each unknown vector θ∗ transition core. Furthermore, we denote the maximum
h
cardinality of the set of reachable states as U, i.e., U := max |S |.
s,a s,a
Remark 1. While Hwang and Oh (2023) assume a homogeneous transition kernel, we
assume an inhomogeneous transition kernel, in which the probability varies depending on
6the current time step h even for the same state transition, which is a more general setting.
Also, for notational simplicity, we denote the true transition kernel P
h
as P θ∗, and the
h
estimated transition kernel by θ as P .
θ
2.2 Assumptions
We introduce some standard regularity assumptions.
Assumption 2 (Boundedness). We assume ∥φ(s,a,s′)∥ ≤ L for all (s,a,s′) ∈ S×A×
2 φ
S , and ∥θ∗∥ ≤ L for all h ∈ [H].
s,a h 2 θ
Assumption 3 (Known reward). We assume that the reward function r is known to the
agent.
Assumption 4 (Problem-dependent constant). Let B (L ) := {θ ∈ Rd : ∥θ∥ ≤ L }.
d θ 2 θ
There exists κ > 0 such that for any (s,a) ∈ S × A and s′,s ∈ S , inf P (s′ |
(cid:101) s,a θ∈B (L ) θ
d θ
s,a)P (s | s,a) ≥ κ.
θ (cid:101)
Discussion of assumptions. Assumption 2 is common in the literature on RL with
function approximation (Jin et al., 2020; Yang and Wang, 2020; Zanette et al., 2020; Ishfaq
et al., 2021; Hwang and Oh, 2023) to make the regret bounds scale-free. Assumption 3
is used to focus on the main challenge of model-based RL that learning about P of the
environment is more difficult than learning r. In the model-based RL literature (Yang and
Wang, 2019; Ayoub et al., 2020; Yang and Wang, 2020; Zhou et al., 2021a; Hwang and
Oh, 2023), the known reward r assumption is widely used. Assumption 4 is typical in
generalized linear contextual bandit (Filippi et al., 2010; Li et al., 2017; Faury et al., 2020;
Abeille et al., 2021; Faury et al., 2022) and MNL contextual bandit literature (Oh and
Iyengar, 2019; Amani and Thrampoulidis, 2021; Oh and Iyengar, 2021; Perivier and Goyal,
2022; Agrawal et al., 2023; Zhang and Sugiyama, 2023; Lee and Oh, 2024) to guarantee
non-singular Fisher information matrix.
3. Computationally Efficient Randomized Algorithm for MNL-MDPs
Previous work for MNL-MDPs (Hwang and Oh, 2023) proposed a UCB-based exploration
algorithm. ConstructingaUCB-basedoptimisticvaluefunctionisnotonlycomputationally
intractable but also tends to overly optimistically estimate the true optimal value function.
Additionally, their algorithm incurs increasing computation costs as episodes progress, as it
requiresallsamplesfromthepreviousepisodetoestimatethetransitioncore. Inthissection,
wepresentanovelmodel-basedRLalgorithmthatincorporatesrandomized exploration and
online parameter estimation for MNL-MDPs.
3.1 Algorithm: RRL-MNL
Online transition core estimation. WhileHwangandOh(2023)estimatethetran-
sition core using maximum likelihood estimation over all samples from previous episodes,
we employ an efficient online parameter estimation method by exploiting the particular
structure of the MNL transition model. The key insight is that the negative log-likelihood
function for the MNL model in each episode k is strongly convex over a bounded domain.
7Algorithm 1 RRL-MNL (Randomized RL for MNL-MDPs)
1: Inputs: Episodic MDP M, Feature map φ : S ×A×S → Rd, Number of episodes K,
Regularization parameter λ, Exploration variance {σ }K , Sample size M, Problem-
k k=1
dependent constant κ
2: Initialize: θ1 h = 0 d, A 1,h = λI d for h ∈ [H]
3: for episode k = 1,2,··· ,K do
4: Observe sk
1
and sample i.i.d. noise vector ξ( km ,h) ∼ N(0 d,σ k2A− k,1 h) for m ∈ [M] and
h ∈ [H]
5: Set
(cid:8) Qk(·,·)(cid:9)
as described in (4)
h h∈[H]
6: for horizon h = 1,2,··· ,H do
7: Select ak = argmax Qk(sk,a) and observe sk
h a∈A h h h+1
8: Update A k+1,h = A k,h+ κ 2 (cid:80) s′∈S k,hφ(sk h,ak h,s′)φ(sk h,ak h,s′)⊤ and θk h+1 as in (2)
9: end for
10: end for
This property allows us to utilize a variation of the online Newton step (Hazan et al., 2007,
2014), which inspired online algorithms for logistic bandits (Zhang et al., 2016) and MNL
contextualbandits(OhandIyengar,2021). Specifically, for(k,h) ∈ [K]×[H], wedefinethe
responsevariableyk = (cid:2) yk(s′)(cid:3) suchthatyk(s′) = 1I(sk = s′)fors′ ∈ S := S .
h h s′∈S h h+1 k,h sk,ak
k,h h h
Then, yk is sampled from the following multinomial distribution:
h
(cid:16) (cid:104) (cid:105)(cid:17)
y hk ∼ multinomial 1, P θ∗ h(s i1 | sk h,ak h),··· ,P θ∗ h(s i
|Sk,h|
| sk h,ak h) ,
where 1 represents that yk is a single-trial sample. We define the per-episode loss ℓ (θ)
h k,h
as follows:
(cid:88)
ℓ (θ) := − yk(s′)logP (s′ | sk,ak).
k,h h θ h h
s′∈S
k,h
Then, the estimated transition core for θ∗ is given by
h
1
θk = argmin ∥θ−θk−1∥2 +(θ−θk−1)⊤∇ℓ (θk−1), (2)
h 2 h A k,h h k−1,h h
θ∈B (L )
d θ
where θ1 can be initialized as any point in B (L ) and A is the Gram matrix defined by
h d θ k,h
k−1
κ (cid:88) (cid:88)
A := λI + φ(si,ai,s′)φ(si,ai,s′)⊤. (3)
k,h d 2 h h h h
i=1s′∈S
i,h
Stochastically optimistic value function. Ensuring that the estimated value
function is optimistic with sufficient frequency is a crucial challenge in analyzing the fre-
quentist regret of randomized algorithms. A common way to promote sufficient exploration
in randomized algorithms is by perturbing the estimated value function or by performing
posterior sampling in the transition model class. Frequentist regret analysis of random-
ized exploration in an RL setting has been conducted for tabular (Osband et al., 2016;
8Agrawal and Jia, 2017; Russo, 2019; Pacchiano et al., 2021; Tiapkin et al., 2022), linear
MDPs (Zanette et al., 2020; Ishfaq et al., 2021), and general function classes (Ishfaq et al.,
2021; Agarwal and Zhang, 2022a,b; Zhang, 2022). In the case of linear MDPs (Zanette
et al., 2020; Ishfaq et al., 2021), since the property that the action-value function is linear
in the feature map allows perturbing the estimated parameter directly to control the per-
turbation of the estimated value function. Also, even though Ishfaq et al. (2021) presented
a randomized algorithm for the general function class using eluder dimension, they assume
stochastic optimism (anti-concentration), which is in fact one of the most challenging as-
pects of frequentist analysis. Other posterior sampling algorithms in RL for the general
function class such as (Agarwal and Zhang, 2022a,b; Zhang, 2022), except for very limited
examples, do not discuss how to define the posterior distribution supported by the given
function class and how to draw the optimistic sample from the posterior. That is why even
after there exists a so-called general function class-based result, it is often the case that
results in specific parametric models are still needed.
Note that in episodic RL, the perturbed estimated value functions are propagated back
through horizontal steps, requiring careful adjustment of the perturbation scheme to main-
tain a sufficient probability of optimism without decaying too quickly with the horizon. For
example, if the probability of the estimated value function being optimistic at horizon h is
denoted as p, this would result in the probability that the estimated value function in the
initial state is optimistic being on the order of pH, implying that the regret can increase
exponentially with the length of the horizon H. Additionally, the non-linearity and sub-
stitution effect of the next state transition in the MNL-MDPs make applying the existing
TS techniques infeasible to guarantee optimism in MNL-MDPs with sufficient frequency.
Instead, we design the stochastically optimistic value function by exploiting the structure
of the MNL transition model. In other words, the prediction error of MNL transition
model (Definition 1) can be bounded by the weighted norm of the dominating feature φˆ
(Lemma 4). Based on such dominating feature, we perturb the estimated value function by
injectingGaussiannoisewhosevarianceisproportionaltotheinverseoftheGrammatrixto
encourage the perturbation with higher variance in less explored directions. To guarantee
the optimism with fixed probability, we adapt optimistic sampling technique (Agrawal and
Jia, 2017; Oh and Iyengar, 2019; Ishfaq et al., 2021; Hwang et al., 2023). For each m ∈ [M],
sample i.i.d. Gaussian noise vector ξ(m) ∼ N(0 ,σ2A−1) where σ is an exploration pa-
k,h d k k,h k
rameter, and add the most optimistic inner product value max φˆ
(s,a)⊤ξ(m)
to the
m∈[M] k,h k,h
estimated value function. To summarize for any (s,a) ∈ S ×A, set Qk (s,a) = 0 and for
H+1
h ∈ [H],
(cid:26) (cid:27)
Qk(s,a) = min r(s,a)+ (cid:88) P (s′ | s,a)Vk (s′)+ max φˆ (s,a)⊤ξ(m) ,H , (4)
h θk h+1 k,h k,h
h m∈[M]
s′∈Ss,a
whereVk(s) = max Qk(s,a′)andφˆ (s,a) := φ(s,a,sˆ)forsˆ= argmax ∥φ(s,a,s′)∥ .
h a′ h k,h s′∈Ss,a A− k,1
h
Based on these stochastically optimistic value function, the agent plays a greedy action
ak = argmax Qk(sk,a′). We layout the procedure in Algorithm 1.
h a′ h h
Remark 2. Note that RRL-MNL only requires constant-time computational cost and storage
cost per episode, as it does not require storing all samples from previous episodes, and the
Gram matrix A can be updated incrementally.
k,h
93.2 Regret bound of RRL-MNL
We present the regret upper bound of RRL-MNL. The complete proof is deferred to Ap-
pendix B.
Theorem 1 (Regret Bound of RRL-MNL). Suppose that Assumption 1- 4 hold. For any
√
0 < δ < Φ(− 21) , if we set the input parameters in Algorithm 1 as λ = L2 φ,σ
k
= O(cid:101)(H d)
and M = ⌈1− logH ⌉ where Φ is the normal CDF, then with probability at least 1−δ, the
logΦ(1)
cumulative regret of the RRL-MNL policy π is upper-bounded as follows:
(cid:16) √ (cid:17)
Regret π(K) = O(cid:101)
κ−1d3 2H3
2 T .
Discussion of Theorem 1. To our best knowledge, this is the first result to provide
a frequentist regret bound for the MNL-MDPs. Among the previous RL algorithms using
function approximation, the most comparable techniques to our method are model-free
algorithms with randomized exploration (Zanette et al., 2020; Ishfaq et al., 2021). To
guarantee stochastic optimism, Zanette et al. (2020) established a lower bound on the
difference between the estimated value and the optimal value by the summation of linear
terms with respect to the average feature (Lemma F.1 in (Zanette et al., 2020)). This
property is achievable due to the linear expression of the value function in linear MDPs.
Instead, we established a lower bound on the difference between value functions by the
summation of the Bellman errors (Definition 1) along the sample path obtained through
the optimal policy (Lemma 7). Hence, our analysis significantly differs from that of Zanette
et al. (2020) since the value function in MNL-MDPs is no longer linearly parametrized, and
there is no closed-form expression for it.
Compared to (Ishfaq et al., 2021), they also used an optimistic sampling technique;
however, our theoretical sampling size M = O(logH) is much tighter than that of (Ishfaq
et al., 2021), i.e., O(d) for the linear function class, O(log(T|S||A|)) for the general function
class. While Ishfaq et al. (2021) extend the results of the linear function class to general
function class under the assumption of stochastic optimism (Assumption C in (Ishfaq et al.,
2021)), we provide the frequentist regret analysis for a non-linear model-based algorithm
with randomized exploration without assuming stochastic optimism.
Compared to the optimistic exploration algorithm for MNL-MDPs (Hwang and Oh,
2023), our randomized exploration requires a more involved proof technique to ensure that
the perturbation of the estimated value function has enough variance to maintain opti-
mism with sufficient frequency (Lemma 6). As a result, the established regret of RRL-MNL
√
differs by a factor of d, which aligns with the difference in the existing bounds of lin-
ear bandits between a TS-based algorithm (Abeille and Lazaric, 2017) and a UCB-based
algorithm (Abbasi-Yadkori et al., 2011). Additionally, we achieve statistical efficiency for
the inhomogeneous transition model, which is a more general setting than that of Hwang
and Oh (2023). Our computation cost per episode is O(1) while the computation cost per
episode of Hwang and Oh (2023) is O(K).
4. Statistically Improved Algorithm for MNL-MDPs
Although RRL-MNL is both computationally and statistically efficient, the current analysis
makes its regret bound scale with κ−1. Recall that the problem-dependent constant κ in-
10Algorithm 2 ORRL-MNL (Optimistic Randomized RL for MNL-MDPs)
1: Inputs: Episodic MDP M, Feature map φ : S ×A×S → Rd, Number of episodes K,
Regularization parameter λ, Exploration variance {σ }K , Confidence radius {β }K ,
k k=1 k k=1
Sample size M, Step size η
2: Initialize: θ(cid:101)1 h = 0 d, B 1,h = λI d for all h ∈ [H]
3: for episode k = 1,2,··· ,K do
4: Observe sk
1
and sample i.i.d. noise vector ξ( km ,h) ∼ N(0 d,σ k2B− k,1 h) for m ∈ [M] and
h ∈ [H]
(cid:110) (cid:111)
5: Set Q(cid:101)k(·,·) as described in (7)
h
h∈[H]
6: for horizon h = 1,2,··· ,H do
7: Select ak = argmax Q(cid:101)k(sk,a) and observe sk
h a∈A h h h+1
8: Update B(cid:101)k,h = B k,h+η∇2ℓ k,h(θ(cid:101)k h) and θ(cid:101)k h+1 as in (5)
9: Update B k+1,h = B k,h+∇2ℓ k,h(θ(cid:101)k h+1)
10: end for
11: end for
troduced in Assumption 4 indicates the curvature of the MNL function, i.e., how difficult
it is to learn the true transition core parameter. It is required to ensure the non-singular
Fisher information matrix, hence is typically used in GLM or MNL bandit algorithms that
use the maximum likelihood estimator. As introduced in Faury et al. (2020), κ−1 can be
exponentially large in the worst case. The appearance of κ in existing bounds originates in
the connection between the difference of estimators and the difference of gradients of nega-
tive log-likelihood, usually denoted as G in Filippi et al. (2010). Without considering local
information at all, using a loose lower bound for G incurs κ−1 in regret bound (see Section
4.1 in Agrawal et al. (2023)). Recently, improved dependence on κ has been achieved in
bandit literature (Faury et al., 2020; Abeille et al., 2021; Perivier and Goyal, 2022; Agrawal
et al., 2023; Zhang and Sugiyama, 2023; Lee and Oh, 2024) through the use of generaliza-
tion of the Bernstein-like tail inequality (Faury et al., 2020) and the self-concordant-like
property of the log loss (Bach, 2010). However, a direct adaptation of the MNL bandit
technique would result in sub-optimal dependence on the assortment size in MNL bandit,
which corresponds to the size of the set of reachable states, such as U. In this section, we
introduce a new randomized algorithm for MNL-MDPs, equipped with a tight online pa-
rameter estimation and feature centralization technique that achieves a regret bound with
improved dependence on κ and U.
4.1 Algorithms: ORRL-MNL
Tight online transition core estimation. Zhang and Sugiyama (2023) presented a
jointly efficient UCB-based MNL contextual bandit algorithm using online mirror descent
algorithm. Adapting the update rule from (Zhang and Sugiyama, 2023), the estimated
transition core run by the online mirror descent is given by
1 (cid:13) (cid:13)2
θ(cid:101) hk+1 = θa ∈r Bg dm (Lin
θ) 2η
(cid:13) (cid:13)θ−θ(cid:101)k h(cid:13)
(cid:13) B(cid:101)k,h
+θ⊤∇ℓ k,h(θ(cid:101)k h), (5)
11where θ(cid:101)1
h
can be initialized as any point in B d(L θ), η is a step size, and B(cid:101)k,h is defined as
k−1
(cid:88)
B(cid:101)k,h := B k,h+η∇2ℓ k,h(θ(cid:101)k h), B
k,h
:= λI d+ ∇2ℓ i,h(θ(cid:101)i h+1). (6)
i=1
Note that the MNL model in Zhang and Sugiyama (2023) operates in a multiple-parameter
setting, where there are N unknown choice parameters and one given context feature. In
contrast, our MNL model operates in a single-parameter setting, where there is one un-
known transition core and features for up to U reachable states. This difference results in
variations in applying the self-concordant-like property of the log-loss for the MNL model.
For instance, Zhang and Sugiyama (2023) utilized the fact that the log-loss for the mul-
√
tiple parameter MNL model is 6-self-concordant-like (Lemma 2 in Zhang and Sugiyama
(2023)). Ontheotherhand,LeeandOh(2024)revisittheself-concordant-likepropertyand
√
demonstrate that the log-loss of the single-parameter MNL model is 3 2-self-concordant-
like (Proposition B.1 in Lee and Oh (2024)). This results in a concentration bound that is
independent of κ and U, introduced in Lemma 12.
Optimistic randomized value function. To achieve improved dependence on κ, a
crucial point is to utilize the local gradient information of MNL transition probabilities for
each reachable state when constructing the Gram matrix. In MNL bandit problems (Periv-
ier and Goyal, 2022; Zhang and Sugiyama, 2023), this can be accomplished by substituting
the Hessian of the negative log-likelihood with the Gram matrix using global gradient infor-
mation κ. However, there are fundamental differences between the settings in Perivier and
Goyal (2022); Zhang and Sugiyama (2023) and ours. Perivier and Goyal (2022) address
the case where the reward for each product is uniform (i.e., all products have a reward of
1), and the reward for not selecting a product from the given assortment (also known as
the outside option) is 0. On the other hand, Zhang and Sugiyama (2023) deal with non-
uniform rewards where the reward for each product may vary; however, the rewards for
individual products are known a priori to the agent. In contrast, in MNL-MDPs, the value
foreachreachablestatemayvary(non-uniform)andisnot known beforehand. Duetothese
differences, the analysis techniques in MNL bandits (Perivier and Goyal, 2022; Zhang and
Sugiyama, 2023) cannot be directly applied to our setting. Instead, we adapt the feature
centralizationtechnique(LeeandOh,2024). Then,theHessianoftheper-roundlossℓ (θ)
k,h
is expressed in terms of the centralized feature as follows:
(cid:88)
∇2ℓ (θ) = P (s′ | sk,ak)φ¯(sk,ak,s′;θ)φ¯(sk,ak,s′;θ)⊤.
k,h θ h h h h h h
s′∈S
k,h
where φ¯(s,a,s′;θ) := φ(s,a,s′)−E [φ(s,a,s)] is the centralized feature by θ. For
s∼P (·|s,a) (cid:101)
(cid:101) θ
more details, please refer to Appendix C.2.
Now we introduce the optimistic randomized value function Q(cid:101)k(·,·) for ORRL-MNL. The
h
key point is that when perturbing the estimated value function, we use the centralized
featurebytheestimatedtransitionparameterθ(cid:101)k. Forany(s,a) ∈ S×A,setQ(cid:101)k (s,a) = 0
h H+1
and for each h ∈ [H],
(cid:26) (cid:27)
(cid:88)
Q(cid:101)k h(s,a) := min r(s,a)+ P θ(cid:101)k(s′ | s,a)V(cid:101) hk +1(s′)+ν kra ,hnd(s,a),H , (7)
h
s′∈Ss,a
12where V(cid:101) hk(s) := max a∈AQ(cid:101)k h(s,a) and ν kra ,hnd(s,a) is the randomized bonus term defined by
ν kra ,hnd(s,a) := s′(cid:88) ∈Ss,aP
θ(cid:101)k
h(s′ | s,a)φ¯(s,a,s′;θ(cid:101)k h)⊤ξs k′ ,h+3Hβ k2 sm ′∈Sa sx ,a∥φ(s,a,s′)∥2
B− k,1
h.
Herewesamplei.i.d. Gaussiannoiseξ(m) ∼ N(0 ,σ2B−1)foreachm ∈ [M]andsetξs′ :=
k,h d k k,h k,h
ξm(s′) wherem(s′) := argmax φ¯(s,a,s′;θ(cid:101)k)⊤ξm isthemostoptimisticsamplingindex
k,h m∈[M] h k,h
for a reachable state s′. Based on these optimistic randomized value function, at each
episode the agent plays a greedy action with respect to Q(cid:101)k as summarized in Algorithm 2.
h
Remark 3. Note that the second term in the randomized bonus always has a positive value,
but it rapidly decreases as episode proceeds. While due to the randomness of ξ, the random-
ized bonus νrand itself cannot be guaranteed to always have a positive value. Consequently,
k,h
the constructed value function Q(cid:101)k(·,·) can be optimistic or pessimistic. However, as shown
h
in Lemma 18, optimistic sampling technique ensures that the optimistic randomized value
function Q(cid:101)k has at least a constant probability of being optimistic than the true optimal
h
value function.
Remark 4. As with RRL-MNL, since the transition core is estimated in an online manner
and the Gram matrices with local gradient information B
k,h
and B(cid:101)k,h are updated incremen-
tally, ORRL-MNL also requires constant-time computational cost and storage cost per-episode.
Although ORRL-MNL requires additional computation for feature centralization, the computa-
tion complexity order is the same as that of UCRL-MNL (Hwang and Oh, 2023) and RRL-MNL
because they also need to go over reachable states to calculate the dominating feature φˆ. On
the other hand, ORRL-MNL does not require prior knowledge of κ and achieves a regret with
a better dependence on κ.
4.2 Regret Bound of ORRL-MNL
We present the regret upper bound of ORRL-MNL. The complete proof is deferred to Ap-
pendix C.
Theorem 2 (Regret Bound of ORRL-MNL). Suppose that Assumption 1- 4 hold. For any
0 < δ < Φ(−1) , if we set the input parameters in Algorithm 2 as λ = O(L2dlogU),β =
√ 2 φ k
log(HU)
O( dlogUlog(kH)),σ = Hβ , M = ⌈1− ⌉, and η = O(logU), then with probability
k k logΦ(1)
at least 1−δ, the cumulative regret of the ORRL-MNL policy π is upper-bounded as follows:
(cid:16) √ (cid:17)
Regret (K) = O(cid:101) d3/2H3/2 T +κ−1d2H2 .
π
Discussion of Theorem 2. Theorem2establishesthattheleadingtermintheregret
bound does not suffer from the problem-dependent constant κ−1 and the second term of
the regret bound is independent of the size of set of reachable states. To the extent of
our knowledge, this is the first algorithm that provides a frequentist regret guarantee with
improveddependenceonκ−1 inMNL-MDPs. ComparedtoRRL-MNL,thetechnicalchallenge
lies in ensuring the stochastic optimism of the estimated value for ORRL-MNL. Note that the
predictionerror(Definition1)forORRL-MNLischaracterizedbytwocomponents: onerelated
13to the gradient information of the MNL transition model at each reachable state, and the
other related to the dominating feature with respect to the Gram matrix B (Lemma 16).
k,h
Hence, the probability of the Bellman error at each horizon, when following the optimal
policy, being negative can depend on the size of the reachable states. This implies that
the probability of stochastic optimism can be exponentially small, not only in the horizon
H but also in the size of the reachable states U. However, as shown in Lemma 18, this
challenge has been overcome by using a sample size M that logarithmically increases with
U, effectively addressing the issue.
Optimistic exploration extension. In general, since TS-based randomized explo-
ration requires a more rigorous proof technique than UCB-based algorithms, our technical
ingredients enable the use of optimistic exploration in a straightforward manner. We in-
troduce UCRL-MNL+ in the Appendix D, an optimism-based algorithm for MNL-MDPs. It
is both computationally and statistically efficient compared to UCRL-MNL (Hwang and Oh,
2023), achieving the tightest regret bound for MNL-MDPs.
√
Corollary 1. UCRL-MNL+ (Algorithm 3) has O(cid:101)(dH3/2 T+κ−1d2H2) regret with high prob-
ability.
5. Numerical Experiments
0.4 0.6 0.6
0.6 0.35 0.35 0.35
(1,r = 105 00) s 1 0.05 s 2 0.05 ... 0.05 s n−1 0.4 s n (0.6,r = 1)
1 1 1 1
Figure 1: The “RiverSwim” environment with n states (Osband et al., 2013)
We perform a numerical evaluation on a variant of RiverSwim (Osband et al., 2013) to
demonstrate practicality of our proposed algorithms. The RiverSwim environment (Fig-
ure 1) consists of n states that are arranged in a chain. The agent starts in the leftmost
state with a relatively small reward of 0.005 and aims to reach the rightmost state, which
has a relatively large reward of 1. Choosing to swim to the left moves the agent deter-
ministically to the left, while swimming to the right has a probability of transitioning the
agent toward the right state, but also a high chance of remaining in the current state
or even moving left due to the strong current of river. Therefore, efficient exploration is
crucial in order to learn the optimal policy for this environment. We compare our al-
gorithms (RRL-MNL,ORRL-MNL,UCRL-MNL+) with the state-of-the-art UCRL-MNL (Hwang and
Oh, 2023) for MNL-MDPs. We fine-tuned the hyperparameters for each algorithm within
specific ranges. For each configuration, we report the averaged results over 10 independent
runs. Figure 2a and 2b show the episodic return of each algorithm, which is the sum of all
the rewards obtained in one episode. First, our proposed algorithms (RRL-MNL, ORRL-MNL,
UCRL-MNL+) outperform UCRL-MNL (Hwang and Oh, 2023) for both cases of |S| = 4,8. Sec-
ond, ORRL-MNL and UCRL-MNL+ reach the optimal values quickly compared to the other
14RRL-MNL ORRL-MNL UCRL-MNL+ RRL-MNL ORRL-MNL UCRL-MNL+ RRL-MNL ORRL-MNL
UCRL-MNL Optimal Policy UCRL-MNL Optimal Policy UCRL-MNL UCRL-MNL+
3.0 3.0 4302.3
4000
2.5 2.5
3000
2.0 2.0 2000
1.5 1.5 1000
1.0 0 2000 4000 6000 8000 10000 1.0 0 2000 4000 6000 8000 10000 0 RR2 L0 -M.3 NL UCRL-MNL OR7 R6 L-. M2 NL UCR7 L0 -M.1 NL+
The Number of Episodes The Number of Episodes Algorithms
(a) S =4,H =12 (b) S =8,H =24 (c)Runtimefor1,000episodes
Figure 2: Riverswim experiment results
algorithms, demonstrating improved statistical efficiency. Figure 2c illustrates the compar-
ison in running time of the algorithms for the first 1,000 episodes. Our proposed algorithms
are at least 50 times faster than UCRL-MNL. These differences become more pronounced as
the episodes progress because our algorithms have a constant computation cost, whereas
the computation cost of UCRL-MNL increases over time.
6. Conclusions
We propose both computationally and statistically efficient randomized algorithms for RL
with MNL function approximation. For the first algorithm, RRL-MNL, we use an optimistic
sampling technique to ensure the stochastic optimism of the estimated value functions and
provide the frequentist regret analysis. To achieve a statistically improved regret bound, we
proposeORRL-MNLbyconstructingtheoptimisticrandomizedvaluefunctionusingtheeffects
of the local gradient of the MNL transition model equipped with the centralized feature.
As a result, we achieve a frequentist regret guarantee with improved dependence on κ in
RL with the MNL transition model, which is a significant contribution. The effectiveness
and practicality of our methods are supported by numerical experiments.
15
snruteR
cidosipE
snruteR
cidosipE
)sdnoces(
emiTReferences
Y. Abbasi-Yadkori, D. P´al, and C. Szepesv´ari. Improved algorithms for linear stochastic
bandits. Advances in neural information processing systems, 24:2312–2320, 2011.
M. Abeille and A. Lazaric. Linear Thompson Sampling Revisited. In A. Singh and J. Zhu,
editors, Proceedings of the 20th International Conference on Artificial Intelligence and
Statistics,volume54ofProceedingsofMachineLearningResearch,pages176–184.PMLR,
PMLR, 20–22 Apr 2017.
M. Abeille, L. Faury, and C. Calauz`enes. Instance-wise minimax-optimal algorithms for
logistic bandits. In International Conference on Artificial Intelligence and Statistics,
pages 3691–3699. PMLR, 2021.
A. Agarwal and T. Zhang. Model-based rl with optimistic posterior sampling: Structural
conditions and sample complexity. Advances in Neural Information Processing Systems,
35:35284–35297, 2022a.
A. Agarwal and T. Zhang. Non-linear reinforcement learning in large action spaces: Struc-
tural conditions and sample-efficiency of posterior sampling. In Conference on Learning
Theory, pages 2776–2814. PMLR, 2022b.
P. Agrawal, T. Tulabandhula, and V. Avadhanula. A tractable online learning algorithm
for the multinomial logit contextual bandit. European Journal of Operational Research,
2023.
S. Agrawal and R. Jia. Posterior sampling for reinforcement learning: worst-case regret
bounds. In Advances in Neural Information Processing Systems, pages 1184–1194, 2017.
S. Amani and C. Thrampoulidis. Ucb-based algorithms for multinomial logistic regression
bandits. Advances in Neural Information Processing Systems, 34:2913–2924, 2021.
A.Ayoub,Z.Jia,C.Szepesvari,M.Wang,andL.Yang. Model-basedreinforcementlearning
with value-targeted regression. In International Conference on Machine Learning, pages
463–474. PMLR, 2020.
M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning.
In International Conference on Machine Learning, pages 263–272. PMLR, 2017.
F. Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4
(2):384 – 414, 2010.
P. L. Bartlett, O. Bousquet, and S. Mendelson. Local rademacher complexities. The Annals
of Statistics, 2005.
S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal difference
learning. Machine learning, 22(1):33–57, 1996.
Q. Cai, Z. Yang, C. Jin, and Z. Wang. Provably efficient exploration in policy optimization.
In International Conference on Machine Learning, pages 1283–1294. PMLR, 2020.
16N. Campolongo and F. Orabona. Temporal variability in implicit online learning. Advances
in neural information processing systems, 33:12377–12387, 2020.
O. Chapelle and L. Li. An empirical evaluation of thompson sampling. Advances in neural
information processing systems, 24, 2011.
X. Chen, Y. Wang, and Y. Zhou. Dynamic assortment optimization with changing contex-
tual information. Journal of machine learning research, 2020.
Z. Chen, C. J. Li, H. Yuan, Q. Gu, and M. Jordan. A general framework for sample-
efficient function approximation in reinforcement learning. In The Eleventh International
Conference on Learning Representations, 2023.
C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On
oracle-efficient pac rl with rich observations. In Advances in Neural Information Process-
ing Systems, volume 31, 2018.
S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably
efficient rl with rich observations via latent state decoding. In International Conference
on Machine Learning, pages 1665–1674. PMLR, 2019.
S. Du, S. Kakade, J. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bilinear classes:
A structural framework for provable generalization in rl. In International Conference on
Machine Learning, pages 2826–2836. PMLR, 2021.
S. S. Du, S. M. Kakade, R. Wang, and L. F. Yang. Is a good representation sufficient for
sample efficient reinforcement learning? In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.
L. Faury, M. Abeille, C. Calauz`enes, and O. Fercoq. Improved optimistic algorithms for
logistic bandits. In International Conference on Machine Learning, pages 3052–3060.
PMLR, 2020.
L.Faury, M.Abeille, K.-S.Jun, andC.Calauz`enes. Jointlyefficientandoptimalalgorithms
for logistic bandits. In International Conference on Artificial Intelligence and Statistics,
pages 546–580. PMLR, 2022.
A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov,
F. J. R Ruiz, J. Schrittwieser, G. Swirszcz, et al. Discovering faster matrix multiplication
algorithms with reinforcement learning. Nature, 610(7930):47–53, 2022.
S. Filippi, O. Capp´e, A. Garivier, and C. Szepesv´ari. Parametric bandits: The generalized
linear case. In Proceedings of the 23rd International Conference on Neural Information
Processing Systems - Volume 1, NIPS’10, page 586–594, Red Hook, NY, USA, 2010.
Curran Associates Inc.
D. J. Foster, S. Kale, H. Luo, M. Mohri, and K. Sridharan. Logistic regression: The
importanceofbeingimproper.InConferenceOnLearningTheory,pages167–208.PMLR,
2018.
17D.J.Foster,S.M.Kakade,J.Qian,andA.Rakhlin. Thestatisticalcomplexityofinteractive
decision making. arXiv preprint arXiv:2112.13487, 2021.
D. A. Freedman. On tail probabilities for martingales. the Annals of Probability, pages
100–118, 1975.
E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex opti-
mization. Machine Learning, 69(2):169–192, 2007.
E. Hazan, T. Koren, and K. Y. Levy. Logistic regression: Tight bounds for stochastic and
online optimization. In Conference on Learning Theory, pages 197–209. PMLR, 2014.
E. Hazan et al. Introduction to online convex optimization. Foundations and Trends® in
Optimization, 2(3-4):157–325, 2016.
J. He, D. Zhou, and Q. Gu. Logarithmic regret for reinforcement learning with linear
function approximation. In International Conference on Machine Learning, pages 4171–
4180. PMLR, 2021.
J. He, H. Zhao, D. Zhou, and Q. Gu. Nearly minimax optimal reinforcement learning
for linear markov decision processes. In International Conference on Machine Learning,
pages 12790–12822. PMLR, 2023.
T. Hwang and M.-h. Oh. Model-based reinforcement learning with multinomial logistic
function approximation. In Proceedings of the AAAI conference on artificial intelligence,
pages 7971–7979, 2023.
T. Hwang, K. Chai, and M.-H. Oh. Combinatorial neural bandits. In Proceedings of the
40th International Conference on Machine Learning. PMLR, 2023.
H. Ishfaq, Q. Cui, V. Nguyen, A. Ayoub, Z. Yang, Z. Wang, D. Precup, and L. Yang. Ran-
domizedexplorationinreinforcementlearningwithgeneralvaluefunctionapproximation.
In International Conference on Machine Learning, volume 139, pages 4607–4616. PMLR,
PMLR, 2021.
T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research, 11(4), 2010.
Z. Jia, L. Yang, C. Szepesvari, and M. Wang. Model-based reinforcement learning with
value-targeted regression. In Learning for Dynamics and Control, pages 666–686. PMLR,
2020.
N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual
decision processes with low bellman rank are pac-learnable. In International Conference
on Machine Learning, pages 1704–1713. PMLR, 2017.
C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efficient reinforcement learning with
linear function approximation. In Conference on Learning Theory, pages 2137–2143.
PMLR, 2020.
18C.Jin,Q.Liu,andS.Miryoosefi. Bellmaneluderdimension: Newrichclassesofrlproblems,
and sample-efficient algorithms. Advances in neural information processing systems, 34:
13406–13418, 2021.
K.-S. Jun, A. Bhargava, R. Nowak, and R. Willett. Scalable generalized linear bandits:
Online computation and hashing. Advances in Neural Information Processing Systems,
30, 2017.
Y. Kim, I. Yang, and K.-S. Jun. Improved regret analysis for variance-adaptive linear
banditsandhorizon-freelinearmixturemdps. Advances in Neural Information Processing
Systems, 35:1060–1072, 2022.
J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238–1274, 2013.
A. Krishnamurthy, A. Agarwal, and J. Langford. Pac reinforcement learning with rich
observations. Advances in Neural Information Processing Systems, 29:1840–1848, 2016.
B.Kveton,C.Szepesv´ari,M.Ghavamzadeh,andC.Boutilier.Perturbed-historyexploration
in stochastic linear bandits. In Uncertainty in Artificial Intelligence, pages 530–540.
PMLR, 2020.
J. Lee and M.-h. Oh. Nearly minimax optimal regret for multinomial logistic bandit. arXiv
preprint arXiv:2405.09831, 2024.
L. Li, Y. Lu, and D. Zhou. Provably optimal algorithms for generalized linear contextual
bandits. In International Conference on Machine Learning, pages 2071–2080. PMLR,
2017.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep
reinforcement learning. nature, 518(7540):529–533, 2015.
A. Modi, N. Jiang, A. Tewari, and S. Singh. Sample complexity of reinforcement learn-
ing using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics, pages 2010–2020. PMLR, 2020.
M.-h. Oh and G. Iyengar. Thompson sampling for multinomial logit contextual bandits.
Advances in Neural Information Processing Systems, 32:3151–3161, 2019.
M.-h. Oh and G. Iyengar. Multinomial logit contextual bandits: Provable optimality and
practicality. In Proceedings of the AAAI Conference on Artificial Intelligence, pages
9205–9213, 2021.
I. Osband and B. V. Roy. Model-based reinforcement learning and the eluder dimension.
In Advances in Neural Information Processing Systems, pages 1466–1474, 2014.
I. Osband and B. Van Roy. Why is posterior sampling better than optimism for rein-
forcement learning? In International conference on machine learning, pages 2701–2710.
PMLR, 2017.
19I. Osband, D. Russo, and B. Van Roy. (more) efficient reinforcement learning via posterior
sampling. Advances in Neural Information Processing Systems, 26, 2013.
I. Osband, B. Van Roy, and Z. Wen. Generalization and exploration via randomized value
functions. In International Conference on Machine Learning, pages 2377–2386. PMLR,
2016.
A.Pacchiano,P.Ball,J.Parker-Holder,K.Choromanski,andS.Roberts. Towardstractable
optimisminmodel-basedreinforcementlearning. InUncertainty in Artificial Intelligence,
pages 1413–1423. PMLR, 2021.
N.PerivierandV.Goyal. Dynamicpricingandassortmentunderacontextualmnldemand.
Advances in Neural Information Processing Systems, 35:3461–3474, 2022.
D. Russo. Worst-case regret bounds for exploration via randomized value functions. Ad-
vances in Neural Information Processing Systems, 32, 2019.
D. Russo and B. Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. In Advances in Neural Information Processing Systems, pages 2256–2264,
2013.
D. J. Russo, B. Van Roy, A. Kazerouni, I. Osband, Z. Wen, et al. A tutorial on thompson
sampling. Foundations and Trends® in Machine Learning, 11(1):1–96, 2018.
D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert,
L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge.
nature, 550(7676):354–359, 2017.
D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,
D. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm that masters
chess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.
D. Tiapkin, D. Belomestny, D. Calandriello, E. Moulines, R. Munos, A. Naumov, M. Row-
land, M. Valko, and P. M´enard. Optimistic posterior sampling for reinforcement learning
with few samples and tight guarantees. Advances in Neural Information Processing Sys-
tems, 35:10737–10751, 2022.
R. Wang, R. R. Salakhutdinov, and L. Yang. Reinforcement learning with general value
function approximation: Provably efficient approach via bounded eluder dimension. Ad-
vances in Neural Information Processing Systems, 33, 2020.
Y. Wang, R. Wang, S. S. Du, and A. Krishnamurthy. Optimism in reinforcement learn-
ing with generalized linear function approximation. In 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.
G. Weisz, P. Amortila, and C. Szepesv´ari. Exponential lower bounds for planning in mdps
with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory,
pages 1237–1264. PMLR, 2021.
20L. Yang and M. Wang. Sample-optimal parametric q-learning using linearly additive fea-
tures. In International Conference on Machine Learning, pages 6995–7004. PMLR, 2019.
L. Yang and M. Wang. Reinforcement learning in feature space: Matrix bandit, kernels,
andregretbound. InInternational Conference on Machine Learning, pages10746–10756.
PMLR, 2020.
A. Zanette, D. Brandfonbrener, E. Brunskill, M. Pirotta, and A. Lazaric. Frequentist
regret bounds for randomized least-squares value iteration. In International Conference
on Artificial Intelligence and Statistics, pages 1954–1964. PMLR, 2020.
L. Zhang, T. Yang, R. Jin, Y. Xiao, and Z.-H. Zhou. Online stochastic linear optimization
underone-bitfeedback. InInternationalConferenceonMachineLearning,pages392–401.
PMLR, 2016.
T. Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning.
SIAM Journal on Mathematics of Data Science, 4(2):834–857, 2022.
Y.-J. Zhang and M. Sugiyama. Online (multinomial) logistic bandit: Improved regret
and constant computation cost. In Thirty-seventh Conference on Neural Information
Processing Systems, 2023.
Z. Zhang, Y. Zhou, and X. Ji. Almost optimal model-free reinforcement learningvia
reference-advantage decomposition. In Advances in Neural Information Processing Sys-
tems, volume 33, pages 15198–15207, 2020.
Z. Zhang, J. Yang, X. Ji, and S. S. Du. Improved variance-aware confidence sets for linear
bandits and linear mixture mdp. Advances in Neural Information Processing Systems,
34:4342–4355, 2021a.
Z. Zhang, Y. Zhou, and X. Ji. Model-free reinforcement learning: from clipped pseudo-
regret to sample complexity. In International Conference on Machine Learning, pages
12653–12662. PMLR, 2021b.
D.ZhouandQ.Gu. Computationallyefficienthorizon-freereinforcementlearningforlinear
mixturemdps. Advances in neural information processing systems,35:36337–36349,2022.
D. Zhou, Q. Gu, and C. Szepesvari. Nearly minimax optimal reinforcement learning for
linear mixture markov decision processes. In Conference on Learning Theory, pages
4532–4576. PMLR, 2021a.
D. Zhou, J. He, and Q. Gu. Provably efficient reinforcement learning for discounted mdps
with feature mapping. In International Conference on Machine Learning, pages 12793–
12802. PMLR, 2021b.
21Table of Contents
1 Introduction 1
1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Problem Setting 5
2.1 Multinomial Logistic Markov Decision Processes (MNL-MDPs) . . . . . . . 6
2.2 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Computationally Efficient Randomized Algorithm for MNL-MDPs 7
3.1 Algorithm: RRL-MNL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.2 Regret bound of RRL-MNL . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4 Statistically Improved Algorithm for MNL-MDPs 10
4.1 Algorithms: ORRL-MNL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.2 Regret Bound of ORRL-MNL . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5 Numerical Experiments 14
6 Conclusions 15
A Notations & Definitions 23
A.1 Inhomogeneous MNL transition model . . . . . . . . . . . . . . . . . . . . . 23
A.2 Feature vector. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.3 Response variable & per-episode loss . . . . . . . . . . . . . . . . . . . . . . 24
A.4 Regularity constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.5 Estimated transition core . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.6 Gram matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.7 Confidence radius . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.8 Filtration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.9 Pseudo-noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.10Estimated value functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
A.11Prediction error & Bellman error . . . . . . . . . . . . . . . . . . . . . . . . 26
A.12Good events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
A.13Derivative of MNL transition model . . . . . . . . . . . . . . . . . . . . . . 27
B Detailed Regret Analysis for RRL-MNL (Theorem 1) 29
B.1 Concentration of Estimated Transition Core θk . . . . . . . . . . . . . . . . 29
h
B.1.1 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
B.1.2 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
B.2 Bound on Prediction Error . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
B.3 Good Events with High Probability . . . . . . . . . . . . . . . . . . . . . . . 41
B.4 Stochastic Optimism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
B.4.1 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
B.4.2 Proof of Lemma 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
B.4.3 Proof of Lemma 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
22B.5 Bound on Estimation Part . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
B.6 Bound on Pessimism Part . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
B.7 Regret Bound of RRL-MNL . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
C Detailed Regret Analysis for ORRL-MNL (Theorem 2) 52
C.1 Concentration of Estimated Transition Core θ(cid:101)k . . . . . . . . . . . . . . . . 52
h
C.1.1 Proof of Lemma 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
C.1.2 Proof of Lemma 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
C.1.3 Proof of Lemma 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
C.2 Bound on Prediction Error . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
C.3 Good Events with High Probability . . . . . . . . . . . . . . . . . . . . . . . 67
C.4 Stochastic Optimism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
C.4.1 Proof of Lemma 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
C.4.2 Proof of Lemma 20 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
C.5 Bound on Estimation Part . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
C.5.1 Proof of Lemma 22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
C.5.2 Proof of Lemma 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
C.5.3 Proof of Lemma 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
C.5.4 Proof of Lemma 25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
C.6 Bound on Pessimism Part . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
C.7 Regret Bound of ORRL-MNL . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
D Optimistic Exploration Extension 81
D.1 Optimism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
E Auxiliary Lemmas 85
A. Notations & Definitions
In this section, we formally summarize some definitions and notations used to analyze the
proposed algorithm.
A.1 Inhomogeneous MNL transition model
For h ∈ [H], the probability of state transition to s′ ∈ S when an action a is taken at a
s,a
state s is given by
exp(φ(s,a,s′)⊤θ∗)
P h(s′ | s,a) := P θ∗ h(s′ | s,a) = (cid:80) exp(φ(s,a,sh )⊤θ∗) .
s (cid:101)∈Ss,a (cid:101) h
The estimated transition probability parameterized by θ is denoted as
exp(φ(s,a,s′)⊤θ)
P (s′ | s,a) := .
θ (cid:80) exp(φ(s,a,s)⊤θ)
s (cid:101)∈Ss,a (cid:101)
23A.2 Feature vector
We abbreviate the feature vector as follows:
φ := φ(s,a,s′) for (s,a,s′) ∈ S ×A×S ,
s,a,s′ s,a
φ := φ(sk,ak,s′) for (k,h) ∈ [K]×[H] and s′ ∈ S := S ,
k,h,s′ h h k,h sk,ak
h h
φˆ (s,a) := φ(s,a,sˆ) for sˆ:= argmax∥φ(s,a,s′)∥ ,
k,h A−1
s′∈Ss,a k,h
φ¯ (θ) := φ¯(s,a,s′;θ) = φ(s,a,s′)−E [φ(s,a,s)],
s,a,s′ s (cid:101)∼P θ(·|s,a) (cid:101)
φ¯ (θ) := φ¯(sk,ak,s′;θ).
k,h,s′ h h
A.3 Response variable & per-episode loss
The response variable yk is given by
h
yk := [yk(s′)] where yk(s′) := 1I(sk = s′) for s′ ∈ S .
h h s′∈S k,h h h+1 k,h
The per-episode loss ℓ (θ) is given by
k,h
(cid:88)
ℓ (θ) := − yk(s′)logP (s′ | sk,ak),
k,h h θ h h
s′∈S
k,h
(cid:88)
G (θ) := ∇ℓ (θ) = (P (s′ | sk,ak)−yk(s′))φ ,
k,h k,h θ h h h k,h,s′
s′∈S
k,h
H (θ) := ∇2ℓ (θ)
k,h k,h
(cid:88) (cid:88) (cid:88)
= P (s′ | sk,ak)φ φ⊤ − P (s′ | sk,ak)P (s | sk,ak)φ φ⊤ .
θ h h k,h,s′ k,h,s′ θ h h θ (cid:101) h h k,h,s′ k,h,s
(cid:101)
s′∈S k,h s′∈S k,hs (cid:101)∈S k,h
A.4 Regularity constants
H : Horizon length
K : Episode number
T = KH : Total number of interactions
L : ℓ -norm upper bound of φ(s,a,s), i.e., ∥φ(s,a,s′)∥ ≤ L ,
φ 2 2 φ
L : ℓ -norm upper bound of θ∗, i.e., ∥θ∗∥ ≤ L ,
θ 2 h h 2 θ
κ : Problem-dependent constant such that inf P (s′ | s,a)P (s | s,a) ≥ κ,
θ θ (cid:101)
θ∈B (L )
d θ
U : Maximum cardinality of the set of reachable states, i.e., U := max|S |.
s,a
s,a
A.5 Estimated transition core
The estimated transition core for RRL-MNL is given by
1
θk = argmin ∥θ−θk−1∥2 +(θ−θk−1)⊤∇ℓ (θk−1),
h 2 h A k,h h k−1,h h
θ∈B (L )
d θ
24and the estimated transition core for ORRL-MNL is given by
1 (cid:13) (cid:13)2
θ(cid:101)k h+1 = θa ∈r Bg dm (Lin
θ) 2η
(cid:13) (cid:13)θ−θ(cid:101)k h(cid:13)
(cid:13) B(cid:101)k,h
+θ⊤∇ℓ k,h(θ(cid:101)k h).
A.6 Gram matrices
The Gram matrix with global gradient information κ is given by
k−1
κ (cid:88) (cid:88)
A := λI + φ(si,ai,s′)φ(si,ai,s′)⊤.
k,h d 2 h h h h
i=1s′∈S
i,h
The Gram matrices with local gradient information are given by
k−1
(cid:88)
B(cid:101)k,h := B k,h+η∇2ℓ k,h(θ(cid:101)k h) and B
k,h
:= λI d+ ∇2ℓ i,h(θ(cid:101)i h+1).
i=1
A.7 Confidence radius
For some absolute constants C ,C > 0,
β ξ
(cid:115)
8d (cid:18) kUL2 (cid:19) (cid:18) 32L L 16(cid:19) (1+⌈2log kUL L ⌉)k2 √
α := α (δ) = log 1+ φ + φ θ + log 2 φ θ +2 2+2λL2
k k κ dλ 3 κ δ θ
= O(cid:101)(κ−1/2d1/2),
(cid:115) √
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
H 1+2k k
β := β (δ) = C logU λlog(Uk)+log(Uk)log +dlog 1+ +λL2
k k β δ dλ θ
√
= O( dlogUlog(kH)),
(cid:112)
γ := γ (δ) = C σ dlog(Md/δ).
k k ξ k
A.8 Filtration
For an arbitrary set X, we denote the Σ-algebra generated by X as Σ(X). Then we define
the following filtrations
(cid:16) (cid:110) (cid:111)(cid:17)
F := Σ (cid:8) si,ai,r(si,ai) | i < k,j ≤ H(cid:9) ∪ ξ(m) | i < k,j ≤ H,1 ≤ m ≤ M ,
k j j j j i,j
(cid:16) (cid:110) (cid:111) (cid:110) (cid:111)(cid:17)
F := Σ F ∪ sk,ak,r(sk,ak) | j ≤ h ∪ ξ(m) | j ≥ h,1 ≤ m ≤ M .
k,h k j j j j k,j
A.9 Pseudo-noise
For RRL-MNL, the pseudo-noise is sampled as
ξ(m) ∼ N(0 ,σ2A−1),
k,h d k k,h
and for ORRL-MNL, the pseudo-noise is sampled as
ξ(m) ∼ N(0 ,σ2B−1),
k,h d k k,h
for M times independently.
25A.10 Estimated value functions
The stochastically optimistic value function for RRL-MNL is defined as follows:
Qk (s,a) = 0,
H+1
(cid:26) (cid:27)
Qk(s,a) = min r(s,a)+ (cid:88) P (s′ | s,a)Vk (s′)+ max φˆ (s,a)⊤ξ(m) ,H for h ∈ [H].
h θk h+1 k,h k,h
h m∈[M]
s′∈Ss,a
The optimistic randomized value function for ORRL-MNL is defined as follows:
Q(cid:101)k (s,a) = 0,
H+1
(cid:26) (cid:27)
(cid:88)
Q(cid:101)k h(s,a) := min r(s,a)+ P θ(cid:101)k(s′ | s,a)V(cid:101) hk +1(s′)+ν kra ,hnd(s,a),H for h ∈ [H],
h
s′∈Ss,a
where
ν kra ,hnd(s,a) := s′(cid:88) ∈Ss,aP
θ(cid:101)k
h(s′ | s,a)φ¯(s,a,s′;θ(cid:101)k h)⊤ξs k′ ,h+3Hβ k2 sm ′∈Sa sx ,a∥φ(s,a,s′)∥2
B− k,1
h,
ξs′ := ξm(s′) for m(s′) := argmaxφ¯(s,a,s′;θ(cid:101)k)⊤ξm .
k,h k,h h k,h
m∈[M]
A.11 Prediction error & Bellman error
Definition 1 (Prediction error & Bellman error). For any (s,a) ∈ S × A and (k,h) ∈
[K]×[H], we define the prediction error about θk as
h
(cid:88) (cid:16) (cid:17)
∆k h(s,a) := P θk h(s′ | s,a)−P θ∗ h(s′|s,a) V hk +1(s′).
s′∈Ss,a
Also we define the Bellman error as follows:
ιk(s,a) := r(s,a)+P Vk (s,a)−Qk(s,a).
h h h+1 h
A.12 Good events
For any δ ∈ (0,1), we define the following good events: For RRL-MNL,
(cid:110) (cid:111)
G∆ (δ) := |∆k(s,a)| ≤ Hα (δ)∥φˆ (s,a)∥ ,
k,h h k k,h A−1
k,h
(cid:26) (cid:27)
Gξ (δ) := max ∥ξ(m) ∥ ≤ γ (δ) ,
k,h k,h A k,h k
m∈[M]
(cid:110) (cid:111)
G (δ) := G∆ (δ)∩Gξ (δ) ,
k,h k,h k,h
(cid:92)
G (δ) := G (δ),
k k,h
h∈[H]
(cid:92)
G(K,δ) := G (δ).
k
k≤K
26For ORRL-MNL,
(cid:26) (cid:88) (cid:13) (cid:13) (cid:27)
G∆ k,h(δ) := |∆k h(s,a)| ≤ Hβ k(δ s) ′∈Ss,P
aθ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13) B− k,1
h
+3Hβ k(δ)2 sm ′∈Sa sx ,a∥φ s,a,s′∥2
B− k,1
h
,
(cid:26) (cid:27)
Gξ (δ) := max ∥ξ(m) ∥ ≤ γ (δ) ,
k,h k,h B k,h k
m∈[M]
(cid:110) (cid:111)
G (δ) := G∆ (δ)∩Gξ (δ) ,
k,h k,h k,h
(cid:92)
G (δ) := G (δ),
k k,h
h∈[H]
(cid:92)
G(K,δ) := G (δ).
k
k≤K
A.13 Derivative of MNL transition model
Proposition 1 (Derivative of MNL transition model). The gradient and Hessian of P (· |
θ
·,·) can be calculated as follows:
 
(cid:88)
∇P θ(s′ | s,a) = P θ(s′ | s,a)φ s,a,s′ − P θ(s′′ | s,a)φ s,a,s′′
(8)
s′′∈Ss,a
= P (s′ | s,a)φ¯ (θ),
θ s,a,s′
and
∇2P (s′ | s,a)
θ
= P (s′ | s,a)φ φ⊤
θ s,a,s′ s,a,s′
(cid:88) (cid:16) (cid:17)
−P (s′ | s,a) P (s′′ | s,a) φ φ⊤ +φ φ⊤ +φ φ⊤
θ θ s,a,s′ s,a,s′′ s,a,s′′ s,a,s′ s,a,s′′ s,a,s′′
(9)
s′′∈Ss,a
  ⊤
(cid:88) (cid:88)
+2P θ(s′ | s,a) P θ(s′′ | s,a)φ s,a,s′′ P θ(s′′ | s,a)φ s,a,s′′ .
s′′∈Ss,a s′′∈Ss,a
Proof of Proposition 1. Let θ = (θ ,...,θ ) and [φ ] be the i-th component of φ .
1 d s,a,s′ i s,a,s′
Then, we have
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
exp φ⊤ θ [φ ] exp φ⊤ θ (cid:80) exp φ⊤ θ [φ ]
∂
P (s′ | s,a) =
s,a,s′ s,a,s′ j
−
s,a,s′ s′′∈Ss,a s,a,s′′ s,a,s′′ j
∂θ j θ (cid:80) s′′∈Ss,aexp(cid:16) φ⊤ s,a,s′′θ(cid:17) (cid:16) (cid:80) s′′∈Ss,aexp(cid:16) φ⊤ s,a,s′′θ(cid:17)(cid:17)2
 
(cid:88)
= P θ(s′ | s,a)[φ s,a,s′] j − P θ(s′′ | s,a)[φ s,a,s′′] j .
s′′∈Ss,a
27Then, the gradient of P (s′ | s,a) is given by
θ
(cid:88)
∇P (s′ | s,a) = P (s′ | s,a)φ −P (s′ | s,a) P (s′′ | s,a)φ
θ θ s,a,s′ θ θ s,a,s′′
s′′∈Ss,a
 
(cid:88)
= P θ(s′ | s,a)φ s,a,s′ − P θ(s′′ | s,a)φ s,a,s′′
s′′∈Ss,a
= P (s′ | s,a)φ¯ (θ).
θ s,a,s′
On the other hand, the second derivative ∂ P (s′ | s,a) can be obtained as follows:
∂θi∂θj θ
∂
P (s′ | s,a)
θ
∂θ ∂θ
i j
  
(cid:88) (cid:88)
= P θ(s′ | s,a)[φ s,a,s′] i− P θ(s′′ | s,a)[φ s,a,s′′] i[φ s,a,s′] j − P θ(s′′ | s,a)[φ s,a,s′′] j
s′′∈Ss,a s′′∈Ss,a
   
(cid:88) (cid:88)
+P θ(s′ | s,a)− P θ(s′′ | s,a)[φ s,a,s′′] i− P θ(s (cid:101)| s,a)[φ s,a,s] i[φ s,a,s′′] j
(cid:101)
s′′∈Ss,a s (cid:101)∈Ss,a

= P (s′ | s,a) [φ ] [φ ] − (cid:88) P (s′′ | s,a)(cid:0) [φ ] [φ ] +[φ ] [φ ] (cid:1)
θ s,a,s′ i s,a,s′ j θ s,a,s′′ i s,a,s′ j s,a,s′ i s,a,s′′ j

s′′∈Ss,a
  
(cid:88) (cid:88)
+ P θ(s′′ | s,a)[φ s,a,s′′] i P θ(s′′ | s,a)[φ s,a,s′′] j
s′′∈Ss,a s′′∈Ss,a
(cid:88)
− P (s′′ | s,a)[φ ] [φ ]
θ s,a,s′′ i s,a,s′′ j
s′′∈Ss,a
  
(cid:88) (cid:88) 
+ P θ(s′′ | s,a)[φ s,a,s′′] j P θ(s (cid:101)| s,a)[φ s,a,s] i
(cid:101)

s′′∈Ss,a s (cid:101)∈Ss,a

= P (s′ | s,a) [φ ] [φ ] − (cid:88) P (s′′ | s,a)(cid:0) [φ ] [φ ] +[φ ] [φ ] (cid:1)
θ s,a,s′ i s,a,s′ j θ s,a,s′′ i s,a,s′ j s,a,s′ i s,a,s′′ j

s′′∈Ss,a
(cid:88)
− P (s′′ | s,a)[φ ] [φ ]
θ s,a,s′′ i s,a,s′′ j
s′′∈Ss,a
  
(cid:88) (cid:88) 
+2 P θ(s′′ | s,a)[φ s,a,s′′] i P θ(s′′ | s,a)[φ s,a,s′′] j .

s′′∈Ss,a s′′∈Ss,a
28Thus, we get the desired result as follows:
∇2P (s′ | s,a)
θ
= P (s′ | s,a)φ φ⊤
θ s,a,s′ s,a,s′
(cid:88) (cid:16) (cid:17)
−P (s′ | s,a) P (s′′ | s,a) φ φ⊤ +φ φ⊤ +φ φ⊤
θ θ s,a,s′ s,a,s′′ s,a,s′′ s,a,s′ s,a,s′′ s,a,s′′
s′′∈Ss,a
  ⊤
(cid:88) (cid:88)
+2P θ(s′ | s,a) P θ(s′′ | s,a)φ s,a,s′′ P θ(s′′ | s,a)φ s,a,s′′ .
s′′∈Ss,a s′′∈Ss,a
B. Detailed Regret Analysis for RRL-MNL (Theorem 1)
In this section, we provide the complete proof of Theorem 1. First, we introduce all the
technical lemmas needed to prove Theorem 1 along with their proofs. At the end of this
section, we present the proof of Theorem 1.
B.1 Concentration of Estimated Transition Core θk
h
In this section, we provide the concentration inequality for the estimated transition core
run by the approximate online Newton step. The proof is similar to that given by Oh and
Iyengar (2021). For completeness, we provide the detailed proof.
Lemma 1 (Concentrationofonlineestimatedtransitioncore). Foreachh ∈ [H], ifλ ≥ L2,
φ
then we have
(cid:16) (cid:17)
P ∀k ≥ 1,∥θk −θ∗∥ ≤ α (δ) ≥ 1−δ.
h h A k,h k
where α (δ) is given by
k
(cid:115)
8d (cid:18) kUL2 (cid:19) (cid:18) 32L L 16(cid:19) (1+⌈2log kUL L ⌉)k2 √
α (δ) := log 1+ φ + φ θ + log 2 φ θ +2 2+2λL2 .
k κ dλ 3 κ δ θ
Proof of Lemma 1. Recallthattheper-roundlossℓ (θ)anditsgradientG (θ)isdefined
k,h k,h
as follows:
(cid:88)
ℓ (θ) := − yk(s′)logP (s′ | sk,ak), G (θ) := ∇ ℓ (θ).
k,h h θ h h k,h θ k,h
s′∈S
k,h
For the analysis, we define the conditional expectations of ℓ (θ) & G (θ) as follows:
k,h k,h
ℓ¯ (θ) := E [ℓ (θ) | F ] , G¯ (θ) := E [G (θ) | F ].
k,h yk k,h k,h k,h yk k,h k,h
h h
By Taylor expansion with θ¯ = νθk +(1−ν)θ∗ for some ν ∈ (0,1), we have
h h
1
ℓ (θ∗) = ℓ (θk)+G (θk)⊤(θ∗ −θk)+ (θ∗ −θk)⊤H (θ¯)(θ∗ −θk), (10)
k,h h k,h h k,h h h h 2 h h k,h h h
29where H (θ) is the Hessian of the per-round loss evaluated at θ, i.e.,
k,h
H (θ) := ∇2ℓ (θ) (11)
k,h k,h
(cid:88) (cid:88) (cid:88)
= P (s′ | sk,ak)φ φ⊤ − P (s′ | sk,ak)P (s | sk,ak)φ φ⊤ .
θ h h k,h,s′ k,h,s′ θ h h θ (cid:101) h h k,h,s′ k,h,s
(cid:101)
s′∈S k,h s′∈S k,hs (cid:101)∈S k,h
Note that for θ¯ = νθk +(1−ν)θ∗ with ν ∈ (0,1), we have
h h
(cid:88)
H (θ¯) = P (s′ | sk,ak)φ φ⊤
k,h θ¯ h h k,h,s′ k,h,s′
s′∈S
k,h
(cid:88) (cid:88)
− P (s′ | sk,ak)P (s | sk,ak)φ φ⊤
θ¯ h h θ¯ (cid:101) h h k,h,s′ k,h,s
(cid:101)
s′∈S k,hs (cid:101)∈S
k,h
(cid:88)
= P (s′ | sk,ak)φ φ⊤
θ¯ h h k,h,s′ k,h,s′
s′∈S
k,h
1 (cid:88) (cid:88)
− P (s′ | sk,ak)P (s | sk,ak)(φ φ⊤ +φ φ⊤ )
2 θ¯ h h θ¯ (cid:101) h h k,h,s′ k,h,s (cid:101) k,h,s (cid:101) i,h,s′
s′∈S k,hs (cid:101)∈S
k,h
(cid:88)
⪰ P (s′ | sk,ak)φ φ⊤
θ¯ h h k,h,s′ k,h,s′
s′∈S
k,h
1 (cid:88) (cid:88)
− P (s′ | sk,ak)P (s | sk,ak)(φ φ⊤ +φ φ⊤ )
2 θ¯ h h θ¯ (cid:101) h h k,h,s′ k,h,s′ k,h,s (cid:101) k,h,s (cid:101)
s′∈S k,hs (cid:101)∈S
k,h
(cid:88)
= P (s′ | sk,ak)φ φ⊤
θ¯ h h k,h,s′ k,h,s′
s′∈S
k,h
(cid:88) (cid:88)
− P (s′ | sk,ak)P (s | sk,ak)φ φ⊤ ,
θ¯ h h θ¯ (cid:101) h h k,h,s′ k,h,s′
s′∈S k,hs (cid:101)∈S
k,h
30where the inequality utilizes the fact that xx⊤ + yy⊤ ⪰ xy⊤ + yx⊤ for any x,y ∈ Rd.
Therefore, we have
(cid:88)
H (θ¯) ⪰ P (s′ | sk,ak)φ φ⊤
k,h θ¯ h h k,h,s′ k,h,s′
s′∈S
k,h
(cid:88) (cid:88)
− P (s′ | sk,ak)P (s | sk,ak)φ φ⊤
θ¯ h h θ¯ (cid:101) h h k,h,s′ k,h,s′
s′∈S k,hs (cid:101)∈S
k,h
(cid:88)
= P (s′ | sk,ak)φ φ⊤
θ¯ h h k,h,s′ k,h,s′
s′̸=s˙
k,h
(cid:88) (cid:88)
− P (s′ | sk,ak)P (s | sk,ak)φ φ⊤
θ¯ h h θ¯ (cid:101) h h k,h,s′ k,h,s′
s′̸=s˙ k,hs (cid:101)̸=s˙
k,h
 
(cid:88) (cid:88)
= P θ¯(s′ | sk h,ak h)1− P θ¯(s (cid:101)| sk h,ak h)φ k,h,s′φ⊤
k,h,s′
s′̸=s˙ k,h s (cid:101)̸=s˙ k,h
(cid:88)
= P (s′ | sk,ak)P (s˙ | sk,ak)φ φ⊤
θ¯ h h θ¯ k,h h h k,h,s′ k,h,s′
s′̸=s˙
k,h
(cid:88)
⪰ κφ φ⊤
k,h,s′ k,h,s′
s′̸=s˙
k,h
(cid:88)
= κφ φ⊤ ,
k,h,s′ k,h,s′
s′∈S
k,h
where s˙ is the state satisfying φ(sk,ak,s˙ ) = 0 and the last inequality comes from the
k,h h h k,h d
Assumption 4.
Using the lower bound of the Hessian of the per-round loss evaluated at θ¯, from (10) we
have
 
κ (cid:88)
ℓ k,h(θ∗ h) ≥ ℓ k,h(θk h)+G k,h(θk h)⊤(θ∗ h−θk h)+ 2(θ∗ h−θk h)⊤  φ k,h,s′φ⊤ k,h,s′(θ∗ h−θk h).
s′∈S
k,h
By rearranging, we have
κ
ℓ (θk) ≤ ℓ (θ∗)+G (θk)⊤(θk −θ∗)− (θ∗ −θk)⊤W (θ∗ −θk),
k,h h k,h h k,h h h h 2 h h k,h h h
where we denote W := (cid:80) φ φ⊤ . By taking expectation over yk, we have
k,h s′∈S k,h k,h,s′ k,h,s′ h
κ
ℓ¯ (θk) ≤ ℓ¯ (θ∗)+G¯ (θk)⊤(θk −θ∗)− (θ∗ −θk)⊤W (θ∗ −θk). (12)
k,h h k,h h k,h h h h 2 h h k,h h h
31On the other hand, for any θ ∈ Rd, since we have
ℓ¯ (θ)−ℓ¯ (θ∗)
k,h k,h h
(cid:88) (cid:88)
= − P θ∗(s′ | sk h,ak h)logP θ(s′ | sk h,ak h)+ P θ∗(s′ | sk h,ak h)logP θ∗(s′ | sk h,ak h)
h h h
s′∈S s′∈S
k,h k,h
(cid:88) (cid:16) (cid:17)
= P θ∗(s′ | sk h,ak h) logP θ∗(s′ | sk h,ak h)−logP θ(s′ | sk h,ak h)
h h
s′∈S
k,h
= (cid:88) P θ∗ h(s′ | sk h,ak h)log P Pθ∗ h (( ss ′′ || ss kk h ,, aa kk h ))
s′∈S θ h h
k,h
= D KL(P θ∗ ∥ P θ)
h
≥ 0,
where D (P ∥ Q) is the Kullback-Leibler divergence of P from Q, from (12) we have
KL
0 ≤ ℓ¯ (θk)−ℓ¯ (θ∗)
k,h h k,h h
κ
≤ G¯ (θk)⊤(θk −θ∗)− ∥θ∗ −θk∥2
k,h h h h 2 h h W k,h
κ (cid:16) (cid:17)⊤
= G (θk)⊤(θk −θ∗)− ∥θ∗ −θk∥2 + G¯ (θk)−G (θk) (θk −θ∗). (13)
k,h h h h 2 h h W k,h k,h h k,h h h h
To get an upper bound of G (θk)⊤(θk −θ∗), recall that the estimated transition core is
k,h h h h
given by
1
θk+1 = argmin ∥θ−θk∥2 +(θ−θk)⊤G (θk). (14)
h 2 h A k+1,h h k,h h
θ∈B (L )
d θ
Since the objective function in (14) is convex, by the first-order optimality condition for
any θ ∈ B (L ), we have
d θ
(cid:16) (cid:17)⊤
G (θk)+A (θk+1−θk) (θ−θk+1) ≥ 0,
k,h h k+1,h h h h
which gives
θ⊤A (θk+1−θk) ≥ (θk+1)⊤A (θk+1−θk)−G (θk)⊤(θ−θk+1). (15)
k+1,h h h h k+1,h h h k,h h h
Then, we have
∥θk −θ∗∥2 −∥θk+1−θ∗∥2
h h A k+1,h h h A k+1,h
= (θk)⊤A θk −(θk+1)⊤A θk+1+2(θ∗)⊤A (θk+1−θk)
h k+1,h h h k+1,h h h k+1,h h h
≥ (θk)⊤A θk −(θk+1)⊤A θk+1+2(θk+1)⊤A (θk+1−θk)
h k+1,h h h k+1,h h h k+1,h h h
−2G (θk)⊤(θ∗ −θk+1) (by (15))
k,h h h h
= (θk)⊤A θk +(θk+1)⊤A θk+1−2(θk+1)⊤A θk −2G (θk)⊤(θ∗ −θk+1)
h k+1,h h h k+1,h h h k+1,h h k,h h h h
= ∥θk −θk+1∥2 −2G (θk)⊤(θ∗ −θk+1)
h h A k+1,h k,h h h h
= ∥θk −θk+1∥2 +2G (θk)⊤(θk+1−θk)+2G (θk)⊤(θk −θ∗)
h h A k+1,h k,h h h h k,h h h h
≥ −∥G (θk)∥2 +2G (θk)⊤(θk −θ∗), (16)
k,h h A−1 k,h h h h
k+1,h
32where the last inequality follows by the fact that
(cid:110) (cid:111)
∥θk −θk+1∥2 +2G (θk)⊤(θk+1−θk) ≥ min ∥θ∥2 +2G (θk)⊤θ
h h A k+1,h k,h h h h θ∈B (L ) A k+1,h k,h h
d θ
= −∥G (θk)∥2 .
k,h h A−1
k+1,h
Therefore, from (16) we have
1 1 1
G (θk)⊤(θk−θ∗) ≤ ∥G (θk)∥2 + ∥θk−θ∗∥2 − ∥θk+1−θ∗∥2 . (17)
k,h h h h 2 k,h h A−1 2 h h A k+1,h 2 h h A k+1,h
k+1,h
By substituting (17) into (13), we have
1 1 1
0 ≤ ∥G (θk)∥2 + ∥θk −θ∗∥2 − ∥θk+1−θ∗∥2
2 k,h h A−1 2 h h A k+1,h 2 h h A k+1,h
k+1,h
κ (cid:16) (cid:17)⊤
− ∥θ∗ −θk∥ + G¯ (θk)−G (θk) (θk −θ∗). (18)
2 h h W k,h k,h h k,h h h h
Note that since we have
∥G (θk)∥2
k,h h A−1
k+1,h
(cid:88) (cid:16) (cid:17)(cid:16) (cid:17)
= P (s′ | sk,ak)−yk(s′) P (s | sk,ak)−yk(s) φ⊤ A−1 φ
θk h h h h θk h (cid:101) h h h (cid:101) k,h,s′ k+1,h k,h,s (cid:101)
s′,s∈S
(cid:101) k,h
1 (cid:88) (cid:16) (cid:17)(cid:16) (cid:17)
= P (s′ | sk,ak)−yk(s′) P (s | sk,ak)−yk(s) (φ⊤ A−1 φ +φ⊤ A−1 φ )
2 θk h h h h θk h (cid:101) h h h (cid:101) k,h,s′ k+1,h k,h,s (cid:101) k,h,s (cid:101) k+1,h k,h,s′
s′,s∈S
(cid:101) k,h
(cid:20) (cid:21)
1 (cid:88) (cid:16) (cid:17)2 (cid:16) (cid:17)2
≤ P (s′ | sk,ak)−yk(s′) φ⊤ A−1 φ + P (s | sk,ak)−yk(s) φ⊤ A−1 φ
2 θk h h h h k,h,s′ k+1,h k,h,s′ θk h (cid:101) h h h (cid:101) k,h,s (cid:101) k+1,h k,h,s (cid:101)
s′,s∈S
(cid:101) k,h
(cid:88) (cid:16) (cid:17)2
= P (s′ | sk,ak)−yk(s′) φ⊤ A−1 φ
θk h h h k,h,s′ k+1,h k,h,s′
h
s′∈S
k,h
(cid:88) (cid:12) (cid:12)
≤ (cid:12)P (s | sk,ak)−yk(s)(cid:12)φ⊤ A−1 φ
(cid:12) θk (cid:101) h h h (cid:101) (cid:12) k,h,s′ k+1,h k,h,s′
h
s′∈S
k,h
(cid:88) (cid:16) (cid:17)
≤ P (s | sk,ak)+yk(s′) φ⊤ A−1 φ
θk (cid:101) h h h k,h,s′ k+1,h k,h,s′
h
s′∈S
k,h
(cid:88) (cid:88)
= P (s | sk,ak)φ⊤ A−1 φ + yk(s′)φ⊤ A−1 φ
θk (cid:101) h h k,h,s′ k+1,h k,h,s′ h k,h,s′ k+1,h k,h,s′
h
s′∈S s′∈S
k,h k,h
≤ 2 max ∥φ ∥2 , (19)
s′∈S k,h
k,h,s′ A− k+1
1,h
where the first inequality utilizes the inequality x⊤Ay+y⊤Ax ≤ x⊤Ax+y⊤Ay for any
positive-semidefinite matrix A, and the last inequality holds since 0 ≤ P (s′ | sk,ak) ≤ 1
θk h h
and (cid:80) P (s′ | sk,ak) = 1. h
s′ θk h h
h
33Combining the results of (18) and (19), we have
1 1
0 ≤ max ∥φ ∥2 + ∥θk −θ∗∥2 − ∥θk+1−θ∗∥2
s′∈S k,h k,h,s′ A− k+1 1,h 2 h h A k+1,h 2 h h A k+1,h
κ (cid:16) (cid:17)⊤
− ∥θ∗ −θk∥ + G¯ (θk)−G (θk) (θk −θ∗)
2 h h W k,h k,h h k,h h h h
1 κ 1
= max ∥φ ∥2 + ∥θk −θ∗∥2 + ∥θk −θ∗∥2 − ∥θk+1−θ∗∥2
s′∈S k,h k,h,s′ A− k+1 1,h 2 h h A k,h 4 h h W k,h 2 h h A k+1,h
κ (cid:16) (cid:17)⊤
− ∥θ∗ −θk∥ + G¯ (θk)−G (θk) (θk −θ∗)
2 h h W k,h k,h h k,h h h h
1 κ 1
= max ∥φ ∥2 + ∥θk −θ∗∥2 − ∥θk −θ∗∥2 − ∥θk+1−θ∗∥2
s′∈S k,h k,h,s′ A− k+1 1,h 2 h h A k,h 4 h h W k,h 2 h h A k+1,h
(cid:16) (cid:17)⊤
+ G¯ (θk)−G (θk) (θk −θ∗),
k,h h k,h h h h
where for the first equality we use A = A + κW . By rearranging the terms, we
k+1,h k,h 2 k,h
have
κ
∥θk+1−θ∗∥2 ≤ ∥θk −θ∗∥2 +2 max ∥φ ∥2 − ∥θk −θ∗∥2
h h A k+1,h h h A k,h s′∈S k,h k,h,s′ A− k+1 1,h 2 h h W k,h
(cid:16) (cid:17)⊤
+2 G¯ (θk)−G (θk) (θk −θ∗).
k,h h k,h h h h
Then summing over k gives
k k
(cid:88) κ (cid:88)
∥θk+1−θ∗∥2 ≤ ∥θ −θ∗∥2 +2 max ∥φ ∥2 − ∥θi −θ∗∥2
h h A k+1,h 1,h h A 1,h i=1s′∈S i,h i,h,s′ A− i+1 1,h 2 i=1 h h W i,h
k
+2(cid:88)(cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗)
i,h h i,h h h h
i=1
k k
(cid:88) κ (cid:88)
≤ 2λL2+2 max ∥φ ∥2 − ∥θi −θ∗∥2
θ i=1s′∈S i,h i,h,s′ A− i+1 1,h 2 i=1 h h W i,h
k
+2(cid:88)(cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗).
i,h h i,h h h h
i=1
For the final step, note that (cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗) is a martingale difference
i,h h i,h h h h
sequence. To bound this term, we invoke the following lemmas:
Lemma 2. For δ ∈ (0,1) and (k,h) ∈ [K]×[H], with a probability at least 1−δ we have
k
(cid:88)(cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗)
i,h h i,h h h
i=1
≤
κ (cid:88)k
∥θi −θ∗∥2
+(cid:18) 16L φL
θ +
8(cid:19)
log
(1+⌈2log 2kUL φL θ⌉)k2 +√
2.
4 h h W i,h 3 κ δ
i=1
34Lemma 3 (Generalized elliptical potential). Let S := {x ,...,x } ⊂ Rd. For any
t t,1 t,K
1 ≤ t ≤ T and i ∈ [K], suppose ∥x ∥ ≤ L. Let V := λI +(cid:80)t−1 (cid:80) x x⊤ for some
t,i 2 t d τ=1 i∈Sτ τ,i τ,i
λ > 0. If λ ≥ L2, then we have
T (cid:18) (cid:19)
(cid:88) TKL
max∥x ∥2 ≤ 2dlog 1+ .
i∈[K] t,i V t−1 dλ
t=1
By Lemma 2, with probability at least 1−δ, we have
∥θk+1−θ∗∥2
h h A k+1,h
≤ 2λL2
+2(cid:88)k
max ∥φ ∥2
+(cid:18) 32L φL
θ +
16(cid:19)
log
(1+⌈2log 2kUL φL θ⌉)k2 +2√
2
θ i=1s′∈S i,h i,h,s′ A− i+1 1,h 3 κ δ
(cid:32) (cid:33)
8 kUL2 (cid:18) 32L L 16(cid:19) (1+⌈2log kUL L ⌉)k2 √
≤ 2λL2 + dlog 1+ φ + φ θ + log 2 φ θ +2 2,
θ κ dλ 3 κ δ
where the second inequality comes from Lemma 3. Note that the Gram matrix A in
k,h
Algorithm 1 and the Gram matrix V in Lemma 3 are different by the factor of κ, which
2
results in additional 2 factor for the bound of (cid:80)k max ∥φ ∥2 .
κ i=1 s′∈S i,h i,h,s′ A−1
i+1,h
In the following, we provide all the proofs of the lemmas used to prove Lemma 1.
B.1.1 Proof of Lemma 2
Proof of Lemma 2. Note that (cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗) is a martingale difference
i,h h i,h h h h
sequence, i.e.,
(cid:104) (cid:105)
E (cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗) | F
i,h h i,h h h h i,h
= (cid:0) G¯ (θi)−E(cid:2) G (θi) | F (cid:3)(cid:1)⊤ (θi −θ∗)
i,h h i,h h i,h h h
= 0.
On the other hand, for any θ ∈ Rd, since we have
(cid:13) (cid:13)
(cid:13) (cid:13)
∥G i,h(θ)∥ 2 = (cid:13) (cid:13) (cid:88) (cid:0) P θ(s′ | si h,ai h)−y hi(s′)(cid:1) φ i,h,s′(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)s′∈S (cid:13)
i,h 2
≤ (cid:88) (cid:12) (cid:12)P θ(s′ | si h,ai h)−y hi(s′)(cid:12) (cid:12)∥φ i,h,s′∥
2
s′∈S
i,h
 
(cid:88) (cid:88)
≤ L φ P θ(s′ | si h,ai h)+ y hi(s′)
s′∈S s′∈S
i,h i,h
= 2L ,
φ
35then, it follows by
(cid:12) (cid:12)
(cid:12)(cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗)(cid:12)
(cid:12) i,h h i,h h h h (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
≤ (cid:12)(cid:0) G¯ (θi)(cid:1)⊤ (θi −θ∗)(cid:12)+(cid:12)(cid:0) G (θi)(cid:1)⊤ (θi −θ∗)(cid:12)
(cid:12) i,h h h h (cid:12) (cid:12) i,h h h h (cid:12)
≤ ∥G¯ (θi)∥ ∥θi −θ∗∥ +∥G (θi)∥ ∥θi −θ∗∥
i,h h 2 h h 2 i,h h 2 h h 2
≤ 4L ∥θi −θ∗∥
φ h h 2
≤ 8L L , (20)
φ θ
where the last inequality follows by ∥θi −θ∗∥ ≤ ∥θi∥ +∥θ∗∥ ≤ 2L . Hence, if we denote
h h 2 h 2 h 2 θ
M := (cid:80)k (cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗), then M is a martingale. Note that we
k,h i=1 i,h h i,h h h h k,h
also have
k (cid:20) (cid:21)
Σ = (cid:88) E (cid:16) (cid:2) G¯ (θi)−G (θi)(cid:3)⊤ (θi −θ∗)(cid:17)2
k,h yi i,h h i,h h h h
h
i=1
k (cid:34) (cid:35) (cid:34) (cid:35)
= (cid:88) E (cid:16) (cid:2) G (θi)(cid:3)⊤ (θi −θ∗)(cid:17)2 −E (cid:16) (cid:2) G¯ (θi)(cid:3)⊤ (θi −θ∗)(cid:17)2
yi i,h h h h yi i,h h h h
h h
i=1
k (cid:20) (cid:21)
≤ (cid:88) E (cid:16) (cid:2) G (θi)(cid:3)⊤ (θi −θ∗)(cid:17)2
yi i,h h h h
h
i=1
 2
k
(cid:88) (cid:88) (cid:16) (cid:17)
= E yi  P θi(s′ | si h,ai h)−y hi(s′) φ⊤ i,h,s′(θi h−θ∗ h) 
h h
i=1 s′∈S
i,h
  
k
(cid:88) (cid:88) (cid:16) (cid:17)2 (cid:88) (cid:16) (cid:17)2
≤ E yi  P θi(s′ | si h,ai h)−y hi(s′)  φ⊤ i,h,s′(θi h−θ∗ h) 
h h
i=1 s′∈S s′∈S
i,h i,h
(21)
  
k
(cid:88) (cid:88) (cid:16) (cid:17)2 (cid:88) (cid:16) (cid:17)2
= E yi  P θi(s′ | si h,ai h)−y hi(s′)  φ⊤ i,h,s′(θi h−θ∗ h) 
h h
i=1 s′∈S s′∈S
i,h i,h
k
(cid:88) (cid:88) (cid:16) (cid:17)2
≤ 2 φ⊤ (θi −θ∗) (22)
i,h,s′ h h
i=1s′∈S
i,h
k
(cid:88)
= 2 ∥θi −θ∗∥2 =: B ,
h h W i,h k,h
i=1
where (21) holds by the Cauchy–Schwarz inequality, (22) holds because
(cid:88) (cid:16) (cid:17)2
P (s′ | si,ai)−yi(s′)
θi h h h
h
s′∈S
i,h
= (cid:88) (cid:110) P (s′ | si,ai)(cid:111)2 −2P (s′ | si,ai)yi(s′)+(cid:8) yi(s′)(cid:9)2
θi h h θi h h h h
h h
s′∈S
i,h
≤ 2.
36However, if we denote B := 2(cid:80)k ∥θi −θ∗∥2 , since B is itself a random variable,
k,h i=1 h h W k,h
i,h
to apply Freedman’s inequality to M , we consider two cases depending on the values of
k,h
B .
k,h
Case 1 : B ≤ 4
k,h kU
Suppose that B = 2(cid:80)k ∥θi −θ∗∥2 ≤ 4 . Then we have
k,h i=1 h h W i,h kU
k
M = (cid:88)(cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗)
k,h i,h h i,h h h h
i=1
k
= (cid:88) (cid:88) (cid:0) yi(s′)−E[yi(s′)](cid:1) φ⊤ (θi −θ∗)
h h i,h,s′ h h
i=1s′∈S
i,h
k
(cid:88) (cid:88) (cid:16) (cid:17)
= y hi(s′)−P θ∗(s′ | si h,ai h) φ⊤ i,h,s′(θi h−θ∗ h)
h
i=1s′∈S
i,h
k
(cid:88) (cid:88)
≤ |φ⊤ (θi −θ∗)|
i,h,s′ h h
i=1s′∈S
i,h
(cid:118)
(cid:117)
k
(cid:117) (cid:88) (cid:88) (cid:16) (cid:17)2
≤ (cid:117)kU φ⊤ (θi −θ∗)
(cid:116) i,h,s′ h h
i=1s′∈S
i,h
(cid:114)
B
k,h
= kU
2
√
≤ 2.
Case 2 : B > 4
k,h kU
Suppose that B = 2(cid:80)k ∥θi − θ∗∥2 > 4 . Then, we have both a lower and
k,h i=1 h h W i,h kU
upper bound for B as follows:
k,h
k
4 (cid:88) (cid:88)
< B ≤ 2 ∥φ ∥2∥θi −θ∗∥2 ≤ 8kUL2L2 .
kU k,h i,h,s′ 2 h h 2 φ θ
i=1s′∈S
i,h
37Then by the peeling process from Bartlett et al. (2005), for any η > 0, we have
k
(cid:18) (cid:19)
P M ≥ 2(cid:112) η B + 16η kL φL θ
k,h k k,h
3
(cid:18) (cid:19)
= P M ≥ 2(cid:112) η B + 16η kL φL θ , 4 < B ≤ 8kUL2L2
k,h k k,h 3 kU k,h φ θ
(cid:18) (cid:19)
= P M ≥ 2(cid:112) η B + 16η kL φL θ , 4 < B ≤ 8kUL2L2,Σ ≤ B
k,h k k,h 3 kU k,h φ θ k,h k,h
≤
(cid:88)m P(cid:18)
M ≥
2(cid:112)
η B +
16η kL φL θ
,
4·2j−1
< B ≤
4·2j
,Σ ≤ B
(cid:19)
k,h k k,h k,h k,h k,h
3 kU kU
j=1
≤
(cid:88)m P(cid:32)
M ≥
(cid:114)
η
8·2j
+
16η kL φL
θ
,Σ ≤
4·2j(cid:33)
, (23)
k,h k k,h
kU 3 kU
j=1
(cid:124) (cid:123)(cid:122) (cid:125)
Ij
where m = 1+⌈2log kUL L ⌉. For I , note that from (20) we have
2 φ θ j
(cid:12) (cid:12)
(cid:12)(cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗)(cid:12) ≤ 8L L .
(cid:12) i,h h i,h h h h (cid:12) φ θ
By Freedman’s inequality (Lemma 29), we have
(cid:32) (cid:114) (cid:33)
8·2j 16η L L 4·2j
P M ≥ η + k φ θ ,Σ ≤
k,h k k,h
kU 3 kU
 (cid:18)(cid:113) (cid:19)2 
− η 8·2j + 16η kLφL θ
 k kU 3 
≤ exp 
 (cid:18)(cid:113) (cid:19)
8·2j + 2 ·8L L η 8·2j + 16η kLφL θ 
kU 3 φ θ k kU 3
 (cid:18)(cid:113) √ (cid:19)2 
−η 8·2j + 16 η kLφL θ
 k kU 3 
= exp 
 8·2j + 16LφL θ(cid:113) η 8·2j + 162η kL2 φL2 θ  
kU 3 k kU 32
 (cid:18)(cid:113) √ (cid:19)2 
−η 8·2j + 16 η kLφL θ
 k kU 3 
≤ exp 
 8·2j + 32LφL θ(cid:113) η 8·2j + 162η kL2 φL2 θ  
kU 3 k kU 32
= exp(−η ). (24)
k
By substituting Eq. (24) into Eq. (23), we have
(cid:18) (cid:19)
P M ≥ 2(cid:112) η B + 16η kL φL θ ≤ mexp(−η ).
k,h k k,h k
3
38Then,combiningwiththeresultofCase1&2,lettingη = log m = log (1+⌈2log 2kULφL θ⌉)k2
k δ/k2 δ
and taking union bound over k, with probability at least 1−δ, we have
(cid:118)
M ≤ 2(cid:117) (cid:117) (cid:116)2η (cid:88)k ∥θi −θ∗∥2 + 16η kL φL θ +√ 2. (25)
k,h k h h W i,h 3
i=1
√
By applying 2 ab ≤ a+b to the first term on the right hand side, we have
(cid:118)
(cid:117) k k
2(cid:117) (cid:116)2η (cid:88) ∥θi −θ∗∥2 ≤ 8η k + κ (cid:88) ∥θi −θ∗∥2 . (26)
k h h W i,h κ 4 h h W i,h
i=1 i=1
Combining the results of Eq. (25) & Eq. (26), we have
k
M = (cid:88)(cid:0) G¯ (θi)−G (θi)(cid:1)⊤ (θi −θ∗)
k,h i,h h i,h h h h
i=1
≤
κ (cid:88)k
∥θi −θ∗∥2
+(cid:18) 16L φL
θ +
8(cid:19)
log
(1+⌈2log 2kUL φL θ⌉)k2 +√
2.
4 h h W i,h 3 κ δ
i=1
B.1.2 Proof of Lemma 3
Proof of Lemma 3. By definition of V , we have
t
(cid:32) (cid:33)
(cid:88)
det(V ) = det V + x x⊤
t+1 t t,i t,i
i∈St
(cid:32) (cid:33)
= det(V )det I +
(cid:88) V−1
2x
x⊤V−1
2
t d t t,i t,i t
i∈St
(cid:32) (cid:33)
(cid:88)
= det(V ) 1+ ∥x ∥2
t t,i V−1
t
i∈St
t (cid:32) (cid:33)
(cid:89) (cid:88)
= det(λI ) 1+ ∥x ∥2
d τ,i Vτ−1
τ=1 i∈Sτ
t (cid:18) (cid:19)
(cid:89)
≥ det(λI ) 1+max∥x ∥2 . (27)
d i∈Sτ τ,i V t−1
τ=1
Since λ ≥ L2, we have
L2
max∥x ∥2 ≤ ≤ 1.
i∈Sτ τ,i Vτ−1 λ
39Since for any z ∈ [0,1], it follows that z ≤ 2log(1+z). Hence, we have
T T (cid:18) (cid:19)
(cid:88) (cid:88)
max∥x ∥2 ≤ 2 log 1+max∥x ∥2
i∈St t,i V t−1 i∈St t,i V t−1
t=1 t=1
T (cid:18) (cid:19)
(cid:89)
= 2log 1+max∥x ∥2
i∈St t,i V t−1
t=1
det(V )
T+1
≤ 2log
det(λI )
d
(cid:18) TKL2(cid:19)
≤ 2dlog 1+ ,
dλ
where the second inequality comes from Eq. (27) and the last inequality follows by the
determinant-trace inequality (Lemma 28).
B.2 Bound on Prediction Error
Inthissection,weprovidetheboundonthepredictionerrorinducedbyestimatedtransition
core θk.
h
Lemma 4 (Bound on Prediction Error). For any δ ∈ (0,1), suppose that Lemma 1 holds.
Then for any (s,a) ∈ S ×A, we have
|∆k(s,a)| ≤ Hα (δ)∥φˆ (s,a)∥ .
h k k,h A−1
k,h
Proof of Lemma 4. Recall that
(cid:88) (cid:16) (cid:17)
∆k h(s,a) = P θk h(s′ | s,a)−P θ∗ h(s′ | s,a) V hk +1(s′)
s′∈Ss,a
exp(φ⊤ θk)Vk (s′) exp(φ⊤ θ∗)Vk (s′)
(cid:88) s,a,s′ h h+1 (cid:88) s,a,s′ h h+1
= − .
(cid:80) exp(φ⊤ θk) (cid:80) exp(φ⊤ θ∗)
s′∈Ss,a s (cid:101)∈Ss,a s,a,s (cid:101) h s′∈Ss,a s (cid:101)∈Ss,a s,a,s (cid:101) h
40Then by the mean value theorem, there exists θ¯ = ρθk +(1−ρ)θ∗ for some ρ ∈ [0,1]
h h
satisfying that
(cid:16) (cid:17)(cid:16) (cid:17)
(cid:80) exp(φ⊤ θ¯)Vk (s′)φ⊤ (θk −θ∗) (cid:80) exp(φ⊤ θ¯)
∆k(s,a) =
s′∈Ss,a s,a,s′ h+1 s,a,s′ h h s (cid:101)∈Ss,a s,a,s
(cid:101)
h (cid:16) (cid:17)2
(cid:80) exp(φ⊤ θ¯)
s (cid:101)∈Ss,a s,a,s
(cid:101)
(cid:16) (cid:17)(cid:16) (cid:17)
(cid:80) exp(φ⊤ θ¯)Vk (s′) (cid:80) exp(φ⊤ θ¯)φ⊤ (θk −θ∗)
−
s′∈Ss,a s,a,s′ h+1 s (cid:101)∈Ss,a s,a,s
(cid:101)
s,a,s
(cid:101)
h h
(cid:16) (cid:17)2
(cid:80) exp(φ⊤ θ¯)
s (cid:101)∈Ss,a s,a,s
(cid:101)
(cid:88)
= P (s′ | s,a)Vk (s′)φ⊤ (θk −θ∗)
θ¯ h+1 s,a,s′ h h
s′∈Ss,a
(cid:32)(cid:80) exp(φ⊤ θ¯)Vk (s′)(cid:33)
− s′∈Ss,a s,a,s′ h+1 (cid:88) P (s′ | s,a)φ⊤ (θk −θ∗)
(cid:80) exp(φ⊤ θ¯) θ¯ s,a,s′ h h
s (cid:101)∈S k,h s,a,s (cid:101) s′∈Ss,a
(cid:32) (cid:80) exp(φ⊤ θ¯)Vk (s′)(cid:33)
= (cid:88) Vk (s′)− s′∈Ss,a s,a,s′ h+1 P (s′ | s,a)φ⊤ (θk −θ∗).
h+1 (cid:80) exp(φ⊤ θ¯) θ¯ s,a,s′ h h
s′∈Ss,a s (cid:101)∈Ss,a s,a,s (cid:101)
Since Vk(s′) ≤ H for all s′ ∈ S,k ∈ [K], and h ∈ [H], we have
h
(cid:88)
∆k(s,a) ≤ H P (s′ | s,a)φ⊤ (θk −θ∗)
h θ¯ s,a,s′ h h
s′∈Ss,a
≤ H max |φ⊤ (θk −θ∗)|
s,a,s′ h h
s′∈Ss,a
≤ H max ∥φ ∥ ∥θk −θ∗∥
s′∈Ss,a s,a,s′ A− k,1 h h h A k,h
≤ Hα (δ)∥φˆ (s,a)∥ ,
k k,h A−1
k,h
where the second inequality comes from the fact that P (s′ | s,a) ≤ 1 is a multinomial
θ¯
probability, the third inequality holds due to the Cauchy-Schwarz inequality, and the last
inequality follows from Lemma 1 and the definition of φˆ , i.e., φˆ (s,a) := φ(s,a,sˆ) for
k,h k,h
sˆ= argmax ∥φ(s,a,s′)∥ .
s′∈Ss,a A− k,1
h
B.3 Good Events with High Probability
Lemma 5 (Good event probability). For any K ∈ N and δ ∈ (0,1), the good event G(K,δ′)
holds with probability at least 1−δ where δ′ = δ/(2KH).
Proof of Lemma 5. For any δ′ ∈ (0,1), we have
G(K,δ′) = (cid:92) (cid:92) G (δ′) = (cid:92) (cid:92) (cid:110) G∆ (δ′)∩Gξ (δ′)(cid:111) .
k,h k,h k,h
k≤Kh≤H k≤Kh≤H
On the other hand, for any (k,h) ∈ [K]×[H], by Lemma 30, Gξ (δ′) holds with probability
k,h
at least 1−δ′. Then, for δ′ = δ/(2KH) by taking union bound, we have the desired result
as follows:
P(G(K,δ′)) ≥ (1−δ′)2KH ≥ 1−2KHδ′ = 1−δ.
41B.4 Stochastic Optimism
Lemma 6 (Stochastic optimism). For any δ with 0 < δ < Φ(−1)/2, let σ = Hα (δ) =
√ k k
O(cid:101)(H d). If we take multiple sample size M = ⌈1− logH ⌉, then for any k ∈ [K], we have
logΦ(1)
(cid:16) (cid:17)
P (Vk −V∗)(sk) ≥ 0 | sk,F ≥ Φ(−1)/2.
1 1 1 1 k
Proof of Lemma 6. Before presenting the proof, we introduce the following lemmas.
Lemma 7. For any k ∈ [K], it holds
(cid:34) H (cid:35)
(cid:88)
Vk(sk)−V∗(sk) ≥ E −ιk(x ,a ) | x = sk ,
1 1 1 1 π∗ h h h 1 1
h=1
where ιk(s,a) := r(s,a)+P Vk (s,a)−Qk(s,a).
h h h+1 h
Lemma 8. Let δ ∈ (0,1) be given. For any (k,h) ∈ [K]×[H], let σ = Hα (δ). If we
k k
define the event G∆ (δ) as
k,h
(cid:110) (cid:111)
G∆ (δ) := ∆k(s,a) ≤ Hα (δ)∥φˆ (s,a)∥ ,
k,h h k k,h A−1
k,h
then conditioned on G∆ (δ), for any (s,a) ∈ S ×A, we have
k,h
(cid:16) (cid:17)
P −ιk(s,a) ≥ 0 | G∆ (δ) ≥ 1−Φ(1)M .
h k,h
Lemma 9. Let δ ∈ (0,1) be given. For any (h,k) ∈ [H] × [K], let σ = Hα (δ). If
k k
we take multiple sample size M = ⌈1− logH ⌉, then conditioned on the event G∆(δ) :=
logΦ(1) k
(cid:84) G∆ (δ), we have
h∈[H] k,h
(cid:16) (cid:17)
P −ιk(s ,a ) ≥ 0,∀h ∈ [H] | G∆(δ) ≥ Φ(−1).
h h h k
Now, we define the event of the estimated value function being optimistic at the start
of the k-th episode as
(cid:110) (cid:111)
X := (Vk −V∗)(sk) ≥ 0 .
k 1 1 1
Then for the event G (δ) =: G , we have
k k
P(X ) = 1−P(Xc)
k k
= 1−P(Xc∩G )−P(Xc∩Gc)
k k k k
≥ 1−P(Xc∩G )−P(Gc)
k k k
≥ 1−P(Xc∩G )−δ
k k
where the last inequality comes from Lemma 5.
42On the other hand, by Lemma 7, we have
(cid:34) H (cid:35)
(cid:88)
Vk(sk)−V∗(sk) ≥ E −ιk(x ,a ) | x = sk
1 1 1 1 π∗ h h h 1 1
h=1
H
(cid:88) (cid:104) (cid:105)
= E −ιk(x ,a ) | x = sk .
π∗ h h h 1 1
h=1
If we define an event
(cid:40) H (cid:41)
(cid:88) (cid:104) (cid:105)
Y = E −ιk(x ,a ) | x = sk ≥ 0 ,
k π∗ h h h 1 1
h=1
then, by Lemma 9, we have
P(Y | G ) ≥ Φ(−1) ⇐⇒ P(Yc | G ) ≤ 1−Φ(−1)
k k k k
=⇒ P(Yc∩G ) ≤ (1−Φ(−1))P(G ) ≤ 1−Φ(−1)
k k k
Note that since Xc∩G ⊂ Yc∩G , we can conclude that
k k k k
P(X ) ≥ 1−P(Xc∩G )−δ
k k k
≥ 1−P(Yc∩G )−δ
k k
≥ 1−(1−Φ(−1))−δ
= Φ(−1)−δ
≥ Φ(−1)/2
where the last inequality comes from the choice of δ.
In the following, we provide all the proofs of the lemmas used to prove Lemma 6.
B.4.1 Proof of Lemma 7
Proof of Lemma 7. Inthisproof,weusexk asthestatessampledundertheπ∗ todistinguish
h
with sk. Since we have,
h
Vk(sk)−V∗(sk)
1 1 1 1
≥ Qk(sk,π∗(sk))−Q∗(sk,π∗(sk))
1 1 1 1 1 1
(cid:16) (cid:17)
= r(sk,π∗(sk))+P Vk(sk,π∗(sk))−ιk(sk,π∗(sk))− r(sk,π∗(sk))+P V∗(sk,π∗(sk))
1 1 1 2 1 1 1 1 1 1 1 1 2 1 1
= P (Vk −V∗)(sk,π∗(sk))−ιk(sk,π∗(sk))
1 2 2 1 1 1 1 1
(cid:104) (cid:105)
= E (Vk −V∗)(x) −ιk(sk,π∗(sk))
x|sk,π∗(sk) 2 2 1 1 1
1 1
(cid:104) (cid:105)
≥ E (Qk −Q∗)(xk,π∗(xk)) −ιk(sk,π∗(sk))
xk|sk,π∗(sk) 2 2 2 2 1 1 1
2 1 1
(cid:104) (cid:104) (cid:105) (cid:105)
= E E (Vk −V∗)(x) −ιk(xk,π∗(xk)) −ιk(sk,π∗(sk))
xk∼sk,π∗(sk) x|xk,π∗(xk) 3 3 2 2 2 1 1 1
2 1 1 2 2
(cid:104) (cid:104) (cid:105)(cid:105) (cid:104) (cid:105)
= E E (Vk −V∗)(x) −E ιk(xk,π∗(xk)) −ιk(sk,π∗(sk))
xk∼sk,π∗(sk) x|xk,π∗(xk) 3 3 xk∼sk,π∗(sk) 2 2 2 1 1 1
2 1 1 2 2 2 1 1
(cid:124) (cid:123)(cid:122) (cid:125)
E [(Vk−V∗)(xk)]
xk 3∼π∗|sk
1
3 3 3
43then by applying this argument recursively, we finally have
(cid:34) H (cid:35)
(cid:88)
Vk(sk)−V∗(sk) ≥ E −ιk(x ,a ) | x = sk .
1 1 1 1 π∗ h h h 1 1
h=1
B.4.2 Proof of Lemma 8
Proof of Lemma 8. Since we have
(cid:16) (cid:17)
−ιk(s,a) = Qk(s,a)− r(s,a)+P Vk (s,a)
h h h h+1
 
= min r(s,a)+ (cid:88) P (s′ | s,a)Vk (s′)+ max φˆ (s,a)⊤ξ(m) ,H
θk h+1 k,h k,h
 h m∈[M] 
s′∈Ss,a
(cid:16) (cid:17)
− r(s,a)+P Vk (s,a)
h h+1
 
≥ min (cid:88) P (s′ | s,a)Vk (s′)+ max φˆ (s,a)⊤ξ(m) −P Vk (s,a),0 ,
θk h+1 k,h k,h h h+1
 h m∈[M] 
s′∈Ss,a
it is enough to show that
(cid:88) P (s′ | s,a)Vk (s′)+ max φˆ (s,a)⊤ξ(m) −P Vk (s,a) ≥ 0
θk h+1 k,h k,h h h+1
h m∈[M]
s′∈Ss,a
at least with constant probability.
On the other hand, under the event G (δ), by Lemma 4 we have
k,h
(cid:88) P (s′ | s,a)Vk (s′)+ max φˆ (s,a)⊤ξ(m) −P Vk (s,a)
θk h+1 k,h k,h h h+1
h m∈[M]
s′∈Ss,a
≥ max φˆ
(s,a)⊤ξ(m)
−Hα (δ)∥φˆ (s,a)∥ .
k,h k,h k k,h A−1
m∈[M] k,h
Now, for ∀m ∈ [M], since ξ(m) ∼ N(0 ,σ2A−1), we have
k,h d k k,h
φˆ (s,a)⊤ξ(m) ∼ N(0,σ2∥φˆ (s,a)∥2 ),
k,h k,h k k,h A−1
k,h
which means,
(cid:16) (cid:17)
P φˆ (s,a)⊤ξ(m) ≥ Hα (δ)∥φˆ (s,a)∥ ≥ Φ(−1),
k,h k,h k k,h A−1
k,h
44by setting σ = Hα (δ). Then, finally we have the desired results as follows:
k k
(cid:16) (cid:17)
P −ιk(s,a) ≥ 0 | G∆ (δ)
h k,h
(cid:18) (cid:19)
≥ P max φˆ (s,a)⊤ξ(m) ≥ Hα (δ)∥φˆ (s,a)∥ | G∆ (δ)
k,h k,h k k,h A−1 k,h
m∈[M] k,h
(cid:16) (cid:17)
= 1−P φˆ (s,a)⊤ξ(m) < Hα (δ)∥φˆ (s,a)∥ ,∀m ∈ [M] | G∆ (δ)
k,h k,h k k,h A−1 k,h
k,h
≥ 1−(1−Φ(−1))M
= 1−Φ(1)M .
B.4.3 Proof of Lemma 9
Proof of Lemma 9. For each h ∈ [H] and k ∈ [K], define an event Ek := {−ιk(s ,a ) ≥ 0}
h h h h
Then it holds
(cid:32) H (cid:33)
(cid:16) (cid:17) (cid:92)
P −ιk(s ,a ) ≥ 0,∀h ∈ [H] | G∆(δ) = P Ek | G∆(δ)
h h h k h k
h=1
(cid:32) H (cid:33)
(cid:91)
= 1−P (Ek)c | G∆(δ)
h k
h=1
H
(cid:88) (cid:16) (cid:17)
≥ 1− P (Ek)c | G∆ (δ)
h k,h
h=1
≥ 1−HΦ(1)M
≥ Φ(−1)
where the first inequality uses the union bound, the second inequality comes from the
Lemma 8 and the last inequality holds due to the choice of M = ⌈1− logH ⌉.
logΦ(1)
B.5 Bound on Estimation Part
We decompose the regret into the estimation part and the pessimism part as follows:
K K
(cid:88) (V∗−Vπk )(sk) = (cid:88)(cid:16) V∗−Vk+Vk −Vπk(cid:17) (sk),
1 1 1 1 1 1 1 1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
k=1 k=1
Pessimism Estimation
and we bound these two parts in the following sections, respectively.
Lemma 10 (Boundonestimationpart). For any δ ∈ (0,1), if λ ≥ L2, then with probability
φ
at least 1−δ/2, we have
(cid:88)K
(V 1k −V 1πk )(sk 1) =
O(cid:101)(cid:16)
κ−1d3 2H3
2√ T(cid:17)
.
k=1
45Proof of Lemma 10. For any given k ∈ [K],
(Vk −Vπk )(sk) = (Qk −Qπk )(sk,ak)+ιk(sk,ak)−ιk(sk,ak)
1 1 1 1 1 1 1 1 1 1 1 1 1
= (Qk −Qπk )(sk,ak)+P (Vk −Vπk )(sk,ak)+(Qπk −Qk)(sk,ak)−ιk(sk,ak)
1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1
= P (Vk −Vπk )(sk,ak)−(Vk −Vπk )(sk)+(Vk −Vπk )(sk)−ιk(sk,ak)
1 2 2 1 1 2 2 2 2 2 2 1 1 1
(cid:124) (cid:123)(cid:122) (cid:125)
ζ˙k
1
where the second equality holds due to the variant of ιk(sk,ak) as follows:
h h h
ιk(sk,ak) = r(sk,ak)+P Vk (sk,ak)−Qk(sk,ak)+Qπk (sk,ak)−Qπk (sk,ak)
h h h h h h h+1 h h h h h h h h h h h
(cid:16) (cid:17)
= r(sk,ak)+P Vk (sk,ak)−Qk(sk,ak)+Qπk (sk,ak)− r(sk,ak)+P Vπk (sk,ak)
h h h h+1 h h h h h h h h h h h h+1 h h
= P (Vk −Vπk )(sk,ak)+(Qπk −Qk)(sk,ak).
h h+1 h+1 h h h h h h
Then, by applying this argument recursively for whole horizon, we have
H H
(Vk −Vπk )(sk) = (cid:88) −ιk(sk,ak)+(cid:88) ζ˙k, (28)
1 1 1 h h h h
h=1 h=1
where ζ˙k := P (Vk −Vπk )(sk,ak)−(Vk −Vπk )(sk ).
h h h+1 h+1 h h h+1 h+1 h+1
Letδ′ = δ/(8KH). ByLemma5, thegoodeventG(K,δ′)holdswithprobabilityatleast
1−δ/4. Then under the event G(K,δ′), for any h ∈ [H] we have
(cid:16) (cid:17)
−ιk(sk,ak) = Qk(sk,ak)− r(sk,ak)+P Vk (sk,ak)
h h h h h h h h h h+1 h h
 
= min r(sk,ak)+ (cid:88) P (s′ | sk,ak)Vk (s′)+ max φˆ (sk,ak)⊤ξ(m) ,H
h h θk h h h+1 k,h h h k,h
 h m∈[M] 
s′∈S
k,h
(cid:16) (cid:17)
− r(sk,ak)+P Vk (sk,ak)
h h h h+1 h h
≤ (cid:88) P (s′ | sk,ak)Vk (s′)+ max φˆ (sk,ak)⊤ξ(m) −P Vk (sk,ak)
θk h h h+1 k,h h h k,h h h+1 h h
h m∈[M]
s′∈S
k,h
(cid:12) (cid:12)
(cid:12) (cid:12)
≤ (cid:12) (cid:12)
(cid:12)
(cid:88) P θk h(s′ | sk h,ak h)V hk +1(s′)−P hV hk +1(sk h,ak h)(cid:12) (cid:12) (cid:12)+ mm ∈a [Mx ](cid:12) (cid:12) (cid:12)φˆ k,h(sk h,ak h)⊤ξ k(m ,h)(cid:12) (cid:12) (cid:12)
(cid:12)s′∈S (cid:12)
k,h
≤ |∆k(sk,ak)|+ max ∥φˆ (sk,ak)∥ ∥ξ(m) ∥ (29)
h h h k,h h h A−1 k,h A k,h
m∈[M] k,h
≤ (cid:0) Hα (δ′)+γ (δ′)(cid:1) ∥φˆ (sk,ak)∥ , (30)
k k k,h h h A−1
k,h
where (29) comes from the Cauchy-Schwarz inequality and (30) holds due the the Lemma 4
& 30. Then, with probability at least 1−δ/4, we have
H H
(cid:88) −ιk(sk,ak) ≤ (cid:88)(cid:0) Hα (δ′)+γ (δ′)(cid:1) ∥φˆ (sk,ak)∥ . (31)
h h h k k k,h h h A−1
k,h
h=1 h=1
46On the other hand, for ζ˙k, we have |ζ˙k| ≤ 2H and E[ζ˙k | F ] = 0, which means
h h h k,h
{ζ˙k | F } is a martingale difference sequence for any k ∈ [K] and h ∈ [H]. Hence, by
h k,h k,h
applying the Azuma-Hoeffding inequality with probability at least 1−δ/4, we have
K H
(cid:88)(cid:88) ζ˙k ≤ 2H(cid:112) 2KHlog(4/δ). (32)
h
k=1h=1
Combining the results of (31) and (32), with probability at least 1−δ/2, we have
(Vk −Vπk )(sk)
1 1 1
K H
≤ 2H(cid:112) 2T log(4/δ)+(cid:88)(cid:88)(cid:0) Hα (δ′)+γ (δ′)(cid:1) ∥φˆ (sk,ak)∥
k k k,h h h A−1
k,h
k=1h=1
K H
≤ 2H(cid:112) 2T log(4/δ)+(cid:0) Hα (δ′)+γ (δ′)(cid:1)(cid:88)(cid:88) ∥φˆ (sk,ak)∥ (33)
K K k,h h h A−1
k,h
k=1h=1
(cid:118)
H (cid:117) K
≤ 2H(cid:112) 2T log(4/δ)+(cid:0) Hα (δ′)+γ (δ′)(cid:1)(cid:88)(cid:117) (cid:116)K(cid:88) ∥φˆ (sk,ak)∥2 (34)
K K k,h h h A−1
k,h
h=1 k=1
(cid:115)
≤ 2H(cid:112) 2T log(4/δ)+(cid:0) Hα (δ′)+γ
(δ′)(cid:1)(cid:88)H 4κ−1Kdlog(cid:18)
1+
KUL2 φ(cid:19)
(35)
K K
dλ
h=1
(cid:115)
(cid:18) KUL2 (cid:19)
= 2H(cid:112) 2T log(4/δ)+(cid:0) Hα (δ′)+γ (δ′)(cid:1) 4κ−1THdlog 1+ φ ,
K K
dλ
(cid:16) √ √ (cid:17)
= O(cid:101)
κ−1d3 2H3
2 T +H T ,
where (33) follows from the fact that both α (δ) and γ (δ) are increasing in k, (34) comes
k k
fromCauchy-Schwarzinequalityand(35)holdsbythegeneralizedellipticalpotentiallemma
(Lemma 3).
B.6 Bound on Pessimism Part
Lemma 11 (Bound on pessimism). For any δ with 0 < δ < Φ(−1)/2, let σ = Hα (δ). If
k k
λ ≥ L2 and we take multiple sample size M = ⌈1− logH ⌉, then with probability at least
φ logΦ(1)
1−δ/2, we have
(cid:88)K
(V 1∗−V 1k)(sk 1) =
O(cid:101)(cid:16)
κ−1d23 H3
2√ T(cid:17)
.
k=1
Proof of Lemma 11. Similar to the techniques used in (Zanette et al., 2020), we show that
the difference between the optimal value function V∗ and the estimated value function
1
Vk can be controlled by constructing an upper bound on V∗ and a lower bound on Vk.
1 1 1
In this proof, we consider three kinds of pseudo-noises, ξ,ξ¯ and ξ that we define later
in the proof. Also, for δ′ = δ/10, we denote G(K,δ′),G¯(K,δ′) and G(K,δ′) as the good
events induced by ξ,ξ¯ and ξ respectively. From now on, we denote G(K,δ′) by the event
47G(K,δ′) ∩ G¯(K,δ′) ∩ G(K,δ′). Then, by Lemma 5, the event G(K,δ′) holds with high
probability at least 1−3δ/10.
(m)
First,weconstructthelowerboundofVk. Foranygivenk ∈ [K],let(cid:101)ξ := {(cid:101)ξ } ⊂
1 k,h m∈[M]
Rd be a set of vectors for h ∈ [H] and Vk(·;(cid:101)ξ) be the value function obtained by the Algo-
h
(m) (m)
rithm 1 with non-random (cid:101)ξ in place of ξ . Then consider the following minimization
k,h k,h
problem:
min Vk(sk;(cid:101)ξ)
1 1
(m)
{(cid:101)ξ }
k,h h∈[H],m∈[M]
(m)
s.t. max ∥(cid:101)ξ k,h∥
A
k,h
≤ γ k(δ), ∀h ∈ [H]
m∈[M]
And we denote ξ := {ξ(m) } by a minimizer and Vk(sk) by the minimum of the
k,h h∈[H],m∈[M] 1 1
above minimization problem, i.e., Vk(·) := Vk(·;ξ). Then, under the event G(K,δ′), since
h h
(m)
{ξ } is also a feasible solution of the above optimization problem, and since
k,h h∈[H],m∈[M]
Vk = Vk(;ξ), thus we have
h h
Vk(sk) ≤ Vk(sk). (36)
1 1 1 1
Second, to find an upper bound for V∗, considering i.i.d copies {ξ¯(m) } of
k,h h∈[H],m∈[M]
{ξ(m) } and run Algorithm 1 to get a corresponding value function V¯k and Q¯k
k,h h∈[H],m∈[M] h h
for all h ∈ [H]. Define the event that V¯k(sk) is optimistic in the k-th episode as
1 1
X¯ = {(V¯k −V∗)(sk) ≥ 0}.
k 1 1 1
Then by Lemma 6, for given δ, we have
P(X¯ | sk,F ) ≥ Φ(−1)/2.
k 1 k
Then by the definition of optimism, under the event G(K,δ′), we have
(cid:104) (cid:105)
(V∗−Vk)(sk) ≤ E (V¯k −Vk)(sk)
1 1 1 ξ¯|X¯ 1 1 1
k
(cid:104) (cid:105)
≤ E (V¯k −Vk)(sk) , (37)
ξ¯|X¯ 1 1 1
k
wheretheexpectationsareovertheξ¯’sconditionedontheeventX¯ andthesecondinequal-
k
ity comes from (36). On the other hand, under the event G¯(K,δ′) by the law of the total
expectation, we have
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
E (V¯k −Vk)(sk) = E (V¯k −Vk)(sk) P(X¯ )+E (V¯k −Vk)(sk) P(X¯c)
ξ¯ 1 1 1 ξ¯|X¯ 1 1 1 k ξ¯|X¯c 1 1 1 k
k k
(cid:104) (cid:105)
≥ E (V¯k −Vk)(sk) P(X¯ ), (38)
ξ¯|X¯ 1 1 1 k
k
where(38)comesfromthefactthat{ξ¯(m)
} isalsoafeasiblesolutionoftheabove
k,h h∈[H],m∈[M]
optimization problem under the event G¯(K,δ′), i.e., V¯k(sk) ≥ Vk(sk). Then, by combining
1 1 1 1
48the results of (38) and (37), under the event G(K,δ′), we have
(cid:104) (cid:105)
(V∗−Vk)(sk) ≤ E (V¯k −Vk)(sk)
1 1 1 ξ¯|X¯ 1 1 1
k
(cid:104) (cid:105)
≤ E (V¯k −Vk)(sk) /P(X¯ )
ξ¯ 1 1 1 k
2 (cid:104) (cid:105)
≤ E (V¯k −Vk +Vk −Vk)(sk)
Φ(−1) ξ¯ 1 1 1 1 1
2 (cid:16) (cid:17)
= (Vk −Vk)(sk) +ζ¨ , (39)
Φ(−1) 1 1 1 k
where we denote
2 (cid:16) (cid:104) (cid:105) (cid:17)
ζ¨ := E V¯k(sk) −Vk(sk) .
k Φ(−1) ξ¯ 1 1 1 1
Note that since ξ¯ is the i.i.d copy of ξ, therefore V¯ and V are independent, which
k,1 k,1
means {ζ¨ | F }K is a martingale difference sequence with |ζ¨ | ≤ 2H . Therefore by
k k−1 k=1 k Φ(−1)
applying Azuma-Hoeffiding inequality under the event G(K,δ′), with probability at least
1−δ′, we have
K
(cid:88) ζ¨ ≤ 2H (cid:112) 2Klog(1/δ′). (40)
k
Φ(−1)
k=1
On the other hand, by dividing the first term in (39) into two terms we have
(Vk −Vk)(sk) = (Vk −Vπk )(sk)+(Vπk −Vk)(sk) .
1 1 1 1 1 1 1 1 1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I1 I2
For I , note that since it is related to the estimation error, under the event G(K,δ′) we
1
can bound the sum of I for the total episode number using Lemma 10 as follows:
1
(cid:115)
(cid:88)K
(Vk−Vπk )(sk) ≤ (cid:0) Hα (δ′)+γ (δ′)(cid:1)
4κ−1THdlog(cid:18)
1+
KUL2 φ(cid:19)
+2H(cid:112) 2T log(1/δ′).
1 1 1 K K dλ
k=1
(41)
For I , since we have
2
I = Qπk (sk,ak)−Vk(sk)
2 1 1 1 1 1
≤ Qπk (sk,ak)−Qk(sk,ak) (42)
1 1 1 1 1 1
= Qπk (sk,ak)−Qk(sk,ak)−ιk(sk,ak)+ιk(sk,ak)
1 1 1 1 1 1 1 1 1 1 1 1
= P (Vπk −Vk)(sk,ak)+ιk(sk,ak) (43)
1 2 2 1 1 1 1 1
= P (Vπk −Vk)(sk,ak)−(Vπk −Vk)(sk)+(Vπk −Vk)(sk)+ιk(sk,ak)
1 2 2 1 1 2 2 2 2 2 2 1 1 1
(cid:124) .(cid:123).(cid:122). (cid:125)
ζ k
1
49where (42) comes from ak = argmax Qk(sk,a) and (43) holds by the following definition
1 a 1 1
of ιk(sk,ak)
h h h
ιk(sk,ak) := r(sk,ak)+P Vk (sk,ak)−Qk(sk,ak)
h h h h h h h+1 h h h h h
= r(sk,ak)+P Vk (sk,ak)−Qk(sk,ak)+Qπk (sk,ak)−Qπk (sk,ak)
h h h h+1 h h h h h h h h h h h
(cid:16) (cid:17)
= r(sk,ak)+P Vk (sk,ak)−Qk(sk,ak)+Qπk (sk,ak)− r(sk,ak)+P Vπk (sk,ak)
h h h h+1 h h h h h h h h h h h h+1 h h
= P (Vk −Vπk )(sk,ak)+(Qπk −Qk)(sk,ak).
h h+1 h+1 h h h h h h
Then by applying the same argument recursively for the whole horizon, we have
H H ...
I ≤
(cid:88) ιk(sk,ak)+(cid:88)
ζ
k
,
2 h h h h
h=1 h=1
where we denote
...
ζ k := P (Vπk −Vk )(sk,ak)−(Vπk −Vk )(sk ).
h h h+1 h+1 h h h+1 h+1 h+1
(cid:110)... (cid:111) ...
k k
Note that ζ | F is a martingale difference sequence with |ζ | ≤ 2H. Then,
h k,h h
k,h
under the event G(K,δ′) by applying the Azuma-Hoeffding inequality with probability at
least 1−δ′, we have
K H ...
(cid:88)(cid:88) k (cid:112)
ζ ≤ 2H 2T log(1/δ′). (44)
h
k=1h=1
To bound (cid:80)H ιk(sk,ak), we divide the whole horizon index set into two groups as follows:
h=1 h h h
 
 (cid:88) 
H+ = j ∈ [H] : r(sk,ak)+ P (s′ | sk,ak)Vk (s′)+ max φˆ (sk,ak)⊤ξ(m) > H
 j j θk h j j j+1 m∈[M] k,j j j k,j 
s′∈S
k,j
H− = [H]\H+.
Then, for j ∈ H+ since Qk(sk,ak) = H −j +1, Vk ≤ H −j and r(sk,ak) ≤ 1, we
j j j j+1 j j
have
ιk(sk,ak) = r(sk,ak)+P Vk (sk,ak)−(H −j +1) ≤ 0. (45)
j j j j j j j+1 j j
On the other hand, for j ∈ H−, under the event G(K,δ′) we have
(cid:88)
ιk(sk,ak) = P Vk (sk,ak)− P (s′ | sk,ak)Vk (s′)− max φˆ (sk,ak)⊤ξ(m)
j j j j j+1 j j θk h j j j+1 m∈[M] k,j j j k,j
s′∈S
k,j
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
≤ (cid:12) (cid:12) (cid:12)P jVk j+1(sk j,ak j)− (cid:88) P θk h(s′ | sk j,ak j)Vk j+1(s′)(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)mm ∈a [Mx ]φˆ k,j(sk j,ak j)⊤ξ( km ,j)(cid:12) (cid:12)
(cid:12)
(cid:12) s′∈S (cid:12)
k,j
(cid:12) (cid:12)
≤ Hα (δ′)∥φˆ (sk,ak)∥ + max (cid:12)φˆ (sk,ak)⊤ξ(m)(cid:12) (46)
k k,j j j A− k,1
j
m∈[M](cid:12) k,j j j k,j (cid:12)
≤ Hα (δ′)∥φˆ (sk,ak)∥ + max ∥φˆ (sk,ak)∥ ∥ξ(m)∥
k k,j j j A− k,1 j m∈[M] k,j j j A− k,1 j k,j A k,j
≤ (cid:0) Hα (δ′)+γ (δ′)(cid:1) ∥φˆ (sk,ak)∥ , (47)
k k k,j j j A−1
k,j
50where (46) holds by Lemma 4.
By combining the result of (45) and (47), we have
H ...
I ≤ (cid:88) (cid:0) Hα (δ′)+γ (δ′)(cid:1) ∥φˆ (sk,ak)∥ +(cid:88) ζ k
2 k k k,j j j A−1 h
k,j
j∈H− h=1
H H ...
≤ (cid:88)(cid:0) Hα (δ′)+γ (δ′)(cid:1) ∥φˆ (sk,ak)∥ +(cid:88) ζ k .
k k k,h h h A−1 h
k,h
h=1 h=1
Then summing I over the total number of episodes, under the event G(K,δ′), we have
2
K K H K H ...
(cid:88) (Vπk −Vk)(sk) ≤ (cid:88)(cid:88)(cid:0) Hα (δ′)+γ (δ′)(cid:1) ∥φˆ (sk,ak)∥ +(cid:88)(cid:88) ζ k
1 1 1 k k k,h h h A−1 h
k,h
k=1 k=1h=1 k=1h=1
K H K H ...
≤ (Hα (δ′)+γ (δ′))(cid:88)(cid:88) ∥φˆ (sk,ak)∥ +(cid:88)(cid:88) ζ k
K K k,h h h A−1 h
k,h
k=1h=1 k=1h=1
(cid:115)
(cid:18) KUL2 (cid:19)
≤ (cid:0) Hα (δ′)+γ (δ′)(cid:1) 4κ−1THdlog 1+ φ +2H(cid:112) 2T log(1/δ′),
K K
dλ
(48)
where the last inequality holds due to the Lemma 3 and (44).
Finally, by summing (39) over k and plugging the results of (41), (48) and (40) then,
we have
 (cid:115) 
(cid:88)K (V 1∗−V 1k)(sk 1) ≤ Φ(−4
1)
(cid:0) Hα K(δ′)+γ K(δ′)(cid:1) 4κ−1THdlog(cid:18) 1+ KU dλL2 φ(cid:19) +2H(cid:112) 2T log(1/δ′)
k=1
2H (cid:112)
+ 2Klog(1/δ′)
Φ(−1)
(cid:16) √ √ √ (cid:17)
≤ O(cid:101) κ−1d3/2H3/2 T +H T +H K .
To conclude the proof, by setting δ′ = δ/10 and we take a union bound over the two
...
applications of Azuma-Hoeffding (ζ¨ , ζ k ) and the event G(K,δ′), we get the desired result
k h
with probability at least 1−δ/2.
B.7 Regret Bound of RRL-MNL
Theorem (Restatement of Theorem 1). Suppose that Assumption 1- 4 hold. For any
√
0 < δ < Φ(− 21) , if we set the input parameters in Algorithm 1 as λ = L2 φ,σ
k
= O(cid:101)(H d)
and M = ⌈1− logH ⌉ where Φ is the normal CDF, then with probability at least 1−δ, the
logΦ(1)
cumulative regret of the RRL-MNL policy π is upper-bounded by
(cid:16) √ (cid:17)
Regret π(K) = O(cid:101)
κ−1d3 2H3
2 T .
51Proof of Theorem 1. Wecandecomposetheregretwithestimationpartandpessimismpart
as follows:
K
Regret (K) = (cid:88) (V∗−Vπk )(sk)
π 1 1 1
k=1
K K
= (cid:88) (V∗−Vk)(sk)+(cid:88) (Vk −Vπk )(sk).
1 1 1 1 1 1
k=1 k=1
Since both Lemma 10 and Lemma 11 holds with probability at least 1−δ/2 respectively,
by taking the union bound the following holds with probability at least 1−δ:
(cid:16) √ √ √ (cid:17) (cid:16) √ √ (cid:17)
Regret π(K) = O(cid:101)
κ−1d23 H3
2 T +H T +H K +O(cid:101)
κ−1d3 2H3
2 T +H T
(cid:16) √ (cid:17)
= O(cid:101)
κ−1d23 H3
2 T .
C. Detailed Regret Analysis for ORRL-MNL (Theorem 2)
C.1 Concentration of Estimated Transition Core θ(cid:101)k
h
In this section, we provide the detailed proof of Lemma 12, which demonstrates the con-
centration result for θ(cid:101)k independently of κ and U. Note that we adapt the proof provided
h
by Zhang and Sugiyama (2023) in the MNL contextual bandit setting to MNL-MDPs and
improve the result, making it independent of U. We provide the lemmas for the concentra-
tion of the online transition core for completeness, noting that there are slight differences
compared to their work, which stem from the different problem setting.
Lemma 12 (Concentration of online estimated transition core). Let η = O(logU) and
λ = O(dlogU). Then, for any δ ∈ (0,1] and for any h ∈ [H], we have
(cid:18) (cid:13) (cid:13) (cid:19)
P ∀k ≥ 1,(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
≤ β k(δ) ≥ 1−δ,
B
k,h
√
where β (δ) = O( dlogUlog(kH)).
k
Proof of Lemma 12. Recall that the transition core updated by the online mirror descent
is represented by
1 (cid:13) (cid:13)2
θ(cid:101)k h+1 = θa ∈rg Bm (Li θn )ℓ(cid:101)k,h(θ)+
2η
(cid:13) (cid:13)θ−θ(cid:101)k h(cid:13)
(cid:13) B
k,h,
(cid:13) (cid:13)
where ℓ(cid:101)k,h(θ) = ℓ k,h(θ(cid:101)k h)+(θ −θ(cid:101)k h)⊤∇ℓ k,h(θ(cid:101)k h)+ 1
2
(cid:13) (cid:13)θ−θ(cid:101)k h(cid:13)
(cid:13) ∇2ℓ k,h(θ(cid:101)k h)
. We introduce the
following lemma providing that the estimation error of the online estimator θ(cid:101)k can be
h
bounded by the regret.
52Lemma 13 (Lemma 12 in (Zhang and Sugiyama, 2023)). Let α = logU+2(1+L L ) and
θ φ
λ > 0. If we set the step size η = α/2, then we have
k
(cid:13) (cid:13)2 (cid:88)(cid:16) (cid:17)
(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
≤ α ℓ i,h(θ∗ h)−ℓ i,h(θ(cid:101)i h+1) +λL2
θ
B
k,h i=1
(49)
√ (cid:88)k (cid:13) (cid:13)2 (cid:88)k (cid:13) (cid:13)2
+3 2L3α (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13) − (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13) .
φ (cid:13) h h(cid:13) (cid:13) h h(cid:13)
2 B
i=1 i=1 i,h
Now, we bound the first term of (49). To simplify the presentation, for all (k,h) ∈
[K]×[H], we define the softmax function σ
k,h
: R|S k,h| → [0,1]|S k,h| as follows:
exp([z] )
s′
[σ (z)] = ,
k,h s′ (cid:80)
exp([z] )
s′′∈S s′′
k,h
where [·] denote the element corresponding to s′ ∈ S of the input vector. We also define
s′
the pseudo-inverse of the softmax function σ via [σ+ (p)] = log([p] ) which has the
k,h k,h s′ s′
(cid:16) (cid:17)
propertythatforallp ∈ ∆ , wehaveσ (σ+ (p)) = pand(cid:80) exp [σ+ (p)] =
|S k,h| k,h k,h s∈S k,h k,h s
1.
We denote Φ
k,h
= [φ k,h,s′]
s′∈S
k,h
∈ Rd×|S k,h| for simplicity. Then, the transition model
can also be written as P (s′ | sk,ak) = [σ (Φ⊤ θ∗)] . We further define
θ h h k,h k,h h s′
(cid:18) (cid:19)
z = σ+ E [σ (Φ⊤ θ)] for our analysis. Then, we have
(cid:101)i,h i,h θ∼N(θ(cid:101)i,cB−1) i,h i,h
h i,h
k k k
(cid:88)(cid:16) (cid:17) (cid:88)(cid:16) (cid:17) (cid:88)(cid:16) (cid:17)
ℓ i,h(θ∗ h)−ℓ i,h(θ(cid:101)i h+1) = ℓ i,h(θ∗ h)−ℓ( (cid:101)z i,h,y hi) + ℓ( (cid:101)z i,h,y hi)−ℓ i,h(θ(cid:101)i h+1) .
i=1 i=1 i=1
(50)
We can bound the first term of (50) by the following lemma.
Lemma 14. Let δ ∈ (0,1]. Then, for all (k,h) ∈ [K]×[H], with probability at least 1−δ,
we have
k
(cid:88)(cid:0) ℓ (θ∗)−ℓ(z ,yi)(cid:1) ≤ ΓA(δ),
i,h h (cid:101)i,h h k
i=1
(cid:16) √ (cid:17)
where ΓA(δ) = 5(3log(Uk)+L L )λ+4(3log(Uk)+L L )log H 1+2k +2.
k 4 φ θ φ θ δ
Furthermore, we can bound the second term of (50) by the following lemma.
Lemma 15. Let λ ≥ 72L2cd. Then, for any c > 0 and all (k,h) ∈ [K]×[H], we have
φ
k k
(cid:88)(cid:16) (cid:17) 1 (cid:88)(cid:13) (cid:13)2
ℓ( (cid:101)z i,h,y hi)−ℓ i,h(θ(cid:101)i h+1) ≤
2c
(cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i h(cid:13)
(cid:13) B
+ΓB k(δ).
i=1 i=1 i,h
where ΓB(δ) =
√ 6cdlog(cid:16)
1+
2kL2 φ(cid:17)
.
k dλ
53Combining Lemma 13, Lemma 14, and Lemma 15, and by setting η = α/2,c = 2α/3
√
and λ ≥ max{12 2L3α,48L2dα}, we derive that
φ φ
(cid:13) (cid:13)2
(cid:13)θ(cid:101)k+1−θ∗(cid:13)
(cid:13) h h(cid:13)
B
k,h
√ (cid:88)k (cid:13) (cid:13)2 (cid:16)α (cid:17)(cid:88)k (cid:13) (cid:13)2
≤ αΓA(δ)+αΓB(δ)+λL2 +3 2L3α (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13) + −1 (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13)
k k θ φ (cid:13) h h(cid:13) 2 2c (cid:13) h h(cid:13) B
i=1 i=1 i,h
≤ αΓA(δ)+αΓB(δ)+λL2
k k θ
√
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
H 1+2k k
≤ ClogU λlog(Uk)+log(Uk)log +dlog 1+ +λL2
δ dλ θ
=: β (δ)2 (51)
k
whereC > 0isanabsoluteconstant. Intheabove, wechooseλ = O(dlogU), α = O(logU).
The second inequality of (51) is derived from the fact that
√ (cid:88)k (cid:13) (cid:13)2 (cid:16)α (cid:17)(cid:88)k (cid:13) (cid:13)2
3 2L3α (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13) + −1 (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13)
φ (cid:13) h h(cid:13) 2 2c (cid:13) h h(cid:13) B
i=1 i=1 i,h
√ (cid:88)k (cid:13) (cid:13)2 1 (cid:88)k (cid:13) (cid:13)2
= 3 2L3α (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13) − (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13)
φ (cid:13) h h(cid:13) 2 4 (cid:13) h h(cid:13) B
i=1 i=1 i,h
√ (cid:88)k (cid:13) (cid:13)2 λ (cid:88)k (cid:13) (cid:13)2
≤ 3 2L3α (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13) − (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13)
φ (cid:13) h h(cid:13) 2 4 (cid:13) h h(cid:13) 2
i=1 i=1
≤ 0.
The first inequality holds from B ⪰ λI , and the second inequality is obvious from our
i,h d
setting of λ. Therefore, we can conclude that
(cid:13) (cid:13) √
(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
≤ β k(δ) = O( dlogUlog(kH)).
B
k,h
In the following section, we provide the proofs of the lemmas used in Lemma 12.
C.1.1 Proof of Lemma 13
(cid:16) (cid:17) (cid:13) (cid:13)2
Proof of Lemma 13. Let ℓ(cid:101)i,h(θ) = ℓ i,h(θ(cid:101)i h)+∇ℓ i,h(θ(cid:101)i h)⊤ θ−θ(cid:101)i
h
+ 1
2
(cid:13) (cid:13)θ−θ(cid:101)i h(cid:13)
(cid:13) ∇2ℓ i,h(θ(cid:101)i h)
be
a second-order Taylor expansion of ℓ i,h(θ) at θ(cid:101)i h. Since we have
1 (cid:13) (cid:13)2 1 (cid:13) (cid:13)2
θ(cid:101)k h+1 = θa ∈r Bg dm (Lin
θ) 2η
(cid:13) (cid:13)θ−θ(cid:101)k h(cid:13)
(cid:13) B(cid:101)k,h
+∇ℓ k,h(θ(cid:101)k h)⊤θ = θ∈a Brg (0m d,i Ln θ)ℓ(cid:101)k,h(θ)+
2η
(cid:13) (cid:13)θ−θ(cid:101)k h(cid:13)
(cid:13) B k,h
,
by Lemma 31, if we define ψ(θ) = 1∥θ∥2 we obtain
2 B i,h
1 (cid:18)(cid:13) (cid:13)2 (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:19)
∇ℓ(cid:101)i,h(θ(cid:101)i h+1)⊤(θ(cid:101)i h+1−θ∗ h) ≤
2η
(cid:13) (cid:13)θ(cid:101)i h−θ∗ h(cid:13)
(cid:13) B
−(cid:13) (cid:13)θ(cid:101)i h+1−θ∗ h(cid:13)
(cid:13) B
−(cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i h(cid:13)
(cid:13) B
.
i,h i,h i,h
(52)
54By applying Lemma 33, we have
(cid:68) (cid:69) 1 (cid:13) (cid:13)
ℓ i,h(θ(cid:101)i h+1)−ℓ i,h(θ∗ h) ≤ ∇ℓ i,h(θ(cid:101)i h+1),θ(cid:101)i h+1−θ∗
h
−
α i,h
(cid:13) (cid:13)θ(cid:101)i h+1−θ∗ h(cid:13)
(cid:13) ∇2ℓ i,h(θ(cid:101)i h+1
), (53)
where α = log|S |+2(1+L L ).
i,h i,h φ θ
By setting η = α /2 and merging equations (52) and (53), we arrive at
i,h
(cid:68) (cid:69)
ℓ i,h(θ(cid:101)i h+1)−ℓ i,h(θ∗ h) ≤ ∇ℓ i,h(θ(cid:101) hi+1)−∇ℓ(cid:101)i,h(θ(cid:101)i h+1),θ(cid:101)i h+1−θ∗
h
1 (cid:18)(cid:13) (cid:13)2 (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:19)
+ (cid:13)θ(cid:101)i −θ∗(cid:13) −(cid:13)θ(cid:101)i+1−θ∗(cid:13) −(cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13) .
α i,h (cid:13) h h(cid:13) B i,h (cid:13) h h(cid:13) B i+1,h (cid:13) h h(cid:13) B i,h
(54)
Meanwhile, we obtain
∇ℓ(cid:101)i,h(θ) = ∇ℓ i,h(θ(cid:101)i h)+∇2ℓ i,h(θ(cid:101)i h)(θ−θ(cid:101)i h) (55)
by taking the gradient over both sides of the Taylor approximation of ℓ (θ). Using (55),
i,h
we proceed to bound the first term of (54) as follows:
(cid:68) (cid:69)
∇ℓ i,h(θ(cid:101)i h+1)−∇ℓ(cid:101)i,h(θ(cid:101)i h+1),θ(cid:101)i h+1−θ∗
h
(cid:68) (cid:69)
= ∇ℓ i,h(θ(cid:101)i h+1)−∇ℓ i,h(θ(cid:101)i h)−∇2ℓ i,h(θ(cid:101)i h)(θ(cid:101)i h+1−θ(cid:101)i h),θ(cid:101)i h+1−θ∗
(cid:68) (cid:104) (cid:105) (cid:69)
= D3ℓ i,h(θ¯i h+1 ) θ(cid:101)i h+1−θ(cid:101)i
h
(θ(cid:101)i h+1−θ(cid:101)i h),θ(cid:101)i h+1−θ∗
√ (cid:13) (cid:13) (cid:13) (cid:13)2
≤ 3 2L φ(cid:13) (cid:13)θ(cid:101)i h+1−θ∗ h(cid:13)
(cid:13)
2(cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i h(cid:13)
(cid:13) ∇2ℓ (θ¯i+1)
i,h h
√ (cid:13) (cid:13)2
≤ 3 2L φ(cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i h(cid:13)
(cid:13) ∇2ℓ (θ¯i+1)
i,h h
√ (cid:13) (cid:13)2
≤ 3 2L3 (cid:13)θ(cid:101)i+1−θ(cid:101)i(cid:13)
φ(cid:13) h h(cid:13)
2
where θ¯i+1 is a convex combination of θ(cid:101)i and θ(cid:101)i+1. The second equality arises from the
h h h
Taylor expansion, the first inequality is due to the self-concordant property, and the final
inequality is justified by the following:
∇2ℓ (θ¯i+1 )
i,h h
(cid:88) (cid:88) (cid:88)
= P (s′ | si,ai)φ φ⊤ − P (s′ | si,ai)P (s′′ | si,ai)φ φ⊤
θ¯i+1 h h i,h,s′ i,h,s′ θ¯i+1 h h θ¯i+1 h h i,h,s′ i,h,s′′
h h h
s′∈S s′∈S s′′∈S
i,h i,h i,h
(cid:88)
⪯ P (s′ | si,ai)φ φ⊤
θ¯i+1 h h i,h,s′ i,h,s′
h
s′∈S
i,h
⪯ L2I .
φ d
55By summing over i and reorganizing the terms, we arrive at the final result as follows:
(cid:13) (cid:13)2
(cid:13)θ(cid:101)k+1−θ∗(cid:13)
(cid:13) h h(cid:13)
B
k+1,h
(cid:88)k (cid:16) (cid:17) (cid:13) (cid:13)2 √ (cid:88)k (cid:13) (cid:13)2 (cid:88)k (cid:13) (cid:13)2
≤ α
i,h
ℓ i,h(θ∗ h)−ℓ i,h(θ(cid:101)i h+1) +(cid:13) (cid:13)θ(cid:101)1 h−θ∗ h(cid:13)
(cid:13)
+3 2L3
φ
α i,h(cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i h(cid:13)
(cid:13)
− (cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i h(cid:13)
(cid:13)
B 2 B
i=1 1,h i=1 i=1 i,h
(cid:88)k (cid:16) (cid:17) √ (cid:88)k (cid:13) (cid:13)2 (cid:88)k (cid:13) (cid:13)2
≤ α ℓ i,h(θ∗ h)−ℓ i,h(θ(cid:101)i h+1) +λL2
θ
+3 2L3 φα (cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i h(cid:13)
(cid:13)
− (cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i h(cid:13)
(cid:13)
.
2 B
i=1 i=1 i=1 i,h
where the first inequality holds by Assumption 2 and the last inequality holds since α =
logU +2(1+L L ) ≥ α for all i ∈ [k].
φ θ i,h
C.1.2 Proof of Lemma 14
(cid:18) (cid:19)
Proof of Lemma 14. The norm of z = σ+ E [σ (Φ⊤ θ)] is generally un-
(cid:101)i,h i,h θ∼N(θ(cid:101)i,cB−1) i,h i,h
h i,h
bounded (Foster et al., 2018). In this proof, we utilize the smoothed version of z , defined
(cid:101)i,h
as follows:
(cid:18) (cid:19)
zu = σ+ smoothu E [σ (Φ⊤ θ)]
(cid:101)i,h i,h i,h θ∼N(θ(cid:101)i,cB−1) i,h i,h
h i,h
where the smooth function smoothu (p) = (1−u)p+(u/U)1 with u ∈ [0,1/2], and 1 ∈
i,h
R|S i,h| is an all-one vector.
Exploiting the property of σ+ such that σ (σ+ (p)) = p for any p ∈ ∆ , it is
i,h i,h i,h |S i,h|
straightforward to show that zu = σ+ (smoothu (σ (z ))). Then, by Lemma 34, we
(cid:101)i,h i,h i,h i,h (cid:101)i,h
have
k k
(cid:88) (cid:88)
ℓ(zu ,yi)− ℓ(z ,yi) ≤ 2uk, and ∥zu ∥ ≤ log(U/u). (56)
(cid:101)i,h h (cid:101)i,h h (cid:101)i,h ∞
i=1 i=1
Given the definition of ℓ , we know that ℓ(z∗ ,yi) = ℓ (θ∗), where z∗ = Φ⊤ θ∗. We
i,h i,h h i,h h i,h i,h h
can bound the gap between the loss of θ∗ and zu as follows:
h (cid:101)i,h
k k
(cid:88)(cid:0) ℓ (θ∗)−ℓ(zu ,yi)(cid:1) = (cid:88)(cid:0) ℓ(z∗ ,yi)−ℓ(zu ,yi)(cid:1)
i,h h (cid:101)i,h h i,h h (cid:101)i,h h
i=1 i=1
k k
(cid:88) (cid:88) 1
≤ ⟨∇ ℓ(z∗ ,yi),z∗ −zu ⟩− ∥z∗ −zu ∥2
z i,h h i,h (cid:101)i,h M
i,h
i,h (cid:101)i,h ∇2 zℓ(z∗ i,h,y hi)
i=1 i=1
k k
(cid:88) (cid:88) 1
= ⟨∇ ℓ(z∗ ,yi),z∗ −zu ⟩− ∥z∗ −zu ∥2 ,
z i,h h i,h (cid:101)i,h M
i,h
i,h (cid:101)i,h ∇σ i,h(z∗ i,h)
i=1 i=1
(57)
where M = log(|S |)+2log(U/u), and the second equality holds by a direct calculation
i,h i,h
of the first order and Hessian of the logistic loss.
56Now, we first bound the first term of the right-hand side. Let d = (z∗ −zu )/(M +
i,h i,h (cid:101)i,h
L L ), where M = logU + 2log(U/u). Then, one can check that ∥d ∥ ≤ 1 since
φ θ i,h ∞
∥z∗ ∥ ≤ max ∥φ ∥ ∥θ∗∥ ≤ L L and ∥zu ∥ ≤ log(U/u). Moreover, since z∗
i,h ∞ s′∈S i,h i,h,s′ 2 h 2 φ θ (cid:101)i,h ∞ i,h
and zu are independent of yi, d is F -measurable. Since E[(σ (z∗ )−yi)(σ (z∗ )−
(cid:101)i,h h i,h i,h i,h i,h h i,h i,h
yi)⊤ | F ] = ∇σ (z∗ ) and ∥σ (z∗ )−yi∥ ≤ 2, we can apply Lemma 32. For any k
h i,h i,h i,h i,h i,h h 1
and δ ∈ (0,1], with probability at least 1−δ/H, we have
k
(cid:88)
⟨∇ ℓ(z∗ ,yi),z∗ −zu ⟩
z i,h h i,h (cid:101)i,h
i=1
k
(cid:88)
= (M +L L ) ⟨∇ ℓ(z∗ ,yi),d ⟩
φ θ z i,h h i,h
i=1
(cid:118)
≤ (M +L φL θ)(cid:118) (cid:117) (cid:117) (cid:116)λ+(cid:88)k ∥d i,h∥2
∇σ i,h(z∗
i,h)(cid:117) (cid:117) (cid:117) (cid:116)√ 4λ + √4
λ
log H(cid:113) 1+ λ1 (cid:80)k i=1 δ∥d i,h∥2 ∇σ i,h(z∗ i,h) 
i=1
(cid:118) (cid:115)√ √
(cid:117) k (cid:18) (cid:19)
(cid:117) (cid:88) λ H 1+2k
≤ (M +L L )(cid:116)λ+ ∥d ∥2 +4log , (58)
φ θ i,h ∇σ i,h(z∗ i,h) 4 δ
i=1
wherethesecondinequalityholdssince∥d ∥2 = d⊤ ∇σ (z∗ )d ≤ 2andλ ≥ 1.
i,h ∇σ (z∗ ) i,h i,h i,h i,h
i,h i,h
Plugging (58) into (57) and rearranging the term, we get
k
(cid:88)(cid:0) ℓ (θ∗)−ℓ(zu ,yi)(cid:1)
i,h (cid:101)i,h h
i=1
(cid:118) (cid:115)√ √
(cid:117) k (cid:18) (cid:19) k
(cid:117) (cid:88) λ 1+2k (cid:88) 1
≤ (M +L L )(cid:116)λ+ ∥d ∥2 +4log H − ∥z∗ −zu ∥2
φ θ i,h ∇σ i,h(z∗ i,h) 4 δ M
i,h
i,h (cid:101)i,h ∇σ i,h(z∗ i,h)
i=1 i=1
(cid:118) (cid:115)√ √
(cid:117) k (cid:18) (cid:19) k
(cid:117) (cid:88) λ H 1+2k (cid:88)
≤ (M +L L )(cid:116)λ+ ∥d ∥2 +4log −(M +L L ) ∥d ∥2
φ θ i,h ∇σ i,h(z∗ i,h) 4 δ φ θ i,h ∇σ i,h(z∗ i,h)
i=1 i=1
(cid:32) k (cid:33) (cid:32)√ (cid:18) √ (cid:19)(cid:33)
(cid:88) λ H 1+2k
≤ (M +L L ) λ+ ∥d ∥2 +(M +L L ) +4log
φ θ i,h ∇σ i,h(z∗ i,h) φ θ 4 δ
i=1
k
(cid:88)
−(M +L L ) ∥d ∥2
φ θ i,h ∇σ (z∗ )
i,h i,h
i=1
√
(cid:18) (cid:19)
5 H 1+2k
≤ (M +L L )λ+4(M +L L )log . (59)
φ θ φ θ
4 δ
57Finally, combining (56) and (59), by setting u = 1/k, we derive that
√
k (cid:18) (cid:19)
(cid:88)(cid:0) ℓ (θ∗)−ℓ(z ,yi)(cid:1) ≤ 5 (M +L L )λ+4(M +L L )log H 1+2k +2uk
i,h h (cid:101)i,h h 4 φ θ φ θ δ
i=1
√
(cid:18) (cid:19)
5 H 1+2k
≤ (3log(Uk)+L L )λ+4(3log(Uk)+L L )log +2
φ θ φ θ
4 δ
where the last inequality holds by the definition of M = logU +2log(U/u). Taking the
union bound over h ∈ [H], we conclude the proof.
C.1.3 Proof of Lemma 15
Proof of Lemma 15. WestarttheprooffromtheobservationofProposition2inFosteretal.
(2018), stating that z represents the mixed prediction, which adheres to the following
(cid:101)i,h
property:
(cid:18) (cid:19) (cid:18) (cid:90) (cid:19)
1
ℓ(z ,yi) ≤ −log E [exp(−ℓ (θ))] = −log exp(−L (θ))dθ ,
(cid:101)i,h h θ∼N(θ(cid:101)i h,cB− i,h1) i,h Z
i,h Rd
i,h
(60)
(cid:13) (cid:13)2 (cid:113)
where L i,h(θ) := ℓ i,h(θ)+ 21
c
(cid:13) (cid:13)θ−θ(cid:101)i h(cid:13)
(cid:13)
and Z
i,h
:= (2π)dc|B− i,h1|.
B
i,h
Consider the quadratic approximation
(cid:68) (cid:69) 1 (cid:13) (cid:13)2
L(cid:101)i,h(θ) = L i,h(θ(cid:101)i h+1)+ ∇L i,h(θ(cid:101)i h+1),θ−θ(cid:101)i h+1 +
2c
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) B
.
i,h
√
Using the property that ℓ is 3 2L -self-concordant-like function as asserted by Proposi-
i,h φ
tion B.1 in (Lee and Oh, 2024), and applying Lemma 35, we obtain
(cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2
L i,h(θ) ≤ L(cid:101)i,h(θ)+exp 18L2 φ(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13)
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) i+1
.
2 ∇ℓ i,h(θ(cid:101)
h
)
Also, we have
(cid:90)
1
exp(−L (θ))dθ
i,h
Z
i,h Rd
1 (cid:90) (cid:18) (cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2 (cid:19)
≥
Z i,h
Rdexp −L(cid:101)i,h(θ)−exp 18L2 φ(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) 2
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) ∇ℓ i,h(θ(cid:101)i h+1 )
dθ
(cid:16) (cid:17)
exp −L i,h(θ(cid:101)i h+1) (cid:90) (cid:16) (cid:68) (cid:69)(cid:17)
=
Z
f(cid:101)i+1,h(θ)·exp − ∇L i,h(θ(cid:101)i h+1),θ−θ(cid:101)i h+1 dθ, (61)
i,h Rd
where we define the function f(cid:101)i,h : B(0 d,1) → R as
(cid:18) 1 (cid:13) (cid:13)2 (cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2 (cid:19)
f(cid:101)i+1,h(θ) = exp −
2c
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) B
i,h
−exp 18L2 φ(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) 2
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 )
.
58(cid:82)
We denote Z(cid:101)i+1,h = Rdf(cid:101)i+1,h(θ)dθ ≤ +∞ and define Θ(cid:101)i+1,h as the distribution whose
density function is f(cid:101)i+1,h(θ)/Z(cid:101)i+1,h. Then, we can rewrite (61) as follows:
(cid:90)
1
exp(−L (θ))dθ
i,h
Z
i,h Rd
(cid:16) (cid:17)
exp −L i,h(θ(cid:101)i h+1) Z(cid:101)i+1,h
(cid:104) (cid:16) (cid:68) (cid:69)(cid:17)(cid:105)
≥
Z
i,h
E
θ∼Θ(cid:101)i+1,h
exp − ∇L i,h(θ(cid:101)i h+1),θ−θ(cid:101)i h+1
(cid:16) (cid:17)
exp −L i,h(θ(cid:101)i h+1) Z(cid:101)i+1,h
(cid:16) (cid:104)(cid:68) (cid:69)(cid:105)(cid:17)
≥
Z
i,h
exp −E
θ∼Θ(cid:101)i+1,h
∇L i,h(θ(cid:101)i h+1),θ−θ(cid:101)i h+1
(cid:16) (cid:17)
exp −L i,h(θ(cid:101)i h+1) Z(cid:101)i+1,h
= , (62)
Z
i,h
where the second inequality is by Jensen’s inequality and the last inequality holds because
(cid:104)(cid:68) (cid:69)(cid:105)
Θ(cid:101)i+1,h is symmetric around θ(cid:101)i h+1 and thus E
θ∼Θ(cid:101)i+1,h
∇L i,h(θ(cid:101)i h+1),θ−θ(cid:101)i h+1 = 0.
Combining (60) and (62), we get
ℓ i,h( (cid:101)z) ≤ L i,h(θ(cid:101)i h+1)+logZ i,h−logZ(cid:101)i+1,h. (63)
Moreover, we have
−logZ(cid:101)i+1,h
(cid:18)(cid:90) (cid:18) 1 (cid:13) (cid:13)2 (cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2 (cid:19) (cid:19)
= −log exp − (cid:13)θ−θ(cid:101)i+1(cid:13) −exp 18L2 (cid:13)θ−θ(cid:101)i+1(cid:13) (cid:13)θ−θ(cid:101)i+1(cid:13) dθ
Rd 2c (cid:13) h (cid:13) B i,h φ(cid:13) h (cid:13) 2 (cid:13) h (cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 )
(cid:18) (cid:20) (cid:18) (cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2 (cid:19)(cid:21)(cid:19)
= −log Z(cid:98)i+1,h·E
θ∼Θ(cid:98)i+1,h
exp −exp 18L2 φ(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) 2
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 )
(cid:20) (cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2 (cid:21)
≤ −logZ(cid:98)i+1,h+E
θ∼Θ(cid:98)i+1,h
exp 18L2 φ(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) 2
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 )
(cid:20) (cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2 (cid:21)
= −logZ i,h+E
θ∼Θ(cid:98)i+1,h
exp 18L2 φ(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) 2
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 )
, (64)
(cid:90) (cid:18) 1 (cid:13) (cid:13)2 (cid:19)
whereΘ(cid:98)i+1,h = N(θ(cid:101)i h+1,cB− i,h1)andZ(cid:98)i+1,h = Rdexp −
2c
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) B
i,h
dθ,andthelast
inequality holds because Z(cid:98)i+1,h and Z
i,h
are identical normalizing factors. Integrating (63)
and (64) and summing over k, yields
(cid:88)k (cid:88)k (cid:88)k (cid:20) (cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2 (cid:21)
i=1ℓ( (cid:101)z i,h,y hi) = i=1L i,h(θ(cid:101)i h+1)+ i=1E
θ∼Θ(cid:98)i+1,h
exp 18L2 φ(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) 2
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 )
.
59Moreover,wecanfurtherboundthesecondtermontheright-handsideof (64). ByCauchy-
Schwarz inequality, we get
(cid:20) (cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2 (cid:21)
E exp 18L2 (cid:13)θ−θ(cid:101)i+1(cid:13) (cid:13)θ−θ(cid:101)i+1(cid:13)
θ∼Θ(cid:98)i+1,h φ(cid:13) h (cid:13) 2 (cid:13) h (cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 )
(cid:115) (cid:115)
(cid:20) (cid:18) (cid:13) (cid:13)2(cid:19)(cid:21) (cid:20)(cid:13) (cid:13)4 (cid:21)
≤ E exp 36L2 (cid:13)θ−θ(cid:101)i+1(cid:13) E (cid:13)θ−θ(cid:101)i+1(cid:13) .
θ∼Θ(cid:98)i+1,h φ(cid:13) h (cid:13) 2 θ∼Θ(cid:98)i+1,h (cid:13) h (cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 )
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
(I) (II)
(cid:16) (cid:17)
Since Θ(cid:98)i+1,h = N θ(cid:101)i h+1,cB− i,h1 , θ−θ(cid:101)i h+1 follows the same distribution as
d (cid:114)
(cid:88)
cλ
(cid:16) B−1(cid:17)
X e , where X
i. ∼i.d.
N(0,1),∀j ∈ [d], (65)
j i,h j j j
j=1
(cid:16) (cid:17)
where λ B−1 denotes the j-th largest eigenvalue of B−1 and {e ,...,e } are orthogonal
j i,h i,h 1 d
basis of Rd. Furthermore, since we know that B−1 ≤ λ−1I , we can bound the term (I) by
i,h d
(cid:118)
(cid:117)   (cid:118)
(I) ≤ (cid:117) (cid:117) (cid:116)E Xj (cid:89)d exp(cid:16) 36L2 φcλ−1X j2(cid:17)  = (cid:117) (cid:117) (cid:116)(cid:89)d E Xj (cid:104) exp(cid:16) 36L2 φcλ−1X j2(cid:17)(cid:105)
j=1 j=1
≤ (cid:0)E (cid:2) exp(cid:0) 36L2cλ−1W(cid:1)(cid:3)(cid:1)d 2 ≤ E (cid:2) exp(cid:0) 18L2cλ−1Wd(cid:1)(cid:3)
W∼χ2 φ W∼χ2 φ
whereχ2 isthechi-squaredistributionandthelastinequalityholdsduetoJensen’sinequal-
ity. By choosing λ ≥ 72L2cd, we arrive that
φ
(cid:20) (cid:18) W(cid:19)(cid:21) √
(I) ≤ E exp ≤ 2, (66)
W∼χ2
4
where the last inequality holds because the moment-generating function for χ2-distribution
√
is bounded by E [exp(tW)] ≤ 1/ 1−2t for all t ≤ 1/2. Now, we bound the term (II).
W∼χ2
(cid:115) (cid:115)
(cid:20)(cid:13) (cid:13)4 (cid:21) (cid:20) (cid:21)
(II) = E
θ∼Θ(cid:98)i+1,h
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 )
= E
θ∼N(0,cB− i,h1)
∥θ∥4
∇2ℓ i,h(θ(cid:101)i h+1 )
= (cid:113) E (cid:2) ∥θ∥4(cid:3) ,
θ∼N(0,cB¯−1) 2
i,h
(cid:16) (cid:17)−1/2 (cid:16) (cid:17)−1/2 (cid:16) (cid:17)
where B¯
i,h
= ∇2ℓ i,h(θ(cid:101)i h+1) B
i,h
∇2ℓ i,h(θ(cid:101)i h+1) . Let λ¯
j
:= λ
j
cB¯− i,h1 be the j-th
largest eigenvalue of the matrix. Then, a similar analysis as (65) gives that
(cid:118) (cid:118)
(cid:117) (cid:13) (cid:13)4 (cid:117)  2
(II) =
(cid:117)
(cid:117) (cid:116)E
Xj∼N(0,1)(cid:13)
(cid:13) (cid:13)
(cid:13)(cid:88)d (cid:113)
λ¯ jX je
j(cid:13)
(cid:13) (cid:13)
(cid:13)
 =
(cid:117)
(cid:117) (cid:116)E
Xj∼N(0,1)(cid:88)d
λ¯ jX j2  
(cid:13)j=1 (cid:13) j=1
2
(cid:118) (cid:118)
= (cid:117) (cid:117) (cid:116)(cid:88)d (cid:88)d λ¯ jλ¯ j′E
Xj,X
j′∼N(0,1)[X j2X j2 ′] ≤ (cid:117) (cid:117) (cid:116)3(cid:88)d (cid:88)d λ¯ jλ¯
j′
= √ 3ctr(cid:16) B¯− i,h1(cid:17) ,
j=1j′=1 j=1j′=1
60where the last inequality holds due to E [X2X2] ≤ 3 when considering the case
Xj,X j′∼N(0,1) j j′
(cid:16) (cid:17)2 (cid:16) (cid:17)
where j = j′ and the last equality is derived from the fact that (cid:80)d λ¯ = tr cB¯−1 .
j=1 j i,h
Here, we denote tr(A) as the trace of the matrix A.
WedefinematrixR := λI /2+(cid:80)i ∇2ℓ (θ ). Undertheconditionλ ≥ 2L2,
i+1,h d τ=1 τ,h τ+1,h φ
we have ∇2ℓ (θ ) ⪯ L2I ≤ λI . Then, we have B ⪰ R . Therefore, we can
i,h i+1,h φ d 2 d i,h i+1,h
bound the trace by
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
tr B¯−1 = tr B−1∇2ℓ (θ ) ≤ tr R−1 ∇2ℓ (θ )
i,h i,h i,h i+1,h i+1,h i,h i+1,h
(cid:16) (cid:17) det(R )
= tr R−1 (R −R ) ≤ log i+1,h ,
i+1,h i+1,h i,h det(R )
i,h
where the last inequality holds due to Lemma 4.7 of Hazan et al. (2016). Therefore we can
bound the term (II) as
√ det(R )
i+1,h
(II) ≤ 3clog . (67)
det(R )
i,h
Combining (66) and (67), we get
(cid:20) (cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2 (cid:21) √ det(R )
E exp 6(cid:13)θ−θ(cid:101)i+1(cid:13) (cid:13)θ−θ(cid:101)i+1(cid:13) ≤ 6clog i+1,h . (68)
θ∼Θ(cid:98)i+1,h (cid:13) h (cid:13) 2 (cid:13) h (cid:13) ∇2ℓ i,h(θ(cid:101)i h+1 ) det(R i,h)
Plugging (64) and (68) into (63), and taking summation over k, we derive that
(cid:88)k
ℓ( (cid:101)z i,h,y hi) ≤
(cid:88)k
L i,h(θ(cid:101)i
h+1)+√ 6c(cid:88)k
log
d det e( tR
(Ri+1,
)h)
i,h
i=1 i=1 i=1
= (cid:88) i=k 1(cid:18) ℓ i,h(θ(cid:101)i h+1)+ 21
c
(cid:13) (cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i h(cid:13) (cid:13) (cid:13)2
B
i,h(cid:19) +√ 6c(cid:88) i=k 1log d det e( tR (Ri+ i,1 h, )h)
≤
(cid:88)k (cid:18)
ℓ i,h(θ(cid:101)i h+1)+
21
c
(cid:13)
(cid:13) (cid:13)θ(cid:101)i h+1−θ(cid:101)i
h(cid:13)
(cid:13)
(cid:13)2
B
(cid:19) +√ 6cdlog(cid:32)
1+
2k dL λ2 φ(cid:33)
,
i=1 i,h
wherethelastinequalityholdsbecause(cid:80)k
log
det(R i+1,h)
= log(det(R )/det(λ/2I )) ≤
i=1 det(R ) k+1,h d
i,h
(cid:16) 2kL2 (cid:17)
dlog 1+ φ . By rearranging the terms, we conclude the proof.
dλ
C.2 Bound on Prediction Error
In this section, we present the bound on the prediction error of parameters updated by
ORRL-MNL. First, we compare the problem setting of MNL contextual bandits with ours
and introduce the challenges of applying their analysis to our setting.
MNLdynamicassortmentoptimization(single-parameter&uniformreward)(Periv-
ier and Goyal, 2022) Perivier and Goyal (2022) consider an assortment selection prob-
lem where the user choice is given by a MNL choice model with the single-parameter. At
each time t, the agent observes context features {x }M ⊂ Rd. Then the agent decides on
t,i i=1
61thesetS ⊂ [M]tooffertoauser,with|S | ≤ N. Withoutlossofgenerality,wemayassume
t t
|S | = N. Then the user purchases one single product j ∈ S ∪{0} and the probability of
t t
each product j is purchased by a user follows the MNL model parametrized by a unknown
fixed parameter θ∗ ∈ Rd,
 exp(x⊤ θ∗)
q t,j(S t,θ∗) :=  1+(cid:80) k∈Stet, xj p(x⊤ kθ∗) if j ∈ S t
 1+(cid:80) 1 exp(x⊤θ∗) if j = 0.
k∈St k
Thenthedifferencebetweentherevenueinducedbyθ∗andthatbyanestimatorθinPerivier
and Goyal (2022) is expressed as follows:
(cid:88) (cid:88)
q (S ,θ∗)− q (S ,θ). (69)
t,j t t,j t
j∈St j∈St
IfwedefineQ : RN → R,suchthatforallu = (u ,...,u ) ∈ RN,Q(u) := (cid:80)N exp(ui)
1 N i=1 1+(cid:80)N j=1exp(uj)
and let v∗ = (x⊤ θ∗,...,x⊤ θ∗) and v = (x⊤ θ,...,x⊤ θ), then Eq. (69) can be ex-
t,i1 t,iN t,i1 t,iN
pressed as follows:
(cid:88) (cid:88)
q (S ,θ∗)− q (S ,θ) = Q(v∗)−Q(v)
t,j t t,j t
j∈St j∈St
1
= ∇Q(v∗)⊤(v∗−v)+ (v∗−v)⊤∇2Q(v¯)(v∗−v), (70)
2
where v¯ is a convex combination of v∗ and v. For the first term in Eq. (70), we have
(cid:80) exp(x⊤ θ∗)(v −v∗) (cid:80) exp(x⊤ θ∗)(cid:80) exp(x⊤θ∗)(v −v∗)
∇Q(v∗)⊤(v∗−v) = i∈St t,j j j − i∈St t,j i∈St t,i j j
1+(cid:80) j∈Stexp(x⊤ t,jθ∗) (cid:16) 1+(cid:80) j∈Stexp(x⊤ t,jθ∗)(cid:17)2
(cid:88) (cid:88) (cid:88)
= q (S ,θ∗)x⊤ (θ∗−θ)− q (S ,θ∗)q (S ,θ∗)x⊤(θ∗−θ)
t,j t t,j t,j t t,j t t,i
j∈St j∈Sti∈St
(cid:32) (cid:33)
(cid:88) (cid:88)
= q (S ,θ∗) 1− q (S ,θ∗) x⊤ (θ∗−θ)
t,j t t,i t t,j
j∈St i∈St
(cid:88)
= q (S ,θ∗)q (S ,θ∗)x⊤ (θ∗−θ)
t,j t t,0 t t,j
j∈St
(cid:88)
≤ q (S ,θ∗)q (S ,θ∗)∥x ∥ ∥θ∗−θ∥ , (71)
t,j t t,0 t t,j H− t1(θ∗) Ht(θ∗)
j∈St
where H (θ) is the Gram matrix used in (Perivier and Goyal, 2022) defined by
t
t−1
(cid:88) (cid:88) (cid:88) (cid:88)
H (θ∗) := q (S ,θ∗)x x⊤ − q (S ,θ∗)q (S ,θ∗)x x⊤ .
t τ,j τ τ,j τ,j τ,j τ τ,i τ τ,j τ,i
τ=1j∈Sτ j∈Sτ i∈Sτ
Note that the term ∥θ∗ − θ∥ can be bounded by the concentration result of the
Ht(θ∗)
estimated parameter. On the other hand, to apply the elliptical potential lemma to the
62term(cid:80) q (S ,θ∗)q (S ,θ∗)∥x ∥ , notethatH (θ∗)canbeboundedasfollows:
j∈St t,j t t,0 t t,j H− t1(θ∗) t
(cid:88) 1 (cid:88) (cid:88) (cid:16) (cid:17)
H (θ∗) = H (θ∗)+ q (S ,θ∗)x x⊤ − q (S ,θ∗)q (S ,θ∗) x x⊤ +x x⊤
t t−1 t,j t t,j t,j 2 t,j t t,i t t,j t,i t,i t,j
j∈St j∈Sti∈St
(cid:88) 1 (cid:88) (cid:88) (cid:16) (cid:17)
⪰ H (θ∗)+ q (S ,θ∗)x x⊤ − q (S ,θ∗)q (S ,θ∗) x x⊤ +x x⊤
t−1 t,j t t,j t,j 2 t,j t t,i t t,j t,j t,i t,i
j∈St j∈Sti∈St
(cid:32) (cid:33)
(cid:88) (cid:88)
= H (θ∗)+ q (S ,θ∗) 1− q (S ,θ∗) x x⊤
t−1 t,j t t,i t t,j t,j
j∈St i∈St
(cid:88)
= H (θ∗)+ q (S ,θ∗)q (S ,θ∗)x x⊤ . (72)
t−1 t,j t t,0 t t,j t,j
j∈St
Now since the coefficient q (S ,θ∗)q (S ,θ∗) of ∥x∥ in Eq. (71) aligns with the
t,j t t,0 t H−1(θ∗)
t
coefficients of the lower bound of H (θ∗) in Eq. (72), the elliptical potential lemma can be
t
applied. Note that such a lower bound in Eq. (72) holds since Perivier and Goyal (2022)
deals with the uniform reward, i.e., 1−(cid:80) q (S ,θ∗) = q (S ,θ∗).
i∈St t,i t t,0 t
Mulitinomial logistic bandit problem (Zhang and Sugiyama, 2023) Zhang and
Sugiyama (2023) address the multiple-parameter MNL contextual bandit problem where
at each time step t the agent selects an action x ∈ Rd and receives response feedback
t
y ∈ {0}∪[N] with N +1 possible outcomes. Each outcome i ∈ [N] is associated with a
t
ground-truth parameter θ∗ ∈ Rd, and the probability of the outcome P(y = i | x ) follows
i t t
the MNL model,
exp(x⊤θ∗) (cid:88)N
P(y = i | x ) = t i , P(y = 0 | x ) = 1− P(y = j | x ).
t t 1+(cid:80)N exp(x⊤θ∗) t t t t
j=1 t j j=1
In this model, there are N unknown choice parameter Θ∗ := [θ∗,...,θ∗ ] ∈ Rd×N and the
1 N
agent chooses one context feature x , that is why we call multiple-parameter MNL model.
t
Then, the expected revenue of an action x in (Zhang and Sugiyama, 2023) is given by
t
(cid:88)N exp(x⊤
t
θ∗ i)ρ
i := ρ⊤σ(Θ∗x ),
1+(cid:80)N exp(x⊤θ∗) t
i=1 j=1 t j
where we define the softmax function σ : RN → [0,1]N by
exp([z] ) 1
i
[σ(z)] = ∀k ∈ [N] and [σ(z)] = ∀k ∈ [N],
k 1+(cid:80)N
exp([z] )
0 1+(cid:80)N
exp([z] )
j=1 j j=1 j
and ρ := [ρ ,...,ρ ] ∈ RN+1 represents the reward for each outcome j ∈ [N] with ρ = 0.
1 N + 0
Then, the difference between the revenue induced by Θ∗ and that by an estimator Θˆ
63in (Zhang and Sugiyama, 2023) is expressed by
(cid:16) (cid:17)
ρ⊤ σ(Θ∗x )−σ(Θˆx )
t t
N
(cid:88) (cid:16) (cid:17)
= ρ [σ(Θ∗x )] −[σ(Θˆx )]
k t k t k
k=1
N N
(cid:88) (cid:16) (cid:17)⊤ (cid:88)
= ρ ∇[σ(Θˆx )] (Θ∗−Θˆ)x + ρ ∥(Θ∗−Θ)x ∥ , (73)
k t k t k t Ξ k
k=1 k=1
where Ξ = (cid:82)1 (1−ν)∇2[σ(Θˆx +ν(Θ∗ −Θˆ)x )] dν. Then for the first term in Eq. (73),
k 0 t t k
we have
N
(cid:88) (cid:16) (cid:17)⊤
ρ ∇[σ(Θˆx )] (Θ∗−Θˆ)x
k t k t
k=1
(cid:12) (cid:12)
≤ (cid:12)ρ⊤∇σ(Θˆx )(Θ∗−Θˆ)x (cid:12)
(cid:12) t t(cid:12)
(cid:12) (cid:12)
= (cid:12)ρ⊤∇σ(Θˆx )(I ⊗x⊤)(vec(Θ∗)−vec(Θˆ))(cid:12)
(cid:12) t N t (cid:12)
≤ ∥vec(Θ∗)−vec(Θˆ)∥
∥H−1
2(I ⊗x⊤)∇σ(Θˆx )ρ∥ (74)
Ht t N t t 2
where H is the Gram matrix used in (Zhang and Sugiyama, 2023) defined by
t
t−1
(cid:88)
H := λI + ∇σ(Θˆ x )⊗x x⊤.
t N s+1 s s s
s=1
Note that the term ∥vec(Θ∗)−vec(Θˆ)∥ in Eq. (74) can be bounded by the concentration
Ht
result of the estimated parameter, and the term
∥H−1
2(I ⊗x⊤)∇σ(Θˆx )ρ∥ also can be
t N t t 2
bounded as follows:
∥H− 21 (I ⊗x⊤)∇σ(Θˆx )ρ∥ ≤ ∥ρ∥ ∥H−1 2(I ⊗x⊤)∇σ(Θˆx )∥ .
t N t t 2 2 t N t t 2
HereZhangandSugiyama(2023)boundtheterm∥H− 21 (I ⊗x⊤)∇σ(Θˆx )∥ usingamatrix
t N t t 2
version of elliptical lemma. However, they assume ∥ρ∥ ≤ R (Assumption 2 in (Zhang and
2
Sugiyama, 2023)).
Now, regarding the prediction error in our setting, the estimated values (V(cid:101)k (·)) for
h+1
each reachable state are typically distinct, and we do not assume a constant upper bound
on the ℓ -norm of the estimated value vector for all reachable states. Instead, we can bound
2
the ℓ -norm of the estimated value vector for all reachable states as follows:
2
(cid:12) (cid:12)(cid:113) √
∥V(cid:101) hk +1(s,a)∥
2
≤ max (cid:12) (cid:12)V(cid:101) hk +1(s′)(cid:12)
(cid:12)
|S s,a| ≤ H U,
s′∈Ss,a
(cid:104) (cid:105)
where V(cid:101)k (s,a) := V(cid:101)k (s′) ∈ R|Ss,a|. However, such a bound leads to a looser
h+1 h+1
√ s′∈Ss,a
regretbyafactorof U. Toaddress, weadaptthefeature centralization technique (Leeand
64Oh, 2024) to bound the prediction error independently ofU, without making any additional
assumptions. The key point is that the Hessian of per-round loss ℓ (θ) is expressed in
k,h
terms of the centralized feature as follows:
(cid:88)
∇2ℓ (θ) = P (s′ | sk,ak)φ¯(sk,ak,s′;θ)φ¯(sk,ak,s′;θ)⊤.
k,h θ h h h h h h
s′∈S
k,h
whereφ¯(s,a,s′;θ) := φ(s,a,s′)−E [φ(s,a,s)]isthecentralizedfeaturebyθ. Now,
s∼P (·|s,a) (cid:101)
(cid:101) θ
weprovidetheboundonpredictionerroroftheestimatedparameterupdatedbyORRL-MNL.
Lemma 16 (Bound on the prediction error). For any δ ∈ (0,1), suppose that Lemma 12
holds. Let us denote the prediction error about θ(cid:101)k by
h
(cid:88) (cid:16) (cid:17)
∆k h(s,a) := P θ(cid:101)k h(s′ | s,a)−P θ∗ h(s′ | s,a) V(cid:101) hk +1(s′).
s′∈Ss,a
Then, for any (s,a) ∈ S ×A, we have
(cid:88) (cid:13) (cid:13)
|∆k h(s,a)| ≤ Hβ k(δ) s′∈Ss,aP
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13) B− k,1
h
+3Hβ k(δ)2 sm ′∈Sa sx ,a∥φ s,a,s′∥2
B− k,1
h.
Proof of Lemma 16. Let us define F(θ) := (cid:80) s′∈Ss,aP θ(s′ | s,a)V(cid:101) hk +1(s′). Then, by Taylor
expansion we have
1
F(θ∗) = F(θ(cid:101)k)+∇F(θ(cid:101)k)⊤(θ∗ −θ(cid:101)k)+ (θ∗ −θ(cid:101)k)⊤∇2F(θ¯)(θ∗ −θ(cid:101)k),
h h h h h 2 h h h h
where θ¯ = (1−v)θ∗ +vθ(cid:101)k for some v ∈ (0,1). By Proposition 1, we have
h h
(cid:88)
∇F(θ) = ∇P θ(s′ | s,a)V(cid:101) hk +1(s′)
s′∈Ss,a
 
(cid:88) (cid:88)
= P θ(s′ | s,a)φ
s,a,s′
− P θ(s (cid:101)| s,a)φ s,a,sV(cid:101) hk +1(s′)
(cid:101)
s′∈Ss,a s (cid:101)∈Ss,a
(cid:88)
= P θ(s′ | s,a)φ¯ s,a,s′(θ)V(cid:101) hk +1(s′),
s′∈Ss,a
and
(cid:88)
∇2F(θ) = ∇2P θ(s′ | s,a)V(cid:101) hk +1(s′)
s′∈Ss,a
(cid:88)
= P θ(s′ | s,a)V(cid:101) hk +1(s′)φ s,a,s′φ⊤
s,a,s′
s′∈Ss,a
(cid:88) (cid:88) (cid:16) (cid:17)
− P θ(s′ | s,a)V(cid:101) hk +1(s′) P θ(s′′ | s,a) φ s,a,s′φ⊤
s,a,s′′
+φ s,a,s′′φ⊤
s,a,s′
+φ s,a,s′′φ⊤
s,a,s′′
s′∈Ss,a s′′∈Ss,a
  ⊤
(cid:88) (cid:88) (cid:88)
+2 P θ(s′ | s,a)V(cid:101) hk +1(s′) P θ(s′′ | s,a)φ s,a,s′′ P θ(s′′ | s,a)φ s,a,s′′ .
s′∈Ss,a s′′∈Ss,a s′′∈Ss,a
65Then, the prediction error can be bounded as follows:
|∆k(s,a)| = |F(θ∗)−F(θ(cid:101)k)|
h h h
(cid:12) (cid:12) 1 (cid:12) (cid:12)
≤ (cid:12)∇F(θ(cid:101)k)⊤(θ(cid:101)k −θ∗)(cid:12)+ (cid:12)(θ(cid:101)k −θ∗)⊤∇2F(θ¯)(θ(cid:101)k −θ∗)(cid:12) . (75)
(cid:12) h h h (cid:12) 2 (cid:12) h h h h (cid:12)
For the first term in Eq. (75),
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)∇F(θ(cid:101)k h)⊤(θ(cid:101)k h−θ∗ h)(cid:12) (cid:12)
(cid:12)
= (cid:12) (cid:12)
(cid:12)
(cid:88) P θ(cid:101)k(s′ | s,a)φ¯ s,a,s′(θ(cid:101)k h)⊤(θ(cid:101)k h−θ∗ h)V(cid:101) hk +1(s′)(cid:12) (cid:12)
(cid:12)
h
(cid:12)s′∈Ss,a (cid:12)
(cid:88) (cid:13) (cid:13) (cid:13) (cid:13)
≤ H s′∈Ss,aP
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B− k,1
h(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
B
k,h
(cid:88) (cid:13) (cid:13)
≤ Hβ k(δ) P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B−1
, (76)
s′∈Ss,a k,h
where in the first inequality we use V(cid:101)k (s′) ≤ H and Cauchy-Scharwz inequality, and the
h+1
second inequality follows by the concentration result of Lemma 12.
For the second term in Eq. (75), since 0 ≤ V(cid:101)k (s′) ≤ H,
h+1
(cid:12) (cid:12)
(cid:12)(θ(cid:101)k −θ∗)⊤∇2F(θ¯)(θ(cid:101)k −θ∗)(cid:12)
(cid:12) h h h h (cid:12)
(cid:88) (cid:16) (cid:17)2
≤ H P θ¯(s′ | s,a) (θ(cid:101)k h−θ∗ h)⊤φ
s,a,s′
s′∈Ss,a
(cid:88) (cid:88) (cid:12) (cid:16) (cid:17) (cid:12)
+H P θ¯(s′ | s,a) P θ¯(s′′ | s,a)(cid:12) (cid:12)(θ(cid:101)k h−θ∗ h)⊤ φ s,a,s′φ⊤
s,a,s′′
+φ s,a,s′′φ⊤
s,a,s′
(θ(cid:101)k h−θ∗ h)(cid:12)
(cid:12)
s′∈Ss,a s′′∈Ss,a
(cid:88) (cid:88) (cid:16) (cid:17)2
+H P θ¯(s′ | s,a) P θ¯(s′′ | s,a) (θ(cid:101)k h−θ∗ h)⊤φ
s,a,s′′
s′∈Ss,a s′′∈Ss,a
 2
(cid:18) (cid:19)
(cid:88)
+2H(θ(cid:101)k h−θ∗ h)⊤ P θ¯(s′′ | s,a)φ s,a,s′′ 
s′′∈Ss,a
(cid:88) (cid:13) (cid:13)2
≤ H s′∈Ss,aP θ¯(s′ | s,a)∥φ s,a,s′∥2
B− k,1
h(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
B
k,h
(cid:88) (cid:88) (cid:12) (cid:16) (cid:17) (cid:12)
+H P θ¯(s′ | s,a) P θ¯(s′′ | s,a)(cid:12) (cid:12)(θ(cid:101)k h−θ∗ h)⊤ φ s,a,s′φ⊤
s,a,s′
+φ s,a,s′′φ⊤
s,a,s′′
(θ(cid:101)k h−θ∗ h)(cid:12)
(cid:12)
s′∈Ss,a s′′∈Ss,a
(cid:88) (cid:13) (cid:13)2
+H s′′∈Ss,aP θ¯(s′′ | s,a)∥φ s,a,s′′∥2
B− k,1
h(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
B
k,h
(cid:18) (cid:88) (cid:13) (cid:13) (cid:19)2
+2H P θ¯(s′′ | s,a)∥φ s,a,s′′∥
B−1
(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
, (77)
s′′∈Ss,a k,h B k,h
66whereforthesecondinequalityweuseCauchy-Schwarzinequality, xx⊤+yy⊤ ⪰ xy⊤+yx⊤
for any x,y ∈ Rd, and triangle inequality. Note that
(cid:88) (cid:88) (cid:12) (cid:16) (cid:17) (cid:12)
H P θ¯(s′ | s,a) P θ¯(s′′ | s,a)(cid:12) (cid:12)(θ(cid:101)k h−θ∗ h)⊤ φ s,a,s′φ⊤
s,a,s′
+φ s,a,s′′φ⊤
s,a,s′′
(θ(cid:101)k h−θ∗ h)(cid:12)
(cid:12)
s′∈Ss,a s′′∈Ss,a
(cid:88) (cid:16) (cid:17)2 (cid:88) (cid:16) (cid:17)2
= H P θ¯(s′ | s,a) (θ(cid:101)k h−θ∗ h)⊤φ
s,a,s′
+H P θ¯(s′′ | s,a) (θ(cid:101)k h−θ∗ h)⊤φ
s,a,s′′
s′∈Ss,a s′′∈Ss,a
(cid:88) (cid:13) (cid:13)2
≤ 2H s′∈Ss,aP θ¯(s′ | s,a)∥φ s,a,s′∥2
B− k,1
h(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
B
k,h
. (78)
By substituting Eq. (78) into Eq. (77) we have
(cid:12) (cid:12) (cid:88) (cid:13) (cid:13)2
(cid:12) (cid:12)(θ(cid:101)k h−θ∗ h)⊤∇2F(θ¯)(θ(cid:101)k h−θ∗ h)(cid:12)
(cid:12)
≤ 4H s′∈Ss,aP θ¯(s′ | s,a)∥φ s,a,s′∥2
B− k,1
h(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
B
k,h
(cid:18) (cid:88) (cid:13) (cid:13) (cid:19)2
+2H P θ¯(s′′ | s,a)∥φ s,a,s′′∥
B−1
(cid:13) (cid:13)θ(cid:101)k h−θ∗ h(cid:13)
(cid:13)
s′′∈Ss,a k,h B k,h
(cid:18) (cid:19)2
≤ 4Hβ2 max ∥φ ∥2 +2H β max ∥φ ∥
k
s′∈Ss,a
s,a,s′ B− k,1
h
k
s′∈Ss,a
s,a,s′ B− k,1
h
≤ 6Hβ2 max ∥φ ∥2 , (79)
k
s′∈Ss,a
s,a,s′ B− k,1
h
where for the second inequality follows by Lemma 12 and (cid:80) P (s′ | s,a) = 1. Com-
s′∈Ss,a θ¯
bining the results of Eq. (76) and Eq. (79) and , we conclude the proof.
C.3 Good Events with High Probability
In this section, we introduce the good events used to prove Theorem 2 and show that the
good events happen with high probability.
Lemma 17 (Good event probability). For any K ∈ N and δ ∈ (0,1), the good event
G(K,δ′) holds with probability at least 1−δ where δ′ = δ/(2KH).
Proof of Lemma 17. For any δ′ ∈ (0,1), we have
G(K,δ′) = (cid:92) (cid:92) G (δ′) = (cid:92) (cid:92) (cid:110) G∆ (δ′)∩Gξ (δ′)(cid:111) .
k,h k,h k,h
k≤Kh≤H k≤Kh≤H
On the other hand, for any (k,h) ∈ [K]×[H], by Lemma 30 Gξ (δ′) holds with probability
k,h
at least 1−δ′. Then, for δ′ = δ/(2KH) by taking union bound, we have the desired result
as follows:
P(G(K,δ′)) ≥ (1−δ′)2KH ≥ 1−2KHδ′ = 1−δ.
67C.4 Stochastic Optimism
Lemma 18 (Stochastic optimism). For any δ with 0 < δ < Φ(−1)/2, let σ = Hβ (δ). If
k k
log(HU)
we take multiple sample size M = ⌈1− ⌉, then for any k ∈ [K], we have
logΦ(1)
(cid:16) (cid:17)
P (V(cid:101) 1k −V 1∗)(sk 1) ≥ 0 | sk 1,F
k
≥ Φ(−1)/2.
Proof of Lemma 18. First, we introduce the following lemmas.
Lemma 19. Let δ ∈ (0,1) be given. For any (k,h) ∈ [K]×[H], let σ = Hβ (δ). If we
k k
define the event G∆ (δ) as
k,h
(cid:26) (cid:88) (cid:13) (cid:13)
G∆ k,h(δ) := |∆k h(s,a)| ≤ Hβ k(δ) P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B−1
s′∈Ss,a k,h
(cid:27)
+3Hβ (δ)2 max ∥φ ∥2 ,
k
s′∈Ss,a
s,a,s′ B− k,1
h
then conditioned on G∆ (δ), for any (s,a) ∈ S ×A, we have
k,h
(cid:16) (cid:17)
P −ιk(s,a) ≥ 0 | G∆ (δ) ≥ 1−Φ(1)M .
h k,h
Lemma 20. Let δ ∈ (0,1) be given. For any (h,k) ∈ [H] × [K], let σ = Hβ (δ). If
k k
we take multiple sample size M = ⌈1− log(HU) ⌉, then conditioned on the event G∆(δ) :=
logΦ(1) k
∩ G∆ (δ), we have
h∈[H] k,h
(cid:16) (cid:17)
P −ιk(s ,a ) ≥ 0,∀h ∈ [H] | G∆(δ) ≥ Φ(−1).
h h h k
Based on the result of Lemma 20, using the same argument as in Lemma 6 we obtain
the desired result.
In the following section, we provide the proofs of the lemmas used in Lemma 18.
C.4.1 Proof of Lemma 19
Proof of Lemma 19. Recall the definition of Bellman error (Definition 1), we have
(cid:16) (cid:17)
−ιk h(s,a) = Q(cid:101)k h(s,a)− r(s,a)+P hV(cid:101) hk +1(s,a)
(cid:26) (cid:27)
(cid:88) (cid:16) (cid:17)
= min r(s,a)+ P θ(cid:101)k(s′ | s,a)V(cid:101) hk +1(s′)+ν kra ,hnd(s,a) − r(s,a)+P hV(cid:101) hk +1(s,a)
h
s′∈Ss,a
(cid:26) (cid:27)
(cid:88)
≥ min P θ(cid:101)k(s′ | s,a)V(cid:101) hk +1(s′)−P hV(cid:101) hk +1(s,a)+ν kra ,hnd(s,a),0 .
h
s′∈Ss,a
Then, it is enough to show that
(cid:88)
P θ(cid:101)k(s′ | s,a)V(cid:101) hk +1(s′)−P hV(cid:101) hk +1(s,a)+ν kra ,hnd(s,a) ≥ 0
h
s′∈Ss,a
68atleastwithconstantprobability. Ontheotherhand,undertheeventG∆ (δ),byLemma16
k,h
we have
(cid:88)
P θ(cid:101)k(s′ | s,a)V(cid:101) hk +1(s′)−P hV(cid:101) hk +1(s,a)+ν kra ,hnd(s,a)
h
s′∈Ss,a
= ∆k(s,a)+νrand(s,a)
h k,h
(cid:88) (cid:13) (cid:13)
≥ −Hβ k(δ) s′∈Ss,aP
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13) B− k,1
h
−3Hβ k(δ)2 sm ′∈Sa sx ,a∥φ s,a,s′∥2
B− k,1
h
+ν kra ,hnd(s,a)
= (cid:88) P
θ(cid:101)k
h(s′ | s,a)φ¯ s,a,s′(θ(cid:101)k h)⊤ξs k′ ,h−Hβ k(δ) (cid:88) P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13) (cid:13)
(cid:13)
B−1
.
s′∈Ss,a s′∈Ss,a k,h
Note that since ξ(m) ∼ N(0,σ2B−1), it follows that
k,h k k,h
(cid:18) (cid:13) (cid:13)2 (cid:19)
φ¯ s,a,s′(θ(cid:101)k h)⊤ξ( km ,h) ∼ N 0,σ k2(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B−1
, ∀m ∈ [M].
k,h
Therefore, by setting σ = Hβ (δ), we have for m ∈ [M] and s′ ∈ S ,
k k s,a
(cid:18) (cid:13) (cid:13) (cid:19)
P φ¯ s,a,s′(θ(cid:101)k h)⊤ξ( km ,h) ≥ Hβ k(δ)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B−1
= Φ(−1).
k,h
Recall that ξs k′
,h
:= ξm k,h(s′) where m(s′) := argmax m∈[M]φ¯ s,a,s′(θ(cid:101)k h)⊤ξ( km ,h) . Then, we can
deduce
(cid:18) (cid:13) (cid:13) (cid:19)
P φ¯ s,a,s′(θ(cid:101)k h)⊤ξs k′
,h
≥ Hβ k(δ)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13) B−1
k,h
(cid:18) (cid:13) (cid:13) (cid:19)
= P mm ∈a [Mx ]φ¯ s,a,s′(θ(cid:101)k h)⊤ξ( km ,h) ≥ Hβ k(δ)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B−1
k,h
(cid:18) (cid:13) (cid:13) (cid:19)
= 1−P φ¯ s,a,s′(θ(cid:101)k h)⊤ξ( km ,h) < Hβ k(δ)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B−1
,∀m ∈ [M]
k,h
≥ 1−(1−Φ(−1))M
= 1−Φ(1)M . (80)
69Consequently, we arrive at the conclusion as follows:
P(−ιk(s,a) ≥ 0 | G∆ (δ))
h k,h
 
(cid:88)
≥ P  P θ(cid:101)k(s′ | s,a)V(cid:101) hk +1(s′)−P hV(cid:101) hk +1(s,a)+ν kra ,hnd(s,a) ≥ 0 | G∆ k,h(δ)
h
s′∈Ss,a
 
≥ P  (cid:88) P θ(cid:101)k h(s′ | s,a)φ¯ s,a,s′(θ(cid:101)k h)⊤ξs k′ ,h ≥ Hβ k(δ) (cid:88) P θ(cid:101)k h(s′ | s,a)(cid:13) (cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13) (cid:13) (cid:13) B−1 | G∆ k,h(δ)
s′∈Ss,a s′∈Ss,a k,h
(cid:18) (cid:13) (cid:13) (cid:19)
≥ P φ¯ s,a,s′(θ(cid:101)k h)⊤ξs k′
,h
≥ Hβ k(δ)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13) B−1
, ∀s′ ∈ S
s,a
| G∆ k,h(δ)
k,h
(cid:18) (cid:13) (cid:13) (cid:19)
= 1−P ∃s′ ∈ S
s,a
s.t. φ¯ s,a,s′(θ(cid:101)k h)⊤ξs k′
,h
< Hβ k(δ)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13) B−1
| G∆ k,h(δ)
k,h
(cid:18) (cid:13) (cid:13) (cid:19)
≥ 1−UP φ¯ s,a,s′(θ(cid:101)k h)⊤ξs k′
,h
< Hβ k(δ)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13) B−1
| G∆ k,h(δ) (81)
k,h
≥ 1−UΦ(1)M , (82)
where(81)comesfromthefactthatmax |S | = U andtheunionbound,and(82)follows
s,a s,a
by (80).
C.4.2 Proof of Lemma 20
Proof of Lemma 20. It holds
(cid:16) (cid:17) (cid:16) (cid:17)
P −ιk(s ,a ) ≥ 0,∀h ∈ [H] = 1−P ∃h ∈ [H] s.t. −ιk(s ,a ) < 0
h h h h h h
(cid:16) (cid:17)
≥ 1−HP −ιk(s ,a ) < 0
h h h
≥ 1−HUΦ(1)M
≥ Φ(−1)
where the first inequality uses the Bernoulli’s inequality, the second inequality follows by
log(UH)
Lemma 19, and the last inequality holds due to the choice of M = ⌈1− ⌉.
logΦ(1)
C.5 Bound on Estimation Part
Inthissection,weprovidetheupperboundontheestimationpartoftheregret: (cid:80)K (V(cid:101)k−
k=1 1
V∗)(sk).
1 1
Lemma 21 (Bound on estimation). For any δ ∈ (0,1), if λ = O(L2dlogU), then with
φ
probability at least 1−δ/2, we have
(cid:88)K
(V(cid:101)k −Vπk )(sk) =
O(cid:101)(cid:16) d3/2H3/2√
T
+κ−1d2H2(cid:17)
.
1 1 1
k=1
Proof of Lemma 21. With the same argument in Lemma 10, we have
H H
(V(cid:101)k −Vπk )(sk) = (cid:88) −ιk(sk,ak)+(cid:88) ζ˙k, (83)
1 1 1 h h h h
h=1 h=1
70where ζ˙ hk := P h(V(cid:101) hk +1−V hπ +k 1)(sk h,ak h)−(V(cid:101) hk +1−V hπ +k 1)(sk h+1). Note that
(cid:16) (cid:17)
−ιk h(sk h,ak h) = Q(cid:101)k h(sk h,ak h)− r(sk h,ak h)+P hV(cid:101) hk +1(sk h,ak h)
(cid:88)
≤ P θ(cid:101)k(s′ | sk h,ak h)V(cid:101) hk +1(s′)−P hV(cid:101) hk +1(sk h,ak h)+ν kra ,hnd(sk h,ak h)
h
s′∈S
k,h
(cid:12) (cid:12)
≤ (cid:12)∆k(sk,ak)(cid:12)+νrand(sk,ak)
(cid:12) h h h (cid:12) k,h h h
(cid:88) (cid:13) (cid:13)
≤ Hβ
k
s′∈S
P
θ(cid:101)k
h(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h)(cid:13)
(cid:13) B− k,1
h
+3Hβ k2 s′m ∈Sa kx ,h∥φ k,h,s′∥2
B− k,1
h
k,h
+νrand(sk,ak), (84)
k,h h h
where the last inequality follows by Lemma 16. Now we introduce the following lemma.
Lemma 22. For any (k,h) ∈ [K]×[H] and (s,a) ∈ S ×A, it holds
(cid:88) (cid:13) (cid:13)
P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B−1
s′∈Ss,a k,h
≤ s′(cid:88) ∈Ss,aP
θ(cid:101)k
h+1(s′ | s,a)(cid:13) (cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13) (cid:13)
(cid:13)
B− k,1
h
+ 16 √ηL λφ sm ′∈Sa sx ,a(cid:13) (cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h
.
By plugging the result of Lemma 22 into Eq. (84), we have
(cid:88) (cid:13) (cid:13)
−ιk h(sk h,ak h) ≤ Hβ
k
s′∈S
P
θ(cid:101)
hk+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
k,h
16ηL (cid:13) (cid:13)2
+Hβ
k
√ λφ s′m ∈Sa kx ,h(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
+3Hβ k2 s′m ∈Sa kx ,h∥φ k,h,s′∥2
B− k,1
h
+ν kra ,hnd(sk h,ak h)
(cid:88) (cid:13) (cid:13)
≤ Hβ
k
s′∈S
P
θ(cid:101)
hk+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
k,h
+ (cid:88) P θ(cid:101)k(s′ | sk h,ak h)φ¯ k,h,s′(θ(cid:101)k h)⊤ξs k′
,h
h
s′∈S
k,h
16ηL (cid:13) (cid:13)2
+Hβ
k
√ λφ s′m ∈Sa kx ,h(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
+6Hβ k2 s′m ∈Sa kx ,h∥φ k,h,s′∥2
B− k,1
h.
By letting us denote
16ηL (cid:13) (cid:13)2
Υk h(s,a) := Hβ
k
√ λφ sm ′∈Sa sx ,a(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
+6Hβ k2 sm ′∈Sa sx ,a∥φ s,a,s′∥2
B− k,1
h, (85)
71and summing over all episodes, we have
K K H K H
(cid:88) (V(cid:101)k −Vπk )(sk) = (cid:88)(cid:88) −ιk(sk,ak)+(cid:88)(cid:88) ζ˙k
1 1 1 h h h h
k=1 k=1h=1 k=1h=1
K H
(cid:88)(cid:88) (cid:88) (cid:13) (cid:13)
≤ Hβ
K
k=1h=1s′∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
k,h
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
K H
+(cid:88)(cid:88) (cid:88) P θ(cid:101)k(s′ | sk h,ak h)φ¯ k,h,s′(θ(cid:101)k h)⊤ξs k′
,h
h
k=1h=1s′∈S
k,h
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
K H K H
(cid:88)(cid:88) (cid:88)(cid:88)
+ Υk(sk,ak)+ ζ˙k . (86)
h h h h
k=1h=1 k=1h=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(iii) (iv)
For term (i), we have
K H
(cid:88)(cid:88) (cid:88) (cid:13) (cid:13)
k=1h=1s′∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
k,h
(cid:118) (cid:118)
(cid:117) (cid:117)
K H K H
(cid:117)(cid:88)(cid:88) (cid:88) (cid:117)(cid:88)(cid:88) (cid:88) (cid:13) (cid:13)2
≤ (cid:117)
(cid:116)
k=1h=1s′∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:117)
(cid:116)
k=1h=1s′∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
k,h k,h
(cid:118)
(cid:117)
√ (cid:117)(cid:88)H (cid:88)K (cid:88) (cid:13) (cid:13)2
= T(cid:117)
(cid:116)
h=1k=1s′∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
k,h
(cid:115)
√ (cid:18) KUL2 (cid:19)
φ
≤ T 2Hdlog 1+ , (87)
dλ
where the last inequality follows by the following lemma:
Lemma 23. For each h ∈ [H], if λ ≥ L2, then we have
φ
(cid:88) kK =1s′(cid:88)
∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak
h)(cid:13)
(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k
h+1)(cid:13)
(cid:13)
(cid:13)2
B− k,1
h
≤
2dlog(cid:32)
1+
KU dλL2 φ(cid:33)
.
k,h
72Then, term (i) can be bounded as follows:
K H
(cid:88)(cid:88) (cid:88) (cid:13) (cid:13)
(i) = Hβ
K
k=1h=1s′∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
k,h
(cid:115)
√ (cid:18) KUL2 (cid:19)
φ
≤ Hβ T 2Hdlog 1+
K
dλ
√
= O(cid:101)(dH3/2 T). (88)
For term (ii), we introduce the following lemma:
Lemma 24. Let δ ∈ (0,1) be given. For any (k,h) ∈ [K]×[H] and (s,a) ∈ S ×A, with
probability at least 1−δ, it holds
(cid:88) P θ(cid:101)k(s′ | s,a)φ¯ s,a,s′(θ(cid:101)k h)⊤ξs k′
,h
h
s′∈Ss,a
≤ γ k(δ)(cid:18) s′(cid:88)
∈S
P
θ(cid:101)
hk+1(s′ | sk h,ak h)(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13)
(cid:13) B− k,1
h
+ 16 √ηL λφ s′m ∈Sa kx ,h(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h(cid:19) ,
k,h
(cid:112)
where γ (δ) := C σ dlog(Md/δ) for an absolute constant C > 0.
k ξ k ξ
By Lemma 24, we have
K H
(cid:88)(cid:88) (cid:88) P θ(cid:101)k(s′ | sk h,ak h)φ¯ k,h,s′(θ(cid:101)k h)⊤ξs k′
,h
h
k=1h=1s′∈S
k,h
≤ γ K(δ)(cid:18) (cid:88) kK =1(cid:88) hH =1s′(cid:88)
∈S
P
θ(cid:101)
hk+1(s′ | sk h,ak h)(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13)
(cid:13) B− k,1
h
+ 16 √ηL λφ (cid:88) kK =1(cid:88) hH =1s′m ∈Sa kx ,h(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h(cid:19)
k,h
(cid:115)
≤ γ K(δ)(cid:18)√ T 2Hdlog(cid:18) 1+ KU dλL2 φ(cid:19) + 16 √ηL λφ (cid:88) kK =1(cid:88) hH =1s′m ∈Sa kx ,h(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h(cid:19) ,
(89)
73where the last inequality follows by Eq. (87). Note that
K H
(cid:88)(cid:88) (cid:13) (cid:13)2
k=1h=1s′m ∈Sa kx ,h(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101) hk+1)(cid:13)
(cid:13)
B− k,1
h
K H
(cid:88)(cid:88) (cid:13) (cid:13)2
≤ k=1h=1s′m ∈Sa kx ,h(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101) hk+1)(cid:13)
(cid:13)
A− k,1
h
(cid:13) (cid:13)2
K H (cid:13) (cid:13)
= (cid:88) k=1(cid:88) h=1s′m ∈Sa kx ,h(cid:13) (cid:13)
(cid:13)
(cid:13)φ k,h,s′ −
s
(cid:101)∈(cid:88)
S
k,hP θ(cid:101)k h+1(s (cid:101)| sk h,ak h)φ k,h,s (cid:101)(cid:13) (cid:13)
(cid:13)
(cid:13) A−1
k,h
 (cid:13) (cid:13)2 
K H (cid:13) (cid:13)
≤ (cid:88) k=1(cid:88) h=1s′m ∈Sa kx ,h 2∥φ k,h,s′∥2 A− k,1
h
+2(cid:13) (cid:13) (cid:13)
(cid:13)s
(cid:101)∈(cid:88)
S
k,hP θ(cid:101)k h+1(s (cid:101)| sk h,ak h)φ k,h,s (cid:101)(cid:13) (cid:13) (cid:13)
(cid:13)
A−1 
k,h
K H K H
(cid:88)(cid:88) (cid:88)(cid:88) (cid:88)
≤ 2 max ∥φ ∥2 +2 P (s | sk,ak)∥φ ∥2
k=1h=1s′∈S k,h
k,h,s′ A− k,1
h k=1h=1s∈S
θ(cid:101)k h+1 (cid:101) h h k,h,s
(cid:101)
A− k,1
h
(cid:101) k,h
K H
(cid:88)(cid:88)
≤ 4 max ∥φ ∥2
k=1h=1s′∈S k,h
k,h,s′ A− k,1
h
(cid:32) (cid:33)
KUL2
≤ 16κ−1dHlog 1+ φ , (90)
dλ
where the first inequality holds since B−1 ⪯ A−1, the second inequality follows from
k,h k,h
(x+y)2 ≤ 2x2 +2y2, and the third inequality uses the triangle inequality, and the fourth
inequality uses (cid:80) P (s | sk,ak) = 1, and the last inequality follows by Lemma 3.
s (cid:101)∈S k,h θ(cid:101)k+1 (cid:101) h h
h
By substituting Eq. (90) into Eq. (89), we have
(cid:18)√ (cid:113) 256ηL (cid:19)
(ii) ≤ γ (δ) T
2Hdlog(cid:0) 1+KUL2/(dλ)(cid:1)
+ √
φ κ−1dHlog(cid:0) 1+KUL2/(dλ)(cid:1)
K φ φ
λ
√
= O(cid:101)(d3/2H3/2 T +κ−1d3/2H2). (91)
For term (iii),
(cid:88) kK =1(cid:88) hH =1Υk h(sk h,ak h) = (cid:88) kK =1(cid:88) hH =1(cid:18) Hβ k16 √ηL λφ s′m ∈Sa kx ,h(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h
+6Hβ k2 s′m ∈Sa kx ,h∥φ k,h,s′∥2
B− k,1
h(cid:19)
K H K H
≤ Hβ K16 √ηL λφ (cid:88) k=1(cid:88) h=1s′m ∈Sa kx ,h(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h
+6Hβ K2 (cid:88) k=1(cid:88) h=1s′m ∈Sa kx ,h∥φ k,h,s′∥2
A− k,1
h
256ηL
≤ β √ φ κ−1dH2log(cid:0) 1+KUL2/(dλ)(cid:1) +24κ−1dH2β2 log(cid:0) 1+KUL2/(dλ)(cid:1)
K φ K φ
λ
= O(cid:101)(κ−1d2H2), (92)
where for the second inequality we use the same argument used to derive Eq. (90) and
Lemma 3.
74For term (iv), since we have |ζ˙k| ≤ 2H and E[ζ˙k | F ] = 0, which means {ζ˙k | F }
h h k,h h k,h k,h
is a martingale difference sequence for any k ∈ [K] and h ∈ [H]. Hence, by applying the
Azuma-Hoeffding inequality with probability at least 1−δ/4, we have
K H
(cid:88)(cid:88) ζ˙k ≤ 2H(cid:112) 2KHlog(4/δ). (93)
h
k=1h=1
Combining all results of Eq. (88), (91), (92), and (93), we have the desired result.
K √ √ √
(cid:88) (V(cid:101)k −Vπk )(sk) = O(cid:101)(dH3/2 T +d3/2H3/2 T +κ−1d3/2H2+κ−1d2H2+H T)
1 1 1
k=1
√
= O(cid:101)(d3/2H3/2 T +κ−1d2H2).
In the following, we provide the proof of the lemmas used in Lemma 21.
C.5.1 Proof of Lemma 22
Proof of Lemma 22. Note that
(cid:88) (cid:13) (cid:13)
P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B−1
s′∈Ss,a k,h
(cid:88) (cid:13) (cid:13) (cid:88) (cid:13) (cid:13)
≤ P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
+ P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)−φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
s′∈Ss,a k,h s′∈Ss,a k,h
(cid:88) (cid:13) (cid:13)
≤ s′∈Ss,aP
θ(cid:101)k
h+1(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
(cid:88) (cid:18) (cid:19)(cid:13) (cid:13)
+
s′∈Ss,a
P
θ(cid:101)k
h(s′ | s,a)−P
θ(cid:101)k
h+1(s′ | s,a) (cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
(cid:88) (cid:13) (cid:13)
+ P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)−φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
,
s′∈Ss,a k,h
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
where the first inequality holds by triangle inequality.
For (i), we have
(cid:88) (cid:13) (cid:13)
(i) = ∇P
ϑk
h(s′ | s,a)⊤(θ(cid:101)k h−θ(cid:101)k h+1)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
s′∈Ss,a k,h
(cid:88) (cid:13) (cid:13) (cid:13) (cid:13)
≤ s′∈Ss,a∥∇P
ϑk
h(s′ | s,a)∥
B− k,1
h(cid:13) (cid:13)θ(cid:101)k h−θ(cid:101)k h+1(cid:13)
(cid:13)
B
k,h(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
(94)
75where in the equality we apply the mean value theorem with ϑk = vθ(cid:101)k +(1−v)θ(cid:101)k+1 for
h h h
some v ∈ [0,1], and the inequality follows by Cauchy-Schwarz inequality. Meanwhile, since
we have
(cid:18) (cid:19)
(cid:88)
P ϑk(s′ | s,a) φ¯ s,a,s′(θ(cid:101)k h+1)− P ϑk(s′′ | s,a)φ¯ s,a,s′′(θ(cid:101)k h+1) (95)
h h
s′′∈Ss,a
(cid:32)
(cid:88)
= P (s′ | s,a) φ − P (s | s,a)φ
ϑk
h
s,a,s′ θ(cid:101)k h+1 (cid:101) s,a,s
(cid:101)
s (cid:101)∈Ss,a
(cid:34) (cid:35)(cid:33)
(cid:88) (cid:88)
− P (s′′ | s,a) φ − P (s | s,a)φ
s′′∈Ss,a
ϑk
h
s,a,s′′
s
(cid:101)
θ(cid:101)k h+1 (cid:101) s,a,s
(cid:101)
(cid:88)
= P (s′ | s,a)φ −P (s′ | s,a) P (s | s,a)φ
ϑk
h
s,a,s′ ϑk
h
θ(cid:101)k h+1 (cid:101) s,a,s
(cid:101)
s (cid:101)∈Ss,a
(cid:88)
−P (s′ | s,a) P (s′′ | s,a)φ
ϑk ϑk s,a,s′′
h h
s′′∈Ss,a
(cid:18) (cid:19)
(cid:88) (cid:88)
+P (s′ | s,a) P (s′′ | s,a) P (s | s,a)φ
ϑk
h
s′′∈Ss,a
ϑk
h
s
(cid:101)
θ(cid:101)k h+1 (cid:101) s,a,s
(cid:101)
(cid:124) (cid:123)(cid:122) (cid:125)
1
(cid:88)
= P (s′ | s,a)φ −P (s′ | s,a) P (s′′ | s,a)φ
ϑk s,a,s′ ϑk ϑk s,a,s′′
h h h
s′′∈Ss,a
= ∇P (s′ | s,a),
ϑk
h
by substituting (95) into (94) we have
(cid:13) (cid:13)
(cid:40)(cid:13) (cid:13)
(i) ≤ (cid:88) (cid:13) (cid:13) (cid:13)P ϑk h(s′ | s,a)φ¯ s,a,s′(θ(cid:101)k h+1)−P ϑk h(s′ | s,a) (cid:88) P ϑk h(s′′ | s,a)φ¯ s,a,s′′(θ(cid:101)k h+1)(cid:13) (cid:13)
(cid:13)
s′∈Ss,a (cid:13) s′′∈Ss,a (cid:13) B−1
k,h
(cid:41)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)θ(cid:101)k h−θ(cid:101)k h+1(cid:13)
(cid:13)
B
(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
k,h k,h
(cid:88) (cid:13) (cid:13)2 (cid:13) (cid:13)
≤ P
ϑk
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
(cid:13) (cid:13)θ(cid:101)k h−θ(cid:101)k h+1(cid:13)
(cid:13)
B
s′∈Ss,a k,h k,h
(cid:18) (cid:88) (cid:13) (cid:13) (cid:19)2(cid:13) (cid:13)
+ P
ϑk
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
(cid:13) (cid:13)θ(cid:101)k h−θ(cid:101)k h+1(cid:13)
(cid:13)
B
. (96)
s′∈Ss,a k,h k,h
76Note that by Jensen’s inequality, we have
(cid:18) (cid:88) (cid:13) (cid:13) (cid:19)2 (cid:18) (cid:20)(cid:13) (cid:13) (cid:21)(cid:19)2
s′∈Ss,aP
ϑk
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
= E
s′∼P ϑk h(·|s,a)
(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
(cid:20)(cid:13) (cid:13)2 (cid:21)
≤ E
s′∼P ϑk h(·|s,a)
(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) B− k,1
h
(cid:88) (cid:13) (cid:13)2
= P
ϑk
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
.
s′∈Ss,a k,h
(97)
Also, we introduce the following lemma:
Lemma 25. For any k ∈ [K] and h ∈ [H], the following holds:
(cid:13) (cid:13) 4ηL
(cid:13)θ(cid:101)k+1−θ(cid:101)k(cid:13) ≤ √ φ .
(cid:13) h h(cid:13)
B λ
k,h
Then, substituting (97) into (96), we have
(cid:88) (cid:13) (cid:13)2 (cid:13) (cid:13)
(i) ≤ 2 P
ϑk
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
(cid:13) (cid:13)θ(cid:101)k h−θ(cid:101)k h+1(cid:13)
(cid:13)
B
s′∈Ss,a k,h k,h
≤ 8 √ηL λφ (cid:88) P
ϑk
h(s′ | s,a)(cid:13) (cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B−1
s′∈Ss,a k,h
8ηL (cid:13) (cid:13)2
≤ √ λφ sm ′∈Sa sx ,a(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
, (98)
where the second inequality comes from Lemma 25, and the last inequality holds due to
(cid:80) P (s′ | s,a) = 1.
s′∈Ss,a ϑk
h
For (ii), we have
(cid:88) (cid:13) (cid:13)
(ii) = P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)−φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B−1
s′∈Ss,a k,h
(cid:13) (cid:13)
= s′(cid:88) ∈Ss,aP θ(cid:101)k h(s′ | s,a)(cid:13) (cid:13) (cid:13)E s (cid:101)∼P θ(cid:101)k h(·|s,a)(cid:2) φ s,a,s (cid:101)(cid:3) −E s (cid:101)∼P θ(cid:101)k h+1(·|s,a)(cid:2) φ s,a,s (cid:101)(cid:3)(cid:13) (cid:13) (cid:13)
B− k,1
h
(cid:13) (cid:13)
(cid:13) (cid:18) (cid:19) (cid:13)
(cid:13) (cid:88) (cid:13)
= (cid:13)
(cid:13)
(cid:13)s (cid:101)∈Ss,a
P θ(cid:101)k h(s (cid:101)| s,a)−P θ(cid:101)k h+1(s (cid:101)| s,a) φ s,a,s (cid:101)(cid:13)
(cid:13)
(cid:13) B−1
k,h
(cid:13) (cid:13)
(cid:13) (cid:18) (cid:19)(cid:18) (cid:19)(cid:13)
= (cid:13) (cid:13) (cid:13)
(cid:13)s
(cid:101)∈(cid:88)
Ss,a
P θ(cid:101)k h(s (cid:101)| s,a)−P θ(cid:101)k h+1(s (cid:101)| s,a) φ s,a,s (cid:101)−E s′∼P θ(cid:101)k h+1(·|s,a)(cid:2) φ s,a,s′(cid:3) (cid:13) (cid:13) (cid:13)
(cid:13) B−1
k,h
(cid:13) (cid:13)
(cid:13) (cid:18) (cid:19) (cid:13)
= (cid:13) (cid:13)
(cid:13)
(cid:13)s
(cid:101)∈(cid:88)
Ss,a
P θ(cid:101)k h(s (cid:101)| s,a)−P θ(cid:101)k h+1(s (cid:101)| s,a) φ¯ s,a,s (cid:101)(θ(cid:101)k h+1)(cid:13) (cid:13)
(cid:13)
(cid:13) B−1
k,h
8ηL (cid:13) (cid:13)2
≤ √ λφ sm ′∈Sa sx ,a(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
, (99)
77where the last inequality is obtained through the same argument as used to bound (i).
Combining the results of Eq. (98) and Eq. (99), we have
(cid:88) (cid:13) (cid:13)
P
θ(cid:101)k
h(s′ | s,a)(cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B−1
s′∈Ss,a k,h
≤ s′(cid:88) ∈Ss,aP
θ(cid:101)k
h+1(s′ | s,a)(cid:13) (cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13) (cid:13)
(cid:13)
B− k,1
h
+ 16 √ηL λφ sm ′∈Sa sx ,a(cid:13) (cid:13) (cid:13)φ¯ s,a,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h
C.5.2 Proof of Lemma 23
Proof of Lemma 23. Note that
(cid:88)
B
k+1,h
= B k,h+ P θ(cid:101)k+1(s′ | sk h,ak h)φ¯ k,h,s′(θ(cid:101)k h+1)φ¯ k,h,s′(θ(cid:101)k h+1)⊤
s′∈S h
k,h
(cid:88)
= B k,h+ φ (cid:101)k,h,s′(θ(cid:101)k h+1)φ (cid:101)k,h,s′(θ(cid:101)k h+1)⊤,
s′∈S
k,h
(cid:113)
where we define φ (cid:101)k,h,s′(θ(cid:101)k h+1) := P θ(cid:101)k+1(s′ | sk h,ak h)φ¯ k,h,s′(θ(cid:101)k h+1). Then, we have
h
 
det(B k+1,h) = det(B k,h)detI d+B− k,h1 2 (cid:88) φ (cid:101)k,h,s′(θ(cid:101)k h+1)φ (cid:101)k,h,s′(θ(cid:101)k h+1)⊤B− k,h1 2
s′∈S
k,h
 
(cid:88) (cid:13) (cid:13)2
= det(B k,h)1+ (cid:13) (cid:13)φ (cid:101)k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) B−1
s′∈S k,h
k,h
 
K
(cid:89) (cid:88) (cid:13) (cid:13)2
= det(λI d) 1+ (cid:13) (cid:13)φ (cid:101)k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) B−1 .
k=1 s′∈S k,h
k,h
Taking the logarithm on both sides yields
 
K
log de dt e( tB (λk+
I
d1 ),h) = (cid:88) k=1log1+ s′(cid:88)
∈S
(cid:13) (cid:13) (cid:13)φ (cid:101)k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h .
k,h
On the other hand, since λ ≥ L2,
φ
(cid:88) (cid:13) (cid:13)2 (cid:88) 1 (cid:13) (cid:13)2
(cid:13) (cid:13)φ (cid:101)k,h,s′(θ(cid:101) hk+1)(cid:13)
(cid:13) B−1
≤
λ
(cid:13) (cid:13)φ (cid:101)k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13) 2
s′∈S k,h s′∈S
k,h k,h
(cid:88) 1 (cid:13) (cid:13)2
=
s′∈S
λP
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
2
k,h
≤
L2
φ (cid:88) P (s′ | sk,ak)
λ θ(cid:101)k+1 h h
s′∈S h
k,h
≤ 1,
78where the last inequality uses (cid:80) P (s′ | sk,ak) = 1. From the fact that z ≤
s′∈S k,h θ(cid:101)k+1 h h
h
2log(1+z) for any z ∈ [0,1], it follows that
 
K K
(cid:88) (cid:88) (cid:13) (cid:13)2 (cid:88) 1 (cid:88) (cid:13) (cid:13)2
log1+ (cid:13) (cid:13)φ (cid:101)k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) B−1 ≥ 2 (cid:13) (cid:13)φ (cid:101)k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) B−1 .
k=1 s′∈S k,h k=1 s′∈S k,h
k,h k,h
Finally, we obtain
 
K K
(cid:88) (cid:88) (cid:13) (cid:13)2 (cid:88) (cid:88) (cid:13) (cid:13)2
(cid:13) (cid:13)φ (cid:101)k,h,s′(θ(cid:101) hk+1)(cid:13) (cid:13)
B−1
≤ 2 log1+ (cid:13) (cid:13)φ (cid:101)k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) B−1
k=1s′∈S k,h k=1 s′∈S k,h
k,h k,h
det(B )
K+1,h
= 2log
det(λI )
d
(cid:32) (cid:33)
KUL2
φ
≤ 2dlog 1+ ,
dλ
where the last inequality follows by the determinant-trace inequality (Lemma 28).
C.5.3 Proof of Lemma 24
Proof of Lemma 24. Since ξ(m) ∼ N(0,σ2B−1), by Lemma 30 for each m ∈ [M], we have
k,h k k,h
(m) (cid:112)
∥ξ ∥ ≤ C σ dlog(Md/δ).
k,h B k,h ξ k
Following the result of Lemma 22, we have
(cid:88) (cid:13) (cid:13) (cid:88) (cid:13) (cid:13)
s′∈S
P
θ(cid:101)k
h(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h)(cid:13)
(cid:13)
B− k,1
h
≤
s′∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
k,h k,h
16ηL (cid:13) (cid:13)2
+ √ λφ s′m ∈Sa kx ,h(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
.
Then, we obtain
(cid:88) P θ(cid:101)k(s′ | sk h,ak h)φ¯ k,h,s′(θ(cid:101)k h)⊤ξs k′
,h
h
s′∈S
k,h
≤ (cid:88) P
θ(cid:101)k
h(s′ | sk h,ak h)(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h)(cid:13) (cid:13)
(cid:13) B−1
∥ξs k′ ,h∥
B k,h
s′∈S k,h
k,h
(cid:112) (cid:88)
≤ C ξσ
k
dlog(Md/δ) P
θ(cid:101)k
h(s′ | sk h,ak h)∥φ¯ k,h,s′(θ(cid:101)k h)∥
B− k,1
h
s′∈S
k,h
≤ γ k(δ)(cid:18) s′(cid:88)
∈S
P
θ(cid:101)
hk+1(s′ | sk h,ak h)(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13)
(cid:13) B− k,1
h
+ 16 √ηL λφ s′m ∈Sa kx ,h(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h(cid:19) .
k,h
79C.5.4 Proof of Lemma 25
Proof of Lemma 25. We provide a proof for Lemma 25 since it is slight modification of
Lemma 20 of (Zhang and Sugiyama, 2023). From the definition, we know that
(cid:16) (cid:17)⊤ 1 (cid:13) (cid:13)2 (cid:16) (cid:17)⊤
θ(cid:101)k h+1 ∇ℓ k,h(θ(cid:101)k h)+
2η
(cid:13) (cid:13)θ(cid:101)k h+1−θ(cid:101)k h(cid:13)
(cid:13) B(cid:101)k,h
≤ θ(cid:101)k
h
∇ℓ k,h(θ(cid:101)k h).
By rearranging the terms, the following holds:
1 (cid:13) (cid:13)2 (cid:16) (cid:17)⊤
2η
(cid:13) (cid:13)θ(cid:101)k h+1−θ(cid:101)k h(cid:13)
(cid:13) B(cid:101)k,h
≤ θ(cid:101)k h−θ(cid:101)k h+1 ∇ℓ k,h(θ(cid:101)k h)
(cid:13) (cid:13) (cid:13) (cid:13)
≤ (cid:13) (cid:13)θ(cid:101)k h−θ(cid:101)k h+1(cid:13)
(cid:13)
B(cid:101)k,h(cid:13) (cid:13)∇ℓ k,h(θ(cid:101)k h)(cid:13)
(cid:13)
B(cid:101)− k,1
h
Thus, we get
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)θ(cid:101)k h+1−θ(cid:101)k h(cid:13)
(cid:13)
B(cid:101)k,h
≤ 2η(cid:13) (cid:13)∇ℓ k,h(θ(cid:101)k h)(cid:13)
(cid:13)
B(cid:101)− k,1
h
.
Since B
k,h
⪯ B(cid:101)k,h and B(cid:101)− k,1
h
⪯ λ−1I d, we obtain
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2η (cid:13) (cid:13) 4ηL
(cid:13) (cid:13)θ(cid:101)k h+1−θ(cid:101)k h(cid:13)
(cid:13)
B
k,h
≤ (cid:13) (cid:13)θ(cid:101)k h+1−θ(cid:101)k h(cid:13)
(cid:13)
B(cid:101)k,h
≤ 2η(cid:13) (cid:13)∇ℓ k,h(θ(cid:101)k h)(cid:13)
(cid:13)
B(cid:101)− k,1
h
≤ √
λ
(cid:13) (cid:13)∇ℓ k,h(θ(cid:101)k h)(cid:13)
(cid:13)
2
≤ √ λφ .
(100)
For the last inequality of (100), we provide the upper bound of l -norm of ∇ℓ (θ). Since
2 k,h
(cid:88)
ℓ (θ) = − yk(s′)logP (s′ | sk,ak),
k,h h θ h h
s′∈S
k,h
the gradient of the loss function is given by
 
(cid:88) (cid:88)
∇ℓ k,h(θ) = − y hk(s′)φ s,a,s′ − P θ(s′′ | sk h,ak h)φ s,a,s′′
s′∈S s′′∈S
k,h k,h
(cid:88) (cid:88) (cid:88)
= yk(s′) P (s′′ | sk,ak)φ − yk(s′)φ
h θ h h s,a,s′′ h s,a,s′
s′∈S s′′∈S s′∈S
k,h k,h k,h
(cid:88) (cid:88)
= P (s′′ | sk,ak)φ − yk(s′)φ
θ h h s,a,s′′ h s,a,s′
s′′∈S s′∈S
k,h k,h
(cid:88) (cid:16) (cid:17)
= P (s′ | sk,ak)−yk(s′) φ .
θ h h h s,a,s′
s′∈S
k,h
Therefore, we have
(cid:13) (cid:13)
(cid:13) (cid:13)
∥∇ℓ k,h(θ)∥
2
= (cid:13) (cid:13)
(cid:13)
(cid:88) (cid:16) P θ(s′ | sk h,ak h)−y hk(s′)(cid:17) φ s,a,s′(cid:13) (cid:13)
(cid:13)
(cid:13)s′∈S (cid:13)
k,h 2
(cid:88) (cid:12) (cid:12)
≤ (cid:12)P (s′ | sk,ak)−yk(s′)(cid:12)∥φ ∥
(cid:12) θ h h h (cid:12) s,a,s′ 2
s′∈S
k,h
≤ 2L
φ
and this concludes the proof.
80C.6 Bound on Pessimism Part
Inthissection,weprovidetheupperboundonthepessimismpartoftheregret: (cid:80)K (V∗−
k=1 1
V(cid:101)k)(sk).
1 1
Lemma 26 (Bound on pessimism). For any δ with 0 < δ < Φ(−1)/2, let σ = Hβ . If
k k
λ = O(L2dlogU) and we take multiple sample size M = ⌈1−log(HU) ⌉, then with probability
φ logΦ(1)
at least 1−δ/2, we have
(cid:88)K (cid:16) √ (cid:17)
(V∗−Vk)(sk) = O(cid:101) d3/2H3/2 T +κ−1d2H2 .
1 1 1
k=1
Proof of Lemma 26. As seen in Lemma 18, by using multiple sampling technique we show
that the optimistic randomized value function V(cid:101) of ORRL-MNL is optimistic than the true
optimal value with constant probability Hence, with the same argument used in Lemma 11,
we can show that the pessimism term of ORRL-MNL is upper bounded by a bound of the
estimation term times the inverse probability of being optimistic, i.e.,
K (cid:32) K (cid:33)
(cid:88)(cid:16) V∗−Vk(cid:17)
(sk) ≤ O(cid:101)
1 (cid:88)(cid:16)
Vk
−Vπk(cid:17)
(sk) .
1 1 1 Φ(−1) 1 1 1
k=1 k=1
C.7 Regret Bound of ORRL-MNL
Theorem (Restatement of Theorem 2). Suppose that Assumption 1- 4 hold. For any
0 < δ < Φ(−1) , if we set the input parameters in Algorithm 2 as λ = O(L2dlogU),β =
√ 2 φ k
log(HU)
O( dlogUlog(kH)),σ = Hβ , M = ⌈1− ⌉, and η = O(logU), then with probability
k k logΦ(1)
at least 1−δ, the cumulative regret of the ORRL-MNL policy π is upper-bounded by
(cid:16) √ (cid:17)
Regret (K) = O(cid:101) d3/2H3/2 T +κ−1d2H2 .
π
Proof of Theorem 2. Since both Lemma 21 and Lemma 26 holds with probability at least
1−δ/2 respectively, by taking the union bound we conclude the proof.
D. Optimistic Exploration Extension
In this section, we introduce UCRL-MNL+ (Algorithm 3), which is both computationally and
statistically efficient for MNL-MDPs with UCB-based exploration. The main difference
compared to ORRL-MNL is that UCRL-MNL+ constructs an optimistic value function that is
greater than the optimal value function with high probability. At each episode k ∈ [K],
with the estimated transition core parameter θ(cid:101)k (5), for (s,a) ∈ S×A, set Qˆk (s,a) = 0.
h H+1
For each h ∈ [H],
Qˆk(s,a) := r(s,a)+ (cid:88) P (s′ | s,a)Vˆk (s′)+νopt(s,a), (101)
h θ(cid:101)k h+1 k,h
h
s′∈Ss,a
81whereVˆk(s) := min{max Qˆk(s,a),H}andνopt(s,a)istheoptimisticbonusterm defined
h a∈A h k,h
by
ν ko ,p ht(s,a) := Hβ
k
s′(cid:88) ∈Ss,aP
θ(cid:101)k
h(s′ | s,a)∥φ¯(s,a,s′;θ(cid:101)k h)∥
B− k,1
h
+3Hβ k2 sm ′∈Sa sx ,a∥φ(s,a,s′)∥2
B− k,1
h.
Basedontheseoptimistic value function Qˆk,ateachepisodetheagentplaysagreedyaction
h
with respect to Qˆk as summarized in Algorithm 3.
h
Algorithm 3 UCRL-MNL+ (Upper Confidence RL for MNL-MDPs)
1: Inputs: Episodic MDP M, Feature map φ : S ×A×S → Rd, Number of episodes K,
Regularization parameter λ, Confidence radius {β }K , Step size η
k k=1
2: Initialize: θ(cid:101)1 h = 0 d, B 1,h = λI d for all h ∈ [H]
3: for episode k = 1,2,··· ,K do
(cid:110) (cid:111)
4: Observe sk and set Qˆk(·,·) as described in (101)
1 h
h∈[H]
5: for horizon h = 1,2,··· ,H do
6: Select ak = argmax Qˆk(sk,a) and observe sk
h a∈A h h h+1
7: Update B(cid:101)k,h = B k,h+η∇2ℓ k,h(θ(cid:101)k h) and θ(cid:101)k h+1 as in (5)
8: Update B k+1,h = B k,h+∇2ℓ k,h(θ(cid:101)k h+1)
9: end for
10: end for
The main difference in regret analysis lies in ensuring the optimism of the estimated
valuefunctionQˆk (Lemma27). Inthefollowingstatement(formalstatementofCorollary1),
h
we provide a regret guarantee for UCRL-MNL+, which enjoys the tightest regret bound for
MNL-MDPs.
Theorem 3 (Regret Bound of UCRL-MNL+). Suppose that Assumption 1- 4 hold. For any
δ ∈ (0,1), if we set the input parameters in Algorithm 3 as λ = O(L2dlogU),β =
√ φ k
O( dlogUlog(kH)) η = O(logU), then with probability at least 1 − δ, the cumulative
regret of the UCRL-MNL+ policy π is upper-bounded by
(cid:16) √ (cid:17)
Regret (K) = O(cid:101) dH3/2 T +κ−1d2H2 .
π
Proof of Theorem 3. ByLemma17,supposethatthegoodeventG(K,δ′)holdswithproba-
bilityatleast1−δ. Then,weshowthattheoptimisticvaluefunctionQˆk isdeterministically
h
greater than the true optimal value function as follows:
Lemma 27 (Optimism). Suppose that the event G∆ (δ) holds for all k ∈ [K] and h ∈ [H].
k,h
Then for any (s,a) ∈ S ×A, we have
Q∗(s,a) ≤ Qˆk(s,a).
h h
Conditioned on G(K,δ′), by Lemma 27 we have
(V∗−Vπk )(sk) = Q∗(sk,π∗(sk))−Qπk (sk,ak)
1 1 1 1 1 1 1 1 1
≤ Qˆk(sk,π∗(sk))−Qπk (sk,ak)
1 1 1 1 1 1
≤ Qˆk(sk,ak)−Qπk (sk,ak) = νopt(sk,ak)+P (Vˆk −Vπk )(sk,ak).
1 1 1 1 1 1 k,1 1 1 1 2 2 1 1
82Note that
(cid:104) (cid:105)
P (Vˆk −Vπk )(sk,ak) = E (Vˆk −Vπk )(s) = (Vˆk −Vπk )(sk)+ζ˙k,
1 2 2 1 1 s|sk,ak 2 2 (cid:101) 2 2 2 1
(cid:101) 1 1
(cid:104) (cid:105)
where we denote ζk := (Vˆk −Vπk )(sk )−E (Vˆk −Vπk )(s) . Then, with the
h h+1 h+1 h+1 s|sk,ak h+1 h+1 (cid:101)
(cid:101) h h
same argument, we have
H H
(V∗−Vπk )(sk) ≤ (cid:88) νopt(sk,ak)+(cid:88) ζ˙k.
1 1 1 k,h h h h
h=1 h=1
By summing over all episodes, we have
K H K H
Regret (K) ≤
(cid:88)(cid:88) νopt(sk,ak)+(cid:88)(cid:88)
ζ˙k. (102)
π k,h h h h
k=1h=1 k=1h=1
On the other hand, note that
K H
(cid:88)(cid:88) νopt(sk,ak)
k,h h h
k=1h=1
K H K H
(cid:88)(cid:88) (cid:88) (cid:88)(cid:88)
= k=1h=1Hβ
k
s′∈S
P
θ(cid:101)k
h(s′ | sk h,ak h)∥φ¯ k,h,s′(θ(cid:101)k h)∥
B− k,1
h
+ k=1h=13Hβ k2 s′m ∈Sa kx ,h∥φ k,h,s′∥2
B− k,1
h
k,h
K H K H
(cid:88)(cid:88) (cid:88) (cid:88)(cid:88)
≤ Hβ
K
k=1h=1s′∈S
P
θ(cid:101)k
h(s′ | sk h,ak h)∥φ¯ k,h,s′(θ(cid:101)k h)∥
B− k,1
h
+3Hβ K2 k=1h=1s′m ∈Sa kx ,h∥φ k,h,s′∥2
B− k,1
h
k,h
K H
(cid:88)(cid:88) (cid:88) (cid:13) (cid:13)
≤ Hβ
K
k=1h=1s′∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
k,h
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
K H K H
+ 16 √ηL λφ Hβ K(cid:88) k=1(cid:88) h=1s′m ∈Sa kx ,h(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h+3Hβ K2 (cid:88) k=1(cid:88) h=1s′m ∈Sa kx ,h∥φ k,h,s′∥2
B− k,1
h,
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(ii) (iii)
where the last inequality follows by Lemma 22.
Term (i) can be bounded as in Eq. (88):
(cid:88)K (cid:88)H (cid:88) (cid:13) (cid:13) √
Hβ
K
k=1h=1s′∈S
P
θ(cid:101)k
h+1(s′ | sk h,ak h)(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13)
(cid:13)
B− k,1
h
= O(cid:101)(dH3/2 T). (103)
k,h
For term (ii), recall that as in Eq. (90) we have
(cid:88) kK =1(cid:88) hH
=1s′m ∈Sa kx
,h(cid:13)
(cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k
h+1)(cid:13)
(cid:13)
(cid:13)2
B− k,1
h
≤
16κ−1dHlog(cid:32)
1+
KU dλL2 φ(cid:33)
.
83Then, we have
K H
16 √ηL λφ Hβ K(cid:88) k=1(cid:88) h=1s′m ∈Sa kx ,h(cid:13) (cid:13) (cid:13)φ¯ k,h,s′(θ(cid:101)k h+1)(cid:13) (cid:13) (cid:13)2
B− k,1
h
= O(cid:101)(κ−1dH2). (104)
For term (iii), since we have
K H K H
(cid:88)(cid:88) (cid:88)(cid:88)
3Hβ2 max ∥φ ∥2 ≤ 3Hβ2 max ∥φ ∥2
K
k=1h=1s′∈S k,h
k,h,s′ B− k,1
h
K
k=1h=1s′∈S k,h
k,h,s′ A− k,1
h
≤ 12κ−1dH2β2 log(cid:0) 1+KUL2/(dλ)(cid:1)
K φ
= O(cid:101)(κ−1d2H2). (105)
Combining the results of Eq. (103), (104), and (105), we have
K H √
(cid:88)(cid:88) νopt(sk,ak) = O(cid:101)(dH3/2 T +κ−1d2H2).
k,h h h
k=1h=1
Finally, by Azuma-Hoeffiding inequality as in Eq. (93) we have
K H √
(cid:88)(cid:88)
ζ˙k = O(cid:101)(H T).
h
k=1h=1
This concludes the proof.
In the following, we provide the proof of Lemma 27.
D.1 Optimism
Proof of Lemma 27. We prove this by backwards induction on h. For the base case h = H,
since V∗ (s) = Vˆk (s) = 0 for all s ∈ S, we have
H+1 H+1
Qˆk (s,a) = r(s,a) = Q∗ (s,a).
H H
Suppose that the statement holds for h+1 where h ∈ [H −1]. Then, for h and for any
(s,a) ∈ S ×A,
Qˆk(s,a)
h
= r(s,a)+ (cid:88) P (s′ | s,a)Vˆk (s′)+νopt(s,a)
θ(cid:101)k h+1 k,h
h
s′∈Ss,a
≥ r(s,a)+ (cid:88) P (s′ | s,a)V∗ (s′)+νopt(s,a)
θ(cid:101)k h+1 k,h
h
s′∈Ss,a
= r(s,a)+ (cid:88) P θ∗ h(s′ | s,a)V h∗ +1(s′)+ (cid:88) (cid:16) P θ(cid:101)k h(s′ | s,a)−P θ∗ h(s′ | s,a)(cid:17) V h∗ +1(s′)+ν ko ,p ht(s,a)
s′∈Ss,a s′∈Ss,a
(cid:88)
≥ r(s,a)+ P θ∗(s′ | s,a)V h∗ +1(s′)
h
s′∈Ss,a
= Q∗(s,a),
h
where the first inequality follows from the induction hypothesis and the second inequality
holds by Lemma 16.
84E. Auxiliary Lemmas
Lemma28(Determinant-traceinequality(Abbasi-Yadkorietal.,2011)). Supposex ,...,x ∈
1 t
Rd and for any 1 ≤ τ ≤ t, ∥x ∥ ≤ L. Let V = λI +(cid:80)t x x⊤ for some λ > 0. Then,
τ 2 t d τ=1 τ τ
det(V ) ≤ (λ+tL2/d)d.
t
Lemma 29 (Freedman’s inequality (Freedman, 1975)). Consider a real-valued martingale
{Y : k = 0,1,2,...} with difference sequence {X : k = 0,1,2,3,...}. Assume that the
k k
difference sequence is uniformly bounded, X ≤ R almost surely for k = 1,2,3,.... Define
k
the predictable quadratic variation process of the martingale:
k
(cid:88)
W := E [X2] for k = 1,2,3,....
k j−1 j
j=1
Then, for all t ≥ 0 and σ2 > 0,
P(cid:0)
∃k ≥ 0 : Y ≥ t and W ≤
σ2(cid:1)
≤
exp(cid:18)
−
−t2/2 (cid:19)
.
k k σ2+Rt/3
Lemma 30 (Gaussian noise concentration (Lemma D.2 in (Ishfaq et al., 2021))). Let
ξ(1),ξ(2),...,ξ(M) be M independent d-dimensional multivariate normal distributed vector
with mean 0 and covariance σ2A−1 for some σ > 0 and a positive definite matrix A−1,
d
i.e., ξ(m) ∼ N(0 ,σ2A−1) for m ∈ [M]. Then for any δ ∈ (0,1), with probability at least
d
1−δ, we have
(cid:112)
max ∥ξ(m)∥ ≤ C σ dlog(Md/δ) := γ(δ),
A ξ
m∈[M]
where C is an absolute constant.
ξ
Lemma 31 (Proposition 4.1 in Campolongo and Orabona, 2020). Let the w be the
t+1
solution of the update rule
w = argminηℓ (w)+D (w,w ),
t+1 t ψ t
w∈V
where V ⊆ W ⊆ Rd is a non-empty convex set and D (w ,w ) = ψ(w ) − ψ(w ) −
ψ 1 2 1 2
⟨∇ψ(w ),w − w ⟩ is the Bregman Divergence w.r.t. a strictly convex and continuously
2 1 2
differentiable function ψ : W → R. Further supposing ψ(w) is 1-strongly convex w.r.t. a
certain norm ∥·∥ in W, then there exists a g ∈ ∂ℓ (w ) such that
t t t+1
⟨η g′,w −u⟩ ≤ ⟨∇ψ(w )−∇ψ(w ),w −u⟩
t t t+1 t t+1 t+1
for any u ∈ W.
Lemma 32. Let {F }∞ be a filtration. Let {z }∞ be a stochastic process in B (U) =
t t=1 t t=1 2
{z ∈ RU | ∥z∥ ≤ 1} such that z is F measurable. Let {ε }∞ be a martingale difference
∞ t t t t=1
sequence such that ε ∈ RU is F measurable. Furthermore, assume that conditional on
t t+1
85F , we have ∥ε ∥ ≤ 2 almost surely, and denote by Σ = E[ε ε⊤|F ]. Let λ > 0 and for
t t 1 t t t t
any t ≥ 1 define
t−1 t−1
(cid:88) (cid:88)
U = ⟨ε ,z ⟩ and B = λ+ ∥z ∥2 ,
t i i t i Σi
i=1 i=1
Then, for any δ ∈ (0,1], we have
(cid:34) (cid:32)√ (cid:32)(cid:114) (cid:33)
(cid:18)
(cid:19)(cid:33)(cid:35)
(cid:112) λ 4 B t 4 2
Pr ∃t ≥ 1,U ≥ B + √ log + √ log ≤ δ.
t t
4 λ λ λ δ
Lemma 33 (Lemma 1 in Zhang and Sugiyama, 2023). Let ℓ(z,y) =
(cid:80)K
1{y = k} ·
k=0
(cid:16) (cid:17)
log 1 , a ∈ [−C,C]K, y ∈ {0}∪[K] and b ∈ RK where C > 0. Then, we have
[σ(z)]
k
1
ℓ(a,y) ≥ ℓ(b,y)+∇ℓ(b,y)⊤(a−b)+ (a−b)⊤∇2ℓ(b,y)(a−b).
log(K +1)+2(C +1)
Lemma 34 (Lemma 17 in Zhang and Sugiyama, 2023). Let ℓ(z,y) =
(cid:80)K
1{y = k} ·
k=0
(cid:16) (cid:17)
log 1 and z ∈ RK be a K-dimensional vector. Define zµ ≜ σ+(smooth (σ(z))),
[σ(z)] µ
k
where smooth (p) = (1−µ)p+µ1/(K +1). Then, for µ ∈ [0,1/2], we have
µ
ℓ(zµ,y)−ℓ(z,y) ≤ 2µ
for any y ∈ {0}∪[K]. We also have ∥zµ∥ ≤ log(K/µ).
∞
Lemma 35 (Lemma 18 in Zhang and Sugiyama, 2023). Let L (θ) := ℓ (θ)+ 1 ∥θ −
√ i,h i,h 2c
θi∥2 . Assume that ℓ is a N-self-concordant-like function. Then, for any θ,θi ∈
h B i,h h
i,h
B(0 d,1), the quadratic approximation L(cid:101)i,h(θ) = L i,h(θ(cid:101)i h+1) + ⟨∇L i,h(θ(cid:101)i h+1),θ − θ(cid:101)i h+1⟩ +
(cid:13) (cid:13)2
1 (cid:13)θ−θ(cid:101)i+1(cid:13) satisfies
2c (cid:13) h (cid:13)
B
i,h
(cid:18) (cid:13) (cid:13)2(cid:19)(cid:13) (cid:13)2
L i,h(θ) ≤ L(cid:101)i,h(θ)+exp N (cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13)
(cid:13) (cid:13)θ−θ(cid:101)i h+1(cid:13)
(cid:13) i+1
.
2 ∇ℓ i,h(θ(cid:101)
h
)
86