Xwin-LM: Strong and Scalable Alignment Practice for LLMs
BolinNi1,3,JingchengHu2,3,YixuanWei2,3,
HouwenPeng3,⋆,ZhengZhang3,GaofengMeng1,HanHu3
1InstituteofAutomation,CAS. 2TsinghuaUniversity 3MicrosoftResearchAsia
nibolin2019@ia.ac.cn hujc22@mails.tsinghua.edu.cn {t-yixuanwei, zhez, houwen.peng}@microsoft.com
gfmeng@nlpr.ia.ac.cn ancientmooner@gmail.com
Abstract
Performance Evolution on AlpacaEval Performance Evolution on MT-bench
98 96.89 8.0 7.87 7.90
In this work, we present Xwin-LM, a com- 95.82 7.64
prehensive suite of alignment methodologies 9492.47 93.40 7.5 7.24
forlargelanguagemodels(LLMs). Thissuite 91.76 7.06
90.43 7.0 encompasses several key techniques, includ- 89.75 6.76 90 6.63
88.21
ingsupervisedfinetuning(SFT),rewardmodel- 6.5 6.32
ing(RM),rejectionsamplingfinetuning(RS), 86 Xwin-LM-70B 6.0 5.84
anddirectpreferenceoptimization(DPO).The 83.02 Xwin-LM-13B
Xwin-LM-7B
key components are as follows: (1) Xwin- 82 5.5
SFT RSFT DPO SFT RSFT DPO
LM-SFT,modelsinitiallyfinetunedwithhigh- Alignment Step Alignment Step
quality instruction data; (2) Xwin-Pair, a
Figure 1: Performance evolution. The performance
large-scale,multi-turnpreferencedatasetmetic-
evolution on the AlpacaEval (left) and MT-bench
ulously annotated using GPT-4; (3) Xwin-
(right)benchmarkssuggeststhestrengthandscalabil-
RM,rewardmodelstrainedonXwin-Pair,de-
ityforXwin-LM,whichcancontinuouslyimprovethe
veloped at scales of 7B, 13B, and 70B pa-
instruction-followingabilityonthe7B,13B,and70B
rameters; (4) Xwin-Set, a multiwise prefer-
scales. ‘SFT’,‘RSFT’,and‘DPO’denotesupervised
ence dataset in which each prompt is linked
finetuning, rejection sampling finetuning, and direct
to 64 unique responses generated by Xwin-
preferenceoptimization,respectively.
LM-SFTandscoredbyXwin-RM;(5)Xwin-
LM-RS, models finetuned with the highest-
scoring responses from Xwin-Set; (6) Xwin-
LM-DPO,modelsfurtheroptimizedonXwin- et al., 2023) has been proposed. This approach
Set using the DPO algorithm. Our evalua- involves initially gathering preferences from hu-
tions on AlpacaEval and MT-bench demon-
manorAIsources,followedbyoptimizingapol-
strateconsistentandsignificantimprovements
icy model against a clearly built Reward Model
acrossthepipeline,demonstratingthestrength
(RM) (Ouyang et al., 2022) or an implicit prefer-
and scalability of Xwin-LM. The repository
encelearningtarget(Rafailovetal.,2024). While
https://github.com/Xwin-LM will be con-
tinuallyupdatedtofostercommunityresearch. effective, the inherent complexity and high costs
posesignificantbarriers,limitingextensiveexplo-
1 Introduction
rationwithintheresearchcommunity.
Recent advances in artificial intelligence, epito- Inthiswork,wedevelopandreleaseastrongand
mized by large language models (LLMs) such scalableRLHFpipelinenamedXwin-LM.Wede-
as GPT-4 (Achiam et al., 2023) and Claude (An- tailourapproachfordevelopingXwin-LM,which
thropic,2023),havedemonstratedremarkableca- includes supervised finetuning, preference anno-
pabilities across diverse real-world applications. tation,rewardmodeling,andpolicyoptimization,
Ensuringthesemodelsalignwithhumanexpecta- along with observations and insights associated
tions and values is crucial, especially as they are with each step. Specifically, we start with pre-
integratedintoandutilizedacrossnumerousappli- trainedmodels Llama-2(Touvronet al.,2023), a
cations(Ouyangetal.,2022;Baietal.,2022). collectionofprompts,andawell-trainedannotator,
To achieve this alignment, the technique of GPT-4. First,wetrainoursupervisedlearningmod-
Reinforcement Learning from Human/AI Feed- els,Xwin-LM-SFT,usinganinstruction-following
back (RLHF/RLAIF) (Stiennon et al., 2020; Lee datasetannotatedbyGPT-4toestablishaninitial
⋆contactperson capabilityasacoldstart. Wethencollectaprefer-
4202
yaM
03
]LC.sc[
1v53302.5042:viXra
)%(
etarniW
erocS
4-TPGencedataset,Xwin-Pair,whereresponsesaresam- Num. #Turn #Token #Token #Token
pled from Xwin-LM-SFT and preferences are la- Dataset of per per per per
conv. conv. conv. prompt response
beled by GPT-4. This dataset is used to train the
Step1:SFT
rewardmodel,Xwin-RM.Next,weuseXwin-LM-
ShareGPT-Part-I 6,206 7.3 4504.8 156.8 462.9
SFTtosamplemultipleresponsesforeachprompt
Step2:RM
from another set and employ Xwin-RM to rank
ShareGPT-Part-II 29,565 3.3 1634.7 67.6 414.1
these responses, creating a multiwise preference Evo-Instruct-V2 142,992 1.0 733.1 145.0 572.8
datasetnamedXwin-Set. Subsequently,Xwin-LM-
Step3:RSFT&Step4:DPO
RSisobtainedbyapplyingtherejectionsampling ShareGPT-Part-III 39,861 2.4 1811.5 108.7 501.9
(RS) finetuning technique to the highest-scoring
Table1: Statisticsofdatasets. Thepromptsarefrom
responses in Xwin-Set. Finally, beyond learning
ShareGPT and Evo-Instruc-V2. We split the conver-
only from positive samples, Xwin-LM-DPO em-
sationsinShareGPTintothreedisjointparts. There-
ploysthedirectpreferenceoptimizationtechnique,
sponsesusedinstep1arefromgpt-4,andtheresponses
involving negative samples in Xwin-Set to learn
instep2andstep3arefromourXwin-LM-SFT.The
fromunexpectedbehavior. #Turnand#Tokenareaveragedacrossallsamples.
We evaluate Xwin-LM on two popular
instruction-followingbenchmarks,AlpacaEval(Li
the data pair. We find that the dispreferred
et al., 2023) and MT-bench (Zheng et al., 2023).
responseshouldcloselymatchthepolicy’sout-
Fig. 1 illustrates the performance evolution of
putdistribution.
Xwin-LM throughout our pipeline. It is evident
that Xwin-LM-SFT achieves a satisfactory cold
2 OverviewofXwinLM
start,andsubsequentrejectionsamplingfinetuning
and direct preference optimization steps signifi- 2.1 High-levelMethodology
cantlyimprovemodelperformance,indicatingthe
WebeginwiththepretrainedLLMLlama-2(Tou-
strengthoftheproposedpipeline. OurXwin-LM
vron et al., 2023), a distribution of prompts, and
achieves state-of-the-art performance among all
well-trained AI annotators gpt-4. We then apply
Llama2-basedmodels.
thefollowingfoursteps.
In addition to the strong results, we have also
Step 1: Supervised Fine-Tuning (SFT). We
gleaned several observations and insights associ-
firstfinetuneapretrainedLlama-2onademonstra-
atedwiththepipeline:
tion dataset in a supervised fashion to obtain an
(1) Themodel’suppercapabilitylimitremains
initiallyalignedmodel.
fairlyconstantduringRLHF;performance
Step2: Collectcomparisondata,andtraina
gains are mainly due to enhanced stability
rewardmodel(RM).Wecollectadatasetofcom-
ingeneratinghigh-qualityresponses. Specif-
parisonsbetweenmodeloutputs,whereannotators
ically, we observe that performance on two
indicatewhichoutputtheyprefer. Wethentraina
benchmarks and the RM score on our valida-
rewardmodeltopredictthequalityofoutput.
tionsetunderthebest-of-1protocolimproved
Step 3: Rejection Sampling (RS) finetuning.
steadily,whilethoseunderthebest-of-64pro-
Foreachprompt,wegeneratemultipleresponses
tocolremainedfairlyconstant.
fromthefine-tunedmodelobtainedinStep1,and
(2) For SFT, a linear enhancement in perfor- subsequentlyfinetunemodelsusingtheresponses
mancehingesonanexponentialincreasein withthehighestRMscores.
datascale. Furthermore,asthedatascalecon- Step 4: Direct Policy Optimization (DPO).
tinues to increase, performance gradually ap- Buildingontheimitationofoptimalresponsesin
proachessaturation. Step 3, DPO is utilized to further minimize the
likelihoodofsuboptimalresponses.
(3) Best-of-nevaluationisadiscriminativemet-
ric for evaluating RMs and can also be an 2.2 Datasets
indicatorforprobingthepotentialoptimiza-
Sourceofprompts. Ourpromptdatasetcomprises
tionupperboundforalignment.
ShareGPT(Chiangetal.,2023)andEvo-Instruct-
(4) The DPO algorithm shows a certain sensi- V2 (Xu et al., 2023). The responses from these
tivity to the dispreferred responses within datasets are only utilized during the SFT stage.Detailedstatisticsaboutthedatasetsareshownin
Data Scaling
Tab.1. Itisimportanttonotethatourfocusisonex- 85.0
82.98
ploringaneffectiveandscalablealignmentpipeline 82.5 81.18
79.56 ratherthancreatingamodelwiththestrongestper- 80.0
formance. Therefore,weusealimiteddatasource 77.5 74.93
74.72 75 75.27
anddonotemployothertask-specificdatasets. 75.0
Annotator. WeusetheGPT-4APIastheannota- 72.5 69.92 71.55
torsincerecruitingandtraininghumanannotators 70.0 Conv. w/ gpt-4 response
Conv. w/ gpt-3.5 response
is time-consuming and expensive (Ouyang et al., 67.5
2K 4K 8K 16K 32K 64K 128K
2022). Theannotatorsandevaluatorsusedinthis
Number of turns
workareconsistentlyGPT-4,ensuringthetransfer-
Figure 2: Data scaling in SFT. The performance is
abilityofourpipeline.
exponentially related to the data scale and gradually
tendstosaturate. Themodeltrainedonresponsefrom
2.3 Evaluation
gpt-4 are significantly better than those from gpt-3.5-
Toassessinstruction-followingcapabilities,weuti-
turbo.
lizetwowidelyrecognizedbenchmarks:
• AlpacaEval (Li et al., 2023) is a single-turn of4,096tokens,resultinginatotalof10,605con-
benchmarkconsistingof805questionsacross versations. Weformattheconversationsfollowing
varioustopics,primarilyfocusingonhelpful- theVicuna(Chiangetal.,2023)guidelines.
ness. Models are evaluated by GPT-4, and
Implementationdetails. Weinitializeourmodel
thedefinitivemetricisthepairwisewinrate
using pretrained Llama-2. The models are fine-
comparedtothetext-davinci-003.
tunedfor3epochsusingtheAdamW(Loshchilov
and Hutter, 2017) optimizer. The AdamW opti-
• MT-bench(Zhengetal.,2023)presentsatwo-
mizer’shyperparametersaresetasfollows: β =
turn evaluation with 160 questions covering 1
0.9,β = 0.999,ϵ = 1×10−8,andweightdecay
eightdiversefieldssuchaswriting,reasoning, 2
of 0. We employ a cosine learning rate schedule
andmathematics. Themodelisrequiredtonot
with a maximum learning rate of 2×10−5. The
only provide an answer to the first question
hyperparametersremainconsistentamongthe7B,
but also a subsequent, predefined follow-up.
13B,and70Bmodels.
The responses are evaluated by GPT-4 on a
scalefrom1-10,andthemodel’soverallscore
3.2 ExperimentResultsandAnalysis
isaveragedonallquestions.
Dataquantityandquality. Weexaminetheeffect
We use gpt-4-0613 API as the evaluator. These of data quantity on performance using responses
benchmarks have established human agreement frombothGPT-4andGPT-3.5-turbo. Specifically,
metrics to ensure their reliability. For MT-bench, we sample conversations from ShareGPT-Part-I,
we observe fluctuations in the results; hence, we withthenumberofturnsrangingfrom2kto32k,
conduct the evaluation three times and report the and from ShareGPT-Part-II, with the number of
medianvalue. turnsrangingfrom19kto154k. Thecorresponding
resultsareillustratedinFig.2.
3 Step1: SupervisedFinetune
Initially, in the data incorporating GPT-4 re-
Weinitiateourinstruct-followingpipelinewithsu- sponses,itisevidentthatalinearenhancementin
pervised finetuning (SFT) on a high-quality con- performancenecessitatesanexponentialincrease
versation dataset using pretrained models. The indataquantity. Moreover,theaccelerationinper-
training loss is computed exclusively for tokens formancegainsbeginstodeceleratewhenthenum-
associatedwiththeresponsesegment. ber of turns surpasses 8k, indicating diminishing
marginal utility in data quantity during the SFT
3.1 ExperimentSetup
stage.
Dataset. We utilize a subset of ShareGPT con- Although the scope of this experiment is re-
taining6,206conversationsrespondedtobyGPT- stricted due to the finite responses obtained from
4(Wangetal.,2023)forSFT.Theselongconversa- GPT-4,asimilartrendisobservedintheconversa-
tionsaresplitintoblockswithamaximumlength tionsincorporatingGPT-3.5-turboresponses. An
)%(
etar-niW
lavEacaplASignificantly Slightly Negligibly Model AlpacaEval(%) MT-Bench
Better Total
Better Better Better/Unsure Closed-sourcemodels
GPT-4-1106(Turbo)(Achiametal.,2023) 97.7 9.32
Train 35,596 57,832 124,638 9,843 227,909
GPT-4-0613(Achiametal.,2023) 93.8 9.18
(16%) (25%) (55%) (4%) (100%) GPT-4-0314(Achiametal.,2023) 94.8 8.96
Claude-2(Anthropic) 91.4 8.06
Eval 1,834 2,986 6,618 557 11,995
GPT-3.5-Turbo(OpenAI,2023) 93.4 8.39
(15%) (25%) (55%) (4%) (100%)
Open-sourcemodelsLLaMA-2-7B
Llama-2-7B-chat(Touvronetal.,2023) 71.4 6.27
Table2: StatisticofXwin-Pair. The“significantlybet- Vicuna-7B-v1.5(Zhengetal.,2023) - 6.17
Tulu27B-DPO(Ivisonetal.,2023) 85.1 7.00
ter"categoryconstitutes15%ofthetotaldata,whilethe
Xwin-LM-SFT-7B(ours) 83.0 5.84
majorityofthedatafallsintothe“better"and“slightly Xwin-LM-RS-7B(ours) 88.2 6.32
better"categories,accountingfor80%. Xwin-LM-DPO-7B(ours) 90.4 6.63
Open-sourcemodelsLLaMA-2-13B
Llama-2-13b-chat(Touvronetal.,2023) 81.1 6.65
Significantly SlightlyNegligibly Vicuna-13B-v1.5(Zhengetal.,2023) - 6.57
Better Avg WizardLM-13B-v1.2(Xuetal.,2023) 89.2 7.06
Better Better Better
Tulu213B-DPO(Ivisonetal.,2023) 89.5 7.00
Xwin-RM-7B 77.71 70.56 65.91 82.51 69.45 OpenChat3.2SUPER(Wangetal.,2023) 89.5 7.19
Xwin-LM-SFT-13B(ours) 89.8 6.76
Xwin-RM-13B 78.25 70.06 66.03 62.80 69.52
Xwin-LM-RS-13B(ours) 91.8 7.06
Xwin-RM-70B 71.48 82.93 72.23 67.34 71.48 Xwin-LM-DPO-13B(ours) 93.4 7.24
Open-sourcemodelsLLaMA-2-70B
Table3: GranularbinaryaccuracyofXwin-RMon Llama-2-70b-chat(Touvronetal.,2023) 92.7 6.86
WizardLM-70B-v1.0(Xuetal.,2023) 92.9 7.78
ourvalidationset. Generally,rewardmodelsexhibit
Tulu270B-DPO(Ivisonetal.,2023) 92.9 7.89
higheraccuracyonmoredistinctresponses(e.g.,signif- Xwin-LM-SFT-70B(ours) 92.5 7.64
icantlybetter)andloweraccuracyonsimilarresponses Xwin-LM-RS-70B(ours) 95.8 7.87
Xwin-LM-DPO-70B(ours) 96.9 7.90
(e.g.,negligiblybetter).
Table 4: AlpacaEval and MT-bench results. Xwin-
LMachievestheSoTAperformancestepbystep.
increaseindataquantityfrom38kto154kresults
inonlyamarginalimprovementof+0.45%.
Furthermore,uponcomparingmodelstrainedon ity of conversations in the dataset span multiple
different response sources, it is obvious that the turns, we unfold each conversation into multiple
performanceofmodelstrainedonGPT-4responses datainstancesbyturn,withalengthlimitof4,096
significantlyoutperformsthosetrainedonGPT-3.5- tokens.
turboresponses. Forexample,using16kturnsfrom
Responses generation. For each unfolded in-
GPT-4outperforms19kturnsfromGPT-3.5-turbo
stance, we discard the response attached to the
byaremarkable+9.63%winrate. Weconjecture
lastturnintheoriginalconversation. Instead,we
that there are two potential explanations: 1) data
use the conversation history and the query from
quality holds more significance than quantity in
the last turn as the prompt to the Xwin-LM-SFT
SFT, and 2) the GPT-4 judge in AlpacaEval may
to obtain the response. Sequentially, two distinct
favormodelsfinetunedonitsoutputs.
responsesaresampledfromXwin-LM-SFT-70Bat
atemperatureof0.7and0.3,respectively. Boththe
4 Step2: RewardModeling
top-pandtop-k aresetto1.
Our primary objective is to explore the RLHF Preference annotation. Then, we employ gpt-
pipeline rather than to seek the strongest RM. 4-0314 API as a judge to provide three types of
Therefore, we do not employ the existing prefer- annotationforeachinstance: 1)whichresponseis
ence datasets (Cui et al., 2023; Bai et al., 2022) better;2)thereasonforthisjudgment;and3)arat-
inthecommunity;instead,webuildapreference ingamong‘significantlybetter’,‘better’,‘slightly
datasetnamedXwin-Pair,startingfromacollection better’,and‘negligiblybetter’followingLlama-2.
of real users’ queries, and train Xwin-RM at 7B, We randomly shuffle the two responses to avoid
13B,and70Bscalesbasedonit. the judge’s preference for the order of candidate
Prompts collection. First, we randomly sample responses.
29,566 conversations from ShareGPT and also Dataset statistics. We obtain a total of 239,904
integrate the Evo-Instruct-V2 dataset (Xu et al., preference data instances, splitting 227,909 in-
2023) motivated by the scaling trends in Llama- stancesfortrainingandtheremaining11,995for
2 (Touvron et al., 2023), leading to an aggregate validation. MorestatisticsarepresentedinTab.2.
of 172,558 conversations. Given that the major- We highlight that Xwin-Pair is the largest multi-Best-of-64 evaluation Best-of-64 evaluation
98 95.96 96.58 979 .27 6.70 8.0 7.64 7.78 7.77 7.87
96 93.90 94.52 94.47 7.5
94 92.67 7.15 7.18
92 91.30 7.0 6.89 6.97 89.75 90.06 6.65
90 6.52
6.43
88 6.5
86 Xwin-LM-SFT-70B
83.74 Xwin-LM-SFT-13B 6.0 5.87
84 Xwin-LM-SFT-7B
w/o RM Xwin-RM-7B Xwin-RM-13B Xwin-RM-70B w/o RM Xwin-RM-7B Xwin-RM-13B Xwin-RM-70B
(best-of-1) (best-of-1)
Reward Model Reward Model
Figure3:ComparingRMsonbest-of-64evaluationprotocol.Comparedtosamplingasingleresponse,employing
RMenablestheselectionofhigh-qualityresponsesfromapoolof64candidates,andlargerRMscanselectresponses
withsuperiorperformance,indicatingthatthecapabilitiesofRMincreasewithsize.
7B-SFT w/ 7B-RM 7B-SFT w/ 13B-RM 7B-SFT w/ 70B-RM
90.7 5.2 94.5 5.2 96.2 5.0
88.4 4.2 92.3 4.2 95.3 4.1
86.0 3.2 90.1 3.2 94.3 3.3
2 4 8 16 32 64 2 4 8 16 32 64 2 4 8 16 32 64
13B-SFT w/ 7B-RM 13B-SFT w/ 13B-RM 13B-SFT w/ 70B-RM
92.2 -0.4 95.2 -0.4 97.0 -0.6
89.0 -1.5 92.7 -1.3 95.6 -1.4
85.9 -2.5 90.2 -2.3 94.2 -2.3
2 4 8 16 32 64 2 4 8 16 32 64 2 4 8 16 32 64
70B-SFT w/ 7B-RM 70B-SFT w/ 13B-RM 70B-SFT w/ 70B-RM
96.1 6.3 98.5 6.6 98.3 6.9
90.4 5.2 94.0 5.6 96.2 5.9
84.7 4.0 89.6 4.5 94.1 5.0
2 4 8 16 32 64 2 4 8 16 32 64 2 4 8 16 32 64
Number of samples Number of samples Number of samples
Figure4: Best-of-nevaluation. Weselectthebestresponsefromdifferentnumbersofsamplestoevaluatethe
alignmentbetweenXwin-RMsandtheoff-the-shelfAIjudge. Lefty-axis: AlpacaEvalwinratejudgedbygpt-4.
Righty-axis: RMscorepredictedbyXwin-RM-70B.
turnpreferencedatasetwithadditionalexplanations modelforpromptxandresponseywithparameters
andfine-grainedratings. θ,andD isthepreferencedataset.
RM
Rewardmodeling. Xwin-RMtakestheresponse
4.1 ExperimentSetup
and its corresponding prompt (including the con-
versation history) as inputs and outputs a scalar ImplementationDetails. Wetrainforoneepoch
scoretoindicatethequalityofthisresponse. We overthetrainingset. Themaximumlearningrate
startfromXwin-LM-SFTandthenaddarandomly is2×10−6 fortheXwin-RM-70Band1×10−5
initialized linear head that outputs a scalar score. for Xwin-RM-7B and 13B. The learning rate is
Specifically,givenapromptx,thetrainingobjec- decreasedonacosinelearningrateschedule,down
tive is to predict which response y ∈ {y ,y } is to0. Weuseawarm-upof3%ofthetotalnumber
0 1
better. Ifthebetterresponseisy ,wecanwritethe of steps. The effective batch size is 128 pairs for
i
lossfunctionas: 70BRMand256fortherest.
L(r ) = −E [log(σ(r (x,y )−r (x,y )))] 4.2 ExperimentResultsandAnalysis
θ DRM θ i θ 1−i
(1) Granularbinaryaccuracy. Theexperimentalre-
where r (x,y) is the scalar output of the reward sultsarepresentedinTab.3,wheretherearetwo
θ
)%(
etarniW
)%(
etarniW
)%(
etarniW
)%(
etarniW
lavEacaplA
erocS
hcneb-TM
erocS
MR
erocS
MR
erocS
MRobservations: 1)althoughmoredistinctivesamples
6 Top 1: 5.90
(e.g.,‘significantlybetter’)accountforonlyasmall
5
portion(15%)ofthetrainingdata,therewardmod-
4 Top 32: 4.01
els exhibit higher accuracy on these samples, yet
3
loweraccuracyonsimilarsamples(e.g.,‘slightly
2
better’),despitetheirhigherdataproportion(55%).
1
Theinfluenceoftheproportionoftrainingdatawith
0
differentlevelsofdistinctivenessonthefinalper-
1
1 8 16 24 32 40 48 56 64
formanceofRMremainsunclear,andwereserve Rank of responses
thismatterforfutureinvestigation;2)largerRMs
Figure5: DistributionofRMscoresofresponsesat
achievehigheraverageaccuracy,buttheimprove-
eachrank. Responsesaresortedbyscorefromhighest
ment is not particularly dramatic. The accuracy tolowest,withthehorizontalaxisindicatingtherank
onlyimprovedby2.03%whenscalingthesizeof oftheresponses;asmallerranksignifiesahigherscore.
RM from 7B to 70B. We believe this is due to a Thescoresareaveragedacrossallprompts.
combinationoftheinherentdifficultyofthistask
andtheinstabilityofthepreferenceannotation. Top K
90.0 87.35 84.95 84.59 82.17
SFT: 83.74
Best-of-nevaluationasapracticalindicator. Be- 80.0
yond referencing binary accuracy, we further ex- 67.53
70.0
aminethegeneralizabilityofXwin-RMbyemploy-
60.0
ingabest-of-nevaluationonAlpacaEvalandMT- 1 8 16 32 64
K
bench,whichofferadiversearrayofquestions. For Best of N
each question, the policy model generates 64 an- 88.0 87.12 87.35
86.58
swers,afterwhichXwin-RMscoreseachanswer,
86.0
selecting the highest-scoring one for evaluation. 84.53
83.85
DifferentRMsmaketheirselectionsfromthesame 84.0 SFT: 83.74
pool of response candidates for the same policy. 82.0
1 8 16 32 64
TheresultsaredepictedinFig.3. Thethreescales N
ofXwin-RMsareallcapableofeffectivelyselect-
Figure6: Dataselectioninrejectionsamplingfine-
inghigh-qualityresponses. Althoughthetraining tuning. Weexploretherankofselectedsamples(top)
dataforRMincludesonlyresponsesfromthe70B andthenumberofresponsecandidates(bottom).
SFT policy, Xwin-RMs also perform well on the
7B and 13B models, further demonstrating their
5 Step3: RejectionSamplingFinetune
generalizability. Additionally,whilethesmaller7B
RM can also select good answers for policies of
For each prompt, we sample N responses from
varyingscales, itisapparentthatlargerRMscan
the SFT policy model, and the response with the
choosebetterresponsesthansmallerones,reaffirm-
highestRMscoreisconsideredasthenewground
ingthesignificanceofRMmodelsize.
truth. Subsequently, we finetune our model on
theseresponses,therebyreinforcingthereward.
Xwin-RM aligns well with the off-the-shelf AI
5.1 ExperimentSetup
judge. To inspect the alignment between Xwin-
RM and AI judge (e.g., gpt-4), we conduct the Dataset construction. We randomly sample
best-of-{2,4,8,16,32,64}evaluationonAlpacaEval, 27,424ShareGPTconversations,distinctfromthe
andcomparethescoresofRMandwin-rateofthe SFT and RM stages, and split them into 39,861
benchmark. Thecomprehensiveresultspresented sub-conversationswithalengthlimitof4,096to-
in Fig. 4 demonstrate a strict positive correlation kenseach. Foreachsub-conversation, weunfold
betweenthescoresoftheXwin-RMandthewin- thefirstthreeturns(iftheyexist)withtheconversa-
ratesonthebenchmark,indicatingthatXwin-RMs tionhistoryandgenerate64responsesperlastturn
canaccuratelyscoreresponsesofvaryingquality, using Xwin-SFT-70B. These responses are then
and their scoring outcomes are highly consistent rankedwithXwin-RM-70B,yieldingamultiwise
withthejudgmentsofGPT-4. preference dataset named Xwin-Set, which com-
erocS
MR
egarevA
)%(
etarniW
lavEacaplA
)%(
etarniW
lavEacaplAScore evolution of 7B models Score evolution of 13B models Score evolution of 70B models
Xwin-LM-DPO-7B 7 Xwin-LM-DPO-13B 7 Xwin-LM-DPO-70B
6 Xwin-LM-RS-7B Xwin-LM-RS-13B Xwin-LM-RS-70B
Xwin-LM-SFT-7B 6 Xwin-LM-SFT-13B 6 Xwin-LM-SFT-70B
5
5 5
4
4 4
3
3 3
2
2 2
1
1 1
0
0 0
0 10 20 30 40 50 60 0 10 20 30 40 50 60 0 10 20 30 40 50 60
Rank of responses Rank of responses Rank of responses
Figure7: EvolutionofRMscorewithalignmentpipeline. Withthealignmentpipeline,thereisasignificant
increase in the lower bound (e.g., responses at low rank) of the model’s output scores, while the upper bound
(e.g.,responsesathighrank)exhibitsminimalchange. Thisreflectsthattheenhancementinmodelperformanceis
achievedbyimprovingthestabilityofgeneratinghigh-qualityresponses.
plingdataset.
best-of-64 evaluation best-of-1 evaluation
7B Model 7B Model Influence of the rank of selected samples. The
97.9 7.3
toppanelofFig.6demonstratesthatmodelstrained
93.6 6.9
with higher-ranked samples consistently achieve
89.3 6.4
superiorperformance,corroboratingtheprecision
84.9 6.0
80.6 5.6 ofourRMinrankingcandidateresponses.
SFT RSFT DPO SFT RSFT DPO
Effect of the number of response candidates.
13B Model 13B Model
99.9 7.5 The bottom panel of Fig. 6 reveals that increas-
97.0 7.3 ing the number of response candidates enhances
94.0 7.1 thequalityofthesamplesobtained. However,the
91.1 6.9 marginal gains begin to plateau beyond a sample
88.1 6.6
sizeof32. Doublingthenumberofsamplesfrom
SFT RSFT DPO SFT RSFT DPO
70B Model 70B Model 32to64leadstoamodestwinrateimprovement
99.2 8.0
of+0.23%,whichmeansthatthesamplesizeof32
97.3 7.9
representsagreatbalancebetweencomputational
95.3 7.8
efficiencyandsamplequality.
93.3 7.7
91.4 7.6
SFT RSFT DPO SFT RSFT DPO 6 Step4: DirectPreferenceOptimization
Figure 8: Evolution of best-of-1 performance and We employ DPO (Rafailov et al., 2024) instead
best-of-64performance. Althoughmodelperformance ofPPO(Schulmanetal.,2017)fortwomainrea-
consistentlyimprovesonthebest-of-1evaluationproto- sons: 1)weobservethatPPOischallengingtotrain
col,itexhibitsarelativelysteadytrendonthebest-of-64 onlargerpolicymodels,includinghyperparameter
evaluation. Left: AlpacaEval. Right: MT-bench.
issuesandcomputationalcosts;2)therichprefer-
encerelationshipspresentintheXwin-Setremain
underutilized,whereasDPOinherentlyutilizesthe
prises a total of 96,277 prompts and 96,277 x 64
preferencedata.
responses. Fig.5displaystheaverageRMscores
The DPO method directly updates the target
forallsamplesateachrank.
policyfromthepreferencepairs(x,y ,y )asfol-
w l
Implementation details. We employ the same lows:
hyperparameters as in the SFT stage, except for L (π )=−E (2)
reducingthelearningrateto1×10−5. Notethatfor
DPO (cid:20)θ
(cid:18)
(cid:18)(x,yw,yl)∼D
(cid:19)(cid:19)(cid:21)
π (y |x) π (y |x)
logσ β log θ w −log θ l .
eachsub-conversation,weonlycomputetheloss π (y |x) π (y |x)
ref w ref l
ontheresponseofthelastturn,astheresponsesin
Theintuitiveobjectiveistoincreasethelikelihood
earlierturnsarenotgeneratedbyourmodel.
ofthepreferredresponsey anddecreasethelike-
w
lihoodofthedispreferredresponsey .
l
5.2 ExperimentResultsandAnalysis
Construction of preference pair. For selecting
WeconductapreliminaryinvestigationusingXwin- preferredanddispreferreddata,wefollowthein-
LM-RS-7B with half of the entire rejection sam- tuitionthatamodelshouldlearnfromthehighest
erocS
MR
egarevA
)%(
etarniW
lavEacaplA
)%(
etarniW
lavEacaplA
)%(
etarniW
lavEacaplA
erocS
hcneb-TM
erocS
hcneb-TM
erocS
hcneb-TM
erocS
MR
egarevA
erocS
MR
egarevA(e.g.,top-8,16)andlowerquality(e.g.,top-48,64)
90.9 6.7
asthedispreferreddata. Fig.9demonstratesthat
90.0 6.6
themodelperformsoptimallyonbothAlpacaEval
89.2 6.4 andMT-benchwhenthetop-32responseservesas
the dispreferred data. Both excessively high and
88.3 6.3
low-qualityresponsesasdispreferenceleadtoper-
87.5 6.2
formancedegradation,correlatingwiththedegree
8 16 32 48 64
Rank of dispreferred response in DPO of quality deviation. This supports our hypoth-
esis that the dispreferred response should mirror
Figure 9: Exploring the rank of dispreferred re-
sponse. Thetop-32responseisclosestinscoretothat the policy model’s output to correct the frequent
generatedbyXwin-LM-RS-7B.Usingeitherahigheror errors. Additionally, lower-quality responses as
lowerqualityresponseasthedispreferredoneimpacts dispreferreddataoutperformhigher-qualityones,
performance. Left y-axis: AlpacaEval. Right y-axis: providingpracticalinsightsforDPOdatacuration.
MT-Bench.
RLHFenhancesthemodel’sstabilityofgener-
atingqualityanswers,withthecapabilityupper
qualitydatawhilecorrectingitsmostcommonmis- boundremainingrelativelyunchanged. InFig.8,
takes. Based on this premise, we use the top-1 wedepicttheperformanceevolutionunderthebest-
responseasthepreferredresponse. Forthedispre- of-1andbest-of-64evaluationduringthealignment
ferredresponse,astraightforwardapproachwould pipeline. In the best-of-64 evaluation, we gener-
betohavethepolicymodelgenerateanswersforall ate64potentialresponsesforeachpromptanduse
questionsintheXwin-Set. However,thismethod Xwin-RM-70B to select the one with the highest
incursadditionalgenerationcosts. Therefore,we RMscoreforevaluation. Weobserveaconsistent
approximatethemodel’scurrentcapabilityusing enhancementinbest-of-1performancewithdeeper
existing responses in the Xwin-Set. Specifically, alignment,whilebest-of-64performanceremains
we sample 500 prompts from the Xwin-Set, gen- largely unchanged. For instance, the 7B model’s
erate responses with the policy, and score them best-of-1performanceonAlpacaEvalimprovesby
usingXwin-RM-70B.Wethenselecttheresponse +7.41%, in contrast to a mere +1.05% increase
fromtheXwin-Setwithascoreclosesttothepol- in best-of-64 performance. In addition to public
icymodel’sscoreasthedispreferredresponse. The benchmarks,weevaluatethemodelonaheld-out
experimentsinFig.9confirmourspeculation. setof500promptsfromShareGPT.Fig.7displays
the evolution in the RM score distribution of the
6.1 ExperimentSetup
generatedresponses. Notably,thetopscoresshow
Dataset. we utilize all prompts from the Xwin- nodrasticchanges, suggestingthatthequalityof
Set. The preferred responses across all models thebestresponsesdoesnotsignificantlyimprove,
correspondtothetop-1responseswithintheXwin- whereasthescoresoflower-qualityresponsesex-
Set. As for the dispreferred responses, we select perienceamarkedincreasewithalignment,signif-
top-32forthe7Bmodel,top-20forthe13Bmodel, icantlyreducingthegapwithtopresponses. This
andtop-8forthe70Bmodel. furtherunderscoresthattheessenceofRLHFlies
Implementationdetails. Xwin-LM-DPOisinitial- inincreasingthestabilityofqualityoutputrather
izedfromXwin-LM-RSandtrainedfortwoepochs. than elevating the capability upper bound, which
Weusealinearlearningrateschedulerwithapeak remainsrelativelyconstant.
learningrateof5e-7and100warmupsteps. The
Best-of-nperformancecanbeviewedastheopti-
globalbatchsizeis32andβ issetto0.1.
mizationupperboundofalignment. InFig.8,we
observethatafterRLHF,theDPOmodel’sbest-of-
6.2 ExperimentResultsandAnalysis
1performancecloselyapproachestheSFTmodel’s
Thedispreferreddatashouldcloselymatchthe best-of-64performance. Thisindicatesthatasim-
policy’soutputdistribution. WeuseXwin-LM- pleSFTmodeliscapableofproducingqualityan-
DPO-7B to explore the selection of dispreferred swers,butcannotensuretheirstability. Therefore,
data. Thetop-32responseinXwin-Setistheclos- wecanemploythebest-of-nevaluationmethodto
est in score to that generated by Xwin-LM-RS- probetheupperlimitsofperformance,servingasa
7B.Wealsotrytheresponseswithhigherquality guideforsubsequentalignmentefforts.
)%(
etarniW
lavEacaplA
erocS
hcneb-TM7 ConclusionandLimitation HarrisonLee,SamratPhatale,HassanMansoor,Kellie
Lu, Thomas Mesnard, Colton Bishop, Victor Car-
Inthiswork,wepresentXwin-LM,astrong,and bune, and Abhinav Rastogi. 2023. Rlaif: Scaling
scalablealignmentpractice. Ourpipelineincludes reinforcementlearningfromhumanfeedbackwithai
feedback. arXivpreprintarXiv:2309.00267.
supervisedfinetuning,rewardmodeling,rejection
sampling finetuning, and direct preference opti- XuechenLi,TianyiZhang,YannDubois,RohanTaori,
mization. Our model maintained the top-1 posi- IshaanGulrajani,CarlosGuestrin,PercyLiang,and
TatsunoriB.Hashimoto.2023. Alpacaeval: Anau-
tion on the AlpacaEval from September 2023 to
tomatic evaluator of instruction-following models.
November 2023. However, our work has several
https://github.com/tatsu-lab/alpaca_eval.
limitations: 1)wedidnotdeliberatelyenhancethe
model’smulti-turncapabilities,whichmaylimitits Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
practicaluserexperience;2)weutilizedalimited
arXiv:1711.05101.
datasource,whichcouldaffectthemodel’soverall
performance;3)weobservedthatthemodelsuffers OpenAI. 2023. Gpt-3.5 turbo fine-tuning and api up-
dates.
fromhallucinationstosomeextent,whichmaybe
causedbytrainingonself-generateddata;4)weno- LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
ticedthattheannotationsandevaluationsbyGPT-4 CarrollWainwright,PamelaMishkin,ChongZhang,
exhibitacertaindegreeofinstability. SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
2022. Training languagemodelsto followinstruc-
tions with human feedback. NeurIPS, 35:27730–
27744.
References
RafaelRafailov,ArchitSharma,EricMitchell,Christo-
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
pherDManning,StefanoErmon,andChelseaFinn.
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
2024. Directpreferenceoptimization:Yourlanguage
DiogoAlmeida,JankoAltenschmidt,SamAltman, modelissecretlyarewardmodel. NeurIPS,36.
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774. John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
Anthropic. Modelcardandevaluationsforclaudemod- malpolicyoptimizationalgorithms. arXivpreprint
els. arXiv:1707.06347.
Anthropic.2023. Modelcardandevaluationsforclaude Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
models. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
DarioAmodei,andPaulFChristiano.2020. Learn-
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda ing to summarize with human feedback. NeurIPS,
Askell, AnnaChen, NovaDasSarma, DawnDrain, 33:3008–3021.
StanislavFort,DeepGanguli,TomHenighan,etal.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
2022. Trainingahelpfulandharmlessassistantwith
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
reinforcementlearningfromhumanfeedback. arXiv
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
preprintarXiv:2204.05862.
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
arXiv:2307.09288.
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephEGonzalez,etal.
GuanWang,SijieCheng,XianyuanZhan,XiangangLi,
2023. Vicuna: Anopen-sourcechatbotimpressing
SenSong,andYangLiu.2023. Openchat: Advanc-
gpt-4with90%*chatgptquality. Seehttps://vicuna.
ingopen-sourcelanguagemodelswithmixed-quality
lmsys.org(accessed14April2023),2(3):6.
data. arXivpreprintarXiv:2309.11235.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
WeiZhu,YuanNi,GuotongXie,ZhiyuanLiu,and
PuZhao,JiazhanFeng,ChongyangTao,andDaxin
MaosongSun.2023. Ultrafeedback: Boostinglan-
Jiang. 2023. Wizardlm: Empowering large lan-
guage models with high-quality feedback. arXiv guagemodelstofollowcomplexinstructions. arXiv
preprintarXiv:2310.01377.
preprintarXiv:2304.12244.
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Nathan Lambert, Matthew Peters, Pradeep Dasigi, Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Joel Jang, David Wadden, Noah A Smith, Iz Belt- Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
agy,etal.2023. Camelsinachangingclimate: En- Judging llm-as-a-judge with mt-bench and chatbot
hancing lm adaptation with tulu 2. arXiv preprint arena. arXivpreprintarXiv:2306.05685.
arXiv:2311.10702.