Sparse Linear Regression when Noises and Covariates are
Heavy-Tailed and Contaminated by Outliers
Takeyuki Sasai ∗ Hironori Fujisawa †
August 5, 2024
Abstract
Weinvestigateaproblemestimatingcoefficientsoflinearregressionundersparsityassump-
tion when covariates and noises are sampled from heavy tailed distributions. Additionally,
weconsiderthesituationwherenotonlycovariatesandnoisesaresampledfrom heavytailed
distributions but also contaminated by outliers. Our estimators can be computed efficiently,
and exhibit sharp error bounds.
1 Introduction
Sparse estimation has been studied extensively over the past 20 years to handle modern high-
dimensional data with [40] as a starting point. Because the advancement of computer technology
hasmadeitpossibletocollectveryhighdimensionaldataefficiently,sparseestimationwillcontinue
to be an important and effective method for high dimensional data analysis in the future. In this
study,wefocusontheestimationofcoefficientsinsparselinearregression. Wedefinesparselinear
regressionmodel as follows:
y =x β +ξ , i=1, ,n, (1.1)
i ⊤i ∗ i
···
where x n is a sequence of independent and identically distributed (i.i.d.) random vectors,
β R{ d ii s} ti= he1 true coefficient vector,and ξ n is a sequence of i.i.d. randomvariables,andβ
∗ ∈ { i }i=1 ∗
is the sparse regressioncoefficient with s(<d) non-zero elements.
In this paper, we focus on constructing tractable estimators from the observation y x n ,
{ i i }i=1
and deriving non-asymptotic error bounds. This problem has been considered in many litera-
ture. Manystudiesdealtwiththesituationwhere x n and ξ n aresampledfromGaussian
{ i }i=1 { i }i=1
distribution and univariate Gaussian distribution, respectively. Some studies weakens the as-
sumption and deal with the situation where x n and ξ n are sampled from a multivariate
{ i }i=1 { i }i=1
sub-Gaussian distribution (Chapter 3 of [41]) and univariate sub-Gaussian distribution (Chapter
2 of [41]), respectively (e.g.,[42, 33, 5]).
Several studies investigated robust estimation methods with respect to the tail heaviness of
data. For example, [1, 38] considered the case where ξ n is sampled from a heavy tailed
{ i }i=1
distribution. However,veryfew studies tackledthe casewhere x n is sampledfroma heavier-
{ i }i=1
tailed distribution than Gaussian and subGaussian [37, 22, 21, 24]. [37, 22] considered the case
where x n is sampled from a multivariate sub-exponential distribution (Chapter 3 of [41]),
{ i }i=1
which is a heavier-tailed distribution than multivariate Gaussian. [21, 24] considered a more
relaxed assumption where x n is sampled from a finite kurtosis distribution:
{ i }i=1
∗DepartmentofStatisticalScience,TheGraduateUniversityforAdvancedStudies,SOKENDAI,Tokyo,Japan.
Email: sasai@ism.ac.jp
†The Institute of Statistical Mathematics, Tokyo, Japan. Department of Statistical Science, The Graduate
UniversityforAdvancedStudies,SOKENDAI,Tokyo,Japan. CenterforAdvancedIntegratedIntelligenceResearch,
RIKEN,Tokyo,Japan. Email:fujisawa@ism.ac.jp
1
4202
guA
2
]LM.tats[
1v63310.8042:viXraDefinition 1.1 (Finite kurtosisdistribution). A random vector z Rd is said to be sampled from
a finite kurtosis distribution if for every v Rd, ∈
∈
E hv,z −Ez i4 ≤K4 E hv,z −Ez i2 2 =K4 kΣ21 v k4
2
, (1.2)
where K is a constant and E(z Ez)(z (cid:0)Ez) =Σ. (cid:1) (cid:16) (cid:17)
⊤
− −
We note that the finite kurtosis distribution contains multivariate sub-Gaussian and sub-
exponentialdistributions. Inthepresentpaper,weassumethefinitekurtosisconditionon x n
{ i }i=1
and we construct an estimator which has properties similar to the ones of [21, 24] under different
assumption on ξ n and different conditions on β , n, s, d and the covariance of x n .
{ i }i=1 ∗ { i }i=1
Another aspect of robustness is robustness to outliers. Estimation against outliers is very ac-
tivelystudiedinrecentyears[14]. Toinvestigatethe robustnessagainstheavytaileddistributions
andoutliersforestimatingcoefficientsinsparselinearregression,weconsiderthefollowingmodel:
y
i
=X⊤i β ∗+ξ i+√nθ i, i=1,
···
,n, (1.3)
where X = x +̺ for i = 1, ,n and ̺ n and θ n are the outliers. We allow the
i i i ··· { i }i=1 { i }i=1
adversary to inject arbitrary values into arbitral o samples of y ,x n . Let be the index set
{ i i }i=1 O
of the injected samples and = (1, ,n) . Therefore, ̺ = (0, ,0) and θ = 0 hold
i ⊤ i
I ··· \O ···
for i . We note that ̺ and θ can be arbitral values and they are allowed to
i i i i
correl∈ ateI freely among the{ m a} n∈ dO correla{ te} w∈ iO th x n and ξ n . Some studies considered
{ i }i=1 { i }i=1
the problem of constructing estimators of β from (1.3). [11, 4, 25, 34] dealt with the case
∗
where x n is sampled from a Gaussian or multivariate subGaussian distribution. However,
{ i }i=1
few studies considered the case where the covariates are sampled from a heavy tailed distribution
and contaminated by outliers. An exception to this is [28], and the methods in [28] assume that
x n sampled from a finite kurtosis distribution. In the present paper, we consider the case
{ i }i=1
where x n is sampled from a finite kurtosis distribution, and our estimator attains a shaper
{ i }i=1
error bound than the one of [28].
In Section 2, we describe a method to estimate β from (1.1), and present our result, and
∗
introduceprecedingworkrelatedto ourresult. InSection3,we describeamethod toestimate β
∗
from (1.3), and present our result, and introduce preceding works related to our result. Proofs of
the statements in the main text are given in the appendix.
2 Method, result and related work
In Section 2, we consider the case where there are no outliers. That is, consider the problem of
estimatingβ in(1.1). Beforepresentingourmethodandresultprecisely,weroughlyintroduceour
∗
result. For linear regression problem (1.1) with heavy-tailed x ,ξ n , our estimator βˆ satisfies
{ i i }i=1
log(d/δ)
P kΣ21 (βˆ −β ∗) k2 ≤C {x i,ξi}n
i=1
rs
n
!≥1 −δ, (2.1)
iw sh be are seC d{ ox ni,ξi s} in i m=1 pli es ta hrc eo sn hs ot la dn it ngde fp oe rnd xing non ath ne dm ℓo -m pee nn at lip zr eo dp Her uti be es ro lf o{ ssx ,i, aξ ni d}n i= o1 u. rO esu tr imm ae tt oh ro id
s
{ i }i=1 1
tractable. The result is similar to the one of normal lasso under Gaussian data. The difference
between our results and the case of normal lasso under Gaussian data is discussed in Section 2.3.
2.1 Method
For the problem of estimating β from (1.1), we propose ROBUST-SPARSE-ESTIMATION I
∗
(Algorithm 1).
THRESHOLDINGisaproceduretomakecovariatesbounded,whichisoriginatedfrom[21,23].
By bounding the covariates through THRESHOLDING, it becomes possible to derive concentra-
tioninequalitieswithsufficientsharpnesstohandlesparsity. PENALIZED-HUBER-REGRESSION
2Algorithm 1 ROBUST-SPARSE-ESTIMATION I
Input: {y i,x
i
}n
i=1
and the tuning parameters τx, λ
o
and λ
s
Output: βˆ
1: {x˜ i }n i=1 ←THRESHOLDING( {x i }n i=1,τx)
2: βˆ ←PENALIZED-HUBER-REGRESSION( {y i,x˜ i }n i=1, λ o, λ s)
is a Huber regression with ℓ -norm penalization. From the preceding works [31, 12, 38], the ℓ -
1 1
penalized Huber regression has robustness against heavy-tailed noise ( ξ n ) and outliers in the
{ }i=1
output, while also being capable of handling the sparsity of the coefficient.
In Sections 2.1.1 and 2.1.2, we describe the details of THRESHOLDING and PENALIZED-
HUBER-REGRESSION, respectively.
2.1.1 THRESHOLDING
Define the j-th element of x as x . THRESHOLDING (Algorithm 2) makes the covariates
i ij
bounded to obtain sharp concentration inequalities.
Algorithm 2 THRESHOLDING
Input: data {x
i
}n
i=1
and tuning parameter τx.
Output: thresholded data x˜ n .
{ i }i=1
For i=1:n
For j =1:d
x˜
ij
= sgn(x ij) ×min |x ij|,τx
return x˜ n .
{ i }i=1 (cid:0) (cid:1)
2.1.2 PENALIZED-HUBER-REGRESSION
PENALIZED-HUBER-REGRESSION (Algorithm 3) is a type of regression using the Huber loss
with ℓ penalization. Define the Huber loss function as
1
t 1/2 (t >1)
H(t)= | |− | | , (2.2)
(t2/2 (t 1)
| |≤
and let
d t (t >1)
h(t)= H(t)= | | . (2.3)
dt (sgn(t) (t 1)
| |≤
We consider the following optimization problem. For any vector v, define the ℓ norm of v as
1
v .
1
k k
Algorithm 3 PENALIZED-HUBER-REGRESSION
Input: Input data y ,x n and tuning parameters λ ,λ .
{ i i }i=1 o s
Output: estimator βˆ.
Let βˆ be the solution to
n y x β
argmin λ2H i − ⊤i +λ β , (2.4)
β ∈Rd Xi=1 o (cid:18) λ o√n (cid:19) s k k1
return βˆ.
32.2 Result
Before we state our first main result, we introduce the restricted eigenvalue condition of the
covariance matrix, which is often used in the context of sparse estimation [9]. For a vector v and
an index set J, define v as the vector such that the i-th element of v and v is equal for i J
I I
∈
and i-th element of v is zero for i / J.
J
∈
Definition2.1(Restrictedeigenvalueconditionofthecovariancematrix). AcovariancematrixΣ
is said to satisfy the restricted eigenvalue condition RE(s,c ,κ) with some constants c ,κ>0
RE RE
if Σ21v
2
κ v
J 2
for any v Rd and any set J 1, ,d such that J s and v
Jc 1
k k ≥ k k ∈ ⊂ { ··· } | | ≤ k k ≤
c v .
RE J 1
k k
For a v Rd, define v as the number of non-zero elements of v. Then, we state our
0
∈ k k
assumption.
Assumption 2.1. Assume that
(i) x n is a sequence of i.i.d. d( 3)-dimensional random vectors with Ex = 0, finite
k{ uri t} oi= si1 s with K and max Ex2≥ 1. Additionally, Ex x = Σ satisfies Ri E(s,c ,κ),
1 ≤j ≤d ij ≤ i ⊤i RE
and minv ∈Sd −1, kv k0≤sv ⊤Σv ≥κ2 l >0,
(ii) ξ n is a sequence of i.i.d. random variables such that Eξ σ,
{ i }i=1 | i |≤
(iii) Eh ξi x =0.
λo√n × i
(cid:16) (cid:17)
Note that assumption (iii) is weaker than independence between x n and ξ n . Define
{ i }i=1 { i }i=1
j =1, ,d, and max β =c . Our first result is the following.
···
1 ≤j ≤d
|
j∗
|
β
Theorem 2.1. Suppose that Assumption 2.1 holds. Suppose that the parameters τx,λ
o
and λ
s
satisfy
n c +1 log(d/δ)
τx = , λ o√n 18K4(σ+1), λ
s
=c
s
RE λ o√n , (2.5)
log(d/δ) ≥ c 1 n
r RE − r
where c 16, and r , r and r satisfy
s Σ 1 2
≥
r =12c √sλ , r =c √sr , r =c r , (2.6)
Σ r1 s 1 r1 Σ 2 r2 Σ
where c =c (1+c )/κ, c =c (1+c )/κ and c 6. Assume that r 1 and
r1 r RE r2 r RE l r
≥
Σ
≤
7 3
max (9K2c r1 rslog( nd/δ) ,K
c
r14 c β41 s1 4 (cid:18)log( nd/δ) (cid:19)8 , kβ ∗ k1K4 (cid:18)log( nd/δ) (cid:19)2 )≤1. (2.7)
Then, withprobability atleast1 2δ,theoutputofROBUST-SPARSE-ESTIMATIONIβˆsatisfies
−
Σ21 (βˆ β ∗)
2
r Σ, βˆ β
∗ 1
r 1, βˆ β
∗ 2
r 2. (2.8)
k − k ≤ k − k ≤ k − k ≤
Remark 2.1. When we set c =1, c =6 and λ √n=18K4(σ+1) in Theorem 2.1. Then, the
s r o
error bounds become
K4(1+σ) log(d/δ)
kΣ21 (βˆ −β ∗)
k2
.
CRE
κ
s
n
, (2.9)
r
K4(1+σ) log(d/δ)
βˆ β . s , (2.10)
k −
∗ k2 CRE
κκ n
l r
where . is an inequality up to numerical and C factor.
CRE RE
4Remark 2.2. Consider the case of normal lasso estimator when x n and ξ n are sampled
from Gaussian distributions with Eξ2 =σ2 when β is estimated{ byi } ni o= r1 mal la{ ssi o}i e= s1 timator. The
i ∗
error bound of this case is . σ 1 slogd + log(1/δ) . The result (2.9) is similar to this.
CRE κ n n
However, the error bound of our esti(cid:18) maqtor does noqt converg(cid:19) e to 0 when σ 0, and our estimator
→
has across termsuchthat√s log(1/δ). Whether onecanconstructtractableestimators without
× n
these limitations in our situatioqn is a future work.
2.3 Related work
In this section, we introduce relatedwork which dealt with estimating coefficients in sparse linear
regressionby tractable estimator under finite kurtosis condition without outlier contamination.
One of the tractable estimation methods proposed in [21] can be applied to estimate β from
∗
the data y ,x n is derived from (1.1). Let the obtained estimator be denoted by βˆ and λ
{ i i }i=1 1 min
as smallest eigenvalue of Σ. Then, βˆ demonstrates the following error bound: for any γ >0,
1
1 γslogd
P βˆ β . 1 d1 γ, (2.11)
1 ∗ 2 −
k − k λ
minr
n !≥ −
when Ex = 0, x n and ξ n are independent, β , E(x β + ξ )4 and s logd are
i { i }i=1 { i }i=1 k ∗ k1 ⊤i ∗ i n
sufficientlysmall. Theadvantageofourresultscomparedto(2.11)liesinmorerelaxedaqssumption
on β , the moment of the noise, the minimum eigenvalue of Σ and sample complexity.
∗
One of the estimation methods introduced in [24] can also be applicable. The estimator βˆ
2
demonstrates the following error bound:
slogd 1
P kβˆ
2
−β
∗ k2
.ρ25 σ
r
n
!≥1
−
d2, (2.12)
under the condition Eξ i = 0, Eξ i2 = σ2 and ρ7s(logd) × log µα ρk 23β σ∗k2 slon gd . n, where
(cid:18) (cid:19)
ρ = µ /µ , and µ and µ are ‘smoothness’ and ‘strong-convexity’ paqrameters, respectively.
L α L α
Depending onthe shapeofthecovariancematrix,‘smoothness’and‘strong-convexity’parameters
canbesignificantlylargeandsmall,respectively,and[24]doesnotexplicitlyevaluatetheseparam-
eters. Theexplicitevaluationoftheeffectofthecovarianceandmorerelaxedmomentassumption
on ξ n is the advantage of our results over (2.12). On the other hand, the error bound in
{ i }i=1
(2.12) has exact recovery when σ 0.
→
None of the methods, including ours, have been able to eliminate the dependence on β .
∗ 2
k k
Constructing anestimator thatis independent of β while preservingthe exactrecoverywhen
∗ 2
k k
σ 0 remains a challenge for future work.
→
3 Case of contamination
Before presenting our method and result precisely, we roughly introduce our result. For linear
regressionproblem (1.3) with heavy-tailed x ,ξ n , our estimator satisfies
{ i i }i=1
log(d/δ) o
P kΣ21 (βˆ −β ∗) k2 ≤C {x i,ξi}n
i=1
rs
n
+
rn
!!≥1 −δ, (3.1)
tfo or ts hu effi mci oe mnt el ny tl par rg oe pen rts ieu sch ofth xat ,s ξ2 . n{x i a, nξi d}n i=1 β, kβ ∗k.1 On u, rw mhe er te ho. d{x isi,ξ bi a}n i s= e1 d, koβ n∗k1 simis pa lein theq reu sa hl oit ly diu np
g
{ i i }i=1 k ∗ k1
for x n , mitigating the impact of outliers on x n by robust sparse PCA and ℓ -penalized
{ i }i=1 { i }i=1 1
Huber loss, and our estimator is tractable. The impact of outliers on the error bound depends
5solely on the proportionof outliers. However,the requirement for the number of samples n being
proportionaltothesquareofthesparsitys2isdifferentfromtheusuallasso. Thispointisdiscussed
in Section 3.2.
3.1 Method
To estimate β in (1.3), we propose ROBUST-SPARSE-ESTIMATION II (Algorithm 4), which
∗
is an extension of ROBUST-SPARSE-ESTIMATION-I. ROBUST-SPARSE-ESTIMATION II is
inspiredbyamethodproposedin[32]. [32]proposedsomemethodsforestimatingβ whenβ has
∗ ∗
no sparsity, and derived sharp error bounds. One of the methods in [32] consists of two steps: (i)
pre-processing covariates, and (ii) executing the Huber regression with pre-processed covariates.
Ourmethodisbasedonthisone. However,wefollowdifferentpre-processings(THRESHOLDING
and COMPUTE-WEIGHT) and use the ℓ -penalized Huber regression to enable us to tame the
1
sparsity of β .
∗
Algorithm 4 ROBUST-SPARSE-ESTIMATION II
OIn up tu pt u:
t{
:y
βi
ˆ,X
i
}n
i=1
and the tuning parameters τx, λ ∗, τ suc, ε, λ
o
and λ
s
1: {X˜ i }n i=1 ←THRESHOLDING( {X i }n i=1,τx)
2: {wˆ i }n i=1 ←COMPUTE-WEIGHT( {X˜ i }n i=1,λ ∗,τ suc,ε)
3: βˆ ←PENALIZED-HUBER-REGRESSION {wˆ iy i,wˆ iX˜ i }n i=1, λ o, λ s
(cid:16) (cid:17)
COMPUTE-WEIGHT relies on the semi-definite programming developed by [4], which pro-
vides a method for sparse PCA to be robust to outliers. [4] considered a situation where samples
aredrawnfromaGaussiandistributionandcontaminatedbyoutliers. THRESHOLDINGenables
us to cast our heavy tailed situation into the framework of [4]. In Section 3.1.1, we describe the
details of COMPUTE-WEIGHT.
3.1.1 COMPUTE-WEIGHT
Define the j-th element of X
i
as X ij. For a matrix M ∈Rd1×d2 = {m
ij }1 ≤i ≤d1,1 ≤j
≤d2, define
d1 d2
M = m , M = max m . (3.2)
1 ij ij
k k i=1j=1| | k k∞ 1 ≤i ≤d1,1 ≤j ≤d2| |
XX
For a symmetric matrix M, we write M 0 if M is positive semidefinite. For a vector v, define
(cid:23)
the ℓ norm of v as v , and for a matrix M, define the absolute maximum element of M as
∞ k k∞
M . Define the following two convex sets:
k k∞
M = M Rd d : Tr(M) r2, M 0 , U = U Rd d : U λ, U 0 , (3.3)
r × λ ×
∈ ≤ (cid:23) ∈ k k∞ ≤ (cid:23)
where Tr((cid:8)M) for matrix M is the trace of(cid:9)M. To re(cid:8)duce the effects of outliers of (cid:9)covariates,
we require COMPUTE-WEIGHT (Algorithm 5) to compute the weight vector wˆ =(wˆ , ,wˆ )
1 n
···
such that the following quantity is sufficiently small:
n
Ms ∈u Mp
r
Xi=1wˆ i hX˜ iX˜ ⊤i M i−λ ∗kM k1 !, (3.4)
where λ is a tuning parameter. Evaluation of (3.4) is required in the analysis of WEIGHTED-
∗
PENALIZED-HUBER-REGRESSIONandthe roleof (3.4)isrevealedinthe proofofProposition
3.2. For COMPUTE-WEIGHT, we use a variant of Algorithm 4 of [4]. define the probability
simplex ∆n 1 as
−
n
1
∆n 1 = w [0,1]n : w =1, w . (3.5)
− i
( ∈ k k∞ ≤ n(1 ε) )
i=1 −
X
6COMPUTE-WEIGHT is as follows.
Algorithm 5 COMPUTE-WEIGHT
Input: data X˜ n and tuning parameters λ , τ and ε.
{ i }i=1 ∗ suc
Output: weight estimate wˆ = wˆ , ,wˆ .
1 n
{ ··· }
Let wˆ be the solution to
n
w
∈m ∆i nn −1Mm ∈a Mx
r
i=1w i hX˜ iX˜ ⊤i ,M i−λ ∗kM k1
!
(3.6)
X
if the optimal value of (3.6) τ
suc
≤
return wˆ
else
return fail
We note that, from the arguments of [43, 29, 30], we have
n n
min max w X˜ X˜ ,M λ M = min min max w X˜ X˜ U,M .
w ∈∆n −1M ∈M r Xi=1
i
h
i ⊤i
i− ∗k
k1
! w ∈∆n −1U ∈U λ ∗M ∈M r* Xi=1
i i ⊤i
− +
(3.7)
COMPUTE-WEIGHT and Algorithm 4 of [4] are very similar. For any fixed w, our objective
function and the constraints are the same as the ones in Section 3 of [43] except for the values of
the tuning parameters, and we can efficiently find the optimal M M . Therefore, COMPUTE-
r
∈
WEIGHTS can be solved efficiently for the same reason as Algorithm 4 of [4].
To analyze COMPUTE-WRIGHT, we introduce the following proposition. The poof of the
following proposition is provided in the appendix (Section F).
Proposition 3.1. Assume(i) of Assumption 2.1 holds. For anymatrix M M , with probability
r
∈
at least 1 δ, we have
−
i=n
1
(cid:10)x˜ ix˜ ⊤i n,M
(cid:11)
≤
√2K2 rlog( nd/δ) +τx2log( nd/δ) +2K τx24 !kM k1+ kΣ kopr2, (3.8)
X
where Σ is the operator norm of Σ.
op
k k
We consider the succeeding condition of Algorithm 5. Define λ and w n as
∗ { i◦ }i=1
log(d/δ) log(d/δ) K4 1 i
λ =c √2K2 +τ2 +2 , w = n(1 ε) ∈I , (3.9)
∗ ∗ r n x n τ x2 ! i◦ (0 − i
∈O
where c 1 . Then, we have, with probability at least 1 δ,
∗ ≥ 1 −ε −
n
(a)
max wˆ X˜ X˜ ,M λ M max w X˜ X˜ ,M λ M
M ∈M r
Xi=1
i
h
i ⊤i
i− ∗k
k1
! ≤ M ∈M r
Xi
∈I
i◦
D
i ⊤i
E− ∗k
k1
!
= max w x˜ x˜ ,M λ M
M ∈M r
i
i◦ i ⊤i
− ∗k
k1
!
X∈I (cid:10) (cid:11)
n
(b) 1
max x˜ x˜ ,M λ M
≤ M ∈M r
i=1
n(1 −ε)
i ⊤i
− ∗k
k1
!
X (cid:10) (cid:11)
n
1 1
= Mm ∈a Mx
r
i=1
n
x˜ ix˜ ⊤i ,M −λ ∗(1 −ε) kM k1
!× 1 −ε
X (cid:10) (cid:11)
(c) Σ
k kop r2, (3.10)
≤ 1 ε
−
7where(a)followsfromtheoptimalityofwˆ ,o/n εand w n ∆n 1,(b)followsfrompositive
i ≤ { i◦ }i=1 ∈ −
semi-definiteness of M, and (c) follows from Proposition 3.1 and the definition of λ . Therefore,
when τ suc
≥
k 1Σ ko εpr2, we see that COMPUTE-WEIGHT succeed and return wˆ wit∗ h probability
atleast1 δ. W−e note that(3.10)is usedin the proofofthe followingproposition,which is plays
−
an important role in the proof of Theorem 3.1.
Proposition 3.2. Assume that (i) of Assumption 2.1 holds and λ satisfies (3.9). Let r be
Σ
∗
some positive constant, and r = c √sr , r = r = c r , where c = c (1+c )/κ, c =
1 r1 Σ 2 r2 Σ r1 r RE r2
c r(1+c RE)/κ l and c r ≥6. Further, let ε=c εno and τ suc = k 1Σ ko εpr 22, where c ε ≥1. Lastly, assume
1 ε > 0, and define c = max 1 ,c . Suppose that (3.10− ) holds and COMPUTE-WEIGHT
re− turns wˆ. Then, for an′ ∗y u {R1 −nε suc∗ h} that u c for a numerical constant c and for any
v r Bd r Bd r Bd, wk e hk a∈ ve k k∞ ≤
∈ 1 1∩ 2 2∩ Σ Σ
o log(d/δ) log(d/δ) K4 o
(cid:12)
(cid:12) (cid:12)i
X∈Owˆ iu iX˜
⊤i
v
(cid:12)
(cid:12)
(cid:12)≤2cc
′
∗c r2kΣ21
kop rn
+c r1√s sK2
r n
+τ x2
n
+
τ x2
rnr Σ,
(cid:12) (cid:12)  (3.11)
(cid:12) (cid:12)
Proof of the proposition above in the appendix (Section F).
3.2 Result
Under Assumption 2.1, we have the following theorem.
Theorem 3.1. Suppose that Assumption 2.1, and assumptions in Proposition 3.2 holds. As
Proposition 3.2, let ε = c o, where c 1 and c = max 1 ,c . Suppose that the parameters
λ o, τx and λ
s
satisfy λ
o√ε nn 18K4(σε +≥
1),
′ ∗ {1 −ε ∗}
≥
1
τx = n 4 , λ
s
=c sc RE+1 λ o√n log(d/δ) +c′ Σ21 opc r2 o , (3.12)
(cid:18)log(d/δ) (cid:19) c RE −1 r n ∗k k c r1rsn !
where c 16, and r , r , r and r satisfies
s Σ 1 2
≥
r =12c √sλ , r =c √sr , r=r =c r , (3.13)
Σ r1 s 1 r1 Σ 2 r2 Σ
where c r1 = c r(1+c RE)/κ, c r2 = c r(1+c RE)/κ l and c r
≥
6. and τ suc = k 1Σ ko εpr 22. Assume that
r 1 and −
Σ
≤
max K4 log(d/δ) 41 ,K4 c41 s41 log(d/δ) 13 6 , 3K4c2 r1 s log(d/δ) 1. (3.14)
( (cid:18) n (cid:19) c r1 β (cid:18) n (cid:19) c2 r2kΣ21 kop r n )≤
and
3
log(d/δ) 4 log(d/δ)
max (K4 kβ ∗ k1
(cid:18)
n
(cid:19)
,72K4c2 r1s
r
n
)≤1. (3.15)
Then,withprobabilityatleast1 3δ,theoutputofROBUST-SPARSE-ESTIMATIONIIβˆsatisfies
−
Σ21 (βˆ β ∗)
2
r Σ, βˆ β
∗ 1
r 1, βˆ β
∗ 2
r 2. (3.16)
k − k ≤ k − k ≤ k − k ≤
Remark 3.1. When we set c = 16, c = 6, λ √n = 18K4(σ+1) and c = 2. Then, the error
s r o ′
bounds become ∗
kΣ1 2(βˆ −β ∗)
k2
.
CRE
K4(1+σ)
κ1 rslog n(d/δ)
+
kΣ κ21
lkop
rno
!, (3.17)
kβˆ −β ∗ k2 . CRE K4(1+σ)
κ1
κ
lrslog n(d/δ)
+
kΣ κ21
2 lkop
rno
!, (3.18)
8f ho or ws eu vffi erci ne on ttly opl ta ir mge aln bes cu ac uh setha tht es2 te. rm{x i c,ξ oi n}n i t= a1 i, nkβ in∗ gk1 on. inW oh pe tn imd al= co1 n, vth eris ger ne cs eult rai ts en ie sar (l oy /o np )t
3
4im [3a ]l,
.
Whether one can construct optimal and tractable estimator in our situation is a future work.
Remark 3.2. In Theorem 3.1, we require n is sufficiently larger than s2log(d/s), and this is
a stronger condition than the ones of Theorem 2.1. This phenomenon is due to sparse PCA in
COMPUTE-WEIGHT. [43] revealed that there is no randomized polynomial time algorithm to
estimate the top eigenvector in a scheme where n is proportional to s (in [43], s is the number
of non-zero elements of the top eigenvector of covariance matrices) under the assumptions of in-
tractability of a variant of Planted Clique Problem. In addition to this research, other studies
such as [17, 8] suggested that a sparse PCA may inevitably have a dependence on n of s2 even in
the absence of outliers when using polynomial-time algorithms. Therefore, with our framework, it
would be difficult to avoid the dependence on s2 of n, even with a different analysis from ours. Re-
cently, various studies on sparse estimation problems have emerged, suggesting that the unnatural
dependence on s2 might be unavoidable for polynomial-time algorithms, using frameworks such as
SQ lower bounds and low-degree tests (i.e., [26, 36, 2, 20, 16]). These works considered sparse
clustering, mixed sparse regression, robust sparse mean estimation, tensor pca. Deriving trade-
offs between statistical and computational tradeoffs for problem similar to ours remains a future
challenge.
3.3 Related work
There are not many papers dealing with robust sparse estimation problem from data following
heavy-tailed distributions with outliers. For instance, [15, 13] addressed estimation problem of
sparsemeanvector,while [28]dealtwith sparselinear regression. Here, wewill providea detailed
introduction to [28], which shares a common problem setting with our paper. [28] proposed some
robust gradient methods for some objective functions and they are applicable to estimating β
∗
from the data y ,x n from (1.3) with slightly strong assumption on ̺ ,θ . That is to say,
{ i i }i=1 { i i }
the assumption is x ,ξ remains i.i.d.. Applying the method in [28] for squared loss as the
i i i
objective function,{ after} T∈ sI tep starting from β0 with β0 β R, we have an estimator βˆ
∗ 1 3
k − k ≤
such that,
R (1+σ2)√s log(d/δ ) o
P βˆ β . + ′ + 1 δ, (3.19)
k 3 − ∗ k2 2T/2√s minv ∈Sd −1, kv k0≤2sv ⊤Σv r n ′ rn ′!!≥ −
where n =n/T and δ =δ/T, under the condition that finite kurtosis for x n , E y x 2 and
Ey2arei′
nfinite.
Notet′ hat,theirgradientmethodisapplicablefordifferento{ bji e} ci t= iv1 efuk nci tioi k n2
sand
i
thisconditioncanbemadeweaker. Ourerrorboundissharperfromtheperspectiveofconvergence
rate because our result avoid the coss term such that so/n. However, the method in [28] has
somemeritsthanours. Forexample,thesamplecomplexityissmallerthanoursandtheirmethod
p
does not require the assumption such that s2 . n. The computational cost required in [28] is
significantly lighter than ours, and the extensive numerical experiments in [28] demonstrate the
practical effectiveness of their method.
Constructing an estimator with sharp error bounds that can be computed with light compu-
tational burden is considered a very interesting challenge.
References
[1] Pierre Alquier, Vincent Cottet, and Guillaume Lecu´e. Estimation bounds and sharp oracle
inequalities of regularized procedures with lipschitz loss functions. The Annals of Statistics,
47(4):2117–2144,2019.
[2] Gabriel Arpino and Ramji Venkataramanan. Statistical-computational tradeoffs in mixed
sparse linear regression. In The Thirty Sixth Annual Conference on Learning Theory, pages
921–986.PMLR, 2023.
9[3] Ainesh Bakshi and Adarsh Prasad. Robust linear regression: Optimal rates in polynomial
time. InProceedings of the 53rd AnnualACM SIGACT Symposium on Theory of Computing,
pages 102–115,2021.
[4] Sivaraman Balakrishnan, Simon S Du, Jerry Li, and Aarti Singh. Computationally efficient
robust sparse estimation in high dimensions. In Conference on Learning Theory, pages 169–
212. PMLR, 2017.
[5] Pierre C Bellec, Guillaume Lecu´e, and Alexandre B Tsybakov. Slope meets lasso: improved
oracle bounds and optimality. The Annals of Statistics, 46(6B):3603–3642,2018.
[6] St´ephane Boucheron, G´abor Lugosi, and Pascal Massart. Concentration inequalities: A
nonasymptotic theory of independence. Oxford university press, 2013.
[7] Olivier Bousquet. A bennett concentration inequality and its application to suprema of
empirical processes. Comptes Rendus Mathematique, 334(6):495–500,2002.
[8] MatthewBrennan,GuyBresler,SamuelBHopkins,JerryLi,andTselilSchramm. Statistical
queryalgorithmsandlow-degreetestsarealmostequivalent.arXivpreprintarXiv:2009.06107,
2020.
[9] PeterBu¨hlmannandSaraVanDeGeer. Statisticsfor high-dimensional data: methods, theory
and applications. Springer Science & Business Media, 2011.
[10] Xi Chen and Wen-Xin Zhou. Robust inference via multiplier bootstrap. The Annals of
Statistics, 48(3):1665–1691,2020.
[11] Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under
adversarial corruption. In International Conference on Machine Learning, pages 774–782.
PMLR, 2013.
[12] Arnak Dalalyan and Philip Thompson. Outlier-robust estimation of a sparse linear model
using ℓ -penalized Huber’s m-estimator. In H. Wallach, H. Larochelle, A. Beygelzimer,
1
F. d’Alch´e Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems 32, pages 13188–13198.Curran Associates, Inc., 2019.
[13] Ilias Diakonikolas, Daniel Kane, Jasper Lee, and Ankit Pensia. Outlier-robust sparse mean
estimationforheavy-taileddistributions.AdvancesinNeuralInformationProcessingSystems,
35:5164–5177,2022.
[14] Ilias Diakonikolas and Daniel M Kane. Algorithmic high-dimensional robust statistics. Cam-
bridge university press, 2023.
[15] Ilias Diakonikolas, Daniel M Kane, Sushrut Karmalkar, Ankit Pensia, and Thanasis Pittas.
Robustsparsemeanestimationviasumofsquares. InConference on Learning Theory, pages
4703–4763.PMLR, 2022.
[16] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds for
robust estimation of high-dimensional gaussians and gaussian mixtures. In 2017 IEEE 58th
Annual Symposium on Foundations of Computer Science (FOCS), pages 73–84.IEEE,2017.
[17] Yunzi Ding, Dmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira. Subexponential-
time algorithms for sparse pca. Foundations of Computational Mathematics, pages 1–50,
2023.
[18] Sjoerd Dirksen. Tail bounds via generic chaining. Electronic Journal of Probability, 20:1–29,
2015.
[19] JianqingFan,HanLiu,QiangSun,andTongZhang.I-lammforsparselearning: Simultaneous
control of algorithmic complexity and statistical error. Annals of statistics, 46(2):814,2018.
10[20] JianqingFan,HanLiu,ZhaoranWang,andZhuoranYang.Curseofheterogeneity: Computa-
tionalbarriersinsparsemixturemodelsandphaseretrieval.arXivpreprintarXiv:1808.06996,
2018.
[21] Jianqing Fan, Weichen Wang, and Ziwei Zhu. A shrinkage principle for heavy-tailed data:
High-dimensional robust low-rank matrix recovery. Annals of statistics, 49(3):1239,2021.
[22] Martin Genzel and Christian Kipp. Generic error bounds for the generalized lasso with sub-
exponential data. Sampling Theory, Signal Processing, and Data Analysis, 20(2):15, 2022.
[23] YuanKe,StanislavMinsker,ZhaoRen, QiangSun, andWen-XinZhou. User-friendlycovari-
ance estimation for heavy-tailed distributions. Statistical Science, 34(3):454–471,2019.
[24] Liu Liu, Tianyang Li, and Constantine Caramanis. High dimensional robust m-estimation:
Arbitrary corruption and heavy tails. arXiv preprint arXiv:1901.08237, 2019.
[25] Liu Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. High dimensional robust
sparse regression. In International Conference on Artificial Intelligence and Statistics, pages
411–421.PMLR, 2020.
[26] MatthiasLo¨ffler,AlexanderSWein,andAfonsoSBandeira. Computationallyefficientsparse
clustering. Information and Inference: A Journal of the IMA, 11(4):1255–1286,2022.
[27] Gabor Lugosi and Shahar Mendelson. Risk minimization by median-of-means tournaments.
Journal of the European Mathematical Society, 22(3):925–965,2019.
[28] Ibrahim Merad and St´ephane Ga¨ıffas. Robust methods for high-dimensional linear learning.
Journal of Machine Learning Research, 24(165):1–44,2023.
[29] Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequali-
ties with lipschitz continuous monotone operators and smooth convex-concave saddle point
problems. SIAM Journal on Optimization, 15(1):229–251,2004.
[30] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming,
103(1):127–152,2005.
[31] Nam H Nguyen and Trac D Tran. Robust lasso with missing and grossly corrupted observa-
tions. IEEE transactions on information theory, 59(4):2036–2058,2012.
[32] AnkitPensia,VarunJog,andPo-LingLoh. Robustregressionwithcovariatefiltering: Heavy
tails and adversarialcontamination. arXiv preprint arXiv:2009.12976, 2020.
[33] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for
correlatedgaussiandesigns. The Journal of Machine Learning Research, 11:2241–2259,2010.
[34] TakeyukiSasai and HironoriFujisawa. Outlier robust and sparse estimation of linear regres-
sion coefficients. arXiv preprint arXiv:2208.11592, 2022.
[35] TakeyukiSasai and HironoriFujisawa. Outlier robust and sparse estimation of linear regres-
sion coefficients. arXiv preprint arXiv:2208.11592, 2022.
[36] TselilSchrammandAlexanderSWein. Computationalbarrierstoestimationfromlow-degree
polynomials. The Annals of Statistics, 50(3):1833–1858,2022.
[37] Vidyashankar Sivakumar, Arindam Banerjee, and Pradeep K Ravikumar. Beyond sub-
gaussian measurements: High-dimensional structured estimation with sub-exponential de-
signs. Advances in neural information processing systems, 28, 2015.
[38] Qiang Sun, Wen-Xin Zhou, and Jianqing Fan. Adaptive Huber regression. Journal of the
American Statistical Association, 115(529):254–265,2020.
11[39] Michel Talagrand. Upper and lower bounds for stochastic processes, volume 60. Springer,
2014.
[40] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society: Series B, 58(1):267–288,1996.
[41] Roman Vershynin. High-dimensional probability: An introduction with applications in data
science, volume 47. Cambridge university press, 2018.
[42] MartinJWainwright.Sharpthresholdsforhigh-dimensionalandnoisysparsityrecoveryusing
ℓ -constrained quadratic programming (lasso). IEEE transactions on information theory,
1
55(5):2183–2202,2009.
[43] Tengyao Wang, Quentin Berthet, and Richard J Samworth. Statistical and computational
trade-offsinestimationofsparseprincipalcomponents. The Annals of Statistics,44(5):1896–
1930,2016.
A Key propositions for Theorems 2.1 and Theorems 3.1
In this section, we provide four propositions needed to prove Theorems 2.1 and 3.1. First, we
introduce Proposition A.1, which is used in the proof of both Theorems 2.1 and 3.1. Proposition
A.1 pertains to the ℓ -penalized Huber loss and is stated without considering the randomness of
1
the input and output, i.e., it does not take Assumption 2.1 into account. The applicability of
Proposition A.1 under Assumption 2.1 is ensured by the subsequent propositions.
PropositionA.1. DefinetheinputdataforPENALIZED-HUBER-REGRESSIONas Z ,Y n .
Suppose that, for any v r Bd r Bd r Bd, { i i }i=1
∈ 1 1∩ 2 2∩ Σ Σ
n 1 Y Z ,β
(cid:12)
(cid:12)λ o√n
Xi=1
nh
(cid:18)
i −
λ
oh √i
n
∗ i (cid:19)Z ⊤i v
(cid:12)
(cid:12)≤r a,Σr Σ, (A.1)
(cid:12) (cid:12)
and for any v ∈r 1Bd 1∩r 2(cid:12) (cid:12)Bd
2
such that kΣ21v
k2
=r Σ, (cid:12) (cid:12)
n 1 Y Z ,β +v Y Z ,β
b kΣ21 v k2 2−r b,Σr
Σ
≤λ o√n
n
−h i −h λi √n∗ i +h i − λh √i
n
∗ i Z⊤i v, (A.2)
i=1 (cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19)
X
where r ,r 0 and b>0. Suppose that Ex x =Σ satisfies RE(s,c ,κ) with κ >0, and
a,Σ b,Σ
≥
i ⊤i RE l
λ + ra,Σ
λ
r a,Σ
>0,
s cr1√s
c , (A.3)
s − c √s λ ra,Σ ≤ RE
r1 s − cr1√s
2
r r +r +c √sλ , r =c √sr r =c r (A.4)
Σ
≥ b
a,Σ b,Σ r1 s 1 r1 Σ 2 r2 Σ
(cid:0) (cid:1)
hold, where c =c (1+c )/κ, c =c (1+c )/κ and c 6. Then, we have the following:
r1 r RE r2 r RE l r
≥
β
∗
βˆ
1
r 1, β
∗
βˆ
2
r 2, Σ21 (β
∗
βˆ)
2
r Σ. (A.5)
k − k ≤ k − k ≤ k − k ≤
In[35],apropositionalmostidenticaltoPropositionA.1wasintroduced,andforcompleteness,
we provide a proof. We note that Proposition A.1 is a modification of the claim found in [27, 1]
to deal with the case where the covarianceof the covariatehas a generalform (not identity). The
proof of Proposition A.1 is provided in Sections C and D.
Second, we introduce Propositions A.2 and A.3. By these propositions, we see that (A.1) and
(A.2) are satisfied with high probability for appropriate values of b > 0,r ,r ,r ,r ,r 0
a,Σ b,Σ 1 2 Σ
≥
under the assumptions in Theorem 2.1.
12Proposition A.2. Suppose that the Assumption 2.1 holds and r = c √sr , r = c r , where
1 r1 Σ 2 r2 Σ
c , c are some positive constants. Then, for any v r Bd r Bd r Bd, with probability at
r1 r2 ∈ 1 1 ∩ 2 2 ∩ Σ Σ
least 1 δ, we have
−
n 1 x x˜ ,β +ξ
h h i − i ∗ i i x˜ ,v
i
(cid:12)
(cid:12)Xi=1
n
(cid:18)
λ o√n (cid:19)h i(cid:12)
(cid:12)
(cid:12) (cid:12) (cid:12) ≤4 c r1 rslog( nd/δ) +c r1τx√slog( n(cid:12) (cid:12) (cid:12)d/δ) +K4c r1√
τ
x3s +K4c β41 s3 4 (cid:18)τ1
x
(cid:19)3 −1 4 !r Σ. (A.6)
Proposition A.3. Suppose that the Assumption 2.1 holds and r = c √sr , r = c r , where
1 r1 Σ 2 r2 Σ
c , c are some positive constants. Assume that λ √n 18K4(σ +1). Then, for any v
r1 r2 o
≥ ∈
r 1Bd 1∩r 2Bd
2
such that kΣ21v
k2
=r Σ, where r
Σ
≤1, with probability at least 1 −δ, we have
n 1 x x˜ ,β +ξ x˜ v x x˜ ,β +ξ
λ o√n
n
−h h i − λi √n∗ i i
−
λ⊤i
√n
+h h i − λi √n∗ i i x˜ ⊤i v
i=1 (cid:18) (cid:18) o o (cid:19) (cid:18) o (cid:19)(cid:19)
X
2 K4 K4 s
≥kΣ1 2v k2
2
1 −K2
sλ o√n
sσ+ kβ
∗k1 τ x3
+ s1+c r1√s
τ x3
!−12K4c2
r1τ x2 !
K2 log(d/δ) slog(d/δ)
−3λ o√nc
r1
1+
τx
+τx
r
n
!r
n
kΣ1 2v k2. (A.7)
To prove Theorem 3.1, we need the following propositions (Propositions A.4 and A.5), in
additiontothepropositionsabove. Thepoofofthe propositionsareprovidedinSectionF.Define
I as the index set such that I = m.
m m
| |
Proposition A.4. Suppose that (3.8)holds. For any u Rn such that u c for a numerical
constant c and for any v r Bd r Bd r Bd, we have∈ k k∞ ≤
∈ 1 1∩ 2 2∩ Σ Σ
1 m log(d/δ) log(d/δ) K4 m
(cid:12)
(cid:12) (cid:12)i X∈Im
nu ix˜ ⊤i v (cid:12)
(cid:12)
(cid:12)≤2c c r2kΣ21 kop rn +c r1√s sK2 r n +τx2 n + τ x2 rnr Σ.
(cid:12) (cid:12)  (A.8)
(cid:12) (cid:12)
Let I and I be the sets of the indices such that w <1/(2n) and w 1/(2n), respectively.
< i i
≥ ≥
Proposition A.5. Suppose 0<ε<1. Then, for any w ∆n 1, we have I 2nε.
− <
∈ | |≤
B Auxiliary lemmas
For an event E, define the indicator function of E as I . For a vector v Rd, we denote the i-th
E
∈
element of the vector as v . In this section, we introduce some useful lemmas.
i
Lemma B.1. Define z R as a random variable such that Ez4 σ4 . Then we have
∈ ≤ z,4
σ4
EI z,4 . (B.1)
|z |≥τz ≤ τ z4
Proof.
(a) (b) z 4 σ4
EI P(z τ ) E| | z,4 , (B.2)
|z |≥τz ≤ | |≥ z ≤ τ z4 ≤ τ z4
where (a) follows fromthe relationshipbetween expectationand probability,and(b) followsfrom
Markov’s inequality.
13Lemma B.2. Define z Rd as a random vector such that max Ez4 σ4 , and define ˜z
∈ 1 ≤j ≤d j ≤ z,4
as a random vector such that for any j 1, ,d , z˜
j
= sgn(z j)min(τz, z
j
) with a positive
constant τz. Then for any v Rd, we hav∈ e { ··· } | |
∈
4σz4
,4 v 2+E(z v)2 E(˜z v)2
4σz4
,4 v 2+E(z v)2. (B.3)
− τz2 k k1 ⊤ ≤ ⊤ ≤ τz2 k k1 ⊤
Proof. From simple algebra, we have
E(˜z v)2 =E ˜z˜z ,vv
⊤ ⊤ ⊤
h i
=E h˜z˜z⊤i −zz⊤,vv⊤ i+E hzz⊤,vv⊤
i
=E ˜z˜z zz ,vv +E(z v)2, (B.4)
⊤ ⊤ ⊤ ⊤
h − i
and we have
E ˜z˜z⊤ zz⊤,vv⊤ +E(z⊤v)2 E(˜z⊤v)2 E ˜z˜z⊤ zz⊤,vv⊤ +E(z⊤v)2, (B.5)
−| h − i| ≤ ≤| h − i|
For any 1 j,k, d, we have
≤ ≤
E(z˜ z˜ z z ) = E(z˜ (˜z z )+z (˜z z ))
j k j k j k k k j j
| − | | − − |
E˜z (˜z z )+ Ez (˜z z )
j k k k j j
≤| − | − |
E˜z ˜z z +Ez ˜z z
j k k k j j
≤ | || − | | || − |
=E |˜z j ||˜z k −z k |I |z k|>τz +E |z k ||˜z j −z j |I |z j|>τz
≤E |z j ||˜z k −z k |I |z k|>τz +E |z k ||˜z j −z j |I |z j|>τz
≤2E |z j ||z k |I |z k|>τz +2E |z k ||z j |I |z j|>τz
≤2 Ez2 jz2 k EI |z k|>τz +2 Ez2 kz2 j I |z j|>τz
q q q σ4 q
=4 Ez2 jz2 k EI |z k|>τz ≤4 τz z2,4 , (B.6)
q q
where the last inequality follows from Lemma B.1 and max Ez4 σ4 .
1 ≤j ≤d j ≤ z,4
From (B.5) and (B.6), we have
σ4 σ4
−4 τz z2,4 kv k2 1+E(z⊤i v)2 ≤E(˜z⊤v)2 ≤4 τz z2,4 kv k2 1+E(z⊤v)2. (B.7)
Lemma B.3. Define z
∈
Rd as a random vector such that max
1 ≤j
≤dEz4
j ≤
σz4 ,4, and define ˜z
as a random vector such that for any j 1, ,d , z˜
j
= sgn(z j)min(τz, z
j
) with a positive
constant τz. Then for any v Rd, we hav∈ e { ··· } | |
∈
E ˜z z,v 2 v
σz4
,4 (B.8)
|h − i|≤ k k1 τ z3
14Proof. From simple algebra, we have
E ˜z z,v =E (˜z z )v
|h − i| (cid:12) j − j j(cid:12)
(cid:12)1 j d (cid:12)
(cid:12) ≤X≤ (cid:12)
(cid:12) (cid:12) E (˜z z )v (cid:12) (cid:12)
≤ (cid:12) | j − j j(cid:12)|
1 j d
≤X≤
≤2 E z jI |z j|≥τzv j
1 j d
≤X≤ (cid:12) (cid:12)
(cid:12) 1 (cid:12)
( =a) 2 E |z jv
j
|4 4 EIz
j≥τz
43
1 ≤Xj ≤d(cid:16) (cid:17)
(cid:0) (cid:1)
3
(b) σz4
,4
4 σz4
,4
=2 |σz,4v
j | τ z4 !
=2 kv
k1 τ z3
, (B.9)
1 j d
≤X≤
where (a) follows from Ho¨lder’s inequality, and (b) follows from Lemma B.1.
Lemma B.4. Define z n as a sequence of i.i.d. d-dimensional random vector such that
max
1 ≤j
≤dEz2
ij ≤
1. Fo{ r ai } ni y=1 1
≤
i
≤
n and 1
≤
j
≤
d define z˜
ij
= sgn(z ij)min(τz, |z ij|) with a
positive constant τz. Define {α
i
}n
i=1
as a sequence of i.i.d. Rademacher random variables which
are independent of ˜z n . Then we have
{ i }i=1
n
1 logd logd
E
v
∈su r1p
Bd 1(cid:12) (cid:12)n
Xi=1α iz˜
⊤i
v
(cid:12) (cid:12)≤
r2
n
r 1+τz
n
r 1. (B.10)
(cid:12) (cid:12)
Proof. WenotethatEα2z˜2 1a(cid:12)ndEαp z˜ p (cid:12) τp 2Ez˜2 τp 2. Then,wehave 1 n Eαp z˜ p
i ij ≤ (cid:12) i| ij| ≤(cid:12) z− ij ≤ z− n i=1 i| ij| ≤
τp 2. From Lemma 14.12 of [9] and d 3, we have
z− ≥ P
n
1 logd logd
E α i˜z
i
2 +τz . (B.11)
(cid:13) (cid:13) (cid:13)n Xi=1 (cid:13) (cid:13)
(cid:13)∞
≤ r n n
Lastly, from Ho¨lder’s inequality(cid:13), the proof(cid:13)is complete.
(cid:13) (cid:13)
C Preparation of proof of Proposition A.1
For η (0,1), let
∈
θ =βˆ β , θ =(βˆ β )η. (C.1)
∗ η ∗
− −
We introduce the following four lemmas, that are used in the proof of Proposition A.1.
Lemma C.1. Suppose that (A.1), (A.3), θ
η 1
r 1, θ
η 2
= r
2
and Σ21θ
η 2
r Σ, where
k k ≤ k k k k ≤
r =c √sr and r =c r hold. Then, for any fixed η (0,1), we have
1 r1 Σ 2 r2 Σ
∈
1+c
θ
η 2
3 RE Σ21θ
η
2. (C.2)
k k ≤ κ k k
l
Lemma C.2. Suppose that (A.1), (A.3), θ
η 1
= r 1, θ
η 2
r
2
and Σ21θ
η 2
r Σ, where
k k k k ≤ k k ≤
r =c √sr and r =c r hold. Then, for any fixed η (0,1), we have
1 r1 Σ 2 r2 Σ
∈
1+c
θ
η 1
3 RE√s Σ21θ
η
2. (C.3)
k k ≤ κ k k
Lemma C.3. Suppose that (A.1), (A.2), θ
η 1
r 1, θ
η 2
r
2
and Σ21θ
η 2
= r Σ, where
k k ≤ k k ≤ k k
r =c √sr and r =c r hold. Then, for any η (0,1), we have
1 r1 Σ 2 r2 Σ
∈
1
kΣ21θ
η k2
≤ b
r a,Σ+r b,Σ+c r1√sλ
s
. (C.4)
(cid:0) (cid:1)
15C.1 Proof of Lemma C.1
Foravectorv=(v , ,v ),define v#, ,v# asanon-increasingrearrangementof v , , v ,
1 ··· d { 1 ··· d } {| 1 | ··· | d |}
and v# Rd as a vector such that v# =v#. For the sets S = 1,...,s and S = s+1,...,d ,
∈ |i i 1 { } 2 { }
let v#1 =v# and v#2 =v#.
S1 S2
In Section C.1.1, we have
√1+c
θ
η 2
RE Σ1 2θ
η 2
(C.5)
k k ≤ κ k k
l
assuming θ θ /√s, and in Section C.1.2, we have
η 2 η 1
k k ≤k k
2
kθ
η k2
≤2 kθ η#1
k2
≤ κ
kΣ1 2θ
η k2
(C.6)
l
assuming θ θ /√s. From the above two inequalities, we have
η 2 η 1
k k ≥k k
3+c
θ
η 2
RE Σ1 2θ
η
2. (C.7)
k k ≤ κ k k
l
C.1.1 Case I
In Section C.1.1, suppose that θ θ /√s. Let
η 2 η 1
k k ≤k k
1 n Y Z ,β +θ Y Z ,β
Q ′(η)=λ o√n
n
−h i −h λi √n∗ η i +h i − λh √i
n
∗ i Z ⊤i θ. (C.8)
i=1(cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19)
X
From the proof of Lemma F.2. of [19], we have ηQ(η) ηQ(1) and this means
′ ′
≤
n 1 Y Z ,β +θ Y Z ,β
λ o√n
n
−h i −h λi √n∗ η i +h i − λh √i
n
∗ i Z⊤i θ
η
i=1 (cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19)
X
n 1 Y Z ,βˆ Y Z ,β
≤λ o√n nη −h i − λh √ni i +h i − λh √i
n
∗ i Z ⊤i θ. (C.9)
i=1 o ! (cid:18) o (cid:19)!
X
Let ∂v be the sub-differentialof v . Adding ηλ ( βˆ β ) to both sides of (C.9), we have
1 s 1 ∗ 1
k k k k −k k
n 1 Y Z ,β +θ Y Z ,β
λ o√n
i=1
n
(cid:18)−h
(cid:18)
i −h λi o√n∗ η i (cid:19)+h
(cid:18)
i −
λ
oh √i
n
∗ i (cid:19)(cid:19)Z ⊤i θ η+ηλ ∗( kβˆ k1 −kβ ∗ k1)
X
(a) n 1 Y Z ,βˆ Y Z ,β
≤
λ o√n
i=1
nη −h i −
λ
oh √ni i !+h
(cid:18)
i −
λ
oh √i
n
∗ i (cid:19)!Z ⊤i θˆ+ηλ s h∂βˆ,θ
i
X
( =b) λ o√n n n1 h Y i − λhZ √i n,β ∗ i Z⊤i θ η, (C.10)
i=1 (cid:18) o (cid:19)
X
where (a) follows from βˆ β ∂βˆ,θ ,which is the definition of the sub-differential,and
1 ∗ 1
k k −k k ≤h i
(b) follows from the optimality of βˆ.
From the convexity of Huber loss, the first term of the left hand side of (C.10) is positive and
we have
n 1 Y Z ,β
0
≤
λ o√n nh i − λh √i
n
∗ i Z⊤i θ η+ηλ s( kβ ∗ k1 −kβˆ k1). (C.11)
i=1 (cid:18) o (cid:19)
X
From (A.1), the first term of the right-hand side of (C.11) is evaluated as
n λ o√n n1 h Y i − λhZ √i n,β ∗ i Z ⊤i θ η ≤r a,Σr Σ ( ≤a) r ca,Σ r 2 ( =b) r ca,Σ kθ η k2 ( ≤c) cr a √,Σ skθ η k1, (C.12)
i=1 (cid:18) o (cid:19) r2 r2 r2
X
16where(a)followsfromr =c r ,(b)followsfromtheassumption θ =r and(c)followsfrom
2 r2 Σ
k
η k2 2
the assumption θ θ /√s. From (C.11), (C.12) and the assumption θ θ /√s,
η 2 η 1 η 2 η 1
k k ≤k k k k ≤k k
we have
r
0 a,Σ θ +ηλ ( β βˆ ). (C.13)
η 1 s ∗ 1 1
≤ c √sk k k k −k k
r2
fD oe rfi in ∈e JJ aa aa ns dth θe η,i Jn ad |e ix =se 0t fo of rn io ∈/n- Jz aer .o Fe un rt tr hi ee rs mo of ra e, ,a wn ed sθ eeη, Ja as a vector such that θ η, Ja|i =θ η |i
r
0 a,Σ θ +ηλ ( β βˆ )
η 1 s ∗ 1 1
≤ c √sk k k k −k k
r2
r
≤ c
r2a √,Σ s( kθ η, Jβ∗k1+ kθ η, Jβc ∗k1)+ηλ s( kβ J∗
β∗
−βˆ Jβ∗k1 −kβˆ Jβc ∗k1)
r r
= (cid:18)λ s+
c
r2a √,Σ
s
(cid:19)kθ η, Jβ∗k1+ (cid:18)−λ s+
c
r2a √,Σ
s
(cid:19)kθ η, Jβc ∗k1. (C.14)
Then, we have
λ + ra,Σ λ + ra,Σ
kθ η, Jβc ∗k1
≤
λs
s
−
c crr r2 2a,√ √Σs skθ η, Jβ∗k1 ( ≤a) λs
s
−
c crr ra1 1,√ √Σs skθ η, Jβ∗k1 ( ≤b) c RE kθ η, Jβ∗k1, (C.15)
where (a) follows from the fact that c c and (b) follows from (A.3), and from the definition
r2
≥
r1
of θ#2 and θ#1 , we have
k η k1 k η k1
θ#2 c θ#1 . (C.16)
k η k1 ≤ RE k η k1
Then, from the standard shelling argument, we have
θ#2 2 = d (θ# )2 d θ# 1 s θ# 1 θ#1 θ#2 c RE kθ η#1 k2 1 c θ#1 2.
k η k2 η |i ≤ η |i s η |j ≤ sk η k1 k η k1 ≤ s ≤ RE k η k2
i=s+1 i=s+1 j=1
X X (cid:12) (cid:12) X(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (C.17)
and from the definition of κ , we have
l
κ2 lkθ
η
k2
2
≤κ2
l
kθ η#1 k2 2+ kθ η#2 k2
2
≤κ2 l(1+c RE) kθ η#1 k2
2
≤(1+c RE) kΣ1 2θ
η
k2 2. (C.18)
(cid:0) (cid:1)
C.1.2 Case II
In Section C.1.2, suppose that θ θ /√s.
η 2 η 1
k k ≥k k
d d s
1 1
θ#2 2 = (θ# )2 θ# θ# θ#1 θ#2 θ#1 θ .
k η k2 η |i ≤ η |i s η |j ≤ sk η k1 k η k1 ≤k η k2 k η k2
i=s+1 i=s+1 j=1
X X (cid:12) (cid:12) X(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (C.19)
Then, we have
θ 2 θ#1 2+ θ#2 2 θ#1 θ + θ#1 θ θ 2 θ#1 , (C.20)
k η k2 ≤k η k2 k η k2 ≤k η k2 k η k2 k η k2 k η k2 ⇒k η k2 ≤ k η k2
and we have
2 2
kθ
η k2
≤2 kθ η#1
k2
≤ κ
kΣ21θ η#1
k2
≤ κ
kΣ21θ k2. (C.21)
l l
17C.2 Proof of Lemma C.2
From the same argument of the proof of Lemma C.1, we have (C.11). From (A.1), the first term
of the right-hand side of (C.11) is evaluated as
n 1 Y Z ,β (a) 1
λ o√n nh i − λh √i
n
∗ i Z ⊤i θ η ≤r a,Σr Σ
≤ c
√sr a,Σ kθ η k1. (C.22)
i=1 (cid:18) o (cid:19) r1
X
where (a) follows from r =c √sr . From (C.11) and (C.22), we have
1 r1 Σ
r
0 a,Σ θ η 1+ηλ s( β ∗ 1 βˆ 1). (C.23)
≤ c √sk k k k −k k
r1
Furthermore, we see
r
0 a,Σ θ η 1+ηλ s( β ∗ 1 βˆ 1)
≤ c √sk k k k −k k
r1
r
≤ c
r1a √,Σ s( kθ η, Jβ∗k1+ kθ η, Jβc ∗k1)+ηλ s( kβ J∗
β∗
−βˆ Jβ∗k1 −kβˆ Jβc ∗k1)
r r
= (cid:18)λ s+
c
r1a √,Σ
s
(cid:19)kθ η, Jβ∗k1+ (cid:18)−λ s+
c
r1a √,Σ
s
(cid:19)kθ η, Jβc ∗k1. (C.24)
Then, we have
λ + ra,Σ
kθ η, Jβc ∗k1
≤
λs
s
−
c crr ra1 1,√ √Σs skθ η, Jβ∗k1 ( ≤a) c RE kθ η, Jβ∗k1, (C.25)
where (a) follows from (A.3), and we have
kθ η k1 = kθ η, Jβ∗k1+ kθ η, Jβc ∗k1 ≤(1+c RE) kθ η, Jβ∗k1 ≤(1+c RE)√s kθ η, Jβ∗k2. (C.26)
From (C.25) and the restricted eigenvalue condition, we have
1+c
kθ
η k1
≤(1+c RE)√s kθ
η, Jβ∗k2
≤
κRE√s kΣ1 2θ
η
k2. (C.27)
C.3 Proof of Lemma C.3
From the same argument of the proof of Lemma C.1, we have (C.11). From (C.11), we have
n 1 Y Z ,β +θ Y Z ,β
λ o√n
n
−h i −h λi √n∗ η i +h i − λh √i
n
∗ i Z⊤i θ
η
i=1 (cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19)
X
n 1 Y Z ,β
≤λ o√n nh i − λh √i
n
∗ i Z ⊤i θ η+ηλ s( kβ ∗ k1 −kβˆ k1). (C.28)
i=1 (cid:18) o (cid:19)
X
Weevaluateeachtermof (C.28). From(A.2)andfromr
Σ
= Σ21θ
η
2,theleft-handsideof (C.28)
k k
is evaluated as
n 1 Y Z ,β +θ Y Z ,β
λ o√n
n
−h i −h λi √n∗ η i +h i − λh √i
n
∗ i Z⊤i θ
η
≥b kΣ21θ
η
k2 2−r
b,Σ
kΣ21θ
η
k2.
i=1 (cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19)
X
(C.29)
From (A.1) and r
Σ
= Σ1 2θ
η
2, the first term of the right-hand side of (C.28) is evaluated as
k k
n 1 Y Z ,β
λ o√n nh i − λh √i
n
∗ i Z ⊤i θ η ≤r a,Σ kΣ21θ η k2. (C.30)
i=1 (cid:18) o (cid:19)
X
18Fromr
1
=c r1√sr
Σ
=c r1√s kΣ1 2θ
η
k2,thesecondtermoftheright-handsideof (C.28)isevaluated
as
ηλ s( kβ
∗ k1
−kβˆ k1) ≤λ
s
kθ
η k1
≤c r1√sλ
s
kΣ1 2θ
η
k2. (C.31)
Combining the inequalities above, we have
b kΣ21θ
η
k2
2 ≤
r a,Σ+r b,Σ+c r1√sλ
s
kΣ21θ
η
k2, (C.32)
and from Σ21θ
η 2
0, we have (cid:0) (cid:1)
k k ≥
1
kΣ21θ
η k2
≤ b
r a,Σ+r b,Σ+c r1√sλ
s
, (C.33)
(cid:0) (cid:1)
and the proof is complete.
D Proof of Proposition A.1
D.1 Step1
Wederiveacontradictionif θ 1>r 1, θ 2>r
2
and Σ21θ
2
>r
Σ
hold. Assumethat θ
1
>r 1,
k k k k k k k k
kθ
k2
> r
2
and kΣ21θ
k2
> r Σ. Then we can find η 1, η 2,η
2′
∈
(0,1) such that kθ
η1k1
= r 1,
kθ
η2k2
= r
2
and kΣ21θ
η 2′k2
=r
Σ
hold. Define η
3
= min {η 1,η 2,η
2′
}. We consider the case η
3
= η
2′
in Section D.1.1, the case η =η in Section D.1.2, and the case η =η in Section D.1.3.
3 2 3 1
D.1.1 Step 1(a)
Assumethatη
3
=η 2′. We seethat kΣ21θ
η3k2
=r Σ, kθ
η3k1
≤r
1
and kθ
η3k2
≤r
2
hold. Then,from
Lemma C.3, we have
1
kΣ21θ
η3k2
≤ b
r a,Σ+r b,Σ+c r1√sλ
s
. (D.1)
(cid:0) (cid:1)
The case η
3
=η
2′
is a contradiction from kΣ21θ
η3k2
=r
Σ
and (A.4).
D.1.2 Step 1(b)
Assumethatη
3
=η 2. We seethat kΣ21θ
η3k2
≤r Σ, kθ
η3k1
≤r
1
and kθ
η3k2
=r
2
hold. Then,from
Lemma C.1, we have
3+c 3+c 1+c
kθ
η3k2
≤ κ
RE kΣ21θ
η3k2
≤ κ
RE r
Σ
≤3
κ
RE r Σ. (D.2)
l l l
The case η =η is a contradiction from θ =r and (A.4).
3 2
k
η3k2 2
D.1.3 Step 1(c)
Assumethatη
3
=η 1. We seethat kΣ21θ
η3k2
≤r Σ, kθ
η3k1
=r
1
and kθ
η3k2
≤r
2
hold. Then,from
Lemma C.2, for η =η , we have
3
1+c 1+c 1+c
kθ
η3k1
≤
κRE√s kΣ21θ
η3k2
≤
κRE√sr
Σ
≤3 κRE√sr Σ. (D.3)
The case η =η is a contradiction from θ =r and (A.4).
3 1
k
η3k1 1
19D.2 Step 2
From the arguments in Section D.1, we have Σ1 2θ
2
r
Σ
or θ
1
r
1
or θ
2
r
2
holds.
k k ≤ k k ≤ k k ≤
(a) In Section D.2.1, assume that Σ21θ
2
r
Σ
and θ
1
> r
1
and θ
2
> r
2
hold and then
k k ≤ k k k k
derive a contradiction.
(b) In Section D.2.2, assume that Σ21θ
2
> r
Σ
and θ
1
r
1
and θ
2
> r
2
hold and then
k k k k ≤ k k
derive a contradiction.
(c) In Section D.2.3, assume that Σ21θ
2
> r
Σ
and θ
1
> r
1
and θ
2
r
2
hold and then
k k k k k k ≤
derive a contradiction.
(d) In Section D.2.4, assume that Σ21θ
2
> r
Σ
and θ
1
r
1
and θ
2
r
2
hold and then
k k k k ≤ k k ≤
derive a contradiction.
(e) In Section D.2.5, assume that Σ21θ
2
r
Σ
and θ
1
> r
1
and θ
2
r
2
hold and then
k k ≤ k k k k ≤
derive a contradiction.
(f) In Section D.2.6, assume that Σ21θ
2
r
Σ
and θ
1
r
1
and θ
2
> r
2
hold and then
k k ≤ k k ≤ k k
derive a contradiction.
Finally, we have
Σ1 2(βˆ β ∗)
2
r Σ, βˆ β
∗ 2
r 2, and βˆ β
∗ 1
r 1, (D.4)
k − k ≤ k − k ≤ k − k ≤
and the proof is complete.
D.2.1 Step 2(a)
Assume that kΣ21θ
k2
≤r
Σ
and kθ
k1
>r
1
and kθ
k2
>r
2
hold,andthenwecanfindη 4,η
4′
∈(0,1)
such that kθ
η4k1
= r
1
and kθ
η 4′k2
= r
2
hold. We note that kΣ1 2θ
η4k2
≤
r
Σ
and kΣ21θ
η 4′k2
≤
r
Σ
also hold. Then, from the same arguments of Sections D.1.2 and D.1.3, we have a contradiction.
D.2.2 Step 2(b)
sA us cs hum the at th ka Σt
1k
2Σ
θ
η21 5θ
k2k2
=>
r
Σr
Σ
aa nn dd
kk
θθ
ηk
5′k1
2≤
=r
1
r
2an hd
olk
dθ
.k
W2
e> nr
o2
teho tl hd a, ta kn θd ηt 5h ke 1n ≤w re 1c aa nn dfin kθd ηη
5′5
k, 1η
≤5′
∈
r
1(0 a, ls1 o)
hold. Then, from the same arguments of Sections D.1.1 and D.1.2, we have a contradiction.
D.2.3 Step 2(c)
Assume that kΣ21θ
k2
>r
Σ
and kθ
k1
>r
1
and kθ
k2
≤r
2
holdand, thenwecanfindη 6,η
6′
∈(0,1)
such that kθ
η6k1
= r
1
and kΣ21θ
η 6′k2
= r
Σ
hold. We note that kθ
η6k2
≤
r
2
and kθ
η 6′k2
≤
r
2
also
hold. Then, from the same arguments of Sections D.1.1 and D.1.3, we have a contradiction.
D.2.4 Step 2(d)
Assume that Σ21θ
2
> r
Σ
and θ
1
r
1
and θ
2
r
2
hold and, then we can find η
7
(0,1)
k k k k ≤ k k ≤ ∈
suchthat kΣ21θ
η7k2
=r
Σ
holds. We note that kθ
η7k1
≤r
1
and kθ
η7k2
≤r
2
alsohold. Then,from
the same arguments of Section D.1.1, we have a contradiction.
D.2.5 Step 2(e)
Assume that Σ21θ
2
r
Σ
and θ
1
> r
1
and θ
2
r
2
hold and, then we can find η
8
(0,1)
k k ≤ k k k k ≤ ∈
suchthat kθ
η8k1
=r
1
holds. We notethat kΣ21θ
η8k2
≤r
Σ
and kθ
η8k2
≤r
2
alsohold. Then,from
the same arguments of Section D.1.3, we have a contradiction.
20D.2.6 Step 2(f)
Assume that Σ21θ
2
r
Σ
and θ
1
r
1
and θ
2
> r 2hold and, then we can find η
9
(0,1)
k k ≤ k k ≤ k k ∈
suchthat kθ
η9k2
=r
2
holds. We notethat kΣ21θ
η9k2
≤r
Σ
and kθ
η9k1
≤r
1
alsohold. Then,from
the same arguments of Section D.1.2, we have a contradiction.
E Proof of Proposition A.2
Proof. From simple algebra, we have
n 1 y x˜ ,β
sup h i −h i ∗ i x˜ ,v
i
v ∈r1Bd 1∩r2Bd 2∩rΣBd ΣXi=1 n (cid:18) λ o√n (cid:19)h i
n 1 x x˜ ,β +ξ
= sup h h i − i ∗ i i x˜ ,v
i
v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ Xi=1 n (cid:18) λ o√n (cid:19)h i!
n 1 x x˜ ,β +ξ x x˜ ,β +ξ
sup h h i − i ∗ i i x˜ ,v Eh h i − i ∗ i i x˜ ,v
i i
≤v ∈r1Bd 1∩r2Bd 2∩rΣBd ΣXi=1 n (cid:18) λ o√n (cid:19)h i− (cid:18) λ o√n (cid:19)h i
T1
x x˜ ,β +ξ x x˜ ,β +ξ
| + sup Eh h i − i ∗ i {zi x˜ ,v Eh h i − i ∗ i i x ,}v
i i
v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ(cid:26) (cid:18) λ o√n (cid:19)h i− (cid:18) λ o√n (cid:19)h i (cid:27)
T2
x x˜ ,β +ξ ξ
+| sup Eh h i − i ∗ i i{zx ,v Eh i x ,v . (E.1})
i i
v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ(cid:26) (cid:18) λ o√n (cid:19)h i− (cid:18)λ o√n (cid:19)h i (cid:27)
T3
First, we|evaluate T . From union bound, we{zhave }
1
n 1 x x˜ ,β +ξ n 1 x x˜ ,β +ξ
T max h h i − i ∗ i i x˜ E h h i − i ∗ i i x˜ r . (E.2)
1
≤j=1, ···,d(cid:12)
(cid:12)Xi=1
n
(cid:18)
λ o√n
(cid:19)
ij
−
Xi=1
n
(cid:18)
λ o√n
(cid:19)
ij
(cid:12)
(cid:12)
1
(cid:12) (cid:12)
(cid:12) (cid:12)
We note that, fo(cid:12)r any 1 j d, (cid:12)
≤ ≤
x x˜ ,β +ξ x x˜ ,β +ξ 2 x x˜ ,β +ξ 2
E h h i − i ∗ i i x˜ Eh h i − i ∗ i i x˜ Eh h i − i ∗ i i x˜2 Ex˜2 1,
λ √n ij − λ √n ij ≤ λ √n ij ≤ ij ≤
(cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19) (cid:19) (cid:18) o (cid:19)
x x˜ ,β +ξ x x˜ ,β +ξ p
E h h i − i ∗ i i x˜ Eh h i − i ∗ i i x˜
λ √n
ij
− λ √n
ij
(cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19) (cid:19)
x x˜ ,β +ξ x x˜ ,β +ξ 2
≤(2τx)p −2E h h i − λi √n∗ i i x˜
ij
−Eh h i − λi √n∗ i i x˜
ij
≤(2τx)p −2, (E.3)
(cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19) (cid:19)
and from Bernstein’s inequality (Lemma 5.1 of [18]), we have
P
(cid:12)
(cid:12)Xi=n
1
n1
h
(cid:18)hx
i −
λx˜ oi, √β
n∗
i+ξ
i (cid:19)x˜
ij
−E
Xi=n
1
n1
h
(cid:18)hx
i −
λx˜ oi, √β
n∗
i+ξ
i (cid:19)x˜
ij
(cid:12) (cid:12)≥
r2
nt +2τ nxt
!≤e−t,
(cid:12) (cid:12) (E.4)
(cid:12) (cid:12)
(cid:12) (cid:12)
and, with probability at least 1 δ, we have
−
log(d/δ) τxlog(d/δ)
T 2 +2 r . (E.5)
1 1
≤ r n n !
21Second, we evaluate T .
2
(a) x x˜ ,β +ξ
T Eh h i − i ∗ i i (x˜ x ) r
2 i i 1
≤ λ √n −
(cid:13) (cid:18) o (cid:19) (cid:13)
= j(cid:13) (cid:13) (cid:13) ∈{m 1,a ··x
·,d
}(cid:12)Eh (cid:18)hx i − λx˜ oi, √β n∗ i+ξ i (cid:19)((cid:13) (cid:13) (cid:13)x˜∞ ij −x ij) (cid:12)r 1
(cid:12) (cid:12)
( ≤b)
j
m 1,ax
,d
(cid:12) (cid:12)E x˜ ij −x ij r 1 (cid:12) (cid:12)
∈{ ··· }
=
j
m 1,ax
,d
E (cid:12) (cid:12)x˜ ij −x ij (cid:12) (cid:12)I |x ij|≥τxr 1
∈{ ··· }
≤j
m 1,ax
,d
2E(cid:12)
(cid:12) x ij I |x
ij(cid:12)
(cid:12) |≥τxr 1
∈{ ··· }
( ≤b) 2
j
m 1,ax
,d
(E(cid:12) (cid:12) x4 ij(cid:12) (cid:12) )1 4(EI |x ij|≥τx)3 4r 1 ( ≤c) 2K
τ
x34 r 1, (E.6)
∈{ ··· }
where (a) follows from Ho¨lder’s inequality, (b) follows from 1 h() 1, and (c) follows from
− ≤ · ≤
Lemma B.1.
Lastly, we evaluate T . Define h() as the differential of h(). Let 0<υ <1. We have
3 ′
· ·
3
T 3
(a)
sup (E x i,v 4)1 4 E h
hx
i
−x˜ i,β
∗
i+ξ
i h
ξ
i
34 4
≤ v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ h i (cid:12)
(cid:12)
(cid:18) λ o√n (cid:19)− (cid:18)λ o√n (cid:19)(cid:12)
(cid:12)
!
(b)
Kr E h
hx
i
−x˜ i,β
∗
i+ξ
i
(cid:12) (cid:12)
h
ξ
i
υ
h
hx
i
−x˜ i,β
∗
i+ξ
i
(cid:12) (cid:12)
h
ξ
i
4 3−υ 43
Σ
≤ (cid:12) (cid:18) λ o√n (cid:19)− (cid:18)λ o√n (cid:19)(cid:12) (cid:12) (cid:18) λ o√n (cid:19)− (cid:18)λ o√n (cid:19)(cid:12) !
(cid:12) (cid:12) (cid:12) (cid:12)
(c) 21 −3 4υ Kr(cid:12) (cid:12)
Σ
E h hx i −x˜ i,β ∗ i+ξ i h (cid:12) (cid:12)ξ i(cid:12) (cid:12) υ 43 (cid:12) (cid:12)
≤ λ √n − λ √n
(cid:18) (cid:12) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:12) (cid:19)
(d) (cid:12) (cid:12)(x x˜ ) β υ 3 4 (cid:12) (cid:12)
21 −3 4υ Kr
Σ
E(cid:12) i − i ⊤ ∗ (cid:12)
≤ λ √n
(cid:18) (cid:12) o (cid:12) (cid:19)
(cid:12) (cid:12)
(e) 21 −3 4υ K(E x i(cid:12) (cid:12) x˜ i,β ∗ υ)3 4 (cid:12) (cid:12)r Σ, (E.7)
≤ |h − i|
where (a) follows from Ho¨lder’s inequality, (b) follows from the finite kurtosis property of x , (c)
i
follows from the fact that h() 1, (d) follows from Lipschitz continuity of h() and from the
| · | ≤ ·
fact h() 1, and (e) follows from λ √n 1.
′ o
| · |≤ ≥
We compute (E x x˜ ,β υ)43 . Define dom(β ) as the set of the indices such that β =0.
|h
i
−
i ∗
i|
∗ j∗
6
We have
υ 3
4
(E |hx i −x˜ i,β ∗ i|υ)3 4 ≤E  |β j∗(x ij −x˜ ij) | 
 j ∈dXom(β ∗) 
 3
(a) 4
E β (x x˜ ) υ
≤ 
j∗ ij
−
ij

j ∈dXom(β ∗)
(cid:12) (cid:12)
≤c β3 4υ s3 4 E x ij −(cid:12) x˜ ij υ 43 (cid:12) 
≤23 4υ c β3 4υ(cid:0) s43(cid:12) (cid:12) E I |x ij|≥(cid:12) (cid:12) τx(cid:1) x ij υ 3 4
( ≤b) 23 4υ c β3 4υ s3 4(cid:16) E(cid:12) (cid:12) (cid:12)x4
ij
υ 4×3 4 E(cid:12) (cid:12) (cid:12)I |x(cid:17)
ij|≥τx
4 −4υ ×43
(c) 23 4υ c3 4υ K3(cid:16) s3
4
1(cid:17) 3 −3(cid:16) 4υ
,
(cid:17)
(E.8)
≤ β τx
(cid:18) (cid:19)
22where (a) follows from the subadditivity, (b) follows Ho¨lder’s inequality, and (c) follows from
Lemma B.1.
Set υ =1/3. Combining the arguments above, with probability at least 1 δ, we have
−
n 1 x x˜ ,β +ξ
sup h h i − i ∗ i i x˜ ,v
i
v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ(cid:12) (cid:12)Xi=1 n (cid:18) λ o√n (cid:19)h i(cid:12) (cid:12)
≤
r2log( nd/δ) (cid:12) (cid:12) (cid:12)+2τxlog n(d/δ) !r 1+2K
τ
x34 r 1+4K4(cid:12) (cid:12) (cid:12)c β41 s43 (cid:18)τ1
x
(cid:19)3 −3 4υ r
Σ
( ≤a) 4 c
r1
rslog( nd/δ) +c r1τx√slog( nd/δ) +K4c r1√
τ
x3s +K4c β1 4s43 (cid:18)τ1
x
(cid:19)3 −1 4 !r Σ, (E.9)
where (a) follows from c √sr =r .
r1 Σ 1
E.1 Proof of Proposition A.3
Proof. Define
VΣ
r1,r2,rΣ
= {v ∈Rd |v ∈r 1Bd 1∩r 2Bd 2, kΣ21 v
k2
=r
Σ
}. (E.10)
This proposition is proved in a manner similar to the proof of Proposition B.1 of [10]. The L.H.S
of (A.7) divided by λ2 can be expressed as
o
n x x˜ ,β +ξ x˜ v x x˜ ,β +ξ x˜ v
h h
i
−
i ∗
i
i ⊤i
+h h
i
−
i ∗
i
i ⊤i
. (E.11)
− λ √n − λ √n λ √n λ √n
i=1(cid:18) (cid:18) o o (cid:19) (cid:18) o (cid:19)(cid:19) o
X
From the convexity of the Huber loss, we have
n x x˜ ,β +ξ x˜ v x x˜ ,β +ξ x˜ v
h h
i
−
i ∗
i
i ⊤i
+h h
i
−
i ∗
i
i ⊤i
(E.12)
− λ √n − λ √n λ √n λ √n
i=1(cid:18) (cid:18) o o (cid:19) (cid:18) o (cid:19)(cid:19) o
X
n x x˜ ,β +ξ x˜ v x x˜ ,β +ξ x˜ v
h h
i
−
i ∗
i
i ⊤i
+h h
i
−
i ∗
i
i ⊤i
I . (E.13)
≥ − λ √n − λ √n λ √n λ √n
Ei
i=1(cid:18) (cid:18) o o (cid:19) (cid:18) o (cid:19)(cid:19) o
X
Define the functions
x2 if x 1/4
| |≤
(x 1/4)2 if 1/4 x 1/2
ϕ(x)= − ≤ ≤ and ψ(x)=I . (E.14)
(x+1/4)2 if −1/2 ≤x ≤−1/4 ( |x |≤1/2)
0 if x >1/2
| |
Let f i(v)=ϕ λx˜ o⊤i √v
n

ψ hx i− λx˜ oi, √β n∗ i+ξi and we have
(cid:16) (cid:17) (cid:16) (cid:17)
n x x˜ ,β +ξ x˜ v x x˜ ,β +ξ x˜ v
h h
i
−
i ∗
i
i ⊤i
+h h
i
−
i ∗
i
i ⊤i
− λ √n − λ √n λ √n λ √n
i=1(cid:18) (cid:18) o o (cid:19) (cid:18) o (cid:19)(cid:19) o
X
n x˜ v 2 (a) n x˜ v x x˜ ,β +ξ n
⊤i
I ϕ
⊤i
ψ h
i
−
i ∗
i
i
= f (v), (E.15)
≥ λ √n
Ei
≥ λ √n λ √n
i
i=1(cid:18) o (cid:19) i=1 (cid:18) o (cid:19) (cid:18) o (cid:19) i=1
X X X
where (a) follows from ϕ(v) v2 for v 1/2. We note that
≥ | |≤
2
x˜ v 1
f (v) ϕ(v ) min
⊤i
, . (E.16)
i i
≤ ≤ ((cid:18)λ o√n (cid:19) 4 )
23To bound n f (v) from below, for any fixed v VΣ , we have
i=1 i ∈ r1,r2,rΣ
P n n n
f (v) Ef(v) sup f (v) E f (v) . (E.17)
i i i
≥ −v VΣ −
Xi=1 ∈ r1,r2,rΣ(cid:12)Xi=1 Xi=1 (cid:12)
(cid:12) (cid:12)
Define the supremum of a random process indexed b(cid:12) y VΣ : (cid:12)
r1,r2,rΣ
n n
∆:= sup f (v) E f (v) . (E.18)
i i
v VΣ (cid:12) − (cid:12)
∈ r1,r2,rΣ(cid:12)Xi=1 Xi=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
From (E.15) and (E.14), we have (cid:12) (cid:12)
n n 2 n 2 n 2
x˜ v x˜ v x˜ v
E i=1f i(v) ≥ i=1E (cid:18)λ o⊤i √n
(cid:19)
− i=1E (cid:18)λ o⊤i √n
(cid:19)
I(cid:12) (cid:12) (cid:12) (cid:12)λx˜ o⊤i √v n(cid:12) (cid:12) (cid:12) (cid:12)≥1/2− i=1E (cid:18)λ o⊤i √n
(cid:19)
I(cid:12) (cid:12) (cid:12)hx˜i− λx˜ oi, √β n∗i+ξi(cid:12) (cid:12) (cid:12)≥1/2.
X X X X
(E.19)
For E x˜⊤i v 2 , from Lemma B.2, we have
λo√n
(cid:16) (cid:17)
K4 K4
E(x˜
⊤i
v)2 ≥−4
τ x2
kv k2 1+E(x
⊤i
v)2 = −2
τ x2
kv k2 1+ kΣ21 v k2
2
K4
≥−4
τx2
r 12+ kΣ21 v k2
2
(a) s
≥
−4K4c2 r1τx2r Σ2 + kΣ1 2v k2
2
(b) s
≥
−4K4c2
r1τ x2
+1 kΣ21 v k2 2, (E.20)
(cid:18) (cid:19)
where (a) follows from r
1
=c r1√sr
Σ
and (b) follows from r
Σ
= kΣ1 2v k2 2.
For E λx˜ o⊤i √v n 2 I(cid:12) (cid:12)
(cid:12)
x˜⊤i v (cid:12) (cid:12)
(cid:12)
1/2, we have
(cid:16) (cid:17) (cid:12)λo√n(cid:12)≥
E(x˜ ⊤i v)2I(cid:12) (cid:12)
(cid:12)
x˜⊤i v (cid:12) (cid:12)
(cid:12) 1/2
=Ev ⊤(x˜ ix˜ ⊤i −x ix ⊤i +x ix ⊤i )vI(cid:12) (cid:12)
(cid:12)
x˜⊤i v (cid:12) (cid:12)
(cid:12) 1/2
(cid:12)λo√n(cid:12)≥ (cid:12)λo√n(cid:12)≥
(a) K4
≤ 4 τx2 kv k2 1+E(x⊤i v)2I(cid:12) (cid:12) (cid:12) x˜⊤i v (cid:12) (cid:12) (cid:12) 1/2
(cid:12)λo√n(cid:12)≥
(b) K4
≤ 4 τ x2 kv k2 1+ E(x ⊤i v)4 EI(cid:12) (cid:12) (cid:12) x˜⊤i v (cid:12) (cid:12) (cid:12) 1/2
q (cid:12)λo√n(cid:12)≥
r
(c) K4 E( x v + x x˜ ,v )
≤
4
τx2
kv k2 1+√2K2 kΣ1 2v k2
2 s
|h ⊤i i|
λ
o| √h n⊤i − i i|
(d) K4 2 K4
≤
4
τ x2
kv k2 1+K2 kΣ1 2v k2
2 sλ
o√nsr Σ+ kv
k1 τ x3
(e) s 2 K4
≤
4K4c2
r1τx2
+K2
sλ
o√ns1+c r1√s
τx3
!kΣ1 2v k2 2, (E.21)
where (a) follows from (B.6) in the proof of Lemma B.2, (b) follows from Ho¨lder’s inequality, (c)
followsfromthefinitekurtosispropertyofx ,therelationshipofexpectationofindicatorfunction
i
and probability, and Markov’s inequality, (d) follows from Lemma B.2 and the definition of x ,
i
and (e) follows from r
1
=c r1√sr Σ, r
Σ
≤1 and r
Σ
= kΣ21v k2 2.
24For E λx˜ o⊤i √v n 2 I(cid:12) (cid:12) (cid:12)hxi− λx˜ oi, √β n∗i+ξi(cid:12) (cid:12) (cid:12)≥1/2,we have
(cid:16) (cid:17)
E(x˜ ⊤i v)2I(cid:12) (cid:12) (cid:12)hxi− λx˜ oi, √β n∗i+ξi(cid:12) (cid:12) (cid:12)≥1/2 =Ev⊤(x˜ ix˜ ⊤i −x ix⊤i +x ix⊤i )vI(cid:12) (cid:12) (cid:12)hxi− λx˜ oi, √β n∗i+ξi(cid:12) (cid:12) (cid:12)≥1/2
(a) K4
≤ 4 τ x2 kv k2 1+E(x ⊤i v)2I(cid:12) (cid:12) (cid:12)hxi− λx˜ oi, √β n∗i+ξi(cid:12) (cid:12) (cid:12)≥1/2
(b) K4
≤ 4 τ x2 kv k2 1+ qE(x ⊤i v)4 EI(cid:12) (cid:12) (cid:12)hxi− λx˜ oi, √β n∗i+ξi(cid:12) (cid:12) (cid:12)≥1/2
q
(c) K4 2
≤
4
τx2
kv k2 1+K2 kΣ1 2v k2
2 sλ o√n
E( |ξ
i
|+ |hx
i
−x˜ i,β ∗i|)
p
(d) K4 2 K4
≤
4
τ x2
kv k2 1+K2 kΣ1 2v k2
2 sλ
o√nsσ+ kβ
∗k1 τ x3
(e) s 2 K4
≤
4K4c2
r1τx2
+K2
sλ
o√nsσ+ kβ
∗k1 τx3
!kΣ21 v k2 2, (E.22)
where (a) follows from Lemma B.2, (b) follows from Ho¨lder’s inequality, (c) follows from the
definition of x , the relationship of expectation of indicator function and probability, Markov’s
i
inequality and the triangular inequality, (d) follows from Lemma B.3, and (e) follows from r =
1
c r1√sr
Σ
and r
Σ
= kΣ1 2v k2 2.
Consequently, from (E.15), (E.17), (E.19) (E.20), (E.21) and (E.22), we have
2 K4 K4 s
kΣ21 v k2
2
1 −K2
sλ o√n
sσ+ kβ
∗k1 τx3
+ s1+c r1√s
τx3
!−12K4c2
r1τx2
!−λ2 o∆
n x x˜ ,β +ξ x˜ v x x˜ ,β +ξ x˜ v
λ2 h h i − i ∗ i i ⊤i +h h i − i ∗ i i ⊤i . (E.23)
≤ o − λ √n − λ √n λ √n λ √n
i=1(cid:18) (cid:18) o o (cid:19) (cid:18) o (cid:19)(cid:19) o
X
Next we evaluate the stochastic term ∆ defined in (E.18). From (E.16) and Theorem 2.1 of
[7], with probability at least 1 δ, we have
−
n
1
∆ E∆+ sup E(f (v) Ef (v))2 2log(1/δ)+ log(1/δ). (E.24)
i i
≤ v
u
uv ∈VΣ
r1,r2,rΣXi=1
−
p
3
t
From (E.16) and r =c √sr , we have
1 r1 Σ
E(f (v) Ef (v))2 Ef2(v)
Ef i(v) Ehx˜ i,v i2 1 K4
v 2+
kΣ21v k2
2
i − i ≤ i ≤ 4 ≤ 4λ2 on ≤ λ2 on τ x2 k k1 4 !
1 s 1
≤ λ2 on
(cid:18)K4c2
r1τx2
+
4
(cid:19)kΣ21 v k2 2. (E.25)
Combiningthis and(E.24),fromthe triangularinequality,withprobabilityatleast1 δ,wehave
−
log(1/δ) log(1/δ) 1
λ2 o∆ ≤λ2 oE∆+λ o√n K2c
r1
ss
τx2n
+
r
2n
!kΣ21 v k2+λ2 o3log(1/δ)
(a) log(d/δ) log(d/δ) log(d/δ)
≤
λ2 oE∆+λ o√n K2c
r1
ss
τ x2n
+ rs
2n
!kΣ1 2v k2+λ2 on
3n
(b) K2 slog(d/δ)
≤
λ2 oE∆+λ o√nc
r1
(cid:18)
τx
+1
(cid:19)r
n
kΣ1 2v k2, (E.26)
25where (a) follows from log(1/δ) log(d/δ) and s 1 and (b) follows from λ √n slog(d/δ) r ,
≤ ≥ o n ≤ Σ
s ≥1 and c
r1
≥1. For E∆, from symmetrization inequality (Lemma 11.4 of [6]),qwe have
n x˜ v x x˜ ,β +ξ
E∆ 2E sup b ϕ ⊤i ψ h i − i ∗ i i
i
≤ v ∈VΣ r1,r2,rΣ(cid:12)
(cid:12)Xi=1
(cid:18)λ o√n
(cid:19) (cid:18)
λ o√n (cid:19)(cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) n x˜ v x x˜ ,β (cid:12)+ξ
2E sup(cid:12) b ϕ ⊤i ψ h i − i ∗ i(cid:12) i , (E.27)
i
≤ v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ(cid:12) (cid:12)Xi=1 (cid:18)λ o√n (cid:19) (cid:18) λ o√n (cid:19)(cid:12) (cid:12)
(cid:12) (cid:12)
where b n is a sequence of i.i.d. R(cid:12)ademacher random variables which are (cid:12)independent of
x˜ ,ξ { n i } .i= W1 edenote E asaconditiona(cid:12) lexpectationof b n given x˜ ,ξ n .(cid:12) FromExercise
{ 2.2i .2i o} fi= [31 9], for any v ∗ r Bd r Bd r Bd, we have { i }i=1 { i i }i=1
0 ∈ 1 1∩ 2 2∩ Σ Σ
n x˜ v x x˜ ,β +ξ
E ∗ sup b iϕ ⊤i ψ h i − i ∗ i i
v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ(cid:12) (cid:12)Xi=1 (cid:18)λ o√n (cid:19) (cid:18) λ o√n (cid:19)(cid:12) (cid:12)
n x˜ v(cid:12) (cid:12) x x˜ ,β +ξ (cid:12) (cid:12) n x˜ v x x˜ ,β +ξ
E ∗ b iϕ ⊤i (cid:12)0 ψ h i − i ∗ i i +E ∗ s(cid:12)up b iϕ ⊤i ψ h i − i ∗ i i .
≤ (cid:12) (cid:12)Xi=1 (cid:18)λ o√n (cid:19) (cid:18) λ o√n (cid:19)(cid:12) (cid:12) v ∈r1Bd 1∩r2Bd 2∩rΣBd ΣXi=1 (cid:18)λ o√n (cid:19) (cid:18) λ o√n (cid:19)
(cid:12) (cid:12) (E.28)
(cid:12) (cid:12)
(cid:12) (cid:12)
For the first term of (E.28), we set v = 0. For the second term of (E.28), from contraction
0
principle (Theorem 11.5 of [6]), we have
n x˜ v x x˜ ,β +ξ n x˜ v
E ∗ sup b iϕ ⊤i ψ h i − i ∗ i i E ∗ sup b iϕ ⊤i ,
v ∈r1Bd 1∩r2Bd 2∩rΣBd ΣXi=1 (cid:18)λ o√n (cid:19) (cid:18) λ o√n (cid:19)≤ v ∈r1Bd 1∩r2Bd 2∩rΣBd ΣXi=1 (cid:18)λ o√n (cid:19)
(E.29)
and from the fact that ϕ is 1-Lipschitz and ϕ(0)=0, and contractionprinciple (Theorem 11.6 in
2
[6]),
n n
x˜ v 1
E sup b ϕ ⊤i E sup b x˜ v. (E.30)
∗
v ∈r1Bd 1∩r2Bd 2∩rΣBd
ΣXi=1
i
(cid:18)λ o√n (cid:19)≤ λ o√n
∗
v ∈r1Bd 1∩r2Bd 2∩rΣBd
ΣXi=1
i ⊤i
and from the basic property of the expectation, we have
n x˜ v x x˜ ,β +ξ
λ2E sup b ϕ ⊤i ψ h i − i ∗ i i
o v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ(cid:12)
(cid:12)Xi=1
i (cid:18)λ o√n
(cid:19) (cid:18)
λ o√n (cid:19)(cid:12)
(cid:12)
(cid:12) n (cid:12)
(cid:12) 1 (cid:12)
≤λ o√nE
v ∈r1Bd
1∩s ru 2p
Bd
2(cid:12)
∩rΣBd
ΣXi=1
nb ix˜ ⊤i v (cid:12)
(a) logd logd
λ o√n 2 +τx r
1
≤ r n n !
(b) log(d/δ) slog(d/δ)
≤
λ o√nc
r1
√2+τx
r n !r n
kΣ21 v k2, (E.31)
where (a) follows from Lemma B.4 and (b) follows from log(d) log(d/δ), 1 log(d/δ), r =
1
≤ ≤
c √sr and c 1.
r1 Σ r1
≥
Combining (E.26) and (E.31), we have
K2 log(d/δ) slog(d/δ)
λ2 o∆ ≤3λ o√nc
r1
1+
τx
+τx
r
n
!r
n
kΣ21 v k2, (E.32)
and combining (E.23), the proof is complete.
26F Proofs of Propositions 3.1, 3.2 and A.4, and Lemma A.5
F.1 Proof of Proposition 3.1
Proof. We note that this proof is similar to the one of Lemma 2 of [21]. For any M M , we
r
∈
have
n n n n
1 1 1 1
x˜ x˜ ,M = x˜ x˜ ,M E x˜ x˜ ,M +E x˜ x˜ ,M . (F.1)
n h
i ⊤i
i n h
i ⊤i
i− n h
i ⊤i
i n h
i ⊤i
i
i=1 i=1 i=1 i=1
X X X X
T4
First, we evaluate T . We note| that, for any 1 {zj ,j d, }
4 1 2
≤ ≤
Ex˜2 x˜2 Ex˜4 Ex˜4 K4, Ex˜2px˜2p τ2(p 2)Ex˜2 x˜2 τ2(p 2)K4. (F.2)
ij1 ij2 ≤ ij1 ij2 ≤ ij1 ij2 ≤ x − ij1 ij2 ≤ x −
q q
From Bernstein’s inequality (Lemma 5.1 of [18]), we have
1 n n 1 t τ2t
P x˜ x˜ E x˜ x˜ K2 2 + x e t. (F.3)
n
i=1
ij ⊤ij
−
i=1
n
ij ⊤ij
≥
r
n n !≤
−
X X
From the union bound, we have
n n
1 1 log(d/δ) log(d/δ)
P x˜ x˜ Ex˜ x˜ √2K2 +τ2 1 δ. (F.4)
(cid:13)n
ij ⊤ij
− n
ij ⊤ij(cid:13)
≤
r
n
x
n !≥ −
(cid:13)
(cid:13)
Xi=1 Xi=1 (cid:13)
(cid:13)∞
(cid:13) (cid:13)
From Ho¨l(cid:13)der’s inequality, we have (cid:13)
log(d/δ) log(d/δ)
P T √2K2 M +τ2 M 1 δ. (F.5)
1 1 x 1
≤
r
n k k n k k !≥ −
Next, we evaluate E x˜ x˜ ,M . We have
i ⊤i
E x˜ x˜(cid:10) ,M =(cid:11)E x˜ x˜ x x ,M +E x x Σ,M +E Σ,M
i ⊤i i ⊤i
−
i ⊤i i ⊤i
− h i
(cid:10) (cid:11)=E (cid:10)x˜ ix˜ ⊤i −x ix ⊤i ,M(cid:11)+E (cid:10) hΣ,M
i
(cid:11) (F.6)
(cid:10) (cid:11)
From Ho¨lder’s inequality and the positive semi-definiteness of M, we have
E Σ,M Σ M = Σ Tr(M)= Σ r2. (F.7)
op op op
h i≤k k k k∗ k k k k
From (B.6) and Ho¨lder’s inequality, we have
K4
E x˜ ix˜ ⊤i −x ix⊤i ,M ≤2
τx2
kM k1. (F.8)
(cid:10) (cid:11)
Finally, combining the arguments above, with probability at least 1 δ, we have
−
(cid:12)
(cid:12)Xi=n
1
(cid:10)x˜ ix˜ ⊤i n,M
(cid:11)(cid:12) (cid:12)≤
√2K2 rlog( nd/δ) +τx2log( nd/δ) +2K τx24 !kM k1+ kΣ kopr2. (F.9)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Define M v,r2 = {M ∈Rd ×d : M =vv ⊤, kv k2 =r 2 }.
27F.2 Proof of Proposition 3.2
We note that, from Ho¨lder’s inequality and w 1/n,
i
| |≤
2 n
(a) o
wˆ u X˜ v c2 wˆ X˜ v2. (F.10)
(cid:12)
i i ⊤i
(cid:12) ≤ n
i
|
⊤i
|
(cid:12) (cid:12)i X∈O (cid:12)
(cid:12)
Xi=1
(cid:12) (cid:12)
We focus on wˆ X˜ v2.(cid:12)For any v R(cid:12)d such that v =r ,
i i | ⊤i | ∈ k k2 2
∈O
Pn n
wˆ (X˜ v)2 = wˆ (X˜ v)2 λ v 2+λ v 2
i ⊤i i ⊤i − ∗k k1 ∗k k1
i=1 i=1
X X
n
(a)
sup wˆ X˜ X˜ ,M λ M +λ v 2
≤ M ∈M r Xi=1 i D i ⊤i E− ∗k k1 ! ∗k k1
(b)
τ +λ v 2, (F.11)
≤ suc ∗k k1
aw rh ge ur me e( na t) sf ao bll oo vw es ,f fr oo rm ant yhe vfac rt Bth dat M
r
Bv, dr2
⊂
r
M Bdr ,2, wa en hd a( vb e) follows from (3.10). Combining the
∈ 1 1∩ 2 2∩ Σ Σ
(a) o o
wˆ iu iX˜
⊤i
v
≤
√2c n√τ suc+√2c
n
λ ∗kv
k1
i r r
X∈O p
o Σ o log(d/δ) log(d/δ) K4
=√2c
rn
rk
1
−ko εp r 22+√2c
rnv
uc
∗
√2K2
r n
+τ x2
n
+2
τ x2
!kv
k1
u
t
(b) o log(d/δ) log(d/δ) K4 o
≤
2cc′ ∗c r2kΣ21
kop rn
+c r1√s sK2
r
n
+τx2
n
+
τx2
rnr Σ,
  (F.12)
where (a) follows the triangular inequality and (b) follows from c =max 1 ,c , r =c √sr
and r =c r . ′ ∗ {1 −ε ∗} 1 r1 Σ
2 r2 Σ
F.3 Proof of Proposition A.4
We note that, from Ho¨lder’s inequality, we have
2 n
u x˜ v 1 1 m 1
i ⊤i u2 (x˜ v)2 c2 (x˜ v)2. (F.13)
(cid:12) n (cid:12) ≤ n i n ⊤i ≤ n n ⊤i
(cid:12) (cid:12)i X∈Im (cid:12)
(cid:12)
i X∈Im i X∈Im Xi=1
(cid:12) (cid:12)
From the proof of(cid:12)Proposition(cid:12)3.1 and Ho¨lder’s inequality, for any v r Bd r Bd r Bd, we
∈ 1 1 ∩ 2 2 ∩ Σ Σ
have
n (x˜ v)2 log(d/δ) log(d/δ) K4
⊤i
n ≤
2K2
r
n
+τx2
n
+2
τx2
!r 12+ kΣ kopr 22. (F.14)
i=1
X
From triangular inequality, r = c √sr and r = c r , for any v r Bd r Bd r Bd, we
1 r1 Σ 2 r2 Σ ∈ 1 1 ∩ 2 2 ∩ Σ Σ
have
1 m log(d/δ) log(d/δ) K4 m
i X∈Im
nu ix˜
⊤i
v ≤2c c r2kΣ21
kop rn
+c r1√s sK2
r
n
+τ x2
n
+
τ x2
rnr Σ.
 (F.15)
28G Proofs of Theorems 2.1 and 3.1
G.1 Proof of Theorem 2.1
To prove Theorem 2.1, it is sufficient that we confirm (A.1) - (A.4) hold under the assumption of
Theorem 2.1.
G.1.1 Confirming (A.1) and (A.2)
First, we confirm (A.1). We note that, from τx = n ,
log(d/δ)
q
K4c r1√
τ
x3s ( ≤a) c r1 rslog( nd/δ) , K4s3 4 (cid:18)τ1
x
(cid:19)3 −1 4 ( ≤b) c r1 rslog( nd/δ) , (G.1)
where(a)and(b)followsfrom(2.7). Then,fromPropositionA.2,foranyv r Bd r Bd r Bd,
∈ 1 1∩ 2 2∩ Σ Σ
we have
n 1 y x˜ ,β log(d/δ)
sup h i −h i ∗ i x˜ ,v 16c s r . (G.2)
v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ(cid:12) (cid:12)Xi=1 n (cid:18) λ o√n (cid:19)h
i
i(cid:12) (cid:12)≤
r1
r n
Σ
(cid:12) (cid:12)
(cid:12) (cid:12)
Next, we confirm (A.2). W(cid:12)e note that, from τx = n (cid:12) ,
log(d/δ)
q
2 K4 K4 s (a) 18(σ+1) 1 (b) 1
K2 σ+ β + 1+c √s +12K4c2 K2 + ,
sλ o√n s k ∗k1 τx3 s r1 τx3 ! r1τx2 ≤ s λ o√n 6 ≤ 2
K2 log(d/δ) (c)
2+ +τx 4, (G.3)
τx
r
n ≤
where (a) and (c) follows from (2.7), (b) follows from the definition of λ .
o
Then, from Proposition A.3, for any v ∈r 1Bd 1∩r 2Bd
2
such that kΣ21v
k2
=r Σ, we have
n √λ o
n
−h y i −h λx˜ i, √β n∗+v i +h y i − λhx √˜ i n,β ∗ i x˜ ⊤i v
≥
kΣ1 2 2v k2 2 −12λ o√nc r1 slog n(d/δ) kΣ21 v k2.
i=1 (cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19) r
X
(G.4)
Therefore, we see that (A.1) and (A.2) hold with
slog(d/δ)
r =r =16λ √nc , b=1/2. (G.5)
a,Σ b,Σ o r1
n
r
G.1.2 Confirming (A.3)
Thirdly, we confirm (A.3). From (G.5), we see that
cr r1a √,Σ
s
=16λ o√n rlog( nd/δ) and λλ ss −+ c crr r ra1 1a ,, √ √ΣΣ s
s
= c cs sc cc cR RR RE EE E− −+ +1 11
1
+ −1
1
≤c RE. (G.6)
G.1.3 Confirming (A.4)
Lastly, we confirm (A.4). From (G.5), the definition of λ and c >1, we have
s RE
2 c +1
r +r +c √sλ 12c RE r =12c √sλ , (G.7)
b
a,Σ b,Σ r1 s
≤
s
c 1
a,Σ r1 s
RE
−
(cid:0) (cid:1)
and we see the condition about r is satisfied. From the definition, conditions about r and r
Σ 1 2
are clearly satisfied.
29G.2 Proof of Theorem 3.1
To prove Theorem 3.1, it is sufficient that we confirm (A.1) - (A.4) hold under the assumption of
Theorem 3.1.
G.2.1 Confirming (A.1) and (A.2)
We note that, from r 1, λ √n 1, we have (log(d/δ))/n 1. From (3.14), we have
Σ o
≤ ≥ ≤
1 3
max K4 log(d/δ) 4 ,K4 s1 4 log(d/δ) 16 1. (G.8)
( (cid:18) n (cid:19) c r1 (cid:18) n (cid:19) )≤
From K 1 and (3.14), we have
≥
c2 log(d/δ)
(K4+K2+1) r1 s 1. (G.9)
c2 r2kΣ21
kop r
n ≤
From (log(d/δ))/n 1 and (3.15), we have
≤
1 3
log(d/δ) 4 log(d/δ) 4 log(d/δ)
max K2 ,K4max β ,c √s ,72K4c2 s 1.
( (cid:18) n (cid:19) {k ∗ k1 r1 } (cid:18) n (cid:19) r1 r n )≤
(G.10)
First, we confirm (A.1). From 1, ,n = I + I + , we see
<
{ ··· } I∩ ≥ I∩ O
n 1 wˆ W wˆ X˜ ,β
nh i i −
λ
oh √i
n
i ∗ i !wˆ iX˜ ⊤i θ η
i=1
X
n 1 y x˜ ,β 1 y x˜ ,β wˆ W wˆ X˜ ,β
≤(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)Xi=1 nh (cid:18) i − λ oh √i n ∗ i (cid:19)x˜ ⊤i θ η (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)+ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)i ∈O∪X( I∩I<)nh (cid:18) i − λ oh √i n ∗ i (cid:19)x˜ ⊤i θ η(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)+ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)i X∈Oh i i (G− λ .oh 1√ 1)i n i ∗ i !wˆ iX˜ ⊤i θ η (cid:12) (cid:12) (cid:12) (cid:12) (cid:12).
We note that, from the definition of τx and (log(d/δ))/n 1 and (G.8), we have
≤
τx√slo ng(d/δ)
≤
rslog( nd/δ)
,
K4√
τ
x3s
≤
rslog( nd/δ)
,
K4s3
4
(cid:18)τ1
x
(cid:19)3 −1 4
≤c r1
rslog( nd/δ)
.
(G.12)
Then, from Proposition A.2, for any v r Bd r Bd r Bd, we have
∈ 1 1∩ 2 2∩ Σ Σ
n 1 y x˜ ,β log(d/δ)
sup h i −h i ∗ i x˜ ,v 16c s r . (G.13)
v ∈r1Bd 1∩r2Bd 2∩rΣBd Σ(cid:12) (cid:12)Xi=1 n (cid:18) λ o√n (cid:19)h
i
i(cid:12) (cid:12)≤
r1
r n
Σ
(cid:12) (cid:12)
We note that, from the(cid:12) (cid:12)definition of τx and from (G.9)(cid:12) (cid:12), we have
log(d/δ) log(d/δ) K4
c r1√s sK2
r
n
+τ x2
n
+
τ x2
≤c r2kΣ1 2 kop. (G.14)
Additionally,notethat 1 h() 1. ThenfromProposition3.2withc=1,andfromProposition
− ≤ · ≤
A.4 with m=3o and c=1, we have
wˆ W wˆ X˜ ,β 1 y x˜ ,β o
max   (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)i X∈Oh i i − λ oh √i n i ∗ i !wˆ iX˜ ⊤i θ η (cid:12) (cid:12) (cid:12) (cid:12) (cid:12), (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)i ∈O∪X( I∩I<)nh (cid:18) i − λ oh √i n ∗ i (cid:19)x˜ ⊤i θ η(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  ≤2 (c G′ ∗c .r 12 5k )Σ1 2 kop rnr Σ.
30From the arguments above and from c 1, for any v r Bd r Bd r Bd we have
∗ ≥ ∈ 1 1∩ 2 2∩ Σ Σ
n 1 wˆ W wˆ X˜ ,β slog(d/δ) o
i=1
nh i i −
λ
oh √i
n
i ∗ i !wˆ iX˜ ⊤i v ≤16c r1
r
n
r Σ+4c′ ∗c r2kΣ21 kop rnr Σ. (G.16)
X
Next, we confirm (A.2). From the same calculation of (G.11), we have
n wˆ W wˆ X˜ ,β +θ wˆ W wˆ X˜ ,β
λ o√n −h i i −h
λ
oi √i
n
∗ η i !+h i i −
λ
oh √i
n
i ∗ i !!wˆ iX˜ ⊤i θ η
i=1
X
n 1 y x˜ ,β +θ y x˜ ,β
≥λ o√n
n
−h i −h λi √n∗ η i +h i − λh √i
n
∗ i x˜ ⊤i θ η
i=1 (cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19)
X
1 y x˜ ,β +θ y x˜ ,β
−(cid:12)λ o√n
n
−h i −h λi √n∗ η i +h i − λh √i
n
∗ i x˜ ⊤i θ η(cid:12)
(cid:12)
(cid:12)
i ∈O∪X( I∩I<) (cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19) (cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) wˆ W wˆ X˜ ,β +θ wˆ W wˆ X˜ ,β (cid:12)
−(cid:12) (cid:12)λ o√n −h i i −h
λ
oi √ni ∗ η i !+h i i −
λ
oh √i
n
i ∗ i !!wˆ(cid:12) iX˜ ⊤i θ η (cid:12). (G.17)
(cid:12)
(cid:12)
i X∈O (cid:12)
(cid:12)
W(cid:12)e note that (cid:12)
(cid:12) (cid:12)
2 K4 K4 s (a) 18(σ+1) 1 (b) 1
K2 σ+ β + 1+c √s +12K4c2 K2 + ,
sλ o√n s k ∗k1 τ x3 s r1 τ x3 ! r1τ x2 ≤ s λ o√n 6 ≤ 2
K2 log(d/δ) (c)
2+ +τx 4, (G.18)
τx
r
n ≤
where (a) and (c) follow from the definition of τx and from (G.10), and (b) follows from the
definition ofλ o. Then, fromPropositionA.3, for any v ∈r 1Bd 1∩r 2Bd
2
suchthat kΣ21v
k2
=r Σ,we
have
n √λ o
n
−h y i −h λx˜ i, √β n∗+v i +h y i − λhx √˜ i n,β ∗ i x˜ ⊤i v
≥
kΣ1 2 2v k2 2 −15λ o√nc r1 slog n(d/δ) kΣ21 v k2.
i=1 (cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19) r
X
(G.19)
From(G.14),Propositions3.2andA.4withm=3oandc=2andc 1,foranyθ r Bd r Bd
such that Σ1 2θ
η 2
=r Σ, we have ′ ∗ ≥ η ∈ 1 1∩ 2 2
k k
1 y x˜ ,β +θ y x˜ ,β
(cid:12)λ o√n
n
−h i −h λi √n∗ η i +h i − λh √i
n
∗ i x˜ ⊤i θ η(cid:12)
(cid:12)
(cid:12)
i ∈O∪X( I∩I<) (cid:18) (cid:18) o (cid:19) (cid:18) o (cid:19)(cid:19) (cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) wˆ W wˆ X˜ ,β +θ wˆ W wˆ X˜ ,β (cid:12) o
(cid:12)+ (cid:12)
(cid:12)
(cid:12)λ o√n
i X∈O
−h i i −h λ oi √i n ∗ η i !+h i i − λ oh √i n i ∗ i !!(cid:12)wˆ iX˜ ⊤i θ η (cid:12)
(cid:12)
(cid:12)≤16 (c G′ ∗c .r 22 0k )Σ1 2 kop rnr Σ.
(cid:12) (cid:12)
(cid:12) (cid:12)
Therefore, we see that (A.1) and (A.2) holds with b=1/2 and
slog(d/δ) o
r a,Σ =r b,Σ =λ o√n 16c r1
r
n
+16c ′ ∗c r2kΣ1 2 kop
rn
!. (G.21)
G.2.2 Confirming (A.3)
Thirdly, we confirm (A.3). From (G.21), we see that
λ + ra,Σ
cr r1a √,Σ
s
=λ o√n 16 rlog( nd/δ) +16c
′
∗c cr r2 1kΣ21
kop
rso
n !
and
λ
ss
−
c crr ra1 1,√ √Σs
s
≤c RE. (G.22)
31G.2.3 Confirming (A.4)
From Lastly, we confirm (A.4). From (G.21), the definition of λ and c >1, we have
s RE
2 c +1
r +r +c √sλ 12c RE r =12c √sλ , (G.23)
b
a,Σ b,Σ r1 s
≤
s
c 1
a,Σ r1 s
RE
−
(cid:0) (cid:1)
and we see the condition about r is satisfied. From the definition, conditions about r and r
Σ 1 2
are clearly satisfied.
32