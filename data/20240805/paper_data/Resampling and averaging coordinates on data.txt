RESAMPLING AND AVERAGING COORDINATES ON DATA
ANDREWJ.BLUMBERG,MATHIEUCARRIE`RE,JUNHOUFUNG,
ANDMICHAELA.MANDELL
Abstract. We introduce algorithms for robustly computing intrinsic coor-
dinates on point clouds. Our approach relies on generating many candidate
coordinatesbysubsamplingthedataandvaryinghyperparametersoftheem-
beddingalgorithm(e.g.,manifoldlearning). Wethenidentifyasubsetofrep-
resentative embeddings by clustering the collection of candidate coordinates
andusingshapedescriptorsfromtopologicaldataanalysis. Thefinaloutputis
theembeddingobtainedasanaverageoftherepresentativeembeddingsusing
generalizedProcrustesanalysis. Wevalidateouralgorithmonbothsynthetic
data and experimental measurements from genomics, demonstrating robust-
nesstonoiseandoutliers.
1. Introduction
A central postulate of modern data analysis is that the high-dimensional data
wemeasureinfactarisesassamplesfromalow-dimensionalgeometricobject. This
hypothesis is the basic justification for dimensionality reduction which is the first
stepinalmostallgeometricdataanalysisalgorithms,particularlyinclusteringand
itshigherdimensionalanalogs. Thisisaimedatmitigatingthe“curseofdimension-
ality”, a broad term for various concentration of measure results that imply that
thegeometryofhigh-dimensionalEuclideanspacesbehavesverycounter-intuitively.
Thecurseofdimensionalitytellsusthatifthishypothesisisnotsatisfied,wecannot
expect to perform any meaningful inference at all.
The recent explosion of data acquisition processes in many different scientific
fields (e.g., single-cell RNA sequencing experiments in computational biology, real-
istic synthetic data sets obtained from deep generative models, multivariate time
seriesgeneratedfromlargenumbersofsensors, etc.) hasledtoadramaticincrease
of publicly available data sets in high ambient dimensions. The need for tractable
andaccuratedatasciencetoolsforprocessingthesedatasetshasthusbecomecrit-
ical. Whilesupervisedmachinelearninganddeeplearningmethodshaveprovento
be very efficient in many different areas and applications of data science, they are
oftenlimitedbythedifficultyoffindingsuitabletrainingdata. Hence, theproblem
of studying this data via dimensionality reduction and exploratory data analysis
has become central.
However, dimensionality reduction algorithms generally have the inherent com-
plicationthattheydependonavarietyofadhochyperparametersandaresensitive
bothtonoiseconcentratedaroundtheunderlyinggeometricobjectandtooutliers.
ThefirstauthorwaspartiallysupportedbytheNSFgrantDMS-1912194andbyONRgrant
N00014-22-1-2679.
ThethirdauthorwassupportedbytheNSFundergrantDMS-1912194.
ThefourthauthorwassupportedbytheONRgrantN00014-22-1-2675.
1
4202
guA
2
]LM.tats[
1v97310.8042:viXra2 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
Moreover, all methods are known to be very sensitive to these hyperparameters:
a slight change in only one parameter can lead to dramatic changes in the out-
put lower-dimensional embeddings. As a consequence, it is challenging to perform
meaningful inference based on such procedures and most serious applications in-
volve a lot of ad hoc cross-validation procedures.
This article proposes a method to resolve this issue by producing robust embed-
dings, employing the following process:
• Subsamplingandvaryinghyperparameterstoproduceanumberofembed-
dingsofagiven(low)dimensionusingdimensionalityreductiontechniques,
• usingaffineisometriesandthesolutiontothegeneral“Procrustesproblem”
to align the embeddings in a way that minimizes a certain natural metric
(explained below),
• clustering the aligned embeddings based on that metric,
• identifyingaclusterof“representative”embeddingsbylookingattheclus-
terdensityandusingtopologicaldataanalysis(TDA)toeliminateclusters
with topologically complex embeddings, and
• taking the average (centroid) of the embeddings in the representative clus-
ter to produce a final low dimensional embedding of most of the points.
(Points which are not present in the averaged embeddings are returned as
“outliers”.)
Thesubsamplingleadstoresultsthatarerobustwithrespecttoisotropicnoiseand
have reduced sensitivity to outlier data points. The clustering discards outlier em-
beddings with a high level of distortion. Essentially any dimensionality reduction
algorithm can be used, but the procedure works best when the algorithm preserves
some global structure; e.g., t-SNE and UMAP, which preserve local relationships
but can radically distort global relationships produce less sharp results. (See Sec-
tion 8 for further discussion of this point.) The intuition behind our use of TDA
invariants (notably persistent homology) to detect the representative embeddings
is that we expect coordinate charts to be contractible subsets: theoretical guaran-
tees for manifold learning imply that the algorithms really only work in this case.
Persistent homology is a convenient way to detect complicated topological features
such as holes.
WeillustratetheprocessinFigure1. Thefigureshowsastandard3-dimensional
“Swiss roll” synthetic data set and uses Isomap to produce a 2-dimensional coor-
dinate chart. This is a particularly easy example, but we achieve similar results in
the presence even of a large number of outliers and across parameter regimes. See
Section 7 for a variety of synthetic examples.
The discussion above emphasizes the robustness in the presence of outliers, but
our technique has an additional use case where it can be employed to reduce the
computational complexity of dimensionality reduction computations: instead of
computing a dimensionality reduction of the entire data set, the procedure above
can be used to produce a global set of coordinates by averaging coordinates from
much smaller subsamples.
In order to demonstrate the use of our procedure on real data, we apply it to
multiple dimensionality reduction methods for producing coordinates on genomic
data sets in Section 8, specifically blood cells (PBMC) and mouse neural tissue.
The results show that our procedure can be used to produce robust coordinatesRESAMPLING AND AVERAGING COORDINATES ON DATA 3
(a) Original data in 3D (b) Isomap results on subsamples
(c) Clustering embeddings (d) Final aligned output
Figure 1. Swiss roll dataset
fromsomemanifoldlearningalgorithmsandindicatestheinstabilityofcoordinates
produced from others.
Relatedwork. Procrustesanalysishaslongbeenusedtostudyshapes[17]. Build-
ing on scaling methods, the orthogonal Procrustes problem was first studied and
solved by Green [16], and Sch¨onemann and Sch¨onemann-Carroll [25, 26], and later
this was generalized to three or more shapes by Gower [14]. Nowadays, there are
many variants of Procrustes problems. Beyond statistical shape analysis [13], Pro-
crustesanalysishasfoundapplicationsinsensoryscience[11],marketresearch[15],
image analysis [12], morphometrics [8], protein structure [22], environmental sci-
ence[24],populationgenetics[34],phylogenetics[4],chemistry[1],ecology[20],and
more. Inforthcomingwork,theauthorswillpresentanapplicationtoneuroscience.
The early applications of Procrustes analysis were in the field of psychometrics,
where point sets are registered to a single “reference” profile that is sometimes
realized in physical space, and datasets are small. More recently, it has been in-
creasingly recognized that the same techniques and ideas can be applied just as
well to the shape of “data” itself with no expectation of a reference whatsoever,
and to high-dimensional datasets [2] with many samples. For example, [21] uses
a version of Procrustes analysis with shuffling to align low-rank representations of
singlecelltranscriptomicdata. Procrustesanalysiscanalsobeappliedtostatistics
itself. Inthissense,partofourcurrentstudycanbeviewedasanexpansionof[19],
which studied a jackknife procedure on multidimensional scaling, a limited form of
manifold learning. Similarly, [33] applied Procrustes analysis to the outputs of4 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
dimensionality reduction, specifically Laplacian eigenmaps and locality preserving
projections, but only for two point clouds at a time.
Summary. In Section 2, we provide a concise review of the background for our
work; we discuss manifold learning and dimensionality reduction algorithms, the
Procrustes distance and alignment problem, and topological data analysis. Next,
in Section 3, we describe our algorithm. We then begin a theoretical analysis of
the behavior of our algorithm by reviewing in detail the solution to the generalized
Procrustes problem in Section 4. We employ a solver based on alternating least
squares minimization, and we describe the stability and convergence behavior of
thisprocessinSection5. Wethenusethistoanalyzethestabilityofouralgorithm
in Section 6. The paper concludes with two sections exploring the behavior of our
algorithm: Section 7 studies how it works on simulated data with various kinds of
noise and outliers, and Section 8 applies our algorithm to single-cell RNAseq data.
Codeavailability. Foraligningtwopointclouds,weusedthecommandscipy.spatial.procrustes
from the SciPy library [32]. For general Procrustes alignment of two or more point
clouds, we developed our own software implementation based on alternating mini-
mization, which can be found at github.com/jhfung/Procrustes.
Acknowledgments. We thank Abby Hickok and Raul Rabadan for useful com-
ments.
2. Background about manifold learning, topological data analysis,
and Procrustes distance
Our algorithm uses as building blocks three fundamental concepts from geo-
metric data analysis. The first building block is dimensionality reduction. Our
algorithm takes as a black box a choice of dimensionality reduction algorithm. We
usemanifoldlearningforthispurpose,andtobeconcrete,wewillfocusonIsomap,
but many other manifold learning (or more general dimensionality reduction algo-
rithms)couldbeusedinstead. Manifoldlearningseekstofindintrinsiccoordinates
on data that reflects the shape of the data. The second building block is persistent
homology, which is the main invariant of topological data analysis (TDA). Per-
sistent homology is a multiscale shape descriptor that provides qualitative shape
information; we will use it to detect when coordinate charts are spread out and
have contractible components. The third building block is the Procrustes distance,
which is a matching distance for (partially defined) vector-valued functions on a
givenfiniteset. ComputingtheProcrustesdistanceinvolvescomputinganoptimal
alignment of the vectors; our algorithm uses both the distance as a metric and also
the computation of the distance as an alignment algorithm.
The manifold learning algorithm takes a point cloud X ⊂ RN and produces an
embedding X → Rd for d ≪ N meant to retain the intrinsic relationship between
thepointsofX. Specifically, forIsomap, theprocedureisasfollows. Givenapoint
cloud X ⊂ RN and a scale parameter ϵ > 0 (or a nearest neighbor limit ℓ), we
formtheweightedgraphGwithvertexsetlabelledbythepointsX andanedgeof
weight ∥x−x′∥ when ∥x−x′∥<ϵ (or when x′ is one of the ℓ closest points to x).
The graph metric on G now provides a new metric on the point cloud X. Finally,
we use MDS (multidimensional scaling) to embed this new metric space into Rd,
for d ≪ N. The intuition is that when the data X comes from a low dimensional
manifold embedded in a high dimensional space, short paths in the ambient spaceRESAMPLING AND AVERAGING COORDINATES ON DATA 5
accurately reflect an intrinsic notion of nearness in the manifold. When the points
X have been uniformly sampled from a convex subset of a manifold such that ϵ is
sufficientlysmallrelativetothecurvatureofthemanifoldandtheinjectivityradius
oftheembedding(whichcanbedescribedintermsofthereachorconditionnumber
of the manifold), the coordinates produced by this procedure do approximate the
intrinsic Riemannian metric on X.
Topological data analysis uses invariants from algebraic topology that encode
qualitative shape information to give a picture of the intrinsic geometry of a point
cloud. The qth homology of a topological space T is a vector space which encodes
theq-dimensionalholesinT;forexample,whenq =0thisismeasuringthenumber
of path-connected components of the space and when q = 1 it is counting the
numberofcirclesthatarenotfilledin. Topologicaldataanalysisassignstoapoint
cloud a family of associated simplicial complexes, usually filtered by a varying
feature scale parameter. The resulting algebraic invariant, persistent homology,
captures the scales at which homological features appear and disappear. In the
casewhenq =0,thepersistenthomologyisbasicallythesingle-linkagehierarchical
clustering dendrogram. When q =1, persistent homology measures the number of
loops in the underlying graph of the data that cannot be filled in at each scale.
Since manifold learning only really makes sense for relatively evenly sampled
points from subsets whose components are contractible (or in the case of Isomap,
the stronger condition of being convex), we can use topological data analysis to
measure how well the resulting chart appears to satisfy this condition. Specifically,
a good chart will have a large cluster for each component in the PH dendrogram
0
and will have only very small (noise) loops in PH .
1
The Procrustes distance we use comes from the Procrustes problem. In basic
form,giventwod×nmatricesX andY,thoughtofastwofunctionsfrom{1,...,n}
to Rd, the Procrustes distance is the minimum of the ℓ2 distances between the
functions after applying an affine isometry g to one of them:
 
n
(cid:88) 1/2
D(X,Y)=mind(g·X,Y)=min d(g·X(j),Y(j))2  .
g g
j=1
More generally, we consider the case when X and Y are partially defined functions
on {1,...,n}. If we let I and I denote the subsets of {1,...,n} where X and Y
X Y
(respectively) are defined, then the Procrustes distance is
 
(cid:88) 1/2
D(X,Y)=min d(g·X(j),Y(j))2  .
g
j∈IX∩IY
ThealgorithmforcalculatingD findsanaffineisometryg minimizingthedistance;
we review it in Section 4. We note that in the partially defined case, the result-
ing Procrustes distance D is not a metric; however, for functions with substantial
overlap in domain, it does provide a measure of dissimilarity on their overlap.
Our algorithm uses a generalization of the Procrustes distance algorithm to
search for optimal rotations to align different embeddings. The algorithm for two
matrices always finds an optimal rotation (and therefore accurately calculates the
Procrustes distance) and in principle gives an exact solution. The algorithm to
align more than two matrices is an iterative optimization procedure, which always6 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
converges, and appears to generically converge to the optimal solution, but is not
guaranteed to converge to it.
3. The algorithm
This section defines the basic algorithm we propose in this paper. It admits
several choices of blackbox subroutines and tolerance parameters. The first major
blackbox subroutine is a procedure DimRed for dimensionality reduction: for
X ⊂ RN produces an embedding X → Rd (where d ≪ N). We let Θ denote the
collection of parameters controlling the behavior of the dimensionality reduction
procedure. For sake of discussion, we take the procedure to be Isomap, which has
parameter the neighborhood size ϵ.
A second major blackbox subroutine is a sampling procedure Samp that gener-
ates random subsamples Y ⊆X of a given size n. Typically, we draw Y uniformly
i
and independently without replacement from X.
A third major blackbox subroutine is a procedure Param that chooses the pa-
rameter Θ for DimRed. In the case of Isomap, typically we take the parameter
over a mesh of reasonable values.
The final major blackbox subroutine is a clustering algorithm Clust for finite
sets with dissimilarity measures.
We describe the various minor parameters controlling the number of iterations,
tolerances forcertainoptimization procedures, number ofsubsamples, etc., asthey
occur below.
Given the parameter choices, the input to the algorithm is a point cloud
X ={X(j)|j =1,...,n}⊂RN.
Step 1. We use Samp to generate many subsamples {Y } and apply DimRed
a
with parameter settings from Param to produce embeddings ϕ : Y → Rd. Let
a,b a
X be the subset of Rd specified by the image, where the elements of each sub-
a,b
sample configuration X inherit the indexing of original data set X. That is,
a,b
each X is indexed by the subset of {1,...,n} corresponding to the points of X
a,b
represented in the subsample Y .
a
Step 2. We calculate the pairwise distances between the images X using the
a,b
Procrustes distance to define a dissimilarity measure on the set of subsample con-
figurations {X } a pseudo-metric space.
a,b
Step 3. We use Clust to form clusters.
Step 4. Choose a distinguished cluster (the “good cluster”) from among these as
follows.
• We discard clusters whose median inter-point distance is above a certain
tolerance (a minor parameter of the algorithm). We refer to the remaining
clusters as “dense clusters”.
• Foreachdenseclusterweselectrandomelementsinthecluster(thenumber
or percentage size of the selection a minor parameter of the algorithm) on
which to calculate the persistent homology PH and essential dimension-
1
ality (number of singular values above given tolerance).RESAMPLING AND AVERAGING COORDINATES ON DATA 7
• Wediscarddenseclusterswhereaselectedelementhasessentialdimension-
ality less than d, and then we choose the good cluster to be the one that
minimizes the maximum length of bars in PH .
1
• If there are no remaining clusters or the maximum length of bars in PH
1
is above a certain tolerance (a minor parameter of the algorithm), we ter-
minate with an error.
Step 5. We discard all the subsets X not in the good cluster, and singularly
a,b
reindex the subsets in the good cluster X = X for i = 1,...,k (with k the
i ai,bi
number of X in the good cluster).
a,b
Step 6. We align the embeddings X by applying an affine isometry X (j) (cid:55)→
i i
Q X (j)+v to each point X (j) in X (for the indices j ∈{1,...,n} that occur in
i i i i i
X ) where Q is a d×d orthogonal matrix and v is a vector in Rd, chosen by the
i i i
alternating least squares method described in Section 4.
Output. The final output is an appropriate average of the aligned embeddings as
follows. For each point X(j) in X (j =1,...,n):
• If no X includes a jth point, then the jth point is omitted from the final
i
embedding; otherwise,
• The final embedding has jth point the average in Rd of the jth points of
the X which have jth points,
i
X¯(j)= 1(X (j)+···+X (j)).
s i1 is
• We also output a list of “outliers” that consists of the omitted indices, the
indices that did not have points in any of the embeddings X chosen to
i
produce the final average.
4. A review of the Procrustes problem
Our main ingredient for averaging coordinates is posing the averaging in terms
of the so-called Procrustes problem, an alignment problem that has been studied
extensively in the psychometrics literature. The material in this section distills the
discussion in [5, 9, 6].
The standard orthogonal and affine orthogonal Procrustes problem. In
its most standard form, given two matrices X,Y of the same shape, this problem
isamatrixapproximationproblemthataimsatfindingthebestorthogonalmatrix
that matches X to Y:
(4.1) Q = argmin ∥QX−Y∥ ,
min F
Q s.t. QTQ=1
where∥·∥ denotestheFrobeniusnorm(thesquarerootofthesumofthesquares
F
of the entries). By interpreting Q as an isometry (in Euclidean space), and
min
X,Y as point clouds (each column representing a point), the standard Procrustes
problem can thus be seen as an alignment problem which aims at finding the best
orthogonal transformation that transports the point cloud X closest to the point
cloudY intermsofthesumofpointwisedistances. Inpractice,itcanreformulated
as finding the best orthogonal matrix Q approximating the given matrix YXT,
min
and proved to be efficiently solved by computing the SVD of
YXT =UΣVT8 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
(with U and V orthogonal and Σ non-negative diagonal) and taking Q =UVT.
min
If we allow the more general affine isometries (that allow a translation compo-
nent), it is easy to check that
(4.2) Ω = argmin∥ΩX−Y∥ ,
min F
Ω∈Aff(d)
is given by the affine isometry
Ω x=Q(x−a)+b=Qx+(b−Qa)
min
whereaisthecentroid(mean)ofX,bisthecentroidofY,andQistheorthogonal
matrix Q that solves equation (4.1) for the matrices X−1a and Y −1b (where
min
1 is the 1×n matrix of 1s: the matrices X−1a and Y −1b are the translations of
X and Y to be centered on the origin.)
In the notation of Section 2, the Procrustes distance D(X,Y) from X to Y is
then
D(X,Y)=∥Ω X−Y∥ .
min F
As discussed in Section 2, we need to consider the more general case when the
matrices are missing columns; we refer to this as the missing points case. Viewing
d×n matrices as functions {1,...,n} to Rd, the missing points case is when we
have partially defined functions X and Y from {1,...,n} to Rd; that is X and
Y are functions from subsets I and I of {1,...,n} to Rd. Let I be the natural
X Y
reindexingoftheintersectionofthedomainsI ∩I . ThentheProcrustesdistance
X Y
D(X,Y) as defined in Section 2 is calculated by the formula
D(X,Y)=∥Ω X −Y ∥
min I I F
whereΩ solves(4.2)forthematricesX ,Y (thematricesobtainedbyrestricting
min I I
X and Y to I ∩I ).
X Y
BackgroundonthegeneralizedProcrustesproblem. Theformulationofthe
Procrustes problem in the previous subsection involved only 2 matrices or matri-
ces missing points (partially defined functions). In this subsection, we begin the
discussion of the problem for multiple matrices. We then generalize to the missing
points case and discuss algorithms in the following subsections.
We begin with some notation. Let X ,X ,...,X be a collection of k point
1 2 k
clouds, each containing n ordered points in Rd. We can represent each X as a
i
d × n matrix, and denote the collection of such by X = (X ). The allowable
i
transformations will be drawn from a group G, which we generally take to be the
group of linear isometries (orthogonal transformations) or affine isometries of Rd.
We write g ∈G for the transformation that will be applied to the ith point cloud
i
X , and g=(g ) for the collection of them.
i i
Definition 4.3. The generalized Procrustes problem is the following: given X, a
set of k input configurations of n points in Rd and group G acting continuously on
Rd, determine the optimal transformations g and configuration Z that minimizes
the loss functional
k k n
1 (cid:88) 1 (cid:88)(cid:88)
E(X,g,Z)= ∥g ·X −Z∥2 = ∥g ·X (j)−Z(j)∥2
k i i F k i i
i=1 i=1j=1
TheexistenceofsolutionstoProcrustesproblemsinthegroupsweareinterested
in follows from simple considerations.RESAMPLING AND AVERAGING COORDINATES ON DATA 9
Proposition 4.4. If G is compact or the semi-direct product of a compact group
and the translation group, then E achieves a global minimum.
The solution is never unique when G contains non-identity isometries of Rd
because for any X,g,Z, and any isometry h∈G, writing hg for (hg ), we have
i
E(X,g,Z)=E(X,hg,h·Z).
InthecasewhenGasubgroupoftheisometriesofRd,wecaneliminatethistrivial
duplication of solutions by considering only solutions that have g a fixed element
1
of G, possibly depending on X. An obvious choice is to take g to be the identity,
1
but the choice we take below is to choose g to be the translation that centers X
1 1
on 0 ∈ Rd. The first fixed formulation of the generalized Procrustes problem (for
G the group of affine isometries) is to find g,Z minimizing the loss functional and
satisfyingthefurtherconditionthatg isthetranslationthatcentersX on0∈Rd.
1 1
We now specialize to the case where G is the group of affine isometries Aff(d)
(consisting of composites of translations, rotations, and reflections in Rd). We
specify an element g of G by a d×d orthogonal matrix Q and a vector v, where
for x∈Rd,
g·x=Qx+v.
On a configuration X, viewed as a d×n matrix, the action is given by matrix
multiplication and addition
g·X =QX+1v
where 1 denotes the 1×n matrix of 1s.
AkeyobservationisthatwecaneliminatetranslationsandZ asvariablesinthe
generalized Procrustes problem:
Proposition4.5. ForfixedX,theminimumvalueofE(X,g,Z)occursatelements
g, Z where the centroid of each g X is the origin and each Z(j) is the average of
i i
X (j),
i
k
1 (cid:88)
Z(j)= g ·X (j).
k i i
i=1
Proof. We begin with the translation part. For fixed X,g, Z, let q ∈ 1,...,k, let
v ∈ Rd, and consider the path g(t) in Gk where g (t) = g for i ̸= q, and g (t) is
i i q
the composite of g followed by the translation x(cid:55)→x+tv. Then
q
(cid:12) k n (cid:18) (cid:12) (cid:19)
d(cid:12) 2 (cid:88)(cid:88) d(cid:12)
dt(cid:12)
(cid:12)
E(X,g(t),Z)=
k
(g i·X i(j)−Z(j))· dt(cid:12)
(cid:12)
(g i(t)·X i(j)−Z(j))
t=0 i=1j=1 t=0
n
2 (cid:88)
= (g ·X (j)−Z(j))·v.
k q q
j=1
If g,Z gives a minimum for E(X,g,Z) then this derivative must be zero for every
q and v, which implies that
n n
(cid:88) (cid:88)
Z(j)= g ·X (j)
q q
j=1 j=1
for all q, and so all the g ·X and Z must have the same centroid. Thus, the
i i
minimum can only occur for a g that centers X on the centroid of Z. In the first
i i
fixed formulation, the common centroid of Z and the X is then the origin. More
i10 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
generally, taking advantage of the symmetry of the problem as a whole, given a
solution g,Z, there exists a solution with the common centroid at the origin by
applying an appropriate translation.
To eliminate Z as a variable, if we keep g fixed and for some r ∈1,...,n, let Z
t
be the path with Z (j)=Z(j) for j ̸=r and Z (r)=Z(j)+tv, we then have
t t
(cid:12) k n (cid:18) (cid:12) (cid:19)
d(cid:12) 2 (cid:88)(cid:88) d(cid:12)
dt(cid:12)
(cid:12)
E(X,g,Z t)=
k
(g i·X i(j)−Z(j))· dt(cid:12)
(cid:12)
(g i·X i(j)−Z t(j))
t=0 i=1j=1 t=0
k
2 (cid:88)
= (g ·X (r)−Z(r))·v.
k i i
i=1
In the case when g,Z gives a minimum for the loss functional, we then conclude
k
1 (cid:88)
Z(r)= g ·X (r)
k i i
i=1
for all r. The minimum can only occur when Z(j) is the average over i of the
g ·X (j). □
i i
By Proposition 4.5, we do not need to search over the space of Z and we can
restrict to searching for the orthogonal transformation part of the g . This leads to
i
the centered formulation of the problem.
Definition 4.6. The centered formulation of the generalized Procrustes problem
for the group of affine isometries is the following: given X, a set of k input con-
figurations of n points in Rd whose centroids are the origin, determine the optimal
orthogonal transformations Q=(Q )∈O(d)n that minimize the loss functional
i
k k n
1 (cid:88) 1 (cid:88)(cid:88)
E(X,Q)= ∥Q X −Z∥2 = ∥Q X (j)−Z(j)∥2
k i i F k i i
i=1 i=1j=1
where Z(j)= 1 (cid:80) Q X (j).
k i i
There is always a solution with Q = Id; the centered first fixed formulation is
1
to find the optimal Q subject to the restriction that Q =Id.
1
In the centered formulation, because Z is the mean of the Q X , we can rewrite
i i
the loss functional in the following form, which is sometimes more convenient:
k
(4.7) E(X,Q)= 1 (cid:88)(cid:0) ∥Q X ∥2 −∥Z∥2(cid:1) .
k i i F F
i=1
Expanding out the definition of Z, this is then:
k (cid:18) k (cid:19)
1 (cid:88) 1 (cid:88)
(4.8) E(X,Q)= ∥Q X ∥2 −∥ Q X ∥2 .
k i i F k i i F
i=1 i=1
The alternating least squares (ALS) method for the generalized Pro-
crustes problem. Thebackgroundintheprevioussubsectionsuggeststhefollow-
ing algorithm for searching for the solution to the generalized Procrustes problem
for the group of affine isometries. In the centered formulation, we iteratively use
the 2 matrix Procrustes problem solution applied to X and Z, to get better and
i
bettermatchesbetweeneachQ X andthemeanoftheQ X . Forthenon-centered
i i i iRESAMPLING AND AVERAGING COORDINATES ON DATA 11
formulation, we first reduce to the centered formulation by translating the original
configurations to be centered on the origin.
Westatethealgorithminthecenteredformulation,wherewearegiventheinput
configurationsX,whereweassumeeachX iscenteredontheorigin,andwesearch
i
for the orthogonal matrices Q minimizing the loss functional E of Definition 4.6.
We assume a small number tol as a pre-selected tolerance for termination and a
large integer max iter for a maximum number of iterations.
Algorithm (Basic ALS method).
Step 0. Initialize Z = 1 (cid:80)k X .
k i=1 i
Step 1. Set loss=E(X,Q).
Step 2. Use SVD to solve ZXT = U Σ VT (for i = 1,...,k) where U , V are
i i i i i i
orthogonal d×d matrices and Σ is a non-negative diagonal matrix with
i
diagonal entries in decreasing order.
Step 3. Update Z = 1 (cid:80)k U VTX .
k i=1 i i i
Step 4. If |loss−E(X,Q)|≥tol and the number of iterations is less than max iter,
iterate from Step 1. Else:
Step 5. Return (U VT),Z.
i i
Wehavewrittenthealgorithmtoemphasizetheconcept;itadmitsmanytweeks
to increase efficiency, some of which are discussed after the final version of the
algorithm in the next subsection.
For stability of output, we should find some normalization of the raw output.
For the first fixed formulation, we look for a solution (Q ),Z with Q =Id. Given
i 1
therawoutput(Q ),Z,wegetatransformedoutput(Q−1Q ),Q−1Z withthesame
i 1 i 1
loss functional value but with the first orthogonal transformation the identity.
Forempiricaldatawherethedifferentinputconfigurationsarenotfarfrombeing
rotationsandtranslationsofeachother, thisalgorithminpracticeconvergestothe
solution to the generalized Procrustes problem. For independent Gaussian random
input configurations, there are many local minima for the loss functional that are
close to the absolute minimum, and this algorithm does not always find the trans-
formations giving the absolute minimum, but does in practice find transformations
with loss functional close to the minimum.
The algorithm above admits some obvious criticisms. First, while generically
it does solve the Procrustes problem for 2 input configurations, there is a low
dimensional space of inputs where the algorithm fails. For example, the algorithm
fails when X = −X . The correct fix for this is not to apply this algorithm with
2 1
fewerthan3inputconfigurations,andusetheprecisenotiterativealgorithminthat
case. More subtly, if the input configurations are at a unstable critical point of the
loss functional, the algorithm will terminate immediately and not find a minimum
or local minimum. The fix for this is to require a minimum number of iterations
before termination.
While this is the obvious algorithm based on the discussion above, ten Berge [5]
doesadeeperanalysisofthegeneralizedProcrustesproblemandfindsanalgorithm
thatinpracticeappearstofindtransformationswithsmallerlossfunctionalthanthe
basic algorithm a large percentage of the time on inputs where the loss functional
has a large number of local minima. The basic algorithm above implicitly uses the
factthatwhenQ,Z solvesthegeneralizedProcrustesproblem,theneachZ(Q X )T
i i
is symmetric positive semi-definite (where we are writing Q for U VT). In terms
i i i12 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
of the algorithm, this is the result of the iteration because at Step 3, we have
Z(Q X )T =ZXTQT =(U Σ VT)(U VT)T =U Σ UT.
i i i i i i i i i i i i
However, a solution to the generalized Procrustes problem actually satisfies the
more restrictive condition that (Z − 1Q X )(Q X )T is symmetric positive semi-
k i i i i
definite. This leads to the following more sophisticated algorithm.
Algorithm (ALS method).
Step 0. Initialize Q =Id, Z = 1 (cid:80)k X .
i k i=1 i
Step 1. Set loss=E(X,Q).
Step 2. For i in 1,...,k
(a) Use SVD to solve (Z − 1Q X )XT = U Σ VT for U , V orthogonal
k i i i i i i i i
d×d matrices and Σ a non-negative diagonal matrix with diagonal
i
entries in decreasing order.
(b) Update Q =U VT
i i i
(c) Update Z = 1 (cid:80)k Q X .
k i=1 i i
Step 3. If |loss−E(X,Q)|≥tol and the number of iterations is less than max iter,
iterate from Step 1. Else:
Step 4. Return (Q ),Z.
i
We note that in the case of 2 input configurations, this algorithm always finds
the solution to the Procrustes problem (but does an extra step from the usual 2
input configuration algorithm).
Again, for stability, we should normalize the output, for example by replacing
(Q ),Z with (Q−1Q ),Q−1Z.
i 1 i 1
The ALS method for the generalized Procrustes problem with missing
points. In the context of missing points, the configurations X only have elements
i
of Rd specified for some but not necessarily all indices {1,...,n}. It is still conve-
nient to represent X as a d×n matrix, where we fill in the zero column for the
i
for the indices where X is not defined. To keep track of which indices are defined
i
in a manner conducive to easily expressed matrix operations, we let K denote the
i
n×ndiagonalmatrixwhichhasdiagonalentry1attheindiceswhereX isdefined
i
and0attheindiceswhereX isnotdefined. Inthiscase, wheneverX′ isanyd×n
i i
that agrees with X on the columns for which X is defined, X =X′K . (In other
i i i i i
words, if we always work with X K , it does not matter how we fill in the columns
i i
where X is not defined.) We assume that the configuration X is non-empty, and
i i
therefore K is not the zero matrix. We let n denote the number of points in X ;
i i i
then 0<n ≤n, and n is the sum of the entries of K .
i i i
LetK
=(cid:80)k
K . ThenK isadiagonalmatrixandthediagonalentriesindicate
i=1 i
the number of configurations in which a particular index occurs. Without loss of
generality,wecanassumethatnoneofthesediagonalentriesiszero: ifitis,wecan
drop that index from consideration and reindex the problem as a whole. Then K
is invertible and setting k =K , j =1,...,n, we have k >0.
j j,j j
Inthisregime,themeanoftheconfigurationsshouldbecalculatedateachindex
j =1,...,n using only the configurations in which that index appears:
k k
1 (cid:88) (cid:88)
Z(j)= (X K )(j)=( X K K−1)(j)
k i i i i
j
i=1 i=1RESAMPLING AND AVERAGING COORDINATES ON DATA 13
(where, generalizing the convention for the configurations X , we are writing Y(j)
i
for the jth column of an arbitrary d×n matrix Y).
When some configurations are missing points, we can no longer center all the
configurations at 0 and expect the mean to be centered at 0, and we can no longer
work in the centered formulation. Specifying an affine isometry using a linear
isometry Q and a translation vector v, the loss function for a particular choice of
Q=(Q ),v=(v ) becomes
i i
k
1 (cid:88)
E(X,K,Q,v)= ∥Q (X +1v )−Z∥2
k i i i F
i=1
where
k
(cid:88)
Z = Q (X +1v )K K−1.
i i i i
i=1
(Here as above 1 denotes the 1×n matrix of 1s.)
The ALS algorithm also needs to be modified to account for translations, and
sincetheoutputconfigurationsmaynolongerbecenteredat0,weshouldnolonger
ask for the input configurations to be centered at 0.
Algorithm (ALS method with missing points).
Step 0 Initialize constants:
k n n
(cid:88) (cid:88) 1 (cid:88)
K = K ,n = K ,a = (X K )(j)
i i j,j i n i i
i
i=1 j=1 j=1
and variables:
k
(cid:88)
Q =Id,v =b =0∈Rd,Z = X K K−1
i i i i i
i=1
Step 1 Set loss=E(X,K,Q,v)
Step 2 For i in 1,...,k
(a) Update b = 1 (cid:80)n (ZK )(j)
i ni j=1 i
(b) Use SVD to solve
((Z−1b )K −(X −1a )K K−1)(Q (X −1a )K )T =U Σ VT
i i i i i i i i i i i i
for U , V orthogonal d×d matrices and Σ a non-negative diagonal
i i i
matrices with diagonal entries in decreasing order.
(c) Update Q =UVT, v =b −Q a
i i i i i
(d) Update Z =(cid:80)k (Q X +1v )K K−1.
i=1 i i i i
Step 3. If |loss−E(X,K,Q,v)| ≥ tol and the number of iterations is less than
max iter, iterate from Step 1. Else:
Step 4. Return (Q ),(v ),Z.
i i
There are several obvious ways to improve the efficiency of this algorithm. We
mentiononlyafewofthemostobviousasmuchwilldependontheimplementation
of the libraries. Clearly several values should be stored rather than recomputed
(including the loss functional E(X,K,Q,v), and the transformed configurations
Q (X +1v )K ). Also, weshouldadjusttheinputsothatthemissingpointsinX
i i i i i
are represented by the zero column (to use X in place of X K ) and is centered on
i i i
zero (to use X instead of (X −1a )K ). In the update of Z, it is more efficient
i i i i14 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
to subtract off the old value of Q (X +1v )K K−1 and add the new value rather
i i i i
than take the sum as written.
5. Behavior of the alternating least squares method
The purpose of this section is to describe what is known about the theoretical
behavior of the alternating least squares (ALS) method to search for solutions to
the generalized Procrustes problem, which is the last step in our main algorithm.
We review the results about this step needed for understanding the robustness of
the main algorithm.
There are three natural questions we address:
(1) When does the ALS method converge to a local optimum?
(2) When are the local optima isolated?
(3) How much does the output change when the input data are perturbed?
Allofthesequestionshavereasonablysatisfyinganswersmoreorlessintheliter-
ature, as we now review. We discuss the case without missing points for simplicity
of notation, but the missing points case works similarly.
Convergence of the ALS method. Standard considerations imply that itera-
tions of the ALS method decrease the loss functional (4.7) monotonically. This
implies that the algorithm always converges. However, examples can be produced
where the ALS method outputs a centroid that does not locally minimize the con-
strainedlossfunctional,atleastinthecasewhenwedonotrequiretheittoperform
a minimum number of iterations. While such bad examples can be constructed by
hand, our experiments and the long literature on the subject bears out the conjec-
turethatgenericallytheconvergenceistoalocalminimum,anditseemstoalways
converge to a local minimum when required to perform a reasonable minimum
numberofiterations. Weconjecturethatthebadexamplesformalowdimensional
subspace of the space of all possible input data when the number of points in each
configuration is larger than the dimension of the ambient Euclidean space.
As we explain below, up to a precision determined by the tolerance setting for
the algorithm, the ALS method always converges to a critical point for the loss
functional,constrainedtotheorbitoftheoriginalinputconfigurations. Inpractice,
wecancheckthattheresultingpointisalocalminimumusingthesecondderivative
test. We give a formula for the second derivative of the constrained loss functional
in the next subsection.
To see that the ALS method always converges to a critical point of the con-
strained loss functional, we use the following notation. After the sth iteration of
the main loop, we have a set of orthogonal transformations that we denote here
as Q[s] and a centroid for the transformed configurations that we denote here as
Z[s]. We let X[s] denote the transformed configurations, X[s] = Q[s]X . The ALS
i i i
methodthenconvergestoX[∞] =X[maxiter],andwearguethatforeachi,thed×d
matrix Z[∞](X[∞])T is (approximately) symmetric: because Q[s+1] is the solution
i
to the classical orthogonal Procrustes problem for X , Z[s], we have that
i
1
(Z[s]− Q[s]X )(Q[s+1]X )T
k i i i i
is symmetric (as discussed in Section 4). This then implies that
1 1
(Z[∞]− X[∞])(X[∞])T =(Z[∞]− Q[∞]X )(Q[∞]X )T
k i i k i i i iRESAMPLING AND AVERAGING COORDINATES ON DATA 15
is (approximately) symmetric and since AAT is always symmetric, we have that
Z[∞](X[∞])T is (approximately) symmetric.
i
The critical points X for the constrained loss functional (4.8) are precisely the
points where the d×d matrices ZXT are symmetric for all i. To explain this, we
i
calculate the derivative of the constrained loss functional. At any point X, the
tangent space of the orbit (in the centered first fixed formulation) is canonically
isomorphic to the product of k−1 copies of the tangent space of O(d), and so an
elementofthistangentspaceisspecifiedbyachoiceofd×danti-symmetricmatrix
A for i = 2,...,k. (To give uniform formulas, we set A to be the d×d zero
i 1
matrix.) Integrating along such an element gives the path in the orbit (Q (t)X )
i i
where Q i(t) = eAit. If we write Z(t) for the centroid as a function of t, we then
have
(cid:12) (cid:18) (cid:12) (cid:19) k n
d(cid:12) d(cid:12) (cid:88)(cid:88)
dt(cid:12)
(cid:12)
E(X,Q(t))=−2kZ· dt(cid:12)
(cid:12)
Z(t) =−2 Z(j)·A iX i(j).
t=0 t=0 i=2j=1
For the derivative to vanish, we therefore must have
n
(cid:88)
Z(j)·AX (j)=0
i
j=1
for all i=2,...,k and all d×d anti-symmetric matrices A. Equivalently, for every
anti-symmetric bilinear form Φ on Rd, we must have
n
(cid:88)
Φ(Z(j),X (j))=0
i
j=1
foralli=2,...,k,andthisisequivalenttotherequirementthatthed×dmatrices
n
(cid:88)
Z(j)X (j)T
i
j=1
aresymmetricforalli=2,...,k(whichalsoimplies(cid:80) Z(j)X (j)T issymmetric).
j 1
Isolation of the critical points. We cannot expect numerical stability of the
limit of the ALS method unless the critical point it converges to is isolated. Nu-
merical experiments indicate that the ALS method converges to a critical point
with positive definite Hessian for the loss functional; mathematically, such points
are always isolated local minima.
TheHessiancanbecalculatedusingthesecondderivativeofthepathsconsidered
in the previous subsection. For Q
i
=eAit, we get
dd t2 2(cid:12) (cid:12) (cid:12)
(cid:12)
E(X,Q(t))=−2(cid:88)n (cid:18) k1(cid:13) (cid:13) (cid:13) (cid:13)(cid:88)k A iX i(j)(cid:13) (cid:13) (cid:13) (cid:13)2 +Z(j)·(cid:0)(cid:88)k A2 iX i(j)(cid:1)(cid:19) .
t=0 j=1 i=2 i=2
For fixed X, defining a quadratic form q(A) by the formula above, the Hessian at
X is given by the formula
H(A,B)= 1(q(A+B)−q(A)−q(B)).
2
Choosing a basis for d×d anti-symmetric matrices, the Hessian becomes a sym-
metric (k−1)(d−1)(d−2)/2-dimensional square matrix. The quadratic form q is
positive definite if and only if the resulting matrix has only positive eigenvalues.16 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
Perturbation of the input data. The ALS method is stable in small perturba-
tions of generic input data in the following sense: for any ϵ>0 and for every point
X in the data space with property that the d×d matrices ZXT is non-singular for
i
all i, there is a neighborhood around X (whose size depends on X) where the ALS
method converges to a point within ϵ of the limit X[maxiter] for X. This stability
is a consequence of two basic well-known stability results for the singular value
decomposition, as we now explain.
The main result of [28, p. 2.1] studies stability of the solution of the orthogonal
Procrustes problem. In our notation, it shows that when each point cloud X and
i
the pointwise mean Z are in general position, then for ϵ > 0 small enough, there
is a neighborhood around the X where the orthonormal matrix Q solving the
i i
orthogonal Procrustes problem for X and Z has Frobenius norm less than ϵ. The
i
size of the neighborhood depends on the smallest two singular values of the ZXT
i
(which are non-zero under the non-singular hypotheses). We have control over the
change in singular values under perturbations by Mirsky’s theorem [29, §2].
Putting these together and using the fact that when actually implemented, the
ALS method has finitely many iterations, we can conclude stability as described
above. While the theoretical bounds give very pessimistic estimates of the size of
neighborhoodofcontrol,inpracticewefindexperimentallythattheALSmethodis
stableforperturbationsroughlythesameorderofmagnitudeasϵ. Inthecasewhen
the point clouds are all isometric up to small perturbations, the stability increases
and input perturbations of order ϵ give output perturbations of order ϵ2 (see [27,
p. 3.1] and the proof of [18, p. 6.1]).
6. Stability and robustness of the algorithm
Under reasonable hypotheses on the input data, with high probability, the algo-
rithm has the following stability and robustness properties.
Stability in choice of subsamples. Thealgorithmisdesignedfordataexpected
to approximate a contractible neighborhood of a manifold embedded in a high
dimensional Euclidean space (or a disjoint union of such). When this is the case,
with high probability a uniformly randomly chosen set of subsamples will have a
unique good cluster, which is the restriction of a unique good cluster for the set of
allsubsamples[7]. Thecentroidoftheuniqueclusteroftherandomsubsampleswill
approximatethecentroidoftheuniquegoodclusterforallsubsamplesinProcrustes
distance.
Stability in presence of noise in data values. Ifthenoiseissmallenoughthat
it results in only a small distortion in the results of the dimensionality reduction
algorithm,thestabilityoftheProcrustesalgorithmdiscussedinSection5combined
with the stability in choice of subsamples implies that the resulting final embed-
ding in the presence of small noise will be close in the Procrustes distance to the
embedding that would have been produced in the absence of noise.
Robustness in presence of outliers. Tautologically, outliers that consistently
distort the embeddings for any subsample containing them will not be in any of
the subsamples in the unique good cluster. Thus, they will be identified as outliers
by the algorithm and omitted from the final embedding. In some cases, the dimen-
sionality reduction algorithm may still return good embeddings in the presence of
a small number of outliers that are comparatively close to the rest of the pointsRESAMPLING AND AVERAGING COORDINATES ON DATA 17
of the subsample. When these embeddings are averaged with the relatively larger
number of embeddings in the good cluster that do not contain the outliers, their
overall contribution to the final embedding becomes small. Considering the two
cases, we see that even with addition of a small number of outliers of any kind the
output of the algorithm is a low-distortion embedding of the majority of the data
that is close in Procrustes distance to the embedding that would be computed if
the outliers were omitted.
Robustness in bad parameter choices. As in the case of sufficiently bad out-
liers, samples corresponding to parameter choices that result in distorted embed-
dings that are far from the unique good cluster will simply be discarded. As a
consequence, the final output is generally insensitive to a small number of isolated
bad parameter choices.
7. Synthetic experiments in manifold learning
This section describes some experiments with synthetic data. The first set of
experiments (Subsections 7.1–7.4) validate the claim about robustness of the algo-
rithmvianumericalexperiments. ItusesthefamiliarSwissrollexample. Inthisex-
ample,ourhypothesesaboutthedatasetholds: namelyitisahigh(er)dimensional
embedding of a contractible subspace of a low dimensional manifold (or a disjoint
of such). Our algorithm consistently constructs a good 2-dimensional embedding
even with the addition of significant noise, outliers, and parameter variation. The
secondexample(Subsection7.5)exploreswhathappenswhenthesehypothesesare
violated, using an analysis of data lying on “buckyballs”, which are spherical and
notcontractible. Thisdatadoesnotadmitalow-distortionembeddingintoR2 and
this can be seen in the intermediate steps of our procedure. Clustering the embed-
dingsinProcrustesspacerevealsthisfailureandmoreoverallowsustoanalyzethe
structure of the collection of embeddings. These steps give clear indication that no
good 2-dimensional embedding exists. Finally, we explore the use of our algorithm
to handle data sets too large to analyze without subsampling (Subsection 7.6).
7.1. Robustnessinthepresenceofambientnoiseconcentratedaroundthe
samples. This subsection describes a warmup experiment that illustrates the way
that our algorithm corrects for some of the variation introduced by subsampling.
In order to make this clearer, rather than simply subsampling noiseless data, we
add noise to the subsamples. In principle this could model a situation in which we
do not have access to the data except through noisy subsamples, but this is not a
common use-case.
Inthisexperimentwesimulatemakingmultiplenoisymeasurementsonthesame
data set, which we use as our subsamples for our algorithm. We create a Swiss roll
dataset with 2000 points. Then, we create 200 simulated noisy measurements by
subsampling and injecting additive Gaussian noise to the subsamples. Unpacking
the steps of our algorithm, the first step is to apply Isomap, and we see that
there are two broad classes of resulting embeddings: one in which the Swiss roll is
successfully unrolled into a sheet, and one in which the Swiss roll remains coiled
up. See Figure 2. (This is what is expected; e.g., see [3].)
ThenextstepinthealgorithmistocalculateProcrustesdistancesbetweenthese
embeddings. Wehavevisualizedtheresultinggraphusingmultidimensionalscaling
MDS) in Figure 3. The two types of embeddings roughly divide into two major18 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
Figure 2. Some example outputs of Isomap applied to a noisy
Swiss roll dataset.
clusters, one consisting of unrolled sheets and one consisting of coiled up sheets.
The appearance of the two clusters indicate the lack of robustness in applying
Isomap to the Swiss roll with the current level of added noise. We also observe
that while most unrolled outputs are close together, the coiled versions can differ
greatly among themselves. (The unrolled outputs form a dense cluster while the
coiledoutputsdonot.) Thenextstepinthealgorithmistocalculatethepersistent
homology of each embedding to identify the cluster consisting of the embeddings
that have only very small (noise) loops in PH . This is precisely the cluster of the
1
unrolled outputs.
The final step is to align and average the embeddings in the unrolled cluster.
The final averaged embedding is illustrated in Figure 4.
7.2. Robustness in presence of ambient noise concentrated around the
manifold. In this experiment we simulate sampling from a data set corrupted by
ambient noise. We again create a Swiss roll dataset with 2000 points. Then, we
injectadditiveGaussiannoisetothisdataset,sothatIsomapproducesacorrupted
coiled output when run on the whole data set. See Figure 5. When we randomly
subsample this dataset to obtain 500 samples of 1000 points each, for almost all
of these samples the result of Isomap is a coiled embedding; only a handful are
unrolled into a sheet. See Figure 6.
ThenextstepinthealgorithmistocalculateProcrustesdistancesbetweenthese
embeddings and cluster. In this case, there are many clusters, but calculating the
persistent homology of each embedding identifies a dense cluster consisting of the
embeddings that have only very small (noise) loops in PH , in contrast to most of
1
the clusters which are diffuse and have significant loops in PH . See Figure 7.
1
Finally, we align and average the embeddings in the unrolled cluster. The final
averaged embedding is illustrated in Figure 8.RESAMPLING AND AVERAGING COORDINATES ON DATA 19
Figure 3. An MDS representation of the Procrustes distances
between Isomap outputs of 200 noisy Swiss rolls: each point rep-
resents an output, and the plot is shaded according to density of
thepoints. Somerepresentativepointsareindicatedtogetherwith
their corresponding Isomap embedding.
Figure4. TheProcrustes-alignedIsomapembeddingcorrespond-
ing to the cluster of unrolled Swiss rolls.
7.3. Robustness in presence of outliers. In these experiments, we imagine a
single dataset of measurements where one or more bad sample points (“outliers”)
are included. We work again with the Swiss roll. Even a single adversarial outlier
can cause a short-circuit in the inferred connectivity of the manifold, and the re-
sulting Isomap embedding will fail to unroll the Swiss roll. See Figure 9, parts a
and b. When we randomly subsample this dataset to obtain 200 samples of 600
points each, most of these samples do not contain the outlier, and in these cases20 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
(a) The noisy swiss roll. (b) Isomap does not return the correct
embedding.
Figure 5. Additive Gaussian noise in the ambient space.
Figure 6. AllbutafewofthesubsamplesresultincoiledIsomap
embeddings.
Isomap manages to unroll the Swiss roll. See Figure 9, part c. In some instances,
Isomap unrolls the roll even if the outlier is included in the subsample if the other
non-outlier points involved in the short-circuiting are omitted. Finally, we align all
theoutputsinthegoodclusterandcomputethecentroid: theresultisasuccessfully
unrolled Swiss roll. See Figure 9, part d.
The previous experiment illustrates the need for some kind of robustification
(in this case by taking subsamples) even in the presence of a single outlier. The
next experiment studies the more expected case where there are multiple outliers.
In this experiment, we add 100 (5%) outliers chosen uniformly from a bounding
box. As in the single outlier case, we subsampled the dataset 200 times to obtainRESAMPLING AND AVERAGING COORDINATES ON DATA 21
(a) Oneofthefewsubsamplesresulting (b)Thepersistencediagramhasasingle
in an unrolled coordinate chart. large class in PH and no large classes
0
in PH .
1
(c) A generic subsample resulting in a (d)Thepersistencediagramhasasingle
coiled coordinate chart. large class in PH but also has a very
0
large class in PH capturing the loop.
1
Figure 7. Comparing clusters in Procrustes space using persis-
tent homology.
samples of size 600 and ran Isomap on these subsamples. With this many outliers,
the coiled outputs are in the majority. We clustered the outputs and identified
a cluster of uncoiled outputs using persistent homology. Then, we computed the
Procrustesalignmentandaveragedoverthesubsetofthesegoodoutputs,producing
the expected unrolled Swiss roll. See Figure 10.
These experiments demonstrate that our procedure is robust to outliers, in the
sense that the output in the presence of a small percentage of outliers is a low-
distortion unrolled embedding of the type we expect from Isomap applied to noise-
less samples from the underlying manifold.
7.4. Parameter variation. All manifold learning algorithms have hyperparame-
ters that have to be chosen by the practitioner. In the case of Isomap, there is a
single parameter corresponding to neighborhood size that needs to be judiciously
chosen in order to obtain good results. The next experiment illustrates that under22 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
Figure8. TheProcrustes-alignedIsomapembeddingcorrespond-
ing to the cluster of unrolled Swiss rolls.
(a) Original data in 3D (b) Isomap result
(c) Isomap applied to subsamples (d) Final aligned output
Figure 9. RobustifyingtheoutputofIsomap(singleoutliercase)
the hypothesis that the data represents a convex subset of a low dimensional man-
ifold embedded in a higher dimensional manifold, we can detect a cluster of good
parameters using our persistent homology methodology.RESAMPLING AND AVERAGING COORDINATES ON DATA 23
(a) Original data in 3D (b) Isomap result
(c) Isomap applied to subsamples (d) Final aligned output
Figure 10. Many outliers
In this experiment, we apply Isomap to the 2000 point Swiss roll synthetic data
setunderawiderangeofparameters. WehavegraphedtheheatmapofProcrustes
distances of the resulting embeddings in Figure 11. We see that at small radii,
Isomap fails to find a good embedding because the points are not connected (these
have high rank large bars in PH ), whereas at large radii, the embeddings “short
0
circuit” in the normal direction of the manifold, leading to coiled representations
(these have a large bar in PH ). Between these extremes, there is a range of
1
values that lead to good embeddings (with no large bars in PH ). Moreover,
1
these embeddingsare closetogether inProcrustes distances. Withinthis range, we
recover the expected unrolled embedding of the Swiss roll dataset. Furthermore,
thereisasharptransitionbetweenunrolledandcoileduprepresentations,asshown
by the large Procrustes distance between each embedding in the unrolled cluster
and each embedding in the coiled cluster. The figure illustrates four clusters in
total: the first box shows the cluster of small radii; the next 19 boxes show the
clusterofunrolledembeddings; thenextsingleboxshowsatransitionalembedding
far from both the unrolled embeddings and the coiled embeddings; and finally, the
last nine boxes show the cluster of coiled embeddings. A more careful analysis of
the distances (which we omit here) reveals that they correspond to the size of the
gapbetweensheetsoftheroll;thisiscloselyrelatedtothe“conditionnumber”and
reach of the manifold [23].
7.5. Detectingfailureofembeddingmethodology. Thenextexperimentstud-
ies a case contrary to the general hypothesis that the data comes from a convex
subset of a low dimensional manifold. In this experiment, we use the vertices of
buckminsterfullereneor“buckyballs”forthebasicdata. Abuckyballisamolecule
consistingof60carbonatomsarrangedonasphere. Itsbondsformshexagonaland24 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
Figure 11. TheProcrustesdistancematrixbetweenIsomapout-
putsforvaryingneighborhoodsizeparameters,increasingfromleft
toright. Theoutputembeddingsareshownalongthebottom,and
thetriangleheatmapshowsthedistances: thedarkerthecolor,the
smaller the Procrustes distance. (To compare two outputs: inter-
sect the 45◦ lines emanating from them to find the box on the
heatmap illustrating their Procrustes distance.)
(a) Original data in 3D (b) Isomap of subsamples (c) MDS embedding
Figure 12. Buckyball dataset
pentagonal rings resembling the surface of a soccer ball. See Figure 12, part a. As
a proxy for the sphere S2, the positions of the atoms of a buckyball in 3D cannot
be faithfully compressed down to two dimensions, unlike the unrolling of the Swiss
roll.
We computed the Isomap outputs of 500 noisy buckyballs, and the Procrustes
distances between them. See Figure 12, part b. We visualized these distances
using MDS in Figure 12, part c; however, in this case the 2-dimensional rendering
obscuresratherthanilluminatesthegeometricstructureoftheProcrustesgraphof
Isomap outputs (as we explain below). Nevertheless, it illustrates enough to show
thatunlikeintheSwissrollstudies,therearenodenseclustersofembeddingsinthis
experiment, and so we cannot expect an averaging procedure to yield a reasonable
result.RESAMPLING AND AVERAGING COORDINATES ON DATA 25
Figure 13. Persistence diagrams for the Procrustes graph of
Isomap outputs for the buckyball dataset
Although not directly related to our algorithm, we can say more about the geo-
metricstructureoftheIsomapoutputs. Wenotethatthetypicaloutputisapproxi-
matelyaflattened sphere: thedimensionalityreductionalgorithmisapproximately
linearly projecting the buckyball one dimension down. Thus, after centering the
buckyballandtheresultingIsomapoutput,weareledtoconsiderthespaceV (R3)
2
of(orthonormal)2-framesinR3,correspondingtothebasisvectorsinR3 thatspan
the plane we are projecting to. Procrustes alignment quotients out the isometries
O(2)ofthisplane,andtheProcrustesgraphofoutputsofIsomaponthebuckyball
dataset should approximate V (R3)/O(2)∼ =RP2, the real projective plane.
2
Wecantesttheconclusionofthisthoughtexperimentusingpersistenthomology.
For the projective plane, using coefficient field F , p prime,
p
(cid:40)
F , if p=2
H (RP2;F )∼ =H (RP2;F )∼ = 2
1 p 2 p
0, if p>2.
Computingthepersistenthomologyforthepointcloudconsistingofthe500Isomap
outputs with the Procrustes metric with Ripser [31], we find persistent classes in
PH (RP2;F ) and PH (RP2;F ), but not in PH (RP2;F ) for p = 3 or 5. See
1 2 2 2 ∗ p
Figure13. Inotherwords,thepersistenthomologysignatureoftheIsomapoutputs
matchesthepredictionthatthelandscapeofIsomapoutputsatleasthomotopically
is an approximation of RP2.
7.6. Divide and conquer for very large data sets. Manifold learning algo-
rithmstendtohavesuperlineartimecomplexity. Thissuggeststhatusingadivide-
and-conquerapproachonlargedatasetsmayresultintimesavings. Theideaisthe
same as before: subsample the dataset, run the algorithm on the individual sub-
samples, and then use the Procrustes alignment to merge the results. A schematic
of this process is shown in Figure 14.
8. Analysis of real data coming from single-cell genomics
The purpose of this section is to explore an application of our algorithm to
real data. We use two data sets as examples where coordinates in dimensionality-
reduced genomic space allows us to determine cell types. The first such data set
comes from blood cells (PBMC) and the second from mouse neural tissue (Tabula
muris). In these applications, in addition to Isomap, we use four other manifold
learning(dimensionalityreduction)algorithms: PCA,Laplacianeigenmaps,t-SNE,26 A.J.BLUMBERG,M.CARRIE`RE,J.H.FUNG,ANDM.A.MANDELL
Figure 14. Starting with a large dataset, repeatedly subsample
thedatasettoobtainmultiplesubsamplesofsmallersize(herethe
subsamples are indicated in red). Then, apply the manifold learn-
ingalgorithmtoeachsubsampleandusetheProcrustesalignment
to average the resulting embeddings.
and UMAP. In each case, we look at coordinates resulting from applying just the
dimensionality reduction algorithm (“base case”) and our algorithm (“robustified
case”).
We see interesting new phenomena in these applications. For one thing, using
the manifold learning algorithms PCA, Laplacian eigenmaps, and Isomap with our
algorithm, we see a single large tight cluster in the Procrustes graph of the sub-
samples. However, unlike the charts for the synthetic Swiss roll examples, both
the good and the bad embeddings have trivial PH , our indicator for contractibil-
1
ity. Instead, what distinguishes the good from the bad embeddings is how evenly
spread they are. We used embeddings in R2 and the bad embeddings are ones con-
centrated along a single axis, being approximately one-dimensional, whereas the
good embeddings have extent in both dimensions. For both theoretical and practi-
cal reasons (as explained in the discussion of the algorithm), the embeddings that
are essentially one-dimensional need to be discarded.
Ontheotherhand,usingt-SNEandUMAP,theProcrustesgraphofthesubsam-
plesarecomparativelyveryspreadoutandformasinglelargebutextremelyspread
outcluster, indicatingthattheaveragingprocedureinouralgorithmcannotbeex-
pected to output reasonable coordinates. This dichotomy is not surprising since
the PCA, Laplacian eigenmaps, and Isomap algorithms strive to perform global
alignment of the local coordinates, whereas the t-SNE and UMAP algorithms are
designed to separate local clusters. (This dichotomy of behavior is a topic of in-
terest in the genomics methodology community, see for example [30] for a critical
review of the use of these kinds of dimensionality reduction to produce pictures.)
8.1. 3k PBMCs scRNA-sequencing. The first dataset is a preprocessed ver-
sionofsinglecellRNA-sequencingdatafrom3kperipheralbloodmononuclearcells
(PBMCs)fromahealthydonor,madefreelyavailableby10xGenomics. Comprised
ofcellsfromavarietyofmyeloidandlymphoidlineages,thedatasethasrichclusterREFERENCES 27
and continuous structure, making it a useful dataset to benchmark new bioinfor-
matic methods. It can be accessed using the scanpy Python package [35] via the
following command:
scanpy.datasets.pbmc3k processed
As is typical in the visualization of scRNA-sequencing data, PCA is used as
a preliminary step prior to constructing the 2D embeddings: we reduce the raw
data to the first 50 principal components. For the base case, we then applied each
of the manifold learning algorithms to reduce the coordinates in R50 to R2. For
the robustified case, we take 50 subsamples of 500 cells, and proceed with our
algorithm with each of the manifold learning algorithms used for dimensionality
reduction from R50 to R2.
Figure 15 shows the MDS plots of the Procrustes distances between the sub-
sample embeddings along with histograms of those values under each of the five
manifold learning algorithms. We can see that PCA, Laplacian eigenmaps, Isomap
havemuchmoreconcentratedProcrustesdistances. AndwhenwelookattheMDS
plot of the Procrustes distances, we see there are tight clusters of the embeddings,
whereas the embeddings for t-SNE and UMAP are significantly more spread out
(with a diameter over 4 times as large). The algorithm then proceeds for PCA,
Laplacian eigenmaps, Isomap and terminates with an error of no remaining clus-
ters for t-SNE and UMAP.
ForPCA,Laplacianeigenmaps,Isomapwefindthatthemainclusterhasembed-
dings that are essentially two-dimensional; the outlying points are the embeddings
that are essentially one-dimensional. In Figures 16 and 17 we plot the subsample
embeddingsforPCAandIsomap. (Therelativelylargernumberofbadembeddings
for PCA in Figure 16 versus Isomap in 17 is consistent with the larger number of
embeddings outside the tight cluster as pictured in Figure 15.) We select the good
cluster and apply averaging procedure to the subsample embeddings it contains.
The output is robustified coordinates for PCA, Laplacian eigenmaps, and Isomap.
ThisisillustratedinFigure18togetherwiththeembeddingsobtainedontheentire
data set. In the figure, the data points are colored by cell type.
8.2. The mouse brain from Tabula muris. The second dataset is another sin-
gle cell RNA-sequencing dataset, this time coming from the Tabula muris project
[10], sequenced using the Smart-seq2 protocol, which we downsample to the 7,249
non-myeloid cells in the brain tissue sample. According to the provided cell type
annotation, this sample contains a mixture of neurons as well as glial cells. We
perform the same experimental protocol as above on the PBMC data set, and find
similarconclusions,exceptthatinthiscaseIsomapgivesasignificantlymorediffuse
cluster in Procrustes space than PCA or Laplacian eigenmaps, and seems to give
as poor a performance as t-SNE and UMAP. See figures 19 and 20.
References
[1] Jose Manuel Andrade et al. “Procrustes rotation in analytical chemistry, a
tutorial”. In: Chemometrics and Intelligent Laboratory Systems 72.2 (2004),
pp. 123–132. doi: https://doi.org/10.1016/j.chemolab.2004.01.007.
[2] AngelaAndreellaandLivioFinos.“ProcrustesAnalysisforHigh-Dimensional
Data”.In:Psychometrika 87.4(2022),pp.1422–1438.doi:10.1007/s11336-
022-09859-5.28 REFERENCES
(a) PCA (b) Isomap
(c) Laplacian eigenmaps. (d) t-SNE.
(e) UMAP.
Figure 15. Procrustes distances between subsample embeddings
for the PBMC dataset.
Figure 16. Subsample embeddings from PCA.
[3] Mukund Balasubramanian and Eric L. Schwartz. “The Isomap Algorithm
and Topological Stability”. In: Science 295.5552 (2002), p. 7. doi: 10.1126/
science.295.5552.7a. url: https://www.science.org/doi/abs/10.
1126/science.295.5552.7a.REFERENCES 29
Figure 17. Subsample embeddings from Isomap.
(a) PCA (b) Isomap (c) Laplacian Eigen-
maps
Figure 18. Original and robustified embeddings for the PBMC
dataset.30 REFERENCES
(a) PCA (b) Isomap
(c) Laplacian eigenmaps. (d) t-SNE.
(e) UMAP.
Figure 19. Procrustes distances between subsample embeddings
for the TM dataset.
(a) PCA (b) Isomap (c) Laplacian Eigen-
maps
Figure 20. Original and robustified embeddings for the TM
dataset.REFERENCES 31
[4] JuanAntonioBalbuena,RaulMiguez-Lozano,andIsabelBlasco-Costa.“PACo:
ANovelProcrustesApplicationtoCophylogeneticAnalysis”.In:PLOS ONE
8.4 (2013), pp. 1–15. doi: 10.1371/journal.pone.0061048.
[5] Jos M. F. ten Berge. “Orthogonal Procrustes rotation for two or more matri-
ces”. In: Psychometrika 42.2 (1977), pp. 267–276.
[6] Jos M. F. ten Berge, Henk A. L. Kiers, and Jacques J. F. Commandeur.
“OrthogonalProcrustesrotationformatriceswithmissingvalues”.In:British
J. Math. Statist. Psych. 46.1 (1993), pp. 119–134. issn: 0007-1102. doi: 10.
1111/j.2044-8317.1993.tb01005.x. url: https://doi.org/10.1111/j.
2044-8317.1993.tb01005.x.
[7] Andrew J. Blumberg et al. “Robust Statistics, Hypothesis Testing, and Con-
fidence Intervals for Persistent Homology on Metric Measure Spaces”. In:
Found. Comput. Math. 14.4 (2014), pp. 745–789.
[8] Fred L. Bookstein. Morphometric Tools for Landmark Data: Geometry and
Biology.CambridgeUniversityPress,1992.doi:10.1017/CBO9780511573064.
[9] Jacques J. F. Commandeur. Matching configurations. DWSO Press, 1991.
[10] The Tabula Muris Consortium. “A single-cell transcriptomic atlas character-
izes ageing tissues in the mouse”. In: Nature 583 (2020), pp. 590–595. doi:
10.1038/s41586-020-2496-1.
[11] GarmtDijksterhuis.“Multivariatedataanalysisinsensoryandconsumersci-
ence:anoverviewofdevelopments”.In:Trends in Food Science & Technology
6.6 (1995), pp. 206–211. doi: 10.1016/S0924-2244(00)89056-1.
[12] C. A. Glasbey and K. V. Mardia. “A review of image-warping methods”.
In: Journal of Applied Statistics 25.2 (1998), pp. 155–171. doi: 10.1080/
02664769823151.
[13] Colin Goodall. “Procrustes Methods in the Statistical Analysis of Shape”.
In: Journal of the Royal Statistical Society: Series B (Methodological) 53.2
(1991), pp. 285–321. doi: 10.1111/j.2517-6161.1991.tb01825.x.
[14] J.C.Gower.“GeneralizedProcrustesanalysis”.In:Psychometrika40.1(1975),
pp. 33–51. doi: 10.1007/BF02291478.
[15] John C. Gower and Garmt B. Dijksterhuis. “Multivariate analysis of cof-
fee images: A study in the simultaneous display of multivariate quantitative
andqualitativevariablesforseveralassessors”.In:Quality and Quantity 28.2
(1994), pp. 165–184. doi: 10.1007/BF01102760.
[16] B.F. Green. “The orthogonal approximation of an oblique structure in fac-
tor analysis”. In: Psychometrika 17.4 (1952), pp. 429–440. doi: 10.1007/
BF02288918.
[17] DavidG.Kendall.“ShapeManifolds,ProcrusteanMetrics,andComplexPro-
jectiveSpaces”.In:Bulletin of the London Mathematical Society 16.2(1984),
pp. 81–121. doi: 10.1112/blms/16.2.81.
[18] S. P. Langron and A. J. Collins. “Perturbation Theory for Generalized Pro-
crustesAnalysis”.In:JournaloftheRoyalStatisticalSociety.SeriesB(Method-
ological) 47.2 (1985), pp. 277–284.
[19] Jan de Leeuw and Jacqueline Meulman. “A special jackknife for multidi-
mensional scaling”. In: Journal of Classification 3.1 (1986), pp. 97–112. doi:
10.1007/BF01896814.32 REFERENCES
[20] Francy Junio Gon¸calves Lisboa et al. “Much beyond Mantel: Bringing Pro-
crustes Association Metric to the Plant and Soil Ecologist’s Toolbox”. In:
PLOS ONE 9.6 (2014), pp. 1–9. doi: 10.1371/journal.pone.0101238.
[21] RongMaetal.“Principledandinterpretablealignabilitytestingandintegra-
tion of single-cell data”. In: Proceedings of the National Academy of Sciences
121.10 (2024). doi: 10.1073/pnas.2313719121.
[22] A.D.McLachlan.“Amathematicalprocedureforsuperimposingatomiccoor-
dinatesofproteins”.In:ActaCrystallographicaSectionA28.6(1972),pp.656–
657. doi: 10.1107/S0567739472001627.
[23] Partha Niyogi, Stephen Smale, and Shmuel Weinberger. “Finding the Ho-
mology of Submanifolds with High Confidence from Random Samples”. In:
Discrete and Computational Geometry 39 (1 2008), pp. 419–441.
[24] MichaelB.RichmanandStephenJ.Vermette.“TheuseofProcrustesTarget
Analysistodiscriminatedominantsourceregionsoffinesulfurinthewestern
U.S.A.” In: Atmospheric Environment. Part A. General Topics 27.4 (1993),
pp. 475–481. doi: https://doi.org/10.1016/0960-1686(93)90205-D.
[25] Peter H. Sch¨onemann. “A generalized solution of the orthogonal Procrustes
problem”.In:Psychometrika31.1(1966),pp.1–10.doi:10.1007/BF02289451.
[26] PeterH.Sch¨onemannandRobertM.Carroll.“Fittingonematrixtoanother
underchoiceofacentraldilationandarigidmotion”.In:Psychometrika 35.2
(1970), pp. 245–255. doi: 10.1007/BF02291266.
[27] Robin Sibson. “Studies in the Robustness of Multidimensional Scaling: Per-
turbational Analysis of Classical Scaling”. In: Journal of the Royal Statisti-
cal Society. Series B (Methodological) 41.2 (1979), pp. 217–229. url: http:
//www.jstor.org/stable/2985036.
[28] Inge S¨oderkvist. “Perturbation analysis of the orthogonal Procrustes prob-
lem”. In: BIT 33.4 (1993), pp. 687–694. issn: 0006-3835. doi: 10.1007/
BF01990543. url: https://doi.org/10.1007/BF01990543.
[29] G. W. Stewart. “Perturbation theory for the singular value decomposition”.
UMIACS-TR-90-124. 1998.
[30] Chari T. and Pachter L. “The specious art of single-cell genomics”. In: PLoS
Comput Biol 19.8 (2023). url: https://doi.org/10.1371/journal.pcbi.
1011288.
[31] Christopher Tralie, Nathaniel Saul, and Rann Bar-On. “Ripser.py: A Lean
Persistent Homology Library for Python”. In: The Journal of Open Source
Software 3.29 (2018), p. 925. doi: 10.21105/joss.00925.
[32] PauliVirtanenetal.“SciPy1.0:FundamentalAlgorithmsforScientificCom-
putinginPython”.In:NatureMethods 17(2020),pp.261–272.doi:10.1038/
s41592-019-0686-2.
[33] ChangWangandSridharMahadevan.“ManifoldalignmentusingProcrustes
analysis”. In: Proceedings of the 25th International Conference on Machine
Learning. 2008, pp. 1120–1127. doi: 10.1145/1390156.1390297.
[34] ChaolongWangetal.“ComparingSpatialMapsofHumanPopulation-Genetic
VariationUsingProcrustesAnalysis”.In:Statistical Applications in Genetics
and Molecular Biology 9.1 (2010). doi: doi:10.2202/1544-6115.1493.
[35] F. Alexander Wolf, Philipp Angerer, and Fabian J. Theis. “SCANPY: large-
scale single-cell gene expression analysis”. In: Genome Biology 19.15 (2018).
doi: 10.1186/s13059-017-1382-0.REFERENCES 33
Irving Institute for Cancer Dynamics, Departments of Mathematics and Computer
Science, Columbia University, NY
Email address: andrew.blumberg@columbia.edu
DataShape, Centre Inria d’Universite´ d’Azur, Biot, France
Email address: mathieu.carriere@inria.fr
Department of Systems Biology, Columbia University, NY
Email address: jf3380@cumc.columbia.edu
Department of Mathematics, Indiana University, IN
Email address: mmandell@iu.edu