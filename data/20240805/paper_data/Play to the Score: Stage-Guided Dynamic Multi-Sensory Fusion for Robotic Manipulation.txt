Play to the Score: Stage-Guided Dynamic
Multi-Sensory Fusion for Robotic Manipulation
RuoxuanFeng1 DiHu1,* WenkeMa2 XuelongLi3
1GaolingSchoolofArtificialIntelligence,RenminUniversityofChina
2ShenzhenTaoboticsCo.,Ltd.
3InstituteofArtificialIntelligence(TeleAI),ChinaTelecom
{fengruoxuan, dihu}@ruc.edu.cn, wenke@taobotics.com, xuelong li@ieee.org
Abstract:Humanspossessaremarkabletalentforflexiblyalternatingtodifferent
senses when interacting with the environment. Picture a chef skillfully gauging
thetimingofingredientadditionsandcontrollingtheheataccordingtothecolors,
sounds, and aromas, seamlessly navigating through every stage of the complex
cookingprocess. Thisabilityisfoundeduponathoroughcomprehensionoftask
stages,asachievingthesub-goalwithineachstagecannecessitatetheutilization
ofdifferentsenses. Inordertoendowrobotswithsimilarability,weincorporate
thetaskstagesdividedbysub-goalsintotheimitationlearningprocesstoaccord-
ingly guide dynamic multi-sensory fusion. We propose MS-Bot, a stage-guided
dynamic multi-sensory fusion method with coarse-to-fine stage understanding,
which dynamically adjusts the priority of modalities based on the fine-grained
state within the predicted current stage. We train a robot system equipped with
visual, auditory, andtactilesensorstoaccomplishchallengingroboticmanipula-
tiontasks: pouringandpeginsertionwithkeyway. Experimentalresultsindicate
thatourapproachenablesmoreeffectiveandexplainabledynamicfusion,aligning
morecloselywiththehumanfusionprocessthanexistingmethods.
Keywords: Multi-Sensory,RoboticManipulation,Multi-Stage
1 Introduction
Humans are blessed with the ability to flexibly use various sensors to perceive and interact with
the world. Over the years, endowing robots with this potent capability has remained a dream for
humanity. Fortunately, with the advancement of computing devices and sensors [1, 2], building a
multi-sensoryrobotsystemcapableofassistinghumanshasgraduallybecomeachievable. Wehave
witnessedtheemergenceofsomeexcitingmulti-sensoryrobots,suchasOptimusandFigure01.
One challenge in tasks that robots assist humans with is the complex object manipulation task,
which requires achieving a series of sub-goals. These sub-goals naturally divide a complex task
intomultiplestages. Manyeffortsattempttomakemodelscomprehendthesetaskstages,whether
through hierarchical learning [3, 4, 5] or employing LLMs [6, 7, 8]. However, when incorporated
intomulti-sensoryrobots,thischallengebecomesmoreprofound. Weneedtonotonlyunderstand
thestagesthemselvesbutalsorethinkmulti-sensoryfusionfromthefreshperspectiveoftaskstages.
Completing sub-goals within a complex task may require different senses, with varying modality
importance at each stage. Thus, stage transitions may entail changes in modality importance. To
illustratethis,webuildamulti-sensoryrobotsystem(Fig.1,left). Wethenuseimitationlearningto
trainaMULSA[9]modelwithself-attentionfusiontocompletepouringtaskandrecordthetesting
confidence (Fig. 1, right). We observe correlations between the confidence and task stages after
maskingonemodality. Forexample,visionismostimportantinthealigningstage,andmaskingit
causes a notable confidence drop. We also note minor modality confidence changes inside stages.
*Correspondingauthor.“Playtothescore”isslangforadjustingactionstofitcurrentcircumstances.
4202
guA
2
]OR.sc[
1v66310.8042:viXraMulti-Sensory Robot System Confidence Variation in Pouring Task
Confidence LowConfidenceFluctuations!
Conf (aaaaaaaaaaaaa)
Conf(aaXaaaaaaaaaaa)
Conf (aaaaaaXaaaaaaa)
Conf(aaaaaaaaaaXaaa)
Audio
Vision
ASt lia gg ne i n1 g: StaS rt ta Pg oe u2 r: ing HoS ldta inge g 3 S: till EnS dt a Pg oe u 4 r: ing Time Touch
Vision(RGB,Depth) Audio Touch
Figure 1: An illustration for our multi-sensory robot system and a visualization of Modality
Temporalityinamulti-stagetask: pouring. Weshowconfidenceinactionpredictionwhenusing
theinputsofallmodalitiesandselectivelymaskinguni-modalfeatures. Duetothechangingimpor-
tance of modalities, both evident inter-stage (coarse-grained) and minor intra-stage (fine-grained)
changesinconfidenceareobservedwhenmaskinguni-modelfeatures. Thelowconfidencefluctua-
tionsnearstageboundariesalsoreflectinsufficienttaskstageunderstanding.
Theseindicatethatmodalityimportanceundergoesbothinter-stagechangesandintra-stageadjust-
ments.Wesummarizethisasachallengeinmulti-sensoryimitationlearning:ModalityTemporality.
Thekeytoaddressingthischallengeisunderstandingthecurrenttaskstageanditsconcreteinternal
state,i.e.,coarse-to-finestagecomprehension. Multi-sensoryfeaturesshouldbedynamicallyfused
tomeettheneedsofthecoarse-grainedstageandadjusttothefine-grainedinternalstate. However,
mostoftheexistingworksthatutilizemultiplesensors[10,11,12,13]essentiallyusestaticfusion
likeconcatenating. Theylacktheadaptabilitytodynamicchangesofmodalityimportanceincom-
plexmanipulationtasks. Somerecentworks[14,9]attempttouseattentionfordynamicfusion,but
theystillfailtobreakouttheparadigmoffusionbasedsolelyoncurrentobservations. Theyneglect
theimportanceofstageunderstandinginmulti-sensoryfusion.
Inlightofthiskeyinsight,weincorporatethestagesdividedbysub-goalsintomulti-sensoryfusion.
To perform coarse-to-fine stage understanding, we first encode current observations and historical
actionsintoastatetokenandidentifythecurrentstagebasedonit. Wetheninjectthestageinfor-
mationintothestatetokenbyweightinglearnablestagetokens. Forfusion,weproposeadynamic
Multi-Sensory fusion method for roBot (MS-Bot) to allocate modality weights based on the state
withincurrentstage. Usingcross-attention[15]withstage-injectedstatetokenasquery,wedynam-
icallyselectmulti-sensoryfeaturesofinterestbasedonthefine-grainedinternalstate. Thedynamic
allocationofattentionscoretofeaturetokensensureseffectivefusionalignedwithrequirements.
To evaluate our MS-Bot method in the real world, we build a robot system that can perceive the
environment with audio, visual and tactile sensors. We test it on two challenging manipulation
tasks: pouring,wheretherobotneedstopourtinybeadsofspecificquality,andpeginsertionwith
keyway,whererobotneedstorotatethepegwithakeytoalignthekeywayduringtheinsertion.
Tosummarize,ourcontributionsareasfollows:
• We summarize Modality Temporality as a key challenge and identify the coarse-to-fine stage
comprehensionasacrucialpointtohandlethechangingmodalityimportanceincomplextasks.
• Based on the above insight, we propose a dynamic multi-sensory fusion method guided by the
predictionoffine-grainedstatewithintheidentifiedtaskstage.
• Webuildamulti-sensoryrobotsystemandevaluateourmethodontwochallengingroboticma-
nipulationtasks: pouringandpeginsertionwithkeyway.
Experimental results show that MS-Bot better comprehends the fine-grained state in current task
stage and dynamically adjusts modality attention as needed, outperforming prior methods in both
tasks.Wehopethisworkopensupanewperspectiveonmulti-sensorfusion,inspiringfutureworks.
22 RelatedWork
Multi-Sensory Robot Learning. Recent works have extensively integrated visual, auditory, and
tactilesensorsintorobotsystems. Thetwocommonvisualmodalities,RGBanddepth,arewidely
combinedandappliedingrasping[16,17,18,19],navigation[20,21],andposeestimation[22,23,
24] tasks. In recent years, the development of tactile sensors has sparked interest in the study of
tactilemodality. Fine-grainedtactiledatacanassistintaskssuchasgrasping[25,26,10,11],shape
reconstruction [27, 28] and peg insertion [29, 13]. Audio modalities are typically applied in tasks
such as pouring [9, 12], object tracking [30, 31], and navigation [32, 33, 34]. Some works even
attempttojointlyusevisual,audio,andtactilemodalitiestomanipulateobjects[35,36,9]. Based
ontheseefforts,wefurtherexploreeffectivemulti-sensoryfusionincomplexmanipulationtasks.
Robot Learning for Multi-Stage Tasks. Complex multi-stage tasks have always been a focal
point of researchin robot manipulation. Previousworks hierarchically decompose the multi-stage
tasks,whereupper-levelmodelspredictthenextsub-goal,andlower-levelnetworkspredictspecific
actions based on the sub-goal. Following this paradigm, many hierarchical reinforcement learn-
ing[5,37,38,39,40,41]andimitationlearning[3,4,42,43,44]methodshavedemonstratedtheir
effectivenessacrossvarioustasks. [6,7,8]useLLMsorVLMstodecomposelong-horizontasks.
Recently,[45]introducesChain-of-Thoughtintorobotcontrol. Whiletheseworksprimarilyfocus
onuni-sensoryscenarios,weaimtoinvestigatetheimpactofstagechangesonmulti-sensoryfusion.
DynamicMulti-ModalFusion. Dynamicmulti-modalfusionessentiallyadaptsthefusionprocess
basedontheinputdatabyassigningdifferentweightstotheuni-modalfeatures. Asimpleandef-
fectivewaytoassignweightsistotrainagatenetworkthatscoresbasedonthequalityofuni-modal
features[46,47,48,49]. Anotherapproachtoassessingthequalityofmodalitiesanddynamically
fusingthemisuncertainty-basedfusion[50,51,52]. Withtheintroductionofthetransformer[15],
self-attentionprovidesanaturalmechanismtoconnectmulti-modalsignals[53]. Hence,attention-
basedfusionmethodsrapidlybecomeafocalpointofresearch[9,54,55]. However,thesemethods
primarilyperformdynamicfusionbasedsolelyontheinputsthemselves,whilewedesigndynamic
fusionfromtheperspectiveoftaskstagesinroboticscenarios.
3 Method
3.1 ChallengesinMulti-SensoryImitationlearning
Imitation learning is a data-efficient robot learning algorithm where a robot learns a task by mim-
icking the behavior demonstrated by an expert. However, directly applying it to complex tasks
with multi-sensory data could encounter issues, such as low confidence fluctuations and changing
modalityimportanceshowninFig.1. Weattributethecausesoftheseissuestotwokeychallenges:
Non-Markovity. Training data for imitation learning is typically derived from human demonstra-
tions, usually collected through teleoperation devices. However, when humans manipulate robots,
theiractionsarenotsolelybasedondataobservedbyrobotsensors,butarealsoinfluencedbymem-
ory. Thisbecomesparticularlysignificantinmorecomplexmulti-stagetasks, asmemorycanpro-
videcuesaboutthetaskstages. Identifyingthecurrentstagesolelybasedoncurrentmulti-sensory
observations can be much more challenging. This challenge is not brought by the introduction of
multiplesensors. SomeworkshavealreadyrelaxedtheMarkovassumptionandutilizedactionhis-
tory as an additional input or leveraged sequence models to enhance performance [56, 57]. We
identifytheactionhistoryasacrucialcueforunderstandingthecurrentstageandthefine-grained
statewithinthestage,anduseittopredictstagesratherthandirectlypredictingthenextaction.
ModalityTemporality. Inacomplexmanipulationtask,theimportanceofvariousuni-modalfea-
turescouldchangeoverstages. Attimestepsfromdifferentstages,aparticularmodalitymaycon-
tribute significantly to the prediction, or serve as a supplementary role to the primary modality,
or provide little useful information. Moreover, different states within a stage, such as the begin-
ning and end, may also exhibit minor changes in modality importance. We distinguish them as
coarse-grainedandfine-grainedimportancechange.However,previousworkstypicallytreatedeach
modalityequallyanddidnotconsidertheissueofchangingmodalityimportanceinmethoddesign.
3Multi-Sensory Observations ActionHistory(Memory)
I’ve been moving
left for a long while
Feature Extractor
LSTM
Uni-modal Gate
Encoders (Softmax)
MLP
State
Feature Token
Tok 𝐟𝐭ens … … … ToS kt ea nt ie
zer
𝒛𝒕𝒔𝒕𝒂𝒕𝒆
DynamicFusion Stage Stage Score𝒈𝒕
Key,Value Comprehension
Stage 2
Query
It’s the beginning
Stage-Guided Stage-Guided Dynamic Fusion ofStage 2now! 3 1 2
Cross-Attention Importance Rank
×𝜷
Atte…… ntionS……
core
MLP
S Sta tat We g eT i t Io h nk fe on
×𝟏−𝜷𝒛𝒕𝒔𝒕𝒂𝒕𝒆 1
2 3 StL ae ga er Tn oa kb ele ns
(Prediction) 𝒛𝒕∗ 4 [𝒔𝒕𝒂𝒈𝒆]
Figure 2: The pipeline of our method MS-Bot. It consists of four parts: feature extractor, state
tokenizer,stagecomprehensionmodule,anddynamicfusionmodule.
This could lead to sub-optimal outcomes as uni-modal features with low quality could disrupt the
overallprediction. Onthecontrary,weutilizetaskstageinformationtoguidemulti-sensoryfusion,
dynamicallyassigningweightsofmodalitiesbasedonthefine-grainedstateinthecurrentstage.
3.2 Stage-GuidedDynamicMulti-SensoryFusionforRobot(MS-Bot)
Inthissection,weintroducehowourmethodMS-Botaddressestheabovechallenges. Considering
Non-Markovity,weuseactionhistoryasinputtoassistthemodelinassessingthecurrenttaskstate.
ForModalityTemporality,weestablishacoarse-to-finestagecomprehension,decomposingthetask
stateintocoarse-grainedstagesandfine-grainedstateswithinastage.Dynamicfusionguidedbythe
fine-grainedstatewithinthecurrentstageisemployedtohandlethechangingmodalityimportance.
Tointroducetheunderstandingofstagesintotheprocessofimitationlearning,weincorporatetask
stages divided by sub-goals into the training samples. Given a sample (X ,a ) at timestep t in a
t t
trajectory where X = {X1,X2,...,XM} is the observation of M modalities and a is the corre-
t t t t t
spondingaction,astagelabels ∈{1,2,...,S}isadded,whereSisthenumberofstagesinthetask
t
dividedbydifferentsub-goals.Thesampleattimesteptisthenrepresentedas(X ,a ,s ).Basedon
t t t
thesamplewithstagedivision,wethenintroducethefourmaincomponentsofourmethodMS-Bot:
Featureextractor. Thiscomponentconsistsofseveraluni-modalencoders. Eachencodertakesa
brief history of observations Xm ∈ RT×Hm×Wm×Cm of the modality m as input, where T is the
t
timestep number of the brief history and H ,W ,C indicates the input shape of modality m.
m m m
Theseobservationsarethenencodedintofeaturetokensf ∈RM×T×dwhereddenotesdimension.
t
State tokenizer. This component aims to encode the observations and action history
(a ,a ,...,a )intoatokenthatcanrepresentthecurrentstate. Actionhistoryissimilartohuman
1 2 t−1
memoryandcanhelptoindicatethecurrentstatewithinthewholetask. Weinputtheactionhistory
asaone-hotsequenceintoanLSTM[58],thenconcatenatetheoutputwiththefeaturetokensand
encodethemintoastatetokenzstatethroughaMulti-LayerPerceptron(MLP).
t
Stage comprehension module. Merely using the state token alone is insufficient for achieving a
comprehensive understanding of the task stages. This module aims to perform coarse-to-grained
stage understanding by injecting stage information into the state token. For a task with S stages,
4weuseS learnablestagetokens[stage ],...,[stage ]torepresenteachstage. Eachstagetokenis
1 S
initializedwiththemeanofallstatetokensonsampleswithinthestageafterwarmup. Next,weuse
a gate network (MLP) to predict the current stage, i.e., coarse-grained stage comprehension. The
S-dimensionaloutputscoresg ofthegatearesoftmax-normalizedandmultipliedbyeachoftheS
t
stagetokens,followedbysumminguptheresultstoobtainthestagetokenzstageattimesteptvia:
t
g=(g1,...,gS)=softmax(MLP(zstate)),
t t t
1 (cid:88)S (1)
zstage = (gj ·[stage ]).
t S t j
j=1
Weuseasoftmaxscoreinsteadofone-hotencodingbecausethetransitionofstagesisacontinuous
andgradualprocess. Finally,wecomputetheweightedsumofthestatetokenzstateandthecurrent
t
stagetokenzstageusingaweightβ toobtainthestage-injectedstatetokenz∗:
t t
z∗ =β·zstate+(1−β)·zstage. (2)
t t t
Different from the old state token zstate, the new state token z∗ represents the fine-grained state
t t
withinastage. Inthiscase,zstage isregardedasananchorstage,whilezstate canindicatetheshift
t t
insidethestage,therebyachievingcoarse-to-finestagecomprehension.
Duringthetrainingprocess,weutilizestagelabelstosupervisethestagescoresoutputbythegate
net. Specifically, we penalize scores that do not correspond to the current stage. Additionally, for
sampleswithinarangenearthestageboundaries,wedonotpenalizeitsscoreontheneareststage,
achievingasoftconstrainteffect. ThelossforthegatenetL onthei-thsampleisasfollows:
gate
S
(cid:88)
L = (wj ·gj), j ∈{1,2,...,S},
gate,i i i
j=1 (3)
(cid:26)
1, (s =j)or(∃k, |k−i|≤γ, s ̸=s ),
wj = i i k
i 0, otherwise,
wherekindicatesanearbysampleinthesametrajectory,s ands representstagelabelsandγ isa
i k
hyper-parameterusedtodeterminetherangenearthestageboundaries. Thissoftconstraintallows
forsmoothervariationsinthepredictedstagescores.
Dynamicfusionmodule. Duetovariationsinmodalityimportanceacrossdifferentstagesandthe
minorimportancechangeswithinastage,weaimforthemodeltodynamicallyselectthemodalities
of interest based on the fine-grained state within the current stage. We use the state token with
stage information z∗ as query, and the feature tokens f as key and value for cross-attention [15].
t t
This mechanism dynamically allocates attention scores to feature tokens based on the state token
z∗,therebyachievingdynamicmulti-sensoryfusionovertime. Italsocontributestoamorestable
t
multi-sensoryfusion,astheanchorzstage changesminimallywithinastage. Thefeaturesfromall
t
modalitiesareintegratedintoafusiontokenzfusbasedonthecurrentstage’srequirements. Finally,
t
thefusiontokenzfusisfedintoanMLPtopredictthenextactiona .
t t
Inordertopreventthemodelfromsimplymemorizingtheactionscorrespondingtoattentionscore
patterns,wealsointroducerandomattentionblurmechanism. Foreachinput,wereplacetheatten-
tionscoresonfeaturetokenswiththesameaveragevalue 1 withaprobabilityp. Weencourage
M×T
themodeltosimultaneouslylearnbothstageinformationandfeatureinformationinthefusiontoken.
The loss during training consists of two components: the classification loss for action prediction
L andthepenaltyonthescoresofthegatenetworkL . ThetotaltraininglossLisasfollows:
cls gate
L=L +λL , (4)
cls gate
whereλisahyper-parametertocontroltheintensityofthescorepenalty.
4 Experiments
Inthissection,weexploretheanswerstothefollowingquestionsthroughexperiments: (Q1)Com-
paredtopreviousmethods,howmuchperformanceimprovementdoesstage-guideddynamicmulti-
sensory fusion provide? (Q2) Why is stage-based dynamic fusion superior to fusion based solely
55cm
0.5cm
7cm
Cylinder with
90/120g beads initially
13cm Target
Peg with key Cylinder
Base with keyway
Peg Insertion with Keyway Pouring
Figure3: Anillustrationofthetasksetupforpeginsertionwithkeywayandpouringtask.
on the input itself? How does stage comprehension help? (Q3) Can stage-guided dynamic fusion
maintainitsadvantageoverstaticfusioninout-of-distributionsettings?
4.1 TaskSetup
Pouring. Forthepouringtask,therobotneedstopourtinysteelbeadsofspecificqualityfromthe
cylinderinthehandintoanothercylinder. Thistaskconsistsoffourstages[41]: 1)Aligning,where
therobotneedstomoveagraduatedcylindercontainingbeadsandalignitwiththetargetcylinder,
2) Start Pouring, where the robot rotates the end effector to pour out the beads at an appropriate
speed,3)HoldingStill,wheretherobotmaintainsitsposturetocontinuepouringoutbeadssteadily,
and4)EndPouring,wheretherobotrotatestheendeffectorattheappropriatetimetostoptheflow
ofbeads.Theinitialmassofthebeadsis90g/120g,whilethetargetmassforpouringoutis40g/60g,
bothindicatedbyprompts. Weuseaudio,vision(RGB)andtouchmodalitiesinthistask.
Peg Insertion with Keyway. This task is an upgraded version of peg insertion, where the robot
needstoalignapegwithakeytothekeywayonthebasebyrotatingandtheninsertthepegfully.
The alignment primarily relies on tactile feedback, as the camera cannot observe inside the hole.
Thistaskconsistsofthreestages:1)FirstInsertion,wheretherobotalignsthepegwiththeholeand
inserts it until the key collides with the base, 2) Rotating, where the robot aligns the key with the
keyway on the base by rotating the peg based on tactile feedback, and 3) Second Insertion, where
therobotfurtherinsertsthepegtothebottom. WeuseRGB,depthandtouchmodalitiesinthistask.
Fig.3brieflyshowsthetwotasks. SeeSec.Aforamoredetailedtasksetupforbothtasks.
4.2 PhysicalSetupandExperimentalDetails
Robot Setup and Data Collection. We use a 6-DoF UFACTORY xArm 6 robotic arm equipped
with a Robotiq 2F-140 gripper in all experiments. For both tasks, we generate Cartesian space
displacementcommandsatapolicyfrequencyof5Hz. WeuseanIntelRealSenseD435icamerato
recordvisualdata(RGBanddepth)witharesolutionof640×480atafrequencyof30Hz. Weuse
adesktopmicrophonetorecordaudioatasamplingrateof44.1kHz. Tactiledataarerecordedbya
GelSight[1]minisensorwitharesolutionof400×300andafrequencyof15Hz.
Baselines. Wecompareourmethodwiththreebaselinesinbothtasks: 1)theconcatmodelwhich
directly concatenates all the uni-modal features, 2) Du et al. [59] that uses LSTM to fuse the uni-
modal features and the additional proprioceptive information, and 3) MULSA [9] which fuses the
uni-modalfeaturesviaself-attention. Wealsocompareourmethodwithtwovariantsineachtask:
MS-Bot (w/o A/D) and MS-Bot (w/o T/R) where one modality in the task is removed (audio and
touchforpouringwhiledepthandRGBforpeginsertion).
4.3 Howmuchimprovementdoesstage-guideddynamicmulti-sensoryfusionprovide?
To evaluate how much improvement MS-Bot brings compared to previous methods, we conduct
comparativeexperimentsonthetwotasks. Foreachtask(consideringchanginginitialweightand
6PouringInitial(g) PouringTarget(g) Insertion
Methods
90 120 40 60 SuccessRate
Concat 4.80±1.14 8.72±2.39 8.40±2.21 6.54±2.14 5/10
Duetal.[59] 4.32±1.22 7.79±2.11 8.54±2.04 6.26±2.01 5/10
MULSA[9] 3.05±1.01 6.42±1.98 7.12±1.66 4.19±1.24 6/10
MS-Bot(w/oA/D) 10.55±2.25 15.76±3.18 15.64±3.25 13.02±3.13 5/10
MS-Bot(w/oT/R) 8.32±1.74 12.99±3.04 13.02±3.13 9.77±1.65 5/10
MS-Bot 1.60±1.10 5.58±1.79 6.48±1.55 1.80±0.95 8/10
Table1:Comparisonofperformanceonpouring(mean±standarddeviation)andpeginsertionwith
keyway(Q1). A:Audio,D:Depth,T:Touch,R:RGB.
Attention
Score
Audio
Vision
Touch
Time
Stage
Score Stage 1
Stage 2
Stage 1: Stage 2: Stage 3: Stage 4: Stage 3
Aligning Start Pouring HoldingStill EndPouring
Stage 4
Pouring Time
Figure4: Visualizationoftheaggregatedattentionscoresforeachmodalityandstagescoresinthe
pouringtask(Q2). Ateachtimestep, weaveragetheattentionscoresonallfeaturetokensofeach
modalityseparately. Thestagescoreistheoutputofthegatenetworkaftersoftmaxnormalization.
targetweightastwotasks),wecollect30humandemonstrationsanduse20ofthemasthetraining
set,leavingtheremaining10asthevalidationset. Forbothtasks,weconduct10real-worldtests.
Tab.1showstheperformanceofbaselinesandourmethodontwotasks. OurMS-Botoutperforms
allbaselinesinboththepouringtaskandpeginsertiontask,demonstratingthesuperiorityofstage-
guideddynamicmulti-sensoryfusion. Despiteusingproprioceptiveinformation, theimprovement
ofDuetal.[59]overtheconcatmodelisminimal,indicatingthattheinadequateunderstandingof
the coarse-grained and fine-grained modality importance change is the critical barrier. Moreover,
these two baselines exhibit delayed responses to stage changes, causing excessive pouring in the
pouring task and misalignment in the insertion task. MULSA employs self-attention for dynamic
fusion,butitslackofathoroughstageunderstandingpreventsitfromfullyleveragingtheadvantages
ofdynamicfusion.OurMS-Botachievesthebestperformancebybetterallocatingmodalityweights
duringfusionbasedontheunderstandingofthefine-grainedstatewithinthecurrentstage(Q1).
The notable performance drop when one modality is removed indicates that each modality con-
tributessignificantly.Inthepouringtask,removingaudioandtactilemodalitiesdegradesthemodel’s
abilitytoassessthequalityofpouredbeads.Theimpactismoresignificantwhentheaudiomodality
isremoved, correspondingtothenotabledecreaseinconfidenceobservedaftermaskingtheaudio
featuresinStage3asshowninFig.1. Inthepeginsertiontask,removingeithertheRGBordepth
modalitywilldegradethealignmentperformance.Itisworthnotingthatevenwithouttheassistance
ofthedepthorRGBmodality,ourMS-Botachievesanequalsuccessratecomparedtoconcatmodel
andDuetal.[59],demonstratingthesuperiorityofstagecomprehensionanddynamicfusion(Q1).
4.4 Whyisstage-baseddynamicfusionsuperiortofusionbasedsolelyontheinputitself?
To better understand the working principles of MS-Bot and the reasons for performance improve-
ment, we visualize the attention scores of each modality and the scores of the stage tokens. As
shown in Fig. 4, MS-Bot accurately predicts the rapid stage changes. Due to the model’s coarse-
7Pouring Insertion
Methods Overall Methods
Color Color Mess
MS-Bot 3.88±2.55 Concat 8.75±2.02 2/10 3/10
-AttentionBlur 4.01±2.54 Duetal.[59] 8.04±1.85 3/10 3/10
-StageComprehend 4.62±2.34 MULSA[9] 5.56±1.13 4/10 5/10
-StateTokenizer 5.19±2.21
MS-Bot 2.41±1.47 6/10 6/10
Table 2: Impact of each component of our Table3:Comparisonofmodelsinsceneswithvi-
methodinpouringtask(Q2).‘-’indicatesfurther sualdistractors(Q3). “Color”indicatesthecolor
removingthemodulefromthemodelinthepre- ofthecylinderorthebaseischangedand“Mess”
viousline. Overallerroronallsettingsisshown. indicatestherearesomeclutternearthebase.
to-fine task stage comprehension, the aggregated attention scores of the three modalities remain
relativelystable,exhibitingclearinter-stagechangesandminorintra-stageadjustment(Q2). Vision
initiallydominatesduringthefirststage.Uponenteringthesecondstage,themodelbeginstousethe
soundofbeadstofindtheappropriateangleanddistinguishesthisstagefromthelaststagethrough
touch. Duringtheholdingstillstage,themodelprimarilyreliesonaudioandtactiledeformationto
assessthemassofbeads. Inthefinalstage,themodeldiscernsthecompletionofthepouringbased
ontactiledeformationandtheriseoftactileattentionscoresisobserved. Thechangesinattention
scoresforthepeginsertiontask(seeFig.9b)alsofollowasimilarpattern,demonstratingthevalidity
andexplainabilityoftheproposedstage-guideddynamicmulti-sensoryfusion(Q2).
We also conduct ablation experiments on the pouring task to examine the contributions of each
module,asshowninTab.2.Weobservesignificantcontributionstoperformancefromboththestate
tokenizerandthestagecomprehensionmodule(Q2). Thesetwomodulesprovidecriticaltaskstate
understandingandcoarse-to-finestagecomprehension. Inaddition,removingrandomattentionblur
resultsinaminorperformancedecline. ThecompleteablationresultsarelocatedinTab.4.
4.5 Canstage-guideddynamicfusionmaintainitsadvantageinout-of-distributionsettings?
Toverifythegeneralizationofourmethodtodistractors, weconductexperimentswithvisualdis-
tractorsinbothtasks. Inthepouringtask,thecylinder’scolorwaschangedfromwhitetored. For
thepeginsertiontask,alterationsincludedchangingthebasecolorfromblacktogreen(referredto
as “Color”), and placing clutter around the base (referred to as “Mess”). As shown in Tab. 3, our
methodexhibitsminimalimpactacrossvariousscenarioswithdistractorsandconsistentlymaintains
performancesuperiority,demonstratingthegeneralizabilityofstagecomprehension(Q1,Q3). Two
baselinemethods,theconcatmodelandDuetal.[59],sufferfromperformancedegradationwhen
thevisualmodality(includingRGBanddepth)isdisturbed.Duetotheirlackofabilitytounderstand
thestagesanddynamicallyadjustmodalityweights,evenwheninformationfromvisualmodalityis
notneededinaparticularstage,themodelisstillaffectedbydistractors. Thisleadstounsatisfying
performance. Our method dynamically allocates modality weights based on the understanding of
thecurrentstage,therebyreducingtheimpactofvisualdistractorsonthefusedfeatures(Q2,Q3).
Consequently, it outperforms the two baseline methods using static fusion and the MULSA that
solelyreliesonthecurrentobservationfordynamicfusion.
5 ConclusionandLimitations
WepresentMS-Bot,astage-guideddynamicmulti-sensoryfusionmethodwithcoarse-to-finestage
comprehension, which dynamically focuses on relevant modalities based on the state within the
currentstage.Evaluatedonthechallengingpouringandpeginsertionwithkeywaytasks,ourmethod
outperformspreviousstaticanddynamicfusionmethods. Wefindthatstagecomprehensioncanbe
generalizedtosceneswithvariousdistractors,reducingtheimpactofinterferencefromonemodality
onmulti-sensoryfusion. Wehopeourworkcaninspirefutureworkonmulti-sensoryrobots.
Limitations: The stage division in this work involves human factors. A potential improvement
couldinvolveusingunsupervisedalgorithmsorleveragingLLMsforstagelabeling. Additionally,
proposingmorechallengingmulti-sensorymanipulationtaskswouldbeinterestingforfutureworks.
8References
[1] W. Yuan, S. Dong, and E. H. Adelson. Gelsight: High-resolution robot tactile sensors for
estimatinggeometryandforce. Sensors,17(12):2762,2017.
[2] P. Schmidt, J. Scaife, M. Harville, S. Liman, and A. Ahmed. Intel® realsense™ tracking
camerat265andintel®realsense™depthcamerad435-trackinganddepth. RealSense,2019.
[3] J.Luo,C.Xu,X.Geng,G.Feng,K.Fang,L.Tan,S.Schaal,andS.Levine. Multi-stagecable
routingthroughhierarchicalimitationlearning. IEEETransactionsonRobotics,2024.
[4] P. Sharma, D. Pathak, and A. Gupta. Third-person visual imitation learning via decoupled
hierarchicalcontroller. AdvancesinNeuralInformationProcessingSystems,32,2019.
[5] O.Nachum,S.S.Gu,H.Lee,andS.Levine.Data-efficienthierarchicalreinforcementlearning.
Advancesinneuralinformationprocessingsystems,31,2018.
[6] S.Belkhale,T.Ding,T.Xiao,P.Sermanet,Q.Vuong,J.Tompson,Y.Chebotar,D.Dwibedi,
and D. Sadigh. Rt-h: Action hierarchies using language. arXiv preprint arXiv:2403.01823,
2024.
[7] W.Huang,F.Xia,T.Xiao,H.Chan,J.Liang,P.Florence,A.Zeng,J.Tompson,I.Mordatch,
Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language
models. InConferenceonRobotLearning,pages1769–1782.PMLR,2023.
[8] P. Sermanet, T. Ding, J. Zhao, F. Xia, D. Dwibedi, K. Gopalakrishnan, C. Chan, G. Dulac-
Arnold, N. J. Joshi, P. Florence, et al. Robovqa: Multimodal long-horizon reasoning for
robotics. In2ndWorkshoponLanguageandRobotLearning: LanguageasGrounding,2023.
[9] H.Li,Y.Zhang,J.Zhu,S.Wang,M.A.Lee,H.Xu,E.Adelson,L.Fei-Fei,R.Gao,andJ.Wu.
See, hear, and feel: Smart sensory fusion for robotic manipulation. In Conference on Robot
Learning,pages1368–1378.PMLR,2023.
[10] Y. Han, K. Yu, R. Batra, N. Boyd, C. Mehta, T. Zhao, Y. She, S. Hutchinson, and Y. Zhao.
Learninggeneralizablevision-tactileroboticgraspingstrategyfordeformableobjectsviatrans-
former. arXivpreprintarXiv:2112.06374,2021.
[11] I. Guzey, B. Evans, S. Chintala, and L. Pinto. Dexterity from touch: Self-supervised pre-
trainingoftactilerepresentationswithroboticplay. InConferenceonRobotLearning,pages
3142–3166.PMLR,2023.
[12] H.Liang,C.Zhou,S.Li,X.Ma,N.Hendrich,T.Gerkmann,F.Sun,M.Stoffel,andJ.Zhang.
Robustroboticpouringusingauditionandhaptics.In2020IEEE/RSJInternationalConference
onIntelligentRobotsandSystems(IROS),pages10880–10887.IEEE,2020.
[13] L.Fu,H.Huang,L.Berscheid,H.Li,K.Goldberg,andS.Chitta.Safeself-supervisedlearning
inreal ofvisuo-tactilefeedbackpoliciesfor industrialinsertion. In 2023IEEE International
ConferenceonRoboticsandAutomation(ICRA),pages10380–10386.IEEE,2023.
[14] C.Sferrazza, Y.Seo, H.Liu, Y.Lee, andP.Abbeel. Thepowerofthesenses: Generalizable
manipulationfromvisionandtouchthroughmaskedmultimodallearning. 2023.
[15] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polo-
sukhin. Attention is all you need. Advances in neural information processing systems, 30,
2017.
[16] H.-S. Fang, C. Wang, M. Gou, and C. Lu. Graspnet-1billion: A large-scale benchmark for
generalobjectgrasping. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages11444–11453,2020.
9[17] S. Yu, D.-H. Zhai, Y. Xia, H. Wu, and J. Liao. Se-resunet: A novel robotic grasp detection
method. IEEERoboticsandAutomationLetters,7(2):5238–5245,2022.
[18] Y.Song,J.Wen,D.Liu,andC.Yu. Deeproboticgraspingpredictionwithhierarchicalrgb-d
fusion. InternationalJournalofControl,AutomationandSystems,20(1):243–254,2022.
[19] R.Qin,H.Ma,B.Gao,andD.Huang. Rgb-dgraspdetectionviadepthguidedlearningwith
cross-modal attention. In 2023 IEEE International Conference on Robotics and Automation
(ICRA),pages8003–8009.IEEE,2023.
[20] L. Wellhausen, R. Ranftl, and M. Hutter. Safe robot navigation via multi-modal anomaly
detection. IEEERoboticsandAutomationLetters,5(2):1326–1333,2020.
[21] C. Huang, O. Mees, A. Zeng, and W. Burgard. Visual language maps for robot navigation.
In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 10608–
10615.IEEE,2023.
[22] C. Xu, J. Chen, M. Yao, J. Zhou, L. Zhang, and Y. Liu. 6dof pose estimation of transparent
objectfromasinglergb-dimage. Sensors,20(23):6790,2020.
[23] M. Tian, L. Pan, M. H. Ang, and G. H. Lee. Robust 6d object pose estimation by learning
rgb-dfeatures. In2020IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
pages6218–6224.IEEE,2020.
[24] Y.He,H.Huang,H.Fan,Q.Chen,andJ.Sun. Ffb6d: Afullflowbidirectionalfusionnetwork
for6dposeestimation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages3003–3013,2021.
[25] S.Cui,R.Wang,J.Wei,J.Hu,andS.Wang. Self-attentionbasedvisual-tactilefusionlearning
forpredictinggraspoutcomes. IEEERoboticsandAutomationLetters,5(4):5827–5834,2020.
[26] S. Cui, R. Wang, J. Wei, F. Li, and S. Wang. Grasp state assessment of deformable objects
using visual-tactile fusion perception. In 2020 IEEE International Conference on Robotics
andAutomation(ICRA),pages538–544.IEEE,2020.
[27] E.Smith,R.Calandra,A.Romero,G.Gkioxari,D.Meger,J.Malik,andM.Drozdzal.3dshape
reconstructionfromvisionandtouch.AdvancesinNeuralInformationProcessingSystems,33:
14193–14206,2020.
[28] Y.Wang,W.Huang,B.Fang,F.Sun,andC.Li.Elastictactilesimulationtowardstactile-visual
perception. InProceedingsofthe29thACMInternationalConferenceonMultimedia, pages
2690–2698,2021.
[29] M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei, A. Garg, and J. Bohg.
Makingsenseofvisionandtouch: Self-supervisedlearningofmultimodalrepresentationsfor
contact-rich tasks. In 2019 International Conference on Robotics and Automation (ICRA),
pages8943–8950.IEEE,2019.
[30] J.WilsonandM.C.Lin. Avot: Audio-visualobjecttrackingofmultipleobjectsforrobotics.
In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 10045–
10051.IEEE,2020.
[31] X. Qian, Z. Wang, J. Wang, G. Guan, and H. Li. Audio-visual cross-attention network for
roboticspeakertracking. IEEE/ACMTransactionsonAudio,Speech,andLanguageProcess-
ing,31:550–562,2022.
[32] C. Gan, Y. Zhang, J. Wu, B. Gong, and J. B. Tenenbaum. Look, listen, and act: Towards
audio-visual embodied navigation. In 2020 IEEE International Conference on Robotics and
Automation(ICRA),pages9701–9707.IEEE,2020.
10[33] C. Chen, U. Jain, C. Schissler, S. V. A. Gari, Z. Al-Halah, V. K. Ithapu, P. Robinson, and
K.Grauman. Soundspaces:Audio-visualnavigationin3denvironments. InComputerVision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
PartVI16,pages17–36.Springer,2020.
[34] A.Younes,D.Honerkamp,T.Welschehold,andA.Valada. Catchmeifyouhearme: Audio-
visual navigation in complex unmapped environments with moving sounds. IEEE Robotics
andAutomationLetters,8(2):928–935,2023.
[35] R.Gao,Y.-Y.Chang,S.Mall,L.Fei-Fei,andJ.Wu. Objectfolder: Adatasetofobjectswith
implicitvisual,auditory,andtactilerepresentations. InConferenceonRobotLearning,pages
466–476.PMLR,2022.
[36] R.Gao,Z.Si,Y.-Y.Chang,S.Clarke,J.Bohg,L.Fei-Fei,W.Yuan,andJ.Wu. Objectfolder
2.0: A multisensory object dataset for sim2real transfer. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10598–10608,2022.
[37] X. Yang, Z. Ji, J. Wu, Y.-K. Lai, C. Wei, G. Liu, and R. Setchi. Hierarchical reinforcement
learning with universal policies for multistep robotic manipulation. IEEE Transactions on
NeuralNetworksandLearningSystems,33(9):4727–4741,2021.
[38] B. Beyret, A. Shafti, and A. A. Faisal. Dot-to-dot: Explainable hierarchical reinforcement
learningforroboticmanipulation. In2019IEEE/RSJInternationalConferenceonintelligent
robotsandsystems(IROS),pages5014–5019.IEEE,2019.
[39] M.M.Botvinick. Hierarchicalreinforcementlearninganddecisionmaking. Currentopinion
inneurobiology,22(6):956–962,2012.
[40] Z.Yang,K.Merrick,L.Jin,andH.A.Abbass. Hierarchicaldeepreinforcementlearningfor
continuous action control. IEEE transactions on neural networks and learning systems, 29
(11):5174–5184,2018.
[41] D.Zhang,Q.Li,Y.Zheng,L.Wei,D.Zhang,andZ.Zhang.Explainablehierarchicalimitation
learningforroboticdrinkpouring. IEEETransactionsonAutomationScienceandEngineer-
ing,19(4):3871–3887,2021.
[42] F. Xie, A. Chowdhury, M. De Paolis Kaluza, L. Zhao, L. Wong, and R. Yu. Deep imitation
learning for bimanual robotic manipulation. Advances in neural information processing sys-
tems,33:2327–2337,2020.
[43] B.Li,J.Li,T.Lu,Y.Cai,andS.Wang. Hierarchicallearningfromdemonstrationsforlong-
horizon tasks. In 2021 IEEE International Conference on Robotics and Automation (ICRA),
pages4545–4551.IEEE,2021.
[44] K. Hakhamaneshi, R. Zhao, A. Zhan, P. Abbeel, and M. Laskin. Hierarchical few-shot imi-
tationwithskilltransitionmodels. InInternationalConferenceonLearningRepresentations,
2021.
[45] Z. Jia, F. Liu, V. Thumuluri, L. Chen, Z. Huang, and H. Su. Chain-of-thought predictive
control. arXivpreprintarXiv:2304.00776,2023.
[46] J.Arevalo,T.Solorio,M.Montes-yGo´mez,andF.A.Gonza´lez. Gatedmultimodalunitsfor
informationfusion. arXivpreprintarXiv:1702.01992,2017.
[47] J.Kim,J.Koh,Y.Kim,J.Choi,Y.Hwang,andJ.W.Choi. Robustdeepmulti-modallearning
basedongatedinformationfusionnetwork. InAsianConferenceonComputerVision, pages
90–106.Springer,2018.
[48] S.Wang,J.Zhang,andC.Zong.Learningmultimodalwordrepresentationviadynamicfusion
methods. InProceedingsoftheAAAIconferenceonartificialintelligence,volume32,2018.
11[49] J.Arevalo,T.Solorio,M.Montes-yGomez,andF.A.Gonza´lez. Gatedmultimodalnetworks.
NeuralComputingandApplications,32:10209–10228,2020.
[50] Z. Han, F. Yang, J. Huang, C. Zhang, and J. Yao. Multimodal dynamics: Dynamical fusion
for trustworthy multimodal classification. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pages20707–20717,2022.
[51] Q. Zhang, H. Wu, C. Zhang, Q. Hu, H. Fu, J. T. Zhou, and X. Peng. Provable dynamic
fusion for low-quality multimodal data. In International conference on machine learning,
pages41753–41769.PMLR,2023.
[52] M. K. Tellamekala, S. Amiriparian, B. W. Schuller, E. Andre´, T. Giesbrecht, and M. Valstar.
Coldfusion:Calibratedandordinallatentdistributionfusionforuncertainty-awaremultimodal
emotionrecognition. IEEETransactionsonPatternAnalysisandMachineIntelligence,2023.
[53] A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and C. Sun. Attention bottlenecks
formultimodalfusion. Advancesinneuralinformationprocessingsystems,34:14200–14213,
2021.
[54] S. Li, C. Zou, Y. Li, X. Zhao, and Y. Gao. Attention-based multi-modal fusion network for
semanticscenecompletion. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume34,pages11402–11409,2020.
[55] V.Chudasama,P.Kar,A.Gudmalwar,N.Shah,P.Wasnik,andN.Onoe.M2fnet:Multi-modal
fusion network for emotion recognition in conversation. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages4652–4661,2022.
[56] P.-L.Guhur, S.Chen, R.G.Pinel, M.Tapaswi, I.Laptev, andC.Schmid. Instruction-driven
history-aware policies for robotic manipulations. In Conference on Robot Learning, pages
175–187.PMLR,2023.
[57] X. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing, W. Zhang, H. Liu, et al.
Vision-languagefoundationmodelsaseffectiverobotimitators. InTheTwelfthInternational
ConferenceonLearningRepresentations,2023.
[58] S.HochreiterandJ.Schmidhuber. Longshort-termmemory. Neuralcomputation,9(8):1735–
1780,1997.
[59] M.Du,O.Y.Lee,S.Nair,andC.Finn.Playitbyear:Learningskillsamidstocclusionthrough
audio-visualimitationlearning. arXivpreprintarXiv:2205.14850,2022.
[60] K.He, X.Zhang, S.Ren, andJ.Sun. Deepresiduallearningforimagerecognition. InPro-
ceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–778,
2016.
[61] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014.
12Appendix
A DetailsoftheTaskSetup
In Sec. 4.1 of the main paper, we briefly introduced the basic information of the pouring task and
thepeginsertionwithkeywaytask,includingthetaskobjectivesandstagedivisions. Inthissection,
weprovideamoredetailedintroductiontothesetupofthesetwotasks. Wecontroltherobotarm
throughakeyboardtocompletethetasksandcollecthumandemonstrations.
A.1 Pouring
Cylinderwith
90/120g beads initially
Target
Cylinder
Shiftintrainingdemonstrations
6cm 3cm Shiftinreal-worldtesting
Figure5: Illustrationofthepouringtask. Werandomlyshiftthefixedtargetcylindersidewaysby
0 ∼ 3cm(indicatedbythebluearrow)intrainingdemonstrations,andshiftby0 ∼ 6cm(indicated
bytheorangearrow)duringtesting.
Setup details. For the pouring task, the robot needs to pour tiny steel beads of specific quality
from the cylinder in the hand into another cylinder. In the demonstrations, we randomly shift the
fixedtargetcylindersidewaysby0∼3cm,whileduringtesting,thisrangeexpandsto0∼6cm,as
illustratedinFig.5. Followingthepreviouswork[9],weusesmallbeadswithadiameterof1mm
tosimulateliquids. Theinitialmassofthebeadsis90g/120g,whilethetargetmassforpouringout
is 40g/60g, both indicated by prompts. Since the camera cannot capture the interior of the target
cylinder, other modalities are needed to assess whether the poured-out quantity meets the target.
Hence,weusevision(RGB),audioandtouchmodalitiesinthistask.
Robotactionspace. Inthistask,therobotcanactina2-dimensionalactionspacealongtheaxisx
andϕ,wherexrepresentsthehorizontalmovementandϕrepresentstherotationofthegripper. The
actionstepsizeis∆x = 0.5mmand∆ϕ = 0.12◦. Thereareatotalof5possibleactions(±∆x,
±∆ϕand0),correspondingtothetwodirectionsofthetwodimensionsof(x,ϕ)andholdingstill.
Stagedivision. Thistaskconsistsoffourstages[41]: 1)Aligning,wheretherobotneedstomovea
graduatedcylindercontainingbeadsandalignitwiththetargetcylinder,2)StartPouring,wherethe
robotrotatestheendeffectortopouroutthebeadsatanappropriatespeed,3)HoldingStill,where
the robot maintains its posture to continue pouring out beads steadily, and 4) End Pouring, where
the robot rotates the end effector at the appropriate time to stop the flow of beads. In trajectories,
weconsiderthetimestepsofthefirstdownwardrotationofthegripper(−∆ϕ),thefirstholdingstill
afterrotation,andthefirstupwardrotationofthegripper(+∆ϕ)asstagetransitionpoints.
A.2 PegInsertionwithKeyway.
Setup details. This task is an upgraded version of peg insertion, where the robot needs to first
align a peg with a key to the keyway on the base by rotating, and then insert the peg fully. The
alignment between the key and the keyway in this task primarily relies on tactile feedback, as the
1312cm Shift of the base in training demonstrations
Possibleinitial position of the robot arm in
training demonstrations
40cm 20cm Fixedpositionofthebaseduringtesting
Initial position of the robot arm during testing
Figure 6: Illustration of the peg insertion with keyway task. We randomly shift the fixed base
alonga12cm-longparallellineonthedesktop(theyellowarrow), andrandomlyinitializethepo-
sition of the robot arm inside a 40cm × 20cm rectangular area (the green rectangle) in training
demonstrations. During testing, we fix the position of the base (the red points) and the robot arm
(thegreenpoints)toseveralpre-definedpoints.
cameracannotobserveinsidethehole. Hence,weuseRGB,depthandtouchmodalitiesinthistask.
Considering generalization, we randomly fix the base at any position along a 12cm-long parallel
line on the desktop in demonstrations, as illustrated in Fig. 6. The robot arm holding the peg can
alsobeinitializedinsidea40cm×20cmrectangularareaaroundthebase. Duringtesting,toensure
fairness,thepositionsofthebaseandtherobotarmareseveralpre-definedpoints.
Robotactionspace. Inthistask,therobotarmcanmoveonthethreeaxesx,y,zofCartesiancoor-
dinate,wherex,yrepresentsthehorizontalmovementandzrepresentstheverticalmovement. The
grippercanalsorotatealongaxisϕtoalignthekeywiththekeyway. Sincetheverticalmovement
of the robot arm (−∆z) and the rotation of the gripper (+∆ϕ) are both unidirectional, there are a
totalof7possibleactions(±∆x,±∆y,−∆z,+∆ϕand0).
Stagedivision. Thistaskconsistsofthreestages: 1)FirstInsertion,wheretherobotalignsthepeg
withtheholeandinsertsituntilthekeycollideswiththebase,2)Rotating,wheretherobotaligns
the key with the keyway on the base by rotating the peg based on tactile feedback, and 3) Second
Insertion, where the robot further inserts the peg to the bottom. Similar to the pouring task, we
considerthetimestepsofthefirstgripperrotation(+∆ϕ)andthefirstdownwardmovement(−∆z)
afterrotationasstagetransitionpoints.
A.3 GeneralizationExperimentswithDistractors
Toverifythegeneralizationofourframeworktodistractorsintheenvironment,weconductexperi-
mentswithvisualdistractorsonbothtasks.Forthepouringtask,wechangethecolorofthecylinder
fromwhitetored. Asforthepeginsertiontask,werespectivelychangethecolorofthebasefrom
blacktogreen,andscattersomeclutteraroundthebase. ThesetasksettingsareillustratedinFig.7.
Thedistractorsonlyexistduringtesting. Besidesintroducingdistractors,thesettingsforthesetasks
remainconsistentwiththemainexperiments.
B ImplementationDetails
Following[9],weresizevisualandtactileframesto140×105andrandomlycropthemto128×96
duringtraining. Wealsousecolorjitterforimageaugmentation. Foraudiomodality,weresample
thewavesignalat16kHZandgeneratea64×50mel-spectrogramthroughshort-timeFouriertrans-
form, with 400 FFT windows, hop length of 160, and mel bin of 64. We employ ResNet-18 [60]
network as the uni-modal encoder. Each encoder takes a brief history of observations spanning
14(a)Color(Pouring) (b)Color(Insertion) (c)Mess(Insertion)
Figure7: Illustrationoftaskswithdistractors. Forthepouringtask,wechangethecolorofthe
cylinderfromwhitetored(denotedas“Color(Pouring)”). Forthepeginsertionwithkeywaytask,
werespectivelychangethecolorofthebasefromblacktogreen(denotedas“Color(Insertion)”),
andscattersomeclutteraroundthebase(denotedas“Mess(Insertion)”).
T = 6 timesteps. We train all the models using Adam [61] with a learning rate of 10−4 for 75
epochs. Weperformlinearlearningratedecayeverytwoepochs,withadecayfactorof0.9.
Foractionhistory,weuseabufferoflength200tostoreactions. Eachactionisencodedusingone-
hotencoding. Actionsequencesshorterthan200arepaddedwithzeros. Forbothtasks,weconsider
the15timestepsnearthestagetransitionpointasthesoftconstraintrange(γ =15). Wesetλ=5.0
for both tasks to control the intensity of the penalty. For the learnable stage token [stage ], we
i
initializeitwiththemeanofallstatetokensonsampleswithinthei-thstageafterwarmuptraining
for 1 epoch. We use β = 0.5 for the calculation of the stage-injected state token z∗. Moreover,
t
to prevent gradients from becoming too large, we also truncated the gradients of the stage score
penalty,restrictingthemtosolelyinfluencingthegatenetwork.
C RandomAttentionBlur
Stage-Guided Cross-Attention
Key
(𝑴×𝑻tokens) … …
Original
Query Attention Score
(1 token) … … (𝑴×𝑻)
𝟏
Random Attention Blur
(probability =𝒑)
Averaged
𝑴𝟏 ×𝑻 … … Atte (n 𝑴ti ×on 𝑻 S )core
𝟎
Attention
Score
Figure8:Illustrationofrandomattentionblurinthestage-guideddynamicfusionmodule. We
randomlyreplacetheattentionscoresonallfeaturetokenswiththesameaveragevalue 1 with
M×T
aprobabilityp,whichisahyper-parameter.
Inordertopreventthemodelfromsimplymemorizingtheactionscorrespondingtoattentionscore
patterns,weintroducearandomattentionblurmechanismtothestage-guideddynamicfusionmod-
ule.Foreachinput,wereplacetheattentionscoresonallfeaturetokenswiththesameaveragevalue
1 withaprobabilityp, whereM isthenumberofmodalitiesandT isthetimestepnumberof
M×T
thebriefobservationhistory,asillustratedinFig.8. Wesetp=0.25inbothtasks.
We introduce this mechanism due to the potential overfitting issue in the model, where the policy
head (MLP in the dynamic fusion module) learns the correspondence between the distribution of
15attentionscoresandactions. Forinstance,whenusingthetrainedMS-Botmodel(withoutrandom
attentionblur)tocompletepouringtasks,manuallyincreasingtheattentionscoresontactilefeature
tokens invariably leads the model to predict the next action as upward rotation (+∆ϕ), regardless
of the current stage of the task. This phenomenon suggests that the stage comprehension module
in the model partially assumes the role of action prediction rather than focusing solely on stage
understanding. Therefore, randomlyblurringattentionscorescancompelthepolicyheadtofocus
on the information from the feature tokens and better decouple the stage comprehension module
fromthedynamicfusionmodule.
D ComparisonofAttentionScoresbetweenMULSAandMS-BOT
Attention Attention
Score Score
Audio Depth
Vision RGB
Touch Touch
Stage Time Stage Time
Score Score
Stage 1 Stage 1
Stage 2 Stage 2
Stage 1: Stage 2: Stage 3: Stage 4: Stage 3 Stage 1: Stage 2: Stage 3: Stage 3
Aligning StartPouring HoldingStill EndPouring Stage 4 FirstInsertion Rotating SecondInsertion
Pouring Time PegInsertionwithKeyway Time
(a)Pouring (b)Peginsertionwithkeyway
Figure 9: Visualization of the aggregated attention scores for each modality and stage scores
ofMS-Botinbothtasks. Ateachtimestep,weaveragetheattentionscoresonallfeaturetokensof
eachmodalityseparately. Thestagescoreisthesoftmaxnormalizedoutputofthegatenetwork.
Attention Attention
Score Score
Audio Depth
Vision RGB
Touch Touch
Pouring Time Peg Insertion with Keyway Time
(a)Pouring (b)Peginsertionwithkeyway
Figure 10: Visualization of the aggregated attention scores of MULSA for each modality in
bothtasks. Ateachtimestep,weaveragetheattentionscoresonallfeaturetokensofeachmodality
separately. TherangeoftheattentionscoreaxisinthefigureisconsistentwithFig.9.
In Sec. 4.4 of the main paper, we illustrate the aggregated attention scores for each modality and
thestagescoresofMS-Botinthepouringtask,asshowninFig.9a. Wealsorecordthechangesin
attention scores and stage scores in the peg insertion with keyway task, as shown in Fig. 9b. The
resultsinthefiguredemonstratethatourmodelaccuratelypredictstherapidchangesinstagesfor
both tasks. As a result, the attention scores across modalities guided by stage comprehension are
alsorelativelystable,exhibitingclearinter-stagechangesandminorintra-stageadjustment.
Asacomparison, wealsovisualizetheattentionscoresofanotherbaselineMULSA[9]withself-
attentionfusion,asshowninFig.10. Therangeoftheattentionscoreaxisinthefigureisconsistent
withFig.9(0.0∼1.0). ItisevidentthattheattentionscoresofthemodalitiesintheMULSAmodel
arecloseandexhibitasmallvariationalongthetimeaxis,lackingclearstagecharacteristics. This
indicatesthattheMULSAmodelfailstofullyleveragetheadvantagesofdynamicfusioncompared
totheconcatmodel.
16PouringInitial(g) PouringTarget(g)
Methods
90 120 40 60
MS-Bot 1.60±1.10 5.58±1.79 6.48±1.55 1.80±0.95
-AttentionBlur 1.72±1.09 5.70±1.74 6.55±1.38 1.95±1.32
-StageComprehension 2.52±1.12 6.15±1.64 6.80±1.59 2.92±1.40
-StateTokenizer 3.05±1.01 6.42±1.98 7.12±1.66 4.19±1.24
Table4: Impactofeachcomponentofourframeworkinthepouringtask(mean±standarddevia-
tion). ‘-’indicatesfurtherremovingthemodulefromthemodelinthepreviousline.
PouringInitial(g) PouringTarget(g)
Methods
90 120 40 60
Concat 8.75±2.02 10.69±2.45 10.72±2.35 8.46±2.03
Duetal.[59] 8.51±1.79 9.54±2.10 9.98±2.20 8.04±1.85
MULSA[9] 4.72±1.30 7.83±1.71 8.45±1.50 5.56±1.13
MS-Bot 2.04±1.40 6.10±1.49 7.62±1.94 2.41±1.47
Table 5: Comparison of performance in scenes with visual distractors in the pouring task(mean ±
standarddeviation). Wechangethecolorofthecylinderfromwhitetoredduringtesting.
E DetailedExperimentalResultsofPouring
InSec.4.4and4.5ofthemainpaper,weshowtheoverallerroronallsettingsofthepouringtask.
In this section, we comprehensively present the detailed result of the component ablation and the
distractor experiment on all the settings of the pouring task in Tab. 4 and 5. As shown in Tab. 4,
thecontributionsfromboththestatetokenizerandthestagecomprehensionmoduleonallsettings
demonstratetheimportanceofthecoarse-to-finestagecomprehension. TheconsistentleadofMS-
BOTacrossallsettingsofthepouringtaskinTab.3alsodemonstratesthegeneralizabilityofstage
comprehension.
17F EvaluationofHyper-parameterSettings
     
 0 H D Q  ( U U R U  R I  & R Q F D W  0 R G H O  0 H D Q  ( U U R U  R I  & R Q F D W  0 R G H O  0 H D Q  ( U U R U  R I  & R Q F D W  0 R G H O
   0  0 H  H D  D Q  Q    (  ( U  U U  U R  R U  U    R  R I  I    0  0 8  6  /  % 6  R $  W    0  0 H  H D  D Q  Q    (  ( U  U U  U R  R U  U    R  R I  I    0  0 8  6  /  % 6  R $  W    0  0 H  H D  D Q  Q    (  ( U  U U  U R  R U  U    R  R I  I    0  0 8  6  /  % 6  R $  W
     
     
     
     
     
     
     
                                                               
 6 W D J H  6 F R U H  3 H Q D O W \  , Q W H Q V L W \   6 R I W  & R Q V W U D L Q W  5 D Q J H   5 D Q G R P  $ W W H Q W L R Q  % O X U  3 U R E D E L O L W \ p
(a)StageScorePenaltyIntensityλ (b)SoftConstraintRangeγ (c)AttentionBlurProbabilityp
Figure 11: Performance of MS-Bot on one setting of the pouring task when changing the
hyper-parametersλ, γ andp. Wefixtheotherswhenchangingonehyper-parameter. Wereport
the mean error of the concat model, MULSA and MS-Bot. The error bars represent the standard
deviationoferrorsforMS-Bot.
Inthissection,wetestthesensitivityofMS-Bottodifferenthyper-parametersettings. Specifically,
wetesttheperformanceofMS-Botunderdifferentpenaltyintensityλ,softconstraintrangeγ and
probabilityofrandomattentionblurpinonesettingofthepouringtask. Wetrainallthemodelsto
pourout40gofsmallbeadswithdifferentinitialmasses(90g/120g). Wesetλ = 5.0,γ = 15and
p=0.25bydefault,andkeeptheothersfixedwhenwemodifyoneofthehyper-parameters.
WepresenttheevaluationresultsinFig.11. OurMS-Botconsistentlyoutperformsboththeconcat
modelandMULSAacrossvarioushyper-parametersettings,indicatingthattheperformanceofour
MS-Bot is relatively stable to hyper-parameter variations. However, we also observe that when
settingλandγtosmallvalues(λ=1.0inFig.11aandγ =0inFig.11b),theperformanceofMS-
BotdropsandbecomesclosertoMULSAwithsimpleself-attentionfusion. Thisisbecausewhenλ
istoosmall,thepredictionofthecurrentstagebecomesinaccurateduetotheinsufficienttraining.
Whenγistoosmall,themodelisforcedtomakedrasticchangesinthestagescorepredictionwithin
veryfewtimesteps,leadingtounstablestagepredictions. Bothofthesefactorsweakentheefficacy
ofstagecomprehensionwithinMS-Bot. Wealsofindthatsettingtoolargep(p > 0.3inFig.11c)
canbringnegativeimpactsastheattentionblurtruncatesthegradientsbackpropagatedtothestage
comprehensionmoduleandthestatetokenizer. Excessiveattentionblurcanimpairthetrainingof
thesetwomodules.
18
  J   U R U U (   J   U R U U (   J   U R U U (