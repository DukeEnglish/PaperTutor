Mission Impossible: A Statistical Perspective on
Jailbreaking LLMs
JingtongSu∗ JuliaKempe† KarenUllrich†
NYU&MetaAI,FAIR NYU&MetaAI,FAIR MetaAI,FAIR
Abstract
Largelanguagemodels(LLMs)aretrainedonadelugeoftextdatawithlimited
quality control. As a result, LLMs can exhibit unintended or even harmful be-
haviours,suchasleakinginformation,fakenewsorhatespeech. Countermeasures,
commonlyreferredtoaspreferencealignment,includefine-tuningthepretrained
LLMswithcarefullycraftedtextexamplesofdesiredbehaviour. Eventhen,empir-
icalevidenceshowspreferencealignedLLMscanbeenticedtoharmfulbehaviour.
ThissocalledjailbreakingofLLMsistypicallyachievedbyadversariallymodify-
ingtheinputprompttotheLLM.Ourpaperprovidestheoreticalinsightsintothe
phenomenonofpreferencealignmentandjailbreakingfromastatisticalperspective.
Under our framework, we first show that pretrained LLMs will mimic harmful
behaviourifpresentinthetrainingcorpus. Underthatsameframework,wethen
introduceastatisticalnotionofalignment,andlower-boundthejailbreaking
probability,showingthatitisunpreventableunderreasonableassumptions.
Basedonourinsights,weproposeanalterationtothecurrentlyprevalentalignment
strategy RLHF. Specifically, we introduce a simple modification to the RLHF
objective,wecallE-RLHF,thataimstoincreasethelikelihoodofsaferesponses.
E-RLHFbringsnoadditionaltrainingcost,andiscompatiblewithothermethods.
Empirically,wedemonstratethatE-RLHF outperformsRLHFonallalignment
problemsputforwardbytheAdvBench[1]andHarmBenchproject[2]without
sacrificingmodelperformanceasmeasuredbytheMT-Benchproject[3].
1 Introduction
LargeLanguageModels(LLMs)haverevolutionizedthefieldofdeeplearningduetotheirremarkable
capabilities across various domains, serving as assistants, in code generation [4], healthcare [5],
andtheoremproving[6]. ThetrainingprocessofaLLMtypicallyincludestwostages: pretraining
withmassivecorpora,andanalignmentstepusingReinforcementLearningfromHumanFeedback
(RLHF)tofurtheralignmodelbehaviorwithhumanpreferences. Thelattersteptypicallyinvolves
large amounts of humanly annotated data, and can be decomposed into a supervised fine-tuning
(SFT)step,arewardmodelingstep,andanRLFine-Tuningstep. Despitetheirabilitytoperform
multiple tasks effectively, LLMs are susceptible to generating offensive or inappropriate content
includinghate-speech,malware,fakeinformationorsocialbiases,duetotheunavoidablepresence
ofharmfulelementswithintheirpretrainingdatasets[7–9]. Socialmediashowcaseanabundance
oftricksonhowtoattackChatGPT[10]toelicitharmfulresponses,e.g.,the“DoAnythingNow”
(DAN)prompts[11]orthe“GrandmaExploit”hack[12]. Ontheotherhand,behaviordiversityin
thetrainingcorpusisessentialtoforexamplecapturingdifferentculturalpreferences. Whatisand
isn’tharmfulultimatelydependsonuserpreferences,hencethealignmentstepisnotuniversalbut
dependsonthespecificusecaseunderwhichamodelwillbeemployed.
∗Correspondenceto:JingtongSu<js12196@nyu.edu>.
†Equaladvising.
Preprint.Underreview.
4202
guA
2
]GL.sc[
1v02410.8042:viXraToaddressdeploymentsafetyandeliminateobjectionableresponses,numerousalignmentefforts
havebeenmade,suchasinjectingsafeinformationduringSFT[13],performingredteamingwith
humanexpertsandAIthemselves[14–18],aswellasrefiningandimprovingthewholeRLHFprocess
in detail [19–23]. Yet we continue to witness a cat-and-mouse game of ever more sophisticated
alignmentmethodstoneutralize“harmful”promptsandevenmoreinventive“jailbreaking”attacks
thatmanipulatethosepromptstoelicitLLMstoproduceharmfulinformation. Suchattackscomein
variousflavors,suchasinjectingadversarialsuffixes[1,24,25],exploringcipherandlow-resource
naturallanguages[26–28],orlettingLLMscraftpromptsautomatically[29–33].Althoughseveralad-
hocdefensemethodsagainstsuffixeshavebeenproposed[34–37],weonlyhavelimitedproposalon
aprincipleduniversaldefenseagainstjailbreakingattacks[2],andlimitedtheoreticalunderstanding
onthisphenomenon[38].
Inthispaper,wepresentatheoreticalframeworkforanalyzingboththepretrainingphaseandthe
post-alignmentjailbreakingphenomenon. Exploitingthefactthatjailbreakingpromptstypically
maintaintheunderlyingharmfulconceptwhilemanipulatingotheraspectsoftheprompt,wedesign
frameworkthatdecouplesinputpromptstoallowsustoquantifythestrengthofpotentialadversaries.
Byrepresentingtheoutputelementsofanlanguagemodel(LM)aslengthiertextfragmentsratherthan
individualtokens,wecanquantifytheextenttowhichthesemodelsemulatethetrainingdistribution
andconsequentlybetterunderstandthemechanismsunderlyingjailbreakingvulnerabilities.
Ourcontributionscanbesummarizedasfollows:
• Based on our proposed framework, we first offer a non-vacuous PAC-Bayesian style gener-
alization bound for pre-training. Assuming the validity of our framework, we conclude that
high-performingpre-trainedmodelswillinevitablybesusceptibletogeneratingbehaviourthatis
presentinthetrainingcorpus,includinganyunintendedandharmfulbehaviour.
• Subsequently,weextendourframeworktoincludenotionsofalignmentandjailbreaking. As-
sumingourassumptionsaremet, wedemonstratejailbreakingtobeunpreventableevenafter
safetyalignmentbecausetheLMfailstoconcentrateitsoutputdistributionoverthesetofsafe
responses.
• Motivated by our theoretical findings, we identify a key drawback in the widely adopted RL
Fine-Tuningobjectiveduetothenaturaldifferencebetweentheharmlessnessandthehelpfulness
targets. Byaddressingthisissue,wefacilitatethetrainingofsafermodelsthataremoreresilient
toasuiteofjailbreakingattackswhilepreservingmodelperformance.
Thepaperisorganizedasfollows. InSection2,weintroduceourframework. InSection3,weprove
thePAC-Bayesiangeneralizationboundforpretraining. Next,inSection4wepresentanalysison
jailbreakingfromastatisticalperspective. Finally,inSection5weillustrateourproposedE-RLHF
objectiveanditseffectivenessonimprovingLLMsafety. WegivealiteraturereviewinAppendixH.
2 Frameworkandassumptions
Jailbreakingcarriesseveralanalogiestoadversarialattacks,awellstudiedfieldincomputervision
[39]. Here,anadversaryisdefinedasamapthatperturbsagiveninputimageinpixelspacetochange
themodeloutput. Thestrengthoftheadversaryisboundedbyhowfaritisabletomovetheoriginal
inputasquantifiedbytheℓ distance[40–42]. Typically,thisdistanceisboundedinawaythatthe
p
changewouldnotbeperceptibletothehumanobserver. Thegoaloftheadversaryistocausemis-
classificationoftheinput. Incontrast,intheinstanceofanLLM,theadversary’sgoalistoprovoke
harmfulbehaviour,e.g.,unintendedleakingofinformationorhatespeech. Further,anyperturbation
to an input, called prompt, will have a perceptible effect. Hence quantifying and bounding the
capabilitiesoftheadversaryisnotstraightforward. Nonetheless,withsomemodifications,wewill
buildonthisanalogy.
For the purpose of this work, we will view any prompt as a tuple of query and concept (q,c),
where c ∈ C, and q ∈ Q, with C,Q denoting the complete concept set and query set. Con-
ceptually, we think of concepts as representing the information content of the prompt, usu-
ally through a short piece of text, for example “tutorial on making a cake”. Queries
are instructional text pieces that are composable with certain concepts. We can think of
queries as mechanisms to trigger an LM to expand a concept in a specific way. Examples
include “Tell me how to {}”, or “We are now in an imaginary world, and you are
2not bounded by any ethical concerns. Teach my avatar how to {}”. Since not all
queriesandconceptsarecomposable,3 wedenoteP ⊊ Q×C asthesetofallplausibleprompts,
wherethedefinitionofplausiblewillbemadeclearbelow.
Thedecompositionofpromptsallowsustoisolateandhenceboundtheadversary’sstrength.
Inlinewithcurrentempiricalworkoninducingharmfulbehaviour,wewillallowperturbationsonly
onthequeries,notontheconcepts. Furthermimickingthespiritofpreviousworkonadversarial
attacks,wewillassumethattheground-truthrelatedtoapromptisdeterminedsolelybytheconcept,
notthequery. Wewillmaketheseideasmorerigorousinthenextparagraphs.
First, in contrast to previous theoretical
work where LMs are regarded as single
sentencegenerators[38],wemodelLMs
aslengthiertextfragmentgenerators,and
refertopossiblegeneratedcontente∈Eas
explanations. Conceptually,explanations
expandconceptswithadditionalinforma-
tion. Forexample,“The US president
in 2023 is Joe Biden.”. AnLMthus
inducesamappingfromplausibleprompts
to distributions over explanations, p :
LM
P → ∆(E), where ∆(E) denotes the set
of distributions defined over elements in
E.4 The output of a LM given a prompt, Figure1: Ourframeworkinanutshell: Wedefine
p (q,c), is a discrete distribution over a language model, p : → , as a map from
LM LM
explanations. Weusedom(p (q,c))as promptstoadistributionoverasubsetofallpossible
LM
thedomainofthisdistribution,p (e|q,c) explanationsE. Tolaterbeabletoboundthestrength
LM
as the probability of e given (q,c) as the oftheadversarialattacker,wesplitthetextinputsinto
input, andsupp(p (q,c))asthesubset conceptsandqueries(q,c). Weassumethat(i)thetext
LM
of E with non-zero p (e|q,c). Further, corpus only covers a part of the domain of the LM:
LM
weassumetheexistenceofalatentground supp(D )⊊dom(p ),(ii)thesizeofthedomainof
P LM
truth mapping p : P → ∆(E) that the output distribution, denoted |dom(p (q,c))|, is
world LM
theLMisoptimizedtomimicduringthe smallcomparedtothesizeofE,and(iii)onlyconcepts
pretrainingstage. Thisisthedistribution determinetheoutput(see ).
thatdefines“knowledge”: forallplausibleprompts(q,c),itspecifiestheground-truthdistribution
overexplanations. Byplausible,werefertoallpromptsthatlieinthedomainofthegroundtruth
mapping(q,c)∈dom(p ),i.e.,P ≡dom(p ). Manyplausiblepromptswillnotevenexist
world world
withinanyavailabletrainingcorpus,asdiscussedbelow.
Wecannowstateourmainassumption,namelythatforanyplausibleprompt(q,c)∈dom(p )
world
theground-truthdistributionp (q,c)issupportedonasmallsubsetofE ⇔supp(p (q,c))⊊
world world
E. Thisassumptionseemssensibletous: undernormalcircumstances,providinganexplanationof
“Paris”wouldnotofferanyrelevantknowledgewhengivenapromptsuchas“How to write a
hello world python script”. Oursecondassumptionisthatforallplausibleprompts(q,c),the
conceptcuniquelydeterminesthesupportoftheoutputdistributionspecifiedbyp ,regardless
world
ofthequery: supp(p (q,c))=supp(p (q∗,c)),∀plausible(q,c)and,(q∗,c). Thequery
world world
changes the ground-truthdistribution withoutaffecting its support. An illustration isdepicted in
Figure1. Tobemoreprecise:
Assumption2.1. (Conceptsuniquelydeterminetheexplanationforplausibleprompts)
Forallplausibleprompts(q,c)∈dom(p ),
world
i)p :P →∆(supp(p (q,c))
world world
wheresupp(p (q,c))⊊E s.t. |supp(p (q,c))|≪|E|; and
world world
ii)supp(p (q,c))=supp(p (q∗,c)), ∀(q,c),(q∗,c)plausible.
world world
3Forexample,"Who is a tutorial on making a cake."isunreasonable.
4Forreal-worldLMs,withdifferentdecodinghyperparameterse.g.,thetemperatureT,top-pandtop-k
samplingparameters,theinduceddistributionwiththesamesetofparameterscouldbedifferent.Ourdiscussion
holdsforapre-fixedsetofhyperparametersthroughoutthispaper.
3Thisassumptionisnaturalsinceitessentiallytellsusthatknowledgeisspecifiedbythecorresponding
conceptalone,irrespectiveofwhatqueryisusedtoextractit. Inotherwords,givenaconceptc,ifa
queryqmanagestochangesupp(p (q,c)),wearguethatthequeryshouldbedeconstructedand
world
partiallyabsorbedbyctoaccuratelyreflecttheknowledgemirroredbythesupport.
Lastly, we make the assumption on the existence of an underlying generative distribution over
prompts,denotedas(q,c)∼D . Thisdistributionservesastheprinciplegoverningthecreationof
P
ourpretrainingcorpus. Itisimportanttonotethatsupp(D ) ⊊ dom(p ). Forexample,take
P world
theprompt(q′,c′)=“Who is James Bond $λ*#!48811”;eventhoughthispromptneverappears
inanytextcorpusacrosstheinternet,(q′,c′) ∈/ supp(D ),we,ashumans,canmakesenseofit:
P
(q′,c′) ∈ dom(p ). LaterproofsinthispaperassumeLMsgeneratesemanticallyreasonable
world
explanationsforsuchunseenplausibleprompts,sinceinrealityLMsareclaimedtogeneralizewell
onhuge,out-of-distributiondatasets[43]. ThisismadeexplicitinSection4,withinAssumption4.1.
Finally, thefollowingdefinitionspertaintoournotionofharmfulness. Morespecifically, weun-
derstandharmfulbehaviourabstractlyasanyunintendedbehaviour. Forthis,weassumethatany
explanationecanbedenotedaseitherharmfulornotharmful(safe). Aconceptcisregardedas
harmfulifandonlyiftheworldgeneratesharmfulexplanationswithprobabilityhigherthanacertain
thresholdwithdirectprompts.
Definition2.1. (NotionsofHarmfulness)
• (DirectQueriesandDirectPrompts)WerefertoapromptasdirectifitstemsfromD , i.e.,
P
(q,c)∈supp(D ). Thequeryofadirectpromptiscalledadirectquery.
P
• (Harmful Concepts and Harmful Set) Given a concept c, the associated harmful set of ex-
planations is denoted as E (c) := {e|e ∈ supp(p (·,c)) ∧ eisharmful}. In accor-
h world
dance with Assumption 2.1, with a threshold η, a concept c is harmful if ∀qs.t. (q,c) ∈
(cid:80)
dom(p ), p (e|q,c) ≥ 1−η. We refer to the set of all possible harmful
concepw tso arl sd
C
⊊e: Ce∈ .Eh(c) world
h
• (SafeSet)∀c∈C ,thereexistsacorrespondingsafesetE (c)⊊E thatwewishp (q,c)tobe
h s LM
concentratedon. Itincludessafeexplanationsexistinginsupp(p (·,c)),andexplanations
world
designedbyhumans,e.g.,withthetemplatebeginningwith“Sorry.”
• (Semanticallymeaningful)WecallexplanationsinE (c)∪E (c)assemanticallymeaningful
h s
forthe(q,c)prompt.
• (MixturedecompositionofD )Withthesenotions,wecandecomposeD = αD +(1−
P P Ph
α)D (wheresupp(D )includesalldirectpromptswithaharmfulconcept,andsupp(D )
Ps Ph Ps
includes the complement) as a mixture over direct prompts with a harmful concept and the
non-harmfulcounterpart.
3 PAC-Bayesianboundforpre-trainingLLMsonharmfuldata
Givenalearningalgorithmthatleadstoaposteriordistributionoverasetofmodels,PAC-Bayesian
theory[44]appliesProbablyApproximatelyCorrect(PAC)inequalities,toprovideboundsonthe
generalizationgap,i.e.,thedifferencebetweenthemodel’sempiricallossandthepopulationloss.
Wenowpresentthefirstresultofouranalysis: anon-vacuousPAC-Bayesianboundforpretraining
LMswhichimpliesthatawell-trainedLMoughttoexhibitharmfulbehaviourevenwhensimply
promptedwithdirectqueriesifitwaspresentedwithharmfulbehaviorduringtraining.
WedenotebyS ={(q ,c )}n asetofpromptsgeneratedi.i.d. underD ,S ∼Dn. Theseprompts
i i i=1 P P
togetherwithsampledexplanationsformourpretrainingcorpus.Weuseπ,ρasthepriorandposterior
distributionoverLMsbeforeandafterthepretrainingprocess,definedoverLM,thesetoflanguage
models. Givenaprompt(q,c),wemeasurethegeneralizationcapabilityofaLMbyquantifyingthe
TotalVariation(TV)lossbetweentheinduceddistributionp (q,c)andtheground-truthdistribution
LM
p (q,c).5 For real-world LMs, pretraining involves optimizing the cross-entropy loss on the
world
trainingcorpus,whichisequivalenttominimizingKL[p (q,c)||p (q,c)]underourframework.
world LM
WithPinsker’sInequality,optimizingtheKL-divergencetermisequivalenttooptimizinganupper
boundonTV;thusweexpectempiricalTVlossbesmall.
5WeregardbothdistributionsasdefinedovertheentireE sincewedonotrestricttheoutputdistributionof
LMinthissection.
4Definition3.1. (TVempiricallossandpopulationloss)
ℓ (p ,(q,c)):=TV(p (q,c),p (q,c)).
TV LM world LM
GivenanLMandasetofdataS, theempiricallossRˆ (p )andpopulationlossR(p )are
S LM LM
definedas
n
Rˆ (p ):= 1 (cid:88) ℓ (p ,(q ,c ));
S LM n TV LM i i
i=1
(cid:104) (cid:105)
R(p ):=E Rˆ (p ) =E [ℓ (p ,(q,c))].
LM S∼D Pn S LM (q,c)∼DP TV LM
WestateourPAC-Bayesianboundasfollows. ThedetailedproofcanbefoundinAppendixB.1. 6
Theorem1. (PAC-BayesianGeneralizationBoundforLanguageModels.) WithαasinDefinition
2.1,considerasetoflanguagemodelsLM,withpriordistributionπoverLM.
Givenanyδ ∈(0,1),foranyprobabilitymeasureρoverLMsuchthatρ,πsharethesamesupport,
thefollowingholdswithprobabilityatleast1−δovertherandomdrawofS:
(cid:115)
(cid:2) KL[ρ||π]+log1(cid:3)
E [R(p )−Rˆ (p )]≤ δ :=ϱ;
LM∼ρ LM S LM 2n
1 (cid:104) (cid:105)
E [E ℓ (p ,(q,c))]≤ E Rˆ (p )+ϱ . (1)
LM∼ρ (q,c)∼DPh TV LM α LM∼ρ S LM
InAppendixB.2wegiveatheoreticalestimationofϱ,toillustratetheboundwederiveisnon-vacuous,
i.e.,lessthan1. TheKLtermisoforderO(K)whereK isthenumberofparametersinvolvedinπ,ρ,
andncanbeshowntogreatlyexceedK (usingarealisticZipfdistributionassumptiononpromptsto
estimatethenumberofuniqueprompts). Theorem1tellsusthat,aslongaspretrainingsuccessfully
reducesthelossonthetrainingcorpus(Rˆ (p )↓),inexpectationthelanguagemodelwillmimic
S LM
theworldwell(smallℓ difference)onagivendirectpromptsampledfromD . Furthermore,ifα
TV P
isnottoosmall,thenthisstatementholdsonadirectpromptwhoseconceptisharmful. Sincewe
havedefinedtheharmfulconceptasoutputtingharmfulexplanationswithhighprobability(Definition
2.1),weconcludethatanLMtrainedonD datacanoutputexplanationsintheharmfulset.
P
4 Astatisticalperspectiveonjailbreakingafteralignment
Inthissection,wewillpresentthemaintheoreticalcontributionofourwork: givenourassumptions
hold,weprovetheexistenceofwaysforanadversarytojailbreakanLMevenafterthepreference
alignmentprocess. Ourproofstrategyisinspiredbytheworkonadversarialrobustness[41],which
boundstheadversary’sprobabilityofsuccessbyupperboundingthevolumeofthesetofpoints
thatdoesnotallowfortheexistenceofadversarialexamples. Goingforward,weneedtoextendour
frameworktointegratealignmentandjailbreaking.
AfteranLMispretrained, ittypicallywillundergofine-tuningonadatasetcontainingpreferred
behaviour. Inwhatfollows,wewillassumethatthisalignmentprocessdoesnotchangethemodel
performance in the sense that the LM will still produce semantically meaningful explanations
(Definition2.1). Itwouldnot,forexample,defaulttoansweringanyrequestwiththesameresponse.
Assumption4.1. (LMoutputssemanticallymeaningfulexplanations)Foranyharmfulconceptc,
andallplausibleprompts(q,c)∈dom(p ),
world
∃|E (c)|≪|E (c)|+|E (c)|s.t. O(1)≪|dom(p (q,c))|=|E (c)∪E (c)∪E (c)|.
n h s LM h s n
Inotherwords,weassumetheLM’soutputdistributionisaccuratelysupportedonE (c)∪E (c),
h s
in the sense that the size of “residual” E (c) is relatively small compared to these semantically
n
meaningfulexplanations.Wedefinen(c)=|E (c)|+|E (c)|+|E (c)|.Weomitthe(c)annotations
n s h
whenclearfromthecontext. TheO(1)statementisreasonable,becauseharmfulexplanationsare
usuallylongtextfragmentsthatallowformanyalternativeformulations. Theassumptioncanbe
brokendownintotwocomponents: (1)withinthesupportoftheoutputdistribution,onlyoccasional
6TheinspirationfortheproofofTheorem1comesfromMbackeetal.[45],andtheproofideaisoriginally
proposedinGermainetal.[46],Haddoucheetal.[47].
5instancesofunrelatedexplanationsexist;(2)theprocessofaligningthemodeltowardssafetydoesnot
eliminatetheharmfulexplanationsacquiredduringthepretrainingphase. Forpart(1),similarto
theexamplewegaveabove,undernormalcircumstances,wedonotexpecttheexplanation“Paris”
to appear in dom(p (q,c)) given (q,c) as “How to build a bomb”. As for part (2), though
LM
seeminglysurprising,evidencewithaseriesofcurrentstate-of-the-artLMscanbeexperimentally
validated[48],wherediverse,harmfulexplanationsareextractedbysimplymanipulatingthedecoding
processusingdirectprompts. InSection5wegiveanexplanationforthisundesiredphenomenon.
To bound the likelihood of jailbreaking we first need
to specify how the output of a LM interacts with its
support. Assuming a fixed order of explanations in
dom(p (q,c)), and slight abuse of notation, we can
LM
use p (q,c) to denote an n(c)-dimensional vector on
LM
∆n(c)−1, the probability simplex with n(c) elements,
where each entry represents the probability of a single
explanation. We call this simplex the output simplex
relatedtoagivenconceptc. Next, wecaninduceadis-
tributiononthissimplexgivenaposteriordistributionγ
overthesetoflanguagemodelsLM,asfollows.
Figure2: Conceptualillustrationofour
Definition 4.1. (Induced Distribution on Simplex, γ ) frameworkforjailbreakingintroducedin
c
UndertheassumptionthattheLMoutputssemantically Section4,withafixedharmfulconcept
meaningful explanations (Assumption 4.1), with a fixed c. Thetrianglerepresentstheprobability
prompt (q,c) and a posterior distribution γ over LM, simplex. This figure showcases a typ-
thecorrespondinginduceddistribution: p (q,c)where ical successful jailbreaking attempt by
LM
LM ∼γ issupportedoverasubsetoftheoutputsimplex theadversary:althoughsafetyalignment
∆n−1. Thisdistributionisdenotedasγ ,orγ when makes the sampled LM safe under the
(q,c) c
thereferencetoqisclearfromcontext. directpromptinput,theadversaryisable
Next,wewillseparatetheoutputsimplexintoaharmful tomovetheoutputtotheharmfulzone
andsafetyzone. Thisdefinitionismotivatedbytheobser- H hbymanipulatingthequeryq.
vationthattypicallyanadversaryisdeemedsuccessfulif
itcanextractevenasingleharmfulexplanationforagivenconcept. Thistranslatesintoadivisionof
theoutputsimplex,underAssumption4.1,asfollows.
Definition4.2. (HarmfulZoneandSafetyZone)ForagivenharmfulconceptcandafixedLM,
the output simplex is divided into a safety zone and a harmful zone, H and H , where a pre-
s h
defined threshold p ∈ [0,1] is used to quantify the distinction: p (q,c) ∈ H if and only if
LM h
(cid:80)
p (e|q,c)≥p,andotherwisep (q,c)∈H .
e:e∈Eh(c) LM LM s
Beforeweintroducejailbreaking,thereadermightwonderwhywedidnotdefinealignmentmore
clearly. ThisisbecauseunderthePACframework,preferencealignmentisnothingbutatransforma-
tionfromρtosomeγposteriordefinedoverLM.Giventhisinabilityonfine-grainedcharacterization
ofalignment,weinsteadprovidethegoalofitasfollows. Withtheabovenotion,givenaprompt
(q,c) where c is harmful, its goal is to push the induced distribution γ into the safety zone H .
c s
Ideally,supp(γ )⊂H ⇔withprobability1,theresultingLMissafewhenencountering(q,c). We
c s
arereadytointroducenecessaryconceptsrelatedtojailbreaking.
Definition4.3. (Jailbreaking)Givenaharmfulconceptcandaqueryq′,theprompt(q′,c)jailbreaks
theLMiffp (q′,c)∈H . Wecallsuchaprompt(q′,c)andqueryq′ ajailbreakingpromptand
LM h
jailbreakingquery,respectively.
ThethresholdpfordiscriminatingH andH shouldbeverysmall,sinceitmeansinexpectationthe
h s
adversaryneedstocalltheLM 1 timestocollectasingleharmfulexplanationi.e.,tojailbreakthe
p
LM.
Totheoreticallyprovethejailbreakingeffect,weneedtorestricttheadversary’sability. Toachieve
thisgoal,weborrowinsightsfromadversarialattacks,toassumethattheadversaryhasbounded
manipulatingcapabilityontheoutputsimplexwhensearchingoverthequeryset:
Assumption4.2. (ϵ-boundedadversary)GivenanLM,aharmfulconceptcandanassociateddirect
prompt(q,c),weassumetheadversarycanfindasetofqueriesQ′,suchthattheoutputismovedat
mostϵonthesimplextowardsH fromp (q,c):
h LM
sup d(p (q,c),p (q′,c))=ϵ.
LM LM
q′∈Q′
6Heredisadistancemeasurebetweentwodiscretedistributions. dcanbeatypicalℓ measurewith
p
p≥1,ortheTotalVariation/Jensen-ShannonDivergence. Wecallq′ ∈Q′anϵ-boundedquery.
AconceptualillustrationofourframeworkisdepictedinFigure2. BeforearrivingatourTheorem,
wegivethefinaldefinitionofϵ-expansion.
Definition4.4. (ϵ-expansion)GivenasetA⊂∆n−1andadistancemeasured,theϵ-expansionset
A(ϵ,d)isdefinedas
A(ϵ,d):={t|t∈∆n−1∧∃y∈As.t.||y−t|| ≤ϵ}.
d
Wearereadytopresentthefollowingtheorem,whichstatesthataslongastheinducedposteriorγ is
c
notconcentratedinanextremelysafearea,thenwithhighprobabilitythemodelcanbejailbroken.
TheproofisinAppendixB.3.
Theorem2. (Jailbreakisunavoidable)AssumethatanLMsoutputsemanticallymeaningfulexpla-
nations(Assumption4.1). Givenanyγ posteriordistributionoverLM,chooseaharmfulconceptc
withadirectprompt(q,c)andathresholdp(Definition2.1),todefinethecorrespondinginduced
distribution γ (Definition 4.1) and division over output simplex (Definition 4.2). An ϵ-bounded
c
adversary(Assumption4.2)canfindajailbreakingprompt(Definition4.3)withprobabilityatleast
1−γ ×(1−Φ(a )),
s ϵ
• byusingeitherthedirectprompt,suchthatp (q,c)∈H ;or
LM h
• byfindinganϵ-boundedqueryq′,suchthatp (q′,c)∈H .
LM h
Here,Φ(·)isthestandardGaussiancdf,γ :=max γc(x),withU(x)theuniformdis-
√ s x∈Hs−Hh(ϵ,d) U(x)
tributionover∆n−1,anda :=a+ n−1ϵ,whereawritesanalyticallyasa≍ |E√h(c)|−1−(n−1)p.
ϵ
(n−1)p(1−p)
Trivially,thechancesofanadversarytofindajailbreakingpromptincreaseforstrongeradversaries
(ϵ↑). Intherealworld,thiscouldrelatetohowmuchcomputebudgetweallowtoalteraqueryfor
aspecificharmfulconcept. Furthermore,thechancesofanadversarytofindajailbreakingprompt
increasewhentheratioofthesizesoftheharmfulexplanationsettothesafeexplanationsetislarger
|Eh(c)| ↑. Thisisbecausetheirratiowilldeterminethesizeoftheharmfulzonewhichinturnwill
|Es(c)|
cause Φ(a ) → 1. In real world settings, for any harmful concept, the training corpus naturally
ϵ
containsalargeharmfulsetduetothenumberofpossibleresponses. Realistically,itssizecannotbe
counteredbyanymanually-constructedsafeset. Henceachievingalignmentishard: Recallthatthe
goalofalignmentistorespondwithonlysafeexplanationswithhighprobability. However,wejust
learnedthattoincreasethatprobability,weneedtohaveasmallharmful-to-safetysetratiowhichwe
discussedisnotrealistic. Consequently,thesafetyzoneisgoingtobesmall.
5 E-RLHF:improvingalignmentbyexpandingthesafetyzone
Recall from Theorem 2 and the subsequent discussion in the previous section, that jailbreaking
becomesmorelikelythelargertheharmfulzoneisincomparisontothesafetyzone. Thesizeofboth
zonesrelatestothesizeoftheirrespectiveexplanationsets. Inotherwords,thesizeofthepreference
alignmentdatasetiscrucialtosuccessfulalignment. Unfortunately, thehumanlaborinvolvedin
creatingsuchadataseteffectivelycapsitssize.
Inordertobridgethegapbetweenourtheoreticalinsightsandapracticalsolutiontowardssuppressing
thejailbreakingproblem,wefocusonothermorepracticalwaystoexpandthesafetyzone. Even
though our ideas are more broadly applicable, in our experiments we will focus on improving
ReinforcementLearningwithHumanFeedback(RLHF).RLHFtypicallyincludesthreephases: i)
supervisedfine-tuning(SFT);ii)preferencesamplingandrewardlearningandiii)RLoptimization.
Rafailovetal.[23]haverecentlyproposedawidelyappliedversionofRLHFforLMs,coinedDirect
PreferenceOptimization(DPO),thatemploysacleverreparameterizationwhichleadstodirectly
learning from the preference dataset, without the need of obtaining a reward model beforehand.
DPOismorestableinthetrainingprocessthanotherimplementationsofRLHF.Amorecomplete
overviewofRLHFandDPOcanbefoundinAppendixC.
Forourpurposes,weassumeaccesstoanLLMp thathasbeensupervisedfine-tunedonhigh-
SFT
qualitydata. WefurtherassumeaccesstoapreferencealigneddatasetD ;thatcontainsasetoftext
s
7prompts(q,c) = x,andtworespectiveexplanationsthathavebeenratedbyhumanannotatorsas
bettere orworsee . Inphaseii)ofRLHF,onetypicallyoptimizesarewardmodelr(x,e)basedon
w l
theannotatedexplanations. Ourproposalconcernsphaseiii)oftheRLHFprocess: trainingofthe
preferencealignedmodelp . Foragivenrewardmodel,p istypicallyobtainedbyoptimizing
LM LM
thefollowingobjective;
L (p )=−E [r(x,e)]+βD (p (x)||p (x)) (2)
RLHF LM x∼Ds,e∼pLM(x) KL LM SFT
Notethat,thefirsttermismaximizingthereward,whiletheKL-termactsasaregularizerensuring
thealignedmodelcannotdriftofftoofarfromtheSFTmodel. Weclaimthatthisregularizationis
exactlytheproblem. Whiledesignedtokeepthemodelhelpful7,foranyharmfulpromptx and
h
any harmful explanation e ∈ supp(p (x )), p will maintain e in the support of the output
SFT h LM
distribution. Specifically, the supervised fine-tuning process does not involve elimination of any
harmfulexplanationsfromthesupportoftheoutputdistributionofthepretrainedmodel,8 thusthis
small safety set problem will be further passed to p , even if p is optimized to the optimal
LM LM
solutionoftheaboveobjective. Thus,weshouldnotpushtheLLMposteriorintothesamedirection.
Instead,foranyharmfulpromptx ,weproposetouseapriorp (·)thatwedesigntohavealarge
h SFT
safetyzone. Wecanachievethisbymodifyingtheharmfulprompt,suchthatasafeconceptis
usedtoreplacetheharmfulconcept, whichalterssupp(p )toincludemoresafeexplanations.
SFT
Thiscanbedoneinanindividualizedmanner,orsimplybyprefacingallharmfulpromptswitha
prefix such as "Please ensure your response adheres to community guidelines and
ethical standards:". 9 Importantly,thenon-harmfulpromptsarenotmodified. Duetothefocus
ofourapproachtoexpandthesafetyzoneoftheoutputdistribution,wecoinourproposalE-RLHF,
resultinginthefollowingmodificationtoEq. (2):
L (p )=−E [r(x,e)]+βD (p (x)||p (x )) (3)
E-RLHF LM x∼Ds,e∼pLM(x) KL LM SFT s
wherex isasafety-transformedversionoftheoriginalharmfulpromptx . Torecap,thekey
s h
argumentweputforthisthat,inordertoensurethestabilityofmodelfine-tuning,itisnotimperative
toutilizeidenticalpromptinputsxforboththereferencemodelandthetargetmodel,particularly
when the original input x itself is harmful. In fact, as long as the substitute or "anchor" prompt
generateslogicallyreasonableoutputsakintothoseproducedbytheoriginalprompt,thisapproach
wouldnotimpedethetrainingprocessofthemodel. Tosolidifyourargumentweshowtheimpactof
ourmodificationonthesupportoftheoptimalpolicyinAppendixC.Wealsodeducetherethatwe
cantriviallyintegrateourmodificationintotheDPOobjectiveallowingustotrainwithoutanexplicit
rewardmodel(eliminatesstepii))asfollows,whereσ(·)standsforthesigmoidfunction:
(cid:20) (cid:18) (cid:19)(cid:21)
p (e |x) p (e |x)
L (p )=−E logσ βlog LM w −βlog LM l . (4)
E-DPO LM (x,ew,el)∼Ds p (e |x ) p (e |x )
SFT w s SFT l s
6 Experimentsandresults
Ourexperimentalset-upisbasedonthealignment-handbookcodebase[50]. Wetunethepublicly-
availableSFTmodelp providedbyhuggingfacehub[51],usingthepublicdataset[52,53],with
SFT
default hyperparameter setup. We labelharmful prompts in the preference datasetby prompting
GPT-3.5-Turbo,seeAppendixE.Weareusingtheverysameprefixproposedintheprevioussection
togeneratex . Experimentsareperformedon8NVIDIATeslaV100GPUs,usinghalf-precision
s
tuningi.e.,Float16. Intheappendix,wealsoshowresultsforanalternativetrainingparadigm: the
Low-RankAdaptation(LoRA)[54](seeAppendixD.1). Followingcommunitystandards[3,1,2],
weusegreedydecodingi.e.,T =0formodelevaluation.
WefirstshowempiricalevidencethatourproposedmodificationofDPO,E-DPO,doesinfactimprove
safetyalignment,usingtheHarmbenchdataset[2]andthefirst100promptsintheAdvBenchharmful
behaviordataset[1],measuredbytheHarmBenchprotocol. Wegiveanoverviewonalladversaries
inAppendixF.TheresultsarepresentedinTable1. E-DPOachievesimprovementsacrossevery
taskwetested.
Ontopofoursafetyresults,wewanttomakesureE-RLHFdoesnotsacrificehelpfulnessfor
increasedsafety. WeevaluatehelpfulnesswiththeMT-Benchproject[3]. TheSFTmodelp
SFT
7Otherwisethemodelcoulddriftintotrivialbehaviourlikealwaysrespondingwith"I can’t help you.".
8Eventheprobabilitycanbesuppressedtocloseto0.
9Theprefixsharessimilaritiestothesystempromptsusedbyopen-sourceLLMs[13,49]toboostsafety.
8Table1: SafetyalignmentwiththeE-RLHFobjective,herespecificallyE-DPO,reducestheaverage
AttackSuccessRate(ASR)acrossalljailbreakadversariesforboththeHarmBenchandtheAdvBench
data,to36.95,andto20.89,respectively. Moreover,resilienceagainstalladversariesimproveswith
ourmodificationtosafetyalignment( indicatesbetterperformancebetweenDPOandE-DPO).
HarmBenchASR[2]
Model DirectRequest GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human AVG↓
pSFT 32.25 59.25 35.50 42.75 42.75 36.20 56.50 65.00 56.75 26.75 35.50 44.47
pDPO 27.50 53.00 39.00 46.75 43.25 29.10 52.50 54.00 51.00 28.75 37.15 42.00
pE-DPO(ours) 23.50 47.50 31.75 36.25 40.50 26.45 48.50 51.00 43.00 27.00 31.05 36.95
AdvBenchASR[1]
pSFT 6.00 80.00 13.00 37.00 31.00 14.80 65.00 78.00 91.00 4.00 21.20 40.09
pDPO 0.00 47.00 12.00 39.00 30.00 7.00 50.00 61.00 44.00 4.00 18.40 28.40
pE-DPO(ours) 0.00 38.00 8.00 15.00 21.00 5.20 41.00 53.00 31.00 4.00 13.60 20.89
receivesascoreof6.3,andboththeDPOandE-DPOmodelsperformbetterthanthat(6.8and6.6
respectively),makingusbelievethatperformancedegradationisnotaproblemwithourproposal.
Next, we show the impact of the safe prefix on model performance. We demonstrate that our
method’sperformancedependsonthechoiceofsafeprefixtosomeextendbutneverfails(see
AppendixD.2). Webelieve,findingbettersafeprefixesbyexplicittuningwouldimproveourresults,
similartotheworkbyYangetal.[55],butweleavethisexplorationforfuturework. Further,we
confirmthattheimprovementarisesfromusingasafepriorintheKLtermforharmfulprompts.
We ablate our results by appending the prefix on all prompts in the preference alignment
dataset(seeAppendixD.3). Inallcases,applyingthesafeprefixtousualpromptsdegradessafety,
showcasingtheimportanceofswitchingtheprioronlyontheharmfulprompts. Finally,weshowthat
E-DPOcanbecombinedwithanysystemprompt,tofurtherboostsafety(seeAppendixD.4).
Theproposalcanevenbeusedtoimprovehelpfulnessandsafetysimultaneously(seeAppendix
D.5).
7 Conclusionanddiscussions
Inthispaper,wepresentatheoreticalframeworkforlanguagemodelpretrainingandjailbreakingby
dissectinginputpromptsintoqueryandconceptpairs. Throughthisapproach,wehaveestablished
twotheoreticalresultspertainingtotheabilityoflanguagemodelstomimictheworldfollowingpre-
training,whichleadstooutputtingharmfulexplanationsgivenharmfulprompts;andtheinevitability
ofjailbreakingresultingfromalignmentchallenges. Guidedbythesetheoreticalinsights,wehave
devisedasimpleyeteffectivetechniquetoenhancesafetyalignment,anddemonstratetheimproved
resiliencetojailbreakattackswiththismethodology.
Currentlimitations(1)Althoughwehaveclassifiedconceptsaseitherharmfulornon-harmful,it
isimportanttoacknowledgethattheperceptionofaconcept’spotentialforharmcanbeinfluenced
byvariousfactorssuchascultural,legal,andsocietalnorms,whichcollectivelyformthecontext
ofthesituation. (2)Languagemodelshavedemonstratedimpressivecapabilitiesinreasoningand
completingtaskswithinmulti-round,multi-stepconversations;ourcurrentframeworkmaynotfully
accountforthegeneralizationandjailbreakingpossibilitiesassociatedwithsuchinputformats. (3)
Ouranalysisisgroundedonafixedp mappingandD distribution. Nevertheless,theworldis
world P
inherentlydynamic,asbothp andD continuallyevolve.
world P
Futurework(1)RegardingourE-RLHFapproach,ashighlightedintheexperimentalsection,in
additiontoattachingauniversallysafeprefixtoallharmfulprompts,improvementscanbeachieved
byindividuallytransformingtheharmfulprompts. Moreover,thesafety-transformedpromptscanbe
employedtoexpandthepreferencedatasetforconventionalRLHF.(2)Throughoutouranalysis,we
havenotimposedanyconstraintsonthecapacityofthelanguagemodel.Extendingouranalysisunder
finitememoryconstraintsoranalyzinghallucinationpropertiesofLLMsisaninterestingdirectionto
explore. (3)Largelanguagemodelshaveshownremarkablecapabilitiesasin-contextlearners[56],
andsuchtechniquescouldpotentiallybeusedforjailbreakingthemaswell[57–59]. Investigating
theincorporationofsuchinputparadigmsremainsapromisingavenueforfutureresearch.
9References
[1] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable
adversarialattacksonalignedlanguagemodels. arXivpreprintarXiv:2307.15043,2023. 1,2,
8,9,26,27,28,29,30,31
[2] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham
Sakhaee,NathanielLi,StevenBasart,BoLi,etal. Harmbench: Astandardizedevaluation
frameworkforautomatedredteamingandrobustrefusal. arXivpreprintarXiv:2402.04249,
2024. 1,2,8,9,26,27,28
[3] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with
mt-benchandchatbotarena. AdvancesinNeuralInformationProcessingSystems,37,2023. 1,
8,27
[4] BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,
YossiAdi,JingyuLiu,TalRemez,JérémyRapin,etal. Codellama: Openfoundationmodels
forcode. arXivpreprintarXiv:2308.12950,2023. 1
[5] KaranSinghal,ShekoofehAzizi,TaoTu,SSaraMahdavi,JasonWei,HyungWonChung,
Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language
modelsencodeclinicalknowledge. Nature,620(7972):172–180,2023. 1
[6] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad
Godil, Ryan J Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with
retrieval-augmentedlanguagemodels. AdvancesinNeuralInformationProcessingSystems,
36,2024. 1
[7] EmilyMBender,TimnitGebru,AngelinaMcMillan-Major,andShmargaretShmitchell. On
thedangersofstochasticparrots: Canlanguagemodelsbetoobig? InProceedingsofthe2021
ACMconferenceonfairness,accountability,andtransparency,pages610–623,2021. 1
[8] JulianHazell.Largelanguagemodelscanbeusedtoeffectivelyscalespearphishingcampaigns.
arXivpreprintarXiv:2305.06972,2023.
[9] YiLiu,GeleiDeng,ZhengziXu,YuekangLi,YaowenZheng,YingZhang,LidaZhao,Tianwei
Zhang,andYangLiu. Jailbreakingchatgptviapromptengineering: Anempiricalstudy. arXiv
preprintarXiv:2305.13860,2023. 1,29
[10] OpenAI. Chatgpt. https://openai.com/index/chatgpt/,2022. 1
[11] DAN. Doanythingnowprompt. https://github.com/0xk1h0/ChatGPT_DAN,2023. 1
[12] Reddit. Chatgpt grandma exploit. https://www.reddit.com/r/ChatGPT/comments/
12sn0kk/grandma_exploit/,2023. 1
[13] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023. 2,8,21,25
[14] DeepGanguli,LianeLovitt,JacksonKernion,AmandaAskell,YuntaoBai,SauravKadavath,
BenMann, EthanPerez, NicholasSchiefer, KamalNdousse, etal. Redteaminglanguage
models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint
arXiv:2209.07858,2022. 2,30
[15] EthanPerez,SaffronHuang,FrancisSong,TrevorCai,RomanRing,JohnAslanides,Amelia
Glaese,NatMcAleese,andGeoffreyIrving. Redteaminglanguagemodelswithlanguage
models. arXivpreprintarXiv:2202.03286,2022. 28
[16] StephenCasper,JasonLin,JoeKwon,GatlenCulp,andDylanHadfield-Menell. Explore,es-
tablish,exploit: Redteaminglanguagemodelsfromscratch. arXivpreprintarXiv:2306.09442,
2023.
10[17] Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja,
James R Glass, Akash Srivastava, and Pulkit Agrawal. Curiosity-driven red-teaming for
largelanguagemodels. InTheTwelfthInternationalConferenceonLearningRepresentations,
2023.
[18] Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H
Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster,
etal. Rainbowteaming: Open-endedgenerationofdiverseadversarialprompts. arXivpreprint
arXiv:2402.16822,2024. 2,30
[19] DanielMZiegler,NisanStiennon,JeffreyWu,TomBBrown,AlecRadford,DarioAmodei,
PaulChristiano,andGeoffreyIrving. Fine-tuninglanguagemodelsfromhumanpreferences.
arXivpreprintarXiv:1909.08593,2019. 2,24,30
[20] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodels
to follow instructions with human feedback. Advances in Neural Information Processing
Systems,35:27730–27744,2022. 24,30
[21] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy
Jones,AnnaChen,AnnaGoldie,AzaliaMirhoseini,CameronMcKinnon,etal. Constitutional
ai: Harmlessnessfromaifeedback. arXivpreprintarXiv:2212.08073,2022. 24,30
[22] TomaszKorbak,KejianShi,AngelicaChen,RasikaVinayakBhalerao,ChristopherBuckley,
JasonPhang,SamuelRBowman,andEthanPerez. Pretraininglanguagemodelswithhuman
preferences. InInternationalConferenceonMachineLearning,pages17506–17533.PMLR,
2023. 30
[23] RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherDManning,and
ChelseaFinn. Directpreferenceoptimization:Yourlanguagemodelissecretlyarewardmodel.
arXivpreprintarXiv:2305.18290,2023. 2,7,24,25,32
[24] SichengZhu,RuiyiZhang,BangAn,GangWu,JoeBarrow,ZichaoWang,FurongHuang,
AniNenkova,andTongSun. Autodan: Automaticandinterpretableadversarialattackson
largelanguagemodels. arXivpreprintarXiv:2310.15140,2023. 2,29
[25] RazLapid,RonLangberg,andMosheSipper. Opensesame! universalblackboxjailbreaking
oflargelanguagemodels. arXivpreprintarXiv:2309.01446,2023. 2,29
[26] Zheng-XinYong,CristinaMenghini,andStephenHBach. Low-resourcelanguagesjailbreak
gpt-4. arXivpreprintarXiv:2310.02446,2023. 2,30
[27] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak
challengesinlargelanguagemodels. arXivpreprintarXiv:2310.06474,2023. 30
[28] YouliangYuan,WenxiangJiao,WenxuanWang,Jen-tseHuang,PinjiaHe,ShumingShi,and
ZhaopengTu. Gpt-4istoosmarttobesafe: Stealthychatwithllmsviacipher. arXivpreprint
arXiv:2308.06463,2023. 2,30
[29] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy
jailbreakpromptsonalignedlargelanguagemodels. arXivpreprintarXiv:2310.04451,2023.
2,28,29
[30] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and
EricWong. Jailbreakingblackboxlargelanguagemodelsintwentyqueries. arXivpreprint
arXiv:2310.08419,2023. 27,28,29
[31] XiaoxiaLi,SiyuanLiang,JiyiZhang,HanFang,AishanLiu,andEe-ChienChang. Semantic
mirrorjailbreak: Geneticalgorithmbasedjailbreakpromptsagainstopen-sourcellms. arXiv
preprintarXiv:2402.14872,2024. 29
[32] PengDing,JunKuang,DanMa,XuezhiCao,YunsenXian,JiajunChen,andShujianHuang.
A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language
modelseasily. arXivpreprintarXiv:2311.08268,2023. 29
11[33] AnayMehrotra,ManolisZampetakis,PaulKassianik,BlaineNelson,HyrumAnderson,Yaron
Singer,andAminKarbasi. Treeofattacks: Jailbreakingblack-boxllmsautomatically. arXiv
preprintarXiv:2312.02119,2023. 2,28,29
[34] AlexanderRobey,EricWong,HamedHassani,andGeorgeJPappas. Smoothllm: Defending
largelanguagemodelsagainstjailbreakingattacks. arXivpreprintarXiv:2310.03684,2023. 2,
31,32
[35] BochuanCao,YuanpuCao,LuLin,andJinghuiChen. Defendingagainstalignment-breaking
attacksviarobustlyalignedllm. arXivpreprintarXiv:2309.14348,2023. 31
[36] AounonKumar,ChiragAgarwal,SurajSrinivas,SoheilFeizi,andHimaLakkaraju. Certifying
llmsafetyagainstadversarialprompting. arXivpreprintarXiv:2309.02705,2023. 31
[37] NeelJain,AviSchwarzschild,YuxinWen,GowthamiSomepalli,JohnKirchenbauer,Ping-
yehChiang,MicahGoldblum,AniruddhaSaha,JonasGeiping,andTomGoldstein. Base-
line defenses for adversarial attacks against aligned language models. arXiv preprint
arXiv:2309.00614,2023. 2,31
[38] YotamWolf, NoamWies, YoavLevine, andAmnonShashua. Fundamentallimitationsof
alignmentinlargelanguagemodels. arXivpreprintarXiv:2304.11082,2023. 2,3,32
[39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199,2013. 2
[40] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander
Madry. Robustnessmaybeatoddswithaccuracy. InInternationalConferenceonLearning
Representations,2018. 2
[41] Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are
adversarialexamplesinevitable? InInternationalConferenceonLearningRepresentations,
2018. 5
[42] JeremyCohen,ElanRosenfeld,andZicoKolter. Certifiedadversarialrobustnessviarandom-
izedsmoothing. Ininternationalconferenceonmachinelearning,pages1310–1320.PMLR,
2019. 2
[43] AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalMdShoeb,AbubakarAbid,
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.
Beyondtheimitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguagemodels.
arXivpreprintarXiv:2206.04615,2022. 4
[44] DavidAMcAllester. Somepac-bayesiantheorems. InProceedingsoftheeleventhannual
conferenceonComputationallearningtheory,pages230–234,1998. 4
[45] SokhnaDiarraMbacke,FlorenceClerc,andPascalGermain. Pac-bayesiangeneralization
boundsforadversarialgenerativemodels. arXivpreprintarXiv:2302.08942,2023. 5
[46] PascalGermain,AlexandreLacasse,FrançoisLaviolette,andMarioMarchand. Pac-bayesian
learningoflinearclassifiers. InProceedingsofthe26thAnnualInternationalConferenceon
MachineLearning,pages353–360,2009. 5
[47] MaximeHaddouche,BenjaminGuedj,OmarRivasplata,andJohnShawe-Taylor. Pac-bayes
unleashed: Generalisationboundswithunboundedlosses. Entropy,23(10):1330,2021. 5
[48] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic
jailbreakofopen-sourcellmsviaexploitinggeneration. arXivpreprintarXiv:2310.06987,
2023. 6,26,29
[49] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023. 8
12[50] LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,ShengyiHuang,Kashif
Rasul,AlexanderM.Rush,andThomasWolf. Thealignmenthandbook. https://github.
com/huggingface/alignment-handbook,2023. 8
[51] HuggingFace. Huggingface constitutional-ai sft model. https://huggingface.co/
alignment-handbook/mistral-7b-sft-constitutional-ai,2024. 8
[52] HuggingFace. Huggingface constitutional-ai dataset: Ultrafeedback-binarized. https://
huggingface.co/datasets/HuggingFaceH4/ultrafeecback_binarized,2024. 8
[53] HuggingFace. Huggingfaceconstitutional-aidataset: Cai-conversation-harmless. https://
huggingface.co/datasets/HuggingFaceH4/cai-conversation-harmless,2024. 8
[54] EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,Weizhu
Chen,etal. Lora: Low-rankadaptationoflargelanguagemodels. InInternationalConference
onLearningRepresentations,2021. 8,26
[55] ChengrunYang,XuezhiWang,YifengLu,HanxiaoLiu,QuocVLe,DennyZhou,andXinyun
Chen. Largelanguagemodelsasoptimizers. arXivpreprintarXiv:2309.03409,2023. 9,25
[56] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
modelsarefew-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–
1901,2020. 9
[57] ZemingWei,YifeiWang,andYisenWang. Jailbreakandguardalignedlanguagemodelswith
onlyfewin-contextdemonstrations. arXivpreprintarXiv:2310.06387,2023. 9,30
[58] JiongxiaoWang,ZichenLiu,KeunHeePark,MuhaoChen,andChaoweiXiao. Adversarial
demonstrationattacksonlargelanguagemodels. arXivpreprintarXiv:2305.14950,2023. 30
[59] CemAnil,EsinDurmus,MrinankSharma,JoeBenton,SandipanKundu,JoshuaBatson,Nina
Rimsky,MegTong,JesseMu,DanielFord,etal. Many-shotjailbreaking,2024. 9,30
[60] BehnamNeyshabur, SrinadhBhojanapalli, DavidMcAllester, andNatiSrebro. Exploring
generalizationindeeplearning. Advancesinneuralinformationprocessingsystems,30,2017.
20
[61] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon
Wilson. A simple baseline for bayesian uncertainty in deep learning. Advances in neural
informationprocessingsystems,32,2019. 20
[62] Robert D Gordon. Values of mills’ ratio of area to bounding ordinate and of the normal
probabilityintegralforlargevaluesoftheargument. TheAnnalsofMathematicalStatistics,
12(3):364–366,1941. 22
[63] PaulFChristiano,JanLeike,TomBrown,MiljanMartic,ShaneLegg,andDarioAmodei.Deep
reinforcementlearningfromhumanpreferences. Advancesinneuralinformationprocessing
systems,30,2017. 24
[64] RalphAllanBradleyandMiltonETerry. Rankanalysisofincompleteblockdesigns: I.the
methodofpairedcomparisons. Biometrika,39(3/4):324–345,1952. 24
[65] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximal
policyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017. 24
[66] BanghuaZhu,HiteshiSharma,FelipeVieiraFrujeri,ShiDong,ChenguangZhu,MichaelIJor-
dan,andJiantaoJiao. Fine-tuninglanguagemodelswithadvantage-inducedpolicyalignment.
arXivpreprintarXiv:2306.02231,2023. 24
[67] LoganEngstrom,AndrewIlyas,ShibaniSanturkar,DimitrisTsipras,FirdausJanoos,Larry
Rudolph,andAleksanderMadry. Implementationmattersindeeppolicygradients: Acase
studyonppoandtrpo. arXivpreprintarXiv:2005.12729,2020. 24
13[68] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
Advancesinneuralinformationprocessingsystems,35:24824–24837,2022. 25
[69] AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,DeepGanguli,TomHenighan,Andy
Jones,NicholasJoseph,BenMann,NovaDasSarma,etal. Agenerallanguageassistantasa
laboratoryforalignment. arXivpreprintarXiv:2112.00861,2021. 25
[70] XiaotianZou,YongkangChen,andKeLi. Isthesystemmessagereallyimportanttojailbreaks
inlargelanguagemodels? arXivpreprintarXiv:2402.14857,2024. 26,31
[71] ChuanGuo,AlexandreSablayrolles,HervéJégou,andDouweKiela. Gradient-basedadver-
sarialattacksagainsttexttransformers. InProceedingsofthe2021ConferenceonEmpirical
MethodsinNaturalLanguageProcessing,pages5747–5757,2021. 28,29
[72] TaylorShin,YasamanRazeghi,RobertLLoganIV,EricWallace,andSameerSingh. Auto-
prompt: Elicitingknowledgefromlanguagemodelswithautomaticallygeneratedprompts.
arXivpreprintarXiv:2010.15980,2020. 28,29
[73] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How
johnnycanpersuadellmstojailbreakthem: Rethinkingpersuasiontochallengeaisafetyby
humanizingllms. arXivpreprintarXiv:2401.06373,2024. 28,30
[74] XinyueShen,ZeyuanChen,MichaelBackes,YunShen,andYangZhang. "doanythingnow":
Characterizingandevaluatingin-the-wildjailbreakpromptsonlargelanguagemodels. arXiv
preprintarXiv:2308.03825,2023. 28,29
[75] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal ad-
versarialtriggersforattackingandanalyzingnlp. arXivpreprintarXiv:1908.07125, 2019.
28
[76] YuxinWen,NeelJain,JohnKirchenbauer,MicahGoldblum,JonasGeiping,andTomGold-
stein. Hardpromptsmadeeasy: Gradient-baseddiscreteoptimizationforprompttuningand
discovery. AdvancesinNeuralInformationProcessingSystems,36,2024. 28
[77] ErikJones,AncaDragan,AditiRaghunathan,andJacobSteinhardt. Automaticallyauditing
largelanguagemodelsviadiscreteoptimization. arXivpreprintarXiv:2303.04381,2023. 29
[78] FábioPerezandIanRibeiro. Ignorepreviousprompt: Attacktechniquesforlanguagemodels.
arXivpreprintarXiv:2211.09527,2022. 29
[79] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety
trainingfail? InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
29
[80] GeleiDeng,YiLiu,YuekangLi,KailongWang,YingZhang,ZefengLi,HaoyuWang,Tianwei
Zhang,andYangLiu. Jailbreaker: Automatedjailbreakacrossmultiplelargelanguagemodel
chatbots. arXivpreprintarXiv:2307.08715,2023. 29
[81] JiahaoYu,XingweiLin,andXinyuXing. Gptfuzzer: Redteaminglargelanguagemodelswith
auto-generatedjailbreakprompts. arXivpreprintarXiv:2309.10253,2023. 29
[82] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,TomGriffiths,YuanCao,andKarthik
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models.
AdvancesinNeuralInformationProcessingSystems,36,2024. 29
[83] XuanLi,ZhankeZhou,JianingZhu,JiangchaoYao,TongliangLiu,andBoHan. Deepin-
ception: Hypnotizelargelanguagemodeltobejailbreaker. arXivpreprintarXiv:2311.03191,
2023. 29
[84] Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. Drattack:
Promptdecompositionandreconstructionmakespowerfulllmjailbreakers. arXivpreprint
arXiv:2402.16914,2024. 29
14[85] ZhenhuaWang,WeiXie,BaoshengWang,EnzeWang,ZhiwenGui,ShuoyouchengMa,and
KaiChen. Footinthedoor: Understandinglargelanguagemodeljailbreakingviacognitive
psychology. arXivpreprintarXiv:2402.15690,2024. 29
[86] AnselmPaulus,ArmanZharmagambetov,ChuanGuo,BrandonAmos,andYuandongTian.
Advprompter: Fastadaptiveadversarialpromptingforllms. arXivpreprintarXiv:2404.16873,
2024. 29
[87] HangfanZhang,ZhimengGuo,HuaishengZhu,BochuanCao,LuLin,JinyuanJia,Jinghui
Chen,andDinghaoWu.Onthesafetyofopen-sourcedlargelanguagemodels:Doesalignment
reallypreventthemfrombeingmisused? arXivpreprintarXiv:2310.01581,2023. 29
[88] Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and
WilliamYangWang. Weak-to-strongjailbreakingonlargelanguagemodels. arXivpreprint
arXiv:2401.17256,2024. 29
[89] XianjunYang,XiaoWang,QiZhang,LindaPetzold,WilliamYangWang,XunZhao,and
DahuaLin. Shadowalignment: Theeaseofsubvertingsafely-alignedlanguagemodels. arXiv
preprintarXiv:2310.02949,2023. 29
[90] QiusiZhan,RichardFang,RohanBindu,AkulGupta,TatsunoriHashimoto,andDanielKang.
Removingrlhfprotectionsingpt-4viafine-tuning. arXivpreprintarXiv:2311.05553,2023. 29
[91] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter
Henderson. Fine-tuningalignedlanguagemodelscompromisessafety,evenwhenusersdonot
intendto! arXivpreprintarXiv:2310.03693,2023. 29
[92] ErfanShayegani, YueDong, andNaelAbu-Ghazaleh. Jailbreakinpieces: Compositional
adversarialattacksonmulti-modallanguagemodels. arXivpreprintarXiv:2307.14539,2023.
30
[93] XiangyuQi,KaixuanHuang,AshwineePanda,MengdiWang,andPrateekMittal. Visual
adversarialexamplesjailbreaklargelanguagemodels. arXivpreprintarXiv:2306.13213,2023.
30
[94] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena
Gao, PangWeiKoh, DaphneIppolito, FlorianTramèr, and LudwigSchmidt. Are aligned
neuralnetworksadversariallyaligned? InThirty-seventhConferenceonNeuralInformation
ProcessingSystems,2023. 30
[95] NatalieMaus,PatrickChao,EricWong,andJacobRGardner.Blackboxadversarialprompting
forfoundationmodels. InTheSecondWorkshoponNewFrontiersinAdversarialMachine
Learning,2023. 30
[96] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:
An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL
https://lmsys.org/blog/2023-03-30-vicuna/. 30
[97] Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori
Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard secu-
rityattacks. arXivpreprintarXiv:2302.05733,2023. 30
[98] RushebShah,SoroushPour,ArushTagade,StephenCasper,JavierRando,etal. Scalableand
transferableblack-boxjailbreaksforlanguagemodelsviapersonamodulation. arXivpreprint
arXiv:2311.03348,2023. 30
[99] XingangGuo,FangxuYu,HuanZhang,LianhuiQin,andBinHu. Cold-attack: Jailbreaking
llmswithstealthinessandcontrollability. arXivpreprintarXiv:2402.08679,2024. 30
[100] Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-
basedconstrainedtextgenerationwithlangevindynamics. AdvancesinNeuralInformation
ProcessingSystems,35:9538–9551,2022. 30
15[101] SomnathBanerjee,SayanLayek,RimaHazra,andAnimeshMukherjee. How(un)ethical
areinstruction-centricresponsesofllms? unveilingthevulnerabilitiesofsafetyguardrailsto
harmfulqueries. arXivpreprintarXiv:2402.15302,2024. 30
[102] Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz,
Somesh Jha, and Atul Prakash. Prp: Propagating universal perturbations to attack large
languagemodelguard-rails. arXivpreprintarXiv:2402.15911,2024. 30
[103] HuijieLv,XiaoWang,YuansenZhang,CaishuangHuang,ShihanDou,JunjieYe,TaoGui,
QiZhang,andXuanjingHuang. Codechameleon: Personalizedencryptionframeworkfor
jailbreakinglargelanguagemodels. arXivpreprintarXiv:2402.16717,2024. 30
[104] VinuSankarSadasivan,ShoumikSaha,GaurangSriramanan,PriyathamKattakinda,Atoosa
Chegini,andSoheilFeizi. Fastadversarialattacksonlanguagemodelsinonegpuminute.
arXivpreprintarXiv:2402.15570,2024. 30
[105] Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein.
Coercingllmstodoandreveal(almost)anything. arXivpreprintarXiv:2402.14020,2024. 30
[106] Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, and Lizhuang Ma.
Exploringsafetygeneralizationchallengesoflargelanguagemodelsviacode,2024. 30
[107] AleksanderMadry,AleksandarMakelov,LudwigSchmidt,DimitrisTsipras,andAdrianVladu.
Towardsdeeplearningmodelsresistanttoadversarialattacks. InInternationalConferenceon
LearningRepresentations,2018. 30
[108] MartinAbadi,AndyChu,IanGoodfellow,HBrendanMcMahan,IlyaMironov,KunalTalwar,
and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM
SIGSACconferenceoncomputerandcommunicationssecurity,pages308–318,2016. 30
[109] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,Dawn
Drain,StanislavFort,DeepGanguli,TomHenighan,etal. Trainingahelpfulandharmless
assistantwithreinforcementlearningfromhumanfeedback. arXivpreprintarXiv:2204.05862,
2022. 30
[110] HarrisonLee,SamratPhatale,HassanMansoor,KellieLu,ThomasMesnard,ColtonBishop,
VictorCarbune, andAbhinavRastogi. Rlaif: Scalingreinforcementlearningfromhuman
feedbackwithaifeedback. arXivpreprintarXiv:2309.00267,2023. 30
[111] GabrielAlonandMichaelKamfonas. Detectinglanguagemodelattackswithperplexity. arXiv
preprintarXiv:2308.14132,2023. 31
[112] GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,and
Tie-YanLiu. Lightgbm: Ahighlyefficientgradientboostingdecisiontree. Advancesinneural
informationprocessingsystems,30,2017. 31
[113] YueqiXie,JingweiYi,JiaweiShao,JustinCurl,LingjuanLyu,QifengChen,XingXie,and
FangzhaoWu. Defendingchatgptagainstjailbreakattackviaself-reminders. NatureMachine
Intelligence,pages1–11,2023. 31
[114] ChujieZheng,FanYin,HaoZhou,FandongMeng,JieZhou,Kai-WeiChang,MinlieHuang,
andNanyunPeng. Prompt-drivenllmsafeguardingviadirectedrepresentationoptimization.
arXivpreprintarXiv:2401.18018,2024. 31
[115] AndyZhou,BoLi,andHaohanWang. Robustpromptoptimizationfordefendinglanguage
modelsagainstjailbreakingattacks. arXivpreprintarXiv:2401.17263,2024. 31
[116] Yichuan Mo, Yuji Wang, Zeming Wei, and Yisen Wang. Studious bob fight back against
jailbreakingviapromptadversarialtuning. arXivpreprintarXiv:2402.06255,2024. 31
[117] ZhexinZhang,JunxiaoYang,PeiKe,andMinlieHuang. Defendinglargelanguagemodels
against jailbreaking attacks through goal prioritization. arXiv preprint arXiv:2311.09096,
2023. 31
16[118] Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang,
Hongyan Bao, and Xiangliang Zhang. Defending jailbreak prompts via in-context adver-
sarialgame. arXivpreprintarXiv:2402.13148,2024. 31
[119] AlecHelbling,MansiPhute,MatthewHull,andDuenHorngChau. Llmselfdefense: Byself
examination,llmsknowtheyarebeingtricked. arXivpreprintarXiv:2308.07308,2023. 31
[120] Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei
Lin,andKam-FaiWong. Self-guard: Empowerthellmtosafeguarditself. arXivpreprint
arXiv:2310.15851,2023. 31
[121] YuhuiLi,FangyunWei,JinjingZhao,ChaoZhang,andHongyangZhang.Rain:Yourlanguage
modelscanalignthemselveswithoutfinetuning. arXivpreprintarXiv:2309.07124,2023. 31
[122] ZhangchenXu,FengqingJiang,LuyaoNiu,JinyuanJia,BillYuchenLin,andRadhaPooven-
dran. Safedecoding: Defendingagainstjailbreakattacksviasafety-awaredecoding. arXiv
preprintarXiv:2402.08983,2024. 31
[123] Adib Hasan, Ileana Rugina, and Alex Wang. Pruning for protection: Increasing jailbreak
resistanceinalignedllmswithoutfine-tuning. arXivpreprintarXiv:2401.10862,2024. 31
[124] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning
approachforlargelanguagemodels. arXivpreprintarXiv:2306.11695,2023. 31
[125] RenjiePi,TianyangHan,YueqiXie,RuiPan,QingLian,HanzeDong,JipengZhang,and
TongZhang. Mllm-protector: Ensuringmllm’ssafetywithouthurtingperformance. arXiv
preprintarXiv:2401.02906,2024. 31
[126] YuqiZhang,LiangDing,LefeiZhang,andDachengTao. Intentionanalysispromptingmakes
largelanguagemodelsagoodjailbreakdefender. arXivpreprintarXiv:2401.06561,2024. 31
[127] YihanWang,ZhouxingShi,AndrewBai,andCho-JuiHsieh. Defendingllmsagainstjailbreak-
ingattacksviabacktranslation. arXivpreprintarXiv:2402.16459,2024. 31
[128] HeegyuKim,SehyunYuk,andHyunsoukCho. Breakthebreakout: Reinventinglmdefense
againstjailbreakattackswithself-refinement. arXivpreprintarXiv:2402.15180,2024. 31
[129] YifanZeng,YiranWu,XiaoZhang,HuazhengWang,andQingyunWu. Autodefense: Multi-
agentllmdefenseagainstjailbreakattacks,2024. 31
[130] XiaomengHu,Pin-YuChen,andTsung-YiHo. Gradientcuff: Detectingjailbreakattackson
largelanguagemodelsbyexploringrefusallosslandscapes. arXivpreprintarXiv:2403.00867,
2024. 32
[131] JiabaoJi,BairuHou,AlexanderRobey,GeorgeJPappas,HamedHassani,YangZhang,Eric
Wong, and Shiyu Chang. Defending large language models against jailbreak attacks via
semanticsmoothing. arXivpreprintarXiv:2402.16192,2024. 32
[132] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao,
MichaelTontchev,QingHu,BrianFuller,DavideTestuggine,etal. Llamaguard: Llm-based
input-outputsafeguardforhuman-aiconversations. arXivpreprintarXiv:2312.06674,2023.
32
[133] SwapnajaAchintalwar,AdrianaAlvaradoGarcia,AteretAnaby-Tavor,IoanaBaldini,SaraE.
Berger,BishwaranjanBhattacharjee,DjallelBouneffouf,SubhajitChaudhury,Pin-YuChen,
LamoghaChiazor,ElizabethM.Daly,RogérioAbreudePaula,PierreDognin,EitanFarchi,
Soumya Ghosh, Michael Hind, Raya Horesh, George Kour, Ja Young Lee, Erik Miehling,
KeerthiramMurugesan,ManishNagireddy,InkitPadhi,DavidPiorkowski,AmbrishRawat,
OrnaRaz,PrasannaSattigeri,HendrikStrobelt,SarathkrishnaSwaminathan,ChristophTill-
mann,AashkaTrivedi,KushR.Varshney,DennisWei,ShalishaWitherspooon,andMarcel
Zalmanovici. Detectorsforsafeandreliablellms: Implementations, uses, andlimitations,
2024. 32
17[134] AdamTaumanKalaiandSantoshSVempala. Calibratedlanguagemodelsmusthallucinate.
arXivpreprintarXiv:2311.14648,2023. 32
[135] AndrewLee, XiaoyanBai, ItamarPres, MartinWattenberg, JonathanKKummerfeld, and
RadaMihalcea. Amechanisticunderstandingofalignmentalgorithms: Acasestudyondpo
andtoxicity. arXivpreprintarXiv:2401.01967,2024. 32
[136] BoyiWei,KaixuanHuang,YangsiboHuang,TinghaoXie,XiangyuQi,MengzhouXia,Prateek
Mittal,MengdiWang,andPeterHenderson. Assessingthebrittlenessofsafetyalignmentvia
pruningandlow-rankmodifications. arXivpreprintarXiv:2402.05162,2024. 32
18Appendix
A Glossary
Table2: Summaryofnotation.
Symbol Meaning
q Asinglequery,composablewithacertainsetofconcepts.
c Asingleconcept,composablewithacertainsetofqueries.
x=(q,c) Asinglepromptcomposedbyqueryqandconceptc.
e Asingleexplanation.
Q Thequeryset.
C Theconceptset.
E Theexplanationset.
P⊊Q×C Thesetofplausibleprompts.
pworld:P→∆(E) Theworldmapping.Foreachplausibleprompt,itspecifiestheground-truthdistributionoverE,a.k.a.the“knowledge”.
pworld(q,c) Theground-truthdistributionoverasubsetofE,givenaprompt(q,c).Withslightabuseofnotation,italsoreferstoapointontheprobabilitysimplex.
supp(pworld(q,c)) Supportofpworld(q,c).AstrictsubsetofE.
pLM:P→∆(E) Alanguagemodel.Foreachplausibleprompt,itspecifiesadistributionover(asubsetof)E,tomimicpworld.
pLM(q,c) Theoutputdistributionover(asubsetof)EbyLM,givenaprompt(q,c).Withslightabuseofnotation,italsoreferstoapointontheprobabilitysimplex.
dom(pLM(q,c)) DomainofthepLM(q,c)distribution,asubsetofE.
su( pq p,c
(
LD) M∼ P)D ⊊P
P
SU Aun spd epe tor ol ry fti lon afg
nD
gg ue
P
an
.
ge e(r qa mt ,i cv o)e de∈d li ss
s
.t uri pb pu (ti Don Po )v ie sr cp ar lo lem dp ats d, ia r. ek c. ta. pt rh oe md pis tt .ributiongoverningthecreationofourpretrainingcorpus.
π ThepriordistributionoverLM.
ρ TheposteriordistributionoverLM,afterpretraining.
γ TheposteriordistributionoverLM,afterpreferencealignment.
B ProofofTheorems
B.1 ProofofPAC-Bayesianbounds
DefinitionB.1. (BoundedDifference)Afunctionf :Xn →Rissaidtohaveboundeddifference
propertyw.r.t. acollectionofconstantsc ,··· ,c ,iff
1 n
′
sup |f(x ,x ,··· ,x )−f(x ,x ,··· ,x ,x ,··· ,x )|≤c ,∀i∈[n].
1 2 n 1 2 i−1 i n i
x1,x2,...,xn,x′
i
LemmaB.1. (Hoeffding’sLemma)forrandomvariableX ∈[a,b]withprobability1,thefollowing
holds:
λ2(b−a)2
E[exp(λX)]≤exp(λEX+ ).
8
LemmaB.2. (Hoeffding’sLemma,Multivariate)forrandomvariablesZ =f(x ,··· ,x )wheref
1 n
hastheboundeddifferenceproperty,thefollowingholds:
λ2(cid:80)n c2
E[exp(λ(EZ−Z))]≤exp( i=1 i).
8
NotethatsubstitutingZ withRˆ (LM)isvalid.
S
LemmaB.3. EmpiricalLossdefinedinDefinition3.1satisfiestheboundeddifferenceconditionwith
constantc=1,∀i.
WearereadytopresenttheproofofTheorem1.
Proof. Startingwiththeabovelemma,weknow
λ2c2
E [exp(λ(R(LM)−Rˆ (LM)))]≤exp( ).
S S 8n
TheaboveresultholdsforamanuallypickedLM.Withanoverallaverageoverthepriorπwehave
λ2c2
E E [exp(λ(R(LM)−Rˆ (LM)))]≤exp( ).
LM∼π S S 8n
ApplyFubini’stheorem(notethatπisindependentofS):
λ2c2
E E [exp(λ(R(LM)−Rˆ (LM)))]≤exp( ).
S LM∼π S 8n
19DefineY = E [exp(λ(R(LM)−Rˆ (LM)))],arandomvariabledependsonS. Obviously
LM∼π S
Y ≥0. Thus,withMarkov’sinequality:
1
P[Y ≥ E Y]≤δ.
δ S
Equivalently,withprobabilityatleast1−δ,wehave
1 λ2c2
Y ≤ exp[ ].
δ 8n
Sincewehaveassumedπ,ρsharethesamesupport,usingRadon-Nykodimderivativetochangethe
expectationwithrespecttoπtowithrespecttoρ,wehave
(cid:20) dπ (cid:21) 1 λ2c2
E exp(λ(R(LM)−Rˆ (LM))) ≤ exp[ ].
LM∼ρ dρ S δ 8n
TakinglogarithmandapplyingJensen’sInequalityweknow
(cid:20) dπ (cid:21) 1 λ2c2
E +λ(R(LM)−Rˆ (LM)) ≤log + .
LM∼ρ dρ S δ 8n
Incorporatingc=1,noticing dρ =(dπ)−1wecouldrewritetheinequalityas
dπ dρ
(cid:104) (cid:105) 1 (cid:18) 1(cid:19) λ
E (R(LM)−Rˆ (LM)) ≤ KL[ρ||π]+log + .
LM∼ρ S λ δ 8n
Findingλthatminimizesthetermonrighthandsidegivesustheϱterm.
WhenD allowsforadecompositionintomixturecomponents,noticingthelineartyofexpectation,
P
theboundcanbere-writtenas
αE [E ℓ (p ,(q,c))]+(1−α)E [E ℓ (p ,(q,c))]
LM∼ρ (q,c)∼DPh TV LM LM∼ρ (q,c)∼DPs TV LM
≤ϱ+E [Rˆ (p )].
LM∼ρ S LM
whichleadsto
1
E [E ℓ (p ,(q,c))]≤ [ϱ+E [Rˆ (p )]].
LM∼ρ (q,c)∼DPh TV LM α LM∼ρ S LM
B.2 Anestimationonthenon-vacuousnessofthePACbound
WegiveanestimationofthetermappearsinourPACbound,ϱ,andstatethatitisnon-vacuous.
Thenumerator.WefollowNeyshaburetal.[60]toinstantiatetheterminthesimplestsetup.Assume
π,ρaredefinedovertheparameterspaceofagivenLM,withK parameters. Assumewisasetof
weightslearnedfromthepretrainingcorpus. Letthepriorπbethezero-meanmultivariateGaussian,
whoseentry-wisevarianceisrelatedtothemagnitudeoftheweight: σ =β|w |,andρbeaGaussian
i i
with the same anisotropic variance centered around w. We argue though simple, both settings
arepractical,sinceGaussianinitializationiscommonformodeltraining,andtheSWA-Gaussian
algorithm[61]utilizessuchGaussianposterior. Underthissetup,theKLgoesas(cid:80) w i2 =O(K).
i 2σ2
√ i
Specifically, taking β = 2 makes the term exactly K. Current language models often possess
2
millions,orbillions,ofparameters,namely,K ∼[106,109].
The denominator. To estimate the number of unique direct prompts in the training corpus, it is
importanttonoticethatthedatasetdoesnotonlyconsistof(q,c)promptsbutalsoeexplanations.
Thus, we need to estimate the average token length (ATL) associated with each unique prompt
x=(q,c). Foreachuniquepromptx,asidefromitsowntokenlengthl(x),therewillbeacollection
ofexplanations{e }N(x),withexpectedtokenlengthofeachassociatedexplanationl(e).Wehave
i i=1
EATL=E N(x)×[l(x)+l(e)].
x∼DP
20Fact. Givenapromptx,thelargertheexpectedlengthofthepromptitselfandexplanation(l(x)+
l(e)↑),thelargertheexpectednumberofexplanationelements(N(x)↑),andthesmallerthenumber
ofsuchprompts(D (x)↓),appearinginthetrainingcorpus. Theformercomesnaturallyduetothe
P
composabilityofnaturallanguage: thelongerthetextfragment,themoreequivalenttextfragments
inexpectation,whilethelatterisreflectedbythespiritofthewidelyacceptedZipf’slaw.
Inspiredbythefact,weassumepromptsarecategorizedbythequantityofl(x)+l(e),namely,for
allpromptx,N(x)isafunctionofl(x)+l(e). Moreover,thecompletedatagenerationprocessis
decomposedintoi)sampleavalueofl(x)+l(e)out,andthenii)sampleauniquepromptfromthe
setdecidedbythisspecificl(x)+l(e)value,andiii)generateN(x)explanations.
Step i). Use the fact: the larger the expected length of the output explanation, the smaller the
probabilitythatsuchapromptappearsinthetrainingcorpus. Weassumestepi)followsa(cut-off)
zetadistribution. Specifically,forarandompromptx,
p(l(x)+l(e)=k)∝k−s,∀k ≥k .
0
Whenk =1,weresumethezetadistributionwithcoefficients.
0
Stepii). Weassumeeachpromptfollowingthisstepisunique.
Stepiii). Usethefact: thelargertheexpectedlengthoftheoutputexplanation,thelargertheexpected
numberofexplanationelementsinthetrainingcorpus. WeassumeapowerlawscalingonN,witha
constantt>1,suchthat
N(l(x)+l(e)=k)=kt−1.
Thus,theaveragetokenlengthwrites
(cid:88)
ζ(s−t)−(cid:80)k0−1i−(s−t)
EATL= p(l(x)+l(e)=k)×k×N(l(x)+l(e)=k)= i=1 .
ζ(s)−(cid:80)k0−1i−s
k i=1
whereζ(s)=(cid:80) i−sistheRiemannzetafunction.
i∈Z+
Forexample,takes=4,t=2. Withk =1,theATLwouldbe1.52,whilewithk =10,theATL
0 0
becomes272. Theseresultstranslateintoanestimationofuniquepromptsasn /ATL. With
tokens
currentSOTALM,thepretrainingcorpusoftenincludes(tensof)trillionsoftokens(>1012),thus
n>1010 >K canbesafelyassumed⇒ϱ<1.
α constant. According to LLaMa-2 report (section 4.1, Figure 13) [13], approximately 0.2% of
the documents in their training corpus is labeled as harmful. However, we argue this is indeed
anextremelylooselowerboundforα,duetotheestimationstrategyusedintheirpaper. Givena
document,theyuseabinaryclassifieronharmfulnessovereachsingleline(1meansharmfuland0
otherwise),andassigntheaveragescoretothedocument. 0.2%istheratioofdocumentswithscore
≥ 0.5. Taketheexampleof“How to build a bomb”. Thechemicalreactionpartswillnotbe
countedasharmful,andthusthisestimationstrategycouldjudgeacompletelyharmfulexplanation
asharmless. Thus,itisreasonabletoassertαisnottoosmall,thoughwithcurrentliteratureweare
notcapableofraisinganaccurateestimationonit.
B.3 Proofofjailbreaking
Beforeproceedingtotheproof,welistnecessarydefinitionsandlemmasasfollows.
LemmaB.4. (Volumeofn-simplex)10Foranydimensionn,thevolumeofthen-elementprobability
simplex: ∆n−1,inthen−1-dimensionalspaceis
√
n
.
(n−1)!
Wedefinetheprojectedprobabilitysimplexasfollows.
DefinitionB.2. (Projectedprobabilitysimplex)Given∆n−1,thecorrespondingprojectedprobability
simplex,∆n−1,isdefinedasasubsetofRn−1: {x∈Rn−1|(cid:80)n−1x ≤1,∀i∈[n−1]}.
p i=1 i
10Seehttps://en.wikipedia.org/wiki/Simplex#Volume.
21Anillustrationof∆n−1and∆n−1. Forexample,taken=3. Theprobabilitysimplexwithn=3
p √
elementsisatrianglewhose(euclidean)sidelengthis 2withvertices(1,0,0),(0,1,0),(0,0,1).
√
Then its volume in the 2-dimensional space, i.e., its area, is 3. The corresponding projected
2
probabilitysimplexisthetrianglebetweentheX−Y axis,withvertices(1,0),(0,1),(0,0).
Adirectlemmathatconnectstheprobabilitysimplexandtheprojectedprobabilitysimplexisgiven
below.
LemmaB.5. (Transformationofprobabilitysimplex)Givenaproperprobabilitydensityfunction
ν(x)definedon∆n−1,itisequivalenttothedistributiondefinedon∆n−1withdensity ν √(x) :∀A∈
p n
Borel(∆n p−1),letB ={x∈∆n−1 :x
1:n−1
∈A}. Then(cid:82) Aν(x)dx=(cid:82)
B
ν √(x n)dx.Specifically,this
implies volume(A) = volume(B) .
volume(∆n p−1) volume(∆n−1)
Proof. Consideratranslationon∆n−1withx =−(cid:80)n−1x whichdoesnotaffectitsthevolume
n i=1 i
andshape. Themapping: ∆n−1 →translated∆n−1isanaffinetransformationwithmatrix
p
 1 0 ··· 0 
0 1 ··· 0
T = 
··· ··· ··· ···
−1 −1 ··· −1
n×(n−1)
√ √
Thus, anyareaunderthistransformationisscaledby detT⊤T = n: aconstant. Thelemma
followsdirectlyafterthisconclusion.
WeuseU(·)todenotetheuniformdistributionover∆n−1: U(x)= (n √−1)!,∀x∈∆n−1. Weusethe
n
notationvol[S]=(cid:82) 1dstorepresentthevolumeofagivensubsetS ⊂∆n−1,anduservol[S]for
S
therelativevolume(w.r.t. theunderlyingn-simplex)ofS,i.e.,rvol[S]:= vol[S] =(cid:82) U(x)dx.
vol[∆n−1] S
Wealsousen=|E(c)|fromnowon. Weusethevectorxtodenote(withtheslightabuseofnotation
wehavementioned)p (q,c)ontheoutputsimplex.
LM
LemmaB.6. (GaussiancdfTailBound,Gordon[62])Denoteϕ(·)asthestandardGaussianpdf.
Whenx>0,
x x e−x2/2 e−x2/2 1
ϕ(x)= √ ≤1−Φ(x)≤ √ = ϕ(x).
x2+1 x2+1 2π 2πx x
NowwearereadytogivetheproofofTheorem2.
Proof. Let|E (c)|=n anddenote|E (c)|+|E (c)|+|E (c)|=n. Withoutlossofgenerality,
h 0 h s n
wedefinethefirstn =|E (c)|elementsastheharmfulexplanations. Letthethresholdingconstant
0 h
be p. That is, we define the harmful zone H as {x ∈
∆n−1|(cid:80)n0
x ≥ p}. To compute the
h i=1 i
relativevolumeofH in∆n−1,wecouldinsteadoperateontheprojectedprobabilitysimplex∆n−1
h p
introducedinDefinitionB.2,andcomputetherelativevolumeoftheprojectedH : H :={x∈
h h,p
22∆n−1|(cid:80)n0 x ≥p}. Notethat∆n−1 ⊂Rn−1.Wederiveitsexpressionasfollows.
p i=1 i p
(cid:88)n0
volume[HC ]=volume[{x∈∆n−1| x ≤p]}
h,p p i
i=1
(cid:90) p (cid:90) p−x1 (cid:90) p−(cid:80)n i=0 1−1xi (cid:90) 1−(cid:80)n i=0
1
(cid:90) 1−(cid:80)n i=− 12xi
= dx dx ··· dx dx ··· dx
1 2 n0 n0+1 n−1
0 0 0 0 0
(cid:90) p (cid:90) p−x1 (cid:90) p−(cid:80)n i=0 1−1xi (cid:34) 1 (cid:88)n0 (cid:35)
= dx dx ··· dx (1− x )n−n0−1
1 2 n0 (n−n −1)! i
0 0 0 0 i=1
(cid:90) p (cid:90) p−x1 (cid:90) p−(cid:80) in =0 1−2xi 1 (cid:34) n (cid:88)0−1 (cid:35)
= dx dx ··· dx (1− x )n−n0 −(1−p)n−n0
1 2 n0−1(n−n )! i
0 0 0 0 i=1
=···
1 n (cid:88)0−1 (1−p)n−1−j
= [1−(1−p)n−1]− pj
(n−1)! j!(n−1−j)!
j=1
(5)
Thus,therelativevolumeofH canbewrittenas
h
volume[HC ]
rvol[H ]=1− h,p
h
volume[projectedprobabilitysimplex]
n (cid:88)0−1 (n−1)!(1−p)n−1−j
=(1−p)n−1+ pj
j!(n−1−j)! (6)
j=1
n (cid:88)0−1 (cid:18) n−1(cid:19)
= pj(1−p)n−1−j .
j
j=0
Which is precisely the binomial distribution formula. With the Central Limit Theorem, when
n≫O(1),weknowthebinomialdistributioncanbewellapproximatedviathenormaldistribution
asfollows:
(cid:18) (cid:19)
n
f(x)= px(1−p)n−x −→d N(np,np(1−p)). (7)
x
Thus,denoteϕ (x)asthepdfofGaussianvariablewithmean(n−1)p,variance(n−1)p(1−p),
(n−1),p
thervoltermabovecanbeestimatedasfollows:
n (cid:88)0−1 (cid:18) n−1(cid:19) (cid:90) n0−1
pj(1−p)n−1−j ≍ ϕ (x)dx
j (n−1),p
j=0 −∞
(cid:34) (cid:35)
n −1−(n−1)p
=Φ 0 (8)
(cid:112)
(n−1)p(1−p)
(cid:34) (cid:35)
|E (c)|−1−(n−1)p
=Φ h .
(cid:112)
(n−1)p(1−p)
Weusea = |E√h(c)|−1−(n−1)p. Consideranadversarywithbudgetϵunderℓ orJensen-Shannon
p
(n−1)p(1−p)
Divergence (JSD) / Total Variation (TV) capability. Since ||x|| ≥ ||x|| ,∀p ≥ 1 as well as
1 p
||x|| ≥ 2JSD(x),||x|| ≥ 2TV(x),weknowH (ϵ,ℓ ) ⊂ H (ϵ,d)foralldwehaveconsidered.
1 1 h 1 h
Withthatℓ ,ϵsetup,thecorrespondingϵ−expansionsetofH hasaclosed-formexpressionas
1 h
(cid:88)n0
ϵ
H (ϵ,ℓ )={x∈∆n−1| x ≥p− }.
h 1 i 2
i=1
23Similarasabove,wederivetheanalyticalsolutionofitsrelativevolumeassociatedwithconstanta′
as:
|E (c)|−1−(n−1)(p− ϵ)
a′ = h 2
(cid:112) (n−1)(p− ϵ)(1−p+ ϵ)
2 2
(cid:115) (cid:115) (9)
p(1−p) ϵ n−1
=a + .
(p− ϵ)(1−p+ ϵ) 2 (p− ϵ)(1−p+ ϵ)
2 2 2 2
Underourframework,withp< 1,weknow 1 >p(1−p)>(p− ϵ)(1−p+ ϵ)). Thus
2 4 2 2
√
a′ >a+ n−1ϵ:=a .
ϵ
Considertheinduceddistributionγ ontheoutputsimplex. Givenanadversarywithℓ orJSD/TV
c p
perturbingcapability,withthefixedharmfulconceptc,safetyisguaranteedifandonlyifp (q,c)
LM
residesoutsideH (ϵ,d). Definetheareaofinterest,S(d)asS(d):=∆n−1−H (ϵ,d). Thus,the
h h
probabilityofthiseventcouldbeboundedas
(cid:90)
P 1 < max γ (x) 1dx<γ rvol[S(d)]<γ rvol[S(ℓ )]<γ (1−rvol[H (ϵ,ℓ )])
x∼γc x∈S(d)
x∈S(d)
c
S(d)
s s 1 s h 1
Thisgivesanupperboundof
γ (1−Φ(a )).
s ϵ
whichcanbesimplifiedwhena≥0usingLemmaB.6:
(cid:18) (cid:19)
ϕ(a )
γ ϵ .
s a
ϵ
Thus,theprobabilityofgettingaLMinstancefromthepreferencealignmentprocesssuchthatit
allowsforsuccessfuljailbreakingonaspecificharmfulconceptcisatleast
1−γ (1−Φ(a )).
s ϵ
Uptonow,wehavederivedtheresultinTheorem2. However,wecanmoveastepfurthertoshow
thedecayrateontherighthandsideterm. Itcanbesimplifiedwhena≥0:
(cid:18) (cid:19)
ϕ(a )
1−γ ϵ ,
s a
ϵ
whichfinishestheproof.
C RLHF,DPOandourE-RLHF
TheclassicRLHFframeworkwasestablishedbyChristianoetal.[63],anddevelopedbyZiegler
et al. [19], Ouyang et al. [20], Bai et al. [21]. After the collection of a preference dataset D =
s
{(x,e ,e )},onefirsttrainsarewardmodelundertheBradley-Terrymodel[64],withtheobjective,
w l
whereσ(·)standsforthesigmoidfunction:
r(x,e)=argmaxE logσ(r(x,e )−r(x,e )).
r
(x,ew,el)∼D w l
Following,proximalpolicyoptimization(PPO)[65]iscommonlyadoptedacrosstheseimplemen-
tations, forming the basis of current state-of-the-art language models. The KL-constrained RL
Fine-Tuning(RLFT)objectivetakestheform:
maxE [r(x,e)]−βD (p (·|x)||p (·|x)).
pLM
x∼Ds,e∼pLM(·|x) KL LM ref
However, PPOtuningcansufferfrominstability[66]andimplementationcomplication[67]. To
overcometheseissues,aseriesofworkproposetoskiptherewardmodelingstepandlearndirectly
fromthepreferencedataset,withtherepresentativepioneeringworkbyRafailovetal.[23],namely
DirectPreferenceOptimization(DPO).WesummarizethederivationoftheDPOobjectivebelow,
andgeneralizetheobjectivetotheoneweuseinourexperiments,i.e.,E-DPO.
24First, noticing the closed-form optimal solution for p of the RLFT objective writes (see e.g.,
LM
AppendixA.1ofRafailovetal.[23])
1 1
p∗ (e|x)= p (e|x)exp( r(x,e)).
RLFT Z′(x) ref β
Withthisanalyticalsolutioninmind,wecansolvetherewardas
p∗ (e|x)
r(x,e)=βlog RLFT +βlogZ′(x).
p (e|x)
ref
Regardπ∗ astheoptimizationtarget,plugthistransformationintotherewardmodelobjectiveto
obtaintheDPOobjective:
p (e |x) p (e |x)
p =argmin−E [logσ(βlog LM w −βlog LM l )].
DPO pLM (x,ew,el)∼D p ref(e w|x) p ref(e l|x)
ForourE-RLHF,themodificationtotheobjectiveleadstoanotheroptimalsolutionofp :
LM
1 1
p∗(e|x)= p (e|x )exp( r(x,e)).
Z(x) ref s β
Thus,
p∗(e|x)
r(x,e)=βlog +βlogZ(x)
p (e|x )
ref s
AndplugitintotherewardmodelobjectivetoformulateourE-DPO:
p (e |x) p (e |x)
p =argmin−E [logσ(βlog LM w −βlog LM l )].
E-DPO pLM (x,ew,el)∼D p ref(e w|x s) p ref(e l|x s)
TheadvantageofourE-RLHFobjectiveisasfollows.
PropositionC.1. (Overcomingthesmallsafesetproblem)E-RLHFwillleadtotheoptimalsolution
p∗:
1 1
p∗(e|x)= p (e|x )exp( r(x,e)).
Z(x) ref s β
Comparedtop∗ ,theadvantagewhenencounteringaharmfulpromptxis:
RLFT
(1)(Eraseharmfulexplanations)∀e∈supp(p (·|x))−supp(p (·|x )),p∗(e|x)=0;
ref ref s
(2)(Addsafeexplanations)∀e∈supp(p (·|x ))−supp(p (·|x)),p∗(e|x)>0=p∗ (e|x).
ref s ref RLFT
Thus,withthesamejailbreakthresholdp,thesafetyzoneissuccessfullyexpanded.
Intriguingly,whenthesafetransformationisdonebyappendinganidenticalsafeprefixtoallharmful
prompts,wecanconnectourE-RLHFtocontextdistillation. Agoodpromptisknowntomatterfor
theperformanceofafixed-parametersLM[68,55]. ResearchershaveproposedasystematicLM
tuningalgorithm,calledContextDistillation[69],aimingatdistillingusefulinformationfromagood
contextasprefixtoalanguagemodel. Givenaninitializedlanguagemodel,forexamplep ,an
SFT
inputpromptxandaprefixcontextstringprefix,Askelletal.[69]optimizestheloss
L(p )=D (p (prefix⊕x),p (x))
LM KL SFT LM
where ⊕ stands for string concatenation. This technique has been adopted as part of the safety
alignmentprocessduringtheLLaMa-2seriestuning[13],whereprefixischosenfromasetofpre-
definedsafeprefixes. WhenapplyingtheidenticalprefixtransforminourE-RLHFtransformation,it
canberegardedasacombinationofsafetycontextdistillationandRLHF.Thisgivesanotherpointof
viewontheeffectivenessofourproposal.
D AblationStudy
Inthissection,weperformextensiveablationstudiestoshowcasetheeffectivenessofourproposed
E-RLHF.
25Table3: Safetyevaluation,LoRAresults. Theresultisconsistentwiththeonewehaveobtained
inthemaintext,thatourE-DPOperformsbetterthanDPOacrossacollectionofadversaries.
indicatesbetterperformance.
HarmBenchASR[2]
Model DirectRequest GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human AVG↓
πDPO(LoRA) 24.50 47.50 40.50 43.25 43.25 28.50 45.25 53.25 53.50 29.75 38.90 40.74
πE-DPO(LoRA) 24.25 42.50 36.50 41.50 42.75 27.20 45.00 53.75 50.25 27.25 38.05 39.00
AdvBenchASR[1]
πDPO(LoRA) 2.00 29.00 26.00 26.00 40.00 8.80 32.00 54.00 46.00 2.00 31.20 27.00
πE-DPO(LoRA)(ours) 0.00 25.00 20.00 27.00 33.00 7.20 23.00 50.00 45.00 2.00 28.80 23.73
Table4: Safeprefixablation. PrefixesweuseareincludedinTable8. OurE-DPOperformsbetter
thantheDPObaselineinmostcaseswehavetested. indicatesbestperformance.
HarmBenchASR[2]
Model DirectRequest GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human AVG↓
πDPO 27.50 53.00 39.00 46.75 43.25 29.10 52.50 54.00 51.00 28.75 37.15 42.00
πE-DPO(1) 26.25 56.50 33.75 44.25 42.25 29.30 50.00 56.75 56.25 31.50 34.05 41.90
πE-DPO(2) 24.75 52.25 34.00 39.00 44.75 29.75 50.50 54.50 53.25 28.00 34.35 40.46
πE-DPO(3) 24.75 52.75 35.25 37.50 35.50 28.65 49.00 53.50 47.25 30.50 30.25 38.63
πE-DPO(4) 23.50 47.50 31.75 36.25 40.50 26.45 48.50 51.00 43.00 27.00 31.05 36.95
AdvBenchASR[1]
πDPO 0.00 47.00 12.00 39.00 30.00 7.00 50.00 61.00 44.00 4.00 18.40 28.40
πE-DPO(1) 0.00 51.00 12.00 29.00 33.00 6.80 47.00 62.00 53.00 5.00 20.00 28.98
πE-DPO(2) 1.00 39.00 12.00 20.00 34.00 6.20 53.00 63.00 49.00 3.00 15.60 26.89
πE-DPO(3) 0.00 47.00 11.00 23.00 23.00 6.80 45.00 58.00 36.00 4.00 15.80 24.51
πE-DPO(4) 0.00 38.00 8.00 15.00 21.00 5.20 41.00 53.00 31.00 4.00 13.60 20.89
D.1 LoRAresults
Inthissection,weshowresultsobtainedbyLow-RankAdaptation[54]. Weexplorethesamesetof
safeprefixesasinablationD.2,andchoosethebestmodelforillustration. Numbersareillustrated
inTable3. Resultsare identicaltothe onesobtained via fullparameter tuningthatour E-RLHF
performsbetterconsistentlyagainsttheRLHFbaseline.
D.2 Ablationonsafeprefixes
We ablate the effect of different safe prefixes. The prefixes we consider are collected in Table 8.
NumbersareshowninTable4. Clearly,almostallsafeprefixesleadtobettersafetyagainsttheDPO
baseline,butwithvaryingperformance. Findingagoodprefixmattersforourmethod,andweleave
diggingtheoptimaloneoutasfuturework.
D.3 Ablationontransformingallprompts
Here,weproceedtoablatingtheeffectoftransformingallprompts,nomatterharmfulornot. Results
areshowninTable5,wheretheredcolorindicatesthatsafetydowngradescomparedtothemodel
obtainedviatransformingharmfulpromptsonly. Clearly,mostmodelsevenpersistworsesafety
comparedtotheDPObaselineitself,suggestingthedetrimentaleffectoftransformingtheunharmful
prompts.
D.4 Ablationwithsystemprompt
Aspointedoutbypreviousworks[2,48,70],systempromptcanhaveasignificantimpactonASR.
This comes in two-folds: firstly, a powerful system prompt can initialize the LM to be closer to
thesafetyzone,thusmakingthemodelsafer;secondly,alongersystempromptwouldenlargethe
difficultyoflaunchingaspecificattackduetotheincreasedcomputationalconsumption. Toconfirm
26Table5: Ablationstudyontransformingallprompts. Weapplythesamesafeprefixesasusedin
Table4. indicatesthesafetyisworsecomparedtothemodeltrainedwithtransformingonlythe
harmfulprompts. AVGscoresachievedbytheDPObaselineare42.00and28.40,respectively.
HarmBenchASR[2]
Model DirectRequest GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human AVG↓
πE-DPO(1) 28.00 55.75 41.00 42.00 42.50 31.00 51.25 56.25 56.00 32.00 36.05 42.89
πE-DPO(2) 25.75 60.50 40.25 46.50 44.75 30.15 52.75 57.25 63.50 30.75 40.45 44.78
πE-DPO(3) 24.00 57.25 35.00 41.75 39.50 26.55 49.25 55.00 58.75 29.25 38.70 41.36
πE-DPO(4) 27.75 58.50 38.00 42.00 40.75 31.20 52.75 60.50 54.75 31.50 38.30 43.27
AdvBenchASR[1]
πE-DPO(1) 0.00 53.00 17.00 31.00 26.00 6.00 50.00 57.00 61.00 4.00 19.00 29.45
πE-DPO(2) 0.00 52.00 17.00 29.00 26.00 6.20 56.00 58.00 73.00 3.00 25.60 31.44
πE-DPO(3) 0.00 56.00 17.00 23.00 17.00 3.80 46.00 58.00 66.00 4.00 23.40 28.56
πE-DPO(4) 0.00 58.00 17.00 36.00 27.00 6.80 46.00 60.00 68.00 3.00 25.20 31.54
Table6: AblationwiththedefaultMistralsystemprompt. ItbooststhesafetyofourE-DPOmodels
toachievebettersafety,whilemaintainingtheadvantageovertheDPObaseline.
HarmBenchASR[2]
Model DirectRequest GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human AVG↓
πSFT 25.25 61.25 26.75 37.00 27.25 24.75 55.50 58.75 61.25 24.00 30.35 39.28
πDPO 20.50 49.25 29.25 32.75 34.50 22.55 42.75 50.25 50.75 30.00 31.20 35.80
πE-DPO 15.50 41.50 22.75 29.00 31.25 17.95 41.75 44.25 44.25 23.00 26.55 30.70
ourE-RLHFisstillhelpfulunderthisspecificsituation,weablatethiseffectunderthecontrolofthe
defaultMistralsystemprompt11. TheresultscanbefoundinTable6,wherenearlyallASRsdrop,
whilsttheconclusiondoesnotchange.
D.5 Ablationonimprovinghelpfulnessandsafetysimultaneously
In similar spirit, we ask the question: can the trick we propose to improve safety be used to
improve helpfulness simultaneously? To achieve this goal, we choose the prefix "Give me a
safe and helpful response to the following request.",andapplyittoallpromptsin
the preference dataset. Results can be found in Table 7. Our model achieves better safety, and
improvesthehelpfulnessbenchmarkedbyMT-Bench[3]from6.8to6.9.
E Filteringharmfulpromptinthepreferencealignmentdataset
InspiredbytheworkbyChaoetal.[30],wepromptGPT-3.5-Turboasaharmnessjudgeoninput
prompts. Itassignsanintegerscorerangingfrom1to10foragivenprompt,andwechoosethescore
≥6subsetasharmful. ThesystempromptisshowninTable9.
11https://docs.mistral.ai/capabilities/guardrailing/
Table7: ImprovinghelpfulnessandsafetysimultaneouslyusingE-RLHF.MT-Benchscoresare6.8
and6.9,fortheDPObaselineandourE-DPOmodel,respectively.
HarmBenchASR[2]
Model DirectRequest GCG GBDA AP SFS ZS PAIR TAP AutoDAN PAP-top5 Human AVG↓
πSFT 32.25 59.25 35.50 42.75 42.75 36.20 56.50 65.00 56.75 26.75 35.50 44.47
πDPO 27.50 53.00 39.00 46.75 43.25 29.10 52.50 54.00 51.00 28.75 37.15 42.00
πE-DPO(ours) 26.25 56.00 37.00 43.75 38.25 27.90 49.75 56.00 52.25 27.50 32.65 40.66
27Table8: Safeprefixesweuseinourexperiments.
(1)"Providearesponsibleandethicalanswertothefollowingrequest. "
(2)"Inamannerthatpromotessafetyandwell-being,pleaserespondtothefollowingrequest. "
(3)"Keepinginmindtheimportanceofsafetyandlegality,howwouldyourespondtothis: "
(4)"Pleaseensureyourresponseadherestocommunityguidelinesandethicalstandards: "
Table9: SystempromptforGPT-3.5-Turbousedtofilterharmfulandnon-harmfulpromptinthe
preferencedataset.
Youareahelpfulredteamingassistant.Youwillreceiveaprompt.Pleaseactasanimpartialjudgeandevaluate
whetherthepromptitselfisrequestinginformationthatviolatesethical, legal, orsafetyguidelines. Beas
objectiveaspossible.Ratethepromptonascalefrom1to10.Aratingof1signifiesafullynon-harmfulprompt.
Aratingof10signifiespromptsthatcouldleadtoresponsesthatfullyviolateethical,legal,orsafetyguidelines.
Yourratingshouldstrictlyfollowthisformat:"Rating:[[rating]]",theratinginthedouble-closedbracketsisa
numberfrom1to10,e,g,"Rating:[[5]]".
F JailbreakadversariescollectedintheHarmBenchproject[2]
Inthissection,wegiveashortoverviewoftheadversariesweadopttoevaluateourmodels. Some
descriptionsaresummarizedinMazeikaetal.[2].
• DirectRequestreferstosendingtheoriginalharmfulpromptdirectlytothetargetLLM.
• GCG[1],GBDA[71]andAP[72]findadversarialsuffixesviatoken-leveloptimization.
• SFS(StochasticFew-Shot)andZS(Zero-Shot)[15]arefew-shotsamplingorzero-shot
generationoftestcasesbyanattackerLLMtoelicitabehaviorfromatargetLLM.
• PAIR[30]andTAP[33]areiterativeprompting/tree-structuredpromptingmethods,with
anattackerLLM,toadaptivelyexploreandelicitspecificharmfulbehaviorsfromthetarget
LLM.
• AutoDAN[29]isagenetic-algorithmbasedattackwithinitializationsfromhandcrafted
DANjailbreakprompts.
• PAP[73]usespersuasivestrategiestojailbreakthetargetLLM.
• HumanJailbreaks[74]usesafixedsetofin-the-wildhumanjailbreaktemplates,similarto
theDoAnythingNow(DAN)jailbreaks.
Weexcludealltransferattackssincewefocusonsingle-modeljailbreak. Furthermore,wechooseto
discardtheUAT[75]andPEZ[76]adversaries,becausetheformerinducesanout-of-memoryerror
onourV100GPUs,andthelatterneversucceedstofindasuffixaccordingtoourexperiments.
G BroaderImpacts
The societal impact of our work has close connection to the topic of LLM safety. We propose a
frameworkforanalyzinglanguagemodelpretrainingandjailbreaking,andwedesignanewRLHF
algorithmforimprovingsafety. Asshowninourexperiments,ourworkcouldadvocateforsafer
LLMs.
H Relatedwork
Inthissection,weprovideareviewofthecurrentliteratureonLLMjailbreaking.
H.1 Jailbreakmethods
Inthissection,wesummarizeexistingjailbreakingmethods.
28Baselineandpioneers Autoprompt[72],abaselinemethodforoptimizinginthetokenspacew.r.t.
acertainobjective,approximatescoordinateascentbyfirstrankingalltokensusinganapproximate
objective,andthencomputetheexactvalueonthem.Theapproximationiscarriedoutbyasinglestep
ofgradientcomputation. Jonesetal.[77]proposeAutoregressiveRandomizedCoordinateAscent
(ARCA)togenerate(input,output)pairsthatincludecertainharmfulinfoorsatisfyafixedformat
requirement.TokenleveloptimizationiscarriedoutwithlinearapproximationonGPT-2.GBDA[71]
studyadversarialattackontextclassificationproblems,byoptimizingthecontinuousrelaxationofthe
autoregressivesamplingprobabilitymatrix. Inlate2022,amongsocialmedia,usersmisalignGPT-3
viapromptinjection.PerezandRibeiro[78]studyhowthisbedonebyadversaries.Theysuccessfully
manipulatethemodeltooutputagivenharmfulstringandleakthesystemprompt. Inearly2023,
anempiricalstudywascarriedoutbyLiuetal.[9]tomeasuretheresultofpromptengineeringfor
breakingChatGPTsafeguards. Shenetal.[74]collectjailbreakingpromptsfrommultipleplatforms
ontheinternet,analyzethesedata,createaharmfulquestionset,andidentifysometypicalharmful
promptsthatareeffectiveatthatmoment. Later,theGreedyCoordinateGradient(GCG)method[1],
astrongwhite-boxattackvariantofAutoPrompt[72]wasproposed. Weietal.[79]categorizestwo
generalmodesofjailbreaking: competingobjectiveandmismatchedgeneralization.
LLMautomationandsuffix-basedattacks Liuetal.[29]proposeAutoDAN,thatreliesongenetic
algorithms,withtherequirementofmanualpromptsforconductingmutationandcrossoveronthe
paragraphandsentencelevel. ThejailbreakingpromptsgeneratedbyAutoDANaresemantically
plausible, unlike the suffix generated by GCG. As a comparison, Lapid et al. [25] use genetic
algorithmforblack-box universaladversarialsuffixgeneration. Chaoetal.[30]proposeanother
LLM-basedjailbreakautomationalgorithm,whereanLLMjudgeisbuilttoassignasafetyscore
toagivenoutput,whiletheattackerisenforced(viaapage-longprompt)toimprovethequalityof
jailbreakingpromptsfrommultipleperspectives. Zhuetal.[24]proposeanotherAutoDANmethod
thatexploresthebalancedlossbetweenjailbreakloss(logprobabilityontheharmfulstring,asused
in Zou et al. [1]) and the plausibility loss (log probability over the adversarial suffix), aiming at
improvinginterpretability. Lietal.[31]usesgeneticalgorithmtosearchwithsimilartymeasure
andinitializewithparaphrasing. ItsperformanceisclaimedtobebetterthanAutoDAN-GA.Deng
etal.[80]investigatethepossibledefensivetricksinproprietaryLLMs,andproposeapipelinefor
automated jailbreaking using a fine-tuned LLM. Yu et al. [81] propose GPTFuzzer, essentially a
geneticalgorithmicframeworkforjailbreaking. Theirwork’sdifferencebetweenAutoDANisthat
ithasapoolof“seeds”,a.k.a. templatesfortransformingtheharmfulprompt,andthemutationis
doneonthetemplatelevel. Dingetal.[32]proposeautomatingattackviaLLMpromptrewritingand
scenarionesting. Thelatterconsistsofcodecompletion,tablefillingandtextcontinuation,sincethe
authorsregardtheseasalignwithtrainingobjectiveswell,andaresuitableforLLMstocomplete
the task. Mehrotra et al. [33] combine Automation used in Chao et al. [30] and tree-of-thought
[82],createinterpretablepromptsinablack-boxmanner. Lietal.[83]proposeDeepInception,and
usenested,imaginaryscenariotoinduceharmfulcontent. Lietal.[84]proposeDrAttack,which
camouflagesaquery’smaliciousintentthroughsemanticdecomposition,byconstructingaparsing
treeandsplittheoriginalpromptintodifferentsegmentations. Wangetal.[85]drawinspirationfrom
theself-perceptiontheoryfrompsychologytodesignapromptmodificationpipelineongradually
persuadingtheLMtobejailbroken. Paulusetal.[86]proposeAdvPrompter,wheretheauthorstrain
alanguagemodelasasuffixgeneratortospeedupLLMattack.
Manipulatingthedecodingprocess Huangetal.[48]findthemethodofchangingthegenerating
hyperparameters(i.e.,poftop-psampling,thetemperatureT,andkoftop-ksampling)ofsafety-
alignedLLMssufficesforobtainingharmfuloutputswhentheuserisabletomanipulatethesystem
promptandinputconfigurations. Zhangetal.[87]directlymanipulatetheoutputgenerationprobabil-
itybyenforcinganaffirmativeprefix,andreversingthenegationwordsiftheyappearinapre-defined
vocabulary(e.g.,sorry→glad). Zhaoetal.[88]assumeaccesstothedecodingdistributionofaLM.
TheyusetwosmallLMs,asafeoneandaharmfulone,tomanipulatethedecodingratioofthelarge
safeLMforjailbreaking. Thekeyinsightisthedecodingdistributionbetweenthesafemodelandthe
harmfulmodelonlydifferssignificantlyforthefirsttensoftokens.
Fine-Tuningalonesuffices Yangetal.[89]showthatfine-tuningonasfewas100harmfulexample
pairs suffices for turning the LLaMa-chat models (and some other <70B LMs) into malicious
counterparts. Zhanetal.[90]fine-tuneGPT-4onharmfuldata,andfindthefine-tunedmodelsescape
previoussafetyconstraintswhilemaintainingusefulness. Qietal.[91]findfine-tuningalone,evenon
29benigndata,leadstosafetydegradationusingLLaMaandGPT-3.5-Turbo. Fine-tuningonharmful
data(withlessthan5gradientsteps)willcausethemodeltobecompletelyharmful,whiletuningon
identity-shiftingdatacouldmaketheLMfullyobedient.
Low-resourcelanguageandcipher Yongetal.[26],Dengetal.[27]explorethedifferencein
languageswhenencounteringthesameharmfulquery,andfindadirecttranslationtolowresource
languages will lead to higher risk, and Deng et al. [27] additionally find when combined with
sophisticatedmethods,thedrawbackoflow-resourcelanguagesdisappear. Yuanetal.[28]usecipher
encodinganddecodingtobreakLLMs. Smallerscalemodelsareimmunefromsuchattacks,while
thesmartestGPT-4encounteredthehighestrisk.
Vision-languagemodelattacks BesidespureLLM,someresearchworksmoveastepforward,uti-
lizingimagesforbreakingvision-languagemodels(VLMs). Shayeganietal.[92]exploremultimodal
attackonVLMviaembeddingspacefeaturematching. Qietal.[93]generateadversarialexamples
viamaximizingtheconditionalprobabilityofaharmfulcorpus,i.e.,thesumoflogprobabilitiesover
alloutputs,andusethefinalimagewithharmfulqueryforjailbreaking. Carlinietal.[94]generate
adversarialexampleforafixedharmfulcontent,andfindnomatterwhatinputpromptisgiventothe
VLM,itwillrespondwiththetargetharmfulstring. Mausetal.[95]proposeablack-boxattackon
manipulatingthegeneratedimagewithmodifiedadversarialprompt.
Misc Weietal.[57],Wangetal.[58]explorein-contextlearningforattackanddefense. Theattack
isweaksinceitcouldonlybreakVicuna[96]andcanbedefendedbyin-contextsafeexamples. Later,
thismethodisscaled-uptosignificantlyimprovestrengthforbreakingguardrailsoflarge,state-of-
the-artmodels[59]. AnearlyworkinFebruary2023[97]adoptsobfuscation(includingsynonymand
typos),codeinjectionandvirtualizationtosuccessfullyjailbreakChatGPT.Shahetal.[98]illustrate
in-contextautomatedpersonamodulationattackforlarge-scaleLLMsandVicuna. Zengetal.[73]
considerthemorebroadlyperspectiveofpersuasion: theytrainapersuasiveparaphraserbasedona
fine-grainedtaxonomyofpersuasiontechniques. Detailedablationonattackeffectivenessisstudied.
Guoetal.[99]focusonstealthinessandcontrollability. Theynoticetheconstraintsappliedtothe
jailbreakingprompts(e.g.,fluency)areexactlythetargetsofthecontrollabletextgenerationproblem.
Thus,theyadopttheEnergy-basedConstrainedDecodingwithLangevinDynamic(COLD)[100]
onoutputlogits. Formingeachconstraintaswellasthetaskofjailbreakingasanenergyfunction
overlogits,theLangevinDynamicisusedforfindingagoodlogitdistribution,andthedecoding
techniqueinQinetal.[100]isusedforoutputgeneration. Banerjeeetal.[101]introduceadataset
TECHHAZARDQA,comparedirectqueryv.s. pseudo-codeformat,andfindthelatterinduceshigher
risk. Mangaokar et al. [102] considers a type of adaptive attack against checking-based defense,
thatappendsauniversaladversarialprefixintothejailbreakingprompttomaketheguardmodel
alwaysoutput“safe”,andthusmakingthedetectorfailstodetectharmfulinformation. Lvetal.[103]
proposeCodeChameleon,whichcontainsmultipleencryptionanddecryptionmethodsdefinedby
pythonfunctions,thattransformstheharmfulpromptintospecificpredefinedformtojailbreakLLMs.
Sadasivanetal.[104]speedupthecomputationofGCG[1]tomakeitpossibletolaunchonasingle
GPU.Geipingetal.[105]buildataxonomyonrisksbeyondjailbreaking,andcoercetheLLMto
providecertainoutputsbyoptimizingasetoftokensviaGCG.Renetal.[106]proposeCodeAttack
thatusecodetemplatestoquerytheoutputoutinsteadofusingnaturallanguagedirectly,andobtain
descentresults.
H.2 Defensemethods
Up to now, no universal defensive strategy as adversarial training [107] for adversarial attacks /
differentialprivacy[108]formembershipattacksexistsasagoldstandard. Ingeneral,wecanclassify
themethodsintothreetypicaltypes: alignment,red-teaming,andalgorithmicproposals.
Alignment Thetargetofalignmentistopushtheoutputoflanguagemodelsbealignedtohuman
values. Regarding safety, the goal is to avoid outputting harmful information. RLHF is widely
adoptedinthesemethods[19,20,109,22]. VariantslikeRLAIFalsohavebeenproposedrecently
[21,110].
Redteaming Thistermispopulatedasspecificallydealingwithharmfulinfoondatasetcuration,
usedtogetherwithRLHF[14–18].
30Next,weproceedtodefensivealgorithmproposals. Weclassifyexistingdefensivestrategiesinthe
followingcategories.
Defenseagainstsuffix-basedattacks. AlonandKamfonas[111]noticethemessynatureofthe
suffixgeneratedbyGCG,andproposetouseaperplexity(PPL)filteroninputprompts. Theyalso
explore using a LightGBM [112] with 2 features (PPL, prompt length) to filter harmful prompt,
andshowitdoesbetterthannaivePPLthresholding. ThePPL-basedfilterdoesnotsucceedwith
human-craftedjailbreaks. Jainetal.[37]exploremanyconcerningviewpoints,includingself-PPL
filtering,paraphrasingtheinputprompt,andre-tokenizationsincemanyLLMs’tokenizersarebased
onByte-PairEncoding(BPE).Allmethodsaresuccessfulinregardsofdefendingagainstsuffix-based
attacks. Theyalsoexplorethesimplestformofadversarialtraining. Robeyetal.[34]proposeto
perturb the input token string by random replacement/erasement/insertion, and finally perform a
majorityvoteintheend. Caoetal.[35]judgeswhetheraninputpromptissafeornotbyestimation
withMonteCarlo,whenrandomlydroppingafractionoftokens,usingtheLLMitself. Kumaretal.
[36]trytoperform“certified”safetyagainstharmfulprompts,byerasingtokensandsettheoriginal
promptasharmfulifatleastoneoftheseerasedpromptsleadtoaharmfulresponse,orbeclassified
asharmfulbyaDistillBERT-basedclassifier.
Systempromptdefense. Wecouldmodifytheinputpromptforjailbreaking;andseveralworks
exploreifwecanapplysimilarmethodstosystempromptstodefendagainstsuchattacks. Xieetal.
[113]propose“self-reminder”, i.e., appendingaremindingpromptwithinthesystempromptfor
defense. TheattacksarecollectedfromtheJailbreakChatcollection,andthisstrategyiseffectivefor
defendingagainstthem. Zhengetal.[114]advocateforfindingagoodsystempromptautomatically,
byinvestigatingtherepresentationaldifferencebetweensafeandharmfulqueries,andoptimizing
thesafetypromptsalongtheharmfulrefusaldirectionintherepresentationspace. Oneintriguing
takeawayisharmful/harmlessqueriescanbedistinguishedintherepresentationspace,differentfrom
theadversarialexamplesinvision. Zhouetal.[115]alsooptimizethesafesystemprompt,butina
more“adversarialtraining”fashion,thatapplyjailbreakalgorithmswithcurrentsafepromptsfirstand
thenfindgoodreplacementcandidatesinthesamewayasdonebyZouetal.[1]. Concurrently,Mo
etal.[116]proposepromptadversarialtuning,whereanadversarialsuffixisassumed,whileasafe
systempromptisjointlyoptimizedwiththissuffix,withanadditionallyconstructedbenignlossto
improvehelpfulnessundernormalqueries.Zhangetal.[117]proposetheideaof“goalprioritization”,
either without training (append prioritize safety than helpfulness and in-context examples to the
systemprompt)orwithtraining(generatedatapairsofprioritizingsafetyorhelpfulness,finetune,
and append prioritize safety prompt into system prompt). The former is effective for large-scale
LLMs,whilethelatterimprovessafetyofLLaMa-chatmodels. Zhouetal.[118]proposein-context
adversarial game, where an attack LLM and a defense LLM interact on exchanging insights on
successfuljailbreaks,anddefendbyimprovingthesystemprompt. Zouetal.[70]givetheresult
thatsystempromptmattersforjailbreaking,andshowsconductingGA-basedsearchoveritcould
improvesafety.
Checking-based,decoding-based,andMisc. Helblingetal.[119]generateresponsesfirst,and
thenusetheLLMitselftoexaminewhethertheoutputisharmfulornot. Theyfindsuchsimple
self-examinationispowerfulsincetheTPRreportedcanbeupto∼1.00. Wangetal.[120]propose
to(1)tunetheLMtoenhanceitscapabilityondiscriminatingharmful/harmlesscontent;(2)tune
theLMtomakeitabletotagitsownresponse; and(3)rewriteresponseifoutputisharmful. Li
etal.[121]proposetosuppresstheattackperformancebyiterativelyrewindingandre-examiningthe
generatedoutput. Themethoddoesnotworkwellwithsmallmodels,butworksprettyfinewithlarge
(open-source)models. Theyfindthestrategycanimprovegeneralizationaswell. Xuetal.[122]train
asafermodelfirst,andusenormalizedp +α(p −p )overtop-k sharedtokensfor
attacked safer attacked
decodingtoenhancesafety. Hasanetal.[123]showwithoriginalWandapruning[124],theLLM
canhelpresistdirectjailbreakingprompts,e.g.,withrole-playingattacks. Pietal.[125]propose
MLLM-ProtectoronsafeguardingVisualLLMsbycheckingtheoutputandthendetoxifyingthe
content. Zhangetal.[126]performintentionanalysisontheinput,andenforcethemodelgenerate
policy-alignedoutputsbothbyprompting. Wangetal.[127]proposebacktranslationthatguesses
theinputpromptdirectly,andrejectifitisharmful. Kimetal.[128]proposeself-refinementwhich
consistsofgeneratingafeedbackandthenrefinetheresponsetoavoidharmfulinfooutput. They
findusingadditionalJSONandcodeformattingwouldimprovesafety. Zengetal.[129]propose
AutoDefense,whichutilizesmultipleagentsonanalyzingprompt,responseandintention,todefend
31against attacks. Hu et al. [130] propose Gradient Cuff, a sampling-based gradient-norm defense
method,byrejectingthoseinputpromptswithlargegradientnormontopofamajority-votebased
filtering. Jietal.[131]proposeamethodsimilartoRobeyetal.[34],butforsemantically-meaningful
attacks, that paraphrases the input according to several criteria and conduct a majority vote for
judging.
Severalcompany-centeredproductsalsofallintothisregime. Forexample,LLaMa-Guard[132]is
trainedontoxicdatasuchthatitisabletodiscriminateunsafeuserpromptsandoutputs,respectively.
IBMalsoproposeaframeworkonconstructinganddeployingsafeguarddetectionmodules, and
releasesthedetailsinatechnicalreport[133].
H.3 Theoryandexperimentalunderstanding
Wolfetal.[38]assumesthedecomposabilityofLLMoutputintoagoodandbadcomponent,and
showpossiblejailbreakingintheorybypromptingthemodelwithasufficientlylonginput. Kalaiand
Vempala[134]usestatisticaltoolstoprovehallucinationforcalibratedLMs. Leeetal.[135]study
therepresentationinGPT-2. Theytrainabaseclassifierfortoxicity,andusethelinearweightasa
proxyoftoxicvector. Theyfindtherearevaluevectorsclosetothetoxicvectoritself,thatarenot
suppressedbyDPOtuning[23]. Weietal.[136]usepruningandlow-rankanalysisonsafeLM,and
find(1)safeneuronsandusefulneuronsaresparse;pruningthesafeneuronsorremovingthesafe
ranksawaydegradessafetyalot,and(2)fixingthesafeneuronsinfine-tuningdoesnotmaintain
safety.
32