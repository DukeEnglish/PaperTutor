Prompt Recursive Search: A Living Framework with
Adaptive Growth in LLM Auto-Prompting
XiangyuZhaoa andChengqianMaa
aXiamenUniversity
ORCID(XiangyuZhao): https://orcid.org/0009-0007-0903-0352,ORCID(ChengqianMa):
https://orcid.org/0009-0005-9269-3880
Abstract. Large Language Models (LLMs) exhibit remarkable 1 Introduction
proficiency in addressing a diverse array of tasks within the Nat-
Stem cells differentiate into other types of cells with distinct func-
ural Language Processing (NLP) domain, with various prompt de-
tions upon induction by signaling molecules[2, 20]. A similar pro-
signstrategiessignificantlyaugmentingtheircapabilities.However,
cess,asdepictedin2,isproposedbyusintheuseofLargeLanguage these prompts, while beneficial, each possess inherent limitations.
Models(LLMs)tosolveproblems.Previouswork[26,9,31,30,13]
Theprimarypromptdesignmethodologiesaretwofold:Thefirst,ex-
has indicated that a single interaction often fails to adequately ad-
emplifiedbytheChainofThought(CoT),involvesmanuallycraft-
ing prompts specific to individual datasets, hence termed Expert- dress problems. Instead, a more effective approach is to divide the
Designed Prompts (EDPs). Once these prompts are established, problem-solvingprocessintomultiplesteps:firstproposingasolu-
tiontotheproblem,andthenexecutingitstepbystepbytheLLM.
they are unalterable, and their effectiveness is capped by the ex-
Each interaction with the LLM involves input to the model and its
pertise of the human designers. When applied to LLMs, the static
subsequent output. A single interaction can lead to the refinement,
nature of EDPs results in a uniform approach to both simple and
detailing, summarization, or execution of a solution. Each interac-
complex problems within the same dataset, leading to the ineffi-
tioncorrespondstotherealizationofathoughtwithinthethoughts
cient use of tokens for straightforward issues. The second method
that make up the solution. The functionality of a thought evolves
involves prompts autonomously generated by the LLM, known as
LLM-Derived Prompts (LDPs), which provide tailored solutions fromamorebasicstatetobecomeincreasinglyspecific,muchlike
howastemcelldifferentiatesintoacellwithaparticularfunction.
to specific problems, mitigating the limitations of EDPs. However,
Similarly,anoriginalthoughtcandifferentiateinresponsetovarious
LDPsmayencounteradeclineinperformancewhentacklingcom-
problems,givingrisetothoughtscapableofsolvingdifferentissues.
plexproblemsduetothepotentialforerroraccumulationduringthe
Controlovertheevolutionofthoughtstraditionallyinvolvestwo
solutionplanningprocess.Toaddressthesechallenges,wehavecon-
ceived a novel Prompt Recursive Search (PRS) framework that methods. The first method entrusts the transformation process of
thoughtsentirelytoexperienceddomainexperts.Theseexpertscan,
leverages the LLM to generate solutions specific to the problem,
based on their understanding of problems within a particular field,
thereby conserving tokens. The framework incorporates an assess-
plan the problem-solving process in a sequential chain, addressing
mentofproblemcomplexityandanadjustablestructure,ensuringa
the problem step by step. This is the contribution of the Chain of
reductioninthelikelihoodoferrors.Wehavesubstantiatedtheeffi-
Thought(CoT)approach[26].Priortothis,thepracticeofpresent-
cacyofPRSframeworkthroughextensiveexperimentsusingLLMs
ingaproblemtoLLMsandexpectingadirectanswerplacedexces-
withdifferentnumbersofparametersacrossaspectrumofdatasetsin
sivedemandsontheLLMs,yieldingsuboptimalresultsandfailingto
variousdomains.ComparedtotheCoTmethod,thePRSmethodhas
fullyleverageLLM’spotential.Giventhatthereisoftenmorethan
increasedtheaccuracyontheBBHdatasetby8%usingLlama3-7B
onemethodtosolveaproblem,andmultipleapproachescanyield
model,achievinga22%improvement.
answers for a specific issue, we can compare the answers derived
fromvariousmethodstoselectthemostoptimalone,thusobtaining
the best solution to the problem. Plan-and-Solve (PS)[24] Prompt-
Table 1. Analysis of prompt methodologies in relation to their capabili-
ing is a concrete implementation of this line of thinking. Building
ties,utilizingthefollowingsymbols:“⋆”forcomprehensivesupport,“⋆”
uponthis,theTreeofThought(ToT)[27]furtherextendsthiswork
forlimitedsupport,and“Ø”forabsenceofsupport.CE:ComputationalRe-
byconsideringascenariowhereathoughtrepresentsacriticalstep
sourceUtilizationEfficiency,IH:IndependencefromHumanExpertise,SE:
inthesolution.Ifthisthoughtisflawed,continuingtoreasonbased
SupervisionofErrorsintheInferenceProcess.
onitcanleadtomoresevereerrors.Therefore,theauthorsofToT
Scheme CEIHSE propose a prompt structure that allows for a retreat from an erro-
Expert-DesignedPrompt[26,24,27,3]Ø Ø ⋆ neous solution path to reconsider and pursue the correct approach.
LLM-DerivedPrompt[18,10,34] ⋆ ⋆ Ø
Thisstructureishierarchical,andthefeatureisknownastheback-
PromptRecursiveSearch ⋆ ⋆ ⋆
trackingfunctionduringthetreetraversalprocess.Onthefoundation
ofToT,theGraphofThought(GoT)[3]structureisevenmoreflexi-
ble.Byperformingoperationssuchasaggregationandrefinementon
4202
guA
2
]LC.sc[
1v32410.8042:viXramultiplethoughts,theexpert-designedpromptstructurerepresented employ methods mentioned in the first two approaches: evaluating
byGoTishighlyfault-tolerant.Theoutcomeofonethoughtcanbe and discarding prompts with lower quality, or constraining them
verifiedbymultiplethoughts,andtheknowledgederivedfrommul- withedge-casequestions.Additionally,wecanutilizetheAutomatic
tiplethoughtscanbeintegratedintoasinglethought. Prompt Optimization[18] method: composing all possible prompts
We refer to the prompt design structures represented by CoT into a prompt space, using a prompt generated by the LLM as the
(Chain of Thought), ToT (Tree of Thought), and GoT (Graph startingpointwithinthisspace,andoptimizingthepromptbasedon
of Thought) as Expert-Designed Prompts (EDP): These prompt itsevaluationfromLLMasthetextualgradient.Throughrepeatedit-
frameworks are manually crafted, thus necessitating a substantial erations,theoptimalsolutioncanbefoundwithinthepromptspace,
amountofhumanexpertexperience.Theapproachdemandsahigh employing a textual gradient descent method to identify the most
level of expertise from the designer, who must draw on their pro- idealsolution.ThesolutionsproposedbytheLLM,inconjunction
fessional experience and conduct multiple experiments to derive a with the problem context or textual gradient, often achieve effects
prompt structure capable of addressing various scenarios within a comparabletothoseofhuman-designedprompts,whichwetermas
particular domain. Given that this method accounts for a multi- LLM-DerivedPrompts(LDP).
tudeofpotentialpromptstructures,itisuniversallyeffectiveforall However,whendealingwithexcessivelycomplexproblems,such
problems, which may result in a complex structure. While this ap- aslarge-scaleplanningissues,thecompletedelegationoftheplan-
proachoftenexcelsintacklingcomplexissues,itcanbeinefficient ningofsolutionstotheLLMcanbecomeproblematic.Withnumer-
for simpler problems, as many steps within the prompt structure ous steps involved, there is a high likelihood of error propagation
become redundant, leading to unnecessary computational resource andaccumulation.Aminorerroratanystagecanberapidlymagni-
wastage.Furthermore,ifthehumanexpertresponsiblefordesigning fiedacrossmultiplesteps,leadingtosuboptimalperformancebythe
thepromptlackssufficientproficiencyorfailstoconsiderallpossi- LLM.
blescenarioscomprehensively,thestaticnatureoftheprompt,once Expert-DesignedPrompts(EDPs)oftennecessitateamorecom-
designed,canleadtothepersistentpresenceofanyinherentflawsin plexstructuretoaddressallscenarioswithinadataset,whichcanre-
thestructureduringeachapplication. sultinredundantstepswhenEDPsareappliedtosolvesimplerprob-
To mitigate the dependency of Large Language Model (LLM) lemswithinthatdataset.Ontheotherhand,LLM-DerivedPrompts
promptdesignonhumanexperience,asecondapproachtoprompt (LDPs)delegatetheentiretaskofpromptdesigntotheLargeLan-
design has been proposed. The primary issues with human- guageModel(LLM).Duringtheprocessofgeneratingsolutionsto
engineered prompt structures are twofold: Firstly, due to the limi- problems,theLLMishighlysusceptibletotheaccumulationofer-
tationsofhumancognition,humanexpertscannotfullyaccountfor rors,whichsignificantlyincreasesthedifficultyofaccuratelygener-
allscenarioswithinadomain.Secondly,onceapromptstructureis atingsolutionsforcomplextasks.
designedbyahumanexpert,itbecomesimmutable,whichcanlead We have identified that the characteristics of Expert-Designed
to the waste of substantial computational resources when address- Prompts(EDPs)andLLM-DerivedPrompts(LDPs)are,toacertain
ingsimpleproblems,asthecomprehensivelycomplexstructure,ini- extent, complementary. Consequently, after thorough consideration
tiallyastrength,becomesredundant.However,entrustingthetaskof oftheadvantagesanddisadvantagesofbothEDPsandLDPs,wepro-
promptstructuredesigntoanLLMaddressestheseconcerns.Given pose an entirely new Prompt Recursive Search(PRS) framework
thevasttrainingcorporaoflargelanguagemodels,theirbreadthof for prompt design which is depicted in 2. This framework enables
knowledgecansurpassthatofindividualhumanexperts.Addition- LargeLanguageModels(LLMs)toavoidoverlycomplexplanning
ally,thepromptframeworkdesignfunctionalityprovidedbyLLMs whentacklingsimplisticproblems,whilealsoallowingLLMstoun-
cancommenceuponthepresentationofaspecificproblem,thereby dertakeaportionofthepromptdesignwork,thusreducingthehigh
beingtailoredtothatparticularissue.Forproblemsthatarestraight- dependencyonhumanexperts.Drawinginspirationfromthediffer-
forward and can be resolved in a single step, the LLM-generated entiationprocessofhumanstemcells:whenthebody,promptedby
prompt structure will not incur the unnecessary computational re- signaling molecules, requires cells to perform a specific task, stem
sourceexpenditureassociatedwiththeredundantstepsfoundinhu- cells differentiate into new cells with distinct characteristics to ad-
manexpert-designedpromptstructures. dress issues that other cells cannot. Our prompt framework, when
In the work of Automatic Prompt Engineering (APE)[34], the confrontedwithanewproblem,firstevaluatestheproblemandhas
Large Language Model (LLM) initially proposes a set of prompts theLLMassignacomplexityscore.Iftheproblem’scomplexityis
thatencompasssolutionstospecificproblems.Thequalityofthese deemed too high, we break down the problem into multiple steps
prompts varies, leading to a selection process where lower-quality forresolution.Thesimplifiedproblems,post-dissection,becomenew
prompts are discarded, and higher-quality ones are retained. Then targets, for which the LLM provides solutions. This is akin to al-
LLM can use the higher-quality prompts as references to gener- lowingtheLLMtocompleteadifferentiationdesignprocessforthe
ate similar ones, enhancing the diversity of the prompt collection. prompt,resultinginpromptswithspecificfunctionalities.Duringthe
This iterative process continues until satisfactory prompts are ob- differentiation steps, we recursively apply this differentiation func-
tained.DuringthepromptgenerationbytheLLM,themodelcanact tionuntiltheproblemhasbeensufficientlybrokendownintosimple
notonlyasaproblemsolverprovidingsolutionstodomain-specific components.
questionsbutalsoasaquestiongenerator,posingsomeextremeor By employing LLMs with varying numbers of parameters, we
marginal questions within a domain. These questions can be uti- compared the PRS method with traditional prompt design ap-
lizedtooptimizetheprompts,therebyimprovingtheirquality.Build- proachesontheBBHdataset,whichspansmultipledomains,thereby
ingonthisapproach,Intent-basedPromptCalibration(IPC)[10]op- validatingtheeffectivenessofthePRSmethod.Throughthispaper,
timizes the prompts using these edge-case questions after design- wehavemadethefollowingcontributions:
ing the prompts and related questions, ensuring the quality of the
promptsbyleveragingthesedomain-marginalissues.Followingthe
• We have categorized existing prompts based on their source of
proposal of prompts by the LLM, to ensure their quality, we can
acquisitionintotwotypes:Expert-DesignedPrompts(EDP)andIf you follow these instructions, do you return to the starting point? Always face forward. Take 7 steps backward. Take 10 steps
right. Take 4 steps forward. Take 1 step left. Options: Yes. No.
Assess the complexity of the following question, then only reply "The complexity score is [s].", where s is an integer from 1to10.
The complexity score is 6.
Let's first understand the following question, and then devise a plan consisting of a total of 3steps
to solve it then assess the complexity of these plans, Your output must be of the following format.
1. Define Initial Orientation: Assume a forward direction (e.g., north)
as the starting point with coordinates (0, 0).
2. Simulate Movement: Translate each instruction into coordinate 2.1 Subtract 7 from the y-coordinate to simulate taking 7 steps backward.
changes: "backward" as subtraction from the y-coordinate, 2.2 Add 10 to the x-coordinate to simulate taking 10 steps to the right.
"right" as addition to the x-coordinate, "forward" as addition to 2.3 Add 4 to the y-coordinate to simulate taking 4 steps forward.
the y-coordinate, and "left" as subtraction from the x-coordinate. 2.4 Subtract 1 from the x-coordinate to simulate taking 1 step to the left.
3. Analyze Final Position: Calculate the final coordinates after all
movements and determine if they match the starting point (0, 0)
to conclude if the path leads back to the origin.
1. Initial Position: (0, 0).
2. Take 7 steps backward: Position becomes (0, 0 - 7) = (0, -7). Final answer;
3. Take 10 steps right: Position becomes (0 + 10, -7) = (10, -7). 1-ordersolution;
4. Take 4 steps forward: Position becomes (10, -7 + 4) = (10, -3). 2-orer solution;
5. Take 1 step left: Position becomes (10 - 1, -3) = (9, -3). Fixed prompt;
6. The final position is (9, -3), which is not the same as the starting point (0, 0).Therefore, the answer is: No. Unfixedprompt.
Figure1. ExampleofourproposedmethodPromptRecursiveSearchwithcomplexityscorelimitation(PRS).
LLM-DerivedPrompts(LDP); 2.2 Expert-DesignedPrompts
• Byintroducingbiologicalprinciples,wehaveproposedanauto-
matedframeworkforaddressingcomplexproblemsthatintegrates 2.2.1 ChainofThought(CoT)
theadvantagesofbothEDPandLDP.
ThistechnologyisdesignedtoenhancetheabilityofLargeLanguage
• Through validation on the BBH[21] dataset across multiple do-
Models(LLMs)tohandlecomplexissues.Itsimulatesthecontinu-
mainsusingmodelswithparametersspanningdifferentordersof
ousthoughtprocessofhumanproblem-solvingbyembeddingase-
magnitude,wehaveconfirmedtheeffectivenessofthismethod.
riesoflogicalreasoningstepswithintheinputprompts.Thesesteps
progressivelyguidethemodeltothesolutionoftheproblem,aiding
intheprocessingoftasksthatrequirecontinuouslogicormathemat-
ical computation. By demonstrating examples of problem-solving,
2 RelatedWork
theaccuracyandlogicalityofthemodel’soutputcanbeimproved,
evenwithoutspecifictasktraining.
2.1 LLM-DerivedPrompts
2.2.2 Plan-and-Solve(PS)Prompting
ComparedwithChainofThought(CoT),Plan-and-SolvePrompting
Large Language Models (LLMs) are an advanced tool in the field
significantlyimprovestheperformanceofLargeLanguageModels
ofNaturalLanguageProcessing(NLP)technology[32,33].Theyare
(LLMs)inmulti-stepreasoningtasksbyguidingthemodelstofor-
basedondeeplearningarchitectures,suchasTransformers[22],and
mulateplansforsolutionsandthenexecutethem.Ithasdemonstrated
havelearnedcomplexpatternsoflanguagethroughextensivetraining
superior performance over Zero-shot-CoT thinking across multiple
onvastamountsoftextdata.Thesemodelsarenotonlycapableof
datasetsandiscomparabletoCoTmethodsthatrequiremanualex-
understandingthenuancesoflanguagebutalsogeneratingcoherent
amples,showcasingthepotentialtostimulatethereasoningcapabil-
andgrammaticallycorrecttext,therebyperformingwellonavariety
itiesofLLMswithouttheneedformanualexamples.
oflanguagetasks.Theycanbeusedinawiderangeofapplications,
includingchat-bots[17], recommendation systems[12],contentcre-
ation aids[5], and educational tools[16, 14]. However, LLMs may 2.2.3 TreeofThoughts
also replicate and amplify biases present in their training data, so
ethical[4,7,1,14]andbias[28,15]considerationsmustbetakeninto TheTreeofThought(ToT)frameworkaimstoenhancethecapabili-
accountintheirdesignanduse.NotableexamplesofLLMsinclude tiesofLargeLanguageModels(LLMs)inproblem-solving.Itallows
theBERT[6]andGPT[19]models,whichhaveachievedsignificant the model to explore various reasoning paths and to self-assess its
success in tasks involving natural language understanding[19, 23] decisions,leadingtomoredeliberatechoices.Thisapproachviews
andgeneration[11,8]. problem-solvingasasearchprocesswithina"thoughttree,"whereFigure2. PipelineofourproposedmethodPromptRecursiveSearchwithcomplexityscorelimitation(PRS).
each"thought"isacoherenttextblockthatrepresentsasteptowards Thisoperationofgeneratingdatasamplesopensupnewavenuesfor
thesolution. promptdesign:notonlycanwehaveLLMsgenerateanswersforus,
butwecanalsohaveLLMsgeneratequestionsthatcan,inturn,lead
tobetteranswers.
2.2.4 GraphofThoughts
GraphofThoughts(GoT)significantlyenhancestheperformanceof
LargeLanguageModels(LLMs)incomplextaskprocessingbycon- 2.2.7 AutomaticPromptOptimization
ceptualizingthereasoningprocessasagraphstructurecomposedof
nodes,whichrepresentthemodel’s"thoughts,"andedges,whichde- AfterthegenerationofpromptsbyaLLM,itisoftennecessarytore-
notetheconnectionsbetweenthesethoughts.NotonlydoesGoTim- finethem.Thisrefinementcanbeaccomplishedthroughsimplerank-
provethequalityoftaskexecution,witha62%increaseinefficiency ingandselection,knownasApproachbyAPE,orbymakingmod-
overexistingtechnologiesinsortingtasks,butitalsoachievesare- ificationstothepromptbasedonthedataprovidedbytheLLM,re-
ductionincostsbymorethan31%.ThedesignofGoTgreatlyfacil- ferredtoasIPC.However,whentheobjectiveistoidentifythemost
itatestheapproximationofLLMs’reasoningcapabilitiestohuman appropriatepromptwithinadensespaceofprompts,gradientdescent
thoughtpatterns. emergesasamorerationalmethod.TheprocessbeginswiththeLLM
generatingaresponsebasedonaninitialprompt.Subsequently,the
LLManalyzestheresponseinconjunctionwiththeprompttopro-
2.2.5 AutomaticPromptEngineer(APE)
duceTextualGradients.TheseTextualGradientsarethenintegrated
The manual design of prompts necessitates a high level of exper- toformulateanewprompt.Thiscycleofgenerationandrefinement
tise from the designer and is constrained by the relatively limited isrepeatediterativelyuntiltheoptimalpromptisachieved.
knowledgebaseandsubjectivefactorsofhumanexpertscompared
toLargeLanguageModels(LLMs).Handcraftedpromptsaresubject
tonumerouslimitationsstemmingfromhumanfactors,whichLLMs 3 Methodology
arenotboundby.Therefore,entrustingthetaskofpromptdesignto
anLLMcanbothresolvetheseconstraintsandeliminatetheincon- OurapproachisfoundedupontheprinciplesoftheEDPandtheLDP,
venience of manual prompt design, thereby automating the use of takingintoaccountthatwhileEDPisadeptataddressingstraightfor-
LLMs.Initially,theLLMistaskedwithgeneratingasetofprompts, wardproblemswithacomprehensiveframeworkdesignedtoencom-
which arethen not utilizedin theirentirety. Instead, theseprompts passallscenariosofsuchissues,itmaynecessitatesuperfluoustoken
areevaluatedandthehighest-qualityonesareselected.Basedonthe expenditure when applied to simpler problems. On the other hand,
highest-qualityprompts,avarietyofsimilarpromptsaregenerated LDP can lead to the accumulation of errors; if an LLM is tasked
toenhancetheirdiversity. with designing prompts without constraints or corrections, it must
adheretoaprocessthatinvolvesplanningasolutionandthenexe-
cutingit.Errorsintroducedduringtheplanningphasecanpropagate
2.2.6 Intent-basedPromptCalibration(IPC)
through to subsequent steps, potentially undermining the problem-
Large Language Models (LLMs) are highly sensitive to the input solvingprocess.Tomitigatethecostsassociatedwithmanuallyde-
content, where even seemingly irrelevant random noise can signif- signedprompts,wedelegatetheresponsibilityofdevisingsolutions
icantlyaltertheLLM’soutput.Therootcauseofthissensitivityis totheLLMitself.TheLLMwilldesigntailoredsolutionsforeach
alackofanaccurateandcomprehensiveunderstandingoftheprob- problem, thereby avoiding unnecessary token usage. Furthermore,
lem.ToensurethatLLMsconsidertheproblemthoroughly,basedon topreventtheaccumulationoferrorsduringtheLLM’spromptde-
theuser’srequirementsandtheinitialprompt,newpromptsanddata signphase,weemploycomplexitydetectionmethodsandprocedural
samplesareiterativelygenerated.Thepromptsarethenoptimizedus- planning techniques to constrain the LLM and validate its solution
ingedgedata,allowingtheLLMtofullycontemplatetheproblem. planning.Ourmethodisshowedin1and2.3.3 RecursivelyProposingSolutionstoProblems
Algorithm1:DesignSolution
Input:ProblemP,StepsSteps Followingthefirsttwosteps,wehaveestablishedthenecessarysteps
Output:SolutionSolution forasolution,whicharedeterminedbasedonthecomplexityofthe
1 Solution←[] problem.Thisapproach,underpinnedbyourdiscoveredrelationship
2 Initial_Steps←Get_Initial_Solution(P,Steps) betweencomplexitylevelsandsolutionsteps,mitigatestheaccumu-
3 foreachStepinInitial_Stepsdo lationoferrorsduringtheproblemformulationbytheLLM.Itad-
4 Complexity←Evaluate_Complexity(Step) dressestheshortcomingsoftheLLM-DerivedPrompt(LDP)while
5 ifComplexity>Thresholdthen
fully leveraging its strengths in devising tailored solutions for spe-
6 New_Steps←Calculate_Steps(Complexity)
7 Solution← cificproblems,therebyalsoresolvingtheissuesassociatedwiththe
Solution+Design_Solution(Step,New_Steps) Expert-DesignedPrompt(EDP).
8 else
9 Solution←Solution+[Step] 3.4 Obtainingacompletesolution
10 end
11 end Subsequently,wecanemploytheidentifiedsolutionstepstopropose
12 returnSolution resolutions. Recognizing that providing detailed steps for complex
problems in a single response demands a high level of proficiency
fromtheLLM,weadoptarecursivestrategyforthemoreintricate
Algorithm2:PRSPipeline stepsofthesolution.Eachcomplexstepistreatedasanewproblem
Input:ProblemdescriptionP tobebrokendown,andamorerefinedsolutionisproposed,prompt-
desc
Output:AnswerAns ingtheLLMtoelaborateonthespecificsofthesolution.Thisrecur-
// Evaluate Complexity siveprocesscontinuesuntilallsolutionsaresimplifiedtothepoint
1 C←Evaluate_Complexity(P desc) wheretheycanberesolvedinasinglestep.Bytransformingcom-
// Calculate Steps
plex problems into simpler ones, we prevent the LLM from being
2 Steps←Calculate_Steps(C) overwhelmedbycomplexityandensuretheprovisionofaccurateso-
// Design Solution
lutions.
3 Sol←Design_Solution(P desc,Steps)
// Get Answer
4 Ans←Get_Answer(P desc,Sol) 3.5 ObtaininganAnsweraccordingtothesolution
5 returnAns
Afterobtainingasolutionthroughtheprecedingplan,wecanthen
applythissolutiontoaddresstheproblem.Additionally,wecanim-
3.1 EvaluatingtheComplexityofaProblem
poseconstraintsontheformatoftheanswersoutputtedbytheLarge
Language Model (LLM) with respect to specific datasets, ensuring
Beforedevisingsolutionsforaproblem,wefirstassessitscomplex- theproductionofcoherentandappropriateresponses.
itytofacilitatethedesignofmorecomprehensivesolutionsformore
complexissuesandmorestraightforwardsolutionsforsimplerones.
4 Experiments
Thecomplexitylevelcanprovideeffectiveguidanceforformulating
oursolutions.Intheprocessofevaluatingthecomplexityofaprob- 4.1 ModelSelection
lembyaLargeLanguageModel(LLM),weconsiderthecharacteris-
Intheprocessofmodelselection,consideringthatLargeLanguage
ticsoftextualdescriptionsofproblems:directlydescribingthecom-
Models(LLMs)withasubstantialnumberofparametersinherently
plexity using natural language can lead to an inability to distinctly
possess higher performance, the advent of prompt design method-
differentiatebetweenproblemsbasedonthismetric.Therefore,we
ologieshasbeenpredominantlyaimedatenhancingthecapabilities
opt to use numerical values instead of the original complexity de-
ofLLMswithmoderatetosmallerparametercounts.Moreover,the
scriptions.Wecategorizethecomplexityofproblemsintotenlevels,
efficacy of prompt design methods is more pronounced in LLMs
representedbytenintegersrangingfrom1to10.
with smaller parameter volumes. We can enhance the performance
of LLMs by designing prompts, fully tapping into the potential of
3.2 PlanningSolutionforaProblem LLMs;however,LLMswithalargernumberofparametersalready
possessrelativelygoodcapabilities,andevenwiththeuseofprompt
techniques,theimprovementinLLMperformancemaynotbevery
Upondeterminingthecomplexityofaproblem,ourfocusshiftsto
significant.Wewilldemonstratethispointthroughtheresultsofab-
designingsolutionsbasedonthatcomplexity.Inpractice,theaspect
lationexperiments.Consequently,wehaveelectedtoutilizetheYi-
ofasolutionthatisdirectlyrelatedtocomplexityisthenumberof
34Bmodel,whichhasamoderatelysizedparametervolume,along-
steps involved. Through extensive experimentation, we have corre-
sidetheMeta-Llama-3-8Bmodel,whichfeaturesasmallerparame-
latedthecomplexitylevelsproposedbytheLargeLanguageModel
tercount.
(LLM)forproblemswiththestepsrequiredintheirsolutions.Our
findingsindicatealinearrelationship:thenumberofstepstoresolve Yi-34B TheYi-34Bmodel[29]isamiddle-scalelanguagemodel
aproblemtypicallyrangesbetweenoneandfive,andthecomplex- trained on a diverse corpus of 3 terabytes, designed with a bilin-
itylevelofaproblem,whendividedbytwo,oftenyieldsthenumber gual focus. It has demonstrated significant potential across various
ofstepsneededforitsresolution.Thismacroscopicstatisticalpattern aspectssuchaslanguagecomprehension,commonsensereasoning,
allowsustoseamlesslyincorporatetheinitialcomplexityassessment andreadingcomprehension,indicativeoftheemergentcapabilities
intothedesignofsolutions,therebyspecifyingthenumberofsteps associatedwithlargemodels[25].Thisemergentcapabilityimplies
requiredforresolution. that LLMs can exhibit human-like logical thinking, reasoning, andFigure3. ComparisonbetweenPRS(ours)and0-shot-cotontheBBHdataset.
analyticalabilities,whicharepreciselytheskillsthatLLMsneedto aproblemviaaLargeLanguageModel(LLM),weutilizethislinear
engageinreasoningwiththeaidofprompts.Suitableforamultitude relationshiptotranslatethecomplexityintothecorrespondingsteps
of applications, the Yi-34B model is fully accessible for academic necessaryforproblemresolution.
researchandconcurrentlyoffersfreecommerciallicensinguponap-
plication.
4.4 EvaluationMetrics.
Meta-Llama-3-8B The Llama-3 model represents an auto-
regressivelanguagemodelengineeredwithanenhancedTransformer AfterobtainingtheanswerfromtheLargeLanguageModel(LLM),
[22]architecture.Itfeaturesrefinediterationsthatemploysupervised itisnecessarytocompareitwiththecorrectanswertoassessitsac-
fine-tuning (SFT) coupled with reinforcement learning augmented curacyandcalculatetherateofcorrectness.Therearetwoprimary
byhumanfeedback(RLHF),ensuringalignmentwithhuman-centric methods for comparison: The first method involves the LLM itself
valuesofutilityandsafety.Llama3hasbeenpre-trainedonanex- comparingtheprovidedanswertothecorrectoneandthenmaking
tensive corpusexceeding 15trillion tokens, sourcedfrom avariety ajudgment.Thisapproachiscontingentontheperformanceofthe
ofpublicrepositories.Forfine-tuning,itleveragesacompilationof LLM,whichcarriesthepotentialforerror.Thesecondmethodisap-
publiclyaccessibleinstructionaldatasetsalongsideasubstantialcol- plicableonlywhenthecorrectanswersinthedatasetadheretoamore
lectionofover10millionexamplesannotatedbyhumans. uniformandstandardizedformat.Inthiscase,wesimplyinstructthe
LLMtopayattentiontotheformatwhenpresentingtheanswer.Sub-
sequently,wecanemployregularexpressionstoextracttheanswer
4.2 DatasetIntroduction
andusestringmethodsforcomparison.Thismethoddemandsless
The BIG-Bench Hard (BBH) is a subset culled from the original fromtheLLM’sperformancebuthasamorelimitedrangeofappli-
BIG-Benchassessmentsuite,focusingontasksthatposeachallenge cability.TheanswerformatfortheproblemsintheBBHdatasetis
to existing language models. BBH comprises 23 tasks and 27 sub- relativelystandardized,thereforewecanemploythesecondmethod,
datasets,andwhencreatingtheBBHdataset,researchersadheredto whichismoreaccurateformakingjudgments.
specific filtering criteria, including the number of examples in the Since the correct answers in the dataset we used have specific
task,thepresenceofhumanraterperformancedata,thetypeoftask, formattingrequirements,weprovidedtheLLMwiththeanswerto
andtheperformanceofpreviousmodels,amongothers.Thisdataset thefirstsampleinthedatasetasatemplate,instructingtheLLMto
is designed to drive improvements in language model performance mimictheformatofthefirstanswerwhenresponding.Theevalua-
oncomplexreasoningtasksandprovidesavaluablebenchmarkfor tionoftheLLM’sresponsesbeginswiththesecondsample,ensuring
futureresearch.Thisdatasetisrelativelydifficultandcoversawide thattheformatoftheLLM’sanswersismaintainedwhilepreventing
rangeoftopics,withmanyofitssub-datasetsfocusingonassessing dataleakage.
thereasoningcapabilitiesofLLMs,makingitanidealtooltotestthe Beforedeterminingthecorrectnessoftheanswers,wecategorized
abilitiesofPRS. thetypesofanswersinthedataset,dividingthemintosingle-choice,
multiple-choice,numerical,etc.,andformulatedappropriateregular
expressionstomatchtheanswersaccordingly.
4.3 SetUp
Wehaveconfirmedthelinearrelationshipbetweenthecomplexityof 4.5 Result
aproblemandthenumberofstepsrequiredtosolveitthroughexper-
imentsconductedacrossmultipledatasets.OntheLlama3modeland WeconductedextensiveexperimentsontheBBHdataset,whereout
theBBHdataset,thislinearrelationshipisspecificallymanifestedas ofits27sub-datasets,only24werefoundtobevaluableforexplo-
thenumberofstepsrequiredtosolveaproblembeinghalfofitscom- ration.The"dyck_languages"and"multistep_arithmetic_two"tasks,
plexity.Therefore,oncewehaveascertainedthecomplexitylevelof whichwerediscarded,primarilyassesstheLLM’sabilitytodiscernsymbols,whichisunrelatedtotheperformanceoftheprompt.PRS callydesignpromptsforproblems,whichhastheadvantageofsav-
outperformedCoTin19outof25sub-datasets,theaccuracyrateim- ingcomputationalresources.Atthesametime,byjudgingthecom-
provedfrom36%withCoTto44%withPRS,whichisanincrease plexityoftheproblem,itsupervisestheerrorsinthepromptdesign
of22%. process,therebyensuringaccuracy.Throughexperimentsinmultiple
domainsandwithvariousmodels,wehavedemonstratedtheeffec-
tivenessofPRS.However,itisundeniablethatourframeworkstill
4.6 AblationExperiment
hascertainshortcomings,whichareasfollows:
ByapplyingthisframeworktotheYi-34Bmodel,wealsocompared
1. WemandatethattheLargeLanguageModel(LLM)outputsan-
the CoT and PRS methods and found that the average accuracy of
swersinaspecificformatthroughourprompt,asdeviationfrom
PRSincreasedfrom45%withtheCoTmethodto49%,achievinga
thisformatwouldrenderthetrue-falsejudgmentinfeasible.How-
9%improvement.Thisdemonstratesthatourmethodisalsoapplica-
ever,theLLMisnotalwayscompliantwiththeprescribedoutput
bletoLLMswithalargernumberofparametersandalsovalidates
format,andanswersthatdonotconformtotheformatcouldbe
ourviewpointthattheenhancementeffectofpromptdesignmethods
eithererroneousorcorrect.OurconditionfordeeminganLLM-
onLLMswithlargeparametervolumesisrelativelylesspronounced.
providedanswerascorrectstipulatesthatnotonlymustthecon-
tent be accurate, but the format must also be correct. This es-
sentiallyelevatesthecriteriaforacceptableresponses.Situations
wherethecontentiscorrectbuttheformatisnotarenotaccounted
for in our assessment, leading to an underestimation of our true
accuracyrateinthestatisticswecompile.
2. Our framework is predicated on a crucial assumption: the com-
plexity of a problem is directly proportional to the number of
stepsrequiredtosolveit.Wehavesubstantiatedthisintuitiveas-
sumption through numerous experiments, yet it is important to
recognizethatthisassumptionrepresentsamacroscopicruleand
thereareafewexceptionalcasesthatdonotconformtoit.Con-
sequently,thenumberofstepsnecessarytoresolveaproblemre-
mainsatopicworthyofexploration.Wethusleavethisquestion
forfutureresearchendeavors.
Figure4. TheablationexperimentsbasedontheYi-34BmodelontheBBH 3. Evenforthesameinput,theresponsesfromtheLargeLanguage
dataset.Theresultsrepresentmetricsontheformal_fallaciessubset. Model(LLM)canvarywitheachinvocation,thusreflectingthat
the performance of the Prompt Recursive Search (PRS) frame-
workhasaninherentdegreeofrandomness.Theexperimentalre-
sultspresentedinthispaperaretheaverageofthreetrials,which
5 Conclusion
maystilldifferfromthetruecapabilitiesofthemethod.
4. DuetotheinherentlystrongcapabilitiesofLargeLanguageMod-
Traditional prompt design methods can be divided into LDP and
els (LLMs) with a larger number of parameters, applying the
EDP,whereEDPheavilyreliesonthesubjectiveexperienceofhu-
Prompt Recursive Search (PRS) method proposed in this paper
man experts. Its immutable characteristics once involved make it
doesnotyieldasignificantenhancementinthiskindLLM’sabili-
wasteredundantcomputationalresourceswhendealingwithsimple
ties.Infact,theeffectivenessofthisframeworkiscontingentupon
problems. On the other hand, LDP is generated and optimized by
theLLM’sanalyticalcapacityregardingtheproblemathand.Con-
LLMs,butduetothelackofeffectivesupervisionduringtheprompt
sequently,LLMswithalargerparametercountarebettersuitedto
generation process, it is prone to error accumulation. We have in-
leverage the full potential of the PRS framework. Our ablation
tegrated the advantages of both by designing a brand-new prompt
studyprovedthisperspectivebutdidnotdelveintoadetailedin-
frameworkcalledPRS.Iteliminatesthehighdependenceonhuman
vestigation. We reserve the exploration of this aspect for future
expertknowledgethroughautomaticpromptdesignandsavescom-
work.
putationalresources.Byeffectivelysupervisingthecomplexityofthe
problem,itpreventserroraccumulationduringthereasoningprocess.
Insummary,wehaveproposedabrand-newmethodforpromptde-
Wehavedemonstratedtheeffectivenessofthisframeworkbycom-
signandhavevalidateditthroughextensiveexperiments,providing
paring its performance with the CoT method using the Llama3-7B
astartingpointforfutureresearch.
modelacrossmultipledomaindatasets,aswellaswiththeablation
experimentsusingtheYi-34model.
Acknowledgements
6 Limitations By using the ack environment to insert your (optional) acknowl-
edgements,youcanensurethatthetextissuppressedwheneveryou
By summarizing traditional prompt design techniques, we have use the doubleblind option. In the final version, acknowledge-
categorized traditional prompt design methods into EDP (Expert- mentsmaybeincludedontheextrapageintendedforreferences.
DesignedPrompts)andLDP(LLM-DerivedPrompts).Upondiscov-
eringthecomplementarynatureoftheadvantagesbetweenthetwo,
inspiredbythedifferentiationphenomenonofstemcellsinbiolog-
icalprinciples,wehaveinnovativelyproposedanewpromptstruc-
ture: Prompt Recursive Search (PRS). This structure can automati-References [18] R.Pryzant,D.Iter,J.Li,Y.T.Lee,C.Zhu,andM.Zeng. Automatic
promptoptimizationwith"gradientdescent"andbeamsearch.InCon-
ferenceonEmpiricalMethodsinNaturalLanguageProcessing,2023.
[1] J.Bang,B.-T.Lee,andP.Park. Examinationofethicalprinciplesfor
URLhttps://api.semanticscholar.org/CorpusID:258546785.
llm-basedrecommendationsinconversationalai. 2023International
[19] A. Radford and K. Narasimhan. Improving language understanding
ConferenceonPlatformTechnologyandService(PlatCon),pages109–
bygenerativepre-training. 2018. URLhttps://api.semanticscholar.org/
113,2023.URLhttps://api.semanticscholar.org/CorpusID:262977578.
CorpusID:49313245.
[2] A.Becker,E.A.McCulloch,andJ.E.Till. Cytologicaldemonstra-
[20] L. Siminovitch, E. A. McCulloch, and J. E. Till. The distribution
tionoftheclonalnatureofspleencoloniesderivedfromtransplanted
of colony-forming cells among spleen colonies. Journal of cellu-
mouse marrow cells. Nature, 197:452–454, 1963. URL https://api.
lar and comparative physiology, 62:327–36, 1963. URL https://api.
semanticscholar.org/CorpusID:11106827.
semanticscholar.org/CorpusID:43875977.
[3] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski,
[21] M.Suzgun,N.Scales,N.Scharli,S.Gehrmann,Y.Tay,H.W.Chung,
L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk,
A.Chowdhery,Q.V.Le,E.H.hsinChi,D.Zhou,andJ.Wei. Chal-
andT.Hoefler. Graphofthoughts:Solvingelaborateproblemswith
lengingbig-benchtasksandwhetherchain-of-thoughtcansolvethem.
largelanguagemodels. InM.J.Wooldridge,J.G.Dy,andS.Natara-
InAnnualMeetingoftheAssociationforComputationalLinguistics,
jan,editors,Thirty-EighthAAAIConferenceonArtificialIntelligence,
2022.URLhttps://api.semanticscholar.org/CorpusID:252917648.
AAAI 2024, Thirty-Sixth Conference on Innovative Applications of
[22] A.Vaswani,N.M.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.
Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educa-
Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need.
tional Advances in Artificial Intelligence, EAAI 2014, February 20-
In Neural Information Processing Systems, 2017. URL https://api.
27,2024,Vancouver,Canada,pages17682–17690.AAAIPress,2024.
semanticscholar.org/CorpusID:13756489.
doi:10.1609/AAAI.V38I16.29720. URLhttps://doi.org/10.1609/aaai.
[23] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bow-
v38i16.29720.
man. Glue:Amulti-taskbenchmarkandanalysisplatformfornatu-
[4] J.Cabrera,M.S.Loyola,I.Magaña,andR.Rojas. Ethicaldilemmas,
rallanguageunderstanding. InBlackboxNLP@EMNLP,2018. URL
mentalhealth,artificialintelligence,andllm-basedchatbots. InInter-
https://api.semanticscholar.org/CorpusID:5034059.
nationalWork-ConferenceonBioinformaticsandBiomedicalEngineer-
[24] L.Wang,W.Xu,Y.Lan,Z.Hu,Y.Lan,R.K.-W.Lee,andE.-P.Lim.
ing,2023.URLhttps://api.semanticscholar.org/CorpusID:259335839.
Plan-and-solveprompting:Improvingzero-shotchain-of-thoughtrea-
[5] Y.Cao,S.Li,Y.Liu,Z.Yan,Y.Dai,P.S.Yu,andL.Sun. Acom-
soningbylargelanguagemodels.InAnnualMeetingoftheAssociation
prehensivesurveyofai-generatedcontent(aigc):Ahistoryofgener-
forComputationalLinguistics,2023. URLhttps://api.semanticscholar.
ative ai from gan to chatgpt. ArXiv, abs/2303.04226, 2023. URL
org/CorpusID:258558102.
https://api.semanticscholar.org/CorpusID:257405349.
[25] Z.Wang,S.Mao,W.Wu,T.Ge,F.Wei,andH.Ji. Unleashingthe
[6] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova. Bert:Pre-training
emergentcognitivesynergyinlargelanguagemodels:Atask-solving
ofdeepbidirectionaltransformersforlanguageunderstanding.InNorth
agentthroughmulti-personaself-collaboration. 2023. URLhttps://api.
American Chapter of the Association for Computational Linguistics,
semanticscholar.org/CorpusID:259765919.
2019.URLhttps://api.semanticscholar.org/CorpusID:52967399.
[26] J.Wei,X.Wang,D.Schuurmans,M.Bosma,E.H.hsinChi,F.Xia,
[7] L. Goetz, M. Trengove, A. A. Trotsyuk, and C. A. Federico. Un-
Q.Le,andD.Zhou. Chainofthoughtpromptingelicitsreasoningin
reliable llm bioethics assistants: Ethical and pedagogical risks. The
largelanguagemodels. ArXiv,abs/2201.11903,2022. URLhttps://api.
American Journal of Bioethics, 23:89 – 91, 2023. URL https://api.
semanticscholar.org/CorpusID:246411621.
semanticscholar.org/CorpusID:263774936.
[27] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and
[8] Z.Ji,N.Lee,R.Frieske,T.Yu,D.Su,Y.Xu,E.Ishii,Y.Bang,D.Chen,
K. Narasimhan. Tree of thoughts: Deliberate problem solving with
W.Dai,A.Madotto,andP.Fung. Surveyofhallucinationinnatural
large language models. ArXiv, abs/2305.10601, 2023. URL https:
languagegeneration. ACMComputingSurveys,55:1–38,2022. URL
//api.semanticscholar.org/CorpusID:258762525.
https://api.semanticscholar.org/CorpusID:246652372.
[28] K.-C.Yeh,J.-A.Chi,D.-C.Lian,andS.-K.Hsieh. Evaluatinginter-
[9] T.Kojima,S.S.Gu,M.Reid,Y.Matsuo,andY.Iwasawa. Largelan-
faced llm bias. In Taiwan Conference onComputational Linguistics
guagemodelsarezero-shotreasoners. ArXiv,abs/2205.11916,2022.
and Speech Processing, 2023. URL https://api.semanticscholar.org/
URLhttps://api.semanticscholar.org/CorpusID:249017743.
CorpusID:264555752.
[10] E.Levi,E.Brosh,andM.Friedmann. Intent-basedpromptcalibration:
[29] A.A.Young,B.Chen,C.Li,C.Huang,G.Zhang,G.Zhang,H.Li,
Enhancingpromptoptimizationwithsyntheticboundarycases. ArXiv,
J. Zhu, J. Chen, J. Chang, K. Yu, P. Liu, Q. Liu, S. Yue, S. Yang,
abs/2402.03099,2024. URLhttps://api.semanticscholar.org/CorpusID:
S. Yang, T. Yu, W. Xie, W. Huang, X. Hu, X. Ren, X. Niu, P. Nie,
267412105.
Y. Xu, Y. Liu, Y. Wang, Y. Cai, Z. Gu, Z. Liu, and Z. Dai. Yi:
[11] M.Lewis,Y.Liu,N.Goyal,M.Ghazvininejad,A.rahmanMohamed,
Openfoundationmodelsby01.ai. ArXiv,abs/2403.04652,2024. URL
O.Levy,V.Stoyanov,andL.Zettlemoyer. Bart:Denoisingsequence-
https://api.semanticscholar.org/CorpusID:268264158.
to-sequence pre-training for natural language generation, translation,
[30] J.Yu,R.He,andR.Ying. Thoughtpropagation:Ananalogicalap-
andcomprehension. InAnnualMeetingoftheAssociationforCom-
proach to complex reasoning with large language models. ArXiv,
putational Linguistics, 2019. URL https://api.semanticscholar.org/
abs/2310.03965,2023. URLhttps://api.semanticscholar.org/CorpusID:
CorpusID:204960716.
263831197.
[12] Q.Liu,N.Chen,T.Sakai,andX.-M.Wu. Afirstlookatllm-powered
[31] Z.Zhang,A.Zhang,M.Li,andA.J.Smola.Automaticchainofthought
generativenewsrecommendation. ArXiv,abs/2305.06566,2023. URL
prompting in large language models. ArXiv, abs/2210.03493, 2022.
https://api.semanticscholar.org/CorpusID:263891105.
URLhttps://api.semanticscholar.org/CorpusID:252762275.
[13] A.Madaan,N.Tandon,P.Gupta,S.Hallinan,L.Gao,S.Wiegreffe,
[32] H.Zhao,H.Chen,F.Yang,N.Liu,H.Deng,H.Cai,S.Wang,D.Yin,
U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, B. P. Ma-
andM.Du. Explainabilityforlargelanguagemodels:Asurvey. ACM
jumder,S.Gupta,A.Yazdanbakhsh,andP.Clark. Self-refine:Itera-
TransactionsonIntelligentSystemsandTechnology,2023. URLhttps:
tiverefinementwithself-feedback.ArXiv,abs/2303.17651,2023.URL
//api.semanticscholar.org/CorpusID:261530292.
https://api.semanticscholar.org/CorpusID:257900871.
[33] W.X.Zhao,K.Zhou,J.Li,T.Tang,X.Wang,Y.Hou,Y.Min,B.Zhang,
[14] G.F.N.Mvondo,B.Niu,andS.Eivazinezhad. Generativeconversa-
J.Zhang,Z.Dong,Y.Du,C.Yang,Y.Chen,Z.Chen,J.Jiang,R.Ren,
tionalaiandacademicintegrity:Amixedmethodinvestigationtoun-
Y.Li,X.Tang,Z.Liu,P.Liu,J.Nie,andJ.rongWen. Asurveyof
derstandtheethicaluseofllmchatbotsinhighereducation.SSRNElec-
largelanguagemodels. ArXiv,abs/2303.18223,2023. URLhttps://api.
tronicJournal,2023. URLhttps://api.semanticscholar.org/CorpusID:
semanticscholar.org/CorpusID:257900969.
261311676.
[34] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and
[15] A.F.Oketunji,M.Anas,andD.Saina. Largelanguagemodel(llm)
J.Ba.Largelanguagemodelsarehuman-levelpromptengineers.ArXiv,
bias index - llmbi. ArXiv, abs/2312.14769, 2023. URL https://api.
abs/2211.01910,2022. URLhttps://api.semanticscholar.org/CorpusID:
semanticscholar.org/CorpusID:266521434.
253265328.
[16] M.S.Orenstrakh,O.Karnalim,C.A.Suárez,andM.Liut. Detect-
ing llm-generated text in computing education: A comparative study
for chatgpt cases. ArXiv, abs/2307.07411, 2023. URL https://api.
semanticscholar.org/CorpusID:259924631.
[17] S.Peng,W.Swiatek,A.Gao,P.Cullivan,andH.Chang. Airevolution
onchatbot:Evidencefromarandomizedcontrolledexperiment.ArXiv,
abs/2401.10956,2024. URLhttps://api.semanticscholar.org/CorpusID:
267068546.