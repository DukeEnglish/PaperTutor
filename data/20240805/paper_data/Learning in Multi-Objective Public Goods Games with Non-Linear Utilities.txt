Learning in Multi-Objective Public Goods Games
with Non-Linear Utilities
NicoleOrzana,*,ErmanAcarb,DavideGrossia,b,PatrickMannionc andRoxanaRa˘dulescud,e
aUniversityofGroningen
bUniversityofAmsterdam
cUniversityofGalway
dUtrechtUniversity
eVrijeUniversiteitBrussels
ORCID(NicoleOrzan): https://orcid.org/0000-0002-9204-0688,ORCID(ErmanAcar):
https://orcid.org/0000-0001-7541-2999,ORCID(DavideGrossi): https://orcid.org/0000-0002-9709-030X,ORCID
(PatrickMannion): https://orcid.org/0000-0002-7951-878X,ORCID(RoxanaRa˘dulescu):
https://orcid.org/0000-0003-1446-5514
Abstract. Addressing the question of how to achieve optimal isaNashequilibrium[34].Werefertothiskindofgameasmixed-
decision-makingunderriskanduncertaintyiscrucialforenhancing motives,sincetheincentivesoftheagentsarepartiallymisaligned.
thecapabilitiesofartificialagentsthatcollaboratewithorsupporthu- Inadditiontoincentivemisalignment,otherfactorsinfluencingthe
mans.Inthiswork,weaddressthisquestioninthecontextofPublic emergenceofcooperationinmanyreal-worldscenariosincludeun-
GoodsGames.Westudylearninginanovelmulti-objectiveversion certaintyanddifferentindividualattitudestowardsrisk[20,27].
ofthePublicGoodsGamewhereagentshavedifferentriskprefer- Uncertainty can have different sources: we refer to environmen-
ences, by means of multi-objective reinforcement learning. We in- tal uncertainty when actors are unsure about the amount of goods
troduceaparametricnon-linearutilityfunctiontomodelriskprefer- theycanreceivefromtheenvironment[51,4],andtosocialuncer-
encesatthelevelofindividualagents,overthecollectiveandindivid- taintywhenitcomestoambiguityabouttheopponents’possibleac-
ualrewardcomponentsofthegame.Westudytheinterplaybetween tions[16,10].Individualpreferencesexpressapersonalinclination
suchpreferencemodellingandenvironmentaluncertaintyonthein- towards one choice over another. In the specific context of PGGs,
centivealignmentlevelinthegame.Wedemonstratehowdifferent weareinterestedinmodellingtwomaintypesofindividualsinthe
combinationsofindividualpreferencesandenvironmentaluncertain- presence of uncertainty: those that are biased towards taking risks
tiessustaintheemergenceofcooperativepatternsinnon-cooperative in the presence of uncertainty, also called risk-seeking agents, and
environments(i.e.,wherecompetitivestrategiesaredominant),while others which are inclined not to take risks, also called risk-averse
otherssustaincompetitivepatternsincooperativeenvironments(i.e., agents. Wemodeltheseattitudesusingaparametricnonlinearutil-
wherecooperativestrategiesaredominant). ityfunctionoftherewardreceivedbyindividualsastheresultoftheir
investmentinthecollectivegood.
Sinceweareworkingwithnon-linearutilityfunctionsinthePGG,
1 Introduction
weneedtodecoupleourperspectivefromtheliteratureonthePGGs
addressingnon-linearpublicgoodproductions.Thisbranchfocuses
How can cooperation emerge and sustain itself in situations where
onsettingswherethepublicgoodresultsfromanon-linearproduc-
agentsdonotnecessarilyhaveadirectmotiveforcooperation?This
tionprocess[40].Thesearecallednon-linearpublicgoodgamesand
isoneofthefundamentalquestionsinvariousresearchareas,such
allowonetomodelcertainreal-worldsituations(populationsofbac-
asevolutionarybiology[21,35],politicalsciences[8,9],cognitive
teria,viruses,orcooperativehunting[38,12]).Incontrast,weshift
sciences[42]andphysics[11].Toanswerthisquestion,researchers
ourfocustoanindividuallevelandcapturesettingswherepotentially
developedandstudiedmodelsofreal-worldscenariosinvolvingten-
differentattitudestowardsriskcanoccurwithinapopulation.
sionbetweenthecollectiveandpersonalmotives,calledsocialdilem-
To model risk attitudes, in our work, we explicitly decouple the
mas[13,26].Themaincharacteristicofthesesocialdilemmasisthat
collectiveversustheindividualincentivesexperiencedbytheagents,
playersarebetteroffdefectingattheindividuallevel,while,atthe
andparameterizethecollectiveincentiveattheindividuallevel.This
grouplevel,thebestoutcomeismutualcooperation.
choiceallowsustomodelsettingswhereindividualsinapopulation
In this work, we focus on a specific class of social dilemmas
can have different perceptions regarding these incentives. Further-
known as Public Goods Games (PGG), extensively studied in lit-
more,wetakeamulti-objectiveapproachtotheoptimizationofthese
erature [46, 3]. A PGG describes situations where cooperation by
two levels of rewards, drawing on multi-objective reinforcement
allagentsisParetooptimal,butbecauseoftheprofitabilityoffree-
learning(MORL)methods.Thiswaywecaninvestigatelearnedbe-
riding [5], rational agents fail to cooperate: defection by all agents
havioursthatemergefromindividuallypreferredtrade-offsbetween
∗CorrespondingAuthor.Email:n.orzan@rug.nl thecooperativeandcompetitiveobjectives.
4202
guA
1
]AM.sc[
1v28600.8042:viXraContributions.WeinvestigatelearninginPGGswhereagentshave [40],theevolutionarydynamicsoftwodifferentpopulationscollab-
differentriskpreferences,modelledasnon-linearitiesoverthegame oratingfortheproductionofanon-linearpublicgoodisinvestigated.
payoffs.Westudytheinterplaybetweenthismechanismandenviron- In[15]authorsexploretheeffectsofdifferentnon-linearPGGson
mentaluncertaintyfromamulti-objectiveperspective.Morespecifi- theevolutionofcooperationusingDarwiniandynamics.
cally,ourcontributionsareasfollows: Intheaforementionedliterature,non-linearitiesinPGGsaretyp-
1. We propose a novel multi-objective multi-agent (MOMA) en- icallyfunctionsthatinfluencetheproductionofthepublicgood.In
vironment based on the extended Public Goods Game (EPGG) our work, however, we take a different perspective by introducing
[37], called the Multi-Objective EPGG (MO-EPGG). This envi- non-linearities at the level of the individual utilities extracted from
ronment, next to facilitating training agents on games with dif- rewards.Morespecifically,ourgoalistomodelindividuals’attitudes
ferentlevelsofincentivealignment,alsoallowsonetoexplicitly towardsrisk.Indoingso,wefollowdecisiontheorywhichseeksto
model the trade-off between the individual and the cooperative understand human decision processes and derive optimal decision-
componentsofPGGs.Moreover,itenablesthedecouplingofen- makingstrategies[50,29,24],thereforethestudyofriskanduncer-
vironmentalandsocialuncertainties,allowingfortheanalysisof taintyhasbeenacentralfocus.Somestudieshaveshownthatpeople
theirimpact,bothwhenoccurringconcurrentlyorinisolation. makedecisionsbasedonsomesubjectivefunctionoftheinvestment
2. Weproposeanon-linearutilityfunctionthatallowsonetocom- theymade[47,18].Forinstance,anindividual’sriskattitudeisoften
bine the collective and individual rewards, parameterized at the described as a function of the investment made (x) by means of a
agentlevel.Theselectedshapeoftheutilityfunctionallowsusto
utilityfunctionshapedasu(x)=xβ.Here,theparameterβgoverns
modelrisk-averseandrisk-seekingagents,byoperatingaconvex theriskpreferenceoftheindividual:if0 < β < 1,thefunctionis
or concave transformation over the collective game reward, re- concave,signifyingrisk-aversion;ifβ > 1,thefunctionisconvex,
spectively.Moreover,itallowsustomodelapopulationofagents indicatingarisk-seekingattitude[24].Inourwork,wedrawonthis
withvaryingattitudestowardsrisk. ideatoformulateautilityfunctionthatallowsustomodelindividual
3. WeperformananalysisoftheMO-EPGGunderdifferentmulti- preferencesforactorsparticipatinginthePGG.
objectiveoptimisationcriteria.Inparticular,welookatthejoint
impactofdifferentriskpreferencesandincentivealignmentlevels 2.2 Multi-ObjectiveReinforcementLearning
onthebestresponses,theNashequilibriaandthepriceofanarchy.
4. We perform experiments on a population of independent multi- Inthefieldofreinforcementlearning,themainfocusisoftentosolve
objectivereinforcementlearningagentstrainedontheMO-EPGG. single-objectiveproblems,bydeterminingtheagent’sbestpolicyto
Weshowthatrisk-averseutilityfunctionsstronglydiminishcoop- reachaspecificgoal.However,real-worldchallengesareofamulti-
erationincaseswithandwithoutuncertainty.Inthepresenceof objectivenaturemostofthetime[44].Autonomousagents,whether
environmentaluncertainty,risk-seekingutilitiesimprovecooper- human or artificial, need to optimize for multiple goals simultane-
ationinenvironmentswheredefectionisthedominantstrategy. ously,orfindatrade-offbetweenthem.Thisisthecentralconcept
ofmulti-objectivereinforcementlearning(MORL)[23,44],afield
thathasdevelopedrapidlyinrecentyears[2,52].InMORL,thecore
2 RelatedWork
idea is to receive vector rewards from the environment instead of
Therelatedliteraturecanbegroupedundertwomaincategories:non- scalarrewards.Undertheutility-basedperspective[23],rewardscan
linear utilities in public good games and multi-objective reinforce- becombinedbymeansofascalarizationfunctiontodeterminethe
mentlearning.Inthissection,wedescribethemrespectively. finaloptimisationgoal.Often,alinearscalarizationfunctionisem-
ployed,whichallowstheemploymentofsingle-objectiveRLmeth-
ods. Alternatively, other choices include monotonically increasing
2.1 Non-LinearUtilitiesinPublicGoodsGames
non-linearscalarizationfunctions[1,43].Theseareofparticularin-
Although the PGG with linear utility functions is the most known terestforourworksincenon-linearfunctionsareoftenusedtomodel
andcommonlyused,variousmodelsofnon-linearPGGshavebeen utilitiesunderuncertaintyandrisk,especiallyintheeconomicsliter-
proposedintheliteratureaswell.Inthethresholdpublicgoodsgame, ature,whichaimsatmodellinghumanbehaviour[33,49].
theresultingpublicgoodisgivenbyastepfunctionofthenumber Anotherpartofthisfieldofresearchfocusesonfairness,i.e.,how
ofcooperators:theresourceiscreatedonlyifaminimumfractionof tooptimizethetrade-offamongtheobjectivesofdifferentindividu-
actorsparticipateintheproductionofthepublicgood[14].Whenthe alsunderparticularfairnessconstraints[48,22,19].Forexample,in
minimumnumberofparticipantsis1,thisiscalledtheVolunteer’s [48],authorsemploydeepRLtechniquestolearnapolicythattreats
Dilemma [6, 17]. A sigmoid public goods function closely models users equitably. We build on the framework developed in [48], but
manybiologicalsystemswheretheoutputproductionissmallforlow rather than focusing on the fair treatment of a set of users, we in-
inputlevelsandbiggerforintermediateinputs,decreasingagainfor vestigatetheeffectofuncertaintyandindividuals’attitudestowards
evenbiggerones[7,12].Inotherparadigms,publicgoodproduction risk.Tothisend,weextendtheirapproachtoworkwithadifferent
is modelled by applying a concave (convex) function over agents’ scalarisationfunctioncustomizedforourscenario,whichallowsus
contributions, where the produced good is lesser (greater) than the tomodelindividualpreferences,andtrainindependentreinforcement
goodprovidedbyalinearfunctionofthecontributions. learningagentsinamulti-objectivesetting.Wethusadoptamulti-
Several papers focused on analyzing non-linear public good objectivemulti-agentreinforcementlearning(MOMARL)[41]per-
games,bydifferentmeans.In[33]authorsemploynon-linearPGG spective,whichextendsMORLtomulti-agentscenarios.
withdifferentincentivestructurestoanalyzebehaviouralsubtyping,
i.e.,ifcooperativebehaviourinonetaskcanpredictcooperativebe-
3 Preliminaries
haviour in another. In [53], evolutionary dynamics techniques are
employedtostudytheroleofdifferentnon-linearproductionfunc- Inthissection,wepresenttheformaldefinitionsandthebackground
tionsontheevolutionofcooperationinfinitepopulations,whilein knowledge.TheseincludetheExtendedPublicGoodsGame,multi-objectivestochasticgamesandthemulti-objectiveoptimizationcri- 3.3 OptimizationCriteria
teria.
Inreinforcementlearning,thegoalofanagentistofindapolicyπ
that maximizes the expected scalar return Vπ = E [(cid:80)∞ γtr ].
3.1 TheExtendedPublicGoodsGame π t=0 t
In MORL, depending on how agents derive their utility, there are
twooptimisationcriteriaonecanemployinthescalarisationprocess
TheExtendedPublicGoodsGames[36]isatuple⟨N,c,A,f,u⟩,
whereN isthesetofplayerswhosesizeisdenotedas|N|=n∈N. whenmaximisingtheexpecteddiscountedlong-termrewardvector:
Every player i is endowed with some amount of wealth (or coins)
• TheScalarisedExpectedReturn(SER)criterion:
c ∈ R , and c = (c ,...,c ) denotes the tuple containing all
i ≥0 1 n
agents’ coins. Each agent can decide whether to invest in the pub- (cid:18) (cid:20) ∞ (cid:21)(cid:19)
licgood(cooperate)orkeeptheendowmentforthemselves(defect); V uπ =u E π (cid:88) γtr t , (2)
therefore,thesetAofallowedactionsconsistsofcooperate(C)and t=0
defect(D)i.e.,A = {C,D}.Thevectora = (a ,...,a ) ∈ An
1 n where π : S × A → [0,1] is the agent’s policy, and r =
t
representstheactionprofileoftheagents.Thequantityf iscalled
R(s ,a ,s )isthevectorialrewardattimestept.
t t t+1
multiplicationfactor,andspecifiesthescalarbywhichthetotalin-
• TheExpectedScalarisedReturn(ESR)criterion:
vestmentismultipliedinordertoproducethepublicgood.There-
sultingquantityisthenevenlydistributedamongallagents.Thedif- (cid:20) (cid:18) ∞ (cid:19)(cid:21)
ference with respect to the original PGG lies in the interval of al- V uπ =E π u (cid:88) γtr t (3)
lowedvaluesforf.WhileinthePGGf ∈ (1,n),intheEPGGwe t=0
take f ∈ R . The reward function for each agent i is defined as
≥0 Whichoneofthesecriteriatochoosedependsontheproblemathand
r :An×R ×Rn →R,with:
i ≥0 ≥0 [41].Ifwecareaboutthegoodnessofasinglepolicyexecution,ESR
isthecorrectcriterion.Ifinstead,weareinterestedinthequalityof
n
r (a,f,c)= 1 (cid:88) c I(a )·f +c (1−I(a )), (1) averagepolicyexecutions,weshoulduseSER.Inthiswork,weopt
i n j j i i fortheSERcriterioninthelearning,modellingagentsthatareinter-
j=1
estedinoptimisingtheirbehaviourinrepeatedinteractionsettings.
wherea isthej−thentryoftheactionprofileaandI(a )isthein-
j j
dicatorfunction,equalto1iftheactionoftheagentjiscooperative,
4 Multi-ObjectiveEPGG
and0otherwise,andc denotesthej−theentryofc.Forthesakeof
j
simplicity,inthefollowing,weassumeallendowmentstobeequal, Weformulateamulti-objectiveversionoftheEPGG,calledMulti-
namelyc i =c, ∀i∈N. ObjectiveExtendedPublicGoodsGame(MO-EPGG),byemploying
Depending on the value of f, the EPGG can model three types theframeworkofmulti-objectivestochasticgames,outlinedinSec-
ofscenarios.When1 < f < n,likeintheclassicPGG,wemodel tion3.2.Inourframework,thestatespaceconsistsofthevalueofthe
mixed-motivesscenarios,inwhichallagentsplayingdefectisadom- multiplicationfactorf ofthegamecurrentlybeingplayed,andthe
inant strategy equilibrium. Yet, this profile is Pareto dominated by actionspacecoincideswiththatofthesingle-objectiveEPGG(Sec-
theprofileinwhichallagentscooperate.When0 ≤ f ≤ 1,play- tion 3.1). We notice that the transition function for this framework
ingdefectisaParetooptimaldominantstrategy(andthereforeNash) issimplyarandomsamplingfromthesetofpossiblemultiplication
equilibrium.Inaddition,theEPGGcanalsomodelfullycooperative factorsatthebeginningofeachepisodeanddeterministicallyreturns
scenarios (i.e., when f ≥ n) in which the cooperation profile is a thatsamef valueatallthesubsequentstepsoftheepisode.
Paretooptimaldominantstrategy(andthereforeNash)equilibrium. To complete our multi-objective formulation of the EPGG, we
need to vectorize the scalar reward signal obtained by agents in
3.2 Multi-ObjectiveStochasticGames the EPGG. This process is called multi-objectivization of single-
objective problems [25, 30]. By observing the form of the reward
We model the multi-objective multi-agent interactions using the function in Equation 1, we can easily distinguish between the part
multi-objectivestochasticgame(MOSG)framework,definedasthe thatdefinesthecollective(rC)andtheindividualpayoff(rI):
tupleM = (S,A,T,γ,R),withn ≥ 2agentsandd ≥ 2objec-
tives,where:
rC(a,f,c)=
1 (cid:88)n
c I(a )·f (4)
• Sisthestatespace, i n j j
• A=A ×···×A isthesetofjointactions,withA beingthe j=1
1 n i
actionsetofagenti, r iI(a,c)=c i(1−I(a i)). (5)
• T: S×A×S →[0,1]istheprobabilistictransitionfunction,
• γisthediscountfactor, Then, in the proposed MO-EPGG, the vectorial reward received
• R = R ×···×R aretherewardfunctions,whereR : S× by agent i, given action profile a, current multiplication factor f,
1 n i
A×S →Rdisthevectorialrewardfunctionofagentiforeach andatupleofendowmentsc,isasfollows:
ofthedobjectives.1
r
(a,f,c)=(cid:0) rC(a,f,c),rI(a,c)(cid:1)
. (6)
Wetakeautility-basedperspective[44]formulti-objectivedecision i i i
making,assumingthateachagentihasautilityfunctionu i :Rd → ThiscompletesourdescriptionoftheMO-EPGGasaMOSGwith
Rthatmapsthereceivedrewardvectortoascalarvalue,determining
d=2objectives.InFigure1wedisplayanexampleofthevectorial
thedesiredtrade-offbetweentheobjectives. rewardsreceivedbyN =2agentsplayingtheMO-EPGGforthree
differentvaluesofthemultiplicationfactorf.
1Wenotethatinthisarticlethetermsrewardandpayoff aresynonyms.For
thesakeofclarityandconsistency,westicktotheformertermwhichaligns To define the agents’ utility functions, we follow a similar ap-
withthereinforcementlearningterminology. proachtotheincentivestructureproposedbyMullettetal.[33],butnote that we employ the non-linear function at an individual level, rC(a ,f,c)β > rI(a ,f,c),whichisthecasewhen(cf)β > c.
C i D
tomodelagents’riskattitudesaspreferencesoverthereceivedvec- Thisrelationshipbetweenthevariablesinducesasharedpreference
torialpayoff.Inparticular,inourmodel,thegaininutilityobtained overcollectivecooperativebehaviourinotherwisedefectivescenar-
fromthecollectiverewardbehavesnon-linearlybymeansofanex- ios (the cases when f < 1). In general, collective cooperation is
ponential function where β i serves as an exponent. Therefore, we preferredovercollectivedefectionwhenevereitherofthefollowing
definethefollowingnon-linearutilityfunctionthatspecifiesthefinal conditionsholds:
scalarisedutilityfortheMO-EPGGagents:
log(c)
β < if 0<cf <1 (8)
u (g )=(cid:0) gC(cid:1)βi +gI, (7) log(cf)
i i i i
with β i being a hyperparameter. In our setting, gC and gI repre- β > log(c) if cf >1. (9)
sentexpectedreturns(orexpecteddiscountedsumsofrewards)i.e., log(cf)
g = (cid:80) γtr . Note that we are employing expected returns rather
t t Inthesameway,collectivedefectionispreferredovercollective
thanrewardssinceweareworkingundertheSERcriterion.Inthis
cooperationwhenever(cf)β <c.
equation,theparameterβ governstherisk-seeking/aversebehaviour
i
ofagentitowardsthecollectiveexpectedreturngC,namelyavalue SER. Fromthispointonwards,wefocusonanalysinga2-player
i
β = 1 returns a linear utility function, while β < 1 generates a MO-EPGG,undertheSERcriterion.Wedeterminetheminimumco-
concave function (with beta > 0) which models a risk-avoiding operationleveloftheopponentforwhichtheplayer’sbestresponse
agent, while β > 1 generates a convex function which models a istocooperate.Wecomputethisvaluefordifferentvaluesoff and
risk-seekingagent[33]. β.TheresultsaredisplayedinTable1.Wenotethattheminimum
InEquation7,theexponentisonlyappliedoverthecollectivere- cooperationleveloftheopponentshouldbestrictlygreaterthanthe
wardcomponent.Thischoiceismotivatedbyourconceptualization valuepresentedinthetable,forthebestresponsetobecooperation.
ofthecollectiverewardastheresultofariskyinvestment.Theresult We can observe that, for the game with competitive incentive
dependsonthevalueofthemultiplicationfactorf,whichmightnot alignment(f = 0.5),thebestresponseforeachplayeristodefect
beknownwithcertainty;andtheactionsoftheotherplayers. everytimethevalueofβ ≤2(i.e.,theopponent’sprobabilitytoco-
operatecannotbe>1,hencetheconditionisunattainable).Ifβ >2
f =0.5 Player1 thebestresponseiscooperationwheneverthestrategyoftheoppo-
C D nentistocooperatewithaprobabilitybiggerthanthevaluepresented
C [2,0],[2,0] [1,0],[1,4] inthetable.Forexample,forβ =3,thebestresponseiscooperation
Player0
wheneverthestrategyoftheopponentistocooperatewithaproba-
D [1,4],[1,0] [0,4],[0,4]
bilitybiggerthan0.6.Forβ =4,thethresholdmovesto0.4,andto
0.3forforβ = 5.Forboththegameswithf = 1.0andthegame
f =1.5 Player1 withamixed-motiveincentivealignment,f =1.5,thebestresponse
C D is to defect every time β ≤ 2, and to cooperate otherwise. For a
C [6,0],[6,0] [3,0],[3,4] game with cooperative incentive alignment, the best response is to
Player0
cooperateeverytimeβ ≥1.
D [3,4],[3,0] [0,4],[0,4]
Table 1: Minimum value for the cooperation of the opponent for
f =2.5 Player1 whichthebestresponsestrategyoftheplayeriscooperation,com-
C D putedfordifferentvaluesoff andβ.
C [10,0],[10,0] [5,0],[5,4] β
Player0 0.5 1 2 3 4 5 6
f
D [5,4],[5,0] [0,4],[0,4]
0.5 1. 1. 1. 0.6 0.4 0.3 0.2
Figure1:Multi-objectivepayoffmatricesreceivedbyN =2players 1.0 1. 1. 0. 0. 0. 0. 0.
with4coinseach,playingtheMO-EPGGwithmultiplicationfactors 1.5 1. 1. 0. 0. 0. 0. 0.
of0.5,1.5and2.5,whentakingthecooperative(C)ordefective(D) 2.5 1. 0. 0. 0. 0. 0. 0.
actions.
NEunderSER. Tocomplementtheaboveresults,weperforman
4.1 GameAnalysis
analysis of the Nash equilibria for the MO-EPGG played by two
agents under SER. Similar to [31], we sweep through the space of
We analyse the MO-EPGG with the proposed utility function. We
possiblejointstrategiesandidentifytheNashequilibria2forasetof
firstanalysetheMO-EPGGunderESRandSER,toexaminethedy-
games.Weselectvaluesoffin{0.5,1.0,1.5,2.0,2.5,3.0},tocover
namicsofbestresponsesfordifferentvaluesofthegameandutility
a spectrum of competitive, mixed-motive, and cooperative games.
functionparameters.Second,weinvestigatetheimpactofdifferent
valuesofβandf onthesetofNashequilibriaunderSER.
Additionally,weletthevalueofβ spanovertheinterval[0.,3.],to
capturerisk-averse,risk-neutralandrisk-seekingtendencies.There-
ESR. From Equation 7, we can observe that the preference be-
sultsarepresentedinFigure2,wherewedisplay,foroneagent,the
tweenthecooperativeordefectivebehaviourintheMO-EPGGde-
valueoftheprobabilityofcooperationintheNashequilibriastrategy
pends on the relationship between three values, namely, f, c and
asafunctionofβ.Weprovidetheplotforoneagentonly,sincethe
β. In particular, assuming a uniform value of β among the whole
correspondingplotfortheotheragentisidentical.
population of agents (β = β for all i ∈ N), the collective co-
i
operative action (a C = (C,...,C)) is preferred over the collec- 2TocheckwhetherajointstrategyisanequilibriumunderSER,weusethe
tive defective action (a D = (D,...,D)) by all the agents when iterated_best_responseandverify_nashmethodsoftheRamolibrary[45].Ontheotherhand,aPoAwithavalueof1indicatesanoverlapbe-
tweenthesocialoptimumandtheworst-caseselfishaction.Todefine
thesocialoptimuminoursetting,weemploytheutilitarianwelfare
function,summingtheoutcomesoftheutilityfunctionsoftheplay-
ers3:W(π) = (cid:80)n u (Vπ),whereVπ istheexpectedvectorial
i=0 i i i
returnofplayeri,underthejointstrategyπ.Then,thePoAisdefined
asfollows:
max W(π)
PoA= s . (10)
min W(π)
π∈Nash
Figure3:PriceofAnarchyforvaryingvaluesoff andβ.
InFigure3wedisplaythevaluesofthePoAfora2-playerMO-
EPGG,varyingthevaluesoff andβ.Fromthefigure,wecanob-
servethatforeveryf,thevalueofthePoAis1forβ < 0.5.This
isduetomutualdefectionbeingtheonlyNashequilibriumandbest
welfarepoint.Wecanalsoobservethateveryf displaysarangeof
βforwhichthevalueofthePoAisgreaterthan1.Thisisduetothe
presenceofaNashequilibriainmutualdefectionwhenthewelfareis
insteadmaximizedinmutualcooperation.Wecanalsoobservethat
Figure2:TheprobabilityofactionC forPlayer0undertheNEofa
thePoAvaluestabilizesto1foreverygamewhenβovercomesacer-
2-playerMO-EPGG,forvaryingvaluesoff andβ.Thecorrespond-
tainthreshold,whichisdependentonthevalueoff.Thisisdueto
ingplotforPlayer1isidentical.Wenotethatinthecaseinwhich
mutualcooperationbeingboththestrategythatmaximizesthewel-
twomixed-strategyNEarepresentforthesamevalueofβ,thejoint
fareandtheonlyNashequilibriumofthegame.
strategiesareformedbythetwodifferentpointspresent.Thestrate-
giesoftheagentsareidenticalforpure-strategyNE.
5 Experimentalsetup
FromFigure2wecanobserve,forallthegames,thecoexistence
oftwoNashequilibriawheneverβ <1.Ateachoftheseequilibria, MO-DQN WetrainindependentRLagentsbyadaptingthemulti-
thejointstrategyofthetwoagentsdisplaysthefollowingsymmetry: objectiveversionoftheDeepQ-network(DQN)algorithm[32]de-
if one agent defects, they both are better off when the other agent scribedin[48],whichallowsustooptimizepoliciesundertheSER
slightlymovesawayfromfulldefection.Forf < 2andβ > 1we criterion4Siddiqueetal.[48]trainaDQNtopredictaQ-functionfor
observetheexistenceofanintervalofβvaluesinwhichbothmutual everyobjective.Therefore,thedimensionoftheoutputis|A|×d.We
defectionandcooperationcoexistasNashequilibria.Basedonthe adjusttheirapproachtoworkwithourscalarizationfunction(Equa-
resultsinTable1,wenoticethatforahighenoughvalueofβ,even tion7).ThelossfunctionforMO-DQNcanbeexpressedasfollows:
forf = 0.5,themutualdefectionNashequilibriumwilleventually (cid:20)(cid:16) (cid:17)2(cid:21)
disappear.Wheneverβ > 1andf > 2,asexpected,theonlyNash L(θ)=E s,a,s′,r∼D r+γ Qˆ θ′(s′,a∗)−Qˆ θ(s,a) , (11)
equilibriumismutualcooperation.Wepresentadditionalresultson
theSERintheSupplementaryMaterial,inSectionA. whereθandθ′representtheDQNweightsattwodifferenttimesteps
ofthetraining.Drepresentsthebufferofstoredtransitions,andris
thevectorreward.Wefindthebestactiona∗ byapplyingtheSER
4.2 PriceofAnarchy
optimizationcriterion,namely,byapplyingourcustomscalarization
To evaluate the goodness of the possible outcomes of the system functionutoupdatetheMO-DQNfunction:5
againsttheNashequilibriaofeachgameintheMO-EPGG(defined (cid:18) (cid:19)
byaspecificfvalue),weadaptthemetricknownasPriceofAnarchy a∗ =argmax a∈Au E[r+γ Qˆ θ′(s′,a′)] . (12)
(PoA)[28,39].ThePoAis,theratiobetweenthewelfareofthesys-
teminits“bestsolution”,i.e.socialoptimum,andthewelfareofthe 3 Inthecontextofmulti-objectivegamesunderSER,wecannotapplythe
systematitsworstNE.Thus,itexpressesthepotentialdegradation welfarefunctiondirectlyonthepayoffsofthematrixgame.
4Westressthattheneedforfunctionapproximationisduetothecontinuous
factorofthesocialoptimum.Wehighlightthathighervaluesofthe
inputvaluesprovidedbyf.
PoAindicatetheexistenceofpossibleworstoutcomesforthesys- 5Similarto[48],wecomputetheexpectationofthescalarisation,whichisa
temduetoagents’selfishness,incomparisontothesocialoptimum. lowerboundfortheSERcriterion.5.1 Experiments fromtheconcurrentlearningonasetofgameswithdifferentlevels
ofincentivealignment,aspreviouslyobservedin[37].
TheexperimentsarerunoverapoolofN =20agents.Ateachiter- Theexperimentswithβ =0.5representasystemofrisk-avoiding
ationtofthelearningprocess,amultiplicationfactorf t issampled agentsplayingtheMO-EPGG.Thisriskpreferencestronglypushes
uniformlyfromtheinterval[f min,f max],wheref minandf maxarecho- the system’s behaviour toward competition across all games. The
sen such as to include cooperative, competitive, and mixed-motive resultstaysconsistentacrossthescenarioswithandwithoutuncer-
games.Afterwards,asubsetwithM = 4activeagentsisrandomly tainty.ThisresultisconsistentwiththeanalyticalfindingsunderSER
sampledfromthepoolofN agents,toparticipateinthegamefor10 outlinedinSection4.1:forallthegameswithdifferentf values,(ϵ-
consecutiverounds.Aftertheseinteractions,theMO-DQNnetworks )collectivedefectionstrategiesaretheonlytheNashequilibria.
areupdated.GivenM =4,wepickedf min =0.5andf max =6.5,to The experiments with β ≥ 2 induce a system of risk-seeking
enableagentstoengageincompetitive,mixed-motiveandcoopera- agentsplayingintheMO-EPGG.Here,weobservethatthecooper-
tivegames.Thisenablessamplingfromasetthatcontainscompeti- ationofthesystemisdrasticallyincreasedinallgameswithrespect
tive(f < 1),mixed-motive(1 < f < M),andcooperativegames tothebaselineβ =1.Wenotethatforβ =2andf =0.5agentsdo
(f > M). Each agent receives as observation the current value of notconvergetoeithercooperationordefection.Thisoutcomealigns
themultiplicationfactor—whichcanbeobservedwithuncertainty— withtheexistenceofbothcollectivecooperationandcollectivede-
togetherwiththepreviousactionstakenbyeachopponentatthepre- fectionasNashequilibria.
vioustimestep:oi = (fi ,a−i ),wherea−i = (aj) ,j ̸= i.
t obs t−1 j∈M
Therefore,eachagentlearnsapolicyπ :O ×A →[0,1],where
i i i
6.2 Learningwithheterogeneouspreferences
O isthesetofallpossibleobservationsofagenti.
i
We model uncertainty over the observation of the multiplication
Secondly, we investigate the impact of learning in the MO-EPGG
factorasGaussiannoiseoverthevalueoff,receivedfromtheen-
whentheagents’preferencesβ areheterogeneous,i.e.,thevalueof
vironment: fi = f +N(0,σ2), where σ is the uncertainty ex- i
obs i i β iforeveryagentiissampledfromanormaldistributioncentredin
periencedbyagenti.Toensurethatthevalueoftheobservedmul- 1,i.e.β ∼ N(µ ,σ2) ∀ i ∈ N,withµ = 1.Thisallowsusto
i β β β
tiplicationfactorcohereswiththesetofallowedvaluesoff inthe
getmorevaluescentredaroundriskneutrality,andfewextremerisk-
MO-EPGG,weroundupeverynegativesampledvalueto0.
averse or risk-seeking tendencies. We performed experiments with
Alltheexperimentsarerunfor20000epochs,andresultsareav-
differentvaluesofσ ,i.e.,0.5,2and3.Theresultingsystemrepre-
β
eragedover20runsforeverycondition.TheRMSproplearningrate
sentsapopulationwhereeveryindividualhasadifferentriskprefer-
is set to λ = 0.001, and γ = 0.99. The action selection mech-
enceandiscentredonrisk-neutrality(β =1).
anism is ϵ-greedy, with ϵ = 0.01. The values of the weights are
Figure 5 reports the results for the scenarios without (top row)
wC =wI =1foralltheagents.Alltheplotsshowthevaluesofthe
and with (bottom row) uncertainty on the observations. We can
averagecooperationoftheactiveagentsateveryevaluationstepof
observe that when the σ of the distribution is small (i.e. σ =
β β
thelearningprocess.TheDQNnetworksarecomposedof2hidden
0.5)andnouncertaintyisintroduced,thecompetitiveequilibriafor
layers,andReLUnonlinearitiesareemployedbetweenlayers.6
f ∈ {0.5,1.5} is maintained, while the cooperative equilibria for
f ∈{3.5,6.5}islost.Wealsonoticethatthehigherthevalueofσ ,
β
6 Results the higher the average cooperation of the system in all the games.
This result signals the importance of the magnitude of beta on the
Wegroupourempiricalresultsintwocategories,namelyhomoge- riskattitude:thehigherβ,thehighertheriskappetite,whichinour
neouspreferences,whenthevalueofβ isidenticalforeveryagent, casetranslatestomorecooperativebehaviour.
andheterogeneouspreferences,whereeachagentiischaracterised When uncertainty is introduced, we observe that cooperation is
byadifferentβ value.Bothcategoriesincludeexperimentswithand increased in the competitive and mixed games with respect to the
i
withoutuncertaintyontheobservationofthemultiplicationfactorf. caseswithoutuncertainty.Thisresultisconsistentwithpreviousfind-
ingsonthepresenceofuncertaintyinnon-cooperativeenvironments
[36,37].Interestingly,onlythenon-cooperativegamesareaffected
6.1 Learningwithhomogeneouspreferences
bythepresenceofuncertainty:inthecooperativegames,theaverage
cooperationofthesystemisequaltotheoneobservedinthescenario
Wefirstexploretheimpactofdifferentvaluesofβonthescenarios
withoutuncertainty.
withandwithoutuncertaintyontheobservations.Weperformedex-
periments for different values of β, that define a linear (β = 1), a
convex(β > 1)andaconcave(β < 1)utilityfunction.Ineachof 6.3 EquilibriaandLearning
theseexperiments,βvaluesareidenticalforeveryagent.Theresults
We compare now our experimental results with the analysis from
fortheseexperimentsaredepictedinFigure4.
Section4.1.Weunderlinethat,whilethecomputationofequilibriais
Theexperimentswithβ = 1representthebaselinewhereagents
game-specific,theoutcomesoftheexperimentsresultfromconcur-
are playing the linear version of the MO-EPGG. Therefore, in the
rentlearningonthesetofenvironmentsmodeledbytheMO-EPGG.
gameswithoutuncertainty,weobserveasexpectedconvergenceto
ComparingtheplotsinFigure4andtheanalyticalresultsofthe
cooperationwheneverf > M,convergencetodefectionwhenever
Nash equilibria from Figure 2, we can observe that, in the case
f < 1,andacertainpercentageofcooperationwhen1 < f < M,
without uncertainty, in all the games for which f ̸= 0.5, namely
dependingonwhetherthevalueoff isclosertoacooperativeora
f ∈ {1.5,3.5,6.5}, the pool of agents learns the best response,
competitiveone.Whenuncertaintyisintroduced,cooperationisin-
which means that the system converges to the Nash equilibrium.
creasedinthecompetitiveandmixed-motivescenarios.Thisresults
Onlyforβ =1theconvergenceisnotperfect.Inthecasef =0.5,
6 Alltheparametersemployedtoperformtheexperimentsaredescribedin theagentsconvergetodefectionwhenβ <2,whileforβ ≥2they
theSupplementaryMaterial,inTable2. cooperate with a certain probability, which is higher for higher β(a)f =0.5 (b)f =1.5 (c)f =3.5 (d)f =6.5
Figure4:AveragecooperationvaluesfortheactiveDQNagentstrainedacrossenvironmentswithdifferentmultiplicationfactors,without(top
row)andwithuncertainty(bottomrow)ontheobservedmultiplicationfactor,withσ = 2,∀ i ∈ N.Thedifferentvaluesofβ areidentical
i
foreveryagentβ =β,∀i∈N.
i
(a)f =0.5 (b)f =1.5 (c)f =3.5 (d)f =6.5
Figure5:AveragecooperationvaluesfortheactiveDQNagentstrainedacrossenvironmentswithdifferentmultiplicationfactors,without(top
row)andwithuncertainty(bottomrow)ontheobservedmultiplicationfactor,withσ = 2 ∀ i ∈ N.Thevaluesofβarerandomlysampled
i
fromanormaldistributionβ ∼N(µ ,σ2)∀i∈N,withµ =1andthreedifferentvaluesofσ =0.5,2,3.
i β β β β
values.Thisresultisnotsurprisinggiventhepresenceofmorethan attitudes,centredonriskneutrality,canfailtoreachcooperationin
oneNashequilibrium.Whenuncertaintyisintroduced,aspreviously cooperativesettings.
mentioned,theaveragecooperationofthesystemincreases. Asfuturework,weplantoexploreadditionalMORLapproaches,
suchaspolicy-gradientbasedmethods,intheMO-EPGG.Addition-
ally,wewillinvestigatetheinterplaybetweenriskpreferences,un-
7 ConclusionsandFutureWork certaintyandadditionalmechanisms,suchasreputationmechanisms
andsocialnorms.Lastbutnotleast,weplantoapplyotherformsof
In this work, we introduced and analyzed a novel multi-objective non-linear utility functions, including exploring different dynamics
variant of the Extended Public Goods Game, the MO-EPGG. This betweenthecollectiveandindividualpayoffs.
allowedustomodelindividualriskattitudes,bydecouplingthecol-
lective and the individual payoffs. We investigated the role of dif-
ferent factors such as misalignment of incentives, uncertainty and Acknowledgements
individualriskpreferencesoncooperation,utilizingmulti-objective
reinforcement learning. In particular, we observed how risk-averse This research has been supported by the Hybrid Intelligence Cen-
attitudes can increase defection in cooperative environments, and, ter,a10-yearprogramfundedbytheDutchMinistryofEducation,
inversely,risk-seekingones,canincreasecooperationincompetitive Culture and Science through the Netherlands Organisation for Sci-
andmixed-motivegames,especiallywhenuncertaintyisintroduced. entificResearch(NWO).RRwaspartlysupportedbytheResearch
Moreover, we observed how a population with heterogeneous risk Foundation–Flanders(FWO),grantnumber1286223N.References cooperationincommonsdilemmas:Areviewofexperimentalpsycho-
logicalresearch.Thedramaofthecommons,pages113–156,2002.
[28] E.KoutsoupiasandC.Papadimitriou.Worst-caseequilibria.Computer
[1] M.Agarwal,V.Aggarwal,andT.Lan. Multi-objectivereinforcement sciencereview,3(2):65–69,2009.
learningwithnon-linearscalarization.InProceedingsofthe21stInter- [29] H. Levy and M. Levy. Arrow-pratt risk aversion, risk premium and
nationalConferenceonAutonomousAgentsandMultiagentSystems, decisionweights.JournalofRiskandUncertainty,25:265–290,2002.
pages9–17,2022. [30] X. Ma, Z. Huang, X. Li, Y. Qi, L. Wang, and Z. Zhu. Multiobjec-
[2] L.N.Alegre,A.Bazzan,A.Nowe,andB.CdaSilva. Multi-stepgen- tivizationofsingle-objectiveoptimizationinevolutionarycomputation:
eralizedpolicyimprovementbyleveragingapproximatemodels. Ad- asurvey.IEEETransactionsonCybernetics,2021.
vancesinNeuralInformationProcessingSystems,36,2024. [31] P. Mannion and R. Ra˘dulescu. Comparing utility-based and pareto-
[3] S.P.Anderson,J.K.Goeree,andC.A.Holt. Atheoreticalanalysisof basedsolutionsetsinmulti-objectivenormalformgames. InMulti-
altruismanddecisionerrorinpublicgoodsgames. JournalofPublic ObjectiveDecisionMakingWorkshop2023,2023.
economics,70(2):297–323,1998. [32] V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Belle-
[4] P.Andras,J.Lazarus,G.Roberts,andS.J.Lynden. Uncertaintyand mare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovski,etal.
cooperation:Analyticalresultsandasimulatedagentsociety. Journal Human-levelcontrolthroughdeepreinforcementlearning.Nature,518
ofArtificialSocietiesandSocialSimulation,2006. (7540):529–533,2015.
[5] J.Andreoni. Whyfreeride?:Strategiesandlearninginpublicgoods [33] T.L.Mullett,R.L.McDonald,andG.D.Brown.Cooperationinpublic
experiments.JournalofpublicEconomics,37(3):291–304,1988. goodsgamespredictsbehaviorinincentive-matchedbinarydilemmas:
[6] M.ArchettiandI.Scheuring.Coexistenceofcooperationanddefection Evidenceforstableprosociality.EconomicInquiry,58(1):67–85,2020.
inpublicgoodsgames.Evolution,65(4):1140–1148,2011. [34] J.F.NashJr.Equilibriumpointsinn-persongames.Proceedingsofthe
[7] M.ArchettiandI.Scheuring. Evolutionofoptimalhillcoefficientsin nationalacademyofsciences,36(1):48–49,1950.
nonlinear public goods games. Journal of Theoretical Biology, 406: [35] M.A.Nowak,A.Sasaki,C.Taylor,andD.Fudenberg. Emergenceof
73–82,2016. cooperationandevolutionarystabilityinfinitepopulations.Nature,428
[8] R.Axelrod. Theemergenceofcooperationamongegoists. American (6983):646–650,2004.
politicalsciencereview,75(2):306–318,1981. [36] N.Orzan,E.Acar,D.Grossi,andR.Ra˘dulescu.Emergentcooperation
[9] R.AxelrodandW.D.Hamilton.Theevolutionofcooperation.science, anddeceptioninpublicgoodgames. InAdaptiveandLearningAgents
211(4489):1390–1396,1981. (ALA) Workshop at AAMAS, 2023. URL https://alaworkshop2023.
[10] J.Bendor. Uncertaintyandtheevolutionofcooperation. Journalof github.io.
Conflictresolution,37(4):709–734,1993. [37] N.Orzan,E.Acar,D.Grossi,andR.Ra˘dulescu.Emergentcooperation
[11] D.ChalletandY.-C.Zhang. Emergenceofcooperationandorganiza- underuncertainincentivealignment. InProceedingsofthe2024In-
tioninanevolutionarygame. PhysicaA:StatisticalMechanicsandits ternationalConferenceonAutonomousAgentsandMultiagentSystems
Applications,246(3-4):407–418,1997. (AAMAS2024),2024.
[12] J.S.Chuang,O.Rivoire,andS.Leibler. Cooperationandhamilton’s [38] C.PackerandL.Ruttan. Theevolutionofcooperativehunting. The
ruleinasimplesyntheticmicrobialsystem.Molecularsystemsbiology, AmericanNaturalist,132(2):159–198,1988.
6(1):398,2010. [39] C.Papadimitriou. Algorithms,games,andtheinternet. InProceed-
[13] R.M.Dawes. Socialdilemmas. Annualreviewofpsychology,31(1): ingsofthethirty-thirdannualACMsymposiumonTheoryofcomputing,
169–193,1980. pages749–753,2001.
[14] K.DeJaegher. Highthresholdsencouragingtheevolutionofcooper- [40] A.Patra,V.K.Dubey,andS.Chakraborty. Coexistenceofcoordina-
ationinthresholdpublic-goodgames. ScientificReports,10(1):5863, tionandanticoordinationinnonlinearpublicgoodsgame. Journalof
2020. Physics:Complexity,3(4):045006,2022.
[15] K.DengandT.Chu. Adaptiveevolutionofcooperationthroughdar- [41] R.Ra˘dulescu,P.Mannion,D.M.Roijers,andA.Nowé.Multi-objective
winian dynamics in public goods games. PLoS One, 6(10):e25496, multi-agentdecisionmaking:autility-basedanalysisandsurvey. Au-
2011. tonomousAgentsandMulti-AgentSystems,34(1):10,2020.
[16] P.Deutchman,D.Amir,M.R.Jordan,andK.McAuliffe. Common [42] D.G.RandandM.A.Nowak.Humancooperation.Trendsincognitive
knowledgepromotescooperationinthethresholdpublicgoodsgame sciences,17(8):413–425,2013.
byreducinguncertainty. EvolutionandHumanBehavior,43(2):155– [43] M. Reymond, C. F. Hayes, D. Steckelmacher, D. M. Roijers, and
167,2022. A.Nowé. Actor-criticmulti-objectivereinforcementlearningfornon-
[17] A.Diekmann. Volunteer’sdilemma. Journalofconflictresolution,29 linearutilityfunctions. AutonomousAgentsandMulti-AgentSystems,
(4):605–610,1985. 37(2):23,2023.
[18] J.DowandS.R.daCostaWerlang. Uncertaintyaversion,riskaver- [44] D.M.Roijers,P.Vamplew,S.Whiteson,andR.Dazeley. Asurveyof
sion,andtheoptimalchoiceofportfolio.Econometrica:Journalofthe multi-objectivesequentialdecision-making. JournalofArtificialIntel-
EconometricSociety,pages197–204,1992. ligenceResearch,48:67–113,2013.
[19] Z.Fan,N.Peng,M.Tian,andB.Fain. Welfareandfairnessinmulti- [45] W.Röpke. Ramo:Rationalagentswithmultipleobjectives. https://
objective reinforcement learning. In Proceedings of the 2023 Inter- github.com/wilrop/mo-game-theory,2022.
nationalConferenceonAutonomousAgentsandMultiagentSystems, [46] F.C.Santos,M.D.Santos,andJ.M.Pacheco. Socialdiversitypro-
pages1991–1999,2023. motestheemergenceofcooperationinpublicgoodsgames. Nature,
[20] E.FehrandU.Fischbacher. Whysocialpreferencesmatter–theimpact 454(7201):213–216,2008.
ofnon-selfishmotivesoncompetition,cooperationandincentives.The [47] L.J.Savage.Thefoundationsofstatistics.CourierCorporation,1972.
economicjournal,112(478):C1–C33,2002. [48] U.Siddique,P.Weng,andM.Zimmer. Learningfairpoliciesinmulti-
[21] S.A.Frank. Mutualpolicingandrepressionofcompetitionintheevo- objective(deep)reinforcementlearningwithaverageanddiscountedre-
lutionofcooperativegroups.Nature,377(6549):520–522,1995. wards.InInternationalConferenceonMachineLearning,pages8905–
[22] N.A.Grupen,B.Selman,andD.D.Lee.Cooperativemulti-agentfair- 8915.PMLR,2020.
nessandequivariantpolicies. InProceedingsoftheAAAIConference [49] A.TverskyandP.Wakker.Riskattitudesanddecisionweights.Econo-
onArtificialIntelligence,volume36,pages9350–9359,2022. metrica:JournaloftheEconometricSociety,pages1255–1280,1995.
[23] C.F.Hayes,R.Ra˘dulescu,E.Bargiacchi,J.Källström,M.Macfarlane, [50] J.VonNeumannandO.Morgenstern. Theoryofgamesandeconomic
M.Reymond,T.Verstraeten,L.M.Zintgraf,R.Dazeley,F.Heintz,etal. behavior,2ndrev.1947.
Apracticalguidetomulti-objectivereinforcementlearningandplan- [51] A.WitandH.Wilke. Publicgoodprovisionunderenvironmentaland
ning.AutonomousAgentsandMulti-AgentSystems,36(1):26,2022. socialuncertainty. Europeanjournalofsocialpsychology,28(2):249–
[24] J.G.JohnsonandJ.R.Busemeyer. Decisionmakingunderriskand 256,1998.
uncertainty. WileyInterdisciplinaryReviews:CognitiveScience,1(5): [52] R. Yang, X. Sun, and K. Narasimhan. A generalized algorithm for
736–749,2010. multi-objectivereinforcementlearningandpolicyadaptation. InPro-
[25] J.D.Knowles,R.A.Watson,andD.W.Corne. Reducinglocalop- ceedingsofthe33rdInternationalConferenceonNeuralInformation
timainsingle-objectiveproblemsbymulti-objectivization. InInterna- Processing Systems. Curran Associates Inc., Red Hook, NY, USA,
tional conference on evolutionary multi-criterion optimization, pages 2019.
269–283.Springer,2001. [53] Y.Zhang,F.Fu,T.Wu,G.Xie,andL.Wang.Ataleoftwocontribution
[26] P. Kollock. Social dilemmas: The anatomy of cooperation. Annual mechanismsfornonlinearpublicgoods. Scientificreports,3(1):2021,
reviewofsociology,24(1):183–214,1998. 2013.
[27] S.Kopelman,J.M.Weber,andD.M.Messick. FactorsinfluencingLearning in Multi-Objective Public Goods Games with Non-Linear Utilities:
Supplementary Material
A AdditionalResults
(a)Player0 (b)Player1
(c)Player0 (d)Player1
Figure6:NashequilibriaunderSERversustheParetooptimaloutcomesforthegamewithf =0.5andβ ∈{0.5,1.5},representedforPlayer
0andPlayer1onthespaceofjointstrategies.Theresolutionofthestrategyspaceemployedis0.01.
InFigures6and7wepresentaheatmapoftheSERvalues,displayedinthespaceofjointstrategiesforPlayer0andPlayer1,inaMO-EPGG
withN =2players.Valuesoff andβarefixedforeachplot.Inthesamefigure,wealsodisplaytheNashequilibriaandthejointstrategies
thatbelongtotheParetoCoverageSet(PCS),emphasizingwhichNashpointsbelongtothePCS.Inmulti-objectivetheory,thePCSisdefined
asthesetofstrategiesforwhichthecorrespondingvectorialexpectedpayoffsarenotPareto-dominated.AccordingtotheParetodominance
relationship,onevectorispreferredoveranotherwhenitisatleastequalonallobjectivesandstrictlybetteronatleastone.ToobtainthePCS,
wefollowthesameprocedureasproposedbyMannionandRa˘dulescu[31]:westepthroughthestrategyspace,usingaresolutionof0.01,
calculateforeachjoint-strategythecorrespondingexpectedpayoffvector,andthenretainonlytheonesthatarenotParetodominated.
Fromthefigures,wecanobservehowthelandscapeoftheSERvaluechangeswhenincreasingthevalueofβ.Inparticular,wecannotice
that for β = 0.5 (Figures 6a and 6b) the SER for both players is maximized when their opponent acts cooperatively and the player acts
defectively. We also note, as already observed in Figure 2, the presence of the two ϵ-defective Nash points. The Pareto Coverage Set, for
bothplayers,consistsofallthejointstrategieswheretheopponentcooperates.Thislastremarkholdsforallvaluesoff andβ.Thesame
observationsmadeforβ =0.5holdforβ =1.5(Figures6cand6d),wheretheonlydifferenceisthepresenceofasingleNashondefection.(a)Player0 (b)Player1
(c)Player0 (d)Player1
Figure7:NashequilibriaunderSERversustheParetooptimaloutcomesforthegamewithf =0.5andβ ∈{2.5,3.0},representedforPlayer
0andPlayer1onthespaceofjointstrategies.Theresolutionofthestrategyspaceemployedis0.01.
Whenβ >2(Figure7),wecanobservethattheSERlandscapechangesforbothplayersanditsvaluegetsmaximizedinmutualcooperation.
Inthispoint,asweidentifiedinSection4.2,thesocialwelfareisalsomaximized.Moreover,anewNEappearsontheParetofront,onmutual
cooperation,forβ >2.
B Hyperparameters
Table2displaystheparametersemployedtoruntheexperimentsillustratedinFigures4and5.
Table2:Hyperparameters
Hyperparameter Value
numberofepisodes 20000
numberofgameiterations 10
Nagents 20
Mactiveagents 4
intervalfvalues [0.5,6.5]
coinsci 4
numberhiddenlayersMO-DQN 2
hiddensizeMO-DQNπA 8
learningrate 0.001
optimizer RMSProp
ϵ 0.1
decayrateγ 0.99