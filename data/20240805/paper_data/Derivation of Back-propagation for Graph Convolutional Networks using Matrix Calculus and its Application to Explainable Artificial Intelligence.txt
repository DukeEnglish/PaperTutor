Derivation of Back-propagation for Graph
Convolutional Networks using Matrix Calculus and its
Application to Explainable Artificial Intelligence
Yen-CheHsiao
DepartmentofElectricalandComputerEngineering
UniversityofConnecticut
StorrsCT06269,USA
yen-che.hsiao@uconn.edu
RongtingYue
DepartmentofElectricalandComputerEngineering
UniversityofConnecticut
StorrsCT06269,USA
AbhishekDutta
DepartmentofElectricalandComputerEngineering
UniversityofConnecticut
StorrsCT06269,USA
Abstract
This paper provides a comprehensive and detailed derivation of the backpropa-
gationalgorithmforgraphconvolutionalneuralnetworksusingmatrixcalculus.
Thederivationisextendedtoincludearbitraryelement-wiseactivationfunctions
and an arbitrary number of layers. The study addresses two fundamental prob-
lems,namelynodeclassificationandlinkprediction. Tovalidateourmethod,we
compareitwithreverse-modeautomaticdifferentiation. Theexperimentalresults
demonstratethatthemediansumofsquarederrorsoftheupdatedweightmatrices,
whencomparingourmethodtotheapproachusingreverse-modeautomaticdiffer-
entiation,fallswithintherangeof10−18to10−14. Theseoutcomesareobtained
fromconductingexperimentsonafive-layergraphconvolutionalnetwork,applied
toanodeclassificationproblemonZachary’skarateclubsocialnetworkandalink
predictionproblemonadrug-druginteractionnetwork. Finally,weshowhowthe
derivedclosed-formsolutioncanfacilitatethedevelopmentofexplainableAIand
sensitivityanalysis.
1 Introduction
Graphneuralnetwork(GNN)isaneuralnetworkmodelforprocessingdatarepresentedingraph
domains,encompassingcyclic,directed,andundirectedgraphs[1]. Graphconvolutionalnetwork
(GCN)isatypeofGNNmodelthatemployslayer-wisepropagationrule,operatingdirectlyongraphs
[2]. AGCNlayercomprisesmessagepassingovernodes/edges,succeededbyanaggregation/pooling
strategyandafullyconnectedlayer[3].
Nodeclassificationisoneofthemostcommonresearchdirectionsingraphanalysis,wheretheobjec-
tiveofthetaskistopredictaspecificclassforeachunlabelednodeinagraphusinggraphinformation
Preprint.Underreview.
4202
guA
2
]GL.sc[
1v80410.8042:viXra[4]. J.Zhangetal. [5]appliedGCNfordistributionsystemanomalydetection,categorizingnodes
intonormaldata,dataanomalies,oreventanomalies,andimplementedtheirmethodontheIEEE
37-nodedistributionsystems. Linkpredictionseekstoinferunobserved/missinglinksorpredict
futureonesbasedontheconnectionswithincurrentlyobservedpartialnetworks[6]. R.Yueetal. [7]
appliedaGCNthatutilizesprotein-proteininteractions,drug-proteininteractions,anddrug-target
interactionstopredictpotentialdrugmoleculescapableofbindingwithdisease-relatedproteins.
Back-propagation(BP)wasproposedbyDavidRumelhart,GeoffreyHinton,andRonaldWilliams
asalearningprocedurefortrainingneuralnetworksutilizingasetofinput-outputtrainingsamples
[8,9]. BPmakesuseofchainruletocomputethegradientofthenetwork’serrorwithrespectto
everysinglemodelparameter, allowingfortheadjustmentofweightandbiastermsviagradient
descenttoreducetheerror[10,11]. Reversemodeautomaticdifferentiationistheprimarytechnique
intheformoftheback-propagationalgorithmfortrainingneuralnetworksduetoitscomputational
efficiency,particularlyforanobjectivefunctionswithalargenumberofinputsandascalaroutput
[12].
M.A.Nielsen[13]derivedtheBPformulti-layerperceptrons(MLPs)inamatrix-basedformusing
theHadamardproductfornumericalefficiency. N.M.Mishachev[14]utilizedHadamardproduct
andKroneckerproducttoderiveanexplicitmatrixversionoftheBPequationsforMLPsandreduce
thenumberoftheindicesintheequations. M.Naumov[15]demonstratedthatthegradientofthe
weights can be expressed as a rank-1 and rank-t matrix for MLPs and recurrent neural networks
attimestept,respectively. Y.Cheng[16]derivedtheBPalgorithmforMLPsbasedonderivative
amplificationcoefficients.
Explainable Artificial Intelligence (XAI) focuses on developing AI systems that not only make
accuratepredictionsbutalsoprovideclear,interpretableexplanationsfortheiractionsanddecisions,
thusincreasingtheirtrustworthinessforhumanusers[17]. InXAI,sensitivityanalysisiscrucial
becauseitrevealswhichparameters,orgroupsofparameters,havethemostsignificantinfluenceon
thepredictionsmadebymachinelearningmodels[18].
Inthiswork,weusedmatrixcalculustoderiveananalyticandexactclosed-formsolutionofthe
gradientofthelossfunctionwithrespecttoweightmatricesforthetrainingofGCN.Thederivation
considerstheproblemsofbinarynodeclassificationandlinkprediction. Wefirstconsideredthe
problemofnodeclassificationwiththree-layerGCNandweextendedtheGCNmodeltoamulti-
layerGCNwithdlayersandappliedtheprocedureforalinkpredictionproblem. Tovalidateour
approach,weappliedourweightgradientcalculationfornodeclassificationonaZachary’sKarate
Clubgraph[19]andforlinkpredictionusingaself-prepared100-nodedrug-druginteractionnetwork.
Subsequently,wecomparedtheupdatedweightmatrixusingourmethodtothemethodemploying
reversemodeautomaticdifferentiationforgradientcomputation. Ourresultsdemonstrateaignorable
differencebetweenourweightmatrixandtheweightmatrixupdatedusingthegradientfromreverse
modeautomaticdifferentiation,whichverifiesthecorrectnessofourmethod. Asthereisnoexisting
workderivingtheBPofGCNinmatrixform,webelieveitisimportantforthemachinelearning
communitytohavethisclosed-formsolutionavailable. Inaddition,weappliedthesameprocedureto
determinethesensitivityofthelossoroutputwithrespecttotheinputfeaturematrixinGCNfor
XAI.ThedefinitionofnotationsandsomepropertiesusedinthispaperareprovidedinAppendixA.
2 Back-propagationofGraphConvolutionalNetwork
Abasicgraphstructureisdefinedas:
G=(V,E), (1)
where|V|=nisthenumberofnodesinthegraphand|E|=n isthenumberofedges. Denoting
e
v ∈ V asanodeande = (v ,v ) ∈ E asanedgepointingfromv tov ,theadjacencymatrix,
i ij i j i j
A∈{0,1}n×n,isann×nmatrixwherea equals1iftheedgee exists,anda equals0ife
ij ij ij ij
doesnotbelongtoE; inaddition,agraphmaypossessnodeattributesrepresentedbythematrix
H
0
∈Rn×n0,whereh
0v
∈Rn0 isthefeaturevectorofanodevwithn 0features[20].
22.1 Binaryclassificationofnodes
2.1.1 Backpropagationfor3-layerGCNwithReLUandsigmoidactivationfunction
Weconsidera3-layerGCNdefinedas
H =σ (AH W ), (2)
1 ReLU 0 1
H =σ (AH W ), (3)
2 ReLU 1 2
H =σ (AH W ), (4)
3 ReLU 2 3
Yˆ =σ (H ), (5)
sigmoid 3
whereA∈{0,1}n×nisann×nadjacencymatrix,H
0
∈Rn×n0 isthefeaturematrixfornnodes
with n
0
features, Yˆ ∈ Rn×n3 is the output matrix, n
3
= 1 for binary node classification, and
W
1
∈Rn0×n1,W
2
∈Rn1×n2,andW
3
∈Rn2×n3 aretrainableparametermatrices,n 1,n 2,andn
3
arethenumberoffeaturesinH
1
∈ Rn×n1,H
2
∈ Rn×n2,andH
3
∈ Rn×n3,respectively,σ
ReLU
denotesanelement-wiserectifiedlinearunit(ReLU)functionσ (v)=max(v,0)[21],σ
ReLU sigmoid
denotesanelement-wisesigmoidfunctionσ (v)= 1 ..
sigmoid 1+e−v
Thelossfunctionofthe3-layerGCNforbinarynodeclassificationcanbedefinedas
L=L +L , (6)
1 2
where
(cid:88)n (cid:88)n3
L =− (y ln(yˆ )), (7)
1 ij ij
i=1j=1
(cid:88)n (cid:88)n3
L =− ((1−y )ln(1−yˆ )), (8)
2 ij ij
i=1j=1
wherey
ij
denotestheelementintheithrowandjthcolumnofthegroundtruthofnodesY ∈Rn×n3,
yˆ denotestheelementintheithrowandjthcolumnofYˆ.
ij
ToderivethederivativeofthelossfunctionLwithrespecttothethirdweightmatrixW ,wefirst
3
write
∂L ∂L ∂L
= 1 + 2 . (9)
∂W ∂W ∂W
3 3 3
Then,thederivativeofL in(7)withrespecttoW canbewrittenas
1 3
∂L
1
=−(cid:88)n (cid:88)n3
y ·
∂ln(yˆ ij)
∂W ij ∂W
3 3
i=1j=1
=−(cid:88)n (cid:88)n3
y ·
∂ln(σ sigmoid(h 3ij))
ij ∂W
3
i=1j=1
(10)
=−(cid:88)n (cid:88)n3
y ·
1
·yˆ ·(1−yˆ )·
∂h
3ij
ij yˆ ij ij ∂W
ij 3
i=1j=1
=−(cid:88)n (cid:88)n3
y ·(1−yˆ )·
∂h
3ij.
ij ij ∂W
3
i=1j=1
Similarly,thederivativeofL in(8)withrespecttoW canbewrittenas
2 3
∂L
2
=−(cid:88)n (cid:88)n3
(1−y )·
∂ln(1−yˆ ij)
∂W ij ∂W
3 3
i=1j=1
(11)
=(cid:88)n (cid:88)n3
(1−y )·yˆ ·
∂h
3ij.
ij ij ∂W
3
i=1j=1
Thelastterm, ∂h3ij,in(10)and(11)canbewrittenas ∂h3ij = ∂σReLU(Ai∗H2W3∗j) using(4),where
∂W3 ∂W3 ∂W3
A istheithrowofAandW isthejthcolumnofW . Theexpression,
∂σReLU(Ai∗H2W3∗j)
,is
i∗ 3∗j 3 ∂W3
solvedusingthefollowingdefinitionsandtheorem.
3Definition2.1. Anelement-wiseactivationfunctionΣ:Rp×q →Rp×q isdefinedasamultivariate
matrix-valuedfunction[22]:
 
σ(x ) ··· σ(x )
11 1q
Σ:X(cid:55)→Σ(X):= . . . . , (12)
 . . 
σ(x ) ··· σ(x )
p1 pq
whereX∈Rp×q andσ :R→R,x (cid:55)→σ(x )foralli=1,2,...,pandj =1,2,...,q.
ij ij
Definition2.2. Thederivativeofanelement-wiseactivationfunctionΣ(X)(p×q)withrespecttoa
matrixX∈Rp×q iswrittenas:
 σ′(x ) ··· σ′(x )
11 1q
Σ′(X)= ∂Σ(X) = . . . . , (13)
∂X  . . 
σ′(x ) ··· σ′(x )
p1 pq
whereσ′(v)=lim σ(v+u)−σ(v) isdefinedasthederivativeofareal-valuedfunctionσ.
u→0 u
Theorem2.3. LetF:Rp×q →Rm×n beam×nmultivariatematrix-valuedfunctionofap×q
matrixW∈Rp×q,thederivativeofΣ(F(W))withrespecttoWcanbewrittenas:
∂Σ(F(W)) ∂F(W)
=(J ⊗Σ′(F(W)))⊙ , (14)
∂W p×q ∂W
where⊗istheKroneckerproductin(46),⊙istheHadamardproductin(47),andJ ∈Rp×q is
p×q
anall1matrix.
Proof. TheproofcanbefoundinAppendixC.
Fromtheorem2.3,wecanwritethelasttermin(10)and(11)as
∂h ∂σ (A H W )
3ij = ReLU i∗ 2 3∗j
∂W ∂W
3 3 (15)
∂A H W
=(J ⊗σ′ (A H W ))⊙ i∗ 2 3∗j.
n2×n3 ReLU i∗ 2 3∗j ∂W
3
Using(52),thelasttermin(15)canbewrittenas
∂A H W ∂A H ∂W
i∗ 2 3∗j = i∗ 2(I ⊗W )+(I ⊗(A H )) 3∗j
∂W ∂W n3 3∗j n2 i∗ 2 ∂W
3 3 3 (16)
∂W
=(I ⊗(A H ))
3∗j.
n2 i∗ 2 ∂W
3
Using(44),(52),and(53),thelasttermin(16)canbewrittenas
∂W e
3 j
∂W
3∗j
=
(n3)
∂W ∂W
3 3
∂ e
j
= ∂W 3(I ⊗ e )+(I ⊗W ) (n3) (17)
∂W n3 j n2 3 ∂W
3 (n3) 3
=U¯ (I ⊗ e ),
n2×n3 n3 j
(n3)
where e isan -dimensionalcolumnvectorwhichhas“1”inthejthrowandzeroelsewhereand
j 3
(n3)
U¯ n2×n3 ∈Rn2 2×n2 3 isapermutationrelatedmatrixdefinedin(49).
4Thus,thederivativeofthelossfunctionLwithrespecttothethirdweightmatrixW in(9)canbe
3
writtenas
∂L
=−(cid:88)n (cid:88)n3
((y −yˆ )·
∂h
3ij) (18)
∂W ij ij ∂W
3 3
i=1j=1
(cid:88)n (cid:88)n3
=− ((y −yˆ )·((J ⊗σ′ (A H W ))
ij ij n2×n3 ReLU i∗ 2 3∗j
i=1j=1
⊙((I ⊗(A H ))U¯ (I ⊗ e )))). (19)
n2 i∗ 2 n2×n3 n3 j
(n3)
Using(18),thederivativeofthelossfunctionLin(6)withrespecttothesecondweightmatrixW
2
canbewrittenas
∂L
=−(cid:88)n (cid:88)n3
((y −yˆ )·
∂h
3ij) (20)
∂W ij ij ∂W
2 2
i=1j=1
=−(cid:88)n (cid:88)n3
((y −yˆ )·((J ⊗σ′ (A H W ))⊙
∂A i∗H 2W
3∗j)), (21)
ij ij n1×n2 ReLU i∗ 2 3∗j ∂W
2
i=1j=1
where(21)followsfromtheorem2.3.
Using(52),thelasttermof(21)canbewrittenas
∂A H W ∂H
i∗ 2 3∗j =(I ⊗A ) 2 (I ⊗W ). (22)
∂W n1 i∗ ∂W n2 3∗j
2 2
ThederivativeofH withrespecttoW in(22)canbewrittenas
2 2
∂H ∂σ (AH W )
2 = ReLU 1 2 (23)
∂W ∂W
2 2
∂AH W
=(J ⊗σ′ (AH W ))⊙ 1 2) (24)
n1×n2 ReLU 1 2 ∂W
2
∂W
=(J ⊗σ′ (AH W ))⊙((I ⊗(AH )) 2) (25)
n1×n2 ReLU 1 2 n1 1 ∂W
2
=(J ⊗σ′ (AH W ))⊙((I ⊗(AH ))U¯ ), (26)
n1×n2 ReLU 1 2 n1 1 n1×n2
where(23)followsfrom(4),(24)followsfrom(15),(25)followsfrom(52),and(26)followsfrom
(53).
Thederivativeofthelossin(6)withrespecttothefirstweightmatrixW is
1
∂L
(cid:88)n (cid:88)n3
=− ((y −yˆ )·((J ⊗σ′ (A H W ))
∂W ij ij n0×n1 ReLU i∗ 2 3∗j
1
i=1j=1
∂H
⊙((I ⊗A ) 2 (I ⊗W )))), (27)
n0 i∗ ∂W n1 3∗j
1
where(27)followsfrom(21)and
∂H ∂σ (AH W )
2 = ReLU 1 2 (28)
∂W ∂W
1 1
∂H
=(J ⊗σ′ (AH W ))⊙((I ⊗A) 1 (I ⊗W )), (29)
n0×n1 ReLU 1 2 n0 ∂W n1 2
1
where(28)followsfrom(4),(29)followsfromtheorem(2.3)and(52),and
∂H
1 =(J ⊗σ′ (AH W ))⊙((I ⊗(AH ))U¯ ), (30)
∂W n0×n1 ReLU 0 1 n0 0 n0×n1
1
where(30)followsfrom(26).
52.1.2 Back-propagationformulti-layerGCNwithReLUandsigmoidactivationfunction
Weconsiderad-layerGCNdefinedas
Yˆ =σ (H ), (31)
sigmoid d
where
H =σ (AH W ), (32)
d ReLU d−1 d
d∈Z+,A∈{0,1}n×nisann×nadjacencymatrix,H
d−1
∈Rn×nd−1 isthefeaturematrixforn
nodeswithn
d−1
features,Yˆ ∈Rn×nd istheoutputmatrix,n
d
=1forbinarynodeclassification,
andW
1
∈Rn0×n1,W
2
∈Rn1×n2,···,andW
d
∈Rnd−1×nd aretrainableparametermatrices.
Byobserving(19),(21),(22),(26),(27),(29),and(30),ifthelossfunctionofthed-layerGCNfor
nodeclassificationisdefinedas
(cid:88)n (cid:88)nd
L=− (y ln(yˆ )+(1−y )ln(1−yˆ )), (33)
ij ij ij ij
i=1j=1
thederivativeofthelossin(33)withrespecttothesthweightmatrixW is
s
∂L
(cid:88)n (cid:88)nd
=− ((y −yˆ )·((J ⊗σ′ (A H W ))
∂W ij ij (s−1)×s ReLU i∗ d−1 d∗j
s
i=1j=1
∂A H W
⊙
i∗ d−1 d∗j)),
(34)
∂W
s
wheres∈Z+,s≤d,
 (I ⊗(A H ))·U¯ (I ⊗ e ),
∂A H W
 ns−1 i∗ d−1 ns−1×ns ns j ifs=d
i∗ d−1 d∗j
=
(nd)
(35)
∂W s  (I
ns−1
⊗A i∗)· ∂ ∂H Wd−1(I
ns
⊗W d∗j), ifs<d
s
and
∂H
d−1
∂W
s
 (J ⊗σ′ (AH W ))⊙((I ⊗(AH ))U¯ ),
 ifsns =−1 d×n −s
1
ReLU d−2 d−1 ns−1 d−2 ns−1×ns
= ∂H
(J ⊗σ′ (AH W ))⊙((I ⊗A) d−2(I ⊗W )),
 ifsns <−1 d×n −s
1.
ReLU d−2 d−1 ns−1 ∂W
s
ns d−1
(36)
2.1.3 Back-propagationformulti-layerGCNwitharbitraryactivationfunctions
Weconsiderad-layerGCNdefinedas
Yˆ =Σ (H ), (37)
d+1 d
where
H =Σ (AH W ), (38)
d d d−1 d
d∈Z+,A∈{0,1}n×nisann×nadjacencymatrix,H
d−1
∈Rn×nd−1 isthefeaturematrixforn
nodeswithn
d−1
features,Yˆ ∈Rn×nd istheoutputmatrix,n
d
=1forbinarynodeclassification,
W
1
∈Rn0×n1,W
2
∈Rn1×n2,···,andW
d
∈Rnd−1×nd aretrainableparametermatrices,andΣ 1,
Σ ,···,andΣ areanyelement-wiseactivationfunction.
2 d+1
Ifthelossfunctionofthed-layerGCNfornodeclassificationisdefinedas
(cid:88)n (cid:88)nd
L=− (y ln(yˆ )+(1−y )ln(1−yˆ )), (39)
ij ij ij ij
i=1j=1
6thederivativeofthelossin(39)withrespecttothesthweightmatrixW is
s
∂L
=−(cid:88)n (cid:88)nd
(y
∂ln(yˆ ij)
+(1−yˆ
)∂ln(1−yˆ ij)
)
∂W ij ∂W ij ∂W
s s s
i=1j=1
=−(cid:88)n (cid:88)nd ((y
ij −
1−y
ij)Σ′ (h )·
∂Σ d(A i∗H d−1W d∗j)
) (40)
yˆ 1−yˆ d+1 dij ∂W
ij ij s
i=1j=1
=−(cid:88)n (cid:88)nd
(
y
ij
−yˆ
ij Σ′ (h )·((J ⊗Σ′(A H W ))
yˆ (1−yˆ ) d+1 dij (s−1)×s d i∗ d−1 d∗j
ij ij
i=1j=1
∂A H W
⊙
i∗ d−1 d∗j)),
(41)
∂W
s
wheres ∈ Z+,s ≤ d,(40)followsfromchainruleanddefinition2.2,(41)followsfromtheorem
(2.3),
∂Ai∗Hd−1Wd∗j
isthesameasin(35),and
∂Ws
∂H
d−1
∂W
s
 (J ⊗Σ′ (AH W ))⊙((I ⊗(AH ))U¯ )
 ifsns =−1 d×n −s
1
d−1 d−2 d−1 ns−1 d−2 ns−1×ns
(42)
= ∂H
(J ⊗Σ′ (AH W ))⊙((I ⊗A) d−2(I ⊗W ))
 ifsns <−1 d×n −s
1.
d−1 d−2 d−1 ns−1 ∂W
s
ns d−1
Followingasimilarprocedure,wecanderivethesensitivityofthelosswithrespecttothefeature
matrixH ,asshowninAppendixE.1.
0
2.2 Linkprediction
Forthederivationofback-propagationandsensitivityinamulti-layerGCNwitharbitraryactivation
functionsforthelinkpredictionproblem,pleaserefertoAppendicesDandE.2,respectively.
3 Experiments
Inthissection,wedemonstratethecorrectnessofourmethodthroughseveralexperimentsconsidering
anodeclassificationproblemonZachary’sKarateClub[19]inFigure3(a)andalinkprediction
problemonadrug-druginteraction(DDI)networkinFigure3(b). Weadoptthestochasticgradient
descent(SGD)formodeltraining. Wecompareourmethodtothegradientcomputationusingreverse
modeautomaticdifferentiationinPytorch[23]bycomputingthesumofsquarederror(SSE)defined
as
n (cid:88)s−1(cid:88)ns
SSE = (w(AD)−w(KP))2, (43)
sij sij
i=1 j=1
wherew(AD) istheithrowandthejthcolumnofthesthweightmatrixW updatedusingSGD
sij s
withreversemodeautomaticdifferentiationandw(KP)istheithrowandthejthcolumnofthesth
sij
weightmatrixW updatedusingSGDwithourmatrix-basedmethod.
s
Inaddition,wedeterminethesensitivityofthelosswithrespecttothefeaturematrixH fornode
0
classificationandlinkpredictiontodemonstratetheapplicationofourmethodtoXAI.
Thepythoncodesareavailableat: https://github.com/AnnonymousForPapers/GCN-proof/
tree/main.
3.1 Nodeclassification
WetestedourmethodfornodeclassificationonZachary’sKarateClub[19](seeAppendixF.1for
moredetails).
73.1.1 1-layerGCNwithidentityfunctionandsigmoidactivationfunction
The 1-layerGCNis definedby(37)and thelossfunctionis definedby(39), where d = 1isthe
numberoflayers,H = I ∈ Rn×n isthefeaturematrixforn = 34nodeswithn = 34features,
0 n
the matrix Yˆ ∈ Rn×nd represents the ground truth of nodes, indicating class assignments per
node,Yˆ ∈ Rn×nd denotesthefinallayerpredictionsoftheGCNmodel,n
d
= 1forbinarynode
classification, W
1
∈ Rn0×nd is a trainable parameter matrix, n
0
= 34, Σ
1
is an element-wise
identityfunction,andΣ isanelement-wisesigmoidfunction.
2
TheGCNistrainedusingSGDwithalearningrateof0.1andwith100iterationsforbothmethods.
InFigure4,theevolutionofthekarateclubnetworkshowsthatthetwographupdatedbythetwo
methodshavethesameloss,accuracy,andclassificationresults. InFigure1(a),itshowstheSSE
oftheweightmatrixW betweenthereversemodeautomaticdifferentiationandourmatrix-based
1
methodislowerthan10−13. Thisindicatesthatouranalyticexpressionalignswiththeexactmethod,
andthesmalldifferencemightbeduetotheprecisionofdifferentdatatypes.
Next,weshowthelosssensitivitywithrespecttotheinputfeaturematrix,computedusingtheresult
inAppendixE.1. InFigure2(a),itisshownthatastrainingprogresses,thesensitivitydecreases.
ThisresultisexpectedsincetheinputfeaturematrixoftheGCNfortheKarategraphisanidentity
matrix. Thepredictionoftheclassofanodedependsonlyonitsedgesratherthantheinputfeatures.
Therefore,changesintheinputmatrixshouldnotsignificantlyaffectthepredictionfromthetrained
GCN,asitshouldlearntomakepredictionsbasedsolelyontheexistenceofedgesbetweennodes.
AdditionalexperimentalresultsonZachary’sKarateClub[19]areprovidedinAppendixG.0.1.
3.2 Linkprediction
Wetestedourmethodforlinkpredictionona10-nodeDDInetwork(seeAppendixF.2formore
details).
3.2.1 2-layerGCN
The 2-layerGCNis definedby(60)and thelossfunctionis definedby(62), where d = 2isthe
number of layers, the matrix Yˆ ∈ Rn×n represents ground-truth adjacency matrix, Yˆ ∈ Rn×n
denotesthefinallinkpredictionsoftheGCNmodel,W
1
∈Rn0×n1 andW
2
∈Rn1×n2 aretrainable
parametermatrices,n =20,n =10,n =5,Σ isanelement-wiseReLUfunction[21],Σ isan
0 1 2 1 2
element-wiseidentityfunction,andΣ isanelement-wisesigmoidfunction.
3
TheGCNistrainedusingSGDwithalearningrateof0.01andwith150iterationsforbothmethods.
Ineachiteration,thirteennegativeedges,whichisthesamenumberasthenumberofpositiveedges,
areuniformlysampledfromasetofalltheunconnectededgesfortheGCNtrainingusingthereverse
modeautomaticdifferentiation. Thesamplednegativeedgesineachiterationaresavedinalistand
usedinthetrainingoftheGCNusingourmatrix-basedmethod. InFigure7,theevolutionoftheDDI
networkshowsthatthetwographupdatedbythetwomethodshavethesamelossandclassification
results. InFigure1(b), itshowstheSSEofthetwoweightmatrices, W andW , betweenthe
1 2
reversemodeautomaticdifferentiationandourmatrix-basedmethodislowerthan10−14.
Next,weshowtheoutputsensitivitywithrespecttotheinputfeaturematrix,computedusingthe
resultinAppendixE.2. InFigure2(b),aheatmapofthesensitivityofthepredictionforthelink
betweennode2andnode7isshown. Thesensitivitiesofthefeaturesinnode2andnode7arehigher
thanthoseofthefeaturesinothernodes. Thefeaturesofnodes(node1andnode8)thatdonothave
aconnectiontoeithernode2ornode7,orthefeaturesofnodes(node0)thatneedtotraverseatleast
threeedgestoreachnode2ornode7,havealmostzerosensitivitytothelink. Thisdemonstrates
thepropertyofGCNs,wheretheneighborhoodinformationoftwonodesisaggregatedbytaking
theweightedsumofthefeaturesofneighboringnodes[24]. SincetheGCNhasonlytwolayers,
theinformationofnodesthatarethreeedgesawayfromthesetwonodesdoesnotcontributetothe
predictionoftheedge.
AdditionalexperimentalresultsontheDDInetworkareprovidedinAppendixG.0.2. Heatmapsof
thesensitivityofthepredictionforallthelinksareinFigures8,9,and10intheAppendix.
8Figure1: (a)Theevolutionofthesumofsquarederrorbetweenthetrainableweightmatrixobtained
fromourmethodandthematrixobtainedusingreversemodeautomaticdifferentiationinsection
3.1.1. (b) The evolution of the sum of squared error between the two trainable weight matrices
obtainedfromourmethodandthematricesobtainedusingreversemodeautomaticdifferentiationin
section3.2.1.
Figure2: (a)Theevolutionoftheabsolutesumofthesensitivityofthelosswithrespecttotheinput
featurematrixH inSection3.1.1. (b)Theheatmapofthesensitivityofthepredictionforthelink
0
betweennode2andnode7withrespecttotheinputfeaturematrixH inSection3.2.1.
0
4 Conclusion
Inthiswork, weprovidedetailedderivationsoftheanalyticalexpressionsinmatrixformforthe
derivativesofalossfunctionwithrespecttoeachweightmatrixforagraphconvolutionalnetwork
consideringbinarynodeclassificationandlinkprediction. UtilizingKroneckerproduct,Hadamard
product,andmatrixcalculus,wecomputedthegradientsofthelossfunctioninathree-layergraph
convolutionalnetworkandextendedthesolutiontoaccommodategraphconvolutionalnetworkswith
arbitrarylayersandelement-wiseactivationfunctions. Theweightmatricesobtainedthroughour
approachwerecomparedwiththoseobtainedusingreversemodeautomaticdifferentiationinbinary
nodeclassificationexperimentsconductedona34nodesZachary’sKarateClubnetwork[19]and
inlinkpredictionexperimentsusinga10-nodedrug-druginteractionnetwork. Theexperimental
resultsindicatethatthediscrepancybetweentheweightmatriceshasamediansumofsquarederror
rangingfrom10−18to10−14,affirmingtheaccuracyofourmethodology. Inaddition,weconduct
sensitivityanalysisforbinarynodeclassificationandlinkpredictiontodemonstratetheapplication
ofourderivationmethodinXAI.
We note that our derivation already accommodates RNNs when A is an identity matrix, and it
accommodates CNNs since CNNs operate on 2-dimensional matrices, which are a special case
ofgraphs. Additionally,ourmethodincursasignificantlyhighercomputationalcostcomparedto
reverse mode AD. As future work, we aim to find a way to improve the computational speed of
back-propagationthroughourderivedanalyticalsolution.
9References
[1] FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabrieleMonfardini.
Thegraphneuralnetworkmodel. IEEEtransactionsonneuralnetworks,20(1):61–80,2008.
[2] ThomasNKipfandMaxWelling. Semi-supervisedclassificationwithgraphconvolutional
networks. arXivpreprintarXiv:1609.02907,2016.
[3] AbhishekDutta. Deepgraphgenerationofsmallmoleculesguidedbydrug-likeness. InIEEE
ConferenceonComputationalIntelligenceinBioinformaticsandComputationalBiology,pages
1–2.IEEE,2022.
[4] ShunxinXiao,ShipingWang,YuanfeiDai,andWenzhongGuo. Graphneuralnetworksinnode
classification: surveyandevaluation. MachineVisionandApplications,33:1–19,2022.
[5] JinxianZhang, JunboZhao, FeiDing, JingYang, andJunhuiZhao. Agraphconvolutional
network for active distribution system anomaly detection considering measurement spatial-
temporalcorrelations. In2023NorthAmericanPowerSymposium(NAPS),pages1–6.IEEE,
2023.
[6] Xing Wang and Alexander Vinel. Benchmarking graph neural networks on link prediction.
arXivpreprintarXiv:2102.12557,2021.
[7] RongtingYueandAbhishekDutta. Repurposingdrugsforcovid-19bygraphconvolutional
network. PREPRINT,April122023. Version1,availableatResearchSquarehttps://doi.
org/10.21203/rs.3.rs-2408594/v1.
[8] DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams. Learningrepresentationsby
back-propagatingerrors. nature,323(6088):533–536,1986.
[9] SergiosTheodoridis. Chapter18-neuralnetworksanddeeplearning. InSergiosTheodoridis,
editor,MachineLearning(SecondEdition),pages901–1038.AcademicPress,secondedition,
2020.
[10] Aurélien Géron. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. "
O’ReillyMedia,Inc.",2022.
[11] Werbos. Backpropagation: Pastandfuture. InIEEE1988InternationalConferenceonNeural
Networks,pages343–353.IEEE,1988.
[12] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark
Siskind. Automaticdifferentiationinmachinelearning:asurvey. JournalofMarchineLearning
Research,18:1–43,2018.
[13] MichaelANielsen. Neuralnetworksanddeeplearning,volume25. DeterminationpressSan
Francisco,CA,USA,2015.
[14] NMMishachev. Backpropagationinmatrixnotation. arXivpreprintarXiv:1707.02746,2017.
[15] MaximNaumov. Feedforwardandrecurrentneuralnetworksbackwardpropagationandhessian
inmatrixform. arXivpreprintarXiv:1709.06080,2017.
[16] YipingCheng. Derivationofthebackpropagationalgorithmbasedonderivativeamplification
coefficients. arXivpreprintarXiv:2102.04320,2021.
[17] VikasHassija,VinayChamola,AtmeshMahapatra,AbhinandanSingal,DivyanshGoel,Kaizhu
Huang,SimoneScardapane,IndroSpinelli,MuftiMahmud,andAmirHussain. Interpreting
black-box models: a review on explainable artificial intelligence. Cognitive Computation,
16(1):45–74,2024.
[18] Bas Van Stein, Elena Raponi, Zahra Sadeghi, Niek Bouman, Roeland CHJ Van Ham, and
ThomasBäck. Acomparisonofglobalsensitivityanalysismethodsforexplainableaiwithan
applicationingenomicprediction. IEEEAccess,10:103364–103381,2022.
[19] WayneWZachary. Aninformationflowmodelforconflictandfissioninsmallgroups. Journal
ofanthropologicalresearch,33(4):452–473,1977.
[20] ZonghanWu,ShiruiPan,FengwenChen,GuodongLong,ChengqiZhang,andSYuPhilip. A
comprehensivesurveyongraphneuralnetworks. IEEEtransactionsonneuralnetworksand
learningsystems,32(1):4–24,2020.
10[21] VinodNairandGeoffreyEHinton.Rectifiedlinearunitsimproverestrictedboltzmannmachines.
InProceedingsofthe27thinternationalconferenceonmachinelearning(ICML-10),pages
807–814,2010.
[22] DirkOstwaldandFranziskaUsée. Aninductionproofofthebackpropagationalgorithmin
matrixnotation. arXivpreprintarXiv:2107.09384,2021.
[23] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,
TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal. Pytorch: Animperative
style, high-performance deep learning library. Advances in neural information processing
systems,32,2019.
[24] David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon Denman, Clinton Fookes, and
Lars Petersson. A survey on graph-based deep learning for computational histopathology.
ComputerizedMedicalImagingandGraphics,95:102027,2022.
[25] JohnBrewer. Kroneckerproductsandmatrixcalculusinsystemtheory. IEEETransactionson
circuitsandsystems,25(9):772–781,1978.
[26] Hai-huaZhu,Zi-gangChen,andTaoLeng. Randompermutation-basedmixed-doublescram-
blingtechniqueforencryptingmqirimage. JournalofAppliedPhysics,135(1),2024.
[27] Jan R Magnus. On the concept of matrix derivative. Journal of Multivariate Analysis,
101(9):2200–2206,2010.
[28] SeongjunYun,SeoyoonKim,JunhyunLee,JaewooKang,andHyunwooJKim. Neo-gnns:
Neighborhoodoverlap-awaregraphneuralnetworksforlinkprediction. AdvancesinNeural
InformationProcessingSystems,34:13683–13694,2021.
[29] CristopherMoore,XiaoranYan,YaojiaZhu,Jean-BaptisteRouquier,andTerranLane. Active
learningfornodeclassificationinassortativeanddisassortativenetworks. InProceedingsofthe
17thACMSIGKDDinternationalconferenceonKnowledgediscoveryanddatamining,pages
841–849,2011.
[30] ADavidRodrigues. Drug-druginteractions. CRCPress,2019.
[31] DavidSWishart,YannickDFeunang,AnCGuo,ElvisJLo,AnaMarcu,JasonRGrant,Tanvir
Sajed,DanielJohnson,CarinLi,ZinatSayeeda,etal. Drugbank5.0: amajorupdatetothe
drugbankdatabasefor2018. Nucleicacidsresearch,46(D1):D1074–D1082,2018.
[32] DanielBlanco-Melo,BenjaminENilsson-Payant,Wen-ChunLiu,SkylerUhl,DaisyHoagland,
RasmusMøller,TristanXJordan,KoheiOishi,MarylinePanis,DavidSachs,etal. Imbalanced
hostresponsetosars-cov-2drivesdevelopmentofcovid-19. Cell,181(5):1036–1045,2020.
[33] MinoruKanehisa,YokoSato,andMasayukiKawashima. Keggmappingtoolsforuncovering
hiddenfeaturesinbiologicaldata. ProteinScience,2021.
[34] David Weininger. Smiles, a chemical language and information system. 1. introduction to
methodology and encoding rules. Journal of chemical information and computer sciences,
28(1):31–36,1988.
[35] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415,2016.
[36] Djork-ArnéClevert,ThomasUnterthiner,andSeppHochreiter. Fastandaccuratedeepnetwork
learningbyexponentiallinearunits(elus). arXivpreprintarXiv:1511.07289,2015.
[37] AndrewLMaas,AwniYHannun,AndrewYNg,etal. Rectifiernonlinearitiesimproveneural
networkacousticmodels. InProc.icml,volume30,page3.Atlanta,GA,2013.
[38] J.D.Hunter. Matplotlib: A2dgraphicsenvironment. ComputinginScience&Engineering,
9(3):90–95,2007.
A BasicnotationandpropertiesofKroneckerproductandmatrixcalculus
A.1 Basicnotation
Acolumnvectorisdenotedbylowercaseboldface(e.g.,v,withitsithelementbeingv ). Matrices
i
is denoted by upper case boldface (e.g., A). The ith row for a matrix such as A is denoted A
i∗
andtheithcolumnisdenotedA . The(i,j)elementofAisdenoteda . An(m×n)all-ones
∗i ij
11matrixwhereallofitselementsareequalto1isdenotedasJ . An(m×n)zeromatrixwhere
m×n
allofitselementsareequalto0isdenotedasO . The(n×n)identitymatrixisdenotedI .
m×n n
Thek-dimensionalcolumnvectorwhichhas“1”inthejthrowandzeroelsewhereiscalledtheunit
vectorandhasdenoted e . TheithcolumnofamatrixAcanbewrittenas:
j
(k)
A =A·e . (44)
∗i i
(q)
Theelementarymatrix
E(p×q) ≜ e eT (45)
ij i j
(p)(q)
hasdimensions(p×q),has“1”inthe(i,j)thelement,andhaszeroelsewhere.
The Kronecker product of a matrix A with dimensions (p×q) and a matrix B with dimensions
(m×n)isrepresentedasA⊗B. Theresultingmatrixisofsizepm×qnandisdefinedby
a B a B ··· a B
11 12 1q
.
a B . . 
A⊗B≜ 21 . (46)
 . 
 . . 
a B ··· a B
p1 pq
TheHadamardproductofA(p×q)andC(p×q)isdenotedA⊙Candisa(p×q)matrixdefined
by
a c a c ··· a c 
11 11 12 12 1q 1q
.
a c . . 
A⊙C≜ 21 21 . (47)
 . 
 . . 
a c ··· a c
p1 p1 pq pq
Thepermutationmatrixisasquare(pq×pq)matrixdefinedby[25]
p q
U ≜(cid:88)(cid:88) E(p×q)⊗E(q×p). (48)
p×q ij ji
i j
EachrowandcolumnofthesquarematrixU hasonlyoneelementwithavalueof1,andall
p×q
remainingelementsarezero[26].
Arelatedmatrixisarectangular(p2×q2)matrixdefinedby[25]
p q
U¯ ≜(cid:88)(cid:88) E(p×q)⊗E(p×q). (49)
p×q ij ij
i j
ThematrixderivativeofamatrixfunctionF = [f (W)] : Rp×q → Rm×n withrespecttoa
ij m×n
matrixX(p×q)isoforder(pm×qn)andisdefinedas[27]
∂F(X)
···
∂F(X)
∂F(X) ≜

∂x .
.
.11 ∂x .
.
.1q 
, (50)
∂X  
∂F(X) ··· ∂F(X)
∂xp1 ∂xpq
where
∂f11(X)
···
∂f1n(X)
∂F(X) ≜ ∂ . .x ∂ . .x  (51)
∂x  . . 
∂fm1(X) ··· ∂fmn(X)
∂x ∂x
forx∈R.
12A.2 PropertiesofKroneckerproductandmatrixcalculus
The matrix operations related to Kronecker product are listed below and are adopted from [25].
Thematricesinthissubsectionhavethefollowingdimension: A:Rs×t →Rp×q,B∈Rs×t,and
C:Rs×t →Rq×r.
∂A(B)C(B) ∂A(B) ∂C(B)
= (I ⊗C(B))+(I ⊗A(B)) . (52)
∂B ∂B t s ∂B
∂A
=U¯ . (53)
∂A p×q
∂AT
=U . (54)
∂A p×q
Theequations(52),(53),and(54)areemployedinderivingthederivativeofthelossfunctionwith
respecttotheweightmatrixofGCN.
B Back-propagationofGraphConvolutionalNetwork
Abasicgraphstructureisdefinedas:
G=(V,E), (55)
where|V|=nisthenumberofnodesinthegraphand|E|=n isthenumberofedges. Denoting
e
v ∈ V asanodeande = (v ,v ) ∈ E asanedgepointingfromv tov ,theadjacencymatrix,
i ij i j i j
A∈{0,1}n×n,isann×nmatrixwherea equals1iftheedgee exists,anda equals0ife
ij ij ij ij
doesnotbelongtoE; inaddition,agraphmaypossessnodeattributesrepresentedbythematrix
H
0
∈Rn×n0,whereh
0v
∈Rn0 isthefeaturevectorofanodevwithn 0features[20].
C Proofoftheorem2.3
LetF(W)=[f (W)] :Rp×q →Rm×nbeam×nmultivariatematrix-valuedfunctionofa
ij m×n
p×qmatrixW ∈Rp×q,fromdefinition2.1,Σ(F(W))mapsam×nmatrixF(W)toam×n
matrixΣ(F(W)). From(50),thederivativeofΣ(F(W))(m×n)withrespecttoW(p×q)can
bewrittenas:
∂Σ(F(W))
···
∂Σ(F(W))
∂Σ(F(W))  ∂w .11 ∂w .1q 
= .
.
.
.
. (56)
∂W  
∂Σ(F(W)) ··· ∂Σ(F(W))
∂wp1 ∂wpq
Using(51),chainrule,anddefinition2.2,thefirstelementin(56)isgivenby
∂Σ(F(W))
∂w
11
 
∂σ(f11(W)) ··· ∂σ(f1n(W))
∂w11 ∂w11
 . . 
= . . . . 
 
∂σ(fm1(W)) ··· ∂σ(fmn(W))
∂w11 ∂w11 (57)
 
σ′(f11(W))∂f ∂1 w1( 1W
1
) ··· σ′(f1n(W))∂f ∂1 wn 1(W
1
)
= . . . . 
 . . 
σ′(fm1(W))∂f ∂m w1 1( 1W) ··· σ′(fmn(W))∂fm ∂wn 1( 1W)
∂F(W)
=Σ′(F(W))⊙ ,
∂w
11
whereΣ′(F(W))isdefinedas
 σ′(f (W)) ··· σ′(f (W))
11 1n
Σ′(F(W))= . . . . . (58)
 . . 
σ′(f (W)) ··· σ′(f (W))
m1 mn
13Applyingthecalculationof(57)toalltheotherelementsin(56),wecanget
∂Σ(F(W))
∂W
 Σ′(F(W))⊙ ∂F(W) ··· Σ′(F(W))⊙ ∂F(W)
∂w11 ∂w1q
 . . 
= . . . . 
 
Σ′(F(W))⊙ ∂F(W) ··· Σ′(F(W))⊙ ∂F(W)
∂wp1 ∂wpq (59)
 
(cid:34)Σ′(F(W))··· Σ′(F(W))(cid:35) ∂ ∂F w(W 11) ··· ∂ ∂F w(W 1q)
= . . . . ⊙ . . . . 
. .  . . 
Σ′(F(W))··· Σ′(F(W)) ∂F(W) ··· ∂F(W)
∂wp1 ∂wpq
∂F(W)
=(J ⊗Σ′(F(W)))⊙ .
p×q ∂W
D Back-propagationformulti-layerGCNwitharbitraryactivationfunctions
Weconsiderad-layerGCNdefinedas
Yˆ =Σ (H HT), (60)
d+1 d d
where
H =Σ (AˆH W ), (61)
d d d−1 d
d∈Z+,Aˆ =D˜−1 2(A+I n)D˜−1 2 ∈Rn×nrepresentsthen×nnormalizedadjacencymatrix[28],
D˜ ∈Rn×n isadegreematrixdefinedbyd˜ =1+(cid:80) a ,A∈{0,1}n×n isann×nadjacency
ii j ij
matrixwhosediagonalelementsareallequaltozero),H
d
∈Rn×nd isthefeaturematrixfornnodes
withn
d
features, Yˆ ∈ Rn×nd istheoutputmatrix, andW
1
∈ Rn0×n1, W
2
∈ Rn1×n2, ···, and
W
d
∈Rnd−1×nd aretrainableparametermatrices,andΣ 1,Σ 2,···,andΣ d+1areanyelement-wise
activationfunction.
ThelossfunctionoftheGCNforlinkpredictioncanbedefinedas
(cid:88) (cid:88)
L=− ln(yˆ )− ln(1−yˆ ), (62)
ij ij
(i,j)∈E (i,j)∈S
wherey
ij
denotestheelementintheithrowandjthcolumnofthetrainingmatrixY ∈Rn×nd,yˆ
ij
denotestheelementintheithrowandjthcolumnofYˆ,andS isasetofedgesthatcontainsn
e
negativeedges(v¯,v¯ )∈S randomlysampledfromEc.
i j
Thederivativeofthelossin(62)withrespecttothesthweightmatrixW is
s
∂L =− (cid:88) 1 ∂yˆ ij + (cid:88) 1 ∂yˆ ij , (63)
∂W yˆ ∂W 1−yˆ ∂W
s ij s ij s
(i,j)∈E (i,j)∈S
14wheres∈Z+,s≤d,
∂yˆ
ij
∂W
s
∂H HT
=Σ′ (H HT ) di∗ dj∗ (64)
d+1 di∗ dj∗ ∂W
s
∂Σ (Aˆ H W )Σ (Aˆ H W )T
=Σ′ (H HT )· d i∗ d−1 d d j∗ d−1 d (65)
d+1 di∗ dj∗ ∂W
s
∂Σ (Aˆ H W )
=Σ′ (H HT )·( d i∗ d−1 d (I ⊗Σ (Aˆ H W )T)
d+1 di∗ dj∗ ∂W ns d j∗ d−1 d
s
∂Σ (Aˆ H W )T
+(I ⊗Σ (Aˆ H W )) d j∗ d−1 d ) (66)
ns−1 d i∗ d−1 d ∂W
s
=Σ′ (H HT )
d+1 di∗ dj∗
∂Aˆ H W
·(((J ⊗Σ′(Aˆ H W ))⊙ i∗ d−1 d)·(I ⊗Σ (Aˆ H W )T)
ns−1×ns d i∗ d−1 d ∂W ns d j∗ d−1 d
s
∂(Aˆ H W )T
+(I ⊗Σ (Aˆ H W ))·((J ⊗Σ′(Aˆ H W )T)⊙ j∗ d−1 d )),
ns−1 d i∗ d−1 d ns−1×ns d j∗ d−1 d ∂W
s
(67)
(64)followsfromchainruleanddefinition2.2,(65)followsfrom(61),(66)followsfrom(52),(67)
followsfromtheorem(2.3),

∂Aˆ i∗H d−1W
d
= (I ns−1 ⊗(Aˆ i∗H ∂Hd−1))U¯ ns−1×ns, ifs=d
(68)
∂W (I ⊗Aˆ ) d−1(I ⊗W ), ifs<d,
s  ns−1 i∗ ∂W ns d
s
∂(Aˆ j∗H d−1W d)T =  U ns−1×ns(I ns ⊗ ∂H(A Tˆ j∗H d−1)T), ifs=d
(69)
∂W s  (I ns−1 ⊗W dT) ∂Wd−1(I ns ⊗AˆT j∗), ifs<d,
s
(68)followsfrom(52)and(53),(69)followsfrom(52)and(54), ∂Hd−1 issimilarto(42)butwith
∂Ws
A=Aˆ,
∂HT
d−1
∂W
s
 (J ⊗Σ′ (AˆH W )T)⊙(U (I ⊗(AˆH )T)),
 ifsns =−1 d×n −s
1
d−1 d−2 d−1 ns−1×ns ns d−2
= ∂HT
 i( fJ sns <−1 d×n −s 1⊗
,
Σ′ d−1(AˆH d−2W d−1)T)⊙((I ns−1 ⊗W dT −1) ∂Wd− s2 ·(I ns ⊗AˆT)),
(70)
and(70)followsfrom(52),(54),andtheorem(2.3).
E Sensitivityanalysis
E.1 Sensitivityoflosswithrespecttofeaturematrix
TheexpressionofthesensitivityoflosswithrespecttothefeaturematrixH issimilartotheresult
0
insection2.1.3exceptthatwechangethedimensionoftheidentitymatrix,theall-onematrix,and
thepermutationrelatedmatrix,andwechangetheresultin(35)and(42)forthelastlayer. Thus,the
derivative,orsensitivity,ofthelossin(39)withrespecttotheinputfeaturematrixH
0
∈Rn×n0 is
∂L
=−(cid:88)n (cid:88)nd
(
y
ij
−yˆ
ij Σ′ (h )·((J ⊗Σ′(A H W ))
∂H yˆ (1−yˆ ) d+1 dij n×n0 d i∗ d−1 d∗j
0 ij ij
i=1j=1
∂A H W
⊙
i∗ d−1 d∗j)),
(71)
∂H
0
15where
∂Ai∗Hd−1Wd∗j
isthedefinedas
∂H0

(I ⊗A )·U¯ (I ⊗W ) ifd=1
∂A i∗H d−1W
d∗j
= n i∗ ∂Hn×n0 n0 d∗j
, (72)
∂H (I ⊗A )· d−1(I ⊗W ) ifd>1
0  n i∗ ∂H n0 d∗j
0
and
∂H
d−1
∂H
0
 (J ⊗Σ′ (AH W ))⊙((I ⊗A)U¯ (I ⊗W ))
 ifdn× −n0 1=1d−1 d−2 d−1 n n×n0 n0 d−1
(73)
= ∂H
(J ⊗Σ′ (AH W ))⊙((I ⊗A) d−2(I ⊗W ))
 ifdn× −n0 1>1.d−1 d−2 d−1 n ∂H
0
n0 d−1
E.2 Sensitivityofoutputwithrespecttofeaturematrix
Similarly,thederivative,orsensitivity,ofeachelementoftheoutputin(60)withrespecttotheinput
featurematrixH
0
∈Rn×n0 is
∂yˆ
ij
∂H
0
=Σ′ (H HT )
d+1 di∗ dj∗
∂Aˆ H W
·(((J ⊗Σ′(Aˆ H W ))⊙ i∗ d−1 d)·(I ⊗Σ (Aˆ H W )T)
n×n0 d i∗ d−1 d ∂H n0 d j∗ d−1 d
0
∂(Aˆ H W )T
+(I ⊗Σ (Aˆ H W ))·((J ⊗Σ′(Aˆ H W )T)⊙ j∗ d−1 d )),
n d i∗ d−1 d n×n0 d j∗ d−1 d ∂H
0
(74)
where

∂Aˆ i∗H d−1W
d
= (I n⊗Aˆ i∗)U ∂¯ Hn×n0(I n0 ⊗W d), ifd=1
(75)
∂H (I ⊗Aˆ ) d−1(I ⊗W ), ifd>1,
0  n i∗ ∂H n0 d
0
∂(Aˆ j∗H d−1W d)T =  (I n⊗W dT)U ∂Hn× Tn0(I n0 ⊗(Aˆ j∗H d−1)T), ifd=1
(76)
∂H 0  (I n⊗W dT) ∂Hd−1(I n0 ⊗AˆT j∗), ifd>1,
0
∂Hd−1 issimilarto(73)butwithA=Aˆ,and
∂H0
∂HT
d−1
∂H
0
 (J ⊗Σ′ (AˆH W )T)⊙((I ⊗WT )U (I ⊗AˆT)),
 ifdn× −n0 1=1d−1 d−2 d−1 n d−1 n×n0 n0
(77)
= ∂HT
 i( fJ dn× −n0 1⊗ >Σ 1.′ d−1(AˆH d−2W d−1)T)⊙((I n⊗W dT −1) ∂Hd− 02 ·(I n0 ⊗AˆT)),
F Datadescription
F.1 Nodeclassification
WetestedourmethodfornodeclassificationonZachary’sKarateClub[19]. Zachary’sKarateClub
isasocialnetworkconsistingof34membersofakarateclub, whereundirectededgesrepresent
friendships,asshowninFigure3(a)[29]. Everynodewaslabeledbyoneoffourclassesobtained
viamodularity-basedclusteringin[2]andwesetclass2toclass0andclass3toclass1forbinary
nodeclassification. Thegoalistoclassifythenodesintothecorrectclass.
16Figure3: (a)Zachary’skarateclubsocialnetwork[19]. Thenodecolorssignifyclasses,withblue
representingclass0andorangerepresentingclass1. (b)Drug-druginteractionnetwork. Blacklinks
betweeneachnoderepresentgraphedges.
F.2 Linkprediction
Drug-DrugInteraction(DDI)commonlyariseswhenmultipledrugsthattargetthesamereceptors
areadministeredconcurrently. Suchinteractionscanresultindiminishedtherapeuticefficacyby
hinderingdrugabsorptionandamplifyingadversedrugreactions, whichmayleadtounforeseen
off-targetsideeffects[30]. WetestedourmethodforlinkpredictionusingaDDInetwork,whichwas
formedbyextractingdatafromtheDrugBankdatabase[31]. Thisnetworkrevolvedarounddrugsof
interest,identifiedthroughtheanalysisofdifferentiallyexpressedgenesfromRNA-sequencedata
withintheGSE147507dataset,particularlyinthecontextofaCOVID-19study[32]. Interactions
betweendrugsandproteinswereestablishedusingDrugBank’savailabledrug-proteininteractions,
servingascommunicationpathways. Themostdisruptedsignalingpathway,"Herpessimplexvirus1
infection,"comprised495proteinsandwasselectedbasedontheanalysisofdifferentiallyexpressed
genesandKEGGpathwaydatabase[33]. Subsequently,214drugswereextractedfromDrugBank
for their direct interactions with the 495 proteins of interest. In total, 468 drugs (including the
aforementioned214)with48,460DDIsweresourcedfromDrugBank,capableofinteractingwith
the214drugs. TheDDInetworkwasconstructedusingthese468drugs. Forthisstudy,thefirst100
drugs,basedontheirDrugBankindex,wereextractedtoformamanageablesubsetforanalysis.
Thedrugfeaturesanalyzedinthisstudyencompassed20molecularpropertiessuchasweightand
watersolubility. Furthermore,pairwisechemicalsimilaritiesbetweendrugswerecomputedusingthe
Jaccardcoefficient,quantifyingstructuralsimilaritybasedonSimplifiedMolecularInputLineEntry
System(SMILES)strings[34]. PrincipalComponentsAnalysiswasusedandthefirst20(outof488)
principalcomponents,whichexplained98%ofvariabilityindrugfeaturedata,wereusedasinput
featurestoGCN.
G Theevolutionofbinarynodeclassificationandlinkprediction
G.0.1 5-layerGCNwithidentityfunctionandsigmoidactivationfunctionforbinarynode
classification
The5-layerGCNisintheformof(37)andthelossfunctionisdefinedby(39),whered=5isthe
numberoflayers,H = I ∈ Rn×n isthefeaturematrixforn = 34nodeswithn = 34features,
0 n
thematrixY ∈ Rn×nd representsthegroundtruthofnodes, Yˆ ∈ Rn×nd denotesthefinallayer
predictionsoftheGCNmodel,n
d
=1forbinarynodeclassification,W
1
∈Rn0×n1,W
2
∈Rn1×n2,
W
3
∈ Rn2×n3, W
4
∈ Rn3×n4, and W
5
∈ Rn4×nd are trainable parameter matrices, n
0
= 20,
n =2,n =3,n =2,n =3,Σ isanelement-wiseReLUfunction[21],Σ isanelement-wise
1 2 3 4 1 2
sigmoid linear Unit (SiLU) function [35], Σ is an element-wise exponential linear unit (ELU)
3
17functionwithα=1[36],Σ isanelement-wiseLeakyReLUfunction[37],Σ isanelement-wise
4 5
identityfunction,andΣ isanelement-wisesigmoidfunction.
6
The GCN is trained using SGD with a learning rate of 3×10−5 and with 10 iterations for both
methods. TheGCNistrained1060timesandtheweightmatricesarereinitializedeverytime. If
thecalculationofthelossfunctionhasnanvalue,thetrainingatthistimewillbeskippedandwill
notbecounted. ThelineplotoftheSSEofthefiveweightmatrices,W ,W ,W ,W ,andW ,
1 2 3 4 5
betweenthereversemodeautomaticdifferentiationandourmatrix-basedmethodisshowninFigure5
(a). Theboxplotofthelog oftheSSEinFigure5(b)showsthatthemedianofthefiveweight
10
matricesisbetween10−18and10−14. IntheboxplotofFigure5(b),theboxextendsfromthefirst
quartiletothethirdquartileofthedata,withalineatthemedian,whilethehorizontallinesextending
fromthebox,whicharecalledwhiskers,indicatingthefarthestdatapointlyingwithin1.5timesthe
inter-quartilerangefromthebox,andcirclesarethosepointspasttheendofthewhiskers[38].
Figure4: Evolutionofkarateclubnetworknodeclassificationobtainedfroma1-layerGCNmodel
after100trainingiterations. Thetrainingloss, accuracy,andclassificationresultsexhibitsimilar
trendswhenusingeitherreverse-modeautomaticdifferentiationorourmatrix-basedmethod. Nodes
arerepresentedbycirclewiththenumbertodistinguisheachnode. Colorsrepresentclasses,where
bluerepresentsclass0andorangerepresentsclass1. Blacklinksbetweeneachnoderepresentgraph
edges.
G.0.2 5-layerGCNforlinkprediction
The 5-layerGCNis definedby(60)and thelossfunctionis definedby(62), where d = 5isthe
number of layers, the matrix Yˆ ∈ Rn×n represents ground-truth adjacency matrix, Yˆ ∈ Rn×n
denotesthefinallinkpredictionsoftheGCNmodel,W
1
∈Rn0×n1,W
2
∈Rn1×n2,W
3
∈Rn2×n3,
W
4
∈ Rn3×n4, andW
5
∈ Rn4×n5 aretrainableparametermatrices, n
0
= 20, n
1
= 2, n
2
= 3,
n =5,n =3,n =40,Σ isanelement-wiseLeakyReLUfunction[21],Σ isanelement-wise
3 4 5 1 2
ELUfunction[35],Σ isanelement-wiseSiLUfunctionwithα=1[36],Σ isanelement-wise
3 4
ReLUfunction[37],Σ isanelement-wiseidentityfunction,andΣ isanelement-wisesigmoid
5 6
function.
TheGCNistrainedusingSGDwithalearningrateof0.9andwith10iterationsforbothmethods.
Ineachiteration,thirteennegativeedgesareuniformlysampledfromasetofalltheunconnected
edgesfortheGCNtrainingusingthereversemodeautomaticdifferentiation. Thesamplednegative
edgesineachiterationaresavedinalistandusedinthetrainingoftheGCNusingourmatrix-based
method. TheGCNistrained1060timesandtheweightmatricesarereinitializedeverytime. Ifthe
calculationofthelossfunctionhasnanvalue,thetrainingatthistimewillbeskippedandwillnotbe
counted. ThelineplotoftheSSEofthefiveweightmatrices,W ,W ,W ,W ,andW ,between
1 2 3 4 5
thereversemodeautomaticdifferentiationandourmatrix-basedmethodisshowninFigure6(a).
Theboxplotofthelog oftheSSEinFigure6(b)showsthatthemedianofthefiveweightmatrices
10
isbetween10−18and10−14.
18Figure 5: (a) The line plot of the 1060 evolution of the sum of squared error (SSE) between the
trainableweightmatricesobtainedfromourmethodandthematrixobtainedusingreversemode
automaticdifferentiationinsectionG.0.1. (b)Theboxplotofthe1060evolutionoftheSSEbetween
thetrainableweightmatricesobtainedfromourmethodandthematrixobtainedusingreversemode
automaticdifferentiationinsectionG.0.1.
19Figure 6: (a) The line plot of the 1060 evolution of the sum of squared error (SSE) between the
trainableweightmatricesobtainedfromourmethodandthematrixobtainedusingreversemode
automaticdifferentiationinsectionG.0.2. (b)Theboxplotofthe1060evolutionoftheSSEbetween
thetrainableweightmatricesobtainedfromourmethodandthematrixobtainedusingreversemode
automaticdifferentiationinsectionG.0.2.
20Figure7:EvolutionofDDInetworknodeclassificationobtainedfroma2-layerGCNmodelafter150
trainingiterations. Thetraininglossandclassificationresultsexhibitsimilartrendswhenusingeither
reverse-modeautomaticdifferentiationorourmatrix-basedmethod. Nodesarerepresentedbycircle
withthenumbertodistinguisheachnode. Blacklinksbetweeneachnoderepresenttruthgraphedges.
Redlinksbetweeneachnoderepresenttheedgesthatdoesnotexistinthegroundtruthadjacency
matrix. Thetransparencyoftheblackandredlinkisproportionaltothepredictedlinkprobability.
21Figure8: HeatmapsofoutputsensitivityoftheDDInetwork(1)
22Figure9: HeatmapsofoutputsensitivityoftheDDInetwork(2)
23Figure10: HeatmapsofoutputsensitivityoftheDDInetwork(3)
24