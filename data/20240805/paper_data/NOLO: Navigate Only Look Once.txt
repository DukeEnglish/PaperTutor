NOLO: Navigate Only Look Once
BohanZhou JiangxingWang ZongqingLu
SchoolofComputerScience SchoolofComputerScience SchoolofComputerScience
PekingUniversity PekingUniversity PekingUniversity
zhoubh@stu.pku.edu.cn jiangxiw@stu.pku.edu.cn BAAI
zongqing.lu@pku.edu.cn
Abstract
Thein-contextlearningabilityofTransformermodelshasbroughtnewpossibilities
tovisualnavigation. Inthispaper,wefocusonthevideonavigationsetting,where
anin-contextnavigationpolicyneedstobelearnedpurelyfromvideosinanoffline
manner, withoutaccesstotheactualenvironment. Forthissetting, wepropose
NavigateOnlyLookOnce(NOLO),amethodforlearninganavigationpolicythat
possessesthein-contextabilityandadaptstonewscenesbytakingcorresponding
contextvideosasinputwithoutfinetuningorre-training. Toenablelearningfrom
videos, we first propose a pseudo action labeling procedure using optical flow
torecovertheactionlabelfromegocentricvideos. Then, offlinereinforcement
learningisappliedtolearnthenavigationpolicy. Throughextensiveexperiments
ondifferentscenes,weshowthatouralgorithmoutperformsbaselinesbyalarge
margin,whichdemonstratesthein-contextlearningabilityofthelearnedpolicy.
1 Introduction
…… a …… a
t
VN Bert 视觉导航 决策模型
…… ……
h
f a f a a f g o f a f a a f g o
1 1 2 22 T-1 T t 1 1 2 22 T-1 T t t
^ ^ ^ ^ ^ ^
GMFlow Action Decoder 光流动作预测模块
……
f f context f hidden current f f 输入视频序列 f 隐状态 当前观测
1 2 information T state observation 1 2 T
Figure1: TheframeworkofNOLO.Pseudoactions{aˆ }T−1arefirstextractedfromacontextvideo.
t 1
Then,allcontextframes{f }T andlabeledactions{aˆ }T−1alongwithacurrentobservationo anda
t 1 t 1 t
goalimagegaresubsequentlytakenbyabidirectionalrecurrentTransformer,VN⟲Bert,togenerate
thediscreteactiondistributionπ (·|g,o ,f T,{aˆ }T−1)fornavigation. Theyellowcircleindicates
θ t t1 t 1
additionalaggregationviafusingelement-wiseproductbetweencontextfeaturesandobservation
visualfeatureslikeVLN-Bert[1].
Preprint.Underreview.
4202
guA
2
]VC.sc[
1v48310.8042:viXraVisualnavigationhaslongbecomearesearchfocusduetoitswideapplicationindifferentfields,
includingmobilerobots[2], autonomousvehicles[3], andvirtualassistants[4]. Alargebodyof
researchhasbeenproposedtosolvethisproblemviaeithercombiningmoduleslikeobjectdetectors,
keypointmatchers,depthestimators,andSLAMmodules[5,6,7,8,9]ortrainingasinglepolicy
modelend-to-end[1,10,11]. Althoughthesemethodshaveachievedgreatsuccess,theyoftensuffer
frompoorgeneralizationorrequireadditionalinteractionsandtrainingwhentransferredtoanovel
scene,whichleadstonon-trivialproblemsinreal-worldapplications.
Unlikethesemethods,humanscansolvethevisualnavigationproblem,morespecificallytheindoor
visualnavigationproblem,inamucheasierway.Insteadofwalkingaroundandexploringanewroom
inperson,wepossessaninnateabilitytoeasilyunderstandtheroomconfigurationbyjustwatchingan
egocentrictraversalvideoofthisroomandthenfigureouthowtoreachdifferenttargetsintheroom.
However,unlikehumans,themajorityofexistingmethodsheavilyrelyonheterogeneoussensorsto
perceivetheenvironment,suchasposesensorslikeGPS+Compass[12,13]forestimatingposition
andorientation,RGB-Dcameras[14]forobservingthesurroundingenvironmentandmeasuringthe
distance,andIMU[15]fortracingmovement. Whereashumans,donotexplicitlyrelyonposesor
distancemeasurementandcanstillnavigatetodifferenttargets.
Inspiredbyhumannavigation,anaturalandintuitiveideawouldbeofflinetraininganavigationpolicy
thatcangeneralizetoanewscenegivenanegocentrictraversalvideoofthenewroom,whichaligns
withtheideaofin-contextlearning[16]. Ithasbeenshownthatlargelanguagemodels(LLMs)can
workwellonanewtaskwhenfewdemonstrationsofthecorrespondingtaskaregivenasthecontext,
withoutanyadditionaltrainingprocess[17]. Thein-contextlearningmethodhasprovenitsvaluein
manyfieldsrangingfromLLM-basedagents[18],tometa-algorithmlearning[19,20,21]. Given
thesuccessofin-contextlearning,weproposeanewsettingofthevisualnavigationproblem,Video
Navigation. Thegoalofvideonavigationistotrainanin-contextpolicytofindobjectsoccurredin
thecontextvideo. Duringthetrainingprocess,wehaveonlyaccesstoadatasetofegocentrictraversal
videosindifferentscenesandweneedtotrainanin-contextnavigationpolicyinanofflinemanner.
Duringinference,thevideoofanewscenewillbegivenasthecontext,andthelearnedpolicyshould
takethecontextvideoasinputandgeneralizetothenewscenewithoutfinetuningorre-training.
The setting of video navigation is promising to equip mobile agents with human-like navigation
capabilities. With video navigation, imagine that a new sweeping robot can quickly and freely
navigatetoanycornerofyourhouseanddocleaningafteronlywatchingashortvideoclipofthe
houselayout. Thus, wecansayvideonavigationismoresuitableforreal-worldapplications, as
finetuningorre-trainingthepolicyisusuallyinfeasibleafterdeployment. Althoughencouraging,
videonavigationposesconsiderablechallenges:
• First, due to the lack of sufficient information, including actual actions, spatial correla-
tion,andvisualunderstandingofcomplexscenes,learningtonavigatebyonlywatching
egocentrictraversalvideosischallenging.
• Second,egocentrictraversalvideosdonotexhibitexplicitintentions. Asaresult,inverse
reinforcementlearningorlearningfromdemonstrationscannotbedirectlyapplied,since
experttrajectoryisanimperativeprerequisite.
• Moreimportantly,inthevideonavigationsetting,wehavenoaccesstocameraintrinsics,
robotposes,maps,odometers,anddepthinputs,whichdistinguishesourworkfromothers.
Mostexistingmethodscannotleavewithposesordepthimagestoexplicitlybuildamapfor
furtherplanning. Inthiswork,weexpecttheagentcannavigatelikeahuman,onlywith
egocentricobservations.
To address these challenges, we propose Navigate Only Look Once (NOLO). NOLO learns a
generalizablein-contextnavigationpolicyconditionedonacontextvideo,suchthatitcanadaptto
thenewscenewithoutfinetuningorre-trainingbyunderstandingthecorrespondingcontextvideo.
Totrainsuchapolicy,wefirstextractpseudoactionsfromcontextvideosusingopticalflowtoget
completecontexttrajectories. Subsequently,anofflinereinforcementlearningmethodisadoptedto
learnthein-contextnavigationpolicy. Ourkeycontributionscanbesummarizedasfollows:
• WepresentVideoNavigation,anovelandpracticalsettingforvisualnavigation. Tothe
bestofourknowledge,itisthefirstattempttolearnanin-contextnavigationpolicyinan
offlinemannerpurelyfromvideossuchthatthelearnedpolicycanadapttodifferentscenes
bytakingcorrespondingvideosascontext.
2• WeproposeNOLOtosolvethevideonavigationproblem. NOLOseamlesslyincorporates
opticalflowintoofflinereinforcementlearningviapseudo-actionlabeling. Toencourage
thetemporalcoherenceofrepresentation,wefurtherproposeatemporalcoherencelossfor
temporallyalignedcontextvisualrepresentation.
• EmpiricalevaluationsinRoboTHORandHabitatdemonstratetheeffectivenessofNOLO
invideonavigation. Notably,thisisachievedbyonlywatchingasingle30-secondvideo
clipineachscene.
2 RelatedWork
VisualNavigation. Thevisualnavigationproblemcanbecategorizedintoindoornavigation[22,23,
24]andoutdoornavigation[25,26]. Inthispaper,weonlyfocusontheindoornavigationproblem,
suchthatallscenesareindoorscenarios. Forindoornavigation,researchmainlyfocusesonfinding
thelocationofspecificpositions[13,27],rooms[28],orobjectinstances[29,9,27,30]. Thegoals
ofnavigationcanbedescribedincoordinates[13],naturallanguage[31,8,32,11,33],goalimage
oftargetposition[28,8,32],orgoalimageofobjectinstances[29,9,27,30]. Ourvideonavigation
offindingobjectsoccurredinthecontextvideofallsintothelastgroup.
Visual navigation methods can also be categorized by whether exploration is needed [34]. Most
workspursuegeneralizationoverunseengoalsorunseenenvironmentsandusuallyrelyonlearning
algorithms that require massive online interactions, e.g., reinforcement learning [35, 36, 37], to
exploretheenvironmentandaccomplishthetask. Whilesomeworkstrytoimprovedataefficiency,
thesemethodsstillneedtointeractwiththeenvironmentusingarandompolicytolearnaninverse
dynamicmodel[38]orassumehigh-qualitydatasetsincludingfeatureslikerobotposes[7]ordepth
images[14].Unlikepreviouswork,ourmethodfreesrobotsfrominteractingwithactualenvironments
andtargetstoonlylearnfromvideoswithoutactualactions.
In-Context Learning. The ability of in-context learning is first demonstrated in large language
models[17,39,40],wherethelearnedmodelcanperformanewtask,givenafewcorresponding
examples as context. Similar abilities are also observed in vision language models [41, 42, 43]
andmodelsforothermodalities[44,45]. Theabilityofin-contextlearningcanbeappliedtoAI
agents[46,47]tosolvedifferenttasks. IthasalsobeenshownthatlargeTransformermodelscanbe
usedtolearnameta-algorithmtoimitatethereinforcementlearningalgorithm[19,20,48]. Ourwork
focusesonthefusionofin-contextlearningandthevisualnavigationproblem,andtothebestof
ourknowledge,isthefirstattempttotrainanin-contextnavigationpolicythatcanadapttodifferent
scenesbytakingcorrespondingvideosascontext.
LearningfromVideos. Therearefourmainlinesofresearchstudylearningfromvideos. (1)Learn
distinctiverepresentationsforintrinsicrewards. Theseapproachesmainlyincludepredictingthe
future [49, 50, 51], learning temporal abstractions [51, 52, 53, 54], contrastive learning [55, 56],
adversarial learning [57, 58, 59, 60] or other transition-learning based methods [61, 62, 63]. (2)
Recovervaluefunctionalongwithrepresentations[64,65,66,67]. (3)Pretrainaction-freemodelin
thefirststageandthenboostthemodelwithadditionalactioninformation[68,69,70]. (4)Discover
latentactions[71,72,73,74]andthenaligntotherealactionsonlineintheenvironment[71,72,73],
byhumanannotation[74]orfromofflinedatasets[73]. Fewattemptsaredevotedtodecodingreal
actions directly from videos. A recent work [75] attempts to principally use similar rule-based
methodstodirectlyinferactionsforrobotmanipulation. However, itdoesnotexhibitin-context
learningcapabilitiesandrequiresadditionaldepthimageinputs. Unlikepreviouswork,wefocuson
thevisualnavigationproblemandbyutilizingitsspecificity,proposeanofflinemethodthatdirectly
learnstheactualnavigationpolicy,notthelatentone,fromvideos.
3 LearningtoNavigatefromVideos
3.1 TheProblemFormulationofVideoNavigation
Invideonavigation,weofflinerecoveranavigationpolicyπ (a|g,o,V),whichtakesagoalimageg,
θ
currentobservationo,andcontextvideoV asinputandproducesactionaasoutput. Suchapolicy
isexpectedtofindobjectsoccurredinthecontextvideo. Totrainsuchapolicy,weassumeavideo
datasetDcontainsN videosofN differentscenes,D =(cid:8) V1,V2,...,VN(cid:9) . EachvideoVicontains
3Move
GMFlow optical dominant discrete
Network flow vectors action Left
Right
Figure2: Twoadjacentframesaretakenbyapretrainedopticalflowmodeltogetaflowmap. Some
representativedominantvectorsarefilteredforactionselection.
T framesfi,Vi =(cid:8) fi,fi,...,fi(cid:9) . Duringinference,wedeploythelearnedpolicyinanewscene
t 1 2 T
givenacontextvideoVnew ∈/ Dandagoalframeg ∈Vnew. Thepolicyisexpectedtoadapttothis
newsceneviathecontextvideowithoutanyfinetuningorre-training.
3.2 PseudoActionLabeling
Torecoverapolicyfromvideos,lotsofworkadherestotheparadigmoflatentactionextractionfrom
adjacentframesutilizinganinversedynamicsmodel(IDM)[76,71,72,73,74],followedbylatent
policylearningandpolicydecodingusingactualactionlabels.Withinthecontextofvisualnavigation,
weproposetouseGMFlow[77]asanimplicitIDM,F ,toestimatediscretepixeldisplacements
ξ
betweentwoconsecutiveframes,therebyenablingthepredictionofthenavigationactionaˆ with
t
simple rule-based post-processing p. Continuous actions like linear velocity and angle velocity
areusuallyappliedtoreal-worldnavigation,whilehigh-leveldiscreteactionsarealsocommonin
competitionslikehabitatnavigationchallenge[78]andmodernnavigationtasks[25]. Thesequential
processcanbeformalizedinEquation(1),
∆f =F (f ,f )
t ξ t t+1
(1)
aˆ =p(∆f ).
t t
Occasionally,wenoticethatthedisplacementofpixelsobservedacrossframesmanifestsirregular
patterns. Tomitigatethisissue,foreachpairofadjacentframes(f ,f ),wefilterdominantvectors
t t+1
ν thatexhibitgradientamplitudeswithintheupperdecile,whicharesubsequentlydiscretizedinto
t
actionsderivedfromtheactionspace,makingitpossibletotracethetrajectoryoftheagentovertime.
ThepseudoactionlabelingisillustratedinFigure2,andseeAppendixB.1formoredetails.
3.3 In-ContextPolicyModeling
Withpseudoactionlabelsdecodedfromopticalflow,wecanthenbuildaframe-actiondataset,and
trainanin-contextnavigationpolicyπ onthisdataset. InspiredbyVLN-Bert[1],weproposea
θ
recurrentBert-likestructure,VN⟲Bert,tomodelthein-contextnavigationpolicy. Atinferencestage,
F decodespseudoactionsequenceaˆnewfromagivencontextvideoVnew,whichisconcatenatedwith
ξ
VnewtoformatrajectoryT. Ateachtimestept,VN⟲Berttakesagoalimageg,currentobservation
o ,andthetrajectoryT toproducetheactiondistributionπ (·|g,o ,T)andthehiddenstateh at
t θ t t+1
nexttimestep. Theprocesscanbeformalizedasfollows:
h ,π (a |g,o ,T)=VN⟲Bert(g,o ,T). (2)
t+1 θ t t t
Similar to VLN-Bert that encodes language instructions into discrete tokens, VN⟲Bert encodes
frame-action pairs of context into embeddings. Specifically, as depicted in Figure 3, all context
framesandthecurrentobservationareencodedbyalearnableResNetencoderE pretrainedfrom
ζ
Places365[79]togetthevisualembeddingses =E (f ). Actionsareencodedbyalearnableaction
t ζ t
embeddinglayerE togettheactionembeddingsea =E (aˆ )andthegoalisencodedbyafixed
α t α t
pretrainedCLIP[80]encodertogetthegoalembeddingeg =CLIP(g).
At the initialization stage, a zero-padded hidden state h is additionally inserted at the back of
init
the context embedding sequence for recurrence. The entire token embedding sequence e =
init
[h ,es ,ea ...,ea ,es ]isencodedbyamulti-layerbidirectionalself-attentionmodule
init init,1 init,1 init,T−1 init,T
(SA)ase = [ec,h ] = SA(e ). Theoutputfeatureofthehiddenstateh servesastheinitial
0 0 init 0
4Token Re>currence MLP
aggregation
Multi-Modal Multi-Head Cross Attention (CA)
Positional Encoding + Bidirectional Multi-Head Self Attention (SA) Fuse action embedding
>
Recurret
Hidden
Move Right Left State
Figure 3: Structure of VN⟲Bert. At the initialization stage, a trajectory T and a zero-padded
hiddenstateh areprocessedbyabidirectionalmulti-headself-attentionmoduletoobtaincontext
init
embeddingecandinitialhiddenstateh . Ateachtimestepafterinitialization,thecurrentobservation
0
o andgoalframegareencodedintoesandaretakenbyamulti-headcross-attentionmoduletogether
t t
withfixedecandrecurrentlyupdatedh toproducepolicyπ ,Q-valueQ ,andterminalsignalδ .
t θ ω υ
compressed representation of the context trajectory. After initialization, the context embedding
sequenceeckeepsunchanged. Ateachtimestept,thecurrentobservationo andthegoalimagegare
t
encodedintoesandeg,respectively. Theconcatenatedembeddingsequencee =[h ,ec,es,eg]is
t t t t
thenfedintothecross-attentionmodule(CA)toobtaintheupdatedembeddinge =CA(e )which
t+1 t
isfurtheraggregatedandprojectedtoactiondistributionπ (a |g,o ,T)andthenexthiddenstate
θ t t
h . Theupdatedhiddenstateh issubsequentlyprocessedbyamultilayerperceptron(MLP)
t+1 t+1
P toregressQ-valuesQ (g,o ,a ,T)(discussedlaterinSection3.4)andbyanotherMLPP to
Q ω t t D
predictthebinaryterminalsignalδ (d |g,o ,T),whered isusedforpredictingtheterminationof
υ t t t
thecurrenttask.
To train VN⟲Bert, we sample a context video Vi from dataset D and then construct a context
trajectoryTi ={f ,aˆ }T asdescribedinSection3.2. Then,wefirstrandomlysampleaframef
t t t=1 t
asobservationimageo andthecorrespondingpseudoactionaˆ fromTi,thenwesampleanother
t t
frameasthegoalimagegfromTi. Wesetd =I(o =g)fortheterminationprediction,whereI
t t
standsfortheindicatorfunction. Weforwardthemodelandcalculatediscreteactionpredictionloss
andterminationpredictionlossinEquation(3)respectively,
L =−E logπ (cid:0) aˆ |g,o ,Ti(cid:1)
a g,ot,aˆt∼Ti θ t t
(3)
L =−E logδ (cid:0) d |g,o ,Ti(cid:1) .
d g,ot∼Ti υ t t
3.4 Batch-ConstrainedQ-Learning
Asthepolicyusedtorecordthevideocouldbehighlysuboptimalforthevideonavigationproblem,
wepreferofflinereinforcementlearningoverimitationlearningforpolicytraining. Specifically,we
adoptBCQ[81],anofflinereinforcementlearningmethodthatemphasizesconstrainingtheaction
selectiontothosewithinthedistributionoftheobservedframe-actionpairs. TheBellmanupdatein
Equation(4)isusedforlearningQ-functionQ ,giventhetemporaldifferencetargetinEquation(5)
ω
wherer(g,o ,aˆ )=I(o =g)isabinaryrewardindicatingthesuccessofatask,
t t t
L = E
(cid:2)
Q
(cid:0)
g,o ,aˆ
,Ti(cid:1)
−y
(cid:3)2
, (4)
q ω t t t
ot,ot+1,g,aˆt∼Ti
y =r(g,o ,aˆ )+γ max Q
(cid:0)
g,o ,a˜
,Ti(cid:1)
. (5)
t t t ω t+1 t+1
a˜t+1∈Aβ
Sincetheactionspaceisdiscrete,toselecta˜ ,wedefineasetAβ,whereβ denotesthethreshold
t+1
fortheactionselection. Werestricttheactionchoiceconsideringtheactiondistributioninthecontext
trajectorydataset,
(cid:110) (cid:12) π
(cid:0)
·|g,o
,Ti(cid:1)
(cid:111)
Aβ := a˜ (cid:12) θ t+1 >β . (6)
t+1(cid:12)maxπ (·|g,o ,Ti)
θ t+1
ThelearnedQ-functionQ canguidetheagenttowardsthegivengoalg.
ω
5
> >3.5 TemporalCoherence
Learningtemporallyalignedrepresentationofvideosiscrucialforunderstandingthecausalityofthe
contextvideo. InspiredbyRLHF[82],welearna"fuzzy"measureofprogressfromtheirnatural
temporalordering,wherethehigherpreferencescoreisassignedtothelaterframesinthevideo. To
achievethis,welearnatemporalindicatoruˆ (e),whichconsistsof1Dself-attentionandspectral
ϕ
normalizationmodulestomapavisualcontextembeddingestoautilityscore. Suchanindicatorcan
t
beoptimizedasfollows:
minL =−E logσ(cid:2) uˆ (cid:0) E (f )(cid:1) −uˆ (cid:0) E (f )(cid:1)(cid:3) , (7)
ζ,ϕ
t ft−,ft+∼V ϕ ζ t+ ϕ ζ t−
wheret >t andf ,f standsforthelaterandearlierframeinthevideo. Thisself-supervised
+ − t+ t−
paradigmnaturallydrivesthetemporalindicatoruˆ (s)tomaximizelikelihoodtoensurethetemporal
ϕ
orderpreferencef ≻f ,automaticallyleadingtotemporallyalignedcontextvisualrepresentation,
t+ t−
whichisbeneficialtobetterunderstandthetemporalrelationinanovelscene,asdemonstratedlater
inSection4.5.
4 Experiments
4.1 ExperimentalSetups
Configurations.Weapplyouralgorithmtoopen-sourcesimulation-to-realplatformRoboTHOR[83]
andHabitat[84],whichserveasrealisticembodiednavigationtestbeds. Foreachnavigationtaskin
RoboTHORandHabitat,weinitializeamobilerobotwitharandomposition,orientation,andagiven
goalimageextractedfromthecontextvideoindicatingaspecificgoalobject. Ateachtimestep,the
agent,mountedwithonlyoneRGBcameraof224×224resolution,receivesanegocentricimage
andcanexecuteanactionfromafixedactionspace[MoveForward,TurnLeft,TurnRight,STOP].
Theminimumunitsofrotationandforwardmovementare30°and0.25metersrespectively. When
thegoalobjectisobservedandthedistancebetweenthegoalobjectandtheagentiswithin1.0meter,
theagentsucceeds,otherwiseitfailswhenthemaximumtimestepisreached,whichis500inallour
experiments.
TaskDivision. InRoboTHOR,thereare15differentroomsand5possiblelayoutsforeachroom,in
total75differentscenes. Eachroomhasadifferenttopologyfromotherrooms,andthelocationof
objectsineachlayoutisdifferentfromotherlayouts. Inordertotestthein-contextlearningabilityin
differentlevels,wedividedthetrainingandtestscenesasfollows. Forthetrainingset,weselect12
rooms,and4layoutsforeachroom. Fortheunseenlayouttestingset,weselecttheremaining1
layoutofthe12trainingsetrooms,intotal12scenes. Fortheunseenroomtestingset,weselectthe
remaining3roomsandalltheirlayouts,intotal15scenes. InHabitat,wedivideall90building-scale
scenesfromMatterport3D(MP3D) [85]into70trainingscenesand20testingscenestotestthe
generalizationabilities. SeeAppendixC.1fordetails.
DatasetConstruction. Weusearule-basedpolicytocollectthevideodataset. Thepolicykeepsmov-
ingforwarduntilcollidingwithanobstacleandthenrandomlyturnstogetridofit. Simultaneously,
theagentkeepsrecordinganegocentrictraversalvideotillreachingthemaximum900steps,i.e.,a
30-secondvideoclipineachscene. Afterthat,weemployapretrainedobjectdetectorDetic[86]to
detectalltheobjectsoccurredinthevideo. Thecollectedegocentrictraversalvideosanddetected
goalframesareusedforpolicytrainingandevaluation.
Task Evaluation. To evaluate the comprehensive ability of the in-context navigation policy, we
designthreelevelsofgeneralizationtest:
• Evaluateintheunseenlayouttestingsettotestthegeneralizationabilityoverlayouts.
• Evaluateintheunseenroomtestingsettotestthegeneralizationabilityoverrooms.
• Evaluateinscenesfromadifferentsimulatortotestthegeneralizationabilityoverdomains.
Forevaluation,weconsideralldetectedgoalsinavideo,andsample10robotstartpositionsand
orientationsforeachgoal. Eachstart-goalpairbecomesanevaluationtask. Weconduct3runswith
differentrandomseedsandquantitativelyassesstheperformanceusingthefollowingmetrics:
• SuccessRate(SR):Thepercentageoftasksinwhichtheagentreachedthegoalobject.
6• SuccessweightedbynormalizedinversePathLength(SPL)[34]: Measuredbytheaverage
numberofstepstakentoreachthegoalobjectcomparedtotheshortestpossiblepath. Let
S beabinaryindicatorofsuccess,p bethelengthofthepathactuallytakenbytheagent
i i
andℓ betheshortestpathdistancefromtheagent’sstartingpositiontothegoalinepisode
i
i,SPLisgivenasfollows:
N
1 (cid:88) S ℓ i . (8)
N i max(p ,ℓ )
i i
i=1
• TrajectoryLength(TL):Theaveragenumberofstepsforalltrajectories.
• NavigationError(NE):Theaveragedistancebetweenthefinalpositionoftheagenttothe
goalobject.
For visual clarity, we mainly plot the two most important metrics SR and SPL for performance
comparisonandleaveTLandNEinTables1andAppendixD.Andweonlyhighlightthemetricsof
NOLOinallourplots.
Baselines. ItisworthnotingthatNOLOisthefirstmethodtoincorporatecontextvideointopolicy
todoin-contextnavigationtasks. Therefore,wecompareNOLOwithrandompolicytoindicatethat
NOLOcanlearnusefulnavigationpolicyfromcontextvideos. Toprovethein-contextgeneralization
abilityofNOLO,weadditionallycompareNOLOwithtwolargemultimodalmodels(LMM)GPT-4o
and Video-LLaVA [87], which also exhibit remarkable zero-shot multimodal understanding and
generationcapabilities. ToadapttheseLMMstothevideonavigationsetting,aqueueofsizeK is
allocatedformaintainingcontextvideoframes,currentobservation,andgoalimage. Contentsin
thequeueareconcatenatedastheinputofLMMs. WedesignaconsistentprompttoguideLMMs
toadaptthenavigationtask,understandthecontext,andconsequentlyoutputlanguageresponses
bothinRoboTHORandHabitat,andthenextractactionsfromthereceivedlanguageresponses. See
AppendixC.2formoredetails.
4.2 RoboTHOR
NOLO Random GPT-4o Video-LLaVA
Train25 Train35 Train45 Train25 Train35 Train45 Val35 Val11 Val12 Val35 Val11 Val12
Train15 00..660.73 0.6 0.77 0.78Train55 Train15 00..3333 0.29 0. T23 ra0 in.2 28 50.32TrT ara ii nn 355 5 TraVianl343V 5a 0l .3 814 0.640.75 00..7711 0.6 0 0. .8 6V 3 7al1 V3 al14 Val33Val34
0.3
00 .1.2 9900..3311 0.24 00 .2.3 67Val1 V3 al14
Tra Tin r1 a2 in5 110 5.7 06
.76
0.77 0.65 0.66
00 .. 87 T4 raT inr 7ai 5n65 Tra Tin r1 a2 in5 1150 0. .3 3T2 6rai 0n .21 85
0.3
000...2660 80 .2. 2730.3 Tr0 a.T i6 nr 7ai 5n650.7 V7 al V3 a02 l3.70 1. 879 0T .7r 3ai 0n .65 35
0.64
0.760.0 7. 1570.72 ValV 2a 1l15Val V3 a2
l31
0.2 08 .27
0.25
0.25
0.330. 01 .7
31
0.34 ValV 2a 1l15
Train105
Train95
Train85 TrTarianin1120550T.r7ai6n95 Train85 0.7V4al25TrVaainl2645Val23 Val22 Val25
Val24 Val23
Val22
(a)UnseenLayout-SR (b)UnseenLayout-SPL (c)UnseenRoom-SR (d)UnseenRoom-SPL
0.76 0.8
Figure4: AverageSRaTnradin1S15PLinRoboTH0.6O6RseTeranin7ro5omsandunseenrooms.
0.77 0.65
Train105 Train85
WeaveragethemetricsofvideonavigationtaskTsraiinn95alltestingscenesandshowcasethemeanresults
inFigure4. Figure4aand4bshowtheperformanceintheunseenlayouttestingset. Figure4cand
4dshowtheevaluationresultsintheunseenroomtestingset. NOLOdemonstratesoverwhelming
advantagesoverbaselines,includingrandomandLMMbaselinesinbothtestingsets,whichproves
itsstronggeneralizationabilitiestonewscenesindifferentgeneralizationlevels. Consideringan
unseenroomalsomeansanunseenlayout,itisahardertestingsetthantheunseenlayouttestingsetin
termsofthegeneralizationability. Therefore,NOLOdisplaysslightlybetternavigationperformance
in the unseen layout testing set (71.92% SR and 29.26% SPL) than the unseen room testing set
(70.48% SR and 27.74% SPL). GPT-4o outperforms random policy in most cases, while Video-
LLaVAunderperformsrandompolicyinmostcases. ThisisbecausethemodelsizeofVideo-LLaVA
issmallerthanGPT-4o. ThemodelsizeofNOLOisalsosmallerthanGPT-4o,yetitshowsbetter
performanceinthenavigationproblem, whichagaindemonstratestheeffectivenessofNOLOin
trainingin-contextnavigationpolicy. SeeAppendixDfordetailedresults.
4.3 Habitat
7NOLO Random GPT-4o Video-LLaVA We evaluate NOLO similarly in Habi-
gY 5ev RK qG bZ 2 9a 1z cQ Z1 Zb E 7U S6 yF Zw vq gY 5ev RK qG bZ 2 9a 1z cQ Z1 Zb E 7U S6 yF Zw vq tat to demonstrate the effectiveness of
rV pz Dq Efb Ah p eL 7e q4 rw GQ rV pz Dq Efb Ah p eL 7e q4 rw GQ NOLO in larger-scale scenes. As illus-
S sN R8 33 wY 2J 00..55 0.42 0. T7 rainT p2b SH A5J jPru Train35 S TsN R8 r33 awY i2J n45 00..2299 0.39 T pb SH AJ jPru tratedinFigure5and6,NOLOstillout-
g cT VV J8 CF 9G 0.54 0.380.340.25 Train10.3 57 0.52 0.7Ux8 3WF5 yx 9y e 0.6 g cT VV J80CF.9G77 0.29 Tr0a.0 i1. n61 58 05.070.18 0.14 0.23 Ux8 WF5 yx 9y e p ane drfo acrm his evo et she sr lib ga hs tle yli bn ee ts tei rn pte er rm fos rmof anS cR e
U sw G p bV as V4w8 no3 3 At kH yM k
Eqs Vtn hu
m
0.4 010 .3.3 90.3 05 .4T 8r 0a .4i 1n1205.40 500 ... 0 743 . 67 64400.. o66
L gB 9M
in8 5 8N1 Lz B v9s bN 9 4 Lu no HL k4 ZH U sw G p bV as V4w8 no3 3 At kH yM k
Eqs Vtn hu
m0. 07 .8 0 7. 421 00 .. 11 T84 0 r 0. a .1 2i3 2n06.165 0.30 1.220 0. 01 .2.8 2 43
oL gB 9M
in8 5 8N1 Lz B v9s bN 9 4 Lu no HL k4 ZH i r an i gc et ser Som f Ps LNo O af nLS dOPL Si R. nC R ao rom ebp lo oa Tr we H ed O r,w R ait n,h dthth te he eam v aee vrt - --
V Wt j2 Cq FJd 2 0.67 0Q B.U7 BC 56T sXc6 V Wt j20 Cq. FJ8d 2 Q BU BC 5T sXc6 erageNEandTL(SeeAppendixD)are
r Aq ofA iTL qe Z V6M iBFT uQ wrCaiXnZ71NH1oy sM o5h 0.77 0.65 0.66 Tr AqrofaA iTiLnqe7Z5 V6M iBF uQ wCX Z7 NH oy sM oh higher. This is mostly because scenes
Figure5: AverageSTrRain1i0n520 FigTurarine856: Average SPL in in Habitat are significantly larger than
Train95
Habitattestingscenes. 20Habitattestingscenes. thoseinRoboTHOR,resultinginmore
challengingnavigationtasksacrossmul-
tipleroomsandevenfloorsandrequiring
longertraversaldistances. However,weobservethatNOLO,trainedfrom30-secondvideoclips,
is still capable of acquiring many necessary sub-skills like entering or exiting a room, avoiding
obstacles,andexploring. SeeAppendixDfordetailedresults.
4.4 Cross-DomainEvaluation
Topursuetheultimategoalofvideonavigation,weevaluateNOLOacrossRoboTHORandHabitat,
evenifthevisualinputsmaysufferfromaseveredomaingap. Specifically,NOLOtrainedfrom48
RoboTHORscenesisevaluatedin20Habitatscenes(R2H),andNOLOtrainedfrom70Habitat
scenes is evaluated in 27 RoboTHOR scenes (H2R). For comparison, we also plot the average
in-domainresults(R2RandH2H).WeaveragealltheresultsontestingscenesinFigure7,from
whichwesurprisinglydiscoverthatR2HandH2RshowcompetitiveperformancecomparedtoR2R
andH2H.ThisisbecausethepretrainedimageencoderinNOLOprovidesageneralrepresentationof
indoorscenes,andthetemporalcoherenceloss(Equation(7))alsoleadstoageneralrepresentation
reflectingthenaturaltemporalordering,whichtogetherleadtothegeneralizationabilityofNOLO
acrossdomains.
R2R H2R R2R H2R H2H R2H H2H R2H
VV V2V 2 2 V3 45V 3 213 V20 2.V 2 70 600 . 073 10.. V. 67 3 .6 749 33 10 1V .8 01 5.3 6 0 00 V0 7 .5 .. . 754 50 70 10 29 3.0 .V 5 .40 4. 66 4 0 4. 963 4 0 .60 0 . V4 7. .0 07 35 9. .
0
15 30 4 4 0 .. 2 4T 86 .7
3
300 00 41 0.. ..66 55 V. 00 45 .0 0. 8 6 14 .0 .3 58T . 097 4 20.32 50 .0 72 V0 .. 3 1.6 05 6 90 0 1.4 . 0 60.0T 6 5 0. 5..2 651 .6 173 5 T10 6.7 105 7 .T 2700 6.. 0 T064 7 0 5. .58 6. 7 17 6 75 T 4 1T05 .8 51T T5 0T6 T 9 57 85 55 5 VV V2V 2 2 V3 45V 3 213 V20 2.V 2 333 10 V.00 0 30.3 . . 1. 120 2 2 3V 5. 7 5 52 50 0803 0. . V. .3 303 2 0 4.4 52 . 10 23 00V . 4 0.2. 1 42 .3 03 2760 .0 8 V.0 21 . 0. 65 1009 .2
0
14 2..9 .1 02 4 3T 5 .8 74 300 001 .. .. 033 22 0 V0 .33 99 .2.5 22 30 140 01..T 1 .2 28 2009 082 .. .22 3 V63 0 105 0. 0 . 13 0 02 . 0.0T 21 . .2 1 .2 3. 2 372 17 20 03 T28 0. .3 2.2 0 11 95 0 2 . 0T 22. 0 .3 80 3. T024 2. 6 5.3 8 3 15 T 1T5 51T T5 0T6 T 9 57 85 55 5 Uswg G pc bT V aV s VV 4w8J no38 3C AS tkHF ys MN k9 ER G q8 sV3 t3w nhY u2 mJ VWrV 0p tz .D j25q Cq4Ef Fb JA 0h d2.0 4g . 010 5 rY 0 A5 .. q33 .e 0v o3 f98R 0 A.K i0 4 0 T0 .q L. 3 1G .. 2b q33 1 0 e8Z 504 .0 4. 0. 00 2 81 Z... 7 302 8 1 V2 69 9.a 8 M5 00 00 11 i00z B.. .. 8cQ .. 43 43 FZ 55 u18 78 Q1Z w0b C0 . 04. 04 .6 60 X.E 02 02 3.0 Z7
.
7U 4.8 6. 0S 2 NH45 76 .09y 0 01 4 oyF .Z . . 54 s3 M4wv o0807 0 2q h... 43 050 Qp 67 .. B4e 7L U0 47 Be . Cq4 5 5r Tw 2 sG XcQ o 6T Lp gb BS 9HA MinJj Ux 85 8NPr 8 1Lzu W B vF 9s bN9 45y Lu nx o9 HL ky 4e ZH Uswg G pc bT V aV s VV 4w8J no38 3C AS tkHF ys MN k9 ER G q8 sV3 t3w nhY u2 mJ VWrV 0p . tzD 2 j2q C9 qEf 0 Fb J.Ah d2 208 0 .2.g 1 1 rY A5 5 q0e 0v of0 .R . A1K i1. T0 1q 80 L4G . 06b 0 . q1 e10.0Z . 08 11 9.. .140 06 2 Z307 . 202 1 V. 69 00.a 8 M0 00 0 00 01 i8.z B. .. . .. 0c 7Q 2 12 2 12 FZ 4 u9 69 3 53 Q1 0Z w0b .0 0 C. 01. .92 0 3 08 X0E 2 . .0 2 3. Z7 0 0 7U . 10 0 4 11S . N. H6. 0.6 2 24 2y 2 . oyF 6 21Z 21 sM0w 8v o0.1q h.00 280 . Q. 4p 2. 2 B3e 3L U37 9 Be Cq4 5r Tw sG XcQ o 6T Lp gb BS 9HA MinJj Ux 85 8NPr 8 1Lzu W B vF 9s bN9 45y Lu nx o9 HL ky 4e ZH
(a)H2R-SR (b)H2R-SPL (c)R2H-SR (d)R2H-SPL
Figure7: AverageSRandSPLofcrossdomainevaluationresults.
4.5 Ablations
Table1: AblationstudiesofNOLO.Foreachmethod,weaveragethemetricsoveralltestingscenes
andaverageallthemeanresultsoverthreerandomseedstoreportthemeanandstandarddeviation.
Robothor Habitat
Method
SR(%)↑ SPL(%)↑ TL↓ NE↓ SR(%)↑ SPL(%)↑ TL↓ NE↓
NOLO 71.12±0.85 28.41±0.72 244.21±3.80 1.84±0.04 43.65±1.09 20.77±0.46 347.57±3.32 5.17±0.09
NOLO-C 67.78±0.60 27.70±1.35 259.65±0.53 1.98±0.02 33.58±0.92 17.41±0.43 381.26±3.23 6.58±0.26
NOLO-T 69.03±0.87 28.15±0.09 252.84±6.33 1.93±0.04 37.48±1.01 18.51±0.71 369.85±2.72 6.70±0.07
NOLO(M) 66.15±0.37 26.66±1.61 267.95±1.96 2.02±0.00 36.73±1.89 18.17±0.31 373.92±3.46 6.69±0.25
Inthissection,weuseasetofablationstudiestoexaminethecontributionofseveralkeycomponents
inNOLO.Morespecifically,weaimtoanswerthefollowingquestions.
8STG STG-
goal frame saliency with goal saliency without goal
STG STG-
goal frame saliency with goal saliency without goal
NOLO NOLO-T
NOLO NOLO-T
(a)Visualizationofsaliencymapsaboutcontextframes (b)Visualizationofcontextembeddingsequencesofa
withrespecttogoalframesinRoboTHORandHabitat. sampledvideofromRoboTHOR.
Figure8: WecomparethesaliencymapsbetweenNOLOandNOLO-Candcontextembedding
sequencesbetweenNOLOandNOLO-T.
Does the context video provide cues for navigation? Yes. We conduct an ablation study by
maskingthecontext. WedenotetheablationvariantasNOLO-C,whichisevaluatedinRoboTHOR
and Habitat with experimental configurations unchanged. We can see from Table 1 that a mean
performancedropofNOLO-CisperceivedbothinRoboTHORandHabitatcomparedwithNOLO
duetolackofcontextinformation.
Does temporal coherence loss contribute to better visual representation and performance?
Yes. Weexaminetheeffectofthetemporalcoherenceloss(Equation(7))inimprovingperformance
andrepresentation. Anablationvariant,namedNOLO-T,isconductedbyremovingthetemporal
coherenceloss.TheresultsareshowninTable1.TheperformancedropsslightlyinRoboTHORwhile
moreobviouslyinHabitat. Tofigureouttheunderlyingreasonsfortheirdiscrepancyinperformance,
wefurthercomparethelearnedvisualrepresentationofNOLOandNOLO-T.Werandomlyselecta
contextvideoinRoboTHORandutilizet-SNEprojectiontovisualizetheirembeddingsequences
encoded by E (f ) in Figure 8b. The embeddings learned by NOLO-T exhibit more irregular
ζ t
stochasticity. In contrast, NOLO learns a better-structured embedding, which demonstrates the
effectivenessofthetemporalcoherenceloss.DetailedresultsrefertoAppendixD.
CanNOLOincorporateotheroff-the-shelfmodulestodecodeactionsfromvideos? Yes. Key
pointsmatchingmethodslikeSuperGlue[88]canalsobemodifiedtoextractactionsfromadjacent
video frames. We follow a similar pipeline to decode actions via matching key points between
adjacentvideoframesandthenconductBCQ.Thenewbaseline,entitledNOLO(M),isevaluated
underthesamesettingsasNOLO.Byreferringtotrueactionlabels,wemeasuretheaverageaccuracy
ofactiondecodingofNOLO(M)andNOLO(80.43%vs92.44%),whichmeanstheactionsdecoded
fromkeypointmatchingarelessaccuratethanGMFlow. FromTable1,NOLO(M)exhibitsaround
5%decayinaveragesuccessrateinRoboTHORand7%inHabitat. Therefore, toachievemore
satisfactoryin-contextgeneralization,wechooseGMFlowastheactiondecoderforNOLO.However,
asatwo-stagegeneralframeworkforin-contextlearningfromvideos,NOLOisnotlimitedtoany
kindofpseudoactionlabelingmethodsandiscapableofincorporatingmoreadvancedmodulesfor
betterperformance.
DoesNOLOunderstandtherelationbetweengoalandcontextvideo? Yes. Todemonstratethe
relationbetweengoalandcontextvideolearnedinNOLO,wevisualizetheGrad-CAM[89]saliency
mapofthevisualencodertoshowcasetheeffectofgoalonvideocontext. AsdemonstratedinFigure
8a,whenthegoalimageisgiven,NOLOpreciselycapturestheimportantsemanticcuesinthegoal
imageandwiselypaysmoreattentiontotherelatedregionsinthecontextframe,whichmeansit
buildsconnectionsbetweengoalandcontextvideo. Onthecontrary,whenthegoalimageisnot
given,NOLOcannotconcentrateontheobjectspecifiedinthegoalimageduetoalackofcontext
guidance. SucharesultindicatesthatNOLOcanunderstandthegoalimageandcapturerelevant
informationinthecontextvideo.
5 ConclusionandLimitaiton
Inthispaper,weproposeandinvestigatethevideonavigationproblem,wherethegoalistotrain
anin-contextnavigationpolicytofindobjectsoccurredinthecontextvideo,andthetrainingofthis
9
ROBOTHOR
HABITAT
ROBOTHOR
HABITATpolicyhastobedoneinanofflinemannerpurelyfromvideos. Tosolvethisproblem,wepropose
ouralgorithm,NOLO,totrainageneralizablein-contextnavigationpolicytakingacontextvideoas
input,suchthatitcangeneralizetoanewscenewithoutfinetuningorre-trainingbyunderstanding
thecontextvideoofthenewscene. InNOLO,wefirstextractpseudoactionsfromadjacentvideo
frames using optical flow, and then use an offline reinforcement learning method, BCQ, to train
thein-contextnavigationpolicy. Wealsoproposeatemporalcoherencelosstolearnrepresentation
consistentwiththenaturaltemporalordering. WeconductexperimentsonRobothorandHabitat
anddemonstratethegeneralizationabilityofNOLOindifferentlevels. Wealsoverifytheeffectof
severalkeycomponentsofNOLOinablationstudies,whichleadstoabetterunderstandingofhow
NOLOworks.
ItisimportanttoacknowledgethelimitationsofNOLO,whichcurrentlycanonlyaccuratelyextract
movementactionsfromtwoadjacentframes. Methodsfordecodingmorecomplicatedactionsfrom
multiplevideoframesareworthexploring. Additionally,infuturework,weplantopretrainNOLO
onlarger-scalescenedatasetstoverifyitsin-contextgeneralizationabilityacrossmorediversescenes.
References
[1] YicongHong,QiWu,YuankaiQi,CristianRodriguez-Opazo,andStephenGould. Vlnbert:Arecurrent
vision-and-languagebertfornavigation. InProceedingsoftheIEEE/CVFconferenceonComputerVision
andPatternRecognition(CVPR),pages1643–1653,2021.
[2] ChangChen,YuechengLiu,YuzhengZhuang,SitongMao,KaiwenXue,andShunboZhou. Scale:Self-
correctingvisualnavigationformobilerobotsviaanti-noveltyestimation.arXivpreprintarXiv:2404.10675,
2024.
[3] Dong Hu, Chao Huang, Jingda Wu, and Hongbo Gao. Pre-trained transformer-enabled strategies
with human-guided fine-tuning for end-to-end navigation of autonomous vehicles. arXiv preprint
arXiv:2402.12666,2024.
[4] BernadettaMustikaDewi, MayaArliniPuspasari, BillyMuhamadIqbal, andTegarSeptyanHidayat.
Developingvirtualassistantinavirtualretailstoreenvironmentforproductsearcheffectiveness. InAIP
ConferenceProceedings,2024.
[5] DevendraSinghChaplot, DhirajGandhi, SaurabhGupta, AbhinavGupta, andRuslanSalakhutdinov.
Learningtoexploreusingactiveneuralslam. arXivpreprintarXiv:2004.05155,2020.
[6] DevendraSinghChaplot,RuslanSalakhutdinov,AbhinavGupta,andSaurabhGupta. Neuraltopological
slamforvisualnavigation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),2020.
[7] MatthewChang, TheophileGervet, MukulKhanna, SriramYenamandra, DhruvShah, SoYeonMin,
KavitShah,ChrisPaxton,SaurabhGupta,DhruvBatra,etal. Goat: Gotoanything. arXivpreprint
arXiv:2311.06430,2023.
[8] KarmeshYadav,RamRamrakhya,ArjunMajumdar,Vincent-PierreBerges,SachitKuhar,DhruvBatra,
AlexeiBaevski,andOleksandrMaksymets. Offlinevisualrepresentationlearningforembodiednavigation.
InWorkshoponReincarnatingReinforcementLearningatICLR2023,2023.
[9] JacobKrantz,TheophileGervet,KarmeshYadav,AustinWang,ChrisPaxton,RoozbehMottaghi,Dhruv
Batra,JitendraMalik,StefanLee,andDevendraSinghChaplot. Navigatingtoobjectsspecifiedbyimages.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(CVPR),2023.
[10] DhruvShah,AjaySridhar,NitishDashora,KyleStachowicz,KevinBlack,NoriakiHirose,andSergey
Levine. Vint:Afoundationmodelforvisualnavigation. InConferenceonRobotLearning,pages711–733.
PMLR,2023.
[11] Valay Bundele, Mahesh Bhupati, Biplab Banerjee, and Aditya Grover. Scaling vision-and-language
navigationwithofflinerl. arXivpreprintarXiv:2403.18454,2024.
[12] TheophileGervet,SoumithChintala,DhruvBatra,JitendraMalik,andDevendraSinghChaplot.Navigating
toobjectsintherealworld. ScienceRobotics,8(79):eadf6991,2023.
[13] Ruslan Partsey, Erik Wijmans, Naoki Yokoyama, Oles Dobosevych, Dhruv Batra, and Oleksandr
Maksymets. Ismappingnecessaryforrealisticpointgoalnavigation? InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),2022.
[14] MeeraHahn,DevendraSinghChaplot,ShubhamTulsiani,MustafaMukadam,JamesMRehg,andAbhinav
Gupta. Norl,nosimulation:Learningtonavigatewithoutnavigating. InNeuralInformationProcessing
Systems(NeurIPS),2021.
10[15] PinLyu,BingqingWang,JizhouLai,ShiyuBai,MingLiu,andWenbinYu. Afactorgraphoptimization
method for high-precision imu based navigation system. IEEE Transactions on Instrumentation and
Measurement,2023.
[16] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,and
ZhifangSui. Asurveyonin-contextlearning. arXivpreprintarXiv:2301.00234,2022.
[17] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
[18] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,JiakaiTang,
XuChen,YankaiLin,etal. Asurveyonlargelanguagemodelbasedautonomousagents. Frontiersof
ComputerScience,18(6):1–26,2024.
[19] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald,
DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement learning
withalgorithmdistillation. arXivpreprintarXiv:2210.14215,2022.
[20] JonathanLee,AnnieXie,AldoPacchiano,YashChandak,ChelseaFinn,OfirNachum,andEmmaBrunskill.
Supervisedpretrainingcanlearnin-contextreinforcementlearning. InNeuralInformationProcessing
Systems(NeurIPS),2024.
[21] Hao Liu and Pieter Abbeel. Emergent agentic transformer from chain of hindsight experience. In
InternationalConferenceonMachineLearning,2023.
[22] Valay Bundele, Mahesh Bhupati, Biplab Banerjee, and Aditya Grover. Scaling vision-and-language
navigationwithofflinerl. arXivpreprintarXiv:2403.18454,2024.
[23] Pierre-LouisGuhur,MakarandTapaswi,ShizheChen,IvanLaptev,andCordeliaSchmid. Airbert: In-
domainpretrainingforvision-and-languagenavigation. InProceedingsoftheIEEE/CVFconferenceon
ComputerVisionandPatternRecognition(CVPR),2021.
[24] JiazhaoZhang,KunyuWang,RongtaoXu,GengzeZhou,YicongHong,XiaomengFang,QiWu,Zhizheng
Zhang,andWangHe. Navid: Video-basedvlmplansthenextstepforvision-and-languagenavigation.
arXivpreprintarXiv:2402.15852,2024.
[25] JialuLi,AishwaryaPadmakumar,GauravSukhatme,andMohitBansal. Vln-video: Utilizingdriving
videosforoutdoorvision-and-languagenavigation. arXivpreprintarXiv:2402.03561,2024.
[26] DhruvShah,Błaz˙ejOsin´ski,SergeyLevine,etal. Lm-nav: Roboticnavigationwithlargepre-trained
modelsoflanguage,vision,andaction. InConferenceonRobotLearning(CORL),2023.
[27] ApoorvKhandelwal,LucaWeihs,RoozbehMottaghi,andAniruddhaKembhavi. Simplebuteffective:
Clipembeddingsforembodiedai. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pages14829–14838,2022.
[28] LinaMezghan, SainbayarSukhbaatar, ThibautLavril, OleksandrMaksymets, DhruvBatra, PiotrBo-
janowski,andKarteekAlahari. Memory-augmentedreinforcementlearningforimage-goalnavigation. In
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),2022.
[29] JacobKrantz,StefanLee,JitendraMalik,DhruvBatra,andDevendraSinghChaplot. Instance-specific
imagegoalnavigation:Trainingembodiedagentstofindobjectinstances.arXivpreprintarXiv:2211.15876,
2022.
[30] XiaohanLei,MinWang,WengangZhou,LiLi,andHouqiangLi. Instance-awareexploration-verification-
exploitationforinstanceimagegoalnavigation. arXivpreprintarXiv:2402.17587,2024.
[31] SanthoshKumarRamakrishnan,DevendraSinghChaplot,ZiadAl-Halah,JitendraMalik,andKristen
Grauman.Poni:Potentialfunctionsforobjectgoalnavigationwithinteraction-freelearning.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),2022.
[32] Karmesh Yadav, Arjun Majumdar, Ram Ramrakhya, Naoki Yokoyama, Alexei Baevski, Zsolt Kira,
Oleksandr Maksymets, and Dhruv Batra. Ovrl-v2: A simple state-of-art baseline for imagenav and
objectnav. arXivpreprintarXiv:2303.07798,2023.
[33] JiazhaoZhang,KunyuWang,RongtaoXu,GengzeZhou,YicongHong,XiaomengFang,QiWu,Zhizheng
Zhang,andWangHe. Navid: Video-basedvlmplansthenextstepforvision-and-languagenavigation.
arXivpreprintarXiv:2402.15852,2024.
[34] PeterAnderson,AngelChang,DevendraSinghChaplot,AlexeyDosovitskiy,SaurabhGupta,Vladlen
Koltun,JanaKosecka,JitendraMalik,RoozbehMottaghi,ManolisSavva,etal.Onevaluationofembodied
navigationagents. arXivpreprintarXiv:1807.06757,2018.
[35] YukeZhu,RoozbehMottaghi,EricKolve,JosephJLim,AbhinavGupta,LiFei-Fei,andAliFarhadi.
Target-driven visual navigation in indoor scenes using deep reinforcement learning. In 2017 IEEE
internationalconferenceonroboticsandautomation(ICRA),2017.
11[36] XinyuSun,PeihaoChen,JugangFan,JianChen,ThomasLi,andMingkuiTan. Fgprompt:Fine-grained
goalpromptingforimage-goalnavigation. InNeuralInformationProcessingSystems(NeurIPS),2024.
[37] ZiadAl-Halah,SanthoshKumarRamakrishnan,andKristenGrauman. Zeroexperiencerequired:Plug&
playmodulartransferlearningforsemanticvisualnavigation.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),2022.
[38] MatthewChang,ArjunGupta,andSaurabhGupta.Semanticvisualnavigationbywatchingyoutubevideos.
InNeuralInformationProcessingSystems(NeurIPS),2020.
[39] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama:Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023.
[40] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,
PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm: Scalinglanguage
modelingwithpathways. JournalofMachineLearningResearch,24(240):1–113,2023.
[41] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelfor
few-shotlearning. Advancesinneuralinformationprocessingsystems,35:23716–23736,2022.
[42] MariaTsimpoukelli,JacobLMenick,SerkanCabi,SMEslami,OriolVinyals,andFelixHill. Multimodal
few-shotlearningwithfrozenlanguagemodels. AdvancesinNeuralInformationProcessingSystems,
34:200–212,2021.
[43] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXiv
preprintarXiv:2303.08774,2023.
[44] QianHuang,HongyuRen,PengChen,GregorKržmanc,DanielZeng,PercySLiang,andJureLeskovec.
Prodigy:Enablingin-contextlearningovergraphs. AdvancesinNeuralInformationProcessingSystems,
36,2024.
[45] ZhongbinFang,XiangtaiLi,XiaLi,JoachimMBuhmann,ChenChangeLoy,andMengyuanLiu.Explore
in-contextlearningfor3dpointcloudunderstanding. AdvancesinNeuralInformationProcessingSystems,
36,2024.
[46] YongliangShen,KaitaoSong,XuTan,DongshengLi,WeimingLu,andYuetingZhuang. Hugginggpt:
Solvingaitaskswithchatgptanditsfriendsinhuggingface. AdvancesinNeuralInformationProcessing
Systems,36,2024.
[47] SiruiHong,XiawuZheng,JonathanChen,YuhengCheng,JinlinWang,CeyaoZhang,ZiliWang,Steven
KaShingYau,ZijuanLin,LiyangZhou,etal. Metagpt:Metaprogrammingformulti-agentcollaborative
framework. arXivpreprintarXiv:2308.00352,2023.
[48] JakeGrigsby,LinxiFan,andYukeZhu. Amago:Scalablein-contextreinforcementlearningforadaptive
agents. arXivpreprintarXiv:2310.09971,2023.
[49] DeyaoZhu,YuhuiWang,JürgenSchmidhuber,andMohamedElhoseiny. Guidingonlinereinforcement
learningwithaction-freeofflinepretraining. arXivpreprintarXiv:2301.12876,2023.
[50] AlejandroEscontrela,AdemiAdeniji,WilsonYan,AjayJain,XueBinPeng,KenGoldberg,Youngwoon
Lee,DanijarHafner,andPieterAbbeel. Videopredictionmodelsasrewardsforreinforcementlearning.
arXivpreprintarXiv:2305.14343,2023.
[51] BohanZhou, KeLi, JiechuanJiang, andZongqingLu. Learningfromvisualobservationviaoffline
pretrainedstate-to-gotransformer. InNeuralInformationProcessingSystems(NeurIPS),2024.
[52] PierreSermanet,CoreyLynch,YevgenChebotar,JasmineHsu,EricJang,StefanSchaal,SergeyLevine,
andGoogleBrain. Time-contrastivenetworks:Self-supervisedlearningfromvideo. InIEEEInternational
ConferenceonRoboticsandAutomation(ICRA),2018.
[53] YusufAytar,TobiasPfaff,DavidBudden,ThomasPaine,ZiyuWang,andNandoDeFreitas. Playinghard
explorationgamesbywatchingyoutube. InNeuralInformationProcessingSystems(NeurIPS),2018.
[54] JakeBruce,AnkitAnand,BogdanMazoure,andRobFergus. Learningaboutprogressfromexperts. In
InternationalConferenceonLearningRepresentations(ICLR),2023.
[55] MichaelLaskin,AravindSrinivas,andPieterAbbeel. Curl:Contrastiveunsupervisedrepresentationsfor
reinforcementlearning. InInternationalConferenceonMachineLearning(ICML),2020.
[56] AdamStooke,KiminLee,PieterAbbeel,andMichaelLaskin. Decouplingrepresentationlearningfrom
reinforcementlearning. InInternationalConferenceonMachineLearning(ICML),2021.
[57] XueBinPeng,ZeMa,PieterAbbeel,SergeyLevine,andAngjooKanazawa. Amp:Adversarialmotion
priorsforstylizedphysics-basedcharactercontrol. ACMTransactionsonGraphics(ToG),40(4):1–20,
2021.
12[58] FarazTorabi,GarrettWarnell,andPeterStone.Imitationlearningfromvideobyleveragingproprioception.
InInternationalJointConferenceonArtificialIntelligence(IJCAI),2019.
[59] HareshKarnan,GarrettWarnell,FarazTorabi,andPeterStone. Adversarialimitationlearningfromvideo
usingastateobserver. InInternationalConferenceonRoboticsandAutomation(ICRA),2022.
[60] MinghuanLiu,TairanHe,WeinanZhang,ShuichengYan,andZhongwenXu. Visualimitationlearning
withpatchrewards. arXivpreprintarXiv:2302.00965,2023.
[61] MaksimBobrin,NazarBuzun,DmitriiKrylov,andDmitryVDylov. Alignyourintents:Offlineimitation
learningviaoptimaltransport. arXivpreprintarXiv:2402.13037,2024.
[62] SiyuanLi,ShijieHan,YingnanZhao,ByLiang,andPengLiu.Auxiliaryrewardgenerationwithtransition
distancerepresentationlearning. arXivpreprintarXiv:2402.07412,2024.
[63] SeohongPark,TobiasKreiman,andSergeyLevine.Foundationpolicieswithhilbertrepresentations.arXiv
preprintarXiv:2402.15567,2024.
[64] BenjaminEysenbach,TianjunZhang,SergeyLevine,andRussRSalakhutdinov. Contrastivelearningas
goal-conditionedreinforcementlearning. InNeuralInformationProcessingSystems(NeurIPS),2022.
[65] YechengJasonMa,ShagunSodhani,DineshJayaraman,OsbertBastani,VikashKumar,andAmyZhang.
Vip:Towardsuniversalvisualrewardandrepresentationviavalue-implicitpre-training. arXivpreprint
arXiv:2210.00030,2022.
[66] DibyaGhosh,ChethanAnandBhateja,andSergeyLevine. Reinforcementlearningfrompassivedatavia
latentintentions. InInternationalConferenceonMachineLearning(ICML).PMLR,2023.
[67] ChethanBhateja,DerekGuo,DibyaGhosh,AnikaitSingh,MananTomar,QuanVuong,YevgenChebotar,
SergeyLevine,andAviralKumar. Roboticofflinerlfrominternetvideosviavalue-functionpre-training.
arXivpreprintarXiv:2309.13041,2023.
[68] WeiruiYe,YunshengZhang,PieterAbbeel,andYangGao. Becomeaproficientplayerwithlimiteddata
throughwatchingpurevideos. InInternationalConferenceonLearningRepresentations(ICLR),2022.
[69] YounggyoSeo,KiminLee,StephenLJames,andPieterAbbeel. Reinforcementlearningwithaction-free
pre-trainingfromvideos. InInternationalConferenceonMachineLearning(ICML),2022.
[70] JialongWu,HaoyuMa,ChaoyiDeng,andMingshengLong. Pre-trainingcontextualizedworldmodels
within-the-wildvideosforreinforcementlearning. InNeuralInformationProcessingSystems(NeurIPS),
2024.
[71] AshleyEdwards,HimanshuSahni,YannickSchroecker,andCharlesIsbell. Imitatinglatentpoliciesfrom
observation. InInternationalConferenceonMachineLearning(ICML),2019.
[72] MinghuanLiu,ZhengbangZhu,YuzhengZhuang,WeinanZhang,JianyeHao,YongYu,andJunWang.
Planyourtargetandlearnyourskills: Transferablestate-onlyimitationlearningviadecoupledpolicy
optimization. arXivpreprintarXiv:2203.02214,2022.
[73] DominikSchmidtandMinqiJiang. Learningtoactwithoutactions. arXivpreprintarXiv:2312.10812,
2023.
[74] JakeBruce,MichaelDennis,AshleyEdwards,JackParker-Holder,YugeShi,EdwardHughes,Matthew
Lai,AditiMavalankar,RichieSteigerwald,ChrisApps,etal. Genie:Generativeinteractiveenvironments.
arXivpreprintarXiv:2402.15391,2024.
[75] Po-ChenKo,JiayuanMao,YilunDu,Shao-HuaSun,andJoshuaBTenenbaum. Learningtoactfrom
actionlessvideosthroughdensecorrespondences. arXivpreprintarXiv:2310.08576,2023.
[76] NathanGavenski,JuarezMonteiro,RogerGranada,FelipeMeneguzzi,andRodrigoCBarros. Imitating
unknownpoliciesviaexploration. arXivpreprintarXiv:2008.05660,2020.
[77] HaofeiXu,JingZhang,JianfeiCai,HamidRezatofighi,andDachengTao. Gmflow: Learningoptical
flowviaglobalmatching. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition(CVPR),2022.
[78] RamRamrakhya,EricUndersander,DhruvBatra,andAbhishekDas. Habitat-web:Learningembodied
object-searchstrategiesfromhumandemonstrationsatscale. InProceedingsoftheIEEE/CVFconference
onComputerVisionandPatternRecognition(CVPR),2022.
[79] BoleiZhou,AgataLapedriza,AdityaKhosla,AudeOliva,andAntonioTorralba. Places:A10million
imagedatabaseforscenerecognition. IEEETransactionsonPatternAnalysisandMachineIntelligence
(TPAMI),2017.
[80] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalConferenceonMachineLearning(ICML),2021.
13[81] ScottFujimoto,DavidMeger,andDoinaPrecup. Off-policydeepreinforcementlearningwithoutexplo-
ration. InInternationalConferenceonMachineLearning(ICML),2019.
[82] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollowinstructionswith
humanfeedback. InNeuralInformationProcessingSystems(NeurIPS),2022.
[83] MattDeitke,WinsonHan,AlvaroHerrasti,AniruddhaKembhavi,EricKolve,RoozbehMottaghi,Jordi
Salvador,DustinSchwenk,EliVanderBilt,MatthewWallingford,etal. Robothor:Anopensimulation-to-
realembodiedaiplatform. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition(CVPR),pages3164–3174,2020.
[84] ManolisSavva,AbhishekKadian,OleksandrMaksymets,YiliZhao,ErikWijmans,BhavanaJain,Julian
Straub,JiaLiu,VladlenKoltun,JitendraMalik,etal. Habitat:Aplatformforembodiedairesearch. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(CVPR),2019.
[85] AngelChang,AngelaDai,ThomasFunkhouser,MaciejHalber,MatthiasNiessner,ManolisSavva,Shuran
Song,AndyZeng,andYindaZhang. Matterport3d: Learningfromrgb-ddatainindoorenvironments.
InternationalConferenceon3DVision(3DV),2017.
[86] XingyiZhou,RohitGirdhar,ArmandJoulin,PhilippKrähenbühl,andIshanMisra. Detectingtwenty-
thousandclassesusingimage-levelsupervision. InEuropeanConferenceonComputerVision(ECCV),
2022.
[87] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunitedvisual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[88] Paul-EdouardSarlin,DanielDeTone,TomaszMalisiewicz,andAndrewRabinovich. Superglue:Learning
featurematchingwithgraphneuralnetworks. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition(CVPR),2020.
[89] RamprasaathRSelvaraju,MichaelCogswell,AbhishekDas,RamakrishnaVedantam,DeviParikh,and
DhruvBatra. Grad-cam: Visualexplanationsfromdeepnetworksviagradient-basedlocalization. In
InternationalConferenceonComputerVision(ICCV),2017.
[90] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert:Pre-trainingofdeepbidirec-
tionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
[91] HaoTanandMohitBansal. Lxmert:Learningcross-modalityencoderrepresentationsfromtransformers.
arXivpreprintarXiv:1908.07490,2019.
[92] WeituoHao,ChunyuanLi,XiujunLi,LawrenceCarin,andJianfengGao. Towardslearningageneric
agentforvision-and-languagenavigationviapre-training. InProceedingsoftheIEEE/CVFconferenceon
ComputerVisionandPatternRecognition(CVPR),2020.
[93] EricKolve,RoozbehMottaghi,WinsonHan,EliVanderBilt,LucaWeihs,AlvaroHerrasti,MattDeitke,
KianaEhsani,DanielGordon,YukeZhu,etal. Ai2-thor: Aninteractive3denvironmentforvisualai.
arXivpreprintarXiv:1712.05474,2017.
14A ModelArchitecture
WedeployResNet18pretrainedfromPlaces368[79]asvisualEncoderandfrozenResNet50-based
Clip visual encoder as goal image encoder. In VN⟲Bert, the self-attention modules consist of 9
Bert[90]layersandthecross-attentionmodulesconsistof4LXMERT[91]layers. Thestructureof
VN⟲BertismodifiedfromPrevalent1[92]. Theself-attentionmodulesandcross-attentionmodules
are also initialized with the checkpoints of Prevalent. Our codes and documents are included in
supplementarymaterials.
B MethodologyDetails
B.1 PseudoActionLabeling
We deploy optical flow and filter dominant vectors with the help of the flow map. Theoretically,
discretizingfilteredvectorsintoseveralactioncategoriescanbeachievedbyanyunsupervisedpattern
recognitionmethods,suchasclustering. Inpractice,weuseasimplethreshold-basedheuristic. We
observethatthefeatureofmovingforwardislessprominentthanturningleftorright. Consequently,
we focus more on the horizontal displacement of the pixels in the flow map. If the horizontal
componentofhorizontaldisplacementexceedsathresholdτ andtheverticalcomponentstayswithin
x
athresholdτ thenweclassifyitasaturningaction,orotherwiseitisconsideredasaforwarding
y
action.
B.2 PolicyInference
Wechoosethethresholdofchoosingactionβ = 0.5.AfterthelossofBCQconverges,wemake
decisionsbasedonthelearnedpolicyandQ-values. Duringinference,anactionischosenbasedon
theQ-valuesweightedbymaskedactiondistributionπ
(cid:0)
a |g,o
,Ti(cid:1)
withprobabilityε = 0.999
θ t t
andotherwiserandomly. Besides,aslongasP outputsastrongsignalfortermination,theagent
D
willdirectlyoutputactionSTOP.
B.3 SemanticActions
Toensuremorestablenavigation,theVN⟲Bertiselaboratelydesignedtooutput"semanticactions",
i.e. actionsfromtheactionspacealongwiththeirduration. Forexample,(MoveForward,3)means
repeatedlymovingforwardfor3timesteps. STOPcanexecutedatmostonce. Wefindthatlimiting
eachactionupto3timesatmosteffectivelyimprovestherobustnessofthelearnedpolicyandthefinal
performance. Previousmethods[14,38]tendtoadoptamorecomplicatedhierarchicalframework
consisting of a high-level policy to propose sub-goals from a constructed topological map and a
low-levelpolicytodecomposeseveralactionsintheactionspacetoreacheachsub-goal. Instead,
weadoptasimplerbutmorepracticalmid-levelpolicytoregularizethenavigationbehavior. From
anotherperspective,weenlargetheoriginalactionspacetoanextendedonewith10newactions.
C ExperimentDetails
C.1 DatasetDivision
Derived from Ai2THOR2 [93], valid scenes in RoboTHOR3 are in the form Floor-
Plan_Train{1:12}_{1:5} or FloorPlan_Val{1:3}_{1:5}. We use Habitat4 simulator and scene
datasets according to https://github.com/facebookresearch/habitat-sim/blob/main/
DATASETS.md. The list of all scene names can refer to https://kaldir.vc.in.tum.de/
matterport/v1/scans.txt. See Table 2 for training and testing scene splits. We include de-
tailedstepsintheREADME.mdcontainedinthesupplementarymaterialswhichindicatehowtorun
scriptstocreatevideodatasetsandformtestingpoints.
1https://github.com/YicongHong/Recurrent-VLN-BERT,MiTLicense.
2https://github.com/allenai/ai2thor,ApacheLicense.
3https://ai2thor.allenai.org/robothor
4https://github.com/facebookresearch/habitat-sim,MITLicense
15Table2: DatasetdivisionofRoboTHORandHabitat
Train
S9hNv5qa7GM XcA2TqTSSAj PX4nDJXEHrG 5LpN3gDmAk7 WYY7iVyf5p8
5q7pvUzZiYa VFuaQ6m2Qom EDJbREhghzL rPc6DW4iMge 2n8kARJN3HM
HxpKQynjfin PuKPg4mmafe 759xd9YjKW5 aayBHfsNo7d jtcxE69GiFV
B6ByNegPMKs p5wJjkQkbXX D7N2EKCX4Sj 1LXtFkjw3qL YVUC4YcDtcY
YmJkqBEsHnH 7y3sRwLe3Va r47D5H71a5s vyrNrziPKCB gxdoqLR6rwA
1pXnuDYAj8r VLzqgDo317F kEZ7cmS4wCh sT4fr6TAbpF ARNzJeq3xxb
Vvot9Ly1tCj cV4RVeZvu5T VVfe2KiqLaN jh4fc5c5qoQ 2t7WUuJeko7
D7G3Y4RVNrH uNb9QFRL6hY V2XKFyX4ASd e9zR4mvMWw7 RPmz2sHmrrY
E9uDoFAP3SH 17DRP5sb8fy JeFG25nYj2p mJXqzFtmKg4 YFuZgdQ5vWj
ac26ZMwG7aT JF19kD82Mey Pm6F8kyY3z2 sKLMLpTHeUy wc2JMjhGNzB
dhjEzFoUFzH 82sE5b5pLXE gZ6f7yhEvPG 8WUmhLawc2A 5ZKStnWn8Zo
pRbA3pwrgk9 29hnd4uzFmX GdvgFV5R1Z5 ULsKaCPVFJR q9vSo1VnCiC
Uxmj2M2itWa qoiz87JEwZ2 s8pcmisQ38h ur6pFq6Qu1A fzynW3qQPVF
ZMojNkEp431 r1Q1Z4BcV1o b8cTxDM8gDG i5noydFURQK JmbYfDe2QKZ
Eval
2azQ1b91cZZ x8F5xyUWy9e QUCTc6BB5sX Vt2qJdWjCF2 gTV8FGcVJC9
EU6Fwq7SyZv zsNo4HB9uLZ X7HyMhZNoso yqstnuAEVhm SN83YJsR3w2
pLe4wQe7qrG 8194nk5LbLH Z6MFQCViBuw pa4otMbVnkk VzqfbhrpDEA
TbHJrupSAjP oLBMNvg9in8 rqfALeAoiTq UwV83HsGsw3 gYvKGZ5eRqb
C.2 LMMBaselines
ForbothGPT-4oandVideo-LLaVA,wemaintainaqueueofsizeK =16toincorporatesampled
context frames, current observation, and goal image into a video clip and designed a consistent
prompttofacilitatetheirunderstandingofthevideocontentandtoguidethemtoprovideareasonable
actionateachtimestep. Thepromptisstructuredasfollows:
These frames pertain to a navigation task. The goal of the
video is represented by the last frame, and the second-to-last
frame is the current observation. Choose the best action
from [move forward, turn left, turn right, stop] that will
help achieve the goal depicted in the last frame of the video.
Please provide the selected action directly.
Wethenutilizeregularexpressionstoextractactionsfromthereceivedlanguageresponses. Incase
ofinvalidoutput,weattemptuptothreeretries. Ifallattemptsfail,arandomactionisthenselected
fromtheactionspaceasoutput.
C.3 Hyperparameters
Table3outlinesthehyperparametersforVN⟲Berttraining.
C.4 ComputationResources
ForbothRoboTHORandHabitat,wetrainNOLOandallNOLO-variantsonasingle40GA100
GPUforaround1day. WeevaluateNOLOacrossallcreatedtestingtasks(singlethread)ona24G
NvidiaRTX4090TiGPUforabout6hoursinRoboTHORand5hoursinHabitat.
D EvaluationResults
TablesbelowoutlinetheaverageperformanceofdifferentmethodsinRoboTHORandHabitatas
wellascrossevaluationresultsover3randomseeds.
16Table3: HyperparametersforVN⟲BertTraining
Hyperparameter Value
Optimizer AdamW
LearningRate 1e-4
Betas (0.9,0.95)
WeightDecay 0.1
Batchsize 26
SALayers 9
CALayers 4
ActionEmbeddingDimension 256
VisualEmbeddingDimension 512
HiddenStateDimension 768
BCQClipThreshold 0.5
TypeofGPUs A100,orNvidiaRTX4090Ti
D.1 RoboTHORVideoNavigationPerformance
Table4: NOLOPerformanceonRoboTHORTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
FloorPlan_Train10_5 77.45±4.36 28.21±1.58 216.35±14.21 1.68±0.13
FloorPlan_Train11_5 76.44±3.82 36.19±2.32 218.66±13.06 1.62±0.16
FloorPlan_Train12_5 76.27±3.54 31.89±0.94 228.36±4.96 1.50±0.09
FloorPlan_Train1_5 59.72±2.39 33.15±1.72 260.69±9.97 2.06±0.05
FloorPlan_Train2_5 72.54±0.81 28.83±1.45 231.64±4.56 1.76±0.11
FloorPlan_Train3_5 60.20±1.21 23.29±0.21 289.70±6.56 2.34±0.04
FloorPlan_Train4_5 77.42±2.74 27.64±1.66 216.64±6.83 1.71±0.13
FloorPlan_Train5_5 77.58±2.27 32.03±1.45 227.46±4.43 1.54±0.09
FloorPlan_Train6_5 74.44±4.37 30.13±1.04 217.56±6.36 1.90±0.21
FloorPlan_Train7_5 80.00±1.77 22.41±1.20 239.99±18.16 1.39±0.11
FloorPlan_Train8_5 66.25±8.84 27.60±3.87 249.83±31.73 2.07±0.35
FloorPlan_Train9_5 64.67±3.40 29.72±1.78 270.19±12.79 1.95±0.11
Average 71.92 29.26 238.92 1.79
FloorPlan_Val1_1 70.83±3.06 30.72±0.78 241.85±11.56 1.74±0.17
FloorPlan_Val1_2 60.45±3.04 23.57±1.53 285.97±5.54 2.31±0.07
FloorPlan_Val1_3 82.98±2.37 36.90±2.38 178.16±11.44 1.41±0.11
FloorPlan_Val1_4 66.85±2.05 26.25±2.42 277.40±15.40 1.85±0.19
FloorPlan_Val1_5 71.81±2.27 34.42±0.91 238.96±5.10 1.87±0.12
FloorPlan_Val2_1 56.97±2.61 17.31±0.98 326.74±9.13 2.30±0.20
FloorPlan_Val2_2 70.56±5.93 31.03±1.34 241.85±24.72 1.92±0.17
FloorPlan_Val2_3 76.41±3.16 33.33±2.68 216.07±8.19 1.49±0.11
FloorPlan_Val2_4 64.36±3.57 25.24±0.84 263.61±15.84 2.04±0.15
FloorPlan_Val2_5 63.33±3.02 24.99±0.36 284.43±5.26 2.44±0.12
FloorPlan_Val3_1 72.86±1.03 26.81±2.75 240.61±8.40 1.90±0.07
FloorPlan_Val3_2 79.31±4.62 27.97±1.41 213.71±14.15 1.60±0.23
FloorPlan_Val3_3 81.48±3.02 30.05±1.48 213.88±10.77 1.46±0.08
FloorPlan_Val3_4 63.81±5.10 18.56±0.80 283.22±10.86 2.11±0.27
FloorPlan_Val3_5 75.14±0.20 28.88±2.25 220.18±9.62 1.65±0.04
Average 70.48 27.74 248.44 1.87
17Table5: RandomPerformanceonRoboTHORTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
FloorPlan_Train10_5 27.15±0.50 16.55±0.85 371.96±3.07 2.84±0.00
FloorPlan_Train11_5 22.83±4.50 11.98±2.67 406.66±15.35 3.24±0.22
FloorPlan_Train12_5 20.68±1.26 11.93±0.51 413.58±4.25 3.02±0.04
FloorPlan_Train1_5 22.00±2.00 13.41±1.47 396.63±0.20 3.08±0.09
FloorPlan_Train2_5 32.83±0.93 14.00±0.57 369.18±2.40 2.54±0.06
FloorPlan_Train3_5 21.26±6.38 10.25±0.81 421.76±1.69 3.36±0.09
FloorPlan_Train4_5 21.32±2.23 9.23±1.62 415.40±9.71 3.08±0.06
FloorPlan_Train5_5 34.95±4.14 16.82±1.57 373.65±12.36 2.55±0.16
FloorPlan_Train6_5 33.67±2.00 17.65±1.43 357.99±0.93 2.76±0.05
FloorPlan_Train7_5 30.75±0.75 12.89±0.42 391.34±6.28 2.54±0.12
FloorPlan_Train8_5 27.00±1.75 11.85±1.68 407.04±8.17 3.16±0.17
FloorPlan_Train9_5 17.50±1.50 12.73±0.25 412.05±6.07 2.96±0.02
Average 26.00 13.27 394.77 2.93
FloorPlan_Val1_1 14.50±2.50 9.64±0.75 436.59±3.65 3.13±0.22
FloorPlan_Val1_2 14.50±4.50 8.63±1.24 433.62±8.48 3.25±0.32
FloorPlan_Val1_3 31.61±6.87 16.30±0.77 382.54±14.30 2.77±0.13
FloorPlan_Val1_4 12.28±3.28 8.49±0.73 436.72±9.52 3.18±0.11
FloorPlan_Val1_5 16.17±1.33 10.29±0.50 424.95±9.29 3.23±0.12
FloorPlan_Val2_1 5.41±3.68 4.71±1.47 464.95±11.81 3.17±0.03
FloorPlan_Val2_2 25.61±0.06 13.89±0.33 395.96±1.47 3.17±0.12
FloorPlan_Val2_3 31.04±2.04 17.05±0.02 377.52±5.43 2.17±0.06
FloorPlan_Val2_4 23.73±3.96 14.14±0.76 398.90±3.78 2.50±0.10
FloorPlan_Val2_5 18.20±1.37 11.74±0.15 417.68±0.58 3.25±0.08
FloorPlan_Val3_1 30.45±0.50 13.00±0.84 381.07±4.86 2.55±0.01
FloorPlan_Val3_2 34.08±1.75 12.83±0.82 368.68±1.57 2.79±0.06
FloorPlan_Val3_3 28.39±0.87 11.83±0.51 394.93±4.61 2.84±0.05
FloorPlan_Val3_4 25.21±3.83 10.30±1.26 397.19±9.02 3.01±0.15
FloorPlan_Val3_5 35.33±3.00 17.42±1.22 376.74±3.21 2.34±0.07
Average 23.10 12.02 405.87 2.89
18Table6: GPT-4oPerformanceonRoboTHORTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
FloorPlan_Train10_5 35.29 19.09 358.81 3.00
FloorPlan_Train11_5 36.00 18.32 363.55 3.15
FloorPlan_Train12_5 35.29 20.42 359.38 3.17
FloorPlan_Train1_5 33.33 21.14 362.02 2.78
FloorPlan_Train2_5 28.57 13.10 367.66 2.76
FloorPlan_Train3_5 20.00 10.86 377.68 3.30
FloorPlan_Train4_5 37.27 18.58 357.89 3.12
FloorPlan_Train5_5 32.73 18.66 363.37 2.95
FloorPlan_Train6_5 41.67 21.92 351.87 3.24
FloorPlan_Train7_5 21.25 8.14 374.45 3.10
FloorPlan_Train8_5 36.25 19.38 356.12 3.07
FloorPlan_Train9_5 26.00 16.57 366.02 2.72
Average 31.97 17.18 363.23 3.03
FloorPlan_Val1_1 24.00 13.84 374.19 3.45
FloorPlan_Val1_2 21.82 11.66 373.75 3.22
FloorPlan_Val1_3 46.32 26.44 342.01 2.53
FloorPlan_Val1_4 15.56 8.23 385.98 3.48
FloorPlan_Val1_5 16.59 5.07 390.28 3.10
FloorPlan_Val2_1 28.43 17.75 365.67 3.31
FloorPlan_Val2_2 48.95 23.77 346.67 2.09
FloorPlan_Val2_3 58.52 30.56 329.43 1.82
FloorPlan_Val2_4 35.23 20.31 360.92 2.09
FloorPlan_Val2_5 26.41 14.60 370.46 3.04
FloorPlan_Val3_1 7.48 2.94 391.37 4.18
FloorPlan_Val3_2 41.67 21.22 349.08 2.99
FloorPlan_Val3_3 31.11 13.08 369.33 2.73
FloorPlan_Val3_4 25.71 11.31 373.28 3.22
FloorPlan_Val3_5 47.50 26.64 346.99 2.09
Average 31.69 16.49 364.63 2.89
19Table7: Video-LLaVAPerformanceonRoboTHORTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
FloorPlan_Train10_5 25.88 13.74 370.67 2.86
FloorPlan_Train11_5 22.67 8.79 380.99 3.58
FloorPlan_Train12_5 17.65 8.67 379.87 3.30
FloorPlan_Train1_5 20.00 7.78 383.98 2.77
FloorPlan_Train2_5 20.95 8.18 382.31 2.67
FloorPlan_Train3_5 14.12 5.89 388.76 3.84
FloorPlan_Train4_5 19.09 6.39 385.62 3.31
FloorPlan_Train5_5 18.18 7.55 385.68 3.04
FloorPlan_Train6_5 23.84 8.69 378.77 2.78
FloorPlan_Train7_5 19.86 7.67 382.04 3.01
FloorPlan_Train8_5 15.79 7.32 384.94 2.84
FloorPlan_Train9_5 22.83 9.11 379.34 2.98
Average 20.07 8.31 381.92 3.08
FloorPlan_Val1_1 25.92 10.42 374.65 3.01
FloorPlan_Val1_2 13.64 5.17 389.82 3.44
FloorPlan_Val1_3 10.92 1.66 396.36 3.65
FloorPlan_Val1_4 20.47 8.40 382.13 3.13
FloorPlan_Val1_5 9.17 3.28 395.26 3.63
FloorPlan_Val2_1 14.21 6.74 388.84 3.61
FloorPlan_Val2_2 22.12 8.90 377.86 3.30
FloorPlan_Val2_3 26.38 12.39 372.44 3.13
FloorPlan_Val2_4 13.53 4.34 391.22 4.01
FloorPlan_Val2_5 20.85 6.12 387.35 3.21
FloorPlan_Val3_1 20.00 6.59 381.67 2.93
FloorPlan_Val3_2 17.46 6.13 387.79 2.94
FloorPlan_Val3_3 16.29 5.24 388.70 2.91
FloorPlan_Val3_4 14.36 5.76 388.94 3.55
FloorPlan_Val3_5 17.38 7.51 382.77 3.34
Average 17.51 6.58 385.72 3.32
20Table8: NOLO-CPerformanceonRoboTHORTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
FloorPlan_Train10_5 72.94±5.08 27.83±0.15 252.83±8.62 1.86±0.29
FloorPlan_Train11_5 82.00±1.44 36.28±2.90 195.77±8.38 1.50±0.05
FloorPlan_Train12_5 73.53±0.83 32.13±1.26 230.53±8.63 1.59±0.05
FloorPlan_Train1_5 64.72±1.04 32.41±1.39 258.61±2.70 1.94±0.06
FloorPlan_Train2_5 70.63±3.40 29.21±3.18 243.38±16.56 1.78±0.11
FloorPlan_Train3_5 62.16±1.94 23.55±0.62 288.69±8.45 2.27±0.09
FloorPlan_Train4_5 72.12±5.77 27.65±0.21 244.76±16.37 1.89±0.20
FloorPlan_Train5_5 75.61±0.57 32.77±1.68 232.83±3.66 1.60±0.04
FloorPlan_Train6_5 67.78±4.37 27.60±0.31 254.15±5.21 2.00±0.12
FloorPlan_Train7_5 75.62±1.84 19.90±2.02 256.17±8.42 1.55±0.03
FloorPlan_Train8_5 65.62±2.55 26.02±2.55 253.82±8.94 2.00±0.07
FloorPlan_Train9_5 56.67±4.11 24.40±2.52 303.16±8.15 2.18±0.05
Average 69.95 28.31 251.23 1.85
FloorPlan_Val1_1 64.33±5.44 25.29±5.10 283.93±29.92 2.12±0.22
FloorPlan_Val1_2 57.42±3.57 22.97±1.36 305.57±10.25 2.34±0.15
FloorPlan_Val1_3 82.98±1.38 41.56±1.40 197.25±10.54 1.43±0.04
FloorPlan_Val1_4 62.22±3.27 25.73±1.58 284.70±10.51 2.26±0.10
FloorPlan_Val1_5 58.61±1.42 31.95±2.50 276.70±4.67 2.53±0.12
FloorPlan_Val2_1 52.73±1.96 17.41±1.69 336.15±1.47 2.59±0.08
FloorPlan_Val2_2 67.22±4.33 28.52±2.17 268.21±2.39 1.95±0.07
FloorPlan_Val2_3 79.74±3.46 35.81±0.85 209.43±23.21 1.34±0.08
FloorPlan_Val2_4 62.95±6.01 25.65±0.77 276.85±16.89 2.10±0.19
FloorPlan_Val2_5 58.70±0.94 24.36±1.83 290.25±2.98 2.55±0.05
FloorPlan_Val3_1 71.43±2.33 26.32±3.77 240.97±20.02 2.00±0.14
FloorPlan_Val3_2 71.53±1.75 26.78±0.32 238.58±3.53 1.99±0.09
FloorPlan_Val3_3 72.59±4.57 27.82±1.74 242.52±9.40 1.73±0.18
FloorPlan_Val3_4 57.30±2.65 18.35±0.51 300.88±11.16 2.30±0.08
FloorPlan_Val3_5 70.97±3.16 29.52±1.81 243.77±19.50 1.96±0.14
Average 66.05 27.20 266.38 2.08
21Table9: NOLO-TPerformanceonRoboTHORTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
FloorPlan_Train10_5 76.47±3.53 29.97±2.31 225.76±18.05 1.71±0.16
FloorPlan_Train11_5 78.00±0.67 36.00±0.43 210.91±7.94 1.56±0.10
FloorPlan_Train12_5 75.88±0.59 30.77±1.29 233.25±4.82 1.62±0.02
FloorPlan_Train1_5 64.17±2.50 32.47±4.30 254.97±6.37 2.07±0.01
FloorPlan_Train2_5 65.71±6.67 26.12±4.84 259.38±29.69 1.95±0.25
FloorPlan_Train3_5 59.41±4.12 22.67±0.98 303.37±11.79 2.39±0.08
FloorPlan_Train4_5 72.73±0.91 27.02±2.10 238.05±10.76 1.89±0.09
FloorPlan_Train5_5 78.18±3.64 33.88±0.94 224.37±22.06 1.48±0.12
FloorPlan_Train6_5 74.17±0.83 32.28±1.91 216.92±17.75 1.83±0.11
FloorPlan_Train7_5 80.62±0.62 23.85±1.03 232.86±15.75 1.38±0.08
FloorPlan_Train8_5 66.25±2.50 30.95±2.62 245.14±15.49 2.06±0.12
FloorPlan_Train9_5 61.00±1.00 28.92±1.60 285.27±2.75 1.80±0.20
Average 71.05 29.57 244.19 1.81
FloorPlan_Val1_1 64.00±2.00 26.54±1.03 277.16±12.08 2.25±0.02
FloorPlan_Val1_2 64.55 25.30±0.81 272.34±12.70 2.20±0.01
FloorPlan_Val1_3 80.53±5.79 36.93±0.98 201.32±9.73 1.62±0.22
FloorPlan_Val1_4 59.44±7.22 25.50±1.25 283.97±24.48 2.14±0.21
FloorPlan_Val1_5 62.50±0.83 32.49±0.23 268.84±5.32 2.06±0.01
FloorPlan_Val2_1 56.36±0.91 17.59±0.30 329.25±10.75 2.44±0.04
FloorPlan_Val2_2 65.56±2.22 30.85±2.09 256.13±9.46 2.07±0.07
FloorPlan_Val2_3 76.92±1.54 34.22±0.03 209.07±4.52 1.59±0.02
FloorPlan_Val2_4 68.08±0.38 25.24±0.19 265.69±6.21 1.93±0.00
FloorPlan_Val2_5 65.22±3.48 24.79±0.74 290.28±11.04 2.29±0.14
FloorPlan_Val3_1 69.52±0.95 25.09±0.17 244.60±10.47 2.04±0.03
FloorPlan_Val3_2 75.83±0.83 27.14±2.16 223.59±6.15 1.81±0.06
FloorPlan_Val3_3 73.70±2.59 24.63±2.77 245.72±15.48 1.75±0.15
FloorPlan_Val3_4 52.38±3.81 17.61±0.82 316.57±4.58 2.46±0.12
FloorPlan_Val3_5 76.67±0.83 31.29±1.91 211.91±5.29 1.62±0.04
Average 67.42 27.01 259.76 2.02
22Table10: NOLO(M)PerformanceonRoboTHORTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
FloorPlan_Train10_5 80.59±4.12 32.92±1.93 202.68±22.39 1.47±0.12
FloorPlan_Train11_5 77.33±2.67 36.00±3.27 218.04±1.64 1.64±0.01
FloorPlan_Train12_5 78.82 36.62±0.82 211.44±9.89 1.55±0.01
FloorPlan_Train1_5 50.83±5.83 27.99±2.85 302.14±14.69 2.35±0.10
FloorPlan_Train2_5 69.05±1.43 25.09±2.83 256.49±1.55 1.85±0.13
FloorPlan_Train3_5 54.71±0.59 21.49±0.94 312.69±3.18 2.61±0.08
FloorPlan_Train4_5 73.64±2.73 25.06±1.03 245.16±17.39 1.78±0.08
FloorPlan_Train5_5 75.45±0.91 31.23±1.14 234.16±1.94 1.75±0.07
FloorPlan_Train6_5 65.83±2.50 24.81±0.36 276.04±19.72 1.95±0.01
FloorPlan_Train7_5 70.62±0.63 19.64±0.81 281.29±2.88 1.77±0.05
FloorPlan_Train8_5 59.38±6.87 23.84±2.27 287.47±17.78 2.38±0.37
FloorPlan_Train9_5 59.00±7.00 31.86±5.95 278.41±28.49 2.12±0.22
Average 67.94 28.05 258.83 1.93
FloorPlan_Val1_1 60.00±3.00 25.32±3.23 294.85±21.57 2.29±0.09
FloorPlan_Val1_2 60.00±1.82 24.32±3.48 291.72±6.24 2.18±0.09
FloorPlan_Val1_3 82.63±1.58 35.75±3.13 200.15±3.24 1.41±0.09
FloorPlan_Val1_4 62.22±1.11 24.95±0.16 288.34±6.13 2.19±0.21
FloorPlan_Val1_5 64.58±4.58 31.77±3.05 265.98±16.41 2.04±0.16
FloorPlan_Val2_1 49.09±1.82 14.51±2.21 358.64±2.52 2.68±0.06
FloorPlan_Val2_2 66.11±2.78 30.06±1.07 261.83±10.29 2.11±0.15
FloorPlan_Val2_3 77.69±0.77 31.33±1.61 232.93±6.27 1.49±0.02
FloorPlan_Val2_4 57.69±1.54 23.61±0.69 296.56±8.78 2.26±0.07
FloorPlan_Val2_5 59.57±4.78 23.29±0.57 303.90±19.02 2.48±0.30
FloorPlan_Val3_1 70.48±1.90 25.14±1.24 254.00±0.10 1.87±0.06
FloorPlan_Val3_2 71.25±0.42 25.59±0.67 244.68±2.23 1.88±0.04
FloorPlan_Val3_3 67.78±0.37 24.20±1.78 258.00±2.40 1.89±0.03
FloorPlan_Val3_4 54.29±0.95 13.15±1.33 330.42±8.02 2.40±0.00
FloorPlan_Val3_5 67.50±1.67 30.21±4.40 246.60±8.73 2.05±0.04
Average 64.73 25.55 275.24 2.08
D.2 HabitatVideoNavigationPerformance
D.3 CrossEvaluationResults
E SocialImpactandFutureWork
We see our work as a starting point to propose a general and scalable method to offline learn a
navigationpolicyfromlimitedsourcesforanymobileagents. Inthefuture,WebelieveNOLOwill
benefitalotfromtherapiddevelopmentinopticalflowandcomputervisionfoundationmodels,
andtherewillbeextensiveapplicationsofNOLO,suchasdeploymenttosweepingrobotsorrescue
robots. Furtherimprovementcanbeconductedtoexamineandenhancethereliabilityofthedecoded
actionsfromopticalflow. Itisalsoagreatideatoincorporatefirst-personandthird-personvideosto
showcasemoreadvancedintelligenceofspatialunderstandingandegocentricdecision-makingin
morecomplex3Dembodiedenvironments.
23Table11: NOLOPerformanceonHabitatTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
2azQ1b91cZZ 50.26±3.10 28.99±1.40 311.44±11.24 4.30±0.12
8194nk5LbLH 43.94±1.55 22.60±0.51 341.28±1.28 4.60±0.08
EU6Fwq7SyZv 42.33±1.70 18.15±1.13 362.99±5.59 4.40±0.06
QUCTc6BB5sX 44.67±6.24 21.94±1.36 344.75±21.03 5.75±0.84
SN83YJsR3w2 38.33±6.24 15.74±2.48 379.77±17.81 4.47±0.97
TbHJrupSAjP 36.67±10.19 14.29±3.23 376.57±28.90 5.35±0.79
UwV83HsGsw3 30.48±3.75 13.66±0.67 402.48±14.42 6.30±0.58
Vt2qJdWjCF2 34.58±4.60 12.75±4.26 387.33±21.56 8.13±0.81
VzqfbhrpDEA 33.70±1.89 17.94±1.15 377.91±7.07 6.75±0.22
X7HyMhZNoso 66.67±4.38 30.84±2.47 275.02±20.74 2.87±0.22
Z6MFQCViBuw 40.67±6.18 16.25±3.41 369.67±21.35 6.12±0.64
gTV8FGcVJC9 53.67±4.03 29.49±2.20 292.08±12.03 5.48±0.12
gYvKGZ5eRqb 25.00±8.16 7.34±4.04 433.63±30.22 7.56±0.59
oLBMNvg9in8 45.67±4.03 24.45±3.33 335.14±14.15 3.69±0.16
pLe4wQe7qrG 70.00±2.72 38.63±2.66 218.44±6.09 2.38±0.24
pa4otMbVnkk 41.11±11.00 20.99±6.68 346.89±36.77 7.45±1.17
rqfALeAoiTq 47.50±3.12 21.92±2.23 338.93±11.95 4.07±0.11
x8F5xyUWy9e 51.82±0.74 23.36±0.97 320.58±4.80 4.41±0.06
yqstnuAEVhm 38.89±0.79 17.68±1.86 369.21±3.18 4.04±0.12
zsNo4HB9uLZ 37.08±3.28 18.48±2.66 367.31±11.94 5.27±0.34
Average 43.65 20.77 347.57 5.17
Table12: RandomPerformanceonHabitatTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
2azQ1b91cZZ 28.73±6.04 13.11±3.30 386.34±24.39 3.91±0.26
8194nk5LbLH 32.68±4.95 11.77±0.59 380.37±10.02 3.78±0.13
EU6Fwq7SyZv 21.50±2.50 8.57±0.97 409.05±12.59 5.07±0.18
QUCTc6BB5sX 21.50±0.50 10.60±1.00 399.77±3.10 5.67±0.00
SN83YJsR3w2 12.00±3.00 5.89±1.05 428.02±12.97 4.97±0.14
TbHJrupSAjP 18.07±0.21 5.57±1.55 427.09±10.29 5.89±0.37
UwV83HsGsw3 15.93±1.93 4.78±1.42 435.46±1.59 5.57±0.16
Vt2qJdWjCF2 20.75±1.75 8.75±0.86 415.93±1.07 6.02±0.05
VzqfbhrpDEA 25.06±2.28 7.90±1.88 412.03±12.59 5.14±0.05
X7HyMhZNoso 45.21±0.21 17.71±0.35 342.76±8.05 2.86±0.03
Z6MFQCViBuw 17.50±4.50 5.40±1.41 430.17±14.77 6.83±0.48
gTV8FGcVJC9 34.00±5.00 13.64±1.80 370.20±16.76 4.76±0.08
gYvKGZ5eRqb 7.00±3.00 5.39±2.65 457.60±20.20 7.21±0.10
oLBMNvg9in8 20.50±0.50 8.57±1.53 418.56±8.32 4.28±0.10
pLe4wQe7qrG 41.17±7.17 15.29±4.11 348.37±24.83 3.17±0.90
pa4otMbVnkk 22.83±7.83 14.06±0.92 391.77±29.37 6.59±0.03
rqfALeAoiTq 22.00±1.33 9.19±2.09 410.70±6.22 4.25±0.36
x8F5xyUWy9e 29.05±3.23 12.57±0.77 388.25±1.57 4.69±0.14
yqstnuAEVhm 23.67±1.17 9.23±0.06 410.58±9.25 3.90±0.13
zsNo4HB9uLZ 31.37±1.13 12.54±0.88 377.44±10.84 4.78±0.14
Average 24.53 10.03 402.02 4.97
24Table13: GPT-4oPerformanceonHabitatTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
2azQ1b91cZZ 31.67 17.77 348.79 4.19
8194nk5LbLH 26.67 12.07 360.59 4.21
EU6Fwq7SyZv 37.41 20.31 347.59 5.10
QUCTc6BB5sX 36.90 20.04 347.43 6.38
SN83YJsR3w2 37.24 21.26 344.79 4.69
TbHJrupSAjP 36.89 23.76 339.05 2.99
UwV83HsGsw3 32.86 22.14 344.40 4.97
Vt2qJdWjCF2 32.50 18.00 350.69 7.19
VzqfbhrpDEA 36.67 19.97 346.93 5.26
X7HyMhZNoso 42.86 26.08 335.63 2.66
Z6MFQCViBuw 24.00 12.83 362.88 7.99
gTV8FGcVJC9 38.49 21.29 344.03 4.25
gYvKGZ5eRqb 37.46 23.24 340.84 4.58
oLBMNvg9in8 38.34 23.79 335.41 2.06
pLe4wQe7qrG 29.32 18.54 350.50 5.77
pa4otMbVnkk 34.82 16.11 351.36 5.98
rqfALeAoiTq 39.49 23.56 339.48 3.32
x8F5xyUWy9e 40.91 23.94 338.39 4.23
yqstnuAEVhm 35.00 21.56 345.06 3.41
zsNo4HB9uLZ 33.75 21.45 342.88 4.72
Average 35.16 20.39 345.84 4.70
Table14: Video-LLaVAPerformanceonHabitatTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
2azQ1b91cZZ 20.00 10.21 381.51 3.77
8194nk5LbLH 12.50 7.28 387.78 5.17
EU6Fwq7SyZv 14.00 7.92 385.03 4.04
QUCTc6BB5sX 19.00 12.24 378.96 5.26
SN83YJsR3w2 20.00 12.34 379.00 3.72
TbHJrupSAjP 7.14 3.01 395.36 5.05
UwV83HsGsw3 8.57 4.70 393.87 5.29
Vt2qJdWjCF2 11.32 6.66 388.67 6.24
VzqfbhrpDEA 16.20 10.37 384.72 5.42
X7HyMhZNoso 11.27 4.48 393.52 4.49
Z6MFQCViBuw 24.60 15.43 371.56 4.64
gTV8FGcVJC9 25.00 15.86 371.61 4.50
gYvKGZ5eRqb 10.00 7.19 389.55 5.23
oLBMNvg9in8 14.00 9.90 380.34 4.40
pLe4wQe7qrG 13.33 12.94 377.50 3.17
pa4otMbVnkk 10.00 5.57 387.50 6.02
rqfALeAoiTq 12.50 8.34 386.06 3.91
x8F5xyUWy9e 15.91 11.81 380.44 3.43
yqstnuAEVhm 10.54 6.06 390.36 5.19
zsNo4HB9uLZ 15.01 8.03 387.20 5.30
Average 14.55 9.02 384.53 4.71
25Table15: NOLO-CPerformanceonHabitatTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
2azQ1b91cZZ 37.18±4.28 22.84±3.63 359.71±17.26 6.38±0.69
8194nk5LbLH 36.97±1.55 19.47±0.99 366.57±4.66 4.97±0.34
EU6Fwq7SyZv 34.00±2.16 18.48±0.86 378.66±2.54 5.18±0.31
QUCTc6BB5sX 40.33±1.25 23.52±0.77 355.46±1.65 6.83±0.32
SN83YJsR3w2 15.00±7.07 6.92±2.91 455.47±15.62 6.56±1.16
TbHJrupSAjP 20.95±5.99 6.05±1.97 437.43±14.11 6.53±0.51
UwV83HsGsw3 26.67±1.35 17.00±1.76 401.74±3.26 6.77±0.41
Vt2qJdWjCF2 19.17±5.62 8.34±0.78 437.86±13.29 13.49±0.85
VzqfbhrpDEA 25.19±1.05 15.50±2.16 398.60±9.64 7.91±1.16
X7HyMhZNoso 59.05±2.94 30.08±0.75 290.51±7.25 4.00±0.27
Z6MFQCViBuw 38.00±5.66 15.43±1.57 375.89±16.03 7.19±0.22
gTV8FGcVJC9 41.33±1.70 25.04±1.93 335.27±13.56 6.63±0.73
gYvKGZ5eRqb 11.67±2.36 3.51±1.45 474.47±10.70 12.91±0.46
oLBMNvg9in8 44.33±2.36 25.58±1.67 332.98±8.66 3.85±0.12
pLe4wQe7qrG 52.22±3.14 26.99±1.25 316.10±10.12 3.54±0.48
pa4otMbVnkk 14.44±5.67 7.67±3.04 447.38±20.68 9.15±1.53
rqfALeAoiTq 36.94±4.63 18.40±1.58 370.27±11.73 4.45±0.07
x8F5xyUWy9e 46.97±4.71 21.66±1.96 335.51±11.35 4.43±0.57
yqstnuAEVhm 38.61±6.17 17.60±3.23 372.49±22.68 4.89±0.55
zsNo4HB9uLZ 32.50±3.54 18.14±0.90 382.85±11.49 5.92±0.54
Average 33.58 17.41 381.26 6.58
Table16: NOLO-TPerformanceonHabitatTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
2azQ1b91cZZ 41.28±2.97 25.50±2.75 344.03±15.14 6.45±0.08
8194nk5LbLH 39.70±2.61 19.67±0.74 364.86±3.70 5.04±0.27
EU6Fwq7SyZv 41.00±3.27 21.33±2.90 357.03±11.76 5.37±0.17
QUCTc6BB5sX 41.50±4.50 21.52±2.39 357.77±11.40 7.51±0.73
SN83YJsR3w2 21.67±11.79 7.83±4.20 432.92±32.98 7.79±1.89
TbHJrupSAjP 29.52±0.67 11.53±0.91 400.50±5.36 6.45±0.39
UwV83HsGsw3 28.10±0.67 15.16±2.66 398.37±9.11 6.95±0.19
Vt2qJdWjCF2 18.33±3.12 7.74±1.46 443.81±10.96 13.66±0.67
VzqfbhrpDEA 30.00±0.91 16.90±0.92 387.73±5.64 9.28±0.43
X7HyMhZNoso 63.33±0.89 32.24±0.69 270.01±2.15 4.31±0.39
Z6MFQCViBuw 40.67±3.40 15.79±3.40 380.14±11.92 6.87±0.44
gTV8FGcVJC9 46.50±1.50 28.97±1.96 313.77±0.72 7.54±0.94
gYvKGZ5eRqb 10.00 2.73±0.50 471.45±3.05 11.34±1.00
oLBMNvg9in8 47.00±1.00 24.69±1.35 332.24±4.97 3.92±0.16
pLe4wQe7qrG 65.00±1.67 28.04±5.24 290.87±27.90 2.58±0.09
pa4otMbVnkk 33.33±3.33 18.79±0.48 365.55±13.95 9.17±0.66
rqfALeAoiTq 43.89±2.19 22.01±1.75 348.34±8.51 4.55±0.29
x8F5xyUWy9e 52.42±0.43 22.44±0.94 328.36±2.28 4.13±0.02
yqstnuAEVhm 38.61±3.14 17.06±2.82 372.31±14.27 4.83±0.13
zsNo4HB9uLZ 26.67±1.18 16.10±1.78 400.21±5.77 6.74±0.08
Average 37.93 18.80 368.01 6.72
26Table17: NOLO(M)PerformanceonHabitatTestingSet
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
2azQ1b91cZZ 35.77±0.38 21.70±0.98 367.77±0.44 7.26±0.06
8194nk5LbLH 38.18±0.91 19.20±0.74 365.92±3.05 5.05±0.18
EU6Fwq7SyZv 40.50±4.50 20.97±1.42 360.27±14.82 5.04±0.49
QUCTc6BB5sX 40.00 22.11±1.36 354.65±1.51 6.64±0.25
SN83YJsR3w2 32.50±7.50 10.28±2.55 414.92±23.22 6.92±1.20
TbHJrupSAjP 24.29±1.43 7.48±0.50 427.30±6.51 6.63±0.07
UwV83HsGsw3 30.71±2.14 17.89±3.05 390.06±15.81 6.03±0.21
Vt2qJdWjCF2 13.75±1.25 6.65±0.62 454.84±0.97 14.59±0.18
VzqfbhrpDEA 28.33±3.89 14.89±2.38 402.32±9.62 9.08±0.47
X7HyMhZNoso 64.64±1.79 30.95±0.11 276.48±5.50 3.94±0.15
Z6MFQCViBuw 37.00±7.00 16.86±3.37 374.19±22.49 6.90±0.33
gTV8FGcVJC9 50.50±7.50 28.67±1.23 306.25±9.88 6.54±0.72
gYvKGZ5eRqb 12.50±2.50 3.79±1.73 475.62±14.77 11.55±1.20
oLBMNvg9in8 48.00±2.00 28.67±0.73 319.90±6.13 3.61±0.16
pLe4wQe7qrG 53.33±3.33 26.82±1.01 311.75±0.38 3.26±0.19
pa4otMbVnkk 20.00 8.13±0.40 431.58±1.42 10.87±0.33
rqfALeAoiTq 45.42±2.08 20.42±2.06 352.25±7.54 4.19±0.23
x8F5xyUWy9e 49.55±4.09 22.50±1.80 331.82±12.30 4.14±0.47
yqstnuAEVhm 42.08±4.58 18.98±3.12 360.02±16.95 4.89±0.05
zsNo4HB9uLZ 27.50 16.36±1.47 400.51±1.49 6.76±0.10
Average 36.73 18.17 373.92 6.69
27Table18: NOLOPerformanceinRoboTHORaftertraininginHabitat
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
FloorPlan_Train10_5 60.00±0.96 26.63±1.37 303.93±7.01 2.30±0.13
FloorPlan_Train11_5 64.89±3.50 31.85±3.64 286.70±20.00 2.09±0.22
FloorPlan_Train12_5 52.16±5.79 26.13±3.37 326.15±15.30 2.36±0.24
FloorPlan_Train1_5 50.00±6.24 29.40±1.07 320.91±10.47 2.46±0.04
FloorPlan_Train2_5 47.94±4.56 20.57±4.91 339.62±26.92 2.42±0.18
FloorPlan_Train3_5 39.22±3.09 18.48±0.60 377.95±6.26 3.30±0.22
FloorPlan_Train4_5 64.24±1.87 30.56±0.79 279.33±3.76 2.13±0.13
FloorPlan_Train5_5 61.52±0.86 27.48±2.16 305.47±7.33 2.06±0.11
FloorPlan_Train6_5 60.56±2.83 31.06±4.18 285.57±25.83 2.30±0.16
FloorPlan_Train7_5 50.83±0.59 20.54±1.48 334.78±0.93 2.30±0.17
FloorPlan_Train8_5 54.58±4.12 28.68±1.69 297.57±10.62 2.41±0.02
FloorPlan_Train9_5 38.67±6.60 22.46±4.39 364.06±18.55 2.53±0.15
Average 53.72 26.15 318.50 2.39
FloorPlan_Val1_1 54.00±4.32 28.45±3.73 316.52±19.28 2.34±0.15
FloorPlan_Val1_2 48.48±4.09 23.46±3.96 338.57±19.55 2.61±0.12
FloorPlan_Val1_3 74.04±4.06 39.60±2.33 235.18±8.91 1.71±0.09
FloorPlan_Val1_4 44.07±5.83 23.73±1.69 350.40±17.36 2.65±0.12
FloorPlan_Val1_5 49.44±4.10 27.90±4.66 324.46±26.83 2.68±0.17
FloorPlan_Val2_1 32.73±3.71 14.01±1.79 404.18±12.65 3.19±0.10
FloorPlan_Val2_2 48.89±0.91 24.29±3.87 338.47±12.65 2.96±0.19
FloorPlan_Val2_3 67.18±1.92 35.17±2.13 278.73±10.96 1.63±0.08
FloorPlan_Val2_4 52.69±0.38 25.33±0.32 340.45±7.34 2.33±0.03
FloorPlan_Val2_5 44.35±2.61 22.94±1.82 353.16±22.72 2.94±0.11
FloorPlan_Val3_1 59.05±2.86 29.58±2.96 279.42±18.76 2.24±0.23
FloorPlan_Val3_2 56.39±1.71 22.96±1.40 310.36±17.14 2.40±0.08
FloorPlan_Val3_3 59.51±4.58 26.33±3.53 297.91±19.58 2.22±0.14
FloorPlan_Val3_4 41.59±1.19 15.47±1.16 360.06±9.11 2.95±0.01
FloorPlan_Val3_5 60.00±2.36 27.78±2.98 298.36±11.92 2.15±0.15
Average 52.83 25.80 321.75 2.47
28Table19: NOLOPerformanceinHabitataftertraininginRoboTHOR
Scene SR(%)↑ SPL(%)↑ NE↓ TL↓
2azQ1b91cZZ 38.21±0.96 23.38±0.54 355.25±4.81 6.45±0.16
8194nk5LbLH 41.82±2.23 21.25±0.96 351.39±6.97 4.85±0.14
EU6Fwq7SyZv 45.67±2.87 22.27±1.75 345.99±10.11 4.72±0.37
QUCTc6BB5sX 44.00±2.45 23.97±1.79 346.11±11.27 6.91±0.19
SN83YJsR3w2 18.33±4.71 7.28±0.33 449.10±9.22 9.20±0.77
TbHJrupSAjP 28.10±2.94 8.99±0.80 422.23±3.90 6.39±0.26
UwV83HsGsw3 27.62±4.10 14.83±3.12 408.73±7.70 7.07±0.60
Vt2qJdWjCF2 17.92±2.12 8.39±1.65 436.55±5.17 14.59±0.41
VzqfbhrpDEA 27.41±2.28 15.64±0.72 396.57±6.72 9.19±0.40
X7HyMhZNoso 62.86±1.17 29.77±1.18 278.76±4.89 4.16±0.42
Z6MFQCViBuw 47.33±3.40 15.48±1.83 368.63±8.25 5.93±0.66
gTV8FGcVJC9 50.00±1.63 27.88±2.24 316.60±11.00 7.03±0.44
gYvKGZ5eRqb 18.33±2.36 3.68±0.50 468.87±6.26 9.14±1.13
oLBMNvg9in8 47.67±1.70 27.95±2.42 322.73±10.51 3.65±0.08
pLe4wQe7qrG 51.11±1.57 26.30±4.24 317.16±15.91 3.56±0.22
pa4otMbVnkk 31.11±9.56 13.82±3.98 398.71±26.33 9.85±1.45
rqfALeAoiTq 38.61±3.22 18.14±1.66 369.56±11.93 4.89±0.11
x8F5xyUWy9e 50.00±3.40 21.01±1.50 332.68±12.40 4.41±0.13
yqstnuAEVhm 41.11±2.39 18.79±1.28 363.67±11.33 4.81±0.08
zsNo4HB9uLZ 28.75±2.70 16.22±0.45 399.03±8.76 6.50±0.24
Average 37.80 18.25 372.41 6.67
29