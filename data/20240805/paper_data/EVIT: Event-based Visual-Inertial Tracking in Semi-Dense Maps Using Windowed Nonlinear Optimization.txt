EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps
Using Windowed Nonlinear Optimization
Runze Yuan, Tao Liu, Zijia Dai, Yi-Fan Zuo, and Laurent Kneip
Abstract‚ÄîEvent cameras are an interesting visual extero- Semi-denseMap
ceptive sensor that reacts to brightness changes rather than ùëä
integratingabsoluteimageintensities.Owingtothisdesign,the
sensor exhibits strong performance in situations of challenging
ùëá$% #&‚Äô
dynamicsandilluminationconditions.Whileevent-basedsimul-
ùëá$% "&‚Äô
taneoustrackingandmappingremainsachallengingproblem,a
ùëá$% !&‚Äô TSM !
numberofrecentworkshavepointedoutthesensor‚Äôssuitability
for prior map-based tracking. By making use of cross-modal
registrationparadigms,thecamera‚Äôsego-motioncanbetracked TSM "
acrossalargespectrumofilluminationanddynamicsconditions
on top of accurate maps that have been created a priori by ùìï ! !ùêºùëÄùëà TSM #
more traditional sensors. The present paper follows up on a
ùìï "
recently introduced event-based geometric semi-dense tracking
!ùêºùëÄùëà
paradigm, and proposes the addition of inertial signals in
order to robustify the estimation. More specifically, the added
signals provide strong cues for pose initialization as well as ùìï #
regularization during windowed, multi-frame tracking. As a
Fig.1. IllustrationofEVIT.Ratherthanregisteringonlysingletime-surface
result,theproposedframeworkachievesincreasedperformance mapswithrespecttoasemi-densepointcloud,weproposetodowindowed
underchallengingilluminationconditionsaswellasareduction joint registration of multiple adjacent TSMs, which improves registration
of the rate at which intermediate event representations need stability. The added IMU integration terms form the connections between
to be registered in order to maintain stable tracking across adjacentkeyframes,therebycreatingavirtualmulti-camerarigwithelastic
highly dynamic sequences.Our evaluationfocuses on adiverse connections.
set of real world sequences and comprises a comparison of
our proposed method against a purely event-based alternative
running at different rates. themeasurementsreturnedbyaneventcameraarenolonger
just a function of pose and scene geometry or appearance,
I. INTRODUCTION but also the instantaneous relative dynamics of the camera.
Eventcamerashaverecentlybecomepopularinthevision Combined with the unusual and noisy nature of events,
and robotics community but remain less explored than their the solution to the full SLAM problem with DVS sensors
traditional camera correspondents. Unlike the latter, event remains an ongoing challenge.
cameras‚ÄîalsocalledDynamicVisionSensors(DVS)‚Äîreact In this paper, we address the question how we can solve
to brightness changes rather than integrating absolute image the ego-motion estimation problem under the often practical
intensities.Eachpixelactsasynchronouslyandfiresaneither assumptionofanexistingpriormapgeneratedbyadifferent
positive or negative event whenever the perceived brightness sensor?Ifthemappingproblemistakenoutoftheequation,
levelaugmentsordecrementsbyacertainthresholdamount. the target simply becomes prior map-based tracking. We
Each event comes with a high resolution time-stamp in the followupontherecentlyproposedstrategyofabstractingthe
order of micro-seconds, and the camera has high dynamic map of the environment in the form of a semi-dense point
range(120dBcomparedto60dBforregularcameras).Owing cloud[9].Thesemi-densemapcanbegeneratedfromregular
to these properties, event cameras do not suffer from tradi- imagery, and each point in the representation corresponds to
tionalblureffectsandlendthemselvestostrongperformance alocationinspacearoundwhichwecommonlyobservehigh
in challenging illumination and dynamics conditions. Often appearancegradient(i.e.structuraledgesanddiscontinuities,
referred to as a silicon retina [1], this power-efficient bio- ortexturalappearanceboundaries).Basedontheassumption
inspiredsensorthereforehasevolvedintoapromisingchoice that the generated events pre-dominantly react to such high-
for motion sensing in a variety of contexts, including eye gradientregions,thecurrentlocationoftheeventcameracan
tracking [2], hand tracking [3], human body tracking [4], beoptimizedbyaligningedgesinthesurfaceofactiveevents
[5], and‚Äîas targeted in this work‚Äîego-motion tracking. withthereprojectedsemi-densepointcloud.Whilethissemi-
The traditional solution to vision-based ego-motion track- dense cross-modal registration paradigm has already been
ing is given by the Simultaneous Localization And Mapping proposed in prior art [10], [9], challenges remain in case of
(SLAM) paradigm [6], and has already been proposed for high dynamics or ambiguous event distributions.
event cameras [7], [8]. However, unlike for regular cameras, We propose to tackle this issue by fusion with an Iner-
tial Measurement Unit (IMU). This achieves the following
MobilePerceptionLab,ShanghaiTechUniversity contributions and advancements:
4202
guA
2
]VC.sc[
1v07310.8042:viXra‚Ä¢ Ratherthandoingsingleframeregistration,theaddition In particular, the latter two methods are again based on line
of an IMU permits the joint registration of multiple features. In general, a truly robust continuous tracking and
adjacent frames in the form of a windowed tracking mappingframeworkthatemploysonlyasingleeventcamera
problem. The states corresponding to the individual as an exteroceptive sensor remains a challenging problem.
times of each registered frame are lifted to include Vidal et al. [20] therefore investigate the combination with
first-order translational velocity and IMU bias terms, a normal camera, and Zhou et al. [21] present a stereo event
thereby enabling the use of IMU pre-integration terms camera alternative.
for a pair-wise regularization of adjacent frames. Sets A different line of research that is more related to our
of multiple adjacent frames are thereby considered as work aims at pure tracking based on known structure priors.
a virtual multi-camera rig with elastic connections. The Early approaches make use of fiducial markers [22], [23]
joint registration of multiple frames helps in avoiding or known targets with distinctive texture [24], [25], [26],
unobservable directions of displacement caused by am- [27]. Of particular interest are the works of Gallego et
biguous event distributions in individual frames (e.g. al. [28] and Bryner et al. [29], who propose direct tracking
events distributed along multiple parallel lines). of event cameras from photometric depth maps. However,
‚Ä¢ IMUpre-integrationtermsmayagainbeusedinorderto their approaches are computationally demanding and fail to
createimprovedinitializationofnewframes.Especially employ a geometrically optimal objective.
insituationsofjerkymotionwithsuddenabruptchanges The most highly related works to ours are all based on
in the direction of motion, the inertial signal-based the idea of using a semi-dense map representation. The
prediction strongly outperforms vision-only alternatives latter have originally been introduced by Engel et al. [30]
such as constant-velocity motion models. through their SDVO framework. Later on, Kneip et al. [31],
‚Ä¢ Owing to poor prediction abilities, handling situations Kuse and Shaojie [32], and Zhou et al. [33] have introduced
of high acceleration in a vision only setting typically improved geometric semi-dense registration paradigms for
requires a high temporal density of tracked frames. In normal cameras. The latter work in particular has formed
turn,thesuperiorpredictionabilityofferedbyIMUsig- the foundation for the first geometric semi-dense tracking
nalsenablesasubstantialreductionoftheframedensity paradigmsforeventcameras.Zuoetal.[10]proposeDEVO,
during windowed optimization. In fact, we propose an an event-based incremental motion estimation framework
adaptive mechanism that places frames in dependence thatmakesuseofanadditionaldepthchannel.Finally,Zuoet
of the number of occurred events. al. [9] propose Canny-EVT, which applies a similar strategy
We test our new framework‚ÄîEvent-based Visual-Inertial to global tracking of a single event camera based on a semi-
Tracking (EVIT)‚Äîon publicly available benchmark se- dense map. The latter work in particular forms the reference
quencescoveringavarietyofchallengessuchasnormalmo- implementation against which we compare our contribution.
tion, changing illumination, and high dynamics. As demon-
III. METHOD
strated, the addition of an IMU strongly supports tracking
In this section, we present our newly proposed event-
performance with respect to the vision-only alternative. We
inertial,semi-densepointcloud-basedtrackingsystemEVIT.
also plan to release the complete framework code later to
The framework comprises an efficient adaptive measure-
enable re-usability.
mentpreprocessingmodule,alooselycoupledbootstrapping
II. RELATEDWORK scheme, and a tightly coupled back-end optimization for
A good survey paper on event-based vision is given by the event camera-IMU setup. The structure of the proposed
Gallego et al. [11]. The addressed topics in the literature framework is shown in Fig. 2, in which we highlight the
cover many aspects such as human motion tracking [4], [5], essential modules by dashed lines.
hand tracking [3], and eye ball tracking [2]. The current Let us define notations used in this paper. The ultimate
literature review focusses on SLAM and motion tracking goal for this framework is to estimate the body frame pose
with only event cameras or with added inertial readings. Tw = [Rw,pw] ‚àà SE(3) (transformation from the body
b b b
Pure approaches to monocular event camera SLAM have frame to the world frame) of the event camera and IMU rig.
been proposed by Kim et al. [7] and Rebecq et al. [8]. The For practical reasons, we set the IMU frame equal to the
former is a filter-based method reconstructing photometric body frame, denoted by b. The constant extrinsic transfor-
depthmaps,whilethelatterisageometricapproachthataims mation Tc between IMU and event camera is assumed to be
b
atthereconstructionofavolumetriceventdensityfield[12]. wellcalibrated.Visual-inertialtrackingintroducesadditional
In an effort to increase the performance of event camera- variablestothestateestimation,leadingtotheexpandedstate
based SLAM, more recent methods have investigated the vector
use of line features [13], [14], [15]. The latter two methods
S =[Œ∏ ,p ,v ,b ,b ], (1)
are limited to local dynamics estimation, and all methods
i i i i Œ±i œâi
are restricted in that they require a sufficient number of line where Œ∏ is a minimal rotation representation, p is the
i i
features.TheinclusionofanIMUintotheincrementaltrack- position of the body frame, v is the velocity in the world
i
ing and motion framework is proposed by Zhu et al. [16], reference frame, and b and b are the accelerometer and
Œ±i œâi
Rebecq et al. [17], Le Gentil et al. [18], and Xu et al. [19]. gyroscope biases, respectively.Initialization SlidingWindowOptimization
No
HighFrequencyEventVision-onlyLocalization Factorgraph
Semi-denseMap
Optimization
Window No MeasurementPreProcessing
Size>5?
EventCamera TimeSurfaceMap SlidingWindow
Adaptive
Yes Yes
Keyframe Initialized? PosePrediction ¬∑¬∑¬∑ ¬∑¬∑¬∑
Initialize Determination
ùë£,b , b ,ùëá IMU IMUPre-integration
! "
Fig.2. Blockdiagramofthefullevent-basedvisual-inertialtrackingpipeline.ThesystemtakesstreamofeventsandIMUmeasurements(coloredblock)
as input and tracks against the reconstructed semi-dense map. The measurement processing module (Section III-B) dynamically choose keyframes and
processrawdatastreamintousablesingleframeobservations.Theinitialization(SectionIII-C)moduleutilizeshighfrequencyeventlocalizationresults
to provide bootstrapping states for subsequent prediction and optimization. The optimization module (Section III-D) tightly fuses IMU pre-integration
measurementsandTSMrepresentationstoachieveaccuratestateestimation.
A. System overview of inter-frame pre-integration terms adds regularization con-
straints between adjacent keyframes, and thereby transforms
We start by introducing the entire pipeline and describing
the registered set of keyframes into a virtual multi-camera
thefunctionalityofeachcoremoduleinourframework.The
rig with elastic extrinsic connections.
frameworkstartswithanoffline-constructed,inertial-aligned
semi-densepointcloudandtakesasinputastreamofevents
B. Measurement preprocessing
and IMU measurements from a calibrated event-inertial sen-
sor setup. A specially designed data collection module is re- In traditional visual-inertial tracking frameworks, it is
sponsible for the asynchronous collection of these two types common to select keyframes based on image rate given
of data thereby achieving maximum effectiveness. The data that conventional cameras are synchronously triggered and
collection module is followed by a measurement processing usually operate at frequencies lower than IMUs. Consider-
module that takes the quasi-continuous input stream of data ing a single event provides limited information, for event
and converts it into compact single frame observations (i.e. cameras we are equally inclined to preprocess events and
time surface maps and IMU pre-integration terms). The somehow group them in batches that are triggered within a
generated frames are henceforth referred to as keyframes. specific time interval so they can be jointly aligned against
Details on the intermediate, frame-based event representa- the map. In analogy to the work of Zuo et al. [9], we
tion are provided in Section III-B. Note that the rate at propose to summarize the contribution of entire intervals of
which keyframes are generated is adaptive and depending eventsinsnap-shotsoftheTimeSurfaceMap(TSM),which
on the actual number of events that have elapsed since can then be used to extract edges and perform individual
the last keyframe generation. The generation of IMU pre- framealignmentagainstthereprojectedsemi-densemap.The
integration terms adjusts to the asynchronous keyframe rate. question is at what rate such TSM-based keyframes should
The proposed dynamic, adaptive frame generation strategy be generated.
simultaneously satisfies two objectives. First, it avoids the 1) Adaptive keyframe determination: An event camera
generation of redundant keyframes when no actual change measures motion with respect to the scene, and the rate at
or camera motion has happened. Second, it avoids a lack of which events are generated is related to the actual amount
sufficient observations under fast displacements. of displacement. The fixed frequency keyframe selection
During the initialization phase (cf. Section III-C), we per- scheme proposed in previous works [21] therefore is sub-
formhigh-frequencyevent-onlysinglekeyframelocalization ideal and would lead to inferior optimization performance.
to retrieve initial body frame poses. When the displacement It would either lead to redundant frames if little motion
is sufficient, a closed-form visual-inertial solver is executed occurs,orinsufficientframedensityandlargeinter-keyframe
in order to give coarse initial values for IMU states to be baselinesunderfastdisplacements.ConsideringtheIMU,the
estimated, meaning the camera velocity and IMU biases. pre-integration also requires sufficient readings in order to
After successful bootstrapping, the tracking module (see ensure noise cancellation effects and good Signal-to-Noise
Section III-D) starts to predict poses for new keyframes Ratio (SNR). We therefore propose an adaptive keyframe
using a second-order motion model (i.e. using IMU forward generation scheme that buffers events and inertial readings
integration) and activates a tightly coupled event-inertial and‚Äîonly when a minimum number of both events and
back-endoptimizeremployingaslidingwindowoverafixed IMU readings has been surpassed‚Äîoutputs the accumulated
number of recent keyframes. Rather than only optimizing series of observations to the frame builder. The latter then
keyframe poses, this factor graph optimization module op- processes the raw data into TSMs and IMU pre-integration
timizes the full above-mentioned inertial states including terms.Thethresholdsfortheminimumnumberofeventsand
camera velocity and biases for each keyframe. The addition IMUobservationsaredenotedn andn ,respectively.
event imuThe time stamp of the keyframe is determined by the last where noise is modeled as Gaussian white noise, i.e. n ‚àº
Œ±
IMU measurement. N(0,œÉ2),n ‚àº N(0,œÉ2), and biases are modeled as a
Œ± œâ œâ
2) Event representation: As mentioned, we rely on snap- random walk.
shots of the Time Surface Map (TSM) for semi-dense regis- The pre-integration terms between consecutive frames are
tration. The latter is efficient to update and does not involve given by
costly feature extractors. It is defined as follows. Let us (cid:90)(cid:90)
assumeastreamofN eventsdenotedase ={x ,t ,p },i‚àà Œ±bk = Rbk(Œ±ÀÜ ‚àíb ‚àín )dt2
N, which includes location x
i
= {u i,v
i}i
,
timesi tami
p
i
t
i
and
bk+1
(cid:90)
t‚àà[tk,tk+1]
t t Œ±t Œ±
polarity p i ‚àà {‚àí1,1}. A time surface map is a 2D map in Œ≤bk = Rbk(Œ±ÀÜ ‚àíb ‚àín )dt
which the value at a pixel x is defined by the most recent bk+1 t‚àà[tk,tk+1] t t Œ±t Œ±
event occurring at that location. It is notably depending on (cid:90) 1
Œ≥bk = ‚Ñ¶(œâÀÜ ‚àíb ‚àín )Œ≥bkdt
an exponential decay kernel defined as bk+1 2 œât œâ t
t‚àà[tk,tk+1]
(cid:18) (cid:19)
T(x,t)=.
exp
‚àít‚àít last(x)
(2)
where
Œ¥ Ô£Æ Ô£π
(cid:20) ‚àí‚åäœâ‚åã œâ(cid:21) 0 ‚àíœâ z œâ y
where Œ¥ is a constant decay rate parameter typically set to a ‚Ñ¶(œâ)= ‚àíœâT√ó 0 ,‚åäœâ‚åã √ó =Ô£∞ œâ x 0 ‚àíœâ xÔ£ª
small value. t last(x) is the timestamp corresponding to the ‚àíœâ y œâ x 0
most recent triggered event at x. t is a given time stamp
Œ±bk ,Œ≤bk , and Œ≥bk represent the pre-integrated po-
for which the TSM is constructed. Note that in the context bk+1 bk+1 bk+1
sition, velocity and rotation measurements. A covariance
of our work, the TSM can be regarded as the historical
matrix Œ£ is also calculated during the pre-integration
trajectory of scene edges moving in the image plane, and bk,bk+1
process. In our practical implementation, we use first-order
sharpcontoursremainidentifiableatthereprojectionlocation
integration for processing discrete time IMU measurements.
of the currently observed edges in the scene. It is this
phenomenon that encourages a direct use of TSMs towards
C. Initialization
semi-dense registration.
For practical use, we usually use the inverse (or negated) Tightlycoupledvisual-inertialoptimization-basedtracking
TSM given by usually needs good initial values for states to achieve con-
vergence. The goal of the initialization module is to align
T(x,t)=1‚àíT(x,t). vision-based and IMU measurements to calibrate the bias
parameters of the IMU and provide initial states. Inspired
It has the advantage that the locations corresponding to the by [35], we employ a light-weight loosely coupled initial-
reprojectionofthecurrentlyobservededgesnowpresentthe ization method in our localization framework. In contrast to
lowest values in the map. As a result, the negated TSM can traditional VIO frameworks, the states to be bootstrapped in
be directly used as a cost field against which the reprojected our localization system are fewer and only include velocity,
semi-dense map can be registered in nonlinear optimization. biases and pose.
The relationship between an actual distance field and the The initialization module proceeds by first constructing
negated TSM is introduced in [21]. We may readily perform many intermediate TSMs between adjacent frames and then
nonlinear optimization on this geometric field. utilizing this information for high-frequency event-based
For the sake of efficient measurement preprocessing, we vision-only localization. This procedure will give initial
maintain a global surface of active events (SAE) [34] which poses for each frame, and requires the solution of indi-
stores the most recent event time stamp at each pixel. We vidual optimization problems that minimize the values of
also map the pixel values in the TSM from [0,1] to [0,255] the negated TSM field at the reprojected semi-dense pixel
for easy visualization and stable numerical computation. We locations for each frame. The problem formulation for an
make use of the truncated TSM in which all values below a individual frame is given by
given threshold Œ¥ are set to 0. Finally, we apply a Gaussian
(cid:88)
kernel on the TSM to further smoothify the field, reduce argmin œÅ(T(œÄ e(Rc bRT(Œ∏)(P j ‚àíp)+tc b))), (3)
ripple effects, and reduce the influence of outlier events. Œ∏,p Pj‚ààD
3) Imu pre-integration: The IMU provides measurements
where œÅ is a robust loss function (e.g. Huber loss), œÄ is
e
for acceleration Œ± and angular velocity œâ of the sensor at
b b a function which projects the 3D points in event camera
a high frequent rate. We follow the IMU sensor model and
frameontotheimageplane,R(¬∑)isthefunctionthatmapsa
continuous-time quaternion-based derivation of IMU pre-
minimal rotation representation to a 3-by-3 rotation matrix,
integrationtermsintroducedinpreviouswork[35][36].The
and P ‚ààD is a 3D world point within the current field of
j
raw IMU measurements Œ±ÀÜ , œâÀÜ are given by
t t view of the event camera.
When sufficiently many frames for bootstrapping have
Œ±ÀÜ =Œ± +b +Rt gw+n
t t Œ±t w Œ± been collected, we adopt the IMU measurement residual
œâÀÜ =œâ +b +n
t t œât œâ functionfrom[35]andlooselyaligntheIMUpre-integrationùê± ùê± ùê± ùê± ùê± ùê±
ùüé ùüè ùüê ùüë % ùê¢‚Äôùüè
Semi-dense Map
ùê± ùüé FixedNode ùê± ! StateNode IMUMeasurements IMUPre-integrationFactor Event-alignmentFactor EventMeasurements
Fig. 3. Factor graphrepresentation for our sliding window optimization. The graphfuses observations from the event camera, the IMU, andthe semi-
dense map. Two types of factors are introduced to construct the factor graph: (a) IMU pre-integration factors, (b) event alignment factors using TSMs.
TheformulationofthesefactorsarediscussedinSectionsIII-CandIII-D.Wefixthefirstnodetomaintainconsistencywithmaturednodesthatleftthe
window.
termswiththeevent-basedvision-onlyposeestimations.The window and the oldest frame is removed. It is furthermore
IMU measurement residual function is given by important to remain consistent with mature nodes that al-
Ô£Æ Ô£π readylefttheslidingwindowsuchthatasmoothertrajectory
RT(Œ∏)(p ‚àíp +1gw‚àÜt2‚àív‚àÜt)‚àíŒ±ÀÜi
Ô£Ø i i+1 i 2 i i i i+1 Ô£∫ can be maintained. Here we choose a simple solution that
Ô£Ø Ô£Ø RT(Œ∏ i)(v i+1+gw‚àÜt i‚àív i)‚àíŒ≤ÀÜi i+1 Ô£∫ Ô£∫ consists of fixing the first node in the graph. Combining
Ô£Ø Ô£∫
r(S i,S i+1)=Ô£Ø Ô£Ø 2[Q(Œ∏ i)‚àí1‚äóQ(Œ∏ i+1)‚äó(Œ≥ÀÜi i+1)‚àí1] xyz Ô£∫ Ô£∫, the residuals in (4) and (3), the objective of the event-based
Ô£Ø Ô£∫
Ô£Ø Ô£Ø
Ô£∞
b Œ±i+1‚àíb Œ±i Ô£∫ Ô£∫
Ô£ª
visual-inertial sliding window optimizer finally becomes
b ‚àíb N
œâi+1 œâi (cid:88) (cid:88)
(4) argmin œÅ(T(œÄ (RcRT(Œ∏ )(P ‚àíp )+tc)))
e b i j i b
whereQ(¬∑)transformstheminimalrotationrepresentationto S1,...,SNPj‚ààPi=1
a quaternion and Œ±ÀÜi ,Œ≤ÀÜi ,Œ≥ÀÜi represent the IMU pre- N‚àí1
i+1 i+1 i+1 (cid:88)
integration vector using raw IMU measurements. Given N + ||r(S i,S i+1)||2 Œ£i,i+1. (6)
frames, the loosely coupled visual-inertial alignment objec- i=1
tive is given by whereN istheslidingwindowsizeandP isthemaintained
N‚àí1 activatepointset.WeuseCeressolver[37]tosolveallabove
(cid:88)
argmin ||r(S ,S )||2 . (5) nonlinear problems.
i i+1 Œ£i,i+1
vi,bŒ±i,bœâi
i=1
IV. EXPERIMENTS
Note that we fix the keyframe poses and optimize the re-
We now proceed to the experimental evaluation of our
maining states only. Since the existing IMU pre-integrations
event-based visual-inertial tracking framework. All experi-
are based on the old states, they should be re-propagated
ments are conducted on the small scale sequences of the
using the optimized state once the initialization is finished.
publicly available VECtor dataset [38]. The dataset con-
D. Sliding window optimization tains event stereo cameras (Prophesee Gen3 CD) with VGA
The initialization module constructs a window of frames resolution (640√ó480), only the left one of which we use,
withinitialstates.Oncegiven,weuseasecond-ordermotion and a nine-axis IMU (XSens MTi-30 AHRS) at 200Hz.
model to obtain predictions for each new keyframe state. The ground truth is generated by the OptiTrack motion
In practice, the field of view of frames within the sliding capture system. All sensors are well synchronised though
window typically only covers part of the semi-dense map. a micro-controller unit at hardware level. We noticed that
In order to reduce the complexity of the optimization, we at certain moments in some sequences, there is only a
maintain a set of activate points. The set will keep the small overlap between the event camera‚Äôs field of view and
points visible in the last sliding window optimization. We the reconstructed semi-dense map, which leads to tracking
also construct a wider frustum for new keyframes using failures for all methods. Therefore, we apply clipping as
their predicted pose to select potentially visible new points necessary on certain sequences. We first compare the pro-
and merge them into the activate points set. We randomly posed EVIT framework against several state-of-art event-
sample a fixed-size subset P from this set of points to based localization approaches on both normal and challeng-
ensure the optimization maintain approximately constant ing sequences. To conclude, we conduct an ablation study
time complexity. usingdifferentmotionmodelstoanalyzetheimpactofusing
Fig.3illustratesthefactorgraphoftheconcludingtightly- IMU predictions on tracking performance. We provide both
coupled sliding window optimization framework. The opti- qualitative and quantitative results to show the robustness
mization is executed each time a new frame is added to the and effectiveness of our method.We use the evo odometry evaluation tool [39] to align a certain milestone is not reached. Since EVIT and EVPT
trajectories and conduct comparisons based on the RMSE all adopt dynamic frame determination schemes, the rate
oftheaveragetrajectoryerror(ATE)metric.Thetrajectories dependsonthemotionandthescene.Theactualrateislower
arealignedbyanSE(3)transformationderivedfromthefirst for mild motion and higher during aggressive motion, and
pair of the synchronized camera and ground truth poses. typicallyvaliesbetween30and60Hz.AsshowninTable.I,
though the improvement brought by EVIT is not significant
A. Evaluation on VECtor
on normal sequences, both EVT and EVIT outperform
In the first experiment, we compare the performance of EVPT. This is sensible given that photometric localization
the proposed system with existing event-based localization methods strongly rely on the consistency of the actual light
frameworks.Notethatthepresentworkisanextensionofthe conditionswhenthemapwasbuilt,whichisnotonlyhardto
event vision tracking module Canny-EVT presented by Zuo achieveforvaryingtimesoftheday,butalsohardtoachieve
et al. [9]. As mentioned in there, the EVT module relies on across different sensors. Furthermore, photometric methods
ahigh-frequencylocalizationmethodwhichtradesefficiency involve error metrics with no direct geometric meaning, and
for robustness, thereby resulting in an offline method with it is thus hard to ensure optimality of the motion estimation.
no real-time capabilities. We compare our proposed EVIT In terms of EVIT, it can be observed that the addition of
againstEVTrunningatdifferentratesandonthesamesemi- the IMU arguably offers only a limited improvement over
dense map. the event-only alternative when the motion is normal and
The global semi-dense map is constructed from the open- not exhibiting strong dynamics. However, EVIT generally
source framework [40], which uses the probabilistic semi- produces small errors, often leading to the absolutely best
densemappingtechniqueproposedbyMur-ArtalandTardos tracking result.
[41].Notethattheconstructededgesdonotnecessarilyneed
TABLEII
to be geometric edges in 3D and may still be appearance
edges. In order to avoid the scale ambiguity issue caused
ABSOLUTETRAJECTORYERROR(ATE)ONCHALLENGINGSEQUENCES.
by monocular slam, we replace each keyframe pose by
POSITION:[CM],ORIENTATION:[‚ó¶]
the corresponding ground truth pose. We also manually
30%sequence 50%sequence 100%sequence
transform the reconstructed semi-dense map to a global Sequence Method Frequency
Pos. Orient. Pos. Orient. Pos. Orient.
world reference frame where the negative z-axis direction
300hz 20.45 7.84 - - - -
aligns with the direction of gravity to ensure consistency EVT 100hz 22.66 8.56 - - - -
sofafast 30hz - - - - - -
with IMU data. EVPT (dyn) 25.29 6.53 26.32 6.77 23.24 6.18
EVIT (dyn) 9.02 4.02 8.11 3.60 7.08 3.22
TABLEI 300hz 3.76 2.83 4.12 2.92 - -
EVT 100hz 5.47 3.67 9.15 4.49 - -
ABSOLUTETRAJECTORYERROR(ATE)ONNORMALSEQUENCES.
robotfast 30hz - - - - - -
POSITION:[CM],ORIENTATION:[‚ó¶] EVPT (dyn) - - - - - -
EVIT (dyn) 3.22 2.67 3.24 2.80 3.85 2.98
300hz 4.03 2.47 4.52 2.35 14.65 7.95
Sequence Method Frequency 30%sequence 50%sequence 100%sequence EVT 100hz 2.24 2.19 2.48 2.52 - -
Pos. Orient. Pos. Orient. Pos. Orient. deskfast 30hz 5.13 3.35 15.4 7.23 - -
EVPT (dyn) 5.80 2.91 5.63 3.99 7.13 4.27
300hz 3.26 1.34 3.66 1.75 3.37 1.65 EVIT (dyn) 1.53 1.69 1.61 2.23 3.59 3.01
EVT 100hz 3.16 1.33 6.98 2.70 5.43 2.28
sofanormal 30hz 3.54 1.58 - - - -
EVPT (dyn) 15.64 5.08 15.27 5.50 - -
In contrast, the numerical results shown in Table. II illus-
EVIT (dyn) 3.23 1.40 3.23 1.56 3.15 1.53
300hz 0.91 0.89 1.04 1.08 1.09 1.31 trate a substantial improvement in accuracy and robustness
EVT 100hz 0.97 0.91 1.09 1.09 1.16 1.31
in challenging scenarios. The majority of the alternative
robotnormal 30hz 1.02 0.90 1.11 1.11 1.20 1.35
EVPT (dyn) 15.36 9.927 13.40 8.39 14.17 9.03 methods either fail to complete the entire sequence or result
EVIT (dyn) 0.84 0.89 1.04 1.10 1.00 1.14
in large localization errors. Fig. 4 visualizes a comparison
300hz 1.21 0.74 1.87 0.81 2.25 0.88
EVT 100hz 1.34 0.77 1.87 0.84 2.14 0.92 of actual trajectories from one of the more challenging
desknormal 30hz 1.21 0.77 1.92 0.87 2.30 0.96
EVPT (dyn) 7.71 1.83 11.02 2.61 16.08 3.33 sequences, demonstrating how all semi-dense comparison
EVIT (dyn) 1.32 0.80 1.79 0.87 2.22 0.94
methods that do not employ inertial readings fail after at
most 4s.
We also compare the EVIT with the photometric event-
B. Ablation study on motion model
based camera localization method proposed by Bryner et
al. [29]. We refer to it here as EVPT. The photometric Thetrackingprocessinthisworkisbasedonthenonlinear
mapisdirectlyconstructedusingtheRGBDdataandground optimizationtechniquewhichisoftensensitivetoinitialstate
truth poses contained in the dataset. We directly re-project given by motion model. Therefore, we also conduct an abla-
all valid pixels back into the 3D space and carefully polish tion study to evaluate the effect of different motion models.
themapbyhandtoreduceedgeeffectscausedbytheRGBD The latter can be adopted based on the system‚Äôs available
observations. sensors. When there is no internal motion information, a
Note that not all methods can track the entire sequence so simple strategy would be the initialize the pose of a newly
we define three milestones at 30%, 50% and 100% of the incoming keyframe with the pose of the previous one, a
sequence.Thecorrespondingerrorissimplynotdisplayedif paradigm we may refer to as a zeroth-order motion model.3 GT EVT-30hz -20 GT EVT-30hz
EVPT EVT-100hz EVPT EVT-100hz
2 EVIT EVT-300hz -40 EVIT EVT-300hz
-60
1 -80
0 -100
-120
-1 -140
2.5
-20
2 -30
-40
1.5 -50
1 -60
-70
0.5 -80
1
50
0.5
0 0
-0.5
-1 -50
-1.5
0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10
t [s] t [s]
(a) translationestimates (b) rotationestimates
Fig.4. Comparisonoftrajectoriesgeneratedbyvariouspureandinertial-supportedsemi-denseevent-basedtrackingsolutions,aswellasaphotometric
alternative.ThesequenceissofafastfromtheVECtorbenchmark[38].ThemethodproposedinthisworkiscalledEVIT.
TABLEIII
A more advanced model could be the first-order motion
ABSOLUTETRAJECTORYERROR(ATE)ONDIFFERENTMOTION
model(i.e.constantvelocitymodel),whichassumesthatthe
MODELS.POSITION:[CM],ORIENTATION:[‚ó¶]
velocity remains constant over small periods of time, and
thereby permits the initialization of a new keyframe with a
zeroth-order first-order second-order
relativeposetothepreviousonethatissimilartotherelative Sequence
pose between the previous keyframe and its preceding one. Pos. Orient. Pos. Orient. Pos. Orient.
Equipped with an IMU, we can introduce the second-order sofa fast - - - - 7.08 3.22
motion model, which uses acceleration and angular velocity
sofa normal - - 2.84 1.60 3.15 1.53
readings to propagate the previous pose to the current one.
robot fast - - - - 3.85 2.98
robot normal 1.08 1.32 1.46 1.23 1.00 1.14
desk fast - - - - 3.59 3.01
desk normal 3.75 1.59 2.77 1.16 2.22 0.94
(a) zeroth-ordermodel (b) first-ordermodel (c) second-ordermodel
multipleadjacentkeyframesthroughtheadditionofpair-wise
Fig. 5. Initial projection of semi-dense cloud on TSM using different regularizationterms.Theproposedslidingwindowalignment
motionmodels. shows benefits in tracking robustness as the contours in a
single TSM are often orthogonal to the projected instanta-
The three motion models are tested with the same tightly
neousdirectionofmotion,andthusdonotfullyconstrainan
coupled optimization back end. Fig. 5 shows qualitative
individual pose. Furthermore, the addition of the IMU leads
results for the different models, demonstrating how the
to better initializations and thereby improves tracking per-
second-order motion model provides better initial poses for
formance in highly dynamic situations without a substantial
the optimization back end. The full ATE results are listed
increase in frame rate. Interestingly, the proposed windowed
in Table. III. Although the differences are not significant on
trackingmethodcouldbeequallyappliedtoregularcameras.
mildsequences,itshowsthatevenunderthesameoptimiza-
tion scheme, the motion model has a crucial implication in VI. ACKNOWLEDGEMENT
fast-motion scenarios.
We would like to acknowledge the funding support pro-
vided by project 62250610225 by the Natural Science
V. CONCLUSION
Foundation of China, as well as projects 22DZ1201900,
In this work, we have introduced a novel event-inertial
22ZR1441300, and dfycbj-1 by the Natural Science Foun-
tracking framework that aligns with previous approaches
dation of Shanghai.
in that it proposes the registration of a semi-dense point
cloud with contours observed in the time surface map. REFERENCES
However, rather than simply aligning individual frames, the
[1] M.A.MahowaldandC.Mead,‚ÄúThesiliconretina,‚ÄùScientificAmer-
introductionofinertialsignalspermitsthejointalignmentof ican,vol.264,no.5,pp.76‚Äì83,1991.
]m[
X
]m[
Y
]m[
Z
]ged[
lloR
]ged[
hctiP
]ged[
waY[2] A.N.Angelopoulos,J.Martel,A.Kohli,J.Conradt,andG.Wetzstein, DynamicVisionSensor,‚ÄùinProceedingsoftheIEEE/RSJConference
‚ÄúEvent-basednear-eyegazetrackingbeyond10,000hz,‚ÄùIEEETrans- onIntelligentRobotsandSystems(IROS),2013.
actions on Visualization and Computer Graphics, vol. 27, no. 5, pp. [23] G. Chen, W. Chen, Q. Yang, Z. Xu, L. Yang, J. Conradt, and
2577‚Äì2586,2021. A.Knoll,‚ÄúAnovelvisiblelightpositioningsystemwithevent-based
[3] V.Rudnev,V.Golyanik,J.Wang,H.-P.Seidel,F.Mueller,M.Elgharib, neuromorphic vision sensor,‚Äù IEEE Sensors, vol. 20, no. 17, pp.
andC.Theobalt,‚ÄúEventHands:Real-timeneural3dhandreconstruc- 10211‚Äì10219,2020.
tionfromaneventstream,‚Äù2021. [24] E.Mueggler,B.Huber,andD.Scaramuzza,‚ÄúEvent-based,6-dofpose
[4] L.Xu,W.Xu,V.Golyanik,M.Habermann,L.Fang,andC.Theobalt, trackingforhigh-speedmaneuvers,‚ÄùinProceedingsoftheIEEE/RSJ
‚ÄúEventcap:Monocular3dcaptureofhigh-speedhumanmotionsusing ConferenceonIntelligentRobotsandSystems(IROS),2014.
aneventcamera,‚ÄùinProceedingsoftheIEEEConferenceonComputer [25] J. Bertrand, A. Yigit, and S. Durand, ‚ÄúEmbedded event-based visual
VisionandPatternRecognition(CVPR),2020. odometry,‚ÄùinIEEEInternationalConferenceonEvent-basedControl,
[5] Z. Zhang, K. Chai, H. Yu, R. Majaj, F. Walsh, E. Wang, U. Mah- Communication,andSignalProcessing(EBCCSP),2020.
bub, H. Siegelmann, D. Kim, and T. Rahman, ‚ÄúNeuromorphic high- [26] W.Chamorro,J.Andrade-Cetto,andJ.Sola`,‚ÄúHigh-speedeventcam-
frequency 3d dancing pose estimation in dynamic environment,‚Äù eratracking,‚ÄùinProceedingsoftheBritishMachineVisionConference
Neurocomputing,vol.547,p.126388,2023. (BMVC),2020.
[6] C.Cadena,L.Carlone,H.Carrillo,Y.Latif,D.Scaramuzza,J.Neira, [27] D. Reverter Valeiras, G. Orchard, S.-H. Ieng, and R. B. Benosman,
I.Reid,andJ.J.Leonard,‚ÄúPast,present,andfutureofsimultaneous ‚ÄúNeuromorphic event-based 3d pose estimation,‚Äù Front. Neurosci.,
localization and mapping: Toward the robust-perception age,‚Äù IEEE vol.9,p.522,2016.
TransactionsonRobotics(T-RO),vol.32,no.6,pp.1309‚Äì1332,2016. [28] G.Gallego,J.E.A.Lund,E.Mueggler,H.Rebecq,T.Delbruck,and
D.Scaramuzza,‚ÄúEvent-based,6-dofcameratrackingfromphotometric
[7] H.Kim,S.Leutenegger,andA.J.Davison,‚ÄúReal-time3dreconstruc-
depth maps,‚Äù IEEE Transactions on Pattern Analysis and Machine
tionand6-doftrackingwithaneventcamera,‚ÄùinProceedingsofthe
Intelligence(PAMI),vol.40,no.10,pp.2402‚Äì2412,2018.
EuropeanConferenceonComputerVision(ECCV),2016.
[29] S.Bryner,G.Gallego,H.Rebecq,andD.Scaramuzza,‚ÄúEvent-based,
[8] H.Rebecq,T.Horstscha¬®fer,G.Gallego,andD.Scaramuzza,‚ÄúEvo:A
direct camera tracking from a photometric 3d map using nonlinear
geometricapproachtoevent-based6-dofparalleltrackingandmapping
optimization,‚Äù in Proceedings of the IEEE International Conference
in real time,‚Äù IEEE Robotics and Automation Letters, vol. 2, no. 2,
onRoboticsandAutomation(ICRA),2019.
pp.593‚Äì600,2016.
[30] J.Engel,J.Sturm,andD.Cremers,‚ÄúSemi-densevisualodometryfor
[9] Y. Zuo, W. Xu, X. Wang, Y. Wang, and L. Kneip, ‚ÄúCross-Modal
amonocularcamera,‚ÄùinProceedingsoftheInternationalConference
Semi-Dense 6-DoF Tracking of an Event Camera in Challenging
onComputerVision(ICCV),2013.
Conditions,‚Äù IEEE Transactions on Robotics (T-RO), vol. 40, pp.
[31] L. Kneip, Z. Yi, and H. Li, ‚ÄúSDICP: Semi-dense tracking based on
1600‚Äì1616,2024.
iterativeclosestpoints.‚ÄùinProceedingsoftheBritishMachineVision
[10] Y.-F.Zuo,J.Yang,J.Chen,X.Wang,Y.Wang,andL.Kneip,‚ÄúDevo: Conference(BMVC),2015.
Depth-event camera visual odometry in challenging conditions,‚Äù in [32] M.KuseandS.Shen,‚ÄúRobustcameramotionestimationusingdirect
Proceedings of the IEEE International Conference on Robotics and edgealignmentandsub-gradientmethod,‚ÄùinProceedingsoftheIEEE
Automation(ICRA),2022. InternationalConferenceonRoboticsandAutomation(ICRA),2016.
[11] G.Gallego,T.Delbruck,G.Orchard,C.Bartolozzi,B.Taba,A.Censi, [33] Y. Zhou, H. Li, and L. Kneip, ‚ÄúCanny-vo: Visual odometry with
S. Leutenegger, A. Davison, J. Conradt, and K. Daniilidis, ‚ÄúEvent- rgb-d cameras based on geometric 3-d‚Äì2-d edge alignment,‚Äù IEEE
basedvision:Asurvey,‚ÄùIEEETransactionsonPatternAnalysisand TransactionsonRobotics(T-RO),vol.35,no.1,pp.184‚Äì199,2018.
MachineIntelligence(PAMI),vol.44,no.1,pp.154‚Äì180,2020. [34] R. Benosman, C. Clercq, X. Lagorce, S.-H. Ieng, and C. Bartolozzi,
[12] H. Rebecq, G. Gallego, and D. Scaramuzza, ‚ÄúEMVS: Event-based ‚ÄúEvent-basedvisualflow,‚ÄùIEEEtransactionsonneuralnetworksand
multi-view stereo,‚Äù in Proceedings of the British Machine Vision learningsystems,vol.25,no.2,pp.407‚Äì417,2013.
Conference(BMVC),2016. [35] T.Qin,P.Li,andS.Shen,‚ÄúVINS-mono:Arobustandversatilemonoc-
[13] W.Chamorro,J.Sola,andJ.Andrade-Cetto,‚ÄúEvent-basedlineslam ular visual-inertial state estimator,‚Äù IEEE Transactions on Robotics
in real-time,‚Äù IEEE Robotics and Automation Letters, vol. 7, no. 3, (T-RO),vol.34,no.4,pp.1004‚Äì1020,2018.
pp.8146‚Äì8153,2022. [36] S. Shen, N. Michael, and V. Kumar, ‚ÄúTightly-coupled monocular
[14] X. Peng, W. Xu, J. Yang, and L. Kneip, ‚ÄúContinuous Event-Line visual-inertial fusion for autonomous flight of rotorcraft MAVs,‚Äù in
ConstraintforClosed-FormVelocityInitialization,‚ÄùinProceedingsof Proceedings of the IEEE International Conference on Robotics and
theBritishMachineVisionConference(BMVC),2021. Automation(ICRA),2015.
[15] L.Gao,H.Su,D.Gehrig,M.Cannici,D.Scaramuzza,andL.Kneip, [37] S.Agarwal,K.Mierle,andT.C.S.Team,‚ÄúCeresSolver,‚Äù102023.
‚ÄúA 5-Point Minimal Solver for Event Camera Relative Motion Esti- [Online].Available:https://github.com/ceres-solver/ceres-solver
mation,‚ÄùinProceedingsoftheInternationalConferenceonComputer [38] L. Gao, Y. Liang, J. Yang, S. Wu, C. Wang, J. Chen, and L. Kneip,
Vision(ICCV),2023. ‚ÄúVector: A versatile event-centric benchmark for multi-sensor slam,‚Äù
[16] A.Z.Zhu,N.Atanasov,andK.Daniilidis,‚ÄúEvent-basedvisualinertial IEEERoboticsandAutomationLetters,vol.7,no.3,pp.8217‚Äì8224,
odometry,‚ÄùinProceedingsoftheIEEEConferenceonComputerVision 2022.
andPatternRecognition(CVPR),2017,pp.5391‚Äì5399. [39] M.Grupp,‚Äúevo:Pythonpackagefortheevaluationofodometryand
[17] H. Rebecq, T. Horstschaefer, and D. Scaramuzza, ‚ÄúReal-time visual- SLAM,‚Äùhttps://github.com/MichaelGrupp/evo,2017.
inertial odometry for event cameras using keyframe-based nonlinear [40] S. He, X. Qin, Z. Zhang, and M. Jagersand, ‚ÄúIncremental 3d line
optimization,‚Äù in Proceedings of the British Machine Vision Confer- segmentextractionfromsemi-denseslam,‚Äùin201824thInternational
ence(BMVC),2017. ConferenceonPatternRecognition(ICPR). IEEE,2018,pp.1658‚Äì
[18] C.LeGentil,F.Tschopp,I.Alzugaray,T.Vidal-Calleja,R.Siegwart, 1663.
andJ.Nieto,‚ÄúIDOL:Aframeworkforimu-dvsodometryusinglines,‚Äù [41] R. Mur-Artal and J. D. Tardo¬¥s, ‚ÄúProbabilistic semi-dense mapping
inProceedingsoftheIEEE/RSJConferenceonIntelligentRobotsand from highly accurate feature-based monocular slam.‚Äù in Proceedings
Systems(IROS),2020,pp.5863‚Äì5870. ofRobotics:ScienceandSystems(RSS),2015.
[19] W.Xu,X.Peng,andL.Kneip,‚ÄúTightFusionofEventsandInertial
MeasurementsforDirectVelocityEstimation,‚ÄùIEEETransactionson
Robotics(T-RO),vol.40,pp.240‚Äì256,2023.
[20] A.R.Vidal,H.Rebecq,T.Horstschaefer,andD.Scaramuzza,‚ÄúUlti-
mate SLAM? Combining events, images, and IMU for robust visual
SLAMinHDRandhigh-speedscenarios,‚Äùvol.3,no.2,pp.994‚Äì1001,
2018.
[21] Y.Zhou,G.Gallego,andS.Shen,‚ÄúEvent-basedstereovisualodome-
try,‚ÄùIEEETransactionsonRobotics(T-RO),vol.37,no.5,pp.1433‚Äì
1450,2021.
[22] A. Censi, J. Strubel, C. Brandli, T. Delbruck, and D. Scaramuzza,
‚ÄúLow-latency localization by Active LED Markers tracking using a