Point Prediction for Streaming Data
Aleena Chanda1, N. V. Vinodchandran2, Bertrand Clarke,1†
1Department of Statistics, U. Nebraska-Lincoln, 340 Hardin Hall North
Wing, Lincoln, 68583-0963, NE, USA.
2School of Computing, U. Nebraska-Lincoln, Avery Hall, 1144 T St
Suite 256, Lincoln, 68508, NE, USA.
Contributing authors: achanda2@huskers.unl.edu; vinod@unl.edu;
bclarke3@unl.edu;
†Corresponding author; this work was mainly done by the first author
under the supervision of the second and third.
Abstract
Wepresenttwonewapproachesforpointpredictionwithstreamingdata.Oneis
based on the Count-Min sketch (CMS) and the other is based on Gaussian pro-
cesspriorswitharandombias.Thesemethodsareintendedforthemostgeneral
predictiveproblemswherenotruemodelcanbeusefullyformulatedforthedata
stream. In statistical contexts, this is often called the M-open problem class.
Under the assumption that the data consists of i.i.d samples from a fixed dis-
tributionfunctionF,weshowthattheCMS-basedestimatesofthedistribution
function are consistent.
Wecompareournewmethodswithtwoestablishedpredictorsintermsofcumula-
tiveL1 error.OneisbasedontheShtarkovsolution(oftencalledthenormalized
maximum likelihood) in the normal experts setting and the other is based on
Dirichletprocesspriors.Thesecomparisonsarefortwocases.Thefirstisone-pass
meaningthattheupdatingofthepredictorsisdoneusingthefactthattheCMS
is a sketch. For predictors that are not one-pass, we use streaming K-means to
givearepresentativesubsetoffixedsizethatcanbeupdatedasdataaccumulate.
Preliminary computational work suggests that the one-pass median version of
the CMS method is rarely outperformed by the other methods for sufficiently
complexdata.WealsofindthatpredictorsbasedonGaussianprocesspriorswith
randombiasesperformwell.TheShtarkovpredictorsweuseheredidnotperform
as well probably because we were only using the simplest example. The other
predictors seemed to perform well mainly when the data did not look like they
came from an M-open data generator.
Keywords:hashfunctions,Countminsketch,Gaussianprocesses,Shtarkovsolution
1
4202
guA
2
]LM.tats[
1v81310.8042:viXra1 Problem Formulation
Consider a string of bounded real numbers, say yn = (y ,y ,··· ,y ,···) with y ∈
1 2 n i
[m,M] ∀i for some M > m > 0 and suppose our goal is sequential prediction. That
is, we want to form a good predictor Yˆ = Yˆ (yn) of y . This is often called
n+1 n+1
prediction along a string or streaming data when no assumptions can be made about
the distributional properties of the y ’s – apart, here, from boundedness.
i
One of the earliest contributions to this class of problems, chiefly in the classi-
fication context, was [10] and it was the main topic of the celebrated book by [4].
PredictionalongastringroughlycorrespondstoM-openproblemsinthesenseof[3],
see also [16]. When we say that there simply is no true model for the data, we are
effectively forced into the prequential setting of [8]; for a more recent exposition see
[23]. There is little systematic work on prequential prediction for M-open data even
though one can argue this is the most important setting.
HereweproposetwonewpredictorsforYˆ.Thefirstisbasedonusingyn toforman
estimatedempiricaldistributionfunction(EDF).Thatis,weusethedatauptoatime
n to define an EDF that we estimate. Our estimate is based on the Count-Min sketch,
see [7], extended to continuous random variables. This is based on the probabilistic
selection of hash functions. The reason to use the estimated EDF (EEDF) is that we
wanttobesurethatwecanshrinktheintervallengthinthehistogramgeneratedfrom
the Count-Min sketch so small that it gives an arbitrarily good approximation of the
EDF we use and hence DF if it exists. Our hash-based predictor (HBP) is effectively
a mean of the EEDF. In our computational work, we use multiple versions of our
Count-Min sketch based predictor.
OurCount-MinbasedpredictorwilloutperformtheusualEDFpredictorwhenthe
samplesizeandnumberofitemsinthestreamisverylarge.Thisispartiallyduetothe
presence of memory constraints. Briefly, the memory needed by the Count-Min sketch
canbechosenbytheuser.Morememoryproducesbetterqualitypredictorsandthere
are formal results ensuring the estimators are close to the true value when it exists.
In fact, by construction, estimators from the Count-Min sketch never underestimate
the true frequencies of elements so it favors high frequency elements even though
low frequency elements maybe overestimate albeit not by much. In essence, using the
Count-Min sketch gives a sort of data compression that we hope preserves only the
predictively important aspects of the data.
The second predictor we propose is based on Gaussian processes that have a ran-
dom additive bias. It has long been known that the posterior distribution can be
regarded as an input-output relation giving a distribution for a specific data set as if
deterministically, see [6], Sec. 3. This means Bayesian predictors can legitimately be
used. On the other hand, in M-open problems, we may not be able to identify useful
properties of the data generator. So, we want to prevent posterior distributions from
convergingandtherebymisleadingusintobelievingtheirlimit.ModifyingaGaussian
processtoincludearandombiashelpsensurethatunwantedconvergencewon’toccur.
There are a variety of other existing techniques for this class of problem. Perhaps
one of the earliest is the Shtarkov solution, see [20]), sometimes called the normal-
ized maximum likelihood. The Shtarkov solution is based on log-loss and requires the
analyst to choose a collection of ‘experts’, essentially parametric families, and tries to
2match the performance of the best of them. Different Shtarkov solutions result from
differentchoicesofexperts.ComputationalandtheoreticalworkontheShtarkovsolu-
tionisextensiveandoftenfromaverygeneralperspective,see[2]and [25].Moreover,
the log-loss is commonly used to define the concept of regret, see [24]. Here, we use a
Bayesian version of the Shtarkov solution that is easier to work with see [14] and [15].
The specific form of Shtarkov predictor we use here is based on normality and is a
ratio of Shtarkov solutions. Thus, it mimics a conditional density. It is not strictly a
conditionaldensitybecausetheShtarkovsolutiondoesnotsatisfythemarginalization
requirement for stochastic processes. However, the mode of the Shtarkov predictor
often performs well.
In addition, in our computational comparisons, we include two other well-known
Bayesianpredictors,onebasedonregularGaussianprocessesandonebasedonDirich-
let processes. Being Bayesian, both of these require prior selection and when needed
weuseanempiricalBayesapproach.Ingeneral,Bayesianmethodsassumeastochastic
modelforthedataandareexpectedtoperformbestwhenthemodelisapproximately
true but poorly otherwise.
One of the important features of sketches is that they can be implemented in one
pass. So, for the sake of completeness we compare one-pass versions of our predictors
when they exist to versions of our predictors formed by reducing past data to a rep-
resentativesubset.WedothisusingstreamingK-meansbutanystreamingclustering
algorithm that can give cluster centers could be used. In simple cases such as those
here where there are no explanatory variables and y is unidimensional the choice of
clustering procedure should not make much difference.
Other techniques that can in principle be applied to M-open data include various
formsofconformalprediction[1,21].However,itisnotclearhowtomeshthisapproach
with M-open data when the defining feature of conformal techniques is that future
data‘conforms’topastdata.Thesamecriticismappliestotimeseriesandotheronline
techniques, again with the caveat they could be used as long as they are regarded
strictlyasinput-outputrelationsratherthanhavinganynecessaryrelationtothedata
generator which is M-open.
Also, Bayesian techniques (especially nonparametrics [12]), score function based
techniques (see [13] [9]), and conventional frequentist point prediction can be used in
the M-open context provided these techniques are regarded strictly as input-output
relations or actions, i.e., as not having any modeling validity. Apart from DPP’s and
GPP’s, we have not included more of these various methods for space limitations
because they are so numerous.
As suggested by our focus on M-open predictive problems, methods designed for
streamingdatathatcannotbeeffectivelystochasticallymodeledtendtoperformbet-
ter than methods that aren’t. Accordingly, our computational work is consistent with
the proposal that there is some sort of matching principle between the complexity
of the data and the complexity of an optimal or at least good predictor. Among the
predictors we develop and study, one version of our HBP predictor (one pass, median
based) has the best performance in a cumulative L1 sense for the most complex data
we use. Second place among the methods we studied on the data sets we used goes
to a predictor based on Gaussian processes with a random bias. Shtarkov predictors
3can perform roughly comparably in some cases but are usually worse; we think this
is so because the Shatarkov predictor we use here is the simplest possible. Theoreti-
cal comparisons of these methods in the context that interests us are difficult because
they represent three conceptually different class of predictors.
In the next three sections we formally present the three classes of methods we
study here. All are applicable in streaming data settings where no stochastic model
canbeassumed.InSec.2,wedefineHBPmethodsandgivevariouspropertiesofthem
includingasortofconsistency,spacebound,and‘classical’convergenceproperties.In
Sec.3,wepresentourBayesianpredictors.WedefinestandardGaussianprocessprior
(GPP) predictors and extend them to the case that a random additive bias term is
included enabling us to derive a new class of GPP based predictors. We also define
Dirichlet process prior point prediction. In Sec. 4, we present our Shtarkov based
predictors, based on the normalized maximum likelihood in the normal case where
explicit expressions can be derived. Then in Sec. 5 we present our computational
comparisons. We conclude in Sec. 6 with some general observations.
2 Hash Function Based Predictors
WeadapttheCount-Minsketchalgorithmsoitcanbeusedwithrealdatatoestimate
an empirical distribution function. The idea is to partition the real data into intervals
of equal size and then compute the relative frequencies of the intervals as an approxi-
mation of the distribution function. We call techniques based on hash functions hash
based predictors (HBP’s). Once we define our predictors, we establish some of their
properties while preserving the M-open nature of the data.
2.1 The HBP Method
For the domain [K] = {1,2,··· ,K} and the range [W] = {1,2,··· ,W}, let Let
H⊆{h|h:[K]−→[W]} be a class of hash functions. Ideally, we would like H to be
entireclassoffunctions.However,incomputationalsettings,duetospacelimitations,
we assume H to be a 2-universal family. This means that for any x ,x ∈ [K] with
1 2
x ̸= x and y ,y ∈ [W], P (H(x ) = y ∩H(x ) = y ) = 1 , where H is the
1 2 1 2 H 1 1 2 2 N2
random variable varying over H with probability P uniform over h∈H.
H
Now we describe our predictor. We assume that the stream (y ,y ,...,y ,...) are
1 2 n
real numbers from the range (m,M] for some real m and M. Fix K ∈ N. Partition
(m,M] into K intervals each of length M−m. Further, for the rest of the discussion,
K
without loss of generality, we let m = 0. Denote the kth interval by I , for k =
Kk
1,2,··· ,K. That is,
(cid:18) (cid:21)
M M
I =I = (k−1) ,k . (2.1)
k Kk K K
The goal is to predict yn+1 after seeing (y ,··· ,y ). Let
1 n
a =a (n)=#{y ∈I |i=1,··· ,n}.
k Kk i k
That is, a (n) is the frequency of items in the stream (y ,··· ,y ) that fall in I .
k 1 n k
4Let h ,··· ,h be d randomly chosen hash functions where the domain is [K]
1 dK K
and the range is [W ]. That is, ∀j = 1,··· ,d ; h : {1,··· ,K} −→ {1,··· ,W }.
K K j K
Here d and W are parameters that can be chosen by the user. For effective data
K K
compression, or space efficiency, typically W << K. Next, we extend our hash
K
functions to the continuous domain (m,M] by defining
h˜ :(m,M]−→{1,2,··· ,W }
j K
where h˜ (s)=h (k) for s∈I . Note that when i≤n, y ∈I gives
j j k i k
h˜ (y )=h (k).
j i j
Denote the number of times the j-th hash function makes an error, i.e., h assigns
j
the same value to two different elements k,ℓ of its domain I by
k
(cid:40)
1, if h (k)=h (l);k ̸=l
I = j j (2.2)
kjl
0, otherwise.
Wetoleratethe(small)errorforthesakeofcompression.Next,weextend(2.2)tothe
interval case by writing
(cid:40)
1, if h˜ (s )=h˜ (s );s ≁s
I = j 1 j 2 1 2 (2.3)
s1,j,s2
0, otherwise,
where≁meansthats ands areindisjointsub-intervalsof(m,M].Welinktheextent
1 2
to which h is not one-to-one with the occurrence of y in the intervals by defining
j i
k
(cid:88)
X (n)= I a (n). (2.4)
kj s1,j,s2 l
l=1
In (2.4), as in (2.3), we think of s ∈I and s ∈I and note X ≥0. More precisely,
1 k 2 l kj
we see that X (n) is the number of elements e.g., y ’s, in the stream up to time n
kj i
that are not in I but still give h (k), i.e., h˜ (y )=h (k).
k j j i j
We next define an estimate of a (frequency of the kth interval) denoted by aˆ , at
k k
time n. For the jth hash function h , an interval k and time n, define count (j,h (k))
j n j
as follows:
count (j,h (k))=#{i≤n| h˜ (y )=h (k)}.
n j j i j
Forthejthhashfunctionletaˆ (n)=count (j,h (k)).Thentheestimateaˆ isdefined
jk n j k
as
aˆ (n)=min aˆ (n)≥0. (2.5)
k jk
j
5To find the next prediction, we use two methods. These are essentially weighted
means and medians which we define for the sake of being explicit. Given yn, the
predicted value for y is:
n+1
• the weighted mean of the midpoints of the intervals I ;k = 1,2,··· ,K defined in
k
(2.1), where the weights are aˆ as defined in (2.5). Let the mid-point interval I be
k k
m . Then,
k
K
yˆ =yˆ
=(cid:88)
m
aˆ k(n)
; (2.6)
n+1 K,n+1 k n
k=1
• the weighted median of m k with weights W K = (cid:80)K kaˆ =k 1aˆk is defined as the average
of m and m , where m satisfies
q q+1 q
q K
(cid:88) 1 (cid:88) 1
W ≤ and W ≥ . (2.7)
k 2 k 2
k=1 k=q+1
2.2 A Few Key Properties
Here we prove several important properties of our use of the Count-Min sketch to
estimatetheEDF.Thefirstisaboundonaˆ (n)intermsofa (n).Partofthenovelty
jk k
here is that the mode of convergence is defined by P , a distribution on the hash
H
functionsnotonthedata.WhenwewriteH wemeantherandomvariabledistributed
according to P that assumes values h. This allows us to preserve the assumption
H
thattheyn doesnothaveadistributionandhenceremainsM-open.Thatis,wetreat
the y as if they were real numbers with no stochastic properties. We indicate clearly
i
below the few cases where we depart from this to show ‘counterfactual’ results.
2.2.1 Bounds on error and storage
Our first theorem is that aˆ estimates the frequency of an interval I well, asymptoti-
k k
cally. Let ||a||
=(cid:80)K
a (n)=n be the sum over k of the number of elements y up
1 k=1 k i
to time n that land in I (K and n are suppressed in the notation ||a|| for brevity).
k 1
Our first result – for fixed K – is a consistency result for (2.5). After that, we provide
a ‘space bound’ on the storage requirement for the sketch. Both are similar to the
guarantees for the Count-Min sketch.
Theorem 1. ∀ϵ > 0 ∀δ > 0 : ∃ N ∀d > N such that P(∀j = 1,··· ,d ;aˆ (n) ≤
K K jk
a (n)+ϵ||a|| )≤δ.
k 1
Remark:Asneeded,weusethefactthata (n)≤aˆ (n)foranyn,byconstruction,
k k
without further comment.
6Proof. For ease of readability, we break the proof into three steps.
Step 1: E(I )=1.P (h (k)=h (l))
kjl j j
(cid:88)WK
= P (h (k)=v =h (l))
j j
v=0
indep(cid:88)WK
= P (h (k)=v)P (h (l)=v)
j j
v=0
u
=nif(cid:88)WK
1 1
=
W
K
+1
=
1
<
1
≤ϵ/e.
W +1W +1 (W +1)2 W +1 W
K K K K K
v=0
(cid:32) K (cid:33)
(cid:88)
Step 2: E(X (n))=E I a (n)
kj kjl l
k=1
K
(cid:88)
= a E(I )
l kjl
k=1
k
(cid:88)
< a ϵ/e=||a|| ϵ/e.
l 1
k=1
Step 3: P (aˆ jk(n)>a k(n)+ϵ||a|| 1)=P
(cid:0)
(cid:8)a
k(cid:8)(n(cid:8)
)+X kj(n)>(cid:8)a
k(cid:8)(n(cid:8)
)+ϵ||a||
1(cid:1)
=(P(X (n)>eE(X (n))).[Since,eE(X )<ϵ||a|| ]
kj kj kj 1
1
≤ E(X (n))=1/e.
eE(X (n)) kj
kj
Since, the aˆ (n)′s are independent (their hash functions are independent), if we
kj
set −logδ =d , we have
K
P(∀j;aˆ (n)>a (n)+ϵ||a|| )≤(1/e)dK <(1/e)log(1/δ) =δ. □
kj k 1
Remark: Here, ∥a∥ = n because we are looking at data streams in the ‘cash
1
register’ model, i.e., items only accumulate. We conjecture extensions to the turnstile
model can be given even if they do not seem relevant here.
Next, we address the storage requirement for the procedure used in Theorem 1.
Heuristically, observe that the storage is upper bounded by the number of hash func-
tionslog(1/δ)multipliedbythenumberofvalueseachhashfunctioncantake,namely
7e/ϵ or O(1/ϵ) giving O((1/ϵ)log(1/δ)). Following [17], we see in the following that
O(1/ϵ) will suffice.
Theorem 2. Let δ >0. If the storage available is Ω(1/ϵ)1, then,
P(aˆ ≤a +ϵ||a|| )≤δ
jk k 1
Proof. From [17], we get the proof for the space requirement for estimating the fre-
quency a of the distinct elements k of the stream with an error of ϵ||a|| . Since, our
k 1
set up is different from [17] in that we are looking at continuous data rather than dis-
cretedatawerewritethedistinctelements1,2,··· ,K in[17].Wewritethefrequency
vector a = a (n) = (a (n),a (n),··· ,a (n)) as the frequencies of the K intervals.
k 1 2 K
The estimated frequency in [17] can be rewritten as aˆ for any interval k. Now, ||a|| ,
k 1
which is the sum of all frequencies of the K intervals. Choosing K = 1/(2ϵ) in [17],
we recall that one needs to use Ω(1/ϵ) unit of space to obtain
(cid:16)1(cid:17)log(1) (cid:16)1(cid:17)log(1)
P(aˆ ≤a +ϵ||a|| )≤ δ < δ =δ;
jk k 1 2 e
see [17] for details.
2.2.2 Convergence of the EEDF in probability
We extend Theorem 1 by letting K,d ,n → ∞ at appropriate rates to get a con-
K
sistency result for the ‘estimated’ EDF (EEDF). That is, our EEDF converges to an
EDF2 basedonthestreamingdatathatisnotnecessarilythetrueDFsinceitneedn’t
exist. This is the content of our next result.
Theorem 3. Let y ∈(m,M]. Then, pointwise in y ,
i i
Fˆ (y )−F (y )→P 0 as d ,K,and n−→∞.
n i n i K
where the EEDF is Fˆ (y )=(cid:80) aˆk(n) and the EDF is F (y )=(cid:80) ak(n).
n i k≤yi n n i k≤yi n
Proof. We have
(cid:18) (cid:19)
E
aˆ k(n)
=
1 EmdK
incount (j,h (k))
n n j=1 n j
(cid:34) (cid:26) K (cid:27)(cid:35)
=
a k(n)
+
1
E
mdK
in
(cid:88)
I a (n)
n n j=1 k,j,l l
l=1
a (n)
= k +Γ,
n
1Ω-notationgivesalowerboundincontrasttobig-Onotationthatgivesanupperbound.
2Strictly speaking, our Fn is not the usual empirical distribution function. It is the estimator of a
distributionfunctionbasedonahistogramestimator.Wedothisforthesakeofreadability.
8(cid:20) (cid:26) (cid:27)(cid:21)
where Γ= 1E mindK (cid:80)K I a (n) . Setting j =1 gives an upper bound. So,
n j=1 l=1 k,j,l l
for any K, we have
(cid:34) K (cid:35)
1 (cid:88)
Γ≤ E I a (n)
n k,1,l l
l=1
K
=(cid:88)a l(n)
EI
n k,1,l
l=1
K (cid:18) (cid:19)
≤(cid:88) 1 a l(n)
W n
K
l=1
K
=
1 (cid:88)a l(n)
=
1
.
W n W
K K
l=1
Letting W →∞ gives that the RHS goes to zero. Thus, as W increases,
K K
(cid:18) (cid:19)
aˆ (n) a (n)
E k − k −→0
n n
and for each k
aˆ (n) a (n)
k − k →P 0.
n n
Hence,
(cid:88) aˆ k(n) − (cid:88) a k(n) →P 0, i.e.,Fˆ (y )−F (y )→P 0. □
n n n i n i
k≤yi k≤yi
Next, if the EDF based on streaming data actually has a limiting DF then the
EEDF and the EDF itself converge to F. For this Corollary, we are counterfactually
assuming the data stream is M-complete or -closed. Our result is the following.
Corollary 2.1. If y ∼F independently and identically for some DF F whose density
i
has a bounded derivative, then for any y ∈ [m,M], F (y ) −→ F(y ) and Fˆ (y ) →
i n i i n i
F(y ) in probability.
i
Remark: These statements are included for the sake of confirming intuition and
can be greatly generalized. In fact, since our application is to M-open data, we can
never make stochastic assumptions.
Proof. TheestimatorF istheEDFfromahistogramestimatoranddoesnotdepend
n
on hash functions – only on yn and K. Under the stated conditions, the histogram
estimator converges to the density of F in probability and hence Fˆ (y ) → F(y ) in
n i i
9theprobabilitydefinedbyF.ThesecondstatementfollowsfromusingTheorem3and
the triangle inequality, adding and subtracting F . The mode of convergence for the
n
terms is different; one is P and the other is F.
H
2.2.3 A streaming Glivenko-Cantelli theorem.
AversionofthestandardGlivenko-CantelliTheoremcanbefoundin[5].Thestructure
ofourproofbelowisbasedheavilyon[19].Ourresultforstreamingdatagoesbeyond
this by assessing convergence in the joint distribution of the hash functions and the
data. So, this result, like the last, must be interpreted counterfactually. For any DF
G we write G(y−) to mean its limit from the left at y .
i i
Theorem 4. Suppose the y ’s are independently and identically distributed according
i
to F and let y ∈ (0,M]. Then, as n → ∞, there are rates K ,d , and W → ∞
i n Kn Kn
so that the following Clauses are satisfied.
Clause I: If F (y )→P F(y ) and F (y−)→P F(y−) pointwise for all y ∈(0,M], then,
n i i n i i i
Sup|F (y )−F(y )|−→0 in probability.
n i i
yi∈R
Clause II: If Clause I holds, then Sup|Fˆ (y )−F(y )|a →.s. 0.
n i i
yi∈R
Clause III: The EEDF converges to the EDF, i.e., Sup|Fˆ (y )−F (y )|a →.s. 0 .
n i n i
yi∈R
Remarks:ClauseIistheusualGlivenko-CantellitheorembutformallyforF rather
n
than for the usual EDF. Clause II is for the CMS-generated EEDF to converge to F.
Becauseitinvolvesonlyempiricalquantities,ClauseIIIiswhatwereallywant,namely
the convergence of the EEDF to the EDF in a mode stronger than that used in Cor.
2.1.Again,weonlyshowthesimplest(IID)casesinceourpointisonlytodevelopthe
heuristics. These results can be greatly generalized to include many dependent data
settings, although this is not important for the use of our hash based predictor: our
interest is in the M-open case where such assumptions are irrelevant.
Proof. A complete proof is given in the Appendix, Sec. 7.
It is essential to remember that the randomness in Fˆ (y ) does not come from
n i
the data points y except when we used a distribution on y to prove results such as
those above. In fact, the randomness in Fˆ (y ) comes from the hash functions via the
n i
aˆ ’s. One of the points of Theorem 3 or Theorem 4 is that in principle we can obtain
k
asymptotically valid prediction intervals, not just point predictors, from an EDF or
EEDF, at least in the M-closed and -complete cases.
A useful property of the EEDF is that it can track the location of the data. For
example, if we have an initial set of n data points that follow a N(0,1) distribution,
the EEDF for these points will look like a N(0,1). However, if later points follow a
N(1,1) distribution, as they accumulate the EEDF will shift from N(0,1) to N(1,1).
The EEDF is adaptive in that it can reconverge to a new distribution.
103 Bayesian Predictors
In this section we define three Bayesian predictors. The first is the usual Gaussian
Process Prior predictors. The second is an extension of this to include a random
additive bias. The third is the usual Dirichlet Process Prior predictor, essentially the
Bayesian’s histogram possibly mimicking the EDF or EEDF. Predictive distributions
are well-known for the first and third of these; we review them here for the sake
of completeness. We provide full details for the second since it seems to be new.
Recallthatthesemustbeseenaspredictorsonly;thedatabeingM-openmeansthat
modeling e.g., by the convergence of a Bayes model, would be a contradiction.
3.1 No Bias
We assume Y = f +ϵ , i = 1,··· ,n where the ith data point y is distributed
i i i i
according to Y and f = (f ,f ,··· ,f )T is equipped with a Gaussian process prior.
i 1 2 n
That is, f ∼ N(a,σ2K ), where, a = (a ,a ,··· ,a )T is the mean and K =
11 1 2 n 11
(cid:18)(cid:16) (cid:17)(cid:19)
k ;i,j = 1,··· ,n is the covariance function in which k = k (y ,y ). First,
ij ij ij i j
we assume there is no bias i.e., a = 0 for all i, so the joint distribution of Y =
i
(Y ,Y ,··· ,Y )T and Y is
1 2 n n+1
     
Y f ϵ
1
. . .
 .  =  . + . 
 .   .   . 
Y f ϵ
n+1 n+1 n+1
  . 
  .
0 K +I . K
11 12
 .  . 
∼ N  . ,σ2 .  (3.1)
 .   ··· . ··· 
 0  . . 
K . K +1
21 22
where K = (k ,k ,··· ,k )T and K = KT , K = k . More
12 1,n+1 2,n+1 n,n 21 12 22 n+1,n+1
compactly, we write
Yn+1 ∼N(0n+1,σ2(I+K) ). (3.2)
n+1×n+1
It is well known that the predictive distribution of Y given Y is
n+1
Y |Y ∼ N(µ∗,Σ∗)
n+1
where
µ∗ = σ2K {σ2(K +I)}−1y =K {(K +I)}−1y (3.3)
12 11 12 11
and
Σ∗ = σ2(K +1)−K {σ2(K +I)}−1σ2K
22 21 11 12
11= σ2(K +1)−K (K +I)−1K (3.4)
22 21 11 12
Hence, in the zero bias case, our optimal point predictor (under squared error loss for
instance) is simply the conditional mean µ∗ in (3.3).
To complete the specification it remains to estimate σ2 for use in (3.4). In the
general case, we have Y ∼ N(a,σ2(I + K) n×n). Hence, (I + K)1 2Y ∼ N(a,σ2I).
LettingY′ =(I+K)21Y andS
k
= n−1 1(cid:80)n i=1(y i′−y¯′)k wecanestimateσ2 byS 2.Note
that σ2 cancels out in (3.3) and since we are only looking at point prediction in our
computations below we do not have to use (3.4).
3.2 Random Additive Bias
ConsideraGaussianprocesspriorinwhichthebiasa=(a ,...,a )T israndom.That
1 n
is, when we estimate function value – an f for i≤n – the prior adds a small amount
i
of bias effectively enlarging the range of the estimate. For the prediction of f a
n+1
similar sort of widening happens. To see this, write
a∼N(γ1 ,σ2δ2I ) (3.5)
n n×n
where the expected bias is γ ∈R and σ2 >0 has distribution
σ2 ∼IG(α,β). (3.6)
Here,α,β,andδarestrictlypositive,and,likeγ areunknown.Expression(3.5)means
that, with some loss of generality, the biases are independent, identical, symmetric,
unimodal, and have light tails. Since
Y ∼N(a,σ2(I +K )), (3.7)
n×n n×n
its likelihood is
L (a,σ2|y) = N(a,σ2(I +K ))(y)
1 n×n n×n
e− 2σ1 2(y−a)′(In×n+Kn×n)−1(y−a)
= (3.8)
(2π)n 2(σ2)n 2|I n×n+K n×n|21
and the joint prior for (a,σ2) is
w(a,σ2) = N(γ1,σ2δ2I ) IG(α,β)
n×n
= e− 2σ1 2(a (− 2γ π1 ))
n
2′( (δ σ2I 2n δ× 2n ))
n
2−1(a−γ1) Γβ (αα
)
×(cid:18) σ1 2(cid:19)α+1 e− σβ 2. (3.9)
Our first result is the identification of the posterior predictive density for Y
n+1
given yn. We have the following.
12Theorem5. Theposteriorpredictivedistributionofthefutureobservationy given
n+1
the past observations yn is
(cid:18) β∗∗ (cid:19)
m(y |yn)=St A , (y ), (3.10)
n+1 2α+n 1 2α+n n+1
2
where St (θ,Σ) denotes the Student’s t distribution with v degrees of freedom with
v
parameters θ and Σ. In (3.10), β∗∗ =β+A
2
and A
1
= γ2− γy 1′ng 1n . Expressions for g 1n,
γ , γ , and A are given in the proof and can be explicitly written as functions of the
1 2 2
variance matrix K , yn, γ, and δ.
n+1×n+1
Remark: Estimation of γ and δ will be discussed after the statement of Theorem
6 has been given below.
Proof. A complete proof is given in the Appendix, Sec. 7.
To use this result, we must have a way to obtain values for the hyperparameters
α, β, and δ and for the bias γ. Starting with α and β, recall (3.6) and define S =
k
1 (cid:80)n (y′−y¯′)k. For an inverse gamma we have
n−1 i=1 i
β
E(S ) = (3.11)
2 α−1
β2
Var(S ) = . (3.12)
2 (α−1)2(α−2)
Now, we can solve solve for α and β from (3.11) and (3.12) and invoke the method of
moments to find
S
αˆ ≈ 2 +2 (3.13)
S −S2
4 2
βˆ≈S (αˆ+1), (3.14)
2
where we have used the same estimate S of σ2 as in Subsec. 3.1.
2
To estimate the parameters γ and δ2 we form the likelihood L (y|γ,δ2,σ2) by
3
integrating out a from the product of (3.5) and (3.8) and maximize it. To do this,
we state a result that gives the forms of the likelihood we want to maximise, writing
the same likelihood in two different ways so the optimization will be clear. We also
use this result to estimate the parameters in the location and scale of the predictive
distribution in Theorem 5.
Theorem 6. The likelihood of yn given γ,δ2 and σ2, marginalizing out a, can be
written in two equivalent forms:
Clause I:
L (yn|γ,δ2,σ2) = h(γ) |V n×n|1 2
2 (2πσ2δ2)n 2|(I+K) n×n|21
(cid:104) (cid:8) (cid:9) (cid:105)
×e− 2σ1
2
y′n (I+K)− n×1 n+(I+K)− n×1 nVn×n(I+K)− n×1
n
yn
, (3.15)
13where
− 1
(cid:104) −2γy′n(I+K)− n×1 nVn×n1+γ21′(cid:16)
I
−Vn×n(cid:17) 1n(cid:105)
h(γ) = e 2σ2 δ2 δ2 δ4 . (3.16)
and Clause II:
L 2(yn|γ,δ2,σ2) = g(δ2)×
(2πσ2)n
21
|I+K|1
2e− 2σ1 2[y′n(I+K)− n×1 nyn], (3.17)
where
(cid:12)(cid:110) (cid:111)(cid:12)1
(cid:12) (I+K)−1 +(δ2I )−1 (cid:12)2
(cid:12) n×n n×n (cid:12)
g(δ2)= ×
(δ2)n
2
e2σ1
2(cid:104)
y′n(I+K)− n×1
n(cid:8)
(I+K)− n×1
n+(δ2In×n)−1(cid:9)−1
(I+K)− n×1 nyn+2 δγ 2y′n(I+K)− n×1
n(cid:8)
(I+K)− n×1
n+(δ2In×n)−1(cid:9)−1
1n(cid:105)
×e2σ1 2(cid:104) γ δ42 1′n(cid:8) (I+K)−1+(δ2In×n)−1(cid:9)−1 1−γ δ22 1′n1n(cid:105)
. (3.18)
Remark:ClauseIletsusfindthemaximumlikelihoodestimatorγˆ bylooking
MLE
only at h(γ) while Clause II lets us find δˆ by looking only at g(δ2).
MLE
Proof. A complete proof is given in the Appendix, Sec. 7.
To use Theorem 6 to find estimates of γ and δ2 we start with γ, taking logarithms
on both sides of (3.16) to get
(cid:34) (cid:32) (cid:33) (cid:35)
1 (I+K)−1 V γ2 V
lnh(γ)=− −2γy′n n×n n×n 1n+ 1′n I − n×n 1n . (3.19)
2σ2 δ2 δ2 n×n δ2
Differentiating (3.19) with respect to γ, and equating it to zero gives
d
log h(γ)=0.
dγ e
So we have that
2 y′n(I+K)−1 V1 2γ (cid:16) V (cid:17)
n×n − 1′n I − n×n 1n =0
2σ2 δ2 2σ2δ2 n×n δ2
(cid:16) V (cid:17)
=⇒ γ1′n I − n×n 1n =y′n(I+K)−1 V 1n
n×n δ2 n×n n×n
y′n(I+K)−1 V 1n
=⇒ γˆ = n×n n×n , (3.20)
(cid:16) (cid:17)
1′n I − Vn×n 1n
n×n δ2
14in which it is seen that σ2 does not appear. The second derivative is
(cid:32) (cid:33)
d2lnh(γ) 1 V
=− 1′n I − n×n 1n
dγ2 σ2δ2 n×n δ2
which is typically less than 0 because, as we will see, δ2 is usually small. Hence, our
solution to (3.20) will typically be a local maximum.
Next, we use (3.18) to help find a good estimate of δ. Unfortunately, we cannot
simplydifferentiateg(δ2),setthederivativetozero,andsolve.Theresultingequations
arejusttoocomplicatedtobeusefulinanyobviousway.So,wedidagridsearchover
interval I⊂R+ to maximize g. In computational work not shown here, we found that
the optimal δ ∈ I was almost always the left hand end point, even as I moved closer
and closer to 0. In the limit of δ →0, γˆ →0 as well. This suggests that the mean and
variance of the bias a are zero i.e., there is no bias.
Even though σ2 appears in (3.18), we always pragmatically set δ to be small so
that in our computations here the bias would not overwhelm the data. For instance,
we typically set δ =.1 and tested larger values like δ =1. When we recomputed with
larger values we typically found that the predictive error increased very slowly.
3.3 Dirichlet Process prior prediction
Suppose a discrete prior G is distributed according to a Dirichlet Process and
write G ∼ DP(α,G ) where α is the mass parameter and G is the base mea-
0 0
sure with E(F) = G . Then, by construction, we have the following; see [11].If
0
the sample space R is partitioned into A ,A ,··· ,A , then the random vec-
1 2 k
tor of probabilities (G(A ),G(A ),··· ,G(A )) follows a Dirichlet distribution, i.e.,
1 2 k
p(G(A ),G(A ),··· ,G(A )) ∼ Dirichlet(α(A ),α(A ),··· ,α(A )), where α(R) =
1 2 k 1 2 k
M, which we take here to be one.
Now, the posterior distribution of G(A ),G(A ), ··· ,G(A )|Y ,Y ,··· ,Y is also
1 2 k 1 2 n
Dirichletbutwithparametersα(A )+n where,n
=(cid:80)n
I(Y ∈A );j =1,2,...,k.
j j j i=1 i j
IfY′;j =1,2,··· ,k arethedistinctobservationsin{Y ;i=1,2,··· ,n},theposterior
j i
predictive distribution of Y |Y ,Y ,··· ,Y is
n+1 1 2 n
(cid:40) δ ,with probability nj ;j =1,2,··· ,k; and
Y n+1|Y 1,Y 2,··· ,Y
n
= Y j′ M+n .
G ,with probability M
0 M+n
Thus, our Dirichlet Process Prior (DPP) predictor is
k
Yˆ = (cid:88) y′ n j + M median(G ). (3.21)
n+1 jM +n M +n 0
j=1
4 Shtarkov Solution Based Predictors
We distinguish between the Shtarkov solution that solves a specific optimization
problem giving the normalized maximum likelihood estimator as the minimax regret
15solution and the Shtarkov predictor that is the ratio of two Shtarkov solutions. The
latter can be derived explicitly for the normal case when the variance is known and
we use it as one of our predictors.
Here,forthesakeofcompleteness,wegivetheShtarkovsolutionandpredictorsin
general. Then we look at special cases to present the predictor we actually use in our
computational comparisons.
4.1 The Shtarkov solution
Consider a game being played between Nature N and a Player P. P has access to
experts indexed by θ ∈ Θ ⊂ Rk. The goal of P is to make the best prediction of
the value that N issues. Let us consider the univariate case. Suppose P can call on
experts and they provide their best predictive distributions p(·|θ). After receiving
these, P announces the prediction q(·). In practice, P might choose q(·) to match the
performance of the best expert θ.
Assume the y ’s are from a univariate data stream y ,y ,... issued by N. N can
i 1 2
issue y ’s by any rule s/he wants or, here, by no rule, probabilistic or otherwise, since
i
we are regarding the y ’s as M-open. Regardless of how N generates data, after the
i
nth step, P′s cumulative regret with respect to expert θ is given by
1 1 p(yn|θ)
log −log =log ; (4.1)
q(yn) p(yn|θ) q(yn)
If P wants to minimize the maximum regret, s/he chooses
p(yn|θ) p(yn|θˆ)
q (yn)=argminsupsup log = (4.2)
opt q yn θ q(yn) (cid:82) p(yn|θˆ)dyn
where θˆ= θˆ(yn) where θˆis the maximum likelihood estimator, provided the integral
exists; see [20] and [18]. The normalized maximum likelihood q is called the (fre-
opt
quentist) Shtarkov solution. If a weighting function w(θ) across experts is given then
the Bayesian form of (4.2) is
w(θ˜(yn))p(yn|θ˜(yn))
q (yn)= (4.3)
opt,B (cid:82) w(θ˜(yn))p(yn|θ˜(yn))dyn
where θ˜is the posterior mode.
4.2 The Shtarkov Predictors
We take as our frequentist Shtarkov point predictor the mode of
q (yn+1)
q (y )= opt , (4.4)
Sht n+1 q (yn)
opt
16the ratio of two Shtarkov solutions. The analogous ratio denoted q (y ) using
Sht,B n+1
(4.3)givestheBayesShtarkovpointpredictor.Expression(4.4)lookslikeaconditional
density but in fact is just a distribution because Shtarkov solutions don’t marginalize
properly. Here, we use the mode of the numerator given that yn is fixed. The mode
turns out to be a good predictor – better than the mean or median because often
q is often highly skewed (see [16]). In a heuristic sense, q can be regarded as an
Sht Sht
approximation to a conditional density for y , i.e., q(y |yn) if it were to exist.
n+1 n+1
4.3 Special Cases
Different examples of Shtarkov solutions and predictors arise from choosing different
classes of experts – and weights in the Bayesian case. Here we limit ourselves to
exponential families. We record the following proposition without proof because it
follows from applying the definitions.
Proposition4.1. Letpbeafull-rankexponentialfamilyinthecanonicalparametriza-
tion η, i.e.,
p(y|η) = h(y)exp(ηTT(y)−A(η)), (4.5)
where T is the natural sufficient statistic and A is the normalizing constant. Then:
1. If µ(η)=E (T(Y)), we find that q is
η opt
p(yn|µ−1(1 (cid:80)n T(Y )))
q (yn) = n i=1 i .
opt (cid:82) p(yn|µ−1(1 (cid:80)n T(Y )))dyn
yn n i=1 i
(4.6)
2. Let p(η) be a conjugate prior for (4.5) with hyperparameters denoted β and ν. In
the Bayes case we have
p(ηˆ|β,ν)×p(yn|ηˆ)
q opt,B(yn) = (cid:82)
p(ηˆ|β,ν)×p(yn|ηˆ)dyn
(4.7)
yn
where ηˆ is µ−1 of the posterior mode.
Proof. Omitted.
InstancesoftheShtarkovpredictorcanbeworkedoutforseveralcasesusingProp.
4.1.Here,weonlyusethefrequentistnormalpredictorandonlyconsidertwocases:i)
µunknownandσ known,andii)bothµandσ unknown.Asweshallsee,theShtarkov
point predictor for these two cases is the same.
We start with case i). For data y ,y ,..., write y¯=y¯ for the sample mean from
1 2 n
the first n observations. The frequentist Shtarkov solutions for yn is the normalized
version of the maximum likelihood which for n+1 is
(cid:18) (cid:19)n+1
p(yn+1|µˆ n+1,σ2) = σ21
2π
2 e− 2σ1 2 (cid:80)n i=+ 11(yi−y¯n+1)2 (4.8)
17where µˆ =y¯ is the MLE. So, if we write
n
ny¯ +y
y¯ = n n+1. (4.9)
n+1 n+1
and use (4.9) in (4.8), we get
n
lnp(yn+1|µˆ ,σ2) = −n+1 ln(σ22π)− 1 (cid:88) y2+ 2ny¯ nny¯ n+y n+1
n+1 2 2σ2 i 2σ2 n+1
i=1
n (ny¯ +y )2 y2 2y ny¯ +y
− n n+1 − n+1 + n+1 n n+1
2σ2 (n+1)2 2σ2 2σ2 n+1
1 (ny¯ +y )2
− n n+1 . (4.10)
2σ2 (n+1)2
Taking the derivative, and setting it equal to zero, and re-arranging, gives that
solving d lnp(yn+1|µˆ |σ2)=0 leads to
dyn+1 n+1
(cid:20) 2 1 n+1 (cid:21) n2y¯ ny¯ 2 n
y − − = n + n − y¯ .
n+1 σ2(n+1) σ2 σ2(n+1)2 σ2(n+1)2 σ2(n+1)2 σ2n+1 n
Now, we find
n2y¯n + ny¯n − 2 n y¯
σ2(n+1)2 σ2(n+1)2 σ2n+1 n
yˆ = .
n+1 2 − 1 − n+1
σ2(n+1) σ2 σ2(n+1)2
=⇒ yˆ =
n( (n n+ +1 1) )y 2¯n − n2 +n 1y¯
n =−
nn +y¯n
1 =y¯ . (4.11)
n+1 2 −1− n+1 1 −1 n
(n+1) (n+1)2 n+1
Hence,thefrequentistShtarkovpointwisepredictorwithnormalexpertsissimplythe
mean, independent of the value of σ.
In case ii), where both µ and σ2 are unknown, we have µˆ = y¯ and, σˆ2 =
n n n
1 (cid:80)n (y −µ)2. Then,
n i=1 i
p(yn|µˆ ,σˆ2 ) =
nn 2e−n
2
n n (2π)n 2 (cid:80)n i=1(y i−y¯ n)2
1
∝ . (4.12)
(cid:80)n (y −y¯ )2
i=1 i n
Hence
p(yn+1|µˆ ,σˆ2 )∝ 1 . (4.13)
n+1 n+1 (cid:80)n+1(y −y¯ )2
i=1 i n+1
18Taking logarithms on both sides of (4.13), we have
n+1
lnp(yn+1|µˆ ,σˆ2 ) ∝ −ln(cid:88) (y −y¯ )2. (4.14)
n+1 n+1 i n+1
i=1
Using (4.9) in (4.14), we get
lnp(yn+1|µ,σˆ2
) ∝
−ln(cid:34) (cid:88)n (cid:18)
y −
ny¯ n+y
n+1(cid:19)2
n+1 i n+1
i=1
(cid:18)
ny¯ +y
(cid:19)2(cid:35)
+ y − n n+1
n+1 n+1
=
−ln(cid:34) (cid:88)n (cid:18)
y −
ny¯ n+y
n+1(cid:19)2
i n+1
i=1
1
(cid:18) (cid:19)2(cid:35)
+ ny −ny¯ . (4.15)
(n+1)2 n+1 n
Again, differentiating and setting the derivative equal to zero, i.e., solving
d lnp(yn+1|µˆ ,σˆ2 )=0, gives
dyn+1 n+1 n+1
n (cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)
(cid:88) 2 y − ny¯ n+y n+1 − 1 + 2n ny −ny¯ =0
i n+1 n+1 (n+1)2 n+1 n
i=1
=⇒ yˆ =y¯ . (4.16)
n+1 n
Hence,inthenormalmeancaseswegetthesamepointpredictor,thesamplemean
ofthepastdata.WeusethisinourcomputationsinSec.5.Shtarkovpointpredictors,
Bayesandfrequentist,canbefoundinmanyotherexponentialfamiliescases,butnot
in general in closed form.
5 Computational comparisons
To present our computational results we begin by listing our predictors. Then we
describe the settings for our comparisons. Finally, we present our computations and
interpret what they imply about the methods.
5.1 Our predictors
We computationally compare the predictors that have been presented in the earlier
sections. There were two predictors based on hash functions. These HPB’s used the
mean and the median of the empirical DF generated by the Count-Min sketch. They
wereexplicitlygivenby(2.6)and(2.7)inSec.2.TherewerethreeBayesianpredictors
namelyGPP’swithnobias,GPP’switharandomadditivebias,andDPP’s.Theywere
19givenin(3.3),Theorem5,and(3.21).Thepredictorin(3.3)requirestheestimationof
parameters as discussed in Subsec. 3.1. The predictor in Theorem 5 was denoted A
1
andwasnotgivenexplicitlybutgn andγ canbefoundfrom(7.33)whileγ iscanbe
1 1 2
foundfrom(7.34).TheestimationofparametersrequiredtouseGPP’switharandom
additive bias is discussed in Subsec. 3.2. The ‘parameter’ G in DPP predictors has
0
to be chosen and is user dependent. Finally, we used one frequentist Shtarkov point
predictor based on normality. It was simply the mean, as derived in Subsec. 4.3.
5.2 Settings for the comparisons
We compare point predictors by their cumulative L1 error. That is, for each method,
we have a sequence of errors |y −yˆ| where yˆ depends on y ,...,y (and possibly
i i i 1 i−1
a burn-in set D ) and we find the cumulative predictive error
b
n
1 (cid:88)
CPE =CPE(n)= |y −yˆ|. (5.1)
n i i
i=1
It is seen that
1
CPE(n+1)= (nCPE(n)+|y −yˆ |)
n+1 n+1 n+1
so it is easy to update the CPE from time step to time step. For each method, we
report the final CPE.
Since we are using HBP’s, it is natural to exploit the fact they can be computed
inonepass.WecandothisreadilyfortheShtarkovpredictorandtheDPPpredictor.
However,itisdifficulttodothisforeitheroftheGPPpredictorsbecausethevariance
matrix increases in dimension.
So, to include GPP’s in our comparisons we have to ensure the variance matrices
in the GPP predictors do not increase excessively. We do this by using what we call
a representative subset of fixed size that is updated from time step to time step.
Essentially,weusetheclustercentersfromstreamingK-meansforafixedchoiceofK,
here K = 200. Under streaming K-means, the cluster centers at time step n update
easily to give the cluster centers at time step n+1. We then use the cluster centers
at time n as the data to form our predictors for time n+1.
Thus, we have two sets of comparisons of CPE’s, one for the four methods that
can be implemented in one pass and another for all six methods using streaming
representative subsets. In fact, we compare all of them in an effort to understand how
the various methods behave.
We use different forms of the three predictors for different data sets. However, the
quantitiesthatmustbechosenarethesameinallcases.FortheHPBmethods(mean
and median), we must choose K, d , and W . For the Bayesian methods our choices
K K
are as follows. For GPP, we chose the variance matrix K to be of the form of a
n×n
20correlation matrix for an AR(1) time series. That is, for given correlation ρ we used

1 ρ ···
ρn−1
 ρ 1 ··· ρn−2 
K n×n = 

. .
.
. .
.
... . .
.
 

ρn−1 ρn−2 ··· 1
andsetρ=.8.FortheGPPwithrandombias,weusedA asourpointpredictorandso
1
onlyhadtofindvaluesforgn,γ ,γ ,δandσ.WelistedourchoicesattheendofSubsec.
1 1 2
3.2.ForDPP’swechosethebasemeasureG tobeaDiscreteUniformDistributionon
0
therange[min{y ,...,y },max{y ,...,y }].FortheShtarkovpredictorinthenormal
1 n 1 n
case, we got an expression involving data only. The predictor was fully specified once
the family of experts and weighting had been fixed.
Tofinishthegeneralspecificationsofourpredictors,weinitializedalloursequences
of predictions using 10% of the total data we intended to use. Thus, in the Columbia
rainfall data below where we had n = 5000, we used a burn-in of the first 500 data
points to form the predictions for each of the 501 time step. These then gave us the
first terms in our CPE’s for the ten methods.
5.3 Results
In this subsection we use the ten methods described in Subsec. 5.1 on four real data
sets.ThefirstthreearerainfalldatasetsfromthreejurisdictionsherecalledColumbia,
India, and Bhubaneshwar. Note that because our methods are designed for M-open
problems, simulated data will not be complex enough in general. Indeed, in other
computationalworknotshownhere,wefoundaverydifferentorderingoftechniquesby
performance.Thetechniquesdesignedforstreamingdataperformedrelativelypoorly.
Another comment: There are many classes of complex data and it is at this time
nearlyimpossibletocharacterizewheneach(orindeedany)methodworksbetterthan
the others. So, we have chosen a collection of data sets that, as will be seen, illustrate
our general points.
The Columbia data set can be found at https://data.world/hdx/
f402d5ef-4a74-4036-8829-f04d6f38c8e9. The dataset contains daily values of total
precipitation (mm) in Columbia over a period of four years ending in the year 2013.
They were collected from 27 different base stations and the ‘time’ index cycles
through them. We suggest this cycling will be typical of many kinds of streaming
data. We use the first 5000 rows of the value column of the dataset. This data set,
like India below, is not a pure time series – it’s as if there were a mixing distribution
over the stations. However, there is a pattern that would allow prediction so this is a
fair test of how well a predictor can perform on complex data.
The India data set is similar and can be found at https://data.world/hdx/
687c4f99-6ec6-4b30-ada2-a5a0f9eac629. Parallel to Columbia, this dataset contains
values of daily total precipitation (mm) cycling over 76 different base stations. Mea-
surementsoftotalprecipitationforatwoyearperiod(2010-2011)canbefoundinthe
dataset. Again we use the first 5000 rows of the value column of the dataset For the
21HBP’s computations with Columbia and India, d =10, W =50, and K =100. We
K K
setK =⌈n/50⌉inallcasesbutsimplychosed andW largerforlargersamplesizes.
K K
The Bhubhneshwar data set can be found at https://www.kaggle.com/datasets/
vanvalkenberg/historicalweatherdataforindiancities. It has daily precipitation data
(mm) from 01/01/1990 to 07/20/2022. The column prcp was used for getting the val-
ues of CPE. Rows with missing values were deleted leaving 6838 data points. For the
computations with Bhubhneshwar, d = 15, W = 50, K = 137. This data set like
K K
accelerometer below, is a time series.
The fourth data set is drawn from the phones accelerometer benchmark data that
can be found at [22], which provides a complete description. We extracted the first
10,000 rows of the data set and used the column “y” for our computing. We split the
data into four parts, i.e., sets of 2500 each, and computed the results. For the HBP
computations with accelerometer, d =10, W =20, and K =50.
K K
Inourtables,wefollowtheconventionthatthenumbersinbolddenotethesmall-
est errors and the asterisk (*) represents the second best. Headings indicate whether
the error in a column is from a one-pass method or used a representative subset
from K-means. We abbreviate the namesof our methods as Sht, DPP, GPP(RB) and
GPP(no RB) to mean the Shtarkov (Normal), Dirichlet process prior, and GPP with
and without random bias, respectively.
Turning to the numerical results, we begin with the CPE’s for Columbia given in
Table 1. In this case we see exactly the pattern of errors that we expect. Namely, the
one-passHBPmedianhasthelowesterrorandGPP(RB)hasthesecondlowesterror.
The other methods performed notably worse. We attribute the good performance of
GPP(RB)totheextraspreadfromtherandomadditivebiasandthepoorperformance
of Shtarkov to its extreme simplicity.
Onepass Representative
HBP HBP HBP HBP GPP GPP
Sht DPP Sht DPP
(Mean) (Median) (Mean) (Median) (RB) (noRB)
1006.8 944.8 986.8 989.1 1049.7 960.1 959.7 985.8 947.2* 1000.0
Table 1: Final CPE’s for the ten predictors using the Columbia rainfall data.
Table2presentsthefinalCPE’sfortheIndiadata.Itisseenthatthebestmethods
have CPE around 1050. These are one pass HBP median, one-pass Shtarkov, and
GPP (RB). The only possible surprise here is that one-pass Shtarkov is doing so well.
However, this does not contradict our basic inference that the top methods for the
class of data this example represents are one-pass HBP median and GPP(RB).
Onepass Representative
HBP HBP HBP HBP GPP GPP
Sht DPP Sht DPP
(Mean) (Med) (Mean) (Median) (RB) (noRB)
1231 1052 1049 1151 1120 1171 1227 1237 1050* 1066
Table 2: Final CPE’s for the ten predictors using the India rainfall data.
22Table 3 presents the final CPE’s for the Bhubhneshwar data. It is seen that again
the one-pass HBP median predictor is best and ties with the representative HBP
median predictor. Second place goes to GPP with no RB rather than GPP (RB),
although GPP (RB) comes in third.
Onepass Representative
HBP HBP HBP HBP GPP GPP
Sht DPP Sht DPP
(Mean) (Med) (Mean) (Median) (RB) (noRB)
9.619 7.102 9.633 9.934 10.09 7.102 10.077 9.609 8.613 7.460*
Table 3: Final CPE’s for the ten predictors using the Bhubhneshwar rainfall data.
To explain this, we suggest that the Bhubhneshwar data is slightly less complex
than the Columbia data and hence a little bit easier to predict. So, we plotted the
Columbia and Bhubhneshwar data as time series. This is shown in Fig. 1. Although
hardtoseeatthescaleofthisplot,theBhubhneshwardatashowsmoreregularitythan
the Columbia data which looks much more patternless. Since patterns can indicate
structure to improve prediction, the prediction problem of Bhubhneshwar may be a
little easier so that the random bias term does not help the GPP. The pattern in the
time series plot may also ensure that a representative subset from K-means really is
representative enough to help prediction substantially.
Table 4 presents the final CPE’s for the accelerometer data. The third and fourth
rows are in accord with our expectations, except that the two methods, one-pass
median HBP and GPP (RB) tie and tie with other methods including the Shtarkov
predictor with a representative subset.
Rows one and two bear some comment. First, row two shows that the one-pass
HBP median performs best but the GPP (RB) is third, being outperformed by the
DPP which is second. Second, row one shows GPP (RB) is best while one-pass HBP
median does worse than some methods (although better than others).
Onepass Representative
HBP HBP HBP HBP GPP GPP
Sht DPP Sht DPP
(Mean) (Median) (Mean) (Median) (RB) (noRB)
0.326* 0.337 0.770 0.335 0.339 0.347 0.328 0.335 0.308 0.339
0.045 0.032 0.204 0.042 0.155 0.070 0.076 0.040* 0.074 0.353
0.069 0.026 0.094 0.027* 0.065 0.026 0.029 0.026 0.026 0.395
0.084 0.026 0.099 0.027* 0.031 0.026 0.026 0.026 0.027* 0.383
Table 4: Final CPE’s for the ten cases using the accelerometer data. Each row of numbers
corresponds to a quarter of the first 10,000 data points, in order.
To explore rows one and two further we plotted histograms of the two quarters
of data in Fig. 2. The histogram of the first quarter is trimodal. It is possible that
this makes the data set broader and hence better captured by GPP (RB) because the
random bias matches the spread of the data more easily. The other methods, being
23Fig. 1: Left: Plot of the Columbia data as a time series. Right: Plot of the Bhubhneshwar
data as a time series.
more purely locations may just not be able to capture the modes at all – indeed it is
seenthattherangeoferrorsisnarrownamely[.326,.347]exceptforone-passShtarkov
that has error .770 and is completely insensitive to spread.
Thehistogramofthesecondquarterisunimodalwithroughlysymmetrictailsthat
donotlookheavy.Infact,itisclosetonormalapartfromalittlebitofleftskew.The
one-pass HBP median may simply converge relatively quickly to a distribution that
matchesthehistogramwhiletheotherconvergemoreslowlyorhavetheirperformances
harmed by the skew.
6 Discussion
Themaincontributionofthispaperistopresentandevaluateseveralpredictivetech-
niques for complex data, specifically M-open data. We presented two new techniques
a hash function based predictor using the Count-Min sketch and a Gaussian process
priorwithrandomadditivebiaspredictor.Wegavesomeofthekeypropertiesofthese
predictors. In addition, we derived a Shtarkov based predictor with normal experts.
We compared these predictors computationally. In our work we distinguished two
cases – methods that were one pass and methods that relied on a representative data
24Fig. 2: Plot of first two quarters of the accelerometer data.
set, hereobtained asthe cluster centers from streaming K-means. We argued thatfor
the most complex data, the one-pass hash function based median predictor typically
performed best in an L1 sense – at least for the data sets we used here. The Gaussian
process prior with random additive bias predictor sometimes tied with the one-pass
hash-based median predictor but more often performed slightly worse; perhaps this
occcurs because GPP’s represent more of a model than HBP’s do. The Shtarkov
predictor sometimes performed well but was likely too simple a version of the more
general class of Shtarkov predictors to perform well reliably. For the data we used, we
regard it as coming third place. The other methods were not generally intended for
streamingM-opendataandtypicallydidnotperformaswellasthesethreemethods.
An important caveat to all our conclusions is that the class of complex or M-
open data is huge and there are doubtless many subclasses that can be defined. The
essential and only common property the M-open data class has is that there is no
modelthatcanbeusefullyformulatedforit.Consequently,weregardalotofstreaming
data as M-open. The two new methods we have proposed here seem best designed
for this sort of data but we are as yet unable to be more precise as to which subclass
of M-open data our methods are most appropriate for. Indeed, our examples only
show that there is a large subclass of data sets for which our methods are possibly
best. In other work not discussed here we have found data sets where our methods
performpoorly.Usually,theseareobviouslynotM-opene.g.,M-closedmeaningthey
do follow a model, however complex, that we can identify. Hence we have not shown
any simulation results.
As a final point, we ask if there are data sets that are so complex that stochastic
assumptionsbecometoostrongforthemtosatisfy.Afterall,assumingadatagenerator
follows a probability model means that there are large collections of strings of data
25that get probability zero. Consider a sequence of real valued responses x ,...,x ,....
1 n
IftheyarefromaN(0,1)thenwecannothavex¯→cforanyc̸=0.Thatis,countable
sequences that are the limit points of sets having positive probability in the n-fold
samplespacesmustcorrespondtoahyperplaneinRℵ0.However,thevastmajorityof
Rℵ0 getsprobabilityzero.Thisconflictswithincreasingcomplexitybecauseweexpect
thatasaresponsebecomesmorecomplex,itshouldassumeagreaternumberofvalues.
Indeed, countable sequences with a limit are themselves only a fraction of the points
in Rℵ0 and it is conceivable that some data sources may routinely give outcomes that
simply do not have a limit in any commonly used sense.
References
R.Barber,E.Cand`es,A.Ramdas,R.Tibshirani(2023) Conformalpredictionbeyond
exchangeability. Ann. Statist., 51, 816-845.
A. Barron, Teemu Roos, Kazuho Watanabe (2014). Bayesian Properties of Normal-
ized Maximum Likelihood and its Fast Computation. Proc. IEEE International
Symposium on Information Theory. Honolulu, HI, 1667-1671.
J. Bernardo, A. F. M. Smith (2000) Bayesian Theory, John Wiley and Sons,
Chichester.
N. Cesa-Bianchi and G. Lugosi Prediction, Learning, and Games. Cambridge
University Press, Cambridge.
K. L. Chung (1974) A First Course in Probability Theory 2nd Ed. Academic Press,
San Diego.
C.-F. Chen (1985) On asymptotic normality of limiting density functions with
Bayesian implications. J. R. Stat. Soc. Ser. B, 47, 540-546.
G.CormodeandS.Muthukrishnan(2005) AnImprovedDataStreamSummary:The
CountpMin Sketch and Its Applications J. Algorithms, 55, 58-85.
A. P. Dawid (1984) Present Position and Potential Developments: Some Personal
Views: Statistical Theory: The Prequential Approach J. Roy. Stat. Soc. Ser. A,
147,278-292.
A. P. Dawid, M. Musio, L. Ventura (2016) Minimum Scoring Rules Inference. Scan.
J. Stat., 43, 123-138.
D. Haussler and A. R. Barron (1993). How well do Bayes methods work for on-line
prediction of + or -1 values? Computational Learning and Cognition: Proc. Third
NEC Research Symposium, SIAM, Philadelphia, pp.74-101.
S. Ghoshal (2010). The Dirichlet process, related priors and posterior asymptotics.
In: Bayesian nonparametrics Hjort, Holmes, Muller, and Walker Eds. 28–35.
26S.Ghoshal,A.vanderVaart(2017) FundamentalsBayesianNonparametricInference,
Cambridge University Press, Cambridge.
T. Gneiting (2011) Making and Evaluating Point Forecasts. J. Amer. Stat. Assoc.,
108, 746-762.
P. Kontkanen and P. Myllymaki (2007). A linear-time algorithm for computing the
multinomial stochastic complexity. Inform. Process. Lett., 103, 227–233.
T. Le and B. Clarke (2016) Using the Bayesian Shtarkov solution for predictions.
Comp. Stat. and Data Analysis, 104, 183–196
T. Le and B. Clarke (2017) A Bayes Interpretation of Stacking for M-Complete and
M-Open Settings. Bayesian Anal., 12, 807-829.
S. Muthukrishnan, S. (2009) Data stream algorithms. The Barbados Workshop on
Computational Complexity.
J. Rissanen (1996) Fisher Information and Stochastic Complexity. Trans. Inform.
Theory, 41, 40-47.
A. Shaikh (2009) https://home.uchicago.edu/∼amshaikh/classes/topics winter09.
html and https://home.uchicago.edu/∼amshaikh/webfiles/glivenko-cantelli.pdf.
Last accessed 19 May 2024.
Y. Shtarkov (1988). Universal sequential coding of single messages. Translation from
Problem of Information Transmission, 3-17. San Mateo, Calif.: Morgan Kaufmann.
G. Shafer, V. Vovk (2008) A Tutorial on Conformal Prediction J. Machine Learning
Res., 9, 371-421.
Blunck, Henrik; Bhattacharya, Sourav; Prentow, Thor; Kjrgaard, Mikkel ; and
Dey, Anind. (2015). Heterogeneity Activity Recognition. UCI Machine Learning
Repository. https://doi.org/10.24432/C5689X.
V. Vovk, A. Shen (2001) Prequential Randomness and Probability. Theoretical
Computer Science, 411, 632-646.
Q. Xie and A. Barron (2000). Asymptotic minimax regret for data compression,
gambling, and prediction. IEEE Trans. Inform. Theory, 46, 431–445.
X. Yang and A.R. Barron (2017) Minimax compression and large alphabet approx-
imation through Poissonization and tilting. IEEE Trans. Inform. Theory, 63,
2866-2884.
277 Appendix
In this Appendix we present the proofs from Subsubsec. 2.2.3 and Subsec. 3.2. We
begin by proving Theorem 4. Then we turn to proofs for Theorem 5 and Theorem 6.
7.1 Proof of Theorem 4
Proof. Throughout the proof we treat n, K, d , and W as fixed but large, only
K K
letting them increase compatibly at the end of each result. We begin with the proof
of Clause I; it requires the following.
Lemma 7.1. Let F be an arbitrary distribution function on [m,M] and let ϵ > 0.
Then, there is a finite partition of the form m=u <u <···<u =M so that for
0 1 L
0≤j ≤L−1 we have
F(u− )−F(u )≤ϵ.
j+1 j
Proof of 7.1: For j ≥0; define
u =Sup{v :F(v)≤F(u )+ϵ}. (7.1)
j+1 j
From Eqn (7.1), we have F(u ) ≥ F(u )+ϵ. Thus the height of jump between
j+1 j
F(u ) and F(u ) is at least ϵ. This can happen only a finite number of times and
j+1 j
so we get the partition of the desired form. □
Proof of Clause #1.Forϵ>0,wemustfindN =N(ϵ)sothatforn>N theevent
sup|F (y )−F(y )|<ϵ (7.2)
n i i
yi
has probability going to one. Consider a finite partition of [m,M] as in Lemma 7.1
giving
ϵ
F(u− )−F(u )≤ . (7.3)
j+1 j 2
for0≤j ≤L−1.Now,fory ∈[m,M]thereisaj sothatu ≤y ≤u .So,wehave
i j i j+1
F (u )≤F (y )≤F (u− ) (7.4)
n j n i n j+1
and
F(u )≤F(y )≤F(u− ). (7.5)
j i j+1
Together,(7.5) and (7.4) imply
F (u )−F(u− )≤F (y )−F(y )≤F (u− )−F(u ).
n j j+1 n i i n j+1 j
Re-arranging gives
F (u )−F(u )+F(u )−F(u− )≤F (y )−F(y ) (7.6)
n j j j j+1 n i i
and
F (u− )−F(u− )+F(u− )−F(u )≥F (y )−F(y ). (7.7)
n j+1 j+1 j+1 j n i i
Using (7.3) in (7.6) and (7.7) we have
28F (u )−F(u )− ϵ ≤F (y )−F(y )
n j j 2 n i i
F (u− )−F(u− )+ ϵ ≥F (y )−F(y ).
n j+1 j+1 2 n i i
For each j, let N = N (ϵ) be such that for n > N we have F (u )−F(u ) > −ϵ
j j j n j j 2
and let M =M (ϵ) be such that for n>M we have
j j j
F (u−)−F(u−)< ϵ.
n j j 2
WriteN = max {N ,M }.Then,forn>N andforanyx∈[m,M]wegetthat(7.2)
j j
1≤j≤K
has probability going to zero. Hence, Clause #1 follows.
The proof of Clause II requires the following.
Lemma 7.2. Suppose F and F are DF’s on [m,M] so that F (y )−→F(y ) point-
n n i i
wiseinprobabilityfor y ∈Q.SupposealsothatthejumppointsofF arewell-behaved,
i
i.e., F (y )−F (y−) −→ F(y )−F(y−) pointwise in probability for all jump points
n i n i i i
of F. Then, for all y , we have
i
F (y )−→F(y )
n i i
and
F (y−)−→F(y−).
n i i
in the probability on the data.
Proof of Lemma 7.2: For any y ∈(0,M] and w,z ∈Q with w <y <z. We have
i i
F (w)≤F (y )≤F (z).
n n i n
So, if y is a continuity point we have
i
F(w)≤ess.liminfF (y )≤ess.limsupF (y )≤F(z).
n i n i
n−→∞ n−→∞
and the lemma follows. Likewise, if y is a jump point of F we have
i
F (w)+F (y )−F (y−)≤F (y )≤F (z)
n n i n i n i n
and hence taking convergences in the distribution for the data
F(w)+F(y )−F(y−)≤essliminfF (y )≤esslimsupF (y )≤F(z).
i i n i n i
n−→∞ n−→∞
Since lim F(w)=F(y−) and lim F(w)=F(y ) the first part follows.
i i
w→y− z→y+
i i
To show F (y−)−→F(y−) in probability, let x be a continuity point of F. Since,
n i i
F (y−)≤F(y ), it follows that
n i i
esslimsupF (y−)≤esslimsupF (y )≤F(y )≤F(y−).
n i n i i i
n−→∞ t−→∞
Again, for any w ∈Q with w <y , we have F (w)≤F (y−) which implies that
i n n i
F(w)≤essliminfF (y−).
n i
n−→∞
29Since lim F(w) = F(y−) the statement follows. Finally, support again that y is a
i i
w−→y−
i
jump point of F. By assumption F (y )−F (y−)−→F(y )−F(y−) and it has been
n i n i i i
shown that F (y )−→F(y ). Thus, F (y−)−→F(y−). □
n i i n i i
Proof of Clause #2. We must show that there is a set N with P{N} = 0 so
that for all w ∈/ N (i) Fˆ (y ) −→ F(y ) for y ∈ Q and (ii) Fˆ (y )−Fˆ (y−) −→
n i i i n i n i
F(y )−F(y−)foralljumppointsofF,bothinthejointprobabilitiesofP ofH and
i i H
P of the data where P has DF F.
To do this, recall that we have Fˆ (y ) −→ F(y ) in probability from Cor. 2.1.
n i i
Since,Fˆ (y ) is bounded that will imply L convergence which again implies conver-
n i 1
gence almost surely. So, for each y ∈ Q, let N be a set satisfying P{N } = 0 and
i yi yi
Fˆ (ω;y ) −→ F(y ) for all ω ∈/ N . Define N = (cid:83) N . By construction, for all
n i i yi 1 yi
yi∈Q
ω ∈/ N , Fˆ (ω;y )−→F(y ). Since, Q is countable, P{N }=0.
1 n i i 1
Next, define J as the set of jump points of F of size at least 1/i; for i ≥ 1. For
i
(cid:83)
eachi,J isfinite.Now,J = J isthesetofalljumppointsofF.LetM satisfy
i i yi
1≤i≤∞
P{M }=0 so that for all ω ∈/ M we have Fˆ (ω;y )−Fˆ (ω;y−)−→F(y )−F(y−)
yi
(cid:83)
yi n i n i i i
in probability. Let N = M . Since J is countable, P{N }=0.
2 yi 2
yi(cid:83)∈J
To finish, set N = N N so that by construction for ω ∈/ N, (i) and (ii) hold.
1 2
Hence Clause I and Lemma 7.2 give Clause II.
Proof of Clause #3.This proof is similar to the proof of Clause #2. First
note that Cor. 2.1 gives Fˆ (y ) −→ F (y ) in probability. Now, since Fˆ (y ) is a
n i n i n i
bounded function, convergence in probability implies convergence in L norm and,
1
hence, convergence almost sure.
TostrengthenthemodeofconvergencetoKolmogorov-Smirnovdistance,beginby
defining T to be a set such that P{T }=0 and for all ω ∈/ T , Fˆ (y )−→F (y ).
yi yi yi n i n i
Also define T = (cid:83) T . As before, P{T } = 0. Thus, for all ω ∈ T , Fˆ (ω;y ) −→
1 yi 1 1 n i
yi∈Q
F (y ).
n i
Again as before, let J be the set of jump points of F of size at least 1/i so that
i n
(cid:83)
J is finite for each i and let J = J . Now, for each y ∈J, let M denote a set
i i i yi
1≤i≤∞
such that P{M } = 0 and for all ω ∈/ M , Fˆ (y )−Fˆ (y−) −→ F (y )−F (y−).
y (cid:83)i yi n i n i n i n i
Also, let T = M . Since, J is countable, P{T }=0.
2 yi 2
yi∈J
To finish, let T = T (cid:83) T . By construction, for ω ∈/ T, Fˆ (y ) −→ F (y ) and
1 2 n i n i
Fˆ (y )−Fˆ (y−)−→F (y )−F (y−). So, by Clause I of the Theorem and by Lemma
n i n i n i n i
7.2 Clause III follows. □
7.2 Proof of Theorem 5
Proof. We use p generically to indicate probability densities. We use w when we want
to emphasize that a density is a prior or posterior and m to emphasize that a density
is a mixture of densities for its indicated arguments. Now, the posterior density for
30an,σ2|yn is given by:
L (a,σ2|yn)×w(an,σ2) p(yn,an,σ2)
p(an,σ2|yn) = 1 = . (7.8)
m(yn) m(yn)
. We know that
Yn ∼N(an,σ2(I +K )) (7.9)
n×n n×n
and
w(an,σ2) = N(γ1n,σ2δ2I ) IG(α,β)
n×n
= e− 2σ1 2(an− (2γ1 πn ))
n
2′( (δ σ2I 2n δ× 2n ))
n
2−1(an−γ1n) Γβ (αα )(cid:18) σ1 2(cid:19)α+1 e− σβ 2. (7.10)
From (7.9) and (7.10) the numerator in (7.8) is given by
p(an,σ2|yn) =
e− 2σ1 2(an−γ1n)′(δ2In×n)−1(an−γ1n)
(2π)n 2(σ2δ2)n
2
×
βα (cid:18) 1 (cid:19)α+1
e− σβ 2
e− 2σ1 2(yn−an)′(I+K)− n×1 n(yn−an)
Γ(α) σ2 (2π)n 2(σ2)n 2|(I+K) n×n|21
βα
=
(2π)n 2+n 2|(I+K) n×n|1 2(δ2)n 2Γ(α)
× e− σ1 2[β+1 2(yn−an)′(I+K)− n×1 n(yn−an)+1 2(an−γ1n)′(δ2In×n)−1(an−γ1n)]
(cid:18)
1
(cid:19)α+1+n 2+n
2
× .
σ2
(7.11)
We simplify the terms in the exponent in (7.11) as follows. It is
1 1
β+ (yn−an)′(I+K)−1 (yn−an)+ (an−γ1n)′(δ2I )−1(an−γ1n)
2 n×n 2 n×n
1 1 1 1
= β+ y′n(I+K)−1 yn− a′n(I+K)−1 yn− y′n(I+K)−1 an+ a′n(I+K)−1 an
2 n×n 2 n×n 2 n×n 2 n×n
1 1 1 1
+ a′n(δ2I )−1an− γ1′n(δ2I )−1an− γa′n(δ2)−11n+ γ21′n(δ2I )−11n
2 n×n 2 n×n 2 2 n×n
1
= β+ a′n[(I+K)−1 +(δ2I )−1]an−a′n[(I+K)−1 yn+γ(δ2I )−11n]
2 n×n n×n n×n n×n
1 1
+ y′n(I+K)−1 yn+ γ21′n(δ2I )−11n
2 n×n 2 n×n
1
= β+ a′n[{(I+K)−1 +(δ2I )−1}−1]−1an
2 n×n n×n
−a′n[{(I+K)−1 +(δ2I )−1}−1]−1[(I+K)−1 +(δ2I )−1]−1
n×n n×n n×n n×n
311 1
[(I+K)−1 yn+γ(δ2I )−11n]+ y′n(I+K)−1 yn+ γ21′n(δ2I )−11n.
n×n n×n 2 n×n 2 n×n
So
w(an,σ2|yn)
βα (cid:18) 1 (cid:19)α+1+n 2+n 2
= ·
(2π)n 2+n 2|(I+K) n×n|1 2(δ2)n 2Γ(α) σ2
×e− σ1 2[β+1 2a′n[{(I+K)− n×1 n+(δ2In×n)−1}−1]−1an]
×e− σ1 2[−a′n[{(I+K)− n×1 n+(δ2In×n)−1}−1]−1[(I+K)− n×1 n+(δ2In×n)−1]−1][(I+K)− n×1 nyn+γ(δ2In×n)−11n]
×e− σ1 2{1 2y′n(I+K)− n×1 nyn+1 2γ21′n(δ2In×n)−11n]}
βα
=
(2π)n 2+n 2|(I+K) n×n|1 2(δ2)n 2Γ(α)
×(cid:18)
1
(cid:19)α+1+n 2+n
2 e− σ1 2{β+1 2y′n(I+K)− n×1 nyn+ 21γ21′n(δ2In×n)−11n]}
σ2
×e− σ1 2[1 2a′n[{(I+K)− n×1 n+(δ2In×n)−1}−1]−1an
×e− σ1 2[−a′n[{(I+K)− n×1 n+(δ2In×n)−1}−1]−1[(I+K)− n×1 n+(δ2In×n)−1]−1][(I+K)− n×1 ny+γ(δ2In×n)− (71 .11n 2].)
So, if we set
V = [(I+K)−1 +(δ2I )−1]−1 (7.13)
n×n n×n n×n
µ = [(I+K)−1 +(δ2I )−1]−1[(I+K)−1 yn+γ(δ2I )−11n]
n×n n×n n×n n×n
= V [(I+K)−1 yn+γ(δ2I )−11n] (7.14)
n×n n×n n×n
1 1 1
β∗ = β+ y′n(I+K)−1 yn+ γ21nδ2I ]−11n− µ′nV−1 µn (7.15)
2 n×n 2 n×n 2 n×n
α∗ = n+α,
(7.16)
the expression in (7.12) becomes
βα
w(an,σ2|yn) =
(2π)n|(I+K) n×n|1 2(δ2)n 2Γ(α)
(cid:18)
1
(cid:19)α∗+1
× e− σ1 2(β∗+1 2µ′nV n− ×1 nµn+1 2a′nV n− ×1 nan−a′nV n− ×1 nµn×n)·
σ2
βα (cid:18) 1 (cid:19)α∗+1
= e− σ1 2[ 21(an−µn)′V n− ×1 n(an−µn)]e− σ1 2β∗ .
(2π)n|I+K|1 2(δ2)n 2Γ(α) σ2
The denominator in (7.8) is given by:
(cid:90) (cid:90)
m(yn) = L (an,σ2|yn)×w(an,σ2)dandσ2.
1
R+ Rn
32(cid:34)
(cid:90) (cid:90) 1 1
= ×
R+ Rn (2π)n 2(σ2)n 2|(I+K) n×n|1 2 (2π)n 2(σ2δ2)n 2
(cid:35)
×e− 2σ1 2(yn−an)′ (I+K)− n×1 n(yn−an)e− 2σ1 2(an−γ1n)′ (δ2In×n)−1(an−γ1n)dan
×
βα (cid:16) 1 (cid:17)α+1
e− σβ 2dσ2
Γ(α) σ2
(cid:34)
(cid:90) 1
=
R+
(2π)n(σ2)n|(I+K) n×n|1 2(δ2)n
2
×(cid:90) (cid:110) e− 2σ1 2[y′n(I+K)− n×1 ny−a′n(I+K)− n×1 nyn−y′n(I+K)− n×1 nan+a′n(I+K)− n×1 nan]
Rn
×e− 2σ1
2[a′n(δ2I)−1an−γ1′n(δ2In×n)−11n−γa′n(δ2In×n)−11n+γ21′n(δ2In×n)−11n](cid:111)
dan
(cid:35)
×
βα (cid:16) 1 (cid:17)α+1
e− σβ 2 dσ2
Γ(α) σ2
(cid:90) (cid:32) e− 2σ1 2[y′n(I+K)− n×1 nyn+γ21′n(δ2In×n)−11n]
= ×
R+
(2π)n(σ2)n|(I+K) n×n|1 2(δ2)n
2
(cid:34) (cid:104) (cid:110) (cid:111) (cid:110) (cid:111)(cid:110) (cid:111)−1(cid:105)
(cid:90) e− 2σ1
2
a′n (I+K)− n×1 n+(δ2In×n)−1 an−2a′n (I+K)− n×1 n+(δ2In×n)−1 (I+K)− n×1 n+(δ2In×n)−1
Rn
(cid:110) (cid:111)(cid:105) (cid:35) (cid:33)
×e− 2σ1 2 (I+K)− n×1 nyn+γ(δ2In×n)−11n dan × βα (cid:16) 1 (cid:17)α+1 e− σβ 2 dσ2. (7.17)
Γ(α) σ2
Rewriting (7.17) in terms of equations (7.13) to (7.16) gives
(cid:90) (cid:34) e− 2σ1 2[y′n(I+K)− n×1 nyn+γ21′n(δ2In×n)−11n]
=
R+
(2π)n(σ2)n|(I+K) n×n|1 2(δ2)n
2
(cid:32) (cid:104) (cid:105) (cid:33) (cid:35)
× (cid:90) e− 2σ1 2 a′ V n− ×1 nan−2a′nV n− ×1 nµn dan × βα (cid:16) 1 (cid:17)α+1 e− σβ 2 dσ2.
Γ(α) σ2
Rn
(7.18)
We complete the square in the inner integral (with respect to an) by multiplying and
dividing (7.18) by e− 2σ1 2µ′nV n− ×1 nµn. This gives
m(yn) =
(cid:90) (cid:34) e− 2σ1 2[y′n(I+K)− n×1 nyn+γ21′n(δ2In×n)−11n]
e2σ1 2µ′nV n− ×1 nµn
R+
(2π)n(σ2)n|(I+K) n×n|1 2(δ2)n
2
(cid:32) (cid:33) (cid:35)
× (cid:90) e− 2σ1 2(an−µn)′ V n− ×1 n(an−µn)dan βα (cid:16) 1 (cid:17)α+1 e− σβ 2 dσ2.
Γ(α) σ2
Rn
33(7.19)
The integral with respect to an in (7.19) becomes 1 if we divide and multiply (7.53)
by (2π)n 2(σ2)n 2|V n×n|21, i.e.,
m(yn) = (cid:90) R+(cid:34) e− 2 (σ1 22 π[y )′n n( (I σ+ 2K )n)− n |(×1 Iny +n+ Kγ )2 n1′ ×n n(δ |2 21I (n δ× 2n ))
n
2−11n] (2π)n 2(σ2)n 2|V n×n|1 2e2σ1 2µ′nV n− ×1 nµn ×
(cid:32) (cid:33) (cid:35)
1 (cid:90) e− 2σ1 2(an−µn)′ V n− ×1 n(an−µn)dan × βα (cid:16) 1 (cid:17)α+1 e− σβ 2 dσ2
(2π)n 2(σ2)n 2|V n×n|21
R
Γ(α) σ2
(cid:34)
= (cid:90) (2π)n 2(σ2)n 2|V n×n|21 e2σ1 2µ′nV n− ×1 nµn e− 2σ1 2[y′n(I+K)− n×1 nyn+γ21′n(δ2In×n)−11n]
R+
(2π)n(σ2)n|(I+K) n×n|1 2(δ2)n
2
(cid:35)
×
βα (cid:16) 1 (cid:17)α+1
e− σβ 2 dσ2. (7.20)
Γ(α) σ2
Rearranging (7.20) gives
m(yn) =
|V n×n|21 βα
(2πδ2)n 2|(I+K) n×n|1 2 Γ(α)
(cid:104) (cid:110) (cid:111)(cid:105)
×(cid:90) (cid:16) 1 (cid:17)α+n 2+1 e− σ1
2
β+1
2
y′n(I+K)− n×1 nyn+γ21′n(δ2In×n)−11n−µ′V n− ×1 nµn
dσ2.
σ2
R+
(7.21)
Recall from (7.15) and (7.16) that:
1 1 1
β∗ = β+ y′n(I+K)−1 yn+ γ21nδ2I ]−11n− µ′nV−1 µn
2 n×n 2 n×n 2 n×n
α∗ = n+α
. Using them in (7.21) gives
m(yn) =
|V n×n|21 βα (cid:90) (cid:16) 1 (cid:17)α∗−n 2+1
e− σ1 2β∗ dσ2. (7.22)
(2πδ2)n 2|(I+K) n×n|1 2 Γ(α) R+ σ2
The integrand in (7.22) will be the pdf of an Inverse Gamma distribution and the
integral will be 1, if we multiply and divide (7.22) by
β∗α∗−n
2
. So we have
Γ(α∗−n)
2
m(yn) =
(2πδ2|V
)n
n
2×
|In| +21
K|1
2
Γβ (αα )Γ β(α ∗α∗ ∗−
−n
2n 2)(cid:90)
R+
Γβ (α∗α ∗∗ −−n 2
n
2)(cid:16) σ1 2(cid:17)α∗−n 2+1
e− σ1 2β∗ dσ2.
=
|V n×n|1 2 βα Γ(α∗− n 2)
. (7.23)
(2πδ2)n 2|I+K|1 2 Γ(α) β∗α∗−n 2
34Using (7.16) in (7.23) for n+1 and n gives
|Vn+1×n+1|21 βα Γ(α+n+ 21)
m(yn+1) (2πδ2)n+ 21 |(I+K)n+1×n+1|1 2 Γ(α) β∗α+n+ 21
= n+1
m(yn) |Vn×n|21 βα Γ(α+n 2)
(2πδ2)n 2|(I+K)n×n|1
2
Γ(α)
β
n∗α+n
2
=
|V n+1×n+1|21 |(I+K) n×n|21 (2πδ2)n 2 Γ(α Γ+ (αn )+ 21) βα(β n∗ +1)−(α+n+ 21)
|V n×n|1 2 |(I+K) n+1×n+1|1 2 (2πδ2)n+ 21 Γ( Γα (+ α)n 2) βα (β n∗)−(α+n 2)
=
c(β n∗ +1)−(α+n+ 21)
, (7.24)
(β n∗)−(α+n 2)
where
Γ(α+n+1)
c =
|V n+1×n+1|21 |(I+K) n×n|1 2 (2πδ2)n 2 Γ(α)2 βα
. (7.25)
|V n×n|21 |(I+K) n+1×n+1|1 2 (2πδ2)n+ 21 Γ(α+n 2) βα
Γ(α)
From (7.14) and (7.15), we have
µn = V [(I+K)−1 y +γ(δ2I)−11 ]
n×n n×n n n
1 1 1
β∗ = β+ y′(I+K)−1 y+ γ21 [δ2I]−11 − µ′nV−1 µn
n 2 n n×n 2 n n 2 n×n
µ′nV−1 µn = [V {(I+K)−1 yn+γ(δ2I)−11n}]′V−1 [V {(I+K)−1 yn+γ(δ2I)−11n}]
n×n n×n n×n n×n n×n n×n
(cid:20) (cid:21) (cid:20) (cid:21)
γ γ
= y′n(I+K)−1 +1′n V′ V−1 V (I+K)−1 yn+ 1n . (7.26)
n×n δ2 n×n n×n n×n n×n δ2
Since V is symmetric, i.e., V′ =V, we have
γ
µ′nV−1 µn = y′n(I+K)−1 V (I+K)−1 yn+2 y′n(I+K)−1 V 1n
n×n n×n n×n n×n δ2 n×n n×n
γ2
+ 1′nV 1′n. (7.27)
δ4 n×n
Using (7.27) in (7.26), we get
1 1
β∗ = β+ y′(I+K)−1 y+ γ21 [δ2I]−11
n 2 n n×n 2 n n
1(cid:20) γ γ2 (cid:21)
− y′n(I+K)−1 V (I+K)−1 yn+2 y′n(I+K)−1 V 1n+ 1′nV 1′n
2 n×n n×n n×n δ2 n×n n×n δ4 n×n
1 γ
= β+ y′n[(I+K)−1 −(I+K)−1 V (I+K)−1 ]yn− y′n(I+K)−1 V 1n
2 n×n n×n n×n n×n δ2 n×n n×n
35nγ2 1γ2
+ − 1′nV 1n.
2 δ2 2δ4 n×n
So,
1
β∗ = β+ y′n+1[(I+K)−1 −(I+K)−1 V (I+K)−1 ]yn+1
n+1 2 n+1×n+1 n+1×n+1 n+1×n+1 n+1×n+1
γ n+1γ2 1γ2
− y′n+1(I+K)−1 V 1n+1+ − 1′n+1V 1n+1.
δ2 n+1×n+1 n+1×n+1 2 δ2 2δ4 n+1×n+1
(7.28)
Define
Γ = (I+K)−1 −(I+K)−1 V (I+K)−1 (7.29)
1,n+1×n+1 n+1×n+1 n+1×n+1 n+1×n+1 n+1×n+1
γ
Γn+1 = y′n+1(I+K)−1 V 1n+1 (7.30)
2 δ2 n+1×n+1 n+1×n+1
n+1γ2 1γ2
and ∆ = − 1′n+1V 1n. (7.31)
2 δ2 2δ4 n+1×n+1
Using (7.29), (7.30), and (7.31) in (7.28), we get
1
β∗ = β+ y′n+1Γ yn+1−y′n+1Γn+1+∆. (7.32)
n+1 2 1,n+1×n+1 2
Now, we partition yn+1, Γ , and Γn+1. Write
1,n+1×n+1 2
 . 
Γ . . gn
y′n+1Γ yn+1 = (cid:0) yn y (cid:1)  1,n×n . . 1  (cid:18) yn (cid:19)
1,n+1×n+1 n+1  ··· . ··· y
 .  n+1
g′n . . γ
1 1
= y′nΓ yn+2y′ngny +y2 γ (7.33)
1,n×n 1 n+1 n+1 1
and
y′n+1Γn+1 = (cid:0) yn y
(cid:1)(cid:18) Γn 2(cid:19)
=y′nΓn+y γ . (7.34)
2 n+1 γ 2 n+1 2
2
Using (7.33) and (7.34) in (7.32), we get
β∗ = β+y′nΓ yn+2y′ngny +y2 γ +y′nΓn+y γ +∆.
n+1 1,n×n 1 n+1 n+1 1 2 n+1 2
1 1
= β+ y′nΓ yn−y′nΓn+∆+ γ y2 −y (γ −y′ngn). (7.35)
2 1,n×n 2 2 1 n+1 n+1 2 1
We complete the square again. The terms in (7.35) containing y become
n+1
1
γ y2 −y (γ −y′ngn)
2 1 n+1 n+1 2 1
361 (cid:20) γ −y′ngn (cid:18) γ −y′ngn(cid:19)2(cid:21) 1(γ −y′ngn)2
= γ y2 −2y 2 1 + 2 1 − 2 1
2 1 n+1 n+1 γ γ 2 γ
1 1 1
γ
(cid:20)
γ
−y′ngn(cid:21)2
1
= 1 y − 2 1 − (γ −y′ngn)2. (7.36)
2 n+1 γ 2γ 2 1
1 1
For brevity, let
γ −y′ngn
A = 2 1
1 γ
1
1 1
A = y′nΓ yn−y′nΓn+∆− (γ −y′ngn)2. (7.37)
2 2 1,n×n 2 2γ 2 1
1
Using (7.37) in (7.35), we have
γ
β∗ = β+ 1(y −A )2+A .
n+1 2 n+1 1 2
Now, since m(yn) is the marginal density of yn and, m(yn+1) is the marginal density
of yn+1,
(cid:90) m(yn+1)
dy =1. (7.38)
m(yn) n+1
R
From (7.24) we have that
(cid:0) (cid:1)
(cid:90) β∗ − α+n+ 21
c× n+1 dy = 1. (7.39)
(cid:0) (cid:1) n+1
R β∗− α+n
2
n
So solving for c gives
(cid:0) (cid:1)
β∗− α+n
2
c = n .
(cid:0) (cid:1)
(cid:82) Rβ n∗ +1− α+n+ 21 dy
n+1
Using (7.39) in (7.24), we have
(cid:0) (cid:1)
m(yn+1)
=
β n∗− α+n 2
×
(β n∗ +1)−(α+n+ 21)
m(yn) (cid:82) Rβ n∗ +1−(cid:0) α+n+ 21(cid:1) dy
n+1
(β n∗)−(α+n 2)
(cid:0) (cid:1)
β∗ − α+n+ 21
= n+1 . (7.40)
(cid:0) (cid:1)
(cid:82) Rβ n∗ +1− α+n+ 21 dy
n+1
37Now,
β n∗ +1−(α+n+ 21) =
(cid:20)
β+ γ 21(y n+1−A 1)2+A
2(cid:21)−(α+n+ 21)
. (7.41)
Rename, β∗∗ =β+A . Then, (7.41) becomes
2
β n∗ +1−(α+n+ 21) =
(cid:20)
β∗∗+ γ 21(y n+1−A
1)2(cid:21)−(α+n+ 21)
(cid:0) (cid:1)
=
β∗∗−(cid:0)
α+n
2(cid:1)
β∗∗−1
2(cid:20)
1+
2βγ
1 ∗∗(y n+1−A
1)2(cid:21)− α+n+ 21
. (7.42)
By definition, the t-density is given by
Γ(v+d) (cid:16) (g−τ)′Σ−1(g−τ)(cid:17)−v+d
St (τ,Σ)(g) = 2 1+ 2 . (7.43)
v Γ(v 2)πd 2|vΣ|1
2
v
So if we let
β∗∗ 1
v =2α,d=1,Σ= ,g =y ,τ =A . (7.44)
2α+n γ n+1 1
2 1
and use (7.44) in (7.43), we get
(cid:18) β∗∗ (cid:19) Γ(2α+n+1)
St A , (y ) = 2
2α+n 1 2α+n n+1 (cid:12) (cid:12)1
2 Γ(2α 2+n)π1 2(cid:12) (cid:12) (cid:12)(2α+n) 2β
α
2∗ +∗
n
γ1 1(cid:12) (cid:12) (cid:12)2
(cid:18) (cid:19)−1
(y −A )′ β∗∗ 1 (y −A )
×(cid:16)
1+
n+1 1 2α 2+n γ1 n+1 1 (cid:17)−2α+ 2n+1
2α+n
= Γ Γ( (2α 2α+ 2 2+n+ n1 )) γ 11 2 (2π1
)1
2β∗∗−1 2(cid:20) 1+ 2βγ 1 ∗∗(y n+1−A 1)2(cid:21)−2α+ 2n+1 .
Hence,
(cid:0) (cid:1)
β∗∗−
21(cid:20)
1+
2βγ
1 ∗∗(y n+1−A
1)2(cid:21)− α+n+ 21
=
Γ(2α+ 2n+1)(2π)21
×St
(cid:18)
A ,
β∗∗ (cid:19)
(y ).
Γ(2α+n) 1 2α+n 1 2α+n n+1
2 γ 12 2
(7.45)
38Using (7.45) in (7.42), and (7.42) in (7.40), we have
m(yn+1)
=
β∗∗−(α+n 2) Γ(2α 2+n) (2π)21
m(yn) (cid:82) Rβ n∗ +1−(α+n+ 21)dy n+1Γ(2α+ 2n+1) (γ 1)21
(cid:18) β∗∗ (cid:19)
×St A , (y ). (7.46)
2α+n 1 2α+n n+1
2
Since m(yn+1) = m(y |yn) is a density, (cid:82) m(yn+1)dy = 1. Integrating the right
m(yn) n+1 R m(yn) n+1
hand side of(7.46) w.r.t y gives that
n+1
β∗∗−(α+n 2) Γ(2α 2+n) (2π)1
2
(cid:90)
St
(cid:18)
A ,
β∗∗ (cid:19)
(y )dy (7.47)
(cid:82) Rβ n∗ +1−(α+n+ 21)dy n+1Γ(2α+ 2n+1) (γ 1)21 R 2α+n 1 2α 2+n n+1 n+1
equals 1, since y is only in the argument of the t distribution. The integral of the
n+1
t distribution being one means (7.47) gives
β∗∗−(α+n 2) Γ(2α 2+n) (2π)1
2
=1. (7.48)
(cid:82) Rβ n∗ +1−(α+n+ 21)dy n+1Γ(2α+ 2n+1) (γ 1)21
Finally using (7.48) in (7.46), we get
(cid:18) β∗∗ (cid:19)
m(y |yn) = St A , (y ). □
n+1 2α+n 1 2α+n n+1
2
7.3 Proof of Theorem 6
Proof. The joint likelihood of yn and an given σ2, γ and δ2 takes the form
L 2(yn,an|σ2,γ,δ2) =
(2π)n 2(σ2)n
2|I1
n×n+K
n×n|21e− 2σ1 2(yn−an)′ (In×n+Kn×n)−1(yn−an)
×
(2π)n
2(1
σ2δ2)n
2
e− 2σ1 2(an−γ1n)′ (δ2In×n)−1(an−γ1n).
Integrating out an using the density of (an|σ2,γ,δ2) gives
L (yn|γ,δ2,σ2)
3
1 1
=
(2π)n 2(σ2)n 2|I n×n+K n×n|21 (2π)n 2(σ2δ2)n 2
(cid:90)
× e− 2σ1 2(yn−an)′ (In×n+Kn×n)−1(yn−an)e− 2σ1 2(an−γ1n)′ (δ2In×n)−1(an−γ1n)dan.
R
391
=
(2π)n(σ2)n|I n×n+K n×n|1 2(δ2)n 2
×(cid:90) (cid:110) e− 2σ1 2[y′n(In×n+Kn×n)−1y−a′n(In×n+Kn×n)−1yn−y′n(In×n+Kn×n)−1an]
R
×e− 2σ1 2[a′n(In×n+Kn×n)−1an+a′n(δ2In×n)−1an−γ1′ (δ2In×n)−11n−γa′n(δ2In×n)−11n+γ21′n(δ2In×n)−11n](cid:111) dan
e− 2σ1 2[y′n(In×n+Kn×n)−1yn+γ21′n(δ2In×n)−11n]
=
(2π)n(σ2)n|I n×n+K n×n|1 2(δ2)n 2
(cid:40) (cid:110) (cid:111)
×(cid:90) e− 2σ1 2a′n (In×n+Kn×n)−1+(δ2In×n)−1 an
R
(cid:104) (cid:110) (cid:111)(cid:110) (cid:111)−1(cid:105)
×e− 2σ1
2
−2a′n (In×n+Kn×n)−1+(δ2In×n)−1 (In×n+Kn×n)−1+(δ2In×n)−1
(cid:104) (cid:110) (cid:111)(cid:105)(cid:41)
×e− 2σ1
2
−2a′n (In×n+Kn×n)−1yn+(δ2In×n)−11n
dan.
(7.49)
To prove both Clause #1 and Clause #2 we derive a simplified form of
L (yn|σ2,γ,δ2).WestartbyrewritingL (yn|σ2,γ,δ2)bysubstitutingfromtheexpres-
3 3
sions for V and µn from (7.13) and (7.14). That is, from (7.13) and (7.14) we
n×n
get
V = [(I +K )−1+(δ2I )−1]−1 (7.50)
n×n n×n n×n n×n
µn = V [(I +K )−1yn+γ(δ2I )−11n],
n×n n×n n×n n×n
(7.51)
and hence
L (yn|σ2,γ,δ2) =
e− 2σ1 2[y′n(In×n+Kn×n)−1yn+γ21′n(δ2In×n)−11n]
3 (2π)n(σ2)n|I n×n+K n×n|21(δ2)n 2
(cid:104) (cid:105)
(cid:90) − 1 a′nV−1 an−2a′nV−1 µn
× e 2σ2 n×n n×n dan.
(cid:102)
R
(7.52)
We complete the square in the exponent in the integral by multiplying and dividing
(7.52) by e− 2σ1 2µ′nV n− ×1 nµn. This gives
L 3(yn|γ,δ2,σ2) = e− 2σ1 2 ([y 2′ πn )( nIn (× σn 2+ )nK |n I× nn ×) n−1 +yn K+γ n2 ×1 n′n |( 21δ (2 δI 2n )× nn 2)−11n] e2σ1 2µ′nV n− ×1 nµ (cid:102)n
(cid:90)
× e− 2σ1 2(a′nV n− ×1 nan−2a′nV n− ×1 nµ+µ′ V n− ×1 nµ)dan.
R
40= e− 2σ1 2[y′(In×n+Kn×n)−1y+γ21′(δ2In×n)−11] e2σ1 2′ V n− ×1 nµn
(2π)n(σ2)n|I n×n+K n×n|1 2(δ2)n 2
(cid:90)
× e− 2σ1 2(an−µn)′ V n− ×1 n(an−µn)dan.
R
(7.53)
The integral in (7.53) becomes 1 if we divide and multiply by (2π)n 2(σ2)n 2|V n×n|1 2,
i.e., we get
L 3(yn|γ,δ2,σ2) = e− 2 (σ1 22 π[y )′n n( (I σ+ 2K )n)− n |(×1 Iny +n+ Kγ )2 n1′ ×n n(δ |2
1
2I (n δ× 2n ))
n
2−11n] e2σ1 2µ′nV n− ×1 nµn (2π)n 2(σ2)n 2|V n×n|21
(cid:34) (cid:35)
× 1 (cid:90) e− 2σ1 2(an−µn)′ V n− ×1 n(an−µn)dan.
(2π)n 2(σ2)n 2|V n×n|1 2 R
=
(2π)n 2(σ2)n 2|V n×n|21
(2π)n(σ2)n|(I+K) n×n|1 2(δ2)n
2
×e2σ1 2µ′nV n− ×1 nµn e− 2σ1 2[y′n(I+K)− n×1 nyn+γ21′n(δ2In×n)−11n]. (7.54)
ToproveClause#1,wecollectthetermsthatdependonγ from(7.54).Notethat,
the factor
(2π)n 2(σ2)n 2|V n×n|1 2 e− 2σ1 2[y′n(I+K)−1yn] (7.55)
(2π)n(σ2)n|(I+K) n×n|21(δ2)n
2
from (7.54) does not depend on γ. From Eqn (7.51), we know that µ contains γ, so
the part of (7.54) that contains γ is:
e− 2σ1 2[−µ′nV n− ×1 nµn+γ21′n(δ2I)−11n]
(cid:104) (cid:110) (cid:111)′ (cid:110) (cid:111)(cid:105)
=e− 2σ1
2
− (I+K)− n×1 nyn+γ(δ2In×n)−11n V n′ ×nV n− ×1 nVn×n (I+K)− n×1 nyn+γ(δ2In×n)−11n
×e− 2σ1 2γ21′n(δ2In×n)−11n . (7.56)
Here V′ =V . So,
n×n n×n
e− 2σ1 2[−µ′nV n− ×1 nµn+γ21′n(δ2In×n)−11n]
(cid:104) (cid:110) (cid:111)′ (cid:110) (cid:111) (cid:105)
=e− 2σ1
2
− (I+K)−1y+γ(δ2In×n)−11n Vn×n (I+K)−1y+γ(δ2I)−11n +γ21′(δ2In×n)−11n
.
(cid:104) (cid:105)
=e2σ1
2
y′n(I+K)− n×1 nVn×n(I+K)− n×1 nyn+γ1′n(δ2In×n)−1Vn×n(I+K)− n×1 nyn
(cid:104) (cid:105) (cid:104) (cid:105)
×e2σ1
2
γy′n(I+K)− n×1 nVn×n(δ2In×n)−11+γ21′n(δ2In×n)−2Vn×n1n e− 2σ1
2
γ21′n(δ2In×n)−11n
.
(7.57)
41We name the terms containing γ from the expression in right hand side of (7.57) as
h(γ) .
− 1
(cid:104) −2γy′n(I+K)− n×1 nVn 1n+γ21′n(cid:16) In×n−Vn×n(cid:17) 1n(cid:105)
h(γ)=e 2σ2 δ2 δ2 δ4
(7.58)
The term that does not contain γ in Eqn (7.57) is
(cid:104) (cid:105)
e2σ1
2
y′n(I+K)− n×1 nVn×n(I+K)− n×1 nyn
(7.59)
The product of (7.58) that contains γ with (7.55) and (7.59) that do not contain γ is
(3.15).
To prove Clause #2, we separate the factors with δ2 and without δ2 in (7.54).
From (7.50), we know that V contains δ2. The part of (7.54) that depends on δ2
n×n
is |Vn×n|× the factor in (7.57). Substituting for V from (7.50), we have that g(δ2)
n n×n
δ22
equals
(cid:12)(cid:110) (cid:111)(cid:12)1
(cid:12) (I+K)−1 +(δ2I )−1 (cid:12)2
(cid:12) n×n n×n (cid:12)
(δ2)n
2
e2σ1
2(cid:104)
y′n(I+K)− n×1
n(cid:8)
(I+K)− n×1
n+(δ2In×n)−1(cid:9)−1
(I+K)− n×1 nyn+2 δγ 2y′n(I+K)− n×1
n(cid:8)
(I+K)− n×1
n+(δ2In×n)−1(cid:9)−1 1n(cid:105)
e− 2σ1 2(cid:104) γ δ42 1′n(cid:8) (I+K)− n×1 n+(δ2In×n)−1(cid:9)−1 1n−γ δ22 1′n1n(cid:105)
.
(7.60)
The remaining factors in (7.54) are:
(2π)n 2(σ2)n
2 e− 2σ1 2[y′n(I+K)− n×1 nyn] (7.61)
(2π)n(σ2)n|(I+K) n×n|21
. The product of (7.60) and (7.61) gives (3.17).
42