The Quest for the Right Mediator: A History, Survey, and
Theoretical Grounding of Causal Interpretability
Aaron Mueller1∗, Jannik Brinkmann2, Millicent Li1, Samuel Marks3, Koyena Pal1,
Nikhil Prakash1, Can Rager4, Aruna Sankaranarayanan5, Arnab Sen Sharma1,
Jiuding Sun1, Eric Todd1, David Bau1, Yonatan Belinkov6
1 Northeastern University 2 University of Mannheim 3 Anthropic 4 Independent
5 Massachusetts Institute of Technology 6 Technion – IIT
Abstract
Interpretability provides a toolset for understanding how and why neural networks behave
in certain ways. However, there is little unity in the field: most studies employ ad-hoc eval-
uations and do not share theoretical foundations, making it difficult to measure progress
and compare the pros and cons of different techniques. Furthermore, while mechanistic
understanding is frequently discussed, the basic causal units underlying these mechanisms
are often not explicitly defined. In this paper, we propose a perspective on interpretability
research grounded in causal mediation analysis. Specifically, we describe the history and
current state of interpretability taxonomized according to the types of causal units (medi-
ators) employed, as well as methods used to search over mediators. We discuss the pros
and cons of each mediator, providing insights as to when particular kinds of mediators and
searchmethodsaremostappropriatedependingonthegoalsofagivenstudy. Wearguethat
this framing yields a more cohesive narrative of the field, as well as actionable insights for
future work. Specifically, we recommend a focus on discovering new mediators with better
trade-offs between human-interpretability and compute-efficiency, and which can uncover
more sophisticated abstractions from neural networks than the primarily linear mediators
employed in current work. We also argue for more standardized evaluations that enable
principled comparisons across mediator types, such that we can better understand when
particular causal units are better suited to particular use cases.
1 Introduction
Tounderstandhowneuralnetworks(NNs)generalize,wemustunderstandthecausesoftheirbehavior. These
causesincludeinputs, butalsotheintermediatecomputationsofthenetwork. Howcanweunderstandwhat
these computations represent, such that we can arrive at a deeper algorithmic understanding of how and
why models behave the way they do? For example, if a model decides to refuse a user’s request, was the
refusal mediated by an underlying concept of toxicity, or by the presence of superficial correlates of toxicity
(such as the mention of particular demographic groups)? The former would be significantly more likely to
robustly and safely generalize. These questions motivate the field of causal interpretability, where we aim to
extract causal graphs explaining how intermediate NN computations mediate model outputs.
Thissurveytakesanopinionatedstanceoninterpretabilityresearch: wegroundthestateofthefieldthrough
the lens of causal mediation analysis (§2). We start by presenting a history of causal interpretability for
neural networks (§3), from backpropagation (Rumelhart et al., 1986) to the beginning of the current causal
and mechanistic interpretability wave.
We then survey common mediators (units of causal analysis) used in causal interpretability studies (§4),
discussing the pros and cons of each mediator type. Should one analyze individual neurons? Full MLP
output vectors? Model subgraphs? More broadly: what is the right unit of abstraction for analyzing
∗Correspondencetoaa.mueller@northeastern.edu. Middleauthorsinalphabeticalorder.
1
4202
guA
2
]GL.sc[
1v61410.8042:viXraand discussing neural network behaviors? Any NN component has pros and cons related to its level
of granularity, whether it is a causal bottleneck, and whether it is natively part of the model (as opposed
to whether it is learned on top of the model). After discussing common mediator types, we then categorize
and discuss methods for searching over mediators of a given type to find those that are causally relevant to
sometask(§5). Finally,aftersurveyingthefield,we(1)pointoutmediatorswhichhavebeenunderexplored,
but have significant potential to yield new insights; (2) propose criteria that future mediators should satisfy,
basedonthegoalsofone’sstudy;and(3)suggestwaystomeasureprogressincausalinterpretabilitymoving
forward (§7).
2 Preliminaries
The counterfactual theory of causality. Lewis (1973) poses that a causal dependence holds iff the
following condition holds:
“An event E causally depends on C [iff] (i) if C had occurred, then E would have occurred,
and (ii) if C had not occurred, then E would not have occurred.”
Lewis (1986) extends this definition of causal dependence to be whether there is a causal chain linking
C to E; a causal chain is a connected series of causes and effects that proceeds from an initial event to a
final one, with potentially many intermediate events between them. This idea was later extended from a
binary notion of whether the effect happens to a more nuanced notion of causes having influence on how or
when events occur (Lewis, 2000). Other work defines notions of cause and effect as continuous measurable
quantities (Pearl, 2000); this includes direct and indirect effects (Robins & Greenland, 1992; Pearl, 2001),
which are common metrics in causal interpretability studies.
Causal abstractions in neural network interpretability. Causal interpretability is based on the ab-
straction of causal graphs. These graphs consist of nodes, which can be inputs, causes, actions, compu-
tations, transformations, among other events. They also consist of directed edges, which represent causal
relationships between nodes; the source of the edge is the cause, and the target of the edge is the effect. If
we are given an input cause x and output effect y, there may exist many causal nodes between them; these
intermediate nodes are called mediators.
In the causality literature, a mechanism is defined as a causal chain from cause C to effect E. The mecha-
nisticinterpretabilityliterature,whilecloselyrelatedtocausalinterpretability,doesnotenforcethiscausally-
groundeddefinitionofmechanism: mechanisticinterpretabilityisoftendefinedasreverse-engineeringneural
networkstobetterunderstandhowandwhytheybehaveincertainways(cf.Milleretal.,2024;Nandaetal.,
2023). Theoverlapbetweenmechanisticandcausalinterpretabilityissignificant,butnottotal: forexample,
sparse autoencoders (Bricken et al., 2023; Cunningham et al., 2024) are correlational, but many methods,
such as circuit discovery (Elhage et al., 2021; Conmy et al., 2023) and alignment search (Geiger et al., 2021;
2024), employ methods to causally implicate model components (or other abstractions discovered in poten-
tially correlational ways) in model behavior. We believe that the causality-based definition of mechanism
is a useful one that makes precise the main challenge of mechanistic interpretability—to reverse-engineer
an algorithmic understanding of neural network behaviors, where “algorithm” is essentially equivalent to a
complete causal graph explaining how a model will generalize.
The abstraction of causal graphs extends naturally to neural networks: we can treat the computation graph
of a neural network as the full causal graph which explains how inputs x are transformed into a probability
distributionoveroutputsy. Inthiscase, allmodelcomponents1 canbeviewedascausalnodesthatmediate
the transformation of x to y. This is discussed in detail in §4.
Counterfactual interventions. In interpretability, “causal method” generally refers to a method that
employs counterfactual interventions (Lewis, 1973) to some part of the model or its inputs. Much early
1Wewilluse“component”primarilytorefertoneuronsandattentionheads. Thisisimprecise,butcapturesatomicunitsof
thecomputationgraphthatareoftenusedasmediatorsincurrentwork. Italsoservestocontrastwithmediatorsthatcannot
beeasilyextractedfromthecomputationgraph,suchasnon-basis-aligneddirections.
2Figure 1: Visual summary of causal mediation analysis. We are given a cause (input) x, which results in
effect (output) y. There often exist intermediate causal variables such as z that mediate the transformation
of x to y. One of the most common ways of quantifying the importance of z is by measuring its indirect
effect, where one intervenes on z and then measures the change in y compared to when no intervention was
performed.
interpretability work focused on interpreting model decision boundaries by intervening on the inputs, but
contemporary work is primarily concerned with understanding which intermediary model components are
responsibleforsomebehaviorgivensomeinput—i.e.,findingtherightnodesfromthelow-levelcomputation
graph to keep in the high-level causal graph.
Causal mediation analysis (Pearl, 2001) provides a unified framework for performing counterfactual inter-
ventions, in which the causal influence of a node x in a causal graph on a downstream node y is quantified
as x’s indirect effect (IE; Pearl, 2001; Robins & Greenland, 1992). This metric is based on the notion
of counterfactual dependence, where we measure the difference in some output metric m before and after
intervening on a given mediator z. We measure m given a normal run of the model on input x, where z
takesitsnaturalvaluez , andthencomparethistomonanormalrunofthemodelgivenxwheremediator
1
z is set to some alternate value z :2
2
IE(m;x;z,z ,z )=m(x|z =z )−m(x|do(z =z )) (1)
1 2 1 2
See Figure 1 for an illustration.
3 A History of Causal Interpretability
Interpretability at the beginning of deep learning. In1986,Rumelhartetal. publishedanalgorithm
for backpropagation and an analysis of this algorithm. This enabled and massively popularized research in
training multi-layer perceptrons—now often called feedforward layers. This paper arguably represents the
first mechanistic interpretability study: the authors evaluated their method by inspecting each activation
and weight in the neural network, and observing whether the learned algorithm corresponded to the human
intuition of how the task should be performed. In other words, they reverse-engineered the algorithm of the
network by labeling the rules encoded by each neuron and weight!
Throughout the 1990s and early 2000s, the idea of extracting rules from the parameters and activations
of NNs remained popular. At first, this was a manual process: networks were either small enough to be
manuallyinterpreted(Rumelhartetal.,1986;McClelland&Rumelhart,1985)orinterpretedwiththeaidof
carefully crafted datasets (Elman, 1989; 1990; 1991); alternatively, researchers could prune them (Mozer &
Smolensky, 1988; Karnin, 1990) to a sufficiently small size to be manually interpretable. Later, researchers
proposed techniques for automatically extracting rules (Hayashi, 1990) or decision trees from NNs (Craven
& Shavlik, 1994; 1995; Krishnan et al., 1999; Boz, 2002)—often after the network had been pruned. At this
point,interestincausalmethodsbasedoninterventionshadnotyetbeenestablished,asnetworkswereoften
small and/or simple enough to directly understand without significant abstraction. Nonetheless, as the size
of neural networks scaled up, the number of rules encoded in a network increased; thus, rule/decision tree
2AppendixAsurveysmethodsforsourcingz2. Thiscancomefromalternateinputswheretheanswerisflipped,meansover
manyinputs,or(typically0).
3extractiontechniquescouldnotgenerateeasilyhuman-interpretableexplanationsoralgorithmicabstractions
of model behaviors beyond a certain size. This led to the rise of visualization methods in the 2000s,
which became a popular way to demonstrate the complexity of phenomena that models had learned to
encode. Designing visualizations of inputs and outputs of the network (Tzeng & Ma, 2005) and interactive
visualizations of model activations (Erhan et al., 2009) were valuable initial tools for generating hypotheses
as to what kinds of concepts models could represent. While visualization research was generally not causal,
this subfield would remain influential for interpretability research as neural networks scaled in size in the
following decade.
Large-scale pre-trained models. The 2010s were a time of rapid change in machine learning. In 2012,
the first large-scale pre-trained neural network, AlexNet (Krizhevsky et al., 2012), was released. Not long
after,pre-trainedwordembeddingsbecamecommoninnaturallanguageprocessing(Mikolovetal.,2013a;b;
Penningtonetal.,2014), andfurtherpre-traineddeepnetworksfollowed(Heetal.,2016). Thesewerebased
on ideas from deep learning. This represented a significant paradigm shift: formerly, each study would build
ad-hoc models which were not shared across studies, but which were generally more transparent.3 After
2012, there was a transition toward using a shared collection of significantly larger and more capable—but
also more opaque—models. This raised new questions on what was encoded in the representations of these
shared scientific artifacts. The rapid scaling of these models rendered old rule extraction methods either in-
tractableormadeitsresultsdifficulttointerpret; thus,interpretabilitymethodsintheearly2010stendedto
prominentlyfeaturescalableandrelativelyfastcorrelationalmethods,includingvisualizations(Zeiler&Fer-
gus, 2014) and saliency maps (Simonyan et al., 2014). This trend continued into 2014–2015, when recurrent
neural network–based (Elman, 1990) language models (Mikolov et al., 2010) began to overtake statistical
models in performance (Bahdanau et al., 2015); for example, visualizing RNN and LSTM (Hochreiter &
Schmidhuber, 1997) hidden states was proposed as a way to better understand their incremental processing
(Karpathy et al., 2016; Strobelt et al., 2017).
Atthesametime,interpretabilitymethodsstartedtoembraceauxiliary(correlational)models—forexample,
LIME (Ribeiro et al., 2016a;b) and Anchors (Ribeiro et al., 2018). These models aimed to learn local
decision boundaries, or some human-interpretable simplified representation of a model’s behavior. Other
worksinterpretedpredictionsviafeatureimportancemeasureslikeSHAP(Lundberg&Lee,2017). Influence
functions(Koh&Liang,2017)tracedthemodel’sbehaviorbacktospecificinstancesfromthetrainingdata.
Another line of work also seeked to directly manipulate intermediate concepts to control model behavior at
test time (Koh et al., 2020). The primary difference between these visualization-/correlation-/input-based
methodsandcurrentmethodsisthatthesemethodsprioritizediscoveringhigh-levelpatternsaboutresponses
to particular kinds of inputs, such that we can generate hypotheses as to the types of input concepts these
models are sensitive to. In contrast, current work prioritizes highly localized and causal explanations of
how and in which regions of the computation graph models translate particular inputs into general output
behaviors.
2017–2019 featured perhaps the largest architectural shift (among many) in machine learning methods at
this time: Transformers (Vaswani et al., 2017) were released and quickly became popular due to scalability
andhighperformance. Thisleddirectlytothefirstsuccessfullarge-scalepretrainedlanguagemodels,suchas
(Ro)BERT(a) (Devlin et al., 2019; Liu et al., 2019b) and GPT-2 (Radford et al., 2019). These significantly
outperformed prior models, though it was unclear why—and at this scale, analyzing neural networks at
the neuron level using past techniques had long become intractable. This combination of high performance
and little mechanistic understanding created demand for interpretability techniques that allowed us to see
how language models had learned to perform so well. Hence, correlational probing methods rose to meet
this demand: here, classifiers are trained on intermediate activations to extract some target phenomenon.
Probing classifiers were used to investigate the latent morphosyntactic structures encoded in static word
embeddings(Köhn,2015;Guptaetal.,2015)orintermediatehiddenrepresentationsinpre-trainedlanguage
models—forexample,inneuralmachinetranslationsystems(Shietal.,2016;Belinkovetal.,2017;Conneau
et al., 2018) and pre-trained language models (Hewitt & Manning, 2019; Hewitt et al., 2021; Lakretz et al.,
2019; 2021). However, probing classifiers lack consistent baselines, and the claims made in these studies
3Manysystemsbuiltbeforedeeplearningwerebasedonfeatureengineering,andsotheinformationtheyreliedonwasmore
transparentthanincurrentsystems.
4Figure2: Visualizationofcommonmediatortypesinneuralnetworks. Onecanimplicateindividualneurons
or attention heads in performing a model behavior, or a full layer vector. One can also implicate a
multidimensional subspace, which could be neuron-basis-aligned (as in a group of neurons, pictured
here) or non-basis-aligned. Non-basis-aligned mediators—e.g., non-basis-aligned directions—have recently
become a popular mediator type due to their monosemanticity. However, discovering non-basis-aligned
mediators requires external modules such as classifiers, autoencoders, or other modifications to the original
computationgraph. NotethatwhilethisfiguredepictsaTransformer,manyofthemediatortypesgeneralize
to other architectures (the primary exception being attention heads).
were not often causally verified (Belinkov, 2021). For instance, although an intervention may target a task
mappingofA→B,analternativepropertyC maybepredicted,whichcanpotentiallyimpedecausalclaims
about A → B (Ravichander et al., 2021). This likely encouraged researchers to search for more causally
efficacious methods.
The rise of causal interpretability. 2017–2018 featured the first hints of our current wave of causal
interpretability,withresearchthatdirectlyinvestigatedinterveningonactivationsandmanipulatingneurons.
For example, Giulianelli et al. (2018) trained a probing classifier, but then used gradients from the probe to
modify the activations of the network. Other studies analyzed the functional role of individual neurons in
static word embeddings (Li et al., 2017) or latent representations of generative adversarial networks (GANs;
Goodfellow et al., 2014) by forcing certain neurons on or off (Bau et al., 2019b). The idea of manipulating
neurons to steer behaviors was then applied to downstream task settings, such as machine translation (Bau
et al., 2019a). The field was more widely popularized in 2020, when Vig et al. (2020) proposed a method for
assigningcausalimportancescorestoneuronsandattentionheads. Itwasanapplicationofthecounterfactual
theory ofcausality (Lewis, 1973; 1986), aswell as Pearl’s operationalization and measurementsof individual
causal nodes’ effect sizes (Pearl, 2001; 2000). This encouraged a new line of work that aimed to faithfully
localize model behaviors to specific components, such as neurons or attention heads—an idea that would
become foundational to current causal and mechanistic interpretability research.
Atthesametime,however,researchersbegantorealizethesignificantperformanceimprovementsthatcould
be gained by massively increasing the number of parameters and training corpus sizes of neural networks
(Brown et al., 2020; Kaplan et al., 2020). Massively increasing model sizes resulted in more interesting
subjects of study, but also rendered causal interpretability significantly more difficult just as its popularity
began. Thus,aprimarychallengeofcausalinterpretabilityhasbeentobalancetheoften-contradictorygoals
of(i)obtainingacausallyefficaciousunderstandingofhowandwhymodelsbehaveinagivenmanner,while
also (ii) designing methods that are efficient enough to scale to ever-larger models.
Presently,thereexistmanysubfieldsofinterpretabilitythatproposeandapplycausalmethodstounderstand
which model components contribute to an observed model behavior (e.g., Elhage et al., 2021; Geiger et al.,
2021; Conmy et al., 2023). Recently, there have also been efforts to discover more human-interpretable
mediators by moving toward latent-space structures aside from (collections of) neurons (Cunningham et al.,
2024;Brickenetal.,2023;Wuetal.,2023). Thesemethodsandtheirapplicationsarethefocusofthesurvey
that follows.
54 Selecting a mediator type
In this section, we discuss different types of causal mediators in neural networks, and the pros and cons of
each. Figure 2 visualizes a computation graph, and units thereof that are often used as mediators in causal
interpretability studies. In causal interpretability, we often do not want to treat the full computation graph
as the final causal graph, as it is large and difficult to directly interpret. Thus, we typically want to build
higher-level causal abstractions that capture only the most important mediators, and/or where each causal
node is human-interpretable. It is also possible to group components together into a single causal node,
meaning there are many possible mediators of a given type.
One possible mediator type is a full layer ℓ—typically the output of a layer (§4.1). This generally refers
to a vector aℓ composed of activations aℓ, where we will refer to each a as a neuron.4 One can also
i i
use the output vector of an intermediate submodule within the layer (e.g., an MLP), rather than the
output of the whole layer. For example, in Transformers (Vaswani et al., 2017),5 a layer typically consists
of two submodules: a multi-layer perceptron (MLP) and an attention block, which can be arranged either
sequentially or in parallel. The output of these submodules is also a vector of activations, so we will refer to
their individual dimensions as neurons as well.6
Onecanalsousesingleneuronsorsetsofneuronsasamediator(§4.2). Ifweuseasetofneurons(possiblyof
size1){a ,a ,...}fromavectora,thisisreferredtoasa basis-aligned subspaceofa. Aone-dimensional
i j
basis-aligned subspace is equivalent to a neuron; we will use basis-aligned subspace primarily to refer to
neuron groups of size > 1. Basis alignment is a key concept: values that align with neuron directions will
be discoverable without any external modules appended to the original computation graph. For example, it
is straightforward to exhaustively search over and intervene on single neurons; it is less tractable, but still
theoretically possible, to enumerate all 2n possible combinations of neurons without using any additional
parameters.
However,causallyrelevantfeaturesarenotguaranteedtobealignedwithneuronsinactivationspace;indeed,
therearemanycaseswherehuman-interpretablefeaturescorrespondtosubspacesthatarenotspannedbya
(set of) neuron(s) (Elhage et al., 2022b; Bricken et al., 2023). Thus, in recent studies, it is common to study
non-basis-aligned spaces (§4.3). Each channel of a non-basis-aligned subspace can be defined as weighted
linear combination of neuron activations. For example, to obtain a non-basis-aligned direction,7 we could
learn coefficients α and β to weight activations a and a (optionally with a bias term b):
i j
d=α·a +β·a +...+b (2)
i j
Note that these new constants α and β are not part of the original computation graph. This means that
discovering directions often requires external components that weight components from the computation
graph in some way—e.g., classifiers or autoencoders (§5.2).
The primary trade-off between these mediator types is their granularity and number. This section proceeds
in order of increasing granularity and quantity.
4.1 Full layers and submodules.
Fulllayersandsubmodulesarerelativelycoarse-grainedmediators. Thus,theyareacommonstartingpointif
onedoesnotknowwhereinamodelaparticulartypeofcomputationishappening. Earlyprobingclassifiers
studied the information encoded in full layers (Shi et al., 2016; Hupkes et al., 2018; Belinkov et al., 2017;
Conneau et al., 2018; Hewitt & Manning, 2019; Giulianelli et al., 2018), and recent studies that leverage
classifiersaspartofcausaltechniquesstillfrequentlydothesame(e.g.,Elazaretal.,2021;Marks&Tegmark,
4Inotherwords,weuse“neuron”torefertoanybasis-aligneddirectioninactivationspace.
5Transformers are currently the dominant architecture for vision and/or language modeling; as such, there is much more
work on interpreting the decisions of models built with this architecture. However, our ideas are presented in a general way
that will also apply with minor modifications to other neural-network-based architectures, such as recurrent neural networks
(Mikolovetal.,2010)andstatespacemodels.
6Usingthesamenotationemphasizesthatthesearemediatorsofthesamegranularity,butweacknowledgethatthisobscures
thatneuronsindifferentlocationsoftenencodedifferenttypesoffeatures.
7Wewilluse“direction”torefertoone-dimensional(sub)spaces.
62023; Li et al., 2023). This makes layers a natural mediator for exploratory interventions where the usage
of more fine-grained mediators is infeasible, as in Conmy et al. (2023), or where broad characterizations
of information flow are sufficient, as in Geva et al. (2023); Sharma et al. (2024). That said, it is rare to
completelyablatealayerandthenobservehowthischangesbehavior; thishasbeendoneinapruningstudy
wherethemotivationwasnotinterpretability(Sajjadetal.,2023),butthistechniquehaspotentialtoinform
our understanding of which general model regions are more responsible for certain kinds of behaviors (e.g.
Lad et al., 2024).
In some cases, coarse-grained mediators like these can inform methods for understanding factual recall in
language models (Geva et al., 2023), or updating these factual associations (Meng et al., 2022; 2023). How-
ever, full layers encode many features and have many causal roles in a network, which makes it difficult to
interpret how, exactly, relevant information is encoded in a layer (Conmy et al., 2023). Additionally, inter-
vening on full layers or submodules often causes side effects outside the scope of the intervention (McGrath
et al., 2023).
Theprimaryadvantageofusingfulllayersasmediatorsistheirsmallquantityandbroadscopeofinformation.
This means that even slow or resource-intensive methods will generally be easy to apply to all layers. In
some cases, this is enough granularity. However, an obvious disadvantage is that this mediator is generally
opaque: even if we know that information is encoded in a layer somehow, it is unclear precisely how this
information is encoded, composed, or used. Thus, layers and submodules have little explanatory power, and
are better used as coarser starting points for later finer-grained investigations (e.g., Brinkmann et al., 2024;
Geva et al., 2023) or for downstream applications such as model editing (Meng et al., 2022; 2023; Sharma
et al., 2024; Gandikota et al., 2023; 2024).
4.2 Basis-aligned subspaces
Neurons. Comparedtofulllayersandsubmodules,neuronsrepresentmorefine-grainedcomponentswithin
neural networks that could feasibly represent individual features (though we discuss below that this is not
often the case due to polysemanticity). Individual neurons can be considered the smallest meaningful unit
within a neural network; an activation from a neuron is simply a scalar corresponding to a single dimension
(1-dimensional subspace) of a hidden representation vector. Each neuron can differ from another based on
its functional role in the network; for instance, Bau et al. (2020) locate neurons in a GAN responsible for
generating specific types of objects in images, such as trees or windows, and verify this causally by ablating
or artificially activating those neurons.
Neuronsareanaturalchoiceformediator,astheyarebothfine-grainedandeasytoexhaustivelyiterateover
(see§5.1). However,amajordisadvantageofusingneuron-basedinterpretabilitymethodsispolysemanticity.
Individual neurons are often polysemantic—i.e. they respond to multiple seemingly unrelated inputs (Arora
et al., 2018). For example, if the same neuron were sensitive to capitalized words, animal names, one-digit
numbers, among other phenomena, it would be difficult to disentangle each of these individual patterns
such that we can assign a coherent textual label to the neuron. Elhage et al. (2022b) investigate this
phenomenon and suggest that neural networks represent features through linear superposition, where they
represent features along non-basis-aligned linear subspaces, resulting in interpretable units being smeared
across multiple neurons. In other words, in an activation vector of size n, a model can encode m ≫ n
concepts as linear directions (Park et al., 2023), such that only a sparse subset of concepts are active given
a particular input.
Basis-aligned multi-dimensional subspaces. The computations of individual neurons are not entirely
independent: it may often be the case that sets of neurons compose to encode some concept. For example,
inlanguagemodels,localizedsubsetsofneuronscanbeimplicatedinencodinggenderbias(Vigetal.,2020),
andimplementingfundamentallatentlinguisticphenomena(Finlaysonetal.,2021;Muelleretal.,2022;Bau
et al., 2019a; Lakretz et al., 2019). Thus, some initial causal interpretability work employed heuristic-based
searchesoversetsofneuronresponsibleforsomebehavior(e.g.,Bauetal.,2019b;Vigetal.,2020;Caoetal.,
2021). This is a generalization of individual neurons as mediators, where multiple dimensions in activation
space are intervened upon simultaneously.
7Using arbitrarily-sized sets of neurons gives us strictly more information, and thus potentially more descrip-
tivemediators. Despitethis,basis-alignedmultidimensionalsubspacesarenotcommonlystudiedmediators.
This is for two primary reasons: (1) There is a combinatorial explosion when we are allowed to search over
arbitrarily-sizedsetsofneurons,whichmakesexhaustivesearchesintractable. (2)Additionally,interpretable
concepts are not guaranteed to be aligned to neuron bases, meaning that leveraging groups of neurons still
does not directly address the problem of polysemanticity—in fact, it may exacerbate the problem by adding
even more information (Morcos et al., 2018; Chughtai et al., 2023; Wang et al., 2023).
Attention heads. Similartoneurons, attentionheadsarefundamentalcomponentsofTransformer-based
neuralnetworks: theymediatetheflowofinformationbetweentokenpositions(Vaswanietal.,2017). Thus,
using attention heads as units of causal analysis can help us understand how models synthesise contextual
information(Maetal.,2021;Neoetal.,2024)topredictsubsequenttokens(Wangetal.,2023;Hannaetal.,
2023; Prakash et al., 2024; García-Carrasco et al., 2024; Brinkmann et al., 2024). For practical purposes,
each head within a layer can be understood as an independent operation, contributing a result that is then
added into the residual stream.8 For example, some heads specialise on syntactic relationships (Chen et al.,
2024a),othersonsemanticrelationshipssuchasco-reference(Vigetal.,2020),andothersstillonmaintaining
long-range dependencies in text (Wu et al., 2024a). Attention heads have also been directly implicated in
acquiring the ability to perform in-context learning (Olsson et al., 2022; Brown et al., 2020), or to detect
and encode functions in latent space (Todd et al., 2024).
Attention heads are attractive mediators because they are easily enumerable (there are far fewer attention
heads than neurons in a model) and because they often encode sophisticated multi-token relationships.
However, in contrast to the activation of a neuron, the output of an attention head is multi-dimensional.
Thus, it is difficult to directly interpret the full set of functional roles a single head might have; indeed,
attention heads are almost always polysemantic, so one cannot typically determine the function(s) of an
attentionheadsolelybyobservingitsactivations(Janiaketal.,2023)—aswithneurons.9 Ithasadditionally
been observed that ablating attention heads can cause other attention heads to compensate, which further
complicates their analysis (Jermyn et al., 2023; Wang et al., 2023; McGrath et al., 2023).10
4.3 Non-basis-aligned spaces
Non-basis-aligned multi-dimensional subspaces. Due to their polysemanticity, neurons, attention
heads, and sets thereof do not necessarily correspond to cleanly interpretable features or concepts. For
example, it is common that individual neurons activate on many seemingly unrelated inputs (Elhage et al.,
2022b), and this issue cannot be cleanly resolved by adding more dimensions. This is because the features
may actually be encoded in directions or subspaces that are not aligned to neuron bases (Mikolov et al.,
2013a; Arora et al., 2016).
To overcome this disadvantage, one can generalize causal mediators to include arbitrary non-neuron-basis-
aligned activation subspaces. This allows us to capture more sophisticated causal abstractions encoded in
latent space, such as causal nodes corresponding to greater-than relationships (Wu et al., 2023), or equality
relationships (Geiger et al., 2024). A common way of locating these is through learned rotation operations
(Geiger et al., 2021; 2024), which preserve linearities and therefore are still in the activation subspace.
Theprimaryadvantageofconsideringanarbitrarysubspaceasamediatorisitsexpressivity: subspacesoften
capture distributed abstractions that are not fully captured by a single neuron. However, they are generally
8Thisistheresidualstreamperspective(Elhageetal.,2021)ofTransformers,whichhasbeenadoptedinrecentinterpretability
research (Ferrando et al., 2024). The residual stream perspective suggests that the residual stream, which comprises the sum
of the outputs of all the previous layers and the original input embedding, acts as a passive communication channel through
whichtheMLPandattentionsubmodulesroutetheinformationtheyadd.
9However,thereisinitialevidencethatsomedimensionsofanattentionhead’soutputcanbemeaningfullyexplained(Merullo
etal.,2024a;b). Thus, bydecomposingthevectoroutputofaheadintosmallersubspacesorevenindividualneurons, itmay
beeasiertoexplainthesetoffunctionalrolesofagivenhead.
10This phenomenon, where downstream components only have causal relevance after an upstream component has been
ablated, issometimescalledpreemptioninthecausalityliterature(Mueller,2024). Preemptionisnotnecessarilylimitedto
attentionheads;futureworkshouldthusanalyzehowcommonpreemptionisbetweenothertypesofcomponents,suchasMLP
submodules.
8more difficult to locate than basis-aligned components, or non-basis-aligned directions, as we are typically
requiredto havespecifichypothesesas tohow modelsaccomplisha task, access tolabeleddatathat isolates
the target subspace, or enough compute to cluster existing mediators in an unsupervised manner. This is
discussed in more detail in §5.2.
Directions. A recent line of work aims to automatically identify specific directions (one-dimensional
spaces) that correspond to monosemantic concept representations. Identifying and labeling these monose-
manticmodelabstractions(oftencalledfeatures;Brickenetal.,2023;Cunninghametal.,2024;Huangetal.,
2024) can reveal units of computation the model uses to solve tasks in a way that is often easier for humans
to interpret.11
Thereisalsoinitialevidencethatthesedirectionsmayenablefine-grainedmodelcontrol(Panicksseryetal.,
2024; Marks et al., 2024; Tigges et al., 2023). Past work has found initial signs that basis-aligned directions
could be leveraged to edit (Meng et al., 2022) or steer (Turner et al., 2023; Paulo et al., 2024) model
behavior, whereas more recent work has tended toward non-basis aligned directions. For example, there is
work that uses linear probes to understand and ablate the effects of a direction on the model behavior Chen
et al. (2024b); Ravfogel et al. (2020); Elazar et al. (2021); Ravfogel et al. (2021); Lasri et al. (2022); Marks
& Tegmark (2023), as well as work that ablates (Marks et al., 2024; Cunningham et al., 2024) or injects
(Templeton et al., 2024) directions corresponding to fine-grained concepts such as typically female names or
the Golden Gate Bridge.
Nonetheless, directions still have key disadvantages. The search space over non-basis-aligned directions is
infinite, making it impossible to exhaustively search over them. In fact, to discover these, we are generally
required to modify the computation graph, as learning the coefficients on each neuron requires us to learn
new parameters corresponding to the desired features. Regardless of the method used, each introduces
confounds due to the stochastic optimization or significant manual effort required to locate these directions
or subspaces.
4.4 Non-linear Mediators
Non-basis-aligned directions/subspaces are the most general linear mediator type. However, recent work
has demonstrated that some features in language models can be represented non-linearly and/or using mul-
tiple dimensions. For example, there exist circular features representing days of the week or months of the
year (Engels et al., 2024). Similarily, past work has found that many concepts can be more easily extracted
usingnon-linearprobes(Liuetal.,2019a),andthatnon-linearconcepterasuretechniquestendtooutperform
strictly linear techniques (Iskander et al., 2023; Ravfogel et al., 2022). However, in causal and mechanistic
interpretability, most work has thus far tended toward using linear representations as units of causal anal-
ysis. Thus, there is significant potential in future work for systematically locating non-linearly-represented
features—e.g., using group sparse autoencoders (Theodosis & Ba, 2023), which could isolate multiple direc-
tionssimultaneously,and/orprobingandclusteringtechniquestoidentifymulti-dimensionalfeatures(Engels
et al., 2024). Non-linear features have not been extensively studied, despite their expressivity; we therefore
advocate investigating these mediators in §7.
5 Searching for task-relevant mediators
Once one has selected a task and a type of mediator, how does one identify task-relevant mediators of that
type? Theanswerdependslargelyonthetypeofmediatorchosen. IfNNshaveonlyafinitesetofmediators
ofthechosentype—asisthecasefornativemodelcomponentssuchasneurons,layers,andsubmodules—one
could perform an exhaustive search over all possible mediators, choosing which to keep according to some
metric; §5.1 discusses this approach. However, other mediator types, including non-basis-aligned directions
andsubspaces,carryacontinuousspaceofpossiblemediators,renderinganexhaustivesearchimpossible. A
11Notethatthesedirectionsarenotnecessarilysubspacesofactivationspace: thereareoftennon-linearitiesusedincomputing
them, even though the vectors in activation space are involved in computing the directions. Therefore, we will refer to any
1-dimensionalspaceasadirection,butdonotrequireittobeasubspaceofactivationspace.
9common solution to this problem is to employ optimization to either search this space or narrow the space
into an enumerable discrete set, as discussed in §5.2.
5.1 Exhaustive search over mediators
Suppose we are given a neural network with a finite set of candidate mediators {z }N , such as the set
i i=1
of all neurons. One way to identify task-relevant mediators from this set is to assign each mediator z a
i
task-relevancy score S(z ) and then select the mediators with the top scores. This generally entails iterating
i
over each candidate mediator z , setting its activation to some counterfactual value (either from a different
i
input where the answer is flipped, or a value that destroys the information within the neuron, such as its
mean value), and then measuring how much this intervention changes the output. For example, Vig et al.
(2020) and Finlayson et al. (2021) perform counterfactual interventions to the activation of each neuron
individually and then quantify how much each neuron changes the probability of correct completions. The
task relevancy score S(z ) is typically the indirect effect (IE; Pearl, 2001; Robins & Greenland, 1992), as
i
defined in Eq. 1.12 This metric is based on the notion of counterfactual dependence, where we measure the
difference in some output metric m before and after intervening on a given component z .
i
Exhaustive searches have many advantages: their results are comprehensive, causally efficacious, and rela-
tively conceptually precise if our mediators are fine-grained units, like neurons. They are also open-ended,
meaning that we are not required to have a pre-existing causal hypothesis as to how a model performs the
task: we may simply ablate, artificially activate, or otherwise influence a component’s activations, and then
observe how it changes the output behavior or probability of some continuation. Due to these advantages,
this method is the most common when we have a finite set of mediators—for example, in neuron-based
analyses (Vig et al., 2020; Geiger et al., 2021; Finlayson et al., 2021) or attention-head-based analyses (Vig
et al., 2020; Conmy et al., 2023; Syed et al., 2023).
However, exhaustive searches also have two significant disadvantages. The most obvious is that, in its exact
form,anexhaustivesearchrequiresO(N)forwardpasses,whichdoesnotscaleefficientlyasmodelsscale;this
is both because the number of components increases, but also because the computational cost of inference
scaleswithmodelsize. Thismaybewhyexhaustivesearcheshavenotoftenbeenextendedtosetsof neurons
or heads, as this results in a combinatorial explosion in the size of the search space. Searches over sets of
componentscanbeapproximatedusinggreedyortop-kapproaches,asinVigetal.(2020),butthisdoesnot
provide a comprehensive solution to the problem of assigning causal credit to groups of components. That
said, there exist fast linear approximations to activation patching that are technically not causal and not
always accurate, but that only require O(1) forward and backward passes—most prominently, attribution
patching (Kramár et al., 2024; Syed et al., 2023) and improved versions thereof inspired by integrated
gradients (Sundararajan et al., 2017; Marks et al., 2024; Hanna et al., 2024).13
Thesecondandmoredifficultdisadvantagetoovercomeisthatusingexhaustivesearchconstrainsustofinite
sets of mediators. Thus, this approach will not work well as-is if the search space is continuous (infinitely
large). This is a key motivation behind the methods in the following subsection.
5.2 Optimizing over large spaces of mediators
For some types of mediators, the collection of candidate mediators is continuous or far too large to exhaus-
tively search over; this precludes using methods described in §5.1. To search over large but enumerable sets,
someresearchersemploymodifiedversionsofexhaustivesearch,includinggreedysearchmethods(Vigetal.,
2020) or manual searches (Wang et al., 2023). For continuous spaces, however, interpretability researchers
generally use optimization. We taxonomize these optimization problems based on whether they require the
12Other causal metrics include the direct effect, which measures the direct influence of the input on the output behavior
except via the mediator. While more rarely used, it can be a helpful metric in tandem with indirect effects, as in Vig et al.
(2020). There is also the total effect, which is the impact of changing the input on the model’s output behavior. Note that
thetotaleffectdoesnotdirectlyimplicateanyparticularcomponentinmodelbehavior,asitdependsonlyontheinput.
13Gradient-based methods are not causal because they do not directly establish counterfactual dependence. However, they
doprovideascalarvaluewhosemagnitudecanbeinterpretedasalocalapproximationofamodelcomponent’simpactonthe
output.
10Figure3: Neuronsarenotguaranteedtoencodeinterpretablefeatures. Ifnon-basis-aligneddirectionsencode
the true features of interest, then a neuron may activate on many different features that are non-orthogonal
to its corresponding basis. Locating non-basis-aligned mediators requires components in addition to the
model’scomputationgraphthatencodethecoefficientsoneachactivation. Forexample,onecanobtainthese
coefficients via supervised optimization with probing classifiers (§5.2.1) or unsupervised optimization with
sparse autoencoders (§5.2.2). Note that optimization-based techniques sometimes introduce non-linearities,
meaning that the discovered directions will not necessarily be a subspace of activation space.
interpretability researcher to manually select and incorporate task-specific information into the loss func-
tion (supervised methods; §5.2.1) or not (unsupervised methods; §5.2.2). We illustrate the intuition behind
optimization-based search in Figure 3.
5.2.1 Supervised mediator searches
By supervised mediator searches, we mean parametric approaches which require labeled task data and/or
human-generated hypothesized causal graphs. For example, these methods might require the researcher to
propose candidate intermediate concepts which they expect the model to use in performing some task, or
a candidate mechanism by which the model might complete the task. Others might simply require labeled
data for training classifiers.
Supervised probing. In supervised probing approaches, the researcher proposes task-relevant concepts,
andsearchesformediatorswhosevaluesarecorrelated withthoseconcepts. Generally,oneprobesthehidden
representation vector at the end of a particular layer, and the probe searches over all possible subspaces and
directions therein for signals that are predictive of the labels. There exist many papers that employ probing
classifiers (Belinkov & Glass, 2019), though many of them do not validate the causal efficacy of the probe’s
findings (Belinkov, 2021). A drawback of this is that NN units are often correlated with a concept without
causally mediating the concept. Thus this approach can return many false positives—i.e. it may return
proposed mediators which do not actually causally mediate the concept in question (Hewitt & Liang, 2019;
Elazar et al., 2021; Amini et al., 2023; Belinkov, 2021).
Thus,muchrecentworkcomplementssupervisedprobingapproacheswithadditionalchecksofcausality—for
example, by applying causal mediation analysis to the directions identified by supervised probing (Marks
& Tegmark, 2023; Nanda et al., 2023); by backpropagating from the classifier to modify the behavior of
the model (Giulianelli et al., 2018) or generate counterfactual representations (Tucker et al., 2021); or by
directly comparing the probe’s predictions to a causally grounded probe (Amini et al., 2023). Another
line of work uses the directions discovered by probes to guard or erase information about a particular
concept from the model’s representations. For example, a direction in a model’s activation space that
is most predictive of the target concept can be nullified via orthogonal projections, such that the model
can no longer use that information (Ravfogel et al., 2020); this process can then be repeated until linear
guarding is achieved. Concept erasure and guarding can then be used to measure the causal importance
11of particular concepts, as in Elazar et al. (2021), though studies that employ methods like these tend to
focusonsinglelayers. Morerecently,techniquessuchasLEACE(Belroseetal.,2023)andfollow-ups(Singh
et al., 2024b) have generalized this idea to provably prevent any linear classifier from using a concept; this
moves beyond orthogonal projections and projects out the information at every layer. One could use such
methodstocausallyunderstandthesetof directionsthatencodesomeconcept. Notethatthesemethodsare
still susceptible to the problems entailed by using linear mediators; thus, future work could extend erasure
methods to non-linear mediator types.
Counterfactual-based optimization. Another class of approaches involves using the result of causal
mediation analysis as a metric to directly optimize. One such line of work includes methods like Distributed
Alignment Search (DAS) and follow-up methods such as Boundless DAS that align hypothesized high-level
causal variables with underlying neural representations (Geiger et al., 2024; Wu et al., 2023; Huang et al.,
2024). These methods decompose the search space into learnable subspaces that can be aligned with a
hypothesized causal variable.
Another line of work learns a binary mask over enumerable sets of components to determine which are
relevantmediatorsforatask,wherethesizeofthesetdependsonthedesiredgranularity(e.g. setsofneurons,
attentionheads,layers,etc.). Examplesincludesubnetworkprobing(Caoetal.,2021)andDesiderata-based
Component Masking (DCM) (Davies et al., 2023; Prakash et al., 2024).
Thesemethodsprovideatime-efficientwaytosearchforhuman-interpretablevariablesencodedinintractably
largeorinnumerablemediatorsets. However,currentoptimization-basedmethodscanonlysearchforcausal
nodesgivenaclearpre-existingcausalhypothesisofhowamodelaccomplishessomebehavior,and/orlabeled
data. These methods can be evaluated with respect to accuracy in capturing model behavior, but they do
not directly indicate a priori what those hypotheses should be, or in what specific ways our hypotheses
are wrong. They also require sufficient training data to demonstrate the behavior of interest. As with all
parametric methods, the above approaches are subject to overfitting or underfitting, which can be a key
concern when not enough data is available.
5.2.2 Unsupervised mediator searches
Supervisedsearchmethods(see§5.2.1)requirespecifichypothesesabouttheinternalrepresentationsofneural
networks. However,neuralnetworksimplementvariousbehaviors,manyofwhichmaybecounterintuitiveto
humans and therefore more likely to be missed. For example, while Li et al. (2023) hypothesized a constant
board state representation in Othello, Nanda et al. (2023) later found that the model actually switches the
boardstaterepresentationwitheveryturn, takingtheviewof“mypiecesvs.opponent’spieces”ratherthan
“black pieces vs. white pieces”. Therefore, it can be desirable to use techniques for searching for mediators
without specifying a hypothesis ahead of time.
Hence, some studies employ unsupervised methods. Because these techniques are unsupervised, they return
alarge—butfinite—collectionofmediators. Unsupervisedmethodsarelargelycorrelative,meaningthatthe
discovered mediators may not necessarily capture causally relevant or faithful aspects of the NN’s computa-
tion. However, the discovered mediators can then be implicated in NN computation post-hoc by employing
additional techniques, such as those from §5.1, to select task-relevant mediators from this collection.
Feature disentanglement using sparse autoencoders. Exhaustive search for meaningful non-basis-
aligned directions is impossible due to the infinite search space. The feature disentanglement literature
tackles this problem by performing an unsupervised search for directions in neuron activations which both
(1) capture the information encoded in the internal representations and (2) are disentangled from other
meaningfuldirections. Bengioetal.(2013)characterizedisentangledrepresentationsasfactorsforvariations
inthetrainingdataset. Toidentifythesefactorsofvariations,Sharkeyetal.(2023)usedsparseautoencoders
(SAEs) to perform dictionary learning on a one-layer transformer, identifying a large (overcomplete) basis
of features. SAEs are trained to reconstruct the input activations while only activating a sparse subset of
dictionaryfeatures. Cunninghametal.(2024)thenappliedSAEstolanguagemodelsanddemonstratedthat
theobserveddictionaryfeaturesarehighlyinterpretableandcanbeusedtolocalizeandeditmodelbehavior.
Since then, numerous researchers have explored this area (Templeton et al., 2024; Rajamanoharan et al.,
122024; Braun et al., 2024; Bricken et al., 2023, inter alia). In practice, SAEs have shown initial promising
results in identifying functionally relevant and human-interpretable features. However, they are not able to
perfectlyreconstructtheinternalactivations. Mostimportantly, however, wedonotknowa priori whatthe
ground truth features are in the model’s computation, and can only use the reconstruction performance as
a proxy measure of performance.
Correlation-based clustering. Another unsupervised way of discovering meaningful units is clustering
mediators by the similarity of their behavior. This idea is not new (cf. Elman, 1990), but running causal
verifications of the qualitative insights from clustering studies is relatively rare. Dalvi et al. (2020) cluster
neurons, and are able to maintain performance when ablating a significant portion of them; the goal of this
study was not interpretability, but their results nonetheless causally verify that redundancy is very common
in neural networks.
Therehasrecentlybeenrenewedinterestinclustering-basedmediatorsearch. Michaudetal.(2023)propose
a method to identify interpretable behaviors within neural networks by clustering parameters. Because the
identified behaviors tend to be coherent, the units implicated in each cluster can be viewed as a set of
components that have a functionally coherent role in the network. Marks et al. (2024) and Engels et al.
(2024) generalize this from gradients to neuron or sparse autoencoder activations. The activations that
compose the clusters are then labeled according to the dataset samples on which they activate most highly.
These clusters are a subset of all mediators which are relevant to performing some prediction task; thus,
one could perform interventions to the mediator sets that compose a cluster. This idea has not yet been
extensively employed or explored. However, ablating the elements within these clusters could be a useful
way to establish the functional role of groups of components in future work, or assess whether a subset of a
model’s behavior is implicated in a more complex task. We discuss this in §7.
6 Related Work
Causally-grounded interpretability surveys do not always focus on model internals, and surveys that focus
on model internals do not necessarily require causal grounding. We give a brief overview of both types here.
Mechanistic/model-internal interpretability surveys. Some surveys catalogue studies that aim to
understand the latent representations of neural networks (Belinkov & Glass, 2019; Danilevsky et al., 2020;
Belinkov, 2021; Sajjad et al., 2022); these have often called for more causal validations of correlational
observations. More recent surveys tend to focus increasingly on categorizing or giving overviews of methods
for intervening on model internals (Ferrando et al., 2024), understanding the trajectory of the mechanistic
interpretability field (Räuker et al., 2023), and/or cataloguing the impacts of the field (Bereska & Gavves,
2024). We propose a more theoretically grounded framing, and categorize interpretability work from many
domains as part of the causal interpretability literature. Moreover, we treat the units of causal analysis
that a study employs, as well as the way in which the study searches over those units, as primary factors in
categorizing the study.
Causal interpretability surveys. Moraffah et al. (2020) is a causal interpretability survey that cate-
gorizes various streams of causal interpretability research according to the methods they employ, though
the catalogued studies are not necessarily based in the ideas of causal mediation analysis nor aimed toward
understanding model internals. Other interpretability surveys (Subhash et al., 2022; Gilpin et al., 2018;
Singh et al., 2024a) focus on methods for explaining the decisions of neural networks without prioritizing
causallygroundingtheexplanationmethodsorfocusingonmodelinternals. Manycausality-focusedsurveys
are domain-specific, including areas such as cybersecurity (Rawal et al., 2024) and healthcare (Wu et al.,
2024b). Some focus on particular domains; for example, in NLP, some focus on how causal inference can
improveinterpretability(Federetal.,2022),orwaystoexplain(Lyuetal.,2024)orinterpret(Madsenetal.,
2022) neural NLP systems.
Tools. Severallibrarieshaverecentlybeenreleasedtofacilitatecausalinterpretabilitymethodsthatinvolve
interventions to model components. These tools can implicitly prioritize certain types of mediators over
13others. For instance, TransformerLens (Nanda & Bloom, 2022) and libraries based on it (Prisma; Joseph,
2023) are interpretability tools for examining Transformer-based neural networks. Its standardized interface
across model architectures tends to encourage a focus on basis-aligned components, subspaces, and layers,
asinterventionstothesemediatorsarenativelysupported. NeuroX(Dalvietal.,2023)similarlyincentivizes
neuron-level interpretability in particular. NNsight (Fiotto-Kaufman et al., 2024) and Baukit (Bau, 2022)
are more transparent interfaces that expose the model architecture, which may make it slightly harder to
generalize basis-aligned interventions across architectures, but which more easily allows modifications to
the computation graph—thus better incentivizing research on non-basis-aligned spaces. Pyvene (Wu et al.,
2024c) is designed specifically to aid in locating non-basis-aligned multidimensional subspaces; this library
could be particularly useful for those wishing to verify existing causal hypotheses.
7 Discussion and Conclusions
7.1 What is the right mediator?
There are pros and cons to any mediator, and the best mediator will therefore depend on one’s goals. If
the goal is to explain model behaviors, then in the absence of compute restrictions and with no strong
priorhypothesesastohowamodelperformssomebehavior, unsupervisedoptimization-basedmethodsover
fine-grained mediators (such as non-basis-aligned directions) provide a strong starting point. For example,
unsupervised methods like sparse autoencoders provide a fine-grained and potentially editable interface to a
model’scomputation; thatsaid,autoencoderfeaturesarenotguaranteedtobefaithfultoaneuralnetwork’s
behavior in next-token prediction, and they require either a human or an LLM to label or interpret the
features,whichislaboriousandexpensive. Moreover, naturallanguageexplanationsofneuronsandfeatures
haveinherentflaws(Huangetal.,2023): theymayoftenexhibitbothlowprecisionandrecall. Second,inter-
pretablemediatorslikedirectionsrequiremorehumaneffortand/orcomputethanbasis-alignedcomponents
to locate, and optimization may need to be rerun if a model has been significantly fine-tuned or edited.14
If one has a clear idea as to how a model accomplishes a task and simply wishes to verify a mechanistic
hypothesis, then non-basis-aligned subspaces may be the right mediators, and a reasonable corresponding
search method might be counterfactual-based optimization. One can automatically search for the subspaces
which correspond to a particular node in one’s hypothesized causal graph using alignment search methods,
as in Geiger et al. (2024); Wu et al. (2023). Alignment search entails learning a rotation to some activation
subspace given an input, and then performing causal interventions by substituting activations from other
runsintherotatedlatentspace. Thisallowsustolocatedistributedrepresentationsthatactassinglecausal
variables in non-basis-aligned spaces. This is relatively scalable, and enables us to qualitatively understand
intermediate model computations. The primary downside is that we must anticipate the mechanisms that
models employ to perform a task; if we cannot anticipate them, then curating data and refining one’s causal
hypotheses may require significant human effort. Additionally, the same causal graph could correspond to
many different qualitative explanations, depending on the data used to discover the causal graph. Finally,
this mediator type is subject to the same confounds as other optimization-based techniques.
If one’s goal is to localize some phenomenon in a model and not to understand how a model implements
a behavior, then exhaustive searches over basis-aligned subspaces or full layers may be sufficient. There are
many comprehensive causal techniques for locating these, including causal tracing (Meng et al., 2022) and
activation patching (Vig et al., 2020), as well as techniques for locating graphs of basis-aligned mediators,
such as circuit discovery algorithms (Goldowsky-Dill et al., 2023; Wang et al., 2023; Conmy et al., 2023).
Some of these methods are relatively slow in their exact form, but fast approximations exist to these causal
metrics, including attribution patching (Syed et al., 2023) and improved versions thereof (Kramár et al.,
2024; Hanna et al., 2024; Marks et al., 2024). Even in the absence of a deep understanding of the role of
thesemediators,localizationcanbeusefulfordownstreamapplicationslikemodelediting(Mengetal.,2022;
2023)15 and model steering (Todd et al., 2024; Goyal et al., 2020). That said, if meaningful features are not
actually aligned with neurons/heads, then we are not guaranteed to get the best performance until we move
14Though Prakash et al. (2024) find that the same model components are implicated in an entity tracking task before and
afterfine-tuning.
15ThoughHaseetal.(2023)findthatcausallocalizationsdonotreflecttheoptimallocationsforeditingmodels.
14beyond basis-aligned spaces. Future work should analyze the performance of model editing and steering
methods when using different kinds of mediators. For example, Marks et al. (2024) compare the efficacy of
ablating neurons versus sparse features (non-basis-aligned directions), and find that ablating sparse features
is significantly more effective; the difference may be much smaller at the coarse granularity of full layers and
submodules, but there is not yet much empirical evidence on what kinds of mediators are most effective for
particular applications.
7.2 Suggestions for Future Work
7.2.1 Are there better causal mediators?
There are almost certainly better causal mediators that have not yet been discovered. Current work on
improving mediators tends to focus on non-basis-aligned directions, such as sparse features or directions
discovered from supervised probing on the activations of a single layer/submodule. One could consider
pursuing coarser-grained mediators by discovering multi-layer model regions or component sets which
accomplish a single behavior. Because these regions can cross layers, they would include non-linearities that
allow them to represent more complex functions or concepts.
Non-linear and multi-dimensional feature discovery. As discussed in §4.4, there is recent work
demonstrating the existence of human-interpretable multi-dimensional features. For example, days of the
week are encoded circularly as a set of 7 directions in a two-dimensional subspace (Engels et al., 2024),
and current methods cannot easily capture such multi-dimensional features. Group sparse autoencoders
(Theodosis & Ba, 2023) or clusters of autoencoder features could be a way to capture multi-dimensional
non-basis-aligned features in an unsupervised manner, but thus far, empirical work has not yet demon-
strated whether this will be effective for interpreting neural networks. Additionally, many current causal
interpretability methods require binary distinctions between correct and incorrect answers, whereas causal
mediation analysis does not have any theoretical linearity, dimensionality, or Boolean restrictions.
There may also exist higher-order non-linear concepts in latent space that we have not yet been able to
find, due to the linear focus of contemporary methods. For example, a subgraph or subcircuit can encode
a coherent variable representation or functional role, as in Lepori et al. (2023). How can we discover these
subgraphs? Path patching (Goldowsky-Dill et al., 2023; Wang et al., 2023) provides a manual approach to
implicating subgraphs as causal mediators, we do not yet have automatic methods that can scalably search
over all possible subgraphs in a network. How might non-linear and/or coarse-grained mediators like these
beusefulinpractice? Asanexample,wewouldexpectfundamentalphenomenalikesyntaxtobeimplicated
in downstream tasks like question answering, if we expect that language models are robustly parsing the
meaning of their inputs. Thus, we could implicate the entire syntax region(s) in the model’s final decisions;
if it is not strongly implicated in QA performance, then we have a strong hint that the model may instead
be relying on a mixture of surface-level spurious heuristics to parse inputs.
7.2.2 Inherently interpretable model components
Moreambitiously,onecouldconsiderbuildingmodelswithinherentlyinterpretablecomponents—i.e.,whose
fundamentalunitsofcomputation(orsomesubsetthereof)aredesignedtobesparse,monosemantic,and/or
human-interpretable, but ideally still expressive enough to attain good performance on downstream tasks.
Examples based in neural networks include differentiable masks (De Cao et al., 2020; Bastings et al., 2019),
transcoders(Dunefskyetal.,2024),codebookfeatures(Tamkinetal.,2023),andsoftmaxlinearunits(Elhage
etal.,2022a). Theseareprimarilypost-hoccomponentsthatdecomposemodelcomponentsintointerpretable
units, but they could potentially be integrated into the network itself during pre-training alongside a loss
term(inadditiontothelanguagemodelingloss)thatenablesfine-grainedinterpretabilityatallstagesofpre-
training. Alternatively, more focus could be devoted to building models that are designed from the ground
up to be interpretable, such as backpack language models (Hewitt et al., 2023). Perhaps least invasively,
we could consider pre-training methods that encourage interpretable features to be aligned to neuron bases;
this would remove the need for optimization to find non-basis-aligned components, and therefore make
interpreting NN decisions significantly easier and less confounded. However, this would reduce the number
15of features that could be encoded per neuron, so it would likely entail training significantly larger models,
or accepting degradations in performance.
7.2.3 Scalable search
As the size of neural networks increases, the number of potential mediators to search over will also increase.
Thesituationworsensaswestartsearchingovercontinuoussetsoffine-grainedmediatorssuchasnon-basis-
aligned directions. Although a few gradient-based or optimization-based approximations to causal influence
have been proposed to improve time efficiency, such as attribution patching (Syed et al., 2023) and DCM
(Davies et al., 2023), more work is still needed to evaluate the efficacy of these techniques in identifying the
correct causal mediators. Additionally, better techniques beyond greedy search methods should be devised
to identify causally important groups of mediators; these should aim to produce Pareto improvements over
time complexity and causal efficacy.
As discussed in §5.2.1, optimization-based mediator search methods often require a pre-existing hypothesis
about how a model implements a particular behavior of interest. Another path toward scaling mediator
search and interpretability is to automate the process of hypothesis generation. Qiu et al. (2024) showed
that current LLMs can generate hypotheses, and Shaham et al. (2024) showed that hypothesis refinement
via LLMs can aid humans in interpreting the causal role of neurons in multimodal models. Similary, LLMs
could be used to automate and scale hypothesis generation regarding the role of particular mediators across
a wider variety of tasks and models. Optimization-based based methods such as DAS or DCM could then
be used to causally verify the automatically generated hypotheses.
7.2.4 Benchmarking progress in mechanistic interpretability
Another key direction will be standard benchmarks for measuring progress in mechanistic inter-
pretability. Currently, most studies develop ad-hoc evaluations, and generally only compare to similar
methods that employ the same mediators. Thus, to measure whether new mediators or search methods are
truly giving us improvements over previous ones, we need to develop principled methods for direct com-
parisons. In circuit discovery, it is theoretically possible to use the same metrics to compare any circuit
discovered for a particular model and task, regardless of whether sparse autoencoders are used, whether the
circuit is based on nodes or edges, among other variations. Direct comparisons like these are not standard,
thoughsomerecentworkhasbeguntoperformdirectcomparisonsacrossmediatortypes,suchasMilleretal.
(2024). Huangetal.(2024)proposetodirectlyevaluateinterpretabilitymethodsaccordingtothegenerality
of the abstractions they recover, and do directly compare across different mediator types given the same
model and task. Arora et al. (2024) and Makelov et al. (2024) also propose standardized interpretability
benchmarks that allowus to compare across mediator search methods, though they do not directly compare
across mediator types.
Direct comparisons require defining criteria for success, but there is little agreement about the kinds of
phenomena we should be measuring, and precisely how they should be measured. Taking circuit discovery
as a case study, there are (at least) five key metrics: (1) faithfulness, or how well the circuit captures the
full model’s behavior; (2) generalization, or how well the circuit generalizes outside the distribution of
the dataset used to discover it; (3) completeness, or whether we have captured all relevant components;
(4) minimality, or whether we have not included any superfluous components; and (5) interpretability,
which refers to how human-understandable the circuit is. Some of these metrics currently have multiple
contradictorysenses: (1)issometimesusedtorefertowhetherthecircuitcapturesthefullmodel’sbehavior
(includingproductiveand counterproductivecomponents),andinothercasesisusedtorefertowhetherthe
circuit only captures components that result in high performance on the task.16 The choice of mediator will
also significantly affect how well we can perform each of these, as obtaining a complete and faithful circuit
requires working at the correct level of abstraction (i.e., where meaningful units of computation are actually
represented). Additionally, these metrics can be difficult to automatically measure; for example, (5) may
require human evaluations at first (Saphra et al., 2024), as many NLP tasks such as machine translation
16Even if we decide on one of these definitions, there is still the issue of how to compare the full model to the discovered
subgraph. We could use KL divergences, compare probabilities of a given token, among other possibilities; Zhang & Nanda
(2024)investigatethisindetail.
16required before the introduction of now-standard17 metrics such as BLEU, COMET, and METEOR scores
(Papineni et al., 2002; Rei et al., 2020; Banerjee & Lavie, 2005). Some tasks have started to integrate hu-
man/user evaluations, which will be especially useful for building interpretability tools that are grounded in
real-world use cases and settings (Saphra et al., 2024). Future work could consider defining a broad set of
models, target tasks, and informative metrics on which researchers can compare their interpretability meth-
ods, mediator types, and search methods in a standardized way. This will enable us to assess whether new
searchmethodsandmediatortypesareproducingrealParetoadvancementswithrespecttointerpretability,
efficiency, and description length.
For many methods and tasks, more task-specific measures and benchmark datasets will be needed. For ex-
ample, Cohen et al. (2024) and Zhong et al. (2023) propose benchmarks to evaluate model editing methods
on out-of-distribution examples, and Karvonen et al. (2024) propose to measure progress in feature disen-
tanglement using board game models. While not the main focus of this paper, we believe that building
standardized benchmarks will be a key means to the end of assessing whether advancements in causal medi-
ators are producing real gains in interpretability. More robust evaluation metrics and methods will lead to
better science, which will ideally allow us to assess whether new causal abstractions are fundamentally more
useful—both for describing the computations of a neural network and for practical applications.
References
Afra Amini, Tiago Pimentel, Clara Meister, and Ryan Cotterell. Naturalistic causal probing for morpho-
syntax. Transactions of the Association for Computational Linguistics, 11:384–403, 2023. doi: 10.1162/
tacl_a_00554. URL https://aclanthology.org/2023.tacl-1.23.
Aryaman Arora, Dan Jurafsky, and Christopher Potts. CausalGym: Benchmarking causal interpretability
methods on linguistic tasks. arXiv:2402.12560, 2024. URL https://arxiv.org/abs/2402.12560.
SanjeevArora,YuanzhiLi,YingyuLiang,TengyuMa,andAndrejRisteski.Alatentvariablemodelapproach
toPMI-basedwordembeddings.TransactionsoftheAssociationforComputationalLinguistics,4:385–399,
2016. doi: 10.1162/tacl_a_00106. URL https://aclanthology.org/Q16-1028.
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of
wordsenses,withapplicationstopolysemy. TransactionsoftheAssociationforComputationalLinguistics,
6:483–495, 2018. doi: 10.1162/tacl_a_00034. URL https://aclanthology.org/Q18-1034.
DzmitryBahdanau, KyunghyunCho, andYoshuaBengio. Neuralmachinetranslationbyjointlylearningto
align and translate. In International Conference on Learning Representations, 2015.
Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or summarization, pp. 65–72, 2005. URL https:
//aclanthology.org/W05-0909/.
Jasmijn Bastings, Wilker Aziz, and Ivan Titov. Interpretable neural predictions with differentiable binary
variables. In Anna Korhonen, David Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pp. 2963–2977, Florence, Italy, July 2019. As-
sociationforComputationalLinguistics. doi: 10.18653/v1/P19-1284. URLhttps://aclanthology.org/
P19-1284.
AnthonyBau,YonatanBelinkov,HassanSajjad,NadirDurrani,FahimDalvi,andJamesR.Glass. Identify-
ing and controlling important neurons in neural machine translation. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019a.
URL https://openreview.net/forum?id=H1z-PsR5KX.
David Bau. Baukit, 2022. URL https://github.com/davidbau/baukit.
17"Standard"doesnotnecessarilymean"representativeofhumanjudgments".
17David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, and
Antonio Torralba. Visualizing and understanding generative adversarial networks. In International Con-
ference on Learning Representations, 2019b. URL https://openreview.net/forum?id=Hyg_X2C5FX.
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba. Under-
standing the role of individual units in a deep neural network. Proceedings of the National Academy of
Sciences, 117(48):30071–30078, 2020. doi: 10.1073/pnas.1907375117. URL https://www.pnas.org/doi/
abs/10.1073/pnas.1907375117.
Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and alternatives. CoRR, abs/2102.12452,
2021. URL https://arxiv.org/abs/2102.12452.
YonatanBelinkovandJamesGlass. Analysismethodsinneurallanguageprocessing: Asurvey. Transactions
oftheAssociationforComputationalLinguistics,7:49–72,2019. doi: 10.1162/tacl_a_00254. URLhttps:
//aclanthology.org/Q19-1004.
Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural machine
translation models learn about morphology? In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of
the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
861–872, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/
P17-1080. URL https://aclanthology.org/P17-1080.
Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman.
LEACE:Perfectlinearconcepterasureinclosedform.InThirty-seventhConferenceonNeuralInformation
Processing Systems, 2023. URL https://openreview.net/forum?id=awIpKpwTwF.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new per-
spectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2013. doi:
10.1109/TPAMI.2013.50. URL https://dl.acm.org/doi/10.1109/TPAMI.2013.50.
LeonardBereskaandEfstratiosGavves. MechanisticinterpretabilityforAIsafety–Areview. arXivpreprint
arXiv:2404.14082, 2024. URL https://arxiv.org/abs/2404.14082.
Olcay Boz. Extracting decision trees from trained neural networks. In Proceedings of the Eighth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02, pp. 456–461,
New York, NY, USA, 2002. Association for Computing Machinery. ISBN 158113567X. doi: 10.1145/
775047.775113. URL https://doi.org/10.1145/775047.775113.
Dan Braun, Jordan Taylor, Nicholas Goldowsky-Dill, and Lee Sharkey. Identifying functionally important
features with end-to-end sparse dictionary learning. arXiv preprint arXiv:2405.12241, 2024. URL https:
//arxiv.org/abs/2405.12241.
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,
CemAnil,CarsonDenison,AmandaAskell,RobertLasenby,YifanWu,ShaunaKravec,NicholasSchiefer,
Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,
Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monose-
manticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023.
URL https://transformer-circuits.pub/2023/monosemantic-features/index.html.
Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, and Christian Bartelt. A mechanistic
analysisofatransformertrainedonasymbolicmulti-stepreasoningtask.arXivpreprintarXiv:2402.11917,
2024. URL https://arxiv.org/abs/2402.11917.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Lan-
guage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and
18H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
Steven Cao, Victor Sanh, and Alexander Rush. Low-complexity probing via finding subnetworks. In
Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard,
Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, pp. 960–966, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
naacl-main.74. URL https://aclanthology.org/2021.naacl-main.74.
Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L Leavitt, and Naomi Saphra. Sudden
drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs. In The Twelfth
International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?
id=MO5PiKHELW.
Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel,
Jan Riecke, Shivam Raval, Olivia Seow, et al. Designing a dashboard for transparency and control of
conversational AI. arXiv preprint arXiv:2406.07882, 2024b. URL https:arxiv.org/abs/2406.07882.
Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how
networkslearngroupoperations.InProceedingsofthe40thInternationalConferenceonMachineLearning.
JMLR, 2023. URL https://dl.acm.org/doi/10.5555/3618408.3618656.
Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the Ripple Effects of
Knowledge Editing in Language Models. Transactions of the Association for Computational Linguistics,
12:283–298, 04 2024. ISSN 2307-387X. doi: 10.1162/tacl_a_00644. URL https://doi.org/10.1162/
tacl_a_00644.
Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso.
Towardsautomatedcircuitdiscoveryformechanisticinterpretability.InA.Oh,T.Naumann,A.Globerson,
K.Saenko,M.Hardt,andS.Levine(eds.),AdvancesinNeuralInformationProcessingSystems,volume36,
pp.16318–16352.CurranAssociates,Inc.,2023. URLhttps://proceedings.neurips.cc/paper_files/
paper/2023/file/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Paper-Conference.pdf.
Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. What you can
cramintoasingle$&!#*vector: Probingsentenceembeddingsforlinguisticproperties. InIrynaGurevych
and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 2126–2136, Melbourne, Australia, July 2018. Association for
Computational Linguistics. doi: 10.18653/v1/P18-1198. URL https://aclanthology.org/P18-1198.
Mark Craven and Jude Shavlik. Extracting tree-structured representations of trained networks. Advances
in Neural Information Processing Systems, 8, 1995.
Mark W. Craven and Jude W. Shavlik. Using sampling and queries to extract rules from trained neu-
ral networks. In William W. Cohen and Haym Hirsh (eds.), Machine Learning Proceedings 1994, pp.
37–45. Morgan Kaufmann, San Francisco (CA), 1994. ISBN 978-1-55860-335-6. doi: https://doi.org/
10.1016/B978-1-55860-335-6.50013-1. URL https://www.sciencedirect.com/science/article/pii/
B9781558603356500131.
HoagyCunningham,LoganRiggsSmith,AidanEwart,RobertHuben,andLeeSharkey.Sparseautoencoders
findhighlyinterpretablefeaturesinlanguagemodels.InTheTwelfthInternationalConferenceonLearning
Representations, 2024. URL https://openreview.net/forum?id=F76bwRSLeK.
Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained
transformer models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the
2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.4908–4926,Online,
November2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-main.398. URL
https://aclanthology.org/2020.emnlp-main.398.
19Fahim Dalvi, Hassan Sajjad, and Nadir Durrani. NeuroX library for neuron analysis of deep NLP models.
InDanushkaBollegala,RuihongHuang,andAlanRitter(eds.),Proceedings of the 61st Annual Meeting of
theAssociationforComputationalLinguistics(Volume3: SystemDemonstrations),pp.226–234,Toronto,
Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-demo.21. URL
https://aclanthology.org/2023.acl-demo.21.
Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey
of the state of explainable AI for natural language processing. In Kam-Fai Wong, Kevin Knight, and
Hua Wu (eds.), Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for
Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,
pp. 447–459, Suzhou, China, December 2020. Association for Computational Linguistics. URL https:
//aclanthology.org/2020.aacl-main.46.
Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, and David Bau. Discovering variable
binding circuitry with desiderata. arXiv preprint arXiv:2307.03637, 2023. URL https://arxiv.org/
abs/2307.03637.
Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz, and Ivan Titov. How do decisions emerge across
layersinneuralmodels? Interpretationwithdifferentiablemasking.InBonnieWebber,TrevorCohn,Yulan
He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP),pp.3243–3255, Online, November2020.AssociationforComputationalLinguistics.
doi: 10.18653/v1/2020.emnlp-main.262. URL https://aclanthology.org/2020.emnlp-main.262.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirec-
tionaltransformersforlanguageunderstanding.InJillBurstein,ChristyDoran,andThamarSolorio(eds.),
Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Min-
neapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://aclanthology.org/N19-1423.
Jacob Dunefsky, Philippe Chlenski, and Neel Nanda. Transcoders find interpretable LLM feature circuits.
2024. URL https://arxiv.org/abs/2406.11944.
Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation
with amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:160–175,
032021. ISSN2307-387X. doi: 10.1162/tacl_a_00359. URLhttps://doi.org/10.1162/tacl_a_00359.
NelsonElhage, NeelNanda,CatherineOlsson, TomHenighan,NicholasJoseph, BenMann, AmandaAskell,
Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,
Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom
Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for
transformer circuits. Transformer Circuits Thread, 2021. URL https://transformer-circuits.pub/
2021/framework/index.html.
Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer
ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal
Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-
Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Ja-
cobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei,
and Christopher Olah. Softmax linear units. Transformer Circuits Thread, 2022a. URL https:
//transformer-circuits.pub/2022/solu/index.html.
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac
Hatfield-Dodds,RobertLasenby,DawnDrain,CarolChen,RogerGrosse,SamMcCandlish,JaredKaplan,
Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition. Transformer
Circuits Thread, 2022b. URL https://transformer-circuits.pub/2022/toy_model/index.html.
20JeffreyLElman. Representationandstructureinconnectionistmodels. University of California, San Diego,
Center for Research in Language, 1989.
Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14:179–211, 1990. URL https://api.
semanticscholar.org/CorpusID:2763403.
JeffreyL.Elman. Distributedrepresentations,simplerecurrentnetworks,andgrammaticalstructure. Mach.
Learn., 7(2–3):195–225, sep 1991. ISSN 0885-6125. doi: 10.1007/BF00114844. URL https://doi.org/
10.1007/BF00114844.
JoshuaEngels,IsaacLiao,EricJ.Michaud,WesGurnee,andMaxTegmark. Notalllanguagemodelfeatures
are linear, 2024. URL https://arxiv.org/abs/2405.14860.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of
a deep network. Technical Report 1341, University of Montreal, June 2009. Also presented at the ICML
2009 Workshop on Learning Feature Hierarchies, Montréal, Canada.
Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty,
JacobEisenstein,JustinGrimmer,RoiReichart,MargaretE.Roberts,BrandonM.Stewart,VictorVeitch,
and Diyi Yang. Causal inference in natural language processing: Estimation, prediction, interpretation
and beyond. Transactions of the Association for Computational Linguistics, 10:1138–1158, 2022. doi:
10.1162/tacl_a_00511. URL https://aclanthology.org/2022.tacl-1.66.
JavierFerrando,GabrieleSarti,AriannaBisazza,andMartaR.Costa-jussà. Aprimerontheinnerworkings
of transformer-based language models, 2024. URL https://arxiv.org/abs/2405.00208.
MatthewFinlayson,AaronMueller,SebastianGehrmann,StuartShieber,TalLinzen,andYonatanBelinkov.
Causal analysis of syntactic agreement mechanisms in neural language models. In Chengqing Zong, Fei
Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on Natural Language Processing
(Volume1: LongPapers),pp.1828–1843,Online,August2021.AssociationforComputationalLinguistics.
doi: 10.18653/v1/2021.acl-long.144. URL https://aclanthology.org/2021.acl-long.144.
Jaden Fiotto-Kaufman, Alexander R Loftus, Eric Todd, Jannik Brinkmann, Caden Juang, Koyena Pal,
Can Rager, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Michael Ripa, Adam
Belfki, Nikhil Prakash, Sumeet Multani, Carla Brodley, Arjun Guha, Jonathan Bell, Byron Wallace,
and David Bau. Nnsight and ndif: Democratizing access to foundation model internals, 2024. URL
https://arxiv.org/abs/2407.14561.
Rohit Gandikota, Joanna Materzyńska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from
diffusion models. In Proceedings of the 2023 IEEE International Conference on Computer Vision, 2023.
Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzyńska, and David Bau. Unified concept
editing in diffusion models. IEEE/CVF Winter Conference on Applications of Computer Vision, 2024.
Jorge García-Carrasco, Alejandro Maté, and Juan Trujillo. How does GPT-2 predict acronyms? Extracting
and understanding a circuit via mechanistic interpretability, 2024. URL https://arxiv.org/abs/2405.
04156.
AtticusGeiger,HansonLu,ThomasFIcard,andChristopherPotts. Causalabstractionsofneuralnetworks.
InA.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan(eds.),AdvancesinNeuralInformation
Processing Systems, 2021. URL https://openreview.net/forum?id=RmuXDtjDhG.
AtticusGeiger, ZhengxuanWu, ChristopherPotts, ThomasIcard, andNoahGoodman. Findingalignments
between interpretable causal variables and distributed neural representations. In Causal Learning and
Reasoning, pp. 160–187. PMLR, 2024.
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations
in auto-regressive language models. In The 2023 Conference on Empirical Methods in Natural Language
Processing, 2023. URL https://openreview.net/forum?id=F1G7y94K02.
21Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. Explain-
ing explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International
Conference on data science and advanced analytics (DSAA), pp. 80–89. IEEE, 2018.
Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema. Under the hood:
Using diagnostic classifiers to investigate and improve how language models track agreement information.
In Tal Linzen, Grzegorz Chrupała, and Afra Alishahi (eds.), Proceedings of the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 240–248, Brussels, Belgium,
November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5426. URL https:
//aclanthology.org/W18-5426.
Nicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato, and Aryaman Arora. Localizing model behavior with
path patching, 2023. URL https://arxiv.org/abs/2304.05969.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes,
N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, vol-
ume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/
2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.
YashGoyal,AmirFeder,UriShalit,andBeenKim. Explainingclassifierswithcausalconcepteffect(CaCE).
arXiv preprint arXiv:1907.07165, 2020. URL https://arxiv.org/abs/1907.07165.
Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Padó. Distributional vectors encode ref-
erential attributes. In Lluís Màrquez, Chris Callison-Burch, and Jian Su (eds.), Proceedings of the
2015 Conference on Empirical Methods in Natural Language Processing, pp. 12–21, Lisbon, Portu-
gal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1002. URL
https://aclanthology.org/D15-1002.
MichaelHanna,OllieLiu,andAlexandreVariengien. HowdoesGPT-2computegreater-than?: Interpreting
mathematicalabilitiesinapre-trainedlanguagemodel. InA.Oh,T.Naumann,A.Globerson,K.Saenko,
M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp.
76033–76060. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/
paper/2023/file/efbba7719cc5172d175240f24be11280-Paper-Conference.pdf.
Michael Hanna, Sandro Pezzelle, and Yonatan Belinkov. Have faith in faithfulness: Going beyond circuit
overlap when finding model mechanisms. In ICML 2024 Workshop on Mechanistic Interpretability, 2024.
URL https://openreview.net/forum?id=grXgesr5dT.
PeterHase, MohitBansal, BeenKim, andAsmaGhandeharioun. Doeslocalizationinformediting? Surpris-
ing differences in causality-based localization vs. knowledge editing in language models. In Thirty-seventh
Conference on Neural Information Processing Systems,2023. URLhttps://openreview.net/forum?id=
EldbUlZtbd.
Yoichi Hayashi. A neural expert system with automated extraction of fuzzy if-then rules and its application
to medical diagnosis. In R.P. Lippmann, J. Moody, and D. Touretzky (eds.), Advances in Neural Infor-
mation Processing Systems, volume 3. Morgan-Kaufmann, 1990. URL https://proceedings.neurips.
cc/paper_files/paper/1990/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.
John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In Kentaro Inui, Jing
Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pp. 2733–2743, Hong Kong, China, November 2019. Association for Computational
Linguistics. doi: 10.18653/v1/D19-1275. URL https://aclanthology.org/D19-1275.
22John Hewitt and Christopher D. Manning. A structural probe for finding syntax in word representations.
In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the
NorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,
Volume 1 (Long and Short Papers), pp. 4129–4138, Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1419. URL https://aclanthology.org/N19-1419.
John Hewitt, Kawin Ethayarajh, Percy Liang, and Christopher Manning. Conditional probing: Measuring
usable information beyond a baseline. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and
Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pp.1626–1639, OnlineandPuntaCana, DominicanRepublic, November2021.Associationfor
Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.122. URL https://aclanthology.org/
2021.emnlp-main.122.
John Hewitt, John Thickstun, Christopher Manning, and Percy Liang. Backpack language models. In
Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9103–9125, Toronto,
Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.506. URL
https://aclanthology.org/2023.acl-long.506.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, nov
1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.
9.8.1735.
Jing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts. Rigorously
assessing natural language explanations of neurons. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet,
Najoung Kim, Arya McCarthy, and Hosein Mohebbi (eds.), Proceedings of the 6th BlackboxNLP Work-
shop: Analyzing and Interpreting Neural Networks for NLP, pp. 317–331, Singapore, December 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.24. URL https:
//aclanthology.org/2023.blackboxnlp-1.24.
Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, and Atticus Geiger. RAVEL: Evaluating inter-
pretability methods on disentangling language model representations. arXiv preprint arXiv:2402.17700,
2024. URL https://arxiv.org/abs/2402.17700.
Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. Visualisation and ’diagnostic classifiers’ reveal how
recurrent and recursive neural networks process hierarchical structure. Journal of Artificial Intelligence
Research, 61:907–926, 2018.
Shadi Iskander, Kira Radinsky, and Yonatan Belinkov. Shielded representations: Protecting sensitive
attributes through iterative gradient-based projection. In Anna Rogers, Jordan Boyd-Graber, and
Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 5961–
5977, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.
findings-acl.369. URL https://aclanthology.org/2023.findings-acl.369.
Jett Janiak, cmathw, and Stefan Heimersheim. Polysemantic attention head in a 4-
layer transformer, 2023. URL https://www.lesswrong.com/posts/nuJFTS5iiJKT5G5yh/
polysemantic-attention-head-in-a-4-layer-transformer. Accessed: 2024-05-26.
Adam Jermyn, Chris Olah, and Tom Henighan. Attention head superposition, 2023. URL https:
//transformer-circuits.pub/2023/may-update/index.html#attention-superposition.
Sonia Joseph. Vit prisma: A mechanistic interpretability library for vision transformers. https://github.
com/soniajoseph/vit-prisma, 2023.
JaredKaplan,SamMcCandlish,TomHenighan,TomB.Brown,BenjaminChess,RewonChild,ScottGray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361, arXiv:2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.
23E.D.Karnin. Asimpleprocedureforpruningback-propagationtrainedneuralnetworks. IEEETransactions
on Neural Networks, 1(2):239–242, 1990. doi: 10.1109/72.80236.
Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. In
The Fourth International Conference on Learning Representations,2016. URLhttps://arxiv.org/abs/
1506.02078.
Adam Karvonen, Benjamin Wright, Can Rager, Rico Angell, Jannik Brinkmann, Logan Riggs Smith, Clau-
dioMayrinkVerdun,DavidBau,andSamuelMarks.Measuringprogressindictionarylearningforlanguage
modelinterpretabilitywithboardgamemodels. InICML 2024 Workshop on Mechanistic Interpretability,
2024. URL https://openreview.net/forum?id=qzsDKwGJyB.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Doina
Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning Research, pp. 1885–1894. PMLR, 06–11 Aug 2017. URL
https://proceedings.mlr.press/v70/koh17a.html.
Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy
Liang. Concept bottleneck models. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume119ofProceedings of Machine Learning Research,
pp. 5338–5348. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/koh20a.html.
Arne Köhn. What’s in an embedding? Analyzing word embeddings through multilingual evaluation. In
LluísMàrquez,ChrisCallison-Burch,andJianSu(eds.),Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pp. 2067–2073, Lisbon, Portugal, September 2015. Association
forComputationalLinguistics.doi: 10.18653/v1/D15-1246.URLhttps://aclanthology.org/D15-1246.
János Kramár, Tom Lieberum, Rohin Shah, and Neel Nanda. AtP*: An efficient and scalable method for
localizing llm behaviour to components. arXiv preprint arXiv:2403.00745, 2024. URL https://arxiv.
org/abs/2403.00745.
R. Krishnan, G. Sivakumar, and P. Bhattacharya. Extracting decision trees from trained neu-
ral networks. Pattern Recognition, 32(12):1999–2009, 1999. ISSN 0031-3203. doi: https://doi.
org/10.1016/S0031-3203(98)00181-2. URL https://www.sciencedirect.com/science/article/pii/
S0031320398001812.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (eds.), Advances in Neural
Information Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://proceedings.
neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
Vedang Lad, Wes Gurnee, and Max Tegmark. The remarkable robustness of LLMs: Stages of inference?,
2024. URL https://arxiv.org/abs/2406.19384.
YairLakretz,GermanKruszewski,TheoDesbordes,DieuwkeHupkes,StanislasDehaene,andMarcoBaroni.
The emergence of number and syntax units in LSTM language models. In Jill Burstein, Christy Doran,
and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pp. 11–20, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1002. URL https://aclanthology.org/N19-1002.
Yair Lakretz, Théo Desbordes, Jean-Rémi King, Benoît Crabbé, Maxime Oquab, and Stanislas Dehaene.
Can RNNs learn recursive nested subject-verb agreements? arXiv preprint arXiv:2101.02258, 2021. URL
https://arxiv.org/abs/2101.02258.
Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. Probing for the
usage of grammatical number. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.),
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
24Long Papers), pp. 8818–8831, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:
10.18653/v1/2022.acl-long.603. URL https://aclanthology.org/2022.acl-long.603.
MichaelA.Lepori,ThomasSerre,andElliePavlick. Uncoveringintermediatevariablesintransformersusing
circuit probing. 2023. URL https://arxiv.org/abs/2311.04354.
David Lewis. Causation. In Tim Crane and Katalin Farkas (eds.), Philosophical Papers II, pp. 159–213.
Oxford University Press, 1986.
DavidLewis. Causationasinfluence. The Journal of Philosophy,97(4):182–197,2000. ISSN0022362X. URL
http://www.jstor.org/stable/2678389.
David K. Lewis. Counterfactuals. Blackwell, Malden, Massachusetts, 1973.
Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure,
2017. URL https://arxiv.org/abs/1612.08220.
Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg.
Emergentworldrepresentations: Exploringasequencemodeltrainedonasynthetictask. InThe Eleventh
International Conference on Learning Representations,2023. URLhttps://openreview.net/forum?id=
DeG07_TcZvT.
MaximilianLi,XanderDavies,andMaxNadeau. Circuitbreaking: Removingmodelbehaviorswithtargeted
ablation. arXivpreprintarXiv:2309.05973,arXiv:2309.05973,2024. URLhttps://arxiv.org/abs/2309.
05973.
Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic
knowledge and transferability of contextual representations. In Jill Burstein, Christy Doran, and Thamar
Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),pp.1073–
1094, Minneapolis, Minnesota, June 2019a. Association for Computational Linguistics. doi: 10.18653/v1/
N19-1112. URL https://aclanthology.org/N19-1112.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,Luke
Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv
preprint arXiv:1907.11692, 2019b. URL https://arxiv.org/abs/1907.11692.
Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceedings
of the 31st International Conference on Neural Information Processing Systems, NIPS’17, pp. 4768–4777,
Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
Qing Lyu, Marianna Apidianaki, and Chris Callison-Burch. Towards faithful model explanation in NLP: A
survey. Computational Linguistics, 50(2):657–723, 06 2024. ISSN 0891-2017. doi: 10.1162/coli_a_00511.
URL https://doi.org/10.1162/coli_a_00511.
Weicheng Ma, Kai Zhang, Renze Lou, Lili Wang, and Soroush Vosoughi. Contributions of transformer
attention heads in multi- and cross-lingual tasks. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto
Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics
and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
pp. 1956–1966, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
acl-long.152. URL https://aclanthology.org/2021.acl-long.152.
Andreas Madsen, Siva Reddy, and Sarath Chandar. Post-hoc interpretability for neural NLP: A survey.
ACM Comput. Surv., 55(8), dec 2022. ISSN 0360-0300. doi: 10.1145/3546577. URL https://doi.org/
10.1145/3546577.
AleksandarMakelov,GeorgeLange,andNeelNanda. Towardsprincipledevaluationsofsparseautoencoders
for interpretability and control. arXiv preprint arXiv:2405.08366, 2024. URL https://arxiv.org/abs/
2405.08366.
25SamuelMarksandMaxTegmark. Thegeometryoftruth: Emergentlinearstructureinlargelanguagemodel
representationsoftrue/falsedatasets. arXiv preprint arXiv:2310.06824,2023. URLhttps://arxiv.org/
abs/2310.06824.
Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse
feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint
arXiv:2403.19647, arXiv:2403.19647, 2024. URL https://arxiv.org/abs/2403.19647.
James L. McClelland and David E. Rumelhart. Distributed memory and the representation of general and
specific information. Journal of experimental psychology. General, 114 2:159–97, 1985. URL https:
//api.semanticscholar.org/CorpusID:7745106.
Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg. The hydra effect:
Emergent self-repair in language model computations. arXiv preprint arXiv:2307.15771, 2023. URL
https://arxiv.org/abs/2307.15771.
KevinMeng,DavidBau,AlexJAndonian,andYonatanBelinkov. Locatingandeditingfactualassociations
in GPT. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in
Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=-h6WAS6eE4.
KevinMeng,ArnabSenSharma,AlexJAndonian,YonatanBelinkov,andDavidBau. Mass-editingmemory
in a transformer. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=MkbcAHIYgyS.
Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Circuit component reuse across tasks in transformer
language models. In The Twelfth International Conference on Learning Representations, 2024a. URL
https://openreview.net/forum?id=fpoAYV6Wsk.
Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Talking heads: Understanding inter-layer communication
in transformer language models. arXiv preprint arXiv:2406.09519, 2024b. URL https://arxiv.org/
abs/2406.09519.
Eric J Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural scaling. In
Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.
net/forum?id=3tbTw2ga8K.
Tomas Mikolov, Martin Karafiát, Lukáš Burget, Jan Černocký, and Sanjeev Khudanpur. Recurrent neural
network based language model. In Interspeech, pp. 1045–1048, 2010. doi: 10.21437/Interspeech.2010-343.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in
vector space. arXiv preprint arXiv:1301.3781, arXiv:1301.3781, 2013a. URL https://arxiv.org/abs/
1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Cur-
ran Associates, Inc., 2013b. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/
9aa42b31882ec039965f3c4923ce901b-Paper.pdf.
JosephMiller,BilalChughtai,andWilliamSaunders. Transformercircuitfaithfulnessmetricsarenotrobust.
2024. URL https://arxiv.org/abs/2407.08734.
RahaMoraffah,MansoorehKarami,RuochengGuo,AdrienneRaglin,andHuanLiu. Causalinterpretability
for machine learning-problems, methods and evaluation. ACM SIGKDD Explorations Newsletter, 22(1):
18–33, 2020.
Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, and Matthew Botvinick. On the importance of
single directions for generalization. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=r1iuQjxCZ.
26Michael C. Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a network
viarelevanceassessment. InD.Touretzky(ed.), Advances in Neural Information Processing Systems, vol-
ume 1. Morgan-Kaufmann, 1988. URL https://proceedings.neurips.cc/paper_files/paper/1988/
file/07e1cd7dca89a1678042477183b7ac3f-Paper.pdf.
AaronMueller. Missedcausesandambiguouseffects: Counterfactualsposechallengesforinterpretingneural
networks. In ICML 2024 Workshop on Mechanistic Interpretability, 2024. URL https://openreview.
net/forum?id=pJs3ZiKBM5.
Aaron Mueller, Yu Xia, and Tal Linzen. Causal analysis of syntactic agreement neurons in multilingual
language models. In Antske Fokkens and Vivek Srikumar (eds.), Proceedings of the 26th Conference
on Computational Natural Language Learning (CoNLL), pp. 95–109, Abu Dhabi, United Arab Emirates
(Hybrid), December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.conll-1.8.
URL https://aclanthology.org/2022.conll-1.8.
Neel Nanda and Joseph Bloom. TransformerLens. https://github.com/TransformerLensOrg/
TransformerLens, 2022.
Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of
self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya
McCarthy, and Hosein Mohebbi (eds.), Proceedings of the 6th BlackboxNLP Workshop: Analyzing and
Interpreting Neural Networks for NLP, pp. 16–30, Singapore, December 2023. Association for Compu-
tational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.2. URL https://aclanthology.org/2023.
blackboxnlp-1.2.
Clement Neo, Shay B Cohen, and Fazl Barez. Interpreting context look-ups in transformers: Investigating
attention-MLP interactions. arXiv preprint arXiv:2402.15055, 2024. URL https://arxiv.org/abs/
2402.15055.
CatherineOlsson,NelsonElhage,NeelNanda,NicholasJoseph,NovaDasSarma,TomHenighan,BenMann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,
Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning
and induction heads. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/
2022/in-context-learning-and-induction-heads/index.html.
Nina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner.
SteeringLlama2viacontrastiveactivationaddition. arXiv preprint arXiv:2312.06681,2024. URLhttps:
//arxiv.org/abs/2312.06681.
KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. Bleu: amethodforautomaticevaluationof
machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.), Proceedings of the 40th
AnnualMeetingoftheAssociationforComputationalLinguistics,pp.311–318,Philadelphia,Pennsylvania,
USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https:
//aclanthology.org/P02-1040.
Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry
of large language models. In Causal Representation Learning Workshop at NeurIPS 2023, 2023. URL
https://openreview.net/forum?id=T0PoOJg8cK.
Gonçalo Paulo, Thomas Marshall, and Nora Belrose. Does transformer interpretability transfer to RNNs?
arXiv preprint arXiv:2404.05971, 2024. URL https://arxiv.org/abs/2404.05971.
Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.
Judea Pearl. Direct and indirect effects. In Proceedings of the Seventeenth Conference on Uncertainty in
Artificial Intelligence, pp. 411–420. Morgan Kaufmann, 2001.
27Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word repre-
sentation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543, Doha,
Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL
https://aclanthology.org/D14-1162.
Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-tuning enhances
existingmechanisms: Acasestudyonentitytracking.InTheTwelfthInternationalConferenceonLearning
Representations, 2024. URL https://openreview.net/forum?id=8sKcAWOf2D.
Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang,
YoonKim,YejinChoi,NouhaDziri,andXiangRen.Phenomenalyetpuzzling: Testinginductivereasoning
capabilities of language models with hypothesis refinement. In The Twelfth International Conference on
Learning Representations, 2024. URL https://openreview.net/forum?id=bNt7oajl2a.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are
unsupervised multitask learners. 2019.
Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, János Kramár,
Rohin Shah, and Neel Nanda. Improving dictionary learning with gated sparse autoencoders, 2024. URL
https://arxiv.org/abs/2404.16014.
Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out: Guarding
protected attributes by iterative nullspace projection. In Dan Jurafsky, Joyce Chai, Natalie Schluter,
and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, pp. 7237–7256, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/
v1/2020.acl-main.647. URL https://aclanthology.org/2020.acl-main.647.
Shauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav Goldberg. Counterfactual interventions reveal the
causal effect of relative clause representations on agreement prediction. In Arianna Bisazza and Omri
Abend (eds.), Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 194–
209,Online,November2021.AssociationforComputationalLinguistics.doi: 10.18653/v1/2021.conll-1.15.
URL https://aclanthology.org/2021.conll-1.15.
ShauliRavfogel,FranciscoVargas,YoavGoldberg,andRyanCotterell. Adversarialconcepterasureinkernel
space. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing,pp.6034–6055,AbuDhabi,UnitedArabEmirates,
December2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.emnlp-main.405. URL
https://aclanthology.org/2022.emnlp-main.405.
AbhilashaRavichander, YonatanBelinkov, andEduardHovy. Probingtheprobingparadigm: Doesprobing
accuracy entail task relevance? In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (eds.), Proceedings
of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main
Volume,pp.3363–3377,Online,April2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2021.eacl-main.295. URL https://aclanthology.org/2021.eacl-main.295.
Atul Rawal, Adrienne Raglin, Danda B. Rawat, Brian M. Sadler, and James McCoy. Causality for trust-
worthy artificial intelligence: Status, challenges and perspectives. ACM Comput. Surv., may 2024. ISSN
0360-0300. doi: 10.1145/3665494. URL https://doi.org/10.1145/3665494.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. COMET: A neural framework for MT
evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2685–2702, Online,
November2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-main.213. URL
https://aclanthology.org/2020.emnlp-main.213.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "Why should I trust you?": Explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on
28KnowledgeDiscoveryandDataMining,KDD’16,pp.1135–1144,NewYork,NY,USA,2016a.Association
forComputingMachinery. ISBN9781450342322. doi: 10.1145/2939672.2939778. URLhttps://doi.org/
10.1145/2939672.2939778.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Model-agnostic interpretability of machine learn-
ing, 2016b. URL https://arxiv.org/abs/1606.05386.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic expla-
nations. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth
Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational
Advances in Artificial Intelligence, AAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018. ISBN 978-1-57735-
800-8.
James M. Robins and Sander Greenland. Identifiability and exchangeability for direct and indirect effects.
Epidemiology, 3(2):143–155, 1992. ISSN 10443983. URL http://www.jstor.org/stable/3702894.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-
propagating errors. Nature, 323(6088):533–536, 1986. doi: 10.1038/323533a0. URL https://doi.org/
10.1038/323533a0.
Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent AI: A survey
on interpreting the inner structures of deep neural networks, 2023. URL https://arxiv.org/abs/2207.
13243.
HassanSajjad,NadirDurrani,andFahimDalvi. Neuron-levelinterpretationofdeepNLPmodels: Asurvey.
Transactions of the Association for Computational Linguistics, 10:1285–1303, 2022. doi: 10.1162/tacl_
a_00519. URL https://aclanthology.org/2022.tacl-1.74.
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On the effect of dropping layers of pre-
trained transformer models. Computer Speech & Language, 77:101429, 2023.
Naomi Saphra, Eve Fleisig, Kyunghyun Cho, and Adam Lopez. First tragedy, then parse: History re-
peats itself in the new era of large language models. In Kevin Duh, Helena Gomez, and Steven Bethard
(eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 2310–2326, Mexico
City, Mexico, June 2024. Association for Computational Linguistics. URL https://aclanthology.org/
2024.naacl-long.128.
Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob An-
dreas, and Antonio Torralba. A multimodal automated interpretability agent. In Forty-first International
Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=mDw42ZanmE.
Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with
sparse autoencoders, 2023. URL https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/
interim-research-report-taking-features-out-of-superposition. Accessed: 2023-05-10.
Arnab Sen Sharma, David Atkinson, and David Bau. Locating and editing factual associations in Mamba.
arXiv preprint arXiv:2404.03646, 2024. URL https://arxiv.org/abs/2404.03646.
Xing Shi, Inkit Padhi, and Kevin Knight. Does string-based neural MT learn source syntax? In Jian Su,
KevinDuh,andXavierCarreras(eds.),Proceedings of the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pp. 1526–1534, Austin, Texas, November 2016. Association for Computational
Linguistics. doi: 10.18653/v1/D16-1159. URL https://aclanthology.org/D16-1159.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising
image classification models and saliency maps. arXiv preprint arXiv:1312.6034, arXiv:1312.6034, 2014.
URL https://arxiv.org/abs/1312.6034.
29Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. Rethinking in-
terpretability in the era of large language models. arXiv preprint arXiv:2402.01761, 2024a. URL
https://arxiv.org/abs/2402.01761.
Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, and Ponnurangam
Kumaraguru. Representation surgery: Theory and practice of affine steering, 2024b. URL https:
//arxiv.org/abs/2402.09631.
DivyanshSrivastava,TuomasOikarinen,andTsui-WeiWeng. Corruptingneuronexplanationsofdeepvisual
features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1877–1886,
2023.
Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, and Alexander M Rush. LSTMVis: A tool for
visualanalysisofhiddenstatedynamicsinrecurrentneuralnetworks. IEEETransactionsonVisualization
and Computer Graphics, 24(1):667–676, 2017.
Varshini Subhash, Zixi Chen, Marton Havasi, Weiwei Pan, and Finale Doshi-Velez. What makes a good
explanation?: A harmonized view of properties of explanations. In Progress and Challenges in Building
Trustworthy Embodied AI, 2022. URL https://openreview.net/forum?id=YDyLZWwpBK2.
MukundSundararajan,AnkurTaly,andQiqiYan.Axiomaticattributionfordeepnetworks.InProceedingsof
the34thInternationalConferenceonMachineLearning-Volume70,ICML’17,pp.3319–3328.JMLR.org,
2017.
AaquibSyed,CanRager,andArthurConmy. Attributionpatchingoutperformsautomatedcircuitdiscovery.
In NeurIPS Workshop on Attributing Model Behavior at Scale, 2023. URL https://openreview.net/
forum?id=tiLbFR4bJW.
Alex Tamkin, Mohammad Taufeeque, and Noah D. Goodman. Codebook features: Sparse and discrete
interpretability for neural networks. 2023. URL https://arxiv.org/abs/2310.17230.
Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam
Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Cal-
lum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua
Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Ex-
tracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https:
//transformer-circuits.pub/2024/scaling-monosemanticity/index.html.
Emmanouil Theodosis and Demba Ba. Learning silhouettes with group sparse autoencoders. In ICASSP
2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.
1–5, 2023. doi: 10.1109/ICASSP49357.2023.10095958.
CurtTigges,OskarJohnHollinsworth,AtticusGeiger,andNeelNanda. Linearrepresentationsofsentiment
in large language models. arXiv preprint arXiv:2310.15154, 2023.
Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function
vectors in large language models. In The Twelfth International Conference on Learning Representations,
2024. URL https://openreview.net/forum?id=AwyxtyMwaG.
Mycal Tucker, Peng Qian, and Roger Levy. What if this modified that? Syntactic interventions with
counterfactual embeddings. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Find-
ings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 862–875, Online, Au-
gust 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.76. URL
https://aclanthology.org/2021.findings-acl.76.
Alexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid.
Activation addition: Steering language models without optimization. arXiv preprint arXiv:2308.10248,
2023. URL https://arxiv.org/abs/2308.10248.
30F.-Y. Tzeng and K.-L. Ma. Opening the black box - Data driven visualization of neural networks. In VIS
05. IEEE Visualization, 2005., pp. 383–390, 2005. doi: 10.1109/VISUAL.2005.1532820.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,
and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume30.CurranAssociates,Inc.,2017.URLhttps://proceedings.neurips.cc/paper_files/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart
Shieber. Investigating gender bias in language models using causal mediation analysis. In H. Larochelle,
M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Sys-
tems, volume 33, pp. 12388–12401. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper_files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf.
KevinRoWang,AlexandreVariengien,ArthurConmy,BuckShlegeris,andJacobSteinhardt. Interpretabil-
ity in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International
Conference on Learning Representations,2023. URLhttps://openreview.net/forum?id=NpsVSN6o4ul.
Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically
explains long-context factuality. arXiv preprint arXiv:2404.15574, 2024a. URL https://arxiv.org/
abs/2404.15574.
Xing Wu, Shaoqi Peng, Jingwen Li, Jian Zhang, Qun Sun, Weimin Li, Quan Qian, Yue Liu, and Yike Guo.
Causal inference in the medical domain: A survey. Applied Intelligence, pp. 1–24, 2024b.
Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. Interpretability
at scale: Identifying causal mechanisms in alpaca. In Thirty-seventh Conference on Neural Information
Processing Systems, 2023. URL https://openreview.net/forum?id=nRfClnMhVX.
Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah Goodman, Christopher
Manning, and Christopher Potts. pyvene: A library for understanding and improving PyTorch models
via interventions. In Kai-Wei Chang, Annie Lee, and Nazneen Rajani (eds.), Proceedings of the 2024
Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies (Volume 3: System Demonstrations), pp. 158–165, Mexico City, Mexico, June
2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-demo.16. URL https:
//aclanthology.org/2024.naacl-demo.16.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In David Fleet,
Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision – ECCV 2014, pp. 818–833,
Cham, 2014. Springer International Publishing. ISBN 978-3-319-10590-1.
Fred Zhang and Neel Nanda. Towards best practices of activation patching in language models: Metrics
and methods. In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=Hf17y6u9BC.
Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. MQuAKE:
Assessing knowledge editing in language models via multi-hop questions. In Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing, pp. 15686–15702, 2023.
A Types of interventions
Recall that to compute the indirect effect, we must replace the activation z of a component z with some
1
counterfactual activation z . In effect, the goal of an intervention is to reveal a neuron for whom swapping
2
z with z will produce a large effect on the output metric. There are many ways to derive z ; some of these
1 2 2
depend on x, some depend on the expected output, and some depend on neither. We describe each of these
classes of interventions, and list their pros and cons.
31Input-dependentinterventions. Ifonecaresaboutneuronsthataresensitivetoaspecificcontrast,then
one can use input-dependent interventions (i.e., interventions where z depends on x). For example, assume
2
our target task is subject-verb agreement. Given an input x = “The key”, we want to locate neurons that
increase the probability difference m=p(is)−p(are). In this case, we can obtain z by running x through
1
modelM inaforwardpass(denotedM(x))andstoringtheactivationofcomponent(s)a (whichcouldbea
i
neuron, forexample). Wecanthenobtain z byrunningM(x′), where x′ isaminimallydifferentinputthat
2
swaps the answer; here, x′ would be “The keys”. This type of intervention preserves all example-specific
information, and varying only the grammatical number of the subject. This makes this intervention precise:
it will only reveal neurons for which swapping grammatical number and nothing else will significantly affect
m.
Thismethodiseasilycontrollableandrevealscomponentstargetedtoaspecificphenomenon;inotherwords,
this is a high-precision intervention. However, humans must carefully curate highly controlled input pairs
in which only one phenomenon is varied across x and x′. Additionally, input-dependent interventions work
best for (and arguably, are only principled given) binary contrasts: we contrast two minimally different
inputs, which isolates neurons sensitive only to the difference between items in the pair. When working
with categorical or ordinal variables, it is not immediately clear how to construct x′ to recover all relevant
components. Additionally, it does not recover all task-relevant components, but rather those related to the
contrast between x and x′; in other words, this is a low-recall method. For instance, Vig et al. (2020)
showcases the requirement of enumerating through all possible gender pronouns and personas related to a
specificgendertomeasurethespecificphenomenon, evennotingthatfullgeneralizabilitytoallgrammatical
genderpronounsisdifficult. Furthermore,suchinterventionscanbeprivytounreliableexplanationsasshown
in Srivastava et al. (2023) wherein the input data can be corrupted to manipulate the concept assigned to
a neuron. Hence, input-dependent interventions may require additional safeguards to ensure safety and
fairness in critical real-life applications.
Class-dependent interventions. Incontrasttoinput-dependentinterventions,whichlocatecomponents
sensitivetoaspecificphenomenon,class-dependentinterventionsdefineasingleinterventiontoproducesome
behavior associated with a class of outputs. For example, Li et al. (2024) learn an ablation mask over the
computational graph of a language model to prevent the model from producing toxic content; here, the
classes are toxic and not toxic, and the intervention is the same regardless of the input.
Class-dependentinterventionsprovideasingleflexibleinterventionthatworksforanygiveninput. However,
they require a dataset of input-label pairs that can be used to learn the interventions. This is often time-
consuming and can lead to errors as many labels we care about are hard to quantify (e.g., bias or toxicity).
Class- and input-independent interventions. This type of intervention does not rely on either the
input or a class label, and its goal is generally to fully remove the information encoded by a mediator—
regardless of whether the information is task-relevant. A common ablation type is zero ablations, where
the activation of a component is set to 0. This is not entirely principled, since 0 has no inherent meaning
in an activation—for example, a neuron’s default activation may be non-zero, whereas 0 itself is out of
distribution relative to what the model expects. A more principled ablation type is a mean ablation,
where the neuron’s activation is set to its mean value over some distribution—either task-specific data or
general text data. A resampling ablation is a special case of mean ablations where the sample size is 1.
This is a more general intervention type that can be run without access to contrastive input/output pairs,
andwithoutlabeledinputs. Itallowsustotellwhetherany oftheinformationinamediatorisnecessaryfor
a model to perform the task, but it may also affect other information in unanticipated ways; in other words,
it has high recall and low precision relative to other intervention types. It also may cause performance
on a task to drop in a way that reveals spurious mediators, rather than mediators that are conceptually
relevant—for example, ablating a neuron that detects the word “dog” may reduce the probability of the
correct verb form “is” over the incorrect verb form “are”, but this is a highly specific neuron that does not
on its own reveal general information about how models process subject-verb agreement.
32