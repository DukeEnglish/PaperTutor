Autoencoders in Function Space
Justin Bunker jb2200@cantab.ac.uk
Department of Engineering
University of Cambridge
Cambridge, CB2 1TN, United Kingdom
Mark Girolami mgirolami@turing.ac.uk
Department of Engineering, University of Cambridge
and Alan Turing Institute
Cambridge, CB2 1TN, United Kingdom
Hefin Lambley hefin.lambley@warwick.ac.uk
Mathematics Institute
University of Warwick
Coventry, CV4 7AL, United Kingdom
Andrew M. Stuart astuart@caltech.edu
Computing + Mathematical Sciences
California Institute of Technology
Pasadena, CA 91125, United States of America
T. J. Sullivan t.j.sullivan@warwick.ac.uk
Mathematics Institute & School of Engineering
University of Warwick
Coventry, CV4 7AL, United Kingdom
Abstract
Autoencoders have found widespread application, in both their original deterministic form
and in their variational formulation (VAEs). In scientific applications it is often of interest
to consider data that are comprised of functions; the same perspective is useful in image
processing. In practice, discretisation (of differential equations arising in the sciences) or
pixellation (of images) renders problems finite dimensional, but conceiving first of algo-
rithms that operate on functions, and only then discretising or pixellating, leads to better
algorithms that smoothly operate between different levels of discretisation or pixellation.
In this paper function-space versions of the autoencoder (FAE) and variational autoen-
coder (FVAE) are introduced, analysed, and deployed. Well-definedness of the objective
functiongoverningVAEsisasubtleissue,eveninfinitedimension,andmoresoonfunction
space. The FVAE objective is well defined whenever the data distribution is compatible
with the chosen generative model; this happens, for example, when the data arise from a
stochasticdifferentialequation. TheFAEobjectiveisvalidmuchmorebroadly,andcanbe
straightforwardly applied to data governed by differential equations. Pairing these objec-
tiveswithneuraloperatorarchitectures,whichcanthusbeevaluatedonanymesh,enables
new applications of autoencoders to inpainting, superresolution, and generative modelling
of scientific data.
Keywords: Variational inference on function space; operator learning; variational au-
toencoders; regularised autoencoders; scientific machine learning.
©2024JustinBunker,MarkGirolami,HefinLambley,AndrewM.StuartandT.J.Sullivan.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/.
4202
guA
2
]LM.tats[
1v26310.8042:viXraBunker, Girolami, Lambley, Stuart, and Sullivan
1 Introduction
Functional data, or data that can be thought of as a high-resolution approximation of func-
tional data, is ubiquitous in data science (Ramsay and Silverman, 2002). Recent years
have seen increasing interest in machine learning in this setting, with the promise of ar-
chitectures that can be discretised, trained, and evaluated across resolutions. A variety
of methods now exist for the supervised learning of operators mapping between function
spaces, starting with the early paper of Chen and Chen (1993), followed by DeepONet (Lu
et al., 2021) and PCA-Net (Bhattacharya et al., 2021) and leading to Fourier neural op-
erators (FNOs; Li et al., 2021) and variants (Kovachki et al., 2023). This function-space
perspective has proven successful in applications such as surrogate modelling for expensive
numerical simulators of dynamical systems (Azizzadenesheli et al., 2024).
Practical algorithms for functional data must necessarily operate on finite-dimensional
representations of the underlying infinite-dimensional objects, ideally by identifying salient
features independent of the data resolution. Some models such as PCA-Net and DeepONet
make this dimension reduction explicit by representing outputs as a linear combination
of basis functions learned from data, while others do this implicitly, as in, for example,
the deep layered structure of FNOs involving repeated application of the discrete Fourier
transform followed by pointwise activation.
Linear dimension-reduction methods such as principal component analysis are readily
adapted to function space, and indeed this is the foundation of PCA-Net; but there are
many types of data, such as solutions to advection-dominated partial differential equations
(PDEs), for which linear approximations are provably inefficient—a phenomenon known
as the Kolmogorov barrier (Peherstorfer, 2022). This suggests the need for nonlinear
dimension-reduction techniques on function space. Motivated by this we propose an ex-
tension of variational autoencoders (VAEs; Kingma and Welling, 2014) to functional data
using operator learning; we refer to the resulting model as the functional variational au-
toencoder (FVAE). As a probabilistic latent-variable model, the FVAE allows for both
dimension reduction and principled generative modelling.
Toensurethatthevariationalobjectiveisdefinedinfunctionspace,weaveragetheusual
VAElossoverthedatadistribution,revealingtheneedforafinite-information condition
forbidding the FVAE generative model from being misspecified relative to the data. This
condition has received little attention hitherto but is in fact present in finite-dimensional
settings; its failure leads to foundational issues in the probabilistic interpretation of FVAE
andpracticaltrainingissuesintheinfinite-resolutionandinfinite-datalimits. Priorworkon
adaptingVAEstofunctionspace,e.g.,thevariationalautoencodingneuraloperator(VANO;
Seidman et al., 2023), have already shown promising empirical results, but the barriers we
discover render the meaning of the variational objective unclear without suitable conditions
on the data. Our applications of FVAE rest on establishing decoder noise with the correct
level of regularity so that the objective is defined, and we show that this is possible for
numerous problems in the physical sciences (Stuart, 2010; Hairer et al., 2011).
However, there are many problems arising in the sciences, and in generative models
for partial differential equations in particular, for which application of VAEs on function
space fails because the model is typically misspecified relative to the data distribution. To
overcome this we propose a deterministic regularised autoencoder that can be applied in
2Autoencoders in Function Space
very general settings, which we call the functional autoencoder (FAE). We complement
the FVAE and FAE objectives with encoder and decoder architectures based on neural
operators that can be discretised on arbitrary, and possibly distinct, meshes. The ability
to discretise both encoder and decoder on any mesh extends prior work and is highly
empowering, enabling a variety of novel applications of the autoencoder methodology to
scientific data, including inpainting and superresolution.
Contributions. Wemakethefollowingcontributionstothedevelopmentandapplication
of autoencoders in the setting of functional data:
(C1) we propose FVAE, an extension of VAEs to function space, finding that the training
objectivemakessenseonlyunderafinite-informationconditionensuringcompatibility
between the data distribution and the generative model associated with FVAE;
(C2) we complement our training objective with architectures that permit discretisation on
any mesh—even irregular, non-grid meshes—and propose a self-supervised training
scheme exploiting the ability to evaluate the encoder and decoder on distinct meshes;
(C3) we show numerically and analytically that a failure of the finite-information condition
in(C1)canleadtoadivergenceofthetraininglossintheinfinite-resolutionorinfinite-
data limits, or a breakdown of the probabilistic foundations of FVAE;
(C4) weproposeFAE,anextensionofregularisedautoencoderstofunctionspace,andshow
that this objective is defined in many situations where FVAE fails;
(C5) we validate FAE and FVAE numerically on examples from across science and engi-
neering, including data arising from stochastic differential equations (SDEs) and from
PDEs, and in so doing we demonstrate the value of having both an encoder and de-
coder that can be discretised on any mesh by using our mesh-invariant autoencoders
for inpainting, superresolution, and generative modelling.
Outline. In Section 2, we extend VAEs to function space under the condition described
in (C1) forbidding misspecification of the generative model. This condition is significant in
infinite dimensions, but is satisfied, for example, when the data are realisations of an SDE.
Wethenstatethemesh-invariantarchitecturesdescribedin(C2)andvalidateourapproach,
FVAE, on numerical examples. In Section 3, we give examples illustrating the possible
failure of the finite-information condition, as described in (C3), which occurs exceptionally
in finite dimensions and frequently in infinite dimensions. In Section 4, we propose a non-
probabilistic regularised autoencoder, FAE, as discussed in (C4), which applies even when
the finite-information condition fails. We validate this approach on two examples taken
from scientific machine learning—fluid flows governed by the Navier–Stokes equations, and
pressure fields arising from subsurface flows—and make use of FAE’s mesh-invariance for
the applications in (C5). In Section 5, we discuss related work, and in Section 6 we discuss
limitations and topics for future research.
2 Variational Autoencoders on Function Space
VAEs are probabilistic models for dimension reduction and generative modelling using neu-
ralnetworks,basedonanobjectivethatjointlytrainsagenerativemodelmappingGaussian
noise z to data u through a decoder neural network g, and performs variational inference to
3Bunker, Girolami, Lambley, Stuart, and Sullivan
approximate the distribution z u using an encoder neural network f. Together, the maps
|
f and g result in an autoencoder: an approximate factorisation of the identity mapping,
factored through a low-dimensional latent space.
Compared to autoencoders in general, VAEs have the added benefit of regularising
the latent space to be approximately Gaussian, which makes VAEs particularly suited to
generative modelling and discovery of semantic structure in the data. The goal of this
section is to develop a well-defined extension of the VAEs objective to function space, and
to pair this objective with operator architectures that can be discretised at any resolution,
resulting in a model we call the functional variational autoencoder (FVAE).
In Section 2.1, we review the VAE objective in finite dimensions proposed by Kingma
and Welling (2014), and in Section 2.2, we extend this objective to function space. Our key
insight is to average the usual VAE objective over the data; this reveals an essential finite-
information condition necessary for VAEs to be well-defined and that requires great care
in infinite dimensions. In Section 2.3, we specialise our proposed objective to two settings
where the finite-information condition is satisfied: path distributions of SDEs and Bayesian
posteriors arising in inverse problems. In Section2.4, we describehowto discretise the well-
defined continuum objective, and complement this with encoder and decoder architectures
based on neural operators, resulting in our proposed mesh-invariant FVAE. In Section 2.5,
we validate our approach on several numerical examples.
2.1 Objective in Finite Dimensions
Let = RdU and denote by P( ) the set of Borel probability measures on . Assume that
U U U
data are distributed according to the measure Υ P( ), from which we observe finitely
∈ U
many samples. Fixing a latent space = RdZ, typically with d
Z
d U, the starting
Z ≪
point in the formulation of VAEs is to specify a generative model for u by specifying the
distributionP (withdensityp )ofthelatentvariablez andthedistributionP (with
z z u|z
∈ Z
density p ) of the conditional u z. A common simplifying choice is
u|z
|

u z = g(z;θ )+η,
  | z
∼
P z :=g N(cid:0) 0,I dZ(cid:1) , (2.1)
  η P := N(cid:0) 0, β I (cid:1) ,
∼ η 2 dU
with the decoder g: Θ defined as a neural network parametrised by θ Θ
g g g
Z × → U ∈
and with β > 0 fixed as a hyperparameter. The model (2.1) specifies a joint distribution
on (z,u) with marginal densities p and
pθg;
moreover, conditioning on u leads to a
z u
∈ U
posterior z u with density
pθg
. Here and in Section 2.2, we explicitly note the dependence
| z|u
of the measures and densities on the neural-network parameters except where there is no
ambiguity. Since accessing z u is typically intractable, VAEs perform variational inference
|
with the variational family
(cid:16) (cid:17)
Qθ
f := N
m(cid:0)
u;θ
(cid:1) ,Σ(cid:0)
u;θ
(cid:1)
, u (2.2)
z|u f f ∈ U
θ
having associated density q f , with mean vector m(u;θ ) and symmetric, positive-
z|u f ∈ Z
definite covariance matrix Σ(u;θ ) ( ) given by an encoder neural network f :=
f +
∈ S Z
4Autoencoders in Function Space
(m,Σ): Θ ( ) parametrised by θ Θ . To derive an objective that jointly
f + f f
U× → Z×S Z ∈
trains the generative model and performs variational inference, the log-marginal likelihood
for data u under the model is written as
∈ U
logpθ ug(u) = D KL(cid:0) q zθ |f u(cid:13) (cid:13)pθ zg |u(cid:1) + LVAE(u;θ f,θ g,β), (2.3)
where we assume each term is finite and define (u;θ ,θ ,β) to be the evidence lower
VAE f g
L
bound (ELBO) of Kingma and Welling (2014, eq. (3)), which takes, up to a finite constant
C(β), the form of a regularised misfit when u z is Gaussian:
|
LVAE(u;θ f,θ g,β) = z∼E
q
(cid:104) logpθ ug |z(u)(cid:105) −D KL(cid:0) q zθ |f u(cid:13) (cid:13)p z(cid:1) (2.4a)
z|u
= z∼E
q
(cid:2) −β−1 ∥g(z;θ g) −u ∥2 2(cid:3) −D KL(cid:0) q zθ |f u(cid:13) (cid:13)p z(cid:1) +C(β). (2.4b)
z|u
Under mild conditions on the data, maximising the ELBO (2.4b) over θ and θ (viewing β
f g
asafixedhyperparameter)simultaneouslyperformsvariationalinferenceandminimisesthe
Kullback–Leibler (KL) divergence from the generative model p to the data distribution.
u
2.2 Well-Defined VAE Objective in Infinite Dimensions
Suppose now that is a (possibly infinite-dimensional) separable Banach space and, as
U
before, assume data are distributed according to Υ P( ) and the latent space = RdZ.
∈ U Z
Extending the formulation of VAEs to this setting necessitates several changes: the decoder
noise η must now take values in the Banach space , and the expression (2.3) must be
U
adapted to make sense in infinite dimensions, where measures do not have (Lebesgue)
probability densities (Sudakov, 1959).
We argue that the appropriate generalisation of (2.3) and the ELBO (2.4) is to take
expectations with respect to u Υ and consider the underlying population loss. Doing
∼
this reveals the conditions necessary to ensure that the underlying VAE objective is well-
definedandfinite—conditionsnotreadilyapparentundertheper-sampleviewpointof (2.4).
Proceeding similarly to the previous section, we take the generative model

u z = g(z;θ )+η,
 g
 |
z P := N(0,I ), (2.5)
∼
z dZ
  η P ,
η
∼
withthedecodernoisemeasureP nowacentrednondegenerateGaussianon . Bythis, we
η
U
meanthatwheneverℓisacontinuouslinearfunctionalon ,thescalarrandomvariableℓ(η)
U
has mean-zero, positive-variance Gaussian distribution (Bogachev, 1998). When dim =
U
, there is no standard Gaussian distribution on with identity covariance, so the choice
∞ U
of P becomes significant. We also assume the decoder g: Θ may have range
η g
Z× → V ⊆ U
strictly smaller than , for reasons that we shall soon discuss.
The model (2.5) iU nduces a joint measure Pθg P( ) that factorises in two ways:
z,u
∈ Z ×U
denoting by
Pθg
the distribution of u z and by
Pθg
the distribution of z u, we have
u|z | z|u |
Pθg (dz,du) = P (dz)Pθg (du), (2.6)
z,u z u|z
Pθg
(dz,du) =
Pθg (dz)Pθg(du).
(2.7)
z,u z|u u
5Bunker, Girolami, Lambley, Stuart, and Sullivan
We again take the Gaussian variational family (2.2) with mean and covariance matrix
parametrised by an encoder f := (m,Σ): Θ ( ), and define the approximate
f +
U× → Z×S Z
joint measure
Qθ
f (dz,du) :=
Qθ
f (dz)Υ(du). (2.8)
z,u z|u
The observation motivating our training objective is that, while the expression (2.3) no
longer makes sense as it is formulated in terms of Lebesgue probability densities, the true
goal of a VAE is to maximise the ELBO over a training dataset of samples u Υ. In this
∼
setting the natural population loss with which to work is the KL divergence
D KL(cid:0)Qθ zf ,u(cid:13) (cid:13)Pθ zg ,u(cid:1) := (z,u)E
∼Q
z,u(cid:20) log d dQ
P
zz ,, uu (z,u;θ f,θ g)(cid:21)
(2.9)
(cid:20) dQ (cid:21)
= E E log z,u (z,u;θ ,θ ) .
u∼Υz∼Q
z|u
dP
z,u
f g
AnecessaryconditionforthistobedefinedisthatQθ
f isabsolutelycontinuouswithrespect
z,u
to Pθg , denoted Qθ f Pθg , so that the density dQ /dP exists. The factorisations
z,u z,u z,u z,u z,u
≪
(2.6), (2.7), and (2.8) reveal that this is the case under a condition on the -marginals.
U
Proofs for the results that follow are provided in Appendix A.1.
Lemma 2.1 Fix θ Θ and θ Θ , and suppose that Υ
Pθg
for all z . Then
f ∈ f g ∈ g ≪ u|z ∈ Z
Υ Pθg and Qθ f Pθg , with
u z,u z,u
≪ ≪
dQ z,u dΥ dQ z|u dΥ dQ z|u
(z,u;θ ,θ ) = (u;θ ) (z;θ ) = (u;θ ) (z;θ ,θ ).
dP f g dP g dP f dP g dP f g
z,u u|z z u z|u
We shall see that the hypothesis of Lemma 2.1 is satisfied under our formulation given
a condition on the data measure Υ and the decoder range . Before stating this, let us
V
supposefurtherthatthedivergence(2.9)isfinite. Inthiscase,weobtainanaturalextension
oftheexpression(2.3),justifyingtheinterpretationof (2.9)asthepopulationlossforVAEs.
Theorem 2.2 Fix θ Θ and θ Θ , and suppose Υ
Pθg
for all z and
f ∈ f g ∈ g ≪ u|z ∈ Z
D (Qθ f Pθg ) < . Then
KL z,u z,u
∥ ∞
−D KL(cid:0) Υ(cid:13) (cid:13)Pθ ug(cid:1) = u∼E Υ(cid:104) D KL(cid:0)Qθ zf |u(cid:13) (cid:13)Pθ zg |u(cid:1)(cid:105) −D KL(cid:0)Qθ zf ,u(cid:13) (cid:13)Pθ zg ,u(cid:1) . (2.10)
The equality (2.10) is indeed the extension of (2.3), since the expectation of the log-
likelihood
logpθg(u)
over u Υ is, up to a constant, equal to D (Υ
Pθg).
This equality
u KL u
∼ − ∥
forms the foundation of a tractable objective under the additional assumptions that both
Pθg P forallz andafinite-information conditionthatD (Υ P ) < holds;
u|z ≪ η ∈ Z KL ∥ η ∞
henceforth, we write
P( P η) := (cid:8) µ P( )(cid:12) (cid:12)D KL(µ P η) < (cid:9) .
U∥ ∈ U ∥ ∞
6Autoencoders in Function Space
Theorem 2.3 (ELBO under finite-information condition) Fix θ Θ , θ Θ ,
f f g g
∈ ∈
and Υ P( P ). Suppose Υ Pθg P for all z and D (Qθ f Pθg ) < .
∈ U∥ η ≪ u|z ≪ η ∈ Z KL z,u ∥ z,u ∞
Then
D KL(cid:0)Qθ zf ,u(cid:13) (cid:13)Pθ zg ,u(cid:1) = E (cid:2) (u;θ f,θ g)(cid:3) +D KL(Υ P η), (2.11)
u∼Υ −L ∥
(cid:20) dP (cid:21)
L(u;θ f,θ g) := z∼E
Qθf
log dPu η|z (u;θ g) −D KL(cid:0)Qθ zf |u(cid:13) (cid:13)P z(cid:1) . (2.12)
z|u
This result naturally suggests a practical continuum training objective which corre-
sponds, up to a finite constant, to the population loss D (Qθ f Pθg ):
KL z,u z,u
∥
(θ ,θ ) := E (cid:2) (u;θ ,θ )(cid:3) . (2.13)
f g f g
J u∼Υ −L
Indeed, it is reasonable to seek to minimise since its infimum over the parameters θ and
f
J
θ is finite if Υ P( P ) and there exist θ Θ and θ Θ such that
g η f f g g
∈ U∥ ∈ ∈
(cid:0) (cid:1)
f(u;θ ) = 0,I , u , g(z;θ ) = 0, z .
f dZ
∈ U
g
∈ Z
In other words, under this condition on Θ and Θ , we know that the loss function is finite
f g
at at least one point. Let us now address the question of whether Υ P P for all
u|z η
≪ ≪
z . Since we must assume in Theorem 2.3 that Υ P( P ) to obtain a practical
η
∈ Z ∈ U∥
objective, it is certainly the case that Υ P . But thanks to the Cameron–Martin
η
≪
theorem (Bogachev, 1998, Corollary 2.4.3), the shift P = P ( g(z;θ )) is equivalent
u|z η g
· −
in the sense of measures to P , i.e., P P and P P , provided the decoder range
η u|z η η u|z
≪ ≪
is a subset of the Cameron–Martin space H(P ) associated with the Gaussian
η
V ⊂ U
measure P . Under this assumption on , it thus follows that Υ P for all z .
η u|z
V ≪ ∈ Z
Beforestatingthetheorem, werecallthefollowingrelevantfactsfromGaussianmeasure
theory. The space H(P ) is Hilbert, and for fixed h H(P ), the H(P )-inner product
η η η
∈
f⟨ uh n, c· t⟩ iH on(P aη l) (e Bxt oe gn ad cs hu evn ,iq 1u 9e 9l 8y ,( Tup het oo re eq mui 2v .a 1l 0e .n 1c 1e ),P wη- ha il cm ho wst ee dv ee nry ow teh uere)toam hea ,usur ∼able .linear
∈ U (cid:55)→ ⟨ ⟩H(P η)
Proposition 2.4 (Cameron–Martin theorem) Let µ P( ) be a Gaussian measure
0
∈ U
with Cameron–Martin space H(µ ), let h , and suppose that µh := µ ( h) is the shift
0 0
∈ U · −
of µ by h. Then µh is equivalent if h H(µ ), and
0 0
∈
(cid:16) 1 (cid:17)
µh(du) = exp h,u ∼ h 2 µ (du). (2.14)
⟨ ⟩H(µ0)− 2∥ ∥H(µ0) 0
Otherwise, µh and µ are mutually singular, i.e., µh µ and µ µh, denoted µh µ .
0 0 0 0
̸≪ ̸≪ ⊥
Consequently, we shall assume throughout that := H(P ), so that, as a result of the
η
V
Cameron–Martin formula (2.14), the ELBO (u;θ ,θ ) may be written as
f g
L
(cid:20) (cid:21)
(u;θ ,θ ) = E g(z;θ ),u ∼ 1 g(z;θ ) 2 D (Qθ f P ). (2.15)
L f g z∼Q ⟨ g ⟩H(P η)− 2∥ g ∥H(P η) − KL z|u∥ z
z|u
7Bunker, Girolami, Lambley, Stuart, and Sullivan
Remark 2.5 (a) Applying the reparametrisation trick (Kingma and Welling, 2014) and
writing out the KL divergence on the latent space analytically makes the interpretation
of the objective (θ ,θ ) as that of a regularised autoencoder more obvious:
f g
J
(cid:34) (cid:35)
(cid:16) (cid:16) (cid:17) (cid:17)
(θ ,θ ) = E g m(u;θ )+Σ(u;θ )ξ;θ ,u + (u;θ ) ,
f g f f g f
J u∼Υ M R
ξ∼P
z
where the misfit term (h,u) is given by
M
1
(h,u) := h 2 h,u ∼ ,
M 2∥ ∥H(P η)−⟨ ⟩H(P η)
and the regularisation term (u;θ ) is given by
f
R
(u;θ ) := 1 m(u;θ ) 2+ 1 tr(cid:0) Σ(u;θ ) logΣ(u;θ )(cid:1) d Z .
f f 2 f f
R 2∥ ∥ 2 − − 2
The term (h,u) can be seen as the mean-squared error (MSE) 1 h u 2 with
M 2∥ − ∥H(P η)
the almost-surely-infinite term 1 u 2 subtracted (see Stuart, 2010, Remark 3.8).
2∥ ∥H(P η)
(b) The requirement that D (Υ P ) < is not unique to the infinite-dimensional
KL η
∥ ∞
setting. In finite dimensions, however, the failure of these conditions is “exceptional”,
whereas in infinite dimensions it is typical: even insisting Υ P is a very stringent
η
≪
condition. We discuss this issue in more detail in Section 3.
2.3 Applications
Wenow give twopractical examplesof settingsfor which it ispossible toapply our method-
ology, arising in the study of random dynamical systems (Section 2.3.1) and in Bayesian
inverse problems (Section 2.3.2). In both cases, the structure of the data distribution en-
sures that there is a known Gaussian measure P such that the finite-information condition
η
Υ P( P ) holds, and the properties of the decoder noise measure P dictate the form
η η
∈ U∥
of the resulting training objective.
2.3.1 Stochastic Differential Equations
In this subsection, we focus on the random dynamical systems setting (e.g., E et al., 2004)
and assume that Υ is the distribution of sample paths of the Rm-valued diffusion
du = b(u )dt+√εdw , u = 0, t [0,T], (2.16)
t t t 0
∈
where (w ) is a Brownian motion on Rm. We take ε > 0 and assume that the drift
t t∈[0,T]
functionb: Rm Rm possessessufficientregularityfor(2.16)tobewelldefined. Thetheory
→
we outline applies more generally to systems with anisotropic diffusion and time-dependent
coefficients (S¨arkk¨a and Solin, 2019, Sec. 7.3), but we focus on the setting (2.16) to simplify
the exposition. The measure Υ is defined on the space = C ([0,T],Rm) of continuous
0
U
functions u(t) with u(0) = 0, and we shall take the decoder noise measure P to be the
η
distribution of the modified SDE
dv = c(v )dt+√εdw , v = 0, t [0,T], (2.17)
t t t 0
∈
8Autoencoders in Function Space
with drift c: Rm Rm and with ε > 0 being the same as in (2.16). Our formulation of
→
the FVAE objective assumed that P was Gaussian and Υ P( P ); these conditions
η η
∈ U∥
can be verified with the aid of the Girsanov theorem (see, e.g., Liptser and Shiryaev, 2001,
Chap. 7). Indeed, under specific choices of c, the decoder noise η is Gaussian on , and the
U
Girsanov formula yields the density dP /dP needed in the ELBO (2.15):
u|z η
(a) c(x,t) = 0, resulting in P being the Wiener measure, i.e., the law of a Brownian
η
motion on Rm, for which
log dP u|z (u) = 1 (cid:90) T h′(t) 2dt 1 (cid:90) T h′(t)du , (2.18)
− dP 2ε ∥ ∥2 − ε t
η 0 0
where the latter integral is an Itˆo stochastic integral (see S¨arkk¨a and Solin, 2019).
(b) c(x,t) = θx for some constant θ > 0, resulting in µ being the law of a θ-Ornstein–
0
−
Uhlenbeck (OU) process, for which H(µ ) again consists of H1-functions with u(0) =
0
0, and, thanks to the Girsanov formula (Øksendal, 2003)
log dP u|z (u) = 1 (cid:90) T1 h′(t) 2 θ h′(t),u(t) dt 1 (cid:90) T h′(t)du . (2.19)
− dP ε 2∥ ∥2 − ⟨ ⟩ − ε t
η 0 0
The finite-information condition Υ P( P ) is satisfied under relatively mild condi-
η
∈ U∥
tions on b and c (Lemma A.1), with the resulting KL divergence given by
(cid:20) 1 (cid:90) T (cid:21)
D (Υ P ) = E b(u ) c(u ) 2 ;
KL η t t 2
∥ u∼Υ 2ε 0 ∥ − ∥
this divergence is obviously finite if, for example, b and c are bounded. To ensure that
g(z;θ )(0) 0, we add a further zero-penalty term with scale λ > 0, and we add a
g
≈
regularisation scale β > 0 to better control the strength of the regularisation, leading to
the training objective
(cid:34) (cid:34) (cid:35) (cid:35)
dP
(θ ,θ ,λ,β) := E E log u|z (u)+λ g(z;θ )(0) 2 +βD (Q P ) .
JSDE f g u∼Υ z∼Q z|u − dP η ∥ g ∥2 KL z|u ∥ z
(2.20)
We enforce that g(z;θ ) lies in the Cameron–Martin space H(P ) implicitly: if g(z;θ ) /
g η g
∈
H(P ), then at least one of the terms in the loss (2.20) is infinite. This objective extends
η
naturally to the case that the underlying data SDE (2.16) has nonzero initial condition u
0
by translating the data by u and shifting the decoded function g(z;θ ) by u so that
0 g 0
−
realisations from the FVAE generative model have (approximate) initial condition u .
0
2.3.2 Bayesian Inverse Problems
Another case in which the structure of the data measure suggests an appropriate decoder
noisemeasurearisesinthestudyofBayesianinverseproblems(Stuart,2010). Weexemplify
this setting with a simple additive-noise Bayesian inverse problem. Let be a separable
U
Hilbert space, let Y = Rd Y, and let : Y be a (possibly nonlinear) observation
G U →
operator. Suppose that y Y is given by the model
∈
y = (u)+ξ, u µ , ξ N(0,Σ), (2.21)
0
G ∼ ∼
9Bunker, Girolami, Lambley, Stuart, and Sullivan
withnoisecovarianceΣ (Y),andwithprior measureµ = N(0,C)havingcovariance
+ 0
∈ S
operatorC: . Modelsofthistypearise, forexample, inLagrangiandataassimilation
U → U
problemsinoceanography(Cotteretal.,2010). Givenanobservationy Y fromthemodel
∈
(2.21), the Bayesian approach seeks to infer the unknown u by computing the posterior
∈ U
measureµy on ,representingthedistributionofu y. Inthesettingof (2.21),theposterior
U |
µy has a density with respect to the prior µ thanks to Bayes’ rule (e.g., Dashti and Stuart,
0
2017, Theorem 14), taking the form
dµy (u) = 1 exp(cid:0) Φ(u;y)(cid:1) , Φ(u;y) = 1 (u) y 2, = Σ−1/2 ,
Σ Σ 2
dµ Z(y) − 2∥G − ∥ ∥·∥ ∥ ·∥
0
where Z(y) (0,1] owing to the nonnegativity of Φ. A simple calculation then reveals
∈
D (µy µ ) = E
(cid:20)
log
dµy (u)(cid:21)
= E (cid:2) logZ(y) Φ(u)(cid:3) ⩽ logZ(y) < .
KL 0
∥ u∼µy dµ 0 u∼Υ − − − ∞
Consequently, µy P( µ ), and our FVAE objective can be applied to data from the
0
∈ U∥
posterior µy, taking the Gaussian prior µ as the decoder noise P . In this Hilbert setting,
0 η
H(P ) = C1/2 , and the norm and inner product in (2.15) admit the simple form
η
U
∥h ∥H(P η) = ∥C−1/2h ∥U, ⟨h,u ⟩∼ H(P η) = ⟨C−1/2h,C−1/2u ⟩U, h ∈ H(P η), u ∈ U.
Similar arguments apply quite generally for observation models other than (2.21), provided
the resulting log-density logdµy/dµ satisfies suitable boundedness or integrability condi-
0
tions. In any case, provided the finite-information condition µy P( µ ) holds, we may
0
∈ U∥
specialise the training objective to obtain
(cid:20) (cid:20) (cid:21) (cid:21)
BIP(θ f,θ g) = E E 1 C− 21 g(z;θ g) 2
U
C−1 2g(z;θ g),C− 21 u
U
+D KL(Q
z|u
P z) .
J u∼Υ z∼Q 2∥ ∥ −⟨ ⟩ ∥
z|u
Thismethodologycouldbeusedtogeneratefurtherapproximatesamplesfromtheposterior,
taking as data the output of a function-space MCMC method (Cotter et al., 2013).
2.4 Architecture and Algorithms
After establishing the well-defined FVAE objective on function space (Section 2.2) and
concrete settings for which the objective applies (Section 2.3), the remaining tasks are to
discretise the losses and to parametrise the encoder and decoder with learnable mappings
that can be evaluated on any mesh.
Ourframeworkhastheflexibilitytouseawidevarietyofneuraloperatorsintheencoder
and decoder, including DeepONet (Lu et al., 2021) and Fourier neural operators (Kovachki
et al., 2023). We focus on architectures allowing both the encoder and decoder to be
discretisedonarbitrarymeshes; asweshalldiscussinSection2.5, thereissignificantbenefit
in being able to use distinct meshes in the encoder and decoder.
We propose a common architecture to be used across all numerical experiments, fo-
cussing on the simplest method which remains competitive with more sophisticated neural-
network architectures in order to concentrate on our methodological contributions. Our
proposal bears similarities with existing operator architectures in the literature, such as
10Autoencoders in Function Space
the variable-input DeepONet (VIDON; Prasthofer et al., 2022) and the nonlinear manifold
decoder (NOMAD; Seidman et al., 2022). We also take inspiration from architectures for
point-cloud data, e.g., PointNet (Qi et al., 2017). As in Section 2.3, we assume that is
U
a Banach space of functions with domain Ω Rd and range Rm; we further assume that
⊆
functions u are evaluable pointwise.
∈ U
In Section 2.4.1, we discuss the discrete approximation of the objectives derived in
the previous subsection. In Section 2.4.2 and Section 2.4.3, we propose mesh-invariant
architecturesfortheencoderanddecoderinspiredbyexistingfunction-to-vectorandvector-
to-function maps based on neural operators. In Section 2.4.4, we exploit the possibility of
discretisingtheencoderanddecoderondifferentmeshestoproposeaself-supervisedtraining
scheme with the aim of improving robustness to changes of mesh.
2.4.1 Discretised Losses
For numerical implementation, the ELBO (2.12) must be discretised to allow computation
of the expectations over Υ and Q , and of the function-space norms and inner products.
z|u
Discretisation of expectations. The expectation over u Υ is empiricalised by taking
∼
the mean over the (finite) training dataset. Following Kingma and Welling (2014), the
expectation over z Q is approximated by Monte Carlo sampling, with the number of
z|u
∼
samples treated as a hyperparameter of the model.
Discretisation of function-space norms. In practice, data consists of discrete repre-
sentations of functions u: Ω Rm, possibly on heterogeneous and irregular meshes, and
→
we must discretise the continuum training objectives derived in Section 2.3 for numerical
implementation. In the case that is Hilbert, it is often natural to represent the data as
U
coefficients of an orthonormal basis, and thus the norms and inner products can be sim-
ply approximated by truncated sums. Frequently, however, is a space of functions and
U
the norms and inner products can be viewed as integral functionals of the data, e.g., the
L2-inner product
(cid:90)
f,g = f(x)g(x)dx,
L2
⟨ ⟩
Ω
and in this setting it is natural to approximate the integral through the normalised sum.
Computing decoder derivatives with automatic differentiation. In the objective
derived in Section 2.3.1, terms involving the derivative g(z;θ )′(t) with respect to the time
g
variable arise frequently. As we shall parametrise g(z;θ ) as a differentiable neural network,
g
it is natural to compute this derivative using automatic differentiation. This offers signifi-
cant advantages over finite-difference approximation of the derivative in terms of numerical
stability, and we adopt this strategy throughout the following numerical experiments.
Approximation of stochastic integrals. For the Itˆo stochastic integrals arising in Sec-
tion 2.3.1, we discretise on the partition 0 = t < t < < t = T with
0 1 n
···
(cid:90) T n
(cid:88)
g(z;θ )′(t)T du g(z;θ )′(t )T(u(t ) u(t )).
g t g j−1 j j−1
≈ −
0
j=1
This converges in probability to the stochastic integral (S¨arkk¨a and Solin, 2019, eq. (4.6)).
11Bunker, Girolami, Lambley, Stuart, and Sullivan
2.4.2 Encoder Architecture
Atthefunction-spacelevel, theencoderisaparametrisedmappingtakinganinputfunction
u: Ω Rm to the mean vector m(u;θ ) and covariance matrix Σ(u;θ ) ( ) of
f f +
→ ∈ Z ∈ S Z
the Gaussian variational distribution
Qθ
f on the latent space :
z|u Z
f := (m,Σ): Θ ( ).
f +
U × → Z ×S Z
As is typical for VAEs, we assume Σ(u;θ ) is a diagonal matrix for all u and write
f
∈ U
(cid:16) (cid:17)
f(u;θ ) = f˜(u;θ ),diag(cid:0) exp(cid:0) f˜(u;θ )(cid:1)(cid:1)
f 1 f 2 f
for some parametrised function-to-vector map f˜:= (f˜ 1,f˜ 2): Θ
f
RdZ RdZ = R2dZ.
We define f˜as an integration of the learnable vector-valuedU ke× rnel κ→ : Ω R× m Θ Rℓ,
f
× × →
followed by the learnable mapping ρ: Rℓ Θ
f
R2dZ:
× →
(cid:18)(cid:90) (cid:19)
f˜(u;θ ) = ρ κ(cid:0) x,u(x);θ (cid:1) dx;θ . (2.22)
f f f
Ω
We parametrise κ as a neural network with 2 hidden layers and output dimension ℓ = 64,
using GELU activation (Hendrycks and Gimpel, 2016), and we parametrise ρ as the linear
layer ρ(x;θ f) = Wx+b, with W R2dZ×ℓ and b R2dZ. We augment the coordinate x Ω
∈ ∈ ∈
in κ with 16 random Fourier features (Tancik et al., 2020) as described in Appendix B.1 to
aid learning of high-frequency features in the integral kernel; this proves essential given the
pivotal role of the kernel in our architecture.
Givenafunctionudiscretisedonthemesh x n ,theintegralin(2.22)isapproximated
{ i }i=1
asanormalisedsum,resemblinglearnableset-to-vectormapsproposedundervariousnames,
e.g., deep sets (Zaheer et al., 2017), PointNets (Qi et al., 2017), and statistic networks
(Edwards and Storkey, 2017), all of which take the form
(cid:8)(cid:0) (cid:1)(cid:12) (cid:9) (cid:16) (cid:16) (cid:8) (cid:0) (cid:1)(cid:12) (cid:9)(cid:17)(cid:17)
x i,u(x i) (cid:12)i = 1,2,...,n ρ pool κ x i,u(x i) (cid:12)i = 1,2,...,n ,
(cid:55)→
where pool is a pooling operation such as the mean; our approach differs from these works
by taking the continuum formulation as the starting point. Related ideas from machine
learning on sets, such as transformers (Vaswani et al., 2017), have already been applied to
operator learning—e.g., the VIDON (Prasthofer et al., 2022), the mesh-independent neural
operator (Lee, 2022), and continuum attention (Calvello et al., 2024). We believe there is
great potential in extending other point-cloud and set architectures to the continuum limit.
Atthefunction-spacelevel,ourencoderbearssimilaritieswiththelinear-functionallayer
used in Fourier neural mappings (Huang et al., 2024) and the neural functional of Rahman
et al. (2022). Our approach differs by allowing κ to depend on u, and by computing the
integralinphysicalspace. Thedistinctadvantageofourapproachistheabilitytoencodeon
any mesh, in contrast to approaches based on convolutions and the fast Fourier transform,
which are largely limited to grids. We investigated the possibility of preceding (2.22) with
a neural operator mapping between functions as in the work of Huang et al. (2024) and
Rahman et al. (2022), but found no advantage on the datasets we consider.
12Autoencoders in Function Space
2.4.3 Decoder Architecture
The decoder, at the continuum, is a parametrised vector-to-function map taking a latent
vector z to a function g(z;θ ): Ω Rm in the Cameron–Martin space = H(P ):
g η
∈ Z → V
g: Θ H(P ).
g η
Z × →
Several approaches exist in the literature to parametrise vector-to-function mappings,
and we briefly summarise. Huang et al. (2024) propose to lift the input vector to a func-
tion through multiplication with a learnable constant function, and then apply operator
architectures such as FNO. Seidman et al. (2023) propose both a DeepONet-inspired de-
coder based on a linear combination of learnable basis functions, and a nonlinear decoder
essentially the same as what we propose, which is also closely related to the architecture
adopted by the NOMAD model. There can be significant benefits to embedding known
structure in the decoder architecture but, for simplicity, we parametrise the decoder using
a coordinate neural network γ: Ω Θ Rm with 5 hidden layers of width 100 using
g
Z × × →
GELU activation throughout, so that
g(z;θ )(x) = γ(z,x;θ ). (2.23)
g g
As before, we augment x Ω with 16 random Fourier features (Appendix B.1) to allow γ to
∈
more easily capture high-frequency features. While we impose no smoothness requirement
at the architectural level, the training objective (2.13) implicitly ensures that g(z;θ ) is
g
smooth enough to lie in H(P ): if g(z;θ ) / H(P ), at least one term in the loss is infinite.
η g η
∈
Our proposed architecture allows discretisation of the decoded function g(z;θ ) on any
g
mesh, and the cost of evaluating the decoder (2.23) grows linearly with the number of mesh
points. Moreover, queries at each coordinate x Ω can be readily computed in parallel.
∈
The use of coordinate neural networks is closely related to the literature on implicit
neural representations (see, e.g., Sitzmann et al., 2020), in which one regresses on a fixed
image using a coordinate neural network, treating the resulting weights as a resolution-
independent representation of the underlying data.
2.4.4 Self-Supervised Training
The strategy of self-supervised learning—learning to predict the missing data from a
masked input—has proven valuable in both language models such as BERT (Devlin et al.,
2019) and vision models such as the masked autoencoder (MAE; He et al., 2022), yielding
benefits such as reduced training time and improved generalisation.
After dividing input images into patches, MAE encodes the data with 75% of patches
removed, decoding only the patches which were unseen at encoding time. Since we can
discretise the encoder and decoder on arbitrary distinct meshes, we propose a very natural
extension of this idea: mask both the input mesh on which the encoder is discretised and
the output mesh on which the decoder is discretised.
We chiefly focus on the MAE-inspired strategy of masking a fixed proportion of mesh
points in the input, randomly selected at each batch, and decoding only on the masked
points, but many other strategies are also possible, such as masking with polygons. We
explore the computational tradeoffs of various masking strategies in Section 4.3.
13Bunker, Girolami, Lambley, Stuart, and Sullivan
We argue that self-supervised training is of particular value for resolution-invariant
architectures: masking prevents the model from overfitting to the training mesh, leading to
demonstrable benefits on tasks involving changes of mesh (Section 4.3.1).
2.5 Numerical Experiments
We apply our proposed FVAE on two examples where Υ is an SDE path measure. These
examples serve as prototypes for more complex problems in science and engineering, moti-
vated by applications such as molecular dynamics (MD). For all experiments, we adopt the
architecture described in Section 2.4. A summary of the conclusions to be drawn from the
numerical experiments with these examples is as follows:
(a) FVAE accurately captures pathwise and ensemble properties of the underlying data,
with the learned latent variables having physically relevant meaning (Section 2.5.1);
(b) choosing a decoder noise process that accurately reflects the stochastic variability in
the data is essential to obtain a high-quality generative model (Section 2.5.1);
(c) FVAEisrobusttochangesofmeshintheencoderanddecoder, enablingtrainingwith
heterogeneous data and principled generative modelling at any resolution, backed up
by a well-defined probabilistic model (Section 2.5.2).
2.5.1 Brownian Dynamics
The Brownian dynamics model (see Schlick, 2010, Chapter 14), also known as the Langevin
model, is a stochastic approximation of deterministic Newtonian models for MD. In this
model, the configuration u (in some configuration space X Rm) of a molecule is assumed
t
⊆
to follow the gradient flow of a potential U: X R perturbed by additive thermal noise
→
with temperature ε > 0, described by the Langevin SDE
du = U(u )dt+√εdw , t [0,T], (2.24)
t t t
−∇ ∈
where (w ) is a Brownian motion on Rm.
t t∈[0,T]
As a prototype for the more sophisticated, high-dimensional potential functions arising
inMD,suchastheLennard–Jonespotential(seeSchlick,2010),wetakeX = Randconsider
a one-dimensional asymmetric double-well potential given by
U(x) 3x4 2x3 6x2+6x. (2.25)
∝ − −
This potential has a local minimum at x = 1 and a global minimum at x = 1 (Fig-
1 2
−
ure 2.1(a)). As discussed in Section 2.3.1, we internally translate by the initial condition
u = x so that the FVAE generative model approximately matches the true initial condi-
0 1
tion. WetakeΥbethedistributionoftheuntranslatedpaths, withtemperatureε = 1, final
time T = 5, and initial condition u = x . The training dataset consists of 8,192 paths with
0 1
timestep 5/512 in [0,T], where it is assumed that each sample path has 50% of timesteps
missing; data are generated using an Euler–Maruyama solver as described in Appendix B.2.
Sample paths drawn from Υ start at x and transition with very high probability to
1
the potential-minimising state x ; the time at which the transition begins is determined by
2
the thermal noise, but, once the transition has begun, the manner in which the transition
14Autoencoders in Function Space
(a)PotentialU(x) (b)Samplepaths(u)
t t [0,5]
∈
1
8
0
1
−
0
x1= −1 x2=+1 0 1 2 3 4 5
x t
Figure 2.1: (a) Realisations of the SDE (2.24) follow the gradient flow of the potential U.
(b) Sample paths (u ) Υ begin at x = 1 and transition with high probability to
t t∈[0,T] 1
∼ −
the lower-potential state x = +1 as a result of the additive thermal noise.
2
occurs is largely consistent across realisations. Such phenomena occur quite generally in
the study of random dynamical systems and large-deviation theory (e.g., E et al., 2004).
We train FVAE using the SDE loss (Section 2.3.1) with regularisation scale β = 1 and
zero-penalty scale λ = 10, using several choices of decoder-noise measure P . Motivated by
η
the observation that the trajectories are determined chiefly by the transition time, we use
latent dimension d = 1.
Z
Choice of noise process η. As discussed in Section 2.3.1, the choice of SDE (2.17) for
the noising process η greatly affects the performance of FVAE.
(a) Held-out samples u Υ (red) (b) Samples g(z;θ )+η from P , (c) Samples from P with
g u u
& reconstructions g(f(u∼ )) (black) z P , η P 1 standard deviation of noise η
(a)Reconstructions (b)Real∼izatioznsof∼g(z;ηθg)+η (c)Distributionofg(z;θg)+η
g(z;θg) 1SD
1
0
1
−
1
0
1
−
1
0
1
−
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
t t t
Figure 2.2: The SDE loss gives much freedom in the choice of decoder noise process η.
The top row is governed by Brownian motion in the decoder and the second and third
rows are governed, respectively, by two OU processes with different asymptotic variances.
Taking Brownian motion as the noise process leads to high-quality reconstructions but a
poor-quality generative model. Using a well-chosen OU process gives a much-improved
generative model without perceptible degradation in reconstruction quality.
15
noitomnainworB
)52=θ(UO
)00001=θ(UOBunker, Girolami, Lambley, Stuart, and Sullivan
UsingBrownianmotionasthedecodernoise(Figure2.2), i.e., c(x,t) = 0in(2.17), leads
to excellent reconstructions, but samples from the generative model P , i.e., realisations
u
u z = g(z;θ )+η, z P , η P (2.26)
g z η
| ∼ ∼
appear qualitatively different from the training data. By taking the law of an OU process
as the reference measure, i.e., with c(x,t) = θx for some θ > 0, we obtain qualitatively
−
similar reconstructions to those achieved under Brownian motion, but with samples that
match the data distribution much more closely (Figure 2.2). This is because the variance
of Brownian motion grows unboundedly with time, while the asymptotic variance under
the OU process is ε , better reflecting the qualitative behaviour of the underlying data for
2θ
suitably chosen θ. In what follows, we use an OU process as the decoder noise with θ = 25.
Unsupervised learning of physically relevant quantities. Our choice of latent di-
mensiond = 1wasmotivatedbytheheuristicthatthetimeofthetransitionfromx = 1
Z 1
−
to x = +1 essentially determines the full SDE trajectory. FVAE identifies this variable
2
purely from data: by taking equally spaced values of z [ 2.5,2.5] and examining the
∈ −
resulting functions g(z;θ ), we see that larger values of z correspond to the transition oc-
g
curring later in the time interval [0,T] (Figure 2.3(a)). While the values of z are equally
spaced in [ 2.5,2.5], the fact that more of the decoded paths transition in the time interval
−
[0,1] reflects the distribution of transition times in the underlying data.
(a) Decoded (ap )ga (t zh ;θs g)(g t)(z fo; rθ
zg
∈) [( −t 2) ., 5,z
2.5 ∈]
[ −2.5,2.5] (b) D (bis )t Tr ii mb eu tt oi fo fin rsto cf roT ss0in( gu a) bo[ v1 e6 0, (3 N8 =4 16s 3a 84m
)
ples]
1 0.5 FVAE
Directnumericalsimulation
0.4
0.3
0
0.2
0.1
−1
0 1 2 3 4 5
0.0
0 1 2 3 4 5
t
z
T0(u)
2 1 0 1 2
− −
Figure 2.3: (a) The latent variable z identified by FVAE is in correspondence with the
first-crossing time T of the decoded path g(z;θ ). (b) Kernel density estimates of the
0 g
distributions of T under the FVAE generative model and direct numerical simulations
0
closely agree.
We also compare the distributions of the first-crossing time
T 0(u) = inf(cid:8) t > 0(cid:12) (cid:12)u
t
⩾ 0(cid:9) ,
estimated using 16,384 paths from the generative model (2.26) and 16,384 direct numerical
simulations of the SDE. We summarise these distributions using kernel density estimates
with Gaussian kernels, selecting the bandwidths according to Scott’s rule (Scott, 2015,
eq.(6.44)). Thetwodistributionscloselyagree(Figure2.3(b)),showingthatFVAEcaptures
not only individual paths but also ensemble statistical properties of Υ.
16
)52=θ(UO
ytisneDAutoencoders in Function Space
2.5.2 Estimation of Markov State Models
The evolution of molecules on long timescales is of significant interest in a variety of sci-
entific applications. One example arises in the study of protein folding (Konovalov et al.,
2021), where one wishes to capture the complex, multistage transitions of proteins between
configurations. The high dimension of these systems means that numerical simulations are
possible only on timescales orders of magnitudes shorter than those of physical interest.
Markov state models (MSMs) offer one method of distilling many short MD simulations
into a statistical model permitting much longer simulations (see, e.g., Husic and Pande,
2018). Under the assumption that the underlying dynamics are given by a random process
(u t) t⩾0 taking values in the configuration space X, MSMs partition the space into finitely
many disjoint state sets, X = X X , and, for some lag time τ > 0, consider the
1 p
∪···∪
discrete-time process (U k) k∈N defined such that U k takes value i if u kτ X i. When τ is
∈
large enough that (U k) k∈N is a Markov process, we can characterise the distribution of the
process by estimating transition probabilities from the MD simulations; the resulting MSM
can then be used to simulate on much longer timescales.
Motivated by this application, we show the power of FVAE in estimating transition
probabilities using training data sampled at sparse or irregular intervals, where direct com-
putationfromthedatamaynotbefeasible. Weillustratethisideawithanexamplebasedon
the Brownian dynamics model (2.24) on the configuration space X = R2 using a multiwell
potentialU (Figure2.4(a))givenasthesumofaquadraticbowlperturbedalinearfunction
and the negative of six Gaussian densities, with minima at (0,0), (0.2,0.2), ( 0.2, 0.2),
− −
(0.2, 0.2), (0,0.2) and ( 0.2,0). We give the full expression for U in Appendix B.3.
− −
Similarly to Section 2.5.1, we define the data space = C ([0,T],X) and let Υ P( )
0
U ∈ U
be the path measure associated with the SDE (2.24) with potential U, temperature ε = 0.1,
finaltimeT = 3andinitialconditionu = 0. Thetrainingdatasetconsistsof16,384sample
0
paths provided at a timestep of 3/512, where, for each sample, it is assumed that 50% of
timesteps are missing from the training data in order to demonstrate FVAE’s ability to
work with heterogeneous data. Data are generated using an Euler–Maruyama scheme as
described in Appendix B.3. We train FVAE using the SDE loss (Section 2.3.1) using an
OU process with θ = 100 as the decoder noise, zero-penalty scale λ = 50, regularisation
scale β = 0.02, and latent dimension d = 16.
Z
(a) Potential U: X R (b) Maximum-likelihood transition matrices
and states X i, i=1,→...,9 (i) Direct numerical (ii) FVAE
00.30.3 simulation TDNS(τ) TFVAE(τ)
0.4 True Data (ShorTtr)ue Data (Short) ModMoedle l SSaammplepsles
0.4 7 8 9 00.15.15 1.0 11..00
0.20.2 00 .00 0.8 00..88
-0.15 x2 0.00.0 4 5 6 0.15 0.6 00..66
-0.3 0.30
0.4 00..44 -0.2 0.2 - 00 .45.45
-0.40.4
1 2 3 -00.60.6 0.2 00..22
- 00 .75.75 0 1 2 3 4 5 60.07 8 0 1 2 3 4 5 6 7 8 0 0.. 00
-00..44 -00..22 00..00 00..22 00..44 0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8
x1
Figure 2.4: Transition matrices with lag time τ = 3/512 computed using FVAE compared to
direct numerical simulation, both on the time interval [0,T], T = 3.
17
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5 6
7
8
00
11
22
33
4455667788Bunker, Girolami, Lambley, Stuart, and Sullivan
Partitioning the configuration space X. The states of a MSM can be selected manu-
allyusingexpertknowledgeorautomaticallyusingvariationalormachine-learningmethods
(Mardt et al., 2018). For simplicity, we choose the states by hand, partitioning X = R2 into
p = 9 disjoint regions (Figure 2.4(a)) divided by the four lines x = 0.1 and x = 0.1.
1 2
± ±
Using FVAE to estimate transition probabilities. Aftertraining,FVAEcanbeused
as a generative model to sample new trajectories similar to the training data, as already
established in Section 2.5.1. Using FVAE allows us to generate these trajectories at any
time discretisation, giving us significant flexibility in the choice of lag time τ, even if the
original training trajectories are discretised at irregular timesteps which differ from the
desired lag time.
The lag time τ has significant impact on the statistical properties of the resulting MSM
(Nu¨ske et al., 2017); while various methods exist to verify that the lag time is sufficiently
large to ensure the process (U k) k∈N is approximately Markov (see, e.g., Prinz et al., 2011,
Sec. IV.F), for simplicity we focus on a fixed lag τ = 3/512 and set aside the question of
Markovianity.
We draw N = 2,048 trajectories (u(m))N from the FVAE generative model g(z)+η,
m=1
z
∼
N(0,I dZ), η
∼
P η, discretised on a mesh of K = 513 points with timestep 3/512 on the
time interval [0,T]. We then compute the count matrix
K N
CFVAE(τ) = (cid:0) CFVAE(τ)(cid:1) , CFVAE(τ) = (cid:88) (cid:88) 1(cid:104) u(m) X and u(m) X (cid:105)
ij i,j∈{1,...,p} ij kτ ∈ i (k+1)τ ∈ j
k=0m=1
and derive the corresponding maximum-likelihood transition matrix TFVAE(τ) by normalis-
ing each row of CFVAE(τ) to sum to one; for simplicity we do not constrain the transition
matrix to satisfy the detailed-balance condition (see Prinz et al., 2011, Sec. IV.D).
We compare the resulting transition matrix TFVAE(τ) to the transition matrix TDNS(τ)
computed analogously using 2,048 direct numerical simulations on the regular timestep τ,
finding close agreement between the two transition matrices (Figure 2.4(b)).
3 Problems with VAEs in Infinite Dimensions
In Section 2.2, we derived a well-defined VAE objective in infinite dimensions under the
assumption that Υ P( P ), i.e.,
η
∈ U∥
(cid:20) (cid:21)
dΥ
D (Υ P ) = E log (u) < . (3.1)
KL ∥ η u∼Υ dP η ∞
This condition is also necessary in finite dimensions, but becomes increasingly significant—
and difficult to satisfy—in infinite dimensions. In this section, we discuss the situations in
which (3.1) fails, and the pathologies that arise when this is the case:
(a) The ELBO (u;θ ,θ ) may be infinite for almost all u . This can result in the
f g
L ∈ U
discretised training loss diverging as resolution is refined.
(b) The ELBO (u;θ ,θ ) may be finite for each u , but its expectation over the
f g
L ∈ U
data—the training objective (θ ,θ )—may be infinite for all θ Θ and θ Θ .
f g f f g g
J ∈ ∈
This manifests as a divergent training loss in the infinite-data limit.
18Autoencoders in Function Space
(c) Theobjective (θ ,θ )maybeinfiniteevenwhenthepopulationlossD (Qθ f Pθg )
f g KL z,u z,u
J ∥
is finite, invalidating the probabilistic interpretation of FVAE.
Related issues arise quite broadly in statistical models with infinite-dimensional data. For
nonparametric Bayesian methods, prior misspecification can lead to significant issues in
interpreting the resulting posterior (see Ghosal and van der Vaart, 2017, Chapter 1.3). In
the machine-learning literature, variational inference on function space arises in the context
of Bayesian neural networks (Sun et al., 2019; Burt et al., 2021; Cinquin and Bamler, 2024),
and the consequent issues are closely connected to those encountered in our study.
In Section 3.1, we describe the issues arising when dΥ/dP is undefined, and give a
η
numerical example of a divergent training loss in the infinite-resolution limit as described
in (a). In Section 3.2, we discuss the case that the density dΥ/dP exists but tthe finite-
η
information condition (3.1) still fails, which can occur even in finite dimensions and leads
to the situations described in (b) and (c).
3.1 Infinite ELBO
For the KL divergence D (Υ P ) to be finite, it is an essential prerequisite that Υ P .
KL η η
∥ ≪
In finite dimensions, this is almost always the case provided P has full support; in infinite
η
dimensions,however,absolutecontinuityismuchmoresensitivetosmalldifferencesbetween
measures, and a lack thereof is the “typical” behaviour even in relatively tame situations.
Example 3.1 (No absolute continuity with Gaussian data and noise) Let be a
U
separable Hilbert space, let Υ = N(0,C) be a nondegenerate Gaussian measure with co-
variance operator C: , and let P = N(0,αC) for some α > 0. Owing to the
η
U → U
Feldman–H´ajek dichotomy (see Bogachev, 1998, Ex. 2.7.4), even slight misspecification of
P leads to a failure of (3.1): the measures Υ and P are mutually singular unless α = 1.
η η
Issues arising from a lack of absolute continuity are not purely theoretical, and can lead
to the ELBO (u;θ ,θ ) being almost surely infinite, rendering training meaningless; in
f g
L
this case, discretisations of the continuum training objective will diverge as resolution is
refined.
Example 3.2 (Infinite ELBO when Υ P ) Consider the following generative model
η
⊥
for (generalised) functions u defined on (0,1):
θ Uniform(0,1),
∼
u θ = δ ,
θ
|
where δ is a Dirac function centred at x (0,1). Since u almost surely takes values in
x
∈
the Sobolev space Hs((0,1)), s < 1/2, we take = H−1((0,1)), and let Υ P( )
− U ∈ U
denote the law of u. Suppose that η is white noise on L2((0,1)), so that P P( ) and
η
∈ U
H(P ) = L2((0,1)). The measures Υ and P are mutually singular, so (3.1) must fail;
η η
furthermore, the ELBO
(cid:20) (cid:21)
1
(u;θ ,θ ) = E g(z;θ ),u ∼ g(z;θ ) 2 D (Q P ) (3.2)
L f g z∼Q ⟨ g ⟩L2 − 2∥ g ∥L2 − KL z|u ∥ z
z|u
19Bunker, Girolami, Lambley, Stuart, and Sullivan
may be infinite or even undefined for each u , since the term g(z;θ ),u ∼ is only
∈ U ⟨ g ⟩L2
defined up to P -measure zero sets. This manner in which this problem manifests depends
η
on the parametrisation of the decoder g and the discretisation of the terms in (3.2). We
(a) FVAE training loss (Section 2) (b) FAE training loss (Section 4)
Trainingloss(ours) P ηT=raNini(n0g,lIo)ss;(5V0ArNuOn)s =HTr− ai1 n(i(n0g,l1os)s);(o5u0rs)runs Trainingloss(VANO)
U
0.025 0.025
9 d ofis Dcr ie rt ai csa δt θion 0 9 d ofis Dcr ie rt ai csa δt θion 0
0.020 0.020
l mea er an na µble −10 l mea er an na µble median −10
0.015 median 0.015
20 20
− −
0.010 30 0.010 30
− −
learnable reconstruction learnable reconstruction
SDσ g(z;θg) SDσ g(z;θg)
40 40
0.005 − 0.005 −
50 50
− −
0 θ 1 816 32 64 128 816 320 64 θ 128 1 816 32 64 128 816 32 64 128
x Resolution ResRoeslouluttiioonn x ResRoeslouluttiioonn Resolution
Figure 3.1: When the data measure Υ is singular with respect to P , the expectation of
η
the ELBO (3.2) over the data measure is ill-defined, so the discretised training losses do
not converge to a well-defined continuum limit. Further details in Appendix B.4. Using the
FAE loss we shall propose in Section 4 leads to a well-defined continuum objective.
demonstrate one possibility numerically (Figure 3.1) using a specially designed encoder and
decoder architecture (Appendix B.4), finding that discretisations of the objective diverge to
as the mesh is refined.
−∞
The behaviour in the previous example can be ascribed to the misspecification of the
decoder noise, and consequently of the FVAE generative model P : realisations from P
u u
are swamped in white noise and bear little resemblance to true samples from the data
distribution. The previous example is also relevant in the analysis of VANO, with white
decoder noise being the setting in which VANO works; we discuss this further in Section 5.
3.2 Lack of Correspondence Between Training Objective and Population Loss
Absolute continuity is necessary but not sufficient for (3.1) to hold: the expectation over
u Υ may be infinite even if dΥ/dP (u) exists for all u . Such situations arise even in
η
∼ ∈ U
finitedimensions, andresultinadivergenceofthecontinuumtrainingobjective (θ ,θ )in
f g
J
the infinite-data limit. Though this is of little concern in the typical machine-learning setup
of a fixed training dataset, this issue is relevant in online learning and when fine-tuning an
existing model on new data, since in this setting the dataset cannot be viewed as a finite
collection. We give an example in = R, and point out that our argument applies equally
U
to the training objective corresponding to the usual ELBO of Kingma and Welling (2014).
Example 3.3 Let = R, suppose that Υ P( ) is the law of a Cauchy distribution with
U ∈ U
median 0 and scale 1 and let η N(0,σ2) for some σ > 0. Even though Υ P , the
η
∼ ≪
finite-information condition (3.1) fails since the Cauchy data distribution has far heavier
tails than the Gaussian noise distribution. This leads to several undesirable behaviours.
20Autoencoders in Function Space
(a) When Q = P for all u and g(z;θ ) = 1 for all z , the FVAE ELBO
z|u z g
∈ U ∈ Z
(u;θ f,θ g) = u 1/2 is clearly finite for each u . But, since all moments of the
L − ∈ U
Cauchy distribution of order at least one are infinite, the training objective is infinite:
(θ f,θ g) = E (cid:2) 1/2 u(cid:3) = .
J u∼Υ − ∞
Similar arguments apply to the VAE ELBO (u;θ ,θ ,β). Moreover, examining
VAE f g
L
the joint measures (2.6) and (2.8) reveals
P (dz,du) = P (du g(z;θ ))P (dz), Q = Υ(du)P (dz),
z,u η g z z,u z
−
and so D (Q P ) = D (Υ P ( 1)) = .
KL z,u z,u KL η
∥ ∥ · − ∞
(b) Let h: R R be an increasing map such that if z P , then h(z) Υ, derived, for
z
→ ∼ ∼
example, through the inverse transform method. When Q = P for all u and
z|u z|u
∈ U
g(z;θ ) = h(z), we conclude that P = Υ P and the joint measures (2.7) and (2.8)
g u η
∗
factorise as
P (dz,du) = (Υ P )(du)P (dz), Q (dz,du) = Υ(du)P (dz),
z,u η z|u z,u z|u
∗
so D (Q P ) = D (Υ Υ P ) < . But, as a consequence of (2.11), since
KL z,u z,u KL η
∥ ∥ ∗ ∞
D (Υ P ) = , the training objective (θ ,θ ) must also be infinite.
KL η f g
∥ ∞ J
Consequently, when (3.1) fails, the correspondence between minimising the population loss
D (Qθ f Pθg ) and the training objective (θ ,θ ) is broken.
KL z,u z,u f g
∥ J
4 Regularised Autoencoders on Function Space
Unless a suitable choice of decoder noise P exists with so that the finite-information condi-
η
tionΥ P( P )holds, theFVAEobjectivemaybeill-definedandthetrainingobjective
η
∈ U∥
infinite. This is a significant obstacle in infinite dimensions: for many data distributions, it
maybedifficult—orevenimpossible—tofindanappropriatechoiceofP . Toovercomethis,
η
we set aside the probabilistic motivation for VAEs and define a regularised autoencoder in
function space—which we call the functional autoencoder (FAE)—that avoids the need
for onerous conditions on the data measure.
In Section 4.1, we state this objective and point out connections to the FVAE objective.
In Section 4.2, we outline minor adaptations to the FVAE encoder and decoder to the
regularised autoencoder framework. In Section 4.3, we demonstrate our proposed method,
FAE, on two examples from scientific machine learning.
4.1 Objective and Algorithms
Let be a separable Banach space, let = RdZ, and assume Υ P( ). Define a
U Z ∈ U
parametrised class of encoders f: Θ and a class of decoders g: Θ ,
f g
U × → Z Z × → U
where, unlike in Section 2, f now returns a single latent vector f(u;θ ) and g is no
f
∈ Z
longer constrained to take values in a subspace .
V ⊂ U
21Bunker, Girolami, Lambley, Stuart, and Sullivan
We define a regularised autoencoder as the sum of a misfit term on the data space
U
and a regularisation term on the latent space with regularisation scale β > 0:
Z
(cid:34) (cid:35)
JFAE(u;θ f,θ g,β) = u∼E
Υ
21(cid:13) (cid:13)g(cid:0) f(u;θ f);θ g(cid:1) −u(cid:13) (cid:13)2
U
+β(cid:13) (cid:13)f(u)(cid:13) (cid:13)2
2
. (4.1)
There is great flexibility in the choice of regularisation term; we adopt the Euclidean
norm as a simplifying choice consistent with using a Gaussian prior in FVAE (Remark 2.5).
While (4.1) has much in common with the FVAE objective (θ ,θ ), it is not marred by
f g
J
the foundational issues raised by our analysis in Section 3; indeed, the FAE objective is
very broadly applicable thanks to the following result, proven in Appendix A.2.
Proposition 4.1 Let be a separable Banach space. Suppose that Υ P( ) has finite
second moment, i.e.,
EU(cid:2)
u
2(cid:3)
< . Then, provided that there exists θ
∈
Θ
U
and θ Θ
∥ ∥U ∞ f ∈ f g ∈ g
such that f(u;θ ) = 0 and g(z;θ ) = 0, the infimum of (4.1) is finite.
f g
In the numerical examples of Section 4.3, we take = L2(Ω), where Ω = Td or Ω =
U
[0,1]d. We then discretise the -norm, on a given mesh x n , with the sum
U { i }i=1
n
1 (cid:88)
u 2 u(x ) 2, u .
∥
∥L2(Ω)
≈ n ∥
i ∥2
∈ U
i=1
Remark 4.2 We emphasise that our framework can be applied whenever is Banach.
U
One can readily imagine, for example, taking to be a Sobolev space of order s ⩾ 1 when
U
derivative information is known for the data (Czarnecki et al., 2017), and approximating the
L2 norm of the data and its derivative by sums. More generally we may use characterising
linear functionals on the Banach space as the starting point for approximation.
4.2 Architecture and Training
We propose a slight modification of the FVAE architecture proposed in Section 2.4 for use
with our regularised autoencoding framework. The only practical change is in the encoder,
whichnolongercomputesparametersforavariationaldistribution; instead, weparametrise
the encoder f: Θ directly as a function-to-vector map as in (2.22), with range
f
U × → Z
RdZ. All other architectural details remain unchanged from Section 2.4, an in particular we
shall frequently make use of the self-supervised training scheme described in Section 2.4.4.
Tocomplementthismesh-invariantarchitecture,wenowproposeaself-supervisedtrain-
ing scheme that appears to be new to the operator-learning literature; the practical benefits
of this scheme will be demonstrated in Section 4.3.
4.3 Numerical Examples
In Sections 4.3.1 and 4.3.2, we apply FAE as an out-of-the-box method to discover a low-
dimensional latent space for solutions to the incompressible Navier–Stokes equations and
the Darcy model for flow in a porous medium, respectively. We find that:
(a) FAE’s mesh-invariant architecture autoencodes with performance broadly compara-
ble to convolutional neural network (CNN) architectures of comparable size (Sec-
tion 4.3.1).
22Autoencoders in Function Space
(b) The ability to discretise the encoder and decoder on different meshes enables new
applications to inpainting and data-driven superresolution (Section 4.3.1), as well as
extensions of existing zero-shot superresolution as proposed by VANO (Section 4.3.2).
(c) Self-supervised training significantly improves performance under mesh changes (Sec-
tion4.3.1)andcanacceleratetrainingwhilereducingmemorydemand(Section4.3.2).
(d) Training a generative model on the FAE latent space leads to a resolution-invariant
generative model which accurately captures distributional properties (Section 4.3.2).
4.3.1 Navier–Stokes Equations
We first illustrate how FAE can be used to learn a low-dimensional representation for
snapshots of the vorticity of a fluid flow in two spatial dimensions governed by the Navier–
Stokes equations, and illustrate some of the benefits of our resolution-invariant framework.
Let Ω = T2 be the periodic torus, viewed as the square [0,1]2 with opposing edges
identified and unit normal zˆ, and let = L2(Ω). While the Navier–Stokes equations are
U
typically formulated in terms of the primitive variables of velocity u and pressure p, it is
more natural in this case to work with the vorticity–streamfunction formulation (see, e.g.,
Chandler and Kerswell, 2013, eq. (2.6)). In particular the vorticity u is zero except in
∇×
the out-of-plane component zˆω. The scalar component of the vorticity, ω, then satisfies
(cid:0) (cid:1)
∂ ω = zˆ (u ωzˆ) +ν∆ω+φ on Ω (0,T],
t
· ∇× × × (4.2)
ω(x,0) = ω (x) on Ω.
0
In this formulation the velocity u = (ψzˆ) where the streamfunction ψ satisfies ω = ∆ψ.
∇×
Thus ψ is uniquely defined, up to an irrelevant constant, in terms of ω, and (4.2) defines a
closed evolution equation for ω.
We suppose that Υ P( ) is the distribution of the scalar vorticity ω( ,T) at T = 50
∈ U ·
given by (4.2), with viscosity ν = 10−4, and forcing
1 (cid:16) (cid:0) (cid:1) (cid:0) (cid:1)(cid:17)
φ(x) := sin 2π(x +x ) +cos 2π(x +x ) .
1 2 1 2
10
We assume that the initial condition ω has distribution N(0,C) with covariance operator
0
C = 73/2(49I ∆)−5/2, where ∆ is the Laplacian operator for spatially-mean-zero functions
−
on the torus T2. The training dataset, based on that of Li et al. (2021), consists of 8,000
samples from Υ generated on a 64 64 grid using a pseudospectral solver, and a further
×
2,000 independent samples are held out as an evaluation set; the data are scaled so that
ω(x,T) [0,1] for all x Ω. Further details are provided in Appendix B.5.
∈ ∈
WetrainFAEwithlatentdimensiond = 64andregularisationscaleβ = 10−3, anduse
Z
self-supervised training (Section 2.4.4), masking 70% of input mesh points and evaluating
the decoder on the masked mesh points. Throughout, given a mesh x n , we evaluate
{ i }i=1
autoencodingperformancethroughtheMSEin averagedovertheheld-outdata(u(k))N :
U k=1
N n
MSE(θ ,θ )2 1 (cid:88)(cid:88)(cid:16) g(cid:0) f(u(k);θ );θ (cid:1) (x ) u(k)(x )(cid:17)2 . (4.3)
f g f g i i
≈ nN −
k=1 i=1
23Bunker, Girolami, Lambley, Stuart, and Sullivan
Performance at fixed resolution. We compare the autoencoding performance of our
mesh-invariantFAEarchitecturetoafixed-resolutionCNNused“offtheshelf”,bothtrained
usingtheFAEobjectivewithallotherhyperparametersthesame. Ourgoalistounderstand
whether our architecture is competitive even without the inductive bias of CNNs.
We train one instance of FAE with each ar-
chitecture on data discretised on a 64 64 grid, Autoencoding on evaluation set (64 64 grid)
× ×
without masking (details in Appendix B.5). MSE (4.3) Parameters
When autoencoding held-out data from the FAE architecture 4.81 10−4 64,857
evaluation set, we find that our FAE architec- ±0.17× ×10−4
CNN architecture 2.32 10−4 71,553
ture achieves MSE only slightly larger than the ±0.05× ×10−4
CNN architecture, with a comparable number
Mean 1 standard deviation; 5 training runs
of parameters (Figure 4.1). It is reasonable to ±
Figure 4.1: Our resolution-invariant ar-
expect some tradeoff between fixed-resolution
chitectureperformscomparablytooff-the-
performance and the ability to change meshes;
shelf CNN architectures at similar param-
further research is desirable to close this gap
eter counts.
through better mesh-invariant architectures.
Inpainting. FAE can encode and decode on
differentmeshes,makingitanaturaltoolforinpainting—inferringmissingpartsoftheinput
function based on the observed data and knowledge of the underlying data distribution
acquired through training. Methods to solve inpainting and related inverse problems are
actively researched across the computer-vision community; see, e.g., Quan et al. (2024).
Owing to the mesh-invariance of our architecture, our proposed FAE can solve a variety of
inpainting tasks with a single trained model.
We evaluate the inpainting performance of FAE—trained as described at the start of
the section—by taking data drawn from the evaluation set, discretised on a 64 64 grid,
×
and applying a variety of masks:
(i) masking of 95% of mesh points, randomly selected;
(ii) masking of a square with random centre and random side length; and
(iii) maskingofa 0.05-superlevelsetofaGaussianrandomfieldN(0,(τ2I ∆)−d),where
− −
∆ is the Laplacian on the torus and τ = 30 and d = 1.2.
We encode each masked sample and decode on a 64 64 grid (Figure 4.2), finding that
×
reconstructionsagreewiththegroundtruthandFAEprovesrobusteventosignificantlevels
of masking. As a consequence of the autoencoding procedure, the unmasked region of the
input may also be modified, an effect that is most pronounced in (ii), where some features
in the input are lost in the reconstruction. We hypothesise that the failure to capture
fine-scale features could be mitigated with more sophisticated neural-operator architectures
that better capture the very fine-scale features; we leave this to future work.
The percentage of mesh points masked during self-supervised training has a significant
influence on inpainting quality; to quantify the tradeoffs, we train instances of FAE with
masking, preserving a point ratio of 10%, 50% and 90% of the input mesh points during
training, respectively, holding all other hyperparameters constant. We evaluate each model
by encoding held-out data with a given point ratio and computing the reconstruction MSE
(4.3) on a 64 64 grid (Figure 4.2(b)). We find that models trained with the encoder
×
discretised on a low point ratio perform better when evaluated on data with a high point
24Autoencoders in Function Space
(a) Inpainting with arbitrary corruptions (b) Reconstruction MSE [ 10−4]
input reconstruction ground truth Mean 1 standard devia× tion;
0.7 00.7.7 ±5 training runs
0.6 00.6.6 14.5
(i) 00 .. 45 00 00 .. 45 .. 45 Point%(train)
0.3 00.3.3 10
0.2 00.2.2
0.1 00.1.1 12.0 50
0.8 00..88 90
(ii) 0.6 00..66 9.5
0.4 00..44
0.2 00..22
7.0
0.6 00..66
(iii) 0.5 00..55
0.4 00..44
4.5
0.3 00..33 10 30 50 70 90
0.2 00..22 Point % (evaluation)
Figure 4.2: (a) By encoding inputs on a given mesh and decoding on a grid, FAE can solve
a variety of inpainting tasks. Further samples are provided in Appendix B.5. (b) Training
the encoder on a sparse mesh has a regularising effect on FAE, leading to lower evaluation
MSE on dense meshes, but harms performance when evaluating on very sparse meshes.
ratio,whichwehypothesiseisduetotheregularisingeffectoftrainingwithfewmeshpoints.
On the other hand, such models perform poorly on sparsely sampled evaluation data with a
low point ratio, likely because when training on sparse meshes the encoder is very unlikely
to have seen a mesh that coincides with the mesh seen at evaluation time. We conclude
that training with relatively low levels of masking (e.g., 90% of points unmasked) is best
when the goal is to evaluate on very sparse data, but this incurs a moderate cost when the
evaluation mesh is dense. Further analysis on this tradeoff is provided in Appendix B.5.
While we do not consider it here, the random masking applied during self-supervised
training could be further augmented with other types of masks, such as polygons, with the
aim of improving the robustness of FAE to similar masks at evaluation time.
Superresolution. The ability to encode and decode on different meshes also makes FAE
a flexible method for (single-image) superresolution: reconstruction of a high-resolution
output from a single low-resolution input (see Li et al., 2024). Deep-learning-based super-
resolution methods have also found use in fluid dynamics, e.g., in increasing the resolution
(“downscaling”) of numerical simulations (Kochkov et al., 2021; Bischoff and Deck, 2024).
An advantage of FAE over many other approaches is that a single trained model can be
applied to any input resolution and upsampling factor.
We first evaluate the performance of FAE for data-driven superresolution, where
the model is trained at high resolution and used to enhance lower-resolution inputs. After
training, we encode inputs at resolution 8 8 and 16 16 and decode on the original 64 64
× × ×
training resolution (Figure 4.3(a)). FAE is able to resolve features not visible in the inputs
using structure learned from the training data, and, while reconstruction MSE grows as the
input resolution is decreased (Figure 4.3(b)), the perceptual quality remains good even at
resolution8 8. Aswithinpainting, superresolutionperformancecouldbefurtherimproved
×
with an architecture that is better able to capture the turbulent dynamics in the data.
25
]4
01
[ESM
−
×Bunker, Girolami, Lambley, Stuart, and Sullivan
(a) Examples of data-driven superresolution (b)MRecSoEnstvrusctEionncMoSdEeornDfulilmmesh
input reconstruction ground truth
101−0−22
0.6 00.6.6
0.5 00.5.5
(i)
0.4 00.4.4
0.3 00.3.3
0.2 00.2.2
8 8 64 64 64 64
× ×
0.7
× 00.7.1 701−0−33
median
0.6 00.6.6
(ii) 0.5 00.5.5
0.4 00.4.4
0.3 00.3.3
0.2 00.2.2 88 1166 3322 6644
16 16 64 64 64 64 Resolution
× × × Encoder Dim
Figure 4.3: (a) FAE is able to encode inputs on low-resolution grids and decode at higher
resolution,recoveringfine-scalefeaturesusingknowledgeoftheunderlyingdatadistribution.
Further uncurated samples are provided in Appendix B.5. (b) Superresolution becomes
increasingly difficult as the resolution of the input decreases, but FAE performs well even
with very low resolution inputs.
We also investigate the stability of FAE for zero-shot superresolution (see, e.g., Li
et al., 2021), where the model is evaluated on higher resolutions than seen during training.
Our architecture proves robust when autoencoding on meshes much finer than the original
64 64 training grid (Figure 4.4(a)). As in our discussion of inpainting, the main limitation
×
is the underlying representation capacity of the architecture, which results in the loss of
some fine-scale features present even in the 64 64 input. Thanks to the coordinate MLP
×
architecture of our decoder, we can decode on extremely fine meshes without exhausting
the GPU memory (details in Appendix B.5). We point out that zero-shot superresolution
is only possible with VANO when the input is given on the same mesh seen during training;
FAE does not have this constraint.
In our investigation of zero-shot superresolution, we do not embed prior information or
train FAE above resolution 64 64, so we do not expect to resolve high-frequency features
×
beyondthoseseenduringtraining. Nevertheless,therobustnesstomeshchangesweobserve
validates the approximate resolution-invariance of our architecture.
Applications of the latent space . The regularised FAE latent space gives a well-
Z
structured finite representation of the infinite-dimensional data u , extracting the in-
∈ U
trinsic features of the data in a resolution-invariant way. We expect there to be benefit
in using this latent representation as a building block for applications such as supervised
learning and generative modelling on functional data, and draw parallels with the many
other operator-learning methods with encoder–decoder structure (Seidman et al., 2022).
Toverifythatthelatentspaceisindeedwell-structured,wedrawsamplesu andu from
1 2
the evaluation set, compute the latents z = m(u ) and z = m(u ) , and evaluate the
1 1 2 2
∈ Z
decodergalongtheconvexcombinationz α+z (1 α). Thisleadstoasensibleinterpolation
1 2
−
in , indicating that the latent space has been effectively regularised (Figure 4.4(b)).
U
Efficient superresolution on regions of interest. Since the FAE decoder can be eval-
uated on any mesh, we can perform superresolution in a specific region of interest without
26Autoencoders in Function Space
(a) Autoencoding beyond training resolution with zero-shot superresolution
00.7.7
00.6.6 00.6.6
00.5.5 00.5.5
32 00.4.4 512 00.4.4
incre×ase 00.3.3 incre×ase 00.3.3
00.2.2 00.2.2
64 64 2,048 2,048 64 64 32,768 32,768
× × (cid:0) × (cid:1) ×
(b) Latent interpolation g f(u ;θ )α+f(u ;θ )(1 α);θ
1 f 2 f g
−
00.7.7 0.7
00.6.6 0.6
00.5.5 0.5
00.4.4 0.4
00.3.3
0.3
00.2.2
0.2
u 1 α = 0.1 α = 0.3 α = 0.5 α = 0.7 α = 0.9 u 2
Figure 4.4: (a) FAE can stably decode at resolutions much higher than the training res-
olution (best viewed digitally). (b) The regularised latent space allows for meaningful
Z
interpolation between samples. Further examples are given in Appendix B.5.
the need to upsample across the whole domain. This could also be done adaptively, decod-
ing first on a coarse grid and then refining on a specific region to resolve finer details on
demand. Doing this leads to significant savings through reduced inference time, memory
usage, and energy cost. We note that similar strategies could also be possible with VANO.
To illustrate the potential savings, we consider the task of decoding on a circular subre-
gionofinterestwithtargetmeshspacing1/400(Figure4.5(a)). Achievingthisresolutionover
the whole domain—corresponding to a 400 400 grid—would involve 160,000 evaluations
×
of the decoder network; decoding on the subregion requires just 1/4 of this (Figure 4.5(b)).
(a) Superresolution on patches (b) Number of decoder network evaluations
00.70..77 (i) superresolution on full 400 400 grid
00.60..66 ×
00.50..55 160k
00.40..44
(ii) patch superresolution with same mesh spacing
00.30..33
00.20..22 38k
00.10. .11
refine in specific region
input data on (mesh spacing 1 )
coarse mesh (64 64) 400
×
Figure 4.5: The FAE decoder can be discretised on arbitrary meshes, allowing for efficient
superresolution on regions of interest (best viewed digitally).
4.3.2 Darcy Flow
Darcyflowisamodelofsteady-stateflowinaporousmedium,derivablefromfirstprinciples
using homogenisation; see, e.g., Freeze and Cherry (1979, Sec. 2.11) and Keller (1980). We
restrict attention to the two-dimensional domain Ω = [0,1]2 and suppose that, for some
permeability field k: Ω R and forcing φ: Ω R, the pressure field p: Ω R satisfies
→ → →
(cid:0) (cid:1)
k p = φ on Ω,
−∇· ∇ (4.4)
p = 0 on ∂Ω.
27Bunker, Girolami, Lambley, Stuart, and Sullivan
We assume φ := 1 and that k µ is distributed as the pushforward of the measure
N(cid:0) 0,( ∆+9I)−2(cid:1) , where∆isthe∼ NeumannLaplacian, underthemapψ(x) := 3+9 1(cid:2) x ⩾
0(cid:3) . We− take = L2(Ω) and define the data measure Υ P( ) to be the pushforw· ard of
U ∈ U
µ under the solution operator : k p to the system (4.4). While solutions to the elliptic
G (cid:55)→
PDE (4.4) can be expected to have much greater than L2-regularity (see, e.g., Evans, 2010,
Sec. 6.3), we assume only that p L2(Ω) to show the versatility of our method.
∈
The training dataset is based on that of Li et al. (2021) and consists of 1,024 samples
from Υ discretised on a 421 421 grid, generated using a finite-difference method, with a
×
further 1,024 samples generated in the same manner held out as an evaluation set. Data
are scaled so that p(x) [0,1] for all x Ω, and we provide further details of the training
∈ ∈
data in Appendix B.6. We train FAE with latent dimension d = 64, regularisation scale
Z
β = 10−3, and use self-supervised training, masking 70% of input mesh points and decoding
on unseen mesh points. Where specified, we downsample training and evaluation data as
described in Appendix B.6, and, as in (4.3), autoencoding is evaluated with a discretisation
of the MSE in .
U
Accelerating training using self-supervised learning. As well as improving recon-
struction quality and robustness to mesh changes, self-supervised training can significantly
reduce the time and memory cost of training. As evaluation cost scales linearly with the
number of mesh points, masking a high proportion of mesh points in both the encoder and
decoder significantly reduces GPU demand.
To illustrate the numerical benefits, we compare the training dynamics over time for
FAEtrainedonthedatadownsampledtoresolution211 211withtheencoderanddecoder
×
discretisedwiththesamepointratioondistinctrandommeshestomaximisecomputational
savings. Both models are trained on a single NVIDIA GeForce RTX 4090 with 24GB of
VRAM with batch size 32, stopping once reconstruction MSE on a held-out set plateaus.
We evaluate both models through MSE on evaluation data discretised on a 211 211 grid.
×
(a) Reconstruction MSE on held-out set (211 211) (b) Zero-shot reconstruction MSE
0.016 P (to ri an int i%
ng×
)
1.2×10−
t
r2
r eta
sr
oain
i ln
uin
i
tng
ig onresolution
(100 runs)
53×53
10%
0.014
50% 1.1
100%
0.012
1.0
0.010 0.9 median
0.008 0.8
0 20 40 60 80 100 1x1× 4/4/33x× 22x× 4x4×
Wall-clock time [s] TrainUinpgsRamespolluintigonfaRctaotiro
Figure 4.6: (a) For high-resolution data, training with masking in the encoder and decoder
reduces computational cost and allows for better resource use, leading to significantly faster
convergence. (b)OntheDarcyflowdataset,FAE’szero-shotreconstructionMSEcompared
to high-resolution ground truth remains low even at 4 training resolution.
×
Themodelstrainedwithmaskingconvergesignificantlyfasterasthesmallerdatatensors
allow for better use of the GPU parallelism (Figure 4.6(a)). At higher resolutions, memory
28Autoencoders in Function Space
constraints may entirely preclude training on the full grid, while training with masking
enables training at much higher resolutions without exhausting GPU memory.
Related ideas have been used in the adaptive-subsampling training scheme for FNOs
proposed by Lanthaler et al. (2024), which involves training first on a coarse grid and
refining the mesh each time a given evaluation metric plateaus. Our approach differs by
dropping mesh points randomly, which would not be possible with the grid-based FNO.
One can readily imagine training FAE with a combination of adaptive subsampling and
self-supervised learning; we leave this to future work.
Zero-shot superresolution. WeevaluatetheperformanceofFAE’szero-shotsuperreso-
lution quantitatively using the high-resolution ground truth available in this dataset. After
trainingFAEondatadownsampledtoresolution53 53,weevaluateFAEonheld-outdata,
×
discretising the encoder on a 53 53 grid and discretising the decoder on a finer mesh. We
×
find that, on this dataset, upsampling by factors 4/3 and 2 possibly leads to slightly reduced
MSE, which we expect is a result of minor artifacts caused by the downsampling procedure
during training, while upsampling by a factor of 4 leads to modestly increased MSE (Fig-
ure 4.6(b)). These results suggest that FAE is robust at resolutions far above those seen
during training, and does not suffer from severe degeneration when changing resolution.
We emphasise that the smooth pressure fields in this dataset (see Figure 4.7(a)) mean
thereislittlehigh-frequencycontentbeyondthatseenduringtraining, soitisnotsurprising
that the error is relatively stable at higher resolutions; our intention is not to extrapolate
but to verify stability of our architecture under mesh changes. For the turbulent flows in
Section 4.3.1, we expect that the error would significantly increase at higher resolutions
owing to the fine-scale features unseen during training.
Generative modelling. Unlike FVAE, our proposed FAE is not inherently a generative
model. Instead, we can construct a generative model on the regularised autoencoder la-
tent space (see Ghosh et al., 2020; Vahdat et al., 2021), resulting in a resolution-invariant
generative model. More precisely, by learning the distribution Λ P( ) of z given by
∈ Z
z u = f(u;θ ), u Υ (4.5)
f
| ∼
with a fixed-resolution generative model P P( ), it is then straightforward to sample
z
∈ Z
from the FAE generative model P P( ) for u given by
u
∈ U
u z = g(z;θ ), z P . (4.6)
g z
| ∼
Provided Λ P and g(f(u)) u for u Υ, samples from P will be approximately
z u
≈ ≈ ∼
distributed according to Υ. Unlike many other generative models on function space (which
we survey in Section 5), our approach pushes forward a finite-dimensional latent variable
that can be sampled at low cost; thanks to our mesh-invariant architecture, this can be
decoded on any mesh and refined on demand, similar to the experiments in Section 4.3.1.
As a simple illustration of the idea, we take FAE, trained at resolution 47 47 with
×
self-supervised training as described at the start of the section, and seek to extend this
with a generative model. We fit a Gaussian mixture model P with 10 components to the
z
distributionΛdescribedin(4.5)usingtheexpectation-maximisationalgorithm(seeBishop,
2006, Section 9.2.2). Samples from the generative model P closely resemble samples from
u
the underlying data measure Υ (Figure 4.7(a)).
29Bunker, Girolami, Lambley, Stuart, and Sullivan
(a)(i) FAE samples g(z;θ g), z P z (47 47 grid) (a)(ii) Samples p Υ (47 47 grid)
∼ × 11.0.0 ∼ × 11.0.0
00.8.8 00.8.8
00.6.6 00.6.6
00.4.4 00.4.4
00.2.2 00.2.2
00.0.0 00.0.0
(b) Kernel density estimates for quantities of interest Q (p) (1,024 samples)
i
(i) Q 1(p)=m x∈a Ωxp(x) (ii) Q 2(p)= ∥p ∥L2
5 5 12 12 FAE FAE
10 10 GroundGTrrouuthndTruth
4 4
8 8
3 3
6 6
2 2
4 4
1 1 2 2
0 0 0 0
0.5 0.05.6 0.06.7 0.07.8 0.80.9 0.91.0 1.0 0.30 0.300.35 0.350.40 0.400.45 0.450.50 0.50
Figure 4.7: (a) Uncurated samples from the FAE generative model for the pressure field
p. Further samples are provided in Appendix B.6. (b) The distributions of quantities of
interest computed using the FAE generative model closely agree with the ground truth.
Tomeasuregenerativeperformance, weapproximatethedistributionsofphysicallyrele-
vantquantitiesofinterestQ (p)dependingonthedatap using1,024samplesgenerated
i
∈ U
from P and Υ. Following the procedure used in Figure 2.3(b) to generate kernel density
u
estimates, we see close agreement between the distributions (Figure 4.7(b)). While it would
also be possible to evaluate the generative model using statistical distances such as maxi-
mummeandiscrepancy(Borgwardtetal.,2006), wefocusoncomparisonsusingmoreeasily
interpretable quantities relevant to the physical system at hand.
Though we adopt the convention of training the autoencoder and generative model
separately (Rombach et al., 2022) here, the models could also be trained jointly; we leave
this, and an investigation of generative models on the FAE latent space, to future work.
5 Related Work
Variationalautoencodingneuraloperators. TheVANOmodel(Seidmanetal.,2023)
also extends the VAE objective to function space and uses ideas from operator learning to
construct a model that can decode at any resolution. Our approach differs substantially in
both the training objective and in practical implementation, as we outline below.
The most significant difference is in the formulation of the underlying loss on function
space. The VANO loss is derived using the same generative model (2.5) as our work,
restricted to the case that the decoder noise η is white noise on L2(Ω), with Ω = [0,1]d,
i.e., P = N(0,I); under these assumptions, η takes values in = H−s(Ω), s < d/2.
η
U −
Unlike our approach, VANO’s loss (Seidman et al., 2023, Theorem 4.1) is formulated as an
extension of the ELBO (2.4), which, in our notation takes the form
(cid:20) dP (cid:21)
LVANO(u;θ f,θ g,β) = z∼E
Q
z|u
−log dPu η|z (u;θ g) +βD KL(cid:0)Qθ zf |u(cid:13) (cid:13)P z(cid:1) (5.1)
(cid:20) (cid:21)
= z∼E
Q
21 ∥g(z;θ g) ∥2 L2(Ω)−⟨g(z;θ g),u ⟩∼
L2(Ω)
+βD KL(cid:0)Q z|u(cid:13) (cid:13)P z(cid:1) ,
z|u
30Autoencoders in Function Space
where β > 0 is a hyperparameter. Analogously to (2.3), when β = 1, this ELBO is viewed
as satisfying the equality
log d dP Pu (u;θ f,θ g,β) = D KL(cid:0)Qθ zf |u(cid:13) (cid:13)Pθ zg |u(cid:1) + LVANO(u;θ f,θ g,β),
η
and the quantity log
dP
u(u) is interpreted as an analogue of the log-likelihood of the data
dP
η
u under the model. Even if the per-sample loss (5.1) makes sense for each u , further
∈ U
justification is needed to ensure that the expectation over the data u Υ is finite in light
∼
of the issues raised in Section 3. Indeed, the datasets considered by Seidman et al. (2023)
all have at least L2-regularity, meaning that the underlying data distribution Υ is mutually
singular with the decoder noise distribution P ; thus, the finite-information condition (3.1)
η
must fail and there is ambiguity in the probabilistic interpretation of their loss.
To avoid such ambiguities, we develop our theory around the underlying population
loss, taking care to ensure that it is defined at the continuum before discretising, which
necessarily results in our FVAE being restricted to data measures having finite information
with respect to the decoder noise. To maximise the applicability of FVAE, we allow greater
freedom in the choice of decoder noise compared to VANO and find that a well-chosen noise
distribution can lead to significant improvements in the generative model (Section 2.5.1).
To explain the empirical successes of VANO despite the foundational challenges, we
reinterpret the VANO objective through the lens of Section 4 as a regularised autoencoder
similar to FAE, with the major difference in the formulation being whether the encoder
returns a point or a distribution. Along with the differences in the underlying training
objective and its interpretation, our work differs substantially in architecture and training
procedure. While the VANO decoders can be discretised on any mesh—and, indeed, our
proposed decoder (Section 2.4.3) closely resembles their nonlinear decoder—the encoders
used by Seidman et al. (2023) assume a fixed mesh for all inputs at training and inference
time. In contrast, our encoder (Section 2.4.2) can be discretised on any mesh; it is this
combination of a discretisation-invariant encoder and decoder which enables many of our
novel contributions, e.g., self-supervised training, inpainting, and superresolution.
Generative models on function space. Aside from VANO, recent years have seen
significant interest in extending generative models to function space. Several extensions
of score-based (Song et al., 2021) and denoising diffusion models (Sohl-Dickstein et al.,
2015; Ho et al., 2020) to function space have been proposed (see, e.g., Pidstrigach et al.,
2023; Hagemann et al., 2023; Lim et al., 2023; Kerrigan et al., 2023; Zhang and Wonka,
2024). Rahman et al. (2022) propose the generative adversarial neural operator, which
extends the formulation of Wasserstein generative adversarial networks (Arjovsky et al.,
2017) to function space and uses FNOs in the generator and discriminator networks to
achieve resolution-invariance. These methods are united by the aim of pushing forward a
Gaussianrandomfieldusinganoperator, whereasgenerativemodelsbasedonautoencoders
involve the pushforward of a Gaussian distribution on the finite-dimensional latent space;
asdiscussedinSection4.3.2, ourapproachhasthebenefitofbeingabletostraightforwardly
generate and refine a single sample at any resolution.
Unsupervised learning on point clouds. As discussed in Section 2.4, our architecture
takes inspiration from the growing literature on machine learning on point clouds, where
31Bunker, Girolami, Lambley, Stuart, and Sullivan
the data are viewed as sets of points which could have arbitrary cardinality. A variety of
modelsforautoencodingandgenerativemodellingonpointcloudshavebeenproposed,such
as energy-based processes (Yang et al., 2020) and SetVAE (Kim et al., 2021). Our approach
differs from these methods by formulating a well-defined loss at the function-space level,
ensuring that our model converges to a well-defined continuum limit as the mesh is refined.
6 Outlook
Our study of autoencoders on function space has led to FVAE, which imposes stringent re-
quirements on the data distribution in infinite dimensions but benefits from firm probabilis-
tic foundations, and the non-probabilistic FAE, which can be applied much more broadly.
Forbothmodels, pairingthefunction-spaceobjectivewithanencoderanddecoderthatcan
be discretised on arbitrary and distinct meshes has significant practical benefit, enabling
training with heterogeneous data, inpainting, superresolution, and generative modelling.
Limitations. Owing to the need to satisfy the finite-information condition (3.1), FVAE
applies only when the data measure is sufficiently close to being Gaussian. This condition
may be impossible to satisfy or onerous to verify, and is a fundamental barrier to applying
any VAE in infinite dimensions. FAE is an alternative autoencoding method in function
space to overcome this, but it does not share the probabilistic foundations of FVAE.
The desire to discretise the encoder and decoder on arbitrary meshes rules out many
high-performing architectures, including convolutional networks and FNOs. We believe
this is the limiting factor in the experiments of Section 4.3, and that combining our work
with more complex mesh-invariant architectures (e.g., Kovachki et al., 2023) or continuum
extensions of point-cloud CNNs (Li et al., 2018) would yield further improvements.
Future work. Ourworkgivesnewmethodsfornonlineardimensionreductioninfunction
space, and we expect there to be benefit in building operator-learning methods that make
use of the resulting latent space, in the spirit of PCA-Net (Bhattacharya et al., 2021). For
FAE, which unlike FVAE is not inherently a generative model, we anticipate particular
benefit in the use of more sophisticated generative models on the latent space, for example
diffusion models, analogous to Stable Diffusion (Rombach et al., 2022).
While our focus has been on scientific problems with synthetic data, our methodology
could also be applied to real-world data, for example in computer vision. We expect that
improved mesh-invariant architectures will be increasingly important in these applications.
Acknowledgments and Disclosure of Funding
JB is supported by Splunk Inc. MG is supported by a Royal Academy of Engineer-
ing Research Chair, and Engineering and Physical Sciences Research Council (EPSRC)
grants EP/T000414/1, EP/W005816/1, EP/V056441/1, EP/V056522/1, EP/R018413/2,
EP/R034710/1, and EP/R004889/1. HL is supported by the Warwick Mathematics Insti-
tute Centre for Doctoral Training and gratefully acknowledges funding from the University
of Warwick and the EPSRC (grant EP/W524645/1). AMS is supported by a Department
ofDefenseVannevarBushFacultyFellowshipandbytheSciAICenter, fundedbytheOffice
32Autoencoders in Function Space
of Naval Research (ONR), under grant N00014-23-1-2729. For the purpose of open access,
the authors have applied a Creative Commons Attribution (CC BY) licence to any Author
Accepted Manuscript version arising.
References
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In
D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on
Machine Learning (ICML 2017),volume70ofProceedings of Machine Learning Research,
pages 214–223, 2017. arXiv:1701.07875.
K. Azizzadenesheli, N. Kovachki, Z. Li, M. Liu-Schiaffini, J. Kossaifi, and A. Anandkumar.
Neural operators for accelerating scientific simulations and design. Nat. Rev. Phys., 6:
320–328, 2024. doi:10.1038/s42254-024-00712-5.
K. Bhattacharya, B. Hosseini, N. B. Kovachki, and A. M. Stuart. Model reduction
and neural networks for parametric PDEs. SMAI J. Comput. Math., 7:121–157, 2021.
doi:10.5802/smai-jcm.74.
T. Bischoff and K. Deck. Unpaired downscaling of fluid flows with diffusion bridges. Artif.
Intell. Earth Syst., 3:e230039, 22pp., 2024. doi:10.1175/AIES-D-23-0039.1.
C. M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statis-
tics. Springer, 2006. ISBN 978-0-387-31073-2.
V. I. Bogachev. Gaussian Measures, volume 62 of Mathematical Surveys and Monographs.
American Mathematical Society, 1998. doi:10.1090/surv/062.
K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Sch¨olkopf, and A. J. Smola.
Integrating structuredbiologicaldata bykernel maximum meandiscrepancy. Bioinform.,
22(14):e49–e57, 2006. doi:10.1093/bioinformatics/btl242.
D.R.Burt,S.W.Ober,A.Garriga-Alonso,andM.vanderWilk. Understandingvariational
inference in function-space. In 3rd Symposium on Advances in Approximate Bayesian
Inference, 2021. arXiv:2011.09421.
E. Calvello, N. B. Kovachki, M. E. Levine, and A. M. Stuart. Continuum attention for
neural operators, 2024. arXiv:2406.06486.
G. J. Chandler and R. R. Kerswell. Invariant recurrent solutions embedded in a
turbulent two-dimensional Kolmogorov flow. J. Fluid. Mech., 722:554–595, 2013.
doi:10.1017/jfm.2013.122.
T. Chen and H. Chen. Approximations of continuous functionals by neural networks with
application to dynamic systems. IEEE Transactions on Neural Networks, 4(6):910–918,
1993. doi:10.1109/72.286886.
T. Cinquin and R. Bamler. Regularized KL-divergence for well-defined function-space vari-
ational inference in Bayesian neural networks, 2024. arXiv:2406.04317.
33Bunker, Girolami, Lambley, Stuart, and Sullivan
S. L. Cotter, M. Dashti, and A. M. Stuart. Approximation of Bayesian inverse problems
for PDEs. SIAM J. Numer. Anal., 48(1):322–345, 2010. doi:10.1137/090770734.
S. L. Cotter, G. O. Roberts, A. M. Stuart, and D. White. MCMC methods for func-
tions: Modifying old algorithms to make them faster. Statistical Science, 28(3), 2013.
doi:10.1214/13-STS421.
W.M.Czarnecki,S.Osindero,M.Jaderberg,G.Swirszcz,andR.Pascanu. Sobolevtraining
for neural networks. In I. Guyon, U. von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 30, pages 4278–4287, 2017. ISBN 9781510860964. arXiv:1706.04859.
M. Dashti and A. M. Stuart. The Bayesian approach to inverse problems. In Handbook
of Uncertainty Quantification. Vol. 1, 2, 3, chapter 7, pages 311–428. Springer, Cham,
2017. doi:10.1007/978-3-319-12385-1 7.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirec-
tional transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio,
editors, Proceedings of the 2019 Conference of the North American Chapter of the Associ-
ationforComputationalLinguistics: HumanLanguageTechnologies, Volume1(Longand
Short Papers), pages 4171–4186, Minneapolis, MN, 2019. Association for Computational
Linguistics. doi:10.18653/v1/N19-1423.
W. E, W. Ren, and E. Vanden-Eijnden. Minimum action method for the study of rare
events. Comm. Pure Appl. Math., 57(5):637–656, 2004. doi:10.1002/cpa.20005.
H. Edwards and A. Storkey. Towards a neural statistician. In The Fifth International
Conference on Learning Representations (ICLR 2017), 2017. arXiv:1606.02185.
L. Evans. Partial Differential Equations, volume 19 of Graduate Studies in Mathematics.
American Mathematical Society, 2nd edition, 2010. doi:10.1090/gsm/019.
R. A. Freeze and J. A. Cherry. Groundwater. Prentice-Hall, Englewood Cliffs, NJ, 1979.
ISBN 0-13-365312-9.
S. Ghosal and A. van der Vaart. Fundamentals of Nonparametric Bayesian Inference.
Cambridge University Press, first edition, 2017. doi:10.1017/9781139029834.
P. Ghosh, M. S. M. Sajjadi, A. Vergari, M. Black, and B. Sch¨olkopf. From variational to
deterministic autoencoders. In The Eighth International Conference on Learning Repre-
sentations (ICLR 2020), 2020. arXiv:1903.12436.
P. Hagemann, L. Ruthotto, G. Steidl, and N. T. Yang. Multilevel diffusion: Infinite dimen-
sional score-based diffusion models for image generation, 2023. arXiv:2303.04772.
M. Hairer, A. Stuart, and J. Voss. Signal processing problems on function space: Bayesian
formulation, stochastic PDEs and effective MCMC methods. In The Oxford Handbook
of Nonlinear Filtering, pages 833–873. Oxford University Press, Oxford, 2011. ISBN
9780199532902.
34Autoencoders in Function Space
K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, and R. Girshick. Masked autoencoders are
scalable vision learners. In 2022 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 15979–15988, New Orleans, LA, USA, 2022. IEEE.
doi:10.1109/CVPR52688.2022.01553. arXiv:2111.06377.
D.HendrycksandK.Gimpel. Gaussianerrorlinearunits(GELUs),2016. arXiv:1606.08415.
J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Infor-
mation Processing Systems, volume 33, pages 6840–6851. Curran Associates, Inc., 2020.
ISBN 9781713829546. arXiv:2006.11239.
D. Z. Huang, N. H. Nelsen, and M. Trautner. An operator learning perspective on
parameter-to-observable maps, 2024. arXiv:2402.06031.
B.E.HusicandV.S.Pande. Markovstatemodels: Fromanarttoascience. J. Am. Chem.
Soc., 140(7):2386–2396, 2018. doi:10.1021/jacs.7b12191.
J. B. Keller. Darcy’s law for flow in porous media and the two-space method. In Non-
linear Partial Differential Equations in Engineering and Applied Science, pages 429–443.
Routledge, 1980. doi:10.1201/9780203745465-27.
G. Kerrigan, J. Ley, and P. Smyth. Diffusion generative models in infinite dimensions.
In F. Ruiz, J. Dy, and J.-W. van de Meent, editors, Proceedings of the 26th Interna-
tional Conference on Artificial Intelligence and Statistics (AISTATS 2023), volume 206
of Proceedings of Machine Learning Research, pages 9538–9563, 2023. arXiv:2212.00886.
J. Kim, J. Yoo, J. Lee, and S. Hong. SetVAE: Learning hierarchical composition
for generative modeling of set-structured data. In 2021 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pages 15054–15063. IEEE, 2021.
doi:10.1109/CVPR46437.2021.01481.
D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In J. Bengio
and Y. LeCun, editors, The Third International Conference on Learning Representations
(ICLR 2015), 2015. arXiv:1412.6980.
D.P.KingmaandM.Welling.Auto-encodingvariationalBayes.InY.BengioandY.LeCun,
editors, The Second International Conference on Learning Representations (ICLR 2014),
2014. arXiv:1312.6114.
D. Kochkov, J. A. Smith, A. Alieva, Q. Wang, M. P. Brenner, and S. Hoyer. Machine
learning–accelerated computational fluid dynamics. Proc. Nat. Acad. Sci. USA, 118(21):
e2101784118, 8pp., 2021. doi:10.1073/pnas.2101784118.
K. A. Konovalov, I. C. Unarta, S. Cao, E. C. Goonetilleke, and X. Huang. Markov state
models to study the functional dynamics of proteins in the wake of machine learning. J.
Amer. Chem. Soc. Au, 1(9):1330–1341, 2021. doi:10.1021/jacsau.1c00254.
35Bunker, Girolami, Lambley, Stuart, and Sullivan
N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and A. Anand-
kumar. Neural operator: Learning maps between function spaces with applications to
PDEs. J. Mach. Learn. Res., 23:1–97, 2023. arXiv:2108.08481.
S. G. Krein and Yu. I. Petunin. Scales of Banach spaces. Russ. Math. Surv., 21(2):85–159,
1966. doi:10.1070/RM1966v021n02ABEH004151.
S. Lanthaler, A. M. Stuart, and M. Trautner. Discretization error of Fourier neural opera-
tors, 2024. arXiv:2405.02221.
S. Lee. Mesh-independent operator learning for partial differential equations. In 2nd
AI4Science Workshop at the 39th International Conference on Machine Learning, 2022.
URL https://openreview.net/pdf?id=JUtZG8-2vGp.
J. Li, Z. Pei, W. Li, G. Gao, L. Wang, Y. Wang, and T. Zeng. A systematic survey of deep
learning-based single-image super-resolution. ACM Comput. Surv., 56(10):1–40, 2024.
doi:10.1145/3659100.
Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen. PointCNN: Convolution on -
X
transformed points. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Sys-
tems, volume 31, pages 820–830. Curran Associates, Inc., 2018. ISBN 9781510884472.
arXiv:1801.07791.
Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anand-
kumar. Fourier neural operator for parametric partial differential equations. In
The Ninth International Conference on Learning Representations (ICLR 2021), 2021.
arXiv:2010.08895.
J. H. Lim, N. B. Kovachki, R. Baptista, C. Beckham, K. Azizzadenesheli, J. Kossaifi,
V. Voleti, J. Song, K. Kreis, J. Kautz, C. Pal, A. Vahdat, and A. Anandkumar. Score-
based diffusion models in function space, 2023. arXiv:2302.07400.
R. S. Liptser and A. N. Shiryaev. Statistics of Random Processes. Springer, Berlin, Heidel-
berg, 2001. doi:10.1007/978-3-662-13043-8.
L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators
via DeepONet based on the universal approximation theorem of operators. Nat. Mach.
Intell., 3(3):218–229, 2021. doi:10.1038/s42256-021-00302-5.
A. Mardt, L. Pasquali, H. Wu, and F. No´e. VAMPnets for deep learning of molecular
kinetics. Nat. Commun., 9(1):5, 11pp., 2018. doi:10.1038/s41467-017-02388-1.
F.Nu¨ske, H.Wu, J.-H.Prinz, C.Wehmeyer, C.Clementi, andF.No´e. Markovstatemodels
from short non-equilibrium simulations–analysis and correction of estimation bias. J.
Chem. Phys., 146(9):094104, 16pp., 2017. doi:10.1063/1.4976518.
B. Øksendal. Stochastic Differential Equations. Universitext. Springer, Berlin, Heidelberg,
2003. doi:10.1007/978-3-642-14394-6.
36Autoencoders in Function Space
B. Peherstorfer. Breaking the Kolmogorov barrier with nonlinear model reduction. Not.
Am. Math. Soc., 69(5):725–733, 2022. doi:10.1090/noti2475.
J. Pidstrigach, Y. Marzouk, S. Reich, and S. Wang. Infinite-dimensional diffusion models
for function spaces, 2023. arXiv:2302.10130.
M. Prasthofer, T. De Ryck, and S. Mishra. Variable-input deep operator networks, 2022.
arXiv:2205.11404.
J.-H. Prinz, H. Wu, M. Sarich, B. Keller, M. Senne, M. Held, J. D. Chodera, C. Schu¨tte,
and F. No´e. Markov models of molecular kinetics: Generation and validation. J. Chem.
Phys., 134(17):174105, 23pp., 2011. doi:10.1063/1.3565032.
C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep learning on point sets for
3D classification and segmentation. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 77–85. IEEE, 2017. doi:10.1109/CVPR.2017.16.
W. Quan, J. Chen, Y. Liu, D.-M. Yan, and P. Wonka. Deep learning-based image and video
inpainting: A survey. Int. J. Comput. Vis., 132:2364–2400, 2024. doi:10.1007/s11263-
023-01977-6.
M. A. Rahman, M. A. Florez, A. Anandkumar, Z. E. Ross, and K. Azizzadenesheli. Gener-
ative adversarial neural operators. Transact. Mach. Learn. Res., 2022. arXiv:2205.03017.
J. O. Ramsay and B. W. Silverman, editors. Applied Functional Data Analysis: Methods
and Case Studies. Springer Series in Statistics. Springer, 2002. doi:10.1007/b98886.
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image
synthesis with latent diffusion models. In 2022 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 10674–10685, New Orleans, LA, USA,
2022. IEEE. doi:10.1109/CVPR52688.2022.01042.
S. S¨arkk¨a and A. Solin. Applied Stochastic Differential Equations. Cambridge University
Press, first edition, 2019. doi:10.1017/9781108186735.
T. Schlick. Molecular Modeling and Simulation: An Interdisciplinary Guide, volume 21
of Interdisciplinary Applied Mathematics. Springer, New York, second edition, 2010.
doi:10.1007/978-1-4419-6351-2.
D. W. Scott. Multivariate Density Estimation. Wiley Series in Probability and Statistics.
John Wiley and Sons, second edition, 2015. doi:10.1002/9781118575574.
J. H. Seidman, G. Kissas, P. Perdikaris, and G. J. Pappas. NOMAD: Nonlinear manifold
decoders for operator learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, vol-
ume 35, pages 5601–5613. Curran Associates, Inc., 2022. ISBN 9781713871088.
J. H. Seidman, G. Kissas, G. J. Pappas, and P. Perdikaris. Variational autoencoding neural
operators. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett,
editors, Proceedings of the 40th International Conference on Machine Learning (ICML
37Bunker, Girolami, Lambley, Stuart, and Sullivan
2023), volume 202 of Proceedings of Machine Learning Research, pages 30491–30522,
2023. arXiv:2302.10351.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. In J. Bengio and Y. LeCun, editors, The Third International Conference on
Learning Representations (ICLR 2015), 2015. arXiv:1409.1556.
V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein. Implicit
neural representations with periodic activation functions. In H. Larochelle, M. Ran-
zato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 7462–7473. Curran Associates, Inc., 2020. ISBN
9781713829546. arXiv:2006.09661.
J.Sohl-Dickstein,E.Weiss,N.Maheswaranathan,andS.Ganguli. Deepunsupervisedlearn-
ing using nonequilibrium thermodynamics. In F. Bach and D. Blei, editors, Proceedings
of the 32nd International Conference on Machine Learning (ICML 2015), volume 37 of
Proceedings of Machine Learning Research, pages 2256–2265, 2015. arXiv:1503.03585.
Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based
generativemodelingthroughstochasticdifferentialequations. InThe Ninth International
Conference on Learning Representations (ICLR 2021), 2021. arXiv:2011.13456.
A. M. Stuart. Inverse problems: A Bayesian perspective. Acta Numer., 19:451–559, 2010.
doi:10.1017/S0962492910000061.
V. N. Sudakov. Linear sets with quasi-invariant measure. Dokl. Akad. Nauk SSSR, 127:
524–525, 1959.
S.Sun,G.Zhang,J.Shi,andR.Grosse. FunctionalvariationalBayesianneuralnetworks. In
The Seventh International Conference on Learning Representations (ICLR 2019), 2019.
arXiv:1903.05779.
M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal,
R. Ramamoorthi, J. T. Barron, and R. Ng. Fourier features let networks learn high
frequency functions in low dimensional domains. In H. Larochelle, M. Ranzato, R. Had-
sell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Sys-
tems, volume 33, pages 7537–7547. Curran Associates, Inc., 2020. ISBN 9781713829546.
arXiv:2006.10739.
A. Vahdat, K. Kreis, and J. Kautz. Score-based generative modeling in latent space. In
M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, edi-
tors, Advances in Neural Information Processing Systems, volume34, pages11287–11302.
Curran Associates, Inc., 2021. ISBN 9781713845393. arXiv:2106.05931.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L(cid:32) . Kaiser, and
I. Polosukhin. Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. Wal-
lach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems, volume 30, pages 5998–6008, 2017. ISBN 9781510860964.
38Autoencoders in Function Space
M. Yang, B. Dai, H. Dai, and D. Schuurmans. Energy-based processes for exchangeable
data. In H. Daum´e III and A. Singh, editors, Proceedings of the 37th International
Conference on Machine Learning (ICML 2020), volume 119 of Proceedings of Machine
Learning Research, pages 10681–10692, 2020. arXiv:2003.07521.
M.Zaheer,S.Kottur,S.Ravanbhakhsh,B.P´oczos,R.Salakhutdinov,andA.J.Smola.Deep
sets. In I. Guyon, U. von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30,
pages 3391–3401, 2017. ISBN 9781510860964. arXiv:1703.06114.
B. Zhang and P. Wonka. Functional diffusion. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR), pages 4723–4732, 2024.
arXiv:2311.15435.
39Bunker, Girolami, Lambley, Stuart, and Sullivan
Appendix A. Supporting Results
A.1 Supporting Results for Section 2
Proof of Lemma 2.1 First, suppose that P (A) = (cid:82) P (A)P (dz) = 0. Then, P -
u Z u|z z z
almost surely, P (A) = 0, and since Υ P for all z , we conclude that Υ(A) = 0,
u|z u|z
≪ ∈ Z
i.e., Υ P . The fact that Q P then follows from the factorisations (2.6)–(2.8), as
u z,u z,u
≪ ≪
does the expression for the joint density.
A useful tool in deriving the ELBO (2.15) is the data-processing inequality for
KL divergence (see, e.g., Burt et al., 2021, Proposition 1): given measurable spaces
U
and , probability measures µ and ν on , and a measurable map g: , we have
V U U → V
D (g µ g ν) ⩽ D (µ ν).
KL ♯ ♯ KL
∥ ∥
Proof of Theorem 2.2 By the data-processing inequality, since D (Q P ) < , it
KL z,u z,u
∥ ∞
follows that
D (Υ P ) = D (πUQ πUP ) ⩽ D (Q P ) < . (A.1)
KL u KL ♯ z,u ♯ z,u KL z,u z,u
∥ ∥ ∥ ∞
Thus, using the joint density from Lemma 2.1,
D (Q P ) = E E
(cid:20)
log dΥ (u)+log
dQ
z|u
(z;u)(cid:21)
KL z,u ∥ z,u u∼Υz∼Q
z|u
dP
u
dP
z|u
= D (Υ P )+ E (cid:2) D (Q P )(cid:3) ,
KL u KL z|u z|u
∥ u∼Υ ∥
and all terms are finite as a consequence of (A.1).
Proof of Theorem 2.3 Again using Lemma 2.1, along with the fact that Υ P P
u|z η
≪ ≪
for all z , we see that
∈ Z
D (Q P ) = E E
(cid:20)
log dΥ (u;z)+log
dQ
z|u
(z;u)(cid:21)
KL z,u ∥ z,u u∼Υz∼Q
z|u
dP
u|z
dP
z
= E E
(cid:20)
log dΥ (u) log
dP
u|z (u;z)+log
dQ
z|u
(z;u)(cid:21)
u∼Υz∼Q
z|u
dP
η
− dP
η
dP
z
(cid:20) (cid:20) dP (cid:21) (cid:21)
= D (Υ P )+ E E log u|z (u;z) +D (Q P ) .
KL ∥ η u∼Υ z∼Q z|u − dP η KL z|u ∥ z
Notethat,sinceQ
z|u
= N(m(u;θ f),Σ(u;θ f))andP
z
= N(0,I dZ)areGaussianon
Z
= RdZ,
the KL divergence D (Q P ) admits the analytical form
KL z|u z
∥
1 1 d 1
D (Q P ) = m(u;θ ) 2+ trΣ(u;θ ) Z logdetΣ(u;θ ) <
KL z|u z f 2 f f
∥ 2∥ ∥ 2 − 2 − 2 ∞
Finiteness of each term thus follows from the hypotheses that D (Q P ) < and
KL z,u z,u
∥ ∞
D (Υ P ) < , and the fact that D (Q P ) < .
KL η KL z|u z
∥ ∞ ∥ ∞
Lemma A.1 Suppose that = C ([0,T],Rm) and that µ P( ) and ν P( ) are the
0
U ∈ U ∈ U
laws of the Rm-valued diffusions
du = b(u )dt+√εdw , u = 0, t [0,T]
t t t 0
∈
dv = c(v )dt+√εdw , u = 0, t [0,T].
t t t 0
∈
40Autoencoders in Function Space
where w is a Brownian motion on Rm. Suppose that the Novikov condition (e.g., Øksendal,
t
2003, eq. (8.6.8)) holds for both processes, i.e.,
(cid:20) 1 (cid:90) T (cid:21) (cid:20) 1 (cid:90) T (cid:21)
E b(u ) 2dt < and E c(v ) 2dt < .
t 2 t 2
u∼µ 2ε ∥ ∥ ∞ v∼ν 2ε ∥ ∥ ∞
0 0
Then
(cid:20) 1 (cid:90) T (cid:21)
D (µ ν) = E b(u ) c(u ) 2dt .
KL t t 2
∥ u∼µ 2ε ∥ − ∥
0
Proof As the Novikov condition is satisfied, µ ν and the Girsanov formula yields
∼
dµ (cid:18) 1 (cid:90) T 1 (cid:90) T (cid:19)
(u) = exp b(u ) c(u ) 2dt b(u ) c(u )dW , (A.2)
t t 2 t t t
dν 2ε ∥ − ∥ − ε −
0 0
(cid:82)t
where W = w b(u )ds. Thus, using (A.2) in the definition of the KL divergence,
t t − 0 s
(cid:20) 1 (cid:90) T 1 (cid:90) T (cid:21)
D (µ ν) = E b(u ) c(u ) 2dt b(u ) c(u )dW .
KL t t 2 t t t
∥ u∼µ 2ε ∥ − ∥ − ε −
0 0
Theresultfollowsasthestochasticintegralisamartingalewithzeroexpectationunderµ.
We point out that the Novikov condition in Lemma A.1 is merely used as a convenient
sufficient condition to apply the Girsanov theorem, and more general conditions such as the
Kazamaki condition (Liptser and Shiryaev, 2001, p. 249) could also be applied.
A.2 Supporting Results for Section 4
ProofofProposition4.1 Thisfollowsimmediatelyfromevaluating withparameters
FAE
J
θ and θ such that f(u;θ ) = 0 and g(z;θ ) = 0, since
f g f g
(cid:20) (cid:21)
1
(u;θ ,θ ,β) = E u 2 ,
JFAE f g u∼Υ 2 ∥ ∥U
and the expectation is finite by hypothesis.
Appendix B. Experimental Details
In this section, we provide additional details, training configurations, and analysis for the
numerical experiments in Section 2.5 and Section 4.3. We also provide further samples and
reconstructions from our learned FVAE and FAE models.
B.1 Base Architecture
WeusethecommonarchitecturedescribedinSection2.4andSection4.2forallexperiments.
41Bunker, Girolami, Lambley, Stuart, and Sullivan
Positional encodings. In each case, both the encoder and decoder may make use of
Gaussian random Fourier features (Tancik et al., 2020), pairing the query coordinate x
∈
Ω Rd with a positional encoding γ(x) R2k. To generate these encodings, a matrix
⊂ ∈
B Rk×d with independent N(0,I) entries is sampled and viewed as a hyperparameter of
∈
the model to be used in both the encoder and decoder. The positional encoding γ(x) is
then given by the concatenated vector
γ(x) = (cid:2) cos(2πBx);sin(2πBx)(cid:3)T R2k,
∈
where the sine and cosine functions are applied componentwise to the vector 2πBx.
Optimiser. For all experiments we use the Adam optimiser (Kingma and Ba, 2015) with
the default hyperparameters.
B.2 Brownian Dynamics
The training data consists of 8,192 samples from the path measure Υ of the SDE on the
time interval [0,T], T = 5, at timestep 5/512, with 50% of timesteps assumed to be missing
in each sample. Trajectories are generated using the Euler–Maruyama scheme with internal
timestep 1/8192, and the resulting paths are then subsampled by a factor of 80 to obtain the
training data.
Experimental setup. We train for 50,000 steps using an initial learning rate of 10−3
with an exponential decay of 0.98 being applied every 1,000 steps. We use a batch size of 32
and take 4 Monte Carlo samples for Q . We use the same base architecture described in
z|u
Appendix B.1 but with a a latent dimension d = 1, β = 1 and λ = 10. The three models
Z
in Figure 2.2 use θ = 0, θ = 25 and θ = 10,000, respectively.
B.3 Estimation of Markov State Models
Surface of Potential Heatmap of Potential
0.30
0.4
0.15
0.2 0.2 0.00
0.0
0.2 0.15
0.4 0.0
0.30
0.6
0.2 0.45
0.4
0.2
0.4 0.0 0.60
0.2 0.2 x2 0.4
0.0
0.2 0.4
x 1 0.4 0.75
0.4 0.2 0.0 0.2 0.4
x1
Figure B.1: Potential function U for Section 2.5.2, given by (B.1).
42
2x
laitnetoPAutoencoders in Function Space
Data and discretisation. To validate the ability of FVAE to model higher-dimensional
SDE trajectories, we specify a simple-to-visualise potential with qualitative features similar
to those arising in the more complex potential surfaces arising in MD. To this end, define
the centres c = (0,0), c = (0.2,0.2), c = ( 0.2, 0.2), c = (0.2, 0.2), c = (0,0.2) and
1 2 3 4 5
− − −
c = ( 0.2,0); standard deviations σ = σ = σ = σ = 0.1 and σ = σ = 0.03; and
6 1 2 3 4 5 6
−
masses m = m = m = m = 0.1 and m = m = 0.01. Then let
1 2 3 4 5 6
(cid:34) 6 (cid:35)
U(x) = 0.3 0.5(x +x )+x2+x2 (cid:88) m N (cid:0) x;c ,σ2I (cid:1) . (B.1)
1 2 1 2 i i i 2
−
i=1
The potential (B.1) has three key components: a linear term breaking the symmetry, a
quadratic term preventing paths from veering too far from the origin (i.e., the path’s start-
ing point), and negative Gaussian densities—serving as potential wells—positioned at the
centres c (Figure B.1). Sample paths with initial condition u = 0, temperature ε = 0.1
i 0
and final time T = 3 show significant diversity, with many paths transitioning at least once
between different wells (Figure B.3).
Experimental setup. The training set consists of 16,384 sample paths using an Euler–
Maruyama scheme with timestep 1/8,192, subsampling by a factor 48 to obtain an equally
spacedmeshof513points. WeadoptthebasearchitecturedescribedinAppendixB.1, with
the SDE objective (Section 2.3.1) using latent dimension d = 16, β = 0.02, θ = 100, and
Z
λ = 50. We implement the same training strategy as described in Appendix B.3 by training
on data where 50% of the points on the path are missing. We also use the same learning
rate, learning rate decay schedule, maximum number of steps, and batch size.
Results. FVAE produces convincing samples and reconstructions (Figures B.2 and B.3).
Figure B.2: Reconstructions of sample paths from the SDE in Section 2.5.2.
43Bunker, Girolami, Lambley, Stuart, and Sullivan
(a) FVAE
(b) Dataset
Figure B.3: Samples of the SDE in Section 2.5.2, where the evolution through time t [0,3]
∈
is depicted as a transition in colour from blue to green.
44Autoencoders in Function Space
B.4 Dirac Distributions
Data and discretisation. We view the data measure Υ as a probability measure on
the space = H−1((0,1)). At each resolution, we discretise the domain (0,1) using an
U
evenly spaced mesh of points and approximate the Dirac mass δ , θ (0,1), by the optimal
θ
∈
L1-approximation: a discretised function which is zero except at the mesh point closest
to θ, normalised to have unit L1-norm. The training dataset consists of a Dirac function
centred at each mesh point; we emphasise the goal is not to train a practical model for
generalisation, but to isolate the effect of the population loss.
Experimental setup. We train both the FVAE model and the FAE model at resolutions
8, 16, 32, 64, and 128. For both models, we train for 30,000 steps with a batch size of 6.
At each resolution, we perform 50 runs using different random seeds.
Architecture. Toisolatetheeffectoftheill-definedELBO,andtoavoidissuesassociated
withoperatingonfunctionsofnegativeSobolevregularity,wedefineanencoderanddecoder
architecture tailored to this problem. As in Section 2.4.2, we restrict to the case that the
covariance matrix learned by the encoder is diagonal, and take
(cid:32) (cid:33)
f˜(u;θ ) = ρ argmax(φ u)(x);θ ,
f f
∗
x∈[0,1]
where ρ: R Θ R R is a neural network with 3 hidden layers of width 128 and
f
× → ×
φ: R R is some compactly supported smooth mollifier, chosen such that φ u is smooth
→ ∗
and the maximum is well-defined. The decoder is chosen to return the Gaussian density
(cid:16) (cid:17)
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)2
g z;θ (x) = N µ z;θ ,σ z;θ ;x ,
g g g
withlearnablemeanµ(z;θ )andstandarddeviationσ(z;θ )computedfroma3-layerneural
g g
network of width 128. For numerical stability, we impose a lower bound on σ based on the
mesh spacing ∆x, given by σ (∆x) := (2π)−1/2∆x.
min
FVAE configuration. We take decoder noise distribution P = N(0,I) on L2((0,1)),
η
withP P(H−s((0,1))), s < 1/2. WemodifytheFVAEobjective(2.13)byreweighting
η
∈ −
the term D (Q P ) by β = 10−4, and take 16 Monte Carlo samples for Q . We use
KL z|u z z|u
∥
an initial learning rate of 10−4, decaying exponentially by a factor 0.7 every 1,000 steps.
FAE configuration. For the FAE objective (4.1), we discretise the H−1-norm, viewed
as a weighted sum of frequencies (see, e.g., Krein and Petunin, 1966, Sec. 9), as
n
u 2 (cid:88)(cid:0) 1+j2(cid:1)−1 α 2, u = (cid:88) α e , e (x) = √2sin(πjx),
∥
∥H−1((0,1))
≈ |
j
|
j j j
j=1 j∈N
withthecoefficientsα calculatedfromadiscretisationofuusingthediscretesinetransform.
i
We use an initial learning rate of 10−4 decaying exponentially by a factor 0.9 every 1,000
steps, and take β = 10−12.
45Bunker, Girolami, Lambley, Stuart, and Sullivan
(a) Schematic diagram of (b) FAE loss (c) FVAE & VANO loss
Dirac experiment =H−1((0,1)) P =N(0,I) on L2((0,1))
Trainingloss(ours) η Trainingloss(VANO)
U
0.025
9 d ofis Dcr ie rt ai csa δt θion 0
0.020
learnable 10
meanµ −
0.015
20
−
0.010 30
learnable reconstruction −
SDσ g(z;θg)
0.005 −40
50
−
0 θ 1 816 32 64 128 816 32 64 128
x ReRseosolluuttioinon RReessoolluutitonion
Figure B.4: (a): The discrete representations (squares) of δ and g(z;θ ) on a grid of 8
θ g
points. (b) and (c): By discretising the two losses at an increasing sequence of resolutions,
we see that the FAE loss rapidly converges, whereas a doubling of resolution approximately
leads to a doubling of the FVAE loss, suggesting the continuum limit is infinite. Each dot
in (b) and (c) represents one of 50 training runs with distinct random seed, with the red
line connecting the medians.
Results. As expected, the final training loss under both models decreases as the reso-
lution is refined, since the lower bound σ decreases. The striking difference, however,
min
is that the FAE loss appears to converge and is stable across runs, while the FVAE loss
appearsto diverge and becomesincreasingly unstable across runs. This provides convincing
empirical evidence that the underlying population loss for FVAE is not defined as a result
of the misspecified decoder noise; the use of FAE with an appropriate data norm alleviates
this issue. Since the FVAE objective with P = N(0,I) coincides with the VANO objective,
η
this issue would also be present for VANO. We point out that, under both models, train-
ing becomes increasingly unstable at high resolutions: when σ is small, the loss becomes
extremely sensitive to changes in µ; this instability is unrelated to the divergence of the
FVAE training loss and is instead a consequence of training through gradient descent.
B.5 Navier–Stokes Equations
Viscosity ν Resolution # Train # Eval. Snapshot Time T
10−3 64 64 4,000 1,000 50
×
10−4 64 64 8,000 2,000 50
×
10−5 64 64 960 240 20
×
Table B.1: Details of Navier–Stokes datasets.
Data and discretisation. The data used here was provided online by Li et al. (2021).
Solutions from (4.2) are generated numerically by first sampling the initial condition from
46Autoencoders in Function Space
the Gaussian random field N(0,C), C = 73/2(49I ∆)−5/2 and then performing the time
−
evolution using a pseudospectral method, as described by Li et al. (2021).
While the data of Li et al. (2021) includes the full time evolution, we use only snapshots
of the solution vorticity field at the final timestep. Every snapshot is a 64 64 image,
×
normalised to take values in [0,1] using the minimum and maximum across all the data.
The details of this dataset can be found in Table B.1.
Effectsofpointratios. AsdescribedinSection2.4.4,theself-supervisedtrainingscheme
we adopt involves masking a fixed proportion of randomly selected points before encoding,
and then decoding only on the masked mesh points. We call the proportion of points
available to the encoder the (encoder) point ratio, which serves as an additional hyper-
parameter of FVAE and FAE. Here, we extend the analysis of Figure 4.2(b) for FAE to
understand how the point ratio during training affects the reconstruction performance.
We first train two FAE models on the Navier–Stokes dataset with viscosity ν = 10−4,
using encoder point ratio 10% and 90% respectively. Selecting an arbitrary sample from the
held-out set, we draw 1,000 distinct meshes with specified point ratio, evaluate the encoder
on each of these meshes, and then decode on the full mesh, using the reconstruction MSE
on the full grid to evaluate performance. Using the same procedure as in Section 2.5.1, we
construct a kernel density estimate of the resulting distribution of reconstruction MSEs,
shown in Figure B.5 for both models.
We find that the model trained with encoder point ratio 10% is much more sensitive to
thelocationoftheevaluationmeshpoints, especiallywhentheevaluationpointratioislow.
With sufficiently high encoder point ratio at evaluation time, however, the reconstruction
MSE of the model trained using point ratio 10% surpasses that of the model trained at
90%. This suggests a tradeoff whereby a higher training point ratio provides more stability,
with the price to be paid being an increase in autoencoding MSE, particular when the point
ratio of the evaluation data is high. We hypothesise that a lower training ratio implicitly
regularises the model to attain a more robust internal representation.
20000 60000 Point%(train)
50000 10
30
15000
40000 50
70
10000 30000 90
20000
5000
10000
0 0
0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030
MSE MSE
(a) For a model trained with point ratio 10%. (b) For a model trained with a point ratio 90%.
Figure B.5: Kernel density estimates for full-grid reconstruction MSE on the reference
sample across 1,000 randomly chosen meshes. Training with a low point ratio regularises,
reducing MSE when the evaluation data has a high point ratio, but at the cost of greater
variance when evaluating on low point ratios.
We also investigate the sensitivity of the models to a specific encoder mesh, seeking to
understandwhetheranencodermeshachievinglowMSEononeimageleadstolowMSEon
47
ytisneD ytisneDBunker, Girolami, Lambley, Stuart, and Sullivan
other images. The procedure is as follows: we select an image arbitrarily from the held-out
set (the reference sample) and draw 1,000 random meshes with point ratio 10%; then, we
select the mesh resulting in the lowest reconstruction MSE for each of the two models. For
the nearest neighbours of the chosen sample in the held-out set, the reconstruction error on
this MSE-minimising mesh is lower than average (Figure B.6(a); dashed lines), suggesting
that a good configuration will yield good results on similar samples. On the other hand,
using the MSE-minimising mesh on arbitrary samples from the held-out set yields an MSE
somewhat lower than a randomly chosen mesh; unsurprisingly, however, the arbitrarily
chosen samples appear to benefit less than the nearest neighbours (Figure B.6(b)).
idx=1 idx=644 idx=1208 idx=1930 idx=703
4000 3000
High
4000 3000 34 00 00 00 34 00 00 00 2000 Low
2000
2000 2000
2000 1000
1000 1000 1000
0 0 0 0 0
0.001 0.002 0.003 0.001 0.002 0.003 0.001 0.002 0.003 0.001 0.002 0.003 0.001 0.002 0.003
(a) Nearest neighbours of the reference sample in the held-out set.
idx=105 idx=1088 idx=999 idx=1705 idx=515
3000 6000 6000 10000 4000 H Loig wh
7500 2000 4000 4000
5000 2000
1000 2000 2000 2500
0 0 0 0 0
0.001 0.002 0.003 0.001 0.002 0.003 0.001 0.002 0.001 0.002 0.002 0.004
(b) Arbitrarily chosen samples from the held-out set.
Figure B.6: Kernel density estimates of full-grid reconstruction MSE for models trained at
10% (Low) and 90% (High) point ratios on samples from the held-out set. Dashed lines
indicate the MSE obtained using the mesh minimising MSE on the reference sample.
Experimental setup. We train for 50,000 steps using a batch size of 32. An initial
learning rate of 10−3 is used with an exponential decay factor of 0.98 applied every 1000
steps. At every step, we randomly sample 30% of the points to be fed to the encoder. The
decoder and loss are evaluated on the complement (i.e., 70%) of these points. We have
found that this ratio provides a good balance of robustness to different masks and overall
performance.
48
ytisneD
ytisneDAutoencoders in Function Space
Architecture. For the sake of comparison, we define a standard CNN-based FAE follow-
ing the VGG-inspired methodology (Simonyan and Zisserman, 2015) of gradually contract-
ing/expanding the feature map while increasing/decreasing the channel dimensions. The
architecture is adapted to have a similar parameter count to our baseline FAE model. Both
of these models use Gaussian random positional encodings with k = 16.
The encoder consists of four CNN layers with output channel dimension 4, 4, 8, and 16
respectively and kernel sizes are 2, 2, 4, and 4 respectively, all with stride 2. The result is
flattened and passed through a single-hidden-layer MLP of width 64 to obtain a vector of
dimension64. Thedecoderconsistsofasingle-layerMLPofwidth64andoutputdimension
512, which is then rearranged to a 4 4 feature map with channel size 32. This feature
×
map is then passed through four layers of transposed convolutions that respectively map to
16, 8, 4, and 4 channel dimensions, with kernel sizes 4, 4, 2, and 2 respectively, and stride
2. The result is then mapped by two CNN layers with kernel size 3, stride 1, and output
channel dimension 8 and 1 respectively.
Uncurated samples. As described in Section 4.3.2, we apply FAE as a generative model
by fitting a Gaussian mixture with 10 components on the latent space. Samples from the
generative model trained at at viscosity ν = 10−3, ν = 10−4 and ν = 10−5 are provided in
Figures B.7, B.8, and B.9 respectively.
Uncurated reconstructions. Reconstructions from randomly selected data from the
held-out sets for viscosities ν = 10−3, ν = 10−4 and ν = 10−5 are provided in Figures B.10,
B.11, and B.12 respectively.
Evaluation at very high resolutions. In Figure 4.4, we demonstrate zero-shot resolu-
tion by evaluating the decoder on grids of resolution 2,048 2,048 and 32,768 32,768. We
× ×
pointoutthat, whiletheformerrequiresapproximately16MBtostoreusing32-bitfloating
point numbers, the latter requires 4.3 GB, vastly exceeding the available memory of typical
consumer GPUs. To allow evaluation of the decoder at this resolution, we partition the
domain into 1,000 chunks and evaluate the decoder on each chunk in turn and reassemble
the resulting data in the RAM. To ensure that each chunk has an integer number of points,
we take the first 824 chunks to contain 1,073,742 mesh points (approximately 4 MB), and
take the remaining 176 chunks to contain 1,073,741 points.
49Bunker, Girolami, Lambley, Stuart, and Sullivan
(a) FAE
(b) Dataset
Figure B.7: Samples of Navier–Stokes data with viscosity ν = 10−3.
50Autoencoders in Function Space
(a) FAE
(b) Dataset
Figure B.8: Samples of Navier–Stokes data with viscosity ν = 10−4.
51Bunker, Girolami, Lambley, Stuart, and Sullivan
(a) FAE
(b) Dataset
Figure B.9: Samples of Navier–Stokes data with viscosity ν = 10−5.
52Autoencoders in Function Space
Original Masked Reconstructed Relative Diff. Original Masked Reconstructed Relative Diff.
Figure B.10: FAE reconstructions of Navier–Stokes data with viscosity 10−3.
Original Masked Reconstructed Relative Diff. Original Masked Reconstructed Relative Diff.
Figure B.11: FAE reconstructions of Navier–Stokes data with viscosity 10−4.
53Bunker, Girolami, Lambley, Stuart, and Sullivan
Original Masked Reconstructed Relative Diff. Original Masked Reconstructed Relative Diff.
Figure B.12: FAE reconstructions of Navier–Stokes data with viscosity ν = 10−5.
54Autoencoders in Function Space
B.6 Darcy Flow
Data and discretisation. The dataset used in Section 4.3.2 is based on that of Li et al.
(2021), provided on a 421 421 grid and generated through a finite-difference scheme.
×
Where described, we downsample this data to lower resolutions by applying a low-pass
filter in Fourier space and subsampling the resulting filtered image. The low-pass filter is a
mollification of an ideal sinc filter with bandwidth selected to eliminate frequencies beyond
theNyquistfrequencyofthetargetresolution, computedinpracticebyconvolvingtheideal
filter in Fourier space with a Gaussian kernel of with standard deviation σ = 0.1, truncated
to a 7 7 convolutional filter.
×
Experimental setup. We follow the same setup used for the Navier–Stokes dataset: we
train for 50,000 steps, with batch size 32 and random sampling of 30% of points in the
encoder, evaluating the decoder on the unseen mesh points. An initial learning rate of 10−3
is used with an exponential decay factor of 0.98 applied every 1000 steps. We also make use
of positional embeddings (Appendix B.1) using k = 16 Gaussian random Fourier features.
When performing the wall-clock training time experiment (Figure 4.6(a)), we downsam-
ple the training and evaluation data to resolution 211 211. For the zero-shot superres-
×
olution experiment (Figure 4.6(b)), we downsample training data, and the encoder inputs
during evaluation, to resolution 53 53. For the FAE generative model experiment, we
×
downsample training and evaluation data to resolution 47 47.
×
Uncurated reconstructions. Reconstructions of randomly selected examples from the
held-out evaluation dataset are shown in Figure B.13.
Original Masked Reconstructed Relative Diff. Original Masked Reconstructed Relative Diff.
Figure B.13: FAE reconstructions of Darcy flow data.
Uncurated samples. Samples from the FAE latent generative model and random draws
from the evaluation dataset are shown in Figure B.14.
55Bunker, Girolami, Lambley, Stuart, and Sullivan
(a) FAE
(b) Dataset
Figure B.14: Samples of Darcy flow data.
56