Transformers are Universal In-context Learners
Takashi Furuya Maarten V. de Hoop Gabriel Peyr´e
Shimane Univ. Rice Univ. CNRS, ENS, PSL Univ.
takashi.furuya0101@gmail.com mvd2@rice.edu gabriel.peyre@ens.fr
Abstract
Transformersaredeeparchitecturesthatdefine“in-contextmappings”whichenablepredicting
new tokens based on a given set of tokens (such as a prompt in NLP applications or a set of
patches for vision transformers). This work studies in particular the ability of thesearchitectures
tohandleanarbitrarilylargenumberofcontexttokens. Tomathematicallyanduniformlyaddress
the expressivity of these architectures, we consider the case that the mappings are conditioned
on a context represented by a probability distribution of tokens (discrete for a finite number of
tokens). The related notion of smoothness corresponds to continuity in terms of the Wasserstein
distance between these contexts. We demonstrate that deep transformers are universal and can
approximatecontinuousin-contextmappingstoarbitraryprecision,uniformlyovercompacttoken
domains. A key aspect of our results, compared to existing findings, is that for a fixed precision,
a single transformer can operate on an arbitrary (even infinite) number of tokens. Additionally,
it operates with a fixed embedding dimension of tokens (this dimension does not increase with
precision) and a fixed number of heads (proportional to the dimension). The use of MLP layers
between multi-head attention layers is also explicitly controlled.
1 Introduction
Transformers have revolutionized the field of machine learning with their powerful attention mecha-
nisms,asintroducedbyVaswanietal. [35]. Theexceptionalperformanceandexpressivityoflarge-scale
transformershavebeenempiricallywellestablishedforbothNLP[4]andvisionapplications[10]. One
key property of these architectures is their ability to leverage contexts of arbitrary length, which en-
ables the parameterization of “in context” mappings with an arbitrarily large complexity. In this
paper,we presenta rigorousformalismto modelinputs andthe associatedcontextwith anarbitrarily
large number of tokens, defining a notion of continuity that enables the analysis of their expressivity.
1.1 Previous work
Universality, from neural networks to neural operators. Multilayer Perceptrons(MLP) with
two layers are universal approximators,as shown decades ago in [8, 18], with a comprehensive review
in [29]. The significance of depth in enhancing expressivity is explored in [17, 39]. These results have
beenextendedtocoveravarietyofarchitecturalconstraintsonthenetworks,forinstance,usingweight
sharinginConvolutionalNeuralNetworks(CNN)[42]andskipconnexionsinResNets[7,34]. Itisalso
possible to design equivariant architectures, in particular for graph neural networks [23, 19, 38] and
neural networks operating on sets of points [30, 9]. The connection between transformers and graph
neural networks is exposed in [27]. We focus on this article from a related but different point of view,
viewing transformers as operating on probability distribution rather than sets of points. Related to
this setup are extensions of neural networks to infinite dimensional input functional spaces using the
concept of neural operator [20], which universality is studied in [14]. They can be generalizedto cope
with data in metric spaces, addressing topological obstructions, in [22].
Mathematical modeling of transformers. It is now customary to describe transformers as per-
forming“in context”prediction, whichmeans thatit maps tokento token,but that this mapdepends
onasetofpreviouslyseentokens. Thesizeofthiscontextmightbeverylong,possiblyarbitrarilylong,
which is the focus of this article. The ability of trained transformers to effectively perform in-context
1
4202
guA
2
]LC.sc[
1v76310.8042:viXracomputation has been supported by both empirical studies [36] and theoretical results [2, 25, 32, 41]
on simplified architectures (typically linear attention) and specific data generation processes.
To make a rigorous analysis of arbitrarily long token lengths, and also describe a “mean field”
limit of an infinite number of tokens, it is convenient to view attention as operating over probability
distributions of tokens [37, 31]. The smoothness (Lipschitz continuity) of these attention layers is
analyzed in [5]. Deep transformers (with the residual connection) can be described by a coupled
system of particles evolving across the layers. The analysis of the clustering properties of such an
evolution is studied in [15, 16].
Universality of transformers. [40] provides, to the best of our knowledge, the most detailed
account of the universality of transformers. The authors rely on shallow transformers with only 2
heads but require that the transformers operate over an embedding dimension which grows with the
number of tokens. This resultis refinedin [28] which highlights the difficulty of attentionmechanisms
to capture smooth functions. Our focus is different, since we consider deep transformers with a fixed
embedding dimension, but which are universal for an arbitrary number of tokens.
We note that there exist variations over the initial transformers architecture which enjoys univer-
sality results, for instance, the Sumformer [3] and stochastic deep netorks [9], which also requires an
embeddingdimensionthatgrowswiththenumberoftokens. Wealsomentionprobabilistictransform-
ers [21] which can approximate embeddings of metric spaces. The work of [1] provides an abstract
universal interpolation result for equivariant architectures under genericity conditions, but it is not
known whether there exist generic attention maps.
While this is not directly related to our results, a line of works studies the expressivity of trans-
formersas operating ona discrete set of tokensas formalsystems [6, 26, 33, 12]. Another line of work
studies the impact of positional encoding on the expressivity [24]
1.2 Our contributions
Our work provides a rigorous formalization of transformer expressivity and continuity as operating
overthespaceofprobabilitydistributions. Themainmathematicalresultisthe universalitypresented
in Theorem 1. Our approach effectively handles an arbitrary number of tokens and leverages deep
architectures without requiring arbitrary width. The embedding dimension and the number of heads
are proportional to the dimension of the input tokens and are independent of precision.
A limitation of our approach is that it does not consider masked attention, which would require
going beyond the current framework based on Wasserstein distances. Despite this, our formalism
provides a significant step forward in the theoretical modeling of transformers, offering new insights
into their underlying mechanics and potential applications.
1.3 Notation
For natural number N N, we denote by [N] := 1,...,N . For vector x Rd, the Euclidean norm
of x is denoted by x.
F∈
or two vectors x,y
Rd,{
the
Eucl}
idean inner
prod∈
uct of x and y is denoted
| | ∈
by x, y and the component-wise multiplication of x and y is denoted by x y. The vector 1 is the
d
vech tor oi f dimension d with all coordinates equal to 1, that is, 1 :=(1,...,1)⊙ Rd.
d
In the following, we denote by µ (Ω) a probability measure on some c∈ ompact domain Ω Rd
of tokens’ embeddings. We denote
∈ byP
(Ω) the space of continuous functions from Ω to
R.⊂
Our
construction makes use of the push-forwC ard operator T . For T : Ω Rd Ω Rd′ a measurable
♯ ′
⊂ → ⊂
map, a measure µ (Ω) is mapped to T µ (Ω). It operates over discrete measures by simply
♯ ′
∈ P ∈ P
displacing the support
n n
1 1
T δ := δ .
♯
n
xi
n
T(xi)
(cid:16) Xi=1 (cid:17) Xi=1
For a general measure, ν =T µ is defined by a change of variables in integration:
♯
g (Ω), g(y)dν(y):= g(T(x))dµ(x). (1)
′
∀ ∈C ZΩ′ ZΩ
2Weemplytheweak topologyon (Ω),whichisassociatedwiththefollowingnotionofconvergence
∗
P
of sequences:
µ
k
⇀∗ µ f (Ω), f(x)dµ k(x) f(x)dµ(x) .
⇔ ∀ ∈C →
(cid:16) Z Z (cid:17)
Intuitively,thiscorrespondstoa“soft”notionofconvergencewherethesupportofµ approachesthat
k
of µ.
In the special case of discrete measures with a fixed number n of points, this corresponds, up to
relabeling of the points, to the usual convergence of points in finite dimensions:
n n
1 1
δ ⇀ δ X =(x ) Rd n X =(x ) Rd n .
n
xk
i
∗
n
xi
⇔
k i,k i
∈
×
→
i i
∈
×
(cid:16) Xi=1 Xi=1 (cid:17) (cid:16) (cid:17)
While we do notmakeuse ofthis inthis paper(since our claimsarenotquantitative),it is possible to
metrize this weak topology using the Wasserstein Optimal Transport distance, which is defined, for
∗
1 p<+ , as
≤ ∞
W (µ,ν)p := min x y pdπ(x,y) : π =µ,π =ν ,
p 1 2
π (Ω2) k − k
∈P (cid:26)Z (cid:27)
where π =(P ) π are the marginals of π with P (x,y)=x and P (x,y)=y. One has
i i ♯ 1 2
µ
k
⇀∗ µ W p(µ,ν) 0.
⇔ →
Anavenueforfutureworkistoobtainquantitativeapproximationresultsforin-contextmappingsthat
are, for instance, Lipschitz continuous according to the Wasserstein distance.
2 Measure-theoretic in-context mappings
Transformers are defined by alternating multi-head attention layers (which compute interactions be-
tween tokens), MLP and normalization layers (which operate independently over each token). For
the sake of simplicity, we omit normalization in the following analysis. We first recall their definition
and then explain how they can be equivalently re-written using in-context mappings. This definition
provides new insights and can also be generalized to an “infinite” number of tokens encoded in a
probability measure.
2.1 Attention as in context mappings on token ensembles
Classical definition. Asetofn tokensx Rdin isdenotedbyX =(x )n Rdin n. Anattention
head maps these n tokens to the same numi b∈ er n of tokens in Rdhead usingi i=1 ∈ ×
X Rdin n, Att (X):=VX SoftMax(X Q KX/√k) Rdhead n,
× θ ⊤ ⊤ ×
∀ ∈ ∈
where the parameters are the (Key, Query, Value) matrices θ := (Q,K,V) Rk din Rk din
× ×
Rdhead din. Here, the SoftMax function operates in a row-wise manner: ∈ × ×
×
eZi,j n
∀Z ∈Rn ×n, SoftMax(Z):=
eZi,ℓ
∈R +n ×n.
(cid:18) ℓ (cid:19)i,j=1
P
Multipleheadswithdifferentparametersθ :=(Wh,θh)H arecombinedinalinearwayinamulti-head
h=1
attention:
H
MAtt (X):= WhAtt (X),
θ θh
h=1
X
where Wh Rdout dhead and θh :=(Qh,Kh,Vh).
×
∈
In the following, we denote the various dimensions of a multi-head attention layer by: d (θ),
in
d (θ), d (θ) for the input, output, and head dimensions, respectively, and k(θ) for the key/query
out head
dimensions, and H(θ) for the number of heads.
3In-context mappings form. The mappingX MAtt (X)canbe re-writtenas the applicationof
θ
7→
an “in context” function G (X, ) to each token,
θ
·
x G (X,x ) i.e. MAtt (X)=(G (X,x ))n ,
i 7→ θ i θ θ i i=1
where the in-context mapping is
H n exp 1 Qhx, Khx
(X,x) Rdin n Rdin, G (X,x):= Wh √kh j i Vhx . (2)
∀ ∈ × × θ n e(cid:16) xp 1 Qhx, Khx(cid:17) j
h X=1 Xj=1 ℓ=1 √kh ℓ i
(cid:16) (cid:17)
P
Here,theterminology“incontext”referstothefactthatG (X, )dependsonthetokensX themselves,
θ
·
andcanthusbeseenasaparametricmapthatismodifiedforeachtokendependingonitsinteractions
withtheothertokens. Whilethisre-writingisequivalenttotheoriginalone,ithighlightsthefactthat
transformersdefine spatialmappings. This alsoallowsus to clearly state the associatedmathematical
question at the core of this paper, which is the approximation of arbitrary in-context mappings by
(compositions of) such parametric maps. Another interest in this reformulation is that it enables the
definition of generalized attention operating over a possibly infinite number of tokens, as explained in
Section 2.2.
Composition of in-context mappings. A transformer (ignoring normalization layers at this mo-
ment) is a composition of L attention layers and Multi-Layer Perceptrons (MLP):
MLP MAtt ... MLP MAtt . (3)
ξL◦ θL◦
◦
ξ1◦ θ1
Here, the MLP functions process each token independently from one another:
ξ
MLP (X)=(F (x ))n ,
ξ ξ i i=1
i.e., they are “context-free” mappings (in the above notation, F (X,x) = F (x)), while the attention
ξ ξ
maps, G (X, ), depend on the context X.
θ
·
On the level of in-context mappings, the composition of layers in (3) induces a new “in-context”
composition rule, which we denote by ,
⋄
X =(x )n , x : (G G )(X,x):=G (X ,G (X,x)) where X =(G (X,x )) . (4)
∀ i i=1 ∀ 2 ⋄ 1 2 1 1 1 1 i i
This rule can be applied whether G (X, ) or G (X, ) depends on the context X or not (such as for
1 2
· ·
the F mappings above, which are independent of the context). Using this rule, the transformer’s
ξ
definition (3) translates into a composition of in-context and context-free maps:
F G ... F G . (5)
ξL
⋄
θL
⋄ ⋄
ξ1
⋄
θ1
The core question this paper addresses is the uniform approximation of a continuous (in a suitable
topology) in-context map (X,x) G(X,x) by transformers’ in-context mappings of the form (3),
7→
with clear control of the dimensions and the number of heads involved in the different layers. The
main originality of our approach is that we aim to do so for an arbitrary number n of tokens, as we
now explain.
2.2 Measure-theoretic in-context mappings
A key observation is that the definition in (2) makes sense irrespective of the number, n, of tokens.
To make this more explicit, and also handle the limit of an infinite number of tokens, we represent a
set X of tokens using a probability distribution µ (Rdin) over Rd. A finite number of tokens is
∈ P in
encoded using a discrete empirical measure,
n
1
µ= δ (Rdin). (6)
n
xi
∈P
i=1
X
This encoding is not only for notional convenience, it also allows us to define clearly a correct notion
of smoothness for the in-context mappings. This smoothness corresponds to the displacement of the
4tokens and is quantified through the optimal transport distance as presented in Section 1.3. This
enables us to compare context with different sizes and, for instance, to compare a set of tokens with a
large (but finite) n to a continuous distribution.
Using probability distributions, the in-context mapping (2) is now defined as
H exp 1 Qhx, Khy
(µ,x) (Rdin) Rdin, Γ (µ,x):= Wh √kh i Vhydµ(y). (7)
∀ ∈P × θ h X=1 Z exp √(cid:16) 1 khQhx, Khz
i
d(cid:17) µ(z)
(cid:16) (cid:17)
R
We can include a skip connection, that is, Γ can be redefined as
θ
H exp 1 Qhx, Khy
(µ,x) (Rdin) Rdin, Γ (µ,x):=x+ Wh √kh i Vhydµ(y). (8)
∀ ∈P × θ exp (cid:16) 1 Qhx, Khz d(cid:17) µ(z)
h X=1 Z √kh i
R (cid:16) (cid:17)
In this case, we assume that d =d .
in out
The discrete case is contained in this more general definition in the sense that
n
1
X =(x )n , G (X,x)=Γ δ ,x .
∀ i i=1 θ θ n xi
(cid:16) Xi=1 (cid:17)
In the following, we will invoke, whenever convenient, the following slight abuse of notation,
Γ (µ,x)=Γ (µ)(x),
θ θ
sothatΓ (µ):Rdin Rdout definesamapbetweenEuclideanspaces. Usingthisgeneraldefinition,the
θ
→
attentionmapX MAtt (X)canberewrittenasdisplacingthetokens’positions,whichcorresponds
θ
7→
to applying a push-forward to the measure as defined in (1),
µ (Rdin) Γ (µ) µ (Rdout).
θ ♯
∈P 7−→ ∈P
This formulation of transformers as a mapping between probability measures was introduced in [31]
and also used in [5] to prove a convergence result of deep transformers. We re-use it here but put
emphasis on the in-context mapping itself, which is the object of interest of this paper (rather than
on studying the mapping between measures).
Composition of in-context measure-theoretic mappings. The definitionofcompositionin(4)
generalizes to the measure-theoretic setting as
(Γ Γ )(µ,x):=Γ (µ ,Γ (µ,x)), where µ :=Γ (µ) µ, (9)
2 1 2 1 1 1 1 ♯
⋄
i.e.,(Γ Γ )(µ)=Γ (µ ) Γ (µ). Transformersoperatingoveranarbitrary(possiblyinfinite)number
2 1 2 1 1
⋄ ◦
of tokens are then obtained by replacing the original definition (5) by
F Γ ... F Γ . (10)
ξL
⋄
θL
⋄ ⋄
ξ1
⋄
θ1
Here, the F are “context-free” MLP mappings, i.e., F (µ,x) = F (x) is independent of µ. It is
ξ ξ ξ
important to keep in mind that when restricted to finite discrete empirical measures of the form (6),
definitions(5)and(10)coincide. Ourtheoryencompassesclassicaltransformersaswellastheir“mean
field” limits operating over arbitrary measures.
2.3 Training transformers and applications
Transformers are trained in a supervised or unsupervised way to approximate some (unknown) in-
context map Λ⋆(µ,x) to perform in-context prediction by minimizing some loss function ℓ:
min ℓ(F Γ (µ ,x ),y ),
(θℓ,ξℓ)L
ℓ=1 p
ξL
⋄···⋄
θ1 p p p
X
5where (µ ,x ) are pairs of contexts and tokens, and y are the labels to predict (typically another
p p p
token). In most situations of practical interest, x belongs to the support of the measure µ .
p p
In computer vision applications, in particular for generative image modeling (diffusion models)
using Vision Transformers, µ is a set of patches (with positional encoding added) extracted from
p
a noisy image, and Λ⋆(µ ,x ) maps a noisy patch x to approximate its denoised version y . In
p p p p
NLP applications, the leading paradigm is next-token prediction through self-supervised learning.
Tokens extracted from a sentence are of the form (x(t),t) n , and the measure representation is
{ }t=1
µ = 1 T δ . Note that here, following previous work,we have appended the token index t in
p n t=1 (x(t),t)
the sentence (since for NLP, transformers are not permutation equivariant). In this case, for a token
P
x = x(t) at some position t in the sentence, the token to predict is y = x(t+1). For these NLP
p p
applications, the self-attention maps should be made causal to ensure that the problem is not trivial
and to enable use in a generative model through auto-regressive evaluation of the map. This is made
explicit in the definition,
Γ (µ,(x,t)):=
H
Wh
exp √1 khQhx,Khy
i
1 t ≤t′
Vhy dµ(y,t), (11)
θ (cid:16) (cid:17) ′
exp 1 Qhx,Khz 1 dµ(z,s)
h X=1 Z √kh i t ≤s
(cid:16) (cid:17)
R
where 1 t t′ is a masking function that is 1 if t t ′ and 0 otherwise. We leave the extension of our
≤ ≤
results to such a causal in-context mapping for future work.
2.4 Main result and discussion
Our main result is the following uniform approximation theorem.
Theorem 1. Let Ω Rd be a compact set and Λ⋆ : (Ω) Ω Rd′ be continuous, where (Ω) is
⊂ P × → P
endowed with the weak topology. Then for all ε > 0, there exist L and parameters (θ ,ξ )L , such
∗ ℓ ℓ ℓ=1
that
(µ,x) (Ω) Ω, F Γ ... F Γ (µ,x) Λ⋆(µ,x) ε,
∀ ∈P × |
ξL
⋄
θL
⋄ ⋄
ξ1
⋄
θ1
− |≤
with d (θ ),d (θ ) d+3d, d (θ )=k(θ )=1, H(θ )=d.
in ℓ out ℓ ′ head ℓ ℓ ℓ ′
≤
The two main strengths of this result are (i) the approximating architecture performs the approx-
imation independently of n (it even works for an infinite number of tokens), and (ii) the number of
heads and the embedding dimension do not depend on ε.
AweaknessisthatwehavenoexplicitcontroloverthedependencyofthenumberofMLPparame-
tersξ onε. Anotherlimitationofourprooftechniqueisthatthenumberofheadsgrowsproportionally
ℓ
to the output dimension while each head only outputs a scalar d (θ )=1. Obtaining a better bal-
head ℓ
ance between these two parameters is an interesting problem. As explained in the proof, these MLPs
approximate a real-valued squaring operator a R a2 R, so we expect this dependency to be
∈ 7→ ∈
well-behavedin common situations, but our construction does not provide any a priori bound on how
the magnitude of the tokens grows through the layers. The main hypothesis of Theorem 1 is that the
underlying map, Λ⋆, is a smooth (at least continuous)map for the weak topology over measures (see
∗
Section1.3forsomebackground). Sinceourresultsarenotquantitative,thisisnotastrongrestriction,
and it enables a unifying study of transformers for any number, n, of tokens. It is, however,not clear
how much this is a good model to conduct further quantitative studies, and we leave this exploration
for future work.
3 Proof of Theorem 1
The proof of Theorem 1 is divided into three steps. First, we approximate the map Λ⋆ by “cylin-
drical maps”, which are spanned by pointwise multiplication of self-attention maps, including also
affine transformations. This approximation leverages the Stone-Weierstrass theorem. Second, we ap-
proximate the multiplication obtained in the first step using MLPs. We note that these MLPs are
“context-free”,thatis,independentofthemeasures. Finally,weshowhowtheobtainedapproximation
can be represented using a deep transformer operating over a higher dimensional embedding space.
63.1 Step 1: Approximation by cylindrical mapping
In this section, we approximate the map Λ⋆ by a class of functions that are pointwise products of
elementary attention and affine transforms. We coin these functions “cylindrical mappings” because
similar functions have been used in [13] to define Sobolev regularity over the Wasserstein space. In
contrast to deep transformer maps, which operate by composition, we consider here simple multipli-
cation. Section 3.3 details how to switch from these multiplications to compositions. To this end,
we apply the Stone-Weierstrass theorem to the h-th component map, (µ,x) Λ⋆(µ,x) , for each
h
h [d], where Λ⋆(µ,x) R is h-th component of vector Λ⋆(µ,x) Rd′ . Th7→ e key is to prove that
′ h
∈ ∈ ∈
the subalgebra of cylindrical mapping, constructed by the span of compositions of self-attentions and
affine transformsseparatespoints. This is provedby using the injectivity of the Radontransform(see
Appendix A).
For t [T] and n [N], we define
∈ ∈
d′ exp Q˜h x, K˜h y
Γ (µ,x):=x+ W˜h h t,n t,n i V˜h ydµ(y), x Rd′ , (12)
θ˜ t,n h X=1 t,n Z exp h(cid:16) Q˜h t,nx, K˜ th ,ny
i
d(cid:17) µ(y) t,n ∈
(cid:16) (cid:17)
R
where d (θ˜ )=d (θ˜ )=d, d (θ˜ )=k(θ˜ )=1 and
in t,n out t,n ′ head t,n t,n
θ˜ t,n := {W˜ th ,n,V˜ th ,n,Q˜h t,n,K˜ th ,n}h=1,...,d′ ⊂Rd′ ×1 ×R1 ×d′ ×R1 ×d′ ×R1 ×d′ .
We define affine transforms, :Rd Rd′ , by
t,n
A →
(x):=A x+b , (13)
t,n t,n t,n
A
where A Rd′ d, b Rd′ . Then we have the composition,
t,n × t,n
∈ ∈
Γ (µ,x)=Γ (( ) µ, (x))=A x+b
θ˜
t,n
⋄At,n θ˜
t,n
At,n ♯ At,n t,n t,n
d′ exp Q˜h (A x+b ), K˜h (A y+b )
+ W˜h h t,n t,n t,n t,n t,n t,n i V˜h (A y+b )dµ(y).
t,n exp (cid:16) Q˜h (A x+b ), K˜h (A y+b ) d(cid:17) µ(y) t,n t,n t,n
h X=1 Z h t,n t,n t,n t,n t,n t,n i
R (cid:16) (cid:17)
In what follows, we write
Γ˜ (µ,x):=Γ (µ,x). (14)
t,n θ˜
t,n
⋄At,n
Lemma 1. For any ε>0, there exist T,N N, θ˜ , A ,b such that
t,n t [T],n [N] t,n t,n t [T],n [N]
∈ { } ∈ ∈ { } ∈ ∈
N
(µ,x) (Ω) Ω, Γ˜ (µ,x) Γ˜ (µ,x) Λ⋆(µ,x) ε.
T,n 1,n
∀ ∈P × (cid:12) ⊙···⊙ !− (cid:12)≤
(cid:12) n X=1 (cid:12)
(cid:12) (cid:12)
Proof. We write (cid:12) (cid:12)
(cid:12) (cid:12)
A =(a1 ,...,ad′ ), b =(b1 ,...,bd′ ),
t,n t,n t,n t,n t,n t,n
where ah Rd′ and bh R. We choose
t,n ∈ t,n ∈
W˜h =(0,...,0, 1 ,0,...,0)=e ,
t,n h
h th
−
independently of t,n , where e h h [d′] is the stan|d{azr}d basis in Rd′ , and
{ } ∈
V˜h =(0,...,0,vh ,0...,0), Q˜h =(0,...,0, ch ,0...,0), K˜h =(0,...,0, 1 ,0...,0),
t,n t,n t,n t,n t,n
h th
h th h th −
− −
|{z}
where vh R, ch R|.{Tz}hen, as |{z}
t,n ∈ t,n ∈
Q˜h (A x+b )=ch ( ah , x +bh ),
t,n t,n t,n t,n h t,n i t,n
K˜h (A y+b )= ah , x +bh , V˜h (A y+b )=vh ( ah , x +bh ),
t,n t,n t,n h t,n i t,n t,n t,n t,n t,n h t,n i t,n
7we see that
d′
Γ˜ (µ,x)=Γ (µ,x)= e ah , x +bh
t,n θ˜ t,n ⋄At,n h "h t,n i t,n
h=1
X
exp ( ah , x +bh )ch ( ah , y +bh )
+ h t,n i t,n t,n h t,n i t,n vh ( ah , y +bh )dµ(y) .
exp ((cid:16) ah , x +bh )ch ( ah , z +bh ) d(cid:17) µ(z) t,n h t,n i t,n #
Z h t,n i t,n t,n h t,n i t,n
R (cid:16) (cid:17)
Thus, we only need to show that the set,
N
:=span (Ω) Ω (µ,x) m˜ (µ,x) m˜ (µ,x) R ,
A (P × ∋ 7→
λT,n
···
λ1,n
∈ )
n=1
X
is dense in ( (Ω) Ω;R); here, m˜ (µ,x) is defined by
C P ×
λt,n
m˜ (µ,x):= a , x +b
λt,n
h
t,n
i
t,n
exp ( a , x +b )c ( a , y +b )
t,n t,n t,n t,n t,n
h i h i
+ v ( a , y +b )dµ(y),
(cid:16) (cid:17) t,n h t,n i t,n
exp ( a , x +b )c ( a , z +b ) dµ(z)
Z t,n t,n t,n t,n t,n
h i h i
R (cid:16) (cid:17)
where
λ := a ,b ,c ,v Rd R R R.
t,n t,n t,n t,n t,n
{ }⊂ × × ×
Ifwecanshowthat,byapplyingthedensenessresulttotheh-thcomponentmap,(µ,x) Λ⋆(µ,x)
h
R for each h [d], where Λ⋆(µ,x) R is the h-th component of the vector Λ⋆(µ,x7→ ) Rd′ , the∈ n
′ h
∈ ∈ ∈
we can approximate the h-th component map with an element in . The concatenation of obtained
elementstakestheform N Γ˜ (µ,x) Γ˜ (µ,x), therebypA rovingLemma1. Inwhatfollows,
n=1 T,n ⊙···⊙ 1,n
we verify the application of the Stone-Weierstrass theorem to the set .
P A
It is straightforward to establish the stability of the sum and scalar product. The set contains
A
the constant function, namely, by choosing T =N =1, c =v =0, a =0 and b =1.
1,1 1,1 1,1 1,1
For the separation, we first assume that x = x (that is, assume that i [d] such that x = x ).
6
′
∃ ∈
i
6
′i
Specifically, by choosing T =N =1, a =(0,...,0, 1 ,0,...,0), c =v = 0 and b =0, we see
1,1 1,1 1,1 1,1
i th
that −
|{z}
m˜ (µ,x)= a , x =x =x = a , x =m˜ (µ,x).
λ1,1
h
1,1
i
i
6
′i
h
1,1 ′
i
λ1,1 ′ ′
Next, we assume that x=x. Again, we choose T =N =1. It is enough to show that
′
λ , m˜ (µ,x)=m˜ (µ,x) implies that µ=µ.
∀
1,1 λ1,1 λ1,1 ′ ′
We note that the left-hand side implies that
exp ( a , x +b )c a , y a , y dµ(y)
1,1 t,n 1,1 1,1 1,1
h i h i h i
R (cid:16) exp a , x +b )c a (cid:17) , z dµ(z)
1,1 1,1 1,1 1,1
h i h i
R (cid:16) (cid:17)
exp ( a , x +b )c a , y a , y dµ(y)
1,1 1,1 1,1 1,1 1,1 ′
h i h i h i
= .
R (cid:16) exp ( a , x +b )c a (cid:17) , z dµ(z)
h 1,1 i 1,1 1,1 h 1,1 i ′
By choosing b R so that a , x +b = 0 (R while(cid:16) x is frozen), letting a = a(cid:17) , and choosing
1,1 1,1 1,1 1,1
c =c/( a ,
x∈
+b )
withh
a
Rdi
and c
6R
arbitrary, we obtain
1,1 1,1 1,1
h i ∈ ∈
L(µ)(a,c)=L(µ′)(a,c), a Rd,c R,
∀ ∈ ∈
where
exp(c a, y ) a, y dµ(y)
L(µ)(a,c):= h i h i .
exp(c a, z )dµ(z)
R h i
Then, Lemma 5 in Appendix A implies that µ=R µ
′
and the proof is complete.
83.2 Step 2: Approximation of the multiplication map by MLPs
InSection3.1(Step1),weshowedthatthemap(µ,x) Λ⋆(µ,x)canbeapproximatedbyacylindrical
7→
map
N
(µ,x) Γ˜ (µ,x) Γ˜ (µ,x),
T,n 1,n
7→ ⊙···⊙
n=1
X
where Γ˜ was defined in (14). To obtain an approximator that can be represented by deep trans-
t,n
formers, we will further approximate the multiplication map in the above using MLPs. We note that
approximationof multiplication by MLP has been studied in detail, see for instance [11, Lemma 6.2].
We now prove the following result.
Lemma 2. Let Γ˜ : (Rd′ ) Rd′ Rd′ be defined in (14). For any ε > 0, there exists an MLP
t,n
Φ:R2d′ Rd′
such
thaP
t
× →
→
N
(µ,x) (Ω) Ω, Γ˜ (µ,x) Γ˜ (µ,x)
T,n 1,n
∀ ∈P × (cid:12) ⊙···⊙
(cid:12)n X=1
N (cid:12)
(cid:12)
Φ Γ˜ T,n(µ(cid:12),x),Φ Γ˜ T 1,n(µ,x), Φ Γ˜ 2,n(µ,x),Φ Γ˜ 1,n(µ,x),1 d′ ε.
− − ··· (cid:12)≤
n X=1 (cid:16) (cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)(cid:17)(cid:12)
(cid:12)
Proof. We note that (cid:12)
(cid:12)
Γ˜ T,n(µ)(x) Γ˜ 1,n(µ)(x)=Γ˜ T,n(µ)(x) Γ˜ T 1,n(µ)(x) Γ˜ 2,n(µ)(x) Γ˜ 1,n(µ)(x) 1 d′ .
⊙···⊙ ⊙ − ⊙···⊙ ⊙ ⊙
(cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)
Because the component-wise multiplication map (x,y)
R2d′
x y
Rd′
is continuous, by the
universality of MLPs, for any ε>0 and R>0, there
exi∈
sts an
M7→ LP⊙ Φ:R∈ 2d′ Rd′
such that
→
∀(x,y) ∈B R2d′(0,R), |x ⊙y −Φ(x,y) |≤ε. (15)
Since Ω Rd is compact then 0 C :=sup x is finite. Thus we estimate that
⊂ ≤ Ω x ∈Ωk k2
d′
Γ˜ (µ,x) A C + b + ( A C + b ) W˜ h V˜h
t,n ≤k t,n k2 Ω k t,n k2 k t,n k2 Ω k t,n k2 k t,n t,nk2
(cid:12) (cid:12) h X=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
d′
max ( A C + b )(1+ W˜h V˜h ) =:C for all (µ,x) (Ω) Ω, (16)
≤t [T],n [N] k t,n k2 Ω k t,n k2 k t,n t,nk2  Γ˜ ∈P ×
∈ ∈ h=1
X
 
where the constant, C > 0, depends on Ω, W˜h ,V˜h ,A ,b , but is independent of t,n,µ and x.
Γ˜ t,n t,n t,n t,n
Thus,using the universalityin (15), choosing a largeradius R>0 depending on the constantC >0,
Γ˜
we can show that there exists an MLP
Φ:R2d′ Rd′
such that
→
Γ˜ (µ,x) Γ˜ (µ,x)
T,n 1,n
(cid:12) ⊙···⊙
(cid:12)
(cid:12)
(cid:12) ε
(cid:12) Φ Γ˜ T,n(µ,x),Φ Γ˜ T 1,n(µ,x), Φ Γ˜ 2,n(µ,x),Φ Γ˜ 1,n(µ,x),1 d′ ,
− − ··· (cid:12)≤ N
(cid:16) (cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)(cid:17)(cid:12)
(cid:12)
which completes the proof. (cid:12)
(cid:12)
Remark 1. (The challenge to derive quantitative estimates.) The key is to approximate and capture
the mentioned multiplicity by MLPs, for which quantitative estimates have been studied, e.g., [11,
Lemma 6.2], which is a variant of [39, Proposition 2]. However, the depth and width of MLPs depend
ontheboundofinputvariables. Specifically, anexistentialΦintheabovedepends ontheboundC (see
Γ˜
(16)), which in turndepends on parameters in Γ˜ that are chosen to approximate Γ within ε through
t,n ∗
the application of the Stone-Weierstrass theorem (see Lemma 1). Thus, providing the quantitative
estimate for the MLP Φ is challenging.
93.3 Step 3: Realization of cylindrical mappings by deep transformers
In (Step 1) and (Step 2), we have shown that the map (µ,x) Λ⋆(µ,x) can be approximated by the
7→
map
N
Φ Γ˜ T,n(µ,x),Φ Γ˜ T 1,n(µ,x), Φ Γ˜ 2,n(µ,x),Φ Γ˜ 1,n(µ,x),1 d′ ,
− ···
n X=1 (cid:16) (cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)(cid:17)
whereΓ˜ ,definedby(14),isthe compositionofself-attentionandanaffinetransform,andΦ issome
t,n
“context-free”MLP.To obtain the main result (Theorem1), it is sufficient to show that this map can
be represented by a deep transformer in the form of (10). Here, Γ :
(Rd+3d′
)
Rd+3d′ Rd+3d′
θt,n
P × →
is given by
Γ (µ ,(x,u,p,w))=(x,u,p,w)
θt,n t,n
d′ exp Qh (x,u,p,w), Kh (y ,v ,q ,z )
+ W th
,n
(cid:16)h t,n t,n ′ ′ ′ ′ i
(cid:17)
V th ,n(y′,v′,q′,z′)dµ t,n(y′,v′,q′,z′)
exp Qh (x,u,p,w), Kh (y,v,q,z) dµ (y,v,q,z)
h X=1 Z h t,n t,n i t,n
for µ (RR d+3d′(cid:16) ), x Rd, u,p,w Rd′ , where (cid:17)
t,n
∈P ∈ ∈
θ t,n = {W th ,n,V th ,n,Qh t,n,K th ,n}h=1,...,d′ ⊂R(d+3d′) ×1 ×R1 ×(d+3d′) ×R1 ×(d+3d′) ×R1 ×(d+3d′);
thus, Γ has the following size,
θt,n
d (θ )=d (θ )=d+3d, d (θ )=k(θ )=1, H(θ )=d.
in t,n out t,n ′ head t,n t,n t,n ′
We denote the MLPs by F
:Rd+3d′ Rd+3d′
, with weight and bias parameters ξ .
ξt,n
→
t,n
The aim of this section is to prove, by construction, the following result.
Lemma 3. Let Γ˜ : (Rd′ ) Rd′ Rd′ be defined in (14). Let Φ : R2d′ Rd′ be an MLP. There
t,n
P × → →
exist ξ , ξ , ξ , and θ , θ , θ such that
0 t,n 0 t,n
∗ ∗
N
(µ,x) (Ω) Ω, Φ Γ˜ T,n(µ,x),Φ Γ˜ T 1,n(µ,x), Φ Γ˜ 2,n(µ,x),Φ Γ˜ 1,n(µ,x),1 d′
∀ ∈P × − ···
n X=1 (cid:16) (cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)(cid:17)
=F Γ N T F Γ F Γ (µ,x).
ξ∗ ⋄ θ∗ ⋄ ⋄n=1⋄t=1 ξt,n ⋄ θt,n ⋄ ξ0 ⋄ θ0
Proof. The proof is based on the following scheme: (cid:0) (cid:1)
x
x
Fξ0⋄Γθ0 A1,1(x)

−[−S−te−p−−A→] ϕ 1,1(x)
 f 1(x) 
 
 x  x x
Fξ1,1⋄Γθ1,1 A2,1(x)

Fξ2,1⋄Γθ2,1 FξT−1,1⋄ΓθT−1,1 AT,1(x)

FξT,1⋄ΓθT,1 A1,2(x)

−−[S−t−e−p−B−]→ ϕ 2,1(x) −−[S−t−e−p−B−]→···−−−−[S−t−e−p−B−]−−→ ϕ T,1(x) −−[−S−te−p−−C−]→ ϕ 1,2(x)
 f 1(x)   f 1(x)   f 2(x) 
     
 x   x   x 
Fξ1,2⋄Γθ1,2 A2,2(x)

Fξ2,2⋄Γθ2,2 FξT−1,2⋄ΓθT−1,2 AT,2(x)

FξT,2⋄ΓθT,2 A1,3(x)

−−[S−t−e−p−B−]→ ϕ 2,2(x) −−[S−t−e−p−B−]→···−−−−[S−t−e−p−B−]−−→ ϕ T,2(x) −−[−S−te−p−−C−]→ ϕ 1,3(x)
 f (x)   f (x)   f (x) 
 2   2   3 
  .    
.
.
x x x
Fξ1,N⋄Γθ1,N A2,N(x)

Fξ2,N⋄Γθ2,N FξT−1,N⋄ΓθT−1,N AT,N(x)

FξT,N⋄ΓθT,N A1,N+1(x)

−−[−S−te−p−−B−]→ ϕ 2,N(x) −−[−S−te−p−−B−]→···−−−−[−S−te−p−−B−]−−→ ϕ T,N(x) −−−[S−t−e−p−C−]−→ ϕ 1,N+1(x)
 f N(x)   f N(x)   f N+1(x) 
     
 N    
Fξ∗⋄Γθ∗ f N+1(x)= Φ Γ˜ T,n(µ,x),Φ Γ˜ T 1,n(µ,x), Φ Γ˜ 2,n(µ,x),Φ Γ˜ 1,n(µ,x),1 d′ ,
−[−S−te−p−−D→] − ···
n X=1 (cid:16) (cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)(cid:17)
10where ϕ :Rd Rd′ is given by
t,n
→
ϕ t,n(x):= Φ Γ˜ t −1,n(µ,x),Φ Γ˜ t −2,n(µ,x), ···Φ Γ˜ 2,n(µ,x),Φ Γ˜ 1,n(µ,x),1 d′ , t ≥2 ,
( 1 d(cid:16)′, (cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)(cid:17) t=1
and f :Rd Rd′ by
n
→
f n(x):=
0
in =− 11ϕ T,i(x), n
n≥
=2
1
.
(cid:26) P
Furthermore, the :Rd Rd′ are the affine transforms chosen in Lemma 1. Here, Γ , Γ , Γ ,
At,n
→
θ0 θt,n θ∗
F , F and F will be specified below, in the following steps:
ξ0 ξt,n ξ∗
[Step A] Let Γ (µ):Rd Rd be
θ0
→
Γ (µ,x)=x,
θ0
and let F :Rd Rd+3d′ be the affine transform defined by
ξ0
→
F ξ0(x):=(x,A 1,1x+b 1,1,1 d′,0)=(x, A1,1(x),ϕ 1,1(x),f 1(x)).
Then we see that
F Γ (µ,x)=(x, (x),ϕ (x),f (x)),
ξ0
⋄
θ0 A1,1 1,1 1
and
µ :=(F Γ (µ)) µ=(µ,( ) µ,(ϕ ) µ,(f ) µ).
1,1 ξ0 ⋄ θ0 ♯ A1,1 ♯ 1,1 ♯ 1 ♯
We proceed with [Step B] in which we handle the case when n=t=1.
[Step B] Let t=1,...,T 1 and n=1,...,N. We already have that
−
⋄jt −=1 1F
ξj,n
⋄Γ
θj,n
⋄
⋄in =−11 ⋄T s=1F
ξs,i
⋄Γ
θs,i
⋄F
ξ0
⋄Γ θ0(µ,x)=(x, At,n(x),ϕ t,n(x),f n(x))
(cid:0) (cid:1) (cid:0) (cid:1)
and
µ
t,n
:= ⋄t j−=1 1F
ξj,n
⋄Γ
θj,n
⋄
⋄in =−11 ⋄T s=1F
ξs,i
⋄Γ
θs,i
⋄F
ξ0
⋄Γ θ0(µ) ♯µ
(cid:0)(cid:0) (cid:1) (cid:0) (cid:1) =(cid:1)(µ,( t,n) ♯µ,(ϕ t,n) ♯µ,(f n) ♯µ).
A
Whenn=1ort=1,the abovereducesto ⋄in =−11 ⋄T s=1F ξs,i⋄Γ θs,i =I d+3d′ or ⋄t j−=1 1F ξj,n⋄Γ θj,n =I d+3d′.
Let Γ (µ
):Rd+3d′ Rd+3d′
be given by
θt,n t,n
→
Γ (µ ,(x,u,p,w))
θt,n t,n
d′ exp Q˜h u, K˜h v
= x,u+ W˜h h t,n t,n ′ i V˜h v dµ (y ,v ,q ,z ),p,w
 h X=1 t,n Z exp hQ˜h t,n(cid:16) u, K˜ th ,nv
i
dµ t,n(cid:17) (y,v,q,z) t,n ′ t,n ′ ′ ′ ′ 
 R (cid:16) (cid:17) = x,Γ (( ) µ,u),p, w ,
θ˜
t,n
At,n ♯
(cid:16) (cid:17)
where
{W˜ th ,n,V˜ th ,n,Q˜h t,n,K˜ th ,n}h=1,...,d′ ⊂Rd′ ×1 ×R1 ×d′ ×R1 ×d′ ×R1 ×d′ ,
which were specified in Lemma 1. Here, we choose
Wh =(O,W˜h ,O,O), Vh =(O,V˜h ,O,O), Qh =(O,Q˜h ,O,O), Kh =(O,K˜h ,O,O).
t,n t,n t,n t,n t,n t,n t,n t,n
Let F
:Rd+3d′ Rd+3d′
be an MLP defined by
ξt,n
→
F (x,u,p,w)=(x,A x+b ,Φ(u,p),w)=(x, (x),Φ(u,p),w).
ξt,n t+1,n t+1,n At+1,n
11Then we have
( ⋄t j=1F
ξj,n
⋄Γ θj,n) ⋄( ⋄in =−11 ⋄T s=1F
ξs,i
⋄Γ θs,i) ⋄F
ξ0
⋄Γ θ0(µ,x)
=F
ξt,n
⋄Γ
θt,n
⋄( ⋄t j−=1 1F
ξj,n
⋄Γ θj,n) ⋄( ⋄in =−11 ⋄T s=1F
ξs,i
⋄Γ θs,i) ⋄F
ξ0
⋄Γ θ0(µ,x)
=F Γ (µ ,(x, (x),ϕ (x),f (x)))
ξt,n
⋄
θt,n t,n At,n t,n n
=F (x,Γ (( ) µ, (x)),ϕ (x),f (x))
ξt,n θ˜
t,n
At,n ♯ At,n t,n n
=F (x,Γ˜ (µ,x),ϕ (x),f (x))
ξt,n t,n t,n n
=(x, (x),ϕ (x),f (x)))
t+1,n t+1,n n
A
and
µ
t+1,n
:= ⋄t j=1F
ξj,n
⋄Γ
θj,n
⋄
⋄in =−11 ⋄T s=1F
ξs,i
⋄Γ
θs,i
⋄F
ξ0
⋄Γ θ0(µ) ♯µ
=(cid:0)(µ(cid:0),( t+1,n) ♯µ,(ϕ t+(cid:1)1,n(cid:0)) ♯µ,(f n) ♯µ). (cid:1) (cid:1)
A
We repeat [Step B] until obtaining µ . Once µ is obtained, we proceed with [Step C].
T,n T,n
[Step C] Let Γ (µ
):Rd+3d′ Rd+3d′
be given by
θT,n T,n
→
Γ (µ ,(x,u,p,w))
θT,n T,n
d′ exp Q˜h u, K˜h v
= x,u+ W˜h h T,n T,n ′ i V˜h v dµ (y ,v ,q ,z ),p,w
 h X=1 T,n Z exp hQ˜h T,n(cid:16) u, K˜ Th ,nv
i
dµ T,n(cid:17) (y,v,q,z) T,n ′ T,n ′ ′ ′ ′ 
 (cid:16) (cid:17) 
R = x,Γ (( ) µ,u),p,w .
θ˜
T,n
AT,n ♯
Let F
:Rd+3d′ Rd+3d′
be an MLP defined by
(cid:16) (cid:17)
ξT,n
→
F ξT,n(x,u,p,w)=(x,W 1,n+1x+b 1,n+1,1 d′,w+Φ(u,p))=(x, A1,n+1(x),ϕ 1,n+1(x),w+Φ(u,p)).
When n=N, we define by (x):=0 and ϕ :=0 in the above. We find that
1,N+1 1,N+1
A
( ⋄T j=1F
ξj,n
⋄Γ θj,n) ⋄( ⋄in =−11 ⋄T s=1F
ξs,i
⋄Γ θs,i) ⋄F
ξ0
⋄Γ θ0(µ,x)
=F (x,Γ˜ (µ,x),ϕ (x),f (x))
ξT,n T,n T,n n
=(x, (x),ϕ (x),f (x)))
1,n+1 1,n+1 n+1
A
and
µ
T+1,n
:= ⋄T j=1F
ξj,n
⋄Γ
θj,n
⋄
⋄in =−11 ⋄T s=1F
ξs,i
⋄Γ
θs,i
⋄F
ξ0
⋄Γ θ0(µ) ♯µ
=(cid:0)(µ(cid:0),( 1,n+1) ♯µ,(ϕ 1(cid:1),n+(cid:0)1) ♯µ,(f n+1) ♯µ). (cid:1) (cid:1)
A
Denoting
µ :=µ ,
1,n+1 T+1,n
we return to [Step B], and repeat [Step B] and [Step C] until obtaining µ . Once µ is
T+1,N T+1,N
obtained, we proceed with [Step D].
[Step D] Let Γ (µ
):Rd+3d′ Rd+3d′
be given by
θ∗ T+1,N
→
Γ (µ ,(x,u,p,w))=(x,u,p,w)
θ∗ T+1,N
and let F
:Rd+3d′ Rd′
be the affine transform defined by
ξ∗
→
F (x,u,p,w):=w.
ξ∗
Then we conclude that
F Γ N T F Γ F Γ (µ,x)
ξ∗ ⋄ θ∗ ⋄ ⋄n=1⋄s=1 ξs,n ⋄ θs,n ⋄ ξ0 ⋄ θ0
(cid:0) (cid:1)
=F Γ (µ ,(x, (x),ϕ (x),f (x))))=f (x)
ξ∗
⋄
θ∗ T+1,N A1,N+1 1,n+1 N+1 N+1
N
= Φ Γ˜ T,n(µ,x),Φ Γ˜ T 1,n(µ,x), Φ Γ˜ 2,n(µ,x),Φ Γ˜ 1,n(µ,x),1 d′ .
− ···
n X=1 (cid:16) (cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)(cid:17)
12Discussion
Alimitation ofour method is that it is not quantitative. Using, for instance,the Wassersteindistance
between token distributions could be a way to impose smoothness on the map to obtain quantitative
bounds. Ourproofreliesontheapproximationofthe mapalongeachdimensionandtheuseofacom-
mutingarchitecture(the transformerlayersaremultipliedtogethertoobtainthe output). Thisresults
in a growth of the number of heads proportional to the dimension. Lowering this dependency would
require the development of new proof techniques beyond the use of the Stone-Weierstrass theorem.
Additionally, it does not handle masked attention, which means that the architectures considered are
not causal and are permutation invariant.
Acknowledgements
T. Furuya was partially supported by JSPS KAKENHI Grant Number JP24K16949. M.V. de Hoop
carried out the work while he was an invited professor at the Centre Sciences des Donn´ees at Ecole
Normale Sup´erieure,Paris. He acknowledgesthe support of the Simons Foundation under the MATH
+ X Program,the Department of Energy under grant DE-SC0020345,and the corporate members of
the Geo-Mathematical Imaging Group at Rice University. The work of G. Peyr´e was supported by
theEuropeanResearchCouncil(ERCprojectWOLF)andthe Frenchgovernmentundermanagement
of Agence Nationale de la Recherche as part of the “Investissements d’avenir” program, reference
ANR-19-P3IA-0001(PRAIRIE 3IA Institute).
References
[1] Andrei Agrachev and Cyril Letrouit. Generic controllability of equivariant systems and applica-
tions to particle systems and neural networks. arXiv preprint arXiv:2404.08289, 2024.
[2] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to imple-
ment preconditioned gradient descent for in-context learning. Advances in Neural Information
Processing Systems, 36, 2024.
[3] Silas Alberti, Niclas Dern, Laura Thesing, and Gitta Kutyniok. Sumformer: Universal approx-
imation for efficient transformers. In Topological, Algebraic and Geometric Learning Workshops
2023, pages 72–86. PMLR, 2023.
[4] Tom Brown,Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredD Kaplan,PrafullaDhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901,2020.
[5] Val´erie Castin, Pierre Ablin, and Gabriel Peyr´e. How smooth is attention? In ICML 2024, 2024.
[6] DavidChiang,PeterCholak,andAnandPillay. Tighterboundsontheexpressivityoftransformer
encoders. In International Conference on Machine Learning, pages 5544–5562.PMLR, 2023.
[7] ChristaCuchiero,MartinLarsson,andJosefTeichmann. Deepneuralnetworks,genericuniversal
interpolation,andcontrolledodes. SIAM Journal on Mathematics of Data Science,2(3):901–919,
2020.
[8] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of
control, signals and systems, 2(4):303–314,1989.
[9] GwendolineDeBie,GabrielPeyr´e,andMarcoCuturi. Stochasticdeepnetworks. InInternational
Conference on Machine Learning, pages 1556–1565.PMLR, 2019.
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
13[11] Dennis Elbr¨achter, Philipp Grohs, Arnulf Jentzen, and Christoph Schwab. Dnn expression rate
analysis of high-dimensional pdes: Application to option pricing. Constructive Approximation,
55(1):3–71,2022.
[12] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for
transformer circuits. Transformer Circuits Thread, 1(1):12, 2021.
[13] Massimo Fornasier, Giacomo E Sodini, and Giuseppe Savar´e. Density of subalgebras of lipschitz
functions in metric sobolev spaces and applications to sobolev-wasserstein spaces. Journal of
Functional Analysis, 285(11), 2023.
[14] Takashi Furuya, Michael Puthawala, Matti Lassas, and Maarten V de Hoop. Globally injective
and bijective neural operators. arXiv preprint arXiv:2306.03982, 2023.
[15] Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. The emergence of
clusters in self-attention dynamics. arXiv preprint arXiv:2305.05465, 2023.
[16] Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. A mathematical
perspective on transformers. arXiv preprint arXiv:2312.10794, 2023.
[17] BorisHaninandMarkSellke. Approximatingcontinuousfunctionsbyrelunetsofminimalwidth.
arXiv preprint arXiv:1710.11278, 2017.
[18] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359–366,1989.
[19] Nicolas Keriven and Gabriel Peyr´e. Universal invariant and equivariant graph neural networks.
Advances in Neural Information Processing Systems, 32, 2019.
[20] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces
with applications to pdes. Journal of Machine Learning Research, 24(89):1–97,2023.
[21] AnastasisKratsios,ValentinDebarnot,andIvanDokmani´c.Smalltransformerscomputeuniversal
metric embeddings. Journal of Machine Learning Research, 24(170):1–48,2023.
[22] Anastasis Kratsios, Chong Liu, Matti Lassas, Maarten V de Hoop, and Ivan Dokmani´c. An
approximationtheoryfor metricspace-valuedfunctions witha view towardsdeeplearning. arXiv
preprint arXiv:2304.12231, 2023.
[23] Anastasis Kratsios and L´eonie Papon. Universal approximation theorems for differentiable geo-
metric deep learning. Journal of Machine Learning Research, 23(196):1–73,2022.
[24] Shengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. Your transformer
may not be as powerful as you expect. Advances in Neural Information Processing Systems,
35:4301–4315,2022.
[25] Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is
provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint
arXiv:2307.03576, 2023.
[26] William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of
thought. arXiv preprint arXiv:2310.07923, 2023.
[27] Luis Mu¨ller, Mikhail Galkin, Christopher Morris, and Ladislav Ramp´aˇsek. Attending to graph
transformers. arXiv preprint arXiv:2302.04181, 2023.
[28] Swaroop Nath, Harshad Khadilkar, and Pushpak Bhattacharyya. Transformers are expressive,
but are they expressive enough for regression? arXiv preprint arXiv:2402.15478, 2024.
[29] AllanPinkus. Approximationtheoryofthemlpmodelinneuralnetworks. Acta numerica,8:143–
195, 1999.
14[30] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point
sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 652–660,2017.
[31] Michael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyr´e. Sinkformers: Transform-
ers with doubly stochastic attention. In International Conference on Artificial Intelligence and
Statistics, pages 3515–3530.PMLR, 2022.
[32] Michael E Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, and Gabriel Peyr´e. How do
transformersperformin-contextautoregressivelearning? arXiv preprint arXiv:2402.05787, 2024.
[33] Lena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin. What formal lan-
guages can transformers express? a survey. Transactions of the Association for Computational
Linguistics, 12:543–561,2024.
[34] PauloTabuadaandBahmanGharesifard. Universalapproximationpowerofdeepresidualneural
networks through the lens of control. IEEE Transactions on Automatic Control, 2022.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[36] JohannesvonOswald,EyvindNiklasson,MaximilianSchlegel,SeijinKobayashi,NicolasZucchet,
NinoScherrer,NolanMiller,MarkSandler,MaxVladymyrov,RazvanPascanu,etal. Uncovering
mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023.
[37] James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of
attention. arXiv preprint arXiv:2007.02876, 2020.
[38] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
[39] Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks,
94:103–114,2017.
[40] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077, 2019.
[41] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-
context. arXiv preprint arXiv:2306.09927, 2023.
[42] Ding-Xuan Zhou. Universalityofdeep convolutionalneuralnetworks. Applied and computational
harmonic analysis, 48(2):787–794,2020.
15A Radon transform and injectivity
Lemma 4. Let Ω R be a compact set, and let µ,ν (Ω). Then,
⊂ ∈P
L (µ)(c)=L (ν)(c), c R, µ=ν.
1 1
∀ ∈ ⇒
where, for k N,
∈ ecyykdµ(y)
L (µ)(c):=
k ecydµ(y)
R
Proof. One has R
L (µ)(c)=L (µ)(c) L (µ)(c)L (µ)(c).
k ′ k+1 k 1
−
So by recursion, we have that
L (µ)(c)=L (ν)(c), c R, L (µ)(c)=L (ν)(c), c R, k 1,
1 1 k k
∀ ∈ ⇒ ∀ ∈ ∀ ≥
So evaluating this at c=0, we obtain that
L (µ)(0)=L (ν)(0), k 1 k, ykdµ(y)= ykdν(y)
k k
∀ ≥ ⇔ ∀
Z Z
which is equivalent to µ=ν.
Lemma 5. Let Ω Rd be a compact set, and let µ,ν (Ω). Then,
⊂ ∈P
L(µ)(a,c)=L(ν)(a,c), a Rd, c R, µ=ν.
∀ ∈ ∀ ∈ ⇒
where
exp(c a, y ) a, y dµ(y)
L(µ)(a,c):= h i h i .
exp(c a, y )dµ(y)
R h i
Proof. We define R
e Sd, µe :=(P ) µ
e ♯
∀ ∈
where Sd is the d-dimensional sphere, and P (x)= x,e is the projection on e. We see that
e
h i
exp(c e, y ) e, y dµ(y) ecssdµe(s)
L(µ)(e,c)= h i h i = .
exp(c e, y )dµ(y) ecsdµe(s)
R h i R
By Lemma 4, we can show that R R
e,(P ) µ=(P ) ν
e ♯ e ♯
∀
which implies that by the injectivity of the Radon transform
µ=ν.
16