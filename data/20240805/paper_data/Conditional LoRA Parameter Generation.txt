Conditional LoRA Parameter Generation
XiaolongJin1∗, KaiWang1∗,† DongwenTang1∗, WangboZhao1,
YukunZhou1, JunshuTang2, YangYou1
1NationalUniversityofSingapore 2ShanghaiJiaoTongUniversity
Code: NUS-HPC-AI-Lab/CONDP-DIFF
Abstract
Generativemodelshaveachievedremarkablesuccessinimage, video, andtext
domains. Inspiredbythis,researchershaveexploredutilizinggenerativemodelsto
generateneuralnetworkparameters. However,theseeffortshavebeenlimitedby
theparametersizeandthepracticalityofgeneratinghigh-performanceparameters.
In this paper, we propose COND P-DIFF, a novel approach that demonstrates
the feasibility of controllable high-performance parameter generation, particu-
larlyforLoRA(Low-RankAdaptation)weights,duringthefine-tuningprocess.
Specifically,weemployanautoencodertoextractefficientlatentrepresentations
forparameters. Wethentrainaconditionallatentdiffusionmodeltosynthesize
high-performing model parameters from random noise based on specific task
conditions. Experimental results in both computer vision and natural language
processing domains consistently demonstrate that COND P-DIFF can generate
high-performanceparametersconditionedonthegiventask. Moreover,weobserve
thattheparameterdistributiongeneratedby COND P-DIFF exhibitsdifferences
comparedtothedistributionobtainedthroughnormaloptimizationmethods,in-
dicatingacertainlevelofgeneralizationcapability. Ourworkpavesthewayfor
furtherexplorationofcondition-drivenparametergeneration,offeringapromising
directionfortask-specificadaptationofneuralnetworks.
1 Introduction
Recentadvancementsingenerativemodels [41,39,44,2]havemarkedsubstantialprogressacross
several domains of artificial intelligence. In the computer vision domain, generative adversarial
networks [12],diffusionmodels [16],andotherapproaches [6,40]haveshownimpressiveresults
inimagesynthesisandmanipulation. Notably,modelssuchasStableDiffusion [41],DALL-E2
[39],andImagen [44]havesetnewbenchmarksinthequalityandresolutionofgeneratedimages.
Moreover, video generation models like Sora [32] have shown promising results in producing
coherentandhigh-qualityvideosequences,openingnewavenuesforapplicationsinentertainment
and media. In the natural language processing domain [37, 22, 55], autoregressive models like
GPT[2]andLlama[51]havedemonstratedpromisinggenerationcapabilitiesandalignmentwith
humanpreference[20,33,38,21],whichunderscorethepotentialofgenerativemodels.
Inspired by these achievements, recent studies [34, 54] have begun to explore the application of
generativemodelsinnovelareas,generatinghigh-performingmodelparameters. Thesestudiesfocus
ondirectlygeneratingnovelmodelparameterstoacceleratethetrainingprocess,uncoveringparame-
tersthatachievecomparableperformancewiththoseobtainedthroughconventionaloptimization
methods.
∗equalcontribution,jinxiaolong1129@gmail.com,kai.wang@comp.nus.edu.sg,mtdovent@gmail.com
†correspondingauthor
Preprint.Underreview.
4202
guA
2
]IA.sc[
1v51410.8042:viXraByharnessingthepowerofgenerativemodels,itispossibletosubstantiallyreducethecomputational
costandtimerequiredformodeloptimization[34,43,24]. Besides,examiningthelatentrelation-
shipsbetweenmodelparametersandperformanceprovidesvaluableinsightsintothebehaviorand
characteristicsofneuralnetworks[13].
However,previousworksonparametergeneration[54,34,50,46,26]faceseverallimitations. On
the one hand, the scale of parameters generated by prior methods [50, 34, 54] is insufficient for
practicalapplications. Forexample,G.pt[34]hasbeenevaluatedonlyonrelativelysimpledatasets
suchasMNISTandCIFAR-10,whichmaynotsufficientlydemonstrateitsgeneralizationability
when applied to more complex tasks, and p-diff [54] can generate small-scale high-performance
parametersforsimplearchitectures. Besides, [46]learnahyper-representationonmodelzoosfor
generativeusetosamplenewsmall-scalemodelweights. Ontheotherhand,previousmethodsdonot
supportconditionalhigh-performanceparametergeneration. P-diff[54]lackssupportforconditional
parameter generation, a crucial feature for real-world applications. Although G.pt [34] enables
controllableparametergenerationasanoptimizer, itcanhardlyexhibitcomparableperformance
comparedtonetworkstrainedbyconventionaloptimizationmethods.
Therefore,despitetheirpromisingpotential,thesemethodsgrapplewithconstraintsaboutparameter
size,practicality,andoverallperformance,whichyieldtheprimaryquestiontobeexploredinthis
paper:(Q)Canwesynthesizehigh-performanceparametersconditionedonthegiventaskpractically?
To enhance the practicality of parameter gen-
eration, two main challenges exist. First, pa- Imagestyle
rametergenerationforcomplexmodelsentails
significantdatapreparationcosts. Forexample,
G.pt [34] requires training 23 million models, LoRA 1
which is infeasible for large models. Second,
controllableparametergenerationischallenging
Task description
due to the difficulty in modeling the distribu-
+
tionofparameters,makingfullparametergen-
Few-shotexamples
erationhighlycomplex. Consequently, wefo- LoRA 2
cusontheconditionalgenerationoffine-tuned
Figure 1: High-performance LoRA parameters
LoRA (Low-Rank Adaptation) parameters in
variousdomainsasLoRAimprovesdownstream
generationprocessbyCONDP-DIFFinvisionand
languagedomains.
taskperformancewithfewparametersandarel-
atively more stable distribution. Specifically,
LoRA [17]isaparameter-efficientfine-tuningtechniquethatadaptspre-trainedmodelstospecific
tasksbylearninglow-rankmatricesthatmodifythemodel’sweights.
Toachievehigh-performancecontrollableconditionalparametergeneration,weproposeConditional
Parameter Diffusion, named COND P-DIFF, which utilizes a standard latent diffusion model to
performconditionalgeneration,synthesizinganewsetofparameterstailoredtospecificconditions.
Specifically,weuseanautoencoderandaconditionallatentdiffusionmodeltocapturethedistribution
ofnetworkweights. First,theautoencoderistrainedonaselectedsetofparametersfrommodels
optimizedwithnormaloptimizationmethods,e.g.,SGD[43],ondifferentdatasets,creatinglatent
representationsoftheseparameters. Second,weutilizeadomain-specificcondition,e.g.,text,style
image, projector to encode the condition information and train a conditional diffusion model to
reconstruct latent representations. Finally, as shown in Figure 1, the trained conditional latent
diffusionmodelCONDP-DIFFgenerateslatentrepresentationsfromrandomnoiseintheinference
processbasedonspecifictaskconditions. Then,thedecoderofthetrainedautoencoderprocesses
thesegeneratedrepresentationstoproducenew,high-performingmodelparameters.
Ourmethodhasthefollowingcharacteristics: i)Itdemonstratescomparableorsuperiorperformance
relativetomodelstrainedwithconventionalmethods,spanningvariousdatasetsandarchitectures.
ii) The parameters generated by our approach significantly differ from the parameters obtained
duringnormaltraining,highlightingitscapabilitytosynthesizenovelparametersratherthanmerely
replicating the training examples. iii) Extensive evaluations demonstarte the robustness of our
approach. OurmethodCOND P-DIFFalsoshowsgeneralizabilityingeneratedhigh-performance
model weights space. We hope that our findings will provide new insights into the potential of
applyingconditionaldiffusionmodelstoparametergenerationandhighlightapromisingdirection
fortask-specificparametergenerationofneuralnetworks.
2
Vision
Language
COND-PDIFF2 Preliminary
2.1 PreliminariesofLoRA
Low-RankAdaptation(LoRA)[17]enhancestheefficiencyoffine-tuninglargepre-trainedlanguage
modelsbyminimizingthecomputationaldemandsusuallyrequiredforfullmodelretraining. LoRA
introduces two trainable matrices, B ∈ Rd×r and A ∈ Rr×k, to each transformer layer. These
matrices, where r is much smaller than hidden layer dimension d and task-specific dimension k,
performalow-rankapproximationofthetypicalupdatesmadeduringfine-tuning. Thecoreideais
thatthenecessaryadjustmentsfortask-specificadaptationhavealow"intrinsicdimension,"allowing
significantreductionsintrainableparameterswhilemaintainingperformance. Thepretrainedweight
matrixW remainsunchanged,withonlyBandAbeingoptimized,thusspeedinguptrainingand
0
decreasingmemoryandcomputationalneeds. ThemodifiedforwardpassinLoRAisrepresentedas:
W x+∆Wx=W x+B(Ax) (1)
0 0
where∆W =BAistheupdate. Initially,Biszero,ensuringnochangestoW ,andAstartswitha
0
smallrandomGaussiandistribution. Indeployment,thelearnedlow-rankmatricesBandAcanbe
integratedintoW . Inthiswork,weaimtosynthesizeLoRAparametersbecauseofthepracticality
0
andeffectiveLoRAfusionthatshowthecontinuousdistributioninLoRAparameterspace.
2.2 PreliminariesofConditionalDiffusionModels
Conditional diffusion models [16, 41, 59] extend the standard diffusion model by incorporating
conditionsintoboththeforwardandreverseprocesses. Thisconditionalinformationdefinedbyc
allowsthemodeltogeneratedatatailoredtospecificattributesorrequirements.
Conditionalforwardprocess: Theforwardprocessinconditionalmodelsinvolvesaddingnoise
toaninitialsamplewhileconditioningonc. Theprobabilityoftransitioningfromx tox under
t−1 t
conditioncismodeledasaGaussiandistribution:
(cid:112)
q(x |x ,c)=N(x ; 1−β x ,β I) (2)
t t−1 t t t−1 t
whereβ arethetimestep-dependentnoiselevels,andIrepresentstheidentitymatrix. Thecomplete
t
forwardprocessconditionedoncisgivenby:
T
(cid:89)
q(x |x ,c)= q(x |x ,c) (3)
1:T 0 t t−1
t=1
ConditionalReverseProcess: Thereverseprocessaimstoreconstructtheoriginalsamplefromits
noisieststatex conditionedonc. Itisformulatedby:
T
p (x |x ,c)=N(x ;µ (x ,t,c),Σ (x ,t,c)) (4)
θ t−1 t t−1 θ t θ t
Inthisprocess,µ andΣ arefunctionsestimatedbyaneuralnetwork,whichalsoprocessesthe
θ θ
conditionc,ensuringthattherecoveryofdatarespectstheconditionalconstraints.
OptimizationandInferencewithConditions: Thetrainingprocedureinvolvesminimizingthe
Kullback-Leibler(KL)divergencebetweentheforwardandreverseconditionaldistributions,specifi-
cally:
L =E [D (q(x |x ,x ,c)∥p (x |x ,c))] (5)
dm q(x0,c) KL t−1 t 0 θ t−1 t
Duringinference,themodelgeneratesnewsamplesbyconditioningoncandsequentiallyapplying
thelearnedreversetransitionsfromanoisedistribution,enablingthegenerationofdatathatclosely
adherestothespecifiedconditions.
3 Methodology
3.1 Overview
Weproposeconditionalparametergenerationtosynthesizenewparameterstailoredtospecifictask
conditions. Fig2illustratesourproposedCONDP-DIFFframework. First,givenatrainingdataset
3ofmodelparameters,weuseanautoencoder[25]toextractlatentrepresentationsoftheparameters
andreconstructthelatentvectorsbydecoder. Then,inspiredby[54],wetrainaconditionallatent
diffusionmodeltogeneratehigh-performanceparametersconditionedonspecifictaskinformation.
Finally,aftertraining,weemployCONDP-DIFFbyfeedingrandomnoiseandtask-specificconditions
intoaconditionalparameterdiffusionmodeltogeneratethedesiredparameters.
Autoencoder training Conditional parameter diffusion model Imagestyle
𝑤 ℰ 𝓏 Diffusion Process 𝓏 𝑇
Text
𝑤 ℰ 𝓏 𝜉 𝓏 𝒟 𝑤ෝ ∙ ∙ T Fa es wk - sd he os tc eri xp at mion p les
𝑤ෝ 𝒟 𝓏 Denoising Net 𝓏 𝑇
𝜏
Conditional parameter generation 𝑤 Parameter
𝓏 Latent Vector
Random noise 𝝐
𝒟 Decoder of AE
Conditional PDM 𝒟 Generated LoRA parameters ℰ Encoder of AE
Task 𝜏 𝜏 Condition Projector
condition Frozen
Trainable
Figure 2: The framework of COND P-DIFF. The autoencoder is employed to extract the latent
representationofLoRAparametersandreducememoryconsumption. Theconditionalparameter
diffusionmodelaimstosynthesizehigh-performanceparametersbasedonspecifictaskconditions.
3.2 Parameterautoencoder
Datasetpreparation. Inthiswork,wefocusonsynthesizingLoRAlearnablematrixparametersof
fine-tunedmodelsbydefault. Toobtainthetrainingdatasetfortheparameterautoencoder,wefine-
tunethepre-trainedmodelusingLoRAonthedatasetfortaskqandcollectN differentcheckpoints
inthelastN steps. WedenotethetrainingdatasetasΘ=[θ ,...,θ ,...,θ ],whereθ represents
1 n N k
theweightsofLoRAforthemodelataspecificfine-tuningstage. Becausethetrainingdatasetfor
CONDP-DIFFcontainsmodelparametersratherthanconventionalimageorlanguagedatasets,we
proposetasknormalization. Specifically,weemployZ-Scorenormalizationontheparametersof
eachtaskindividually[18].
Training procedure. Given a training sample θ , we flatten parameter matrix θ to a one-
n n
dimensionalvectorw ∈ RK×1,whichK isthetotalnumberofparameterweightsofw . Then,
n n
weutilizeanauto-encodertoobtainmeaningfulandrobustlatentrepresentations. Specifically,we
formulatetheprocessasEquation6,whereE andDrepresenttheencoderanddecoderfunctions,
respectively. z is the latent representation of the parameter matrix. wˆ is the reconstruction of
n n
parameter w . To enhance the generalization and robustness of the autoencoder, we introduce
n
Gaussiannoiseξ tothelatentvector. Thefinalauto-encoderprocessisformulatedasfollows:
z
z =E(w )=Encoder(w ) (6a)
n n n
wˆ =D(z )=Decoder(z +ξ ) (6b)
n n n z
Wetraintheautoencoderfunctionbyminimizinglossfunctionbelow.
N
L= 1 (cid:88) ∥w −wˆ ∥2 (7)
N n n
n=1
43.3 Conditionalparametergeneration
Weutilizeaconditionallatentdiffusionmodeltosynthesizehigh-performanceparametersbased
on conditions y such as text and image. To handle different tasks and modalities, we adopt the
domain-specificencoder,whichisdenotedasτ (y;ρ),whereyrepresentstheinputcondition
domain
andρdenotestheencoderparameters. Forexample,intheNLPexperimentsofthiswork,weemploy
thetextdecoderinCLIP[36]. Inspiredbyin-contextlearning,theinputconditionyconsistsofatask
descriptionandtwo-shotexamplestocapturethetaskinformation.Besides,weutilizestylizedimages
asconditionsinstyletransfertasksandadoptResNet[14]toextractstylelatentrepresentationsas
theconditionvector. MoredetailsabouttheconditionareshowninAppendix6.1. Regardingthe
U-Netarchitecture,weapplyone-dimensionalconvolutionsindenoisingautoencodersbecausethe
weightmatrixparametersdonotshowstrongpositionalrelationshipsdifferentfromimageswhere
pixelshavetwo-dimensionalspatialrelationships.
Therefore, given the condition and training parameters samples, we train the conditional latent
diffusionmodelthrough
L :=E (cid:2) ∥ϵ−ϵ (p ,t,τ (y))∥2(cid:3) , (8)
LDM ϵ∼N(0,1),t θ t domain,ρ
whereϵ islearnedviaEq. 8. Finally,afterconditionaldiffusionmodeltraining,wefeedspecific
θ
conditions corresponding to tasks and random noise to reverse the inference process to obtain
high-performingweightsforspecifictasks.
4 Experiment
Inthissection,wefirstshowtheexperimentsetup. Then,wepresenttheevaluationresults,ablation
studies,andanalysisofCONDP-DIFF.
4.1 Experimentsetup
Datasets and metrics. We evaluate our method across various domains. Specifically, in NLP
experiments,wetestonthelanguageunderstandingGLUEbenchmark[53]. InCVexperiments,
we focus on the style-transfer tasks. We use the SemArt and WikiArt datasets [10, 45], which
containdiverseartisticimages,andevaluatethemusingtheFréchetInceptionDistance(FID,[15],as
employedbyStyleGAN[23],withlowerscoresindicatingbetterperformance.
Datasetcollectingandtrainingprocedures. InNLPexperiments,wecollect150trainingsamples
for models, including BERT, Roberta, GPT-2 by default. For instance, in the case of BERT, we
fixedpre-trainedparametersandfine-tunedthenetworkusingLoRA.Specifically,weconductthe
hyperparametersearchforfixedvaluesofrandαandselectthefine-tuninghyperparametersthat
yieldthebestaverageperformance. Duringthefine-tuningprocess,wesavethecheckpointsofthe
last 150 steps as the training dataset, which includes the LoRA learnable matrix weights. In the
frameworkofCONDP-DIFF,theautoencoderincludes1DCNN-basedencodersanddecoders. We
utilizethetextencoderfromCLIPastheconditiontextencoder. Inimagestyletransfertasks,we
fine-tuneattentionmodulesofapopulartext-to-imagemodel,PIXART-αmodel[4]usingLoRAand
collectedthelast64LoRAcheckpointsofthetrainingprocessoncein10steps. Intheframework
ofCONDP-DIFF,weusedpre-trainedResNet18toextractstylelatentastheconditionvector. All
experimentswereconductedontheLinuxserverwithfourNVIDIAA100GPUs. Thenoiseξ is
z
Gaussiannoisewithanamplitudeof0.001bydefault. DetailedtraininghyperparametersforLoRA
fine-tuningandCONDP-DIFFframeworkareprovidedinAppendixB.
Inference procedures. In NLP tasks, we generate 20 LoRA parameters for each task using a
conditionaldiffusionmodelthroughrandomnoiseandmergethesegeneratedparametersintothe
pre-trainedmodel. Weselectthemodelthatexhibitsthebestperformanceonthetrainingdataset
andreportitsperformanceonthevalidationdataset. Instyle-transfertasks,wesynthesizeLoRA
parametersofthecorrespondingstylesbyfeedingtheconditionaldiffusionmodelwithimagesin
variousstylesasconditions. WethenmergeparameterswithPIXART-α’sandutilizethemtogenerate
imagesusingasetofprompts. Finally,wecomputetheFIDscoreofthegeneratedimages.
Baselines. 1)original: Thebestvalidationperformanceamongtheoriginallytrainedmodels. 2)
modelsoup: Thevalidationperformanceofthemodelwhoseweightistheaverageofthetraining
dataset. BecauseMitchelletal. [57]showsaveragingtheweightsoffine-tunedmodelswithdifferent
5hyperparameterconfigurationsoftenimprovesaccuracyandrobustness. Instyle-transferexperiments,
weintroduceanadditionalbaselineno-lora: wedirectlyemploythepredefinedPIXART-αmodelto
demonstratetheeffectivenessofLoRAfine-tuninginstyle-transfertasks.
4.2 Experimentresults
COND P-DIFFcangeneratehigh-performanceparametersbasedontaskconditions. Table1
presentscomparisonresultsofCONDP-DIFFandbaselinemethodsacrosslanguageunderstanding
GLUEbenchmarkforthreemodelswithdifferentLoRAconfigurations. Weobservethat COND
P-DIFF consistently yields comparable performance in most scenarios, demonstrating it learns
conditionalparameterdistributionseffectivelyandstably.Besides,wenotethatthebaselineaverage’s
performanceinsomecasessurpassesthebaseline,validatingthepotentialofmodelaveragingto
enhanceperformance[57].
Table2illustratestheresultsofCOND P-DIFFandthebaselineintheimagestyletransfertaskfor
differentstyles. WeemploytheFID[15]toquantitativelyassessthequalityofstyle-conditioned
imagegeneration. LowerFIDrepresentsbetterimagegenerationquality. Basedonourfindings,
CONDP-DIFFefficientlysynthesizesspecificstyle-adaptedLoRAparameterstogeneratehigh-quality
images. AdditionalvisualresultsareshowninFigure3(a). ThisdemonstratesthatCONDP-DIFFcan
practicallygeneratehigh-performancemodelparametersbasedonspecificconditions.
Table1:ResultsofCONDP-DIFFonGLUE.Wepresentresultsintheformatof’CONDP-DIFF/orginal/model
soup’. CONDP-DIFFobtainscomparableorevenbetterperformancethanbaselines.’Size’istheparametersize
ofLoRA.’Rank’istheparameterrinLoRA.Full’representsfullyfine-tuningresults.
Model Rank Size SST2 RTE MRPC COLA QNLI STSB Average
1 73728 91.6/91.6/90.8 57.4/58.9/57.9 87.2/83.4/83.9 52.4/52.6/52.1 88.7/88.7/88.1 81.8/81.4/81.7 76.5/76.1/75.8
2 147456 91.4/91.4/91.5 57.5/59.9/60.1 87.3/85.1/85.5 51.4/51.3/50.7 88.6/88.1/87.4 82.6/81.6/81.7 76.5/76.2/76.2
4 294912 91.6/91.9/92.0 62.7/63.2/62.8 85.4/85.4/85.5 53.7/53.4/52.5 89.8/89.6/88.9 80.6/80.9/80.7 77.3/77.4/77.1
BERT 16 1179648 92.1/91.6/91.5 64.2/64.3/64.5 87.4/87.0/86.8 56.9/57.0/57.5 89.8/90.1/90.2 83.8/83.3/82.3 79.0/78.9/78.8
Full 109482240 93.5 66.4 88.9 52.1 90.5 85.8 79.5
1 73728 93.3/93.7/94.1 65.6/68.6/68.0 86.9/84.7/85.0 49.8/50.2/50.5 92.4/92.0/91.4 87.3/87.5/86.9 79.2/79.4/79.3
2 147456 93.5/93.7/93.8 63.2/68.2/68.3 87.7/85.0/84.6 50.3/50.7/50.6 92.8/92.5/92.2 86.8/87.3/87.6 79.0/79.6/79.5
RoBERTa 4 294912 93.8/93.5/93.1 69.8/69.7/69.5 87.9/88.3/87.9 54.1/54.0/54.1 92.0/92.4/92.9 88.3/88.2/88.6 81.0/81.0/81.0
Full 124645632 94.8 78.7 90.2 63.6 92.8 91.2 85.2
1 92160 94.4/94.4/94.7 61.4/61.0/61.5 84.0/84.0/83.2 56.8/57.0/56.1 92.4/92.8/92.1 87.4/87.8/87.0 79.4/79.5/79.1
DeBERTa 2 184320 94.9/94.8/94.0 62.2/62.1/62.0 86.2/85.8/86.2 58.6/58.3/57.4 92.1/92.0/92.1 85.2/85.2/84.5 79.9/79.4/79.4
4 368640 94.6/94.5/94.7 63.2/62.8/61.9 87.1/86.9/86.2 60.3/60.3/59.9 93.4/93.5/93.1 88.7/88.7/88.7 81.2/81.1/80.7
Table2:FIDresultsofimage-transfertasks.Lower
Table3: Ablationresultsoftrainingdatasetsize
FIDisbetter.Bestresultsarebolded.
N.LargerN canenhanceperformances.
Style original modelsoup no-Lora CONDP-DIFF
N SST2 STSB MRPC
VanGogh 27.92 28.08 102.95 28.03
Edvard 27.10 27.13 96.18 26.98 1 90.23 80.71 82.71
Chalk 36.22 36.00 171.82 36.18 100 91.63 80.91 83.52
Charcoal 40.80 40.19 132.76 40.60 200 91.63 81.81 87.24
500 91.63 81.80 87.25
Average 33.01 32.86 125.93 32.94
4.3 Ablationstudy
Inthissection,weconductmultipleablationstudiestoreportthecharacteristicsofCONDP-DIFF.
WefocusontheperformanceofgeneratedLoRAparameters(rankr =1)ofBERTonSST2,RTE,
andMRPCdatasets. ThetrainingsettingisthesameasexperimentsTable1.
SizeofthetrainingdatasetAsdescribedinSection3.2,wecollectN differentcheckpointsinthe
lastN stepsasatrainingdatasetfortaskqusingLoRA.Weexploretherelationshipbetweendataset
sizeN andperformanceinTable3. Weobservethattheperformanceimprovesasthesizeofthe
trainingdatasetincreases. Specifically,alargertrainingdatasetcanprovideabroaderexploration
space,therebyenabling COND P-DIFF togeneratehigherperformanceparameters. Forinstance,
performanceontheMRPCtaskimprovedby4.53%.
6Table4: Ablationstudiesof COND P-DIFF. Weablatethenormalizationmethodsinthetraining
process,theconditionrepresentation,andthelocationofemploying COND P-DIFF. TheDefault
settingsinCONDP-DIFFaremarkedin gray. Boldentriesarebestresults.
(a) Comparisonamongno norm., (c) COND P-DIFF is effective in
batch norm. and task norm.. (b) FewshotexamplesboostCOND certain blocks but can boost per-
task norm. canimproveperfor- P-DIFFcapabilitywithtaskinforma- formanceonwholeLoRAparame-
mance. tiondescription. ters.
Norm. SST2 STSB MRPC Condtion SST2 STSB MRPC LoRAlayers SST2 STSB MRPC
nonorm. 55.67 49.07 47.01 one-hot 90.05 77.12 80.34 0-1 91.63 81.43 83.45
batchnorm. 90.60 80.90 82.50 learnablevector 90.10 80.03 81.81 0-4 91.63 81.45 83.61
tasknorm. 91.63 81.81 87.24 taskinfo 90.25 80.32 81.98 0-8 91.63 81.80 85.61
taskinfo+few-shot 91.63 81.81 87.24 0-11 91.63 81.81 87.24
NormalizationapproachAsdescribedinSection3.2,weusetasknormalizationmethod. Table4(a)
showstheimpactsofdifferentnormalizationstrategiesonperformance,includingnonorm.,batch
norm.,andtasknorm.. Specifically,tasknorm. referstonormalizingtheparameterscorrespondingto
eachtaskindividually. batchnorm. representsbatchnormalization. TheexperimentalsetupinTable
4(a)isconsistentwiththatoftheexperimentinTable1. Wefindthattasknorm. consistentlyyields
thebestaverageperformance. nonorm. leadstotheworstperformancebecausethewidevariancein
weightdistributionsacrossdifferenttasksandoutliershinderstheconvergenceoftheautoencoder.
Besides,batchnorm. performedinferiortotasknorm.,asitintroducesspuriouscorrelationsamong
parametersacrossdifferenttasks.
ConditioninformationTherepresentationoftheconditioncriticallyaffectsgenerationresults. We
explorehowtorepresentthetaskconditioneffectivelytoguideconditionalparametergeneration,
asdetailedinTable4(b). Ourapproachcategorizesrepresentationsintofourtypes: usingone-shot
vectors, using only the task description, using only two-shot examples, and using both the task
description and two-shot examples. Table 4(b) shows that combining the task description with
examplesyieldsbetteroutcomes,suggestingthatin-contextlearningcanprovidemoreinformationto
establishrelationshipswiththeweightparameters.
WhichpartofparameterstosynthesisWegenerateLoRAparametersforallblocksbydefaultin
Table1.ToexploretheeffectivenessofCONDP-DIFFondifferentblocks,wepresenttheperformance
whengeneratingLoRAparametersforonlycertainblocks. TheexperimentsinTable4(c)illustrate
thatthemethodismoreeffectivewhengeneratingparametersforallblocks. Wehypothesizethatas
thenumberofsynthesizedparametersincreases,themodelhasalargerexplorationspace,thereby
boostingperformance. Conversely,performanceisconstrainedbytheexplorationspaceandoriginal
parameterswhenfocusingononlyasubsetofparameters.
81.8
Style Generated
SST2-Ori. 81.6
S RTS ET -2 O-G rie .n. 81.4 O STri Sg Bin /a Cl m one dt -r Pic
d
ir fa fnge
RTE-Gen. 81.2
MRPC-Ori.
MRPC-Gen. 81.0
0.025 0.026 0.027 0.028 0.029 0.030 0.031
87.0
86.0 Original metric range
85.0 MRPC / Cond-Pdiff
84.0
A man wearing A train is moving along
glasses with a smile a stretch of track 83.0
(a) Visualizationofimagesgenerated (b) t-SNEoftheLoRApa- 0.065 0.066 0.0 L6 27 Sim0.0 il6 a8 rity0.069 0.070 0.071
byCOND P-DIFFparametersinstyle rametersoforiginalmodel(c) Similaritycomparisonsoffine-
transfertasks andgeneratedparameters. tuned parameters and parameters
generatedbyCONDP-DIFF
Figure3: (a)visualizetheimagesgeneratedbyCONDP-DIFFsyntheticparametersinstyletransfer
tasks. (b) shows the t-SNE of LoRA parameters of the original models, COND P-DIFF models
onthreedatasetsCOLA,QNLI,andSTSB.(SST2-Ori. meansoriginalparametersandSST-Gen.
meansgeneratedparameters)(c)displaystheaccuracyandsimilarityoffine-tunedperformanceand
parametersgeneratedbyCONDP-DIFF.
7
cirteM
cirteMStyle-1 Generated Style-2
Original
Model soup
Start point
End point
𝜆=0.0 𝜆=0.2 𝜆=0.5 𝜆=0.8 𝜆=1.0
(a) Visualizationoftheinterpolationoftwogeneratedparametersin(b) Visualization of parame-
differentstyles. ter generation trajectories of
CONDP-DIFFinstyle-transfer
tasks.
Figure4: (a)visualizesimagesgeneratedbyinterpolatedparametersbetweenStyle-1andStyle-2. As
λincreasesfromlefttoright,thestylegraduallyshiftstowardsStyle-2fromStyle-1. (b)exhibitsthe
generatedparameters’trajectoryatdifferenttimestepsduringtheinferencestageusingt-SNEfrom
fiverandomnoisestartpointsinimage-transfertasks.
4.4 Analysis
In this section, we conduct a detailed analysis of COND P-DIFF. Specifically, we explore two
criticalquestions: First,doesCONDP-DIFFmerelyreplicatetrainingdata,orcanitgeneratehigh-
performance model parameters that are distinct from the originals? Second, does the generated
parameterspaceofCONDP-DIFFhavegeneralizability?
CONDP-DIFFisnotmerelycloningmodelparameters.
Similarityvs. Performance First,wecalculatetheL distancebetweenthegeneratedandoriginal
2
parameters. Figure3(c)illustratestherelationshipbetweenthesimilarityofthegeneratedparameters
andperformance. Weobservethat COND P-DIFF attainsvarioussimilaritiesandachievesbetter
performancecomparedtooriginalfine-tunedweightsacrossvariousdatasets.
ParameterdistributionWeemployt-SNE[52]toanalyzethedistributionsofgeneratedparameters
andoriginalweightsoffine-tunedmodelsondatasetsCOLA,QNLI,andSTSB,asshowninFigures
3(b). We observe that the distribution of generated parameters by COND P-DIFF significantly
differsfromtheoriginalparameters. Thedistributionoftheoriginalparameterscanbeviewedas
followingthetrajectoryoftheoptimizationprocess. Incontrast,CONDP-DIFFgeneratesnovelhigh-
performanceparametersbylearningthedistributionofparameters. Besides,thehigh-performance
parametersgeneratedby COND P-DIFF aredispersedmorebroadly, underscoringthegenerative
model’spotentialtoidentifynovelhigh-performanceparametersbeyondtraditionaloptimization
pathways. Interestingly,thehigh-performanceparameterdistributionsgeneratedbyCONDP-DIFF
forthethreedatasetsareverysimilar,demonstratingthenecessityofexploringthehigh-performance
parameterspace.
TrajectoriesofCONDP-DIFFprocess. Figure4(b)visualizesthegeneratedparametersatdifferent
timestepsduringtheinferencestageusingt-SNE[52]toexplorethegenerationprocessintheimage
style-transfertasks. Wedisplayfivetrajectoriesinitializedfromfivedifferentrandomnoisesand
presentthemodelsoupandtheoriginalmodelparameters. Theparametersderivedfromthemodel
souparelocatedneartheoriginalparameters. Weobservethatthegeneratedparametersgradually
approachtheoriginalparametersbutultimatelymaintainsomedistancefromthem,indicatingthat
COND P-DIFF generates high-performance parameters that are distributed differently from the
original parameters rather than directly replicating them. The variations in the trajectories also
demonstratetherobustnessofCONDP-DIFF.
Generalizability We examine the generalization of the generated parameter space in the task of
imagestyletransfer. Weselectparameters,θ style1andθ style2,generatedbyCONDP-DIFFconditioned
twodistinctstyles, style1andstyle2. Tointerpolatebetweenthesestyles, wecomputeanewset
ofparametersθ asθ =(1−λ)θ +λθ ,whereλ∈[0,1]istheinterpolationfactor.
interp interp style1 style2
Subsequently, we evaluate the effectiveness of θ in style transfer. Figure 4(b) illustrates the
interp
8visualizationofimagesgeneratedbyinterpolatedparametersbetweenStyle-1andStyle-2. Asλ
increasesfromlefttoright,thestylegraduallyshiftstowardsStyle-2. Thecontinuousstylechange
demonstratesthegeneralizationofthegeneratedparameterspace. Wealsoexplorethegeneralization
oftheconditionspaceintheAppendixC
5 Relatedwork
DiffusionmodelsDiffusionmodels[16,5,35]haverecentlyemergedasapowerfulclassofgenerative
models,enablinghigh-fidelitysynthesisofcomplexdatadistributions. Theresearchonthediffusion
modelcanbegenerallyclassifiedintofourcategories. Thefirstcategoryaimstoenhanceimage
synthesisquality[41,39,44]Second,researchersfocusonacceleratingthesamplingprocess[49,28].
Third,recentresearchhasalsofocusedonreevaluatingdiffusionmodelsthroughthelensofcontinuous
analysislikescore-basedgenerativemodeling[8].Fourth,thesuccessofdiffusionmodelshassparked
theirapplicationinvariousdomains,[27,29,56]. Inthiswork,weexploretheconditionaldiffusion
modelintheparametergenerationdomain.
ConditionalgenerationConditionalgenerationhasgainedsignificantattentionincomputervision
andnaturallanguageprocessing. Threeprominentframeworkshaveemerged: conditionalGANs[31,
19,60],conditionalVAEs[48,58],andconditionaldiffusionmodelsxw[41,16],whichincorporate
conditionstoguidethegenerationprocess,enablingthecreationofvisuallycoherentandsemantically
meaningfuldatasamples. ConditionalGANsincorporateconditioninformationintoGANtogenerate
imagesconditionedonspecificattributesorlabels. Conditionaldiffusionmodelstakethisfurther
bygeneratingvisuallycoherentandsemanticallymeaningfulimagesfromthetextualdescription,
demonstrating superior image synthesis quality compared to GANs. Building upon the success
ofconditionaldiffusionmodels,weproposetoextendthisapproachtogeneratingneuralnetwork
parametersbasedonspecificconditions.
Parameter generation The field of parameter generation has seen significant progress in recent
years,withHyperNetworks([13]andgenerativemodelsofneuralnetworkcheckpoints[34]emerging
aspromisingapproaches. [13]introducedHyperNetworks,whichusesahypernetworktolearnthe
parametersforanotherneuralnetwork. [9]proposesModel-AgnosticMeta-Learning,whichlearns
aninitializationforefficientfine-tuning. [34]introducethemodelG.pttopredictthedistribution
overparameterupdatesgivenaninitialinputparametervectorandapromptedlossorerror. [46]
trainedautoencoderonamodelzootolearnahyper-representationforgenerativeusetosamplenew
modelweights[26]useaGNN-basedmodeltosamplenetworkparameters. [7]directlyleverages
MLPweightsandgeneratesneuralimplicitfieldsencodedbysynthesizedMLPweights. [54]usesa
diffusionmodeltogeneratehigh-performingneuralnetworkparametersacrossvariousarchitectures
anddatasets. Differentfromthepreviousworks,wefocusonconditionalparametergenerationto
generatehigh-performingweightsbasedonspecifictaskconditionspractically.
6 Conclusion
Inthiswork,weproposedanapproachCONDP-DIFFforhigh-performancecontrollableparameter
generation, specially for LoRA parameters. We utilize an autoencoder and a conditional latent
diffusionmodeltocapturethedistributionofhigh-performingparametersandperformconditional
generation, synthesizing a new set of parameters tailored to specific conditions. We show that
our method can efficiently synthesize novel and high-quality model parameters. The parameter
distributiongeneratedbyCONDP-DIFFexhibitsdifferencescomparedtothedistributionobtained
throughconventionaloptimizationmethods,indicatingacertainlevelofgeneralizationcapability.
6.1 Limitationandfuturework
Nonetheless,itisessentialtorecognizethatdiffusioninparametergenerationisstilllargelyunex-
ploreddespitethesignificantadvancesintherealmofimageandvideosynthesis. Inthiswork,we
presentapreliminarymethodologyforconditionalparameterdiffusion. However,severalchallenges
remainunresolved,includingreducingmemorydemandsforlargemodelarchitectures,enhancing
thegeneralizabilityofgenerationtechniques,andimprovingtherepresentationofdatasetconditions.
Furthermore,integratingknowledgegraphswithconditionaldiffusionofferspromisingdirectionsfor
controllingconditionalgeneration.
9References
[1] Md.BahadurBadsha,EvanAMartin,andAudreyQiuyanFu. Mrpc: Anrpackageforaccurate
inferenceofcausalgraphs,2018.
[2] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. NeurIPS,33:1877–1901,2020.
[3] DanielCer,MonaDiab,EnekoAgirre,InigoLopez-Gazpio,andLuciaSpecia. Stsbenchmark.
https://paperswithcode.com/dataset/sts-benchmark,2017. ACL.
[4] JunsongChen,JinchengYU,ChongjianGE,LeweiYao,EnzeXie,ZhongdaoWang,James
Kwok,PingLuo,HuchuanLu,andZhenguoLi. Pixart-$\alpha$: Fasttrainingofdiffusion
transformerforphotorealistictext-to-imagesynthesis. InICLR,2024.
[5] PrafullaDhariwalandAlexNichol. DiffusionModelsBeatGANsonImageSynthesis,June
2021.
[6] LaurentDinh,DavidKrueger,andYoshuaBengio. Nice: Non-linearindependentcomponents
estimation. arXivpreprintarXiv:1410.8516,2014.
[7] Ziya Erkoç, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. Hyperdiffusion:
Generatingimplicitneuralfieldswithweight-spacediffusion. InICCV,pages14300–14310,
2023.
[8] BerthyT.Feng,JamieSmith,MichaelRubinstein,HuiwenChang,KatherineL.Bouman,and
WilliamT.Freeman. Score-BasedDiffusionModelsasPrincipledPriorsforInverseImaging,
August2023.
[9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast
AdaptationofDeepNetworks,July2017.
[10] NoaGarciaandGeorgeVogiatzis. HowtoReadPaintings: SemanticArtUnderstandingwith
Multi-ModalRetrieval,October2018.
[11] LeonA.Gatys,AlexanderS.Ecker,andMatthiasBethge. ImageStyleTransferUsingConvolu-
tionalNeuralNetworks. InCVPR,pages2414–2423,2016.
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair,AaronCourville,andYoshuaBengio. Generativeadversarialnets. NeurIPS,27,2014.
[13] DavidHa,AndrewDai,andQuocVLe. Hypernetworks. arXivpreprintarXiv:1609.09106,
2016.
[14] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages770–778,2016.
[15] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter.
Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. NeurIPS,30,
2017.
[16] JonathanHo,AjayJain,andPieterAbbeel.DenoisingDiffusionProbabilisticModels,December
2020.
[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprintarXiv:2106.09685,2021.
[18] SergeyIoffeandChristianSzegedy. Batchnormalization: Acceleratingdeepnetworktraining
byreducinginternalcovariateshift. InICML,pages448–456.pmlr,2015.
[19] PhillipIsola,Jun-YanZhu,TinghuiZhou,andAlexeiA.Efros. Image-to-ImageTranslation
withConditionalAdversarialNetworks,November2018.
10[20] XiaolongJin,ZhuoZhang,andXiangyuZhang. Multiverse: Exposinglargelanguagemodel
alignmentproblemsindiverseworlds. arXivpreprintarXiv:2402.01706,2024.
[21] SauravKadavath,TomConerly,AmandaAskell,TomHenighan,DawnDrain,EthanPerez,
NicholasSchiefer,ZacHatfield-Dodds,NovaDasSarma,EliTran-Johnson,etal. Language
models(mostly)knowwhattheyknow. arXivpreprintarXiv:2207.05221,2022.
[22] JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,
ScottGray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguage
models. arXivpreprintarXiv:2001.08361,2020.
[23] TeroKarras,SamuliLaine,andTimoAila.AStyle-BasedGeneratorArchitectureforGenerative
AdversarialNetworks,March2019.
[24] DiederikPKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXivpreprint
arXiv:1412.6980,2014.
[25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
[26] BorisKnyazev,MichalDrozdzal,GrahamW.Taylor,andAdrianaRomero-Soriano. Parameter
PredictionforUnseenDeepArchitectures,October2021.
[27] ZhifengKong,WeiPing,JiajiHuang,KexinZhao,andBryanCatanzaro. DiffWave:AVersatile
DiffusionModelforAudioSynthesis,March2021.
[28] ChengLu,YuhaoZhou,FanBao,JianfeiChen,ChongxuanLi,andJunZhu. DPM-Solver:
AFastODESolverforDiffusionProbabilisticModelSamplinginAround10Steps,October
2022.
[29] ShitongLuoandWeiHu. DiffusionProbabilisticModelsfor3DPointCloudGeneration,June
2021.
[30] AndrzejMac´kiewiczandWaldemarRatajczak.Principalcomponentsanalysis(pca).Computers
&Geosciences,19(3):303–342,1993.
[31] MehdiMirzaandSimonOsindero. ConditionalGenerativeAdversarialNets,November2014.
[32] OpenAI. Sora,2024. Accessed: 2024-05-08.
[33] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelsto
followinstructionswithhumanfeedback. NeurIPS,35:27730–27744,2022.
[34] WilliamPeebles,IlijaRadosavovic,TimBrooks,AlexeiA.Efros,andJitendraMalik. Learning
toLearnwithGenerativeModelsofNeuralNetworkCheckpoints,September2022.
[35] WilliamPeeblesandSainingXie. ScalableDiffusionModelswithTransformers,March2023.
[36] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML,pages8748–8763.PMLR,2021.
[37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[38] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,and
ChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel.
NeurIPS,36,2024.
[39] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchical
Text-ConditionalImageGenerationwithCLIPLatents,April2022.
[40] DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagation
andapproximateinferenceindeepgenerativemodels. InICML,pages1278–1286.PMLR,
2014.
11[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-ResolutionImageSynthesiswithLatentDiffusionModels,April2022.
[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
biomedicalimagesegmentation. CoRR,abs/1505.04597,2015.
[43] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747,2016.
[44] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,Seyed
KamyarSeyedGhasemipour,BurcuKaragolAyan,S.SaraMahdavi,RaphaGontijoLopes,Tim
Salimans,JonathanHo,DavidJ.Fleet,andMohammadNorouzi. PhotorealisticText-to-Image
DiffusionModelswithDeepLanguageUnderstanding,May2022.
[45] BabakSalehandAhmedElgammal. Large-scaleClassificationofFine-ArtPaintings: Learning
TheRightMetriconTheRightFeature,May2015.
[46] Konstantin Schürholt, Boris Knyazev, Xavier Giró-i Nieto, and Damian Borth. Hyper-
representations as generative models: Sampling unseen neural network weights. NeurIPS,
35:27906–27920,2022.
[47] RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherD.Manning,AndrewNg,
andChristopherPotts. Recursivedeepmodelsforsemanticcompositionalityoverasentiment
treebank. InProceedingsofthe2013ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages1631–1642,Seattle,Washington,USA,October2013.ACL.
[48] KihyukSohn, HonglakLee, andXinchenYan. LearningStructuredOutputRepresentation
usingDeepConditionalGenerativeModels. InNeurIPS,volume28.CurranAssociates,Inc.,
2015.
[49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models,
October2022.
[50] BedionitaSoro,BrunoAndreis,HayeonLee,SongChong,FrankHutter,andSungJuHwang.
Diffusion-basedneuralnetworkweightsgeneration. arXivpreprintarXiv:2402.18153,2024.
[51] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[52] LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. JMLR,9(11),2008.
[53] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelRBowman.
Glue: Amulti-taskbenchmarkandanalysisplatformfornaturallanguageunderstanding. arXiv
preprintarXiv:1804.07461,2018.
[54] KaiWang,ZhaopanXu,YukunZhou,ZelinZang,TrevorDarrell,ZhuangLiu,andYangYou.
Neuralnetworkdiffusion. arXivpreprintarXiv:2402.13144,2024.
[55] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou, etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
NeurIPS,35:24824–24837,2022.
[56] JuliaWolleb,FlorentinBieder,RobinSandkühler,andPhilippeC.Cattin. DiffusionModelsfor
MedicalAnomalyDetection,October2022.
[57] MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,
AriSMorcos,HongseokNamkoong,AliFarhadi,YairCarmon,SimonKornblith,etal. Model
soups: averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasing
inferencetime. InICML,pages23965–23998.PMLR,2022.
[58] Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2Image: Conditional
ImageGenerationfromVisualAttributes,October2016.
12[59] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusionmodels. InICCV,pages3836–3847,2023.
[60] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired Image-to-Image
TranslationusingCycle-ConsistentAdversarialNetworks,August2020.
13A Detailedrelatedwork
DiffusionmodelsDiffusionmodelshaveemergedasapowerfulclassofgenerativemodels,enabling
high-fidelitysynthesisofcomplexdatadistributions. Diffusionmodelsarebasedonnon-equilibrium
thermodynamics, which gradually add noise to data and learn to reverse the diffusion process to
generatesamples.[16,5,35]Theresearchonthediffusionmodelcanbegenerallyclassifiedintofour
categories. Thefirstcategoryaimstoenhanceimagesynthesisquality,asdemonstratedbynotable
modelssuchasStableDiffusion[41],DALL·E2[39],andImagen[44]byleveragingtechniqueslike
CLIP-basedtextencoders,latentspacediffusion,andhierarchicalarchitectures. Second,researchers
focusonacceleratingthesamplingprocess,withkeydevelopmentsincludingDenoisingDiffusion
ImplicitModels[49]andDPM-Solver[28]. Theseapproachesaimtoimprovethecomputational
efficiencyofdiffusionmodelsthroughdeterministicsampling,closed-formexpressions,andnumeri-
calODEsolvers. Third,recentresearchhasalsofocusedonreevaluatingdiffusionmodelsthrough
thelensofcontinuousanalysislikescore-basedgenerativemodeling[8]incontinuous-timesettings.
Fourth,thesuccessofdiffusionmodelshassparkedtheirapplicationinvariousdomains,including
text-to-speechsynthesis[27],3Dshapegeneration[29],andanomalydetectioninmedicalimages
[56], demonstrating the potential of diffusion models beyond image synthesis. In this work, we
exploretheconditionaldiffusionmodelintheparametergenerationdomain.
ConditionalgenerationConditionalgenerationhasgainedsignificantattentioninmachinelearning,
particularlyincomputervisionandnaturallanguageprocessing. Threeprominentframeworkshave
emerged: conditional GANs [31, 19, 60], conditional VAEs [48, 58], and conditional diffusion
models[41,16],whichincorporateconditionstoguidethegenerationprocess,enablingthecreation
of visually coherent and semantically meaningful data samples. Conditional GANs incorporate
condition information into GAN to generate images conditioned on specific attributes or labels.
Conditional diffusion models take this further by generating visually coherent and semantically
meaningful images from the textual description, demonstrating superior image synthesis quality
comparedtoGANs. Buildinguponthesuccessofconditionaldiffusionmodels,weproposetoextend
thisapproachtogeneratingneuralnetworkparametersbasedonspecificconditions.
Parameter generation The field of parameter generation has seen significant progress in recent
years,withHyperNetworks([13]andgenerativemodelsofneuralnetworkcheckpoints[34]emerging
aspromisingapproaches. [13]introducedHyperNetworks,whichusesahypernetworktolearnthe
parametersforanotherneuralnetwork. [9]proposesModel-AgnosticMeta-Learning,whichlearns
aninitializationforefficientfine-tuning. [34]introducethemodelG.pttopredictthedistribution
overparameterupdatesgivenaninitialinputparametervectorandapromptedlossorerror. [46]
trainedautoencoderonamodelzootolearnahyper-representationforgenerativeusetosamplenew
modelweights[26]useaGNN-basedmodeltosamplenetworkparameters. [7]directlyleverages
MLPweightsandgeneratesneuralimplicitfieldsencodedbysynthesizedMLPweights. [54]usesa
diffusionmodeltogeneratehigh-performingneuralnetworkparametersacrossvariousarchitectures
anddatasets. Differentfromthepreviousworks,wefocusonconditionalparametergenerationto
generatehigh-performingweightsbasedonspecifictaskconditionspractically.
B Experimentsetup
In this section, we show detailed experiment setups, including dataset information and training
configuration.
B.1 Styletransferexperiments
Inthissection,weprovidedetailedinformationaboutthetrainingconfigurationsusedforboththe
autoencoderandthediffusionmodelinthestyletransfertask.
Autoencoderconfiguration:Theencoderisa1DCNN-basedmodelwherethechannelofeachlayer
is(16,32,64,128,256,384,512,768,1024,64). Atthebottomlayer,weflattentheparametersand
mapthemtoalatentdimensionof256withalinearlayer. Inthedecoderpart,weusetransposed
convolutionswiththesamenumberofchannelsandlayerstoupsamplebacktotheoriginalshape.
Thetrainingdetailsofhyperparametersareasfollows: totalnumberofparameters516,096,kernel
sizeforCNNmodel9,learningrate2×10−4 withcosineannealing,totaltrainingsteps12,000,
14batchsize64. Inaddition,toreducememoryusageandacceleratecomputations,mixed-precisionis
enabledwithbfloat16forthefirst75%ofthetrainingprocess.
DiffusionModelconfiguration: ThearchitectureoftheDDPMcomprisesa1DCNN-basedU-Net
[42]withchannels(64,128,256,512,768,1024,1024,32). Afullyconnectedlayerisappliedatthe
bottomoftheU-Netafterflattening. InadditiontotheU-Net,weemployastylefeatureextraction
networkastheconditionprojector,consistingoftwoconvolutionallayers,anaveragepoolinglayer,
andafullyconnectedlayer. Theextractedfeaturesareaddedasembeddingstothebottomlayerof
theU-Net. Thetrainingdetailsofhyperparametersareasfollows: kernelsizeforCNNmodel3,
learningrate5×10−4 withcosineannealing,totaltrainingsteps50,000,batchsize128,number
ofdiffusionsteps1,000,β inthediffusionmodelshiftedlinearlyfrom0.0001to0.02indiffusion
models. AndthesameasAEtraining,mixed-precisionisenabledwithbfloat16forthefirst75%of
thetrainingprocess.
Conditional Parameter Generation (ours)
Imagestyle
𝝉 Style condition projector
𝓓 Decoder of AE
𝝉
Noise 𝝐 Conditional PDM 𝓓 Generated LoRA parameters
Image Generation Generated
Prompt: “an elephant and a man.” PixArt-𝛼
Figure5: CONDP-DIFFframeworkinstyle-transfertasks.
Framework: Thissectiondescribestheframeworkandworkflowofthestyletransfertaskwithour
conditionalparametergenerationindetail,asillustratedinFigure5.
DataPreparation: Thefirststepisselectingappropriatedata,includingstyleimageandparameter
data. Forstyleimagedata,weselectatotalof16groupsofdatawithdifferentstyles.
7groups,suchasVanGogh,Edvard,andJacoulet,aremanuallyselectedfromSemArtandWikiArt
[10, 45] datasets, which totally includes more than 250,000 works by 3,000 artists. The other 9
groups,suchasChalkandCharcoal,aregeneratedbyatraditionalimagestyletransferalgorithm
[11]tomakesurethestylesofimagesinaparticulargrouparehighlyconsistent. Forparameter
data,weusethePixArt-α[4]asthebasemodel,whichisatransformer-basedtext-to-imagediffusion
modelwithsmallerparametersizesandcompetitivequality. Wefinetuneditwiththestyleimage
data. EachsetofLoRAparametersholds64checkpointsfromthelast64stepsofonetraining. Thus,
weobtained16setsofparameterdata,with64LoRAparametersineachset.
Training of Autoencoder and Conditional Parameter Diffusion: We introduce details of the
training process of the autoencoder and the diffusion models. For the autoencoder, we use the
parameterdatatotraintheautoencodertoencodetheLoRAparametersintoa256-dimensionallatent
space. Notethatwedidnotusethestyleimagedatainthisprocess. Forconditionaldiffusionmodel,
weusestyleconditionextractortoextractthestylefeaturesofthestyleimagedata,andmergethe
featuresintothediffusionmodelasconditioninformation.
GenerationProcess: Thegenerationprocessisdividedintotwosteps. First,theLoRAparameters
areobtainedbytheconditionalparameterdiffusionmodel,andthentheyaremergedintoPixArt-αto
obtainthestyleimage.
15ParameterGeneration: Intheinferenceprocess,thediffusionmodelisfedwithnoiseandanimage
inaparticularstyleasconditions,andthegeneratedlatentisfedintothedecodertogetcompleted
LoRAparameters.
ImageGeneration: Next,mergethegeneratedLoRAparameterstoPixArt-α. Then,wegetthe
PixArt-αfinetunedwithaparticularstyle. Then,wecanfeeditwithaprompttogetanimagewhose
stylecorrespondstoourinputcondition.
B.2 Languageexperiments
B.2.1 Datasets
InNLPtasks,weuseGLUEbenchmark[53],abenchmarkforevaluatingnaturallanguageunder-
standing capabilities. SST2 [47]: A sentiment analysis benchmark using movie review excerpts,
labeledaspositiveornegative,toaidinsentimentunderstanding.RTE:Adatasetforevaluatingifone
sentencelogicallyentailsanother,testingmodels’understandingoftextualentailment. MRPC[1]:
Containssentencepairstobenchmarkmodels’paraphrasingandsemanticequivalencecapabilities.
CoLA:Testslanguagemodels’graspofEnglishgrammar,withsentenceslabeledasgrammatically
acceptableornot. QNLI:Convertsquestion-answerpairsintoinferencetasks,assessingifsentences
are correct responses to questions. STSB [3]: A benchmark for measuring semantic similarity
betweensentences,ratedonascalefrom0to5fornuancedmeaningcomprehension.
B.2.2 LoRAconfigurations
In this section, we introduce the configuration of LoRA fine-tuning as presented in Table 1. All
modelsarefine-tunedwith20epochsandadropoutrateof0.1. Mixed-precisiontrainingisenabled
withFP16toacceleratecomputationandreducememoryusage. Thelearningrateissetto0.0001,
andawarmupratioof0.1isusedtograduallyincreaseitatthebeginningofthetraining.Additionally,
aweightdecayof0.1isappliedtoregularizethemodelandpreventoverfitting.
Table5: Addcaption
Model BERT RoBERTa DeBERTa
Rank 1 2 4 16 1 2 4 16 1 2 4
alpha 8 8 16 32 8 8 16 32 8 8 16
B.2.3 Condition
This is task ’SST-2’. SST-2 (The Stanford Sentiment Treebank) includes sentences from
moviereviewsandtheirsentimentlabels(positiveornegative). Ittestsamodel’sabilityto
capturesentimentfromtext.
Example1: Sentence: "Themoviewasfantastic!"Label: Positive. Example2: Sentence: "I
didnotenjoythefilmatall."Label: Negative.
Thisistask’RTE.’RTE(RecognizingTextualEntailment)involvespairsofsentencesand
asks whether the second sentence is true (entails), false, or undetermined based on the
informationinthefirstsentence.
Example1: Sentence1: "Thecatsatonthemat."Sentence2: "Thereisacatonthemat."
Label: Entailment. Example 2: Sentence 1: "Sarah bought two tickets to Hawaii for her
honeymoon."Sentence2: "SarahisplanningatriptoHawaii."Label: Entailment.
16Thisistask’MRPC’.MRPC(’MicrosoftResearchParaphraseCorpus’)checksifsentences
areparaphrasedfromeachother.
Example1: "Thestormleftawakeofdestruction."/"Destructionwasleftbythestorm."->
Paraphrase. Example2: "Hesaysthathesawthemanleave."/"Hesaysthemanstayedin."
->NotParaphrase.”’,
Thisistask’COLA’.CoLA(TheCorpusofLinguisticAcceptability)consistsofEnglish
sentenceslabeledasgrammaticallycorrectorincorrect. It’sdesignedtoevaluateamodel’s
abilitytounderstandEnglishgrammar.
Example1: Sentence: "Thecatsatonthemat."Label: Correct. Sentence: "Onthematsat
cat."Label: Incorrect.
Example2: Sentence: "Shereadsbookseveryday."Label: Correct. Sentence: "Booksevery
dayreadsshe."Label: Incorrect.
This is task ’QNLI’. QNLI (Question Natural Language Inference) involves pairs of a
questionandasentence,wherethegoalistodeterminewhetherthesentencecontainsthe
answertothequestion.
Example1: Question: "Whatcoloristhesky?"Sentence: "Theskyisusuallyblue."Label:
Entailment. Example2: Question: "Whowrote’1984’?"Sentence: "GeorgeOrwellisthe
authorof’AnimalFarm’and’1984’."Label: Entailment.
ThisistaskSTSB.STSB(SemanticTextualSimilarityBenchmark)aimstoratesentencepair
similarityona0-5scale.
Example1: "Amanisplayingaguitar."/"Amanisplayinganinstrument."->Score: 4.5.
Example2: "Achildisridingahorse."/"Ahorseisbeingriddenbyachild."->Score: 5.
Style-1 Generated on test set Style-2
𝝀=0.00 𝝀=0.05 𝝀=0.25 𝝀=0.45 𝝀=0.65 𝝀=0.85 𝝀=1.00
Figure6: VisualizationoftheimagegeneratedbyLoRAparameters,whichisgeneratedbyCOND
P-DIFFonthetestsetwithconditionsthatthemodelhasneverseen.
17Figure7:PCAinthelatentspaceoftheLoRAparametersoftrainsetandgeneratedbyCONDP-DIFF
C Explorationsof COND P-DIFF generalizability
WeconsiderthatthegeneralizabilityofCONDP-DIFFislimitedbythecurrentamountofdata. Ifwe
wantthemodeltogaingeneralizability,weneedtosampleenoughLoRAparametersintheparameter
space,whichisdifficulttoachieve. Therefore,inthisexperiment,wefirstmakeastyle-continuous
dataset,whichcanbeequivalenttosamplingenoughdatapointsinasubspacetoprovideenough
dataforourmodel. Wethentrainedourmodelonthestyle-continuousdatasetwecreatedtoverifyits
generalizability.
Makeastyle-continuousdataset:
Sinceitisdifficulttofindstyle-continuousdata,weusesomeAI-generatedimagestomakeastyle-
continuous parameter-image pair dataset, to verify the continuity of the parameter space and the
model’sgeneralizationability. Herearethedetailedsteps:
Firstly,wetraintheLoRAparametersrelevanttostyle-1usingstyle-1images;traintheparameters
relevanttostyle-2usingstyle-2images. Next,weuseformulaθ = (1−λ)θ +λθ to
interp style1 style2
combineLoRAparametersindifferentproportionstoobtain1000LoRAparametersbetweenstyle-1
andstyle-2(λisfrom{0.000,0.001,0.002,··· ,0.999}).Thenwemergethe1000LoRAparameters
toPixArt-αinturnandrandomlyselectsomepromptstogenerateimagesinrelevantstyle. Thus,we
obtainadatasetof1000parameter-imagepairs.
Trainonthestyle-continuousdataset:
Withtheabovestyle-continuousparameter-imagepairdata,wecanverifycontinuityoftheparameter
spaceandthegeneralizationabilityofourmodel. Thedetailedtrainingprocessisasfollows:
First,wesplitthedatasetintoatrainsetandatestset. Weselect500parameter-imagepairsoutofthe
1000pairsasthetrainingset,inwhichλisfrom[0.1,0.2)∪[0.3,0.4)∪[0.5,0.6)∪[0.7,0.8)∪[0.9,1.0),
andtherestareasthetestset. Next,wetrainCONDP-DIFFonthetrainsetaccordingtothenormal
methoddescribedinSection3,andevaluateourmodelonthetestset.
TheresultsareshowninFigure6,wheretheinputconditionsareimageschosenfromthetestset,
which our model has never seen before. We find that the model can still generate images in the
relevantstyle,whichshowsourmodel’sgeneralizability. Inaddition,wevisualizedthetrainingset
parametersandtheparametersgeneratedbyourmodelinthelatentspacebyPCA[30]inFigure7.
Thebluedotsrepresentthedatausedfortraining,andtheplacewherethebluelineisdisconnected
isleftfortesting. Theorangedotsrepresenttheparametersgeneratedby COND P-DIFF,andwe
findthatourmodelcanfittheentiredistributioninsteadofonlypartsofthetrainset,illustratingthe
generalizabilityofthemodel.
18