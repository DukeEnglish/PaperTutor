[
    {
        "title": "Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs",
        "authors": "Yilun HuaYoav Artzi",
        "links": "http://arxiv.org/abs/2408.01417v1",
        "entry_id": "http://arxiv.org/abs/2408.01417v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01417v1",
        "summary": "Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.",
        "updated": "2024-08-02 17:51:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01417v1"
    },
    {
        "title": "NOLO: Navigate Only Look Once",
        "authors": "Bohan ZhouJiangxing WangZongqing Lu",
        "links": "http://arxiv.org/abs/2408.01384v1",
        "entry_id": "http://arxiv.org/abs/2408.01384v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01384v1",
        "summary": "The in-context learning ability of Transformer models has brought new\npossibilities to visual navigation. In this paper, we focus on the video\nnavigation setting, where an in-context navigation policy needs to be learned\npurely from videos in an offline manner, without access to the actual\nenvironment. For this setting, we propose Navigate Only Look Once (NOLO), a\nmethod for learning a navigation policy that possesses the in-context ability\nand adapts to new scenes by taking corresponding context videos as input\nwithout finetuning or re-training. To enable learning from videos, we first\npropose a pseudo action labeling procedure using optical flow to recover the\naction label from egocentric videos. Then, offline reinforcement learning is\napplied to learn the navigation policy. Through extensive experiments on\ndifferent scenes, we show that our algorithm outperforms baselines by a large\nmargin, which demonstrates the in-context learning ability of the learned\npolicy.",
        "updated": "2024-08-02 16:41:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01384v1"
    },
    {
        "title": "Spatial-Spectral Morphological Mamba for Hyperspectral Image Classification",
        "authors": "Muhammad AhmadMuhammad Hassaan Farooq ButtMuhammad UsamaAdil Mehmood KhanManual MazzaraSalvatore Distenano",
        "links": "http://arxiv.org/abs/2408.01372v1",
        "entry_id": "http://arxiv.org/abs/2408.01372v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01372v1",
        "summary": "In recent years, Transformers have garnered significant attention for\nHyperspectral Image Classification (HSIC) due to their self-attention\nmechanism, which provides strong classification performance. However, these\nmodels face major challenges in computational efficiency, as their complexity\nincreases quadratically with the sequence length. The Mamba architecture,\nleveraging a State Space Model, offers a more efficient alternative to\nTransformers. This paper introduces the Spatial-Spectral Morphological Mamba\n(MorpMamba) model. In the MorpMamba model, a token generation module first\nconverts the Hyperspectral Image (HSI) patch into spatial-spectral tokens.\nThese tokens are then processed by a morphology block, which computes\nstructural and shape information using depthwise separable convolutional\noperations. The extracted information is enhanced in a feature enhancement\nmodule that adjusts the spatial and spectral tokens based on the center region\nof the HSI sample, allowing for effective information fusion within each block.\nSubsequently, the tokens are refined in a multi-head self-attention block to\nfurther improve the feature space. Finally, the combined information is fed\ninto the state space block for classification and the creation of the ground\ntruth map. Experiments on widely used Hyperspectral (HS) datasets demonstrate\nthat the MorpMamba model outperforms (parametric efficiency) both CNN and\nTransformer models.",
        "updated": "2024-08-02 16:28:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01372v1"
    },
    {
        "title": "EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using Windowed Nonlinear Optimization",
        "authors": "Runze YuanTao LiuZijia DaiYi-Fan ZuoLaurent Kneip",
        "links": "http://arxiv.org/abs/2408.01370v1",
        "entry_id": "http://arxiv.org/abs/2408.01370v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01370v1",
        "summary": "Event cameras are an interesting visual exteroceptive sensor that reacts to\nbrightness changes rather than integrating absolute image intensities. Owing to\nthis design, the sensor exhibits strong performance in situations of\nchallenging dynamics and illumination conditions. While event-based\nsimultaneous tracking and mapping remains a challenging problem, a number of\nrecent works have pointed out the sensor's suitability for prior map-based\ntracking. By making use of cross-modal registration paradigms, the camera's\nego-motion can be tracked across a large spectrum of illumination and dynamics\nconditions on top of accurate maps that have been created a priori by more\ntraditional sensors. The present paper follows up on a recently introduced\nevent-based geometric semi-dense tracking paradigm, and proposes the addition\nof inertial signals in order to robustify the estimation. More specifically,\nthe added signals provide strong cues for pose initialization as well as\nregularization during windowed, multi-frame tracking. As a result, the proposed\nframework achieves increased performance under challenging illumination\nconditions as well as a reduction of the rate at which intermediate event\nrepresentations need to be registered in order to maintain stable tracking\nacross highly dynamic sequences. Our evaluation focuses on a diverse set of\nreal world sequences and comprises a comparison of our proposed method against\na purely event-based alternative running at different rates.",
        "updated": "2024-08-02 16:24:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01370v1"
    },
    {
        "title": "Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation",
        "authors": "Ruoxuan FengDi HuWenke MaXuelong Li",
        "links": "http://arxiv.org/abs/2408.01366v1",
        "entry_id": "http://arxiv.org/abs/2408.01366v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01366v1",
        "summary": "Humans possess a remarkable talent for flexibly alternating to different\nsenses when interacting with the environment. Picture a chef skillfully gauging\nthe timing of ingredient additions and controlling the heat according to the\ncolors, sounds, and aromas, seamlessly navigating through every stage of the\ncomplex cooking process. This ability is founded upon a thorough comprehension\nof task stages, as achieving the sub-goal within each stage can necessitate the\nutilization of different senses. In order to endow robots with similar ability,\nwe incorporate the task stages divided by sub-goals into the imitation learning\nprocess to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a\nstage-guided dynamic multi-sensory fusion method with coarse-to-fine stage\nunderstanding, which dynamically adjusts the priority of modalities based on\nthe fine-grained state within the predicted current stage. We train a robot\nsystem equipped with visual, auditory, and tactile sensors to accomplish\nchallenging robotic manipulation tasks: pouring and peg insertion with keyway.\nExperimental results indicate that our approach enables more effective and\nexplainable dynamic fusion, aligning more closely with the human fusion process\nthan existing methods.",
        "updated": "2024-08-02 16:20:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01366v1"
    }
]