[
    {
        "title": "Resampling and averaging coordinates on data",
        "authors": "Andrew J. BlumbergMathieu CarriereJun Hou FungMichael A. Mandell",
        "links": "http://arxiv.org/abs/2408.01379v1",
        "entry_id": "http://arxiv.org/abs/2408.01379v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01379v1",
        "summary": "We introduce algorithms for robustly computing intrinsic coordinates on point\nclouds. Our approach relies on generating many candidate coordinates by\nsubsampling the data and varying hyperparameters of the embedding algorithm\n(e.g., manifold learning). We then identify a subset of representative\nembeddings by clustering the collection of candidate coordinates and using\nshape descriptors from topological data analysis. The final output is the\nembedding obtained as an average of the representative embeddings using\ngeneralized Procrustes analysis. We validate our algorithm on both synthetic\ndata and experimental measurements from genomics, demonstrating robustness to\nnoise and outliers.",
        "updated": "2024-08-02 16:37:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01379v1"
    },
    {
        "title": "Transformers are Universal In-context Learners",
        "authors": "Takashi FuruyaMaarten V. de HoopGabriel Peyré",
        "links": "http://arxiv.org/abs/2408.01367v1",
        "entry_id": "http://arxiv.org/abs/2408.01367v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01367v1",
        "summary": "Transformers are deep architectures that define \"in-context mappings\" which\nenable predicting new tokens based on a given set of tokens (such as a prompt\nin NLP applications or a set of patches for vision transformers). This work\nstudies in particular the ability of these architectures to handle an\narbitrarily large number of context tokens. To mathematically and uniformly\naddress the expressivity of these architectures, we consider the case that the\nmappings are conditioned on a context represented by a probability distribution\nof tokens (discrete for a finite number of tokens). The related notion of\nsmoothness corresponds to continuity in terms of the Wasserstein distance\nbetween these contexts. We demonstrate that deep transformers are universal and\ncan approximate continuous in-context mappings to arbitrary precision,\nuniformly over compact token domains. A key aspect of our results, compared to\nexisting findings, is that for a fixed precision, a single transformer can\noperate on an arbitrary (even infinite) number of tokens. Additionally, it\noperates with a fixed embedding dimension of tokens (this dimension does not\nincrease with precision) and a fixed number of heads (proportional to the\ndimension). The use of MLP layers between multi-head attention layers is also\nexplicitly controlled.",
        "updated": "2024-08-02 16:21:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01367v1"
    },
    {
        "title": "Autoencoders in Function Space",
        "authors": "Justin BunkerMark GirolamiHefin LambleyAndrew M. StuartT. J. Sullivan",
        "links": "http://arxiv.org/abs/2408.01362v1",
        "entry_id": "http://arxiv.org/abs/2408.01362v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01362v1",
        "summary": "Autoencoders have found widespread application, in both their original\ndeterministic form and in their variational formulation (VAEs). In scientific\napplications it is often of interest to consider data that are comprised of\nfunctions; the same perspective is useful in image processing. In practice,\ndiscretisation (of differential equations arising in the sciences) or\npixellation (of images) renders problems finite dimensional, but conceiving\nfirst of algorithms that operate on functions, and only then discretising or\npixellating, leads to better algorithms that smoothly operate between different\nlevels of discretisation or pixellation. In this paper function-space versions\nof the autoencoder (FAE) and variational autoencoder (FVAE) are introduced,\nanalysed, and deployed. Well-definedness of the objective function governing\nVAEs is a subtle issue, even in finite dimension, and more so on function\nspace. The FVAE objective is well defined whenever the data distribution is\ncompatible with the chosen generative model; this happens, for example, when\nthe data arise from a stochastic differential equation. The FAE objective is\nvalid much more broadly, and can be straightforwardly applied to data governed\nby differential equations. Pairing these objectives with neural operator\narchitectures, which can thus be evaluated on any mesh, enables new\napplications of autoencoders to inpainting, superresolution, and generative\nmodelling of scientific data.",
        "updated": "2024-08-02 16:13:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01362v1"
    },
    {
        "title": "Sparse Linear Regression when Noises and Covariates are Heavy-Tailed and Contaminated by Outliers",
        "authors": "Takeyuki SasaiHironori Fujisawa",
        "links": "http://arxiv.org/abs/2408.01336v1",
        "entry_id": "http://arxiv.org/abs/2408.01336v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01336v1",
        "summary": "We investigate a problem estimating coefficients of linear regression under\nsparsity assumption when covariates and noises are sampled from heavy tailed\ndistributions. Additionally, we consider the situation where not only\ncovariates and noises are sampled from heavy tailed distributions but also\ncontaminated by outliers. Our estimators can be computed efficiently, and\nexhibit sharp error bounds.",
        "updated": "2024-08-02 15:33:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01336v1"
    },
    {
        "title": "Point Prediction for Streaming Data",
        "authors": "Aleena ChandaN. V. VinodchandranBertrand Clarke",
        "links": "http://arxiv.org/abs/2408.01318v1",
        "entry_id": "http://arxiv.org/abs/2408.01318v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01318v1",
        "summary": "We present two new approaches for point prediction with streaming data. One\nis based on the Count-Min sketch (CMS) and the other is based on Gaussian\nprocess priors with a random bias. These methods are intended for the most\ngeneral predictive problems where no true model can be usefully formulated for\nthe data stream. In statistical contexts, this is often called the\n$\\mathcal{M}$-open problem class. Under the assumption that the data consists\nof i.i.d samples from a fixed distribution function $F$, we show that the\nCMS-based estimates of the distribution function are consistent.\n  We compare our new methods with two established predictors in terms of\ncumulative $L^1$ error. One is based on the Shtarkov solution (often called the\nnormalized maximum likelihood) in the normal experts setting and the other is\nbased on Dirichlet process priors. These comparisons are for two cases. The\nfirst is one-pass meaning that the updating of the predictors is done using the\nfact that the CMS is a sketch. For predictors that are not one-pass, we use\nstreaming $K$-means to give a representative subset of fixed size that can be\nupdated as data accumulate.\n  Preliminary computational work suggests that the one-pass median version of\nthe CMS method is rarely outperformed by the other methods for sufficiently\ncomplex data. We also find that predictors based on Gaussian process priors\nwith random biases perform well. The Shtarkov predictors we use here did not\nperform as well probably because we were only using the simplest example. The\nother predictors seemed to perform well mainly when the data did not look like\nthey came from an M-open data generator.",
        "updated": "2024-08-02 15:12:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01318v1"
    }
]