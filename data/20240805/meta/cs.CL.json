[
    {
        "title": "Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting",
        "authors": "Xiangyu ZhaoChengqian Ma",
        "links": "http://arxiv.org/abs/2408.01423v1",
        "entry_id": "http://arxiv.org/abs/2408.01423v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01423v1",
        "summary": "Large Language Models (LLMs) exhibit remarkable proficiency in addressing a\ndiverse array of tasks within the Natural Language Processing (NLP) domain,\nwith various prompt design strategies significantly augmenting their\ncapabilities. However, these prompts, while beneficial, each possess inherent\nlimitations. The primary prompt design methodologies are twofold: The first,\nexemplified by the Chain of Thought (CoT), involves manually crafting prompts\nspecific to individual datasets, hence termed Expert-Designed Prompts (EDPs).\nOnce these prompts are established, they are unalterable, and their\neffectiveness is capped by the expertise of the human designers. When applied\nto LLMs, the static nature of EDPs results in a uniform approach to both simple\nand complex problems within the same dataset, leading to the inefficient use of\ntokens for straightforward issues. The second method involves prompts\nautonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which\nprovide tailored solutions to specific problems, mitigating the limitations of\nEDPs. However, LDPs may encounter a decline in performance when tackling\ncomplex problems due to the potential for error accumulation during the\nsolution planning process. To address these challenges, we have conceived a\nnovel Prompt Recursive Search (PRS) framework that leverages the LLM to\ngenerate solutions specific to the problem, thereby conserving tokens. The\nframework incorporates an assessment of problem complexity and an adjustable\nstructure, ensuring a reduction in the likelihood of errors. We have\nsubstantiated the efficacy of PRS framework through extensive experiments using\nLLMs with different numbers of parameters across a spectrum of datasets in\nvarious domains. Compared to the CoT method, the PRS method has increased the\naccuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22%\nimprovement.",
        "updated": "2024-08-02 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01423v1"
    },
    {
        "title": "Mission Impossible: A Statistical Perspective on Jailbreaking LLMs",
        "authors": "Jingtong SuJulia KempeKaren Ullrich",
        "links": "http://arxiv.org/abs/2408.01420v1",
        "entry_id": "http://arxiv.org/abs/2408.01420v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01420v1",
        "summary": "Large language models (LLMs) are trained on a deluge of text data with\nlimited quality control. As a result, LLMs can exhibit unintended or even\nharmful behaviours, such as leaking information, fake news or hate speech.\nCountermeasures, commonly referred to as preference alignment, include\nfine-tuning the pretrained LLMs with carefully crafted text examples of desired\nbehaviour. Even then, empirical evidence shows preference aligned LLMs can be\nenticed to harmful behaviour. This so called jailbreaking of LLMs is typically\nachieved by adversarially modifying the input prompt to the LLM. Our paper\nprovides theoretical insights into the phenomenon of preference alignment and\njailbreaking from a statistical perspective. Under our framework, we first show\nthat pretrained LLMs will mimic harmful behaviour if present in the training\ncorpus. Under that same framework, we then introduce a statistical notion of\nalignment, and lower-bound the jailbreaking probability, showing that it is\nunpreventable under reasonable assumptions. Based on our insights, we propose\nan alteration to the currently prevalent alignment strategy RLHF. Specifically,\nwe introduce a simple modification to the RLHF objective, we call E-RLHF, that\naims to increase the likelihood of safe responses. E-RLHF brings no additional\ntraining cost, and is compatible with other methods. Empirically, we\ndemonstrate that E-RLHF outperforms RLHF on all alignment problems put forward\nby the AdvBench and HarmBench project without sacrificing model performance as\nmeasured by the MT-Bench project.",
        "updated": "2024-08-02 17:55:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01420v1"
    },
    {
        "title": "DebateQA: Evaluating Question Answering on Debatable Knowledge",
        "authors": "Rongwu XuXuan QiZehan QiWei XuZhijiang Guo",
        "links": "http://arxiv.org/abs/2408.01419v1",
        "entry_id": "http://arxiv.org/abs/2408.01419v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01419v1",
        "summary": "The rise of large language models (LLMs) has enabled us to seek answers to\ninherently debatable questions on LLM chatbots, necessitating a reliable way to\nevaluate their ability. However, traditional QA benchmarks assume fixed answers\nare inadequate for this purpose. To address this, we introduce DebateQA, a\ndataset of 2,941 debatable questions, each accompanied by multiple\nhuman-annotated partial answers that capture a variety of perspectives. We\ndevelop two metrics: Perspective Diversity, which evaluates the\ncomprehensiveness of perspectives, and Dispute Awareness, which assesses if the\nLLM acknowledges the question's debatable nature. Experiments demonstrate that\nboth metrics align with human preferences and are stable across different\nunderlying models. Using DebateQA with two metrics, we assess 12 popular LLMs\nand retrieval-augmented generation methods. Our findings reveal that while LLMs\ngenerally excel at recognizing debatable issues, their ability to provide\ncomprehensive answers encompassing diverse perspectives varies considerably.",
        "updated": "2024-08-02 17:54:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01419v1"
    },
    {
        "title": "Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs",
        "authors": "Yilun HuaYoav Artzi",
        "links": "http://arxiv.org/abs/2408.01417v1",
        "entry_id": "http://arxiv.org/abs/2408.01417v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01417v1",
        "summary": "Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.",
        "updated": "2024-08-02 17:51:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01417v1"
    },
    {
        "title": "Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer",
        "authors": "Yu YangPan Xu",
        "links": "http://arxiv.org/abs/2408.01402v1",
        "entry_id": "http://arxiv.org/abs/2408.01402v1",
        "pdf_url": "http://arxiv.org/pdf/2408.01402v1",
        "summary": "Decision Transformer (DT) has emerged as a promising class of algorithms in\noffline reinforcement learning (RL) tasks, leveraging pre-collected datasets\nand Transformer's capability to model long sequences. Recent works have\ndemonstrated that using parts of trajectories from training tasks as prompts in\nDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.\nHowever, collecting data from specific environments can be both costly and\nunsafe in many scenarios, leading to suboptimal performance and limited\nfew-shot prompt abilities due to the data-hungry nature of Transformer-based\nmodels. Additionally, the limited datasets used in pre-training make it\nchallenging for Prompt-DT type of methods to distinguish between various RL\ntasks through prompts alone. To address these challenges, we introduce the\nLanguage model-initialized Prompt Decision Transformer (LPDT), which leverages\npre-trained language models for meta-RL tasks and fine-tunes the model using\nLow-rank Adaptation (LoRA). We further incorporate prompt regularization to\neffectively differentiate between tasks based on prompt feature\nrepresentations. Our approach integrates pre-trained language model and RL\ntasks seamlessly. Extensive empirical studies demonstrate that initializing\nwith a pre-trained language model significantly enhances the performance of\nPrompt-DT on unseen tasks compared to baseline methods.",
        "updated": "2024-08-02 17:25:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.01402v1"
    }
]