Evaluating Large Language Models for
Generalization and Robustness via Data Compression
YuchengLi1 YunhaoGuo2 FrankGuerin1 ChenghuaLin3
Abstract (Wangetal.,2018;2019)consistsof10+NLPtasksandis
widelyusedinbenchmarkinglargelanguagemodels(Zhang
Existing methods for evaluating large language
etal.,2022;Chowdheryetal.,2023). Morerecently,GPT-
modelsfacechallengessuchasdatacontamina-
4wastestedon30+humanacademictestsand10+NLP
tion,sensitivitytoprompts,andthehighcostof
benchmarks(Achiametal.,2023).
benchmarkcreation. Toaddressthis,wepropose
alosslessdatacompressionbasedevaluationap- However,thismethodologysuffersfromthreecriticalprob-
proachthattestshowmodels’predictiveabilities lems. Firstly,benchmarkresultsdependheavilyonprompts
generalizeaftertheirtrainingcutoff. Specifically, (Sclaretal.,2023). Differentpromptsforthesamemodel
wecollectcomprehensivetestdataspanning83 can produce wildly different results. This makes it hard
monthsfrom2017to2023andsplitthedatainto to evaluate the models directly, without the evaluation
trainingandtestingperiodsaccordingtomodels’ being inflated by clever prompt engineering. For exam-
training data cutoff. We measure: 1) the com- ple,GoogleGeminioutperformsbaselinemodelswithcus-
pressionperformanceonthetestingperiodasa tomizedprompts,butfailstobeatothermodelswithsimple
measure of generalization on unseen data; and few-shot prompts (Anil et al., 2023), making it difficult
2)theperformancegapbetweenthetrainingand toascertainwhetherthegoodperformanceistheresultof
testing period as a measure of robustness. Our model abilities or well-designed prompts. Secondly, the
experimentstest14representativelargelanguage increasingscaleofpre-trainingdataraisesthepossibilityof
models with various sizes on sources including includingbenchmarkdatainthepre-trainingstageoflarge
Wikipedia,newsarticles,code,arXivpapers,and languagemodels,leadingtothedatacontaminationissue
multi-modaldata. Wefindthatthecompression (Jacovietal.,2023;Sainzetal.,2023). Datacontamination
rate of many models reduces significantly after allows models to achieve higher metrics through “mem-
theircutoffdate,butmodelssuchasMistraland orization”, rather than demonstrating true generalization.
Llama-2demonstrateagoodbalancebetweenper- Thisunderminesthereliabilityofmodelcomparisonsand
formance and robustness. Results also suggest canalsomisleadmodeldevelopment. Studiesestimatethat
that models struggle to generalize on news and 30-80%ofexamplesinpopularbenchmarkslikeMMLU
codedata,butworkespeciallywellonarXivpa- (Hendrycksetal.,2020)andSQuAD(Rajpurkaretal.,2018)
pers. Wealsofindthecontextsizeandtokeniza- arecontaminated,withtheratioincreasingrapidlyovertime
tionimplementationhaveabigimpactofonthe (Lietal.,2023c). Finally,duetothehumaneffortsrequired,
overallcompressionperformance. constructingnewanddiversebenchmarksisveryexpensive.
Thispreventsregularupdatesofexistingbenchmarksand
furtherdiversificationtocovermoredomainsandtasks. For
1.Introduction
example,benchmarkslikeMMLUandC-Eval(Huangetal.,
2023)arecollectedfromhumanacademictestswhichhave
Modernevaluationmethodsforlargelanguagemodelstyp-
accumulated over the past 10 years to reach their current
icallyusecomprehensivebenchmarksuitesthatincludea
scale. Continuing with the same method of construction
broadrangeoftasksanddomains. Thisaimstoassessthe
willmakeitextremelydifficulttocollectnewandupdated
generalizationabilityofdifferentmodelsinvaryingscenar-
testsinthenearfuture. Asaresult,thecommunityurgently
iosandpreventoverfittingonanyspecifictaskordataset
needsnewmethodsforassessinglargelanguagemodels.
(Changetal.,2023). Forexample,theGLUEbenchmark
Inthispaper,weproposeanovelapproachthatevaluates
1University of Surrey, UK 2Harbin Engineering Univer-
baselargelanguagemodelsthoughdatacompressionacross
sity,China3UniversityofManchester,UK.Correspondenceto:
YuchengLi<yucheng.li@surrey.ac.uk>. awiderangeoftimeperiods. Firstly,weuselosslessdata
compression as the metric to assess model performance.
1
4202
beF
1
]LC.sc[
1v16800.2042:viXraEvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
Compressionmetricsarewidelyusedinbenchmarkinglan- 6) Models with larger vocabularies in tokenization have
guagemodeling(Radfordetal.,2019;Daietal.,2019)and moredifficultywithtoken-levelprediction. Ourdataand
areshowntobestronglycorrelatedwithgeneralizationabil- codewillbemadepubliclyavailable.
ity and in-context learning performance (Dele´tang et al.,
2023;Rae,2023). Ascompressionrequiresmodelstowork
2.Background
onrawdata,ourmethodavoidsthepotentialinterference
introduced by prompts and also reduces human effort in 2.1.LanguageModelsEvaluation
creatingbenchmarkquestions. Secondly,similartotheidea
Traditionally,languagemodelperformanceismeasuredby
of the train/test data split in traditional machine learning,
intrinsicmetricssuchasnegativelog-likelihood(NLL)loss,
we split our testing data into the training period and the
perplexity,andbits-per-character(BPC).Thesemetricspri-
testingperiod accordingtomodels’cutoffdates. Models
marilyfocusonamodel’sabilitytopredictorgeneratetext
arefirstevaluatedondatafromthetrainingperiod,whichis
sequences. Forexample,NLLlossisdefinedasfollows.
regardedasthein-distributionperformance. Theyarethen
evaluatedondatafromthetestingperiod(i.e.,dataemerging
n
(cid:88)
afterthetrainingconcludes). Thisfocusesonthemodels’ L(x )=−logP(x )=− logP(x |x ) (1)
1:n 1:n t 1:t−1
generalization to new, unseen data, and aligns well with
t=1
howlanguagemodelsneedtohandlenewdatainrealworld
scenarios. Finally,weusethegapbetweenthetrainingand wherethelikelihoodP(x 1:n)isexpandedtoP(x t|x 1:t−1)
testingperiodsasameasureofmodelrobustnessovertime, basedonthechainrule. Itquantifieshowwellthemodel
similartohowwecomparetrainingerrorandtestingerror predictsagiventextsequence, withlowerNLLlossindi-
intraditionalmachinelearning. Asmalldifferencebetween catingbetterpredictiveaccuracy. Theselanguagemodeling
the training and testing periods indicates a strong robust- metricsaretypicallyusedtogetherwithestablisheddatasets.
nessofmodelperformance,whilealargegapmaysignal Forexample,GPT-2(Radfordetal.,2019)wasevaluatedon
overfitting. Wikitext-2(Merityetal.,2016),PennTreeBank(Marcus
et al., 1994), and enwik8 (Hutter, 2006) using perplexity
We collect test data spanning 83 months, from Jan 2017
and BPC as metrics. More recently, as language models
toNov2023,toanalyzehowmodelsgeneralizeovertime.
are increasingly being used in NLP tasks and real-world
Thisdatacoversabroadrangeofdomainsandalsomulti-
applications, they are often evaluated based on how well
modalsourcesknownfortheirtime-sensitivity,including
theyperformondownstreamtasks(Beechingetal.,2023;
Wikipediapages,GitHubcode,BBCnews,arXivpapers,
OpenCompass,2023). Onecommonwaytoperformtask-
newsimages,andaudio. Ourexperimentscoveradiverse
basedevaluationistofurtherfine-tuneandtestmodelson
range of foundation models including LLaMA (Touvron
task-specificdatasets,whichisusuallyusedforBERT(De-
etal.,2023a),Llama-2(Touvronetal.,2023b),CodeLlama
vlinetal.,2018)andRoBERTa(Liuetal.,2019)models. In
(Roziereetal.,2023), Yi(Yi,2023), Mistral(Jiangetal.,
contrast,anotheremergingwayistousepurepromptingto
2023),Baichuan2(Yangetal.,2023a),InternLM(InternLM,
getmodels’responses. Thismethodismorefrequentlyused
2023), ChatGLM(Duetal.,2021), andQwen(Baietal.,
inmodernlargelanguagemodelssuchasGPT-4,PaLMand
2023)acrossmultiplemodelsizes(6B,7B,13B,34B,65B,
LLaMA.Inthiscase,theevaluationalsotakesthemodels’
and70Bparameters). Wealsodiscusstheimpactofvarious
in-contextlearningabilityintoconsideration,whichrefers
modelcontextsizes(2K,4K,and8K)anddifferenttokeniza-
tohowwellthemodelscanlearnfromtheinformationand
tionapproaches. Finally,wecompareresultsofourmethod
demonstrationsprovidedinthelimitedcontext.
to those of established benchmarks such as HumanEval
(Chenetal.,2021)andMMLU. However, as language models grow in size and consume
moredata,bothmethodsfacechallenges. Firstly,asboth
Ourkeyfindingsareasfollows: 1)Models’compression
methodsrelyoninternet-sourcedtestdatasets,concernsof
performanceovertimecorrelatescloselywiththeirtraining
datacontaminationarise. Thisreferstoevaluatingonexam-
datacollectiontime,withcleardivergencesafterthecutoff
plesthatareexplicitlyorimplicitlyincludedinthetraining
date. 2)Modelswithsimilarperformanceonthetraining
data. Forexample, GPT-3reportedthatWikipedia-based
periodcandemonstratewidelydifferentgeneralizationre-
language modeling benchmarks and the Children’s Book
sultsonnew,unseendata.3)Generalizationdifficultyvaries
Test dataset are almost entirely contained in its training
acrosstestdatasets. Modelsstruggletogeneralizeonwiki-
data,withotherWikipedia-basedreadingcomprehension
text,news,andcode,butgeneralizewellonarXivcontent.
benchmarkssuchasQuAC,SQuADv2andDROPshowing
4) All models fail to compress multi-modal data, show-
over90%contamination(Brownetal.,2020). Robertsetal.
ing limited capabilities in dealing with raw byte streams.
(2023)conductalongitudinalanalysisoftheprogramming
5)Largercontextsgenerallyleadtobetterperformancebut
abilityoflargelanguagemodels,revealingasignificantas-
donotexceedasmallcontext+slidingwindowapproach.
sociation between a code problem’s presence on GitHub
2EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
andamodel’spassrateforthatproblem. Lietal.(2023c) Algorithm1ArithmeticCoding
alsoshowthatbenchmarkcontaminationcaninflateaccu- Require: Astreamofbytesx andalanguagemodelf
1:n
racyby7%to14%onbenchmarkslikeMMLUandC-Eval, Ensure: EncodedbytestreamE
evenwhencontaminationonlyincludesquestionswithout low ←0.0
revealingtheassociatedcorrectanswers. Secondly, large high←1.0
languagemodelshavebeenshowntobehighlysensitiveto forx inx do
i 1:n
promptdesign(Sclaretal.,2023). Therearealsoprompt- range←high−low
ingtechniquessuchasChain-of-Thought(Weietal.,2022), high←low+range×P(x )
1:i
andLotteryprompting(Chenetal.,2023)whichusethis low ←low+range×P(x )
1:i−1
sensitivityandaredesignedtoenhancemodelperformance endfor
on various tasks. This complicates the accurate measure- E ←valuein[low,high)
mentofmodelperformancewhenrelyingsolelyonthepure returnE
promptingapproach.
Algorithm2ArithmeticDecoding
2.2.CompressionandLanguageModels Require: EncodedbytestreamE,lengthofx ,andthe
1:n
samelanguagemodelf
It is well established that compression is essentially pre-
Ensure: Decodedbytestreamx
diction,whicheffectivelylinkscompressionandlangauge 1:n
low ←0.0
models(Dele´tangetal.,2023). Thesourcecodingtheory
high←1.0
fromShannon’sinformationtheory(Shannon,1948)sug-
x←emptybytestream
geststhatthenumberofbitsrequiredbyanoptimalentropy
fori=1tondo
encodertocompressamessagex isequaltothenega-
1:n
range←high−low
tive log likelihood of the message given by a statistical
2
foreachpossiblebytexdo
model(i.e.,−log P(x )). ComparingthistoEq.1,we
2 1:n
high←low+range×P(x )
findthatoptimizingtheNLLlossoflanguagemodelingand 1:i
low ←low+range×P(x )
optimizingtheexpectedmessagelengthtocompressx 1:i−1
1:n
ifcodeisintherangeof[low,high)then
isfundamentallyequivalent. Therefore,priorworkdemon-
appendxtox
stratesthatlanguagemodelsandotherneuralnetworkscan 1:i−1
break
beusedtoachievestate-of-the-artlosslesscompressionvia
endif
arithmetic coding (Yang et al., 2023b; Valmeekam et al.,
endfor
2023). Inaddition,Dele´tangetal.(2023)andRae(2023)
endfor
further argue that the ability to compress information is
returnD
closelyrelatedtotheabilitytogeneralize.Finally,Sutskever
(2023) leverage the ability of compression to explain the
effectivenessofunsupervisedlearning.
to segment the dataset X into chunks of size C and feed
themtothelanguagemodelsone-by-one. Asaresult,the
3.OurMethod actuallikelihoodiscomputedasfollows:
Inthissection,weexplainhowtoperformdatacompression N C
(cid:89)(cid:89)
with a language model. There are typically two stages f(X)=P(X)= P(x j|x 1:j−1) (3)
involved: 1)calculatingthelikelihoodofthedataX with j=1
thelanguagemodelf;and2)applyinganentropyencoding whereN indicatesthenumberofchunks.Itisratherstraight-
algorithm,usuallyarithmeticcoding,whichrepresentsX forwardtofurtherapplylanguagemodelstomulti-modal
based on the computed probabilities to reduce its overall data. Inthiscase,X consistsastreamofbytesinsteadofto-
size. Firstly, we apply language models to the data to be kens.EachbyteinX,rangingfrom<0x00>to<0xFF>,is
compressedX,toobtaintheprobabilitydistributionP(X). individuallymappedtoauniqueinputIDthatthetokenizer
Fortextualdata,considerwehavealanguagemodelf and reservesforbytecharacters. Thelikelihoodofmulti-modal
atextualdatasetX =(x 0,··· ,x n)consistingofastream X isthencalculatedbasedonEq.3, thesameasfortext
oftokens. Thelikelihoodiscomputedasfollows: data.
(cid:89)n Secondly,datasetX anditslikelihoodP(X)aregiventoan
f(X)=P(X)= P(x |x ) (2)
i 1:i−1 entropyencodingalgorithmtoobtainthefinalcompressed
i=1 binaryrepresentation. Weusearithmeticcodinginourex-
However,duetomodernTransformer-basedlanguagemod- periments,whichisknowntobeoptimalintermsofcoding
elshavingalimitedcontextwindowdenotedasC,weneed length. Theencodinganddecodingprocessofarithmetic
3EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
Dataset Modality #Docs Size Totalsize Model Release Cutoff Size Context
Wikitext Text 500 23MB 1.9GB LLaMA 2023-02 2020* 7/13/65B 2048
BBCNews Text 1,270 6MB 490MB InternLM 2023-06 - 7B 2048
GitHub Text 395 35MB 2.8GB Llama-2 2023-07 2022† 7/13/70B 4096
ArXiv Text 533 28MB 2.3GB CodeLlama 2023-08 - 7B 16K
BBCImages Bytes 1,589 12MB 1GB Baichuan2 2023-09 - 7B 4096
Audio-Mix Bytes 103 12MB 1GB Mistral 2023-09 - 7B 32K
Qwen 2023-09 - 7B 32K
ChatGLM3 2023-10 - 6B 32K
Table1.Testdatatocompressinourexperiments.#DocsandSize
Yi 2023-11 - 7/34B 4K/200K
areaveragedpermonth.
codingwithalanguagemodelareshowninAlgorithms1 Table2.Models included in our experiments. Cutoff indicates
and 2 (Rissanen, 1976; Pasco, 1976; Nelson, 2014). As
theirknowledgecutoffdate.*Touvronetal.(2023a)donotinclude
anexactknowledgecutoffdate,butmostoftheirpre-trainingdata
thelanguagemodelisworkingonchunks, thearithmetic
usesCommonCrawldumpsfrom2017-2020.†Themodelcardof
codingisalsoperformedonchunkedbatchesandthenthe
Llama-2statesthatthepre-trainingcutoffforthebasemodelis
compresseddataisconcatenatedintothefinalresult. As-
September2022(Touvronetal.,2023b).
sumingthearithmeticoperationswereimplementedwith
infinite precision, the arithmetic encoding can reach the
informationsource. Whencomparedtoencyclopedicdata,
length −log ⌈P(X)⌉+1, which is close to the theoret-
2 the topic and content of news articles can change more
ical optimal length −log P(X) (Dele´tang et al., 2023).
2 randomlyanddramatically(seeAppendixB).Thissource
Apractical implementationwillintroduce inefficiency of
aims to assess large language models’ generalization on
O(n2−B) bits to the final arithmetic code, where B indi-
sucharapidlychangingdomain. Weonlycollectarticles
catestheprecisionoftheimplementedarithmeticoperations
thathaveappearedonthefrontpageofBBCtoensurethat
(Howard&Vitter,1992). Ourimplementationisbasedon
theyarerepresentative. Theaveragelengthofnewsarticles
32-bitarithmetic,inwhichcasetheoverheadisnegligible.
is12Kcharactersperarticle.
4.Experiment GithubCode. Thissourceaimstoassessmodels’gener-
alization on programming code. We monitor 75 popular
4.1.DataCollection GitHubprojectsthathaverichcommithistory. Thetestdata
onlyincludesnewlyaddedfilesandfilesthathavechanged
Weuseamixtureoftext,image,andaudiosourcesinour
dramatically(morethan50%)ineachmonth.
experiments. As discussed in Dele´tang et al. (2023), lan-
guagemodels,whenappliedonmulti-modaldata,areasked ArXiv. We use arXiv papers to test models’ generaliza-
toidentifyusefulpatternsfromthelimitedcontextforcom- tion on scientific data. We randomly collect papers from
pression. Thus,evaluatingcompressioncapabilityonim- alldisciplines. Authorinformation,bibliographies,andap-
ages and audio can serve as an effective measure of the pendicesareexcludedfromtheLaTeXsourcefileandonly
models’in-contextlearningandgeneralizationabilities. We maincontentstartingfromtheintroductionisusedinour
aim to cover a diverse set of domains that are known for experiments.
theirtime-sensitivity. DetailedstatisticsarereportedinTa-
BBC Images. Images in BBC news articles are used in
ble1. Ourtestdataiscollectedspanning83months,from
our testing. We extract contiguous patches of size 64 ×
Jan2017toNov2023. Allsourceshereareunderanopen
128fromallimages. Then,thesepatchesareflattenedand
sourcelicenseorarefreelyaccessibleforresearchpurposes
convertedtograyscalesothateachbyterepresentsexactly
(seeAppendixA).
onepixel.Whendealingwithbytestreamsinlargelanguage
English Wikipedia. English Wikipedia (also referred to models,wesimplyusethereserved token idsforbytesin
as Wikitext below) is a high-quality material for testing thetokenizer. Eachbyteismappedtoauniqueinput idand
encyclopedicknowledge. Unlikepreviousstudiesthatcol- the output space is also adjusted accordingly to the byte
lectdatafromaspecificWikipediadump,wemonitor500 spaceofsize256. Wecollect12MBofdataforeachmonth,
specificarticlesanddocumenttheirversionmonthly. This andensurethetotalsizeis1GB.
providesuswithanevolvingtimelineallowingustoana-
Audio-MixAudiofromBBCradioandpodcastsareused
lyzehowlargelanguagemodelsgeneralizeovertime. The
asouraudiosource. Allaudiofilesareconvertedto16kHz
averagelengthofthesearticlesis55Kcharactersperarticle
FLACformatfromneitherm4pormp3format. Similarto
in2017,whichisincreasedto61Kin2023.
images,audiosamplesarechunkedintobytebatches. We
BBCNewsArticles. Newsisawell-knowntimesensitive randomlysample12MBofdatapermonth,reachingatotal
4EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
Figure1.Thecorrelationbetweenmodelcompressionrate(%,lowerisbetter)andtheircutoffdate.ThecutoffforLLaMAandLlama-2
is2020(estimated)andSeptember2022,respectively(see§4.2fordetails).
sizeof1GB. off date. We then evaluate large language models with
respecttotheirgeneralizationcapabilityandrobustnessand
4.2.ModelsandMetrics present a model-wise comparison. We also compare our
compression-based metric to established benchmarks. In
Weevaluateawiderangeofopen-sourcefoundationmodels
addition,weanalyzehowmodelsgeneralizedifferentlyon
inourexperiments,withdetailsreportedinTable2. Aswe
differenttestingdatasets. Finally,weanalyzehowcontext
areassessingmodels’generalizationovertime,wealsoin-
sizeandtokenizerimplementationaffectthecompression
cludetheirreleasedateandknowledgecutoffdate.Although
rateoflargelanguagemodels.
manyofthesemodelshavetheirtechnicalreportsreleased,
limiteddetailsaresharedregardingtheirpre-trainingdata.
5.1.GeneralizationandTrainingCutoff
Touvronetal.(2023a)reportedthatthepre-trainingdatafor
theLLaMAmodelsprimarilyconsistsofCommonCrawl Inthissection,wefocusontheLLaMAandLlama-2series
dumps from 2017 to 2020, which contribute 82% of the ofmodelsandanalyzethecorrelationbetweentheirperfor-
data. The remaining 18% is composed of content from manceandcutoffdate. Modelperformanceistestedusing
GitHub,arXiv,andWikipedia. Llama-2modelsextendthe wikitextandBBCnewsandtheresultsareillustratedinthe
pre-trainingdataofLLaMAwithmoreup-to-dateinforma- temporalspaceasshowninFigure1.
tion,butwearenotawareoftheprecisecomposition. The
In wikitext, there is a clear divergence in model perfor-
exactknowledgecutoffdateisSeptember2022(Touvron
mancethatalignscloselywiththecutoffdates. From2017
etal.,2023b). Allmodelslistedarebaselanguagemodels
to2020(thetrainingperiodofboth),LLaMAandLlama-2
without supervised fine-tuning or reinforcement learning.
exhibitcomparableandsteadyperformance. However,the
Allresultspresentedinthenextsectionareobtainedwitha
divergenceoccursafterLLaMA’scutoff. LLaMA’sperfor-
2Kcontextwindowunlessotherwisestated. Theselected
mancebeginstoworsenrapidlyafter2020,asshownbythe
modelsarepredominantlygeneral-purposelanguagemod-
sharpincreasingtrendofitscompressionrates. Incontrast,
els, except for CodeLlama, which is a code-specialized
Llama-2, thanks to its more up-to-date cutoff, exhibits a
foundation model developed by further training Llama-2
bettergeneralizationcomparedtoLLaMAafter2020and
on code-specific datasets. In addition to language mod-
maintains the gradual performance decline even after its
els,wealsoinvolveGzip(Deutsch,1996),PNG(Boutell,
September2022cutoff.Thisperformance-cutoffcorrelation
1997), and FLAC (Coalson, 2008) to provide a compara-
ismore obviousonthe BBCNews dataset. Bothmodels
tive perspective against traditional compression methods.
achievesimilarperformancefromOct2017toJan2020(the
Compressionrate(compressedsize/rawsize)isusedasthe
trainingperiodofboth),butaftertheLLaMAcutoff,there
primarymetrictoassessthecompressionperformanceof
isasignificantincreaseinthecompressionrate,indicating
largelanguagemodels.
arapidfall-offinLLaMA’sgeneralizationonnewandun-
seendata. Onthecontrary,Llama-2’sperformanceremains
5.Results consistent until its own cutoff in September 2022, where
Llama-2modelsalsodemonstrateasharpdeclineaftertheir
In this section, we first discuss how the performance of
correspondingcutoff.
large language models correlates with their training cut-
5EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
Wikitext BBCNews Code arXiv Image Audio
Model Avg. 2023 Avg. 2023 Avg. 2023 Avg. 2023 Avg. 2023 Avg. 2023
Baichuan2-7B 7.825 7.932↑.124 8.092 8.385↑.339 4.118 4.293↑.202 9.118 9.018↓.115 169.3 166.2↓3.58 177.3 177.2↓.125
CodeLlama-7B 9.394 9.371↓.027 9.371 9.565↑.223 3.054 3.479↑.490 10.05 9.889↓.184 162.1 159.5↓2.95 146.3 150.0↑4.26
Chatglm3-6B 8.177 8.314↑.158 8.401 8.726↑.375 4.640 4.712↑.082 9.638 9.500↓.158 181.0 174.1↓8.01 183.5 187.9↑5.01
Internlm-7B 9.969 9.992↑.027 9.694 9.955↑.301 4.623 4.924↑.347 11.00 10.88↓.136 180.0 178.9↓1.26 212.1 209.3↓3.26
LLaMA-7B 7.463 7.752↑.333 8.186 8.532↑.398 4.194 4.448↑.292 9.266 9.186↓.091 194.7 190.9↓4.44 198.7 201.7↑3.41
Llama-2-7B 7.349 7.539↑.219 7.794 8.107↑.361 4.339 4.455↑.134 9.197 9.088↓.126 182.3 179.5↓3.28 162.6 166.7↑4.66
Mistral-7B 7.542 7.642↑.115 7.484 7.827↑.396 3.964 4.043↑.091 8.452 8.372↓.092 202.4 198.8↓4.19 195.3 200.0↑5.37
Qwen-7B 7.505 7.768↑.304 8.158 8.472↑.361 3.679 3.858↑.206 9.006 8.911↓.109 - -
Yi-6B 7.897 8.109↑.244 7.969 8.264↑.340 4.492 4.545↑.061 9.144 9.043↓.117 192.1 187.6↓5.17 197.0 201.0↑4.70
LLaMA-13B 6.488 6.979↑.566 7.959 8.359↑.461 4.013 4.296↑.326 9.003 8.934↓.080 176.0 171.2↓5.55 177.0 179.2↑2.21
Llama-2-13B 6.342 6.658↑.364 7.528 7.886↑.413 4.138 4.288↑.172 8.930 8.838↓.106 158.5 155.6↓3.37 159.5 162.6↑3.12
LLaMA-65B 3.733 4.673↑1.10 7.339 7.986↑.746 3.363 3.756↑.453 8.391 8.376↓.018 175.2 172.4↓3.17 162.2 167.9↑5.68
Llama-2-70B 3.461 3.995↑.622 6.754 7.372↑.711 3.534 3.770↑.272 8.291 8.242↓.056 193.9 189.2↓5.36 175.9 178.2↑2.31
Yi-34B 6.204 6.624↑.490 7.321 7.713↑.452 4.030 4.159↑.149 8.547 8.475↓.083 185.1 183.1↓2.34 180.7 176.1↓4.61
FLAC 95.09 95.08↓.023 95.14 95.05↓.094 95.06 94.95↓.118 96.14 96.00↓.161 81.20 80.96↓.274 76.26 69.98↓7.24
PNG 37.85 37.75↓.115 36.71 37.40↑.800 17.90 16.42↓1.71 33.15 33.08↓.078 64.91 64.70↓.245 86.30 89.47↑3.65
Gzip 37.76 37.66↓.115 36.59 37.29↑.801 17.71 16.21↓1.73 33.07 33.00↓.073 64.88 64.67↓.243 86.29 89.46↑3.66
Table3.Compressionrate(%,lowerisbetter)onsixtestingdata.Avg.=Rate and2023=Rate .Theperformancedifference
17−23 23
(Rate −Rate )isindicatedbyanarrowfollowedbyanumber. ↑indicatesahigher(worse)rateon2023. Supposetherate
23 17−22
changeslinearly,wecouldobtainanestimatedfutureperformanceRate byaddingRate totheratedifferenceobservedin2023.
24 23
Modelsarehighlightedinbold(1stbest)andunderlined(2ndbest)basedontheirestimatedfuturegeneralization.Modelsunderandover
7Barecomparedseparately.
Thesepatternssuggestthatwhilstmodelsmaydemonstrate period),andviceversa. Tovisuallycomparegeneralization
similarperformanceondatafromtheirtrainingperiod,their androbustness,weplaceeachmodelonthe2Dperformance-
abilitytogeneralizeonfuturedatacandiffersignificantly. robustness space in Figure 2 (b). The x-axis represents
Thisobservationconfirmsdatacontaminationissuesinex- performanceonthe2023testingperiod, withlowerrates
istinglanguagemodelevaluations. Manybenchmarksuse indicatingstrongergeneralizationonnewdata. They-axis
datathatoverlapswithmodels’trainingperiods,therefore denotesthegapbetweentrainingandtestingperformance,
potentiallyfailingtoaccuratelymeasuremodelgeneraliza- withsmallergapsdenotingstrongerrobustness. Therelative
tioncapabilities. Asaresult,ourapproachfocusesonthe positionofeachmodelrevealsitscapabilities. Modelsin
performanceonposttraining-perioddata,andincludesthe thetoprightexhibitbothstronggeneralization(lowRate )
23
robustness dimension measured by the performance gap andhighrobustness(smalltraining-testinggap). Modelsin
betweenthetrainingandtestingperiods. thelowerrightachievegoodgeneralizationbutlessrobust-
nessovertime. Modelsinthetopleftdemonstrateweaker
5.2.ModelComparison generalizationbutmaintainrobustnessbetweenperiods. We
alsoillustratecompressionratesovertimeinFigure2(a).
In this section, we test and compare a wide selection of
NotethatQwenisnottestedonmulti-modaldatasetsasit
largelanguagemodelsontheirgeneralizationandrobust-
doesnotreservebytetokensinitsvocabulary.
ness. Inourexperiments,wesplitthethetestdataintothe
trainingperiod(2017-2022)andthetestingperiod(2023). Ourresultsrevealthefollowingkeyfindings.Firstly,models
Althoughmanymodelsdonotreporttheirexactcutoffdates, candemonstratewidelydivergentperformanceontraining
wechoose2023asthetestingperiodaccordingtotheirre- versustestingdata. Forinstance,LLaMA-65B’scompres-
leasetime(cf. Table2). As2023isratherrecent,webelieve sionrateworsens20%on2023Wikipediadatacompared
itisnotlargelyincludedinthepre-trainingstage. There- to2017-2022. Secondly,CodeLlamaexcelsoncodedata
sultsonoursixtestdatasetsarereportedinTable3. Firstly, thankstoitsadditionaltrainingoncodecorporabutexhibits
weshowtheiraveragedperformanceovertheentireperiod lowerrobustnesscomparedtobaseLlama-2. Thissuggests
(i.e,Avg. = Rate ). Then,performanceonthe2023 thatalthoughfurthertrainingondomainknowledgecanlead
17−23
splitisshownspecifically(i.e., 2023 = Rate ). Finally, tobetterdomaincapability,itmayresultinweakergener-
23
theperformancedifferencebetweenthetrainingandtest- alizationcomparedtothebasemodel. Thirdly,analyzing
ingperiodsisreportedasthemeasureforrobustness(i.e., temporaltrendsprovidesinsightsonfuturegeneralization.
Rate −Rate ). We use an arrow to represent the InFigure2(a),MistralandBaichuan2displaymoregrad-
23 17−22
directionofperformancechange. Anuparrowdenotesthat ual upward curves, suggesting better potential for future
amodelachievesaworsecompressionrateon2023(testing performance than models with steeper uptrends. Finally,
period)datacomparedtotheperiodof2017-2022(training the 2-D visualization in Figure 2 (b) enables comparison
6EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
Figure2.(a)Compressionrateonwikitext;(b)Robustness(gapbetweenRate andRate )andperformance(Rate ),testedon
23 17−22 23
Wikitext.InternLMandCodeLlamaareexcludedfrom(a)forthesakeoffigurereadability.
Model HumanEval Code(ours) MMLU ArXiv(ours) 5.4.GeneralizationonDifferentDataSources
CodeLlama 33.5(#1) 3.479(#1) 36.9(#6) 9.889(#6)
Our findings also show insights into how large language
LLaMA-7B 11.6(#6) 4.448(#5) 35.1(#7) 9.186(#5)
Llama-2-7B 12.8(#5) 4.455(#6) 45.3(#5) 9.088(#4) modelsgeneralizeacrossdiversedatasets. Firstly,models
Mistral-7B 30.5(#3) 4.043(#3) 60.1(#1) 8.372(#1) facemorechallengeswith2023Wikipedia,news,andcode
Qwen-7B 32.3(#2) 3.838(#2) 58.2(#2) 8.911(#2)
databutnotwitharXivormulti-modaldata. Gzipcompres-
Baichuan2-7B 18.3(#4) 4.293(#4) 54.2(#3) 9.018(#3)
sionratesarelowerforWikipediaandcodein2023,indi-
InternLM-7B 10.4(#7) 4.924(#7) 51.0(#4) 10.88(#7)
catingmorerepetitivepatterns. However,languagemodels
Table4.Relation between compression rate (Rate , %) and performworseon2023data,revealingdifficultiesingen-
23
benchmarkscore. pass@1forHumanEval,5-shotaccuracyfor eralizingtonewtime-sensitivetextualdata. Surprisingly,
MMLU.ResultsforHumanEvalandMMLUarefromtheofficial forarXiv,modelsmaintainorevenimproveperformance
paperorwebsiteofeachrespectivemodel.
on 2023 data, perhaps due to consistent writing styles in
academicpapers. Secondly,largelanguagemodelsstruggle
withpurebytestreams,asseenintheircompressionrates
ofperformance-robustnesstrade-offs. Overall,Mistral-7B onmulti-modaldatasets,suggestingdifficultyinidentifying
achievesthemostfavorablebalanceamongmodelsunder bytepatterns. Futureworkwillincludemulti-modalmod-
7BandLlama-2-70Bisthebestamongmodelsover7B.We elsinbytestreamcompression. Finally,ontime-sensitive
includemorefiguresofcompressionratevisualizationinthe textual datasets, models exhibit distinct trends. In Fig-
temporalspaceandperformance-robustness2-Drelations ure1,wikitextperformancegraduallydeclinespost-cutoff,
inAppendixD. suggesting slow changes in encyclopedic knowledge for
modelgeneralization.Incontrast,newsdatasetperformance
sharplydrops,indicatingtherapidcontentchangesthatare
5.3.ComparisontoEstablishedBenchmarks
prevalentinnews. Weincludemorefiguresdemonstrating
To further analyze our compression-based evaluation howmodelsgeneralizeonthenews,code,image,andaudio
method, we compare compression rates on our code and datasetsinAppendixC.
arXivdatasetsversusscoresontheestablishedbenchmarks
HumanEval (Chen et al., 2021) and MMLU (Hendrycks
5.5.ContextSizeandPerformance
etal.,2020). HumanEvalisabenchmarkfocusingonpro-
grammingtasks,andMMLUisacomprehensiveevaluation Contextsizeisakeyfactorincompressionalgorithms. Typ-
consistingofacademictests. AsshowninTable4,compres- ically,alargercontextallowsalgorithmstoidentifyuseful
sionperformancecorrelatescloselywithmodels’ranking patternsinabroaderscope,leadingtobetterperformance.
onthesebenchmarks. Foracademictests,wecompareour Thisprinciplealsoappliestolargelanguagemodels,where
arXivresultstoMMLU.AlthougharXivcontainsprimarily anextensivecontextiscrucialforcomplexreasoning,long-
newscientificpapersratherthantraditionalacademicknowl- dependencyquestion-answering,andcodingtasks(Touvron
edge(likeMMLU),itisthemostrelevantsourceamongour etal.,2023b). However,largercontextsinevitablyrequire
collecteddatasets. Overall,thisclosecorrelationsuggests morecomputingandmemory,soatrade-offhastobemade
ourcompression-basedmetriccanbeaneffectivemethod forcompressionalgorithmsandlargelanguagemodels. In
formodelevaluation. this section, we examine how the context size affects the
7EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
2K 4K 8K 2K+SW
Model Rate Time(s) Mem(mb) Rate Time(s) Mem(mb) Rate Time(s) Mem(mb) Rate Time(s) Mem(mb)
Chatglm3-6B 8.314 110 13563 8.144 96 15175 8.064 98 18397 7.965 422 13562
Baichuan2-7B 7.932 134 19548 7.831 146 24843 - - - 7.731 518 19550
Qwen-7B 7.768 100 20043 7.662 93 25347 7.606 92 35951 7.555 401 20041
Mistral-7B 7.642 118 15487 7.509 108 16639 7.462 107 18945 7.385 475 15487
Llama-2-7B 7.539 114 15615 7.409 101 18303 - - - 7.283 421 15615
Table5. Compressionrate(%)comparisonforcontextsizesonthe2023splitofwikitext.
Model Vocab size #Tokens BPT BPB eredasaformofpre-compression(Dele´tangetal.,2023).
Inthissection,weanalyzehowtheimplementationofto-
Qwen-7B 152K 12382K 2.7511 0.6215
kenizationaffectsmodels’compressionrates. Weemploy
Baichuan2-7B 126K 12824K 2.7135 0.6346
twometricsinTable6: bitspertoken(BPT)andbitsper
Chatglm3-6B 65K 13531K 2.6951 0.6652
Llama-2-7B 32K 14324K 2.3086 0.6032 character(BPC),indicatingthenumberofbitsrequiredto
Mistral-7B 32K 14006K 2.3929 0.6113 representatokenorcharacter,respectively. AlowerBPT
or BPC suggests more efficient data compression. How-
Table6.Comparisonofmodelswithdifferenttokenizationimple- ever, they focus on different aspects: BPT assesses the
mentationsonthe2023splitofwikitext. model’sefficiencyatthetokenlevel,whilstBPCevaluates
performanceatthecharacterlevel. Additionally,thetable
includesthetotalnumberoftokens,reflectingtheresulting
compressionrateofmodelsandtheircomputationalcosts.
tokencountfortheentiretestdataset.
In Table 5, we present the performance of models across
Ourkeyfindingsareasfollows: Firstly,modelsoptimized
different context sizes: 2K, 4K, 8K, and 2K with a slid-
formultilingualscenariosoftenhaveasignificantlylarger
ing window (2K+SW). For the first three configurations,
vocabulary size than monolingual models. For example,
compressionisperformedchunkbychunk,withnooverlap
Qwen,asamultilingualmodel,hasa5timeslargervocab-
between chunks. In the sliding window setup, we use a
ulary than that of Llama-2. Secondly, a large vocabulary
stepsizeof512,whereonlytheprobabilitiesofthelast512
generallyleadstofewertokens,whichmeanstokenizersper-
tokensarecalculatedineachstep,andthefirst1536tokens
formbetteronpre-compression. Thirdly,alargevocabulary
serve to provide context. This method requires approxi-
mayleadtoincreasedcomplexityintoken-levelpredictions.
matelyfourtimesthecomputingresourcescomparedtoa
AsshowninTable6,modelswithlargervocabulariestend
2Kcontextbutisexpectedtoyieldbetterperformance. Our
toachieveahigherBPTingeneral,indicatingthechallenge
keyfindingsareasfollows. Firstly,thereisageneraltrend
of token prediction with more fine-grained tokenization.
ofimprovedcompressionratesasthecontextsizeincreases
Note that this analysis was conducted on purely English
from2Kto4Kandthento8Kacrossallmodels. However,
data,whichinherentlyfavorsEnglishmodels,especiallyin
theextentofimprovementdiminisheswithlargercontexts,
tokenizationanalysis. Experimentswithmultilingualtest
withtheperformancegainfrom2Kto4Kbeinggenerally
datashouldbeincludedinfutureworks.
moresignificantthanthatfrom4Kto8K.Secondly,thereis
anoticeableincreaseinmemoryusageasthecontextsize
6.Conclusion
grows. MistralandChatGLM3showimpressivememory
efficiencyindealingwithlargecontext,perhapsthanksto
We proposed to evaluate large language models through
theiruniqueattentionimplementation(Jiangetal.,2023).
losslessdatacompressionacrosstimeongeneralizationand
In contrast, large contexts do not consume more time to
robustness. Our method measures compression rates on
finishcompressing. Thisisbecausetherearefewerchunks
new, unseen data that is created after a model’s training
whenweworkwithlargercontextsandachunk-by-chunk
cutoff,asametricforgeneralization. Italsoquantifiesthe
approach. Lastly,the2Kcontextsizewithaslidingwindow
gapbetweenthetrainingandtestingperiodsasametricof
(2K+SW)consistentlyoutperformsthelargerstaticcontext
robustness.Theproposedmethodavoidsdatacontamination
sizesinallmodels. Thismightresultfromthecontinuous
andthepotentialinterferenceofdifferentpromptsinexisting
contextprovidedbytheslidingwindow,whichwebelieve
benchmark-based evaluation. We provided an extensive
iscrucialfortheinitialpartofeachchunk.
analysisfor14representativelargelanguagemodels,and
alsoinvestigatedtheimpactofcontextsizeandtokenization
5.6.TokenizationandPerformance
implementations.
Tokenization aims to represent linguistic input more effi-
cientlybeforelanguagemodeling,andthusisoftenconsid-
8EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
7.Impact Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and
Salakhutdinov, R. Transformer-xl: Attentive language
Thispaperfocusesonevaluationmethodsforlargelanguage
models beyond a fixed-length context. arXiv preprint
models. There are many potential societal consequences
arXiv:1901.02860,2019.
ofourwork,nonewhichwefeelmustbespecificallyhigh-
lightedhere. Dele´tang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Ge-
newein,T.,Mattern,C.,Grau-Moya,J.,Wenliang,L.K.,
References Aitchison,M.,Orseau,L.,etal. Languagemodelingis
compression. arXivpreprintarXiv:2309.10668,2023.
Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,
Aleman,F.L.,Almeida,D.,Altenschmidt,J.,Altman,S., Deutsch,P. Gzipfileformatspecificationversion4.3. RFC,
Anadkat,S.,etal. Gpt-4technicalreport. arXivpreprint 1996.
arXiv:2303.08774,2023.
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert:
Anil,R.,Borgeaud,S.,Wu,Y.,Alayrac,J.-B.,Yu,J.,Sori- Pre-training of deep bidirectional transformers for lan-
cut,R.,Schalkwyk,J.,Dai,A.M.,Hauth,A.,etal. Gem- guageunderstanding. arXivpreprintarXiv:1810.04805,
ini: afamilyofhighlycapablemultimodalmodels. arXiv 2018.
preprintarXiv:2312.11805,2023.
Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z.,
Bai,J.,Bai,S.,Chu,Y.,Cui,Z.,Dang,K.,Deng,X.,Fan, and Tang, J. Glm: General language model pretrain-
Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical ing with autoregressive blank infilling. arXiv preprint
report. arXivpreprintarXiv:2309.16609,2023. arXiv:2103.10360,2021.
Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
N.,Rajani,N.,Sanseviero,O.,Tunstall,L.,andWolf,T. M., Song, D., and Steinhardt, J. Measuring mas-
Openllmleaderboard. https://huggingface.co/,2023. sive multitask language understanding. arXiv preprint
arXiv:2009.03300,2020.
Boutell,T. Png(portablenetworkgraphics)specification
version1.0. RFC,1997. Howard,P.G.andVitter,J.S. Analysisofarithmeticcoding
fordatacompression. Informationprocessing&manage-
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D., ment,28(6):749–763,1992.
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell,A.,etal. Languagemodelsarefew-shotlearners. Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T.,
Advancesinneuralinformationprocessingsystems,33: Liu, J., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., and
1877–1901,2020. He, J. C-eval: A multi-level multi-discipline chinese
evaluationsuiteforfoundationmodels. arXivpreprint
Chang,Y.,Wang,X.,Wang,J.,Wu,Y.,Yang,L.,Zhu,K., arXiv:2305.08322,2023.
Chen,H.,Yi,X.,Wang,C.,Wang,Y.,etal. Asurveyon
evaluationoflargelanguagemodels. ACMTransactions Hutter,M. 500’000Cprizeforcompressinghumanknowl-
onIntelligentSystemsandTechnology,2023. edge. http://prize.hutter1.net/,2006.
Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,Pinto,H.P.d.O., InternLM. Internlm: Amultilinguallanguagemodelwith
Kaplan,J.,Edwards,H.,Burda,Y.,Joseph,N.,Brockman, progressivelyenhancedcapabilities. (2023-01-06)[2023-
G., etal. Evaluatinglargelanguagemodelstrainedon 09-27].https://github.com/InternLM/InternLM,2023.
code. arXivpreprintarXiv:2107.03374,2021.
Jacovi,A.,Caciularu,A.,Goldman,O.,andGoldberg,Y.
Chen,Y.,Ding,N.,Wang,X.,Hu,S.,Zheng,H.-T.,Liu,Z., Stopuploadingtestdatainplaintext: Practicalstrategies
andXie,P. Exploringlotterypromptsforpre-trainedlan- formitigatingdatacontaminationbyevaluationbench-
guagemodels. arXivpreprintarXiv:2305.19500,2023. marks. arXivpreprintarXiv:2305.10160,2023.
Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra, Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Chaplot,D.S.,Casas,D.d.l.,Bressand,F.,Lengyel,G.,
Gehrmann,S.,etal. Palm: Scalinglanguagemodeling Lample,G.,Saulnier,L.,etal. Mistral7b. arXivpreprint
withpathways. JournalofMachineLearningResearch, arXiv:2310.06825,2023.
24(240):1–113,2023.
Li, Y., Dong, B., Lin, C., and Guerin, F. Compressing
Coalson,J. Freelosslessaudiocodec. https://xiph.org/flac/, contexttoenhanceinferenceefficiencyoflargelanguage
2008. models. arXivpreprintarXiv:2310.06201,2023a.
9EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
Li,Y.,Geurin,F.,andLin,C. Latesteval: Addressingdata Sainz, O., Campos, J. A., Garc´ıa-Ferrero, I., Etxaniz, J.,
contaminationinlanguagemodelevaluationthroughdy- deLacalle,O.L.,andAgirre,E.Nlpevaluationintrouble:
namicandtime-sensitivetestconstruction. arXivpreprint Ontheneedtomeasurellmdatacontaminationforeach
arXiv:2312.12343,2023b. benchmark. arXivpreprintarXiv:2310.18018,2023.
Li,Y.,Guerin,F.,andLin,C. Anopensourcedatacontami- Sclar, M., Choi, Y., Tsvetkov, Y., andSuhr, A. Quantify-
nationreportforlargelanguagemodels. arXivpreprint inglanguagemodels’sensitivitytospuriousfeaturesin
arXiv:2310.17589,2023c. promptdesignor: Howilearnedtostartworryingabout
prompt formatting. arXiv preprint arXiv:2310.11324,
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
2023.
Levy,O.,Lewis,M.,Zettlemoyer,L.,andStoyanov,V.
Roberta: Arobustlyoptimizedbertpretrainingapproach. Shannon,C.E. Amathematicaltheoryofcommunication.
arXivpreprintarXiv:1907.11692,2019. TheBellsystemtechnicaljournal,27(3):379–423,1948.
Marcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre, Sutskever, I. An observation on generalization.
R., Bies, A., Ferguson, M., Katz, K., and Schasberger, https://www.youtube.com/watch?v=AKMuA TVz3A,
B. Thepenntreebank: Annotatingpredicateargument 2023. Accessed: 2024-01-25.
structure. InHumanLanguageTechnology: Proceedings
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
of a Workshop held at Plainsboro, New Jersey, March
M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
8-11,1994,1994.
Azhar,F.,etal. Llama:Openandefficientfoundationlan-
Merity, S., Xiong, C., Bradbury, J., and Socher, R. guagemodels. arXivpreprintarXiv:2302.13971,2023a.
Pointer sentinel mixture models. arXiv preprint
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
arXiv:1609.07843,2016.
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Nelson, M. Data compression with arithmetic Bhosale,S.,etal. Llama2: Openfoundationandfine-
coding. https://marknelson.us/posts/2014/10/19/data- tuned chat models. arXiv preprint arXiv:2307.09288,
compression-with-arithmetic-coding.html,2014. 2023b.
OpenCompass. Opencompass: Auniversalevaluationplat- Valmeekam,C.S.K.,Narayanan,K.,Kalathil,D.,Cham-
form for foundation models. https://github.com/open- berland,J.-F.,andShakkottai,S. Llmzip: Losslesstext
compass/opencompass,2023. compressionusinglargelanguagemodels. arXivpreprint
arXiv:2306.04050,2023.
Pasco,R.C. Sourcecodingalgorithmsforfastdatacom-
pression. PhDthesis,StanfordUniversityCA,1976. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman,S.R. Glue: Amulti-taskbenchmarkandanal-
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
ysisplatformfornaturallanguageunderstanding. arXiv
Sutskever,I.,etal. Languagemodelsareunsupervised
preprintarXiv:1804.07461,2018.
multitasklearners. OpenAIblog,1(8):9,2019.
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,
Rae,J.CompressionforAGI-JackRae—StanfordMLSys
Michael, J.,Hill, F.,Levy, O., andBowman, S. Super-
#76. https://www.youtube.com/watch?v=dO4TPJkeaaU,
glue: Astickierbenchmarkforgeneral-purposelanguage
2023. Accessed: 2024-01-25.
understandingsystems. Advancesinneuralinformation
Rajpurkar,P.,Jia,R.,andLiang,P. Knowwhatyoudon’t processingsystems,32,2019.
know: Unanswerablequestionsforsquad. arXivpreprint
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,
arXiv:1806.03822,2018.
Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought
Rissanen,J.J. Generalizedkraftinequalityandarithmetic prompting elicits reasoning in large language models.
coding. IBMJournalofresearchanddevelopment,20(3): AdvancesinNeuralInformationProcessingSystems,35:
198–203,1976. 24824–24837,2022.
Roberts,M.,Thakur,H.,Herlihy,C.,White,C.,andDooley, Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin,
S. Datacontaminationthroughthelensoftime. arXiv C.,Lv,C.,Pan,D.,Wang,D.,Yan,D.,etal. Baichuan
preprintarXiv:2310.10628,2023. 2: Open large-scale language models. arXiv preprint
arXiv:2309.10305,2023a.
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,
Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Yang, Y., Mandt, S., Theis, L., et al. An introduction to
Code llama: Open foundation models for code. arXiv neuraldatacompression. FoundationsandTrends®in
preprintarXiv:2308.12950,2023. ComputerGraphicsandVision,15(2):113–200,2023b.
10EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
Yi. Aseriesoflargelanguagemodelstrainedfromscratch
bydevelopersat01-ai. https://github.com/01-ai/Yi,2023.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,
etal.Opt:Openpre-trainedtransformerlanguagemodels.
arXivpreprintarXiv:2205.01068,2022.
11EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
A.DataLicense
WikipediaandarXivareundertheCCBY-SAlicensewhichgrantsfreeuseoftheircontent. BBCdataincludingnews,
images,andradioareundertheEducationalRecordingAgency(ERA)license,whichallowseducationalusageoftheir
content. Forthecodedatasets,weonlycollectdatafromGitHubprojectsunderMITorBSDlicenses. Forthepodcastdata
intheaudiodataset,wecollectpodcastaudiofromYouTubethatareundertheCreativeCommonslicense,whichallows
freeuseoftheircontent.
B.TestDataAnalysis
Hereweinvestigatehowdifferentourtestdataiswhencollectedatmonthlyintervals. WeusetheGestaltpatternmatching
algorithm (implemented by Python difflib.SequenceMatcher) to compute text similarity. For Wikipedia, we
performthecomparisonarticle-by-article,whileforothersourcessuchasnews,code,andarXiv,weconcatenatealldata
togetherbeforecomputingthesimilarity. Allsourcesdemonstratelargedifferencesover0.9(similaritybelow0.1)except
Wikipedia. Theaveragedifferencesare0.98,0.91,0.96forBBCnews,Code,andarXivrespectively. AsshowninFigure3,
sincewearemonitoringaspecificsetofWikipediaarticles,thesewikipageschangeapproximatelylinearlyovertime.
Figure3. ThedifferencesinmonthlyWikipediadata.
C.GeneralizationonDifferentDomains
We mentioned that large language models behave quite differently across domains. Here, we add more visualizations
showinghowwellmodelsgeneralizeovertimeondifferenttestdatasets. AsshowninFigure4,wecompareallmodelsunder
7Bparameters,excludinglargermodelstoavoidaclutteredfigure. Thekeyfindingswecouldderivefromthetemporal
visualizationsarethetrendsandslopesofmodelperformanceintherecentperiod. Forexample,ontheBBCnewsdataset,
allmodelsdemonstrateanincreasingtrendonthe2023split. Notably,theirincreasingslopesseemsimilaroverall. This
suggeststhemodelshavecomparablerobustness,perhapsbecausenewstopicsoftenchangerandomlyanddramatically. In
contrast,ontheGitHubcodedataset,theincreasingslopesofCodeLlamaandInternLMaremoresignificantthanother
modelsafter2023,highlightingtheirpotentialweakerrobustnessoncodingtasks.
D.ModelComparisononDifferentSources
Weaddtheperformanceandrobustness2DvisualizationsofGitHubcodeandBBCnewsdatasets,asshowninFigure5.
12EvaluatingLargeLanguageModelsforGeneralizationandRobustnessviaDataCompression
Figure4. Compressionrateschangeovertimeondifferentdomains.
Figure5. TherelationbetweenperformanceandrobustnessonGitHubcode(left)andBBCnews(right).
13