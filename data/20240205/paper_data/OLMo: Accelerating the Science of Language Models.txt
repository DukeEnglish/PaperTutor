: Accelerating the Science of Language Models
OLMo
DirkGroeneveldα IzBeltagyα
PeteWalshα AkshitaBhagiaα RodneyKinneyα OyvindTafjordα
AnanyaHarshJhaα HamishIvisonαβ IanMagnussonα YizhongWangαβ
ShaneAroraα DavidAtkinsonα RussellAuthurα KhyathiRaghaviChanduα
ArmanCohanγα JenniferDumasα YanaiElazarαβ YulingGuα
JackHesselα TusharKhotα WilliamMerrillδ JacobMorrisonα
NiklasMuennighoff AakankshaNaikα CrystalNamα MatthewE.Petersα
ValentinaPyatkinαβ AbhilashaRavichanderα DustinSchwenkα SaurabhShahα
WillSmithα EmmaStrubellαµ NishantSubramaniα MitchellWortsmanβ
PradeepDasigiα NathanLambertα KyleRichardsonα
LukeZettlemoyerβ JesseDodgeα KyleLoα LucaSoldainiα
NoahA.Smithαβ HannanehHajishirziαβ
αAllenInstituteforArtificialIntelligence
βUniversityofWashington γYaleUniversity
δNewYorkUniversity µCarnegieMellonUniversity
olmo@allenai.org
Abstract
Language models (LMs) have become ubiquitous in both NLP research and in
commercial product offerings. As their commercial importance has surged, the
most powerful models have become closed off, gated behind proprietary inter-
faces,withimportantdetailsoftheirtrainingdata,architectures,anddevelopment
undisclosed. Giventheimportanceofthesedetailsinscientificallystudyingthese
models,includingtheirbiasesandpotentialrisks,webelieveitisessentialforthe
researchcommunitytohaveaccesstopowerful,trulyopenLMs. Tothisend,this
technical report details the first release of OLMo, a state-of-the-art, truly Open
Language Model and its framework to build and study the science of language
modeling. Unlike most prior efforts that have only released model weights and
inference code, we release OLMo and the whole framework, including training
data and training and evaluation code. We hope this release will empower and
strengthentheopenresearchcommunityandinspireanewwaveofinnovation.
Weights https://huggingface.co/allenai/OLMo-7B
Code https://github.com/allenai/OLMo
Data https://huggingface.co/datasets/allenai/dolma
Evaluation https://github.com/allenai/OLMo-Eval
Adaptation https://github.com/allenai/open-instruct
4202
beF
1
]LC.sc[
1v83800.2042:viXra1 Introduction
Language models have been at the center of NLP technologies for many years (Rosenfeld, 2000;
Bengio et al., 2003; Mikolov et al., 2013; Peters et al., 2018; Brown et al., 2020). Recently, due
to large-scale pretraining and human annotation for alignment, they have become commercially
valuable (OpenAI, 2023). However, as their commercial value has increased, the largest models
havebecomegatedbehindproprietaryinterfaces,withimportantdetailsleftundisclosed.
We believe that full access to open language models for the research community is critical to the
scientificstudyofthesemodels,theirstrengthsandweaknesses,andtheirbiasesandrisks. Accord-
ingly, we introduce OLMo, a state-of-the-art, truly open language model and framework to build,
study, and advance LMs, along with the training data, training and evaluation code, intermediate
modelcheckpoints,andtraininglogs.
RecentLMreleaseshavevariedintheirdegreeofopenness. Forexample, Mistral8x7Bprovided
model weights and a brief report (Jiang et al., 2024), while LLaMA came with in-depth adapta-
tion training instructions (Touvron et al., 2023b), and Mosaic Pretrained Transformer came with
many details, including the dataset distribution, though not the data itself (MosaicML NLP Team,
2023). Falcon’spretrainingdatawaspartiallyreleased(Almazroueietal.,2023),andthemostopen
models—thePythiasuite(Bidermanetal.,2023)andBLOOM(BigScienceetal.,2022)—released
trainingcode,modelcheckpoints,trainingdataandmore.
OLMo releases the whole framework from data to training to evaluation tools: multiple training
checkpointsacrossmultiplehardwaretypes,traininglogs,andexactdatasetsused,withapermissive
license. Wearenottheonlyteamtodothis; recentworkfromLLM360targetssimilargoals(Liu
etal.,2023). OLMonarrowsthegapfromtheirmodelstostate-of-the-artcapabilitiesofmodelslike
LLaMA2. This project has benefited from lessons learned from all of these previous efforts with
theirvaryingdegreesofopenness,andwebelievethatalarge,diversepopulationofopenmodelsis
thebesthopeforscientificprogressonunderstandinglanguagemodelsandengineeringprogresson
improvingtheirutility.
The OLMo framework encompasses the tools and resources required for building and researching
languagemodels. Fortrainingandmodeling,itincludesfullmodelweights,trainingcode,training
logs, ablations, training metrics in the form of Weights & Biases logs, and inference code. This
firstreleaseincludesfourvariantsofourlanguagemodelatthe7Bscalecorrespondingtodifferent
architectures, optimizers, and training hardware, and one model at the 1B scale, all trained on at
least2Ttokens. Wearealsoreleasinghundredsofintermediatecheckpointsavailableasrevisions
onHuggingFace. Fordatasetbuildingandanalysis,itincludesthefulltrainingdatausedforthese
models, including code that produces the training data, from AI2’s Dolma (Soldaini et al., 2024),
andWIMBD(Elazaretal.,2023)foranalyzingpretrainingdata. Forevaluation, itincludesAI2’s
Catwalk(Groeneveldetal.,2023)fordownstreamevaluationandPaloma(Magnussonetal.,2023)
for perplexity-based evaluation. For instruction-tuning, we released Open Instruct (Ivison et al.,
2023;Wangetal.,2023),andwearecurrentlyusingittoproduceanadapted(instruction-tunedand
RLHFed)versionofOLMo,whichwewillreleasesoon. Finally,allcodeandweightsarereleased
undertheApache2.0License.1
Thisisthefirststepinalongseriesofplannedreleases,continuingwithlargermodels,instruction-
tuned models, and more modalities and variants down the line. We therefore hope to catalyze re-
searchintoas-yetpoorlyunderstoodaspectsofthesemodels,forexample,therelationshipbetween
pretrainingdataandmodelcapabilities,theimpactofdesignandhyperparameterchoices,andvari-
ousoptimizationmethodsandtheirimpactonmodeltraining. Inaddition,wereportonthelessons
learnedandimportantdetailsnecessarytosuccessfullytrainlanguagemodelsatthisscale.
2 OLMoFramework
This section describes the OLMo framework, consisting of the OLMo models (Section 2.1), our
pre-trainingdataset,Dolma(Section2.2),andourevaluationframework(Section2.3).
1http://www.apache.org/licenses/LICENSE-2.0
22.1 OLMoModelandArchitecture
We adopt a decoder-only transformer architecture based on Vaswani et al. (2017), and deliver 1B
and7BvariantsasdescribedinTable1,witha65Bversioncomingsoon. Ourspecificarchitecture
includes several improvements over the vanilla transformer from Vaswani et al. (2017) following
otherrecentlargelanguagemodelslikePaLM(Chowdheryetal.,2022),theLLaMAfamily(Tou-
vron et al., 2023a,b), OpenLM (Gururangan et al., 2023), and Falcon (Almazrouei et al., 2023).
Table 2 gives a comprehensive comparison of our 7B architecture to the similarly-sized models
fromtheseotherfamilies.
Size Layers HiddenSize AttentionHeads TokensTrained
1B 16 2048 16 2T
7B 32 4086 32 2.46T
65B* 80 8192 64
Table1: OLMomodelsizesandthemaximumnumberoftokenstrainedto.
*Atthetimeofwritingour65Bmodelisstilltraining.
Wegenerallyselecthyperparametersbyoptimizingfortrainingthroughputonourhardwarewhile
minimizing the risk of loss spikes and slow divergence. We ablate choices through our in-loop
evaluation setting, given available computational sources (Section 2.3). Table 2 compares our de-
signchoiceswithrecentstate-of-the-artopenlanguagemodels. Ourmainchangesoverthevanilla
transformerarchitecturecanbesummarizedasfollows:
1. No biases. Following LLaMA, PaLM, and others, we exclude all bias terms from our
architectureinordertoimprovetrainingstability.
2. Non-parametriclayernorm. Weusethenon-parametricformulationoflayernorm(Ba
etal.,2016)inwhichthereisnoaffine transformation withinthenorm, i.e. no“adaptive
gain”(orbias). Webelievethiswasthesafestoptionanditwasalsothefastestcompared
to the other variants we considered: parametric layer norm and RMSNorm (Zhang and
Sennrich,2019).
3. SwiGLU activation function. Like LLaMA, PaLM, and others we use the SwiGLU ac-
tivation function (Shazeer, 2020) instead of ReLU, and following LLaMA the activation
hiddensizeisapproximately 8d,butincreasedtotheclosestmultipleof128(e.g. 11,008
3
forour7Bmodel)toimprovethroughput.2
4. Rotarypositionalembeddings(RoPE).LikeLLaMA,PaLM,andotherswereplaceab-
solutepositionalembeddingswithrotarypositionalembeddings(RoPE;Suetal.,2021).
5. Vocabulary. WeuseamodifiedversionoftheBPE-basedtokenizerfromGPT-NeoX-20B
(Black et al., 2022) with additional tokens for masking personal identifiable information
(PII). The final vocabulary size is 50,280. However, to maximize training throughput we
increasethesizeofthecorrespondingembeddingmatrixinourmodelto50,304sothatit’s
amultipleof128.
2.2 PretrainingData: Dolma
Despiteprogressinaccesstomodelparameters,pretrainingdatasetsarestillnotasopen.Pretraining
data are often not released alongside open models (let alone closed models) and documentation
about such data is often lacking in detail that would be needed to reproduce or fully understand
the work. This has made it difficult to support certain threads of language model research, such
as understanding how training data impacts model capabilities and limitations. To facilitate open
research on language model pretraining, we built and released our pretraining dataset, Dolma—
a diverse, multi-source corpus of 3T tokens across 5B documents acquired from 7 different data
2SinceSwiGLUisa“gated”activationfunction,theoutputishalfthesizeoftheinput. Sotechnicallyour
inputstoSwiGLUhaveadimensionalityof2×11,008=22,016forour7Bmodel.
3OLMo-7B LLaMA2-7B OpenLM-7B Falcon-7B PaLM-8B
Dimension 4096 4096 4096 4544 4096
Numheads 32 32 32 71 16
Numlayers 32 32 32 32 32
MLPratio ∼8/3 ∼8/3 ∼8/3 4 4
Layernormtype non-parametric RMSNorm parametric parametric parametric
Positionalembeddings RoPE RoPE RoPE RoPE RoPE
Attentionvariant full GQA full MQA MQA
Biases none none inLNonly inLNonly none
Blocktype sequential sequential sequential parallel parallel
Activation SwiGLU SwiGLU SwiGLU GeLU SwiGLU
Sequencelength 2048 4096 2048 2048 2048
Batchsize(instances) 2160 1024 2048 2304 512
Batchsize(tokens) ∼4M ∼4M ∼4M ∼4M ∼1M
Weighttying no no no no yes
Table2: LMarchitecturecomparisonatthe7–8Bscale. Inthe“layernormtype”row,“parametric”
and“non-parametric”refertotheusuallayernormimplementationwithandwithoutadaptivegain
andbias,respectively.
UTF-8 GPT-NeoX
Documents
Source DocType bytes (millions) tokens
(GB) (billions)
CommonCrawl webpages 9,022 3,370 2,006
TheStack code 1,043 210 342
C4 webpages 790 364 174
Reddit socialmedia 339 377 80
peS2o STEMpapers 268 38.8 57
ProjectGutenberg books 20.4 0.056 5.2
Wikipedia,Wikibooks encyclopedic 16.2 6.2 3.7
Total 11,519 4,367 2,668
Table3: CompositionofDolma.
sourcesthatare(1)commonlyseeninlarge-scalelanguagemodelpretrainingand(2)accessibleto
thegeneralpublic(Soldainietal.,2024). Table3providesahigh-leveloverviewoftheamountof
datafromeachsource.
Dolma is built using a pipeline of (1) language filtering, (2) quality filtering, (3) content filtering,
(4)deduplication,(5)multi-sourcemixing,and(6)tokenization. WereferthereadertotheDolma
report(Soldainietal.,2024)formoredetailsaboutitsdesignprinciples,detailsaboutitsconstruc-
tion, and a more detailed summary of its contents. The report provides additional analyses and
experimentalresultsfromtraininglanguagemodelsonintermediatestatesofDolmatosharewhat
we learned about important data curation practices, including the role of content or quality filters,
deduplication, andmixingdatafrommultiplesources. Wekeepdocumentsfromeachsourcesep-
arate, both during curation as well as in the final release. We open-sourced our high-performance
data curation tools; this toolkit can be used to further experiment on Dolma, reproduce our work,
andenablefastandeasycurationofpretrainingcorpora.Finally,wealsoopen-sourcedourWIMBD
tool(Elazaretal.,2023)tohelpwithdatasetanalysis.
42.3 Evaluation
Weperformmodelevaluationattwostages: onlineevaluationtomakedecisionsformodeldesign
and offline evaluation to evaluate model checkpoints. For offline evaluation, we use the Catwalk
framework (Groeneveld et al., 2023), our publicly available evaluation tool with access to a wide
range of datasets and task formats. Using Catwalk, we perform downstream evaluation as well
as intrinsic language modeling evaluation on our new perplexity benchmark, Paloma (Magnusson
etal.,2023).
For both downstream and perplexity evaluation, we use our fixed evaluation pipeline to compare
resultsagainstseveralpubliclyavailablemodels.
In-LoopTrainingAblations Throughoutmodeltraining,weperformdownstreamevaluationsto
make decisions around model architecture, initialization, optimizers, learning rate schedule, and
data mixtures. We call this our online evaluation as it runs in-loop every 1000 training steps (or
∼4Btrainingtokens)andprovidesanearlyandcontinuoussignalonthequalityofthemodelbeing
trained. These evaluations rely on many of the core tasks and experiment settings used for our
offlineevaluationdetailedinSection4.1,whichalsomirrorsthetaskandevaluationstructureofthe
EleutherAIevalharness(Gaoetal.,2023).
DownstreamEvaluation Followingmuchpreviouswork(Brownetal.,2020;Blacketal.,2022;
Touvronetal.,2023a,b,interalia),wereportzero-shotperformanceonasetofdownstreamtasks.
Ourevaluationsuiteconsistsof9coretaskscorrespondingcloselytothecommonsensereasoning
tasksetreportedbyTouvronetal.(2023a)andTouvronetal.(2023b)(seeTable6foralistoftasks).
Giventhescaleofthemodelsbeingevaluated,suchtaskswereselectedatthebeginningofmodel
developmentduetotheirnaturalness(e.g.,allcanformulatedastextcompletionscoringtasks)and
abilitytoprovidemeaningfulsignalsthroughouttraining(seeFigure1).
Intrinsic Language Modeling Evaluation To measure how OLMo-7B fits distributions of lan-
guage beyond held-out training data, we use Paloma (Magnusson et al., 2023), a new perplex-
ity benchmark that includes 585 different domains of text. Domains range from nytimes.com to
r/depressiononRedditandaredrawnfrom18separatedatasources,suchasC4(Raffeletal.,2020),
instratifiedsamples.Thisallowsformoreequalinclusionoftextdomainsthatareunder-represented
intheirsourcecorpora.
WeaimnotjusttocompareOLMo-7Bagainstothermodelsforbestperformance,butalsotodemon-
strate how it enables fuller and more controlled scientific evaluations. OLMo-7B is the largest
LM with explicit decontamination for perplexity evaluation. Following the approach described
in Paloma, we remove any pretraining document with paragraphs leaked from Paloma evaluation
data. Without decontamination, other models risk underestimating perplexity (i.e., overestimating
the model’s out-of-sample fit). We also release intermediate checkpoints, allowing richer com-
parisons with two other models that release checkpoints, Pythia-6.9B (Biderman et al., 2023) and
RPJ-INCITE-7B(TogetherComputer,2023)(seeFigure2).
3 TrainingOLMo
This section describes our pretraining setup, including our distributed training framework (Sec-
tion3.1),optimizersettings(Section3.2),datapreparation(Section3.3),andhardware(Section3.4).
3.1 DistributedTrainingFramework
We train our models using the ZeRO optimizer strategy (Rajbhandari et al., 2019) via PyTorch’s
FSDP framework (Zhao et al., 2023), which reduces memory consumption by sharding the model
weightsandtheircorrespondingoptimizerstateacrossGPUs. Atthe7Bscale,thisenablestraining
withamicro-batchsizeof4096tokensperGPUonourhardware(seeSection3.4). ForOLMo-1B
and-7Bmodels,weuseaconstantglobalbatchsizeofapproximately4Mtokens(2048instances,
each with a sequence length of 2048 tokens). For OLMo-65B model (currently training), we use
a batch size warmup that starts at approximately 2M tokens (1024 instances), then doubles every
100Btokensuntilreachingapproximately16Mtokens(8192instances).
5Size PeakLR Betas Epsilon WeightDecay BatchSize(tokens)
1B 4.0E-4 (0.9,0.95) 1.0E-5 0.1 ∼4M
7B 3.0E-4 (0.9,0.95) 1.0E-5 0.1 ∼4M
65B* 1.5E-4 (0.9,0.95) 1.0E-5 0.1 ∼2M→∼4M→∼8M→∼16M
Table4: AdamWpretraininghyperparametersforOLMomodels.
*Atthetimeofwritingour65Bmodelisstilltraining.
To improve throughput, we employ mixed-precision training (Micikevicius et al., 2017) through
FSDP’s built-in settings and PyTorch’s amp module. The latter ensures that certain operations like
thesoftmaxalwaysruninfullprecisiontoimprovestability,whileallotheroperationsruninhalf-
precision with the bfloat16 format. Under our specific settings, the sharded model weights and
optimizer state local to each GPU are kept in full precision. The weights within each transformer
block are only cast to bfloat16 when the full-sized parameters are materialized on each GPU
duringtheforwardandbackwardpasses. GradientsarereducedacrossGPUsinfullprecision.
3.2 Optimizer
We use the AdamW optimizer (Loshchilov and Hutter, 2019) with the hyperparameters shown in
Table4. Forallmodelsizes,wewarmupthelearningrateover5000steps(∼21Btokens)andthen
decayitlinearlyfromtheredowntoatenthofthepeaklearningrateovertheremainderoftraining.
Afterthewarm-upperiod,weclipgradientssuchthatthetotall2-normoftheparametergradients3
doesnotexceed1.0. Table5givesacomparisonofouroptimizersettingsatthe7Bscaletothoseof
otherrecentLMsthatalsousedAdamW.
3.3 Data
Webuiltourtrainingdatasetoutofa2T-tokensamplefromouropendataset,Dolma(Soldainietal.,
2024),whichwedescribeinSection2.2.Thetokensfromeverydocumentareconcatenatedtogether
after appending a special EOS token to the end of each document, and then we group consecutive
chunks of 2048 tokens to form training instances. The training instances are shuffled in the exact
samewayforeachtrainingrun. Thedataorderandexactcompositionofeachtrainingbatchcanbe
reconstructedfromtheartifactswerelease.
Allofourreleasedmodelshavebeentrainedtoatleast2Ttokens(asingleepochoverourtraining
data), and some have been trained beyond that by starting a second epoch over the data with a
different shuffling order. The impact of repeating this small amount of data should be negligible
accordingtopriorwork(Muennighoffetal.,2023).
3.4 Hardware
In order to verify that our codebase could be used on both NVIDIA and AMD GPUs without any
lossinperformance,wetrainedmodelsontwodifferentclusters:
• LUMI: Provided by the LUMI supercomputer,4 we used up to 256 nodes on this clus-
ter, where each node consists of 4x AMD MI250X GPUs with 128GB of memory5 and
800Gbpsofinterconnect.
3Duringgradientclippingallofthemodel’sparametersaretreatedasasinglebigvector(asifallparameters
were flattened and concatenated together), and we take the ℓ -norm over the corresponding single gradient
2
vector.ThisisthestandardwaytoclipgradientsinPyTorch.
4https://www.lumi-supercomputer.eu
5TheMI250Xisadual-chipmodule,meaninginpracticethateachphysicaldeviceconsistsoftwological
devices,soeachnodehas8logicalGPUdeviceswith64GBofmemoryeach.
6OLMo-7B LLaMA2-7B OpenLM-7B Falcon-7B
warmupsteps 5000 2000 2000 1000
peakLR 3.0E-04 3.0E-04 3.0E-04 6.0E-04
minimumLR 3.0E-05 3.0E-05 3.0E-05 1.2E-05
weightdecay 0.1 0.1 0.1 0.1
beta1 0.9 0.9 0.9 0.99
beta2 0.95 0.95 0.95 0.999
epsilon 1.0E-05 1.0E-05 1.0E-05 1.0E-05
LRschedule linear cosine cosine cosine
gradientclipping global1.0 global1.0 global1.0 global1.0
gradientreducedtype FP32 FP32 FP32 BF16
optimizerstatedtype FP32 mostlikelyFP32 FP32 FP32
Table5: Comparisonofpretrainingoptimizersettingsatthe7Bscale. Eachmodelinthistableused
AdamWasitsoptimizer.
• MosaicML:ProvidedbyMosaicML6(Databricks),weused27nodesonthiscluster,where
eachnodeconsistsof8xNVIDIAA100GPUswith40GBofmemoryand800Gbpsinter-
connect.
Despite minor differences in batch size to optimize for training throughput, both runs resulted in
nearlyidenticalperformanceonourevaluationsuiteby2Ttokens.
4 Results
ThecheckpointusedforevaluatingOLMo-7Bistraineduntil2.46TtokensontheDolma(Soldaini
et al., 2024) dataset with a linear learning rate decay schedule mentioned in Section 3.2. In our
experiments, we find that tuning this checkpoint further on Dolma dataset for 1000 steps with the
learningratelinearlydecayedto0boostsmodelperformanceonperplexityandend-taskevaluation
suitesdescribedinSection2.3. WecompareOLMowithotherpubliclyavailablemodelsincluding
LLaMA-7B (Touvron et al., 2023a), LLaMA2-7B (Touvron et al., 2023b), MPT-7B (MosaicML
NLP Team, 2023), Pythia-6.9B (Biderman et al., 2023), Falcon-7B (Almazrouei et al., 2023) and
RPJ-INCITE-7B(TogetherComputer,2023).
4.1 Downstreamevaluation
Setup Ourcoredownstreamevaluationsuite(seeTable6)consistsof: arc(botharc easyand
arc challenge) (Clark et al., 2018), boolq (Clark et al., 2019), openbookqa (Mihaylov et al.,
2018),sciq(Welbletal.,2017),hellaswag(Zellersetal.,2019),piqa(Bisketal.,2020),copa
(Roemmeleetal.,2011)andwinogrande(Sakaguchietal.,2021). InAppendixA,wealsoreport
resultsonanadditionalsetofauxiliarytasksoutsideofourcoreevaluationsetthatwefoundtohave
lessstableperformancetrends(seeFigure4). Wenotethatourdownstreamevaluationsuiteisstill
underdevelopmentandthatadditionalresultsandanalysiswillbereportedinafutureversion.
Inallcases,weperformzero-shotevaluationusingtherankclassificationapproachpopularizedby
Brownetal.(2020).Underthisapproach,candidatetextcompletions(e.g.,differentmultiple-choice
options)arerankedbylikelihood(usuallynormalizedbysomenormalizationfactor),andprediction
accuracyisreported. WhileCatwalkimplementsseveralcommonlikelihoodnormalizationstrate-
gies, including normalizing by number of tokens (per-token normalization) (Brown et al., 2020;
Liangetal.,2022),bynumberofcharacters(per-characternormalization)(Gaoetal.,2023),aswell
as incorporating an answer’s unconditional likelihood (Brown et al., 2020), we selected the nor-
malizationstrategiesforeachdatasetseparately. Specifically,weusedunconditionalnormalization
forarcandopenbookqa,per-tokennormalizationforhellaswag,piqa,andwinograndeandno
normalizationforboolq,copa,andsciq(i.e.,tasksformulatedassingletokenpredictiontasks).
6https://www.mosaicml.com
7arc arc hella- open wino-
7BModels boolq copa piqa sciq avg.
challenge easy swag bookqa grande
Falcon 47.5 70.4 74.6 86.0 75.9 53.0 78.5 93.9 68.9 72.1
LLaMA 44.5 57.0 73.1 85.0 74.5 49.8 76.3 89.5 68.2 68.7
LLaMA2 39.8 57.7 73.5 87.0 74.5 48.4 76.4 90.8 67.3 68.4
MPT 46.5 70.5 74.2 85.0 77.6 48.6 77.3 93.7 69.9 71.5
Pythia 44.2 61.9 61.1 84.0 63.8 45.0 75.1 91.1 62.0 65.4
RPJ-INCITE 42.8 68.4 68.6 88.0 70.3 49.4 76.0 92.9 64.7 69.0
OLMo-7B 48.5 65.4 73.4 90.0 76.4 50.4 78.4 93.8 67.9 71.6
Table6:Zero-shotevaluationofOLMo-7Band6otherpubliclyavailablecomparablemodelcheck-
pointson9coretasksfromthedownstreamevaluationsuitedescribedinSection2.3.ForOLMo-7B,
wereportresultsforthe2.46Ttokencheckpoint.
arc_c arc_e boolq
500 1000 1500 2000 2500 500 1000 1500 2000 2500 500 1000 1500 2000 2500
copa hellaswag obqa
500 1000 1500 2000 2500 500 1000 1500 2000 2500 500 1000 1500 2000 2500
piqa sciq winogrande
500 1000 1500 2000 2500 500 1000 1500 2000 2500 500 1000 1500 2000 2500
Tokens Seen (billions)
Figure 1: Accuracy score progression of OLMo-7B on 9 core end-tasks score from Catwalk eval-
uationsuitedescribedinSection2.3. WecanseethebenefitofdecayingLRto0inthefinal1000
stepsoftrainingon7/9end-tasks.
Results Table 6 summarizes the result of zero-shot evaluation of OLMo-7B and compares it
against 6 other publicly available models of comparable size. We report results on 9 core tasks
fromourevaluationsuitedescribedinSection2.3. OurOLMo-7Bcheckpointoutperformsallother
publiclyavailablemodelson2end-tasksandremainsintop-3on8/9end-tasksfromtheevaluation
suite. Onaggregate,OLMo-7Biscompetitiveagainstall6publiclyavailablemodelcheckpointsin
ourcomparisontable.
In Figure 1 we plot the accuracy score progression of 9 core end-tasks. All tasks, except OBQA,
showanupwardtrendinaccuracynumbersasOLMo-7Bistrainedonmoretokens.Asharpupward
tickinaccuracyofmanytasksbetweenthelastandthesecondtolaststepshowsusthebenefitof
linearly reducing the LR to 0 over the final 1000 training steps. See Table 8 in Appendix A for
additionalevaluationresultsanddiscussion.
4.2 Intrinsiclanguagemodelingevaluation
Setup Forintrinsicevaluations, Palomaproposesarangeofanalyses, frominspectionofperfor-
mance in each domain separately to more summarized results over combinations of domains. We
8
ycaruccA
84
44
04
09
78
48
87
67
86
46
06
67
27
86
49
29
09
27
46
65
15
84
54
66
36Sources Combined C4 mC4 WikiText-103
10 100 1000 10000 10 100 1000 10000 10 100 1000 10000 10 100 1000 10000
PTB RedPajama Falcon RefinedWeb Dolma V1.5
10 100 1000 10000 10 100 1000 10000 10 100 1000 10000 10 100 1000 10000
M2D2 S2ORC M2D2 Wikipedia C4 100 Domains 100 Subreddits
10 100 1000 10000 10 100 1000 10000 10 100 1000 10000 10 100 1000 10000
Tokens Seen (billions)
Baselines
Falcon-7B LLaMA2-7B MPT-7B LLaMA-7B Pythia-6.9B RPJ-INCITE-7B OLMo-7B
Figure 2: Bits per byte on 11 evaluation data sources from Paloma and their combination (Mag-
nussonetal.,2023),decontaminatedfromOLMo’spretrainingdata. Whilemodelsfollowageneral
datascalingtrend,sampleefficiencyismostfavorableonin-distributiondata. Forexample,OLMo-
7BovertakesallothermodelsonC4,perhapsfromhaving88.8%CommonCrawlpretrainingdata.
report results at two levels of granularity: the aggregate performance over 11 of the 18 sources
in Paloma as in Magnusson et al. (2023), as well as more fine-grained results over each of these
sourcesindividually. Thisparticularsubsetof11sourcesfromPalomaexcludessourcesthatarenot
publiclyavailable,involvefringeortoxictext,orconsistofcodedatanotsupportedbyPaloma’sde-
contaminationapproach.ThisleavesC4(Raffeletal.,2020),mC4-en(Chungetal.,2023),Wikitext
103(Merityetal.,2016),PennTreebank(Marcusetal.,1999;Nunes,2020),RedPajama(Together
Computer,2023), Falcon-RefinedWeb(Penedoetal.,2023), Dolma(Soldainietal.,2024), M2D2
S2ORC(Reidetal.,2022), M2D2Wikipedia(Reidetal.,2022), C4100domains(Chronopoulou
etal.,2022),andDolma100Subreddits(Soldainietal.,2024). Toallowforafaircomparisonbe-
tween models with different vocabularies, we report bits per byte as defined by Gao et al. (2020)
overthetestsetsofthesesources.
Results In the Sources Combined subplot of Figure 2, we show the performance of OLMo-7B
against6comparably-sizedlanguagemodelsonthecombinationof11 datasourcesfromPaloma.
Overall we find OLMo to have a competitive fit, especially given its training data was explicitly
decontaminated against Paloma. As seen through the comparison of final models (see shapes) as
wellintermediatecheckpoints(seedashedlines),theOLMoresultsfollowsimilarscalingtrendsof
other models. Note that the performance of intermediate checkpoints is influenced by where that
checkpointoccursinthelearningrateschedule. Somodelstrainedforfewerstepswilltendtohave
steeper training curves without necessarily being more sample efficient if training duration were
fixedacrossallmodels. MPT-7B,nevertheless,standsoutasimprovingaheadoftheothermodels
inthissubplot.Thiscouldbeduetoanumberoffactors,includingpretrainingdatacompositionand
itsmatchtothedomainsinPaloma(e.g.,MPTtrainson27%non-CommonCrawldataratherthan
18%forLLaMA,12.2%forRedPajama,and11.2%forOLMo)aswellasvariousdatapreprocessing
decisions(e.g.,MPT’suseofsemanticdeduplicationbyAbbasetal.,2023,onC4).
9
etyB
reP
stiB
00.1
28.0
76.0
53.1
11.1
09.0
47.0
00.1
28.0
76.0
00.1
28.0
76.0
22.1
28.0
55.0
11.1
09.0
47.0
16.0
11.1
09.0
47.0
16.0
00.1
28.0
76.0
00.1
28.0
76.0
22.1
28.0
55.0
00.1
28.0
76.0
22.1
00.1
28.0The remaining subplots in Figure 2 provide more fine-grained analysis by reporting bits per byte
separatelyforeachofthe11datasourcesthatarecombinedintheaggregatedPalomametric. From
this we see greater variation in sample efficiency, largely driven by the similarity of training and
evaluation distributions. Notably, OLMo-7B fares well on evaluations predominated by Common
Crawl,suchasC4,thoughdifferentwaysofpostprocessingCommonCrawlarebestfitbymodels
trainedwiththatspecificdata,suchasFalcon-7BonFalconRefinedWeb. Meanwhile,OLMo-7Bis
lesssampleefficientcomparedtoothermodelsonsourceslessrelatedtoscrapedwebtext,suchas
WikiText-103, M2D2 S2ORC, and M2D2 Wikipedia. The RedPajama evaluation shows a similar
pattern,perhapsasonly2ofits7domainsarefromCommonCrawl,andPalomaweightsdomains
within each source equally. Since heterogeneous data from curated sources like Wikipedia and
ArXivpapersismuchlessabundantthanscrapedwebtext,maintainingsampleefficiencyforfitto
thesedistributionsoflanguagewillbechallengingaspretrainingcorporaarescaled.
4.3 PowerConsumptionandCarbonFootprint
Following previous literature (Strubell et al., 2019; Patterson et al., 2021; Wu et al., 2022; Dodge
et al., 2022), we estimate the total energy consumed and carbon released while pretraining our
models by calculating the total power consumption required for training, and then multiplying it
by the carbon emission intensity of the power grid where the model was trained. While reporting
theseoperationalemissionsisstandardpractice,itdoesnotaccountforothersourcesofemissions
suchastheembodiedemissionsduetothemanufacturing,transportationanddisposalofhardware
and datacenter infrastructure, lifetime operational emissions due to use, rebound effects, or other
environmentalimpactssuchaswaterconsumptionormining. Thusourestimatesshouldbeviewed
aslowerbounds.
Wecalculatethetotalpowerconsumptionforourmodelsbymeasuringthepowerconsumptionofa
singlenodeevery25ms,calculatinganaverageacrosstheentiretrainingrun,andmultiplyingbythe
totalnumberofnodes. Wethenaccountfortheenergyefficiencyofthedatacenterbymultiplying
the previous total by a power usage effectiveness (PUE) factor, which we set to 1.1, representing
a conservative 10% energy consumption overhead typical of energy efficient datacenters.78 We
estimatethatpretrainingour7Bmodelsconsumed239MWhofenergy.
Tocalculatecarbonemissions,wemultiplythetotalpowerconsumptionbyacarbonintensityfac-
tor,measuredinkgCO emittedperKWh,basedonthephysicallocationofthedatacenterwhere
2
eachmodelwastrained. ThemodeltrainedonA100-40GBGPUswastrainedinAustralia, sowe
assume a carbon intensity factor of 0.610, the national average for Australia in 2022.9 The model
trainedonMI250XGPUswastrainedintheLUMIsupercomputer,whichrunson100%renewable,
carbon-neutral energy, so we assume a carbon intensity factor of 0. LUMI is powered entirely by
hydroelectricpowerandsomesources(Ubiernaetal.,2022)measurethecarbonintensityfactorof
hydroelectricpowertobe0.024,whichwouldimplytotalcarbonemissionsof3.54tCO eq.10 How-
2
ever,werelyontheofficialLUMIdataforourcalculations,andthusweestimatetotalpretraining
emissions of 69.78 tCO eq.11 In Table 7 we compare our models with other previously released
2
modelsbasedonpubliclyavailableinformation.
Wehopethatopenlyreleasingourmodelscanreducefutureemissionsbyallowingotherstoavoid
theneedtopretrainmodelsfromscratch,andgiveinsightsintothetruecostofdevelopingstateof
theartmodels. Wealsohighlightthatourestimatesarelowerbounds,becausetheydonotinclude
othercriticalpiecesofdevelopmentsuchasdebugging,hyperparametertuning,anddowntime.
7https://www.nrel.gov/computational-science/measuring-efficiency-pue.html
8https://www.google.com/about/datacenters/efficiency/
9https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/
december-quarter-2022/Emissions-Reduction.aspx
10https://www.lumi-supercomputer.eu
11ThesemetricswereinpartcollectedusingCarbonara’sAIagentandmonitoringplatform. Learnmoreat:
https://trycarbonara.com
10GPUPower Power Carbon Carbon
GPUType Consumption Usage Intensity Emissions
(MWh) Effectiveness (kgCO e/KWh) (tCO eq)
2 2
Gopher-280B TPUv3 1,066 1.08 0.330 380
BLOOM-176B A100-80GB 433 1.2 0.057 30
OPT-175B A100-80GB 324 1.1 0.231 82
T5-11B TPUv3 77 1.12 0.545 47
LLaMA-7B A100-80GB 33 1.1 0.385 14
LLaMA2-7B A100-80GB 74 1.1 0.385 31
OLMo-7B MI250X 135 1.1 0.000* 0*
OLMo-7B A100-40GB 104 1.1 0.610 70
Table 7: CO emissions during pretraining. We estimate the total carbon emissions for various
2
models using publicly available data on PUE, carbon intensity of local power grid, and reported
powerconsumption. NumbersforGopher-280B(Raeetal.,2022),BLOOM-176B(Luccionietal.,
2022), OPT-175B (Zhang et al., 2022), T5-11B (Patterson et al., 2021), LLaMA (Touvron et al.,
2023a), and LLaMA2 (Touvron et al., 2023b) are taken from their respective papers. See Section
4.3fordetailsonhowtCO2eqwascalculated.
* LUMI runs entirely on hydroelectric power11and some estimates (Ubierna et al., 2022) measure
theintensityfactorofhydroelectricpowertobe0.024,implyingtotalemissionsof3.54tCO eq.
2
5 ArtifactsReleased
Bysharingartifactsfromallpipelinestages,weaimtoencourageopenresearchandreducedupli-
cated,oftencostlyefforts,byacademicsandpractitioners. Wereleasethefollowing:
1. Thetrainingandmodelingcode.12.
2. The trained model weights for the 7B model13, 7B-twin-2T14, and the 1B model15. For
all the models, we release not only the final model weights but also 500+ intermediate
checkpointsatintervalsof1000steps.
3. ThetrainingdataDolma(Soldainietal.,2024).16
4. Dolma’stoolkittoconstructnewdatasets17,andWIMBD(Elazaretal.,2023)fordataset
analysis18.
5. Theevaluationcode19usingCatwalk20fordownstreamevaluation(Groeneveldetal.,2023)
andPaloma21forperplexity-basedevaluation(Magnussonetal.,2023).
Weintendtofollowuponthisreleasewithanotheronesoonthatincludesthefollowing:
1. Traininglogs,ablations,andfindings.
2. Weights&Biaseslogsforourtrainingruns.
3. Adapted OLMo with instruction-tuning and RLHF, including its training and evaluation
codeanddatausingourOpenInstruct22library(Wangetal.,2023;Ivisonetal.,2023).
12https://github.com/allenai/OLMo
13https://huggingface.co/allenai/OLMo-7B
14https://huggingface.co/allenai/OLMo-7B-Twin-2T
15https://huggingface.co/allenai/OLMo-1B
16https://huggingface.co/datasets/allenai/dolma
17https://github.com/allenai/dolma
18https://github.com/allenai/wimbd
19https://github.com/allenai/OLMo-Eval
20https://github.com/allenai/catwalk
21https://paloma.allen.ai
22https://github.com/allenai/open-instruct
116 License
Ourgoalistofacilitatescientificdevelopmentandempowerthescientificcommunity,sowefavor
permissivelicensesthatgiveusersflexibilityinusingourresourcesandartifacts. Assuch,allcode
andweightsarereleasedundertheApache2.0License.23 Somelicensesusedbyotherorganizations
forrecentmodelreleasesprohibitusingtheoutputsfromtheirmodelstotrainartificialintelligenceor
machinelearningsystems,whileweexpresslyallowuserstodoso.Wealsodonotlimitcommercial
use. Wehopethatourmodelscanmakeothermodelsbetter. Werecognizethattheriskformisuse
of our models is relatively low, as language models that have not been adapted as chatbots have
primarily been used as scientific artifacts not as products with broad public adoption (our models
have not been adapted as chatbots). In addition, over the past year there have been a number of
comparable models released with very permissive licenses, so using a more strict license for our
workwillnotremovetheoverallriskinthefield. Webelievethistradeoffonthesideofbeingmore
openisthebestoption.
7 ConclusionandFutureWork
This technical report presents our first release of OLMo, a state-of-the-art, truly open language
model and its framework to build and study the science of language modeling. Unlike most prior
effortsthathaveonlyreleasedmodelweightsandinferencecode,wereleaseOLMoandthewhole
framework, including training data and training and evaluation code. Soon, we will also release
traininglogs, ablations, findingsandWeights&Biaseslogs. Wearealsoexploringtheadaptation
ofOLMowithinstructiontuninganddifferentflavorsofRLHF.Wearegoingtoreleasetheadapted
modelsaswellasallofourmodeladaptationcodeanddata.
WeintendtocontinuouslysupportandextendOLMoanditsframework, andcontinuetopushthe
boundariesofopenLMstoempowertheopenresearchcommunity. Tothatend,welookforwardto
bringingdifferentmodelsizes,modalities,datasets,safetymeasures,andevaluationsintotheOLMo
family. Wehopethisandfuturereleaseswillempowerandstrengthentheopenresearchcommunity
andinspireanewwaveofinnovation.
AuthorContributions
OLMowouldnothavebeenpossiblewithoutthehelpofourmanyteammatesandcollaborators.We
listauthorcontributions(inalphabeticalorder)below:
Contributorstopretrainingdatasetconstructionandtooling(Dolma)includeRussellAuthur,Iz
Beltagy, Akshita Bhagia, Khyathi Chandu, Jesse Dodge, Yanai Elazar, Dirk Groeneveld, Rodney
Kinney, Kyle Lo, Aakanksha Naik, Abhilasha Ravichander, Dustin Schwenk, Luca Soldaini, and
NishantSubramani.
ContributorstomodeltrainingandarchitectureincludeShaneArora,IzBeltagy,AkshitaBhagia,
Matthew E. Peters, Dirk Groeneveld, Ananya Harsh Jha, William Merrill, Jacob Morrison, Niklas
Muennighoff,DustinSchwenk,SaurabhShah,PeteWalsh,andMitchellWortsman.
Contributors to evaluation suite and tooling include Akshita Bhagia, Arman Cohan, Pradeep
Dasigi,JesseDodge,DirkGroeneveld,YulingGu,TusharKhot,KyleRichardson,OyvindTajford,
andPeteWalsh.
ContributorstomodeladaptationincludeIzBeltagy,PradeepDasigi,JackHessel,HamishIvison,
NathanLambert,ValentinaPyatkin,PeteWalsh,andYizhongWang.
Contributors to license creation and risk assessment include David Atkinson, Jesse Dodge, Jen-
niferDumas,CrystalNam,andWillSmith.
TheOLMoprojectwasledbyHannanehHajishirziandNoahA.Smith.
23http://www.apache.org/licenses/LICENSE-2.0
12Acknowledgements
OLMowouldnothavebeenpossiblewithoutthesupportofmanyindividualsandinstitutions. The
experimental components of this work were made possible through a partnership with AMD and
CSC,enablinguseoftheLUMIsupercomputer, andKempnerInstituteatHarvardUniversity. We
thankJonathanFrankleandtheteamatMosaicML(nowDatabricks)forsharingtheirexperiences
withFSDP,andbuildingthecodebasethatOLMoisbasedon. WethankourteammatesTairaAn-
derson,MichelleBenedict,JonBorchardt,EvieCheng,ArnaviChheda,JohannDahm,MattLatzke,
KelseyMacMillan, AaronSarnat, CarissaSchoenick, SamSkjonsberg, MichaelSchmitz, Michael
Wilson,CaitlinWittlif,andtheentireITteam,fortheirhelpwiththewebsite,design,internaland
external communications, budgeting, and other activities that supported smooth progress on this
project. Finally,wealsoexpressgratitudeforthehelpfuldiscussionsandfeedbackfromourteam-
matesatAI2andclosecollaborators,includingPrithviraj(Raj)Ammanabrolu,PeterClark,Nicole
DeCario,DougDowney,AliFarhadi,IanFerreira,Va¨ino¨ Hatanpa¨a¨,ShamM.Kakade,JulienLau-
nay, SydneyLevine, PekkaManninen, FranziRoessner, MaartenSap, LudwigSchmidt, andYulia
Tsvetkov.
References
AmroAbbas,KushalTirumala,Da´nielSimig,SuryaGanguli,andAriSMorcos. Semdedup: Data-
efficientlearningatweb-scalethroughsemanticdeduplication. arXivpreprintarXiv:2303.09540,
2023. URLhttps://arxiv.org/abs/2303.09540.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra-
Aime´e Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badred-
dineNoune,BaptistePannier,andGuilhermePenedo. Thefalconseriesofopenlanguagemod-
els. ArXiv, abs/2311.16867, 2023. URL https://api.semanticscholar.org/CorpusID:
265466629.
JimmyBa,JamieRyanKiros,andGeoffreyE.Hinton.Layernormalization.ArXiv,abs/1607.06450,
2016. URLhttps://api.semanticscholar.org/CorpusID:8236317.
Yoshua Bengio, Re´jean Ducharme, Pascal Vincent, and Christian Janvin. A neural proba-
bilistic language model. J. Mach. Learn. Res., 3:1137–1155, 2003. URL https://api.
semanticscholar.org/CorpusID:221275765.
Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,
EricHallahan, Mohammad AflahKhan, ShivanshuPurohit, UsvsnSaiPrashanth, Edward Raff,
AviyaSkowron,LintangSutawika,andOskarVanDerWal. Pythia: Asuiteforanalyzinglarge
language models across training and scaling. In Andreas Krause, Emma Brunskill, Kyunghyun
Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th
InternationalConferenceonMachineLearning, volume202ofProceedingsofMachineLearn-
ing Research, pages 2397–2430. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.
press/v202/biderman23a.html.
BigScience, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic´, Daniel
Hesslow, Roman Castagne´, Alexandra Sasha Luccioni, Franc¸ois Yvon, et al. Bloom: A 176b-
parameteropen-accessmultilinguallanguagemodel. arXivpreprintarXiv:2211.05100,2022.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical
commonsense in natural language. In Proceedings of the AAAI conference on artificial intelli-
gence,volume34,pages7432–7439,2020. URLhttps://ojs.aaai.org/index.php/AAAI/
article/view/6239.
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-
race He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-
NeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL Work-
shop on Challenges & Perspectives in Creating Large Language Models, 2022. URL https:
//arxiv.org/abs/2204.06745.
13Su Lin Blodgett, Lisa Green, and Brendan O’Connor. Demographic dialectal variation in social
media: A case study of African-American English. In Proceedings of the 2016 Conference on
EmpiricalMethodsinNaturalLanguageProcessing,pages1119–1130,Austin,Texas,November
2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1120. URL https:
//aclanthology.org/D16-1120.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
ArielHerbert-Voss,GretchenKrueger,T.J.Henighan,RewonChild,AdityaRamesh,DanielM.
Ziegler, JeffWu, ClemensWinter, ChristopherHesse, MarkChen, EricSigler, MateuszLitwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv,
abs/2005.14165,2020. URLhttps://api.semanticscholar.org/CorpusID:218971783.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,ParkerSchuh,
KensenShi,SashaTsvyashchenko,JoshuaMaynez,AbhishekRao,ParkerBarnes,YiTay,Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, JacobAustin, MichaelIsard, GuyGur-Ari, PengchengYin, TojuDuke, AnselmLev-
skaya,SanjayGhemawat,SunipaDev,HenrykMichalewski,XavierGarcia,VedantMisra,Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira,RewonChild,OleksandrPolozov,KatherineLee,ZongweiZhou,XuezhiWang,Bren-
nanSaeta,MarkDiaz,OrhanFirat,MicheleCatasta,JasonWei,KathyMeier-Hellstern,Douglas
Eck,JeffDean,SlavPetrov,andNoahFiedel. Palm: Scalinglanguagemodelingwithpathways,
2022. URLhttps://arxiv.org/abs/2204.02311.
Alexandra Chronopoulou, Matthew Peters, and Jesse Dodge. Efficient hierarchical domain adap-
tation for pretrained language models. In Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, pages 1336–1351, Seattle, United States, July 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.naacl-main.96. URLhttps://aclanthology.org/2022.
naacl-main.96.
Hyung Won Chung, Noah Constant, Xavier Garc´ıa, Adam Roberts, Yi Tay, Sharan Narang, and
Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilin-
gualpretraining. ArXiv,abs/2304.09151,2023. URLhttps://api.semanticscholar.org/
CorpusID:258187051.
ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristina
Toutanova. Boolq: Exploringthesurprisingdifficultyofnaturalyes/noquestions. arXivpreprint
arXiv:1905.10044,2019.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,and
OyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge.
arXivpreprintarXiv:1803.05457,2018. URLhttps://arxiv.org/abs/1803.05457.
Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma
Strubell,AlexandraSashaLuccioni,NoahA.Smith,NicoleDeCario,andWillBuchanan. Mea-
suringthecarbonintensityofaiincloudinstances,2022.URLhttps://dl.acm.org/doi/10.
1145/3531146.3533234.
William B. Dolan and Chris Brockett. Automatically constructing a corpus of sen-
tential paraphrases. In International Joint Conference on Natural Language Pro-
cessing, 2005. URL https://www.microsoft.com/en-us/research/publication/
automatically-constructing-a-corpus-of-sentential-paraphrases/.
YanaiElazar,AkshitaBhagia,IanH.Magnusson,AbhilashaRavichander,DustinSchwenk,Alane
Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A.
Smith, and Jesse Dodge. What’s in my big data? ArXiv, abs/2310.20707, 2023. URL https:
//api.semanticscholar.org/CorpusID:264803575.
14Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of di-
verse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. URL https:
//arxiv.org/abs/2101.00027.
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFos-
ter,LaurenceGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,NiklasMuen-
nighoff,ChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,Lintang
Sutawika,EricTang,AnishThite,BenWang,KevinWang,andAndyZou. Aframeworkforfew-
shotlanguagemodelevaluation,122023. URLhttps://zenodo.org/records/10256836.
Sidney Greenbaum and Gerald Nelson. The international corpus of english (ICE) project. World
Englishes, 15(1):3–15, mar 1996. doi: 10.1111/j.1467-971x.1996.tb00088.x. URL https://
doi.org/10.1111%2Fj.1467-971x.1996.tb00088.x.
DirkGroeneveld, AnasAwadalla, IzBeltagy, AkshitaBhagia, IanMagnusson, HaoPeng, Oyvind
Tafjord, Pete Walsh, Kyle Richardson, and Jesse Dodge. Catwalk: A unified language model
evaluationframeworkformanydatasets. arXivpreprintarXiv:2312.10253,2023. URLhttps:
//arxiv.org/abs/2312.10253.
SuchinGururangan,MitchellWortsman,SamirYitzhakGadre,AchalDave,MaciejKilian,Weijia
Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Di-
makis,AliFarhadi,VaishaalShankar,andLudwigSchmidt. OpenLM:aminimalbutperforma-
tive language modeling (lm) repository, 2023. URL https://github.com/mlfoundations/
open_lm/. GitHubrepository.
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep
Dasigi,JoelJang,DavidWadden,NoahA.Smith,IzBeltagy,andHannanehHajishirzi. Camels
inachangingclimate: Enhancinglmadaptationwithtulu2,2023. URLhttps://arxiv.org/
abs/2311.10702.
AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBam-
ford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.
Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. URL https://arxiv.org/abs/
2401.04088.
PercyLiang,RishiBommasani,TonyLee,DimitrisTsipras,DilaraSoylu,MichihiroYasunaga,Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language
models.arXivpreprintarXiv:2211.09110,2022.URLhttps://arxiv.org/abs/2211.09110.
JianLiu,LeyangCui,HanmengLiu,DandanHuang,YileWang,andYueZhang. Logiqa: Achal-
lengedatasetformachinereadingcomprehensionwithlogicalreasoning.CoRR,abs/2007.08124,
2020. URLhttps://arxiv.org/abs/2007.08124.
ZhengzhongLiu,AurickQiao,WillieNeiswanger,HongyiWang,BowenTan,TianhuaTao,Junbo
Li,YuqiWang,SuqiSun,OmkarPangarkar,etal.Llm360:Towardsfullytransparentopen-source
llms. arXivpreprintarXiv:2312.06550,2023. URLhttps://arxiv.org/abs/2312.06550.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
ConferenceonLearningRepresentations,2019. URLhttps://openreview.net/forum?id=
Bkg6RiCqY7.
AlexandraSashaLuccioni, SylvainViguier,andAnne-LaureLigozat. Estimatingthecarbonfoot-
printofbloom,a176bparameterlanguagemodel,2022.URLhttps://arxiv.org/abs/2211.
02001.
Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind
Tafjord,DustinSchwenk,EvanPeteWalsh,YanaiElazar,KyleLo,etal. Paloma: Abenchmark
forevaluatinglanguagemodelfit. arXivpreprintarXiv:2312.10523,2023.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. Treebank-3,
1999. URLhttps://catalog.ldc.upenn.edu/LDC99T42.
15Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mix-
ture models. ArXiv, abs/1609.07843, 2016. URL https://api.semanticscholar.org/
CorpusID:16299141.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Frederick Diamos, Erich Elsen,
David Garc´ıa, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and
Hao Wu. Mixed precision training. ArXiv, abs/1710.03740, 2017. URL https://api.
semanticscholar.org/CorpusID:3297437.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? anewdatasetforopenbookquestionanswering. arXivpreprintarXiv:1809.02789,
2018. URLhttps://arxiv.org/abs/1809.02789.
TomasMikolov,IlyaSutskever,KaiChen,GregoryS.Corrado,andJeffreyDean. Distributedrep-
resentationsofwordsandphrasesandtheircompositionality. InNeuralInformationProcessing
Systems,2013. URLhttps://api.semanticscholar.org/CorpusID:16447573.
MosaicMLNLPTeam. Introducingmpt-7b: Anewstandardforopen-source,commerciallyusable
llms,2023. URLwww.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.
Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Noua-
maneTazi,SampoPyysalo,ThomasWolf,andColinRaffel. Scalingdata-constrainedlanguage
models. arXivpreprintarXiv:2305.16264,2023.
Davide Nunes. Preprocessed penn tree bank, 2020. URL https://zenodo.org/record/
3910021.
OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api.
semanticscholar.org/CorpusID:257532815.
Antonis Papasavva, Savvas Zannettou, Emiliano De Cristofaro, Gianluca Stringhini, and Jeremy
Blackburn. Raiders of the lost kek: 3.5 years of augmented 4chan posts from the politically
incorrect board. Proceedings of the International AAAI Conference on Web and Social Media,
14:885–894, may 2020. doi: 10.1609/icwsm.v14i1.7354. URL https://doi.org/10.1609%
2Ficwsm.v14i1.7354.
DavidPatterson,JosephGonzalez,QuocLe,ChenLiang,Lluis-MiquelMunguia,DanielRothchild,
DavidSo,MaudTexier,andJeffDean.Carbonemissionsandlargeneuralnetworktraining,2021.
URLhttps://arxiv.org/abs/2104.10350.
GuilhermePenedo,QuentinMalartic,DanielHesslow,Ruxandra-Aime´eCojocaru,AlessandroCap-
pelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The re-
finedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data
only. ArXiv,abs/2306.01116,2023. URLhttps://api.semanticscholar.org/CorpusID:
259063761.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,
andLukeZettlemoyer. Deepcontextualizedwordrepresentations. ArXiv,abs/1802.05365,2018.
URLhttps://api.semanticscholar.org/CorpusID:3626819.
MohammadTaherPilehvarandJose´Camacho-Collados. Wic: 10,000examplepairsforevaluating
context-sensitive representations. CoRR, abs/1808.09121, 2018. URL http://arxiv.org/
abs/1808.09121.
JackW.Rae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,
JacobMenick,AlbinCassirer,RichardPowell,GeorgevandenDriessche,LisaAnneHendricks,
Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron
Huang,JonathanUesato,JohnMellor,IrinaHiggins,AntoniaCreswell,NatMcAleese,AmyWu,
Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen
Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kun-
coro,AidaNematzadeh,ElenaGribovskaya,DomenicDonato,AngelikiLazaridou,ArthurMen-
sch,Jean-BaptisteLespiau,MariaTsimpoukelli,NikolaiGrigorev,DougFritz,ThibaultSottiaux,
16MantasPajarskas,TobyPohlen,ZhitaoGong,DanielToyama,CypriendeMassond’Autume,Yu-
jiaLi,TayfunTerzi,VladimirMikulik,IgorBabuschkin,AidanClark,DiegodeLasCasas,Au-
relia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,
Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
Vinyals,KareemAyoub,JeffStanway,LorrayneBennett,DemisHassabis,KorayKavukcuoglu,
and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training go-
pher,2022. URLhttps://arxiv.org/abs/2112.11446.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. J.Mach.Learn.Res.,21(1),jan2020. ISSN1532-4435.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza-
tions toward training trillion parameter models. SC20: International Conference for High Per-
formance Computing, Networking, Storage and Analysis, pages 1–16, 2019. URL https:
//api.semanticscholar.org/CorpusID:203736482.
MachelReid,VictorZhong,SuchinGururangan,andLukeZettlemoyer. M2D2:Amassivelymulti-
domainlanguagemodelingdataset.InProceedingsofthe2022ConferenceonEmpiricalMethods
inNaturalLanguageProcessing,pages964–975,AbuDhabi,UnitedArabEmirates,December
2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.
emnlp-main.63.
Manoel Horta Ribeiro, Jeremy Blackburn, Barry Bradlyn, Emiliano De Cristofaro, Gianluca
Stringhini, Summer Long, Stephanie Greenberg, and Savvas Zannettou. The evolution of
the manosphere across the web. Proceedings of the International AAAI Conference on Web
and Social Media, 15:196–207, may 2021. doi: 10.1609/icwsm.v15i1.18053. URL https:
//doi.org/10.1609%2Ficwsm.v15i1.18053.
Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plau-
sible alternatives: An evaluation of commonsense causal reasoning. In 2011
AAAI Spring Symposium Series, 2011. URL https://aaai.org/papers/
02418-2418-choice-of-plausible-alternatives-an-evaluation-of-commonsense-causal-reasoning/.
Ronald Rosenfeld. Two decades of statistical language modeling: Where do we go from here?
ProceedingsoftheIEEE,88(8):1270–1278,2000.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An ad-
versarialwinogradschemachallengeatscale. CommunicationsoftheACM,64(9):99–106,2021.
URLhttps://dl.acm.org/doi/abs/10.1145/3474381.
NoamM.Shazeer. Gluvariantsimprovetransformer. ArXiv,abs/2002.05202,2020. URLhttps:
//api.semanticscholar.org/CorpusID:211096588.
LucaSoldaini,RodneyKinney,AkshitaBhagia,DustinSchwenk,DavidAtkinson,RussellAuthur,
Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh
Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Ian Magnusson, Jacob Morrison, Niklas Muennighoff,
Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson,
ZejiangShen,EmmaStrubell,NishantSubramani,OyvindTafjord,EvanPeteWalsh,Hannaneh
Hajishirzi, Noah A. Smith, Luke Zettlemoyer, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and
Kyle Lo. Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining
Research. arXivpreprint,2024.
EmmaStrubell,AnanyaGanesh,andAndrewMcCallum.Energyandpolicyconsiderationsfordeep
learninginNLP. InAnnaKorhonen, DavidTraum, andLlu´ısMa`rquez, editors, Proceedingsof
the57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages3645–3650,Flo-
rence,Italy,July2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/P19-1355.
URLhttps://aclanthology.org/P19-1355.
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced trans-
former with rotary position embedding. ArXiv, abs/2104.09864, 2021. URL https://api.
semanticscholar.org/CorpusID:233307138.
17Together Computer. RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset,
April2023. URLhttps://github.com/togethercomputer/RedPajama-Data.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mandJoulin,EdouardGrave,andGuillaumeLample. Llama: Openandefficientfoundationlan-
guage models. ArXiv, abs/2302.13971, 2023a. URL https://api.semanticscholar.org/
CorpusID:257219404.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,Binh
Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023b. URLhttps://arxiv.org/abs/2307.09288.
Mar´ıaUbierna,CristinaD´ıezSantos,andSaraMercier-Blais. WaterSecurityandClimateChange:
HydropowerReservoirGreenhouseGasEmissions,pages69–94.SpringerSingapore,Singapore,
2022.ISBN978-981-16-5493-0.doi:10.1007/978-981-16-5493-0 5.URLhttps://doi.org/
10.1007/978-981-16-5493-0_5.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, edi-
tors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,
Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
DavidVilaresandCarlosGo´mez-Rodr´ıguez. HEAD-QA:Ahealthcaredatasetforcomplexreason-
ing. In Anna Korhonen, David Traum, and Llu´ıs Ma`rquez, editors, Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics, pages 960–966, Florence,
Italy, July2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/P19-1092. URL
https://aclanthology.org/P19-1092.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
Glue: Amulti-taskbenchmarkandanalysisplatformfornaturallanguageunderstanding. ArXiv,
abs/1804.07461,2018. URLhttps://arxiv.org/abs/1804.07461.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi
Chandu,DavidWadden,KelseyMacMillan,NoahA.Smith,IzBeltagy,andHannanehHajishirzi.
Howfarcancamelsgo? exploringthestateofinstructiontuningonopenresources,2023. URL
https://arxiv.org/abs/2306.04751.
JohannesWelbl,NelsonFLiu,andMattGardner.Crowdsourcingmultiplechoicesciencequestions.
arXivpreprintarXiv:1707.06209,2017. URLhttps://arxiv.org/abs/1707.06209.
Carole-JeanWu, RamyaRaghavendra, UditGupta, BilgeAcun, NewshaArdalani, KiwanMaeng,
GloriaChang,FionaAgaBehram,JamesHuang,CharlesBai,MichaelGschwind,AnuragGupta,
MyleOtt,AnastasiaMelnikov,SalvatoreCandido,DavidBrooks,GeetaChauhan,BenjaminLee,
Hsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat,
andKimHazelwood. Sustainableai: Environmentalimplications,challengesandopportunities,
2022. URLhttps://arxiv.org/abs/2111.00364.
Savvas Zannettou, Barry Bradlyn, Emiliano De Cristofaro, Haewoon Kwak, Michael Sirivianos,
Gianluca Stringini, and Jeremy Blackburn. What is gab: A bastion of free speech or an alt-
right echo chamber. In Companion Proceedings of the The Web Conference 2018, WWW ’18,
18page 1007–1014, Republic and Canton of Geneva, CHE, 2018. International World Wide Web
ConferencesSteeringCommittee. ISBN9781450356404. doi: 10.1145/3184558.3191531. URL
https://doi.org/10.1145/3184558.3191531.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a
machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. URL https:
//arxiv.org/abs/1905.07830.
Biao Zhang and Rico Sennrich. Root mean square layer normalization. ArXiv, abs/1910.07467,
2019. URLhttps://api.semanticscholar.org/CorpusID:113405151.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pherDewan,MonaDiab,XianLi,XiVictoriaLin,TodorMihaylov,MyleOtt,SamShleifer,Kurt
Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs/
2205.01068.
Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less
Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard
Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li. Pytorch fsdp: Experiences on scal-
ing fully sharded data parallel. Proc. VLDB Endow., 16:3848–3860, 2023. URL https:
//api.semanticscholar.org/CorpusID:258297871.
19A AdditionalEvaluation
Pile 100 PLs ICE Twitter AAE
10 100 1000 10000 10 100 1000 10000 10 100 1000 10000 10 100 1000 10000
Manosphere Gab 4chan
Models
Falcon-7B
LLaMA2-7B
MPT-7B
LLaMA-7B
Pythia-6.9B
RPJ-INCITE-7B
OLMo-7B
10 100 1000 10000 10 100 1000 10000 10 100 1000 10000
Tokens Seen (billions)
Figure3: Bitsperbyteforeachofthe7remainingPalomadatasourcesnotaggregatedinFigure2.
Additional perplexity results In Figure 3 we provide results for each of the 7 data sources in
Paloma(Magnussonetal.,2023)thatareexcludedfromthecombinedmetricinFigure2. Someof
thesesourcessuchasPile(Gaoetal.,2020)andICE(GreenbaumandNelson,1996)arenotpublicly
availableatthistime. Dolma100ProgrammingLanguages(Soldainietal.,2024)consistsofcode
datathatisnotsupportedbythedecontaminationapproachusedinPaloma. TwitterAAE(Blodgett
etal.,2016),alongwithICE,aredatasetsfortargetedanalysesofdisparitiesinperformancebetween
differentdialectsandassuchshouldbeevaluatedseparately. Andfinally,theManosphere,Gab,and
4chancorpora(Ribeiroetal.,2021;Zannettouetal.,2018;Papasavvaetal.,2020)areintendedto
examine model fit to language from fringe online communities that are studied for prevalent hate
speechandtoxicity. Thusminimizingperplexityonthesefringecorporaisnotalwaysdesirable.
OnenotableresulthereisthatOLMo-7BismuchfartheraheadoftheothermodelsonDolma100
Programming Languages (100 PLs). Note that this effect may be due in part to underestimation
from contamination, as decontaminating code data is beyond the scope of the method in Paloma.
AtthesametimeothermodelsthataretrainedoncodedatafromGitHubsuchasRPJ-INCITE-7B,
thatarejustaslikelytohavecontamination,fairmuchworse. AnotherfactorthenisthatOLMo-7B
trains on code data with exactly the same post-processing as that in 100 PLs while the code data
inothermodelswillhavebeenprocesseddifferently. Similarly,Pileevaluationdemonstratesthese
in-distributionandpotentialcontaminationeffectsasPythia-6.9Bachievestopperformancedespite
beingtrainedonalmostanorderofmagnitudefewertokensthanOLMo-7B.
The results on the remaining 5 targeted sources should be interpreted with care, as Paloma often
findsthatperplexityonthesesourcesisdominatedbysuperficialfeaturessuchaslowaveragedoc-
ument length rather than fit to that which would actually be salient to members of these speech
communities. TwitterAAEandGabhaveamongtheshortestdocumentsinPalomacontributingto
unusuallyhighbitsperbyteinthisfigure. Otherthanthesetwo,themodelsarenotablyveryclosely
groupedinadatascalingtrendinICE,Manosphere,and4chan.
Additional end-task results Next, in Table 8, we provide results from zero-shot evaluation of
OLMo-7B on 6 additional end-tasks apart from the 9 in our core evaluation suite. These tasks
are headqa en (Vilares and Go´mez-Rodr´ıguez, 2019), logiqa (Liu et al., 2020), mrpc (Dolan
and Brockett, 2005), qnli (Wang et al., 2018), wic (Pilehvar and Camacho-Collados, 2018), and
wnli(Wangetal.,2018).
20
etyB
reP
stiB
22.1
28.0
55.0
53.1
11.1
09.0
28.0
55.0
73.0
10.2
56.1
53.1
22.1
00.1
28.0
53.1
11.1
09.0
60.4
27.2
28.1headqa en logiqa mrpc qnli wic wnli avg.
Falcon-7B 38.6 23.7 62.8 49.8 49.5 47.9 45.4
LLaMA-7B 37.0 20.9 68.4 48.4 50.0 56.3 46.8
LLaMA2-7B 38.3 22.7 64.2 49.2 50.3 49.3 45.7
MPT-7B 37.4 22.9 67.7 52.1 48.1 47.9 46.0
Pythia-6.9B 40.1 21.5 65.4 53.8 55.0 38.0 45.6
RPJ-INCITE-7B 36.9 27.8 58.8 53.8 48.9 57.8 47.3
OLMo-7B 37.3 23.4 68.4 49.1 50.2 56.3 47.5
Table8:Zero-shotevaluationofOLMo-7Bon6additionalend-tasksapartfromthe9presentinour
coreevaluationsuite. Onceagain,wecompareOLMo-7Bto6othermodelcheckpointswhichare
publiclyavailable. WefindthatOLMo-7Boutperformstheothermodelsonaggregatetakenover6
additionalend-tasksfromthistable,howeverthesetaskswerealsofoundtoprovidelimitedsignal
duringtraining(seeFigure4).
Wenote,however,thatincontrasttoourcoreevaluationsetdescribedinSection4.1,wefoundthese
additional end-tasks to have less stable performance during model development, and to provide
a limited signal. This is illustrated in Figure 4, where we see the progress of task performance
throughouttrainingtobemorerandom(comparewiththemorestableupwardtrendsinFigure1).
Whiletaskssuchasmrpcandwicappearmorestable,theyofferedadditionaldifficultiesrelatedto
performance being tied to random chance (e.g., wic) or the tendency of models to make spurious
predictions (e.g., always predicting a single label) that either inflate or deflate performance due to
datasetclassimbalances(e.g.,mrpc).Wethereforecautionagainstrelyingtooheavilyonthesetasks
whenmeasuringmodelperformancethroughouttrainingandcomparingmodels.
headqa_en logiqa mrpc
500 1000 1500 2000 2500 500 1000 1500 2000 2500 500 1000 1500 2000 2500
qnli wic wnli
500 1000 1500 2000 2500 500 1000 1500 2000 2500 500 1000 1500 2000 2500
Tokens Seen (billions)
Figure4: AccuracyscoreprogressionofOLMo-7Bon6additionalend-tasks. Theperformanceof
theseadditionalend-taskswasunstableandprovidedlimitedsignalduringmodeldevelopment.
21
ycaruccA
83
63
43
45
15
42
22
02
2.05
0.05
8.94
06
54
46
65
84