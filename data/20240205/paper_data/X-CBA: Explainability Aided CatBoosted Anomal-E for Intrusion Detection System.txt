Accepted by 2024 IEEE International Conference on Communications (ICC), ¬©2023 IEEE
X-CBA: Explainability Aided CatBoosted Anomal-E for
Intrusion Detection System
Kiymet Kaya 1, Elif Ak 1,3, Sumeyye Bas 2,3, Berk Canberk 4, Sule Gunduz Oguducu2
1
Istanbul Technical University, Department of Computer Engineering, Istanbul, Turkey
2
Istanbul Technical University, Department of Artificial Intelligence and Data Engineering, Istanbul, Turkey
3
BTS Group, Istanbul, Turkey
4
School of Computing, Engineering and Built Environment, Edinburgh Napier University, Edinburgh
Email: {kayak16, akeli, bass20}@itu.edu.tr, B.Canberk@napier.ac.uk, sgunduz@itu.edu.tr
Abstract‚ÄîThe effectiveness of Intrusion Detection Systems (IDS) is IDS studies [2], since aggregating network features helps to reveal
criticalinanerawherecyberthreatsarebecomingincreasinglycomplex. diverse and heterogeneous characteristics of cyberattacks. Expanding
Machine learning (ML) and deep learning (DL) models provide an
onthesecondaspect,thereisaconcertedacademicefforttoincorpo-
efficient and accurate solution for identifying attacks and anomalies in
rate Explainable AI (XAI) models into various ML/DL frameworks
computer networks. However, using ML and DL models in IDS has
led to a trust deficit due to their non-transparent decision-making. This to provide local and global explanations of their operations. This
transparencygapinIDSresearchissignificant,affectingconfidenceand endeavor aims to exploit the rationale behind model predictions,
accountability. To address, this paper introduces a novel Explainable whether by clarifying the significance of specific data points (local
IDS approach, called X-CBA, that leverages the structural advantages
explainability) or by shedding light on the model‚Äôs overall behavior
ofGraphNeuralNetworks(GNNs)toeffectivelyprocessnetworktraffic
data, while also adapting a new Explainable AI (XAI) methodology. (global explainability) [1]. Considering the research efforts on IDS
UnlikemostGNN-basedIDSthatdependonlabelednetworktrafficand andthesecurityneedsoforganizations,itisevidentthatanadvanced
nodefeatures‚Äîtherebyoverlookingcriticalpacket-levelinformation‚Äîour GNN-based methodology, combined with an appropriate XAIframe-
approachleveragesabroaderrangeoftrafficdatathroughnetworkflows,
work, would bridge the existing divide. Nevertheless, surprisingly,
including edge attributes, to improve detection capabilities and adapt to
there is a notable scarcity in the literature, with few studies, such as
novelthreats.Throughempiricaltesting,weestablishthatourapproach
not only achieves high accuracy with 99.47% in threat detection but theonebyBaahmedetal.[5],implementinganadvancedGNNmodel
also advances the field by providing clear, actionable explanations of its alongsideanXAItool.Tofillthisgap,theproposedX-CBAenhances
analyticaloutcomes.Thisresearchalsoaimstobridgethecurrentgapand theattackdetectionresultsofstate-of-the-artIDS,anditdemonstrates
facilitatethebroaderintegrationofML/DLtechnologiesincybersecurity
thatthePGExplainer[6]providessuperiorperformanceinexplaining
defensesbyofferingalocalandglobalexplainabilitysolutionthatisboth
preciseandinterpretable. the operational dynamics of sophisticated GNNs for network flows
Index Terms‚Äînetwork intrusion detection system, graph neural net- using local and global explainability. The main contributions of our
works, explainable artificial intelligence, self-supervised learning, edge study can be summarized as follows:
embedding,catboost
‚Ä¢ Flow-based Network Data and Graph Edge Embeddings:
I. INTRODUCTION
The proposed intrusion detection system (IDS) approach uses
In the contemporary digital environment, the continued increase network flows with many critical network metrics, representing
in sophisticated cyber threats still leaves security mechanisms in- themasgraphedgeembeddings.Usingflow-basednetworkdata
adequate. Gartner forecasts that by 2024, a minimum of 50% of with well-modeled graph embedding is particularly effective in
organizations will adopt Machine Learning (ML) or Deep Learning detecting a range of cyber threats, including BruteForce, DDoS,
(DL)aidedSecurityOperationsCenters(SoCs)forfastercyberattack DoS,andsophisticatedattackslikeBot,byuncoveringdistinctive
detection,ashiftthatisalreadyunderwaywithsubstantialinvestments threat patterns.
from leading firms in AI for enhanced security. However, it is ‚Ä¢ Enhanced Detection Performance with GNN and CatBoost
also well-known that there is a notable hesitance among enterprises Integration: Our novel method integrates a GNN-based de-
to adopt ML/DL-augmented network Intrusion Detection Systems tection pipeline with the CatBoost classifier, achieving higher
(IDS) due to the inconceivable black-box decision-making processes, accuracy, F1 score, and detection rates compared to existing
which are perceived as complicated, unpredictable, and unreliable state-of-the-art solutions in intrusion detection.
[1]. Addressing these concerns, the current IDS literature mainly ‚Ä¢ AdvancedExplainabilitywithPGExplainerImplementation:
prioritizes (i) developing advanced ML/DL models for sophisticated The system implements PGExplainer, offering both local and
attackdetection[2],[3],and(ii)employingexplainableAItodemys- global insights into the decision-making processes of the GNN-
tify ML/DL decision-making [1]. For the first aspect, recent studies based IDS. This approach outperforms baseline explainability
indicatethatGraphNeuralNetworks(GNNs),asubsetofDLmodels, models, particularly due to its ability to operate in inductive
are particularly promising for IDS [4]. Since the natural structure of settings,offer anon-black-boxevaluation approach,and explain
networkIDSisgraph-based,wherenodesarethenetworkdevices(e.g. multipleinstancescollectively,makingithighlysuitableforflow-
routers, hosts, etc. ) and edges are connections, packet transfers, or based IDS network data.
networkflowsbetweennetworkdevices.Inthisway,GNNsrevealthe
impactofmaliciousactivitiesonthenetwork‚Äôstopologyandleverage The rest of the paper is organized as follows. Section II presents
the neighboring information among network entities for improved related works.Section III gives details of our proposed model and
detection.Moreover,thecurrentstudiesshowthatusingnetworkflows background methodology. Section IV presents baseline models and
rather than individually packet-based monitoring is more suitable in experimental results. Lastly, Section V concludes the paper.
4202
beF
1
]RC.sc[
1v93800.2042:viXraPre-Modeling Modeling Post-Modeling
embeding space
Step-5: self-supervised edge embedding, DGI
Step-1: flow-based network data capturing
Step-2: data preprocesing E-GraphSAGE
~ encoder, ùê∏
IPV4_SRC_ADDRIPV4_DST_ADDRL4_SRC_PORT ‚Ä¶ IN_PKTSOUT_PKTS ‚Ä¶ LABEL corruption, ùê∂
#!" #!"
Benign
59.166.0.4 149.171.126.1 1060 ‚Ä¶ 89 102 ‚Ä¶ 0 function
59.166.0.1 149.171.126.8 1269 ‚Ä¶ 132 192 ‚Ä¶ 1 - +
59.16‚Ä¶ 6.0.9 149.17‚Ä¶ 1.126.0 101‚Ä¶ 1 ‚Ä¶ ‚Ä¶‚Ä¶ 7‚Ä¶ 5 6‚Ä¶ 8 ‚Ä¶ ‚Ä¶ ‚Ä¶ 0 discriminator,ùê∑ #!" CatBoost Attack
Step-3: time-series IDS dataset
Step-6: intruison detection
ùêª=ùê∂(ùê∫) ùê∫
ùë£ ‚Üí
E-GraphSAGE ùíÆ
ùë¢ ùëí encoder, ùê∏
!"
Read-out, ùëÖ
PGExplainer
ùëí!!"!ùëí!"""ùëí!#"# ùëí!!"!
~
#!" #!" perform
Step-4: graph construction, ùê∫=(ùëâ,ùê∏,ùëí!") Step-5a: encoding Step-5b: decoding Step-7: explainability
training embedding model
revise
trained embedding model
Fig. 1: X-CBA: Explainability Aided CatBoosted Anomal-E Intrusion Detection System based on the DARPA [7] Recommendation
II. LITERATUREREVIEW PGExplainer shown in red-text in Fig. 1 are presented to make the
proposedmodeleasytounderstand.Secondly,theflowoftheX-CBA
The benefit of GNNs in intrusion detection is that they take into
which is presented in Fig. 1.
accountthepropertiesofthecomputernetworktopologysuchasrela-
tionshipsbetweennodes.However,thisalsoincreasesthecomplexity
A. Background Methods
of the prediction model and can be computationally costly in large
graphsandcomplexnetworktopologies[8],[9].Computationalspeed 1) Edge Embedding: E-GraphSAGE: E-GraphSAGE [17] algo-
is important, especially in large networks and real-time applications rithm requires a graph input G(V,E) consisting of nodes (V) and
like intrusion detection. For this reason, many of the current works edges (E). As an extended version of the GraphSAGE algorithm,
[10] utilize a less complex GNN model for representation learning it allows the use of edge features ({e uv ‚àÄuv ‚àà E}) between each
‚àí‚Üí
and predicting network anomalies mainly with tree-based ensemble node u and node v in addition to node features ( x v) for message
models. Moreover, there are still some works [11], [12] that only propagation. By utilising the edge attributes and graph topology, E-
use tree-based eXtreme Gradient Boosting (XGBoost) and CatBoost GraphSagegeneratesoutputcontaininganewvectorialrepresentation
methods for forecasting, since it is hard to learn a powerful GNN a.k.a.embeddingsforeachnode(zK v )andeachedge(zK uv).Theflow-
when computing resources are limited [8]. based NIDS datasets only consist of flow (edge) features rather than
‚àí‚Üí
Among the studies on intrusion detection on NSL-KDD data, node features. Therefore, the feature vectors of nodes are set as x v
in [13] the prediction model 3-layers neural network (NN) was = {1, . . . , 1} and the dimension of all one constant vector is the
explained with SHAP and LIME, in [14] anomalies were predicted sameasthenumberofedgefeatures.E-GraphSAGEsamplesafixed
with RF and the explanations were conducted with SHAP. Patil et numberofneighborskforeachnodeinthegraphdataandaggregates
al. performed intrusion detection with Random Forest models and information from the sampled neighbors to create an embedding for
usedLIMEformodelexplanation[15].Whenreviewingtheliterature the destination node and edge embeddings. The initial value for k=0
on explainability in intrusion detection models, most studies favor (h0 v) is the feature vector ( ‚àí x‚Üí v) of that node for all nodes.
non-GNN-based explanation methods like SHAP and LIME, despite (cid:16) (cid:17)
hk ‚ÜêAGG {hk‚àí1‚à•ek‚àí1,‚àÄu‚ààN(v),uv‚ààE} ;
employingGNN-basedpredictionmodels.Onlyafewstudies[5],[16] N(v) k u uv
includeGNN-basedmodelsandexplanations.Inbothofthesestudies, hk ‚ÜêœÉ(cid:16) Wk¬∑CONCAT(hk‚àí1,hk )(cid:17) (1)
the authors only observed the performance of GNNExplainer to find v v N(v)
theimportantcomponentsofthenetworkwhereascomparativeresults Ineachiterationfromk=1toK,forallnodesinthenode-setV,the
in terms of XAI models were not observed. To this end, this study node v‚Äôs neighborhood is initially sampled and the information from
focuses on improving the predictive accuracy of intrusion detection thesamplednodesiscollectedintoasinglevector.Next,asin1,for
models and conducting a comparative analysis of XAI methods on allkandvvalues,theaggregatedinformationhk atthek-thlayer
N(v)
computernetworkdata,thatprovideadditionalinsightstounderstand andatnodev,basedofthesampledneighborhoodN(v)iscalculated
and interpret conclusions of IDS. withthehelpofneighborhoodaggregatorfunctionAGG .Here,ek‚àí1
k uv
arethefeaturesofedgeuv fromN(v),thesampledneighborhoodof
III. METHODOLOGY
node v, at layer k-1.
TheoverviewoftheMethodologyisorganizedintwosteps.Inthe
first step, Background Methods E-GraphSAGE, DGI, CatBoost, and zK ‚ÜêCONCAT(zK,zK) (2)
uv u vTheaggregatedembeddingsofthesampledneighborhoodhk are max MI(Y,G )=H(Y)‚àíH(Y|G=G ) (5)
N(v) GS S S
thenconcatenatedwiththenode‚Äôsembeddingfromthepreviouslayer
PGExplainerdeterminesG bymaximizingthemutualinformation
hk‚àí1.Here,thecriticaldifferencefromtheGraphSAGEishavingthe S
v MI between the predictions Y and this underlying structure with
edge features. The final node embeddings at depth K are assigned
the help of entropy term H as in 5. The goal of the PGExplainer
and the edge embeddings zK for each edge uv are calculated as the
uv approach is to specifically explain the graph topologies found in
concatenation of the node embeddings of nodes u and v as in 2.
GNNs and identify G such that conditional entropy is minimized.
2) Self-supervised Learning: Deep Graph Infomax: Self- S
However, because there are so many candidate values for G , direct
supervisedlearningaimstolearntheunderlyingfeaturesofthedataby S
optimization is infeasible. Therefore, assuming G follows a Gilbert
creatingitsownpseudo-labelsfromunlabeleddata.Thepseudo-labels S
randomgraphdistribution,selectionsofedgesfromtheoriginalinput
arethelabelsautomaticallyobtainedbytheself-supervisedmodel,not
graph G are conditionally independent of each other, a relaxation
thegroundtruthsofthedata.Teachingthesepseudo-labelscontributes
approach is used. With this relaxation, PGExplainer can recast the
to the creation of good representations for the data and can improve
aim as an expectation, making optimization more manageable.
the prediction performance of the supervised learning model to be
usedlater.DGI[18]providesself-supervisedlearningbymaximizing B. X-CBA: Explainability Aided CatBoosted Anomal-E
mutual local-global information and the trained Encoder of DGI can
TheflowoftheExplainabilityAidedCatBoostedAnomal-Eframe-
bereusedtogenerateedge/nodeembeddingsforsubsequenttaskssuch
work is presented in Fig. 1. In the proposed framework, computer
as edge/node classification. The details of the DGI model presented
intrusion detection tabular data is transformed into attributed multi-
in Step 5 of Fig. 1 are as follows:
graphdataaftertherequiredpreprocessingstepsarecompleted.Here,
‚Ä¢ A corruption function C (i.e., a random permutation of the routers represent nodes and data flows represent edges. The self-
input graph node features which add or remove nodes from the
learning DGI model is tuned with an E-GraphSAGE encoder and
adjacency matrix A) is used to generate a negative (corrupted)
model training is performed on multigraph intrusion detection data.
graph representation H =C(G) from the input graph (G).
Gradient descent optimization, powered by the Adam Optimizer and
‚Ä¢ An encoder E, which can be any existing GNN such as E- the Binary Cross Entropy (BCE) Loss in 3, is used to iteratively
GraphSAGE,generatesedgeembeddingsbothfortheinputgraph
optimize the D, R, and E of DGI. The encoder of the DGI uses
(G) and corrupted graph (H).
a mean aggregation function on a 1-layer E-GraphSAGE model. E-
‚Ä¢ ThereadoutfunctionR,whichisatthecoreofDGI,aggregates GraphSAGE uses a hidden layer size of 256 units and ReLU is used
edge embeddings by taking average of them and then processes
as the activation function. As for the generation of the global graph
them through a sigmoid function to calculate a global graph
summary, we averaged edge embeddings and passed them through a
‚àí‚Üí
summary s (a single embed vector of the entire graph).
sigmoidfunction.BCEisusedasalossfunctionandgradientdescent
‚Ä¢ Thediscriminator ‚àíD ‚àí‚àít ‚Üíhenassessestheseedgeembeddings( ‚àía ‚àír ‚àíe ‚Üíal isusedforbackpropagationwiththeAdamoptimizerusingalearning
edgeembedding z uv(i) andacorruptededgeembedding (cid:101)z uv(j) rateof0.001.Withthetrainedencoder,edgeembeddingsareobtained.
‚àí‚Üí
) using the global summary s as a guide. IntrusiondetectionisperformedwiththeCatBoostClassifierusingthe
P edge embeddings that outperform the meta-features of the data. The
1 (cid:88) ‚àí‚àí‚àí‚Üí ‚àí‚Üí
L=
P
+S( E (X,A))[logD( z uv(i), s)] prediction results are explained with the help of PGExplainer, which
i=1 (3) isanXAImethoddesignedspecificallyforgraphdata.Inthisway,the
(cid:88)S ‚àí‚àí‚àí‚Üí ‚àí‚Üí edges (data flows) that contribute the most to the prediction and are
+ E [(1‚àílogD( z , s))])
(X,A)) (cid:101)uv(j) critical in the network topology are identified. The implementation
j=1 details and the code repository of the proposed IDS are available
‚Ä¢ Comparisons by the discriminator D provides a score between here1.
0 and 1, with the help of binary cross-entropy loss objective
functionin3todiscriminatetheembeddingoftherealedgeand
IV. PERFORMANCEEVALUATION
the corrupted edge to train the encoder E. We first evaluate the proposed X-CBA approach for intrusion
3) Intrusion Detection: Catboost Classifier: CatBoostClassifier detectionperformancecomparingwiththebaselinesinSectionIV-A.
is a gradient-boosting ML algorithm known for its high predictive Then we also provide the explainability performance analysis with
accuracy and speed. It employs techniques such as ordered boosting the state-of-the-art method in Section IV-B.
and oblivious trees to handle various data types effectively and
A. X-CBA Intrusion Detection Evaluation
mitigate overfitting.
4) Explainability: PGExplainer: PGExplainer [6] offers explana- For the intrusion detection experiments, we have chosen baselines
tions on a global level across numerous instances by developing a from two categories: (i) Step 5 in Modeling and (ii) Step 6 in Post-
shared explanation network from nodes and graph representations of ModelingasshowninFig.1.Here,Anomal-E,DGI,andGraphSAGE
the GNN model. It seeks to locate a crucial subgraph that includes modelsareconsideredasedgeembeddingbaselinesfortheevaluation
the most important nodes for the predictions of the given trained of the X-CBA model from the Step 5 category. For the second
GNNmodelmakespredictionsbyremovingnodesandattributes,and category from Step 6, the classifier models to be used as baselines
analyzes their effects on the output of the GNN model. Removing are as follows:
nodes also means removing the edges that are the endpoints of that Principal Component Analysis (PCA) algorithm adapted for
node from the graph, which leads to the identification of crucial intrusiondetection,usingacorrelationmatrixfrom‚Äúbenign‚Äùsamples
pathways.PGExplainerdividestheinputgraphGintotwosubgraphs to identify attacks based on deviation from the benign correlation.
as in 4, G represents the crucial subgraph and ‚àÜG is comprised of Isolation Forest (IF) utilizes tree structures to isolate attacks:
S
samplesclosertotherootofthetreeareidentifiedasattacks,whilethe
unnecessary edges.
G=G S+‚àÜG (4) 1https://github.com/kiymetkaya/xai-catboosted-anomaleTABLE I: Comparative Performance Evaluation of Edge Embedding Baselines
NF-UNSW-NB15-v2 NF-CSE-CIC-IDS2018-v2 Averageacrossdatasets
F1-Macro Acc DR F1-Macro Acc DR F1-Macro
Anomal-E 92.35% 98.66% 98.77% 94.38% 97.80% 82.67% 93.36%
DGI 48.99% 96.02% 0.00% 46.82% 88.03% 0.00% 47.90%
GraphSAGE 54.77% 88.60% 26.63% 94.61% 97.90% 82.60% 74.69%
deeperonesinthetreeareconsidered‚Äúbenign‚Äù,creatinganensemble HBOS, PCA, and CBLOF methods together. The reason for this is
of trees for efficient and effective intrusion detection. thatanyoneofthesemethodsalonedoesnotproducethebestresults
Clustering-BasedLocalOutlierFactor(CBLOF)treatsintrusion in terms of all performance evaluation metrics.
detection as a clustering-based problem, assigning outlier factors
TABLE II: Network Intrusion Detection Prediction Results
based on cluster size and distance between a sample and its nearest
cluster to determine if it is an attack. NF-CSE-CIC-IDS2018-v2
Histogram-BasedOutlierScore(HBOS)Employingahistogram- F1-Macro Acc DR
basedapproach,constructsunivariatehistogramsforeachfeatureand Anomal-E-IF 81.11% 89.79% 91.84%
calculates bin densities, using these density values to determine the Anomal-E-HBOS 91.89% 96.86% 77.79%
HBOS score and classify samples as attacks or not. Anomal-E-PCA 92.57% 97.11% 79.16%
AutoEncoder (AE) is an unsupervised deep learning model, con- Anomal-E-CBLOF 94.38% 97.80% 82.67%
sisting of four Linear layers, the first two used for encoding and X-CBA-RFC 96.53% 98.61% 88.50%
the last two for decoding, which respectively transform {number of X-CBA-AutoEncoder 97.76% 99.13% 94.92%
features, 16, 8, 16, number of features}. After each Linear layer, X-CBA-XGBC 98.45% 99.36% 94.85%
X-CBA-LightGBM 98.56% 99.40% 95.19%
ReLU is used as an activation function.
X-CBA-CatBoost 98.73% 99.47% 95.74%
Random Forest Classifier (RFC) is a bootstrap ensemble model.
It creates several decision trees on data samples and then selects the
On the other hand, when we utilize only CatBoost in the predic-
bestsolutionusingvoting.RFCischosenratherthanasingledecision
tion phase with the X-CBA approach, we obtain the most accurate
tree because it reduces over-fitting and has less variance.
predictions with a single boosting model as seen in Table II green-
XGBoost Classifier (XGBC) stands for Extreme Gradient Boost-
backgrounded row. Table II presents the results we obtained on
ing. XGBoost is an implementation of gradient boosting. The key
the NF-CSE-CIC-IDS2018-v2 dataset with ETC, RFC, AE, XGBC,
ideabehindtheXGBCistheimprovementofspeedandperformance
LightGBM, and CatBoost ensemble models against the prediction
usingthereasonsbehindthegoodperformancesuchasregularization
algorithms in Anomal-E. NF-CSE-CIC-IDS2018-v2 dataset contains
and handling sparse data.
18,893,708 network flows: 16,635,567 (88.05%) benign samples and
LightGBM Classifier is a histogram-based gradient boosting al-
2,258,141(11.95%)attacksamples.Forafaircomparison,wefollow
gorithmthatreducesthecomputationalcostbyconvertingcontinuous
the same preprocessing steps, training procedures, and train-test split
variables into discrete ones. Since the training time of the decision
(70% training, 30% testing) as in Anomal-E [10]. We then present
treesisdirectlyproportionaltothenumberofcomputationsandhence
our results using the same test set for consistency. Moreover, the
the number of splits, LightGBM provides a shorter model training
best CatBoost model was determined with Scikit-learn GridSearch
time and efficient resource utilization.
5-fold-CV,justliketheotherensemblemodels.Accordingly,thebest
The results presented in Table I are a summary result table for hyperparameters for CatBoost are: ‚Äúmin. samples split‚Äù: 4, ‚Äúmin.
the prediction results of the Anomal-E, DGI, and GraphSAGE with samples leaf‚Äù: 4, ‚Äúmax. depth‚Äù: 8.
the best corresponding baseline classifier model evaluated with the
list above. ‚ÄòMacro Average F1-score‚Äô which is the average F1 score B. X-CBA Explainability Evaluation
of all classes, ‚ÄòAccuracy‚Äô and ‚ÄòDetection Rate‚Äô (DR, also known as The explainability performance of X-CBA approach implemented
Recall)thatmeasuresthepercentageofactualattackobservationsthat with PGExplainer [6] is evaluated through state-of-the-art XAI ap-
the model correctly classified are chosen as performance evaluation proaches. We analyze XAI methods designed specifically for GNNs
metrics.AsitcanbeseenfromtheresultsinTableI,thesuperiorityof that have the ability to explain important edges. Table III provides
theAnomal-Eoverthestate-of-the-artmethodsDGIandGraphSAGE a summary of this analysis on XAI methods. In Table III, ‚ÄúGNN
has been proven with the experiments [10] on NF-UNSW-NB15-v2 Design‚Äù indicates whether the XAI method has a specific design
and NF-CSE-CIC-IDS2018-v2 datasets. for GNN models; ‚ÄúBlack Box‚Äù indicates whether the prediction
Afterthepreliminaryexperiments,inthesecondstep,theproposed model is treated as a black-box when being explained, and ‚ÄúTarget‚Äù:
X-CBA approach is compared with Anomal-E [10] since Anomal- represents the target component (N: nodes, NF: node features, E:
E is the edge embedding approach among state-of-the-art as shown edges.) whose explanation is presented. As can be seen in Table
in Table I. For further detailed analysis, X-CBA and Anomal-E are III, the explainability of GNNs‚Äô edges with the current state-of-
evaluatedwithvariousclassifierbaselineslocatedinStep6inthePost- the-art is measured by GNNExplainer and PGExplainer approaches.
Modeling. According to the results in Table II, where the prediction Other GNN explainability approaches are either focuses on nodes or
results are presented in ascending order according to the F1-Macro, node features, which is not suitable for flow-based network intrusion
the proposed X-CBA produced more accurate results comparing to detection explainability.
Anomal-E in both unsupervised (with AutoEncoder) and supervised Moreover, we evaluated the performance of network-flow impor-
(with CatBoost) approaches among baseline models. In other words, tance (a.k.a edge importance) with two metrics [26] as used in
Anomal-E utilizes edge embeddings and predicts attacks using IF, edge explainability performance evaluation: Sparsity metric in 6 andBenign
Network Flow
Explainability
Bot
Network Flow
Explainability
Infiltration
Network Flow
Explainability
Fig. 2: Distribution of edges according to attack type within the edge explanation maximizing mutual information for ‚ÄòBenign‚Äô, ‚ÄòBot‚Äô and
‚ÄòInfiltration‚Äô
TABLEIII:XAIMethodsComparisondueto‚ÄúGNNDesign‚Äù,‚ÄúBlack
Box‚Äù and ‚ÄúTarget‚Äù. F1-Macro
Method GNNDesign BlackBox Target
CAM,Grad-CAM[19] - - N
LRP[20] - - N
Accuracy DR
SHAP[21] - + NF
GraphLIME[22] + + NF
PGM-Explainer[23] + + N
ZORRO[24] + + N
GNNExplainer[25] + + E
X-CBA-PGExplainer[6] + - E
Fidelity+F metric in 7. The Sparsity whose formula is given in Fig. 3: The Fidelity+F comparisons over F1-Macro, Accuracy and
6 measures the proportion of important components (N, NF, E) DR under different Sparsity levels
identified by the XAI method. Here, |m | shows the number of
i
important edges determined by the XAI method, |M | shows the
i
numberofalledgesinthegraphandK isthenumberofgraphs.On
the identified edges are more important for the proposed X-CBA.
the other hand, Fidelity+F whose formula is given in 7 studies the
According to the results in Fig. 3, as it is expected, the PGExplainer
change of prediction score where F shows performance evaluation
outperformstheGNNExplainersignificantlyandconsistently.Thisis
function (F1-Macro, Accuracy, and DR) [26]. G i1‚àími represents
because PGExplainer‚Äôs non-black box explanation ability reveals the
the new graph obtained by keeping features of G based on the
i explanation for flow-based intrusion packets better.
complementary mask (1‚àím ) and y is the original prediction of
i i
Moreover,weinvestigatethemostinfluentialsubgraphsthataffect
the GNN model.
thepredictionofanedgeofagivenattacktypetolocallyobservethe
Sparsity=
1 (cid:88)K (cid:18)
1‚àí
|m i|(cid:19)
(6)
differences between PGExplainer and GNNExplainer. NF-CSE-CIC-
K |M | IDS2018-v2includesdataforbenignandfifteendifferentattacktypes.
i
i=1
For each attack type, we make use of the edges found to maximize
Fidelity+F = 1 (cid:88)K (cid:16) F(G ) ‚àíF(cid:0) G1‚àími(cid:1) (cid:17) (7) mutual information by each XAI model. In Fig. 2, we illustrate the
K i yi i yi distributionofedgetypesinthesubgraphsidentifiedforthreeselected
1 classes:Benign,Bot,andInfiltration.Theseclasseswerechosenfrom
Fig. 3 presents the global graph‚Äôs edge explanation results for a total of 16 (15 attack classes and one benign class) to provide
PGExplainer and GNNExplainer with Fidelity + F and Sparsity. a diverse range of examples for analysis: Benign represents a non-
Fidelity+F measures the F1-Macro, Accuracy, and DR drops when attack scenario, Infiltration serves as a sample attack case, and Bot
important edges are removed. Higher Fidelity + F scores indicate exemplifies a sophisticated attack. The prevalence of benign edgeswithinsubgraphsofvariousattacksisunsurprising,giventheinherent [4] T. Bilot, N. E. Madhoun, K. A. Agha, and A. Zouaoui, ‚ÄúGraph neural
dataset imbalance. The subgraph for ‚ÄòBenign‚Äô in Fig. 2 is therefore networks for intrusion detection: A survey,‚Äù IEEE Access, vol. 11, pp.
49114‚Äì49139,2023.
expectedtocontainonlybenignedges,whichisprovidedonlybythe
[5] A. R. E.-M. Baahmed, G. Andresini, C. Robardet, and A. Appice,
PGExplainer, and failed in GNNExplainer.
‚ÄúUsing graph neural networks for the detection and explanation of
In contrast, Infiltration and Bot attacks are marked by a pre- networkintrusions,‚ÄùInternationalWorkshoponeXplainableKnowledge
dominance of similar connection types in their immediate network DiscoveryinDataMining,2023.
environments. These attacks are known for their ability to spread [6] D. Luo, W. Cheng, D. Xu, W. Yu, B. Zong, H. Chen, and X. Zhang,
‚ÄúParameterizedexplainerforgraphneuralnetwork,‚ÄùAdvancesinneural
across the network, often utilizing similar connections. In Fig. 2,
informationprocessingsystems,vol.33,pp.19620‚Äì19631,2020.
illustrated by red rectangular boxes, PGExplainer distinguishes itself [7] D.GunningandD.Aha,‚ÄúDarpa‚Äôsexplainableartificialintelligence(xai)
in identifying key features of both Infiltration and Bot attacks. program,‚ÄùAImagazine,vol.40,no.2,pp.44‚Äì58,2019.
However,itsperformanceadvantageissomewhatlesspronouncedfor [8] S.S.Du,K.Hou,R.R.Salakhutdinov,B.Poczos,R.Wang,andK.Xu,
‚ÄúGraphneuraltangentkernel:Fusinggraphneuralnetworkswithgraph
Botattacks.ThisisbecauseBotattacksinourdatasetareconsistently
kernels,‚Äù Advances in neural information processing systems, vol. 32,
linked to a specific node (IPV4 ADDR: 18.219.211.138), a pattern
2019.
easilydetectedbybothXAImethods.Consequently,theperformance [9] G. Secinti, P. B. Darian, B. Canberk, and K. R. Chowdhury, ‚ÄúResilient
of the two methods is similar for Bot attacks. end-to-end connectivity for software defined unmanned aerial vehicular
PGExplainer‚Äôstruestrengthisdemonstratedinitsabilitytoidentify networks,‚Äù in 2017 IEEE 28th Annual International Symposium on
Personal, Indoor, and Mobile Radio Communications (PIMRC), 2017,
Infiltration flows. These flows are often challenging to detect as
pp.1‚Äì5.
they are not always directly connected to the Infiltration attack. [10] E.Caville,W.W.Lo,S.Layeghy,andM.Portmann,‚ÄúAnomal-e:Aself-
On the other hand, GNNExplainer significantly underperforms in supervised network intrusion detection system based on graph neural
this area, frequently misidentifying Infiltration attack instances as networks,‚ÄùKnowledge-BasedSystems,vol.258,p.110030,2022.
[11] S. Fraihat, S. Makhadmeh, M. Awad, M. A. Al-Betar, and A. Al-
Bot attacks and missing most of the real Infiltration network flows.
Redhaei,‚ÄúIntrusiondetectionsystemforlarge-scaleiotnetflownetworks
This discrepancy can be attributed to PGExplainer‚Äôs operation in usingmachinelearningwithmodifiedarithmeticoptimizationalgorithm,‚Äù
inductive settings and its capability to collectively explain multiple InternetofThings,p.100819,2023.
instances, enabling it to more effectively uncover local network flow [12] E.AkandB.Canberk,‚ÄúForecastingqualityofservicefornext-generation
data-drivenwifi6campusnetworks,‚ÄùIEEETransactionsonNetworkand
relationships.
ServiceManagement,vol.18,no.4,pp.4744‚Äì4755,2021.
V. CONCLUSIONANDFUTUREWORKS [13] S. Mane and D. Rao, ‚ÄúExplaining network intrusion detection system
usingexplainableaiframework,‚ÄùarXivpreprintarXiv:2103.07110,2021.
Inthisstudy,weproposeanovelIDSmethodology,calledX-CBA, [14] M. Wang, K. Zheng, Y. Yang, and X. Wang, ‚ÄúAn explainable machine
that synergizes the strengths of Graph Neural Networks (GNNs) and learningframeworkforintrusiondetectionsystems,‚ÄùIEEEAccess,vol.8,
Explainable AI (XAI). X-CBA not only outperforms in detecting a pp.73127‚Äì73141,2020.
[15] S. Patil, V. Varadarajan, S. M. Mazhar, A. Sahibzada, N. Ahmed,
widearrayofcyberthreatsthroughitsuseofnetworkflowsandgraph
O. Sinha, S. Kumar, K. Shaw, and K. Kotecha, ‚ÄúExplainable artificial
edgeembeddingsbutalsomarksasignificantleapintheaccuracyand
intelligenceforintrusiondetectionsystem,‚ÄùElectronics,vol.11,no.19,
reliabilityofthreatdetection,asevidencedbyitsremarkable99.47% p.3079,2022.
accuracy, 98.73% F1 rate, and 95.74% recall. Most importantly, X- [16] W. W. Lo, G. Kulatilleke, M. Sarhan, S. Layeghy, and M. Portmann,
CBA addresses the critical issue of transparency in ML/DL-based ‚ÄúXg-bot:Anexplainabledeepgraphneuralnetworkforbotnetdetection
andforensics,‚ÄùInternetofThings,vol.22,p.100747,2023.
security solutions. We evaluated the baseline XAI methods to show
[17] W.W.Lo,S.Layeghy,M.Sarhan,M.Gallagher,andM.Portmann,‚ÄúE-
the strong explainability of our proposed framework in terms of its graphsage:Agraphneuralnetworkbasedintrusiondetectionsystemfor
abilitytofindimportantedges.ByintegratingPGExplainer,itprovides iot,‚ÄùinNOMS2022-2022IEEE/IFIPNetworkOperationsandManage-
bothlocalandglobalexplanationsofitsdecision-makingprocessand mentSymposium. IEEE,2022,pp.1‚Äì9.
[18] P. VelicÀákovic¬¥, W. Fedus, W. L. Hamilton, P. Lio`, Y. Bengio, and R. D.
gives much more accurate results in terms of sparsity and fidelity
Hjelm,‚ÄúDeepgraphinfomax,‚ÄùarXivpreprintarXiv:1809.10341,2018.
metrics compared to baselines, enhancing trust and accountability in [19] P. E. Pope, S. Kolouri, M. Rostami, C. E. Martin, and H. Hoffmann,
its operations. ‚ÄúExplainability methods for graph convolutional neural networks,‚Äù in
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
ACKNOWLEDGEMENTS recognition,2019,pp.10772‚Äì10781.
This research is supported by the Scientific and Technologi- [20] F.BaldassarreandH.Azizpour,‚ÄúExplainabilitytechniquesforgraphcon-
volutionalnetworks,‚ÄùinICML2019Workshop‚ÄùLearningandReasoning
cal Research Council of Turkey (TUBITAK) 1515 Frontier R&D
withGraph-StructuredRepresentations‚Äù,2019.
Laboratories Support Program for BTS Advanced AI Hub: BTS [21] S.M.LundbergandS.-I.Lee,‚ÄúAunifiedapproachtointerpretingmodel
Autonomous Networks and Data Innovation Lab. project number predictions,‚ÄùAdvancesinneuralinformationprocessingsystems,vol.30,
5239903, TUBITAK 1501 project number 3220892, and the ITU 2017.
[22] Q. Huang, M. Yamada, Y. Tian, D. Singh, and Y. Chang, ‚ÄúGraphlime:
Scientific Research Projects Fund under grant number MC¬∏AP-2022-
Localinterpretablemodelexplanationsforgraphneuralnetworks,‚ÄùIEEE
43823. TransactionsonKnowledgeandDataEngineering,2022.
[23] M. Vu and M. T. Thai, ‚ÄúPgm-explainer: Probabilistic graphical model
REFERENCES
explanationsforgraphneuralnetworks,‚ÄùAdvancesinneuralinformation
[1] S.Neupane,J.Ables,W.Anderson,S.Mittal,S.Rahimi,I.Banicescu, processingsystems,vol.33,pp.12225‚Äì12235,2020.
andM.Seale,‚ÄúExplainableintrusiondetectionsystems(x-ids):Asurvey [24] T.Funke,M.Khosla,M.Rathee,andA.Anand,‚ÄúZorro:Valid,sparse,
ofcurrentmethods,challenges,andopportunities,‚ÄùIEEEAccess,vol.10, andstableexplanationsingraphneuralnetworks,‚ÄùIEEETransactionson
pp.112392‚Äì112415,2022. KnowledgeandDataEngineering,2022.
[2] K. He, D. D. Kim, and M. R. Asghar, ‚ÄúAdversarial machine learning [25] Z.Ying,D.Bourgeois,J.You,M.Zitnik,andJ.Leskovec,‚ÄúGnnexplainer:
fornetworkintrusiondetectionsystems:Acomprehensivesurvey,‚ÄùIEEE Generatingexplanationsforgraphneuralnetworks,‚ÄùAdvancesinneural
CommunicationsSurveys&Tutorials,vol.25,no.1,pp.538‚Äì566,2023. informationprocessingsystems,vol.32,2019.
[3] E.AkandB.Canberk,‚ÄúFsc:Two-scaleai-drivenfairsensitivitycontrol [26] H. Yuan, H. Yu, S. Gui, and S. Ji, ‚ÄúExplainability in graph neural
for 802.11ax networks,‚Äù in GLOBECOM 2020 - 2020 IEEE Global networks: A taxonomic survey,‚Äù IEEE transactions on pattern analysis
CommunicationsConference,2020,pp.1‚Äì6. andmachineintelligence,vol.45,no.5,pp.5782‚Äì5799,2022.