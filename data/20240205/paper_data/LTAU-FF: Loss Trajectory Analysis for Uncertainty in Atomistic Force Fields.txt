LTAU–FF: LOSS TRAJECTORY ANALYSIS FOR UNCERTAINTY
IN ATOMISTIC FORCE FIELDS
JoshuaA.Vita∗ AmitSamanta
LawrenceLivermoreNationalLaboratory LawrenceLivermoreNationalLaboratory
Livermore,California94550,USA Livermore,California94550,USA
FeiZhou VincenzoLordi
LawrenceLivermoreNationalLaboratory LawrenceLivermoreNationalLaboratory
Livermore,California94550,USA Livermore,California94550,USA
ABSTRACT
Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep
learning atomistic force fields. Despite this, widespread adoption of ensemble-based uncertainty
quantification (UQ) techniques is limited by the high computational costs incurred by ensembles
during bothtraining and inference. In thiswork we leverage thecumulative distribution functions
(CDFs)ofper-sampleerrorsobtainedoverthecourseoftrainingtoefficientlyrepresentthemodel
ensemble, and couple them with a distance-based similarity search in the model latent space. Us-
ing these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths
of ensemble-based techniques without requiring the evaluation of multiple models during either
training or inference. As an initial test, we apply our method towards estimating the epistemic
uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to
accuratelypredicttesterrorsonmultipledatasetsfromtheliterature. Wethenillustratetheutilityof
LTAU-FFintwopracticalapplications: 1)tuningthetraining–validationgapforanexampledataset,
and 2) predicting errors in relaxation trajectories on the OC20 IS2RS task. Though in this work
wefocusontheuseofLTAUwithdeeplearningatomisticforcefields,weemphasizethatitcanbe
readilyappliedtoanyregressiontask, oranyensemble-generationtechnique, toprovideareliable
andeasy-to-implementUQmetric.
1 Introduction
In computational chemistry and materials science, deep learning force fields have become a standard tool for accel-
erating quantum mechanical calculations [DCC19, MHW20, Mis21]. The goal of these force fields is to perform a
regressiontasktoreproducethepotentialenergysurfaceofasysteminacomputationallyefficientway.Thisisdoneby
learningtopredicttheenergiesandatomicforcesfromahigh-accuracygroundtruth,suchasdensityfunctionaltheory
(DFT)calculationsorsimilarmethods.Anumberofdifferentapproachestothisproblemhavebeendevelopedoverthe
years,includingtheuseofGaussianprocesses[BPKC10,CBFAvL20],symbolicregression[HBY+19],feed-forward
neural networks [BP07, MC20], and message-passing neural networks [BMS+22, BKS+22], among others. How-
ever,withthetransitionawayfrommoreinterpretable,classicalphysics-basedforcefields[Jon24,Bas92,vDDLG01],
questions surrounding the necessary complexity of model architectures, and the domains over which they are valid,
have become prevalent. Hence, developing UQ methods accounting for different sources of epistemic uncertainty,
thereforehelpingtoisolatetheeffectsofmodeluncertainty,isaprimaryconcernofforcefielddevelopers.
WhilemultipleUQtechniqueshavebeenappliedtoatomisticforcefields,includingdropoutneuralnetworks[WT20],
Bayesianframeworks[APK12,XVS+21],orGaussianmixturemodels[ZBMK23],byfarthemostpopularapproach
in practice is the use of ensemble-based methods, where the ensemble variance is taken as a measure of the pre-
∗Correspondenceto:vita1@llnl.gov
4202
beF
1
]GL.sc[
1v35800.2042:viXradictive uncertainty of the model. Particularly beneficial is the fact that these ensemble methods do not require any
modifications to the model architecture or training algorithm, and can therefore be readily applied to any existing
developmentworkflow. Dueinlargeparttothissimplicityandgeneralizability,ensemble-basedUQtechniqueshave
been widely applied towards numerous machine learning and deep learning tasks [LPB16, ZLP+22], and are often
considered to be the gold standard when evaluating the performance of new UQ methods for atomistic force fields
[BBJB+21,HSY+20,WSC21,CMCW+23,ZBMK23,WGC+23].
Despite their ubiquity, ensemble methods typically suffer from two main drawbacks: high computational costs (for
both training and inference) and difficulties maintaining ensemble diversity resulting in poor uncertainty estimates.
Inparticular, ensemblesizesofapproximately5to30modelshavebeenfoundtobenecessaryforvariouspractical
applications of UQ for force fields [GBM17, LGC+23], making ensemble-based methods potentially infeasible for
largetrainingsetsorexpensivemodels. Furthermore,althoughensemble-basedUQtechniqueshavestillbeenshown
to out-perform single-model methods [TUG+23], there is growing evidence that they result in overconfident UQ
measures [KZ22, LGC+23]. Similar issues have been documented in other deep learning applications, where the
overconfidenceisattributedtoalackofensemblediversity[FHL19,EMR+22,ZLP+22], whichinturnstemsfrom
themethodsusedtogeneratetheensemble. Intuitively,themoststraightforwardapproachestoensemblegeneration
(training multiple models with different initial weights, data splits, or training hyperparameters [PCK17]) will only
samplemodelsatdifferentlocalminimaofthelossfunction,andthereforefailtoaccountforthefulltopographyofthe
losssurface, whichhasbeenshowninmanycasestocorrelatewithgeneralizationerror[HS94,CCS+16,LXT+17,
GGG+21,VSK23].
In this work, we develop the new LTAU method, which is an ensemble-based technique that addresses the issues of
computational cost and ensemble diversity, and can be broadly applied to any force field or other regression task.
Specifically,weprovidethefollowingcontributions:
• Weoutlineourproposedmethod,LTAU,whichutilizesthecumulativedistributionfunction(CDF)oferrors
sampled during training to provide estimates of the likelihood that a model’s predictions will fall below a
chosen tolerance threshold. This use of the training error trajectories imposes no additional computational
overheadandinherentlyimprovesthediversityoftheensemblebyincorporatingadditionalinformationfrom
variouspointsonthelosslandscape.
• BycouplingtheCDFtoanearest-neighborsearchinthemodel’slatentspace,weshowhowLTAUcanprovide
acheap,easy-to-implementUQmeasurethatdoesnotrequireevaluatinganensembleduringinference,thus
eliminatingamajorbarriertotheapplicationofensemble-basedUQtechniqueswithlargemodels.
• WeapplyourmethodtowardsUQinatomisticforcefields(LTAU-FF)andshowthatitaccuratelyestimates
in-domain(ID)errorsandcanbereadilytunedtoimprovecalibrationonout-of-domain(OOD)predictions
foravarietyofbenchmarkingdatasets.
• Finally,todemonstratetheutilityofLTAU-FFinpracticalapplications,weconducttwoexperiments: 1)re-
weighting the training set of the model to prioritize high or low confidence samples, and 2) applying our
method to the IS2RS task from the OC20 challenge to predict errors in relaxation trajectories. From these
experiments, we observe that the UQ metric can be used to tune the training–validation performance gap
andcanalsoserveasareliableindicatorofmodelperformanceinreal-worldapplicationsandonlarge-scale
datasets.
2 Background
ThedefiningtraitoftheLTAUmethoddevelopedinthiswork,whichdistinguishesitfromotherensemble-basedUQ
methods,isitsabilitytoleverageinformationfromlargeensemblesofmodelswhileonlyhavingtoevaluateasingle
modelduringinference. Thisisachievedthroughefficientuseofloggederrortrajectoriessampledduringtrainingand
asimilaritysearchinthemodel’slatentspaceduringinference. Themethodbuildsheavilyupontwokeyconceptsin
deeplearning: trainingtrajectoryanalysisanddistance-basedUQ.
2.1 Trainingtrajectoryanalysis
Training trajectories have been used in a number of deep learning applications for the purpose of outlier detection
[SSL+20, PZEW20, ADH20, PGD21, SCQvdS23], data pruning [TSC+18, MBL19, KSRI20, KSR+21, SGS+22,
SCQvdS23], and influence estimation [Fel19, FZ20, GWP+23]. These approaches have leveraged the rich infor-
mation that is sampled over the course of training, including loss gradients [PLSK20, KSRI20, KSR+21, MBL19,
PGD21,MBL19,ADH20],Softmaxoutputs[SSL+20,RTH+22],orothercustommetrics[PZEW20,TSC+18,FZ20,
2SCQvdS23]. However,manyofthesemetricshavebeendesignedprimarilyforclassificationtasks(i.e.,theyarede-
rived using assumptions about the loss function or model architecture that are unique to classification) and require
modificationinordertobeappliedinregressionsettings.
Mostrelevanttothisworkistheconceptofconfidence[SSL+20],whichutilizestheSoftmaxoutputsobtainedduring
trainingtocharacterizethemodel’sconfidenceinagivenprediction.Specifically,themodelissaidtobe“confident”in
itspredictionforagivenpointifthepointhasahighprobabilityofbeingcorrectlypredicted(i.e.,highaverageSoftmax
activation)throughoutthecourseoftraining. Wechosetobuildonthisnotionofconfidencefortworeasons: first,itis
intuitivetounderstandinthecontextofUQ(i.e.,highconfidenceimplieslowuncertainty,andlowconfidenceimplies
highuncertainty);andsecond,itcanbeefficientlycomputedinpost-processing. AswillbediscussedinSection3.4,
inthisworkweusetheevaluationofasample’sCDFoferrorsatachosentolerancethresholdasthemeasureofthe
model’sconfidenceonthatsample.
2.2 Distance-basedUQ
Acriticalassumptionofthemethoddevelopedinthisworkisthatpointswhichareclosetoeachotherinthemodel’s
latent space also have similar CDFs, which is a concept that is discussed further in Appendix D. On a smoothly-
varyingmanifold,proximityamongpointsinthelatentorinputspaceisoftenusedtoinferormodelfunctionvalues.
Thisnotionhasbeensuccessfullyappliedtodeveloptoolslikeradialbasisfunctions,Gaussianprocessregression,and
manyothers.Similarly,thisconcepthasbeenappliedtoUQ,whereasimplemethodforestimatingtheuncertaintyina
model’spredictionforatestpointistousetheweightedEuclideandistance(ineithertheinputspaceoralatentspace)
betweenthetestpointandthenearesttrainingpoint[LW18,APH+21,SGS+22]. Therationalebehinddistance-based
UQ metrics is that a model is less likely to have accurate predictions on points which are further from the training
set. WhilerelatedmethodshavebeenusedforUQwithatomisticforcefields[JDY+19,Per22,ZBMK23],adistance
metric alone is insufficient for constructing a well-calibrated UQ metric, and usually requires additional calibration
techniquesormakingassumptionsregardingthedistributionoferrors[HMUM22]. Inouranalysis, weobservethat
LTAU-FFcanbecalibratedusingasimplescalingfactor,whichwillbediscussedfurtherinSection4.1.
3 Methods
3.1 Model
WeusetheNequIPmodel[BMS+22]asthedeeplearningforcefieldinthiswork. NequIPisagraph-basedmessage-
passingneuralnetworkthatusessphericalharmonicstorepresentlocalatomicenvironmentswithequivariance. The
use of message-passing and equivariant features, respectively, have been shown to be useful for incorporating long-
rangeinteractionsinthemodelandavoidingissuescausedbytheinabilityofinvariantmodelstodistinguishbetween
symmetrically-equivalent atomic environments [BBK+22]. NequIP has achieved state-of-the-art performance for a
number of applications [SUG21, RNE+22]. The hyperparameters of the model are adjusted for each dataset, with
detailsprovidedinAppendixA.
3.2 Datasets
Three training datasets are used in this work: 3BPA [KOK+21], Carbon GAP 20 [RDG+20], and the 200k split of
the S2EF task from the OC20 challenge [CDG+21b]. These datasets were chosen deliberately to provide systematic
tests of the limitations of our method with increasingly complex data. We start with a simple molecular test case
for refining our methods (3BPA), then expand to a more diverse dataset typical of solid-state force field fitting tasks
(Carbon GAP 20), and finally test against a challenging real-world application (IS2RS task from OC20). Detailed
descriptionsofthedatasetsarefoundinAppendixBandthereferencedcitations. Alldatasetsareavailablefromtheir
originalsources.
3.3 Trainingdetails
Forthemostpart,standardpublishedhyperparameterswereusedfortraining,withdetailsprovidedinAppendixA.As
anotableexception,forthe3BPAandCarbon GAP 20datasets,wetrainedmodelsonlytoforces,whichisincontrast
totypicalforcefieldfittingworkflowsthatusuallyalsoincludeanenergyterminthelossfunction.Wechosetoweight
theenergycontributionsto0.0inthesecasestoremoveanypossiblespuriouscorrelationscausedbycouplingbetween
theenergyandforcetermsinthelossfunction,whichmayhaveinterferedwiththeUQanalysis. FortheOC20dataset,
weweightedtheenergytermto1.0tohelpavoidunexpectedbehaviorduringenergyminimization;wenotethatthis
3still resulted in strong correlation between the UQ metric provided by LTAU-FF and the accuracy of the predicted
ground-statestructure,asshowninSection4.3.
3.4 LTAU–FF
As the primary contribution of this paper, we propose the Loss Trajectory Analysis for Uncertainty (LTAU) method
(Algorithm1andAlgorithm2),anddemonstrateitsapplicationtoatomisticforcefields(LTAU-FF:LTAUforatomistic
Force Fields). The LTAU method builds upon the concepts described in Section 1, using the errors of the ensemble
ofmodelssampledduringtrainingtoapproximatetheCDF,thencouplingtheapproximatedCDFtoadistance-based
similarity search for estimating uncertainty on test predictions. Implementing LTAU in an existing workflow will
typicallyonlyrequirepatchingexistingtrainingcodetologper-sampleerrorsateveryepochandaddingsomebasic
post-processing. We provide post-processing code and other utility functions for working with the specific models
presentedinthiswork(seeSection6).
CentraltotheLTAUtechniqueisthenotionofsampleconfidence, asdescribedinSection2.1. In[SSL+20], sample
confidenceisdefinedasthemeanprobabilityofthecorrectclasslabelbeingpredictedacrosstrainingepochsandis
computedusingtheoutputsofthefinalSoftmaxlayer. SinceregressiontasksdonotnormallyuseaSoftmaxoutput
layerandthereisnobinarynotionof“correctness,”weinsteadchoosetousetheCDFoferrorsevaluatedatachosen
tolerancethreshold,atol,asthemeasureofconfidence. Inthissense,asamplewithhighconfidenceisonewhichhas
ahighprobabilityofhavingerrorsbelowthetolerancethresholdoverthecourseoftraining.
Algorithm1:Computeper-sampleuncertaintiesfortrainingpoints
Input :Anuntrainedmodel,F;atrainingsetS ofsizeN;thenumberoftrainingepochs,E
Output:Theuncertaintyofeachtrainingpoint,p={p ,...,p }
1 N
1 fore=1...E do
2 Train(F)
3 Log(Error(F,S),e)
4 end
5 atol=MAE(F,S)
// load ExN matrix of trajectories
6 T ←LoadErrorTrajectories()
// return 1xN probabilities array
7 p=T ≤atol
8 return1− E1 (cid:80)E e pe
ForLTAU-FF,thefirststeptocomputingthesampleconfidenceinthismanneristotrainamodelandlogtheerrors
at every epoch (Algorithm 1). By logging the model’s errors on every sample (i.e., every atom) at every epoch, we
can construct error trajectories T = {ϵ1,··· ,ϵE} for every atom i, where ϵt = |F −Fˆt| is the L norm of
i i i i i,DFT i 2 2
the force error vector for atom i at the end of epoch t, and E is the total number of training epochs. We will then
define the model’s confidence in its prediction of the forces for atom i as p = P(ϵt ∈ T ≤ atol), where atol is
i i i
achosenabsoluteerrortolerancethreshold. ThiscorrespondstotheCDFoftheerrorsevaluatedatatol,andcanbe
easily computed in post-processing after logging the per-sample errors at each epoch. A reasonable choice for atol
wouldbethemeanabsoluteerror(MAE)oftheoptimalmodel, sincep couldthenbeinterpretedastheconfidence
i
thatthemodel’spredictionsonatomiwillbebelowtheMAE.Alternatively,onecouldtuneatolinordertomaximize
the “dispersion” [TNY+20] of the UQ metric. This method can be easily extended to incorporate relative errors by
defining p = P(ϵt ∈ T ≤ atol + rtol × |F | ), where rtol is a relative error threshold; however, for the
i i i i,DFT 2
sake of simplicity we will only use rtol = 0. We note that instead of using the L norm for computing ϵ , each
2 i
Cartesiancoordinatecouldbetreatedindividually,yieldingϵ (withcindexingtheCartesiandirection). Thiswould
ic
allowuncertaintiestobeobtainedalongeachaxis,butwouldincurhighermemorycostswhenloggingtheper-sample
errors. Finally, alternative error metrics can also be used instead of the MAE – for example, the mass-normalized
errorsexploredin[WWS+23]wouldbeparticularlyusefulformulti-elementsystems.
Inordertousethep valuescomputedonthetrainingsetforpredictinguncertaintiesonatestpoint, j, wecompute
i
p = 1 (cid:80) p ,theaverageofthep valuesforthenearestksamples,N ,fromthetrainingset(Algorithm2). The
j k i∈Nj i i j
knearestneighborsareobtainedbyperformingasimilaritysearchinthelatentspaceofthemodel;specifically,using
the output of the last message-passing layer of the NequIP model (prior to the linear readout layers) as the atomic
descriptor. This means that we must evaluate the model for the full training set, store the latent descriptors for all
trainingatoms,thencomparethedescriptorsforeachtestatom,j. Thecostofcomputingp thennecessarilyscales
j
4Algorithm2:Computeuncertaintyonatestpoint
Input :Atrainedmodel,F;atrainingsetS;thetraininguncertainties,p=;atestpoint,x ;thenumberof
j
nearestneighborstosearchfor,k
Output:Theuncertaintyofthepredictiononthetestpoint,p
j
1 D ←Descriptors(F,S)
2 d j ←Descriptor(F,x j)
3 N j ←NearestNeighbors(d j,D,k)
4 return k1 (cid:80) i∈Njp i
withthesizeofthetrainingset. SincetypicaltrainingsetsforforcefieldscanincludeO(104)-O(107)atoms,werely
upontheFAISS[JDJ19]packageforefficientlysearchingfortheknearestneighborsoftestpoints. Additionaldetails
regardingtheFAISShyperparametersusedinthisworkcanbefoundinAppendixA.
In developing the LTAU method, we found that computing p using an ensemble of M models was often valuable
i
in order to obtain better-converged values, especially when working with datasets for which noisier p values were
i
observed. Althoughthep valuescanalreadybethoughtofasbeingcomputedoveranensembleofmodels(sampled
i
duringtraining),additionallyaveragingthemoverasecondensemble(sampledfromdifferenttrainingruns)helpsto
more thoroughly sample the loss landscape of the model, thus resulting in a metric which can be expected to better
reflect the true behavior of the model [FHL19]. In this work, we used ensembles of M = 1, 10, or 20 models for
computing p values for the OC20, 3BPA, or Carbon GAP 20 datasets, respectively, which we observed resulted in
i
reasonable convergence of the metric (see Fig. C3). Throughout the remainder of this paper, we will explicitly use
thenotationµ (·)orσ (·)anytimeavalueisbeingcomputedasanensembleaverage(orstandarddeviation). We
M M
emphasizethatwhiletheuseofanensembleincreasesthetrainingcost,itdoesnot affectthecostofinferencesince
LTAU only relies upon the µ (p ) values (which are computed once during post-processing, then cached) and the
M i
latent descriptors extracted from the single model being used for inference. Furthermore, the use of M = 1 for the
OC20 dataset shows that the method is still viable even in scenarios where it is impractical or not possible to train
multiplemodels. AdditionalanalysisoftheeffectsofM andtrainingdurationonµ (p )andσ (p )canbefound
M i M i
inAppendixC.
The LTAU method outlined above is closely related to the notion of “checkpoint ensembles” [CLL17], which have
beenusedpreviouslyasanefficientalternativetotraditionalensemblesamplingtechniques. Wenote, however, that
checkpointensemblesstillrequireevaluatingallmodelsintheensembleduringinference, andthereforesufferfrom
the same limitations as traditional ensemble UQ methods due to their computational costs. In contrast, LTAU uses
a neighbor lookup along with the pre-computed p values, which is significantly less expensive than even a single
i
model forward pass (see Section 4.4 for details on computational cost). Furthermore, it is worth highlighting that
althoughinthisworkweusetheensembleofmodelsobtainedduringtraining,LTAUcanbecoupledwithanyexisting
ensemble-generation technique by instead computing the per-sample error CDFs over the desired ensemble. This
flexibilitymeansthattheensembleusedforLTAUcanbeadjustedtobettercapturethedesiredsourceofuncertainty
(e.g.,parametricvs. modeluncertainty).
4 ResultsandDiscussion
4.1 CalibratingLTAU–FF
AsaninitialtestoftheaccuracyofLTAU-FFinestimatingerrorsonforcepredictions,wetrainensemblesofmodelsto
the3BPAandCarbon GAP 20datasets,thenqualitativelycomparethecalibrationcurvesofthemodelonsubsetsofthe
testdata,asisdonein[TNY+20].WegeneratecalibrationcurvesshowninFig.1andFig.2byfirstbinningtestpoints
basedontheirp values,thenplottingtheobservedpercentageofpointsineachbinwitherrorsbelowthethreshold.
j
Sincethep valuesaredirectlyinterpretableasthepredictedlikelihoodoffallingbelowthethreshold,wecaneasily
j
seeiftheyarewell-calibratedbasedonhowclosetheresultantcurvesaretotheidealx = yline(shownwiththered
dashedline). Ingeneral,weobservedthatthecalibrationtoin-domaindatacanbeimprovedbytuningthenumberof
neighbors,k,overwhichp valueswerecomputed,whilecalibrationtoout-of-domaindatarequiredre-scalingofthe
j
CDFobtainedfromthetrainingsetinordertobettermatchthehighererrorsobservedontheout-of-domaindata.
As discussed in [TNY+20], good calibration alone is not a sufficient condition for a suitable UQ metric, since a
metric could be well-calibrated while having extremely large uncertainty measures. It is also desirable that a UQ
metricbe“sharp,”meaningthattheuncertaintymeasuresbesmallinordertoprovidetighterconfidenceintervalsover
the model predictions. In the case of LTAU, the sharpness of the metric can be meaningfully adjusted by tuning the
5Figure1: Calibrationcurvesforthe3BPAtestsets. Testsetsforeachtemperatureareplottedasdifferentcolors. Panel
a averages p values over k = 1 neighbors, while panels b and c use values of k = 2. An atol value of 0.0075
i
eV/A˚ wasusedforallcurvesinthisfigure. Fortheun-calibratedcurves(panelsaandb),thescalingfactor,s,isset
to1foralltemperatures. Forthecalibratedcurves(panelc),shasbeenmanuallyadjustedto2and4forthe600Kand
1200Ktestsets,respectively.
thresholdvalueatol.Fig.C1showsthedistributionsofp foralldatasetsexploredinthisworktoguideunderstandings
i
ofhowchangingatolcanbeusedtoadjustthesharpnessofthemetric.
3BPA
InFig.1,weobservethatthesimplestapproach(k =1,withnoadditionalcalibration)yieldsreasonableresultsonly
forthe300K3BPAtestset. Thisisnotsurprisinggiventhatthetrainingsetwasalsosampledat300K,andtherefore
the300Ktestsetislikelystillpredominantlyin-domain. Increasingkto2inFig.1b,inordertohelpaccountforany
noiseintheµ (p )valueswithinthelocalneighborhoodinthelatentspace, greatlyimprovesthecalibrationtothe
M i
300K test data. However, in Fig. 1a and Fig. 1b, the p values under-predict the likelihood that the test points will
j
haveerrorsbelowatolforthe600K/1200Ktestsets. Weattributethismis-calibrationtothefactthattheMAEsforthe
600K/1200Ktestsetsarehigherthanthoseonthe300Ktrain/testsets(0.025and0.064eV/A˚ forthe600K/1200Ktest
sets,respectively,asopposedto0.011eV/A˚ forthe300Ktestset). Inordertoaccountforthelargeraverageerrors
on the higher temperature test sets, in Fig. 1c we introduce a scaling factor, s, and instead calculate the probability
within each bin that the errors fall below atol ×s. The use of a scaling factor equates to the assumption that the
high-temperatureCDFshavethesamedistributionsasthelow-temperatureCDF,butoverawiderrangeoferrors,as
isdiscussedfurtherinAppendixD.WeobserveinFig.1cthatthescalingfactordrasticallyimprovesthecalibration
oftheUQmetric,bringingitinlinewiththeperformanceonthe300Kset.
Carbon GAP 20
TheapplicationofLTAU-FFtotheCarbon GAP 20dataset,showninFig.2,helpstohighlightsomeofthechallenges
associatedwithcalibratingtheUQmetrictodissimilarsubsetsofthedataset. InFig.2aitcanbeseenthatalthough
themetriciswell-calibratedtothetotaltestset(showninblack),thecalibrationonindividualgroupsislessaccurate.
Similartotheresultsshownforthe3BPAdataset,calibrationisimprovedbyincreasingk(notethatahighervalueof
10isusedhere),andascalingfactorishelpfulforsomeofthegroups(Amorphous BulkandAmorphous Surfaces).
However,therearestillsomenotablesubsetsforwhichtheUQmetricstillunderestimates(Liquid)oroverestimates
(Graphene)theobservederrordistributionsregardlessofthescalingfactor,whichisdiscussedfurtherinAppendixD.
4.2 Tuningthetraining–validationgap
Oneoftheprimaryusesofsampledifficultymetricsintheliteratureisfordatasetconstructionandpruning[TSC+18,
SSL+20, PGD21], where difficult samples are hypothesized to be essential for fitting to datasets with long-tailed
distributions[Fel19,FZ20]. In[SGS+22],theroleof“easy”and“hard”sampleswasexploredfurther,demonstrating
thatinmanyimageclassificationtasksitwaspossibletoprunesignificantportionsofthetrainingsetwithouthurting
testaccuracybyonlyremovingthesampleswhichwerecharacterizedasbeingeasyaccordingtovariousmetrics. The
6Figure2: CalibrationcurvesfortheCarbon GAP 20testset. Eachconfig typegroupisplottedasadifferentcolor,
and the total test set is plotted in black. Panel a uses a value of k = 1, while panels b and c use values of k = 10.
Anatolvalueof0.265eV/A˚ wasusedforallcurvesinthisfigure. Fortheun-calibratedcurves(panelsaandb),the
scalingfactor,s,issetto1forallgroups. Forthecalibratedcurve(panelc),s,hasbeenmanuallyadjustedto1.3for
theAmorphous BulkandAmorphous Surfacesgroups. WenotethattheLiquidandGraphenegroupswereleft
un-calibratedbecausewewereunabletoidentifyasuitablesvalue.
rationalizationgivenin[SGS+22]forthisbehaviorwasthateasysamplesprovidecoarse-grainedinformationabout
thetargetfunction,whilehardsamplesprovidefine-grainedinformation.
Iftheseexplanationshold,thenitisreasonabletoexpectthatsamplesfromthisworkwithhighµ (p )values(i.e.,
M i
low difficulty) may be useful for constructing more generalizable models, while samples with low µ (p ) values
M i
(high difficulty) will be useful for improving the quality of fit to data sampled from distributions that are similar to
thetrainingset. Inordertotestiftheµ (p )valuescomputedinthisworkcouldbeusedtotunethegeneralizability
M i
ofamodel,wefirsttrainedanensembleofM = 20modelswithuniformweights(inthelossfunction)forallatoms
in the Carbon GAP 20 training set, then used the observed µ (p ) values to re-weight the training set and train a
M i
newensemblefromscratch. Followingtheapproachtakenin[SSL+20], weconsideredboththesampleconfidence
µ (p ),aswellasthe“variability”,σ (p ),whenre-weightingthetrainingpoints. Additionaldetailsregardingthe
M i M i
re-weightingschemesareprovidedinAppendixE.
AscanbeseeninFig.3,increasingtheweightsofhardsamples(“up-weighthard”)resultedinclearsignsofoverfitting,
wherethetrainingMAEwaslowerthanthe“uniform”weightingcase,butattheexpenseofhighervalidationerrors.
Incontrast, increasingtheweightsofeasysamples(“up-weighteasy”)helpedtoreducethetraining–validationgap,
possibly indicating improved generalizability of the model. While the training–validation gap may have increased
furtherfortheup-weighteasyschemewithadditionaltraining,up-weightingeasysamplesappearstobeareasonable
approach to minimizing the performance gap within a fixed computational budget, although further analysis of this
effect is warranted. We further confirmed the performance gaps by computing the ensemble test errors for each
weightingscheme,asshowninFig.E2. Thoughthetesterrorsfollowedasimilartrendtothatseenforthevalidation
errorsfromFig.3,therewasasmallnumberofanomalouslyhigherrorsoriginatingpredominantlyfromtheLiquid
subsetofthetestdata,whichwebelieveshouldbeinvestigatedmanuallytoconfirmconvergenceandcorrectnessof
theDFT-predictedtargetvalues.
4.3 LeveragingtheUQmetricduringsimulations
ThefinaltestofLTAU-FFthatweperformedwastotrainamodeltothe200ksplitoftheOC20 S2EFtask,thentosee
ifLTAU-FFcouldpredictmodelperformanceontheIS2RStask. IntheIS2RStask,amodelisgivenaninitialatomic
configurationandisaskedtopredicttherelaxedstate,whichisabroadly-applicableandextremelyvaluabletaskfor
materials discovery. While some models use the “direct” approach of predicting the relaxed structure directly from
the input configuration, an alternative approach (shown to achieve better performance in [CDG+21a]) is to perform
relaxationviaenergyminimization,whichiswhatwedidinthiswork.
7Figure 3: Training (solid lines) and validation (dashed lines) curves for ensembles of M = 20 models trained to
the Carbon GAP 20 dataset with different weighting schemes. A random 90:10 training:validation split was used,
where all runs used the exact same split. Shaded bands denote the 95% confidence intervals computed across the
M = 20runsforeachweightingscheme. Increasingtheweightonsamples(asdescribedinAppendixE)withlow
µ (p ) values (“up-weight hard”) increased the degree of overfitting, while increasing the weight on samples with
M i
high µ (p ) (“up-weight easy”) decreased overfitting. The “up-weight easy” models were trained for about 25%
M i
longertoimproveconvergenceofthetraining–validationgap;evenlongerrunsmayhavewidenedthegapfurther,but
wouldhaverequiredanunreasonablyhighcomputationalbudget.
Figure 4: RMSDs between atoms of DFT-relaxed and model-relaxed samples, binned by predicted uncertainty, for
OC20. Panels correspond to different choices of snapshot(s) along the relaxation trajectory to use for predicting the
finalRMSD.Thesplitsidentifiedby[CDG+21b]asbeingin-domain(val id)orout-of-domain(val ood both)are
showninblueororange, respectively. Onlythe“surface”and“adsorbate”atomsareconsideredinthisfigure, since
the“bulk”atomsareheldfixedduringenergyminimizationfollowingthepracticesoutlinedintheOC20 IS2RStask.
Distancesaretakenastheaveragewithineachbin,anderrorbarscorrespondtothestandarderrorforeachbin. Note
that[CDG+21b]usesamaximumRMSDvalueof0.5A˚ whencomputingthe“AverageDistancewithinThreshold”
(ADwT)metricontheIS2RStask.
A particularly challenging aspect of predicting errors from a relaxation trajectory is that relatively small errors at
anypointinthetrajectorymaydrasticallyalterthepredictedrelaxedstructurebycausingthemodeltorelaxtowards
an incorrect local energy minimum. Because of this difficulty, in Fig. 4 we consider four possible UQ metrics for
predictingtheroot-mean-squaredeviation(RMSD)betweenthetrueandpredictedrelaxedstructures:theinitial,final,
maximum, andaverageUQvalue. WeobservefromFig.4thattheinitialandmaximumuncertaintycomputedover
thefulltrajectorybothshowstrongcorrelationwiththefinalRMSD,whichmatchesourexpectationsthaterrorsina
singlestepofthetrajectorymaydisruptthefinalresult. Ontheotherhand,neitherthefinalnortheaverageUQvalue
appeartobeagoodindicatorofthefinalRMSDinthiscase. Furtherworkisneededtounderstandhowper-atomUQ
metricscanbepropagatedthroughatrajectoryinordertoprovideuncertaintypredictionsondynamicalproperties.
4.4 Computationalcost
TobetterunderstandthecomputationalcostofLTAU-FFrelativetootheraspectsofmodelevaluation,weranprofiling
tests computing the atomic forces and uncertainties for 5000 randomly sampled atomic configurations from each
test set (500 for 3BPA). These results were obtained using the CPU implementation of FAISS and a TorchScript-
compiled NequIP model running on a single NVIDIA V100 GPU. The cost breakdown shown in Table A2 reveals
8thatcomputingthep valuesoccupiedlessthan5%ofthewalltime,comparedto20-50%forgraphconstructionand
j
40-70%forthemodelforwardpass.
5 Conclusion
Inthiswork,wedevelopedLTAU,anovelUQmethodbasedontheper-sampleCDFsobtainedduringtraininganda
distance-based similarity search, and demonstrated its utility in the field of atomistic simulations. We outlined how
LTAU-FF,whichavoidsmanyofthedrawbacksofpreviousUQmethodsforatomisticforcefields,canbereadilyap-
pliedtoanymodelwhileintroducingonlynegligiblecomputationaloverheadandrequiringonlyminormodifications
totrainingsoftware. Furthermore,weshowthattheUQmetricproducedbyLTAU-FFcanbecalibratedusingasimple
scalingfactor,andcanbeusedasareliabletoolforre-weightingtrainingsetsandpredictingmodelfailureduringsim-
ulations.FutureworkapplyingLTAUtootherdeeplearningtasks,orcouplingLTAU-FFwithmoreadvancedclustering
methodsandextendingitwitherrorpropagationtechniqueswouldbeparticularlywelcomecontributions. Addition-
ally,workexploringmodificationstoLTAUthatmayfurtherimprovethediversityoftheensemble(e.g.,alternativeloss
functionsortrainingregimen),oranalysisquantifyingtheeffectsofensemblediversitywouldalsobeuseful. Moving
forward,wehopethatthisworkwillserveasavaluabletoolfordevelopingmachine-learnedatomisticforcefieldsby
aidinginthedesignofactivelearningworkflows,failureanalysismethods,anddatapruningtechniques.
6 Dataandcodeavailaiblity
Alltrainingsetscanbefoundattheiroriginalsources. TheLTAU-FFcodeandallmodelsdevelopedinthisworkwill
beprovidedathttps://github.com/LLNL/ltau-ffuponcompletionofinternalreviewofthesoftware.
7 Authorcontributions
Joshua A. Vita: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data Cura-
tion, Writing -Original Draft, Writing- Review& Editing, Visualization. AmitSamanta: Methodology, Writing -
Review&Editing,SupervisionFeiZhou: Methodology,Writing-Review&Editing,SupervisionVincenzoLordi:
Methodology,Resources,Writing-Review&Editing,Supervision,Projectadministration,Fundingacquisition.
8 Acknowledgments
This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National
Laboratory under Contract DE-AC52-07NA27344, funded by the Laboratory Directed Research and Development
ProgramatLLNLunderprojecttrackingcode23-SI-006.
References
[ADH20] Chirag Agarwal, Daniel D’souza, and Sara Hooker. Estimating example difficulty using variance of
gradients,2020.
[APH+21] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad
Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir
Makarenkov, and Saeid Nahavandi. A review of uncertainty quantification in deep learning: Tech-
niques,applicationsandchallenges. InformationFusion,76:243–297,December2021.
[APK12] Panagiotis Angelikopoulos, Costas Papadimitriou, and Petros Koumoutsakos. Bayesian uncertainty
quantification and propagation in molecular dynamics simulations: A high performance computing
framework. TheJournalofChemicalPhysics,137(14),October2012.
[Bas92] M.I.Baskes. Modifiedembedded-atompotentialsforcubicmaterialsandimpurities. PhysicalReview
B,46(5):2727–2742,August1992.
[BBJB+21] JonasBusk,PeterBjørnJørgensen,ArghyaBhowmik,MikkelNSchmidt,OleWinther,andTejsVegge.
Calibrated uncertainty for molecular property prediction using ensembles of message passing neural
networks. MachineLearning: ScienceandTechnology,3(1):015012,December2021.
[BBK+22] IlyesBatatia,SimonBatzner,Da´vidPe´terKova´cs,AlbertMusaelian,GregorN.C.Simm,RalfDrautz,
Christoph Ortner, Boris Kozinsky, and Ga´bor Csa´nyi. The design space of e(3)-equivariant atom-
centeredinteratomicpotentials,2022.
9[BKS+22] Ilyes Batatia, Da´vid Pe´ter Kova´cs, Gregor N. C. Simm, Christoph Ortner, and Ga´bor Csa´nyi. Mace:
Higherorderequivariantmessagepassingneuralnetworksforfastandaccurateforcefields,2022.
[BMS+22] SimonBatzner,AlbertMusaelian,LixinSun,MarioGeiger,JonathanP.Mailoa,MordechaiKornbluth,
NicolaMolinari,TessE.Smidt,andBorisKozinsky. E(3)-equivariantgraphneuralnetworksfordata-
efficientandaccurateinteratomicpotentials. NatureCommunications,13(1),May2022.
[BP07] Jo¨rg Behler and Michele Parrinello. Generalized neural-network representation of high-dimensional
potential-energysurfaces. PhysicalReviewLetters,98(14),April2007.
[BPKC10] AlbertP.Barto´k,MikeC.Payne,RisiKondor,andGa´borCsa´nyi. Gaussianapproximationpotentials:
The accuracy of quantum mechanics, without the electrons. Physical Review Letters, 104(13), April
2010.
[CBFAvL20] AndersS.Christensen,LarsA.Bratholm,FelixA.Faber,andO.AnatolevonLilienfeld.Fchlrevisited:
Fasterandmoreaccuratequantummachinelearning.TheJournalofChemicalPhysics,152(4),January
2020.
[CCS+16] PratikChaudhari,AnnaChoromanska,StefanoSoatto,YannLeCun,CarloBaldassi,ChristianBorgs,
JenniferChayes,LeventSagun,andRiccardoZecchina. Entropy-SGD:BiasingGradientDescentInto
WideValleys. arXiv:1611.01838,2016.
[CDG+21a] LowikChanussot,AbhishekDas,SiddharthGoyal,ThibautLavril,MuhammedShuaibi,MorganeRiv-
iere,KevinTran,JavierHeras-Domingo,CalebHo,WeihuaHu,AiniPalizhati,AnuroopSriram,Bran-
donWood,JunwoongYoon,DeviParikh,C.LawrenceZitnick,andZacharyUlissi. Correctionto“the
open catalyst 2020 (oc20) dataset and community challenges”. ACS Catalysis, 11(21):13062–13065,
October2021.
[CDG+21b] LowikChanussot,AbhishekDas,SiddharthGoyal,ThibautLavril,MuhammedShuaibi,MorganeRiv-
iere,KevinTran,JavierHeras-Domingo,CalebHo,WeihuaHu,AiniPalizhati,AnuroopSriram,Bran-
donWood,JunwoongYoon,DeviParikh,C.LawrenceZitnick,andZacharyUlissi.Opencatalyst2020
(oc20)datasetandcommunitychallenges. ACSCatalysis,11(10):6059–6072,May2021.
[CLL17] HughChen,ScottLundberg,andSu-InLee. Checkpointensembles: Ensemblemethodsfromasingle
trainingprocess,2017.
[CMCW+23] Jesu´s Carrete, Hadria´n Montes-Campos, Ralf Wanzenbo¨ck, Esther Heid, and Georg K. H. Madsen.
Deepensemblesvscommitteesforuncertaintyestimationinneural-networkforcefields: Comparison
andapplicationtoactivelearning. TheJournalofChemicalPhysics,158(20),May2023.
[DCC19] Volker L. Deringer, Miguel A. Caro, and Ga´bor Csa´nyi. Machine learning interatomic potentials as
emergingtoolsformaterialsscience. AdvancedMaterials,31(46),September2019.
[EMR+22] RomainEgele,RomitMaulik,KrishnanRaghavan,BethanyLusch,IsabelleGuyon,andPrasannaBal-
aprakash. Autodeuq: Automateddeepensemblewithuncertaintyquantification. In202226thInterna-
tionalConferenceonPatternRecognition(ICPR).IEEE,August2022.
[Fel19] VitalyFeldman. Doeslearningrequirememorization? ashorttaleaboutalongtail,2019.
[FHL19] StanislavFort,HuiyiHu,andBalajiLakshminarayanan.Deepensembles:Alosslandscapeperspective,
2019.
[FZ20] VitalyFeldmanandChiyuanZhang. Whatneuralnetworksmemorizeandwhy: Discoveringthelong
tailviainfluenceestimation,2020.
[GBM17] Michael Gastegger, Jo¨rg Behler, and Philipp Marquetand. Machine learning molecular dynamics for
thesimulationofinfraredspectra. ChemicalScience,8(10):6924–6935,2017.
[GGG+21] Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur, David Car-
doze,GeorgeDahl,ZacharyNado,andOrhanFirat. Alosscurvatureperspectiveontraininginstability
indeeplearning,2021.
[GWP+23] KelvinGuu,AlbertWebson,ElliePavlick,LucasDixon,IanTenney,andTolgaBolukbasi.Simfluence:
Modelingtheinfluenceofindividualtrainingexamplesbysimulatingtrainingruns,2023.
[HBY+19] Alberto Hernandez, Adarsh Balasubramanian, Fenglin Yuan, Simon A. M. Mason, and Tim Mueller.
Fast,accurate,andtransferablemany-bodyinteratomicpotentialsbysymbolicregression. npjCompu-
tationalMaterials,5(1),November2019.
[HMUM22] Yuge Hu, Joseph Musielewicz, Zachary W Ulissi, and Andrew J Medford. Robust and scalable un-
certainty estimation with conformal prediction for machine-learned interatomic potentials. Machine
Learning: ScienceandTechnology,3(4):045028,December2022.
10[HS94] Sepp Hochreiter and Ju¨rgen Schmidhuber. Simplifying neural nets by discovering flat minima. Ad-
vancesinNeuralInformationProcessingSystems,7,1994.
[HSY+20] LiorHirschfeld,KyleSwanson,KevinYang,ReginaBarzilay,andConnorW.Coley.Uncertaintyquan-
tification using neural networks for molecular property prediction. Journal of Chemical Information
andModeling,60(8):3770–3780,July2020.
[JDJ19] Jeff Johnson, Matthijs Douze, and Herve´ Je´gou. Billion-scale similarity search with GPUs. IEEE
TransactionsonBigData,7(3):535–547,2019.
[JDY+19] Jon Paul Janet, Chenru Duan, Tzuhsiung Yang, Aditya Nandy, and Heather J. Kulik. A quantita-
tiveuncertaintymetriccontrolserrorinneuralnetwork-drivenchemicaldiscovery. ChemicalScience,
10(34):7913–7922,2019.
[Jon24] J. E. Jones. On the determination of molecular fields. —II. from the equation of state of a gas. Pro-
ceedingsoftheRoyalSocietyofLondon.SeriesA,ContainingPapersofaMathematicalandPhysical
Character,106(738):463–477,October1924.
[KOK+21] Da´vidPe´terKova´cs,CasvanderOord,JiriKucera,AliceE.A.Allen,DanielJ.Cole,ChristophOrtner,
andGa´borCsa´nyi. Linearatomicclusterexpansionforcefieldsfororganicmolecules: Beyondrmse.
JournalofChemicalTheoryandComputation,17(12):7696–7711,November2021.
[KSR+21] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer.
Grad-match: Gradientmatchingbaseddatasubsetselectionforefficientdeepmodeltraining,2021.
[KSRI20] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister:
Generalizationbaseddatasubsetselectionforefficientandrobustlearning. 2020.
[KZ22] Leonid Kahle and Federico Zipoli. Quality of uncertainty estimates from neural network potential
ensembles. PhysicalReviewE,105(1),January2022.
[LGC+23] ShuaihuaLu,LucaM.Ghiringhelli,ChristianCarbogno,JinlanWang,andMatthiasScheffler. Onthe
uncertaintyestimatesofequivariant-neural-network-ensemblesinteratomicpotentials,2023.
[LPB16] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertaintyestimationusingdeepensembles,2016.
[LW18] RuifengLiuandAndersWallqvist. Molecularsimilarity-baseddomainapplicabilitymetricefficiently
identifiesout-of-domaincompounds. JournalofChemicalInformationandModeling,59(1):181–189,
November2018.
[LXT+17] HaoLi,ZhengXu,GavinTaylor,ChristophStuder,andTomGoldstein. Visualizingthelosslandscape
ofneuralnets,2017.
[MBJ+23] AlbertMusaelian,SimonBatzner,AndersJohansson,LixinSun,CameronJ.Owen,MordechaiKorn-
bluth,andBorisKozinsky. Learninglocalequivariantrepresentationsforlarge-scaleatomisticdynam-
ics. NatureCommunications,14(1),February2023.
[MBL19] BaharanMirzasoleiman,JeffBilmes,andJureLeskovec.Coresetsfordata-efficienttrainingofmachine
learningmodels. 2019.
[MC20] SergeiManzhosandTuckerCarrington. Neuralnetworkpotentialenergysurfacesforsmallmolecules
andreactions. ChemicalReviews,121(16):10187–10217,October2020.
[MHW20] Tim Mueller, Alberto Hernandez, and Chuhong Wang. Machine learning for interatomic potential
models. TheJournalofChemicalPhysics,152(5),February2020.
[Mis21] Y.Mishin.Machine-learninginteratomicpotentialsformaterialsscience.ActaMaterialia,214:116980,
August2021.
[MY16] Yu. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using
hierarchicalnavigablesmallworldgraphs,2016.
[PCK17] Andrew A. Peterson, Rune Christensen, and Alireza Khorshidi. Addressing uncertainty in atomistic
machinelearning. PhysicalChemistryChemicalPhysics,19(18):10978–10985,2017.
[Per22] Pascal Pernot. The long road to calibrated prediction uncertainty in computational chemistry. The
JournalofChemicalPhysics,156(11),March2022.
[PGD21] MansheejPaul,SuryaGanguli,andGintareKarolinaDziugaite. Deeplearningonadatadiet: Finding
importantexamplesearlyintraining. 2021.
[PLSK20] GarimaPruthi,FrederickLiu,MukundSundararajan,andSatyenKale. Estimatingtrainingdatainflu-
encebytracinggradientdescent,2020.
11[PZEW20] GeoffPleiss,TianyiZhang,EthanR.Elenberg,andKilianQ.Weinberger. Identifyingmislabeleddata
usingtheareaunderthemarginranking,2020.
[RDG+20] Patrick Rowe, Volker L. Deringer, Piero Gasparotto, Ga´bor Csa´nyi, and Angelos Michaelides. An
accurate and transferable machine learning potential for carbon. The Journal of Chemical Physics,
153(3),July2020.
[RNE+22] Patrick Reiser, Marlen Neubert, Andre´ Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam
Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, and Pascal Friederich. Graph neural
networksformaterialsscienceandchemistry. CommunicationsMaterials,3(1),November2022.
[RTH+22] Stephan Rabanser, Anvith Thudi, KimiaHamidieh, AdamDziedzic, andNicolas Papernot. Selective
classificationvianeuralnetworktrainingdynamics,2022.
[SCQvdS23] Nabeel Seedat, Jonathan Crabbe´, ZhaozhiQian, andMihaela van derSchaar. Triage: Characterizing
andauditingtrainingdataforimprovedregression,2023.
[SGS+22] BenSorscher,RobertGeirhos,ShashankShekhar,SuryaGanguli,andAriS.Morcos. Beyondneural
scalinglaws: beatingpowerlawscalingviadatapruning,2022.
[SSL+20] SwabhaSwayamdipta,RoySchwartz,NicholasLourie,YizhongWang,HannanehHajishirzi,NoahA.
Smith,andYejinChoi. Datasetcartography:Mappinganddiagnosingdatasetswithtrainingdynamics,
2020.
[SUG21] KristofT.Schu¨tt,OliverT.Unke,andMichaelGastegger. Equivariantmessagepassingforthepredic-
tionoftensorialpropertiesandmolecularspectra,2021.
[TNY+20] KevinTran,WillieNeiswanger,JunwoongYoon,QingyangZhang,EricXing,andZacharyWUlissi.
Methodsforcomparinguncertaintyquantificationsformaterialpropertypredictions. MachineLearn-
ing: ScienceandTechnology,1(2):025006,May2020.
[TSC+18] MariyaToneva, AlessandroSordoni, RemiTachetdesCombes, AdamTrischler, YoshuaBengio, and
Geoffrey J. Gordon. An empirical study of example forgetting during deep neural network learning,
2018.
[TUG+23] Aik Rui Tan, Shingo Urata, Samuel Goldman, Johannes C. B. Dietschreit, and Rafael Go´mez-
Bombarelli. Single-modeluncertaintyquantificationinneuralnetworkpotentialsdoesnotconsistently
outperformmodelensembles. 2023.
[vDDLG01] AdriC.T.vanDuin,SiddharthDasgupta,FrancoisLorant,andWilliamA.Goddard. ReaxFF: areac-
tiveforcefieldforhydrocarbons. TheJournalofPhysicalChemistryA,105(41):9396–9409,October
2001.
[VSK23] JoshuaAVitaandDanielSchwalbe-Koda. Dataefficiencyandextrapolationtrendsinneuralnetwork
interatomicpotentials. MachineLearning: ScienceandTechnology,4(3):035031,August2023.
[WGC+23] Tom Wollschla¨ger, Nicholas Gao, Bertrand Charpentier, Mohamed Amine Ketata, and Stephan
Gu¨nnemann. Uncertaintyestimationformolecules: Desiderataandmethods,2023.
[WSC21] ShunzhouWan,RobertC.Sinclair,andPeterV.Coveney.Uncertaintyquantificationinclassicalmolec-
ulardynamics. PhilosophicalTransactionsoftheRoyalSocietyA:Mathematical,PhysicalandEngi-
neeringSciences,379(2197),March2021.
[WT20] MingjianWenandElladB.Tadmor. Uncertaintyquantificationinmolecularsimulationswithdropout
neuralnetworkpotentials. npjComputationalMaterials,6(1),August2020.
[WWS+23] ZunWang,HongfeiWu,LixinSun,XinhengHe,ZhirongLiu,BinShao,TongWang,andTie-YanLiu.
Improving machine learning force fields for molecular dynamics simulations with fine-grained force
metrics. TheJournalofChemicalPhysics,159(3),July2023.
[XVS+21] Yu Xie, Jonathan Vandermause, Lixin Sun, Andrea Cepellotti, and Boris Kozinsky. Bayesian force
fieldsfromactivelearningforsimulationofinter-dimensionaltransformationofstanene. npjCompu-
tationalMaterials,7(1),March2021.
[ZBMK23] AlbertZhu,SimonBatzner,AlbertMusaelian,andBorisKozinsky. Fastuncertaintyestimatesindeep
learninginteratomicpotentials. TheJournalofChemicalPhysics,158(16),April2023.
[ZLP+22] Xinlei Zhou, Han Liu, Farhad Pourpanah, Tieyong Zeng, and Xizhao Wang. A survey on epis-
temic(model)uncertaintyinsupervisedlearning: Recentadvancesandapplications. Neurocomputing,
489:449–465,June2022.
12A Hyperparameterdetails
A.1 NequIP
Table A1 outlines some of the most important parameters used for defining the NequIP architecture, the training
algorithm, or computing the µ (p ) values for each dataset. Unless otherwise specified, the recommended set-
M i
tings were used for the NequIP model as provided by https://github.com/mir-group/nequip/blob/main/
configs/full.yaml. For3BPAthepredefinedtrainingsetwasusedwithnovalidationset. ForCarbon GAP 20a
random90-10training–validationsplitwasused. ForOC20thepredefinedS2EFtrainingsetandIS2RSvalidationsets
wereused. TheAMSGradvariantoftheAdamoptimizerwasused,withaninitiallearningrateof0.005,aweightdecay
of 0, and the ReduceLROnPlateau scheduler. Full configuration files will be provided upon completion of internal
reviewofthesoftware.
TableA1:ThemainhyperparametersusedfortheNequIPmodel.r maxistheradialcutoffofthemodel,num layers
is the number of message passing layers, l max is the symmetry order of the equivariant features, and M is the
ensemblesize.
Dataset r max num layers l max Flossweight Elossweight Batchsize M
3BPA 5.0 5 3 1000.0 0.0 5 10
Carbon GAP 20 4.5 4 2 10.0 0.0 1 20
OC20 4.0 2 2 100.0 1.0 4 1
A.2 FAISS
The cost of UQ predictions with LTAU-FF is dictated by the complexity of the similarity search performed by the
FAISS package, as well as the size of the training set and the dimensionality of the latent space embeddings used
astheatomicdescriptors. Dependingonthesefactors,particularlythedatasetsize,differentindexingalgorithms(as
implemented by FAISS) are recommended. For the Carbon GAP 20 and OC20 datasets, the approximate neighbor
searchIndexHNSWFlatindexwasusedasopposedtotheexactbrute-forceapproach(IndexFlatL2)usedfor3BPA.
Additional UQ speedups could be obtained by using the GPU implementation of FAISS, decreasing k, or further
refining the parameters of the indexers. For a more thorough discussion of indexing methods and hyperparameter
choices,werecommendconsultingtheFAISSdocumentation.
WhiletheIndexFlatL2indexisaparameter-freebruteforceapproach,theIndexHNSWFlatindexhastwoprimary
parameterswhichstronglyaffectthecostofthesimilaritysearch. TheHeirarchicalNavigableSmallWorlds(HNSW)
method[MY16]isanapproximatesimilaritysearchalgorithmwhichdecomposesthesearchspaceintoamulti-layered
graphstructure. ThekeyparametersofIndexHNSWFlatareM(thenumberofneighborlinkstoaddforeachpointin
thegraph),efConstruction(thenumberofneighborstoconsiderateachlayerwheninsertingintothegraph),and
efSearch(thenumberofneighborstoconsiderateachlayerwhensearchingthegraph). Inthiswork,weusedvalues
of M= 32, efConstruction= 40, and efSearch= 16, which we found to provide a reasonable balance between
speedandaccuracy. ThecomputationalcostofmodelevaluationusingthesehyperparametersareshowninTableA2,
wherethenumberofnearest-neighborssearchedforisspecifiedask. Weemphasizethatmorethoroughanalysisof
theeffectsoftuningthesehyperparameterswouldbehelpful.
Table A2: Computational costs of evaluating model uncertainty for the datasets used in this work. Results were
obtainedbycomputingforcesanduncertaintiesforasingleNequIPmodelonarandomselectionof5000testsamples
foreachdataset(500for3BPA).
Dataset Trainsize(#atoms) Operation %time
UQ(IndexFlatL2,k =2) 0.9
3BPA 13,500 Graphconstruction 32.9
Forwardpass 65.9
UQ(IndexHNSWFlat,k =10) 1.0
Carbon GAP 20 400,275 Graphconstruction 53.8
Forwardpass 44.9
UQ(IndexHNSWFlat,k =10) 5.0
OC20 14,631,937 Graphconstruction 21.3
Forwardpass 73.1
13TableB1: NumberofatomicconfigurationsandatomswithineachgroupoftheCarbon GAP 20trainingset.
config typegroup #configurations #atoms
Amorphous Bulk 3,053 200,470
Amorphous Surfaces 20 2,648
Crystalline Bulk 78 368
Crystalline RSS 483 8,554
Defects 530 70,022
Diamond 164 1,360
Dimer 26 52
Fullerenes 272 11,782
Graphene 2 400
Graphite 185 6,344
Graphite Layer Sep 7 700
LD iter1 80 14,048
Liquid 6 1,296
Liquid Interface 17 3,672
Nanotubes 138 4,976
SACADA 755 24,503
Surfaces 271 49,079
Total 6,088 400,275
B Datasetdetails
3BPA
Thefirstdatasetusedinthisworkisthe3BPAdataset[KOK+21],whichhasbeenusedpreviouslyintheliteraturefor
benchmarkingextrapolationbehaviorsofforcefields[BKS+22,BBK+22,MBJ+23].The3BPAdatasetisamolecular
datasetconsistingof500trainingconfigurationstakenfrommoleculardynamics(MD)simulationsatatemperature
of 300 K, where each configuration has 27 atoms for a total of 13,500 training points. There are additionally three
separate test sets sampled using MD simulations at temperatures of 300K, 600K, and 1200K, respectively, where
increasingthesamplingtemperaturecanbeexpectedtopushthesimulationstoexploreOODregionsoftheavailable
configurationspaceofthemolecule.
Carbon GAP 20
InordertobetterunderstandhowLTAU-FFperformsonamorechallengingdataset,wealsousedtheCarbon GAP 20
dataset [RDG+20], which was originally intended to be used for fitting a model capable of accurately describing a
broad range of carbon phases. The Carbon GAP 20 dataset includes a training set of 6,088 configurations (400,275
atoms), as well as a larger superset containing 17,525 configurations (1,345,246 atoms) which we used for testing.
Thetrainingandtestsetsarecomprisedofanextremelydiverserangeofphases,includingbulkcrystals,amorphous
carbon, graphene, graphite, fullerene, nanotubes, liquids, surfaces, defectedconfigurations, andotherrareallotropes
obtainedthroughrandomstructuresearchorextractedfromtheliterature. Particularlyvaluableforourpurposesare
the labels provided by the authors of the Carbon GAP 20 dataset for defining conceptually similar clusters over the
atomic configurations in the dataset. Although in our analysis we observed a number of mislabeled configurations,
the provided groupings are still extremely useful for understanding how the models and methods used in this work
performacrossdifferentsubsetsofthedata. Abreakdownofthenumberofatomsineachgroupinthetrainingsetis
providedinTableB1. Amorethoroughdescriptionofthedatasetcanbefoundin[RDG+20].
OC20
Asapracticalapplication,wealsotrainamodeltotheS2EFtaskoftheOC20dataset[CDG+21b]thentesthowwell
theUQestimateprovidedbyLTAU-FFpredictedperformanceontheISR2Stask. Specifically,wetrainedtothe200k
split(14,631,937atoms)oftheS2EFtask,andtestedontheval idandval ood bothsplitsoftheIS2RStask. The
OC20datasetincludesawiderangeofcatalystscomprisedofvariousmaterials,surfaces,andadsorbates. Importantly,
the data includes labels for each atom identifying them as an “adsorbate” atom (part of the adsorbing molecule), a
“surface”atom(thetopfewlayersofthematerial),ora“bulk”atom(everythinginthematerialthatisnotpartofthe
surface).
14C Additionalanalysisofsampleconfidences
FigureC1: Distributionsofthep valuescomputedforallthreedatasets. NotethattheuncertaintypredictedbyLTAU
i
is1−p ,soa“sharper”metricisonewhichpredictsp valuescloserto1. Amore“disperse”metricsisonewitha
i i
widerrangeofp values.
i
Figure C2: Effects of window size and starting position on the average µ (p ) value computed over the entire
M i
Carbon GAP 20 dataset. µ (p ) values are computed over sliding windows of different sizes for every possible
M i
startingpositiongiventhetrajectoryof134epochs. Theatolthresholdisheldconstantat0.265eV/A˚ (thefinaltrain-
ingMAE).
FigureC3: Convergencetestsforµ (p )andσ (p )withincreasingM.
M i M i
D Whenshouldascalingfactorwork?
TheresultsshowninFig.1andFig.2,whilevaluablefortheirdemonstrationofapracticalapproachtocalibratingour
UQmetric,raisesomeinterestingquestionsaboutthedistributionoferrorsanduncertaintieswithinthelatentspaceof
15FigureD1:QualitativeanalysisoftheeffectsofscalingontheCDFs.TheobservedCDFscomputedoveranensemble
ofM modelsareplottedassolidlines,whilescaledversionsofthereferenceCDFareplottedasdashedlines.Inpanel
a, the high-temperature test CDFs from the 3BPA dataset are compared to the low-temperature test CDF, where the
scalingfactors, s, arespecifiedinthelegendandmatchthoseusedinFig.1. Inpanelb, selectsubsetswerechosen
fromCarbon GAP 20toexemplifyagroupforwhichareasonablesvaluecouldbefound(Amorphous Bulk)aswell
asonewhichcouldnotbeeasilycalibratedto(Liquid).
themodel. Themostobviousoftheseis: whyisasimplescalingfactor,s,sufficientforcalibratingtheUQmetricto
testpoints? Webelievethatapartialanswertothisquestioncanbeobtainedbyunderstandingtwokeyaspectsofthe
errordistributions,whichwillbediscussedfurtherhere. However,wealsoemphasizethatfutureworkanalyzingthis
questionfurtherisnecessaryandwouldbequitevaluabletothefield.
The first aspect that should be considered is how the CDFs of the test sets relate to those of the training sets. To
begin with, we will only consider the CDFs of the 3BPA dataset shown in Fig. D1a due to their relative simplicity.
Recall that the µ (p ) values computed for the training points correspond to the CDF of the errors on those points
M i
evaluatedatatol,whichwastakentobetheMAEofthetrainingset. Thescalingfactors,s,canthenbethoughtofas
scalingtheCDFofthetrainingsetinanattempttomatchittotheobservedCDFofthetestset. Critically,ifthetest
CDFisascaledversionofthetrainCDF,thentheµ (p )valuesfromthetrainingsetmaybeexpectedtoreasonably
M i
approximatethep valuesforthetestsetatthescaledthresholdvalueofatol×s. Thequalitativeanalysisofthese
j
CDFsshowninFig.D1acoupledwiththeresultsshowninFig.1csupportthishypothesis,giventhatthescaled300K
CDFseemstobeareasonableapproximationofthe600K/1200KCDFs.However,wenotethatthemismatchbetween
theCDFsissignificantlylargerat1200K,whichisconsistentwithintuitionthatthescalingapproachshouldbeginto
failwhenmovingfurtherOOD.FortheCarbon GAP 20dataset,whereaccuratecalibrationprovedmoredifficultfor
someofthegroups,weobserveinFig.D1bthattheCDFofaproblematictestgroup(Liquid)isnotwellrepresented
bythescaledtrainingCDF.Thisdiscrepancysuggeststhatamoreadvancedcalibrationtechniquemaybenecessary
forproblematicsubsetsofthedata.
GoodagreementbetweenthetestCDFandthescaledtrainingCDF,however,isnotsufficientforensuringthattesting
p values will be similar to the µ (p ) values from their nearest training points. For example, one can imagine
j M i
a scenario where the train/test sets are identical, but their point-wise errors (and corresponding uncertainties) are
shuffled versions of each other. In this situation, the two sets would have identical CDFs, but the nearest neighbor
trainingµ (p )valueswouldbecompletelyuncorrelatedfromtheobservedtestp values.InorderforourUQmetric
M i j
computedonthetrainingsettobeapplicabletopointsfromthetestset(afterscaling),itisalsonecessarythat: 1)the
test set be close enough to the training set that a distance-based similarity search yields related training points, and
2)theUQmetricberelativelyuniformoverthek nearestneighborsreturnedbythesimilaritysearch. Weobservein
Fig.D2thatthe3BPAdatasetappearstohavebothoftheseproperties,whereFig.D2ashowsthatthetestsetsarenot
toofarOOD,andFig.D2bshowstherelativeuniformityofµ (p )withineachcluster.
M i
IntheUMAPplotsoftheCarbon GAP 20dataset(Fig.D3andFig.D4),wefoundthattherewasalackofobvious
clusteringinthelatentspace,andthattheµ (p )valuesweregenerallymuchnoisier. Together,thesetwoattributes
M i
makeitdifficulttodrawconclusionsaboutwhythescalingapproachdoes,ordoesnot,workforspecificgroups.How-
ever,giventheresultsinFig.2andFig.D1b,itseemslikelythattherearenon-obviousregionsofthelatentmanifold
for which the scaling approach is valid. While using a larger ensemble size, M, and number of neigbors, k, when
performingthesimilaritysearchcanhelptoreducenoiseintheµ (p )values,webelievethatfutureworkdevelop-
M i
ingmoreadvancedclusteringtechniquescouldalsobehelpfulforidentifyingappropriatesubsetswhenattemptingto
calibratetheUQmetricviascaling.
16FigureD2: UMAPplotsofthe3BPAdataset. Panelashowsthetrainsetandallthreetestsets,whilepanelbshows
only the train set with the points colored by their µ (p ) values. The uniformity of µ (p ) within each cluster, as
M i M i
well asthe relative similaritybetween the train/testdata are necessary conditions forapplicability of distance-based
UQmetrics,includingtheonedevelopedinthiswork.
FigureD3: UMAPplotsofarandomsub-sampleoftheCarbon GAP 20dataset. Only10%ofthedatasetwasused
inthisplotforeaseofvisualization. Foravisualizationofthefulldataset,seeFig.D4. Thelackofclearclustersin
panela,aswellasthenoiseintheµ (p )valuesobservedinpanelbmakeitdifficulttounderstandhowtoproperly
M i
calibratetheUQmetrictoindividualgroupsinthedataset.
E Datasetre-weighting
In the case of the “up-weight hard” weighting scheme in Fig. 3, atom i was given a weight in the loss function of
w =exp[λ(1−µ (p )+σ (p ))];inthe“up-weighteasy”scheme,theweightwasgivenasw =exp[λ(µ (p )+
i M i M i i M i
σ (p ))]. Values of λ were determined ad hoc, and set to 2.0 and 4.5 for the up-weight easy and hard schemes,
M i
respectively. Fig. E1 provides a visual depiction of the two weighting schemes. Notably, the relationship between
µ (p )andσ (p )observedinFig.E1exactlyfollowstheexpectedpatternfrom[SSL+20],wherethedatasetcan
M i M i
beroughlydividedinto“easy-to-learn”,“hard-to-learn”,and“ambiguous”subsets.
17FigureD4:UMAPplotofthefullCarbon GAP 20dataset.Theoutlyingclustershighlighttheneedforfurtheranalysis
andimprovedclusteringtechniquestorefinethecalibrationoftheUQmetricformorecomplexdatasets.
FigureD5:UMAPplotsofarandomsub-sampleoftheOC 20dataset. Only2%ofthedatasetwasusedinthisplotfor
easeofvisualization.Pointsarecoloredaccordingtotheirgroupontheperiodictable(panela)ortheirp value(panel
i
b). Notetheuseofp insteadofµ (p )becauseeventhe200ksplitoftheS2EFtaskwastooexpensivetotrainmore
i M i
thanasinglemodelwithoutmulti-GPUsupportfromtheNequIPcode. Interestingly,thoughperhapsunsurprisingly,
themodelappearstolearntoclusteratomsbytheirgroupontheperiodictable.
FigureD6: Averagedp valuesforeachelementtype,coloredbygroupontheperiodictable. Thep valuesseemto
i i
follow a similar trend to that observed in Fig. D5, likely due to the relative magnitudes of the target forces for each
elementtype.
18FigureE1: Visualizationofthere-weightingschemesusedfortheCarbon GAP 20datasetinFig.3. Scalingfactors,
λ,wereselectedadhocbasedonexperimentationinordertoobtainweightswhichresultedinanoticeablechangein
thetraining–validationgapasshowninFig.3.
FigureE2: DistributionsoftesterrorsontheCarbon GAP 20datasetforthethreeweightingschemes. Inpanela,the
rangeofthex-axishasbeenclippedtoamaximumvalueof2.0eV/A˚ inordertoimprovevisualizationbyremovinga
smallnumber(3-4%)ofpointswithabnormallylargeerrors.Inpanelb,thefulldistributionsareshownonalog-scale.
Theaveragetesterrorforallweightingschemesafterremovingvalueslargerthan2.0eV/A˚,wasaround0.35eV/A˚.
However,whenincludingalldatafrompanelb,the“up-weighteasy”schemehadanMAEof47eV/A˚ ,whichwasan
orderofmagnitudelowerthantheothertwoschemes.
19