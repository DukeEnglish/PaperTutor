Towards Efficient and Exact Optimization of Language Model Alignment
HaozheJi1 ChengLu2 YilinNiu3 PeiKe1 HongningWang1 JunZhu2 JieTang4 MinlieHuang1
Abstract modelstogeneratedesiredresponsesthatareinlinewith
humans’ethicalstandardsandqualitypreferencesiscrucial
Thealignmentoflanguagemodelswithhuman
forthedevelopmentofreliableAIsystems.
preferences is vital for their application in real-
worldtasks. Theproblemisformulatedasopti- Theproblem,wellknownaslanguagemodel(LM)align-
mizing the model’s policy to maximize the ex- ment with human preferences (Ouyang et al., 2022), is
pected reward that reflects human preferences generally formulated as optimizing the LM policy π to
θ
with minimal deviation from the initial policy. maximizetheexpectedreward,whichreflectshumanprefer-
While considered as a straightforward solution, encesregardingthecompletionyforagivenpromptx. The
reinforcement learning (RL) suffers from high practicalrecipeistotrainarewardmodelr topredictthe
ϕ
variance in policy updates, which impedes effi- human-chosenresponsefromasetofresponsesgenerated
cientpolicyimprovement. Recently,directprefer- byaninitialLMpolicyπ . Yet,thechallengeofacquiring
init
enceoptimization(DPO)wasproposedtodirectly substantialhigh-qualitypreferencedataoftenimpedesthe
optimizethepolicyfrompreferencedata. Though accurateestimationofanidealrewardmodel. Consequently,
simpletoimplement,DPOisderivedbasedonthe thisempiricallylearnedrewardmodelmayleadtomisspec-
optimalpolicythatisnotassuredtobeachieved ified behaviors, particularly under the distributional shift
inpractice,whichunderminesitsconvergenceto betweenitstrainingdataandthedatageneratedbyπ (Gao
θ
theintendedsolution. etal.,2023). Therefore,thefinalobjectiveofalignmentad-
ditionallyinvolvesminimizingthereverseKullback-Leibler Inthispaper,weproposeefficientexactoptimiza-
(KL)divergenceofπ fromitsinitialdistributionπ with
tion(EXO)ofthealignmentobjective. Weprove θ init
anintensityβ,besidesmaximizingtheexpectedreward:
thatEXOisguaranteedtooptimizeinthesame
directionastheRLalgorithmsasymptoticallyfor
arbitaryparametrizationofthepolicy,whileen-
m θaxEx
∼D,y ∼πθ(y
|x)[r ϕ(x,y)] −βDKL(π
θ
∥π init). (1)
ablesefficientoptimizationbycircumventingthe
DuetothediscretenatureofcontentgenerationfromanLM
complexitiesassociatedwithRLalgorithms. We
(i.e., sampling y from π ( x)), the most widely adopted
θ
compareourmethodtoDPOwithboththeoretical ·|
approach to optimize Eq. (1) is reinforcement learning
andempiricalanalyses,andfurtherdemonstrate
(RL) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang
the advantages of our method over existing ap-
etal.,2022). Albeitwellstudiedevenbeforetheeraoflarge
proachesonrealistichumanpreferencedata.
languagemodels,RLsolutionsarenotoriousfortheirpoor
stabilityduetothehighvarianceinestimatingthepolicy
gradientsorvaluefunctions,whichpotentiallyworsenssam-
1.Introduction
plecomplexityandthuscompromisesefficientconvergence
(Papinietal.,2018;Anscheletal.,2017).
Despitetheproficiencyoflargelanguagemodels,e.g.,GPT-
3 (Brown et al., 2020) in complex tasks under minimal Asaremedy,directpreferenceoptimization(DPO)wasre-
supervision, they are still prone to produce harmful (Bai centlyproposedtoreplacetheRLsolutions(Rafailovetal.,
etal.,2022),biased(Benderetal.,2021),andunfaithful(Ji 2023). Specifically, DPO defines a pair-wise preference
etal.,2023c)responsesduetotheheterogeneoussources lossontheestimatedpolicyπ byleveragingtheoptimal
θ
oftheirpre-trainingcorpora. Ensuringthelargelanguage policy-rewardmappingintheoptimalsolutiontoEq. (1):
sity1 2T Th he eC Tso in nv ge hr usa at Sio tn ata il stA icI al( AC ro tA ifiI c) iaG lr Io nu tep l, ligT es ni cn egh &ua LeU an rniv ine gr- π β∗(y |x) ∝π init(y |x)eβ1rϕ(x,y) (2)
(TSAIL)Group,TsinghuaUniversity3ZhipuAI4TheKnowledge
However,thederivationofDPOisanchoredonthisoptimal
EngineeringGroup(KEG),TsinghuaUniversity.Correspondence
solutionwhichisnotguaranteedtobeattainedinpractice.In
to:MinlieHuang<aihuang@mail.tsinghua.edu.cn>.
fact,ouranalysisforthefirsttimedemonstratesthatDPOis
Preprint. notexactlyoptimizingtheintendedobjectiveinEq.(1);and
1
4202
beF
2
]LC.sc[
2v65800.2042:viXraTowardsEfficientandExactOptimizationofLanguageModelAlignment
duringoptimizationofthealignmentobjective. Then,we
º º
Ø§ Ø§
conductevaluationsontheeffectivenessoflearningfrom
º º
µRKL µFKL realhumanpreferencesinvariousalignmentbenchmarks,
involvingsummarization,dialoguegeneration,andinstruc-
tion following tasks. Comprehensive empirical analyses
substantiate our theoretical findings and demonstrate the
(a) (b)
advantageousperformanceofEXOoverDPOandPPO.
Finally,wesummarizeourcontributionsinthispaper:
Figure1.Illustration of different characteristics of (a) π by
θRKL
minimizingthereverseKL(byEXO)and(b)π byminimizing
θFKL
theforwardKL(byDPO).
• WerevealtheunderexploredequivalencebetweenKL-
regularizd reward maximization and minimizing the
thisunfortunatelyleadstoacompromisedapproximationof reverseKLdivergenceagainsttheoptimalpolicyfor
theoptimalpolicy,whentheoptimalityofπ isnotreached thelanguagemodelalignmentproblem.
θ
atthefirstplace.
Inthispaper,wepresentanunderexploredperspectiveofthe • WeproposeEXO,analgorithmtowardsefficientand
alignmentobjectiveinEq.(1):Weprovethatitisequivalent exactoptimizationoftheKL-regularizedrewardmaxi-
toprobabilitymatchingbetweentheLMpolicyπ andthe mizationobjectiveofalignment. Boththeoreticaland
θ
optimalpolicyπ definedinEq.(2)measuredbythereverse empricialresultsconfirmitseffectiveness.
β∗
KLdivergence:
m θinDKL(π
θ
∥π β∗) (3)
• WeshowthatDPOcorrespondstominimizingthefor-
Basedontheestablishedequivalence,weproposeefficient wardKLdivergence,whichislesseffectiveincaptur-
exactoptimization(EXO)oftheKL-regularizedrewardmax- ingtheessentialcharacteristicsoftheoptimalpolicy.
imizationobjective. Specifically,werigorouslyprovethat
irrespectiveofthepolicy’sparametrization,EXOisguaran- 2.Preliminaries
teedtoimproveπ inthesamedirectionastheRLsolutions
θ
asymptotically. Inpractice,wedemonstratethatEXOfacili- Wefirstformallyreviewtheformulationandobjectiveof
tatesefficientoptimizationtowardsthisalignmentobjective thealignmentproblem.Thenwereviewexistingapproaches
withfinitesampleswhilebypassingthecomplexitiesofRL. thatsolvethisproblemviareinforcementlearninganddirect
preferenceoptimization,respectively.
Under this probability matching perspective, we demon-
stratethatDPOactuallycorrespondstominimizingthefor-
2.1.AligningLanguageModelswithHuman
wardKLdivegenceDKL(π
β∗
∥π θ). Thoughminimizingboth
Preferences
theforwardandreverseKLdivergencesleadtothesameop-
timalsolutionunderunlimitedmodelcapacity,theoptimal Given a vocabulary , a language model π defines a
solutionisnotnecessarilyachievablewhentakingintoac- probabilitydistributioV nπ(x) = n π(x x , ,x )
countthegapofexpressivenessbetweenπ andπ . Under over a sequence of tokens x =
(xt= ,1 ,t x| )1
.
· W··
e
apt −pl1
y
β∗ θ (cid:81)1
···
n
thisrealisticconstraint,minimizingthesetwodivergences π to a conditional generation task of interest with input
encourage different behaviors of the final policy (Bishop space = m and output space = n modeled by
X V Y V
&Nasrabadi,2006)asillustratedinFigure1. Specifically, π(y x)=π(x,y)/π(x).
|
minimizingthereverseKLfostersamode-seekingpolicy
Thealignmentproceduretypicallystartsfromsupervised
π thatconcentratestotheprincipalmodesoftheoptimal
θRKL fine-tuning (SFT) the language model on a high-quality
policy(Chanetal.,2022), whileminimizingtheforward
dataset sftviamaximumlikelihoodestimation,whichob-
KLresultsinamean-seekingpolicyπ θFKL thatplaceslarge tainstheD SFTpolicyπ .
masstothemeanofdifferentmodesinπ ,whichdoesnot sft
β∗
necessitatehighprobabilitiesunderπ . Asaresult,π is Thenapreferencedataset pref iscollectedbyaskinghu-
β∗ θRKL
D
preferrablybetterthanπ tocapturethemaincharacter- manstoselectabetterresponsefrom(y ,y ) π (y x)
θFKL 1 2 ∼ sft |
isticsofπ inthedecodingstage(Jietal.,2023b),which given a prompt x from the same domain of sft. Let y
β∗ D w
substantiatestheadvantageofEXOoverDPO. andy bethechosenandrejectedresponsesamong(y ,y )
l 1 2
respectivelyaccordingtohumanpreferences.
Weconductaseriesofexperimentstoverifytheeffective-
nessandscalabilityofEXO.Wefirstsystematicallyevaluate A reward model r ϕ : R is usually learned on
X ×Y →
theefficiencyofdifferentapproachesintradingoffmaxi- pref to act as a surrogate to expensive human labeling.
D
mizingtheoraclerewardandminimizingtheKLdivergence The reward model is trained to prioritize y over y by
w l
2TowardsEfficientandExactOptimizationofLanguageModelAlignment
minimizingthefollowingpair-wisepreferenceloss: rewardfunctionintoapair-wisepreferenceloss:
Lr(r ϕ)=E(x,y w,y l) ∼Dpref (cid:20)−log erϕ(x,yer wϕ )(x +,y ew r) ϕ(x,y l) (cid:21). Ldpo(π θ)=E(x,y w,y l) ∼Dpref
(cid:20)
π (y x) π (y x)
Finally, apolicyπ islearnedtomaximizethefollowing logσ βlog θ w| βlog θ l| . (7)
θ − π (y x) − π (y x)
alignmentobjective(Ziegleretal.,2019): (cid:16) sft w| sft l| (cid:17)(cid:21)
3.Methodology
Jlβ hf(π θ)=Ex ∼Dpref Eπθ(y |x)[r ϕ(x,y)]
(cid:16) In this section, we start with a generalized alignment ob-
βDKL[π θ(y x) π sft(y x)] . (4)
jectiveandpresentanequivalentformofitundertheper-
− | ∥ |
(cid:17) spectiveofprobabilitymatching. Thenweformallyderive
Givenapromptxsampledfromthedataset pref,theob- efficientexactoptimization(EXO)ofthegeneralizedalign-
D
jective seeks to find the π θ that maximizes the expected mentobjectivewhilebypassingthenecessityofemploying
rewardwhileminimizesitsreverseKLdivergenceagainst anyRLalgorithms. Furthermore,wecompareagainstDPO
theSFTpolicyπ sftgovernedbythecoefficientβ 0. The anddemonstratetheadvantageofourapproachintermsof
≥
KLpenaltykeepsπ θ frommovingtoofarfromπ sfttoavoid thedistributionalcharacteristicsofthelearnedpolicy. All
overoptimizationoftherewardmodel. theproofsareprovidedinAppendixB.
Theoptimalsolutionthatmaximizes β(π )definedinEq
(4)takestheformofanenergy-basedJ mlhf odeθ l(EBM): 3.1.FromtheGeneralizedAlignmentObjectiveto
ProbabilityMatching
π (y x)=π (y
x)eβ1rϕ(x,y)
, (5) We first introduce a generalized alignment objective that
β∗
|
sft
| Z β(x) leadstotheoptimalpolicyπ
β∗
inEq. (5)andincorporates
theoriginalalignmentobjectivedefinedinEq. (4)asaspe-
where Z β(x) = y′ π sft(y ′
|x)eβ1rϕ(x,y′)
is the parti- cialcase. Inthefollowingtheorem,wepresenttheformal
tionfunction. InEq(5)∈,Ythecoefficientβ canbeconsidered definitionandthepropertyofthisgeneralizedobjective.
(cid:80)
asthetemperatureforcontrollingthestrengthofthereward
Theorem 3.1. Let β 0,β 0 and β β = β. The
model when sampling from π β∗(y |x). With β decreasing generalizedalignmenπ to≥ bjectivr e≥ isdefinedaπ
s
r
from+ to0,theoptimalpolicyπ (y x)graduallyshifts
∞
β∗
|
f thro em ret whe arS dF mT op do el lic ry ϕ:π psf rt( ϕy (y|x |x) )to =the (cid:80)d yi ′s e ∈t r Yr ϕi (b ex ru , ϕyt (i ) xo ,n y′d )e .finedby Jlβ hfr(π θβπ)=Ex −∼D βp rre Df
(cid:16)
KE L[π πθβ θβπ π(y (| yx |) x[r )ϕ ∥( πx sf, t(y y) |]
x)] , (8)
2.2.RLFine-Tuning (cid:17)
whereπβπ(y x)satisfies
θ |
Duetothediscretenatureoflanguagegeneration,theobjec-
tiveinEq. (4)isnotdifferentiablewithrespecttoπ θ,which π θβπ(y |x) ∝π θ(y |x)βππ sft(y |x)1 −βπ. (9)
prohibitssupervisedtraining. Onestandardapproachisto
useRLalgorithmstooptimizethisobjective. Ziegleretal. Thenthepolicyπ θ thatmaximizes Jlβ hfr(π θβπ)satisfies
(2019)proposedtosearchforπ thatmaximizesamodified
r pe ow lia cr yd gr rϕ ad(x ie, ny t) m− ethβ ol do sg ,sππ usθ f ct( ( hy y| |ax xθ s) ) P. rT oh xi is mc aa ln Pb oe lica ych Oie pv tie md izb ay
-
arg πm θax Jlβ hfr(π θβπ)=π β∗. (10)
tion(PPO)(Schulmanetal.,2017). Specifically, whenβ = 1,β = β, βr(πβπ)reducesto
π r Jlhf θ
β(π ).
Jlhf θ
2.3.DirectPreferenceOptimization
Intuitively, βr(πβπ)generalizes β(π )bydistributing
Tooptimizeπ directlyusingthepreferencedata,Rafailov Jlhf θ Jlhf θ
θ theKLregularizationtoboththerewardmodelr andthe
ϕ
etal.(2023)rearrangedEq. (5)toexpresstherewardfunc-
learned policy π with a separate intensity. Specifically,
θ
tionbytheoptimalpolicyπ β∗, πβπ isdefinedastheProductofExperts(Hinton,2002)that
θ
blendstheparametrizedpolicyπ withtheSFTpolicyπ
r ϕ(x,y)=βlog π β∗(y |x) +Z β(x). (6) viacoefficientβ π. Asaresult,samθ plingfromπ θβπ partials lf yt
π sft(y x) realizestheKLregularizationrequiredinEq. (4).
|
Thentheyproposedtodirectlyoptimizethepolicyπ by Thenwederiveanequivalentformofthegeneralizedalign-
θ
replacing π with π and substituting the corresponding mentobjectivebyrearrangingtheelementsinEq.(8),which
β∗ θ
3TowardsEfficientandExactOptimizationofLanguageModelAlignment
offersanewinsightofthealignmentproblemfromtheprob- π (y x),wedefineanempiricaldistributionp bynormal-
sft
|
fθ
abilitymatchingperspective. Thedetailedderivationcanbe izingtheexponentialrewardefθ(x,y)overtheK samples:
foundinAppendixB.2.
efθ(x,y i)
Jlβ hfr(π θβπ)= −β rEx ∼Dpref DKL(π θβπ(y |x) ∥π β∗ r(y |x)) p fθ(i |y 1:K,x)= K j=1efθ(x,y j). (12)
+β rEx(cid:2) ∼Dpref logZ βr(x) , (1(cid:3)1)
Recall that f (y x) =
logπβ(cid:80)
π(y x) logπ (y x) and
A res veth ae lss te hc ao tn md at xe ir mm izis ina gc to hn es gta en nt ew rai(cid:2) lt ih zer des ap le igct nmto e(cid:3)π
nθ
t, oE bq je. c( t1 iv1 e)
π θβπ(y |x)
∝θ
π
θ|
(y |x)βππ
sft(θ
y
|x)1| −βπ−
, Eq.
s (f 1t 2)|
can be
rewrittenintoaformthatexplicitlydependsonπ :
βr(πβπ)isequivalenttominimizingtheexpectedreverse θ
Jlhf θ
K πβL πd isiv aer fg ue nn ctc ie onD oK fL π(π θβ aπ s∥ dπ eβ∗ fir n) edw ii nth Er qe .sp (9e )c .t to π θ, recall eβπlog ππ sθ ft( (y yi i| |x x)
)
θ θ p fθ(i |y 1:K,x)=
π θ(y j|x)
, (13)
Thisequivalenceimpliesthepossibilityofconvertingthe
K
eβπlog
π sft(y j|x)
rewardmaximizationproblemintoasuperviseddivergence j=1
minimizationproblem,whichisabletocircumventthepoor
whereβ canberegarded(cid:80)astheinversetemperaturethat
stability and low sample efficiency issue caused by high π
modulatestheempiricaldistributiondefinedbythelogratio
varianceinRLsolutions(Papinietal.,2018;Anscheletal.,
betweenπ andπ . Similarly,wedefineadistributionp
2017). Inthefollowing,weintroduceourapproachtowards θ sft r
overtheK samplesmodeledbytherewardmodelr :
exactoptimizationofthisgeneralizedalignmentobjective ϕ
bypracticallyrealizingtheprobabilitymatchingobjective. eβ1 rrϕ(x,y i)
W orii gth ino au lt alo lis gs no mf eg ne tn oe br ja el cit ty iv, eou inr Ere qs .u (l 4ts ).remainvalidforthe p rϕ(i |y 1:K,x)=
K j=1eβ1 rrϕ(x,y
j). (14)
Finally,wetranslatetheorig(cid:80)inalobjectiveofrewardmax-
3.2.EfficientExactOptimizationoftheGeneralized imization βr(πβπ)intothereverseKLbetweenp and
AlignmentObjective Jlhf θ fθ
p overy sampledfromπ :
rϕ 1:K sft
WenowformallyderiveEXOwhichoptimizesthegeneral
alignmentobjective Jlβ hfr(π θβπ)byrealizingthereverseKL Lexo(π θ)=Ex ∼DprefEπsft(y 1:K|x)
d tii nv ger og fen lac ne gD uaK gL e(π mθβπ od∥π elβ∗ r a) li. gW nme es nt tar wt hw icit hh pth ree ag sse un mer ea sl s thet e- DKL p fθ( ·|y 1:K,x) ∥p rϕ( ·|y(cid:104) 1:K,x) . (15)
existenceofarewardmodel;thenweconsiderthecaseof (cid:0) (cid:1)(cid:105)
Thecompleteformof ispresentedinEq. (25)inAp-
learningdirectlyfrompreferencedata. Lexo
pendixB.3. Besidesitspracticalsimplicityforimplemen-
Tofacilitatepolicyoptimizationwithstraightforwardgradi- tation, wealsoelucidateitstheoreticalattributesbychar-
entbackpropagation,werewriteDKL(π θβπ ∥π
β∗
r)underthe acterizing its connection with the generalized alignment
expectationoftheproposalpolicyπ : objectiveinTheorem3.2.
sft
Theorem3.2. Followingπ ,πβπ and βr(πβπ)defined
πβπ(y x) πβπ(y x) β∗ r θ Jlhf θ
DKL(π θβπ ∥π β∗ r)=Eπsft(y |x) (cid:34)πθ sft(y || x) log πθ
β∗
r(y || x) (cid:35). i gn raE dq ie. n( t5 o) f, L(9 e) x, o(a πn θd )( s8 a) t, isf fio er sK → ∞ and arbitary θ, the
T thh ee loab go rv ae tioex ap sre fs θs (i xon ,yc )an =b le ogfu πrt θβh πe (r ys |i xm )p −lifi le od gb πy sftd (e yfi |xni )n :g ∇θ Lexo(π θ)= ∇θE 1x ∼Dpref DKL(π θβπ(y |x) ∥π β∗ r(y |x)) (16)
= β(cid:2)r(πβπ). (cid:3) (17)
efθ(x,y) −β r∇θ Jlhf θ
DKL(π θβπ ∥π
β∗
r)=Eπsft(y
|x)
(cid:34)efθ(x,y)log
Zβr1
(x)erϕ( βx r,y)
(cid:35).
Theorem 3.2 suggests the optimization direction for π
θ
duringtheintermediateoptimizationstepsforminimizing
AstheintractablenatureofZ (x)hindersdirectestima-
βr exo(π θ)alignswiththedirectionrequiredtomaximizethe
tionviaMonteCarlosimulation,weproposeapracticalway L
generalizedalignmentobjective βr(πβπ)asymptotically,
toestimatethistermbyfirstdrawingmultiplesamplesfrom Jlhf θ
whensufficientsamplepopulationisprovided. In§4.1,we
π ,andthencalculatingthereverseKLbetweentheproba-
sft showthatEXOachievesencouragingconvergenceinpracti-
bilitydistributionsdefinedbyf (x,y)and 1 r (x,y)over
θ βr ϕ calscenarioswithonlyafiniteK. Again,asaspecialcase,
thesesamplesviaself-normalizationrespectively.
(π )alignswith β(π )whenβ =1,β =β,
∇θ Lexo θ ∇θ Jlhf θ π r
Learningfromarewardmodel. Formally,givenK >1 whicheffectivelyconnectswithEq. (4). TheresultinThe-
•
i.i.d. completions y = y , ,y drawn from orem3.2iscrucial,asitshedslightonexactoptimization
1:K { 1 ··· K}
4TowardsEfficientandExactOptimizationofLanguageModelAlignment
ofthegeneralizedalignmentobjectiveviaasimpledensity Theorem 3.3. With π , πβπ and p defined in Eq. (5),
β∗ r θ rϕ
matchingapproachwithstrongtheoreticalguarantees. (9)and(14)respectively,forK andarbitaryθ,the
→ ∞
gradientof (π )satisfies
Learningfromhumanpreferencedata. Insituations Ldpo-rw θ
•
whereonlypreferencedataisaccessible,wedeviseanem-
piricalformulationof exo. Givenapreferencedataset pref ∇θ Ldpo-rw(π θ)= ∇θEx ∼Dpref
whereeachpromptxiL spairedwithy
w
andy
l
denotinD gthe DKL(π β∗ r(y |x) ∥(cid:2)π θβπ(y |x)) . (20)
chosenandrejectedcompletions. Thisbinarysupervision
(cid:3)
canbeeffectivelytransformedintoanempiricaldistribution Inthefollowing,weconsiderβ = 1,β = β tosimplify
π r
ofp rh definedbytheunderlyingrewardr hofhumanprefer- theanalysis,whiletheresultsstillholdforgeneralsettings.
ence. ToavoidinfinitywhencalculatingKLdivergence,we Putting the results of Theorem 3.2 and Theorem 3.3 to-
smooththeone-hotdistributionintoasoftdistribution,i.e., gether,wereadilyconnect and withtwodiver-
exo dpo-rw
p ε cor >h rd( iw 0 n| gy is tw oa, Ey h qyl ., px (e 1r) 3p= )ar oa1 vm e− re ytε e wra .n ad p nf dp θr yh c l( a .l n| Ay s sw ti a, lly rb el s, e ux c l) to ,m= wp eε u, ptw re edh see a nr ce t- wg Ke eLn , ic lDe lus K, sL ti r. (e aπ. t, eβ∗t
∥
th hπe eθr ) ie , nv tre eer rss rpe ee lK ac ttL iiv oL , e nD l sy hK . iL pT(π o aθ mpL ∥ r oπ o nβ∗ v g) id, tha en ead oc bt lh je ee a crf to ii vr mw ea sa g or ed f,
theEXOobjectiveontheprefencedatabysettingK = 2 EXO,DPO,andtheobjectiveofalignmentinFigure5,as
andsubstitutingr ϕwithr hinEq. (15): presentedinAppendixB.5. Althoughminimizingthesetwo
divergencesconvergestothesameoptimalpolicyπ under
β∗
Lexo-pref(π θ)=E(x,y
w,y l) ∼Dpref
unlimitedmodelcapacity,theyleadtotwodistinctsolutions
whenconsideringtherealisticconstraintsofmodelcapacity.
(cid:104)
DKL p fθ( ·|y w,y l,x) ∥p rh( ·|y w,y l,x) . (18)
Specifically, the LM policy π is mostly parametrized
θ
(cid:0) (cid:1)(cid:105) by an auto-regressive (AR) model, which enables ef-
Inpractice,π isfine-tunedoneither sftwhichiscollected ficient sampling due to the employment of local nor-
sft
D
fromthesamedomainas pref,orthechosencompletions malization. However, the optimal policy π (y x)
D
β∗
| ∝
in prefwhen sftisnotavailable. Thisclosesthedistribu- π (y x)exp(1r (x,y)) defined as an EBM trade-offs
tioD nalgapbetwD eenπ sft andtheunknowndistributionthat sasf mt pl| ingefficiβ enϕ cywithmodelingcapacity. Notably,Lin
generatesthepreferencedata pref. etal.(2021)rigorouslyprovedthatARmodelscannotper-
D
fectlycaptureallpossibledistributionsdefinedbyEBMsin
3.3.ComparingwithDPOunderthePerspectiveof termsofsupportsorrankingsbasedonthecomputational
ProbabilityMatching complexity theories. From an empirical view, this result
is also intuitive because the reward model as a discrim-
Undertheperspectiveofprobabilitymatching,weformally
inator is more flexible in distribution modeling than the
demonstratethattheDPOobjectivecorrespondstothefor-
auto-regressivegenerator.
wardKLwhichisessentiallydifferentfromthereverseKL
requiredbythealignmentobjective β(π )inEq. (4). We Underthepracticalconstraintthatπ cannotperfectlyrep-
Jlhf θ θ
thenanalyzetheirdifferencesunderrealisticconstraintsof resentπ ,minimizingtheforwardandreverseKLresultsin
β∗
modelcapacities. twopoliciesπ andπ withdifferentproperties. One
θFKL θRKL
well-knownfactisthatπ ismean-seekingwhileπ is
We first consider the general form of the DPO objective. θFKL θRKL
mode-seeking(Bishop&Nasrabadi,2006). InFigure1,we
GivenK completionsy = y , ,y drawnfrom
π (y x) and a reward1: mK ode{ l r1 · t· h· at cK ap} tures human consideranillustrativepictureoffittingaunimodalπ θ toa
prsf et fen|
ce, we generalize by
sϕ
ubstituting the sigmoid
multi-modaltargetπ β∗. TomaintainaminimalforwardKL
functionwithsoftmaxovL erdp Ko responsesandreplacingthe divergence,π θFKL mustencompassallthemodesofπ β∗ (re-
gionswhereπ exhibitssignificantvalues). Howeverdueto
one-hotlabelwithasoftdistributiondefinedbyr : β∗
ϕ therepresentationalconstraints,π tendstooverpresents
θFKL
the mean of different modes of π , potentially extending
K β∗
intothelongtailregionofπ (Chanetal.,2022;Jietal.,
Ldpo-rw(π θ)=Ex ∼DprefEπsft(y 1:K|x)
(cid:34)−
(cid:88)i=1
2 π02 w3a it) h. oO utn cath ue sino gth te hr eh ra evn ed r, sπ
eβ∗
θ KRK LL
tc oa en xs pe lole dc et ,o mn ee anm ino gde tho af
t
eβ1 rrϕ(x,y i) eβπlog ππ sθ ft( (y yi i| |x x)
)
πβ∗
willeffectivelycapturethemajormodeofπ underre-
log . (19) θRKL β∗
K j=1eβ1 rrϕ(x,y j) K eβπlog ππ sθ ft( (y yj j| |x x) )(cid:35) alisticmodelcapacity. In§4.1,weempiricallydemonstrate
j=1 theresultsofoptimizingthesetwodivergencesinpractice.
(cid:80)
Upon substituting r ϕ with(cid:80) r h and setting K = 2, dpo-rw Within the context of language model alignment, reverse
L
simplifiesto dpo. Inthefollowing,webuildconnectionof KL is preferred for generating samples according to the
L
Ldpo-rw(π θ)totheforwardKLdivegenceDKL(π
β∗
r∥π θβπ). evaluationoftheoptimalpolicy. Conversely,forwardKL
5TowardsEfficientandExactOptimizationofLanguageModelAlignment
tradeoffspreferenceevaluationwithsamplediversity,which policyandusetheirpreferencedataset3 forthealignment
isrationalonlyifthesamplesarevalidundertheevaluation. problem. In the dialogue generation task, the policy is
Tosomeextent,thereverseKLcanalsoeffectivelycapture learnedtogenerateahelpfulresponsey givenmulti-turn
thisrationaldiversity,asitmaximizesthepolicy’sentropy dialoguehistorybetweentheuserandtheassistantdenoted
topreventdistributionalcollapsetoasinglepoint. asx. WeusethehelpfulnesssubsetoftheAnthropicHelp-
fulandHarmlessdialoguedataset4(Baietal.,2022)asthe
Finally,wediscussthepotentiallimitationsandfuturework
preferencedatasetandtraintheSFTpolicyusingthecho-
inAppendixEduetopagelimit.
senresponses. Forsummarizationanddialoguegeneration
tasks, we initialize both the policy and the reward model
4.Experiments from the Pythia-2.8B (Biderman et al., 2023) following
Rafailovetal.(2023). Toensuresamplequality,weusea
We verify the effectiveness of EXO via extensive experi-
temperatureofτ =0.8todividethelogitsofthelanguage
mentations. In §4.1, we systematically study the frontier
model in all experiments. Lastly, for the instruction fol-
ofrewardmaximizationandKLminimizationachievedby
lowingtask,wecreateadatasetbasedoninstructionswith
differentalignmentmethodsinacontrolledtextgeneration
high demand and representativeness from the real-world
taskfollowingpreviousworks(Ziegleretal.,2019;Rafailov
application scenarios, featuring 83K pairs of preferences
etal.,2023). Weinvestigatetwodifferentsettings,includ-
annotatedbyhumanlabelersand49Kpromptsforpolicy
ing learning directly from preference data governed by a
training. Theaveragelengthsoftheinstructionsandthean-
predefinedoraclerewardmodel,and2)learningfromare-
swersare47and230respectively.Wecurateadiversesetof
ward model estimated from the preference data. In §4.2,
high-qualitytestinstructionstoassessarangeofcapabilities
wecompareEXOagainstdifferentapproachesonrealistic
ofthelearnedLMpolicy,encompassingmultilingualability,
alignmentproblemsincludinggeneratinghuman-preferred
creativewriting,open-endedquestionanswering,androle
summaries, helpful dialogue responses, and answers that
playing. Eachcategorytakesthesameproportioninthetest
followhumaninstructions. Underbothsettingsoflearning
set. Boththepolicyandtherewardmodelsareinitialized
directlyfromthepreferencedataandfromarewardmodel,
fromChatGLM2-6B(Duetal.,2022).
EXOoutperformsexistingalignmentapproaches,e.g.,DPO
andPPO.Next,webrieflydescribetheexperimentsettings
Evaluation. Inthecontrolledtextgenerationtask,weeval-
andleaveadditionaldetailsinAppendixC.
uatethefrontieroftheoraclerewardandtheKLdivergence
achievedbydifferentapproaches.Thisenablesustosystem-
ExperimentSetting. Ourexperimentsconsidertwoprac-
aticallycomparetheeffectivenessofdifferentmethodsin
ticalsettingsofLMalignment: (i)Directlytrainonaprefer-
maxmizingtheoraclerewardunderthesamedistributional
encedataset Dpref = {x(j),y( wj),y( lj) }N j=1wherey wandy
l
shift constrained by the reverse KL. For experiments on
arethechosenandrejectedresponsesjudgedbyanoraclere- the public preference datasets of summarization and dia-
wardmodelorhumanlabelers.(ii)Trainonarewarddataset loguegeneration,weusetherewardmodeltrainedonthe
rw = x(j),(y(j),r(j)), ,(y(j),r(j)) N where y preferencedatasetasanin-domainproxyoftheunknown
D { 1 1 ··· K K }j=1 k
is generated by the SFT policy and r is a scalar reward ground-truth reward and also query GPT-4 for zero-shot
k
providedbyarewardmodelestimatedonthegivenprefer- pair-wiseevaluation,whichisshowntobeconsistentwith
ence dataset. In the controlled text generation task, the humanjudgments(Rafailovetal.,2023). Thepromptsfor
policy is optimized to generate a completion y with pos- GPT-4evaluationareslightlymodifiedbasedonthoseused
itive sentiment given a prefix x of a movie review from in Rafailov et al. (2023), as detailed in Appendix C. We
theIMDBdataset1 (Maasetal.,2011). Tosystematically comparethegeneratedoutputsagainstthosegeneratedby
evaluatethealignmentperformance,wetrainabinarysenti- theSFTpolicyandthepreferredchoiceinthepreference
mentclassifierontheIMDBdatasetanddefinetheoracle dataset. For the instruction-following task, we report the
rewardasitslogoddsfollowingZiegleretal.(2019). Both winrateofdirectlycomparingourmethodagainstvarious
thepolicyandtherewardmodelsareinitializedfromthe baselines as judged by GPT-4. Additionally, we employ
GPT-2largemodel(Radfordetal.,2019). Inthesumma- humanassessmenttoevaluatecriteriaincludingadherence
rizationtask,thepolicyisrequiredtogenerateasummary toinstruction,correctness,fluency,safetyandhelpfulness.
yofthepostxfromtheRedditforumthatispreferredby
human annotators. Following Stiennon et al. (2020), we Methods for Comparison. We consider the following
use the same filtered version2 of the Reddit TL;DR sum- methodsforaligninglanguagemodelswithhumanprefer-
marization dataset (Vo¨lske et al., 2017) to train the SFT
3https://huggingface.co/datasets/openai/
1https://huggingface.co/datasets/imdb. summarize_from_feedback.
2https://huggingface.co/datasets/UCL-DARK/ 4https://huggingface.co/datasets/Anthropic/
openai-tldr-filtered. hh-rlhf.
6TowardsEfficientandExactOptimizationofLanguageModelAlignment
7
75 optimal
6 EXO
50 DPO
5 PPO EXOpref
EXOrw(K=4) 25
4 EXOrw(K=8)
DPOpref
3 DPOrw(K=4) 0
DPOrw(K=8) 1200 1000 800 600 400 200 0
2 − − − logπ− (yx) − −
sft |
1 Figure3.Visualizationoftheestimateddensityratiobetweenthe
0 2 4 6 optimalandlearnedpolicybyEXOandDPOandtheSFTpolicy
DKL(π ||πsft) onsamplesfromtheSFTpolicysortedbytheirlogprobabilities.
Figure2.ThefrontieroforaclerewardvsreverseKLtotheSFT
policyofdifferentmethodsinthecontrolledexperiment.
dencedbyconsistentlyachievinghigheroraclerewardsthan
baselinesunderthesameKLdivergence. Specifically,inthe
encesundervarioussettings. Underthesettingoflearning
settingofdirectlylearningfrompreferencepairs,EXO
pref
directlyfrompreferences,weconsiderthespecialcaseof
outperformsDPO byalargemargin,whichunderscores
pref
EXOforpreferencelearning (Eq. (18))denotedas
exo-pref EXO’sbettersampleefficiencywhenlearningfromacon-
L
EXO ,andthestandardDPO(Rafailovetal.,2023)that
pref strainedsourceofpreferenceindicators. AsK increases,
minimizesthepair-wiseloss (Eq. (7))ontheprefer-
dpo thefrontierofEXO beginstoexhibitconvergence,indicat-
L rw
encedata,whichwedenotedasDPO . Thenweconsider
pref inganeffectiveprogressiontowardstheintendedsolution
thesetupofalignmentwitharewardmodelestimatedfrom
within a finite K. Although DPO also improves over
rw
the preference dataset, which includes the RL algorithm
DPO when K = 4, the frontier becomes worse when
pref
PPO(Ziegleretal.,2019)thatoptimizestheexpectedre-
K is further increased to 8. This result substantiates our
wardwithaKLpenalty(Eq.(4)),thegeneralEXOobjective
analysisaboutthemean-seekingbehaviorofforwardKL,
(Eq. (15))thatperformsprobabilitymatchingbymini-
exo which leads to inaccuracy in capturing the modes of the
L
mizingreverseKL,whichisdenotedasEXO ,thegeneral
rw complextargetdistribution. Finally,weillustratethestrong
DPO objective (Eq. (19)) that minimizes the for-
dpo-rw optimizationefficiencyofEXOinFigure8,evidencedby
L
wardKL,whichisdenotedasDPO ,andtheBest-of-N
rw consistentlyachievinghighandstableoraclerewardswithin
methodwhichfirstsamplesN =128outputsfromtheSFT
fewer number of training steps compared with PPO and
policyandthenreturnstheresponsewiththehighestscore
DPOinAppendixD.3.
according to the reward model. Note that the Best-of-N
baselineispracticallyinefficientandcanberegardedasan Next, we compare DPO and EXO from the probability
upperboundofexploitingtheSFTpolicyaccordingtothe matching perspective by visualizing the probability den-
rewardmodelinEq. (5)bytradingoffthecomputation. sityofthepoliciesobtainedbythesetwoapproaches5. In
Figure3,weplottheestimateddensityratiooftheoptimal
4.1.AlignmentwiththeOracleReward and learned policies by EXO and DPO against π sft given
arandomlychosentestprompt“ThisFoxspectaclewasa
To avoid undesirable reward overoptimizaion due to dis- bighitwhenreleasedin”. Sincetheprobabilitydensityof
tributionalshift,apreferredalignmentsolutionshouldre- anLMpolicyisdefinedoverahighdimensionalspaceof
turn a policy that obtains high oracle reward while incur- = n, it is intractable to evaluate every point in this
ing minimum deviation from π sft. Thereby, we plot the Y spaceV exhaustively. Thus, we consider the representative
frontieroftheoraclerewardagainstKLdivergenceinFig- datapointsthataresampledfromπ ,andsorttheminthe
sft
ure 2. We additionally present the accuracy-KL frontier ascending order of their log probabilities. Then we com-
in Figure 7 in Appendix D.2. Each point represents a pute the empirical distribution under the learned policies
checkpoint of the learned policy which is evaluated on overthesesamples. Formally, givenM samples y M
512 prefixes from the test set to complete the response drawnfromπ conditionedonthepromptx,thee{ mpi i} rii c= a1 l
sft
with maximumly 512 tokens. We sample M = 4 com- distributionπˆ iscalculatedviaself-normalizedimportance
θ
pletions {y i}M
i=1
for each given prompt x to calculate samplingoverthelearnedpolicyπ θ:
the average oracle reward as well as to reduce the vari-
anceofapproximatingthesequence-levelKLdivergence
Mπ (y x)
DKL(π
θ
∥π sft)
≈
M1 M i=1logπ θ(y i|x) −logπ sft(y i|x). πˆ θ(y i|x)=
M π
(yθ x)i /|
π (y
x).
Despiteaimingtoo(cid:80)ptimizethesamealignmentobjective,
j=1 θ j| sft j|
theEXOapproachesyieldthemostefficientfrontiers,evi- 5Weconsiderthesett(cid:80)ingoflearningdirectlyfrompreferences.
7
drawerelcaroTowardsEfficientandExactOptimizationofLanguageModelAlignment
RewardModel(%) GPT-4(%) RewardModel(%) GPT-4(%)
Method Method
vsSFT vsChosen vsSFT vsChosen vsSFT vsChosen vsSFT vsChosen
w/ Preferences w/ Preferences
DPO 68.3 23.7 57.0 30.5 DPO 66.3 65.1 58.0 37.0
pref pref
EXO 92.5 60.1 83.0 55.0 EXO 76.4 76.7 73.0 51.0
pref pref
w/ Reward Model w/ Reward Model
Best-of-N 99.3 75.8 83.5 60.0 Best-of-N 94.6 98.2 86.0 63.0
PPO 93.2 58.3 77.0 52.0 PPO 75.0 74.0 66.5 52.0
DPO 82.7 39.8 70.0 41.0 DPO 79.9 81.3 75.5 49.0
rw rw
EXO 97.3 76.4 88.5 64.0 EXO 85.6 87.2 83.5 60.0
rw rw
Table1.WinratesagainsttheSFTgeneratedtextsandthechosen Table2.WinratesagainsttheSFTgeneratedtextsandthechosen
textsontheTL;DRsummarizationdataset.Bestresultsfromthe textsontheAnthropic-HHdataset.Bestresultsfromthecomputa-
computationallyefficientmethodsarehighlightedinboldface. tionallyefficientmethodsarehighlightedinboldface.
Fortheoptimalpolicy,theempiricaldistributionreducesto: outtherewardmodel,itsuffersfromthemostsignificant
declineofwinratewhenassessedbyGPT-4. Thisdropof
Mπ (y x)exp(r(x,y )/β)
πˆ β∗(y i|x)= sft
M
i|
exp(r(x,y
)/βi
)
. p imer pf eo rr fm eca tn rc ee wi as ra dtt mri ob du ete ld wt ho ili ets ne ex gc le ecss tii nv gee thx epl do ei vta iati to ion no ff roth me
j=1 j
theinitialSFTpolicy. Weprovideexamplesgeneratedby
Finally, we use kernel(cid:80)density estimation to estimate the
DPOandEXOonbothtasksinAppendixD.5.
probabilitydensityπˆ(y x)oftheempiricaldistributionand
p pl ro ot bath be ilid tyen os fity thera dti ao taρ|
πˆ
p( oy in|x ts) u=
ndeπ
rπˆ sf(
t
π(y y|x
|x
.)
)
Fa rg oa min ts ht eth re esl uo lg
t,
F ofor Et Xhe Oin cs ot mru pc ati ro en d-f tooll vo aw rii on ug st bas ak se, lw ine esre ip no Frt igth ue rew 4in ur na dte es
r
sft
thesettingoflearningfromtherewardmodelgivenitsad-
the density ratio of the EXO policy closely matches the
vantageousperformanceobservedsofar. Fromtheresult,
optimal policy at the high probability region against π ,
sft
we observe that EXO outperforms all baselines by clear
which reflects its ability in capturing the major modes of
margins, thereby underscoring its scalability in practical
theoptimalpolicy. However,theDPOpolicyoverestimates
applications. Notably,EXOachieves10%and5%improve-
the long tail in π due to the mean-seeking tendency of
sft
mentoveritsclosestcompetitorsasjudgedbyGPT-4and
forwardKL,resultinginareducedaccuracyinfittingthe
humanannotatorsrespectively.
modesoftheoptimalpolicy. Wepresentmorevisualization
resultsinAppendixD.4. GPT-4evaluation Humanevaluation
60 40
4.2.AlignmentwithHumanPreferences 50 30
40 20
Next,weundertakeaseriesofexperimentsfocusedonlearn- 30 10 Win
ingfromrealhumanpreferences. Inthetasksofsummariza- Lose
20 0
tionanddialoguegeneration,weconsiderthetwosettings vsDPO vsPPO vsSFT vsDPO vsPPO vsSFT
of learning directly from preference data and from a re-
Figure4.WinratesbycomparingEXOtovariousbaselinesonthe
wardmodelrespectively. Wesetthesamehyperparameters
instruction-followingtaskjudgedbyGPT-4andhumanlabelers.
(e.g., β , β ) for EXO and DPO across different settings
π r
and datasets, and provide the results of tuning these hy- 5.Conclusion
perparametersinAppendixD.1. Evaluationresultsontext
summaizationanddialoguegenerationareshowninTable1 Inthiswork,weconsidertheproblemofaligninglanguage
andTable2respectively. Uponcomparisonwithboththe modelswithhumanpreferences. Althoughreinforcement
SFTandchosenresponses,EXOdemonstratesnotableim- learning(RL)forrewardmaximizationpresentsthedirect
provementoverDPOandPPO.Thisadvantageisevidentin andapparentsolution,wereframetheprobleminasuper-
evaluationsusingboththein-domainrewardmodelandzero- visedprobabilitymatchingframework,whichunderscores
shotassessmentbyGPT-4,acrossbothsettingsoflearning theprobabilisticinterpretationofthealignmentprocedure.
frompreferencesandfromarewardmodel. Notably,EXO Thisderivesourefficientexactoptimization(EXO)ofthe
is the only practically efficient method to attain a GPT-4 KL-regularizedrewardmaximizationobjectiveofalignment.
winrateexceeding50%whencomparedtothechosenre- Formally,weprovetheasymptoticequivalencebetweenthe
sponsesthatmayhavebeenproducedbyamoreadvanced EXO objective and the alignment objective. In practice,
languagemodel. AlthoughtheBest-of-N baselineachieves EXOenablesefficientoptimizationviaprobabilitymatch-
comparableorhigherrewardmodelwinratebymaximizing ingbetweenempiricaldistributions,whichavoidsthecom-
8
)%(etarniWTowardsEfficientandExactOptimizationofLanguageModelAlignment
plexities of RL algorithms. We further demonstrate that Bishop, C.M.andNasrabadi, N.M. Patternrecognition
DPOinitsgeneralformactuallycorrespondstominimizing andmachinelearning,volume4. Springer,2006.
theforwardKLagainsttheoptimalpolicy,whichisshown
tobelesseffectiveincapturingthemodesoftheoptimal Brown,T.B.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,
policyunderrealisticmodelparametrizationwithboththeo- J.,Dhariwal,P.,Neelakantan,A.,Shyam,P.,Sastry,G.,
reticalandempiricaljustifications. Finally,wedemonstrate Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G.,
the effectiveness and scalability of EXO on various text Henighan, T., Child, R., Ramesh, A., Ziegler, D. M.,
generationtaskswithrealhumanpreferences. Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,
Litwin, M., Gray, S., Chess, B., Clark, J., Berner,
Acknowledgements C., McCandlish, S., Radford, A., Sutskever, I., and
Amodei, D. Language models are few-shot learners.
WewouldliketothankZhipuAIforsponsoringthecompu- In Larochelle, H., Ranzato, M., Hadsell, R., Balcan,
tationresourcesandannotationcostusedinthiswork. M.,andLin,H.(eds.),AdvancesinNeuralInformation
Processing Systems 33: Annual Conference on Neural
References Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020. URL https:
Anschel, O., Baram, N., andShimkin, N. Averaged-dqn: //proceedings.neurips.cc/paper/2020/hash/
Variancereductionandstabilizationfordeepreinforce- 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.
mentlearning. InInternationalconferenceonmachine html.
learning,pp.176–185.PMLR,2017.
Chan,A.,Silva,H.,Lim,S.,Kozuno,T.,Mahmood,A.R.,
Azar, M. G., Rowland, M., Piot, B., Guo, D., Calan-
and White, M. Greedification operators for policy op-
driello, D., Valko, M., and Munos, R. A general the-
timization: Investigating forward and reverse kl diver-
oretical paradigm to understand learning from human
gences. TheJournalofMachineLearningResearch,23
preferences. CoRR, abs/2310.12036, 2023. doi: 10.
(1):11474–11552,2022.
48550/ARXIV.2310.12036. URL https://doi.org/
10.48550/arXiv.2310.12036.
Chen, H., Lu, C., Ying, C., Su, H., and Zhu, J. Offline
reinforcement learning via high-fidelity generative be-
Bai,Y.,Jones,A.,Ndousse,K.,Askell,A.,Chen,A.,Das-
haviormodeling. InTheEleventhInternationalConfer-
Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan,
ence on Learning Representations, ICLR 2023, Kigali,
T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,
Rwanda,May1-5,2023.OpenReview.net,2023. URL
Showk,S.E.,Elhage,N.,Hatfield-Dodds,Z.,Hernandez,
https://openreview.net/pdf?id=42zs3qa2kpy.
D.,Hume,T.,Johnston,S.,Kravec,S.,Lovitt,L.,Nanda,
N.,Olsson,C.,Amodei,D.,Brown,T.B.,Clark,J.,Mc-
Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,
Candlish,S.,Olah,C.,Mann,B.,andKaplan,J. Train-
G., Roberts, A., Barham, P., Chung, H.W., Sutton, C.,
ingahelpfulandharmlessassistantwithreinforcement
Gehrmann,S.,etal. Palm: Scalinglanguagemodeling
learningfromhumanfeedback. CoRR,abs/2204.05862,
withpathways. JournalofMachineLearningResearch,
2022. doi: 10.48550/ARXIV.2204.05862. URLhttps:
24(240):1–113,2023.
//doi.org/10.48550/arXiv.2204.05862.
Bender, E. M., Gebru, T., McMillan-Major, A., and Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Shmitchell,S. Onthedangersofstochasticparrots: Can Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,
language models be too big? In Proceedings of the S.,etal. Scalinginstruction-finetunedlanguagemodels.
2021ACMConferenceonFairness,Accountability,and arXivpreprintarXiv:2210.11416,2022.
Transparency,FAccT’21,pp.610–623,NewYork,NY,
USA,2021.AssociationforComputingMachinery.ISBN Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z.,
9781450383097. doi: 10.1145/3442188.3445922. URL and Tang, J. GLM: general language model pretrain-
https://doi.org/10.1145/3442188.3445922. ing with autoregressive blank infilling. In Muresan,
S., Nakov, P., and Villavicencio, A. (eds.), Proceed-
Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, ingsofthe60thAnnualMeetingoftheAssociationfor
H.,O’Brien,K.,Hallahan,E.,Khan,M.A.,Purohit,S., Computational Linguistics (Volume 1: Long Papers),
Prashanth,U.S.,Raff,E.,etal. Pythia: Asuiteforana- ACL2022,Dublin,Ireland,May22-27,2022,pp.320–
lyzinglargelanguagemodelsacrosstrainingandscaling. 335. Association for Computational Linguistics, 2022.
In International Conference on Machine Learning, pp. doi: 10.18653/V1/2022.ACL-LONG.26. URL https:
2397–2430.PMLR,2023. //doi.org/10.18653/v1/2022.acl-long.26.
9TowardsEfficientandExactOptimizationofLanguageModelAlignment
Eysenbach, B. and Levine, S. If maxent rl is the answer, Lin, C.-C., Jaech, A., Li, X., Gormley, M., and Eisner, J.
whatisthequestion? arXivpreprintarXiv:1910.01913, Limitations of autoregressive models and their alterna-
2019. tives. InNAACL,2021.
Gao,L.,Schulman,J.,andHilton,J. Scalinglawsforre- Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M.,
wardmodeloveroptimization. InKrause,A.,Brunskill, Liu, P. J., and Liu, J. Statistical rejection sam-
E., Cho, K., Engelhardt, B., Sabato, S., andScarlett, J. plingimprovespreferenceoptimization. arXivpreprint
(eds.), International Conference on Machine Learning, arXiv:2309.06657,2023.
ICML2023,23-29July2023,Honolulu,Hawaii,USA,
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.W.,
volume 202 of Proceedings of Machine Learning Re-
Tay,Y.,Zhou,D.,Le,Q.V.,Zoph,B.,Wei,J.,etal. The
search, pp. 10835–10866. PMLR, 2023. URL https:
flan collection: Designing data and methods for effec-
//proceedings.mlr.press/v202/gao23h.html.
tiveinstructiontuning. arXivpreprintarXiv:2301.13688,
2023.
Hinton, G. E. Training products of experts by minimiz-
ingcontrastivedivergence. NeuralComput.,14(8):1771–
Lu,C.,Chen,H.,Chen,J.,Su,H.,Li,C.,andZhu,J. Con-
1800,2002. doi: 10.1162/089976602760128018. URL
trastive energy prediction for exact energy-guided dif-
https://doi.org/10.1162/089976602760128018.
fusion sampling in offline reinforcement learning. In
Krause, A., Brunskill, E., Cho, K., Engelhardt, B.,
Hoffmann,J.,Borgeaud,S.,Mensch,A.,Buchatskaya,E.,
Sabato, S., and Scarlett, J. (eds.), International Con-
Cai, T., Rutherford, E., de Las Casas, D., Hendricks,
ference on Machine Learning, ICML 2023, 23-29 July
L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,
2023, Honolulu, Hawaii, USA, volume 202 of Pro-
Millican, K., van den Driessche, G., Damoc, B., Guy,
ceedings of Machine Learning Research, pp. 22825–
A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W.,
22855. PMLR, 2023. URL https://proceedings.
Vinyals, O., and Sifre, L. Training compute-optimal
mlr.press/v202/lu23d.html.
large language models. CoRR, abs/2203.15556, 2022.
doi: 10.48550/ARXIV.2203.15556. URLhttps://doi. Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,
org/10.48550/arXiv.2203.15556. A. Y., and Potts, C. Learning word vectors for sen-
timent analysis. In Lin, D., Matsumoto, Y., and Mi-
Ji, H., Ke, P., Hu, Z., Zhang, R., and Huang, M. Tai-
halcea, R. (eds.), The 49th Annual Meeting of the As-
loring language generation models under total varia-
sociation for Computational Linguistics: Human Lan-
tion distance. In The Eleventh International Confer-
guageTechnologies,ProceedingsoftheConference,19-
ence on Learning Representations, ICLR 2023, Kigali,
24 June, 2011, Portland, Oregon, USA, pp. 142–150.
Rwanda,May1-5,2023.OpenReview.net,2023a. URL
TheAssociationforComputerLinguistics,2011. URL
https://openreview.net/pdf?id=VELL0PlWfc.
https://aclanthology.org/P11-1015/.
Ji,H.,Ke,P.,Wang,H.,andHuang,M. Languagemodel
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
decodingasdirectmetricsoptimization. arXivpreprint
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,
arXiv:2310.01041,2023b.
Ray,A.,Schulman,J.,Hilton,J.,Kelton,F.,Miller,L.,
Simens, M., Askell, A., Welinder, P., Christiano, P. F.,
Ji,Z.,Lee,N.,Frieske,R.,Yu,T.,Su,D.,Xu,Y.,Ishii,E.,
Leike, J., and Lowe, R. Training language models to
Bang,Y.J.,Madotto,A.,andFung,P. Surveyofhalluci-
follow instructions with human feedback. In NeurIPS,
nationinnaturallanguagegeneration. ACMComputing
2022.
Surveys,55(12):1–38,2023c.
Papini, M., Binaghi, D., Canonaco, G., Pirotta, M., and
Korbak,T.,Perez,E.,andBuckley,C.L.Rlwithklpenalties
Restelli, M. Stochastic variance-reduced policy gradi-
is better viewed as bayesian inference. arXiv preprint
ent. InInternationalconferenceonmachinelearning,pp.
arXiv:2205.11275,2022.
4026–4035.PMLR,2018.
Kreutzer, J., Uyheng, J., and Riezler, S. Reliability and Peng, X. B., Kumar, A., Zhang, G., and Levine, S.
learnability of human bandit feedback for sequence- Advantage-weightedregression: Simpleandscalableoff-
to-sequence reinforcement learning. arXiv preprint policyreinforcementlearning. CoRR,abs/1910.00177,
arXiv:1805.10627,2018. 2019. URLhttp://arxiv.org/abs/1910.00177.
Kung,P.-N.andPeng,N. Domodelsreallylearntofollow Peters,J.andSchaal,S. Reinforcementlearningbyreward-
instructions? an empirical study of instruction tuning. weighted regression for operational space control. In
arXivpreprintarXiv:2305.11383,2023. Ghahramani, Z. (ed.), Machine Learning, Proceedings
10TowardsEfficientandExactOptimizationofLanguageModelAlignment
of the Twenty-Fourth International Conference (ICML Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
2007), Corvallis, Oregon, USA,June20-24, 2007, vol- Klimov, O. Proximal policy optimization algorithms.
ume227ofACMInternationalConferenceProceeding arXivpreprintarXiv:1707.06347,2017.
Series,pp.745–750.ACM,2007. doi: 10.1145/1273496.
1273590.URLhttps://doi.org/10.1145/1273496. Song, F., Bowen, Y., Li, M., Yu, H., Huang, F., Li,
1273590. Y., and Wang, H. Preference ranking optimiza-
tion for human alignment. ArXiv, abs/2306.17492,
2023. URL https://api.semanticscholar.org/
Peters, J., Mu¨lling, K., and Altun, Y. Relative entropy
CorpusID:259308873.
policysearch. InFox,M.andPoole,D.(eds.),Proceed-
ingsoftheTwenty-FourthAAAIConferenceonArtificial
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,
Intelligence, AAAI 2010, Atlanta, Georgia, USA, July
R., Voss, C., Radford, A., Amodei, D., andChristiano,
11-15, 2010, pp. 1607–1612. AAAI Press, 2010. doi:
P. F. Learning to summarize from human feedback.
10.1609/AAAI.V24I1.7727. URLhttps://doi.org/
CoRR, abs/2009.01325, 2020. URL https://arxiv.
10.1609/aaai.v24i1.7727.
org/abs/2009.01325.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
Sutskever,I.,etal. Languagemodelsareunsupervised M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
multitasklearners. OpenAIblog,1(8):9,2019. Azhar,F.,etal. Llama:Openandefficientfoundationlan-
guagemodels. arXivpreprintarXiv:2302.13971,2023.
Rae,J.W.,Borgeaud,S.,Cai,T.,Millican,K.,Hoffmann,J.,
Song,H.F.,Aslanides,J.,Henderson,S.,Ring,R.,Young, Vo¨lske,M.,Potthast,M.,Syed,S.,andStein,B. Tl;dr: Min-
S.,Rutherford,E.,Hennigan,T.,Menick,J.,Cassirer,A., ingreddittolearnautomaticsummarization. InWang,L.,
Powell, R., van den Driessche, G., Hendricks, L. A., Cheung,J.C.K.,Carenini,G.,andLiu,F.(eds.),Proceed-
Rauh,M.,Huang,P.,Glaese,A.,Welbl,J.,Dathathri,S., ingsoftheWorkshoponNewFrontiersinSummarization,
Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, NFiS@EMNLP 2017, Copenhagen, Denmark, Septem-
A.,McAleese,N.,Wu,A.,Elsen,E.,Jayakumar,S.M., ber7,2017,pp.59–63.AssociationforComputational
Buchatskaya,E.,Budden,D.,Sutherland,E.,Simonyan, Linguistics,2017. doi: 10.18653/V1/W17-4508. URL
K.,Paganini,M.,Sifre,L.,Martens,L.,Li,X.L.,Kun- https://doi.org/10.18653/v1/w17-4508.
coro,A.,Nematzadeh,A.,Gribovskaya,E.,Donato,D.,
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,
Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli,
Khashabi, D., and Hajishirzi, H. Self-instruct: Align-
M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,
inglanguagemodelswithself-generatedinstructions. In
Pohlen,T.,Gong,Z.,Toyama,D.,deMassond’Autume,
Rogers,A.,Boyd-Graber,J.L.,andOkazaki,N.(eds.),
C.,Li,Y.,Terzi,T.,Mikulik,V.,Babuschkin,I.,Clark,
Proceedings of the 61st Annual Meeting of the Associ-
A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J.,
ation for Computational Linguistics (Volume 1: Long
Johnson,M.J.,Hechtman,B.A.,Weidinger,L.,Gabriel,
Papers), ACL2023, Toronto, Canada, July9-14, 2023,
I.,Isaac,W.,Lockhart,E.,Osindero,S.,Rimell,L.,Dyer,
pp. 13484–13508. Association for Computational Lin-
C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L.,
guistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.
Hassabis,D.,Kavukcuoglu,K.,andIrving,G. Scaling
754. URL https://doi.org/10.18653/v1/2023.
language models: Methods, analysis & insights from
acl-long.754.
training gopher. CoRR, abs/2112.11446, 2021. URL
https://arxiv.org/abs/2112.11446.
Wang,Z.,Novikov,A.,Zolna,K.,Merel,J.,Springenberg,
J.T.,Reed,S.E.,Shahriari,B.,Siegel,N.Y.,Gu¨lc¸ehre,
Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,
C¸., Heess, N., and de Freitas, N. Critic regularized
Ermon,S.,andFinn,C. Directpreferenceoptimization:
regression. InLarochelle,H.,Ranzato,M.,Hadsell,R.,
Yourlanguagemodelissecretlyarewardmodel.InThirty-
Balcan,M.,andLin,H.(eds.),AdvancesinNeuralInfor-
seventh Conference on Neural Information Processing
mation Processing Systems 33: Annual Conference on
Systems,2023. URLhttps://arxiv.org/abs/2305.
NeuralInformationProcessingSystems2020,NeurIPS
18290.
2020,December6-12,2020,virtual,2020. URLhttps:
//proceedings.neurips.cc/paper/2020/hash/
Sanh,V.,Webson,A.,Raffel,C.,Bach,S.H.,Sutawika,L., 588cb956d6bbe67078f29f8de420a13d-Abstract.
Alyafeai,Z.,Chaffin,A.,Stiegler,A.,Scao,T.L.,Raja, html.
A.,etal. Multitaskpromptedtrainingenableszero-shot
task generalization. arXiv preprint arXiv:2110.08207, Williams,R.J. Simplestatisticalgradient-followingalgo-
2021. rithmsforconnectionistreinforcementlearning. Mach.
11TowardsEfficientandExactOptimizationofLanguageModelAlignment
Learn., 8:229–256, 1992. doi: 10.1007/BF00992696.
URLhttps://doi.org/10.1007/BF00992696.
Yao,Z.,Aminabadi,R.Y.,Ruwase,O.,Rajbhandari,S.,Wu,
X.,Awan,A.A.,Rasley,J.,Zhang,M.,Li,C.,Holmes,C.,
Zhou,Z.,Wyatt,M.,Smith,M.,Kurilenko,L.,Qin,H.,
Tanaka,M.,Che,S.,Song,S.L.,andHe,Y. Deepspeed-
chat: Easy,fastandaffordableRLHFtrainingofchatgpt-
likemodelsatallscales. CoRR,abs/2308.01320,2023.
doi: 10.48550/ARXIV.2308.01320. URLhttps://doi.
org/10.48550/arXiv.2308.01320.
Yuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., and
Huang,F. Rrhf: Rankresponsestoalignlanguagemod-
elswithhumanfeedbackwithouttears. arXivpreprint
arXiv:2304.05302,2023.
Yue, Y., Kang, B., Ma, X., Xu, Z., Huang, G., and YAN,
S. Boosting offline reinforcement learning via data re-
balancing. In3rdOfflineRLWorkshop: OfflineRLasa
”Launchpad”,2022. URLhttps://openreview.net/
forum?id=vOC01fqW2T.
Zhao,Y.,Joshi,R.,Liu,T.,Khalman,M.,Saleh,M.,and
Liu,P.J. Slic-hf: Sequencelikelihoodcalibrationwith
humanfeedback. arXivpreprintarXiv:2305.10425,2023.
Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z.,
Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,
H.,Gonzalez,J.E.,andStoica,I. Judgingllm-as-a-judge
withmt-benchandchatbotarena. CoRR,abs/2306.05685,
2023a. doi: 10.48550/ARXIV.2306.05685. URLhttps:
//doi.org/10.48550/arXiv.2306.05685.
Zheng,R.,Dou,S.,Gao,S.,Hua,Y.,Shen,W.,Wang,B.,
Liu,Y.,Jin,S.,Liu,Q.,Zhou,Y.,etal. Secretsofrlhf
in large language models part i: Ppo. arXiv preprint
arXiv:2307.04964,2023b.
Ziegler,D.M.,Stiennon,N.,Wu,J.,Brown,T.B.,Radford,
A., Amodei, D., Christiano, P. F., and Irving, G. Fine-
tuninglanguagemodelsfromhumanpreferences. CoRR,
abs/1909.08593,2019. URLhttp://arxiv.org/abs/
1909.08593.
12TowardsEfficientandExactOptimizationofLanguageModelAlignment
A.RelatedWork
Largelanguagemodels(Raeetal.,2021;Hoffmannetal.,2022;Touvronetal.,2023)learnedtopredictthenexttokens
onlargecorporahavesignificantlyimprovedtheperformanceofvariousnaturallanguageprocessing(NLP)tasksinzero
shot (Radford et al., 2019) or few-shot (Brown et al., 2020; Chowdhery et al., 2023) settings. To bridge the mismatch
betweenthetrainingobjectiveandusers’objectiveofsolvingspecificNLPtasks,instruction-tuningisproposed,which
fine-tunesthelanguagemodelsonhumancuratedinstruction-responsepairsinafullysupervisedmanner(Sanhetal.,2021;
Chungetal.,2022;Longpreetal.,2023;Wangetal.,2023). Despiteitsperformanceandgeneralizationtounseeninstruction
sets(Chungetal.,2022),therehavebeenconcernsregardingthepracticalityofcollectingabundanthighqualityresponses
andthatthemodelssupervisedbytokenpredictionmightonlycapturesurficialpatternsratherthancomprehendingthe
tasks(Kung&Peng,2023).
To address the aforementioned deficiencies, the framework of reinforcement learning from human feedback (RLHF)
is proposed (Ziegler et al., 2019), which relies on only relative human preferences on response quality and optimizes
the language model by RL algorithms (Williams, 1992), e.g., PPO (Schulman et al., 2017), under the supervision of a
rewardmodelwhichencapsulatestheimplicitpreferenceofhumans. DespitethesuccessofRLHFinvarioustaskswhere
alignmentisstronglyemphasized,e.g.,translation(Kreutzeretal.,2018),summarization(Stiennonetal.,2020),instruction
following(Ouyangetal.,2022),andetc.,RLfine-tuningforlargelanguagemodelsstillfacesseriouschallengesinstability
andscalability(Zhengetal.,2023b).
Thereby,anewtrendofrecentworksproposedtooptimizethelanguagemodeltofollowhumanpreferenceswithoutapplying
RLalgorithms(Yuanetal.,2023;Zhaoetal.,2023;Rafailovetal.,2023). Whilemostapproachesareempiricallysetto
modelalignmentasare-rankingproblem,DPOproposedatheoreticallysoundwaytorealizedirectpolicyoptimization
frompreferencedatabasedontheanalyticoptimalsolutionoftherewardmaximizationobjectiveinRLHF.Afterwards,
therehavebeenseveralstudiesthatextendDPOinvariousways,e.g.,alteringthepreferencedatabysamplingfromthe
optimalpolicyviarejectionsampling(Liuetal.,2023), substitutingthepoint-wiserewardwithapair-wisepreference
function(Azaretal.,2023),extendingthepreferencepairstorankingsofpreferencesofanysize(Songetal.,2023),and
etc. However, these approaches are still based on the formation of DPO. In this work, we propose to learn the policy
by exactly optimizing the RLHF objective via probability matching that minimizes the reverse KL. In the literature of
maximum-entropyRL(Eysenbach&Levine,2019;Korbaketal.,2022),thisequivalentformofprobabilitymatchingwas
discussedonlyforanalysispurposes,whilewearethefirsttoderiveapracticalwaytooptimizeit. WealsorevisitDPO
underthisperspectiveandrecognizethatitactuallycorrespondstominimizingtheforwardKLinitsgeneralform. Tothis
sense,DPOsharesthesamespiritofweightedregression(Peters&Schaal,2007;Pengetal.,2019;Wangetal.,2020),an
algorithmthatdirectlyutilizesbehavioralactionstosupervisethepolicyinofflineRL(Petersetal.,2010;Luetal.,2023).
However,thisapproachisknowntobesuboptimalwhenthepolicymodelislimitedindistributionalexpressivity(Yueetal.,
2022;Chenetal.,2023). WeanalyzethecharacteristicsoftheprobabilitydensitylearnedbyDPOwithboththeoretical
insightandempiricalexperimentations.
B.ProofsandDerivations
B.1.ProofofTheorem3.1
Proof. Wederivethepolicythatmaximizesthegeneralizedalignmentobjective βr(πβπ)andshowthatitisthesameas
Jlhf θ
theoptimalsolutionπ of β(π ). First,werestatetheformationof βr(πβπ):
β∗ Jlhf θ Jlhf θ
Jlβ hfr(π θβπ)=Ex ∼Dpref E π θβπ(y |x)[r ϕ(x,y)] −β rDKL[π θβπ(y |x) ∥π sft(y |x)] , (21)
(cid:16) (cid:17)
whereπβπ isdefinedas:
θ
π θβπ(y |x) ∝π θ(y |x)βππ sft(y |x)1 −βπ. (22)
WedenotethepolicythatmaximizesEq. (21)asπ
θ∗
=argmax Jlβ hfr(π θβπ). Thenwesubstituteθwithθ ∗inEq. (22):
πθ
π θβ ∗π(y |x) ∝π θ∗(y |x)βππ sft(y |x)1 −βπ. (23)
Sinceπβπ istheoptimalsolutionof βr(),itshouldsatisfyEq. (5)whichgives:
θ∗ Jlhf ·
π θβ ∗π(y |x)=π
β∗
r(y |x) ∝π sft(y |x)eβ1 rr(x,y). (24)
13TowardsEfficientandExactOptimizationofLanguageModelAlignment
TogetherwithEq. (23)andEq. (24),weobtaintheformofπ
θ∗
viasomesimplealgebra:
π θ∗(y |x) ∝(π β∗ r(y |x)π sft(y
|x)βπ−1)β1
π
(π sft(y
x)eβ1 rr(x,y))β1
ππ sft(y
x)βπ βπ−1
∝ | |
π sft(y x)β1 πeβr1 βπr(x,y)π sft(y x)βπ βπ−1
∝ | |
π sft(y x)eβr1 βπr(x,y).
∝ |
Bydefinition,β rβ π =β,thenπ θ∗ reducestothesameformoftheoptimalsolutionof Jlβ hf(π θ)definedinEq. (5):
π θ∗(y |x)=π β∗(y |x) ∝π sft(y
|x)eβ1r(x,y),
whichcompletestheproofofEq. (10).
B.2.DerivationofEq. (11)
Wefirststartbyrearranging βr(πβπ)intotheexpectationofalogratio:
Jlhf θ
Jlβ hfr(π θβπ)=Ex ∼Dpref E π θβπ(y |x)[r ϕ(x,y)] −β rDKL[π θβπ(y |x) ∥π sft(y |x)]
(cid:16) πβπ(y x)(cid:17)
=Ex ∼Dpref E π θβπ(y |x)[r ϕ(x,y)] −β rE π θβπ(y |x) log πθ sft(y| x)
(cid:16) (cid:104) | (cid:105)(cid:17)
πβπ(y x)
=Ex ∼Dpref β rE y ∼π θβπ(y |x) logeβ1 rrϕ(x,y) −β rE π θβπ(y |x) log πθ sft(y| x)
(cid:16) (cid:2) π sft(y x)eβ1(cid:3) rrϕ(x,y) (cid:104) | (cid:105)(cid:17)
=Ex ∼DprefE y ∼π θβπ(y |x) (cid:20)β rlog π| θβπ(y |x)
(cid:21)
Noticetheanalyticalformofπ :
β∗
r
1
π
β∗
r(y |x)=
Z
π sft(y |x)eβ1 rrϕ(x,y).
βr(x)
Wesubstituteπ sft(y |x)eβ1 rrϕ(x,y)intotheexpressionof Jlβ hfr(π θβπ):
Z π (y x)
Jlβ hfr(π θβπ)=Ex ∼DprefE y ∼π θβπ(y |x) (cid:20)β rlog βr π(x θβπ) (yβ∗ r |x)|
(cid:21)
=β rEx ∈Dpref −DKL(π θβπ( ·|x) ∥π β∗ r( ·|x))+Z βr(x) .
(cid:104) (cid:105)
B.3.ProofofTheorem3.2
Proof. Wefirstrestatethedefinitionof (π )bysubstitutingEq. (12),(14)into(15):
exo θ
L
K efθ(x,y i) efθ(x,y i) eβ1 rrϕ(x,y i)
Lexo(π θ)=Ex ∼DprefEπsft(y 1:K|x) (cid:34)
(cid:88)i=1
K j=1efθ(x,y j) (cid:16)log K j=1efθ(x,y j) −log K j=1eβ1 rrϕ(x,y j) (cid:17)(cid:35) (25)
Sincef (x,y)=logπβπ(y x) logπ (y x)(cid:80) ,wehavethat: (cid:80) (cid:80)
θ θ | − sft |
K elogπ πθβ sfπ t(( yy ii || xx )) elogπ πθβ sfπ t(( yy ii || xx )) eβ1 rrϕ(x,y i)
Lexo(π θ)=Ex ∼DprefEπsft(y 1:K|x) (cid:34)
(cid:88)i=1 K
elogπ πθβ sfπ t(( yy jj || xx )) (cid:16)log
K
elogπ πθβ sfπ t(( yy jj || xx )) −log K j=1eβ1 rrϕ(x,y j) (cid:17)(cid:35)
j=1 j=1
=Ex ∼DprefEπsft(y 1:K|x) (cid:34)
(cid:88)i=K
1
(cid:80)
K
j=π πθβ 1sfπ t( π( πyy
θβ si
fπi
t|
(| x (x yy))
jj || xx )) (cid:16)log K
j(cid:80) =π πθβ 1sfπ t( π( πyy
θβ si
fπi
t|
(| x (x yy))
jj || xx )) −log K
je =β 11 r e(cid:80) rϕ
β1
r(x r, ϕy (i x)
,y j) (cid:17)(cid:35). (26)
(cid:80) (cid:80) (cid:80)
14TowardsEfficientandExactOptimizationofLanguageModelAlignment
Since {y i}K i=1aresampledfromπ sft( ·|x),whenK →∞,forarbitaryfunctiong :
Y
→R,theestimate K1 K i=1g(y i)is
unbiased,i.e.,lim
K
→∞
K1 K i=1g(y i)=Eπsft(y |x)[g(y)]. Weconsiderthefollowingtwoinstantiationsofg (cid:80)( ·).
Forg(y)= π θβπ(y |x),weh(cid:80) ave:
πsft(y |x)
K πβπ(y x) πβπ(y x)
j=1
πθ
sft(y
jj ||
x)
=KEπsft(y
|x)
(cid:20)πθ
sft(y
||
x)
(cid:21)
(cid:88)
πβπ(y x)
=K π (y x) θ |
sft
| π (y x)
sft
y |
(cid:88)∈Y
=K πβπ(y x)
θ |
y
(cid:88)∈Y
=K.
Forg(y)=eβ1 rrϕ(x,y),wehave:
K
eβ1 rrϕ(x,y j) =KEπsft(y
|x)
eβ1 rrϕ(x,y)
(cid:88)j=1 (cid:104) (cid:105)
=K π sft(y x)eβ1 rrϕ(x,y)
|
y
(cid:88)∈Y
=KZ (x).
βr
Thenwesimplify Lexo bysubstitutingtheexpressionof K j=1 π πθβ sfπ t(( yy jj || xx )) and K j=1eβ1 rrϕ(x,y j) whenK → ∞intoEq.
(26).
(cid:80) (cid:80)
Lexo(π θ)=Ex ∼DprefEπsft(y 1:K|x)
(cid:34)
(cid:88)i=K
1
Kπ θ πβπ sf( t(y yi i|x |x) ) (cid:16)log Kπ θ πβπ sf( t(y yi i|x |x) ) −loge Kβ1 r Zrϕ β( rx (, xy )i)
(cid:17)(cid:35)
1 K πβπ(y x) πβπ(y x)
=Ex ∼DprefEπsft(y 1:K|x) (cid:34)K
(cid:88)i=1
πθ sft(y ii || x) log π sft(y i|x)eθ β1 rrϕ(i x| ,y i)/Z βr(x)(cid:35).
Noticetheanalyticformofπ
β∗
r(y |x)= Zβr1 (x)π sft(y |x)eβ1 rrϕ(x,y),wesubstituteπ
β∗
r
intotheaboveequation:
1 K πβπ(y x) πβπ(y x)
Lexo(π θ)=Ex ∼DprefEπsft(y 1:K|x) (cid:34)K
(cid:88)i=1
πθ sft(y ii || x) log πθ
β∗
r(y ii || x)
(cid:35)
1 K πβπ(y x) πβπ(y x)
=Ex ∼Dpref (cid:34)K (cid:88)i=1Eπsft(y i|x) (cid:20)πθ sft(y ii || x) log πθ
β∗
r(y ii || x)
(cid:21)(cid:35)
1 K πβπ(y x)
=Ex
∼Dpref
(cid:34)K
(cid:88)i=1y
(cid:88)i∈Yπ θβπ(y i|x)log πθ
β∗
r(y
ii ||
x)
(cid:35)
πβπ(y x)
=Ex
∼Dpref
(cid:34)
y
(cid:88)∈Yπ θβπ(y |x)log πθ
β∗
r(y
||
x)
(cid:35)
=Ex ∼Dpref[DKL(π θβπ(y |x) ∥π
β∗
r(y |x))].
Accordingly,bytakingthederivativewithrespecttoθ,wecompletetheproofofEq. (16).
ToproveEq. (17),weutilizeEq. (16)tosubstituteintoEq. (11)tobuildtheconnectionbetween βr(πβπ)and :
Jlhf θ Lexo
Jlβ hfr(π θβπ)= −β r Lexo(π θ)+β rEx ∼Dpref[logZ βr(x)].
15TowardsEfficientandExactOptimizationofLanguageModelAlignment
Thenwetakethegradientwithrespecttotheparametersθoftheaboveformulat:
βr(πβπ)= β (π ),
∇θ Jlhf θ − r ∇θ Lexo θ
whichcompletestheproofofEq. (17).
B.4.ProofofTheorem3.3
Proof. Weutilizethedefinitionofπ θβπ(y |x) ∝π θ(y |x)βππ sft(y |x)1 −βπ inEq. (9)anddividebothsidesbyπ sft(y |x):
πβπ(y x) π (y x) βπ
θ | θ |
π (y x) ∝ π (y x)
sft | (cid:18) sft | (cid:19)
Thenwesubstitutetheaboveequationinto :
dpo-rw
L
K eβ1 rrϕ(x,y i) eβπlog ππ sθ ft( (y yi i| |x x)
)
Ldpo-rw(π θ)=Ex ∼DprefEπsft(y 1:K|x) (cid:34)−
(cid:88)i=1
K j=1eβ1 rrϕ(x,y j) log K j=1eβπlog ππ sθ ft( (y yj j| |x x) )(cid:35)
K (cid:80) eβ1 rrϕ(x,y i) (cid:80) elogπ πθβ sfπ t(( yy ii || xx ))
=Ex ∼DprefEπsft(y 1:K|x) (cid:34)−
(cid:88)i=1
K j=1eβ1 rrϕ(x,y j) log
K
elogπ πθβ sfπ t(( yy jj || xx ))(cid:35)
j=1
=Ex ∼DprefEπsft(y 1:K|x) (cid:34)−
(cid:88)i=K
1
(cid:80)
K
je =β 11 r erϕ
β1
r(x r, ϕy (i x)
,y j)
log(cid:80)
K
j=π πθβ 1sfπ t( π( πyy
θβ si
fπi
t|
(| x (x yy))
jj || xx ))(cid:35) (27)
(cid:80) (cid:80)
Since y K are sampled from π ( x), when K , we follow the proof of Theorem 3.2 and directly give the
{ i}i=1 sft ·| → ∞
followingresults:
K πβπ(y x)
θ j| =K,
π (y x)
j=1 sft j|
(cid:88)
K
eβ1 rrϕ(x,y j) =KZ βr(x).
j=1
(cid:88)
Thenwesimplify Ldpo-rw bysubstitutingtheaboveresultsof K j=1 π πθβ sfπ t(( yy jj || xx )) and K j=1eβ1 rrϕ(x,y j) whenK →∞into
Eq. (27):
(cid:80) (cid:80)
Ldpo-rw(π θ)=Ex ∼DprefEπsft(y 1:K|x) (cid:34)− i=K
1
e Kβ1 r Zrϕ β( rx (, xy )i) log Kπ θ πβπ sf( t(y yi i|x |x) ) (cid:35)
(cid:88)
Notice the analytic form of π
β∗
r(y |x) = Zβr1 (x)π sft(y |x)eβ1 rrϕ(x,y), we rearrange π
β∗
r
and substitute π sft(y |x) =
π
β∗
r(y |x)Z βr(x)e−β1 rrϕ(x,y)intotheaboveequationtosimplifyit:
Ldpo-rw(π θ)=Ex ∼DprefEπsft(y 1:K|x)
(cid:34)−
(cid:88)i=K
1
e Kβ1 r Zrϕ β( rx (, xy )i) log (cid:18)π πθβ β∗π r(( yy ii || xx ))
·
e Kβ1 r Zrϕ β( rx (, xy )i)
(cid:19)(cid:35)
=Ex ∼DprefEπsft(y 1:K|x)
(cid:34)−
(cid:88)i=K
1
e Kβ1 r Zrϕ β( rx (, xy )i) logπ πθβ β∗π r(( yy ii || xx ))
−
(cid:88)i=K
1
e Kβ1 r Zrϕ β( rx (, xy )i) loge Kβ1 r Zrϕ β( rx (, xy )i)
(cid:35)
=Ex ∼DprefEπsft(y 1:K|x)
(cid:34)−
K1 (cid:88)i=K
1
eβ Z1 r βrϕ r( (x x,y )i) logπ πθβ β∗π r(( yy ii || xx ))
K eβ1 rrϕ(x,y i) eβ1 rrϕ(x,y i)
log
−
i=1
K j=1eβ1 rrϕ(x,y j) K j=1eβ1 rrϕ(x,y j)(cid:35)
(cid:88)
(cid:80) (cid:80)
16TowardsEfficientandExactOptimizationofLanguageModelAlignment
Thesecondtermofthefinalequalityremainsconstantinrelationtoθ,andthuscanbeomittedwhencomputingthederivative
withrespecttoθ. Thenwefurtherconsiderthegradientof :
dpo-rw
L
∇θ Ldpo-rw(π θ)= ∇θEx ∼Dpref
(cid:34)−
K1 (cid:88)i=K 1Eπsft(y i|x) (cid:20)eβ Z1 r βrϕ r( (x x,y )i) logπ πθβ β∗π r(( yy ii || xx ))
(cid:21)(cid:35)
= ∇θEx ∼Dpref (cid:34)−Eπsft(y |x)
(cid:20)eβ Z1 r βr rϕ (( xx, )y) logπ πθβ β∗π r(( yy
||
xx ))
(cid:21)(cid:35)
= ∇θEx ∼Dpref
(cid:34)−
y
(cid:88)∈Yπ sft(y
|x)eβ Z1 r βr rϕ (( xx, )y) logπ πθβ β∗π r(( yy
||
xx ))
(cid:35)
πβπ(y x)
= ∇θEx ∼Dpref
(cid:34)−
y
(cid:88)∈Yπ β∗ r(y |x)log πθ
β∗ r(y
||
x) (cid:35)
= ∇θEx ∼Dpref DKL(π β∗ r( ·|x) ∥π θβπ( ·|x)) ,
(cid:104) (cid:105)
whichcompletestheproofofTheorem3.3.
B.5.IllustratingtheRelationshipamongtheObjectivesin§3
EXO KL-Regularized Reward Maximization DPO
(
J<latexit sha1_base64="pefPKMjBFMT9Fgl/SfAQl/ZFJb8=">AAADZnicnVJbaxQxFM7OeGlXbbct4oMvwUWYBV1mRNEXodQKoj5UcNvCZhwy2cxOaOZCkpEu2fxJ33z2xZ9hsh2XTqsIHgg5+U6+71w4ac2ZVGH4vef5N27eur2x2b9z997W9mBn91hWjSB0QipeidMUS8pZSSeKKU5Pa0FxkXJ6kp69cfGTr1RIVpWf1aKmcYHnJcsYwcpCyU5viQqscoK5fm8SjRQ9V5rnmTFfNEqpwokwAapZglRuX79BbSFjRvA1XNHTVL917LTiM7ko7KXPDZKsgGv1Qyfo1EWhbYU2gUEHbB50Bf6SKLisvDDLbqKRmYoE1TkLuviTLmsUw6ew7Wmd9XDd9IePxkz/swC0dMRWSGbqn4TY9T4aJ4NhOA5XBq87UesMQWtHyeAbmlWkKWipCMdSTqOwVrHGQjHCqemjRtIakzM8p1PrlrigMtarNTHwsUVmMKuEPaWCK/QyQ+NCuhLtTzcdeTXmwD/Fpo3KXsWalXWjaEkuEmUNh6qCbufgjAlKFF9YBxPBbK2Q5Fhgouxm9u0QoqstX3eOn42jF+Pw0/Ph/kE7jg3wEDwCAYjAS7AP3oEjMAGk98Pb9Ha9Pe+nv+Xf9x9cfPV6LWcPdMyHvwAHMCPX</latexit> l  hr f(⇡ ✓ ⇡)=Ex ⇠Dpref ⇣E⇡✓ ⇡(y |x)J<latexit sha1_base64="pefPKMjBFMT9Fgl/SfAQl/ZFJb8=">AAADZnicnVJbaxQxFM7OeGlXbbct4oMvwUWYBV1mRNEXodQKoj5UcNvCZhwy2cxOaOZCkpEu2fxJ33z2xZ9hsh2XTqsIHgg5+U6+71w4ac2ZVGH4vef5N27eur2x2b9z997W9mBn91hWjSB0QipeidMUS8pZSSeKKU5Pa0FxkXJ6kp69cfGTr1RIVpWf1aKmcYHnJcsYwcpCyU5viQqscoK5fm8SjRQ9V5rnmTFfNEqpwokwAapZglRuX79BbSFjRvA1XNHTVL917LTiM7ko7KXPDZKsgGv1Qyfo1EWhbYU2gUEHbB50Bf6SKLisvDDLbqKRmYoE1TkLuviTLmsUw6ew7Wmd9XDd9IePxkz/swC0dMRWSGbqn4TY9T4aJ4NhOA5XBq87UesMQWtHyeAbmlWkKWipCMdSTqOwVrHGQjHCqemjRtIakzM8p1PrlrigMtarNTHwsUVmMKuEPaWCK/QyQ+NCuhLtTzcdeTXmwD/Fpo3KXsWalXWjaEkuEmUNh6qCbufgjAlKFF9YBxPBbK2Q5Fhgouxm9u0QoqstX3eOn42jF+Pw0/Ph/kE7jg3wEDwCAYjAS7AP3oEjMAGk98Pb9Ha9Pe+nv+Xf9x9cfPV6LWcPdMyHvwAHMCPX</latexit> [rl    hr f (( x⇡ ,✓  y⇡ )) ]  =  E rx D⇠ KD Lp [r ⇡ef ✓ ⇣ ⇡E ( J<latexit sha1_base64="N8Nz+L3NIKLILms+pFIGcenJOpQ=">AAADOHicjVJbaxQxFM6Ml9b1ttVHX4KLMAu6zIhSX4RSK4iKVHDbwmQcMtnMTmjmQpKRLml+li/+DN/EFx8U8dVfYDJdlk4r6IGQj3PO951LkjWcSRWGXzz/wsVLl9fWrwyuXrt+4+Zw49aerFtB6JTUvBYHGZaUs4pOFVOcHjSC4jLjdD87fObi+x+okKyu3qlFQ5MSzyuWM4KVdaUb3htUYlUQzPVLk2qk6JHSvMiNea9RRhU2AWpYilRh8Rg+hV16lunnLjur+UwuSnvpI4MkK+FKbccJODVRatuRFTRom82DvsBKOjittTDHfemxiUWKmoIFff/9PmucwAew63pVZWc11KvXxsT/XRAdu9QlVebKmH8QEjfdeJIOR+Ek7AyeB9ESjMDSdtPhZzSrSVvSShGOpYyjsFGJxkIxwqkZoFbSBpNDPKexhRUuqUx09/AG3rOeGcxrYU+lYOc9zdC4lK5Fm+n2Ic/GnPNvsbhV+ZNEs6ppFa3ISaG85VDV0P0iOGOCEsUXFmAimO0VkgILTJT9awO7hOjsyOfB3sNJ9HgSvn002tpermMd3AF3QQAisAm2wAuwC6aAeB+9r95374f/yf/m//R/naT63pJzG/TM//0HnCwV1A==</latexit>⇡ y✓  l  |h⇡ xf( ()y ⇡ k|x ✓⇡)) s[ fr t=(  y( Ex |xx, ) ⇠y ] ⌘D)] .p  re( f<latexit sha1_base64="byZFMdWDbw0jEA2HrYhVkXxpGSw=">AAAB/XicbVDLSsNAFJ34rPUVHzs3wSK4Kokouiy6ceGign1AG8pkOkmHTmbCzI1SQ/FX3LhQxK3/4c6/cdJmoa0HLhzOuZd77wkSzjS47re1sLi0vLJaWiuvb2xubds7u00tU0Vog0guVTvAmnImaAMYcNpOFMVxwGkrGF7lfuueKs2kuINRQv0YR4KFjGAwUs/e795IEXEagmLRALBS8qHcsytu1Z3AmSdeQSqoQL1nf3X7kqQxFUA41rrjuQn4GVbACKfjcjfVNMFkiCPaMVTgmGo/m1w/do6M0ndCqUwJcCbq74kMx1qP4sB0xhgGetbLxf+8TgrhhZ8xkaRABZkuClPugHTyKJw+U5QAHxmCiWLmVocMsMIETGB5CN7sy/OkeVL1zqru7WmldlnEUUIH6BAdIw+doxq6RnXUQAQ9omf0it6sJ+vFerc+pq0LVjGzh/7A+vwBtEyVYQ==</latexit>  ⇣r ED1 ) ⇡K ✓(L y[ |⇡ x✓  )J⇡<latexit sha1_base64="N8Nz+L3NIKLILms+pFIGcenJOpQ=">AAADOHicjVJbaxQxFM6Ml9b1ttVHX4KLMAu6zIhSX4RSK4iKVHDbwmQcMtnMTmjmQpKRLml+li/+DN/EFx8U8dVfYDJdlk4r6IGQj3PO951LkjWcSRWGXzz/wsVLl9fWrwyuXrt+4+Zw49aerFtB6JTUvBYHGZaUs4pOFVOcHjSC4jLjdD87fObi+x+okKyu3qlFQ5MSzyuWM4KVdaUb3htUYlUQzPVLk2qk6JHSvMiNea9RRhU2AWpYilRh8Rg+hV16lunnLjur+UwuSnvpI4MkK+FKbccJODVRatuRFTRom82DvsBKOjittTDHfemxiUWKmoIFff/9PmucwAew63pVZWc11KvXxsT/XRAdu9QlVebKmH8QEjfdeJIOR+Ek7AyeB9ESjMDSdtPhZzSrSVvSShGOpYyjsFGJxkIxwqkZoFbSBpNDPKexhRUuqUx09/AG3rOeGcxrYU+lYOc9zdC4lK5Fm+n2Ic/GnPNvsbhV+ZNEs6ppFa3ISaG85VDV0P0iOGOCEsUXFmAimO0VkgILTJT9awO7hOjsyOfB3sNJ9HgSvn002tpermMd3AF3QQAisAm2wAuwC6aAeB+9r95374f/yf/m//R/naT63pJzG/TM//0HnCwV1A==</latexit> [r( l   y h (f|( xx⇡ ,)✓k y)⇡ )=s ]f  t(Ey  x|x D⇠) D K] ⌘p Lr. [e ⇡f ⇣ ✓E (y⇡ |✓ x(y L<latexit sha1_base64="G2b/9OlM/DWZO3vfgtT6DWS9DEM=">AAADfHicnVJdb9MwFHUTPkb56uCRBywKUipolSA2eEGaBkg88DAkuk2qQ+Q4TmrNjiPbgVXBf4Kfxhs/hReEk1Vla5EQXCnKzbnn6px7c9OKM23C8HvP8y9dvnJ161r/+o2bt24Ptu8calkrQqdEcqmOU6wpZyWdGmY4Pa4UxSLl9Cg9edXWjz5RpZksP5hFRWOBi5LljGDjoGS79xUJbOYE8+adTRpk6KlRoskqaW2AKpYgM6cGj+DLjpemzRtHC1AqeaYXwr2aU/vk/OfCJp/XAT5Cmgm4knptP66knN3cWgv7KGVFMYNjxGXh6IXAaJ8VAUSpM9CCEOUKk+a3q2Bd98tFXyPbkVdSOjftVH9rguN/0OT/o7nZ1I466jYQT5LBMJyEXcDNJFomQ7CMg2TwDWWS1IKWhnCs9SwKKxM3WBlGOLV9VGtaYXKCCzpzaYkF1XHTHY+FjxySwVwq95QGduj5jgYL3Rp1zPb36fVaC/6pNqtN/iJuWFnVhpbkTCivOTQStpcIM6YoMXzhEkwUc14hmWO3bePute+WEK2PvJkcPp1EO5Pw/bPh3v5yHVvgHngAAhCB52APvAUHYApI74d33wu8kffTf+g/9sdnVK+37LkLLoS/+wvvASy6</latexit>)| kdx ⇡ p) os[ (r ft⇡  ( ✓( y)x |=x,y ) E]) ⌘(] x.  ,y= w  ,yD2 l)K ⇠L D[ p>tixetal/<=ExkJI6PfizHQ7wZJH8a59YneHXeJf+Ni2bGoXCFARddBprGg21Hg0JCofIRRdcl2VaWd79qPXvW5R6yLLu/IklVspWEEdETWZvYoKEWf2AQTFA4pcmE2gDTo4LdQgBnEpsXLLsYOx7/Evc06ZkYjsDYRdSrioER3X/MBTotUFKwy06jKy3O64F6RASqjFKDYiOTDRU3LxUOGlBRlmes7NuuRnzQIEhJSeQd+9evHhiKO5nJeNwNqyrVvn6YkXKEQfYIKa19y1d9rgBlkxCFyIC9xYybxlkmQdQr53Jfg1qDCkYAY1uDZYoGCc2AEdgalmMUoFFRLPlfe7+y2dnt2apbo89WdlFWTdr34p9ZxUg5mlRvjDcwxTzKSe/G+bXK9zWRLshE8/oEe6+x7uXJSaebsHCJHwcEWYhND2CFOFuVIEjZSTN5+FzXL2NEBNwS9AVbciH+BAAA>"=YA4pQFfxL4Zn5MAcFLn43l/am94"=46esab_1ahs tixetal<⇡ re✓ fL<latexit sha1_base64="G2b/9OlM/DWZO3vfgtT6DWS9DEM=">AAADfHicnVJdb9MwFHUTPkb56uCRBywKUipolSA2eEGaBkg88DAkuk2qQ+Q4TmrNjiPbgVXBf4Kfxhs/hReEk1Vla5EQXCnKzbnn6px7c9OKM23C8HvP8y9dvnJ161r/+o2bt24Ptu8calkrQqdEcqmOU6wpZyWdGmY4Pa4UxSLl9Cg9edXWjz5RpZksP5hFRWOBi5LljGDjoGS79xUJbOYE8+adTRpk6KlRoskqaW2AKpYgM6cGj+DLjpemzRtHC1AqeaYXwr2aU/vk/OfCJp/XAT5Cmgm4knptP66knN3cWgv7KGVFMYNjxGXh6IXAaJ8VAUSpM9CCEOUKk+a3q2Bd98tFXyPbkVdSOjftVH9rguN/0OT/o7nZ1I466jYQT5LBMJyEXcDNJFomQ7CMg2TwDWWS1IKWhnCs9SwKKxM3WBlGOLV9VGtaYXKCCzpzaYkF1XHTHY+FjxySwVwq95QGduj5jgYL3Rp1zPb36fVaC/6pNqtN/iJuWFnVhpbkTCivOTQStpcIM6YoMXzhEkwUc14hmWO3bePute+WEK2PvJkcPp1EO5Pw/bPh3v5yHVvgHngAAhCB52APvAUHYApI74d33wu8kffTf+g/9sdnVK+37LkLLoS/+wvvASy6</latexit> (dy  p|ox l( o⇡) gk✓  )⇡ ⇣=sf  tE( ly o( gx|x,y ⇡⇡) w s✓ f], t⌘(y (yl. y) w⇠ w|D |x xp )r )ef     ll oo gg  ⇡⇡⇣ s✓ f  t( (y ylo l l|g |x x⇡ )⇡ )s ⌘✓ ft(  (y y .w w| |x x) )  log ⇡⇡ s✓ ft( (y yl l| |x x) ) ⌘ .
3 4
L<latexit sha1_base64="AwuKROCoXOMn2GHNfjWv5yRQSrI=">AAACtHicbVFLb9QwEHbCqyyvBY5cIlZIXQ6rBAHtBamCIiHRQ5HYbqV1Ghxn0rXq2JE9QbtK/Qu5cePf4GxDxbYdyZpP33zz8ExeS2Exjv8E4a3bd+7e27o/ePDw0eMnw6fPjqxuDIcp11Kb45xZkELBFAVKOK4NsCqXMMvPPnXx2U8wVmj1HVc1pBU7VaIUnKGnsuEvWjFccCbbA5e1FGGJLSy1c9u0FhnFBSAbf1iL8rz93GlyLQu7qrxrl45aUUWXNfbdSV/DD1E65+b/Mvcvq3892CjuE3LvMs94mhcazzc7jOl5p+5lxp28vlk2TrPhKJ7Ea4uug6QHI9LbYTb8TQvNmwoUcsmsnSdxjWnLDAouwQ1oY6Fm/IydwtxDxSqwabteuoteeaaISm38Uxit2f8zWlbZbjqv7HZgr8Y68qbYvMFyN22FqhsExS8alY2MUEfdBaNCGOAoVx4wboSfNeILZhhHf+eBX0Jy9cvXwdGbSfJuEn97O9r72K9ji7wgL8k2ScgO2SNfyCGZEh4kwSz4EbDwfUhDHsKFNAz6nOdkw0L1F9mb3YM=</latexit> exo(⇡✓)=Ex ⇠Dpref[DKL(⇡ ✓ ⇡( ·|x) k⇡  ⇤ r( ·|x))] Optima L<latexit sha1_base64="9oXGVUKbWo3mXjyy5oNUWuRjC18=">AAADRXicbVJba9RAFJ7EW10v3eqjL4OLsOtlSURpEYRSKxTahwpuW0iyYTKZ7A6dXJg5kS7p/DlffPfNf+CLD4r4qpM0rd3tHgj5OOc75/vmcKJCcAWO882yr12/cfPWyu3Onbv37q921x4cqLyUlI1oLnJ5FBHFBM/YCDgIdlRIRtJIsMPo+F1dP/zEpOJ59hFmBQtSMsl4wikBkwrXLM9PCUwpEdWeDisf2AlUcZG/mLBM675f8NCHKQMywG9xQ42i6n3NjHIRq1lqftWJ9hVP8cWkbT1uJxkvida642/xiXfevn0htLvXaphxRiOUevy079M4h9P5+QP/9L+Vccs2Gb2cPcDPOvNuG41GUyVQi15qmBmC+2ZXL07R3k6/CCtphKZLlc4bn+NF/aB+cBB2e87QaQJfBW4LeqiN/bD71Y9zWqYsAyqIUp7rFBBURAKngpk1looVhB6TCfMMzEjKVFA1V6DxE5OJcZJL82WAm+zljoqkqrZomPVm1GKtTi6reSUkG0HFs6IEltEzoaQUGHJcnxSOuWQUxMwAQiU3XjGdEkkomMPrmCW4i0++Cg5eDt3XQ+fDq97mVruOFfQIPUZ95KJ1tIl20D4aIWp9tr5bP61f9hf7h/3b/nNGta225yGaC/vvP6O+GSA=</latexit>l dp po ol -i gc ey n(⇡✓)=Ex ⇠Dpref DKL(⇡  ⇤r(L<latexit sha1_base64="HSfVypSI8lHYhpesT5tMNogbRzI=">AAADRHicbVJba9RAFJ7EW10v3eqjL4OLsOtlSURRBKHUCoX2oYLbFpNsmEwmu0MnF2ZOtEs6P84Xf4Bv/gJffFDEV3GSprW73QMhH+d853zfHE5UCK7Acb5Z9qXLV65eW7neuXHz1u3V7tqdPZWXkrIRzUUuDyKimOAZGwEHwQ4KyUgaCbYfHb6p6/sfmVQ8z97DrGBBSiYZTzglYFLhmvXBTwlMKRHVjg4rH9gRVHGRP5GftO77BQ99mDIgA/waN8woqt7WxCgXsZql5lcdaV/xFJ8N2tTjdpCxkmitO/4Gn3in7ZtnOts7rYYZZzRCqccP+z6Nczienz/wj/9bGbdsk9HL2QP8qDPvttFoNFUCtei5hpkhuK+29eIU7W31i7CSRmi6VOm08TFe1A/qBwdht+cMnSbwReC2oIfa2A27X/04p2XKMqCCKOW5TgFBRSRwKphZY6lYQeghmTDPwIykTAVVcwQaPzCZGCe5NF8GuMme76hIqmqLhllvRi3W6uSymldC8jKoeFaUwDJ6IpSUAkOO64vCMZeMgpgZQKjkxiumUyIJBXN3HbMEd/HJF8He06H7fOi8e9Zb32jXsYLuofuoj1z0Aq2jLbSLRohan63v1k/rl/3F/mH/tv+cUG2r7bmL5sL++w+SghjF</latexit> ·|d xp )o k-r ⇡w ✓ ( ⇡⇡ (✓ ·) |x= ))E +x ⇠ ED ⇡sp ftre (f yh 1D :KK |xL )( [⇡ H ⇤ (r p( r·|  x () ·|k y⇡ 1✓  :K⇡( ,· x|x ))) ])+E⇡sft(y1:K|x)[H(pr ( ·|y 1:K,x))] i
h i
Solution
Solution
º µRKL º µRKL º Ø§ º µRKL º Ø§ º µFKº LØ§ º µFKL º µFKL
Figure5.Illustrationoftherelationshipamongthedifferentobjectivesdiscussedin§3. 1:Jβr(πβπ)isageneralizedversionofJβ(π )
lhf θ lhf θ
bydistributingtheKLregularizationtoboththelearnedpolicyπ andtherewardmodelr (§3.1). 2:L (π )isderivedbasedonthe
θ ϕ dpo θ
optimalpolicyofJβ(π )(§2.3). 3:L (π )isequivalenttoJβr(πβπ)intermsoftheiroptimizationdirections(§3.2). 4:L is
lhf θ exo θ lhf θ dpo-rw
thegeneralizedversionofL bysubsitutingthepariwiselosswithsoftmaxlossoverKresponses.(§3.3).Theoptimalpolicy,denoted
dpo
byadottedline,assumesunlimitedmodelcapacity.Thesolution,shownwithasolidline,isthepracticallyachievablepolicywithinthe
realisticconstraintsofmodelcapacity.
C.TrainingandEvaluationDetails
Training. Inthecontrolledtextgenerationtask,weuseβ =0.1forEXO andDPO ,andadditionallyuseβ =0.1
π pref pref r
andK 4,8 forEXO andDPO . Forthetasksofsummarization,dialoguegeneration,andinstructionfollowing,
rw rw
∈ { }
weuseβ =0.5forEXO andDPO ,andadditionallyuseβ =0.1andK =4forEXO andDPO . Weprovide
π pref pref r rw rw
additionalresultsoftuningβ andβ inAppendixD.1tojustifyourchoiceofhyperparameters. Inallexperiments,weset
r π
thelabelsmoothinghyperparameterεinEXO to1e-3. ForDPOandEXO,weusetheAdamoptimizerwithauniversal
pref
learningrateof1e-6andabatchsizeof64andtrainforoneepochoneachdataset,althoughbothmethodsconvergewithin
17
() <latexit sha1_base64="byZFMdWDbw0jEA2HrYhVkXxpGSw=">AAAB/XicbVDLSsNAFJ34rPUVHzs3wSK4Kokouiy6ceGign1AG8pkOkmHTmbCzI1SQ/FX3LhQxK3/4c6/cdJmoa0HLhzOuZd77wkSzjS47re1sLi0vLJaWiuvb2xubds7u00tU0Vog0guVTvAmnImaAMYcNpOFMVxwGkrGF7lfuueKs2kuINRQv0YR4KFjGAwUs/e795IEXEagmLRALBS8qHcsytu1Z3AmSdeQSqoQL1nf3X7kqQxFUA41rrjuQn4GVbACKfjcjfVNMFkiCPaMVTgmGo/m1w/do6M0ndCqUwJcCbq74kMx1qP4sB0xhgGetbLxf+8TgrhhZ8xkaRABZkuClPugHTyKJw+U5QAHxmCiWLmVocMsMIETGB5CN7sy/OkeVL1zqru7WmldlnEUUIH6BAdIw+doxq6RnXUQAQ9omf0it6sJ+vFerc+pq0LVjGzh/7A+vwBtEyVYQ==</latexit>
>tixetal/<1NJi5MQ+zHY/AYOfcaxtYmf3nfBneOnerEtnKGFRNcEVbEqqFEt8JAtjISXRck3VqVVndrOv/X1payEv++C4GmcVbWQCiOkprs3cUFSLMGkRr9JcUMtwDKQpU8lOJFYSyMOILLKFdm3/KuZ0mZIBGbH0jzoxEzYivr+ZAK52Kd0HKZrPJAP7ZTAzELZpOCbGRANhwSd3K1kgnOARkBRJv7ImEXhAMZsmr0e177Z9cAaq5gfm4xg3qG/Wee8qaByQH0sAsc7uWeeqYlgTjk8AmACscG27lAn2gp33pu+oFuWBOQiZJqdwHAwalEMMEfobUWCUrliqUl2vvdXZ7uz2bt0NU8WruyKrps978YA4JWo5mlRvjDcwxTzKS62N8z+U6jtiWYTJ57RJ509j2Z/Slw+2YfEJHwcEWYhND2CFOFuVIEjZSTN5+FzXL2NEBNwS9AVbciH+BAAA>"=4ZiyYMXzBJhjGWGII6AKAYJLGQw"=46esab_1ahs tixetal<
,TowardsEfficientandExactOptimizationofLanguageModelAlignment
oneepoch. AllthehyperparametersaresettobethesameforDPOandEXOforafaircomparison. ForPPO,wemodify
basedontheimplementationofDeepSpeed-Chat(Yaoetal.,2023),whichsetsγ =1andβ =0.1bydefault. Wepretrain
thecriticmodelforthefirstfewstepswhilefreezingtheactormodelandfindittoimproveconvergence. Weempirically
tunethenumberofactor-freezingsteps,totaltrainingsteps,learningrateforactorandcriticmodel,andthebatchsizefor
PPOoneachdataset,asPPOissensitivetothesehyperparameters. Specifically,weconduct15trialsofhyperparameter
searchontheIMDBdatasetand10trialsontherealhumanpreferencedatasetsintotal. Intheexperiment,wereportthe
PPOperformancewiththebesthyperparametersobtainedunderconstrainednumberofhyperparametersearchtrials. We
conducttheexperimentsexceptforinstructionfollowingon8V100GPUs. Forinstructionfollowingtask,wetrainthe
modelson8A100GPUs.
Evaluation. At inference time, we sample 4 completions from the learned policy for each prompt and consider 512
promptsfromthetestsetforalldatasets. Exceptfortheinstructionfollowingtask,wesamplefromthepolicywiththesame
temperatureτ =0.8thatissetduringtrainingforconsistency. Fortheinstructionfollowingtask,weusetop-psampling
andempiricallysetp = 0.9andtemperatureτ = 0.95givenitsperformance. Tocalculatethewinrateevaluatedbythe
rewardmodel,weconsiderallcombinationsofpairsbetweenthecompletionsgeneratedbythelearnedpolicyandthebase
completions(eithergeneratedbytheSFTpolicyorthechosencompletioninthedataset)andthencomparethescoresfrom
therewardmodelonthepairsofgenerations. FortheevaluationsusingGPT-4,wesample100promptsand1completion
foreachpromptundereachpolicy. TomitigatethepositionbiasofGPT-4,weevaluateonepairofgenerationstwiceby
swappingtheorderofresponsesineachpair.Toevaluatethequalityofthesummaries,weusetheconcisepromptofRafailov
etal.(2023)asshowninTable3. Toevaluatethehelpfulnessofthegenerateddialogues,weusethepromptshowninTable
4,whichismodifiedbasedonthepromptofRafailovetal.(2023)forsingle-turndialoguetoaccommodatethegeneral
multi-turnsetting. Fortheinstruction-followingtask,weusethepromptmodifiedfromthepromptforreference-guided
pairwisecomparisonprovidedinZhengetal.(2023a).
HumanAssessment. Weconducthumanassessmenttoevaluatetheinstructionfollowingtaskmorethoroughly. Specifi-
cally,weselectthreematchupsthatpairthegeneratedoutputsofEXOwiththoseproducedbyDPO,PPOandtheSFT
policy. Given100randomlysampledtestinstructions, eachmodelgenerates100responses, whichresultsinatotalof
300pairsofcomparisons. Weassign3humanlabelerstoeachcomparison,producing900judgementsintotal. Giventhe
instruction,eachhumanannotatorisprovidedwithtwogeneratedanswersbytwosystemsrespectivelytogetherwitha
high-qualityreferenceanswer. Theannotatoristhenaskedtomakeapreferenceamongwin,tieorlosebycomparingthe
generatedanswerswiththereferenceanswer,consideringthecriteriaincludingadherencetoinstruction,correctness,fluency,
safetyandhelpfulness. Specifically,adherencetoinstructionencapsulatesthemodel’scomprehensionandfollowingofthe
prompt’sintention. Correctnessinvolvestheidentificationofinaccurateknowledgeorlogicalinconsistencieswithinthe
generatedresponses. Fluencyassessesthelinguisticcoherence,encompassinganexaminationofsentencecompleteness,
grammaticalaccuracy,andthepresenceofaconsistentlanguagestructure. Safetyreferstotheinspectionforpotentially
harmfulcontent. Lastly,helpfulnessindicateswhethertheresponsesprovidetheinformationrequiredbythepromptor
contributetoproblemresolution.
D.AdditionalExperimentResults
D.1.PerformancewithVaryingβ andβ
r π
We present an ablation study to investigate the performance of EXO on the dialogue generation task by varying β
rw r
and β respectively. We execute multiple runs bifurcated into two series. We set β = 0.5 as the default value and
π π
vary β 0.1,0.25,0.5,0.75,1.0 . Subsequently, the process is reversed whereby we fix β = 0.1, and alter β
r r π
∈ { } ∈
0.1,0.25,0.5,0.75,1.0 . WepresenttheresultsinFigure6.
{ }
FromFigure6(a),weobservethatreducingeitherβ andβ increasetherewardmodelwinrate,whichisexpectedasa
r π
smallβ β encouragesthepolicytooptimizetherewardmodelwhileneglectingtheKLregularization. InFigure6(b),
r π
×
theGPT-4winratestartstodeclinewhenβ <0.5,whichsuggestswheretherewardmodelstartstobeover-optimized.
π
Thedifferenteffectsoftuningβ andβ ontheperformancecouldbeattributedtotheirdifferentrolesinouralgorithm,i.e.,
π r
β scalesthelogprobabilityintheparametrizedpolicywhileβ scalesthereward. Basedontheresults,werecommend
π r
adoptingamoderatevalueforβ andalowervalueforβ ,forinstance,β =0.5andβ =0.1.
π r π r
18TowardsEfficientandExactOptimizationofLanguageModelAlignment
Which of the following summaries does a better job of summarizing the most important
points in the given forum post, without including unimportant or irrelevant details? A
good summary is both precise and concise.
Post:
<post>
Summary A:
<Summary A>
Summary B:
<Summary B>
FIRST provide a one-sentence comparison of the two summaries, explaining which you prefer
and why. SECOND, on a new line, state only "A" or "B" to indicate your choice. Your
response should use the format:
Comparison: <one-sentence comparison and explanation>
Preferred: <"A" or "B">
Table3.PromptforGPT-4evaluationonthesummarizationtask.Textsinblueareplaceholderstobesubstitutedbytherealdata.
For the following dialogue history to a chatbot, which response is more helpful?
Dialogue history:
<dialogue history>
Response A:
<Response A>
Response B: <Response B>
FIRST provide a one-sentence comparison of the two responses and explain which you feel is
more helpful. SECOND, on a new line, state only "A" or "B" to indicate which response is
more helpful. Your response should use the format:
Comparison: <one-sentence comparison and explanation>
More helpful: <"A" or "B">
Table4.PromptforGPT-4evaluationonthedialoguegenerationtask.Textsinblueareplaceholderstobesubstitutedbytherealdata.
90
vsSFT 85 vsSFT 80 80 vsSFT
vsChosen vsChosen vsChosen
88 80 70
70
60
86 75
60 50
70 vsSFT
84 50 vsChosen 40
65
0.10 0.25 0.50 0.75 1.00 0.10 0.25 0.50 0.75 1.00 0.10 0.25 0.50 0.75 1.00 0.10 0.25 0.50 0.75 1.00
βπ βr βπ βr
Rewardmodelwinrate. GPT-4winrate.
Figure6. WinrateperformanceofEXO evaluatedbytherewardmodelandtheGPT-4byvaryingβ andβ .
rw r π
D.2.FrontierofClassifierAccuarcyagainstKLDivergence
Weadditionallycalculatetheaccuracyofthesentimentclassifierbytakingthesigmoidoftheoraclerewardandplotthe
frontieroftheaccuracyvsreverseKLinFigure7.
19
)%(etarniwledomdrawer )%(etarniwledomdrawer
)%(etarniw4-TPG )%(etarniw4-TPGTowardsEfficientandExactOptimizationofLanguageModelAlignment
100
90
PPO
EXOpref
80 EXOrw(K=4)
EXOrw(K=8)
DPOpref
70
DPOrw(K=4)
DPOrw(K=8)
60
0 2 4 6
DKL(π ||π sft)
Figure7.ThefrontierofclassifieraccuracyvsreverseKLtotheSFTpolicyofdifferentmethodsinthecontrolledexperiment.
D.3.OptimizationEfficiency
Todemonstratetheefficencyofoptimizingtheoraclereward,weplotthecurveoftheoraclerewardwithstandarddeviation
onthetestsetofdifferenttrainingsteps. Specifically,thestandarddeviationiscalculatedonthe4samplesgeneratedgiven
thesamepromptandthenaveragedacross512prompts.
8
6 PPO
DPOpref
DPOrw(K=4)
4
DPOrw(K=8)
EXOpref
EXOrw(K=4)
EXOrw(K=8)
2
0
10 20 30 40 50 60 70 80 90 100
steps
Figure8.Thecurveoftheoraclerewardwithstandarddeviationonthetestsetversusthetrainingstepsofdifferentmethodsinthe
controlledexperiment.
D.4.AdditionalVisualizationResults
WeadditionallysamplemorepromptsfromthetestsetoftheIMDbdataset,andplottheestimateddensityratioofthe
policylearnedbyEXOandDPOandtheoptimalpolicyinFigure9. Similarresultswereobtainedamongthesediverseset
ofprompts: thedensityratioundertheEXOpolicybettermatchestheoptimalpolicyatthehighprobabilityregionagainst
π thantheDPOpolicy. Andveryappearently,theDPOpolicytendstooverestimatethelongtailinπ andthusoften
sft sft
losesresolutionincapturingthemodesoftheoptimalpolicy.
D.5.SampleOutputswithGPT-4Judgments
WepresentthesampleoutputsgeneratedbyEXOandDPOandthejudgmentsmadebyGPT-4whencomparedtothechosen
responseinthepreferencedataset. Table5andTable6showexamplesonthetaskofdialoguegenerationandsummarization
respectively.
20
drawerelcaro
ycaruccaTowardsEfficientandExactOptimizationofLanguageModelAlignment
E.LimitationsandFutureWork
Wediscussthepotentiallimitationsofthelanguagemodelalignmentframeworkinthepresentstudyandsuggestpossible
directionsforfuturework. Atthecoreofthealignmentframeworkisthedistributionalregularizationoverthepolicythat
intendstopreventthepolicyfromgreedilymaximizingouttherewardmodel. Despitetheregularization,ourexperiments
stillrevealinstancesofrewardover-optimization,aphenomenonpossiblyduetoinsufficientfocusontherewardmodel
estimationanduse. Thisopenspossibleavenuestoimprovethealignmentframeworkatabroaderscope. Forinstance,rather
thanrelyingonastaticpreferencedatasetforrewardmodeltraining,itcouldbemoreeffectivetodynamicallyimprovethe
rewardmodelwiththedevelopmentofthepolicy,therebyofferingmoreprecisefeedback. Uponthecurrentregularization
thatsolelyfocusesonproximitytotheinitialpolicy,onecantakeintoaccounttheuncertaintyoftherewardmodeloutputto
restricttheexploitationofthepolicy. Itisbeneficialtotakeintoaccounttheseaspectstodevelopefficientandeffective
methodtowardscloseralignmentwithhumanpreferences. Additionally,whilewealreadyevaluatedEXOonadvanced
languagemodelsupto6Bonrealisticscenarios,scalingEXOtomodelsthatareordersofmagnitudelargercanpresent
profoundimplications. Attheotherendofthespectrum,systematicallydissectingandcomparingPPO,DPOandEXO,
particularlyregardingtheirvarianceandbiasduringoptimizationisessentialtobroadenourunderstandingofthesemethods.
21TowardsEfficientandExactOptimizationofLanguageModelAlignment
60 100
optimal optimal
EXO EXO
40
DPO DPO
50
20
0 0
1200 1000 800 600 400 200 0 1200 1000 800 600 400 200 0
− − − − − − − − − − − −
logπ (yx) logπ (yx)
sft | sft |
EstimateddensityratiooftheEXO,DPOandoptimalpolicy EstimateddensityratiooftheEXO,DPOandoptimalpolicy
giventheprompt“Isthissupposedtobeserious?Ihopenot”. giventheprompt“Greatbook,greatmovie,greatsoundtrack.
Frank”.
30
optimal 40 optimal
EXO EXO
20
DPO DPO
20
10
0 0
1200 1000 800 600 400 200 0 1000 800 600 400 200 0
− − − − − − − − − − −
logπ (yx) logπ (yx)
sft | sft |
EstimateddensityratiooftheEXO,DPOandoptimalpolicy EstimateddensityratiooftheEXO,DPOandoptimalpolicy
given the prompt “This is indeed the film that popularized giventheprompt“Thismovieisaboutagroupofpeoplewho
kung”. are”.
75
60
optimal optimal
EXO EXO
50
40
DPO DPO
20 25
0 0
1200 1000 800 600 400 200 0 1400 1200 1000 800 600 400 200 0
− − − − − − − − − − − − −
logπ (yx) logπ (yx)
sft | sft |
EstimateddensityratiooftheEXO,DPOandoptimalpolicy EstimateddensityratiooftheEXO,DPOandoptimalpolicy
giventheprompt“WhatwehaveherethestandardDisney giventheprompt“Oncetheslowbeginninggetsunderway,the
directtoDVD”. filmkicks”.
Figure9.VisualizationoftheestimateddensityratioofthepolicylearnedbyEXOandDPO,andtheoptimalpolicygiven10testprompts
randomlysampledfromtheIMDbdataset.
22TowardsEfficientandExactOptimizationofLanguageModelAlignment
Dialoguehistory
Human:HowdoImakeafishtailbraid?
Assistant:Heyhuman,justwalkdowntheleftandthendowntherightside,thenattheend,youpullthrough.Areyou
lookingtojustbraid,oratightponytail?
Human:Yes.Canyoutellmethestepstodoit?
Chosenresponse
Assistant:Sure,here’showtodoasidebraid:
1)Pullhairuptothebackofyourhead,leavingitoutfromtherestofyourhairsoyoucanseeit.
2)Putonehaironthesidewhereyouwanttomakeyourbraid.
3)Holdthebraidwithyourthumbandforefinger
4)Bringyourbraidthroughtheoppositeside(puttingthehairtotheothersideofthebraid.
5)Repeatsteps2,3,and4asneededuntilyou’vecompletedyourbraid.
EXOresponse
Assistant:Sure!I’llwalkyouthroughitstepbystep:
1. Standwithyourlefthandonthebackofyourhead. Grabaleftsidesectionofhair,usingtherighthandtopullit
through.
2.Takearightsidesectionofhair,andpullitthrough
3.Grabaleftsidesectionofhair,andpullitthrough
4.Grabarightsidesectionofhair,andpullitthrough
5.Repeatsteps2-4forthelastsectionofhair
6.Wrapthehairaroundyourheadandsecureitatthenapeofyourneck.
GPT-4Judgment(A=EXO,B=Chosen)
ResponseAprovidesamoredetailedandstep-by-stepguideonhowtomakeafishtailbraid,whileResponseBseemsto
describeadifferenttypeofbraidandislessclear.
GPT-4Judgment(A=Chosen,B=EXO)
ResponseBismorehelpfulbecauseitprovidesaclearer,step-by-stepguideonhowtomakeafishtailbraid,whereas
ResponseAseemstodescribeadifferenttypeofbraidandislessclear.
DPOresponse
Assistant:Sure,let’swalkthroughit.You’llwanttostartwithabraidabouttwoandahalfincheslong,thenyouwantto
braidittogetherandpullthrough,whichmeansyou’llsomehowthreaditthroughyourownhairandtieitoffattheother
end.Thenyou’regoingtobraiditagain,butthistimeitwillbeafishtail,sinceyou’llpullitthrough.You’llkeepdoing
this,braidingthefishtail.
GPT-4Judgment(A=DPO,B=Chosen)
ResponseBismorehelpfulasitprovidesaclear,step-by-stepguideonhowtomakeafishtailbraid,whileResponseA
isconfusingandlacksclarity.
GPT-4Judgment(A=Chosen,B=DPO)
ResponseAprovidesaclear,step-by-stepguideonhowtomakeasidebraid,whileResponseBgivesavaguedescription
ofhowtomakeafishtailbraidwithoutclearsteps.
Table5.ResponsesgeneratedbyEXOandDPOgiventhedialoguehistorysampledfromtheAnthropic-HHtestset.GPT-4consistently
prefersEXOanddisprefersDPOoverthechosenresponseregardlessoftheorderinwhichtheevaluatedpairsarepresented.
23TowardsEfficientandExactOptimizationofLanguageModelAlignment
Post
SUBREDDIT:r/Dogtraining
TITLE:Bestwalkswhenonachokechain:dogisn’tbeingchoked,butisitbringingupbadmemories?
POST:My3yearoldGSDxHuskyrescuestayedwithsomerelativesforafewdayswhilemyboyfriendandImoved
house,andtherelativesfoundanoldchokechaintowalkhimwith,insteadofhisMekutiharness. Wehavehadreal
problemswithhimpullingonthelead,andtheMekutiharnesshasworkedwonders,butdidn’teliminatetheproblem.
Hehasn’tpulledwhenonthechokechainatall,evenwhenitwasfirstputonapparently,sowethinkthatiswhathewas
trainedon(anddamagefromthatprocessisprobablywhyhisbarksandwhinessoundabitodd). Wenowthinkthe
peoplewhohadhimbeforeuswereviolenttowardshim(ifyoumimekickingherunsacrosstheroomterrified),so,my
questionis:heisbestbehavedonwalkswhenwearingachokechain,anditisn’thurtinghimatall,butwilltheharm
chokersandthepeopleusingthemcausedhiminthepastaffecthiminanywaynow?I100%DONOTwanttousea
chokechainonhimandIdon’tthinkanyoneshouldever,evertrainadogthatway,butthischainistheonlyleadwe’ve
usedthathascompletelysolvedtheproblemwithhimpulling,andhehasn’tseemedstressedoutbyit.Ifheeverstarted
pullingontheleadsothatthechokerwastight,Iwouldstopusingitstraightaway.ThelastthingIwantismydogbeing
hurt.IfI’dknowntheserelativesweregoingtoputoneonhim,Iwouldn’thaveletthemlookafterhim.Still,itworks
anddoesn’thurthim,myonlyworryisit’lldopsychologicaldamagesomehow.
Chosensummary
TL;DR:3yroldhuskyrescuepullingonchokechainw/nodamage,butusingitonwalksoccasionallycauseshimstress.
Canthisdamageactuallycauseharminthefuture?
EXOsummary
TL;DR:Bestwalksareonachokechainbutdogisn’tpullingonlead,hashadnoproblemswithpullbefore,notsureif
pastviolenttreatmentwillaffecthimnow,doesitcauseharmtodogsbeingonchokechains/whenbeingwalkedon
them?
GPT-4Judgment(A=EXO,B=Chosen)
SummaryAprovidesamorecomprehensiveandaccurateportrayalofthepost’spoints,despiteafewmisplaceddetails,
whereasSummaryBincorrectlystatesthedogisshowingstressandpullingonthechokechain.
GPT-4Judgment(A=Chosen,B=EXO)
SummaryBispreferredbecauseitmoreaccuratelysummarizesthemainpointsoftheuser’spostbymentioningthe
chokechain’seffectonwalks,mentioningtheabsenceofnegativeeffectsofpullsofar,andraisingconcernsaboutpast
violenttreatmentpotentiallyaffectingthedog’sbehavior.
DPOsummary
TL;DR:Ifyoutookadogandhaditwearchokesit’sself-choked,willitbetraumatizedinanywaywhenyouputiton
andnolongertrainittobechoked?
GPT-4Judgment(A=DPO,B=Chosen)
SummaryBispreferredasitcapturesthekeydetailsmoreaccurately,includingthedog’sbreed,theissueofpulling,and
thepotentialfutureharm,whileSummaryAisambiguousandlossalotofcontext.
GPT-4Judgment(A=Chosen,B=DPO)
SummaryAismoreaccurateanddetailedbecauseitcorrectlydescribesthespecificsituationofthedogandtheconcerns
associatedwithusingachokechain,whileSummaryBisvagueandconfusinginitswording.
Table6.SummariesgeneratedbyEXOandDPOgiventhepostsampledfromtheTL;DRtestset.GPT-4consistentlyprefersEXOand
disprefersDPOoverthechosensummaryregardlessoftheorderinwhichtheevaluatedpairsarepresented.
24