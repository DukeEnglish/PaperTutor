[
    {
        "title": "Early Time Classification with Accumulated Accuracy Gap Control",
        "authors": "Liran RingelRegev CohenDaniel FreedmanMichael EladYaniv Romano",
        "links": "http://arxiv.org/abs/2402.00857v1",
        "entry_id": "http://arxiv.org/abs/2402.00857v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00857v1",
        "summary": "Early time classification algorithms aim to label a stream of features\nwithout processing the full input stream, while maintaining accuracy comparable\nto that achieved by applying the classifier to the entire input. In this paper,\nwe introduce a statistical framework that can be applied to any sequential\nclassifier, formulating a calibrated stopping rule. This data-driven rule\nattains finite-sample, distribution-free control of the accuracy gap between\nfull and early-time classification. We start by presenting a novel method that\nbuilds on the Learn-then-Test calibration framework to control this gap\nmarginally, on average over i.i.d. instances. As this algorithm tends to yield\nan excessively high accuracy gap for early halt times, our main contribution is\nthe proposal of a framework that controls a stronger notion of error, where the\naccuracy gap is controlled conditionally on the accumulated halt times.\nNumerical experiments demonstrate the effectiveness, applicability, and\nusefulness of our method. We show that our proposed early stopping mechanism\nreduces up to 94% of timesteps used for classification while achieving rigorous\naccuracy gap control.",
        "updated": "2024-02-01 18:54:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00857v1"
    },
    {
        "title": "Score-based Causal Representation Learning: Linear and General Transformations",
        "authors": "Burak VarıcıEmre AcartürkKarthikeyan ShanmugamAbhishek KumarAli Tajer",
        "links": "http://arxiv.org/abs/2402.00849v1",
        "entry_id": "http://arxiv.org/abs/2402.00849v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00849v1",
        "summary": "This paper addresses intervention-based causal representation learning (CRL)\nunder a general nonparametric latent causal model and an unknown transformation\nthat maps the latent variables to the observed variables. Linear and general\ntransformations are investigated. The paper addresses both the\n\\emph{identifiability} and \\emph{achievability} aspects. Identifiability refers\nto determining algorithm-agnostic conditions that ensure recovering the true\nlatent causal variables and the latent causal graph underlying them.\nAchievability refers to the algorithmic aspects and addresses designing\nalgorithms that achieve identifiability guarantees. By drawing novel\nconnections between \\emph{score functions} (i.e., the gradients of the\nlogarithm of density functions) and CRL, this paper designs a \\emph{score-based\nclass of algorithms} that ensures both identifiability and achievability.\nFirst, the paper focuses on \\emph{linear} transformations and shows that one\nstochastic hard intervention per node suffices to guarantee identifiability. It\nalso provides partial identifiability guarantees for soft interventions,\nincluding identifiability up to ancestors for general causal models and perfect\nlatent graph recovery for sufficiently non-linear causal models. Secondly, it\nfocuses on \\emph{general} transformations and shows that two stochastic hard\ninterventions per node suffice for identifiability. Notably, one does\n\\emph{not} need to know which pair of interventional environments have the same\nnode intervened.",
        "updated": "2024-02-01 18:40:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00849v1"
    },
    {
        "title": "BootsTAP: Bootstrapped Training for Tracking-Any-Point",
        "authors": "Carl DoerschYi YangDilara GokayPauline LucSkanda KoppulaAnkush GuptaJoseph HeywardRoss GoroshinJoão CarreiraAndrew Zisserman",
        "links": "http://arxiv.org/abs/2402.00847v1",
        "entry_id": "http://arxiv.org/abs/2402.00847v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00847v1",
        "summary": "To endow models with greater understanding of physics and motion, it is\nuseful to enable them to perceive how solid surfaces move and deform in real\nscenes. This can be formalized as Tracking-Any-Point (TAP), which requires the\nalgorithm to be able to track any point corresponding to a solid surface in a\nvideo, potentially densely in space and time. Large-scale ground-truth training\ndata for TAP is only available in simulation, which currently has limited\nvariety of objects and motion. In this work, we demonstrate how large-scale,\nunlabeled, uncurated real-world data can improve a TAP model with minimal\narchitectural changes, using a self-supervised student-teacher setup. We\ndemonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing\nprevious results by a wide margin: for example, TAP-Vid-DAVIS performance\nimproves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.",
        "updated": "2024-02-01 18:38:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00847v1"
    },
    {
        "title": "Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI",
        "authors": "Theodore PapamarkouMaria SkoularidouKonstantina PallaLaurence AitchisonJulyan ArbelDavid DunsonMaurizio FilipponeVincent FortuinPhilipp HennigAliaksandr HubinAlexander ImmerTheofanis KaraletsosMohammad Emtiyaz KhanAgustinus KristiadiYingzhen LiJose Miguel Hernandez LobatoStephan MandtChristopher NemethMichael A. OsborneTim G. J. RudnerDavid RügamerYee Whye TehMax WellingAndrew Gordon WilsonRuqi Zhang",
        "links": "http://arxiv.org/abs/2402.00809v1",
        "entry_id": "http://arxiv.org/abs/2402.00809v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00809v1",
        "summary": "In the current landscape of deep learning research, there is a predominant\nemphasis on achieving high predictive accuracy in supervised tasks involving\nlarge image and language datasets. However, a broader perspective reveals a\nmultitude of overlooked metrics, tasks, and data types, such as uncertainty,\nactive and continual learning, and scientific data, that demand attention.\nBayesian deep learning (BDL) constitutes a promising avenue, offering\nadvantages across these diverse settings. This paper posits that BDL can\nelevate the capabilities of deep learning. It revisits the strengths of BDL,\nacknowledges existing challenges, and highlights some exciting research avenues\naimed at addressing these obstacles. Looking ahead, the discussion focuses on\npossible ways to combine large-scale foundation models with BDL to unlock their\nfull potential.",
        "updated": "2024-02-01 17:45:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00809v1"
    },
    {
        "title": "Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics",
        "authors": "Eyup B. UnluMarçal Comajoan CaraGopal Ramesh DahaleZhongtian DongRoy T. ForestanoSergei GleyzerDaniel JusticeKyoungchul KongTom MagorschKonstantin T. MatchevKatia Matcheva",
        "links": "http://arxiv.org/abs/2402.00776v1",
        "entry_id": "http://arxiv.org/abs/2402.00776v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00776v1",
        "summary": "Models based on vision transformer architectures are considered\nstate-of-the-art when it comes to image classification tasks. However, they\nrequire extensive computational resources both for training and deployment. The\nproblem is exacerbated as the amount and complexity of the data increases.\nQuantum-based vision transformer models could potentially alleviate this issue\nby reducing the training and operating time while maintaining the same\npredictive power. Although current quantum computers are not yet able to\nperform high-dimensional tasks yet, they do offer one of the most efficient\nsolutions for the future. In this work, we construct several variations of a\nquantum hybrid vision transformer for a classification problem in high energy\nphysics (distinguishing photons and electrons in the electromagnetic\ncalorimeter). We test them against classical vision transformer architectures.\nOur findings indicate that the hybrid models can achieve comparable performance\nto their classical analogues with a similar number of parameters.",
        "updated": "2024-02-01 17:05:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00776v1"
    }
]