[
    {
        "title": "WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework",
        "authors": "Xiang ZhangJingyang HuangHuan YanPeng ZhaoGuohang ZhuangZhi LiuBin Liu",
        "links": "http://arxiv.org/abs/2402.00822v1",
        "entry_id": "http://arxiv.org/abs/2402.00822v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00822v1",
        "summary": "Recent years have witnessed a growing interest in Wi-Fi-based gesture\nrecognition. However, existing works have predominantly focused on closed-set\nparadigms, where all testing gestures are predefined during training. This\nposes a significant challenge in real-world applications, as unseen gestures\nmight be misclassified as known classes during testing. To address this issue,\nwe propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR)\nframework. Implementing OSGR requires addressing challenges caused by the\nunique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and\ndomains, leads to widely scattered and irregular data distributions in\ncollected Wi-Fi sensing data. Consequently, data ambiguity between classes and\nchallenges in defining appropriate decision boundaries to identify unknowns\narise. To tackle these challenges, WiOpen adopts a two-fold approach to\neliminate uncertainty and define precise decision boundaries. Initially, it\naddresses uncertainty induced by noise during data preprocessing by utilizing\nthe CSI ratio. Next, it designs the OSGR network based on an uncertainty\nquantification method. Throughout the learning process, this network\neffectively mitigates uncertainty stemming from domains. Ultimately, the\nnetwork leverages relationships among samples' neighbors to dynamically define\nopen-set decision boundaries, successfully realizing OSGR. Comprehensive\nexperiments on publicly accessible datasets confirm WiOpen's effectiveness.\nNotably, WiOpen also demonstrates superiority in cross-domain tasks when\ncompared to state-of-the-art approaches.",
        "updated": "2024-02-01 18:05:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00822v1"
    },
    {
        "title": "Examining the Influence of Digital Phantom Models in Virtual Imaging Trials for Tomographic Breast Imaging",
        "authors": "Amar KavuriMini Das",
        "links": "http://arxiv.org/abs/2402.00812v1",
        "entry_id": "http://arxiv.org/abs/2402.00812v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00812v1",
        "summary": "Purpose: Digital phantoms are one of the key components of virtual imaging\ntrials (VITs) that aim to assess and optimize new medical imaging systems and\nalgorithms. However, these phantoms vary in their voxel resolution, appearance,\nand structural details. This study aims to examine whether and how variations\nbetween digital phantoms influence system optimization with digital breast\ntomosynthesis (DBT) as a chosen modality. Methods: We selected widely used and\nopen-access digital breast phantoms generated with different methods. For each\nphantom type, we created an ensemble of DBT images to test acquisition\nstrategies. Human observer localization ROC (LROC) was used to assess observer\nperformance studies for each case. Noise power spectrum (NPS) was estimated to\ncompare the phantom structural components. Further, we computed several gaze\nmetrics to quantify the gaze pattern when viewing images generated from\ndifferent phantom types. Results: Our LROC results show that the arc samplings\nfor peak performance were approximately 2.5 degrees and 6 degrees in Bakic and\nXCAT breast phantoms respectively for 3-mm lesion detection tasks and indicate\nthat system optimization outcomes from VITs can vary with phantom types and\nstructural frequency components. Additionally, a significant correlation (p=\n0.01) between gaze metrics and diagnostic performance suggests that gaze\nanalysis can be used to understand and evaluate task difficulty in VITs.",
        "updated": "2024-02-01 17:49:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00812v1"
    },
    {
        "title": "Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario",
        "authors": "Marta MondelliniMatteo Lavit NicoraPooja PrajodElisabeth AndréRocco VertechyAlessandro AntoniettiMatteo Malosio",
        "links": "http://arxiv.org/abs/2402.00808v1",
        "entry_id": "http://arxiv.org/abs/2402.00808v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00808v1",
        "summary": "In industrial scenarios, there is widespread use of collaborative robots\n(cobots), and growing interest is directed at evaluating and measuring the\nimpact of some characteristics of the cobot on the human factor. In the present\npilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 -\nAdapted to the participant's pace) of a cobot has on the Experiential Locus of\nControl (ELoC) and the emotional state of 31 participants has been examined.\nThe operators' performance, the degree of basic internal Locus of Control, and\nthe attitude towards the robots were also considered. No difference was found\nregarding the emotional state and the ELoC in the three conditions, but\nconsidering the other psychological variables, a more complex situation\nemerges. Overall, results seem to indicate a need to consider the person's\npsychological characteristics to offer a differentiated and optimal interaction\nexperience.",
        "updated": "2024-02-01 17:44:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00808v1"
    },
    {
        "title": "Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction",
        "authors": "Rohan AlurManish RaghavanDevavrat Shah",
        "links": "http://arxiv.org/abs/2402.00793v1",
        "entry_id": "http://arxiv.org/abs/2402.00793v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00793v1",
        "summary": "We introduce a novel framework for incorporating human expertise into\nalgorithmic predictions. Our approach focuses on the use of human judgment to\ndistinguish inputs which `look the same' to any feasible predictive algorithm.\nWe argue that this framing clarifies the problem of human/AI collaboration in\nprediction tasks, as experts often have access to information -- particularly\nsubjective information -- which is not encoded in the algorithm's training\ndata. We use this insight to develop a set of principled algorithms for\nselectively incorporating human feedback only when it improves the performance\nof any feasible predictor. We find empirically that although algorithms often\noutperform their human counterparts on average, human judgment can\nsignificantly improve algorithmic predictions on specific instances (which can\nbe identified ex-ante). In an X-ray classification task, we find that this\nsubset constitutes nearly 30% of the patient population. Our approach provides\na natural way of uncovering this heterogeneity and thus enabling effective\nhuman-AI collaboration.",
        "updated": "2024-02-01 17:23:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00793v1"
    },
    {
        "title": "To Search or To Gen? Exploring the Synergy between Generative AI and Web Search in Programming",
        "authors": "Ryan YenNicole SultanumJian Zhao",
        "links": "http://arxiv.org/abs/2402.00764v1",
        "entry_id": "http://arxiv.org/abs/2402.00764v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00764v1",
        "summary": "The convergence of generative AI and web search is reshaping problem-solving\nfor programmers. However, the lack of understanding regarding their interplay\nin the information-seeking process often leads programmers to perceive them as\nalternatives rather than complementary tools. To analyze this interaction and\nexplore their synergy, we conducted an interview study with eight experienced\nprogrammers. Drawing from the results and literature, we have identified three\nmajor challenges and proposed three decision-making stages, each with its own\nrelevant factors. Additionally, we present a comprehensive process model that\ncaptures programmers' interaction patterns. This model encompasses\ndecision-making stages, the information-foraging loop, and cognitive activities\nduring system interaction, offering a holistic framework to comprehend and\noptimize the use of these convergent tools in programming.",
        "updated": "2024-02-01 16:53:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00764v1"
    }
]