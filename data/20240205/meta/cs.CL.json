[
    {
        "title": "Evaluating Large Language Models for Generalization and Robustness via Data Compression",
        "authors": "Yucheng LiYunhao GuoFrank GuerinChenghua Lin",
        "links": "http://arxiv.org/abs/2402.00861v1",
        "entry_id": "http://arxiv.org/abs/2402.00861v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00861v1",
        "summary": "Existing methods for evaluating large language models face challenges such as\ndata contamination, sensitivity to prompts, and the high cost of benchmark\ncreation. To address this, we propose a lossless data compression based\nevaluation approach that tests how models' predictive abilities generalize\nafter their training cutoff. Specifically, we collect comprehensive test data\nspanning 83 months from 2017 to 2023 and split the data into training and\ntesting periods according to models' training data cutoff. We measure: 1) the\ncompression performance on the testing period as a measure of generalization on\nunseen data; and 2) the performance gap between the training and testing period\nas a measure of robustness. Our experiments test 14 representative large\nlanguage models with various sizes on sources including Wikipedia, news\narticles, code, arXiv papers, and multi-modal data. We find that the\ncompression rate of many models reduces significantly after their cutoff date,\nbut models such as Mistral and Llama-2 demonstrate a good balance between\nperformance and robustness. Results also suggest that models struggle to\ngeneralize on news and code data, but work especially well on arXiv papers. We\nalso find the context size and tokenization implementation have a big impact of\non the overall compression performance.",
        "updated": "2024-02-01 18:56:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00861v1"
    },
    {
        "title": "Can Large Language Models Understand Context?",
        "authors": "Yilun ZhuJoel Ruben Antony MonizShruti BhargavaJiarui LuDhivya PiraviperumalSite LiYuan ZhangHong YuBo-Hsiang Tseng",
        "links": "http://arxiv.org/abs/2402.00858v1",
        "entry_id": "http://arxiv.org/abs/2402.00858v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00858v1",
        "summary": "Understanding context is key to understanding human language, an ability\nwhich Large Language Models (LLMs) have been increasingly seen to demonstrate\nto an impressive extent. However, though the evaluation of LLMs encompasses\nvarious domains within the realm of Natural Language Processing, limited\nattention has been paid to probing their linguistic capability of understanding\ncontextual features. This paper introduces a context understanding benchmark by\nadapting existing datasets to suit the evaluation of generative models. This\nbenchmark comprises of four distinct tasks and nine datasets, all featuring\nprompts designed to assess the models' ability to understand context. First, we\nevaluate the performance of LLMs under the in-context learning pretraining\nscenario. Experimental results indicate that pre-trained dense models struggle\nwith understanding more nuanced contextual features when compared to\nstate-of-the-art fine-tuned models. Second, as LLM compression holds growing\nsignificance in both research and real-world applications, we assess the\ncontext understanding of quantized models under in-context-learning settings.\nWe find that 3-bit post-training quantization leads to varying degrees of\nperformance reduction on our benchmark. We conduct an extensive analysis of\nthese scenarios to substantiate our experimental results.",
        "updated": "2024-02-01 18:55:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00858v1"
    },
    {
        "title": "Towards Efficient and Exact Optimization of Language Model Alignment",
        "authors": "Haozhe JiCheng LuYilin NiuPei KeHongning WangJun ZhuJie TangMinlie Huang",
        "links": "http://arxiv.org/abs/2402.00856v2",
        "entry_id": "http://arxiv.org/abs/2402.00856v2",
        "pdf_url": "http://arxiv.org/pdf/2402.00856v2",
        "summary": "The alignment of language models with human preferences is vital for their\napplication in real-world tasks. The problem is formulated as optimizing the\nmodel's policy to maximize the expected reward that reflects human preferences\nwith minimal deviation from the initial policy. While considered as a\nstraightforward solution, reinforcement learning (RL) suffers from high\nvariance in policy updates, which impedes efficient policy improvement.\nRecently, direct preference optimization (DPO) was proposed to directly\noptimize the policy from preference data. Though simple to implement, DPO is\nderived based on the optimal policy that is not assured to be achieved in\npractice, which undermines its convergence to the intended solution.\n  In this paper, we propose efficient exact optimization (EXO) of the alignment\nobjective. We prove that EXO is guaranteed to optimize in the same direction as\nthe RL algorithms asymptotically for arbitary parametrization of the policy,\nwhile enables efficient optimization by circumventing the complexities\nassociated with RL algorithms. We compare our method to DPO with both\ntheoretical and empirical analyses, and further demonstrate the advantages of\nour method over existing approaches on realistic human preference data.",
        "updated": "2024-02-02 15:50:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00856v2"
    },
    {
        "title": "Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?",
        "authors": "Xue-Yong FuMd Tahmid Rahman LaskarElena KhasanovaCheng ChenShashi Bhushan TN",
        "links": "http://arxiv.org/abs/2402.00841v1",
        "entry_id": "http://arxiv.org/abs/2402.00841v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00841v1",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities to\nsolve a wide range of tasks without being explicitly fine-tuned on\ntask-specific datasets. However, deploying LLMs in the real world is not\ntrivial, as it requires substantial computing resources. In this paper, we\ninvestigate whether smaller, compact LLMs are a good alternative to the\ncomparatively Larger LLMs2 to address significant costs associated with\nutilizing LLMs in the real world. In this regard, we study the meeting\nsummarization task in a real-world industrial environment and conduct extensive\nexperiments by comparing the performance of fine-tuned compact LLMs (e.g.,\nFLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2,\nGPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning,\nfail to outperform larger zero-shot LLMs in meeting summarization datasets.\nHowever, a notable exception is FLAN-T5 (780M parameters), which performs on\npar or even better than many zero-shot Larger LLMs (from 7B to above 70B\nparameters), while being significantly smaller. This makes compact LLMs like\nFLAN-T5 a suitable cost-efficient solution for real-world industrial\ndeployment.",
        "updated": "2024-02-01 18:31:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00841v1"
    },
    {
        "title": "OLMo: Accelerating the Science of Language Models",
        "authors": "Dirk GroeneveldIz BeltagyPete WalshAkshita BhagiaRodney KinneyOyvind TafjordAnanya Harsh JhaHamish IvisonIan MagnussonYizhong WangShane AroraDavid AtkinsonRussell AuthurKhyathi Raghavi ChanduArman CohanJennifer DumasYanai ElazarYuling GuJack HesselTushar KhotWilliam MerrillJacob MorrisonNiklas MuennighoffAakanksha NaikCrystal NamMatthew E. PetersValentina PyatkinAbhilasha RavichanderDustin SchwenkSaurabh ShahWill SmithEmma StrubellNishant SubramaniMitchell WortsmanPradeep DasigiNathan LambertKyle RichardsonLuke ZettlemoyerJesse DodgeKyle LoLuca SoldainiNoah A. SmithHannaneh Hajishirzi",
        "links": "http://arxiv.org/abs/2402.00838v1",
        "entry_id": "http://arxiv.org/abs/2402.00838v1",
        "pdf_url": "http://arxiv.org/pdf/2402.00838v1",
        "summary": "Language models (LMs) have become ubiquitous in both NLP research and in\ncommercial product offerings. As their commercial importance has surged, the\nmost powerful models have become closed off, gated behind proprietary\ninterfaces, with important details of their training data, architectures, and\ndevelopment undisclosed. Given the importance of these details in\nscientifically studying these models, including their biases and potential\nrisks, we believe it is essential for the research community to have access to\npowerful, truly open LMs. To this end, this technical report details the first\nrelease of OLMo, a state-of-the-art, truly Open Language Model and its\nframework to build and study the science of language modeling. Unlike most\nprior efforts that have only released model weights and inference code, we\nrelease OLMo and the whole framework, including training data and training and\nevaluation code. We hope this release will empower and strengthen the open\nresearch community and inspire a new wave of innovation.",
        "updated": "2024-02-01 18:28:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.00838v1"
    }
]