[
    {
        "title": "Boundary Detection Algorithm Inspired by Locally Linear Embedding",
        "authors": "Pei-Cheng KuoNan Wu",
        "links": "http://arxiv.org/abs/2406.18456v1",
        "entry_id": "http://arxiv.org/abs/2406.18456v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18456v1",
        "summary": "In the study of high-dimensional data, it is often assumed that the data set\npossesses an underlying lower-dimensional structure. A practical model for this\nstructure is an embedded compact manifold with boundary. Since the underlying\nmanifold structure is typically unknown, identifying boundary points from the\ndata distributed on the manifold is crucial for various applications. In this\nwork, we propose a method for detecting boundary points inspired by the widely\nused locally linear embedding algorithm. We implement this method using two\nnearest neighborhood search schemes: the $\\epsilon$-radius ball scheme and the\n$K$-nearest neighbor scheme. This algorithm incorporates the geometric\ninformation of the data structure, particularly through its close relation with\nthe local covariance matrix. We discuss the selection the key parameter and\nanalyze the algorithm through our exploration of the spectral properties of the\nlocal covariance matrix in both neighborhood search schemes. Furthermore, we\ndemonstrate the algorithm's performance with simulated examples.",
        "updated": "2024-06-26 16:05:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18456v1"
    },
    {
        "title": "Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers",
        "authors": "Yibo JiangGoutham RajendranPradeep RavikumarBryon Aragam",
        "links": "http://arxiv.org/abs/2406.18400v1",
        "entry_id": "http://arxiv.org/abs/2406.18400v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18400v1",
        "summary": "Large Language Models (LLMs) have the capacity to store and recall facts.\nThrough experimentation with open-source models, we observe that this ability\nto retrieve facts can be easily manipulated by changing contexts, even without\naltering their factual meanings. These findings highlight that LLMs might\nbehave like an associative memory model where certain tokens in the contexts\nserve as clues to retrieving facts. We mathematically explore this property by\nstudying how transformers, the building blocks of LLMs, can complete such\nmemory tasks. We study a simple latent concept association problem with a\none-layer transformer and we show theoretically and empirically that the\ntransformer gathers information using self-attention and uses the value matrix\nfor associative memory.",
        "updated": "2024-06-26 14:49:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18400v1"
    },
    {
        "title": "Second Maximum of a Gaussian Random Field and Exact (t-)Spacing test",
        "authors": "Azaïs Jean-MarcDalmao FedericoDe Castro Yohann",
        "links": "http://arxiv.org/abs/2406.18397v1",
        "entry_id": "http://arxiv.org/abs/2406.18397v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18397v1",
        "summary": "In this article, we introduce the novel concept of the second maximum of a\nGaussian random field on a Riemannian submanifold. This second maximum serves\nas a powerful tool for characterizing the distribution of the maximum. By\nutilizing an ad-hoc Kac Rice formula, we derive the explicit form of the\nmaximum's distribution, conditioned on the second maximum and some regressed\ncomponent of the Riemannian Hessian. This approach results in an exact test,\nbased on the evaluation of spacing between these maxima, which we refer to as\nthe spacing test.\n  We investigate the applicability of this test in detecting sparse\nalternatives within Gaussian symmetric tensors, continuous sparse\ndeconvolution, and two-layered neural networks with smooth rectifiers. Our\ntheoretical results are supported by numerical experiments, which illustrate\nthe calibration and power of the proposed tests. More generally, this test can\nbe applied to any Gaussian random field on a Riemannian manifold, and we\nprovide a general framework for the application of the spacing test in\ncontinuous sparse kernel regression.\n  Furthermore, when the variance-covariance function of the Gaussian random\nfield is known up to a scaling factor, we derive an exact Studentized version\nof our test, coined the $t$-spacing test. This test is perfectly calibrated\nunder the null hypothesis and has high power for detecting sparse alternatives.",
        "updated": "2024-06-26 14:44:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18397v1"
    },
    {
        "title": "Learning pure quantum states (almost) without regret",
        "authors": "Josep LumbrerasMikhail TerekhovMarco Tomamichel",
        "links": "http://arxiv.org/abs/2406.18370v1",
        "entry_id": "http://arxiv.org/abs/2406.18370v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18370v1",
        "summary": "We initiate the study of quantum state tomography with minimal regret. A\nlearner has sequential oracle access to an unknown pure quantum state, and in\neach round selects a pure probe state. Regret is incurred if the unknown state\nis measured orthogonal to this probe, and the learner's goal is to minimise the\nexpected cumulative regret over $T$ rounds. The challenge is to find a balance\nbetween the most informative measurements and measurements incurring minimal\nregret. We show that the cumulative regret scales as\n$\\Theta(\\operatorname{polylog} T)$ using a new tomography algorithm based on a\nmedian of means least squares estimator. This algorithm employs measurements\nbiased towards the unknown state and produces online estimates that are optimal\n(up to logarithmic terms) in the number of observed samples.",
        "updated": "2024-06-26 14:13:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18370v1"
    },
    {
        "title": "Efficient and Accurate Explanation Estimation with Distribution Compression",
        "authors": "Hubert BanieckiGiuseppe CasalicchioBernd BischlPrzemyslaw Biecek",
        "links": "http://arxiv.org/abs/2406.18334v1",
        "entry_id": "http://arxiv.org/abs/2406.18334v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18334v1",
        "summary": "Exact computation of various machine learning explanations requires numerous\nmodel evaluations and in extreme cases becomes impractical. The computational\ncost of approximation increases with an ever-increasing size of data and model\nparameters. Many heuristics have been proposed to approximate post-hoc\nexplanations efficiently. This paper shows that the standard i.i.d. sampling\nused in a broad spectrum of algorithms for explanation estimation leads to an\napproximation error worthy of improvement. To this end, we introduce Compress\nThen Explain (CTE), a new paradigm for more efficient and accurate explanation\nestimation. CTE uses distribution compression through kernel thinning to obtain\na data sample that best approximates the marginal distribution. We show that\nCTE improves the estimation of removal-based local and global explanations with\nnegligible computational overhead. It often achieves an on-par explanation\napproximation error using 2-3x less samples, i.e. requiring 2-3x less model\nevaluations. CTE is a simple, yet powerful, plug-in for any explanation method\nthat now relies on i.i.d. sampling.",
        "updated": "2024-06-26 13:21:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18334v1"
    }
]