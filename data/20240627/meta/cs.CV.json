[
    {
        "title": "On Scaling Up 3D Gaussian Splatting Training",
        "authors": "Hexu ZhaoHaoyang WengDaohan LuAng LiJinyang LiAurojit PandaSaining Xie",
        "links": "http://arxiv.org/abs/2406.18533v1",
        "entry_id": "http://arxiv.org/abs/2406.18533v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18533v1",
        "summary": "3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction\ndue to its superior visual quality and rendering speed. However, 3DGS training\ncurrently occurs on a single GPU, limiting its ability to handle\nhigh-resolution and large-scale 3D reconstruction tasks due to memory\nconstraints. We introduce Grendel, a distributed system designed to partition\n3DGS parameters and parallelize computation across multiple GPUs. As each\nGaussian affects a small, dynamic subset of rendered pixels, Grendel employs\nsparse all-to-all communication to transfer the necessary Gaussians to pixel\npartitions and performs dynamic load balancing. Unlike existing 3DGS systems\nthat train using one camera view image at a time, Grendel supports batched\ntraining with multiple views. We explore various optimization hyperparameter\nscaling strategies and find that a simple sqrt(batch size) scaling rule is\nhighly effective. Evaluations using large-scale, high-resolution scenes show\nthat Grendel enhances rendering quality by scaling up 3DGS parameters across\nmultiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by\ndistributing 40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28\nusing 11.2 million Gaussians on a single GPU. Grendel is an open-source project\navailable at: https://github.com/nyu-systems/Grendel-GS",
        "updated": "2024-06-26 17:59:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18533v1"
    },
    {
        "title": "MatchTime: Towards Automatic Soccer Game Commentary Generation",
        "authors": "Jiayuan RaoHaoning WuChang LiuYanfeng WangWeidi Xie",
        "links": "http://arxiv.org/abs/2406.18530v1",
        "entry_id": "http://arxiv.org/abs/2406.18530v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18530v1",
        "summary": "Soccer is a globally popular sport with a vast audience, in this paper, we\nconsider constructing an automatic soccer game commentary model to improve the\naudiences' viewing experience. In general, we make the following contributions:\nFirst, observing the prevalent video-text misalignment in existing datasets, we\nmanually annotate timestamps for 49 matches, establishing a more robust\nbenchmark for soccer game commentary generation, termed as\nSN-Caption-test-align; Second, we propose a multi-modal temporal alignment\npipeline to automatically correct and filter the existing dataset at scale,\ncreating a higher-quality soccer game commentary dataset for training, denoted\nas MatchTime; Third, based on our curated dataset, we train an automatic\ncommentary generation model, named MatchVoice. Extensive experiments and\nablation studies have demonstrated the effectiveness of our alignment pipeline,\nand training model on the curated datasets achieves state-of-the-art\nperformance for commentary generation, showcasing that better alignment can\nlead to significant performance improvements in downstream tasks.",
        "updated": "2024-06-26 17:57:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18530v1"
    },
    {
        "title": "MultiDiff: Consistent Novel View Synthesis from a Single Image",
        "authors": "Norman MüllerKatja SchwarzBarbara RoessleLorenzo PorziSamuel Rota BulòMatthias NießnerPeter Kontschieder",
        "links": "http://arxiv.org/abs/2406.18524v1",
        "entry_id": "http://arxiv.org/abs/2406.18524v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18524v1",
        "summary": "We introduce MultiDiff, a novel approach for consistent novel view synthesis\nof scenes from a single RGB image. The task of synthesizing novel views from a\nsingle reference image is highly ill-posed by nature, as there exist multiple,\nplausible explanations for unobserved areas. To address this issue, we\nincorporate strong priors in form of monocular depth predictors and\nvideo-diffusion models. Monocular depth enables us to condition our model on\nwarped reference images for the target views, increasing geometric stability.\nThe video-diffusion prior provides a strong proxy for 3D scenes, allowing the\nmodel to learn continuous and pixel-accurate correspondences across generated\nimages. In contrast to approaches relying on autoregressive image generation\nthat are prone to drifts and error accumulation, MultiDiff jointly synthesizes\na sequence of frames yielding high-quality and multi-view consistent results --\neven for long-term scene generation with large camera movements, while reducing\ninference time by an order of magnitude. For additional consistency and image\nquality improvements, we introduce a novel, structured noise distribution. Our\nexperimental results demonstrate that MultiDiff outperforms state-of-the-art\nmethods on the challenging, real-world datasets RealEstate10K and ScanNet.\nFinally, our model naturally supports multi-view consistent editing without the\nneed for further tuning.",
        "updated": "2024-06-26 17:53:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18524v1"
    },
    {
        "title": "ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation",
        "authors": "Shenghai YuanJinfa HuangYongqi XuYaoyang LiuShaofeng ZhangYujun ShiRuijie ZhuXinhua ChengJiebo LuoLi Yuan",
        "links": "http://arxiv.org/abs/2406.18522v1",
        "entry_id": "http://arxiv.org/abs/2406.18522v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18522v1",
        "summary": "We propose a novel text-to-video (T2V) generation benchmark,\nChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the\nT2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast\nto existing benchmarks that focus on the visual quality and textual relevance\nof generated videos, ChronoMagic-Bench focuses on the model's ability to\ngenerate time-lapse videos with significant metamorphic amplitude and temporal\ncoherence. The benchmark probes T2V models for their physics, biology, and\nchemistry capabilities, in a free-form text query. For these purposes,\nChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,\ncategorized into four major types of time-lapse videos: biological,\nhuman-created, meteorological, and physical phenomena, which are further\ndivided into 75 subcategories. This categorization comprehensively evaluates\nthe model's capacity to handle diverse and complex transformations. To\naccurately align human preference with the benchmark, we introduce two new\nautomatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic\nattributes and temporal coherence. MTScore measures the metamorphic amplitude,\nreflecting the degree of change over time, while CHScore assesses the temporal\ncoherence, ensuring the generated videos maintain logical progression and\ncontinuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual\nevaluations of ten representative T2V models, revealing their strengths and\nweaknesses across different categories of prompts, and providing a thorough\nevaluation framework that addresses current gaps in video generation research.\nMoreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k\nhigh-quality pairs of 720p time-lapse videos and detailed captions ensuring\nhigh physical pertinence and large metamorphic amplitude.",
        "updated": "2024-06-26 17:50:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18522v1"
    },
    {
        "title": "CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs",
        "authors": "Zirui WangMengzhou XiaLuxi HeHoward ChenYitao LiuRichard ZhuKaiqu LiangXindi WuHaotian LiuSadhika MalladiAlexis ChevalierSanjeev AroraDanqi Chen",
        "links": "http://arxiv.org/abs/2406.18521v1",
        "entry_id": "http://arxiv.org/abs/2406.18521v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18521v1",
        "summary": "Chart understanding plays a pivotal role when applying Multimodal Large\nLanguage Models (MLLMs) to real-world tasks such as analyzing scientific papers\nor financial reports. However, existing datasets often focus on oversimplified\nand homogeneous charts with template-based questions, leading to an\nover-optimistic measure of progress. We demonstrate that although open-source\nmodels can appear to outperform strong proprietary models on these benchmarks,\na simple stress test with slightly different charts or questions can\ndeteriorate performance by up to 34.5%. In this work, we propose CharXiv, a\ncomprehensive evaluation suite involving 2,323 natural, challenging, and\ndiverse charts from arXiv papers. CharXiv includes two types of questions: 1)\ndescriptive questions about examining basic chart elements and 2) reasoning\nquestions that require synthesizing information across complex visual elements\nin the chart. To ensure quality, all charts and questions are handpicked,\ncurated, and verified by human experts. Our results reveal a substantial,\npreviously underestimated gap between the reasoning skills of the strongest\nproprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the\nstrongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%.\nAll models lag far behind human performance of 80.5%, underscoring weaknesses\nin the chart understanding capabilities of existing MLLMs. We hope CharXiv\nfacilitates future research on MLLM chart understanding by providing a more\nrealistic and faithful measure of progress. Project page and leaderboard:\nhttps://charxiv.github.io/",
        "updated": "2024-06-26 17:50:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18521v1"
    }
]