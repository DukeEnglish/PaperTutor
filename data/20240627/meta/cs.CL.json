[
    {
        "title": "Towards Compositionality in Concept Learning",
        "authors": "Adam SteinAaditya NaikYinjun WuMayur NaikEric Wong",
        "links": "http://arxiv.org/abs/2406.18534v1",
        "entry_id": "http://arxiv.org/abs/2406.18534v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18534v1",
        "summary": "Concept-based interpretability methods offer a lens into the internals of\nfoundation models by decomposing their embeddings into high-level concepts.\nThese concept representations are most useful when they are compositional,\nmeaning that the individual concepts compose to explain the full sample. We\nshow that existing unsupervised concept extraction methods find concepts which\nare not compositional. To automatically discover compositional concept\nrepresentations, we identify two salient properties of such representations,\nand propose Compositional Concept Extraction (CCE) for finding concepts which\nobey these properties. We evaluate CCE on five different datasets over image\nand text data. Our evaluation shows that CCE finds more compositional concept\nrepresentations than baselines and yields better accuracy on four downstream\nclassification tasks. Code and data are available at\nhttps://github.com/adaminsky/compositional_concepts .",
        "updated": "2024-06-26 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18534v1"
    },
    {
        "title": "Symbolic Learning Enables Self-Evolving Agents",
        "authors": "Wangchunshu ZhouYixin OuShengwei DingLong LiJialong WuTiannan WangJiamin ChenShuai WangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor Jiang",
        "links": "http://arxiv.org/abs/2406.18532v1",
        "entry_id": "http://arxiv.org/abs/2406.18532v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18532v1",
        "summary": "The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".",
        "updated": "2024-06-26 17:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18532v1"
    },
    {
        "title": "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation",
        "authors": "Christoph LeiterSteffen Eger",
        "links": "http://arxiv.org/abs/2406.18528v1",
        "entry_id": "http://arxiv.org/abs/2406.18528v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18528v1",
        "summary": "Large language models (LLMs) have revolutionized the field of NLP. Notably,\ntheir in-context learning capabilities also enable their use as evaluation\nmetrics for natural language generation, making them particularly advantageous\nin low-resource scenarios and time-restricted applications. In this work, we\nintroduce PrExMe, a large-scale prompt exploration for metrics, where we\nevaluate more than 720 prompt templates for open-source LLM-based metrics on\nmachine translation (MT) and summarization datasets, totalling over 6.6M\nevaluations. This extensive comparison (1) serves as a benchmark of the\nperformance of recent open-source LLMs as metrics and (2) explores the\nstability and variability of different prompting strategies. We discover that,\non the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to\nseemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations.",
        "updated": "2024-06-26 17:56:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18528v1"
    },
    {
        "title": "ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation",
        "authors": "Shenghai YuanJinfa HuangYongqi XuYaoyang LiuShaofeng ZhangYujun ShiRuijie ZhuXinhua ChengJiebo LuoLi Yuan",
        "links": "http://arxiv.org/abs/2406.18522v1",
        "entry_id": "http://arxiv.org/abs/2406.18522v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18522v1",
        "summary": "We propose a novel text-to-video (T2V) generation benchmark,\nChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the\nT2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast\nto existing benchmarks that focus on the visual quality and textual relevance\nof generated videos, ChronoMagic-Bench focuses on the model's ability to\ngenerate time-lapse videos with significant metamorphic amplitude and temporal\ncoherence. The benchmark probes T2V models for their physics, biology, and\nchemistry capabilities, in a free-form text query. For these purposes,\nChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,\ncategorized into four major types of time-lapse videos: biological,\nhuman-created, meteorological, and physical phenomena, which are further\ndivided into 75 subcategories. This categorization comprehensively evaluates\nthe model's capacity to handle diverse and complex transformations. To\naccurately align human preference with the benchmark, we introduce two new\nautomatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic\nattributes and temporal coherence. MTScore measures the metamorphic amplitude,\nreflecting the degree of change over time, while CHScore assesses the temporal\ncoherence, ensuring the generated videos maintain logical progression and\ncontinuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual\nevaluations of ten representative T2V models, revealing their strengths and\nweaknesses across different categories of prompts, and providing a thorough\nevaluation framework that addresses current gaps in video generation research.\nMoreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k\nhigh-quality pairs of 720p time-lapse videos and detailed captions ensuring\nhigh physical pertinence and large metamorphic amplitude.",
        "updated": "2024-06-26 17:50:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18522v1"
    },
    {
        "title": "CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs",
        "authors": "Zirui WangMengzhou XiaLuxi HeHoward ChenYitao LiuRichard ZhuKaiqu LiangXindi WuHaotian LiuSadhika MalladiAlexis ChevalierSanjeev AroraDanqi Chen",
        "links": "http://arxiv.org/abs/2406.18521v1",
        "entry_id": "http://arxiv.org/abs/2406.18521v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18521v1",
        "summary": "Chart understanding plays a pivotal role when applying Multimodal Large\nLanguage Models (MLLMs) to real-world tasks such as analyzing scientific papers\nor financial reports. However, existing datasets often focus on oversimplified\nand homogeneous charts with template-based questions, leading to an\nover-optimistic measure of progress. We demonstrate that although open-source\nmodels can appear to outperform strong proprietary models on these benchmarks,\na simple stress test with slightly different charts or questions can\ndeteriorate performance by up to 34.5%. In this work, we propose CharXiv, a\ncomprehensive evaluation suite involving 2,323 natural, challenging, and\ndiverse charts from arXiv papers. CharXiv includes two types of questions: 1)\ndescriptive questions about examining basic chart elements and 2) reasoning\nquestions that require synthesizing information across complex visual elements\nin the chart. To ensure quality, all charts and questions are handpicked,\ncurated, and verified by human experts. Our results reveal a substantial,\npreviously underestimated gap between the reasoning skills of the strongest\nproprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the\nstrongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%.\nAll models lag far behind human performance of 80.5%, underscoring weaknesses\nin the chart understanding capabilities of existing MLLMs. We hope CharXiv\nfacilitates future research on MLLM chart understanding by providing a more\nrealistic and faithful measure of progress. Project page and leaderboard:\nhttps://charxiv.github.io/",
        "updated": "2024-06-26 17:50:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18521v1"
    }
]