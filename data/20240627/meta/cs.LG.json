[
    {
        "title": "Towards Compositionality in Concept Learning",
        "authors": "Adam SteinAaditya NaikYinjun WuMayur NaikEric Wong",
        "links": "http://arxiv.org/abs/2406.18534v1",
        "entry_id": "http://arxiv.org/abs/2406.18534v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18534v1",
        "summary": "Concept-based interpretability methods offer a lens into the internals of\nfoundation models by decomposing their embeddings into high-level concepts.\nThese concept representations are most useful when they are compositional,\nmeaning that the individual concepts compose to explain the full sample. We\nshow that existing unsupervised concept extraction methods find concepts which\nare not compositional. To automatically discover compositional concept\nrepresentations, we identify two salient properties of such representations,\nand propose Compositional Concept Extraction (CCE) for finding concepts which\nobey these properties. We evaluate CCE on five different datasets over image\nand text data. Our evaluation shows that CCE finds more compositional concept\nrepresentations than baselines and yields better accuracy on four downstream\nclassification tasks. Code and data are available at\nhttps://github.com/adaminsky/compositional_concepts .",
        "updated": "2024-06-26 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18534v1"
    },
    {
        "title": "Symbolic Learning Enables Self-Evolving Agents",
        "authors": "Wangchunshu ZhouYixin OuShengwei DingLong LiJialong WuTiannan WangJiamin ChenShuai WangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor Jiang",
        "links": "http://arxiv.org/abs/2406.18532v1",
        "entry_id": "http://arxiv.org/abs/2406.18532v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18532v1",
        "summary": "The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".",
        "updated": "2024-06-26 17:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18532v1"
    },
    {
        "title": "Confident Natural Policy Gradient for Local Planning in $q_π$-realizable Constrained MDPs",
        "authors": "Tian TianLin F. YangCsaba Szepesvári",
        "links": "http://arxiv.org/abs/2406.18529v1",
        "entry_id": "http://arxiv.org/abs/2406.18529v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18529v1",
        "summary": "The constrained Markov decision process (CMDP) framework emerges as an\nimportant reinforcement learning approach for imposing safety or other critical\nobjectives while maximizing cumulative reward. However, the current\nunderstanding of how to learn efficiently in a CMDP environment with a\npotentially infinite number of states remains under investigation, particularly\nwhen function approximation is applied to the value functions. In this paper,\nwe address the learning problem given linear function approximation with\n$q_{\\pi}$-realizability, where the value functions of all policies are linearly\nrepresentable with a known feature map, a setting known to be more general and\nchallenging than other linear settings. Utilizing a local-access model, we\npropose a novel primal-dual algorithm that, after $\\tilde{O}(\\text{poly}(d)\n\\epsilon^{-3})$ queries, outputs with high probability a policy that strictly\nsatisfies the constraints while nearly optimizing the value with respect to a\nreward function. Here, $d$ is the feature dimension and $\\epsilon > 0$ is a\ngiven error. The algorithm relies on a carefully crafted off-policy evaluation\nprocedure to evaluate the policy using historical data, which informs policy\nupdates through policy gradients and conserves samples. To our knowledge, this\nis the first result achieving polynomial sample complexity for CMDP in the\n$q_{\\pi}$-realizable setting.",
        "updated": "2024-06-26 17:57:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18529v1"
    },
    {
        "title": "APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets",
        "authors": "Zuxin LiuThai HoangJianguo ZhangMing ZhuTian LanShirley KokaneJuntao TanWeiran YaoZhiwei LiuYihao FengRithesh MurthyLiangwei YangSilvio SavareseJuan Carlos NieblesHuan WangShelby HeineckeCaiming Xiong",
        "links": "http://arxiv.org/abs/2406.18518v1",
        "entry_id": "http://arxiv.org/abs/2406.18518v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18518v1",
        "summary": "The advancement of function-calling agent models requires diverse, reliable,\nand high-quality datasets. This paper presents APIGen, an automated data\ngeneration pipeline designed to synthesize verifiable high-quality datasets for\nfunction-calling applications. We leverage APIGen and collect 3,673 executable\nAPIs across 21 different categories to generate diverse function-calling\ndatasets in a scalable and structured manner. Each data in our dataset is\nverified through three hierarchical stages: format checking, actual function\nexecutions, and semantic verification, ensuring its reliability and\ncorrectness. We demonstrate that models trained with our curated datasets, even\nwith only 7B parameters, can achieve state-of-the-art performance on the\nBerkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.\nMoreover, our 1B model achieves exceptional performance, surpassing\nGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000\nhigh-quality entries, aiming to advance the field of function-calling agent\ndomains. The dataset is available on Huggingface:\nhttps://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the\nproject homepage: https://apigen-pipeline.github.io/",
        "updated": "2024-06-26 17:49:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18518v1"
    },
    {
        "title": "Mental Modeling of Reinforcement Learning Agents by Language Models",
        "authors": "Wenhao LuXufeng ZhaoJosua SpisakJae Hee LeeStefan Wermter",
        "links": "http://arxiv.org/abs/2406.18505v1",
        "entry_id": "http://arxiv.org/abs/2406.18505v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18505v1",
        "summary": "Can emergent language models faithfully model the intelligence of\ndecision-making agents? Though modern language models exhibit already some\nreasoning ability, and theoretically can potentially express any probable\ndistribution over tokens, it remains underexplored how the world knowledge\nthese pretrained models have memorized can be utilized to comprehend an agent's\nbehaviour in the physical world. This study empirically examines, for the first\ntime, how well large language models (LLMs) can build a mental model of agents,\ntermed agent mental modelling, by reasoning about an agent's behaviour and its\neffect on states from agent interaction history. This research may unveil the\npotential of leveraging LLMs for elucidating RL agent behaviour, addressing a\nkey challenge in eXplainable reinforcement learning (XRL). To this end, we\npropose specific evaluation metrics and test them on selected RL task datasets\nof varying complexity, reporting findings on agent mental model establishment.\nOur results disclose that LLMs are not yet capable of fully mental modelling\nagents through inference alone without further innovations. This work thus\nprovides new insights into the capabilities and limitations of modern LLMs.",
        "updated": "2024-06-26 17:14:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18505v1"
    }
]