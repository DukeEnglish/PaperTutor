[
    {
        "title": "Symbolic Learning Enables Self-Evolving Agents",
        "authors": "Wangchunshu ZhouYixin OuShengwei DingLong LiJialong WuTiannan WangJiamin ChenShuai WangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor Jiang",
        "links": "http://arxiv.org/abs/2406.18532v1",
        "entry_id": "http://arxiv.org/abs/2406.18532v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18532v1",
        "summary": "The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".",
        "updated": "2024-06-26 17:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18532v1"
    },
    {
        "title": "APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets",
        "authors": "Zuxin LiuThai HoangJianguo ZhangMing ZhuTian LanShirley KokaneJuntao TanWeiran YaoZhiwei LiuYihao FengRithesh MurthyLiangwei YangSilvio SavareseJuan Carlos NieblesHuan WangShelby HeineckeCaiming Xiong",
        "links": "http://arxiv.org/abs/2406.18518v1",
        "entry_id": "http://arxiv.org/abs/2406.18518v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18518v1",
        "summary": "The advancement of function-calling agent models requires diverse, reliable,\nand high-quality datasets. This paper presents APIGen, an automated data\ngeneration pipeline designed to synthesize verifiable high-quality datasets for\nfunction-calling applications. We leverage APIGen and collect 3,673 executable\nAPIs across 21 different categories to generate diverse function-calling\ndatasets in a scalable and structured manner. Each data in our dataset is\nverified through three hierarchical stages: format checking, actual function\nexecutions, and semantic verification, ensuring its reliability and\ncorrectness. We demonstrate that models trained with our curated datasets, even\nwith only 7B parameters, can achieve state-of-the-art performance on the\nBerkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.\nMoreover, our 1B model achieves exceptional performance, surpassing\nGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000\nhigh-quality entries, aiming to advance the field of function-calling agent\ndomains. The dataset is available on Huggingface:\nhttps://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the\nproject homepage: https://apigen-pipeline.github.io/",
        "updated": "2024-06-26 17:49:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18518v1"
    },
    {
        "title": "Mental Modeling of Reinforcement Learning Agents by Language Models",
        "authors": "Wenhao LuXufeng ZhaoJosua SpisakJae Hee LeeStefan Wermter",
        "links": "http://arxiv.org/abs/2406.18505v1",
        "entry_id": "http://arxiv.org/abs/2406.18505v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18505v1",
        "summary": "Can emergent language models faithfully model the intelligence of\ndecision-making agents? Though modern language models exhibit already some\nreasoning ability, and theoretically can potentially express any probable\ndistribution over tokens, it remains underexplored how the world knowledge\nthese pretrained models have memorized can be utilized to comprehend an agent's\nbehaviour in the physical world. This study empirically examines, for the first\ntime, how well large language models (LLMs) can build a mental model of agents,\ntermed agent mental modelling, by reasoning about an agent's behaviour and its\neffect on states from agent interaction history. This research may unveil the\npotential of leveraging LLMs for elucidating RL agent behaviour, addressing a\nkey challenge in eXplainable reinforcement learning (XRL). To this end, we\npropose specific evaluation metrics and test them on selected RL task datasets\nof varying complexity, reporting findings on agent mental model establishment.\nOur results disclose that LLMs are not yet capable of fully mental modelling\nagents through inference alone without further innovations. This work thus\nprovides new insights into the capabilities and limitations of modern LLMs.",
        "updated": "2024-06-26 17:14:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18505v1"
    },
    {
        "title": "Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation",
        "authors": "Ahmed NjifenjouVirgile SucalBassam JabaianFabrice Lefèvre",
        "links": "http://arxiv.org/abs/2406.18460v1",
        "entry_id": "http://arxiv.org/abs/2406.18460v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18460v1",
        "summary": "Recently, various methods have been proposed to create open-domain\nconversational agents with Large Language Models (LLMs). These models are able\nto answer user queries, but in a one-way Q&A format rather than a true\nconversation. Fine-tuning on particular datasets is the usual way to modify\ntheir style to increase conversational ability, but this is expensive and\nusually only available in a few languages. In this study, we explore role-play\nzero-shot prompting as an efficient and cost-effective solution for open-domain\nconversation, using capable multilingual LLMs (Beeching et al., 2023) trained\nto obey instructions. We design a prompting system that, when combined with an\ninstruction-following model - here Vicuna (Chiang et al., 2023) - produces\nconversational agents that match and even surpass fine-tuned models in human\nevaluation in French in two different tasks.",
        "updated": "2024-06-26 16:10:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18460v1"
    },
    {
        "title": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers",
        "authors": "Jonas NgnawéSabyasachi SahooYann PequignotFrédéric PreciosoChristian Gagné",
        "links": "http://arxiv.org/abs/2406.18451v1",
        "entry_id": "http://arxiv.org/abs/2406.18451v1",
        "pdf_url": "http://arxiv.org/pdf/2406.18451v1",
        "summary": "Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate strong margin consistency\nwith a strong correlation between their input space margins and the logit\nmargins. Then, we show that we can effectively use the logit margin to\nconfidently detect brittle decisions with such models and accurately estimate\nrobust accuracy on an arbitrarily large test set by estimating the input\nmargins only on a small subset. Finally, we address cases where the model is\nnot sufficiently margin-consistent by learning a pseudo-margin from the feature\nrepresentation. Our findings highlight the potential of leveraging deep\nrepresentations to efficiently assess adversarial vulnerability in deployment\nscenarios.",
        "updated": "2024-06-26 16:00:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.18451v1"
    }
]