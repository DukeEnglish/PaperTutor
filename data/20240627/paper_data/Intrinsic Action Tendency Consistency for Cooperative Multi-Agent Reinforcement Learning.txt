Intrinsic Action Tendency Consistency for Cooperative
Multi-Agent Reinforcement Learning
JunkaiZhang1,2,YifanZhang1,3,4*,XiSherylZhang1,3,4,YifanZang1,2,JianCheng1,3,4
1InstituteofAutomation,ChineseAcademyofSciences
2SchoolofArtificialIntelligence,UniversityofChineseAcademyofSciences
3UniversityofChineseAcademyofSciences,Nanjing
4NanjingArtificialIntelligenceResearchofAI
{zhangjunkai2021,xi.zhang,zangyifan2019}@ia.ac.cn,{yfzhang,jcheng}@nlpr.ia.ac.cn
Abstract
Efficientcollaborationinthecentralizedtrainingwithdecen-
tralizedexecution(CTDE)paradigmremainsachallengein
cooperativemulti-agentsystems.Weidentifydivergentaction
tendenciesamongagentsasasignificantobstacletoCTDE’s
trainingefficiency,requiringalargenumberoftrainingsam-
plestoachieveaunifiedconsensusonagents’policies.This
divergencestemsfromthelackofadequateteamconsensus-
relatedguidancesignalsduringcreditassignmentsinCTDE.
Toaddressthis,weproposeIntrinsicActionTendencyCon- Figure1:Theillustrationoftheconsistentactiontendency.
sistency, a novel approach for cooperative multi-agent rein- In (a) and (b), our agents’ health value is lower than the
forcement learning. It integrates intrinsic rewards, obtained enemies’. At this point, attacking either Enemy 1 or 2 si-
throughanactionmodel,intoareward-additiveCTDE(RA- multaneouslyarethetwobestteampolicies.In(a),Agent1
CTDE) framework. We formulate an action model that en- andAgent2attackenemiesseparatelywithoutagreeingon
ables surrounding agents to predict the central agent’s ac- ateampolicy.Onthecontrary,agentsin(b)achieveacon-
tion tendency. Leveraging these predictions, we compute a
sistentgoalpolicyandagreetoattackacommonenemy.To
cooperativeintrinsicrewardthatencouragesagentstomatch
reflectthepolicyconsistencyamongagents,weproposethe
theiractionswiththeirneighbors’predictions.Weestablish
conceptofactiontendency.Itreflectsthepolicydistribution
theequivalencebetweenRA-CTDEandCTDEthroughtheo-
of agents toward different actions. We propose this action
reticalanalyses,demonstratingthatCTDE’strainingprocess
canbeachievedusingagents’individualtargets.Buildingon tendencynotiontodistinguishitfrompolicy,whichisusu-
thisinsight,weintroduceanovelmethodtocombineintrin- ally the epsilon-greedy of Q functions only concerning the
sicrewardsandCTDE.Extensiveexperimentsonchallenging largestoutputinvalue-basedapproaches.
tasksinSMACandGRFbenchmarksshowcasetheimproved
performanceofourmethod.
that agents can only access their local observations, result-
Introduction
inginunstableenvironments.Scalabilityreferstothechal-
Cooperativemulti-agentreinforcementlearning(MARL)al- lengethatthejointspacesofstatesandactionsincreaseex-
gorithms have shown the great capacity and potential to ponentially as the number of agents grows. To tackle these
solve various real-world multi-agent tasks, such as auto- issues, Centralized Training with Decentralized Execution
maticvehiclescontrol(Sallabetal.2017;Zhouetal.2020b), (CTDE)paradigmisproposed(Sunehagetal.2017),which
traffic intelligence (Cao et al. 2012; Mushtaq et al. 2023; allowsagentstoaccesstheglobalstateinthetrainingstage
Wiering et al. 2000), resource management (Motlaghzadeh and take actions individually. Given the CTDE paradigm,
etal.2023;Sallabetal.2017),gameAI(Berneretal.2019; massive deep MARL methods have been proposed includ-
Linetal.2023)androbotswarmcontrol(Dahiyaetal.2023; ingVDN(Sunehagetal.2017),QMIX(Rashidetal.2020),
Hu¨ttenrauch, Sˇosˇic´, and Neumann 2017). In a cooperative QTRAN(Sonetal.2019),QPLEX(Wangetal.2020b)and
multi-agent system (MAS), every agent relies on their lo- so forth. Their excellent performance can be attributed to
cal observation to cooperate toward a team goal and the thecreditassignments,asrewardsarecriticalasthemostdi-
environment feedbacks a shared team reward. There exist rectandfundamentalinstructionalsignalstodrivebehaviors
two major challenges in cooperative MAS: partial observ- (Silveretal.2021;Zhengetal.2021;Mgunietal.2021).
abilityandscalability.Partialobservabilityreferstothefact
However, it turns out that the sparse team rewards pro-
videdbymanyMASenvironmentscannotsupplysufficient
*Correspondingauthor
Copyright©2024,AssociationfortheAdvancementofArtificial guidance for coordination behaviors, which results in inef-
Intelligence(www.aaai.org).Allrightsreserved. ficienttraining(Matignon,Laurent,andLeFort-Piat2012).
4202
nuJ
62
]AM.sc[
1v25181.6042:viXraWeanalyzetheQMIXtrainingprocessandrealizethatnu- Background
merous unsuccessful episodes are caused by the inconsis- Dec-POMDP
tency of team policy goals among agents like Figure 1 (a).
A fully cooperative multi-agent task can be formulated
Among these episodes, each agent’s action tendency is not
as a Decentralized Partially Observable Markov Deci-
unifiedtothesameglobalpolicy.Wearguethatthereward
sion Process (Dec-POMDP) (Oliehoek and Amato 2015),
inMASisthemostessentialinstructionalsignaltodrivebe-
which is an augmented POMDP formulated by a tuple
haviorsandascribeagents’actiontendencyinconsistencyto
M =< N,S,(O ) ,(A ) ,O,P,R,ρ ,γ >, where
CTDE’slackofsufficientteamconsensusguidancesignals. i i∈N i i∈N 0
every agent can only access the partial state of the envi-
An effective solution to this challenge is to add intrin-
ronmentandtakesactionsindividually.Specifically,wede-
sic rewards into the CTDE paradigm. There exist two ma-
note N = {1,...,N} as the set of agents, where N is the
jor problems: how to design an intrinsic reward to guide
number of agents, S as the global finite state space, O as
agents’unifiedactiontendencyandhowtointegratethein- i
the partial observation of the state, obtained by the func-
trinsicrewardsintotheCTDEframework?InMARL,there
tion O(s,i)| , and A as the action space respectively.
are plenty of works designing intrinsic rewards including s∈S i
γ ∈ [0,1)isadiscountfactorandρ : S → Risthedistri-
curiosity-based incentives (Bo¨hmer, Rashid, and Whiteson 0
bution of the inital state s . The state transition probability
2019; Hernandez-Leal, Kartal, and Taylor 2019; Iqbal and 0
functionoftheenvironmentdynamicsisP :S×A×S →
Sha 2019; Zhang et al. 2023), the mutual influence among
[0,1]whereA:=×N A isthejointactionspaceselected
agents (Chitnis et al. 2020; Jaques et al. 2019; Wang et al. i=1 i
by all agents. Due to the partial observable setting, every
2019) and other specific designs (Strouse et al. 2018; Ma
agent takes its observation-action history τ ∈ {T }N ≡
et al. 2022; Mguni et al. 2021; Du et al. 2019). However, i i i=1
(O ×A )∗×O asthepolicyinputtoacquiremoreinfor-
mostofthemaredesignedtoenhanceexploration andem- i i i
mation. After agentstaking their joint actions a : {ai}N ,
ployedinindependenttrainingways,whichsufferfromun- t i=1
theenvironmentreturnsateamsharedextrinsicrewardrext
stable dynamics of environments. To ease the latter prob-
byfunctionR(S,A):S×A→R.Wedefinethestochas-
lem, EMC (Zheng et al. 2021) proposed a curiosity-driven
tic policy of agent i by π (a |τ ) : T × A → [0,1],
intrinsicrewardandincorporateditintotheCTDEtraining i i i i i
the multi-agent system algorithms are designed to find op-
paradigm. Yet it averages the calculated intrinsic rewards
timalpoliciesπ∗ ={π∗}N tomaximizethejointextrinsic
anddirectlyaddsthemtotheglobalteamreward,whichre- value function Vπ(s) i =i= E1 [(cid:80)∞ γtrext|π], where
sults in losing the diversity of the intrinsic reward’s adjust- s0,a0,... t=0 t
s ∼ρ (s ),π ={π }N .
menttowardcreditassignmentsforeachagent. 0 0 0 i i=1
Inthiswork,weproposeournovelIntrinsicActionTen-
CentralizedTrainingwithDecentralizedExecution
dency Consistency for the cooperative multi-agent rein-
The primary challenge for MAS tasks is that agents can
forcementlearningmethod.Wehopetodesignintrinsicre-
only access partial observation and are incapable to ac-
wardsonthebasisofCTDE,soastoachieveconsistentteam
quire the global state, to which an effective solution is
policy goals among agents in the training process. Specifi-
the CTDE training paradigm (Bernstein et al. 2002). It al-
cally,wefirstproposeanactionmodeltopredictthecentral
lows all agents to access the global state in the central-
agent’sactiontendency.Wedefineourintrinsicrewardasthe
ized training stage and take actions individually in a de-
surroundingagents’actiontendencypredictionerrortoward
centralizedmanner.Formally,itformulatesN individualQ-
thecentralagents.Itencouragesthecentralagenttotakeac-
functions {Q (τ ,a ;θ )} where θ is the network pa-
tionsmatchingthepredictionoftheirneighbors.Afterthat, i i i i i∈N i
rameterforagenti.Meanwhile,itsimultaneouslypreserves
weproposetheoreticalanalysesonCTDEandconvertitinto
a joint action-value function Q (τ,a) constructed by in-
anequivalentvariant,RA-CTDE.ToappropriatelyutilizeN tot
dividual Q functions to help training. In detail, the objec-
intrinsicrewardslikeIQL(Tan1993)trainingparadigm,we
tiveofCTDEistogetanoptimaljointaction-valuefunction
equivalently transform the original global target of CTDE
Q∗ (s,a) = rext(s,a)+γE [max Q∗ (s′,a′)].Inthe
intoN ones.Atlast,weincorporateouractionmodelbased tot s′ a′ tot
centralizedtrainingstage,Q-functions{Q } aretrained
intrinsic reward into RA-CTDE and denote it by IAM. We i i∈N
byminimizingthefollowingtargetfunction:
integrateourmethodintoQMIXandVDN,andconductex-
tensiveexperimentsinStarCraftIIMicromanagementenvi- (cid:104) (cid:105)2
LG(θ,ϕ)=E r+γmaxQ (τ′,a′)−Q (τ,a;θ,ϕ) (1)
ronment(Samvelyanetal.2019)(SMAC)andGoogleFoot- T tot
a′
ballResearchenvironment(Kurachetal.2020)(GRF).Em-
Q (τ,a;θ,ϕ)=F(Q(τ,a ),...,Q (τ ,a ),s;ϕ) (2)
pirical results verify that our method achieves competitive tot 1 1 1 N N N
performanceandsignificantlyoutperformsotherbaselines. where τ ={τ }N ,a={a}N ,θ={θ }N , ϕ is the pa-
i i=1 i=1 i i=1
Keycontributionsaresummarizedasfollows:1)Wepro- rameters of the mixing network F, and D is the replay
pose an action model based intrinsic reward measured by buffer. {τ,a,r,τ′} ∼ D. Q denotes the expected re-
T
predictingthecentralagent’sactiontendency.2)Fromathe- turn target function for the estimation of the global state-
oreticalperspective,weaddresstheissueofCTDEbeingun- actionpair.Thegradientsofθ arecalculatedthroughfunc-
abletoutilizetheintrinsicrewardsdirectlyandconsequently tion F, which factorizes global Q function into decen-
tot
embedourintrinsicrewardsintoit.3)Byincorporatingour tralized ones {Q }N , motivating enormous efforts to find
i i=1
methodintoQMIXandVDN,wedemonstrateIAM’scom- factorization structures among them (Sunehag et al. 2017;
petitiveperformanceonchallengingMARLtasks. Rashidetal.2020;Wangetal.2020b).Figure2:IAM-basedreward.Theblueandgreenzonesrepresentthereceptivefieldofthecentralagentandsurroundingagents.
Theactionmodelintrinsicrewardishighwhenagentitakesactionsthatmatchtheirsurroundingagents’predictions.
RelatedWorks ActionModelBasedIntrinsicReward
ManyoftheintrinsicrewardfunctionsusedinMARLhave For a better interpretation, we first give the following defi-
been adapted from single agent curiosity-based incentives nitions: As shown in Figure 2, when considering a specific
(Hernandez-Leal, Kartal, and Taylor 2019; Iqbal and Sha Agenti,wedefineitasthecentralagent.Duetothepartial
2019;Jaquesetal.2019),whichaimedtoencourageagents observabilityoftheenvironment,agentsthatareobservable
to explore their environment and seek out novel states. To in the surrounding area of Agent i are defined as the sur-
better be applied in MARL, Some MARL-specific intrin- rounding agents and we denote the set as S(i). During the
sic reward functions have been proposed, including con- trainingprocess,wehopethateverycentralagentiwilltake
sidering the mutual influence among agents (Chitnis et al. intoaccountitssurroundingagents’expectationstowardi’s
2020; Jaques et al. 2019; Wang et al. 2019), encouraging policydistribution.Wedenoteitspolicydistributionbythe
agentstorevealorhidetheirintentions(Strouseetal.2018) actiontendency,whichrepresentstherelativemagnitudeof
andpredictingobservationwithalignmenttotheirneighbors anagent’sinclinationtotakedifferentactions.
(Ma et al. 2022). Besides, Intrinsic rewards without task-
oriented bias can increase the diversity of intrinsic reward Reward Calculation In a discrete action space, agent i’s
space,whichcanbeimplementedbybreakingtheextrinsic action tendency can be reflected from two different per-
rewardsviacreditassignment(Duetal.2019)orusingadap- spectives.Fromtheviewpointofthecentralagenti,itsac-
tivelearnerstoobtainintrinsicrewardsonline(Mgunietal. tion tendency can be represented by its Q i function. From
2021).Apartfromindependentmannerstodealingwithre- the viewpoint of i’s surrounding agents, we define the ac-
wards,EMC(Zhengetal.2021)proposedacuriosity-driven tionmodels{F iAM}N i=1 toallowthemtopredictthecentral
intrinsicrewardandintroduceanintegrationwaytoaccom- agent’sactiontendency.FAM isdesignedtoutilizethesame
i
plishtheCTDEtrainingparadigm. network structure as Q i function. Their representation dis-
tance of action tendency reflects the central agent’s consis-
Method tency degree towards the surrounding agents’ expectation.
Therefore we formulate this distance as an action model
In this section, we present our Intrinsic Action Tendency
basedintrinsicreward,i.e.{rAM}N .
Consistency for cooperative MARL denoted by IAM (In- i i=1
trinsicActionModel).Ourpurposeistodesignaneffective
o =F (o ,j) (3)
intrinsic reward to encourage consistent action tendencies
ij im i
andleverageitintoCTDEinanappropriatemanner.Specif- rAM= −1 (cid:88) Dis(cid:0) FAM(o ,·;ω)−Q (o ,·)(cid:1) (4)
ically, we first introduce our action model based intrinsic i |S(i)| i ij i i i
j∈S(i)
reward, which encourages the central agent to take actions
consistentwithitsneighbors’prospects.Thenweproposea The reward calculation process is illustrated in Figure 2
reward-additiveequivalentvariantoftheCTDEframework and Eq 3, 4. During the training phase, every agent first
denoted by RA-CTDE to incorporate our rewards reason- calculates its imagined surrounding agents’ observations,
ably. At last, we analyze the essential difference between then utilizes its Q function and action model to measure
VDN (Sunehag et al. 2017) and IQL (Tan 1993) and then the action tendency distance, and finally obtains its action
demonstratethereasonabilityofourrewardintegrationway. model based intrinsic reward rAM. The imagine function
iFigure 3: IAM training paradigm. The training paradigm consists of two stages: (a) Forward stage and (b) Backward stage.
In the forwardstage, we use mixingnetwork F,Q ,rext and Q in Eq. 1to calculate global TD-errortarget LG, which is
tot T
the same as CTDE. In the backward stage, we first factorize LG into N targets {LE}N by Eq 6 and 7, then add intrinsic
i i=1
rewardsintothemindividuallytoobtainIAMtargets:{LIAM}N .Thegradientsof{θ }N andϕareseparatelycomputedby
i i=1 i i=1
backpropagatingN targets{LIAM}N .
i i=1
F (o ,j) in Eq 3 is defined to represent the surrounding ical probability of τ in the given dataset D , the notation
im i i
agents’simulatedobservationimaginedbythecentralagent τ (cid:76) τ′ denotes < τ′,...,τ′ ,τ ,τ′ ,...,τ′ >, and τ′
i −i 1 i−1 i i+1 n −i
i.Theimaginingprocessisrealizedbyswitchingtheview- denotestheelementsofallagentsexceptforagenti.
points from the central agent into the surrounding agents, In Eq 5, it is easy to see that the Q function essentially
i
i.e., separately setting the positional coordinates of every consistsofthreeitems,andthefirsttwoincludetheexpecta-
surroundingagentastheorigintocalculatethecoordinates tionofone-stepTDtargetvalueoverothers.Itindicatesthat
of the other agents attached with additional information, theQ functionvalueobtainedinVDNincludestheinterac-
i
which does not require any learning parameters (more de- tivehistoricalexpectationtowardotheragents.Althoughthis
tails in the Appendix ). In experiments, we use the L 2 dis- analysis only applies to VDNs, we broaden the supervised
tanceastheDisfunction.Underthisrewardsetting,agents targetQfunctionstoQMIXandalsoachieveeffectiveper-
are encouraged to take actions consistent with their sur- formance improvement. The pseudo-code of our algorithm
roundingagents’prospects. isinterpretedintheAppendix.
Q( it+1)(τ i,a i)= E (cid:104) y(t)(cid:0) τ i⊕τ −′ i,a i⊕a′ −i(cid:1)(cid:105) Reward-AdditiveCTDE(RA-CTDE)
(τ −′ i,a′ −i)∼pD(·|τi)
(cid:124) (cid:123)(cid:122) (cid:125) ThecontradictionforCTDEtoutilizeN intrinsicrewardsis
evaluationoftheindividualactionai thatithasonlyoneglobaltargetLG duringtraining.How-
− n−1 E (cid:104) y(t)(τ′,a′)(cid:105) ever, IQL (Tan 1993) can directly use N different intrin-
n τ′,a′∼pD(·|Λ−1(τi)) sic rewards naturally because it obtains N TD-losses in-
(cid:124) (cid:123)(cid:122) (cid:125) dividually. Based on that, we first factorize the global tar-
counterfactualbaseline get LG in Eq 1 into N individual ones and define it as
+ w (τ )
i i Reward-AdditiveCTDE(RA-CTDE).Thenwedemonstrate
(cid:124) (cid:123)(cid:122) (cid:125) itsequivalencewiththeoriginaltargetLG.Atlast,wedis-
residueterm
(5) cusshowtoaddintrinsicrewardstotheRA-CTDE.
ActionModelTraining ToobtainFAM,weuseQ ifunc- Definition 1. (Reward-Additive CTDE). Let θ = {θ i}N
i=1
tion values as supervised targets. This choice is reasonable be the parameters of Q functions, F be the mixing net-
basedonthefollowinginsight:TheindividualQ valuein- work in CTDE, N = {1,...,N} be the agents set, QN =
i
corporatesinteractioninformationofotheragentstoagent {Q 1(τ 1,a 1;θ 1),Q 2(τ 2,a 2;θ 2),...,Q N(τ N,a N;θ N)},{τ,a,
i,notjustonlytheagenti’sownactiontendencieswithlin- rext,τ′}∼D, assume ∀i,j ∈ N,θ i ̸= θ j, then Reward-
earvaluefactorization.InVDNtrainingparadigm,theindi- Additive CTDE means computing {LE(θ ,ϕ)}N in Eq 6
i i i=1
vidualQ functioncanbefactorizedintoEq5’sform(Wang and Eq 7 and then updating their parameters respectively.
i
etal.2020a),wherep (·|τ )denotestheconditionalempir- ThetermP isnotinvolvedinthegradientcalculationasa
D iscalar.Formally: intotheTD-errortermP .Weadoptthesamereward-adding
i
LE(θ ,ϕ)=E (cid:2) P ·F(QN,s;ϕ)(cid:3) (6) form as VDN and extend it to the RA-CTDE framework.
i i τ,a,rext,τ′∈D Specifically, we choose to add the calculated intrinsic re-
(cid:16) (cid:17)
P =−2 rext+γmaxQ T(τ′,a′)−F(QN,s;ϕ) (7) wardswithparameterβ intoN losses{LE i }N i=1 andgetour
a′ IAMtargetsinEq13,14.Thegradientofθ andϕcanbeob-
i
We propose our reward-additive variant of the CTDE tainedbycomputing ∂LI iAM(θi,ϕ) and 1 ·(cid:80)N ∂LI iAM(θi,ϕ)
frameworkRA-CTDE,wheretheQ (τ′,a′)inEq7isup- ∂θi N i=1 ∂ϕ
T respectively.Figure3showsourwholetrainingparadigm.
dated by the target function of the mixing network F.1 We
considerthatRA-CTDEisequivalenttotheoriginalCTDE LIAM(θ ,ϕ)=E (cid:2) P ·F(QN,s;ϕ)(cid:3) (13)
i i τ,a,rext,τ′∈D i
paradigmbasedonthefollowingtheorem. (cid:16) (cid:17)
P =−2 rext+βrint+γmaxQ (τ′,a′)−F(QN,s;ϕ) (14)
Theorem1. Let{θ }N betheparametersofQfunctions,ϕ i i T
i i=1 a′
betheparametersofthemixingnetworkF inCTDE,LGbe
Though the training paradigm of IAM also uses N tar-
theglobaltargetinEq1,N ={1,...,N}betheagentsset,
getsmotivatedbythereward-addingwaylikeIQL,thetar-
QN={Q (τ ,a ;θ ),Q (τ ,a ;θ ),...,Q (τ ,a ;θ )},
1 1 1 1 2 2 2 2 N N N N get LIAM still contains other agents’ information and the
{τ,a,rext,τ′} ∼ D, assume ∀i,j ∈ N,θ ̸= θ , then i
i j essence of IAM is an improved CTDE instead of an inde-
∀i∈N ,thefollowingequationshold:
pendenttrainingmethod.
∂LG(θ,ϕ) ∂LE(θ ,ϕ)
= i i (8)
∂θ ∂θ Experiments
i i
∂LG(θ,ϕ)
=
1 (cid:88)N ∂LE
i
(θ i,ϕ)
(9)
T exo pld oe im
t
do in fs ft er ra et ne tt eh ne vih ri og nh me ef nfi tscie tonc cy ono df uo cu tr aa ll ag ro gr eith nm um, bw ee
r
∂ϕ N ∂ϕ
i=1 of experiments, including StarCraft II Micromanagement
(SMAC)(Samvelyanetal.2019),GoogleResearchFootball
TheTheorem1isprovedintheAppendix.Accordingto (GRF)(Kurachetal.2020)andMulti-AgentParticleEnvi-
it, we draw the conclusion that the CTDE’s essence in up- ronment(MPE)(MordatchandAbbeel2018).Weconduct5
dating gradients of {θ i}N
i=1
and ϕ is to calculate the global randomseedsforeachalgorithmandreportthe1st,median,
target LG and then respectively perform N gradient back- and 3rd quartile results. Due to space limitations on pages,
propagationstepsforeachagent.Thereforewecanequiva- weleavetheMPEexperimentsintheAppendix.
lentlyfactorizetheglobaltargetLG intoN individualones
denotedbyLE inRA-CTDE.ThefactorizedtargetLE pro- ExperimentsSetup
i i
vides an interface for adding rewards and we exhibit the
StarCraft II Micromanagement The StarCraft Multi-
reward-addingwaybasedonthefollowingcorollary.
AgentChallenge(Samvelyanetal.2019)isapopularbench-
Corollary1. Let{θ i}N i=1betheparametersofQfunctions, markincooperativemulti-agentenvironments,whereagents
N={1,...,N}betheagentsset,LVDN betheLE’sspecial mustformgroupsandworktogethertoattackbuilt-inAIen-
i i
caseofVDN,{τ,a,rext,τ′}∼D,assume∀i,j ∈ N,θ ̸= emies. The controlled units only access local observations
i
θ ,then∀i∈N: within a limited field of view and take discrete actions. At
j
eachtimestep,everyagenttakesanaction,andthentheen-
LVDN(θ)=E [P ·Q (τ ,a ;θ )] (10)
i τ,a,rext,τ′∈D i i i i i vironment feedbacks a global team reward, which is com-
P=−2(cid:16) rext+RVDN+γmaxQ−(τ′,a′)−Q (τ ,a )(cid:17) (11) putedbytheaccumulativedamagepointtotheenemies.To
i i a′ i i i i i i evaluatetheefficacyofdifferentalgorithms,weemploythe
N N trainingparadigmaspreviousnotableworks(Duetal.2019;
(cid:88) (cid:88)
RVDN=γmax Q−(τ′,a′)− Q (τ ,a ) (12) Zhouetal.2020a)whichutilizes32parallelrunnerstogen-
i j j j j j j
a′ eratetrajectoriesandstorethemintobatches.
j=1,j̸=i j=1,j̸=i
GoogleResearchFootball TheAcademyscenariosofthe
WeconsiderthespecialcaseVDN(Sunehagetal.2017)
GoogleResearchFootballenvironment(Kurachetal.2020)
andtransformitintotheRA-CTDEform,wherethemixing
areinherentlycooperativetasksthatsimulatepartialfootball
network F is the calculation of summing over all Q func-
i match scenes. We use the Floats (115-dimensional vector)
tions.OnthebasisoftheEq10,11,and12inCorollary1,
observationsettingincludingplayers’coordinates,ballpos-
we realize that VDN can be factorized into N targets like
session, ball direction, active player, and game mode. The
IQL(Tan 1993). But the essential difference between VDN
GRF is a highly sparse reward benchmark because it only
andIQL(Tan1993)isthattheformeraddscertainintrinsic
feedbacksaglobalteamrewardr intheend,i.e.,+1bonus
rewards {RVDN}N into {P }N . In other words, when
i i=1 i i=1 whenscoringagoaland-1bonuswhenconcedingone.
theRVDN arenotincorporatedinEq11,VDNfundamen-
i
tallyboilsdowntoIQL.TherewardRVDN isincorporated PerformanceComparisons
i
1Please note that although LE in 6 is the same across differ- To demonstrate the effectiveness of IAM, we combine it
i
ent agents, their corresponding computed gradients are different, withtworepresentativeCTDEalgorithms:QMIXandVDN,
whichisdetailedintheAppendix. whichrepresenttwowaysofvaluefactorization,i.e.,linearFigure4:PerformancecomparisonsforvariousmapsinSMAC.
and non-linear. We denote them as QMIX-IAM and VDN- thanprominentCTDEmethods,i.e.QPLEXandQatten.
IAM respectively. To compare the performance of differ-
ent rewards, we choose different types of reward-shaping StrengthsofIAM
methods as baselines: (1) Curiosity-based intrinsic rewards
An explicable example of IAM’s impact. We visualize
in EMC (Zheng et al. 2021). It’s a representative CTDE’s
an illuminating map 8m vs 9m in Figure 5, demonstrating
reward-shaping method based on curiosity. To fairly com-
howtheIAMimprovesQMIX’sperformancebyactionten-
paretheimpactofrewards,weremoveitsepisodicmemory
dency consistency. Among 3 training policy stages, QMIX
andincorporateitwithVDNandQMIXdenotedbyVDN-
takes the most samples to achieve Stage 2. The essential
EmC and QMIX-EmC respectively. (2) Add world model
reason is that agents cannot reach team unanimity when
based reward (Ma et al. 2022) into RA-CTDE, denoted by
attacking, causing their dispersion of firepower. Under the
VDN-WM and QMIX-WM. (3) LIIR (Du et al. 2019) that
guidance of our action model intrinsic reward, agents will
utilizes learned intrinsic rewards. The remaining baselines
take the initiative to cultivate a tacit understanding of each
are CTDE algorithms: (4) QPLEX. (5) Qatten. (6) QMIX.
other’s action tendencies. Then agents can quickly achieve
(7) VDN. For ease of comparison, we separate the perfor-
theteam’sconsistentgoalwithonlyafewtrainingsamples,
mance comparison of QMIX-IAM and VDN-IAM, details
thusgreatlyimprovingsampleefficiencythanQMIX.
onthelatteraredemonstratedintheAppendix.
IAM can also obtain improved performance in
QMIX-IAM outperforms baselines. As shown in Fig- highly sparse reward environments. To evaluate
ure 4, the performance of QMIX has been significantly IAM’s performance in deeply sparse reward environ-
improved after using the action model based reward, and ments, we choose two challenging tasks from GRF
QMIX-IAM outperforms other baselines in most scenar- including Academy run pass and shoot with keeper and
ios, especially on several very hard maps requiring strong Academy pass and shoot with keeper. We choose QMIX
team cooperation. It indicates that the action model based and VDN as baselines. As shown in Figure 6 (a) and (b),
reward can encourage consistent policy behaviors among our method can significantly enhance the performance of
agents and improve the performance of the CTDE algo- QMIX and VDN, which indicates that IAM generalizes
rithm. When using the exploration-based reward alone, wellinsparse-rewardenvironmentalscenarios.
QMIX-EmConlyachievesperformanceimprovementsover
QMIX on 6h vs 8z and 3s vs 5z, which indicates that the Ablation: Our proposed intrinsic reward outperforms
exploration-based reward lacks generalization for cooper- others when using RA-CTDE. In order to compare the
ative tasks. Based on the world model intrinsic reward, performance of different rewards added in RA-CTDE, we
the performance of QMIX only has performance improve- compare IAM with additional baselines: (1) Add curiosity
mentson3s5z.Thisindicatesthattheworldmodelbasedre- based reward in EMC, denoted by VDN-C and QMIX-C.
wardcannotgeneralizewellincomplexscenariosforhigh- (2)Addrandomnetworkdistillation(RND)rewardintoRA-
dimensionalobservationsandlacksinreflectingagents’ac- CTDE,denotedyVDN-RNDandQMIX-RND.Weconduct
tion tendencies. Besides, QMIX-IAM also performs better these algorithms in 8m vs 9m and demonstrate results inFigure5:AvisualizationexampleofIAMon8m vs 9m.Inthistask,agentsneedtoobtainathree-stageteampolicytowin.
Instage1,agentsneedtobespreadouttomaximizethedistractionofenemyattacks.Instage2,agentsneedtomaximizethe
concentrationoffirepoweronthesameenemyandreducetheenemy’snumbers.Instage3,agentsneedtoescapequicklywhen
theyarelowonbloodtoavoidbeingattackedandincreasesurvivaltime.Amongthem,stage2isthehardesttolearnbecause
theagentsneedtocooperatetoachievethesamepolicytarget,i.e.actiontendencyconsistency.(a),(b),and(d)representthree
teampolicystagesofQMIX-IAM.(d)exhibitsthedistributedfireagainsttheenemyofQMIX.
Figure 6: Ablation experiments. (a) and (b) show the performance comparison in two scenes of GRF. (c) and (d) exhibit the
performancecomparisoninRA-CTDEcombinedwithdifferentrewards.
Figure6.BothVDN-IAMandQMIX-IAMoutperformoth- ConclusionsandLimitations
erswhichimpliesthatpredictiveinformationaboutactionis
beneficial for cooperation. Besides, after using RA-CTDE, We find that the CTDE algorithm suffers from low sam-
all these intrinsic rewards have achieved performance im- ple efficiency and attribute it to the team consensus incon-
provements, indicating that the way RA-CTDE uses intrin- sistency among agents. To tackle this problem, we design
sic rewards is reasonable and provides a new direction for a novel intrinsic action model based reward and transform
CTDE to utilize intrinsic rewards. Besides, Exploration- the CTDE into an equivalent variant, RA-CTDE. Then we
based rewards don’t perform as well as the action model useanovelintegrationofintrinsicrewardswithRA-CTDE.
based rewards, which indicates that the RA-CTDE frame- Since our action model intrinsic rewards can boost consis-
workcanusedifferentintrinsicrewardsbutthecooperative tent team policy and our proposed RA-CTDE can flexibly
intrinsicrewardsperformbetterthantheexplorationonein use calculated intrinsic rewards, our method shows signifi-
cooperativemulti-agentsystems. cantoutperformanceonchallengingtasksintheSMACand
GRF benchmarks. The limitations of our work are that we
Besidestheaforementionedexperiments,wealsoconduct did not consider environments with continuous state-action
ablationexperimentstodemonstratetheoutperformanceof space and did not make specific designs for heterogeneous
RA-CTDE’sreward-addingmannerandRA-CTDE’sequiv- agents.Forfuturework,wewillconductadditionalresearch
alencetoCTDE,whicharedetailedintheAppendix. intheaforementioneddirections.Acknowledgements Curriculum Learning and Self-Play. arXiv preprint
arXiv:2302.07515.
ThisworkwassupportedinpartbytheNationalKeyR&D
Program of China (2022ZD0116402), NSFC 62273347, Ma,Z.;Wang,R.;Li,F.-F.;Bernstein,M.;andKrishna,R.
JiangsuKeyResearchandDevelopmentPlan(BE2023016). 2022. ELIGN:ExpectationAlignmentasaMulti-AgentIn-
trinsicReward. AdvancesinNeuralInformationProcessing
References Systems,35:8304–8317.
Berner, C.; Brockman, G.; Chan, B.; Cheung, V.; De˘biak, Matignon, L.; Laurent, G. J.; and Le Fort-Piat, N. 2012.
P.;Dennison,C.;Farhi,D.;Fischer,Q.;Hashme,S.;Hesse, Independent reinforcement learners in cooperative markov
C.;etal.2019. Dota2withlargescaledeepreinforcement games: a survey regarding coordination problems. The
learning. arXivpreprintarXiv:1912.06680. KnowledgeEngineeringReview,27(1):1–31.
Bernstein,D.S.;Givan,R.;Immerman,N.;andZilberstein, Mguni, D. H.; Jafferjee, T.; Wang, J.; Slumbers, O.; Perez-
S.2002. ThecomplexityofdecentralizedcontrolofMarkov Nieves, N.; Tong, F.; Yang, L.; Zhu, J.; Yang, Y.; and
decision processes. Mathematics of operations research, Wang, J. 2021. Ligs: Learnable intrinsic-reward gener-
27(4):819–840. ation selection for multi-agent learning. arXiv preprint
arXiv:2112.02618.
Bo¨hmer,W.;Rashid,T.;andWhiteson,S.2019.Exploration
withunreliableintrinsicrewardinmulti-agentreinforcement Mordatch,I.;andAbbeel,P.2018. Emergenceofgrounded
learning. arXivpreprintarXiv:1906.02138. compositionallanguageinmulti-agentpopulations. InPro-
ceedings of the AAAI conference on artificial intelligence,
Cao,Y.;Yu,W.;Ren,W.;andChen,G.2012. Anoverview
volume32.
ofrecentprogressinthestudyofdistributedmulti-agentco-
ordination. IEEE Transactions on Industrial informatics, Motlaghzadeh,K.;Eyni,A.;Behboudian,M.;Pourmoghim,
9(1):427–438. P.; Ashrafi, S.; Kerachian, R.; and Hipel, K. W. 2023. A
multi-agent decision-making framework for evaluating wa-
Chitnis, R.; Tulsiani, S.; Gupta, S.; and Gupta, A. 2020.
terandenvironmentalresourcesmanagementscenariosun-
Intrinsic motivation for encouraging synergistic behavior.
derclimatechange. ScienceofTheTotalEnvironment,864:
arXivpreprintarXiv:2002.05189.
161060.
Dahiya,A.;Aroyo,A.M.;Dautenhahn,K.;andSmith,S.L.
Mushtaq, A.; Haq, I. U.; Sarwar, M. A.; Khan, A.; Khalil,
2023. A survey of multi-agent Human–Robot Interaction
W.; and Mughal, M. A. 2023. Multi-Agent Reinforcement
systems. RoboticsandAutonomousSystems,161:104335.
LearningforTrafficFlowManagementofAutonomousVe-
Du,Y.;Han,L.;Fang,M.;Liu,J.;Dai,T.;andTao,D.2019. hicles. Sensors,23(5):2373.
Liir:Learningindividualintrinsicrewardinmulti-agentre-
Oliehoek,F.A.;andAmato,C.2015.Aconciseintroduction
inforcementlearning. AdvancesinNeuralInformationPro-
todecentralizedpomdps.
cessingSystems,32.
Rashid,T.;Samvelyan,M.;DeWitt,C.S.;Farquhar,G.;Fo-
Hernandez-Leal, P.; Kartal, B.; and Taylor, M. E. 2019.
erster,J.;andWhiteson,S.2020. Monotonicvaluefunction
Agent modeling as auxiliary task for deep reinforcement
factorisation for deep multi-agent reinforcement learning.
learning. In Proceedings of the AAAI conference on arti-
The Journal of Machine Learning Research, 21(1): 7234–
ficialintelligenceandinteractivedigitalentertainment,vol-
7284.
ume15,31–37.
Hu¨ttenrauch,M.;Sˇosˇic´,A.;andNeumann,G.2017. Guided Sallab,A.E.;Abdou,M.;Perot,E.;andYogamani,S.2017.
Deep reinforcement learning framework for autonomous
deep reinforcement learning for swarm systems. arXiv
driving. arXivpreprintarXiv:1704.02532.
preprintarXiv:1709.06011.
Samvelyan, M.; Rashid, T.; De Witt, C. S.; Farquhar, G.;
Iqbal, S.; and Sha, F. 2019. Coordinated exploration via
Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Fo-
intrinsic rewards for multi-agent reinforcement learning.
erster,J.;andWhiteson,S.2019. Thestarcraftmulti-agent
arXivpreprintarXiv:1905.12127.
challenge. arXivpreprintarXiv:1902.04043.
Jaques,N.;Lazaridou,A.;Hughes,E.;Gulcehre,C.;Ortega,
Silver, D.; Singh, S.; Precup, D.; and Sutton, R. S. 2021.
P.;Strouse,D.;Leibo,J.Z.;andDeFreitas,N.2019. Social
Rewardisenough. ArtificialIntelligence,299:103535.
influence as intrinsic motivation for multi-agent deep rein-
forcementlearning. InInternationalconferenceonmachine Son,K.;Kim,D.;Kang,W.J.;Hostallero,D.E.;andYi,Y.
learning,3040–3049.PMLR. 2019. Qtran: Learning to factorize with transformation for
cooperativemulti-agentreinforcementlearning. InInterna-
Kurach,K.;Raichuk,A.;Stan´czyk,P.;Zaja˘c,M.;Bachem,
tionalconferenceonmachinelearning,5887–5896.PMLR.
O.;Espeholt,L.;Riquelme,C.;Vincent,D.;Michalski,M.;
Bousquet,O.;etal.2020. Googleresearchfootball:Anovel Strouse,D.;Kleiman-Weiner,M.;Tenenbaum,J.;Botvinick,
reinforcement learning environment. In Proceedings of M.;andSchwab,D.J.2018. Learningtoshareandhidein-
the AAAI Conference on Artificial Intelligence, volume 34, tentionsusinginformationregularization. Advancesinneu-
4501–4510. ralinformationprocessingsystems,31.
Lin, F.; Huang, S.; Pearce, T.; Chen, W.; and Tu, W.- Sunehag,P.;Lever,G.;Gruslys,A.;Czarnecki,W.M.;Zam-
W. 2023. TiZero: Mastering Multi-Agent Football with baldi, V.; Jaderberg, M.; Lanctot, M.; Sonnerat, N.; Leibo,J. Z.; Tuyls, K.; et al. 2017. Value-decomposition net-
works for cooperative multi-agent learning. arXiv preprint
arXiv:1706.05296.
Tan, M. 1993. Multi-agent reinforcement learning: Inde-
pendentvs.cooperativeagents. InProceedingsofthetenth
internationalconferenceonmachinelearning,330–337.
Wang, J.; Ren, Z.; Han, B.; Ye, J.; and Zhang, C. 2020a.
Towardsunderstandinglinearvaluedecompositionincoop-
erativemulti-agentq-learning.
Wang, J.; Ren, Z.; Liu, T.; Yu, Y.; and Zhang, C. 2020b.
Qplex: Duplex dueling multi-agent q-learning. arXiv
preprintarXiv:2008.01062.
Wang, T.; Wang, J.; Wu, Y.; and Zhang, C. 2019.
Influence-based multi-agent exploration. arXiv preprint
arXiv:1910.05512.
Wiering, M. A.; et al. 2000. Multi-agent reinforcement
learning for traffic light control. In Machine Learning:
Proceedings of the Seventeenth International Conference
(ICML’2000),1151–1158.
Zhang,S.;Cao,J.;Yuan,L.;Yu,Y.;andZhan,D.-C.2023.
Self-Motivated Multi-Agent Exploration. arXiv preprint
arXiv:2301.02083.
Zheng, L.; Chen, J.; Wang, J.; He, J.; Hu, Y.; Chen, Y.;
Fan, C.; Gao, Y.; and Zhang, C. 2021. Episodic multi-
agent reinforcement learning with curiosity-driven explo-
ration.AdvancesinNeuralInformationProcessingSystems,
34:3757–3769.
Zhou, M.; Liu, Z.; Sui, P.; Li, Y.; and Chung, Y. Y. 2020a.
Learning implicit credit assignment for cooperative multi-
agentreinforcementlearning. Advancesinneuralinforma-
tionprocessingsystems,33:11853–11864.
Zhou, M.; Luo, J.; Villella, J.; Yang, Y.; Rusu, D.; Miao,
J.; Zhang, W.; Alban, M.; Fadakar, I.; Chen, Z.; et al.
2020b. Smarts: Scalable multi-agent reinforcement learn-
ingtrainingschoolforautonomousdriving. arXivpreprint
arXiv:2010.09776.AlgorithmDetails
PseudoCode LIAM(θ ,ϕ)=E (cid:2) P ·F(QN,s;ϕ)(cid:3) (18)
i i τ,a,rext,τ′∈D i
IAM’spseudocodeisshowninAlgorithm1. (cid:16) (cid:17)
P =−2 rext+βrint+γmaxQ (τ′,a′)−F(QN,s;ϕ)
i i T
a′
Algorithm1:IntrinsicActionTendencyConsistencyforco- (19)
operativemulti-agentreinforcementlearning
InourIAMmethod,wesetDisfunctionasL2distance
1: Initialize the agents’ Q functions {Q i}N i=1 with pa- andsetrint asrAM.Besides,asEq18and19indicate,the
rameters θ = {θ }N , Action models {FAM}N i i
i i=1 i i=1 intrinsicrewardcanbeindependentlyaddedintoeachagent-
with parameters {ω i}N i=1, CTDE mixing network associatedlossfunctionreadilysincetheTheorem1facili-
F(Q 1,Q 2,...,Q N,s) with parameters ϕ, replay buffer tatesthefactorizationofthegloballossfunction.
D,numberofmini-batchN ,rolloutnumbersN ,roll-
B r
outlengthT,discountfactorγ,Qfunctionlearningrate ImaginedObservation
α, mixing network F learning rate ν, Action model
learning rate η, intrinsic reward coefficient β, environ- Imagined Observation is obtained by function F im(o i,j),
ment E, CTDE target network Q (s,a), transitions whichiscalculatedbasedonthecentralagent’sobservation
T
(cid:0) st,{ot}N ,{at}N ,rt ,st+1(cid:1) . o i. It represents the relative observation information in the
i i=1 i i=1 ext central agent’s view, by switching the viewpoints from i to
2: forn=1toN r do j,i.e.,thinkofagentj’scoordinatesas(0,0).Figure7de-
3: //CollectRollouts
tails a clear calculation example in a small-scale environ-
4: fort=1toT do
5: SampleglobalstatesstfromenvironmentE ment. This calculation manner is also applicable to general
6: Getobservations{ot}N byfunctionO(st,i) MARL environments due to the compatibility of their ob-
i i=1 servation feature vector settings. We take SMAC as an ex-
7: Sampleactions{at}N from{(Qt(ot,·)}N
i i=1 i i i=1 ample, the observation feature vector of a single agent is a
8: Takeactionsthensampler et xtands t+1fromE
concatenationofallagents’observation,wheretheinforma-
9: AppendtransitionstoreplaybufferD
tionforinvisibleagentsissetto0,asshowninTable1and
10: //TrainAgentsandActionModels
Eq40,Therefore,wecanimagineagentj’sobservationby
11: SampleN B transitions computingrelativeinformationfromj’sperspective,which
12: fori=1toN do
includesrelativedistances,relativecoordinates,correspond-
13: //ComputeActionModelRewards
ingunithealth,unitshield,andunittype.Thiscomputation
14: Compute{o ij}| jS =( 1oi)|usingF im(o i,j) manneralsoappliestoGRF(Kurachetal.2020)andMPE
15: ComputerAM usingEq16 (MordatchandAbbeel2018)becausetheyhavesimilarob-
i
16: ComputeLAM usingEq17 servationfeaturestructures.
i
17: ϕ i ←ϕ i−η∇LA i M
18: //TrainAgentPolicy DefinitionandProofs
19: ComputeIAMLossLIAM usingEq18,19
i DefinitionofRA-CTDE
20: θ i ←θ i−α∇LI iAM
21: ω i ←ω i−ν∇LI iAM Definition 2. (Reward-Additive CTDE). Let θ = {θ i}N i=1
22: endfor be the parameters of Q functions, F be the mixing net-
23: UpdatetargetnetworkQ T(s,a) work in CTDE, N = {1,...,N} be the agents set, QN =
24: endfor {Q 1(τ 1,a 1;θ 1),Q 2(τ 2,a 2;θ 2),...,Q N(τ N,a N;θ N)},{τ,a,
25: endfor rext,τ′}∼D, assume ∀i,j ∈ N,θ i ̸= θ j, then Reward-
AdditiveCTDEmeanscomputing{LE(θ ,ϕ)}N inEq20
i i i=1
andEq21andthenupdatingtheirparametersrespectively.
Theo isobtainedbytheimaginefunction:
ij
The term P is not involved in the gradient calculation but
o ij =F im(o i,j) (15) asascalar.Formally:
Theintrinsicrewardisobtainedbytheactionmodelpre- LE(θ ,ϕ)=E (cid:2) P ·F(QN,s;ϕ)(cid:3)
dictionerror: i i τ,a,rext,τ′∈D
(20)
1 (cid:88)
r iAM =−
|S(i)|
Dis(F iAM(o ij,·;ω i)−Q i(o i,·;θ i))
P
=−2(cid:16)
rext+γmaxQ
(τ′,a′)−F(QN,s;ϕ)(cid:17)
j∈S(i) a′ T
(16) (21)
Theactionmodelistrainedtominimizethelossfunction:
N ProofofTheorem1
(cid:88)
LA i M = ||F iAM(o i,·;ω i)−Q i(o i,·;θ i)||2 2 (17) Theorem2. Let{θ i}N i=1betheparametersofQfunctions,ϕ
i=1 betheparametersofthemixingnetworkF inCTDE,LGbe
Q functions and mixing network F are trained to mini- theglobaltargetinEq24,N ={1,...,N}betheagentsset,
mizethelossfunction: QN={Q (τ ,a ;θ ),Q (τ ,a ;θ ),...,Q (τ ,a ;θ )},
1 1 1 1 2 2 2 2 N N N NFigure7:AnexampleoftheImaginedObservationcalculation.Weonlyusetheagenti’sobservationtocalculatetheimagined
observation of agent j, by switching the central viewpoint. In the i’s viewpoint, the i’s coordinate position is (0, 0), and the
i ’s coordinate position is (0, 1). When we set j as the central viewpoint, the j’s coordinate position is (0, 0), then we can
j
calculatethei’scoordinatepositionas(0,-1),whichrepresentsapartofo whichstandsforagentj’sobservationfromagent
ij
i’simagination,showninthegreencirculararea.Besides,itshouldbenotedthattheo isobtainedbythepartialobservation
ij
o ,thereforetheactualmeaningfulobservationofo isthereceptivefieldintersectionofagentiandj,asshowninthegrey-
i ij
shadedarea.
{τ,a,rext,τ′} ∼ D, assume ∀i,j ∈ N,θ ̸= θ , then Based on the Definition 2, we can easily deduce the fol-
i j
∀i∈N ,thefollowingequationshold: lowingderivations:
∂LG(θ,ϕ) = ∂LE i (θ i,ϕ) (22) ∂LE i (θ i,ϕ) =E(cid:20) P · ∂F(QN,s;θ i,ϕ) · ∂Q i(cid:21) (27)
∂θ i ∂θ i ∂θ i ∂Q i ∂θ i
∂LG ∂( ϕθ,ϕ)
=
N1 ·(cid:88)N ∂LE
i
∂( ϕθ i,ϕ)
(23) ∂LE
i
(θ,ϕ) =E(cid:20)
P ·
∂F(QN,s;θ,ϕ)(cid:21)
(28)
i=1
∂ϕ ∂ϕ
Proof. In the Centralized Training with Decentralized Ex- Since the derivatives of ϕ computed for each target
ecution (CTDE) paradigm, Q functions are trained by the LE
i
(θ i,ϕ)aresame,weobtainthefollowingconclusions:
followingglobalTD-errortarget:
(cid:104) (cid:105)2 ∂LG(θ,ϕ) ∂LE(θ ,ϕ)
LG(θ,ϕ)=E rext+γmaxQ (τ′,a′)−F(QN,s) (24) = i i (29)
T
a′ ∂θ i ∂θ i
tioW nse
:
use the chain rule and we get the following deriva- ∂LG(θ,ϕ)
=
1 ·(cid:88)N ∂LE
i
(θ i,ϕ)
(30)
∂LG(θ,ϕ) =E(cid:20) ∂LG
·
∂P
·
∂F(QN,s;θ,ϕ))
·
∂Q i(cid:21) ∂ϕ N i=1 ∂ϕ
∂θ ∂P ∂F(QN,s) ∂Q ∂θ
i i i
(cid:20) ∂F(QN,s;θ,ϕ) ∂Q (cid:21)
=E −P ·(−1)· · i ProofofCorollary
∂Q ∂θ
i i Corollary2. Let{θ }N betheparametersofQfunctions,
=E(cid:20)
P ·
∂F(QN,s;θ,ϕ)
·
∂Q i(cid:21)
(25)
N={1,...,N}bethi eai= ge1 ntsset,LV
i
DN betheLE
i
’sspecial
∂Q i ∂θ i caseofVDN,{τ,a,rext,τ′}∼D,assume∀i,j ∈ N,θ i ̸=
θ ,then∀i∈N:
∂LG(θ,ϕ) (cid:20) ∂LG ∂P ∂F(QN,s;θ,ϕ))(cid:21) j
=E · · LVDN(θ)=E [P ·Q (τ ,a ;θ )] (31)
∂ϕ ∂P ∂F(QN,s) ∂ϕ i τ,a,rext,τ′∈D i i i i i
(cid:16) (cid:17)
(cid:20) ∂F(QN,s;θ,ϕ)(cid:21) P=−2 rext+RVDN+γmaxQ−(τ′,a′)−Q (τ ,a ) (32)
=E −P ·(−1)· i i a′ i i i i i i
∂ϕ
N N
(cid:20) ∂F(QN,s;θ,ϕ)(cid:21) RVDN=γmax (cid:88) Q−(τ′,a′)−(cid:88) Q (τ ,a ) (33)
=E P · (26) i a′ j j j j j j
∂ϕ j=1,j̸=i j=1,j̸=iProof. VDNtrainingparadigmuseslinearvaluefactoriza- Discussion: Reward-adding way Please note that our
tion,specifically, reward integrating way in Eq 18, 19 is different from
that in EMC (Zheng et al. 2021). The latter calculates N
N
Q (τ′,a′)=(cid:88) Q−(τ′,a′) (34) individual intrinsic rewards for each agent but averages
T i i i
them to one then adds it to the global team reward, i.e.
i=1
rint = 1 (cid:80)N ||Q˜ (τ ,·) − Q (τ ,·)|| . This shared in-
F(QN,s)=(cid:88)N
Q (τ ,a ) (35)
trinsicrewN ardi d= o1 esn’ti rei wardagei ntsi diffe2
rently,i.e.∀i,j ∈
i i i N,rint = rint in Eq 19, which provides an overall cu-
i=1 i j
riosity evaluation of all agents’ behaviors. In contrast, our
WedefineLVDN(θ )asinEq31thenwecanderivethat:
i i reward-adding way retains the individual behavior differ-
∂LG(θ) (cid:20) ∂F(QN,s;θ,ϕ) ∂Q (cid:21) encebetweenagentsandrewardsgoodandbadactionsdif-
=E P · · i
∂θ i i ∂Q i ∂θ i ferentlyasRV i DN does.Thisallowsintrinsicrewardstopro-
(cid:20) (cid:21) videmoretailoredandpersonalizedfeedbacktoeachagent,
∂Q
=E P ·1· i whichhelpstoimprovetheoverallperformance.
i ∂θ
i
∂LVDN(θ ) ExperimentSettings
= i i (36)
∂θ In this section, we describe the physical meanings of ob-
i
where: servations in three environments, which help illustrate the
(cid:16) (cid:17) feasibilityofcomputingimagineobservationsbyswitching
P=−2 rext+RVDN+γmaxQ−(τ′,a′)−Q (τ ,a ) (37)
i i i i i i i i viewpoints.
a′
N N
RVDN=γmax (cid:88) Q−(τ′,a′)−(cid:88) Q (τ ,a ) (38) SMAC
i j j j j j j
a′ TheStarCraftMulti-AgentChallengeisawell-knownreal-
j=1,j̸=i j=1,j̸=i
timestrategy(RTS)game.AkintomostRTSs,independent
agents must form groups and work together to solve chal-
Now we have proven the Corollary 2. It’s worth noting lenging combat scenarios, facing off against an opposing
thatthelossfunctionLIQLofIQLis: armycontrolledcentrallybythegame’sbuilt-inscriptedAI.
i
Thecontrolledunitsonlyaccesslocalobservationswithina
LIQL(θ )=E[P′·Q (τ ,a )]
i i i i i i limited field of view. In the agent’s sight range, the feature
(cid:16) (cid:17)
P′ = rext+γmaxQ−(τ′,a′)−Q (τ ,a ) (39) vector contains attributes related to both allied and enemy
i a′ i i i i i i units,includingdistancefromtheunit,relativex,andypo-
By comparing the difference between Eq 12 and Eq 39, sitions,healthandshieldstatus,aswellasthetypeofunit,
wecanconcludethattheVDNisaspecialtypeofIQL,ex- i.e. what kind of unit it is. Details are shown in Table 1.
ceptthatitaddsanintrinsicrewardRVDN.Inotherwords, The observation feature of each agent can be concatenated
i
thesuperiorperformanceofVDNoverIQLcanbeattributed by different types of features, as shown in Eq. 40. The ab-
tothisrewardfunctionRVDN.Besides,itshouldbenoticed breviationsaredetailedinTable1.Itisworthnotingthatal-
i
that:1)RVDN iscalculatedthroughtheinteractiveinforma- thougheachagentcanonlyobservelocalinformationwithin
i
tion from other agents, i.e. the difference between the tar- itsfield,theinformationofinvisibleagentsisstillrecorded
get Q functions and Q functions. 2) The intrinsic rewards inthecorrespondingvectorposition,whereitissetto0.
{RVDN}N possessed by each agent are different, which
i i=1
Observation Feature=Concat(amf,
allows each agent to have its own reward guidance rather
than a uniform one. Similarly, our reward design and inte- ef ×Enemy number,
(40)
grationwayincorporatethesetwoaspects,whichalsocon- af ×Ally number,
siderotheragents’informationandprovideeachagentwith
auf)
an individual guidance signal. This demonstrates the ratio-
nalityofourIAMmethod. Theactionspaceofeachagentincludes:donothing,move
up,movedown,moveleft,moveright,attack [enemyID].At
Discussions eachtimestep,everyagenttakesactionsandthentheenvi-
Discussion: The IAM impact on credit assignment ronmentreturnsaglobalteamreward,whichiscomputedby
CTDEessentiallycompletesthepotentialcreditassignment theaccumulativedamagepointandthekillingbonusdealtto
for each agent, which is entirely determined by extrinsic theenemies.
team reward. The specific assignment process can be illus-
Multi-agentParticleEnvironment
tratedinEq25.Whenthemixingnetworkisfixed,thelarger
theabsolutevalueofPis,themoretheallocationagentican AsshowninFigure8,theCooperativeNavigationisatask
gain.However,whenteamrewardsaresparse,thecalculated thatteststhequalityofteamcooperation,whereagentsmust
creditassignmentmaynotbeentirelyreasonable.Byadding cooperatetoreachasetofLlandmarks.Besides,Agentsare
intrinsicrewardsintothefirstterm,wecanalleviatethein- collectively rewarded based on the occupancy of any agent
sufficientcreditassignmentcausedbysparserewards. onanygoallocationandpenalizedwhencollidingwitheachFeature PhysicalMeaning
Agentmovementfeatures(amf) whereitcanmoveto,heightinformation,pathinggrid
Enemyfeatures(ef) availabletoattack,health,relativex,relativey,shield,unittype
Allyfeatures(af) visible,distance,relativex,relativey,shield,unittype
Agentunitfeatures(auf) health,shield,unittype
Table1:FeaturevectorsofobservationsinSMAC
Feature PhysicalMeaning
Ballinformation position,direction,rotation,ball-ownedteam,ball-ownedplayer
Leftteam positionanddirection,tiredfactor,yellowcard,active,roles
Rightteam positionanddirection,tiredfactor,yellowcard,active,roles
Controlledplayerinformation active,designated,stickyactions
Matchstate score,stepsleft,gamemode
Table2:FeaturevectorsofobservationsinGRF
other.Ateverytimestep,eachagentobtainsthefeaturesof (Kurachetal.2020),whichconsistsofplayers’coordinates,
the neighbors and landmarks, which are illustrated in Ta- ballpossession,balldirection,activeplayerandgamemode.
ble 3, and then takes action from the action space (move Details are described in Table 2. The action space consists
up, move down, move left, move right, do nothing). After of move actions (in 8 directions), different ways to kick
that, the environment gives a global team reward as feed- the ball(short and long passes, shooting, and high passes),
back,whichisbasedontheoccupancyofgoallocationsand sprint,intercept.Theenvironmentfeedbackasparseglobal
thecollisionwitheachother. teamrewardrintheend,i.e.+1bonuswhenscoringagoal
and-1bonuswhenconcedingone.
Academy run pass and shoot with keeper. Two of our
playersattempttoscorefromoutsidethepenaltyarea,with
one positioned on the same side as the ball and unmarked,
whiletheotherisinthecenternexttoadefenderandfacing
theopposingkeeper.
Academy pass and shoot with keeper. Two of our
playersattempttoscorefromoutsidethepenaltyarea,with
one positioned on the same side as the ball and next to a
defender, while the other is in the center unmarked and
facingtheopposingkeeper.
Figure8:CooperativeNavigation
AdditionalExperiments
IAM can improve the performance of VDN. Figure 10
Feature PhysicalMeaning showstheresultsoftheperformancecomparisonsinSMAC.
We integrate IAM with VDN and denote it by VDN-IAM.
Agentfeatures visiblemask,position,velocity
In order to reflect the impact of IAM on VDN, we utilize
Landmarkfeatures visiblemask,position,velocity
the followingbaselines: (1)We removethe Episodic Mem-
ory design from EMC (Zheng et al. 2021) and incorporate
Table3:FeaturevectorsofobservationsinMPE
it with VDN (Sunehag et al. 2017), which is denoted by
VDN-EmC.(2)Weusetheworldmodelbasedintrinsicre-
GoogleResearchFootball wardinadecentralizedwayasELIGN(Mgunietal.2021)
does and denote it as IQL-WM. (3)LIIR (Du et al. 2019).
FootballAcademyisasubsetoftheGoogleResearchFoot-
(4)VDN(Sunehagetal.2017).(5)IQL(Matignon,Laurent,
ball(GRF)(Kurachetal.2020)thatcontainsdiversesmall-
and Le Fort-Piat 2012). When using linear value factoriza-
scale training scenarios with varying difficulty. The three
tioninCTDE,VDN-IAMoutperformsotherbaselinesinsix
challenging scenarios tested in our experiments are de-
hardmapsandgainsasignificantperformanceimprovement
scribed as below. The observation in Football Academy is
in8m vs 9m,3s5z vs 3s6z.
global to every agent, which differs in agent ID. For the
sake of convenience in observation calculation, we use the Performance comparison with the world model. The
Floats(115-dimensionalvector)observationsettinginGRF CooperativeNavigationtaskestablishesbarrierstotrainingFigure9:ExperimentsindifferentscenariosofCooperativeNavigationinMPE.
cooperative behaviors among the agents, as they must col- Conclusion Our experiments and conclusions can be re-
laboratetoreachdifferentreachingpointstoobtainrewards cursivelyinferredbythefollowinglogicalreasoning.1)Ex-
andavoidpenalties.Toassesstheperformanceoftheaction periments in the paper’s explicable example and Figure 9
modelintrinsicrewardinMPE,weconductedthefollowing illustrate that action model based intrinsic rewards can en-
experiments:WeusethesametrainingsettingsasELIGNin courage action tendency consistency and outperform the
3scenariosasshowninFigure9.Weemployedanindepen- worldmodelbasedintrinsicreward.2)TheTheorem2proof
dent training approach, combining the action model intrin- and the ablation experiment results in Figure 12 illustrate
sic reward and world model intrinsic reward with SACD, thatRA-CTDEisanequivalentvariantofCTDEthatcanuti-
denoted as SACD-AM and ELIGN, respectively. The test lizeN intrinsicrewards.3)ThetextitPerformanceCompar-
occupancyrateincooperativenavigationreflectsthedegree isonexperimentsdemonstratethattheactionmodelintrinsic
ofcooperationamongagents.AsshowninFigure9,theac- rewardcombinedwithQMIX(denotedbyQMIX-IAM)out-
tionmodelintrinsicrewardexhibitedasignificantimprove- performs other baselines. Since VDN and QMIX represent
mentintestoccupancycomparedtoELIGN,indicatingthat two main approaches in CTDE (i.e. linear value factoriza-
itcanencouragecooperativebehaviormoreeffectivelythan tion and nonlinear value factorization), the improved per-
theworldmodel. formance results demonstrate that our reward function can
enhancetheperformanceofCTDEtothehighestextent.4)
The proposed reward-adding way is better than EMC. ExperimentsinGRFindicatethattheIAMgeneralizeswell
To demonstrate the different performances in a reward- inenvironmentswithsparserewards.
addingwaybetweenIAMandEMC,weconductedthefol-
lowingexperiments.wecomparetheintegrationmethodof
RA-CTDEwiththatofEMC’scentralizedaddingapproach.
The latter are denoted as VDN-IAMC and QMIX-IAMC.
Wetestalgorithmsin8m vs 9mand3s5z vs 3s6zmapsand
results can be demonstrated in Figure 11. Although VDN-
IAMC and QMIX-IAMC have performance improvement,
theyarestilloutperformedbyVDN-IAMandQMIX-IAM.
ThissuggeststhatusingRA-CTDEtoleverageintrinsicre-
ward is better than using EMC directly. This combing way
isstillreasonablefromtheperspectiveofcreditassignment.
Theintrinsicrewardscandistinguishtheagents’actionsin-
dividually,whichaddstheglobalTD-lossterminEq14dur-
ingcreditassignmentandmakesteamrewardsassignmore
onbetteractions.
TheequivalenceofRA-CTDE Todemonstratetheaccu-
racy of Theorem 2, we conduct experiments on maps with
varyingdifficultylevels:2s3z,8m vs 9mandMMM2.With-
outusingadditionalintrinsicrewards,weintegratetheRA-
CTDE training paradigm with QMIX and VDN, denoted
asQMIX-RAandVDN-RArespectively.Thebaselinesare
QMIX and VDN. The results shown in Figure 12 illustrate
thattheperformanceofourRA-CTDEtrainingparadigmis
equivalenttoCTDEwhennointrinsicrewardisused,which
isconsistentwithourtheoremconclusion.Figure10:PerformanceofVDN-IAMinvariousmapsofSMAC.
Figure11:PerformanceComparisonindifferentwaysofcombiningIAM-basedrewardintoCTDE.
Figure12:PerformancecomparisonswhennointrinsicrewardsareaddedtoRA-CTDE.