ChronoMagic-Bench: A Benchmark for Metamorphic
Evaluation of Text-to-Time-lapse Video Generation
ShenghaiYuan1,JinfaHuang3,YongqiXu1,YaoyangLiu1,ShaofengZhang4,
YujunShi5,RuijieZhu6,XinhuaCheng1,2,JieboLuo3,LiYuan1,2,†
Project: https://github.com/PKU-YuanGroup/ChronoMagic-Bench
1PekingUniversity,ShenzhenGraduateSchool,2RabbitpreIntelligence,
3UniversityofRochester,4ShanghaiJiaoTongUniversity,
5NationalUniversityofSingapore,6UniversityofCaliforniaSantaCruz
{2401212886,chengxinhua}@stu.pku.edu.cn, shi.yujun@u.nus.edu, rzhu48@ucsc.edu
{yaoyangliu319, yongqixuing}@gmail.com, sherrylone@sjtu.edu.cn
yuanli-ece@pku.edu.cn, {jhuang90@ur,jluo@cs}.rochester.edu
Abstract
We propose a novel text-to-video (T2V) generation benchmark, ChronoMagic-
Bench1,toevaluatethetemporalandmetamorphiccapabilitiesoftheT2Vmodels
(e.g. Sora [8] and Lumiere [3]) in time-lapse video generation. In contrast to
existingbenchmarksthatfocusonvisualqualityandtextualrelevanceofgenerated
videos,ChronoMagic-Benchfocusesonthemodels’abilitytogeneratetime-lapse
videoswithsignificantmetamorphicamplitudeandtemporalcoherence.Thebench-
markprobesT2Vmodelsfortheirphysics,biology,andchemistrycapabilities,in
afree-formtextquery. Forthesepurposes,ChronoMagic-Benchintroduces1,649
prompts and real-world videosas references, categorized into four major types
of time-lapse videos: biological, human-created, meteorological, and physical
phenomena,whicharefurtherdividedinto75subcategories. Thiscategorization
ensures a comprehensive evaluation of the models’ capacity to handle diverse
and complex transformations. To accurately align human preference with the
benchmark,weintroducetwonewautomaticmetrics,MTScoreandCHScore,to
evaluatethevideos’metamorphicattributesandtemporalcoherence. MTScore
measuresthemetamorphicamplitude,reflectingthedegreeofchangeovertime,
whileCHScoreassessesthetemporalcoherence,ensuringthegeneratedvideos
maintainlogicalprogressionandcontinuity. BasedontheChronoMagic-Bench,
weconductcomprehensivemanualevaluationsoftenrepresentativeT2Vmodels,
revealingtheirstrengthsandweaknessesacrossdifferentcategoriesofprompts,and
providingathoroughevaluationframeworkthataddressescurrentgapsinvideo
generationresearch. Moreover,wecreatealarge-scaleChronoMagic-Prodataset,
containing460khigh-qualitypairsof720ptime-lapsevideosanddetailedcaptions.
Eachcaptionensureshighphysicalpertinenceandlargemetamorphicamplitude,
whichhaveafar-reachingimpactontheT2Vgenerationcommunity.
1 Introduction
Text-to-video(T2V)generativemodels[84,86,40,22,69,62,28,79]havebeendevelopedrapidly
recently. Asthenumberofmodelscontinuestogrow,thereisanurgentneedforevaluationmethods
1"Chrono"isderivedfromtheGreekword"chronos",whichmeans"time".
Preprint.Underreview.
4202
nuJ
62
]VC.sc[
1v22581.6042:viXraTable 1: Comparison of the characteristics of our ChronoMagic-Bench with existing T2V
benchmarks. Mostofthemonlyassesstwodimensions: visualqualityandtextrelevance.
Benchmark Type VisualQuality TextRelevance MetamorphicAmplitude TemporalCoherence
UCF-101[61] General ✓ ✓ ✗ ✗
Make-a-Video-Eval[59] General ✓ ✓ ✗ ✗
MSR-VTT[75] General ✓ ✓ ✗ ✗
FETV[43] General ✓ ✓ ✗ ✓
VBench[26] General ✓ ✓ ✗ ✓
T2VScore[72] General ✓ ✓ ✗ ✗
ChronoMagic-Bench Time-lapse ✓ ✓ ✓ ✓
thatalignwithhumanperception,accuratelyreflectingthespecificstrengthsandweaknessesofeach
model,therebyenablingcommunitytoadoptarchitecturesthatmeettheirrequirementsmoreeasily.
However, the current T2V benchmarks [43, 61, 59, 75, 26, 28] primarily assess the capability of
generatinggeneralvideosinsteadoftime-lapsevideos,failingtoreflecttheextentofphysicalpriors
encodedbythemodels. Additionally,theevaluationmetricstheyusemainlyfocusonvisualquality
and textual relevance, from early metrics including FID [24], FVD [64], and CLIPScore [23] to
morerecentonessuchasUMTScore[43],T2VQA[30],andUMT-FVD[43],almostallofwhich
overlooktwoothercrucialaspectsofvideos: metamorphicamplitudeandtemporalcoherence. These
limitationshinderthedevelopmentofT2Vmodelsingeneratingvideoswithrichphysicalcontent.
Duetothegreatermetamorphicamplitudeandtemporalcoherenceoftime-lapsevideos,theycontain
morephysicalpriorscomparedtogeneralvideos[79]. Therefore, toaddresstheaforementioned
issues,weintroduceabenchmarkcalledChronoMagic-BenchforMetamorphicEvaluationofTime-
LapseText-to-VideoGeneration,whichprovidesacomprehensiveevaluationsystemforT2V.We
specificallydesignfourmajorcategoriesfortime-lapsevideos,includingbiological,human-created,
meteorological,andphysicalvideos,andextendtheseto75subcategories.Basedonthis,weconstruct
ChronoMagic-Bench,comprising1,649promptsandtheircorrespondingreferencetime-lapsevideos.
AsshowninTable1,Incontrasttoexistingbenchmarks[59,75,43,26,72,61],ChronoMagic-Bench
emphasizesgeneratingvideoswithhighpersistenceandstrongvariation,i.e.,metamorphicvideos
withhighphysicalpriorcontent. Additionally,wedevelopMTScoreforevaluatingmetamorphic
amplitudeandCHScorefortemporalcoherencetoaddressthedeficienciesinevaluationmetrics
andperspectives. WithChronoMagic-Bench,weconductcomprehensivequalitativeandquantitative
evaluationsofalmostallopen-sourceT2Vmodels,enablinganalysisoftheirstrengthsandweaknesses.
TheresultshighlighttheweaknessesofexistingT2Vmodels,including1)failurebyalmostallmodels
togeneratetime-lapsevideoswithlargevariations;2)pooradherencetoprompts(thusnecessitating
multipleinferencestoachievesatisfactoryresults);and3)flickeringeventhoughthevisualqualityof
singleframesmaybehigh(indicatingpoortemporalcoherence).
Furthermore,wehavemeticulouslycuratedthedatasetChronoMagic-Protoprovidethecommunity
withthefirstlarge-scaleT2Vdatasetspecificallydesignedfortime-lapsevideogenerationwithhigher
physicalpriorcontent. ChronoMagic-ProstandsoutfrompreviousT2Vdatasets[75,2,70,15,68]
asitcomprisestime-lapsevideos(e.g.,icemeltingandflowersblooming)characterizedbystrong
physicalcharacteristics,highpersistence,andvariability.Consideringthedomaindifferencesbetween
time-lapsevideosandgeneralvideos,weproposeanautomatictime-lapsevideocollectionframework
toensuretheintegrityofvideocontentandimproveannotationquality.
Insummary,thecontributionsofthisworkareasfollows:
i)ANewT2VBenchmark. WeintroduceChronoMagic-BenchtoevaluatetheexistingT2Vmodels
fromvisualquality,textualrelevance,metamorphicamplitude,andtemporalcoherence.
ii)NewAutomaticMetrics. WedevelopMTScoreandCHScore,whichalignbetterwithhuman
judgmentthanexistingmetrics,forassessingmetamorphicattributesandtemporalcoherence.
iii)NewInsightsforT2VModelSelection. OurevaluationsusingChronoMagic-Benchprovide
crucialinsightsintothestrengthsandweaknessesofvariousopen/close-sourceT2Vmodels.
iv)ALarge-ScaleTime-lapseVideo-TextDataset.WecreateChronoMagic-Pro,adatasetwith460k
high-quality720ptime-lapsevideosanddetailedcaptions,promotingadvancesinT2Vresearch.
2Biological Covers all content related to living Human Creation Includes all objects created or
organisms in nature influenced by human activities.
Meteorological Includes all content related to Physical Includes all content related to non-
meteorological phenomena. biological physical phenomena.
Figure1: ExampleoffourmajorcategoriesinChronoMagic-Bench. Thesecategoriescomprehen-
sivelyencompassthephysicalworld,allowingourbenchmarkanddatasettoempowertheresearch
community. Duetolimitedspace,onlythereferencevideoisshownherewithoutprompts.
2 RelatedWork
AutomaticMetricsforText-to-VideoGeneration. Existingbenchmarks[26,31,73,34,54]typi-
callyutilizeFrechetInceptionDistance(FID)[24],FrechetVideoDistance(FVD)[64],CLIPScore
[23],ortheirimprovedversionstoassessthevisualqualityandtextrelevanceofgeneratedvideos.
Forexample,FETV[43]enhancesFVDandCLIPScorewithintheUMT[36]featurespace,resulting
inUMT-FVDandUMTScore. Additionally,theCLIPScorefeatureextractorcanbereplacedwith
BLIP[82]toevaluatetherelevancebetweentextandgeneratedcontent. Tothebestofourknowledge,
existingT2Vbenchmarks[42,43,78,18,48]mainlyassessthesetwoaspects,withpromptsbased
ongeneralvideos. Thismeansthattemporalcoherenceandmetamorphicamplitudeinvideoshave
been overlooked, leading to the absence of automated metrics that indirectly reflect the physical
contentencodedbyvideomodels. Althoughsomework[43,26,42]assesscoherence,theyarebased
onfeaturespaceorhumanevaluation,whichisexpensiveandnotsufficientlyintuitive. Therefore,
we propose the Metamorphic Score (MTScore) and Coherence Score (CHScore) to measure the
metamorphicdegreeandtemporalcoherenceofvideos,fillingthisgapinthefield.
DatasetsforText-to-VideoGeneration. Large-scalehigh-qualitytext-contentpairdata[9,60,
49,25]areessentialfortraininggenerationmodels[83,53,52,51,39,4,85,13,47,37,5,6,20,
46, 81, 58]. To enable models to learn better representation spaces that simulate the real world,
thelargerthedatasetandthericherthephysicalknowledgecontainedinthevideos,thebetterthe
trainingeffect. Researchersoftenconstructtheselarge-scaledatasetsthroughwebscraping. For
example,existingvideogenerationmodelstypicallyuseWebVid-10M[2],whichcontains10million
videosandcaptions. Recentlyreleaseddatasets,suchasPanda-70M[15],HD-VG-130M[68],and
InternVid[70],contain70million,130million,and7.1milliontext-videopairs,respectively. Despite
their large sizes, these datasets consist of general videos with small metamorphic amplitude and
shortpersistenceofchange,resultinginlimitedphysicalknowledge. Consequently,modelstrained
onthesedatasetsstruggletogeneratemetamorphicvideos. Toaddressthisissue,weproposethe
firstlarge-scaledatasetoftime-lapsevideos,comprising460k720Presolutionvideoclipsandtheir
correspondingcaptions,whichfeaturesstrongpersistenceofchanges,andhighphysicalcontent.
3 ChronoMagic-Bench
3.1 BenchmarkConstruction
PromptConstruction. Tocomprehensivelytestthetime-lapsevideogenerationcapabilitiesof
existingT2Vmodels,thedesignedtextpromptsneedtocoverasmanymetamorphictypesaspossible,
andthecorrespondingreferencevideosmustbeofrelativelyhighquality. Manualconstructionis
impractical;therefore,tobuildaT2Vbenchmarkrichinvisualconcepts,wefirstmanuallycreateda
searchtermdatabasesuitablefordiverseandbroadlyapplicabletime-lapsevideos. Wethencounted
thenumberofvideosobtainableforeachsearchtermandfilteredthembasedonfrequency,resulting
inasearchdatabasecontaining75categoriesoftime-lapsevideos. Additionally,sincetherearefour
3bbbbiiiioooollllooooggggiiiiccccaaaallll cccchhhh rrrreeeeuuuu aaaammmm ttttiiiiaaaa oooonnnn nnnn m om om om ollllooooeeee ggggtttteeee iiiiccccoooo aaaarrrr llll---- pppphhhhyyyyssssiiiiccccaaaallll
Figure2: CategoriesofTime-lapseVideos: First,weclassifythevideosintofourmajorcategories
(biological,human-created,meteorological,physical),whicharefurthersubdividedinto75subcate-
gories(e.g.,animal,parking,beach,melting).
WWoorrdd CCoouunntt RRaannggee
22%%
2244%% 2277%%
4477%%
<<3300 3300--4400 4400--5500 >>6600
((aa)) ((bb))
Figure3: ThewordcloudandwordcountrangeofthepromptsinChronoMagic-Bench. It
showsthatpromptsmainlydescribevideoswithlargemetamorphicamplitudeandlongpersistence.
majornaturephenomena: biologicalcoversallcontentrelatedtolivingorganisms, suchasplant
growth,animalactivities,microbialmovement,etc. Human-createdincludesallobjectscreatedor
influencedbyhumanactivities,suchastheconstructionprocessofbuildings,urbantrafficflow,etc.
Meteorologicalincludesallcontentrelatedtometeorologicalphenomena,suchascloudmovement,
stormformation,etc. Physicalincludesallcontentrelatedtonon-biologicalphysicalphenomena,
suchaswaterflow,volcaniceruptions,etc. Wedividethe75subcategoriesintofourmajorcategories
(biological,human-created,meteorological,andphysical),asshowninFigure1. Then,weusethe
searchtermstocrawl20high-qualityvideosforeachcategoryfromvideoplatforms. Finally,weused
GPT-4o[1]toaccuratelycaptionthese1,649videosandtreatedthesecaptionsasthetextpromptsfor
thebenchmark. Formoredetailsaboutbenchmarkconstruction,pleaserefertoAppendixE.
Benchmark Statistics. We collect a total of 1,649 prompts with corresponding videos and
categories,thespecificdatadistributionisshowninFigure2,indicatingthat75categorieshavea
comparablenumberoftestcasestoreflectthetime-lapsevideogenerationcapabilitiesofdifferent
modelsaccurately. EachdatasampleinChronoMagic-Benchconsistsoffourelements: promptp,
referencevideov,sub-categoryc ,andmajorcategoryc . SinceexistingT2Vmodelstypicallyuse
1 2
CLIPasthetextencoder,whichsupportsamaximuminputof77tokens,wehavelimitedthelengthof
ptowithin77tokensforgeneralapplicability,asshowninFigure3(a). Althoughthelengthislimited,
thediversityremainsrich.Bycomparingthemainwordsinthewordcloud,asshowninFigure3(b),it
isobservedthattermsrelatedtotime-lapsevideossuchas"transitioning,""progressing,""increasing,"
and"gradually"appearmostfrequently. ThesetermssignificantlyhighlightChronoMagic-Bench’s
focusonlargemetamorphicamplitude, strongpersistenceofchanges, andhighphysicalcontent.
In addition, words from four major categories are distributed, such as biological (seed, butterfly,
etc.),human-created(Minecraft,traffic,etc),meteorological(sunset,tide,etc),andphysical(burning,
explosion,etc). Fordetailedexplanationsofthe75subcategories,pleaserefertotheAppendixE.
3.2 NewAutomaticMetrics
Aspreviouslymentioned,existingevaluationmetricsmainlyassesstwoaspects: visualqualityand
textualrelevance, andthepromptsonlydescribegeneralvideos. Thisindicatesalackofmetrics
for evaluating the capability to generate time-lapse videos, which not only need to measure the
aforementionedtwoaspectsbutalsoneedtoassessmetamorphicamplitudeandtemporalcoherence.
4MetamorphicScore. Tothebestofourknowledge, thereisnoexistingautomatedevaluation
metricforassessingmetamorphicamplitude. AsimplewayistousequestionnairesorGPT-4o[1],
which,althoughhighlyeffective,isexpensive. Anotherwayistousetheopen-sourcemodel[71],
which,althoughlesseffective,ismuchcheaper. Toaddressthis,weproposebothcoarse-grainedand
fine-grainedscorestomeasurethemetamorphicamplitude,aimingtobalancecostandperformance.
Forthecoarse-grainedscore(i.e. MTScore),weinitiallydesignedN retrievalsentences(pleaserefer
toAppendixB.1formoredetails). Wetheninputthesesentencesintoavideoretrievalmodel[71],
resultinginthecomputationofprobabilitiesfornmetamorphicandmgeneralvideos. LetPmetaand
i
Pgenrepresenttheprobabilitiesforthei-thmetamorphicandgeneralretrievalsentences,respectively.
i
Wethenintegratetheseprobabilitiestoderiveacoarse-grainedmetamorphicscoreS :
c
(cid:80)n Pmeta
S = i=1 i (1)
c (cid:80)n Pmeta+(cid:80)m Pgen
i=1 i i=1 i
Due to the strong instruction-following capability and world-understanding ability of GPT-4o, it
canpartiallyreplacehumans. Forthefine-grainedscore(GPT4o-MTScore),weuseGPT-4oasthe
evaluator. Specifically,weseta5-pointevaluationstandard,thenuniformlysampleT framesand
inputthemintoGPT-4o[1]toobtainthescore. MoredetailsareprovidedinAppendixB.1.
TemporalCoherenceScore. Temporalcoherenceiscrucialfortime-lapsevideosbecausethey
spanalargetimerange. Currentbenchmarksassesscoherenceeitherthroughquestionnaires[43]
or by employing methods based on feature space calculations [26, 42]. The former approach is
time-consuming,whereasthelatterlacksintuitivenessanddoesnotsupportvisualization. Therefore,
wedevelopedtheCoherenceScore(CHScore)basedonavideotrackingmodel[27]. Moredetails
areprovidedintheAppendixB.2. Specifically,wefirstprocessinputvideousingthepre-trained
modelwithgridsizeGandthresholdT togetp . Then,wecountthenumberofmissingtracking
vis
pointsm[i]ineachframe,andthechangeinmissedpointsbetweenconsecutiveframes∆m[i]:
N
1 (cid:88)
m[i]← (1−p [i,j]) (2)
N vis
j=1
∆m[i]←m[i+1]−m[i] (3)
whereN =G×G,irepresentsthepositionoftheframe,andj identifiesdifferenttrackingpoints.
Basedonthese,wethencalculatetheR ,whichrepresentstheaverageproportionofmissed
missed
pointsperframeinthevideo. AndtheV ,whichmeasuresthevariationinthenumberofmissed
missed
pointsbetweenconsecutiveframes,indicatingframe-to-framecoherence:
 
F N
1 (cid:88) 1 (cid:88)
R missed = F  N (1−p vis[i,j]) (4)
i=1 j=1
(cid:118)
(cid:117) F−1
V =(cid:117) (cid:116) 1 (cid:88) (∆m[i]−∆¯m)2 (5)
missed F −1
i=1
where∆m[i]=m[i+1]−m[i],∆¯misthemeanof∆m[i],F isthetotalnumberofframes,N is
thenumberofpointsperframe,andp [i,j]indicatesthevisibilityofpointj inframei. Inaddition,
vis
we need to calculate the R , which indicates the ratio of frames that need to be cut to the total
cut
numberofframes,reflectingtheextentofvideoeditingrequired. AndtheC ,whichindicates
missed
thenumberofconsecutivechangesinmissedpointsexceedingthethreshold,indicatingfrequent
large-scaleinstabilityinpointtracking:
|{i:∆m[i]>T}|
R = (6)
cut F
F−1
(cid:88)
C = ∆m[i] (7)
missed
i=1
∆m[i]>T
whereT isthethresholdforsignificantmissedpointvariation,and|{i : ∆m[i] > T}|represents
thenumberofframeswithsignificantmissedpointvariation. ThenwecalculatetheM ,which
missed
5Table2: ComparisonofthestatisticsofourChronoMagic-ProwithexistingT2Vdatasets.
Dataset #Categories Videoclips Resolution Type Averagelength Videoduration(h)
MSR-VTT[75] General 10K 240p Video-Text 15.0s 40
WebVid-10M[2] General 10M 360p Video-Text 18.7s 52K
InternVid[70] General 234M 720p Video-Text 11.9s 760K
Panda-70M[15] General 70M 720p Video-Text 8.5s 166K
HD-VG-130M[68] General 130M 720p Video-Text 4.9s 178K
Time-Lapse-D[74] Time-lapse 2K 360p Video - -
SkyTime-Lapse[77] Time-lapse 17K 1080p Video - -
ChronoMagic[79] Time-lapse 2K 720p Video-Text 11.4s 7
ChronoMagic-Pro Time-lapse 460K 720p Video-Text 234.1s 30K
ChronoMagic-ProH Time-lapse 150K 720p Video-Text 190.2s 8K
measuresthemaximumcontinuouschangeinmissedpoints,reflectingthemostseverecontinuity
breaksinthevideo,andfinallygettheCoherenceScore(CHScore):
M =max(∆m) (8)
missed
1
S = (9)
CHS R +V +R +C +M
missed missed cut missed missed
3.3 ApplicationScope
ChronoMagic-Benchproposesautomaticscoresformeasuringmetamorphicamplitudeandtemporal
coherence. Whencombinedwithexistingmetricsforvisualqualityandtextualrelevance,suchas
FVD[64],CLIPScore[23],UMT-FVD[43],andUMTScore[43],acomprehensiveevaluationof
T2Vmodelsacrossfourdimensionscanbeachieved. Additionally,wecanusehumanevaluationto
moreaccuratelyassessthesefourdimensions.
4 ChronoMagic-Pro
Multi-AspectDataCuration. Aspreviouslymentioned,existinglarge-scaletext-videodatasets
primarilyconsistofgeneralvideoswithlimitedphysicalinformationcontent,restrictingopen-source
models[5,59,22]togeneratingonlygeneralvideosratherthantime-lapsevideos. Toaddressthis,
weconstructthefirstlarge-scaletime-lapsevideodatasetbycollectingtime-lapsevideosbasedon
the search terms outlined in Section 3.1, ultimately obtaining 66,226 original videos. Following
thePanda70mmethod[15],wesplitthesevideostoproduce460Ksemanticallyconsistentsingle-
scene video clips (ChronoMagic-Pro). Finally, we utilize the video annotation strategy similar
toMagicTime[79],replacingGPT-4V[1]withtheopen-sourceShareGPT4Video[12]toreduce
computationaloverheadwhileensuringhigh-qualityvideocaptions. Additionally,weemployavideo
retrievalmodel[71]tofilteroutlow-qualityvideos,resultingin150Kvideoclipsofhigherpurity
andquality(ChronoMagic-ProH).VerificationexperimentareshownintheAppendixD.3.
DatasetStatistics. Wecollecttime-lapsevideosfrom75categoriesmanuallysetbythehuman,
withproportionsbeingroughlysimilar. SomesamplescanbefoundinAppendixC.3. Unlikebefore,
ChronoMagic-Proisthefirsthigh-qualitylarge-scaletime-lapseT2Vdataset,whichcontainsmore
physicalknowledgethangeneralvideos,asshowninTable2. AsshowninFigure4,intermsof
duration,morethanhalf(53.3%)ofthevideoshaveadurationof0-15seconds,aquarter(27.1%)are
longerthan60seconds,12.1%arebetween15-30seconds,andtheremainingvideosaredistributed
between30-60seconds. Regardingresolution,97%arehighresolution(720P),2%areultra-high
resolution(1080P),andtheremainingvideoshavelowerresolutionsrangingfrom360Pto480P.As
thenumberofwordsacceptedbythetextencoderincreases,werequirethegeneratedcaptionstobe
asdetailedaspossible,with95%ofcaptionscontainingmorethan100words. Foraestheticscore
[57],73%videosreceivehighscoresrangingfrom4to6. 14%ofthevideoshadaestheticindicators
exceeding6,andonlyasmallportionofthevideosscorebelow3. Thisindicatesthatthequalityof
mostvideosishigh. Fortheworddistributionofthegeneratedcaptions,pleaserefertoAppendixC.2.
SimilartoFigure3,ChronoMagic-Promainlyfocusesonchanges(gradually,progressing,increasing,
etc.),processesspanningalargeamountoftime,suchasflowerbloomingandicemelting.Inaddition,
thecategories,durations,resolutions,andwordcountrangesofChronoMagic-ProHaresimilarto
thoseofChronoMagic-Pro;however,ChronoMagic-ProHexhibitssuperiorqualityandpurity.
6Table3: Quantitativecomparisonwithstate-of-the-artT2Vgenerationmethodsforthetext-to-
videotaskinChronoMagic-Bench. "↓"denoteslowerisbetter. "↑"denoteshigherisbetter.
Method Venue Backbone UMT-FVD↓ UMTScore↑ MTScore↑ CHScore↑ GPT4o-MTScore↑
ModelScopeT2V[66] Arxiv’23 U-Net 194.77 2.909 0.401 11.03 2.86
ZeroScope[62] CVPR’23 U-Net 227.02 2.350 0.400 25.13 2.09
T2V-zero[28] ICCV’23 U-Net 209.66 2.661 0.400 1.68 2.55
LaVie[69] Arxiv’23 U-Net 166.97 2.763 0.346 8.60 2.46
AnimateDiff[22] ICLR’24 U-Net 197.89 2.944 0.467 11.36 2.62
VideoCrafter2[10] Arxiv’24 U-Net 178.45 2.753 0.433 8.27 2.68
MCM[80] Arxiv’24 U-Net 202.08 2.33 0.417 14.08 3.04
MagicTime[79] Arxiv’24 U-Net 257.56 1.916 0.478 10.66 3.13
Latte[45] Arxiv’24 DiT 192.12 2.111 0.363 13.81 2.20
OpenSora1.1[86] Github’24 DiT 195.43 2.678 0.444 10.03 2.52
OpenSora1.2[86] Github’24 DiT 166.92 2.781 0.375 4.69 2.56
OpenSoraPlanv1.1[40] Github’24 DiT 188.53 2.421 0.327 10.35 2.19
Video Durations Video Resolution Word Count Range Distribution of Aesthetic Scores
40
27% 23% 18% 30
0.4% 2.6% 1% 20
53%
3% 5% 21% 10
12% 37% 0
97% Score Interval
0-15 15-30 30-45 45-60 60+ 480P 720P 1080P 0-90 90-180 180-270 270-360 >360 <3 3-4 4-5 5-6 >6
Video Durations Video Resolution Word Count Range Distribution of Aesthetic Scores
50
16%
26% 24% 40
30
0.1% 14.9% 2%
2% 54% 20% 20
5% 10
12% 85% 38% 0
Score Interval
0-15 15-30 30-45 45-60 60+ 480P 720P 1080P 0-90 90-180 180-270 270-360 >360 <3 3-4 4-5 5-6 >6
Figure4: Videoclipsstatisticsin(Top)ChronoMagic-Proand(Bottom)ChronoMagic-ProH.The
datasetincludesadiverserangeofcategories,durationsandcaptionlengths,withmostofthevideos
atthe720Presolution. ChronoMagic-ProHhashigherqualityandpurity(e.g. AestheticScore)
5 Experiments
5.1 EvaluationModels
Weselecttenopen-sourceT2Vmodelsforevaluation,includingbothrelativelyadvancedU-Netbased
modelsandemergingDiT-basedmodels. Allinferenceparametersfollowtheofficialimplementation.
MoredetailsabouttheexperimentcanbefoundinAppendixD.
U-NetBasedModels. IncludingModelScopeT2V[66],ZeroScope[62],T2V-zero[28],LaVie
[69],AnimateDiff[22],MCM[80],VideoCrafter2[10],andMagicTime[79].
DiT-BasedModels. IncludingLatte[45],OpenSoraPlanv1.1[40]andOpenSora1.1&1.2[86].
5.2 EvaluationSetups
Evaluation Criteria. We assess video quality primarily from the following four aspects: (a)
VisualQuality,measurestheclarity,colorsaturation,contrast,andoverallaestheticeffect,using
UMT-FVD[43],anenhancedversionofFVD[64]. (b)TextRelevancemeasuresthecorrelation
betweenthepromptandthevideousingUMTScore[43],anenhancedversionofCLIPScore[23]. (c)
MetamorphicAmplitudemeasuresthediversityanddynamicchangesinthevideocontent,using
theproposedMetamorphicScore. (d)TemporalCoherencemeasuresthesmoothnessandlogical
sequenceofthevideocontentovertime,usingtheproposedCoherenceScore. Additionally,weuse
humanevaluationtocross-verifythereliabilityofthefourmetrics.
7
egatnecreP
egatnecreP“Time-lapse of radish seedlings germinating and growing: starting with a single seedling emerging from the soil, additional seedlings sprouting, and all progressively growing ... “
V2TepocSledoM
M
CM
epocSoreZ em
iTcigaM
o Vr 2e Tz- ettaL
eiVaL
1.1v
nalParoSnepO
ffiDetam
inA
1.1
aroSnepO
22rreettffaarrCCooeeddiiVV 2.1
aroSnepO
Figure5: QualitativecomparisonwithdifferentT2Vgenerationmethodsforthetext-to-video
taskinChronoMaigc-Bench. Mostmodelscannotfollowinstructionstogeneratetime-lapsevideos.
ImplementationDetails. Foreachbaseline,wegeneratecorrespondingtripleresultsbasedonthe
1,649promptscontainedintheChronoMagic-Bench,resultingin4,947videosforeachmodel. We
thenusethefourautomatedmetricsmentionedabovetoassessallthegeneratedvideos.
5.3 ComprehensiveAnalysis
QuantitativeEvaluation. Wefirstpresentandanalyzetheresultsfromaqualitativeperspective.All
inputtextsarefromourChronoMagic-Bench. Unlikeexistingbenchmarksthatonlyassessgeneral
videos, our evaluation task focuses on generating metamorphic videos, such as the construction
ofhousesinMinecraft,thebloomingofflowers,thebakingofbreadrolls,andthemeltingofice
cubes. AsshowninFigure5,almostallU-Net-basedandDiT-basedmodelsarelimitedtogenerating
generalvideosandfailtofollowpromptstoproducevideoswithsignificantmotionandtemporal
spans,exceptforMagicTime[79](trainingdatacontainstime-lapsevideos),whichunderscoresthe
importanceofChronoMagic-Prodataset. SinceT2V-Zero[28]isazero-shotvideogenerationmodel,
itscoherenceissignificantlylacking,althoughitsvisualqualityisacceptable. Additionally,videos
generated by AnimateDiff [22] have the best visual quality and text relevance, with high clarity
andaccurateadherencetotheprompt’sinstructions. AmongtheemergingDiT-basedvideomodels,
OpenSora1.2[86]standsoutasarepresentativemodelthatmatchestheperformanceofU-Netbased
methods. ItisfollowedbyOpenSoraPlanv1.1[40],OpenSora1.1[86],andLatte[45]. However,due
totheinherentlimitationsofOpenSora1.2’sVideoVAEand2+1DDiTarchitecture,thegenerated
videosarepronetoflickering,particularlyduringsignificantchanges,resultinginthelowesttemporal
coherence(CHScore). AllvideosgeneratedonChronoMagic-Benchareavailableonourhomepage.
QualitativeEvaluation. Next,wepresentandanalyzetheresultsofdifferentT2Vmodelsfrom
aqualitativeperspective. Duetothelackofmetrics,weproposethefirstMTScoreandCHScore
to evaluate video motion extent and coherence, with results shown in Table 3. Consistent with
Figure5,MagicTime[79],astheonlymodelcapableofgeneratingmetamorphicvideos,hasthe
highestMTScoreandGPT4o-MTScoreamongallmodels. Theothermodels,trainedonlyongeneral
videos,producevideoswithlimitedmotionrangeduetotheminimalphysicalknowledgeencodedin
themodels. ItcanalsobeseenthattheresultsoftheMTScorebasedonfeaturespacewithlower
overheadandtheGPT4o-MTScorebasedonquestionansweringwithhigheroverheadareroughly
similar,provingtheeffectivenessoftheproposedindicators.Additionally,ZeroScope[62]haslimited
metamorphicamplitudebutthebestcoherence,whilethezero-shotalgorithmT2V-Zero[28]has
thelowestCHScore. U-NetbasedandDiT-basedalgorithmshavesimilarCHScore,buttheformer
8222888000 ðð == --00..66 // ££ == --00..88 ðð == --00..44 // ££ == --00..55 ðð == 00..44 // ££ == 00..55 111222 ðð == 00..88 // ££ == 00..99 ðð == 00..88 // ££ == 00..99
↑↑↑
↓↓↓222666000 ↑↑↑ ↑↑↑111000 eee
TTT DDD MMMVVV UUUFFF--- 1222 1222 1222 24 24 24 80 80 8000 00 00 00 00 00 eeerrrooo cccSSSTTTMMM eee rrrooo cccSSSHHH CCC 888 666 rrrooo cccSSSTTTMMM ---ooo
444
111666000 333...444 333...555 333...666 333...777 333...888 444 333...222 333...444 333...666 333...888TTTPPP GGG
VVViiisssuuuaaalll QQQuuuaaallliiitttyyy ↑↑↑ CCCooohhheeerrreeennnccceee ↑↑↑
AAAnnniiimmmaaattteeeDDDiiiffffff MMMaaagggiiicccTTTiiimmmeee VVViiidddeeeoooCCCrrraaafffttteeerrr222 OOOpppeeennnSSSooorrraaa 111...111 OOOpppeeennnSSSooorrraaa 111...222 OOOpppeeennnSSSooorrraaaPPPlllaaannn vvv111...111
Figure6:Alignmentbetweenautomaticmetricsandhumanperceptionintermsofvisualquality,
textualrelevance,metamorphicamplitude,andtemporalcoherence. ðand£representKendall↑
andSpearman↑coefficients,respectively. ↑denoteshigherisbetter.
showssuperioraveragemetamorphicamplitude. Forvisualqualityandtextrelevance,theemerging
OpenSoraPlanv1.1[40]andOpenSora1.1&1.2[86]havevisualqualitycomparabletoU-Netbased
methods,butslightlyinferiortextrelevanceandtemporalcoherence. Additionally,OpenSora1.2
[86]hasthelowestUMT-FVD[43]withhighercolorsaturation,butQuantitativeEvaluationshows
thatLaVie[69]hasthehighestclarityandmostaccuratecolorreproduction. OnlyMagicTime[79]
followstheprompttogenerateatime-lapsevideo,buttheUMTScore[43]isthelowest. Weinfer
thattheUMT-FVD[43]andUMTScore[43]areinconsistentwithhumanperception.
HumanPreference. Finally,wecross-validatetheeffectivenessofthedifferentmetricsthrough
HumanStudy. Werandomlyselectthegeneratedvideoscorrespondingto16promptsandinvited
171participantstovote,obtainingmanualevaluationresults. Toenhanceusersatisfaction,weselect
onlyfiverepresentativebaselineresultsfromwhichuserscanchoose. Table6showsthecorrelation
between automatic metrics and human perception. It is evident that the proposed three metrics,
MTScore,CHScore,andGPT4o-MTScore,areconsistentwithhumanperceptionandcanaccurately
reflectthemetamorphicamplitudeandtemporalcoherenceofT2Vmodels. Notably,althoughmost
modelsexhibitgoodcoherence,theyhavelowmetamorphicamplitude. Inotherwords,theycannot
generatevideoswithsignificantphysicalpriors,suchasseedgermination,egghatching,orsunrise.
ThisisachallengethatT2Vmodelsneedtoovercomeinthefuture. Additionally, asmentioned
earlier,UMTScore[43]cannotaccuratelymeasuretextrelevance,especiallyinevaluatingtime-lapse
videos,whereitsKendallandSpearmancoefficientsarethelowest. Weinferthatitsfeaturespaceis
notsuitablefortime-lapsevideo. AppendixD.6providesmoredetailsabouthumanevaluation.
5.4 ExtendedAnalysisofClosed-SourceModels
Inthissection, weexploretheperformanceandlimitationsofclosed-sourcemodels, specifically
U-Netbased: Gen-2[56],Pika-2.0[33],DiT-based: DreamMachine[44],andKeLing[32]. Given
theimpracticalityofmanuallytestingall1,649promptsinChronoMagic-Bench,weselectedtwo
hardpromptsfromeachofthe75categories,resultinginChronoMagic-Bench-150.
First,wepresentandanalyzetheresultsfromaqualitativeperspective,asshowninFigure7. For
metamorphicamplitude,mostmethodscanonlygeneratesimpletime-lapsevideos,suchastraffic
flow;onlyDreamMachine[44]cangenerateamoderatelychallengingfullprocessofnight-to-day
transformation;nomethodcangeneratecomplexchangeslikeplantgrowthorbuildingconstruction.
Intermsoftemporalcoherence,theperformanceofvariousclosed-sourcemodelsiscomparable,
withminorvisibledifferences. Regardingvisualquality,theDiT-basedmethodsDreamMachine[44]
andKeLing[32]outperformthosebasedonU-Net,producingmorerealisticplants,moreaccurately
saturatedskycolors,andclearertrafficflow. Intermsoftextrelevance,allmethodsadheretothe
prompt’s instructions to generate content relevant to the theme, except for Pika-2.0 [33], which
mistakenlyinterpretsday-to-nightasnight-to-day.
Next,weanalyzetheresultsfromaquantitativeperspective,asshowninTable4. Theresultsare
consistentwithFigure7,withDreamMachine[44]performingbetterinmetamorphicamplitude
(MTScore, GPT4o-MTScore) and Pika-2.0 [33] showing the worst text relevance (UMTScore).
DiT-basedmethodsoutperformU-Netbasedonesinvisualquality. Tofacilitatecomparisonundera
unifiedstandard,wealsotestopen-sourcemodelsonChronoMagic-Bench-150. Itisevidentthatfor
mostmodels,theMTScoreandGPT4o-MTScorearelow,andtheyareunabletogeneratevideos
involvingcomplexstatechanges. Additionally,duetotheinherentlimitationsofUMT-FVD[43]and
UMTScore[43],theyfailtoaccuratelyreflectthedifferencesbetweenopen-sourceandclosed-source
9
↑↑↑
eeerrrooocccSSSTTTMMMUUU
333
222
111
...555
333
...555 222
...555 333
TTTeeexxxttt RRR
333 ...555eeellleeevvvaaannn
ccceee ↑↑↑
444
000
000
000 ...555
...444555
000 ...444 ...333 555
000 ...333
MMM
333 ...222eeetttaaammm 333 ...444ooo
rrrppp hhh
333 ...666iiiccc
AAA mmm
333 ...888ppp
llliiitttuuu ddd
444
↑↑↑
333
222
111
...555
333
...555 222
...555 333 ...222MMM
eeetttaaammm
333 ...444ooo
rrrppp hhh
333 ...666iiiccc
AAA mmm
333 ...888ppp
llliiitttuuu ddd
444
↑↑↑““TTiimmee--llaappssee ooff aann uunniiddeennttiiffiieedd ppllaanntt ggrroowwiinngg oovveerr ssiixx mmoonntthhss:: bbrrooaadd lleeaavveess eemmeerrggee ffrroomm aa cceennttrraall sstteemm,, ffuurrtthheerr eelloonnggaattee,, aanndd iinnccrreeaassee iinn ddeennssiittyy.. BByy DDaayy 116622,, tthhee cceenntteerr hhaass ffuulllleerr,, ddeennsseerr ffoolliiaaggee.. TThhee ppllaanntt ......””
nneeGG22--
eenniihhccaaMM
mm
aaeerrDD
aa00 kk.. ii22 PP-- ggnniiLLeeKK
“Time-lapse capturing a full day-to-night cycle over a city. It starts with a night sky featuring a crescent moon, transitions through a colorful dawn to a bright daylight, and ends with a sunset casting a ...”
neG2-
enihcaM
m
aerD
a0 k. i2 P- gniLeK
“Time-lapse of a nighttime city intersection showing varying traffic flow and pedestrian activity. The video captures light trails from moving vehicles, changes in traffic light signals, decorative building ...”
neG2-
enihcaM
m
aerD
a0 k. i2 P- gniLeK
Figure7: QualitativecomparisonwithClose-Sourcegenerationmethodsforthetext-to-video
taskinChronoMagic-Bench-150. Mostmethodscanonlygeneratesimpletime-lapsevideossuchas
trafficflowsandstarryskies,andareincapableofgeneratingcomplexchangessuchasplantgrowth
orbuildingconstruction.
Table 4: Quantitative Comparison with Closed-Source Generation Methods for the Text-to-
VideoTaskinChronoMagic-Bench-150. Tofacilitatecomparisonunderaunifiedstandard,wealso
testOpen-Sourcemodels. ThecurrentUMT-FVD[43]andUMTScore[43]metricslackaccuracy,
preventing them from effectively highlighting the significant disparity between open-source and
closed-sourcemodels. ↓denoteslowerisbetter. ↑denoteshigherisbetter.
Method Venue Backbone Status UMT-FVD↓ UMTScore↑ MTScore↑ GPT4o-MTScore↑
Gen-2[56] Runway U-Net Close-Source 218.99 2.400 0.373 2.62
Pika-2.0[33] PikaLab U-Net Close-Source 223.05 2.317 0.347 2.48
DreamMachine[44] LUMA DiT Close-Source 214.91 2.387 0.474 3.11
KeLing[32] Kwai DiT Close-Source 202.32 2.517 0.369 2.74
ModelScopeT2V[66] Arxiv’23 U-Net Open-Source 230.74 2.783 0.409 3.01
ZeroScope[62] CVPR’23 U-Net Open-Source 260.61 2.232 0.403 2.29
T2V-zero[28] ICCV’23 U-Net Open-Source 250.22 2.559 0.399 2.62
LaVie[69] Arxiv’23 U-Net Open-Source 210.39 2.714 0.350 2.50
AnimateDiff[22] ICLR’24 U-Net Open-Source 239.31 2.837 0.470 2.62
VideoCrafter2[10] Arxiv’24 U-Net Open-Source 214.06 2.763 0.437 2.87
MCM[80] Arxiv’24 U-Net Open-Source 244.49 2.282 0.422 3.06
MagicTime[79] Arxiv’24 U-Net Open-Source 294.72 1.763 0.479 3.05
Latte[45] Arxiv’24 DiT Open-Source 232.29 2.122 0.366 2.42
OpenSora1.1[86] Github’24 DiT Open-Source 241.09 2.676 0.448 2.57
OpenSora1.2[86] Github’24 DiT Open-Source 210.93 2.681 0.383 2.50
OpenSoraPlanv1.1[40] Github’24 DiT Open-Source 228.70 2.459 0.331 2.21
models. However,thequalitativeanalysisacrossallmodelsdemonstratesthatclosed-sourcemodels
consistentlysurpassopen-sourcemodelsinvisualqualityandtextualrelevance. Furthermore,itis
worthnotingthattheresultswithinthesamedomain(open/closed)alignwithhumanevaluations.
5.5 GuidelineforT2VModelsSelection
WiththeincreasingnumberofT2Vmodels,thecommunityfaceschallengesinselectingthemost
appropriatemodelduetothetendencyofeachmodeltoshowcaseitsbestresults. Toaddressthis
issue,weprovideaguidelineaccordingtotheTable4formodelselectionbasedonChronoMagic-
Bench: (1)ExceptforMagicTime[79]andDreamMachine[44],mostT2Vmodelsexhibitminimal
10metamorphicamplitudeandcannotgeneratecompleteprocessesrichinphysicalchanges,suchas
seedgermination,sunrise,orbuildingconstruction;(2)Thevisualqualityofasingleframemaybe
high,butwhenviewedinsequence,flickeringoftenoccurs,indicatingpoortemporalcoherence. This
issueisparticularlyevidentinT2V-zero[28]andOpenSora1.2[86],whereasclosed-sourcemodels
donotexhibitthisproblem;(3)TheemergenceofSora[8]haspromotedtherapiddevelopmentof
DiT-basedmethods.Closed-sourcemodelsbasedonDiThavecomprehensivelysurpassedthosebased
onU-Net. However,open-sourcemodels’visualquality,text-followingcapability,andmetamorphic
amplitude still lag behind U-Net-based methods. We speculate that DiT-based models are more
scalableandrequiremoredata,givingclosed-sourcemodelsasignificantadvantageoveropen-source
models;(4)Itisexpensivetoaccessmassivedataandcomputingresources. First,theycanbuild
datasetsbycrawlingvideoswithoutcopyrightdisputes. Second,adoptingtheU-DiTarchitecturemay
balanceperformanceandcosttoacertainextent;(5)OrdinaryuserswhowanttotryT2Vmodels
canprioritizeDreamMachine[44]andKeLing[32]. Researcherswhowishtoconductin-depth
researchonT2VcanprioritizethestudyofmetamorphicvideogenerationwithOpenSoraPlan[40]
andOpenSora[86],asneitheropen-sourcenorclosed-sourcemodelscanachievethisfunction.
6 Conclusion
Inthispaper,wepresentChronoMagic-Bench,thefirstbenchmarkspecificallydesignedtoassess
thegenerationoftime-lapsevideos. Itaddressestheshortcomingsofcurrentbenchmarks,which
primarilyfocusonstandardvideosandoverlookcriticalelementssuchasmetamorphicamplitude
and coherence. Additionally, we introduce two new automated metrics, MTScore and CHScore,
whichalignwithhumanperception. BasedonChronoMagic-Bench,weconductacomprehensive
evaluationofalmostallopen-sourceleadingtext-to-video(T2V)modelsandprovidecrucialinsights
intothestrengthsandweaknessesofvariousmodels. Moreover,weproposeChronoMagic-Pro,the
firstlarge-scaletime-lapsetext-to-videodataset,tofacilitatefurtherresearchbythecommunity.
LimitationsandFutureWork. WhileChronoMagic-Benchoffersarobustevaluationframework,
therearetwolimitationstothiswork. First,althoughMTScoreandCHScoreareintroducedtoassess
metamorphicattributesandtemporalcoherence,thesemetricsonlyrelativelyreflectthequalityof
differentT2Vmodels. Andtheydonotascertainwhetherthevideosadheretophysicallaws. For
example,theMTScoreofMCM[80]isrelativelyhigh,butthevideochangesstrangely. Second,we
usethebestexistingUMT-FVDandUMTScoretoevaluatevisualqualityandtextrelevance,but
duetotheinapplicabilityoffeaturespace,modelsindifferentdomains(open/closedsource)arenot
comparable. Wewilladdressthisissueinthefuture.
11References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] MaxBain,ArshaNagrani,GülVarol,andAndrewZisserman. Frozenintime: Ajointvideo
andimageencoderforend-to-endretrieval. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages1728–1738,2021.
[3] OmerBar-Tal,HilaChefer,OmerTov,CharlesHerrmann,RoniPaiss,ShiranZada,ArielEphrat,
JunhwaHur,YuanzhenLi,TomerMichaeli,etal. Lumiere: Aspace-timediffusionmodelfor
videogeneration. arXivpreprintarXiv:2401.12945,2024.
[4] OmerBar-Tal, LiorYariv, YaronLipman, andTaliDekel. Multidiffusion: Fusingdiffusion
pathsforcontrolledimagegeneration. ICML,2023.
[5] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Do-
minikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion:
Scalinglatentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023.
[6] AndreasBlattmann, RobinRombach, HuanLing, TimDockhorn, SeungWookKim, Sanja
Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent
diffusionmodels. InCVPR,pages22563–22575,2023.
[7] GaryBradski,AdrianKaehler,etal. Opencv. Dr.Dobb’sjournalofsoftwaretools,3(2),2000.
[8] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,Joe
Taylor,TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh. Video
generationmodelsasworldsimulators. OpenAI,2024.
[9] JoaoCarreira,EricNoland,AndrasBanki-Horvath,ChloeHillier,andAndrewZisserman. A
shortnoteaboutkinetics-600. arXivpreprintarXiv:1808.01340,2018.
[10] HaoxinChen,YongZhang,XiaodongCun,MenghanXia,XintaoWang,ChaoWeng,andYing
Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models.
arXivpreprintarXiv:2401.09047,2024.
[11] JunsongChen,JinchengYu,ChongjianGe,LeweiYao,EnzeXie,YueWu,ZhongdaoWang,
JamesKwok,PingLuo,HuchuanLu,etal. Pixart-alpha: Fasttrainingofdiffusiontransformer
forphotorealistictext-to-imagesynthesis. arXivpreprintarXiv:2310.00426,2023.
[12] LinChen,XilinWei,JinsongLi,XiaoyiDong,PanZhang,YuhangZang,ZehuiChen,Haodong
Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and
generationwithbettercaptions. arXivpreprintarXiv:2406.04325,2024.
[13] MinghaoChen,IroLaina,andAndreaVedaldi. Training-freelayoutcontrolwithcross-attention
guidance. InCVPR,pages5343–5353,2024.
[14] MinghaoChen,IroLaina,andAndreaVedaldi. Training-freelayoutcontrolwithcross-attention
guidance. InCVPR,pages5343–5353,2024.
[15] Tsai-ShienChen,AliaksandrSiarohin,WilliMenapace,EkaterinaDeyneka,Hsiang-weiChao,
ByungEunJeon,YuweiFang,Hsin-YingLee,JianRen,Ming-HsuanYang,etal. Panda-70m:
Captioning70mvideoswithmultiplecross-modalityteachers.arXivpreprintarXiv:2402.19479,
2024.
[16] LaionCoCo. Laioncoco: 600msyntheticcaptionsfromlaion2b-en. LaionCoCo,2022.
[17] TimDettmers,ArtidoroPagnoni,andHoltzman. Qlora: Efficientfinetuningofquantizedllms.
NeurIPS,36,2024.
[18] MingDing,WendiZheng,WenyiHong,andJieTang. Cogview2: Fasterandbettertext-to-
imagegenerationviahierarchicaltransformers. AdvancesinNeuralInformationProcessing
Systems,35:16890–16902,2022.
12[19] PengGao,LeZhuo,ZiyiLin,ChrisLiu,JunsongChen,RuoyiDu,EnzeXie,XuLuo,Longtian
Qiu,YuhangZhang,etal. Lumina-t2x: Transformingtextintoanymodality,resolution,and
durationviaflow-basedlargediffusiontransformers. arXivpreprintarXiv:2405.05945,2024.
[20] SongweiGe, SeungjunNah, GuilinLiu, TylerPoon, AndrewTao, BryanCatanzaro, David
Jacobs,Jia-BinHuang,Ming-YuLiu,andYogeshBalaji. Preserveyourowncorrelation: A
noisepriorforvideodiffusionmodels. InICCV,pages22930–22941,2023.
[21] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala,
Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
15180–15190,2023.
[22] YuweiGuo,CeyuanYang,AnyiRao,YaohuiWang,YuQiao,DahuaLin,andBoDai. Ani-
matediff: Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning.
arXivpreprintarXiv:2307.04725,2023.
[23] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,andYejinChoi. Clipscore: A
reference-freeevaluationmetricforimagecaptioning. arXivpreprintarXiv:2104.08718,2021.
[24] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter.
Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. Advancesin
neuralinformationprocessingsystems,30,2017.
[25] Ming Hu, Lin Wang, Siyuan Yan, Don Ma, Qingli Ren, Peng Xia, Wei Feng, Peibo Duan,
LieJu,andZongyuanGe. Nurvid: Alargeexpert-levelvideodatabasefornursingprocedure
activityunderstanding. AdvancesinNeuralInformationProcessingSystems,36,2024.
[26] ZiqiHuang,YinanHe,JiashuoYu,FanZhang,ChenyangSi,YumingJiang,YuanhanZhang,
TianxingWu,QingyangJin,NattapolChanpaisit,etal. Vbench: Comprehensivebenchmark
suiteforvideogenerativemodels. arXivpreprintarXiv:2311.17982,2023.
[27] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and
ChristianRupprecht. Cotracker: Itisbettertotracktogether. arXiv:2307.07635,2023.
[28] LevonKhachatryan,AndranikMovsisyan,VahramTadevosyan,RobertoHenschel,Zhangyang
Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion
modelsarezero-shotvideogenerators. arXivpreprintarXiv:2303.13439,2023.
[29] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
[30] Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min,
GuangtaoZhai,andNingLiu. Subjective-aligneddatesetandmetricfortext-to-videoquality
assessment. arXivpreprintarXiv:2403.11956,2024.
[31] Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min,
GuangtaoZhai,andNingLiu. Subjective-aligneddatesetandmetricfortext-to-videoquality
assessment. arXivpreprintarXiv:2403.11956,2024.
[32] Kwai. Keling. Kwai,2024.
[33] PikaLab. Pika-2.0labdiscordserver. PikaLab,2024.
[34] TiepLe,VasudevLal,andPhillipHoward. Coco-counterfactuals: Automaticallyconstructed
counterfactual examples for image-text pairs. Advances in Neural Information Processing
Systems,36,2024.
[35] BoLi,KaichenZhang,HaoZhang,DongGuo,RenruiZhang,FengLi,YuanhanZhang,Ziwei
Liu,andChunyuanLi. Llava-next: Strongerllmssuperchargemultimodalcapabilitiesinthe
wild,May2024.
[36] KunchangLi,YaliWang,YizhuoLi,YiWang,YinanHe,LiminWang,andYuQiao. Unmasked
teacher: Towardstraining-efficientvideofoundationmodels. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages19948–19960,2023.
13[37] YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,JianweiYang,JianfengGao,Chunyuan
Li,andYongJaeLee. Gligen: Open-setgroundedtext-to-imagegeneration. InCVPR,2023.
[38] YumengLi,WilliamBeluch,MargretKeuper,DanZhang,andAnnaKhoreva.Vstar:Generative
temporalnursingforlongerdynamicvideosynthesis. arXivpreprintarXiv:2403.13501,2024.
[39] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing
promptunderstandingoftext-to-imagediffusionmodelswithlargelanguagemodels. arXiv
preprintarXiv:2305.13655,2023.
[40] BinLin,ShenghaiYuan,ZhenyuTang,JunwuZhang,XinhuaCheng,LiuhanChen,YangYe,
BinZhu,YunyangGe,XingZhou,ShaolingDong,YeminShi,YonghongTian,andLiYuan.
Tinysora. InGithub,March2023.
[41] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunited
visualrepresentationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[42] YaofangLiu,XiaodongCun,XueboLiu,XintaoWang,YongZhang,HaoxinChen,YangLiu,
TieyongZeng,RaymondChan,andYingShan. Evalcrafter:Benchmarkingandevaluatinglarge
videogenerationmodels. arXivpreprintarXiv:2310.11440,2023.
[43] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and
LuHou.Fetv:Abenchmarkforfine-grainedevaluationofopen-domaintext-to-videogeneration.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[44] Luma. Dream-machine. Luma,2024.
[45] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian
Chen,andYuQiao. Latte: Latentdiffusiontransformerforvideogeneration. arXivpreprint
arXiv:2401.03048,2024.
[46] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang.
Revideo: Remakeavideowithmotionandcontentcontrol. arXivpreprintarXiv:2405.13865,
2024.
[47] ChongMou,XintaoWang,LiangbinXie,YanzeWu,JianZhang,ZhongangQi,YingShan,and
XiaohuQie.T2i-adapter:Learningadapterstodigoutmorecontrollableabilityfortext-to-image
diffusionmodels. arXivpreprintarXiv:2302.08453,2023.
[48] MayuOtani,RikuTogashi,YuSawai,RyosukeIshigami,YutaNakashima,EsaRahtu,Janne
Heikkilä,andShin’ichiSatoh. Towardverifiableandreproduciblehumanevaluationfortext-
to-imagegeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages14277–14286,2023.
[49] GauravParmar,RichardZhang,andJun-YanZhu. Onaliasedresizingandsurprisingsubtleties
inganevaluation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages11410–11420,2022.
[50] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pages4195–4205,2023.
[51] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023.
[52] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchical
text-conditionalimagegenerationwithcliplatents,2022. arXivpreprint:2204.06125,2022.
[53] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,Mark
Chen, andIlyaSutskever. Zero-shottext-to-imagegeneration. InICML,pages8821–8831,
2021.
[54] ArijitRay,FilipRadenovic,AbhimanyuDubey,BryanPlummer,RanjayKrishna,andKate
Saenko. cola: Abenchmarkforcompositionaltext-to-imageretrieval. AdvancesinNeural
InformationProcessingSystems,36,2024.
14[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer.
High-resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,Jun2022.
[56] Runway. Gen-2: Thenextstepforwardforgenerativeai. Runway,2024.
[57] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton
Mullis,AarushKatta,TheoCoombes,JeniaJitsev,andAranKomatsuzaki. Laion-400m: Open
datasetofclip-filtered400millionimage-textpairs. arXivpreprintarXiv:2111.02114,2021.
[58] YujunShi,ChuhuiXue,JiachunPan,WenqingZhang,VincentYFTan,andSongBai. Dragdif-
fusion: Harnessingdiffusionmodelsforinteractivepoint-basedimageediting. arXivpreprint
arXiv:2306.14435,2023.
[59] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,
HarryYang,OronAshual,OranGafni,etal. Make-a-video: Text-to-videogenerationwithout
text-videodata. arXivpreprintarXiv:2209.14792,2022.
[60] LucasSmaira,JoãoCarreira,EricNoland,EllenClancy,AmyWu,andAndrewZisserman. A
shortnoteonthekinetics-700-2020humanactiondataset. arXivpreprintarXiv:2010.10864,
2020.
[61] KhurramSoomro,AmirRoshanZamir,andMubarakShah. Ucf101: Adatasetof101human
actionsclassesfromvideosinthewild. arXivpreprintarXiv:1212.0402,2012.
[62] SpencerSterling. zeroscope. InHuggingface,2023.
[63] KeqiangSun,JuntingPan,YuyingGe,HaoLi,HaodongDuan,XiaoshiWu,RenruiZhang,
Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: A benchmark for generative image
understanding. AdvancesinNeuralInformationProcessingSystems,36,2024.
[64] ThomasUnterthiner,SjoerdvanSteenkiste,KarolKurach,RaphaëlMarinier,MarcinMichalski,
andSylvainGelly. Fvd: Anewmetricforvideogeneration. openreview,2019.
[65] RubenVillegas,MohammadBabaeizadeh,Pieter-JanKindermans,HernanMoraldo,HanZhang,
MohammadTaghiSaffar,SantiagoCastro,JuliusKunze,andDumitruErhan. Phenaki: Variable
lengthvideogenerationfromopendomaintextualdescriptions. InInternationalConferenceon
LearningRepresentations,2022.
[66] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.
Modelscopetext-to-videotechnicalreport. arXivpreprintarXiv:2308.06571,2023.
[67] WeiminWang,JiaweiLiu,ZhijieLin,JiangqiaoYan,ShuoChen,ChetwinLow,TuyenHoang,
JieWu,JunHaoLiew,HanshuYan,etal. Magicvideo-v2: Multi-stagehigh-aestheticvideo
generation. arXivpreprintarXiv:2401.04468,2024.
[68] WenjingWang,HuanYang,ZixiTuo,HuiguoHe,JunchenZhu,JianlongFu,andJiayingLiu.
Videofactory: Swapattentioninspatiotemporaldiffusionsfortext-to-videogeneration. arXiv
preprintarXiv:2305.10874,2023.
[69] YaohuiWang,XinyuanChen,XinMa,ShangchenZhou,ZiqiHuang,YiWang,CeyuanYang,
YinanHe,JiashuoYu,PeiqingYang,etal. Lavie: High-qualityvideogenerationwithcascaded
latentdiffusionmodels. arXivpreprintarXiv:2309.15103,2023.
[70] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen,
XinyuanChen,YaohuiWang,etal. Internvid: Alarge-scalevideo-textdatasetformultimodal
understandingandgeneration. arXivpreprintarXiv:2307.06942,2023.
[71] YiWang, KunchangLi, XinhaoLi, JiashuoYu, YinanHe, GuoChen, BaoqiPei, Rongkun
Zheng,JilanXu,ZunWang,etal.Internvideo2:Scalingvideofoundationmodelsformultimodal
videounderstanding. arXivpreprintarXiv:2403.15377,2024.
[72] JayZhangjieWu,GuianFang,HaoningWu,XintaoWang,YixiaoGe,XiaodongCun,DavidJun-
haoZhang,Jia-WeiLiu,YuchaoGu,RuiZhao,etal. Towardsabettermetricfortext-to-video
generation. arXivpreprintarXiv:2401.07781,2024.
15[73] JayZhangjieWu,GuianFang,HaoningWu,XintaoWang,YixiaoGe,XiaodongCun,DavidJun-
haoZhang,Jia-WeiLiu,YuchaoGu,RuiZhao,etal. Towardsabettermetricfortext-to-video
generation. arXivpreprintarXiv:2401.07781,2024.
[74] WeiXiong,WenhanLuo,LinMa,WeiLiu,andJieboLuo. Learningtogeneratetime-lapse
videosusingmulti-stagedynamicgenerativeadversarialnetworks. InCVPR,pages2364–2373,
2018.
[75] JunXu,TaoMei,TingYao,andYongRui. MSR-VTT:Alargevideodescriptiondatasetfor
bridgingvideoandlanguage. CVPR,pages5288–5296,2016.
[76] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava:
Parameter-freellavaextensionfromimagestovideosforvideodensecaptioning. arXivpreprint
arXiv:2404.16994,2024.
[77] Hongwei Xue, Bei Liu, Huan Yang, Jianlong Fu, Houqiang Li, and Jiebo Luo. Learning
fine-grained motion embedding for landscape animation. In Proceedings of the 29th ACM
InternationalConferenceonMultimedia,pages291–299,2021.
[78] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay
Vasudevan, AlexanderKu, YinfeiYang, BurcuKaragolAyan, etal. Scalingautoregressive
modelsforcontent-richtext-to-imagegeneration. arXivpreprintarXiv:2206.10789, 2(3):5,
2022.
[79] Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng,
Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic
simulators. arXivpreprintarXiv:2404.05014,2024.
[80] YuanhaoZhai,KevinLin,ZhengyuanYang,LinjieLi,JianfengWang,Chung-ChingLin,David
Doermann,JunsongYuan,andLijuanWang. Motionconsistencymodel: Acceleratingvideo
diffusionwithdisentangledmotion-appearancedistillation. arXivpreprintarXiv:2406.06890,
2024.
[81] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu,
DifeiGao, andMikeZhengShou. Show-1: Marryingpixelandlatentdiffusionmodelsfor
text-to-videogeneration. arXivpreprintarXiv:2309.15818,2023.
[82] LeiZhang,FangxunShu,SuchengRen,BingchenZhao,HaoJiang,andCihangXie. Compress
&align: Curatingimage-textdatawithhumanknowledge. arXivpreprintarXiv:2312.06726,
2023.
[83] ShaofengZhang,JinfaHuang,QiangZhou,ZhibinWang,FanWang,JieboLuo,andJunchiYan.
Continuous-multipleimageoutpaintinginone-stepviapositionalqueryandadiffusion-based
approach. arXivpreprintarXiv:2401.15652,2024.
[84] ZhaoyangZhang,ZiyangYuan,XuanJu1,YimingGao1,XintaoWang,ChunYuan,andYing
Shan1. Mira: Amini-steptowardssora-likelongvideogeneration. InGithub,April2024.
[85] ShihaoZhao, DongdongChen, Yen-ChunChen, JianminBao, ShaozheHao, LuYuan, and
Kwan-Yee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models.
NeurIPS,36,2024.
[86] Zangwei Zheng, Xiangyu Peng, and Yang You. Open-sora: Democratizing efficient video
productionforall. InGithub,March2024.
16Appendix
A MoreRelatedWork: Text-to-VideoGenerationModels 17
B MoreDetailsaboutAutomaticMetrics 17
B.1 ConstructionofRetrievalSentencesforMetamorphicScore. . . . . . . . . . . . . 17
B.2 MoreDescriptionofTemporalCoherenceScore . . . . . . . . . . . . . . . . . . . 18
B.3 VisualizationoftheDifferentScoresofMTScoreandCHScore . . . . . . . . . . . 19
C MoreDetailsaboutChronoMagic-Pro 19
C.1 Multi-AspectDataPreprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.2 DistributionoftheGeneratedCaptions . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 SamplesoftheChronoMagic-Pro . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D MoreDetailsaboutExperiment 23
D.1 ComputationResourceDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.2 DetailsofEvaluationT2VModels . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.3 VerificationExperimentonChronoMagic-Pro . . . . . . . . . . . . . . . . . . . . 24
D.4 MoreQualitativeEvaluationonChronoMagic-Bench . . . . . . . . . . . . . . . . 25
D.5 MoreQuantitativeEvaluationonChronoMagic-Bench-150 . . . . . . . . . . . . . 25
D.6 DetailsofHumanEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
E MoreDetailsabout75SubcategoriesinChronoMaigc-Bench 28
F Licensing,HostingandMaintenancePlan 31
A MoreRelatedWork: Text-to-VideoGenerationModels
The emergence of large-scale text-to-image models [83, 53, 52, 51, 39, 4, 85, 13, 47, 37] has
significantlyadvancedthefieldofText-to-Video(T2V)generation[59,5,6,20,66,81]. Existing
T2V architectures can be categorized into two types: U-Net-based and DiT-based. The former
typicallybuildsonStableDiffusion[55],extendingthe2DU-Nettoa3DU-Netbyaddingtemporal
layers,therebyachievinghigh-qualityvideogeneration[67,14,22,3,10,38]. Thelatterfocuses
on recreating open-source structures similar to Sora [8], using the DiT (Diffusion-Transformer)
[50]frameworkforT2Vgeneration[40,86,84,19]. However,thegenerationqualityofDiT-based
architecturesstilllagsbehindthatofU-Net-basedarchitectures. MagicTime[79]notesthatalthough
thesemodelshaveachievedbasicvideogeneration,thevideosaretypicallylimitedtosimpleactions
andscenes,resultingintheproductionofgeneralvideosratherthanthoseenrichedwithphysical
priorslikemetamorphic/time-lapsevideos. Foramoreintuitiverepresentation,wehavedetaileda
comparisonofthemetamorphicvideogenerationcapabilitiesofdifferentalgorithms.
B MoreDetailsaboutAutomaticMetrics
B.1 ConstructionofRetrievalSentencesforMetamorphicScore
ToobtainaneffectiveMetamorphicScore(MTScore),wemeticulouslydesignedtendistinctretrieval
textstodifferentiatebetweentime-lapseandnormalvideos. Although,intheory,onlytworetrieval
sentencesareneededtodistinguishbetweengeneralandtime-lapsevideos,multipletextswereused
to enhance the model’s robustness and accuracy. This approach also provides diverse linguistic
representationsforeachvideocategory,ensuringcomprehensivecoverageandminimizingbias. As
showninTable5,thefirstfivesentences(Index0-4)describegeneralvideos,capturingstandard,
unalteredvideocontentinuniquephrasings. Thelastfivesentences(Index5-9)describetime-lapse
17videos,characterizedbyacceleratedplaybackorcondensedtimesequences,alsophrasedinvarious
waystocapturedifferentnuances. WhencalculatingtheMTScore,thevideoretrievalmodeluses
thesetextstoevaluateeachframeofthevideo,assigningprobabilitiesbasedonthematches. The
finalresultisobtainedbysummingthegeneralprobabilityandthemetamorphicprobability. For
GPT4o-MTScore,weusedafive-pointratingscaleandprovideddetailedscoringguidelinesinthe
prompt,asshowninTable6.
Table5: Retrievalsentencesforcoarse-grainedscore(MTScore)
Index Sentence
1 Aconventionalvideo,notatime-condensedvideo.
2 Ausualvideo,notanacceleratedvideosequence.
3 Anormalvideo,notatime-lapsevideo.
4 Astandardvideo,notatime-lapse.
5 Anordinaryvideo,differentfromafast-motionvideo.
6 Atime-lapsevideo,distinctfromaregularrecording.
7 Atime-lapsefootage,notyourtypicalvideo.
8 Afast-motionvideo,unlikeastandardvideo.
9 Atime-condensedvideo,notaconventionalvideo.
10 Anacceleratedvideosequence,notausualvideo.
Table6: ScoringCriteriaforGPT4o-MTScore. Wesetguidelinesforeachscoretoensurethat
GPT-4omakeschoicesbasedonconsistentcriteria.
Score BriefReasoningStatement
1 Minimalchange. Thesceneappearsalmostlikeastillimage,withstaticelements
remainingmotionlessandonlyminorchangesinlightingorsubtlemovementsof
elements. Nosignificantactivityisnoticeable.
2 Slightchange. Thereisasmallamountofmovementorchangeintheelementsofthe
scene,suchasafewpeopleorvehiclesmovingandminorchangesinlightorshadows.
Theoverallvariationisstillminimal,withchangesmostlybeingquantitative.
3 Moderatechange. Multipleelementsinthesceneundergochanges,buttheoverall
paceisslow. Thisincludesgradualchangesindaylight,movingclouds,growing
plants,oroccasionalvehicleandpedestrianmovements. Thescenebeginstoshowa
transitionfromquantitativetoqualitativechange.
4 Significantchange. Theelementsinthesceneshowobviousdynamicchangeswitha
higherspeedandfrequencyofvariation. Thisincludesnoticeablechangesincity
traffic,crowdactivities,orsignificantweathertransitions. Thescenedisplaysamixof
quantitativeandqualitativechanges.
5 Dramaticchange. Elementsinthesceneundergocontinuousandrapidsignificant
changes,creatingaveryrichvisualeffect. Thisincludeseventslikesunriseandsunset,
constructionofbuildings,andseasonalchanges,makingthevariationprocessvivid
andimpactful. Thesceneexhibitsclearqualitativechange.
B.2 MoreDescriptionofTemporalCoherenceScore
WepresentaconciseglobaldescriptionofthealgorithmforcomputingtheTemporalCoherence
Score,asshowninAlgorithm1. First,weprocesstheinputvideousingapre-trainedmodelwith
gridsizeGandthresholdT toobtainp . Next,wecountthenumberofmissingtrackingpoints
vis
m[i]ineachframeandthechangeinmissedpointsbetweenconsecutiveframes∆m[i]. Wethen
calculatetheaverageproportionofmissedpointsperframeR ,indicatingtheoverallvisibility
missed
issueacrossthevideo. Followingthis, wecomputethevariationinthenumberofmissedpoints
betweenconsecutiveframesV ,measuringframe-to-framecoherence. Wealsodeterminethe
missed
ratioofframesthatneedtobecutR ,reflectingtheextentofvideoeditingrequired,andcountthe
cut
numberofconsecutivechangesinmissedpointsexceedingthethresholdC ,indicatingfrequent
missed
large-scaleinstabilityinpointtracking. Additionally,wemeasurethemaximumcontinuouschange
inmissedpointsM ,highlightingthemostseverecontinuitybreaksinthevideo. Finally,we
missed
integratethesemetricstocalculatetheCoherenceScore(CHScore).
18≈
erocS2.0
≈
erocS4.0
T T
M M
≈
erocS3.0
≈
erocS5.0
T T
M M
≈ erocS0 H. C2 ≈ erocS0 H.5 C1
≈ erocS0 H. C8 ≈ erocS0. H6 C2
Figure8: VisualReferenceforVaryingScoresofMTScoreandCHScore. Itisobservedthat
higherscorescorrelatewithincreasedmetamorphicamplitudeandcoherence.
Algorithm1CalculationofCoherenceScore
1: Input: Video,pre-trainedmodelwithgridsizeGandthresholdT
2: Output: CoherencescoreS
CHS
3: Processinputvideousingpre-trainedmodelwithgridsizeGandthresholdT togetp
vis
4: foreachframeido
5: countthenumberofmissingtrackingpointsineachframe
6: m[i]← 1 (cid:80)N (1−p [0,i,j])
N j=1 vis
7: endfor
8: foreachframeido
9: ∆m[i]←m[i+1]−m[i]
10: if∆m[i]>T then
11: frameiwillbeaddedtothesetframes_to_be_cut
12: C ←C +∆m[i]
missed missed
13: endif
14: endfor
15: R ← len(frames_to_be_cut)
cut frames
16: R ← 1 (cid:80)framesm[i]
missed frames i=1
17: V ←std(∆m)
missed
18: M ←max(∆m)
missed
19: TSI_sum←R +V +R +C +M
missed missed cut missed missed
20: S ← 1
CHS TSI_sum
B.3 VisualizationoftheDifferentScoresofMTScoreandCHScore
WeprovidesomesamplesofdifferentscoringmagnitudesforMTScoreandCHScore,asshownin
Figure8. Itcanbeseenthatbothscoresareconsistentwithhumanperception.
C MoreDetailsaboutChronoMagic-Pro
C.1 Multi-AspectDataPreprocessing
Duetotheabundanceoflow-qualityvideosonvideoplatforms,wefilteroutlower-qualityvideos
based on metadata such as view counts, comments, and likes after acquiring the original videos,
ultimatelyobtaining66,226originalvideos. Then,weuseathree-stagemethodtofurtherprocess
andfilterthedata.
AdaptiveVideoTransitionCutting. Instageone: sinceourtrainingdataissourcedfromvideo
platforms(e.g.,YouTube)wherevideosaredesignedtoengagetheaudience,theyinherentlycontain
many transitions (significant changes in content during video playback). To address this issue,
we follow the method described in Panda70M [15] to split the videos into multiple semantically
19PPhhaassee oonnee
AAuuttoommaattiicc VViiddeeoo CClliippss CCuuttttiinngg
vviiddeeoo cclliipp::
VViiddeeoo CClliippss RReeccoovveerryy
sseemmaannttiiccaallllyy
ccoonnssiisstteenntt
TThhee ttiimmee ooff aa ssiinnggllee ffrraammee oonn tthhee cclliipp
CCuuttttiinngg
UUnnrreeccoovveerr
RReeccoovveerr
PPhhaassee ttwwoo MMeettaammoorrpphhiicc--GGeenneerraall ggeenneerraall mmeettaammoorrpphhiicc
vviiddeeoo cclliipp::
CCllaassssiiffiiccaattiioonn SSttrraatteeggyy
ssiinnggllee--sscceennee,,
sseemmaannttiiccaallllyy
OOuuttppuutt::
ccoonnssiisstteenntt mmeettaammoorrpphhiicc mmeettaammoorrpphhiicc mmeettaammoorrpphhiicc ggeenneerraall
UUnniiffoorrmmllyy ssaammppllee ffrraammeess
CCaappttiioonn ooff aa ssiinnggllee
ffrraammee
TThhee ttiimmee ooff aa ssiinnggllee
ffrraammee oonn tthhee cclliipp
SShhaarreeGGPPTT44VVIIddeeoo
TThhee ttiimmee ooff aa ssiinnggllee ffrraammee oonn tthhee cclliipp VViiddeeoo TTiimmeerr
ffrraammee 11 ffrraammee 3300 ffrraammee 6600 ffrraammee 9900 ffrraammee 112200 ffrraammee 115500 ffrraammee 119922
CCaappttiioonn
ttiimmee 11 ttiimmee 3300 ttiimmee 6600 ttiimmee 9900 ttiimmee 112200 ttiimmee 115500 ttiimmee 119922
PPhhaassee tthhrreeee
VViiddeeoo--TTeexxtt ppaaiirrss
Figure 9: The pipeline of constructing ChronoMagic-Pro/ProH. (Top) We first use OpenCV
[7]andImageBind[21]tosplitthevideoandgetsemanticallyconsistentsingle-scenevideoclips.
(Middle)Then,weusethevideoretrievalmodel[71]tofilteroutvideosthatarenotlow-qualityvideos.
(Bottom)Finally,uniformlysampleN framesandobtaincaptionsforeachusingShareGPT4Video
[12],andletitsummarizethevideobasedonthesecaptionsandtheirframepositions.
consistentsingle-sceneclips. Specifically,OpenCV[7]initiallysplitsthevideobyanalyzingpixel
differencesbetweenadjacentframes. LetI betheimageframeattimet;thedifferencebetweentwo
t
adjacentframescanbecomputedas:
H W
(cid:88)(cid:88)
D = |I (i,j)−I (i,j)| (10)
t t t+1
i=1j=1
where H and W are the height and width of the frame, and i and j represent pixel positions,
respectively. VideosaresplitintoclipswhereD exceedsacertainthresholdτ. Then,theImageBind
t
model[21]recombineserroneouslysplitclipsbyanalyzingfeaturespacedifferencesbetweenadjacent
clips. Letϕ(I )representthefeaturevectorofframeI obtainedfromtheImageBindmodel. The
t t
featurespacedifferencebetweenadjacentclipsC andC canbecomputedas:
i i+1
(cid:13) (cid:13)
F
i
=(cid:13)ϕ(I ti)−ϕ(I ti+1)(cid:13)
2
(11)
wheret andt arethetimesofthelastframeofC andthefirstframeofC ,respectively. Clips
i i+1 i i+1
arerecombinedwhereF isbelowacertainthresholdη. Thisprocessresultsin460Ksemantically
i
consistentsingle-scenevideoclips(ChronoMagic-Pro).
VideoRedundancyElimination. Instagetwo: videopublishersoftenuseeye-catchingtitles,
descriptions,orhashtagstoattracttraffic. Asaresult,time-lapsevideosfoundthroughsearchterms
maybegeneralvideos. Manuallyscreeninglarge-scalevideosisimpractical,weconstructazero-shot
metamorphic-generalclassificationstrategybasedonthevideoretrievalmodel[71]. Specifically,
wecarefullyconstructed10retrievalsentencestodescribemetamorphicandgeneralvideos. LetT
i
representthei-thretrievalsentencefromthesetofretrievalsentences{T ,T ,...,T }. Foreach
1 2 10
videoV inthedataset,thevideoretrievalmodelcomputestherelevanceprobabilityP(T |V)for
i
20ccaaTT nn oo ooccrr oo ff pphh nnoooocc tt ff ccaaee tt ppnnhh mm aa aaee t ot oddss rr rrbb eeii bb ddvv tt nnrr ttpp ii iitt oo ssuu bbeeii ssll ttddrrooee ssdd ee ggttii rr uu rruu iiiisscc ll ssaaee rraa ttdd nndd aattccffii tttt eennoo iitt ss ssiieeii ttppee oouu rrnnnn sscc ii ,, ttuu aa uunnoorr aass gg ssgg aa ee llrrnnaa cchh ,,nn tt bbii ee nn ,, ee kk ttoouuiill tt ,,oo tt eessddss aa,, ooee uurrrr hhssww ff ,, tt ttaa rree lltt cc aaee ss ee wwtt aayycc hh eemmrr hh mm aa hh aa ii eess oouu ee nnmmssaa tt ii eess aaeeaall vvcc cc ee uu eerree nn bb aaaa ll nneett wwddaa rr lleess ddvv dd ee uu rriicc cc eeiikk tt iinn eeaa aarr ss eeeeaa tt uu hh ttpp,,gg ee cc ll ll ee llww hhrr llttaao eo eww33 ee hh rr vv hh eeaa nnff ss ssmmDD hh oo ccii ee ddii cciizz bb nntt ooaaii ff ooss eedd ee tteerr uu aaff rr nnii nn,,ee nnddaatt ssddee aa ttoo ttttgg eehh dd tt iinn tt uuww eess vv ffbb hhll ee ooee eedd rraauu mmii mmffnnyy eess rr aammddee ssrr eeoo pp ii aann llbb ssff rr bbaa ssrrcc ee oo ooiiee-- bbee aarr nneeeeee ll rrnn rrpp dddd eeccdd ii eettaann ggaacc aa aa ii ee aa aa aa ssttnn eevv oowwkk rrss nn ii cc cc ..ee ss yynnoo--cc nnss ee mmii hhaatt ..ll SSrr nnee aa ssll ii ddtt pp hh aass ee DDkk nnnn uu sshh iiee eett ttee ddeeddee tt eerr eett eehh ee ss.. ssrr yy ttrr ssmm ssee iirree AAoo oo ttgghhii ppee ii ssxx aaff bb aa iiuu aa ss nnoo iioo ttcc nnnn tt ttttee nn ll rrttaa rrtt gg.. ttee eegg ooeeaa dd hh ii iiii hh ..IIrr uu zz ff rr kkmm oocc iiee ttii eennii llnn oo iihh nnoo hhaa aa ttrroo nn gg nn .. rr hhrrll tt.. vv iidd gg tt ,, ss TT hh ii tt eeAA hh ee oo dd hh ee hh aarr nn eeee nn ee tt oo ssTT gg ee eemmhh aa tt xxii mm ttcc uu ppss cc ss iioo ll nnee lloo vvuuoo ttii oonn gg mm iimm ggiirr rroo ddcc pp ,,gg aa aa bbrr rr eellss ggeeee ttgg eeiitt ooiiaa nn ss ii ee tt vvrr ooll ttaa wweeee ee uudd nnssaa tt vv cc nn ooii ww pp ii oogg ee oott ii oo uu iittpp rr ttii ffuu ll ttnn lltt ss hhee dd rr hhmmss aa tteeaa oo ee aatt iill ll hhiirr nn ff .. iioo nn gg hhcc kk iiss nnTT dd sshh aabbrr oo ee ee tt ttee hh rrrr ll vvoo gg,,oo yy aa iiiiee ii bbpp aazzcc rr wwtt bb ddee mmeekk oo ttaaii iiooee eenn uu ww nnttrr eeaann ss aahh cc oo aa ss aa cc llhh oo ccrrtt pp nnaa rrss rreeii ookkhh aa dd tt iicc nnaadd bbaa cc ssee aarr vvss tteett ee ee iicc nnee ii mmtt hh ss ss rrtt ddss aaee tteeii .. oouu uuii gg llnn hhmm TT nn llrr ww nn ssss aaoo eeee mmhhuu hh aahh ttrr tt ccll ee nn iiaa eeoo ff hhaa ooww rrdd tteett nntt aa ee nneeaa yy ttaaee ll tt ..ff nn aattoo ttss rr vv oogg tteeuu oo ii oo iinn rriirrrraa kkrr rr oomm oocc ff ee ooss bbnnbb ll uurr nnss uu uuaa uu nnll oo oo mmddii iiaa ii ddkk nn rr llvv ll ee dd eedd ee tt ..ii ss iinn dd ii TT nn nn ttee gghhgg oo ee ccssT TT T oott oobb cc ffTT hhh hh hmm tt nneeccee eehh e ee e tthh ssnnoo aaii ee hhnn tteenn eenn oob bb b oo gg ee rrrrss nn nn ..tt u au a ff aa uu eett eeii ll TTcccc ii bb nn yymm ccllrr bbrr kkoo dd hh uuaadd ttuu ,,ee lloonn ggii iirr ee eeoo cc llnnww bbss rrii ee dd ddccll tttt ggtt aaooaa jjii iiii kk tt nnrr ee cc nnuuoopp ii ffii ssss hhuu ooss ttcc ggnnnn rrss bb cc ii tt ,, ooddee ttss oott oott yy pptt oo mmww ii hh hh nntt tt ttvv rree oo tthh hh ssee hhee ttuu ooii ffdd hhttdd aa hh ee iiaa cc .. ii hhbb ccmm eeee tt tt nnrrtt eeeeTT hh ee uu hhoo ee aa ss eecc oo tt ii sshh ee dd iiss ll ii gg nndd hh hhbbnn ttdd ss ee rr ddvvaaee rree ii jjoo ii ss ooff ssiinn uueebb nnpp nn dd oo oo vv uuhhgg ccgg ccuu ssii ffee iicccc nnaa oo tttteehhii dd gg oouutt ll dd ww uuss ttssoodd eess rrss ss rr hh nniiwwaaii ooooaa uu tt ss eennaa eeoo nn rrdd pphhnn tt ..gg aa ff aadd ,, uu rrss hh vvTT rrcc ii yyii ooaacc aann iitt rrcc eehhss ddhh ll uuee nnhh ooll bb aallaa eeee eettmm gg ddyynn uuee rree ooaa hh hhee gg ss aa bbaa aa tt iirr ..vv ee oo hhtt ttccdd aa uuccoo ii ee uu dd bb ttkk iiccee hhff iitt llttpp ee ssee ll ooii dd .. hhuu nn --oo aa rroo nntt iiTT cceeeebb gg ff ss nn,, hh ss oobb hh yy ggaattssaa eess iibb mmuu --ee nn rr ddkknnhh iill ssii vv ppuu ssooll yydd ddaatt ttdd ii llcc cc ,, pp iidd hhee ee ii ii ttttkk aann ee ttpp ee ee ttiiiiss nngg oooo.. ee oo ii,, dd nnnnss dd..
IInnppuutt VViiddeeoo ((aa)) 1133BB--VViiddeeoo ((bb)) 3344BB--VViiddeeoo ((cc)) 88BB--FFrraammeess
Figure10: AblationondifferentCaptioningmethod. Directlyinputtingthevideointothemodel
andhavingitdescribethecontentislesseffectivethaninputtingkeyframesintoit.
eachretrievalsentenceT . WedenotetheseprobabilitiesasP ,P ,...,P . Todetermineifavideo
i 1 2 10
isgeneralormetamorphic,wesumtherelevanceprobabilitiesforeachretrievalsentence. LetS
gen
bethetotalrelevanceprobabilityforgeneralvideosandS bethetotalrelevanceprobabilityfor
meta
metamorphicvideos:
5 10
(cid:88) (cid:88)
S = P(T |V) S = P(T |V) (12)
gen i meta i
i=1 i=6
wethenjudgewhetherthevideoisatime-lapsevideothroughavotingstrategy:
(cid:26)
general ifS >0.5
Class(V)= general (13)
metamorphic otherwise
IfS >S ,thevideoisclassifiedasgeneral;otherwise,itisclassifiedasmetamorphic.
general metamorphic
Tothisend,weobtain150Kvideoclipswithhigherpurityandquality(ChronoMagic-ProH).
Time-AwareAnnotation Instagethree: afterobtaininghigh-qualitytime-lapsevideoclips,itis
crucialtoaddappropriatecaptions. Thesimplestapproachistoinputthevideoclipsintoalarge
multimodal model to generate text descriptions of the video content. However, our experiments
foundthatthe8B[41],13B[76],and34B[35]modelscouldnotaccuratelydescribethecontentof
time-lapsevideos,resultinginseverehallucinations,asshowninFigure10. Therefore,wedecided
tofollowtheannotationstrategyofMagicTime[79]. UnlikeMagicTime,duetohighercosts,we
adoptedanopen-sourcemodel[12]insteadoftheclosed-sourceGPT-4V[1]. AsshowninFigure
4, we first uniformly sample N frames from each video segment, input these N frames into the
multimodallargemodeltodescribethecontent,andfinallyhavethemodelsummarizethefinalvideo
captionsbasedonthetextualdescriptionsofN framesandthecorrespondingpositionofeachframe
inthevideo. Tobalancecostandeffectiveness,wechosetousethe8Blargemultimodalmodel[12]
insteadofthe34B.
C.2 DistributionoftheGeneratedCaptions
ToanalyzetheworddistributioninourgeneratedcaptionswithinChronoMagic-Pro,wecomputed
theirfrequencydistributions. Theresults,showninFigure11,revealaprevalenceoftermsrelated
totime-lapsevideos,including"change,""transition,"and"progressing."Additionally,wordsfrom
fourprimarycategoriesareevident: biological(e.g.,mealworm,flower,tree),human-created(e.g.,
building,painting,walking),meteorological(e.g.,eclipse,cloud,sunrise),andphysical(e.g.,burning,
explosion). ThesetermsunderscoreChronoMagic-Pro’sfocusonlarge-scalemetamorphicchanges,
persistent transformations, and substantial physical interactions. The distribution of Captions in
ChronoMagic-ProHissimilartothatinChronoMagic-Pro.
C.3 SamplesoftheChronoMagic-Pro
Figure12showcasesadiversearrayofsamplesfromtheChronoMagic-Prodataset,whichfeatures
anextensivecollectionoftime-lapsevideosacrossseveralcategories,includingplants,buildings,
ice, food, andvariousotherobjectsandphenomena. Eachvideocapturesdynamicchangesover
time,providingrichvisualinformationthatsurpassesthephysicalknowledgecontainedinmany
existingText-to-Video(T2V)datasets. Thesesamplesillustratethedataset’sdiversityanddepth,
21(a) Word Cloud of ChronoMagic-Pro (b) Word Cloud of ChronoMagic-ProH
Figure11: Thewordcloudsofthegeneratedcaptionsof(Top)ChronoMagic-Proand(Bottom)
ChronoMagic-ProH.Thedatasetfocusesonchanges(gradually,progressing,increasing,etc.),pro-
cessesspanningalargeamountoftime,suchasflowerblooming,icemelting,buildingconstruction,
sunriseandsunset.
Biological Covers all content related to living Human Creation Includes all objects created or
organisms in nature influenced by human activities.
Prompt: Time-lapse of a magnolia flower blooming process, capturing the transition Prompt: Time-lapse of a modern house being constructed in Minecraft, beginning with a
from a tightly closed bud with a touch of pink to a fully open, wide-petaled bloom basic structure and progressively adding walls, roof details, and new sections to create a
showcasing its inner floral structures. cohesive and unified appearance by the end of the video.
Prompt: Time-lapse of microgreens germinating and growing in a white planter: starting Prompt: Time-lapse of a 3D printing process: starting with the creation of the base layer,
as seeds with minimal sprouting, sprouts emerge and leaves grow upward, developing gradually developing the woven patterns, continuing with vertical expansion and
into a dense green cover, and ultimately resulting in a thick, even canopy of ... intricate weaving, and concluding with the vase fully formed, showcasing ...
Meteorological Includes all content related to Physical Includes all content related to non-
meteorological phenomena. biological physical phenomena.
Prompt: Time-lapse of a solar eclipse showing the moon's passage across the sun from Prompt: Time-lapse of an ice cube melting on a solid surface, showcasing a forward
the upper right, progressing through stages of a partial eclipse, reaching over half sequence from its fully intact state to nearly complete dissolution into a wide puddle of
coverage, significantly obscuring the sun, until a slender crescent remains. Finally, ... water, with progressive signs of melting and water accumulation evident throughout ...
Prompt: Time-lapse of a beach sunset capturing the sun's descent and the accompanying Prompt: Time-lapse of a cake baking in an oven, depicting its gradual rise and browning.
color transition in the sky. Starting with a bright sun above the horizon, the video shows Starting with a pale, smooth surface, the cake steadily expands and develops a golden
the sun lowering and the sky changing from bright to orange-purple hues, leading ... crust over the course of the video, reaching maximum volume and a deep golden ...
Figure12: SamplesfromtheChronoMagic-Prodataset. Thedatasetconsistsoftime-lapsevideos,
whichexhibitmorephysicalknowledgethantheexistingT2Vdataset.
encompassing biological, human-created, meteorological, and physical categories, designed to
supportadvancedresearchinhigh-dynamictext-to-videogenerationandrelatedfields. Additionally,
thedatasetincludesbothtime-lapsevideoswithsignificantstatechanges(e.g.,flowersblooming)
andvideoswithsmallerstatechanges(e.g.,cloudsfloating).
22D MoreDetailsaboutExperiment
D.1 ComputationResourceDetails
We employ two types of GPUs: NVIDIA H100 and NVIDIA A100. All implementations are
conductedbasedontheofficialcodeusingthePyTorchframework.
D.2 DetailsofEvaluationT2VModels
ModelScopeT2V. ModelDetails. ModelScopeT2V[66],featuringaU-Netarchitecture,extends
theT2ImodelStableDiffusion[55]byincorporating1Dtemporalconvolutionandattentionmodules
alongside the 2D modules for video modeling. Its training data consists primarily of image-text
pairs (LAION [57]) and general video-text pairs (WebVid-10M [2] and MSR-VTT [75]), but it
doesnotincludethetime-lapsevideosdiscussedinthispaper. ImplementationSetups. Weutilized
theModelScopeT2VcodeandmodelofficiallyreleasedonHuggingFace,maintainingtheoriginal
parametersettings. Weusedaspatialresolutionof256×256andaframerateof8fpstogeneratea
2-second(16-frame)video.
ZeroScope. ModelDetails. ZeroScope[62]isawatermark-freeU-Net-basedvideomodelbuilt
onModelScopeT2V[66],capableofgeneratinghigh-quality16:9compositionsandsmoothvideo
outputs. Themodelistrainedon9,923clipsand29,769labeledframes(24framesperclip,576×320
resolution)derivedfromtheoriginalweightsofModelScopeT2V[66]. Theofficialdocumentation
does not specify the exact training data; we speculate that time-lapse videos were not included.
ImplementationSetups. WeutilizedtheZeroScope_v2_576wcodeandmodelofficiallyreleasedon
HuggingFace,maintainingtheoriginalparametersettings. Weusedaspatialresolutionof576×320
andaframerateof8fpstogeneratea3-second(24-frame)video.
Text2Video-Zero. Model Details. Text2Video-Zero [28], featuring a U-Net architecture, is a
zero-shotvideogenerationmethodbasedontheT2ImodelStableDiffusion[55]. Itgenerateslatent
codesforallframesusingrichmotiondynamicsandutilizesaself-attentionmechanismtoenable
allframestointeractwiththelatentcodesofthefirstframe. Thisprocessultimatelyachieveshigh
spatialandtemporalconsistencyinthevideothroughdenoising. Itdoesnotrequiretrainingdata
and,therefore,doesnotusetime-lapsevideosastrainingdata. ImplementationSetups. Weutilized
theofficiallyreleasedText2Video-Zerocodeandmodel,maintainingtheoriginalparametersettings.
Specifically,weusedthedreamlike-photoreal-2.0versionofStableDiffusion[55],withaspatial
resolutionof512×512andaframerateof8fps,togeneratea2-second(16-frame)video.
LaVie. ModelDetails.ModelDetails.LaVie[69],featuringaU-Netarchitecture,isanextensionof
theT2ImodelStableDiffusion[55]. ItconvertstheT2ImodelintoaT2Vmodelbyaddingtemporal
dimensionattentionafterthespatialmodulesandadoptinganimage-videojointtrainingstrategy.
Itstrainingdataprimarilyconsistsofimage-textpairs(LAION[57])andgeneralvideo-textpairs
(WebVid-10M[2]andVimeo25M[69]),butitdoesnotincludethetime-lapsevideosdiscussedin
thispaper. ImplementationSetups. WeusedtheofficiallyreleasedLaViecodeandmodel. Although
LaVie[69]providesoptionsforframeinterpolationandsuper-resolutionaftervideogeneration,we
didnotusethemtomaintainfairness. Wefollowedtheoriginalparametersettings,usingaspatial
resolutionof512×320andaframerateof8fps,togeneratea2-second(16-frame)video.
AnimateDiff. ModelDetails. AnimateDiff[22],featuringaU-Netarchitecture,isanextension
oftheT2ImodelStableDiffusion[55]. Itattachesanewlyinitializedmotionmodelingmoduleto
afrozentext-to-imagemodel,thentrainsitonvideoclipstoextractreasonablemotionpriorsfor
videogeneration. Itstrainingdataprimarilyconsistsofgeneralvideo-textpairs(WebVid-10M[2]),
excludingthetime-lapsevideosdiscussedinthispaper.ImplementationSetups.Weusedtheofficially
releasedAnimateDiffV3codeandmodel,maintainingtheoriginalparametersettings. Weuseda
spatialresolutionof384×256andaframerateof8fpstogeneratea2-second(16-frame)video.
VideoCrafter2. ModelDetails. VideoCrafter2[10],featuringaU-Netarchitecture,issimilarto
AnimateDiff[22],asbothaddtemporalmodulestoStableDiffusion[55]toachievevideogeneration.
However,VideoCrafter2differsbyencodingfpsasaconditionintothemodelandimplementing
theI2Vfunction. Itstrainingdataprimarilyincludesimage-textpairs(LAION-COCO[16],JDB
[63])andgeneralvideo-textpairs(WebVid-10M[2]),butitdoesnotincludethetime-lapsevideos
discussedinthispaper. ImplementationSetups. WeusedtheofficiallyreleasedVideoCrafter2code
andmodel,maintainingtheoriginalparametersettings. Weusedaspatialresolutionof512×320and
aframerateof10fpstogeneratea2-second(20-frame)video.
23MCM. ModelDetails. MCM[80],featuringaU-Netarchitecture,isadistillationvideogeneration
methodbasedontheT2ImodelStableDiffusion[55]. Itproposemotionconsistencymodels(MCM)
toimprovevideodiffusiondistillationbydisentanglingmotionandappearancelearning,addressing
framequalityissuesandtraining-inferencediscrepancies. Itstrainingdataprimarilyincludesimage-
textpairs(LAION-aes[57])andgeneralvideo-textpairs(WebVid-2M[2]),butitdoesnotinclude
thetime-lapsevideosdiscussedinthispaper. ImplementationSetups. Weusedtheofficiallyreleased
MCM-modelscopet2v-laioncodeandmodel,maintainingtheoriginalparametersettings. Weuseda
spatialresolutionof256×256andaframerateof7fpstogeneratea2-second(14-frame)video.
MagicTime. ModelDetails. MagicTime[79]isaU-Net-basedmetamorphicvideogeneration
model built on AnimateDiff [22]. It is capable of generating time-lapse videos with significant
timespansandpronouncedstatechanges,suchastheentireprocessofaseedbloomingorbuilding
construction. The model is trained using 2,265 metamorphic (time-lapse) clips and the original
weightsfromAnimateDiffV3[22]. ItstrainingdataprimarilyincludesChronoMagic[79],making
ittheonlyexistingT2Vmodelthatusestime-lapsevideosinthetrainingprocess. Implementation
Setups.WeusedtheofficiallyreleasedMagicTimecodeandmodel,maintainingtheoriginalparameter
settings. Weusedaspatialresolutionof512×512andaframerateof8fpstogeneratea2-second
(16-frame)video.
Latte. ModelDetails. Latte[45]isapioneerinopen-sourceDiT-basedT2Valgorithms. Itinherits
the pure Transformer architecture of the T2I algorithm PixArt-α [11] and extends it by adding
temporalmodulesaftereachspatialmodule,trainingfromtheoriginalweightsofPixArt-α[11]to
achieveaDiT-basedT2Valgorithm. Itstrainingdataprimarilyincludesgeneralvideo-textpairs
(Vimeo25M[69]andWebVid-10M[2]). Althoughitincludesthetime-lapsevideosmentionedinthis
paper,theyprimarilyconsistofskyvideoswithfewerphysicalpriors,makingitunabletogenerate
videossuchasseedgerminationandflowerblooming. ImplementationSetup. Weusedtheofficially
releasedLatteT2Vcodeandmodel,maintainingtheoriginalparametersettings. Weusedaspatial
resolutionof512×512andaframerateof8fpstogeneratea2-second(16-frame)video.
OpenSoraPlanv1.1. ModelDetails. OpenSoraPlanv1.1[40]isahigh-qualityvideogeneration
modelbasedonLatte[45]. ItreplacestheImageVAE[29]withVideoVAE(CausalVideoVAE[40]),
similartoSora[8],enablingthegenerationofvideosuptoapproximately21secondslongandhigh-
qualityimages. Itstrainingdataconsistsofvideosandimagesscrapedfromopen-sourcewebsites
undertheCC0license,labeledusingShareGPT4Video[12]tocreateahigh-qualityself-builtdataset.
Theofficialdocumentationdoesnotspecifytheexacttrainingdata; wespeculatethattime-lapse
videoswerenotused. ImplementationSetup. WeusedtheofficiallyreleasedOpenSoraPlanv1.1
codeandmodel. AlthoughitprovidesT2Vmodelsinthreeversions: 65frames,221frames,and513
frames,wechosethe65-frameversiontoensurefairnessbymaintainingasimilarvideolengthto
othermodels. Wekepttheoriginalparametersettings,usingaspatialresolutionof512×512anda
framerateof24fpstogeneratea3-second(65-frame)video.
OpenSora1.1&1.2. ModelDetails. OpenSora1.1&1.2[86]isahigh-qualityDiT-basedT2V
modelthatintroducestheST-DiT-2architecture,buildingonLatte[45]theformerisbasedonthe
DiffusionModelandthelatterisbasedontheFlowModel. Itsupportsthegenerationofimages
or videos with any aspect ratio, different resolutions, and durations. Its training data consists of
imagesandvideosscrapedfromopen-sourcewebsitesandalabeledself-builtdataset. Theofficial
documentationdoesnotspecifytheexacttrainingdata;wespeculatethattime-lapsevideoswerenot
used. ImplementationSetup. WeusedtheofficiallyreleasedOpenSora1.1&1.2codeandmodel.
ForOpenSora1.1,weemployedthestage-3checkpoint,settingthespatialresolutionto512×512
andtheframerateto24fps,togeneratea2-second(48-frame)video. ForOpenSora1.2,wesetthe
spatialresolutionto1280×720andtheframerateto24fps,producinga4-second(96-frame)video.
D.3 VerificationExperimentonChronoMagic-Pro
ToverifythevalidityandrobustnessoftheChronoMagic-Prodataset,weconductedquantitativeand
qualitativevalidationexperimentsbasedonOpenSoraPlanv1.1[40]. Specifically,wefine-tunedthe
temporalmoduleoftheOpenSoraPlanv1.1modelusingauniformframeextractionstrategyand
LoRA[17],basedontheweightsofOpenSoraPlanv1.1[40]. Duetolimitedcomputationalresources,
werandomlyselectedonly10,000video-textpairsfromChronoMagic-Profortraining. Theresults
are shown in Table 7. After fine-tuning with ChronoMagic-Pro, the visual quality (UMT-FVD),
textrelevance(UMTScore),andmetamorphicamplitude(MTScoreandGPT4o-MTScore)wereall
effectivelyimproved. Notably,theenhancementinmetamorphicamplitudeendowedOpenSoraPlan
24Table7: QuantitativecomparisonofOpenSoraPlanv1.1[40]beforeandafterfine-tuningusing
ChronoMagic-Pro. "↓"denoteslowerisbetter. "↑"denoteshigherisbetter.
Method Venue UMT-FVD↓ UMTScore↑ MTScore↑ CHScore↑ GPT4o-MTScore↑
OpenSoraPlanv1.1[40] Github’24 188.53 2.421 0.327 10.35 2.19
OpenSoraPlanv1.1[40]+ChronoMagic-Pro Our 185.72 2.753 0.341 5.626 3.03
“Time-lapse of sunflower seed germination and early growth over several days, showing the ...” “Time-lapse of a snow-covered mountain transitioning from early morning to later morning ...”
erofeB
retfA
“Time-lapse of a sunrise progression: starting in pre-dawn darkness, gradually lightening as the ...” “Time-lapse of a cityscape at night showing varying traffic patterns on a riverbank road ...”
erofeB
retfA
Figure13: QualitativecomparisonofOpenSoraPlanv1.1[40]beforeandafterfine-tuningusing
ChronoMagic-Pro. Afterfine-tuning,thechangesinthegeneratedvideosarenolongerlimitedto
lightingandcameramovement,butareextendedtochangesinthestateofobjects. Additionally,it
ensuresthatthevisualquality,textrelevance,andcoherencearemaintainedwithoutloss.
[40]withtheabilitytogeneratetime-lapsevideosofsignificantstatechanges, suchasblooming
flowersandcitytraffic. WealsoprovideQualitativeAnalysis,asshowninFigure13. Itisevident
that,afterfine-tuning,thegeneratedvideoscanextendchangesbeyondmerelightingandcamera
movementstoalterationsinthestateofobjects,whileensuringthatthevisualquality,textrelevance,
and coherence remain uncompromised. This proves that ChronoMagic-Pro can support existing
modelsingeneratinghigh-qualitytime-lapsevideoswithsignificantstatechanges,providinganew
approachforfutureT2Vmodeltraining. However,usingauniformframeextractionstrategyand
LoRA[17]formodelfine-tuningmayleadtodecreasedvideocoherence,whichneedstobeaddressed
infuturework. Inthisstudy,weemploythesemethodssolelyforverificationexperiments.
D.4 MoreQualitativeEvaluationonChronoMagic-Bench
Duetospacelimitations,additionaltime-lapsevideosgeneratedbydifferentbaselinemethodsare
showninFigure14. Similartotheresultsinthemaintext,mostalgorithms,exceptforMagicTime
[79],failtogeneratetime-lapsevideoswithsignificantstatechanges,suchasbuildingconstruction.
However,fortime-lapsevideoswithsmallerstatechanges,essentiallyfaster-movingvideoslikecity
trafficchanges,U-Net-basedmethods[66,62,28,69,22,10,79]exhibitmuchbettervisualquality,
textrelevance,andcoherencecomparedtoDiT-basedmethods[45,40,86]. Thisagaindemonstrates
thatU-Net-basedmethodsarecurrentlymorestableandcapableofproducingsatisfactoryresults
withminimalinference. AllvideosgeneratedbyallmodelsonChronoMagic-Benchwillbemade
publiclyavailable.
D.5 MoreQuantitativeEvaluationonChronoMagic-Bench-150
We conduct a quantitative analysis of the temporal coherence of T2V models, with the results
presentedinTable8. TheCHScorevaluerangeswithinthesamedomain(open/close)areconsistent,
accurately reflecting the temporal coherence of different algorithms. Specifically, the temporal
coherenceofclosed-sourcemodelsisgenerallysimilar,whereasT2V-Zero[28]andOpenSora1.2
[86]inopen-sourcemodelsexhibittheworsttemporalcoherence. Thereasonsareasfollows: (1)
Currentvideotrackingalgorithmsperformpoorlyindetectingfluids(clouds,waterflow,trafficflow),
whicharecrucialcomponentsintime-lapsevideos;(2)Closed-sourcemodelscangeneraterealistic
fluidsfollowingpromptinstructions,whereasopen-sourcemodelsproducelessrealisticstaticfluids.
25“Time-lapse of a neighborhood construction process in Minecraft: starting from an empty plot of land, passing through initial wooden structures, progressing with houses and …”
V2TepocSledoM
M
CM
epocSoreZ em
iTcigaM
o Vr 2e Tz- eettttaaLL
eiVaL
1.1v
nalParoSnepO
ffffiiDDeettaamm
iinnAA
1.1
aroSnepO
22rreettffaarrCCooeeddiiVV 1.1
aroSnepO
“Time-lapse of a neighborhood construction process in Minecraft: starting from an empty plot of land, passing through initial wooden structures, progressing with houses and …”
V2TepocSledoM
M
CM
epocSoreZ em
iTcigaM
o Vr 2e Tz- ettaL
eiVaL
1.1v
nalParoSnepO
ffffiiDDeettaamm
iinnAA
1.1
aroSnepO
2retfarCoediV 2.1
aroSnepO
Figure14: MoreQualitativeComparisonwithdifferentT2Vgenerationmethodsforthetext-
to-videotaskinChronoMaigc-Bench. Mostmethodsstruggletofollowtheprompttogenerate
time-lapsevideoswithhighphysicspriorcontent.
Consequently,CHScorecanonlybeadaptedtocurrentopen-sourcemodels,indirectlyhighlighting
thesignificantdifferencesbetweenopen-sourceandclosed-sourcemodels.
D.6 DetailsofHumanEvaluation
To validate the effectiveness of the automated metrics, we selected a subset of videos for user
evaluation,inviting171participantstoprovidemanualevaluationresults.Toenhanceusersatisfaction,
we chose five representative baseline results (AnimateDiff [22], MagicTime [79], VideoCrafter2
[10],Opensora1.1[86],OpenSoraPlanv1.1[40])foruserstochoosefrom. Followingestablished
methodologiesfrompriorstudies[53,65,79,59],wedesignedadetailedquestionnaireforhuman
evaluatorstoratethegeneratedcontent. Theevaluationfocusedonfourprimaryaspects: Visual
Quality,TextRelevance,MetamorphicAmplitude,andCoherence. Foreachcriterion,weemployeda
26Table8: MoreQuantitativeComparisonwithT2VGenerationMethodsfortheText-to-Video
TaskinChronoMagic-Bench-150. "↓"denoteslowerisbetter. "↑"denoteshigherisbetter.
Method Venue Backbone Status CHScore↑
Gen-2[56] Runway U-Net Close-Source 5.27
Pika-2.0[33] PikaLab U-Net Close-Source 4.00
DreamMachine[44] LUMA DiT Close-Source 2.30
KeLing[32] Kwai DiT Close-Source 3.69
ModelScopeT2V[66] Arxiv’23 U-Net Open-Source 10.64
ZeroScope[62] CVPR’23 U-Net Open-Source 24.10
T2V-zero[28] ICCV’23 U-Net Open-Source 1.84
LaVie[69] Arxiv’23 U-Net Open-Source 9.58
AnimateDiff[22] ICLR’24 U-Net Open-Source 11.09
VideoCrafter2[10] Arxiv’24 U-Net Open-Source 7.78
MCM[80] Arxiv’24 U-Net Open-Source 14.14
MagicTime[79] Arxiv’24 U-Net Open-Source 11.58
Latte[45] Arxiv’24 DiT Open-Source 13.79
OpenSora1.1[86] Github’24 DiT Open-Source 10.46
OpenSora1.2[86] Github’24 DiT Open-Source 5.60
OpenSoraPlanv1.1[40] Github’24 DiT Open-Source 10.32
Figure15: VisualizationoftheQuestionnaireforHumanEvaluation. Weemployafive-pointrat-
ingscaleandprovidescoringguidelinestoensureconsistentselectionsbyusers,therebyminimizing
assessmentbias.
five-pointratingscaleandprovidedscoringguidelinestoensureconsistentuserselections,thereby
minimizingassessmentbias. Fordetailedcriteria,pleaserefertoFigure15.
27E MoreDetailsabout75SubcategoriesinChronoMaigc-Bench
Duetospacelimitations,weprovidedetaileddescriptionsofthe75searchtermsusedinChronoMagic-
Benchbelow(eachtermincludesthephrase"time-lapse"),allofwhichpertaintotime-lapse.Because
ofsearchenginelimitations,someprecisesearchtermsmaynotyieldoptimalresults. Therefore,
tocollectsearchtermsmorecomprehensively,someoverlapmayexistbetweenbroadertermslike
"plant"andprecisetermslike"flower".
Biological:
• Animal. Capturesthemovements,behaviors,andinteractionsofvariousanimalsoveran
extendedperiod. Thisincludeseverythingfromthedailyactivitiesofpetstothecomplex
behaviorsofwildanimalsintheirnaturalhabitats.
• SpiderWeb. Showcasestheintricateprocessofspidersspinningtheirwebs. Ithighlightsthe
changesthewebundergoesovertime.
• Butterfly. Focuses on the life cycle of butterflies, particularly the metamorphosis from
caterpillartochrysalistoadultbutterfly. Itincludestheintricateprocessofpupationand
emergence.
• Hatching. Documentsthehatchingprocessofvariouseggs,includingthoseofbirds,reptiles,
andinsects. Thiscategorycapturesthemomentofemergenceandtheinitialactivitiesofthe
newborns.
• FlowerDying. Capturestheend-of-lifeprocessofflowers,showinghowtheywiltanddecay
overtime.
• Mealworm. Showcasesthebehaviorofmealworms,includingtheirfeedinghabits.
• PlantGrowing. Thisbroadcategoryincludestime-lapsevideosofvariousplantsasthey
growfromseedstomatureplants. Itencompassesrootdevelopment,stemelongation,and
theemergenceofleavesandflowers.
• Ripening. Documentstheripeningprocessoffruitsandvegetables,showingthechangesin
color,texture,andoverallappearanceastheybecomereadyforconsumption.
• Leaves. Focusesonthegrowth,movement,andchangesofleavesonplants. Thisincludes
theunfoldingofnewleaves,changesincolor,andresponsestoenvironmentalfactors.
• Seed. Capturesthegerminationandinitialgrowthstagesofseeds,fromthefirstsignsof
sproutingtotheestablishmentofseedlings. Itfocusesontheearlyandoftendelicatestages
ofplantdevelopment.
• Blooming. Showcasestheprocessofflowersblooming,capturingthegradualopeningof
petalsandthetransformationfrombudstofullblossoms.
• Mushroom. Captures the rapid growth and development of mushrooms, from the initial
emergenceofthemyceliumtothefulldevelopmentofthefruitingbody.
Human-Created:
• 3DPrinting. Capturestheprocessof3Dprintingobjects. Thesevideosshowtheadditive
manufacturingprocesslayerbylayer,fromtheinitialbasetothefinal,completeobject.
• Painting. Showcasestheprocessofcreatingapainting,fromtheinitialsketchtothefinal
strokes.
• LaserEngraving. Showtheprocessoflaserengravingonvariousmaterials,suchasthe
processofpatternformation.
• Building. Documentstheconstructionofvariousstructures,includingresidential,commer-
cial,andindustrialbuildings. Thiscategoryhighlightsthestep-by-stepdevelopmentfrom
foundationtocompletion.
• MinecraftBuild. Capturestheconstructionofcomplexstructuresandlandscapeswithinthe
gameMinecraft.
• Demolition. Capturestheprocessofdemolishingbuildingsandstructures.
28• Fireworks. Capturesthedisplayoffireworks,showcasingtheentireprocessfromthelaunch
oftheexplosiveintotheskytoitstransformationintoburstsofcolorandpatternsinthe
nightsky.
• People. Focusesontheactivitiesandmovementsofpeopleinvarioussettings,including
streets,parks,andpublicspaces.
• Sport. Capturessportingeventsandactivities,highlightingthemovementofathletes,the
progressionofgames,andtheenergyofthecrowd.
• City. Focusesonthedynamicactivitieswithinacity,includingurbandevelopment,traffic
flow,anddailylife. Thesevideosoftenshowcasethebustlingandever-changingnatureof
urbanenvironments.
• Factory. Highlightstheoperationswithinafactory,includingassemblylines,manufacturing
processes,andthemovementofgoods.
• Market. Documentstheactivitieswithinamarket,includingthesettingupofstalls,move-
mentofpeople,andtradingofgoods.
• Office. Capturesthedailyactivitieswithinanofficeenvironment,includingtheebbandflow
ofworkers,meetings,andthegeneralhustleandbustleofofficelife.
• Restaurant.Documentstheactivitieswithinarestaurant,includingfoodpreparation,service,
andcustomerinteractions.
• Road. Capturethetrafficflow,andchangesinroadconditionsovertime.
• Station. Focusesontheactivitieswithintransportationstations,suchastrainstations,bus
terminals,andairports. Thesevideoscapturetheflowofpassengers,arrivals,departures,
andthehustleandbustleoftravelhubs.
• Traffic. Capturesthemovementofvehiclesonroadsandhighways,includingthetrafficflow,
congestion,andthechangingpaceofvehicularmovementthroughoutday.
• Walking. Focusesonpeoplewalkinginvariousenvironments,suchascitystreets,parks,
andmalls.
• Parking. Capturesthemovementofvehiclesinparkinglotsorgarages,includingtheflow
ofcarsastheyenter,park,andexit.
Meteorological:
• DaytoNight. Showthetransitionsfromdaylighttonighttime,capturingthegradualshiftin
lightandatmosphereasdayturnstonight.
• NighttoDay. Showsthetransitionsfromnighttimetodaylight,showingthegradualchange
inlightingandenvironmentasnightturnstoday.
• Day. Capturestheprogressionofdaylighthours, highlightingchangesinlightintensity,
shadows,andweatherconditions.
• Night. Showsthesequencesofnighttimescenes,oftencapturingthemovementofstars,
phasesofthemoon,andnocturnalactivities.
• Cloud. Showstheformation,movement,anddissipationofclouds,providingadynamic
viewoftheever-changingsky.
• LunarEclipse. ShowsthegradualmovementofthemoonthroughtheEarth’sshadowand
theresultingchangesinappearanceduringalunareclipse.
• Rainbow. Capturestheformation,duration,andfadingofrainbows,providingacolorful
displayovertime.
• Sky. Captures a variety of atmospheric phenomena such as cloud movements, sunrises,
sunsets,andweatherchangesovertime.
• Snowstorm. Showstheaccumulationofsnowandthechangingconditionsduringandafter
asnowstorm.
29• Storm. Highlightstheintensityandmovementofstormcloudsandlightningduringvarious
typesofstorms.
• Sunrise. Capturesthegradualincreaseinlightandtheawakeningoftheenvironmentduring
sunrise.
• Sunset. Showcasesthebeautifulcolorsandgradualfadingoflightasthedayendsduring
sunset.
• Aurora. CapturesthedynamicchangesandmovementoftheNorthernandSouthernLights,
showcasingtheevolvingnaturallightdisplaysovertime.
• Tide. Illustratestheriseandfallofsealevelsandtheirimpactoncoastallandscapesover
time.
• Wind. Capturestheeffectsofwindonlandscapes,includingthemovementofvegetation,
duststorms,andchangingcloudpatternsovertime.
• Seasons. Showsthedramaticchangesacrossdifferentseasons,highlightingthetransforma-
tionoflandscapesthroughouttheyear.
• Nature. Capturesvariousnaturalscenes,includingthegrowthofplants,changesinland-
scapes,andwildlifeactivity.
• Beach. Illustratethechangesintides,waves,andshiftingweatherconditionsthroughout
day.
• Desert. Showsthedramaticchangesinlight,temperature,andatmosphereindesertland-
scapesovertime.
• Forest.Illustrateschangesinfoliage,lightpatterns,andwildlifeactivityinforeststhroughout
thedayorseasons.
• Grassland. Highlightthesubtleyetsignificantchangesinvegetationandweatheringrass-
landsovertime.
• Lake. Captures reflections, water level changes, and the transformation of surrounding
landscapes.
• Mountain. Showcaseschangesinlight,weather,andcloudmovementaroundmountainous
peaksovertime.
• Ocean. Highlightsthecontinuousmotionofwaves, tides, andtheimpactofweatheron
oceanscenesovertime.
• Plain. Shows the transformation of open landscapes due to changing light and weather
conditionsovertime.
• River. Illustrates the flow of water, changes in water levels, and the transformation of
surroundinglandscapesovertime.
• Valley. Highlightschangesinlight,weather,andseasonaltransformationsinvalleyareas
overtime.
Physical:
• Baking. Showsthetransformationofdoughorbatterasitrisesandturnsintobakedgoods,
highlightingchangesincolor,texture,andvolumeovertime.
• Cooking. Showsthevariousstagesoffoodpreparationandcooking,highlightingchangesin
texture,color,andform.
• CandleBurning. Illustratesthegradualmeltingandburningofacandle,includingchanges
inthewaxandtheflickeringflame.
• TeaDiffusing. Illustrateshowtealeavesreleasetheircolorandflavorintohotwater,showing
thegradualdiffusionprocessandchangesintheliquid.
• Corrosion. Capturestheslowprocessofmaterialsdeterioratingduetochemicalreactions
withtheirenvironment,oftenresultinginrustorotherformsofdecay.
30• Decompose. Showsorganicmaterialsbreakingdownovertime,illustratingtheprocessof
decompositionandthechangesinformandstructure.
• FruitRotting. Illustratesthegradualdecayandbreakdownoffruit,showingchangesincolor,
texture,andstructureasitrots.
• Explosion. Capturestherapidanddramaticreleaseofenergy,showingthesuddenchangein
materialsandtheenvironment.
• Burning. Captures the process of combustion, showing how materials ignite, burn, and
reducetoashorotherresidues.
• Gasification.Showstheprocessofasolidorliquidturningintogas,highlightingthechanges
instateandmovementofparticles.
• IceMelting. Capturesthetransitionoficefromsolidtoliquid,showingthegradualmelting
processandchangesinshapeandvolume.
• InkDiffusing. Illustrateshowinkspreadsanddispersesinaliquid,showingthedynamic
patternsandchangesinconcentrationovertime.
• Melting. Showstheprocessofasolidturningintoaliquid,highlightingchangesinformand
consistencyasthematerialmelts.
• Rusting. Captures the slow formation of rust on metal surfaces, showing the chemical
changesandresultingtextureandcolorchanges.
• WaterFreezing. Showsthetransitionofwaterfromliquidtosolid,capturingtheformation
oficeandchangesinvolumeandstructure.
F Licensing,HostingandMaintenancePlan
AuthorStatement. Wetakefullresponsibilityforthelicensing,distribution,andmaintenanceof
ourChronoMaigc-BenchandChronoMagic-Pro.
License. ChronoMaigc-BenchandChronoMagic-ProareunderCC-BY4.0license.
Hosting. ThecodeanddatasetareuploadedtoGitHubandHuggingFaceandmadepublic. The
datasetisintheJSONfileformat.
Metadata. MetadataandBenchmarkareuploadedtoHuggingfacehttps://huggingface.co/
spaces/BestWishYsh/ChronoMagic-Bench.
31