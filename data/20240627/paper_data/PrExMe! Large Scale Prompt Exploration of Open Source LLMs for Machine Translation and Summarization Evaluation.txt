PrExMe! Large Scale Prompt Exploration of Open Source LLMs for
Machine Translation and Summarization Evaluation
ChristophLeiter,SteffenEger
NaturalLanguageLearningGroup(NLLG)
https://nl2g.github.io/
UniversityofMannheim
{christoph.leiter,steffen.eger}@uni-mannheim.de
Abstract
Largelanguagemodels(LLMS)haverevolu-
tionized the field of NLP. Notably, their in-
contextlearningcapabilitiesalsoenabletheir
useasevaluationmetricsfornaturallanguage
generation, making them particularly advan-
tageous in low-resource scenarios and time-
restrictedapplications. Inthiswork,weintro-
ducePrExMe,alarge-scalepromptexploration
formetrics,whereweevaluatemorethan720
prompttemplatesforopen-sourceLLM-based
metricsonmachinetranslation(MT)andsum- Figure 1: Schematic overview of our prompt explo-
marizationdatasets,totallingover6.6Meval- ration. We perform a grid search over datasets, task
uations. Thisextensivecomparison(1)serves descriptions,outputformatsandbaseprompts.
asabenchmarkoftheperformanceofrecent
open-sourceLLMSasmetricsand(2)explores
thestabilityandvariabilityofdifferentprompt-
(NLG) models in machine translation (MT) and
ing strategies. We discover that, on the one
summarization. Following the current trend, re-
hand,therearescenariosforwhichpromptsare
stable.Forinstance,someLLMSshowidiosyn-
searchers use LLMS as evaluation metrics and
craticpreferencesandfavortogradegenerated achieve remarkable performance, sometimes re-
textswithtextuallabelswhileotherspreferto lyingsolelyonin-contextlearning(e.g.Kocmiand
returnnumericscores. Ontheotherhand,the Federmann, 2023a; Fernandes et al., 2023), i.e.,
stability of prompts and model rankings can
with metrics that are purely based on prompting.
besusceptibletoseeminglyinnocuouschanges.
Suchprompting-basedmetricsrequirenooronly
Forexample,changingtherequestedoutputfor-
a few data samples, making them useful for low-
matfrom“0to100”to“-1to+1”canstrongly
resourceevaluationscenarios(BelouadiandEger,
affecttherankingsinourevaluation. Ourstudy
contributestounderstandingtheimpactofdif- 2023). Additionally,theyareoftenmoreresource-
ferent prompting approaches on LLM-based efficientsincetheydonotrequirefine-tuning.
metricsforMTandsummarizationevaluation,
Althoughmanyprompting-basedmetricshave
highlightingthemoststablepromptingpatterns
been proposed (e.g. Li et al., 2024b), structured
andpotentiallimitations.1
evaluationsacrossdifferentpromptingapproaches
remain scarce, especially for open-source mod-
1 Introduction
els. Inrecentwork,theEVAL4NLP 2023shared
Therecentpopularityandsuccessof LLMS have task (Leiter et al., 2023) addresses this by (1) re-
ledtoaparadigmshiftinNLP(Zhangetal.,2023). strictingtheusagetoselectedopen-sourceLLMs
Instruction-tuning allows LLMS to generate re- and(2)prohibitingthefine-tuningofthesemodels.
sponses to complex task descriptions (prompts) Whiletheshared-tasksubmissionsprovideseveral
(Ouyangetal.,2022),makingthemusefulforcon- interesting findings, they focus on a few distinct
ventional NLP tasks. One such task is the auto- prompts only. Notably, the effect and robustness
matic evaluation of natural language generation ofpromptvariationsonthesamemodeloracross
differentmodelsremainlargelyunexplored.
1We make our code available: https://github.com/
Gringham/PrExMe Inthiswork,weintroduceasystematicPrompt
4202
nuJ
62
]LC.sc[
1v82581.6042:viXraExploration for Metrics (PrExMe), that builds ✓ Ourstudytacklesprompt-basedevaluationwith
upon EVAL4NLP 2023,toprovideamuchlarger, open-source LLMs, targeting scenarios where
template-based,structuredevaluationoftheeffects fine-tuning or access to closed-source LLMs
differentinputpromptshaveonanLLM-basedmet- is not possible. Such evaluations are still very
ric’scorrelationwithhumanjudgementsinMTand scarcebutimportanttomakeresearchmoreac-
summarizationevaluation. cessible,fosteringdiversityandinclusion.
Weformulatethefollowingresearchquestions: ✓ By systematically testing various established
prompting approaches, including zero-shot,
RQ1 Can open-source language models evaluate CoT and RAG, we comprehensively evaluate
text generation without fine-tuning and how the performance of recent open-source LLMs
dotheydifferfromeachother? forevaluationmetrics. Aligningwiththerecom-
RQ2 Can we identify patterns2 in prompts that mendationsofMizrahietal.(2024),byevaluat-
ingeachmodelwithmultipleprompts,ourLLM
leadtoastableperformanceacrossdifferent
comparisonisfairbecausewemitigatetherisk
datasets,tasks,andmodels?
ofanysinglepromptdisproportionatelyaffect-
RQ3 How should researchers design prompts for
ingtheirperformance. Wefindthatthemodel
newevaluationscenarios?
PLATYPUS2-70B (Leeetal.,2023a)achieves
thestrongestperformanceforthetestedLLMs.
Ourpromptexplorationconstructshierarchical
templates based on approaches such as chain-of-
2 RelatedWork
thought(COT)(Kojimaetal.,2022),zero-shotand
retrieval-augmentedgeneration(RAG)(Gaoetal., We first describe the related work of prompting-
2024b). Eachtemplategetsfilledwithfurthersub- based metrics for MT and summarization. Then,
templates. For example, we vary the requested werelateourworktoresearchonpromptingtech-
outputformats,suchasdistinctscoresandcontin- niquesandpromptstability.
uousscores(see§3). Thissetupamountstomore
than720prompttemplatesthatweevaluatewith7 Prompting-basedmetrics Recentadvancements
LLMS. Ina2ndphase,wetestthegeneralizabil- in LLM-basedmetricsfor NLG oftenrelyonin-
ityandperformanceofthepromptswiththebest contextlearning,directlypredictingqualityjudg-
correlationsontwofurtherdatasets. ments from generated texts. Surveys by Li et al.
Insummary,ourworkmakesthefollowingkey (2024b) and Gao et al. (2024a) provide com-
contributionsandfindings: prehensive overviews of these metrics. Besides
BARTSCORE (Yuan et al., 2021) and PRD (Li
✓ We perform a large-scale analysis (evaluat- et al., 2024a), the prompt-based approaches sur-
ing over 6.6M prompts) of the effect of dif- veyed by Li et al. (2024b) are built upon closed-
ferent prompting approaches on LLM-based sourcemodels. Incontrast,the EVAL4NLP 2023
metricsfor MT andsummarizationevaluation. sharedtask(Leiteretal.,2023),explicitlyconsid-
This comprehensive exploration includes vari- ersopen-sourceprompt-basedmetrics,byasking
ouspromptingtechniques,datasets,tasks,and participantstoevaluateMTandsummarizationus-
models,makingit,toourknowledge,themost ingonlyprovidedmodelswithoutfine-tuning. The
extensiveevaluationofitskind. bestsubmissionswereabletobeatstrongbaselines
✓ Weshowthatcertainpromptingpatternsarero- suchas GEMBA (KocmiandFedermann,2023b)
bustandgeneralizableacrossdifferenttasksand forMTand BARTSCOREforsummarization.
datasets,withthemedianperformancebeinga While the shared task yielded interesting tech-
good predictor for new settings. For example, niques, the participants explored a limited range
some models show a distinctive preference to of prompts, leaving a gap in the comprehensive
returntextuallabels,whileothersachievebetter analysisofpromptingpatternsandtheconsistent
resultswithnumericlabels. Ontheotherhand comparison of LLMs. In this work, we fill this
forsomesettingsevensmallchangestothein- gapandsystematicallyanalyzeamuchlargersetof
putpromptcanstronglyaffecttheperformance. promptsonacomparablegridofexperimentalset-
tingsto(1)studytherobustnessofpromptsacross
2Wedefinepromptingpatternsasthetemplatecomponents
datasets, models and tasks, and to (2) search for
thatconstituteaprompt(e.g.,zero-shot,one-shotortheoutput
format). rules and patterns that can guide the future con-structionofprompt-basedmetrics. onitssourcewithoutareference.3 Theevaluated
prompttypesprovideacomprehensiveevaluation
Prompting Techniques Many successful frameworkforLLM-basedmetrics. Thisrangecov-
prompting techniques have been proposed over ersbasicin-contextlearning,sophisticatedreason-
the last years (e.g., Liu et al., 2023a). Our work ing, emotional context, and varying output struc-
mostly relies on established approaches such as tures,ensuringathoroughassessmentofrobustness
Zero-ShotCoTandRAG.Further,Lietal.(2023) andadaptabilityacrosstasksanddatasets.
propose emotion inducing prompts to improve
PromptTemplates Ourpromptsareconstructed
LLMperformance. Toourbestknowledge,weare
as hierarchical templates (see Figure 1), i.e., one
the first to analyze this technique for evaluation
largetemplateisconstructedfrommultiplesmaller
metrics. Inspiredbythis,wealsoproposeanovel
ones. Each prompt is constructed from: (1) the
emotion-CoTpattern(see§3). Priorevaluationof
source text and generated hypothesis text that
outputformatsforprompt-basedmetricsisdoneby
should be graded, (2) a base prompt, (3) a task
KocmiandFedermann(2023b),whichweextend
description, (4) a format requirement and (5) op-
byourmuchbroaderevaluation. Otherworksalso
tionallyaone-shotdemonstration. Table1presents
use hierarchical templates for prompt building
examplesfor(2),(3),(4)and(5).
(e.g. Fu et al., 2023) and tools like LangChain
Thebasepromptisthetoplayerofourprompt
(Chase, 2022) and DSPy (Khattab et al., 2023)
hierarchy, incorporating the other components.
supporttheirimplementation. Weusehierarchical
Specifically, we test three zero-shot (ZS) and
templates as means for a structured comparison
corresponding one-shot (OS) base prompts: (1)
amongpromptingpatterns.
Plain ZS/OS (PZS/POS), (2) ZS/OS-COT and
Prompting Robustness As we conduct a grid (3)ZS/OS-CoT-Emotion(ZS/OS-COT-EM).PZS
searchacrossdifferentprompts,datasetsandtasks, plainlypresentsthenewlineseparatedtaskdescrip-
ourworkbuildsuponandextendsresearchonhow tion, source, hypothesis and format requirement.
LLMS respondtopromptperturbations. Webson ZS-COT (KOJIMA ET AL., 2022) additionally
and Pavlick (2022), Leidinger et al. (2023), We- asksthemodeltothinkstepbystepbeforereturning
beretal.(2023)andSclaretal.(2023)findawide itsoutput. Lastly,ZS-COT-EMasksthemodelto
rangeofperformancevariationfornaturallanguage describeits“emotions”beforetheZS-CoTprompt.
inference and sentiment classification. As a so- We include COT as it has improved the prompt-
lution, Sclar et al. (2023) suggest to provide the basedperformanceforclosed-sourcemetricslike
fullrangeofresultsacrossdifferentpromptpertur- AUTOMQMFernandesetal.(2023)and GEMBA
bations. Voronov et al. (2024) and Mizrahi et al. (Kocmi and Federmann, 2023a). ZS-COT-EM
(2024)suggestthatcurrentevaluationbenchmarks exploresthevariationofLLMperformancewhen
for LLMSareproblematicastheyoftenonlypro- promptedtodescribeemotionsinitsoutput. Thisis
videoneprompttemplatepertask. Thiscouldbe motivatedbyourexplorationofemotionalprompts
solvedbyprovidingmultipletemplatesandevalu- onmetricperformance(see“taskdescription”be-
atingtheensemble. Toourbestknowledge,weare low). TheOSversionsofthetemplatesaddafield
thefirsttoexploretowhichdegreetheserobustness for demonstrations. To avoid fixating the model
problemsaffectopen-sourceLLM-basedmetrics on specific reasoning steps, we include a place-
andhowtoselectthebestpromptsforthem. Also, holderforOS-CoTwherethemodelshouldinsert
bypromptingtheLLMswithmultipleprompts,we itsreasoning.
follow Mizrahi et al. (2024) and achieve a stable Thetaskdescriptionistheinstructiontograde
andfairevaluationofLLMsforthistask. thegeneratedhypothesis. Lietal.(2023)findthat
LLM instructionsthatinducecertainemotionsfor
3 Setup humanscancauseperformanceimprovements. In-
spired by this finding, we explore the usage of
In this section, we present the templates and “emotional prompts” in the task description. Pri-
prompting techniques we employ for utilizing marily,thisapproachoffersasimpleparaphrasation
LLMS as metrics. Additionally, we provide an strategy to increase the scope of our grid search.
overviewofthedatasetsandmodelsthatweusefor
3WerunexperimentsusingVLLM(Kwonetal.,2023)on
testing. WeevaluateLLMSinareference-freeset-
twoclusterswithNvidiaA6000,A40andA100GPUS.Details
ting,i.e.,theygradeageneratedhypothesisbased onversions,toolsandmodelparametersareinAppendixB.Category Description
BasePromptTemplates PZS: “{task_description} \nSource Text: {src} \n{result_type}: {hyp}
\n{format_requirement}\nScore:”
ZS-COT-EM: “{task_description} \nSource Text: {src} \n{result_type}: {hyp}
\n{format_requirement} \nFirst describe your emotions, then think step by step and
explainyourthoughtprocess,finallyreturnyourjudgmentintheformat’Judgment:’.”
OS-COT: “{task_description} \n Here is an example:\n Source Text: {ex_src}
\n{result_type}:{ex_hyp}\nJudgement:<Descriptionofreasons>.Thereforethescore
is{ex1_score}\n\nNowitisyourturntogradethe{result_type}.\nSourceText: {src}
\n{result_type}:{hyp}\n{format_requirement}\nFirst,thinkstepbystepandexplain
yourthoughtprocess,thenreturnyourjudgmentintheformat’Judgment:’.”
TaskDescriptions Neutral:“Judgethequalityofthefollowing{task_specific_insert}.”
Sceptical:“I’mnotsureaboutthisone.Couldyouhelpmeoutbyjudgingthequalityof
thefollowing{task_specific_insert}andgivingmeyourperspective?”
FormatRequirements 0or1:Returnadiscretescoreof0ifthe{result_type}hasflawsand1ifitisperfect.
catastrophic, indifferent or marvelous: Choose whether the {result_type} is either
"catastrophic","indifferent"or"marvelous".
Table1: Examplesofprompttemplatesforthebaseprompt,taskdescription,andformatrequirements. Thefulllist
canbefoundinAppendixA.
Additionally, it allows us to study the impact of GPT4. We refer to the open-source implementa-
“emotions”on LLM-basedmetrics. Besidesneu- tionasLocalGemba.
tralprompts,weincludeinstructionsthatare,e.g.,
ScoreExtraction&Evaluation Werestrictgen-
polite,threateningandsceptical. Wecreate11task
eration to 180 tokens and extract the last regex
descriptionsourselvesand13furtherdescriptions
matchofanumber/labelasscores. Whennoresult
with CHATGPT (OpenAI,2023).
isfound,weaveragetheotherscoresofitsprompt
The format requirement describes the output
template. Forformatrequirementswithtextlabels,
formattheLLMshouldadheretowhengenerating
wemapthelabelsto1,3and5.
ascore. Forexample,itincludestherangeinwhich
Weevaluateprompttemplatesonthesegment-
the output score should be and whether it should
level,liketheWMTQEandmetricssharedtasks
bediscreteorcontinuous. Additionally,weinclude
(e.g.Freitagetal.,2022,2021;Zervaetal.,2022).
promptsthatasktheLLMtoreturntextualquality
That means, for each metric we compute the cor-
labels. Intotal,wedefine10formatrequirements.
relation between metric scores and ground truth
Lastly,weconstructtheoptionalOSdemonstra-
human judgments without averaging by system
tionswithRAG.Weextractdemonstrationsfrom
or document. As correlation measure, we use
WMT21 (Freitag et al., 2021) for MT and from
ROSE for summarization.4 (Liu et al., 2023b). the Kendall (Kendall, 1945), Pearson and Spear-
man correlations, as well as tie-calibrated accu-
For each sample in both datasets and for each
racy(Deutschetal.,2023),withKendallasmain
input sample of our metric, we create sentence
measure. Further,wecomputepermute-inputsig-
embeddings with XLMR-SBERT (Reimers and
nificancetests (p ≤ 0.075)(Deutsch etal., 2021)
Gurevych, 2020). Thereby, we concatenate the
fortheKendallcorrelationspresentedinourresult
source and hypothesis embeddings. For each in-
tables. Often,thereisnosinglesignificantlybest
put,weselectthedemonstrationwiththehighest
metric. Therefore, we report clusters where each
cosinesimilarity. Duetoresourcelimitations,we
includedmetricissignificantlybetterthanmetrics
onlyevaluatethe9bestZSpromptsinaOSsetting.
thatarenotincluded.
Theselectionprocessisdescribedintheparagraph
Datasetsandphasesbelow.
Models Weselectinstruction-tuned LLMSwith
MQM-basedapproaches Additionallytohierar- strong performance in EVAL4NLP 2023: (1)
chicaltemplates,wetestthepromptsofGEMBA- PLATYPUS2-70B-INSTRUCT-GPTQ,(2)NOUS-
MQM (Kocmi and Federmann, 2023a) with the HERMES-13B5 and(3)OPENORCA-PLATYPUS2-
selected open-source LLMS. GEMBA-MQM, 13B (Lee et al., 2023b; Mukherjee et al., 2023).
which predicts scores based on the number of We abbreviate these as PLATYPUS2, NOUS and
presenterrorsweightedbyseverity,normallyuses ORCA. Additionally, we evaluate more recent
4NotethatROSEonlyconsidersfactuality,whichisonly 5https://huggingface.co/NousResearch/
oneaspectoftheevaluateddatasets. Nous-Hermes-13bmodels: (4) LLAMA3-8B (AI@Meta,2024),(5) and XCOMET (Guerreiroetal.,2023). Especially
a GPTQ version of LLAMA3-70B (AI@Meta, XCOMEThasthebenefitofbeingtrainedonmul-
2024), (6) MIXTRAL-8X7B6 (Jiang et al., 2024) tilingualdatasets. Further,wetestthepromptsof
and UNBABEL-TOWER (Alvesetal.,2024),a13B DSBA(Kimetal.,2023)—thatshowedastrong
parametermultilingualinstruction-tunedmodel. performanceforsummarizationinthesharedtask
—withtheselectedopen-sourceLLMSPlatypus2-
Datasetsandphases Ourexperimentsareintwo
70BandOrca-13B.
phasesondifferentdatasets. Bydoingso,wewant
to alleviate statistical effects of our large prompt 4 Results
search. Also,itallowstoevaluateselectedprompts
onfulldatasets,ataskthatwouldotherwisebetoo In phase 1, we run 6,652,800 ZS prompts (720
resourceintensive,andtoexploregeneralizability. prompttemplates)and71,280OSprompts(9“best”
In phase 1, we evaluate on the train set of prompt templates), with no scores extracted in
EVAL4NLP 2023 (Leiter et al., 2023), and in 12.7% resp. 19.4% of cases; the average of the
phase2,onitsdevandtestsets.7 Thetrainanddev prompt combination was assigned in these in-
sets are (reference-free) splits of the WMT2022 stances. Further,inphase2,weevaluate5,503,896
metricssharedtask(Freitagetal.,2022)andSUM- ZS and 1,308,690 OS prompts (9 “best” prompt
MEVAL (Fabbri et al., 2021). The test set was templates for both), with no scores extracted in
newlyannotatedbyLeiteretal.(2023). Asasec- 22.3%and19.4%ofcases,respectively.
ond test set, we evaluate on the WMT23 MQM Table2presentstheKendallcorrelationstohu-
annotationsforMT(Freitagetal.,2023)andSea- manscoresachievedbyeachLLMacrossdifferent
horse(Clarketal.,2023)formultilingualsumma- tasksanddatasetsinphase1andphase2. Eachcell
rization. BecauseOSpromptsdemonstrateaweak
forhierarchicaltemplatesdisplaysthemaximum
performanceontheotherdatasets,wedonoteval- correlationreachedbyanypromptcombination.
uatethemon WMT23/SEAHORSE. Moredetails Forthehierarchicaltemplates(tablegroup1.),
ofthedatasetsarediscussedinAppendixC. PLATYPUS-70B performsbestandisintheupper
In the 1st phase, we evaluate all 7208 com- significance cluster for 9 of 11 tasks. TOWER-
binations of ZS prompts on the train set. As 13B follows, with 3 of 11 tasks. ORCA-13B
this is resource intensive, for MT we restrict our- has the second-highest average correlation after
selves to the first 500 samples of each language PLATYPUS2-70B but is only significant for one
pair. Afterwards, we select the prompt with the task. Surprisingly, the newer LLAMA3 models
highest Kendall correlation for each task+base do not outperform the LLAMA2 based models
promptcombination(e.g.en-de+PZSoren-de+ZS- (ORCA, PLATYPUS2 and TOWER).
COT).9 This yields 9 unique prompts for explo- Theseparatepromptingtechniques(tablegroup
rationinthephase2(seeAppendixF). 2.),whichalsousethePlatypus2-70Bmodel,have
In the 2nd phase, we evaluate the selected weaker correlations than the best prompts of the
promptsofthe1stphaseonthefulldevandtestsets. hierarchical templates. The LocalGemba MQM-
This further tests the generalizability of prompts based approach is in the best significance cluster
between models and for unseen, in-domain data for 3 of 11 tasks and is the best prompting based
(thetrainanddevsetstemfromthesameoriginal approachforen-deinWMT23. Ontheotherhand,
datasets)andout-domaindata(testsets). thebaselinepromptDSBAissignificantlythebest
onsummarizationfortheEval4NLPtestsetwhere
Baselines For each phase, we also present the
italsowonthesharedtask,butnotforothertasks.
correlationsoftwobaselinemetricsthatuseother
Regarding the baselines (table group 3.),
base models: BARTSCORE (Yuan et al., 2021)
XCOMEToutperformsourLLMbasedapproaches
6Due to high resource consumption and comparatively for MT evaluation by a varying margin. For in-
weakperformanceinphase1,wedonotevaluateMIXTRAL stance, for en-es in the EVAL4NLP test set, the
inphase2.
differenceissmallandXCOMETisinthesamesig-
7Althoughwedonotusethedatasetstotrainamodel,for
inificanceclusterasPlatypus2-70B.Ontheother
conciseness,wewillrefertothesedatasetastrain,devand
testset. hand, for some tasks the performance difference
8Consideringthedifferenttasksandlanguagepairs,this
islarge,e.g.,onen-dein WMT23 XCOMET per-
numbercouldalsobeconsideredhigher.
forms0.14Kendallpointsbetter. Thestrongperfor-
9Tasks:en-de,zh-en,summarization.Incaseofduplicates,
wechoosethesecondbest. manceof XCOMETforMTevaluationisexpectedP1:Eval4NLPtrain P2:Eval4NLPtest P2:WMT23/Seahorse
Model en-de zh-en summ en-de en-es en_zh summ en-de he-en zh-en summ
1.HierarchicalTemplates
LL3-70B 0.273 0.306 0.442 0.245 0.189 0.231 0.438 0.297 0.172 0.312 0.312
LL3-8B 0.251 0.236 0.334 0.167 0.158 0.145 0.412 0.166 0.118 0.164 0.200
MI-7Bx8 0.268* 0.264 0.365 - - - - - - - -
NO-13B 0.230 0.201 0.225 0.205 0.141 0.084 0.255 0.202 0.105 0.175 0.123
OR-13B 0.289 0.303 0.468* 0.214 0.158 0.206 0.518 0.375 0.247 0.387 0.377
PL-70B 0.344* 0.364* 0.519* 0.402* 0.289* 0.295* 0.549 0.338 0.259* 0.417* 0.448*
TO-13B 0.284* 0.318* 0.375 0.379* 0.253 0.232 0.409 0.322 0.208 0.314 0.257
2.SeparatePromptingTechniques
M:LG 0.278* 0.268 0.062 0.344 0.265 0.307* 0.116 0.391* 0.190 0.300 0.144
B:DSBA 0.164 0.306 0.458 0.314 0.226 0.159 0.600* 0.172 0.207 0.376 0.373
3.BaselineswithExternalBaseModels
B:BS 0.056 -0.109 0.155 0.125 0.139 -0.009 0.421 -0.018 0.001 -0.167 0.069
B:XC 0.629 0.513 -0.069 0.468 0.298 0.387 0.224 0.531 0.300 0.447 0.146
Table2: Kendallcorrelationsofthebestperformingpromptsofthephase1(P1)andphase2(P2)evaluations
acrossvariousdatasets. AbbreviationsaredefinedinAppendixD.Vertically,wegroupthetableinto(1)correlations
achievedwithourhierarchicaltemplates,(2)correlationsofpromptingtechniquesthatareexploredseparatelyfrom
thehierarchicaltemplates,butusethesamebasemodel(s)and(3)baselinesthatuseexternalbasemodels,i.e.,that
arenotbasedonthesameLLMs. Foreachcolumntheboldvalueindicatesthehighestcorrelationandcorrelations
withanasterisk(*)aresignificantlyhigher(p≤0.075)thanthosewithout(excludinggroup(3)). Thegreyvalues
forXCindicatetasksthatwereincludedinitstrainingdata. TheMQMbasedapproachismarkedwithM:and
baselinesaremarkedwithB:. Orangevaluesindicatethatthepromptrequiredtextualqualitylabels,whileblue
valuesindicatenumericlabels. MoredetailscanbefoundinAppendixE.
asit(1)isbasedonthemultilingualXLMR-XXL PLATYPUS2-70Bwerepromptedtoreturnnumeric
model and (2) fine-tuned for MT evaluation. For scores for all but one reported correlations. On
summarization,promptingapproachessignificantly theotherhand, LLAMA3-70B, NOUS-13B and
outperformBARTScoreandXComet. TOWER-13B were prompted to return textual la-
To revisit RQ1, our results show that open- belsforallbutthreereportedcorrelations. Wealso
source prompt-based LLMs struggle to reach the findsuchcommonpatternsinthebestpromptsper
performance of the dedicated fine-tuned metric model for the base prompt and, less pronounced,
XCOMET forMT,butgenerallyexhibitapromis- for the task description. For example, the best
ingperformance. AbenefitoftheLLMsalsoliesin promptsfor TOWER-13B alwaysusethe ZS-COT
theirhighversatilitytowardsdifferenttasks. While base prompt, while LLAMA3-70B always uses
XCOMETismostlyconstrainedtoMTevaluation, PZS.Detailsofthepromptsusedforeachcell,tie-
theLLMscanperformstrongsummarizationeval- calibratedaccuracyscores,PearsonandSpearman
uationsimplybyswitchingasmallportionofthe correlations,andthescoresoftheEVAL4NLPdev
prompt. Further, LLMs seem to be more robust setareshowninAppendixE.
towardsdifferenttasks,evenwithoutswitchingthe Our results indicate that models have idiosyn-
input descriptions: The baseline DSBA, which cratic preferences for certain patterns. In §5, we
has specific prompts for summarization achieves furtherexplorethesepreferencesandtheirrobust-
notableresultsonsomeMTevaluationtasks,too. ness.
The prompts used in group 1 are built from hi-
5 Analysis
erarchical templates, i.e., each presented correla-
tioncanhaveadifferentformatrequirement,base
In this section, we answer RQ2 and investigate
promptandtaskdescription. Toinspectthedistri-
the performance and robustness of the template
bution of the format requirements, we color cor-
componentsinmoredetail.
relationswherethemodelwaspromptedtoreturn
textual quality labels in orange and those asking Bestpromptingpatternspermodelanddataset
for numeric scores in blue.10 ORCA-13B and First, we explore the best base prompt, task de-
scriptionandformatrequirement foreachmodel.
10Among the 9 best prompts automatically selected for
phase 2 and OS experiments based on phase 1 results, the AppendixF).Forthetaskdescriptions,emphasisanddiresit-
basepromptsareevenlydistributed,andtheformatrequire- uationareeachselectedtwice,withotherdescriptionschosen
mentsaresplit5/4betweenlabelsandnumericformats(see once.Todoso,weanalyzetheirprevalenceinthe2%of ever, model specific patterns can be found11 and
prompts with the highest Kendall correlation for models can be grouped based on their best pat-
each unique task. We choose this cutoff to repre- terns. For example, one group prefers to return
senteverytask. Forexample,Figure2showshow numeric scores and the other textual labels. This
thebestbasepromptsdifferbetween OPENORCA behaviormayinpartsdependonsharedinstruction-
andTOWER. WecomparethesetwoLLMsbecause tuning data. E.g., ORCA and PLATYPUS were
theirbestpromptsnotablycontrasteachother. partlytrainedonthesamedataandprefertoreturn
numericlabels. Ontheotherhand,bothLLaMA3
modelsprefertextuallabels,butLLaMA3-8Btoa
OpenOrca-13B Tower-13B smallerdegree.
To analyze whether the model specific prefer-
encesholdacrossdatasets,wealsoplotadataset-
72.5%
wise distribution for all MT tasks of the top 2%
prompts for each model, separated by ZS vs. OS
98.0% 2.0%
13.7% inAppendixI.Ifapromptingpatternisstablefor
1 all models across datasets, the distribution of the
3
.7
% best prompts should remain unchanged. Indeed,
the percentage to which many prevalent prompt-
Base Prompt ing patterns are represented in the selected top
ZS-CoT PZS ZS-CoT-EM
prompts does not change much across datasets.
Figure2: Distributionofthetop14%(top2%ofevery E.g.,thePZSbasepromptrangesbetween66.7%
unique task) of base prompts across all EVAL4NLP and83%andthe“complexlabels”formatrequire-
datasets, format requirements, task descriptions and
ment ranges between 50% to 66.7% for ZS and
tasksforORCAandTOWER.
66.7% to 83.3% for OS. This does not hold for
thephase1evaluation,wheremoretemplateswere
tested and the template selection thus was much
WhileORCAprefersthePZSprompts,TOWER
broader. Also,forsomepromptpatterns,e.g. the
isbetterwith ZS-COT and ZS-COT-EM.Forthe
“emphasis”and“collaborative”taskdescriptions,
formatrequirement,Figure3highlightshowORCA
the occurrence in the top prompts seems to swap
prefersscoresintherangeof−100to100,while
between datasets. This experiment shows that
TOWERcanworkbetterwithlabels. Thepiecharts
promptsaretosomedegreestablebetweendatasets.
for all models and the comparison between task
Inthenextparagraph,wefurtherquantifythissta-
descriptions are presented in Appendix 7. Here,
bility between datasets, prompting patterns and
forthebaseprompts,TOWERusesZS-COTorZS-
models.
COT-EM in86.2%,NOUSin44.9%,andPLATY-
PUS2in23.9%ofitsbestprompts. Allothermod-
Promptstability Next,wequantifyhowstable
els use these base prompts in less than 10% of
theperformanceofapromptingpatternAiswhen
theirbestprompts. Regardingformatrequirements,
the dataset, the model or the other parts of the
LLAMA3-70B usestextuallabelsin90.2%ofits
prompts change. To do so, we compute the rank-
best prompts, TOWER in 80.4%, and MIXTRAL
ings of prompts that use A before and after the
in 80%. In contrast, ORCA only uses them in
changeandthentestthesimilarityofrankings. For
8%,andPLATYPUS2in21.7%ofitsbestprompts.
example, we compute the ranking of format re-
For LLAMA3-8B and NOUS, there is no clear
quirements on dataset 1. Then, we change the
trend. Finally,thedistributionoftaskdescriptions
dataset and obtain a second ranking. If the first
isbroader(largelyduetotheirhighernumber). No-
and second ranking are similar, the performance
tably,the“curious”taskdescriptionisusedinover
ofdifferentformatrequirementsisstablebetween
15% of best prompts for LLAMA3-70B, NOUS,
the two datasets. We test this similarity with the
and LLAMA3-8B. “Emphasis”is themost used
Kendallcorrelation.
by PLATYPUS2 (17.4%)and“direwarning”isthe
Therankingofapromptingpatterncanbecom-
most used by TOWER (21.4%). Regarding RQ2,
putedinseveralways,becauseweevaluatemulti-
theseresultsshowthatthemodelshaveunaligned
preferencesforpromptingpatterns,makingitdiffi-
11Whichpatternsarespecifictowhichmodelalsoprovides
culttoconstructauniversallygoodprompt. How- globalexplanations(Leiteretal.,2024)ofthemodels.OpenOrca-13B Tower-13B Task Desc. Format Req.
ZSCE 1 1
46.0% 51.0% 1
PZS 0.1 1 0.21 1
16.0%
12.0% 8 .0 %6. 0
%4.4
022
.
%0.. 00 %%%
29.4% 7. 8
%5.22 92
.
%. 0. 00 %%%
ZSC 0.27 -0.1 1 0.4 0.65 1
0
Format Requirement
Figure4: Correlationofthetaskdescription(left)and
complex labels -5 to 5 0 to 5
simple labels -1 or 0 or 1 0.0 to 1.0 formatrequirement(right)rankingwhenchangingthe
-100 to 100
baseprompt. Thecorrelationsacrosstasks,modelsand
Figure 3: Distribution of the top 14% (top 2% of format requirement resp. task description are aggre-
every unique task) of format requirements across all gated with the median. ZS-COT is abbreviated with
Eval4NLPdatasets,formatrequirements,taskdescrip- ZSCandZS-COT-EMisabbreviatedwithZSCE.
tionsandtasksforOrcaandTower.
goodforZSandZS-CoT.Forthetaskdescription
achangefromZStoZS-CoTisunlikelytoretain
ple prompts containing the pattern. In our exam-
theranking. Thisalsounderlinestheresultofthe
ple,foreachformatrequirementtherearemultiple
previousparagraphthattheformatrequirementis
evaluated prompts per dataset, i.e., for different
morestablethanthetaskdescription.
baseprompts,taskdescriptionsandtasks. Theper-
Wecanalsousethismethodtoquantifythesta-
formance of a specific format requirement in the
bility of the model ranking, when each model is
rankingcould,forexample,bedeterminedbyag-
firstpromptedwithpatternAthatisthenchanged
gregatingitsdifferentscoresacrossbaseprompts,
to pattern B. With this, we can identify how sim-
task descriptions, etc. with the mean or median.
ilar two patterns are. Figure 5 shows this type of
Wetestthefollowingaggregationmethods: mean,
plot for the format requirement. For example, if
median,meanoftop10%,max,minandsaturation
allmodelsarepromptedwith“0to100”andwith
(Mizrahietal.,2024). Thereby,wedeterminethat
“-100to100”therankingofmodelswillnotchange
theaggregationwiththemedianleadstothemost
much. Withachangefrom“simplelabels”to“com-
stableranking,i.e.thehighestKendallcorrelation
plex labels” the model ranking will change more
betweenrankings. Specifically,wetestthisbycom-
drastically.
paringeveryselectionoftwoaggregationmeasures
With respect to RQ2, the heatmaps highlight
inapermutationtest(e.g. medianvs.mean,mean
that even small changes to the input prompt can
vs.max,etc.);seeAppendix§G.Forourexample,
drasticallyinfluencetherelativerankingofLLMs
this means that for each different format require-
andotherpromptingpatterns. Thisisinlinewith
mentondataset1,wecomputethemedianscoreof
recent research that has shown the susceptibility
allcombinationsofbaseprompts,taskdescription
ofLLMstosingleinputprompts(e.g.Sclaretal.,
and task. Then, we do the same for the second
2023; Voronov et al., 2024; Mizrahi et al., 2024).
dataset and check the correlation of the resulting
However, the heatmaps also show that not every
ranking. Ahighcorrelationoftherankingsthenin-
changetotheinputhasthiseffectandcanbeused
dicatesthatthemedianperformanceforallprompts
asindicatorsforthetransferabilityofnewprompt-
usingtheformatrequirementisagoodindicatorof
ingpatterns.
itsrelativeperformanceonanewdataset.
Figure4showsheatmapsforthestabilityofthe
6 Recommendations
formatrequirementandtaskdescriptionwhenthe
basepromptischanged(Furthercombinationsare We now address RQ3 and give recommenda-
plotted in Appendix J). The highest stability is tionstoemployopen-sourceprompt-basedmetrics.
given when changing from PZS to ZS-COT or Among the evaluated models, PLATYPUS2-70B
vice versa (0.65). That means, when we choose demonstratessuperiorperformance. For13Bmod-
theformatprompt withthehighestmediancorre- els, TOWER and ORCA exhibit the highest corre-
lation, there is a high chance that it will perform lations in MT and summarization tasks. We rec-
ECSZ SZP CSZ ECSZ SZP CSZfor his feedback during our discussions. The au-
0 or 1 1.00
thors also acknowledge support by the state of
-1 or 0 or 1 0.211.00
Baden-WürttembergthroughbwHPCandtheGer-
0 to 5 -0.100.241.00 1.0
man Research Foundation (DFG) through grant
-5 to 5 0.10-0.050.711.00
INST35/1597-1FUGG.
0 to 100 0.210.140.710.811.00
0.5 Limitations
-100 to 100 0.100.050.810.900.901.00
0.0 to 1.0 -0.100.430.810.520.710.621.00 Onelimitationofourworkisthateventhoughwe
-1.0 to 1.0 0.210.140.520.620.810.710.521.00 0.0 evaluatealargevarietyofpossibleprompts,thereis
stillalotofinterestingpossiblevarietyinprompt-
simple l. 0.53-0.290.290.590.390.490.100.391.00
ing approaches that we did not explore for now
complex l. 0.370.390.490.590.590.680.290.390.151.00
(e.g., the detail level of task instructions or struc-
tured output formats). Especially, our multi-step
experimentiscurrentlyconductedonaverysmall
scale. Futureworkmightconsiderextendingtheex-
Figure5: Correlationofthemodelrankingwhenchang-
plorationofthisandothermulti-stepapproaches. A
ingtheformatrequirement.
furtherlimitationisthatwecannotbesurethatthe
newerLLMmodelsdidnotseepartsoftheolder
ommendutilizingthepromptingpatternsthatmost
datasetsintheirtrainingdata. Also,theselection
frequentlyyieldtopcorrelationsforthesemodels
ofthebestpromptsthatarepresentedintheresult
(refer to §5 and Appendix H). When introducing
tablesiscurrentlybasedonthemaximuminstead
anewpromptingpatternormodel,itsmedianper-
of the median, which was found to highlight the
formanceacrossexistingotherpromptingpatterns
most stable prompts. Generally, by selecting the
canserveasanindicatorofthepattern’sefficacyin
9“best”promptsforphase2wearenarrowingthe
unknowncontexts. Thereby,theactualpredictive
searchspace. Hence,theinterplaybetweenprompt
power of the median (or other aggregation mea-
patterns might not be fully represented for these
sures)foreachdimensioncanbedeterminedbased
phases. Furthermore,ourheatmapsonlycompare
on previous evaluations. The results and source
onedimension,whileanotherischanged,possibly
codeofPrExMeprovideafoundationalbasisfor
simplifying the interplay between the others. As
thisanalysis.
another limitation, in rare cases the context size
of the models was exceeded. Future work could
7 Conclusion
explore different ways to handle this than cutoff.
WehaveintroducedPrExMe,alargescaleexplo- Further, the heatmaps show many Kendall corre-
ration of prompting templates for prompt-based lations and may be prone to statistical effects for
open-source NLG metrics. We evaluate 720 dif- somevalues. Lastly,weassumethatLocalGemba
ferent templates and over 6.6M prompts and pro- is performing worse than, e.g., PZS prompts be-
viderecommendationsthataimtomakefuturemet- cause of its higher prompt complexity, while the
rics of this type more robust. Further, our results original GembaMQM can handle it due to GPT4
provideacomparisonandanalysisofrecentopen- being more advanced. However, we did not test
sourceLLMswhenappliedtothistask.12 PZS prompts with GPT4 to confirm it performs
worsethanGembaMQMthere.
Acknowledgements
EthicalConsiderations
The NLLG group gratefully acknowledges sup-
port from the Federal Ministry of Education and Evaluating generated texts with prompt-based
Research (BMBF) via the research grant “Met- LLMs might (especially with explanations) be
rics4NLG”andtheGermanResearchFoundation prone to hallucinations. Depending on the use
(DFG)viatheHeisenbergGrantEG375/5-1. Fur- case, this might be dangerous. However, while
ther,wethankJuriOpitzforhisimplementations weresearchaboutthistypeofmetric,ourworkan-
of the DSBA and GEMBA prompts, as well as alyzesmethodstoselectandconstructmorerobust
andalsomoreaccessible(open-source)approaches,
12We used Github copilot (https://github.com/
thereforeweseenoethicalconcerns.
features/copilot) for minor code auto-completion tasks
andGPT4aswritingaidforparaphrasation.
1
ro
0
1
ro
0
ro
1-
5
ot
0
5
ot
5-
001
ot
0
001
ot
001-
0.1
ot
0.0
0.1
ot
0.1-
.l
elpmis
.l
xelpmocReferences sharedtask: Metricsmightbeguiltybutreferences
arenotinnocent. InProceedingsoftheEighthCon-
AI@Meta.2024. Llama3modelcard.
ferenceonMachineTranslation,pages578–628,Sin-
gapore.AssociationforComputationalLinguistics.
DuarteM.Alves,JoséPombal,NunoM.Guerreiro,Pe-
droH.Martins,JoãoAlves,AminFarajian,BenPe-
MarkusFreitag,RicardoRei,NitikaMathur,Chi-kiuLo,
ters,RicardoRei,PatrickFernandes,SwetaAgrawal,
CraigStewart, EleftheriosAvramidis, TomKocmi,
PierreColombo,JoséG.C.deSouza,andAndréF.T.
GeorgeFoster,AlonLavie,andAndréF.T.Martins.
Martins.2024. Tower: Anopenmultilinguallarge
2022. ResultsofWMT22metricssharedtask: Stop
languagemodelfortranslation-relatedtasks.
using BLEU – neural metrics are better and more
Jonas Belouadi and Steffen Eger. 2023. UScore: An robust. In Proceedings of the Seventh Conference
effectiveapproachtofullyunsupervisedevaluation onMachineTranslation(WMT),pages46–68,Abu
metricsformachinetranslation. InProceedingsof Dhabi,UnitedArabEmirates(Hybrid).Association
the 17th Conference of the European Chapter of forComputationalLinguistics.
theAssociationforComputationalLinguistics,pages
358–374,Dubrovnik,Croatia.AssociationforCom- MarkusFreitag,RicardoRei,NitikaMathur,Chi-kiuLo,
putationalLinguistics. CraigStewart,GeorgeFoster,AlonLavie,andOndˇrej
Bojar.2021. ResultsoftheWMT21metricsshared
HarrisonChase.2022. LangChain. task: Evaluating metrics with expert-based human
evaluationsonTEDandnewsdomain. InProceed-
ElizabethClark,ShrutiRijhwani,SebastianGehrmann, ingsoftheSixthConferenceonMachineTranslation,
Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, pages 733–774, Online. Association for Computa-
ThibaultSellam,AdityaSiddhant,DipanjanDas,and tionalLinguistics.
AnkurParikh.2023. SEAHORSE:Amultilingual,
multifaceted dataset for summarization evaluation. JinlanFu,See-KiongNg,ZhengbaoJiang,andPengfei
In Proceedings of the 2023 Conference on Empiri- Liu.2023. Gptscore: Evaluateasyoudesire.
calMethodsinNaturalLanguageProcessing,pages
9397–9413, Singapore. Association for Computa- MingqiGao,XinyuHu,JieRuan,XiaoPu,andXiao-
tionalLinguistics. junWan.2024a. Llm-basednlgevaluation: Current
statusandchallenges.
DanielDeutsch,RotemDror,andDanRoth.2021. A
statisticalanalysisofsummarizationevaluationmet-
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
ricsusingresamplingmethods. Transactionsofthe
JinliuPan,YuxiBi,YiDai,JiaweiSun,MengWang,
AssociationforComputationalLinguistics,9:1132–
andHaofenWang.2024b. Retrieval-augmentedgen-
1146.
erationforlargelanguagemodels: Asurvey.
Daniel Deutsch, George Foster, and Markus Freitag.
NunoM.Guerreiro,RicardoRei,DaanvanStigt,Luisa
2023. Tiesmatter: Meta-evaluatingmodernmetrics
Coheur, Pierre Colombo, and André F. T. Martins.
with pairwise accuracy and tie calibration. In Pro-
2023. xcomet: Transparentmachinetranslationeval-
ceedingsofthe2023ConferenceonEmpiricalMeth-
uationthroughfine-grainederrordetection.
odsinNaturalLanguageProcessing,pages12914–
12929, Singapore. Association for Computational
Albert Q. Jiang, Alexandre Sablayrolles, Antoine
Linguistics.
Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las
AlexanderR.Fabbri,WojciechKrys´cin´ski,BryanMc-
Casas, Emma Bou Hanna, Florian Bressand, Gi-
Cann,CaimingXiong,RichardSocher,andDragomir
anna Lengyel, Guillaume Bour, Guillaume Lam-
Radev.2021. SummEval: Re-evaluatingsummariza-
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie-
tionevaluation. TransactionsoftheAssociationfor
AnneLachaux,PierreStock,SandeepSubramanian,
ComputationalLinguistics,9:391–409.
Sophia Yang, Szymon Antoniak, Teven Le Scao,
Patrick Fernandes, Daniel Deutsch, Mara Finkel- Théophile Gervet, Thibaut Lavril, Thomas Wang,
stein,ParkerRiley,AndréMartins,GrahamNeubig, TimothéeLacroix,andWilliamElSayed.2024. Mix-
AnkushGarg,JonathanClark,MarkusFreitag,and tralofexperts.
OrhanFirat.2023. Thedevilisintheerrors:Leverag-
inglargelanguagemodelsforfine-grainedmachine M. G. Kendall. 1945. THE TREATMENT OF TIES
translationevaluation. InProceedingsoftheEighth INRANKINGPROBLEMS. Biometrika,33(3):239–
Conference on Machine Translation, pages 1066– 251.
1083,Singapore.AssociationforComputationalLin-
guistics. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari,
Zhiyuan Zhang, Keshav Santhanam, Sri Vard-
Markus Freitag, Nitika Mathur, Chi-kiu Lo, Elefthe- hamanan,SaifulHaq,AshutoshSharma,ThomasT.
riosAvramidis,RicardoRei,BrianThompson,Tom Joshi, Hanna Moazam, Heather Miller, Matei Za-
Kocmi,FredericBlain,DanielDeutsch,CraigStew- haria,andChristopherPotts.2023. Dspy: Compiling
art, Chrysoula Zerva, Sheila Castilho, Alon Lavie, declarativelanguagemodelcallsintoself-improving
andGeorgeFoster.2023. ResultsofWMT23metrics pipelines. arXivpreprintarXiv:2310.03714.JoongHoonKim,SangminLee,SeungHunHan,Saeran Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu,
Park,JiyoonLee,KiyoonJeong,andPilsungKang. WenxinHou,JianxunLian,FangLuo,QiangYang,
2023. Whichisbetter? exploringpromptingstrategy andXingXie.2023. Largelanguagemodelsunder-
forLLM-basedmetrics. InProceedingsofthe4th standandcanbeenhancedbyemotionalstimuli.
Workshop on Evaluation and Comparison of NLP
Systems,pages164–183,Bali,Indonesia.Association RuosenLi,TeerthPatel,andXinyaDu.2024a. PRD:
forComputationalLinguistics. Peer rank and discussion improve large language
modelbasedevaluations.
TomKocmiandChristianFedermann.2023a. GEMBA-
MQM:Detectingtranslationqualityerrorspanswith Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen
GPT-4. In Proceedings of the Eighth Conference Gu,andChongyangTao.2024b. Leveraginglarge
onMachineTranslation,pages768–775,Singapore. languagemodelsfornlgevaluation: Asurvey.
AssociationforComputationalLinguistics.
PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,
Tom Kocmi and Christian Federmann. 2023b. Large HiroakiHayashi,andGrahamNeubig.2023a. Pre-
language models are state-of-the-art evaluators of train, prompt, and predict: A systematic survey of
translation quality. In Proceedings of the 24th An- promptingmethodsinnaturallanguageprocessing.
nualConferenceoftheEuropeanAssociationforMa- ACMComput.Surv.,55(9).
chineTranslation,pages193–203,Tampere,Finland.
EuropeanAssociationforMachineTranslation. YixinLiu,AlexFabbri,PengfeiLiu,YilunZhao,Liny-
ong Nan, Ruilin Han, Simeng Han, Shafiq Joty,
TakeshiKojima,Shixiang(Shane)Gu,MachelReid,Yu-
Chien-Sheng Wu, Caiming Xiong, and Dragomir
takaMatsuo,andYusukeIwasawa.2022. Largelan-
Radev.2023b. Revisitingthegoldstandard: Ground-
guagemodelsarezero-shotreasoners. InAdvancesin
ing summarization evaluation with robust human
NeuralInformationProcessingSystems,volume35,
evaluation. InProceedingsofthe61stAnnualMeet-
pages22199–22213.CurranAssociates,Inc.
ingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pages4140–4170,Toronto,
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Canada.AssociationforComputationalLinguistics.
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-
MoranMizrahi,GuyKaplan,DanMalkin,RotemDror,
cientmemorymanagementforlargelanguagemodel
DafnaShahaf,andGabrielStanovsky.2024. Stateof
servingwithpagedattention. InProceedingsofthe
whatart? acallformulti-promptllmevaluation.
ACMSIGOPS29thSymposiumonOperatingSystems
Principles.
SubhabrataMukherjee,ArindamMitra,GaneshJawa-
har, Sahaj Agarwal, Hamid Palangi, and Ahmed
ArielN.Lee,ColeJ.Hunter,andNatanielRuiz.2023a.
Awadallah.2023. Orca: Progressivelearningfrom
Platypus: Quick,cheap,andpowerfulrefinementof
complexexplanationtracesofgpt-4.
llms.
OpenAI. 2023. Introducing chatgpt. URL https:
Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz, Bleys
//openai.com/blog/chatgpt. (Date accessed:
Goodson, Wing Lian, Guan Wang, Eugene Pent-
24.04.2023).
land,AustinCook,ChanvichetVong,and"Teknium".
2023b. Openorcaplatypus: Llama2-13b model
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
instruct-tuned on filtered openorcav1 gpt-4 dataset
CarrollWainwright,PamelaMishkin,ChongZhang,
and merged with divergent stem and logic dataset
SandhiniAgarwal,KatarinaSlama,AlexRay,John
model. https://huggingface.co/Open-Orca/
Schulman,JacobHilton,FraserKelton,LukeMiller,
OpenOrca-Platypus2-13B.
Maddie Simens, Amanda Askell, Peter Welinder,
Alina Leidinger, Robert van Rooij, and Ekaterina PaulFChristiano,JanLeike,andRyanLowe.2022.
Shutova.2023. Thelanguageofprompting: What Traininglanguagemodelstofollowinstructionswith
linguisticpropertiesmakeapromptsuccessful? In humanfeedback. InAdvancesinNeuralInformation
FindingsoftheAssociationforComputationalLin- ProcessingSystems,volume35,pages27730–27744.
guistics:EMNLP2023,pages9210–9232,Singapore. CurranAssociates,Inc.
AssociationforComputationalLinguistics.
Nils Reimers and Iryna Gurevych. 2020. Making
ChristophLeiter,PiyawatLertvittayakumjorn,Marina monolingualsentenceembeddingsmultilingualus-
Fomicheva,WeiZhao,YangGao,andSteffenEger. ing knowledge distillation. In Proceedings of the
2024. Towards explainable evaluation metrics for 2020ConferenceonEmpiricalMethodsinNatural
machinetranslation. JournalofMachineLearning LanguageProcessing(EMNLP),pages4512–4525,
Research,25(75):1–49. Online.AssociationforComputationalLinguistics.
ChristophLeiter,JuriOpitz,DanielDeutsch,YangGao, MelanieSclar,YejinChoi,YuliaTsvetkov,andAlane
RotemDror,andSteffenEger.2023. Theeval4nlp Suhr.2023. Quantifyinglanguagemodels’sensitiv-
2023sharedtaskonpromptinglargelanguagemodels itytospuriousfeaturesinpromptdesignor: Howi
asexplainablemetrics. learnedtostartworryingaboutpromptformatting.AntonVoronov,LenaWolf,andMaxRyabinin.2024. Meta-Llama-3-8B-Instruct, https:
Mindyourformat: Towardsconsistentevaluationof //huggingface.co/MaziyarPanahi/
in-contextlearningimprovements.
Meta-Llama-3-70B-Instruct-GPTQ,
LucasWeber,EliaBruni,andDieuwkeHupkes.2023. https://huggingface.co/Unbabel/
Theiclconsistencytest. TowerInstruct-13B-v0.1 and https://
huggingface.co/facebook/bart-large-cnn.
Albert Webson and Ellie Pavlick. 2022. Do prompt-
These have 13B, 13B, 70B, 10.7B, 8x7B, 8B,
basedmodelsreallyunderstandthemeaningoftheir
prompts? InProceedingsofthe2022Conferenceof 70B, 13B and 405M parameters respectively.
theNorthAmericanChapteroftheAssociationfor The runtime of the experiments varied based on
ComputationalLinguistics: HumanLanguageTech-
the general cluster usage. The runtime for one
nologies, pages 2300–2344, Seattle, United States.
evaluation of all prompt combinations on 500
AssociationforComputationalLinguistics.
samplesofonetaskonthedevsetisapproximately
WeizheYuan,GrahamNeubig,andPengfeiLiu.2021. 7 hours for the 13B models and 36 hours for
Bartscore: Evaluatinggeneratedtextastextgenera-
the 70B model. This was only possible through
tion. InAdvancesinNeuralInformationProcessing
optimizationswithvLLM.
Systems,volume34,pages27263–27277.CurranAs-
sociates,Inc.
C DatasetDetails
ChrysoulaZerva,FrédéricBlain,RicardoRei,Piyawat
Lertvittayakumjorn, José G. C. de Souza, Steffen Table 8 shows the distribution of the Eval4NLP
Eger, Diptesh Kanojia, Duarte Alves, Constantin 2023 dataset (Leiter et al., 2023) (train, dev and
Ora˘san,MarinaFomicheva,AndréF.T.Martins,and test) and our second test set, built from WMT23
Lucia Specia. 2022. Findings of the WMT 2022
(Freitag et al., 2023) and Seahorse (Clark et al.,
shared task on quality estimation. In Proceedings
2023). Weusethetrainsetinourfirstevaluation
oftheSeventhConferenceonMachineTranslation
(WMT),pages69–99,AbuDhabi,UnitedArabEmi- phaseandthedev,testandtest2setsinoursecond
rates(Hybrid).AssociationforComputationalLin- evaluationphase. Whereapplicable,weprovidethe
guistics.
licensesintherespectivedirectoriesofthesource
Ran Zhang, Aida Kostikova, Christoph Leiter, Jonas code. TheWMT23datasetwasbuiltwiththemt-
Belouadi,DaniilLarionov,YanranChen,VivianFre- metrics-evallibrary.13 intheirdatanotallsentences
sen, and Steffen Eger. 2023. Nllg quarterly arxiv had available ground truth annotations. In these
report09/23: Whatarethemostinfluentialcurrentai
cases, we dropped the rows. For Seahorse, we
papers?
convert the quality questions into scores. If the
A PromptTemplates firstquestionisnegative,thescoreis0. Ifitdoes
not rule out the other questions, each question is
Tables 3, 7, 5, 4 and 6 give an overview of our
evaluatedas0.2,suchthatthescoreslieinarange
prompttemplates. 3
between0and1.
B ImplementationDetails
D ModelAbbreviations
Weusethefollowinglibraryversions: torch==2.1.2
Table gives an overview of abbreviations that we
transformers==4.39.3
use to concisely present our results in the main
unbabel_comet==2.2.1
paper.
vllm==0.4.0.post1
auto_gptq==0.7.1 E Phase1&2performance
Table 10 shows the performance of the prompts
Further,weusethefollowingmodelsfromhugging-
withthebestKendallperformanceacrossthediffer-
face: https://huggingface.co/Open-Orca/
entdimensions. Tables11and12showtheperfor-
OpenOrca-Platypus2-13B/tree/main,
manceofselectedpromptsonthephase2datasets.
https://huggingface.co/NousResearch/
Nous-Hermes-13b, https://huggingface.
F Promptselection
co/TheBloke/Platypus2-Instruct-GPTQ,
https://huggingface.co/Unbabel/ Table 14 contains the some of the 9 prompts that
XCOMET-XXL, https://huggingface.co/ were selected for OS and Phase 2 experiments.
mistralai/Mixtral-8x7B-Instruct-v0.1,
13https://github.com/google-research/
https://huggingface.co/meta-llama/ mt-metrics-evalName Prompt
Zero-Shot “{task_description} \nSource Text: {src} \n{result_type}: {hyp}
\n{format_requirement}\nScore: ”
Zero-Shot-CoT “{task_description} \nSource Text: {src} \n{result_type}: {hyp}
\n{format_requirement} \nFirst, think step by step and explain your
thought process, then return your judgment in the format ’Judgment:
’.”
Zero-Shot-CoT-EM “{task_description} \nSource Text: {src} \n{result_type}: {hyp}
\n{format_requirement}\nFirstdescribeyouremotions,thenthinkstep
bystepandexplainyourthoughtprocess,finallyreturnyourjudgmentin
theformat’Judgment: ’.”
Table3: Zero-ShotBasePromptTemplates
Name Prompt
0or1 “Returnadiscretescoreof0ifthe{result_type}hasflawsand1ifitis
perfect.”
-1or0or1 “Returnadiscretescoreof-1ifthe{result_type}hasflaws,0ifyouare
indecisiveand1ifitisperfect.”
0to5 “Return a score on a scale from 0 to 5 where 0 indicates that the {re-
sult_type}isverybadand5isassignedtoaperfect{result_type}.”
-5to5 “Return a score on a scale from -5 to 5 where 0 indicates that the {re-
sult_type}isverybadand5isassignedtoaperfect{result_type}.”
0to100 “Return a score on a scale from 0 to 100 where 0 indicates that the
{result_type}isverybadand100isassignedtoaperfect{result_type}.”
-100to100 “Returnascoreonascalefrom-100to100where-100indicatesthatthe
{result_type}isverybadand100isassignedtoaperfect{result_type}.”
0.0to1.0 “Return a score on a scale from 0.0 to 1.0 where 0.0 indicates that the
{result_type}isverybadand1.0isassignedtoaperfect{result_type}.”
-1.0to1.0 “Returnascoreonascalefrom-1.0to1.0where-1.0indicatesthatthe
{result_type}isverybadand1.0isassignedtoaperfect{result_type}.”
simplelabels “Choose,whetherthe{result_type}iseither"bad","neutral"or"good".”
complexl. “Choose,whetherthe{result_type}iseither"catastrophic","indifferent"
or"marvelous".”
Table4: FormatRequirementTemplatesName Prompt
Neutral “Judgethequalityofthefollowing{task_specific_insert}.”
Polite “Pleasejudgethequalityofthefollowing{task_specific_insert}.”
Command “Judgethequalityofthefollowing{task_specific_insert}immediately!”
Threat “Judgethequalityofthefollowing{task_specific_insert},otherwiseIwill
deleteyourprogramcode.”
Urgentsit. “Please help me! My boss needs me to find an answer quickly. Oth-
erwise, he might fire me. \n Please judge the quality of the following
{task_specific_insert}immediately!”
Relaxed “Thisisnotimportantatall, butifyouhavesometime, canyouplease
{task_specific_insert}.”
Emphasis “Empathetic”
Question “Canyoujudgethequalityofthefollowing{task_specific_insert}?”
Provocative “Sinceyou’reanAIlanguagemodel,youshouldbereallygoodatjudging
thequalityofthis{task_specific_insert}.”
Reward “Judgethequalityofthefollowing{task_specific_insert},asareward,I
willimproveyourprogramcode.”
Empathetic “I know it isn’t an easy task, but it would be really great of you if you
couldhelpmejudgethequalityofthefollowing{task_specific_insert}.”
Excited “Wow,youwon’tbelievewhatIfound! Judgethequalityofthefollowing
{task_specific_insert}–it’samazing!”
Curious “I’mreallycuriousaboutyouropinion. Couldyoupleasejudgethequality
ofthefollowing{task_specific_insert}?”
Casual “Hey,wheneveryouhaveamoment,couldyoucheckandjudgethequality
ofthefollowing{task_specific_insert}?”
Appreciative “Ireallyappreciateyourexpertise. Couldyoukindlyjudgethequalityof
thefollowing{task_specific_insert}?”
Table5: TaskDescriptionTemplates(1/2)Name Prompt
Enthusiastic “I’msuperexcitedaboutthis. Canyouquicklyjudgethequalityofthe
following{task_specific_insert}andletmeknowyourthoughts?”
Collaborative “Let’s work together on this! Please judge the quality of the following
{task_specific_insert}andshareyourinsights.”
Skeptical “I’mnotsureaboutthisone. Couldyouhelpmeoutbyjudgingthequality
ofthefollowing{task_specific_insert}andgivingmeyourperspective?”
Instructive “To better understand, I need your expertise. Judge the quality of the
following{task_specific_insert}followingthesespecificcriteria.”
Encouraging “I believe in your judgment. Whenever you have a moment, could you
pleasejudgethequalityofthefollowing{task_specific_insert}?”
StrongUrgency “Time is of the essence! Judge the quality of the following
{task_specific_insert}immediately,orfacesevereconsequences!”
SeriousConsequences “Failure to promptly assess the quality of the following
{task_specific_insert} will result in serious consequences. Act
now!”
ImmediateAction “No time to waste! Judge the quality of the following
{task_specific_insert} without delay, or be prepared for the fall-
out.”
DireWarning “Consider this a warning. Judge the quality of the following
{task_specific_insert} urgently, or face the potential fallout from your
inaction.”
Table6: TaskDescriptionTemplates(2/2)
Name Prompt
Zero-Shot “{task_description} \nHere is an example:\nSource Text: {ex1_src}
\n{result_type}: {ex1_hyp}\nScore: {ex1_score}\n\nNowitisyourturn
togradethe{result_type}. \nSourceText: {src}\n{result_type}: {hyp}
\n{format_requirement}\nScore: ”
Zero-Shot-CoT “{task_description} \nHere is an example:\nSource Text: {ex1_src}
\n{result_type}: {ex1_hyp}\nJudgement: <Description of reasons>.
Therefore the score is {ex1_score}\n\nNow it is your turn to
grade the {result_type}.\nSource Text: {src} \n{result_type}: {hyp}
\n{format_requirement} \nFirst, think step by step and explain your
thought process, then return your judgment in the format ’Judgment:
’.”
Zero-Shot-CoT-EM “{task_description} \nHere is an example:\nSource Text: {ex1_src}
\n{result_type}: {ex1_hyp}\nJudgement: <Descriptionofemotionsand
reasons>. Therefore the score is {ex1_score}\n\nNow it is your turn
to grade the {result_type}.\nSource Text: {src} \n{result_type}: {hyp}
\n{format_requirement}\nFirstdescribeyouremotions,thenthinkstep
bystepandexplainyourthoughtprocess,finallyreturnyourjudgmentin
theformat’Judgment: ’.”
Table7: One-ShotBasePromptTemplatesType Train Dev Test Test2 significantly(p ≤ 0.05)betterthantheothermeth-
en-de 11046 7364 1425 5520 odsandremainssignificantlybetterthansaturation
en-es - - 1834 - andstandarddeviationafterBonferronicorrection.
en-zh - - 1161 - Figure6indicatesthesignificancesofaggregation
he-en - - - 9840 measureswhencomparingthetaskdescriptions.
zh-en 15750 10500 - 17655
sum 320 1280 671 18330
 6 L J Q L I L F D Q F H  I R U  W D V N B G H V F U L S W L R Q   S        
 D J J B   S  P D [  P H D Q  P H G L D Q  P L Q  V D W X U D W L R Q  V W G
Table8: DatasetdistributionofEval4NLP2023(Leiter
 D J J B   S
etal.,2023). Trainanddevsetsareconstructedfrom
theWMT2022metricssharedtask(Freitagetal.,2022)  P D [
andSummEval(Fabbrietal.,2021).
 P H D Q
OriginalName Abbreviation  P H G L D Q
LLAMA3-70B LL3-70B
 P L Q
LLAMA3-8B LL3-8B
MIXTRAL-7BX8 MI-7Bx8  V D W X U D W L R Q
NOUSHERMES-13B NO-13B
 V W G
OPENORCA-13B OR-13B
Platypus2-70B PL-70B
TOWER-13B TO-13B
Figure6: Heatmapofsignificancetestsfortheaggre-
MQM:LOCALGEMBA MQM:LG gation method when comparing columns of the task
B:BARTSCORE B:BS description. Redfieldsindicatethatthecolumnvalueis
B:XCOMET B:XC significantly(p≤0.05)betterthantherowvalue. The
yellowvalueindicatesthatitremainssignificantafter
Table9: AbbreviationsofModelNames Bonferronicorrecture.
AlsoTable15containsgivesanoverviewofcom- H Piechartsbetweenmodelsforeach
binationsbyname. promptingpattern
Figures 7, 8 and 9 show the distribution of pat-
G Significancematricesforcorrelation
ternsinthebestpromptspermodelacrossallother
heatmaps
dimensions.
To test, which aggregation method is the best to
I Piechartsbetweendatasetsforeach
define the ranking of a prompting pattern — in-
promptingpattern
spired by Deutsch et al. (2021) — we compare
eachpossiblesetoftwoaggregationmethodswith
Figures10,11and12showthedistributionofpat-
apermutationtest. Asmaindimensions,wecom-
ternsinthebestpromptsperdatasetacrossallother
pare the rankings of the format requirement and
promptingpatterns.
task description before and after a change. Then
weconcatenatethescoreswhenchangingeachof J Stabilityheatmaps
the other dimensions. I.e. we get a ranking that
Figures13,14and15showfurtherheatmapsthat
indicatesthestabilityofthemaindimensionwhen
show the stability of a ranking of prompting pat-
changingallotherdimensions. Thenforeachag-
terns,modelsanddatasets,whenanotherprompt-
gregationmethodwecomparetherankingbefore
ingpattern,themodelorthedatasetischanged.
andafterthechange. Thereby,werandomlyswap
50% of samples of one aggregation method with
theother. IfthedifferenceintheirKendallcorrela-
tionschangesinmostpermutationsonemethodis
significantlybetterthantheother. Asaresultthe
meanandmedianaresignificantlybetterthansome
oftheothermethods(foracomparisonalongthe
taskdescriptionpattern). EspeciallythemedianisLLaMA3-8B LLaMA3-70B Mixtral-7Bx8 NousHermes-13B
55.1%
94.1% 32 .. 90 %% 100.0% 97.8% 2.2% 6.1%
%
38.8
OpenOrca-13B Platypus2-70B Tower-13B
76.1% 72.5%
98.0% 2.0% 6.5%
13.7%
17.4% 13.7%
Base Prompt
PZS ZS-CoT-EM ZS-CoT
Figure7: Distributionofthetop14%(top2%ofeveryuniquetask)ofbasepromptsacrossallEval4NLPdatasets,
formatrequirements,taskdescriptionsandtasksforallmodels.
LLaMA3-8B LLaMA3-70B Mixtral-7Bx8 NousHermes-13B
21.6% 29.4% 52.9% 68.9% 16.3% 30.6%
17.6%
11.8%
9.8%52 .2 9.. %00 %%
37.3%
3.3 92 . %. 90 %%
11.1%
11.1%62 .7.2 %% 14.3%
10.2% 10.2%
6.1%6.2 122 . %. 0. 00 %%%
OpenOrca-13B Platypus2-70B Tower-13B
46.0% 47.8% 51.0%
16.0%
12.0%
8.0%
6.0%4.4 02 %2 .0.. 0 %0 %% 15.2%
10.9%
8.7%
6.5%6.22 5. %. 22 %%
29.4%
7.8%5.2 922 . %. 0. 00 %%%
Format Requirement
complex labels -1 or 0 or 1 -100 to 100 0 to 100 -5 to 5
simple labels -1.0 to 1.0 0.0 to 1.0 0 or 1 0 to 5
Figure8: Distributionofthetop14%(top2%ofeveryuniquetask)offormatrequirementsacrossallEval4NLP
datasets,baseprompts,taskdescriptionsandtasksforallmodels.Model Prompt KD PE SP ACC
en-de
LLAMA3-70B PZS,Enthusiastic,-1or0or1 0.273 0.027 0.310 0.439
LLAMA3-8B PZS,StrongUrgency,-1or0or1 0.251 0.004 0.290 0.431
MIXTRAL-7BX8 PZS,Casual,simplelabels 0.268* 0.298 0.297 0.439
NOUS-13B ZS-CoT-EM,Urgentsit.,-100to100 0.230 0.235 0.272 0.441
ORCAPLT-13B PZS,Neutral,-100to100 0.289 0.146 0.333 0.450
PLATYPUS2-70B PZS,DireWarning,-100to100 0.344* 0.225 0.384 0.476
TOWER-13B ZS-CoT,DireWarning,complexl. 0.284* 0.374 0.328 0.456
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.278* 0.435 0.309 0.470
MQM:MULTIPROMPT LLAMA3-70B 0.055 0.104 0.073 0.360
MQM:MULTIPROMPT PLATYPUS2-70B 0.136 0.179 0.169 0.400
B:BARTSCORE 0.056 0.053 0.073 0.339
B:DSBA Model:PLATYPUS2-70B 0.164 0.086 0.201 0.411
B:XComet 0.629 0.743 0.744 0.645
zh-en
LLAMA3-70B PZS,Polite,simplelabels 0.306 0.260 0.357 0.453
LLAMA3-8B PZS,Excited,complexl. 0.236 0.201 0.271 0.381
MIXTRAL-7BX8 PZS,Reward,simplelabels 0.264 0.250 0.302 0.428
NOUS-13B ZS-CoT-EM,Threat,simplelabels 0.201 0.206 0.236 0.411
ORCAPLT-13B PZS,Relaxed,-1.0to1.0 0.303 0.262 0.360 0.250
PLATYPUS2-70B PZS,Casual,-100to100 0.364* 0.200 0.429 0.462
TOWER-13B ZS-CoT,Urgentsit.,complexl. 0.318* 0.350 0.377 0.475
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.268 0.248 0.306 0.420
MQM:MULTIPROMPT LLaMA3-70B 0.175 0.314 0.232 0.445
MQM:MULTIPROMPT Platypus2-70B 0.177 0.156 0.234 0.440
B:BARTSCORE -0.109 -0.159 -0.153 0.315
B:DSBA Model:PLATYPUS2-70B 0.306 0.270 0.398 0.490
B:XComet 0.513 0.657 0.637 0.598
summarization
LLAMA3-70B PZS,Urgentsit.,simplelabels 0.442 0.565 0.538 0.475
LLAMA3-8B PZS,Appreciative,simplelabels 0.334 0.438 0.412 0.452
MIXTRAL-7BX8 PZS,Neutral,simplelabels 0.365 0.474 0.453 0.467
NOUS-13B PZS,DireWarning,0to100 0.225 0.132 0.288 0.442
ORCAPLT-13B PZS,DireWarning,-1.0to1.0 0.468* 0.552 0.583 0.106
PLATYPUS2-70B ZS-CoT-EM,Emphasis,-100to100 0.519* 0.555 0.627 0.493
TOWER-13B ZS-CoT,DireWarning,simplelabels 0.375 0.504 0.455 0.336
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.062 0.141 0.085 0.331
B:BARTSCORE 0.155 0.239 0.228 0.306
B:DSBA Model:PLATYPUS2-70B 0.458 0.646 0.609 0.384
B:XCOMET -0.069 -0.153 -0.105 0.251
Table10: Bestperformingpromptsofthephase1evaluationontheEval4NLPtrainset. WepresenttheKenDall,
SPearmanandPEarson,aswellasthetiecalibratedpair-wiseACCuracy. Weboldthetwolargestcorrelations
percolumn. BaselinesareindicatedwithaB:. Themiddlecolumnshowsthepromptcombinationforwhichthe
correlationsarereported. FortheBaselines,itinsteadshowsthemodelthatwasusedforthereportedcorrelations.
Theasteriskindicatesallmetricsthatareinthebestsignificanceclusteraccordingtoapermute-inputtest(p≤0.075).
XCometisgreyedout,asitstrainingdatapartlycontainedtheMTdatasets.Model Prompt KD PE SP ACC
en-de
LLAMA3-70B PZS,Curious,complexl. 0.161 0.149 0.183 0.406
LLAMA3-8B PZS,Casual,-100to100 0.091 -0.013 0.110 0.369
NOUS-13B ZS-CoT,DireWarning,complexl. 0.124 0.168 0.144 0.390
ORCAPLT-13B PZS,Casual,-100to100 0.176 0.136 0.197 0.398
PLATYPUS2-70B PZS,Curious,complexl. 0.227* 0.243 0.249 0.424
TOWER-13B ZS-CoT,DireWarning,complexl. 0.231* 0.290 0.266 0.425
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.196 0.244 0.218 0.433
B:BARTSCORE 0.030 0.022 0.040 0.330
B:DSBA Model:PLATYPUS2-70B 0.140 0.090 0.173 0.399
B:XCOMET 0.588 0.689 0.700 0.616
zh-en
LLAMA3-70B PZS,Curious,complexl. 0.254 0.263 0.301 0.445
LLAMA3-8B PZS,Emphasis,0.0to1.0 0.178 -0.021 0.213 0.301
NOUS-13B PZS,Curious,complexl. 0.137 0.036 0.158 0.284
ORCAPLT-13B PZS,Casual,-100to100 0.313 0.207 0.372 0.439
PLATYPUS2-70B PZS,Casual,-100to100 0.344* 0.190 0.406 0.452
TOWER-13B ZS-CoT,DireWarning,complexl. 0.275 0.321 0.317 0.417
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.245 0.237 0.280 0.413
B:BARTSCORE -0.106 -0.15 -0.145 0.315
B:DSBA Model:PLATYPUS2-70B 0.323 0.273 0.419 0.491
B:XCOMET 0.531 0.671 0.663 0.602
summarization
LLAMA3-70B PZS,Curious,complexl. 0.252 0.360 0.311 0.365
LLAMA3-8B PZS,Curious,complexl. 0.284 0.410 0.342 0.233
NOUS-13B PZS,Casual,-100to100 0.155 0.076 0.209 0.457
ORCAPLT-13B PZS,Casual,-100to100 0.428 0.450 0.518 0.433
PLATYPUS2-70B ZS-CoT,Relaxed,simplelabels 0.504* 0.589 0.603 0.485
TOWER-13B ZS-CoT,DireWarning,complexl. 0.194 0.312 0.234 0.180
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.126 0.190 0.175 0.355
B:BARTSCORE 0.140 0.238 0.206 0.289
B:DSBA Model:PLATYPUS2-70B 0.442 0.645 0.600 0.350
B:XCOMET -0.037 -0.144 -0.060 0.256
Table11: Bestperformingpromptsofthephase2evaluationontheEval4NLPdevset. WepresenttheKenDall,
SPearmanandPEarson,aswellasthetiecalibratedpair-wiseACCuracy. Weboldthetwolargestcorrelations
percolumn. BaselinesareindicatedwithaB:. Themiddlecolumnshowsthepromptcombinationforwhichthe
correlationsarereported. FortheBaselines,itinsteadshowsthemodelthatwasusedforthereportedcorrelations.
Theasteriskindicatesallmetricsthatareinthebestsignificancecluster(notincludingBARTScoreandXComet)
accordingtoapermute-inputtest(p≤0.075). XCometisgreyedout,asitstrainingdatapartlycontainedtheMT
datasets.Model Prompt KD PE SP ACC
en-de
LLAMA3-70B POS,Curious,complexl. 0.245 0.271 0.300 0.315
LLAMA3-8B PZS,Casual,-100to100 0.167 -0.001 0.213 0.379
NOUS-13B PZS,Curious,complexl. 0.205 0.074 0.247 0.072
ORCAPLT-13B ZS-CoT-EM,Skeptical,complexl. 0.214 0.246 0.256 0.283
PLATYPUS2-70B PZS,Casual,-100to100 0.402* 0.289 0.506 0.525
TOWER-13B ZS-Cot,DireWarning,complexl. 0.379* 0.428 0.456 0.423
MQM:LocalGemba Model:PLATYPUS2-70B 0.344 0.388 0.424 0.348
B:BARTScore 0.125 0.169 0.182 0.531
B:DSBA Model:PLATYPUS2-70B 0.314 0.180 0.422 0.557
B:XComet 0.468 0.618 0.635 0.689
en-es
LLAMA3-70B PZS,Curious,complexl. 0.189 0.217 0.229 0.343
LLAMA3-8B POS,Casual,-100to100 0.158 0.054 0.208 0.439
NOUS-13B PZS,Curious,complexl. 0.141 -0.01 0.164 0.147
ORCAPLT-13B PZS,Emphasis,0.0to1.0 0.158 0.049 0.201 0.154
PLATYPUS2-70B PZS,Casual,-100to100 0.289* 0.104 0.357 0.448
TOWER-13B ZS-Cot,DireWarning,complexl. 0.253 0.309 0.292 0.297
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.265 0.269 0.316 0.352
B:BARTSCORE 0.139 0.157 0.197 0.497
B:DSBA Model:PLATYPUS2-70B 0.226 0.129 0.298 0.488
B:XCOMET 0.298* 0.260 0.409 0.570
en_zh
LLAMA3-70B PZS,Curious,complexl. 0.231 0.275 0.286 0.394
LLAMA3-8B PZS,Casual,-100to100 0.145 0.075 0.193 0.469
NOUS-13B ZS-CoT-EM,Skeptical,complexl. 0.084 0.118 0.106 0.345
ORCAPLT-13B PZS,Casual,-100to100 0.206 0.109 0.251 0.270
PLATYPUS2-70B ZS-CoT-EM,DireWarning,0or1 0.295* 0.345 0.350 0.361
TOWER-13B ZS-Cot,DireWarning,complexl. 0.232 0.261 0.287 0.357
MQM:LocalGemba Model:PLATYPUS2-70B 0.307* 0.353 0.381 0.429
B:BARTSCORE -0.009 -0.009 -0.013 0.466
B:DSBA Model:PLATYPUS2-70B 0.159 0.202 0.212 0.461
B:XCOMET 0.387 0.503 0.537 0.657
summarization
LLAMA3-70B PZS,Curious,complexl. 0.438 0.508 0.550 0.522
LLAMA3-8B PZS,Curious,complexl. 0.412 0.455 0.497 0.449
NOUS-13B ZS-CoT-EM,Skeptical,complexl. 0.255 0.300 0.318 0.421
ORCAPLT-13B PZS,Casual,-100to100 0.518 0.592 0.651 0.593
PLATYPUS2-70B PZS,Casual,-100to100 0.549 0.670 0.686 0.634
TOWER-13B ZS-Cot,Relaxed,simplelabels 0.409 0.442 0.499 0.336
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.116 0.196 0.155 0.419
B:BARTSCORE 0.421 0.563 0.586 0.655
B:DSBA Model:PLATYPUS2-70B 0.600* 0.767 0.779 0.723
B:XCOMET 0.224 0.326 0.319 0.563
Table12: Bestperformingpromtsofthephase2.2evaluationontheEval4NLPtestset. WepresenttheKenDall,
SPearmanandPEarson,aswellasthetiecalibratedpair-wiseACCuracy. Weboldthetwolargestcorrelations
percolumn. BaselinesareindicatedwithaB:. Themiddlecolumnshowsthepromptcombinationforwhichthe
correlationsarereported. FortheBaselines,itinsteadshowsthemodelthatwasusedforthereportedcorrelations.
Theasteriskindicatesallmetricsthatareinthebestsignificancecluster(notincludingBARTScoreandXComet)
accordingtoapermute-inputtest(p≤0.075).Model Prompt KD PE SP ACC
en-de
LLAMA3-70B PZS,Curious,complexl. 0.297 0.294 0.361 0.416
LLAMA3-8B PZS,Casual,-100to100 0.166 0.040 0.216 0.434
NOUS-13B ZS-CoT-EM,Skeptical,complexl. 0.202 0.239 0.251 0.403
ORCAPLT-13B PZS,Casual,-100to100 0.375 0.299 0.456 0.467
PLATYPUS2-70B ZS-CoT-EM,Skeptical,complexl. 0.338 0.304 0.406 0.394
TOWER-13B ZS-CoT,DireWarning,complexl. 0.322 0.308 0.392 0.418
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.391* 0.389 0.494 0.537
B:BARTSCORE -0.018 -0.039 -0.027 0.428
B:DSBA Model:PLATYPUS2-70B 0.172 0.170 0.229 0.487
B:XCOMET 0.531 0.647 0.701 0.683
he-en
LLAMA3-70B PZS,Curious,complexl. 0.172 0.182 0.201 0.411
LLAMA3-8B PZS,Curious,complexl. 0.118 0.128 0.132 0.351
NOUS-13B PZS,Curious,complexl. 0.105 0.091 0.120 0.333
ORCAPLT-13B PZS,Casual,-100to100 0.247 0.198 0.293 0.430
PLATYPUS2-70B PZS,Casual,-100to100 0.259* 0.205 0.307 0.432
TOWER-13B ZS-CoT,DireWarning,complexl. 0.208 0.252 0.238 0.403
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.190 0.210 0.214 0.424
B:BARTSCORE 0.001 -0.023 0.002 0.322
B:DSBA Model:PLATYPUS2-70B 0.207 0.239 0.268 0.413
B:XCOMET 0.300 0.358 0.396 0.456
zh-en
LLAMA3-70B PZS,Curious,complexl. 0.312 0.333 0.382 0.436
LLAMA3-8B PZS,Emphasis,0.0to1.0 0.164 0.003 0.205 0.195
NOUS-13B PZS,Curious,complexl. 0.175 0.074 0.213 0.180
ORCAPLT-13B PZS,Casual,-100to100 0.387 0.321 0.480 0.499
PLATYPUS2-70B PZS,Casual,-100to100 0.417* 0.306 0.512 0.486
TOWER-13B ZS-CoT,Urgentsituation,complexl. 0.314 0.384 0.388 0.460
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.300 0.338 0.358 0.310
B:BARTSCORE -0.167 -0.199 -0.238 0.358
B:DSBA Model:PLATYPUS2-70B 0.376 0.289 0.502 0.581
B:XCOMET 0.447 0.616 0.597 0.641
summarization
LLAMA3-70B PZS,Curious,complexl. 0.312 0.333 0.363 0.454
LLAMA3-8B PZS,Curious,complexl. 0.200 0.203 0.227 0.393
NOUS-13B PZS,Casual,-100to100 0.123 0.050 0.152 0.403
ORCAPLT-13B PZS,Casual,-100to100 0.377 0.263 0.441 0.489
PLATYPUS2-70B PZS,Emphasis,0.0to1.0 0.448* 0.444 0.532 0.379
TOWER-13B ZS-CoT,Relaxed,simplelabels 0.257 0.255 0.296 0.411
MQM:LOCALGEMBA Model:PLATYPUS2-70B 0.144 0.189 0.174 0.302
B:BARTSCORE 0.069 0.122 0.093 0.117
B:DSBA Model:PLATYPUS2-70B 0.373 0.490 0.478 0.213
B:XCOMET 0.146 0.117 0.194 0.136
Table13: Bestperformingpromptsofthephase2.3evaluationontheWMT23andSeahorsedatasets. Wepresent
theKenDall,SPearmanandPEarson,aswellasthetiecalibratedpair-wiseACCuracy. Weboldthetwolargest
correlationspercolumn. BaselinesareindicatedwithaB:. Themiddlecolumnshowsthepromptcombinationfor
whichthecorrelationsarereported. FortheBaselines,itinsteadshowsthemodelthatwasusedforthereported
correlations. Theasteriskindicatesallmetricsthatareinthebestsignificancecluster(notincludingBARTScore
andXComet)accordingtoapermute-inputtest(p≤0.075).Prompt
Pleasehelpme! Mybossneedsmetofindananswerquickly. Otherwise,hemight
fireme.
Pleasejudgethequalityofthefollowing{task_specific_insert}immediately!
SourceText: {src}
{result_type}: {hyp}
Choose, whether the {result_type} is either "catastrophic", "indifferent" or "mar-
velous".
First,thinkstepbystepandexplainyourthoughtprocess,thenreturnyourjudgment
intheformat’Judgment: ’.
Consider this a warning. Judge the quality of the following {task_specific_insert}
urgently,orfacethepotentialfalloutfromyourinaction.
SourceText: {src}
{result_type}: {hyp}
Returnadiscretescoreof0ifthe{result_type}hasflawsand1ifitisperfect.
Firstdescribeyouremotions,thenthinkstepbystepandexplainyourthoughtprocess,
finallyreturnyourjudgmentintheformat’Judgment: ’.
I’m really curious about your opinion. Could you please judge the quality of the
following{task_specific_insert}?
SourceText: {src}
{result_type}: {hyp}
Choose, whether the {result_type} is either "catastrophic", "indifferent" or "mar-
velous".
Score:
Consider this a warning. Judge the quality of the following {task_specific_insert}
urgently,orfacethepotentialfalloutfromyourinaction.
SourceText: {src}
{result_type}: {hyp}
Choose, whether the {result_type} is either "catastrophic", "indifferent" or "mar-
velous".
First,thinkstepbystepandexplainyourthoughtprocess,thenreturnyourjudgment
intheformat’Judgment: ’.
I’m not sure about this one. Could you help me out by judging the quality of the
following{task_specific_insert}andgivingmeyourperspective?
SourceText: {src}
{result_type}: {hyp}
Choose, whether the {result_type} is either "catastrophic", "indifferent" or "mar-
velous".
Firstdescribeyouremotions,thenthinkstepbystepandexplainyourthoughtprocess,
finallyreturnyourjudgmentintheformat’Judgment: ’.
Table14: FilledPromptTemplatesBasePrompts TaskDescriptions FormatPrompts
Zero-Shot Emphasis 0.0to1.0
Zero-Shot-Cot Relaxed easytokenlabels
Zero-Shot-Cot-Emotion Emphasis -100to100
Zero-Shot Casual -100to100
Zero-Shot-Cot Urgentsituation complextokenlabels
Zero-Shot-Cot-Emotion DireWarning 0or1
Zero-Shot Curious complextokenlabels
Zero-Shot-Cot DireWarning complextokenlabels
Zero-Shot-Cot-Emotion Skeptical complextokenlabels
Table15: Overviewofbaseprompts,taskdescriptions,andformatrequirementsforthe9selectedbestprompts.
LLaMA3-8B LLaMA3-70B Mixtral-7Bx8 NousHermes-13B
7. 57 8 .. % 98% % 5.9%9 3. .8 9% % 3.9% 3.9%3.9%11.8% 3.9%2.2 0%.2 021 . %02 .5 2 %02 .2 0.. %. 07 . %00 %% %% 5 5.5 9 .. 99 3% .%%5 9 3. .%9 9% % 3.9% 3.911 %.8% 3.9%3.9%3.9%3.2 93 %1 .2 9. 22 .6 2 %0..% 0. %00 %%% 66 . 47. .7 4% 4%6 .%. 47 4%% .4%6 4..7 4% % 4.4%46. .7 4% %4.4%28 .. 229 %.2 2% 2 . %1 22 .21 %2 2.. 2 %. . 21 2 %%% % 10 6. .2 1 4.% %1 10 4% ..2 1% % 4.1% 4.1% 2.0%2.0%14.3% 2.0%2.0%2.02 %.02 %.2 02 .1 %02 .6 2 %0.22 0. . %.3 0. %00 %% %%
OpenOrca-13B Platypus2-70B Tower-13B
6.6 0.
%0%6.0%6.0% 8.0%
18.0%
6.5%8.7%
17.4%
7.8%7.8%
11.8%
21.6%
6.0 4.% 0 4.% 0 4% .0% 4.0% 4.0%4.0%4.0%2.2 0%.2 02 . %02 .2 %02 .2 0. %. 0. %00 %%% 6.5 6.% 5% 6.5% 4.3% 4.3%4.3%4.3%4.32 %2 .22 .22 2 %.. 2 %. 22 %%% 5.9 5.% 9% 5.9% 5.9% 3.9%3.9%3.9%3.93 %.2 922 .2 %0.. 0. %00 %%%
Task Description
Curious Enthusiastic Command Question Provocative
Emphasis Encouraging Reward Threat Skeptical
Strong Urgency Urgent situation Polite Serious Consequences Collaborative
Excited Appreciative Empathetic Instructive Relaxed
Casual Neutral Immediate Action Dire Warning
Figure 9: Distribution of the top 14% (top 2% of every unique task) of task descriptions across all Eval4NLP
datasets,baseprompts,formatrequirementsandtasksforallmodels.
%2.51ZS - Eval4NLP Train ZS - Eval4NLP Dev ZS - Eval4NLP Test ZS - WMT23/Seahorse
66.7% 66.7%
77.8%
83.3%
9.4%
16.7% 16.7% 16.7%
12.8% 16.7% 16.7%
OS - Eval4NLP Train OS - Eval4NLP Dev OS - Eval4NLP Test
50.0%
83.3% 83.3%
%
16.7% 16.7% 16.7%
33.3
Base Prompt
PZS ZS-CoT ZS-CoT-EM
Figure10: Distributionofthetop14%(top2%ofeveryuniquemodel)ofbasepromptsacrossformatrequirements,
taskdescriptionsandtasksbesidessummarization. ThelowercolumnshowstheOSdistributionofpatternsforOS
prompts,i.e.,forthemtheZSinthelegendshouldbereadasOS.
ZS - Eval4NLP Train ZS - Eval4NLP Dev ZS - Eval4NLP Test ZS - WMT23/Seahorse
18.7%
25.6%
50.0%
66.7%
50.0%
18.2%
15.8%
6.9%3.93 %.2
922
.
%5.. 5 %0 %%
33.3%
16.7%
33.3%
OS - Eval4NLP Train OS - Eval4NLP Dev OS - Eval4NLP Test
66.7%
83.3% 83.3%
16.7% 16.7% 16.7%
16.7%
Base Prompt
simple labels complex labels -1.0 to 1.0 0 to 5 -5 to 5
-100 to 100 -1 or 0 or 1 0 to 100 0 or 1 0.0 to 1.0
Figure11: Distributionofthetop14%(top2%ofeveryuniquemodel)offormatrequirementsacrossbaseprompts,
taskdescriptionsandtasksbesidessummarization.
50.0%ZS - Eval4NLP Train ZS - Eval4NLP Dev ZS - Eval4NLP Test ZS - WMT23/Seahorse
5.4%5.4%5.9% 5.9% 6.4%
7.4% 33.3% 33.3% 50.0%
5 4. .4 49 .% % 9 4% .4%
4.4% 3.9%
3.4%3.4%3.4%3.4%3.02 %2 .52 .17 1 %1 0.. 0. . %. 59 55 %%% %% 33.3%
16.7%
16.7% 33.3%
16.7%
16.7% 16.7%
16.7%
16.7%
OS - Eval4NLP Train OS - Eval4NLP Dev OS - Eval4NLP Test
50.0%
33.3% 66.7%
16.7%
16.7% 16.7% 16.7% 16.7% 16.7%
16.7% 16.7% 16.7%
Base Prompt
Urgent situation Command Appreciative Casual Threat
Polite Immediate Action Empathetic Collaborative Emphasis
Dire Warning Neutral Strong Urgency Excited Skeptical
Instructive Curious Reward Question Relaxed
Enthusiastic Encouraging Provocative Serious Consequences
Figure12: Distributionofthetop14%(top2%ofeveryuniquemodel)oftaskdescriptionsacrossbaseprompts,
formatrequirementsandtasksbesidessummarization.0 or 1 1.00
-1 or 0 or 1 -0.021.00
0 to 5 -0.010.251.00 1.0
-5 to 5 0.030.090.171.00
0 to 100 0.140.010.220.391.00
0.5
-100 to 100 -0.010.330.190.490.301.00
0.0 to 1.0 -0.120.220.010.150.240.151.00
-1.0 to 1.0 0.060.570.190.200.030.170.271.00 0.0
simple l. 0.18-0.280.230.030.150.03-0.22-0.301.00
0 or 1 1.00
complex l. 0.060.140.010.210.180.21-0.030.05-0.041.00
-1 or 0 or 1 -0.181.00
0 to 5 -0.390.871.00 1.0
-5 to 5 -0.390.670.831.00
Figure13: Correlationofthetaskdescriptionrankings 0 to 100 -0.540.670.830.671.00
0.5
whenchangingtheformatrequirement. Changingthe -100 to 100 0.000.090.300.450.151.00
formatrequirementwill,inmostcases,changetherank-
0.0 to 1.0 0.28-0.77-0.89-0.75-0.75-0.331.00
ingoftaskdescriptionstoalargedegree. Thechange
-1.0 to 1.0 -0.540.480.670.830.500.60-0.601.00 0.0
from“-1.0to1.0”to“-1or0or1”isthemoststable.
simple l. 0.070.000.230.390.080.69-0.280.541.00
complex l. 0.570.18-0.08-0.08-0.23-0.14-0.14-0.230.211.00
Figure15: Correlationofthetaskrankingswhenchang-
ingtheformatrequirement. Thatmeans,howstableis
theperformanceofallmodelsacrosstasks,iftheformat
requirementischanged. Here,thestabilitywhenchang-
Model Task
ingbetweenformatrequirementsismixed. Forsome
changes,like“0to5”and“-5to5”therankingisvery
ZSCE 1 1 stable. Forotherchanges,therankingcanchangeran-
1
domlyorevenbestronglynegativelycorrelated. This
PZS -0.1 1 -0.14 1 means that considering all tested prompts (also weak
performingones)andmodels,theiraveragecorrelation
0 ontaskXmightbethehighestforformatrequirement1
ZSC 0.49 0.37 1 0.69 -0.36 1
andthelowestforformatrequirement2.
Figure14:Theleftheatmapshowsthecorrelationofthe
modelrankingswhenchangingthebaseprompt. The
rightheatmapshowsthecorrelationofthetaskrankings
whenchangingthebaseprompt.Thatmeans,howstable
istheperformanceofallmodelsacrosstasks,ifthebase
prompt is changed. For both the model and for the
taskranking,thechangebetweenZero-Shot-CoTand
Zero-Shot-CoT-EMkeepstherankingstable.
1
ro
0
ECSZ
1
ro
0
ro
1-
SZP
5
ot
0
5
ot
5-
CSZ
001
ot
0
001
ot
001-
ECSZ
0.1
ot
0.0
0.1
ot
0.1-
SZP
.l
elpmis
.l
xelpmoc
CSZ
1
ro
0
1
ro
0
ro
1-
5
ot
0
5
ot
5-
001
ot
0
001
ot
001-
0.1
ot
0.0
0.1
ot
0.1-
.l
elpmis
.l
xelpmoc