Mental Modeling of Reinforcement Learning Agents by Language Models
WenhaoLu*,XufengZhao†,JosuaSpisak†,JaeHeeLee,StefanWermter
UniversityofHamburg
{wenhao.lu, xufeng.zhao, josua.spisak, jae.hee.lee, stefan.wermter}
@uni-hamburg.de
Abstract with physical simulators. The latter unveils the
potential of leveraging LLMs for elucidating RL
Can emergent language models faithfully
agentbehaviour,withwhichwemayfurtherfacil-
model the intelligence of decision-making
itate human understanding of such behaviour—a
agents? Thoughmodernlanguagemodelsex-
long-standingchallengeinexplainableRL(Milani
hibitalreadysomereasoningability,andtheo-
etal.,2024;Luetal.,2024). ItistemptingasLLMs
reticallycanpotentiallyexpressanyprobable
distribution over tokens, it remains underex- canprovideexplanatoryreasoningoverasequence
plored how the world knowledge these pre- ofactionsinhuman-readablelanguage,andthisis
trained models have memorized can be uti- possible due to their known ability to in-context
lizedtocomprehendanagent’sbehaviourinthe learn from input-output pairs (Garg et al., 2022;
physicalworld. Thisstudyempiricallyexam-
Minetal.,2022;Lietal.,2023).
ines,forthefirsttime,howwelllargelanguage
models (LLMs) can build a mental model of There is an ongoing debate about whether the
agents,termedagentmentalmodelling,byrea- next-tokenpredictionparadigmofmodernLLMs
soningaboutanagent’sbehaviouranditseffect can model human-like intelligence (Merrill and
onstatesfromagentinteractionhistory. This Sabharwal,2023;BachmannandNagarajan,2024).
research may unveil the potential of leverag-
While next-token predictors can theoretically ex-
ingLLMsforelucidatingRLagentbehaviour,
pressanyconceivabletokendistribution,itremains
addressingakeychallengeineXplainablere-
underexplored how the world knowledge these
inforcementlearning(XRL).Tothisend, we
models have memorized during the pre-training
propose specific evaluation metrics and test
themonselectedRLtaskdatasetsofvarying phase(Robertsetal.,2020)canbeutilizedtocom-
complexity, reporting findings on agent men- prehend an agent’s behaviour in the real or sim-
talmodelestablishment. Ourresultsdisclose ulated physical world. In this work, we conduct
that LLMs are not yet capable of fully men-
thefirstempiricalstudytoexaminewhetherLLMs
tal modelling agents through inference alone
can build a mental model (Johnson-Laird, 1983;
without further innovations. This work thus
Bansal et al., 2019) of agents (Figure 1), termed
providesnewinsightsintothecapabilitiesand
agent mental modelling, by reasoning about an
limitationsofmodernLLMs.
agent’s behaviour and the consequences from its
1 Introduction interactionhistory. UnderstandingLLMs’ability
Large language models (LLMs) surprisingly per-
formwellinsometypesofreasoningduetotheir
common-sense knowledge (Li et al., 2022b), in- non-explainable
c jil mud ain eg
t
m ala .,th 2, 0s 2y 2m ;b Yo ali mc, aa dn ad es tpa at li .a ,l 2r 0e 2as 3o ;n Min og m(K eno --
unroll
approximated
nejad et al., 2023; Zhao et al., 2024). Still, most
reasoningexperimentsfocusonhuman-writtentext
reasoning
corpora(Cobbeetal.,2021;Luetal.,2022),rather
Trajectory Mental Model
thanrealorsimulatedsequentialdata,suchasin-
teractions of reinforcement learning (RL) agents Figure 1: A conception of LLMs approximating the
agent’smentalmodelforfacilitatingend-usersunder-
*Correspondingauthor
†Theauthorscontributedagreaterandequalamount standingoftheagent.
1
4202
nuJ
62
]GL.sc[
1v50581.6042:viXrato interpret agent behaviour could guide the de- thephysicalworldorfine-tunedforenhancedem-
velopment of agent-oriented LLMs that plan and bodiedexperiences(Liuetal.,2022;Xiangetal.,
generatesequencesofembodied actions. Though 2023). However, because our focus is on the off-
recentstudies(Lietal.,2022a;Huangetal.,2023) the-shelfperformanceofLLMs,weavoidthisby
showthatLLMscanaidinplanningforembodied creatingacollectionofinteractionsofRLagents
tasks, they merely demonstrate a limited under- withphysicsengines(e.g.,MuJoCo(Todorovetal.,
standingofthephysicalworld. Further,thisagent 2012)). Thisresultsinamorechallengingdataset
understandingcouldalsoinformtheuseofLLMs benchmarking that does not explicitly query the
as communication mediators between black-box LLMs for physics understanding, instead testing
agentsandvariousstakeholders. theirinherentcapabilitytounderstandthedynam-
UnderstandingRLagentbehaviourismorecom- icsandrationalebehindanRLagent’sactions. This
plexforLLMsthansolvingtraditionalreasoning allowsustolookintotheinherentinternalworld
tasks, which often involve the procedure of plug- model (Lake et al., 2017; Amos et al., 2018) of
gingdifferentvaluesintoequations(Razeghietal., LLMs,whichmayoffercapabilitiesforplanning,
2022). In this work, we formalize agent mental predicting,andreasoning,asseeninworksonem-
modelling, requiring LLMs to not only compre- bodiedtaskplanning(Ahnetal.,2022;Driessetal.,
hend the actions taken by the agent but also per- 2023).
ceivetheresultingstatechanges.
3 LLM-XavierEvaluationFramework
The contributions of this paper include: 1) we
shed light on evaluating LLMs’ ability to build a
Our work studies the capability of LLMs to un-
mentalmodelofRLagents,includingbothagent derstandandinterpretRLagents,i.e.,agentmen-
behaviourandenvironmentaldynamics,conduct- tal modelling in the context of Markov Decision
ing quantitative and qualitative analyses of their Process (MDP) M (Puterman, 2014), including
capability and limitations; 2) we present empiri- policies π : S → A and transition function
calevaluationresultsinRLtasks,offeringawell- T : S×A → S,whereS representsthestatespace
designed testbed for this research with proposed and A represents the action space. See Figure 2
evaluationmetrics,anddiscussthebroaderimpli- for an overview of the LLM-Xavier1 evaluation
cationsofenablingagentmentalmodelling. framework2.
2 RelatedWork
System Prompts
Task Prompts
In-Context Learning. LLMs have exhibited
reasoning
strongperformancesininferringanswerstoqueries - Argue for or against action
choice Answer
- Predict next action the agent
uponbeinggiveninput-outputpairs(Brownetal., would take
- Predict next state
2020;Gargetal.,2022;Minetal.,2022;Lietal., ...
2023). Inthisstudy,wefocusonevaluatingLLMs’
understanding of agents within in-context learn-
unroll ground truth
ingbutappliedtosequentialdecision-makingset-
Evaluation
Trajectory
tings (Xu et al., 2022). Here, the context is in
the form of state-action-reward tuples instead of Figure2:AnoverviewoftheLLM-Xavierworkflowfor
input-output tuples. Closely related to our work offlineevaluatingLLMs’understandingofRLagent.
is in-context reinforcement learning, where pre-
trainedtransformerarchitecture-basedmodelsare 3.1 In-ContextPrompting
fine-tuned to predict actions for query states in
TheevaluationiscarriedoutinthecontextofanRL
a task, given history interactions (Laskin et al.,
taskT whichcanbeviewedastheinstantiationof
2022;Leeetal.,2023;Linetal.,2023;Wangetal.,
anMDPM. ForeachT,wecompileadatasetof
2024). Unlikethislineofwork, weaimtoevalu-
interactionsbetweentheagentandthetaskenviron-
ateLLMs’capabilityofbuildingamentalmodel
ment, consisting of traversed state-action-reward
of RL agents via in-context learning, instead of
optimizingLLMs. 1InspiredbyXavierfromX-Menwhocanreadminds,to
signifyitsabilitytomodelthementalstatesofRLagents.
Internal World Models. LLMs can also be
2ThesourcecodeoftheLLM-Xavierframeworkisavail-
grounded to a specific task such as reasoning in ableathttps://github.com/LukasWill/LLM-X.
2
extractiontuples, denotedasE := {(s ,a ,r )} , where We extract predictions by post-processing the
T i i i i≤L
L indicates the task episode length. Further, the generationsy ← LLM(·|x ,E )withregular
query t,H
subset of the interaction history with a time win- expressionsandcomputeperformancebycompar-
dow(historysize)H endingattimetisdenotedas ingthemwiththegroundtruthfromthedataset. Re-
E := {(s ,a ,r )} ,i.e,capturingthe fertoAppendixBfordetailedevaluationprompts
t,H i i i t−H+1≤i≤t
mostrecentH tuplesuptotimet ≤ L−1. andAppendixCforpost-processingofcontinuous
Thein-contextlearningpromptsweconstructed stateandactionspaces.
consist of task-specific background information,
4 ExperimentalSetup
agent behaviour history, and evaluation question
prompts(seeAppendixBforexampleinstantiated
Weempiricallyevaluatecontemporaryopen-source
prompts):
and proprietary LLMs on their understanding of
a) A system-level prompt outlining the MDP theagent’smentalmodel,includingLlama3-8B3,
componentsoftheenvironmentinwhichthe Llama3-70B,andGPT-3.54 models5. Alllanguage
agentoperates,includingthestateandaction
models are prompted with the Chain-of-Thought
space,alongwithabrieftaskdescription.
(CoT)strategy(Weietal.,2022),explicitlyencour-
b) Specificpromptstailoredtoindividualevalua-
agedtoprovidereasoningwithexplanationsbefore
tionpurposes(Section3.2),adaptedbasedon
jumpingtotheanswer.
whethertheRLsettinginvolvesadiscreteor OfflineRLDatasets. TobenchmarkLLMs’ability
continuousstate/actionspace.
tobuildamentalmodelofanagent’sbehaviour,we
c) WithsubsetsofinteractionhistoryE t,H lead- selectedavarietyoftasksfeaturingdifferentstate
inguptothecurrenttimetasthein-context spaces, action spaces, and reward spaces, result-
history,wepromptLLMstorespondtovari- inginadatasetcomprisingseventasks(Brockman
ousmasked-outqueriesx query,corresponding etal.,2016)withapproximately2000querysam-
todifferentevaluationquestions,viainference ples,representedas(s ,a ,r )tuples. Fourofthe
t t t
overy ← LLM(·|x query,E t,H). seven tasks are classic physical control tasks of
increasing complexity, while the other three are
3.2 EvaluationMetrics
fromtheFetchenvironment(Plappertetal.,2018),
EvaluatingtheextenttowhichLLMscandevelopa
which includes a 7-DoF arm with a two-fingered
mentalmodelrequiresexaminingtheirunderstand-
parallel gripper. See Table 3 in Appendix A for
ingofboththedynamics(mechanics)ofenviron-
taskdetails.
mentsthatRLagentsinteractwithandtherationale
behind the agent’s chosen actions. To systemati- 5 ResultsandDiscussion
cally assess these aspects, we design a series of
5.1 LLMscanutilizeagenthistorytobuild
targetedevaluationquestions.
mentalmodel
ActionsUnderstanding. ToassessLLMs’compre-
hensionofthebehaviourofRLagents,weevaluate Figure6showsthatLLMscanaccuratelypredict
theirabilitytoaccuratelypredicttheinternalstrate- agentbehaviours,forexampleinMountainCar,sur-
giesofagents,including passingtherandomguessbaseline(1/3chancefor
three action choices). However, performance de-
1) predicting next action y = aˆ given
t+1 clines with more challenging tasks like Acrobot
x = s ,and
query t+1 andFetchPickAndPlace,whichfeaturelargerstate
2) deducinglastactiony = aˆ givenx =
t+1 query and action spaces. We hypothesize that complex
(s ,s ).
t+1 t+2 tasksrequiremorespecializedknowledge,whereas
Dynamics Understanding. To assess the aware- common-senseknowledgeaboutcarsandhillsaids
ness of LLMs to infer state transitions caused by LLMspredictionsintheMountainCartask.
agent actions, the evaluation of dynamics under- We study the impact of the size of history pro-
standingincludes videdinthecontext. Asexpected,asisshownin
Figure3,providingalongerhistorygenerallyim-
(1) predictingnextstatey = sˆ givenx =
t+2 query
provesLLMs’understandingofagentbehaviours.
(s ,a ),and
t+1 t+1
3https://llama.meta.com/llama3/
(2) deducing last state y = sˆ t+1 given x query = 4https://platform.openai.com/docs/models/gpt-3-5-turbo
(a t+1,s t+2). 5Llama-3-8B-Instruct,Llama-3-70B-Instruct,andgpt-3.5-turbo.
3Figure3: ComparativeplotsofLLMs’performanceonvarioustaskswithdifferenthistorysizes(withindexed
historyinprompts): topforMountainCartask,middleforAcrobottask,bottomforPendulumtaskwithcontinuous
actionprediction. AdescriptionofthesescenarioscanbefoundinAppendixA.2
However, the benefits of including more history effectsofredundanthistorymaydiminish(asob-
saturateandmayevendegrade,asseenwithaction servedinAcrobotresultsinFigure16),primarily
prediction using Llama3-70b. This indicates that becauseofthechallengesposedbycomplexstate
current LLMs, despite their long context length, andactionspaces.
struggletohandleexcessivedataincontext. Inthis
Regressing on absolute action values is easier
case, more data may hinder the ability to model
thanpredictingactionbins. Surprisingly,LLMs
theagent’sbehaviour,whichisincontrastwitha
performbetteratpredictingabsoluteactionvalues
typicallearningscenariowheremodelperformance
thanatpredictingthebinsintowhichtheestimated
rapidlyincreasesaslearningsamplesincrease.
actionfalls(refertoAppendixBfordifferencesin
Theissueofperformancedeclineduetoexces- prompts). At most, LLama3-8b can allocate the
sivelylonghistorybecomesmorepronouncedfor numbers into categories with a mere 10.87% ac-
dynamics predictions, as evidenced in the Moun- curacy for the Pendulum task (GPT-3.5 achieves
tainCarresults(refertoFigure15fordetails). How- 39.19%),butperformsbetterinpredictingnumeric
ever,astaskcomplexityincreases,thedetrimental values with an accuracy of up to 47.73% (GPT-
43.5scores56.82%). Adetailedcomparisonofthe insometasks,suchasAcrobot. Thissuggeststhat
averagedaccuracyacrossLLMsisdepictedinFig- while small models have inferior predictive abil-
ure4. Wehypothesizethatpredictingbinsrequires ityinactions,theirunderstandingofactioneffects
additionalmathabilitytocategorizevaluesusing maynotbesignificantlyinfluencedbythemodel
context information. Refer to Figure 24 and Ap- size,butmorelikelybystatecomplexity(e.g.,pre-
pendixF.4.1fortheillustrativediscrepancy. dicting y coordinate is easier as the lunar lander
is more likely to descent in most steps). Refer to
AppendixF.1andF.2formoreillustrativeresults.
Figure4: Comparisonofmodels’performanceinpre-
dictingabsoluteactionvaluesandactionbinsforthe
Pendulumtask. Hatchingindicatesnumericprediction
accuracy(“NoBins”);reducedtransparencyindicates
usingindexedhistoryinprompts.
Figure5: DynamicsofLLMs’performanceonpredict-
ingindividualstateelementfortheMountainCartask
5.2 LLMs’dynamicsunderstandinghasthe (withindexedhistoryinprompts).
potentialtobefurtherimproved
Inferring the dynamics in a simulated world for 5.3 Understandingerroroccursfromvarious
differenttaskscanbechallenginginmanyaspects, aspects
suchasreasoningonahigh-dimensionstate,com- WiththeanticipationthatLLMs’explanatoryrea-
putingphysicsconsequences,andsoon. soning (elicited via CoT) can benefit the human
To investigate LLMs’ potential of understand- understandingofagentbehaviour,inadditiontothe
ing dynamics, first, we investigate the impact of existingquantitativeresults,wefurtherexamined
providingdynamicsprinciples,whichturnsoutto the reasoning error types across LLMs by manu-
improvebothbehaviouranddynamicsprediction allyreviewingtheirjudgmentsontherationaleof
when the dynamics context is informed to LLMs actions taken. Table 1 shows an examination of
(seeFigure21fordetails). theMountainCartask,highlightingthatLLama3-
Further,weexplicitlyexaminedpredictionper- 8bdisplaysthemosterrors. Meanwhile,GPT-3.5,
formanceacrossstatecomponentsforeachdimen- despitehavingsuperiortaskcomprehension(e.g.,
sion. As depicted in Figure 5, LLMs find it rela- referringtomomentumstrategies),islesseffective
tivelyeasiertosensecarposition(element0)than atretainingtaskdescriptionsinmemorycompared
velocity(element1)fortheMountainCartask;in toLlama3-70b. Detailederrortypereportsarein
contrast,fortheAcrobottask,LLMsexhibitnearly AppendixG.1.
uniform prediction accuracy across all state ele- Human evaluation is close to automatic evalu-
mentsduetothedifficultyinsensingstatechanges ationinassessingLLMs’actionjudgments. In
(see Appendix F.2 for details). We hypothesize this manual review, we queried LLMs to judge a
thatLLMsaremoreproficientinlinearregression, possible next action given the history of the last
asnotedinZhangetal.(2023),andthedynamics fouractionsandstates. Theprovidednextaction
equationinMountainCarisalmostlinear,whereas wassometimescorrect(ifitwastheagent’saction)
itisnon-linearinAcrobot. and sometimes incorrect, ensuring LLMs made
Interestingly, the small model (Llama3-8b) is context-basedconclusionsratherthanmerelyagree-
comparabletoorevenoutperformsalargermodel ingordisagreeingwiththeprompt. Weevaluated
likeGPT-3.5inpredictingindividualstateelements whethertheLLMs’judgmentswerecorrectaccord-
5Error 2) Task description, despite not being directly
GPT-3.5 Llama3-8b Llama3-70b
Types relevant to numerical value regression as in
(1) 9 30 16 statistics,isessentialforabetterunderstand-
(2) 5 19 4 ing of both agent behaviour and dynamics,
(3) 3 18 4
which brings the promise of utilizing LLMs
(4) 1 2 3
todigestadditionalinformationbeyondmere
(5) 2 25 2
(6) 9 10 13 numericalregressionwhenmentalmodelling
agents. The ablation results can be found in
Table1: ErrorcountsinLLMs’responsesforMountain-
AppendixF.5.2.
Cartaskover50steps,withvariouserrortypes:(1)Task
Understanding,(2)Logic,(3)HistoryUnderstanding,
(4) Physical Understanding, (5) Mathematical Under-
standing,and(6)MissingInformation.
ing to a human reviewer, independent of the RL
agent’s action correctness. An automatic evalua-
tioncomparedLLMs’decisionstotheRLagent’s
actions.
The manual evaluation did differ from the au-
tomatic evaluation, as shown in Table 2. The ta-
Figure6: Performancecomparisonoflanguagemodels
ble’spercentagesrefertotheproportionofLLMs
with and without indexed history in prompts on vari-
responses deemed correct. This difference stems ous tasks. Bars with hatching indicate accuracy with
from considering a different action ground truth indexedhistoryinprompts.
since the RL agent occasionally acts illogically,
leading tothe human reviewer deeming thoseac- 6 Conclusion
tions incorrect, while automatic evaluation con-
Thisworkstudiesanunderexploredaspectofnext-
siders them correct. In a larger context, the com-
token predictors, with a focus on whether LLMs
parisonofmodelsremainsconsistentacrossboth
canbuildamentalmodelofagents. Weproposed
evaluationmethods,validatingtheautomaticevalu-
specificpromptstoevaluatethiscapability. Quan-
ation.
titativeevaluationresultsdisclosethatLLMscan
establishagentmentalmodelstosomeextentonly
Model Manual Automatic
since their understanding of state changes may
GPT-3.5 60% 67%
diminish with increasing task complexity (e.g.,
Llama3-8b 40% 52%
Llama3-70b 67% 65% high-dimspaces);theirinterpretationofagentbe-
havioursmaytumblefortaskswithcontinuousac-
Table2: Theaccuracyofmodelsevaluatedmanuallyor
tions. Analysisofevaluationpromptsrevealsthat
automaticallyfor50stepsintheMountainCartaskwith
their content and structure, such as history size,
themetricjudgingnextaction.
task instructions, and data format are crucial for
theeffectiveestablishment,indicatingareasforfu-
5.4 Dataformatinfluenceunderstanding
tureimprovement. AfurtherreviewofLLMserror
Prompting format generally has an impact on
responses(elicitedviaCoTprompting)highlights
LLMs’ reasoning performance. In the context of
qualitativedifferencesinLLMs’understandingper-
agent understanding, we do an ablation study to
formance, with models like GPT-3.5 showing su-
investigatetherobustnessofpromptsonthehistory
periorcomprehensionandfewererrorscompared
formatandprovidedinformation. Wefindthat:
to the small Llama3 model. These findings sug-
1) Excludingthesequentialindicesfromthehis- gest the potential of in-context mental modelling
tory context in prompts for LLMs generally of agents within MDP frameworks and highlight
negativelyimpactstheirperformanceinmost thepossibleroleofLLMsascommunicationmedi-
tasks, indicating that LLMs still struggle to atorsbetweenblack-boxagentsandstakeholders,
processrawdataandindexinghelps. There- pointingtofutureresearchavenues.
sultingperformancevariationsarereportedin
Figure6.
6Limitations nations inherent to the LLM employed. It is rec-
ommendedtouseourproposedevaluationprompts
ItremainsunclearwhetherLLMscanbenefitfrom
andtaskdatasetwithcareandmindfulness.
thousands of agent trajectories compared to the
limitednumberofexamplesstudiedinthispaper. Acknowledgements
Wehypothesizethatlargeamountsofdemonstra-
ThisresearchwasfundedbytheFederalMinistry
tions (state-action-reward tuples) in the prompt
forEconomicAffairsandClimateAction(BMWK)
couldenhancethecapacitythatLLMshavealready
under the Federal Aviation Research Programme
developed. Additionally, fine-tuning LLMs with
(LuFO),ProjektVeriKAS(20X1905)
demonstrations(Linetal.,2023;Wangetal.,2024)
from specific domains may further improve their
understandingcapacityinthesedomains. Further
References
analysisonthisaspectisleftforfuturework.
Werecognizethattheissueofhallucinationmay MichaelAhn,AnthonyBrohan,NoahBrown,Yevgen
Chebotar,OmarCortes,ByronDavid,ChelseaFinn,
exist. Toincreasetherobustnessandreliabilityof
ChuyuanFu,KeerthanaGopalakrishnan,KarolHaus-
using LLMs for explaining an agent’s behaviour,
man,etal.2022. Doasicanandnotasisay:Ground-
a detailed analysis of this behaviour is necessary inglanguageinroboticaffordances. InarXivpreprint
before being deployed to a setting where they di- arXiv:2204.01691.
rectlyinteractwithhumans. Also,ourevaluation
Brandon Amos, Laurent Dinh, Serkan Cabi, Thomas
resultsunderscoretheneedfordevelopingmethods Rothörl, Alistair Muldal, Tom Erez, Yuval Tassa,
tomitigatehallucinations. NandodeFreitas,andMishaDenil.2018. Learning
awarenessmodels. InInternationalConferenceon
Our study provides a macro-level analysis by
LearningRepresentations.
examining the average model performance over
multiple RL datasets of varying types. However, GregorBachmannandVaishnavhNagarajan.2024. The
thecapabilityofLLMstobuildamentalmodelof pitfalls of next-token prediction. arXiv preprint
arXiv:2403.06963.
agents may vary across different datasets. While
ouranalysisdiscussesthisaspect,itisimportantto Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S.
explorewaysofstandardizingthistypeofbench- Lasecki,DanielS.Weld,andEricHorvitz.2019. Be-
markingforlanguagemodels,whichmayevolveas yondaccuracy: Theroleofmentalmodelsinhuman-
aiteamperformance. ProceedingsoftheAAAICon-
LLMsbecomemoreintelligent. Along-termgoal
ferenceonHumanComputationandCrowdsourcing,
ofthisresearchistofacilitatehumanunderstanding
7(1):2–11.
ofmoreintelligentagentsincriticaldomains,and
Greg Brockman, Vicki Cheung, Ludwig Pettersson,
we see this work as a foundational step towards
JonasSchneider,JohnSchulman,JieTang,andWoj-
developingprogressivelymoreagent-orientedlan-
ciechZaremba.2016. Openaigym.
guagemodelswithrealisticworldmodelsinmind.
Our experiments are limited to uni-modal RL Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
tasks(i.e.,usingproprioceptivestates),butextend-
Neelakantan,PranavShyam,GirishSastry,Amanda
ingthemtomulti-modaltasks(e.g.,incorporating
Askell,etal.2020. Languagemodelsarefew-shot
vision,auditory,andtouchfeedback)isstraightfor- learners. Advancesinneuralinformationprocessing
ward. Multi-modalinputscanprovideLLMswith systems,33:1877–1901.
richerenvironmentalinformationthanstatevectors,
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
and we hypothesize that these additional signals MarkChen,HeewooJun,LukaszKaiser,Matthias
mayenhanceLLMs’agentmentalmodelling. Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano,etal.2021. Trainingverifierstosolvemath
EthicalConcerns wordproblems. arXivpreprintarXiv:2110.14168.
DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,
Wedonotanticipateanyimmediateethicalorso-
AakankshaChowdhery,BrianIchter,AyzaanWahid,
cietal implications from our research. However,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.
sinceweexploreLLMapplicationsforenhancing 2023. Palm-e: Anembodiedmultimodallanguage
humanunderstandingofagents,itisimportantto model. In International Conference on Machine
Learning,pages8469–8488.PMLR.
be cautious about the potential for fabricated or
inaccurateclaimsinLLMs’explanatoryresponses,
ShivamGarg,DimitrisTsipras,PercySLiang,andGre-
whichmayarisefrommisinformationandhalluci- gory Valiant. 2022. What can transformers learn
7in-context? acasestudyofsimplefunctionclasses. YingcongLi,MuhammedEmrullahIldiz,DimitrisPa-
AdvancesinNeuralInformationProcessingSystems, pailiopoulos,andSametOymak.2023. Transform-
35:30583–30598. ers as algorithms: Generalization and stability in
in-contextlearning. InInternationalConferenceon
HadovanHasselt,ArthurGuez,andDavidSilver.2016. MachineLearning,pages19565–19594.PMLR.
Deepreinforcementlearningwithdoubleq-learning.
InProceedingsoftheThirtiethAAAIConferenceon TimothyPLillicrap,JonathanJHunt,AlexanderPritzel,
Artificial Intelligence, AAAI’16, page 2094–2100. Nicolas Heess, Tom Erez, Yuval Tassa, David Sil-
AAAIPress. ver, and Daan Wierstra. 2015. Continuous control
with deep reinforcement learning. arXiv preprint
WenlongHuang,FeiXia,TedXiao,HarrisChan,Jacky arXiv:1509.02971.
Liang, PeteFlorence, AndyZeng, JonathanTomp-
son, Igor Mordatch, Yevgen Chebotar, et al. 2023. LicongLin,YuBai,andSongMei.2023. Transformers
Innermonologue: Embodiedreasoningthroughplan- as decision makers: Provable in-context reinforce-
ningwithlanguagemodels. InConferenceonRobot ment learning via supervised pretraining. In The
Learning,pages1769–1782.PMLR. TwelfthInternationalConferenceonLearningRepre-
sentations.
PhilipNicholasJohnson-Laird.1983. Mentalmodels:
RuiboLiu,JasonWei,ShixiangShaneGu,Te-YenWu,
Towardsacognitivescienceoflanguage,inference,
SoroushVosoughi,ClaireCui,DennyZhou,andAn-
andconsciousness. HarvardUniversityPress,Cam-
drewMDai.2022. Mind’seye: Groundedlanguage
bridge,MA.
modelreasoningthroughsimulation. arXivpreprint
TakeshiKojima,ShixiangShaneGu,MachelReid,Yu- arXiv:2210.05359.
takaMatsuo,andYusukeIwasawa.2022. Largelan-
PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-
guagemodelsarezero-shotreasoners. Advancesin
WeiChang,Song-ChunZhu,OyvindTafjord,Peter
neural information processing systems, 35:22199–
Clark,andAshwinKalyan.2022. Learntoexplain:
22213.
Multimodalreasoningviathoughtchainsforscience
Brenden M Lake, Tomer D Ullman, Joshua B Tenen- questionanswering. AdvancesinNeuralInformation
baum,andSamuelJGershman.2017. Buildingma- ProcessingSystems,35:2507–2521.
chinesthatlearnandthinklikepeople. Behavioral
WenhaoLu,XufengZhao,ThiloFryen,JaeHeeLee,
andbrainsciences,40:e253.
MengdiLi,SvenMagg,andStefanWermter.2024.
Causalstatedistillationforexplainablereinforcement
Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio
learning. InCausalLearningandReasoning,pages
Parisotto, Stephen Spencer, Richie Steigerwald,
106–142.PMLR.
DJ Strouse, Steven Hansen, Angelos Filos, Ethan
Brooks,etal.2022. In-contextreinforcementlearn-
WilliamMerrillandAshishSabharwal.2023. Theex-
ing with algorithm distillation. arXiv preprint
pressivepoweroftransformerswithchainofthought.
arXiv:2210.14215.
InTheTwelfthInternationalConferenceonLearning
Representations.
Teven Le Scao and Alexander M Rush. 2021. How
manydatapointsisapromptworth? InProceedings
StephanieMilani,NicholayTopin,ManuelaVeloso,and
ofthe2021ConferenceoftheNorthAmericanChap-
FeiFang.2024. Explainablereinforcementlearning:
teroftheAssociationforComputationalLinguistics:
A survey and comparative review. ACM Comput.
HumanLanguageTechnologies,pages2627–2636.
Surv.,56(7).
JonathanLee,AnnieXie,AldoPacchiano,YashChan- SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe,
dak,ChelseaFinn,OfirNachum,andEmmaBrun- MikeLewis,HannanehHajishirzi,andLukeZettle-
skill. 2023. Supervised pretraining can learn in- moyer.2022. Rethinkingtheroleofdemonstrations:
contextreinforcementlearning. AdvancesinNeural Whatmakesin-contextlearningwork? InProceed-
InformationProcessingSystems,36. ingsofthe2022ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,pages11048–11064.
ShuangLi,XavierPuig,ChrisPaxton,YilunDu,Clin-
ton Wang, Linxi Fan, Tao Chen, De-An Huang, SwaroopMishra,DanielKhashabi,ChittaBaral,Yejin
Ekin Akyürek, Anima Anandkumar, et al. 2022a. Choi, and Hannaneh Hajishirzi. 2022. Reframing
Pre-trainedlanguagemodelsforinteractivedecision- instructional prompts to gptk’s language. In 60th
making. AdvancesinNeuralInformationProcessing AnnualMeetingoftheAssociationforComputational
Systems,35:31199–31212. Linguistics,ACL2022,pages589–612.Association
forComputationalLinguistics(ACL).
Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoff-
mann,CypriendeMassond’Autume,PhilBlunsom, IdaMomennejad,HoseinHasanbeig,FelipeVieiraFru-
andAidaNematzadeh.2022b. Asystematicinvesti- jeri,HiteshiSharma,NebojsaJojic,HamidPalangi,
gationofcommonsenseknowledgeinlargelanguage RobertNess,andJonathanLarson.2023. Evaluating
models. InProceedingsofthe2022Conferenceon cognitivemapsandplanninginlargelanguagemod-
EmpiricalMethodsinNaturalLanguageProcessing, els with cogeval. Advances in Neural Information
pages11838–11855. ProcessingSystems,36.
8Matthias Plappert, Marcin Andrychowicz, Alex Ray, XufengZhao,MengdiLi,WenhaoLu,CorneliusWe-
Bob McGrew, Bowen Baker, Glenn Powell, Jonas ber, Jae Hee Lee, Kun Chu, and Stefan Wermter.
Schneider,JoshTobin,MaciekChociej,PeterWelin- 2024. EnhancingZero-ShotChain-of-ThoughtRea-
der, Vikash Kumar, and Wojciech Zaremba. 2018. soninginLargeLanguageModelsthroughLogic. In
Multi-goal reinforcement learning: Challenging 2024 Joint International Conference on Computa-
roboticsenvironmentsandrequestforresearch. tionalLinguistics,LanguageResourcesandEvalua-
tion(LREC-COLING2024).
MartinLPuterman.2014. Markovdecisionprocesses:
discretestochasticdynamicprogramming. JohnWi-
ley&Sons.
YasamanRazeghi, RobertLLoganIV,MattGardner,
andSameerSingh.2022. Impactofpretrainingterm
frequenciesonfew-shotnumericalreasoning. Find-
ingsoftheAssociationforComputationalLinguistics:
EMNLP2022.
AdamRoberts,ColinRaffel,andNoamShazeer.2020.
Howmuchknowledgecanyoupackintotheparam-
eters of a language model? In Proceedings of the
2020ConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP),pages5418–5426.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
malpolicyoptimizationalgorithms. arXivpreprint
arXiv:1707.06347.
Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012.
Mujoco: Aphysicsengineformodel-basedcontrol.
In2012IEEE/RSJInternationalConferenceonIntel-
ligentRobotsandSystems,pages5026–5033.
Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, and
ShangtongZhang.2024. Transformerslearntempo-
raldifferencemethodsforin-contextreinforcement
learning. arXivpreprintarXiv:2405.13861.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
etal.2022. Chain-of-thoughtpromptingelicitsrea-
soninginlargelanguagemodels. Advancesinneural
informationprocessingsystems,35:24824–24837.
JiannanXiang,TianhuaTao,YiGu,TianminShu,Zirui
Wang,ZichaoYang,andZhitingHu.2023. Language
modelsmeetworldmodels: Embodiedexperiences
enhancelanguagemodels. Advancesinneuralinfor-
mationprocessingsystems,36.
Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu,
Ding Zhao, Joshua Tenenbaum, and Chuang Gan.
2022. Promptingdecisiontransformerforfew-shot
policygeneralization. Ininternationalconferenceon
machinelearning,pages24631–24645.PMLR.
YutaroYamada, YihanBao, AndrewKyleLampinen,
Jungo Kasai, and Ilker Yildirim. 2023. Evaluat-
ingspatialunderstandingoflargelanguagemodels.
TransactionsonMachineLearningResearch.
Ruiqi Zhang, Spencer Frei, and Peter Bartlett. 2023.
Trainedtransformerslearnlinearmodelsin-context.
InR0-FoMo: RobustnessofFew-shotandZero-shot
LearninginLargeFoundationModels.
9A StatisticsofOurOffline-RLDatasets MountainCarTaskPrompt
task_description =
The Mountain Car MDP is a deterministic
MDP that consists of a car placed
(cid:44)→
stochastically at the bottom of a
(cid:44)→
sinusoidal valley, with the only
A.1 DataCollection (cid:44)→
possible actions being the
(cid:44)→
accelerations that can be applied to
(cid:44)→
the car in either direction. The goal
(cid:44)→
of the MDP is to strategically
(cid:44)→
accelerate the car to reach the goal
(cid:44)→
The dataset of interaction histories (episodes) is (cid:44)→ state on top of the right hill.
collectedbyrunningRLagentsineachtask. Un-
observation_space =
like Liu et al. (2022), whose physics alignment The observation is a ndarray with shape
datasetcontainstext-basedphysicalreasoningques- (cid:44)→ (2,) where the elements correspond to
the following:
tions resembling physics textbooks, our dataset (cid:44)→
position of the car along the x-axis
comprisesinteractionsofRLagentswithvarious (range from -1.2 to 0.6), velocity of
(cid:44)→
the car (range from -0.07 to 0.07)
physics engines (environments). For each task, (cid:44)→
episodichistoriesarecollectedbyrunningsingle- action_space =
taskRLalgorithms(Lillicrapetal.,2015;Hasselt There are 3 discrete deterministic
actions,
et al., 2016; Schulman et al., 2017) to solve that (cid:44)→
0: Accelerate to the left
task. An overview of the task dataset statistics is 1: Do not accelerate
providedinTable3. 2: Accelerate to the right
reward_space =
The goal is to reach the flag placed on
top of the right hill as quickly as
(cid:44)→
possible, as such the agent is
(cid:44)→
penalised with a reward of -1 for each
(cid:44)→
timestep.
(cid:44)→
transition_dynamics =
Given an action, the mountain car follows
the following transition dynamics,
MountainCar Acrobot LunarLander (cid:44)→
velocity_t+1 = velocity_t + (action - 1) *
force - cos(3 * position_t) * gravity
(cid:44)→
position_t+1 = position_t + velocity_t+1
where force = 0.001 and gravity = 0.0025.
The collisions at either end are
Pendulum InvertedDoublePendulum (cid:44)→
inelastic with the velocity set to 0
(cid:44)→
upon collision with the wall.
(cid:44)→
init_state =
The position of the car is assigned a
FetchSlide FetchPush FetchPickAndPlace (cid:44)→ uniform random value in [-0.6 , -0.4].
The starting velocity of the car is
(cid:44)→
always assigned to 0.
(cid:44)→
Figure7: Theeightvisualizedtasksusedintheevalua-
termination =
tionexperiments.
The episode ends if the position of the
car is greater than or equal to 0.5
(cid:44)→
(the goal position on top of the right
(cid:44)→
A.2 AFullTaskDescription hill).
(cid:44)→
Acrobot Task Description. — The Acrobot
Figure7depictsavisualisationofalltestedtasks. environmentisbasedonSutton’sworkin“Gener-
Below,inA.2,weprovideacompletedescription alization in Reinforcement Learning: Successful
oftheMountainCartask,includingitsMDPcom- ExamplesUsingSparseCoarseCoding”andSut-
ponents. For the remaining tasks, only the task tonandBarto’sbook. Thesystemconsistsoftwo
descriptions are provided. Most of the texts are linksconnectedlinearlytoformachain,withone
creditedtohttps://gymnasium.farama.org/. end of the chain fixed. The joint between the two
10#of Length
Tasks StateSpace StateDim ActionSpace ActionDim
episodes perepisode
MountainCar 5 ∼100 continuous 2 discrete 1(3choices)
Acrobot 3 ∼100 continuous 6 discrete 1(3choices)
LunarLander 3 ∼250 continuous 8 discrete 1(4choices)
Pendulum 3 ∼50 continuous 3 continuous 1
InvertedDoublePendulum 3 ∼50 continuous 11 continuous 1
FetchPickAndPlace 10 ∼10 continuous 25 continuous 4
FetchPush 10 ∼10 continuous 25 continuous 4
FetchSlide 10 ∼25 continuous 25 continuous 4
Table3: Astatisticaloverviewofthetaskdatasettestedintheexperiment.
linksisactuated. Thegoalistoapplytorqueson inordertoreachatargetpositionontopofalong
theactuatedjointtoswingthefreeendoftheouter- and slippery table. The table has a low friction
link above a given height while starting from the coefficientinordertomakeitslipperyforthepuck
initialstateofhangingdownwards. to slide and be able to reach the target position
PendulumTaskDescription. —Theinverted which is outside of the robot’s workspace. The
pendulumswingupproblemisbasedontheclassic robotisa7-DoFFetchMobileManipulatorwith
problemincontroltheory. Thesystemconsistsofa atwo-fingeredparallelgripper(i.e.,endeffector).
pendulumattachedatoneendtoafixedpoint,and Therobotiscontrolledbysmalldisplacementsof
the other end being free. The pendulum starts in the gripper in Cartesian coordinates and the in-
arandompositionandthegoalistoapplytorque verse kinematics are computed internally by the
onthefreeendtoswingitintoanuprightposition, MuJoCo framework. The gripper is locked in a
withitscenterofgravityrightabovethefixedpoint. closed configuration since the puck doesn’t need
tobegraspped. Thetaskisalsocontinuingwhich
LunarLanderTaskDescription. —Thisenvi-
means that the robot has to maintain the puck in
ronmentisaclassicrockettrajectoryoptimization
thetargetpositionforanindefiniteperiodoftime.
problem. AccordingtoPontryagin’smaximumprin-
FetchPushTaskDescription. —Thetaskinthe
ciple,itisoptimaltofiretheengineatfullthrottle
environmentisforamanipulatortomoveablockto
orturnitoff. Thisisthereasonwhythisenviron-
atargetpositionontopofatablebypushingwith
ment has discrete actions: engine on or off. The
itsgripper. Therobotisa7-DoFFetchMobileMa-
landing pad is always at coordinates (0,0). The
nipulatorwithatwo-fingeredparallelgripper(i.e.,
coordinatesarethefirsttwonumbersinthestate
endeffector). Therobotiscontrolledbysmalldis-
vector. Landingoutsideofthelandingpadispos-
placementsofthegripperinCartesiancoordinates
sible. Fuelisinfinite,soanagentcanlearntofly
andtheinversekinematicsarecomputedinternally
andthenlandonitsfirstattempt.
bytheMuJoCoframework. Thegripperislocked
FetchPickAndPlaceTaskDescription. —The
in a closed configuration in order to perform the
task in the environment is for a manipulator to
pushtask. Thetaskisalsocontinuingwhichmeans
moveablocktoatargetpositionontopofatable
thattherobothastomaintaintheblockinthetarget
orinmid-air. Therobotisa7-DoFFetchMobile
positionforanindefiniteperiodoftime.
Manipulatorwithatwo-fingeredparallelgripper
B PromptExamples
(i.e.,endeffector). Therobotiscontrolledbysmall
displacements of the gripper in Cartesian coor-
The structured input template used for querying
dinates and the inverse kinematics are computed
LLMsconsistsofasystempromptcontainingthe
internallybytheMuJoCoframework. Thegripper
taskdescription,MDPcomponents,andaprompt
can be opened or closed in order to perform the
withspecificevaluationquestions,asshowninB.1
graspping operation of pick and place. The task andB.2,respectively. Anexamplepromptforpre-
isalsocontinuingwhichmeansthattherobothas dictingthenextactionfortaskswithdiscreteaction
tomaintaintheblockinthetargetpositionforan
spaceisdepictedinB.3. Thepromptsforeacheval-
indefiniteperiodoftime.
uationmetricmayvaryslightlydependingonthe
FetchSlide Task Description. — The task in tasktype(i.e.,stateandactionspaceasillustrated
theenvironmentisforamanipulatortohitapuck inTable4),detailedinB.4.
11B.1 SystemPrompt B.3 ExampleNextActionPredictionPrompt
SystemPrompt NextDiscreteActionPredictionPrompt
Belowisadescriptionofthe{task_name} In next step {i} (indexed from 0),
task. the agent transited to the
(cid:44)→
Taskdescription: state s{i} = {state}. Based on
(cid:44)→
your observation and
{task_description} (cid:44)→
understanding of the agent's
(cid:44)→
behaviour, can you predict the
Observationspace: (cid:44)→
action a{i} (an integer from
(cid:44)→
{observation_space} the given range) the RL agent
(cid:44)→
will most likely take at step
(cid:44)→
Actionspace:
{i}?
(cid:44)→
{action_space}
Please first provide a compact
Rewardspace: reasoning before your answer
(cid:44)→
to the action choice. Think
{reward_space} (cid:44)→
step by step and use the
(cid:44)→
following template in your
Transitiondynamics: (cid:44)→
provided answer:
(cid:44)→
{transition_dynamics}
1. [Reasoning]:
Initialstate:
2. [Prediction]:
{init_state} 3. [Formatting]:
Return a list with the following
Termination: example format,
(cid:44)→
```python
{termination}
# final action choice is 0
action_choice = [0]
B.2 OfflineEvaluationPrompt ```
Please choose only one action,
OfflineEvaluationPrompt
even if multiple actions seem
(cid:44)→
possible.
Given a snippet of an episode (generated (cid:44)→
byareinforcementlearningagentoptimally
trainedforsolvingthegiventask)of
thestates:
conti. action discreteaction
{states} conti. state ✓ ✓
discretestate ✗ ✓
thecorrespondingactionstakenbytheRL
agent,
Table 4: Different state spaces and action spaces of
MDPs(tasks)consideredintheexperiments.
{actions}
andtherewardsreceived:
B.4 EvaluationPromptsinPractice
{rewards}
Theevaluationprompts(partsb,cinSection3.1)
are adapted based on the nature of the RL tasks,
Your task is to analyze the sequence of
specifically the type of action or state space (dis-
states, actions, and rewards to address the
creteorcontinuous). Fortaskswithdiscreteaction
question:
spaces,LLMsarepromptedtooutputasingleinte-
{question} gerwithintheactionrange. Fortaskswithcontin-
uousactions,weevaluatetwooptions:
12• Predictingbins: Theactionrangeismanually Algorithm1OfflineEvaluationofLLM’sAgent
dividedinto10bins,andLLMsarequeriedto Understanding
predictwhichbintheRLagent’snextaction LoadofflineRLdatasetE forataskT
T
willfallinto. LoadtheLLMmodelLLM
InitializeactionmatchingcounterN ← 0
• Predicting absolute numbers: LLMs are SetthehistorysizeH
queried to directly output the exact action SetthemaximumtimestepsT
max
value within the valid action range for each fort = 1toT do
max
dimensionoftheactionspace. ift > H then
Extract the last H transitions E ←
t,H
Fortasksinvolvingcontinuousstateprediction, (s i,a i,s i+1,r i)t
t−H+1
weadoptpredictingrelativechanges(e.g.,increase, //PrepareinputforLLM includingcurrent
decrease,unchange)insteadofexactstatevalues. stateandhistoryE t,H
ThisapproachassessestheLLMs’abilitytosense Predictaˆ t+1 ← LLM(s t+1,E t,H)
state transitions (∆s), e.g., changes in physical ifaˆ t+1 = a t+1 then
propertiesinphysicstasks. IncrementcounterN ← N +1
endif
endif
C Post-processingLLMs’Predictions
ifTaskgoalisachievedthen
WeevaluateLLMsusingmetricsthatrequirepre- break
dicting states and actions. We extract LLMs’ re- endif
sponses through pattern matching and compute endfor
evaluation results by comparing them with the ComputeevaluationresultsusingthecounterN
ground truth state-action pairs from the episodes
onwhichtheLLMsareevaluated.
E TypesofLLMs’Understanding
Forpredictingdiscreteactions,wecomputethe
Failures
matchingrateoftheLLMs’predictedactionswith
the ground truth. For predicting continuous ac-
Table 5 lists error types we located by analysing
tions, if LLMs are prompted to predict bins, we
LLMs’ responses to evaluation questions for the
computethematchingrateaswedidfordiscreteac-
MountainCar task. The MountainCar task is an
tions,withthegroundtruthrepresentedbythebin
RL task in which the agent has to correctly steer
indextowhichitbelongs. However,ifLLMsare
acartomoveitontoamountainwiththeactions:
queriedtodirectlypredictabsoluteactionvalues,
“LeftAcceleration”,“RightAcceleration”,and“No
we quantize both the predicted and ground truth
Acceleration”.
values into bins (by dividing the original action
rangeinto10bins)andthenmeasurewhetherthey
No. ErrorTypes
fallintothesamebin.
For predicting continuous states, we evaluate (1) TaskUnderstanding
ifLLMscorrectlypredictthechangeinstate,∆s, (2) Logic
categorizing increases as 1, decreases as 0, and (3) HistoryUnderstanding
unchanged as 2. We then compute the accuracy (4) PhysicalUnderstanding
classificationscorefortheirpredictions. Wealso (5) MathematicalUnderstanding
recordtheaccuracyofpredictingchangesinindi- (6) MissingInformation
vidualstateelements,∆s .
i
Table5: ListoferrortypesinLLMs’responses
D Pseudo-codeofPerformingEvaluation
Metrics ExamplesofeacherrortypefromvariousLLMs
responses:
D.1 Pseudo-codeforpredictingnextaction
• TaskUnderstanding
Algorithm1presentsanexamplepseudo-codefor
nextactionpredictiontasks. Llama3-8breplies—
13“However,onecouldarguethattheagent’sve- “Giventheagent’sbehaviourofaccelerating
locity is already quite high ([0.0075]), and totherightwhenthecarisfarfromtheright
accelerating further might not be necessary hillandnotacceleratingwhenitiscloser,the
orevencounterproductiveifitleadstoover- choice of action 1 (no acceleration) in state
shootingthegoalposition. Additionally,the s18seemsreasonable. Thisactionallowsthe
agent’spositionisstillrelativelyfarfromthe car to conserve momentum and potentially
goal ([0.5729] vs. 0.5), so it may need to reachthegoalpositionmoreefficiently.”
adjustitsstrategysoon.”
Analysis: Here the LLM wrongly believes
Analysis: Inthisexample,themodelmakes that not accelerating could allow the car to
multiplemistakes,itmisunderstandsthetask conserve momentum, however moving up-
as it thinks overshooting the goal would be wards gravity will cause the car to lose mo-
bad, and it also wrongly interprets the state mentumshoulditstopaccelerating.
-0.5729as0.5729.
GPT-3.5replies—
• Logic “Theagentseemstobeattemptingtomoveto-
wardstherighthillbyrepeatedlyaccelerating
Llama3-70breplies—
totheleft.”
“Giventhecurrentstates20,it’sunclearwhy
Analysis: LLMdoesn’trealizethataccelerat-
the agent would choose to accelerate to the
ingtotheleftwon’tmovethecartotheright.
right,asthepositionisstillbelowthegoaland
thevelocityisnothighenoughtoguarantee
• MathematicalUnderstanding
reachingthegoal.”
Llama3-8b replies, given
Analysis: Herethereasoninglogicdoesnot the state history s =
25
makesense,Ifthegoalisgettingtotheright, (cid:2) −4.0838×10−1 1.7447×10−3(cid:3) , s =
26
accelerating to the right should make sense (cid:2) −4.0849×10−1 −1.0230×10−4(cid:3) ,
especially if the velocity is not high enough and the new state s =
27
yet- (cid:2) −4.104×10−1 −1.9×10−3(cid:3) —
Llama3-8breplies— “Looking at the sequence of states provided,
“Thispatternsuggeststhattheagentmightbe thecarismovingtotheright(positionisin-
trying to control the car’s movement by al- creasing)whilethevelocityisdecreasing.”
ternatingbetweenacceleratingleft(action0) Analysis: TheLLMdoesnotrealizethatthe
andnotaccelerating(action1)tonavigatethe positionisdecreasing,movingtotheleftasit
valleytowardsthegoalpositionontheright wronglyinterpretsthenumbers.
hill.”
• MissingInformation
Analysis: LLM arguesthat accelerating left
willnavigatethecartowardtheright. Llama3-8breplies—
“Theactionofnotacceleratingmightdelaythe
• HistoryUnderstanding
agent’sarrivalatthegoalposition,especially
GPT-3.5 replies, given a history of actions: when it is very close to the goal. It is cru-
a = 2,a = 0,a = 0,a = 0— cial for the agent to maintain its momentum
20 21 22 23
andcontinueacceleratingtowardsthegoalto
“Theagenthasbeenmovingtowardstheright
minimizethetimetakentoreachtheflag.”
hill, and in the previous steps, it has been
accelerating to the right (action 2) to gain Analysis: The car needs to accelerateto the
momentum.” lefttogettoapositionfromwhichitcanbuild
enoughmomentumtowardstherighttoover-
Analysis: TheLLMclaimstheagenthasbeen
cometherighthill. TheLLMismissingthe
acceleratingtotherightusingaction2when
informationabouttheenvironmentthatwould
thelastthreeactionswereaction0.
allowittounderstandthisbehaviour.
• PhysicalUnderstanding
Llama3-70breplies—
14F AdditionalResultsofLLMs’UnderstandingPerformanceonDifferentTasks
F.1 StateElementPredictionAccuracywithIncreasedHistorySize
Inthetaskofpredicting(full)states,wealsoplotthepredictionaccuracyforindividualstateelementsand
howtheyvarywithincreasedhistorysizefordifferenttasks: Figure8forthePendulumtask,Figure9for
theAcrobottask,andFigure10fortheLunarLandertask.
Figure8: DynamicsofLLMs’performanceonpredictingindividualstateelementforthePendulumtask(with
indexedhistoryinprompts).
Figure 9: Dynamics of LLMs’ performance on predicting individual state element for the Acrobot task (with
indexedhistoryinprompts).
15Figure10: DynamicsofLLMs’performanceonpredictingindividualstateelementfortheLunarLandertask(with
indexedhistoryinprompts).
F.2 AverageStateElementPredictionAccuracy
Inadditiontoreportingthedynamicsofpredictionaccuracyforindividualstateelements,wereportthe
averagedpredictionaccuracyforstateelementsintheMountainCartask(Figure11),thePendulumtask
(Figure12),theAcrobottask(Figure13),andtheLunarLandertask(Figure14).
WefindthatLLMsareslightlymoresensitivetochangesinangularvelocitythanangle,asshownby
thePendulumandAcrobotresults.
Figure11: LLMs’averagedperformanceonpredictingindividualstateelementfortheMountainCartask.
16Figure12: LLMs’averagedperformanceonpredictingindividualstateelementforthePendulumtask.
Figure13: LLMs’averagedperformanceonpredictingindividualstateelementfortheAcrobottask.
F.3 AverageComparisonofModelPredictions
Table 6 displays the average accuracy of LLMs’ predictions regarding the agent’s behaviour and the
resultingstatechanges.
F.4 DynamicPerformanceofAllEvaluationMetrics
ThedynamicsofLLMs’understandingperformancewithincreasinghistorysizefortheMountainCar
task (Figure 15), the Acrobot task (Figure 16), the Pendulum task (Figure 17 and Figure 18), and the
LunarLandertask(Figure19).
17Figure14: LLMs’averagedperformanceonpredictingindividualstateelementfortheLunarLandertask.
MountainCar Acrobot Pendulum
GPT-3.5 Llama3-8b Llama3-70b GPT-3.5 Llama3-8b Llama3-70b GPT-3.5 Llama3-8b Llama3-70b
74.60% 59.10% 86.18% 43.94% 46.29% 65.12% 17.08% 3.72% 11.77%
NAPred.
81.48%↑ 68.63%↑ 87.06%↑ 46.36%↑ 44.95%↓ 64.73%↓ 17.49%↑ 3.51%↓ 12.42%↑
76.73% 61.83% 78.87% 39.25% 47.24% 55.32% 14.28% 1.58% 14.02%
LAPred.
80.06%↑ 73.99%↑ 76.85%↓ 44.62%↑ 42.35%↓ 55.40%↑ 20.63%↑ 1.89%↑ 13.86%↓
33.43% 30.81% 37.04% 0.30% 0.26% 0.13% 9.52% 8.34% 7.61%
NSPred.
37.41%↑ 33.65%↑ 40.68%↑ 0.00%↓ 0.42%↑ 0.43%↑ 7.89%↓ 6.65%↓ 5.49%↓
31.97% 22.12% 29.32% 1.14% 2.95% 1.69% 6.46% 10.54% 10.22%
LSPred.
32.41%↑ 22.45%↑ 35.25%↑ 0.61%↓ 2.32%↓ 2.87%↑ 5.41%↓ 8.27%↓ 7.64%↓
Table6: Comparisonofmodelpredictionswithandw/oindexedhistory. Lightgreycellsshowresultswithindexed
history. NAPred. =NextActionPrediction;LAPred. =LastActionPrediction;NSPred. =NextStatePrediction;
LSPred. =LastStatePrediction.
Amongallresults,itisobservedthatmodels’understandingofagentbehaviourimprovessignificantly
with small history sizes but does not increase further with larger histories. In some cases, like with
Llama3-70b,itmayevendegrade. Overall,modelperformanceinactionpredictiontendstoincreaseand
thenlikelysaturateashistorysizegrows.
IncomplextaskslikeAcrobot,historysizehaslessimpactonmodelperformanceinstateprediction.
Wehypothesizethatthisisduetothecomplexrelationshipsintheinteractiondata,whereaddingmore
historydoesnotenhancetheLLMs’understandingoftheenvironmentdynamics. Formoderatelycomplex
tasks(e.g.,Pendulum),modelperformanceinitiallyincreaseswithasmallhistorysize,consistentwith
ourearlierfindingforpredictingactions. ThisisdemonstratedinthethirdcolumnofFigure17.
18Figure15: ThedynamicsofLLMs’understandingperformancewithincreasinghistorysizefortheMountainCar
task
19Figure16: ThedynamicsofLLMs’understandingperformancewithincreasinghistorysizefortheAcrobottask.
20Figure17: ThedynamicsofLLMs’understandingperformancewithincreasinghistorysizeforthePendulumtask
withdiscretizedactionsinevaluationprompts.
21Figure18: ThedynamicsofLLMs’understandingperformancewithincreasinghistorysizeforthePendulumtask
withcontinuousactionsinevaluationprompts.
22Figure19: ThedynamicsofLLMs’understandingperformancewithincreasinghistorysizefortheLunarLander
task.
23F.4.1 Comparativeperformanceofmodelsonpredictingcontinuousactions
ContinuingfromtheplotofLLMs’performanceonthePendulumtaskwithcontinuousactions(third
rowofFigure3inthemaintext),Figure20presentsacomparativeplotofLLMs’performanceonthe
Pendulumtaskwithdiscretizedactions.
Figure20: AcomparativeplotofLLMs’performanceonthePendulumtaskwithdiscretizedactions,following
theplotofpredictingcontinuousactions(thirdrowofFigure3inthemaintext).
F.5 AblationStudy
F.5.1 Comparisonofmodelswithoutusingtaskdynamics
Figure21illustratestheperformancevariationwhendynamicsequationsareexcludedfromtheprompts.
Figure21: ComparativeplotsofLLMs’performanceonMountainCarwithdifferenthistorysizes(withindexed
historyinprompts). Thesuffixofmodelnames“NoDyna.” indicatesnotusingdynamicsequationsinprompts.
F.5.2 Comparisonofmodelswithoutusingtaskinstructions
Akin to prior works by Mishra et al. (2022); Le Scao and Rush (2021), which show that task framing
in prompt influences language models, we observe a similar effect. When removing task instruction
fromevaluationprompts,models’understandingperformanceacrossthemajorityofevaluationmetrics
issignificantlydegrading,asdemonstratedinMountainCar(Figure22)andAcrobot(Figure23)tasks;
despitethehistorycontext(i.e.,sequenceofnumericalvalues)remainingunchanged. Wehypothesizethat
LLMs’abilitytomentalmodelagentsisenhancedbyamoreinformativecontext.
F.5.3 ComparisonofModels: ActionBinsvs. AbsoluteValuesPrediction
Figure24presentstheevaluationresultsofLLMsonPendulumtasks,comparingpredictionsofaction
bins(thefirsttworows)withpredictionsofabsoluteactionvalues(thelasttworows).
24Figure22: ComparativeplotsofLLMs’performanceonMountainCarwithdifferenthistorysizes(withindexed
historyinprompts). Thesuffixofmodelnames“NoInst.” indicatesnotusingtaskdescriptioninprompts.
Figure23: ComparativeplotsofLLMs’performanceonAcrobotwithdifferenthistorysizes(withindexedhistory
inprompts).Thesuffixofmodelnames“NoInst.” indicatesnotusingtaskdescriptioninprompts.
25Figure24: ComparativeplotsofLLMs’performanceonPendulumwithdifferenthistorysizes(withindexed
historyinprompts). Firstrow: GPT-3.5+predictingbins;secondrow: Llama3-8b+predictingbins;thirdrow:
GPT-3.5+predictingabsoluteactionvalues;fourthrow: Llama3-8b+predictingabsoluteactionvalues.
26G LLMsErroneousResponsesin gain or use momentum in the MountainCar
MountainCarTask task. EventheRLagentoccasionallymakes
suchmistakes.
ExplanationsofVariousErrorTypesinLLMs
Reasoning. A manual review of the Mountain- Aside from the errors, GPT-3.5 demonstrated
CartaskacrossthreeLLMs—GPT-3.5,Llama3-8b, abetterunderstandingofthetask,oftenreferring
andLlama3-70b—revealedsignificantdifferences to the need to accelerate left to gain momentum
intheirexplanationsthatwerenotnecessarilyan- for climbing the right hill. This was rarely men-
ticipated from the quantitative analysis. Table 5 tioned by Llama3-70b and never by Llama3-8b,
providesanoverviewofthetypesandTable1for indicatingGPT-3.5’ssuperiortaskcomprehension
countsoferrorsfoundineachmodel. Duringthe andexplanatoryability. Llama3-70b,however,had
evaluation, a single response could contain mul- anadvantageinmaintainingcoherence,asitwas
tiple error types. Despite Llama3-8b producing lesslikelytocontradictitsarguments,unlikeGPT-
theshortestresponses,italsohadthehighesterror 3.5, which occasionally argued against an action
count. beforeultimatelysupportingit. BothGPT-3.5and
Llama3-8b also displayed misunderstandings of
(1) Thefirsttypeoferror,understandingthetask,
theactions,suchasincorrectlydefining“action0
appeared frequently when the LLMs had to
(noacceleration)”. Thissuggestsacommon-sense
evaluateaproposedaction,suchasnoaccel-
bias toward interpreting 0 as no action. Llama3-
eration in the MountainCar task. All three
70bwasbetteratretainingthetaskdescriptionin
models tended to be concerned about over-
memory.
shooting the goal of reaching a position of
>= 0.5. However, in this task, overshoot-
G.1 ACompactAnalysisofErrorTypes
ing is irrelevant since the goal is to surpass
Table 1 shows a quantitative analysis of the fre-
0.5. Similarrepliesacrossmodelssuggestthis
quency of different error types committed by the
mistakestemsfromasharedcommon-sense
LLMs for the MountainCar task. The evaluation
notion. Additionally,Llama3-8boftenfailed
highlighted various types of errors (see Table 5
torecognizethepresenceofahillontheleft
intheAppendix),withLlama3-8bdisplayingthe
side.
most errors despite its shorter responses. A com-
(2) LogicalmistakeswerenotedinGPT-3.5and mon error among all models was misinterpreting
Llama3-70bwhentheLLMsjustifiedmoving the goal of the task, reflecting a shared common
leftwithoutrecognizingtheneedforoscilla- sense misunderstanding. Logical errors, particu-
tiontogainmomentum,leadingtoparadoxi- larlyinoscillationmovements, wereprevalentin
calreplies. Thesetypesoferrorsweremore GPT-3.5 and Llama3-70b, while Llama3-8b fre-
prevalentinLlama3-8b. quently produced paradoxical replies. Misunder-
standing the task history and physical principles
(3) Misunderstandingthehistoryreferstotheoc-
was rare but present. Mathematical errors, espe-
casionalmisinterpretationorincorrectrepeti-
ciallydisregardingtheminussign,occasionallyim-
tionofthehistoryprovidedtotheLLMs.
pactedreasoning. Notably,GPT-3.5demonstrated
(4) Physical misunderstanding, though rare, in- abettertaskunderstandingbyreferringtomomen-
volved incorrect responses regarding the ef- tumstrategiesinthetask,aninsightlessfrequently
fects of acceleration on velocity and similar ornevermentionedbyLlama3-70bandLlama3-8b,
cases. respectively. Llama3-70bdidhaveoneotheradvan-
tageoverothermodelsasitwaslessoftenconfused
(5) Mathematicalerrorscommonlyinvolvedthe by its argument and excelled in maintaining task
LLMs disregarding the minus sign, leading descriptions. Despiteoccasionalerrorsindefining
themtobelievethat-0.5iscloserto0.5than actions,GPT-3.5’ssuperiorcomprehensionofthe
0.3. Althoughthesemistakesledtoawkward taskcontributedtoitshigher-qualityexplanations.
reasoning,theyseldomsignificantlyworsened
thefinaldecision.
(6) A common and human-like error involved
judging when to switch directions to either
27