Multimodal Reaching-Position Prediction for
ADL Support Using Neural Networks
Yutaka Takase1* and Kimitoshi Yamazaki1
1Mechanical System Engnieering, Shinshu University, Wakasato 4-17-1,
Nagano, 3808553, Nagano, Japan.
*Corresponding author(s). E-mail(s): yutaka takase@shinshu-u.ac.jp;
Contributing authors: kyamazaki@shinshu-u.ac.jp;
Abstract
This study aimed to develop daily living support robots for patients with hemi-
plegia and the elderly. To support the daily living activities using robots in
ordinaryhouseholdswithoutimposingphysicalandmentalburdensonusers,the
system must detect the actions of the user and move appropriately according to
theirmotions.Weproposeareaching-positionpredictionschemethattargetsthe
motion of lifting the upper arm, which is burdensome for patients with hemi-
plegia and the elderly in daily living activities. For this motion, it is difficult
to obtain effective features to create a prediction model in environments where
large-scalesensorsysteminstallationisnotfeasibleandthemotiontimeisshort.
We performed motion-collection experiments, revealed the features of the target
motion and built a prediction model using the multimodal motion features and
deeplearning.Theproposedmodelachievedanaccuracyof93%macroaverage
and F1-score of 0.69 for a 9-class classification prediction at 35% of the motion
completion.
Keywords:reaching-positionprediction,multimodalnetworks,dailylivingsupport
1 Introduction
With an aging society, the demand for intelligent robots to support activities of daily
life (ADL) for elderly and disabled people living alone is increasing. These robots are
required to work close to human users in environments that are relatively narrow and
difficult to sense, in contrast with industrial robots, which work in well-controlled
1
4202
nuJ
62
]OR.sc[
1v26181.6042:viXraenvironments of factories or warehouses. The influence of the support provided by
the autonomous robot on the mental health of the user must also be considered. For
example, is not always appropriate for the assisting robot to fully support a user who
intends to reach a distant object by picking up the object and bringing it to the user.
In this case, despite the intention of users to move on their own, the support ends
upunderminingtheintention.Therefore,itisessentialfortheADLsupportingsystem
nottoinhibittheactivemotivationandself-efficacyofusers(Bandura(1978);Stewart
et al (2019). In Stewart et al (2019)), the authors reported a correlation between SE
evaluation scores and movement speed and accuracy for reaching motions of patients
with residuals from stroke.
This study aims to develop an autonomous ADL support robot, considering the
intentionsoftheuser.Wefocusoncooperationtaskcompletionwithuserstomaintain
and improve their self-efficacy. In the previous example, the robot could support the
arm of the user to reach out to pick up the object or move the target object into
an easier-to-pick position. Such a support system, not only maintains the self-efficacy
of the users but also improves it through the experience of accomplishing tasks that
would be difficult to accomplish alone.
To achieve the goals of the study, in this paper, we propose a novel scheme to
predict reaching position in reaching motion involving upper-arm lifting. Although
lifting the upper arm is an essential part of ADL, such as picking up an object from
a high place, putting it up, or drying laundry, it is difficult for elderly or disabled
patientsbecauseoftheneedtomaintaintheirarmsathighpositions,whichmaycause
an imbalance in the torso. There are many possible actions that the assisting robot
can perform to support the motions, such as directly supporting the arms and torso
of the user and interfering with objects that are the target of movements of the user.
Therefore, establishing a prediction scheme for this motion along with an analysis
ofthefeaturesofthemotionwillbeusefulforbothhardwareandsoftwaredevelopment
of support robots.
The main contributions are summarized as follows:
• We collected the motion data of lifting upper arm, which imitates object grabbing
using multiple sensors, and analyzed the motion features.
• We created a multimodal-neural-network model to predict the reaching position as
a classification problems that can be adapted to real-time robot control.
Theremainderofthispaperisstructuredasfollows:Inthenextsection,weintroduce
related works. In Section 3, we defined our research problems and approaches. In
Section 4, we describe the motion data collection and analysis. Then, we proposed
our multimodal reaching-position prediction network in Section 5. The results and
discussion are presented in Sections 6 and 7. Finally, Section 8 concludes the paper.
2 Related Work
There are various approaches to developing cooperative robots. Research themes in
this area are shifting from proposing robot motion generation methods to developing
human-motion estimation frameworks. For tasks where robots and humans work in
2proximity,asinthisstudy,therearesystemsaimedatsharingtheworkspacetoavoid
interference with each other, and systems aimed at cooperating when performing a
single task, like in handover or load-sharing tasks.
Assembly tasks at a factory are typical scenes of cooperative tasks between robots
andhumans(e.g.Araietal(2020);Mainpriceetal(2015)).Inthecaseofarobotand
humans sharing a workspace and working individually, the robot must predict their
motion trajectory to avoid collisions with humans. In Arai et al (2020), the authors
addressedtheproblemofestimatingthereachingmotionofhumanworkersasamulti-
classclassificationproblem.Theyreportedthattheaccuracyoftheproposedmethod,
which used 3D point-cloud data, was around 80 % after 50% of the operation. In
Mainprice et al (2015), the authors collected data on the reaching motions of workers
using a motion capture system and used the data to predict the arm trajectory of
human worker. The construction of advanced sensor environments is beneficial in
factory and laboratory environments.
In research on handover tasks, which require the positions of robots and humans
to be close, sensors such as voice and electromyography sensors are used to predict
the trajectory of a workers’ arms (Wang et al (2021); Wu et al (2019)).
Intheareaofhuman-robotinteraction,manystudieshavebeenconstructedmodels
topredictuseractivitiesandintentionsusingvariousfeaturesaswellasthemovement
of the users to achieve a natural interaction between humans and robots that is simi-
lartohumanandhumaninteraction.Forexample,in?Zadehetal(2016),theauthors
proposed emotion estimation models using the facial expressions and verbal features
of the user. In Wang et al (2021), the authors proposed a deep-neural-network model
to estimate the order of the users for the robot by using both verbal and nonverbal
features. In Yuguchi et al (2019), the user intention to service robots was estimated
usingfacialdirection.Althoughusinghumannaturalmotions,suchasfacialdirection,
seems effective in informing the intention or purpose of the motion to systems, the
system we aim for, as described above, targets supporting daily life activities accord-
ing to the actions of the user. Therefore, it is not appropriate to build complex sensor
systems for trajectory tracking in the home or give voice instructions to robots like ”I
want to get the book on the upper right shelf.” In this study, we address these prob-
lems by using simple sensor systems. In addition, we deal with the reaching-position
predictionproblemforupper-armliftingmotionasamulti-classclassificationtaskand
create a novel model that uses multi-nonverbal features. The proposed method could
be applied in the future to load-sharing tasks (DelPreto and Rus (2019); Sirintuna
etal(2022).Currently,studiesfocusoncontrolmethodsandalgorithmsafterhumans
androbotsholdtheload;however,thisstudyproposesoneapproachtotheimportant
problem of how a robot can hold objects together according to human intentions.
In the following section, we present our research questions and approaches.
3 Research questions and Our approaches
3.1 Research questions
This study addressed two research questions: First, we investigated the practical fea-
tures of upper-arm lifting motion to construct a reaching-position prediction model;
3second, we built a neural-network model using the features. Considering our goal,
we assumed the following use environment and scenario: The system would be used
in the everyday household environment, the users would be patients with hemiplegic
and older adults with weakened muscles, and the support system should operate
autonomously,andavoidingcompromisingtheself-efficacyoftheuserbynotproviding
full support.
Foraspecifictask,assumethattheusertakesanobjectfromashelfwithahealthy
arm. The system recognizes the reaching position of the motion and interacts with
the user’s arm and the object to be grabbed. This means that the system supports
the task by, for example, keeping the arm or torso or moving the object to a position
that is easier to grasp.
Basedontheseassumptions,theproposedmethodhasthefollowingrequirements.
• It deals only with available information without installing or attaching large sensor
systems to the user or environment.
• The proposed method assumes that the support system works autonomously
without active manipulation by the user for operation.
• It provides an environment in which the user does not have to wait for support or
adjust their operating speed.
3.2 Approach
Under the conditions described above, our approach to investigating the research
question is as follows. First, we collect target motion data of multiple subjects in an
assumedenvironment.Next,thefeaturesofthemotionsareselectedfromthecollected
data, which are considered adequate for constructing a prediction model. Finally, as
in Arai et al (2020), we constructed a prediction model of the reaching position as a
multi-class classification problem using deep learning and evaluated its performance.
Thenextsectiondescribesthedatacollectionmethod,itsfeatures,andthefeatures
that can be used to predict the arrival position.
4 Analysis of reaching motions
4.1 Motion collection
Fig. 1 shows the environment settings for the data collection. The motion data collec-
tion procedure is as follows. As illustrated in the figure, the participant sat on a chair
in front of a shelf divided into nine regions. The participant performed the motion of
grabbing things from the area randomly indicated by the experimenter. Every indica-
tionwaspresentedvisuallyaftera3-secondcountdowninoneregiontobedetermined
instantly on a display set in front of the participant. One set of trials consisted of
four randomly ordered motions to each region repeated four times, and all partici-
pants sequentially performed seven sets of trials. The participants were instructed to
place their right hands on their knees and face the display in front of them during
the countdown. The sensors used were an RGBD camera (Microsoft, Azure Kinect)
installed in front of the participant and an inertial measurement unit (IMU) sensor
4Shelf
Shelf
1 2 3
Top Left Top Center Top Right Top Left Top Center Top Right
Center Left Center Center Right 17 cm
4 5 6 1177 cm
Center Left Center Center Right
������cm Bottom Left Bottom Center Bottom Right
Botto7m Left Bottom8 Center Botto9m Right
Display
RGBD camera
160 cm
IMIMUUセ seンnsサor
IMU sensor
160
160 cm 40 cm
cm Participant
9900 ccmm
Fig. 1 Overviewofdatacollectionenvironment
RRGGBBDDカ cメamラ
Table 1 Descriptivestatisticsofthereachingmotionstime(s)bytarget
region
User
Region N Mean Median Max Min SD
Top-left(TL) 173 1.56 1.56 2.20 1.10 0.230
Top-center(TC) 169 1.56 1.50 2.28 0.969 0.295
Top-Right(TR) 171 1.58 1.56 2.43 1.07 0.267
Center-left(CL) 174 1.45 1.46 2.77 1.00 0.260
Center(C) 172 1.42 1.37 2.10 0.900 0.242
Center-right(CR) 170 1.48 1.47 2.38 0.902 0.289
Bottom-left(BL) 171 1.44 1.45 1.97 0.883 0.219
Bottom-center(BC) 170 1.33 1.33 1.93 0.868 0.227
Bottom-right(BR) 170 1.37 1.35 2.57 0.732 0.285
(MicroStrain, 3DM-Gx5-45) attached to the right arm of participants. Color (resolu-
tion: 1280 × 720, field of view: 90◦ × 59◦) and depth images (resolution: 640 × 576,
field of view: 75◦ × 65◦) were acquired from the RGBD sensor at 15 frames per sec-
onds(FPS),andmagnetometer,angularvelocity,andaccelerationdatawereobtained
from the IMU sensor at 100 FPS.
Six able-bodied male participants (aged 22-25, all right-handed) were recruited
from our laboratory. Excluding data recording failures, the effective number of data
samples was 1538. Fig. 3 shows an example of the collected data: the sequence of
reaching motion to the center-left region. The numbers indicate the elapsed frames
from the start of the motion.
4.2 Motion analysis
Table 1. shows the descriptive statistics for the reaching times to each region derived
from the collected data. Here, the reaching time is measured based on video data,
5from the moment the participant starts the movement after the target region is indi-
cated by the display to when the extended right arm becomes stationary. Therefore,
thetimeittakesforthevisualreactionisnotincluded.One-wayANOVAandthepost
hoc Tukey HSD test ( p < .05 was considered significant ) were conducted and sug-
gestedthatthereweresignificantdifferencesbetweentheregions(F(8,1529)=19.87,
p < .001). All the post hoc test results are shown in Fig. 5, which will be discussed
later. From the results, reaching the uppermost regions, which are the farthest from
the right hand’s initial position, top-left (TL), top-center (TC), and top-right (TR),
required approximately 1.56, 1.56, and 1.58 s, respectively. And, there was no signifi-
cantdifferenceobservedbetweenthem.Similarly,nosignificantdifferencewasobserved
among the middle regions, center-left (CL), center (C), and center-right (CR). These
resultssuggestthat,withinthisexperimentalsetup,participantsunconsciouslyadjust
their movement speed to reach regions at the same height. This adjustment equals
modulating the waiting time until the next movement’s target position is presented.
Therefore, it might be influenced by experimental conditions. On the other hand, for
the bottom regions, reaching the bottom-center (BC), which is located /directly in
front of the body, was the fastest, with an average time of about 1.33 s. According to
the post hoc test results, there was no significant difference between BC and bottom-
right (BR). However, significant differences (p<.05) were observed between BC and
the bottom-left (BL), where required to extend the right arm to the front-left. This
suggests that such a movement appears to be particularly difficult, even for healthy
individuals.Additionally,itwasobservedthatthemaximumreachingtimetoBRwas
relativelylargercomparedtotheotherbottomregions.Uponreviewingthevideodata
of this motion, it was noted that, after quickly getting closer to the target location,
participantscontinuedaslowapproachmovementwithoutcomingtoacompletestop.
This data has not been excluded, as it is considered not to affect future analyses or
system development significantly. Thus, in simple reaching movements, while there is
anobservedtendencytounconsciouslyadjustspeed,itbecameclearthatduetopres-
ence of locations significantly more difficult to reach, reaching speed and movements
are not solely determined by simple distance between the arm and the destination.
This study uses the average value of 1.47 s from all data as a guideline for devel-
oping a system to support this task. The proposed system must perform user motion
recognition, predict the reaching position, and provide support actions all within this
time frame.
Additionally, Fig. 2 shows the differential images created using the SAD (Sum of
Absolute Differences) method from color images over 10 frames after the start of the
motion. 2A and 2B are from specific motions extracted from the collected dataset,
while 2C is generated from all data. Bright areas in the images indicate regions of
significant movement within the frames. The collected video data and the differential
image also revealed the following features.
1)Thetargetmotionconsistedofmovementsofspecificbodyparts,thatis,upper
body, right arm, and face, rather than the entire body; in particular, from the dif-
ferential image, the upper body movements appear not to be significant in the initial
phaseofthemotion,makingitseemchallengingtousethisinformationtopredictthe
reaching position. On the other hand, the image shows significant movement near the
6A C
B
Fig. 2 Visualization of variations within 10 frames after the start of the motion using the sum of
absolutedifference(SAD).A,B:Examplesfromthecollectedmotions.C:Resultbasedontheentire
collectedmotions.
face and around the right arm. Therefore, it was considered effective to use the right
arm motion with visual features and to capture changes from the early stage of the
motion together with face direction changes.
2) The preparatory motion was not useful for prediction; in other words, the time
used to predict from the start of the motions directly affected the time available for
the support action, and it was also necessary to recognize the timing of the start of
the motion. However, it was challenging to obtain the exact timing of the start of the
motion due to there was no prior motions. This point is discussed in Section 7 as a
future issue.
4.3 Modals for prediction
Basedontheobservationresultsstatedabove,weselectedtheface,visual,andmotion
featurestoconstructareaching-positionpredictionmodel.Theobservationresultsand
Yuguchietal(2019)suggestedthatthefacedirectionorfeatureswouldbeanessential
cuetoestimatethefollowingmotions.Additionally,tousethissystemuniversally,itis
more appropriate to estimate the reaching position using depth information as visual
cues rather than color image data, which contains redundant external information
relatedtotheuserandtheenvironment.Furthermore,motionfeaturesacquiredusing
theIMUsensorattachedtothehealthysidewristoftheuserwereemployed,because,
considering future robots supporting the user’s arms or grasping objects, it becomes
imperative to understand the three-dimensional movements and postures of the arm.
Althoughtherearemanytechnologiestoestimatehumanposturesbyusingonlycolor
images Cao et al (2021), considering the typical house environment in Japan, it is
difficult to obtain a camera angle of view sufficient to estimate the arm’s posture in
reaching an unspecified direction to a shelf placed in front of the user. Therefore,
estimatedposturedatawerenotused.Eye-trackingdeviceswerealsonotusedtoavoid
complicating the system.
70 4 8 12 16
21 25 29 33 38
Fig. 3 Example of the collected motion data; it is a sequence of color images subject to reach the
center-leftregion.Thenumberindicateselapsedframesfromthestartofthemotion.
4.4 Motion data extraction
In this study, the start and end recognition of the motion was not performed. There-
fore, it was necessary to extract data of each motion from the collected data based
on some criteria. We manually annotated the motion start and end timings based on
the following definition: The motion-start timing was defined as the frame when the
right hand, initially positioned at the knee, began to move. The motion-end timing
was determined as a frame when the extended arm started to retract at the reach-
ingposition.Thecollecteddataweredividedintoindividualmotionsaccordingtothe
annotated timings.
In the next section, we organize the features discussed in this section into specific
feature data and discuss the construction of the reaching-position prediction model.
5 Prediction Model
5.1 Features
This section describes the features used to build our prediction model.
5.1.1 Face Features
Face feature was set with the expectation of capturing the direction of the face,
its variations, and the characteristics of gaze transition. In studies to create predic-
tion models of human behavior, movements of the head and gaze are often used as
features (Holman et al (2021); Yang et al (2023)). We attempted to capture such fea-
tures without attaching sensors to the users. In this study, the face mesh data was
employedasfacefeatures.WeusedGoogleMediapipe(Lugaresietal(2019))toobtain
468 3D face landmark positions from the color images. The time elapsed from the
start time of the motion was added to each frame, resulting in data of 1405 (468 × 3
+ 1) dimensions.
85.1.2 Depth Features
Depthfeaturesweresetwiththeexpectationofextractingthethree-dimensionalchar-
acteristicsofmovementswhilereducingdependencyonclothingandtheexperimental
environment. These features are valuable for understanding the user’s position and
posture in future support scenarios. The depth image data was acquired at 15 FPS
andcroppedtotheusercenter.Theresolutionwasreducedto256×188.Theelapsed
time was also added to each frame, as described below.
5.1.3 Motion Features
Motion features were set to capture the characteristics of rapid three-dimensional
movementsofthearm.Asthemotionfeatures,thedatafromtheIMUsensorattached
totherightwristprovidedtendimensionsofinformation(geomagnetism,acceleration,
and angular acceleration). The elapsed time was also added to the data to obtain 11
dimensions.
5.2 Network structure
We constructed a multimodal 9-class classification neural-network model to predict
reachingpositionsasseeninSection4.Thelongshort-termmemory(LSTM)(Hochre-
iter and Schmidhuber (1997)), and the local attention mechanism (Vaswani et al
(2017)) were used to construct our machine-learning model. The network structure is
showninFig.4.Asshowninthefigure,theoutputfromallmodallayersarecombined
through late fusion (Gadzicki et al (2020); Sun et al (2021)). This is the method used
inmodelingmultimodalinformation.Thecompositionofeachunimodalnetworkisas
follows.
5.2.1 Face layers
A bi-directional LSTM layer with 1405-dimensional input and 2048-dimensional out-
put was employed to train the face modal. The final output data passed through
a self-attention layer and was output as 2048-dimensional data. The number of
parameters in the network was 44,554,241.
5.2.2 Motion layers
Thesamestructureasthefacemodelwasusedtotrainthemotionmodel.Thenumber
of input dimensions was 11, and the number of output dimensions was 512. It had
2,139,137 network parameters.
5.2.3 Depth layers
Depthfeatureswerelearnedbycombininglatentrepresentationofdepthimagesusing
a convolutional neural network (CNN) and time-series learning using LSTM (Mou
et al (2021)). The CNN parameters used are shown in Table 2. The CNN + LSTM
network had a total of 204,509,720 parameters, and the latent representation of each
frame of the CNN data was combined with the elapsed time, described earlier, and
input to the LSTM layer.
9Face model
Motion model
Depth model
Input LSTM Attention Convolution Pooling Classification
Fig. 4 Multimodallatefusionmodel
Table 2 CNNmodelparametLeSrTMsfordepthnetwork
1DCNN
Convolutionallayer Filter=8,kernel-size=(16,16),stride=1
Max-pooling+Dropout pool-size=(2,3)
Convolutionallayer Filter=16,kernel-size=(8,8),stride=1
Max-pooling+Dropout pool-size=(8,8)
Fullyconnectedlayer input-size=25088,output-size=2047
5.2.4 Classification layers
The three output vectors from the unimodal layer were simply combined into a 1 ×
4097-dimensionalvector.Theyweretheninputtoafullyconnectedlayerwithdropout
at each layer. The dimension of the last output layer was nine, the number of classes.
The dropout rates were set to 0.6, 0.4, and 0.2, respectively, and the ReLU function
was used as the activation function.
5.3 Input frames
As shown in Table 1, the target motion time for this task was a minimum of aproxi-
mately 1.33 s to complete the motion. Even if we disregard the movement time of the
support robot, we still need a prediction of a shorter time to assist the robot’s move-
ment. The time used for the prediction was set to 0.5 s (7 frames for the face and
depth models and 50 frames for motion model). This means the model used informa-
tionfrom32%to36%ofthemotiontime.Thisisashortpredictiontimecomparedto
the previous study (Arai et al (2020)). To improve the performance of the prediction
model,datainterpolationshouldbeperformedformissingdata.However,considering
real-timeuse,therawdataobtainedfromthesensorsshouldbeinputtothepredictor
with as little processing as possible. Therefore, data shaping was kept to a minimum.
Forexample,frameswherefacemeshcouldnotberecognizedwerepaddedwithzeros.
10Table 3 SinglevsMultimodalmodelperformance
comparison
Model Accuracy F1-score Precision Recall
faceModel 0.91 0.59 0.60 0.59
imuModel 0.93 0.66 0.66 0.68
depthModel 0.92 0.64 0.65 0.64
FusionModel 0.93 0.69 0.69 0.69
* * * * *
TL
* *
CL
* * * *
BL
* * *
TC
* * *
C
* * * * * *
B C
* * * * *
T R
* * * *
C R
* * * * *
B R
TL CL BL TC C B C T R C R B R
Predicted Class
Fig. 5 Confusion matrix of the proposed fusion model. the percentages represent classification
rate.The asterisks indicate pairs for which a significant difference in motion speed was found as a
resultofthepost-hocTukeyHSDtestdescribeinSection4.2.
In the next section, we discuss the result of the training and the features of our
prediction model.
6 Evaluation and Result
6.1 Model performance
For training and evaluation the proposed late fusion model, firstly, the whole dataset
is randomly divided into the training set and test set with 1341 and 197 motions,
respectively. Then, we trained the model using a 10-fold cross-validation strategy.
Table 3. shows model accuracy and Macro Precision, Recall, and F1-score, which are
used to evaluate multi-class classification problems, of the fusion model and each uni-
modal model for comparison. These values are obtained by calculating the scores for
each class in a one-vs-rest manner and then averaging these results across all classes.
11
ssalC
lautcAThe unimodal models were trained only using LSTM (CNN-LSTM) and a classifier
structure for each model in the Fig. 4. The results found that the proposed model
performs well or better than other unimodal models. In particular, the fusion model
has the highest F1-score of 0.69, indicating that it was the most balanced model. In
a previous study addressing a similar 9-class reaching-position prediction problem, an
accuracy of 80% was reported at approximately 50% of the motion completion time.
In contrast, our method achieved higher accuracy (93 %) at an earlier stage of the
motion (32%). Fig. 5 shows a confusion matrix obtained from 197 test data. The
rows represent the actual classes and columns represent the predicted classes by the
fusion model. The percentages represent the precision of the classification results for
each class. For example, data classified as TL are correctly classified with an 88.0%
probability; however, it shows a 4.0% probability of ML, BL, and BC motions being
incorrectly classified as TL. The results show that the precision increases from the
right side, near the starting point of motion, towards the upper left. Also, as a trend
across the classes, there is some confusion within the same column. In particular, the
precision for the middle row was relatively low, often confused with motions to the
same column. This is an interesting result, suggesting that even from the initial few
frames, movements towards the top and bottom regions are distinctively characteris-
tic. Distinguishing whether the arm stops in the middle row or continues moving up
ordownisdifficultforthecurrentmodel.However,theresultsfromthepost-hoctest,
conductedinSection4,alsoshowpotentialforclassification.Theasterisksinthefigure
indicate pairs where a p < .05 significant difference in motion speed was observed in
thepost-hocTukeyHSDtest.Fromtheresult,significantdifferenceswereobservedin
the final reaching motion time between CR and TR, and CR and BR, indicating that
there are differences in arm motions between them. Even if there are differences in
the current input frames regarding movement trajectories, this model, which predicts
reaching positions by combining arm, face, and depth features, has cases where clas-
sification may fail due to factors other than motion features. To analyze the factors
causing differences in reaching times, arm trajectories and features must be analyzed,
andneuralnetworkscapableofextractingthesedatamustbeconstructed.Thesetasks
remain for future research. Additionally, it is impossible to avoid the possibility of
misclassification completely. Based on these results, the operation of future support
systems will be discussed in Section 7.
6.2 Estimation speed
Finally,wemeasuredtheestimationspeedoftheproposedmodel—with282,930,211
parameters.ThecomputerusedforinferencewasUbuntu20.04.6LTSforOS,Intel(R)
Xeon(R) W-2225 @ 4.10GH CPU, NVIDIA RTX A5000 and 24GB RAM. The input
data utilized the motion data collected in Section 4. These data are stored using the
functionalities of Robot Operating System Quigley et al (2009), allowing for the sim-
ulationofreceivingcameraimagesandIMUsensordatawhilemaintainingtimestamp
information. However, delays such as the camera’s image acquisition time or data
transmission between the sensor and the computer are not considered. The measure-
ment program measured the time from when it collected the number of input frames
12ofsensordatathroughtheconversionandtrimmingintoaformatsuitableforthepre-
dictive model to obtain the prediction results. The prediction model is trained using
PyTorch 1 and optimized with Nvidia TensorRT 2. The average prediction time for
100 data inputs was 0.0086 s, with a standard deviation of 0.0036 s. The maximum
value was 0.022 s, and even if this worst-case scenario is adopted, the time required
forestimationisapproximately1.5%ofthemotiontimeofourcollectedmotiondata,
which is considered sufficiently small. This result indicates that the proposed method
requires a prediction time of approximately 0.5 s (for collecting motion data) + 0.086
s (for estimating the target position) for a reaching motion that takes approximately
1.47 s on average. Therefore, the proposed system leaves appropriately 0.96 s of grace
time for the support robot.
7 Limitations and Discussions
In this section, we elaborate on insights encountered while conducting data collec-
tion, creating the prediction model, and analyzing the results. Due to the COVID-19
pandemic, it was impossible to recruit a sufficient number and variety of partici-
pants. Whether the motion of people who have a stroke or have advanced age is
the same as that of the participants should be considered in further investigations.
InCoderre et al (2010); Scott and Dukelow (2011),theauthorsconductedareaching-
tasks experiment for mostly right-handed patients with hemiplegia. They report that
there is no difference in the motor function of the unaffected arm between left- and
right-affected patients. However, in comparisons between these patients and elderly
individuals without paralysis, it is shown that patients with paralysis exhibited infe-
rior motor function even in the unaffected arm. Furthermore, In Heller et al (1987),
it is shown that the arm motor functions of healthy elderly individuals differ depend-
ing on the dominant arm. From this, while it may seem difficult to directly apply the
proposedmodelorthecollecteddatafromhealthyindividualsinthisstudytosupport
a system intended for elderly or hemiplegic patients, the requirements for assistive
systems identified through this study, along with the series of methods for model cre-
ation, can be considered useful. Using the proposed model, it would be possible to
realize a system where support robots autonomously operate triggered by the user’s
activemovements,supportingthecompletionofusertasks.InPhillips et al (2013),the
authorsreportedthatthereisacorrelationbetweenphysicalactivityandself-efficacy,
or life satisfaction in the elderly. On the other hand, there have also been reported
that elderly individuals have psychological barriers to engaging in physical activities
in the first place (Lee et al (2008)). Support systems in ADL can encourage active
movements from familiar activities, as seen in this task, reduce psychological barriers
to physical activity, and promote more extensive social activities. To achieve this, the
support system aims to appropriately assist users’ activities in daily life, enhancing
their self-efficacy and motivation for active engagement. For this purpose, one of the
future challenges is to enable support at multiple levels based on the user’s physical
condition, from simple arm support to higher-level assistance like directly retrieving
1PyTorch:https://pytorch.org
2NvidiaTensorRT:https://developer.nvidia.com/tensorrt
13objects and handing them to the user’s extended hand. As mentioned in Section 6,
it is impossible to eliminate the possibility of misclassification when the classification
model is used in the wild. While it is important to improve model performance, it is
equally crucial to build and operate a system that is robust against misclassification.
The proposed model is expected to improve in accuracy with increased input frames
(i.e., as the motion progresses). Additionally, the results from Fig. 5 suggest that the
proposed model achieves very high accuracy in the 3-class classification of rows (Left,
Center, Right), with respective accuracies of 97.15%, 96.60%, and 93.47%. Hence,
in the actual operation of support robots, for instance, at the beginning of a move-
ment, the robot might perform lateral shifts, and as time progresses, it could execute
more detailed movements based on predictive results. For interactions with people or
objects, it would be practical to utilize proximity sensors equipped on the robot for
precise positional adjustments. However, it is difficult to say that the current model
ensures sufficient time for actual support operations. As seen in Section 6.2, the time
available for assistance is only about 0.96 s, which is clearly insufficient for a sta-
tionary robot to approach a user or target object and provide support. Therefore, to
achieveourgoal,wemustsolveproblemsfrommanydirections,suchasestablishinga
fast support method, developing a soft robot that considers collision with humans or
surrounding objects, and integrating these technologies, including this study. These
are challenges for future study.
8 Conclusions
Weproposedanovelschemeforconstructingareaching-positionpredictionmodelfor
the reaching motion involving upper-arm lifting, which is part of activities of daily
living (ADL), to develop a support robot. Based on the results of the motion col-
lection experiment and its analysis, we developed a target position prediction model
usingtime-seriesdataofface,motion,anddepthfeatures.Theproposedmodel,which
demandsthatthesupportsystemautonomouslyoperatestriggeredbytheuser’smove-
ments using data from simple sensors, achieved an accuracy of 93% at 35% of the
motioncompletiontime.Thismodel,utilizingonly0.5sofdata,wasabletomakepre-
dictions in approximately 0.086 s of computation time. However, it is difficult to say
thatsufficienttimehasbeensecuredfortheoperationofsupportrobots,andtheissue
of misclassification needs to be resolved to adapt the classification model in the wild.
Inthefuture,alongwithimprovingpredictionaccuracy,weaimtodeveloprobustsup-
port methods against misclassification and robots that support ADL in close contact
with users, striving to realize the proposed system.
Declarations
Availability of data and materials
The datasets used and/or analysed during the current study are available from the
corresponding author on reasonable request.
14Competing interests
The authors declare that they have no competing interests.
Funding
This work is supported by JST [Moonshot R&D],[Grant Number JPMJMS2034]
Ethical Approval and consent to participate
Ethicalapprovalwasnotrequiredasperinstitutionalguidelines.Allparticipantswere
informed about the purpose of the study, the anonymity and confidentiality of their
results, and provided informed consent prior to participation.
Authors’ contributions
Y.T. worked on the research concept, participated in the design and development
of the method, and drafted the thesis. K.Y. participated in the research design. All
authors reviewed the results and approved the final version of the manuscript.
Acknowledgement
Not applicable.
References
Arai S, Pettersson AL, Hashimoto K (2020) Fast prediction of a worker’s reaching
motion without a skeleton model (F-PREMO). IEEE Access 8:90340–90350
Bandura A (1978) Self-efficacy: Toward a unifying theory of behavioral change.
Advances in Behaviour Research and Therapy 1(4):139–161
Cao Z, Hidalgo G, Simon T, et al (2021) OpenPose: Realtime Multi-Person 2D pose
estimationusingpartaffinityfields.IEEETransPatternAnalMachIntell43(1):172–
186
Coderre AM, Amr Abou Zeid, Dukelow SP, et al (2010) Assessment of upper-limb
sensorimotor function of subacute stroke patients using visually guided reaching.
Neurorehabil Neural Repair 24(6):528–541
DelPreto J, Rus D (2019) Sharing the load: Human-Robot team lifting using muscle
activity. In: 2019 International Conference on Robotics and Automation (ICRA).
IEEE, pp 7906–7912
Gadzicki K, Khamsehashari R, Zetzsche C (2020) Early vs late fusion in multimodal
convolutional neural networks. In: 2020 IEEE 23rd International Conference on
Information Fusion (FUSION). IEEE, pp 1–6
15Heller A, Wade DT, Wood VA, et al (1987) Arm function after stroke: measurement
andrecoveryoverthefirstthreemonths.JNeurolNeurosurgPsychiatry50(6):714–
719
Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput
9(8):1735–1780
Holman B, Anwar A, Singh A, et al (2021) Watch where you’re going! gaze and head
orientation as predictors for social robot navigation. In: 2021 IEEE International
Conference on Robotics and Automation (ICRA), pp 3553–3559
Lee LL, Arthur A, Avis M (2008) Using self-efficacy theory to develop interven-
tions that help older people overcome psychological barriers to physical activity: a
discussion paper. Int J Nurs Stud 45(11):1690–1699
Lugaresi C, Tang J, Nash H, et al (2019) MediaPipe: A framework for perceiving and
processing reality. In: Third Workshop on Computer Vision for AR/VR at IEEE
Computer Vision and Pattern Recognition (CVPR) 2019
MainpriceJ,HayneR,BerensonD(2015)Predictinghumanreachingmotionincollab-
orative tasksusing inverse optimalcontrol anditerative re-planning.In: 2015IEEE
International Conference on Robotics and Automation (ICRA), vol 2015-June.
IEEE, pp 885–892
Mou L, Zhou C, Zhao P, et al (2021) Driver stress detection via multimodal fusion
using attention-based CNN-LSTM. Expert Syst Appl 173:114693
Phillips SM, W´ojcicki TR, McAuley E (2013) Physical activity and quality of life in
older adults: an 18-month panel analysis. Qual Life Res 22(7):1647–1654
Quigley M, Conley K, Gerkey B, et al (2009) ROS: an open-source robot operating
system. In: ICRA workshop on open source software
Scott SH, Dukelow SP (2011) Potential of robots as next-generation technology for
clinical assessment of neurological disorders and upper-limb therapy. J Rehabil Res
Dev 48(4):335–354
SirintunaD,GiammarinoA,AjoudaniA(2022)Human-Robotcollaborativecarrying
of objects with unknown deformation characteristics. In: 2022 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS), pp 10681–10687
Stewart JC, Lewthwaite R, Rocktashel J, et al (2019) Self-efficacy and reach per-
formance in individuals with mild motor impairment due to stroke. Neurorehabil
Neural Repair 33(4):319–328
Sun L, Xu M, Lian Z, et al (2021) Multimodal emotion recognition and sentiment
analysis via attention enhanced recurrent model. In: Proceedings of the 2nd on
16Multimodal Sentiment Analysis Challenge. Association for Computing Machinery,
New York, NY, USA, MuSe ’21, pp 15–20
Vaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you need. In: Guyon
I, Luxburg UV, Bengio S, et al (eds) Advances in Neural Information Processing
Systems, vol 30. Curran Associates, Inc.
Wang H, Li X, Zhang X (2021) Multimodal human-robot interaction on service
robot.IEEEAdvancedInformationTechnology,ElectronicandAutomationControl
Conference (IAEAC) pp 2290–2295
WuM,TaetzB,DickelSaraivaE,etal(2019)On-linemotionpredictionandadaptive
control in Human-Robot handover tasks. In: 2019 IEEE International Conference
on Advanced Robotics and its Social Impacts (ARSO), pp 1–6
Yang B, Huang J, Chen X, et al (2023) Natural grasp intention recognition based on
gaze in Human-Robot interaction. IEEE J Biomed Health Inform 27(4):2059–2070
YuguchiA,InoueT,RicardezGAG,etal(2019)Real-Timegazedobjectidentification
with a variable point of view using a mobile service robot. In: 2019 28th IEEE
International Conference on Robot and Human Interactive Communication (RO-
MAN). IEEE, pp 1–6
Zadeh A, Zellers R, Pincus E, et al (2016) Multimodal sentiment intensity analysis in
videos: Facial gestures and verbal messages. IEEE Intell Syst 31(6):82–88
17