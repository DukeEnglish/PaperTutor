Learning pure quantum states (almost)
without regret
Josep Lumbreras1,∗ Mikhail Terekhov2,† and Marco Tomamichel1,3‡
1Centre for Quantum Technologies, National University of Singapore, Singapore
2 School of Computer and Communication Sciences, EPFL, Switzerland and
3Department of Electrical and Computer Engineering,
Faculty of Engineering, National University of Singapore, Singapore
(Dated: June 27, 2024)
We initiate the study of quantum state tomography with minimal regret. A learner has
sequentialoracleaccesstoanunknownpurequantumstate,andineachroundselectsapure
probe state. Regret is incurred if the unknown state is measured orthogonal to this probe,
and the learner’s goal is to minimise the expected cumulative regret over T rounds. The
challengeistofindabalancebetweenthemostinformativemeasurementsandmeasurements
incurring minimal regret. We show that the cumulative regret scales as Θ(polylogT) using
a new tomography algorithm based on a median of means least squares estimator. This
algorithm employs measurements biased towards the unknown state and produces online
estimates that are optimal (up to logarithmic terms) in the number of observed samples.
1. INTRODUCTION
Quantum state tomography is one of the most fundamental tasks in quantum learning, playing
a critical role in the characterization and validation of quantum states. Given t copies of some
unknown d-dimensional quantum state ρ, the goal of a quantum state tomography algorithm is to
decide which measurements to perform on ρ and use classical post-processing with the outcomes of
thesemeasurementstoproduceanestimateρˆthatisclosetoρinsomedistancemetric. Twoofthe
most relevant distances for this task are the trace distance and infidelity. The sample complexity
of this problem (i.e., how many copies are sufficient to estimate ρ up to some precision on these
distances) is well understood for both coherent measurements (allowing for joint measurements on
multiple copies of the state) and incoherent measurements (only allowing measurements on single
copies of the state) [9, 13, 23, 27].
For incoherent measurements, the most general algorithms are adaptive. Such algorithms can
sequentially measure copies of ρ, deciding which measurement to perform based on the outcomes
of the previous measurements. While adaptive algorithms are strictly more general than non-
adaptive ones, it was early understood [4, 5, 11] that there is no separation in sample complexity
when learning pure quantum states. Recently, it was shown in [9] that there is no separation in
sample complexity between adaptive and non-adaptive algorithms for trace distance when learning
both mixed and pure quantum states. The only regime where adaptive algorithms outperform
non-adaptive ones is when learning “almost” pure quantum states [5, 15, 21]. For non-adaptive
√
algorithmsitwasknownthattherateΩ(1/ t)isunavoidable[13]; however, in[9]theyconstructed
an adaptive algorithm that achieves the rate O(1/t) both for mixed and pure states.
∗ josep.lumbreras@u.nus.edu
† mikhail.terekhov@epfl.ch
‡ marco.tomamichel@nus.edu.sg
4202
nuJ
62
]hp-tnauq[
1v07381.6042:viXra2
In this work we take a different perspective on the adaptive incoherent setting of quantum
state tomography, where we not only try to learn the state efficiently but also use measurements
that only minimally disturb the state. Specifically, we have sequential access to an unknown pure
quantum state |ψ⟩ and at each round t we select a probe state |ψ ⟩ and perform a measurement
t
in the direction of the probe state. The goal is to efficiently learn the unknown state while per-
forming measurements that align well with the unknown state. Mathematically, this problem
can be modelled as a bandit problem [17] since fundamentally we are interested in optimising an
exploration–exploitation trade-off. The “exploration–exploitation” is one of the most fundamental
concepts in reinforcement learning and decision-making captured by the bandit framework and the
rigorous study was initiated almost one century ago in the early work by Thompson [26]. In our
setting the exploration is related to the learning of the unknown state |ψ⟩ through the selection of
probes that are sufficiently informative, and the exploitation to performing measurements on the
direction of |ψ⟩.
Formally the measurements on the direction of the probe |ψ ⟩ are described by a two outcome
t
rank-1 POVM {Π ,I−Π } with corresponding outcomes r ∈ {1,0} and Π = |ψ ⟩⟨ψ |. In order
t t t t t t
to quantify the penalty that the learner suffers for selecting probes that are orthogonal to the
unknown state we use as a figure of merit the regret, which is defined as
T
(cid:88)
Regret(T) = 1−⟨ψ|Π |ψ⟩, (1)
t
t=1
where F(Π,Π ) = ⟨ψ|Π |ψ⟩ is the fidelity between the environment Π = |ψ⟩⟨ψ| and the selected
t t
probe Π = |ψ ⟩⟨ψ | at time step t ∈ [T]. It is important to note that the regret is defined as
t t t
the cumulative sum of infidelities γ = 1−⟨ψ|Π |ψ⟩, which means that there is a high penalty for
t t
measuring on directions orthogonal to the environment. The goal of the learners is to minimise the
regret and we can frame this task as pure quantum state tomography with minimal regret since
minimizing the individual contributions to the regret 1−⟨ψ|Π |ψ⟩ implies finding Π close to Π in
t t
infidelity distance.
We note that the task of minimizing the regret (1) is captured by the multi-armed quantum
bandit (MAQB) framework initiated in [6, 19], where some of the present authors consider the
exploration–exploitation trade-off when learning properties of quantum states using classical al-
gorithms. The work [19] considers a more general problem where the environment can be mixed
or pure and the measurements are not only restricted to rank-1 projectors. They showed that for
√
almost all cases the regret suffers a square root lower bound Regret(T) = Ω( T) in a worst-case
scenario. However, this bound does not apply to our case and the reason is due to the noise model
of the outcomes given by Born’s rule. For our particular model Born’s rule gives that the outcomes
become more deterministic as |ψ ⟩ gets close to |ψ⟩ and the lower bounds considered in [20] rely
t
on the fact that the variance of the outcomes r can not get arbitrary close to deterministic. Since
t
our problem is closely related to the MAQB problem we name it pure state multi-armed quantum
bandit (PSMAQB) and we use it to study the following questions at the intersection of the fields
of quantum state tomography and linear stochastic bandits.
• Question 1. Can we perform single copy sample-optimal state tomography in infidelity and
achieve at the same time sublinear regret? How much adaptiveness is needed for this task?
It is important to note that adaptiveness plays a huge role for algorithms that try to minimise
theregretofthePSMAQBproblemandpeformsample-optimalstatetomography. Wecouldtryto
adaptoneoftheexistingsample-optimalalgorithmsintheincoherentsettingsuchas[12,13,16]for
the PSMAQB problem but since all these algorithms use fixed basis or randomized measurements3
thiswillleadinevitablytothelinearscalingRegret(T) = O(T)(weomitdimensionaldependences).
In [19] a simple algorithm with one round of adaptiveness was proposed for the PSMASQ problem
√ √
that achieves Regret(T) = O( T) but gives the infidelity scaling γ = O(1/ T). Thus, it is
t
interesting to see how adaptiveness can help in order to keep γ = O(1/T) and at the same time
t
achieve a sublinear regret.
For the qubit case we can relate the PSMAQB problem to a linear bandit with the action set
being the unit sphere S2 = {x ∈ R3 : ∥x∥ = 1} and this motivates the following question that is
2
related to fundamental bounds for linear stochastic bandits with continuous action sets.
• Question 2. Can we break the square root barrier of the regret for the PSMAQB problem?
In [19] it was shown that the MAQB problem can be reduced to a classical linear stochastic
bandit problem [17, Chapter 19] and that we can adapt algorithms such as LinUCB [1, 10, 24] or
√
linear Thompson sampling [2, 3] to achieve Regret(T) = O˜( T), where we omit the dependency
on the Hilbert space dimension. Classically, it is well known that linear bandits with smooth
√
action sets such as the unit sphere have a lower bound of Regret(T) = Ω( T) [24] [17, Chapter
24] and in particular if the unknown state is mixed the same bound applies to the quantum
setting [19]. However, from [19] a lower bound for the PSMAQB problem is missing and this
√
opens the possibility to achieve a better scaling than the T given by the standard classical bandit
algorithms. Achieving a better scaling on the time horizon T for the PSMAQB would imply the
first non-trivial linear stochastic bandit with continuous action sets that breaks the square root
barrier. Breaking the square root barrier will require new algorithms and techniques that take
advantage of the extra structure of the PSMAQB problem.
2. RESULTS AND TECHNICAL CONTRIBUTION
In this work, we provide affirmative answers at the same time to Questions 1 and 2 through the
following Theorem.
Theorem 1 (informal). Given a PSMAQB with an unknown qubit environment Π = |ψ⟩⟨ψ|, we
can find an algorithm that achieves
E[Regret(T)] = O(cid:0) log2(T)(cid:1) . (2)
Also, at each time step t ∈ [T], this strategy outputs an estimator Π of Π with infidelity scaling
t
(cid:18) (cid:19)
1
E[1−F (Π,Π t)] = O(cid:101) . (3)
t
The above results also hold with high probability.
The proof of Theorem 1 is constructive which means that we design a qubit quantum state to-
mography algorithm and perform the theoretical analysis for it. The exact algorithm and Theorem
can be found in Sections 4 and 5. We note that our algorithm is almost fully adaptive since it uses
O(T/log(T)) rounds of adaptiveness. Intuitively, our algorithm needs to be almost fully adaptive
because keeping the scaling 1−F (Π,Π ) = O(1/t) and breaking the square root regret barrier
t
implies that we want to update our measurements Π at each round to get as close as possible to
t
Π. We say that our algorithm is “online” because it is able to output at each time step t ∈ [T]
an estimator with the almost optimal infidelity scaling (3). We provide numerical experiments in
Section 5. Now we sketch the main idea of how our algorithm updates the measurements.4
1. Estimation. At each time step t ∈ [T] we use the past information of measurements
Π ,...,Π and associated outcomes r ,...,r ∈ {0,1}⊗t−1 to build a high probability
a1 at−1 1 t−1
confidence region C for the unknown environment |ψ⟩.
t
2. Exploration-exploitation. A batch of measurements is performed, given by the directions
of maximum uncertainty of C such that they give enough information to construct C
t t+1
(exploration) and also minimise the regret (1) (exploitation).
For the estimation part, we work with the Bloch sphere representation of the unknown state
Π = |ψ⟩⟨ψ| = I+θ·σ whereθ ∈ S2 andforσ wecantakethestandardPauliBasisi.eσ = (σ ,σ ,σ ).
2 x y z
For each action Π , our algorithm performs k independent measurements using the same action,
at
and it builds the following k online weighted least squares estimators of θ,
t
(cid:88) 1
θ(cid:101)t,i = V t−1
σˆ2(a
)r s,ia
s
for i ∈ [k], (4)
s=1 s s
where r ∈ {0,1} is the outcome of the measurement (up to some renormalization) using the
s,i
projector Π with Bloch vector a ∈ S2, V = I + (cid:80)t 1 a aT is the design matrix and
as s t s=1 σˆ s2(as) s s
σˆ2(a ) is a variance estimator of the real variance associated to the outcome r . The key point
s s s
where we take advantage from the structure of the PSMAQB problem is that the variance of the
outcome r associated to the action Π can be bounded as V[r ] ≤ 1 − Tr(ΠΠ ). The idea is
a a a a
that through a careful choice of actions we can make the terms 1/σˆ2(a ) arbitrarily large and
s s
“boost” the confidence on the directions a in the estimators (4) that are close to θ. However,
s
this comes at a price, and is that in order to get good concentration bounds for our estimator
we need to deal with unbounded random variables and finite variance. We address this issue
using the new ideas of median of means (MoM) for online least squares estimators introduced
in [7, 22, 25]. The construction takes inspiration from the old method of median of means [18,
Chapter 3] for real random variables with unbounded support and bounded variance but requires
non-trivial adaptation for online linear least squares estimators. Similarly to the real case we use
the k independent estimators (4) in order to construct the MoM estimator θ(cid:101)wMoM such that we can
t
build a confidence region with concentration bounds scaling as 1−exp(−k). We give the exact
construction in Section 4.1.
For the exploration-exploitation part, we take the ideas that we develop in [20]. We give the
precise action choice in Section 4.2, and here we sketch the main points. We take inspiration from
the optimistic principle for bandit algorithms which in short tells us to choose the most rewarding
actions with the available information. In order to use this idea, we use the confidence region that
webuildintheestimationpartandweselectmeasurementsthatalignwiththe(unknown)direction
of Π. See Figure 1. Our algorithm also achieves the relation 1−F(Π,Π ) = O(1/λ (V )), where
at min t
λ (V ) quantifies the direction of maximum uncertainty (exploration) of our estimator. The
min t
maximum eigenvalue λ (V ) quantifies the amount of exploitation. We can relate these two
max t
concepts through the Theorem we formally state and prove in [20, Theorem 3], which states that
(cid:112)
for our particular action selection choice we have λ (V ) = Ω( λ (V )). Using this relation
min t max t
and a careful analysis, we can show that λ (V ) = Ω(t2) which gives λ (V ) = Ω(t) and the
max t min t
scaling 1−F(Π,Π ) = O(1/t). We emphasize that the key point that allows to achieve the rate
at
λ (V ) = Ω(t) is the fact that the variance estimators σˆ2 can get as close as possible to zero since
min t s
the variance of the rewards of the PSMAQB problems goes to zero if we select measurements close
to Π. To check the optimality of the regret, we derive a minimax expected regret lower bound
based on the optimal quantum state tomography for pure state results in [14]. The proof does not
follow directly from [14], and we have to adapt it to the bandit setting.5
C
t
+at
Π Π(cid:98)t
Π=|ψ⟩⟨ψ|
Π−
at
FIG. 1. The algorithm at each time step outputs an estimator Π(cid:98)t and builds a high-probability confidence
region C (shaded region) around the unknown state Π = |ψ⟩⟨ψ| on the Bloch sphere representation. Then
t
uses the optimistic principle to select projectors Π± that are close the unknown state Π projecting into the
at
Bloch sphere the extreme points of the largest principal axis of C . This particular choice allows optimal
t
learning of Π (exploration) and simultaneously minimizes the regret (exploitation).
Theorem 2 (informal). Given a PSMAQB with a qubit environment, the cumulative expected
regret for any strategy is bounded by
E[Regret(T)] = Ω(logT), (5)
where the expectation is taken over the probability distribution of rewards and actions induced by
the learner strategy and also uniformly over the set of pure state environments.
This result is formally derived in Section 6. There it is also generalized to the d-dimensional
PSMAQB, in which case the bound is given by E[Regret(T)] = Ω(dlog(T/d)). The proof relies
on the fact that individual actions of a strategy at time t ∈ [T] can be viewed as quantum state
tomographies using t copies of the state. A relation between the fidelity of these tomographies
and the regret of the strategy allows us to convert the fidelity upper bound from [14] to a regret
lower bound. We use measure-theoretic tools to adapt the proof from [14] to a more general case
where the tomography can output an arbitrary distribution of states. We remark that this is a
noteworthy result since in [20] they argue how regret lower bound techniques for classical linear
bandits fail for noise models with vanishing variance.
3. THE MODEL
InthissectionfirstweformallystatethePSMAQBproblemandmakeaconnectionwithalinear
stochastic bandit problem. Then we define a slightly more general model where the key feature is
that the variance of the rewards vanishes with the same behaviour as the PSMAQB problem.6
3.1. Notation
First, we introduce some basic notation and conventions. Let [t] = {1,2,...,t} for t ∈ N. For
realvectorsx,y ∈ Rd wedenotetheirinnerproductas⟨x,y⟩ = x y +...+x y . Givenarealvector
1 1 d d
x ∈ Rd we denote the 2-norm as ∥x∥ and for a real semi-positive definite matrix A ∈ Rd×d, A ≥ 0
2
the weighted norm with A as ∥x∥2 = ⟨x,Ax⟩. The set corresponding to the surface of the unit
A
sphere is Sd−1 = {x ∈ Rd : ∥x∥ = 1}. For a real symmetric matrix A ∈ Rd×d we denote λ (A),
2 max
λ (A) its maximum and minimum eigenvalues respectively. We use the ordering λ (A) ≤
min min
λ (A),....,λ (A) ≤ λ (A) for the i-th λ (A) eigenvalue in increasing order. For a random
2 d−1 max i
variable X (discrete or continuous) we denote E[X] and V[X] its expectation value and variance
respectively. A random variable X is σ-subgaussian if ∀λ ∈ R,E[exp(λX)] ≤ exp(cid:0) λ2σ2/2(cid:1) .
Let S = {ρ ∈ Cd×d : Tr(ρ) = 1,ρ ≥ 0} be the set of quantum states in a d-dimensional Hilbert
d
space H = Cd and S∗ = {ρ ∈ S : ρ2 = ρ} the set of pure states or rank-1 projectors. We will use
d d
the parametrization given in [8] of a d-dimensional quantum state ρ ∈ S ,
θ d
(cid:32)(cid:114) (cid:33)
I d(d−1)
ρ = + θ·σ (6)
θ d 2d2
where θ ∈ Rd2−1, and σ = (σ ,...,σ ) is a vector of orthogonal, traceless, Hermitian matrices
1 d2−1
withthenormalizationconditionTr(σ σ ) = 2δ . Wewillusethesubscriptθ inthequantumstate
i j i,j
ρ inordertodenotethevectoroftheparametrization(6). Inparticularthenormalizationistaken
θ
such that ∥θ∥2 ≤ 1 with equality if ρ is pure. Note that the parametrization enforces ρ† = ρ
2 θ θ θ
and Tr(ρ ) = 1. Also there are some extra conditions on the vector θ regarding the positivity of
θ
the density matrix ρ but we will not use them. For two quantum states ρ,σ ∈ S the fidelity is
θ d
(cid:16) (cid:112)√ √ (cid:17)2
F(ρ,σ) = Tr( σρ σ) and the infidelity 1−F(ρ,σ). For a Hilbert space H, the set of linear
operators on it will be denoted by End(H). The joint state of a system consisting of n copies of
a pure state Π ∈ S∗ is given by the n-th tensor power Π⊗n ∈ End(H⊗n). Using Dirac notation,
θ d θ
we can express Π = |ψ ⟩⟨ψ | for some normalized |ψ ⟩ ∈ H. Then, the span of all n-copy states
θ θ θ θ
of the form |ψ ⟩⊗n is called the symmetric subspace of H⊗n, denoted by H⊗n. Its dimension is
θ +
D = (cid:0)n+d−1(cid:1) . The symmetrization operator Π+ ∈ End(H⊗n) is the projector onto H⊗n.
n d n +
3.2. Multi-armed quantum bandit for pure states
Themodelthatweareinterestedinisthegeneralmulti-armedquantumbanditmodeldescribed
in[19][Section2.3]withtheactionsetbeingallrank-1projectorsandwithpurestateenvironments.
For completeness, we state the basic definitions for this particular case.
Definition 3. Let d ∈ N. A d-dimensional pure state multi-armed quantum bandit (PSMAQB) is
given by a measurable space (A,Σ), where A = S∗ is the action set and Σ is a σ-algebra of subsets
d
of A. The bandit is in an environment, a quantum state Π ∈ S∗, that is unknown.
θ d
The interaction with the PSMAQB is done by a learner that interacts sequentially over t ∈ [T]
rounds with the unknown environment Π ∈ S∗. At each time step t ∈ [T]:
θ d
1. The learner selects an action Π ∈ A.
at
2. Performs a measurement on the unknown environment Π using the two-outcome POVM
θ7
{Π ,I −Π } and receives a reward r ∈ {0,1} sampled according to the Born’s rule, i.e
at d×d at t

Tr(Π Π ) r = 1,


θ at t
Pr (r |Π ) = 1−Tr(Π Π ) if r = 0, (7)
Π θ t at θ at t

0 else.
We note that the reward at time step t after selecting Π ∈ A can be written as
at
r = Tr(Π Π )+ϵ , (8)
t θ at t
where ϵ is a Bernoulli random variable with values ϵ ∈ {1−Tr(Π Π ),−Tr(Π Π )} such that
t t θ at θ at
E[ϵ |F ] = 0, V[ϵ |F ] = Tr(Π Π )(1−Tr(Π Π )), (9)
t t−1 t t−1 θ at θ at
where F := {r ,Π ,...,r ,Π ,Π } is a σ-filtration.
t−1 1 a1 t−1 at−1 t
Formally the learner is described by a policy.
Definition 4. A policy π is a set of conditional probability measures {π t} t∈N on the action set A
of the form
π (·|r ,Π ,...,r ,Π ) : Σ → [0,1]. (10)
t 1 a1 t−1 at−1
Then the policy interacting with the environment Π defines the probability measure over the
θ
set of actions and rewards P : (Σ×{0,1})×T → [0,1] as
Π ,Π
θ
(cid:90) (cid:90)
··· Pr (r |Π )π (dΠ |r ,Π ,...,r ,Π )···Pr (r |Π )π (dΠ ), (11)
Π
θ
T aT T T 1 a1 T−1 aT−1 Π
θ
1 a1 1 a1
where the integrals are taken with respect to the corresponding subsets of actions.
The goal of the learner is to maximize the cumulative expected reward. This is quantified by
the notion of regret that serves to compare with the best possible choice of action.
Definition 5. Given a d-dimensional pure state multi-armed quantum bandit, a policy π and
T ∈ N, the cumulative regret is defined as
T
(cid:88)
Regret(T,π,Π ) := 1−Tr(Π Π ). (12)
θ θ at
t=1
We note that since the quantity max Tr(Π Π) is maximized by Π = Π , then 1 =
Π∈A θ θ
max Tr(Π Π) is the maximal expected reward and the above definition quantifies how close is
Π∈A θ
the chosen action to the unknown Π . Moreover, expressing Π = |ψ ⟩⟨ψ |, Π = |ψ ⟩⟨ψ | for
θ θ θ θ at at at
normalized complex vectors |ψ ⟩,|ψ ⟩ ∈ Cd we have
θ at
T T
(cid:88) (cid:88)
Regret(T,π,Π ) = 1−F(Π ,Π ) = 1−|⟨ψ |ψ ⟩|2, (13)
θ θ at θ at
t=1 t=1
and the term 1−|⟨ψ |ψ ⟩|2 is the infidelity between the pure quantum states |ψ ⟩ and |ψ ⟩.
θ at θ at
The goal of the learner is to minimize the expected cumulative regret that is simply defined as
E [Regret(T,π,Π )] where the expectation E is taken over the probability measure (11). When
Π θ θ Π θ
the context is clear, we will use the notation Regret(T). We refer to the PSMAQB problem as the
task of finding a policy that minimizes the expected regret E [Regret(T,π,Π )]. Minimizing the
Π θ θ
regret means achieving sublinear regret on T since Regret(T) ≤ T holds for any policy.8
3.3. Classical model
In order to study the PSMAQB it is helpful to study it using the linear stochastic bandit
framework. The idea will be to express the actions and unknown quantum states as real vectors
using the parametrization (6).
In the linear stochastic bandit model, the action set is a subset of real vectors i.e A ⊆ Rd, and
the reward at time step t ∈ [T] after selecting action a ∈ A is given by
t
r = ⟨a ,θ⟩+ϵ (14)
t t t
where θ ∈ Rd is the unknown parameter and ϵ is some bounded σ−subgaussian noise that in
t
general can depend on θ and a . The regret for this model is given by
t
T
(cid:88)
Regret (T,π,θ) := max⟨θ,a⟩−⟨θ,a ⟩, (15)
cl t
a∈A
t=1
where the policy π is defined analogously to Definition 4. We used the subscript cl to differentiate
between quantum and classical model.
InordertoexpressthePSMAQBmodelasalinearstochasticbanditwecanusetheparametriza-
tion (6) and express the expected reward for action Π ∈ S∗ as
at d
1
Tr(Π Π ) = (1+(d−1)⟨a ,θ⟩). (16)
at θ
d
t
Inverting the above expression we have
dTr(Π Π )−1
⟨a ,θ⟩ =
θ at
. (17)
t
d−1
Let’s quickly revisit the regret expression and use the above identities in order to connect the
quantum and classical versions of the regret. We denote Π = argmax Tr(ΠΠ ) the optimal
a∗ Π∈A θ
action and recall that 1 = Tr(Π Π ). Then we have
a∗ θ
T T
(cid:88) d−1 (cid:88)
Regret(T,π,Π ) = Tr(Π Π )−Tr(Π Π ) = ⟨θ,a∗−a ⟩. (18)
θ a∗ θ at θ
d
t
t=1 t=1
Note that by the normalization (6) we have that for ρ and Π the corresponding real vecotrs are
θ at
normalized ∥θ∥ = ∥a ∥ = 1. Thus, since a∗ = θ the regret can be written as
2 t
T T
d−1 (cid:88) d−1 (cid:88)
Regret(T,π,Π ) = (1−⟨θ,a ⟩) = ∥θ−a ∥2. (19)
θ d t 2d t 2
t=1 t=1
Now we want to formulate a classical bandit such that the environment and actions are given
by the real vectors that parameterize the quantum states (6). In order to have an expected linear
reward that is linear with respect to θ and a it is sufficient to define a renormalized reward as
t
(cid:26) (cid:27)
dr −1 −1
t
r˜ = ∈ 1, , (20)
t
d−1 d−1
where we used the reward of the quantum model r ∈ {0,1} given by 7. Using E[r |F ] =
t t t−1
Tr(Π ρ ) and (16) it is easy to see that
at θ
E[r˜|F ] = ⟨θ,a ⟩, (21)
t t−1 t9
where naturally we use F = {r˜ ,a ,...,r˜ ,a ,a }. Thus, we can write the reward in the
t−1 1 1 t−1 t−1 t
form (14)
r˜ = ⟨θ,a ⟩+ϵ , E[ϵ |F ] = 0, V[ϵ |F ] = (1−⟨θ,a ⟩)(1+(d−1)⟨θ,a ⟩), (22)
t t t t t−1 t t−1 t t
where the expectation and variance follow from a direct calculation. Then we can study a d-
dimensional PSMAQB as a linear stochastic bandit choosing the action set
Aquantum := {a ∈ Rd2−1 : Π ∈ S∗} (23)
d a d
with unknown parameter θ ∈ Rd2−1 such that Π ∈ S∗. The regret of this linear model is given by
θ d
Regret = 1 (cid:80)T ∥θ−a ∥2 and we have the following relation with the quantum model:
cl 2 t=1 t 2
d−1
Regret(T,π,Π ) = Regret (T,π,θ), (24)
θ d cl
where we take the same strategy π in both sides since we can identify the actions of both bandits
throughtheparametrization(6)andtherelationbetweenrewardsgivenby (20). Whenthecontext
is clear we will simply use Regret(T) for both quantum and classical model.
3.4. Linear bandit with linearly vanishing variance noise
In [20] some of the present authors introduced the framework of stochastic linear bandits with
linear vanishing noise where the setting is a linear bandit with action set A = Sd, unknown
parameter θ ∈ Sd and reward r = ⟨θ,a ⟩+ϵ such that ϵ is σ -subgaussian with E[ϵ |F ] = 0 and
t t t t t t t−1
the property of vanishing noise σ2 ≤ 1−⟨θ,a ⟩2. In order to study a PSMAQB we will relax the
t t
condition on the subgaussian noise and we will replace it by the following condition on the noise
E[ϵ |F ] = 0, V[ϵ |F ] ≤ 1−⟨θ,a ⟩2. (25)
t t−1 t t−1 t
As in the classical model of the previous section using that max ⟨θ,a⟩ = 1 we have that the
a∈A
regret is given by
T T
(cid:88) 1 (cid:88)
Regret(T) = 1−⟨θ,a ⟩ = ∥θ−a ∥2. (26)
t 2 t 2
t=1 t=1
We note that finding an strategy that minimizes regret for the above model will also work for a
d = 2 PSMAQB with unknown Π ∈ S∗ using the relations of last sections since
θ 2
Aquantum = {a ∈ R3 : ∥a∥ = 1} = S2, (27)
2 2
and the variance of the PSMAQB (22) fullfills the relation (25).
4. ALGORITHM FOR BANDITS WITH LINEARLY VANISHING VARIANCE NOISE
In this Section we are going to present an algorithm for the linear bandit model explained
in Section 3.4 that is based on the algorithm LINUCB-VN studied in [20] for linear bandits with
linearly vanishing noise. Later we will show how to use this algorithm for the qubit PSMAQB
problem.10
4.1. Median of means for an online least squares estimator
Firstwediscussthemediansofmeansmethodfortheonlinelinearleastsquaresestimatorintro-
duced in [25]. We are going to use this estimator later in order to design a strategy that minimizes
the regret for the model introduced in Section 3.4. The reason we need this estimator is that in the
analysis of our algorithm we need concentration bounds for linear least squares estimators where
the random variables have bounded variance and a possibly unbounded subgaussian parameter.
The condition of bounded variance is weaker than the usual assumption of bounded subgaussian
noise, however we can recover similar concentration bounds of the estimator if we implement a
median of means.
In order to build the median of means online least squares estimator for linear bandits we
need to sample k independent rewards for each action. Specifically given an action set A ⊂ Rd,
an unknown parameter θ ∈ Rd, at each time step t we select an action a ∈ A and sample k
t
independent rewards using a where the outcome rewards are distributed as
t
r = ⟨θ,a ⟩+ϵ for i ∈ [k], (28)
t,i t t,i
for some noise such that E[ϵ |F ] = 0. We refer to k as the number of subsamples per time step.
t,i t−1
Then at time step t we define k least squares estimators as
t
(cid:88)
θ(cid:101)t,i = V t−1 r s,ia
s
for i ∈ [k], (29)
s=1
where V is the design matrix defined as
t
t
(cid:88)
V = λI+ a aT, (30)
t s s
s=1
with λ > 0 being a parameter that ensures invertibility of V . We note that the design matrix is
t
independent of i. Then the median of means for least squares estimator (MOMLSE) is defined as
θ(cid:101) tMoM := θ˜
t,k∗
where k∗ = argminy j, (31)
j∈[k]
where
y = median{∥θ˜ −θ˜ ∥ : i ∈ [k]/j} for j ∈ [k]. (32)
j t,j t,i Vt
Using the results in [25] we have that the above estimator has the following concentration
property around the true estimator.
Lemma 6 (Lemma 2 and 3 in [25]). Let θ(cid:101)MoM be the MOMLSE defined (31) in with k subsamples
t
with {r } rewards and corresponding actions {a } . Assume that the noise of all
s,i (s,i)∈[t]×[k] s s∈[t]
(cid:104) (cid:105)
rewards has bounded variance, i.e E ϵ2 |F ≤ 1 for all s ∈ [t] and i ∈ [k]. Then we have
s,i t−1
(cid:18) (cid:16)√ (cid:17)2(cid:19) (cid:18) −k(cid:19)
Pr ∥θ−θ(cid:101) tMoM∥2
Vt
≤ 9 9d+λ∥θ∥
2
≥ 1−exp
24
. (33)
We will use a slight modification of the above result with a weighted least squares estimator
like the one used in [20]. The weights will be related to a variance estimator of the noise for action
a ∈ A that at each time step t can be generally defined as
σˆ2 : H ×A → R , (34)
t t−1 >011
where H = {r } ∪{a } contains the past information of rewards and actions
t−1 s,i (s,i)∈[t−1]×[k] s s∈[t−1]
played. For our purposes we will use only the information of the past actions and in order to
simplify notation we will use σˆ2(a) to denote an estimator of the variance for the reward associated
t
actiona ∈ Awiththeinformationcollecteduptotimestept−1. Thenthecorrespondingweighted
versions with k subsamples are defined as
t
(cid:88) 1
θ(cid:101)t,i = V t−1
σˆ2(a
)r s,ia
s
for i ∈ [k], (35)
s=1 s s
with the weighted design matrix
t
(cid:88) 1
V = λI+ a aT. (36)
t σˆ2(a ) s s
s=1 s s
Then the weighted version of the median of means linear estimator is defined analogously to (31)
withthecorrespondingweightedversions(35)(36)andwewilldenoteitasθ(cid:101)wMOM. Inouralgorithm
t
analysis we will use the following analogous concentration bound under the condition that the
estimators σˆ2 overestimate the true variance.
t
Corollary7. Letθ(cid:101) twMOM betheweightedversionoftheMOMLSEwithk subsamples, {r s,i}
(s,i)∈[t]×[k]
rewards with corresponding actions {a } and variance estimator σˆ2. Define the following event
s s∈[t] t
G := {(cid:0) H ,a (cid:1) : V[ϵ ] ≤ σˆ2(a )∀s,i ∈ [t]×[k]}. (37)
t t−1 t s,i s
Then we have
(cid:18) (cid:19)
(cid:16) (cid:17) −k
Pr ∥θ−θ(cid:101) twMOM∥2
Vt
≤ β | G
t
≥ 1−exp
24
, (38)
where
(cid:16)√ (cid:17)2
β := 9 9d+λ∥θ∥ . (39)
2
Proof. The result follows from applying Lemma 6 to the sequences of re-normalized rewards
{
rs,i
} andactions{
as,i
} . Weonlyneedtocheckthatthesequence{
ϵs,i
}
σˆs(as) (s,i)∈[t]×[k] σˆs(as) s∈[t] σˆs(as) (s,i)∈[t]×[k]
has finite variance. Conditioning with the event G and the fact that by definition σˆ2(a ) only
t s s
depend on the past s−1 action and rewards we have that the re-normalized noise has bounded
variance since
(cid:34) (cid:12) (cid:35)
(cid:18) ϵ (cid:19)2(cid:12) 1 V[ϵ ]
E s,i (cid:12)F = E[ϵ2 |F ] = s,i ≤ 1. (40)
σˆ (a ) (cid:12) t−1 σˆ2(a ) s,i t−1 σˆ2(a )
s s (cid:12) s s s s
4.2. Algorithm
ThealgorithmthatwedesignforlinearbanditswithlinearlyvariancevanishingnoiseisLinUCB-
VVN (LinUCB vanishing variance noise) stated in Algorithm 1. The algorithm runs in batches of
2(d−1) actions selected as
a± :=
(cid:101)a±
t,i , a˜± = θwMoM±
1
, θwMoM :=
θ(cid:101) twMoM
, (41)
t,i ∥ (cid:101)a± t,i∥ 2 t,i t (cid:112) λ min(V t−1) t ∥θ(cid:101) twMoM∥ 212
for i ∈ [d−1] and where for each action a± we sample k ≥ 1 independent rewards in order to build
t,i
the weighted MOMLSE defined as in Section 4.1. The design matrix V is updated as
t
d−1
(cid:88)(cid:16) (cid:17)
V = V +ω(V ) a+(a+)T+a−(a−)T (42)
t t−1 t−1 t,i t,i t,i t,i
i=1
where the weights ω and variance estimator are chosen as
(cid:112)
λ (V ) 1
ω(V ) := √max t−1 , σˆ2(a±) := . (43)
t−1 12 d−1β t t,i ω(V t−1)
We note that the definition for σˆ2(a±) fulfills the definition of variance estimator (34) stated in
t t,i
the previous section since it only depends on the past history H .
t−1
Algorithm 1: LinUCB-VVN
Require: λ ∈R , k ∈N, ω :Pd →R
0 >0 + ≥0
Set initial design matrix V ←λ I
0 0 d×d
Choose initial estimator θ ∈Sd for θ at random
0
for t=1,2,··· do
Optimistic action selection
for i=1,2,···d−1 do
Select actions a+ and a− according to Eq. (41)
t,i t,i
Sample k independent rewards for each a±
t,i
for j =1,...,k do
Receive associated rewards r+ and r−
t,i,j t,i,j
end
end
Update estimator of sub-gaussian noise for a+
t,i
σˆ2 ← 1 for t≥2 or σˆ2 ←1 for t=1.
t ω(Vt−1(λ0)) t
Update design matrix
V ←V + 1 (cid:80)d−1(cid:0) a+(a+)T+a−(a−)T(cid:1)
t t−1 σˆ2 i=1 t,i t,i t,i t,i
t
Update LSE for each subsample
for j =1,2,...,k: do
θ(cid:101)w ←V−1(cid:80)t 1 (cid:80)d−1(a+ r+ +a− r− )
t,j t s=1 σˆ2 i=1 s,i t,i,j s,i t,i,j
t
end
Compute θ(cid:101)wMOM using {θ(cid:101)w }k
t t,j j=1
end
4.3. Regret analysis
In this Section we present the analysis of the regret for Algorithm 1. The analysis is similar to
theLinUCB-VNpresentedin[20][AppendixC.1]. Thus, wefocusonthechangesrespecttoLinUCB-
VN and although we present a complete proof we refer to [20] for more detailed computations. The
mainresultweusefrom[20]isatheoremthatquantifiesthegrowthofthemaximumandminimum
eigenvalues of the design matrix V (42).
t13
Theorem 8 (Theorem 3 in [20]). Let {c }∞ ⊂ Sd−1 be a sequence of normalized vectors and
t t=0
ω : Pd → R a function such that
+ ≥0
(cid:112)
ω(X) ≤ C ∥X∥ , (44)
∞
for a constant C > 0 and any X ∈ Pd. Let λ ≥ max(cid:8) 2,(cid:113) 2 2dC + 2 (cid:9) , and define a
+ 0 3(d−1) 3(d−1)
sequence of matrices {V }∞ ⊂ Rd×d as
t t=0
d−1
(cid:88)
V := λ I , V := V +ω(V ) P , (45)
0 0 d×d t+1 t t t,i
i=1
where
a˜±
1
P := a+ (a+ )T+a− (a− )T, a± := t+1,i , a˜± := c ± v , (46)
t,i t+1,i t+1,i t+1,i t+1,i t+1,i ∥a˜± ∥ t+1,i t (cid:112) λ t,i
t+1,i 2 t,1
with λ = λ (V ) the eigenvalues of V with corresponding normalized eigenvectors v ,...,v ∈
t,i i t t t,1 t,d
Sd−1. Then we have
(cid:115)
2
λ (V ) ≥ λ (V ) for all t ≥ 0. (47)
min t max t
3(d−1)
For the proof of the above Theorem we refer to the original reference. Then using this Theorem
andtheconcentrationboundforMOMLSEgiveninCorollary7wecanprovidethefollowingregret
analysis for a stochastic linear bandit with vanishing variance noise.
Theorem 9. Let d ≥ 2, k ∈ N and T = 2(d − 1)kT(cid:101) for some T(cid:101) ∈ N, T(cid:101) ≥ 2. Let ω(X)
defined as in (43) using λ satisfying the constraints in Theorem 8. Then if we apply LinUCB-
0
VVN 1(λ ,k,ω) to a d dimensional stochastic linear bandit with variance as in (25) with probability
0
at least (1−exp(−k/24))T(cid:101) the regret satisfies
(cid:18) (cid:19) (cid:18) (cid:19)
T T
Regret(T) ≤ 4k(d−1)+144d(d−1)kβ2log +24(d−1)3 2kβlog , (48)
2(d−1)k 2(d−1)k
and at each time step t ∈ [T] with the same probability it can output an estimator θˆ ∈ Sd−1 such
t
that
√
576d2β2k+96d d−1βk
∥θ−θˆ∥2 ≤ , (49)
t 2 t
with β defined as in (39).
(cid:16) (cid:17)
From the above Theorem we have that if we set k = ⌈24log T(cid:101) ⌉ for some δ ∈ (0,1) then with
δ
probability at least 1−δ LinUCB-VNN achieves
(cid:18) (cid:19)
Regret(T) = O(cid:0) d4log2(T)(cid:1) , ∥θ−θˆ∥2 = O log(T) . (50)
t 2 t
Proof. From the expression of the regret (26) we have that to give an upper bound it suffices
to gives an upper bound between the distance of the unknown parameter θ and the actions a±
t,i14
selected by the algorithm (41). We denote the step t˜∈ [T(cid:101)] to run over the batches the algorithm
updates the MoM estimator θ(cid:101)wMOM. First we will do the computation assuming that the event
t
E := {H : ∀s ∈ [t˜],θ ∈ C }, (51)
t˜ t˜ s
holdswhereC
s
= {θ′ ∈ Rd : ∥θ′−θ(cid:101) t˜wMOM∥2
Vs
≤ β}. HerethehistoryH t˜isdefinedwiththeprevious
outcomes and actions of our algorithm i.e
(cid:16) (cid:17)
H := r+ ,a+ ,r− ,a− (52)
t˜ s,i,j s,i s,i,j s,i (s,i,j)∈[t˜]×[d−1]×[k]
Later we will quantify the probability that this event always hold. Using the definition of the
actions (41), θ,θ(cid:101)wMOM ∈ Sd−1 and the arguments from [20][Appendix C.1, Eq. (165)] we have that
t˜
9β
∥θ−a±∥2 ≤ . (53)
t˜,i 2 λ (V )
min t˜−1
ThenusingthatthedesignmatrixV (42)isupdatedasinTheorem8andthechoiceofweights(43)
t˜
we fix
(cid:40) (cid:115) (cid:41)
2 d 2
λ ≥ max 2,2 √ + (54)
0
3(d−1)12 d−1β 3(d−1)
(cid:113)
and we have that λ (V ) ≥ 2 λ (V ) applying Theorem 8. Inserting this into the above
min t˜ 3(d−1) max t˜
we have
√
12 d−1β
∥θ−a±∥2 ≤ . (55)
t˜,i 2 (cid:112)
λ (V )
max t˜
Thus, it remains to provide a lower bound on λ (V ). We note that in [20][Appendix C.1] they
max t˜
also had to provide an upper bound but this was because the constant β beta they use depends
on t. From the definition of V (42) we can bound the trace as
t
t˜
(cid:88)
Tr(V ) ≥ 2(d−1)ω(V ) (56)
t˜ s−1
s=2
√
t˜−1
d−1 (cid:88)(cid:112)
= λ (V ). (57)
max s
6β
s=1
Then using the bound Tr(V ) ≥ λ (V )/d and some algebra we arrive at
t˜ max t˜
t˜
1 (cid:88)(cid:112)
λ (V ) ≥ λ (V ). (58)
max t˜ 1+6√d β max s
d−1 s=1
Now we have an inequality with the function λ (V ) at both sides. In order to solve it we use
max s
the technique from [20][Appendix C.1, Equation (197-208)] which consist on extending λ (V )
max t˜
to the continuous with a linear interpolation and then transforming the sum to an integral which
leads to a differential inequality. Solving this leads to
t˜2
λ (V ) ≥ . (59)
max t˜ 4(1+6√d β)2
d−115
Now we can insert the above into (55) and we have
√
24 d−1β(1+6√d β)
∥θ−a±∥2 ≤ d−1 (60)
t˜,i 2 t˜−1
√
144dβ2+24 d−1β
= . (61)
t˜−1
Thus, we can inserted the above bound into the regret expression (26) and we have
T
1 (cid:88)
Regret(T) = ∥θ−a ∥2 (62)
2 t 2
t=1
T˜ d−1 k
1 (cid:88)(cid:88)(cid:88)(cid:16) (cid:17)
= ∥θ−a+∥2+∥θ−a−∥2 (63)
2 t˜,i 2 t˜,i 2
t˜=1 i=1 j=1
T˜ d−1 k
1 (cid:88)(cid:88)(cid:88)(cid:16) (cid:17)
≤ 4k(d−1)+ ∥θ−a+∥2+∥θ−a−∥2 (64)
2 t˜,i 2 t˜,i 2
t˜=2 i=1 j=1
T˜
≤
4k(d−1)+(144d(d−1)kβ2+24(d−1)3 2kβ)(cid:88) 1
(65)
t−1
t˜=2
≤
4k(d−1)+144d(d−1)kβ2logT(cid:101)+24(d−1)3
2kβlogT(cid:101) (66)
(cid:18) (cid:19) (cid:18) (cid:19)
T T
= 4k(d−1)+144d(d−1)kβ2log +24(d−1)3 2kβlog . (67)
2(d−1)k 2(d−1)k
ItremainstoquantifytheprobabilitythattheeventE holds. Forthatwewillusetheconcentration
t˜
boundsofthemedianofmeansforleastsquaresestimatorstatedinCorollary7. Fromthevariance
condition of our model (25) we have
V[ϵ± |F ] ≤ 1−⟨θ,a±⟩2 ≤ 2(1−⟨θ,a±)) = ∥θ−a±∥2, (68)
t˜,i,j t˜−1 t˜,i t˜,i t˜,i 2
where we used 1+⟨θ,a±⟩ ≤ 2. Thus from our choice of weights (43) and (60) we have that
t˜,i
if θ ∈ C ⇒ V[ϵ± |F ] ≤ σˆ2(a± ). (69)
s−1 t˜,i,j t˜−1 s s,i
Then in order to apply Corollary 7 we note that from the choice σˆ2(a± ) = 1 the event G at t˜= 1
s 1,i t˜
is always satisfied i.e Pr(G ) = 1. Then applying Bayes theorem, union bound over the events
1
G ,E ,...,G ,E and Corollary 7 we have
1 1 t−1 t
Pr(E ∩G ) ≥ (1−exp(−k/24))T(cid:101). (70)
T(cid:101) T(cid:101)
This probability also quantifies the probability that (60) holds since the only assumption we used
is θ ∈ C . Then we can take simply one of the actions a± as the estimator θˆ and the result
t˜−1 t˜,i t
follows using the relabeling t = 2(d−1)kt˜and the inequality 1/(t˜−1) ≤ 2/t˜for t˜≥ 2. A more
detailed analogous computation of the above probability can be found in [20][Appendix C.1].
In the previous Theorem we did not set a specific value for the parameter k or the number
of subsamples per action. We note that the regret scales linearly with k but since the success
probability scales exponentially with k it will suffice to set k ∼ log(T) such that in expectation we
get the log2(T) behaviour. We formalize this in the following Corollary.16
Corollary 10. Under the same assumptions of Theorem 9 we can fix k = ⌈24log(T(cid:101)2)⌉ and we
have
(cid:16) (cid:17)
E[Regret(T)] ≤ 344(d−1)log(T)+ 3546d(d−1)β2+1152(d−1)3 2β log2(T) (71)
and for t ∈ [T],
√
(cid:104) (cid:105) 27648d2β2log(T)+4608d d−1βlog(T) 4(d−1)log(T)
E ∥θ−θˆ∥2 ≤ + . (72)
t 2 t T
Using that β = O(d) gives
(cid:104) (cid:105)
(cid:18) d4(cid:19)
E[Regret(T)] = O(d4log2(T)), E ∥θ−θˆ∥2 = O˜ . (73)
t 2 t
Proof. The result of Theorem 9 holds with probability at least (1−exp(−k/24))T(cid:101). Setting k =
⌈24log(T(cid:101)2)⌉ gives
(cid:18)
1
(cid:19)T(cid:101)
1
(1−exp(−k/24))T(cid:101) ≥ 1− ≥ 1− . (74)
T(cid:101)2 T(cid:101)
Then given the event R such that Algorithm 1 achieves the bounds given by Theorem 9 we have
T
that the probability of failure is bounded by
1
Pr(RC) ≤ , (75)
T
T(cid:101)
where we used 1 = Pr(R )+Pr(RC). Then the expectation of the bad events can be bounded as
T T
E(cid:2) Regret(T)I{RC}(cid:3) ≤ 4(d−1)kT(cid:101)Pr(RC) ≤ 4(d−1)k (76)
T T
(cid:104) (cid:105) 4
E ∥θ−θˆ∥2I{RC} ≤ 4Pr(RC) ≤ (77)
t 2 T T
T(cid:101)
where we used Regret(T) ≤ 2T = 4(d−1)kT(cid:101), ∥θ −θˆ t∥2
2
≤ 4. Finally the result follows inserting
the value of k = 24log(T(cid:101)2) into the bounds of Theorem 9 and using T(cid:101) ≤ T.
5. ALGORITHM FOR QUBIT PSMAQB AND NUMERICAL EXPERIMENTS
In this Section we prove our main result that is a regret bound for LinUCB-VVN when applied
to the qubit PSMAQB problem.
Theorem 11. Let T(cid:101) ∈ N and fix T = ⌈96T(cid:101)log(T(cid:101)2)⌉. Then given a PSMAQB with action set
A = S∗ and environment Π ∈ S∗ (qubits) we can apply Algorithm 1 for d = 3 and it achieves
2 θ 2
E[Regret(T)] ≤ C log(T)+C log2(T). (78)
1 2
for some universal constants C ,C ≥ 0. Also at each time step t ∈ [T] it outputs an estimator
1 2
Πˆ ∈ S∗ of Π with infidelity scaling
t 2 θ
(cid:104) (cid:16) (cid:17)(cid:105) C log(T)
E 1−F Π ,Πˆ ≤ 3 , (79)
θ t
t
for some universal constant C ≥ 0.
317
log(cid:16) t (cid:17)
log(t)
T
FIG. 2. Expected regret vs the number or rounds T for the LinUCB-VNN algorithm. We run T = 4·104
roundswithk =10subsamplesforthemedianofmeansconstruction. Weuse100independentsexperiments
and average over them. We obtain results for each round but only plot (red crosses) few for clarity of the
figure. We fit the regression Regret(T)=m log2T +b with m =3.2164±0.0009 and b =0.84±0.016.
1 1 1 1
In the inset plot we plot the expected infidelity of the output estimator at each rounds t ∈ [T] versus the
number of rounds t. We take Π = Π as the estimator given by the median of means linear least
t θwMoM
squares estimator. We fit the
regressiont
1−F(Π,Π ) = b
(cid:16) logt(cid:17)m2
and we obtain m = −0.996±0.002
t 2 t 2
b =0.112±0.007. We note that the number of subsamples of the theoretical results is very conservative in
2
comparison with the value we take for the simulations.
Proof. In order to apply Algorithm 1 to a PSMAQB we set d = 3 (dimension for a classical linear
stochastic bandit) and the actions that we select will be given by Π where a± are updated as
a± t,i
t,i
in (41). Note that they are valid action since a± ∈ S2 imply Π ∈ S∗. The rewards received
t,i a± 2
t,i
by the algorithm follow (22) with the normalization given in (20). This model fits into the linear
bandit with linearly vanishing variance noise model explained in Section 3.4 and thus we can apply
the guarantees established in Theorem 9 and Corollary 10.
The algorithm is set with k = ⌈24log(T(cid:101)2)⌉ batches for the MoM construction. We set λ
0
= 2,
and using ∥θ∥ = 1 we have that the constant β given in (39) has the value
2
(cid:16) √ (cid:17)2 √
β = 9 3 3+2 = 279+108 3. (80)
Then we can check that for d = 3 the condition (54) for the input parameter λ for Theorem 9 to
0
hold is satisfied since
(cid:26) (cid:27)
1 1
λ = 2 ≥ max 2, + √ √ = 2. (81)
0
3 2 6(279+108 3)
)T(tergeR
))tΠ,Π(F−1(gol18
In the above we just substituted all numerical values. Then we are under the assumptions of
Theorem 9 and Corollary 10 and the result follows applying both results with the relation of
regrets between the classical and quantum model given in (24), the relation
(cid:16) (cid:16) (cid:17)(cid:17)
∥θ−θˆ∥2 = 4 1−F Π ,Π , (82)
t 2 θ θˆ
t
and substituting all numerical values. We take the estimator θˆ given in Theorem 9 for d = 3. We
t
use also the bound T(cid:101) ≤ T and reabsorb all the constants into C 1,C 2,C 3.
Remark 1. The constant dependence can be slightly improved taking the estimator for Π as
θ
Π with θwMoM defined in (41).
θwMoM t
t
Remark 2. The result of Theorem 11 also holds with high probability. In particular for the
choice of batches k = 24log(T(cid:101)2) with probability at least 1− 1.
T(cid:101)
6. REGRET LOWER BOUND FOR PSMAQB
While the algorithm for PSMAQB presented above is inspired by classical bandit theory, the
lower bound on the regret that we derive is essentially based on quantum information theory. The
key insight here is that a policy for PSMAQB can be viewed as a sequence of state tomographies.
The expected fidelity of these tomographies is linked to the regret. Hence, existing upper bounds
on tomography fidelity also provide a lower bound for the expected regret of the policy.
6.1. Average fidelity bound for pure state tomography
In its most general form, a tomography procedure takes n copies of an unknown state Π ∈ S∗
d
and performs a joint measurement on the state Π⊗n. This is captured in the following definition.
Let (S∗,Σ) be a σ-algebra. A tomography scheme is a positive operator-valued measure (POVM)
d
T : Σ → End(H⊗n) such that T(S∗) = Π+, where Π+ is the symmetrization operator on H⊗n.
d n n
For any ρ ∈ End(H⊗n), this POVM gives rise to a complex-valued measure
P (A) = Tr(T(A)ρ) (83)
T,ρ
for A ∈ Σ. P becomes a probability measure if ρ satisfies ρ ≥ 0, Π+ρ = ρΠ+ = ρ, and Trρ = 1.
T,ρ n n
Given n copies of Π, the tomography scheme produces the distribution P of the predicted
T,Π⊗n
states. Note that Π⊗n satisfies the properties above, so P is indeed a probability distribution.
T,Π⊗n
The fidelity of this distribution is given by
(cid:90)
F(T,Π) = Tr(Πσ)dP (σ). (84)
T,Π⊗n
Finally, the average fidelity of the tomography scheme is defined as
(cid:90)
F(T) = F(T,|ψ⟩⟨ψ|)dψ, (85)
where the integration is taken with respect to the normalized uniform measure over all pure states.
(cid:82)
In the following, dψ will always imply this measure. We will provide a lower bound on F(T) in
terms of d and n, following the proof technique from [14]. In [14], the proof is only presented for
tomography schemes producing a finite number of predictions. For our definition, we will require
more general measure-theoretic tools. Before we introduce the upper bound on the fidelity, we will
prove some auxiliary lemmas about the nature of the measure P .
T,ρ19
Lemma 12. Let (Ω,Σ) be a σ-algebra, and let O : Σ → End(H(cid:101)) be a POVM with values acting
on a finite-dimensional Hilbert space H(cid:101) with dimH(cid:101) = d˜s.t. O(Ω) ≤ 1, where 1 is the identity
operator. Further, let P
O,σ
: Σ → C be a complex-valued measure, defined for any σ ∈ End(H(cid:101)) as
P (A) = Tr[O(A)σ]. (86)
O,σ
Then, there exists a set of functions {f σ} indexed by σ ∈ EndH(cid:101) that are linear w.r.t. σ for all ω
and that satisfy
(cid:90)
f
σ
: Ω → C s.t. ∀A ∈ Σ P O,σ(A) = f σ(ω)dP O,1(ω). (87)
A
We purposefully formulated this lemma with slightly more general objects than the ones used
in the definition of tomography. That is, Ω does not need to be S∗, and H(cid:101) does not need to be the
d
n-th power H⊗n, although we will focus on this case.
Proof. Let {|i⟩}d i˜ =1 be a basis of H(cid:101) We will first show that P O,σ is dominated by P O,1 for all σ.
Indeed, let A ∈ Σ. Assume that P O,1(A) = 0. This gives us
Tr[O(A)1] = Tr[O(A)] = 0, (88)
and, because O(A) ≥ 0, we also have O(A) = 0. Therefore,
P (A) = Tr[O(A)σ] = 0. (89)
O,σ
Hence, forany|i⟩,|j⟩fromthebasiswecanintroducetheRadon-Nikodymderivativesf , which
|i⟩⟨j|
will satisfy (87). Then, for any σ ∈ EndH(cid:101) we can define
d˜
(cid:88)
f (ω) = ⟨i|σ|j⟩f (ω). (90)
σ |i⟩⟨j|
i,j=1
These f are linear in σ by definition. A direct calculation shows that they also satisfy (87).
σ
Note that for σ ≥ 0, the measure P is finite and nonnegative, but nonnegativity (and even
O,σ
real-valuedness)donotholdforageneralσ ∈ End(H(cid:101)). Byourdefinitionoff σ(ω), itcanbewritten
as
d˜
(cid:88)
f (ω) = Tr[K(ω)σ], where K(ω) = f (ω)|j⟩⟨i|. (91)
σ |i⟩⟨j|
i,j=1
As the following lemma demonstrates, K(ω) ≥ 0 for P O,1-almost every ω:
Lemma 13. Let (Ω,Σ,µ) be a measurable space and V : Ω → End(H(cid:101)) be a measurable operator-
valued function with values acting on a finite-dimensional Hilbert space H(cid:101) such that
(cid:90)
∀A ∈ Σ V(ω)dµ(ω) ≥ 0. (92)
A
Then, V(ω) ≥ 0 µ-almost everywhere.20
Proof. Let |ψ⟩ ∈ H(cid:101) and define
g (ω) = ⟨ψ|V(ω)|ψ⟩. (93)
ψ
By the given condition, for any A ∈ Σ
(cid:90) (cid:90)
g (ω)dµ(ω) = ⟨ψ| V(ω)dµ(ω)|ψ⟩ ≥ 0. (94)
ψ
A A
It follows that g (ω) ≥ 0 µ-almost everywhere. Let
ψ
Z = {ω ∈ Ω s.t. g (ω) < 0} (95)
ψ ψ
We have shown that µ(Z ψ) = 0. Next, since H(cid:101) is finite-dimensional, it is separable. Therefore,
there exists a countable set {|ψ k⟩}
k
dense in H(cid:101). Let
(cid:91)
Z = Z . (96)
ψ
k
k
We have that µ(Z) = 0. Finally, let ω ∈ Ω\Z and |ψ⟩ ∈ H(cid:101). Because {|ψ k⟩} is dense in H(cid:101), there
exists a sequence {|ψ ⟩} converging to |ψ⟩. Then,
ki
i→∞
0 ≤ ⟨ψ |V(ω)|ψ ⟩ −−−→ ⟨ψ|V(ω)|ψ⟩. (97)
ki ki
Overall, we get that
∀ω ∈ Ω\Z, |ψ⟩ ∈ H(cid:101) ⟨ψ|V(ω)|ψ⟩ ≥ 0. (98)
Together with µ(Z) = 0, this gives the desired result.
Now we can apply this analysis to the POVM corresponding to our tomography scheme, and
get the desired upper bound on the fidelity.
Theorem 14. For any tomography scheme T utilizing n copies of the input state, the average
fidelity is bounded by
n+1
F(T) ≤ . (99)
n+d
Proof. We will introduce the density K(ω) from (91) for our tomography scheme T and the cor-
responding measure P . Lemma 12 allows us to introduce for any σ ∈ End(H⊗n) the density
T,σ
f : Ω → C s.t.
σ
(cid:90)
∀A ∈ Σ P T,σ(A) = f σ(ω)dP T,1(ω). (100)
A
This density can be written as f (ω) = Tr(K(ω)σ) for some K(ω) ∈ End(H⊗n). K(ω) can be
σ
considered as the operator-valued density of T w.r.t. P T,1:
(cid:90)
∀A ∈ Σ T(A) = K(ω)dP T,1(ω). (101)
A
Since T(A) ≥ 0, it follows by Lemma 13 that K(ω) ≥ 0 for P T,1-almost all ω. Furthermore, as
T(S∗) = Π+, we have that for all A ∈ Σ, T(A) ≤ Π+. Therefore, T(A)Π+ = Π+T(A) = T(A).
d n n n n21
This means that K˜(ω) = Π+K(ω)Π+ would also satisfy (101). In the following, we will without
n n
loss of generality assume that
K(ω) = Π+K(ω) = K(ω)Π+. (102)
n n
With these tools at hand, we are ready to adapt the proof from [14] to the general case of
POVM tomography schemes. We begin by rewriting the expression (84) for average fidelity:
(cid:90) (cid:90)
F(T) = dψ dP (σ)Tr(σ|ψ⟩⟨ψ|) (103)
T,(|ψ⟩⟨ψ|)⊗n
(cid:90) (cid:90)
= dψ dP
T,1(σ)Tr(|ψ⟩⟨ψ|σ)Tr(cid:0) K(σ)(|ψ⟩⟨ψ|)⊗n(cid:1)
. (104)
Sincefidelityisnonnegativeanditsaverageisboundedby1,wecanchangetheorderofintegration.
Following [14], we introduce notation
σ (k) = 1⊗(k−1)⊗σ⊗1⊗(n−k) ∈ H⊗n. (105)
n
The product of traces in (104) can be rewritten in the following manner:
(cid:90) (cid:90) (cid:16) (cid:17)
F(T) = dP T,1(σ) dψTr (K(σ)⊗1)(|ψ⟩⟨ψ|)⊗(n+1)σ n+1(n+1) . (106)
We can now take the inner integral in closed form. As shown in [14, Eq. (4)],
(cid:90) Π+
dψ(|ψ⟩⟨ψ|)⊗n = n , (107)
D
n
where D =
(cid:0)n+d−1(cid:1)
. Another useful result in this paper is [14, Eq. (8)]:
n d
(cid:32) n (cid:33)
Tr (cid:0) Π+ σ (n+1)(cid:1) = 1 Π+ 1+(cid:88) σ (k) , (108)
n+1 n+1 n+1 n+1 n n
k=1
where Tr : End(H⊗(n+1)) → End(H⊗n) is the partial trace on the (n+1)-st copy of the system.
n+1
These expressions allow us to rewrite (106) as follows:
(cid:90)
F(T) = D1 dP T,1(σ)Tr(cid:0) (K(σ)⊗1)Π+ n+1σ n+1(n+1)(cid:1) (109)
n+1
(cid:90)
= D1 dP T,1(σ)Tr(cid:0) K(σ)Tr n+1(cid:0) Π+ n+1σ n+1(n+1)(cid:1)(cid:1) (110)
n+1
(cid:90) (cid:32) (cid:32) n (cid:33)(cid:33)
1 (cid:88)
= dP T,1(σ)Tr K(σ) 1+ σ n(k) . (111)
(n+1)D
n+1
k=1
Finally, σ (k) ≤ 1, so Tr(K(σ)σ (k)) ≤ Tr(K(σ)), and we can bound the above as
n n
1 (cid:90) TrΠ+ D n+1
F(T) ≤ dP T,1(σ)Tr(K(σ)) = n = n = . (112)
D D D n+d
n+1 n+1 n+122
6.2. Bandit policy as a sequence of tomographies
Theorem 15. Given a d-dimensional pure state general multi-armed quantum bandit we have that
for any policy π the average expected regret is bounded by
(cid:90) (cid:18) (cid:19)
T
dψE [Regret(T,π,|ψ⟩⟨ψ|)] ≥ (d−1)log , (113)
|ψ⟩⟨ψ|,π d+1
where the expectation is taken w.r.t. the measure (11) over actions taken by the bandit, and the
regret is defined in (12).
The above Theorem gives E[Regret(T)] = Ω(dlog T). In the case of qubit environments, we
d
have d = 2 and E[Regret(T)] = Ω(logT).
Proof. Given a policy π, we can introduce a POVM E : (Σ×{0,1})×t → End(H⊗t) such that
t
Pt (A ,r ,...,A ,r ) = Tr(cid:0) (|ψ⟩⟨ψ|)⊗tE (A ,r ,...,A ,r )(cid:1) , (114)
|ψ⟩⟨ψ|,π 1 1 t t t 1 1 t t
where Pt is the probability measure defined by (11), but only for actions and rewards until
|ψ⟩⟨ψ|,π
step t. The construction of this POVM is presented in the proof of Lemma 9 in [19]. We will also
define the coordinate mapping
Ψ (Π ,r ,...,Π ,r ) = Π , (115)
t 1 1 t t t
where Π ∈ A are actions and r ∈ {0,1} are rewards of the PSMAQB. Now we can for each step
i i
t define a tomography scheme T = E ◦ Ψ−1 as the pushforward POVM from E to the space
t t t t
(A,Σ). Informally, this tomography scheme takes t copies of the state, runs the policy π on them,
and outputs the t-th action of the policy as the predicted state. For A ∈ Σ, we can rewrite the
tomography’s distribution on predictions as
(cid:16) (cid:17)
P (A) = Tr(cid:0) T (A)(|ψ⟩⟨ψ|)⊗t(cid:1) = Tr(cid:0) E (Ψ−1(A))(|ψ⟩⟨ψ|)⊗t(cid:1) = Pt ◦Ψ−1 (A).
T,(|ψ⟩⟨ψ|)⊗t t t t |ψ⟩⟨ψ|,π
(116)
Then, the fidelity of T on the input |ψ⟩⟨ψ| can be rewritten as
t
(cid:90)
F(T ,|ψ⟩⟨ψ|) = ⟨ψ|ρ|ψ⟩dP (ρ) (117)
t Tt,(|ψ⟩⟨ψ|)⊗t
(cid:90)
= ⟨ψ|Ψ (Π ,r ,...,Π ,r )|ψ⟩dPt (Π ,r ,...,Π ,r ) (118)
t 1 1 t t |ψ⟩⟨ψ|,π 1 1 t t
= E [⟨ψ|Π |ψ⟩]. (119)
|ψ⟩⟨ψ|,π t
Using the bound for average tomography fidelity on T from Theorem 14, we can now bound the
t
average regret of π:
(cid:90) T (cid:90)
(cid:88)
E [Regret(T,π,|ψ⟩⟨ψ|)]dψ = T − E [⟨ψ|Π |ψ⟩]dψ (120)
|ψ⟩⟨ψ| |ψ⟩⟨ψ| t
t=1
T T
(cid:88) (cid:88) t+1
= T − F(T ) ≥ 1− (121)
t
t+d
t=1 t=1
T (cid:18) (cid:19)
(cid:88) d−1 T
= ≥ (d−1)log , (122)
t+d d+1
t=1
where the last inequality follows from bounding the sum by below with the integral of the function
f(t) = 1/(t+d).23
7. OUTLOOK
From a quantum state tomography perspective, our work introduces completely new techniques
fortheadaptivesettingsuchasthemedianofmeansonlineleastsquaresestimatorortheoptimistic
principle. Weexpectthesetechniquestofindapplicationsinotherquantumlearningsettingswhere
adaptiveness is needed. At a fundamental level our algorithm goes beyond traditional tomography
ideas like adaptive/non-adaptive basis measurements, randomized measurements or SIC POVM’s
and show that is enough to project near the state in order to optimally learn it. From a bandit
perspective, it is surprising that the simple setting of learning pure quantum states gives the first
non-trivial example of a linear bandit with continuous action sets that achieves polylogarithmic
regret. This model motivated our classical work [20] and jointly with the current work we establish
the first bridge between the fields of quantum state tomography and linear stochastic bandits.
Acknowledgements: JL thanks Jan Seyfried and Yanglin Hu for comments and suggestions
and Roberto Rubboli for many technical discussions.
Mikhail Terekhov is grateful to be supported by the EDIC Fellowship from the School of Com-
puter Science at EPFL. Josep Lumbreras and Marco Tomammichel are supported by the National
Research Foundation, Singapore and A*STAR under its CQT Bridging Grant and the Quantum
Engineering Programme grant NRF2021-QEP2-02-P05.
[1] Y. Abbasi-Yadkori, D. P´al, and C. Szepesv´ari. “Improved Algorithms for Linear Stochastic Bandits”.
In Advances in Neural Information Processing Systems, volume 24, (2011).
[2] M. Abeille and A. Lazaric. “Linear Thompson Sampling Revisited”. In Proceedings of the 20th Inter-
national Conference on Artificial Intelligence and Statistics, volume 54, pages 176–184, (2017).
[3] S. Agrawal and N. Goyal. “Thompson sampling for contextual bandits with linear payoffs”. In Inter-
national conference on machine learning, pages 127–135, (2013).
[4] E. Bagan, M. Baig, and R. Mun˜oz Tapia. “Optimal Scheme for Estimating a Pure Qubit State via
Local Measurements”. Phys. Rev. Lett. 89:277904 (2002).
[5] E. Bagan, M. A. Ballester, R. D. Gill, A. Monras, and R. Mun˜oz Tapia. “Optimal full estimation of
qubit mixed states”. Phys. Rev. A 73:032301 (2006).
[6] S. Brahmachari, J. Lumbreras, and M. Tomamichel. “Quantum contextual bandits and recommender
systems for quantum data”. arXiv preprint arXiv:2301.13524 (2023).
[7] S. Bubeck, N. Cesa-Bianchi, and G. Lugosi. “Bandits With Heavy Tail”. IEEE Transactions on
Information Theory 59(11):7711–7717 (2013).
[8] M. S. Byrd and N. Khaneja. “Characterization of the positivity of the density matrix in terms of the
coherence vector representation”. Physical Review A 68(6):062322, (2003).
[9] S. Chen, B. Huang, J. Li, A. Liu, and M. Sellke. “When Does Adaptivity Help for Quantum State
Learning?”. In 2023 IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS),
pages 391–404, Los Alamitos, CA, USA(2023).
[10] V.Dani, T.P.Hayes, andS.M.Kakade. “Stochastic Linear Optimization under Bandit Feedback.”. In
Proceedings of the 21st Conference on Learning Theory, volume 2, page 3, (2008).
[11] R. D. Gill and S. Massar. “State estimation for large ensembles”. Phys. Rev. A 61:042312 (2000).
[12] M. Gu¸t˘a, J. Kahn, R. Kueng, and J. A. Tropp. “Fast state tomography with optimal error bounds”.
Journal of Physics A: Mathematical and Theoretical 53(20):204001 (2020).
[13] J. Haah, A. W. Harrow, Z. Ji, X. Wu, and N. Yu. “Sample-optimal tomography of quantum states”.
In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 913–925,
(2016).
[14] A. Hayashi, T. Hashimoto, and M. Horibe. “Reexamination of optimal quantum state estimation of
pure states”. Physical review A 72(3):032325, (2005).
[15] M.HayashiandK.Matsumoto. “Asymptotic performance of optimal state estimation in qubit system”.24
Journal of Mathematical Physics 49(10):102101 (2008).
[16] R. Kueng, H. Rauhut, and U. Terstiege. “Low rank matrix recovery from rank one measurements”.
Applied and Computational Harmonic Analysis 42(1):88–116 (2017).
[17] T. Lattimore and C. Szepesv´ari. Bandit Algorithms. Cambridge University Press (2020).
[18] M. Lerasle. “Lecture notes: Selected topics on robust statistical learning theory”. arXiv:1908.10761 ,
(2019).
[19] J. Lumbreras, E. Haapasalo, and M. Tomamichel. “Multi-armed quantum bandits: Exploration versus
exploitation when learning properties of quantum states”. Quantum 6:749, (2022).
[20] J.LumbrerasandM.Tomamichel.“Linearbanditswithpolylogarithmicminimaxregret”.arXivpreprint
arXiv:2402.12042 (2024).
[21] D. H. Mahler, L. A. Rozema, A. Darabi, C. Ferrie, R. Blume-Kohout, and A. M. Steinberg. “Adaptive
Quantum State Tomography Improves Accuracy Quadratically”. Phys. Rev. Lett. 111:183601 (2013).
[22] A. M. Medina and S. Yang. “No-regret algorithms for heavy-tailed linear bandits”. In Proceedings
of the 33rd International Conference on International Conference on Machine Learning - Volume 48,
ICML’16, page 1642–1650, (2016).
[23] R. O’Donnell and J. Wright. “Efficient quantum tomography”. In Proceedings of the Forty-Eighth
Annual ACM Symposium on Theory of Computing, STOC ’16, page 899–912, (2016).
[24] P. Rusmevichientong and J. N. Tsitsiklis. “Linearly Parameterized Bandits”. Mathematics of Opera-
tions Research 35(2):395–411 (2010).
[25] H. Shao, X. Yu, I. King, and M. R. Lyu. “Almost optimal algorithms for linear stochastic bandits
with heavy-tailed payoffs”. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, NIPS’18, page 8430–8439, Red Hook, NY, USA(2018).
[26] W. R. Thompson. “On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples”. Biometrika 25(3-4):285–294, (1933).
[27] H. Yuen. “An Improved Sample Complexity Lower Bound for (Fidelity) Quantum State Tomography”.
Quantum 7:890 (2023).