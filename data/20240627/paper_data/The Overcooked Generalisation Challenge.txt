The Overcooked Generalisation Challenge
ConstantinRuhdorfer MatteoBortoletto AnnaPenzkofer AndreasBulling
UniversityofStuttgart,Germany
constantin.ruhdorfer@vis.uni-stuttgart.de
Abstract
WeintroducetheOvercookedGeneralisationChallenge(OGC)–thefirstbench-
marktostudyagents’zero-shotcooperationabilitieswhenfacedwithnovelpartners
andlevelsintheOvercooked-AIenvironment. Thisperspectivestarklycontrasts
alargebodyofpreviousworkthathastrainedandevaluatedcooperatingagents
only on the same level, failing to capture generalisation abilities required for
real-worldhuman-AIcooperation. Ourchallengeinterfaceswithstate-of-the-art
dual curriculum design (DCD) methods to generate auto-curricula for training
generalagentsinOvercooked. Itisthefirstcooperativemulti-agentenvironment
speciallydesignedforDCDmethodsand,consequently,thefirstbenchmarkedwith
state-of-the-artmethods. ItisfullyGPU-accelerated,builtontheDCDbenchmark
suiteminimax,andfreelyavailableunderanopen-sourcelicense: https://git.
hcics.simtech.uni-stuttgart.de/public-projects/OGC. Weshowthat
current DCD algorithms struggle to produce useful policies in this novel chal-
lenge,evenifcombinedwithrecentnetworkarchitecturesthatweredesignedfor
scalability and generalisability. The OGC pushes the boundaries of real-world
human-AIcooperationbyenablingtheresearchcommunitytostudytheimpactof
generalisationoncooperatingagents.
1 Introduction
Developing intelligent agents capable of collaborating with humans remains a key challenge in
artificialintelligence(AI)research[1]. Computationalagentsthatteamupwithhumans[2,3]to
worktowardscommongoalsandachievejointobjectivespromisetovastlyexpandhumanabilities
[4] and enable numerous applications, e.g. in human-robot interaction [5]. Recent years have
seenconsiderableadvancesinunderstandinghumancooperativebehaviour[6,7],computational
modellingofcooperation[8–11]andhumanbehaviour[5],aswellasadvancesinthedevelopmentof
computationalmethodsforhuman-AIcooperation[12–16].Inparallel,severalbenchmarkshavebeen
proposedtofosterthedevelopmentandevaluationofthesemethods[17,2,18,19]. Inparticular,the
Overcooked-AIenvironment[2],usedforevaluatingzero-shothuman-AIcoordination,hasgained
significanttractionandbecameakeybenchmarkinthefield[5,13,20,21,10,15,16].
Acommonthreadamongallthesebenchmarksistheirfocusonevaluatingthecooperativeabilities
of agents in-distribution, meaning they are tested in the same environment in which they were
trained[2,13,10,15,16]. Thisapproachisseverelylimited: First,itrestrictstherangeofstrategies
thatalearningagentcanencounter. ConsidertheCoordinationRinglayoutfromOvercooked-AI
showninFigure1asanexample. Whilethelayoutrequiresagentstoadapttothepartners’preferred
walkingdirections,theamountofinteractionismostlycontrolledbyafewsimplechoices: goingleft
orrightandwhichobjectstopassoverthecounter. Previousworkhasdemonstratedthatanagentcan
effectivelycooperateinsuchalayoutbybeingtrainedwitharelativelysmall,diversepopulationof
partners[13,15,16]. Similarlytohowreinforcementlearning(RL)agentscanrememberthousands
Preprint.Underreview.
4202
nuJ
52
]GL.sc[
1v94971.6042:viXraoflevels1 ingeneralisationbenchmarks[22]andoverfittotheirtrainingenvironments[23],these
agents are likely to overfit to their partner population, as well as the respective layouts. Second,
trainingandtestingagentsonthesamelayoutdoesnotrepresentreal-worldhuman-AIcollaboration.
Thiswouldrequiregenerallycapableartificialagentsthatcansuccessfullycollaborateindifferent
physicalconfigurations,includingthosetheyhavenotbeentrainedon.
Toaddresstheselimitations,weintroducetheOvercookedGeneralisation
Challenge(OGC)–azero-shotcooperationbenchmarkthatchallenges
agentstocooperateinnovellayoutsandwithunknownagents. Toobtain
asuitabletrainingdistributionoflevels,weapplytechniquesintroduced
in research on generalisation to the cooperative multi-agent setting,
includingproceduralcontentgeneration[22],unsupervisedenvironment
design(UED)[24],anddualcurriculumdesign(DCD)[25],whilewe
providehand-designedtestinglevels. Additionally,toassesszero-shot
cooperationontestinglevels,weprovidepopulationsofdiversetesting
agentsforthem,similartohowpreviousworkshaveevaluatedstandard
zero-shot cooperation [13, 12, 15, 16]. To the best of our knowledge
our work is the first to combine DCD techniques with a cooperative Figure 1: Coordina-
multi-agent RL environment and thus bridges the gap between two tion challenges in the
previouslyunrelatedresearchareas;itteststheimpactofgeneralisation Overcooked-AI Coordi-
onhuman-AIcoordinationandtheabilityofDCDalgorithmstodesign nationRinglayout.
optimal auto-curricula for cooperating agents. Furthermore, since
historically UED benchmark environments have been often based on simple navigation tasks
[22, 26, 27, 24, 25], our OGC also can be used to evaluate and compare DCD algorithms in a
settingthatrequiresadditionalinteractionwithobjectsandapartneragentforachievingajointgoal.
WebenchmarkseveralDCDalgorithmsandnetworkarchitecturesonourchallengeandfindthat
onlyPAIRED[24],togetherwithapolicythatincorporatesasoftMixture-of-Experts(SoftMoE)
module [28], has some success at generalising to the testing levels and outperforms competitive
baselines,includingrobustPLR[29,25]andACCEL[30]. Thisissurprisingaspreviousresearchhas
highlightedtheperformancebenefitsofrobustPLR.Takentogether,ourcontributionisthree-fold:
1. WeintroducetheOvercookedGeneralisationChallenge–anovelbenchmarkchallengeinwhich
agentsareaskedtocooperatewithnovelpartnersinpreviouslyunseenlayouts.
2. WeprovideOvercookedUED–anopen-sourceenvironmentthatcanbeusedwithstate-of-the-art
DCDalgorithmsandthatisintegratedintominimax[31],takingfulladvantageofthehardware
accelerationprovidedbyJAX.
3. WebenchmarkourenvironmentbytrainingagentswithcommonDCDalgorithms[24,25,30]
and show that current DCD algorithms struggle on the challenge even if we employ recent
networkarchitectures[32,28]. Furthermore,weassesszero-shotcooperationperformancewitha
populationofdiversepartnerstolinkzero-shotcooperationandgeneralisationandshowthatas
policiesbecomemoregenerallycapable,theyachievebetterzero-shotcooperation.
2 Relatedwork
2.1 Generalisationinreinforcementlearning
ItiswellknownthatRLagentsfailtogeneralisetonewenvironments[23,33–35,22]. Consequently,
several benchmarks have been proposed to evaluate the generalisation capabilities of RL agents
[35,22,26]. Tostudythisgeneralisationgap,hand-picked[35]orprocedurallygenerated[26,22,21]
sets of training and testing levels have been used. These works have shown that RL agents can
memoriselargenumbersoflevelsduringtraining[22]eveniftechniquestoincreasegeneralisation
are applied [33]. They have also shown that regularisation can improve generalisation [33, 22].
While many of these works have studied individual agents, generalisation has also been studied
in multi-agent settings [36–38]. A common finding is that agents must experience sufficiently
1NotethattheOvercookedcommunityoftenreferstoaconcretekitchenintheenvironmentasalayout,
whiletheUEDcommunityreferstoinstancesofanunder-specifiedenvironmentaalevel.Thesetermsrefertoa
similarideabutstemfromdifferentcommunities.Weusethesetermsinterchangeablywhendiscussingspecific
environmentconfigurationsinOvercooked.
2Table1: Overviewoverunsupervisedenvironmentdesignandprocedurallygeneratedenvironment
benchmarks.✓denotespresent,-absent,and? unknown(notopen-source). Closed-sourceprojects
areingraytohighlightthatwecannotcheckthemourselves. Obs. isshorthandforobservations.
Name Multi- Zero- GPU Open Partial Vector Img.
agent shot accel- Source obs. obs. obs.
coop. erated
XLand[46] (✓) - ? - ✓ ? ✓
XLand2.0[47] (✓) - ? - ✓ ? ✓
LaserTag[38] ✓ - - - ✓ - ✓
MultiCarRacing[38] ✓ - - - ✓ - ✓
CoinRun[22] - - - ✓ ✓ - ✓
CoinRun-Platforms[22] - - - ✓ ✓ - ✓
ProcGen[26] - - - ✓ ✓ - ✓
2DMazes[22,24] - - - ✓ ✓ - ✓
CarRacing[25] - - - ✓ ✓ - ✓
BipedalWalker[27,48,30] - - - ✓ ✓ ✓ -
AMaze[31] - - ✓ ✓ ✓ - ✓
XLand-MiniGrid[49] - - ✓ ✓ ✓ ✓ ✓
Craftax[50] - - ✓ ✓ ✓ - ✓
OvercookedUED(ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓
diversetrainingdatatogeneraliseatall[26]. Oneestablishedapproachusesdomainrandomisation
[39, DR]. This approach has been used to successfully detect cars [40], achieve indoor fly [41],
agilelocomotion[42]amongothers[43,44]. Ithasbeenshown,however,thatDRproducesmany
uninformativesamplesthroughthisrandomsamplingprocedure[45]. Thiscanleadtothelearner’s
inabilitytogeneralisetochallengingconfigurationsoftheseparameters[24].
2.2 Unsupervisedenvironmentdesign
Morerecently,generalisationinRLhasbeenviewedthroughthelensofunsupervisedenvironment
design[24,51,52, UED]thataimstocombatthedisadvantagesofDRbyintroducinggenerated
auto-curricula [53]. UED tackles the issue of creating a useful distribution of training tasks and
environments by generating increasingly complex environments that facilitate continued agent
learning. It does so by adapting the free parameters of an under-specified environment to the
capabilitiesofthelearningagent. TheUEDmethodsdiscussedherefallundertheclassofDual
CurriculumDesign[25,DCD]algorithms. DCDrequiresthreeparts: 1)alevelgenerator,2)acurator
and3)alearningagent. Thelevelgeneratorprovidesnewtraininglevelswhilethecuratorpicks
theappropriatetraininglevelsfromnewandpastones. SomepopularUEDalgorithmsbasedon
PrioritisedLevelReplay[29,PLR],includingrobustPLR[25,PLR⊥],parallelPLR[31,PLR⊥,∥],
MAESTRO [38] and ReMiDi [54], combine a random generator with a capable curator. Other
methods like (population) PAIRED [24], ACCEL [30], and parallel ACCEL [31, ACCEL∥] are
environmentgeneratorsoreditorsthatdonotcurate. MethodslikeReplay-EnhancedPAIRED[25,
REPAIRED] combine advantages from both approaches. While the development of these DCD
methodshasbeensteady,theyhavemostlybeenexploredinsimpleenvironments,seeTable1:
Platformgames PlatformgamesarepopularforstudyinggeneralisationinRL.Inthesegames,
agentsmovefromlefttorightinasimple2Dworldwhiledodgingenemiesandobstaclestoreacha
certaingoal. ExamplesincludeCoinRun(-Platforms)[22]andtheSonicbenchmark[34]. Solvable
platformgamesareeasytogenerateprocedurallyandperformrelativelywell. However,theyare
oftenlimitedbytheiractionandinteractionpossibilitiesandthekindofstrategiesrequiredtosolve
them,i.e. theyoftenonlyrequiregoingfromlefttorightanddodgingobjects.
Mazes Boththeoriginalworkonquantifyinggeneralisation[33,22]aswellasmorerecentwork
onDCDalgorithms[24,25,30,31,52,54]makeuseofmazes. Likeplatformgames,mazescanbe
generatedeasilywhileensuringtheyaresolvable. Theyhavethusbecomeacommonbenchmarkfor
3DCDalgorithmsandarepartofrecentlyreleasedDCDbenchmarks[31,55]. Whiletheyareusually
usedtocomparealgorithms,theyarelimitedtoasingleagent,withlimitedoptionstointeractwith
theenvironmentandotheragents. Thisbecomesapparentwhenconsideringthatcurrentalgorithms
cansolvemostmazesinonlyafewtenthousandgradientupdates[31].
Bipedalwalker Bipedalwalker-basedUEDenvironments[27]haveseentheirfairshareofadaption
[48,30]. Theycombinesimilaradvantagesanddisadvantagesasthemazeenvironments,i.e. theyare
easytomanipulateandmakesolvable,withacontinuouscontroltaskanddenserewards.
Other OthernotableUEDenvironmentsincludeCarRacing[25],aracecarenvironmentinwhich
anagentistaskedwithfollowingevolvingsetsofracingtracks,XLand[46,47],aclosed-source
multi-taskuniverseforgeneratingsingle-andmulti-agenttasksandenvironments,XLand-MiniGrid,
an open-source variant of XLand [49] based on MiniGrid [56], LaserTag [57, 38], a competitive
2Dlasertaggame,andMultiCarRacing[58,38],whichextendsCarRacingtomultiplecompeting
agents. CarRacingandXLand-MiniGridsharepropertiessimilartothoseofearlierexamplesofUED
environments. XLand,LaserTag,andMultiCarRacingaretheonlyenvironmentswithmulti-agent
aspects,buttheyarenotopen-source. XLandneitherfocusesonthemulti-agentsettingnorzero-shot
coordination and its impact on human-AI cooperation. LaserTag and MultiCarRacing are most
closelyrelatedtoourworkastheyhavebeenusedtostudymulti-agentUEDinthecompetitivesetting
in[38]. Opposedto[38],westudythecooperativesettingwithdifferentgame-theoreticdynamics[2].
GPU-acceleratedimplementations SinceDCDalgorithmsrequiresignificantruntime,vectorised
and GPU-accelerated Jax-based [59] implementations of these have become popular within the
(multi-agent) RL [60–69] and the UED/DCD [31, 55] communities. These greatly improve the
runningspeedofexperimentsbutalsorequirespecialenvironmentimplementations,compare[31].
2.3 Human-AIcooperationinOvercooked
Overcooked-AI[2]hasemergedasoneofthemostimportantbenchmarksforhuman-AIcooperation
[21, 13, 15, 16, 70–76]. It has been used for zero-shot cooperation [13, 15, 16], language
model-based cooperative agents [70, 71], as well as human modelling in cooperation [10]. The
environmentisfullycooperativeandinvolvestwoagentsincookinganddeliveringasoupofonions
toearnacombinedreward. Commonly,researchinOvercookedfocusesontrainingandevaluating
agentsononeofseverallayoutsthattestdifferentaspectsofcooperation. Whiletheoriginalwork
researchesonion-onlylayouts[2],laterworkshaveincludedadditionalingredientstostudybiases
andpreferencesincooperatingagents[16]. Forthisreason,workscanbehardtocompare,andthe
amountofvariationinlayoutshasonlygrown,seeforinstance[70–75]. Overcookedisonlyone
recentexampleofworkoncooperativemulti-agentreinforcementlearning(MARL)environments
includingmatrixpenaltygames[77,78],multi-agentparticle-worldenvironments[79,80],StarCraft
[17], Hanabi [18], and Google Research Football [19]. While each of these works has its own
merit,Overcookedisuniqueinitsfocusonresearchingandtestingcooperativezero-shothuman-AI
cooperationand,thus,istheonlyoneapplicabletoourresearchinterest. Mostcloselyrelatedto
ourworkis[21]inwhichtheauthorsuseprocedurallygeneratedOvercookedlayoutstoevaluate
human-robotinteraction. However,theyhavenottrainedageneralOvercookedagentorevaluated
RL agents in human-AI cooperation. Our work is the first to explore the impact of cross-level
generalisationforzero-shotcooperationandthefirsttoofferthenecessarytoolstodoso.
3 Preliminaries
Thecooperativemulti-agentUEDsettingisformalisedasadecentralisedunder-specifiedpartially
observableMarkovdecisionprocess(Dec-UPOMDP)withsharedrewards. SuchaDec-UPOMDPis
definedasM=⟨N,A,Ω,Θ,SM,TM,OM,RM,γ⟩inwhichN isthesetofagentswithcardinal-
ityn,ΩisasetofobservationsandSMconstitutesthesetoftruestatesintheenvironment. Partial
observationsoi ∈ΩareobtainedusingtheobservationfunctionO :S×N →Ωbyagenti∈N.
Following[25],alevelM isdefinedasafully-specifiedenvironmentgivensomeparametersθ ∈Θ.
θ
Init,agentseachpickanactiona ∈Asimultaneouslytoproduceajointactiona=(a ,...,a )
i 1 n
andobserveasharedimmediaterewardR(s,a). Then,theenvironmenttransitionstothenextstate
accordingtoatransitionfunctionT :S×A1×...×An×Θ→∆(S)where∆(S)referstothespace
4OvercookedUED
step( )
Curator
obs
Get from curator Update Teacher
Overcooked Mutator
Update with new Train on new
layouts Student layout
mutate( )
Editor
Figure2: AnoverviewoftheOvercookedGeneralisationChallengeandhowitistypicallyusedin
aDCDalgorithm. Itfeaturescomponentsforteacher-basedmethodslikePAIRED[24]viaaUED
environmentandedit-basedmethodslikeACCEL[30]viamutatorfunctionsofexistinglayouts.
ofdistributionsoverS. γ ∈[0,1)specifiesthediscountfactor. Agentslearnapolicyπ. Thejoint
policyπtogetherwiththediscountedreturnR =(cid:80)∞ γir induceajointactionvaluefunction
t i=0 t+1
Qπ =E [R |s ,a ].ThedefinitionextendsaDec-POMDP[81,82]withthefreeparame-
st+1:∞,at+1:∞ t t t
tersoftheenvironmentΘ,analogouslytopreviousworks[24,25,38].Ourdefinitiondiffersfrom[38]
intermsofthesharedrewardsandgeneral-sumnature. WithinourDec-UPOMDP,weperformUED
totrainapolicyoveradistributionoffullyspecifiedenvironmentsthatenableoptimallearning. This
isfacilitatedbyobtaininganenvironmentpolicyΛ[24]thatspecifiesasequenceofenvironmentpa-
rametersΘT forthegivenpolicythatistobetrained.HowΛisobtaineddependsontheDCDmethod.
Forexample,inOvercookedUED,Θrepresentsthepossiblepositionsofwalls,pots,servingspots,
agentstartinglocations,andonionandbowlpileswhichisadjustedbyΛoverthedurationoftraining.
4 TheOvercookedGeneralisationChallenge
Toevaluatetheimpactofgeneralisationoncooperativeagentsand,ultimately,human-AIcollabora-
tion,weintroducetheOvercookedGeneralisationChallenge(OGC,seeFigure2).Itextendsprevious
workbyevaluatingthecooperativeabilitiesout-of-distribution,i.e.,notonlywithnovelpartneragents
butalsoonpreviouslyunseentestinglevels. TheOGCusesproceduralcontentgenerationtogenerate
traininglevels. Aslevelsaregeneratedonthefly,trainingagainstadiversepopulationofexpertson
eachlayoutforzero-shotcapabilitiesispracticallyinfeasible.Thissetupallowsthetrainingandtesting
ondiverselevels. Itis,therefore,morecloselyalignedwithreal-worldhuman-AIcollaborationthat
isnotlimitedtoonespecificphysicalenvironment. IncontrasttoexistingUEDenvironmentsthatare
limitedtoasingleagentandbasedonsimplenavigationtasks,theOGCfocusesontheinteractionof
multipleagentsinacomplex,cooperativetask. Comparedtosingle-agentenvironments,multi-agent
environmentsareinherentlymorecomplexbecausetheagentsinteractwitheachother,aswellaswith
thephysicalenvironment.Morespecifically,intheOGC,twoagentsaretaskedwithcookingasoupto-
getherinthefiveoriginallayoutsofOvercooked-AI[2](seeFigure3),butwithouthavingencountered
themduringtraining.Theoriginalfivelayoutshavebeendesignedtotestandexploredifferentkindsof
cooperationandthusformsuitableout-of-distributiontestlevels. UEDexpandstherangeofpossible
layoutsandbehaviours,furtherchallenginglayoutgenerationandagents’generalisability. Specif-
ically,anenvironmentdesigner(orteacheragent)mustaccountforagentscollaboratingwhentrying
togeneratelayoutsthatareattheforefrontoftheabilitiesofthestudentagent. Suchteacheragents
interactwiththechallengebydesigninglayoutseitherfromscratchthroughinteractingwithOver-
cookedUED-anovelenvironmentforcreatingOvercookedlevels-orbyalternatingexistinglayouts
throughtheOvercookedmutator. Thisenablesthechallengetobeusedbyadiversesetofalgorithms.
4.1 Componentsofthechallenge
TheOGCcomprisesseveralcomponentsthatenableitsintegrationwithDCDalgorithms(seeFigure
2). Mostimportantly,itfeaturesanOvercookedenvironmentcapableofrunningdifferentlevelsfast
andinparallel,aswellascomponentstodesignlayoutsandzero-shotcooperationtestingpopulations.
5Plate Pile Onion Pile Agents Serving Location Pot Location
Figure 3: We study the five evaluation layouts proposed in [2]: Cramped Room, Asymmetric
Advantages,CoordinationRing,ForcedCoordination,andCounterCircuit.
Overcooked OGCbuildsontheOvercookedenvironment. Weadaptedtheversionintroducedin
theJaxMARLproject[64],keepingmostfeaturesconsistentwiththeoriginalimplementation. This
includesactionandobservationspaces,i.e. thesetofactionsisgivenby{left,right,up,down,
interact, stay} and observations are encoded as a stack of 26 boolean masks. We extend the
JaxMARLimplementationbyaddingbackfeaturesfromtheoriginalwork[2],i.e. rewardshaping
andtheoptionforreceivingpartialobservationsbasedonhand-craftedfeatures. Additionally,we
parallelisedifferentlevelsduringrolloutsbyusingpaddingandone-hotencodingofthepositions
ofenvironmentelements. Whilethisfacilitatesfastparallelrollouts,itrequirestheintroductionof
amaximumheighthandwidthw.
OvercookedUED AsafullUEDenvironment,OvercookedUEDgeneratesrandomlayoutstobe
usedbyalgorithmsthatdonotspecifyadesigner,suchasPLR.Asfor[31],OvercookedUEDdoesnot
checkwhetherarandomlayoutissolvableandleavesthetaskofidentifyingsuitabletraininglayoutsto
theDCDmethod.Arandomlayoutalwaysfeaturesoneortwoonionpiles,bowlpiles,potsandserving
locations,andtwoagents. Thenumberofwallsplacedisconfigurable,buttheenvironmentrequiresa
borderwallontheedge. Foralgorithmsthatmakeuseofadesignertocreatelayouts(PAIRED,etc.),
OvercookedUEDprovidesaUEDenvironment(seeFigure2).Thisenvironmentallowsadesignerpol-
icytotakedesignstepstoparameterisetheunderspecifiedMDP.Theactionspaceoftheteacheragent
consistsofthetotalnumberofcellsintheh×wgrid.Thefirstdesignstepsplacewallsonthegrid,later
stepsplaceobjects,piles,andagents. Incaseofaconflict,elementsareplacedrandomlyonfreecells.
Overcookedmutator SomeDCDalgorithms(ACCEL,etc.) relyonalternatingexistinglayouts
by mutating them. OvercookedUED supports layout mutation through five basic operations: (1)
convertingarandomwalltoafreespaceandviceversa,(2)movinggoals,(3)pots,(4)platepiles,
and(5)onionpiles. Givenalayout,ourmutatorrandomlysamplesnoperationsandappliesthem.
Implementation The OGC is
implemented in Jax and integrated Table 2: Mean steps-per-second given varying degrees of
into minimax and can be tested parallelisationmeasuredbytaking1000stepsinallparallel
with all available DCD algorithms environmentswithrandomlysampledactions.
present in the project. We present
the steps-per-seconds on our setup #ParallelEnvs 1 32 256 1024
givenvaryingdegreesofparallelism
AMaze 264 8,141 67,282 264,142
in Table 2 and compare it to their
OvercookedUED 151 4,921 40,011 156,696
GPU-accelerated maze environment
AMaze. OvercookedUED is a more
fully featured environment and needs to take steps with two agents, resolving interactions and
collisionsandconsequentlyachievesfewerstepspersecondonaveragebutissufficientlyfast. In
ourwork,theimplementationachievedupto20,000stepspersecond.
4.2 Evaluation
Our work evaluates agents in two ways. First, by comparing performance on out-of-distribution
Overcookedlayouts,wecanassesthecapabilitiesofaDCDalgorithmtogenerategenerallycapable
agentsand,second,byusingapopulationofhold-outFCPagents,wecanasseszero-shotcooperation
ontheevaluationlayouts.
6DR PLR PAIRED ACCEL
Figure4: Samplelevelsgeneratedbythedifferentmethodsafter15,000(Middle)and30,000(End)
epochs. Even after considerable training, none of the methods can guarantee the generation of
solvablelayouts(Middle-rowleftmostandrightmost).
Zero-shot cooperation Overcooked was originally designed for building tools for human-AI
cooperation. Within it the use of zero-shot cooperation with a diverse population has become a
proxyforassessingtheabilitiesofanagenttocoordinatewithhumans. Similarly,weprovidetrained
fictitiousco-play[13,FCP]populationswith24agentsperlayout. Additionally,weincludetoolsfor
evaluatingagentsagainstthesepopulations. FCPallowstestingagainstdiverseagentswithdifferent
preferencesandskills.
Metrics Weusethreeevaluationmetricsinourchallenge: 1)meanepisoderewardand2)mean
environmentsolvedrateintheevaluationlayouts,similartopreviouswork[31],and3)performance
withtheFCPpopulationasdescribedabove. Theenvironmentissolvedifanagentpairdeliversmore
thanonesoup.Thisensuresthatrandomagentsusuallydonotsolveanenvironmentanddistinguishes
agentsthatmakenoorfewdeliveriesfromonesthatdosoconsistentlyacrossmanyepisodes.
5 Benchmarkingthechallenge
WebenchmarkthechallengewithseveralDCDalgorithmsandnetworkarchitectures. Weaimtoset
aperformancebaselineforfutureworksandshowwhatevaluationsaredoablewiththisbenchmark.
To this end, we first show how difficult it is to play novel layouts in Overcooked and then how
theseagents mightbeused toevaluatezero-shot cooperation. All baselinesuseMAPPO[83]as
thelearningalgorithmandaretrainedviaCentralisedTrainingDecentralisedExecution[84]. As
forDCDalgorithms,wecomparetheperformanceofDR,PLR⊥,∥,Pop. PAIREDandACCEL∥.
Wechosethesemethodsastheyhavebettertheoreticalguarantees(PLR⊥vsPLR),betterruntime
performance(ACCEL∥ andPLR∥),orbecausewefoundthemtoperformbetterempirically(Pop.
PAIREDvsPAIRED).WeexcludedPOET[27]inthisanalysisasitoutputsspecialistsratherthan
generalists, which we require [30]. Additionally, we excluded MAESTRO [38] as it is based in
prioritised fictitious self-play [85, 86] that is not easily adaptable to the cooperative setting [13].
Asin[31],ifnotstatedotherwise,wetrainin32parallelenvironmentsandstopafter30,000outer
training loops, amounting to just under 400 million steps in the environment. Hyperparameters
werepickedafteragridsearchoverreasonablevaluesandallparametersareprovidedinAppendix
A.4. Ourdefaultneuralnetworkarchitectureconsistsofaconvolutionalencoderwitharecurrent
neuralnetworkwithaLSTM[87]. Itispickedforitsgoodperformanceinpreviouswork[16](see
AppendixA.5fordetails). Usingtheseparameters,weverifiedthatagentsalsooverfittotheirlevel
inOvercookedbyevaluatingagentstrainedonasinglelayoutonalllayouts(cf. AppendixA.6.1).
Zero-shot generalisation In addition to our default network architecture, we explore the use
ofSoftMoE[28],whichhaverecentlybeenidentifiedfortheirpotentialforenablingscalingand
generalisation, and S5 layers [32] due to the strong results of structured state-space models [88]
inmetareinforcementlearning[89]. SoftMoEmodulesreplacethepenultimatelayerafterthefeature
extractorandS5layerstheLSTMinallexperiments. Theresultsofthesecomparisonsareshown
inTable3. Ascanbeseenfromthetable,theOGCischallenging. Comparedtocommonlyused
7
elddiM
dnETable3: Meanepisoderewardforthedifferentmethodsaveragedovertherespectivetestinglayouts.
Thebestresultisshowninbold. Wereportaggregatestatisticsoverthreerandomseeds.
Method CNN-LSTM SoftMoE-LSTM CNN-S5
DR 0.46±0.16 5.22±7.19 0.00±0.00
PLR⊥,∥ 0.17±0.06 0.91±0.71 0.12±0.15
Pop. PAIRED 0.19±0.09 13.34±5.70 0.24±0.19
ACCEL∥ 0.20±0.14 0.67±0.60 0.28±0.26
Random Stay DR PLR PAIRED ACCEL
80
60
40
20
0
Coordination Ring Forced Coordination Counter Circuit Asymmetric Adv. Cramped Room
Figure5:Zero-shotcoordinationresultsoftheSoftMoE-LSTMpoliciypairedwithanFCPpopulation
trainedontherespectivelayout. Wereportthemeanepisoderewardandstandarderror.
single-agentMazeenvironments(suchasAMaze,compare[31]),allDCDmethodsstruggletoobtain
goodresults. Overall,thebetterperformanceofSoftMoE-LSTM,combinedwithPop,isnotable.
PAIREDoutperformsallothermodelssignificantly0.01<p<0.05usingaone-sidedpairedt-test.
Thisisalsoshowninthemeansolvedratewhereitreaches14.6±7.7%, whileallothermodels
havesolvedrateofmostly0%(cf. AppendixA.6.2). Whilethismodelperformsbetteronaverage,
layoutsdiffergreatlyintheirdifficulty. Ourbest-performingmodelreachesmodestperformancein
AsymmetricAdvantagesandCrampedRoomwhilemostlyfailingintheotherswithnoothermodel
achieving noteworthy results. Recall that the environment features more moving parts that must
beplacedcorrectlytofacilitatelearning. ThismakesithardforapproacheslikeDRtofindoptimal
placementsbypurechance,asreflectedintheresults. ThefullresultsareintheAppendixA.6.3.
Zero-shot cooperation Ultimately, we want the OGC to connect generalisation and zero-shot
coordination. Tothatend,weproposetousetheincludedpopulationofFCPagents(seeAppendix
A.6.4 for details) to establish how general cooperative agents can coordinate with a diverse set
of policies. We present preliminary results in Figure 5. As performance on out-of-distribution
levelsrises,agentsbecomemorecompetentatzero-shotcooperation. PAIREDalwaysoutperforms
baselines(cf. AppendixA.6.5). AlthoughevenPAIREDpoliciesoftenperformonlyslightlybetter
than random baselines, which signifies the challenges of our benchmark. This is also evidenced
bythekindsoflevelsthesemethodsgenerate(Figure4),astheytendtopivottowardsgenerating
openspacesthateasecooperationbutarenoteablydifferentfromevaluationlayouts.
6 Discussion
6.1 DCDmethodsandOvercookedUED
Previouswork[25]hasfoundthatPLR⊥ tendstooutperformtheotherhere-testedalgorithmsin
navigation-basedtasks. Ourmorechallengingenvironmentsuggeststhatthismightnotalwaysbethe
case. Inourpreliminaryanalysis,PAIREDoutperformedotherDCDmethods. Comparedtomazes,
carracing,orwalkerenvironmentswithfewermovingpieces,Overcookedlayoutsaremorecomplex
todesign,requiringthedesignertoplacemultipleobjectsinrelationtoeachotherandtheagents.
Thisrequiresacapablegeneratorandsuggeststhatsimplenavigation-basedenvironmentsusedto
benchmarkDCDinUEDalgorithmsdonotallowfullperformanceevaluation. Assuch,Overcooke-
dUEDcanbeanimportantpartofevaluatingDCDalgorithms. WeenvisionthatgeneralOvercooked
8
draweR
edosipE
naeMagentsshouldbeevaluatedinscenariosthataredifficultforself-playagentsusingourbenchmark.
These include zero-shot cooperation with strongly-biased agents [16] in Coordination Ring (see
Section1)andAsymmetricAdvantagesasdescribedin[90]andforwhichweprovidethetools.
6.2 Limitations
Despiteitsmanyadvantages,ourchallengeandevaluationsalsohavetwolimitations. First,weartifi-
ciallyrestrictedthemaximumsizeofthelayoutstoallowtheenvironmenttobebothfullyobservable
asin[2]andparseablebyCNN-basedfeatureencoders. Futureworkshouldfocusonmorenatural
representationsofthewholescene,e.g.usinggraphsoritemembeddings.Whileweincludedapartial
observationthatcouldtheoreticallybecomputedindependentlyofsize,similartothevector-based
observationusedforbehaviourcloningagentsin[2],batchingacrosslayoutsinOvercookedUEDstill
requiresthelayoutstobescaledtothesameheightandwidth. Second,whileourchallengeallowsus
tostudyzero-shotcoordinationviageneralisingacrosslayouts,reasoningaboutotheragents[91–95]
mightbeequallyimportanttoachievezero-shotcooperationcapabilitiesonunknownlayouts. This
isplausiblegiventhathumanscanreasonaboutthementalstatesofotheragentsviaTheoryofMind
[96],aswellasthephysicalconfigurationofthespaceinwhichtheyoperate. Futureworkcouldthus
explorereasoningaboutotheragentsinpreviouslyunexploredenvironments.
7 Conclusion
WehavepresentedtheOvercookedGeneralisationChallenge(OGC)–ageneralisationchallenge
focusingon(zero-shot)cooperationinMARLinout-of-distributiontestlevels. Ourchallengeis
thefirstunsuperviseddesignMARLenvironmentandissignificantlymorechallengingthanprevious
environmentscommonlyusedinUEDandDCDresearch. InadditiontousingthechallengeinUED
research,wehaveshownhowtheOGCcanbeusedinfutureresearchonhuman-AIcollaboration
asazero-shotcooperationbenchmarkforgeneralagents. Thatis,ourchallengeestablishesalink
between generalisation and zero-shot coordination. Our work is the first to provide the research
communitywiththetoolstotrainandevaluateagentscapableofcoordinatinginpreviouslyunknown
physicalspacesandwithnovelpartners.
Acknowledgements
TheauthorsthanktheInternationalMaxPlanckResearchSchoolforIntelligentSystems(IMPRS-
IS) for supporting C. Ruhdorfer. A. Penzkofer was funded by the Deutsche Forschungsgemein-
schaft(DFG,GermanResearchFoundation)underGermany’sExcellenceStrategy–EXC2075–
390740016. TheauthorswouldliketoespeciallythankL.Shiforthenumerousinsightfuldiscus-
sions.
References
[1] AllanDafoe,EdwardHughes,YoramBachrach,TantumCollins,KevinR.McKee,JoelZ.
Leibo,KateLarson,andThoreGraepel. OpenProblemsinCooperativeAI,December2020.
URLhttp://arxiv.org/abs/2012.08630.
[2] Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel,
and Anca Dragan. On the utility of learning about humans for human-ai coordination.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf.
[3] Thomas O’neill, Nathan McNeese, Amy Barron, and Beau Schelble. Human-autonomy
teaming: Areviewandanalysisoftheempiricalliterature. HumanFactorsTheJournalofthe
HumanFactorsandErgonomicsSociety,64:1–35,102020. doi: 10.1177/0018720820960865.
[4] DouglasC.Engelbart. Augmentinghumanintellect: aconceptualframework. 1962. URL
https://api.semanticscholar.org/CorpusID:61186602.
9[5] RohanChoudhury,GokulSwamy,DylanHadfield-Menell,andAncaD.Dragan. Ontheutility
ofmodellearninginhri. InProceedingsofthe14thACM/IEEEInternationalConferenceon
Human-RobotInteraction,HRI’19,page317–325.IEEEPress,2020. ISBN9781538685556.
[6] David G. Rand and Martin A. Nowak. Human cooperation. Trends in Cognitive Sci-
ences, 17(8):413–425, August 2013. ISSN 1364-6613, 1879-307X. doi: 10.1016/j.tics.
2013.06.003. URLhttps://www.cell.com/trends/cognitive-sciences/abstract/
S1364-6613(13)00121-6.
[7] LizaVizmathy,KatarinaBegus,GuntherKnoblich,GyörgyGergely,andAriannaCurioni.
Better Together: 14-Month-Old Infants Expect Agents to Cooperate. Open Mind, 8:1–16,
February2024. ISSN2470-2986. doi: 10.1162/opmi_a_00115. URLhttps://doi.org/10.
1162/opmi_a_00115.
[8] Stefanos Nikolaidis and Julie Shah. Human-robot cross-training: computational formula-
tion,modelingandevaluationofahumanteamtrainingstrategy. InProceedingsofthe8th
ACM/IEEE International Conference on Human-Robot Interaction, HRI ’13, page 33–40.
IEEEPress,2013. ISBN9781467330558.
[9] DorsaSadigh,ShankarSastry,SanjitA.Seshia,andAncaD.Dragan.Planningforautonomous
carsthatleverageeffectsonhumanactions. InRobotics: ScienceandSystems,2016. URL
https://api.semanticscholar.org/CorpusID:7087988.
[10] Mesut Yang, Micah Carroll, and Anca D. Dragan. Optimal behavior prior: Data-efficient
human models for improved human-ai collaboration. CoRR, abs/2211.01602, 2022. doi:
10.48550/ARXIV.2211.01602. URLhttps://doi.org/10.48550/arXiv.2211.01602.
[11] LinDing,YongTang,TaoWang,TianleXie,PeihaoHuang,andBingsanYang. ACooperative
Decision-Making Approach Based on a Soar Cognitive Architecture for Multi-Unmanned
Vehicles. Drones,8(4):155,April2024. ISSN2504-446X. doi: 10.3390/drones8040155. URL
https://www.mdpi.com/2504-446X/8/4/155.
[12] HengyuanHu,AdamLerer,AlexPeysakhovich,andJakobFoerster.“Other-play”forzero-shot
coordination. InHalDauméIIIandAartiSingh,editors,Proceedingsofthe37thInternational
ConferenceonMachineLearning,volume119ofProceedingsofMachineLearningResearch,
pages4399–4410.PMLR,13–18Jul2020.
[13] DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett.
Collaborating with humans without human data. In M. Ranzato, A. Beygelzimer,
Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Infor-
mation Processing Systems, volume 34, pages 14502–14515. Curran Associates, Inc.,
2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/
797134c3e42371bb4979a462eb2f042a-Paper.pdf.
[14] Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory diversity for
zero-shotcoordination. InMarinaMeilaandTongZhang,editors,Proceedingsofthe38th
International Conference on Machine Learning, volume 139 of Proceedings of Machine
LearningResearch,pages7204–7213.PMLR,18–24Jul2021.URLhttps://proceedings.
mlr.press/v139/lupu21a.html.
[15] RuiZhao,JinmingSong,YufengYuan,HaifengHu,YangGao,YiWu,ZhongqianSun,and
Wei Yang. Maximum entropy population-based training for zero-shot human-ai coordina-
tion. ProceedingsoftheAAAIConferenceonArtificialIntelligence,37(5):6145–6153,Jun.
2023. doi: 10.1609/aaai.v37i5.25758. URLhttps://ojs.aaai.org/index.php/AAAI/
article/view/25758.
[16] ChaoYu,JiaxuanGao,WeilinLiu,BotianXu,HaoTang,JiaqiYang,YuWang,andYiWu.
Learningzero-shotcooperationwithhumans,assuminghumansarebiased. InTheEleventh
InternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.
net/forum?id=TrwE8l9aJzs.
10[17] MikayelSamvelyan,TabishRashid,ChristianSchroederdeWitt,GregoryFarquhar,Nantas
Nardelli,TimG.J.Rudner,Chia-ManHung,PhilipH.S.Torr,JakobFoerster,andShimon
Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International
ConferenceonAutonomousAgentsandMultiAgentSystems,AAMAS’19,page2186–2188,
Richland,SC,2019.InternationalFoundationforAutonomousAgentsandMultiagentSystems.
ISBN9781450363099.
[18] NolanBard,JakobN.Foerster,SarathChandar,NeilBurch,MarcLanctot,H.FrancisSong,
EmilioParisotto,VincentDumoulin,SubhodeepMoitra,EdwardHughes,IainDunning,Shibl
Mourad,HugoLarochelle,MarcG.Bellemare,andMichaelBowling. Thehanabichallenge:
Anewfrontierforairesearch. ArtificialIntelligence,280:103216,2020. ISSN0004-3702. doi:
https://doi.org/10.1016/j.artint.2019.103216. URL https://www.sciencedirect.com/
science/article/pii/S0004370219300116.
[19] KarolKurach,AntonRaichuk,PiotrStan´czyk,MichałZaja˛c,OlivierBachem,LasseEspeholt,
CarlosRiquelme,DamienVincent,MarcinMichalski,OlivierBousquet,andSylvainGelly.
Googleresearchfootball: Anovelreinforcementlearningenvironment. Proceedingsofthe
AAAIConferenceonArtificialIntelligence,34(04):4501–4510,Apr.2020. doi: 10.1609/aaai.
v34i04.5878. URLhttps://ojs.aaai.org/index.php/AAAI/article/view/5878.
[20] PaulKnott,MicahCarroll,SamDevlin,KamilCiosek,KatjaHofmann,AncaDragan,and
RohinShah. Evaluatingtherobustnessofcollaborativeagents. InProceedingsofthe20th
International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’21,
page1560–1562,Richland,SC,2021.InternationalFoundationforAutonomousAgentsand
MultiagentSystems. ISBN9781450383073.
[21] MatthewFontaine,Ya-ChuanHsu,YulunZhang,BryonTjanaka,andStefanosNikolaidis. On
theImportanceofEnvironmentsinHuman-RobotCoordination. InProceedingsofRobotics:
ScienceandSystems,Virtual,July2021. doi: 10.15607/RSS.2021.XVII.038.
[22] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying
generalizationinreinforcementlearning. InKamalikaChaudhuriandRuslanSalakhutdinov,
editors,Proceedingsofthe36thInternationalConferenceonMachineLearning,volume97of
ProceedingsofMachineLearningResearch,pages1282–1289.PMLR,09–15Jun2019. URL
https://proceedings.mlr.press/v97/cobbe19a.html.
[23] AmyZhang,NicolasBallas,andJoellePineau. Adissectionofoverfittingandgeneralization
incontinuousreinforcementlearning. CoRR,abs/1806.07937,2018. URLhttp://arxiv.
org/abs/1806.07937.
[24] MichaelDennis,NatashaJaques,EugeneVinitsky,AlexandreBayen,StuartRussell,Andrew
Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised
environmentdesign. InProceedingsofthe34thInternationalConferenceonNeuralInforma-
tionProcessingSystems,NIPS’20,RedHook,NY,USA,2020.CurranAssociatesInc. ISBN
9781713829546.
[25] MinqiJiang,MichaelDDennis,JackParker-Holder,JakobNicolausFoerster,EdwardGrefen-
stette,andTimRocktäschel.Replay-guidedadversarialenvironmentdesign.InA.Beygelzimer,
Y.Dauphin,P.Liang,andJ.WortmanVaughan,editors,AdvancesinNeuralInformationPro-
cessingSystems,2021. URLhttps://openreview.net/forum?id=5UZ-AcwFDKJ.
[26] KarlCobbe,ChristopherHesse,JacobHilton,andJohnSchulman. Leveragingprocedural
generationtobenchmarkreinforcementlearning. InProceedingsofthe37thInternational
ConferenceonMachineLearning,ICML’20.JMLR.org,2020.
[27] RuiWang,JoelLehman,JeffClune,andKennethO.Stanley. Pairedopen-endedtrailblazer
(POET):endlesslygeneratingincreasinglycomplexanddiverselearningenvironmentsand
theirsolutions. CoRR,abs/1901.01753,2019. URLhttp://arxiv.org/abs/1901.01753.
[28] JohanS.Obando-Ceron,GhadaSokar,TimonWilli,ClareLyle,JesseFarebrother,JakobN.
Foerster,GintareKarolinaDziugaite,DoinaPrecup,andPabloSamuelCastro. Mixturesof
expertsunlockparameterscalingfordeepRL. CoRR,abs/2402.08609,2024. doi: 10.48550/
ARXIV.2402.08609. URLhttps://doi.org/10.48550/arXiv.2402.08609.
11[29] Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized level replay. In Ma-
rina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on
MachineLearning,volume139ofProceedingsofMachineLearningResearch,pages4940–
4950.PMLR,18–24Jul2021. URLhttps://proceedings.mlr.press/v139/jiang21b.
html.
[30] JackParker-Holder,MinqiJiang,MichaelDennis,MikayelSamvelyan,JakobFoerster,Edward
Grefenstette,andTimRocktäschel. Evolvingcurriculawithregret-basedenvironmentdesign.
InKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,GangNiu,andSivan
Sabato, editors, Proceedings of the 39th International Conference on Machine Learning,
volume 162 of Proceedings of Machine Learning Research, pages 17473–17498. PMLR,
17–23Jul2022.
[31] MinqiJiang,MichaelDennis,EdwardGrefenstette,andTimRocktäschel. minimax: Efficient
baselinesforautocurriculainJAX. CoRR,abs/2311.12716,2023. doi: 10.48550/ARXIV.2311.
12716. URLhttps://doi.org/10.48550/arXiv.2311.12716.
[32] JimmyT.H.Smith,AndrewWarrington,andScottLinderman. Simplifiedstatespacelayersfor
sequencemodeling. InTheEleventhInternationalConferenceonLearningRepresentations,
2023. URLhttps://openreview.net/forum?id=Ai8Hw3AXqks.
[33] ChiyuanZhang,OriolVinyals,RémiMunos,andSamyBengio. Astudyonoverfittingindeep
reinforcementlearning. CoRR,abs/1804.06893,2018. URLhttp://arxiv.org/abs/1804.
06893.
[34] JesseFarebrother,MarlosC.Machado,andMichaelH.Bowling. Generalizationandregular-
izationindqn. ArXiv,abs/1810.00123,2018. URLhttps://api.semanticscholar.org/
CorpusID:52904113.
[35] AlexNichol,VickiPfau,ChristopherHesse,OlegKlimov,andJohnSchulman. Gottalearn
fast: A new benchmark for generalization in RL. CoRR, abs/1804.03720, 2018. URL
http://arxiv.org/abs/1804.03720.
[36] NicolasCarion,NicolasUsunier,GabrielSynnaeve,andAlessandroLazaric. AStructured
PredictionApproachforGeneralizationinCooperativeMulti-AgentReinforcementLearning,
volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/
paper_files/paper/2019/file/3c3c139bd8467c1587a41081ad78045e-Paper.pdf.
[37] YuxinChen,ChenTang,RanTian,ChenranLi,JinningLi,MasayoshiTomizuka,andWei
Zhan. Quantifyingagentinteractioninmulti-agentreinforcementlearningforcost-efficient
generalization. InProceedingsofthe23rdInternationalConferenceonAutonomousAgents
andMultiagentSystems,AAMAS’24,page2201–2203,Richland,SC,2024.International
FoundationforAutonomousAgentsandMultiagentSystems. ISBN9798400704864.
[38] Mikayel Samvelyan, Akbir Khan, Michael D Dennis, Minqi Jiang, Jack Parker-Holder,
JakobNicolausFoerster,RobertaRaileanu,andTimRocktäschel. MAESTRO:Open-ended
environmentdesignformulti-agentreinforcementlearning. InTheEleventhInternational
ConferenceonLearningRepresentations,2023. URLhttps://openreview.net/forum?
id=sKWlRDzPfd7.
[39] NickJakobi. Evolutionaryroboticsandtheradicalenvelope-of-noisehypothesis. AdaptiveBe-
havior,6(2):325–368,September1997. ISSN1741-2633. doi:10.1177/105971239700600205.
URLhttp://dx.doi.org/10.1177/105971239700600205.
[40] Rawal Khirodkar, Donghyun Yoo, and Kris M. Kitani. Domain randomization for scene-
specificcardetectionandposeestimation. 2019IEEEWinterConferenceonApplicationsof
ComputerVision(WACV),pages1932–1940,2018. URLhttps://api.semanticscholar.
org/CorpusID:53307158.
[41] FereshtehSadeghiandSergeyLevine. CAD2RL:realsingle-imageflightwithoutasinglereal
image. InNancyM.Amato,SiddharthaS.Srinivasa,NoraAyanian,andScottKuindersma,
editors,Robotics:ScienceandSystemsXIII,MassachusettsInstituteofTechnology,Cambridge,
Massachusetts, USA, July 12-16, 2017, 2017. doi: 10.15607/RSS.2017.XIII.034. URL
http://www.roboticsproceedings.org/rss13/p34.html.
12[42] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven
Bohez,andVincentVanhoucke. Sim-to-real: Learningagilelocomotionforquadrupedrobots.
InProceedingsofRobotics: ScienceandSystems,Pittsburgh,Pennsylvania,June2018. doi:
10.15607/RSS.2018.XIV.010.
[43] JoshTobin,RachelFong,AlexRay,JonasSchneider,WojciechZaremba,andPieterAbbeel.
Domainrandomizationfortransferringdeepneuralnetworksfromsimulationtotherealworld.
In2017IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),page
23–30.IEEEPress,2017. doi: 10.1109/IROS.2017.8202133. URLhttps://doi.org/10.
1109/IROS.2017.8202133.
[44] Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, V. Jampani, Cem Anil,
ThangTo,EricCameracci,ShaadBoochoon,andStanBirchfield. Trainingdeepnetworkswith
syntheticdata:Bridgingtherealitygapbydomainrandomization.2018IEEE/CVFConference
onComputerVisionandPatternRecognitionWorkshops(CVPRW),pages1082–10828,2018.
URLhttps://api.semanticscholar.org/CorpusID:4929980.
[45] RawalKhirodkar,DonghyunYoo,andKrisM.Kitani. Adversarialdomainrandomization.
CoRR,abs/1812.00491v2,2018. URLhttps://arxiv.org/abs/1812.00491v2.
[46] OpenEndedLearningTeam,AdamStooke,AnujMahajan,CatarinaBarros,CharlieDeck,
Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michaël Mathieu, Nat
McAleese,NathalieBradley-Schmieg,NathanielWong,NicolasPorcel,RobertaRaileanu,
StephHughes-Fitt,ValentinDalibard,andWojciechMarianCzarnecki. Open-endedlearning
leadstogenerallycapableagents,2021. URLhttps://arxiv.org/abs/2107.12808.
[47] Jakob Bauer, Kate Baumli, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-
Schmieg,MichaelChang,NatalieClay,AdrianCollister,VibhavariDasagi,LucyGonzalez,
Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Open-
shaw, JackParker-Holder, ShreyaPathak, NicolasPerez-Nieves, NemanjaRakicevic, Tim
Rocktäschel,YannickSchroecker,SatinderSingh,JakubSygnowski,KarlTuyls,SarahYork,
AlexanderZacherl,andLeiMZhang.Human-timescaleadaptationinanopen-endedtaskspace.
InAndreasKrause,EmmaBrunskill,KyunghyunCho,BarbaraEngelhardt,SivanSabato,and
JonathanScarlett,editors,Proceedingsofthe40thInternationalConferenceonMachineLearn-
ing,volume202ofProceedingsofMachineLearningResearch,pages1887–1935.PMLR,
23–29Jul2023. URLhttps://proceedings.mlr.press/v202/bauer23a.html.
[48] RuiWang,JoelLehman,AdityaRawal,JialeZhi,YulunLi,JeffClune,andKennethO.Stanley.
Enhancedpoet: Open-endedreinforcementlearningthroughunboundedinventionoflearning
challengesandtheirsolutions. InInternationalConferenceonMachineLearning,2020. URL
https://api.semanticscholar.org/CorpusID:213175678.
[49] AlexanderNikulin,VladislavKurenkov,IlyaZisman,ViacheslavSinii,ArtemAgarkov,and
SergeyKolesnikov. XLand-minigrid: Scalablemeta-reinforcementlearningenvironmentsin
JAX. InIntrinsically-MotivatedandOpen-EndedLearningWorkshop@NeurIPS2023,2023.
URLhttps://openreview.net/forum?id=xALDC4aHGz.
[50] MichaelMatthews,MichaelBeukman,BenjaminEllis,MikayelSamvelyan,MatthewThomas
Jackson,SamuelCoward,andJakobN.Foerster. Craftax: Alightning-fastbenchmarkfor
open-endedreinforcementlearning. CoRR,abs/2402.16801,2024. doi: 10.48550/ARXIV.
2402.16801. URLhttps://doi.org/10.48550/arXiv.2402.16801.
[51] Ishita Mediratta, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, and
TimRocktäschel. Stabilizingunsupervisedenvironmentdesignwithalearnedadversary. In
SarathChandar,RazvanPascanu,HanieSedghi,andDoinaPrecup,editors,Conferenceon
LifelongLearningAgents,22-25August2023,McGillUniversity,Montréal,Québec,Canada,
volume232ofProceedingsofMachineLearningResearch, pages270–291.PMLR,2023.
URLhttps://proceedings.mlr.press/v232/mediratta23a.html.
[52] WenjunLi,PradeepVarakantham,andDexunLi. Generalizationthroughdiversity: improving
unsupervisedenvironmentdesign. InProceedingsoftheThirty-SecondInternationalJoint
Conference on Artificial Intelligence, IJCAI ’23, 2023. ISBN 978-1-956792-03-4. doi:
10.24963/ijcai.2023/601. URLhttps://doi.org/10.24963/ijcai.2023/601.
13[53] Alex Graves, Marc G. Bellemare, Jacob Menick, Rémi Munos, and Koray Kavukcuoglu.
Automatedcurriculumlearningforneuralnetworks. InProceedingsofthe34thInternational
ConferenceonMachineLearning-Volume70,ICML’17,page1311–1320.JMLR.org,2017.
[54] MichaelBeukman,SamuelCoward,MichaelMatthews,MattieFellows,MinqiJiang,Michael
Dennis, and Jakob N. Foerster. Refining minimax regret for unsupervised environment
design. CoRR, abs/2402.12284, 2024. doi: 10.48550/ARXIV.2402.12284. URL https:
//doi.org/10.48550/arXiv.2402.12284.
[55] SamuelCoward,MichaelBeukman,andJakobN.Foerster. Jaxued: Asimpleanduseable
UEDlibraryinjax. CoRR,abs/2403.13091,2024. doi: 10.48550/ARXIV.2403.13091. URL
https://doi.org/10.48550/arXiv.2403.13091.
[56] MaximeChevalier-Boisvert,BolunDai,MarkTowers,RodrigodeLazcano,LucasWillems,
SalemLahlou,SumanPal,PabloSamuelCastro,andJordanTerry. Minigrid&miniworld:
Modular&customizablereinforcementlearningenvironmentsforgoal-orientedtasks. CoRR,
abs/2306.13831,2023.
[57] MarcLanctot,ViniciusZambaldi,AudrunasGruslys,AngelikiLazaridou,KarlTuyls,Julien
Perolat,DavidSilver,andThoreGraepel. Aunifiedgame-theoreticapproachtomultiagent
reinforcement learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S.Vishwanathan,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper_files/paper/2017/file/3323fe11e9595c09af38fe67567a9394-Paper.pdf.
[58] WilkoSchwarting,TimSeyde,IgorGilitschenski,LucasLiebenwein,RyanMSander,Sertac
Karaman,andDanielaRus. Deeplatentcompetition: Learningtoraceusingvisualcontrol
policies in latent space. In Conference on Robot Learning, 2021. URL https://api.
semanticscholar.org/CorpusID:231979320.
[59] JamesBradbury,RoyFrostig,PeterHawkins,MatthewJamesJohnson,ChrisLeary,Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
QiaoZhang. JAX:composabletransformationsofPython+NumPyprograms, 2018. URL
http://github.com/google/jax.
[60] Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier
Bachem. Brax - a differentiable physics engine for large scale rigid body simula-
tion. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Informa-
tion Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. URL
https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/
2021/file/d1f491a404d6854880943e5c3cd9ca25-Paper-round1.pdf.
[61] Robert Tjarko Lange. gymnax: A JAX-based reinforcement learning environment library,
2022. URLhttp://github.com/RobertTLange/gymnax.
[62] ShengyiHuang,RousslanFernandJulienDossa,ChangYe,JeffBraga,DipamChakraborty,
KinalMehta,andJoãoG.M.Araújo. Cleanrl: High-qualitysingle-fileimplementationsof
deepreinforcementlearningalgorithms. JournalofMachineLearningResearch, 23(274):
1–18,2022. URLhttp://jmlr.org/papers/v23/21-1342.html.
[63] RihabGorsane,OmaymaMahjoub,RuanJohndeKock,RolandDubb,SiddarthSingh,and
Arnu Pretorius. Towards a standardised performance evaluation protocol for cooperative
marl. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
AdvancesinNeuralInformationProcessingSystems,volume35,pages5510–5521.Curran
Associates,Inc.,2022. URLhttps://proceedings.neurips.cc/paper_files/paper/
2022/file/249f73e01f0a2bb6c8d971b565f159a7-Paper-Conference.pdf.
[64] AlexanderRutherford,BenjaminEllis,MatteoGallici,JonathanCook,AndreiLupu,Garðar
Ingvarsson,TimonWilli,AkbirKhan,ChristianSchroederdeWitt,AlexandraSouly,Sap-
tarashmiBandyopadhyay,MikayelSamvelyan,MinqiJiang,RobertLange,ShimonWhiteson,
Bruno Lacerda, Nick Hawes, Tim Rocktäschel, Chris Lu, and Jakob Foerster. Jaxmarl:
Multi-agentrlenvironmentsandalgorithmsinjax. InProceedingsofthe23rdInternational
14ConferenceonAutonomousAgentsandMultiagentSystems,AAMAS’24,page2444–2446,
Richland,SC,2024.InternationalFoundationforAutonomousAgentsandMultiagentSystems.
ISBN9798400704864.
[65] ArnuPretorius,Kale-abTessera,AndriesP.Smit,ClaudeFormanek,StJohnGrimbly,Kevin
Eloff,SipheleleDanisa,LawrenceFrancis,JonathanP.Shock,HermanKamper,WillieBrink,
HermanA.Engelbrecht,AlexandreLaterre,andKarimBeguir. Mava: aresearchframework
for distributed multi-agent reinforcement learning. CoRR, abs/2107.01460, 2021. URL
https://arxiv.org/abs/2107.01460.
[66] Claude Formanek, Asad Jeewa, Jonathan Shock, and Arnu Pretorius. Off-the-grid marl:
Datasetsandbaselinesforofflinemulti-agentreinforcementlearning. InProceedingsofthe
2023InternationalConferenceonAutonomousAgentsandMultiagentSystems,AAMAS’23,
page2442–2444,Richland,SC,2023.InternationalFoundationforAutonomousAgentsand
MultiagentSystems. ISBN9781450394321.
[67] EdanToledo,LaurenceMidgley,DonalByrne,CallumRhysTilbury,MatthewMacfarlane,
CyprienCourtot,andAlexandreLaterre. Flashbax: Streamliningexperiencereplaybuffers
for reinforcement learning with jax, 2023. URL https://github.com/instadeepai/
flashbax/.
[68] Mathias Lechner, lianhao yin, Tim Seyde, Tsun-Hsuan Johnson Wang, Wei Xiao,
Ramin Hasani, Joshua Rountree, and Daniela Rus. Gigastep - one billion steps
per second multi-agent reinforcement learning. In A. Oh, T. Naumann, A. Glober-
son, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Infor-
mation Processing Systems, volume 36, pages 155–170. Curran Associates, Inc.,
2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
00ba06ba5c324efdfb068865ca44cf0b-Paper-Datasets_and_Benchmarks.pdf.
[69] ClémentBonnet,DanielLuo,DonalJohnByrne,ShikhaSurana,PaulDuckworth,Vincent
Coyette,LaurenceIllingMidgley,SashaAbramowitz,ElshadaiTegegn,TristanKalloniatis,
OmaymaMahjoub,MatthewMacfarlane,AndriesPetrusSmit,NathanGrinsztajn,Raphael
Boige,CemlynNeilWaters,MohamedAliAliMimouni,UlrichArmelMbouSob,RuanJohn
deKock,SiddarthSingh,DanielFurelos-Blanco,VictorLe,ArnuPretorius,andAlexandre
Laterre. Jumanji: a diverse suite of scalable reinforcement learning environments in JAX.
InTheTwelfthInternationalConferenceonLearningRepresentations,2024. URLhttps:
//openreview.net/forum?id=C4CxQmp9wc.
[70] Jijia Liu, Chao Yu, Jiaxuan Gao, Yuqing Xie, Qingmin Liao, Yi Wu, and Yu Wang. Llm-
poweredhierarchicallanguageagentforreal-timehuman-aicoordination. InMehdiDastani,
JaimeSimãoSichman,NatashaAlechina,andVirginiaDignum,editors,Proceedingsofthe
23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems,AAMAS2024,
Auckland, New Zealand, May 6-10, 2024, pages 1219–1228. ACM, 2024. doi: 10.5555/
3635637.3662979. URLhttps://dl.acm.org/doi/10.5555/3635637.3662979.
[71] WeihaoTan,WentaoZhang,ShanqiLiu,LongtaoZheng,XinrunWang,andBoAn. True
knowledge comes from practice: Aligning large language models with embodied environ-
ments via reinforcement learning. In The Twelfth International Conference on Learning
Representations,2024. URLhttps://openreview.net/forum?id=hILVmJ4Uvu.
[72] LanceYing,KunalJha,ShivamAarya,JoshuaB.Tenenbaum,AntonioTorralba,andTianmin
Shu. GOMA: proactive embodied cooperative communication via goal-oriented mental
alignment. CoRR,abs/2403.11075,2024. doi: 10.48550/ARXIV.2403.11075. URLhttps:
//doi.org/10.48550/arXiv.2403.11075.
[73] DapengLi,HangDong,LuWang,BoQiao,SiQin,QingweiLin,DongmeiZhang,QiZhang,
ZhiweiXu,BinZhang,andGuoliangFan. Verco:Learningcoordinatedverbalcommunication
formulti-agentreinforcementlearning. CoRR,abs/2404.17780,2024. doi: 10.48550/ARXIV.
2404.17780. URLhttps://doi.org/10.48550/arXiv.2404.17780.
[74] ElliotFosong,ArrasyRahman,IgnacioCarlucho,andStefanoV.Albrecht. Learningcomplex
teamworktasksusingagivensub-taskdecomposition.InProceedingsofthe23rdInternational
15Conference on Autonomous Agents and Multiagent Systems, AAMAS ’24, page 598–606,
Richland,SC,2024.InternationalFoundationforAutonomousAgentsandMultiagentSystems.
ISBN9798400704864.
[75] PulkitRustagiandSandhyaSaisubramanian. Mitigatingnegativesideeffectsinmulti-agent
systemsusingblameassignment. CoRR,abs/2405.04702,2024. URLhttps://arxiv.org/
abs/2405.04702.
[76] TimFranzmeyer,EdithElkind,PhilipTorr,JakobNicolausFoerster,andJoaoF.Henriques.
Select to perfect: Imitating desired behavior from large multi-agent data. In The Twelfth
InternationalConferenceonLearningRepresentations,2024. URLhttps://openreview.
net/forum?id=L6crLU7MIE.
[77] CarolineClausandCraigBoutilier. Thedynamicsofreinforcementlearningincooperative
multiagentsystems. InProceedingsoftheFifteenthNational/TenthConferenceonArtificial
Intelligence/Innovative Applications of Artificial Intelligence, AAAI ’98/IAAI ’98, page
746–752,USA,1998.AmericanAssociationforArtificialIntelligence. ISBN0262510987.
[78] Spiros Kapetanakis, Daniel Kudenko, and Malcolm J. A. Strens. Reinforcement learning
approachestocoordinationincooperativemulti-agentsystems. InEduardoAlonso,Daniel
Kudenko,andDimitarKazakov,editors,AdaptiveAgentsandMulti-AgentSystems:Adaptation
andMulti-AgentLearning,volume2636ofLectureNotesinComputerScience,pages18–
32.Springer,2002. doi: 10.1007/3-540-44826-8\_2. URLhttps://doi.org/10.1007/
3-540-44826-8_2.
[79] Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch.
Multi-agent actor-critic for mixed cooperative-competitive environments. In I. Guyon,
U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/
2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf.
[80] IgorMordatchandPieterAbbeel. Emergenceofgroundedcompositionallanguageinmulti-
agentpopulations. InSheilaA.McIlraithandKilianQ.Weinberger,editors,Proceedingsof
theThirty-SecondAAAIConferenceonArtificialIntelligence,(AAAI-18),the30thinnovative
ApplicationsofArtificialIntelligence(IAAI-18),andthe8thAAAISymposiumonEducational
Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-
7, 2018, pages 1495–1502. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11492. URL
https://doi.org/10.1609/aaai.v32i1.11492.
[81] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized
POMDPs. SpringerInternationalPublishing,2016. ISBN9783319289298. doi: 10.1007/
978-3-319-28929-8. URLhttp://dx.doi.org/10.1007/978-3-319-28929-8.
[82] ZifanWu, ChaoYu, DehengYe, JungeZhang, haiyinpiao, andHankzHankuiZhuo. Co-
ordinated proximal policy optimization. In A. Beygelzimer, Y. Dauphin, P. Liang, and
J.WortmanVaughan,editors,AdvancesinNeuralInformationProcessingSystems,2021. URL
https://openreview.net/forum?id=iCJFwoy1T-q.
[83] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and
YIWU. Thesurprisingeffectivenessofppoincooperativemulti-agentgames. InS.Koyejo,
S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh,editors,AdvancesinNeuralIn-
formation Processing Systems, volume 35, pages 24611–24624. Curran Associates, Inc.,
2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
9c1535a02f0ce079433344e14d910597-Paper-Datasets_and_Benchmarks.pdf.
[84] JakobFoerster,IoannisAlexandrosAssael,NandodeFreitas,andShimonWhiteson. Learning
to communicate with deep multi-agent reinforcement learning. In D. Lee, M. Sugiyama,
U.Luxburg,I.Guyon,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSys-
tems,volume29.CurranAssociates,Inc.,2016.URLhttps://proceedings.neurips.cc/
paper_files/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf.
16[85] JohannesHeinrich,MarcLanctot,andDavidSilver. Fictitiousself-playinextensive-form
games. In Francis Bach and David Blei, editors, Proceedings of the 32nd International
ConferenceonMachineLearning,volume37ofProceedingsofMachineLearningResearch,
pages805–813,Lille,France,07–09Jul2015.PMLR. URLhttps://proceedings.mlr.
press/v37/heinrich15.html.
[86] OriolVinyals,IgorBabuschkin,WojciechM.Czarnecki,MichaëlMathieu,AndrewDudzik,
JunyoungChung,DavidChoi,RichardPowell,TimoEwalds,PetkoGeorgiev,JunhyukOh,
DanHorgan,ManuelKroiss,IvoDanihelka,AjaHuang,L.Sifre,TrevorCai,JohnP.Agapiou,
MaxJaderberg,AlexanderSashaVezhnevets,RémiLeblond,TobiasPohlen,ValentinDalibard,
DavidBudden,YurySulsky,JamesMolloy,TomLePaine,CaglarGulcehre,ZiyunWang,
TobiasPfaff,YuhuaiWu,RomanRing,DaniYogatama,DarioWünsch,KatrinaMcKinney,
OliverSmith,TomSchaul,TimothyP.Lillicrap,KorayKavukcuoglu,DemisHassabis,Chris
Apps,andDavidSilver. Grandmasterlevelinstarcraftiiusingmulti-agentreinforcementlearn-
ing. Nature,575:350–354,2019. URLhttps://api.semanticscholar.org/CorpusID:
204972004.
[87] SeppHochreiterandJürgenSchmidhuber. Longshort-termmemory. Neuralcomputation,9:
1735–80,121997. doi: 10.1162/neco.1997.9.8.1735.
[88] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with
structuredstatespaces. InInternationalConferenceonLearningRepresentations,2022. URL
https://openreview.net/forum?id=uYLFoz1vlAC.
[89] ChrisLu,YannickSchroecker,AlbertGu,EmilioParisotto,JakobFoerster,SatinderSingh,
and Feryal Behbahani. Structured state space models for in-context reinforcement learn-
ing. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors,
AdvancesinNeuralInformationProcessingSystems,volume36,pages47016–47031.Curran
Associates,Inc.,2023. URLhttps://proceedings.neurips.cc/paper_files/paper/
2023/file/92d3d2a9801211ca3693ccb2faa1316f-Paper-Conference.pdf.
[90] ConstantinRuhdorfer. Intothemindsofthechefs: Usingtheoryofmindforrobustcollabora-
tionwithhumansinovercooked.Master’sthesis,2023.URLhttp://elib.uni-stuttgart.
de/handle/11682/13983.
[91] NeilRabinowitz,FrankPerbet,FrancisSong,ChiyuanZhang,S.M.AliEslami,andMatthew
Botvinick. Machinetheoryofmind. InJenniferDyandAndreasKrause,editors,Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of
Machine Learning Research, pages 4218–4227. PMLR, 10–15 Jul 2018. URL https://
proceedings.mlr.press/v80/rabinowitz18a.html.
[92] KanishkGandhi,GalaStojnic,BrendenM.Lake,andMoiraDillon.Babyintuitionsbenchmark
(BIB):Discerningthegoals,preferences,andactionsofothers. InThirty-FifthConference
onNeuralInformationProcessingSystems, 2021. URLhttps://arxiv.org/abs/2102.
11938.
[93] Matteo Bortoletto, Lei Shi, and Andreas Bulling. Neural reasoning about agents’ goals,
preferences,andactions. ProceedingsoftheAAAIConferenceonArtificialIntelligence,38(1):
456–464,Mar.2024. doi: 10.1609/aaai.v38i1.27800. URLhttps://ojs.aaai.org/index.
php/AAAI/article/view/27800.
[94] Cristian-PaulBara,ZiqiaoMa,YingzhuoYu,JulieShah,andJoyceChai.Towardscollaborative
planacquisition throughtheoryof mindmodelingin situateddialogue. In Proceedingsof
theThirty-SecondInternationalJointConferenceonArtificialIntelligence,pages2958–2966,
2023.
[95] MatteoBortoletto,ConstantinRuhdorfer,AdnenAbdessaied,LeiShi,andAndreasBulling.
Limitsoftheoryofmindmodellingindialogue-basedcollaborativeplanacquisition. InProc.
62ndAnnualMeetingoftheAssociationforComputationalLinguistics(ACL),pages1–16,
2024.
[96] DavidPremackandG.Woodruff. Doesthechimpanzeehaveatheoryofmind? Behavioral
andBrainSciences,4(4):515–629,1978. doi: 10.1017/s0140525x00076512.
17[97] JonathanHeek,AnselmLevskaya,AvitalOliver,MarvinRitter,BertrandRondepierre,Andreas
Steiner,andMarcvanZee. Flax: AneuralnetworklibraryandecosystemforJAX,2023. URL
http://github.com/google/flax.
[98] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InYoshua
BengioandYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,
ICLR2015,SanDiego,CA,USA,May7-9,2015,ConferenceTrackProceedings,2015. URL
http://arxiv.org/abs/1412.6980.
[99] KunihikoFukushima. Cognitron: Aself-organizingmultilayeredneuralnetwork. Biological
Cybernetics,20:121–136,1975. URLhttps://api.semanticscholar.org/CorpusID:
28586460.
[100] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann
machines. InProceedingsofthe27thInternationalConferenceonInternationalConference
onMachineLearning,ICML’10,page807–814,Madison,WI,USA,2010.Omnipress. ISBN
9781605589077.
[101] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,
abs/1607.06450,2016. URLhttp://arxiv.org/abs/1607.06450.
A Appendix/supplementalmaterial
A.1 Accessibilityofthebenchmark
We make our challenge available under the Apache License 2.0 via a code repository: https:
//git.hcics.simtech.uni-stuttgart.de/public-projects/OGC. Ourenvironmentisbuilt
ontopoftheexistingminimaxproject(accessibleunderApacheLicense2.0viahttps://github.
com/facebookresearch/minimax)andisthusaccessibletoresearchersthatarealreadyfamiliar
withtheproject. minimaxisextensivelydocumented,fast,andsupportsmulti-devicetraining. For
alldetails,includingafulldescriptionoftheadvantagesofminimax,wekindlyreferthereaderto
theaccompanyingpublication[31]. OurOvercookedadaptionisextendedfromtheoneinJaxMARL
alsoaccessibleunderApacheLicense2.0viahttps://github.com/FLAIROx/JaxMARL.Ourcode
includesextensivedocumentationandexamplesforhowitmaybeused. Additionally,ourcodeis
writteninamodularfashionandothermulti-agentenvironmentscanbeintegratedwiththerunners
thankstothecarefuldesignoftheoriginalproject.
A.2 Broaderimpacts
Whileourworkislargelyfoundationalandconcernedwithprovidingtheresearchcommunitywith
theappropriatetoolsforthetrainingandevaluationofagentsingame-likeenvironments,special
cautionisalwaysimperativeshouldthisresearchbeappliedtohuman-AIcollaboration. Eventhough
ourgoalistoimprovecollaboration, safeguardsshouldbeappliedtomakesurethathumansare
alwayssafefromharm. Especiallysoinreal-worldapplicationswhereaccidentscouldpotentially
resultinbodilyharms. Sinceourworkisstillfarremovedfromanyreal-worldapplication,wedo
notexpectthatourworkinitspresentformcarriestheriskofmaterialisingtheseharms. Someform
ofunsupervisedenvironmentdesignincollaborativeenvironmentsmightbepartoffuturesystems
andwethereforeacknowledgetheserisks. Thisworkofcoursealsocarriespotentialtoimprove
human-AIcollaborationandwemakeanimportantcontributiontoadvancingthefieldwithpotential
impactsinallkindsofhuman-machineinteraction.
A.3 Infrastructure&tools
WeranourexperimentsonaserverrunningUbuntu22.04,equippedwithNVIDIATeslaV100-SXM2
GPUswith32GBofmemoryandIntelXeonPlatinum8260CPUs. Alltrainingrunsareexecutedon
asingleGPUonly. WetrainedourmodelsusingJax[59]andFlax[97]with1,2and3asrandom
seedfortrainingDCDmethodsand1to8asrandomseedsforthepopulations. TrainingtheDCD
methodsusuallyfinishesinunder24hours,onlySoftMoEandPAIREDbasedmethodstakelonger.
SoftMoEbasedpoliciesoftentakeanextra50%wall-clocktimetotrain. Noticeableisalsothatour
18Table4: Hyperparamtersofthelearningprocess.
Description Value
Optimizer Adam[98]
Adamβ 0.9
1
Adamβ 0.999
2
Adamϵ 1·10−5
LearningRateη 3·10−4
LearningRateAnnealing -
MaxGradNorm 0.5
DiscountRateγ 0.999
GAEλ 0.98
EntropyCoefficient 0.01
ValueLossCoefficient 0.5
#PPOEpochs 8
#PPOMinibatches 4
#PPOSteps 400
PPOValueLoss Clipped
PPOValueLossClipValue 0.2
RewardShaping Yes(linearlydecreasedovertraining)
Table5: Valuesusedforagridsearchoverhyperparametersgoverningthelearningprocess. Finally
usedvaluesappearinbold.
Description Value
LearningRateη [1·10−4,3·10−4,5·10−4,1·10−3]
EntropyCoefficient [0.010.1]
#PPOSteps [256,400]
#HiddenLayers [2,3,4]
RewardShapingAnnealingSteps [0,2500000,5000000,untilend]
S5implementationisthefastest,usuallyneeding30%lesstime. Botharecomparedtothedefault
architecturestrainingtime. InthelongestcasethecombinationofaSoftMoE-LSTMpolicytrained
withPAIREDtakesabout80hourstocompletetraining.
A.4 Hyperparameters
WeoverviewallhyperparametersfortraininginTable4andprovidedetailsonthehyperparameter
search used in Table 5. This search was conducted on smaller single layout runs to determine
reasonablevaluesascompleterunswouldhavebeencomputationallyinfeasible. Furthermorewe
showthehyperparametersforeachDCDmethodseparately: DRhyperparametersinTable6,PLR
hyperparametersinTable7,ACCELhyperparametersinTable8,andPAIREDhyperparametersin
Table9. DRhyperparametersgovernhowOvercookedlevelsaregeneratedrandomlyandapplytoall
otherprocessesinwhicharandomlevelissampled,forinstance,inPLR,inwhichcasethesame
hyperparametersapply.
A.5 Neuralnetworkarchitectures
Thisworkemploysanactor-criticarchitectureusingaseparateactorandcriticinwhichthecriticis
centralisedfortrainingviaMAPPO[83].Fortheactor,theobservationsareofshapeh×w×26,while
forthecentralisedcritic,weconcatenatetheobservationsalongthelastaxistoformacentralised
observation,i.e. thecentralisedobservationhasshapeh×w×52followingpriorwork[16].
Allournetworksfeatureaconvolutionalencoderf . Thisencoderalwaysfeaturesthree2Dconvolu-
c
tionsof32,64and32channelswithkernelsize3×3eachandpadstheinputwithzeros. Ourdefault
activation function is ReLU [99, 100] which we apply after every convolutional block. We feed
19Table6: DRhyperparameters.
Description Value
nwallstoplace Sampleduniformlybetween0–15
nonionpilestoplace Sampleduniformlybetween1–2
nplatepilestoplace Sampleduniformlybetween1–2
npotstoplace Sampleduniformlybetween1–2
ngoalstoplace Sampleduniformlybetween1–2
Table7: PLRspecifichyperparameters.
Description Value
UEDScore MaxMC[25]
PLRreplayprobabilityρ 0.5
PLRbuffersize 4,000
PLRstalenesscoefficient 0.3
PLRtemperature 0.1
PLRscoreranks Yes
PLRminimumfillratio 0.5
PLR⊥ Yes
PLR∥ Yes
PLRforceuniquelevel Yes
theoutputoff toafeed-forwardneuralnetworkf withthreelayerswith64neurons,ReLUand
c e
LayerNorm[101]appliedeach. f takestheflattenedrepresentationproducedbyf andproduces
e c
anembeddinge ∈ Rb×t×64 thatwefeedintoarecurrentneuralnetwork(eitherLSTM[87]orS5
[32])toaggregateinformationalongthetemporalaxis. Weusethisresultingembeddinge ∈Rb×64
t
to produce action logits l ∈ Rb×6 to parameterise a categorical distribution in the actor-network
or directly produce a value v ∈ Rb×1 in the critic network using a final projection layer. This
architectureisinspiredbypreviousworkonOvercooked-AI,specifically[16],seeFigure6foran
overview. WealsotesttheuseofaS5layer[32]inwhichcaseweuse2S5blocks,2S5layers,use
LayerNormbeforetheSSMblockandtheactivationfunctiondescribedintheoriginalwork, i.e.
a(x)=GELU(x)⊙σ(W ∗GELU(x)).
InthecaseoftheSoftMoEarchitecture, wefollowthesameapproachasin[28]andreplacethe
penultimatelayerwithaSoftMoElayer. AsintheirworkweusethePerConvtokenisationtechnique,
i.e. giveninputx ∈ Nh×w×26 wetaketheoutputy ∈ Rh×w×32 off andconstructh×wtokens
c
with dimension d = 32 that we then feed into the SoftMoE layer. We always use 32 slots and 4
expertsforthislayer,see[28]fordetailsonthislayer. Theresultingembeddingisthenpassedinto
thetworemaininglinearlayersbeforebeingalsopassedtoRNNandusedtoproduceanactionor
value,equivalenttothedescriptionabove,compareFigure7.
Lastly,wedescribeournetworksintermsofparametercountinTable10.
A.6 Additionalanalysis
A.6.1 EvidenceofoverfittinginOvercookedagents
WeshowthatagentsheavilyoverfittheirtraininglayoutinOvercookedinTable11. Thisistobe
expectedbutverifyingisnonethelessimportant.
A.6.2 Performanceacrosslevels
ToaccompanytheoverallperformancemeassuredbyrewardinthemainpaperinTable3wealso
meassurethemeansolvedrateondisplayitinTable12.
20Table8: ACCELhyperparameters.
Description Value
UEDScore MaxMC[25]
PLRreplayprobabilityρ 0.8
PLRbuffersize 4,000
PLRstalenesscoefficient 0.3
PLRtemperature 0.1
PLRscoreranks Yes
PLRminimumfillratio 0.5
PLR⊥ Yes
PLR∥ Yes
PLRforceuniquelevel Yes
ACCELMutation OvercookedMutator
ACCELnmutations 20
ACCELsubsamplesize 4
Table9: PAIREDhyperparameters. AllPPOhyperparametersarethesamebetweenthestudentand
theteacher. Theminimaximplementationfollowstooriginalonein[24]andwesticktoittoo.
Description Value
nstudents 2
UEDScore Relativeregret[24]
UEDfirstwallsetsbudget Yes
UEDnoisedim 50
PAIREDCreator OvercookedUED
A.6.3 Performanceonindividuallevels
WelisttheperformanceofeveryindividualmethodoneverysinglelayoutinTable13. Mostnotable
is that some layouts are harder tolearn than others. Ouragents especially seem tostruggle with
layoutsrequiringmorecomplexformsofinteraction,i.e. CoordinationRing,CounterCircuitand
ForcedCoordination. ForcedCoordinationespeciallyseemsdifficulttosolveasnorunachieves
noticeableperformanceonit. Thismightbeduetothespecificfeaturesofthelayout,i.e. thatagents
haveaccesstoseveralobjectsandneedtohandthemoverthecountertoproduceanyresult.
A.6.4 Populationtrainingcurves
Tobothverifythatourimplementationiscorrectandtogiveanintuitionintotheperformanceof
themembersofthepopulation,wepresentthetrainingcurvesoverall8seedsoftraininganFCP
populationinFigure8.
A.6.5 DetailedresultswithFCPpopulations
Wepresentdetailedzero-shotcooperationresultsperlayoutinTables14and15.Asindicatedthrough
theaveragedperformancediscussedinmaintext,wealsofindthatPAIREDperformsbestonfourof
thefiveindividuallayoutsintermsofzero-shotcooperation.
A.7 Validatingtheimplementation
Asanopen-sourcebenchmark,weplaceanemphasisonacorrectimplementationofthebenchmark,
includingallthebaselines. Wedosointwoimportantways. Firstly,webaseourimplementation
ontheimplementationoftheminimaxbenchmark[31],makingsurethatweusepubliclyavailable
code for all unsupervised environment design algorithms. Secondly, we test the implementation
andadaptionoftheOvercooked-AIenvironmentbyfixingthegeneratedtraininglayoutstoasingle
layoutduringtraining. Thisallowsustotrainonthe5classicOvercookedlayoutsusingourown
implementation. Ourimplementationiscapableofsolvingtheselayouts,seeFigure8. Wedothis
21Flatten
Action
LSTM/
Encoder or
S5
Value
d x 64 64 x 64 64 x 64 64 x 64 64 x a or 1
Figure6: BasicarchitecturefeaturingaconvolutionalencoderandanRNN.
Tokenisation
Action
LSTM/
Encoder or
S5
Value
Soft MoE 64 x 64 64 x 64 64 x 64 64 x a or 1
Gate/Combination Individual Expert
Figure7: SoftMoEarchitecturefeaturingaconvolutionalencoder,themixtureofexpertslayerand
anRNN.
inparttoargueforthefactthatourbenchmarkishardtosolveandthisisnotafunctionofpoorly
configuredorwronglyimplementedalgorithms.
22
NL
+
ULeR
+
raeniL
NL
+
ULeR
+
raeniL
NL
+
ULeR
+
raeniL
NL
+
ULeR
+
raeniL
NL
+
ULeR
+
raeniLTable10: Numberoftrainableparametersineachmodel.
CNN-LSTM SoftMoE-LSTM CNN-S5
ParameterCount 197,254 316,102 193,670
Table11: Comparingthelayoutapolicywastrainedonversusonwhichitwasbeingevaluated. The
policiesheavilyoverfitthetraininglayout. Allpolicieswetestedexhibitthisproperty.
Asymm Cramped Counter Forced Coord
Asymm 343.4 0.0 0.0 0.0 0.0
Cramped 1.6 185.6 0.0 0.0 0.0
Counter 0.0 0.0 128.0 0.0 0.0
Forced 0.0 0.2 0.0 141.2 0.0
Coord 0.0 0.0 0.0 0.0 144.6
Table12: Meanepisodesolvedrateforthedifferentmethodsaveragedovertherespectivetesting
layouts. Thebestresultisshowninbold. Wereportaggregatestatisticsoverthreerandomseeds.
Method CNN-LSTM SoftMoE-LSTM CNN-S5
DR 0.02±0.0% 6.31±10.14% 0.00±0.0%
PLR⊥,∥ 0.00±0.0% 0.33±0.3% 0.00±0.0%
Pop. PAIRED 0.00±0.0% 14.62±7.6% 0.00±0.0%
ACCEL∥ 0.00±0.0% 0.08±0.1% 0.00±0.0%
Table13: Performanceonallevaluationlayouts. WeshowthemeanepisoderewardRandthemean
episodesolvedrateSR.Theoverallbestresultperlayoutispresentedinbold.
Layout Method CNN-LSTM SoftMoE-LSTM CNN-S5
R SR R SR R SR
DR 1.70 0.0% 1.54 0.2% 0.00 0.0%
PLR⊥,∥ 1.12 0.0% 5.02 2.1% 0.14 0.0%
Cramped
Pop. PAIRED 1.44 0.0% 37.02 57.7% 0.50 0.0%
ACCEL∥ 0.92 0.0% 0.60 0.0% 0.60 0.0%
DR 0.00 0.0% 0.00 0.0% 0.00 0.0%
PLR⊥,∥ 0.00 0.0% 0.00 0.0% 0.00 0.0%
Coord
Pop. PAIRED 0.00 0.0% 16.78 14.6% 0.00 0.0%
ACCEL∥ 0.00 0.0% 0.04 0.0% 0.02 0.0%
DR 0.00 0.0% 0.02 0.0% 0.00 0.0%
PLR⊥,∥ 0.00 0.0% 0.02 0.0% 0.02 0.0%
Forced
Pop. PAIRED 0.00 0.0% 0.00 0.0% 0.00 0.0%
ACCEL∥ 0.00 0.0% 0.00 0.0% 0.00 0.0%
DR 0.58 0.1% 8.64 4.4% 0.00 0.0%
PLR⊥,∥ 0.08 0.0% 0.10 0.0% 0.08 0.0%
Asymm
Pop. PAIRED 0.28 0.0% 15.64 14.2% 0.08 0.0%
ACCEL∥ 0.14 0.0% 0.04 0.0% 0.02 0.0%
DR 0.00 0.0% 0.00 0.0% 0.00 0.0%
PLR⊥,∥ 0.00 0.0% 0.00 0.0% 0.00 0.0%
Counter
Pop. PAIRED 0.00 0.0% 1.38 0.0% 0.00 0.0%
ACCEL∥ 0.00 0.0% 0.00 0.0% 0.00 0.0%
23200
100
100
50
0 0
0 1 2 3 4 0 1 2 3 4
Steps in Environment 1e7 Steps in Environment 1e7
(a)CoordinationRing(6x9) (b)CrampedRoom(6x9)
150
200
100
50 100
0 0
0 1 2 3 4 0 1 2 3 4
Steps in Environment 1e7 Steps in Environment 1e7
(c)ForcedCoordination(6x9) (d)AsymmetricAdvantages(6x9)
100
50
0
0 1 2 3 4
Steps in Environment 1e7
(e)CounterCircuit(6x9)
Figure8: RunsusedfortheFCPevaluationpopulationswithrandomseeds1–8fortheOGCwith
√
bandsreportingstandarderrorσ/ n. Layoutswerepaddedtoatotalsizeof6x9tobecomptibleto
thegeneralpoliciestrainedviaDCD.
Table 14: Zero-shot results using SoftMoE-LSTM policies playing with an FCP population of
expertstrainedontherespectivelayoutexclusively. Wereportthemeanepisoderewardandstandard
deviation. Thebestresultperlayoutisputinbold.
Method Asymm Counter Cramped Forced Coord
Random 7.43±12.19 8.89±4.65 66.02±38.28 1.95±1.92 20.49±7.82
Stay 5.32±12.07 0.38±1.11 20.67±33.05 0.00±0.00 0.95±2.73
DR 18.18±1.69 6.86±5.27 65.05±5.15 1.09±0.21 17.88±10.27
PLR⊥,∥ 7.64±0.89 5.60±1.29 60.35±6.89 1.76±0.86 21.90±1.26
Pop.PAIRED 24.51±3.44 11.11±1.67 81.92±6.33 1.59±0.57 29.72±4.72
ACCEL∥ 8.60±0.98 10.23±0.85 65.46±4.62 1.81±1.25 19.19±1.93
24
draweR
naeM
draweR
naeM
draweR
naeM
draweR
naeM
draweR
naeMTable15: Zero-shotresultsusingSoftMoE-LSTMpoliciesplayingwithanFCPpopulationofexperts
trainedontherespectivelayoutexclusively. Wereportthemeansolvedrateandstandarddeviation.
Thebestresultperlayoutisputinbold.
Method Asymm Counter Cramped Forced Coord
Random 8.52±17.52% 5.00±6.70% 69.43±38.45% 0.00±0.00% 30.89±3.83%
Stay 6.81±18.04% 0.02±0.14% 21.75±33.71% 0.00±0.00% 0.14±0.74%
DR 24.19±4.60% 4.56±5.32% 72.11±6.29% 0.01±0.01% 23.76±18.85%
PLR⊥,∥ 8.84±1.31% 2.04±0.95% 68.14±1.21% 0.11±0.12% 30.89±3.83%
Pop.PAIRED 32.48±4.00% 7.91±1.38% 85.54±6.08% 0.09±0.07% 48.31±11.08%
ACCEL∥ 9.58±1.12% 6.79±0.91% 69.01±2.03% 0.06±0.06% 24.13±6.01%
25