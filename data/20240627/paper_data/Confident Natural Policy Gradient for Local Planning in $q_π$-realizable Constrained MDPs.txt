Confident Natural Policy Gradient for Local Planning
in q -realizable Constrained MDPs
π
TianTian LinF.Yang
UniversityofAlberta,Edmonton UniversityofCalifornia,LosAngeles
ttian@ualberta.ca linyang@ee.ucla.edu
CsabaSzepesvári
UniversityofAlberta,GoogleDeepMind,Edmonton
szepesva@ualberta.ca
Abstract
TheconstrainedMarkovdecisionprocess(CMDP)frameworkemergesasanim-
portantreinforcementlearningapproachforimposingsafety orothercriticalob-
jectiveswhile maximizingcumulativereward. However, the currentunderstand-
ingofhowtolearnefficientlyinaCMDPenvironmentwithapotentiallyinfinite
numberofstatesremainsunderinvestigation,particularlywhenfunctionapprox-
imation is applied to the value functions. In this paper, we address the learn-
ingproblemgivenlinearfunctionapproximationwithq -realizability,wherethe
π
valuefunctionsofallpoliciesarelinearlyrepresentablewithaknownfeaturemap,
asettingknowntobemoregeneralandchallengingthanotherlinearsettings. Uti-
lizingalocal-accessmodel,weproposeanovelprimal-dualalgorithmthat,after
O˜(poly(d)ǫ 3)1 queries, outputs with high probability a policy that strictly sat-
−
isfies the constraints while nearly optimizing the value with respect to a reward
function. Here, d isthe featuredimensionandǫ > 0 isa givenerror. Thealgo-
rithmreliesonacarefullycraftedoff-policyevaluationproceduretoevaluatethe
policy using historical data, which informs policy updates through policy gradi-
ents and conservessamples. To our knowledge, this is the first result achieving
polynomialsamplecomplexityforCMDPintheq -realizablesetting.
π
1 Introduction
Intheclassicalreinforcementlearning(RL)framework,optimizingasingleobjectiveaboveallelse
canbechallengingforsafety-criticalapplicationslikeautonomousdriving,robotics,andLargeLan-
guage Models (LLMs). For example, it may be difficult for an LLM agent to optimize a single
reward that fulfills the objective of generatinghelpfulresponseswhile ensuringthat the messages
are harmless (Daietal., 2024). In autonomous driving, designing a single reward often requires
reliance on complex parameters and hard-coded knowledge, making the agent less efficient and
adaptive(Kamranetal.,2022). Optimizingasingleobjectiveinmotionplanninginvolvescombin-
ingheterogeneousquantitieslikepathlengthandrisks,whichdependonconversionfactorsthatare
notnecessarilystraightforwardtodetermine(FeyzabadiandCarpin,2014).
The constrained Markov decision process (CMDP) framework (Altman, 2021) emerges as an
important RL approach for imposing safety or other critical objectives while maximizing cu-
mulative reward (WachiandSui, 2020; Daietal., 2024; Kamranetal., 2022; Wenetal., 2020;
1HereO˜(·)hideslogfactors.
Preprint.Underreview.
4202
nuJ
62
]GL.sc[
1v92581.6042:viXraGirardandRezaEmami,2015;FeyzabadiandCarpin,2014). Inadditiontothesinglerewardfunc-
tionoptimizedunderastandardMarkovdecisionprocess(MDP),CMDPconsidersmultiplereward
functions, with one designated as the primary reward function. The goal of a CMDP is to find a
policythatmaximizestheprimaryrewardfunctionwhilesatisfyingconstraintsdefinedbytheother
rewardfunctions. Althoughtheresultsofthispapercanbeappliedtomultipleconstraintfunctions,
forsimplicityofpresentation,weconsidertheCMDPproblemwithonlyoneconstraintfunction.
Our current understanding of how to learn efficiently in a CMDP environmentwith a potentially
infinitenumberofstatesremainslimited,particularlywhenfunctionapproximationisappliedtothe
valuefunctions.Mostworksstudyingthesampleefficiencyofalearnerhavefocusedonthetabular
orsimplelinearCMDPsetting(seerelatedworksformoredetails). However,therehasbeenlittle
workinthemoregeneralsettingssuchastheq -realizability,whichassumesthevaluefunctionof
π
allpoliciescanbeapproximatedbyalinearcombinationofafeaturemapwithunknownparameters.
UnlikeLinearMDPs(YangandWang,2019;Jinetal.,2020),wherethetransitionmodelisassumed
to be linearlyrepresentablebya featuremap, q -realizabilityonlyimposesthe assumptionon the
π
existenceofafeaturemaptorepresentvaluefunctionsofpolicies.
Nevertheless,thegeneralityofq -realizabilitycomeswithaprice,asitbecomesconsiderablymore
π
challengingtodesigneffectivelearningalgorithms,evenfortheunconstrainedsettings. Forthegen-
eralonlinesetting,weareonlyawareofonesample-efficientMDPlearningalgorithm(Weiszetal.,
2023), which, however, is computationally inefficient. To tackle this issue, a line of research
(Kearnsetal., 2002; Yinetal., 2022; Haoetal., 2022; Weiszetal., 2022) applies the local-access
model, wherethe RL algorithmcanrestartthe environmentfromanyvisited states - a settingthat
is also practically motivated, especially when a simulator is provided. The local-access model is
moregeneralthanthegenerativemodel(Kakade,2003;Sidfordetal.,2018;YangandWang,2019;
Lattimoreetal.,2020;Vaswanietal., 2022), whichallowsvisitationto arbitrarystatesin anMDP.
Thelocal-accessmodelprovidestheabilitytounlockboththesampleandcomputationalefficiency
oflearningwith q -realizabilityforthe unconstrainedMDPsettings. However,itremainsunclear
π
whetherwecanharnessthepoweroflocal-accessforCMDPlearning.
In this paper, we present a systematic study of CMDP for large state spaces, given q -realizable
π
functionapproximationinthelocal-accessmodel.Wesummarizeourcontributionsasfollows:
• We design novel, computationally efficient primal-dual algorithms to learn CMDP near-
optimalpolicieswiththelocal-accessmodelandq -realizablefunctionclasses. Thealgo-
π
rithmscanreturnpolicieswithsmallconstraintviolationsorevennoconstraintviolations
andcanhandlemodelmisspecification.
• We provide theoretical guarantees for the algorithms, showing that they can compute an
ǫ-optimalpolicywithhighprobability,makingnomorethanO˜(poly(d)ǫ 3)queriestothe
−
local-accessmodel.Thereturnedpoliciescanstrictlysatisfytheconstraint.
• Underthemisspecificationsettingwithamisspecificationerrorω,weshowthatouralgo-
rithms achieve an O˜(ω) +ǫ sub-optimality with high probability, maintaining the same
sampleefficiencyofO˜(poly(d)ǫ 3).
−
2 Related works
Most provably efficient algorithms developed for CMDP are in the tabular and linear MDP
settings. In the tabular setting, most notably are the works by (Efronietal., 2020; Liuetal.,
2021; ZhengandRatliff, 2020; Vaswanietal., 2022; Kalagarlaetal., 2021; Yuetal., 2021;
Gattamietal.,2021;HasanzadeZonuzyetal.,2021;Chenetal.,2021;Kitamuraetal.,2024). Work
by Vaswanietal. (2022) have showed their algorithm uses no more than O˜ SA samples
(1 γ)3ǫ2
−
to achieve relaxed feasibility and O˜ SA samples to achieve strict f(cid:16)easibility.(cid:17)Here, the
(1 γ)5ζ2ǫ2
−
γ [0,1)is the discountfactorand(cid:16)ζ (0, 1 (cid:17)] is the Slater’sconstant, whichcharacterizesthe
∈ ∈ 1 γ
size ofthefeasibleregionandhencethehardn−essoftheCMDP.Intheirwork,theyhavealsopro-
videdalowerboundofΩ SA onthesamplecomplexityunderstrictfeasibility. However,
(1 γ)5ζ2ǫ2
−
alltheaforementionedresu(cid:16)ltsallscale(cid:17)polynomiallywiththecardinalityofthestatespace.
2For problems with large or possibly infinite state spaces, works by (Jainetal., 2022; Dingetal.,
2021; MiryoosefiandJin, 2022; Ghoshetal., 2024; Liuetal., 2022) have used linear function ap-
proximations to address the curse of dimensionality. All these works, except Jainetal. (2022);
Liuetal.(2022), makethelinearMDPassumption,wherethetransitionfunctionislinearlyrepre-
sentable.
Underthegenerativemodel,fortheinfinitehorizondiscountedcase,theonlinealgorithmproposed
inJainetal.(2022)achievesaregretofO˜(√d/√K)withO˜(√d/√K)constraintviolation,where
K is the number of iterations. Work by Liuetal. (2022) is able to achieve a faster O(ln(K)/K)
convergence rate for both the reward suboptimality and constraint violation. For the online ac-
cess setting under linear MDP assumption, Dingetal. (2021); Ghoshetal. (2024) achieve a re-
gret of O˜(poly(d)poly(H)√T) with O˜(poly(d)poly(H)√T)) violations, where T is the number
ofepisodesandH isthehorizonterm.
MiryoosefiandJin(2022)presentedanalgorithmthatachievesasamplecomplexityofO˜ d3H6 ,
ǫ2
wheredisthedimensionofthefeaturespaceandH isthehorizonterminthefinitehorizon(cid:16)CMD(cid:17)P
setting. Inthemoregeneralsettingunderq -realizability,thebest-knownupperboundsarein the
π
unconstrainedMDPsetting.
Inthe unconstrainedMDPsettingwith accessto a local-accessmodel,earlyworkbyKearnsetal.
(2002) have developed a tree-search style algorithms under this model, albeit in the tabular set-
ting. Underv -realizability,Weiszetal.(2021)presentedaplannerthatreturnsanǫ-optimalpolicy
∗
using O((dH/ǫ) ) queries to the simulator. More works by (Yinetal., 2022; Haoetal., 2022;
|A|
Weiszetal., 2022) have considered the local-access model with q -realizability assumption. Re-
π
centworkby Weiszetal. (2022) haveshown theiralgorithmcanreturna near-optimalpolicythat
achievesasamplecomplexityofO˜ d .
(1 γ)4ǫ2
−
(cid:16) (cid:17)
3 Problem formulation
ConstrainedMDP
Weconsideraninfinite-horizondiscountedCMDP( , ,P,γ,r,c,b,s )consistingapossiblyinfi-
0
S A
nitestatespace withfiniteactions ,atransitionprobabilityfunctionP : ,adiscount
factorγ
[0,1S ),arewardfunctionA
r :
[0,1],aconstraintfunctS io× nA
c
→
:
△S
[0,1],
∈ S ×A → S ×A →
a constraintthresholdb, and a fixed initial state s . Given any stationaryrandomizedpolicy
0
∈ S
π : andtherewardfunction,we definetheaction-valuefunctionwith respecttoa state-
actioS np→ air△ (sA,a)asq πr(s,a)=. E[ ∞ t=0γtr(S t,A t) |S 0 =s,A 0 =a].Theexpectationistakenover
randomnessofthetrajectoryinducedbytheinteractionbetweenpolicyπandtransitionfunctionP.
The action-valuefunctionof the cPonstraintfunctionqc is definedsimilarly to the rewardfunction.
π .
Forthestate-valuefunctionofastates ,wehavevr(s)= π( s),qr(s, ) . Likewise,thevalue
functionoftheconstraintfunctionvc is∈ deS finedsimilarπ lytothh ere· w| ardπ funct· ioi n.
π
TheobjectiveoftheCMDPistofindapolicyπthatmaximizesthestate-valuefunctionvr starting
π
fromagivenstates ,whileensuringthattheconstraintvc(s ) bissatisfied:
0 ∈S π 0 ≥
max vr(s ) s.t. vc(s ) b, (1)
π ∈Πrand π 0 π 0 ≥
whereΠ isthesetofstationaryrandomizedpoliciesoftheCMDP.Weassumetheexistenceof
rand
afeasiblesolutiontoeq.(1)andletπ denotethesolutiontoeq.(1). AquantityuniquetoCMDPis
∗ .
theSlater’sconstant,whichisdenotedasζ = max vc(s ) b. Slate’sconstantcharacterizesthe
π π 0 −
sizeofthefeasibilityregion,andhencethehardnessoftheproblem.
Because the state space can be large or possibly infinite, we use linear functionapproximationto
approximatethevaluesofstationaryrandomizedpolicies. Letφ : Rd beafeaturemap,
S ×A →
wemakethefollowingassumption:
Definition1 (q -realizability) There exists B > 0 and a misspecificationerror ω 0 such that
π
foreveryπ Π ,thereexistsaweightvectorw Rd, w B, andensur≥ es q (s,a)
rand π π 2 π
∈ ∈ k k ≤ | −
w ,φ(s,a) ωforall(s,a) .
π
h i|≤ ∈S×A
3We assume to have access to a local access model, where the agent can only query simulator for
statesthathavebeenencounteredinprevioussimulations. Then,ourgoalistodesignanalgorithm
thatreturnsanear-optimalmixturepolicyπ¯,whoseperformancecanbecharacterisedintwoways.
Foragiventargeterrorǫ>0,the
relaxed feasibility requires the returned policy π¯ whose sub-optimality gap vr (s ) vr(s ) is
π∗ 0 − π¯ 0
boundedbyǫ,whileallowingforasmallconstraintviolation.Formally,werequireπ¯ suchthat
vr (s ) vr(s ) ǫ s.t vc(s ) b ǫ. (2)
π∗ 0 − π¯ 0 ≤ π¯ 0 ≥ −
On the other hand, strict-feasibility requires the returned policy π¯ whose sub-optimality gap
vr (s ) vr(s )isboundedbyǫwhilenotallowinganyconstraintviolation.Formally,werequire
π∗ 0 − π¯ 0
π¯ suchthat
vr (s ) vr(s ) ǫ s.t vc(s ) b. (3)
π∗ 0 − π¯ 0 ≤ π¯ 0 ≥
Notations
For any integer i, we let [i] = 1, ,i and [0,i] = 0,1, ,i . For any integers i ,i , let
1 2
[i , ,i ] to mean i ,i
+1,{ · ,· i· .}
For any real
n{ umbe· r· x· }R,
we let x to denote the
1 2 1 1 2
sma· ll· e·
st integer i
suc{
h that i
·· x·
.
Fo}
r a vector of values x
∈Rd,
we
use⌊ x⌋
= x ,
≤ ∈. k k1 i| i |
x = x2, and x = max x . We let proj (λ) = argmin λ p, and
k truk n2
c
(y)i =.i
min
k mak x∞
y,a
,ai | .i | Foranytwopo[a s1 it,a iv2 e] numbersa,b,wp ∈e[a w1, ra it2 e]|
a
=−P O|
(b)if
[a1,pa2]P
{ {
1
}
2
}
thereexistsanabsoluteconstantc >0suchthata cb. WeusetheO˜ tohideanypolylogarithmic
≤
terms.
4 Confident-NPG-CMDP, a local-accessalgorithm forCMDP
4.1 Aprimal-dualapproach
WeapproachsolvingtheCMDPproblembyframingitasanequivalentsaddle-pointproblem:
maxminL(π,λ),
π λ 0
≥
where L : Π R R is the Lagrangefunction. For a policyπ Π and a Lagrange
rand + rand
multiplierλ R × ,weha→ ve ∈
+
∈ .
L(π,λ)=vr(s )+λ(vc(s ) b).
π 0 π 0 −
Let (π ,λ ) be the solution to this saddle-pointproblem. By an equivalenceto a LP formulation
∗ ∗
andstrongduality(Altman,2021),π isthepolicythatachievestheoptimalvalueintheCMDPas
∗ .
definedineq.(1). TheoptimalLagrangemultiplierλ = argmin L(π ,λ),Therefore,solving
∗ λ 0 ∗
eq.(1)isequivalenttofindingthesaddle-pointoftheLagrangefunc≥tion.
Atypicalprimaldualalgorithmthatfindsthesaddle-pointwillproceedinaniterativefashionalter-
natingbetweenapolicyupdateusingpolicygradientandadualvariableupdateusingmirrordescent.
Thepolicygradientiscomputedwithrespecttotheprimalvalueqp =qr +λ qc andthemir-
πk,λk πk k πk
rordescentiscomputedwithrespecttotheconstraintvaluevc (s )= π ( s ),qc (s , ) .
πk+1 0 h k+1 ·| 0 πk+1 0 · i
Giventhatwedonothaveaccesstoanoracleforexactpolicyevaluations,wemustcollectdatato
estimatetheprimalandconstraintvalues.
Ifwehavetheleast-squaresestimatesofqr andqc ,denotedbyQr andQc,respectively,thenwe
cancomputetheleast-squaresestimateQpπ =k Qr+πk
λ Qc
tobetheek stimatek
oftheprimalvalueqr .
k k k k πk
Additionally,wecancomputeVc (s ) = π ( s ),Qc(s , ) tobetheleast-squaresestimate
k+1 0 h k+1 ·| 0 k 0 · i
oftheconstraintvaluevc (s ). Then,foranygivens,a ,ouralgorithmmakesapolicy
πk+1 0 ∈ S ×A
updateofthefollowingform:
π (as) π (as)exp(η Qp(s,a)), (4)
k+1 | ∝ k | 1 k
followedbyadualvariableupdateofthefollowingform:
λ λ η Vc (s ) b ,
k+1 ← k − 2 k+1 0 −
wheretheη andη arethestep-sizes.
1 2 (cid:0) (cid:1)
4Algorithm1Confident-NPG-CMDP
1: Input: s (initialstate),ǫ(targetaccuracy),δ (0,1](failureprobability);γ (discountfactor)
0
∈
2: Initialize:
3: Define K,η ,m according to Theorem 1 for relaxed-feasibility and Theorem 2 for strict-
1
feasibility,
4: SetL K /( m +1) .
5: Forea← chi⌊ t⌊ erat⌋ ion⌊ k ⌋ [0,K⌋ ]:π Unif( ), Q˜p(, ) 0, Q˜c(, ) 0,andλ 0.
∈ k ← A k · · ← k · · ← k ←
6: Foreachphasel [L+1]: (), D
l l
∈ C ← ←{}
7: Fora : if(s ,a) ActionCov( ),thenappend(s ,a)to andset toD [(s ,a)] ⊲
0 0 0 0 0 0
∈A 6∈ C C ⊥
seeActionCovdefinedineq.(6)
8: whileTruedo ⊲mainloop
9: Letℓbethesmallestintegers.t. D [z ]= forsomez
ℓ ′ ′ ℓ
⊥ ∈C
10: Letzbethefirststate-actionpairin s.t. D [z]=
ℓ ℓ
C ⊥
11: Ifℓ=L+1,thenreturn K1 ⌊ kK =0⌋−1π k
⌊ ⌋
12: k ℓ ( m +1) P ⊲iterationcorrespondingtophaseℓ
ℓ
← × ⌊ ⌋
13: (result,discovered) Gather-data(π , ,α,z)
←
kℓ Cℓ
14: ifdiscoveredisTruethen
15: Appendresultto andset toD [result] ⊲resultisastate-actionpair
0 0
C ⊥
16: Gotoline8
17: D [z] result
ℓ
←
18: if z s.t. D [z ]= then
′ ℓ ℓ ′
6∃ ∈C ⊥
19: k k +( m +1)ifk +( m +1) K otherwise K
ℓ+1 ℓ ℓ
← ⌊ ⌋ ⌊ ⌋ ≤⌊ ⌋ ⌊ ⌋
20: fork =k ,...,k 1do ⊲off-policyiterationsreusing ,D
ℓ ℓ+1 ℓ ℓ
21: Qr, Qc LSE− ( ,D ,π ,π ) C
k k ← Cℓ ℓ k kℓ
22: Fors Cov( ) Cov( ),andfora
ℓ ℓ+1
23:
Q˜∈p(s,a)C \
trunc
C Qr(s,a)+∈ λA
trunc Qc(s,a)
k ← [0, 1−1 γ] k k [0, 1−1 γ] k
24: Q˜c(s,a) Qc(s,a)
k ← k
25: ⊲updatepolicy
26: Fors,a :
∈S×A
π (as) ifs Cov( )
k+1 ℓ+1
27: π k+1(a |s) ←(π k(a |s)| a′∈Aπe kx (p a( ′η |s1 )Q˜ ep k x( ps (, ηa 1) Q)
˜p k(s,a′))
othe∈ rwise C
28: ⊲updatedualvariable P
29: V˜c (s ) trunc π ( s ),Q˜c(s , )
k+1 0 ← [0, 1−1 γ]h k+1 ·| 0 k 0 · i
λ ifs Cov( )
k+1 0 ℓ+1
30: λ ∈ C
k+1 ←(proj
[0,U]
λ
k
−η 2(V˜ kc +1(s 0) −b) otherwise.
(cid:16) (cid:17)
31: Forz s.t. z : appendzto andset toD [z]
ℓ ℓ+1 ℓ+1 ℓ+1
∈C 6∈C C ⊥
54.2 Coresetandleastsquareestimates
Toconstructtheleast-squaresestimates,letusassumefornowthatwearegivenasetofstate-action
pairs, whichwe callthe coreset . Byorganizingthefeaturevectorofeachstate-actionpairin
row-wiseintoamatrixΦ R C d,wecanwritethecovariancematrixasV( ,α)=. Φ Φ +αIC .
|C|× ⊤
Foreachstate-actionpairCi∈ n , supposewe haverunMonteCarlorolloutsusiC ngtherollCoutCpolicy
C
π withthelocalaccesssimulatortoobtainanaveragedMonteCarloreturndenotedbyq¯. Thenfor
anystate-actionpairs,a ,theleast-squareestimateofaction-valueq isdefinedtobe
π
∈S×A
.
Q(s,a)= φ(s,a),V( ,α) 1Φ q¯ . (5)
− ⊤
h C C i
Sincethealgorithmcanonlyrelyonestimatesforpolicyimprovementandconstraintevaluation,it
is imperativethat these estimates closely approximatetheir true action values. In the localaccess
setting,analgorithmmaynotbeabletovisitallstate-actionpairs,sowecannotguaranteethatthe
estimateswillcloselyapproximatethetrueactionvaluesforallstate-actionpairs. However,wecan
ensuretheaccuracyoftheestimatesforasubsetofstates.
Given , let us define a set of state-action pairs whose features satisfies the condition
C
kφ(s,a) kV( C,α)−1 ≤1,thenwecallthissettheaction-coverof C:
.
ActionCov( C)= {(s,a) ∈S×A: kφ(s,a) kV( C,α)−1 ≤1 }. (6)
Followingfromtheaction-cover,wehavethecoverof . Forastatestobeinthecoverof ,allits
C C
actionsa ,thepair(s,a)isintheaction-coverof . Inotherwords,
∈A C
.
Cov( )= s : a ,(s,a) ActionCov( ) .
C { ∈S ∀ ∈A ∈ C }
For any s Cov( ), we can ensure the least square estimate Q(s,a) defined by eq. (5) closely
∈ C
approximatesitstrueactionvalueq (s,a)foralla . However,suchacoreset isnotavailable
π
∈A C
beforethealgorithmisrun.Therefore,weneedanalgorithmthatwillbuildacoresetincrementally
in the local-access setting while planning. To achieve this, we build our algorithm on CAPI-QPI-
Plan (Weiszetal., 2022), usingsimilar methodologyforcoreset buildinganddata gathering. For
thepseudo-codeofouralgorithmConfident-NPG-CMDP,pleaseseealgorithm1.
4.3 Coresetbuildinganddatagatheringtocontroltheaccuracyoftheleast-square
estimates
Confident-NPG-CMDPdoesnotcollectdataineveryiterationbutcollectsdatainintervalofm =
O ln(1+c)poly(ǫ 1(1 γ) 1) , where c 0 is set by the user. By setting c to a non-zero
− −
− ≥
value places an upper boundof 1+c on the per-trajectoryimportancesampling ratio used in the
off(cid:0)-policyevaluation. The m is th(cid:1)en set accordingly. The total number of data collection phases
is L = K /( m +1) . When c is set to 0, we have L = K, recovering a purely on-policy
⌊⌊ ⌋ ⌊ ⌋ ⌋
versionofthealgorithm.TheimportancesamplingratioisusedintheLSE-subroutine(algorithm4
inappendixA)forcomputinganunbiasedq¯usedineq.(5).
Intheiterationthatcorrespondstoadatacollectionphase,thealgorithmperformson-policyevalua-
tion.Betweenanytwodatacollectionphases,thealgorithmperforms( m +1)numberofoff-policy
⌊ ⌋
evaluationsreusing data collected duringthe on-policyiteration. By setting c to a non-zerovalue
placesanupperboundof1+contheper-trajectoryimportancesamplingratiousedintheoff-policy
evaluation. Themisthensetaccordingly. Whencissetto0,wehaveL = K,recoveringapurely
on-policyversion of the algorithm. The importance sampling ratio is used in the LSE-subroutine
(algorithm4inappendixA)forcomputinganunbiasedq¯usedineq.(5).
Similar to CAPI-QPI-Plan of Weiszetal. (2022), Confident-NPG-CMPD maintains a set of core
sets( ) andasetofpolicies(π ) . Duetotheoff-policyevaluations,Confident-NPG-
l l [L+1] k k [K]
CMDC P al∈so maintains a set of data (D ) ∈ . Initially, all core sets are set to the empty set, all
l l [L]
policiesaresettotheuniformpolicy,anda∈llD areempty.
l
Theprogramstartsbyaddinganyfeaturevectorofs ,aforalla thatarenotintheaction-cover
0
∈A
ofC . Thosefeaturevectorsareconsideredinformative. Fortheinformativestate-actionpairs,the
0
algorithmaddsan entry in D and set the value to the placeholder . Thenthe problemfinds an
0
⊥
l [L]inline9ofalgorithm1,suchthatthecorrespondingD hasanentrythatdoesnothaveany
l
∈
6roll-outdata. Iftherearemultiplephaseswiththeplaceholdervalue,thenstartwiththelowestlevel
phasel [L]suchthatD containstheplaceholdervalue. Whensuchaphaseisfound,arunning
l
∈
phasestarts,andisdenotedbyℓinalgorithm1. Wenotethatonlyduringarunningphase,willthe
nextphasecoreset beextendedbyline31,thepoliciesbeupdatedbyline27,anddualvariable
ℓ+1
C
updatedbyline30.
Duringtheroll-outperformedinGather-datasubroutine(algorithm3inappendixA),ifanys,a
∈
isnotintheaction-coverof ,itisaddedto . Onceastate-actionpairisaddedtoacore
ℓ 0
S ×A C C
set byline 7 andline 31, it remainsin thatcoreset forthe durationof the algorithm. Thismeans
that any , l [L+1] can grow in size and be extendedmultiple times duringthe executionof
l
C ∈
the algorithm. When any new state-action pair is addedto , the least-squareestimate should be
l
C
recomputed with the newly added information. This would mean the policy need to be updated
anddatarerun. Wecanavoidrestartingthedatacollectionprocedurebyfollowingasimilarupdate
strategyofCAPI-QPI-Plan(Weiszetal.,2022)forupdatingthepolicyandthedualvariable.
Whenanewstate-actionpairisaddedto ,foreachcorrespondingiterationk [k , ,k 1],
ℓ ℓ ℓ+1
C ∈ ··· −
the least-squaresestimate is recomputedby the LSE subroutine(algorithm4 in appendixA) with
the newly added information. However, the refreshed least-squares estimate Qp is only used to
k
update the policy of states that are newly covered by (i.e., s Cov( ) Cov( )) using
ℓ ℓ ℓ+1
C ∈ C \ C
the updateeq. (4). Foranystates thatarealreadycoveredby (i.e., s Cov( )), the policy
ℓ ℓ+1
remainsunchangedasitwasfirstupdatedbyeq.(4)withtheleC ast-square∈ sestimaC teQp atthattime.
The primalestimate Q˜p in line 23 of algorithm1 capturesthe value with which π is updated.
k k+1
We wanttheaccuracyguaranteeofQ˜p(s,a)with respecttoqp (s,a)notjustforπ butforan
k πk,λk k
extendedsetofpoliciesdefinedasfollows:
Definition2 For any policy π from the set of randomizedpolicies Π and any subset ,
rand
X ⊆ S
theextendedsetofpoliciesisdefinedas:
Π
π,
= π′ Π
rand
π( s)=π′( s)foralls . (7)
X { ∈ | ·| ·| ∈X}
By maintaining a set of core sets, gathering data via the Gather-data subroutine (algorithm 3 in
appendixA),makingpolicyupdatesbyline 27,anddualvariableupdatesbyline30,wehave:
Lemma1 WhenevertheLSE-subroutineonline 21ofConfident-NPG-CMDPisexecuted,forall
k [k , ,k 1],foralls Cov(C )anda ,theleast-squareestimateQ˜p(s,a)satisfies
∈ ℓ ··· ℓ+1 − ∈ ℓ ∈A k
thefollowing,
Q˜p(s,a) qp (s,a) ǫ forallπ Π , (8)
| k − π k′,λk |≤ ′ k′ ∈ πk,Cov( Cℓ)
where ǫ = (1+U)(ω +√αB +(ω +ǫ) d˜) with d˜= O˜(d) and U is an upperbound on the
′
optimalLagrangemultiplier. Likewise,
p
Q˜c(s,a) qc (s,a) ω+√αB+(ω+ǫ) d˜ forallπ Π , (9)
| k − π k′ |≤ k′ ∈ πk,Cov( Cℓ)
p
Theaccuracyguaranteeofeq.(8)andeq.(9) aremaintainedthroughouttheexecutionofthealgo-
rithm. By lemma 4.5ofWeiszetal. (2022) (restatedin lemma 6in appendixA), forany past to
Cl
bea pastversionof andπpast bethe correspondingpolicyassociatedwith past, thenwe have
Cl k Cl
Π Π . If we have eq. (8) and eq. (9) being true for any policy from
Ππk,Cov( Cl) ⊆ ,π thkpa es nt, iC to wv( iClllp aas lst)
obetrueforanyfutureπ .
πpast,Cov( past) k
k Cl
5 Confident-NPG-CMDP satisfiesrelaxed-feasibility
With the accuracy guarantee of the least-square estimates, we prove that at the termination of
Confident-NPG-CMDP, the returnedmixture policy π¯ satisfies relaxed-feasibility. We note that
K
becauseoftheexecutionofline31inalgorithm1,attermination,onecanshowusinginductionthat
allthe forl [L+1]willbethesame. Therefore,thecoverof foralll [L+1]arealsoequal.
l l
C ∈ C ∈
Thus,itissufficienttoonlyconsider attheterminationofthealgorithm.Byline7ofalgorithm1,
0
C
wehaveensureds Cov( ).
0 0
∈ C
7By the primal-dualapproachdiscussed in section 4, we have reducedthe CMDP probleminto an
unconstrainedproblemwithasinglerewardoftheformr = r+λc. Therefore,wecanapplythe
λ
value-differencelemma(lemma14)ofConfident-NPGinthesinglerewardsetting(seeappendixA)
to Confident-NPG-CMDP. Then, we can show the value difference between π and π¯ can be
∗ K
bounded,whichleadsto:
Lemma2 Let δ (0,1] be the failure probability, ǫ > 0 be the target accuracy, and s be the
0
∈
initial state. Assuming for all s Cov( ) and all a , Q˜p(s,a) qp (s,a) ǫ and
∈ C0 ∈ A | k − π k′,λk | ≤ ′
Q˜c(s ,a) qc (s ,a) (ω+√αB+(ω+ǫ) d˜)forallπ Π ,then,withprobability
| k 0 − π k′ 0 |≤ k′ ∈ πk,Cov( C0)
1 δ,Confident-NPG-CMDPreturnsamixture ppolicyπ¯
K
thatsatisfiesthefollowing,
−
5ǫ ( 2ln(A)+1)(1+U)
vr (s ) vr (s ) ′ + ,
π∗ 0 − π¯K 0 ≤ 1 γ (1 γ)2√K
− p −
5ǫ ( 2ln(A)+1)(1+U)
b vc (s ) [b vc (s )] ′ + ,
− π¯K 0 ≤ − π¯K 0 + ≤ (1 γ)(U λ ) (1 γ)2(U λ )√K
− − ∗ p − − ∗
.
whereǫ = (1+U)(ω+(√αB +(ω+ǫ) d˜))withd˜= O˜(d), andU isanupperboundonthe
′
optimalLagrangemultiplier.
p
Bysettingtheparameterstoappropriatevalues,itfollowsfromlemma2thatweobtainthefollowing
result:
Theorem1 With probability 1 −δ, the mixture policy π¯ K = k1 K k=− 01π k returned by confident-
NPG-CMDPensuresthat
P
vr (s ) vr (s )=O˜(√d(1 γ) 2ζ 1ω)+ǫ,
π∗ 0 − π¯K 0 − − −
vc (s ) b O˜(√d(1 γ) 2ζ 1ω)+ǫ .
π¯K 0 ≥ − − − −
(cid:16) (cid:17)
if we choose n = O˜(ǫ 2ζ 2(1 γ) 4d), α = O ǫ2ζ2(1 γ)4 , K = O˜ ǫ 2ζ 2(1 γ) 6 ,
− − − − − −
− − −
η = O˜ (1 γ)2ζK 1/2 , η = ζ 1K 1/2, H = O˜ (1 γ) 1 , m = O˜ ǫ 1ζ 2(1 γ) 2 ,
1 − − 2 − − (cid:0) − −(cid:1) (cid:0) − − − − (cid:1)
andL= K/( m +1) =O˜ ǫ 1(1 γ) 4 totalnumberofdatacollectionphases.
(cid:0)⌊ ⌊ ⌋ ⌋ (cid:1) − − − (cid:0) (cid:1) (cid:0) (cid:1)
Furthermore, the algorithm uti(cid:0)lizes at most O˜(cid:1)(ǫ 3ζ 3d2(1 γ) 11) queries in the local-access
− − −
−
setting.
Remark1:Inthepresenceofmisspecificationerrorω >0,therewardsuboptimalityandconstraint
violationisO˜(ω)+ǫwiththesamesamplecomplexity.
Remark 2: Suppose the Slader’s constant ζ is much smaller than the suboptimality bound of
O˜(ω)+ǫ,anditisreasonabletosetζ = ǫ. Then,thesamplecomplexityisO˜(ǫ 6(1 γ) 11d2),
− −
−
whichisindependentofζ.
Remark3: We note that our algorithmrequiresthe knowledgeof the Slater’s constantζ, which
canbeestimatedbyanotheralgorithm.
6 Confident-NPG-CMDP satisfiesstrict-feasibility
Inthissection,weshowthatthereturnedmixturepolicyπ¯ byConfident-NPG-CMDPsatisfiesthe
K
strictfeasibility.
In orderto obtainan ǫ-optimalpolicythatsatisfies constraint: vc b, we considera morecon-
π¯K ≥
servativeCMDPthatwe callitthe surrogateCMDP. The surrogateCMDP isdefinedbythe tuple
.
( , ,P,r,c,b,s ,γ),whereb = b+ fora 0. Wenotethatb bandtheoptimalpolicy
′ 0 ′ ′
S A △ △ ≥ ≥
ofthissurrogateCMDPisdefinedasfollows:
π argmaxvr(s ) s.t. vc(s ) b. (10)
△∗ ∈ π 0 π 0 ≥ ′
8Notice that π is a more conservativepolicy than π , where π is the optimalpolicy of the orig-
∗ ∗ ∗
inal CMDP o△bjectiveeq. (1). By solving this surrogateCMDP using Confident-NPG-CMDPand
applyingtheresultoftheorem1,weobtainaπ¯ thatwouldsatisfy
K
vr (s ) vr (s ) ¯ǫ s.t. vc (s ) b ǫ¯,
π¯∗ 0 − π¯K 0 ≤ π¯K 0 ≥ ′ −
whereǫ¯= O˜(ω)+ǫ. Expandingoutb,wehavevc (s ) b+ ǫ¯. Ifwecanset suchthat
′ π¯K 0 ≥ △− △
¯ǫ 0,thenvc (s )ǫ b,whichsatisfiesstrict-feasibility. Weshowthisformallyinthenext
△− ≥ π¯K 0 ≥
theorem,where = O(ǫ(1 γ)ζ)andisincorporatedintothealgorithmicparametersforeaseof
△ −
presentation.
Theorem2 With probability 1 δ, a target ǫ > 0, the mixture policy π¯ returned
K
by confident-NPG-CMDP ensure− s that vr (s ) vr (s ) ǫ and vc (s ) b, if
π∗ 0 − π¯K 0 ≤ π¯K 0 ≥
assuming the misspecification error ω ǫζ2(1 γ)3(1 + d˜) 1, and if we choose
−
≤ −
α = O ǫ2ζ3(1 γ)5 ,K = O˜ ǫ 2ζ 4(1 γ) 8 ,n = O˜ ǫ 2ζ 4(1 γ) 8d ,H =
− − − p− − −
− − −
O˜ (1 γ) 1 ,m = O˜ ǫ 1ζ 2(1 γ) 3 ,andL = K/( m +1) = O˜((ǫ 1ζ 2(1 γ) 5))
− (cid:0)− (cid:1) − − −(cid:0) − ⌊(cid:1) ⌊ ⌋ (cid:0)⌋ − − −(cid:1) −
totaldatacollectionphases.
(cid:0) (cid:1) (cid:0) (cid:1)
Furthermore, the algorithm utilizes at most O˜(ǫ 3ζ 6(1 γ) 14d2) queries in the local-access
− − −
−
setting.
7 Conclusion
We have presented a primal-dual algorithm for planning in CMDP with large state spaces,
given q -realizable function approximation. The algorithm, with high probability, returns a pol-
π
icy that achieves both the relaxed and strict feasibility CMDP objectives, using no more than
O˜(ǫ 3d2poly(ζ 1(1 γ) 1))queriestothelocal-accesssimulator.
− − −
−
Ouralgorithmdoesnotquerythesimulatorandcollectdataineveryiteration.Instead,thealgorithm
queriesthesimulatoronlyatfixedintervals. Betweenthesedatacollectionintervals,ouralgorithm
improvesthepolicyusingoff-policyoptimization. Thisapproachmakesit possibletoachievethe
desiredsamplecomplexityinbothfeasibilitysettings.
References
Altman,E.(2021). ConstrainedMarkovdecisionprocesses. Routledge.
Chen,Y., Dong,J., andWang,Z.(2021). Aprimal-dualapproachtoconstrainedmarkovdecision
processes. arXivpreprintarXiv:2101.10895.
Dai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y. (2024). Safe RLHF:
Safereinforcementlearningfromhumanfeedback. InTheTwelfthInternationalConferenceon
LearningRepresentations.
Ding,D.,Wei,X.,Yang,Z.,Wang,Z.,andJovanovic,M.(2021).Provablyefficientsafeexploration
via primal-dualpolicy optimization. In InternationalConference on Artificial Intelligence and
Statistics,pages3304–3312.PMLR.
Efroni,Y.,Mannor,S.,andPirotta,M.(2020).Exploration-exploitationinconstrainedmdps.CoRR,
abs/2003.02189.
Feyzabadi, S. and Carpin, S. (2014). Risk-aware path planning using hirerachical constrained
markovdecisionprocesses. In2014IEEEInternationalConferenceonAutomationScienceand
Engineering(CASE),pages297–303.
Gattami, A., Bai, Q., and Aggarwal, V. (2021). Reinforcement learning for constrained markov
decision processes. In International Conference on Artificial Intelligence and Statistics, pages
2656–2664.PMLR.
Ghosh,A.,Zhou,X.,andShroff,N.(2024).Towardsachievingsub-linearregretandhardconstraint
violation in model-free rl. In InternationalConference on Artificial Intelligence and Statistics,
pages1054–1062.PMLR.
9Girard, J. and Reza Emami, M. (2015). Concurrent markov decision processes for robot team
learning. EngineeringApplicationsofArtificialIntelligence,39:223–234.
Hao,B.,Lazic,N.,Yin,D.,Abbasi-Yadkori,Y.,andSzepesvari,C.(2022). Confidentleastsquare
valueiterationwithlocalaccesstoasimulator. InInternationalConferenceonArtificialIntelli-
genceandStatistics,pages2420–2435.PMLR.
HasanzadeZonuzy,A.,Kalathil,D.,andShakkottai,S.(2021). Model-basedreinforcementlearning
forinfinite-horizondiscountedconstrainedmarkovdecisionprocesses. IJCAI2021.
Jain, A., Vaswani, S., Babanezhad, R., Szepesvari, C., and Precup, D. (2022). Towards painless
policy optimization for constrained mdps. In Uncertainty in Artificial Intelligence, pages 895–
905.PMLR.
Jin,C.,Yang,Z.,Wang,Z.,andJordan,M.I.(2020).Provablyefficientreinforcementlearningwith
linearfunctionapproximation. InConferenceonlearningtheory,pages2137–2143.PMLR.
Kakade,S.M.(2003). Onthesamplecomplexityofreinforcementlearning. UniversityofLondon,
UniversityCollegeLondon(UnitedKingdom).
Kalagarla, K. C., Jain, R., and Nuzzo, P. (2021). A sample-efficientalgorithmfor episodic finite-
horizonmdpwithconstraints. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume35,pages8030–8037.
Kamran, D., Simão, T. D., Yang, Q., Ponnambalam, C. T., Fischer, J., Spaan, M. T., and Lauer,
M.(2022). Amodernperspectiveonsafeautomateddrivingfordifferenttrafficdynamicsusing
constrainedreinforcementlearning. In 2022IEEE25thInternationalConferenceon Intelligent
TransportationSystems(ITSC),pages4017–4023.IEEE.
Kearns, M., Mansour, Y., and Ng, A. Y. (2002). A sparse sampling algorithm for near-optimal
planninginlargemarkovdecisionprocesses. Machinelearning,49:193–208.
Kitamura, T., Kozuno,T., Kato, M., Ichihara,Y., Nishimori, S., Sannai, A., Sonoda,S., Kumagai,
W., andMatsuo, Y. (2024). A policygradientprimal-dualalgorithmforconstrainedmdpswith
uniformpacguarantees. arXivpreprintarXiv:2401.17780.
Lattimore,T.andSzepesvári,C.(2020). Banditalgorithms. CambridgeUniversityPress.
Lattimore,T.,Szepesvari,C.,andWeisz,G.(2020). Learningwithgoodfeaturerepresentationsin
banditsandinrlwithagenerativemodel.InInternationalconferenceonmachinelearning,pages
5662–5670.PMLR.
Liu, T., Zhou, R., Kalathil, D., Kumar, P. R., and Tian, C. (2021). Learningpolicieswith zeroor
boundedconstraintviolationforconstrainedmdps. CoRR,abs/2106.02684.
Liu,T.,Zhou,R.,Kalathil,D.,Kumar,P.R.,andTian,C.(2022).Policyoptimizationforconstrained
mdpswithprovablefastglobalconvergence.
Miryoosefi, S. and Jin, C. (2022). A simple reward-free approach to constrained reinforcement
learning. InInternationalConferenceonMachineLearning,pages15666–15698.PMLR.
Paternain,S., Chamon,L., Calvo-Fullana,M.,andRibeiro, A.(2019). Constrainedreinforcement
learninghaszerodualitygap. AdvancesinNeuralInformationProcessingSystems,32.
Sidford,A., Wang, M., Wu, X., Yang,L., and Ye, Y. (2018). Near-optimaltime and samplecom-
plexities for solving markov decision processes with a generative model. Advances in Neural
InformationProcessingSystems,31.
Vaswani, S., Yang, L., and Szepesvári, C. (2022). Near-optimal sample complexity bounds for
constrainedmdps. AdvancesinNeuralInformationProcessingSystems,35:3110–3122.
Wachi,A.andSui,Y.(2020).Safereinforcementlearninginconstrainedmarkovdecisionprocesses.
InInternationalConferenceonMachineLearning,pages9797–9806.PMLR.
10Weisz, G., Amortila, P., Janzer, B., Abbasi-Yadkori,Y., Jiang, N., and Szepesvári, C. (2021). On
query-efficientplanninginmdpsunderlinearrealizabilityoftheoptimalstate-valuefunction. In
ConferenceonLearningTheory,pages4355–4385.PMLR.
Weisz, G., György,A., andSzepesvári,C. (2022). Confidentapproximatepolicyiterationforeffi-
cientlocalplanninginqpirealizablemdps. arXivpreprintarXiv,2210.
Weisz,G.,György,A.,andSzepesvári,C.(2023).Onlinerlinlinearlyqπ-realizablemdpsisaseasy
asinlinearmdpsifyoulearnwhattoignore.
Wen,L.,Duan,J.,Li,S.E.,Xu,S.,andPeng,H.(2020).Safereinforcementlearningforautonomous
vehiclesthroughparallelconstrainedpolicyoptimization. In2020IEEE23rdInternationalCon-
ferenceonIntelligentTransportationSystems(ITSC),pages1–7.
Yang, L. andWang, M. (2019). Sample-optimalparametricq-learningusinglinearlyadditivefea-
tures. InInternationalconferenceonmachinelearning,pages6995–7004.PMLR.
Yin,D.,Hao,B.,Abbasi-Yadkori,Y.,Lazic´,N.,andSzepesvári,C.(2022). Efficientlocalplanning
withlinearfunctionapproximation.InInternationalConferenceonAlgorithmicLearningTheory,
pages1165–1192.PMLR.
Yu, T., Tian, Y., Zhang, J., and Sra, S. (2021). Provably efficient algorithms for multi-objective
competitiverl. InInternationalConferenceonMachineLearning,pages12167–12176.PMLR.
Zheng,L.andRatliff, L.(2020). Constrainedupperconfidencereinforcementlearning. InBayen,
A. M., Jadbabaie, A., Pappas, G., Parrilo, P. A., Recht, B., Tomlin, C., and Zeilinger, M., edi-
tors,Proceedingsofthe2ndConferenceonLearningforDynamicsandControl,volume120of
ProceedingsofMachineLearningResearch,pages620–629.PMLR.
11A Confident-NPG inasinglereward setting
The pseudo code of Confident-NPG with a single reward setting is the same as Confident-NPG-
CMDPinalgorithm1,exceptthatline29toline30willnotappearinConfident-NPG.Additionally,
the LSE subroutine returns just Qr, and the policy update will be with respect to Q˜r. For the
completepseudocodeofConfident-NPGinthesinglerewardsetting,pleaseseealgorithm2. Inthe
followinganalysis,forconvenience,weomitthesuperscriptr.
Algorithm2Confident-NPG
Input: s 0(initialstate),ǫ(targetaccuracy),δ ∈(0,1](failureprobability),c ≥0,γ,K = (2 1ln γ( | )A4ǫ|) 2,
−
η
1
=(1 −γ) 2ln K( |A|),m=
2ǫ(1
γl )n l( n1+c)
4
,L= ⌊⌊K ⌋/( ⌊m ⌋+1) ⌋.
q − (cid:16)ǫ(1−γ)2 (cid:17)
Initialize: foreachiterationk [0,K]: π Uniform( ), Q˜p(s,a) 0, Q˜c(s,a) 0for
∈ k ← A k ← k ←
alls,a ,andλ 0. Foreachphasel [L+1]: (), D
k l l
∈S×A ← ∈ C ← ←{}
1: fora do
∈A
2: if(s ,a) ActionCov( )then
0 0
6∈ C
3: Append(s ,a)to ; D [(s ,a)]
0 0 0 0
C ←⊥
4: whileTruedo ⊲mainloop
5: Getthesmallestintegerℓs.t. D [z ]= forsomez
ℓ ′ ′ ℓ
⊥ ∈C
6: Getthefirststate-actionpairzin s.t. D [z]=
ℓ ℓ
C ⊥
7: ifℓ=L+1thenreturn K1 ⌊ kK =0⌋−1π k
⌊ ⌋
P
8: k ℓ ( m +1) ⊲iterationcorrespondingtophaseℓ
ℓ
← × ⌊ ⌋
9: (result,discovered) Gather-data(π , ,α,z)
←
kℓ Cℓ
10: ifdiscoveredisTruethen
11: ⊲resultisastate-actionpair
12: Appendresultto ; D [result]
0 0
C ←⊥
13: break
14: ⊲resultisasetofnH-horizontrajectories π startingatz
∼
kℓ
15: D [z] result
ℓ
←
16: if z s.t. D [z ]= then
′ ℓ ℓ ′
6∃ ∈C ⊥
17: k k +( m +1)ifk +( m +1) K otherwise K
ℓ+1 ℓ ℓ
← ⌊ ⌋ ⌊ ⌋ ≤⌊ ⌋ ⌊ ⌋
18: ⊲updatepolicyforeveryk [k ,k 1]using ,D
ℓ ℓ+1 ℓ ℓ
∈ − C
19: fork =k ,...,k 1do
ℓ ℓ+1
20: Qr ,_ LSE(− ,D ,π ,π )
k ← Cℓ ℓ k kℓ
21: ⊲updatevariablesandimprovepolicy
22: foralls Cov( ) Cov( ),andforalla do
ℓ ℓ+1
∈ C \ C ∈A
23: Q˜r(s,a) Trunc Qr(s,a)
k ← [0, 1−1 γ] k
24: foralls,a do
∈S×A
π (as) ifs Cov( )
k+1 ℓ+1
25: π k+1(a |s) ←(π k(a |s)| a′∈Aπe kx (p a( ′η |s1 )Q˜ er k x( ps (, ηa 1) Q)
˜r k(s,a′))
othe∈ rwise C
26: forz s.t. z do P
ℓ ℓ+1
∈C 6∈C
27: Appendzto ;D [z]
ℓ+1 ℓ+1
C ←⊥
A.1 TheGather-datasubroutine
Givenacoreset ,abehaviourpolicyµ,astartingstate-actionpair(s,a) alongwithsome
C ∈S×A
algorithmicparameters,theGather-datasubroutine(algorithm3)willeither1)returnanewlydiscov-
12Algorithm3Gather-data
Input: policy π, core set C, regression parameter α = 25ǫ2 B( 21 (− 1γ +) U2 ), H = ln(4/( 1ǫ(1 γ−γ)2)), n =
−
(1+c)2ln 8d˜L
(cid:16) δ (cid:17),startingstateandaction(s,a)
2ǫ2(1 γ)2
Initializ−e:
Trajectories ()
←
1: ifs Cov( )then
6∈ C
2: fora do
∈A
3: if(s,a) ActionCov( )thenreturn((s,a),True)
6∈ C
4: fori [n]do
5:
τi∈
()
s,a ←
6: Si s, Ai a
7: ap0 p← endSi,0 A←i toτi
0 0 s,a
8: forh=0,...,H 1do
9: Si ,Ri ,− Ci simulator(Si,Ai)
h+1 h+1 h+1 ← h h
10: ifSi Cov( )then
h+1 6∈ C
11: fora do
12: if∈ (SAi ,a) ActionCov( )thenreturn((Si ,a),True)
h+1 6∈ C h+1
13: ⊲nonewinformativefeaturesdiscovered
14: Ai π( Si )
h+1 ∼ ·| h+1
15: appendRi ,Ci ,Si ,Ai toτi
h+1 h+1 h+1 h+1 s,a
16: appendτi toTrajectories
s,a
return(Trajectories,False)
eredstate-actionpair,or2)returnasetofntrajectories.Eachtrajectoryisgeneratedbyrunningthe
behaviourpolicyµwiththesimulatorforH consecutivesteps. Fori = 1,...,n,letτi denotethe
s,a
ithtrajectorystartingfroms,a tobe Si = s,Ai = a,Ri,Ci, ,Si ,Ai ,Ri ,Ci ,Si .
Thenthei-thdiscountedcumulativere{ wa0 rdsG(τ si0 ,a)= H h1 =− 011 γh· R·· hi +1H . F− o1 raH ta− rg1 etpH olicyH π,thH e} n
theempiricalmeanofthediscountedsumofrewards
P
n
1
q¯(s,a)= ρ(τi )G(τi ), (11)
n s,a s,a
i=1
X
whereρ(τ si ,a)=ΠH h=−11 µπ (( AA ii h| SS h ii )) istheimportancesamplingratio.
h| h
Forsomegivens¯anda¯,weestablishthefollowingrelationshipbetweenthetargetpolicyπandthe
behaviorpolicyµ:
ln(1+c)
π(a¯s¯) µ(a¯s¯)exp(f(s¯,a¯)) s.t. sup f(s¯,a¯) , (12)
| ∝ | | |≤ 2H
s¯,a¯
where f(s¯,a¯) : R+ be a functionand c 0 is given and a constant. By establishing
therelationshipiS ne× q.A (1→ 2),theimportancesampling≥ ratioρ(τi )canbeboundedby1+casitis
s,a
proveninthefollowinglemma:
Lemma3 Supposethestate-actionpairs {(S h,A h) }H h=−11extractedfromatrajectory
.
τ = S ,A ,R ,S ,A , ,S ,A ,R µ,
0 0 1 1 1 H 1 H 1 H
{ ··· − − }∼
havetheirbehaviorpolicyµrelatedtoatargetpolicyπbytherelationineq.(12). Inthiscase,the
per-trajectoryimportancesamplingratio
π(A S )
ρ(τ)=ΠH h=−11 µ(Ah | Sh
)
≤1+c. (13)
h h
|
13.
Proof:Letl=sup s,a|f(s,a) |,wheref isdefinedineq.(12). Forany(s,a) ∈{(S h,A h) }H h=−11,
exp(f(s,a)) exp(l)
π(as)=µ(as) µ(as) (14)
| | µ(a s)exp(f(s,a)) ≤ | µ(a s)exp( l)
a′ ′| ′ a′ ′| −
µ(as)exp(2l). (15)
≤ | P P
Byassumption,l ln(1+c),thenexp(2lH) exp 2Hln(1+c) 1+c. (cid:4)
≤ 2H ≤ 2H ≤
(cid:16) (cid:17)
Then,wecanshowthatforall(s,a) , q¯s,a) q (s,a) ǫ,whereǫ>0isagiventargeterror.
( π
∈C | − |≤
Additionally,theaccuracyguaranteeof q¯s,a) q (s,a) ǫcontinuestoholdsfortheextended
( π
| − |≤
setofpoliciesdefinedindefinition2. Formally,westatethemainresultofthissection.
Lemma4 For any s,a , , the Gather-data subroutine will either return with
∈ S × A X ⊂ S
((s,a),True) for some s , or it will return with (D[(s,a)],False), where D[(s,a)] is a set
′ ′ ′
6∈ X
ofnindependenttrajectoriesgeneratedbyabehaviorpolicyµstartingfrom(s,a). WhenGather-
data returns False for (s,a), we assume 1) the behavior policy µ and target policy π for all the
statesandactionsencounteredinthetrajectoriesstoredinD[(s,a)]satisfyeq.(12)and2)q¯(s,a)is
a ren tuu rn nb qi ¯a (s se ,d a)es ct oim nsa tt re uco tf eE dπ fr′, os m,a[ D[(H h s=− ,0 a1 )γ ]h aR cch o+ r1 d] info gr ta ol el qπ .′ (∈ 11Π )wπ, iXll. ,wTh ite hn p, rt oh be aim bip lio tyrt 1ance δ-w ,eighted
′
P −
q¯(s,a) q π′(s,a) ǫ forallπ
′
Π
π,
.
| − |≤ ∈ X
Proof:TheprooffollowssimilarlogictoLemma4.2Weiszetal.(2022).
Recall D[(s,a)] stores n number of trajectories indexed by i, where each trajectory τi =
s,a
(Si = s,Ai = a,Ri,...,Si ) µ. The per-trajectory importance sampling ratio ρ(τi ) =
Π hH0 =−11π µ(( AA ii h|0 S Shi i) ),andth0 ereturH n− is1 ∼ H h=− 01γhR hi +1. Bythetriangleinequality, s,a
h| h
1 n P π(Ai Si)H −1
|q¯(s,a) −q π(s,a) |=
|n
ΠH h=−01 µ(Aih| Sh
i)
γhR hi −q π(s,a)
|
(16)
i=1 h| h h=0
X X
n H 1 H 1 H 1
1 ρ(τi ) − γhRi E − γhR + E − γhR q (s,a). (17)
≤|n s,a h+1− π,s,a h+1 | | π,s,a h − π |
i=1 h=0 h=0 h=0
X X X X
Thegoalistoboundeachofthetwotermsineq.(17)by ǫ sothatthesumofthetwois ǫ.
4 2
By assumption, π,µ for all {(S hi,Ai h) }H h=−11 extractedoutof the i-trajectoryτ si
,a
satisfies eq. (12).
Second, q¯(s,a) is assumed to be an unbiased estimate of E π,s,a H h=− 01γhR h+1 . Note that
each ρ(τ si ,a) H h=− 01γhR hi +1 for all i = 1,...,n are independenhtPrandom variabiles such that
ρ(τ si ,a) H h=− 0P1γhR hi
+1 ∈
0, 11+ γc . Thisis because1) H h=− 01γhR hi
+1 ≤
11
γ
since therewards
− −
takevaluesintherangeof[h0,1],anid2)ρ(τi ) 1+cbylemma3. WeapplyHoeffding’sinequal-
P s,a ≤ P
ity:
1 n H −1 H −1 2nǫ2
P ρ(τi ) γhRi E γhR >ǫ 2exp . (18)
(cid:12)n s,a h+1− π,s,a h+1 (cid:12) !≤  − 1+c 2
(cid:12) Xi=1 h X=0 h X=0 (cid:12) 1 γ
(cid:12) (cid:12)  − 
(cid:12) (cid:12)  (cid:16) (cid:17) 
(cid:12) (cid:12) 2n(ǫ)2
Then,wehavewithprobability1 δ /2,whereδ = 2exp 4 ,thefirsttermineq.(17),
− ′ ′ −(1+c)2
(cid:18) 1−γ (cid:19)
|n1 n i=1ρ(τ si ,a) H h=− 01γhR hi +1−E π,s,a H h=− 01γhR h+1
|≤
4ǫ. Forthesecondtermineq.(17),
P P H −1 P ∞ γH
E γhR q (s,a) = E γhR . (19)
π,s,a h π π,s,a h
| − | | |≤ 1 γ
h=0 h=H −
X X
BythechoiceofH,wehave γH ǫ. Puttingeverythingtogether,weget q¯(s,a) q (s,a) ǫ.
1 γ ≤ 4 | − π |≤ 2
Togetthefinalresult,weneed−tobound |q π(s,a) −q π′(s,a)
| ≤
2ǫ,sothat |q¯ (s,a) −q π′(s,a)
| ≤
q¯ (s,a) q π(s,a) + q π(s,a) q π′(s,a) ǫ.
| − | | − |≤
14Recall that π and π differs in distributions over states that are not in . For a trajectory S =
′ 0
X {
s,A =a,S ,... ,letT bethesmallestpositiveintegersuchthatstateS ,thenthedistribution
0 1 T
} 6∈X
ofthetrajectoryS
0
=s,A
0
=a,S 1,...,S
T
arethesameunderP
π,s,a
andP π′,s,abecauseπ( s)=
·|
π ( s)foralls . Then,
′
·| ∈X
T 1 T 1
q π(s,a) q π′(s,a) = E π,s,a − γtR t+γTv π(S T) E π′,s,a − γtR t+γTv π′(S T)
| − | (cid:12) " #− " #(cid:12)
(cid:12) Xt=0 Xt=0 (cid:12)
(cid:12) (2(cid:12)0)
(cid:12) (cid:12)
(cid:12) (cid:12)
= E π,s,a γTv π(S T) E π′,s,a γTv π′(S T) (21)
−
=(cid:12) (cid:12) P(cid:2)π,s,a(S T −1(cid:3)=s′,A T −(cid:2)1 =a′)P(S(cid:3)T(cid:12) (cid:12)|s′,a′)γTv π(S T) (22)
s′ ,a′
∈XX
P π′,s,a(S
T 1
=s ′,A
T 1
=a ′)P(S
T
s ′,a ′)γTv π′(S T) (23)
− − − |
s′ ,a′
∈XX
= P π,s,a(S
T 1
=s ′,A
T 1
=a ′)P(S
T
s ′,a ′)γT (v π(S T) v π′(S T)) (24)
− − | −
s′ ,a′
∈XX
1
P π,s,a(S
T 1
=s′,A
T 1
=a′)P(S
T
s′,a′)γT (25)
≤ 1 γ − − |
− s′ ,a′
∈XX
1 ∞
= P (S =s,A =a)P(S =s¯s,a)γt (26)
π,s,a t 1 ′ t 1 ′ t ′ ′
1 γ − − |
− s′ ,a′s¯ t=1
∈XX X6∈XX
H 1
1 −
= P (S =s,A =a)P(S =s¯s,a)γt (27)
π,s,a t 1 ′ t 1 ′ t ′ ′
1 γ − − |
− t=1 s′ ,a′s¯
X ∈XX X6∈X
1 ∞
+ P (S =s,A =a)P(S =s¯s,a)γt (28)
π,s,a t 1 ′ t 1 ′ t ′ ′
1 γ − − |
− t=Hs′ ,a′s¯
X ∈XX X6∈X
H 1
1 −
P π,s,a(S
t 1
=s′,A
t 1
=a′)P(S
t
=s¯s′,a′) (29)
≤ 1 γ − − |
− t=1 s′ ,a′s¯
X ∈XX X6∈X
γH ∞
+ γtP (S =s,A =a) (30)
π,s,a t+H 1 ′ t+H 1 ′
1 γ − −
− s′ ,a′t=0
∈XX X
H 1
1 −
P (S =s,A =a)P(S =s¯s,a) (31)
π,s,a t 1 ′ t 1 ′ t ′ ′
≤ 1 γ − − |
− t=1 s′ ,a′s¯
X ∈XX X6∈X
γH
+ (1 γ)d (s,a) (32)
(1 γ)2 − π,s,a ′ ′
− s′ ,a′
∈XX
H 1
1 −
P π,s,a(S
t 1
=s′,A
t 1
=a′)P(S
t
=s¯s′,a′) (33)
≤ 1 γ − − |
− t=1 s′ ,a′s¯
X ∈XX X6∈X
γH
+ . (34)
(1 γ)2
−
15Bythelawoftotalprobability,
P π,s,a(S
t
=s′,A
t
=a′) (35)
= Πt P(S S =s ,A =a )Πt π(A =a S =s ) (36)
i=0 i+1 | i i i i i=1 i i | i i
s1,...,s Xt,a1,...,at
π(A =a S =s )
= Πt P(S S =s ,A =a )Πt i i | i i µ(A =a S =s )
i=0 i+1 | i i i i i=1µ(A =a S =s ) i i | i i
i i i i
s1,...,s Xt,a1,...,at |
(37)
Πt P(S S =s ,A =a )Πt exp(2l)µ(A =a S =s ) (38)
≤ i=0 i+1 | i i i i i=1 i i | i i
s1,...,s Xt,a1,...,at
exp(2tl) Πt P(S S =s ,A =a )Πt µ(A =a S =s ) (39)
≤ i=0 i | i i i i i=1 i i | i i
s1,...,s Xt,a1,...,at
(1+c) Πt P(S S =s ,A =a )Πt µ(A =a S =s ) (40)
≤ i=1 i | i −1 i i −1 i i=1 i i | i i
s1,...,s Xt,a1,...,at
=(1+c)P (S =s,A =a). (41)
µ,s,a t ′ t ′
To get from eq. (37) to eq. (38), recall from the proof of lemma 3, we have π(a |s) exp(2l),
for l =. sup f(s,a). To go fromeq. (39) to eq. (40), we note that exp(2tl) µ(a e| xs) p(≤ 2Hl)since
s,a ≤
t [1,H),andexp(2Hl) 1+cforanylthatsatisfiestheconstraintofeq.(12). Insummary,we
∈ ≤
have
q π(s,a) q π′(s,a) (42)
| − |
H 1
1 −
(1+c)P (S =s,A =a)P(S =s¯s,a) (43)
µ,s,a t 1 ′ t 1 ′ t ′ ′
≤ 1 γ − − |
− t=1 s′ ,a′s¯
X ∈XX X6∈X
γH
+ (44)
(1 γ)2
−
1+c ǫ
P (1 T <H)+ . (45)
µ,s,a
≤ 1 γ ≤ 4
−
To bound P (1 T < H), let us recall that during the execution of Gather-data subroutine
µ,s,a
≤
(algorithm3),everytimethesimulatorreturnsastate-actionpairthatisnotintheaction-cover(i.e.,
dooesnotpass the certaintycheck),the subroutinestops. For each s,a , duringanyof the n
∈ X
rollouts,letI (s,a)denotetheindicatoreventthatduring1 T < H,S inrollouti. Then
i T
E [I (s,a)] = P (1 T < H). WeknowthatI (s,a≤ ) = 0,1 and6∈ thX enforanyǫ > 0,by
µ,s,a i µ,s,a i
≤ { }
anotherHoeffdingsinequality,withprobability1 δ /2,
′
−
n
1 ǫ(1 γ)
E [I (s,a)] I (s,a) − , (46)
µ,s,a i i
| − n |≤ 4(1+c)
i=1
X
Whengather-datasubroutinereturns,allindicatorsI (s,a) =0forall(s,a) andi [n],then
i
∈X ∈
wehave
ǫ(1 γ)
P (1 T <H) − . (47)
µ,s,a
≤ ≤ 4(1+c)
Puttingeverythingtogether,wehavetheresult. (cid:4)
A.2 TheLSEsubroutine
Givenacoreset ,asetoftrajectories,abehaviourpolicyµ,atargetpolicyπ,theLSEsubroutine
C
(algorithm4)returnsaleast-squareestimateQofq .
π
If the core set is empty, we define Q(, ) to be zero. Then, for a target accuracy ǫ > 0 and a
C · ·
uniformmisspecificationerrorωdefinedindefinition1,wehaveaboundontheaccuracyofq¯with
respecttoq asgivenbythenextlemma.
π
16Algorithm4LSE
Input: ,D,π ,π
C
k k′
1: fors,a do
2: fore∈ veC ryτi D[(s,a)]foreveryi [n]do
s,a ∈ ∈
3: extract Si,Ai,Ri,Ci,Si,Ai Si ,Ai fromτi
{ 0 0 1 1 1 1··· H H} s,a
4: computeGi r(s,a) ← H h=− 01γhR hi +1;Gi c(s,a) ← H h=− 01γhC hi +1
5: computeρi(s,a) ←Π PH h=−11 ππ kk ′(( AA i hi h || SS hh ii )) P
6: q¯r(s,a) 1 n ρi(s,a)Gi(s,a); q¯c(s,a) 1 n ρi(s,a)Gi(s,a)
← n i=1 r ← n i=1 c
7: wr Φ ⊤Φ +αP I −1 Φ ⊤q¯r; wc Φ ⊤Φ +αI −1P Φ ⊤q¯c
← C C C ← C C C
8: Qr(s,a(cid:0)) wr,φ(s(cid:1),a) ,Qc(s,a) (cid:0)wc,φ(s,a) f(cid:1)oralls,a
returnQ←r,Qhc i ←h i
Lemma5 [Lemma 4.3 of Weiszetal. (2022)] Let π be a randomized policy. Let =
(s ,a ) be a set of state-action pairs of set size N N. Assume for all i C [N],
i i i [N]
{
q¯(s ,a
)}∈
q (s ,a ) ǫ. Then,foralls,a ,
∈ ∈
i i π i i
| − |≤ ∈S×A
|Q(s,a) −q π(s,a) |≤ω+ kφ(s,a) kV( C,α)−1 √αB+(ω+ǫ)√N . (48)
(cid:16) (cid:17)
Proof:Letw betheparameterthatsatisfies
∗
inf sup φ(s,a) w q (s,a) ǫ, (49)
⊤ π
w ∈Rd, kw k2≤Bs,a ∈S×A| − |≤
foralls,a andanǫ > 0. Letw¯ = V(C) 1 φ(s¯,a¯)φ(s¯,a¯) w . Notethatw¯ is
∈ S ×A ∗ − s¯,a¯ ⊤ ∗ ∗
obtainedw.r.ttostate-actionpairsin . Foranys,a ∈,C
C ∈SP×A
Q(s,a) q π(s,a) φ(s,a)⊤(w w¯∗) + φ(s,a)⊤(w¯∗ w∗) + φ(s,a)⊤w∗ q π(s,a).
| − |≤| − | | − | | − |
(50)
Byapplyingq realizabilityassumption(eq.(49)),wehave φ(s,a) w q (s,a) ǫ.Tobound
π ⊤ ∗ π
− | − |≤
thesecondtermineq.(50),wehave
|φ(s,a) ⊤(w¯ ∗ −w ∗) |≤kφ(s,a) kV(C)−1 kw¯ ∗ −w ∗ kV(C) (51)
≤kφ(s,a) kV(C)−1 kV(C) −1 (φ(s¯,a¯)φ(s¯,a¯) ⊤ +αI)w ∗ −αIw ∗ −w ∗ kV(C) (52)
s¯,a¯ C
X∈ (cid:0) (cid:1)
= φ(s,a) V(C)−1 αV(C)−1w∗ V(C) (53)
k k k− k
=α kφ(s,a) kV(C)−1 kw ∗ kV(C)−1 (54)
≤α kφ(s,a) kV(C)−1 kw ∗ kα1I (55)
1
≤α kφ(s,a) kV(C)−1 αB =√αB kφ(s,a) kV(C)−1. (56)
r
Let α be the smallest eigenvalue of V(C), then by eigendecomposition, V(C) = QΛQ 1
−
≥
Q(αI)Q=αQQ αI sinceQQ isorthonormal.ThisimpliesthatV(C) 1 1I,whichleads
⊤ ≥ ⊤ − ≤ α
toeq.(55). Finally,weboundthefirsttermineq.(50). Bydefinitionofleast-square,q¯=Φw +ξ,
∗
whereξistheerror.Recallthatw=V(C) 1 φ(s¯,a¯)q¯s¯,a¯),andbyassumptionthatforany
− s¯,a¯ (
s¯,a¯ , q¯s¯,a¯) q (s¯,a¯) ǫ,then ∈C
∈C | ( − π |≤ P
ξ(s¯,a¯) = q¯s¯,a¯) φ(s¯,a¯) w q¯s¯,a¯) q (s¯,a¯) + q (s¯,a¯) φ(s¯,a¯) w (57)
( ⊤ ∗ ( π π ⊤ ∗
| | | − |≤| − | | − |
ǫ+ω. (58)
≤
17Itfollowsthatforalls,a ,
∈S×A
φ(s,a) (w w¯ ) = V( ) 1 φ(s¯,a¯) q¯s¯,a¯) φ(s¯,a¯) w ,φ(s,a) (59)
⊤ ∗ − ( ⊤ ∗
| − | |h C − i|
s¯,a¯
X∈C (cid:0) (cid:1)
= V( ) 1 φ(s¯,a¯)ξ(s¯,a¯),φ(s,a) (60)
−
|h C i|
s¯,a¯
X∈C
V( ) 1φ(s¯,a¯)ξ(s¯,a¯),φ(s,a) (61)
−
≤ |h C i|
s¯,a¯
X∈C
(ω+ǫ) V( ) 1φ(s¯,a¯),φ(s,a) (62)
−
≤ |h C i|
s¯,a¯
X∈C
(ω+ǫ) V( ) 1φ(s¯,a¯),φ(s,a) 2 byHolder’sinequality (63)
≤ |C| h C − i
ss¯,a¯
p X∈C
(ω+ǫ) φ(s,a) V( ) 1 φ(s¯,a¯)φ(s¯,a¯) V( ) 1φ(s,a) (64)
≤ |C|v u ⊤ C −  ⊤ C −
p
u
u
s¯ X,a¯
∈C
t  
(ω+ǫ) φ(s,a) (I αV( ) 1)V( ) 1φ(s,a) (65)
≤ |C| ⊤ − C − C −
q
(ω+ǫ)p φ(s,a) V( ) 1φ(s,a) becauseV( ) 1 (1/α)I (66)
≤ |C| ⊤ C − C − ≤
q
=(ω+ǫ)p |C|kφ(s,a)
kV(
C)−1. (67)
p
Altogether,foranys,a ,
∈S×A
|Q(s,a) −q π(s,a) |≤ω+ kφ(s,a) kV( C)−1 √αB+(ω+ǫ) |C| . (68)
(cid:16) p (cid:17)
(cid:4)
A.3 Theaccuracyofleast-squareestimates
Given a core set and a target policy π, lemma 5 ensures that Q(s,a) q (s,a) = O(ω +ǫ)
π
C | − |
for anys within Cov( ). This accuracycomesfromthe factthat for alls Cov( ), the feature
C ∈ C
v ace cc uto rr acφ y(s is,a m) as ia nt ti as ifi ne es dk uφ n( ds e, ra t) hk eV f( rCa, mα) e− w1 o≤ rk1 offo or ural al la go∈ ritA hm. I ,n wt hh ii cs hs ie nc cti lo un d, esw ce ov ne sr ti rf uy ctw inh get th he er ct oh ri es
set,updatingaction-values,andimprovingpolicyupdates. Allofwhichplaysaroleinmaintaining
theaccuracyoftheQestimates.
Wenotethatpolicyimprovementscanonlyoccurduringarunningphaseℓ. Whenall(s,a)pairsin
havetheirplaceholdervalue replacedbytrajectories,algorithm2 executesline 17toline 27.
ℓ
C ⊥
During each iteration from k to k 1, the LSE subroutine is executed. The accuracy of q¯
ℓ ℓ+1 k
−
isused toboundthe estimationerrorin lemma5. Therefore,we will firstverifythattheaccuracy
guarantee of q¯ (s,a), used in lemma 4, is indeed satisfied by the main algorithm and maintained
k
throughoutitsexecution.
Onceastate-actionpairisaddedtoacoreset,itremainsinthatcoresetforthedurationofthealgo-
rithm.Thismeansthatany forl [L+1]cangrowinsize. Whenacoreset isextendedduring
l l
C ∈ C
a running phase when ℓ = l, the least-square estimates will need be updated based on the newly
extended containing newly discovered informative features. However, the improved estimates
l
C
willonlybeusedtoupdatethepolicyofstatesthatarenewlycovered,whicharestatesthatarein
Cov( ) Cov( ). WebreakdownQ˜p toreflectthevaluebasedonwhichπ isupdated,
Cℓ \ Cℓ+1 k k+1
Q˜ (s,a) ifs Cov( )
k ℓ+1
∈ C
Q˜ (s,a) Q (s,a) ifs Cov( ) Cov( ) (69)
k ← k ∈ Cℓ \ Cℓ+1
initialvalue0 ifs Cov( ℓ),
6∈ C

18where Q (s,a) = trunc Qr(s,a). Respective to Q˜ , we have the corresponding policy
k [0, 1−1 γ] k k
updateasfollows,
π (as) ifs Cov( )
k+1 ℓ+1
| ∈ C
π (as) π ( s)exp η trunc Q (s,a) ifs Cov( ) Cov( ) (70)
k+1 | ← ∝ k ·| 1 [0, 1−1 γ] k ∈ Cℓ \ Cℓ+1
π (as) (cid:16) (cid:17) ifs Cov( ).
k ℓ
| 6∈ C
Foralls Cov(),theQ˜ (s, )willbethevalueoftheleast-squareestimateatthetimethepolicy
ℓ k
∈ C ·
makesanNPGupdateoftheform:
π ( s) π ( s)exp η trunc Q (s,a) . (71)
k+1 ·| ∝ k ·| 1 [0, 1−1 γ] k
(cid:16) (cid:17)
Thisisbecauseattheendoftheloopafterline27isrun,thenextphasecoreset = ,which
ℓ+1 ℓ
C C
makeanystatesthatarecoveredby alsobecoveredby .Astatethatwasoncenewlycovered
ℓ ℓ+1
C C
by willnolongerbenewlycoveredagain.IfthealgorithmwastomakeanNPGupdateforstates
ℓ
C
thatarenewlycoveredinsomeloop,inanysubsequentloopwiththesamevalueℓ,thepolicywill
remain unchanged. By updatingpolicies accordingto line 25 of algorithm 2 with resepectto Q˜ ,
k
wehave:
Lemma6 [Lemma4.5ofWeiszetal.(2022)] Foranyl [L],let past beanypastversionof
∈ Cl Cl
and πpast for k [k , ,k 1] be the correspondingpolicies associated with past, then at
k ∈ l ··· l+1 − Cl
anylaterpointduringtheexecutionofthealgorithm,π Π Π forall
k ∈ πk,Cov( Cl) ⊆ π kpast,Cov( Clpast)
k [k , ,k 1].
l l+1
∈ ··· −
Lemma7 For any l [L], for any states to have been covered by , they will continue to be
l
∈ C
coveredby throughouttheexecutionofthealgorithm.
l
C
Proof: Let us consider a past version of and denote it as past. For any states to have been
Cl Cl
covered by past, it will continue to be covered by any future extensions of . This is because
Cl Cl
V( past,α) V( ,α)andthereforeCov( past) Cov( ). (cid:4)
Cl (cid:23) Cl Cl ⊆ Cl
Lemma8 Whenever LSE-subroutine of Confident-NPG is executed, for all iterations k
∈
[k ,...,k 1], for any (s,a) , the importance weighted return q¯ (s,a) is an unbiased
ℓ ℓ+1 ℓ k
− ∈ C
estimateofE π k′,s,a H h=− 01R h+1 forallπ k′ ∈Π πk,Cov( Cℓ).
h i
P
Proof: When LSE-subroutine is executed with ℓ = l, we consider two scenarios. Case 1: The
trajectoriesfor a (s,a) are generatedand storedin D [(s,a)] forthe first time. Case 2: The
ℓ l
∈ C
trajectories for a (s,a) were already generated and stored in D [(s,a)] during a previous
ℓ l
∈ C
iterationwhenℓ=l.
Case1: We considerthecasewhenLSE-subroutineisexecutedwithℓ = l,wherethetrajectories
ofa (s,a) is generatedandsavedtoD [(s,a)] forthefirsttime. Foranytrajectoriestohave
l l
∈ C
been saved to D would mean that the encountered states within the trajectories are in Cov( ).
l l
C
Otherwise,theGather-datawouldhavereturned‘discoveredisTrue’,andanewlydiscoveredstate-
actionpairwouldbeaddedto ,interruptingtheroll-outprocedure. Consequently,notrajectories
0
C
wouldhavebeensavedtoD [(s,a)].
l
Weletτi denotethei-thtrajectory Si =s,Ai =a,Ri,Si,...,Si ,Ai ,Ri generatedby
π
inters a,a ctingwiththesimulator,and{ th0 erearen0 suchtra1 ject1 oriesstH or−e1 dinH D−1 [(s,aH )}
]. Then,forall
kl l
k [k , ,k 1],thereturn
l l+1
∈ ··· −
1 n π (Ai Si)
q¯ k(s,a)=
n
ΠH h=−11 πk (Ah i| Sh i)G(τ si ,a). (72)
i=1 kl h| h
X
Thebehaviorpolicyπ isupdatedinapreviousloopthroughthealgorithmwhenℓ=l 1. Bythe
kl
−
timeLSE-subroutineisexecuted,π willhavebeenthe mostrecentπ to generatethedata. For
kl kl
subsequentiterations,startingwithk =k +1uptok =k 1,thepolicyπ isupdatedbasedin
l l+1 k
−
iterationk 1. Thus,bythetimeLSE-subroutineisexecutedforanykwithin[k +1,...,k 1],
l l+1
− −
19boththemostrecentpolicyπ andthebehaviourpolicyπ areavailableforthecomputationofthe
k kl
importancesamplingratio:ρ k(τ si ,a)=ΠH h=−11 ππ kk l( (A Ai h
i
h| |S Shi hi) ).
For k = k , the importance sampling ratio ρ (τi ) = 1. The importance weighted return
l k s,a
ρ (τi )G(τi )isanunbiasedestimateofE [G(τi )]:
k s,a s,a πk,s,a s,a
H 1
E ρ (τi ) − γhRi (73)
πkl,s,a
"
k s,a h+1
#
h=0
X
H 1
=E πkl,s,a "δδ (( ss ,, aa )) PP (( SS 11 || SS 00 == ss ,, AA 00 == aa )) ππ kk l( (A A1 1| |S S1 1) ). .. .. .. ππ kk l( (A AH H− −1 1| |S SH H− −1 1) )
h
X=− 0γhR hi +1
#
(74)
H 1
−
=E γhRi , (75)
πk,s,a h+1
" #
h=0
X
whereδ(s,a)isthedirac-deltafunction.
Because all of the states encounteredin the trajectories are in Cov( ), the importance sampling
l
ratio ρ (τi ) wouldhavebeenproducedbyanypolicyπ Π C andπ Π
k s,a k′ l ∈ πkl,Cov( Cl) k′ ∈ πk,Cov( Cl)
bydefinition2. Thus,thereturnρ k(τ s,a)G(τ si ,a)isanunbiasedestimateofE π′,s,a[G(τ si ,a]forall
k
π Π ,andthisistrueforalli=1,...,n.Consequently,q¯ (s,a)isanunbiasedestimate
ofk′ E∈
π
k′,sπ ,k a, [Cov H h( =C −l 01) R h+1]forallforallπ
k′
∈Π
πk,Cov(
Cl). k
Case2: WPe considerthecasewhenLSE-subroutineisexecutedwithℓ = l,wherethetrajectories
ofa(s,a) isgeneratedinapreviousloopthroughthealgorithmwithℓ=l.Letusdenote at
ℓ l
thetimeof∈ thC isdataacquisitionas past,andthestoreddataasDpast[(s,a)]. ThetrajectoriesstC ored
Cl l
inDpast[(s,a)]weregeneratedfromthepastbehaviourpolicyπpast interactingwiththesimulator.
l kl
Likewise,weletπpasttodenotethepasttargetpoliciesatthetimeafter26wasexecutedforeachof
k
theiterationsintherangeof[k +1, ,k 1]. Finally,wealsohavethecorrespondingτi,past
l ··· l+1 − s,a
todenotethei-thtrajectorythatisstoredinDpast[(s,a)].
l
BythetimeLSE-subroutineisrunwithℓ=lagain,bythesameargumentsmadeforCase1,forany
k [k , ,k 1]andanyi = 1,...,n, theimportanceweightedreturnρ (τi,past)G(τi,past)
∈ l ··· l+1 − k s,a s,a
is an unbiased estimate of E [G(τi,past)] for all π˜past Π . By lemma 6,
π˜k,s,a s,a k ∈ π kpast,Cov( Clpast)
the most recent π Π . Then, ρ (τi,past)G(τi,past) is an unbiased estimate
of E
[G(τi,pask t)]∈
.
Byπ kp da est fi,C nio tv io(
C
nlpa 2s ,t)
any π
k
Π
s,a s,a
Π ) and π
Π
πk,s,a s,a
Π would have
pk′ ro∈ duceπ dk t,C heov s( aC ml) e⊆ impoπ rkp ta as nt c,C eo sv a( Cmlp pas lt
ing
ratio,k′ l th∈
is
imπ pk ll i, eC sov t( hC al t) ρ⊆ k(τ siπ ,, apkp aa sst t, )C Go (v τ( C si ,,lp apa as st) t) is an unbiased estimate of E
π
k′,s,a[G(τ si ,, apast)] for all π
k′
∈
Π .
πk,Cov( Cl)
Once Dpast[(s,a)] is populated with trajectories, Dpast[(s,a)] remain unchanged throughoutthe
l l
execution of the algorithm. Therefore, G(τi,past) will never change again. Thus, G(τi ) =
s,a s,a
G(τ si ,, apast), and we have ρ k(τ si ,a)G(τ si ,a) is an unbiased estimate of E
π
k′,s,a[G(τ si ,a)] for all π
k′
∈
Π , andthisis trueforalli = 1,...,n. Consequently,q¯ (s,a)isanunbiasedestimate of
E
ππ k′k ,, sC ,ao [v( Cl
H
h)
=− 01R h+1]forallforallπ k′ ∈Π πk,Cov( Cl).
k
(cid:4)
P
Lemma9 WheneverLSE-subroutineofConfident-NPGisexecuted, foranys Cov( ), the be-
ℓ
∈ C
haviourpolicyπ ( s)andtargetpolicyπ ( s)fork [k +1, ,k 1]satisfyeq.(12).
kℓ
·|
k
·| ∈
ℓ
···
ℓ+1
−
Proof:Recallthatthebehaviorpolicyπ isupdatedinapreviousloopthroughthealgorithmwhen
kℓ
ℓ = l 1. By the time LSE-subroutine is executed, π will have been the most recent π to
−
kℓ kℓ
generatethedata. Fortheon-policyiterationwherek = k ,thepolicyπ servesasboththetarget
ℓ kℓ
andbehaviorpolicy,makingeq.(12)triviallysatisfied.
For subsequent iterations, starting with k = k +1 up to k = k 1, in iteration k 1, the
ℓ ℓ+1
− −
policyπ wouldhaveeitherundergoneanNPG updateintheformofeq.(71)forthefirsttimeor
k
20remainunchangedintheformofeq.(71)withrespecttosomepastπ andπ . Eitherway,forany
k kℓ
s Cov( ),thetargetpolicyπ andbehaviourpolicyπ wouldberelatedtoeachotherinform
∈
Cℓ k kℓ
ofeq.(71). SinceQ˜ (s,a) 1 foranyt [k ,k 1],thenitfollowsthat
t ≤ 1 γ ∈ ℓ −
−
k 1
η − Q˜ (s,a) η (k k ) 1 η 1(( ⌊m ⌋+1) −1) . (76)
1 t 1 ℓ
≤ − 1 γ ≤ 1 γ
t X=kℓ − −
By choosing η
1
= (1
−
γ) q2ln K( |A|),m =
2(1
−γ)l ǫn l( n1 (cid:16)+ ǫc ()
1−4
γ)2
(cid:17), and K = (12 −ln γ( )A 4ǫ) 2, we have
η1(( ⌊m ⌋+1) −1) η1m = ln(1+c). By the time LSE-subroutine is executed for any k within
1 γ ≤ 1 γ 2H
[k +1−,...,k 1−],themostrecenttargetpolicyπ andbehaviourpolicyπ willsatisfyeq.(12)
foℓ rallstatesinℓ+ t1
he− coverof .
k kℓ
(cid:4)
ℓ
C
Lemma10 [q-barunchanged]Foranyl [L],onceanentry(s,a)in ispopulatedwithtrajec-
l
∈ C
toriesandstoredin D [(s,a)] anda q¯ (s,a)fora k [k , ,k 1]is computedin theLSE
l k l l+1
∈ ··· −
subroutine in LSE-subroutine for the first time, the q¯ will maintain unchanged as that value for
k
remainderofthealgorithm’sexecution.
Proof:Recallthei-thtrajectoryτ si ,astoredinD l[(s,a)]formsthereturnG(τ si ,a)= H h=− 01γhR hi +1.
Byextractingoutallofthestate-actionpairs {(S hi,Ai h) }H h=−11fromτ si ,a,wehavethei Pmportancesam-
plingratiofortrajectoryi: ρ(τ si ,a) = ΠH h=−11 ππ kk l( (A Ai h i h| |S Shi hi) ). Then,q¯ k(s,a) = n1 n i=1ρ(τ si ,a)G(τ si ,a).
Weseethatthevalueofq¯ (s,a)isinfluencedbytheimportancesamplingratioandthereturns.
k P
The returnsG(τi ) for all i = 1,...,n are based on data D [(s,a)]. Once D [(s,a)] is populated
s,a l l
withtrajectories,D [(s,a)]remainunchangedfortherestofthealgorithm’sexecution.
l
The only source that can change the value of q¯ throughoutthe execution of the algorithm is the
k
importancesampling ratio. Thus, let usconsiderthe runningphase with ℓ = l where q¯ (s,a) for
k
any k [k , ,k 1] is computed the first time in the LSE subroutine in LSE-subroutine.
l l+1
∈ ··· −
Firstofall,thebehaviorpolicyπ wouldalreadyhavebeenupdatedinapreviousloopthroughthe
kl
algorithmwithℓ = l 1. Forsubsequentiterations,startingwithk = k +1uptok = k 1,
l l+1
− −
the policyπ is updatedin iteration k 1. Thus, by the time LSE-subroutineis executedforany
k
−
kwithin[k +1,...,k 1],boththemostrecentpolicyπ andthebehaviourpolicyπ would
l l+1
−
k kl
havebeenupdatedbyline26.
Inline26,foranysthatarenewlycoveredby (i.e.,s Cov( ) Cov( )),thepolicyπ ( s)
l l l+1 k
C ∈ C \ C ·|
makes an update, which will remain unchanged in any subsequent loops through the algorithm
whenℓ=lagain.Thisisbecauseofline30,thestatesthatisconsiderednewlycoveredinoneloop
throughthealgorithmwillbeaddedto bytheendoftheloop,makingthestatenolongerasa
l+1
C
newlycoveredstate. Insummary,π ( s)wouldhavebeenupdatedonceandremainunchangedas
k
·|
thatupdatefortheremainderoftheexecutionofthealgorithm.
By the time LSE-subroutineis executed for any k within [k +1,...,k 1], the most recent
l l+1
−
policy π and the behaviour policy π for all states in the cover of would have already been
k kl Cl
updatedandremainunchangedthroughouttheexecutionofthealgorithm. Sinceallofthestatesin
thetrajectoriesareinthecoverof ,theimportancesamplingratiowouldalsohavebeensetonce
l
C
andremainasthatvalueforrestofthealgorithm’srun. WhenLSE-subroutineisrun,q¯ (s,a)fora
k
k [k , ,k 1]willalsoremainunchangedthroughouttheexecutionofthealgorithm.
l l+1
∈ ··· −
(cid:4)
Lemma11 [q-bar accuracy] Whenever LSE-subroutine of Confident-NPG is executed, for a
δ (0,1], for all k [k , ,k 1], the corresponding q¯ (s,a) for all (s,a) has
′ ℓ ℓ+1 k ℓ
∈ ∈ ··· − ∈ C
withprobability1 δ ,
′
−
|q¯ k(s,a) −q
π
k′(s,a) |≤ǫ forallπ
k′
∈Π
πk,Cov(
Cℓ). (77)
Since ℓ takes on a specific value l [L] in each loop throughthe algorithm, if eq. (77) holds for
∈
allk [k , ,k 1],thentheconditioncontinuestoholdforthereminderofthealgorithm’s
l l+1
∈ ··· −
execution.
21Proof:Weaimtoapplylemma4toeachindividual(s,a) . Todothis,wemustconfirmthetwo
ℓ
∈C
necessaryconditionsofthelemmaforeach(s,a) ,demonstratingthat q¯ (s,a) q (s,a) for
∈Cℓ
|
k
−
πk
|
allk [k , ,k 1]isindeedboundedbyǫforall(s,a) .
ℓ ℓ+1 ℓ
∈ ··· − ∈C
WenotethatbythetimeLSEsubroutineisruninline20,allofthe(s,a) wouldhavetrajecto-
ℓ
∈C
riesgeneratedsuccessfullybyGather-dataandbestoredinD [(s,a)]. Foranytrajectorytobestored
ℓ
inD ,itwouldmeanthatallofthestatesinthetrajectoriesareinthecoverof ,andbylemma7,
ℓ ℓ
C
thesestateswillcontinuetobecoveredby throughouttheexecutionofthealgorithm.Bypicking
ℓ
C
any(s,a) ,weapplylemma9toallofthestatessinthetrajectoriesstoredinD [(s,a)],then
ℓ ℓ
∈ C
wehavethebehaviourpolicyπ ( s)andtargetpolicyπ ( s)fork [k , ,k 1]satisfy
kℓ
·|
k
·| ∈
ℓ+1
···
ℓ+1
−
eq.(12).
Second, by lemma 8, the importance weighted return q¯ (s,a) is unbiased estimate of any
k
E π k′,s,a H h=− 01R h+1 forall π k′
∈
Π πk,Cov( Cℓ). Altogether,bylemma4, we canensureeq.(77)
holds. h i
P
Since ℓ takes on a specific value l [L] in each loop throughthe algorithm, if eq. (77) holds for
∈
allk [k , ,k 1],thentheconditioncontinuestoholdforthereminderofthealgorithm’s
l l+1
∈ ··· −
execution. Forany(s,a) ,bythetimeq (s,a)iscalculatedforthefirsttimeandusedinLSE
l k
∈ C
subroutine, by lemma 10, the q (s,a) will remainas that same value throughoutthe executionof
k
the algorithm. So, if eq. (77) holds for q¯ (s,a), then the condition will continue to hold for the
k
remainderofthealgorithm’sexecution. (cid:4)
Lemma12(Weiszetal.(2022)) At any time during the execution of the main algorithm, for all
l [0,L],thesizeofeach isbounded:
l
∈ C
4dln 1+ 4 =. d˜=O˜(d), (78)
l
|C |≤ α
(cid:18) (cid:19)
wheretheαisthesmallesteigenvalueofV( ,α)andN istheradiusoftheEuclideanballcontain-
C
ingallthefeaturevectors.
Lemma13 WheneverLSE-subroutineofConfident-NPGisexecuted,forallk [k , ,k 1],
ℓ ℓ+1
∈ ··· −
foralls Cov(C )anda ,theleast-squareestimateQ˜r(s,a)satisfiesthefollowingcondition
∈ ℓ ∈A k
|Q˜r k(s,a) −q
π
k′(s,a) |≤ǫ
′
forallπ
k′
∈Π
πk,Cov(
Cℓ), (79)
whereǫ =ω+√αB+(ω+ǫ) d˜.
′
p
Proof: We prove the result by induction similar to Lemma F.1 of Weiszetal. (2022). We let
Cℓ−,π k−,Q˜ −k to denote the value of variable Cℓ,π k,Q˜ k at the time when lines 16 to 31 were most
recentlyexecutedwithℓ = l inapreviousloopthroughthealgorithm. Ifsuchtimedoesnotexist,
we let theirvaluesbe the initializationvalues. Only afterthe executionofline 30will change
Cℓ−
andaswellas ,andthisistheonlytimethat canbechanged. Therefore,atthestartofa
ℓ+1 ℓ+1
C C
newloop,weseethat
Cℓ+1
= Cℓ−. Thisalsoholdsattheinitializationofthealgorithm,weconclude
thatatthestartofeachloop,Cov( Cℓ+1)=Cov( Cℓ−).
At initialization, Q˜ = 0 for any k [0,K] and C = () for all l [L]. By applyinglemma 5
k l
∈ ∈
(Lemma4.3ofWeiszetal.(2022)),foranys,a ,
∈S×A
Q˜ k(s,a) q π′(s,a) ω+√αB ǫ ′, (80)
| − |≤ ≤
whichsatisfieseq.(79)forallk.
Next, let us consider the start of a loop after ℓ is set and assume that the inductive hypothesis
holds for the previous time line 20 to line 25 were executed with the same value of ℓ. For any
s ∈Cov( Cℓ−1),policyπ kℓ( ·|s)wouldhavealreadybeensetinapreviousloopwithvalueℓ −1and
remainsunc−hangedinthecurrentloop.Bylemma5andlemma11,wehaveforanys ∈Cov( Cℓ−1),
−
|Q˜ −kℓ(s, ·) −q π k′ ℓ(s, ·) |≤ω+√αB+(ω+ǫ) d˜ forπ k ℓ′ ∈Π π k− ℓ,Cov( Cℓ− −1), (81)
p
22where kφ(s, ·) kV( Cℓ− −1,α)−1 ≤ 1becauses ∈ Cov( Cℓ− −1)and |C ℓ− −1| ≤ d˜bylemma12. Recallby
sde ∈fin Cit oio vn
(
C,Q ℓ˜ )k ,ℓ |Q˜= kℓQ (˜ s−k ,ℓ ·, )π −kℓ
q
π=
k′
ℓπ (k s− ℓ ,, ·)C |ℓ ≤=
ǫ
′C foℓ− − r1 π, ka ℓ′n ∈d ΠC πo kv ℓ( ,C Cℓ o) v(= Cℓ)C
.
ov( Cℓ− −1). Itfollowsthatforany
For any s that is already coveredby
Cℓ
(i.e., s
∈
Cov( Cℓ−)), and for any off-policyiteration k
∈
[k
ℓ
+1,
···
,k
ℓ+1
−1], Q˜ k(s, ·) = Q˜ −k(s, ·). Additionally,thepolicyπ k( ·|s)wouldalreadyhave
beensetinapreviousloopwiththesamevalueofℓandremainsunchangedinthecurrentloop.For
s ∈Cov( Cℓ−),bylemma5andlemma11,
|Q˜ −k(s, ·) −q π k′(s, ·) |≤ω+√αB+(ω+ǫ) d˜ forπ k′ ∈Π π k−,Cov( Cℓ−), (82)
p
w
|Π
Q˜h
π
ke
k
(r
,C
se ,ok
·v
)φ
( C
−( ℓs ), q·
⊆
π) k′k (V
Π
s(
,πC
·k−ℓ−
),
|, Cα ≤o)−
v
ǫ(1
C ′ℓ−
f≤
o)
r.1
aB
nb
y
yec
d
πa
e
k′u fis ∈ne
it
Πs
io
π∈
n k,
,Cc Q˜o okv
v(
(( CsC
,
ℓℓ− +·)) 1)=a .nd Q˜| −kC (ℓ− s|
,
·≤ )fpord s˜b ∈y Cle om vm
(
Ca ℓ+12 1). B =y Cle om v(m Ca ℓ−6 ),,
Finally,foranysthatisnewlycoveredby (i.e.,s Cov( )),andforallk [k , ,k 1],
ℓ ℓ+1 ℓ ℓ+1
C 6∈ C ∈ ··· −
Q˜ (s, )=Q (s, ). Bylemma5andlemma11,wehave
k k
· ·
|Q k(s, ·) −q π k′(s, ·) |≤ω+√αB+(ω+ǫ) d˜ forπ k′ ∈Π πk,Cov( Cℓ), (83)
where kφ(s, ·) kV( Cℓ,α)−1 ≤1and |Cℓ |≤d˜bylemma12. p (cid:4)
Lemma14 Foranyδ (0,1],atargetaccuracyǫ > 0,misspecificationerrorǫ,andinitialstate
′
∈
s ,withprobabilityatleast1 δ ,thevaluedifferencebetweenanyπ Π andthemixture
0 ′ rand
∈S − ∈
policyπ¯ returnedbyConfident-NPGhasthefollowingvalue-differenceerror:
K
K 1
v π(s 0) −v π¯K(s 0) ≤ 14ǫ ′ γ + K(11 γ) − E s′ ∼dπ(s0),s′ ∈Cov( C0) hQ˜ k(s′, ·),π( ·|s′) −π k( ·|s′) i .
− − Xk=0 h i
(84)
Proof:By line 27 of algorithm2, one can use inductionto show that by the time Confident-NPG
terminates, all the for l [L+1] will be equal. Therefore,the coverof forall l [L+1]
l l
C ∈ C ∈
are also equal. Thus, it is sufficientto only consider at the end of the algorithm. By line 3 of
0
C
algorithm2,s Cov( ).
0 0
∈ C
Foranyl [L],definepolicyfork [k , ,k 1]asfollows,
l l+1
∈ ∈ ··· −
π ( s) ifs Cov( )
π+( s)= k ·| ∈ Cl (85)
k ·| π( s) otherwise.
(cid:26) ·|
Foranyl [L],andforanys Cov( ),k [k, ,k 1],
l l l+1
∈ ∈ C ∈ ··· −
v (s) v (s)=v (s) v (s)+v (s) v (s) (86)
π − πk π − π k+ π k+ − πk
1
= 1 γE s′ ∼dπ(s) hq π k+(s ′, ·),π( ·|s ′) −π k+( ·|s ′) i byperformancedifferencelemma (87)
− h i
I
+ q (s, ),π+( s) q (s, ),π ( s) , (88)
h|π k+ · k ·| i−h{πzk · k ·| i }
II
where|d π(s)isthediscou{nztedstateoccupancy}measureinducedbyfollowingπfroms.
To bound term II, we note that for any s Cov( ), we have π+( s) = π ( s) and both
∈ Cl k ·| k ·|
π ,π+( s) Π . By lemma 13, we have for any s Cov( ),a , Q˜ (s,a)
q
πk k′(sk ,a· )| |≤∈
ǫ
′foπ rk a, nC yov π( k′Cl ∈)
Π
πk,Cov(
Cl). Then,foranys
∈Cov(∈
Cl),a
∈C Al
,
∈ A | k −
q (s,a) q (s,a) q (s,a) Q˜ (s,a) + Q˜ (s,a) q (s,a) 2ǫ. (89)
| π k+ − πk |≤| π k+ − k | | k − πk |≤ ′
23Itfollowsthatforanys Cov( ),
l
∈ C
q (s, ),π+( s) q (s, ),π ( s) = π ( s),q (s, ) q (s, ) (90)
h π k+ · k ·| i−h πk · k ·| i h k ·| π k+ · − πk · i
π ( s), q (s, ) q (s, ) (91)
≤h k ·| | π k+ · − πk · |i
q (s, ) q (s, ) π ( s) (92)
≤k π k+ · − πk · k∞k k ·| k1
2ǫ. (93)
′
≤
ToboundtermI,wenotethatforanys Cov( ),π+( s)=π( s)andπ+ Π ,then
6∈ Cl k ·| ·| k ∈ πk,Cov( Cl)
1
1 γE s′ ∼dπ(s) hq π k+(s ′, ·),π( ·|s ′) −π k+( ·|s ′) i (94)
− h i
1
= 1 γE s′ ∼dπ(s),s′ ∈Cov( Cl) hq π k+(s ′, ·),π( ·|s ′) −π k+( ·|s ′) i (95)
− h i
1
+ 1 γE s′ ∼dπ(s),s′ 6∈Cov( Cl) hq π k+(s′, ·),π( ·|s′) −π k+( ·|s′) i (96)
− h i
1
= 1 γE s′ ∼dπ(s),s′ ∈Cov( Cl) hq π k+(s′, ·),π( ·|s′) −π k+( ·|s′) i (97)
− h i
1
= 1 γE s′ ∼dπ(s),s′ ∈Cov( Cl) hq π k+(s′, ·) −Q˜ k(s′, ·),π( ·|s′) −π k+( ·|s′) i (98)
− h i
1
+ 1 γE s′ ∼dπ(s),s′ ∈Cov( Cl) hQ˜ k(s′, ·),π( ·|s′) −π k+( ·|s′) i (99)
− h i
1
≤ 1 γE s′ ∼dπ(s),s′ ∈Cov( Cl) kq π k+(s′, ·) −Q˜ k(s′, ·) k∞kπ( ·|s′) −π k+( ·|s′) k1 byHolder’sinequality
− h i (100)
1
+ 1 γE s′ ∼dπ(s),s′ ∈Cov( Cl) hQ˜ k(s ′, ·),π( ·|s ′) −π k+( ·|s ′) i (101)
− h i
2ǫ
≤ 1
′
γ
bylemma13and kπ∗( ·|s′) −π k+( ·|s′)
k1
≤2 (102)
−
1
+ 1 γE s′ ∼dπ(s),s′ ∈Cov( Cl) hQ˜ k(s′, ·),π( ·|s′) −π k( ·|s′) i (103)
− h i
2ǫ 1
= 1 ′ γ + 1 γE s′ ∼dπ(s),s′ ∈Cov( Cl) hQ˜ k(s′, ·),π( ·|s′) −π k( ·|s′) i (104)
− − h i
Insummary,foranyl,k [k ,k 1],
l l+1
∈ −
4ǫ 1
v π(s) −v πk(s) ≤ 1 ′ γ + 1 γE s′ ∼d π(s),s′∈Cov(Cl) hQ˜ k(s ′, ·),π ∗( ·|s ′) −π k( ·|s ′) i . (105)
− − h i
Puttingeverythingtogether,thevaluedifferencecanbeboundedasfollows,
1
K −1
1
L kl+1−1
(v (s ) v (s ))= (v (s ) v (s )) (106)
K
π 0
−
πk 0
K
π 0
−
πk 0
k X=0 Xl=0 k X=kl
1
L kl+1−1
4ǫ
′ (107)
≤ K 1 γ
Xl=0 k X=kl −
1
L kl+1−1
+ K(1 γ) E s′ ∼dπ(s0),s′ ∈Cov( Cl) hQ˜ k(s ′, ·),π( ·|s ′) −π k( ·|s ′) i (108)
− Xl=0 k X=kl h i
K 1
≤ 14ǫ ′ γ + K(11 γ) − E s′ ∼dπ(s0),s′ ∈Cov( C0) hQ˜ k(s ′, ·),π( ·|s ′) −π k( ·|s ′) i . (109)
− − k X=0 h i
Goingfromeq.(108)toeq.(109)isbecausewhenthealgorithmterminates,allthe arethesame,
l
andhencetheCov(C )=Cov(C )=...=Cov(C ). C (cid:4)
0 1 L
24B Confident-NPG-CMDP
Weincludetheproofsoflemmasthatappearinpriorworksandsupportinglemmasthatarehelpful
provingthelemmasinthemainsections. Thelemmasthatappearinthemainsectionwillhavethe
samenumberinghere.
B.1 Theaccuracyofleast-squareestimates
Onceastate-actionpairisaddedtoacoreset,itremainsinthatcoresetforthedurationofthealgo-
rithm.Thismeansthatany forl [L+1]cangrowinsize. Whenacoreset isextendedduring
l l
C ∈ C
a running phase when ℓ = l, the least-square estimates will need be updated based on the newly
extended containing newly discovered informative features. However, the improved estimates
l
C
willonlybeusedtoupdatethepolicyofstatesthatarenewlycovered,whicharestatesthatarein
Cov( ) Cov( ). Therefore,webreakdownQ˜p toreflectthevaluebasedonwhichπ is
Cℓ \ Cℓ+1 k k+1
updated,thatisfors,a ,
∈S×A
Q˜p(s,a) ifs Cov( )
k ∈ Cℓ+1
Q˜p(s,a) Qp(s,a) ifs Cov( ) Cov( ) (110)
k ← k ∈ Cℓ \ Cℓ+1
initialvalue0 ifs Cov( ℓ),
6∈ C
whereQp(s,a)=trunc Qr(s,a)+λ trunc Qc(s,a). RespectivetoQ˜p,wehavethe
k [0, 1−1 γ] k k [0, 1−1 γ] k k
correspondingpolicyupdateasfollows,
π (as) ifs Cov( )
k+1 ℓ+1
| ∈ C
π (as) π ( s)exp η trunc Qp(s,a) ifs Cov( ) Cov( ) (111)
k+1 | ← ∝ k ·| 1 [0, 1−1 γ] k ∈ Cℓ \ Cℓ+1
π (as) (cid:16) (cid:17) ifs Cov( ),
k ℓ
| 6∈ C
Foralls Cov(),theQ˜ (s, )willbethevalueoftheleast-squareestimateatthetimethepolicy
ℓ k
∈ C ·
makesanNPGupdateoftheform:
π ( s) π ( s)exp η trunc Q (s,a) . (112)
k+1 ·| ∝ k ·| 1 [0, 1−1 γ] k
(cid:16) (cid:17)
This is because at the end of the loop after line 31 of algorithm 1 is run, the next phase core set
= ,whichmakeanystatesthatarecoveredby alsobecoveredby . Astatethatwas
ℓ+1 ℓ ℓ ℓ+1
C C C C
oncenewlycoveredby willnolongerbenewlycoveredagain. Ifthealgorithmwastomakean
ℓ
C
NPGupdateforstatesthatarenewlycoveredinsomeloop,inanysubsequentloopswiththesame
valueℓ,thepolicywillremainunchanged.
Lemma1 WheneverLSE-subroutineonline21ofConfident-NPG-CMDPisexecuted,forallk
[k , ,k 1],foralls Cov(C )anda
,theleast-squareestimateQ˜p(s,a)satisfiesth∈
e
ℓ ··· ℓ+1 − ∈ ℓ ∈A k
following,
Q˜p(s,a) qp (s,a) ǫ forallπ Π , (113)
| k − π k′,λk |≤ ′ k′ ∈ πk,Cov( Cℓ)
whereǫ =(1+U)(ω+√αB+(ω+ǫ) d˜)withd˜=O˜(d). Likewise,
′
Q˜c(s,a) qc (s,a) ω+√p αB+(ω+ǫ) d˜ forallπ Π , (114)
| k − π k′ |≤ k′ ∈ πk,Cov( Cℓ)
p
Proof: By the primal-dual approach, we have reduced the CMDP problem into an unconstrained
problemwithasinglerewardoftheformr =r+λc.Theproofofthislemmaisadirectapplication
λ
oflemma13inthesinglerewardsettingalongwithafewadjustments.
Theresultoflemma13dependsonlemma11(q-baraccuracy).Forlemma11tobetrue,oneofthe
requirementisthatthebehaviourpolicyπ andthetargetpolicyπ mustsatisfytheeq.(12). The
kℓ k
lemmathatverifiesthisconditionislemma9, andallofthemodificationswouldneedbemadein
lemma9fortheCMDPsetting.
The main modificationto lemma 9 for the CMDP setting is to recognizethat the value Q˜p for all
k
k [0,K]areintherangeof0and 1+U. Theupperboundvalueistheresultoftheprimaryreward
∈ 1 γ
−
25functiontakingvaluesintherangeof[0,1]andthedualvariabletakingvaluesintherangeof[0,U].
ThevalueU isdefinedinlemma15forrelaxed-feasibilityandinlemma17forstrict-feasibility,and
itisanupperboundontheoptimaldualvariable(i.e.,λ U). Fromthisvaluemodification,we
∗
≤
havethestepsizeη 1 = 11 +− Uγ 2ln K( |A|).
q
The next modification is the interval of data collection m. For the total number of iterations
K = 9(√2ln( (| 1A| γ)+ )41 ǫ) 22(1+U)2 and H = ln((90√ d˜(1+ 1U) γ)/((1 −γ)3ǫ)). Then, it follows that m =
− −
(1+U)ln(1+c) . With these changes, we apply lemma 9 to validate one of the
2ǫ(1 γ)ln((90√ d˜(1+U))/((1 γ)3ǫ))
cond−itionsforlemma11. −
Unlikethesingle-rewardsetting,theprimalestimateQ˜p(s,a)alsodependsonthedualvariableλ .
k k
By the time policy π is being updated with respect to Q˜p, the λ would have been set. For
k+1 k k
theon-policyiterationk = k , thedualvariableλ issetin apriorloop. Foroff-policyiterations
ℓ k
k [k +1, ,k 1],thedualvariableλ wouldhavebeensetinthek 1iteration.Thus,in
ℓ ℓ+1 k
∈ ··· − −
anyiterationk [k , ,k 1],themostrecentλ willbeavailablefortheconstructionofq¯ .
ℓ ℓ+1 k k
∈ ··· −
Due to the executionof line 7 and line 31, the initial state s would be guaranteed to be covered
0
byC . Ifs Cov( )forthe firsttime, thenλ makesamirrordescentupdateinline30using
ℓ 0 ℓ k
Vc(s )atthe∈ timeofC theupdateandremainsthatvalueforthereminderexecutionofthealgorithm.
k 0
Thisisbecauseattheendoftheloopafterline31isrun,thenextphasecoreset = ,making
ℓ+1 ℓ
C C
any states that are covered by also be covered by . By lemma 7, these states will remain
ℓ ℓ+1
C C
coveredby fortherestofthealgorithmexecution.Ifthealgorithmwastosetthedualvariable
ℓ+1
C
to the mirrordescentupdate, in any subsequentloopswith the same ℓ again, the dualvariable λ
k
remainsunchanged.Therefore,bylemma10,q¯ willalsoremainunchangedasthevaluewhenitis
k
computedforthefirsttimeinline 20.
With all the conditions of lemma 13 satisfied, we can apply the result to Q˜p. For Q˜c, the result
k
followsfromlemma13. (cid:4)
C Relaxed-feasilibility
Lemma15 [Lemma 4.1 of Jainetal. (2022)] Let λ be the optimal dual variable that satisfies
∗
min max vr(ρ)+λ(vc(ρ) b). Ifwechoose
λ ≥0 π π π −
2
U = , (115)
ζ(1 γ)
−
thenλ U.
∗
≤
. .
Proof:Letπ (ρ)=argmaxvc(ρ),andrecallthatζ =vc (ρ) b>0,then
c∗ π π c∗ −
vr (ρ)=maxminvr(ρ)+λ(vc(ρ) b). (116)
π∗ π λ 0 π π −
≥
(117)
ByPaternainetal.(2019),
vr (ρ)=minmaxvr(ρ)+λ(vc(ρ) b) (118)
π∗ λ 0 π π π −
≥
=maxvr(ρ)+λ (vc(ρ) b) (119)
π π ∗ π −
vr (ρ)+λ (vc (ρ) b) (120)
≥ π c∗ ∗ π c∗ −
vr (ρ)+λ ζ. (121)
≥ π c∗ ∗
Afterrearrangingterms,wehave
λ
v πr ∗(ρ) −v πr c∗(ρ) 1
. (122)
∗
≤ ζ ≤ ζ(1 γ)
−
BychoosingU = 2 ,wehaveλ U. (cid:4)
ζ(1 γ) ∗ ≤
−
26Definition3
K 1
Rp(π ∗,K)= − E s′ ∼dπ∗(s0),s′ ∈Cov( C0) hπ ∗( ·|s ′) −π k( ·|s ′),Q˜r k(s ′, ·)+λ kQ˜c k(s ′, ·)
i
, (123)
k X=0 h i
K 1
Rd(λ,K)= − (λ λ)(V˜c(s ) b). (124)
k − k 0 −
k=0
X
Lemma2 For any failure probability δ (0,1], target accuracy ǫ > 0, and initial state s
0
∈ ∈
, with probability 1 δ, Confident-NPG-CMDP returns a mixture policy π¯ that satisfies the
K
S −
following,
5ǫ ( 2ln(A)+1)(1+U)
vr (s ) vr (s ) ′ + , (125)
π∗ 0 − π¯K 0 ≤ 1 γ (1 γ)2√K
− p −
5ǫ ( 2ln(A)+1)(1+U)
b vc (s ) [b vc (s )] ′ + , (126)
− π¯K 0 ≤ − π¯K 0 + ≤ (1 −γ)(U −λ ∗) ( p1 −γ)2(U −λ ∗)√K
.
whereǫ =(1+U)(ω+(√αB+(ω+ǫ) d˜))withd˜=O˜(d).
′
p
Proof:
We applylemma14with π beingtheoptimalpolicyπ forCMDP, Q˜p = Q˜r +λ Q˜c instead of
∗ k k k k
Q˜ ,andlemma1insteadoflemma13ofthesinglerewardsetting,thenwehave
k
K 1
1 − vp (s ) vp (s ) (127)
K π∗,λk 0 − πk,λk 0
k=0
X
K 1
≤ 14ǫ ′ γ + K(11 γ) − E s′ ∼dπ∗(s0),s′ ∈Cov( C0) hQ˜r k(s ′, ·)+λ kQ˜c k(s ′, ·),π ∗( ·|s ′) −π k( ·|s ′) i
− − Xk=0 h i
(128)
4ǫ Rp(π ,K)
= ′ + ∗ . (129)
1 γ K(1 γ)
− −
By Proposition 28.6 of LattimoreandSzepesvári (2020), the primal regret Rp(π ,K)
∗
≤
1 1+U γ 2Kln( |A|)withη 1 = 11 +− Uγ 2ln K( |A|). Expandingthelefthandsideofeq.(129)intermsof
vr−,vc,
q
p
K 1 K 1
1 − 1 −
vr (s ) vr(s )+ λ (vc (s ) vc (s )) (130)
K π∗ 0 − π 0 K k π∗ 0 − πk 0
k=0 k=0
X X
4ǫ 1+U 2ln(A)
′ + . (131)
≤ 1 γ (1 γ)2 K
r
− −
Furthermore,
K 1 K 1
1 − 1 −
λ (vc (s ) vc (s )) λ (vc (s ) b) (132)
K k πk 0 − π∗ 0 ≤ K k πk 0 −
k=0 k=0
X X
K 1
= 1 − λ (vc (s ) V˜c(s ))+λ (V˜c(s ) b) (133)
K k πk 0 − k 0 k k 0 −
k=0
X
Rd(0,K)
ǫ + (134)
′
≤ K
U
ǫ + . (135)
′
≤ (1 γ)√K
−
27Note that λ (vc (s ) V˜c(s )) U(ω + √αB + ω d˜+ ǫ d˜) ǫ, with d˜defined in
k πk 0 − k 0 ≤ ≤ ′
lemma 12. The update to the dualvariable is a mirrordescentalgorithm. By Proposition28.6 of
p p
LattimoreandSzepesvári(2020),thedualregretRd(0,K)
≤
U 1√ γK withη
2
= U( √1 K−γ). Altogether,
−
K 1
1 − vr (s ) vr(s ) 4ǫ ′ + 1+U 2ln(A) +ǫ + U (136)
K π∗ 0 − π 0 ≤ 1 γ (1 γ)2 r K ′ (1 γ)√K
Xk=0 − − −
5ǫ ( 2ln(A)+1)(1+U)
′ + (137)
≤ 1 γ (1 γ)2√K
− p −
Forboundingtheconstraintviolations,wefirstincorporateRd(λ,K)intoeq.(131)andrearrange
termstoobtain:
K 1 K 1
1 − λ −
vr (s ) vr (s )+ (b vc (s )) (138)
K π∗ 0 − πk 0 K − πk 0
k=0 k=0
X X
K 1
1 − (λ λ)(vc (s ) b)+ 4ǫ ′ + (1+U) 2ln(A) (139)
≤ K k − πk 0 − 1 γ (1 γ)2√K
Xk=0 − − p
K 1 K 1
= 1 − (λ λ)(vc (s ) V˜c(s ))+ 1 − (λ λ)(V˜c(s ) b) (140)
K k − πk 0 − k 0 K k − k 0 −
k=0 k=0
X X
4ǫ (1+U) 2ln(A)
+ ′ + (141)
1 γ (1 γ)2√K
− − p
Rd(λ,K) 4ǫ (1+U) 2ln(A)
=ǫ + + ′ + (142)
′
K 1 γ (1 γ)2√K
− − p
5ǫ (1+U)( 2ln( )+1)
′ + |A| (143)
≤ 1 γ (1 γ)2√K
− −p
Therearetwoconstraintcases. Caseoneisb vc(s ) 0(noviolation),forwhichcase, λ = 0.
− π 0 ≤
Case two is b vc(s ) > 0 (violation),for which case, λ = U. With these choices, Rd(λ,L) is
− π 0
increasinginλ. Usingnotation[x] =max x,0 ,wehave
+
{ }
K 1 K
1 − U
vr (s ) vr (s )+ b vc(s ) (144)
K π∗ 0 − πk 0 K " − π 0 #
Xk=0 k X=0 +
5ǫ (1+U)( 2ln( )+1)
′ + |A| . (145)
≤ 1 γ (1 γ)2√K
− p −
ByLemmaB.2ofJainetal.(2022),wehave
5ǫ ( 2ln(A)+1)(1+U)
[b vc (s )] ′ + . (146)
− π¯K 0 + ≤ (1 −γ)(U −λ ∗) ( p1 −γ)2(U −λ ∗)√K
(cid:4)
Theorem1 With probability 1 −δ, the mixture policy π¯ K = k1 K k=− 01π k returned by confident-
NPG-CMDPensuresthat
P
5(1+U)(1+ d˜)
vr (s ) vr (s )= ω+ǫ, (147)
π∗ 0 − π¯K 0 1 γ p
−
5(1+U)(1+ d˜)
vc (s ) b ω+ǫ . (148)
π¯K 0 ≥ − (1 γ) p !
−
28ifwechoosen = 1013(1 ǫ2+ (c 1)2 γ(1 )+ 4U)2d˜ ln 4d˜(L δ+1) ,α = 225(1 (1− +γ U)2 )ǫ 22 B2,K = 9(√2ln( (| 1A| γ)+ )41 ǫ) 22(1+U)2 ,
− −
η 1 = 11 +− Uγ 2ln K( |A|), η 2 = (cid:16)U( √1 − Kγ),(cid:17) H = ln((90√ d˜(1+ 1U) γ)/((1 −γ)3ǫ)), m =
(1+U)2qln(1+c) ,andU = 2 . −
2ǫ(1 γ)ln((90√ d˜(1+U))/((1 γ)3ǫ)) ζ(1 −γ)
− −
Furthermore, the algorithm utilizes at most O˜(d2(1 + U)3ǫ 3(1 γ) 8) queries in the
− −
−
local-accesssetting.
Proof:Fromlemma2,wehave
5ǫ ( 2ln(A)+1)(1+U)
vr (s ) vr (s ) ′ + , (149)
π∗ 0 − π¯K 0 ≤ (1 γ) (1 γ)2√K
− p −
5ǫ ( 2ln(A)+1)(1+U)
b vc (s ) ′ + , (150)
− π¯K 0 ≤ (1 −γ)(U −λ ∗) ( p1 −γ)2(U −λ ∗)√K
.
LetC = 1 foraζ (0, 1 ]. Bylemma15,wechoseU = 2C andλ C. Itfollowsthat
ζ(1 γ) ∈ 1 γ ∗ ≤
1 1 =− ζ(1 γ) 1,an− dthustherighthandsideofeq.(150)isupperboundedbytheright
U λ∗ ≤ C − ≤
ha− nd side of eq. (149). Recall ǫ =. (1+U) ω+ √αB+(ω+ǫ) d˜ . Then, the goal is to
′
settheparametersH,n,K,andαappropriatel(cid:16)ysoth(cid:16)attheA,B andCpof(cid:17)th(cid:17)efollowingexpression,
whenaddedtogether,islessthanǫ:
5(1+U)(1+ d˜)ω 5(1+U)√αB 5(1+U)ǫ d˜ ( 2ln(A)+1))(1+U)
+ + + . (151)
1 −γ p 1 −γ 1 −γp p (1 −γ)2√K
A B C
| {z } | {z } | {z }
First,wesetnappropriatelysothatthefailureprobabilityiswellcontrolled.Thefailureprobability
depends on the number of times Gather-data subroutine (algorithm 3) is executed. Gather-data
is run in phase [0,L]. Each phase has at most d˜elements, and recall d˜is defined in lemma 12.
Therefore, Gather-data would return success at most d˜times. Altogether, Gather-data can return
success at most d˜(L + 1) times, each with probability of at least 1 δ = 1 δ/(d˜(L + 1)).
′
− −
By a unionbound,Gather-datareturnssuccess inall occasionswithprobability1 δ. By setting
−
H = ln((90√ d˜(1+ 1U) γ)/((1 −γ)3ǫ)) andn= 1013(1 ǫ2+ (c 1)2 γ(1 )+ 4U)2d˜ ln 4d˜(L δ+1) ,wehaveforalll ∈[0,L],
− −
k ∈[k l,
···
,k l+1 −1],the |q¯ k(s,a) −q π k′(s,a)
| ≤
15(( 11 +− Uγ) )ǫ √(cid:16) d˜holdfo(cid:17)rallπ k′
∈
Π πk,Cov( Cl) with
probabilityatleast1 δ. Then,thisisusedintheaccuracyguaranteeoftheleast-squareestimate
−
(lemma1)andfinallyinthesuboptimalityboundoflemma2. Then,wecansetBofeq.(151)tobe
lessthan ǫ.
3
By setting K =
9(√2ln( |A|)+1)2(1+U)2
, we have C of eq. (151) be less than ǫ. Finally, we set
(1 γ)4ǫ2 3
α = 225(1 (1− +γ U)2 )ǫ 22 B2 and hav− e A of eq. (151) to be less than 3ǫ. Altogether, we have the reward
suboptimalitysatisfyingeq.(147)andconstraintsatisfyingeq.(148).
Forthequerycomplexity,wenotethatouralgorithmdoesnotquerythesimulatorineveryiteration,
but at fixed intervals, which we call phases. Each phase is m iterationsin length. There are total
of L = K/( m +1) K/m = O˜ (1+U)(1 γ) 3ǫ 1 phases. In each phases, Gather-
− −
⌊ ⌊ ⌋ ⌋ ≤ −
data subroutine(algorithm3) can be run. Each time Gather-datareturnssuccess with trajectories,
the subroutinewouldhave made at most(cid:0)nH queries. Gather-da(cid:1)tais run for each of the elements
in , l [0,L]. By the time the algorithm terminates, all ’s are the same. Since there are
l l
C ∈ C
at most O˜(d) elements in each , the algorithm will make a total of nH(L+1) number of
l 0
C |C |
queriestothesimulator. SincewehaveH = O˜((1 γ) 1),n =O˜((1+U)2dǫ 2(1 γ) 4)and
− − −
− −
L=O˜((1+U)ǫ 1(1 γ) 3),thesamplecomplexityisO˜(d2(1+U)3(1 γ) 8ǫ 3). (cid:4)
− − − −
− −
29D Strict-feasibility
Lemma16 Let π be defined as in eq. (10) and π be the optimal policy of CMDP. Then, for a
∗ ∗
>0, △
△
vr (s ) vr (s ) λ , (152)
π∗ 0 − π △∗ 0 ≤ ∗ △
whereλ istheoptimaldualvariablethatsatisfiesmin max vr(s )+λ(vc(s ) b).
∗ λ ≥0 π π 0 π 0 − ′
Proof:
v πr △∗ (s 0)=m πaxm λin 0v πr(s 0)+λ(v πc(s 0) −b′). (153)
≥
ByPaternainetal.(2019),
v πr △∗ (s 0)=m λin 0m πaxv πr(s 0)+λ(v πc(s 0) −b′) (154)
≥
=m πaxv πr(s 0)+λ∗(v πc(s 0) −b′) (155)
≥v πr ∗(s 0)+λ∗(v πc ∗(s 0) −(b+ △)) (156)
vr (s )+λ (b b ) becausevc (s ) b (157)
≥
π∗ 0 ∗
− −△
π∗ 0
≥
=vr (s ) λ . (158)
π∗ 0
−
∗
△
Afterrearrangingtheterms,wegettheresult. (cid:4)
Lemma17 Letλ betheoptimaldualvariablethatsatisfiesmin max Vr(s )+λ(Vc(s )
b). Ifwechoose
∗ λ ≥0 π π 0 π 0 −
′
4
U = , (159)
ζ(1 γ)
−
thenλ U requiringthat (0,ζ).
∗ ≤ △∈ 2
. .
Proof:Letπ (s )=argmaxVc(s ),andrecallthatζ =Vc (s ) b>0,then
c∗ 0 π 0 π c∗ 0 −
v πr △∗ (s 0)=m πaxm λin 0v πr(s 0)+λ(v πc(s 0) −b′) (160)
≥
ByPaternainetal.(2019),
v πr △∗ (s 0)=m λin 0m πaxv πr(s 0)+λ(v πc(s 0) −b′) (161)
≥
=maxvr(s )+λ (Vc(s ) b) (162)
π π 0 ∗ π 0 − ′
≥v πr c∗(s 0)+λ∗(v πc c∗(s 0) −(b+ △)) (163)
=v πr c∗(s 0)+λ∗(ζ −△). (164)
Ifwerequire (0,ζ),thenwehave
△∈ 2
ζ
v πr △∗ (s 0) ≥v πr c∗(s 0)+λ∗(ζ
−
2) (165)
λ ζ
=vr (s )+ ∗ (166)
π c∗ 0 2
Afterrearrangingtermsineq.(166),wehave
2(vr (s ) vr (s ))
λ∗ π △∗ 0 − π c∗ 0 2 . (167)
≤ ζ ≤ ζ(1 γ)
−
BychoosingU = 4 ,λ U. (cid:4)
ζ(1 γ) ∗ ≤
−
30Theorem2 With probability 1 δ, a target ǫ > 0, the mixture policy π¯ returned by confident-
K
NPG-CMDP ensures that vr (s− ) vr (s ) ǫ and vc (s ) b, if assuming the misspeci-
π∗ 0 − π¯K 0 ≤ π¯K 0 ≥
ficiation error ω
≤
40(1+△ U(1 )− (1γ +)
√
d˜), and if we choose
△
= ǫ(1 − 8γ)ζ,α = 160△ 02 (1(1 +− Uγ )) 22 B2,K =
64(√2ln( |A|)+1)2(1+U)2 ,n = 7200(1+c)2d˜(1+U)2 ln 4d˜(L+1) ,H = ln (cid:18)24 △0( (1 1+ −U γ) )√ 3d˜ (cid:19),m =
(1 γ)4 2 2(1 γ)4 δ 1 γ
− △ △ − −
4(1+U)ln(1+c) ,U = 4 . (cid:16) (cid:17)
(1 γ)ln
240(1+U)√d˜ ζ(1 −γ)
△ − (cid:18) △(1−γ)3 (cid:19)
Furthermore,thealgorithmutilizesatmostO˜(d2(1+U)3(1 γ) 11ǫ 3ζ 3)queriesinthelocal-
− − −
−
accesssetting.
Proof:Letλ betheoptimaldualvariablethatsatisfiestheLagrangianprimal-dualofthesurrogate
∗
CMDPdefinedbyeq.(10)(i.e.,λ =argmin max vr(s )+λ(vc(s ) b)).
∗ λ ≥0 π π 0 π 0 − ′
vr (s ) vr (s ) (168)
π∗ 0 − π¯K 0
= v πr ∗(s 0) −v πr △∗ (s 0) + v πr △∗ (s 0) −v πr ¯K(s 0) (169)
h i h i
surrogatesuboptimality Confident-NPG-CMDPsuboptimality
λ +ǫ¯, (170)
|∗ {z } | {z }
≤ △
whereǫ¯=
5(1+U)(1+√ d˜)ǫ+5(1+U)√αB+5(1+U)ǫ√ d˜ +(√2ln(A)+1)(1+U)
.Bylemma16,vr (s )
1 γ 1 γ 1 γ (1 γ)2√K π∗ 0 −
vr (s ) λ .− We can furthe− rupperbound− λ by U = − 4 using lemma 17 and requiring
π △∗ 0 ≤ ∗ △ ∗ ζ(1 −γ)
0,ζ . Togetherwiththeorem1,wehaveConfident-NPGreturnπ¯ s.t.
△∈ 2 K
(cid:16) (cid:17)
4
vr (s ) vr (s ) △ +ǫ¯ and (171)
π∗ 0 − π¯K 0 ≤ ζ(1 γ)
−
b Vc (s ) ǫ¯. (172)
′ − π¯K 0 ≤
Now,weneedtoset suchthat1) 0,ζ and2) ǫ¯ 0aresatisfied. Ifwechoose =
△ △∈ 2 △− ≥ △
ǫ(1 −γ)ζ,thenthefirstconditionissatisfied(cid:16). Thi(cid:17)sisbecauseǫ 0, 1 ,andthus ζ < ζ.
8 ∈ 1 γ △≤ 8 2
−
(cid:16) i
Next, we checkif ourchoiceof = ǫ(1 −γ)ζ satisfies ǫ¯ 0. Forthe condition ǫ¯ 0
△ 8 △− ≥ △− ≥
to be true, we make an assumption on the misspecification error ω △(1 −γ) , and pick
≤ 40(1+U)(1+√ d˜)
n,α,K,η ,η ,H,m to be the values outlined in this theorem. Consequently, we have ¯ǫ = 1 .
1 2 2△
Then,wehaveensuredthecondition ¯ǫ 0issatisfied.
△− ≥
We note that because ζ 0, 1 , we have ǫ¯ ǫ ǫ. following from eq. (171), we have
∈ 1 γ ≤ 16 ≤
−
v πr ∗(s 0) −v πr ¯K(s 0) ≤ ǫ and(cid:16)b ′ −V(cid:17) π¯c K(s 0) ≤ △ 2. Then it followsthatb+ △ 2 ≤ V π¯c K(s 0). Strict-
feasilbilityisachieved.
Forthequerycomplexity,wenotethatouralgorithmdoesnotquerythesimulatorineveryiteration,
but at fixed intervals, which we call phases. Each phase is m iterationsin length. There are total
of L = K/( m +1) K/m = O˜ (1+U)(1 γ) 3 1 phases. In each phase, Gather-
− −
⌊ ⌊ ⌋ ⌋ ≤ − △
datasubroutine(algorithm3)canberun.EachtimeGather-datasubroutinereturnswithtrajectories,
the subroutine would have made at most(cid:0)nH queries. Gather-da(cid:1)ta is run for each of the element
in , l [0,L]. By the time the algorithm terminates, all ’s are the same. Since there are
l l
C ∈ C
at most O˜(d) elements in each , the algorithm will make a total of nH(L+1) number of
l 0
C |C |
queriesto the simulator. Since we have H = O˜((1 γ) 1), n = O˜((1+U)2d(1 γ) 4 2),
− − −
− − △
L = O˜ (1+U)(1 −γ) −3 △−1 ,and
△
= ǫζ(1 8−γ),thesamplecomplexityisO˜(d2(1+U)3(1
−
γ) 11ǫ 3ζ 3). (cid:4)
− −(cid:0) − (cid:1)
31