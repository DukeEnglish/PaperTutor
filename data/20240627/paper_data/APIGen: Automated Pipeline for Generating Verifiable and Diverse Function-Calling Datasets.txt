APIGen: Automated PIpeline for Generating
Verifiable and Diverse Function-Calling Datasets
ZuxinLiu,ThaiHoang,JianguoZhang,MingZhu,TianLan,ShirleyKokane,JuntaoTan,
WeiranYao,ZhiweiLiu,YihaoFeng,RitheshMurthy,LiangweiYang,
SilvioSavarese,JuanCarlosNiebles,HuanWang,ShelbyHeinecke,CaimingXiong
SalesforceAIResearch,USA
{zuxin.liu,thai.hoang,jianguozhang}@salesforce.com
Abstract
Theadvancementoffunction-callingagentmodelsrequiresdiverse,reliable,and
high-qualitydatasets. ThispaperpresentsAPIGen,anautomateddatageneration
pipelinedesignedtosynthesizeverifiablehigh-qualitydatasetsforfunction-calling
applications. WeleverageAPIGenandcollect3,673executableAPIsacross21
differentcategoriestogeneratediversefunction-callingdatasetsinascalableand
structuredmanner. Eachdatainourdatasetisverifiedthroughthreehierarchical
stages: format checking, actual function executions, and semantic verification,
ensuringitsreliabilityandcorrectness. Wedemonstratethatmodelstrainedwith
ourcurateddatasets,evenwithonly7Bparameters,canachievestate-of-the-art
performanceontheBerkeleyFunction-CallingBenchmark,outperformingmul-
tipleGPT-4models. Moreover,our1Bmodelachievesexceptionalperformance,
surpassingGPT-3.5-TurboandClaude-3Haiku. Wereleaseadatasetcontaining
60,000high-qualityentries,aimingtoadvancethefieldoffunction-callingagent
domains. ThedatasetisavailableonHuggingface1andtheprojecthomepage2.
1 Introduction
Function-calling agents represent a significant advancement in artificial intelligence, specifically
withintherealmofLargeLanguageModels(LLMs). Thesemodels,suchasGPT4[1],Gemini[2],
andMistral[3],haveevolvedtonotonlyunderstandandgeneratehuman-liketextbutalsotoexecute
functionalAPIcallsbasedonnaturallanguageinstructions. Forinstance,considerauserrequesting
theweatherinPaloAlto,asillustratedinFig. 1. Thefunction-callingagentinterpretsthisquery,
accessestherelevantAPI—suchasget_weather("Palo Alto", "today")—andretrievesthe
weatherinformation, allinreal-time. ThiscapabilityextendstheutilityofLLMsbeyondsimple
conversationtaskstoincludedynamicinteractionswithavarietyofdigitalservicesandapplications,
rangingfromsocialmediaplatformstofinancialservices[4,5,6,7,8].
Despitetheirgrowingpopularityandpotential,thedeploymentoffunction-callingagentsisoften
hamperedbythequalityofthedatasetsusedfortraining. Currentdatasetsarelargelystaticandlack
comprehensiveverification,leadingtopotentialinaccuraciesandinefficienciesofmodelfine-tuning
inreal-worldapplications[9,10,11,12]. Thislimitationisparticularlyevidentwhenmodelstrained
onthesedatasetsencounternew,unseenAPIs. Forexample,amodeltrainedprimarilyonrestaurant
bookingAPIsmaystrugglewhensuddenlytaskedwithretrievingstockmarketdata,asitlacksthe
specifictrainingdataortheadaptabilitytohandlenewdomains.
1https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k
2https://apigen-pipeline.github.io/
Preprint.
4202
nuJ
62
]LC.sc[
1v81581.6042:viXraFigure1: WorkflowofanLLM-basedfunction-callingagent.
Toaddressthesechallenges,weintroduceAPIGen,anAutomatedPIpelineforGeneratingverifiable
and diverse function-calling datasets. Our framework is designed to facilitate the fine-tuning of
function-callingLLMsbyprovidinghigh-quality,diversedatasetsthatbetterreflectthevariability
andcomplexityofreal-worldAPIuse. Crucially,eachgenerateddatapointundergoesrigorousmulti-
stageverificationprocesses—format,execution,andsemantic—toensureaccuracyandapplicability.
We fine-tune function-calling models using the dataset generated by APIGen. The results show
thestrongperformanceofourmodels,surpassingmanyexistingpowerfulLLMswithmuchfewer
parameters,highlightingtheeffectivenessofAPIGenandthehighqualityofthedatasetitproduces.
WithAPIGen,wereleaseacomprehensivedatasetcontaining60,000entrieswith3,673APIsacross
21categories. Theyincludevariousquerystyles,suchasparallelfunctioncallingdata(askingthe
agenttoproducemultipleconcurrentfunctioncallsinasingleresponse)[9],whichisrarelyfoundin
publicdatasets,tothebestofourknowledge. Thislarge-scalesyntheticdatasetisintendedtocatalyze
furtherresearchanddevelopmentinthefieldoffunction-callingagents,offeringresearchersand
developersafoundationfortrainingandtestingtheirmodels. ThedataisavailableonHuggingface
andourprojecthomepage.
Thecontributionsofthisworkaresummarizedasfollows:
• WeintroduceAPIGen,afunction-callingdatagenerationpipelinethatfeaturesverifiability,scala-
bility,anddiversityofthedata.APIGeniscompatiblewitharangeofmodelsandAPIstoconstruct
high-qualitysyntheticfunction-callingdatasets.
• Wetraintwofunction-callingmodelsofdifferentsizes,1.3Band6.7B,usingAPIGen-constructed
trainingdata. Extensiveexperimentsdemonstratethatthe6.7Bmodelachievesarankof6thonthe
BerkeleyFunction-CallingLeaderboard[9],surpassingGPT-4oandGemini-1.5-Pro,whilethe
1.3BmodeloutperformsGPT-3.5-Turbo.
• Wealsoreleaseasyntheticfunction-callingdatasetcontaining60,000high-qualitydatagenerated
byAPIGenusingseveralstrongopen-sourceLLMs,whichcanpotentiallybenefittheresearch
communityindevelopingadvancedfunction-callingmodels.
2 RelatedWork
Tool-useAgent. RecentworkshavedevelopedframeworksandmodelsthatenableLLMstointeract
withAPIsandtools[13,14,15,16,17,18,19,20]. RestGPT[21]connectsLLMstoRESTfulAPIs
usingaPlanner,APIselector,andAPIexecutortohandlecomplexinstructions. Toolformer[22]is
anearlyworkthatenablesagentstousetoolslikeQuestionAnswering,Calculator,andWikipedia
Searchthroughasupervised-finetunedmodel. [23]designsthexLAMmodel,showingstrongtool
usagecapabilityacrossseveralbenchmarks. Octopus-v4[24]presentsamethodologytoincorporate
multiple specialized language models to solve corresponding tasks. While NexusRaven [5] and
GorillaOpenFunctions-v2[25]arestrongopen-sourcedmodelsthatfocusonfunctioncalling,neither
providesaccesstotheirtrainingdatasets.
AgentDatasets. Severaldatasetshavebeencreatedtosupportthedevelopmentofagentmodels.
AgentInstruct[18]consistsof6datasetsfordifferentagenttasks,includingAlfWorld[26],WebShop
[27], Mind2Web [28], Knowledge Graph, Operating System, and Database [29]. APIBank [12]
is a benchmark designed for tool-augmented LLMs, providing a training set containing tool-use
dialoguesfromvariousAPIs. Toolalpaca[30]constructsavariedandwell-structuredtool-usedataset
byrandomlyselectingAPIsandgeneratingdocumentationusingChatGPT.ToolBench[10]creates
2Figure2: Illustrationofthepost-processfilters.
an instruction-tuning dataset for tool use by collecting diverse REST APIs and generating their
descriptionsusingChatGPT.AgentOhana[23]andLumos[31]designaunifieddataandtraining
pipelineforefficientagentlearning,coveringmultipledifferentdatasetsandenvironments. However,
mostofthesedatasetswerenotrigorouslyverified,andusuallycontainnoisydata.
Benchmarks. Recent studies have established several benchmarks to assess agent abilities on
various tasks such as web interactions, reasoning, decision making, function calling, and tool
usage [27, 8, 29, 7, 32, 10, 11, 33, 34, 35]. Specifically, AgentBoard [34] includes 9 tasks, with
ToolOperation and ToolQuery designed to evaluate agent ability on multi-turn interaction with
externaltools. ToolEval[10]assessesfunctionalcallingcapabilitiesviaRapidAPI,containingaround
1,000testcasesandaskingGPT-3.5toassesstheWinRate. Furthermore,theBerkeleyFunction-
CallingLeaderboard(BFCL)[9]providesarobustandcomprehensiveframeworktoevaluatemodels’
abilitiestocallfunctions,with2,000testcasescoveringawiderangeofscenarios. WeuseBFCLas
ourtestinggroundasitprovidesthemostthoroughcomparisonamongpopularLLMs.
3 APIGenFramework
This section introduces the detailed design of APIGen, an Automated PIpeline for Generating
verifiableanddiversefunction-callingdatasets. Ourframeworkisdesignedwiththreekeyfactors
inmind: dataquality,datadiversity,andcollectionscalability. Weachievethesethroughthekey
modulesshowninFig.2: themulti-stagedataverificationprocessensuresdataquality,theseedQA
(query-answer)datasampler,APIsampler,andvariousprompttemplatesensurediversity,andour
structuredmodulardesignusingaunifiedformatenablesthesystemtoscaletodiverseAPIsources,
includingbutnotlimitedtoPythonfunctionsandrepresentationalstatetransfer(REST)APIs.
3.1 DataGenerationOverview
Figure2outlinesthedatagenerationprocessusingtheAPIGenframework,whichbeginsbysampling
oneormoreAPIsandexamplequery-answer(QA)pairs(seeddata)fromthelibrary,thenformatting
themintoastandardizedJSONformat(seeFig. 3forexamples). Aprompttemplateisselectedbased
onthedesireddatagenerationobjectives,whichsteerstheLLMingeneratingrelevantquery-answer
pairs. EachanswerinthegeneratedpairsisafunctioncallformattedinJSON.
TheadoptionofastandardizedJSONformatforAPIs,functioncalls,andgeneratoroutputs(asshown
inFigure3)providesseveraladvantages. Firstly,itestablishesastructuralwaytoverifywhether
3Figure3: JSONdataformatexamples.
thegenerator’soutputcontainsallnecessaryfields. Outputsthatfailtocomplywiththeseformat
requirementsarediscarded. Secondly, theJSONstructureenablesefficientcheckingoffunction
callsforcorrectparsingandvalidityofarguments. Callsthatincludeargumentsnotpresentinthe
APIlibraryorhallucinatenon-existentfunctionsareexcluded,enhancingtheoverallqualityofthe
dataset. Anotherkeybenefitisthescalabilityitenables. Withthisuniformformat,APIGencaneasily
incorporatedatafromdiversesources(Pythonfunctions, RESTAPIs, etc)bydevelopingformat
convertersthatadaptthemintothesebasicJSONelements,withoutmodifyingothercorecomponents,
suchasthepromptinglibrary,makingtheframeworkhighlyadaptableandextensible.
The generated function calls are subjected to a multi-stage verification process to ensure their
correctnessandrelevance. First,aformatcheckerverifiescorrectJSONformattingandparseability.
Next,theAPIexecutionengineprocessesthecallsandsendstheresultsandqueriestoasemantic
checker,anotherLLM,whichassessesalignmentbetweenthefunctioncalls,executionresults,and
queryobjectives. Datapointspassingallstagesareaddedbacktotheseeddatasetashigh-quality
examplestoenhancefuturegenerationdiversity. Wedetaileachcheckerinthenextsection.
3.2 Multi-StageDataVerification
Prioritizingqualityiscrucial, aspreviousresearchhasshownthatsmallamountsofhigh-quality
fine-tuningdatacansubstantiallyenhancemodelperformanceondomain-specifictasks[36]. This
motivatesourmulti-stagedatasetverificationprocesstoalignlargelanguagemodelseffectively.
Thekeyinsightdrivingourframeworkdesignisthat,unlikesyntheticchatdatawhichcanbedifficult
toevaluate,function-callinganswerscanbedirectlyexecutedviatheircorrespondingAPIs. This
enablescheckingiftheoutputAPIandparameters’formatsarecorrect,ifthegeneratedAPIcalls
areexecutable,andifexecutionresultsmatchthequery’sintent,etc. Basedonthisobservation,we
proposeathree-stageverificationprocess:
Stage 1: Format Checker: This stage performs sanity checks to filter out poorly formatted or
incompletedata. TheLLMoutputmuststrictlyfollowaJSONformatwiththe"query"and"answer"
fields,asshowninFig. 3. Weusuallyalsoincludeanadditional"thought"field,whichisknownas
thechain-of-thought(CoT)promptingtechnique[37],toincreasethepassrateofthegenerateddata.
Thedataisdiscardedifthesefieldscannotbeproperlyextractedforfunctioncalls. Additionally,the
functioncallsarecheckedforcorrectJSONparsingandvalidarguments. Generatedcallswhose
argumentsorfunctionsarenotpresentinthegivenAPIsareeliminatedtoreducehallucinationand
improvedataquality.
Stage 2: Execution Checker: Well-formatted function calls from Stage 1 are executed against
the appropriate backend (e.g. Python functions are directly imported and executed in a separate
subprocess,whileRESTAPIsarecalledtoobtainresultsandstatuscodes). Unsuccessfulexecutions
arefilteredout,andfine-grainederrormessagesareprovidedforfailures,includingargumenttype
errors,invalidparameters,runtimeerrors,timeout,syntaxerrors,missingarguments,etc.
Stage 3: Semantic Checker: Successful Stage 2 execution results, available functions, and the
generatedqueryareformattedandpassedtoanotherLLMtoassessiftheresultssemanticallyalign
withthequery’sobjective. Query-answerpairsthatexecutesuccessfullybutproducemeaningless
resultsduetoinfeasiblequeriesorincorrectargumentsarefilteredout. Themaindecisionfactors
4for this stage are: 1) whether the function call aligns with the query’s objective and has proper
arguments;2)whetherthefunctioncallandargumentsareappropriatelychosenfromtheavailable
functions;3)whetherthenumberoffunctioncallsmatchestheuser’sintent;4)whethertheexecution
resultscontainerrorsorindicateunsuccessfulfunctionexecution;5)whethertheexecutionresults
are relevant and match the query’s purpose. APIGen’s design offers the flexibility to select one
ormultipleLLMsascheckers,andthefilteringrulescanbereadilyadjusted—eithertightenedor
relaxed—dependingonspecificusecases.
Data points that pass all three verification stages are regarded as high-quality and added back to
improvefuturediversedatageneration. Thismulti-stageverificationprocessisthekeytoensuring
theAPIGenframeworkproducesadatasetthatisnotonlydiversebutalsoofverifiablehighquality,
enablingmoreeffectivefine-tuningofLLMstodomain-specificAPI-relatedtasks.
3.3 MethodstoImproveDatasetDiversity
Encouragingdiversityintrainingdatasetsiscrucialfordevelopingrobustfunction-callingagents
thatcanhandleawiderangeofreal-worldscenarios. InAPIGen,wepromotedatadiversitythrough
multipleperspectives,includingquerystylediversity,samplingdiversity,andAPIdiversity.
QueryStyleDiversity. APIGen’sdatasetisstructuredintofourmaincategories: simple,multiple,
parallel,andparallelmultiple,eachdesignedtochallengeandenhancethemodel’scapabilitiesin
differentusagescenarios. ThesecategoriesareinspiredbytheBerkeleyfunction-callingbenchmark
[9]andarecontrolledbycorrespondingpromptsandseeddata. Weshowexamplesoftheminthe
supplementarymaterial. Thecategoriesareasfollows:
• Simple: Thisquerystyleincludesstraightforwardscenarioswhereasinglefunctioncallismade
basedontheuser’sinputwithasingleprovidedJSONformatAPIdescription.
• Multiple: In this style, user queries could be answered by one of several function calls. The
challengeliesinselectingthemostappropriatefunctionfrommultipleprovidedAPIs. Itrepresents
oneofthemostcommonreal-worldusecases.
• Parallel: Thisquerystylerequiresexecutingmultiplefunctioncallssimultaneouslyinresponseto
asingleuserquery,whichmayconsistofoneormoresentencesbutwithonlyoneAPIprovided.
Forinstance, iftheuserwantstoknowtheweatherinbothPalo AltoandParis, themodel
shouldcalltheget_weatherfunctiontwicewithcorrespondingcitynamesinasingleresponse.
• ParallelMultiple: Thisquerystylecombinestheparallelandmultiplecategories,wheremultiple
functionandAPIdocumentsareprovided,andeachfunctioncallmightbeinvokedmultipletimes
basedonthequery’srequirements.
Whilethereexistpubliclyavailabletrainingdataforsimpleandmultiplecategories[38,10],however,
tothebestofourknowledge,weofferthefirstlarge-scaleandhigh-qualitydatasetsthatincludethe
parallel-relatedfunction-callingscenario.
SamplingDiversity. APIGenutilizesasamplingsystemdesignedtomaximizethediversityand
relevanceofthegenerateddatasets,whichincludethreemaincomponents,asshowninFig. 2:
• API Sampler: This module extracts one or more function descriptions from executable API
libraries,standardizingthemintoauniformJSONformat. ThediversesourcesofAPIsensurea
widerangeoffunctioncallsareavailableforinclusioninthetrainingdataset.
• ExampleSampler: Itsamplesaspecifiednumberofseedexamplescorrespondingtothedifferent
categories. Theseexamplesaretransformedintostructuredqueries,functiondescriptions,and
answers,servingasanimportantfew-shotreferencefordatageneration.
• Prompt Sampler: This sampler draws from a diverse prompt library to generate a variety of
query-answerpairs. Thepromptsforeachquerystylecontaindifferentcontexts,rangingfrom
simple,concisequery-answerpairstomorerealisticscenarios,suchasambiguousormisspelled
userrequests,enhancingthemodel’sabilitytohandlereal-worldinteractions.
Weprovidesomeprompttemplatesandseeddatainthesupplementarymaterial. InAPIGen,the
numberofexamplesandAPIssampledforeachdatasetiterationisrandomlychosenfromapredefined
range. Thisrandomizationenhancesdatasetvariabilitybypreventingrepetitivepatternsandensuring
abroadcoverageofscenarios. WenextintroduceourAPIdiversity.
54 DatasetPreparationandCollection
Webeginbydiscussingourdatasetpreparationprocess,whichincludesselectingandcleaningAPI
libraries. Thenwepresentourdatasetcollectionsetupandanoverviewoftheresultingdataset.
4.1 DatasetAPISources
Toensureahigh-qualityanddiversedataset,wefocusedoncollectingreal-worldAPIsthatcouldbe
readilyexecutedandcamewiththoroughdocumentation.WeprimarilysourcedAPIsfromToolBench
[10],acomprehensivetool-usedatasetthatincludes16,464RESTAPIsacross49coarse-grained
categoriesfromRapidAPIHub. Thishubisaleadingmarketplacefeaturingavastarrayofdeveloper-
contributedAPIs. TofurtherenhancetheusabilityandqualityoftheAPIs,weperformthefollowing
filteringandcleaningproceduresontheToolBenchdataset:
• DataQualityFiltering:WeremoveAPIswithincorrectlyparseddocumentationandthoselacking
required or optional parameters. APIs requiring no parameters were excluded to maintain the
challengelevelappropriateforourdatasetneeds.
• APIAccessibilityTesting: WetestedAPIaccessibilitybymakingrequeststoeachendpointusing
exampleparametersprovidedinthedatasetandthroughtheStableToolbenchserver[38]. APIs
thatcouldnotbeexecutedorreturnederrors,suchastimeoutsorinvalidendpoints,werediscarded.
• DocstringRegeneration:ToimprovethequalityofAPIdocumentation,weregenerateddocstrings
fortheAPIsthathavenoisyandunusabledescriptions.
After cleaning, we obtain 3,539 executable
REST APIs with good documentation. Addi-
tionally, we incorporated Python functions as
another API type, inspired by the executable
evaluationcategoriesoftheBerkeleyfunction-
callingbenchmark[9]. Wecollected134well-
documentedPythonfunctionscoveringdiverse
fields such as mathematics, finance, and data
management. Sample API examples are pro-
videdinthesupplementarymaterial.
The original ToolBench dataset contained se-
mantically overlapping categories such as
Finance and Financial. We consolidated
theseinto21distinctcategoriestoensureclarity
and balance across the dataset. Figure 4 illus-
trates the distribution of the 3,673 executable
APIsacrosstheseredefinedcategories,spanning
sectorsliketechnology,socialsciences,educa-
tion,andsports. ThisdiversecollectionofAPIs Figure4: Thecategorydistributionofthe3,673exe-
cutableAPIs.
providesastrongfoundationforsyntheticdata
generationandisavaluableassetforensuring
dataqualityandreliability.
4.2 CollectionSetupandDatasetDetails
To validate the effectiveness of the APIGen framework, we generated datasets targeting various
querystylesasoutlinedinSection3.3. WeutilizedseveralbaseLLMsfordatageneration,including
DeepSeek-V2-Chat(236B)[39],DeepSeek-Coder-33B-Inst[40],Mixtral-8x22B-Inst,andMixtral-
8x7B-Inst[3]. Foreachmodel,ourtargetwastogenerate40,000datapointsbysamplingdifferent
combinationsofAPIs,seeddata,andprompttemplates. Tofosterdiversityinthegeneratedresponses,
wesetthegenerationtemperatureto0.7acrossallmodels. Examplesoftheprompttemplatesand
APIsusedareprovidedinthesupplementarymaterialsforreference.
Table1presentsstatisticsforthedatagenerationprocesswithdifferentmodels,includingthetotal
verifieddatapointcountandthenumberoffiltereddatapointsateachverificationstage. Thefiltering
processsuccessfullyremovesmanylow-qualitydatapointsduetoformattingissues,executionerrors,
6orfailuretopassthesemanticcheck. Thefirsttwostages,formatcheckerandexecutionchecker,
typicallyfilteroutthemajorityoflow-qualitydata. Thesedatapointsoftenhaveinfeasibleargument
ranges,incorrecttypes,missingrequiredparameters,ormoresevereissuessuchashallucinationof
functioncallsorparameters. Oursystematicverificationprocessprovidesarigorouswaytoreduce
theoccurrenceofthesesituations.
Table1: FilteringstatisticsforthegenerateddatasetsusingdifferentbaseLLMs.
Model VerifiedData FailFormat FailExecution FailSemantic PassRate
DeepSeek-Coder-33B-Inst 13,769 4,311 15,496 6,424 34.42%
Mixtral-8x7B-Inst 15,385 3,311 12,341 7,963 38.46%
Mixtral-8x22B-Inst 26,384 1,680 5,073 6,863 65.96%
DeepSeek-V2-Chat(236B) 33,659 817 3,359 2,165 84.15%
Thesemanticcheckeralsoplaysacrucialroleinfilteringgenerateddatathatdoesnotalignwiththe
query’sobjectives. Forinstance,ifauser’squerycontainsmultiplerequests,butthereturnedresults
onlyaddressone,orifthegeneratedfunction-calldataandexecutionresultsdonotmatchtheuser’s
query,thedatapointwillbefilteredout. Includingthesedatapointsinthetrainingsetformodel
trainingcouldpotentiallyharmtheperformance,asdemonstratedintheexperiments.
WeobservethatstrongermodelslikeDeepSeek-V2-ChatandMixtral-8x22B-Insthavebetterformat-
followingcapabilitiesandhigherpassrates,whilethetworelativelysmallermodelshaveamuch
higherlikelihoodofproducingdatathatcannotbeexecuted. Thissuggeststhatwhenusingweaker
modelstogeneratedata,astrictverificationprocessisrecommendedtofilteroutlow-qualitydata.
Wearereleasingapproximately60,000high-qualityfunction-callingdatasetsgeneratedfromthetwo
strongestmodels: Mixtral-8x22B-InstandDeepSeek-V2-Chat(236B).Thesedatasetsincludeall
thequerystylesmentionedinSec. 3.3andcoverawiderangeofpracticalsituations,with3,673
diverseAPIsacross21categories. Eachdatapointhasbeenverifiedusingreal-worldAPIstoensure
itsvalidityandusefulness. Bymakingthisdatasetpubliclyavailable,weaimtobenefittheresearch
communityandfacilitatefutureworkinthisarea.
5 Experiments
5.1 ExperimentSetup
Toevaluatetheutilityandeffectivenessofthecollecteddataset,weconductedexperimentsbytraining
function-callingmodelswiththegenerateddata. Ouraimistoanswertwokeyquestions: 1)Towhat
extentcanthegenerateddataboostthemodel’sfunction-callingcapability,andhowdoesitcompare
toexistingmodels? 2)HoweffectiveistheAPIGenframeworkinfilteringoutlow-qualitydata?
Toaddressthesequestions,wetraintwoversionsofbasemodels: DeepSeek-Coder-1.3B-instruct
andDeepSeek-Coder-7B-instruct-v1.5[40]usingthexLAM(largeactionmodel)trainingpipeline
proposedin[23]. WerefertothesemodelsasxLAM-1B(FC)andxLAM-7B(FC),whereFCstands
fortheFunction-Callingmode,similartothismodeinotherexistingmodelsthatoutputJSON-format
function calls [1, 25, 41, 4]. We compare the performance of these small-sized models against
state-of-the-artmodels,includingdifferentversionsofGPT-4series[1],Claude-3series[42],Gemini
series[2],Llama3[43],Mixtral[3],OpenFunctions-v2[25],CommandR+[41],etc.
Benchmark. We evaluate the trained models’ performance on the Berkeley Function-Calling
Benchmark(BFCL)[9],whichprovidesacomprehensiveevaluationframeworkforassessingthe
function-callingcapabilitiesofLLMsacrossvariousprogramminglanguagesandapplicationdomains.
Designedtoreflectreal-worldusecases,theBFCLincludes2,000testingcases,coveringcomplex
scenariossuchasparallelandmultiple-functioncalls. ThebenchmarkcontainsdiverseAPIsources
likeJava,JavaScript,andPython,offeringadetailedanalysisofeachmodel’sabilitytocorrectly
interpretandexecutecommandsunderdifferentconditions. BFCLservesasahighlydetailedand
scalablebenchmarkforevaluatingLLMs’function-callingcapabilitiesandprovidesaleaderboardto
trackthemostrecentandpowerfulLLMs,bothcommercializedandopen-source.
EvaluationMetrics. TheBerkeleyFunction-CallingLeaderboard(BFCL)evaluatesLLMsusing
twomaincategories: AbstractSyntaxTree(AST)EvaluationandExecutableFunctionEvaluation.
7TheASTevaluationfocusesonthesyntacticaccuracyofthegeneratedfunctioncalls,ensuringthat
themodel’soutputmatchesapredefinedfunctiondocumentationinstructureandparameters. This
includeschecksforcorrectfunctionnames,requiredparameters,andappropriatedatatypes. The
ExecutableFunctionEvaluationgoesastepfurtherbyrunningthegeneratedfunctioncallstoverify
theiroperationalcorrectness. Thisexecutabletestensuresthatthefunctionsnotonlycompilebutalso
executecorrectly,providingtheexpectedresults,whichiscrucialforpracticalapplicationswhere
real-timeperformanceisessential.
5.2 ExperimentResultsAnalysis
Canthegenerateddataimprovethemodel’sfunction-callingcapabilityandhowdoesitcompare
toothermostpowerfulmodels? Theperformanceofourmodels,xLAM-7BandxLAM-1B,as
presented in Table 2, highlights the effectiveness of our APIGen framework and the quality of
thedatasetsproduced. Notably,ourxLAM-7Bmodelranks6thamongthemostpowerfulLLMs
listedontheBFCLleaderboard3,surpassingseveralversionsofGPT-4(GPT-4o,GPT4-Turbo-FC),
Llama3-70B,multipleClaude-3models,andaseriesofstrongmodelswhichareknownfortheir
exceptionalcapabilitiesinvarioustasks,includingfunction-calling. Thisachievementdemonstrates
thesignificantimpactofourhigh-qualitydatasetonthemodel’sfunction-callingperformance.
Table 2: PerformancecomparisonofdifferentmodelsonBFCLleaderboard(asofdate06/15/2024). The
rankisbasedontheoverallaccuracy,whichisaweightedaverageofdifferentevaluationcategories. “FC"
standsforfunction-callingmodeincontrasttousingacustomized“prompt"toextractthefunctioncalls.Seethe
benchmark[9]fordetails.
AbstractSyntaxTree(AST)Evaluation EvaluationbyExecutingAPIs
Overall Relevance
Rank Model
Accuracy Parallel Parallel Detection
Simple Multiple Parallel Simple Multiple Parallel
Multiple Multiple
1 88 GPT-4-0125-Preview(Prompt) 88.36 95 90.5 91 99.41 94 84 75 70.42
2 87.65 Claude-3-Opus-0229(Prompt) 86.73 94 86 89 97.65 92 80 75 80.42
3 86.35 Gemini-1.5-Pro-0514(FC) 80.18 92 91 88.5 91.76 88 76 77.5 89.58
4 85.88 Gemini-1.5-Pro-0409(FC) 80 92.5 90 88 90 90 74 77.5 88.75
5 85.88 GPT-4-1106-Preview(FC) 84 91.5 92 87 89.41 92 78 67.5 80.42
6 85.65 xLAM-7B(FC) 80.55 96 90 87.5 90.59 90 86 77.5 80.42
7 85.59 GPT-4-turbo-20240409(Prompt) 86.55 95 90 88.5 97.65 94 80 72.5 62.5
8 84.71 Gorilla-OpenFunctions-v2(FC) 88 95 87.5 87 94.71 94 70 67.5 61.25
9 84.65 GPT-4-0125-Preview(FC) 80.18 93 90.5 85 83.53 92 86 77.5 82.92
10 83.88 Llama-3-70B-Instruct(Prompt) 81.45 93 91.5 85 91.76 88 84 77.5 69.17
11 82.94 GPT-4o-2024-05-13(FC) 78.91 90 87.5 84.5 86.47 78 82 75 81.25
12 82.88 GPT-4-turbo-2024-04-09(FC) 74.73 90 89.5 88 82.94 88 76 67.5 88.75
13 81.82 Claude-3-Sonnet-20240229(Prompt) 83.09 89 88 89.5 93.53 92 84 77.5 51.25
14 81.35 Mistral-Medium-2312(Prompt) 80.55 92 84 78.5 65.88 76 82 70 88.33
15 80.53 GPT-4o-2024-05-13(Prompt) 85.09 84 78.5 61 90 78 70 72.5 82.5
16 80.47 Functionary-Medium-v2.4(FC) 79.45 90.5 87.5 85 68.82 84 80 70 74.17
17 80.35 Gemini-1.5-Flash-Preview-0514(FC) 80.91 93.5 78 73.5 81.76 90 54 72.5 79.58
18 80.35 Command-R-Plus(Prompt)(Optimized) 82.91 88.5 81 82 92.94 90 84 80 54.17
19 80.29 Command-R-Plus(Prompt)(Original) 82.55 90 80 83 92.94 88 84 80 53.75
20 79.94 Functionary-Small-v2.4(FC) 82.18 88.5 82 81.5 78.24 82 80 65 67.92
21 79.76 Command-R-Plus(FC)(Optimized) 79.09 91 88 82.5 81.18 86 74 67.5 63.75
22 77.47 Claude-3-Opus-0229(FC) 82.73 91.5 58 60 90.59 94 38 62.5 82.5
23 76.47 Claude-instant-1.2(Prompt) 79.82 85.5 83 67.5 84.71 80 82 65 57.5
24 74.41 xLAM-1B(FC) 75.09 80.5 76.5 63 79.41 80 78 62.5 72.08
25 74.29 Claude-3-Haiku-0207(Prompt) 84.91 91.5 84.5 55.5 92.94 94 70 25 34.58
26 71.41 Claude-2.1(Prompt) 80.18 76 55.5 52.5 71.18 84 46 47.5 83.33
27 70.94 Command-R-Plus(FC)(Original) 74.91 90 82 76.5 81.76 88 68 55 24.17
28 68.76 Mistral-large-2402(FCAuto) 66.91 94.5 25.5 72 83.53 96 8 52.5 84.17
29 67 Gemini-1.0-Pro-001(FC) 79.09 92.5 30 25.5 86.47 84 44 12.5 80
30 65.88 DBRX-Instruct(Prompt) 64 71.5 72 59 71.18 86 80 62.5 55.83
31 65.18 snowflake-instruct(Prompt) 62.36 69 59 54 87.65 86 74 72.5 59.58
32 64.35 Mistral-large-2402(FCAny) 81.45 93.5 31.5 79.5 94.71 92 8 65 0
33 63.88 GPT-3.5-Turbo-0125(FC) 61.45 66 90.5 81 93.53 80 82 70 2.08
......
43 43.71 Gemma-7b-it(Prompt) 42.18 48 30 44 30 32 40 25 70.83
44 40.76 Mistral-Small-2402(Prompt) 5.82 68 79 68.5 34.12 20 68 30 98.33
45 40.41 Deepseek-v1.5(Prompt) 39.27 49 37 28.5 37.06 38 36 12.5 57.08
46 23.71 Mistral-small-2402(FCAuto) 2 25.5 3 3 56.47 70 6 5 99.58
OursmallerxLAM-1Bmodelalsoshowsremarkableresults,securingthe24thpositionandoutper-
formingmanylargermodels,suchasClaude-3Haiku[42],Command-R-Plus[41],DBRX-Instruct
[44], Mistral-large [3], and GPT-3.5-Turbo-0125. The results highlight the effectiveness of the
APIGen pipeline in enhancing a model’s function-calling capabilities, even with a much smaller
size. BothxLAM-7BandxLAM-1Bdemonstratesubstantialimprovementsinhandlingcomplex
querytypes,particularlyintheparallelandmultiplefunction-callingscenarios,whicharetypically
underrepresentedinexistingpubliclyavailabledataset. Thisvalidatesthevalueofourpipelineand
datasetsinaddressingpracticalscenariosinvolvingcomplexAPIinteractionsandmultipleconcurrent
3https://gorilla.cs.berkeley.edu/leaderboard.html#leaderboard
8APIcalls,especiallyconsideringthatthebasemodel,DeepSeek-Coder-v1.5,onlyranks45thonthe
leaderboardandperformspoorlyinthesecategories.
Next,weanswerthequestion: howeffectiveis
theAPIGenframeworkinfilteringoutlow-
qualitydata? Weconductedanablationstudy
byaddingthedatasetsthatwerefilteredoutby
stage3(semanticchecker)andstage2(execu-
tionchecker)backtothetrainingset,simulating
situationswheregenerateddataisusedwithout
the rigorous verification process. The perfor-
mance comparison on the BFCL benchmark,
showninFig. 5,revealsthatusingthesefiltered
datasetsfortrainingharmsthefinalperformance,
with a more significant impact on the smaller
model. This indicates that directly using gen-
erateddatamightnotyieldthebestresultsand
demonstratestheeffectivenessofourAPIGen
Figure5: Performancecomparisonofusingdifferent
frameworkinfilteringoutlow-qualitydata.
stage’sdatasetsfromAPIGen. “+FailSemanticData"
Theseresultsprovidecompellingevidencefor and“+FailExecutionData"meaningaddingthefiltered
datasetfromstage3andstage2tothetrainingset.
theeffectivenessoftheAPIGenframeworkin
generating high-quality, diverse datasets for function-calling tasks. The impressive performance
achievedbyoursmall-sizedmodelshighlightstheefficiencyofourapproach,demonstratingthatby
focusingondataqualityanddiversity,wecaneffectivelyboosttheperformanceofsmallermodels,
makingthemcompetitivewithmuchlargeronesinthisfunction-callingagentdomain.
6 Conclusion
Inthispaper,weintroducedAPIGen,anovelframeworkthatgeneratesreliableanddiversefunction-
callingdatasetsbyleveragingamulti-stageverificationprocess. Ourexperimentsdemonstratethe
effectivenessofAPIGeninproducinghigh-qualitydatasetsfromawiderangeofexecutableAPIs.
This has significant implications for the development of more efficient and accessible language
models, as it shows that high-quality data can be as important as model size in achieving strong
performance. Byenablingsmallermodelstoachievecompetitiveresultsandsignificantlyenhancing
theirfunction-callingcapabilities,ourapproachandreleaseddatasetopenupnewpossibilitiesforthe
developmentofefficientandpowerfullanguagemodelsintheagenttool-usedomains.
However,thecurrentversionofAPIGenandthegenerateddatasethavesomelimitations. Presently,
theframeworkanddatasetonlyconsiderRESTAPIsandPythonfunctions. Additionally,although
APIGenisageneralframework,itcurrentlyonlyimplementsthegenerationprocedureforsingle-
turn function calling. Future work will focus on extending APIGen to support more scenarios,
programming languages, and APIs. We also plan to extend the framework to handle multi-turn
andmorecomplexinteractionsbetweenagentsandtools. Despitetheselimitations,webelievethat
APIGenandthegenerateddatasetrepresentasignificantstepforwardinthedevelopmentofefficient
andeffectivefunction-callingagents.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-
baptisteAlayrac, RaduSoricut, AngelikiLazaridou, OrhanFirat, JulianSchrittwieser, etal.
Gemini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXiv
preprintarXiv:2403.05530,2024.
[3] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,Chris
Bamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,
etal. Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
9[4] ShishirG.Patil,TianjunZhang,XinWang,andJosephE.Gonzalez. Gorilla: Largelanguage
modelconnectedwithmassiveapis. arXivpreprintarXiv:2305.15334,2023.
[5] VenkatKrishnaSrinivasan,ZhenDong,BanghuaZhu,BrianYu,DamonMosk-Aoyama,Kurt
Keutzer,JiantaoJiao,andJianZhang. Nexusraven: acommercially-permissivelanguagemodel
forfunctioncalling. InNeurIPS2023FoundationModelsforDecisionMakingWorkshop,2023.
[6] ZhiweiLiu,WeiranYao,JianguoZhang,LeXue,ShelbyHeinecke,RitheshMurthy,Yihao
Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al. Bolaa: Benchmarking and
orchestratingllm-augmentedautonomousagents. arXivpreprintarXiv:2308.05960,2023.
[7] WeizeChen,YushengSu,JingweiZuo,ChengYang,ChenfeiYuan,ChenQian,Chi-MinChan,
YujiaQin,YaxiLu,RuobingXie,etal. Agentverse: Facilitatingmulti-agentcollaborationand
exploringemergentbehaviorsinagents. ICLR,2023.
[8] ShuyanZhou,FrankFXu,HaoZhu,XuhuiZhou,RobertLo,AbishekSridhar,XianyiCheng,
YonatanBisk,DanielFried,UriAlon,etal. Webarena:Arealisticwebenvironmentforbuilding
autonomousagents. arXivpreprintarXiv:2307.13854,2023.
[9] FanjiaYan,HuanzhiMao,CharlieCheng-JieJi,TianjunZhang,ShishirG.Patil,IonStoica,and
JosephE.Gonzalez.Berkeleyfunctioncallingleaderboard.https://gorilla.cs.berkeley.
edu/blogs/8_berkeley_function_calling_leaderboard.html,2024.
[10] YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan,YaxiLu,YankaiLin,XinCong,
XiangruTang,BillQian,etal. Toolllm: Facilitatinglargelanguagemodelstomaster16000+
real-worldapis. ICLR,2024.
[11] XingyaoWang,ZihanWang,JiatengLiu,YangyiChen,LifanYuan,HaoPeng,andHengJi.
Mint:Evaluatingllmsinmulti-turninteractionwithtoolsandlanguagefeedback. arXivpreprint
arXiv:2309.10691,2023.
[12] MinghaoLi,YingxiuZhao,BowenYu,FeifanSong,HangyuLi,HaiyangYu,ZhoujunLi,Fei
Huang,andYongbinLi. Api-bank: Acomprehensivebenchmarkfortool-augmentedllms. In
The2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,2023.
[13] GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,
andAnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels.
arXivpreprintarXiv:2305.16291,2023.
[14] SiruiHong, XiawuZheng, JonathanChen, YuhengCheng, JinlinWang, CeyaoZhang, Zili
Wang,StevenKaShingYau,ZijuanLin,LiyangZhou,etal. Metagpt: Metaprogrammingfor
multi-agentcollaborativeframework. arXivpreprintarXiv:2308.00352,2023.
[15] BaianChen,ChangShu,EhsanShareghi,NigelCollier,KarthikNarasimhan,andShunyuYao.
Fireact: Towardlanguageagentfine-tuning. arXivpreprintarXiv:2310.05915,2023.
[16] MengdiXu,PeideHuang,WenhaoYu,ShiqiLiu,XilunZhang,YaruNiu,TingnanZhang,Fei
Xia,JieTan,andDingZhao. Creativerobottoolusewithlargelanguagemodels. arXivpreprint
arXiv:2310.13065,2023.
[17] YihengXu,HongjinSu,ChenXing,BoyuMi,QianLiu,WeijiaShi,BinyuanHui,FanZhou,
YitaoLiu,TianbaoXie,etal. Lemur: Harmonizingnaturallanguageandcodeforlanguage
agents. arXivpreprintarXiv:2310.06830,2023.
[18] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang.
Agenttuning: Enablinggeneralizedagentabilitiesforllms. arXivpreprintarXiv:2310.12823,
2023.
[19] Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham Sabach, and Rasool
Fakoor. Tail: Task-specificadaptersforimitationlearningwithlargepretrainedmodels. arXiv
preprintarXiv:2310.05905,2023.
[20] HongliangHe,WenlinYao,KaixinMa,WenhaoYu,YongDai,HongmingZhang,Zhenzhong
Lan, andDongYu. Webvoyager: Buildinganend-to-endwebagentwithlargemultimodal
models. arXivpreprintarXiv:2401.13919,2024.
10[21] YifanSong,WeiminXiong,DaweiZhu,WenhaoWu,HanQian,MingboSong,HailiangHuang,
ChengLi,KeWang,RongYao,YeTian,andSujianLi. Restgpt: Connectinglargelanguage
modelswithreal-worldrestfulapis,2023.
[22] TimoSchick,JaneDwivedi-Yu,RobertoDessì,RobertaRaileanu,MariaLomeli,EricHambro,
LukeZettlemoyer,NicolaCancedda,andThomasScialom. Toolformer: Languagemodelscan
teachthemselvestousetools. AdvancesinNeuralInformationProcessingSystems,36,2024.
[23] JianguoZhang,TianLan,RitheshMurthy,ZhiweiLiu,WeiranYao,JuntaoTan,ThaiHoang,
LiangweiYang,YihaoFeng,ZuxinLiu,etal. Agentohana: Designunifieddataandtraining
pipelineforeffectiveagentlearning. arXivpreprintarXiv:2402.15506,2024.
[24] Wei Chen and Zhiyuan Li. Octopus v4: Graph of language models. arXiv preprint
arXiv:2404.19296,2024.
[25] CharlieCheng-JieJi,HuanzhiMao,FanjiaYan,ShishirPatil,TianjunZhang,IonStoica,and
JosephGonzalez. Gorillaopenfunctionsv2. Inhttps://gorilla.cs.berkeley.edu/
/blogs/7_open_functions_v2.html,2024.
[26] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and
Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive
learning. arXivpreprintarXiv:2010.03768,2020.
[27] ShunyuYao,HowardChen,JohnYang,andKarthikNarasimhan. Webshop: Towardsscalable
real-worldwebinteractionwithgroundedlanguageagents. AdvancesinNeuralInformation
ProcessingSystems,35:20744–20757,2022.
[28] XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamStevens,BoshiWang,HuanSun,and
YuSu. Mind2web: Towardsageneralistagentfortheweb. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[29] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang
Ding,KaiwenMen,KejuanYang,etal. Agentbench: Evaluatingllmsasagents. arXivpreprint
arXiv:2308.03688,2023.
[30] QiaoyuTang,ZiliangDeng,HongyuLin,XianpeiHan,QiaoLiang,andLeSun. Toolalpaca:
Generalized tool learning for language models with 3000 simulated cases. arXiv preprint
arXiv:2306.05301,2023.
[31] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin
Choi,andBillYuchenLin. Lumos: LearningAgentswithUnifiedData,ModularDesign,and
Open-SourceLLMs. arXivpreprintarXiv:2311.05657,2023.
[32] YueHuang,JiawenShi,YuanLi,ChenruiFan,SiyuanWu,QihuiZhang,YixinLiu,PanZhou,
YaoWan,NeilZhenqiangGong,etal.Metatoolbenchmarkforlargelanguagemodels:Deciding
whethertousetoolsandwhichtouse. arXivpreprintarXiv:2310.03128,2023.
[33] Zuxin Liu, Zijian Guo, Haohong Lin, Yihang Yao, Jiacheng Zhu, Zhepeng Cen, Hanjiang
Hu, Wenhao Yu, Tingnan Zhang, Jie Tan, et al. Datasets and benchmarks for offline safe
reinforcementlearning. arXivpreprintarXiv:2306.09303,2023.
[34] ChangMa,JunleiZhang,ZhihaoZhu,ChengYang,YujiuYang,YaohuiJin,ZhenzhongLan,
LingpengKong,andJunxianHe. Agentboard: Ananalyticalevaluationboardofmulti-turnllm
agents. arXivpreprintarXiv:2401.13178,2024.
[35] ZhiweiLiu,WeiranYao,JianguoZhang,LiangweiYang,ZuxinLiu,JuntaoTan,PrafullaK
Choubey,TianLan,JasonWu,HuanWang,etal. Agentlite: Alightweightlibraryforbuilding
andadvancingtask-orientedllmagentsystem. arXivpreprintarXiv:2402.15538,2024.
[36] ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,JiaoSun,YuningMao,XuezheMa,
Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural
InformationProcessingSystems,36,2024.
11[37] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou, etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
Advancesinneuralinformationprocessingsystems,35:24824–24837,2022.
[38] Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu,
MaosongSun,andYangLiu. Stabletoolbench: Towardsstablelarge-scalebenchmarkingon
toollearningoflargelanguagemodels,2024.
[39] DeepSeek-AI. Deepseek-v2: Astrong,economical,andefficientmixture-of-expertslanguage
model,2024.
[40] DayaGuo,QihaoZhu,DejianYang,ZhendaXie,KaiDong,WentaoZhang,GuantingChen,
Xiao Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets
programming–theriseofcodeintelligence. arXivpreprintarXiv:2401.14196,2024.
[41] Cohere. Command r plus: Enhanced retrieval-augmented generation with microsoft
azure. https://cohere.com/blog/command-r-plus-microsoft-azure, 2024. Ac-
cessed: 2024-04-04.
[42] AIAnthropic. Theclaude3modelfamily: Opus,sonnet,haiku. Claude-3ModelCard,2024.
[43] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[44] Databricks. Introducingdbrx: Anewstate-of-the-artopenllm. https://www.databricks.
com/blog/introducing-dbrx-new-state-art-open-llm,2024. Accessed: 2024-03-27.
12A DatasetDocumentationandAccessibility
A.1 DatasetDocumentationandIntendedUses
ThedatasetgeneratedusingtheAPIGenframeworkisintendedfortrainingandevaluatingfunction-
callingagents. Thedatasetconsistsofdiversequery-answerpairs,wheretheanswersareverified
functioncallsthatcouldaddresstherequestedquerywithprovidedAPIs. TheAPIsandfunction
callsareinastandardizedJSONformat,asdemonstratedinthemainpaperFig. 3. Moredetailsof
theformatandexamplesareavailableinAppendixA.2. ThedatasetcoversawiderangeofAPI
categoriesandincludesvariousquerystyles,suchassimple,multiple,parallel,andparallelmultiple
functioncalls,asintroducedin[9].
Hosting,Licensing,andMaintenancePlan. Thedatasetcurrentlycanbeviewedanddownloaded
from our project homepage 4 or via Huggingface 5. All datasets are licensed under the Creative
Commons Attribution 4.0 License (CC BY). We also plan to open-source the trained models on
Huggingface once after the company’s legal approval. As for maintenance, we have established
along-termplantokeepthedatasetsup-to-date,correctanypotentialissues,andprovidesupport
to users. We also aim to expand these datasets further based on new advances in the field, thus
continuallypromotingprogressinthefieldoffunction-callingagenttraining.
AuthorResponsibilityStatement. Astheauthors,weherebyaffirmthatwebearfullresponsibility
forthedatasetsprovidedinthissubmission. Weconfirmthattothebestofourknowledge,norights
areviolatedinthecollection,distribution,anduseofthesedatasets.
A.2 JSONDataFormatandExamples
ThisJSONdataformatisusedtorepresentaqueryalongwiththeavailabletoolsandthecorresponding
answers. Here’sadescriptionoftheformat:
A.2.1 DatasetStructure
The JSON data structure comprises three main keys: query, a string representing the problem
statement; tools, anarrayoftoolseachdefinedbypropertiessuchasname, description, and
parametersthatfurtherdescribeeachtool’srequiredandoptionalparameterswiththeirtypesand
descriptions;andanswers,anarraydetailingresponseswiththetoolused(name)andthearguments
provided(arguments)foreachanswer,therebyaligningtoolswiththeirrespectivequeryintentions.
Thedetaileddescriptionofeachdatapoint’sentriesisasfollows.
• query(string): Thequeryorproblemstatement.
• tools(array): Anarrayofavailabletoolsthatcanbeusedtosolvethequery.
Eachtoolisrepresentedasanobjectwiththefollowingproperties:
– name(string): Thenameofthetool.
– description(string): Abriefdescriptionofwhatthetooldoes.
– parameters(object): Anobjectrepresentingtheparametersrequiredbythetool.
* Eachparameterisrepresentedasakey-valuepair,wherethekeyistheparametername
andthevalueisanobjectwiththefollowingproperties:
· type(string): Thedatatypeoftheparameter(e.g.,"integer","float","array").
· description(string): Abriefdescriptionoftheparameter.
· required(boolean): Indicateswhethertheparameterisrequiredoroptional.
• answers(array): Anarrayofanswerscorrespondingtothequery.
– Eachanswerisrepresentedasanobjectwiththefollowingproperties:
* name(string): Thenameofthetoolusedtogeneratetheanswer.
* arguments(object): Anobjectrepresentingtheargumentspassedtothetooltogenerate
theanswer.
· Eachargumentisrepresentedasakey-valuepair,wherethekeyistheparametername
andthevalueisthecorrespondingvalue.
4https://apigen-pipeline.github.io/
5https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k
13A.2.2 ExampleData
Here’sanexampleJSONdataforthesimplestscenario.
{
"query": "What is the weather in Palo Alto?",
"tools": [
{
"name": "weather_api.get_current_weather",
"description": "Retrieves the current weather conditions
for a specified location.",
"parameters": {
"location": {
"type": "string",
"description": "The name of the city or geographic
location.",
"required": true
},
"units": {
"type": "string",
"description": "The units for temperature measurement
(e.g., ’Celsius’, ’Fahrenheit’).",
"required": false
}
}
}
],
"answers": [
{
"name": "weather_api.get_current_weather",
"arguments": {
"location": "Palo Alto",
"units": "Celsius"
}
}
]
}
Inthisexample,thequeryasksaboutthecurrentweatherinPaloAlto. Thetoolsarraycontainsa
singleentryforweather_api.get_current_weather,describingthetoolusedtoretrieveweather
data,includingparametersforlocationandunits. TheanswersarrayliststhespecificAPIcallmade
withthelocationsetas"Palo Alto"andunitsas"Celsius".
Here’sanexampleJSONdatafortheparallelfunction-callingcategory,i.e.,theuser’squerycontains
multipleintentionsandtheanswerscontainmultipleparalleltoolcalls:
{
"query": "Find the sum of all the multiples of 3 and 5
between 1 and 1000. Also find the product of the first five
prime numbers.",
"tools": [
{
"name": "math_toolkit.sum_of_multiples",
"description": "Find the sum of all multiples of
specified numbers within a specified range.",
"parameters": {
"lower_limit": {
"type": "integer",
"description": "The start of the range (inclusive).",
"required": true
},
14"upper_limit": {
"type": "integer",
"description": "The end of the range (inclusive).",
"required": true
},
"multiples": {
"type": "array",
"description": "The numbers to find multiples of.",
"required": true
}
}
},
{
"name": "math_toolkit.product_of_primes",
"description": "Find the product of the first n prime
numbers.",
"parameters": {
"count": {
"type": "integer",
"description": "The number of prime numbers to
multiply together.",
"required": true
}
}
}
],
"answers": [
{
"name": "math_toolkit.sum_of_multiples",
"arguments": {
"lower_limit": 1,
"upper_limit": 1000,
"multiples": [3, 5]
}
},
{
"name": "math_toolkit.product_of_primes",
"arguments": {
"count": 5
}
}
]
}
In this example, the query asks to find the sum of multiples of 3 and 5 between 1 and
1000, and also find the product of the first five prime numbers. The available tools are
math_toolkit.sum_of_multiplesandmath_toolkit.product_of_primes,alongwiththeir
parameterdescriptions. Theanswersarrayprovidesthespecifictoolandargumentsusedtogenerate
eachanswer.
A.3 HumanEvaluationofDatasetQuality
To ensure that the three-stage verification process employed by APIGen produces a high-quality
dataset,weconductahumanevaluationonasampleofthegenerateddata. Weengagethreehuman
evaluators to manually inspect a total of 600 samples from our released dataset. The evaluators
assessthequalityofeachsamplebasedonfactorssuchastheaccuracyofparametervaluesandthe
appropriatenessofthenumberofAPIcalls.
15Theresultsofthehumanevaluationrevealthatonly28outofthe600inspectedsampleshaveminor
issues,suchasinaccurateparametervaluesormoreAPIcallsthanexpected. Thismeansthatthe
majorityofthedata,approximately95.3%,areofveryhighquality. Thehighqualityofthedataset
canbeattributedtotheformatandexecutioncheckersimplementedintheAPIGenpipeline.
TheformatcheckerensuresthatthegenerateddataadherestothespecifiedJSONformatandcontains
allthenecessaryfields. Thisstephelpstofilteroutpoorlyformattedorincompletedatapoints. The
executionchecker,ontheotherhand,executesthegeneratedfunctioncallsagainsttheappropriate
backendandverifiestheirsuccessfulexecution. Byprovidingrealexecutionresults,theexecution
checkerplaysacrucialroleinfilteringoutcasesthatmightbedifficulttoidentifybyanLLM-based
semanticcheckeralone.
The combination of these two checkers, along with the final semantic checker, creates a robust
verificationprocessthateffectivelyfiltersoutlow-qualitydatapoints. Thehumanevaluationresults
confirm the effectiveness of this approach, demonstrating that APIGen is capable of generating
high-qualitydatasetsfortrainingfunction-callingagents.
B DatasetGenerationandExperimentDetails
B.1 GeneratorLLMPrompt
ExamplePromptfortheGeneratortoGenerateParallelFunction-CallingData
"""
You are a data labeler. The responsibility for you is to
generate a set of diverse queries and corresponding
answers for the given functions in JSON format.
Construct queries and answers that exemplifies how to use
these functions in a practical scenario. Include in each
query specific, plausible values for each parameter. For
instance, if the function requires a date, use a typical
and reasonable date.
Ensure the query:
− Is clear and concise
− Contain multiple parallel queries in natural language for
the given functions, they could use either the same
function with different arguments or different functions
− Demonstrates typical use cases
− Includes all necessary parameters in a meaningful way. For
numerical parameters, it could be either numerals or words
− Across a variety level of difficulties, ranging from
beginner and advanced use cases
− The corresponding result’s parameter types and ranges match
with the functions descriptions.
Ensure the answer:
− Is a list of function calls in JSON format.
− The length of the answer list should be equal to the number
of requests in the query
− Can solve all the requests in the query effectively
Here are examples of queries and corresponding answers for
similar functions:
{examples}
16Note that the query could be interpreted as a combination of
several independent requests.
Based on these examples and the above instructions, generate
{number} diverse query and answer pairs for the functions
‘{func_name}‘.
The detailed functions description is as follows:
{func_desc}
{format_inst}
Now please generate {number} diverse query and answer pairs
following the above format.
"""
ThetemplateprovidedoutlinesthepromptforanLLMtogeneratedatasetsasdatalabelers,empha-
sizingthediversityofquerytypesandcomplexitytoensurethoroughcoverageofpotentialreal-world
applications. Itspecifiestheimportanceofgeneratingclear,concisequeriesandpreciselyformatted
JSONresponses. Sampleddata,usedtopopulatetheexamplesfield,andAPIinformation,filling
the func_name and func_desc fields, enable a structured approach to dataset generation. The
format_instspecifiestheenforcedJSONoutputformat,asshownbelow.
ExampleFormatInstructiontoGenerateParallelFunction-CallingData
The output MUST strictly adhere to the following JSON format,
and NO other text MUST be included:
‘‘‘
[
{
"query": "The generated query.",
"answers": [
{
"name": "api_name",
"arguments": {
"arg_name": "value",
... (more arguments as required)
}
},
... (more API calls as required)
]
}
]
‘‘‘
TheenforcedJSONoutputformatfacilitatesefficientdataextractionandcost-effectivegeneration.
Byrequestingmultiplequery-answerpairsinasingleinferencewiththenumberfield—referredto
hereasa"batching"technique—tokenusageandcostsaresignificantlyreduced.
B.2 SemanticCheckerLLMPrompt
WepromptedanotherLLMasthesemanticcheckertoevaluatewhethertheexecutionresultsandthe
toolcallsalignwiththeuserquery. WecouldusemultipleLLMswithdifferentpromptsascheckers
heretoincreasethecredibilityofthisverificationstage. Weprovideoneexamplepromptasfollows.
17ExamplePromptfortheSemanticCheckertoVerifytheData
"""
As a data quality evaluator, you must assess the alignment
between a user query, corresponding function calls, and
their execution results.
These function calls and results are generated by other
models, and your task is to ensure these results
accurately reflect the user’s intentions.
Do not pass if:
1. The function call does not align with the query’s
objective, or the input arguments appear incorrect.
2. The function call and arguments are not properly chosen
from the available functions.
3. The number of function calls does not correspond to the
user’s intentions.
4. The execution results are irrelevant and do not match the
function’s purpose.
5. The execution results contain errors or reflect that the
function calls were not executed successfully.
Given Information:
− All Available Functions:
{func_desc}
− User Query: {query}
− Generated Function Calls: {func_call}
− Execution Results: {execution_result}
Note: The query may have multiple intentions. Functions may
be placeholders, and execution results may be truncated
due to length, which is acceptable and should not cause a
failure.
The main decision factor is wheather the function calls
accurately reflect the query’s intentions and the function
descriptions.
Provide your reasoning in the thought section and decide if
the data passes (answer yes or no).
If not passing, concisely explain your reasons in the thought
section; otherwise, leave this section blank.
Your response MUST strictly adhere to the following JSON
format, and NO other text MUST be included.
‘‘‘
{{
"thought": "Concisely describe your reasoning here",
"pass": "yes" or "no"
}}
‘‘‘
"""
18Here, the func_desc field is the same as the generator, while the func_call and
execution_result are the key fields to determine whether the generated data successfully ad-
dressthequery’sintention. WealsoenforcethemodeltooutputaJSON-formattedstring,andthen
extractwhetherweshouldgiveapasstothisdatapoint.
B.3 ModelTraining
Wetraintwofunction-callingmodelsofdifferentsizes,xLAM-1B(FC)andxLAM-7B(FC),using
thedatasetgeneratedbyAPIGen. ThetrainingpipelinemainlyfollowstheAgentOhanapaper[23].
Weuse8NVIDIAA10040GBGPUsfortrainingbothmodels.
SincetheBerkeleyFunction-CallingBenchmark[9]containsarelevancedetectioncategory,which
evaluates a model’s ability to distinguish non-relevant queries and tools, we extend APIGen to
generaterelevancedetectiondatapointsfromthegenerateddatasets. Thesedatapointscovertwo
typesofscenarios:
• Theprovidedtoolscannotsolvethequery(e.g.,query: "IwanttoknowtheweatherinPaloAlto
onDec25,2023,"providedtool: get_house_price(city)).
• Theprovidedtoolsaremissingkeyargumentstosolvethequery(e.g.,query: "Iwanttoknowthe
weatherinPaloAltoonDec25,2023,"providedtool: get_weather(city)).
Inbothcases,thecorrectoutputisanemptytoolcalloraconciseexplanationindicatingthatthe
modelshouldrefusetoanswerduetoinsufficientorirrelevantinformation.
Wecreate8,000suchdatapointsfromthecollecteddatasetby1)randomlydiscardingsometools
that will be called in the answer or 2) randomly dropping some required parameters that were
used in the generated tool calls. Then we relabel the answer to be an empty tool call or with a
conciseexplanation. Byincorporatingrelevancedetectiondatapointsintoourtrainingdatasets,we
canenhancethemodel’sperformanceindeterminingwhentheprovidedtoolsarenotsuitablefor
addressingagivenquery. Thisenablesthetrainingofagentsthatcaneffectivelyassesstherelevance
oftheavailabletoolsandrespondappropriately,eitherbyutilizingtherelevanttoolsorbyrefraining
fromansweringwhenthenecessaryinformationislacking.
Whentrainingthemodel, wefillinthesampledqueryandavailabletoolstothetrainingprompt
template,andthenaskthemodeltopredictthecorrespondingtoolcallsinspecifiedJSONformat.
Thetrainingprompttemplateisasfollows:
ModelTrainingPrompt
"""
[BEGIN OF TASK INSTRUCTION]
You are an expert in composing functions. You are given a
question and a set of possible functions.
Based on the question, you will need to make one or more
function/tool calls to achieve the purpose.
If none of the function can be used, point it out and refuse
to answer.
If the given question lacks the parameters required by the
function, also point it out.
[END OF TASK INSTRUCTION]
[BEGIN OF AVAILABLE TOOLS]
{func_desc}
[END OF AVAILABLE TOOLS]
[BEGIN OF FORMAT INSTRUCTION]
The output MUST strictly adhere to the following JSON format,
and NO other text MUST be included.
19The example format is as follows. Please make sure the
parameter type is correct. If no function call is needed,
please make tool_calls an empty list ’[]’
‘‘‘
{{
"tool_calls": [
{{"name": "func_name1", "arguments": {{"argument1": "
value1", "argument2": "value2"}}}},
... (more tool calls as required)
]
}}
‘‘‘
[END OF FORMAT INSTRUCTION]
[BEGIN OF QUERY]
User Query: {query}
[END OF QUERY]
"""
Thetraininghyperparametersforourmodelsincludealearningrateof5×10−6,fourepochs,and
useoftheAdamWoptimizer. Othersettingsincludeacutofflengthof2048,aper-devicebatchsize
ofsix,twogradientaccumulationsteps,acosinelearningrateschedulerwith50warmupsteps,and
thebfloat16(BF16)datatype.
20