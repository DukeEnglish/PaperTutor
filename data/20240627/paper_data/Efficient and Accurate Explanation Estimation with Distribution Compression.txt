Efficient and Accurate Explanation Estimation with Distribution Compression
HubertBaniecki1 GiuseppeCasalicchio23 BerndBischl23 PrzemyslawBiecek14
Abstract approximating the conditional feature distribution (Chen
etal.,2018;Aasetal.,2021;Olsenetal.,2022;2024).
Exactcomputationofvariousmachinelearning
explanations requires numerous model evalua- However, in many practical settings, a marginal feature
tions and in extreme cases becomes impracti- distribution is used instead to estimate explanations, and
cal. The computational cost of approximation i.i.d. samples from the data typically form the so-called
increaseswithanever-increasingsizeofdataand backgrounddatasamples,alsoknownasreferencepoints
model parameters. Many heuristics have been or baselines, which plays a crucial role in the estimation
proposedtoapproximatepost-hocexplanations process(Lundberg&Lee,2017;Covertetal.,2020;Schol-
efficiently. This paper shows that the standard becketal.,2020;Erionetal.,2021;Ghalebikesabietal.,
i.i.d. samplingusedinabroadspectrumofalgo- 2021;Lundstrometal.,2022). Forexample,Covertetal.
rithmsforexplanationestimationleadstoanap- (2020)mention“[O]ursamplingapproximationforSAGE
proximationerrorworthyofimprovement.Tothis was run using draws from the marginal distribution. We
end,weintroducecompressthenexplain(CTE),a usedafixedsetof512backgroundsamples[...]” andwe
newparadigmformoreefficientandaccurateex- providemoresuchquotesinAppendixAtomotivateour
planationestimation. CTEusesdistributioncom- research question: Can we reliably improve on standard
pressionthroughkernelthinningtoobtainadata i.i.d. samplinginexplanationestimation?
samplethatbestapproximatesthemarginaldistri-
We make a connection to research on statistical theory,
bution.WeshowthatCTEimprovestheestimation
wherekernelthinning(KT,Dwivedi&Mackey,2021;2022)
of removal-based local and global explanations
wasintroducedtocompressadistributionmoreeffectively
withnegligiblecomputationaloverhead. Itoften
thanwithi.i.d. sampling. KThasanefficientimplementa-
achievesanon-parexplanationapproximationer-
tionintheCOMPRESS++algorithm(Shettyetal.,2022)and
rorusing2–3×lesssamples,i.e. requiring2–3×
wasappliedtoimprovestatisticalkerneltesting(Domingo-
lessmodelevaluations. CTEisasimple,yetpow-
Enrichetal.,2023). Buildingonthislineofwork,thispa-
erful,plug-inforanyexplanationmethodthatnow
peraimstothoroughlyquantifytheerrorintroducedbythe
reliesoni.i.d. sampling.
currentsamplethenexplainparadigminfeaturemarginal-
ization,whichisinvolvedintheestimationofbothlocaland
globalremoval-basedexplanations(Covertetal.,2021). We
1.Introduction proposeanefficientwaytoreducethisapproximationerror
basedondistributioncompression(Figure1). Wereferthe
Computationallyefficientestimationofpost-hocexplana-
readertotheargumentsprovidedin(Herrmannetal.,2024)
tionsisattheforefrontofcurrentresearchonexplainable
concerningtheaddedvalueofconductingsuchempirical
machinelearning(Strumbelj&Kononenko,2010;Covert&
researchinmachinelearning.Namely,theresearchquestion
Lee,2021;Slacketal.,2021;Jethanietal.,2022;Chenetal.,
isratheropenandweaimtogaininsightintoapreviously
2023;Donnellyetal.,2023;Muschaliketal.,2024). The
unexploredarea.
majorityoftheworkfocusesonimprovingefficiencywith
respect to the dimension of features (Covert et al., 2020;
Contribution. Insummary,ourworkadvancescurrentlit-
Jethani et al., 2022; Chen et al., 2023; Fumagalli et al.,
eratureinmultipleways:(1)Quantifyingtheerrorofstan-
2023),specificmodelclasseslikeneuralnetworks(Erion
dardi.i.d. sampling: Webringtoattentionandmeasure
etal.,2021)anddecisiontrees(Muschaliketal.,2024),or
theapproximationerrorintroducedbyusingi.i.d. sampling
1University of Warsaw 2LMU Munich 3Munich Center for ofbackgroundandforegrounddatainvariousexplanation
MachineLearning(MCML)4WarsawUniveristyofTechnology.
methods. (2)Compressthenexplain: Weintroduceanew
Correspondenceto:HubertBaniecki<h.baniecki@uw.edu.pl>.
paradigm for estimating post-hoc explanations based on
DMLRWorkshopatthe41stInternationalConferenceonMachine amarginaldistributioncompressedmoreeffectivelythan
Learning,Vienna,Austria,2024.Copyright2024bytheauthor(s). withi.i.d. sampling. (3)Kernelthinningfor(explainable)
1
4202
nuJ
62
]GL.sc[
1v43381.6042:viXraEfficientandAccurateExplanationEstimationwithDistributionCompression
standard i.i.d. sampling Compress Then Explain distribution compression
estimated explanation
priors count is recid.
is recid. priors count
age race
race sex
sex age
charge degree charge degree
stay length stay length
approximation error
Figure1. Garbagesamplein,garbageexplanationout.Samplethenexplainisaconventionalapproachtodecreasethecomputational
costofexplanationestimation. Althoughfast, samplingisunstableandpronetoerror, whichmayevenleadtochangesinfeature
importancerankings.Weproposecompressthenexplain(CTE),anewparadigmforaccurate,yetefficient,estimationofexplanations
basedonamarginaldistributionthatiscompressed,e.g.withkernelthinning.
machinelearning: Weshowexperimentallythat KT out- Concerningdistributioncompression,themethodmostre-
performsi.i.d. samplingincompressingthedistributionof latedtoKT(Dwivedi&Mackey,2021)istheinferiorstan-
populardatasetsusedinresearchonexplainablemachine dardthinningapproach(Owen,2017). Cooperetal.(2023)
learning. Infact,thisisthefirstworktoevaluatedistribu- useinsightsfromKTtoacceleratedistributedtraining,while
tioncompressionviaKTondatasetsforsupervisedlearning. Zimmermanetal.(2024)applyKTinrobotics.Inthecontext
(4)Decreasingthecomputationalcostofexplanationes- ofdata-centricmachinelearning,webroadlyrelatetofind-
timation: Webenchmarkcompressthenexplain(CTE)with ingcoresetstoimprovetheefficiencyofclustering(Agar-
popularexplanationmethodsandshowitresultsinmoreac- waletal.,2004;Har-Peled&Mazumdar,2004)andactive
curateexplanationsofsmallervariance. CTEoftenachieves learning(Sener&Savarese,2018),aswellasdatasetdistil-
on-parerrorusing2–3×lesssamples,i.e. requiring2–3× lation(Wangetal.,2018)anddatasetcondensation(Zhao
lessmodelevaluations. CTEisasimple,yetpowerful,plug- etal.,2021;Kimetal.,2022)thatcreatesyntheticsamples
inforabroadclassofmethodsthatsamplefromadataset, toimprovetheefficiencyofmodeltraining.
e.g. removal-basedandglobalexplanations.
2.Preliminaries
Relatedwork. Ourworkisthefirsttoempiricallyevalu-
ateKTondatasetsforsupervisedlearning,andoneofthe Weaimtoexplainapredictionmodeltrainedonlabeleddata
firsttoreliablyimproveoni.i.d. samplingformultiplepost- anddenotedbyf :X (cid:55)→RwhereX isthefeaturespace;it
hocexplanationmethodsatonce. Labergeetal.(2023)pro- predictsanoutputusinganinputfeaturevectorx. Usually,
poseabiasedsamplingalgorithmtoattacktheestimationof weassumeX ⊆Rd. Withoutlossofgenerality,inthecase
featureattributions,whichfurthermotivatesfindingrobust ofclassification,weexplaintheoutputofasingleclassasa
improvementsfori.i.d. sampling. Ourresearchquestionis posteriorprobabilityfrom[0,1]. Weusuallyalsoassumea
orthogonaltothatofhowtosampleperturbationsaround givendataset{(x(1),y(1)),...,(x(n),y(n))},whereevery
aninput(Petsiuketal.,2018;Slacketal.,2021;Lietal., elementcomesfromX×Y,theunderlyingfeatureandlabel
2021;Ghalebikesabietal.,2021;Lietal.,2023), orhow space,onwhichtheexplanationarecomputed. Depending
toefficientlysamplefeaturecoalitions(Chenetal.,2018; on the explanation method, this can be a training or test
Covert & Lee, 2021; Fumagalli et al., 2023). Instead of dataset, anditcouldalsobeprovidedwithoutlabels. We
generatingsamplesfromtheconditionaldistributionitself,
denotethen×ddimensionaldatamatrixbyXwherex(i)
whichischallenging(Olsenetal.,2022),weexplorehowto
appearsinthei-throwofX,whichisassumedtobesampled
efficientlyselectanappropriatesubsetofbackgrounddata inani.i.d. fashionfromanunderlyingdistributionp(x,y)
forexplanations(Haseetal.,2021;Lundstrometal.,2022). definedonX ×Y. WedenotearandomvectorasX∈X.
SpecificallyforShapley-basedexplanations,Jethanietal. Further,lets⊂{1,...,d}beafeatureindexsetofinterest
(2022) propose to predict them with a learned surrogate withitscomplements¯={1,...,d}\s. Weindexfeature
model, while Kolpaczki et al. (2024) propose their repre- vectorsxandrandomvariablesXbyindexsetsstorestrict
sentationdetachedfromthenotionofmarginalcontribution. them to these index sets. We write p X(x) and p Xs(x s)
Weaimtoproposeageneralparadigmshiftthatwouldbene- formarginaldistributionsonXandX s,respectively,and
fitabroaderclassofexplanationmethodsincludingfeature p Xs|Xt(x s|x t)forconditionaldistributiononX s|X t. We
effects(Apley&Zhu,2020;Moosbaueretal.,2021)and use qX to denote an empirical distribution approximating
expectedgradients(Erionetal.,2021). p XbasedonadatamatrixX.
2EfficientandAccurateExplanationEstimationwithDistributionCompression
Sampling from the data matrix is prevalent in ex- points summarizing a target distribution p , the goal of
X
planation estimation. Various estimators of post-hoc distributioncompressionistoidentifyahighqualitycoreset
explanations sample from the data matrix to efficiently X (cid:101) ofsizen˜ ≪ n. Thisqualityismeasuredwiththecore-
approximate the explanation estimate (Appendix A). set’sintegrationerror(cid:12) (cid:12)1 (cid:80)n h(x(i))− 1 (cid:80)n˜ h(x˜(i))(cid:12) (cid:12)
n i=1 n˜ i=1
For example, many removal-based explanations (Covert forfunctionshinthereproducingkernelHilbertspacein-
et al., 2021) like SHAP (Lundberg & Lee, 2017) and ducedbyagivenkernelfunctionk(Muandetetal.,2017).
SAGE (Covert et al., 2020) rely on marginalizing fea- TherecentlyintroducedKTalgorithm(Dwivedi&Mackey,
tures out of the model function f using their joint 2021;2022)returnssuchacoresetthatminimizesthekernel
conditional distribution E [f(x ,X )] = maximummeandiscrepancy(MMD ,Grettonetal.,2012).
(cid:82)
Xs¯∼pXs¯|Xs=xs s s¯ k
f(x s,x s¯)p Xs¯|Xs=xs(x s¯|x s)dx s¯. Definition2.2(Kernelmaximummeandiscrepancy(Gret-
ton et al., 2012; Dwivedi & Mackey, 2021)). Let k :
Note that the practical approximation of the conditional
Rd ×Rd (cid:55)→ Rbeaboundedkernelfunctionwithk(x,·)
distributionp (x |x )itselfischallenging(Chen
Xs¯|Xs=xs s¯ s measurable for all x ∈ Rd, e.g. a Gaussian kernel.
etal.,2018;Aasetal.,2021;Olsenetal.,2022)andthereis
Kernel maximum mean discrepancy between probability
noidealsolutiontothisproblem(seearecentbenchmark
distributions p,q on Rd is defined as MMD (p,q) :=
byOlsenetal.,2024). Infact,in(Covertetal.,2020,Ap- k
pendixD),itismentionedthatthedefaultforSAGEistoas-
sup
h∈Hk:∥h∥
k≤1(cid:12) (cid:12)E X∼pXh(X)−E X∼qXh(X)(cid:12) (cid:12),whereH
k
isareproducingkernelHilbertinducedbyk.
sumefeatureindependenceandusethemarginaldistribution
p Xs¯|Xs=xs(x s¯|x s):=p Xs¯(x s¯);sodoestheKERNEL-SHAP
An unbiased empirical estimate of MMD can be rela-
estimator(apracticalimplementationofSHAP,Lundberg& k
tivelyeasilycomputedgivenakernelfunctionk(Gretton
Lee,2017). Thistrendcontinuesinmorerecentwork(Fu-
magallietal.,2023;Krzyzin´skietal.,2023).
etal.,2012). COMPRESS++(Shettyetal.,2022)isa √neffi-
cientalgorithmforKTtha √treturnsacoresetofsize nin
Definition 2.1 (Feature marginalization). Given a set of O(nlog3n)timeandO( nlog2n)space,makingKTvi-
observed values x s, we define a model function with ableforlargedatasets. Itwasadaptedtoimprovethekernel
marginalized features from the set s¯ as f(x s;p X) := two-sampletest(Domingo-Enrichetal.,2023).
E [f(x ,X )].
Xs¯∼pXs¯ s s¯
3.CompressThenExplain(CTE)
In practice, the expectation E [f(x ,X )] is es-
timated by i.i.d. sampling
froX ms¯∼ tp hX es¯ datass
et
Xs¯
that ap- Weproposeusingdistributioncompressioninsteadofi.i.d.
proximates the distribution p Xs¯(x s¯). This sampled set samplingforfeaturemarginalizationinremoval-basedex-
of points forms the so-called background data, aka ref- planationsandforaggregatingglobalexplanations. Wenow
erence points, or baselines as specifically in case of formalizetheproblemandprovidetheoreticalintuitionasto
the EXPECTED-GRADIENTS (Erion et al., 2021) expla- whymethodsfordistributioncompressioncanleadtomore
nation method defined as EXPECTED-GRADIENTS(x) := accurateexplanationestimates.
(cid:104) (cid:105)
E X∼pX,α∼U(0,1) (x−X)· ∂f(X+ ∂α x·(x−X)) . Definition3.1(Localexplanationbasedonfeaturemarginal-
ization). Alocalexplanationfunctiong(x;f,p )ofmodel
Furthermore,i.i.d. samplingisusedinglobalexplanation X
f thatreliesonadistributionp forfeaturemarginalization.
methods, which typically are an aggregation of local ex- X
Forestimation,itusesanempiricaldistributionqXinplace
planations. To improve the computational efficiency of
these approximations, often only a subset of X is consid- of p X. Examples include SHAP (Lundberg & Lee, 2017)
andEXPECTED-GRADIENTS(Erionetal.,2021).
ered;calledforegrounddata. Examplesinclude: FEATURE-
EFFECTSexplanations(Apley&Zhu,2020),anaggregation
Localexplanationsareoftenaggregatedintoglobalexplana-
ofLIME(Ribeiroetal.,2016)intoG-LIME(Lietal.,2023),
tionsbasedonarepresentativesamplefromdataresulting
and again SAGE (Covert & Lee, 2021), for which points inestimatesoffeatureimportanceandeffects.
fromXrequiretohavetheircorrespondinglabelsy.
Definition3.2(Globalexplanation). Aglobalexplanation
functionofmodelf thataggregateslocalexplanationsover
Background on distribution compression. Standard samplesfromp , i.e. G(p ;f,g) := E [g(X;f,·)].
sampling strategies can be inefficient. For exam-
X X X∼pX
Examples include FEATURE-EFFECTS like partial depen-
ple, the Monte Carlo estimate 1 (cid:80)n h(x(i)) of an
n i=1 dence plots and accumulated local effects (Apley & Zhu,
unknown expectation E h(X) based on n i.i.d.
p 1o (cid:80)int ns h has (xΘ (i( ))1
(cid:12)
(cid:12)/√ reqn u) iri in nt geX g 1∼ r 0ap 2tX io pn oine tr sro fr or(cid:12) (cid:12)E
10X %∼p rX
elh a( tiX ve) e−
r-
2 re0 q2 u0 ir) e, san ad saS nA iG nE pu( tC lo av be elr ste yt oa fl. t, h2 e0 s2 a0 m), pw leh si dc rh awad nd fi rt oio mna pl Xly
.
n i=1
ror and 104 points for 1% error (Shetty et al., 2022). To Notably, the local explanation function g in SAGE it-
improveoni.i.d. sampling,givenasequenceXofninput selfreliesonfeaturemarginalizationleadingtousingp
X
3EfficientandAccurateExplanationEstimationwithDistributionCompression
Listing1Codesnippetshowingthe3-lineplug-inofdistributioncompressionforSAGEestimation.
X, y, model = ...
from goodpoints import compress
ids = compress.compresspp_kt(X, kernel_type=b"gaussian", g=4)
X_compressed = X[ids]
import sage
imputer = sage.MarginalImputer(model.predict, X_compressed)
estimator = sage.KernelEstimator(imputer)
explanation = estimator(X, y)
# or even
y_compressed = y[ids]
explanation = estimator(X_compressed, y_compressed)
twice(seeListing1). Weaimtoprovidehighqualityexpla- Proof. SeeAppendixB.
nationsstemmingfromcompressedsamplesasmeasured
Effectively,Propositions3.4&3.5statethatadistribution
withagivenapproximationerror.
compressionalgorithmminimizingTVwouldrestrictthe
approximationerrorofexplanationestimation. Notethat
Problemformulation. Theproblemformulationinthis
theempiricalestimatorofTVisnotconsistentanditsprac-
workisstraightforward:
tical estimation in high dimensions is challenging (Sripe-
(cid:13) (cid:13)
m Xi ′n (cid:13)g(x;f,qX)−g(x;f,q X(cid:101))(cid:13) rumbudur et al., 2009). Thus, other metrics between dis-
(cid:13) (cid:13)
tributions are often used like MMD
k
minimized by KT.
or (cid:13)G(qX;f,g)−G(q X(cid:101);f,g)(cid:13) (1)
Lemma3.6isaknownresultrelatingMMD ktoTV.
s.t. |X (cid:101)|=n˜ ≪n Lemma3.6(Theorem21in(Sriperumbuduretal.,2010)).
Fortwodistributionsp,q onRd,wehaveMMD (p,q) ≤
foragivenn˜wherei.i.d. samplingorKTarethetwopoten- √ k
C ·TV(p,q),whereC denotesaconstantthatbounds
tialalgorithmstofindX (cid:101) inanunsupervisedmanner. k k
thekernelfunctionk.
ToformulatePropositions3.4&3.5,werecallthedefinition
In Section 4.2, we show experimentally that, in practical
oftotalvariationdistance.
machinelearningsettings,minimizingMMD leadstode-
k
Definition 3.3 (Total variation distance (Sriperumbudur
creasingotherdistributiondiscrepancieslikeTV.Therefore,
etal.,2009)). Totalvariationdistancebetweenprobability
CTEhasapotentialtopositivelyimpactexplanationestima-
distributionsp,q onRd isdefinedasTV(p,q) := |p−q|,
tion(Propositions3.4&3.5). Sinceintheoryitiseasyto
(cid:82)
where|p|:= |p(x)|dxisthel functionaldistance.
1 provide a counterexample for this intuition (Lemma 3.6),
Proposition 3.4 (Feature marginalization is bounded by futureworkisneededondistributioncompressionmethods
thetotalvariationdistancebetweendatasamples). Fortwo withstrongerguarantees.
(cid:12) (cid:12)
distributions qX,q X(cid:101), we have (cid:12)f(x s;qX) − f(x s;q X(cid:101))(cid:12) ≤
CTEisrelativelysimpletoplug-intothecurrentworkflows
C
f
·TV(qX,q X(cid:101)),whereC
f
denotesaconstantthatbounds
forexplanationestimationasshowninListing1forSAGE.
themodelfunctionf.
WeprovideanalogouscodelistingsforSHAP,EXPECTED-
Proof. Followsfrom(Banieckietal.,2024,Thm. 2). GRADIENTSandFEATURE-EFFECTSinAppendixC.
Proposition 3.4 provides a worst-case bound for feature
4.Experiments
marginalization, the backbone of local explanations, in
termsofdistancebetweenthe(oftencompressed)empirical
In experiments, we empirically validate that the CTE
distributions.Itcomplementstheresultsforinputandmodel
paradigmimprovesexplanationestimationacross4meth-
perturbationsobtainedin(Linetal.,2023,Lemmas1&4),
ods, 2 model classes, and over 50 datasets. We compare
whichalsoshowhowsuchboundpropagatestothelocal
CTEtothewidelyadoptedpracticeofi.i.d. sampling(see
explanationfunctiong. Analogously,wecanderivePropo-
AppendixAforfurthermotivation). Wealsoreportsanity
sition3.5forglobalaggregatedexplanations.
checkresultsforamoredeterministicbaseline–sampling
Proposition 3.5 (Global explanation is bounded by total withk-medoids–wherecentroidsfromtheclusteringdefine
variation distance between data samples). For two distri- a coreset from the dataset. We use the default hyperpa-
(cid:13) (cid:13)
butions qX,q X(cid:101), we have (cid:13)G(qX;f,g) − G(q X(cid:101);f,g)(cid:13)
1
≤ rametersofexplanationalgorithms(detailsareprovidedin
C g·TV(qX,q X(cid:101)),whereC
g
denotesaconstantthatbounds AppendixD.1). Fordistributioncompression,weuseCOM-
thelocalexplanationfunctiong. PRESS++implementedinthegoodpointsPythonpack-
4EfficientandAccurateExplanationEstimationwithDistributionCompression
gaussian gaussian
compas compas
compress
heloc sample heloc
cluster
adult adult
gmsc gmsc
0.000 0.002 0.004 0.006 0.0 0.1 0.2 0.3
MMD TV
gaussian gaussian compress
sample
compas compas cluster
heloc heloc
adult adult
gmsc gmsc
0.0 0.1 0.2 0.3 0.00 0.05 0.10 0.15 0.20 0.25
WD KL
Figure2.COMPRESS++withGaussiankernelon5datasetsforthe4considereddistributionmetrics,colorindicatestheapplieddownsam-
pling.Thelengthofthebarismeanvalue±standarderroracrossstatisticalrepetitions.
age(Dwivedi&Mackey,2021),wherewefollow(Shetty Efficiency. Wemeasuretheefficiencyofcompressionand
√
etal.,2022)touseaGaussiankernelkwithσ = 2d. For estimatingasingleexplanationwithCPUwall-clocktime
allthecomparedmethods,thesubsampledsetofpointsisof (inseconds). Weassumethetimeofi.i.d. samplingis0. We
√
size nasweleaveoversamplingdistributioncompression relyonpopularopen-sourceimplementationsofthealgo-
forfuturework. Werepeatallexperimentswhereweapply rithms(seeAppendixC)andperformefficiencyexperiments
someformofdownsamplingbeforeexplanationestimation onapersonalcomputerwithanM3chip. Weacknowledge
33 times and report the mean and standard error (se.) or thatspecifictimeestimateswillvaryinmoresophisticated
deviation(sd.) ofmetricvalues. settings, but our setup aims to imitate the most standard
workflowofexplanationestimation.
4.1.Evaluationmetrics
Distributionchange. AsasanitycheckforCTE,weshow
Groundtruth. ThegoalofCTEistoimproveexplanation thatCOMPRESS++worksout-of-the-boxonpopulardatasets
estimationoverthestandardi.i.d.sampling.Wemeasurethe for (explainable) machine learning without tuning its hy-
accuracyandeffectivenessofexplanationestimationwith perparameters. Measuring the similarity of distributions
respect to a “ground truth” explanation that is estimated ordatasetsischallenging,andmanymetricswithvarious
using a full validation dataset X, i.e. without sampling properties have been proposed for this task (Gibbs & Su,
or compression. We consider settings where this is very 2002). InSection4.2,wereportthefollowingdistancemet-
inefficient to compute in practice (n := n is between ricsbetweentheoriginalandcompresseddistribution: the
valid
1000and25000samples). Forlargedatasets,wetruncate optimizedMMD k,TV,Kullback–Leiblerdivergence(KL,
the validation dataset to 20× the size of the compressed Gibbs & Su, 2002), and n-dimensional Wasserstein dis-
dataset. Sincesomeexplanationmethodsincludearandom tance(WD,Feydyetal.,2019;Labergeetal.,2023). Since
componentinthealgorithm,werepeattheirgroundtruth approximatingn-dimensionalTVandKLisinfeasiblein
estimation3timesandaveragetheresultingexplanations. practice,wereportanaverageofthetop-3largestdiscrep-
anciesbetweenthe1Ddistributionsoffeatures.
Accuracy. We are mainly interested in the accuracy of
estimating a single explanation, measured by the expla-
4.2.Kernelthinningonmachinelearningdatasets
nation approximation error. Namely, mean absolute er-
r g do 1( Gr x
(cid:13)
(cid:13)( (M Gi); (A f qXE ,q ;) X(cid:101), f)w ,(cid:13) (cid:13) gh
1
)e −fr oe Grw S (qe H X(cid:101)Ah ;a P fv a ,e n gd )n
(cid:13)
(cid:13)v Ea1 1li Xd· wPd E i(cid:80) tC hT dn i= E Gva D1lid =-(cid:13) (cid:13) Gg R d( Ax foD( ri I) E S; ANf GT,q S EX , .a) Wn− d
e
F n
e
ote bir at ss w
l
et ., r,o vw 2r ak 0e
t2
imu o2s n)oe
.
sdt Weh inle esp tfif hr r
l
ee o tp em vrro aot lc h
u
ie dtes ats O
h
te ird op
e
ne ed na sdXt eaa ttAs ,ae sI wt es b
t
ha se en n
w
rd ec ihp
t
shmr ae
l
mat er r sa pk si ln it( nhe Ad gag nn ia s1e ru 0w nr
0
oaa 0l tl
haved
G
=100·(d+d2)forFEATURE-EFFECTS,sincewe
crucial,whichresultsinfivetasks: gaussian(asynthetic
use100uniformlydistributedgridpointsfor1-dimensional
dataset, n = 1250, d = 20), compas (n = 1235,
effectsand10×10uniformlydistributedgridpointsfor2- valid valid
d = 7), heloc (aka FICO, n = 1975, d = 23),
dimensionaleffects(seeAppendixD).Forbroadercontext, valid
adult(n =9045,d=13),andgmsc(GiveMeSome
inSection4.4,wealsomeasuretheprecisionincorrectly valid
Credit,n =20442,d=10). Furtherdetailsondatasets
indicatingthetopkfeatures(thepercentageatop-kfeature valid
andmodelsareprovidedinAppendixD.2. InFigure2,we
identifiedbythedownsampledexplanationisatop-kfeature
observethatCOMPRESS++worksmuchbetterintermsof
inthegroundtruth).
5EfficientandAccurateExplanationEstimationwithDistributionCompression
KernelSHAP KernelSHAP
PermutationSHAP PermutationSHAP
compress
sample
KernelSAGE KernelSAGE
compress
PermutationSAGE PermutationSAGE sample
0.000 0.002 0.004 0.006 0.008 0.5 0.6 0.7 0.8 0.9 1.0
MAE Top-k
KernelSHAP KernelSHAP
PermutationSHAP PermutationSHAP
explain (ground truth)
explain (compressed)
KernelSAGE KernelSAGE
compress
PermutationSAGE explain PermutationSAGE
10 1 100 101 102 103 104 101 102 103 104 105 106
Time [s] Time [s]
Figure3. ComparisonbetweenCTEandi.i.d.samplingforthetwoestimatorsofSHAPandSAGEexplanationsontheadultdataset.We
measuremeanabsoluteerror(MAE,↓)betweenfeatureattributionandimportancevalues,aswellastheprecisionincorrectlyidentifying
the5mostimportantfeatures(Top-k,↑).Analogousresultsfortheother4datasetsareinAppendixE.(mean±se.)
MMD onalldatasetscomparedtostandardi.i.d. sampling
k gaussian compress
ortheclusteringbaseline,whichisnosurpriseasthismetric cluster
is internally optimized by the former. It also leads to no- compas
tableimprovementsinallothermetrics. Overall,thereisno
heloc
consistentimprovementinapproximatingthedistribution
usingclustering. adult
gmsc
4.3.CTEasanefficientalternativetoi.i.d. samplingin
explanationestimation 10 2 10 1 100 101
Time [s]
We find CTE to be a very efficient alternative to standard
i.i.d. samplinginexplanationestimation. Forexample,com- Figure4.Compressing a distribution from 20k to 128 samples
pressingadistributionfrom1kto32samplestakeslessthan takeslessthan1secondtocomputeonaCPU.(mean±se.)
0.1seconds,andfrom20kto128samplestakeslessthan
1second. The exactruntimewill, ofcourse, differbased
onthenumberoffeatures. Figure4reportsthewall-clock
efficientandresultsinhigherdistributionerroronOpenXAI,
timefordatasetsofdifferentsizes. Notethatthepotential
soweomittocompareitagainstCTEhere,butwillinclude
runtimes for distribution compression are of magnitudes
itinfurtherablationsinSections4.5&4.6.
smallerthanthetypicalruntimeofexplanationestimation.
Forexample,estimatingKERNEL-SHAPorPERMUTATION- WenextshowthatCTEimprovestheestimationoffeature
SAGE for1ksamplesusing32backgroundsamplestakes attributionandimportanceexplanations,namelyforSHAP
about 10 seconds, which is about 30× less than estimat- andSAGE. Weexperimentwithtwomodel-agnosticapprox-
ingthegroundtruthexplanation(AppendixE).Moreover, imators:kernel-basedandpermutation-based. Wereportthe
estimatingKERNEL-SHAPfor9ksamplesusing128back- differencesinMAEandTop-kbetweenCTEandstandard
groundsamplestakes30minutes,whichisabout60×less i.i.d. sampling in Figure 3 (adult). Analogous results
thanestimatingthegroundtruthexplanation(Figure3). fortheother4datasetsfromOpenXAIareinAppendixE.
Onallconsideredtasks,CTEresultsinanotabledecrease
4.4.CTEimprovestheaccuracyofestimatingfeature inapproximationerrorwhencomparedtoi.i.d. sampling
attributionandimportanceexplanations andanincreaseinprecision(fortop-kfeatureidentification)
withneglectablecomputationaloverhead.
Wehavealreadyestablishedthatdistributioncompression
isaviableapproachtodatasamplingthatregularlyentails
4.5.CTEimprovesgradient-basedexplanations
a better approximation of feature distribution. Moreover,
itscomputationaloverheadisnegligiblewhenusedforex- Here, we aim to show the broader applicability of CTE
planationestimation. Wehaveshownthatclusteringisless byevaluatingitongradient-basedexplanationsspecificto
neuralnetworks,oftenfittedtolargerunstructureddatasets.
6EfficientandAccurateExplanationEstimationwithDistributionCompression
Explanation: Expected Gradients sample compress cluster
1e 4 mnist_784 1e 3 satimage 1e 4 Fashion-MNIST 1e 4 CIFAR_10
6
2.4 1.8 1.8
4
1.6 1.2 1.2
0.8 2 0.6 0.6
0.0 0 0.0 0.0
128 256 384 512 32 64 96 128 128 256 384 512 64 128 192 256
N samples N samples N samples N samples
Figure5. ComparisonbetweenCTE,i.i.d.samplingandclusteringforEXPECTED-GRADIENTSexplanationsonthe4imageclassificaiton
datasets. Wemeasuremeanabsoluteerror(MAE,↓)betweenfeatureattributionvalues. CTEisnotonlymoreaccuratebutalsomore
stableasmeasuredwithstandarddeviation.Analogousresultsfortheremaining14datasetsareinAppendixF.(mean±sd.)
Sanity check. We first compress the validation sets of
imdb compress
IMDB and ImageNet-1k on a single CPU as a sanity sample
cluster imdb
checkfortheviabilityofCTEinsettingsconsideringlarger imagenet1k
imagenet1k
datasets. FortheIMDBdataset(n valid =25000,d=768), 10 5 10 4 0 5 10
CTEtakesasaninputtextembeddingsfromthepretrained MMD Time [s]
DistilBERTmodel’slastlayer(precedingaclassifier)that
has a dimension of size 768. Similarly, for ImageNet-1k
Figure6.Left: COMPRESS++ effectively optimizes MMD
k
on
IMDBandImagenet-1kdatasets.Right:Compressingadistribu-
(n valid = 50000, d = 512), CTE operates on the hidden tionfrom25k–50kto128samplesin512–768dimensionstakes
representationextractedfromResNet-18. Figure6shows
about5–10secondtocomputeonaCPU.(mean±se.)
theoptimizedMMD metricbetweenthedistributionsand
k
computationtimeinseconds. Notethattheothermetrics
fordistributionchangeconsideredinSection4.2havelittle 4.6.Ablations
applicabilitytosuchdatasets. Wecansee,again,thatproper
compression results in huge benefits w.r.t. MMD (com- WeperformadditionalexperimentstoevaluateCTEonvar-
k
iousdatasets, with adifferent modelclass, andincluding
paredtoi.i.d. samplingandclustering)andonlynegligible
anotherglobalexplanationmethod. Morespecifically,we
computationaloverhead.
useCTEtoimproveFEATURE-EFFECTSofXGBoostmodels
Accuracyandefficiency. WenowstudyCTEtogetherwith trainedonfurther30datasets(n >1000,d<32)from
valid
EXPECTED-GRADIENTSofneuralnetworkmodelstrained OpenML-CC18andOpenML-CTR23. Detailsondatasets
to18datasets(n
valid
> 1000,d ≥ 32)fromtheOpenML-
andmodelsareprovidedinAppendixD.2. Moreover,we
CC18(Bischletal.,2021)andOpenML-CTR23(Fischer includeSHAPandSAGEinthebenchmarksimilarlytoSec-
etal.,2023)collections. Detailsondatasetsandmodelsare tion4.4. Asanotherablation,SAGEisevaluatedintwovari-
providedinAppendixD.2. Figure5showstheexplanation antsthatconsidereithercompressingonlythebackground
approximationerrorfor4imageclassificationtasks,while data(arathertypicalscenario),orusingthecompressedsam-
analogousresultsfortheremaining14datasetsareprovided plesasbothbackgroundandforegrounddata(asindicated
in Appendix F. Here, we vary the number of data points with“fg.”;refertoListing1forthisdistinction).
sampledfromi.i.d. toinspecttheincreaseinefficiencyof
Figure7showstheexplanationapproximationerrorfor3
CTE. Inallcases,CTEachieveson-parapproximationerror
predictivetasks,whileanalogousresultsfortheremaining
usinglesssamplesthani.i.d. sampling,i.e. requiringless
explanationestimatorsand27datasetsareprovidedinAp-
modelevaluations,resultinginfastercomputationandsaved
resources.
pendixG.WeobservethatCTEsignificantlyimprovesthe
estimationofFEATURE-EFFECTS inallcases. Wefurther
Model-agnostic explanation of a language model. In confirmtheconclusionsfromSections4.3&4.4thatCTE
AppendixF,wefurtherexperimentwithapplying CTE to improvesSHAPandSAGE. Anotherinsightisthat,onaver-
improvetheestimationofG-LIME(Lietal.,2023)explain- age,CTEprovideslessimprovementsoveri.i.d. sampling
ingthepredictionsofaDistilBERTlanguagemodeltrained whenconsideringcompressingforegrounddatainSAGE.
ontheIMDBdatasetforsentimentanalysis.
Conclusion form the experiments. In Figure 8, we ag-
gregated results from the benchmark discussed in Sec-
tions4.5&4.6toconcludethemainclaimthatCTEoffers
2–3×improvementsinefficiencyoveri.i.d. sampling.
7
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: california_housing sample compress cluster
1e 2 KernelSHAP 1e 2 PermutationSAGE 1e 2PermutationSAGE (fg.) 1e 1 Feature Effects
7.5
6 7.5 1.2
4 5.0 5.0 0.8
2 2.5 2.5 0.4
0 0.0 0.0 0.0
64 128 192 256 64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples N samples
Dataset: diamonds sample compress cluster
1e 2 KernelSHAP 1e 2 PermutationSAGE 1e 2PermutationSAGE (fg.) 1e 1 Feature Effects
6
3 1.2 1.5
4 2 0.8 1.0
1 0.4 2 0.5
0 0.0 0 0.0
64 128 192 256 64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples N samples
Dataset: grid_stability sample compress cluster
1e 2 KernelSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) 1e 1 Feature Effects
4.5
3.0 7.5 1.8
3.0
5.0 1.2
1.5 2.5 1.5 0.6
0.0 0.0 0.0 0.0
32 64 96 128 32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples N samples
Figure7.CTEimprovestheapproximationerroroflocalandglobalremoval-basedexplanations. SAGEisevaluatedintwovariantsthat
considereithercompressingonlythebackgrounddata(default),orusingthecompressedsamplesasbothbackgroundandforegrounddata
(asindicatedwith“fg.”).Analogousresultsfortheremainingestimatorsand27datasetsareinAppendixG.(mean±sd.)
2 3 4 5 2 3 4 5
(4x) sample (1.4) (5.4) sample compress (1.9) (5.6) sample
compress (2.4) (5.4) cluster (4x) sample (2.1) (4.5) cluster
(3x) sample (2.5) (3.9) sample (2x) (3x) sample (2.8) (4.1) sample (2x)
Figure8.Criticaldifferencediagramsofaverageranks(lowerisbetter)aggregatedover6explanationestimatorsand48dataset–model
pairs:forMAEaveragedoverrepeats(left),andforthestandarddeviationofMAEoverrepeats(right)thatcorrespondstothestabilityof
explanationestimation. CTEoftenachieveson-parexplanationapproximationerrorusing2–3×lesssamples,i.e.requiring2–3×less
modelevaluations,whichisefficient.Moreover,CTEguaranteesmorestableestimatesthani.i.d.sampling.
5.ConclusionandLimitations AlthoughwehaveshownthatthedefaultCOMPRESS++al-
gorithmisarobustbaseline,exploringthetunabilityofits
Weproposecompressthenexplainasapowerfulalternative
hyperparametersisanaturalfutureworkdirection(similarly
totheconventionalsamplethenexplainparadigminexpla-
asinthecaseofconditionalsamplingmethods,Olsenetal.,
nationestimation. CTEhasthepotentialtoimproveapprox-
2024). Especially for tabular datasets, dealing with cate-
imationerroracrossawiderangeofexplanationmethods
goricalfeaturescanbeanissue,whichweelaborateonin
forvariouspredictivetasks. Specifically,weshowaccuracy
AppendixD.2. Forsupervisedlearning,astratifiedvariant
improvementsinpopularremoval-basedexplanationsthat
ofkernelthinningtakingintoaccountadistributionofthe
marginalizefeatureinfluence,andingeneral,globalexpla-
targetfeaturecouldfurtherimproveloss-basedexplanations
nationsthataggregatelocalexplanationsoverasubsetof
likeSAGE,oreventheestimationofgroupfairnessmetrics.
data. Moreover, CTE leads to more efficient explanation
estimationbydecreasingthecomputationalresources(time,
Broaderimpact. Ingeneral,improvingexplanationmeth-
modelevaluations)requiredtoachieveerroronparwitha
odshaspositiveimplicationsforhumansinteractingwithAI
largeri.i.d. samplesize.
systems(Rongetal.,2024). But,specificallyinthecontext
Futureworkonmethodsformarginaldistributioncompres- ofthiswork,biasedsamplingcanbeexploitedtomanipu-
sionotherthankernelthinningandclusteringmightbring latetheexplanationresults(Slacketal.,2020;Baniecki&
additionalimprovementsintheperformanceofexplanation Biecek,2022;Labergeetal.,2023). CTE couldminimize
estimation. Distributioncompressionmethods,bydesign, theriskofsuchadversariesandprevent“randomseed/state
havehyperparametersthatmayimpacttheempiricalresults. hacking”basedontheratherunstablei.i.d. samplingfrom
datainempiricalresearch(Herrmannetal.,2024).
8
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Reproducibility. Code to reproduce all experiments in Chen, J., Song, L., Wainwright, M. J., and Jordan, M. I.
thispaperisavailableonGitHubathttps://github. Learningtoexplain:Aninformation-theoreticperspective
com/hbaniecki/compress-then-explain. onmodelinterpretation. InICML,2018. 1,1,2
Chen,Z.andSun,Q. Extractingclassactivationmapsfrom
Acknowledgements. Thisworkwasfinanciallysupported
non-discriminativefeaturesaswell. InCVPR,2023. A
by the Polish National Science Centre grant number
2021/43/O/ST6/00347.HubertBanieckigratefullyacknowl- Cooper,A.F.,Guo,W.,Pham,K.,Yuan,T.,Ruan,C.F.,Lu,
edgesscholarshipfundingfromthePolishNationalAgency Y.,andSa,C.D. Coordinatingdistributedexampleorders
forAcademicExchangeunderthePreludiumBisNAWA3 forprovablyacceleratedtraining. InNeurIPS,2023. 1
programme.
Covert, I. and Lee, S.-I. Improving KernelSHAP: Practi-
cal Shapley value estimation via linear regression. In
AISTATS,2021. 1,1,2
Covert, I., Lundberg, S. M., and Lee, S.-I. Understand-
ingglobalfeaturecontributionswithadditiveimportance
measures. InNeurIPS,2020. 1,2,3.2,A,D.1
Covert,I.,Lundberg,S.,andLee,S.-I.Explainingbyremov-
ing: Aunifiedframeworkformodelexplanation. Journal
References
ofMachineLearningResearch,22(209):1–90,2021. 1,2
Aas,K.,Jullum,M.,andLøland,A. Explainingindividual
Domingo-Enrich,C.,Dwivedi,R.,andMackey,L. Com-
predictionswhenfeaturesaredependent: Moreaccurate
pressthentest:Powerfulkerneltestinginnear-lineartime.
approximationstoShapleyvalues. ArtificialIntelligence,
InAISTATS,2023. 1,2
298:103502,2021. 1,2
Donnelly, J., Katta, S., Rudin, C., and Browne, E. The
Agarwal,C.,Krishna,S.,Saxena,E.,Pawelczyk,M.,John- Rashomonimportancedistribution:GettingRIDofunsta-
son,N.,Puri,I.,Zitnik,M.,andLakkaraju,H. OpenXAI: ble,singlemodel-basedvariableimportance. InNeurIPS,
Towardsatransparentevaluationofmodelexplanations. 2023. 1
InNeurIPS,2022. 4.2,D.2
Dwivedi, R. and Mackey, L. Kernel thinning. In COLT,
Agarwal,P.K.,Har-Peled,S.,andVaradarajan,K.R. Ap- 2021. 1,1,2,2.2,4,C
proximating extent measures of points. Journal of the
Dwivedi,R.andMackey,L. Generalizedkernelthinning.
ACM,51(4):606–635,2004. 1
InICLR,2022. 1,2
Apley,D.W.andZhu,J. Visualizingtheeffectsofpredictor Erion, G., Janizek, J. D., Sturmfels, P., Lundberg, S. M.,
variablesinblackboxsupervisedlearningmodels. Jour- andLee,S.-I. Improvingperformanceofdeeplearning
naloftheRoyalStatisticalSociety: SeriesB(Statistical models with axiomatic attribution priors and expected
Methodology),82(4):1059–1086,2020. 1,2,3.2,D.1 gradients. NatureMachineIntelligence, 3(7):620–631,
2021. 1,1,2,3.1,A
Baniecki, H.andBiecek, P. ManipulatingSHAPviaAd-
versarialDataPerturbations(StudentAbstract). InAAAI, Feydy,J.,Se´journe´,T.,Vialard,F.-X.,Amari,S.-i.,Trouve,
2022. 5 A.,andPeyre´,G. Interpolatingbetweenoptimaltransport
andMMDusingsinkhorndivergences.InAISTATS,2019.
Baniecki,H.,Casalicchio,G.,Bischl,B.,andBiecek,P. On 4.1
theRobustnessofGlobalFeatureEffectExplanations. In
Fischer,S.F.,Feurer,M.,andBischl,B. OpenML-CTR23
ECMLPKDD,2024. 3.4
– a curated tabular regression benchmarking suite. In
AutoML,2023. 4.5,D.2
Bischl,B.,Casalicchio,G.,Feurer,M.,Gijsbers,P.,Hutter,
F.,Lang,M.,Mantovani,R.G.,vanRijn,J.N.,andVan-
Fumagalli,F.,Muschalik,M.,Kolpaczki,P.,Hu¨llermeier,
schoren,J. OpenMLbenchmarkingsuites. InNeurIPS,
E.,andHammer,B. SHAP-IQ:Unifiedapproximationof
2021. 4.5,D.2
any-orderShapleyinteractions. InNeurIPS,2023. 1,1,2
Chen, H., Covert, I. C., Lundberg, S. M., and Lee, S.-I. Ghalebikesabi,S.,Ter-Minassian,L.,DiazOrdaz,K.,and
AlgorithmstoestimateShapleyvaluefeatureattributions. Holmes,C.C. Onlocalityoflocalexplanationmodels.
NatureMachineIntelligence,5(6):590–601,2023. 1 InNeurIPS,2021. 1,1,A
9EfficientandAccurateExplanationEstimationwithDistributionCompression
Gibbs, A. L. and Su, F. E. On choosing and bounding priors. Artificial Intelligence, 314:103823, 2023. 1, 2,
probabilitymetrics. InternationalStatisticalReview,70 4.5,F
(3):419–435,2002. 4.1
Lin, C., Covert, I., and Lee, S.-I. On the robustness of
Gretton, A., Borgwardt, K. M., Rasch, M. J., Scho¨lkopf, removal-basedfeatureattributions. InNeurIPS,2023. 3
B., and Smola, A. A kernel two-sample test. Journal
ofMachineLearningResearch,13(1):723–773,2012. 2, Lundberg,S.M.andLee,S.-I. Aunifiedapproachtointer-
2.2,2 pretingmodelpredictions. InNeurIPS,2017. 1,2,3.1,
A,D.1
Har-Peled,S.andMazumdar,S. Oncoresetsfork-means
andk-medianclustering. InSTOC,2004. 1 Lundstrom,D.D.,Huang,T.,andRazaviyayn,M. Arigor-
ousstudyofintegratedgradientsmethodandextensions
Hase,P.,Xie,H.,andBansal,M. Theout-of-distribution tointernalneuronattributions. InICML,2022. 1,1
probleminexplainabilityandsearchmethodsforfeature
importanceexplanations. InNeurIPS,2021. 1 Moosbauer,J.,Herbinger,J.,Casalicchio,G.,Lindauer,M.,
andBischl,B. Explaininghyperparameteroptimization
Herrmann, M., Lange, F. J. D., Eggensperger, K., Casal-
viapartialdependenceplots. InNeurIPS,2021. 1,D.1
icchio, G., Wever, M., Feurer, M., Ru¨gamer, D.,
Hu¨llermeier, E., Boulesteix, A.-L., and Bischl, B. Po- Muandet, K., Fukumizu, K., Sriperumbudur, B., and
sitionpaper: Rethinkingempiricalresearchinmachine Scho¨lkopf,B.Kernelmeanembeddingofdistributions:A
learning: Addressingepistemicandmethodologicalchal- reviewandbeyond. FoundationsandTrendsinMachine
lengesofexperimentation. InICML,2024. 1,5 Learning,10(1–2):1–141,2017. 2
Jethani, N., Sudarshan, M., Covert, I. C., Lee, S.-I., and Muschalik,M.,Fumagalli,F.,Hammer,B.,andHu¨llermeier,
Ranganath,R. FastSHAP:Real-timeShapleyvalueesti- E.BeyondTreeSHAP:Efficientcomputationofany-order
mation. InICLR,2022. 1,1 Shapleyinteractionsfortreeensembles. InAAAI,2024.
1
Kim, J.-H., Kim, J., Oh, S. J., Yun, S., Song, H., Jeong,
J.,Ha,J.-W.,andSong,H.O. Datasetcondensationvia
Olsen,L.H.B.,Glad,I.K.,Jullum,M.,andAas,K. Using
efficientsynthetic-dataparameterization. InICML,2022.
Shapleyvaluesandvariationalautoencoderstoexplain
1
predictivemodelswithdependentmixedfeatures.Journal
ofMachineLearningResearch,23(213):1–51,2022. 1,
Kokhlikyan, N., Miglani, V., Martin, M., Wang, E., Al-
1,2
sallakh,B.,Reynolds,J.,Melnikov,A.,Kliushkina,N.,
Araya,C.,Yan,S.,andReblitz-Richardson,O. Captum:
Olsen,L.H.B.,Glad,I.K.,Jullum,M.,andAas,K. Acom-
Aunifiedandgenericmodelinterpretabilitylibraryfor
parativestudyofmethodsforestimatingmodel-agnostic
PyTorch. arXivpreprintarXiv:2009.07896,2020. D.1
Shapleyvalueexplanations.DataMiningandKnowledge
Discovery,pp.1–48,2024. 1,2,5
Kolpaczki,P.,Bengs,V.,Muschalik,M.,andHu¨llermeier,
E. ApproximatingtheShapleyvaluewithoutmarginal
Owen, A. B. Statistically efficient thinning of a markov
contributions. InAAAI,2024. 1
chainsampler. JournalofComputationalandGraphical
Krzyzin´ski, M., Spytek, M., Baniecki, H., and Biecek, P. Statistics,26(3):738–744,2017. 1
SurvSHAP(t): Time-dependentexplanationsofmachine
Petsiuk, V., Das, A., andSaenko, K. RISE:Randomized
learning survival models. Knowledge-Based Systems,
inputsamplingforexplanationofblack-boxmodels. In
262:110234,2023. 2
BMVC,2018. 1
Laberge, G., Aivodji, U., Hara, S., and Mario Marchand,
Ribeiro,M.T.,Singh,S.,andGuestrin,C. “WhyshouldI
F.K. FoolingSHAPwithStealthilyBiasedSampling. In
trustyou?” Explainingthepredictionsofanyclassifier.
ICLR,2023. 1,4.1,5,A
InKDD,2016. 2,F
Li, J., Nagarajan, V., Plumb, G., and Talwalkar, A. A
Rong, Y., Leemann, T., Nguyen, T.-T., Fiedler, L., Qian,
learningtheoreticperspectiveonlocalexplainability. In
P.,Unhelkar,V.,Seidel,T.,Kasneci,G.,andKasneci,E.
ICLR,2021. 1
Towardshuman-centeredexplainableAI:Asurveyofuser
Li, X., Xiong, H., Li, X., Zhang, X., Liu, J., Jiang, H., studies formodel explanations. IEEE Transactionson
Chen,Z.,andDou,D. G-LIME:Statisticallearningfor PatternAnalysisandMachineIntelligence,46(4):2104–
localinterpretationsofdeepneuralnetworksusingglobal 2122,2024. 5
10EfficientandAccurateExplanationEstimationwithDistributionCompression
Scholbeck, C. A., Molnar, C., Heumann, C., Bischl, B.,
andCasalicchio,G. Sampling,intervention,prediction,
aggregation:ageneralizedframeworkformodel-agnostic
interpretations. InECMLPKDD,2020. 1
Sener,O.andSavarese,S. Activelearningforconvolutional
neuralnetworks: Acore-setapproach. InICLR,2018. 1
Shetty,A.,Dwivedi,R.,andMackey,L. Distributioncom-
pression in near-linear time. In ICLR, 2022. 1, 2, 2,
4
Slack,D.,Hilgard,S.,Jia,E.,Singh,S.,andLakkaraju,H.
Fooling LIME and SHAP: Adversarial attacks on post
hocexplanationmethods. InAIES,2020. 5
Slack, D., Hilgard, A., Singh, S., and Lakkaraju, H. Re-
liable post hoc explanations: Modeling uncertainty in
explainability. InNeurIPS,2021. 1,1
Sriperumbudur, B. K., Fukumizu, K., Gretton, A.,
Scho¨lkopf,B.,andLanckriet,G.R.G. Onintegralprob-
abilitymetrics,ϕ-divergencesandbinaryclassification.
arXivpreprintarxiv:0901.2698,2009. 3.3,3
Sriperumbudur, B. K., Gretton, A., Fukumizu, K.,
Scho¨lkopf, B., and Lanckriet, G. R. Hilbert space em-
beddingsandmetricsonprobabilitymeasures. Journal
ofMachineLearningResearch,11(50):1517–1561,2010.
3.6
Strumbelj,E.andKononenko,I. Anefficientexplanation
ofindividualclassificationsusinggametheory. Journal
ofMachineLearningResearch,11:1–18,2010. 1
VanLooveren,A.andKlaise,J. Interpretablecounterfactual
explanations guided by prototypes. In ECML PKDD,
2021. A
Wang,T.,Zhu,J.-Y.,Torralba,A.,andEfros,A.A. Dataset
distillation. arXivpreprintarXiv:1811.10959,2018. 1
Zhao,B.,Mopuri,K.R.,andBilen,H.Datasetcondensation
withgradientmatching. InICLR,2021. 1
Zimmerman,N.,Giusti,A.,andGuzzi,J. Resource-aware
collaborativemontecarlolocalizationwithdistribution
compression. arXivpreprintarXiv:2404.02010,2024. 1
11EfficientandAccurateExplanationEstimationwithDistributionCompression
Appendixfor“EfficientandAccurateExplanationEstimationwithDistributionCompression”
InAppendixB,wederiveaproofforProposition3.5. AppendixCprovidescodelistingsforSHAP,EXPECTED-GRADIENTS
and FEATURE-EFFECTS,analogoustoListing1for SAGE. Additionaldetailsontheexperimentalsetupareprovidedin
AppendixD.AppendicesE,F&Greportresultsfortheremainingdatasets.
A.Motivation: standardi.i.d. samplinginexplanationestimation
We find that i.i.d. sampling from datasets is a heuristic often used (and overlooked) in various estimators of post-hoc
explanations. Ourworkaimstofirstquantifytheapproximationerrorintroducedbysamplethenexplain,andthenproposea
methodtoefficientlyreduceit. Belowareafewexamplesfromtheliteratureonexplainabilitythatmotivatetheshifttoour
introducedcompressthenexplainparadigm.
In(Labergeetal.,2023),weread“Forinstance,whenadatasetisusedtorepresentabackgrounddistribution,explainersin
theSHAPlibrarysuchastheExactExplainerandTreeExplainerwillsubsamplethisdatasetbyselecting100instances
uniformlyatrandomwhenthesizeofthedatasetexceeds100.”
In(Chen&Sun,2023),weread“[Footnote1.] Weusearandomsubsetofsamplesforeachclassintherealimplementation,
toreducethecomputationcostsofclustering.”
In(Ghalebikesabietal.,2021),weread“AftertrainingaconvolutionalneuralnetworkontheMNISTdataset,weexplain
digits with the predicted label 8 given a background dataset of 100 images with labels 3 and 8.”, as well as “Feature
attributionsaresortedbysimilarityaccordingtoapreliminaryPCAanalysisacrossasubsetof2000samplesfromthe
AdultIncomedataset,using2000referencepoints.”
In(Erionetal.,2021),weread“Duringtraining,weletkbethenumberofsampleswedrawtocomputeexpectedgradients
foreachmini-batch. and“Thisexpectation-basedformulationlendsitselftoanatural, samplingbasedapproximation
method: (1)drawsamplesofx′fromthetrainingdataset[...],(2)computethevalueinsidetheexpectationforeachsample
and(3)averageoversamples.”
In(VanLooveren&Klaise,2021),weread“Wealsoneedarepresentative,unlabeledsampleofthetrainingdataset.”,and
inAlgorithms1and2: “AsampleX ={x ,...,x }fromtrainingset.”
1 n
In(Covertetal.,2020),weread“Whencalculatingfeatureimportance,oursamplingapproximationforSAGE(Algorithm
1)wasrunusingdrawsfromthemarginaldistribution. Weusedafixedsetof512backgroundsamplesforthebank,bike
andcreditdatasets,128forMNIST,andall334trainingexamplesforBRCA.”
IntheshapPythonpackage(Lundberg&Lee,2017),thereisawarningsaying“Using110backgrounddatasamplescould
causeslowerruntimes. Considerusingshap.sample(data,K)orshap.kmeans(data,K)tosummarizethebackgroundasK
samples.”,andthedocumentationmentions“Forsmallproblems,thisbackgrounddatasetcanbethewholetrainingset,but
forlargerproblemsconsiderusingasinglereferencevalueorusingthekmeansfunctiontosummarizethedataset.”
12EfficientandAccurateExplanationEstimationwithDistributionCompression
B.Proof
Below,wederiveaproofforProposition3.5.
PropositionB.1(Globalexplanationisboundedbytotalvariationdistancebetweendatasamples). Fortwodistributions
(cid:13) (cid:13)
qX,q X(cid:101), we have (cid:13)G(qX;f,g)−G(q X(cid:101);f,g)(cid:13)
1
≤ C
g
·TV(qX,q X(cid:101)), where C
g
denotes a constant that bounds the local
explanationfunctiong.
Proof. Fortwoprobabilitydistributionsp ,q ,wehave
X X
(cid:13) (cid:13)G(p X;f,g)−G(q X;f,g)(cid:13) (cid:13)
1
=(cid:13) (cid:13)E X∼pX[g(X;f,·)]−E X∼qX[g(X;f,·)](cid:13) (cid:13)
1
(2)
(cid:13)(cid:90) (cid:90) (cid:13)
(cid:13) (cid:13)
=(cid:13) g(x)p X(x)dx− g(x)q X(x)dx(cid:13) (3)
(cid:13) (cid:13)
1
(cid:13)(cid:90) (cid:13)
(cid:13) (cid:0) (cid:1) (cid:13)
=(cid:13) g(x) p X(x)−q X(x) dx(cid:13) (4)
(cid:13) (cid:13)
1
(cid:90)
(cid:13) (cid:0) (cid:1)(cid:13)
≤ (cid:13)g(x) p X(x)−q X(x) (cid:13) 1dx (5)
(cid:90)
(cid:13) (cid:13) (cid:12) (cid:12)
= (cid:13)g(x)(cid:13) 1·(cid:12)p X(x)−q X(x)(cid:12)dx (6)
(cid:90)
(cid:12) (cid:12)
≤ C g·(cid:12)p X(x)−q X(x)(cid:12)dx (7)
(cid:90)
(cid:12) (cid:12)
=C g· (cid:12)p X(x)−q X(x)(cid:12)dx (8)
=C ·TV(p ,q ). (9)
g X X
(cid:13) (cid:13)
Substitutingwithempiricaldistributions,wehave(cid:13)G(qX;f,g)−G(q X(cid:101);f,g)(cid:13)
1
≤C g·TV(qX,q X(cid:101)).
13EfficientandAccurateExplanationEstimationwithDistributionCompression
C.Implementing CTE inpractice
CTE is relatively simple to plug-into the current workflows for explanation estimation as shown in Listing 2 for SHAP,
Listing 3 for EXPECTED-GRADIENTS, and Listing 4 for FEATURE-EFFECTS. We use the goodpoints Python pack-
age(Dwivedi&Mackey,2021,MITlicense).
Listing2Codesnippetshowingthe3-lineplug-inofdistributioncompressionforSHAPestimation.
X, model = ...
from goodpoints import compress
ids = compress.compresspp_kt(X, kernel_type=b"gaussian", g=4)
X_compressed = X[ids]
import shap
masker = shap.maskers.Independent(X_compressed)
explainer = shap.PermutationExplainer(model.predict, masker)
explanation = explainer(X)
Listing3Codesnippetshowingtheplug-inofdistributioncompressionforEXPECTED-GRADIENTS.
X, model = ...
from goodpoints import compress
ids = compress.compresspp_kt(X, kernel_type=b"gaussian", g=4)
X_compressed = X[ids]
import captum
explainer = captum.attr.IntegratedGradients(model)
import torch
inputs = torch.as_tensor(X)
baselines = torch.as_tensor(X_compressed)
explanation = torch.mean(torch.stack([
explainer.attribute(inputs, baselines[[i]], target=1)
for i in range(baselines.shape[0])
]), dim=0)
Listing4Codesnippetshowingtheplug-inofdistributioncompressionforFEATURE-EFFECTS.
X, model = ...
from goodpoints import compress
ids = compress.compresspp_kt(X, kernel_type=b"gaussian", g=4)
X_compressed = X[ids]
import alibi
explainer = alibi.explainers.PartialDependence(predictor=model.predict)
explanation = explainer.explain(X_compressed)
14EfficientandAccurateExplanationEstimationwithDistributionCompression
D.Experimentalsetup
D.1.Explanationhyperparameters
In Section 4, we experiment with 4 explanation methods (6 estimators). Without the loss of generality, in case of
classification models, we always explain a prediction for the 2nd class. For SHAP, we use the KERNEL-SHAP and
PERMUTATION-SHAPimplementationsfromtheshapPythonpackage(Lundberg&Lee,2017,MITlicense)withdefault
hyperparameters(notably,npermutations=10inthelatter). ForSAGE,weusetheKERNEL-SAGEandPERMUTATION-
SAGEimplementationsfromthesagePythonpackage(Covertetal.,2020,MITlicense). Weusedefaulthyperparameters;
notably,across-entropylossforclassificationandmeansquarederrorforregression. For EXPECTED-GRADIENTS,we
aggregatewithmeantheintegratedgradientsexplanationsfromthecaptumPythonpackage(Kokhlikyanetal.,2020,BSD-
3license),forwhichweusedefaulthyperparameters;notably,n steps=50andmethod="gausslegendre". For
FEATURE-EFFECTS,weimplementthepartialdependencealgorithm(Apley&Zhu,2020;Moosbaueretal.,2021)ourselves
formaximumcomputationalspeedincaseof2-dimensionalplots,mimickingthepopularopen-sourceimplementations.1
We use 100 uniformly distributed grid points for 1-dimensional plots and 10×10 uniformly distributed grid points for
2-dimensionalplots.
D.2.Detailsondatasetsandmodels
Table 1 shows details of datasets from the OpenXAI (Agarwal et al., 2022, MIT license) benchmark used in Sec-
tions 4.2, 4.3 & 4.4. To each dataset, there is a pretrained neural network with an accuracy of 92% (gaussian),
85%(compas),74%(heloc),85%(adult)and93%(gmsc). Wedonotfurtherpreprocessdata;notably,featurevalues
arealreadyscaledto[0,1].
Table1. DatasetsfromOpenXAIwithn >1000usedinexperiments.
valid
Dataset n n d No. classes
train valid
gaussian 3750 1250 20 2
compas 4937 1235 7 2
heloc 7896 1975 23 2
adult 36177 9045 13 2
gmsc 81767 20442 10 2
Table2showsdetailsofdatasetsfromtheOpenML-CC18(Bischletal.,2021,BSD-3license)andOpenML-CTR23(Fischer
etal.,2023,BSD-3license)benchmarksusedinSections4.5&4.6. Wefirstsplitalldatasetsin75:25(train:validation)
ratioandleft48datasetswithn >1000forourexperiments. Forthe30smaller(d<32)datasets,wetrainanXGBoost
valid
modelwithdefaulthyperparameters(200estimators)andexplainitwith SHAP, SAGE, FEATURE-EFFECTS. Forthe18
bigger(d≥32)datasets,wetraina3-layerneuralnetworkmodelwith(128,64)neuronsinhiddenReLUlayersandexplain
itwithEXPECTED-GRADIENTS. Weperformbasicpreprocessingofdata: (1)removefeatureswithasingleornunique
values,(2)targetencodecategoricalfeatures,(3)imputemissingvalueswithmean,and(4)standardizefeatures.
Ingeneral,categoricalfeaturescanbeanissueforclusteringanddistributioncompressionalgorithms; soareformany
explanation algorithms and conditional distribution samplers. Although target encoding worked well in our setup, we
envisiontwoadditionalheuristicstodealwithcategoricalfeatures: (1)performdistributioncompressionusingadataset
restrictedtonon-categoricalfeatures,(2)targetencodecategoricalfeaturesonlyfordistributioncompression.
D.3.Computeresources
ResultsdescribedinSections4.2–4.4andFigure6werecomputedonapersonalcomputerwithanM3chipasjustifiedin
Section4.1. BenchmarkdescribedinSections4.5&4.6wascomputedonaclusterwith4AMDRome7742CPUs(256
cores)and4TBofRAMforabout12dayscombined.
1https://docs.seldon.io/projects/alibi/en/latest/api/alibi.explainers.html#alibi.
explainers.PartialDependence;https://interpret.ml/docs/python/api/PartialDependence
15EfficientandAccurateExplanationEstimationwithDistributionCompression
Table2. DatasetsfromOpenML-CC18andOpenML-CTR23withn >1000usedinexperiments.
valid
Dataset TaskID n n d No. classes
train valid
phoneme 9952 4053 1351 5 2
wilt 146820 3629 1210 5 2
cps88wages 361261 21116 7039 6 –
jungle chess 167119 33614 11205 6 3
abalone 361234 3132 1045 8 –
electricity 219 33984 11328 8 2
kin8nm 361258 6144 2048 8 –
california housing 361255 15480 5160 8 –
brazilian houses 361267 8019 2673 9 –
diamonds 361257 40455 13485 9 –
physiochemical protein 361241 34297 11433 9 –
white wine 361249 3673 1225 11 –
health insurance 361269 16704 5568 11 –
grid stability 361251 7500 2500 12 –
adult 7592 36631 12211 14 2
naval propulsion plant 361247 8950 2984 14 –
miami housing 361260 10449 3483 15 –
letter 6 15000 5000 16 26
bank-marketing 14965 33908 11303 16 2
pendigits 32 8244 2748 16 10
video transcoding 361252 51588 17196 18 –
churn 167141 3750 1250 20 2
kings county 361266 16209 5404 21 –
numerai28.6 167120 72240 24080 21 2
sarcos 361254 36699 12234 21 –
cpu activity 361256 6144 2048 21 –
jm1 3904 8163 2722 21 2
wall-robot-navigation 9960 4092 1364 24 4
fifa 361272 14383 4795 28 –
PhishingWebsites 14952 8291 2764 30 2
pumadyn32nh 361259 6144 2048 32 –
GestureSegmentation 14969 7404 2469 32 5
satimage 2074 4822 1608 36 6
texture 125922 4125 1375 40 11
connect-4 146195 50667 16890 42 3
fps benchmark 361268 18468 6156 43 –
wave energy 361253 54000 18000 48 –
theorem-proving 9985 4588 1530 51 6
spambase 43 3450 1151 57 2
optdigits 28 4215 1405 64 10
superconductivity 361242 15947 5316 81 –
nomao 9977 25848 8617 118 2
har 14970 7724 2575 561 6
isolet 3481 5847 1950 617 26
mnist 784 3573 52500 17500 784 10
Fashion-MNIST 146825 52500 17500 784 10
Devnagari-Script 167121 69000 23000 1024 46
CIFAR 10 167124 45000 15000 3072 10
16EfficientandAccurateExplanationEstimationwithDistributionCompression
E. CTE improvestheaccuracyofestimatingremoval-basedexplanations
WereportthedifferencesinMAEandTop-kbetweenCTEandi.i.d. samplinginFigure9(compas),Figure10(heloc),
Figure11(gmsc)andFigure12(gaussian). Onalltheconsideredtasks,CTEoffersanotabledecreaseinapproximation
errorofSHAPandSAGEwithneglectablecomputationaloverhead(asmeasuredbytimeinseconds).
KernelSHAP compress KernelSHAP
sample
PermutationSHAP PermutationSHAP
compress
sample
KernelSAGE KernelSAGE
PermutationSAGE PermutationSAGE
0.00 0.01 0.02 0.03 0.5 0.6 0.7 0.8 0.9 1.0
MAE Top-k
KernelSHAP KernelSHAP
PermutationSHAP PermutationSHAP
compress
explain
KernelSAGE KernelSAGE
explain (ground truth)
PermutationSAGE PermutationSAGE explain (compressed)
10 2 10 1 100 101 102 100 101 102 103
Time [s] Time [s]
Figure9.ExtendedFigure3. CTEimprovesSHAPandSAGEestimationbyusingthecompressedsamplesasbackgrounddataforthe
compasdataset.Wemeasuremeanabsoluteerror(MAE,↓)betweenfeatureattributionandimportancevalues,aswellastheprecisionin
correctlyindicatingthe3mostimportantfeatures(Top-k,↑).Computationalresourcesrequiredtocompressadistributionareneglectable
inthecontextofexplanationestimation.(mean±se.)
KernelSHAP KernelSHAP
PermutationSHAP PermutationSHAP
compress
sample
KernelSAGE KernelSAGE
compress
PermutationSAGE sample PermutationSAGE
0.000 0.002 0.004 0.006 0.008 0.5 0.6 0.7 0.8 0.9 1.0
MAE Top-k
KernelSHAP KernelSHAP
PermutationSHAP PermutationSHAP
explain (ground truth)
explain (compressed)
KernelSAGE KernelSAGE
compress
PermutationSAGE explain PermutationSAGE
10 2 10 1 100 101 102 103 101 102 103 104 105
Time [s] Time [s]
Figure10. ExtendedFigure3. CTEimprovesSHAPandSAGEestimationforthehelocdataset.
17EfficientandAccurateExplanationEstimationwithDistributionCompression
KernelSHAP compress KernelSHAP
sample
PermutationSHAP PermutationSHAP
compress
sample
KernelSAGE KernelSAGE
PermutationSAGE PermutationSAGE
0.000 0.001 0.002 0.003 0.5 0.6 0.7 0.8 0.9 1.0
MAE Top-k
KernelSHAP KernelSHAP
PermutationSHAP PermutationSHAP
explain (ground truth)
explain (compressed)
KernelSAGE KernelSAGE
compress
PermutationSAGE explain PermutationSAGE
10 1 100 101 102 103 104 102 103 104 105 106
Time [s] Time [s]
Figure11.ExtendedFigure3. CTEimprovesSHAPandSAGEestimationbyusingthecompressedsamplesasbackgrounddataforthe
gmscdataset.Wemeasuremeanabsoluteerror(MAE,↓)betweenfeatureattributionandimportancevalues,aswellastheprecisionin
correctlyindicatingthe5mostimportantfeatures(Top-k,↑).Computationalresourcesrequiredtocompressadistributionareneglectable
inthecontextofexplanationestimation.(mean±se.)
KernelSHAP KernelSHAP
PermutationSHAP PermutationSHAP
compress
sample
KernelSAGE KernelSAGE
compress
PermutationSAGE sample PermutationSAGE
0.000 0.005 0.010 0.015 0.5 0.6 0.7 0.8 0.9 1.0
MAE Top-k
KernelSHAP KernelSHAP
PermutationSHAP PermutationSHAP
explain (ground truth)
explain (compressed)
KernelSAGE KernelSAGE
compress
PermutationSAGE explain PermutationSAGE
10 2 10 1 100 101 102 103 101 102 103 104
Time [s] Time [s]
Figure12. ExtendedFigure3. CTEimprovesSHAPandSAGEestimationforthegaussiandataset.
18EfficientandAccurateExplanationEstimationwithDistributionCompression
F. CTE improvesgradient-basedexplanationsspecifictoneuralnetworks
Figure 13 shows the EXPECTED-GRADIENTS approximation error for 18 datasets. In all cases, CTE achieves on-par
approximation error using less samples than i.i.d. sampling, i.e. requiring less model evaluations, resulting in faster
computationandsavedresources.
Explanation: Expected Gradients
1e 4 isolet 1e 3 theorem-proving 1e 4 mnist_784 1e 3 satimage
3 6
3 2.4
2 4
2 1.6
1 1 0.8 2
0 0 0.0 0
32 64 96 128 32 64 96 128 128 256 384 512 32 64 96 128
1e 2 fps_benchmark 1e 4 Devnagari-Script 1e 3 nomao 1e 3 texture
1.2 1.5
3 9
0.8 1.0 2 6
0.4 0.5 1 3
0.0 0.0 0 0
64 128 192 256 128 256 384 512 64 128 192 256 32 64 96 128
1e 3 wave_energy 1e 4 har 1e 4 Fashion-MNIST 1e 3 optdigits
2.4
7.5 9
1.8
1.6 5.0 6
1.2
2.5 3 0.6 0.8
0.0 0 0.0 0.0
128 256 384 512 32 64 96 128 128 256 384 512 32 64 96 128
1e 3 connect-4 1e 4 CIFAR_10 1e 3 pumadyn32nh 1e 2 superconductivity
6
1.8 9 2.4
4 1.2 6 1.6
2 0.6 3 0.8
0 0.0 0 0.0
128 256 384 512 64 128 192 256 32 64 96 128 64 128 192 256
N samples N samples
1e 3 spambase 1e 3GestureSegmentation
6
4.5
4 Method
3.0
sample
2 1.5 compress
cluster
0 0.0
32 64 96 128 32 64 96 128
N samples N samples
Figure13.ExtendedFigure5. CTEimprovestheapproximationerrorofgradient-basedexplanationsspecifictoneuralnetworks.Itisnot
onlymoreaccurate,butalsomorestableasmeasuredwithstandarddeviation.(mean±sd.)
19
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Model-agnostic explanation of a language model. We further experimented with applying CTE to improve
the estimation of global aggregated LIME (Ribeiro et al., 2016), aka G-LIME (Li et al., 2023), which
isamorecomplexsetupthatweleaveforfuturework. WeaimtoexplainthepredictionsofaDistilBERTlanguagemodel2
trainedontheIMDBdataset3forsentimentanalysis. WecalculateLIMEwithk =10forallsamplesfromthevalidationset
usinganA100GPUandaggregatetheselocalexplanationsintoglobaltokenimportancewithameanofabsolutenormalized
values(Lietal.,2023),whichisthe“groundtruth”explanation. Wethencompressthesetwithi.i.d. sampling,CTE,and
clusteringbasedontheinputs’textembeddingsfromthemodel’slastlayer(precedingaclassifier)thathasadimension
of size 768. Figure 14 shows results for explanation approximation error and an exemplary comparison between the
explanationsrelatingtoFigure1. Toobtaintheseresults,weused8×moresamplesthanthetypicalcompressionscenario
(still25×lessthanthefullsample)soastoovercometheissueofraretokensskewingtheresults. Itbecomeschallengingto
computethedistancebetweenthegroundtruthandapproximatedexplanationsasthelattercontainssignificantlylesstokens
(features),asopposedtopreviousexperimentswherethesetwoexplanationsalwayshadequaldimensions. Thus,MAE
becomesbiasedtowardssparseexplanationsandpopulartokens,i.e. anexplanationwithasingletokenofwellapproximated
importancecouldhaveanerrorcloseto0. Forcontext,wemeasureTVbetweenthediscretedistributionsoftokensinlocal
explanationsbeforetheglobalaggregation(lowerisbetter). Wereportresultsfordifferenttokencutoffs,whereweremove
therarelyoccurringtokensfromthegroundtruthexplanation,whichsaturatesat5%tokensleft.
A) D)
B)
C)
Figure14.CTEforG-LIMEofaDistilBERTmodelclassifyingIMDBreviews.A)Itisnotobvioushowtomeasurethedistancebetween
globalexplanationscontainingdifferentsetsoftokens(Tokencoveragein%w.r.t.groundtruth,↑).Therefore,wegraduallyremoverare
tokensfromthemeasurementbasedontheiroccurrenceinthegroundtruthexplanation(Tokencutoffinquantiles).B)Measurementof
meanabsoluteerror(MAE,↓)betweenaggregatedglobalexplanations. C)Measurementoftotalvariationdistance(TV,↓)between
tokenoccurrencesinlocalexplanationsbeforeglobalaggregation. D)Weshowanexemplary“worst-case”explanation,i.e. withthe
lowestMAEforcutoff0.95wheretokencoverageisover99%,forbothCTEandi.i.d.sampling.Forthisvisualization,weonlyshowthe
importanceofthe5mostpositive/negativetokens,and5tokenswiththeimportanceclosesttozero.Explanationapproximationerroris
indicatedwithtransparentbars. Notably,i.i.d. samplingmissescontaininganyinputwithanimportanttoken“superbly”,whileCTE
misses“disgrace”.Samplingoverestimatestheglobalimportanceoftokens“disappointing”,“into”and“get”,whileCTE,forexample,
overestimates“than”andunderestimates“tedious”or“delightful”.
2https://huggingface.co/dfurman/distilbert-base-uncased-imdb
3https://huggingface.co/datasets/stanfordnlp/imdb
20EfficientandAccurateExplanationEstimationwithDistributionCompression
G.Ablations
Figures15–23reporttheexplanationapproximationerrorfor30predictivetasks.WeobservethatCTEsignificantlyimproves
theestimationofFEATURE-EFFECTSinallcases. Anotherinsightisthat,onaverage,CTEprovideslessimprovementsover
i.i.d. samplingwhenconsideringcompressingforegrounddatainSAGE.
Dataset: california_housing
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
6 7.5 7.5 1.2
4 5.0 5.0 0.8
2 2.5 2.5 0.4
0 0.0 0.0 0.0
64 128 192 256
1e 2 PermutationSHAP 1e 2 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
7.5
7.5
6
4 5.0 5.0 Method
sample
2 2.5 2.5 compress
0 0.0 0.0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Dataset: diamonds
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
3 1.8 6 1.5
2 1.2 4 1.0
1 0.6 2 0.5
0 0.0 0 0.0
64 128 192 256
1e 2 PermutationSHAP 1e 2 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
6
3 1.2
2 0.8 4 Method
sample
1 0.4 2 compress
0 0.0 0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Dataset: grid_stability
1e 2 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
4.5
9
3.0 1.8
6 3.0 1.2
1.5 3 1.5 0.6
0.0 0 0.0 0.0
32 64 96 128
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
4.5
7.5
3.0
5.0 3.0 Method
1.5 2.5 1.5 s ca om mp pl re ess
0.0 0.0 0.0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Figure15.ExtendedFigure7(1/10). CTE improvestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
21
EAM
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: abalone
1e 2 KernelSHAP 1e 1 KernelSAGE 1e 1 KernelSAGE (fg.) 1e 1 Feature Effects
1.2
7.5 1.2 1.5
5.0 0.8 0.8 1.0
2.5 0.4 0.4 0.5
0.0 0.0 0.0 0.0
32 64 96 128
1e 2 PermutationSHAP 1e 1 PermutationSAGE 1e 1PermutationSAGE (fg.) N samples
1.2 1.2
7.5
5.0 0.8 0.8 Method
sample
2.5 0.4 0.4 compress
0.0 0.0 0.0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: adult
1e 3 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
2.4 4.5
6 1.2
4 1.6 0.8 3.0
2 0.8 0.4 1.5
0 0.0 0.0 0.0
64 128 192 256
1e 3 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
1.8 1.2
6
4 1.2 0.8 Method
sample
2 0.6 0.4 compress
0 0.0 0.0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Dataset: bank-marketing
1e 3 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
6 4.5
2.4 1.2
4 1.6 0.8 3.0
2 0.8 0.4 1.5
0 0.0 0.0 0.0
64 128 192 256
1e 3 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
6 1.2
1.2
4 0.8 0.8 Method
sample
2 0.4 0.4 compress
0 0.0 0.0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Figure16.ExtendedFigure7(2/10). CTE improvestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
22
EAM
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: brazilian_houses
1e 1 KernelSHAP 1e 1 KernelSAGE 1e1 KernelSAGE (fg.) 1e 1 Feature Effects
1.2 1.8 3.0 9
0.8 1.2 6
1.5
0.4 0.6 3
0.0 0.0 0.0 0
32 64 96 128
1e 1 PermutationSHAP 1e 1 PermutationSAGE 1e1 PermutationSAGE (fg.) N samples
1.2 1.5 3
0.8 1.0 2 Method
sample
0.4 0.5 1 compress
0.0 0.0 0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: churn
1e 3 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
1.2
7.5 1.8 7.5
0.8
5.0 1.2 5.0
2.5 0.4 0.6 2.5
0.0 0.0 0.0 0.0
32 64 96 128
1e 3 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
7.5 9 1.8
5.0 6 1.2 Method
sample
2.5 3 0.6 compress
0.0 0 0.0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: cps88wages
1e 2 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
4.5 9 7.5 1.2
3.0 6 5.0 0.8
1.5 3 2.5 0.4
0.0 0 0.0 0.0
64 128 192 256
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
4.5 9 7.5
3.0 6 5.0 Method
sample
1.5 3 2.5 compress
0.0 0 0.0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Figure17.ExtendedFigure7(3/10). CTE improvestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
23
EAM
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: cpu_activity
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 1 KernelSAGE (fg.) 1e 1 Feature Effects
4.5 6 1.5 7.5
3.0 4 1.0 5.0
1.5 2 0.5 2.5
0.0 0 0.0 0.0
32 64 96 128
1e 2 PermutationSHAP 1e 2 PermutationSAGE 1e 1PermutationSAGE (fg.) N samples
1.2
4.5 4.5
3.0 3.0 0.8 Method
sample
1.5 1.5 0.4 compress
0.0 0.0 0.0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: jungle_chess
1e 2 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
2.4
1.5 6 3.0
1.6
1.0 4
0.5 2 0.8 1.5
0.0 0 0.0 0.0
64 128 192 256
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
2.4
1.8
6
1.2 1.6 Method
4
sample
0.6 2 0.8 compress
0.0 0 0.0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Dataset: electricity
1e 2 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
1.5 9 4.5
2.4
1.0 6 1.6 3.0
0.5 3 0.8 1.5
0.0 0 0.0 0.0
64 128 192 256
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
1.5 4.5 2.4
1.0 3.0 1.6 Method
sample
0.5 1.5 0.8 compress
0.0 0.0 0.0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Figure18.ExtendedFigure7(4/10). CTE improvestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
24
EAM
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: fifa
1e 3 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
1.2 6
1.2
7.5
0.8 4 5.0 0.8
2.5 0.4 2 0.4
0.0 0.0 0 0.0
64 128 192 256
1e 3 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
2.4 4.5
9
6 1.6 3.0 Method
sample
3 0.8 1.5 compress
0 0.0 0.0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Dataset: health_insurance
1e 2 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
1.8 7.5 3 7.5
1.2 5.0 2 5.0
0.6 2.5 1 2.5
0.0 0.0 0 0.0
64 128 192 256
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
1.8 4.5 2.4
1.2 3.0 1.6 Method
sample
0.6 1.5 0.8 compress
0.0 0.0 0.0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Dataset: jm1
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
2.4 1.5 2.4 3
1.6 1.0 1.6 2
0.8 0.5 0.8 1
0.0 0.0 0.0 0
32 64 96 128
1e 2 PermutationSHAP 1e 2 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
1.5 2.4
2.4
1.6 1.0 1.6 Method
sample
0.8 0.5 0.8 compress
0.0 0.0 0.0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Figure19.ExtendedFigure7(5/10). CTE improvestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
25
EAM
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: kin8nm
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
6
1.2
4.5 1.2
3.0 0.8 4 0.8
1.5 0.4 2 0.4
0.0 0.0 0 0.0
32 64 96 128
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
6
9
4.5
4 3.0 6 Method
1.5 3 2 s ca om mp pl re ess
0.0 0 0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: kings_county
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
2.4
1.2 6 3
1.6 0.8 4 2
0.8 0.4 2 1
0.0 0.0 0 0
64 128 192 256
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
2.4 9 6
1.6 6 4 Method
sample
0.8 3 2
compress
0.0 0 0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Dataset: miami_housing
1e 2 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
6 1.8
1.8 7.5
1.2 5.0 4 1.2
0.6 2.5 2 0.6
0.0 0.0 0 0.0
32 64 96 128
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
6 6
1.8
4 4 Method
1.2
sample
0.6 2 2 compress
0.0 0 0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Figure20.ExtendedFigure7(6/10). CTE improvestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
26
EAM
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: naval_propulsion_plant
1e 1 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
2.4
1.2 2.4 9
1.6 0.8 1.6 6
0.4 0.8 3 0.8
0.0 0.0 0 0.0
32 64 96 128
1e 1 PermutationSHAP 1e 2 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
1.2 2.4 9
0.8 1.6 6 Method
sample
0.4 0.8 3
compress
0.0 0.0 0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: pendigits
1e 3 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
7.5 1.2 1.5 6
5.0 0.8 1.0 4
2.5 0.4 0.5 2
0.0 0.0 0.0 0
32 64 96 128
1e 3 PermutationSHAP 1e 2 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
7.5 1.2 1.5
5.0 0.8 1.0 Method
sample
2.5 0.4 0.5 compress
0.0 0.0 0.0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: numerai28.6
1e 3 KernelSHAP 1e 4 KernelSAGE 1e 3 KernelSAGE (fg.) 1e 2 Feature Effects
3 1.2 4.5 1.5
2 0.8 3.0 1.0
1 0.4 1.5 0.5
0 0.0 0.0 0.0
PermutationSAGE (fg.) 128 256 384 512
1e 3 PermutationSHAP 1e 4 PermutationSAGE 1e 3 N samples
4.5 1.2 4.5
3.0 0.8 3.0 Method
sample
1.5 0.4 1.5
compress
0.0 0.0 0.0 cluster
128 256 384 512 128 256 384 512 128 256 384 512
N samples N samples N samples
Figure21.ExtendedFigure7(7/10). CTE improvestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
27
EAM
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: phoneme
1e 2 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
2.4 7.5 4.5 6
1.6 5.0 3.0 4
0.8 2.5 1.5 2
0.0 0.0 0.0 0
32 64 96 128
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
7.5 4.5
2.4
1.6 5.0 3.0 Method
sample
0.8 2.5 1.5 compress
0.0 0.0 0.0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: physiochemical_protein
1e 2 KernelSHAP 1e 1 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
7.5 1.5 6 3
5.0 1.0 4 2
2.5 0.5 2 1
0.0 0.0 0 0
64 128 192 256
1e 2 PermutationSHAP 1e 1 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
9 1.5 6
6 1.0 4 Method
sample
3 0.5 2 compress
0 0.0 0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Dataset: sarcos
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
4.5
1.5 1.8 1.8
1.0 1.2 3.0 1.2
0.5 0.6 1.5 0.6
0.0 0.0 0.0 0.0
64 128 192 256
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
1.8 4.5 3
1.2 3.0 2 Method
sample
0.6 1.5 1
compress
0.0 0.0 0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Figure22.ExtendedFigure7(8/10). CTE improvestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
28
EAM
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: wall-robot-navigation
1e 3 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
6 9 1.5 1.2
4 6 1.0 0.8
2 3 0.5 0.4
0 0 0.0 0.0
32 64 96 128
1e 3 PermutationSHAP 1e 3 PermutationSAGE 1e 3PermutationSAGE (fg.) N samples
6 3 9
4 2 6 Method
sample
2 1 3 compress
0 0 0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: white_wine
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
2.4
4.5 1.2 6
3.0 0.8 4 1.6
1.5 0.4 2 0.8
0.0 0.0 0 0.0
32 64 96 128
1e 2 PermutationSHAP 1e 2 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
4.5 1.2 6
3.0 0.8 4 Method
sample
1.5 0.4 2
compress
0.0 0.0 0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Dataset: wilt
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
6 6
6
3.0 4 4
4
1.5 2 2 2
0.0 0 0 0
32 64 96 128
1e 2 PermutationSHAP 1e 2 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
6
6
3.0 4 4 Method
1.5 2 2 s ca om mp pl re ess
0.0 0 0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Figure23.ExtendedFigure7(9/10). CTE improvestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
29
EAM
EAM
EAM
EAM
EAM
EAMEfficientandAccurateExplanationEstimationwithDistributionCompression
Dataset: video_transcoding
1e 2 KernelSHAP 1e 2 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
1.2
1.8 4.5 1.5
0.8 1.2 3.0 1.0
0.6 0.4 1.5 0.5
0.0 0.0 0.0 0.0
128 256 384 512
1e 2 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
4.5 4.5
1.8
1.2 3.0 3.0 Method
sample
0.6 1.5 1.5 compress
0.0 0.0 0.0 cluster
128 256 384 512 128 256 384 512 128 256 384 512
N samples N samples N samples
Dataset: letter
1e 3 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 2 Feature Effects
1.2 2.4
3.0 3
2 0.8 1.6
1.5
1 0.4 0.8
0.0 0 0.0 0.0
64 128 192 256
1e 3 PermutationSHAP 1e 3 PermutationSAGE 1e 2PermutationSAGE (fg.) N samples
4.5
3 1.2
3.0 2 0.8 Method
sample
1.5 1 0.4 compress
0.0 0 0.0 cluster
64 128 192 256 64 128 192 256 64 128 192 256
N samples N samples N samples
Dataset: PhishingWebsites
1e 3 KernelSHAP 1e 3 KernelSAGE 1e 2 KernelSAGE (fg.) 1e 1 Feature Effects
7.5 6 1.2 1.2
5.0 4 0.8 0.8
2.5 2 0.4 0.4
0.0 0 0.0 0.0
32 64 96 128
1e 3 PermutationSHAP 1e 3 PermutationSAGE 1e 3PermutationSAGE (fg.) N samples
7.5 9
2.4
5.0 1.6 6 Method
sample
2.5 0.8 3 compress
0.0 0.0 0 cluster
32 64 96 128 32 64 96 128 32 64 96 128
N samples N samples N samples
Figure24.ExtendedFigure7(10/10). CTEimprovestheexplanationapproximationerrorofvariouslocalandglobalremoval-based
explanations. SAGE is evaluated in two variants that consider either compressing only the background data (default), or using the
compressedsamplesasbothbackgroundandforegrounddata(asindicatedwith“fg.”).(mean±sd.)
30
EAM
EAM
EAM
EAM
EAM
EAM