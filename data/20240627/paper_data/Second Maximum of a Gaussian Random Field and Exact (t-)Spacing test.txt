Second Maximum of a Gaussian Random Field
and Exact (t-)Spacing test
Jean-Marc Azaïs•, Federico Dalmao◦ and Yohann De Castro⋆,†
•Institut de Mathématiques de Toulouse
Université Paul Sabatier, France
◦DMEL, CENUR Litoral Norte
Universidad de la República, Salto, Uruguay
⋆Institut Camille Jordan, CNRS UMR 5208
École Centrale Lyon, France
†Institut Universitaire de France (IUF)
Abstract: In this article, we introduce the novel concept of the second maximum of a
Gaussian random field on a Riemannian submanifold. This second maximum serves as a
powerful tool for characterizing the distribution of the maximum. By utilizing an ad-hoc
KacRiceformula,wederivetheexplicitformofthemaximum’sdistribution,conditioned
onthesecondmaximumandsomeregressedcomponentoftheRiemannianHessian.This
approachresultsinanexacttest,basedontheevaluationofspacingbetweenthesemaxima,
whichwerefertoasthespacingtest.
WeinvestigatetheapplicabilityofthistestindetectingsparsealternativeswithinGaus-
siansymmetrictensors,continuoussparsedeconvolution,andtwo-layeredneuralnetworks
with smooth rectifiers. Our theoretical results are supported by numerical experiments,
whichillustratethecalibrationandpoweroftheproposedtests.Moregenerally,thistest
can be applied to any Gaussian random field on a Riemannian manifold, and we provide
a general framework for the application of the spacing test in continuous sparse kernel
regression.
Furthermore, when the variance-covariance function of the Gaussian random field is
known up to a scaling factor, we derive an exact Studentized version of our test, coined
thet-spacingtest.Thistestisperfectlycalibratedunderthenullhypothesisandhashigh
powerfordetectingsparsealternatives.
MSC2020 subject classifications: Primary 62E15, 62F03, 60G15, 62H10, 62H15; sec-
ondary60E05,60G10,62J05,94A08.
Keywordsandphrases:Gaussianrandomfield,Secondmaximum,Spacingtest,Kernel
regression,TensorPCA.
Version of June 27, 2024
1. Introduction
1.1. The (t-)spacing test and the second maximum
Thispaperintroducesatestforthemeanm(·)ofaGaussianrandomfieldZ(·),definedonaC2-
compactRiemannianmanifoldM ofdimensiondwithoutboundary,whichcanbedecomposedas
∀t∈M, Z(t)=m(t)+σX(t), (1.1)
where m(·) is any C2-function, σ >0 is the standard deviation, and X(·) is a centered Gaussian
random field such that Var[X(t)] = 1 for every t ∈ M. We consider the simple global null
hypothesis H : “m(·)=0”, even when the standard error σ is unknown. The test investigated
0
is the quantile α of the maximum λ under the null defined as
(cid:98) 1
λ =max{σX(t)} (1.2)
1
t∈M
and we denote by t ∈ M its argument maximum. This quantile is built from the distribution
1
ofλ conditionaltothevaluesoftheso-calledsecondmaximumλ andtheso-calledIndependent
1 2
part of the Riemannian Hessian Ω, under the null. Suppose, in a first step, that σ is known,
in such a case we will demonstrate that the cumulative distribution of this conditional law can
arXiv:math.ST:
1
4202
nuJ
62
]TS.htam[
1v79381.6042:viXrabe expressed as a ratio, resulting in the following expression derived from an ad-hoc Kac-Rice
formula,
(cid:90) +∞
det(uId−Ω/σ)φ(u)du
α:= λ1/σ , (1.3)
(cid:98) (cid:90) +∞
det(uId−Ω/σ)φ(u)du
λ2/σ
where φ(·) is the standard Gaussian density and Id the identity matrix.
We will show the exactness of this test, meaning that α is uniformly distributed on the
(cid:98)
interval(0,1)underthenullhypothesis.Itdemonstratesthatoneminustheratio(1.3)represents
the cumulative distribution function (CDF) of the law of λ conditional to λ and Ω, under the
1 2
null hypothesis. Small values of α indicate that the maximum λ is abnormally large compared
(cid:98) 1
tothevalueofthesecondmaximumλ ,makingαinterpretableasap-value.Thistestisreferred
2 (cid:98)
to as the “spacing test”. The second maximum is defined as
λ ∈arg max {σX|t1(s)}, (1.4)
2
s̸=t1∈M
X(s)−c(s,t)X(t)−∇ c(s,t)⊤Λ−1(t)∇X(t)
with, for s̸=t, X|t(s):= t 2
1−c(s,t)
where c(·,·) is the variance-covariance function of X(·) and Λ (t) := Var[∇X(t)], the formal
2
definition will be given in Section 2.1. Note that X|t(s) is a normalisation of the remainder of
theregressionofX(s)w.r.t.(X(t),∇X(t)).Anexampleofthesecondmaximumintherank-one
tensor detection case, referred to as Spiked tensor PCA, is illustrated in Figure 1 and developed
in Section 5.2.
Figure 1. [Spiked tensor PCA, Section 5.2, example 1/4] The first eigenvector (resp. first eigenvalue)
of a Gaussian symmetric tensor (arrow in the left panel) is the arg maximum (resp. maximum, λ1) of some
GaussianhomogeneouspolynomialX(·).Thesecondeigenvector(resp.secondmaximum,λ2),representedbyan
arrowinthemiddlepanel,istheargmaximumofX|t1(·),someregressionofX(·).AvolumicviewofX|t1(·)is
givenintherightpanel,theheightofthesurfacearoundthesphereisgivenbythevalueoftherandomfield.We
witnessthatasingularity,referredtoasthehelix,appearsatpointt1.Athoroughfullystudyofthishelixwillbe
given in this paper.
Furthermore, we will show how to achieve the same result when the variance σ2 is unknown
usinganestimationσ (cid:98)2 builtfromtheKarhunen-LoèveexpansionofX|t1(·),seetherightpanelof
Figure1foranillustrationofthisrandomfieldinthecaseofrank-onetensordetection.Werefer
tothistestasthet-spacingtestwhichdetectsabnormallylargespacingbetweenλ /σ andλ /σ.
1 (cid:98) 2 (cid:98)
Note that λ and λ are the same for the spacing and the t-spacing test, and these values
1 2
can be computed without knowing σ. These results are supported by numerical experiments as
illustrated in Figure 2.
1.2. Detecting one sparse alternatives in continuous sparse kernel regression
OurtestisperfectlycalibratedforallC2-meanm(·).But,weexpectthistesttohavehighpower
on s-sparse alternatives of the form
s
(cid:88)
m(·)= λ c(·,t ),
0,k 0,k
k=1
2Figure 2. [Spiked tensor PCA, Section 5.2, example 2/4] The CDFs of the p-value of (t-)spacing tests
over 250,000 Monte-Carlo samples for each value of γ. Note that these tests are perfectly calibrated under the
null hypothesis, for which γ=0. The parameter γ is a scaling factor of eigenvalue of the rank-one tensor to be
detected. The value γ = 1 corresponds to the so-called phase transition in Spiked tensor PCA as presented in
Perry et al. (2020, Theorem 1.3). In the t-spacing test, the variance σ has been estimated on X|t1(·).
and we denote one-sparse alternatives by
H (t ,λ ) : “m(·)=λ c(·,t )”, (1.5)
1 0 0 0 0
for some λ ̸= 0 and t ∈ M unknown. Indeed, we will show that the maxima λ and λ
0 0 1 2
correspond to the so-called knots of the Continuous LARS algorithm and are then related to
continuouskernelsparseregression,seeSection1.3.3.WeillustratethisphenomenoninFigure3
in the case of Spiked tensor PCA.
Figure 3.[Spiked tensor PCA, Section 5.2, example 3/4]ThePDFsofthemaximaλ1,λ2 andtheCDF
of the distance d(t0,t1) over 250,000 √Monte-Carlo samples for each value of the parameter γ. The alternative
is given by t0 fixed and λ0 = γ×σ 3log3+3loglog3 ≃ 0.684γσ where γ = 1 corresponds to the so-called
phase transition in Spiked tensor PCA as presented in Perry et al. (2020, Theorem 1.3). The distance d(t0,t1)
isnormalizedsothatitisuniformlydistributedon(0,1)ift1 isuniformlydistributedonthesphere(e.g.,γ=0).
√
InFigure3,thealternativeisgivenbyλ =γ×σ 3log3+3loglog3whereγ =1corresponds
0
totheso-calledphasetransitioninSpikedtensorPCAaspresentedinPerryetal.(2020,Theorem
1.3).Thetopleftpanelshowsthatλ isstochasticallyincreasingasγincreaseswhilethetopright
1
panel shows that the distribution of λ remains unchanged for moderate values of γ, say γ ≥2.
2
It illustrates that the spacing between λ and λ grows linearly with γ. In the moderate regime,
1 2
3the bottom left panel shows that λ is distributed as a Gaussian with mean λ and variance
1 0
σ2 (dashed black line), which is the distribution of σX(t ) under H (t ,λ ). The bottom right
0 1 0 0
panel shows that t ≃ t , for moderate values of γ. It illustrates that (t ,λ ) is weakly close to
1 0 1 1
the distribution of (t ,σX(t )), the (t-)spacing tests detect the alternative H (t ,λ ). We leave
0 0 1 0 0
the proof of this phenomenon for future work.
1.3. A general framework related to continuous kernel regression
1.3.1. The four conditions on the Gaussian random field
We start by giving the assumptions of the Gaussian random field X(·) that we will invoke along
this paper. We consider a real valued Gaussian random field X(·) defined on a C2 compact
Riemannian manifold M of dimension d without boundary. We recall that c(·,·):M ×M →R
denotes its variance-covariance function.
Assumption (A ) We assume that
1-4
◦ the paths of X(·) are C2 almost surely; (A )
1
◦ E[X(t)]=0 and Var[X(t)]=1, for every t∈M; (A )
2
◦ ∀s̸=t, c(s,t)<1; (A )
3
◦ for every t∈M, the gradient ∇X(t) has a non-degenerate Gaussian distribution. (A )
4
Thevariance-covariancematrixoftheGaussiantangentvector∇X(t)isdenotedbyΛ (t),which
2
is invertible by Assumption (A ).
4
1.3.2. A continuous regression framework for the spacing test
General framework We consider the following regression problem given by
Y =λ ψ +σW , (1.6)
0 t0
where λ ∈ R, t ∈ M are unknown parameters; σ is the standard deviation; (E,⟨·,·⟩ ) is any
0 0 E
Euclidean space; ψ : t ∈ M (cid:55)→ ψ ∈ E; W ∈ E is a centered Gaussian vector with variance-
t
covariance matrix Id . The feature map ψ satisfies the following Assumptions (B ):
E 1-6
◦ ψ is a C3-function; (B )
1
◦ ∀t∈M, ∥ψ ∥2 =1; (B )
t E 2
◦ ∀t∈M, ∀s∈M \{t}, ⟨ψ ,ψ ⟩ <1; (B )
s t E 3
◦ ∀t∈M, Jψ Jψ ⊤ has full rank d; (B )
t t 4
◦ the span of {ψ : t∈M} has dimension m such that m>d+1; (B )
t 5
◦ ∀t∈M, ∃s∈M s.t. ψ =−ψ ; (B )
t s 6
where Jψ is the Jacobian matrix of ψ and Jψ ⊤ its transpose. In the next paragraph, we will
t t t
see that X(t)=⟨W,ψ ⟩ and
t E
∀s,t∈M, c(s,t)=⟨ψ ,ψ ⟩ .
s t E
One can check that (B ) implies (A ), (B ) is equivalent to (A ), (B ) is equivalent to (A ),
1 1 2 2 3 3
(B ) is equivalent to (A ), and (B ) is equivalent to the forthcoming Assumption (A ).
4 4 5 5
Remark 1 (Spiked tensor PCA, Section 5.2). In Figures 1, 2 and 3, we considered, as an
example, the detection of a rank one 3-way symmetric tensor. We consider the sphere M = S2.
The Euclidean space of 3-way symmetric tensors of size 3×3×3 is denoted by (E,⟨·,·⟩ ) with
E
dimension m=10, and
ψ :=t⊗3 =(t t t )
t i j k 1≤i,j,k≤3,
(cid:88)
X(t):=⟨W,ψ ⟩ = t t t W ,
t E i j k ijk
ijk
4where W is an order 3 symmetric tensor defined using symmetry and the following independent
terms,
◦ W of variance 1; there are 3 principal diagonal terms,i∈[3];
iii
◦ W of variance 1/3; there are 6 sub-diagonals terms,1≤i̸=j ≤3;
iij
◦ W of variance 1/6; there is 1 off-diagonal term.
123
Maximum likelihood estimator (MLE) The Maximum Likelihood Estimator (MLE) of
the couple (λ ,t ) is given by
0 0
(cid:110)1(cid:13) (cid:13)2(cid:111)
argmin 2(cid:13)Y −λψ t(cid:13)
E
.
(λ,t)
Minimization with respect to λ is straightforward under Assumptions (B ) and (B ) leading to
2 6
arg tminm λin(cid:110)1 2(cid:13) (cid:13)Y −λψ t(cid:13) (cid:13)2 E(cid:111) =arg tmax⟨Y,ψ t⟩ E. (1.7)
Now consider the Gaussian random field Z(t) := ⟨Y,ψ ⟩ , referred to as the profile likelihood.
t E
The argument maximum (resp. the maximum) of Z(·) is exactly the MLE of t (resp. of λ )
0 0
by (1.7), namely
λMLE =maxZ and tMLE =argmaxZ.
By (1.6), note that we uncover the decomposition (1.1) and the alternative hypothesis (1.5) as
Z(t)=⟨Y,ψ ⟩ =λ ⟨ψ ,ψ ⟩ +σ⟨W,ψ ⟩ =λ c(t ,·)+σX(t), (1.8)
t E 0 t0 t E t E 0 0
where X(t):=⟨W,ψ ⟩ and m(·)=λ c(t ,·).
t E 0 0
Remark 2. Assumption (B ) ensures that the profile likelihood is given by Z(·), see (1.7).
6
1.3.3. Parallel with the LARS algorithm
Discrete LARS The LARS algorithm has been introduced in Efron et al. (2004) and has
been widely used in statistics for variable selection. Given an observation Y ∈ Rn and p co-
variates (ψ )p , the Least Angle Regression (LARS) algorithm is a forward stepwise variable
t t=1
selection algorithm giving a sequence (λ ,t ) of the so-called knots. The first knot (λ ,t ) is
i i 1 1
the maximum and the argument maximum of the maximal absolute correlation between the
observation and the covariates. Hence, the first step of the LARS aims at maximizing
λ
1
= max |⟨Y,ψ t⟩Rn| and t
1
=arg max |⟨Y,ψ t⟩Rn|. (1.9)
1≤t≤p 1≤t≤p
We define the so-called residual by, for all λ≤λ ,
1
∀t∈{1,...,p}, Zλ(t):=⟨Y −(λ 1−λ)ψ t1,ψ t⟩Rn, (1.10)
The second knot is defined by
λ −λ :=inf(cid:8) ε>0 : ∃t̸=t s.t. Zλ1−ε(t)≥λ −ε(cid:9) . (1.11)
1 2 1 1
The second knot (λ ,t ) is built so that t is the unique argument maximum of Zλ(·) for
2 2 1
λ
2
<λ≤λ
1
andthatthereexistsasecondpoint,sayt
2
̸=t 1,suchthatZλ2(t 1)=Zλ2(t 2)=λ 2.
Continuous LARS The continuous LARS has been pioneered investigated in Azaïs et al.
(2020) for continuous sparse regression from Fourier measurements, see Section 5.4 for further
details.WeintroducethecontinuousLARSinageneralsettingasfollows.Thefirstknot(λ ,t )
1 1
is the maximum and the argument maximum of the maximal absolute correlation between the
observation Y ∈ E and ψ ∈ E, the features. Hence, the first step of the LARS aims at maxi-
t
mizing
λ =max|⟨Y,ψ ⟩ | and t =argmax|⟨Y,ψ ⟩ |. (1.12)
1 t E 1 t E
t∈M t∈M
Under (B ), note that max |Z(t)| = max Z(t), where Z(t) = ⟨Y,ψ ⟩ . Therefore, the
6 t∈M t∈M t E
definitionofλ givenin(1.2)coincideswiththeaforementioneddefinitionofλ .Onecanchoset
1 1 1
5being the unique argument maximum of Z(·), by (B ) and (B ) (see also (2.4) in Lemma 1).
1 3
We define the so-called residual by, for all λ≤λ ,
1
Zλ(t):=⟨Y −(λ −λ)ψ ,ψ ⟩ =Z(t)−(λ −λ)c(t,t ), (1.13)
1 t1 t E 1 1
where c(s,t)=⟨ψ ,ψ ⟩ . Again, under (B ), note that max |Zλ(t)|=max Zλ(t). More-
s t E 6 t∈M t∈M
over, one can check that Zλ(t )=λ. The second knot is defined by
1
λ −λ :=inf(cid:8) ε>0 : ∃t̸=t s.t. Zλ1−ε(t)≥λ −ε(cid:9) . (1.14)
1 2 1 1
The second knot (λ ,t ) is built so that t is the unique argument maximum of Zλ(·) for
2 2 1
λ
2
<λ≤λ
1
andthatthereexistsasecondpoint,sayt
2
̸=t 1,suchthatZλ2(t 1)=Zλ2(t 2)=λ 2.
Now observe that, for all t̸=t ∈M and all λ,
1
Z(t)−Z(t )c(t,t )
Zλ(t)≤λ ⇔ Z(t)−λ c(t,t )≤λ(1−c(t,t )) ⇔ 1 1 ≤λ ⇔ Z|t1(t)≤λ,
1 1 1 1−c(t,t )
1
where Z|t1(t) is defined as in (1.4), taking into account that the gradient is vanishing at point
t . We uncover that (λ ,t ) is the second knot of the LARS where
1 2 2
λ = max Z|t1(t) and t = max Z|t1(t). (1.15)
2 2
t∈M\{t1} t∈M\{t1}
Notethatthedefinitionofλ in(1.4)isequivalenttotheaforementioneddefinitionofλ (thisis
2 2
also true for the argument maximum). This paper gives the distribution of λ (first knot of the
1
continuousLARS)conditionalto(λ ,Ω)(secondknotofthecontinuousLARSandindependent
2
part of the Hessian at the first knot) under the null hypothesis.
1.4. Contributions and outline
This paper investigates the notion of second maximum λ of a Gaussian random field. The
2
definition of second maximum originates from continuous LARS, see Section 1.3.3, and can be
used in a Kac-Rice formula to compute the conditional law of the maximum λ with respect
1
toλ andtheso-calledindependentpartoftheHessianΩ.Thesecondmaximumisthemaximum
2
of a random field σX|t1(·) with a singularity at point t 1. A first contribution establishes a link
between the eigen-decomposition of Ω and the directional limiting values of σX|t1(·) at the
singularity point t . see Lemma 3. The second contribution is a Kac-Rice type formula, see
1
Appendix B, that gives the conditional law in an explicit form, see Theorem 1. From this point,
we derive an exact test based on the spacing between λ and λ , see Theorem 2. We introduce
1 2
an estimation of the variance σ in Section 4.2, that can be used in a Kac-Rice formula and lead
(cid:98)
to the conditional law of λ /σ in Proposition 7. We derive an exact test based on the spacing
1 (cid:98)
between λ /σ and λ /σ which can be used when the variance σ2 is unknown, see Theorem 3.
1 (cid:98) 2 (cid:98)
Thereadermayconsulthttps://github.com/ydecastro/tensor-spacing/forfurtherdetails
on the numerical experiments of this paper.
The paper is organized as follows. Section 2 introduces the second maximum of a Gaussian
random field. Section 2.3 gives an ad-hoc Kac-Rice formula for the conditional law of λ with
1
respect to λ and Ω. Section 3 gives the exact tests based on the spacing between λ and λ
2 1 2
andthespacingbetweenλ /σ andλ /σ.Section4.4introducestheestimationofthevarianceσ
1 (cid:98) 2 (cid:98) (cid:98)
from the Karhunen-Loève expansion of X|t1(·). Section 5 gives the parallel with continuous
sparse kernel regression.
1.5. Related works
ThefirstpaperwhichconsidersKac-Riceformulaonmanifolds,infactthesphereandtheStiefel
Manifold, is Azaïs and Wschebor (2004). The subject is developed in Adler and Taylor (2009)
and Azaïs and Wschebor (2009). The article Auffinger and Ben Arous (2013) considers the high
dimensionalsphere.AnewsetofhypothesesandproofsisalsogiveninArmentanoetal.(2023).
The Kac-Rice formula is also used in Azaïs et al. (2017) for the study of the maximum of a
Gaussianrandomfieldonthetoruswithapplicationtotheso-calledSuper-Resolutionproblem.It
6fallsunderthegeneraltheoryofcontinuoussparseregressionoverthespaceofmeasureswhichhas
recently attracted a lot of attention in signal processing (Candès and Fernandez-Granda, 2014;
DuvalandPeyré,2015)amongthesuper-resolutioncommunity;inmachinelearning(DeCastro
etal.,2021);andinoptimization(Chizat,2022).Thesuper-resolutionframeworkaimstorecover
fine scale details of an image from just a few low frequency measurements, where ideally the
observation is given by a low-pass filter. It has potential applications in astronomy, medical
imaging, and microscopy. The novel aspects of this body of work rely on new statistical and
optimization guarantees of sparse regression over the space of measures. Initiated by the work
presentedin(Azaïsetal.,2020),oneinvestigatethepossibilityofdetectingasparseobjectfrom
a test on the mean m(·) of a Gaussian random field under a sparsity-type assumption.
General notation
[a] Set of integers {1,...,a};
A⊤ (resp. A ) transpose of a matrix A (resp. i-th line);
i:
Id Identity matrix of dimension d×d;
S Space of d×d symmetric matrices;
d
Sd−1 Euclidean sphere of Rd;
t (and s) Generic value for a vector on the manifold M;
Ck Set of k times differentiable functions;
δ Kronecker symbol;
jk
(const) Positive constant which may change from line to line;
D(U|V) Conditional distribution of U with respect to V;
Var(U) Variance matrix of a random vector U;
Cov(U,V) Covariance matrix of the random vectors U and V;
φ(·) Standard Gaussian density in R;
p (ℓ) Density function of the random variable/vector U at point ℓ;
U
1 (resp. 1{A}) Indicator function of condition A (resp. event A);
A
Leb Lebesgue measure on R;
U(0,1) Uniform law on (0,1);
µ⊗ν Product of measures;
Random fields
Z(·) Observed Gaussian random field, indexed by M;
m(·) Mean function of Z(·);
σ Standard error of the Z(·);
X(·) Centered Gaussian random field with unit variance;
c(s,t) Covariance function of X(·) and Z(·)/σ, c(s,t)=E(X(s)X(t));
Differential geometry
M C2-compact Riemannian manifold of dimension d without boundary;
∇f(t) Riemannian gradient at point t∈M of f : M →R;
∇ g(s,t) Riemannian gradient at point t∈M of g(s,·);
t
∇2f(t) Riemannian Hessian at point t∈M of f : M →R;
Continuous regression
(E,⟨·,·⟩ ) Euclidean space;
E
ψ Feature map from M to E, ψ : t∈M (cid:55)→ψ ∈E;
t
ψ Vector ψ(t)∈E;
t
Jψ (resp. Jψ ⊤) (resp. transpose of) Jacobian matrix of ψ at point t;
t t
W Standard Gaussian random vector of E;
S (resp. T) Generalized spacing test (resp. Generalized t-spacing test);
σ
Table 1
List of notation: Riemannian gradients and Hessians are defined using the Levi-Civita connection. Riemannan
Hessians are represented in matrix form.
72. The second maximum of a Gaussian random field
2.1. The two Gaussian regression remainders
Some useful properties are presented in the next lemma.
Lemma 1. Under Assumption (A ), one has
1-4
◦ c(s,t)=E[X(s)X(t)] and c(t,t)=1; (2.1)
◦ X(t) and ∇X(t) are independent; (2.2)
◦ Λ (t):=Var[∇X(t)]=−Cov[∇2X(t),X(t)] (2.3)
2
◦ the argument maximum of X(·) is unique. (2.4)
Proof. The first property (2.1) is equivalent to (A ). The other properties can be deduced by
2
differentiating c(t,t) = 1. The last statement is a consequence of Tsirelson’s theorem, see for
instance Lifshits (1983, Theorem 3).
The Gaussian random field X|t(·) WeknowthatX(t)and∇X(t)areindependentby(2.2).
For a fixed point t∈M, consider the remainder of Gaussian regression of X(s) with respect to
(X(t),∇X(t)) given by the Gaussian random field
s∈M (cid:55)→X(s)−c(s,t)X(t)−∇ c(s,t)⊤Λ−1(t)∇X(t)∈R,
t 2
where∇ c(s,t)istheRiemanniangradientoft(cid:55)→c(s,t).By(A ),remarkthatΛ (t)isinvertible.
t 4 2
This Gaussian random field is well defined on M and independent of (X(t),∇X(t)). Now, for
s̸=t, set
X(s)−c(s,t)X(t)−∇ c(s,t)⊤Λ−1(t)∇X(t)
X|t(s):= t 2 , (2.5)
1−c(s,t)
thatis,X|t(s)isanormalisationoftheremainderoftheregressionofX(s)w.r.t.(X(t),∇X(t)).
The regression of the Hessian R(t) Now, consider the following regression in the space of
Gaussian symmetric matrices
∇2X(t)=−Λ 2(t)X(t)−Λ 3(t)∇X(t)+R(cid:101)(t), (2.6)
for some well-defined 3-way tensor Λ (t). It will not be necessary to give the explicit expression
3
of Λ (t) for our purposes, this tensor is well defined by Gaussian regression formulas. Thus,
3
one can identify the symmetric matrix R(cid:101)(t) as the remainder of the regression of ∇2X(t) on
(X(t),∇X(t)). For future use, it is convenient to set
R(t):=Λ− 21 (t)R(cid:101)(t)Λ− 21 (t). (2.7)
2 2
Note that R(cid:101)(t) and R(t) are symmetric. The following lemma is straightforward.
Lemma 2. For any t ∈ M, the Gaussian random field X|t(·) and the Gaussian random ma-
trix R(t) are independent of (X(t),∇X(t)).
2.2. The first and second maxima, and the independent part of the Hessian
First maximum: We define the first maximum λ of σX(·) and its argument maximum t by
1 1
λ :=σX(t ) and t :=argmaxX(t). (2.8)
1 1 1
t∈M
The argument maximum is almost surely a singleton by (2.4), hence t is unique almost surely.
1
Second maximum: We define the second maximum λ of σX(·) by
2
∀t∈M, λt := sup {σX|t(s)} and λ :=λt1. (2.9)
2 2 2
s∈M\{t}
8Independent part of the Hessian: We define the independent part of the Hessian Ω as
Ω:=σR(t 1)=σΛ 2− 21 (t 1)R(cid:101)(t 1)Λ−
2
1 2(t 1). (2.10)
At this stage, it is not clear that λt < ∞ a.s. and how X|t(s) is shaped around point t. The
2
next lemma gives a description of s(cid:55)→X|t(s) around point t which proves that λt <∞ almost
2
surely. A proof of this lemma can be found in Appendix A. Note that, since M is compact, by
the Hopf–Rinow theorem, M is geodesically complete and the exponential map exists on the
whole tangent space.
Lemma 3. Let h be a nonzero vector of the tangent space at point t. For all ε ̸= 0, let
s(ε) := exp (εh) ∈ M be the exponential map at t given by the tangent vector εh. Then, under
t
Assumptions (A ), (A ) and (A ),
1 2 4
h⊤R(cid:101)(t)h h⊤ h
lim X|t(s(ε))= = R(t) , almost surely. (2.11)
ε̸=0 h⊤Λ 2(t)h ∥h∥ 2 ∥h∥ 2
ε→0
Furthermore, there exists a unit Euclidean norm tangent vector h at point t such that
0
limsup X|t(s)=h⊤R(t)h =λ (R(t))<∞, almost surely.
0 0 max
s→t
Remark 3. The aforementioned lemma shows that λt varies in (−∞,∞) almost surely. Also,
2
it shows that X|t(·) is a helix random field (Azaïs and Wschebor, 2005, Lemma 4.1) with pole
t: the paths of the random field need not extend to a continuous function at the point t; however,
the paths have radial limits at t and the random filed may take the form of a helix around t. The
shape of the helix locally around the singularity is described by the eigen-decomposition of the
independent part of the Hessian, as shown by (2.11)
Besides, for every t such that ∇X(t)=0 it follows that
X(s)−c(s,t)X(t)
X|t(s)= =:Xt(s). (2.12)
1−c(s,t)
In particular, we have the following identity between random fields X|t1(·)=Xt1(·), which may
not be Gaussian (due to the random point t ). The following lemma is straightforward.
1
Lemma 4. For a fixed t∈M, consider the following indicator functions:
• ı :=1(cid:8) t=t (cid:9) ;
1 1
• ı :=1(cid:8) ∀s∈M \{t}, X(s)≤X(t)(cid:9) ;
2
• ı :=1(cid:8) ∀s∈M \{t}, Xt(s)≤X(s)(cid:9) ;
3
• ı :=1(cid:8) ∇X(t)=0(cid:9)1(cid:8) λt ≤X(t)(cid:9) ;
4 2
then ı =ı =ı =ı and −∞<λ ≤λ <∞, almost surely.
1 2 3 4 2 1
2.3. The conditional distribution of the maximum
We have the following key result giving the distribution of λ , defined by (2.8), conditional
1
to (λ ,Ω).
2
Theorem 1. Let X(·) be a Gaussian random field satisfying Assumption (A ). Then, the
1-4
distribution D(λ |λ ,Ω) of the maximum λ conditional to (λ ,Ω) has a density with respect
1 2 1 2
the Lebesgue measure Leb and this conditional density at point ℓ∈R is given by
dD(λ |λ ,Ω) det(ℓId−Ω)φ(ℓ/σ)
1 2 (ℓ)= 1 ,
dLeb G (λ ) λ2≤ℓ
Ω 2
(cid:90) +∞
where G (λ ):= det(uId−Ω)φ(u/σ)du and φ(·) is the standard Gaussian density.
Ω 2
λ2
Proof. Consider the set of parameters
(cid:110) (cid:111)
B := (ℓ ,ℓ ,r)∈R2×S : ℓ ≤ℓ ,
1 2 d 2 1
9where S is the set of d×d symmetric matrices. Let B be an open set of R×S and b ∈ R,
d 1 d
define
(cid:110) (cid:111)
B = (ℓ ,ℓ ,r)∈R2×S : ℓ <ℓ , ℓ <b, (ℓ ,r)∈B .
1 2 d 2 1 2 1 1
Note that B is an open set of B. Note that this class of open sets generates the Borel sigma
algebra of B. Hence, in order to derive the joint law of the triplet (λ ,λ ,Ω), we compute
1 2
P(cid:8)
(λ ,λ
,Ω)∈B(cid:9)
1 2
=P(cid:8) ∃t∈M : ∇X(t)=0, (σX(t),λt,σR(t))∈B(cid:9) (2.13)
2
=E(cid:2) #(cid:8) t∈M : ∇X(t)=0, (σX(t),λt,σR(t))∈B(cid:9)(cid:3) (2.14)
2
=(cid:90) E(cid:104)(cid:12) (cid:12)det(cid:0) −∇2X(t)(cid:1)(cid:12) (cid:12)1 (σX(t),λt,σR(t))∈B(cid:12) (cid:12) (cid:12)∇X(t)=0(cid:105) p ∇X(t)(0)dν(t) (2.15)
2
M
=(cid:90) E(cid:104) det(cid:0) −∇2X(t)(cid:1)1 (cid:12) (cid:12)∇X(t)=0(cid:105) p (0)dν(t) (2.16)
(σX(t),λt,σR(t))∈B(cid:12) ∇X(t)
2
M
(cid:90) (cid:104) (cid:105)
= E det(X(t)Λ 2(t)−R(cid:101)(t))1
(σX(t),λt,σR(t))∈B
p ∇X(t)(0)dν(t) (2.17)
2
M
(cid:90) (cid:104) (cid:105)
= E det(X(t)Id−R(t))1 p (0)detΛ (t)dν(t), (2.18)
(σX(t),λt,σR(t))∈B ∇X(t) 2
2
M
where ν is the surfacic measure on M and
• we used that (σX(t),λt,σR(t))∈B implies λt ≤σX(t), and invoked Lemma 4 in (2.13);
2 2
• we used the uniqueness of the maximum (2.4) in (2.14);
• weusedaKac-Riceformulatogetthethirdequality(2.15),aproofisgiveninAppendixB;
• weusedthatı =ı (seeLemma4)andthattheHessian−∇2X(t)issemi-definitepositive
1 4
at a maximum to get (2.16);
• weusedtheHessianregressionformula(2.6)with∇X(t)=0andinvokedtheindependence
of ∇X(t) from (σX(t),λt,σR(cid:101)(t)) to get (2.17);
2
Now, we introduce the measures (defined up to normalizing constants)
dν(t):=p (0)detΛ (t)dν(t), (2.19)
∇X(t) 2
(cid:90)
µ⋆(·):= µ (·)dν(t), (2.20)
t
T
where µ :=D(λt,σR(t)) is the distribution of (λt,σR(t)).
t 2 2
Since X(t) is independent of (λt,σR(t)), one has
2
D(σX(t),λt,σR(t))=N(0,σ2)⊗µ ,
2 t
by Assumption (A ). Now, coming back to (2.18) we have
2
(cid:90) (cid:104) (cid:105)
P{(λ ,λ ,Ω)∈B}= E det(X(t)Id−R(t))1 dν(t)
1 2 (σX(t),λt,σR(t))∈B
2
M
(cid:90) (cid:90)
= det(ℓ Id/σ−ω/σ)1 φ(ℓ /σ)dℓ dµ (ℓ ,ω)dν(t)
1 (ℓ1,ℓ2,ω)∈B 1 1 t 2
M RD
(cid:90) (cid:90)
= det(ℓ Id/σ−ω/σ)1 φ(ℓ /σ)dℓ dµ (ℓ ,ω)dν(t)
1 (ℓ1,ℓ2,ω)∈B 1 1 t 2
RD M
(cid:90)
= det(ℓ Id/σ−ω/σ)1 φ(ℓ /σ)dℓ dµ⋆(ℓ ,ω),
1 (ℓ1,ℓ2,ω)∈B 1 1 2
RD
whereD =(d2+d+4)/2isthedimensionofR2×S .Itimpliesthatthejointdensityof(λ ,λ ,Ω)
d 1 2
at point (ℓ ,ℓ ,ω) on the set B has a density proportional to det(ℓ Id/σ −ω/σ)φ(ℓ /σ) with
1 2 1 1
respect to Leb ⊗ µ⋆. This implies in turn that the density of the maximum λ conditional
1
to (λ ,Ω) is proportional to det(ℓ Id−ω)φ(ℓ /σ)1 .
2 1 1 ℓ2≤ℓ1
103. Spacing test for the mean of a random field with known variance
3.1. Testing framework
We observe a random field Z(t) = m(t)+σX(t) and we would like to test the global nullity of
its mean m(·). We define the statistics
λ :=max{Z(t)} and t :=argmax{Z(t)}; (3.1)
1 1
t∈M t∈M
Z(s)−λ c(s,t )
Z|t1(s):= 1 1 ; (3.2)
1−c(s,t )
1
λ :=max{Z|t1(s)} and t :=argmax{Z|t1(s)}; (3.3)
2 2
s∈M s∈M
Ω:=Λ− 21 (t )(∇2Z(t )+λ Λ (t ))Λ− 21 (t ); (3.4)
2 1 1 1 2 1 2 1
In the previous section, we assumed that the Gaussian random field X(·) was centered in (A ).
2
In this section, we give an exact test procedure for the following null hypothesis:
E[Z(·)]=0, (H )
0
as a consequence Z(·)=σX(·) and the notations λ ,t ,λ ,t ,Ω are consistent with Section 2.
1 1 2 2
3.2. Spacing test
We can now state our main result when the variance σ2 is known.
Theorem 2. Let X(·) be a Gaussian random field satisfying Assumption (A ). Under H ,
1-4 0
the test statistic
G (λ /σ)
Ω/σ 1
S (λ ,λ ,Ω):= ∼U(0,1),
σ 1 2 G (λ /σ)
Ω/σ 2
(cid:90) +∞
whereU(0,1)istheuniformdistributionon(0,1),G (ℓ):= det(uId−Ω/σ)φ(u)duandφ(·)
Ω/σ
ℓ
is the standard Gaussian density.
Proof. Withoutlossofgenerality,assumethatσ =1.Itiswellknownthat,ifarandomvariableQ
has a continuous cumulative distribution function F, then F(Q) ∼ U(0,1). This implies that,
conditionally to (λ ,Ω) = (ℓ ,r), G (λ )/G (ℓ ) ∼ U(0,1). Since the conditional distribution
2 2 r 1 r 2
does not depend on (ℓ ,r), it is also the non-conditional distribution as claimed.
2
4. The unknown variance case
4.1. Existence of non-degenerate systems
WeconsiderarealvaluedcenteredGaussianrandomfieldX(·)definedonM satisfyingAssump-
tions (A ), for the moment. We consider the order m Karhunen-Loève expansion in the sense
2
that
m m
(cid:88) (cid:88)
σX(t)= ζ f (t) with Var(ζ )=σ2 and ∀t∈M, |f (t)|2 =1, (KL(m))
i i i i
i=1 i=1
where the equality holds in L2, uniformly in t, and (f ,...,f ) is a system of non-zero func-
1 m
tions orthogonal on L2(M). We say that X(·) satisfies Assumption (KL(m)) if it admits an
order m Karhunen-Loève expansion. Through our analysis, we need to consider the following
non-degeneracy Assumption: X is a.s. differentiable, it holds that m>d+1 and for all t ∈M
1
∃(t ,...,t )∈(M \{t })m−d−1 pairwise distincts s.t.
d+2 m 1
(X(t ),∇X(t ),X(t ),...,X(t )) is non degenerate. (ND(m))
1 1 d+2 m
When X(·) admits an infinite order Karhunen-Loève expansion in the sense that
∞ ∞
(cid:88) (cid:88)
σX(t)= ζ f (t) with Var(ζ )=σ2 and ∀t∈M, |f (t)|2 =1, (KL(∞))
i i i i
i=1 i=1
11wesaythatX(·)satisfiesAssumption(KL(∞)).Inthiscase,thenon-degeneracyconditionreads:
X is a.s. differentiable, for all p>d+1 and for all t ∈M
1
∃(t ,...,t )∈(M \{t })p−d−1 pairwise distincts s.t.
d+2 p 1
(X(t ),∇X(t ),X(t ),...,X(t )) is non degenerated. (ND(∞))
1 1 d+2 p
A standard result shows that if the covariance function of X(·) is C0(M×M) on M compact
then the Karhunen-Loève expansion exists (of finite or infinite order). The following key result
shows that if X has C1 paths almost surely, and satisfies Assumptions (A ) and (A ) then the
2 4
non-degeneracy condition also holds.
Proposition 5. Let X(·) be a real valued Gaussian random field having C1 paths almost surely,
and satisfying Assumptions (A ) and (A ). Then, for all m > d+1, Assumption (KL(m))
2 4
implies Assumption (ND(m)), and also Assumption (KL(∞)) implies Assumption (ND(∞)).
Proof. Since X has C1 paths almost surely, note that the covariance function c(·,·) of the Gaus-
sian random field X(·) has continuous partial derivatives.
ConsiderthecenteredGaussianvectorV :=(∇X(t ),X(t ),X(t ),...,X(t ))withvariance-
1 1 d+2 p
covariance matrix Σ(V)=E[VV⊤] given by
∇X(t1) (X(t1),X(td+2),...,X(tp))
(cid:122) (cid:125)(cid:124) (cid:123) (cid:122) (cid:125)(cid:124) (cid:123)
 
∂ ∂ c(t ,t ) ∂ ∂ c(t ,t ) ... ∂ ∂ c(t ,t ) ∂ c(t ,t ) ∂ c(t ,t ) ... ∂ c(t ,t )
1 1 1 1 1 2 1 1 1 d 1 1 1 1 1 1 1 d+2 1 1 p
∂ 2∂ 1c(t 1,t 1) ∂ 2∂ 2c(t 1,t 1) ... ∂ 2∂ dc(t 1,t 1) ∂ 2c(t 1,t 1) ∂ 2c(t 1,t d+2) ... ∂ 2c(t 1,t p) 
  . . . . . . ... . . . . . . . . . ... . . .  
 
 
∂ d∂ 1c(t 1,t 1) ∂ d∂ 2c(t 1,t 1) ... ∂ d∂ dc(t 1,t 1) ∂ dc(t 1,t 1) ∂ dc(t 1,t d+2) ... ∂ dc(t 1,t p) 
 
 
 
 c(t 1,t 1) c(t 1,t d+2) ... c(t 1,t p) 
 
 ⋆ c(t d+2,t 1) c(t d+2,t d+2) ... c(t d+2,t p) 
  . .
.
. .
.
... . .
.
 
 
c(t ,t ) c(t ,t ) ... c(t ,t )
p 1 p d+2 p p
where ∂ ∂ c(x,x) := E[(∂X/∂x )(x)(∂X/∂x )(x)] and ∂ c(x,y) := E[(∂X/∂x )(x)X(y)] with
i j i j i i
(∂X/∂x )(x) the partial derivative with respect to x at point x=(x ,...,x )∈M in Rieman-
i i 1 d
nian normal coordinates, and y ∈M.
Denote by H the RKHS defined by c(·,·). By a standard result, see for instance Steinwart
and Christmann (2008, Corollary 4.36), it holds that ∂ Φ(t ) belongs to H and
i j
c(t i,t j)=⟨Φ(t i),Φ(t j)⟩H;
∂ ic(t 1,t j)=⟨∂ iΦ(t 1),Φ(t j)⟩H;
∂ i∂ jc(t 1,t 1)=⟨∂ iΦ(t 1),∂ jΦ(t 1)⟩H;
whereΦisthecanonicalfeaturemapof(H,⟨·,·⟩H).WerecognizethatΣ(V)istheGrammatrix,
forthescalarproduct⟨·,·⟩H,ofΨ:=(cid:2) ∂ 1Φ(t 1)∂ 2Φ(t 1) ··· ∂ dΦ(t 1)Φ(t 1)Φ(t d+2) ··· Φ(t p)(cid:3) ∈Hp,
and Σ(V) = Ψ⋆Ψ where Ψ⋆ : h ∈ H (cid:55)→ (⟨∂ 1Φ(t 1),h⟩H,··· ,··· ,··· ,⟨Φ(t p),h⟩H) ∈ Rp is the
adjoint operator of Ψ.
Using Assumption (A ), Assumption (A ) and (2.2), one gets that W = (X(t ),∇X(t )) is
2 4 1 1
non-degenerated, and its variance covariance matrix Σ(W) has full rank. This matrix is also the
Gram matrix of the system of vectors in H given by Ψ(cid:101) := (cid:2) Φ(t 1)∂ 1Φ(t 1)···∂ dΦ(t 1)(cid:3) ∈ Hd+1.
We deduce that is a free system (i.e., it spans a vector space of dimension d+1). One also
knowsthatthedimensionofHisexactlytheorderoftheKarhunen-Loèveexpansion,actuallyit
is standard to prove the Karhunen-Loève expansion from Mercer’s theorem. Hence, it is always
possibletocompleteΨ(cid:101) byp−d−1vectorsoftheformΦ(t)togetapdimensionalfreesystemΨ,
otherwise H would be of dimension less than p.
We give some examples below:
√
• The normalized Brownian motion W / t satisfies Assumption (KL(∞));
t
• AnyGaussianstationaryfieldwithaspectrumthatadmitsanaccumulationpointsatisfies
Assumptions(KL(∞))and(ND(∞))ifdifferentiable,seeforinstanceAzaïsandWschebor
(2009, Exercices 3.4 and 3.5);
12• Any Gaussian random field satisfying conditions of Azaïs and Wschebor (2005, Proposi-
tion 3.1) has (KL(∞)) and (ND(∞))
• Note that in the applications of Section 5 all the Gaussian random fields satisfy (KL(m))
with m finite and explicitly given.
4.2. The Karhunen-Loève estimator of the variance
In practical applications, the assumption that the variance is known is too restrictive. In this
section, we are supposed to observe σX(·) where σ >0 is unknown and X(·) satisfies Assump-
tion (A ). In particular, X(·) admits a Karhunen-Loève expansion of order denoted m (X)
1-4 KL
possibly infinite. We assume the following fifth condition:
κ:=m (X)−d−1≥1. (A )
KL 5
We will make use of the notation m:=m (X) when there is no ambiguity.
KL
Remark 4. Using Assumption (A ) and the fact that the gradient of X(t) is independent
4
of X(t), it can be shown that m (X) is greater than or equal to d+1, by an argument similar
KL
to the proof of Proposition 5. Note Assumption (A ) requires at least d+2 degrees of freedom,
5
and this extra degree of freedom is the price to estimate the variance.
Theinfiniteordercase WhenX(·)satisfies(KL(∞)),notethat,foreveryintegerp≥2,from
the observation of (σX(t ),...,σX(t )) for conveniently chosen points t ,...,t (say uniformly
1 p 1 p
atrandomforinstance),onecanbuildanestimator,sayσ2 ,ofthevarianceσ2 withchi-squared
(cid:98)(p)
distribution σ2χ2(p−1)/(p−1) under H . Making p tend to infinity, classical concentration
0
inequalities and Borel-Cantelli lemma prove that σ2 converges almost surely to σ2 under H .
(cid:98)(p) 0
Thus the variance σ2 is theoretically directly observable from the entire path of σX(·) (though
inpracticalapplicationsonewillestimateitbyaχ2 withalargenumberofdegreesoffreedom).
We still denote by σ2 this observation.
(cid:98)t
Thefiniteordercase ByProposition5,notethatAssumption(ND(m))holdswithm>d+1.
Let t ∈ M and let (t ,...,t ) ∈ Mκ be as in Assumption (ND(m)), then the Gaussian
d+2 m
vector(σX|t(t ),...,σX|t(t ))hasforvariance-covariancematrixσ2ΣwhereΣissomeknown
d+2 m
matrix and an estimator of σ2 is
σ (cid:98)t2 :=(cid:13) (cid:13)Σ−1/2(cid:0) σX|t(t d+2),...,σX|t(t m)(cid:1)(cid:13) (cid:13)2 /κ. (4.1)
DirectalgebrashowsthatX|t(·)inheritsanorderκKarhunen-Loèveexpansionfromtheorderm
Karhunen-Loève expansion of X(·). More precisely, under (H ),
0
κ
(cid:88)
∀s̸=t∈M, σX|t(s)= ζ f (s) with Var(ζ )=σ2, (4.2)
i i i
i=1
where equality holds in L2, uniformly in s, and it holds
κ
σ2 = 1 (cid:88) ζ2 . (4.3)
(cid:98)t κ i
i=1
It shows that σ2 does not depend on the choice of t ,...,t in (ND(m)).
(cid:98)t d+2 m
Proposition 6. Let X(·) satisfy Assumptions (A ), (A ), (A ) and (A ). Let t ∈ M, then
1 2 4 5
the following claims are true under the null hypothesis (H ).
0
(i) σ2 is well defined and follows a σ2χ2(κ)/κ distribution;
(cid:98)t
(ii) σ2 is independent of (X(t),∇X(t));
(cid:98)t
(iii) the random field X|t(·)/σ is independent of the random variable σ .
(cid:98)t (cid:98)t
Proof. Statement (i) follows from (4.3) and Statement (ii) follows from (4.1) and the indepen-
dence of X|t(·) from (X(t),∇X(t)). The last statement is a direct consequence of the indepen-
dence between the angle and the norm of ζ =(ζ ,··· ,ζ ) and (4.2).
1 κ
134.3. Joint law in the unknown variance case
We present below the use of the estimation σ to modify the spacing test. For fixed t ∈ M, by
(cid:98)t
Proposition6,weknowthatX(t),∇X(t), X|t(·)/σ andσ aremutuallyindependent.By(2.7),
(cid:98)t (cid:98)t
we recall that
R(t):=Λ−1 2(t)(cid:104) ∇2X(t)+Λ (t)X(t)+Λ (t)∇X(t)(cid:105) Λ− 21 (t),
2 2 3 2
As Lemma 3 shows, R(t) can be expressed as radial limits of X|t(·) at point t hence we get that
therandomvariables(cid:110) X(t),∇X(t), (cid:0) X|t(·)/σ , R(t)/σ (cid:1) , σ (cid:111) aremutuallyindependentand,
(cid:98)t (cid:98)t (cid:98)t
by consequence, the variables
(cid:34) (cid:35)
λt R(t)
X(t),∇X(t), 2, , σ are mutually independent,
σ σ (cid:98)t
(cid:98)t (cid:98)t
wherewerecallthatλt isdefinedin(2.9).Weconsidert∈Maputativevalueandλt buildfrom
2 1
σX(·) in (2.8). Now, consider the test statistics
λt λt
T := 1 , T :=T , T := 2 , T :=T , σ :=σ and Ω=σR(t ) as in (2.10), (4.4)
1,t σ 1 1,t1 2,t σ 2 2,t1 (cid:98) (cid:98)t1 1
(cid:98)t (cid:98)t
and let µ be the joint law of the couple of random variables (T ,σR(t)/σ ).
t 2,t (cid:98)t
Undernull(H ),notethatthevariableσX(t)isacenteredGaussianvariablewithvarianceσ2
√ 0
and κσ /σisdistributedasaχ-distributionwithκdegreesoffreedom.Hence,σ /σhasdensity
(cid:98)t (cid:98)t
f √χκ κ(s)=
Γ21 (cid:0)− κκ
2
(cid:1)√
κ(cid:0)
s√
κ(cid:1)κ−1 exp(cid:0) −(κs2/2)(cid:1)
,
2
under the null (H ). Then (σX(t),σ /σ,T ,σR(t)/σ ) has a density
0 (cid:98)t 2,t (cid:98)t
(const)sκ−1exp(−(κs2/2))φ(ℓ /σ),
1
with respect to Leb ⊗µ at point (ℓ ,s,t ,r) ∈ R3×S and where the constant (const) may
2 t 1 2 d
depend on m, κ and σ. Using the same method as for the proof of Theorem 1 we have the
following proposition.
Proposition 7. Let X(·) be a Gaussian random field satisfying Assumption (A ). Then,
1-5
under (H ), the joint distribution of (cid:0) λ ,σ/σ,T ,Ω/σ(cid:1) has a density with respect to Leb ⊗µ⋆
0 1 (cid:98) 2 (cid:98) 2
at point (ℓ ,s,t ,r)∈R3×S equal to
1 2 d
(const)det(ℓ Id−σsr)sκ−1exp(−(κs2/2))φ(ℓ /σ)1 ,
1 1 {0<σst2<ℓ1}
where µ⋆ is defined by µ⋆(·):=(cid:82) µ (·)dν(t) and ν(t) by (2.19).
M t
4.4. t-Spacing test, an exact test for the unknown variance case
We have the second main result, when the variance is unknown.
Theorem 3. Let X(·) be a Gaussian random field satisfying Assumption (A ). For all r ∈S ,
1-5 d
define H as
r
(cid:90) +∞ (cid:16) (cid:112) (cid:17)
∀ℓ∈R, H (ℓ):= det(uId−r)f u (m−1)/κ du, (4.5)
r m−1
ℓ
where f is the density of the Student t-distribution with m−1 degrees of freedom. Under the
m−1
null (H ), the test statistic
0
H (T )
T(T ,T ,Ω/σ):=
Ω/σ(cid:98) 1
∼U(0,1),
1 2 (cid:98) H (T )
Ω/σ(cid:98) 2
where T , T , Ω are given by (4.4).
1 2
14Proof. First, using Proposition 7 and the change of variable t =ℓ /σs, the joint density of the
1 1
quadruplet (T ,σ/σ,T ,Ω/σ) at point (t ,s,t ,r) is given by
1 (cid:98) 2 (cid:98) 1 2
(const)det(σs(t Id−r))sκexp(−(κs2/2))φ(st )1
1 1 {0<t2<t1}
(cid:34) (cid:32) (cid:114) (cid:33)2 (cid:35)
κ m−1
=(const)det(t Id−r)sm−1exp − s φ(st )1 .
1 m−1 2 1 {0<t2<t1}
Second,notethatifU andV areindependentwithdensitiesf andf thenW := U hasdensity
U V V
(cid:90)
f (w)= f (wv)vf (v)dv.
W U V
R
In our case, integrating over s and with the change of variable s←s(cid:112) κ/(m−1), it holds
(cid:90) (cid:34) (cid:32) (cid:114) κ (cid:33)2 m−1(cid:35)
φ(st )sm−1exp − s ds
1 m−1 2
R+
(cid:90) (cid:32) (cid:114) m−1(cid:33) (cid:20) s2(m−1)(cid:21)
=(const) φ st ssm−2exp − ds
1 κ 2
R+
(cid:90) (cid:32) (cid:114) m−1(cid:33)
=(const) φ st
1 κ
sf √χm−1(s)ds
R+ m−1
(cid:32) (cid:114) (cid:33)
m−1
=(const)f t .
m−1 1 κ
Putting together, the density of (T ,T ,Ω/σ) at point (t ,t ,r) is now given by
1 2 (cid:98) 1 2
(cid:32) (cid:114) (cid:33)
m−1
(const)det(t Id −r)f t 1 ,
1 d m−1 1 κ {0<t2<t1}
and we conclude using the same trick as the one of Theorem 2.
Figure 4. [Spiked tensor PCA, Section 5.2, example 4/4] The variance estimator is not distributed
according to χ2-distribution with√κ = 7 degrees of freedom, it underestimates the variance. The dashed black
line is the distribution of a χ(7)/ 7, and σ = 1 in these experiments. The probability distribution function is
estimated over 250,000 Monte-Carlo samples for each value of γ.
Remark5(Onthevarianceestimator). Theformula (4.5)involvestheStudentdensityfunction
with m−1 degrees of freedom which necessitates several comments:
• First,thisformulashowsthatκσ2/σ2 (resp.(m−1)σ2/σ2)failstobedistributedaccording
(cid:98) (cid:98)
to a χ2-distribution with κ (resp. m−1) degrees of freedom. This because t is random.
1
• Second, Figure 4 illustrates that σ2 under-estimates the variance σ2 under the null (γ =0)
(cid:98)
or moderate alternatives (γ ≃1). The reason for this is clear: the chi-squared distribution
assumes that the model is pre-specified, not chosen on the basis of σX(·). But the Con-
tinuous LARS procedure has deliberately chosen the strongest predictor ψ , with t ∈ M
t1 1
maximum of Z(·), among all of the available choices, so it is not surprising that it yields
a drop in the value of the variance estimate on the residuals Z|t1(·).
15• Third, Figure 4 shows that κσ2/σ2 has almost χ2-distribution with κ degrees of freedom
(cid:98)
for large alternatives (γ = 5). The reason for this is clear: the mean m(·) is much larger
than noise σX(·) hence t ≃ t and κσ2/σ2 ≃ κσ /σ2 which has χ2-distribution with κ
1 0 (cid:98) (cid:98)t1
degrees of freedom.
5. Examples
5.1. How to deploy our method
We are given an observation Y ∈E as in Section 1.6 and we compute the corresponding profile
likelihood random field given by Z(t)=⟨Y,Ψ ⟩ . We would like to test the global nullity of its
t E
mean m(·) given by hypothesis (H ). The testing framework is depicted in Section 3.1 and we
0
recall that
Ω=Λ− 21 (t )(∇2Z(t )+λ Λ (t ))Λ−1 2(t ). (5.1)
2 1 1 1 2 1 2 1
The first and second maximum (and their arguments) are given by (2.8), which are computed
using a Riemaniann gradient descent algorithm on M. We will give the expression of Λ (t) in
2
each example together with a closed form expression to compute the Riemaniann Hessian from
the Euclidean Hessian and the Euclidean gradient. The Euclidean Hesssian can be computed
using numerical differentiation or, in some cases, is given explicitly.
When the variance σ2 is known, the testing statistics is given by Theorem 2. In particular,
one needs to compute
(cid:90) +∞
G (ℓ)= det(uId−Ω/σ)φ(u)du. (5.2)
Ω/σ
ℓ
When the variance σ2 is unknown, the testing statistics is given by Theorem 3. In particular,
one needs to compute
(cid:90) +∞ (cid:16) (cid:112) (cid:17)
H (ℓ)= det(uId−Ω/σ)f u (m−1)/κ du, (5.3)
Ω/σ(cid:98) (cid:98) m−1
ℓ
where we recall that f (·) is the density of the Student t-distribution with m−1 degrees
m−1
of freedom. Numerical integration can be done but, in some cases, we give explicit expressions
of G (ℓ) and H (ℓ).
AΩ s/ fσ orthevariaΩ n/ cσ(cid:98)eestimation,wedrawκindependentpoints(t
,...,t )uniformlyonM.
d+2 m
They generically satisfy Condition (ND(m)). For a putative point t ∈ M, the Gaussian vector
(X|t(t ),...,X|t(t )) has for variance-covariance matrix σ2Σ where Σ is some known matrix
d+2 m
and an estimator of σ2 is given by (4.1) with t = t as in (4.4). In our experiments, we have
1
drawn 5 independent samples on κ points and we found the same value for σ, as shown by the
(cid:98)
theory (4.3).
5.2. Spiked tensor PCA
We consider a simple example of the detection of a rank one 3-way symmetric tensor observing
Y = λ t⊗3 +σW where W is an order 3 symmetric tensor defined using symmetry and the
0 0
following independent terms,
W of variance 1; there are 3 principal diagonal terms,i∈[3],
iii
W of variance 1/3; there are 6 sub-diagonals terms,i̸=j ∈[3],
iij
W of variance 1/6; there is 1 off-diagonal term.
123
The profile likelihood is given by Z(t) = ⟨Y,t⊗3⟩ = λ ⟨t ,t⟩3 +σX(t) where M = S2 is the
E 0 0
2-sphere and E is the Euclidean space of 3-way symmetric tensors of size 3×3×3 denoted by
(E,⟨·,·⟩ ) with dimension m=10. Hence, we have
E
ψ :=t⊗3 =(t t t )
t i j k 1≤i,j,k≤3
c(s,t):=⟨s,t⟩3
(cid:88)
Z(t):=⟨Y,ψ ⟩ = t t t Y
t E i j k ijk
ijk
Thereadermayconsulthttps://github.com/ydecastro/tensor-spacing/forfurtherdetails
on the numerical experiments.
16Hessian To compute gradient and Hessian we can take advantage of the isometry of the
problem and consider, without loss of generality, the case where t ∈ M is the so-called “north
pole”: (0,0,1). We have
 
3Y
133
Z′(0,0,1)= 3Y 233  .
3Y
333
This shows that Λ =3 Id (on the tangent space), hence
2 2
σ
Ω=σR(t 1)= 3R(cid:101)(t 1).
Since the second fundamental form of the unit sphere is −Id, the Riemannian Hessian is equal
to the Euclidean Hessian Z′′(t) limited to the tangent space −Id times the normal derivative
(oriented outwards). The Euclidean Hessian Z′′(t) limited to the tangent space is given by
(cid:18) (cid:19)
2Y Y
113 123
Y 2Y
123 223
This shows that the gradient is independent from the Hessian. To compute the independent
part R we can use
Z(0,0,1)=Z′(0,0,1)=Y and Z′(0,0,1)=Y .
3 333 3 333
This shows that R(t) is always equal to the Euclidean Hessian restricted to the tangent space.
By (2.10), it yields
σ σ
Ω= 3R(cid:101)(t 1)= 3(Id−Π t1)Z′′(t 1)(Id−Π t1),
where Π := tt⊤ is the orthogonal projection onto the normal space at point t, and direct
t
algebra gives
∂2Z (cid:88)3
(t)=6 Y tk. (5.4)
∂ti∂tj ijk
k=1
Test statistics One can compute the statistics (t ,λ ) and (t ,λ ) using a gradient descent.
1 1 2 2
The expression of Ω has an explicit form by (5.4). Because we deal with 2×2 matrices, we have
for the spacing test the following identities,
(cid:90) +∞
G (ℓ)= det(uId −Ω/σ)φ(u)du
Ω/σ 2
ℓ
(cid:90) +∞
=
(cid:2) (u2−1)−Tr(Ω/σ)u+det(Ω/σ)+1(cid:3)
φ(u)du
ℓ
=ℓφ(ℓ)−Tr(Ω/σ)φ(ℓ)+(det(Ω/σ)+1)(1−Φ(ℓ)),
with
det(Ω/σ)=(1/9σ2)det(cid:2) (Id −Π )Z′′(t )(Id −Π )+Π (cid:3)
3 t1 1 3 t1 t1
Tr(Ω/σ)=(1/3σ)(cid:0) Tr(Z′′(t ))−t⊤Z′′(t )t (cid:1) .
1 1 1 1
As for the t-spacing test, one can check that
(cid:90) +∞ (cid:16) (cid:112) (cid:17)
H (ℓ)= det(uId −Ω/σ)f u (m−1)/κ du
Ω/σ(cid:98) 2 (cid:98) m−1
ℓ
=(cid:90) +∞ (cid:2) u2−Tr(Ω/σ)u+det(Ω/σ)(cid:3)
f
(cid:16) u(cid:112) (m−1)/κ(cid:17)
du
(cid:98) (cid:98) m−1
ℓ
κ√ m−3 Γ(m)Γ(m−3) (cid:114) κ (cid:20) (cid:114) m−3 (cid:114) m−3 (cid:114) m−3 (cid:21)
= √ 2 2 ℓ f (ℓ )+1−F (ℓ )
(m−2) m−1Γ(m−1)Γ(m−2) m−3 κ m−3 κ m−3 κ
2 2
κ√ m−3 Γ(m)Γ(m−3) (cid:114) m−3
−Tr(Ω/σ) √ 2 2 f (ℓ )
(cid:98) (m−2) m−1Γ(m−1)Γ(m−2) m−3 κ
2 2
(cid:114) (cid:114)
κ (cid:16) m−1 (cid:17)
+det(Ω/σ) 1−F (ℓ ) ,
(cid:98) m−1 m−1 κ
17with m = 10, κ = 7, F (resp. f ) the cumulative distribution function (resp. distribution
α α
density) of the Student t-distribution with α degrees of freedom and
det(Ω/σ)=(1/9σ2)det(cid:2) (Id −Π )Z′′(t )(Id −Π )+Π (cid:3)
(cid:98) (cid:98) 3 t1 1 3 t1 t1
Tr(Ω/σ)=(1/3σ)(cid:0) Tr(Z′′(t ))−t⊤σX′′(t )t (cid:1) .
(cid:98) (cid:98) 1 1 1 1
√
Thet-spacingtest InFigures2and3,thealternativeisgivenbyλ =γ×σ 3log3+3loglog3
0
where γ =1 corresponds to the so-called phase transition in Spiked tensor PCA as presented in
Perry et al. (2020, Theorem 1.3). In Figure 3, the top right panel shows that λ is stochastically
1
increasing as γ increases while the top right panel shows that the distribution of λ remains
2
unchangedformoderatevaluesofγ,sayγ ≥2.Itillustratesthatthespacingbetweenλ andλ
1 2
growslinearlywithγ.Inthemoderateregime,thebottomleftpanelshowsthatλ isdistributed
1
asaGaussianwithmeanλ andvarianceσ2(dashedblackline),whichisthedistributionofZ(t )
0 0
under H (t ,λ ). The bottom right panel shows that t ≃t , for moderate values of γ. It illus-
1 0 0 1 0
trates that (t ,λ ) is weakly close to the distribution of (t ,Z(t )), the (t-)spacing tests detect
1 1 0 0
the alternative H (t ,λ ).
1 0 0
5.3. Two-spiked tensor model
We consider a generalization to higher dimensions and two-spiked tensors of the preceding ex-
ample (Section 5.2) by
Y =ν x⊗k+ν y⊗k+σW ,
0,1 0 0,2 0
where:
• the Euclidian space E is given by k-way n-dimensional symmetric tensors (k ≥ 3,n ≥ 4)
equipped with the dot product
(cid:88)
∀ T,U∈(Rn)⊗k, ⟨T,U⟩ = T U , (5.5)
E i1,...,ik i1,...,ik
i1,...,ik∈[n]
with Euclidean or Frobenius norm ∥T∥2 =⟨T,T⟩ , where [n]=1,...,n;
E
• the noise tensor W ∈(Rn)⊗k is defined by
1 (cid:88)
W = Gπ, (5.6)
k!
π∈Sn
where one considers i.i.d standard Gaussian G for indices 1≤i ,...,i ≤n, S is
i1···ik 1 k n
the set of permutations of the set [n], and Gπ =G . Note that the entries of
W with indices i < i < ··· < i form an
i.i i1.· d·· .ikcollectπ i( oi1 n)·· o·π f( Gik a)
ussian random variables,
1 2 k
namely {W } with distribution N(0,1/(k!)).
i1···ik i1<i2<···<ik
• the eigenvectors x and y are normalized and orthogonal, they belong to the Stiefel man-
0 0
ifold M given by
n,2
M :={(x,y)∈(Sn−1)2 :x⊥y}.
n,2
Using the framework of Section 1.3.2, we are led to consider the profile likelihood random field
t=(θ,x,y)∈[0,2π)×M
n,2
ψ =cos(θ)x⊗k+sin(θ)y⊗k
t
Z(t)=(cid:10) ψ ,Y(cid:11) =cos(θ)⟨x⊗k,Y⟩ +sin(θ) ⟨y⊗k,Y⟩ ,
t E E E
So the relevant manifold is M = S1×M , where the circle S1 is represented by [0,2π). The
n,2
dimension of M is 2n−2. Hence we uncover
Y =λ ψ +σW ,
0 t0
with λ =(cid:113) ν2 +ν2 and θ =arccos(cid:0) ν /λ (cid:1).
0 0,1 0,2 0,1 0
The covariance function at points s=(θ′,u,v) and t=(θ,x,y) is given by
c(s,t)=E(cid:0) Y(t)Y(s)(cid:1)
=cos(θ)cos(θ′)⟨u⊗k,x⊗k⟩ +sin(θ)sin(θ′)⟨v⊗k,y⊗k⟩
E E
=cos(θ)cos(θ′)⟨u,x⟩k+sin(θ)sin(θ′)⟨v,y⟩k. (5.7)
18In particular, it follows that c(t,t)=1 for all t∈M and
c((θ′,u,v),(θ,x,y))=c((θ′,Uu,Uv),(θ,Ux,Uy)), (5.8)
for any orthogonal map U in Rn.
Let us compute the matrix Λ (t). Note that because of the partial invariance by isometry
2
given by (5.8), Λ (t) depends on θ only. Let us define for distinct j ̸=i,ℓ̸=i:
2
Y :=Y , with variance 1;
[i] i,...,i
Y :=Y +···+Y =kY , with variance k;
[i],j i,...,i,j j,i,...,i i,...,i,j
(cid:88)
Y := Y with k−2 occurences of i, one j and one ℓ, with variance k(k−1);
[i],j,ℓ i1,i2,i3
k(k−1)
Y has variance .
[i],j,j 2
BecauseoftheindependencepropertieswithinY,allthesevariablesareindependent.Weconsider
the profile likelihood random field at point (θ,e ,e ) where e and e are the first two elements
1 2 1 2
of the canonical basis. We have
Z(θ,e ,e )=cos(θ)Y +sin(θ)Y .
1 2 [1] [2]
The Euclidean gradient at (θ,e ,e ) is given by
1 2
∂Z(θ,e ,e ) ∂Z(θ,e ,e )
1 2 =kcos(θ)Y , 1 2 =ksin(θ)Y ,
∂x [1] ∂y [2]
1 2
∂Z(θ,e ,e ) ∂Z(θ,e ,e )
1 2 =cos(θ)Y ,j ̸=1, 1 2 =sin(θ)Y ,j ̸=2,
∂x [1],j ∂x [2],j
j j
and
∂Z(θ,e ,e )
1 2 =−sin(θ) Y +cos(θ)Y .
∂θ [1] [2]
Using the following orthonormal parameterization of the tangent space of the Stiefel manifold:
 
0 −√∆21
2
 ∆√21 0 
 2 
 
 ∆ 31 ∆ 32 ,
 . . 
 . . . . 
 
∆ ∆
n1 n2
we get that the Riemannian gradient, including θ, is given by
 
sin(θ)Y +cos(θ)Y ·
[1] [2]
 cos(θ)Y[1],2√−sin(θ)Y[2],1 · 
 2 
∇Z :=  cos(θ)Y [1],3 sin(θ)Y [2],3   . (5.9)
 . . 
 . . . . 
 
cos(θ)Y s Y
[1],n [2],n
Hence, we deduce that
Λ (θ)=diag(cid:0) 1,k/2,cos2(θ),...,sin2(θ),...(cid:1) , (5.10)
2
where the last two terms are repeated n−2 times.
Remark 6. Observe that if θ =kπ/2 then the matrix Λ (θ) is singular. We prove in Lemma 8
2
ofAppendix Cthatalmost surely θ ̸=kπ/2 where t =(θ ,x ,y ),hence Λ (t ) isnon-singular.
1 1 1 1 1 2 1
Also, Theorems 2 or 3 can not be directly applied. But, one can retrieve these results by
omittinganε-neighborhoodofthesepointsintheKac-Riceformulaandpassingtothemonotonic
limit as ε tends to 0.
19LetuscomputethematrixΩ. Letususeforshortcandsforcos(θ)andsin(θ)respectively.
We start by computing the Euclidian Hessian at (θ,e ,e ) as follows
1 2
∂2Z(θ,e ,e ) ∂2Z(θ,e ,e )
1 2 =ck(k−1)Y , 1 2 =s Y ,j,j′ ̸=2,
∂x2 [1] ∂y ∂y [2],j,j′
1 j j′
∂2Z(θ,e ,e ) ∂2Z(θ,e ,e )
1 2 =c(k−1)Y ,j ̸=1, 1 2 =ckY ,
∂x ∂x [1],j ∂θ∂x [1]
1 j 1
∂2Z(θ,e ,e ) ∂2Z(θ,e ,e )
1 2 =cY ,j,j′ ̸=1, 1 2 =ckY ,j ̸=1,
∂x ∂x [1]j,j′ ∂θ∂x [1],j
j j′ j
∂2Z(θ,e ,e ) ∂2Z(θ,e ,e )
1 2 =s k(k−1)Y , 1 2 =s kY ,
∂y2 [2] ∂θ∂y [2]
2 2
∂2Z(θ,e ,e ) ∂2Z(θ,e ,e )
1 2 =s (k−1)Y ,j ̸=2, 1 2 =s Y ,j ̸=2,
∂y ∂y′ [2],j ∂θ∂y [2],j
j j j
and
∂2Z(θ,e ,e )
1 2 =−Z.
∂θ2
Note that all the cross derivatives between x and y vanish. The Riemannian Hessian consists of
four parts (itcan be checkedbyan order two Taylor expansion alongthe tangent space) namely
• The projected Euclidean Hessian: the Euclidean Hessian restricted to the tangent space;
• For each of the three vectors, V ,V ,V , normal to the Stiefel manifold, the part of the
1 2 3
second fundamental form associated to the vector multiplied by the normal derivative.
The 3 components of the second fundamental form are
 
1/2 0 0
V 1 =− 0 Id n−2 0 
0 0 0
 
1/2 0 0
V 2 =− 0 0 0 
0 0 Id
n−2
 0 0 0 
V 3 =− 0 0 √1 2Id n−2  .
0 √1 Id
n−2
0
2
Hence, the expression of the Riemannian Hessian ∇2Z(θ,e ,e ) is
1 2
 Z(θ,e ,e ) A −s(cid:0) Y (cid:1) c(cid:0) Y (cid:1) 
1 2 [1],j j>2 [2],j j>2
  ⋆ B (cid:0) √1 2c(cid:0) Y [1],2,j(cid:1) j (cid:1) (cid:0) −√1 2s(cid:0) Y [2],1 (cid:1),j(cid:1) j  ,
 ⋆ ⋆ c (Y ) −kY Id − cY +sY Id /2 
[1],j,j′ j,j′ [1] n−2 [1],2 [2],1 n−2
(cid:0) (cid:1)
⋆ ⋆ ⋆ s (Y ) −kY Id
[2],j,j′ j,j′ [2] n−2
with A= √1 (cid:0) cY −sY (cid:1), and B =−1(cid:0) ckY +skY (cid:1) + 1(cid:0) cY +sY (cid:1).
2 [2],1 [1],2 2 [1] [2] 2 [1],2,2 [2],1,1
To obtain ∇2Z(t) for a generic point (θ,x,y) belonging to M we have to perform an
n,2
isometry as in (5.8). Using (5.1), we deduce the expression of Ω.
Testing procedures The test statistics involve the function G (resp. and H ), recalled
in Equation (5.2) (resp. and Equation (5.3)), which is given
usingΩ/ aσ
n integral.
ThΩ is/σ(cid:98)integration
can be done numerically.
20Figure5.[Super-Resolution,Section5.4]Thesuper-resolutionrandomfieldZ(·)depictstheobservationof
a point source at location x0 on the interval [0,2π) (spacial domain on the x-axis) with phase θ0 given on the
y-axis. The z-axis corresponds to value of the profile likelihood Z(x,θ) at point x with phase θ.
5.4. Super resolution
In Signal processing, the super resolution phenomenon is the ability to distinguish two close
sources(Diracmasses)fromnoisylowfrequencymeasurementsgivenbyanopticalsystem.This
issue can be tackled using sparse regularization on the space of measures using the so-called
Beurling-LASSO,introducedbyDeCastroandGamboa(2012);Azaïsetal.(2015);Candèsand
Fernandez-Granda (2014); Duval and Peyré (2015).
We consider the framework of Azaïs et al. (2020) where one observes n = 2f + 1 noisy
frequencies between −f and f with f ≥ 1. The ℓth Fourier coefficient of a point source (Dirac
mass) at location x
0
with amplitude λ
0
and phase θ
0
is λ 0eıθ0e−ıℓx0. The observation is given
by Y =(y ) where −f ≤ℓ≤f, they are complex random variables given by
ℓ ℓ
y =λ eıθ0e−ıℓx0 +σW
ℓ 0 ℓ
with λ > 0 the amplitude, t = (x ,θ ) ∈ [0,2π)2 the location and the phase of the source,
0 0 0 0
and σ >0 the standard deviation of the noise. The noise is given by W =ξ +ıξ with ξ
ℓ ℓ,1 ℓ,2 ℓ,p
independent standard Gaussian variables, −f ≤ℓ≤f and p=1,2.
The profile likelihood is given by
Z(t)=⟨Y,ψ ⟩ =λ cos(θ−θ )D (x−x )+σX(t)
t E 0 0 f 0
with t=(x,θ), M =[0,2π)2 is the 2-Torus, E =Cn equipped with the standard complex inner
product, X(·) is a (real valued) centered Gaussian random field with covariance function c(·,·),
D istheDirichletkernel.Thefeaturemap,theDirichletkernelandthecovariancefunctionare
f
given by
ψ =eıθ(eıfx,...,e−ıfx)∈Cn
t
c(t,t′)=cos(θ−θ′)D (x−x′),
f
sin(fu/2)
D (u)= ,
f sin(u/2)
An illustration of the super-resolution (profile likelihood) random field Z(·) is given in Figure 5.
One can check that the assumptions of the present article are satisfied by the super-resolution
random field. The spacing test is given by
σ(α λ +α )φ(λ /σ)+(α σ2−α2)(1−Φ)(λ /σ)
S = 1 1 2 1 1 3 1
σ σ(α λ +α )φ(λ /σ)+(α σ2−α2)(1−Φ)(λ /σ)
1 2 2 2 1 3 2
where α ,α ,α have explicit forms given in (Azaïs et al., 2020, Proposition 10), λ is the
1 2 3 1
maximum of Z(·) and λ
2
the maximum of Z|t1(·). The t-spacing is explicitly described in Azaïs
et al. (2020, Proposition 11).
21Hiddenlayer
h1
x1 vij c k
h2
Output
Input x2 h3 y
layer
h4
x3
h5
Figure 6. a two-layers neural network with c
k
and vi,j real numbers.
5.5. Two-layers neural networks with smooth rectifier
We observe a N-sample (x ,y ) , where x ∈ Rn is the input and y ∈ R the output. The
i i i∈[N] i i
outputissuchthaty =h(x )+σw forsomemeasurabletargetfunctionh : Rn →R,standard
i i i
deviation σ >0, and w standard Gaussian variable.
i
We consider a two layer neural network given by
• (hidden layer) a layer of r neurons with activation a C2 function ρ(·);
• (output layer) and a layer which is simply a mean;
See Figure 6 for an illustration. As a consequence the output is 1(cid:80)r c ρ(⟨v ,x ⟩).
r k=1 k k i
The unknown parameters are the weights c and the directions v of the neurons, for k ∈[r].
k k
Weassumethatv ∈Sn−1.Usingthetrickthatthemeanofrrealsistheleastsquaresestimator,
k
we have to minimize
N r N r
1 (cid:88)(cid:16) 1(cid:88) (cid:17)2 1 (cid:88)(cid:88)(cid:16) (cid:17)2
y − c ρ(⟨v ,x ⟩) = y −c ρ(⟨v ,x ⟩) ,
N i r k k i Nr2 i k k i
i=1 k=1 i=1k=1
andwerecognizethemean-squaretrainingerrorinthelefthandsideandtheEuclideandistance
between Y = (y ) and (c ρ(⟨v ,x ⟩)) on the right hand side in E = RN×r equipped with
i i,k k k i i,k
the dot product
N r r
1 (cid:88)(cid:16)1(cid:88) (cid:17)(cid:16)1(cid:88) (cid:17)
⟨(a ),(b )⟩ = a b .
i,k i,k E N r i,k r i,k
i=1 k=1 k=1
The Gaussian regression problem is Y = H +σW where H = (h(x )) and W = (W ) . To
i i,k i i,k
enter into the framework of Section 1.3.2 we need to perform the following change of variables
c V ρ(⟨v ,x ⟩) ρ(⟨v ,x ⟩)
c ρ(⟨v ,x ⟩)=λ k k i =:λa k i ,
k k i λ V k V
with V2 := (cid:80)N ρ2(⟨v ,x ⟩) and λ2 := V2(c2 +···+c2) are normalizing constants. One can
i=1 k i 1 r
check that t = (a ,...,a ,v ,...,v ) ∈ Sr−1×(Sn−1)r with Sr−1 the Euclidean sphere of Rr
1 r 1 r
and that M=Sr−1×(Sn−1)r. We have
(cid:32) (cid:33)
a ρ(⟨v ,x ⟩)
ψ := k k i and c(s,t):=⟨ψ ,ψ ⟩ ,
t (cid:112)(cid:80)
ρ2(⟨v ,x ⟩)
s t E
i′ k i′ k∈[r]
i∈[N]
and the profile likelihood is Z(t) = ⟨H,ψ ⟩ +σX(t) with X(t) = ⟨W,ψ ⟩ having covariance
t E t E
function c(s,t).
22References
Adler, R. J. and Taylor, J. E. (2009). Random fields and geometry. Springer Science & Business
Media.
Armentano, D., Azaïs, J.-M., and León, J. R. (2023). On a general kac-rice formula for the
measure of a level set. ArXiv preprint, abs/2304.07424.
Auffinger, A. and Ben Arous, G. (2013). Complexity of random smooth functions on the high-
dimensional sphere. The Annals of Probability, 41(6):4214–4247.
Azaïs,J.-M.,DeCastro,Y.,andGamboa,F.(2015). Spikedetectionfrominaccuratesamplings.
Applied and Computational Harmonic Analysis, 38(2):177–195.
Azaïs, J.-M., De Castro, Y., and Mourareau, S. (2017). A rice method proof of the null-space
property over the grassmannian. In Annales de l’Institut Henri Poincaré (B) Probabilités et
Statistiques.
Azaïs,J.-M.,DeCastro,Y.,andMourareau,S.(2020).Testinggaussianprocesswithapplications
to super-resolution. Applied and Computational Harmonic Analysis, 48(1):445–481.
Azaïs, J.-M. and Wschebor, M. (2004). Upper and lower bounds for the tails of the distribu-
tion of the condition number of a gaussian matrix. SIAM Journal on Matrix Analysis and
Applications, 26(2):426–440.
Azaïs, J.-M. and Wschebor, M. (2005). On the distribution of the maximum of a gaussian field
with d parameters. The Annals of Applied Probability, 15(1A):254–278.
Azaïs, J.-M. and Wschebor, M. (2009). Level sets and extrema of random processes and fields.
John Wiley & Sons Inc.
Candès, E. J. and Fernandez-Granda, C. (2014). Towards a mathematical theory of super-
resolution. Communications on pure and applied Mathematics, 67(6):906–956.
Chizat, L. (2022). Sparse optimization on measures with over-parameterized gradient descent.
Mathematical Programming, 194(1):487–532.
De Castro, Y., Gadat, S., Marteau, C., and Maugis-Rabusseau, C. (2021). Supermix: sparse
regularization for mixtures. The Annals of Statistics, 49(3):1779–1809.
De Castro, Y. and Gamboa, F. (2012). Exact reconstruction using beurling minimal extrapola-
tion. Journal of Mathematical Analysis and applications, 395(1):336–354.
Duval, V. and Peyré, G. (2015). Exact support recovery for sparse spikes deconvolution. Foun-
dations of Computational Mathematics, 15(5):1315–1355.
Efron, B., Hastie, T., Johnstone, I., Tibshirani, R., et al. (2004). Least angle regression. The
Annals of statistics, 32(2):407–499.
Lifshits, M. A. (1983). On the absolute continuity of distributions of functionals of random
processes. Theory of Probability & Its Applications, 27(3):600–607.
Perry, A., Wein, A. S., and Bandeira, A. S. (2020). Statistical limits of spiked tensor models.
Annales de l’Institut Henri Poincaré, Probabilités et Statistiques, 56(1):230 – 264.
Steinwart, I. and Christmann, A. (2008). Support vector machines. Springer Science & Business
Media.
23Appendix of Second Maximum of a Gaussian Random Field
and Exact (t-)Spacing test
In this appendix, the Riemannian Hessians are represented by 2-forms.
Appendix A: The helix random field
This section is a proof of Lemma 3. The argument is inspired from Azaïs and Wschebor (2005,
Lemma 4.1). Let t ∈ M be a fixed point. On L2, let Π be the projector on the orthogonal
complement to the subspace generated by (X(t),∇X(t)). Consider h a vector of the tangent
space at point t. Consider the exponential map ε (cid:55)→ exp (εh). This function is well defined on
t
a neighborhood of 0. Hence there exists ε > 0 such that for all ε ∈]ε ,ε [, the point s(ε) :=
0 0 0
exp (εh) ∈ M exists. The function ε (cid:55)→ s(ε) is a parametrization of the geodesic starting at
t
point t with velocity h. We denote by ε (cid:55)→ h(ε) the parallel transport of h along this geodesic.
By Assumption (A ), the Taylor formula of order two gives
1
ε2
X(s(ε))=X(t)+ε⟨∇X(t),h⟩+ ∇2X(s(ε′))[h(ε′),h(ε′)], (A.1)
2
for some ε′ ∈[0,ε], and
ε2
c(s(ε),t)=1+ Λ (t)[h,h]+o(ε2), (A.2)
2 2
by Assumption (A ). From (A.1), we have
2
ε2
Π(X(s(ε)))= R(cid:101)(s(ε′))[h(ε′),h(ε′)],
2
Note also that −Π(X(s(ε))) is the numerator of X|t(s(ε)) while the denominator is given by
−ε2Λ (t)[h,h]+o(ε2) thanks to (A.2). We deduce that
2 2
R(cid:101)(s(ε′))[h(ε′),h(ε′)]
X|t(s(ε))=
Λ (t)[h,h]+o(1)
2
From (2.6) and passing to the limit, we deduce that
R(cid:101)(t)[h,h]
limX|t(s(ε))= ,
ε→0 Λ 2(t)[h,h]
invokingthatR(cid:101) iscontinuousbyregressionformulasandAssumption(A 1),andthatΛ 2(t)[h,h]
is positive by Assumption (A ).
4
For the second and last statement, observe that
limsup X|t(s)≥ sup limX|t(exp (εh))= max
R(cid:101)(t)[h,h]
=
R(cid:101)(t)[h 0,h 0]
s→t ∥h∥2=1ε→0 t ∥h∥2=1Λ 2(t)[h,h] Λ 2(t)[h 0,h 0]
where the vector h
0
exists by continuity of h (cid:55)→ R(cid:101)(t)[h,h]/Λ 2(t)[h,h] on the Euclidean sphere,
which is compact. Now, let δ >0 and let s be a sequence such that s →t and
n n
lim X|t(s )≥limsup X|t(s)−δ.
n
n→∞ s→t
Note that the exponential map is a local diffeomorphism on a neighborhood of the point t.
Hence, there exist a sequence of positive reals ε converging to zero and a sequence h of unit
n n
normtangentvectorssuchthats =exp (ε h ).SincetheEuclideansphereiscompact,wecan
n t n n
extract a sequence such that h converges to unit norm tangent vector h. The Taylor formula
n
gives that
ε2
X(s n)=X(t)+ε n⟨∇X(t),h n⟩+ 2n∇2X(s (cid:101)n)[(cid:101)h n,(cid:101)h n],
ε2
c(s ,t)=1+ nΛ (t)[h ,h ]+o(ε2),
n 2 2 n n n
24for some s
(cid:101)n
on the geodesic between t and s
n
and (cid:101)h
n
the parallel transport at point s
(cid:101)n
of the
tangent vector h along this geodesic. By the Π-projection argument above, we deduce that
n
X|t(s )=
R(cid:101)(s (cid:101)n)[(cid:101)h n,(cid:101)h n]
.
n Λ (t)[h ,h ]+o (1)
2 n n n
Passing to the limit by continuity yields
R(cid:101)(t)[h,h] R(cid:101)(t)[h,h]
max ≥ = lim X|t(s )≥limsup X|t(s)−δ.
∥h∥2=1Λ 2(t)[h,h] Λ 2(t)[h,h] n→∞ n s→t
Note that the most left hand side term does not depends on δ >0, hence
R(cid:101)(t)[h,h]
max ≥limsup X|t(s),
∥h∥2=1Λ 2(t)[h,h]
s→t
which concludes the proof.
Appendix B: Ad-hoc Kac-Rice formula
The paper Azaïs and Wschebor (2009, Theorem 7.2) concerns weighted sum of number of roots
when the weight is a continuous function of time and of the level. This has been extended, by
a monotone convergence argument to the case of lower semi-continuous weights in Armentano
et al. (2023, Section 7)). However this is not sufficient mainly because the regularity of λt as a
2
functionoftisdifficulttocontrol.Forthisreasonwemustusedthefollowingtailoredargument.
Denote by dist(s,t) the geodesic distance between points s and t. Define
∀t∈M, λt := sup σX|t(s) and λ :=λt1 , (B.1)
2,ϵ 2,ϵ 2,ϵ
s∈M s.t.dist(s,t)>ϵ
and note that λt is a continuous function of t. Define a monotone approximation ξ (·) of the
2,ϵ n
indicator function 1{·∈B}. Then
ξ (σX(t),λt ,σR(t))↑1{(σX(t),λt,σR(t)∈B}. (B.2)
n 2,1/n 2
SowecanuseArmentanoetal.(2023,Section7)forinstancetocomputetheexpectationofthe
left side of (B.2). Indeed, all conditions are clear except Condition c) of Armentano et al. (2023,
Section 7) for λ which is detailed hereunder. And then use monotone convergence theorem
2,ϵ
gives the result.
CheckingConditionc)ofArmentanoetal.(2023,Section7) Usingregressionformulas,
the distribution of X(·) under the condition X(t )=u admits the representation
0
X(t)=X(cid:101)(t)+uf(t),
where X(cid:101)(t) corresponds to the distribution conditional to X(t 0) = 0. Our goal is to show the
continuity of the distribution of λ
2
in this representation. The conditioned random field X(cid:101)(·)
satisfies
∀{s̸=t}, Cor(X(s),X(t))<1.
so that, by the Tsirelson theorem, the maximum of X(cid:101)(·)+uf(·) is a.s. unique. The first conse-
quence is the continuity, as a function of u, of (cid:98)t. For t fixed, under X(t 0)=u,
X|t(s)=X(cid:101)|t(s)+ug|t(s), dist(s,t)>ϵ,
withobviousnotation.TheproofofLemma2showsthatg|t(s)isboundeduniformlyinsandt.
This gives the desired continuity, as a function of u, of the distribution of λt and then of λ .
2,ϵ 2,ϵ
25Appendix C: Two-spiked tensor profile likelihood
Lemma 8. With probability 1, it holds that θ ̸=kπ/2.
1
Proof. We prove that θ ̸= 0 a.s., the other cases are equivalent. Note that θ = 0 implies that
1 1
there exists a point x such that f(x ) is the maximum of f(x) := ⟨x⊗k,Y⟩ on Sn−1. As a
1 1 E
consequence the gradient along Sn−1 is zero. Now, note that the derivative with respect to θ
at θ = 0 of Z(θ,x ,y) is zero for every y orthogonal to x . Choosing an orthonormal basis we
1 1
obtain n−1 such derivatives.
Then, we use the Bulinskaya lemma (Azaïs and Wschebor, 2009, Prop 6.11). We denote by
F(x),x∈Sn−1 the random field defined on a set of dimension n−1 with values in R2n−2 given
by the 2n−2 derivatives above. To use the Bulinskaya lemma we need to prove that
(i) the function F(·) has C1 paths,
(ii) the density p is uniformly bounded.
F(x)
Then since the dimension of the parameter set is smaller than the dimension of the image set,
a.s there is no point x such that F(x)=u where u is any value and in particular the value 0.
Condition (i) is clear. To address Condition (ii), we can use the invariance by isometry and
study the density p at the particular value x = e . We can consider the derivatives of f(x)
F(x) 1
ate alongthebasisofthetangentspacee ,...,e .WeobtainY ,...,Y .Thenweconsider
1 2 n 2[1] n[1]
thederivativesinθ ofX(θ,x ,y)withy =e ,...,e WeobtainY ,...,Y .Allthesevariables
1 2 n [2] [n]
are independent with fixed variance so the density of F(x) is bounded.
26