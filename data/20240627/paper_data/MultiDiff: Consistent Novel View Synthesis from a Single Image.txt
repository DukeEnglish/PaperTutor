MultiDiff: Consistent Novel View Synthesis from a Single Image
NormanMu¨ller1 KatjaSchwarz1 BarbaraRoessle2 LorenzoPorzi1 SamuelRotaBulo`1
MatthiasNießner2 PeterKontschieder1
MetaRealityLabsZurich1 TechnicalUniversityofMunich2
Reference Image + Trajectory Generated Views Along Trajectory
Figure1. Givenasingleinputimage, MultiDiffsynthesizesconsistentnovelviewsfollowingadesiredcameratrajectory. Thesesyn-
thesized views harmonize well even in areas unseen from the reference view. Examples from RealEstate10K [85] (top two rows) and
ScanNet[11](bottomrow)testsetsdemonstratethatourmodelcanhandlelargecamerachangesandchallengingperspectives.
Abstract sults–evenforlong-termscenegenerationwithlargecam-
era movements, while reducing inference time by an order
We introduce MultiDiff, a novel approach for consistent ofmagnitude. Foradditionalconsistencyandimagequality
novel view synthesis of scenes from a single RGB image. improvements, we introduce a novel, structured noise dis-
Thetaskofsynthesizingnovelviewsfromasinglereference tribution. OurexperimentalresultsdemonstratethatMulti-
imageishighlyill-posedbynature,asthereexistmultiple, Diff outperforms state-of-the-art methods on the challeng-
plausible explanations for unobserved areas. To address ing, real-world datasets RealEstate10K and ScanNet. Fi-
this issue, we incorporate strong priors in form of monoc- nally, our model naturally supports multi-view consistent
ular depth predictors and video-diffusion models. Monoc- editing without the need for further tuning. Project page:
ular depth enables us to condition our model on warped https://sirwyver.github.io/MultiDiff/
reference images for the target views, increasing geomet-
ric stability. The video-diffusion prior provides a strong
proxy for 3D scenes, allowing the model to learn contin- 1.Introduction
uousandpixel-accuratecorrespondencesacrossgenerated
images. In contrast to approaches relying on autoregres- In this work, we address the challenging and highly ill-
sive image generation that are prone to drifts and error posedtaskofviewextrapolationfromasingleimage. The
accumulation, MultiDiff jointly synthesizes a sequence of goal is to synthesize a set of multiple novel views that are
frames yielding high-quality and multi-view consistent re- diverseandinthemselvesconsistent. Asinput,ourmethod
1
4202
nuJ
62
]VC.sc[
1v42581.6042:viXraonlyrequiresasingleinputimageandauser-definedfree- latent diffusion model-based approach for novel view syn-
formcameratrajectorythatmaydeviatesubstantiallyfrom thesis,givenasinglereferenceimageandapre-definedtar-
thereferenceview. Providingasolutiontothisproblemun- getcameratrajectoryasinput. Weaddressthechallengeof
locks applications in virtual & augmented reality and 3D generating pixel-aligned, multi-view consistent image se-
content creation, where generating immersive and multi- quencesbyincorporatingstrongandcomplementarypriors,
viewcoherentscenesisparamount. significantly constraining the ill-posed nature of the task.
Many existing, state-of-the-art approaches for novel view Geometric stability is improved by integrating a monocu-
synthesis are reconstruction-based (e.g., by optimizing a lar depth prior, where we condition our model on warped
Neural Radiance Field [39] from a fixed number of input reference images for desired novel views, using off-the-
views), and are thus inherently limited in generating high- shelfbutpotentiallynoisymonoculardepthestimators. We
qualitynovelviewsforareaswithoutsufficienttrainingcov- alsointroduceastructurednoisedistributionforimproving
erage. Incontrast, weleveragediffusion-based, generative multi-viewconsistency,applyingtheaforementionedwarp-
approaches [23, 62, 64–66], that are capable of producing ingproceduretothereferenceimagenoiseandhencegen-
high-quality, single images or individual, simple 3D ob- eratingcorrelated3Dnoiseinalloverlappingtargetviews.
jects,duetotheirabilityoflearningpowerful(conditional) By integrating a video diffusion model prior, we are able
image priors. Despite significant progress, these models to compensate for missing and geometrically inconsistent
arestillunabletosynthesizeseveral,multi-viewconsistent reference image warpings due to potential issues with the
viewsoflargescenes. Thisislargelyduetothelackofin- monocular depth estimator. Video priors provide a strong
herent3Dmodelingcapabilities,theabsenceoflarge-scale proxyfor3Dsceneunderstanding,enhancingtemporalcon-
3Dgroundtruthdatasets,butalsotheill-posednatureofthe sistency by largely reducing flickering artifacts – particu-
problem, requiring more sophisticated methodological ad- larlyforlong-trajectoryviewsynthesis.However,theirlack
vances. Ultimately,weareaimingforasolutionthati)gen- ofexplicitcameracontrolmakestheirintegrationnontrivial
eratesseamlesslyalignedandmulti-viewconsistentoutput forviewextrapolation.
images w.r.t. a given input image, ii) maintains both high Inordertoavoiderrorpropagationissuesasobservedwith
variability and fidelity in occluded regions and previously autoregressive models, we synthesize entire sequences of
unseen areas, and iii) extends to camera trajectories well novelviewsinaconcurrentandefficientway. Finally,due
beyond the provided input reference image viewpoint or a toourconditioning,wecanadditionallyeditourgenerated
simplistic360◦panoramicview. scenes,allowingfordirectandintuitiveinteractionwithour
model. Wesummarizeourmaincontributionsasfollows:
Some recent works have approached consistent view ex-
• We address the ill-posed view extrapolation problem by
trapolationbyleveraginganautoregressiveapproach: Look
integrating priors from monocular depth estimators and
Outside the Room [50] is a transformer-based approach
videodiffusionmodelsforlearningpixel-wisecorrespon-
combined with locality constraints w.r.t. the input cameras
dences using novel techniques for spatial-aware condi-
for enforcing consistency among generated frames. Sim-
tioningacrosspredictedsequences.
ilarly, Pose-Guided Diffusion Models [72] apply attention
• We simultaneously predict multiple frames for a target
along epipolar lines to condition a diffusion model. Pho-
sequence, overcoming error accumulation of autoregres-
toNVS[82]alsoproposesanautoregressiveattemptwhere
sivemethods,whileretaininghigherresolutionatreduced
thediffusionmodelisconditionedonareferenceviewanda
computational costs compared to methods directly sam-
specializedrepresentationforrelativecamerageometry. A
plingfromthedistributionof3Dscenes.
significant drawback of autoregressive models is their ten-
• By introducing a novel structured noise distribution, we
dency to error accumulation [34, 55]. Repeatedly condi-
obtainmoremulti-viewconsistentsamplingresults.
tioning the model on its previously generated frames can
turnminoroutputdeficienciesquicklyintoundesirableand
2.RelatedWorks
semantically meaningless results – particularly on longer-
termtrajectories. Incontrast,DiffusionwithForwardMod- Image and Video Diffusion. Diffusion Models
els[69](DFM)trainsadiffusionmodeltodirectlysample (DMs) [23, 63, 67] are powerful generative models
from the distribution of 3D scenes, inherently improving that have achieved state-of-the-art results in uncondi-
3Dconsistency. However,DFMiscomputationallyexpen- tional as well as class- and text-guided image synthe-
sive, limited to low image resolutions, slow at inference, sis [4, 13, 14, 18, 20, 25, 42, 43, 47, 49, 53, 56, 74].
andcannotdirectlyintegrate2Ddiffusionpriors. Thegoal Recently, DMs have been extended to the task of video
ofourworkistoovercomeboththemainlimitationsofau- synthesis[10,16,24,26,37,59]. WhilerecentvideoDMs
toregressiveworksandenablingfastandsignificantlymore can be conditioned on different modalities such as text or
stable,long-termgenerationofnovelviews. images[10,19,76],theydonotenableexplicitcontrolthe
To this end, we propose MultiDiff, a novel and improved, cameraviewpointinthegeneratedvideos. Nonetheless,the
2temporal consistency learnt by these models is a powerful and slow inference. Pose-Guided Diffusion [72] and Pho-
prior that we can leverage to tackle the task of novel view toNVS[82]trainapose-conditioned2Ddiffusionmodelto
synthesis in an underconstrained setting. Specifically, we autoregressivelygenerateframesalongagivencameratra-
use the publicly available VideoCrafter1 [10] to initialize jectory. However, especially for long trajectories, autore-
thecorrespondenceattentionlayersinourpipeline. gressivesamplingispronetoerroraccumulation,leadingto
commonstruggleswithloopclosurewhentakingaMarkov
Regression-Based Models for Novel View Synthesis. assumptionandslowinferenceasitcannotbeparallelized.
The goal of novel view synthesis (NVS) is to produce re- Hence,wedonotuseautoregressivesamplingbutgenerate
alisticimagesofagiveninstanceorscenefrompreviously all images jointly, enabling the model to learn short- and
unseencameraviewpoints. Earlierapproachesrequirehun- long-termcorrespondencesbetweenviews.Instarkcontrast
dreds of posed training images per instance and optimize toMVDiffusionthatalsoperformsjointframesynthesis,we
each instance individually [12, 36, 38–40, 60, 70]. By onlyusedepthfromanoff-the-shelfmonoculardepthesti-
learningpriorsacrossmultipletrainingscenes,morerecent matorwithnogeometriccuesaboutthetargetviews.Multi-
works enable NVS from only one or a few images at in- Diffcanthereforegeneratenovelviewsfromasingleinput
ference [9, 15, 21, 32, 44, 45, 52, 57, 61, 71, 75, 81]. image only. The learnt correspondence attention enables
These methods optimize a regression objective, i.e. an L1 our model to achieve better consistency than state-of-the-
or L2 loss to reconstruct the training images. While this artautoregressiveapproacheswhileachievinghigherimage
allows for impressive results on interpolation near input qualitythanrelatedworks.
views, regression-based NVS approaches struggle with re-
constructionambiguityandlonger-rangeextrapolations[8]. 3.Method
As our goal is to synthesize novel views far beyond ob-
servedviews,weinsteadtrainagenerativemodel. GivenasinglereferenceimageI , ourgoalistogenerate
ref
semanticallyplausible,consistentnovelviewsalongacam-
GenerativeModelsforNovelViewSynthesis. Tobetter eratrajectoryC :={cn}N ,whereeachcameraposecnis
n=1
model reconstruction ambiguity and long-range view ex- relative to the camera of the reference image. To this end,
trapolation, multiple recent works deploy generative mod- weproposeapose-conditional2Ddiffusionmodelwithcor-
elsforNVS.EarlierworksuseGANs[30,31,33,46,78], respondenceattention,i.e.,attentionlayersthatjointlyoper-
VAEs[34],orautoregressivemodels[50,51,54]. Interest- ateonallgeneratedviewsofthetrajectory.Akeychallenge
ingly, GeoGPT [54] directly models long-range 3D corre- in novel-view synthesis for the highly under-constrained
spondencesbetweensourceandtargetviewswithanautore- single-imagesettingistoachieveconsistencyinthelackof
gressivetransformer,demonstratingthatanintermediate3D explicitcorrespondencesupervision. Wethereforeleverage
representationmaynotbeneededforNVSfromasingleim- strong priors that excel at related tasks. Most importantly,
age.Morerecently,diffusionmodelshaveachievedimpres- wenotethatthetaskofvideogenerationiscloselyrelated
siveresultsonobject-centricdata[1,35,41,58,77,80,86]. to our problem setting, where temporal consistency is an
While these works focus on relatively constrained camera intrinsicobjective.
motions around a single object, another line of work ad-
Inthefollowing, weexplainhowwecanintegrateandad-
dressessceneswitharguablymorecomplexcameratrajec-
justavideopriorinconjunctionwithdepthandimagepriors
tories [5, 8, 17, 27, 28, 68, 69, 72, 82]. MVDiffusion [68]
to enable free viewpoint control. Next, we provide a de-
performs image synthesis conditioned on depth maps of
tailed explanation of our conditioning mechanism and the
a given mesh, jointly generating all images of the trajec-
correspondence attention which adds viewpoint control to
tory. To increase consistency, cross-view interactions are
our pipeline. Lastly, we introduce structured noise, which
modelledbycorrespondence-awareattentionlayersthatre-
ports approximate correspondences between frames to ob-
quiregivenpixel-to-pixelcorrespondencesusingGTgeom-
tainmoreconsistentsynthesisresults. Ourpipelineisillus-
etry during training and inference. Our approach instead
tratedinFigure2.
aims at learning those multi-view correspondences, which
allowsustosynthesizenovelviewsalongatrajectorygiven Video Prior We build our generative model on top of
just a single RGB image, without the need for any geo- VideoCrafter [10]. VideoCrafter trains a denoising 3D U-
metric information about the target views. This renders Netinafixedlatentspace,usingapretrainedimageencoder
ourmethodapplicabletoawiderrangeofscenarios,where E andapretrainedimagedecoderDtomaptoandfromla-
noprior3Dreconstructionisavailable. DFM[69]trainsa tentspace,respectively. AtthecoreofVideoCrafterisa3D
diffusionmodeltodirectlysamplefromthedistributionof U-Netwithalternatingspatiallayersandtemporalattention.
3D scenes. Modeling the scene with a 3D representation Thespatiallayersprocesseachframeinabatchindividually
is inherently 3D-consistent, but is computationally expen- whilethetemporalattentionoperatesonallframesjointly.
siveandinpracticelimitsDFMtolowerimageresolutions This pretrained 3D U-Net architecture is a well-suited ini-
3Figure2. MultiDiffispose-conditionaldiffusionmodelfornovelviewsynthesisfromasingleimage. Thediffusionmodelistrainedin
thelatentspaceofafixedauto-encoderwithencoderEanddecoderDandisconditionedonareferenceimageI andacameratrajectory
ref
{cn}. Specifically,weembedN posedtargetimages{In}N intolatentspace,applyforwarddiffusionaccordingtoatimesteptand
n=1
structurednoise{ξn},andtraina3DU-Nettopredict{ξn}fromthenoisyinputs{zn}.Foreachsamplen,theU-Net’spredictionξˆn
is
t t
usedtoreconstructthedenoisedsamplezˆnwhichcanthenbedecodedintothepredictedtargetimageˆIn. WeconditiontheU-Netonthe
t
referenceimagebywarpingI tothenovelviewsusingdepthDˆ fromapretrainedestimatorϕ.Thewarps{In}areencodedintolatent
ref ref ref
representations{yn}andinjectedintotheU-NetinaControlNetinspiredmanner.Wefurtherconditionthemodeldirectlyonthecamera
tgt
poseandanembeddingofthereferenceimageaspartofthesemanticcondition{yn }.
sem
tializationforthetaskofNVSasthetemporallayersalready stackthemaskMn,suitablyresized,alongthechanneldi-
provide an inductive bias towards (temporal) consistency. mension. The resulting tensor is denoted y . Inspired by
tgt
During training, we nevertheless finetune all layers of the ControlNet[83],wecreateacopyofthedownsamplinglay-
U-Net for the novel view synthesis task, where instead of ersofdiffusionU-Nettoextractfeaturesfromy , butwe
tgt
ensuring temporal consistency, the attention layers should prepend a convolutional layer to cope with the additional
establishcorrespondencesbetweenmultipleviews. Hence, maskchannel.
we refer to this type of attention as correspondence atten- Theintermediatefeaturemapsarethenprocessedwithzero-
tion. initializedconvolutionsandaddedtotheoutputsofallspa-
tiallayersofthe3DU-Net. Notethatthisdiffersfromthe
Novel-view synthesis In order to generate novel views procedure proposed in ControlNet, which only inserts the
thatadheretothegivencameratrajectoryC,weneedtocon- feature maps into the decoder. We further do not freeze
dition our pipeline on the target camera poses cn ∈ C. A thelayersofVideoCraftertoenablelearningthecorrespon-
na¨ıveapproachtointegratingcontroloverthecameraview- dence attention. In initial experiments we found that fine-
pointistodirectlyconditionthe3DU-Netoncn,e.g.,via
tuning all layers jointly results in better performance than
cross-attention. Inpractice,weconcatenatethemtothese- usingafixedvideoprior.
manticconditionofVideoCrafterthatconsistsofanembed-
Thewarpingoperationisimplementedbyleveraginganoff-
ding of the reference image and the framerate of the input
the-shelf monocular depth estimator and thus error-prone
sequenceyieldingthesemanticconditioningy .
sem and incomplete. By also passing the reference image and
However,thisformofguidancealoneistooweaktodeliver camera poses to the network in the semantic conditioning
satisfactorynovel-viewsynthesisresults(seeSec.4.2). We y , we enable our approach to follow the provided tra-
sem
thereforeintegrateamonoculardepthpriorinordertocon- jectoryeveninabsenceofoverlapwiththereferenceimage.
strainthehighlyill-posednatureofthetask. Inourexper- Werefertoourablation Sec.4.2foradiscussionaboutthe
iments, we use ZoeDepth [6] pretrained on ScanNet [11] importanceoftheindividualdesigndecisions. Intherestof
andrefertothesupplementarymaterialforablationsabout the section we summarize with y all quantities we condi-
alternativemonocularestimators. WeusethedepthD refes- tionourmodelon,namelyreferenceimageI ref,cameratra-
timatedfromthereferenceimageI ref toimplementawarp- jectories C, and all derived ones (estimated depth, warped
ingfunctionΨnthatenableswarpingimagesfromthecam-
referenceimages,correspondingmasks).
era of the reference image to any other camera cn ∈ C.
WedenotebyIn := Ψ (I )thereferenceimagewarped Structured noise distribution N(y). Images of a 3D
ref n ref
to camera cn and by Mn := Ψ (1) the mask indicating scenecapturedfromdifferentpointofviewsexhibitstrong
n
the area of valid warped pixels in camera cn. To facilitate correlations. Hence, it is beneficial to inject similar corre-
learningthe3Dcorrespondencesacrossthespatialfeatures, lations in the noise ϵ that is used by our diffusion model
foreachviewn,weencodeIn intolatentspaceviaE and tosynthesizethedifferentcameraviews,whichwouldoth-
ref
4erwise be a standard normal multi-variate. This helps en- Method PSNR↑ LPS Ih Po Srt ↓-term FID↓ KID↓ FID↓ KIDL ↓ong F- Vte Drm ↓ mTSED↑
forcing more consistent outputs [48]. Since the correla- DFM[69] 18.10 0.299 36.37 0.010 31.20 0.007 120.2 0.972
Text2Room[27] 15.45 0.370 34.41 0.008 84.10 0.048 163.4 0.932
tionsaremainlydrivenbygeometricconstraints,welever- PhotoNVS[82] 15.66 0.376 26.39 0.006 42.99 0.016 117.5 0.907
MultiDiff(Ours)w/oSN 16.21 0.335 27.26 0.005 30.28 0.006 107.5 0.936
age the warping function Ψ introduced in the previous MultiDiff(Ours) 16.41 0.318 25.30 0.003 28.25 0.004 94.37 0.941
n
Text2Room[27] 14.88 0.458 35.41 0.009 91.92 0.050 178.6 0.837
paragraph to warp a standard normal multi-variate ϵ0 to PhotoNVS[82] 15.01 0.452 26.75 0.005 45.08 0.017 130.4 0.801
MultiDiff(Ours)w/oSN 15.55 0.412 29.49 0.008 33.71 0.010 116.4 0.849
all other camera views in C, while filling the gaps with MultiDiff(Ours) 15.65 0.393 25.90 0.004 30.15 0.006 105.9 0.855
independent Gaussian noise. This yields per-view noise
Table1. QuantitativecomparisononRealEstate10K[85]testse-
ξn :=Mn⊙Ψ (ϵ0)+(1−Mn)⊙ϵn,whereϵnisastan-
n quences. Ourmodelachieveshigherimagequalitythanstate-of-
dard normal multi-variate and Mn is the suitably-resized
the-artbaselinesandcomparableconsistencycomparedtoDFM.
warp-validitymask. Thisprocessyieldsξ :=(ξ1,...,ξN),
whichisregardedasasampleofthestructurednoisedistri-
However,sometimesthenovelviewhaslittleornooverlap
butionwedenotedbyN(y).
withthereferenceimage,makingthewarpedreferenceim-
Training Objective. Let V := {(I0,c0),...,(IN,cN)} age,i.e.conditionyn lessinformative.Tofurtherrefinethe
tgt
beaground-truth,posedvideosequence,whereIn andcn results,wecanrunthesamplingagainonthegeneratedse-
are the nth image and camera pose in the sequence, re- quence,butnowusethewarpofthecloestgeneratedimage
spectively. We assume I0 to be the reference image, i.e. inyn whichinpracticethisslightlyimprovesconsistency.
tgt
I := I0, and assume all cameras to be relative to c0.
ref
Weencodealltargetimagesofthesequenceintoajointla- 4.Experiments
tentrepresentationz := (z1,...,zN),wherezn := E(In),
Inthissection,weevaluatetheperformanceofourmethod
andyistheconditioninginformationencompassingtheen-
forthetaskofconsistentnovelviewsynthesisfromasingle
codedreferenceimage,cameraposesandwarpedreference
referenceimage.
imagesdescribedearlierinthesection.Thedenoisingtrain-
ingobjectivetakesthefollowingformforthetrainingexam- Datasets We compare our methods against state-of-the-
pleV: art approaches on RealEstate10K [85] and ScanNet [11].
Bothdatasetsprovidevideosequencestogetherwithregis-
(cid:104) (cid:105)
L(θ;V):=E ∥ξ−ε (z⊕ ξ;y,t)∥2 , (1) tered camera parameters. RealEstate10K is a large dataset
ξ∼N(y) θ t
t∼U(1,T) ofrealestaterecordingsgatheredfromYouTube. Theclips
typicallyfeaturesmoothcameramovementwithlittletono
wheretissampledfromauniformdistributionU(1,T)and
camera roll or pitch. Most frames further show consider-
ξ is noise sampled from the structured noise distribution
√ √ able coverage of the respective rooms. Following previ-
N(y). The term z⊕ ξ := α z+ 1−α ξ perturbs z
t t t ous works [50, 82], we center-crop and downsample the
withnoiseξaccordingtoavariance-preservingformulation
videosto256pxresolution. ScanNetconsistsof1513hand-
with parameters α , from which ε , i.e. our denoising 3D
t θ heldcapturesofindoorenvironments. Thecameratrajecto-
U-Netwithweightsθ,isrequiredtorecoverξ.
riesfollowascan-patternwhichcancontainrapidchanges
Our model is optimized using Adam by minimizing the
and variation of camera orientation. The resulting frames
training loss function averaged over random batches of
encompass close-up object captures as well as wide room
videosequencessampledfromagivendataset.
recordings, leading to heavy occlusions and an overall di-
Inference. At inference time, we assume to be given a versedatadistribution. Theaforementionedfeaturesmake
reference image I and a sequence of cameras C relative ScanNet extremely challenging for novel view synthesis
ref
to it, which we use to compute the conditions y. We gen- from a single image and our evaluations in Sec. 4 indicate
erateavideosequencefromourmodelbyusingtheDDIM thatadditionalpriorsareverybeneficialinthissetting. We
schedule[64],i.e.startingfromz ∼ N(y)weiteratethe resizetheimagesto256×256andremarkthatScanNetcon-
T
followingequation tains3DmeshesthatweuseforMVDiffusionasitrequires
√ predefinedcorrespondencesbetweenframes.
z := α (z ⊖ ε (z ;y,t))
t−1 t−1 t t θ t
(2)
(cid:112)
+ 1−α ε (z ;y,t), Evaluations Weevaluateourapproachintermsofimage
t−1 θ t
fidelity and consistency of the generated outputs. Similar
unti √lweobtainz 0 bysettingα 0 := 1. Thetermz t⊖ tϵ := to [50], we consider both short-term and long-term view
zt− √1 α− tαtϵ recoverszfromz t assumingnoiseϵ. Thefinal synthesis. Specifically, we randomly select 1k sequences
resultz entailsthesynthesizedviewsinlatentspaceforall with200framesfromthetestsetandevaluatethe50thgen-
0
cameras in C, from which we compute the counterparts in erated frame for short-term and the 200th generated frame
pixelspacebyapplyingthedecoderD. NotethatMultiD- forlong-termviewsynthesisforRealEstate10K.Duetothe
iffcangenerateallimagesofthesequencesimultaneously. faster camera motion, on ScanNet instead we choose the
5
xp821
xp652Reference image Sampled views Time
Figure3.Novelviewsfollowingground-truthtrajectories(right)givenareferenceview(left)onRealEstate10K.Throughourjointmulti-
frame prediction combined with effective priors and conditioning, our sequence of novel views is highly realistic and view-consistent
comparedtothebaselines,whichshowseveredegradationovertime.
Method PSNR↑ LPS Ih Po Srt ↓-term FID↓ KID↓ FID↓ KIDL ↓ong F-t Ver Dm ↓ mTSED↑ accuracy. Here, we compute the mean thresholded sym-
MVDiffusion[68] 13.14 0.439 43.28 0.013 43.58 0.013 186.6 0.506 metricepipolardistance(mTSED)overthepixelthresholds
DFM[69] 16.59 0.444 75.19 0.036 111.9 0.069 167.2 0.912
Text2Room[27] 15.01 0.452 39.87 0.008 82.44 0.0041 173.1 0.812 [1.0,1.5,2.0,2.5,3.0,3.5,4.0] and refer to the supplemen-
PhotoNVS[82] 15.23 0.440 49.19 0.019 75.23 0.038 89.04 0.479
MultiDiff(Ours)w/oSN 15.29 0.372 40.36 0.008 43.61 0.011 80.71 0.752 taryfordetailedresults.
MultiDiff(Ours) 15.50 0.356 38.44 0.007 42.41 0.010 74.10 0.776
MVDiffusion[68] 12.88 0.502 50.18 0.017 51.60 0.018 230.1 0.361
Text2Room[27] 14.32 0.514 46.69 0.014 93.09 0.058 201.1 0.631 Baselines We compare our approach against the state-
PhotoNVS[82] 14.61 0.542 63.21 0.033 96.85 0.059 134.2 0.263
MultiDiff(Ours)w/oSN 14.80 0.445 47.10 0.013 50.84 0.016 119.3 0.529 of-the-art approaches for scene synthesis from a single
MultiDiff(Ours) 15.00 0.431 43.84 0.010 47.11 0.013 114.9 0.576
reference image, including DFM [69], PhotoNVS [82],
Table2.QuantitativecomparisononScanNet[11]testsequences. Text2Room [27] and MVDiffusion [68]. As MVDiffusion
Our approach outperform all baselines at 256px resolution and ispurelytext-conditional,weincorporatethereferenceim-
showssignificantlyhigherimagefidelitycomparedtoDFM. ageduringinferenceasfollows. WeuseDDIMinversionto
obtain the noise corresponding to the reference image and
25th frame for short-term and 100th for long-term evalua- include it in the batch during sampling. Due to the global
tion. In the short-term setting, we report Peak Signal-to- awarenessofMVDiffusion,informationfromthereference
NoiseRatio(PSNR)andperceptualsimilarity(LPIPS)[84] imagecanpropagatetoallgeneratedviews. Pleaseseesup-
as standard metrics for novel view synthesis. To evaluate plementary material for more details. DFM trains a diffu-
the extrapolation capacities in regard of image fidelity, we sion model to directly sample from the distribution of 3D
evaluateFre´chetInceptionDistance[22](FID)andKernel scenes. Unlike our approach, DFM cannot directly inte-
Inception Distance [7] (KID) for long-term settings. To grate 2D diffusion priors and does not generalize well to
measure the video-consistency of the generated trajectory out-of-domain inputs as our experiments on ScanNet in-
images, we compute Fre´chet Video Distance (FVD) [73] dicate. PhotoNVS [82] trains a pose-conditioned 2D dif-
scores. Further, we follow [82] and report the symmet- fusion model to iteratively predict the next frame for a
ricepipolardistance(SED)toquantifyfaithfulnesswithre- given camera trajectory. Text2Room [27] uses an auto-
spect to the provided camera trajectory, i.e., relative pose regressive approach of predicting depth and leveraging a
6
MFD
mooR2txeT
SVNotohP
sruO
xp821
xp652Reference image Sampled views Time
Figure4. GeneratedviewsalongScanNet[11]testsequence(right)givenareferenceview(left). Ourmethodsimultaneouslygenerates
sequencesofnovelviewsthatarebothmorerealisticandmoreview-consistentthanthebaselines,DFMandPhotoNVS,whichsufferfrom
aconsiderableperformancedropacrosslargeviewpointchanges.AlthoughMVDiffusionusessensordepthinput,thegeneratedviewsare
muchlessconsistentwiththereferenceimage(e.g.,colorsofthecushions),comparedtoourgenerations,whichdonotrelyonsensordepth.
depth-conditionalT2Imodeltogeneratenewviewsthatare PhotoNVS.Moreover,ourmodeloutperformsallbaselines
usedtoupdateatexturedmesh. Incontrast,MultiDiffgen- intermsofFVDandachievescomparableresultsonLPIPS
eratesmultipleframesfromtheinputimageinparallel,re- and mTSED with respect to DFM. We note that the Pix-
sulting in better long-term view synthesis and faster infer- elNeRF [81] representation of DFM leads to highly con-
ence: Tosynthesizea128×128frame,PhotoNVSrequires sistentresults,thereforegoodscoresonpixel-levelmetrics
≈45s,DFM≈17swhileoursonlytakes≈1s. like short-term PSNR, however, this comes at the cost of
sharpness(asreflectedinFID/KID).
4.1.ConsistentNovelViewSynthesis
Byleveragingstrongimage-andvideo-diffusionpriors,our
Comparison against state of the art We quantitatively methodachievesclearimprovementsoverthebaselineson
evaluateourapproachonthetaskofconsistentnovel-view ScanNet:AsshowninTab.2,MultiDiffoutperformsMVD-
synthesisfromasinglereferenceonRealEstate10K[85]in iffusion on short- and long-term metrics, indicating our
Tab. 1 and ScanNet in Tab. 2. Since DFM does not sup- model’s ability to learn long-term correspondences even
port higher resolutions than 128px due to memory limita- without relying on ground-truth geometry. In comparison
tions,whereastheothermethodsrunatadefaultresolution to DFM, Text2Room and PhotoNVS, we observe strong
of256px,weperformseparateanalysesatbothresolutions. photometric short- and long-term improvements over all
On RealEstate10K, we observe that our method achieves baselines. Figs. 3 and 4 show qualitative comparisons on
consistentlybetterFIDandKIDscoresonbothshort-term, RealEstate10KandScanNet,respectively. Itstandsoutthat
aswellaslong-termevaluations: Theshort-termFIDcom- ourmethodsynthesisesrealisticandconsistentnovelviews
pared to DFM improves from 36.37 to 25.30 (at 128px), even across large viewpoint changes, where the quality of
while the long-term FID improves by 33% compared to thebaselinesdropsnoticeable.
7
noisuffiDVM
MFD
mooR2txeT
SVNotohP
sruOShort-term Long-term
Method PSNR↑ LPIPS↓ FID↓ FID↓ FVD↓ mTSED↑ Reference image Sampled views
MultiDiffnoprior 14.29 0.493 63.56 85.30 236.8 0.587
MultiDiffnovid. 14.68 0.552 37.05 38.43 214.9 0.728
MultiDiffnowarp 13.65 0.557 47.42 58.30 181.1 0.484
MultiDiffnopose 15.53 0.417 27.84 32.25 120.26 0.624
MultiDiff(Ours) 15.65 0.393 25.90 30.15 105.9 0.855
Table 3. Ablation of individual components of our pipeline on
RealEstate10K[85]testsequencesat256pxresolution.
4.2.Ablations
Weshowthecontributionsofindividualcomponentsofour
approachinTab.3andrefertothesupplementarymaterial
formorequalitativecomparisons.
Importanceofpriors AsdescribedinSec.3,weinitial-
Figure5.Withoutstructurednoise(”MultiDiffw/oSN”),thecolor
ize our model with weights obtained by training on large-
ofthediningtableisnotmaintainedw.r.t.thereferenceimage.
scaleimageandvideodatasets. Tostudytheimportanceof
thosepriorsforthetaskofconsistentnovel-viewsynthesis Reference image + mask Sampled views
fromasingleimage,weablatethemonebyone: Asshown
inTab.3,trainingfromscratch(”MultiDiffnoprior”)leads
to strong degradation of image quality, as well as overall
consistency. Removing the video diffusion prior (”Multi-
Diff no vid.”) has strong influence on the long-term con-
sistency(mTSEDdecreasesby12.7%),aswellasthevideo
quality(FVDincreasesbymorethan120%).Wefurtherab-
late the monocular depth estimates on the reference image
asconditiontoourmodelin”MultiDiffnowarp”(Tab.3).
The drop in mTSED from 85.5% to 48.4% indicates that
themodelwithoutreferencewarpsisnotabletocloselyad-
heretotheinputtrajectory. Besidesthedepth-warpingsof
thereferenceimage,ourmethodusesrelativecameraposes
to synthesize images from the desired target poses. When Figure 6. Consistent masking-based editing results on ScanNet
removingthismodality(”MultiDiffnopose”),wenoticeef- testimages.
fectonlong-termgenerationbecomesapparent,wherethere
isminimaltonowarp-guidancetoinformaboutthedesired performsconsistentcompletioninthoseregions. Weshow
cameraposes,hencemTSEDdecreasesfrom0.85to0.62. examplesonScanNettestimagesinFig.6andrefertothe
supplementarymaterialformorequalitativeresults.
Importanceofstructurednoise AsdescribedinSec.3,
5.Conclusions
we introduce structured noise by warping the initial noise
consistently between target views according the depth es- Inthispaper,weintroduceMultiDiff,anovelapproachfor
timates of the reference image. We measure the effect of view extrapolation from a single input image. We identify
noisewarpinginTab.1andTab.2(”MultiDiffw/oSN”)on videopriorsasapowerfulproxyforthissettinganddemon-
RealEstate10KandScanNettrajectories. Onbothdatasets, stratehowtheycanbeincorporatedandadaptedbyconvert-
we observe that the structured noise leads to significantly ing temporal attention to correspondence attention. With
more consistent and higher quality synthesis results. We monoculardepthcues,wefacilitatelearningimprovedcor-
showtheeffectofnoise-warpinginFig.5. respondencesbyconditioningourmodelonreferenceviews
warpedw.r.t.thetargetcameratrajectory. Ourexperiments
4.3.ConsistentEditing on RealEstate10k and ScanNet show significant improve-
mentsoverrelevantbaselines,withparticulargainsonlong-
In contrast to existing works such as DFM or PhotoNVS,
termsequencegenerationandoverallinferencespeed.
our approach directly supports consistent editing without
task-specifictraining. Duringtraining,ourmodelistasked
Acknowledgements
to synthesize consistent novel views even in absence of
meaningfulreferencewarps. Bymaskinganareainarefer- MatthiasNießnerwassupportedbytheERCStartingGrant
enceimagethatshouldnotbewarped,ourmodelnaturally Scan2CAD(804724).
8
ffiDitluM
NS
o/w
ffiDitluMReferences [15] YilunDu,CameronSmith,AyushTewari,andVincentSitz-
mann. Learning to render novel views from wide-baseline
[1] TitasAnciukevicius,ZexiangXu,MatthewFisher,PaulHen-
stereopairs. InCVPR,2023. 3
derson, Hakan Bilen, Niloy J. Mitra, and Paul Guerrero.
[16] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
RenderDiffusion:Imagediffusionfor3Dreconstruction,in-
Jonathan Granskog, and Anastasis Germanidis. Structure
paintingandgeneration. arXiv,2022. 3
and content-guided video synthesis with diffusion models.
[2] GwangbinBae,IgnasBudvytis,andRobertoCipolla. Iron-
InICCV,2023. 2
depth: Iterative refinement of single-view depth using sur-
[17] RafailFridman,AmitAbecasis,YoniKasten,andTaliDekel.
face normal and its uncertainty. In British Machine Vision
Scenescape: Text-driven consistent scene generation. In
Conference(BMVC),2022. 12,16
NeurIPS,2023. 3
[3] Max Bain, Arsha Nagrani, Gu¨l Varol, and Andrew Zisser-
man. Frozenintime: Ajointvideoandimageencoderfor [18] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
end-to-end retrieval. In IEEE International Conference on Zhang, DongdongChen, LuYuan, andBainingGuo. Vec-
ComputerVision,2021. 12 torquantizeddiffusionmodelfortext-to-imagesynthesis.In
CVPR,2022. 2
[4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, [19] XianfanGu,ChuanWen,JiamingSong,andYangGao.Seer:
SamuliLaine,BryanCatanzaro,etal. ediffi: Text-to-image Language instructed video prediction with latent diffusion
diffusionmodelswithanensembleofexpertdenoisers.arXiv models. arXivpreprintarXiv:2303.14897,2023. 2
preprintarXiv:2211.01324,2022. 2 [20] YingqingHe,ShaoshuYang,HaoxinChen,XiaodongCun,
[5] MiguelA´ngelBautista,PengshengGuo,SamiraAbnar,Wal- MenghanXia, YongZhang, XintaoWang, RanHe, Qifeng
ter Talbott, Alexander T Toshev, Zhuoyuan Chen, Laurent Chen, and Ying Shan. Scalecrafter: Tuning-free higher-
Dinh,ShuangfeiZhai,HanlinGoh,DanielUlbricht,Afshin resolution visual generation with diffusion models. arXiv
Dehghan, and Joshua M. Susskind. GAUDI: A neural ar- preprintarXiv:2310.07702,2023. 2
chitect for immersive 3D scene generation. arXiv preprint [21] Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Ro-
arXiv:2207.13751,2022. 3 man Shapovalov, Tobias Ritschel, Andrea Vedaldi, and
[6] ShariqFarooqBhat,ReinerBirkl,DianaWofk,PeterWonka, David Novotny. Unsupervised learning of 3d object cat-
andMatthiasMu¨ller. Zoedepth: Zero-shottransferbycom- egories from videos in the wild. In Proceedings of the
biningrelativeandmetricdepth,2023. 4,16 IEEE/CVF Conference on Computer Vision and Pattern
[7] Mikołaj Bin´kowski, Dougal J. Sutherland, Michael Arbel, Recognition(CVPR),pages4700–4709,2021. 3
andArthurGretton.DemystifyingMMDGANs.InInterna- [22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
tionalConferenceonLearningRepresentations,2018. 6 Bernhard Nessler, and Sepp Hochreiter. GANs trained by
[8] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan- atwotime-scaleupdateruleconvergetoalocalnashequi-
der W. Bergman, Jeong Joon Park, Axel Levy, Miika Ait- librium.Advancesinneuralinformationprocessingsystems,
tala,ShaliniDeMello,TeroKarras,andGordonWetzstein. 30,2017. 6
GeNVS:Generativenovelviewsynthesiswith3D-awaredif-
[23] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
fusionmodels. InCVPR,2023. 3
sionprobabilisticmodels. NeurIPS,2020. 2
[9] AnpeiChen,ZexiangXu,FuqiangZhao,XiaoshuaiZhang,
[24] Jonathan Ho, WilliamChan, ChitwanSaharia, JayWhang,
FanboXiang,JingyiYu,andHaoSu.Mvsnerf:Fastgeneral-
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
izableradiancefieldreconstructionfrommulti-viewstereo.
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
InCVPR,2021. 3
video:Highdefinitionvideogenerationwithdiffusionmod-
[10] VideoCraftercontributors. Videocrafter. Github.Accessed
els. arXivpreprintarXiv:2210.02303,2022. 2
October 15, 2023 [Online] https://github.com/
[25] JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,
AILab-CVC/VideoCrafter. 2,3,12
MohammadNorouzi,andTimSalimans.Cascadeddiffusion
[11] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
modelsforhighfidelityimagegeneration. 23:47–1,2022. 2
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
[26] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Richly-annotated 3d reconstructions of indoor scenes. In
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
ProceedingsoftheIEEEconferenceoncomputervisionand
fusionmodels. ICLR,2022. 2
patternrecognition, pages5828–5839, 2017. 1, 4, 5, 6, 7,
12,15 [27] Lukas Ho¨llein, Ang Cao, Andrew Owens, Justin Johnson,
[12] Peng Dai, Yinda Zhang, Zhuwen Li, Shuaicheng Liu, and and Matthias Nießner. Text2room: Extracting textured 3d
Bing Zeng. Neural point cloud rendering via multi-plane meshesfrom2dtext-to-imagemodels. InICCV,2023. 3,6
projection. InCVPR,2020. 3 [28] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten
[13] PrafullaDhariwalandAlexanderNichol. Diffusionmodels Kreis,KatjaSchwarz,DaiqingLi,RobinRombach,Antonio
beatgansonimagesynthesis. AdvancesinNeuralInforma- Torralba, and Sanja Fidler. Neuralfield-ldm: Scene gener-
tionProcessingSystems,34:8780–8794,2021. 2 ation with hierarchical latent diffusion models. In CVPR,
[14] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score- 2023. 3
based generative modeling with critically-damped langevin [29] Diederik P. Kingma and Jimmy Ba. Adam: A method for
diffusion. InICLR,2022. 2 stochasticoptimization. InICLR,2015. 12,14
9[30] Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, ticimagegenerationandeditingwithtext-guideddiffusion
andPeterAnderson.Pathdreamer:Aworldmodelforindoor models. 2022. 2
navigation. InICCV,2021. 3 [44] Michael Niemeyer, Lars M. Mescheder, Michael Oechsle,
[31] JingYuKoh,HarshAgrawal,DhruvBatra,RichardTucker, and Andreas Geiger. Differentiable volumetric rendering:
AustinWaters,HonglakLee,YinfeiYang,JasonBaldridge, Learningimplicit3drepresentationswithout3dsupervision.
andPeterAnderson.Simpleandeffectivesynthesisofindoor InCVPR,2020. 3
3Dscenes. AAAI,2023. 3 [45] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall,
[32] Jona´s Kulha´nek, Erik Derner, Torsten Sattler, and Robert Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan.
Babuska. Viewformer: Nerf-freeneuralrenderingfromfew Regnerf: Regularizing neural radiance fields for view syn-
imagesusingtransformers. InECCV,2022. 3 thesisfromsparseinputs. InCVPR,2022. 3
[33] Zhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo [46] Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona,
Kanazawa. Infinitenature-zero: Learning perpetual view and Federico Tombari. Shape, pose, and appearance from
generationofnaturalscenesfromsingleimages. InECCV, asingleimageviabootstrappedradiancefieldinversion. In
2022. 3 CVPR,2023. 3
[34] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh [47] Dustin Podell, Zion English, Kyle Lacey, Andreas
Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and
nature: Perpetual view generation of natural scenes from a RobinRombach. SDXL:improvinglatentdiffusionmodels
singleimage. InICCV,2021. 2,3 for high-resolution image synthesis. CoRR, arxiv preprint
[35] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok- arxiv:2307.01952,2023. 2
makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: [48] HaonanQiu,MenghanXia,YongZhang,YingqingHe,Xin-
Zero-shotoneimageto3dobject. InICCV,2023. 3 taoWang,YingShan,andZiweiLiu.Freenoise:Tuning-free
[36] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel longervideodiffusionvianoiserescheduling,2023. 5
Schwartz,AndreasLehrmann,andYaserSheikh.Neuralvol- [49] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
umes: Learningdynamicrenderablevolumesfromimages. and Mark Chen. Hierarchical text-conditional image gen-
arXivpreprintarXiv:1906.07751,2019. 3 erationwithcliplatents. arXivpreprintarXiv:2204.06125,
[37] ZhengxiongLuo,DayouChen,YingyaZhang,YanHuang, 2022. 2
LiangWang,YujunShen,DeliZhao,JingrenZhou,andTie- [50] XuanchiRenandXiaolongWang. Lookoutsidetheroom:
niu Tan. Videofusion: Decomposed diffusion models for Synthesizingaconsistentlong-term3Dscenevideofroma
high-qualityvideogeneration. InCVPR,2023. 2 singleimage. InCVPR,2022. 2,3,5
[38] MoustafaMeshry,DanB.Goldman,SamehKhamis,Hugues [51] ChrisRockwell,DavidFFouhey,andJustinJohnson. Pixel-
Hoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin- synth: Generatinga3D-consistentexperiencefromasingle
Brualla. Neural rerendering in the wild. In CVPR, 2019. image. InICCV,2021. 3
3 [52] Barbara Roessle, Jonathan T. Barron, Ben Mildenhall,
[39] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, PratulP.Srinivasan,andMatthiasNießner. Densedepthpri-
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf: ors for neural radiance fields from sparse input views. In
Representingscenesasneuralradiancefieldsforviewsyn- ProceedingsoftheIEEE/CVFConferenceonComputerVi-
thesis. In European conference on computer vision, pages sionandPatternRecognition(CVPR),2022. 3
405–421.Springer,2020. 2 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
[40] Norman Mu¨ller, Andrea Simonelli, Lorenzo Porzi, PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
Samuel Rota Bulo, Matthias Nießner, and Peter thesiswithlatentdiffusionmodels,2021. 2,12
Kontschieder. Autorf: Learning 3d object radiance [54] Robin Rombach, Patrick Esser, and Bjo¨rn Ommer.
fields from single view observations. In Proceedings of Geometry-freeviewsynthesis: Transformersandno3Dpri-
theIEEE/CVFConferenceonComputerVisionandPattern ors. InICCV,2021. 3
Recognition(CVPR),2022. 3 [55] StephaneRoss,GeoffreyGordon,andDrewBagnell. Are-
[41] Norman Mu¨ller, Yawar Siddiqui, Lorenzo Porzi, ductionofimitationlearningandstructuredpredictiontono-
Samuel Rota Bulo, Peter Kontschieder, and Matthias regretonlinelearning. InInt.Conf.Art.Intell.Stat.PMLR,
Nießner. Diffrf: Rendering-guided 3d radiance field 2011. 2
diffusion. In Proceedings of the IEEE/CVF Conference [56] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
on Computer Vision and Pattern Recognition, pages Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
4328–4338,2023. 3 RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
[42] Alexander Quinn Nichol and Prafulla Dhariwal. Improved etal.Photorealistictext-to-imagediffusionmodelswithdeep
denoising diffusion probabilistic models. In International languageunderstanding. InNeurIPS,2022. 2
ConferenceonMachineLearning,pages8162–8171.PMLR, [57] Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs
2021. 2 Bergmann, Klaus Greff, Noha Radwan, Suhani Vora,
[43] AlexanderQuinnNichol,PrafullaDhariwal,AdityaRamesh, MarioLucic,DanielDuckworth,AlexeyDosovitskiy,Jakob
Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Uszkoreit, Thomas Funkhouser, and Andrea Tagliasacchi.
Sutskever, and Mark Chen. Glide: Towards photorealis- Scenerepresentationtransformer:Geometry-freenovelview
10synthesisthroughset-latentscenerepresentations. InCVPR, [73] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,
2022. 3 RaphaelMarinier,MarcinMichalski,andSylvainGelly.To-
[58] KyleSargent,ZizhangLi,TanmayShah,CharlesHerrmann, wardsaccurategenerativemodelsofvideo:Anewmetric&
Hong-XingYu,YunzhiZhang,EricRyanChan,DmitryLa- challenges. arXivpreprintarXiv:1812.01717,2018. 6
gun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. ZeroNVS: [74] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based
Zero-shot360-degreeviewsynthesisfromasinglerealim- generativemodelinginlatentspace. InNeurIPS,2021. 2
age. arXivpreprintarXiv:2310.17994,2023. 3 [75] QianqianWang,ZhichengWang,KyleGenova,PratulSrini-
[59] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn, vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet:
OranGafni, etal. Make-a-video: Text-to-videogeneration Learningmulti-viewimage-basedrendering.InCVPR,2021.
withouttext-videodata. InICLR,2023. 2 3
[60] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias [76] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,
Nießner, GordonWetzstein, andMichaelZollho¨fer. Deep- Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,
voxels: Learning persistent 3d feature embeddings. In and Jingren Zhou. Videocomposer: Compositional video
CVPR,2019. 3 synthesis with motion controllability. arXiv preprint
[61] Vincent Sitzmann, Michael Zollho¨fer, and Gordon Wet- arXiv:2306.02018,2023. 2
zstein. Scene representation networks: Continuous 3d- [77] Daniel Watson, William Chan, Ricardo Martin-Brualla,
structure-awareneuralscenerepresentations. arXivpreprint Jonathan Ho, Andrea Tagliasacchi, and Mohammad
arXiv:1906.01618,2019. 3 Norouzi. Novel view synthesis with diffusion models. In
[62] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan, ICLR,2023. 3
and Surya Ganguli. Deep unsupervised learning using
[78] OliviaWiles,GeorgiaGkioxari,RichardSzeliski,andJustin
nonequilibriumthermodynamics.pages2256–2265.PMLR,
Johnson. SynSin: End-to-endviewsynthesisfromasingle
2015. 2
image. InCVPR,2020. 3
[63] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah-
[79] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong
eswaranathan, and Surya Ganguli. Deep unsupervised
Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor
learning using nonequilibrium thermodynamics. In ICML,
Verbin, Jonathan T. Barron, Ben Poole, and Aleksander
2015. 2
Holynski.Reconfusion:3dreconstructionwithdiffusionpri-
[64] Jiaming Song, Chenlin Meng, and Stefano Ermon.
ors. arXiv,2023. 16
Denoising diffusion implicit models. arXiv preprint
[80] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin
arXiv:2010.02502,2020. 2,5,14
Tong. 3d-awareimagegenerationusing2ddiffusionmod-
[65] Yang Song and Stefano Ermon. Generative modeling
els. InICCV,2023. 3
by estimating gradients of the data distribution. CoRR,
[81] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.
abs/1907.05600,2019.
pixelNeRF:Neuralradiancefieldsfromoneorfewimages.
[66] Yang Song and Stefano Ermon. Improved techniques for
InCVPR,pages4578–4587,2021. 3,7,16
trainingscore-basedgenerativemodels. Advancesinneural
[82] JasonJ.Yu,FereshtehForghani,KonstantinosG.Derpanis,
informationprocessingsystems,33:12438–12448,2020. 2
andMarcusA.Brubaker. Long-termphotometricconsistent
[67] YangSong,JaschaSohl-Dickstein,DiederikPKingma,Ab-
novelviewsynthesiswithdiffusionmodels. InProceedings
hishekKumar,StefanoErmon,andBenPoole. Score-based
oftheInternationalConferenceonComputerVision(ICCV),
generative modeling through stochastic differential equa-
2023. 2,3,5,6,12,16
tions. arXivpreprintarXiv:2011.13456,2021. 2
[83] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
[68] ShitaoTang,FuayngZhang,JiachengChen,PengWang,and
conditionalcontroltotext-to-imagediffusionmodels,2023.
FurukawaYasutaka. Mvdiffusion: Enablingholisticmulti-
4
viewimagegenerationwithcorrespondence-awarediffusion.
[84] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
arXivpreprint2307.01097,2023. 3,6
and Oliver Wang. The unreasonable effectiveness of deep
[69] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon
featuresasaperceptualmetric. InCVPR,2018. 6
Rezchikov,JoshuaB.Tenenbaum,Fre´doDurand,WilliamT.
Freeman, and Vincent Sitzmann. Diffusion with forward [85] TinghuiZhou, RichardTucker, JohnFlynn, GrahamFyffe,
models: Solvingstochasticinverseproblemswithoutdirect andNoahSnavely.Stereomagnification:Learningviewsyn-
supervision. NeurIPS,2023. 2,3,5,6,12 thesisusingmultiplaneimages. InACMTOG,2018. 1,5,7,
[70] JustusThies,MichaelZollho¨fer,andMatthiasNießner. De- 8,12,13
ferred neural rendering: image synthesis using neural tex- [86] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-
tures. ACMTOG,2019. 3 tillingview-conditioneddiffusionfor3dreconstruction. In
[71] AlexTrevithickandBoYang. Grf: Learningageneralra- CVPR,2023. 3
diance field for 3d scene representation and rendering. In
ICCV,2021. 3
[72] Hung-YuTseng,QinboLi,ChangilKim,SuhibAlsisan,Jia-
BinHuang, andJohannesKopf. Consistentviewsynthesis
withpose-guideddiffusionmodels. InCVPR,2023. 2,3
11Appendix
PhotoNVS We use the official implementation of
the authors https://github.com/YorkUCVIL/
In this appendix, we provide additional qualitative and Photoconsistent-NVS.gitandtheprovidedcheck-
quantitative results and discuss training and evaluation de-
pointonRealestate10K.OnScanNet,weusethepre-trained
tails.
VQGAN provided by the authors and train the model at
256×256resolutionfor500Kiterationsusing8NVIDIA
A.Additionalqualitativeresults A100-SXM4-80BGPUswithaneffectivebatchsizeof64.
We provide additional qualitative comparisons with the
MVDiffusion Since MVDiffusion is a purely text-
baselines on RealEstate10K [85] in Fig. 8 as well as on
conditional model, we adapt the official implemen-
ScanNet[11]inFig.10.WhilePhotoNVS[82]accumulates
tation (https://github.com/Tangshitao/
errorsovertheautoregressivesamplingprocess,ourmodel
MVDiffusion.git) to accept a reference image at
synthesizes realistic images for all target poses jointly. In
inference time. We encode the reference image into latent
comparison to DFM [69], our approach leverages strong
space and then encode it into the diffusion model’s Gaus-
image-andvideo-priorstoachievenoticeablyhigherimage
sian prior space using DDIM inversion. During sampling,
fidelity.
the encoded reference image is added to the batch. Since
Furthermore, we demonstrate the stochasticity of our ap- MVDiffusion uses attention layers that operate on all
proachinFig.9whereusingthesamereferenceimageand images in the batch jointly, the reference frame affects the
targetposes, ourprobabilisticmethodsynthesizesmultiple sampling for all images. However, during sampling the
plausiblenovelviews. score estimate is calculated using the full batch, while for
Wealsopresentanin-the-wildand360°trajectoryinFig.7. DDIM inversion we can only obtain the score estimate for
the reference image. In practice, sampling does hence not
Reference image Sampled views reproduce the reference image faithfully. We address this
issue by additionally optimizing the reference latent after
each denoising step to match the reference image. For the
optimization at each sampling step, we use Adam [29]
withalearningrateof0.1andtrainwithanL2-Lossanda
perceptuallossfor10iterations.
Text2Room We use the official implementation of
the authors (https://github.com/lukasHoel/
text2room)whichalsosupportsimage-conditionalgen-
eration. WefollowtheoriginalsetupanduseIronDepth[2]
for depth prediction and StableDiffusion2 inpainting
(https://huggingface.co/stabilityai/
Figure 7. Out-of-distribution examples: In-the-wild image with stable-diffusion-2-inpainting) for image
unknowncameraparametersand360°camerarotation.
inpainting. Since Text2Room formulates the problem as
pure depth-to-image/inpainting task, the same pretrained
checkpoints can be used for both datasets, RealEstate10K
B.Implementationdetails
andScanNet,andnoadditionaltrainingisrequired.
B.1.Baselines B.2.MultiDiff
DFM We use the official implementation of the au- Training details For the encoding of the warped refer-
thors(https://github.com/ayushtewari/DFM. ence images, we use the encoder layers of the pre-trained
git). On Realestate10K, we evaluate the provided pre- text-to-image model Stable Diffusion 1.5 [53] and use the
trained checkpoint. On ScanNet, we train a model provided VQ-VAE for latent encoding and decoding. We
from scratch, following the official instructions for initializethedenoisinglayersofourU-Netmodelwiththe
RealEstate10k.Morespecifically,wefirsttrainthemodelat pre-trainedweightsofVideoCrafter[10],alatentvideodif-
resolution64×64for75Kiterationswithatotalbatchsize fusion model trained on large scale video data [3]. The
of 16 on 8 NVIDIA A100-SXM4-80GB GPUs. Next, we temporal attention layers serve as strong prior for consis-
fine-tune the model at resolution 128×128 for 60K itera- tency - see performance of ”MultiDiff no vid.” in Table 3
tionsusingatotalbatchsizeof8.Atbothresolutions,larger of the main paper and Fig. 15 for a qualitative compari-
batchsizesdidnotfitinthe80GBmemoryoftheGPUs. son. Nevertheless, wefine-tunealllayersoftheU-Netfor
12
dliw-eht-nI
noitator
°063Reference Sampled views
image
Figure8.AdditionalqualitativecomparisonresultsonRealEstate10K[85].
13
MFD
SVNotohP
sruO
MFD
SVNotohP
sruO
MFD
SVNotohP
sruOSampled views
/Users/
Reference image
Reference image
Figure9.Differentsamplesgeneratedbyourprobabilisticapproachusingthesamereferenceimageandtargettrajectory.
the novel view synthesis task to enable the attention lay- Net). The described refinement is applied on poses where
ers to learn correspondences between multiple views. For thewarpingoverlapisbelow20%, whichoccursin51.8%
training, we use Adam [29] with a learning rate of 1e-05 (47.1%)ofcases.
and batch size of 6 with 16 target views per batch at a
resolution of 256×256. Using 8 NVIDIA A100-SXM4-
C.Evaluation
80B GPUs with an effective batch size of 48, we train for
300Kiterations.WeuseDDPM[64]noiseschedulingusing Dataprocessing OnRealEstate10K,werandomlyselect
t = 1000 time steps for denoising and perform evaluation 1Ksequenceswithatleast200frames. Forevaluation,for
usingDDIM[64]samplingwith35steps. eachsequence, wechoosearandomstartingframeatleast
For noise warping, we found that using nearest-neighbor 200framesaheadofthelastframe. Weselect16framesfor
with a receptive field size of 4px at 256px resolution gave evaluation that we uniformly distribute within the interval
the best results. This limited receptive range ensures that of 200 views from the starting frame. Following previous
the noise distribution remains roughly normal, preventing evaluation protocols, for short-term evaluation, we set the
strongzoomsfromresultinginafewpixelscoveringlarge 5thviewtobe50framesafterthestartingviewintheorig-
imageportions. inal video. For long-term evaluation, the last view corre-
spondstothe200thframeafterthestartingframe.
Inference details Using the estimated depth maps with OnScanNet,foreachofthe100testscenes,wesample10
nearest-neighbor interpolation, we calculated the average starting views ensuring at least 100 frames offset from the
warping overlap of the initial image with the last frame last frame in the recordings, resulting in a set of 1K test
in the sequence: 20.4% (24.7%) on RealEstate10K (Scan- sequences. SincethecameramovementinScanNetrecord-
14
1
elpmaS
2
elpmaS
1
elpmaS
2
elpmaSReference image Sampled views Time
Figure10.AdditionalqualitativecomparisonresultsonScanNet[11].NotethatMVDiffusionrequiresthescenemeshforinference.
15
noisuffiDVM
MFD
SVNotohP
sruO
noisuffiDVM
MFD
SVNotohP
sruO128px s/frame 256px s/frame Datset Method PSNR↑ LPS Ih Po Srt ↓-term FID↓ KID↓ FID↓ KIDL ↓ong F- Vte Drm ↓ mTSED↑
PhotoNVS 45.6 PhotoNVS 183 RE10K M Mu ul lt ti iD Di if ff f+ +I Zro on eDD ee pp tt hh 1 15 5. .4 69 5 0 0. .4 30 92 3 2 28 5. .3 96 0 0 0. .0 00 05 4 3 34 0. .9 11 5 0 0. .0 00 06 6 1 11 05 5.7 .92 0 0. .7 89 57 5
MD ulF tiM Diff 1 17 .0.4 2 MD ulF tiM Diff 1.- 94 ScanNet M Mu ul lt ti iD Di if ff f+ +I Zro on eDD ee pp tt hh 1 15 5. .0 05 0 0 0. .4 43 35 1 4 44 3. .1 85 4 0 0. .0 01 10 0 4 46 7. .8 17 1 0 0. .0 01 13 3 1 11 18 4. .3 9 0 0. .5 50 73 6
Table 5. Qualitative comparison of using IronDepth trained on
Table 4. Comparison of the inference speed evaluated in sec- ScanNetv2asalternativedepthestimatorevaluated256×256reso-
onds per frame using FP32 on an NVIDIA A100-SXM4-80GB.
lution.Wenoticethattheresultsarecomparablefortheshort-term
By jointly inferring multiple frames in parallel and using effi-
metrics.Forlong-termevaluation,weobservethatthenon-metric
cient attention architecture, we achieve noticeably shorter infer-
scalingofIronDepthleadstoworsemTSEDscores.
encetimes.
Novel views
ings is considerably higher and the frame rate noticeably
slower compared to RealEstate10K, we consider sequence
lengthsreducedby50%intheoriginalvideo.Therefore,we
consider the 25th frame for short-term and the 100th view
forlong-termevaluationrelativetothestartingframe.
TSED To compute TSED scores [82], we use the of-
ficial implementation from (https://github.com/
YorkUCVIL/Photoconsistent-NVS) and provide
additionalquantitativeresultsinFig.12and Fig.11. While
DFMachievesthehighestconsistencyscoresduetoitsPix-
Figure13. NeRFfittingfrom16synthesizedviews. Row1: Ner-
elNeRF [81] formulation, it suffers from noticeably worse
facto,row2:Nerfacto+Distillation
imagegenerationqualitycomparedtoMultiDiff(seeTable
1and2ofthemainpaperaswellas Fig.8and Fig.10).As
DFM does not support higher image resolutions, we mea- D.Ablations
sureTSEDat128×128resolution.
We show additional qualitative results ablating our design
decisionsinFig.15. Wenotethattrainingourmodelfrom
scratch (”MultiDiff no prior”) leads to over-smoothed re-
Inference speed We report inference performances of sultsthatdonotcloselyfollowthetargettrajectory.Further-
PhotoNVS,DFM,andMultiDiffinTab.4.Asourapproach more, we showcase the effect of using the image prior but
infersmultipleframesinparallelandusesanefficientatten- notinitializingtheweightsofourcorrespondenceattention
tion architecture, we observe noticeably shorter inference layer with the weights of the pre-trained video prior (see
timeswhileachievinghigherimagefidelityandconsistency ”MultiDiffnovid.”): Theresultsareoveralllessconsistent
thanthebaselines. Wenotethatourapproachalsoscalesto ase.g. thefloorchangesfromcarpettowood. Ourmethod
larger resolutions as the underlying latent video prior can usesdepth-basedimagewarpstoreprojectthereferenceim-
easily be tuned for image sizes. This is in stark contrast agetothetargetposes,providingstrongcuesaboutthetar-
to baselines like PhotoNVS, DFM for which the computa- getviews.WeablatetheimportanceofthisinTable3ofthe
tionalcostsquicklybecometoohighandrequireinfeasible maintable(see”MultiDiffnowarp”)andshowanexample
amountsofmemorywhentrainedonlargerresolutions. of in Fig. 15. Without using the warps of the reference
image, ourmodelisnotabletofaithfullyfollowthetarget
trajectory. Asunderstrongcameramotion,thereislittleto
no overlap with the reference image, we also learn an em-
beddingofthetargetposeandshowtheeffectofremoving
this information (”MultiDiff no pose”) in Fig. 15. Using
the additional pose embeddings provides additional guid-
FittingaNeRF ResultsforfittingaNeRFwithNerfacto anceaboutthetargetposesleadingtobetterTSEDscores.
areshowninFig.13,yieldingsmallpixel-levelinconsisten- Inaddition,weablatetheeffectofusinganalternativedepth
cieswithfloatingartifacts. AsinReconFusion[79],weuse estimatortoZoeDepth[6]inTab.5. Forthis,weuseIron-
distillationtoobtainacleanerrepresentation(secondrowin Depth [2] pretrained on ScanNetv2 and report qualitative
Fig.13). results on RealEstate10K and ScanNet. While we observe
16
FReN
FReN
dellitsiDFigure11. TSEDscoresevaluatedonRealEstate10Kataresolutionof128×128. TheleftchartshowstheTSEDevaluatedatdifferent
thresholds,ontherightweplottheTSEDscoresoverthepairsofframeindicesalongthetrajectory.
Figure12.TSEDscoresevaluatedonScanNetataresolutionof128×128.Ontheleft,weshowtheTSEDevaluatedatdifferentthresholds.
TherightchartplotstheTSEDscoresoverthepairsofframeindicesalongthetrajectory.
comparable results in image quality performance, we note Reference image Sampled views
thatusingIronDepthleadstoworseconsistencyscores. As
IronDepthdoesnotprovideestimatesinmetricscale,using
these depth estimates to warp the reference image leads to
less accurate conditional information. Ultimately, this re-
sults in lower consistency scores - see e.g., mTSED that
decreasesby≈6%onRealEstate10Kand≈13%onScan-
NetusingIronDepthcomparedtousingZoeDepth.
Furthermore, we qualitatively show the effect of using
structurednoiseinFig.14onaScanNettestsequence. We
notethatbystructuringthenoiseusingthedepthestimates,
weobtainmorerealisticandconsistentsynthesisresults.
Figure 14. Additional qualitative comparison of applying struc-
turednoiseonaScanNettestsequence.Applyingstructurednoise
17 leads to more consistent and overall more realistic sampling re-
sults.
ffiDitluM
NS
o/w
ffiDitluMSampled views Time
Figure15.QualitativecomparisonofthedifferentablationsofourmethodonaRealEstate10Ktestsequence.
18
roirp
on
ffiDitluM
.div
on
ffiDitluM
praw
on
ffiDitluM
esop
on
ffiDitluM
ffiDitluM
TG