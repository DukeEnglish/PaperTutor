Detecting Brittle Decisions for Free: Leveraging
Margin Consistency in Deep Robust Classifiers
JonasNgnawé SabyasachiSahoo
IID-UniversitéLavalandMila IID-UniversitéLavalandMila
jonas.ngnawe.1@ulaval.ca sabyasachi.sahoo.1@ulaval.ca
YannPequignot FrédéricPrecioso
IID-UniversitéLaval UniversitéCôted’Azur,CNRS,INRIA,I3S,Maasai
yann.pequignot@iid.ulaval.ca frederic.precioso@univ-cotedazur.fr
ChristianGagné
IID-UniversitéLaval,MilaandCanadaCIFARAIChair
christian.gagne@gel.ulaval.ca
Abstract
Despiteextensiveresearchonadversarialtrainingstrategiestoimproverobustness,
the decisions of even the most robust deep learning models can still be quite
sensitive to imperceptible perturbations, creating serious risks when deploying
themforhigh-stakesreal-worldapplications. Whiledetectingsuchcasesmaybe
critical,evaluatingamodel’svulnerabilityataper-instancelevelusingadversarial
attacksiscomputationallytoointensiveandunsuitableforreal-timedeployment
scenarios. Theinputspacemarginistheexactscoretodetectnon-robustsamples
andisintractablefordeepneuralnetworks. Thispaperintroducestheconceptof
marginconsistency–apropertythatlinkstheinputspacemarginsandthelogit
marginsinrobustmodels–forefficientdetectionofvulnerablesamples. First,we
establishthatmarginconsistencyisanecessaryandsufficientconditiontousea
model’slogitmarginasascoreforidentifyingnon-robustsamples. Next,through
comprehensiveempiricalanalysisofvariousrobustlytrainedmodelsonCIFAR10
and CIFAR100 datasets, we show that they indicate strong margin consistency
withastrongcorrelationbetweentheirinputspacemarginsandthelogitmargins.
Then,weshowthatwecaneffectivelyusethelogitmargintoconfidentlydetect
brittledecisionswithsuchmodelsandaccuratelyestimaterobustaccuracyonan
arbitrarilylargetestsetbyestimatingtheinputmarginsonlyonasmallsubset.
Finally,weaddresscaseswherethemodelisnotsufficientlymargin-consistentby
learningapseudo-marginfromthefeaturerepresentation. Ourfindingshighlight
thepotentialofleveragingdeeprepresentationstoefficientlyassessadversarial
vulnerabilityindeploymentscenarios.
1 Introduction
Deepneuralnetworksareknowntobevulnerabletoadversarialperturbations,visuallyinsignificant
changesintheinputresultingintheso-calledadversarialexamplesthatalterthemodel’sprediction
(Biggioetal.,2013;Goodfellowetal.,2015). Theyconstituteactualthreatsinreal-worldscenarios
(Evtimovetal.,2017;Gnanasambandametal.,2021),jeopardizingtheirdeploymentinsensitive
andsafety-criticalsystemssuchasautonomousdriving,aeronautics,andhealthcare. Researchin
the field has been intense and produced various adversarial training strategies to defend against
Preprint.Underreview.
4202
nuJ
62
]GL.sc[
1v15481.6042:viXrathevulnerabilitytoadversarialperturbationswithboundedℓ norm(e.g.,p = 2,p = ∞)through
p
augmentation,regularization,anddetection(Xuetal.,2017;Madryetal.,2018;Zhangetal.,2019;
Carmonetal.,2019;Wangetal.,2020;Wuetal.,2020;Riceetal.,2020),tociteafew. Theempirical
robustness(adversarialaccuracy)oftheseadversariallytrainedmodelsisstillfarbehindtheirhigh
performanceintermsofaccuracy.Itistypicallyestimatedbyassessingthevulnerabilityofsamplesof
agiventestsetusingadversarialattacks(Carlini&Wagner,2016;Madryetal.,2018)oranensemble
ofattackssuchasthestandardAutoAttack(Croce&Hein,2020b). Theobjectiveofthatevaluationis
todetermineif,foragivennormalsample,anadversarialinstanceexistswithinagivenϵ-ballaround
it. Yet,thisrobustnessevaluationoveraspecifictestsetgivesaglobalpropertyofthemodelbutnot
alocalpropertyspecifictoasingleinstance(Seshiaetal.,2018;Dreossietal.,2019). Beyondthat
particulartestset,obtainingthisinformationforeachnewsamplewouldtypicallyinvolvererunning
adversarialattacksorperformingaformalrobustnessverification,whichincertaincontextsmay
becomputationallyprohibitiveintermsofresourcesandtime. Indeed,inhigh-stakesdeployment
scenarios, knowing the vulnerability of single instances in real-time (i.e., their susceptibility to
adversarialattacks)wouldbevaluable,forexample,toreducerisk,prioritizeresources,ormonitor
operations. Currentresearchlacksefficientandscalablewaystodeterminethevulnerabilityofa
sampleinadeploymentcontext.
Theinputspacemargin(i.e.,thedistanceofthesampletothemodel’sdecisionboundaryintheinput
space),orinputmargininshort,canbeusedasascoretodeterminewhetherthesampleisnon-robust
and, assuch, likely tobevulnerable toadversarialattacks. Computingthe exactinput marginis
intractablefordeepneuralnetworks(Katzetal.,2017;Elsayedetal.,2018;Jordan&Dimakis,2020).
Theseinputmarginsmaynotbemeaningfulforfragilemodelswithzeroadversarialaccuraciesas
allsamplesarevulnerable(closetothedecisionboundary). However,forrobustlytrainedmodels,
whereonlycertaininstancesarevulnerable,theinputmarginisveryusefulinidentifyingthecritical
samples. Previous research studies have explored input margins of deep neural networks during
training, focusing on their temporal evolution (Mickisch et al., 2020; Xu et al., 2023), and their
exploitationinimprovingadversarialrobustnessthroughinstance-reweightingwithapproximations
(Zhangetal.,2020;Liuetal.,2021)andmarginmaximization(Elsayedetal.,2018;Dingetal.,2020;
Xuetal.,2023). However,tothebestofourknowledge,nopreviousresearchstudiestherelationship
betweentheinputspacemarginandthelogitmarginofrobustlytraineddeepclassifiersinthecontext
ofvulnerabilitydetection.
Inthispaper,weinvestigatehowthedeeprepresentationofrobustmodelscanprovideinformation
aboutthevulnerabilityofanysinglesampletoadversarialattacks. Wespecificallyaddresswhether
thelogitmarginasanapproximationofthedistancetothedecisionboundaryinthefeaturespace
of the deep neural network (penultimate layer) can reliably serve as a proxy of the input margin
forvulnerabilitydetection. Whenthisholds,wewillrefertothemodelasbeingmargin-consistent.
Themarginconsistencypropertyimpliesthatthemodelcandirectlyidentifyinstanceswhereits
robustnessmaybecompromisedsimplyfromasimpleforwardpassusingthelogitmargin. Fig.1
illustratesthisideaofmarginconsistency. Thefollowingcontributionsarepresentedinthepaper:
• We introduce the notion of margin consistency1, a property to characterize robust models that
allow using their logit margin as a proxy estimation for the input space margin in the context
ofnon-robustsampledetection. Weprovethatmarginconsistencyisanecessaryandsufficient
conditiontoreliablyusethelogitmarginfordetectingnon-robustsamples.
• Throughanextensiveempiricalinvestigationofpre-trainedmodelsonCIFAR10andCIFAR100
withvariousadversarialtrainingstrategies,mainlytakenfromRobustBench(Croceetal.,2021),
weprovideevidencethatalmostalltheinvestigatedmodelsdisplaystrongmarginconsistency,i.e.,
thereisastrongcorrelationbetweentheinputmarginandthelogitmargin.
• Weconfirmexperimentallythatmodelswithstrongmarginconsistencyperformwellindetecting
samplesvulnerabletoadversarialattacksbasedontheirlogitmargin. Incontrast,modelswith
weakermarginconsistencyexhibitpoorerperformance. Leveragingmarginconsistency,wecan
alsoestimatetherobustaccuracyonanarbitrarilylargetestsetbyestimatingtheinputmargins
onlyonasmallsubset.
• Formodelswheremarginconsistencydoesnothold,exhibitingaweakcorrelationbetweenthe
inputmarginandthelogitmargin,wesimulatemarginconsistencybylearningtomapthemodel’s
feature representation to a pseudo-margin with a better correlation through a simple learning
scheme.
1Codeavailableat:https://github.com/ngnawejonas/margin-consistency
2Input Space Feature Space
Margin Consistency
Robust
Non-robust
adversarial sample original sample ball Robust area with radius Model's Decision Boundary
Figure1: Illustrationoftheinputspacemargin,margininthefeaturespaceandmarginconsistency.
Themargin-consistentmodelpreservestherelativepositionofsamplestothedecisionboundaryin
theinputspacetothefeaturespace.
0.9 0.9
DI0(=0.28) HE1(=0.74)
0.8 X MU R8 00 (( == 00 .6.4 83 )) 0.8 W REU 81 1( (= =0 0. .7 88 0) )
DM0(=0.71) DS1(=0.81)
0.7 AL0(=0.72) 0.7 CU41(=0.82)
CU80(=0.73) CU81(=0.83)
0.6 W SEA 18 00 (( == 00 .7.7 44 )) 0.6 R PAI1 2( 1(=0 =.8 03 .8) 3)
EN0(=0.74) WA81(=0.83)
0.5 TR0(=0.74) 0.5 AD21(=0.84)
DS0(=0.75) AD1(=0.84)
0.4 M ZHD 00 (( == 00 .. 77 55 )) 0.4 R RE A1 11 1( (= =0 0. .8 85 5) )
HE0(=0.76)
0.3 CL0(=0.77) 0.3
RE80(=0.77)
0.2 P AA D2 20 0( (= =0 0. .7 88 1) ) 0.2
AD10(=0.82)
0.1 0.1
0.0 0.0
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.00 0.02 0.04 0.06 0.08 0.10
Input Margin Input Margin
(a)CIFAR10 (b)CIFAR100
Figure2: Marginconsistencyofvariousmodels: thereisastrongcorrelationbetweeninputspace
marginandlogitmarginformostℓ robustmodelstested,theexceptionsbeingDI0andXU8on
∞
CIFAR10. SeeTable1forthereferencesonthemodels. Theplotsaregivenwithstandarderrorfor
they-axisvaluesineachinterval.
2 Methodology
2.1 NotationandPreliminaries
Notation We consider f : Rn → RC a deep neural network classifier with weights θ trained
θ
onadatasetofsamplesdrawniidfromadistributionD onaproductspaceX ×Y. Eachsample
x in the input space X ⊂ Rn has a unique corresponding label y ∈ Y = {1,2,...,C}. The
predictionofxisgivenbyyˆ(x)=argmax fj(x),wherefj(x)isthej-thcomponentoff (x).
j∈Y θ θ θ
Weconsiderthatadeepneuralclassifieriscomposedofafeatureextractorh : X → Rm anda
ψ
linearheadwithC linearclassifiers{w ,b }suchthatfj(x) = w ⊤h (x)+b . Thepredictive
j j θ j ψ j
distributionp (y|x)isobtainedbytakingthesoftmaxoftheoutputf (x). Aperturbedsamplex′
θ θ
canbeobtainedbyaddingaperturbationδtoxwithinanϵ-ballB (x,ϵ),anℓ -normballofradius
p p
ϵ > 0centeredatx;B (x,ϵ) := {x′ : ∥x′−x∥ = ∥δ∥ < ϵ}. Thedistance∥x′−x∥ = ∥δ∥
p p p p p
representstheperturbationsizedefinedas((cid:80)n
|δ
|p)p1
. Inthispaper,wewillfocusonℓ norm
i=1 i ∞
(∥x∥ =max |x |),whichisthemostcommonlyusednormintheliterature.
∞ i=1,...,n i
Local robustness Different notions of local robustness exist in the literature (Gourdeau et al.,
2021;Zhongetal.,2021;Hanetal.,2023).Inthispaper,weequatelocalrobustnesstoℓ -robustness,
p
a standard notion corresponding to the invariance of the decision within the ℓ ϵ-ball around the
p
sample(Bastanietal.,2016;Fawzietal.,2018)andformalizedintermsofϵ-robustness.
3
nigraM
tigoL
nigraM
tigoLDefinition1. Amodelf isϵ-robustatpointxifforanyx′ ∈B (x,ϵ)(x′intheϵ-ballaroundx),
p
wehaveyˆ(x′)=yˆ(x).
Foragivenrobustnessthresholdϵ,adatainstanceissaidtobenon-robustforthemodelifthismodel
isnotϵ-robustonit. Thismeansitispossibletoconstructanadversarialsamplefromthatinstance
in its vicinity (i.e., within an ϵ-ball distance from the original instance). A vulnerable sample to
adversarialattacksisnecessarilynon-robust. Thisnotionoflocalrobustnesscanbequantifiedinthe
worst-caseor,onaverage,insidetheϵ-ball. Wefocushereontheworst-casemeasurementgiven
bytheinputmargin,alsoreferredtoastheminimumdistortionortherobustradius(Szegedyetal.,
2014;Carlini&Wagner,2016;Weng,2019)
Theinputspacemarginisthedistancetothedecisionboundaryoff intheinputspace. Itisthe
normofaminimalperturbationrequiredtochangethemodel’sdecisionatatestpointx:
d (x)=inf{∥δ∥ :δ ∈Rns.t. yˆ(x)̸=yˆ(x+δ)}=sup{ϵ:f isϵ-robustatx}. (1)
in p
Aninstancexisnon-robustforarobustnessthresholdϵifd (x) ≤ ϵ. EvaluatingEq.1fordeep
in
networks is known to be intractable in the general case. An upper bound approximation can be
obtainedusingapointx′,theclosestadversarialcounterpartofxinℓ normbydˆ (x)=∥x−x′∥
0 p in 0 p
(seeFig.1).
The logit margin is the difference between the two largest logits. For a sample x classified as
(cid:18) (cid:19)
i = yˆ(x) = argmax fj(x)thelogitmarginisdefinedas fi(x)−maxfj(x) > 0. Itisan
j∈Y θ θ θ
j,j̸=i
approximation of the distance to the decision boundary of f in the feature space. The decision
θ
boundaryinthefeaturespacearoundz = h (x),thefeaturerepresentationofx,iscomposedof
ψ
(C −1)lineardecisionboundaries(hyperplanes)DB = {z′ ∈ Rm : w⊤z′+b = w⊤z′+b }
ij i i j j
(j ̸= i). The margin in the feature space is, therefore, the distance to the closest hyperplane, i.e.
mind(z,DB ), where the distance d(z,DB ) from z to a hyperplane DB has a closed-form
ij ij ij
j,j̸=i
expression:
fi(x)−fj(x)
d(z,DB )=inf{∥η∥ :η ∈Rm s.t. z+η ∈DB }= θ θ , (2)
ij p ij ∥w −w ∥
i j q
where∥·∥ isthedualnormofp,q = p forp>1(Moosavi-Dezfoolietal.,2016;Elsayedetal.,
q p−1
2018).
Whentheclassifiersw areequidistant(∥w −w ∥ =ω >0, ∀i,j),themarginbecomes:
j i j q
fi(x)−fj(x) 1 (cid:18) (cid:19) 1(cid:18) (cid:19)
min θ θ = min fi(x)−fj(x) = fi(x)−maxfj(x) . (3)
j,j̸=i ω ω j,j̸=i θ θ ω θ j,j̸=i θ
(cid:124) (cid:123)(cid:122) (cid:125)
logitmargin
Undertheequidistanceassumption,thelogitmarginisproportional(equaluptoascalingfactor)to
themargininthefeaturespace. Wewilldenotethelogitmarginofxbyd (x):
out
d (x)=fi(x)−maxfj(x) (4)
out θ θ
j,j̸=i
2.2 MarginConsistency
Definition2. Amodelismargin-consistentifthereisamonotonicrelationshipbetweentheinput
spacemarginandthelogitmargin,i.e.,d (x )≤d (x )⇔d (x )≤d (x ), ∀x ,x ∈X.
in 1 in 2 out 1 out 2 1 2
Amargin-consistentmodelpreservestherelativepositionofsamplestothedecisionboundaryfrom
theinputspacetothefeaturespace. Asamplefurtherfrom(closerto)thedecisionboundaryinthe
inputspaceremainsfurtherfrom(closerto)thedecisionboundaryinthefeaturespacewithrespect
toothersamples,asillustratedinFig.1.
We can evaluate margin consistency by computing the Kendall rank correlation (τ ∈ [−1,1])
betweentheoutputscoresandtheinputmarginsoveratestset. TheKendallrankcorrelationteststhe
existenceandstrengthofamonotonicrelationshipbetweentwovariables. Itmakesnoassumption
onthedistributionofthevariablesandisrobusttooutliers(Chattamvelli,2024). Perfectmargin
consistencycorrespondstoanabsolutevalueof1,and0meanstheabsenceofmarginconsistency.
4monotonic
(margin-consistency)
and
(a)Marginconsistencyimpliesd canperfectly (b) Without margin consistency, d cannot be
out out
separate non robust samples in A from robust a good discriminator for robust and non-robust
ϵ
samples. samples.
Figure3: IllustrationofTheorem1’sproof.
2.3 Non-robustSamplesDetection
Non-robustdetectioncanbedefinedasascored-basedbinaryclassificationtaskwherenon-robust
samples constitute the positive class, and the input margin d induces a perfect discriminative
in
functiongforthat:
(cid:26)
1 if x isnon-robust
g(x)=1 (x)= .
[din(x)≤ϵ] 0 if x isrobust
Ifamodelismargin-consistent, itslogitmargincanalsobeadiscriminativescoretodetectnon-
robustsamples. Thefollowingtheoremestablishesthatthisisanecessaryandsufficientcondition.
Therefore,thedegreetowhichamodelismargin-consistentshoulddeterminethediscriminative
powerofthelogitmargin.
Theorem 1. If a model is margin-consistent, then for any robustness threshold ϵ, there exists a
thresholdλforthelogitmargind thatseparatesperfectlynon-robustsamplesandrobustsamples.
out
Conversely, if for any robustness threshold ϵ, d admits a threshold λ that separates perfectly
out
non-robustsamplesfromrobustsamples,thenthemodelismargin-consistent.
Proof sketch. Fig. 3 presents intuition behind the proof of Theorem 1. For the first part of the
theorem(seeFig. 3a),ifthereisamonotonicrelationshipbetweend andd (marginconsistency),
in out
anypointxwithd lessthanthethresholdϵ(non-robust)willalsohaved lessthanλ=d (x )
in out out 0
(with d (x ) = ϵ). For the second part (see Fig. 3b), if there are two points x and x with
in 0 1 2
non-concordantd andd (nomarginconsistency),thenforathresholdϵ betweend (x )and
in out 0 in 1
d (x ),theywillbothhavedifferentclassesbutnothresholdofd (horizontalline)canclassify
out 2 out
thembothcorrectly. ThecompleteproofofTheorem1isdeferredtoAppendixA.
Commonmetricsfordetectioninclude(Hendrycks&Gimpel,2017;Corbièreetal.,2019;Zhu
etal.,2023): theAreaUndertheReceiverOperatingCurve(AUROC),whichensurestheabilityof
amodeltodistinguishbetweenthepositiveandnegativeclassesacrossallpossiblethresholds;the
AreaUnderthePrecision-RecallCurve(AUPR),whichevaluatesthetrade-offbetweenprecisionand
recallandislesssensitivetoimbalancebetweenpositiveandnegativeclasses;andtheFalsePositive
Rate(FPR)ata95%TruePositiveRate(TPR)(FPR@95),thatiscrucialinsystemswheremissing
positivecasescanhaveseriousconsequences,suchasminimizingthenumberofvulnerablesamples
missed. TheAUROCandAUPRofaperfectclassifieris1,while0.5forarandomclassifier.
2.4 SampleEfficientRobustnessEvaluation
Marginconsistencyenablesempiricalrobustnessevaluationoveranarbitrarilylargetestsetbyonly
estimatingtheinputmarginsofasmallsubsetoftestsamples.Forarobustnessevaluationatthreshold
ϵ(e.g.,ϵ = 8/255inℓ normonCIFAR10andCIFAR100),werandomlysampleasmallsubset
∞
ofthelargetestsetanddeterminethethresholdλforthelogitmarginthatcorrespondstoϵ. The
thresholdλisthenusedtodetectvulnerablesamples. Withthetruelabelsofthesetestsets,wecan
determinetheproportionofcorrectnon-vulnerablesamples,whichisthestandardrobustaccuracyas
describedinAlgorithm1. Anaivewaytosetthethresholdλatline6ofAlgorithm1wouldbetoset
ittothedetectionthresholdatα=95%TPRorα=90%TPR,butthelogitmarginthresholdcould
5Algorithm1SampleEfficientRobustnessEstimation
1: Input: TestDataset(X,Y)∈(X ×Y)N,Robustnessthresholdϵ>0,Subsetsizen≪N.
2: Output: RobustAccuracyEstimationA r
3: -SelectuniformlyatrandomasubsetX nofnsamplesfromX.
4: -ComputetheestimationsoftheinputmarginsonX n,D n ={dˆ in(x):x∈X s}
5: -Creategroundtruthlabelsforvulnerabilityatthresholdϵi.e. 1 [dˆ in(x)≤ϵ](x),forx∈X s.
6: -Determinethethresholdλofd outthatgivesbestapproximationofrobustaccuracyonX s.
7: -A r =|{x∈X : d out(x)>λandyˆ(x)=y}|/N
varyfromonemodeltoano;thereforeabetterwayistoselectitbytuningovervaluesα>=0.80
thatgivesthebestapproximationoftherobustaccuracyintermsoftheabsoluteerroronthesmall
subsetX . Thesamelogicappliesifwewanttoestimatethevulnerabilityofalargedatasetwithout
s
thelabels.
3 Evaluation
3.1 ExperimentalSetup
Datasets and models We investigate various pre-trained models on CIFAR10 and CIFAR100
datasets. ThemajorityofmodelswereloadedfromtheRobustBenchmodelzoo2(Croceetal.,2021),
withafewmoremodelsthatareResNet-18(Heetal.,2016)modelswetrainedonCIFAR10with
StandardAdversarialTraining(Madryetal.,2018),TRADES(Zhangetal.,2019),LogitPairing
(ALPandCLP,Kannanetal.(2018)),andMART(Wangetal.,2020),usingtheexperimentalsetup
ofWangetal.(2020).
Inputmarginestimation ThisisdoneusingFABattack(Croce&Hein,2020a),whichisanattack
that minimally perturbs the initial instance. Xu et al. (2023) used it in their adversarial training
strategyasareliablewaytocomputetheclosestboundarypointgivenenoughiterations. Weperform
theuntargetedFABattackwithoutrestrictingthedistortiontofindtheboundaryforallthesamplesin
thetestsetinsteadofconstrainingtheperturbationinsideagivenϵ-ballwhenevaluatingrobustness.
As a sanity check for the measured distances, we compare the ratio of correct samples x with
estimatedinputmarginsgreaterthanϵ=8/255andtherobustaccuracyinℓ normmeasuredwith
∞
AutoAttack(Croce&Hein,2020b)atϵ = 8/255. Bothquantitiesestimatethesamething,witha
meanandamaximumabsolutedifferenceoverthemodelsrespectivelyof1.3and6.1forCIFAR10,
0.48and0.75onCIFAR100,whicharereasonable(cf. Fig.8inappendixB.1forthecomparisonfor
allthemodels).
The estimation of the input margins over the 10,000 test samples allows us to create for a given
thresholdϵapoolofvulnerablesamplesthatcanbesuccessfullyattackedatthresholdϵandnon-
vulnerablesamplesthatwerenotabletobeattacked. Traininganddistanceestimationswererunon
anNVIDIATitanXpGPU(1x).
3.2 ResultsandAnalysis
Correlation analysis The results presented in Fig. 2 show that the logit margin has a strong
correlation (up to 0.86) with the input margin, which means that they have a level of margin
consistencyforthosemodels. Theplotsaregivenwithstandarderrorforthey-axisvaluesineach
interval. However,wealsoobservethattwomodels(i.e.,DI0(Dingetal.,2020)andXU80(Xuetal.,
2023)WideResNetsonCIFAR10)haveaweakercorrelation. WeshowinSec.3.3thatwecanlearn
tomapthefeaturerepresentationofthesemodelstoapseudo-marginthatreflectsthedistancetothe
decisionboundaryintheinputspace.
Vulnerablesamplesdetection Wepresenttheresultsfortherobustnessthresholdϵ=8/255in
Table1. Asexpectedwiththestrongcorrelations,theperformanceoverthenon-robustdetectiontask
isexcellent. Wecannotethatthemetricsarelowerforthetwomodelswithlowcorrelationswith
2https://github.com/RobustBench/robustbench
60.9 0.9
0.8 MD0 C AL L0 0TR0 EA ND 010 AD Z2 H0 0 HE0 DS0RE8P 0A20 WA80 0.8 RI1 AD1AD2 R1 ER H1A E11 1W1 UP 1A21CU4 R1 ED 8S 11 WA C8 U1 81
0.7 MR0 SE10 DM0 CU80 0.7
DI0 PA20 AD20
0.6 DS0 HE0 CU80 0.6
XU80 TR0 RE80 AD1 DS1 RA11 RI1
0.5 DM0 CL0 ZH0 0.5 AD21 HE1 RE81 WA81
MR0 EN0 WA80 XU80 CU81 PA21 RE11 WU1
0.4 M SED 10 0 AL0 AD10 0.4 CU41
0.3 DI0 0.3
40 45 50 55 60 65 20.0 22.5 25.0 27.5 30.0 32.5 35.0 37.5 40.0
Robust accuracy Robust accuracy
(a)CIFAR10 (b)CIFAR100
Figure4: Distributionofthecorrelationbetweeninputmarginsandlogitmarginsinℓ withrobust
∞
accuracy. Thestrengthofthecorrelation,whichindicatesthelevelofmarginconsistency,doesnot
dependontherobustaccuracy. ReferencesonmodelsaregiveninTable1.
ModelID Kendallτ(↑) AUROC(↑) AUPR(↑) FPR@95(↓) Acc Rob.Acc Architecture
DI0(Wuetal.,2020) 0.28 67.49 70.91 82.56 84.36 41.44 WideResNet-28-4
XU80(Xuetal.,2023) 0.43 83.30 80.50 83.42 93.69 63.89 WideResNet-28-10
MR0(Wangetal.,2020) 0.68 92.95 94.92 29.76 79.69 39.12 ResNet-18
DM0(Debenedettietal.,2023) 0.71 94.31 93.20 32.76 91.30 57.27 XCiT-M12
AL0(Kannanetal.,2018) 0.72 94.67 95.98 24.93 80.38 40.21 ResNet-18
CU80(Cuietal.,2023) 0.73 96.87 94.42 17.90 92.16 67.73 WideResNet-28-10
WA80(Wangetal.,2023) 0.74 96.82 94.33 17.60 92.44 67.31 WideResNet-28-10
SE10(Sehwagetal.,2021) 0.74 96.03 94.66 19.13 84.59 55.54 ResNet-18
EN0(Engstrometal.,2019) 0.74 95.16 95.07 24.10 87.03 49.25 ResNet-50
TR0(Zhangetal.,2019) 0.74 94.63 96.13 30.93 80.72 42.23 ResNet-18
DS0(Debenedettietal.,2023) 0.75 95.80 95.08 24.65 90.06 56.14 XCiT-S12
MD0(Madryetal.,2018) 0.75 95.36 97.00 23.23 81.85 36.91 ResNet-18
ZH0(Zhangetal.,2019) 0.75 95.86 95.65 24.91 84.92 53.08 WideResNet-34-10
HE0(Hendrycksetal.,2019) 0.76 96.35 95.68 20.01 87.11 54.92 WideResNet-28-10
CL0(Kannanetal.,2018) 0.77 95.93 96.98 20.01 81.12 40.08 ResNet-18
RE80(Rebuffietal.,2021) 0.77 97.33 95.70 13.87 87.33 60.73 WideResNet-28-10
PA20(Pangetal.,2022) 0.78 97.65 96.39 14.40 88.61 61.04 WideResNet-28-10
AD20(Addepallietal.,2022) 0.81 97.67 97.46 13.42 85.71 52.48 ResNet-18
AD10(Addepallietal.,2021) 0.82 97.86 97.68 13.26 80.24 51.06 ResNet-18
HE1(Hendrycksetal.,2019) 0.74 94.43 97.39 30.40 59.23 28.42 WideResNet-28-10
WU1(Wuetal.,2020) 0.78 95.81 98.00 23.34 60.38 28.86 WideResNet-34-10
RE81(Rebuffietal.,2021) 0.80 96.87 98.30 18.06 62.41 32.06 WideResNet-28-10
DS1(Debenedettietal.,2023) 0.81 96.78 98.30 19.18 67.34 32.19 XCiT-S12
CU41(Cuietal.,2023) 0.82 97.07 98.48 17.21 64.08 31.65 WideResNet-34-10
CU81(Cuietal.,2023) 0.83 97.41 98.24 15.62 73.85 39.18 WideResNet-28-10
RI1(Riceetal.,2020) 0.83 96.61 99.05 18.14 53.83 18.95 PreActResNet-18
PA21(Pangetal.,2022) 0.83 97.66 98.82 13.83 63.66 31.08 WideResNet-28-10
WA81(Wangetal.,2023) 0.83 97.51 98.28 14.96 72.58 38.83 WideResNet-28-10
AD21(Addepallietal.,2022) 0.84 97.46 98.92 16.00 65.45 27.67 ResNet-18
AD1(Addepallietal.,2021) 0.84 97.65 98.99 13.88 62.02 27.14 PreActResNet-18
RE11(Rebuffietal.,2021) 0.85 97.97 99.05 13.21 56.87 28.50 PreActResNet-18
RA11(Rade&Moosavi-Dezfooli,2021) 0.85 98.01 99.08 12.36 61.50 28.88 PreActResNet-18
Table 1: Correlations and vulnerable points detection performance at ϵ = 8/255 on different
adversariallytrainedmodels.
particularlyveryhighFPR@95. Theperformanceremainsquitegoodwithdifferentvaluesofϵ(cf.
appendixB.2).
Sample Efficient Robustness Estimation We recover the robust accuracy of the investigated
modelsevaluatedwithover10,000usingonlyasmallsubset. Fig. 5showstheabsoluteerrorof
theestimationfor500samples. Theestimationsarestilldescentwith100samples(cf. Fig.10in
appendixB.3).
MarginConsistencyandLipschitzSmoothness Aneuralnetworkf issaidtobeL-Lipschitz
if ∥f(x )−f(x ∥ ≤ L∥x −x ∥, ∀x ,x . Lipschitz smoothness is important for adversarial
1 2 1 2 1 2
robustnessbecauseasmallLipschitzconstantLguaranteesthenetwork’soutputcannotchangemore
thanafactorLofthechangeintheinput. TherearestrategiestodirectlyconstrainttheLipschitz
constanttoachieve1-Lipschitznetworks(Cisseetal.,2017;Lietal.,2019;Serrurieretal.,2021;
Araujoetal.,2023). EmpiricaladversarialtrainingstrategiesaimtoachieveLipschitz’ssmoothness
indirectly. Note,however,thatLipschitzcontinuitydoesnotimplymarginconsistency,forexample,
twopointsx andx with0 < d (x ) < d (x ). WhiletheL-Lipschitzconditionimpliesthat
1 2 in 1 in 2
7
01RAFIC
001RAFIC
lladneK lladneKsubset size=500 subset size=500
70 W Wi it th h A Lou gto itA Mtt aa rc gk ins 1.0 1.1 40 W Wi it th h A Lou gto itA Mtt aa rc gk ins 1.4 1.2
11.1
3.1 1.1 1.3
60
1.2 1.6 1.6 35 1.6 1.8
1.4 1.4 1.0
1.6 1.1 2.0
50 30 1.2 1.0 0.8 1.0 1.4 1.4
1.4
40 1.8 1.4 1.9 1.3 14.7 25
30 20 0.8
MD0 MR0 CL0 AL0 DI0 TR0 EN0 AD10 AD20 ZH0 HE0 SE10 DS0 DM0 RE80 PA20 XU80 WA80 CU80 RI1 AD1 AD21 HE1 RE11 WU1 RA11 PA21 CU41 RE81 DS1 WA81 CU81
Models Models
(a)CIFAR10 (b)CIFAR100
Figure5: EstimationsoftherobustaccuracyreportedbyRobustbenchusinglogitmarginswithonly
500 samples are quite accurate both on CIFAR10 and CIFAR100 for strongly margin-consistent
models. Thenumbersindicatetheabsolutedifferencebetweenthetwovalues,averagedoverten
subsets. SeeTable1forthespecificreferencesonthemodelID.
1.0 XU80(=0.49) 1.0 HE1(=0.78)
DI0(=0.57) CU41(=0.80)
SE10(=0.66) RE81(=0.81)
PA20(=0.66) AD21(=0.81) 0.8 RE80(=0.66) 0.8 AD1(=0.81)
CU80(=0.71) WU1(=0.82)
WA80(=0.71) CU81(=0.83)
MR0(=0.74) RI1(=0.83)
0.6 HE0(=0.75) 0.6 WA81(=0.85)
AL0(=0.75) RE11(=0.85)
CL0(=0.75) RA11(=0.86)
EN0(=0.75) PA21(=0.86)
0.4 AD10(=0.77) 0.4 DS1(=1.00)
MD0(=0.77)
ZH0(=0.78)
TR0(=0.81)
0.2 AD20(=0.83) 0.2
DM0(=1.00)
DS0(=1.00)
0.0 0.0
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.00 0.02 0.04 0.06 0.08 0.10
Input Space: x x0 Input Space: x x0
(a)CIFAR10. (b)CIFAR100.
Figure6: Thecorrelationsbetweentheinputmargin(∥x−x′∥)andthedistancebetweenthefeature
representationsofsamplesandtheiradversarialcounterparts(∥h (x)−h (x′)∥)suggestsalocal
ψ ψ
behaviourofthefeatureextractorasanisometry(uptoascalingfactor). SeeTable1forthespecific
referencesonthemodelID.Theplotsaregivenwithstandarderrorforthey-axisvaluesineach
interval.
d (x )≤Ld (x )fori=1,2,itisclearlypossibletohaved (x )<d (x ),thusviolating
out i in i out 2 out 1
themarginconsistencycondition.
Fig.4aand4bshowthatthestrengthofthecorrelation,i.e. thelevelofmarginconsistency,doesnot
dependontherobustaccuracy.
Insightintowhenmarginconsistencymayhold? Wehypothesizethatmarginconsistencycan
occurwhenthefeatureextractorh behaveslocallyasanisometry(preservingdistances,uptoa
ψ
scaling factor κ), i.e., ∥h (x)−h (x′)∥ = κ∥x−x′∥ . We can experimentally see that there
ψ ψ p p
is a high correlation between the input margin (∥x−x′∥) and the distance between the feature
representationsofxandx′ (Fig.6). Givenaninputsamplex,bydefinitiond (x) = ∥z−z′∥
out p
where z = h (x) and z′ an orthogonal projection of z on the boundary hyperplane. The points
ψ
z, z′ and h (x′) will form a right triangle so the side ∥z−z′∥ will directly correlate with side
ψ p
∥h (x)−h (x′)∥ .
ψ ψ p
3.3 LearningaPseudo-Margin
Forthetwomodelsthatareweaklymargin-consistent,weareproposingtodirectlylearnamapping
thatmapsthefeaturerepresentationofasampletoapseudo-marginthatreflectstherelativeposition
of the samples to the decision in the input space. We use a learning scheme similar to the one
8
ycaruccA
tsuboR
)0x(h
)x(h
:ecapS
erutaeF
ycaruccA
tsuboR
)0x(h
)x(h
:ecapS
erutaeFDI0 XU80
0.9 0.9
Logit Margin ( =0.28, FPR@95=76.36)
0.8 Learned Pseudo-Margin ( =0.57, FPR@95=51.13) 0.8
0.7 0.7
0.6 0.6
0.5 0.5
0.4 0.4
0.3 0.3
0.2 0.2
Logit Margin ( =0.43, FPR@95=85.37)
0.1 0.1
Learned Pseudo-Margin ( =0.62, FPR@95=33.00)
0.0 0.0
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.02 0.04 0.06 0.08 0.10
Input Margin Input Margin
Figure7: Correlationimprovementofthelearnedpseudo-marginoverthelogitmarginforDI0(Ding
etal.,2020)andXU80(Xuetal.,2023).
ModelID Margin Kendallτ(↑) AUROC(↑) AUPR(↑) FPR@95(↓) Acc. Rob.Acc
Logitmargin 0.28 67.49 70.91 82.56
DI0(Dingetal.,2020) 84.36 41.44
Learnedpseudo-margin 0.57 88.49 89.04 51.13
Logitmargin 0.43 83.30 80.50 83.42
XU80(Xuetal.,2023) 93.69 63.89
Learnedpseudo-margin 0.62 93.66 90.22 33.00
Table2:Comparisonofthecorrelationanddetectionperformancebetweentheactuallogitmarginand
thepseudo-marginlearned. Themodelsareinitiallyweaklymargin-consistent,butthepseudo-margin
learnedfromfeaturerepresentationssimulatesthemarginconsistencywithhighercorrelationand
betterdiscriminativepower.
of Corbière et al. (2019), with a small ad hoc neural network for learning the confidence of the
instances(cf. Fig.12inappendixB.4). Givensomesampleswithestimationsoftheirinputmargins,
theobjectiveistolearntomaptheirfeaturerepresentationtoapseudo-marginthatcorrelateswith
theinputmargins. Thislearningtaskcanbeseenasalearning-to-rankproblem. Weuseasimple
learning-to-rank algorithm for that purpose, which is a pointwise regression approach (He et al.,
2008)relyingonthemeansquarederrorasasurrogateloss.
Fortheexperiment,weusedasimilararchitectureandtrainingprotocolas(Corbièreetal.,2019)
withafullyconnectednetworkwithfivedenselayersof512neurons,withReLUactivationsforthe
hiddenlayersandasigmoidactivationattheoutputlayer. Welearnusing5000examplessampled
randomlyfromthetrainingset,with20%(1000examples)heldasavalidation. Fig.7andTable2
showtheimprovedcorrelationonthelearnedscorecomparedtothelogitmarginforbothmodels.
Theplotsaregivenwithstandarderrorforthey-axisvaluesineachinterval. Thenetworkhaslearned
torecovertherelativepositionsofthesamplesfromthefeaturerepresentation.
4 RelatedWork
Detectiontasksinmachinelearningarefoundtobeofthreemaintypes:
• AdversarialDetection Thegoalofadversarialdetection(Xuetal.,2017;Carlini&Wagner,
2017)istodiscriminateadversarialsamplesfromcleanandnoisysamples. Anadversarialexample
isamaliciousexamplefoundbyadversariallyattackingasample;ithasadifferentclasswhile
being close to the original sample. A vulnerable (non-robust) sample is a normal sample that
admitsanadversarialexampleclosetoit. Thetwodetectiontasksareverydistinct. Adversarial
detectionisadefencemechanismlikeadversarialtraining;Tramer(2022)hasestablishedthatboth
tasksareequivalentproblemswiththesamedifficulty.
• Out-of-Distribution(OOD)detection InOODdetection(Hendrycks&Gimpel,2017;Peng
etal.,2024),theobjectiveistodetectinstancesthathaveadifferentlabelfromthelabelsonwhich
themodelwastrainedon. Forexample,foramodeltrainedontheCIFAR10dataset,samplesfrom
theSVHNdatasetareOODsamplesforsuchamodel.
9
nigraM nigraM• Misclassification Detection (MisD) It consists in detecting if the classifier’s prediction is
incorrect. ThisisalsoreferredtoasFailureDetection(Corbièreetal.,2019)orTrustworthiness
Detection (Jiang et al., 2018; Luo et al., 2021). MisD is often used for selective classification
(classificationwitharejectoption)(Geifman&El-Yaniv,2017)toabstainfrompredictingsamples
onwhichthemodelislikelytobewrong. Ascorefornon-robustdetectioncannottellifthesample
isincorrect,asavulnerablesamplecouldbefromanysideofthedecisionboundary. Recentwork
byMoutonetal.(2024)showsthatinputmarginscanpredictthegeneralizationgaponlyinspecific
constraineddirectionsthatexplainthevarianceofthetrainingdatabutnotingeneral.
Formalrobustnessverificationaimsatcertifyingwhetheragivensampleisϵ-robustorifitisnot
anadversarialcounter-examplecanbeprovided(Brixetal.,2023b). Somecompleteexactmethods
basedonsolvingSatisfiabilityModuloTheoryproblems(Katzetal.,2017;Carlinietal.,2017;Huang
etal.,2017)orMixed-IntegerLinearProgramming(Chengetal.,2017;Lomuscio&Maganti,2017;
Fischetti&Jo,2017)provideformalcertificationgivenenoughtime. However,inpractice,theyare
tractableonlyupto100,000activations(Tjengetal.,2019). Incompletebuteffectivemethodsbased
onlinearandconvexrelaxationmethodsandBranch-and-Boundmethods(Zhangetal.,2018;Salman
etal.,2019;Xuetal.,2020,2021;Zhangetal.,2022;Shietal.,2023)arefasterbutconservative,
withoutguaranteedcertificationsevenifgivenenoughtime. Scalingthemtolargerarchitectures
suchasWideResNetsandlargeTransformersisstillchallengingevenwithGPUaccelartion(Brix
etal.,2023a;Königetal.,2024). Wengetal.(2018)convertstheproblemoffindingtherobustradius
(inputmargin)asalocalLipschitzconstantestimationproblem. ComputingtheLipschitzconstantof
DeepNetsisNP-hard(Virmaux&Scaman,2018)andJordan&Dimakis(2020)provedthatthereis
noefficientalgorithmtocomputethelocalLipschitzconstant. TheestimationprovidedbyWeng
et al. (2018) requires random sampling and remains computationally expensive to obtain a good
approximation. Vulnerabilitydetectionwithmargin-consistentmodelsdoesnotprovidecertificates
butanempiricalestimationoftherobustnessofasampleasevaluatedbyadversarialattacks. Atscale,
itcanhelpfilterthesamplestoundergoformalverificationandamorethoroughadversarialattack
forresourceprioritization.
5 LimitationsandPerspectives
Vulnerabilitydetectionscope Thescopeofthisworkisℓ robustnessmeasuredbytheinputspace
p
margin;theminimumdistortionthatchangesthemodel’sdecisionwhilethisdoesnotgiveafull
viewoftheℓ robustness. Samplesmaybeatthesamedistancetothedecisionboundaryandhave
p
unequalunsafeneighbourhoodsgivenanaverageestimationovertheϵ-neighbourhoodconsidered.
Theaverageestimationoflocalrobustnessforagivenϵ-neighborhoodremainsanopenproblem,so
whetheritispossibletoextractothernotionsofrobustnessfromthefeaturerepresentationefficiently
couldbeapotentialavenueforfurtherexploration.
Attack-basedverification Themarginconsistencypropertydoesnotrelyonattacks;however,its
verificationandthelearningofapseudo-marginwithanattack-basedestimationmaynotbepossible
ifthemodelcannotbeattackedonsufficientsamples. Theimplicitassumptionisthatwecanalways
successfullyprovidetheclosestpointtothedecisionwithasufficientbudget. Thisisareasonable
assumptionsincethestudiedmodelsarenotperfectlyrobust,andtheempiricalevidencesofarwith
adaptive attacks is that no defence is foolproof, which justifies the need to detect the non-robust
samples. Insomecases, wemightneedtocombinewithanattacksuchasCW-attack (Carlini&
Wagner,2016)tofindtheclosestadversarialsample.
Influenceofterminalphaseoftraining TheworkofPapyanetal.(2020)showsthatwhendeep
neuralnetworkclassifiersaretrainedbeyondzerotrainingerrorandbeyondzerocross-entropyloss
(akaterminalphaseoftraining),theyfallintoastateknownasneuralcollapse. Neuralcollapseisa
statewherethewithin-classvariabilityofthefeaturerepresentationscollapsestotheirclassmeans,
theclassmeans,andtheclassifiersbecomeself-dualandconvergetoaspecificgeometricstructure,
anequiangulartightframe(ETF)simplex,andthenetworkclassifierconvergestonearesttrainclass
center. Thisimpliesthatwemaylosethemarginconsistencyproperty. Whileneuralcollapsepredicts
thatallrepresentationscollapseontheirclassmean,inpractice,perfectcollapseisnotquiteachieved,
anditispreciselythedivergenceofarepresentationfromitsclassmean(orequivalentlyitsclassifier’s
classmean)thatencodestheinformationweseekaboutthedistancetothedecisionboundaryinthe
inputspace. Exploringtheimpactoftheneuralcollapseonmarginconsistencyasmodelstendtoward
acollapsedstatecouldprovidevaluableinsightsintogeneralizationandadversarialrobustness.
106 Conclusion
This work addresses the question of efficiently estimating local robustness in the ℓ sense at a
p
per-instancelevelinrobustdeepneuralclassifiersindeploymentscenarios. Weintroducemargin
consistencyasanecessaryandsufficientconditiontousethelogitmarginofadeepclassifierasa
reliableproxyestimationoftheinputmarginfordetectingnon-robustsamples. Ourinvestigation
ofvariousrobustlytrainedmodelsshowsthattheyhavestrongmarginconsistency,whichleadsto
ahighperformanceofthelogitmarginsindetectingvulnerablesamplestoadversarialattacksand
estimatingrobustaccuracyonarbitrarilylargetestsetsusingonlyasmallsubset. Wealsofindthat
marginconsistencydoesnotalwayshold,withsomemodelshavingaweakcorrelationbetweenthe
inputmarginandthelogitmargin. Insuchcases, weshowthatitispossibletolearntomapthe
featurerepresentationtoabetter-correlatedpseudo-marginthatsimulatesthemarginconsistency
andperformsbetteronvulnerabilitydetection. Finally,wepresentsomelimitationsofthiswork,
mainlythescopeofrobustness,theattack-basedverificationandtheimpactofneuralcollapsein
terminalphasesoftraining. Beyonditshighlypracticalimportance,weseethisasamotivationto
extendtheanalysisofrobustmodelsandthepropertiesoftheirfeaturerepresentationsinthecontext
ofvulnerabilitydetection.
Acknowledgements
ThisworkissupportedbytheDEELProjectCRDPJ537462-18fundedbytheNaturalSciencesand
EngineeringResearchCouncilofCanada(NSERC)andtheConsortiumforResearchandInnovation
in Aerospace in Québec (CRIAQ), together with its industrial partners Thales Canada inc, Bell
TextronCanadaLimited,CAEincandBombardierinc.3
References
Addepalli, S., Jain, S., Sriramanan, G., Khare, S., and Radhakrishnan, V. B. Towards achieving
adversarialrobustnessbeyondperceptuallimits. InICML2021WorkshoponAdversarialMachine
Learning,2021. URLhttps://openreview.net/forum?id=SHB_znlW5G7.
Addepalli,S.,Jain,S.,Sriramanan,G.,andVenkateshBabu,R. Scalingadversarialtrainingtolarge
perturbationbounds. InEuropeanConferenceonComputerVision,pp.301–316.Springer,2022.
Araujo,A.,Havens,A.J.,Delattre,B.,Allauzen,A.,andHu,B. Aunifiedalgebraicperspectiveon
lipschitzneuralnetworks. InTheEleventhInternationalConferenceonLearningRepresentations,
2023. URLhttps://openreview.net/forum?id=k71IGLC8cfc.
Bastani,O.,Ioannou,Y.,Lampropoulos,L.,Vytiniotis,D.,Nori,A.,andCriminisi,A. Measuring
neuralnetrobustnesswithconstraints. Advancesinneuralinformationprocessingsystems,29,
2016.
Biggio, B., Corona, I.,Maiorca, D., Nelson, B., Šrndic´, N., Laskov, P.,Giacinto, G., andRoli, F.
Evasionattacksagainstmachinelearningattesttime. InJointEuropeanconferenceonmachine
learningandknowledgediscoveryindatabases,pp.387–402.Springer,2013.
Brix,C.,Bak,S.,Liu,C.,andJohnson,T.T. Thefourthinternationalverificationofneuralnetworks
competition(vnn-comp2023): Summaryandresults. arXivpreprintarXiv:2312.16760,2023a.
Brix,C.,Müller,M.N.,Bak,S.,Johnson,T.T.,andLiu,C. Firstthreeyearsoftheinternational
verificationofneuralnetworkscompetition(vnn-comp). InternationalJournalonSoftwareTools
forTechnologyTransfer,25(3):329–339,2023b.
Carlini,N.andWagner,D. Adversarialexamplesarenoteasilydetected: Bypassingtendetection
methods. InProceedingsofthe10thACMworkshoponartificialintelligenceandsecurity,pp.
3–14,2017.
Carlini,N.andWagner,D.A. Towardsevaluatingtherobustnessofneuralnetworks. 2017IEEE
SymposiumonSecurityandPrivacy(SP),pp.39–57,2016.
3https://deel.quebec
11Carlini,N.,Katz,G.,Barrett,C.,andDill,D.L. Provablyminimally-distortedadversarialexamples.
arXivpreprintarXiv:1709.10207,2017.
Carmon,Y.,Raghunathan,A.,Schmidt,L.,Duchi,J.C.,andLiang,P.S. Unlabeleddataimproves
adversarialrobustness. Advancesinneuralinformationprocessingsystems,32,2019.
Chattamvelli,R. CorrelationinEngineeringandtheAppliedSciences: ApplicationsinR. Springer
Nature,2024.
Cheng,C.-H.,Nührenberg,G.,andRuess,H. Maximumresilienceofartificialneuralnetworks. In
AutomatedTechnologyforVerificationandAnalysis: 15thInternationalSymposium,ATVA2017,
Pune,India,October3–6,2017,Proceedings15,pp.251–268.Springer,2017.
Cisse,M.,Bojanowski,P.,Grave,E.,Dauphin,Y.,andUsunier,N. Parsevalnetworks: Improving
robustnesstoadversarialexamples. InInternationalconferenceonmachinelearning,pp.854–863.
PMLR,2017.
Corbière, C., Thome, N., Bar-Hen, A., Cord, M., andPérez, P. Addressingfailurepredictionby
learningmodelconfidence. AdvancesinNeuralInformationProcessingSystems,32,2019.
Croce,F.andHein,M. Minimallydistortedadversarialexampleswithafastadaptiveboundaryattack.
InInternationalConferenceonMachineLearning,pp.2196–2205.PMLR,2020a.
Croce,F.andHein,M. Reliableevaluationofadversarialrobustnesswithanensembleofdiverse
parameter-freeattacks. InInternationalconferenceonmachinelearning,pp.2206–2216.PMLR,
2020b.
Croce,F.,Andriushchenko,M.,Sehwag,V.,Debenedetti,E.,Flammarion,N.,Chiang,M.,Mittal,
P.,andHein,M. Robustbench: astandardizedadversarialrobustnessbenchmark. InThirty-fifth
ConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2021.
URLhttps://openreview.net/forum?id=SSKZPJCt7B.
Cui,J.,Tian,Z.,Zhong,Z.,Qi,X.,Yu,B.,andZhang,H. Decoupledkullback-leiblerdivergence
loss. arXivpreprintarXiv:2305.13948,2023.
Debenedetti,E.,Sehwag,V.,andMittal,P. Alightrecipetotrainrobustvisiontransformers. In2023
IEEEConferenceonSecureandTrustworthyMachineLearning(SaTML),pp.225–253.IEEE,
2023.
Ding,G.W.,Sharma,Y.,Lui,K.Y.C.,andHuang,R. Mmatraining: Directinputspacemarginmax-
imizationthroughadversarialtraining. InInternationalConferenceonLearningRepresentations,
2020. URLhttps://openreview.net/forum?id=HkeryxBtPB.
Dreossi,T.,Ghosh,S.,Sangiovanni-Vincentelli,A.,andSeshia,S.A. Aformalizationofrobustness
fordeepneuralnetworks. arXivpreprintarXiv:1903.10033,2019.
Elsayed,G.,Krishnan,D.,Mobahi,H.,Regan,K.,andBengio,S. Largemargindeepnetworksfor
classification. Advancesinneuralinformationprocessingsystems,31,2018.
Engstrom,L.,Ilyas,A.,Salman,H.,Santurkar,S.,andTsipras,D. Robustness(pythonlibrary),2019.
URLhttps://github.com/MadryLab/robustness.
Evtimov,I.,Eykholt,K.,Fernandes,E.,Kohno,T.,Li,B.,Prakash,A.,Rahmati,A.,andSong,D.
Robustphysical-worldattacksonmachinelearningmodels. arXivpreprintarXiv:1707.08945,2
(3):4,2017.
Fawzi,A.,Fawzi,H.,andFawzi,O. Adversarialvulnerabilityforanyclassifier. Advancesinneural
informationprocessingsystems,31,2018.
Fischetti,M.andJo,J. Deepneuralnetworksas0-1mixedintegerlinearprograms: Afeasibility
study. arXivpreprintarXiv:1712.06174,2017.
Geifman,Y.andEl-Yaniv,R. Selectiveclassificationfordeepneuralnetworks. Advancesinneural
informationprocessingsystems,30,2017.
12Gnanasambandam,A.,Sherman,A.M.,andChan,S.H. Opticaladversarialattack. InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pp.92–101,2021.
Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples.
InBengio,Y.andLeCun,Y.(eds.),3rdInternationalConferenceonLearningRepresentations,
ICLR2015, SanDiego, CA,USA,May7-9, 2015, ConferenceTrackProceedings, 2015. URL
http://arxiv.org/abs/1412.6572.
Gourdeau,P.,Kanade,V.,Kwiatkowska,M.,andWorrell,J. Onthehardnessofrobustclassification.
TheJournalofMachineLearningResearch,22(1):12521–12549,2021.
Han,T.,Srinivas,S.,andLakkaraju,H. Efficientestimationoflocalrobustnessofmachinelearning
models. InICML3rdWorkshoponInterpretableMachineLearninginHealthcare(IMLH),2023.
URLhttps://openreview.net/forum?id=ZGSfAElJmp.
He,C.,Wang,C.,Zhong,Y.-X.,andLi,R.-F. Asurveyonlearningtorank. In2008International
ConferenceonMachineLearningandCybernetics,volume3,pp.1734–1739.Ieee,2008.
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearningforimagerecognition. InProceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition,pp.770–778,2016.
Hendrycks,D.andGimpel,K.Abaselinefordetectingmisclassifiedandout-of-distributionexamples
in neural networks. In International Conference on Learning Representations, 2017. URL
https://openreview.net/forum?id=Hkg4TI9xl.
Hendrycks, D., Lee, K., andMazeika, M. Usingpre-trainingcanimprovemodelrobustnessand
uncertainty. InInternationalconferenceonmachinelearning,pp.2712–2721.PMLR,2019.
Huang,X.,Kwiatkowska,M.,Wang,S.,andWu,M. Safetyverificationofdeepneuralnetworks. In
Internationalconferenceoncomputeraidedverification,pp.3–29.Springer,2017.
Jiang,H.,Kim,B.,Guan,M.,andGupta,M. Totrustornottotrustaclassifier. Advancesinneural
informationprocessingsystems,31,2018.
Jordan, M. and Dimakis, A. G. Exactly computing the local lipschitz constant of relu networks.
AdvancesinNeuralInformationProcessingSystems,33:7344–7353,2020.
Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373,2018.
Katz,G.,Barrett,C.,Dill,D.L.,Julian,K.,andKochenderfer,M.J. Reluplex:Anefficientsmtsolver
forverifyingdeepneuralnetworks.InComputerAidedVerification:29thInternationalConference,
CAV2017,Heidelberg,Germany,July24-28,2017,Proceedings,PartI30,pp.97–117.Springer,
2017.
König,M.,Bosman,A.W.,Hoos,H.H.,andvanRijn,J.N. Criticallyassessingthestateoftheartin
neuralnetworkverification. JournalofMachineLearningResearch,25(12):1–53,2024.
Li, Q., Haque, S., Anil, C., Lucas, J., Grosse, R. B., and Jacobsen, J.-H. Preventing gradient
attenuation in lipschitz constrained convolutional networks. Advances in neural information
processingsystems,32,2019.
Liu,F.,Han,B.,Liu,T.,Gong,C.,Niu,G.,Zhou,M.,Sugiyama,M.,etal. Probabilisticmarginsfor
instancereweightinginadversarialtraining. AdvancesinNeuralInformationProcessingSystems,
34:23258–23269,2021.
Lomuscio,A.andMaganti,L. Anapproachtoreachabilityanalysisforfeed-forwardreluneural
networks. arXivpreprintarXiv:1706.07351,2017.
Luo,Y.,Wong,Y.,Kankanhalli,M.S.,andZhao,Q. Learningtopredicttrustworthinesswithsteep
slopeloss. AdvancesinNeuralInformationProcessingSystems,34:21533–21544,2021.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., andVladu, A. Towardsdeeplearningmodels
resistanttoadversarialattacks. InInternationalConferenceonLearningRepresentations,2018.
URLhttps://openreview.net/forum?id=rJzIBfZAb.
13Mickisch, D., Assion, F., Greßner, F., Günther, W., and Motta, M. Understanding the decision
boundaryofdeepneuralnetworks: Anempiricalstudy. arXivpreprintarXiv:2002.01810,2020.
Moosavi-Dezfooli,S.-M.,Fawzi,A.,andFrossard,P. Deepfool: asimpleandaccuratemethodto
fooldeepneuralnetworks. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pp.2574–2582,2016.
Mouton, C., Theunissen, M. W., and Davel, M. H. Input margins can predict generalization too.
ProceedingsoftheAAAIConferenceonArtificialIntelligence,38(13):14379–14387,Mar.2024.
doi: 10.1609/aaai.v38i13.29351. URLhttps://ojs.aaai.org/index.php/AAAI/article/
view/29351.
Pang,T.,Lin,M.,Yang,X.,Zhu,J.,andYan,S. Robustnessandaccuracycouldbereconcilableby
(proper)definition. InInternationalConferenceonMachineLearning,pp.17258–17277.PMLR,
2022.
Papyan,V.,Han,X.,andDonoho,D.L. Prevalenceofneuralcollapseduringtheterminalphaseof
deeplearningtraining. ProceedingsoftheNationalAcademyofSciences,117(40):24652–24663,
2020.
Peng,B.,Luo,Y.,Zhang,Y.,Li,Y.,andFang,Z. Conjnorm: Tractabledensityestimationforout-of-
distributiondetection.InProceedingsoftheInternationalConferenceonLearningRepresentations,
2024.
Rade,R.andMoosavi-Dezfooli,S.-M. Helper-basedadversarialtraining: Reducingexcessivemargin
toachieveabetteraccuracyvs.robustnesstrade-off. InICML2021WorkshoponAdversarial
MachineLearning,2021. URLhttps://openreview.net/forum?id=BuD2LmNaU3a.
Rebuffi, S.-A., Gowal, S., Calian, D. A., Stimberg, F., Wiles, O., and Mann, T. Fixing data
augmentationtoimproveadversarialrobustness. arXivpreprintarXiv:2103.01946,2021.
Rice,L.,Wong,E.,andKolter,Z. Overfittinginadversariallyrobustdeeplearning. InInternational
ConferenceonMachineLearning,pp.8093–8104.PMLR,2020.
Salman,H.,Yang,G.,Zhang,H.,Hsieh,C.-J.,andZhang,P. Aconvexrelaxationbarriertotight
robustnessverificationofneuralnetworks. AdvancesinNeuralInformationProcessingSystems,
32:9835–9846,2019.
Sehwag, V., Mahloujifar, S., Handina, T., Dai, S., Xiang, C., Chiang, M., and Mittal, P. Robust
learningmeetsgenerativemodels: Canproxydistributionsimproveadversarialrobustness? arXiv
preprintarXiv:2104.09425,2021.
Serrurier, M., Mamalet, F., González-Sanz, A., Boissin, T., Loubes, J.-M., and Del Barrio, E.
Achieving robustness in classification using optimal transport with hinge regularization. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
505–514,2021.
Seshia,S.A.,Desai,A.,Dreossi,T.,Fremont,D.J.,Ghosh,S.,Kim,E.,Shivakumar,S.,Vazquez-
Chanlatte, M., and Yue, X. Formal specification for deep neural networks. In International
SymposiumonAutomatedTechnologyforVerificationandAnalysis,pp.20–34.Springer,2018.
Shi,Z.,Jin,Q.,Kolter,J.Z.,Jana,S.,Hsieh,C.-J.,andZhang,H. Formalverificationforneural
networkswithgeneralnonlinearitiesviabranch-and-bound. 2ndWorkshoponFormalVerification
ofMachineLearning(WFVML2023),2023.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R.
Intriguingpropertiesofneuralnetworks. InBengio,Y.andLeCun,Y.(eds.),2ndInternational
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
ConferenceTrackProceedings,2014. URLhttp://arxiv.org/abs/1312.6199.
Tjeng, V., Xiao, K. Y., and Tedrake, R. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=HyGIdiRqtm.
14Tramer,F. Detectingadversarialexamplesis(nearly)ashardasclassifyingthem. InInternational
ConferenceonMachineLearning,pp.21692–21702.PMLR,2022.
Virmaux,A.andScaman,K. Lipschitzregularityofdeepneuralnetworks: analysisandefficient
estimation. AdvancesinNeuralInformationProcessingSystems,31,2018.
Wang,Y.,Zou,D.,Yi,J.,Bailey,J.,Ma,X.,andGu,Q. Improvingadversarialrobustnessrequires
revisitingmisclassifiedexamples. InInternationalConferenceonLearningRepresentations,2020.
URLhttps://openreview.net/forum?id=rklOg6EFwS.
Wang,Z.,Pang,T.,Du,C.,Lin,M.,Liu,W.,andYan,S. Betterdiffusionmodelsfurtherimprove
adversarialtraining. InInternationalConferenceonMachineLearning(ICML),2023.
Weng,T.-W. Proven: Verifyingrobustnessofneuralnetworkswithaprobabilisticapproach-pow-
erpointpresentation.https://icml.cc/media/Slides/icml/2019/grandball(11-11-00)
-11-12-15-4739-proven_verifyi.pdf,2019. (Accessedon05/23/2023).
Weng,T.-W.,Zhang,H.,Chen,P.-Y.,Yi,J.,Su,D.,Gao,Y.,Hsieh,C.-J.,andDaniel,L. Evaluating
therobustnessofneuralnetworks: Anextremevaluetheoryapproach. InInternationalConference
onLearningRepresentations,2018. URLhttps://openreview.net/forum?id=BkUHlMZ0b.
Wu, D., Xia, S.-T., and Wang, Y. Adversarial weight perturbation helps robust generalization.
AdvancesinNeuralInformationProcessingSystems,33:2958–2969,2020.
Xu,K.,Shi,Z.,Zhang,H.,Wang,Y.,Chang,K.-W.,Huang,M.,Kailkhura,B.,Lin,X.,andHsieh,
C.-J. Automaticperturbationanalysisforscalablecertifiedrobustnessandbeyond. Advancesin
NeuralInformationProcessingSystems,33,2020.
Xu,K.,Zhang,H.,Wang,S.,Wang,Y.,Jana,S.,Lin,X.,andHsieh,C.-J.FastandComplete:Enabling
completeneuralnetworkverificationwithrapidandmassivelyparallelincompleteverifiers. In
InternationalConferenceonLearningRepresentations,2021. URLhttps://openreview.net/
forum?id=nVZtXBI6LNn.
Xu,W.,Evans,D.,andQi,Y. Featuresqueezing: Detectingadversarialexamplesindeepneural
networks. arXivpreprintarXiv:1704.01155,2017.
Xu, Y., Sun, Y., Goldblum, M., Goldstein, T., and Huang, F. Exploring and exploiting decision
boundary dynamics for adversarial robustness. In The Eleventh International Conference on
LearningRepresentations,2023. URLhttps://openreview.net/forum?id=aRTKuscKByJ.
Zhang,H.,Weng,T.-W.,Chen,P.-Y.,Hsieh,C.-J.,andDaniel,L. Efficientneuralnetworkrobustness
certificationwithgeneralactivationfunctions.AdvancesinNeuralInformationProcessingSystems,
31:4939–4948,2018. URLhttps://arxiv.org/pdf/1811.00866.pdf.
Zhang,H.,Yu,Y.,Jiao,J.,Xing,E.,ElGhaoui,L.,andJordan,M. Theoreticallyprincipledtrade-off
betweenrobustnessandaccuracy.InInternationalconferenceonmachinelearning,pp.7472–7482.
PMLR,2019.
Zhang, H., Wang, S., Xu, K., Wang, Y., Jana, S., Hsieh, C.-J., and Kolter, Z. A branch and
boundframeworkforstrongeradversarialattacksofReLUnetworks. InProceedingsofthe39th
InternationalConferenceonMachineLearning,volume162,pp.26591–26604,2022.
Zhang,J.,Zhu,J.,Niu,G.,Han,B.,Sugiyama,M.,andKankanhalli,M. Geometry-awareinstance-
reweightedadversarialtraining. arXivpreprintarXiv:2010.01736,2020.
Zhong,Z.,Tian,Y.,andRay,B.Understandinglocalrobustnessofdeepneuralnetworksundernatural
variations. InInternationalConferenceonFundamentalApproachestoSoftwareEngineering,pp.
313–337.Springer,Cham,2021.
Zhu,F.,Cheng,Z.,Zhang,X.-Y.,andLiu,C.-L. Openmix: Exploringoutliersamplesformisclassifi-
cationdetection. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.12074–12083,2023.
1570 W Wi it th h A Esu tt io mA att ta ec dk Distance 3.6 1.1 1.0 40 W Wi it th h A Esu tt io mA att ta ec dk Distance 0.6 0.8
65
0.6 1.1
35
60 0.7 0.7 1.4 2.1 0.4 0.4 0.4 0.7
55 1.5 0.8 0.6 0.4 30 0.5 0.6 0.4 0.3 0.4 0.4
50
6.1
25
45
0.6
0.9 0.8
0.9
40
0.9 20 0.6
MD0 MR0 CL0 AL0 DI0 TR0 EN0 AD10 AD20 ZH0 HE0 SE10 DS0 DM0 RE80 PA20 XU80 WA80 CU80 RI1 AD1 AD21 HE1 RE11 WU1 RA11 PA21 CU41 RE81 DS1 WA81 CU81
Models Models
(a)CIFAR10 (b)CIFAR100
Figure8: RobustAccuracyreportedinRobustbenchvsRobustAccuracyestimatedwiththeinput
marginsestimations. Theyarebothclose,indicatingagoodestimationoftheinputmargins.
A ProofofTheorem1
Theorem 1. If a model is margin-consistent, then for any robustness threshold ϵ, there exists a
thresholdλforthelogitmargind thatperfectlyseparatesnon-robustsamplesandrobustsamples.
out
Conversely, if for any robustness threshold ϵ, d admits a threshold λ that perfectly separates
out
non-robustsamplesfromrobustsamples,thenthemodelismargin-consistent.
Proof. Formally,forafinitesampleS andnon-negativevaluesϵ≥0,λ≥0,wedefine:
AS :={x∈S :d (x)≤ϵ} and BS :={x∈S :d (x)≤λ}.
ϵ in λ out
Wesaythatd perfectlyseparatesnon-robustsamplesfromrobustsamplesifforanyfinitesample
out
S ⊆X andeveryϵ≥0thereexistsλ≥0suchthatAS =BS.
ϵ λ
Necessity: Let’sassumethatthemodelisnotmargin-consistent,i.e.,thereexisttwosamplesx and
1
x suchthatd (x )≤d (x )andd (x )>d (x ).BytakingS ={x ,x }andϵ=d (x )
2 out 1 out 2 in 1 in 2 1 2 in 2
wehavethatAS ={x }. Howeverforanyλ≥0,ifx ∈BS,thend (x )≤d (x )≤λand
ϵ 2 2 λ out 1 out 2
sox ∈BS. Therefored doesnotperfectlyseparatesnon-robustsamplesfromrobustsamples.
1 λ out
Sufficiency: Let’sassumethatthemodelismargin-consistent. LetS beafinitesampleandconsider
athresholdϵ. Letx betheelementofthefinitesetAS withmaximumd (x )andd (x ). Since
0 ϵ in 0 out 0
themodelismargin-consistent,thenforx∈S:
x∈AS ⇔d (x)≤ϵ⇔d (x)≤d (x )⇔d (x)≤d (x )⇔d (x)≤λ⇔x∈BS.
ϵ in in in 0 out out 0 out λ
(cid:124) (cid:123)(cid:122) (cid:125)
marginconsistency
ThismeanswehaveAS =BS ,,whichshowsthatd perfectlyseparatesnon-robustsamplesfrom
ϵ λ0 out
robustsamples.
B SupportingMaterials
B.1 InputMarginsEstimationSanityCheck
Onewaytoverifythereliabilityofourestimatedinputmarginsistocomparetherobustaccuracy
measuredbyAutoAttackatϵ = 8/255andtheproportionofcorrectlyclassifiedtestsampleswith
estimatedinputmarginsgreaterthanϵ;bothquantitiesshouldbeapproximatelyequalwhichhappens
tobethecase–SeeFig.8.
B.2 DetectionPerformanceatDifferentValuesofϵ
WepresentinFig.9theperformanceofthedetectionforvariousvaluesoftherobustnessthreshold.
Wecanseethatthestrongmarginconsistencyallowsthelogitmargintobeagoodproxyfordetection
16
ycaruccA
tsuboR
ycaruccA
tsuboRCIFAR10 AD10( : 0.82)
1.0 1.0
0.8 0.8
0.6 0.6
=2/255 (AUC = 0.99) =2/255 (AUPR = 0.97)
=4/255 (AUC = 0.99) =4/255 (AUPR = 0.97)
0.4 =8/255 (AUC = 0.98) 0.4 =8/255 (AUPR = 0.98)
=12/255 (AUC = 0.97) =12/255 (AUPR = 0.98)
=16/255 (AUC = 0.97) =16/255 (AUPR = 0.99)
0.2 0.2
=32/255 (AUC = 0.97) =32/255 (AUPR = 1.00)
chance level (AUC = 0.5) chance level (AUC = 0.5)
perfect level (AUC = 1.0) perfect level (AUC = 1.0)
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate Recall
(a)ModelAD10(Addepallietal.,2021)onCIFAR10
CIFAR100 RA11( : 0.84)
1.0 1.0
0.8 0.8
0.6 0.6
=2/255 (AUC = 0.99) =2/255 (AUPR = 0.98)
=4/255 (AUC = 0.99) =4/255 (AUPR = 0.98)
0.4 =8/255 (AUC = 0.98) 0.4 =8/255 (AUPR = 0.99)
=12/255 (AUC = 0.98) =12/255 (AUPR = 0.99)
=16/255 (AUC = 0.97) =16/255 (AUPR = 1.00)
0.2 0.2
=32/255 (AUC = 0.98) =32/255 (AUPR = 1.00)
chance level (AUC = 0.5) chance level (AUC = 0.5)
perfect level (AUC = 1.0) perfect level (AUC = 1.0)
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate Recall
(b)RA11(Rade&Moosavi-Dezfooli,2021)onCIFAR100
Figure9: VariationofAUROCscorefordifferentvaluesofthethresholdϵ.
atvariousthresholds. Notethatbelowϵ = 2/255andbeyondϵ = 16/255,theratioofvulnerable
pointstonon-vulnerablepointsbecomestooimbalanced,withlittletonopositiveinstancesbeyond
ϵ=32/255.
B.3 SampleEfficientRobustAccuracyEstimation
WeplotthevariationoftheabsoluteerrorwithsubsetsizefortheapproximationoftheAutoAttack
RobustAccuracybytheestimationofAlgorithm1. ResultsarepresentedinFig.10andFig.11for
CIFAR10andCIFAR100,respectively. From100samples,theapproximationisalreadygoodfor
somemodels.
B.4 Pseudo-marginLearningSetup
Thearchitectureandlearningsetupforthepseudo-marginisinspiredfromCorbièreetal.(2019). A
multilayerperceptron(Fig.12)learnsapseudo-marginfromthefeaturerepresentationsofthesamples
byminimizingthemean-squarederrorlossbetweentheoutputpseudo-marginandanestimationof
theinputmargin.
17
etaR
evitisoP
eurT
etaR
evitisoP
eurT
noisicerP
noisicerPMD0 MR0 CL0 AL0
10
8 =0.77 8 =0.68 10 10 =0.72
246 =0.75 =0.75
=0.75 =0.75=0.75=0.75
246 =0.66 =0.70
=0.68 =0.68 =0.68=0.68
2468 =0.77
=0.77 =0.77=0.76 =0.76=0.77 =0.77
2468 =0.73
=0.72 =0.72 =0.72 =0.73=0.72
0 0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
DI0 TR0 EN0 AD10
112 5050 =0.23=0.29=0.23
=0.27=0.28=0.28=0.28
11 246802 =0.76
=0.76
=0.75=0.75 =0.74=0.74 =0.74
11 246802 =0.74
=0.73=0.74 =0.74=0.74 =0.74=0.74
1234567 =0.83
=0.83
=0.83 =0.83=0.83=0.82 =0.82
0 0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
AD20 ZH0 HE0 SE10
10 6 =0.75
2468 =0.78
=0.82
=0.80 =0.80=0.81=0.81=0.81
11 246802 =0.75
=0.76 =0.76=0.74=0.75=0.76
=0.75
1 24680 =0.76
=0.77
=0.76 =0.75=0.76=0.76=0.76
12345
=0.73
=0.74=0.73 =0.74=0.74 =0.74
0 0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
DS0 DM0 RE80 PA20
2468
=0.77 =0.76 =0.74=0.74
=0.75=0.75
=0.75
11 246802
=0.67 =0.72 =0.70=0.71
=0.71=0.70
=0.71
123456 =0.78
=0.77=0.76
=0.77=0.77
=0.77
2468
=0.76 =0.79 =0.77=0.78
=0.78=0.78
=0.78
0 0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
XU80 WA80 CU80
50 10 =0.74
40 =0.43 =0.44 8 8 =0.76
123 000 =0.43=0.42=0.43
=0.45 =0.44
246
=0.74 =0.73=0.73=0.74=0.74
246 =0.74
=0.74=0.73=0.73 =0.74=0.74
0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
Figure10: CIFAR10: Variationoftheabsoluteerror, withrespecttosubsetsize. Theresultsare
averagedover10variantsforeachsubsetsize. TheKendallτ approximationisalsogivenabovethe
boxplot. Thecorrelationestimationdoesnotvaryalotwithsamplesize. Ingeneral,thevarianceof
theabsoluteerrorandthevaluedecreaseswiththeincreaseinsubsetsize. Inmostcases,theaverage
isalreadybelow2from300samples. Thetwomodelswithlowmarginconsistency(DI0andXU8)
alsohaveabadapproximation.
B.5 VerificationofEquidistanceAssumptionoftheLinearClassifiers
Eq.3inSec.2.1showsthatwecanapproximatethemargininthefeaturespacebythelogitmargin
iftheclassifiersw areequidistant, i.e. ∥w −wj∥ = C, ∀i,j ∈ {1,..,C}. Foreachmodel, we
j i
computedthe C(C−1) possiblevaluesofthedistancesbetweenpairsofclassifiers(45forCIFAR10
2
and 4950 for CIFAR100). We confirm this hypothesis for our investigated models in Fig. 13 by
plottingtheboxplotofthedistributionofvalues. Foreachmodel,thevaluesvaryonlyinasmall
range.
18MD0 MR0 CL0 AL0
10
8 =0.77 8 =0.68 10 10 =0.72
246 =0.75 =0.75
=0.75 =0.75=0.75=0.75
246 =0.66 =0.70
=0.68 =0.68 =0.68=0.68
2468 =0.77
=0.77 =0.77=0.76 =0.76=0.77 =0.77
2468 =0.73
=0.72 =0.72 =0.72 =0.73=0.72
0 0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
DI0 TR0 EN0 AD10
112 5050 =0.23=0.29=0.23
=0.27=0.28=0.28=0.28
11 246802 =0.76
=0.76
=0.75=0.75 =0.74=0.74 =0.74
11 246802 =0.74
=0.73=0.74 =0.74=0.74 =0.74=0.74
1234567 =0.83
=0.83
=0.83 =0.83=0.83=0.82 =0.82
0 0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
AD20 ZH0 HE0 SE10
10 6 =0.75
2468 =0.78
=0.82
=0.80 =0.80=0.81=0.81=0.81
11 246802 =0.75
=0.76 =0.76=0.74=0.75=0.76
=0.75
1 24680 =0.76
=0.77
=0.76 =0.75=0.76=0.76=0.76
12345
=0.73
=0.74=0.73 =0.74=0.74 =0.74
0 0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
DS0 DM0 RE80 PA20
2468
=0.77 =0.76 =0.74=0.74
=0.75=0.75
=0.75
11 246802
=0.67 =0.72 =0.70=0.71
=0.71=0.70
=0.71
123456 =0.78
=0.77=0.76
=0.77=0.77
=0.77
2468
=0.76 =0.79 =0.77=0.78
=0.78=0.78
=0.78
0 0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
XU80 WA80 CU80
50 10 =0.74
40 =0.43 =0.44 8 8 =0.76
123 000 =0.43=0.42=0.43
=0.45 =0.44
246
=0.74 =0.73=0.73=0.74=0.74
246 =0.74
=0.74=0.73=0.73 =0.74=0.74
0 0 0
50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000 50 100 200 300 500 1000 5000
Figure11: CIFAR100: Variationoftheabsoluteerrorwithrespecttosubsetsize. Theresultsare
averagedover10variantsforeachsubsetsize. TheKendallτ approximationisalsogivenabovethe
boxplot. Thecorrelationestimationdoesnotvaryalotwithsamplesize. Ingeneral,thevarianceof
theabsoluteerrorandthevaluedecreaseswiththeincreaseinsubsetsize. Inmostcases,theaverage
isalreadybelow2from300samples.
MLP
Feature
Classifier
Extractor
Logits
Feature
Representation
Figure12: LearningSetupforpseudo-margininspiredfromCorbièreetal.(2019).
19120
50
100
40
80
60 30
40
20
20
0 10
(a)CIFAR10 (b)CIFAR100
Figure 13: Equidistance of classifiers: the boxplot reports the distances’ minimum value, lower
quartile(Q1),median,upperquartile(Q3),andmaximumvalue. TheinterquartilerangeisQ3−Q1
issmallenoughformostofthemodels.
20
0ID 0SD 08UX 0MD 0RM 0DM 01ES 02AP 0EH 0RT 0LC 0NE 0LA 02DA 08UC 08ER 0HZ 08AW 01DA 0DA 02DA 08UC 04UC 0SD 0EH 02AP 01AR 08ER 01ER 0IR 08AW 0UW