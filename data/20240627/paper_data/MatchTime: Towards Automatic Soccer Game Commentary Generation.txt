MatchTime: Towards Automatic Soccer Game Commentary Generation
JiayuanRao1,2‚àó,HaoningWu1,2‚àó,ChangLiu1,2,YanfengWang1,2,WeidiXie1,2
1ShanghaiJiaoTongUniversity,China 2ShanghaiAILaboratory,China
https://haoningwu3639.github.io/MatchTime/
explores the development of a high-quality, auto-
Abstract maticsoccercommentarysystem.
In the literature on video understanding, there
Soccerisagloballypopularsportwithavastau-
hasbeenrelativelylittleattentiononsportsvideos.
dience,inthispaper,weconsiderconstructing
PioneeringworksuchasSoccerNet(Giancolaetal.,
anautomaticsoccergamecommentarymodel
2018a) introduced the first soccer game dataset,
toimprovetheaudiences‚Äôviewingexperience.
In general, we make the following contribu- containing videos of 500 soccer matches. Subse-
tions: First,observingtheprevalentvideo-text quently,SoccerNet-Caption(Mkhallatietal.,2023)
misalignment in existing datasets, we manu- compiledtextualcommentarydatafor471ofthese
ally annotate timestamps for 49 matches, es- matches from the Internet, establishing the first
tablishingamorerobustbenchmarkforsoccer
datasetandbenchmarkforsoccergamecommen-
game commentary generation, termed as SN-
tary. However, upon careful examination, we ob-
Caption-test-align;Second,weproposeamulti-
serve that the quality of existing data is often un-
modaltemporalalignmentpipelinetoautomat-
icallycorrectandfiltertheexistingdatasetat satisfactory. Forinstance,asillustratedinFigure1
scale, creating a higher-quality soccer game (left),sincethetextualcommentariesareoftencol-
commentary dataset for training, denoted as lected from the text live broadcast website, there
MatchTime;Third,basedonourcurateddataset, can be a delay with respect to the visual content,
wetrainanautomaticcommentarygeneration
leadingtoprevalentmisalignmentbetweentextual
model,namedMatchVoice. Extensiveexperi-
commentariesandvideoclips.
mentsandablationstudieshavedemonstrated
In this paper, we start by probing the effect of
theeffectivenessofouralignmentpipeline,and
theabove-mentionedmisalignmentonthesoccer
trainingmodelonthecurateddatasetsachieves
state-of-the-art performance for commentary gamecommentarysystems. Specifically,weman-
generation, showcasing that better alignment uallycorrectthetimestampsofcommentariesfor
can lead to significant performance improve- 49matchesintheSoccerNet-Captiontestsettoob-
mentsindownstreamtasks. tainanewbenchmark,termedasSN-Caption-test-
align. With manual check, we observe that these
1 Introduction misalignments can result in temporal offsets for
upto152seconds,withanaverageabsoluteoffset
Soccer,asoneofthemostpopularsportsglobally,
of16.63seconds. AsdepictedinFigure1(right),
hascaptivatedover3.5billionviewerswithitsdy-
after manual correction, pre-trained off-the-shelf
namicgameplayandintensemoments. Commen-
SN-Captionmodel(Mkhallatietal.,2023)hasex-
taryplaysacrucialroleinimprovingtheviewing
hibited large performance improvements, under-
experience,providingcontext,analysis,andemo-
scoringtheeffectoftemporalalignment.
tionalexcitementtotheaudience. However,creat-
Toaddresstheaforementionedmisalignmentis-
ingengagingandinsightfulcommentaryrequires
suebetweentextualcommentariesandvisualcon-
significantexpertiseandcanberesource-intensive.
tent,weproposeatwo-stagepipelinetoautomat-
In recent years, advancements in artificial intelli-
ically correct and filter the existing commentary
gence,particularlyinfoundationalvisual-language
trainingsetatscale. WefirstadoptWhisperX(Bain
models,haveopenednewpossibilitiesforautomat-
et al., 2023) to extract narration texts with corre-
ingvariousaspectsofcontentcreation. Thispaper
spondingtimestampsfromthebackgroundaudio,
*:Theseauthorscontributeequallytothiswork. whicharethensummarisedintoeventdescriptions
1
4202
nuJ
62
]VC.sc[
1v03581.6042:viXra35 w/o Align
ùí± ‚Ä¶ 30 w/ Align
25
20
23:53 28:16 28:41 45:19 83:46 84:12 15
Temporal Aligned Temporal Misaligned Unalignable Temporal Misaligned 10
Goal! [PLAYER] ([TEAM]) picks up the ball inside the box and We are about to witness a substitution. 5
ùíû fires in a shot which is deflected past [PLAYER]. He makes it 2:0. [PLAYER] ([TEAM]) for [PLAYER]. 0
T cah re
d
f ao nu dl ab y
y
e[P llL owAY isE dR u]
l
y([ sT hE oA wM
n
b])
y
i s
[R
w Eo Frt Eh Ry Eo Ef ]a
.
T poh se
s
ep se sr ic oe nn t ia
s
g 6e
6
o :3f
4
b .all B@1 B@4 METEOR ROUGE-L CIDEr
Figure1: Overview. (a)Left: Existingsoccergamecommentarydatasetscontainsignificantmisalignmentbetween
visualcontentandtextcommentaries. Weaimtoalignthemtocurateabettersoccergamecommentarybenchmark.
(b)Right: Whileevaluatingonmanuallyalignedvideos,existingmodelscanachievebettercommentaryqualityina
zero-shotmanner. (Thetemporalwindowsizeissetto10secondshere.)
byLLaMA-3(AI@Meta,2024)atfixedintervals. offset ‚â•0
Subsequently, we utilize LLaMA-3 to select the 800 offset <0
mostappropriatetimeintervalsbasedonthesim-
P!""#$%&‚Äô(=85.03%
600
P!""#$%&)*=60.21%
ilarity between these timestamped event descrip- P!""#$%&* =26.29%
tions and textual commentaries. Given such an
400
operationonlyprovidesroughalignment,wefur-
theralignthevideoandcommentarybytraininga 200
multi-modaltemporalalignmentmodelonasmall
setofmanuallyannotatedvideos. 0
-100 -50 0 50 100 150
Offset(s)
Ouralignmentpipelineenablestosignificantly
Figure2: Distributionoftemporaloffsetsinourman-
mitigate the temporal offsets between the visual
uallycorrectedSN-Caption-test-align. Throughmanual
content and textual commentaries, resulting in a
annotation, we find that the temporal discrepancy be-
higher-quality soccer game commentary dataset,
tweenthetextualcommentaryandthevisualcontentin
namedMatchTime. Withsuchacurateddataset,we
theexistingbenchmarkcanevenexceed100seconds.
furtherdevelopavideo-languagemodelbyconnect-
ingvisualencoderswithlanguagemodel,termedas 2 BenchmarkCuration
MatchVoice,thatenablestogenerateaccurateand
professionalcommentariesforsoccermatchvideos. Toprobetheeffectofmisalignmentontheperfor-
Experimentally, wehavethoroughlyinvestigated mance of soccer game commentary models, we
thedifferentvisualencoders,demonstratingstate- have manually annotated the timestamps of tex-
of-the-art (SOTA) performance in both precision tualcommentariesfor49matchesinthetestsetof
andcontextualrelevance. SoccerNet-Caption,resultinginanewbenchmark,
denotedasSN-Caption-test-align.
To summarize, we make the following contri-
butions: (i) we show the effect of misalignment Mannual Annotations. We recruit 20 football
in automatic commentary generation evaluation fanstomanuallyaligntextualcommentarieswith
by manually correcting the alignment errors in video content for 49 matches from the test set of
49 soccer matches, which can later be used as a SoccerNet-Caption(Mkhallatietal.,2023),follow-
newbenchmarkforthecommunity,termedasSN- ingseveralrules: (i)Volunteersshouldwatchthe
Caption-test-align, as will be detailed in Sec. 2; entirevideo,andadjustthetimestampsoforiginal
(ii) we further propose a multi-modal temporal textualcommentariestomatchthemomentswhen
video-textalignmentpipelinethatcorrectsandfil- events occur; (ii) To ensure the continuity of ac-
tersexistingsoccergamecommentarydatasetsat tionssuchasshots,passes,andfouls,themanually
scale, resulting in a high-quality training dataset annotatedtimestampsareadjusted1secondearlier
forcommentarygeneration,namedMatchTime,as tocapturethefullcontext;(iii)Forsceneswithre-
willbedetailedinSec.3;(iii)wepresentasoccer plays,thetimestampoftheevent‚Äôsfirstoccurrence
gamecommentarymodelnamedMatchVoice,es- ismarkedasthecorrespondingcommentarytimes-
tablishing a new state-of-the-art performance for tamptomaintainvisualintegrityandconsistency.
automaticsoccergamecommentarygeneration,as Here,ourannotateddatasetservestwopurposes:
willbedetailedinSec.4. first,itactsasamoreaccuratebenchmarkforevalu-
2
selpmaS#(a) Pre-processing with ASR and LLMs
Narration-text
ASR ‚Ä¶‚Ä¶ Summarize
60:34 ‚Äì 60:35 " Lehmann gets himself yellow. "
60:30 WhisperX 60:46 ‚Äì 60:48 " Also for him the third as for Rafinha‚Ä¶ " LLaMA-3
60:49 ‚Äì 60:51 " Foul to Kingsley Coman. "
‚Ä¶‚Ä¶
60:35 Event-Description
(Coarse-Aligned) ‚Ä¶‚Ä¶
60:20 - 60:30 "Bayern Munich needs to use these free kicks better, especially ‚Ä¶"
60:30 - 60:40 "Lehmann has been given a yellow card, which is his third of ‚Ä¶"
60:41
(annotated GT) 6 ‚Ä¶0 ‚Ä¶:40 - 60:50 "Kingsley Coman has been fouled, and Guardiola has made his ‚Ä¶‚Äù Coarse-Aligned
SN-Caption ùë° !=60:35
60:52 " Matthias Lehmann (1. FC Koln) is cautioned by the referee for
(SN-Caption) ùë° ! = 60:52 a foul that he committed a little earlier. "
‚ùÑ üî• üî• ‚ùÑ
Visual ‚Ä¶ ùî∏" Text
Encoder Encoder
ùë°ÃÉ =60:41
{ùêº } around ùë° !
! " ‚ùÑ: frozen parameters
(b) Fine-grained Temporal Alignment üî•: trainable parameters
Figure3: TemporalAlignmentPipeline. (a)Pre-processingwithASRandLLMs: WeuseWhisperXtoextract
narrationtextsandcorrespondingtimestampsfromtheaudio,andleverageLLaMA-3tosummarizetheseintoa
seriesoftimestampedevents,fordatapre-processing. (b)Fine-grainedTemporalAlignment: Weadditionallytrain
amulti-modaltemporalalignmentmodelonmanuallyaligneddata,whichfurtheralignstextualcommentariesto
theirbest-matchingvideoframesatafine-grainedlevel.
atingsoccergamecommentarygeneration;second, 3.1 ProblemFormulation
itcanbeusedassuperviseddatafortrainingand
Given a soccer match video from the SoccerNet-
evaluatingtemporalalignmentmodels.
Caption dataset, i.e., X = {V,C}, where V =
DataStatistics. Aftermanuallyannotatingthetest {(I ,tÀÜ ),...,(I ,tÀÜ )} denotes key frames of the
1 1 n n
setvideos,weobtainatotalof3,267videoclipand videoandtheircorrespondingtimestamps,andC =
text pairs. As depicted in Figure 2, we show the {(C ,t ),...,(C ,t )} represents the k textual
1 1 k k
temporal offset between the original noisy times- commentariesandtheirprovidedtimestampsinthe
tampsofthetextualcommentaryandthemanually video,withn ‚â´ k. Here,ourgoalistoimprovethe
annotatedgroundtruth,whichrangesfrom-108to soccergamecommentarydatasetbybetteraligning
152 seconds, with an average offset of 13.85 sec- textualcommentarieswithkeyframes. Concretely,
ondsandameanabsoluteoffsetof16.63seconds. we adopt a contrastive alignment pipeline to up-
Only26.29%,60.21%,74.96%,and85.03%ofthe datetheirtimestamps: tÀú= Œ¶(V,C;Œò ),whereŒò
1 1
data falls within 10s, 30s, 45s, and 60s windows denotesthetrainableparametersofthealignment
around the key frames, respectively. This high- modelŒ¶,andtÀúrepresentsthemodifiedtimestamps
lightstheseveremisalignmentinexistingdatasets, foralltextualcommentaries.
whichwillpotentiallyconfusethemodeltraining
forautomaticcommentarygeneration. 3.2 Method
As depicted in Figure 3, we propose a two-stage
3 AligningCommentaryandVideos temporal alignment pipeline: (i) pre-processing
withanoff-the-shelfautomaticspeechrecognition
Inthissection,wedevelopanautomaticpipeline model (ASR) and large language model (LLMs),
foraligningthetimestampsofgiventextualcom- (ii)trainanalignmentmodelwithcontrastivelearn-
mentaries to the corresponding video content in ing. Wewillelaborateonthedetailsasfollows.
existing soccer game commentary datasets. In Pre-processing with ASR and LLMs. We pro-
Sec.3.1,westartwiththeproblemformulationfor posetoroughlyalignthetextualcommentarywith
temporalalignment,andsubsequently,inSec.3.2, video content by leveraging the audio narration,
weelaborateonthedetailsofourproposedmulti- whichmayincludekeyeventdescriptions. Specifi-
modaltemporalalignmentpipeline. cally,wefirstadoptWhisperX(Bainetal.,2023)
3
PLM PLM
3-AMaLLforautomaticspeechrecognition(ASR),toobtain Datasets Alignment #SoccerMatches #Commentary
the converted narration text with corresponding
Test Manual 49 3,267
timestamp intervals from the audio. Given that Validation Automatic 49 3,418
livesoccercommentarytendstobefragmentedand Training Automatic 373 26,058
colloquial,weuseLLaMA-3(AI@Meta,2024)to
Table 1: Data Statistics on our MatchTime and SN-
summarizetheASRresultsintoeventdescriptions
Caption-test-aligndatasets.
foreach10-secondvideoclipwiththepromptde-
scribed in Appendix A.2. Subsequently, we feed
truthtimestampsareutilizedfortraining. Atinfer-
theseeventdescriptionsandthetextualcommen-
encetime,consideringthatdatapre-processinghas
tariesintoLLaMA-3topredictnewtimestampsfor
providedacoarsealignmentandthattheremightbe
thetextualcommentariesbasedonsentencesimi-
replaysinsoccermatchvideos,wesampleframes
laritiesusingthepromptdetailedinAppendixA.2.
at 1FPS from 45 seconds before and 30 seconds
Note that, as some videos may not have audio
afterthecurrenttextualcommentarytimestampas
commentary,ornarrationsthatareirrelevanttothe
visual candidates for alignment. To validate the
videocontent,suchasthebackgroundinformation
effectivenessofouralignmentmodel,weevaluate
forcertainplayers,suchpre-processingonlyallows
iton292samplesof4unseenannotatedmatches,
foracoarse-grainedalignmentofthecommentary
resultscanbefoundinSec.5.1.
tovideokeyframes.
Withthetrainedmodel,weperformfine-grained
Fine-grainedTemporalAlignment. Here,wefur- temporal alignment for each textual commentary
therproposetotrainamulti-modaltemporalalign- C by updating its timestamp to tÀú with tÀÜ of the
i i j
mentmodelwithcontrastivelearning. Concretely, visual frame I , which exhibits the highest cross-
j
we adoptpre-trained CLIP (Radfordet al., 2021) modalsimilarityscoreamongallthecandidates:
to encode textual commentaries and key frames,
followedbytrainableMLPs,i.e.,f(¬∑)andg(¬∑): tÀú i := tÀÜ j, where j = argmax(AÀÜ[i,:])
Usingthealignmentpipelinedescribedabove,we
C,V = f(Œ¶ (C)), g(Œ¶ (V))
CLIP-T CLIP-V
have aligned all the pre-processed training data
whereC ‚àà Rk√ód,V ‚àà Rn√ód denotestheresulting fromSoccerNet-Caption. Asforthematcheslack-
textualandvisualembeddings,respectively. ing audio, which cannot undergo pre-processing,
Wecomputetheaffinitymatrixbetweenthetex- wedirectlyapplyourfine-grainedtemporalalign-
tualcommentariesandvideokeyframesas: ment model. As a result, we have aligned 422
videos (373 as the training set and 49 as the vali-
C ¬∑V
AÀÜ[i,j] = i j , AÀÜ ‚àà Rk√ón dation set), amounting to 29,476 video-text pairs
||C ||¬∑||V ||
i j (26,058fortrainingand3,418forvalidation)into-
tal. Thiscontributesahigh-qualitydataset,termed
WiththemanualannotatedSN-Caption-test-align
as MatchTime, for training an automatic soccer
as introduced in Sec. 2, we can construct the
gamecommentarysystem. Thedetailedstatistics
groundtruthlabelmatrixwiththesameform,i.e.,
Y ‚àà {0,1}k√ón,Y[i,j] = 1ifthei-thcommentary ofourdatasetsarelistedinTable1.
correspondstothej-thkeyframe,otherwise0.
4 AutomaticSoccerGameCommentary
Wetrainthejointvisual-textualembeddingsfor
alignment with contrastive learning (Oord et al., Basedonthecurateddataset,weconsidertraining
2018),bymaximisingsimilarityscoresbetweenthe avisual-languagemodelforautomaticcommentary
commentaryanditscorrespondingvisualframe: generationongiveninputvideosegments,termed
asMatchVoice. Specifically,westartbydescribing
1
(cid:88)k (cid:20)(cid:80)n
j
Y[i,j]exp(AÀÜ[i,j])(cid:21)
theproblemscenario,andfollowedbyelaborating
L = ‚àí log
align k (cid:80)nexp(AÀÜ[i,j]) onourproposedarchitecture.
i=1 j
ProblemFormulation. Givenasoccergamevideo
TrainingandInference. Attrainingtime,weuse with multiple clips, i.e., V = {V ,V ,...,V },
1 2 T
the45manuallyannotatedvideoswith2,975video ourgoalistodevelopavisual-languagemodelthat
clip-text pairs from our curated SN-Caption-test- generates corresponding textual commentary for
align. Framessampledat1FPSwithatwo-minute eachvideosegment,i.e.,CÀÜ = Œ®(V ;Œò ),where
i i 2
window around the manually annotated ground Œò referstothetrainableparameters.
2
4Learnable queries ‚Ä¶
‚ùÑ: frozen parameters üî•: trainable parameters
‚Ä¶
Visual
MLP üî•
features
üî•Aggregator & MLP ‚Ä¶ ‚Ä¶
Prefix tokens [BOS] [A] [corner][is] [sent] ‚Ä¶ [header] [into]‚Ä¶ Feed forward
‚ùÑVisual Encoder Œ®
"$#
Œ® ‚ùÑ LLM Decoder Cross Attention
!"#
ùë°
√ó N Self Attention
ùêÇ" ‚Ä¶
[A] [corner][is] [sent] [to] ‚Ä¶ [goal] [.] [EOS] {ùë£ %,‚Ä¶,ùë£ &}
‚Ä¶
ùêï ‚Ñí #‚Äô(("$)*+, Learnable queries
(a) Architecture of MatchVoice (b) Temporal Aggregator & MLP
Figure4: MatchVoiceArchitectureOverview. OurproposedMatchVoicemodelleveragesapretrainedvisual
encoder to encode video frames into visual features. A learnable temporal aggregator aggregates the temporal
informationamongthesefeatures. ThetemporallyaggregatedfeaturesarethenprojectedintoprefixtokensofLLM
viaatrainableMLPprojectionlayer,togeneratethecorrespondingtextualcommentary.
Architecture. As depicted in Figure 4, our pro- Pre-processing (cid:37) (cid:33) (cid:37) (cid:33)
posedmodelcomprisesofthreecomponents. Here, Contrastive-Align (cid:37) (cid:37) (cid:33) (cid:33)
we focus on processing one segment, and ignore
avg(‚àÜ)(s) 10.21 -0.96 6.35 0.03
thesubscriptsforsimplicity. avg(|‚àÜ|)(s) 13.89 13.75 12.15 6.89
First, we adopt the frozen, pre-trained visual window 10(%) 35.32 34.86 77.06 80.73
window (%) 65.60 69.72 83.49 91.28
encodertocomputetheframewisefeatureswithin 30
window (%) 77.98 80.28 86.70 95.41
45
the video clip, i.e., {v ,v ,...,v } = Œ® (V). window (%) 88.07 85.32 90.37 98.17
1 2 n enc 60
Notethat,allvisualencodersareframewise,except
Table 2: Alignment Statistics. We report the tempo-
InternVideo,whichtakes8framespersecondand
raloffsetstatisticson4manuallyannotatedtestvideos
aggregatestheminto1featurevectorbyitself.
(comprisingatotalof292samples). ‚àÜandwindow
t
Second,weuseaPerceiver-likearchitecture(Jae- representthetemporaloffsetandthepercentageofcom-
gleetal.,2021)aggregatortoaggregatethetempo-
mentariesthatfallwithinawindowoftsecondsaround
thevisualkeyframes,respectively.
ralinformationamongvisualfeatures. Specifically,
we adopt two transformer decoder layers, with a
fixed-length learnable query, and visual features
5.1 Video-CommentaryTemporalAlignment
askeysandvalues,toobtainthetemporally-aware
features,i.e.,F = Œ® (v ,v ,...,v ). Inthispart,wefirstintroducetheimplementation
agg 1 2 n
detailsandevaluationmetricsofourtemporalalign-
Last, an MLP projection layer is used to map
mentpipeline,followedbyaquantitativecompari-
theoutputqueriesintodesiredfeaturedimensions,
sonandanalysisofthealignmentresults.
usedasprefixtokensforadecoder-onlylargelan-
guagemodel(LLMs),togeneratethedesiredtex- Implementation Details. We use pretrained off-
tualcommentary,i.e.,CÀÜ = Œ® (Œ® (F)). With the-shelf CLIP ViT-B/32 model to extract visual
dec proj
thegroundtruthcommentaryforthesoccervideo and textual features for our alignment pipeline,
clips,themodelisthentrainedwithstandardnega- which are then passed through two MLP layers
tivelog-likelihoodlossforlanguagegeneration. to get 512-dim features for contrastive learning.
WeusetheAdamW(LoshchilovandHutter,2019)
optimizerandthelearningrateissetto5√ó10‚àí4
5 Experiments totrainthealignmentmodelfor50epochs.
EvaluationMetrics. Toevaluatetemporalvideo-
Inthissection,weseparatelydescribetheexperi- textalignmentquality,wereportvariousmetricson
mentresultsfortheconsideredtasks,namely,soc- 4 unseen videos (with 292 samples) from our cu-
cer commentary alignment (Sec. 5.1), and auto- ratedSN-Caption-test-alignbenchmark,including
maticsoccercommentarygeneration(Sec.5.2). theaveragetemporaloffset(avg(‚àÜ)),theaverage
5
‚Ä¶
lanoitisoP
‚Ä¶Method VisualFeatures BLEU-1 BLEU-4 METEOR ROUGE-L CIDEr GPT-score
TrainedonoriginalSoccerNet-Caption
C3D 22.13 4.25 23.14 23.25 11.97 5.80
SN-Caption ResNet 26.46 5.33 23.58 23.58 13.71 6.28
Baidu 29.61 6.83 25.38 25.28 20.61 6.72
C3D 28.85 5.62 23.29 26.69 19.06 6.90
ResNet 28.75 5.87 23.78 26.69 20.65 6.75
MatchVoice
InternVideo 28.50 6.24 24.30 30.75 23.34 6.80
(Ours)
CLIP 28.65 6.62 24.20 27.33 24.35 6.78
Baidu 30.32 8.45 25.25 29.40 33.84 7.07
TrainedonouralignedMatchTime
C3D 26.81 5.24 23.57 23.12 13.78 6.27
SN-Caption ResNet 27.63 5.75 24.05 23.42 15.65 6.33
Baidu 29.74 7.31 26.40 26.19 23.74 6.84
C3D 28.67 6.55 24.46 27.38 26.53 6.89
ResNet 29.21 6.60 24.11 24.32 28.56 6.84
MatchVoice
InternVideo 29.18 6.89 25.04 28.18 30.22 6.99
(Ours)
CLIP 29.56 6.90 24.62 31.25 28.66 6.82
Baidu 31.42 8.92 26.12 29.66 38.42 7.08
Table 3: Quantitative comparison on Commentary Generation. All variants of baseline methods and our
MatchVoiceareretrainedonboththeoriginalunalignedSoccerNet-CaptionandourtemporallyalignedMatchTime
trainingsets,andthenevaluatedonourmanuallycuratedSN-Caption-test-alignbenchmark. Ineachunit,wedenote
thebestperformanceinREDandthesecond-bestperformanceinBLUE.
absolute temporal offset (avg(|‚àÜ|)), and the per- resultsfrombothquantitativeandqualitativeper-
centageoftextualcommentariesfallingwithin10s, spectives. Finally,wevalidatetheeffectivenessof
30s,45s,and60swindowsaroundeachkeyframe. themodulesthroughablationexperiments.
QuantitativeResults. AsdepictedinTable2,our ImplementationDetails. Ourautomaticcommen-
proposed automatic temporal alignment pipeline tarymodelcanutilizevariousvisualfeaturessuch
effectively aligns visual content and textual com- asC3D(Tranetal.,2015),ResNet(Heetal.,2016),
mentary in a coarse-to-fine manner. Specifically, Baidu (Zhou et al., 2021), CLIP (Radford et al.,
our approach reduces the average absolute offset 2021),andInternVideo(Wangetal.,2022). Allvi-
by7.0sseconds(from13.89secondsto6.89sec- sualfeaturesareextractedfromthevideoat2FPS,
onds)andsignificantlyenhancesthealignmentof except for InternVideo and Baidu, which are ex-
textualcommentarywithkeyframes. Itisimpor- tracted at 1FPS. The number of query vectors in
tanttohighlightthat,incomparisontosolelyusing thetemporalaggregatorisfixedat32,andtheMLP
acontrastivealignmentmodel,incorporatingdata projection layer projects the aggregated features
pre-processing enhances coarse alignment. This toa768-dimensionalprefixtokenthatisthenfed
provides a robust foundation for subsequent fine- intoLLaMA-3(AI@Meta,2024)fordecodingthe
grainedalignment,consistentlyleadingtofurther textual commentaries. The learning rate is set to
improvements in performance. Furthermore, the 1√ó10‚àí4 to train the commentary model for 100
proportionofcommentarythatalignswithinapre- epochs. All experiments are conducted with one
cise10-secondwindowincreasesdramaticallyby singleNvidiaRTXA100GPU.Forbaselines,we
45.41% (from 35.32% to 80.73%). Remarkably, retrainseveralvariantsofSN-Caption(Mkhallati
nearlyall(98.17%)textualcommentariesnowfall etal.,2023)usingitsofficialimplementation.
within a 60-second window surrounding the key
EvaluationMetrics. Toevaluatethequalityofgen-
frames,underscoringtheefficacyofourtwo-stage
eratedtextualcommentaries,weadoptvariouspop-
alignmentpipeline.
ularmetrics,includingBLEU(B)(Papinenietal.,
2002),METEOR(M)(BanerjeeandLavie,2005),
5.2 SoccerCommentaryGeneration
ROUGE-L(R-L)(Lin,2004),CIDEr(C)(Vedan-
In this part, we first elaborate on the implemen- tametal.,2015). Additionally,wealsoreportthe
tation details and evaluation metrics of the com- GPT-score(Fuetal.,2023),rangingfrom1to10,
mentarygenerationmodel. Then,weanalyzethe based on semantic information, expression accu-
6Align Win(s) B@1 B@4 M R-L C Coarse Fine B@1 B@4 M R-L C
10 25.02 5.00 23.32 24.65 19.34 (cid:37) (cid:37) 30.32 8.45 25.25 29.40 33.84
(cid:37) 30 30.32 8.45 25.25 29.40 33.84 (cid:33) (cid:37) 30.52 8.90 25.73 28.18 37.53
45 30.29 7.97 25.26 24.62 29.37
(cid:37) (cid:33) 30.55 8.81 26.03 29.40 36.13
60 30.08 8.60 25.41 23.96 35.08
(cid:33) (cid:33) 31.42 8.92 26.12 29.66 38.42
10 29.01 8.38 25.49 24.94 40.51
30 31.42 8.92 26.12 29.66 38.42
(cid:33) Table5: Ablationstudyonalignmentstrategy. The
45 30.07 8.32 25.65 29.65 36.51
quality of temporal alignment is directly reflected in
60 29.87 8.13 25.43 24.30 36.00
downstreamcommentarygenerationtasks: betteralign-
Table 4: Ablation study on window size. Using the mentleadstobettercommentarygenerationquality.
visualcontentwithin30saroundkeyframesyieldsthe
bestcommentaryperformance,andtemporalalignment
ditionally,thealigneddataimprovesperformance
ofdataleadstoauniversalperformanceimprovement.
acrossalltemporalwindowsettings,especiallyin
theextremecaseofa10swindow,demonstrating
racy,andprofessionalism. Thisscoreisprovided the necessity of temporal alignment. (ii) Align-
byGPT-3.5usingthegroundtruthandgenerated mentStrategy. Tovalidatethebenefitsoftempo-
textualcommentaryasinputs,withthepromptde-
ral alignment on downstream tasks, we train our
scribedinAppendixA.3.
MatchVoice model using data with different lev-
QuantitativeResults. AsdepictedinTable3,we els of alignment, with a fixed window size of 30
candrawthefollowingthreeobservations: (i)Our seconds, and compare their performance (where
proposedMatchVoicesignificantlyoutperformsex- ‚ÄòCoarse‚Äô refers to only data pre-processing and
isting methods in generating professional soccer ‚ÄòFine‚Äôstandsforfine-grainedtemporalalignment).
game commentary, establishing new state-of-the- AsdepictedinTable5,comparedtousingtheorig-
artperformance;(ii)Boththebaselinemethodsand inalmisaligneddataset,trainingoneithercoarse-
our MatchVoice benefit from temporally aligned alignedorfine-aligneddatasignificantlyimproves
data, demonstratingthesuperiorityandnecessity performance. Furthermore, the model trained on
oftemporalalignment; (iii)Commentarymodels thetwo-stagealigneddataexhibitsthelargestper-
basedonBaiduvisualencoderperformbetterthan formance improvement, which demonstrates the
others,weconjecturethisisbecausethepretrain- necessityoftemporalalignmenttoboostcommen-
ingonsoccerdatafurtherimprovesthequalityof tarygenerationquality.
commentarygeneration.
6 RelatedWorks
Qualitative Results. In Figure 5, we show the
predictionsfromourMatchVoicemodel,andcom-
Temporalvideo-textalignmentaimstoprecisely
pare them with baseline results and ground truth.
associate textual descriptions or narratives with
Itcanbeseenthatourproposedmodelcangener-
their corresponding video segments. Large-scale
ate textual commentaries for professional soccer
instructionalvideossuchasHowTo100M(Miech
gamesthatarericherinsemanticinformation,more
etal.,2019)andYouCook2(Zhouetal.,2018)have
accurate,andmorecomprehensive.
alreadycatalyzedextensivemulti-modalalignment
Ablation Studies. All ablation experiments are worksbasedonvision-languageco-training. Con-
conductedusingMatchVoicewithBaiduvisualfea- cretely,TAN(Hanetal.,2022)directlyalignspro-
tures. (i)WindowSize. Thesizeofthetemporal cedure narrations transcribed through Automatic
windowaffectsthenumberofinputframes,which Speech Recognition (ASR) with video segments.
in turn impacts the performance of commentary DistantSup(Linetal.,2022)andVINA(Mavroudi
generation. Therefore, we sample frames within et al., 2023) further explore leveraging external
windowsof10s,30s,45s,and60saroundthecom- knowledge bases (Koupaee and Wang, 2018) to
mentary timestamps, and then train and evaluate assistthealignmentprocess,whileLietal.(2023c)
the commentary generation model to assess the proposeintegratingbothactionandsteptextualin-
effectofwindowsizeongenerationquality. formationtoaccomplishthevideo-textalignment.
AsshowninTable4,ourMatchVoiceperforms Inthispaper,wetrainamulti-modalalignment
best with a window size of 30 seconds, since it model to automatically correct existing data and
providesenoughvisualinformationtosummarize build a higher-quality soccer game commentary
events without introducing excessive noise. Ad- dataset. Moreover, we further demonstrate the
7a
SN-Caption: [PLAYER] ([TEAM]) is caught offside !
MatchVoice:[PLAYER] ([TEAM]) is forced to stop his attacking move after the linesman
signals for offside.
GT:[PLAYER] ([TEAM]) is offside and the linesman raises his flag.
b SN-Caption: [PLAYER] ([TEAM]) takes the ball and sets it for the free kick .
MatchVoice: [PLAYER] ([TEAM]) picks up the ball on the edge of the box and produces
a brilliant low drive into the bottom right corner.
GT: The ball is whipped in from the free kick and finds the head of [PLAYER]
([TEAM]), who rises and produces an amazing header inside the right post.
c
SN-Caption: [PLAYER] ([TEAM]) takes the corner but fails to find any of his teammates .
MatchVoice: [PLAYER] ([TEAM]) takes the corner with a short pass.
GT: [PLAYER] ([TEAM]) quickly takes the corner kick with a short pass.
d SN-Caption: [PLAYER] ([TEAM]) is clearly asking for some medical attention with his
painful gestures . The extent of his injury is yet to be discovered .
MatchVoice: [PLAYER] ([TEAM]) is being forced to leave the pitch in order to receive
medical treatment and his team will play with a man short for a few minutes.
GT: The game is interrupted now, [PLAYER] ([TEAM]) picks up a knock and
the physio has to come on.
Figure5: Qualitativeresultsoncommentarygeneration. OurMatchVoicedemonstratesadvantagesinmultiple
aspects: (a)richersemanticdescriptions,(b)fullcommentariesofmultipleincidentsinasinglevideo,(c)accuracy
ofdescriptions,and(d)predictionsofincomingevents.
superiorityandindispensabilityofouralignment dressvariouschallengesrelatedtosoccer,including
pipeline through downstream commentary tasks, player detection (Vandeghen et al., 2022), action
confirmingitscriticalsignificance. spotting (Giancola et al., 2018a), replay ground-
Video captioning has been a long-standing re- ing (Held et al., 2023), player tracking (Cioppa
searchchallengeincomputervision(Krishnaetal., et al., 2022), camera calibration (Giancola et al.,
2017;Yangetal.,2023),primarilyduetothelim- 2018b)andre-identification(Deliegeetal.,2021).
itedannotationandexpensivecomputation. Bene- These endeavours have paved the way for more
fitingfromthedevelopmentofLLMs,recentmod- ambitiousresearchgoals,suchasutilizingAIfor
els, such as LLaMA-VID (Li et al., 2023b) and soccergamecommentary(Mkhallatietal.,2023;
Video-LLaMA(Zhangetal.,2023)proposestrate- Qi et al., 2023). Additionally, other approaches
giesforlinkingvisualfeaturestolanguageprompts, have targeted aspects of sports analysis, such as
harnessing the capabilities of LLaMA (Touvron basketball game narration (Yu et al., 2018) and
etal.,2023a,b)modelsforvideodescription. Fur- tacticsanalysis(Wangetal.,2024).
thermore,VideoChat(Lietal.,2023a,2024)treats Aconcurrentwork,SoccerNet-Echoes(Gautam
video captioning as a subtask of visual question etal.,2024)proposestoleverageaudiofromvideos
answering, while StreamingCaption (Zhou et al., forASRandtranslationtoobtainrichertextcom-
2024)cangeneratecaptionsforstreamingvideos mentary data. However, this approach overlooks
usingamemorymechanism. that unprocessed audios often contain non-game-
Notably,theAutoADseries(Hanetal.,2023b,a, relatedutterances,whichmayconfusemodeltrain-
2024)applyvideocaptioningtoaspecificdomain ing. Building upon the aforementioned progress,
‚Äì synthesizing descriptive narrations for movie our goal is to construct a dataset with improved
scenes to assist visually impaired individuals in alignmenttotrainamoreprofessionalsoccergame
watchingmovies. Similarly,inthecontextofsoc- commentarymodel,therebyachievingabetterun-
cer,adistinctivesportsscenario,wedevelopatai- derstandingofsportsvideo.
loredsoccergamecommentarymodeltoenrichthe
7 Conclusion
viewingexperienceforaudiences.
Sportsvideounderstanding(Thomasetal.,2017) In this paper, we consider a highly practical and
has widely attracted the interest of researchers commerciallyvaluabletask: automaticallygener-
due to its complexity and professional relevance. atingprofessionaltextualcommentaryforsoccer
EarlyworkssuchasFineGym(Shaoetal.,2020) games. Specifically, we have observed a preva-
and FineDiving (Xu et al., 2022) aim to develop lent misalignment between visual contents and
fine-grained datasets for action recognition and textual commentaries in existing datasets. To ad-
understanding in specific sports. Subsequently, dress this, we manually correct the timestamps
focusing on soccer, a series of SoccerNet (Gi- of textual commentary in 49 videos in the ex-
ancola et al., 2018a) datasets systematically ad- isting dataset, establishing a new benchmark for
8the community, termed as SN-Caption-test-align. ConferenceonComputerVisionandPatternRecog-
Buildinguponthemanuallycheckeddata,wepro- nition,pages3491‚Äì3502.
poseamulti-modaltemporalvideo-textalignment
Adrien Deliege, Anthony Cioppa, Silvio Giancola,
pipelinethatautomaticallycorrectsandfiltersex- MeisamJSeikavandi,JacobVDueholm,KamalNas-
istingdataatscale,whichenablesustoconstruct rollahi,BernardGhanem,ThomasBMoeslund,and
ahigher-qualitysoccergamecommentarydataset, MarcVanDroogenbroeck.2021. Soccernet-v2: A
dataset and benchmarks for holistic understanding
namedMatchTime. Basedonthecurateddataset,
of broadcast soccer videos. In Proceedings of the
wepresentMatchVoice,asoccergamecommen-
IEEE Conference on Computer Vision and Pattern
tarymodel,whichcanaccuratelygenerateprofes- Recognition,pages4508‚Äì4519.
sionalcommentaryforgivenmatchvideos,signifi-
JinlanFu,See-KiongNg,ZhengbaoJiang,andPengfei
cantlyoutperformingpreviousmethods. Extensive
Liu.2023. Gptscore: Evaluateasyoudesire. arXiv
experimentshavevalidatedthecriticalperformance preprintarXiv:2302.04166.
improvementsachievedthroughdataalignment,as
Sushant Gautam, Mehdi Houshmand Sarkhoosh, Jan
wellasthesuperiorityofourproposedalignment
Held,CiseMidoglu,AnthonyCioppa,SilvioGian-
pipelineandcommentarymodel.
cola, Vajira Thambawita, Michael A Riegler, P√•l
Halvorsen, and Mubarak Shah. 2024. Soccernet-
Limitations
echoes: A soccer game audio commentary dataset.
arXivpreprintarXiv:2405.07354.
AlthoughourproposedMatchVoicemodelcangen-
erate professional textual commentary for given Silvio Giancola, Mohieddine Amine, Tarek Dghaily,
and Bernard Ghanem. 2018a. Soccernet: A scal-
soccer game videos, it still inherits some limita-
abledatasetforactionspottinginsoccervideos. In
tionsfromexistingdataandmodels: (i)Following
ProceedingsoftheIEEEConferenceonComputer
previous work, our commentary remains anony- Vision and Pattern Recognition Workshops, pages
mousandcannotaccuratelydescribeplayerinfor- 1711‚Äì1721.
mation on the field. This is left for future work,
Silvio Giancola, Mohieddine Amine, Tarek Dghaily,
where we aim to further improve the dataset and
and Bernard Ghanem. 2018b. Soccernet: A scal-
incorporateknowledgeandgamebackgroundinfor- abledatasetforactionspottinginsoccervideos. In
mationasadditionalcontext;and(ii)MatchVoice ProceedingsoftheIEEEConferenceonComputer
Vision and Pattern Recognition Workshops, pages
may sometimes struggle to distinguish between
1711‚Äì1721.
highlysimilaractions,suchascornerkicksandfree
kicks. This mainly stems from the current frozen Silvio Giancola and Bernard Ghanem. 2021.
pre-trainedvisualencodersandlanguagedecoders. Temporally-aware feature pooling for action
spottinginsoccerbroadcasts. InProceedingsofthe
Ourpreliminaryfindingssuggestthatfine-tuning
IEEE Conference on Computer Vision and Pattern
on soccer-specific data might effectively address
Recognition,pages4490‚Äì4499.
thisissueinthefuture.
Tengda Han, Max Bain, Arsha Nagrani, Gul Varol,
WeidiXie,andAndrewZisserman.2023a. Autoad
References ii: Thesequel-who,when,andwhatinmovieaudio
description. InProceedingsoftheInternationalCon-
AI@Meta.2024. Llama3modelcard. ferenceonComputerVision,pages13645‚Äì13655.
MaxBain,JaesungHuh,TengdaHan,andAndrewZis- Tengda Han, Max Bain, Arsha Nagrani, G√ºl Varol,
serman.2023. Whisperx: Time-accuratespeechtran- WeidiXie,andAndrewZisserman.2023b. Autoad:
scriptionoflong-formaudio. INTERSPEECH2023. Moviedescriptionincontext. InProceedingsofthe
IEEE Conference on Computer Vision and Pattern
SatanjeevBanerjeeandAlonLavie.2005. Meteor: An Recognition,pages18930‚Äì18940.
automaticmetricformtevaluationwithimprovedcor-
relationwithhumanjudgments. InProceedingsof Tengda Han, Max Bain, Arsha Nagrani, G√ºl Varol,
theACLWorkshoponIntrinsicandExtrinsicEvalua- Weidi Xie, and Andrew Zisserman. 2024. Autoad
tionMeasuresforMachineTranslationand/orSum- iii: The prequel - back to the pixels. In Proceed-
marization,pages65‚Äì72. ingsoftheIEEEConferenceonComputerVisionand
PatternRecognition,pages18164‚Äì18174.
Anthony Cioppa, Silvio Giancola, Adrien Deliege,
LeKang,XinZhou,ZhiyuCheng,BernardGhanem, TengdaHan,WeidiXie,andAndrewZisserman.2022.
and Marc Van Droogenbroeck. 2022. Soccernet- Temporal alignment networks for long-term video.
tracking: Multipleobjecttrackingdatasetandbench- InProceedingsoftheIEEEConferenceonComputer
markinsoccervideos. InProceedingsoftheIEEE VisionandPatternRecognition,pages2906‚Äì2916.
9KaimingHe,XiangyuZhang,ShaoqingRen,andJian Ilya Loshchilov and Frank Hutter. 2019. Decoupled
Sun.2016. Deepresiduallearningforimagerecog- weightdecayregularization. InProceedingsofthe
nition. In Proceedings of the IEEE Conference on International Conference on Learning Representa-
ComputerVisionandPatternRecognition,pages770‚Äì tions.
778.
EffrosyniMavroudi,TriantafyllosAfouras,andLorenzo
JanHeld,AnthonyCioppa,SilvioGiancola,Abdullah Torresani. 2023. Learning to ground instructional
Hamdi, BernardGhanem, andMarcVanDroogen- articlesinvideosthroughnarrations. InProceedings
broeck.2023. Vars: Videoassistantrefereesystem of the IEEE Conference on Computer Vision and
for automated soccer decision making from multi- PatternRecognition,pages15201‚Äì15213.
pleviews. InProceedingsoftheIEEEConference
AntoineMiech,DimitriZhukov,Jean-BaptisteAlayrac,
onComputerVisionandPatternRecognition,pages
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
5085‚Äì5096.
2019. Howto100m: Learning a text-video embed-
ding by watching hundred million narrated video
SeppHochreiterandJ√ºrgenSchmidhuber.1997. Long
clips. In Proceedings of the International Confer-
short-termmemory. NeuralComputation,9(8):1735‚Äì
enceonComputerVision,pages2630‚Äì2640.
1780.
Hassan Mkhallati, Anthony Cioppa, Silvio Giancola,
Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol
Bernard Ghanem, and Marc Van Droogenbroeck.
Vinyals,AndrewZisserman,andJoaoCarreira.2021.
2023. Soccernet-caption: Dense video captioning
Perceiver:Generalperceptionwithiterativeattention.
forsoccerbroadcastscommentaries. InProceedings
InProceedingsoftheInternationalConferenceon
oftheIEEEConferenceonComputerVisionandPat-
MachineLearning,pages4651‚Äì4664.PMLR.
ternRecognitionWorkshops,pages5074‚Äì5085.
MahnazKoupaeeandWilliamYangWang.2018. Wiki- AaronvandenOord,YazheLi,andOriolVinyals.2018.
how: Alargescaletextsummarizationdataset. arXiv Representationlearningwithcontrastivepredictive
preprintarXiv:1810.09305. coding. arXivpreprintarXiv:1807.03748.
RanjayKrishna,KenjiHata,FredericRen,LiFei-Fei, KishorePapineni,SalimRoukos,ToddWard,andWei-
and Juan Carlos Niebles. 2017. Dense-captioning JingZhu.2002. Bleu: amethodforautomaticeval-
eventsinvideos. InProceedingsoftheInternational uation of machine translation. In Association for
ConferenceonComputerVision,pages706‚Äì715. ComputationalLinguistics,pages311‚Äì318.
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen- JiQi,JifanYu,TengTu,KunyuGao,YifanXu,Xinyu
haiWang,PingLuo,YaliWang,LiminWang,and Guan, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi
YuQiao.2023a. Videochat: Chat-centricvideoun- Li, et al. 2023. Goal: A challenging knowledge-
derstanding. arXivpreprintarXiv:2305.06355. groundedvideocaptioningbenchmarkforreal-time
soccercommentarygeneration. InProceedingsofthe
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, 32ndACMInternationalConferenceonInformation
Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo andKnowledgeManagement,pages5391‚Äì5395.
Chen, Ping Luo, Limin Wang, and Yu Qiao. 2024.
AlecRadford,JongWookKim,ChrisHallacy,Aditya
Mvbench: Acomprehensivemulti-modalvideoun-
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
derstandingbenchmark. InProceedingsoftheIEEE
try, Amanda Askell, Pamela Mishkin, Jack Clark,
ConferenceonComputerVisionandPatternRecog-
etal.2021. Learningtransferablevisualmodelsfrom
nition,pages22195‚Äì22206.
naturallanguagesupervision. InProceedingsofthe
InternationalConferenceonMachineLearning.
Yanwei Li, Chengyao Wang, and Jiaya Jia. 2023b.
Llama-vid: Animageisworth2tokensinlargelan-
Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. 2020.
guagemodels. arXivpreprintarXiv:2311.17043.
Finegym: A hierarchical video dataset for fine-
grainedactionunderstanding. InProceedingsofthe
ZeqianLi,QiruiChen,TengdaHan,YaZhang,Yanfeng
IEEE Conference on Computer Vision and Pattern
Wang, and Weidi Xie. 2023c. A strong baseline
Recognition,pages2616‚Äì2625.
for temporal video-text alignment. arXiv preprint
arXiv:2312.14055. Graham Thomas, Rikke Gade, Thomas B Moeslund,
PeterCarr,andAdrianHilton.2017. Computervi-
Chin-YewLin.2004. Rouge: Apackageforautomatic sion for sports: Current applications and research
evaluation of summaries. In Text Summarization topics. ComputerVisionandImageUnderstanding,
BranchesOut,pages74‚Äì81. 159:3‚Äì18.
XudongLin, FabioPetroni, GedasBertasius, Marcus HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Rohrbach, Shih-Fu Chang, and Lorenzo Torresani. Martinet,Marie-AnneLachaux,Timoth√©eLacroix,
2022. Learning to recognize procedural activities BaptisteRozi√®re,NamanGoyal,EricHambro,Faisal
withdistantsupervision. InProceedingsoftheIEEE Azhar, et al. 2023a. Llama: Open and effi-
ConferenceonComputerVisionandPatternRecog- cient foundation language models. arXiv preprint
nition,pages13853‚Äì13863. arXiv:2302.13971.
10Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- Luowei Zhou, Chenliang Xu, and Jason Corso. 2018.
bert, Amjad Almahairi, Yasmine Babaei, Nikolay Towardsautomaticlearningofproceduresfromweb
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti instructional videos. In Proceedings of the AAAI
Bhosale, et al. 2023b. Llama 2: Open founda- ConferenceonArtificialIntelligence,volume32.
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288. XinZhou,LeKang,ZhiyuCheng,BoHe,andJingyu
Xin. 2021. Feature combination meets attention:
DuTran,LubomirBourdev,RobFergus,LorenzoTor- Baidusoccerembeddingsandtransformerbasedtem-
resani, and Manohar Paluri. 2015. Learning spa- poraldetection. arXivpreprintarXiv:2106.14447.
tiotemporalfeatureswith3dconvolutionalnetworks.
XingyiZhou,AnuragArnab,ShyamalBuch,ShenYan,
InProceedingsoftheInternationalConferenceon
AustinMyers, XuehanXiong, ArshaNagrani, and
ComputerVision,pages4489‚Äì4497.
CordeliaSchmid.2024. Streamingdensevideocap-
tioning. InProceedingsoftheIEEEConferenceon
Renaud Vandeghen, Anthony Cioppa, and Marc
ComputerVisionandPatternRecognition.
VanDroogenbroeck.2022. Semi-supervisedtraining
to improve player and ball detection in soccer. In
ProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition,pages3481‚Äì3490.
RamakrishnaVedantam,CLawrenceZitnick,andDevi
Parikh. 2015. Cider: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
ConferenceonComputerVisionandPatternRecog-
nition,pages4566‚Äì4575.
YiWang,KunchangLi,YizhuoLi,YinanHe,Bingkun
Huang,ZhiyuZhao,HongjieZhang,JilanXu,YiLiu,
ZunWang,etal.2022. Internvideo: Generalvideo
foundationmodelsviagenerativeanddiscriminative
learning. arXivpreprintarXiv:2212.03191.
Zhe Wang, Petar VelicÀákovic¬¥, Daniel Hennes, Nenad
Toma≈°ev, Laurel Prince, Michael Kaisers, Yoram
Bachrach, Romuald Elie, Li Kevin Wenliang, Fed-
ericoPiccinini,etal.2024. Tacticai: anaiassistant
forfootballtactics. NatureCommunications,15(1):1‚Äì
13.
JinglinXu,YongmingRao,XuminYu,GuangyiChen,
JieZhou,andJiwenLu.2022. Finediving: Afine-
grained dataset for procedure-aware action quality
assessment. InProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition,pages
2949‚Äì2958.
AntoineYang,ArshaNagrani,PaulHongsuckSeo,An-
toine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef
Sivic,andCordeliaSchmid.2023. Vid2seq: Large-
scalepretrainingofavisuallanguagemodelfordense
videocaptioning. InProceedingsoftheIEEECon-
ferenceonComputerVisionandPatternRecognition,
pages10714‚Äì10726.
Huanyu Yu, Shuo Cheng, Bingbing Ni, Minsi Wang,
JianZhang,andXiaokangYang.2018. Fine-grained
videocaptioningforsportsnarrative. InProceedings
of the IEEE Conference on Computer Vision and
PatternRecognition,pages6006‚Äì6015.
HangZhang, XinLi, andLidongBing.2023. Video-
llama: An instruction-tuned audio-visual language
model for video understanding. In Proceedings of
the Conference on Empirical Methods in Natural
LanguageProcessinng.
11A Appendix LLaMA-3(AI@Meta,2024)topredictthetimes-
tampsforthetextualcommentariesbasedonsen-
A.1 DatasetSplit
tence similarity, providing a solid foundation for
We split the total 471 matches of our dataset fine-grainedalignment. Thepromptusedforthis
(including automatically aligned MatchTime and stepisasfollows:
manually curated SN-Caption-test-align bench-
"I have a text commentary of a soccer game
mark)intotraining(373matches),validation(49
event at the original time stamp: \n \nOrig-
matches), and test (49 matches) sets, consisting
inal timestamp here: {Original commentary
of 26,058, 3,418, and 3,267 video clip-text pairs,
here (from SoccerNet-Caption)} \n \n and I
respectively. Notably,alltestsamplesarefromour
want to locate the time of this commentary
manually checked SN-Caption-test-align, which
among the following events with timestamp:
servesasabetterbenchmarkonsoccergamecom-
\n {timestamp intervals of 10s: summarized
mentarygenerationforthecommunity.
events}. \n These are the words said by nar-
ratorandIwantyoutotemporallyalignthe
A.2 ImplementationDetails
firsttextcommentaryaccordingtothesewords
Inthissection,weprovideadditionaldetailsregard- bynarratorssincethereisafairchancethat
ingtheimplementationsasfollows. theoriginaltimestampissomehowinaccurate
BaselineMethods. Forbaselines,weretrainsev- in time. So please return me with a number
eralvariantsofSN-Caption(Mkhallatietal.,2023) oftimestampthateventismostlikelytohap-
withitsofficialimplementation. NetVLAD++(Gi- pen. I hope that you can choose a number
ancola and Ghanem, 2021) is adopted to aggre- of time stamp from the ranges of candidates.
gatethetemporalinformationoftheextractedfea- Butifreallynoneofthecandidatesissuitable,
tures. Thenthepooledfeaturesaredecodedbyan youcanjustreturnmewiththeoriginaltime
LSTM(HochreiterandSchmidhuber,1997). stamp. Youransweris:"
EventSummarization. Consideringthatthenar-
A.3 EvaluationMetrics
rationsbycommentatorsmaybefragmentedand
Inthispaper,mostevaluationmetrics(BLEU(Pap-
colloquial, we feed the ASR-generated narration
inenietal.,2002),METEOR(BanerjeeandLavie,
textsintotheLLaMA-3(AI@Meta,2024)model
2005),ROUGE-L(Lin,2004),CIDEr(Vedantam
andusethefollowingprompttosummarizethem
etal.,2015))arecalculatedusingthesamefunction
intoeventdescriptionsforevery10seconds:
settingswithSoccerNet-Caption(Mkhallatietal.,
"Iwillgiveyouanautomaticallyrecognized 2023), by the implementation of pycocoevalcap
speech with timestamps from a soccer game library. GPT-score (Fu et al., 2023) is given by
video. Thenarratorinthevideoiscomment- GPT-3.5withthefollowingtextasprompt:
ingonthesoccergame. Yourtaskistosumma-
"You are a grader of soccer game commen-
rizethekeyeventsforevery10seconds,each
taries. There is a predicted commentary by
commentaryshouldbeclearabouttheperson
AImodelaboutasoccergamevideoclipand
name and soccer terminology. Here is this
you need to score it comparing with ground
automaticallyrecognizedspeech: \n\n{times-
truth. \n \n You should rate an integer score
tampintervals: ASRsentences}\n\nYouneed
from 0 to 10 about the degree of similarity
tosummarize6sentencecommentariesfor0-
withgroundtruthcommentary(Thehigherthe
10s, 10-20s, 20-30s, 30-40s, 40-50s, 50-60s
score,themorecorrectthecandidateis). You
accordingtothetimestampsinautomatically
mustfirstconsidertheaccuracyofthesoccer
recognized speech results, every single sen-
events,thentoconsideraboutthesemanticin-
tencecommentaryshouldbeclearandconsise
formationinexpressionsandtheprofessional
about the incidents happened within that 10
soccer terminologies. The names of players
secondsforaround20-30words. Nowplease
and teams are masked by "[PLAYER]" and
writethese6commentaries.\nAnswer:"
"[TEAM]". \n \n The ground truth commen-
Timestamp Prediction. With the event descrip- tary of this soccer game video clip is: \n \n
tions and their corresponding timestamps, we in- "{Groundtruthhere.}"\n\nIneedyoutorate
putthemalongwiththetextualcommentariesinto
12the following predicted commentary from 0 A.6 MoreQualitativeResults
to10: \n\n"{PredictedCommentaryhere.}"
Inthispart,weprovidesomequalitativeresultsof
\n \n The score you give is (Just return one
our alignment model in Figure 6. Moreover, we
number,nootherwordorsentences):"
present more qualitative results of our proposed
MatchVoice model on soccer game commentary
A.4 DetailsofTemporalAlignment
generation,showninFigure7.
Forourproposedfine-grainedtemporalalignment
model,samplingappropriatepositiveandnegative
examplesforcontrastivelearningaffectstheresults.
Window(s) 60 120 150 180 240
avg(‚àÜ)(s) -0.54 0.03 0.44 2.34 -5.77
avg(|‚àÜ|)(s) 14.06 6.89 15.06 11.94 16.77
7:57 7:29
window (%) 97.71 98.17 91.28 91.28 85.78
10 ‚Äú[PLAYER] ([TEAM]) puts a cross into the box from the corner
window (%) 94.04 95.41 88.07 88.53 82.57
30 but there is no panic from the opposition and they easily clear.‚Äù
window (%) 81.65 91.28 84.40 83.94 81.65
45
window (%) 59.17 80.73 75.23 79.36 78.90
60
Table6: AlignmentResultsofDifferentWindows
As depicted in Table 6, we have experimented
with sampling windows of different lengths and
observedthatusinga120-secondwindowaround 85:21 84:51
‚Äú[COACH] decides to make a substitution. [PLAYER] will be
themanuallyannotatedgroundtruth(i.e.,60sec-
replaced by [PLAYER] ([TEAM]).‚Äù
ondsbeforeto60secondsafter)canyieldoptimal
alignmentperformance. Specifically,foreachtext
commentary,wetreatthekeyframecorresponding
toitsgroundtruthtimestampasthepositivesam-
ple,whileothersampleswithinafixedwindowsize,
sampledat1FPS,serveasnegativesamples(i.e.,
51:48 51:30
thosewithin5to60secondstemporaldistanceto
‚Äú[PLAYER] ([TEAM]) infringed the rules and goes into the book.
thegroundtruthtimestamp). [REFEREE] pulls out a yellow card.‚Äù
Considering that data pre-processing based on
ASR and LLM provides a coarse alignment and
thattheremightbereplaysinsoccergamevideos,
duringtheinferencestage,weusekeyframesfrom
45secondsbeforeto30secondsafterthecurrent
textualcommentarytimestampascandidates.
74:51 74:10
‚ÄúGoal! [PLAYER] puts a cross into the box and finds [PLAYER]
A.5 DivergenceAmongAnnotators
([TEAM]), who latches on to it and strikes the ball into the
Although the recruitedvolunteersare all football bottom left corner. He makes it 6:0.‚Äù
enthusiasts,thereexistsnoticeablesubjectivityand
variabilityinmanualannotationsduetodifferent
understandingsofsoccerterminologyandactions.
Toquantifythis,threevolunteersareaskedtoan-
notatetwomatchesfromourSN-Caption-test-align
benchmark. Weobservean‚Äúalignable/unalignable‚Äù 24:25 24:07
disagreementamongdifferentannotatorson6.29% "[PLAYER] ([TEAM]) is clearly asking for some medical
attention with his painful gestures. The extent of his injury is yet
of the samples. Additionally, the average of
to be discovered."
maximum discrepancy between the timestamps
provided by different annotators is 5.57 seconds, Figure6:MorequalitativeresultsonTemporalAlign-
whichcansomehowseenastheperformanceupper- ment. Timestamps before alignment are in Orange,
boundofautomaticalignmentmodels. thoseafteralignmentareinGreen.
13MatchVoice:[PLAYER] ([TEAM]) latches on to a precise low pass on the edge of the box and unleashes a shot that goes narrowly wide of theleft post.
GT:[PLAYER] ([TEAM]) strikes a shot towards goal from the edge of the penalty box, but it flies just wide of the left post.
MatchVoice:[PLAYER] ([TEAM]) latches on to a precise pass on the edge of the box and unleashes a low drive towards the middle of the goal. [PLAYER] pulls off a
comfortable save. The ball goes out of play and [TEAM] will have a goal kick.
GT:[PLAYER] ([TEAM]) shoots from the edge of the penalty area. The ball travels towards the bottom right corner, but [PLAYER] easily deals with the threat.
MatchVoice:[PLAYER] ([TEAM]) tries to find [PLAYER], but he puts too much weight on his pass.
GT:[PLAYER] ([TEAM]) crosses the ball forward but it is intercepted
MatchVoice:[PLAYER] ([TEAM]) takes the corner kick and sends the ball into the box, but one of the defenders reacts well to avert the danger.
GT:[PLAYER] ([TEAM]) whips the ball in from the long-range free kick, but the first man gets it clear.
MatchVoice:[PLAYER] ([TEAM]) will take the responsibility and he is already preparing the ball.
GT:[PLAYER] ([TEAM]) confidently powers his spot-kick into the left side of the goal.
MatchVoice:The referee stopsplay so that a substitution can be made and [PLAYER] ([TEAM]) comes onto the pitch for [PLAYER].
GT: [COACH] has decided to make a change. [PLAYER] ([TEAM]) replaces [PLAYER].
MatchVoice:[PLAYER] ([TEAM]) is booked after bringing down an opponent. [REFEREE] made the right call.
GT:[PLAYER] ([TEAM]) picks up a yellow card for a foul. [TEAM] win a free kick. It's a promising situation for a direct shot.
MatchVoice:[PLAYER] ([TEAM]) goes over to take the corner kick and it looks like he will send the ball into the penalty box.
GT:[PLAYER] ([TEAM]) will try to find the head of one of his teammates from a corner kick.
Figure7: Morequalitativeresultsoncommentarygeneration.
14