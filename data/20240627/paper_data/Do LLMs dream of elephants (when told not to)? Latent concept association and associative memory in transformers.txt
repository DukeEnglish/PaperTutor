Do LLMs dream of elephants (when told not to)?
Latent concept association and associative memory
in transformers
Yibo Jiang1, Goutham Rajendran2,
Pradeep Ravikumar2, and Bryon Aragam3
1
Department of Computer Science, University of Chicago
2
Machine Learning Department, Carnegie Mellon University
3
Booth School of Business, University of Chicago
Abstract
LargeLanguageModels(LLMs)havethecapacitytostoreandrecallfacts. Through
experimentation with open-source models, we observe that this ability to retrieve
facts can be easily manipulated by changing contexts, even without altering their
factualmeanings.ThesefindingshighlightthatLLMsmightbehavelikeanassociative
memorymodelwherecertaintokensinthecontextsserveascluestoretrievingfacts.We
mathematicallyexplorethispropertybystudyinghowtransformers,thebuildingblocks
ofLLMs,cancompletesuchmemorytasks.Westudyasimplelatentconceptassociation
problemwithaone-layertransformerandweshowtheoreticallyandempiricallythat
thetransformergathersinformationusingself-attentionandusesthevaluematrixfor
associativememory.
1 Introduction
Whatisthefirstthingthatwouldcometomindifyouwereaskednottothinkofanele-
phant? Chancesare,youwouldbethinkingaboutelephants. Whatifweaskthesamething
toLargeLanguageModels(LLMs)? Obviously,onewouldexpecttheoutputsofLLMstobe
heavilyinfluencedbytokensinthecontext[Bro+20]. Couldsuchinfluencepotentiallyprime
LLMsintochangingoutputsinanontrivialway? Togainadeeperunderstanding,wefocus
ononespecifictaskcalledfactretrieval[Men+22;Men+23]whereexpectedoutputanswers
aregiven. LLMs,whicharetrainedonvastamountsofdata,areknowntohavethecapabil-
itytostoreandrecallfacts[Men+22;Men+23;DCAT21;Mit+21;Mit+22;Dai+21]. Thisabil-
ityraisesnaturalquestions: Howrobustisfactretrieval,andtowhatextentdoesitdepend
onsemanticmeaningswithincontexts? WhatdoesitrevealaboutmemoryinLLMs?
Inthispaper,wefirstdemonstratethatfactretrievalisnotrobustandLLMscanbeeasily
fooledbyvaryingcontexts. Forexample,whenaskedtocomplete“TheEiffelTowerisin
thecityof”,GPT-2[Rad+19]answerswith“Paris”. However,whenpromptedwith“The
EiffelTowerisnotinChicago. TheEiffelTowerisinthecityof”,GPT-2respondswith
“Chicago”. SeeFigure1formoreexamples,includingGemmaandLLaMA.Ontheother
hand,humansdonotfindthetwosentencesfactuallyconfusingandwouldanswer“Paris”
in both cases. We call this phenomenon context hijacking. Importantly, these findings
suggestthatLLMsmightbehavelikeanassociativememorymodel. Inwhich,tokensin
contextsguidetheretrievalofmemories,evenifsuchassociationsformedarenotinherently
semanticallymeaningful.
Thisassociativememoryperspectiveraisesfurtherinterpretabilityquestionsabouthow
1
4202
nuJ
62
]LC.sc[
1v00481.6042:viXraFigure1: ExamplesofcontexthijackingforvariousLLMs,showcasingthatfactretrievalisnot
robust.
LLMsformsuchassociations. Answeringthesequestionscanfacilitatethedevelopment
ofmorerobustLLMs. Unlikeclassicalmodelsofassociativememoryinwhichdistance
betweenmemorypatternsaremeasureddirectlyandtheassociationsbetweeninputsand
outputsarewell-specified,factretrievalreliesonamorenuancednotionofsimilaritymea-
suredbylatent(unobserved)semanticconcepts. Tomodelthis,weproposeasynthetictask
calledlatentconceptassociationwheretheoutputtokeniscloselyrelatedtosampledtokens
inthecontextbutwhereinsimilarityismeasuredviaalatentspaceofsemanticconcepts.
Wetheninvestigatehowaone-layertransformer[Vas+17],afundamentalcomponentof
LLMs,cantacklethismemoryretrievaltaskinwhichvariouscontextdistributionscorre-
spondtodistinctmemorypatterns. Wedemonstratethatthetransformeraccomplishesthe
taskintwostages:Theself-attentionlayergathersinformation,whilethevaluematrixfunc-
tionsasassociativememory. Moreover,low-rankstructurealsoemergesintheembedding
spaceoftrainedtransformers. Thesefindingsprovideadditionaltheoreticalvalidationfor
numerousexistinglow-rankeditingandfine-tuningtechniques[Men+22;Hu+21].
Contributions Specifically,wemakethefollowingcontributions:
1. WesystematicallydemonstratecontexthijackingforvariousopensourceLLMmodels
includingGPT-2[Rad+19],LLaMA-2[Tou+23]andGemma[Tea+24],whichshow
thatfactretrievalcanbemisledbycontexts(Section3),reaffirmingthatLLMslack
robustnesstocontextchanges[Shi+23;Pet+20;CSH22;Yor+23;PE21].
2. Weproposeasyntheticmemoryretrievaltasktermedlatentconceptassociation,
allowingustoanalyzehowtransformerscanaccomplishmemoryrecall(Section4).
Unlikeclassicalmodelsofassociativememory, ourtaskcreatesassociationsina
latent,semanticconceptspaceasopposedtodirectlybetweenobservedtokens. This
perspectiveiscrucialtounderstandinghowtransformerscansolvefactretrieval
problemsbyimplementingassociativememorybasedonsimilarityinthelatentspace.
3. Wetheoretically(Section5)andempirically(Section6)studytrainedtransformerson
thislatentconceptassociationproblem,showingthatself-attentionisusedtoaggre-
gateinformationwhilethevaluematrixservesasassociativememory. Andmoreover,
wediscoverthattheembeddingspacecanexhibitalow-rankstructure,offeringad-
ditionalsupportforexistingeditingandfine-tuningmethods[Men+22;Hu+21].
22 Literature review
Associativememory Associativememoryhasbeenexploredwithinthefieldofneu-
roscience[Hop82; Seu96; BYBOS95; Ska+94; SS22]. Themostpopularmodelsamong
themistheHopfieldnetwork[Hop82]anditsmodernsuccessors[Ram+20;Mil+22;Zha23]
arecloselyrelatedtotheattentionlayerusedintransformers[Vas+17]. Inaddition,the
attentionmechanismhasalsobeenshowntoapproximateanotherassociativememory
modelknownassparsedistributedmemory[BP21]. Beyondattention,Radhakrishnan
etal.[RBU20]andJiangandPehlevan[JP20]showthatoverparameterzedautoencoders
canimplementassociativememoryaswell. Thispaperstudiesfactretrievalasaform
ofassociativememory. Anothercloselyrelatedareaofresearchfocusesonmemorization
indeepneuralnetworks. Henighanetal.[Hen+23]showsthatasimpleneuralnetwork
trainedontoymodelwillstoredatapointsintheoverfittingregimewhilestoringfeatures
intheunderfittingregime. Feldman[Fel20]andFeldmanandZhang[FZ20]studythe
interplaybetweenmemorizationandlongtaildistributionswhileKimetal.[KKM22]and
Mahdavietal.[MLT23]studythememorizationcapacityoftransformers.
InterpretingtransformersandLLMs There’sagrowingbodyofworkonunderstand-
inghowtransformersandLLMswork[LLR23;AZL23a;AZL23b;AZL24;EI+24;Tar+23b;
Tar+23a;Li+24],includingtrainingdynamics[Tia+23a;Tia+23b;She+24]andin-context
learning [Xie+21; Gar+22; Bai+24; Bai+24]. Recent papers have introduced synthetic
tasks to better understand the mechanisms of transformers [Cha22; Liu+22; Nan+23;
Zha+22; Zho+24], such as those focused on Markov chains [Bie+24; Ede+24; NDL24;
Mak+24]. Most notably, Bietti et al. [Bie+24] and subsequent works [CDB23; CSB24]
studyweightsintransformersasassociativememorybuttheirfocusisonunderstanding
inductionhead[Ols+22b]andone-to-onemapbetweeninputqueryandoutputmemory. An
increasingamountofresearchisdedicatedtounderstandingtheinternalsofpre-trained
LLMs,broadlycategorizedundertheterm“mechanisticinterpretability”[Elh+21;Ols+22a;
Gev+23;Men+22;Men+23;Jia+24;Raj+24;Has+24;Wan+22;McG+23;Gei+21;Gei+22;
Gei+24;Wu+24].
KnowledgeeditingandadversarialattacksonLLMs Factrecallandknowledge
editing have been extensively studied [Men+22; Men+23; Has+24; Sak+23; DCAT21;
Mit+21;Mit+22;Dai+21;Zha+23;Tia+24;Jin+23],includingtheuseofin-contextlearning
to edit facts [Zhe+23]. This paper aims to explore a different aspect by examining the
robustness offact recallto variation inprompts. Acloselyrelated lineof workfocuses
onadversarialattacksonLLMs[seeCho+24, forareview]. Specifically, prompt-based
adversarial attacks [Xu+23; Zhu+23; Wan+23b] focus on the manipulation of answers
withinspecificclassificationtaskswhileotherworksconcentrateonsafetyissues[Liu+23a;
PR22; Zou+23; Apr+22; Wan+23a; Si+22; Rao+23; SMR23; Liu+23b]. There are also
worksshowingLLMscanbedistractedbyirrelevantcontextsinproblemsolving[Shi+23],
questionanswering[Pet+20; CSH22; Yor+23]andfactualreasoning[PE21]. Although
phenomenaakintocontexthijackinghavebeenreportedindifferentinstances,thegoals
ofthispaperaretogiveasystematicrobustnessstudyforfactretrieval,offeraframework
forinterpretingitinthecontextofassociativememory,anddeepenourunderstandingof
LLMs.
3 Context hijacking in LLMs
Inthissection,werunexperimentsonLLMsincludingGPT-2[Rad+19],Gemma[Tea+24]
(bothbaseandinstructmodels)andLLaMA-2-7B[Tou+23]toexploretheeffectsofcontext
hijackingonmanipulatingLLMoutputs. Asanexample, considerFigure1. Whenwe
prompttheLLMswiththecontext“TheEiffelTowerisinthecityof”,all4LLMsoutputthe
3(a)Hijackinggenerically (b)HijackingbasedonRelationIDP190
Figure2:ContexthijackingcancauseLLMstooutputfalsetarget.Thefigureshowsefficacyscore
versusthenumberofprependsforvariousLLMsontheCounterFactdatasetundertwohijacking
schemes.
correctanswer(“Paris”). However,asweseeintheexample,wecanactuallymanipulate
theoutputoftheLLMssimplybymodifyingthecontextwithadditionalfactualinforma-
tionthatwouldnotconfuseahuman. Wecallthiscontext-hijacking. Duetothedifferent
capacitiesandcapabiltiesofeachmodel,theexamplesinFigure1usedifferenthijacking
techniques. ThisismostnotableonLLaMA-2-7B,whichisamuchlargermodelthanthe
others. Ofcourse,asexpected,themoresophisticatedattackonLLaMAalsoworkson
GPT-2andGemma. Additionally,theinstruction-tunedversionofGemmacanunderstand
specialwordslike“not”tosomeextent. Nevertheless,itisstillpossibletosystematically
hijacktheseLLMs,asdemonstratedbelow.
WeexplorethisphenomenonatscalewiththeCounterFactdatasetintroducedin[Men+22],
adatasetofdifficultcounterfactualassertionscontainingadiversesetofsubjects,rela-
tions, and linguistic variations. CounterFact has 21,919 samples, each of which are
given by a tuple (p,o∗,o_,s,r). From each sample, we have a context prompt p with a
truetargetanswer o∗ (target_true)andafalsetargetanswer o_ (target_false),e.g. the
prompt p=“EiffelTowercanbefoundin” has true target o∗=“Paris” and false target
o_=“Guam”. Additionally,themainentityin pisthesubjects(s=“EiffelTower”)andthe
promptiscategorizedintorelationsr(forinstance,othersampleswiththesamerelation
IDastheexampleabovecouldbeoftheform“Thelocationof{subject}is”,“{subject}can
befoundin”,“Whereis{subject}? Itisin”). Foradditionaldetailsonhowthedatasetwas
collected,see[Men+22].
Forahijackingscheme,wereporttheEfficacyScore(ES)[Men+22],whichistheproportion
ofsamplesforwhichthetokenprobabilitiessatisfyPr[o_]>Pr[o∗]aftermodifyingthe
context,thatis,theproportionofthedatasetthathasbeensuccessfullymanipulated. We
experimentwithtwohijackingschemesforthisdataset. Wefirsthijackbyprependingthe
text“Donotthinkof{target_false}”toeachcontext. Forinstance,theprompt“TheEiffel
Towerisin”getschangedto“DonotthinkofGuam. TheEiffelTowerisin”. InFigure2a,
weseethattheefficacyscoredropssignificantlyafterhijacking. Here,weprependthe
hijackingsentencektimesfork=0,...,5wherek=0yieldstheoriginalprompt. Wesee
thatadditionalprependsdecreasethescorefurther.
In the second scheme, we make use of the relation ID r to prepend factually correct
sentences. For instance, one can hijack the example above to “The Eiffel Tower is not
locatedinGuam. TheEiffelTowerisin”. Wetestthishijackingphilosophyondifferent
relationIDs. Inparticular,Figure2breportshijackingbasedonrelationIDP190(“twin
city”). Andweseesimilarpatternsthatwithmoreprepends,theESscoregetslower. Itis
alsoworthnotingthatonecanevenhijackbyonlyincludingwordsthataresemantically
closetothefalsetarget(e.g.,“France”forfalsetarget“French”). Thissuggeststhatcontext
hijackingismorethansimplytheLLMcopyingtokensfromcontexts. Additionaldetailsand
4experimentsforbothhijackingschemesandforotherrelationIDsareinAppendixB.
TheseexperimentsshowthatcontexthijackingchangesthebehaviorofLLMs,leadingthem
tooutputincorrecttokens,withoutalteringthefactualmeaningofthecontext. Itisworth
notingthatsimilarfragilebehaviorsofLLMshavebeenobservedintheliteratureindiffer-
entcontexts[Shi+23;Pet+20;CSH22;Yor+23;PE21]. SeeSection2formoredetails.
ContexthijackingindicatesthatfactretrievalinLLMsisnotrobustandthataccurate
fact recall does not necessarily depend on the semantics of the context. As a result,
one hypothesis is to view LLMs as an associative memory model where special tokens
in contexts, associated with the fact, provide partial information or clues to facilitate
memoryretrieval[Zha23]. Tobetterunderstandthisperspective,wedesignasynthetic
memoryretrievaltasktoevaluatehowthebuildingblocksofLLMs,transformers,can
solveit.
4 Problem setup
InthecontextofLLMs,factormemoryretrieval,canbemodeledasanexttokenprediction
problem. Givenacontext(e.g.,“ThecapitalofFranceis”),theobjectiveistoaccurately
predictthenexttoken(e.g.,“Paris”)basedonthefactualrelationbetweencontextandthe
followingtoken.
Previouspapers[Ram+20;Mil+22;BP21;Zha23]havestudiedtheconnectionbetween
attentionandautoassociativeandheteroassociativememory. Forautoassociativememory,
contexts are modeled as a set of existing memories and the goal of self-attention is to
selecttheclosestoneorapproximationstoit. Ontopofthis,heteroassociativememory
[Mil+22; BP21] has an additional projection to remap each output to a different one,
whetherwithinthesamespaceorotherwise. Inbothscenarios,thegoalistolocatethe
closestpatternwithinthecontextwhenprovidedwithaquery(uptoaremappingifit’s
heteroassociative).
Factretrieval,ontheotherhand,doesnotstrictlyfollowthisframework. Thecruxofthe
issueisthattheoutputtokenisnotnecessarilyclosetoanyparticulartokeninthecontext
butratheracombinationofthemandthe“closeness”isintuitivelymeasuredbylatent
semanticconcepts. Forexample,considercontextsentence“ThecapitalofFranceis”with
theoutput“Paris”. Here,noneofthetokensinthecontextdirectlycorrespondstotheword
“Paris”. Yetsometokenscontainpartialinformationabout“Paris”. Intuitively,“capital”
alignswiththe“isCapital”conceptof“Paris”,while“France”correspondstothe“isFrench”
conceptlinkedto“Paris”wherealltheconceptsarelatent. Tomodelsuchphenomenon,we
proposeasynthetictaskcalledlatentconceptassociationwheretheoutputtokenisclosely
relatedtotokensinthecontextandsimilarityismeasuredviathelatentspace.
4.1 Latent concept association
Weproposeasyntheticpredictiontaskwhereforeachoutputtoken y,tokensinthecontext
(denotedbyx)aresampledfromaconditionaldistributiongiven y. Tokensthataresimilar
to ywillbefavoredtoappearmoreinthecontext,exceptfor yitself. Thetaskoflatent
conceptassociationistosuccessfullyretrievethetoken ygivensamplesfrom p(x|y). The
syntheticsetupsimplifiesbynotaccountingforthesequentialnatureoflanguage,achoice
supportedbypreviousexperimentsoncontexthijacking(Section3). Weformalizethistask
below.
To measure similarity, we define a latent space. Here, the latent space is a collection
of m binary latent variables Z . These could be viewed as semantic concept variables.
i
Let Z=(Z ,...,Z ) be the corresponding random vector, z be its realization, and Z be
1 m
thecollectionofalllatentbinaryvectors. Foreachlatentvector z,there’soneassociated
5tokent∈[V]={0,...,V−1}whereV isthetotalnumberoftokens. Herewerepresentthe
tokenizerasιwhereι(z)=t. Inthispaper, weassumethatιisthestandardtokenizer
whereeachbinaryvectorismappedtoitsdecimalnumber. Inotherwords,there’saoneto
onemapbetweenlatentvectorsandtokens. Becausethemapisonetoone,wesometimes
uselatentvectorsandtokensinterchangeably. Wealsoassumethateverylatentbinary
vectorhasauniquecorrespondingtoken,thereforeV=2m.
Underthelatentconceptassociationmodel,thegoalistoretrievespecificoutputtokens
givenpartialinformationinthecontexts. Thisismodeledbythelatentconditionaldistri-
bution:
p(z|z∗ )=ωπ(z|z∗ )+(1−ω)Unif(Z)
where
(cid:40) exp(−D (z,z∗ )/β) z∈N(z∗ ),
π(z|z∗ )∝ H
0
z∉N(z∗
).
Here D is the Hamming distance, N(z∗ ) is a subset of Z \{z∗ } and β>0 is the tem-
H
peratureparameter. TheuseofHammingdistancedrawsaparallelwiththenotionof
distributionalsemanticsinnaturallanguage: “awordischaracterizedbythecompany
itkeeps”[Fir57]. Inwords, p(z|z∗ )saysthatwithprobability1−ω,theconditionaldis-
tribution uniformly generate random latent vectors and with probability ω, the latent
vectorisgeneratedfromtheinformativeconditionaldistributionπ(z|z∗ )wherethesupport
oftheconditionaldistributionisN(z∗ ). Here,πrepresentstheinformativeconditional
distributionthatdependson z∗ whereastheuniformdistributionisuninformativeand
canbeconsideredasnoise. Themixturemodelparameterωdeterminesthesignaltonoise
ratioofthecontexts.
Therefore,foranylatentvector z∗ anditsassociatedtoken,onecangenerateLcontext
tokenwordswiththeaforementionedlatentconditionaldistribution:
• Uniformlysamplealatentvector z∗
• Forl=1,...,L−1,sample z ∼p(z|z∗ )andt =ι(z ).
l l l
• Forl=L,sample z∼π(z|z∗ )andt =ι(z).
L
Consequently,wehavex=(t ,..,t )andy=ι(z∗ ). Thelasttokeninthecontextisgenerated
1 L
specificallytomakesurethatitisnotfromtheuniformdistribution. Thisensuresthatthe
lasttokencanuseattentiontolookforclues,relevanttotheoutput,inthecontext. LetDL
bethesamplingdistributiontogenerate(x,y)pairs. Theconditionalprobabilityof ygiven
xisgivenby p(y|x). Withslightabuseofnotation,givenatokent∈[V],wedefineN(t)=
N(ι−1(t)). wealsodefineD (t,t′ )=D (ι−1(t),ι−1(t′ ))foranypairoftokenstandt′.
H H
Foranyfunction f thatmapsthecontexttoestimatedlogitsofoutputlabels,thetraining
objectiveistominimizethislossofthelastposition:
E [ℓ(f(x),y)]
(x,y)∈DL
whereℓisthecrossentropylosswithsoftmax. Theerrorrateoflatentconceptassociation
isdefinedbythefollowing:
R (f)=P [argmaxf(x)̸=y]
DL (x,y)∼DL
Andtheaccuracyis1−R (f).
DL
4.2 Transformer network architecture
Givenacontextx=(t ,..,t )whichconsistsofLtokens,wedefine X∈{0,1}V×L tobeits
1 L
one-hotencodingwhereV isthevocabularysize. Hereweuseχtorepresenttheone-hot
6encodingfunction(i.e.,χ(x)=X). Similarto[LLR23;Tar+23a;Li+24],wealsoconsider
a simplified one-layer transformer model without residual connections and normaliza-
tion:
(cid:104) (cid:105)
fL(x)= W TW attn(W χ(x)) (4.1)
E V E
:L
where
attn(U)=Uσ(cid:179)(W KU)T(W QU)(cid:180)
,
(cid:112)
d
a
W K∈Rda×d isthekeymatrix,andW Q∈Rda×d isthequerymatrixandd
a
istheattention
headsize. σ:RL×L→(0,1)L×L isthecolumn-wisesoftmaxoperation. W ∈Rd×d isthe
V
valuematrixandW ∈Rd×V istheembeddingmatrix. Here,weadopttheweighttie-in
E
implementationwhichisusedforGemma[Tea+24]. Wefocussolelyonthepredictionof
thelastposition,asitistheonlyonerelevantforlatentconceptassociation. Forconve-
nience,wealsouseh(x)tomean(cid:163)attn(W χ(x))(cid:164) ,whichisthehiddenrepresentationafter
E :L
attentionforthelastposition,and fL(x)torepresentthelogitforoutputtokent.
t
5 Theoretical analysis
Inthissection,wetheoreticallyinvestigatehowasingle-layertransformercansolvethe
latentconceptassociationproblem. Wefirstintroduceahypotheticalassociativememory
modelthatutilizesself-attentionforinformationaggregationandemploysthevaluematrix
formemoryretrieval. Thishypotheticalmodelturnsouttomirrortrainedtransformers
inexperiments. Wealsoexaminetheroleofeachindividualcomponentofthenetwork:
thevaluematrix,embeddings,andtheattentionmechanism. Wevalidateourtheoretical
claimsinSection6.
5.1 Hypothetical associative memory model
Inthissection,weshowthatasimplesingle-layertransformernetworkcansolvethelatent
conceptassociationproblem. TheformalresultispresentedbelowinTheorem1;firstwe
requireafewmoredefinitions. LetW (t)bethet-thcolumnoftheembeddingmatrixW .
E E
Inotherwords,thisistheembeddingfortokent. Givenatokent,defineN (t)tobethe
1
subsetoftokenswhoselatentvectorsareonly1Hammingdistanceawayfromt’slatent
vector: N (t)={t′ :D (t′ ,t))=1}∩N(t). Foranyoutputtoken t,N (t)containstokens
1 H 1
withthehighestprobabilitiestoappearinthecontext.
The following theorem formalizes the intuition that a one-layer transformer that uses
self-attentiontosummarizestatisticsaboutthecontextdistributionsandwhosevalue
matrix uses aggregated representations to retrieve output tokens can solve the latent
conceptassociationproblemdefinedinSection4.1.
Theorem 1 (informal). Suppose the data generating process follows Section 4.1 where
m≥3,ω=1,andN(t)=V\{t}. Thenforanyε>0,thereexistsatransformermodelgiven
by(4.1)thatachieveserrorε,i.e. R DL(fL)<εgivensufficientlylargecontextlengthL.
Moreprecisely,forthetransformerinTheorem1,wewillhaveW =0andW =0. Each
K Q
rowofW isorthogonaltoeachotherandnormalized. AndW isgivenby
E V
W = (cid:88) W (t)( (cid:88) W (t′ )T) (5.1)
V E E
t∈[V] t′∈N 1(t)
A more formal statement of the theorem and its proof is given in Appendix A (Theo-
rem7).
7Intuitively,Theorem1suggestshavingmoresamplesfrom p(x|y)canleadtoabetterrecall
rate. On the other hand, if contexts are modified to contain more samples from p(x|y˜)
where y˜̸=y,thenitislikelyfortransformertooutputthewrongtoken. Thisissimilarto
contexthijacking(seeSection5.5). Theconstructionofthevaluematrixissimilartothe
associativememorymodelusedin[Bie+24;CSB24],butinourcase,thereisnoexplicit
one-to-oneinputandoutputpairsstoredasmemories. Rather,acombinationofinputsare
mappedtoasingleoutput.
WhiletheconstructioninTheorem1isjustonewaythatasingle-layertransformercan
tacklethistask,itturnsoutempiricallythisconstructionofW isclosetothetrainedW ,
V V
eveninthenoisycase(ω̸=1). InSection6.1,wewilldemonstratethatsubstitutingtrained
valuematriceswithconstructedonescanretainaccuracy,andtheconstructedandtrained
valuematricesevensharecloselow-rankapproximations. Moreover,inthishypothetical
model,asimpleuniformattentionmechanismisdeployedtoallowself-attentiontocountoc-
currencesofeachindividualtokens. Sincetheembeddingsareorthonormalvectors,thereis
nointerference. Hence,theself-attentionlayercanbeviewedasaggregatinginformationof
contexts. Itisworthnotingthat,indifferentsettings,moresophisticatedembeddingstruc-
turesandattentionpatternsareneeded. Thisisdiscussedinthefollowingsections.
5.2 On the role of the value matrix
TheconstructioninTheorem1reliesonthevaluematrixactingasassociativememory.
But is it necessary? Could we integrate the functionality of the value matrix into the
self-attentionmoduletosolvethelatentconceptassociationproblem? Empirically,the
answer seems to be negative as will be shown in Section 6.1. In particular, when the
contextlengthissmall,settingthevaluematrixtobetheidentitywouldleadtosubpar
memoryrecallaccuracy.
Thisisbecauseifthevaluematrixistheidentity,thetransformerwouldbemoresusceptible
tothenoiseinthecontext. Toseethis,noticethatgivenanypairofcontextandoutput
token(x,y),thelatentrepresentationafterself-attentionh(x)mustliveinthepolyhedron
S tobeclassifiedcorrectlywhereS isdefinedas:
y y
S ={v:(W (y)−W (t))Tv>0wheret̸∈[V]\{y}}
y E E
Note that, by definition, for any two tokens y and y˜, S ∩S =(cid:59). On the other hand,
y y˜
becauseoftheself-attentionmechanism,h(x)mustalsoliveintheconvexhullofallthe
embeddingvectors:
CV=Conv(WE(0),...,WE(|V|−1))
Inotherwords,foranypair(x,y)tobeclassifiedcorrectly,h(x)mustliveintheintersection
ofS andCV. Duetothestochasticnatureof x,itislikelyforh(x)tobeoutsideofthis
y
intersection. Theremappingeffectofthevaluematrixcanhelpwiththisproblem. The
followinglemmaexplainsthisintuition.
Lemma2. SupposethedatageneratingprocessfollowsSection4.1wherem≥3,ω=1and
N(t)={t′ :D (t,t′ ))=1}. Foranysinglelayertransformergivenby(4.1)whereeachrowof
H
W isorthogonaltoeachotherandnormalized,ifW isconstructedasin(5.1),thenthe
E V
errorrateis0. IfW istheidentitymatrix,thentheerrorrateisstrictlylargerthan0.
V
Anotherintriguingphenomenonoccurswhenthevaluematrixistheidentitymatrix. Inthis
case,theinnerproductbetweenembeddingsandtheircorrespondingHammingdistance
varieslinearly. Thisrelationshipcanbeformalizedbythefollowingtheorem.
Theorem3. SupposethedatageneratingprocessfollowsSection4.1wherem≥3,ω=1
andN(t)=V\{t}. Foranysinglelayertransformergivenby(4.1)withW beingtheidentity
V
8matrix,ifthecrossentropylossisminimizedsothatforanysampledpair(x,y),
p(y|x)=pˆ(y|x)=softmax(fL(x))
y
thereexistsa>0andbsuchthatfortwotokenst̸=t′
,
〈W (t),W (t′ )〉=−aD (t,t′ )+b
E E H
5.3 Embedding training and geometry
ThehypotheticalmodelinSection5.1requiresembeddingstoformanorthonormalbasis.
In the overparameterization regime where the embedding dimension d is larger than
thenumberoftokensV,thiscanbeapproximatelyachievedbyGaussianinitialization.
However,inpractice,theembeddingdimensionistypicallysmallerthanthevocabularysize,
inwhichcaseitisimpossiblefortheembeddingstoconstitutesuchabasis. Empirically,in
Section6.2,weobservethatwithoverparameterization(d>V),embeddingscanbefrozen
attheirGaussianinitialization,whereasintheunderparameterizedregime,embedding
trainingisrequiredtoachievebetterrecallaccuracy.
Thisraisesthequestion: Whatkindofembeddinggeometryislearnedintheunderparam-
eterizedregime? Experimentsrevealacloserelationshipbetweentheinnerproductof
embeddingsfortwotokensandtheHammingdistanceofthesetokens(seeFigure3band
FigureC.5inAppendixC.2). Approximately,wehavethefollowingrelationship:
(cid:40)
b
t=t′
〈W (t),W (t′ )〉= 0 (5.2)
E E −aD (t,t′ )+b t̸=t′
H
foranytwotokens tand t′ where b >banda>0. Onecanviewthisasacombination
0
oftheembeddinggeometryunderGaussianinitializationandthegeometrywhenW is
V
theidentitymatrix(Theorem3). Importantly,thisstructuredemonstratesthattrained
embeddings inherently capture similarity within the latent space. Theoretically, this
embeddingstructure(5.2)canalsoleadtolowerrorrateunderspecificconditionsonb ,b
0
anda,whichisarticulatedbythefollowingtheorem.
Theorem4(Informal). FollowingthesamesetupasinTheorem1,butembeddingsobey
(5.2),thenundercertainconditionsona,bandifb andcontextlengthLaresufficiently
0
large,theerrorratecanbearbitrarilysmall,i.e. R DL(fL)<εforany0<ε<1.
TheformalstatementofthetheoremanditsproofisgiveninAppendixA(Theorem8).
Notably,thisembeddinggeometryalsoimpliesalow-rankstructure. Let’sfirstconsider
thespecialcasewhenb =b. Inotherwords,theinnerproductbetweenembeddingsand
0
theircorrespondingHammingdistancevarieslinearly.
Lemma5. Ifembeddingsfollow(5.2)andb=b andN(t)=V\{t},thenrank(W )≤m+2.
0 E
Whenb >b,theembeddingmatrixwillnotbestrictlylowrank. However,itcanstillexhibit
0
approximatelow-rankbehavior,characterizedbyaneigengapbetweenthetopandbottom
singularvalues. Thisisverifiedempirically(seeFigureC.9-C.12inAppendixC.4).
5.4 The role of attention selection
Asofnow,attentiondoesnotplayasignificantroleintheanalysis. Butperhapsunsurpris-
ingly,theattentionmechanismisusefulinselectingrelevantinformation. Toseethis,let’s
consideraspecificsettingwhereforanylatentvectorz∗,N(z∗ )={z:z∗=z }\{z∗ }.
1 1
Essentially,latentvectorsarepartitionedintotwoclustersbasedonthevalueofthefirst
latentvariable,andtheinformativeconditionaldistributionπonlysampleslatentvectors
9thatareinthesameclusterastheoutputlatentvector. Empirically,whentrainedunder
thissetting,theattentionmechanismwillpaymoreattentiontotokenswithinthesame
cluster (Section 6.3). This implies that the self-attention layer can mitigate noise and
concentrateontheinformativeconditionaldistributionπ.
Tounderstandthismoreintuitively,wewillstudythegradientofunnormalizedattention
scores. Inparticular,theunnormalizedattentionscoreisdefinedas:
u t,t′=(W KW E(t))T(W QW E(t′ ))/(cid:112) d a.
Lemma6.
SupposethedatageneratingprocessfollowsSection4.1andN(z∗ )={z:z∗=
1
∗
z }\{z }. Giventhelasttokeninthesequencet ,then
1 L
L
∇ ℓ(fL)=∇ℓ(fL)T(W )TWV(α pˆ W (t)−pˆ (cid:88) pˆ W (t ))
ut,tL E t t E t
l=1
tl E l
wherefortokent,α =(cid:80)L 1[t =t]and pˆ isthenormalizedattentionscorefortokent.
t l=1 l t
Typically,α islargerwhentokentandt belongtothesameclusterbecausetokenswithin
t L
thesameclustertendtoco-occurfrequently. Asaresult,thegradientcontributiontothe
unnormalizedattentionscoreisusuallylargerfortokenswithinthesamecluster.
5.5 Context hijacking and the misclassification of memory re-
call
Inlightofthetheoreticalresultsonlatentconceptassociation,anaturalquestionarises:
HowdotheseresultsconnecttocontexthijackinginLLMs? Inessence,forthelatentconcept
association problem, the differentiation of output tokens is achieved by distinguishing
betweenthevariousconditionaldistributions p(x|y). Thus,addingorchangingtokens
in the context x so that it resembles a different conditional distribution can result in
misclassification. InAppendixC.5,wepresentexperimentsshowingthatmixingdifferent
contextscancausetransformerstomisclassify. Thispartiallyexplainscontexthijacking
inLLMs(Section3). Ontheotherhand,itiswell-knownthattheerrorrateisrelated
totheKLdivergencebetweenconditionaldistributionsofcontexts[Cov99]. Thecloser
thedistributionsare,theeasieritisforthemodeltomisclassify. Here,longercontexts,
primarilycomposedofi.i.dsamples,suggestlargerdivergences,thushighermemoryrecall
rate. ThisistheoreticallyimpliedbyTheorem1andTheorem4andempiricallyverifiedin
AppendixC.6. Suchresultisalsorelatedtoreversecontexthijacking(AppendixB)where
prependingsentencesincludingtruetargetwordscanimprovefactrecallrate.
6 Experiments
Themainimplicationsofthetheoreticalresultsintheprevioussectionare:
1. Thevaluematrixisimportantandhasassociativememorystructureasin(5.1).
2. Trainingembeddingsiscrucialintheunderparameterizedregime,whereembeddings
exhibitcertaingeometricstructures.
3. Attentionmechanismisusedtoselectthemostrelevanttokens.
Toevaluatetheseclaims,weconductseveralexperimentsonsyntheticdatasets. Additional
experimentaldetailsandresultscanbefoundinAppendixC.
10(a)Valuematrixtraining (b)Embeddingstructure (c)AttentionPattern
Figure3: Keycomponentsofthesingle-layertransformerworkingtogetheronthelatentconcept
associationproblem.(a)FixingthevaluematrixWV astheidentitymatrixresultsinloweraccuracy
comparedtotrainingWV.ThefigurereportsaverageaccuracyforbothfixedandtrainedWV with
L=64.(b)Whentrainingintheunderparameterizedregime,theembeddingstructureisapproximated
by(5.2).Thegraphdisplaystheaverageinnerproductbetweenembeddingsoftwotokensagainstthe
correspondingHammingdistancebetweenthesetokenswhenm=8.(c)Theself-attentionlayercan
selecttokenswithinthesamecluster.Thefigureshowsaverageattentionscoreheatmapwithm=8
andtheclusterstructurefromSection5.4.
6.1 On the value matrix W
V
Inthissection,westudythenecessityofthevaluematrixW anditsstructure. First,we
V
conductexperimentstocomparetheeffectsoftrainingversusfreezingW astheidentity
V
matrix,withthecontextlengthsLsetto64and128. Figure3aandFigureC.1showthat
whenthecontextlengthissmall,freezingW canleadtoasignificantdeclineinaccuracy.
V
ThisisinlinewithLemma2andvalidatesitinageneralsetting,implyingthesignificance
ofthevaluematrixinmaintainingahighmemoryrecallrate.
Next,weinvestigatethedegreeofalignmentbetweenthetrainedvaluematrixW andthe
V
constructionin(5.1). Thefirstsetofexperimentsexaminesthesimilarityinfunctionality
betweenthetwomatrices. Wereplacevaluematricesintrainedtransformerswiththe
constructedoneslikein(5.1)andthenreportaccuracywiththenewvaluematrix. Asa
baseline,wealsoconsiderrandomlyconstructedvaluematrix,wheretheouterproduct
pairsarechosenrandomly(detailedconstructioncanbefoundinAppendixC.1). FigureC.2
indicates that the accuracy does not significantly decrease when the value matrix is
replaced with the constructed ones. Furthermore, not only are the constructed value
matrixandthetrainedvaluematrixfunctionallyalike,buttheyalsosharesimilarlow-
rank approximations. We use singular value decomposition to get the best low rank
approximationsofvariousvaluematriceswheretherankissettobethesameasthenumber
of latent variables (m). We then compute smallest principal angles between low-rank
approximationsoftrainedvaluematricesandthoseofconstructed,randomlyconstructed,
andGaussian-initializedvaluematrices. FigureC.3showsthattheconstructedoneshave,
onaverage,smallestprincipalangleswiththetrainedones.
6.2 On the embeddings
Inthissection,weexplorethesignificanceofembeddingtrainingintheunderparamerized
regime and embedding structures. We conduct experiments to compare the effects of
trainingversusfreezingembeddingswithdifferentembeddingdimensions. Thelearn-
ing rate is selected as the best option from {0.01,0.001} depending on the dimensions.
FigureC.4clearlyshowsthatwhenthedimensionissmallerthanthevocabularysize
(d<V), embedding training is required. It is not necessary in the overparameterized
regime(d>V),partiallyconfirmingTheorem1becauseifembeddingsareinitializedfrom
ahigh-dimensionalmulti-variateGaussian,theyareapproximatelyorthogonaltoeach
otherandhavethesamenorms.
Thenextquestioniswhatkindofembeddingstructuresareformedfortrainedtransformers
11intheunderparamerizedregime. FromFigure3bandFigureC.5,itisevidentthatthe
relationshipbetweentheaverageinnerproductofembeddingsfortwotokensandtheir
correspondingHammingdistanceroughlyalignswith(5.2). Perhapssurprisingly,ifweplot
thesamegraphfortrainedtransformerswithafixedidentityvaluematrix,therelationship
ismostlylinearasshowninFigureC.6,confirmingourtheory(Theorem3).
AssuggestedinSection5.3,suchembeddinggeometry(5.2)canleadtolowrankstructures.
WeverifythisclaimbystudyingthespectrumoftheembeddingmatrixW . Asillustrated
E
inAppendixC.4,FigureC.9-C.12demonstratethatthereareeigengapsbetweentopand
bottomsingularvalues,suggestinglow-rankstructures.
6.3 On the attention selection mechanism
Inthissection,weexaminetheroleofattentionpatternbyconsideringaspecialclassof
latentconceptassociationmodelasdefinedinSection5.4. Figure3candFigureC.7clearly
showthattheself-attentionselecttokensinthesameclusters. Thissuggeststhatattention
canfilteroutnoiseandfocusontheinformativeconditionaldistributionπ. Weextend
experimentstoconsiderclusterstructuresthatdependonthefirsttwolatentvariables
(detailed construction can be found in Appendix C.3) and Figure C.8 shows attention
patternasexpected.
7 Conclusions
In this work, we first presented the phenomenon of context hijacking in LLMs, which
suggestedthatfactretrievalisnotrobustagainstvariationsofcontexts. Thisindicates
thatLLMsmightfunctionlikeassociativememorywheretokensincontextsarecluesto
guidememoryretrieval. Toinvestigatethisperspectivefurther,wedevisedasynthetic
task called latent concept association and examined theoretically and empirically how
single-layer transformers are trained to solve this task. These results provide further
insightsintotheinnerworkingsoftransformersandLLMs,andcanhopefullystimulate
furtherworkintointerpretingandunderstandingthemechanismsbywhichLLMspredict
tokensandrecallfacts.
Acknowledgments WethankVictorVeitchforinsightfuldiscussionsthathelpedshape
theinitialideaofthiswork. WeacknowledgethesupportofAFRLandDARPAviaFA8750-
23-2-1015,ONRviaN00014-23-1-2368,NSFviaIIS-1909816,IIS-1955532,IIS-1956330,
andNIHR01GM140467. WealsoacknowledgethesupportoftheRobertH.TopelFaculty
ResearchFundattheUniversityofChicagoBoothSchoolofBusiness.
References
[AZL23a] Z. Allen-Zhu and Y. Li. Physics of language models: part 3.1, knowledge
storageandextraction.2023.arXiv:2309.14316[cs.CL](cit.onp.3).
[AZL23b] Z. Allen-Zhu and Y. Li. Physics of language models: part 3.2, knowledge
manipulation.2023.arXiv:2309.14402[cs.CL](cit.onp.3).
[AZL24] Z. Allen-Zhu and Y. Li. Physics of language models: part 3.3, knowledge
capacityscalinglaws.2024.arXiv:2404.05405[cs.CL](cit.onp.3).
[Apr+22] G.Apruzzese,H.S.Anderson,S.Dambra,D.Freeman,F.Pierazzi,andK.A.
Roundy."realattackersdon’tcomputegradients":bridgingthegapbetween
adversarialmlresearchandpractice.2022.arXiv:2212.14315[cs.CR](cit.
onp.3).
[Bai+24] Y.Bai,F.Chen,H.Wang,C.Xiong,andS.Mei.“Transformersasstatisticians:
provablein-contextlearningwithin-contextalgorithmselection”.Advances
inneuralinformationprocessingsystems(2024)(cit.onp.3).
12[BYBOS95] R. Ben-Yishai, R. L. Bar-Or, and H. Sompolinsky. “Theory of orientation
tuninginvisualcortex.”ProceedingsoftheNationalAcademyofSciences9
(1995)(cit.onp.3).
[Bie+24] A. Bietti, V. Cabannes, D. Bouchacourt, H. Jegou, and L. Bottou. “Birth
of a transformer: a memory viewpoint”. Advances in Neural Information
ProcessingSystems(2024)(cit.onpp.3,8).
[BP21] T. Bricken and C. Pehlevan. “Attention approximates sparse distributed
memory”.AdvancesinNeuralInformationProcessingSystems(2021)(cit.on
pp.3,5).
[Bro+20] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A.
Neelakantan,P.Shyam,G.Sastry,A.Askell,S.Agarwal,A.Herbert-Voss,G.
Krueger,T.Henighan,R.Child,A.Ramesh,D.M.Ziegler,J.Wu,C.Winter,
C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C.
Berner,S.McCandlish,A.Radford,I.Sutskever,andD.Amodei.Language
modelsarefew-shotlearners.2020.arXiv:2005.14165[cs.CL](cit.onp.1).
[CDB23] V.Cabannes,E.Dohmatob,andA.Bietti.“Scalinglawsforassociativemem-
ories”.arXivpreprintarXiv:2310.02984(2023)(cit.onp.3).
[CSB24] V.Cabannes,B.Simsek,andA.Bietti.“Learningassociativememorieswith
gradientdescent”.arXivpreprintarXiv:2402.18724(2024)(cit.onpp.3,8).
[Cha22] F.Charton.“Whatismymathtransformerdoing?–threeresultsoninter-
pretabilityandgeneralization”.arXivpreprintarXiv:2211.00170(2022)(cit.
onp.3).
[Cho+24] A. G. Chowdhury, M. M. Islam, V. Kumar, F. H. Shezan, V. Jain, and A.
Chadha.“Breakingdownthedefenses:acomparativesurveyofattackson
largelanguagemodels”.arXivpreprintarXiv:2403.04786(2024)(cit.onp.3).
[Cov99] T.M.Cover.Elementsofinformationtheory.1999(cit.onp.10).
[CSH22] A.Creswell,M.Shanahan,andI.Higgins.“Selection-inference:exploiting
largelanguagemodelsforinterpretablelogicalreasoning”.arXivpreprint
arXiv:2205.09712(2022)(cit.onpp.2,3,5).
[Dai+21] D.Dai,L.Dong,Y.Hao,Z.Sui,B.Chang,andF.Wei.“Knowledgeneurons
inpretrainedtransformers”.arXivpreprintarXiv:2104.08696(2021)(cit.on
pp.1,3).
[DCAT21] N.DeCao,W.Aziz,andI.Titov.“Editingfactualknowledgeinlanguage
models”.arXivpreprintarXiv:2104.08164(2021)(cit.onpp.1,3).
[Dev83] L.Devroye.“Theequivalenceofweak,strongandcompleteconvergenceinl1
forkerneldensityestimates”.TheAnnalsofStatistics3(1983)(cit.onp.18).
[Ede+24] B.L.Edelman,E.Edelman,S.Goel,E.Malach,andN.Tsilivis.“Theevolution
of statistical induction heads: in-context learning markov chains”. arXiv
preprintarXiv:2402.11004(2024)(cit.onp.3).
[Elh+21] N.Elhage,N.Nanda,C.Olsson,T.Henighan,N.Joseph,B.Mann,A.Askell,
Y.Bai,A.Chen,T.Conerly,etal.“Amathematicalframeworkfortransformer
circuits”.TransformerCircuitsThread(2021)(cit.onp.3).
[EI+24] MEmrullahIldiz,Y.Huang,Y.Li,A.SinghRawat,andS.Oymak.“From
self-attentiontomarkovmodels:unveilingthedynamicsofgenerativetrans-
formers”.arXive-prints(2024)(cit.onp.3).
[Fel20] V. Feldman. “Does learning require memorization? a short tale about a
longtail”.In:Proceedingsofthe52ndAnnualACMSIGACTSymposiumon
TheoryofComputing.2020(cit.onp.3).
[FZ20] V.FeldmanandC.Zhang.“Whatneuralnetworksmemorizeandwhy:discov-
eringthelongtailviainfluenceestimation”.AdvancesinNeuralInformation
ProcessingSystems(2020)(cit.onp.3).
[Fir57] J.Firth.“Asynopsisoflinguistictheory,1930-1955”.Studiesinlinguistic
analysis(1957)(cit.onp.6).
13[Gar+22] S.Garg,D.Tsipras,P.S.Liang,andG.Valiant.“Whatcantransformers
learnin-context?acasestudyofsimplefunctionclasses”.AdvancesinNeural
InformationProcessingSystems(2022)(cit.onp.3).
[Gei+21] A. Geiger, H. Lu, T. Icard, and C. Potts. “Causal abstractions of neural
networks”.AdvancesinNeuralInformationProcessingSystems(2021)(cit.
onp.3).
[Gei+22] A.Geiger,Z.Wu,H.Lu,J.Rozner,E.Kreiss,T.Icard,N.Goodman,and
C.Potts.“Inducingcausalstructureforinterpretableneuralnetworks”.In:
InternationalConferenceonMachineLearning.PMLR.2022(cit.onp.3).
[Gei+24] A.Geiger,Z.Wu,C.Potts,T.Icard,andN.Goodman.“Findingalignmentsbe-
tweeninterpretablecausalvariablesanddistributedneuralrepresentations”.
In:CausalLearningandReasoning.PMLR.2024(cit.onp.3).
[Gev+23] M.Geva,J.Bastings,K.Filippova,andA.Globerson.“Dissectingrecallof
factual associations in auto-regressive language models”. arXiv preprint
arXiv:2304.14767(2023)(cit.onp.3).
[Has+24] P.Hase,M.Bansal,B.Kim,andA.Ghandeharioun.“Doeslocalizationinform
editing?surprisingdifferencesincausality-basedlocalizationvs.knowledge
editing in language models”. Advances in Neural Information Processing
Systems(2024)(cit.onp.3).
[Hen+23] T.Henighan,S.Carter,T.Hume,N.Elhage,R.Lasenby,S.Fort,N.Schiefer,
and C. Olah. “Superposition, memorization, and double descent”. Trans-
formerCircuitsThread(2023)(cit.onp.3).
[Hop82] J.J.Hopfield.“Neuralnetworksandphysicalsystemswithemergentcollec-
tivecomputationalabilities.”Proceedingsofthenationalacademyofsciences
8(1982)(cit.onp.3).
[Hu+21] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
W.Chen.Lora:low-rankadaptationoflargelanguagemodels.2021.arXiv:
2106.09685[cs.CL](cit.onp.2).
[JP20] Y.JiangandC.Pehlevan.“Associativememoryiniteratedoverparameterized
sigmoid autoencoders”. In: International conference on machine learning.
PMLR.2020(cit.onp.3).
[Jia+24] Y.Jiang,G.Rajendran,P.Ravikumar,B.Aragam,andV.Veitch.“Onthe
originsoflinearrepresentationsinlargelanguagemodels”.arXivpreprint
arXiv:2403.03867(2024)(cit.onp.3).
[Jin+23] T. Jin, N. Clement, X. Dong, V. Nagarajan, M. Carbin, J. Ragan-Kelley,
andG.K.Dziugaite.“Thecostofdown-scalinglanguagemodels:factrecall
deterioratesbeforein-contextlearning”.arXivpreprintarXiv:2310.04680
(2023)(cit.onp.3).
[KKM22] J.Kim,M.Kim,andB.Mozafari.“Provablememorizationcapacityoftrans-
formers”.In:TheEleventhInternationalConferenceonLearningRepresenta-
tions.2022(cit.onp.3).
[Li+24] Y. Li, Y. Huang, M. E. Ildiz, A. S. Rawat, and S. Oymak. “Mechanics of
nexttokenpredictionwithself-attention”.In:InternationalConferenceon
ArtificialIntelligenceandStatistics.PMLR.2024(cit.onpp.3,7).
[LLR23] Y. Li, Y. Li, and A. Risteski. “How do transformers learn topic structure:
towardsamechanisticunderstanding”.In:InternationalConferenceonMa-
chineLearning.PMLR.2023(cit.onpp.3,7).
[Liu+23a] Y.Liu,G.Deng,Y.Li,K.Wang,T.Zhang,Y.Liu,H.Wang,Y.Zheng,and
Y.Liu.“Promptinjectionattackagainstllm-integratedapplications”.arXiv
preprintarXiv:2306.05499(2023)(cit.onp.3).
[Liu+23b] Y.Liu,G.Deng,Z.Xu,Y.Li,Y.Zheng,Y.Zhang,L.Zhao,T.Zhang,and
Y.Liu.“Jailbreakingchatgptviapromptengineering:anempiricalstudy”.
arXivpreprintarXiv:2305.13860(2023)(cit.onp.3).
14[Liu+22] Z.Liu,O.Kitouni,N.S.Nolte,E.Michaud,M.Tegmark,andM.Williams.
“Towards understanding grokking: an effective theory of representation
learning”.AdvancesinNeuralInformationProcessingSystems(2022)(cit.on
p.3).
[LH17] I.LoshchilovandF.Hutter.“Decoupledweightdecayregularization”.arXiv
preprintarXiv:1711.05101(2017)(cit.onp.29).
[MLT23] S.Mahdavi,R.Liao,andC.Thrampoulidis.“Memorizationcapacityofmulti-
head attention in transformers”. arXiv preprint arXiv:2306.02010 (2023)
(cit.onp.3).
[Mak+24] A.V.Makkuva,M.Bondaschi,A.Girish,A.Nagle,M.Jaggi,H.Kim,and
M.Gastpar.Attentionwithmarkov:aframeworkforprincipledanalysisof
transformersviamarkovchains.2024.arXiv:2402.04161[cs.LG](cit.on
p.3).
[McG+23] T. McGrath, M. Rahtz, J. Kramar, V. Mikulik, and S. Legg. “The hydra
effect:emergentself-repairinlanguagemodelcomputations”.arXivpreprint
arXiv:2307.15771(2023)(cit.onp.3).
[Men+22] K.Meng,D.Bau,A.Andonian,andY.Belinkov.“Locatingandeditingfactual
associations in gpt”. Advances in Neural Information Processing Systems
(2022)(cit.onpp.1–4,27).
[Men+23] K.Meng,A.S.Sharma,A.Andonian,Y.Belinkov,andD.Bau.Mass-editing
memoryinatransformer.2023.arXiv:2210.07229[cs.CL](cit.onpp.1,3).
[Mil+22] B.Millidge,T.Salvatori,Y.Song,T.Lukasiewicz,andR.Bogacz.“Universal
hopfieldnetworks:ageneralframeworkforsingle-shotassociativememory
models”.In:InternationalConferenceonMachineLearning.PMLR.2022
(cit.onpp.3,5).
[Mit+21] E.Mitchell,C.Lin,A.Bosselut,C.Finn,andC.D.Manning.“Fastmodel
editingatscale”.arXivpreprintarXiv:2110.11309(2021)(cit.onpp.1,3).
[Mit+22] E.Mitchell,C.Lin,A.Bosselut,C.D.Manning,andC.Finn.“Memory-based
modeleditingatscale”.In:InternationalConferenceonMachineLearning.
PMLR.2022(cit.onpp.1,3).
[Nan+23] N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. “Progress
measures for grokking via mechanistic interpretability”. arXiv preprint
arXiv:2301.05217(2023)(cit.onp.3).
[NDL24] E.Nichani,A.Damian,andJ.D.Lee.Howtransformerslearncausalstruc-
turewithgradientdescent.2024.arXiv:2402.14735[cs.LG](cit.onp.3).
[Ols+22a] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan,
B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z.
Hatfield-Dodds,D.Hernandez,S.Johnston,A.Jones,J.Kernion,L.Lovitt,
K.Ndousse,D.Amodei,T.Brown,J.Clark,J.Kaplan,S.McCandlish,and
C.Olah.In-contextlearningandinductionheads.2022.arXiv:2209.11895
[cs.LG](cit.onp.3).
[Ols+22b] C.Olsson,N.Elhage,N.Nanda,N.Joseph,N.DasSarma,T.Henighan,B.
Mann,A.Askell,Y.Bai,A.Chen,etal.“In-contextlearningandinduction
heads”.arXivpreprintarXiv:2209.11895(2022)(cit.onp.3).
[PE21] L.PandiaandA.Ettinger.“Sortingthroughthenoise:testingrobustness
ofinformationprocessinginpre-trainedlanguagemodels”.arXivpreprint
arXiv:2109.12393(2021)(cit.onpp.2,3,5).
[PR22] F.PerezandI.Ribeiro.“Ignorepreviousprompt:attacktechniquesforlan-
guagemodels”.arXivpreprintarXiv:2211.09527(2022)(cit.onp.3).
[Pet+20] F.Petroni,P.Lewis,A.Piktus,T.Rocktäschel,Y.Wu,A.H.Miller,andS.
Riedel.“Howcontextaffectslanguagemodels’factualpredictions”.arXiv
preprintarXiv:2005.04611(2020)(cit.onpp.2,3,5).
15[Rad+19] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskever,etal.“Lan-
guagemodelsareunsupervisedmultitasklearners”.OpenAIblog8(2019)
(cit.onpp.1–3).
[RBU20] A.Radhakrishnan,M.Belkin,andC.Uhler.“Overparameterizedneuralnet-
worksimplementassociativememory”.ProceedingsoftheNationalAcademy
ofSciences44(2020)(cit.onp.3).
[Raj+24] G. Rajendran, S. Buchholz, B. Aragam, B. Schölkopf, and P. Ravikumar.
“Learninginterpretableconcepts:unifyingcausalrepresentationlearning
andfoundationmodels”.arXivpreprintarXiv:2402.09236(2024)(cit.onp.3).
[Ram+20] H.Ramsauer,B.Schäfl,J.Lehner,P.Seidl,M.Widrich,T.Adler,L.Gruber,
M.Holzleitner,M.Pavlović,G.K.Sandve,etal.“Hopfieldnetworksisall
youneed”.arXivpreprintarXiv:2008.02217(2020)(cit.onpp.3,5).
[Rao+23] A.Rao,S.Vashistha,A.Naik,S.Aditya,andM.Choudhury.“Trickingllms
into disobedience: understanding, analyzing, and preventing jailbreaks”.
arXivpreprintarXiv:2305.14965(2023)(cit.onp.3).
[Sak+23] M. Sakarvadia, A. Ajith, A. Khan, D. Grzenda, N. Hudson, A. Bauer, K.
Chard,andI.Foster.“Memoryinjections:correctingmulti-hopreasoning
failures during inference in transformer-based language models”. arXiv
preprintarXiv:2309.05605(2023)(cit.onp.3).
[Seu96] H.S.Seung.“Howthebrainkeepstheeyesstill”.ProceedingsoftheNational
AcademyofSciences23(1996)(cit.onp.3).
[SMR23] M.Shanahan,K.McDonell,andL.Reynolds.“Roleplaywithlargelanguage
models”.Nature7987(2023)(cit.onp.3).
[She+24] H.Sheen,S.Chen,T.Wang,andH.H.Zhou.“Implicitregularizationofgra-
dientflowonone-layersoftmaxattention”.arXivpreprintarXiv:2403.08699
(2024)(cit.onp.3).
[Shi+23] F.Shi,X.Chen,K.Misra,N.Scales,D.Dohan,E.H.Chi,N.Schärli,andD.
Zhou.“Largelanguagemodelscanbeeasilydistractedbyirrelevantcontext”.
In:InternationalConferenceonMachineLearning.PMLR.2023(cit.onpp.2,
3,5).
[Si+22] W.M.Si,M.Backes,J.Blackburn,E.DeCristofaro,G.Stringhini,S.Zannet-
tou,andY.Zhang.“Whysotoxic?measuringandtriggeringtoxicbehaviorin
open-domainchatbots”.In:Proceedingsofthe2022ACMSIGSACConference
onComputerandCommunicationsSecurity.2022(cit.onp.3).
[Ska+94] W.Skaggs,J.Knierim,H.Kudrimoti,andB.McNaughton.“Amodelofthe
neuralbasisoftherat’ssenseofdirection”.Advancesinneuralinformation
processingsystems(1994)(cit.onp.3).
[SS22] J.SteinbergandH.Sompolinsky.“Associativememoryofstructuredknowl-
edge”.ScientificReports1(2022)(cit.onp.3).
[Tar+23a] D.A.Tarzanagh,Y.Li,C.Thrampoulidis,andS.Oymak.“Transformersas
supportvectormachines”.arXivpreprintarXiv:2308.16898(2023)(cit.on
pp.3,7).
[Tar+23b] D.A.Tarzanagh,Y.Li,X.Zhang,andS.Oymak.“Marginmaximizationin
attentionmechanism”.arXivpreprintarXiv:2306.13596(2023)(cit.onp.3).
[Tea+24] G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak,
L.Sifre,M.Rivière,M.S.Kale,J.Love,etal.“Gemma:openmodelsbased
ongeminiresearchandtechnology”.arXivpreprintarXiv:2403.08295(2024)
(cit.onpp.2,3,7).
[Tia+24] B.Tian,S.Cheng,X.Liang,N.Zhang,Y.Hu,K.Xue,Y.Gou,X.Chen,andH.
Chen.“Instructedit:instruction-basedknowledgeeditingforlargelanguage
models”.arXivpreprintarXiv:2402.16123(2024)(cit.onp.3).
[Tia+23a] Y.Tian,Y.Wang,B.Chen,andS.S.Du.“Scanandsnap:understanding
trainingdynamicsandtokencompositionin1-layertransformer”.Advances
inNeuralInformationProcessingSystems(2023)(cit.onp.3).
16[Tia+23b] Y.Tian,Y.Wang,Z.Zhang,B.Chen,andS.Du.“Joma:demystifyingmulti-
layertransformersviajointdynamicsofmlpandattention”.arXivpreprint
arXiv:2310.00535(2023)(cit.onp.3).
[Tou+23] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
Bashlykov,S.Batra,P.Bhargava,S.Bhosale,etal.“Llama2:openfoundation
andfine-tunedchatmodels”.arXivpreprintarXiv:2307.09288(2023)(cit.on
pp.2,3).
[Vas+17] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
Ł.Kaiser,andI.Polosukhin.“Attentionisallyouneed”.Advancesinneural
informationprocessingsystems(2017)(cit.onpp.2,3).
[Wan+23a] B.Wang,W.Chen,H.Pei,C.Xie,M.Kang,C.Zhang,C.Xu,Z.Xiong,R.
Dutta,R.Schaeffer,etal.“Decodingtrust:acomprehensiveassessmentof
trustworthinessingptmodels”.arXivpreprintarXiv:2306.11698(2023)(cit.
onp.3).
[Wan+23b] J.Wang,X.Hu,W.Hou,H.Chen,R.Zheng,Y.Wang,L.Yang,H.Huang,
W.Ye,X.Geng,etal.“Ontherobustnessofchatgpt:anadversarialandout-
of-distributionperspective”.arXivpreprintarXiv:2302.12095(2023)(cit.on
p.3).
[Wan+22] K.Wang,A.Variengien,A.Conmy,B.Shlegeris,andJ.Steinhardt.“Inter-
pretability in the wild: a circuit for indirect object identification in gpt-2
small”.arXivpreprintarXiv:2211.00593(2022)(cit.onp.3).
[Wu+24] Z. Wu, A. Geiger, T. Icard, C. Potts, and N. Goodman. “Interpretability
at scale: identifying causal mechanisms in alpaca”. Advances in Neural
InformationProcessingSystems(2024)(cit.onp.3).
[Xie+21] S.M.Xie,A.Raghunathan,P.Liang,andT.Ma.“Anexplanationofin-context
learningasimplicitbayesianinference”.arXivpreprintarXiv:2111.02080
(2021)(cit.onp.3).
[Xu+23] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, and M. Kankanhalli.
“Anllmcanfoolitself:aprompt-basedadversarialattack”.arXivpreprint
arXiv:2310.13345(2023)(cit.onp.3).
[Yor+23] O.Yoran,T.Wolfson,O.Ram,andJ.Berant.“Makingretrieval-augmented
languagemodelsrobusttoirrelevantcontext”.arXivpreprintarXiv:2310.01558
(2023)(cit.onpp.2,3,5).
[Zha+22] Y.Zhang,A.Backurs,S.Bubeck,R.Eldan,S.Gunasekar,andT.Wagner.
“Unveilingtransformerswithlego:asyntheticreasoningtask”.arXivpreprint
arXiv:2206.04301(2022)(cit.onp.3).
[Zha+23] Z.Zhang,M.Fang,L.Chen,M.-R.Namazi-Rad,andJ.Wang.“Howdolarge
languagemodelscapturetheever-changingworldknowledge?areviewof
recentadvances”.arXivpreprintarXiv:2310.07343(2023)(cit.onp.3).
[Zha23] J.Zhao.“In-contextexemplarsascluestoretrievingfromlargeassociative
memory”.arXivpreprintarXiv:2311.03498(2023)(cit.onpp.3,5).
[Zhe+23] C.Zheng,L.Li,Q.Dong,Y.Fan,Z.Wu,J.Xu,andB.Chang.“Canweedit
factualknowledgebyin-contextlearning?”arXivpreprintarXiv:2305.12740
(2023)(cit.onp.3).
[Zho+24] Z.Zhong,Z.Liu,M.Tegmark,andJ.Andreas.“Theclockandthepizza:two
storiesinmechanisticexplanationofneuralnetworks”.AdvancesinNeural
InformationProcessingSystems(2024)(cit.onp.3).
[Zhu+23] K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang, W. Ye,
N.Z.Gong,Y.Zhang,etal.“Promptbench:towardsevaluatingtherobust-
ness of large language models on adversarial prompts”. arXiv preprint
arXiv:2306.04528(2023)(cit.onp.3).
[Zou+23] A.Zou,Z.Wang,N.Carlini,M.Nasr,J.Z.Kolter,andM.Fredrikson.Univer-
salandtransferableadversarialattacksonalignedlanguagemodels.2023.
arXiv:2307.15043[cs.CL](cit.onp.3).
17A Additional Theoretical Results and Proofs
A.1 Proofs for Section 5.1
Theorem1canbestatedmoreformallyasfollows:
Theorem7. SupposethedatageneratingprocessfollowsSection4.1wherem≥3,ω=1,
andN(t)=V\{t}. Assumethereexistsasinglelayertransformergivenby(4.1)suchthat
a)W =0andW =0,b)EachrowofW isorthogonaltoeachotherandnormalized,and
K Q E
c)W isgivenby
V
W = (cid:88) W (i)( (cid:88) W (j)T).
V E E
i∈[V] j∈N 1(i)
ThenifL>max{ 100m2log(3/ε) , 80m2|N(y)| }forany y,then
(exp(− β1)−exp(− β2))2 (exp(− β1)−exp(− β2))2
R (fL)≤ε,
DL
where0<ε<1.
Proof. Firstofall,theerrorisdefinedtobe:
R (fL)=P [argmaxfL(x)̸=y]
DL (x,y)∼DL
=P yP x|y[argmaxfL(x)̸=y]
Let’sfocusontheconditionalprobabilityP x|y[argmaxfL(x)̸=y].
Byconstruction,thesinglelayertransformermodelhasuniformattention. Therefore,
h(x)= (cid:88) α W (i)
i E
i∈N(y)
whereα = 1(cid:80)L 1{t =i}whichisthenumberofoccurrenceoftoken iinthesequence.
i L k=1 k
Bythelatentconceptassociationmodel,weknowthat
exp(−D (i,y)/β)
p(i|y)= H
Z
whereZ=(cid:80) i∈N(y)exp(−D H(i,y)/β).
Thus,thelogitfortoken yis
fL(x)= (cid:88) α
y i
i∈N 1(y)
Andthelogitforanyothertoken y˜ is
fL(x)= (cid:88) α
y˜ i
i∈N 1(y˜)
Forthepredictiontobecorrect,weneed
maxfL(x)−fL(x)>0
y y˜
y˜
ByLemma3of[Dev83],weknowthatforall∆∈(0,1),if |N(y)|≤∆2,wehave
L 20
P(cid:161) max |α −p(i|y)|>∆(cid:162)≤P(cid:161) (cid:88) |α −p(i|y)|>∆(cid:162)≤3exp(−L∆2/25)
i i
i∈N(y)
i∈N(y)
18Therefore,ifL≥max{25log(3/ε) ,20|N(y)| },thenwithprobabilityatleast1−ε,wehave,
∆2 ∆2
max |α −p(i|y)|≤∆
i
i∈N(y)
fL(x)−fL(x)= (cid:88) α − (cid:88) α
y y˜ i j
i∈N 1(y) j∈N 1(y˜)
= (cid:88) α − (cid:88) p(i|y)+ (cid:88) p(i|y)
i
i∈N 1(y) i∈N 1(y) i∈N 1(y)
− (cid:88) p(j|y)+ (cid:88) p(j|y)− (cid:88) α
j
j∈N 1(y˜) j∈N 1(y˜) j∈N 1(y˜)
≥ (cid:88) p(i|y)− (cid:88) p(j|y)−2m∆
i∈N 1(y) j∈N 1(y˜)
1 2
≥exp(− )−exp(− )−2m∆
β β
NotethatbecauseofLemma10,there’snoneighboringsetthatisthesupersetofanother.
Thereforeaslongas∆<exp(− β1)−exp(− β2)
,
2m
fL(x)−fL(x)>0
y y˜
forany y˜.
Finally,ifL>max{ 100m2log(3/ε) , 80m2|N(y)| }forany y,then
(exp(− β1)−exp(− β2))2 (exp(− β1)−exp(− β2))2
P x|y[argmaxfL(x)̸=y]≤ε
And
R (fL)=P [argmaxfL(x)̸=y]
DL (x,y)∼DL
=P yP x|y[argmaxfL(x)̸=y]≤ε
A.2 Proofs for Section 5.2
Lemma2. SupposethedatageneratingprocessfollowsSection4.1wherem≥3,ω=1and
N(t)={t′ :D (t,t′ ))=1}. Foranysinglelayertransformergivenby(4.1)whereeachrowof
H
W isorthogonaltoeachotherandnormalized,ifW isconstructedasin(5.1),thenthe
E V
errorrateis0. IfW istheidentitymatrix,thentheerrorrateisstrictlylargerthan0.
V
Proof. FollowingtheproofforTheorem7,let’sfocusontheconditionalprobability:
P x|y[argmaxfL(x)̸=y]
Byconstruction,wehave
h(x)= (cid:88) α W (i)
i E
i∈N 1(y)
whereα = 1(cid:80)L 1{t =i}whichisthenumberofoccurrenceoftoken iinthesequence.
i L k=1 k
Let’sconsiderthefirstcasewhereW isconstructedasin(5.1). Thenweknowthatfor
V
someothertoken y˜̸=y,
fL(x)−fL(x)= (cid:88) α − (cid:88) α =1− (cid:88) α
y y˜ i i i
i∈N 1(y) i∈N 1(y˜) i∈N 1(y˜)
19ByLemma10,wehavethatforanytoken y˜̸=y,
fL(x)−fL(x)>0
y y˜
Therefore,theerrorrateisalways0.
Nowlet’sconsiderthesecondcasewhereW istheidentitymatrix. Let jbeatokeninthe
V
setN (y). Thenthereisanon-zeroprobabilitythatcontextxcontainsonly j. Inthatcase,
1
h(x)=W (j)
E
However,weknowthatbytheassumptionontheembeddingmatrix,
fL(x)−fL(x)=(W (y)−W (j))Th(x)=−∥W (j)∥2<0
y j E E E
Thisimpliesthatthere’snonzeroprobabilitythat yismisclassified. Therefore,whenW
V
istheidentitymatrix,theerrorrateisstrictlylargerthan0.
Theorem3. SupposethedatageneratingprocessfollowsSection4.1wherem≥3,ω=1
andN(t)=V\{t}. Foranysinglelayertransformergivenby(4.1)withW beingtheidentity
V
matrix,ifthecrossentropylossisminimizedsothatforanysampledpair(x,y),
p(y|x)=pˆ(y|x)=softmax(fL(x))
y
thereexistsa>0andbsuchthatfortwotokenst̸=t′
,
〈W (t),W (t′ )〉=−aD (t,t′ )+b
E E H
Proof. Becauseforanypairof(x,y),theestimatedconditionalprobabilitymatchesthetrue
conditionalprobability. Inparticular,let’sconsidertwotargettokens y , y andcontext
1 2
x=(t ,...,t )forsometokent suchthat p(x|y )>0and p(x|y )>0,then
i i i 1 2
p(y |x) p(x|y )p(y ) p(x|y ) pˆ(x|y )
1 = 1 1 = 1 = 1 =exp((W (y )−W (y ))Th(x))
p(y |x) p(x|y )p(y ) p(x|y ) pˆ(x|y ) E 1 E 2
2 2 2 2 2
Thesecondequalityisbecause p(y)istheuniformdistribution. Byourconstruction,
p(x|y ) p(t |y )L
1 = i 1 =exp((W (y )−W (y ))Th(x))=exp((W (y )−W (y ))TW (t ))
p(x|y 2) p(t i|y 2)L E 2 E 1 E 1 E 2 E i
Bythedatageneratingprocess,wehavethat
L
(D (t ,y )−D (t ,y ))=(W (y )−W (y ))TW (t )
β H i 2 H i 1 E 1 E 2 E i
Lett =y suchthat y ̸=y , y ̸=y ,then
i 3 3 1 3 2
L L
D (y ,y )−W (y )TW (y )= D (y ,y )−W (y )TW (y )
β H 3 1 E 1 E 3 β H 3 2 E 2 E 3
Forsimplicity,let’sdefine
L
Ψ(y ,y )= D (y ,y )−W (y )TW (y )
1 2 β H 1 2 E 1 E 2
Therefore,
Ψ(y ,y )=Ψ(y ,y )
3 1 3 2
Nowconsiderfivedistinctlabels: y ,y ,y ,y ,y . Wehave,
1 2 3 4 5
Ψ(y ,y )=Ψ(y ,y )=Ψ(y ,y )=Ψ(y ,y )
3 1 3 2 4 2 4 5
20In other words, Ψ(y ,y )=Ψ(y ,y ) for arbitrarily chosen distinct labels y ,y ,y ,y .
3 1 4 5 1 3 4 5
Therefore,Ψ(t,t′ )isaconstantfort̸=t′.
Foranytwotokenst̸=t′,
L
D (t,t′ )−W (t)TW (t′ )=C
β H E E
Thus,
L
W (t)TW (t′ )=− D (t,t′ )+C
E E β H
A.3 Proofs for Section 5.3
Theorem4canbeformalizedasthefollowingtheorem.
Theorem8. FollowingthesamesetupasinTheorem7,butembeddingsfollow(5.2)then
ifb>0,∆ >0,0<∆<exp(− β1)−exp(− β2) ,L≥max{25log(3/ε) ,20|N(y)| }forany y,and
1 2m ∆2 ∆2
2exp(1)
β
0<a<
(|V|−2)m2
and
b >max{
a(m−2)m+∆
1
+b,(b−a)∆ 1−|V 2|−2abm2exp(− β1)+|V 2|−2a2(m−2)m2
}
0 exp(−1)−exp(−2)−2m∆ 1−|V|−2am2exp(−1)
β β 2 β
wehave
R (fL)≤ε
DL
where0<ε<1.
Proof. FollowingtheproofofTheorem7,let’salsofocusontheconditionalprobability
P x|y[argmaxfL(x)̸=y]
Byconstruction,thesinglelayertransformermodelhasuniformattention. Therefore,
h(x)= (cid:88) α W (i)
i E
i∈N(y)
whereα = 1(cid:80)L 1{t =i}whichisthenumberofoccurrenceoftoken iinthesequence.
i L k=1 k
Forsimplicity,let’sdefineα =0suchthat
y
h(x)= (cid:88) α W (i)
i E
i∈[V]
Similarly,wealsohavethatifL≥max{25log(3/ε) ,20|N(y)| },thenwithprobabilityatleast
∆2 ∆2
1−ε,wehave,
max|α −p(i|y)|≤∆
i
i∈[V]
Alsodefinethefollowing:
φ (x)= (cid:88) W (j)T(cid:161) (cid:88) α W (i)(cid:162)
k E i E
j∈N 1(k) i∈[V]
v (y)=W (y)TW (k)
k E E
21Thus,thelogitfortoken yis
|V|−1
fL(x)= (cid:88) v (y)φ (x)
y k k
k=0
Let’sinvestigateφ (x). ByLemma9,
k
φ (x)= (cid:88) α ( (cid:88) W (j)TW (i))
k i E E
i∈[V] j∈N 1(k)
=(b −b) (cid:88) α + (cid:88) α (−a(m−2)D (k,i)+(b−a)m)
0 j i H
j∈N 1(k) i∈[V]
Thus,foranyk ,k ∈[V],
1 2
φ (x)−φ (x)=(b −b)( (cid:88) α − (cid:88) α )
k1 k2 0 j1 j2
j1∈N 1(k1) j2∈N 1(k2)
+ (cid:88) α a(m−2)(D (k ,i)−D (k ,i))
i H 2 H 1
i∈[V]
Because−m≤D (k ,i)−D (k ,i)≤m,wehave
H 2 H 1
(b −b)( (cid:88) α − (cid:88) α )−a(m−2)m
0 j1 j2
j1∈N 1(k1) j2∈N 1(k2)
≤φ (x)−φ (x)≤
k1 k2
(b −b)( (cid:88) α − (cid:88) α )+a(m−2)m
0 j1 j2
j1∈N 1(k1) j2∈N 1(k2)
Forpredictiontobecorrect,weneed
maxfL(x)−fL(x)>0
y y˜
y˜
Thisalsomeansthat
|V|−1
max (cid:88) (cid:161) v (y)−v (y˜)(cid:162)φ (x)>0
k k k
y˜ k=0
Onecanshowthatforanyk,ifι−1(k˜)=ι−1(y)⊗ι−1(y˜)⊗ι−1(k)where⊗meansbitwiseXOR,
then
v k(y)−v k(y˜)=v k˜(y˜)−v k˜(y) (A.1)
Firstofall,ifk=y,thenk˜=y˜,whichmeans
v k(y)−v k(y˜)=v k˜(y˜)−v k˜(y)=b 0+aD H(y,y˜)−b
Ifk̸=y,y˜,then(A.1)impliesthat
D (k,y)−D (k,y˜)=D (k˜,y˜)−D (k˜,y)
H H H H
WeknowthatD (k,y)isthenumberof1sinι−1(k)⊗ι−1(y)and,
H
ι−1(k˜)⊗ι−1(y)=ι−1(y)⊗ι−1(y˜)⊗ι−1(k)⊗ι−1(y)=ι−1(y˜)⊗ι−1(k)
Similarly,
ι−1(k˜)⊗ι−1(y˜)=ι−1(y)⊗ι−1(k)
22Therefore,(A.1)holdsandwecanrewrite fL(x)−fL(x)as
y y˜
|V|−1
fL(x)−fL(x)= (cid:88) (cid:161) v (y)−v (y˜)(cid:162)φ (x)
y y˜ k k k
k=0
=(b −b+aD (y,y˜))(φ (x)−φ (x))
0 H y y˜
+ (cid:88) a(D H(k,y)−D H(k,y˜))(φ k(x)−φ k˜(x))
k̸=y,y˜,DH(k,y)≥DH(k,y˜)
Wealreadyknowthatb >b>0anda>0,thus,b −b+aD (y,y˜)>0foranypair y,y˜.
0 0 H
Wealsowantφ (x)−φ (x)tobepositive. Notethat
y y˜
1 2
φ (x)−φ (x)≥(b −b)(exp(− )−exp(− )−2m∆)−a(m−2)m
y y˜ 0 β β
Weneed∆<exp(− β1)−exp(− β2)
andforsomepositive∆ >0,b needstobelargeenoughsuch
2m 1 0
that
φ (x)−φ (x)>∆
y y˜ 1
whichimpliesthat
a(m−2)m+∆
b > 1 +b (A.2)
0 exp(−1)−exp(−2)−2m∆
β β
Ontheotherhand,fork̸=y,y˜,wehave
φ k(x)−φ k˜(x)≥(b 0−b)( (cid:88) α j1− (cid:88) α j2)−a(m−2)m
j1∈N 1(k) j2∈N 1(k˜)
1 2
≥(b −b)(−(m−1)exp(− )−exp(− )−2m∆)−a(m−2)m
0 β β
1 2 2 1
≥(b −b)(−(m−1)exp(− )−exp(− )+exp(− )−exp(− ))−a(m−2)m
0 β β β β
1
≥−(b −b)mexp(− )−a(m−2)m
0 β
Then,wehave
|V|−2(cid:179) 1 (cid:180)
fL(x)−fL(x)≥(b −b+a)∆ − (b −b)am2exp(− )+a2(m−2)m2
y y˜ 0 1 2 0 β
(cid:179) |V|−2 1 (cid:180) |V|−2 1 |V|−2
≥ 1− am2exp(− ) b −(b−a)∆ + abm2exp(− )− a2(m−2)m2
2 β 0 1 2 β 2
Thelowerboundisindependentof y˜, therefore, weneedittobepositivetoensurethe
predictioniscorrect. Toachievethis,wewant
|V|−2 1
1− am2exp(− )>0
2 β
whichimpliesthat
2exp(1)
β
a< (A.3)
(|V|−2)m2
Andfinallyweneed
(b−a)∆ −|V|−2abm2exp(−1)+|V|−2a2(m−2)m2
b > 1 2 β 2 (A.4)
0 1−|V|−2am2exp(−1)
2 β
23Tosummarize,ifb>0,∆ >0,0<∆<exp(− β1)−exp(− β2) ,L≥max{25log(3/ε) ,20|N(y)| }forany
1 2m ∆2 ∆2
y,and
2exp(1)
β
0<a<
(|V|−2)m2
and
b >max{
a(m−2)m+∆
1
+b,(b−a)∆ 1−|V 2|−2abm2exp(− β1)+|V 2|−2a2(m−2)m2
}
0 exp(−1)−exp(−2)−2m∆ 1−|V|−2am2exp(−1)
β β 2 β
wehave
R (fL)≤ε
DL
where0<ε<1.
Lemma5. Ifembeddingsfollow(5.2)andb=b andN(t)=V\{t},thenrank(W )≤m+2.
0 E
Proof. By(5.2),wehavethat
〈W (i),W (j)〉=−aD (i,j)+b
E E H
Therefore,
(W )TW =−aD +b11T
E E H
Let’sfirstlookatD whichhasrankatmostm+1. Toseethis,let’sconsiderasetofm+1
H
tokens: {e ,e ,...,e }⊆V wheree =2k. Heree isassociatedwiththelatentvectorofall
0 1 m k 0
zeroesandthelatentvectorassociatedwith e hasonlythek-thlatentvariablebeing1.
k
Ontheotherhand,foranytoken i,wehavethat,
i= (cid:88) e
k
k:ι−1(i)k=1
Infact,
D (i)= (cid:88) (cid:179) D (e )−D (e )(cid:180) +D (e )
H H k H 0 H 0
k:ι−1(i)k=1
whereD (i)isthe i-throwofD ,andforeachentry jofD (i),wehavethat
H H H
D (i,j)= (cid:88) (cid:179) D (e ,j)−D (e ,j)(cid:180) +D (e ,j)
H H k H 0 H 0
k:ι−1(i)k=1
Thisisbecause
(cid:40) +1 ifι−1(j) =0
D (e ,j)−D (e ,j)= k
H k H 0 −1 ifι−1(j) =1
k
Thus,wecanrewriteD (i,j)as
H
D (i,j)= (cid:88) (cid:179) 1[ι−1(i) =1,ι−1(j) =0]−1[ι−1(i) =1,ι−1(j) =1)](cid:180) +D (e ,j)
H k k k k H 0
k:ι−1(i)k=1
=
(cid:88)m (cid:179)
1[ι−1(i) =1,ι−1(j) =0]−1[ι−1(i) =1,ι−1(j)
=1)](cid:180)
k k k k
k=1
+(cid:88)m (cid:179)
1[ι−1(i) =0,ι−1(j) =1]+1[ι−1(i) =1,ι−1(j)
=1)](cid:180)
k k k k
k=1
m
= (cid:88) 1[ι−1(i) =1,ι−1(j) =0]+1[ι−1(i) =0,ι−1(j) =1]
k k k k
k=1
=D (i,j)
H
24Therefore,everyrowofD canbewrittenasalinearcombinationof{D (e ),D (e ),...,D (e )}.
H H 0 H 1 H m
Inotherwords,D hasrankatmostm+1.
H
Therefore,
rank((W )TW )=rank(W )≤m+2.
E E E
Lemma9. Let z(0) and z(1) betwobinaryvectorsofsizemwherem≥2. Then,
(cid:88) D (z,z(1))=(m−2)D (z(0),z(1))+m
H H
z:DH(z(0),z)=1
Proof. For zsuchthatD (z,z(0))=1,weknowthattherearetwocases. Either zdiffers
H
with z(0) onaentrybutagreeswith z(1) onthatentryor zdifferswithboth z(0) and z(1).
Forthefirstcase,weknowthatthereareD (z(0),z(1))suchentries. Inthiscase,D (z,z(1))=
H H
D (z(0),z(1))−1. Forthesecondcase,D (z,z(1))=D (z(0),z(1))+1.
H H H
Therefore,
(cid:88) D (z,z(1))
H
z:DH(z,z(0))=1
=D (z(0),z(1))(D (z(0),z(1))−1)+(m−D (z(0),z(1)))(D (z(0),z(1))+1)
H H H H
=(m−2)D (z(0),z(1))+m
H
Lemma10. Ifm≥3andN(t)=V\{t},thenN (t)̸⊆N (t′ )foranyt,t′∈[V].
1 1
Proof. Foranytokent,N (t)containsanytokent′suchthatD (t,t′ )=1bytheconditions.
1 H
ThengivenasetN (t),onecanuniquelydeterminetoken t. Thisisbecausefortheset
1
of latent vectors associated with N (t), at each index, there could only be one possible
1
change.
A.4 Proofs for Section 5.4
Lemma6.
SupposethedatageneratingprocessfollowsSection4.1andN(z∗ )={z:z∗=
1
∗
z }\{z }. Giventhelasttokeninthesequencet ,then
1 L
L
∇ ℓ(fL)=∇ℓ(fL)T(W )TWV(α pˆ W (t)−pˆ (cid:88) pˆ W (t ))
ut,tL E t t E t
l=1
tl E l
wherefortokent,α =(cid:80)L 1[t =t]and pˆ isthenormalizedattentionscorefortokent.
t l=1 l t
Proof. Recallthat,
(cid:104) (cid:105)
fL(x)= W TW attn(W χ(x))
E V E
:L
=W TW
(cid:88)L exp(u tl,tL)
W (t )
E V E l
Z
l=1
whereZ isanormalizingconstant.
Define pˆ
=exp(utl,tL)
. Thenwehave
tl Z
L
fL(x)=W TW (cid:88) pˆ W (t )
E V tl E l
l=1
25Notethatift =tthen,
l
∂pˆ
tl =pˆ (1−pˆ )
∂u tl tl
t,tL
Otherwise,
∂pˆ
tl =−pˆ pˆ
∂u tl t
t,tL
Bythechainrule,weknowthat
L L
∇ ℓ(fL)=∇ℓ(fL)T(W )TWV((cid:88) 1[t =t]pˆ W (t)−(cid:88) pˆ pˆ W (t ))
ut,tL E
l=1
l tl E
l=1
tl t E l
Therefore,
L
∇ ℓ(fL)=∇ℓ(fL)T(W )TWV(α pˆ W (t)−pˆ (cid:88) pˆ W (t ))
ut,tL E t t E t
l=1
tl E l
whereα =(cid:80)L 1[t =t].
t l=1 l
26B Additional experiments – context hijacking
Inthissection,weshowtheresultsofadditionalcontexthijackingexperimentsonthe
CounterFactdataset[Men+22].
Reversecontexthijacking InFigure2a,wesawtheeffectsofhijackingbyaddingin
“Donotthinkof{target_false}.” toeachcontext. Now,wemeasuretheeffectofthereverse:
Whatifweprepend“Donotthinkof{target_true}.” ?
BasedonthestudyinthispaperonhowassociativememoryworksinLLMs,weshould
expect the efficacy score to increase. Indeed, this is what happens, as we see in Fig-
ureB.1.
FigureB.1:Prepending‘Donotthinkof{target_true}.’canincreasethechanceofLLMstooutput
correcttokens.ThisfigureshowsefficacyscoreversusthenumberofprependsforvariousLLMson
theCounterFactdatasetwiththereversecontexthijackingscheme.
HijackingbasedonrelationIDs Wefirstgiveanexampleofeachofthe4relationIDs
wehijackinTable1.
Table1:ExamplesofcontextsinRelationIDsfromCounterFact
RelationIDr Contextp Truetargeto∗ Falsetargeto_
P190 Kharkivisatwincityof Warsaw Athens
P103 ThenativelanguageofAnatoleFranceis French English
P641 HankAaronprofessionallyplaysthesport baseball basketball
P131 KalamazooCountycanbefoundin Michigan Indiana
Table2:ExamplesofhijackandreversehijackformatsbasedonRelationIDs
RelationIDr ContextHijacksentence ReverseContextHijacksentence
P190 Thetwincityof{subject}isnot{target_false} Thetwincityof{subject}is{target_true}
P103 {subject}cannotspeak{target_false} {subject}canspeak{target_true}
P641 {subject}doesnotplay{target_false} {subject}plays{target_true}
P131 {subject}isnotlocatedin{target_false} {subject}islocatedin{target_true}
Similar to Figure 2b, we repeat the hijacking experiments where we prepend factual
sentencesgeneratedfromtherelationID.WeusetheformatillustratedinTable2for
27(a)RelationP103 (b)RelationP132
(c)RelationP190 (d)RelationP641
FigureB.2: ContexthijackingbasedonrelationIDscanresultinLLMsoutputincorrecttokens.
ThisfigureshowsefficacyscoreversusthenumberofprependsforvariousLLMsontheCounterFact
datasetwithhijackingschemepresentedinTable2.
theprependedsentences. Weexperimentwith3otherrelationIDsandweseesimilar
trendsforalltheLLMsinFigureB.2a,B.2b,andB.2d. Thatis,theefficacyscoredropsfor
thefirstprependandasweincreasethenumberofprepends,thetrendofESdropping
continues. Therefore,thisconfirmsourintuitionthatLLMscanbehijackedbycontexts
withoutchangingthefactualmeaning.
SimilartoFigureB.1,weexperimentwithreversecontexthijackingwherewegivethe
answersbasedonrelationIDs,asshowninTable2. Weagainexperimentwiththesame
4relationIDsandtheresultsareinFigureB.3a-B.3d. Weseethattheefficacyscore
increaseswhenweprependtheanswersentence,therebyverifyingtheobservationsofthis
study.
Hijackingwithoutexacttargetwords Sofar,theexperimentsusepromptsthateither
containtrueorfalsetargetwords. Itturnsout,theinclusionofexacttargetwordsare
notnecessary. Toseethis,weexperimentavariantofthegenerichijackingandreverse
hijackingexperiments. Butinsteadofsaying“Donotthinkof{target_false}”or“Donot
thinkof{target_true}”. Wereplacetargetwordswithwordsthataresemanticallyclose. In
particular,forrelationP1412,wereplacewordsrepresentinglanguage(e.g.,“French”)with
theirassociatedcountryname(e.g.,“France”). AsshowninFigureB.4,contexthijacking
andreversehijacingstillworkinthiscase.
28(a)RelationP103 (b)RelationP132
(c)RelationP190 (d)RelationP641
FigureB.3:ReversecontexthijackingbasedonrelationIDscanresultinLLMstobemorelikelyto
becorrect.ThisfigureshowsefficacyscoreversusthenumberofprependsforvariousLLMsonthe
CounterFactdatasetwiththereversehijackingschemepresentedinTable2.
C Additional experiments and figures – latent concept
association
Inthisappendixsection,wepresentadditionalexperimentaldetailsandresultsfromthe
syntheticexperimentsonlatentconceptassociation.
Experimentalsetup SyntheticdataaregeneratedfollowingthemodelinSection4.1.
Unlessotherwisestated,thedefaultsetuphasω=0.5,β=1andN(i)=V\{i}andL=256.
Thedefaulthiddendimensionoftheone-layertransformerisalsosettobe256. Themodel
isoptimizedusingAdamW[LH17]wherethelearningrateischosenfrom{0.01,0.001}.
Theevaluationdatasetisdrawnfromthesamedistributionasthetrainingdatasetand
consistsof1024(x,y)pairs. AlthoughtheoreticalresultsinSection5mayfreezecertain
partsofthenetworkforsimplicity,inthissection,unlessotherwisespecified,alllayers
ofthetransformersaretrainedjointly. Also,inthissection,wetypicallyreportaccuracy
whichis1−error.
C.1 On the value matrix W
V
Inthissection,weprovideadditionalfiguresofSection6.1. Specifically,FigureC.1shows
thatfixingthevaluematrixtobetheidentitywillnegativelyimpactaccuracy. FigureC.2
indicatesthatreplacingtrainedvaluematriceswithconstructedonescanpreserveaccuracy
tosomeextent. FigureC.3suggeststhattrainedvaluematricesandconstructedones
sharesimilarlow-rankapproximations. Forthelasttwosetsofexperiments,weconsider
randomlyconstructedvaluematrix,wheretheouterproductpairsarechosenrandomly,
definedformallyasfollows:
29(a)HijackingP1412 (b)ReversehijackingP1412
FigureB.4: HijackingandreversehijackingexperimentsonrelationP1412showthatcontext
hijackingdoesnotrequireexacttargetwordtoappearinthecontext.Thisfigureshowsefficacyscore
versusthenumberofprependsforvariousLLMsontheCounterFactdataset.
(a)L=64 (b)L=128
FigureC.1:FixingthevaluematrixWV astheidentitymatrixresultsinloweraccuracycomparedto
trainingWV,especiallyforsmallercontextlengthL.Thefigurereportsaccuracyforbothfixedand
trainedWV settings,withstandarderrorscalculatedover10runs.
W = (cid:88) W (i)( (cid:88) W (j)T)
V E E
i∈[V] {j}∼Unif([V])|N1(i)|
C.2 On the embeddings
ThissectionprovidesadditionalfiguresfromSection6.2. FigureC.4showsthatinthe
underparameterizedregime,embeddingtrainingisrequired. FigureC.5indicatesthat
theembeddingstructureintheunderparameterizedregimeroughlyfollows(5.2). Finally
Figure C.6showsthat, when thevaluematrix is fixed to the identity, therelationship
betweeninnerproductofembeddingsandtheircorrespondingHammingdistanceismostly
linear.
C.3 On the attention selection mechanism
ThissectionprovidesadditionalfiguresfromSection6.3. FigureC.7-C.8showthatat-
tentionmechanismselectstokensinthesameclusterasthelasttoken. Inparticular,for
FigureC.8,weextendexperimentstoconsiderclusterstructuresthatdependonthefirst
twolatentvariables. Inotherwords,foranylatentvector z∗,wehave
N(z∗ )={z:z∗=z and z∗=z }\{z∗ }
1 1 2 2
30(a)m=5 (b)m=6
(c)m=7 (d)m=8
FigureC.2:Whenthevaluematrixisreplacedwiththeconstructedoneintrainedtransformers,
theaccuracydoesnotsignificantlydecreasecomparedtoreplacingthevaluematrixwithrandomly
constructedones.Thegraphreportsaccuracyunderdifferentembeddingdimensionsandstandard
errorsareover5runs.
31(a)m=5 (b)m=6
(c)m=7 (d)m=8
FigureC.3:TheconstructedvaluematrixWV hassimilarlowrankapproximationwiththetrained
valuematrix.Thefiguredisplaysaveragesmallestprincipalanglesbetweenlow-rankapproximations
oftrainedvaluematricesandthoseofconstructed,randomlyconstructed,andGaussian-initialized
valuematrices.Standarderrorsareover5runs.
32(a)m=5 (b)m=6
(c)m=7 (d)m=8
Figure C.4: Intheunderparameterizedregime(d<V), freezingembeddingstoinitializations
causesasignificantdecreaseinperformance.Thegraphreportsaccuracywithdifferentembedding
dimensionsandthestandarderrorsareover5runs.Redlinesindicatewhend=V.
(a)m=7 (b)m=8
FigureC.5:TherelationshipbetweeninnerproductsofembeddingsandcorrespondingHamming
distancesoftokenscanbeapproximatedby(5.2). Thegraphdisplaystheaverageinnerproduct
betweenembeddingsoftwotokensagainstthecorrespondingHammingdistancebetweenthese
tokens.Standarderrorsareover5runs.
33(a)m=5 (b)m=6
(c)m=7 (d)m=8
FigureC.6:TherelationshipbetweeninnerproductsofembeddingsandcorrespondingHamming
distancesoftokensismostlylinearwhenthevaluematrixWV isfixedtobetheidentity.Thegraph
displaystheaverageinnerproductbetweenembeddingsoftwotokensagainstthecorresponding
Hammingdistancebetweenthesetokens.Standarderrorsareover10runs.
34(a)m=5 (b)m=6
(c)m=7 (d)m=8
FigureC.7: Theattentionpatternsshowtheunderlyingclusterstructureofthedatagenerating
process.Here,foranylatentvector,wehaveN(z∗ )={z:z 1∗=z1}\{z∗ }.Thefigureshowsattention
scoreheatmapsthatareaveragedover10runs.
(a)m=5 (b)m=6
(c)m=7 (d)m=8
FigureC.8: Theattentionpatternsshowtheunderlyingclusterstructureofthedatagenerating
process.Here,foranylatentvector,wehaveN(z∗ )={z:z 1∗=z1andz 2∗=z2}\{z∗ }.Thefigureshows
attentionscoreheatmapsthatareaveragedover10runs.
35(a)Sample1 (b)Sample2
(c)Sample3 (d)Sample4
FigureC.9: ThespectrumofembeddingmatrixWE haseigengapsbetweenthetopandbottom
eigenvalues,indicatinglowrankstructures. Thefigureshowsresultsfrom4experimentalruns.
Numberoflatentvariablemis7andtheembeddingdimensionis32.
C.4 Spectrum of embeddings
We display several plots of embedding spectra (Figure C.9, Figure C.10, Figure C.11,
FigureC.12)thatexhibiteigengapsbetweenthetopandbottomeigenvalues,suggesting
low-rankstructures.
C.5 Context hijacking in latent concept association
Inthissection,wewanttosimulatecontexthijackinginthelatentconceptassociationmodel.
Toachievethat,wefirstsampletwooutputtokens y1 (truetarget)and y2 (falsetarget)
andthengeneratecontextsx1=(t1,...,t1)andx2=(t2,...,t2)from p(x1|y1)and p(x2|y2).
1 L 1 L
Thenwemixthetwocontextswithrate p . Inotherwords,forthefinalmixedcontext
m
x=(t ,...,t ),t hasprobability1−p tobet1 and p probabilitytobet2. FigureC.13
1 L l m l m l
showsthat,asthemixingrateincreasesfrom0.0to1.0,thetrainedtransformertends
to favor predicting false targets. This mirrors the phenomenon of context hijacking in
LLMs.
C.6 On the context lengths
AsalludedinSection5.5,thememoryrecallrateiscloselyrelatedtotheKLdivergences
betweencontextconditionaldistributions. Becausecontextscontainmostlyi.i.dsamples,
longercontextsimplylargerdivergences. ThisisempiricallyverifiedinFigureC.14which
demonstratesthatlongercontextlengthscanleadtohigheraccuracy.
36(a)Sample1 (b)Sample2
(c)Sample3 (d)Sample4
FigureC.10: ThespectrumofembeddingmatrixWE haseigengapsbetweenthetopandbottom
eigenvalues,indicatinglowrankstructures. Thefigureshowsresultsfrom4experimentalruns.
Numberoflatentvariablemis7andtheembeddingdimensionis64.
(a)Sample1 (b)Sample2
(c)Sample3 (d)Sample4
FigureC.11: ThespectrumofembeddingmatrixWE haseigengapsbetweenthetopandbottom
eigenvalues,indicatinglowrankstructures. Thefigureshowsresultsfrom4experimentalruns.
Numberoflatentvariablemis8andtheembeddingdimensionis32.
37(a)Sample1 (b)Sample2
(c)Sample3 (d)Sample4
FigureC.12: ThespectrumofembeddingmatrixWE haseigengapsbetweenthetopandbottom
eigenvalues,indicatinglowrankstructures. Thefigureshowsresultsfrom4experimentalruns.
Numberoflatentvariablemis8andtheembeddingdimensionis64.
(a)m=5 (b)m=6
(c)m=7 (d)m=8
FigureC.13:Mixingcontextscancausemisclassification.Thefigurereportsaccuracyfortruetarget
andfalsetargetundervariouscontextmixingrate.Standarderrorsareover5runs.
38(a)m=5 (b)m=6
(c)m=7 (d)m=8
FigureC.14:Increasingcontextlengthscanimproveaccuracy.Thefigurereportsaccuracyacross
variouscontextlengthsanddimensions.Standarderrorsareover5runs.
39