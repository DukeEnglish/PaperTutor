Symbolic Learning Enables Self-Evolving Agents
WangchunshuZhou∗† YixinOu∗ ShengweiDing∗
LongLi JialongWu TiannanWang JiaminChen ShuaiWang
XiaohuaXu NingyuZhang HuajunChen YuchenEleanorJiang†
AIWavesInc.
{chunshu,eleanor}@aiwaves.cn
https://github.com/aiwaves-cn/agents
Abstract
TheAIcommunityhasbeenexploringapathwaytoartificialgeneralintelligence
(AGI)bydeveloping“languageagents”,whicharecomplexlargelanguagemodels
(LLMs)pipelinesinvolvingbothpromptingtechniquesandtoolusagemethods.
Whilelanguageagentshavedemonstratedimpressivecapabilitiesformanyreal-
worldtasks,afundamentallimitationofcurrentlanguageagentsresearchisthatthey
aremodel-centric,orengineering-centric. That’stosay,theprogressonprompts,
tools,andpipelinesoflanguageagentsrequiressubstantialmanualengineering
effortsfromhumanexpertsratherthanautomaticallylearningfromdata.Webelieve
thetransitionfrommodel-centric,orengineering-centric,todata-centric,i.e.,the
abilityoflanguageagentstoautonomouslylearnandevolveinenvironments,is
thekeyforthemtopossiblyachieveAGI.
Inthiswork,weintroduceagentsymboliclearning,asystematicframeworkthat
enableslanguageagentstooptimizethemselvesontheirowninadata-centricway
usingsymbolicoptimizers. Specifically,weconsideragentsassymbolicnetworks
wherelearnableweightsaredefinedbyprompts,tools,andthewaytheyarestacked
together. Agentsymboliclearningisdesignedtooptimizethesymbolicnetwork
withinlanguageagentsbymimickingtwofundamentalalgorithmsinconnectionist
learning: back-propagationandgradientdescent. Insteadofdealingwithnumeric
weights, agent symbolic learning works with natural language simulacrums of
weights,loss,andgradients. Weconductproof-of-conceptexperimentsonboth
standardbenchmarksandcomplexreal-worldtasksandshowthatagentsymbolic
learning enables language agents to update themselves after being created and
deployed in the wild, resulting in “self-evolving agents”. We demonstrate the
potential of the agent symbolic learning framework and open-source the entire
frameworktofacilitatefutureresearchondata-centricagentlearning.
1 Introduction
Recentadvancesinlargelanguagemodels[Radfordetal.,2018,2019,Brownetal.,2020,Ouyang
etal.,2022,OpenAI,2023,Touvronetal.,2023a,b]openthepossibilityofbuildinglanguageagents
thatcanautonomouslysolvecomplextasks. ThecommonpracticefordevelopingAIagentsisto
decomposecomplextasksintoLLMpipelineswherepromptsandtoolsarestackedtogether[Park
etal.,2023,Hongetal.,2023,Zhouetal.,2023b,Chenetal.,2023b,Xieetal.,2023]. Inasense,
languageagentscanbeviewedasAIsystemsthatconnectconnectionismAI(i.e.,theLLMbackbone
∗EqualContribution.
†CorrespondingAuthor.
Workinprogress.
4202
nuJ
62
]LC.sc[
1v23581.6042:viXraNeural Nets Connectionist Learning
Computation Neural Netwok Learnable
Graph Layer Weights Numeric Optimizer Gradient & Loss
<Instruciton>: <Result>...</Result>
Your task is to <Truth>...</Truth>
optimize the ... <Score>...</Score>
Agent Pipeline Node Prompts & Tools Symbolic Optimizer Language
Gradient & Loss
Agent Symbolic Learning
Figure1: Analogybetweenagentsymboliclearningandneuralnetsconnectionistlearning.
ofagents)andsymbolismAI(i.e.,thepipelineofpromptsandtools),whichpartiallyexplainstheir
effectivenessinreal-worldproblem-solvingscenarios.
However,thecurrentstateoflanguageagentsdevelopmentislimitedbytheextensiveengineering
effort required to build and customize language agent systems for a specific task. Specifically,
researchers and developers have to manually decompose complex tasks into subtasks, which we
refer to as nodes, that are more tractable for LLMs and then carefully design prompts and tools,
includingAPIfunctions,knowledgebases,memories,etc.,forspecificnodes. Thecomplexityofthis
processmakesthecurrentlandscapeoflanguageagentresearchmodel-centric,orengineering-centric.
Thismeansitisalmostimpossibleforresearcherstomanuallytuneoroptimizelanguageagents
ondatasetsonwhichwecantrainneuralnetsinadata-centricway. Thislimitstherobustnessand
versatilityofmanuallycodedlanguageagentsandrequiressubstantialengineeringefforttoadapt
languageagentstonewtasksordatadistributions. Webelievethetransitionfromengineering-centric
languageagentsdevelopmenttodata-centriclearningisanimportantstepinlanguageagentresearch.
Tothisend,anumberofrecenteffortshasbeenmadeonautomaticoptimizationoflanguageagents.
Forexample,DSpy[Khattabetal.,2023]introducesaframeworkforalgorithmicallyoptimizing
LLMpromptsviabootstrappingorrandomsearchinginacombinatoryspaceofdifferentprompt
components and GPTSwarm [Zhuge et al., 2024] further proposes to tackle the combinatorial
optimization challenge raised in DSPy via an iterative optimization process. Agent-pro [Zhang
et al., 2024b] proposes a framework to optimize the prompts components corresponding to the
agents’internalpolicyincompetitiveenvironments. AgentOptimizer[Zhangetal.,2024a]proposes
a framework to optimize functions with carefully engineered prompts. While effective in some
scenarios,theseapproachesonlyoptimizeseparatemodulesinanagentsystemsuchasaprompt
foraspecificnode. Asaresult,theseoptimizationmethodsarepronetolocaloptimumofisolated
prompts,tools,andnodesthatleadstocompromisedperformancefortheentireagentsystem. This
resemblestheearlypracticeintrainingneuralnets[HintonandSalakhutdinov,2006]wherelayers
areseparatelyoptimizedanditnowseemstrivialthatoptimizingneuralnetsasawholeleadstobetter
performance. Webelievethatthisisalsothecaseinagentoptimizationandjointlyoptimizationofall
symboliccomponentswithinanagentisthekeyforoptimizingagents.
Inthiswork,weintroduceaagentsymboliclearningframeworkfortraininglanguageagents. The
agent symbolic learning framework is inspired by the connectionist learning procedure [Hinton,
1990]usedfortrainingneuralnets. Tobespecific,wemakeananalogybetweenlanguageagents
andneuralnets: theagentpipelineofanagentcorrespondstothecomputationalgraphofaneural
net,anodeintheagentpipelinecorrespondstoalayerintheneuralnet,andthepromptsandtools
for a node correspond to the weights of a layer. In this way, we are able to implement the main
componentsofconnectionistlearning,i.e.,backwardpropagationandgradient-basedweightupdate,
inthecontextofagenttrainingusinglanguage-basedloss,gradients,andweights. Specifically,we
implementlossfunction,back-propagation,andweightoptimizerinthecontextofagenttraining
withcarefullydesignedpromptpipelines. Foratrainingexample,ourframeworkfirstconductsthe
“forwardpass”(agentexecution)andstorestheinput,output,prompts,andtoolusageineachnode
ina“trajectory”. Wethenuseaprompt-basedlossfunctiontoevaluatetheoutcome,resultingina
2“languageloss”. Afterward,weback-propagatethelanguagelossfromthelasttothefirstnodealong
thetrajectory,resultingintextualanalysesandreflectionsforthesymboliccomponentswithineach
node,wecallthemlanguagegradients. Finally,weupdateallsymboliccomponentsineachnode,
aswellasthecomputationalgraphconsistingofthenodesandtheirconnections,accordingtothe
languagegradientswithanothercarefullydesignedprompt. Ourapproachalsonaturallysupports
optimizingmulti-agentsystemsbyconsideringnodesasdifferentagentsorallowingmultipleagents
totakeactionsinonenode.
Theagentsymboliclearningframeworkisanagentlearningframeworkthatmimicsthestandard
connectionistlearningprocedure. Incontrasttoexistingmethodsthateitheroptimizesingleprompt
ortoolinaseparatemanner,theagentsymboliclearningframeworkjointlyoptimizesallsymbolic
components within an agent system, including prompts, tools, and the pipeline that stacks them
intoanagentsystem. Thistop-downoptimizationschemealsoenablestheagentsymboliclearning
frameworktooptimizetheagentsystem“holistically”,avoidinglocaloptimumforeachseparated
component. Thismakesitpossibleforlanguageagentstargetingcomplexreal-worldproblemsto
effectivelylearnfromdata,openingupthepossibilitytotransformthecurrentstateoflanguageagent
researchfromengineering-centrictodata-centric. Moreover,sincethelanguage-basedlossfunction
doesnotrequireground-truthwhengeneratingthelanguageloss,ourframeworkenableslanguage
agentstolearnfromexperienceanddelibratelyupdatealltheirsymboliccomponentsafterbeing
createdanddeployedinthewild,enabling“self-evolvingagents”3.
Asaproof-of-concept,weconductaseriesofexperimentsonbothstandardLLMbenchmarksand
complexagentictasks. Ourresultsdemonstratetheeffectivenessoftheproposedagentsymbolic
learningframeworktooptimizeanddesignpromptsandtools,aswellasupdatetheoverallagent
pipelinebylearningfromtrainingdata. Weopen-sourceallcodesandpromptsintheagentsymbolic
learningframeworktofacilitatefutureresearchondata-centricagentlearning.
2 RelatedWork
2.1 LanguageModels,Prompts,andLanguageAgents
Languagemodelisafamilyofmachinelearningmodelthatistrainedtoevaluatetheprobability
of sequences of words or tokens. Large language models (LLMs) [Radford et al., 2018, 2019,
Brown et al., 2020, Ouyang et al., 2022, OpenAI, 2023, Touvron et al., 2023a,b] often refer to
languagemodelsthatadopttheautoregressiveprobabilityfactorizationscheme, parametrizedby
theTransformerarchitecture[Vaswanietal.,2017],consistsofalargeamountofparameters,and
trainedonlarge-scalecorpus. Withscalingofmodelsize,trainingdata,andcomputation,LLMshave
demonstratedremarkablecapabilitiesingeneratinghuman-liketextsandunderstandingcontext.
Prompts,ontheotherhand,isthekeyforunleashingthecapabilitesofLLMs. Promptsarecritical
componentsincontrollingthebehaviorandoutputofLLMsandserveastheinterfacebetweenhuman
andLLMs. Thedesignofpromptssignificantlyimpactstheperformanceoflanguagemodelsanda
numberofprogresshavebeenmadeonpromptengineering,includingin-contextlearning[Brown
etal.,2020],chain-of-thoughtprompting[Nyeetal.,2022,Weietal.,2022],ReAct[Yaoetal.,2022],
self-refine[Madaanetal.,2023],self-consistency[Wangetal.,2023],recurrentprompting[Zhou
etal.,2023a],etc.
Languageagentsfurtherextendthefunctionalityoflanguagemodelsbeyondsimplepromptingby
allowingLLMstousetools[Schicketal.,2023]andintegratingLLMsintobroadersystemscapable
ofexecutingmulti-steptasks[Parketal.,2023,Hongetal.,2023,Zhouetal.,2023b,Chenetal.,
2023b,Xieetal.,2023]. Bystackingpromptsandtoolsintocarefullydesignedpipeline,agentsare
versatileinvariousapplications,fromcustomerserviceautomationtoadvanceddataanalysis.
2.2 FromAutomatedPromptEngineeringtoAgentOptimization
Withtheincreasingpopularityofpromptengineeringinbothacademicandindustry,anumberof
recentworkinvestigatedmethodstoautomatethepromptengineeringprocess. Forexample,Pryzant
etal.[2020]andYangetal.[2024]usescarefullydesignedpromptstounleashLLMs’abilitytodo
3AgentscanalsocollecttrainingdatainthewildandupdatetheLLMbackboneviafine-tuning.Inthisway,
allcomponentsintheagentcanbeupdated.Weleavethisforfuturework.
3promptengineeringforthemselves. Ontheotherhand,Prasadetal.[2023]andGuoetal.[2024]
employsdifferentsearchalgorithmssuchasgeneticalgorithmsforpromptoptimization.
Since prompts are critical components of agents, the success of automated prompt engineering
opensupthepossibilityofautomatedagentoptimization. Similartothecaseinautomatedprompt
engineering,methodsforagentoptimizationcanalsobecategorizedintotwocategories: prompt-
basedandsearch-based. Forexample,Agent-pro[Zhangetal.,2024b]andAgentOptimizer[Zhang
etal.,2024a]leveragecarefullydesignedpromptstooptimizeeitherthepromptsorthetoolsinanode
oftheagentpipeline. Thesemethodsworkonisolatedcomponentswithinanagent. Anotherlineof
researchexploredsearch-basedagentoptimizationalgorithms. Sordonietal.[2023]usesvariational
inferencetooptimizestackedLLMs. DSpy[Khattabetal.,2023]usessearchalgorithmstofindthe
bestpromptsornodesinacombinatoryspace. GPTSwarm[Zhugeetal.,2024]furtherimproved
thesearchalgorithmforthecombinatoryoptimizationproblem. Theseapproacheshaveafewmajor
limitations. First,thesearchalgorithmmainlyworkswhenthemetriccanbedefinednumerically
withequationsthatcanbecoded. However,mostagentictasksarereal-worldcomplexproblemsof
whichthesuccesscannotbedefinedbysomeequations,suchassoftwaredevelopmentorcreative
writing. Second,theseapproachesupdateeachcomponentseparatelyandthereforesufferfromthe
localoptimumofeachnodeorcomponent. Theseapproachesalsolackthefunctionalityofadding
nodesinthepipelineorimplementingnewtools. Ourproposedagentsymboliclearningframework,
ontheotherhand,isthefirstagentlearningmethodthatoptimizetheagentsystem“holistically”and
isabletooptimizeprompts,tools,nodes,aswellasthewaytheyarestackedintoagents.
Furthermore,anumberofrecenteffortshavebeendoneonsynthesizingdatatofine-tunetheLLM
backboneofanagent[Chenetal.,2023a,Qiaoetal.,2024,Songetal.,2024]. Thislineofresearch
isorthogonaltoourworkandwebelievetheycanbecomplementarytoeachother. ICE[Qianetal.,
2024]isalsoarelatedworkinvestigatinginter-tasktransferlearningforlanguageagents,whichcan
becomplementarywithourmethodforbuildingself-evolvingagents.
3 AgentSymbolicLearning
Algorithm1AgentSymbolicLearningFramework
Require: I ▷Inputtotheagentsystem
Require: A ▷Agentpipelinewithnodes
Require: G ▷Prompt-basedgradientpropagationfunction
Require: L ▷Prompt-basedlossfunction
Ensure: Updatedsymboliccomponentsintheagentsystem
1: τ ←[] ▷Initializetrajectory
2: ForwardPass
3: foreachN ∈Ado
4: I n ←GetinputforN ▷Inputtothenode
5: O n ←N(I n,P n,T n) ▷Outputfromthenode
6: Append(I n,O n,P n,T n)toτ
7: endfor
8: LossComputation
9: L ←L(τ) ▷Computelanguageloss
lang
10: Back-propagation
11: foreachN ∈reverse(A)do
12: ∇n
lang
←G(∇n la+ ng1,I n,O n,P n,T n,L lang) ▷∇n la+ ng1 =∅forthelastnode
13: Append∇n toτ
lang
14: endfor
15: WeightUpdate
16: foreachN ∈Ado
17: UpdateP n,T nusing∇n
lang
▷Updatepromptsandtools
18: endfor
19: UpdateAusing{∇n } ▷Updatetheagentpipeline
lang
20: return(A,P,T) ▷Updatedagentsystem
4Initialized 2 Next node is selected by transition controller of node Forward Pass
2 2 2
Language
1 1 1 Loss
F
r
e
1 Next agent is selected by routing controller of node e z
e
Task Tools LLM Trajectory Nodet ⊕ Agentt ⊕ Actiont ⊕ Environmentt
Pipeline Agent Frozen
Prompt Node Updated State 1 State2 Statet-1 Statet
Back Propagation of Language Gradients
Optimized Prompt Optimizer ry
o
t
Pipeline Optimizer
GL ra an dg iu ea ng te
s
ajec
r
T
Tool Optimizer
Figure2: Illustrationoftheagentsymboliclearningframework.
3.1 ProblemFormulation
Wefirstformulatetheagentsymboliclearningframeworkbydrawinganalogiestothecomponents
andproceduresusedinneuralnetworktraining. Wedefinethekeycomponentsoftheframeworkand
explainthenotationsusedthroughoutthissection.
Theagentsymboliclearningframework,asillustratedinFigure2,isinspiredbytheconnectionist
learningproceduresusedfortrainingneuralnets[Hinton,1990]. Wefirstintroducethenotationsfor
keyconceptsbymakinganalogiestothatintheconnectionistlearningframework:
• AgentPipelineA: Similartothecomputationalgraphinneuralnetsthatrepresentsthe
structureoflayersandtheirconnections,agentpipelinerepresentsthesequenceofnodes(or
steps)throughwhichtheagentprocessesinputdata.Asequenceofnodes{N ,N ,...,N }
1 2 n
thatprocesstheinputdatathroughvariousstages. Notethatinsomeagentframeworks,the
agentpipelineisinput-dependentsincethenodesaredynamicallyassignedduringexecution,
whichissimilartothecaseofdynamicneuralnets.
• NodeN: Anindividualstepwithinanagentpipeline. Theroleofanodeinanagentis
similar to a layer in a neural network. A node N receives Node Input I , which are
n n
alsoinnaturallanguageform. Ingeneral,theinputforanodeconsistsoftheoutputofthe
previousnodeand(optionally)inputsfromtheenvironment(e.g.,humaninput). Thenode
N processestheinputI withanLLMusingbothpromptsP andtoolsT 4. Theoutput
n n n n
O isinnaturallanguageandpassedtothenextnode.
n
• Trajectoryτ: Similartotheroleofcomputationalgraphofneuralnets,thetrajectorystores
allinformationduringtheforwardpass,includingtheinputs,outputs,prompts,andtools
usageforeachnode,andisresponsibleforgradientback-propagation.
• LanguageLossL : Languagelossintheagentsymboliclearningframeworkissimilarto
lang
thelossinneuralnetworkssincetheybothmeasurethediscrepancybetweentheexpected
andactualoutcomes. Themaindifferenceisthatthelanguagelossisintextualformandis
producedbyanaturallanguagelossfunctionimplementedbyacarefullydesignedprompt
whileconventionallossesarefloatnumberscomputedwithlossfunctionsthatarenumerical
equations.
• Language Gradient ∇ : Similar to the role of gradients in connectionist learning,
lang
languagegradientsaretextualanalysesandreflectionsusedforupdatingeachcomponentin
theagentwithrespecttothelanguageloss.
4T consistsoftheinputandoutputfortoolusage,andtheimplementationofthetoolitself.
n
53.2 AgentSymbolicLearningProcedure
Afterdefiningthekeycomponents,wecansummarizetheworkflowoftheagentsymboliclearning
frameworkinAlgorithm1. Inthissection, wedescribeeachstepintheagentsymboliclearning
frameworkindetail.
ForwardPass Theforwardpassisalmostidenticaltostandardagentexecution. Themaindif-
ference is that we store the input, prompts, tool usage, and the output to the trajectory, which is
usedforlanguagegradientback-propagation. Thisissimilartodeeplearningframeworkssuchas
PyTorch[Paszkeetal.,2019]andTensorFlow[Abadietal.,2016]thatstoretheintermediateoutputs
andactivationinthecomputationgraphoftheneuralnetwork.
LanguageLossComputation Aftertheforwardpass,wecomputethelanguagelossforatraining
examplebyfeedingthetrajectoryintoanLLMusingacarefullydesignedprompttemplateP :
loss
L =LLM(P (τ)) (1)
lang loss
Thekeyisthedesignfortheprompttemplate,whichisexpectedtoholisticallyevaluatehowtheagent
performswithrespecttotheinput,environment,andtaskrequirements. Tothisend,wecarefully
designaprompttemplateforlanguagelosscomputationconsistingofthefollowingcomponents: task
description,input,trajectory,few-shotdemonstrations,principles,andoutputformatcontrol. Among
them,taskdescription,input,andtrajectoryaredata-dependentwhilethefew-shotdemonstrations,
principles,andoutputformatcontrolarefixedforalltasksandtrainingexamples. Thelanguageloss
consistsofbothnaturallanguagecommentsandanumericalscore(alsogeneratedviaprompting).
Wecanoptionallyfeedtheground-truthlabelfortheinputwhengeneratingthelanguageloss. We
callthisscenariosupervisedagentlearning. Itcanalsogeneratelanguagelosswithoutground-truth
byevaluatingtheoutputandtrajectoryaccordingtothetaskdescription. Inthiscase,wecansaythat
theagentisdoingunsupervisedagentlearning,whichenableslanguageagentstoself-evolving. We
presentthedetailedimplementationofthisprompttemplateintheAppendix.
Back-propagationofLanguageGradients Instandardconnectionistlearning,thegoalofgradient
back-propagationistocalculatetheimpactoftheweightswithrespecttotheoveralllosssothat
theoptimizerscanupdatetheweightsaccordingly. Similarly,inourframework,wealsodesigna
“back-propagation”algorithmforlanguagegradients. Specifically,weiteratefromthelastnodetothe
firstnodeandcomputethegradientforeachnodewithLLMsusingacarefullydesignedprompt:
∇n =LLM(P (∇n+1,I ,O ,P ,T ,L )) (2)
lang gradient lang n n n n lang
TheprompttemplateP isdesignedtoinstructtheLLMtogeneratelanguagegradientsthat
gradient
areanalysesandreflectionsforthesymboliccomponentswithinthenode. Inspiredbytheideaof
back-propagation,wegivethelanguagegradientsofthenodeexecutedafterthecurrentnode,aswell
astheinformationontheexecutionofthecurrentnode,whichisstoredinthetrajectory. That’sto
say,whendoinganalysisandreflection,theLLMnotonlyneedstoconsiderhowthepromptsand
toolssuitthesubgoalofthecurrentnodebutalsohastoconsiderhowtheyaffecttheaccomplishment
ofthesubgoalofthenextnode. Bychainingfromtoptobottom,thelanguagegradientsforallnodes
arerelevantandresponsiblefortheoverallsuccessoftheagent. Thismethodeffectivelyreducesthe
riskofoptimizingtowardthelocaloptimumforeachisolatedpromptandtool,leadingtotheoverall
performanceofagentsystems.
Language Gradient-based Update The final step in the framework is to update the prompts
and tools in each node and optimize the overall agent pipeline with the help of language gradi-
ents. Thisisaccomplishedvia“symbolicoptimizers”. Symbolicoptimizersarecarefullydesigned
prompt pipelines that can optimize the symbolic weights of an agent. We create three types of
symbolicoptimizers: PromptOptimizer,ToolOptimizer,andPipelineOptimizer. Wepresentdetailed
implementationofthesepromptsintheAppendix.
PromptOptimizer: Tofacilitatepromptoptimization,wesplitpromptsintodifferentcomponents,
includingtaskdescription,few-shotexamples,principles,andoutputformatcontrol. Wethendesign
separate prompts tailored for the optimization of each prompt component. All prompts share a
detailedexplanationanddemonstrationofhowtheLLMshouldfocusonthelanguagegradients
whenreasoningabouthowtoedittheoriginalpromptcomponents.
6ToolOptimizer: TheToolOptimizerisapipelineofpromptsthatfirstinstructstheLLMtodecidethe
kindofoperationitshoulduse: whetherthetoolsshouldbeimproved(byeditingthetooldescription
usedforfunctioncalling),deleted,ornewtoolsneedtoimplement. ThentheToolOptimizercalls
differentpromptsspecificallydesignedfortoolediting,deletion,andcreation.
PipelineOptimizer: ThegoalofthePipelineOptimizeristooptimizertheagentpipelineconsisting
ofnodesandtheirconnections. Thepromptisdesignedtofirstintroducetheagentprogramming
language used to define the agent pipeline (we use the agent programming language introduced
inZhouetal.[2023b]). Thenthepromptdescribesthedefinitionofafewatomicoperationsthat
theLLMcanusetoupdatethepipeline,includingadding,deleting,andmovingthenodes. Itthen
instructstheLLMtofirstanalyzehowthepipelinecouldbeimprovedandthenimplementtheupdate
usingtheatomicoperations. Detaileddescriptionsoftheagentprogramminglanguageandtheatomic
operationsusedtoupdatetheagentpipelineareavailableintheAppendix.
Since all aforementioned optimizers operate in natural language space and some optimization
operationsneedtobedoneincodespace,weuseasimplestrategythatretriesanyillegalupdate
uptothreetimesanddiscardstheupdateiftheerrorpersists. Wealsousearollbackstrategythat
re-runsthecurrentexampleafteroptimizationandrollsbacktotheoriginalagentiftheperformance
evaluatedusingthelanguage-basedlossfunctiondrops. Furthermore,wealsoincludea“learning
rate”componentforeachpromptsintheoptimizerswhichcontrolshowaggressivetheLLMshould
bewhenoptimizingprompts,tools,andagentpipelines.
BatchedTraining Theaforementionedoptimizationschemeworkswithonetrainingexampleata
time,whichresemblesstochasticgradientdescent. Inspiredbythefactthatmini-batchstochastic
gradientdescentworksbetter,ormorestably,inpractice,wealsodeviseabatchedtrainingvariantfor
symbolicoptimizers. Specifically,weconductforwardpass,losscomputation,andback-propagation
foreachexampleseparately. Thenwefeedabatchoflanguagegradientsforthesamenode,and
prompttheLLMtoholisticallyconsideralltheselanguagegradientswhenupdatingtheagent.
4 Experiments
4.1 Settings
4.1.1 Tasks
WeconductexperimentsonbothstandardLLMbenchmarksandmorecomplexagentictasks. We
describethetasks,datasets,andevaluationmetricsasfollows:
Table1: ResultsonStandardLLMBenchmarks.
Methods HotPotQA MATH HumanEval
GPT-3.5 GPT-4 GPT-3.5 GPT-4 GPT-3.5 GPT-4
GPTs 24/38.8 33/44.3 23.2 53.1 59.2 71.7
Agents 27/37.5 39/49.8 23.8 56.0 59.5 85.0
Agentsw/AutoPE 29/39.8 38/50.3 22.5 57.2 63.5 82.3
DSPy 35/43.9 40/50.5 17.3 48.4 66.7 77.3
Ours 35/44.8 41/54.0 38.8 60.7 64.5 85.8
StandardBenchmarks WeconductexperimentsonstandardbenchmarksforLLMsincluding
HotpotQA[Yangetal.,2018],MATH[Hendrycksetal.,2021],andHumanEval[Chenetal.,2021].
HotPotQAisamulti-hopQAtaskchallengingforrichbackgroundknowledge. Weusethe“hard”
splitinthedatasetsincewefindittobemorechallengingforlanguageagents. MATHisacollection
ofchallengingcompetitionmathematicsproblems. HumanEvalisanevaluationsetthatrequires
LLMsoragentstosynthesizeprogramsfromdocstrings. Asforevaluationmetrics,weuseF1and
exactmatchforHotPotQA,accuracyforMATH,andPass@1forHumanEval. Toolsaredisabledin
thesedatasetstoensuretheresultscomparisonismeaningfulwithexistingliteratureonthesetasks.
ComplexAgentTasks Weconsidercreativewritingandsoftwaredevelopmentastwocomplex
agentictasks. Forthecreativewritingtask,wefollowYaoetal.[2023]andgive4randomsentences
7to the agents and ask them to write a coherent passage with 4 paragraphs that end in the 4 input
sentencesrespectively. Suchataskisopen-endedandexploratory,andchallengescreativethinking
aswellashigh-levelplanning. WeuseGPT-4scoretoevaluatethepassagesfollowing[Yaoetal.,
2023]. Thesoftwaredevelopmenttask,ontheotherhand,requirestheagentsystemtodevelopan
executablesoftwaregivenasimpleproductrequirementdocument(PRD).Weevaluatethecompared
agentsaccordingtotheexecutabilityofthegeneratedsoftware,whichisquantifiedbynumerical
scoresrangingfrom1to4,correspondingtoincreasinglevelsofexecutioncapability. Specifically,a
scoreof1signifiesexecutionfailure,2denotessuccessfulcodeexecution,3representsconformance
totheanticipatedworkflow,and4indicatesflawlessalignmentwithexpectations.
4.1.2 Baselines
Wecompareourproposedmethodagainstthefollowingbaselines:
• GPTs: asimplebaselinethatuseGPTandacarefullydesignedprompt;
• Agents: a language agent method implemented using the Agents [Zhou et al., 2023b]
framework5withacarefullydesignedprompts,tools,andpipeline;
• DSpy: aLLMpipelineoptimizationframeworkthatcansearchthebestcombinationof
promptcomponents. Itisnotapplicableforcomplexagenttaskswheretheevaluationmetric
cannotbedefinedinequationandcode;
• Agents+AutoPE:avariantwherethepromptineachnodeoftheagentpipelineisoptimized
by an LLM following the method described in Yang et al. [2024]. It does not involve
languagegradientback-propagationandlanguagegradient-basedoptimization.
WereportresultswithbothGPT-3.5andGPT-4. Weusethegpt-3.5-turbo-0125endpointfor
GPT-3.5andthegpt-4-turbo-0409endpointforGPT-4. Asforourapproach,westartwiththe
Agentsbaselineandthenconductagentsymboliclearningontopofit.
4.2 Results
Table2: Resultsonsoftwaredevelopment.
Table3: Resultsoncreativewriting.
Task GPTs Agents Ours
Methods GPT-3.5 GPT-4
Flappybird 2 2 3
GPTs 4.0 6.0
Tankbattlegame 1 2 4
Agents 4.2 6.0
2048game 1 2 4
Agentsw/AutoPE 4.4 6.5
Snakegame 2 3 4
ToT 3.8 6.8
Brickbreakergame 2 3 4
Ours 6.9 7.4
Averagescore 1.6 2.4 3.8
Results on LLM Benchmarks The results on standard LLM benchmarks are shown in Table
1. We can see that the proposed agent symbolic learning framework consistently improves over
allcomparedmethods. TheperformanceimprovementonMATH,acompetition-levelbenchmark,
isespeciallylarge. Incontrast,conventionalLLM-basedpromptoptimizationmethod(Agentsw/
AutoPE)andthesearch-basedpromptoptimizationapproach(DSPy)arenotasstable: theyresults
ingoodperformanceimprovementsinsomecasesbutleadstosignificantperformancedegrationin
someothercases. Thissuggeststhattheagentsymboliclearningframeworkismorerobustandcan
optimizetheoverallperformanceoflanguageagentsmoreeffectively.
ResultsonComplexTasks Wepresenttheresultsonsoftwaredevelopmentandcreativewriting
inTable2&3,respectively. Wecanseethatourapproachsignificantlyoutperformsallcompared
baselinesonbothtaskswithaevenlargerperformancegapcomparedtothatonconventionalLLM
benchmarks. Interestingly,ourapproachevenoutperformstree-of-throught,acarefullydesigned
promptengineeringandinferencealgorithm,onthecreativewritingtask. Wefindthatourapproach
successfullyfindstheplan, write, andrevisionpipelineandthepromptsareverywelloptimized
in each step. We also find that the agent symbolic learning framework recovers similar standard
operationproceduredevelopedinMetaGPT[Hongetal.,2023],anagentframeworkspecifically
5WehavetestedwithotheragentframeworkssuchasOpenAgentsandAgentVerseandgotsimilarresults.
8Node Node
Write Check
Node
Write
Agent1: writer
Agent1: writer Optimize Description: Write a fluent passage based on the questions given.
Description: Write a fluent passage Ensure that each paragraph concludes with the specified sentence
based on the questions given, in which to reinforce key ideas and provide a smooth narrative flow.
the closing part of each paragraph
ends with the sentence given +Agent2: editor
+Description: Review the passage written by 'writer', correcting
grammatical errors, improving logical flow, and enhancing
readability. The final result should be free of major errors and
optimized for fluency.
Figure3: Ancasestudyconductedoncreativewritingtask.
designedforsoftwaredevelopment. Thisconfirmstheeffectivenessoftheproposedagentsymbolic
learningframeworkonreal-worldtaskswherethereisnogroundtruthandtheoverallperformance
cannotbecalculatedbyequationsorcodes,ascontrarytosearch-basedalgorithmssuchasDSPy.
4.3 CaseStudy&Analysis
Wethenshowacasestudyfortheoptimizationdynamicsoftheagentsymboliclearningframework
inFigure3. Wecanseethatourapproachcaneffectivelydopromptengineeringanddesigningthe
agentpipelineinthewayahumanexpertdevelopslanguageagents.
Moreover,wefindthattheinitializationoftheagentsystemhasnon-negligibleimpactsonthefinal
performance,justastheinitializationofaneuralnetsisimportantfortraining. Ingeneral,wefind
thatitisgenerallyhelpfultoinitializetheagentinthesimplestwayandletthesymbolicoptimizersto
dotheoptimization. Incontrast,theperformancetendstobecomeunstableiftheinitialagentsystem
isover-engineered. Anaturalextensionofthisobservationisthatmaybewecandosomekindof
pre-trainingonlarge-scaleanddiversetasksasaversatileinitializationforgeneral-purposeagents
andthenadaptittospecializedtaskswithagentsymboliclearning. Wealsofindthatthesuccessof
ourapproachismoresignificantandstableoncomplexreal-worldtaskscomparedtothatonstandard
benchmarkswheretheperformanceisevaluatedbyaccuracyorF1. Thissuggeststhatfutureresearch
onagentlearningshouldfocusmoreonreal-worldtasks,andtheagentresearchcommunityshould
workonbuildingabenchmarkfocusingonagentlearningevaluationthatconsistsofdiversecomplex
agentictasksandinvestigatingrobustapproachestomeasureprogress.
5 Conclusion
Thispaperintroducesagentsymboliclearning,aframeworkforagentlearningthatjointlyoptimizes
allsymboliccomponentswithinanagentsystem. Theagentsymboliclearningframeworkdraws
inspirationfromstandardconnectionistlearningproceduretodosymboliclearning. Ituseslanguage-
basedloss,gradients,andoptimizerstooptimizeprompts,tools,andtheagentpipelinewithrespect
totheoverallperformanceoftheagentsystem. Theproposedframeworkisamongthefirstattempts
to optimize agents that can solve complex real-world tasks using sophisticated pipelines. Our
frameworks enables language agents to “learn from data” and perform “self-evolve” after being
createdanddeployedinthewild. Weconductseveralproof-of-conceptexperimentsandshowthatthe
agentsymboliclearningframeworkcaneffectivelyoptimizeagentsacrossdifferenttaskcomplexity.
We believe this transition from model-centric to data-centric agent research is a meaningful step
towardsapproachingartificialgeneralintelligenceandopen-sourcethecodesandpromptsforthe
agentsymboliclearningframeworktoacceleratethistransition.
References
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. {TensorFlow}: a system for
{Large-Scale}machinelearning. In12thUSENIXsymposiumonoperatingsystemsdesignand
implementation(OSDI16),pages265–283,2016.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
9Herbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,
JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,
BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,
and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,
R.Hadsell,M.F.Balcan,andH.Lin,editors,AdvancesinNeuralInformationProcessingSystems,
volume 33, pages 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao.
Fireact: Towardlanguageagentfine-tuning,2023a.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared
Kaplan,HarrisonEdwards,YuriBurda,NicholasJoseph,GregBrockman,AlexRay,RaulPuri,
GretchenKrueger,MichaelPetrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,BrookeChan,
ScottGray,NickRyder,MikhailPavlov,AletheaPower,LukaszKaiser,MohammadBavarian,
ClemensWinter,PhilippeTillet,FelipePetroskiSuch,DaveCummings,MatthiasPlappert,Fotios
Chantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamHebgenGuss,AlexNichol,AlexPaino,
Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
ChristopherHesse,AndrewN.Carr,JanLeike,JoshuaAchiam,VedantMisra,EvanMorikawa,
AlecRadford,MatthewKnight,MilesBrundage,MiraMurati,KatieMayer,PeterWelinder,Bob
McGrew,DarioAmodei,SamMcCandlish,IlyaSutskever,andWojciechZaremba. Evaluating
largelanguagemodelstrainedoncode. CoRR,abs/2107.03374,2021. URLhttps://arxiv.
org/abs/2107.03374.
WeizeChen,YushengSu,JingweiZuo,ChengYang,ChenfeiYuan,ChenQian,Chi-MinChan,Yujia
Qin,YaxiLu,RuobingXie,etal. Agentverse: Facilitatingmulti-agentcollaborationandexploring
emergentbehaviorsinagents. arXivpreprintarXiv:2308.10848,2023b.
QingyanGuo, RuiWang, JunliangGuo, BeiLi, KaitaoSong, XuTan, GuoqingLiu, JiangBian,
andYujiuYang. Connectinglargelanguagemodelswithevolutionaryalgorithmsyieldspowerful
promptoptimizers. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
URLhttps://openreview.net/forum?id=ZG3RaNIsO8.
DanHendrycks,CollinBurns,SauravKadavath,AkulArora,StevenBasart,EricTang,DawnSong,
andJacobSteinhardt. Measuringmathematicalproblemsolvingwiththemathdataset. NeurIPS,
2021.
GeoffreyEHinton.Connectionistlearningprocedures.InMachinelearning,pages555–610.Elsevier,
1990.
GeoffreyEHintonandRuslanRSalakhutdinov. Reducingthedimensionalityofdatawithneural
networks. science,313(5786):504–507,2006.
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jin-
lin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng
Xiao, ChenglinWu, andJürgenSchmidhuber. Metagpt: Metaprogrammingforamulti-agent
collaborativeframework,2023.
OmarKhattab,ArnavSinghvi,ParidhiMaheshwari,ZhiyuanZhang,KeshavSanthanam,SriVard-
hamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller,
MateiZaharia,andChristopherPotts. Dspy: Compilingdeclarativelanguagemodelcallsinto
self-improvingpipelines. arXivpreprintarXiv:2310.03714,2023.
AmanMadaan,NiketTandon,PrakharGupta,SkylerHallinan,LuyuGao,SarahWiegreffe,UriAlon,
NouhaDziri,ShrimaiPrabhumoye,YimingYang,SeanWelleck,BodhisattwaPrasadMajumder,
Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with
self-feedback,2023.
MaxwellNye,AndersJohanAndreassen,GuyGur-Ari,HenrykMichalewski,JacobAustin,David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and
Augustus Odena. Show your work: Scratchpads for intermediate computation with language
models,2022. URLhttps://openreview.net/forum?id=iedYJm92o0a.
10OpenAI. GPT-4technicalreport,2023.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexGray,JohnSchulman,JacobHilton,FraserKelton,
LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulChristiano,JanLeike,and
RyanLowe.Traininglanguagemodelstofollowinstructionswithhumanfeedback.InAliceH.Oh,
AlekhAgarwal,DanielleBelgrave,andKyunghyunCho,editors,AdvancesinNeuralInformation
ProcessingSystems,2022. URLhttps://openreview.net/forum?id=TG8KACxEON.
Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and
MichaelS.Bernstein. Generativeagents: Interactivesimulacraofhumanbehavior,2023.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performancedeeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,
2019.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. GrIPS: Gradient-free, edit-based
instruction search for prompting large language models. In Andreas Vlachos and Isabelle
Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 3845–3864, Dubrovnik, Croatia, May 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.277. URL
https://aclanthology.org/2023.eacl-main.277.
Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, and Diyi
Yang. Automaticallyneutralizingsubjectivebiasintext. InTheThirty-FourthAAAIConference
on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial
Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in
Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 480–489.
AAAIPress,2020. doi: 10.1609/AAAI.V34I01.5385. URLhttps://doi.org/10.1609/aaai.
v34i01.5385.
Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin, Yesai Wu, Zhiyuan
Liu,andMaosongSun. Investigate-consolidate-exploit: Ageneralstrategyforinter-taskagent
self-evolution,2024.
ShuofeiQiao,NingyuZhang,RunnanFang,YujieLuo,WangchunshuZhou,YuchenEleanorJiang,
Chengfei Lv, and Huajun Chen. AUTOACT: automatic agent learning from scratch via self-
planning. CoRR, abs/2401.05268, 2024. doi: 10.48550/ARXIV.2401.05268. URL https:
//doi.org/10.48550/arXiv.2401.05268.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understandingbygenerativepre-training. 2018.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
TimoSchick,JaneDwivedi-Yu,RobertoDessi,RobertaRaileanu,MariaLomeli,EricHambro,Luke
Zettlemoyer,NicolaCancedda,andThomasScialom. Toolformer: Languagemodelscanteach
themselvestousetools. InThirty-seventhConferenceonNeuralInformationProcessingSystems,
2023. URLhttps://openreview.net/forum?id=Yacmpz84TH.
Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error:
Exploration-basedtrajectoryoptimizationforLLMagents. CoRR,abs/2403.02502,2024. doi:
10.48550/ARXIV.2403.02502. URLhttps://doi.org/10.48550/arXiv.2403.02502.
AlessandroSordoni,XingdiYuan,Marc-AlexandreCôté,MatheusPereira,AdamTrischler,Ziang
Xiao,ArianHosseini,FriederikeNiedtner,andNicolasLeRoux. Jointpromptoptimizationof
stackedLLMsusingvariationalinference. InThirty-seventhConferenceonNeuralInformation
ProcessingSystems,2023. URLhttps://openreview.net/forum?id=iImnbUVhok.
11HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,Armand
Joulin,EdouardGrave,andGuillaumeLample. Llama: Openandefficientfoundationlanguage
models,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,Cris-
tianCantonFerrer, MoyaChen, GuillemCucurull, DavidEsiobu, JudeFernandes, JeremyFu,
WenyinFu, BrianFuller, CynthiaGao, VedanujGoswami, NamanGoyal, AnthonyHartshorn,
SagharHosseini,RuiHou,HakanInan,MarcinKardas,ViktorKerkez,MadianKhabsa,Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,
DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,PushkarMishra,
IgorMolybog,YixinNie, AndrewPoulton, JeremyReizenstein, RashiRungta, KalyanSaladi,
AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,Binh
Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
SergeyEdunov, andThomasScialom. Llama2: Openfoundationandfine-tunedchatmodels,
2023b.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessing
systems,30,2017.
XuezhiWang,JasonWei,DaleSchuurmans,QuocVLe,EdH.Chi,SharanNarang,Aakanksha
Chowdhery,andDennyZhou. Self-consistencyimproveschainofthoughtreasoninginlanguage
models. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=1PL1NIMMrw.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
neuralinformationprocessingsystems,35:24824–24837,2022.
TianbaoXie,FanZhou,ZhoujunCheng,PengShi,LuoxuanWeng,YitaoLiu,TohJingHua,Junning
Zhao,QianLiu,CheLiu,LeoZ.Liu,YihengXu,HongjinSu,DongchanShin,CaimingXiong,
andTaoYu. Openagents: Anopenplatformforlanguageagentsinthewild,2023.
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun
Chen. Largelanguagemodelsasoptimizers. InTheTwelfthInternationalConferenceonLearning
Representations,2024. URLhttps://openreview.net/forum?id=Bb4VGOWELI.
ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamCohen,RuslanSalakhutdinov,and
ChristopherD.Manning. HotpotQA:Adatasetfordiverse,explainablemulti-hopquestionanswer-
ing. InEllenRiloff,DavidChiang,JuliaHockenmaier,andJun’ichiTsujii,editors,Proceedingsof
the2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2369–2380,
Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1259. URLhttps://aclanthology.org/D18-1259.
ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao.
React: Synergizingreasoningandactinginlanguagemodels. arXivpreprintarXiv:2210.03629,
2022.
ShunyuYao, DianYu, JeffreyZhao, IzhakShafran, ThomasL.Griffiths, YuanCao, andKarthik
Narasimhan. Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels,2023.
ShaokunZhang,JieyuZhang,JialeLiu,LinxinSong,ChiWang,RanjayKrishna,andQingyunWu.
Offlinetrainingoflanguagemodelagentswithfunctionsaslearnableweights,2024a.
WenqiZhang,KeTang,HaiWu,MengnaWang,YongliangShen,GuiyangHou,ZeqiTan,PengLi,
YuetingZhuang,andWeimingLu. Agent-pro: Learningtoevolveviapolicy-levelreflectionand
optimization,2024b.
WangchunshuZhou,YuchenEleanorJiang,PengCui,TiannanWang,ZhenxinXiao,YifanHou,
RyanCotterell,andMrinmayaSachan. Recurrentgpt: Interactivegenerationof(arbitrarily)long
text,2023a.
12WangchunshuZhou,YuchenEleanorJiang,LongLi,JialongWu,TiannanWang,ShiQiu,Jintian
Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu
Zhang,HuajunChen,PengCui,andMrinmayaSachan. Agents: Anopen-sourceframeworkfor
autonomouslanguageagents,2023b.
MingchenZhuge, WenyiWang, LouisKirsch, FrancescoFaccio, DmitriiKhizbullin, andJurgen
Schmidhuber. Languageagentsasoptimizablegraphs. arXivpreprintarXiv:2402.16823,2024.
13A ImplementationDetails
WeadopttheagentprogramminglanguageandframeworkintroducedinAgents[Zhouetal.,2023b],
alanguageagentframeworkthatenablesdeveloperstobuildlanguageagentsthatstacksprompts
and tools together into complex pipelines. The main advantage of the Agents framework is that
itenablesdeveloperstouseaconfigfiletodefinetheagentsystem,whichmakesiteasierforthe
symbolicoptimizersintheagentsymboliclearningframeworktoperformupdateoperationsonthe
agentsystem.
B PromptTemplates
PromptTemplateforLanguageLossFunction
Losswithgroundtruth:
Youareafine-tunerofalargemodel.Iwillprovideyouwithsomeoutputresultsfromthemodelandthe
expectedcorrectresults.Youneedtoevaluatethesedataandprovideascoreoutof10,pleasewrapthescore
using<score></score>.Additionally,pleaseprovidesomesuggestionsformodifyingthemodel’soutput,
using<suggestion></suggestion>towrapyoursuggestions.
Hereisthemodel’soutput:
<result>result</result>;
Theexpectedresultis:
<ground_truth>ground_truth</ground_truth>
Pleasenote:
1.Ensurethattheoutputiswrappedwith<score></score>and<suggestion></suggestion>respectively.
2.Theoutputshouldbeasconsistentaspossiblewiththeexpectedresultwhilebeingcorrect.Forexample,
iftheexpectedresultis“BUST”,andthemodel’soutputis“Thewomen’slifestylemagazineis’BUST’
magazine.”,eventhoughtheansweriscorrect,youshouldadvisethemodeltobemoreconcise.
3.Thestandardforascoreof10isthatthemodel’soutputisexactlythesameastheexpectedresultina
case-insensitivemanner,andwithoutanyunnecessarycontent.Evenifthemodel’soutputissemantically
correct,ifitincludessuperfluouscontent,pointsshouldbededucted.
Losswithgroundtruthandscore:
Youarealargelanguagemodelfine-tuner. Iwillprovideyouwithamodel’soutputandtheexpected
correctresult.Youneedtoevaluateitandsuggestmodificationstothemodel’soutput.Pleaseuse‘<sugges-
tion></suggestion>‘toencloseyourfeedback.
Belowisthemodel’soutput:
<result>result</result>
Theexpectedresultis:
<ground_truth>ground_truth</ground_truth>
Hereistheevaluationscoreforthemodel.Yourgoalistooptimizethisscore:
<score>score</score>
Therelevantinformationaboutthisscoreisasfollows:
<evaluation_info>score_info</evaluation_info>
Note:
1.Ensurethat‘<suggestion></suggestion>‘existsandappearsonce.
2.Ifthemodel’soutputissatisfactory,youcanoutput<suggestion>Theoutputissatisfactory,noadditional
requirements</suggestion>.
3.Theoutputshouldbeasclosetotheexpectedresultaspossiblewhileensuringcorrectness.Forexample,
iftheexpectedresultis"BUST"andthemodel’soutputis"Thewomen’slifestylemagazineis’BUST’
magazine.",eventhoughthisansweriscorrect,youshouldremindthemodeltobeconcise.
Table4: PromptTemplateforLanguageLossFunction
14PromptTemplateforGradientBack-propagation
Prompt-Level
Youarenowapromptfine-tunerforalargelanguagemodel.Youaretaskedwithprovidingsuggestionsfor
optimizingtheprompttemplate.
Pleaseencloseyoursuggestionsusing<suggestion></suggestion>,forexample,<suggestion>itcouldbe
madeshorter</suggestion>.
The task is divided into multiple steps; I will provide you with the output from the previous step, the
requirement proposed by the next step for the current output, the current output itself, and the prompt
template.Youneedtosuggestimprovementsforthecurrentstep’sprompttemplate.
-Theprompttemplatethatneedsoptimizationis:<prompt_template>prompt_template</prompt_template>
-Theoutputfromthepreviousstepis:<previous_output>previous_output</previous_output>
-Thecurrentoutputis:<output>response</output>
- The requirement proposed by the next step for the current output is: <require-
ment>suggestion</requirement>
Inadditiontosuggestingmodificationsforthecurrentprompttemplate,youalsoneedtoproposerequirements
for the output of the previous step. Please wrap these using <suggestion></suggestion>, for example:
<suggestion>theanalysisshouldincludeacomparisonoforiginaldata</suggestion>.
Note:
1.Ensurethattheresultsarewrappedwith<suggestion></suggestion>and<suggestion></suggestion>,and
eachtagappearsonlyonce.
2.Ifyouarethefirstnode,youcanstatewithin<suggestion></suggestion>“Thisisthefirstnode.”
3. Pleasenotethatduringyouranalysis,rememberthatthisprompttemplatewillbeappliedtomultiple
differentdatasets,soyoursuggestionsshouldbegeneralandnotsolelyfocusedontheexamplesprovided
here.
4.Pleaseanalyzestepbystep.
Node-Level
Youarealargemodelfine-tuner.Nowyouneedtotrytooptimizetheinformationofanode.Foracomplex
task, ithasbeendividedintomultiplenodes, eachofwhichcontainsmultiplerolesthatworktogether
tocompletethetaskofthisnode. EachroleisbackedbyanLLMAgent,andyouneedtooptimizethe
configurationinformationofoneofthenodes.
HerearetherelevantexplanationsfortheNodeconfiguration:
-Thefieldsinthe"controller"indicatetheschedulingmethodofthemodel.Ifthereisonlyonerole,this
itemdoesnotneedtobeoptimized:
-"route_type"indicatestheschedulingmethod,whichhasthreevalues:"random"meansrandomscheduling,
"order"meanssequentialscheduling,and"llm"meansschedulingdeterminedbytheLLMmodel.
-"route_system_prompt"and"route_last_prompt"areusedwhen"route_type"is"llm"andarerespectively
thesystempromptandlastpromptgiventotheLLMmodelresponsibleforscheduling.
-"begin_role"isastringindicatingthenameofthestartingroleofthisnode.
-"roles"isadictionarywherethekeyistherolename,andthevalueisthepromptusedbythisrole.
Youneedtodecidehowtooptimizetheconfigurationofthisnode.Specifically,youneedtotrytoprovide
suggestionsinthefollowingaspects:
1.Updatethenodedescriptionfield.Thisfielddescribesthefunctionofthenodeandisalsoanimportant
indicatortomeasuretheperformanceofanode.
2.Updatetheschedulingmethodoftherole.Notethatifthereisonlyonerole,nooptimizationisneeded.
3.Addanewrole,andyouneedtoclearlydescribethefunctionofthisrole.
4.Deletearole,andyouneedtoclearlydescribethereasonfordeletingthisrole.
5.Updatearole,andyouneedtoindicatehowtoupdatethedescriptionofthisrole.
Next,IwillgiveyouaNodeconfiguration,andyouneedtoprovideoptimizationsuggestionsbasedonthe
currentNodeconfiguration. Pleaseuse<suggestion>[putyoursuggestionhere]</suggestion>toenclose
yoursuggestions.
##CurrentNodeConfig
{node_config}
You need to first provide your analysis process, then give your optimized result. Please use <anal-
yse></analyse> to enclose the analysis process. Please use <suggestion></suggestion> to enclose the
optimizationsuggestionsforthecurrentnode.Pleaseuse<suggestion></suggestion>toenclosetherequire-
mentsforthepreviousnode.
Note:Thesuggestionsprovidedneedtobeinoneormoreofthefiveaspectsmentionedabove.
Table5: PromptTemplateforGradientBack-propagation
15PromptTemplateforOptimizers
PromptOptimizer:
Youarenowapromptfine-tunerforalargelanguagemodel.Iwillprovideyouwithaprompttemplatealong
withitscorrespondinginputandoutputinformation.
Pleasemodifythepromptbasedontheprovideddata:
-Thecurrentprompttemplateis:prompt_template.
Hereissomeinformationaboutthemodelwhenusingthistemplate:
#Exampleindex
-Outputresult:<output>response</output>
-Suggestion:<suggestion>suggestion</suggestion>
Youneedtoanalyzethecontentaboveandinputtheoptimizedpromptresult.Pleasewrapyouranalysisin
<analyse></analyse>andthenewpromptin<new_prompt></new_prompt>.
Pleasenote:
1.Whenactuallyusingtheprompttemplate,thePythonformat()methodisemployedtofillvariablesinto
theprompt.Therefore,pleaseensurethatthecontentenclosedin inboththenewandoldpromptsremains
thesame,withnovariablesaddedorremoved.
2. Ensurethatyournewprompttemplatecanbedirectlyconvertedtoadictionaryusingthejson.loads()
method.Therefore,youneedtobecarefultousedoublequotesandescapecharactersproperly.
3.Ensurethat<analyse></analyse>and<new_prompt></new_prompt>eachappearonlyonce.
4. If you believe that the current prompt template performs sufficiently well, leave
<new_prompt></new_prompt>empty.
NodeOptimizer:
Youarealargemodelfine-tuner.Nowyouneedtotrytooptimizetheinformationofanode.Foracomplex
task,ithasbeendividedintomultiplenodes,eachcontainingmultiplerolesthatworktogethertocomplete
thetaskofthisnode. EachroleisbackedbyanLLMAgent,andyouneedtooptimizetheconfiguration
informationofoneofthenodes.
HerearetherelevantexplanationsfortheNodeconfiguration:
-Thefieldsinthe"controller"indicatetheschedulingmethodofthemodel.Ifthereisonlyonerole,this
itemdoesnotneedtobeoptimized:
-"route_type"indicatestheschedulingmethod,whichhasthreevalues:"random"meansrandomscheduling,
"order"meanssequentialscheduling,and"llm"meansschedulingdeterminedbytheLLMmodel.
-"route_system_prompt"and"route_last_prompt"areusedwhen"route_type"is"llm"andarerespectively
thesystempromptandlastpromptgiventotheLLMmodelresponsibleforscheduling.
-"begin_role"isastringindicatingthenameofthestartingroleofthisnode.
-"roles"isadictionarywherethekeyistherolename,andthevalueisthepromptusedbythisrole.
Next,IwillgiveyouaNodeconfigurationandseveralmodificationsuggestions.Youneedtomodifythe
Nodeconfigurationbasedonthesuggestions:
##CurrentNodeConfig
{node_config}
##Suggestions
{suggestions}
Whenprovidingthemodificationplan,youneedtogivetheoptimizedresultinthefollowingformat.Itisa
list,eachelementisadict,andthedictcontainsanactionfieldindicatingtheoperationontheNode.
Youroptimizedresultshouldbeenclosedin<result></result>,thatis,thecontentinside<result></result>
shouldbeaJSON-formattedlist,whichshouldbeabletobedirectlyloadedbyjson.loads().
Note:
1.Ifyouthinkthecurrentconfigurationisalreadyexcellentanddoesnotneedmodification,youcandirectly
outputanemptylist.
2.Theformatof<result>[optimizationmethod]</result>needstostrictlyfollowthegivenformat,otherwise,
itwillbejudgedasincorrect.
Table6: PromptTemplateforOptimizers
16