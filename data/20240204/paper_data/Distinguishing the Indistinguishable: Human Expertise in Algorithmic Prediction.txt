Distinguishing the Indistinguishable: Human Expertise in Algorithmic
Prediction
RohanAlur1,ManishRaghavan1,2,andDevavratShah1
1DepartmentofElectricalEngineering&ComputerScience,MIT
2SloanSchoolofManagement,MIT
ABSTRACT
Weintroduceanovelframeworkforincorporatinghumanexpertiseintoalgorithmicpredictions. Our
approachfocusesontheuseofhumanjudgmenttodistinguishinputswhich‘lookthesame’toanyfeasible
predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in
predictiontasks,asexpertsoftenhaveaccesstoinformation—particularlysubjectiveinformation—which
isnotencodedinthealgorithm’strainingdata. Weusethisinsighttodevelopasetofprincipledalgorithms
for selectively incorporating human feedback only when it improves the performance of any feasible
predictor. Wefindempiricallythatalthoughalgorithmsoftenoutperformtheirhumancounterpartson
average,humanjudgmentcansignificantlyimprovealgorithmicpredictionsonspecificinstances(which
canbeidentifiedex-ante). InanX-rayclassificationtask,wefindthatthissubsetconstitutesnearly30%
ofthepatientpopulation. Ourapproachprovidesanaturalwayofuncoveringthisheterogeneityandthus
enablingeffectivehuman-AIcollaboration.
1 Introduction
Despiteremarkableadvancesinmachinelearning,humanjudgmentcontinuestoplayacriticalroleinmanyhigh-stakes
predictiontasks. Forexample,considertheproblemoftriageintheemergencyroom,wherehealthcareprovidersquickly
assessandprioritizepatientsforimmediatecare. Ononehand,prognosticalgorithmsoffersignificantpromiseforimproving
triagedecisions;indeed,carefullyimplementedalgorithmsareoftenmoreaccuratethanevenexperthumandecisionmakers
(Cowgill,2018;Dawesetal.,1989;Groveetal.,2000;Kleinbergetal.,2017;Kunceletal.,2013a;Dawes,1971;Currieand
MacLeod,2017;MullainathanandObermeyer,2019). Ontheotherhand,predictivealgorithmsmayfailtofullycapturethe
relevantcontextforeachindividual. Forexample,analgorithmicriskscoremayonlyhaveaccesstotabularelectronichealth
recordsorotherstructureddata(e.g.,medicalimaging),whileaphysicianhasaccesstomanyadditionalmodalities—not
leastofwhichistheabilitytodirectlyexaminethepatient!
Thesetwoobservations—thatalgorithmsareoftenmoreaccuratethanhumans,buthumansoftenhaveaccesstoamuch
richerinformationset—arenotinconflictwitheachother. Indeed,Aluretal.(2023)findexactlythisphenomenoninan
analysisofemergencyroomtriagedecisions. Thissuggeststhat,eveninsettingswherealgorithmsoutperformhumans,
wecanstillbenefitfromcollaborationbetweenalgorithmsandhumans. Ideallythiscollaborationwillyield‘human-AI
complementarity’(Bansaletal.,2020b),inwhichajointsystemoutperformseitherahumanoralgorithmworkingalone.
Rastogietal.(2022)highlighthumans’andalgorithms’differinginformationsetsasoneofthekeymechanismsforenabling
complementarity. Ourworkthusbeginswiththefollowingquestion:
Whencanhumanfeedbackimprovethepredictionsofanyalgorithmactingalone?
Example: X-rayclassification. Considerthetaskofdiagnosingatelectasis(apartiallyorfullycollapsedlung;westudythis
taskindetailinSection5). Today’sstate-of-the-artdeeplearningmodelscanperformwellonthistaskusingonlyapatient’s
1
4202
beF
1
]GL.sc[
1v39700.2042:viXraAPREPRINT
chestX-rayasinput(Irvinetal.,2019;Rajpurkaretal.,2021). However,weareinterestedinwhetherwecanimprovethese
algorithmicpredictionsbyincorporatinga‘secondopinion’fromaphysician,particularlybecausethephysicianmayhave
accesstoinformation(e.g.,bydirectlyobservingthepatient’ssymptoms)whichisnotpresentintheX-ray.
Afirstheuristic,withoutmakinganyassumptionsaboutthekindofpredictivemodelswhichareavailable,istoask
whetheraphysiciancandistinguishapairofpatientswhoseimagingdataareidentical. Ifaphysiciancancorrectlyindicate
thatonepatientissufferingfromatelectasiswhiletheotherisnot—despitethepatientshavingidenticalchestX-rays—the
physicianmusthaveaccesstoinformationthattheX-raydoesnotcapture. Inprinciple,thiscouldformthebasisforasimple
hypothesistest: wecouldaskwhetherthephysicianperformsbetterthanrandomindistinguishingalargenumberofsuch
pairs. Ifso,evenapredictivealgorithmwhichoutperformsthephysician’sdiagnosticaccuracymightbenefitfromphysician
input.
Ofcourse,weareunlikelytofindidenticalobservationsinafinitedataset,especiallywhendataarecontinuous-valued
and/or high-dimensional (as is the case with X-rays). A natural relaxation is to instead consider whether a human can
distinguishpairsofobservationswhicharesufficiently‘similar’,assuggestedbyAluretal.(2023). Inthisworkwepropose
amoregeneralnotionofalgorithmicindistinguishability,whichwedefineascoarsersubsetsoftheinputspaceinwhich
noalgorithm(insomerich,user-definedclassofpredictivemodels)hassignificantpredictivepower. Weshowthatthese
subsetscanbeefficientlydiscoveredviaanovelconnectiontomulticalibration(Hébert-Johnsonetal.,2018),andformally
demonstratethatusinghumaninputtopredictoutcomeswithinthesesubsetscanoutperformanyalgorithmicpredictor(in
thesameuser-definedclass). Thesetwostepsyieldasimplemeta-algorithmfor(1)detectingwhetheranexpertprovides
informationwhichisnotencodedin(orcannotbelearnedfrom)thetrainingdataand(2)selectivelyincorporatingthisinput
onlywhenitaddssignificantpredictivevalue. Weelaborateonthesecontributionsbelow.
Contributions. Weproposeanovelframeworkforhuman/AIcollaborationinpredictiontasks. Ourapproachuseshuman
feedbacktorefinealgorithmicpredictionswithinsetsofinputswhichare‘indistinguishable’onthebasisofthetrainingdata.
InSection3wepresentafamilyofalgorithmswhichincorporatethisfeedbackonlywhenitimprovesthesquarederrorof
thebestfeasiblepredictivemodel(andpreciselyquantifythisimprovement). Thisworkextendsthe‘omnipredictors’result
ofGopalanetal.(2021)inthespecialcaseofsquarederror,whichmaybeofindependentinterest.1 InSection5wepresent
empiricalresultsdemonstratingthatalthoughhumansfailtooutperformalgorithmicpredictorsonaverage,ourapproach
enablestheidentificationofspecificinstancesonwhichhumansaremoreaccuratethanthebestavailablealgorithm.2 In
Section6weextendourresultstosettingsinwhichanalgorithmprovidesrecommendationstomanydownstreamusers,
whoindependentlychoosewhentocomplywiththeserecommendations. Weprovideconditionsunderwhichapredictive
algorithmcanproviderecommendationswhichare(nearly)optimalforarichclassofpossibleusercompliancepatterns.
2 Relatedwork
Therelativestrengthsofhumansandalgorithms. Ourworkismotivatedbylargebodyofliteraturewhichstudiesthe
relativestrengthsofhumanjudgmentandalgorithmicdecisionmaking(Cowgill,2018;Dawesetal.,1989;Groveetal.,
2000; Kuncel et al., 2013b) or identifies behavioral biases in human decision making (Tversky and Kahneman, 1974;
CamererandJohnson,1991;Arnoldetal.,2020;Rambachan,2022). Morerecentworkfocusesontheuseofalgorithmsto
improvehumandecisionmakinginhigh-stakessettings(Kleinbergetal.,2017;MullainathanandObermeyer,2019;Bastani
etal.,2021). Thisresearchiscomplementarytootherwork,includingours,whichinsteadconsiderswhenhumanjudgment
canimprovealgorithmicpredictions.
Recommendations, deferral and complementarity. One popular approach for incorporating human judgment into
algorithmicdecisionpipelinesisbyallowingapredictivemodeltodefersomeinstancestoahumandecisionmaker(Madras
etal.,2018;Raghuetal.,2019;MozannarandSontag,2020;Keswanietal.,2021;Okatietal.,2021;Keswanietal.,2022).
Otherworkstudiescontextswherehumandecisionmakersarefreetooverridealgorithmicrecommendations(De-Arteaga
etal.,2020;Beedeetal.,2020;CowgillandStevenson,2020;Dietvorstetal.,2018),whichmaysuggestalternativedesign
criteriaforthesealgorithms(Bansaletal.,2020a;BenzandRodriguez,2023). Moregenerally,systemswhichachieve
1WeelaborateonthisconnectioninAppendixA.4.
2Code,dataandinstructionstoreproduceourexperimentsareavailablehere.
2APREPRINT
human/AIcomplementarity(asdefinedinSection1)havebeenpreviouslystudiedinAgrawaletal.(2018);Bansaletal.
(2020a);Wilderetal.(2020);Donahueetal.(2022);Steyversetal.(2022);Deetal.(2020).
Rastogietal.(2022)provideanexcellentsurveyofthisareaanddevelopataxonomytoinvestigatewhencomplemen-
taritymaybefeasible. Theseworkstakethepredictorasgiven,orlearnanalgorithmwhichisoptimizedtocomplementa
particularmodelofhumandecisionmaking. Incontrast,wegivestrongerresultswhichdemonstratewhenhumanjudgment
canimprovetheperformanceofanymodelinarichclassoffeasiblepredictivealgorithms(Section3),orwhenasingle
algorithmcaneffectivelycomplementmanyheterogeneoususers(Section6).
Algorithmicmonoculture. Ourresultscanbeviewedasoneapproachtomitigatingalgorithmicmonoculture,inwhich
differentalgorithmsmakesimilardecisionsandthussimilarmistakes(KleinbergandRaghavan,2021;Toupsetal.,2023).
Thiscouldoccurbecausethesesystemsaretrainedonsimilardatasets,orbecausetheysharesimilarinductivebiases. We
arguethatthesearepreciselythesettingsinwhichan‘uncorrelated’humanopinionmaybeespeciallyvaluableforrefining
algorithmpredictions. WefindempiricalevidenceforthisinterpretationinSection5: oninstanceswheremultiplemodels
agreeonaprediction,humanjudgmentaddssubstantialpredictiveinformation.
Multicalibration, omnipredictors and boosting. Our results make use of tools from theoretical computer science,
particularlyworkonomnipredictors(Gopalanetal.,2021)anditsconnectionstomulticalibration(Hébert-Johnsonetal.,
2018). Inthecontextofsupervisedlearning,Gopalanetal.(2021)giveconditionsunderwhichthereexistsapartitionofthe
featurespacewhich“extractsallthepredictivepower"fromagivenclassofpredictivemodels(inawaywemakeprecise
below). Dworketal.(2020)showthatmulticalibrationistightlyconnectedtoacryptographicnotionofindistinguishability,
whichservesasconceptualinspirationforourwork. Globus-Harrisetal.(2023)provideanelegantboostingalgorithmfor
learningmulticalibratedpartitionsthatwemakeuseofinourexperiments.
3 Results
Inthissectionwepresentourmaintechnicalresults. AllproofsaredeferredtoAppendixA.
Notationandpreliminaries. LetX ∈X bearandomvariabledenotingtheinputs(or‘features’)whichareavailablefor
makingpredictionsaboutanoutcomeY ∈[0,1]. LetYˆ ∈[0,1]beanexpert’spredictionofY. Wearebroadlyinterestedin
whethertheexpertpredictionYˆ providesinformationwhichisnotcapturedbyany‘feasible’predictivealgorithm,whichwe
denotebyaclassoffunctionsF whichmapX to[0,1]. OurresultsdependcruciallyonthechoiceofmodelclassF,which
weturntobelow.
ChoiceofmodelclassF. WeplacenorestrictionsatthispointonthenatureofF,butit’shelpfultoconsideraconcrete
modelclass(e.g.,aspecificneuralnetworkarchitecture)fromwhich,givenasetoftrainingdata,alearningalgorithmcould
deriveaparticularpredictivemodel(e.g.,viaempiricalriskminimizationoverF). ThechoiceofF couldbeguidedby
practicalconsiderations;forexample,wemightrequiremodelswhicharestraightforwardtointerpret(e.g.,linearfunctions
orshallowdecisiontrees)orbesubjecttocomputationalconstraints. Wemightalsosimplybelievethatacertainarchitecture
orfunctionalformiswellsuitedtothepredictiontaskofinterest. Inanycase,weareinterestedinwhetherhumanfeedback
canprovideinformationwhichisnotconveyedbyanymodelinthisclass,butareagnosticastohowthisisaccomplished:
anexpertmayhaveinformationwhichisnotencodedinX,orbedeployingadecisionrulewhichisnotinF—orboth!
AnotherchoiceistotakeF tomodelmoreabstractlimitationsontheexpert’scognitiveprocess. Inparticular,tomodel
expertswhoaresubjectto“boundedrationality"(Simon,1957;KlaesandSent,2005),wemighttakeF tobethesetof
functionswhichcanbeefficientlycomputed(e.g.,byacircuitoflimitedcomplexity,orinpolynomialtime). Inthiscase,an
expertwhoprovidesapredictionwhichcannotbemodeledbyanyf ∈ F musthaveaccesstoinformationwhichisnot
presentinthetrainingdata. WetakethechoiceofF asgiven,butemphasizethatthesetwoapproachesyieldqualitatively
differentinsightabouthumanexpertise.
IndistinguishabilitywithrespecttoF. Intuitively,ourapproachwillbetousethehumanpredictionYˆ todistinguishsets
ofobservationsS ⊆X whichareindistinguishabletoanypredictorf ∈F. Weformalizethisnotionofindistinguishability
asfollows:
3APREPRINT
Definition3.1(α-Indistinguishablesubset). Forsomeα≥0,asetS ⊆X isα-indistinguishablewithrespecttoafunction
classF andtargetY if,forallf ∈F,
|Cov(f(X),Y |X ∈S)|≤α. (1)
Tointerpretthisdefinition,observethatitissatisfiedwithα = 0iff(X) ⊥⊥ Y | X ∈ S forallf ∈ F;conditionalon
knowingthatX ∈S,nof ∈F isinformativeaboutY. Forexample,thiscouldoccurbecauseeverypredictoragreeswithin
S;e.g.,f(X)=1forallf ∈F andX ∈S. Whileasubsetsatisfyingthisstrongerconditionalindependenceconditionis
unlikelytoexistwhenF isarichclassoffunctions,we’llshowthatDefinition3.1generalizesthesameintuitiontorealistic
settings. Wefirstadoptthedefinitionofamulticalibratedpartition(Gopalanetal.,2021)asfollows:
Definition3.2(α-Multicalibratedpartition). ForK ≥1,S ...S ⊆X isanα-multicalibratedpartitionofX withrespect
1 K
toF andY if(1)S ...S partitionsX and(2)eachS isα-indistinguishablewithrespecttoF andY.3
1 K k
Intuitively,thepartition{S } “extract[s]allthepredictivepower”fromF (Gopalanetal.,2021);withineachelement
k k∈[K]
ofthepartitionS ,eachf ∈F isonlyweaklyrelatedtotheoutcomeY. It’snotobviousthatsuchpartitionsarefeasibleto
k
compute,oreventhattheyshouldexist. We’llshowinSection4howeverthatthispartitioncanbeefficientlycomputed
formanynaturalclassesoffunctions. Wheretherelevantpartitionisclearfromcontext,weuseE [·],Var (·),Cov (·,·)
k k k
to denote expectation, variance and covariance conditional on the event that {X ∈ S }. For a subset S ⊆ X, we use
k
E [·],Var (·)andCov (·,·)analogously.
S S S
Incorporatinghumanjudgmentintopredictions. Givenamulticalibratedpartition,anaturalheuristicforassessingthe
valueofahumanpredictionYˆ istotestwhethertheconditionalcovarianceCov (Y,Yˆ)is‘large’withinanyindistinguishable
k
subsetS . Intuitively,thistellsusthatwithinS ,theexpertpredictionisinformativeeventhougheverymodelf ∈F isnot.
k k
Thissuggestsasimplemeta-algorithmforincorporatinghumanexpertise: first,learnapartitionwhichismulticalibrated
withrespecttoF,andthenuseYˆ topredictY withineachindistinguishablesubset. Weshowthatthisapproachsufficesto
outperformthesquarederrorachievedbyanyf ∈ F,andthatthisimprovementisindeedquantifiedbytheconditional
covarianceCov (Y,Yˆ).
k
Theorem 3.1. Let {S } be an α-multicalibrated partition with respect to a model class F and target Y. Let the
k k∈[K]
randomvariableJ(X)∈[K]besuchthatJ(X)=kiffX ∈S . Defineγ∗,β∗ ∈RK as
k
(cid:20)(cid:16) (cid:17)2(cid:21)
γ∗,β∗ ∈ argmin E Y −γ +β Yˆ (2)
J(X) J(X) i
γ∈RK,β∈RK
Then,foranyf ∈F,
(cid:20)(cid:16) (cid:17)2(cid:21)
E Y −γ∗−β∗Yˆ +4Cov (Y,Yˆ)2 (3)
k k k k
(cid:104) (cid:105)
≤E (Y −f(X))2 +2α. (4)
k
That is, the squared error incurred by the univariate linear regression of Y on Yˆ within each indistinguishable subset
outperforms the squared error incurred by any f ∈ F. This improvement is at least 4Cov (Y,Yˆ)2, up to an additive
k
approximationerror2α(recallDefinition3.2). WeemphasizethatF isanarbitraryclass,andinparticularmayinclude
complex,nonlinearpredictors. Nonetheless,givenamulticalibratedpartition,asimplelinearpredictorcanimproveonthe
bestf ∈F. Furthermore,thisapproachallowsustoselectivelyincorporatehumanfeedback: wheneverCov (Y,Yˆ)=0,
k
werecoveracoefficientβ∗of0.4 Althoughwestateourresultsintermsofthepopulationquantitiesβ∗,γ∗forsimplicity,
k
estimatesofthesecoefficientscanbeobtainedfromamodestsampleoftrainingdataviaordinaryleastsquaresregression.
Nonlinearfunctionsandhigh-dimensionalfeedback. AlthoughTheorem3.1isspecifictolinearregression,thesame
insightgeneralizesreadilytootherfunctionalforms. Forexample,ifthetargetY isabinaryoutcome,itmightbedesirable
3Thisiscloselyrelatedtoα-approximatemulticalibration(Gopalanetal.,2021), whichasksthatDefinition3.1merelyholdin
expectationovertheevents{X ∈S }...{X ∈S }.Weworkwithastrongerpointwisedefinitionforclarity,butoursubsequentresults
1 K
canalsobeinterpretedasholdingforthe‘typical’elementofanα-approximatelymulticalibratedpartition.
4RecallthatthepopulationcoefficientinaunivariatelinearregressionofY onYˆ is Cov(Y,Yˆ).
Var(Yˆ)
4APREPRINT
toinsteadincorporateYˆ viaalogisticregressionofY onYˆ. Moregenerally,othernonlinearfunctionsofYˆ mightperform
betterthanasimplelinearprediction. Weprovideasimilarguaranteefornonlinearpredictorsviathefollowingcorollaryof
Theorem3.1.
Corollary3.1. LetS beanα-indistinguishablesubsetwithrespecttoamodelclassF andtargetY. Letg :[0,1]→[0,1]
beafunctionwhichsatisfiesthefollowingapproximateBayes-optimalityconditionforη ≥0:
E [(Y −g(Yˆ))2]≤E [(Y −E [Y |Yˆ])2]+η. (5)
S S S
Then,foranyf ∈F,
(cid:104) (cid:105)
E (Y −g(Yˆ))2 +4Cov (Y,Yˆ)2 (6)
S S
(cid:104) (cid:105)
≤E (Y −f(X))2 +2α+η. (7)
S
Thatis, anyfunctiong whichisnearlyasaccurate(intermsofsquarederror)astheunivariateconditionalexpectation
functionE [Y |Yˆ]providesthesameguaranteeasinTheorem3.1. Thisconditionalexpectationfunctionisexactlywhat
S
e.g.,alogisticregressionofY onYˆ seekstomodel.
WhiletheresultsabovefocusoncasesinwhichanexpertdirectlyprovidesapredictionYˆ ∈ [0,1], wealsoshow
thatthisresultextendstoricherformsoffeedback. Forexample,inamedicaldiagnosistask,aphysicianmightproduce
free-formclinicalnoteswhichcontaininformationthatisnotavailableintabularelectronichealthrecords. Incorporating
thiskindoffeedbackrequiresalearningalgorithmbettersuitedtohigh-dimensionalinputs(e.g.,arandomforestordeep
neuralnetwork),whichmotivatesourfollowingresult.
Corollary3.2. LetS beanα-indistinguishablesubsetwithrespecttoamodelclassF andtargetY. LetH ∈Hdenote
expertfeedbackwhichtakesvaluesinsomearbitrarydomain(e.g,. Rd),andletg :H→[0,1]beafunctionwhichsatisfies
thefollowingapproximatecalibrationconditionforsomeη ≥0andforallβ,γ ∈R:
E [(Y −g(H))2]≤E [(Y −γ−βg(H))2]+η. (8)
S S
Then,foranyf ∈F,
E (cid:2) (Y −g(H))2(cid:3) +4Cov (Y,g(H))2 (9)
S S
(cid:104) (cid:105)
≤E (Y −f(X))2 +2α+η. (10)
S
To interpret this result, notice that (8) requires only that the prediction g(H) cannot be significantly improved by any
linearpost-processingfunction. Forexample,thisconditionissatisfiedbyanycalibratedpredictorg(H).5 Perhapsmore
importantly,anyg(H)whichdoesnotsatisfy(8)canbetransformedbylettingg˜(H) = min E[(Y −γ −βg(H))2];
γ,β
i.e.,bylinearlyregressingY ong(H),inwhichcaseg˜(H)satisfies(8). ThisresultmirrorsTheorem3.1,butallowsusto
incorporaterichhumanfeedbackwithinsubsetsthatareindistinguishableonthebasisofX alone.
Testingforinformativeexperts. Whilewehavethusfarfocusedondevelopingalgorithmstoincorporatehumanfeedback,
observethatTheorem3.1canalsobeinterpretedassuggestingatestforhumanexpertise: iftheconditionalcovariance
Cov (Y,Yˆ)islargewithinS ,thenYˆ is‘moreinformative’—inthenarrowsenseofimprovingsquarederror—thanany
k k
5AcalibratedpredictorisonewhereE[Y |g(H)]≈g(H).Thisisafairlyweakcondition;forexample,itissatisfiedbytheconstant
predictorg(H)≡E[Y](FosterandVohra,1998).
5APREPRINT
f ∈F withinS . Thismaybeofinterestevenifthisfeedbackcannotbedirectlyincorporatedintoalgorithmicpredictions
k
(e.g.,whenconsideringwhethertoautomateagivenpredictiontaskatall).
Ofcourse,minimizingsquarederrorisjustonepossibleobjective. Weshownowthat,givenaclassofbinary-valued
functionsFbinary,thecovarianceofY andYˆ withinanindistinguishablesubsetservesasatestforwhetherYˆ providesany
informationaboutY whichwecouldnothopetolearnviatheclassF.
Theorem3.2. Let{S } beanα-multicalibratedpartitionforabinary-valuedmodelclassFbinaryandtargetoutcome
k k∈[K]
Y. Forallk ∈[K],lettherebef˜ ∈F suchthatY ⊥⊥Yˆ |f˜(X),X ∈S . Then,forallk ∈[K]
k k k
(cid:114)
(cid:12) (cid:12) α
(cid:12)Cov (Y,Yˆ)(cid:12)≤ . (11)
(cid:12) k (cid:12) 2
Thatis,ifthereexistssomesetofpredictors{f˜} which‘explain’thesignalprovidedbytheexpert,thenthenthe
k k∈[K]
covarianceofY andYˆ isboundedwithineachS ⊆X. Thecontrapositiveofthisresultimpliesthatobservingasufficiently
k
largecovariancebetweenY andYˆ withinanindistinguishablesubsetservesasacertificateforthepropertythatnosubsetof
F canfullyexplaintheinformationthatYˆ providesaboutY. Thisresultcanthusbeviewedasafiner-grainedextensionof
Aluretal.(2023).
Takentogether,ourresultsdemonstratethatindistinguishabilityprovidesaprincipledwayofreasoningaboutwhether
humanexpertisecancomplementalgorithmicpredictions. Furthermore,thisapproachyieldsaconcretemethodologyfor
incorporatingthisexpertise: wecansimplyusehumanfeedbacktopredictY withinsubsetswhichareindistinguishableon
thebasisofX alone. Ofcourse,ourresultsdependcriticallyontheabilitytofindtheseindistinguishablesubsets. Weturnto
thisproblemnext.
4 Learningmulticalibratedpartitions
InthissectionwediscusstwosetsofconditionsonF whichenabletheefficientcomputationofmulticalibratedpartitions.
AnimmediateimplicationofourfirstresultisthatanyclassofLipschitzpredictorsinduceamulticalibratedpartition. We
makeuseofbothoftheresultsinthissectionourexperiments(Section5).
LevelsetsofF aremulticalibrated. ObservethatonewayinwhichDefinition3.1istriviallysatisfied(withα = 0)is
whenevereveryf ∈F isconstantwithinasubsetS ⊆X. Werelaxthisinsightasfollows: ifthevarianceofeveryf ∈F is
boundedwithinS,thenS isapproximatelyindistinguishablewithrespecttoF.
Lemma4.1. LetF beaclassofpredictorsandS ⊆X beasubsetoftheinputspace. If:
maxVar(f(X)|X ∈S)≤4α2, (12)
f∈F
thenS isα-indistinguishablewithrespecttoF andY.
This result yields a natural corollary: the approximate level sets of F (i.e., sets in which the range of every f ∈ F is
bounded)areapproximatelyindistinguishable. WestatethisresultformallyasCorollaryA.1inAppendixA.Weuseexactly
thisapproachtofindingmulticalibratedpartitionsinourstudyofachestX-rayclassificationtaskinSection5.
Lemma4.1alsoimpliesasimplealgorithmforfindingmulticalibratedpartitionswhenF isLipschitzwithrespect
tosomedistancemetricd : X ×X → R: observationswhicharecloseunderd(·,·)areguaranteedtobeapproximately
indistinguishablewithrespecttoF. WestatethisresultformallyasCorollaryA.2inAppendixA.
Multicalibrationviaboosting. RecentworkbyGlobus-Harrisetal.(2023)demonstratethatmulticalibrationisclosely
relatedtoboostingoverafunctionclassF. Inthissectionwefirstprovideconditions,adaptedfromGlobus-Harrisetal.
(2023),whichimplythatthelevelsetsofacertainpredictorh:X →[0,1]aremulticalibratedwithrespecttoF;thatis,the
set{x|h(x)=v}foreveryvintherangeofhisapproximatelyindistinguishable. Wethendiscusshowtheseconditions
6APREPRINT
yieldanaturalboostingalgorithmforlearningapredictorhwhichinducesamulticalibratedpartition. Inthelemmabelow,
weuseR(f)todenotetherangeofafunctionf.
Lemma4.2. LetF beafunctionclasswhichisclosedunderaffinetransformations;i.e.,f ∈ F ⇒ a+bf ∈ F forall
a,b∈R,andletF˜ ={f ∈F |R(f)⊆[0,1]}. LetY ∈[0,1]bethetargetoutcome,andh:X →[0,1]besomepredictor
withcountablerangeR(h)⊆[0,1]. If,forallf ∈F,v ∈R(h):
E(cid:2) (h(X)−Y)2−(f(X)−Y)2 |h(X)=v(cid:3) <α2, (13)
thenthelevelsetsofhare(2α)-multicalibratedwithrespecttoF˜andY.
Tointerpretthisresult,observethat(13)isthedifferencebetweenthemeansquarederroroff andthemeansquarederrorof
hwithineachlevelsetS ={x∈X |h(x)=v}. Thus,ifthebestf ∈F failstosignificantlyimproveonthesquarederror
v
ofhwithinagivenlevelsetS ,thenS isindistinguishablewithrespecttoF˜(whichismerelyF restrictedtofunctions
v v
thatliein[0,1]). Globus-Harrisetal.(2023)giveaboostingalgorithmwhich,givenasquarederrorregressionoracle6forF,
outputsapredictorhwhichsatisfies(13). WemakeuseofthisalgorithminSection5.
Takentogether,theresultsinthissectiondemonstratethatmulticalibratedpartitionscanbeefficientlycomputedfor
manynaturalclassesoffunctions,whichinturnenablestheapplicationofresultsinSection3. Toillustratethis,wenow
examinetheuseofmulticalibrationtorefinealgorithmicpredictionsinapairofreal-worldclassificationtasks.
5 Experiments
5.1 ChestX-rayinterpretation
In this section we return to the chest X-ray classification task outlined in Section 1. We study the performance of the
eightpredictivemodelsconsideredinRajpurkaretal.(2021),whichwereselectedfromtheleaderboardofalargepublic
competitionforbenchmarkingmachinelearningalgorithmsinmedicalimagingtasks. Thesemodelsweretrainedona
datasetof224,316chestradiographscollectedacross65,240patients(Irvinetal.,2019),andthenevaluatedonaholdoutset
of500randomlysampledradiographs. Thisholdoutsetwasannotatedbyeightradiologistsforthepresence(Y =1)or
absence(Y =0)offiveselectedpathologies;themajorityvoteoffiveoftheseradiologistsservesasagroundtruthlabel,
whiletheremainingthreeserveasbenchmarksfortheaccuracyofindividualradiologists(Rajpurkaretal.,2021).
Inthissectionwefocusondiagnosingatelectasis(apartialorcompletecollapseofthelung);weprovidetheanalogous
resultsfortheotherfourpathologiesinAppendixC.Wefirstshow,consistentwithIrvinetal.(2019);Rajpurkaretal.(2021),
thatradiologistsfailtoconsistentlyoutperformalgorithmicclassifiersonaverage. Wethendemonstrate,byapplyingthe
resultsofSection3,thatradiologistassessmentscansubstantiallyoutperformalgorithmicpredictionsonasizableminority
ofpatients. Becauseradiologistsandthealgorithmicpredictorsonlyhaveaccesstothepatient’schestX-ray,weinterpret
theseresultsasprovidinga‘lowerbound’ontheimprovementwhichisachievablebyincorporatinghumanexpertise. In
particular,physicianswithaccesstoadditionalinformation(e.g.,theabilitytodirectlyexaminepatients)mightachieve
furtherperformancegains.
Algorithmsarecompetitivewithexpertradiologists.Wefirstcomparetheperformanceofthethreebenchmarkradiologists
tothatoftheeightleaderboardalgorithmsinFigure1. FollowingRajpurkaretal.(2021),weusetheMatthew’sCorrelation
Coefficient(MCC)asastandardmeasureofbinaryclassificationaccuracy(ChiccoandJurman,2020). TheMCCissimply
therescaledcovariancebetweeneachpredictionandtheoutcome,whichcorrespondstoourdefinitionofindistinguishability
(Definition3.1). InFigure1weseethatradiologistperformanceisstatisticallyindistinguishablefromthatofthealgorithmic
classifiers.
Radiologistscanrefinealgorithmicpredictions. WenowapplytheresultsofSection3toinvestigateheterogeneityinthe
relativeperformanceofhumansandalgorithms. Todothis,wefirstpartitionthepatientsintotheapproximatelevelsetsof
6Informally,asquarederrorregressionoracleforF isanalgorithmwhichcanefficientlyoutputargmin E[(Y −f(X)]2]forany
f∈F
distributionoverX,Y.Whenthedistributionisoverafinitesetoftrainingdata,thisisequivalenttoempiricalriskminimization.We
refertoGlobus-Harrisetal.(2023)foradditionaldetails,includinggeneralizationarguments.
7APREPRINT
predictor
drnet
ihil
0.6 jfaboy
ngango2
ngango3
sensexdr
uestc
yww211
0.5
radiologist 1
radiologist 2
radiologist 3
algorithmic radiologist
Prediction Type
Figure1: Therelativeperformanceofradiologistsandpredictivealgorithmsfordetectingatelectasis. EachbarplotstheMatthews
CorrelationCoefficientbetweenthecorrespondingpredictionandthegroundtruthlabel.Pointestimatesarereportedwith95%bootstrap
confidenceintervals.
theeightpredictors.7 PerLemma4.1andCorollaryA.1,theselevelsetsareapproximatelyindistinguishablewithrespect
totheeightpredictivealgorithmsweconsider. Weplottheconditionalperformanceofboththeradiologistsandtheeight
leaderboardalgorithmswithineachoftheseapproximatelyindistinguishablesubsetsinFigure2.
0.6 predictor
random baseline
0.4 drnet
ihil
jfaboy
0.2
ngango2
ngango3
0.0 sensexdr
uestc
yww211
-0.2
radiologist 1
radiologist 2
-0.4 radiologist 3
α=0, μ=1, n=148 α=0.46, μ=0.18, n=352
Model Prediction Bins
Figure 2: The conditional performance of radiologists and predictive algorithms for detecting atelectasis. Each subset is α-
indistinguishablewithrespecttothealgorithmicpredictors.µindicatesthefractionofpositivealgorithmicpredictionsandnindicatesthe
numberofpatients.Arandompermutationofthetruelabelsisincludedasabaseline.AllelseisasinFigure1.Theconfidenceintervals
forthealgorithmicpredictorsarenotstrictlyvalid(thesubsetsarechosenconditionalonthepredictionsthemselves),butareincludedfor
referenceagainstradiologistperformance.
Theradiologists’performanceisstatisticallyindistinguishablefromthatofallbutoneofthealgorithmswithinthe
‘mixed’ subset (α = 0.46,µ = .18; µ indicates the fraction of positive algorithmic predictions), where the algorithms
generallypredictanegativelabelbutvarysubstantiallyintheirpredictions. However,withinthe‘positive’(α=0,µ=1)
subset—wherealleightalgorithmspredictapositivelabel—weseethatallthreeradiologistsprovideassessmentswhich
aresignificantlymoreaccuratethanthealgorithmicpredictions. Importantly,thisheterogeneitywasidentifiedex-anteby
partitioningthefeaturespaceintoapairofindistinguishablesubsets. Inparticular,alleightofthemodelsweconsidercan
beimprovedbysolicitingfeedbackfromtheradiologistswithinthe‘positive’bin.
Otherpathologies. Althoughwefocushereondiagnosingatelectasis,andthefindingsaboveareconsistentwithourresults
fortwooftheotherfourpathologiesconsideredinRajpurkaretal.(2021)(pleuraleffusionandconsolidation): although
radiologistsfailtooutperformalgorithmicpredictorsonaverage,atleasttwooftheradiologistscanoutperformalgorithmic
7ThisamountstominimizingChebyshevdistanceinthe8-dimensionalspacedefinedbythepredictionsofeachleaderboardalgorithm
(Gonzalez,1985).Seehttps://github.com/ralur/human-expertise-algorithmic-predictionforadditionaldetail.
8
tneicfifeoC
noitalerroC
tneicfifeoC
noitalerroCAPREPRINT
predictionsonasizableminorityofpatients. OurresultsfortheothertwopathologiesselectedinRajpurkaretal.(2021)
(cardiomegalyandedema)appearqualitativelysimilar,butwelackstatisticalpowertodrawfirmconclusions. Wepresent
theseresultsinAppendixC.
5.2 Predictionofsuccessinhumancollaboration
WenextconsiderthevisualpredictiontaskstudiedinSaveskietal.(2021). Inthiswork,theauthorscurateadatasetof
photostakenofparticipantsaftertheyattemptan‘EscapetheRoom’puzzle—“aphysicaladventuregameinwhichagroup
istaskedwithescapingamazebycollectivelysolvingaseriesofpuzzles"(Saveskietal.,2021). Inthistask,subjectsare
assignedtooneoffourtreatmentconditionsandaskedtopredictwhetherthegrouppicturedineachphotographsucceeded
incompletingthepuzzle.8 Subjectsinthecontrolarmofthestudyperformthistaskwithoutanyformoftraining,while
subjectsintheremainingarmsareprovidedwithfour,eightandtwelvelabeledexamples,respectively,beforebeginningthis
task. Theirperformanceiscomparedtothatoffiveoff-the-shelflearningalgorithms,whichusehigh-levelfeaturesextracted
fromeachphoto(e.g.,numberofpeopleinthephoto,genderandethnicdiversity,agedistribution,whetherparticipantsare
smilingetc.) tomakeacompetingprediction.
Accuracyandindistinguishabilityinvisualprediction. AsintheX-raydiagnosistask,wefirstcomparetheperformance
ofhumansubjectstothatofthefiveoff-the-shelfpredictivealgorithmsconsideredinSaveskietal.(2021). Weagainfind
thatalthoughhumansfailtooutperformthebestpredictivealgorithms,theirpredictionsaddsignificantpredictivevalueon
instanceswherethealgorithmsagreeonapositivelabel. Asourresultsaresimilartothoseintheprevioussection,wedefer
themtoAppendixD.Wenowusethistasktoillustrateanotherfeatureofourframework,whichistheabilitytoincorporate
humanjudgmentintoasubstantiallyricherclassofmodels.
Multicalibrationoveraninfiniteclass. Whileourpreviousresultsillustratethathumanjudgmentcancomplementasmall,
fixedsetofpredictivealgorithms,it’spossiblethataricherclasscouldobviatetheneedforhumanexpertise. Toexplorethis,
wenowconsideraninfinitelylargebutnonetheless‘simple’classofshallow(depth≤5)decisiontrees. Wedenotethis
classbyFDT5.
Asinprevioussections,ourfirststepwillbetolearnapartitionwhichismulticalibratedwithrespecttothisclass.
However,becauseFDT5isinfinitelylarge,thesimpleapproachweusedinpriorexperiments—enumeratingeachf ∈FDT5
andclusteringobservationsaccordingtotheirpredictions—isinfeasible. Instead,weapplytheboostingalgorithmproposed
inGlobus-Harrisetal.(2023)toconstructabinaryclassifierh : X → {0,1}suchthatnof ∈ FDT5 cansubstantially
improveonthesquarederrorofhwithineitherofitslevelsets{x|h(x)=1}and{x|h(x)=0}.9 Weplotthecorrelation
ofthehumansubjects’predictionswiththetruelabelwithineachoftheselevelsetsinFigure3.
Figure3highlightsakeyinsightprovidedbyourframework. Ononehand,thepredictionsmadebyharemoreaccurateout
ofsample(75.2%)thaneventhebestperformingcohortofhumansubjects(67.3%). Nonetheless,thepredictionsmadeby
allfourcohortsofhumansubjectsaresubstantiallycorrelatedwiththeoutcomewithinbothlevelsetsofh. Thissuggests
thathumansprovideinformationwhichcannotbeextractedfromthedatabyanyf ∈FDT5. Whilewefocusontheclass
ofshallowdecisiontreesforconcreteness, ourapproachappliestoanyfunctionclassforwhichitisfeasibletolearna
multicalibratedpartition.
8Wefocusonstudy2inSaveskietal.(2021);study1analyzesthesametaskwithonlytwotreatmentarms.
9Thealgorithmterminateswhennof ∈FDT5canreducesquarederrorwithinthelevelsetsofh.Althoughtheclassofbinary-valued
decisiontreesisnotclosedunderaffinetransformations(seeLemma4.2),thispartitioncapturesthespiritofourmainresult:whileno
f ∈FDT5canimproveaccuracywithineitherlevelset,humansprovidesubstantialpredictiveinformationwithinbothofthem.
9APREPRINT
0.4
0.3
predictor
random baseline
0.2 multicalibrated predictor
human (control)
human (4 examples)
0.1 human (8 examples)
human (12 examples)
0.0
Negative Positive
Model Prediction Bins
Figure3: HumanpredictionswithinthelevelsetsofapredictorhwhichismulticalibratedwithrespecttoFDT5. The‘negative’bin
istheset{x|h(x)=0},andthe‘positive’binis{x|h(x)=1}. Becausehisconstantwitheachofthesebins,itisconditionally
uncorrelatedwiththeoutcome.Arandompermutationofthetruelabelsisincludedasabaseline.
6 Robustnesstononcompliance
Wehavethusfarfocusedonhowtoincorporatehumanjudgmentintoalgorithmicpredictions(e.g.,viathechoiceofβ and
γ inTheorem3.1). However,manydecisionsupporttoolsareinsteaddeployedinsettingswheretheuserdecideswhen
todefertothealgorithm. Forexample,aprognosticriskscoremaybedeployedtomanyhospitals,whicheachemploy
differentnormsandpoliciesgoverningitsuse(Lebovitzetal.,2022). Althoughitistemptingtoignorethesepoliciesand
simplyprovideuserswiththemostaccuratepredictor,Bansaletal.(2020a)arguethatthisapproachissuboptimalifusers
onlyselectivelycomplywiththealgorithm’srecommendations. WeformalizethisargumentviaLemmaA.3inAppendixA,
wherewedemonstratethatlearningapredictorwhichisrobusttoarbitrary‘noncompliance’patternsisinfeasible. Weshow
nexthoweverthat,givenapartitionwhichismulticalibratedovertheclassofpossibleusercompliancepatterns,wecan
learnpredictorswhichremainoptimalevenwhenusersonlyselectivelyadoptthealgorithm’srecommendations. Thisisa
naturalwayofimposingstructureonuserswithoutfullymodelingtheirbehavior—forexample,wecouldassumethatthe
user’sdecisiontocomplywiththealgorithmcanbemodeledbysome‘simple’rule(e.g.,ashallowdecisiontree),butone
whichwedonotknowex-ante.
Theorem6.1. LetΠbeaclassofbinarycompliancepolicies,where,forπ ∈Π,π(x)=1indicatesthattheusercomplies
withthealgorithmatX =x. LetF beaclassofpredictorsandlet{S } beapartitionwhichisα-multicalibrated
k k∈[K]
withrespecttoΠandtheproductclass{f(X)π(X)|f ∈F,π ∈Π}. Then,∀f ∈F,π ∈Π,k ∈[K]:
E [(Y −E [Y])2 |π(X)=1] (14)
k k
6α
≤E [(Y −f(X))2 |π(X)=1]+ . (15)
k P (π(X)=1)
k
Tounpackthisresult,observethat(14)isthesquarederrorincurredbytheconstantpredictionE [Y]withineachindistin-
k
guishablesubsetwhentheuserdeferstothealgorithm. Importantly,althoughthispredictiondoesnotdependonthepolicy
π(·),itremainscompetitivewiththesquarederrorincurredbyanyf ∈ F foranypolicy(15). Theapproximationerror
dependsonboththequalityofthepartitionαandtherateofcomplianceP (π(X)=1). Unsurprisingly,theboundbecomes
k
vacuousasP (π(X)=1)goesto0(wecannothopetolearnanythingonarbitrarilyraresubsets). Thisisconsistentwith
k
ourinterpretationofπ(·)however,astheperformanceofthealgorithmmatterslittleifthedecisionmakerignoresnearlyall
recommendations.
ThisresultiscomplementarytothoseinSection3—ratherthanlearningtoincorporatefeedbackfromasingleexpert,
wecaninsteadlearnasinglepredictorwhichis(nearly)optimalforarichclassofdownstreamuserswhosebehavioris
modeledbysomeπ ∈Π.
10
tneicfifeoC
noitalerroCAPREPRINT
7 Discussionandlimitations
Inthisworkweproposeaframeworkforenablinghuman/AIcollaborationinpredictiontasks. Underthisframework,we
developafamilyofalgorithmsforincorporatinghumanjudgmentintoalgorithmicpredictions,andextendourresultsto
coversettingsinwhichusersonlyselectivelyadoptalgorithmicrecommendations. Beyondimprovingpredictionaccuracy,
wearguethatthisframingclarifieswhenandwhyhumanjudgmentcanimprovealgorithmicpredictions.
Akeylimitationofourworkisasomewhatnarrowfocusonminimizingmeansquarederrorinpredictiontasks. This
requiresthatinputscomefromaawell-defined(andstationary)distribution,andfailstomodeldecisionmakerswithricher
preferences(e.g.,ensuringfairnessaswellasaccuracy). Furthermore,wecautionthatevenincontextswithawell-defined
algorithmicobjective,humandecisionmakerscanbecriticalforensuringinterpretabilityandaccountability. Atatechnical
level,ourresultsrelyontheabilitytoefficientlylearnpartitionswhicharemulticalibratedwithrespecttothefunctionclass
ofinterest. WhilewegiveconditionsunderwhichthisisfeasibleinSection4,findingsuchpartitionscanbeprohibitively
expensive(intermsoftrainingdataand/orcomputationalresources)forrichfunctionclasses. Despitetheselimitations,we
hopeourworkprovidesbothconceptualandmethodologicalinsightsforenablingeffectivehuman/AIcollaboration.
8 Acknowledgements
ThisworkisgenerouslysupportedbyaStephenA.SchwarzmannCollegeofComputingSeedGrant,withfundsprovided
byAndrewW.HoustonandDropboxInc. WealsothankSarahCen,DeanEckles,NikhilGarg,ChristopherHays,Adam
TaumanKalai,AnnieLiang,SendhilMullainathan,EmmaPierson,AsheshRambachan,DennisShung,SeanSinclair,Jann
Spiess,andKeyonVafaforinvaluablefeedbackanddiscussions.
References
Agrawal,A.,Gans,J.,andGoldfarb,A.(2018). Exploringtheimpactofartificialintelligence: Predictionversusjudgment.
Technicalreport,NationalBureauofEconomicResearch,Cambridge,MA.
Alur,R.,Laine,L.,Li,D.K.,Raghavan,M.,Shah,D.,andShung,D.(2023). Auditingforhumanexpertise.
Arnold,D.,Dobbie,W.,andHull,P.(2020). Measuringracialdiscriminationinbaildecisions. Technicalreport,National
BureauofEconomicResearch,Cambridge,MA.
Bansal,G.,Nushi,B.,Kamar,E.,Horvitz,E.,andWeld,D.S.(2020a). Isthemostaccurateaithebestteammate? optimizing
aiforteamwork.
Bansal,G.,Wu,T.,Zhou,J.,Fok,R.,Nushi,B.,Kamar,E.,Ribeiro,M.T.,andWeld,D.S.(2020b). Doesthewholeexceed
itsparts? theeffectofaiexplanationsoncomplementaryteamperformance.
Bastani,H.,Bastani,O.,andSinchaisri,W.P.(2021). Improvinghumandecision-makingwithmachinelearning.
Beede, E., Baylor, E. E., Hersch, F., Iurchenko, A., Wilcox, L., Ruamviboonsuk, P., and Vardoulakis, L. M. (2020).
A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy.
Proceedingsofthe2020CHIConferenceonHumanFactorsinComputingSystems.
Benz,N.L.C.andRodriguez,M.G.(2023). Human-alignedcalibrationforai-assisteddecisionmaking.
Camerer,C.andJohnson,E.(1991). Theprocess-performanceparadoxinexpertjudgment: Howcanexpertsknowsomuch
andpredictsobadly? InEricsson,A.andSmith,J.,editors,TowardaGeneralTheoryofExpertise: ProspectsandLimits.
CambridgeUniversityPress.
Chicco,D.andJurman,G.(2020).Theadvantagesofthematthewscorrelationcoefficient(MCC)overF1scoreandaccuracy
inbinaryclassificationevaluation. BMCGenomics,21(1):6.
Cowgill,B.(2018). Biasandproductivityinhumansandalgorithms: Theoryandevidencefromresumescreening.
Cowgill,B.andStevenson,M.T.(2020). Algorithmicsocialengineering. AEAPap.Proc.,110:96–100.
Currie,J.andMacLeod,W.B.(2017). Diagnosingexpertise: Humancapital,decisionmaking,andperformanceamong
physicians. J.LaborEcon.,35(1):1–43.
11APREPRINT
Dawes,R.M.(1971). Acasestudyofgraduateadmissions: Applicationofthreeprinciplesofhumandecisionmaking. Am.
Psychol.,26(2):180–188.
Dawes,R.M.,Faust,D.,andMeehl,P.E.(1989). Clinicalversusactuarialjudgment. Science,243(4899):1668–1674.
De,A.,Okati,N.,Zarezade,A.,andGomez-Rodriguez,M.(2020). Classificationunderhumanassistance.
De-Arteaga,M.,Fogliato,R.,andChouldechova,A.(2020). Acaseforhumans-in-the-loop: Decisionsinthepresenceof
erroneousalgorithmicscores.
Dietvorst,B.,Simmons,J.,andMassey,C.(2018). Overcomingalgorithmaversion: Peoplewilluseimperfectalgorithmsif
theycan(evenslightly)modifythem. ManagementScience,64:1155–1170.
Donahue,K.,Chouldechova,A.,andKenthapadi,K.(2022). Human-algorithmcollaboration: Achievingcomplementarity
andavoidingunfairness.
Dwork,C.,Kim,M.P.,Reingold,O.,Rothblum,G.N.,andYona,G.(2020). Outcomeindistinguishability.
Foster,D.P.andVohra,R.V.(1998). Asymptoticcalibration. Biometrika,85(2):379–390.
Globus-Harris,I.,Harrison,D.,Kearns,M.,Roth,A.,andSorrell,J.(2023). Multicalibrationasboostingforregression.
Gonzalez,T.F.(1985). Clusteringtominimizethemaximuminterclusterdistance. Theor.Comput.Sci.,38:293–306.
Gopalan,P.,Kalai,A.T.,Reingold,O.,Sharan,V.,andWieder,U.(2021). Omnipredictors.
Grove,W.M.,Zald,D.H.,Lebow,B.S.,Snitz,B.E.,andNelson,C.(2000). Clinicalversusmechanicalprediction: a
meta-analysis. PsycholAssess,12(1):19–30.
Hébert-Johnson,U.,Kim,M.,Reingold,O.,andRothblum,G.(2018).Multicalibration:Calibrationforthe(computationally-
identifiable)masses. InInternationalConferenceonMachineLearning,pages1939–1948.PMLR.
Irvin,J.,Rajpurkar,P.,Ko,M.,Yu,Y.,Ciurea-Ilcus,S.,Chute,C.,Marklund,H.,Haghgoo,B.,Ball,R.,Shpanskaya,K.,
Seekins,J.,Mong,D.A.,Halabi,S.S.,Sandberg,J.K.,Jones,R.,Larson,D.B.,Langlotz,C.P.,Patel,B.N.,Lungren,
M.P.,andNg,A.Y.(2019). Chexpert: Alargechestradiographdatasetwithuncertaintylabelsandexpertcomparison.
Keswani,V.,Lease,M.,andKenthapadi,K.(2021). Towardsunbiasedandaccuratedeferraltomultipleexperts. Proceedings
ofthe2021AAAI/ACMConferenceonAI,Ethics,andSociety.
Keswani,V.,Lease,M.,andKenthapadi,K.(2022). Designingclosedhuman-in-the-loopdeferralpipelines.
Klaes, M. and Sent, E.-M. (2005). A conceptual history of the emergence of bounded rationality. Hist. Polit. Econ.,
37(1):27–59.
Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J., and Mullainathan, S. (2017). Human decisions and machine
predictions.
Kleinberg,J.andRaghavan,M.(2021). Algorithmicmonocultureandsocialwelfare. ProceedingsoftheNationalAcademy
ofSciences,118(22):e2018340118.
Kuncel,N.R.,Klieger,D.M.,Connelly,B.S.,andOnes,D.S.(2013a). Mechanicalversusclinicaldatacombinationin
selectionandadmissionsdecisions: ameta-analysis. JApplPsychol,98(6):1060–1072.
Kuncel,N.R.,Klieger,D.M.,Connelly,B.S.,andOnes,D.S.(2013b). Mechanicalversusclinicaldatacombinationin
selectionandadmissionsdecisions: ameta-analysis. JApplPsychol,98(6):1060–1072.
Lebovitz,S.,Lifshitz-Assaf,H.,andLevina,N.(2022). Toengageornottoengagewithaiforcriticaljudgments: How
professionalsdealwithopacitywhenusingaiformedicaldiagnosis. OrganizationScience,33.
Madras,D.,Pitassi,T.,andZemel,R.S.(2018). Predictresponsibly: Improvingfairnessandaccuracybylearningtodefer.
InAdvancesinNeuralInformationProcessingSystems31,pages6150–6160.
Mozannar,H.andSontag,D.A.(2020). Consistentestimatorsforlearningtodefertoanexpert. InInternationalConference
onMachineLearning.
Mullainathan,S.andObermeyer,Z.(2019). Diagnosingphysicianerror: Amachinelearningapproachtolow-valuehealth
care. Technicalreport,NationalBureauofEconomicResearch,Cambridge,MA.
12APREPRINT
Okati,N.,De,A.,andGomez-Rodriguez,M.(2021). Differentiablelearningundertriage.
Raghu,M.,Blumer,K.,Corrado,G.,Kleinberg,J.,Obermeyer,Z.,andMullainathan,S.(2019). Thealgorithmicautomation
problem: Prediction,triage,andhumaneffort.
Rajpurkar,P.,Joshi,A.,Pareek,A.,Ng,A.Y.,andLungren,M.P.(2021). Chexternal: Generalizationofdeeplearning
modelsforchestx-rayinterpretationtophotosofchestx-raysandexternalclinicalsettings.
Rambachan,A.(2022). Identifyingpredictionmistakesinobservationaldata.
Rastogi,C.,Leqi,L.,Holstein,K.,andHeidari,H.(2022). Ataxonomyofhumanandmlstrengthsindecision-makingto
investigatehuman-mlcomplementarity.
Saveski, M., Awad, E., Rahwan, I., and Cebrian, M. (2021). Algorithmic and human prediction of success in human
collaborationfromvisualfeatures. ScientificReports,11:2756.
Simon,H.A.(1957). ModelsofMan: SocialandRational. Wiley.
Steyvers,M.,Tejeda,H.,Kerrigan,G.,andSmyth,P.(2022). Bayesianmodelingofhuman-AIcomplementarity. Proc.Natl.
Acad.Sci.U.S.A.,119(11):e2111547119.
Toups, C., Bommasani, R., Creel, K. A., Bana, S. H., Jurafsky, D., and Liang, P. (2023). Ecosystem-level analysis of
deployedmachinelearningrevealshomogeneousoutcomes. InAdvancesinNeuralInformationProcessingSystems37.
Tversky,A.andKahneman,D.(1974). Judgmentunderuncertainty: Heuristicsandbiases. Science,185(4157):1124–1131.
Wilder,B.,Horvitz,E.,andKamar,E.(2020). Learningtocomplementhumans.
13APREPRINT
A Proofsandadditionaltechnicalresults
Inthissectionwepresentproofsofourmainresults. ProofsofauxiliarylemmasaredeferredtoAppendixB.
A.1 OmittedproofsfromSection3
LemmaA.1. Thefollowingsimplelemmawillbeusefulinoursubsequentproofs. LetX ∈ {0,1}beabinaryrandom
variable. Thenforanyotherrandomvariable,Y:
Cov(X,Y) (16)
=P(X =1)(E[Y |X =1]−E[Y]) (17)
=P(X =0)(E[Y]−E[Y |X =0]) (18)
Thisisexactlycorollary5.1inGopalanetal.(2021). WeprovidetheproofinAppendixB.
ProofofTheorem3.1
Proof. Awellknownfactaboutunivariatelinearregressionisthatthecoefficientofdetermination(orr2)isequaltothe
squareofthePearsoncorrelationcoefficientbetweentheregressorandtheoutcome(orr). Inourcontext,thismeansthat
withinanyindistinguishablesubsetS wehave:
k
(cid:20)(cid:16) (cid:17)2(cid:21)
E Y −γ∗−β∗Yˆ
k k k Cov (Y,Yˆ)2
1− = k (19)
E k(cid:104) (Y −E k[Y])2(cid:105) Var k(Y)Var k(Yˆ)
(cid:104) (cid:105) (cid:20)(cid:16) (cid:17)2(cid:21) Cov (Y,Yˆ)2
⇒E (Y −E [Y])2 −E Y −γ∗−β∗Yˆ = k (20)
k k k j j Var(Yˆ)
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:104) (cid:105) Cov (Y,Yˆ)2
⇒E Y −γ∗−β∗Yˆ =E (Y −E [Y])2 − k (21)
k j j k k Var(Yˆ)
(cid:104) (cid:105)
≤E (Y −E [Y])2 −4Cov (Y,Yˆ)2 (22)
k k k
Where(22)isanapplicationofPopoviciu’sinequalityforvariances,andmakesuseofthefactthatYˆ ∈[0,1]almostsurely.
Wecanthenobtainthefinalresultbyapplyingthefollowinglemma,whichextendsthemainresultinGopalanetal.(2021).
WeprovideaproofinAppendixB,butfornowsimplystatetheresultasLemmaA.2below.
LemmaA.2. Let{S } beanα-multicalibratedpartitionwithrespecttoareal-valuedfunctionclassF ={f :X →
k k∈[K]
[0,1]}andtargetoutcomeY ∈[0,1]. Forallf ∈F andk ∈[K],itfollowsthat:
(cid:104) (cid:105) (cid:104) (cid:105)
E (Y −E[Y])2 ≤E (Y −f(X))2 +2α (23)
k k
We provide further discussion of the relationship between Lemma A.2 and the main result of Gopalan et al. (2021) in
AppendixA.4below.
Chaininginequalities(23)and(22)yieldsthefinalresult:
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:104) (cid:105)
E Y −γ∗−β∗Yˆ ≤E (Y −f(X))2 +2α−4Cov (Y,Yˆ)2 ∀ f ∈F (24)
k j j k k
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:104) (cid:105)
⇒E Y −γ∗−β∗Yˆ +4Cov (Y,Yˆ)2 ≤E (Y −f(X))2 +2α ∀ f ∈F (25)
k j j k k
14APREPRINT
ProofofCorollary3.1
Proof. Observethat,becausetheconditionalexpectationfunctionE [Y |Yˆ]minimizessquarederrorwithrespecttoall
k
univariatefunctionsofYˆ,wemusthave:
(cid:104) (cid:105) (cid:104) (cid:105)
E (Y −E [Y |Yˆ])2 ≤E (Y −γ∗−β∗Yˆ)2 (26)
S S S
Whereγ∗ ∈ R,β∗ ∈ RarethepopulationregressioncoefficientsobtainedbyregressionY ong(H)asinTheorem3.1.
Thisfurtherimplies,bytheapproximateBayes-optimalitycondition(5):
(cid:104) (cid:105) (cid:104) (cid:105)
E (Y −g(Yˆ))2 ≤E (Y −γ∗−β∗Yˆ)2 +η (27)
S S k k
TheproofthenfollowsimmediatelyfromthatofTheorem3.1.
ProofofCorollary3.2
Proof. Theproofisalmostimmediate. Letγ∗,β∗ ∈RbethepopulationregressioncoefficientsobtainedbyregressingY
ong(H)withinS (asinTheorem3.1;theonlydifferenceisthatweconsiderasingleindistinguishablesubsetratherthana
multicalibratedpartition). Thisfurtherimplies,bytheapproximatecalibrationcondition(8):
E (cid:2) (Y −g(H))2(cid:3) ≤E (cid:2) (Y −γ∗−β∗g(H))2(cid:3) +η (28)
S S k k
TheproofthenfollowsfromthatofTheorem3.1,replacingYˆ withg(H).
ProofofTheorem3.2
Proof. Fixanyk ∈[K].
(cid:12) (cid:12)
(cid:12)Cov (Y,Yˆ)(cid:12) (29)
(cid:12) k (cid:12)
(cid:12) (cid:12)
=(cid:12)E [Cov (Y,Yˆ |f˜(X)]+Cov (E [Y |f˜(X)],E [Yˆ |f˜(X)])(cid:12) (30)
(cid:12) k k k k k k k k (cid:12)
(cid:12) (cid:12)
=(cid:12)Cov (E[Y |f˜(X)],E [Yˆ |f˜(X)])(cid:12) (31)
(cid:12) k k k k (cid:12)
(cid:113)
≤ Var(E [Y |f˜(X)])Var (E[Yˆ |f˜(X)]) (32)
k k k k
1(cid:113)
≤ Var (E [Y |f˜(X)]) (33)
2 k k k
Where(30)isthelawoftotalcovariance, (31)followsfromtheassumptionthatY ⊥⊥ Yˆ | f˜(X),X ∈ S , (32)isthe
k k
Cauchy-Schwarz inequality and (33) applies Popoviciu’s inequality to bound the variance of E[Yˆ | f˜(X)] (which is
k
assumedtoliein[0,1]almostsurely).
15APREPRINT
WenowfocusonboundingVar (E [Y |f˜(X)]). Recallthatbyassumption,|Cov (Y,f˜(X))|≤α,soweshouldexpect
k k k k k
thatconditioningonf˜(X)doesnotchangetheexpectationofY bytoomuch.
k
Var (E [Y |f˜(X)]) (34)
k k k
=E [(E [Y |f˜(X)]−E [E [Y |f˜(X)]])2] (35)
k k k k k k
=E [(E [Y |f˜(X)]−E [Y])2] (36)
k k k k
=P (f˜(X)=1)(E [Y |f˜(X)=1]−E [Y])2+P (f˜(X)=0)(E [Y |f˜(X)=0]−E [Y])2
k k k k k k k k k k
(37)
(cid:12) (cid:12) (cid:12) (cid:12)
≤P (f˜(X)=1)(cid:12)E [Y |f˜(X)=1]−E [Y](cid:12)+P (f˜(X)=0)(cid:12)E [Y |f˜(X)=0]−E [Y](cid:12)
k k (cid:12) k k k (cid:12) k k (cid:12) k k k (cid:12)
(38)
WherethelaststepfollowsbecauseY isassumedtobeboundedin[0,1]almostsurely. ApplyingLemmaA.1to(38)yields:
(cid:12) (cid:12)
Var (E [Y |f˜(X)])≤(cid:12)2Cov (Y,f˜(X))(cid:12)≤2α (39)
k k k (cid:12) k k (cid:12)
WherethesecondinequalityfollowsbecauseouranalysisisconditionalonX ∈S forsomeα-indistinguishablesubsetS .
k k
Plugging(39)into(33)completestheproof.
A.2 OmittedproofsfromSection4
ProofofLemma4.1
Proof. Wewanttoshow|Cov(Y,f(X) | X ∈ S)| ≤ αforallf ∈ F andsomeS suchthatmax Var(f(X) | X ∈
f∈F
S)≤4α2.
Fixanyf ∈F. Wethenhave:
|Cov(Y,f(X)|X ∈S)| (40)
(cid:112)
≤ Var(Y |X ∈S)Var(f(X)|X ∈S) (41)
(cid:114)
1
≤ ×Var(f(X)|X ∈S) (42)
4
(cid:114)
1
≤ ×4α2 (43)
4
=α (44)
Where(41)istheCauchy-Schwarzinequality,(42)isPopoviciu’sinequalityandmakesuseofthefactthatY isboundedin
[0,1]byassumption,and(43)usestheassumptionthatmax Var(f(X)|X ∈S)≤4α2.
f∈F
CorollaryA.1. LetF beaclassofpredictorswhoserangeisboundedwithinsomeS ⊆X. Thatis,forallf ∈F:
maxf(x)−minf(x′)≤4α (45)
x∈S x′∈S
16APREPRINT
ThenS isα-indistinguishablewithrespecttoF.
ProofofCorollaryA.1
Proof. Wewanttoshowthat∀ f ∈F:
|Cov(Y,f(X)|X ∈S)|≤α (46)
Byassumption,f(X)isboundedinarangeof4αwithinS. FromthisitfollowsbyPopoviciu’sinequalityforvariancesthat
∀ f ∈F:
(4α)2
Var(f(X)|X ∈S )≤ =4α2 (47)
j 4
TheproofthenfollowsfromLemma4.1.
CorollaryA.2. LetFLip(L,d)bethesetofL-Lipschitzfunctionswithrespecttosomedistancemetricd(·,·)onX. Thatis:
|f(x)−f(x′)|≤Ld(x,x′) ∀ f ∈FLip(L,d) (48)
Let{S } forK ⊆Nbesome(4α/L)-netonX withrespecttod(·,·). Then{S } isα-multicalibratedwithrespect
k k∈K k k∈K
toFLip(L,d).
ProofofCorollaryA.2
Proof. Wewanttoshowthat∀ f ∈FLip(L,d),k ∈K:
|Cov (Y,f(X))|≤α (49)
k
BecauseS ispartofa4α/L-net,thereexistssomem∈[0,1]suchthatP(f(X)∈[m,m+4α]|X ∈S )=1;thatis,
k k
f(X)isboundedalmostsurelyinsomeintervaloflength4α. FromthisitfollowsbyPopoviciu’sinequalityforvariances
that∀ f ∈FLip(L,d),k ∈K:
(4α)2
Var (f(X))≤ =4α2 (50)
k 4
TheremainderoftheprooffollowsfromLemma4.1.
ProofofLemma4.2
Proof. TheresultfollowsLemma3.3andLemma6.8inGlobus-Harrisetal.(2023). Weprovideasimplifiedproofbelow,
adapted to our notation. We’ll use E [·] to denote the expectation conditional on the event that {h(X) = v} for each
v
v ∈R(h). WeuseCov (·,·)analogously.
v
17APREPRINT
Ourproofwillproceedintwosteps. Firstwe’llshowthat:
∀v ∈R(h),f ∈F,E [(h(X)−Y)2−(f(X)−Y)2]<α2 (51)
v
⇒E [f(X)(Y −v)]<α ∀ v ∈R(h),f ∈F˜ (52)
v
Thisconditionstatesthatiftheredoesnotexistsomevintherangeofhwherethebestf ∈F improvesonthesquared
errorincurredbyhbymorethanα2,thenthepredictorh(·)isα-multicalibratedinthesenseofGlobus-Harrisetal.(2023)
withrespecttotheconstrainedclassF˜. Wethenshowthatthelevelsetsofapredictorh(·)whichsatisfies(52)forma
multicalibratedpartition(Definition3.2). Thatis:
E [f(X)(Y −v)]≤α ∀v ∈R(h),f ∈F˜ ⇒Cov (f(X),Y)≤2α ∀v ∈R(h),f ∈F˜ (53)
v v
Thatis,thelevelsetsS ={x|h(x)=v}forma(2α)-multicalibratedpartitionwithrespecttoF˜.
v
First,we’llprovethecontrapositiveof(52). ThisproofisadaptedfromthatofLemma3.3inGlobus-Harrisetal.(2023).
Supposethereexistssomev ∈R(h)andf ∈F˜suchthat
E [f(X)(Y −v)]≥α (54)
v
Thenthereexistsf′ ∈F suchthat:
E [(f′(X)−Y)2−(h(X)−Y)2]≥α2 (55)
v
Proof: letη = α andf′ =v+ α f(X)=v+ηf(X). Then:
E v[f(X)2] E v[f(X)2]
E (cid:2) (h(X)−Y)2−(f′(X)−Y)2(cid:3) (56)
v
=E (cid:2) (v−Y)2−(v+ηf(X)−Y)2(cid:3) (57)
v
=E (cid:2) v2+Y2−2Yv−v2−η2f(X)2−Y2−2vηf(X)+2vY +2ηf(X)Y(cid:3) (58)
v
=E (cid:2) 2ηf(X)(Y −v)−η2f(X)2(cid:3) (59)
v
α2
=E [2ηf(X)(Y −v)]− (60)
v E [f(X)2]
v
α2
≥2ηα− (61)
E [f(X)2]
v
α2
= (62)
E [f(X)2]
v
≥α2 (63)
Wherethelaststepfollowsbecausewetookf ∈ F˜,thesubsetofthefunctionclassF whichonlytakesvaluesin[0,1].
ThisimpliesthatifinsteadE [(f′(X)−Y)2−(h(X)−Y)2]<α2forallv ∈R(h),f′ ∈F,thenE [f(X)(Y −v)]<α
v v
forallv ∈ R(h)andf ∈ F˜. Nextweprove(53); thatis,E [f(X)(Y −v)] < αforallv ∈ R(h)andf ∈ F˜ implies
v
|Cov (f(X),Y)|≤2αforallv ∈R(h),f ∈F˜.
v
18APREPRINT
TheproofisadaptedfromthatofLemma6.8inGlobus-Harrisetal.(2023);ourproofdiffersbeginningat(71). Fixsome
f ∈F˜andv ∈R(h). Byassumptionwehave,forallv ∈R(h)andf ∈F˜,
E [f(X)(Y −v)]<α (64)
v
Thenwecanshow:
|Cov (f(X),Y)| (65)
v
=|E [f(X)Y]−E [f(X)]E [Y]| (66)
v v v
=|E [f(X)Y]−E [f(X)]E [Y]+vE [f(X)]−vE [f(X)]| (67)
v v v v v
=|E [f(X)(Y −v)]+E [f(X)](v−E [Y])| (68)
v v v
≤|E [f(X)(Y −v)]|+|E [f(X)](v−E [Y])| (69)
v v v
=|E [f(X)(Y −v)]|+|E [f(X)](E [Y]−v)| (70)
v v v
≤α+|E [f(X)](E [Y]−v)| (71)
v v
Wherethelaststepfollowsfromtheassumption(64). Now,letf′(X)≡E [f(X)]betheconstantfunctionwhichtakesthe
v
valueE [f(X)]. Wecanwrite(71)asfollows:
v
α+|E [f(X)](E [Y]−v)|=α+|f′(X)(E [Y]−v)| (72)
v v v
=α+|E [f′(X)(Y −v)]| (73)
v
BecauseF isclosedunderaffinetransformations,itcontainsallconstantfunctions,andthus,f′(X)∈F. F˜,bydefinition,
isthesubsetofF whoserangeliesin[0,1]. Becausef ∈F˜,itmustbethatE [f(X)]∈[0,1]andthus,f′ ∈F˜. So,wecan
v
againinvoke(64)toshow:
α+|E [f′(X)(Y −v)]|≤2α (74)
v
Whichcompletestheproof.
A.3 OmittedproofsfromSection6
LemmaA.3. LetF besomeclassofpredictorswhichmapacountableinputspaceX to[0,1]. Weinterpretacompliance
policyπ :X →[0,1]suchthatπ(x)=1indicatesthattheusercomplieswiththealgorithm’srecommendationatX =x.
Forallf ∈F,unlessf =E[Y |X]almosteverywhere,thenthereexistsadeferralpolicyπ :X →{0,1}andconstant
c∈[0,1]suchthat:
E[(Y −f(X))2 |π(X)=1]>E[(Y −c)2 |π(X)=1] (75)
LemmaA.3indicatesthatforanypredictorf whichisnottheBayesoptimalpredictor,thereexistsacompliancepolicy
which causes it to underperform a constant prediction on the instances for which it is ultimately responsible. Because
learningtheBayesoptimalpredictorfromafinitesampleofdataisgenerallyinfeasible,thisindicatesthatapredictorcannot
reasonablybemaderobusttoanarbitrarydeferralpolicy. Theproof,whichweprovidebelow,isintuitive: thedecision
makercansimplychoosetocomplyonexactlythoseinstanceswheref performspoorly.
19APREPRINT
ProofofLemmaA.3
Proof. Let f ∈ F be any model and let S ⊆ X be a subset such that (1) P (S) > 0 (2) the Bayes optimal predictor
X
E[Y | X]isconstantwithinS and(3)f(X) ̸= E[Y | X = x]forallx ∈ S. Suchasubsetmustexistbyassumption. It
followsimmediatelythatchoosing
(cid:40)
1ifx∈S
π(x)=
0otherwise
sufficestoensurethatf(X)underperformstheconstantpredictionc =E[Y |X ∈S]onthesubsetwhichπdelegatesto
S
f. ThisimpliesthatevenifF includestheclassofconstantpredictors{f(X)=c|c∈R}—perhapsthesimplestpossible
classofpredictors—thenwecannothopetofindsomef∗ ∈F whichissimultaneouslyoptimalforanychoiceofdeferral
policy.
ProofofTheorem6.1
Proof. We start with the assumption that {S } is α-multicalibrated with respect to Π and the product class
k k∈[K]
{f(X)π(X)|f ∈F,π ∈Π}. Thatis,bothofthefollowinghold:
|Cov (Y,π(X))|≤α∀π ∈Π,k ∈[K] (76)
k
|Cov (Y,f(X)π(X))|≤α∀f ∈F,π ∈Π,k ∈[K] (77)
k
First, we’ll show that this implies that the covariance of Y and f(X) is bounded even conditional on compliance. To
streamlinepresentationwestatethisasaseparatelemma;theproofisprovidedfurtherbelow.
LemmaA.4. GiventhesetupofTheorem6.1,thefollowingholdsforallk ∈[K],f ∈F andπ ∈Π:
2α
|Cov (Y,f(X)|π(X)=1)|≤ (78)
k P (π(X)=1)
k
WeprovideaproofinAppendixB.ByLemmaA.2,LemmaA.4implies,forallk ∈[K],f ∈F andπ ∈Π:
(cid:104) (cid:105) (cid:104) (cid:105) 4α
E (Y −E [Y |π(X)=1])2 |π(X)=1 ≤E (Y −f(X))2 |π(X)=1 + (79)
k k k P(π(X)=1)
Thisisclosetowhatwewanttoprove,exceptthatthepredictionE [Y |π(X)=1]dependsonthechoiceofthepolicyπ(·).
k
We’llarguethatby(76),E [Y |π(X)=1]≈E [Y]. Indeed,becauseπ(·)isbinary,wecanapplyLemmaA.1torecover:
k k
|Cov(π(X),Y)|=P (π(X)=1)|E [Y |π(X)=1]−E [Y]| (80)
k k k
α
⇒|E [Y |π(X)=1]−E [Y]|≤ (81)
k k P (π(X)=1)
k
WerewritetheLHSof (79)tomakeuseofthisidentityasfollows:
20APREPRINT
(cid:104) (cid:105)
E (Y −E [Y |π(X)=1])2 |π(X)=1 (82)
k k
(cid:104) (cid:105)
=E ((Y −E [Y])+(E [Y]−E [Y |π(X)=1]))2 |π(X)=1 (83)
k k k k
=E (cid:2)(cid:0) (Y −E [Y])2+(E [Y]−E [Y |π(X)=1])2+2(Y −E [Y])(E [Y]−E [Y |π(X)=1])(cid:1) |π(X)=1(cid:3)
k k k k k k k
(84)
≥E (cid:2)(cid:0) (Y −E [Y])2+2(Y −E [Y])(E [Y]−E [Y |π(X)=1])(cid:1) |π(X)=1(cid:3) (85)
k k k k k
=E (cid:2) (Y −E [Y])2 |π(X)=1(cid:3) +2(E [Y]−E [Y |π(X)=1])(E [Y |π(X)=1]−E [Y]) (86)
k k k k k k
≥E (cid:2) (Y −E [Y])2 |π(X)=1(cid:3) − 2α (87)
k k P (π(X)=1)
k
Wherethelaststepfollowsbyobservingthateither(1)E [Y]=E [Y |π(X)=1]or(2)exactlyoneof(E [Y]−E [Y |
k k k k
π(X)=1])or(E [Y |π(X)=1]−E [Y])isstrictlypositive. AssumethatE [Y]̸=E [Y |π(X)=1];otherwisethe
k k k k
boundfollowstrivially. WeboundthepositivetermbyrecallingthatY liesin[0,1],andweboundthenegativetermby
applying(81). Thus,theproductofthesetwotermsisatleast −α . Finally,combining(87)with(79)completesthe
P k(π(X)=1)
proof.
A.4 RelatingLemmaA.2toOmnipredictors(Gopalanetal.,2021)
InthissectionwecompareLemmaA.2tothemainresultofGopalanetal.(2021). WhilethemainresultofGopalanetal.
(2021)appliesbroadlytoconvex,Lipschitzlossfunctions,wefocusonthespecialcaseofminimizingsquarederror. In
thiscase,weshowthatLemmaA.2extendsthemainresultofGopalanetal.(2021)tocoverreal-valuedoutcomesunder
somewhatweakerandmorenaturalconditions. Weproceedinthreesteps: first,toprovideaself-containedexposition,
westatetheresultofGopalanetal.(2021)forreal-valuedoutcomesinthespecialcaseofsquarederror(LemmaA.5and
LemmaA.6below). Second,wederiveamatchingboundusingLemmaA.2(ourresult),whichwedobydemonstrating
thattheconditionsofLemmaA.6implytheconditionsofLemmaA.2. Finally,weshowthatLemmaA.2appliesinmore
generalitythanLemmaA.6,underconditionswhichmatchthoseofDefinition3.2.
WefirststatethemainresultofGopalanetal.(2021)(adaptedtoournotation)below,whichholdsforbinaryoutcomes
Y ∈{0,1}.10
LemmaA.5(Omnipredictorsforbinaryoutcomes,specializedtosquarederror(Gopalanetal.(2021),Theorem6.3)). Let
S beasubsetwhichisα-indistinguishablewithrespecttoareal-valuedfunctionclassF andabinarytargetoutcome
Y ∈{0,1}. Then,forallf ∈F,
(cid:104) (cid:105) (cid:104) (cid:105)
E (Y −E[Y])2 ≤E (Y −f(X))2 +4α (88)
S S
Thisresultmakesuseofthefactthatforanyfixedy ∈[0,1],thesquarederrorfunctionis2-Lipschitzwithrespecttof(x)
overtheinterval[0,1]. ThisissimilartoLemmaA.2,butrequiresthatY isbinary-valued. Incontrast,LemmaA.2allows
forreal-valuedY ∈ [0,1],andgainsafactorof2ontheRHS.11 Gopalanetal.(2021)provideanalternateextensionof
LemmaA.5tobounded,real-valuedY,whichwepresentbelowforcomparisontoLemmaA.2.
10AsdiscussedinSection1,wealsocontinuetoelidethedistinctionbetweenthe‘approximate’multicalibrationofGopalanetal.
(2021)andourfocusonindividualindistinguishablesubsets.Theresultsinthissectioncanagainbeinterpretedasholdingforthe‘typical’
elementofanapproximatelymulticalibratedpartition.
11NotethatLemmaA.2alsorequiresthateachf ∈F takesvaluesin[0,1],butthisiswithoutlossofgeneralitywhentheoutcomeis
boundedin[0,1];projectingeachf ∈F onto[0,1]canonlyreducesquarederror.
21APREPRINT
ExtendingLemmaA.5toreal-valuedY. Fixsomeϵ>0,andletB(ϵ)={0,1,2...⌊2⌋}. LetY˜ bearandomvariable
ϵ
whichrepresentsadiscretizationofY intobinsofsize 2ϵ. Thatis,Y˜ =min b∈B(ϵ)(cid:12) (cid:12)Y − b 2ϵ(cid:12) (cid:12). LetR(Y˜)denotetherangeof
Y˜. Observethatthefollowingholdsforanyfunctiong :X →[0,1]:
(cid:12) (cid:12)
(cid:12)E[(Y˜ −g(X))2]−E[(Y −g(X))2](cid:12)≤ϵ (89)
(cid:12) (cid:12)
Where(89)followsbecausethefunction(y−g(x))2is2-Lipschitzwithrespecttog(x)over[0,1]forally ∈[0,1]. Wenow
workwiththediscretizationofY˜,andprovideananaloguetoLemmaA.5underamodifiedindistinguishabilitycondition
fordiscrete-valuedY˜,whichwe’llshowisstrongerthanDefinition3.1.
LemmaA.6(ExtendingLemmaA.5toreal-valuedY (Gopalanetal.(2021),adaptedfromTheorem8.1)). LetR(f)denote
therangeofafunctionf,andlet1(·)denotetheindicatorfunction. LetSbeasubsetwhichsatisfiesthefollowingcondition
withrespecttoafunctionclassF anddiscretizedtargetY˜:
Forallf ∈F andy˜∈R(Y˜),if:
(cid:12) (cid:12)
(cid:12)Cov (1(Y˜ =y˜),f(X))(cid:12)≤α (90)
(cid:12) S (cid:12)
Then:
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:20)(cid:16) (cid:17)2(cid:21) (cid:24) 2(cid:25)
E Y˜ −E [Y˜] ≤E Y˜ −f(X) +2 α (91)
S S S ϵ
To interpret this result, observe that (91) yields a bound which is similar to Lemma A.5 under a modified ‘pointwise’
indistinguishabilitycondition(90)foranydiscretizationY˜ ofY. Combining(91)with(89)furtherimplies:
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:104) (cid:105) (cid:24) 2(cid:25)
E Y −E [Y˜] ≤E (Y −f(X))2 +2 α+2ϵ (92)
S S S ϵ
DerivingLemmaA.6usingLemmaA.2
Weshownextthatthe‘pointwise’condition(90)forα≥0impliesourstandardindistinguishabilitycondition(Definition3.1)
forα′ =(cid:6)2(cid:7) α. ThiswillallowustoapplyLemmaA.2toobtainaboundwhichisidenticalto(92). Thus,weshowthat
ϵ
LemmaA.2isatleastasgeneralasLemmaA.6.
LemmaA.7. LetS beasubsetsatisfying(90). Then,forallf ∈F,
(cid:12) (cid:12) (cid:24) 2(cid:25)
(cid:12)Cov (Y˜,f(X))(cid:12)≤ α (93)
(cid:12) S (cid:12) ϵ
WeprovideaproofinAppendixB.Thus,combiningassumption(90)withLemmaA.2and(89)recoversaresultwhichis
identicaltoLemmaA.6. Thatis,forallf ∈F:
22APREPRINT
(cid:12) (cid:12)
(cid:12)Cov (1(Y˜ =y˜),f(X))(cid:12)≤α (94)
(cid:12) S (cid:12)
(cid:12) (cid:12) (cid:24) 2(cid:25)
⇒(cid:12)Cov (Y˜,f(X))(cid:12)≤ α (95)
(cid:12) S (cid:12) ϵ
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:20)(cid:16) (cid:17)2(cid:21) (cid:24) 2(cid:25)
⇒E Y˜ −E [Y˜] ≤E Y˜ −f(X) +2 α (96)
S S S ϵ
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:104) (cid:105) (cid:24) 2(cid:25)
⇒E Y −E[Y˜] ≤E (Y −f(X))2 +2 α+2ϵ (97)
S S ϵ
Where(95)followsfromLemmaA.7,(96)followsfromLemmaA.2and(97)followsfrom(89).
ExtendingLemmaA.2beyondLemmaA.6
Finally, to show that Lemma A.2 extends Lemma A.6, it suffices to provide a distribution over f(X) for some f ∈ F
andadiscrete-valuedY˜ takingl ≥1valuessuchthatDefinition3.1issatisfiedatlevelα≥0,but(90)isnotsatisfiedat
α′ =(α/l)(thoughinfactthattakingα′ =αalsosufficesforthefollowingcounterexample).
Considerthejointdistributioninwhichtheevents{Y˜ = 0,f(X) = 0},{Y˜ = 1,f(X) = 1}and{Y˜ = 1,f(X) = 1}
2 2 2
occurwithequalprobability 1 conditionalon{X ∈S}forsomeS ⊆X. Wesuppresstheconditioningevent{X ∈S}for
3
clarity. Then:
(cid:16) (cid:17) 1
Cov(1(Y˜ =0),f(X))=P(Y˜ =1) E[f(X)|Y˜ =0]−E[f(X)] =− (98)
6
Ontheotherhandwehave:
Cov(Y˜,f(X))=E[Y˜f(X)]−E[Y˜]E[f(X)] (99)
=E[Y˜E[f(X)|Y˜]]−E[Y˜]E[f(X)] (100)
(cid:18) (cid:19)
1 2 1 3 1 1 1
= ×0+ × × − × = (101)
3 3 2 4 3 2 12
(cid:12) (cid:12) (cid:12) (cid:12)
Thatis,wehave(cid:12)Cov(Y˜,f(X))(cid:12) = 1 < 3(cid:12)Cov(1(Y˜ =0),f(X))(cid:12) = 1. Thus,LemmaA.2establishesaresultwhich
(cid:12) (cid:12) 12 (cid:12) (cid:12) 2
issimilarto(A.6)forreal-valuedY undertheweakerandmorenaturalconditionthat|Cov(Y,f(X))|isbounded,which
remainswell-definedforreal-valuedY,ratherthanrequiringthestrongerpointwisebound(90)forsomediscretizationY˜.
Finally,webrieflycompareLemmaA.2toTheorem8.3inGopalanetal.(2021),whichgeneralizesLemmaA.6toholdfor
linearcombinationsofthefunctionsf ∈F andtofurtherquantifythegapbetweenthe‘canonicalpredictor’E [Y]andany
k
f ∈F (orlinearcombinationsthereof). Theseextensionsarebeyondthescopeofourwork,butwebrieflyremarkthatthe
apparentlysharperboundofTheorem8.3isduetoanincorrectassumptionthatthesquaredloss(y−g(x))2is1-Lipschitz
withrespecttog(x)overtheinterval[0,1],foranyy ∈[0,1]. CorrectingthistoaLipschitzconstantof2recoversthesame
boundas(97).
B Proofsofauxiliarylemmas
ProofofLemmaA.1
23APREPRINT
Proof. We’llfirstprove(17).
Cov(X,Y)=E[XY]−E[X]E[Y] (102)
=E[E[XY |X]]−E[X]E[Y] (103)
=P(X =1)E[XY |X =1]+P(X =0)E[XY |X =0]−E[X]E[Y] (104)
=P(X =1)E[Y |X =1]−E[X]E[Y] (105)
=P(X =1)E[Y |X =1]−P(X =1)E[Y] (106)
=P(X =1)(E[Y |X =1]−E[Y]) (107)
Asdesired. Toprove(18),letX′ =1−X. Applyingthepriorresultyields:
Cov(X′,Y)=P(X′ =1)(E[Y |X′ =1]−E[Y]) (108)
BecauseX′ =1⇔X =0,itfollowsthat:
Cov(X′,Y)=P(X =0)(E[Y |X =0]−E[Y]) (109)
Finally,becausecovarianceisabilinearfunction,Cov(X′,Y)=Cov(1−X,Y)=−Cov(X,Y). Chainingthisidentity
with(109)yieldstheresult.
ProofofLemmaA.2
TheresultwewanttoprovespecializesTheorem6.3inGopalanetal.(2021)tothecaseofsquarederror,butourresult
allowsY ∈[0,1]ratherthanY ∈{0,1}. ThefirstfewstepsofourproofthusfollowthatofTheorem6.3inGopalanetal.
(2021);ourproofdivergesstartingat(113). WeprovideadetailedcomparisonofthesetworesultsinAppendixA.4above.
Proof. Fixanyk ∈[K]. Wewanttoprovethefollowingbound:
E [(Y −E [Y])2]≤E [(Y −f(X))2]+4α (110)
k k k
Itsufficestoshowinsteadthat:
E [(Y −E [f(X)])2]≤E [(Y −f(X))2]+4α (111)
k k k
Fromthistheresultfollows,asE [(Y −E [Y])2]≤E [(Y −c)2]foranyconstantc. Tosimplifynotation,wedropthe
k k k
subscriptkandinsteadlettheconditioningevent{X ∈S }beimplicitthroughout. Wefirstshow:
k
E[(Y −f(X))2]=E(cid:2)E(cid:2) (Y −f(X))2 |Y(cid:3)(cid:3) ≥E(cid:2) (Y −E[f(X)|Y])2(cid:3) (112)
WherethesecondinequalityisanapplicationofJensen’sinequality(thesquaredlossisconvexinf(X)). Fromthisit
followsthat:
24APREPRINT
E(cid:2) (Y −E[f(X)])2(cid:3) −E(cid:2) (Y −f(X))2(cid:3) ≤E(cid:2) (Y −E[f(X)])2−(Y −E[f(X)|Y])2(cid:3) (113)
=E(cid:2)E[f(X)]2−2YE[f(X)]−E[f(X)|Y]2+2YE[f(X)|Y](cid:3)
(114)
=2(E[YE[f(X)|Y]−YE[f(X)]])−E(cid:2)E[f(X)|Y]2+E[f(X)]2(cid:3)
(115)
=2(E[Yf(X)]−E[Y]E[f(X)])−E(cid:2)E[f(X)|Y]2+E[f(X)]2(cid:3)
(116)
=2Cov(Y,f(X))−E(cid:2)E[f(X)|Y]2(cid:3) +E[f(X)]2 (117)
=2Cov(Y,f(X))−Var(E[f(X)|Y]) (118)
≤2α (119)
Whereeachstepuntil(119)followsbysimplygroupingtermsandapplyinglinearityofexpectation. (119)followsbythe
multicalibrationconditionandthefactthatthevarianceofanyrandomvariableisnonnegative.
ProofofLemmaA.4
Proof. Foranyπ ∈ Π,f ∈ F,assumption(77)givesus|Cov(Y,f(X)π(X))| ≤ α. We’llexpandtheLHStoshowthe
result.
|Cov (Y,f(X)π(X))| (120)
k
=|E [Cov (Y,f(X)π(X)|π(X))]+Cov (E [Y |π(X)],E [f(X)π(X)|π(X)])| (121)
k k k k k
=|P (π(X)=1)Cov (Y,f(X)π(X)|π(X)=1)+P (π(X)=0)Cov (Y,f(X)π(X)|π(X)=0) (122)
k k k k
+Cov (E [Y |π(X)],E [f(X)π(X)|π(X)])| (123)
k k k
=|P(π(X)=1)Cov (Y,f(X)|π(X)=1)+Cov (E [Y |π(X)],E [f(X)π(X)|π(X)])| (124)
k k k k
Where(121)istheapplicationofthelawoftotalcovariance. ObservenowthatCov (Y,f(X)|π(X)=1)isexactlywhat
k
wewanttobound. Todoso,wenowfocusonexpandingCov (E [Y |π(X)],E [f(X)π(X)|π(X)]). Thisis:
k k k
E [E [Y |π(X)]E [f(X)π(X)|π(X)]]−E [E [Y |π(X)]]E [E [f(X)π(X)|π(X)]] (125)
k k k k k k k
=P(π(X)=1)E [Y |π(X)=1]E [f(X)|π(X)=1]−E [Y]P(π(X)=1)E [f(X)|π(X)=1] (126)
k k k k
=P(π(X)=1)E [f(X)|π(X)=1](E [Y |π(X)=1]−E [Y]) (127)
k k k
Becauseπ(·)isabinaryvaluedfunction,wecanapplyLemmaA.1towrite
Cov (Y,π(X))
E [Y |π(X)=1]−E [Y]= k
k k P(π(X)=1)
Plugginginthisidentityyields:
P(π(X)=1)E [f(X)|π(X)=1](E [Y |π(X)=1]−E [Y]) (128)
k k k
=E [f(X)|π(X)=1]Cov (Y,π(X)) (129)
k k
Plugging(129)into(124)yields:
25APREPRINT
|Cov (Y,f(X)π(X))| (130)
k
=|P(π(X)=1)Cov (Y,f(X)|π(X)=1)+E [f(X)|π(X)=1]Cov (Y,π(X))| (131)
k k k
=|P(π(X)=1)Cov (Y,f(X)|π(X)=1)−(−E [f(X)|π(X)=1]Cov (Y,π(X)))| (132)
k k k
≥||P(π(X)=1)Cov (Y,f(X)|π(X)=1)|−|E [f(X)|π(X)=1]Cov (Y,π(X))|| (133)
k k k
≥|P(π(X)=1)Cov (Y,f(X)|π(X)=1)|−|E [f(X)|π(X)=1]Cov (Y,π(X))| (134)
k k k
Where(133)istheapplicationofthereversetriangleinequality.CombiningtheinitialassumptionthatS isindistinguishable
k
withrespectto{f(X)π(X)|f ∈F,π ∈Π}(77)and(134)yields:
|P(π(X)=1)Cov (Y,f(X)|π(X)=1)|−|E [f(X)|π(X)=1]Cov (Y,π(X))|≤α (135)
k k k
⇒|P(π(X)=1)Cov (Y,f(X)|π(X)=1)|≤α+|E [f(X)|π(X)=1]Cov (Y,π(X))| (136)
k k k
=α+E [f(X)|π(X)=1]|Cov (Y,π(X))| (137)
k k
≤α+|Cov (Y,π(X))| (138)
k
≤2α (139)
Whichfinallyimplies|Cov (Y,f(X)|π(X)=1)|≤ 2α ,asdesired. (137)and(138)followfromtheassumption
k P(π(X)=1)
thatf(X) ∈ [0,1],and(139)followsfromtheinitialassumptionthatS isα-indistinguishablewithrespecttoeveryΠ
k
(76).
ProofofLemmaA.7
Proof. RecallthatY˜ isadiscreterandomvariabletakingvalues0, ϵ,ϵ,3ϵ...⌊2⌋ϵ. WeagainuseR(Y˜)todenotetherange
2 2 ϵ 2
ofY˜. Ouranalysisbelowproceedsconditionalontheevent{X ∈S},whichwesuppressforclarity. Wecanshow
26APREPRINT
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)Cov(Y˜,f(X))(cid:12)=(cid:12)E[Y˜f(X)]−E[Y˜]E[f(X)](cid:12) (140)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
=(cid:12) (cid:12)E[Y˜f(X)]−E[f(X)] (cid:88) y˜P(Y˜ =y˜)(cid:12) (cid:12) (141)
(cid:12) (cid:12)
(cid:12) y˜∈R(Y˜) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
=(cid:12) (cid:12)E[Y˜E[f(X)|Y˜]]−E[f(X)] (cid:88) y˜P(Y˜ =y˜)(cid:12) (cid:12) (142)
(cid:12) (cid:12)
(cid:12) y˜∈R(Y˜) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
=(cid:12) (cid:12) (cid:88) y˜P(Y˜ =y˜)E[f(X)|Y˜ =y˜]−E[f(X)] (cid:88) y˜P(Y˜ =y˜)(cid:12) (cid:12) (143)
(cid:12) (cid:12)
(cid:12)y˜∈R(Y˜) y˜∈R(Y˜) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
=(cid:12) (cid:12) (cid:88) y˜P(Y˜ =y˜)(cid:16) E[f(X)|Y˜ =y˜]−E[f(X)](cid:17)(cid:12) (cid:12) (144)
(cid:12) (cid:12)
(cid:12)y˜∈R(Y˜) (cid:12)
= (cid:88) y˜P(Y˜ =y˜)(cid:12) (cid:12)(cid:16) E[f(X)|Y˜ =y˜]−E[f(X)](cid:17)(cid:12) (cid:12) (145)
(cid:12) (cid:12)
y˜∈R(Y˜)
= (cid:88) y˜(cid:12) (cid:12)Cov(1(Y˜ =y˜),f(X))(cid:12) (cid:12) (146)
(cid:12) (cid:12)
y˜∈R(Y˜)
(cid:88)
≤ y˜α (147)
y˜∈R(Y˜)
(cid:88)
≤ α (148)
y˜∈R(Y˜)
(cid:24) (cid:25)
2
≤ α (149)
ϵ
(cid:12) (cid:12)
Where (145) makes use of the fact that y˜ ≥ 0, (146) makes use of the identity (cid:12)Cov(1(Y˜ =y˜),f(X))(cid:12) = P(Y˜ =
(cid:12) (cid:12)
(cid:12)(cid:16) (cid:17)(cid:12)
y˜)(cid:12) E[f(X)|Y˜ =y˜]−E[f(X)] (cid:12)(thisisastraightforwardanalogueofLemmaA.1),(147)appliesassumption(90),and
(cid:12) (cid:12)
(148)makesuseofthefactthaty˜≤1.
C Additionalexperimentalresults: chestX-raydiagnosis
InthissectionweprovideresultswhichareanalagoustothosepresentedinSection5forthefouradditionalpathologies
studiedinRajpurkaretal.(2021). Foreachpathologywefirstpresentafigurecomparingtheaccuracyofthebenchmark
radiologiststothatoftheeightleaderboardalgorithms,asinFigure1foratelectasis. Wethenpresentafigurewhichplots
theconditionalperformanceofeachradiologistwithinapairofindistinguishablesubsets,asinFigure2.
ResultsfordiagnosingapleuraleffusionarepresentedinFigure4andFigure5. Resultsfordiagnosingcardiomegalyare
presentedinFigure6andFigure7. ResultsfordiagnosingconsolidationarepresentedinFigure8andFigure9. Finally,
resultsfordiagnosingedemaarepresentedinFigure10andFigure11.
27APREPRINT
predictor
0.8
drnet
ihil
jfaboy
0.7
ngango2
ngango3
sensexdr
uestc
0.6
yww211
radiologist 1
radiologist 2
radiologist 3
0.5
algorithmic radiologist
Prediction Type
Figure4:Therelativeperformanceofradiologistsandpredictivealgorithmsfordetectingapleuraleffusion.EachbarplotstheMatthews
CorrelationCoefficientbetweenthecorrespondingpredictionandthegroundtruthlabel.Pointestimatesarereportedwith95%bootstrap
confidenceintervals.
predictor
random baseline
0.50 drnet
ihil
jfaboy
ngango2
0.25
ngango3
sensexdr
uestc
0.00 yww211
radiologist 1
radiologist 2
radiologist 3
α=0, μ=1, n=131 α=0.43, μ=0.17, n=369
Model Prediction Bins
Figure5: Theconditionalperformanceofradiologistsandpredictivealgorithmsfordetectingapleuraleffusion. Eachsubsetisα-
indistinguishablewithrespecttotheeightalgorithmicpredictors. µindicatesthefractionofpositivealgorithmicpredictionsandn
indicatesthenumberofpatients.AllelseisasinFigure4.
predictor
drnet
0.7
ihil
jfaboy
ngango2
0.6 ngango3
sensexdr
uestc
yww211
0.5 radiologist 1
radiologist 2
radiologist 3
algorithmic radiologist
Prediction Type
Figure6:Therelativeperformanceofradiologistsandpredictivealgorithmsfordetectingcardiomegaly.EachbarplotstheMatthews
CorrelationCoefficientbetweenthecorrespondingpredictionandthegroundtruthlabel.Pointestimatesarereportedwith95%bootstrap
confidenceintervals.
28
tneicfifeoC
noitalerroC
tneicfifeoC
noitalerroC
tneicfifeoC
noitalerroCAPREPRINT
predictor
random baseline
0.50
drnet
ihil
jfaboy
ngango2
0.25
ngango3
sensexdr
uestc
yww211
0.00
radiologist 1
radiologist 2
radiologist 3
α=0, μ=0, n=214 α=0.49, μ=0.76, n=286
Model Prediction Bins
Figure 7: The conditional performance of radiologists and predictive algorithms for detecting cardiomegaly. Each subset is α-
indistinguishable with respect to the eight algorithmic predictors. µ indicates the fraction of positive algorithmic predictions and
nindicatesthenumberofpatients.AllelseisasinFigure6.
0.6
predictor
drnet
0.5
ihil
jfaboy
0.4 ngango2
ngango3
sensexdr
0.3 uestc
yww211
radiologist 1
0.2 radiologist 2
radiologist 3
algorithmic radiologist
Prediction Type
Figure8:Therelativeperformanceofradiologistsandpredictivealgorithmsfordetectingconsolidation.EachbarplotstheMatthews
CorrelationCoefficientbetweenthecorrespondingpredictionandthegroundtruthlabel.Pointestimatesarereportedwith95%bootstrap
confidenceintervals.
0.8 predictor
random baseline
0.6 drnet
ihil
jfaboy
0.4 ngango2
ngango3
sensexdr
0.2
uestc
yww211
0.0 radiologist 1
radiologist 2
radiologist 3
-0.2
α=0, μ=1, n=92 α=0.44, μ=0.12, n=408
Model Prediction Bins
Figure 9: The conditional performance of radiologists and predictive algorithms for detecting consolidation. Each subset is α-
indistinguishablewithrespecttotheeightalgorithmicpredictors. µindicatesthefractionofpositivealgorithmicpredictionsandn
indicatesthenumberofpatients.AllelseisasinFigure8.
29
tneicfifeoC
noitalerroC
tneicfifeoC
noitalerroC
tneicfifeoC
noitalerroCAPREPRINT
predictor
drnet
0.6
ihil
jfaboy
ngango2
ngango3
0.5 sensexdr
uestc
yww211
radiologist 1
radiologist 2
0.4 radiologist 3
algorithmic radiologist
Prediction Type
Figure 10: The relative performance of radiologists and predictive algorithms for detecting edema. Each bar plots the Matthews
CorrelationCoefficientbetweenthecorrespondingpredictionandthegroundtruthlabel.Pointestimatesarereportedwith95%bootstrap
confidenceintervals.
predictor
random baseline
drnet
0.50
ihil
jfaboy
ngango2
0.25 ngango3
sensexdr
uestc
yww211
0.00 radiologist 1
radiologist 2
radiologist 3
α=0, μ=0, n=310 α=0.46, μ=0.79, n=190
Model Prediction Bins
Figure11:Theconditionalperformanceofradiologistsandpredictivealgorithmsfordetectingedema.Eachsubsetisα-indistinguishable
withrespecttotheeightalgorithmicpredictors.µindicatesthefractionofpositivealgorithmicpredictionsandnindicatesthenumberof
patients.AllelseisasinFigure10.
D Additionalexperimentalresults: predictionfromvisualfeatures
InthissectionwepresentadditionalexperimentalresultsforthevisualpredictiontaskstudiedinSaveskietal.(2021).
Humans fail to outperform algorithms. As in the X-ray diagnosis task in Section 5, we first directly compare the
performanceofhumansubjectstothatofthefiveoff-the-shelflearningalgorithmsstudiedinSaveskietal.(2021). We
againusetheMatthew’sCorrelationCoefficient(MCC)asameasureofbinaryclassificationaccuracy(ChiccoandJurman,
2020). OurresultsconfirmoneofthebasicfindingsinSaveskietal.(2021),whichisthathumansfailtooutperformthebest
algorithmicpredictors. WepresenttheseresultsinFigure12.
Althoughtheseresultsindicatethathumansfailtooutperformalgorithmsonaverageinthisvisualpredictiontask,wenow
applytheresultsofSection3toinvestigatewhetherhumanssubjectscanrefinealgorithmicpredictionsonspecificinstances.
Resolvingindistinguishabilityviahumanjudgment. AsinSection5,wefirstformapartitionofthesetofinputimages
which is multicalibrated with respect to the five predictors considered in Figure 12. As indicated by Lemma 4.1 and
CorollaryA.1,wedothisbypartitioningthespaceofobservationstominimizethevarianceofeachofthefivepredictors
withineachsubset.12 Becausetheoutcomeisbinary,itisnaturaltopartitionthespaceofimagesintotwoclusters. Wenow
12ThisclusteringprocedureamountstominimizingtheChebyshevdistanceinthe8-dimensionalspacedefinedbythepredictionsof
eachleaderbordalgorithm.Seehttps://github.com/ralur/human-expertise-algorithmic-predictionforadditionaldetail.
30
tneicfifeoC
noitalerroC
tneicfifeoC
noitalerroCAPREPRINT
predictor
0.40
naive bayes
random forest
0.35
logistic regression
gradient boosting
0.30 linear svm
human (control)
human (4 examples)
0.25
human (8 examples)
human (12 examples)
0.20
human subjects algorithmic
Prediction Type
Figure12: Comparingtheaccuracyofhumansubjects’predictionstothosemadebyoff-the-shelflearningalgorithmsacrossfour
treatmentconditions.Subjectsinthecontrolconditionaregivennotraining,whilesubjectsineachofthethreeremainingconditionsare
presentedwithasmallnumberoflabeledexamplesbeforebeginningthetask.EachbarplotstheMatthewscorrelationcoefficientbetween
thecorrespondingpredictionandthetruebinaryoutcome;pointestimatesarereportedwith95%bootstrapconfidenceintervals.
examinetheconditionalcorrelationbetweeneachpredictionandthetruebinaryoutcomewithineachofthesesubsets,which
weplotinFigure13.
0.5
predictor
0.4
gradient_boosting
linear_svm
0.3 logistic_regression
naive_bayes
random_forest
0.2
human (control)
human (4 examples)
0.1 human (8 examples)
human (12 examples)
0.0
α=0, μ=1, n=189 α=0.44, μ=0.18, n=671
Model Prediction Bins
Figure13:Theperformanceofhumanandalgorithmicpredictionswithinclustersdefinedbythefivealgorithmicpredictorsconsideredin
Figure12.Eachsubsetisα-indistinguishablewithrespecttothefivealgorithmicpredictors.µindicatesthefractionofpositivealgorithmic
predictionsandnindicatesthenumberofimages. AllelseisasinFigure12. Theconfidenceintervalsforthealgorithmicpredictors
arenotstrictlyvalid(thesubsetsarechosenconditionalonthepredictionsthemselves),butareincludedforreferenceagainsthuman
performance.
Aswecansee,thehumansubjects’predictionsperformcomparablytothealgorithmswithinthe‘negative’(µ = .18;µ
indicatesthefractionofpositivealgorithmicpredictions)bin,butaddsubstantialinformationwhenallfivemodelspredict
a positive label (µ = 1). Thus, although the human subjects fail to outperform the algorithmic predictors on average
(Figure12),thereissubstantialheterogeneityintheirrelativeperformancethatcanbeidentifiedex-antebypartitioningthe
observationsintotwoapproximatelyindistinguishablesubsets. Inparticular,asintheX-rayclassificationtaskstudiedin
Section5,wefindthathumansubjectscanidentifynegativeinstanceswhichareincorrectlyclassifiedaspositivebyallfive
algorithmicpredictors.
31
tneicfifeoC
noitalerroC
tneicfifeoC
noitalerroC