Geometry Transfer for Stylizing Radiance Fields
HyunyoungJung1∗ SeonghyeonNam2 NikolaosSarafianos2
SungjooYoo1 AlexanderSorkine-Hornung2 RakeshRanjan2
1SeoulNationalUniversity 2MetaRealityLabs
https://hyblue.github.io/geo-srf
Figure1. Givenareference3Dsceneandapairofstyleguides: anRGBimageandadepthmap,wecoherentlystylizeboththescene’s
appearanceandshapetobestexpressthegivenstyle.
Abstract renderingoneimageinthestylisticmannerofanother,pro-
ducing a new image that combines the foundational struc-
Shape and geometric patterns are essential in defining
tureoftheformerwiththeaestheticqualitiesofthelatter.
stylistic identity. However, current 3D style transfer meth-
In its early phases, style transfer was primarily ap-
ods predominantly focus on transferring colors and tex-
plied to 2D images [9, 34, 40, 49] and later extended to
tures,oftenoverlookinggeometricaspects.Inthispaper,we
videos[24,41,76,80]toachievetemporallyconsistentstyl-
introduceGeometryTransfer,anovelmethodthatleverages
ization across image sequences. Recent works have tack-
geometricdeformationfor3Dstyletransfer. Thistechnique
ledthe3Dstyletransferproblem,byapplyingstylesto3D
employs depth maps to extract a style guide, subsequently
models,suchaspointclouds[25,54]andmeshes[23,44].
applied to stylize the geometry of radiance fields. More-
Theystandapartfrom2Dmethods,aimingtoensureaco-
over,weproposenewtechniquesthatutilizegeometriccues
hesive style across multiple camera angles and enabling
fromthe3Dscene,therebyenhancingaestheticexpressive-
free-viewpoint rendering. Due to the error-prone geome-
nessandmoreaccuratelyreflectingintendedstyles.Ourex-
trystemmingfromtherequired3Dreconstructionofthem,
tensiveexperimentsshowthatGeometryTransferenablesa
thestylizationofradiancefields[53]hasbeenactivelyex-
broaderandmoreexpressiverangeofstylizations, thereby
plored. Methods have incorporated global [55] and lo-
significantlyexpandingthescopeof3Dstyletransfer.
cal [83] constraints, utilized stylized reference views [86],
andenhanceddiversitythroughhashencodingandseman-
1.Introduction ticmatching[56].Zero-shotapproaches[45]havealsobeen
developedtocircumventtediousoptimization.
Withtheincreasingdemandforcontentcreationforvirtual
These works focus on transferring aesthetic qualities in
andaugmentedreality,styletransfer[17]hasemergedasan
termsofcolors,texture,andbrushstrokesfromstyleimages
innovativetechniquethatbridgesthebeautyofartwiththe
toenhancestylization, effectivelyapplyingtheseattributes
precisionoftechnology. Atitscore,styletransferinvolves
to 3D scenes. However, the potential benefits of geometry
*ThisworkwasconductedduringaninternshipatMeta remainlargelyunexploredandneglected. Eventhough3D
1
4202
beF
1
]VC.sc[
1v36800.2042:viXrascenes and objects naturally possess both shape and color weintroduceanovelstyleaugmentationstrategytobringa
attributes,mosttechniquesfocussolelyoncolor,leavingthe richersenseofscenedepth. Ourcontributionsaresumma-
geometric parameters unchanged during the style transfer. rizedasfollows:
Nguyen-Phuoc et al. [55] adjust geometry, but the output • Forthefirsttimeinstyletransferliterature,weintroduce
shapes do not deviate from the original content and fail to Geometry Transfer, a method that extracts style from a
reflectgeometriccuesfromthestyleimages. depthmapandstylizesthegeometryofradiancefields.
Asnotedbyarttheoristsandimagecreationexperts[1, • Weproposeanovelusageofdeformationfieldstoensure
22, 33], geometry has historically played a crucial role in coherentstylizationofbothshapeandappearance.
defining and influencing style. The shapes and geometric • We introduce novel RGB-D stylization techniques, en-
patterns in an artwork are essential to its stylistic identity. hancing expressiveness and better reflecting the style by
From this perspective, in the literature of 2D image style leveragingscenegeometries.
transfer, techniques like correspondence search and image • Our proposed methods can be seamlessly incorporated
warping [33, 47, 48] have been employed to distort image intoexistingPanopticRadianceFields[68],enablingpar-
shapes,showcasinghowgeometrycanenhancetheexpres- tialstylizationofscenesformorepracticalapplicability.
siveness of stylization. When applied to 2D images, how-
ever,geometricdistortioninherentlyhasitslimitations. Al- 2.RelatedWorks
though shapes are fundamentally 3D forms, images cap-
Neural style transfer. Neural style transfer is the process
tureonlytheir2Dprojections;thus,warpinganddistorting
ofcreatinganewimagethatfusesthestructuralelementsof
edgesinimagesofferonlyanimplicitsenseoftheintended
acontentimagewiththeaestheticcharacteristicsofastyle
style. It is somewhat limited to assert that they accurately
image. Gatys et al. [17] described the style transfer as an
reflecttheshapeoftheobjectsinthestyleimage.
iterative optimization that aligns feature correlations from
In this study, we primarily focus on the benefits of in- both images, using a deep feature network [69]. Building
corporating geometric deformation into 3D style transfer. on this, various techniques [5, 9, 34, 36–38, 40, 49] have
Unlike previous approaches, we define “geometric style” advancedstylizationthroughsemanticcorrespondence[27,
as a distinct and clear characteristic that truly captures the 43, 85], image blending [4, 32, 50, 71, 84] and novel loss
geometric essence of the style image. Our objective is to formulasforfeaturestatisticscomputation[21,51,62]. To
transfertheseintricateformsintothecontentofa3Dscene. address the slow convergence of iterative optimization, a
Tothebestofourknowledge, ourworkisthefirstinstyle feed-forward network has been widely adopted to facili-
transferliteraturetoproposeGeometryTransfer,whichex- tate arbitrary stylization [12, 19, 26, 42, 46, 58, 67] in
tracts geometric style from a style template using a depth real-time [29, 64, 77, 79, 80]. In response to inconsis-
mapandthendirectlystylizestheshapeofneuralradiance tent stylization across multiple images, techniques to styl-
fields. However, merely replacing the RGB style image ize videos [7, 15, 24, 41, 63, 75, 76, 80] and stereo im-
fromthepreviousmethodswithadepthmapdoesnotpro- ages [8, 18] have been proposed. Given that shape and
ducethedesiredresults. Thisissuearisesfromtheintrinsic geometry are essential for expressive stylization, certain
separation between appearance and shape in the radiance methodshavefocusedondistortingthecontent’sstructure,
fieldsrepresentation. Whentheshapeisdirectlyoptimized, specifically for faces [82] and text [81]. More general
the resulting colors are not well-aligned with the updated methods [33, 47, 48] utilize correspondence searches and
form. Toovercomethischallenge,weintroduceanovelap- image warping to align content and style from the same
plication of deformation fields [61] that predicts the offset class identity. Since shapes and geometry are fundamen-
vector for each 3D point. This ensures a harmonious styl- tally3Dforms,however,modificationsto2Dimagesoften
ization of both appearance and shape during optimization. failtocapturetheaccuratestyleofgeometry. Ourproposed
Consequently,wedemonstratethepotentialofstylizingthe method aims to directly stylize the shape of the 3D scene
geometry of a 3D scene using a depth map as a style im- usingtheestimateddepthmapfromthestyleimage.
age. Beyond this demonstration, we introduce innovative 3D style transfer. Recent techniques have applied style
techniquestohighlighthowstylizationcanbenefitfromthe transferto3Dmodelstoensurecoherentstylizationacross
incorporationofgeometry. imagesrenderedfrommultipleviewpoints.Earliermethods
Building on our geometry transfer, we propose a new stylized explicit representations, such as point clouds [25,
3DstyletransfermethodusinganRGB-Dpairasthestyle 54], and mesh [23, 44]. More recent techniques [10, 14,
image,aimingformoreexpressivestylizationthatbetterre- 45, 55, 56, 83, 86, 87] have actively explored the styliza-
flects the given style in terms of both shape and appear- tion of implicit representations, i.e. radiance fields [53].
ance.Towardthisgoal,weproposegeometry-awarematch- In optimization-based approaches [14, 56], Nguyen-Phuoc
ingtoenhancethediversityofstylizationwhilepreserving etal.[55]alternatedbetweenrenderingand2Dstylization,
localgeometrythroughapatch-wisescheme. Additionally, usingaglobalstylelosstostylizethe3Dscene.Meanwhile,
2Figure2. Overviewofourmethod. Firstwepre-trainTensoRF[6]onreal-worldimagestoobtainthecolorgridG anddensitygridG ,
c σ
enablingphotorealisticreconstruction. Subsequently,weextractVGGfeaturesfromstyleimagesasanRGB-Dpairtostylizetheshape
andappearanceofradiancefields.Here,theshapeismodifiedthroughtheadditionaldeformationgridG ,whileG remainsfixed.
∆ σ
Zhang et al. [83, 86] employed a nearest-neighbor match- begins with a pre-trained NeRF on a real-world 3D scene.
ing loss [35] and utilized a reference stylized view [86] to We use TensoRF [6] as our scene representation. It intro-
enhance detail preservation. Another direction avoids per- duces two separate grids, G and G , each with per-voxel
c σ
style optimization and instead adopts hypernetworks [10], multi-channel features where the former models appear-
feature transformations [45], and Lipschitz mappings [87] ance,andthelatterthevolumedensity. Toensureefficient
tofacilitatearbitrarystylizationof3Dscenes. Whilemost rendering and compact representation, TensoRF factorizes
techniquesprioritizeappearancewithoutalteringgeometry them into multiple low-rank components. For pre-training
during style transfer, we emphasize geometry distortion to on the target 3D scene, which includes training images
improve stylization expressiveness and style accuracy. To {I }N andtheircorrespondingcameraposes{p }N ,we
i i=1 i i=1
ourknowledge,thisisthefirstuseofadepthmapasastyle followthetrainingschemeoutlinedintheoriginalpaperand
guide to optimize the radiance fields’ geometry. Instead referthereaderthereforadditionaldetails.
of using reference images for style as above, several ap- We primarily follow the methods of stylizing radiance
proachesstylizeradiancefieldsusingtextpromptsviaCLIP fields in ARF [83]. In each stylization iteration, we ran-
embedding[72,73]andleveragediffusionmodels[20,31]. domly select a viewpoint p and render the image Iˆ . We
i pi
These demonstrate the potential for a wide range of appli- thenextract2DfeaturemapsFrgb fromIˆ andFrgb from
I pi S
cations. However,astext-guidedstylestendtobelessspe- thestyleimage,S ,usingVGG[69]. Afterthis,wecom-
rgb
cific,ourmethodaimsformoreprecisestylization,aligning pute the style loss L between these feature maps, for-
style
closelywithreference-guidedstyletransferapproaches. mulatedasanearest-neighbormatchingloss[35,83]:
Deformation fields. Deformation fields have been widely
usedinitiallytomodelthetarget3Dshapeofobjectswhile
1 (cid:88)
preserving their geometric details [13, 30, 74]. They de- L = minD(Frgb(i,j),Frgb(i′,j′)), (1)
finetheshapeasasurfacedeformationofthetemplate3D
style N
i,j
i′,j′ I S
models. Pumarola et al. [61] introduced D-NeRF, which
whereD(,)computesthecosinedistancebetweentwonor-
employs a time-varying deformation function to capture
malizedfeaturevectors.
thetransformationbetweencanonicalanddeformedscenes.
This approach allows for the reconstruction of dynamic
3.1.GeometryTransfer
scenesusingasinglemovingcamera. Buildingonthiscon-
cept,subsequentstudies[16,59,60,70]haveaddressedthe Ourapproachstemsfromafundamentalquestion: Canwe
viewsynthesischallengeindynamicscenes. However,our transfer geometry in the same manner as we transfer col-
approach distinguishes itself by innovatively using defor- ors? To explore this, we introduce the use of a depth map
mationstoensurealignmentbetweenshapeandappearance asastyleguidetotransferitsgeometrytoa3Dscene. An
whendirectlystylizingthegeometryoftheradiancefields. overviewofourapproachisdepictedinFig.2.
3.Methodology 3.1.1 DepthMapasaStyleGuide
Preliminaries: StylizingRadianceFields. Stylizingradi- Instead of using an RGB image as the style guide, we re-
ance fields is conceptualized as a fine-tuning process that place it with a depth map, denoted as S , which captures
D
3Figure4.Samplingw/andw/odeformationfields.Comparisons
ofthesamplingdensityσ andcolorc fora3Dpointx withand
i i i
withoutdeformationfields.Thecurvesrepresentthe2Dprojected
surfacesofobjects, wheregreendepictsthestylizedsurfaceand
bluetheoriginalsurface.Bysamplingwithdeformationfields,we
coherentlysamplebothvaluesfromtheoriginalsurface.
predictingathree-dimensionaldisplacementvector,∆x ∈
i
R3, that maps a 3D point x to its canonical location
i
Figure 3. Comparisons of the stylized results obtained by op-
x +∆x . Inourcontext,thecanonicalspacereferstothe
i i
timizing the density grid (a), and by optimizing the deformation
originalscenebeforestylization.Werepresentthedeforma-
fields(b). Whendirectlyoptimizingthedensity,backgroundcol-
tionnetworkusinganothervoxelgrid,G ,andupdateitex-
orsareassignedtotheupdatedpartsoftheforegroundobject. ∆
clusivelyforthepurposeofstylizingthegeometry,keeping
adistinctstyleofshape. Duringthestyletransferprocess, G σ unchanged. After the stylization, the canonical surface
we render the depth map D and optimize the style loss remainsintact. Whenrenderingthestylizedscene,boththe
pi
between D and S . Since the VGG network expects 3- densities and colors are sampled from the original surface
pi D
channel images as input, we concatenate the depth maps locations, as described in Fig. 4 (b). This ensures that co-
along the channel dimensions by replicating them three herentcolorsareassociatedwiththemodifiedareas,leading
times. Thereplicateddepthmapsarethenfedintothenet- tothedifferencesshowninFig.3(b).
work. GiventhatD relatessolelytovolumedensity, the
pi 3.2.RGB-DStylization
lossfunctionoptimizesthedensitygrid,G . Asillustrated
σ
To realize a more expressive stylization that modifies both
in Fig. 3 (a), this approach revealed that we could manip-
colors and geometry, we employ a pair of style guides: an
ulateshapesinthesamemannerthatweapplystyletrans-
RGBimageandadepthmap. GivenanRGBstyleS ,we
fertocolors. However,achallengearises: whiletheshape rgb
use a zero-shot depth estimation network [3] to derive its
adaptstothestyleimage,thecolorfieldsremainstatic,lead-
depthmap. Thisthenservesasthestyledepth,S .
ing to undesired outcomes. For instance, background col- D
orsmightbeappliedtoupdatedportionsofforegroundob-
jects,eventhoughideally,thecolorsoftheseobjectsshould 3.2.1 Geometry-awareNearestMatching
evolvecohesivelywiththeirshape. Tostylizeusingtwostyleimages,specificallyS andS ,
rgb D
thestylelossmustbeadjustedtoaccountformultiplestyle
sources. Since our goal is to align both colors and geom-
3.1.2 ModelingDeformationFields
etry,computingthenearestmatchinglossindependentlyis
Afterpre-trainingonreal-worldscenes,thedensitygridG inappropriateduetopotentialinconsistenciesbetweenpat-
σ
formsasurfacedistributionthatmirrorsthetarget3Dscene. ternsofappearanceandshape. Amoreeffectivemethodis
Concurrently,colorvaluesintheappearancegridG areup- to initially identify the closest match between content and
c
datedincoherencewiththecorrespondinglocationsofthe style features in one domain, then compute the style loss
surface distribution in G . This synchronization leads to fortheotherdomainusingthesepredeterminedpairs.Alter-
σ
therenderingofprecisesurfaceswithaccurateappearance. natively,bothcolorandgeometryfeaturescouldbeusedto
However,whenthegeometryisstylized,thesurfacedistri- searchforthenearestneighborsconcurrently.Afterextract-
butionwithinG changes, yetG remainsconsistent. Dur- ingVGGfeaturemapsfromthetwomodalities,weconcate-
σ c
ing sampling of 3D points along rays and querying colors natethemalongthechanneldimensionandthenperforma
anddensitiesfromthesemisalignedgrids,thecolorsofthe searchtofindthenearestpair:
modifiedareasarepredominantlysourcedfromthenewsur- j =argminD([Frgb(i),FD(i)],[Frgb(i′),FD(i′)]) (2)
facelocationsinG c,asshowninFig.4(a),eventhoughthe i′ I I S S
colorfieldsstillalignwiththeoriginaldistribution. We then optimize the cosine distance D separately for
To address this issue, we introduce a deformation net- featuresfromeachmodality:
work to enable synchronous modifications of both shape
L(i)=D(Frgb(i),Frgb(j))+D(FD(i),FD(j)), (3)
and appearance. This network is designed as a function I S I S
4The style loss is calculated as the mean across all feature images by downsampling them at multiple scales {s }N .
i i=1
vectors:L = 1 (cid:80) L(i).Thisstrategy,whichinvolves Thisprocessresultsinaseriesofstylepairs,{S }N ,where
style N i i i=1
incorporatinggeometryfeaturesintothematchingprocess, S =(S ,S ).Wesetthescaleofthefirstbin,s ,to1.To
rgb D 1
notonlyenhancesdiversitybutalsobetterpreservesscene reflectreal-worldconditions, thescalesofsubsequentbins
structure,asdemonstratedinSec.4. arecalculatedbasedontheirrelativedistancefromthefirst
binas: s =C /C .
i 1 i
During stylization, each pixel in the rendered image is
3.2.2 Patch-wiseOptimization
assigned to a bin B , based on the shortest distance from
i′
With an RGB style image, it is straightforward to deter-
the pixel’s z-coordinate to the center C of the bin. This
i′
mine if the output aligns with the style in terms of color,
methodtransformstherenderedimageintoaformatakinto
texture, and other visual attributes. However, in geome-
alayereddepthimage[65]. Eachlayeristhenstylizedus-
try, depth maps provide limited cues to identify the style.
ingitscorrespondingstylepairS ,whichisdownsampled
i′
This is because shapes are defined not by isolated pixels,
to the appropriate scale. Consequently, larger patterns are
butbytheirrelationshiptotheirsurroundings. Theexisting
mapped onto surfaces closer to the viewer, while smaller
nearest matching loss, which conducts matching on a per-
patterns are applied to more distant surfaces, thereby en-
pixelbasis,isnotenoughfortransferringthestyleofgeom-
hancingtheoverallsenseofdepth.
etryeffectively. Toaddressthis,weintroduceapatch-wise
matchingschemethatbroadensthereceptivefields,thereby
4.Experiments
becomingmoreeffectiveincapturingspatialinteractions.
Given the extracted VGG feature maps F and F , we ImplementationDetails. Weimplementedourworkbased
I S
first partition each feature map into sets of k × k non- onthecodeofARF[83],usingTensoRF[6]astheunderly-
overlapping patches: {Pi} and {Pi} . The patch-wise ingNeRFrepresentation. ForthetrainingofTensoRFdur-
I i S i
stylelossL isthengivenby: ingthephotorealisticreconstructionstage,wefollowedthe
SP
trainingschemefromitsoriginalpaperandutilizedadistor-
tionregularizer[2]tomitigateartifactssuchasfloatersand
L = 1 (cid:88) minDP(Pi,Pj), (4) background collapse. In this stage, the deformation fields
SP |P I| j I S are randomly initialized and are optimized to output zeros
i
for all sampled input points. After pre-training, we main-
where DP(P ,P ) computes the sum of the cosine dis- tainedthedensitygridataconstantbutupdatedboththeap-
1 2
tances between feature vectors at corresponding locations pearanceanddeformationgridstostylizethereconstructed
withineachpatch: 3D scene using style loss. We employed the conv2 and
conv3 layers of the VGG-16 [69], when computing the
k2 style loss. We also used the view-consistent color trans-
(cid:88)
DP(P 1,P 2)= D(F 1i,F 2i), (5) ferfromARF[83],asapre-processingandpost-processing
i stepforstylization.
Datasets. We demonstrate our contributions through ex-
Here,Dcalculatesthecosinedistance,andF represents
1,2
periments conducted on the LLFF dataset [52], which
featurevectorsthatconstituteeachpatch. Toachievelarger
compriseshigh-resolutioncapturesofreal-world, forward-
receptivefieldswithoutincreasingcomputation,eachpatch
facing scenes, as used in recent 3D style transfer meth-
canbedefinedwithadilationraterasahyperparameter.
ods [55, 83, 86]. Furthermore, we utilized the ScanNet
dataset [11] to verify the capability of our approach on
3.2.3 PerspectiveStyleAugmentation
scenescapturedfromdiversecameraviewpoints,highlight-
We typically select style images with distinct patterns as ingourmethod’spotentialforpartialstylization. TheScan-
showninFig.1,sincethisaidsinthecleareridentification Net dataset includes multiple sequences of real-world in-
of their geometric style. To enhance diversity and the per- door scenes characterized by varied trajectories and a col-
ceptionofdepth,wecanvarythesizesofthesepatterns,ap- lectionofcommonfurnituretypes.
plyingthemdifferentlytosurfacesbasedontheirdistance. Evaluationmetric.WeuseSingleImageFre´chetInception
Before the stylization process, we gather 3D points in Distance (SIFID) [66] to evaluate the stylizations. SIFID
world coordinates from all training viewpoints and cat- calculatesthefeaturedistancebetweentwoimages,indicat-
egorize them into N bins, {B }N , based on their z- ingthestylesimilaritybetweenthestyleimageandthestyl-
i i=1
coordinates. Each bin B is linked to a central value C , izedresultsforquantitativeevaluationinimagestyletrans-
i i
determinedby averagingthe z valuesof pointswithinthat fer[57,78].Weintroducethreemethodstoassesshowboth
bin. Giventhatpatternsizescanvarywiththerelativereso- theshapeandappearancereflectthespecifiedstyleguide:
lutionsofcontentandstyleimages[28],wemodifythestyle • RGB:Toevaluatestylization,wecomputetheSIFIDbe-
5Figure5.QualitativecomparisonswithSNeRF[55],ARF[83]andRef-NPR[86]onthetrexandfernscenes[52].
tweentheRGBstyleimage,S ,andtherenderedRGB and fern scenes [52]. All these methods stylize radiance
rgb
image,Iˆ. fields,guidedbyasinglestyleimage. Thescaleofthestyle
• Gray: Recognizing that shape and pattern forms, be- image plays a crucial role in replicating the patterns from
yond color, influence style, we convert both S and Iˆ the style image; hence, we manually tuned these methods
rgb
tograyscale. WethencomputetheSIFIDbetweenthese to find the optimal configurations. Since SNeRF did not
images,allowingustoexcludetheinfluenceofcolorand provide an official implementation, we used an alternative
measuretheotherstyleelements. versionprovidedbyZhangetal.[86],enablingadensityup-
• Depth: To evaluate the geometry style, we compute the dateasmentionedintheiroriginalpaper. ForRef-NPR,we
SIFIDbetweenthedepthstyleS andtherendereddepth utilized NNST [35] to generate a reference stylized view.
D
map,Dˆ. For ARF, we used the authors’ provided TensoRF version
since the geometry of the scene is noisy and very inaccu-
4.1.QualitativeandQuantitativeComparisons rateintheoriginalversionwithPlenoxels. Weappliedthe
distortionregularizer[2]torefineitsgeometryduringpre-
In Fig. 5, we qualitatively compare our results with re-
trainingforfaircomparisons.
cent top-performing 3D style transfer methods, including
SNeRF[55],ARF[83],andRef-NPR[86]onthethetrex Asshowninthefigure,ourmethodprovidesclearercol-
6trex fern
Method
RGB Gray Depth RGB Gray Depth
SNeRF[55] 1.62 0.81 0.59 1.32 0.64 0.40
ARF[83] 1.54 0.64 0.51 1.11 0.48 0.36
Ref-NPR[86] 1.59 0.72 0.61 1.75 0.79 0.41
Ours 1.43 0.58 0.44 0.81 0.37 0.28
Table 1. Quantitative comparisons of SIFID [66] for RGBs,
grayscale images, and depth maps with recent methods. Lower
scores indicate better performance. For each scene, images are
renderedfrom30viewpoints,andtheiraveragescoreiscomputed.
Figure6. Ablationstudyontheimpactofgeometricfeatures.
Comparisonoftheresultsusingnearestmatchingbasedoncolor
orsandmoreaccuratelystylizedshapes. Notably,ourstyl- featuresversusgeometry-awarematching. Geometryfeaturesen-
izedresultsreplicatetheclearandcompleteformsofstyle hancediversityandenabledistinctstylizations,differentiatingob-
jectswithsimilarcolors.
patterns, an ability not achievable by merely stylizing col-
ors, due to the limited space available for mapping com-
pletepatternswithoutalteringgeometry. Tobespecific,in
order to stylize the fern leaves, it is necessary to change
theirshapebecausetheleavesaresharpandnarrow,which
cannot display patterns of the style image without a shape
deformation. Ourmethodaccuratelystylizesthoseregions
whiletheothersarelimitedtostylizingonlyappearanceto
justhallucinatetheshape. EventhoughSNeRFupdatesthe
density during stylization, the resulting geometry does not
reflectanycuesfromthestyleimagebecauseitlacksproper
guidanceforgeometrystyle.
In Table 1, we compare the SIFID [66] to measure the
style similarity between the style images and the rendered
images. Ourmethodoutperformsothersinallmetrics, en-
compassing both appearance and geometry. This demon- Figure 7. Impact of patch-wise optimization. The patch-wise
stratesthatincorporatingstylizationintogeometry,aswell schemeenhancestheclarityandaccuracyofpatternsandshapes.
as colors, enhances the overall style representation and
moreaccuratelyreflectstheintendedstyles.
(thetopfigure),eachfeaturevectorinthecontentandstyle
featuremapsisindependentlymatchedbasedontherespec-
4.2.AblationExperiments
tivecosinedistances,whichleadstoafailureinmaintaining
Geometry-aware nearest matching. In Fig. 6, we com- localgeometry. Duetoitssmallreceptivefields,thescene
pare the results of the nearest matching exclusively with oftencontainsonlyincompletepartsofpatternsandshapes,
the color features extracted from S and our proposed resultingindecreasedstyleaccuracy. Incontrast,whenap-
rgb
geometry-aware strategy. When using only color features, plyingthepatch-wiseoptimization(thebottomfigure),the
the resulting style includes similar colors and overlapping positionsoflocalneighborswithinthefeaturemapsarepre-
patternsinregionswithasimilarappearance.Thisapproach servedduringthematchingprocess,enablingthecaptureof
tendstowashoutobjectboundaries,renderingthemindis- larger receptive fields. This approach results in the intact
tinguishable,andleadstoalossofcontentstructureanddi- andcompletereproductionofpatternsfromthestyleimage.
versity,particularlyinsemi-transparentobjects. Incontrast, Perspectivestyleaugmentation.InFig.8,wecomparethe
when geometry features are also used, the combined con- effects of our proposed perspective style augmentation on
sideration of shape and color during the matching process stylization. Asdepictedontheleft,stylizingtheentiresur-
results in distinct colorization and patterns across objects, facewithpatternsofuniformsize,regardlessofthedistance
eventhosewithsimilarappearances. Thisdifferentiationof fromthecamera, violatesperspectiverulesanddiminishes
boundariesenhancescontentpreservation. the sense of depth. Conversely, the right column figures
Patch-wiseoptimization.InFig.7,wecomparetheresults demonstratethatdecreasingthepatternsizesbasedontheir
ofthenearestneighborlosswithandwithoutourproposed depth location enhances the perception of depth in the 2D
patch-wise optimization. Without the patch-wise scheme renderedimageandaidsinpreservingthescene’sstructure.
7Figure9.Stylizationwith3Dsemanticlifting.WestylizePanop-
ticLifting, pre-trainedontheScanNetdataset[11], allowingfor
Figure8.Perspectivestyleaugmentationimpact.Theproposed thefreealterationoftargetobjectsforstylizationduringruntime.
augmentation enhances depth perception by mapping larger pat- Asthestylizationaltersthecolorsandshapesofobjects,theseg-
ternstoclosersurfacesandsmallerpatternstomoredistantones. mentationadaptstotheirupdatedforms.
4.3.Application: PartialStylization objects from the stylized grid G c′. This sampling is con-
ductedafterapplyingdeformationtothepoints,denotedas
InFig.9,wedemonstratetheapplicabilityofourmethodin x +∆x . The rest of the scene is rendered using colors
i i
partiallystylizing3Dscenes. Lahirietal.[39]alsotackled fromtheoriginalgridG ,withnodeformationapplied.
c
partial stylization of radiance fields using a pre-computed Limitations and future work. Scene geometry is a cru-
segmentationmaskandoptimizedstylelossforthemasked cialfactorinmaintainingthecontentstructure. Ourmethod
objects.However,thisstrategylimitsflexibility,asitcannot is designed to maximize the expressiveness of stylization
freelychangethetargetobjectswithoutrespectiveoptimiza- by actively distorting the original shapes, although this in-
tionforeachnewsetofobjects.Incontrast,weintegrateour evitablyleadstosomesacrificeoftheoriginalcontent. An-
methodwithPanopticLifting[68],whichinvolvesvolumet- otherlimitationisthefocusonaccuratelytransferringofthe
ricrepresentationsthatrenderview-consistentpanopticseg- exactshapeandpatternsfromasingle-viewstyleimageto
mentations,alongwithRGBsandshapes. Ourmethodcan the 3D scene. This task is highly ill-posed, as the patterns
beseamlesslyincorporatedintothissystem,allowingusto in 3D scenes do not appear identical when rendered from
freelyrenderstylizedobjectclassesandinstancesaccording significantlydifferentviewpoints,likethoseintheScanNet
toourpreferencesduringruntime.
dataset. We circumvent this challenge by selecting a style
The Panoptic Lifting models a function that maps a 3D image with irregular shapes and patterns. However, to ac-
pointx itocolorc i,volumedensityσ i,semanticclassprob- curatelystylizesceneswithdiverseviewpointchanges,such
ability κ i, and object id distribution π i over each class as: asin360-degreescenes,itwouldbemeaningfultoexplore
κ i(k)π i(j). TheunderlyingrepresentationadoptsTensoRF thepossibilityofusingmulti-viewstyleguidesor3Dstyle
and consists of a color grid G c and G σ. To stylize Panop- guides,movingbeyondthesingle-imagestylereference.
ticLifting,weintroduceanadditionaldeformationgridG
∆
andapplyourproposedRGB-Dstylizationmethodstoopti-
5.Conclusion
mizebothG andG . Afterthestyletransfer,weobtainthe
c ∆
stylizedcolorgridG c′ anduseittofreelyrenderthestylized WeproposedGeometryTransfer,anovelmethodthatuses
scene. Itisimportanttonotethateveniftheshapechanges adepthmapasastylisticguideformodifyingthegeometry
afterstylization,ouruseofdeformationfieldsenablessam- ofradiancefields. Byinnovativelyemployingdeformation
plingfromthecanonicalspaceforcoloranddensity,aswell fields, we achieved coherent alteration of both shape and
asfortheclassesandobjectids. Thus,ifthestylizational- appearance in 3D scenes. Building upon this foundation,
terstheoriginalshapes,thesemanticpredictionscohesively we developed novel RGB-D stylization techniques, lever-
adapttothenewshapes. aging geometric cues to enhance aesthetic expressiveness
To render the partially stylized view for specific target and more accurately reflect intended styles. Extensive ex-
classes or objects, we begin by estimating the target class perimentshaveshownthatourmethodsfacilitateabroader
for each 3D point along the rays. During RGB rendering, spectrumofstylizationscomparedtopreviousapproaches,
colorissampledforthe3Dpointsthatcomprisethetarget significantlyexpandingthescopeof3Dstyletransfer.
8References [19] ShuyangGu,CongliangChen,JingLiao,andLuYuan. Ar-
bitrarystyletransferwithdeepfeaturereshuffle. InCVPR,
[1] RudolfArnheim. Artandvisualperception: Apsychology
2018. 2
ofthecreativeeye. UnivofCaliforniaPress,1954. 2
[20] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander
[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Holynski,andAngjooKanazawa. Instruct-nerf2nerf: Edit-
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded ing3dsceneswithinstructions. InICCV,2023. 3
anti-aliasedneuralradiancefields. InCVPR,2022. 5,6 [21] EricHeitz, KennethVanhoey, ThomasChambon, andLau-
[3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter rent Belcour. A sliced wasserstein loss for neural texture
Wonka, and Matthias Mu¨ller. Zoedepth: Zero-shot trans- synthesis. InCVPR,2021. 2
ferbycombiningrelativeandmetricdepth. arXivpreprint [22] DouglasR.Hofstadter. Metamagicalthemas: Variationson
arXiv:2302.12288,2023. 4 athemeastheessenceofimagination. ScientificAmerican,
[4] JunyanCao,YanHong,andLiNiu. Painterlyimageharmo- 1983. 2
nizationindualdomains. InAAAI,2023. 2 [23] Lukas Ho¨llein, Justin Johnson, and Matthias Nießner.
[5] BinditaChaudhuri,NikolaosSarafianos,LindaShapiro,and Stylemesh: Style transfer for indoor 3d scene reconstruc-
TonyTung.Semi-supervisedsynthesisofhigh-resolutioned- tions. InCVPR,2022. 1,2
itabletexturesfor3dhumans. InCVPR,2021. 2 [24] HaozhiHuang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao
Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu. Real-time
[6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
neuralstyletransferforvideos. InCVPR,2017. 1,2
Hao Su. Tensorf: Tensorial radiance fields. In ECCV.
[25] Hsin-PingHuang,Hung-YuTseng,SaurabhSaini,Maneesh
Springer,2022. 3,5
Singh, and Ming-Hsuan Yang. Learning to stylize novel
[7] DongdongChen,JingLiao,LuYuan,NenghaiYu,andGang
views. InICCV,2021. 1,2
Hua. Coherentonlinevideostyletransfer. InICCV,2017. 2
[26] XunHuangandSergeBelongie. Arbitrarystyletransferin
[8] DongdongChen,LuYuan,JingLiao,NenghaiYu,andGang
real-time with adaptive instance normalization. In ICCV,
Hua. Stereoscopicneuralstyletransfer. InCVPR,2018. 2
2017. 2
[9] Tian Qi Chen and Mark Schmidt. Fast patch-based style [27] ZixuanHuang,JinghuaiZhang,andJingLiao. Stylemixer:
transferofarbitrarystyle. arXivpreprintarXiv:1612.04337, Semantic-aware multi-style transfer network. In Computer
2016. 1,2 GraphicsForum.WileyOnlineLibrary,2019. 2
[10] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei- [28] Yongcheng Jing, Yang Liu, Yezhou Yang, Zunlei Feng,
ShengLai,andWei-ChenChiu. Stylizing3dsceneviaim- Yizhou Yu, Dacheng Tao, and Mingli Song. Stroke con-
plicitrepresentationandhypernetwork. InWACV,2022. 2, trollablefaststyletransferwithadaptivereceptivefields. In
3 ECCV,2018. 5
[11] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal- [29] JustinJohnson,AlexandreAlahi,andLiFei-Fei. Perceptual
ber, Thomas Funkhouser, and Matthias Nießner. Scannet: losses for real-time style transfer and super-resolution. In
Richly-annotated 3d reconstructions of indoor scenes. In ECCV,2016. 2
CVPR,2017. 5,8 [30] YucheolJung,WonjongJang,SoongjinKim,JiaolongYang,
[12] YingyingDeng,FanTang,WeimingDong,WenSun,Feiyue XinTong,andSeungyongLee. Deepdeformable3dcarica-
Huang, and Changsheng Xu. Arbitrary style transfer via tureswithlearnedshapecontrol. InACMSIGGRAPH2022
multi-adaptationnetwork. InProceedingsofthe28thACM ConferenceProceedings,2022. 3
internationalconferenceonmultimedia,2020. 2 [31] HiromichiKamata,YuikoSakuma,AkioHayakawa,Masato
Ishii, and Takuya Narihira. Instruct 3d-to-3d: Text in-
[13] YuDeng,JiaolongYang,andXinTong. Deformedimplicit
struction guided 3d-to-3d conversion. arXiv preprint
field: Modeling 3d shapes with learned dense correspon-
arXiv:2303.15780,2023. 3
dence. InCVPR,2021. 3
[32] ZhanghanKe,ChunyiSun,LeiZhu,KeXu,andRynsonWH
[14] ZhiwenFan,YifanJiang,PeihaoWang,XinyuGong,Dejia
Lau.Harmonizer:Learningtoperformwhite-boximageand
Xu, andZhangyangWang. Unifiedimplicitneuralstyliza-
videoharmonization. InECCV,2022. 2
tion. InECCV,2022. 2
[33] SunnieSYKim,NicholasKolkin,JasonSalavon,andGre-
[15] Anna Fru¨hstu¨ck, Nikolaos Sarafianos, Yuanlu Xu, Peter
goryShakhnarovich. Deformablestyletransfer. InECCV,
Wonka, and Tony Tung. VIVE3D: Viewpoint-independent
2020. 2
videoeditingusing3D-awareGANs. InCVPR,2023. 2
[34] Nicholas Kolkin, Jason Salavon, and Gregory
[16] ChenGao,AyushSaraf,JohannesKopf,andJia-BinHuang.
Shakhnarovich. Styletransferbyrelaxedoptimaltransport
Dynamicviewsynthesisfromdynamicmonocularvideo. In andself-similarity. InCVPR,2019. 1,2
ICCV,2021. 3
[35] Nicholas Kolkin, Michal Kucera, Sylvain Paris, Daniel
[17] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Sykora, Eli Shechtman, and Greg Shakhnarovich. Neural
Imagestyletransferusingconvolutionalneuralnetworks. In neighbor style transfer. arXiv preprint arXiv:2203.13215,
CVPR,2016. 1,2 2022. 3,6
[18] XinyuGong,HaozhiHuang,LinMa,FuminShen,WeiLiu, [36] DmytroKotovenko, ArtsiomSanakoyeu, SabineLang, and
andTongZhang. Neuralstereoscopicimagestyletransfer. BjornOmmer.Contentandstyledisentanglementforartistic
InECCV,2018. 2 styletransfer. InICCV,2019. 2
9[37] Dmytro Kotovenko, Artsiom Sanakoyeu, Pingchuan Ma, [55] ThuNguyen-Phuoc,FengLiu,andLeiXiao. Snerf:stylized
SabineLang, andBjornOmmer. Acontenttransformation neuralimplicitrepresentationsfor3dscenes.ACMTOG,41,
blockforimagestyletransfer. InCVPR,2019. 2022. 1,2,5,6,7
[38] Dmytro Kotovenko, Matthias Wright, Arthur Heimbrecht, [56] Hong-Wing Pang, Binh-Son Hua, and Sai-Kit Yeung. Lo-
and Bjorn Ommer. Rethinking style transfer: From pixels callystylizedneuralradiancefields. InICCV,2023. 1,2
toparameterizedbrushstrokes. InCVPR,2021. 2 [57] Yingxue Pang, Jianxin Lin, Tao Qin, and Zhibo Chen.
[39] Dishani Lahiri, Neeraj Panse, and Moneish Kumar. S2rf: Image-to-image translation: Methods and applications.
Semanticallystylizedradiancefields. InICCVW,2023. 8 IEEETransactionsonMultimedia,24,2021. 5
[58] DaeYoungParkandKwangHeeLee. Arbitrarystyletrans-
[40] Chuan Li and Michael Wand. Combining markov random
ferwithstyle-attentionalnetworks. InCVPR,2019. 2
fieldsandconvolutionalneuralnetworksforimagesynthesis.
[59] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
InCVPR,2016. 1,2
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo
[41] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang.
Martin-Brualla. Nerfies: Deformableneuralradiancefields.
Learninglineartransformationsforfastarbitrarystyletrans-
InICCV,2021. 3
fer. arXivpreprintarXiv:1808.04537,2018. 1,2
[60] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan
[42] YijunLi,ChenFang,JimeiYang,ZhaowenWang,XinLu,
Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-
andMing-HsuanYang. Universalstyletransferviafeature
matableneuralradiancefieldsformodelingdynamichuman
transforms. InNeurIPS,2017. 2
bodies. InICCV,2021. 3
[43] Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing
[61] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
Kang. Visualattributetransferthroughdeepimageanalogy.
FrancescMoreno-Noguer.D-nerf:Neuralradiancefieldsfor
ACMTOG,36(4),2017. 2
dynamicscenes. InCVPR,2021. 2,3
[44] Hsueh-TiDerekLiu,MichaelTao,andAlecJacobson. Pa- [62] EricRisser,PierreWilmot,andConnellyBarnes. Stableand
parazzi: Surface editing by way of multi-view image pro- controllableneuraltexturesynthesisandstyletransferusing
cessing. ACMTransactionsonGraphics,2018. 1,2 histogramlosses. arXivpreprintarXiv:1701.08893,2017. 2
[45] Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, [63] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.
Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, and Artisticstyletransferforvideosandsphericalimages.IJCV,
Eric P Xing. Stylerf: Zero-shot 3d style transfer of neural 126(11),2018. 2
radiancefields. InCVPR,2023. 1,2,3 [64] ArtsiomSanakoyeu, DmytroKotovenko, SabineLang, and
[46] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling Bjorn Ommer. A style-aware content loss for real-time hd
Wang, Xin Li, Zhengxing Sun, Qian Li, and Errui Ding. styletransfer. InECCV,2018. 2
Adaattn: Revisit attention mechanism in arbitrary neural [65] Jonathan Shade, Steven Gortler, Li-wei He, and Richard
styletransfer. InICCV,2021. 2 Szeliski. Layereddepthimages. InProceedingsofthe25th
[47] Xiao-Chang Liu, Xuan-Yi Li, Ming-Ming Cheng, and Annual Conference on Computer Graphics and Interactive
Peter Hall. Geometric style transfer. arXiv preprint Techniques,1998. 5
arXiv:2007.05471,2020. 2 [66] TamarRottShaham,TaliDekel,andTomerMichaeli. Sin-
gan: Learningagenerativemodelfromasinglenaturalim-
[48] Xiao-ChangLiu,Yong-LiangYang,andPeterHall.Learning
age. InICCV,2019. 5,7
towarpforstyletransfer. InCVPR,2021. 2
[67] LuSheng,ZiyiLin,JingShao,andXiaogangWang.Avatar-
[49] FujunLuan,SylvainParis,EliShechtman,andKavitaBala.
net: Multi-scale zero-shot style transfer by feature decora-
Deepphotostyletransfer. InCVPR,2017. 1,2
tion. InCVPR,2018. 2
[50] FujunLuan,SylvainParis,EliShechtman,andKavitaBala.
[68] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo`, Nor-
Deeppainterlyharmonization.InComputergraphicsforum.
man Mu¨ller, Matthias Nießner, Angela Dai, and Peter
WileyOnlineLibrary,2018. 2
Kontschieder. Panoptic lifting for 3d scene understanding
[51] RoeyMechrez, ItamarTalmi, andLihiZelnik-Manor. The
withneuralfields. InCVPR,2023. 2,8
contextual loss for image transformation with non-aligned
[69] KSimonyanandAZisserman. Verydeepconvolutionalnet-
data. InECCV,2018. 2
worksforlarge-scaleimagerecognition. InICLR,2015. 2,
[52] BenMildenhall, PratulPSrinivasan, RodrigoOrtiz-Cayon, 3,5
NimaKhademiKalantari,RaviRamamoorthi,RenNg,and [70] EdgarTretschk,AyushTewari,VladislavGolyanik,Michael
AbhishekKar. Locallightfieldfusion: Practicalviewsyn- Zollho¨fer,ChristophLassner,andChristianTheobalt. Non-
thesiswithprescriptivesamplingguidelines. ACMTransac- rigidneuralradiancefields: Reconstructionandnovelview
tionsonGraphics(TOG),38,2019. 5,6 synthesis of a dynamic scene from monocular video. In
[53] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, ICCV,2021. 3
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf: [71] Yi-HsuanTsai, XiaohuiShen, ZheLin, KalyanSunkavalli,
Representingscenesasneuralradiancefieldsforviewsyn- XinLu,andMing-HsuanYang. Deepimageharmonization.
thesis. InECCV,2020. 1,2 InCVPR,2017. 2
[54] FangzhouMu,JianWang,YichengWu,andYinLi.3dphoto [72] CanWang,MengleiChai,MingmingHe,DongdongChen,
stylization:Learningtogeneratestylizednovelviewsfroma andJingLiao. Clip-nerf: Text-and-imagedrivenmanipula-
singleimage. InCVPR,2022. 1,2 tionofneuralradiancefields. InCVPR,2022. 3
10[73] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He,
DongdongChen,andJingLiao.Nerf-art:Text-drivenneural
radiancefieldsstylization. IEEETransactionsonVisualiza-
tionandComputerGraphics,2023. 3
[74] Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich
Neumann. 3dn:3ddeformationnetwork. InCVPR,2019. 3
[75] WenjingWang,JizhengXu,LiZhang,YueWang,andJiay-
ingLiu. Consistentvideostyletransferviacompoundregu-
larization. AAAI,2020. 2
[76] Wenjing Wang, Shuai Yang, Jizheng Xu, and Jiaying Liu.
Consistentvideostyletransferviarelaxationandregulariza-
tion. IEEETransactionsonImageProcessing,29,2020. 1,
2
[77] Xiaolei Wu, Zhihao Hu, Lu Sheng, and Dong Xu. Style-
former: Real-time arbitrary style transfer via parametric
stylecomposition. InICCV,2021. 2
[78] ZijieWu,ZhenZhu,JunpingDu,andXiangBai. Ccpl:con-
trastivecoherencepreservinglossforversatilestyletransfer.
InECCV.Springer,2022. 5
[79] XideXia,MengZhang,TianfanXue,ZhengSun,HuiFang,
Brian Kulis, and Jiawen Chen. Joint bilateral learning for
real-time universal photorealistic style transfer. In ECCV,
2020. 2
[80] Xide Xia, Tianfan Xue, Wei-sheng Lai, Zheng Sun, Abby
Chang, BrianKulis, andJiawenChen. Real-timelocalized
photorealisticvideostyletransfer. 2021. 1,2
[81] Shuai Yang, Zhangyang Wang, Zhaowen Wang, Ning Xu,
Jiaying Liu, and Zongming Guo. Controllable artistic text
styletransferviashape-matchinggan. InICCV,2019. 2
[82] Jordan Yaniv, Yael Newman, and Ariel Shamir. The face
ofart: Landmarkdetectionandgeometricstyleinportraits.
ACMTOG,38(4),2019. 2
[83] KaiZhang, NickKolkin, SaiBi, FujunLuan, ZexiangXu,
Eli Shechtman, and Noah Snavely. Arf: Artistic radiance
fields. InECCV,2022. 1,2,3,5,6,7
[84] LingzhiZhang,TarmilyWen,andJianboShi. Deepimage
blending. InWACV,2020. 2
[85] PanZhang,BoZhang,DongChen,LuYuan,andFangWen.
Cross-domain correspondence learning for exemplar-based
imagetranslation. InCVPR,2020. 2
[86] YuechenZhang,ZexinHe,JinboXing,XufengYao,andJi-
ayaJia. Ref-NPR:Reference-basednon-photorealisticradi-
ancefieldsforcontrollablescenestylization.InCVPR,2023.
1,2,3,5,6,7
[87] Zicheng Zhang, Yinglu Liu, Congying Han, Yingwei Pan,
TiandeGuo,andTingYao.Transformingradiancefieldwith
lipschitznetworkforphotorealistic3dscenestylization. In
CVPR,2023. 2,3
11