Early Time Classification with Accumulated Accuracy Gap Control
LiranRingel1,RegevCohen2,DanielFreedman2,MichaelElad2,andYanivRomano1,3
1DepartmentofComputerScience,TechnionIIT,Haifa,Israel
2VerilyAI,Israel
3DepartmentofElectricalandComputerEngineering,TechnionIIT,Haifa,Israel
Abstract
Earlytimeclassificationalgorithmsaimtolabelastreamoffeatureswithoutprocessingthefullinputstream,
whilemaintainingaccuracycomparabletothatachievedbyapplyingtheclassifiertotheentireinput.Inthispaper,we
introduceastatisticalframeworkthatcanbeappliedtoanysequentialclassifier,formulatingacalibratedstoppingrule.
Thisdata-drivenruleattainsfinite-sample,distribution-freecontroloftheaccuracygapbetweenfullandearly-time
classification. WestartbypresentinganovelmethodthatbuildsontheLearn-then-Testcalibrationframeworkto
controlthisgapmarginally,onaverageoveri.i.d. instances. Asthisalgorithmtendstoyieldanexcessivelyhigh
accuracygapforearlyhalttimes,ourmaincontributionistheproposalofaframeworkthatcontrolsastrongernotion
oferror,wheretheaccuracygapiscontrolledconditionallyontheaccumulatedhalttimes.Numericalexperiments
demonstratetheeffectiveness,applicability,andusefulnessofourmethod.Weshowthatourproposedearlystopping
mechanismreducesupto94%oftimestepsusedforclassificationwhileachievingrigorousaccuracygapcontrol.
1 Introduction
Question: What was the nationality of Ronald Fisher?
Options: (1) American (2) British (3) Canadian (4) Australian
Context: Sir Ronald Aylmer Fisher FRS (17 February 1890 – 29 July 1962) was a British polymath
who was active as a mathematician, statistician, biologist, geneticist, and academic. For his work
in statistics, he has been described as ”a genius who almost single-handedly created the foundations
for modern statistical science” and ”the single most important figure in 20th century statistics”.
In genetics, his work used mathematics to combine Mendelian genetics and natural selection...
Answer: (2) British.
Figure1: Anillustrationofareadingcomprehensiontask. AnLLMsequentiallyprocessesthegivendocumentto
findtheanswertothequestionprovidedand,ideally,shouldstopscanningthedocumentimmediatelyaftertherequired
informationisfound. ThecontextistakenfromWikipedia.
Thegoalofearlytimeseriesclassification(ETSC)istopredictthelabelofagiveninputdatastreamasquicklyas
possible. Suchmethodsareespeciallyadvantageousinscenariosrequiringpromptpredictiveinference. Forexample,
considertheproblemofreadingcomprehension,illustratedinFigure1. Supposeweemployanautoregressivelarge
languagemodel(LLM)toanalyzeagivendocument(context)andselectananswertotheprovidedquestion. Given
thattheinferencetimeofLLMsincreaseswiththenumberofprocessedtokens(orsentences),wewishtoterminate
the processing of the context retrieved from the document as soon as the necessary information is found, rather
thanprocessingtheentiredocumentnaively. OthertasksforwhichETSCishighlydesiredincludereal-timesong
identification(thinkoftheShazamapplication)andreducingradiationexposureincomputationaltomography(CT)
systems,amongmanyothers. Inalloftheseapplications,theobjectiveistostoptheinferenceprocessearlywhile
preservingaccuracy,asifthepredictivemodelhadbeenappliedtotheentiredatastream.
Consider labeled pairs of the form (X,Y) sampled i.i.d. from P XY, where X = (X1,X2,...,Xtmax) ∈ X
representsanobservedinputsequencewithamaximumlengthoft ,e.g. asequenceoftokensrepresentingsentences
max
1
1
4202
beF
1
]GL.sc[
1v75800.2042:viXrainadocumentandaquestion. ThevariableY ∈ Y = {1,...,K}istheunknownlabelwewishtopredict,e.g. the
correctanswertothegivenquestion. Supposewearehandedapre-trainedclassifierfˆ:X →[0,1]K thatprocessesthe
inputX sequentiallyand,ateachtimestept,mapsX≤t =(X1,...,Xt)toanestimatedprobabilitydistributionover
thelabels. Weemployastoppingrulefunctionthat,ateachtimestept,decideswhethertostoptheinferenceprocess
onlybasedonthedataobserveduptotimestept. Denotethestoppingtimebyτˆ(X)∈{1,...,t }andletYˆ (τˆ)
max early
andYˆ bethepredictedlabelsobtainedbyfˆ(X≤τˆ(X))andfˆ(X),respectively. Withthesenotationsinplace,we
full
definetheaccuracygapastheproportionofsamplesforwhichtheclassifier’spredictioniscorrectwhenappliedtothe
entiresequencebutincorrectwhenthesameclassifierisappliedonlyuptotheearlytimestepτˆ(X).
Letα∈(0,1),e.g.,10%,bethetolerableaccuracygap,representingtheacceptabletrade-offforearlystopping.
DenotebyD ={(X ,Y )}ncal aholdoutcalibrationset,withsamplesdrawni.i.d. fromP . Ourinitialobjectiveis
cal i i i=1 XY
toleverageD toidentifyanearlystoppingruleτˆ(X)thatminimizesthehalttimewhileensuringtheaccuracygap
cal
remainsbelowαwithaprobabilityofatleast1−δ:
P (cid:0) Rmarginal(τˆ)≤α(cid:1) ≥1−δ, (1)
Dcal gap
where
(cid:104) (cid:105)
Rmarginal(τˆ)=E L (Y,Yˆfull,Yˆearly(τˆ)) , (2)
gap PXY gap
and
(cid:16) (cid:17)
L (Y,Yˆfull,Yˆearly(τˆ))= I −I . (3)
gap Y=Yˆfull Y=Yˆearly(τˆ)
+
Notably,theprobabilityin(1)istakenovertherandomnessinD ,andδisauser-definedlevel,e.g.,1%. Theoperator
cal
(z) in(3)returnsthevaluezifz ≥0andzerootherwise,andtheindicatorfunctionI equals1whena=band
+ a=b
zerootherwise. Insimplerterms, theexpectedvalueofL (Y,Yˆfull,Yˆearly(τˆ)) ∈ {0,1}reflectstheproportionof
gap
samplesinwhichthedecisiontostopearlyincreasestheerrorrate. Wereferto(1)asmarginalriskcontrolasitstates
thattheaccuracygapwillnotexceedα,onaverageoverfutureobservationsandstoppingtimes. InSection3,we
presentanalgorithmthatrigorouslyattains(1),termedthemarginalmethod.
Whilethemarginalguaranteein(1)providesacontrolledmechanismforearlyclassification,itmaynotbeentirely
satisfying in most practical settings. This is because an algorithm that controls the accuracy gap over all possible
sequencesispermittedtoperformpoorlyonsequenceswithearlyhalttimeswhileexcellingonsequenceswithlatehalt
times. This,inturn,canunderminethereliabilityofpredictionsforsequenceswithearlyhalttimes. Recognizingthis
limitation,ourmaincontributionisanovelalgorithmthataimstocontroltheaccuracygapconditionalonthehalttime
beinglessorequaltot. Moreformally,let
(cid:104) (cid:105)
R≤t(τˆ)=E L (Y,Yˆfull,Yˆearly(τˆ))|τˆ(X)≤t . (4)
gap PXY gap
Ourgoalistoformulateastoppingrulethatachieves
P (cid:0) R≤t(τˆ)≤αforallt≥t (cid:1) ≥1−δ, (5)
Dcal gap 0
wheret isdefinedasthefirsttimestepforwhichP(τˆ(X) ≤ t ) > 0,asotherwise(4)isundefined. Inparticular,
0 0
controlling(5)impliesthatwealsocontroltheaccuracygapmarginally,asR gm aa prginal =R g≤ at pmax. Throughoutthiswork,
wereferto(5)asconditionalriskcontrolontheaccumulatedhalttime,orsimply,conditionalriskcontrol. InSection4
wepresentanalgorithmthatachievesthisgoal,whichwerefertoastheconditionalmethod.
Itiscrucialtodistinguish(5)fromthestrongertime-orinstance-conditionalguarantee,wheretheobjectiveisto
controltheaccuracygapforaspecifictimesteptorforaspecificX. Unfortunately,attainmentofnon-trivialstopping
ruleswithtime-orinstance-conditionalriskcontrolisinfeasiblewithoutresortingtounrealisticassumptions[1–3],
whichweaimtoavoid: wepursuedistribution-free,finite-sampleguarantees. Asaconsequence,wepositthattherisk
in(5)strikesareasonablecompromisebetweencontrollingtherelativelyweakmarginalriskandtheunattainabletime-
orinstance-conditionalrisk.
1.1 AMotivatingExample: ReadingComprehension
Toemphasizetheimportanceofthetransitionfromthemarginal(1)totheconditionalguarantee(5),wenowreturnto
thereadingcomprehensionproblemdiscussedearlier. TheQuALITYdataset[4]consistsof4609triplets,containing
2Figure 2: Comparison between the marginal and conditional methods for the reading comprehension task.
Nominalaccuracygaplevelisα=10%andδ =1%. Left: empiricalconditionalaccuracygap,Rˆ≤t,across100trials;
gap
eachcurvecorrespondstoadifferentrandomsplitofthecalibrationandtestdata. Right: accumulatedhalttimesasa
functionoft,averagedover100randomsplits;theshadedarearepresentsa95%confidenceinterval.
(i) a question, (ii) multiple choice answers, and (iii) a long context, with each triplet accompanied by the correct
labeledanswer. Weutilizeapre-trainedautoregressiveLLMasthebasepredictivemodel. Thisclassifiersequentially
processesthecontextandselectsananswerfromthefourpossibilities. Forthecalibrationoftheearlystoppingrule,
weemploy3073labeledsamplestoformD whilereservingtheremaining1536samplesfortesting. Followingthis,
cal
wecomparetheperformanceoftheproposedmarginalandconditionalcalibrationmethodspresentedinSections3
and4,respectively. Specifically,wereporttwoperformancemetrics: (i)Rˆ≤t,definedastheempiricalaccuracygapof
gap
sampleswithahalttimeτˆ(X)equaltoorlessthant;and(ii)thecumulativenumberofsamplesonwhichthemodel
halteduntiltimestept.
TheresultsarepresentedinFigure2.Followingtheleftpanelinthatfigure,wecanseethatwhilethetwoapproaches
control the marginal risk, the conditional accuracy gap Rˆ≤t tends to be higher than the desired 10% level for the
gap
marginalmethod. Thisimpliesthatthemarginalstoppingruletendstohalttooearly,asevidencedintherightpanelof
Figure2,wherewecanseetherelativelylargenumberofsampleshaltedattimestept=1. Incontrast,theconditional
approachmaintainstheconditionalaccuracygapbelowαacrossalltimesteps(leftpanel)whileattaininganeffective
earlystoppingmechanism(rightpanel).
1.2 Previewofourmethods
Thecruxofthisworkistheformulationofastoppingruleτˆ(X)thatattainsvalidriskcontrol.Denotebyπˆ :X →[0,1]
ascorethatheuristicallyreflectshowconfidenttheclassifierisinitspredictionbasedonX≤t. Forexample,πˆ(X≤t)
canbethelargestsoftmaxvalueofaneuralnetclassifier. Withthisinplace,wecanformulate
τˆ(X)=τ (X)=min{t:πˆ(X≤t)≥λˆ ort=t }, (6)
λˆ t max
whereλˆ isahyperparameter,beingthet-thelementinavectorofthresholdsλˆ =(λˆ ,λˆ ,...,λˆ ). Inplainwords,
t 1 2 tmax
wechoosetohalttheinferenceprocessforthefirsttimetthattheclassifieris“confidentenough”initsprediction.
Buthowcanweproperlychoosethevectorofhyperparametersλˆthatattainsavalidriskcontrol? Notably,thistask
becomesparticularlychallengingwhendealingwithalargenumberofhyperparametersthatrequiretuning;inour
case,wehavet parameters. Animproperchoiceofhyperparameterscanfailtoachievethedesiredaccuracygapon
max
futuretestdata,andthisproblemisespeciallypronouncedwhentheaccuracygapisanon-monotonefunctionofthe
hyperparameters,whichmayoccurinoursettingduetothecomplexnatureofthepre-trainedclassifierathand.
Totacklethischallenge,webuildontheLearnthenTest(LTT)framework[5]thatformulatestheproblemoffinding
hyperparametersthatyieldriskcontrolasamultiplehypothesistestingproblem,whereeachhypothesiscorrespondsto
adifferentchoiceofhyperparameters. However,insituationswithavastarrayofparametersthatneedtobetuned,this
methodfacestwopracticalobstacles[6]. First,thesheervolumeofpotentialconfigurations,whichgrowsexponentially
witht ,makesanextensivesearchofhyperparametersinfeasible. Second,theLTTmethodmayexperiencealossof
max
powerwhenconfrontedwithsuchanexponentialnumberoftests. Thisdrawbackcanresultinouralgorithmstopping
toolate,potentiallymissingtheopportunitytoselectamorerefinedsetofhyperparametersforthedownstreamtask.
To alleviate these limitations, we propose a two-stage calibration framework that exploits the special structure
of the underlying ETSC problem. In the first stage, we find a candidate set of hyperparameters using a novel
computationallyefficientprocedure. Then,weapplyamultipletestingprocedureonthecandidatesettoselectavalid
setofhyperparametersthatyieldsriskcontrol. Overall,thenovelalgorithmweintroducecanefficientlyhandlelong
3sequences,whileselectingadata-adaptivethresholdvectorλˆthatformulatesastatisticallyvalidearlystoppingrule. In
turn,thecontributionsofthisworkarethefollowing:
1. AnovelapplicationforLTT:weintroduce,forthefirsttime,methodologiesthatsupportETSCalgorithmswith
rigorousdistribution-free,finite-samplerisk-controllingguarantees.
2. Marginalriskcontrol: wepresentaflexibleframeworkthatallowspredictivemodelstostopearlytheinference
processwhilecontrollingtheaverageaccuracygap.
3. Conditionalriskcontrol: next,weintroduceanovelalgorithmforearlystoppingthatcontrolstheaccuracygap
conditionalontheaccumulatedhalttimes.
4. Theorypreciselyholdsinpractice: weillustratetheeffectivenessofouralgorithmsbyapplyingthemtodiverse
tasks.Theseincludestandardtimeseriesclassificationdatasetsandanovelapplicationinnaturallanguageprocessing
(NLP).Ourmethodscontrolstheriskwhilesavingupto94%ofthetimestepsavailabletomakepredictions. A
softwarepackageimplementingtheproposedmethodsispubliclyavailableonGitHub.1
2 Related Work
There is active research in developing machine learning models for ETSC with stopping rules that aim to balance
accuracyandearlytermination[7–14]. Whilethesetoolsareeffectiveinpractice,theyoftenlackstatisticalassurance.
Ourproposalenrichesthisimportantlineofresearchbyintroducingversatiletools,compatiblewithanystate-of-the-art
ETSCmodel,whichrigorouslycontroltheaccuracygap,beitinamarginalorconditionalsense.
Ourproposaliscloselyrelatedtocalibratedpredictiveinferencetechniques,includingconformalprediction,risk-
controlling methods, and selective classification [15–29]. Specifically, we expand the toolbox of risk-controlling
tools,particularlywhenfacingsituationswithhighdimensionalhyperparameterspace. ThepioneeringLTTworkby
Angelopoulosetal.[5]offersanapproachtofindadata-drivenconfigurationofparametersthat,forexample,canbeused
tosimultaneouslycontrolmultiplerisks. However,thisapproachcanmostlyhandlelowdimensionalhyperparameter
spaceandbecomesintractablewhenthesearchspaceislarge. Recognizingthislimitation,Laufer-Goldshteinetal.
[6,30]utilizeBayesianoptimizationtoolstofindParetooptimalcandidateconfigurationsacrossvariousrisks,which,in
turn,improvethecomputationalandstatisticalefficiencyofLTT.Thislineofworksharessimilaritieswiththechallenges
wefaceinthispaper;however,insteadofutilizingageneralpurposeBayesianoptimizationtoolforparametertuning,
orusingexhaustivesearch,wedesignaspecializedprocedurethatbuildsuponthestructureoftheETSCproblem. Our
proposalresultsinacomputationallyefficienttechniquetoidentifyplausibleconfigurationsamongthepotentially
enormoussearchspaceofλ,thatcontrolstheaccuracygapwithmeaningfulstatisticalpower.
Ourapproachisalsoalignedwithrecenteffortstodesigncalibrationmethodsthataimtoreducethecomputational
complexityofLLMs[31,32]. Thesemethodsinvolveformulatinganearlyexitmechanismwith,forexample,marginal
accuracygapcontrol. Akeydifferencebetweentheabovemethodsandourproposalisthatweapplyearlyexitover
thetimehorizonratherthanovertheintermediatetransformerlayers. Furthermore,acrucialconceptualandtechnical
differenceisourtransitionfrommarginaltoconditionalguarantees,departingfromthecontributionsmentionedabove.
3 Warm-up: Marginal Accuracy Gap Control
To set the stage for our framework for conditional risk control, we start by presenting a method that achieves the
modestmarginalguaranteein(1). Thedevelopmentofthismethodalsoexposesthereadertothestatisticalprinciples
ofLTT.Later,inSection4,wewillbuildonthefoundationsofthemethodpresentedhereandintroduceourmain
contribution—amethodologythatattainstheconditionalguaranteeof(5). Tofurthersimplifytheexpositionofthe
proposedmarginalapproach,considertuningasingleparameterλˆ ∈[0,1]∪{∞}foralltimestepssothatthestopping
ruleτ (X)=min{t:πˆ(X≤t)≥λˆort=t }achieves(1).
λˆ max
Tostartwith,supposewehandedacandidateparameterλ,e.g.,λ=0.7,andweareinterestedintestingwhetherit
controlstheaccuracygap. FollowingtheLTT[5]approach,wedefinethenullhypothesisinducedbyλasfollows:
H :Rmarginal(τ )>α. (7)
0,λ gap λ
1https://github.com/liranringel/etc
4Thatis, ifthenullisfalse,ourcandidateλcontrolsthemarginalaccuracygap. Withthisinplace, weformulatea
statisticaltestthatutilizestheobservedlabeleddata—thecalibrationset—todecidewhetherwecanrejectH and
0,λ
reportthatλislikelytocontroltheriskoracceptH ifthereisnotenoughevidencetorejectthenull. Toformulate
0,λ
suchatest,wecomputeap-valuep ,whereavalidp-valuesatisfiesthefollowingpropertyunderthenull:
λ
P (p ≤u|H )≤u, u∈[0,1]. (8)
Dcal λ 0,λ
Inplainwords,ifH istrue,thep-valueisstochasticallygreaterthanorequaltouniformdistributionon[0,1]. Hence,
0,λ
consideringasinglehypothesis,whenobservingp ≤ δ wecansafelyrejectH ,knowingthattheprobabilityof
λ 0,λ
falselyrejectingthenull(typeIerror)isatmostδ.
Tocomputesuchap-value,weleveragethefactthatthelossL isbinary, andthuswecanemploytheexact
gap
tail bound from Bates et al. [20] (Appendix B); see also Brown et al. [33]. In more detail, denote the cumulative
distribution function of the binomial distribution by CDF (kˆ;n,α) where kˆ is the number of successes, n is the
bin
numberofindependentBernoullitrials,andαistheprobabilityofsuccess. Thus,inourcase,thep-valueispˆ =
λ
(cid:16) (cid:17)
CDF nRˆ (τ );n,α ,whereRˆ (τ )istheempiricalaccuracygapobtainedbythestoppingruleτ ,evaluated
bin gap λ gap λ λ
onn=|D |i.i.d. samples. Putsimply,thisformulatransformstheempiricalrisk,evaluatedonthecalibrationsetD ,
cal cal
intoap-valuethatsatisfies(8).
Thusfarwehavediscussedtheproblemoftestingforasinglehypothesis,i.e.,testingwhetheraspecificcandidateλ
doesnotcontroltheaccuracygap.However,naturally,thetaskoffindingλˆthatpromotesearlystoppingwhilecontrolling
theriskinvolvestestingformultiplehypotheses: eachhypothesisH correspondstoadifferentλ ∈Λ, 1≤i≤|Λ|,
0,λi i
whereΛ={0,∆,2∆,...,1}={λ ,λ ,...,λ }isadiscretizedgridofpossiblevaluesand∆∈(0,1)definesthe
1 2 |Λ|
resolutionofthegrid.
Thechallengethatarisesisthatwemusttestallhypothesessimultaneously. Toclarify,anaiverejectionrulep ≤δ
λi
canleadtoahighprobabilitythatsomeofthetruenullhypothesesarerejectedbychancealone,andthisprobability
increaseswiththenumberoftruenullsthataretested[34]. Totacklethisissue,wefollowAngelopoulosetal.[5]and
formulateamultipletestingprocedurethatcontrolsthefamily-wiseerrorrate(FWER).Formally,letV bethenumber
oftruenullsthatarefalselyrejectedbythetestingprocedure,anddefineFWER = P(V ≥1)astheprobabilityof
falselyrejectingatleastonetruenullhypothesis. Therefore,tocontrol(1),weshoulddesignatestingprocedurethat
ensurestheFWERdoesnotexceedδ.
TorigorouslycontroltheFWER,weadoptthefixedsequencetestingprocedure[35]usedinLTT,asfollows. First,
weorderthehypothesesfrommostplausibletoleastwithoutlookingatthecalibrationdata. Inourcontext,higher
thresholdsaremorelikelytocontroltheriskin(1),andthereforeweorderthehypothesesfromthelargestλ tothe
|Λ|
smallestλ . Then,wearrangethep-valuesaccordingtothisorderingandsequentiallycompareeachp-valuetothe
1
desiredlevelδ. Thissequentialtestingprocedureterminatesthefirsttimej thatp > δ,resultinginasetofvalid
λj
thresholdsR = {λ : i > j}∪{∞}. Importantly,anythresholdinthesetRisguaranteedtocontrol(1),including
i
thetrivialchoiceforwhichλ = ∞. (Whenλ = ∞themodelwillneverstopearlyandthustriviallyachieveszero
accuracygap.) Sinceourgoalistoformulatearulethatstopsasearlyaspossible,wesetthefinalλˆtobethesmallestλ
amongtherejectedones,i.e.,λˆ =λ ,orλˆ =∞ifp >δ. Foreaseofreference,thisprocedureissummarizedin
j+1 λ|Λ|
AlgorithmA.3,presentedinAppendixA,anditsvalidityisadirectconsequenceofusingfixedsequencetestingto
controltheFWERatlevelδ.
Proposition1. Assumingthecalibrationandtestsamplesarei.i.d.,withλˆselectedasoutlinedinAlgorithmA.3,the
stoppingruleτ (X)satisfies(1).
λˆ
AllproofsarepresentedinAppendixB.Inplainwords,theabovepropositionimpliesthatAlgorithmA.3formulates
a stopping rule that achieves marginal accuracy gap control given a finite calibration set, no matter what the data
distributionis,andregardlessofthechoiceofthe“black-box”classifier.WhileProposition1isappealing,theusefulness
ofthemarginalguaranteeinreal-worldscenariosmaybelimited,asdiscussedanddemonstratedinSection1.1. This
limitationpromptsourexplorationinthenextsection.
4 Conditional Accuracy Gap Control
Wenowturntopresentthefocalpointofthiswork: aframeworkdesignedtocontroltheconditionalaccuracygap(5).
Beyondthetransitionfrommarginaltoconditionalguarantee,inthissectionweutilizeamoregeneralformulationof
5thestoppingrule,inwhichτˆ(X)=τ (X)=min{t:πˆ(X≤t)≥λˆ }withλˆ =(λˆ ,λˆ ,...,λˆ ). Thischoiceadds
λˆ t 1 2 tmax
additionalflexibilitytotheproposedframeworkcomparedtotuningasingleparameter(asinSection3),allowingusto
formulatemoreeffectiveearlystoppingrules.
AnalogouslytoSection3,wewilladoptthefixedsequencetestingproceduretoconstructarejectionsetRthat
contains the configurations of λ that control the conditional risk. In the view of multiple testing, now each null
hypothesesisformulatedas
H :R≤t(τ )>α foratleastonet≥t , (9)
0,λ gap λ 0
wheret isthefirsttimestepatwhichtheprobabilityforanearlystoppingeventisnotzero,i.e.,P(τ (X)≤t )>0.
0 λ 0
InstrikingcontrasttoSection3,theformulationofaFWER-controllingprocedureinthiscaseisfarmorechallenging
duetothefollowing.
1. Thereare(|Λ|+1)tmax possibleconfigurationsforλandthusitisinfeasibletosweepoverthisexponentialnumber
ofhypotheses. Giventhissheervolume,computingap-valueforeachhypothesisexceedsreasonablecomputational
limits.
2. Toachievegoodstatisticalpowerwithfixedsequencetesting,carefulorderingofhypothesesisessential: inadequate
orderingmayleadtoarejectionsetRthatincludeslesseffectivethresholdvectors. AsdiscussedinSection3,there
isanaturalorderingofthehypotheseswhenconsideringthetuningofasinglethreshold;wecansimplyorderthe
hypothesesfromthelargestλtothesmallestone. However,itisunclearhowtoorderthehypotheseswhenworking
withavectorλ.
3. Whenfacedwithasmallsamplesize,thep-valuemaybetoohigheveniftheriskislowerthanα. Thisisattributed
to the fact that the p-value produced by CDF takes into account the number of samples used to calculate the
bin
empiricalrisk. Importantly,thisisnotanabstractconcern; inpractice,aswestriveforconditionalriskcontrol,
situationswithasmallsamplesizebecomeprevalent,particularlyfortheveryearlytimesteps.
Inwhatfollows,wepresentamethodthatalleviatestheseissues,takinginspirationfromtheprincipleofsplitfixed
sequencetestingproposedinLTT.Inthisapproach,wefirstsplitthecalibrationsetD intotwodisjointsets: D
cal cal-1
andD . Then,weproceedwithatwo-stagealgorithm,describedbelowatahighlevel.
cal-2
Stage1:CandidateScreening: UseD toheuristicallyfindadata-adaptivethresholdvectorηˆ,withaneyetowards
cal-1
earlystoppingwithconditionalriskcontrol.
Stage2: Testing: Applyfixedsequencetestingtoconfigurationsderivedfrom ηˆ. Here, weusetheindependent
holdoutsetD toensurethevalidityofthetest.
cal-2
4.1 Stage1: CandidateScreening
WepresentagreedyalgorithmthattakesasinputapredictivemodelandcalibrationdataD andreturnsacandidate
cal-1
threshold vector ηˆ. This procedure, summarized in Algorithm 1, sequentially updates the elements in the vector
ηˆ as follows. It starts by updating the first element ηˆ that corresponds to the timestep t = 1, then proceeds to
1
ηˆ for t = 2, and continues until reaching ηˆ at t = t . Specifically, at timestep t, we are handed the vector
2 max max
ηˆ = (ηˆ ,...,ηˆ ,∞,...,∞), and set its t-th element ηˆ to be the smallest η such that Rˆ≤t(τ ) ≤ α, or keep
1 t−1 t t gap ηˆ
ηˆ =∞ifthereisnoη thatsatisfiesthisconstraint. Above,Rˆ≤t(τ )istheempiricalaccuracygapofthesampleswith
t t gap ηˆ
halttimethatislessthanorequaltot(seeline14).
Beforemovingtothenextstage,wepausetodiscussthepropertiesofthisgreedymethod. First,thecomputational
complexity of the proposed algorithm is O(t ·|Λ|·|D |), which is attributed to the fact that we choose to
max cal-1
sequentiallyupdatethevectorηˆ. Second,bydesign,thechoiceofηˆ fort′ >tdoesnotaffectRˆ≤t′ fort′ ≤t. Third,
t′ gap
thisgreedymethodseeksavectorηˆthatyieldsastoppingrulewhoseempiricalconditionalriskistightlyregulated
aroundα,butnotexceeded. Thispropertyiscrucialtoattaininganeffectiveearlystoppingrule. Inprinciple,instead
ofdeterminingηˆ solelybasedonempiricalrisk, wecouldchoosethesmallestη whosep-valuefallsbelowδ, an
t t
approachthatisakintothesplitfixedsequencetestingideaofAngelopoulosetal.[5]. However,wedecidedtowork
directlywiththeempiricalrisk,asitisarguablystraightforwardtoimplement,andwefoundthesetwoapproachesto
6Algorithm1CandidateScreening(Stage1)
1: Input: CalibrationsetD cal-1 ={(X i,Y i)}n i=ca 1l-1,tolerableaccuracygapα,gridresolution∆.
2: ηˆ←{∞,...,∞}
3: //Findηˆ greedilyduringthet-thiteration.
t
4: fort=1,...,t maxdo
5: η ←ηˆ
6: //Findthelowestη ∈Λs.t. Rˆ≤t ≤α.
t gap
7: forξ =0,∆,2∆,...,1do
8: η ←ξ
t
9: I ←{i:τ η(X i)≤t} //Findsampleswithahalttime≤t.
10: ifI =∅then
11: //Cannotcalculatetheempiricalrisk.
12: Breakinnerloopandsetηˆ =∞
t
13: endif
14: Rˆ g≤ at
p
← |I1 |(cid:80) i∈IL gap(Y i,Yˆ ifull ,Yˆ iearly (τ η)) //Calculatetheempiricalrisk.
15: ifRˆ≤t ≤αthen
gap
16: //Foundthelowestη s.t. Rˆ≤t ≤α.
t gap
17: Breakinnerloopandsetηˆ ←ξ
t
18: endif
19: endfor
20: endfor
21: Output: ηˆ
havesimilarhalttimes. Inanycase,whilesensible,theprocessoffindingthevectorηˆisheuristicinthesensethatitis
notguaranteedtocontroltheconditionalriskforfuturetestpoints. Thisissuenaturallyleadsustothenextstage.
4.2 Stage2: Testing
Inthistestingstage,webuildonthecandidatevectorηˆtoformastatisticallyvalidstoppingrulethatattains(5). A
naiveandoptimisticapproachwouldbetotestforasinglenullH definedin(9)forthechoiceλ=ηˆ. Rejectionof
0,λ
thisnullhypothesiswithasignificancelevelofδimpliesthatηˆattains(5),achievingapowerfulstoppingruledueto
thedesignofηˆ. However,ifwefailtorejectthisnull,ourfallbackisthetrivialconfigurationλˆ = (∞,...,∞)that
resultsinaconditionalaccuracygapofzero. However,inthiscase,thestoppingruleweformisthemostconservative
one,asthemodelwillneverstopearly.
Toalleviatethis,weemployfixedsequencetesting,designedtoyieldaneffectivestoppingrulewithFWER-control,
evenincaseswherethenullhypothesisH withthe“optimistic”configurationλ=ηˆwouldnotberejected.Recallthe
0,λ
underlyingprincipleoffixedsequencetesting:orderthehypothesesfromthemostplausibletotheleast,withoutlooking
attheholdoutdataD . BuildingonthestructureoftheETSCproblem,wedefinethesequenceofconfigurations
cal-2
λtmax = (∞,...,∞,ηˆ ), λtmax−1 = (∞,...,∞,ηˆ ,ηˆ ), allthewaytoλ1 = (ηˆ ,ηˆ ,...ηˆ ). Thatis, the
tmax tmax−1 tmax 1 2 tmax
t′-thelementinthevectorλt isλt t′ = ηˆ t′ ift′ ≥ t,andλt t′ = ∞otherwise. Importantly,thestoppingruleτ λt does
notallowstoppingtheclassificationprocessattimestepssmallerthant. Withthisconstructioninplace,wesuggest
applyingthefixedsequencetestingproceduretothehypothesesorderedfromtheoneinducedbyλtmax,i.e.,H
0,λtmax
downtotheonecorrespondingtoλ1,i.e.,H . Notethatthisorderingisparticularlypowerfulwhentheaccuracygap
0,λ1
ofthemodeltendstodecreasewiththenumberoftimestepsobserved—asensiblecharacteristicinETSC.Additionally,
thesuggestedorderingenablesustopostponethetestingofhypothesesinvolvinglimitedsamplesizestolaterstagesof
theprocedure,whichisattractiveasitismorelikelythatwewillfailtorejectthosenulls.
Havingdefinedtheorderingofthehypotheses,weturntodescribehowtocomputeavalidp-valueforeachofthe
individualhypotheses,usingtheholdoutdataD . ConsiderthehypothesisH in(9)forthechoiceλ=λt,and
cal-2 0,λ
defineitsfinernullhypothesesasfollows:
Ht′ :R≤t′ (τ )>α for t′ =t,...,t . (10)
0,λt gap λt max
7Algorithm2Testing(Stage2)
1: Input: CalibrationsetD cal-2 ={(X i,Y i)}n i=ca 1l-2,candidatethresholdsηˆ,tolerableaccuracygapα,significancelevel
δ,gridresolution∆.
2: //Startwiththemostconservativestoppingrule.
3: λˆ ←{∞,...,∞}
4: //Graduallyrevealanotherηˆ fromtheendandtestit.
t
5: fort=t max,...,1do
6:
λ←λˆ
7: λ ←ηˆ //Setλtoλt.
t t
8: //TestHt′ forallt′ ≥t.
0,λt
9: fort′ =t,...,t maxdo
10: I ←{i:τ λ(X i)≤t′} //Findsampleswithahalttime≤t′.
11: ifI =∅then
12: //Noevidencetorejectthenull,stoptesting.
13: Breakbothloops
14: endif
15: Rˆ g≤ at p′ ← |I1 |(cid:80) i∈IL gap(Y i,Yˆ ifull ,Yˆ iearly (τ λ)) //Calculatetheempiricalrisk.
(cid:16) (cid:17)
16: pˆt λ′ t ←CDF bin Rˆ g≤ at p′ ·|I|;|I|,α //Computeap-value.
17: ifpˆt′ >δthen
λt
18: //Failedtorejectthenull,stoptesting.
19: Breakbothloops
20: endif
21: endfor
22: λˆ ←λ //H 0,λt wasrejected,updatethechosenλˆ.
23: endfor
24: Output: λˆ
Observe that H in (9) is true if and only if there exists t′ ≥ t such that Ht′ is true. Observe also that, by
0,λt 0,λt
construction,τ cannotstopattimestepssmallerthant,andthust in(9)satisfiest ≥t. Importantly,theformulation
λt 0 0
ofthefinernullsin(10)pavesthewaytotesttheindividualhypothesisH . Specifically,itimpliesthatwecanreject
0,λt
theindividualhypothesisH ifallthefinerhypothesesHt′ , t′ ≥ tarerejected. Thisamountstocomputinga
0,λt 0,λt
p-valuepˆt′ foreachfinerhypothesisHt′ andrejectingH ifpˆt′ ≤δforallt′ ≥t. Putsimply,werejectH if
λt 0,λt 0,λt λt 0,λt
pˆ =max{pˆt′ :t′ ≥t}≤δ.
λt λt
Algorithm2summarizestheproposedtestingprocedure. Theouterloopinthisalgorithmsequentiallyiteratesover
thehypothesesH
0,λt
fromλtmax toλ1. TheinnerloopteststhenullH
0,λt
understudybybreakingitintothefiner
hypothesesHt′ , t′ ≥t. Thisalgorithmreturnstheconfigurationλˆ =λt correspondingtothesmallesttinwhich
0,λt
H wasrejected. ThecomplexityofAlgorithm(2)isO(t2 ·|D |),andthevalidityoftheresultingstoppingrule
0,λt max cal-2
τ isasfollows.
λˆ
Proposition2. Assumingthecalibrationandtestsamplesarei.i.d.,withλˆ selectedasoutlinedinAlgorithm2,the
stoppingruleτ (X)satisfies(5).
λˆ
SimilarlytoProposition1,theaboveresultstatesthatAlgorithm(2)achievesafinite-sample,distribution-freerisk
control. But,incontrastwithProposition1,herewecontrolastrongernotionoferror—theconditionalaccuracygap.
5 Experiments
Inthissection,weevaluatetheproposedmethodsbothonstructuredtimeseriesdatasetsthatarewidelyusedintheETSC
literatureandonthemultiple-choiceansweringtask,whichwasintroducedinSection1.1. Theperformancemetrics
includetheconditionalR g≤ at p(τˆ)andmarginalR gm aa prginal(τˆ)=R g≤ at pmax(τˆ)accuracygap,evaluatedonunseentestdataD test.
8Wealsoreportthegaininearlystopping,definedastheaveragenormalizedhalttime: T = 1 (cid:80) τˆ(Xi).In
avg |Dtest| Xi∈Dtest tmax
allexperiments,wesetthetargetaccuracygapleveltoα=10%,withδ =1%and∆=0.01. Throughoutthissection,
themarginalmethodcanbethoughtofasabaseline,asitcloselyresemblesthecalibrationproceduresuggestedby
Schusteretal.[32]tocontroltheaccuracygapforearlyexitintransformers.
5.1 ApplicationtoStructuredData
Table 1: Summary of performance metrics for the proposed marginal and conditional methods across all
structureddatasets. Resultsarepresentedforanominalaccuracygapofα=10%andδ =1%. Thetableprovides
theaccumulatedaccuracygapoverthe20%and50%earlieststoppingtimesdeterminedbyτˆforeachmethod,along
withthemarginalaccuracygap. Therightmostcolumnpresentstheaveragenormalizedstoppingtime. Allperformance
metricsareaveragedover100randomcalibration/testsplits. Allstandarderrorsarelessthan0.008andthusomitted.
Acc.GapforEarliestτˆ(X)
Dataset LateAcc. Method EarlyAcc. T
20%earliest 50%earliest Marginal avg
Marginal 0.757 0.081 0.084 0.093 0.209
Tiselac 0.816
Conditional 0.771 0.064 0.065 0.085 0.215
Marginal 0.809 0.117 0.108 0.079 0.471
ElectricDevices 0.873
Conditional 0.825 0.030 0.031 0.075 0.552
Marginal 0.912 0.086 0.090 0.080 0.446
PenDigits 0.989
Conditional 0.940 0.049 0.050 0.051 0.567
Marginal 0.608 0.171 0.135 0.086 0.580
Crop 0.673
Conditional 0.642 0.057 0.063 0.079 0.450
Marginal 0.884 0.004 0.054 0.079 0.125
WalkingSittingStanding 0.962
Conditional 0.901 0.033 0.039 0.067 0.061
Inthissubsection,wetesttheapplicabilityofourmethodsonfivedatasets:Tiselac[36],ElectricDevices[37],
PenDigits[38],Crop[39],andWalkingSittingStanding[40]. Thesedatasetsarepubliclyavailablevia
theaeontoolkit. WerefertotheseasstructureddatasetsasX ∈Rtmax×dandXt ∈Rd. SeeTableC.2inAppendixC.1
formoredetails.
To implement and evaluate our methods, we partition each dataset into four distinct sets: 80% of the samples
areallocatedformodelfitting,whiletheremainingsamplesareequallydividedtoformD ,D ,andD . For
cal-1 cal-2 test
themarginalmethod,wesetD =D ∪D . Inallexperiments,weemployanLSTMmodel[41]asthebase
cal cal-1 cal-2
sequentialclassifier. AdetaileddescriptionofthemodelarchitectureandtrainingstrategyisprovidedinAppendixC.2.
TheresultsobtainedbythemarginalandconditionalmethodsaresummarizedinTable1;seeAppendixC.3formore
detailedresultsforeachdataset. Followingthistable,thetwomethodscontrolthemarginalaccuracygap,supporting
ourtheory. However,themarginalmethodfailstocontroltheconditionalriskforsequenceswithearlyhalttimes,in
contrastwiththeconditionalapproachthatattainsvalidriskcontrolovertheaccumulatedhalttimes—asguaranteedby
ourtheory. Thestatisticalefficiencyofbothmethodsiscomparable,asevidencedbytheaveragenormalizedhalttime
T performancemetric. Infact,althoughtheconditionalmethodcontrolsastrongernotionoferror,itresultedina
avg
smalleraveragenormalizedstoppingtimeT in2outof5datasets. Weattributethisgaintoourdecisiontoemploy
avg
avectorofthresholdstoformtheconditionalstoppingrule,asopposedtothesinglethresholdusedinthebaseline
marginalapproach. Lastly,FigureC.4inAppendixC.3illustratesthetrade-offbetweenthetolerableaccuracygapα
andtheaveragestoppingtimefortheTiselacdataset. There,onecanseethattheconditionalmethodallowsfor
earlierstoppingtimeswhenahigheraccuracygapispermitted.
5.2 AnNLPApplication
We now revisit the reading comprehension task introduced in Section 1.1, where the goal is to select the correct
answerfromasetoffouroptionsbasedonagivencontext. Toallowthesequentialprocessingofthedata,wefirst
divide the context of each question into sentences. These sentences are then grouped into t = 10 sets. When
max
thetotalnumberofsentencescannotbegroupedinto10equallysizedsets,weincludetheremainingsentencesin
thelastset. ToformulatetheinputsequenceX≤t,weconstructapromptthatincludesthecontextsentencesupto
9timestept,alongwiththequestionanditsfouroptions,labeled‘A’,‘B’,‘C’,and‘D’.Thepromptconcludeswith“The
answer is:\n\n”,whichisthenfedtotheVicuna-13Bmodel[42]tomakeaprediction;themodelisaccessiblevia
HuggingFace. WeemploythevLLMframework[43]tocomputetheprobabilityassignedtoeachofthefouroptions,
resultinginfˆ(X≤t) ∈ [0,1]4. Lastly, wedefinethefunctionπˆ(X≤t) = max{fˆ(X≤t) : k = 1,...,4}, whichis
k
utilizedtoformulatethestoppingruleτˆ.
TheresultsobtainedbythemarginalandconditionalmethodsarepresentedinFigure2. Asportrayedintheleft
panel,theconditionalapproachrigorouslycontrolstheconditionalaccuracygapontheaccumulatedhalttimes,in
contrast with the marginal method that merely controls the marginal risk. The right panel in Figure 2 shows that
themarginalmethodtendstostopearlier. Thisisalsoindicatedbyitsloweraveragenormalizedhalttimeof0.483
comparedto0.831fortheconditionalmethod. However,thisgainisnotnecessarilydesired,asthemarginalapproach
tendstomakeerrorsintheearlyhalttimes.
FigureD.5intheAppendixpresentsanablationstudy,underscoringtheimportanceofthetestingphase(Stage2)
oftheconditionalmethod. Asillustrated,thecandidateconfigurationηˆobtainedbythegreedycandidatescreening
algorithm(Stage1)doesnotproviderigorouscontroloftheconditionalaccuracygapinthesenseof(5). Thisstandsin
contrastwiththeconditionalmethodthatincludesthetestingstage. Nevertheless,thecandidateηˆprovidesareasonable
initialsetofconfigurationsforthehyperparameterstobetested,asityieldsastoppingrulethatroughlycentersaround
thenominalaccuracygaplevelα.
6 Conclusion
Inthispaper,wepresentedanovelstatisticalframeworkthatrigorouslycontrolstheaccuracygapconditionalonthe
accumulatedhalttimes. Additionally,weperformedaseriesofnumericalexperimentsthathighlightthesignificance
oftransitioningfrommarginaltoconditionalguarantees, whichvalidatesourtheoryandunderscoresthepractical
implicationsofourproposal.
Ourworkopensseveralfutureresearchdirections. First,itwouldbeintriguingtodesignmoreeffectivestopping
rulesatthecostofincreasingthecomputationalcomplexityoftheproposedtwo-stagecalibrationprocedure. Another
directionistoaddressthelimitationofourwork—therelianceonthei.i.d. assumption, whichmaybeviolatedin
practice. Itwillbeilluminatingtoextendthetoolswepresentedandrelaxthisassumption,possiblybyrelyingon
thefoundationsofBarberetal.[29]. Lastly,anaturalprogressionistoextendthetoolswedevelopedtoregression
problems[44]. ThechallengehereisthatthelossL mightnotbebinary,andthustheexactp-valueweusedinthis
gap
paperwouldnotbeapplicable.
References
[1] VladimirVovk. Conditionalvalidityofinductiveconformalpredictors. InAsianconferenceonmachinelearning,
pages475–490.PMLR,2012.
[2] JingLeiandLarryWasserman. Distribution-freepredictionbandsfornon-parametricregression. Journalofthe
RoyalStatisticalSocietySeriesB:StatisticalMethodology,76(1):71–96,2014.
[3] RinaFoygelBarber,EmmanuelJCandes,AadityaRamdas,andRyanJTibshirani. Thelimitsofdistribution-free
conditionalpredictiveinference. InformationandInference: AJournaloftheIMA,10(2):455–482,2021.
[4] Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh
Padmakumar,JohnnyMa,JanaThompson,HeHe,andSamuelR.Bowman. QuALITY:Questionanswering
withlonginputtexts,yes! InConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics: HumanLanguageTechnologies,pages5336–5358,2022.
[5] AnastasiosNAngelopoulos,StephenBates,EmmanuelJCande`s,MichaelIJordan,andLihuaLei. Learnthen
test: Calibratingpredictivealgorithmstoachieveriskcontrol. arXivpreprintarXiv:2110.01052,2021.
[6] BrachaLaufer-Goldshtein,AdamFisch,ReginaBarzilay,andTommiSJaakkola. Efficientlycontrollingmultiple
riskswithparetotesting. InInternationalConferenceonLearningRepresentations,2022.
10[7] ThomasHartvigsen,CansuSen,XiangnanKong,andElkeRundensteiner. Adaptive-haltingpolicynetworkfor
earlyclassification. InProceedingsofthe25thACMSIGKDDInternationalConferenceonKnowledgeDiscovery
&DataMining,pages101–110,2019.
[8] AshishGupta,HariPrabhatGupta,BhaskarBiswas,andTanimaDutta. Approachesandapplicationsofearly
classificationoftimeseries: Areview. IEEETransactionsonArtificialIntelligence,1(1):47–61,2020.
[9] AmirGhodrati,BabakEhteshamiBejnordi,andAmirhosseinHabibian. Frameexit: Conditionalearlyexiting
forefficientvideorecognition. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages15608–15618,2021.
[10] AminSabet,JonathonHare,BashirAl-Hashimi,andGeoffVMerrett. Temporalearlyexitsforefficientvideo
objectdetection. arXivpreprintarXiv:2106.11208,2021.
[11] RaphaelTang,KarunKumar,JiXin,PiyushVyas,WenyanLi,GefeiYang,YajieMao,CraigMurray,andJimmy
Lin.Temporalearlyexitingforstreamingspeechcommandsrecognition.InInternationalConferenceonAcoustics,
SpeechandSignalProcessing,pages7567–7571.IEEE,2022.
[12] ThomasHartvigsen,WalterGerych,JidapaThadajarassiri,XiangnanKong,andElkeRundensteiner. Stop&hop:
Early classification of irregular time series. In Proceedings of the 31st ACM International Conference on
Information&KnowledgeManagement,pages696–705,2022.
[13] Huiling Chen, Ye Zhang, Aosheng Tian, Yi Hou, Chao Ma, and Shilin Zhou. Decoupled early time series
classificationusingvaried-lengthfeatureaugmentationandgradientprojectiontechnique. Entropy,24(10):1477,
2022.
[14] ShubhranshuShekhar,DhivyaEswaran,BryanHooi,JonathanElmer,ChristosFaloutsos,andLemanAkoglu.
Benefit-aware early prediction of health outcomes on multivariate EEG time series. Journal of Biomedical
Informatics,139:104296,2023.
[15] VladimirVovk,AlexanderGammerman,andGlennShafer. Algorithmiclearninginarandomworld,volume29.
Springer,2005.
[16] Harris Papadopoulos and Haris Haralambous. Reliable prediction intervals with regression neural networks.
NeuralNetworks,24(8):842–851,2011.
[17] JingLei,MaxG’Sell,AlessandroRinaldo,RyanJTibshirani,andLarryWasserman. Distribution-freepredictive
inferenceforregression. JournaloftheAmericanStatisticalAssociation,113(523):1094–1111,2018.
[18] RyanJTibshirani,RinaFoygelBarber,EmmanuelCandes,andAadityaRamdas. Conformalpredictionunder
covariateshift. Advancesinneuralinformationprocessingsystems,32,2019.
[19] YanivRomano,MatteoSesia,andEmmanuelCandes. Classificationwithvalidandadaptivecoverage. Advances
inNeuralInformationProcessingSystems,33:3581–3591,2020.
[20] Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael Jordan. Distribution-free,
risk-controllingpredictionsets. JournaloftheACM(JACM),68(6):1–34,2021.
[21] AnastasiosN.AngelopoulosandStephenBates. Conformalprediction: Agentleintroduction. Foundationsand
Trends®inMachineLearning,16(4):494–591,2023. ISSN1935-8237.
[22] IsaacGibbsandEmmanuelCandes. Adaptiveconformalinferenceunderdistributionshift. InAdvancesinNeural
InformationProcessingSystems,2021.
[23] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Conformal prediction intervals with temporal dependence.
TransactionsonMachineLearningResearch,2022. ISSN2835-8856.
[24] AnastasiosNAngelopoulos,StephenBates,AdamFisch,LihuaLei,andTalSchuster. Conformalriskcontrol.
arXivpreprintarXiv:2208.02814,2022.
11[25] Adam Fisch, Tommi S. Jaakkola, and Regina Barzilay. Calibrated selective classification. Transactions on
MachineLearningResearch,2022. ISSN2835-8856.
[26] ShaiFeldman,LiranRingel,StephenBates,andYanivRomano. Achievingriskcontrolinonlinelearningsettings.
TransactionsonMachineLearningResearch,2023. ISSN2835-8856.
[27] DonghwanLee,XinmengHuang,HamedHassani,andEdgarDobriban. T-cal: Anoptimaltestforthecalibration
ofpredictivemodels. JournalofMachineLearningResearch,24(335):1–72,2023.
[28] MaximeCauchois,SuyashGupta,AlnurAli,andJohnCDuchi. Robustvalidation: Confidentpredictionseven
whendistributionsshift. JournaloftheAmericanStatisticalAssociation,pages1–22,2023.
[29] RinaFoygelBarber,EmmanuelJCandes,AadityaRamdas,andRyanJTibshirani. Conformalpredictionbeyond
exchangeability. TheAnnalsofStatistics,51(2):816–845,2023.
[30] BrachaLaufer-Goldshtein,AdamFisch,ReginaBarzilay,andTommiJaakkola. Risk-controllingmodelselection
viaguidedbayesianoptimization. arXivpreprintarXiv:2312.01692,2023.
[31] TalSchuster,AdamFisch,TommiJaakkola,andReginaBarzilay. Consistentacceleratedinferenceviaconfident
adaptivetransformers. InConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages4962–4979,
2021.
[32] TalSchuster,AdamFisch,JaiGupta,MostafaDehghani,DaraBahri,VinhTran,YiTay,andDonaldMetzler.
Confidentadaptivelanguagemodeling. AdvancesinNeuralInformationProcessingSystems,35:17456–17472,
2022.
[33] LawrenceDBrown,TTonyCai,andAnirbanDasGupta. Intervalestimationforabinomialproportion. Statistical
science,16(2):101–133,2001.
[34] R.G.J.Miller. SimultaneousStatisticalInference. SpringerSeriesinStatistics.SpringerNewYork,2012.
[35] PeterBauer. Multipletestinginclinicaltrials. Statisticsinmedicine,10(6):871–890,1991.
[36] Dino Ienco. Tiselac: time series land cover classification challenge, 2017. https://www.
timeseriesclassification.com/description.php?Dataset=Tiselac.
[37] Yanping Chen, Eamonn Keogh, Bing Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen, and Gus-
tavoBatista. TheUCRtimeseriesclassificationarchive,July2015. www.cs.ucr.edu/˜eamonn/time_
series_data/.
[38] E.AlpaydinandFevzi.Alimoglu. Pen-basedrecognitionofhandwrittendigits. UCIMachineLearningRepository,
1998.
[39] ChangWeiTan,GeoffreyIWebb,andFranc¸oisPetitjean. Indexingandclassifyinggigabytesoftimeseriesunder
timewarping. InSIAMinternationalconferenceondatamining,pages282–290.SIAM,2017.
[40] JorgeReyes-Ortiz,DavideAnguita,AlessandroGhio,LucaOneto,andXavierParra. Humanactivityrecognition
usingsmartphones. UCIMachineLearningRepository,2012.
[41] SeppHochreiterandJu¨rgenSchmidhuber. Longshort-termmemory. Neuralcomputation,9(8):1735–1780,1997.
[42] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,Zhuohan
Li,DachengLi,EricP.Xing,HaoZhang,JosephE.Gonzalez,andIonStoica. JudgingLLM-as-a-judgewith
MT-BenchandChatbotArena. arXivpreprintarXiv:2306.05685,2023.
[43] WoosukKwon,ZhuohanLi,SiyuanZhuang,YingSheng,LianminZheng,CodyHaoYu,JosephE.Gonzalez,
HaoZhang,andIonStoica. Efficientmemorymanagementforlargelanguagemodelservingwithpagedattention.
InProceedingsoftheACMSIGOPS29thSymposiumonOperatingSystemsPrinciples,2023.
12[44] CassandraTongYe,JiashuHan,KunzanLiu,AnastasiosAngelopoulos,LindaGriffith,KristinaMonakhova,and
SixianYou. Learned,uncertainty-drivenadaptiveacquisitionforphoton-efficientmultiphotonmicroscopy. arXiv
preprintarXiv:2310.16102,2023.
[45] DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,
2014.
13A Marginal Risk Control Algorithm
AlgorithmA.3presentsthemarginalmethoddescribedinSection3.
AlgorithmA.3Fixedsequencetestingformarginalriskcontrol
1: Input: CalibrationsetD cal ={(X i,Y i)}n i=ca 1l,tolerableaccuracygapα,significancelevelδ,gridresolution∆.
2: λˆ ←∞ //Use∞asafallbackifthefirstnullisnotrejected.
3: λ←1 //Starttestingwiththelargestλ∈Λ.
4: whileλ≥0do
5: Rˆ gap ← n1
cal
(cid:80) (cid:16)n i=ca 1l L gap(Y i,Yˆ ifull (cid:17),Yˆ iearly(τ λ)) //Computetheempircalrisk.
6: pˆ←CDF bin Rˆ gap·n cal;n cal,α //Computeap-value.
7: ifpˆ>δthen
8: break //Failedtorejectthenull,stoptesting.
9: endif
10: λˆ ←λ //H 0,λwasrejected,updatethechosenλˆ.
11: λ←λ−∆ //Nexttestwilltestalowerthreshold.
12: endwhile
13: Output: λˆ
B Proofs
ProofofProposition1. Thevalidityofthepropositionisadirectconsequenceofusingfixedsequencetesting. For
completeness, we add a proof that fixed sequence testing controls the FWER at level δ. Denote by H the j-th
0,j
orderedhypothesis. Ifallthehypothesesarefalse, wetriviallygetthatP(V ≥ 1) = 0. Next, denotetheindexof
thefirsttruenullbyj ,i.e.,H istrueandtheprecedingH , j′ <j arefalse. Bytheconstructionofthefixed
0 0,j0 0,j′ 0
sequencetestingprocedure,wemayencounterthisfirsttruenullonlyatstepj oftheprocedure. Now,observethat
0
P(V ≥ 1) = 1−P(V = 0) = 1−P(pˆ > δ) = P(pˆ ≤ δ) ≤ δ. Above, thesecondequalityholdssincethe
λj0 λj0
testingprocedurestopsthefirsttimethatanyp-valueexceedsδ,andthuswegetV =0ifandonlyifpˆ >δ;under
thisevent,theprocedurewouldterminatewithoutrejectingH andH , j′ > j . Thelastineqλ uj a0 lityfollows
0,λj0 0,λ j′ 0
fromthevalidityofthep-valueunderthenull(8).
ProofofProposition2. Toprovetheresult,itsufficestoshowthatAlgorithm2controlstheFWERatlevelδ. First,
observe that the outer loop in Algorithm 2 tests the hypotheses H
0,λt
sequentially, starting from λtmax down to λ1.
As such, it follows the protocol of fixed sequence testing for FWER control. Second, each of the p-values pˆt′ ,
λt
corresponding to the finer null hypotheses (10), are valid since they are calculated using i.i.d. samples from the
distribution P . Third, the max p-value pˆ = max{pˆt′ : t′ ≥ t} used to test each of H satisfies
XY|τˆ(X)≤t′ λt λt 0,λt
P(cid:0)
pˆ ≤δ |H
(cid:1)
≤δ[5,Proposition6]. Combiningthesethreeargumentscompletestheproof.
λt 0,λt
C Further Details on Experiments with Structured Datasets
InSection5.1ofthemainmanuscript,weintroducethestructureddatasetsonwhichweappliedourmethods. Here,we
providemoredetailsoneachdataset,elaborateonthemodelarchitectureandtrainingstrategy,andpresentadditional
results.
C.1 Datasets
TableC.2providesmoredetailsoneachdataset.
14TableC.2: Summaryofstructureddatasets.
Dataset #Samples #Timesteps #Features #Classes Type
Tiselac 99687 23 10 9 Image
ElectricDevices 16637 96 1 7 Device
PenDigits 10992 8 2 10 Motion
Crop 24000 46 1 24 Image
WalkingSittingStanding 10299 206 3 6 Motion
C.2 ModelArchitectureandTrainingStrategy
We used a standard LSTM for feature extraction with one recurrent layer with a hidden size of 32, except for
WalkingSittingStandingwhereweused2recurrentlayers,eachwithahiddensizeof256. Theoutputofthe
lastrecurrentlayerispluggedtotwofullyconnectedclassificationheads,oneforclassifyingthelabelfˆ(X≤t)∈[0,1]K
andtheotherforestimatingtheconfidenceintheclassificationπˆ(X≤t) ∈ [0,1]. ThelossLfˆ forupdatingfˆisthe
CE
cross-entropy,andthelossLπˆ forupdatingπˆ isthebinarycross-entropy. Thewholenetworkistrainedtominimize
BCE
Lfˆ (fˆ(X≤t),Y)+γ·Lπˆ (πˆ(X≤t),B(X≤t)),wherethefunctionB ∈{0,1}returnsthevalue1iffˆ(X≤t)correctly
CE BCE
predictsthelabelY andzerootherwise. Wesetthehyperparameterγ to0.2inallexperiments. Weaugmentthedataby
fittingthemodelonallpossibleprefixesX≤t,t=1,...,t . Theoptimizerusedtominimizetheobjectivefunctionis
max
Adam[45],withalearningrateof0.001,andabatchsizeof64. Weallocate1/8ofthetrainingsamplestoavalidation
setandoptimizethemodelontheremaining7/8ofthesamples. Trainingcontinuesuntilthereisnoimprovementin
thelossonthevalidationsetfor30epochs. Themodelwiththebestvalidationsetlossisthensaved.
C.3 AdditionalResults
Table 1 from the main manuscript summarizes the performance of the marginal and conditional methods on the
structureddatasets. Inaddition,FigureC.3presentsmoredetailedresults,illustratingtheaccumulatedaccuracygap
andaccumulatedstoppingtimesasafunctionoftobtainedbythemarginalandconditionalmethods.
FigureC.4showshowdifferenterrorlevelsαaffecttheaveragehalttimeswiththeconditionalmethod.Asexpected,
whenallowingforahigherlevelofrisk,thecalibrationmanagestoidentifythresholdsthatresultinshorterhalttimes.
D Ablation Study on the NLP Application
Inthissection,wepresentanablationstudytoassessthesignificanceofthesecondstageintheconditionalmethod: the
testingphase. FigureD.5summarizestheresultsdiscussedinSection5.2ofthemainmanuscript.
15(a)Tiselac (b)ElectricDevices
(c)PenDigits (d)Crop
(e)WalkingSittingStanding
FigureC.3: Comparisonbetweenthemarginalandconditionalmethodsforthestructureddatasets. Theother
detailsareasinFigure2.
FigureC.4: NormalizedhalttimeT vs. tolerableaccuracygapα. Theresultsareaveragedover100randomsplits
avg
oftheTiselacdataset,with(tiny)standarderrorbars.
16FigureD.5: Theimportanceofthetestingprocedure—Stage2. Comparisonofconditionalaccuracygapobtained
bycandidatescreening(Stage1,blackcurves)andbythefullconditionalmethod(Stage1+2,orangecurves). The
resultsarepresentedfor100randomcalibration/testsplitsoftheQuALITYdataset,witheachcurvecorrespondingtoa
differentsplit.
17