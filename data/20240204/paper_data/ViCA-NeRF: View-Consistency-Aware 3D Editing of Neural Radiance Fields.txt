ViCA-NeRF: View-Consistency-Aware 3D Editing of
Neural Radiance Fields
JiahuaDong Yu-XiongWang
UniversityofIllinoisUrbana-Champaign
{jiahuad2, yxw}@illinois.edu
“Turn him into iron man” Instruct-NeRF2NeRF ViCA-NeRF (Ours)
“Add some flowers” Instruct-NeRF2NeRF ViCA-NeRF (Ours)
Figure1: OurViCA-NeRFisthefirstworkthatachievesmulti-viewconsistent3Deditingwith
textinstructions,applicableacrossabroadrangeofscenesandinstructions. Moreover,ViCA-NeRF
exhibits controllability, allowing for early control of final results by editing key views. Notably,
ViCA-NeRFisalsoefficient,surpassingstate-of-the-artInstruct-NeRF2NeRFbybeing3timesfaster.
Abstract
WeintroduceViCA-NeRF,thefirstview-consistency-awaremethodfor3Dediting
with text instructions. In addition to the implicit neural radiance field (NeRF)
modeling,ourkeyinsightistoexploittwosourcesofregularizationthatexplicitly
propagatetheeditinginformationacrossdifferentviews,thusensuringmulti-view
consistency. For geometric regularization, we leverage the depth information
derivedfromNeRFtoestablishimagecorrespondencesbetweendifferentviews.
Forlearnedregularization,wealignthelatentcodesinthe2Ddiffusionmodel
betweeneditedanduneditedimages,enablingustoeditkeyviewsandpropagate
the update throughout the entire scene. Incorporating these two strategies, our
ViCA-NeRF operates in two stages. In the initial stage, we blend edits from
differentviewstocreateapreliminary3Dedit.Thisisfollowedbyasecondstageof
NeRFtraining,dedicatedtofurtherrefiningthescene’sappearance. Experimental
results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times
faster)editingwithhigherlevelsofconsistencyanddetails, comparedwiththe
stateoftheart. Ourcodeisavailableat: https://dongjiahua.github.io/VICA-NeRF.
1 Introduction
Therecentadvancementsin3Dreconstructiontechnology,exemplifiedbytheneuralradiancefield
(NeRF)[1]anditsvariants,havesignificantlyimprovedtheconvenienceofcollectingreal-world
37thConferenceonNeuralInformationProcessingSystems(NeurIPS2023).
4202
beF
1
]VC.sc[
1v46800.2042:viXra3Ddata. Suchprogresshasopenedupnewopportunitiesforthedevelopmentofvarious3D-based
applications. By capturing RGB data from multiple views and obtaining corresponding camera
parameters, NeRFenablesefficient3Drepresentationandrenderingfromanyviewpoint. Asthe
availabilityofdiverse3Ddataincreases,sodoesthedemandfor3Deditingcapabilities.
However, performing 3D editing on NeRF is not straightforward. The implicit representation of
NeRF makes it challenging to directly modify the 3D scene. On the other hand, since NeRF is
trainedonRGBimages,itcaneffectivelyleverageawealthofexisting2Dmodels. Motivatedby
this, state-of-the-art methods, such as Instruct-NeRF2NeRF [2], apply 2D editing on individual
imagesviaatext-instructeddiffusionmodel(e.g.,Instruct-Pix2Pix[3]),andthenrelyexclusivelyon
NeRFtolearnfromtheupdateddatasetwitheditedimagesandpropagatetheeditinginformation
across different views. Despite yielding encouraging results, this indirect 3D editing strategy is
time-consuming,becauseofitsiterativeprocessthatrequirescyclicallyupdatingtheimagedataset
whileprogressivelyfine-tuningNeRFparameters. Andmoreseriously,duetothelackofinherent3D
structure,theeditingresultsfromthe2Ddiffusionmodeloftenexhibitsignificantvariationsacross
differentviews. Thisraisesacriticalconcernregarding3Dinconsistencywhenemployingsucha
strategy,particularlyincaseswheretheeditingoperationisaggressive,e.g.,“Addsomeflowers”and
“Turnhimintoironman”asillustratedinFigure1.
Toovercometheselimitations,inthisworkweintroduceViCA-NeRF,aView-Consistency-Aware
NeRFeditingmethod. Ourkeyinsightistoexploittwoadditionalsourcesofregularizationthat
explicitlyconnecttheeditingstatusacrossdifferentviewsandcorrespondinglypropagatetheediting
information from edited to unedited views, thus ensuring their multi-view consistency. The first
oneisgeometricregularization. GiventhatdepthinformationcanbeobtainedfromNeRFforfree,
weleverageitasaguidancetoestablishimagecorrespondencesbetweendifferentviewsandthen
projecteditedpixelsintheeditedimagetothecorrespondingpixelsinotherviews,therebyenhancing
consistency. Thisapproachallowsustocompletepreliminarydatamodificationsbeforefine-tuning
NeRF,aswellasenablinguserstodetermine3Dcontentbyselectingtheeditingresultsfromkey2D
views. Moreover,asourmethodallowsfordirectandconsistentedits,itismoreefficientbyavoiding
theneedforiterativeuseofthediffusionmodel.
Thesecondoneislearnedregularization. ThedepthinformationderivedfromNeRFcanoftenbe
noisy,yieldingcertainincorrectcorrespondences. Tothisend,wefurtheralignthelatentcodesin
the2Ddiffusionmodel(e.g.,Instruct-Pix2Pix)betweeneditedanduneditedimagesviaablending
refinementmodel. Doingsoupdatesthedatasetwithmoreview-consistentandhomogeneousimages,
therebyfacilitatingNeRFoptimization.
Our ViCA-NeRF framework then operates in two stages, with these two crucial regularization
strategiesintegratedinthefirststage. Here,weeditkeyviewsandblendsuchinformationintoevery
viewpoint. Toachievethis,weemploythediffusionmodelforblendingafterasimpleprojection
mixup. Afterward,theeditedimagescanbeuseddirectlyfortraining. Anefficientwarm-upstrategy
isalsoproposedtoadjusttheeditingscale. Inthesecondstage,whichistheNeRFtrainingphase,we
furtheradjustthedataset. Ourmethodisevaluatedonvariousscenesandtextprompts,rangingfrom
facestooutdoorscenes. Wealsoablatedifferentcomponentsinourmethodandcomparethemwith
previouswork[2]. ExperimentalresultsshowthatViCA-NeRFachieveshigherlevelsofconsistency
anddetail,comparedwiththestateoftheart.
Ourcontributionsarethree-fold. (1)WeproposeViCA-NeRF,thefirstworkthatexplicitlyenforces
multi-viewconsistencyin3Deditingtasks. (2)Weintroducegeometricandlearnedregularization,
making3Deditingmoreflexibleandcontrollablebyeditingkeyviews. (3)Ourmethodsignificantly
improvestheefficiencyof3Dediting,achievingaspeed3timesfasterthanthestateoftheart.
2 RelatedWork
Text-to-ImageDiffusionModelsfor2DEditing. Diffusionmodelshaverecentlydemonstrated
significantcapabilitiesintext-to-imageeditingtasks[3–8]. Earlierstudieshaveleveragedpre-trained
text-to-imagediffusionmodelsforimageediting. Forinstance,SDEdit[8]introducesnoiseintoan
inputimageandsubsequentlydenoisesitthroughadiffusionmodel. Despiteachievingreasonable
results, this approach tends to lose some of the original image information. Other studies have
concentratedonlocalinpaintingusingprovidedmasks[4,5],enablingthegenerationofcorresponding
contentwithinthemaskedareawithtextguidance. Morerecently,Instruct-Pix2Pix[3]proposesan
2Stage1: Dataset Editing
Text: Turn him into Iron Man Stage2: NeRF Training
Instruct-Pix2Pix
Key Views Mixup Blending
Depth
Refine
Source NeRF Original Images
Figure2: OverviewofourViCA-NeRF.OurproposedmethoddecouplesNeRFeditingintotwo
stages. Inthefirststage,wesampleseveralkeyviewsandeditthemthroughInstruct-Pix2Pix. Then,
weusethedepthmapandcameraposestoprojecteditedkeyframestootherviewsandobtainamixup
dataset. Theseimagesarefurtherrefinedthroughourblendingmodel. Inthesecondstage,theedited
datasetisdirectlyusedtotraintheNeRFmodel. Optionally,wecanconductrefinementtothedataset
accordingtotheupdatedNeRF.
approachtoeditingimagesbasedontextinstructions. Instruct-Pix2Pixacceptsoriginalimagesand
textpromptsasconditions. Trainedondatasetsofeditinginstructionsandimagedescriptions,this
approachcaneffectivelycaptureconsistentinformationfromtheoriginalimagewhileadheringto
detailedinstructions.
Inthispaper,weadoptInstruct-Pix2Pixasoureditingmodel. Importantly,wefurtherexplorethe
potentialofInstruct-Pix2Pixbyextendingitsapplicationtoediting,blending,andrefinementtasksin
morechallenging3Deditingscenarios.
Implicit3DRepresentation. Recently,neuralradiancefield(NeRF)anditsvariantshavedemon-
stratedsignificantpotentialinthefieldof3Dmodeling[1,9–14]. Unlikeexplicit3Drepresentations
suchaspointcloudsorvoxels,NeRFusesanimplicitrepresentationtocapturehighlydetailedscene
geometryandappearance.WhiletrainingaconventionalNeRFmodelcanbetime-consuming,Instant-
NGP[10]acceleratesthisprocessbytrainingamulti-layerperceptron(MLP)withmulti-resolution
hashinputencoding. Notably,NeRFStudio[9]offersacomprehensiveframeworkthatcanhandle
varioustypesofdata. ItmodularizeseachcomponentofNeRF,therebysupportingabroadrangeof
data. Theseeffortsbridgethegapbetweenimagedataand3Drepresentation,makingitpossibleto
guide3Dgenerationfrom2Dmodels. Ourworkusesthe‘nerfacto’modelfromNeRFStudiotofit
realscenes. Wealsoleveragethedepthestimatedfromthemodeltomaintainconsistency.
3DGeneration. Inspiredbythesuccessof2Ddiffusionmodels[15,16],3Dgenerationhasdrawn
great attention in recent years [17–21]. Dream Fusion first proposes to use a score distillation
sampling(SDS)lossasthetrainingtarget. Later,Magic3D[20]generateshigherresolutionresults
byacoarse-to-fineprocedure. Variousscenarioshavebeenexplored,includinggeneratingadetailed
meshfromtextprompts[21]andgeneratingnovelobjectsbasedonimagesfromafewviews[18].
Ratherthangeneratingnewcontent,ourworkfocusesoneditingthe3Drepresentationwithtext
prompts. Byexplicitlyleveraging2Dediting,weprovideamoredirectapproachtocontrolthefinal
3Dresult.
NeRFEditing.BecauseoftheimplicitrepresentationofNeRF[1],editingNeRFremainsachallenge.
Early attempts [22–32] perform simple editing on shape and color, by using the guidance from
segmentation masks, text prompts, etc. NeuralEditor [33] proposes to exploit a point cloud for
editingandobtainthecorrespondingNeRFrepresentation. Anotherlineofworktriestomodifythe
contentthroughtextpromptsdirectly. NeRF-Art[34]usesapre-trainedCLIP[35]modeltoserve
asanadditionalstylizedtargetfortheNeRFmodel. Morerecently,Instruct-NeRF2NeRF[2]and
Instrcut-3D-to-3D[36]leverageInstruct-Pix2Pix[3]forediting. TheycaneditthecontentofNeRF
withlanguageinstructions. However,bothrequireusingthediffusionmodeliterativelyintheNeRF
trainingprocess,makingthemlessefficient. Moreimportantly,thoughitshowspromisingresults
on real-world data, Instruct-NeRF2NeRF can only work when the 2D diffusion model produces
consistentresultsin3D.Thus,itisunabletogenerateadetailedresultwhere2Deditstendtobe
different,anditalsosuffersfrommakingdiverseresultsfromthesametextprompt.
3Image Mixup Blending Model
Edited Text: Turn him into Iron Man
Text Cond Text Cond
Diffusion Mean Diffusion Mean
E PD ProDi rf ocf iu fe cfs eusis sso sin o n D E PD ProDi rf ocf iu fe cfs eusis sso sin o n D
Projection Process Process
Mixup Blended Image
Image Cond Image Cond
Depth
Origin Modified Inp2p
Original Images Mixup Image with average
Figure3: Illustrationofmixupprocedureandblendingmodel. Wefirstmixuptheimagewith
theeditedkeyviews. Then,weintroduceablendingmodeltofurtherrefineit. Theblendingmodel
utilizestwomodifiedInstruct-Pix2Pix(‘Inp2p’)processes. Ineachprocess,wegeneratemultiple
resultsandtaketheiraverageonthelatentcodetodecodeasinglefinalresult.
Inourwork,weproposetousedepthguidancefordetailed3Dediting. OurViCA-NeRFonlyneeds
toupdatethedatasetatthestartoftrainingandfurtherrefineitonce,makingitsignificantlymore
efficient. Inaddition,ourmethodresultsinmoredetailedanddiverseeditresultscomparedwith
Instruct-NeRF2NeRF.Asseveralkeyviewsguidethewhole3Dscene,weallowuserstopersonalize
thefinaleffectonNeRFbychoosing2Deditresults.
3 Method
Our ViCA-NeRF framework is shown in Figure 2. The main idea is to use key views to guide
theeditingofotherviews,ensuringbetterconsistencyanddetail. GiventhesourceNeRF,wefirst
extract each view’s depth from it and sample a set of key views. Then, we edit these key views
usingInstruct-Pix2Pix[3]andpropagatetheeditresulttootherviews. Toaddresstheproblemof
incorrectdepthestimationfromNeRF,weperformmixuponeachimageandthenrefineitthrough
ourblendingmodel. TheobtaineddatacanbeuseddirectlyforNeRFtraining. Optionally,wecan
performpost-refinementtothedatasetafterobtainingmoreconsistentrenderingresultsfromNeRF.
3.1 Preliminary
WefirstelaborateonNeRF,includinghowweobtainrenderedRGBimagesandcoarsedepthfrom
theNeRFmodel. Subsequently,wepresentInstruct-NeRF2NeRF,adiffusionmodelfor2Dediting.
Withthismodel,wecanperform2Deditingusingtextandthenfurtherenable3Dediting.
NeuralRadianceField. NeRF[1]anditsvariants[9,10]areproposedfor3Dscenerepresentation.
Theapproachmodelsa3Dsceneasa5Dcontinuousfunction,mappingspatialcoordinate(x,y,z)
and viewing direction (θ,ϕ) to volume density σ and emitted radiance c. For a given set of 2D
images,NeRFistrainedtoreproducetheobservedimages. Thisisachievedbyvolumerendering,
whereraysaretracedthroughthesceneandcolorsareaccumulatedalongeachraytoformthefinal
renderedimage. WhileNeRFdoesnotexplicitlypredictdepth,onecommonapproachistocompute
aweightedaverageofthedistancevaluesalongeachray,wheretheweightsarethepredictedvolume
densities. Thisprocesscanbeconceptualizedasapproximatingthedepthalongtheray.
Instruct-Pix2Pix. Instruct-Pix2Pix[3]isa2Ddiffusionmodelforimageediting. Ittakesatext
promptc andaguidanceimagec asconditions. Givenanimagez ,itfirstaddsnoisetotheimage
T I 0
tocreatez ,wheretisthetimesteptostartdenoising. WithdenoisingU-Netϵ ,thepredictednoise
t θ
canbecalculatedas
ϵ=ϵ (z ,t,c ,c ). (1)
(cid:101) θ t I T
When a predicted noise is calculated, Instruct-Pix2Pix follows the standard denoising process to
updatetheimagez stepbystepandobtainthefinalresultz ,whichisaneditedimageconsistent
t 0
withtheinitialimageandtextinstruction.
4
egamI
puxiM“Add some flowers” “Make it looks like snowed”
“Turn him into a robot” “Turn him into Spider man with mask”
“Turn the bear into a robot bear” “Make it Cyberpunk Style”
“Turn the bear into a grizzly bear” “Make it surrounded by flowers”
Original NeRF Instruct-NeRF2NERF Ours Instruct-NeRF2NERF Ours
Figure 4: Qualitative comparison with Instruct-NeRF2NeRF. Our ViCA-NeRF provides more
detailscomparedwithInstruct-NeRF2NeRF.Inaddition,ourViCA-NeRFcanhandlechallenging
prompts,suchas“Turnhimintoarobot,”whereasInstruct-NeRF2NeRFfailsinsuchcases.
3.2 Depth-GuidedSequentialEditing
Inourapproach,wechoosetoreflecttheuser’s2Deditsin3D.Thus,weproposeasequentialediting
strategy. Bydefiningcertainkeyviews,wesequentiallyeditthembyprojectingthepreviousedits
tothenextkeyview. Thisisthegeometricregularizationthatexplicitlyenforcestheconsistency
betweendifferentviews.
SequentialEditingwithMixup. Theoverallprocedureofeditingissimpleyeteffective,which
consistsofeditingkeyviewsandprojectingtootherviews. First,weselectasequenceofkeyviews
starting from a random view. Then, we sequentially use Instruct-Pix2Pix to edit each view and
projectthemodificationtothefollowingviews. Atthesametime,non-keyviewsarealsomodified
byprojection. Aftereditingallkeyviews,theentiredatasetisedited.
Giventhatweprojectseveralkeyviewsontootherviewswhentheyareedited,eachviewcouldbe
modifiedmultipletimes.Onlypixelsthathavenotyetbeenmodifiedarealteredtopreventinadvertent
overwriting. Therefore,modificationsfromkeyviewsaremixedupwitheachview.
Consequently,ourmethodyieldsafinalresultthatalignscloselywiththemodifiedviews,facilitating
a controllable editing process. Since previously modified pixels are not changed until blending
refinement(explainedinSection3.3),theeditsofthefirstfewviewscandominatethefinalresult.
KeyViewSelection. Weemploykeyviewstomaketheeditingprocessconsistent. Thisstrategy
allowstheentirescenetobeconstrainedbyahandfulofeditedviews. Toensuretheeditedviews
maintainconsistencyandbearsimilarcontent,wedefinethemodifiedratioρforeachviewasthe
fractionofmodifiedpixels. Then,weusethefollowingequationtoselecttheviewwiththelargest
5weightwasthenextkeyview:
(cid:26)
ρ ρ<ϕ
w = (2)
ϕ−(ρ−ϕ) ρ≥ϕ,
whereϕsignifiesthemaximumdesiredratio,whichissettoϕ=0.3inallexperiments. Intuitively,
eachtimethenextselectedkeyviewwillhaveasuitableoverlapwiththeeditedregionfromeachof
thepreviouskeyviews. Thekeyviewselectionendswhenallviews’minimumρexceedsacertain
threshold.
Depth-GuidedProjection. ModernNeRFmodelsarecapableofgeneratingdepthmapsdirectly,
therebymakingitpossibletoconstructgeometricrelationshipsbetweendifferentviews. Weillustrate
anexampleofprojectingfromeditedviewj toviewiasbelow.
GivenanRGBimageI anditscorrespondingestimateddepthmapD ,the3Dpointcloudassociated
i i
withviewicanbecalculatedas:
p =P−1(D ,K ,ξ ). (3)
i i i i
Here, K denotes the intrinsic parameters, ξ represents the extrinsic parameters, and P−1 is a
i i
functionmapping2Dcoordinatestoa3Dpointcloud.
Tofindthecorrespondingpixelsonanotherview,weprojectthispointcloudtoviewj asfollows:
I =P(I ,K ,ξ ,p ), (4)
i,j j j j i
whereI signifiesthecolorofthepixelsprojectedfromI ontoI . Inpractice,toensurethecorrect
i,j j i
correspondenceandmitigatetheeffectsofocclusion,wecomputethereprojectionerrortofilterout
invalidpairs. Byusingthiscorrespondence,wecanprojectpixelsfromviewj toviewi.
3.3 BlendingRefinementModel
The depth information derived from NeRF [1] can often be quite noisy, resulting in noticeable
outliers, soonlyapplyingthegeometricregularizationisinsufficient. Specifically, whilewecan
filter out incorrect correspondences, the remaining pixels and projections from varying lighting
conditions may still introduce artifacts. Furthermore, a reasonable 2D edit in a single view may
appear strange when considered within the overall 3D shape. To address these challenges, we
introduceablendingrefinementmodelthatemploystwoInstruct-Pix2Pix[3]processes,actingasthe
learnedregularizationtoimprovetheoverallquality.
DetailsaboutthemodelareshowninFigure3,whereeachdiffusionprocessservesuniquefunctions.
Duringthefirstpass,weusethemixupimageasinputandtheoriginalimageasacondition. This
procedureaimstopurifythenoisymixupimagebypreservingthestructuralintegrityoftheoriginal
image. Theresultingimageappearsdissimilartotheconsistentmixupimagebutwithoutanynoise.
Thusduringthesecondpass,weusetheresultingimagefromthefirstpassasinputandthemixup
image as the condition. Given the shared semantic information between these two images, the
outcomestrivestocloselyalignwiththemixupimage’sdetailedcharacteristics. Doingsoallows
ustoswiftlygenerateaclean,stylizedresult,bypassingtheneedforiterativeupdatestothedataset
duringtraining.
An essential component of our refinement model involves using an average latent code for each
of the two processes. We discovered that the diffusion model tends to generate diverse results,
makingitchallengingtocloselyalignwiththetargetstructureorcontent. However,thiseffectcanbe
substantiallymitigatedbyaveragingmultipleruns. Specifically,weintroducen noisedlatentcodes
r
andupdatethemindependently. Finally,wecomputetheaverageofthelatentcodesanddecodeit
backintotheimage. Thismethodprovidesamorestableandconsistentoutput,enhancingtheoverall
qualityoftheeditingprocess.
3.4 Warm-UpandPost-RefinementStrategies
Tomaketheeditingmoreefficientandfurtherimprovethequality,weproposeawarm-upstrategy
alongwithapost-refinementstrategy.
Warm-Up. When editing via instruct-Pix2Pix, the extent of editing changes can be modulated
throughhyperparameters,suchastheguidancescale[15]. However,achievingthedesiredoverall
6“Vincent van Gogh” “Tolkien Elf” “Fauvism” “Lord Voldemort” “Edvard Munch”
Original NeRF
“Make him look like Vincent Van “Turn him into the Tolkien Elf” “Make him look like a Fauvism “Turn him into Lord Voldemort” “Make him look like an Edvard
Gogh” painting” Munch painting”
Figure5: ComparisononNeRF-Art. WecomparetheeditingresultsbasedonNeRF-Art’s [34]
sequences and edits. Our ViCA-NeRF produces more detailed information and achieves more
substantialchangestothecontent.
styleremainsaproblem.AlthoughInstruct-NeRF2NeRFensuresconsistentstylization,itnecessitates
iterativeupdates,whichcouldinefficientlyinvolvealargenumberofiterations.
Toovercometheselimitations,weintroduceastrategytoefficientlywarmuptheeditingprocessby
blendingeditsdirectly. Thisapproachisperformedbeforetheentireeditingprocess. Specifically,we
randomlyselectaviewtoedit,andthenprojecttheeditedviewontoallotherviews. Foragivenview
iandaneditedviewj,anupdateiscalculatedasfollows:
I′ =λI ⊙m +(1−λ)I ⊙m +I ⊙(1−m ). (5)
i i w i,j w i w
Here,m denotesthebinarymaskfortheprojectedarea,andλisahyperparameteremployedto
w
controltheratiopreservedfromtheoriginalpixelvalue. Thisefficientwarm-upstrategyaccelerates
theeditingprocesswhilepreservingahighdegreeofconsistencyinthestylization.
Post-Refinement. To further enhance the consistency of the edited results, we employ a post-
refinementstrategyoncetheNeRFhasbeentrainedonourupdateddata. Westillutilizethemodified
Instruct-Pix2Pixwithaveragingarchitecturetogenerateamoreaccuratetargetforeachview. Unlike
thefirststage,thistimeweemploytherenderedimageasinputwhilecontinuingtousethemixup
imageasacondition. Thispost-refinementstrategyfurtherrefinestheconsistencyandqualityofthe
finaloutput,contributingtoamorecohesiveandvisuallyappealingresult.
4 Experiments
Weconductexperimentsonvariousscenesandtextprompts. Allourexperimentsarebasedonreal
sceneswithNeRFStudio[9]. Wefirstshowsomequalitativeresultsandcomparisonsbetweenour
methodandInstruct-NeRF2NeRF[2]. Forartisticstylization,wetestourmethodwiththecasesfrom
NeRF-Art[34]. Experimentsshowthatweachievemoredetailededits. Basedonthesescenes,we
furtherconductablationstudiesonourmethod,includingtheeffectsofdifferentcomponentsinthe
framework,thewarm-upstrategy,failurecases,andrepresentativehyperparameters. Wealsoconduct
quantitativeevaluationoftheresults,testingthetextualalignment,consistency,andefficiencyofour
method.
4.1 ImplementationDetails
Forafaircomparison,ourmethodemploysaconfigurationsimilartothatofInstruct-NeRF2NeRF.
Weutilizethe‘nerfacto’modelfromNeRFStudioandInstruct-Pix2Pix[3]asour2Deditingmodel.
Forthediffusionmodel,weapplydistincthyperparametersfordifferentcomponents. Duringthe
editingofkeyviews,theinputtimesteptissetwithintherange[0.5,0.9]. Weemploy10diffusion
stepsforthisphase. Intheblendingrefinementmodel, wesettto0.6andn to5, anduseonly
r
3diffusionsteps. Thereducednumberofstepsenablesthemodeltooperatemorequickly,since
encodinganddecodingareexecutedonlyonce. Thediffusionmodelprovidesadditionaladjustable
7
trA-FReN
FReN2FReN-tcurtsnI
sruOEdited 2D Key View Rendered Result Edited 2D Key View Rendered Result
“Turn him into Link from Zelda”
Figure6: Different2Deditsandcorresponding3Dedits. OurViCA-NeRFachieveshighcorre-
spondencesbetween2Deditsand3Dedits,thusenablinguserstocontrolthefinaledits. Ontheother
hand,ourmethodexhibitshighdiversitywithdifferentrandomseeds.
Origin Image Mixup Blended Post-Refined Rendered
Improving Consistency
Figure 7: Image update through our ViCA-NeRF. Our blending model effectively eliminates
artifactsandboundariescausedbymixup. Thepost-refinementstepfurtherimprovestheconsistency
withthemixupimageandthe3Dcontent.
hyperparameters,suchastheimageguidancescaleS andthetextguidancescaleS . Weadoptthe
I T
defaultvaluesofS =1.5andS =7.5fromthemodelwithoutmanualadjustment.
I T
FortrainingtheNeRF,weuseanL1andLPIPS(LearnedPerceptualImagePatchSimilarity)loss
throughouttheprocess. Initially,wepre-trainthemodelfor30,000iterationsfollowingthe‘nerfacto’
configuration. Subsequently,wecontinuetotrainthemodelusingourproposedmethod. Theoptional
post-refinementprocessoccursat35,000iterations. Weobtainthefinalresultsat40,000iterations.
4.2 QualitativeEvaluation
Our qualitative results, illustrated in Figure 4 and Figure 5, demonstrate the effectiveness of our
methodineditingscenesthroughmultipleviews. Wepresentresultsonthreesceneswithvarying
complexity,fromhumanfigurestolargeoutdoorscenes. Wealsoevaluatethestylizationcapabilities
ofNeRF-Art[34]andcompareourresultswiththepreviousstateoftheart.
For comparison with Instruct-NeRF2NeRF, we replicate some cases from their work and assess
severalmorechallengingcases. Forsimplertaskssuchas“turnthebearintoagrizzlybear”or“make
itlooklikeithassnowed,”ourmethodachievesresultssimilartothoseofInstruct-NeRF2NeRF.This
maybeduetotheinherentconsistencyofdiffusionmodelsinhandlingsuchscenarios. Formore
complextasksthatrequireadditionaldetail,liketransforminganobjectintoarobotoraddingflowers,
our method shows significant improvements. A notable example is the “Spiderman cas,” where
Instruct-NeRF2NeRFstruggleswithdifferentiatinglinesonthemask,whileourmethodrendersa
clearappearance.
WhencomparedonNeRF-Art,ourmethodclearlyoutperformstheprevioustechniquesbyexhibiting
moredetailsandadheringmorecloselytothedesiredstyle. Althoughweusethesame2Dediting
modelasInstruct-NeRF2NeRF,weareabletoproducemorevividcolorsanddetails,whicharenot
achievablewithiterativeupdates.
Moreover,asignificantadvantageofourmethodisthatthefinal3Deditscanbedirectedthrough
severalkeyviews. Thisallowsustoachieveconsistenteditsnotonlyacrossdifferent3Dviewsbut
alsobetweenthe2Deditsandthe3Drenders,asshowninFigure6.
4.3 AblationStudy
Inourablationstudy,wefirstevaluatetheindividualeffectsofmixup,blending,andpost-refinement
onthequalityofediting. Furtherinvestigationincludestheimpactofwarm-upiterationsandNeRF
8Origin Image warmup=0 warmup=10
Figure8: Editdistributionwithdifferentwarm-upconfigurations. Whenweusewarm-up,theedit
distributionbecomesclosertothetargetdistribution.
depthestimationaccuracy. Wealsoexploretheinfluenceoftheparameterϕonkeyviewselectionfor
efficientediting. Eachofthesecomponentsiscrucialforunderstandingthenuancesofourmethod’s
performance.
ModelComponentAblation. AsshowninFigure7,themixupimagemayinitiallyappearvery
noisy and contain artifacts. When we apply our blending model to refine it, the boundary from
copy-pastingandartifactsisremoved. However,whilemaintainingalmostalltheoriginalcontent,the
appearanceoftheimagestillchangesslightly. Finally,whenweapplyourpost-refinementstrategyto
updateit,theresultimprovesandbecomessimilartotherenderedresult. Noticethattheresultafter
blendingissufficientlygood,makingfurtherrefinementanoptionalstep.
Warm-UpIterationAblation. ConsideringthatInstruct-Pix2Pixreliesheavilyontheinputimage,
modificationstotheinputcansignificantlyaffecttheoutputdistribution. Therefore,tovalidateour
warm-upprocedure,whichisdesignedtoalterthescaleofediting,weadjustthisparameterfrom0
(nowarm-up)to30. AsFigure8illustrates,ourfindingsindicatethatwhilepreservingtheprimary
geometry,ourwarm-upprocedurecaneffectivelyactasacontrollerforadjustingthescaleofediting,
introducingonlyminorcomputationalcosts.
Ablation for Depth Estimation Errors. The projection procedure is critical in our framework;
therefore,inferiordepthestimationmayaffectthefinalresult. Weconductanablationstudyfocusing
on two prevalent scenarios: glossy surfaces and sparser views. As Figure 9a demonstrates, our
modelcanaccuratelymakemodificationstothebookshelf. Ifthereprojectioncheckisomitted,more
extensivechangesarepossible,albeitatthecostoflosingoriginaldetails. Withsparserviews,as
depictedinFigure9b,ourmodeliscapableofeditingprovidedthattheNeRFissuccessfullytrained.
AblationonKeyViews. Weperformexperimentsbasedonthehyperparameterϕ. Specifically,we
testtheresultswithϕsetto0,0.3,and1,asillustratedinFigure10. Whenϕ=0,theviewwiththe
leastoverlapisselected,resultinginaclearboundary(highlightedintheredbox). Inothercases,this
issueismitigated. However,whenϕ=1,nearlyallviewsaredesignatedaskeyviews,whichleads
toreducedefficiency.
4.4 LocalEditing
Existing methods often induce simultaneous changes in both background and foreground during
editing,whichcontradictsspecificuserintentionsthattypicallyfocusonalteringdistinctobjects.
Original Data Trained NeRF
20% (13) views 50% (33) views 50% (59) views
ViCA-NeRF (Ours) Ours (w/o reprojection check) “Turn his face in to a skull”
(a)Resultswithglossysurface (b)Ablationstudyoninputviews
Figure9: Ablationstudywithincorrectdepth. Westudybothglossysurfacesandsparserviews,
where the results demonstrate that as long as NeRF is built successfully, our model can manage
incorrectdepthestimation.
9
flehs
latem
a otni
flehs
eht
nruT
lanigirO
)sruO(
FReN-ACiV�=0 “Turn him �int=o 0s.p3ider man” �=1
Figure10: Ablationstudyonϕinkeyframeselection. Ourkeyframeselectionsuccessfullyavoids
theboundaryproblemandachievesefficiency.
propogate
trained NeRF w/o local edit w/ local edit
“Turn his face in to a skull”
(a)Localmaskpropagation (b)Localediting
Figure11: Applicationforlocalediting. OurViCA-NeRFcanbeappliedwithlocalediting. The
editscanbedoneforthetargetedobjectwithoutchangingthebackground(e.g.,wall).
Toaddressthisdiscrepancy,weintroduceastraightforward,user-friendlylocaleditingtechnique
withinourframework. Initially,weemploySegmentAnythingThing(SAM)[37]toderivemasks
encompassingallcomponentswithineachimage. Subsequently,userspinpointthetargetinstance
with a single point to isolate it. Following this, we utilize the depth information to project the
identifiedmaskacrossdifferentviewsprogressively. Thisprocesscontinuesaslongastheprojected
masksufficientlyoverlapswiththecorrespondingmaskinthecurrentview. Then,weintegratethe
wrappedsegmentationmaskwiththesegmentedmasksinthecurrentview.
Figure11exemplifiestheprocess,resultinginamulti-viewconsistentsegmentationthatpermits
exclusivemodificationofthehumanfigure,leavingthebackgroundintact.
4.5 Discussion
Forquantitativeevaluation,weadoptthedirectionalscoremetricutilizedinInstruct-Pix2Pix[3]and
thetemporalconsistencylossemployedinInstruct-NeRF2NeRF[2]. Duetotheundisclosedsplitand
textpromptsfromInstruct-NeRF2NeRF,weestablishvariousdifficultylevelstoassessperformance.
Resultsandcomprehensivedetailsareprovidedinthesupplementarymaterial.
AfurtherbenefitofourapproachisthedecouplingoftheNeRFtrainingfromthedatasetupdate
process. Thisseparationsignificantlyreducesthecomputationalburdentypicallyimposedbythe
diffusionmodelateachiteration. Theonlycomputationalexpensearisesfromthewarm-up,blending,
and post-refinement stages. We analyze the time cost of the “Face” scene and compare it with
Instruct-NeRF2NeRF on an RTX 4090 GPU. Both methods undergo 10,000 iterations; however,
Instruct-NeRF2NeRFrequiresapproximately45minutes,whereasourViCA-NeRFonlyneeds15
minutes.
Limitations. Ourmethod’sefficacyiscontingentuponthedepthmapaccuracyderivedfromNeRF,
whichunderscoresourrelianceonthequalityofNeRF-generatedresults. Althoughourblending
refinementcanmitigatetheimpactofincorrectcorrespondencestosomeextent,thequalityofthe
resultsdeteriorateswithinaccuratedepthinformation. WealsonotethatakintoInstruct-NeRF2NeRF,
theeditedoutcomestendtoexhibitincreasedblurrinessrelativetotheoriginalNeRF.Weinvestigate
thisphenomenonfurtherandprovideadetailedanalysisinthesupplementarymaterial.
5 Conclusion
Inthispaper,wehaveproposedViCA-NeRF,aview-consistency-aware3Deditingframeworkfor
text-guidedNeRFediting. Givenatextinstruction,wecanedittheNeRFwithhighefficiency. In
addition to simple tasks like human stylization and weather changes, we support context-related
operationssuchas“addsomeflowers”andeditinghighlydetailedtextures. Ourmethodoutperforms
severalbaselinesonawidespectrumofscenesandtextprompts. Inthefuture,wewillcontinueto
improvethecontrollabilityandrealismof3Dediting.
10Acknowledgements
ThisworkwassupportedinpartbyNSFGrant#2106825,NIFAAward#2020-67021-32799,the
Illinois-InsperPartnership,theJumpARCHESendowmentthroughtheHealthCareEngineering
SystemsCenter,theNationalCenterforSupercomputingApplications(NCSA)attheUniversityof
IllinoisatUrbana-ChampaignthroughtheNCSAFellowsprogram,theIBM-IllinoisDiscoveryAc-
celeratorInstitute,andtheAmazonResearchAward. ThisworkusedNVIDIAGPUsatNCSADelta
throughallocationsCIS220014andCIS230012fromtheAdvancedCyberinfrastructureCoordination
Ecosystem: Services&Support(ACCESS)program,whichissupportedbyNSFGrants#2138259,
#2138286,#2138307,#2137603,and#2138296.
References
[1] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoor-
thi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis.
CommunicationsoftheACM,65(1):99–106,2021. 1,3,4,6
[2] AyaanHaque,MatthewTancik,AlexeiEfros,AleksanderHolynski,andAngjooKanazawa.
Instruct-NeRF2NeRF:Editing3Dsceneswithinstructions. InICCV,2023. 2,3,7,10,14,17
[3] TimBrooks,AleksanderHolynski,andAlexeiAEfros. InstructPix2Pix: Learningtofollow
imageeditinginstructions. InCVPR,2023. 2,3,4,6,7,10,13,16
[4] OmriAvrahami,DaniLischinski,andOhadFried. Blendeddiffusionfortext-driveneditingof
naturalimages. InCVPR,2022. 2
[5] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchical
text-conditionalimagegenerationwithCLIPlatents. arXivpreprintarXiv:2204.06125,2022. 2
[6] BahjatKawar,ShiranZada,OranLang,OmerTov,HuiwenChang,TaliDekel,InbarMosseri,
andMichalIrani. Imagic:Text-basedrealimageeditingwithdiffusionmodels. InCVPR,2023.
[7] AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDanielCohen-Or.
Prompt-to-promptimageeditingwithcrossattentioncontrol. arXivpreprintarXiv:2208.01626,
2022.
[8] ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefano
Ermon. SDEdit: Guidedimagesynthesisandeditingwithstochasticdifferentialequations. In
ICLR,2022. 2
[9] MatthewTancik,EthanWeber,EvonneNg,RuilongLi,BrentYi,TerranceWang,Alexander
Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo
Kanazawa. Nerfstudio: A modular framework for neural radiance field development. In
SIGGRAPH,2023. 3,4,7,17
[10] ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphics
primitiveswithamultiresolutionhashencoding. ACMTrans.Graph.,2022. 3,4
[11] AlbertPumarola,EnricCorona,GerardPons-Moll,andFrancescMoreno-Noguer. D-NeRF:
Neuralradiancefieldsfordynamicscenes. InCVPR,2020.
[12] AlexYu,RuilongLi,MatthewTancik,HaoLi,RenNg,andAngjooKanazawa. PlenOctrees
forreal-timerenderingofneuralradiancefields. InICCV,2021.
[13] RicardoMartin-Brualla,NohaRadwan,MehdiSMSajjadi,JonathanTBarron,AlexeyDosovit-
skiy,andDanielDuckworth. NeRFinthewild: Neuralradiancefieldsforunconstrainedphoto
collections. InCVPR,2021.
[14] JiamingSun,XiChen,QianqianWang,ZhengqiLi,HadarAverbuch-Elor,XiaoweiZhou,and
NoahSnavely. Neural3Dreconstructioninthewild. InSIGGRAPH,2022. 3
[15] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022. 3,6
[16] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. In
ICLR,2021. 3
[17] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fantasia3D:DisentanglingGeometryand
AppearanceforHigh-qualityText-to-3DContentCreation. InICCV,2023. 3
11[18] AmitRaj,SrinivasKaza,BenPoole,MichaelNiemeyer,BenMildenhall,NatanielRuiz,Shiran
Zada,KfirAberman,MichaelRubenstein,JonathanBarron,YuanzhenLi,andVarunJampani.
DreamBooth3D:Subject-DrivenText-to-3DGeneration. ICCV,2023. 3
[19] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall. Dreamfusion: Text-to-3Dusing
2Ddiffusion. ICLR,2023.
[20] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang,Karsten
Kreis,SanjaFidler,Ming-YuLiu,andTsung-YiLin. Magic3D:High-resolutiontext-to-3D
contentcreation. InCVPR,2023. 3
[21] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh:
Text-drivenneuralstylizationformeshes. InCVPR,2022. 3
[22] StevenLiu,XiumingZhang,ZhoutongZhang,RichardZhang,Jun-YanZhu,andBryanRussell.
EditingConditionalRadianceFields. InICCV,2021. 3
[23] SosukeKobayashi,EiichiMatsumoto,andVincentSitzmann. DecomposingNeRFforediting
viafeaturefielddistillation. InNeurIPS,2022.
[24] BangbangYang,YindaZhang,YinghaoXu,YijinLi,HanZhou,HujunBao,GuofengZhang,
and Zhaopeng Cui. Learning object-compositional neural radiance field for editable scene
rendering. InICCV,2021.
[25] Pei-ZeChiang,Meng-ShiunTsai,Hung-YuTseng,Wei-ShengLai,andWei-ChenChiu. Styliz-
ing3Dsceneviaimplicitrepresentationandhypernetwork. InWACV,2022.
[26] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh, and Ming-Hsuan Yang.
Learningtostylizenovelviews. InICCV,2021.
[27] Yi-HuaHuang,YueHe,Yu-JieYuan,Yu-KunLai,andLinGao. StylizedNeRF:Consistent3D
scenestylizationasstylizednerfvia2D-3Dmutuallearning. InCVPR,2022.
[28] QilingWu,JianchaoTan,andKunXu. PaletteNeRF:Palette-basedcoloreditingforNeRFs. In
CVPR,2023.
[29] ChongBao,YindaZhang,BangbangYang,TianxingFan,ZesongYang,HujunBao,Guofeng
Zhang,andZhaopengCui.SINE:Semantic-drivenimage-basedNeRFeditingwithprior-guided
editingfield. InCVPR,2023.
[30] KaiZhang,NickKolkin,SaiBi,FujunLuan,ZexiangXu,EliShechtman,andNoahSnavely.
ARF:Artisticradiancefields. InECCV,2022.
[31] ThuNguyen-Phuoc,FengLiu,andLeiXiao. SNeRF:Stylizedneuralimplicitrepresentations
for3Dscenes. InWACV,2022.
[32] ClémentJambon,BernhardKerbl,GeorgiosKopanas,StavrosDiolatzis,ThomasLeimkühler,
andGeorgeDrettakis. NeRFshop: Interactiveeditingofneuralradiancefields. Proceedingsof
theACMonComputerGraphicsandInteractiveTechniques,6(1),2023. 3
[33] Jun-KunChen,JipengLyu,andYu-XiongWang. NeuralEditor: Editingneuralradiancefields
viamanipulatingpointclouds. InCVPR,2023. 3
[34] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao.
NeRF-Art: Text-drivenneuralradiancefieldsstylization. IEEETransactionsonVisualization
andComputerGraphics,2023. 3,7,8,15,16
[35] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML.PMLR,2021. 3,14
[36] HiromichiKamata,YuikoSakuma,AkioHayakawa,MasatoIshii,andTakuyaNarihira. Instruct
3D-to-3D:TextInstructionGuided3D-to-3Dconversion. arXivpreprintarXiv:2303.15780,
2023. 3
[37] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,PiotrDollár,andRossGirshick.
Segmentanything. InICCV,2023. 10
[38] RinonGal,OrPatashnik,HaggaiMaron,GalChechik,andDanielCohen-Or. Stylegan-nada:
CLIP-guideddomainadaptationofimagegenerators. SIGGRAPH,2022. 14
12A AdditionalQualitativeExperiments
A.1 Diversity
Importantly,ViCA-NeRFcanproducediverseresults,leveragingthediversityinherentinthe2D
diffusion model. By setting different random seeds, our edits exhibit much more diversity than
Instruct-NeRF2NeRF.Specifically,Instruct-NeRF2NeRFproducessimilarresultsonthesametext
prompt,asevidencedbythecomparisonbetweenFigure12andFigure6inthemainpaper.
Original Instruct-NeRF2NeRF Result-1 Instruct-NeRF2NeRF Result-2 Instruct-NeRF2NeRF Result-3
“Turn him into Link from Zelda”
Figure 12: Lack of generation diversity in Instruct-NeRF2NeRF. Compared with our editing
resultsinFigure6,Instruct-NeRF2NeRFsuffersfromthelimiteddiversityofitsfinal3Dedits.
A.2 2DEditConsistency
AnothermaindifferencebetweenourmethodandInstruct-NeRF2NeRFisourabilitytoperform
view-consistenteditspriortoNeRFtraining. Particularly,wevalidatethatoureditswithoutNeRF
tuning are even more consistent than the final updated edits produced by Instruct-NeRF2NeRF.
AsshowninFigure13, evenonslightlydifferentviews, Instruct-NeRF2NeRFtendstoeditvery
differently,whereasourViCA-NeRFachievesconsistent2Dedits.
ViCA-NeRF (Ours)
Instruct-NeRF2NeRF
Figure 13: 2D edit consistency comparison. We compare our 2D edit result with Instruct-
NeRF2NeRF,whereoureditsaremuchmoreview-consistentashighlightedintheredrectangles.
A.3 MoreAblations
A.3.1 ImpactofDifferentComponents
WefurtherinvestigatetheimpactofdifferentcomponentsinViCA-NeRFon3Dedits.Specifically,we
graduallyintroducecomponentsintoaview-independenteditingbaseline.Thisbaselineindependently
editseachviewonceusingInstruct-Pix2Pix[3]andthentraintheNeRFmodel. Therenderedimages
inFigure14showthatViCA-NeRFimprovesthequalitywitheachincorporatedcomponent.
A.3.2 Warm-UpIterations
Since the main distribution of 2D edits is changed through the warm-up procedure, as shown in
Figure8,itisalsoimportanttostudyitsimpactonthefinalrenderedresult. Here,wecomparethe
3Deditswithwarm-upiterationsof0,10,and30. AsdemonstratedinFigure15,afewwarm-up
iterationsgreatlyincreasethescaleofediting.
13
”nam
nori
otni
mih
nruT““Add fireworks to the sky”
“Add rivers”
“Add big birds to the sky”
Original NeRF Instruct-NeRF2NeRF ViCA-NeRF (Ours)
Figure14: Additionalqualitativeresultsonrepresentativeoutdoorscenes. Wefurtherexplore
editingvariouschallengingscenes. OurViCA-NeRFcanperformcontenteditingonthesescenes,
whileInstruct-NeRF2NeRFfails.
warmup=0 warmup=10 warmup=30
“Turn it into cyberpunk style”
Figure 15: Warmup Ablation: We ablate different warmup iterations. More warmup iterations
improvethechangingscaleoftheediting.
A.3.3 AblationonAveragedDiffusionProcess
Intuitively,the2Deditingcanhavemorestableresultswithaveraging,thusimprovingconsistency.
AsshowninFigure16,whileitmayintroducesomesmoothnessintotheimage,theoverall3Dediting
resultissignificantlyimproved,e.g.,theclearnessofthewall,whichshowcasestheimportanceof
suchadesign.
A.4 MoreQualitativeResults
Tofurthervalidatethatourmethodisapplicabletovariousreal-worldscenes,weshow3additional
editson2outdoorscenesinFigure17. Theresultsdemonstratethatwecanconducteditswhere
Instruct-NeRF2NeRF[2]fails.
B AdditionalQuantitativeEvaluation
B.1 Metrics
WemainlyfollowthequantitativeevaluationinInstruct-NeRF2NeRF[2]. Thereare2metricsused:
text-imagedirectionscore[38]andconsistencyscore. Thetext-imagedirectionscoreisaimedto
estimatethesimilaritybetweentextchangesandimagechanges. UsingtheCLIP[35]encoder,we
14Original Mixup W/o average W/ average
(a)2Ddataeditingcomparison
Edited key view W/o averaging W/ averaging
(b)Renderingresultcomparison
Figure16: Ablationstudyonaveragingandblurriness. Whileusingamodifieddiffusionprocess
with averaging may introduce slight blurriness (Figure 16a), our design effectively improves the
overallqualitywithsignificantlybetterconsistency(Figure16b). Notably,thewallandthecloth’s
textureappearmuchclearer.
extracttheembeddingsoftheoriginalrenderedimageoI andeditedrenderedimageeI asC (oI)
i i I i
andC (eI),respectively. Fortextprompts,wecreatecaptionsfororiginalscenesasoT andedited
I i i
scenesaseT. ByusingCLIP,thetextembeddingscanbeextractedasC (oT)andC (eT). The
i T i T i
text-imagedirectionscoreiscalculatedas:
cos(C (eI)−C (oI),C (eT)−C (oT)), (6)
I i I i T i T i
wherecosmeanscosinesimilarity.
Asfortheconsistencyscore,itevaluatesthesimilarityofchangesbetweenadjacentviews,comparing
thealterationsinoriginalrenderedimageswiththoseineditedrenderedimages. Specifically,itcan
becalculatedas:
cos(C (eI )−C (eI),C (oI ))−C (oI)). (7)
I i+1 I i T i+1 T i
B.2 Results
Giventhatthechoiceofsplitandtextpromptscangreatlyimpactthefinalresult,wecategorizeour
experimentsintosimpleanddifficultsettings. Inthesimplesetting,wefocusonthefacescenefrom
NeRF-Art[34]andevaluate5prompts,resultinginatotalof5edits. Forafaircomparison,weuse
thesame5textpromptsasemployedinInstruct-NeRF2NeRF.AsshowninTable1,ourViCA-NeRF
outperformsInstruct-NeRF2NeRFinbothtext-imagedirectionscoreandconsistencyscore,withthe
lattershowingaslightimprovement.
Inthedifficultsetting,weevaluate10editsusingreal-worldscenesfromInstruct-NeRF2NeRF.The
resultsareshowninTable2. Astheeditsbecomemorechallenging,weachievesignificantlybetter
scoresonbothmetrics.
15“Add fireworks to the sky”
“Add rivers”
“Add big birds to the sky”
Original NeRF Instruct-NeRF2NeRF ViCA-NeRF (Ours)
Figure17: Additionalqualitativeresultsonrepresentativeoutdoorscenes: Wefurtherexplore
theeditsonotherscenes. Ourmethodcanperformcontenteditingonthesescenes,whileInstruct-
NeRF2NeRFfails.
Method Text-ImageDirectionScore ConsistencyScore
Instruct-NeRF2NeRF 0.1477 0.9004
ViCA-NeRF(Ours) 0.1545 0.9035
Table1: Quantitativeevaluationonsimpleedits. 5editsareevaluatedfromNeRF-Art[34]. Our
methodismoreconsistenttotextprompts.
C MoreDetailedDiscussiononLimitations
C.1 LimitationsfromInstruct-Pix2Pix
We inherit limitations from instruct-Pix2Pix [3]. While we significantly avoid the convergence
probleminInstruct-NeRF2NeRF,westillencounterothercommonissues,including: (1)inabilityto
performlargespatialchanges,suchasmanipulationsoncurrentobjectsandaddingorremovinglarge
objects;and(2)difficultyindealingwithhighlydetailedinstructions,particularlythoseinvolving
interactionsbetweendifferentpartsoftheimage. WenoticethatthelatterarisesbecauseInstruct-
Pix2Pixfailstounderstanddetailedtext,eventhoughitcansomewhatfollowinstructions. Thus,the
editstendtomainlyfocusonaspecificpartortheoverallstyleoftheimage.
C.2 LimitationsfromCurrentPipeline
Tosomeextent,wealsosufferfromtheblurryproblem,similartoInstruct-NeRF2NeRF.However,
ourfindingspointtodifferentreasons. TheresultinFigure16ashowsthatsmoothingoccurswhen
themixupimageispassedthroughthediffusionmodel. Inbothcases(withorwithoutaveraging),we
canseethatasubstantialamountofhigh-frequencyinformationislost,e.g.,textureontheface,beard,
andhair. Consequently,theresulting3DeditsviaNeRFareblurred. Thissmoothingeffectarises
sincetheInstruct-Pix2Pixdiffusionmodeltriestoreducenoiseinthemixupimageintroducedduring
projection. Despiteprovidingtheclearoriginalimageasguidance,theInstruct-Pix2Pixdiffusion
modelfailstocreateclearedits.
16Method Text-ImageDirectionScore ConsistencyScore
Instruct-NeRF2NeRF 0.1016 0.5161
ViCA-NeRF(Ours) 0.1412 0.6498
Table2: Quantitativeevaluationondifficultedits. 10editsareevaluatedonscenesfromInstruct-
NeRF2NeRF[2]. Ourmethodperformssignificantlybetteronbothmetricsinmorechallenging
editingscenarios.
D BroaderImpacts
3Dsceneeditingservesvariouspurposes,suchasconvenientlyaltering3Dscenemodelsandsupport-
ingaugmentedreality(AR)applications.Inourapproach,wefocusonsequentiallyediting2Dimages
whilemaintaining3Dconsistency. Thismethodconsiderablyimprovescontrollability,efficiency,
anddiversityintheeditingprocess. Withtherapidlyadvancingdiffusionmodel,thisapproachmay
enableabroaderrangeandmoredetailededitingoperations. Simplifyingandstreamliningtheediting
of3Dscenes,ourmethodfacilitateseasyoperationevenforuntrainedindividuals. Moreover,our
approachhasimplicationsfor3Deditingunderresource-limitedconditions,becauseiteliminatesthe
needtointegratethediffusionmodelintotheNeRFtrainingprocess. Thisnotonlymakestheediting
processsimplerandmoreconvenient,butalsodemonstratesastrategythatrequireslessGPUtime.
E AdditionalImplementationDetails
Ourmethodemploysapre-trainedNeRFmodel,namelythenerfactomodelfromNeRFStudio[9],
whichistrainedonoriginaldatafor30,000iterations. Wefine-tuneitwithourmethodfor10,000
moreiterations. Notethatweuse10,000iterations,sinceitissufficientforallscenes. Forsmall
scenes,around5,000iterationsarefoundtobeadequate.
Wetailorthewarm-uphyperparameterwithdifferentvaluesbasedonthescaleofscenes. Specifically,
Weuse10iterationsforface-centricscenesand30iterationsforoutdoorscenes. Additionally,wedo
notemploypost-refinementforlarge-scaleoutdoorsceneslike“campsite”and“farm.” Asthescene
becomeshighlycomplex,post-refinementdoesnotfurtherimproveconsistency.
Wesetthevalidthresholdforthecorrespondencematchingasl <5px,wherel isthereprojection
r r
error. Invalidcorrespondencesareignored.
17