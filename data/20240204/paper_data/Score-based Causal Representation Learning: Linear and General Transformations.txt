SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Score-based Causal Representation Learning:
Linear and General Transformations
BurakVarıcı∗ VARICB@RPI.EDU
Electrical,Computer,andSystemsEngineering
RensselaerPolytechnicInstitute
Troy,NY12180,USA
EmreAcartürk∗ ACARTE@RPI.EDU
Electrical,Computer,andSystemsEngineering
RensselaerPolytechnicInstitute
Troy,NY12180,USA
KarthikeyanShanmugam KARTHIKEYANVS@GOOGLE.COM
GoogleResearchIndia
Bangalore,India
AbhishekKumar ABHISHK@GOOGLE.COM
GoogleDeepMind
MountainView,CA94043,USA
AliTajer TAJER@ECSE.RPI.EDU
Electrical,Computer,andSystemsEngineering
RensselaerPolytechnicInstitute
Troy,NY12180,USA
Abstract
Thispaperaddressesintervention-basedcausalrepresentationlearning(CRL)underageneral
nonparametriclatentcausalmodelandanunknowntransformationthatmapsthelatentvariablesto
theobservedvariables. Linearandgeneraltransformationsareinvestigated. Thepaperaddresses
both the identifiability and achievability aspects. Identifiability refers to determining algorithm-
agnostic conditions that ensure recovering the true latent causal variables and the latent causal
graphunderlyingthem. Achievabilityreferstothealgorithmicaspectsandaddressesdesigning
algorithmsthatachieveidentifiabilityguarantees. Bydrawingnovelconnectionsbetweenscore
functions(i.e.,thegradientsofthelogarithmofdensityfunctions)andCRL,thispaperdesignsa
score-basedclassofalgorithmsthatensuresbothidentifiabilityandachievability. First,thepaper
focusesonlineartransformationsandshowsthatonestochastichardinterventionpernodesuffices
toguaranteeidentifiability. Italsoprovidespartialidentifiabilityguaranteesforsoftinterventions,
includingidentifiabilityuptoancestorsforgeneralcausalmodelsandperfectlatentgraphrecovery
forsufficientlynon-linearcausalmodels. Secondly,itfocusesongeneraltransformationsandshows
thattwostochastichardinterventionspernodesufficeforidentifiability. Notably,onedoesnotneed
toknowwhichpairofinterventionalenvironmentshavethesamenodeintervened.
Keywords: causalrepresentationlearning,causality,interventions
*.Equalcontribution
1
4202
beF
1
]GL.sc[
1v94800.2042:viXraVARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
1. Overview
Causalrepresentationlearning(CRL)aimstoformacausalunderstandingoftheworldbylearning
appropriaterepresentationsthatsupportcausalinterventions,reasoning,andplanning(Schölkopf
et al., 2021). Specifically, CRL considers a data-generating process in which high-level latent
causally-relatedvariablesaremappedtolow-level,generallyhigh-dimensionalobserveddatathrough
anunknowntransformation. Formally,consideracausalBayesiannetwork(Pearl,2009)encoded
by a directed acyclic graph (DAG) with n nodes and generating causal random variables Z ≜
G
[Z ,...,Z ]⊤. Theserandomvariablesaretransformedbyanunknownfunctiong : Rn Rd to
1 n
→
generatethed-dimensionalobserved randomvariablesX ≜ [X ,...,X ]⊤ accordingto:
1 d
X = g(Z). (1)
CRListheprocessofusingtheobserveddataX andrecovering(i)thecausalstructure and(ii)the
G
latentcausalvariablesZ. Achievingtheseimplicitlyinvolvesanotherobjectiveofrecoveringthe
unknowntransformationg aswell. AddressingCRLconsistsoftwocentralquestions:
• Identifiability,whichreferstodeterminingthenecessaryandsufficientconditionsunderwhich
G
andZ canberecovered. Thescopeofidentifiability(e.g.,perfectorpartial)criticallydepends
on the extent of information available about the data, the underlying causal structure, and the
transformation. Thenatureoftheidentifiabilityresultsisgenerallyalgorithm-agnosticandcanbe
non-constructivewithoutspecifyinghowtorecover andZ.
G
• Achievability,whichcomplementsidentifiabilityandpertainstodesigningalgorithmsthatcan
recover andZ whilemaintainingidentifiabilityguarantees. Achievabilityhingesonforming
G
reliableestimatesforthetransformationg.
CRLfromInterventions. Identifiabilityisknowntobeimpossiblewithoutadditionalsupervision
orsufficientstatisticaldiversityamongthesamplesoftheobserveddataX. Asshownin(Hyvärinen
and Pajunen, 1999; Locatello et al., 2019), this is the case even for the simpler settings in which
the latent variables are statistically independent (i.e., graph has no edges). Interventions are
G
effectivemechanismsforintroducingproperstatisticaldiversityforthemodelsoftheobserveddata.
Specifically,aninterventiononasetofcausalvariablesaltersthecausalmechanismsthatgenerate
thosevariables. Notethatthesecausalmechanismscapturetheeffectofparentsonthechildvariable.
Suchinterventions,evenwhenimposingsparsechangesinthestatisticalmodels,createvariations
intheobserveddatasufficientforlearninglatentcausalrepresentations. ThishasledtoCRLvia
interventionasanimportantclassofCRLproblems,whichinitsgeneralformhasremained
anopenproblem(Schölkopfetal.,2021). Thispaperaddressesthisopenproblembydrawing
anovelconnectiontoscorefunctions,i.e.,thegradientsofthelogarithmofdensityfunctions.
It is noteworthy that as a form of supervision via interventions, several studies have used
counterfactualpairsofobservationsamples,onecollectedbeforeandoneafteranintervention(Ahuja
et al., 2022b; Locatello et al., 2020; von Kügelgen et al., 2021; Yang et al., 2021; Brehmer et al.,
2022). However,acquiringsuchdataisdifficultsinceitimpliesbeingabletodirectlycontrolthe
latentrepresentation. Instead, weuseaweakerformofsupervisionviahavingaccesstoonlythe
pairofdistributionsbeforeandafteranintervention(asopposedtosamplepairs). Suchsupervision
is more realistic than access to counterfactual pairs and bodes well for practical applications in
genomics(Tejada-Lapuertaetal.,2023)androbotics(Leeetal.,2021).
2SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Contributions. This paper provides both identifiability and achievability results for CRL from
interventionsundergenerallatentcausalmodelsandgeneraltransformations. Weestablishthese
resultsbyuncoveringhithertounknownconnectionsbetweenscorefunctions(i.e.,thegradientsofthe
logarithmofdensityfunctions)andCRL.WeleveragetheseconnectionstodesignCRLalgorithms
that serve as constructive proofs for the identifiability and achievability results. We do not make
anyparametricassumptiononthelatentcausalmodel,i.e.,therelationshipsamongelementsofZ
takeanyarbitraryform. Forthetransformationg,weconsideradiffeomorphism(i.e.,bijectivesuch
thatbothg andg−1 arecontinuouslydifferentiable)ontoitsimage. Ourresultsarecategorizedinto
twomaingroupsbasedontheformofg: (i)lineartransformation,and(ii)general(nonparametric)
transformation. Weconsiderbothstochastichardandsoftinterventions,andourcontributionsare
summarizedbelow.
Lineartransformations. Wefirstfocusonlineartransformationsandinvestigatevariousextentsof
identifiabilityandachievability(perfectandpartial)givenoneinterventionpernode. Inthissetting,
weconsiderbothhardandsoftinterventions.
✔ On identifiability from hard interventions, we show that one hard intervention per node
sufficestoguaranteeperfectidentifiability.
✔ Onidentifiabilityfromsoftinterventions,weshowthatonesoftinterventionpernodesuffices
toguaranteeidentifiabilityuptoancestors–transitiveclosureofthelatentDAGisrecovered
and latent variables are recovered up to a linear function of their ancestors. Importantly,
theseresultsaretightsinceidentifyingthelatentlinearmodelsbeyondancestorsusingsoft
interventionsisknowntobeimpossible.
✔ Wefurthertightenthepreviousresultandshowthatwhenthelatentcausalmodelissufficiently
non-linear,perfectDAGrecoverybecomespossibleusingsoftinterventions. Furthermore,we
recoveralatentrepresentationthatisMarkovwithrespecttothelatentDAG,preservingthe
trueconditionalindependencerelationships.
✔ Onachievability,wedesignanalgorithmreferredtoasLinearScore-basedCausalLatent
EstimationviaInterventions(LSCALE-I).Thisalgorithmachievestheidentifiabilityguaran-
teesunderbothsoftandhardinterventions. LSCALE-Iconsistsofmodules(i)forachieving
identifiabilityuptoancestors,and(ii)inthecaseofhardinterventions,refinestheoutputof
module(i)withoutrequiringanyconditionalindependencetest.
Generaltransformations. Inthissetting, wedonothaveanyrestrictiononthetransformation
fromthelatentspacetotheobservedspace. Inthisgeneralsetting,ourcontributionsareasfollows.
✔ On identifiability, we show that observational data and two distinct hard interventions per
node suffice to guarantee perfect identifiability. This result generalizes the recent results
in the literature in two ways. First, we do not require the commonly adopted faithfulness
assumptiononlatentcausalmodels. Secondly,weassumethelearnerdoesnotknowwhich
pairofenvironmentsinterveneonthesamenode.
✔ Moreimportantly,onachievability,wedesignthefirstprovablycorrectalgorithmthatrecovers
andZ perfectly. ThisalgorithmisreferredtoasGeneralizedScore-basedCausalLatent
G
EstimationviaInterventions(GSCALE-I).WenotethatGSCALE-Irequiresonlythescore
3VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
functions of observed variables as its inputs and computes those of the latent variables by
leveragingtheJacobianofthedecoders.
✔ We also establish new results that shed light on the role of observational data. Specifically,
whentwointerventionalenvironmentspernodearegiveninpairs,observationaldataisonly
neededforrecoveringthelatentDAG.Furthermore,weshowthatobservationaldatacanbe
dispensedwithunderaweakfaithfulnessconditiononthelatentcausalmodel.
Beforedescribingournovelmethodology,werecallthegeneralapproachtosolvingCRL.Recovering
thelatentcausalvariableshingesonfindingtheinverseofg basedontheobserveddataX,which
facilitatesrecoveringZ viaZ = g−1(X). Inotherwords,referringto(g−1,g)asthetrueencoder-
decoderpair,wesearchforanencoderthattakesobservedvariablesbacktothetruelatentspace.
Avalidencodershouldnecessarilyhaveanassociateddecodertoensureaperfectreconstruction
of observed variables from estimated latent variables. However, there are infinitely many valid
encoder-decoderpairsthatsatisfythereconstructionproperty. Thepurposeofusinginterventionsis
toinjectvariationsintotheobserveddata,whichcanhelpusdistinguishamongthesesolutions.
Score-based methodology. We start by showing that an intervention on a latent node induces
changesonlyinthescorefunction’scoordinatescorrespondingtotheintervenednodeanditsparents.
Furthermore,whentwointerventionsonthesamenodearegiven,suchchangeswillbelimitedonly
totheintervenednode(parentsintact). Thisimpliesthatscorechangesaregenerallysparse. Fur-
thermore,thesescorechangescontainalltheinformationaboutthelatentcausalstructure. Motivated
by these key properties, we formalize a score-based CRL framework based on which we design
provablycorrectdistinctalgorithmsLSCALE-IandGCALE-Iforlinearandgeneraltransformations,
respectively,presentedinSection5. WebrieflydescribethekeytechniqueinGSCALE-Iforgeneral
transformation via two interventions. LSCALE-I involves more finesse to achieve identifiability
usingonlyoneinterventionpernodebyexploitingthelinearityofthetransformation.
Algorithm sketch for general transformations. Consider two interventional environments in
whichthesamenodeisintervened. Weshowthatthescorefunctionsofthelatentvariablesunder
thesetwoenvironmentsdifferonlyatthecoordinateoftheintervenednode. Subsequently,thekey
ideaistofindanencoderthatminimizesthevariationsinthescorefunctionsoftheestimatedlatent
variablesundersaidencoder. Tothisend,weassumethattwointerventionalmechanismsactingon
thesamenodearesufficientlydistinct(referredtoasinterventionaldiscrepancy)sothattheyprovide
diversity in information. Subsequently, we show that minimizing the total number of non-zero
elements in the score differences of the estimated latent variables finds an encoder that perfectly
recovers and Z. An important process in this methodology is projecting the score changes in
G
observeddatatothelatentspace. Weshowthatthiscanbedonebymultiplyingtheobservedscore
differencebytheJacobianofthedecoder. Therefore,recovering andZ isfacilitatedbysolvingthe
G
followingproblem:
min Jacobianofdecoder scoredifferencesofX , (2)
valid × 0
enc-dec envi (cid:88)pro ain rm sent(cid:13)
(cid:13) (cid:0)
(cid:1)(cid:13)
(cid:13)
(cid:13) (cid:13)
inwhich scoredifferencesof observed variablesare computedacrosstwoenvironmentswith the
sameintervenednode. Theexpectationiswithrespecttotheprobabilitymeasureofobservational
data, and the minimization is performed over the set of valid encoder-decoder pairs that ensure
perfectreconstructionofX.
4SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
u<latexit sha1_base64="TXpu1qoHEX19/UYgM5JS5QxwzzU=">AAACCnicbVDLSgMxFL1TX7W+qi7dRIvgqswUquKq4MZlBfuAdiiZNNOGZpIhyShl6NqNv+LGhSJu/QJ3/o1pOwttPRA4nHPvzb0niDnTxnW/ndzK6tr6Rn6zsLW9s7tX3D9oapkoQhtEcqnaAdaUM0EbhhlO27GiOAo4bQWj66nfuqdKMynuzDimfoQHgoWMYGOlXvG4q0OUiJGQDwJxbKgwSFE7QluW1ZTcsjsDWiZeRkqQod4rfnX7kiSRHUA41rrjubHxU6wMI5xOCt1E0xiTER7QjqUCR1T76eyUCTq1Sh+FUtlnV5mpvztSHGk9jgJbGWEz1IveVPzP6yQmvPRTJuLE3kjmH4UJR0aiaS6ozxQlho8twUQxuysiQ6wwMTa9gg3BWzx5mTQrZe+8XL2tlGpXWRx5OIITOAMPLqAGN1CHBhB4hGd4hTfnyXlx3p2PeWnOyXoO4Q+czx/9kpsV</latexit> nknown latent representation t<latexit sha1_base64="WqBe4+d1Gq4vUO/4pQsaclny+iE=">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgadkN+MBTwIvHCOaByRJmJ7PJkNnZZaZXCCF/4cWDIl79G2/+jZNkD5pY0FBUddPdFaZSGPS8b6ewtr6xuVXcLu3s7u0flA+PmibJNOMNlshEt0NquBSKN1Cg5O1UcxqHkrfC0e3Mbz1xbUSiHnCc8iCmAyUiwSha6bFrIoKaKuP2yhXP9eYgq8TPSQVy1Hvlr24/YVnMFTJJjen4XorBhGoUTPJpqZsZnlI2ogPesVTRmJtgMr94Ss6s0idRom0pJHP198SExsaM49B2xhSHZtmbif95nQyj62AiVJohV2yxKMokwYTM3id9oTlDObaEMi3srYQNqaYMbUglG4K//PIqaVZd/9K9uK9Wajd5HEU4gVM4Bx+uoAZ3UIcGMFDwDK/w5hjnxXl3PhatBSefOYY/cD5/AB1OkIk=</latexit> rans. o<latexit sha1_base64="tP3QoEPUp2rfgOaRJpLkMRWI19E=">AAAB+nicbVDLSgNBEJz1GeNro0cvg0HwFHYDPvAU8OIxgnlAsoTZ2d5kyOzMMjMbCTGf4sWDIl79Em/+jZNkD5pY0FBUddPdFaacaeN5387a+sbm1nZhp7i7t39w6JaOmlpmikKDSi5VOyQaOBPQMMxwaKcKSBJyaIXD25nfGoHSTIoHM04hSEhfsJhRYqzUc0tdHWMZalAjiHBEDOm5Za/izYFXiZ+TMspR77lf3UjSLAFhKCdad3wvNcGEKMMoh2mxm2lICR2SPnQsFSQBHUzmp0/xmVUiHEtlSxg8V39PTEii9TgJbWdCzEAvezPxP6+Tmfg6mDCRZgYEXSyKM46NxLMccMQUUMPHlhCqmL0V0wFRhBqbVtGG4C+/vEqa1Yp/Wbm4r5ZrN3kcBXSCTtE58tEVqqE7VEcNRNEjekav6M15cl6cd+dj0brm5DPH6A+czx/isJO+</latexit> bserved data i<latexit sha1_base64="d873uSqmn5fC9CaDo/Co6/trKcM=">AAAB9HicbVDLSgNBEOz1GeMr6tHLYBA8hd2ADzwFvHiMYB6QLGF20psMmZ1dZ2YDYcl3ePGgiFc/xpt/4yTZgyYWNBRV3XR3BYng2rjut7O2vrG5tV3YKe7u7R8clo6OmzpOFcMGi0Ws2gHVKLjEhuFGYDtRSKNAYCsY3c381hiV5rF8NJME/YgOJA85o8ZKfleHhMsQFUqGvVLZrbhzkFXi5aQMOeq90le3H7M0QmmYoFp3PDcxfkaV4UzgtNhNNSaUjegAO5ZKGqH2s/nRU3JulT4JY2VLGjJXf09kNNJ6EgW2M6JmqJe9mfif10lNeONnXCapsV8tFoWpICYmswRInytkRkwsoUxxeythQ6ooMzanog3BW355lTSrFe+qcvlQLddu8zgKcApncAEeXEMN7qEODWDwBM/wCm/O2Hlx3p2PReuak8+cwB84nz+jlpIA</latexit>nference
<latexit sha1_base64="K2gMgv91UyG+C2KnfjCDf20c+Ys=">AAAB73icbVDLSgNBEOyNrxiNRj16GQyCp7Ar+DhJwEM8RjAPSJYwO5lNhszMrjOzQlhy9yyCB0W8+h36Bd78GyePgyYWNBRV3XR3BTFn2rjut5NZWl5ZXcuu5zY281vbhZ3duo4SRWiNRDxSzQBrypmkNcMMp81YUSwCThvB4HLsN+6o0iySN2YYU1/gnmQhI9hYqZm2CeaoMuoUim7JnQAtEm9GiuXs/eNn5SNf7RS+2t2IJIJKQzjWuuW5sfFTrAwjnI5y7UTTGJMB7tGWpRILqv10cu8IHVqli8JI2ZIGTdTfEykWWg9FYDsFNn09743F/7xWYsJzP2UyTgyVZLooTDgyERo/j7pMUWL40BJMFLO3ItLHChNjI8rZELz5lxdJ/bjknZZOrm0aFzBFFvbhAI7AgzMowxVUoQYEODzAM7w4t86T8+q8TVszzmxmD/7Aef8BCICS9g==</latexit> <latexit sha1_base64="DT9XL/geQMFgzOoUf9edSm5U66o=">AAACMHicbVDLSsQwFE3Hd31VXboJDoKroR18LUUXuhzB0YHpUNL0zhhM05Kkg0OZT3Ljp+hGQRG3foVpp+DzQriHc+5NTk6Ycqa06z5btanpmdm5+QV7cWl5ZdVZW79USSYptGnCE9kJiQLOBLQ10xw6qQQShxyuwpuTQr8aglQsERd6lEIvJgPB+owSbajAOfXLO3IJ0dj2QxgwkYcx0ZLdju1O4GHfx6Y3Tbf9YZRoVaBOENk+iOhrNHDqbsMtC/8FXgXqqKpW4Dz4UUKzGISmnCjV9dxU93IiNaMcjJdMQUroDRlA10BBYlC9vDQ7xtuGiXA/keYIjUv2+0ZOYqVGcWgmjcFr9VsryP+0bqb7h72ciTTTIOjkoX7GsU5wkR6OmASq+cgAQiUzXjG9JpJQbTIuQvB+f/kvuGw2vP3G3vlu/ei4imMebaIttIM8dICO0BlqoTai6A49ohf0at1bT9ab9T4ZrVnVzgb6UdbHJxkzqGE=</latexit> X <latexit sha1_base64="odcEuNFshEmOKnvQvGj8Ruilu9U=">AAACMXicbVDLSgMxFM3UVx1fVZdugkVwVWaKr2XRTZcV7AM6ZchkbmswkxmSTLEM/SU3/om46UIRt/6Embagtl4I93DOvcnJCRLOlHaciVVYWV1b3yhu2lvbO7t7pf2DlopTSaFJYx7LTkAUcCagqZnm0EkkkCjg0A4ebnK9PQSpWCzu9CiBXkQGgvUZJdpQfqnuTe/IAp7C2PYCGDCRBRHRkj2O7Y7vYs/DpldNt71hGGuVo44f2h6I8GfUL5WdijMtvAzcOSijeTX80osXxjSNQGjKiVJd10l0LyNSM8pzL6mChNAHMoCugYJEoHrZ1O0YnxgmxP1YmiM0nrK/NzISKTWKAjNpDN6rRS0n/9O6qe5f9TImklSDoLOH+inHOsZ5fDhkEqjmIwMIlcx4xfSeSEK1CTkPwV388jJoVSvuReX89qxcu57HUURH6BidIhddohqqowZqIoqe0Ct6Q+/WszWxPqzP2WjBmu8coj9lfX0DAh6o2A==</latexit> X
G 1 1
Z<latexit sha1_base64="5uXesc7yCRnlkRqMtXEN/AviWBs=">AAAB6nicbVBNS8NAEJ3UrxqtVj16WSwFTyUpqD0WBPFY0X5gG8pmu2mXbjZhdyOU0J/gxYMiXsUf4k/w5r9x0/agrQ8GHu/NMDPPjzlT2nG+rdza+sbmVn7b3tkt7O0XDw5bKkokoU0S8Uh2fKwoZ4I2NdOcdmJJcehz2vbHl5nffqBSsUjc6UlMvRAPBQsYwdpIt/f9ar9YcirODGiVuAtSqhc+k/KV/dHoF796g4gkIRWacKxU13Vi7aVYakY4ndq9RNEYkzEe0q6hAodUeens1CkqG2WAgkiaEhrN1N8TKQ6VmoS+6QyxHqllLxP/87qJDmpeykScaCrIfFGQcKQjlP2NBkxSovnEEEwkM7ciMsISE23SsU0I7vLLq6RVrbjnlbMbk0YN5sjDMZzAKbhwAXW4hgY0gcAQHuEZXixuPVmv1tu8NWctZo7gD6z3H2UnkB4=</latexit> 2 X 2 X 2
Z<latexit sha1_base64="P8tdODjE+Ccd0sgUfyQHLZQF3qg=">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gV1BwDgniMaB6YLGF2MpsMmZldZmaFsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hwpk2nvftrKyurW9s5rbc7Z387l5h/6Ch41QRWicxj1UrxJpyJmndMMNpK1EUi5DTZji8nPjNB6o0i+WdGSU0ELgvWcQINla6ve/63ULRK3tToGXiz0mxmv9MS1fuR61b+Or0YpIKKg3hWOu27yUmyLAyjHA6djuppgkmQ9ynbUslFlQH2fTUMSpZpYeiWNmSBk3V3xMZFlqPRGg7BTYDvehNxP+8dmqiSpAxmaSGSjJbFKUcmRhN/kY9pigxfGQJJorZWxEZYIWJsem4NgR/8eVl0jgt++flsxubRgVmyMERHMMJ+HABVbiGGtSBQB8e4RleHO48Oa/O26x1xZnPHMIfOO8/Y6OQHQ==</latexit> 1 E<latexit sha1_base64="XpyDk+YXOp09qs0B538WFCdAntg=">AAAB8XicdVBNSwMxEJ2tX7V+VT16CRbBU9ktWltPBRE8VrAf2K4lm6ZtaJJdkqxQlv4LLx4U8eq/8ea/MdtWUNEHA4/3ZpiZF0ScaeO6H05maXlldS27ntvY3Nreye/uNXUYK0IbJOShagdYU84kbRhmOG1HimIRcNoKxhep37qnSrNQ3phJRH2Bh5INGMHGSrdJl2COLqd3opcvuEXXolxGKfEqrmdJtVoplarIm1muW4AF6r38e7cfklhQaQjHWnc8NzJ+gpVhhNNprhtrGmEyxkPasVRiQbWfzC6eoiOr9NEgVLakQTP1+0SChdYTEdhOgc1I//ZS8S+vE5tBxU+YjGJDJZkvGsQcmRCl76M+U5QYPrEEE8XsrYiMsMLE2JByNoSvT9H/pFkqeuXi6fVJoXa+iCMLB3AIx+DBGdTgCurQAAISHuAJnh3tPDovzuu8NeMsZvbhB5y3T4LskNE=</latexit> m Z<latexit sha1_base64="XzykGxFxVDn5LGLipGKDe8cWFFk=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5oHJEmYnvcmQ2dllZlYISz7BiwdFvPpF3vwbJ8keNFrQUFR1090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6nvqtR1Sax/LejBP0IzqQPOSMGivdPfRkr1xxq+4M5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP66QmvPQzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enZ7Wqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4xs9sI3J</latexit> n Z<latexit sha1_base64="c3JsndHsAkHJBa5PbBl/REI1p04=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr5MEvHhMwDwwWcLspJOMmZ1dZmaFsOQLvHhQxKuf5M2/cZLsQRMLGoqqbrq7glhwbVz328mtrK6tb+Q3C1vbO7t7xf2Dho4SxbDOIhGpVkA1Ci6xbrgR2IoV0jAQ2AxGt1O/+YRK80jem3GMfkgHkvc5o8ZKtYduseSW3RnIMvEyUoIM1W7xq9OLWBKiNExQrdueGxs/pcpwJnBS6CQaY8pGdIBtSyUNUfvp7NAJObFKj/QjZUsaMlN/T6Q01HocBrYzpGaoF72p+J/XTkz/2k+5jBODks0X9RNBTESmX5MeV8iMGFtCmeL2VsKGVFFmbDYFG4K3+PIyaZyVvcvyRe28VLnJ4sjDERzDKXhwBRW4gyrUgQHCM7zCm/PovDjvzse8NedkM4fwB87nD7rNjOQ=</latexit> g <latexit sha1_base64="x4POlxkUjz9TkQ8QDCEODZODRIM=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBhPBU9gN+DgGvXiMaB6QLGF2MrsZMjuzzMwKYcknePGgiFe/yJt/4yTZgyYWNBRV3XR3BQln2rjut1NYW9/Y3Cpul3Z29/YPyodHbS1TRWiLSC5VN8CaciZoyzDDaTdRFMcBp51gfDvzO09UaSbFo5kk1I9xJFjICDZWeqhG1UG54tbcOdAq8XJSgRzNQfmrP5QkjakwhGOte56bGD/DyjDC6bTUTzVNMBnjiPYsFTim2s/mp07RmVWGKJTKljBorv6eyHCs9SQObGeMzUgvezPxP6+XmvDaz5hIUkMFWSwKU46MRLO/0ZApSgyfWIKJYvZWREZYYWJsOiUbgrf88ipp12veZe3ivl5p3ORxFOEETuEcPLiCBtxBE1pAIIJneIU3hzsvzrvzsWgtOPnMMfyB8/kDhr2NTw==</latexit> X<latexit sha1_base64="msl/X9qDA/lnLAU3kxEoSMFOz3I=">AAAB6HicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdZKAF48JmAckS5id9CZjZmeXmVkhLPkCLx4U8eonefNvnCR70MSChqKqm+6uIBFcG9f9dlZW19Y3Ngtbxe2d3b390sFhU8epYthgsYhVO6AaBZfYMNwIbCcKaRQIbAWju6nfekKleSwfzDhBP6IDyUPOqLFSvd0rld2KOwNZJl5OypCj1it9dfsxSyOUhgmqdcdzE+NnVBnOBE6K3VRjQtmIDrBjqaQRaj+bHTohp1bpkzBWtqQhM/X3REYjrcdRYDsjaoZ60ZuK/3md1IQ3fsZlkhqUbL4oTAUxMZl+TfpcITNibAllittbCRtSRZmx2RRtCN7iy8ukeV7xriqX9Yty9TaPowDHcAJn4ME1VOEeatAABgjP8ApvzqPz4rw7H/PWFSefOYI/cD5/ALfFjOI=</latexit> 2 6
6
6X. . . d3 7
7
7
2 6
6
6X. . . d3 7
7
7
G<latexit
sha1_base64="dJDqXscr5qw3vk8LaCT7YHx3UXM=">AAAB+HicbVDLSgMxFM3UV62Pjrp0EyyCG8uM+FpJpYgKLiraB7RDyaSZNjSZDElGrEO/xI0LRdz6Ke78G9N2Ftp64MLhnHu59x4/YlRpx/m2MnPzC4tL2eXcyuraet7e2KwpEUtMqlgwIRs+UoTRkFQ11Yw0IkkQ9xmp+/3yyK8/EKmoCO/1ICIeR92QBhQjbaS2nW9xXzwml3fl85uL/eth2y44RWcMOEvclBRAikrb/mp1BI45CTVmSKmm60TaS5DUFDMyzLViRSKE+6hLmoaGiBPlJePDh3DXKB0YCGkq1HCs/p5IEFdqwH3TyZHuqWlvJP7nNWMdnHoJDaNYkxBPFgUxg1rAUQqwQyXBmg0MQVhScyvEPSQR1iarnAnBnX55ltQOiu5x8ej2sFA6S+PIgm2wA/aAC05ACVyBCqgCDGLwDF7Bm/VkvVjv1sekNWOlM1vgD6zPH7IBknY=</latexit>L<latexit sha1_base64="NCThbQeI64lIaQkwMLEZxXJu90U=">AAAB+HicbVDLSgMxFM3UV62Pjrp0EyyCG8uM+FpJpQgKXVS0D2iHkkkzbWgyGZKMWId+iRsXirj1U9z5N6btLLT1wIXDOfdy7z1+xKjSjvNtZRYWl5ZXsqu5tfWNzby9tV1XIpaY1LBgQjZ9pAijIalpqhlpRpIg7jPS8Aflsd94IFJREd7rYUQ8jnohDShG2kgdO9/mvnhMKnfly8rV4c2oYxecojMBnCduSgogRbVjf7W7AsechBozpFTLdSLtJUhqihkZ5dqxIhHCA9QjLUNDxInyksnhI7hvlC4MhDQVajhRf08kiCs15L7p5Ej31aw3Fv/zWrEOzr2EhlGsSYini4KYQS3gOAXYpZJgzYaGICypuRXiPpIIa5NVzoTgzr48T+pHRfe0eHJ7XChdpHFkwS7YAwfABWegBK5BFdQABjF4Bq/gzXqyXqx362PamrHSmR3wB9bnD7m9kns=</latexit> S SC CA AL LE E- -I
I
G<latexit sha1_base64="H50X2yIOmZ58AMXzr4LFOmQXIIM=">AAACE3icbVDLSgMxFM3UV62vUZdugkUQF2VGfIEgBRe6rGAf2CnlTpppQzOZIcmIZZh/cOOvuHGhiFs37vwb08dCWw8EDuecy809fsyZ0o7zbeXm5hcWl/LLhZXVtfUNe3OrpqJEElolEY9kwwdFORO0qpnmtBFLCqHPad3vXw79+j2VikXiVg9i2gqhK1jACGgjte0Drwc69Qjw9CrLsHeOvdCPHlJPBRhEZ6wMI3dZ2y46JWcEPEvcCSmiCSpt+8vrRCQJqdCEg1JN14l1KwWpGeE0K3iJojGQPnRp01ABIVWtdHRThveM0sFBJM0TGo/U3xMphEoNQt8kQ9A9Ne0Nxf+8ZqKDs1bKRJxoKsh4UZBwrCM8LAh3mKRE84EhQCQzf8WkBxKINjUWTAnu9MmzpHZYck9KxzdHxfLFpI482kG7aB+56BSV0TWqoCoi6BE9o1f0Zj1ZL9a79TGO5qzJzDb6A+vzB2kwndw=</latexit>ˆ and Zˆ
Z<latexit sha1_base64="G8/pR9sI0eZgbOJNGBTvgoYNAhY=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5oHJEmYnvcmQ2dllZlYISz7BiwdFvPpF3vwbJ8keNFrQUFR1090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6nvqtR1Sax/LejBP0IzqQPOSMGivdPfR4r1xxq+4M5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP66QmvPQzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enZ7Wqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4xs2HI3E</latexit> i 4 5 4 5
<latexit sha1_base64="u1DPd3lC7isH1d2FqSjJFTwbnbM=">AAAB+XicdVDLSsNAFJ3UV62vqEs3g0VwFZL0qauCCC4r2Ac0sUwmk3bo5MHMpFBC/8SNC0Xc+ifu/BsnbQUVPXDhcM693HuPlzAqpGl+aIW19Y3NreJ2aWd3b/9APzzqijjlmHRwzGLe95AgjEakI6lkpJ9wgkKPkZ43ucr93pRwQePoTs4S4oZoFNGAYiSVNNR1R1Lmk8zBiMHr+X041MumcdGs2zUbmoZpNuxKPSd2o2pXoKWUHGWwQnuovzt+jNOQRBIzJMTAMhPpZohLihmZl5xUkAThCRqRgaIRColws8Xlc3imFB8GMVcVSbhQv09kKBRiFnqqM0RyLH57ufiXN0hl0HQzGiWpJBFeLgpSBmUM8xigTznBks0UQZhTdSvEY8QRliqskgrh61P4P+nahlU3arfVcutyFUcRnIBTcA4s0AAtcAPaoAMwmIIH8ASetUx71F6012VrQVvNHIMf0N4+AbGok7M=</latexit>˜m
E <latexit sha1_base64="JvAU3TSH2kdG57TpveGy9yZGTpU=">AAACOXicbVDLSgMxFM34rPVVdekm2Ap1YZkRX8uiG5cV7AM6pWTSO21oZjIkGbEM81tu/At3ghsXirj1B0wfgrY9EDice+4jx4s4U9q2X6yFxaXlldXMWnZ9Y3NrO7ezW1MilhSqVHAhGx5RwFkIVc00h0YkgQQeh7rXvx7W6/cgFRPhnR5E0ApIN2Q+o0QbqZ2ruB7rFhN3NCmR0EmxG3jiIVFUSMDCx4VGIU3xMf71eDyG+abhqKN2Lm+X7BHwLHEmJI8mqLRzz25H0DiAUFNOlGo6dqRbCZGaUQ5p1o0VRIT2SReahoYkANVKRrek+NAoHewLaV6o8Uj925GQQKlB4BlnQHRPTdeG4rxaM9b+ZSthYRRrCOl4kR9zrAUexog7TALVfGAIoZKZWzHtEUmoNmFnTQjO9JdnSe2k5JyXzm5P8+WrSRwZtI8OUBE56AKV0Q2qoCqi6BG9onf0YT1Zb9an9TW2LliTnj30D9b3D5n/rV4=</latexit> score of X score of X
 
   
Figure 1: An overview of LSCALE-I and GSCALE-I algorithms. For each latent variable Z ,
i
therearetwointerventionalmechanisms,denotedbyredandblue. Foreachpairofenvironments,the
scorefunctionsofobservedvariablesarefedintoGSCALE-Ialgorithm. Then,GSCALE-Iusesthis
inputtocomputelatentscoredifferencesforagivenencoderandreturnstheencoderthatminimizes
theselatentscoredifferences. Thisencoderisusedtocomputeestimates ˆandZˆ.
G
Organization. Therestofthepaperisorganizedasfollows. Section2providesanoverviewof
theliteraturewiththemainfocusonCRLfrominterventions. Section3providesthepreliminaries
forformulatingtheproblemandspecifiesthenotationsanddefinitionsusedthroughoutthepaper.
Section4summarizesourmainidentifiabilityandachievabilityresultsforbothlinearandgeneral
transformationsandinterpretstheminthecontextoftheexistingliterature. InSection5,weintroduce
ourscore-basedframeworkanddesigntheCRLalgorithms. InSection6,weprovidethedetailsof
thescore-basedmethodology,analyzeitskeysteps,andestablishconstructiveproofsoftheresultsin
Section4. Overviewsoftheproofsarepresentedinthemainbodyofthepaper,andtherestofthe
proofsarerelegatedtotheappendices. Finally,inSection7,weempiricallyassesstheperformance
oftheproposedCRLalgorithmsforrecoveringthelatentcausalvariablesandthelatentcausalgraph.
2. RelatedWork
In this paper, we address identifiability and achievability in CRL given different interventional
environmentsinwhichtheinterventionsactonthelatentspace. Wefirstprovideanoverviewofthe
literaturethatinvestigatesCRLfrominterventionaldata,withthemainresultsofthemostclosely
relatedworksummarizedinTables1and2. Then,wediscusstheotherrelevantlinesofwork.
Interventionalcausalrepresentationlearning. Themajorityoftherapidlygrowingliterature
on CRL from interventions focuses on parametric settings, i.e., a parametric form is assumed
for the latent model, the transformation, or both of them. Among related studies, Ahuja et al.
(2023a) consider polynomial transformations without restrictions on the latent causal model and
showidentifiabilityunderdeterministicdointerventions. Theyalsoproveidentifiabilityundersoft
interventionswithindependentsupportassumptionsonlatentvariables. Squiresetal.(2023)consider
a linear latent model with a linear mapping to observations and prove identifiability under hard
interventions. Theyalsoshowtheimpossibilityofperfectidentifiabilityundersoftinterventionsand
proveidentifiabilityuptoancestors. Buchholzetal.(2023)focusonlinearGaussianlatentmodels
and extend the results of Squires et al. (2023) to prove identifiability for general transformations.
Zhangetal.(2023)considerpolynomialtransformationsundernonlinearityassumptionsonlatent
5VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Table1: ParametricSettings. Comparisonoftheresultstopriorstudiesinparametricsettings. Onlythe
mainresultsofthemostcloselyrelatedstudiesarelistedandstandardsharedassumptionsareomitted. Formal
definitionsofidentifiabilitymeasuresareprovidedinSection3.
Work Transform Latent Int.Data Identifiability
Model (oneenv.pernode) result
Squiresetal.(2023) Linear Lin.Gaussian Hard perfect
Linear Lin.Gaussian Soft uptoancestors
Ahujaetal.(2023a) Polynomial General do perfect
Polynomial BoundedRV Soft perfect
Buchholzetal.(2023) General Lin.Gaussian Hard perfect
General Lin.Gaussian Soft uptoancestors
Zhangetal.(2023) Polynomial Non-linear Soft uptoancestors
Polynomial Non-linear(polytree) Soft perfect
JinandSyrgkanis(2023) Linear Non-linear Soft perfectDAGand
Linear Lin.non-Gaussian Soft mixingw.surrounding
Theorem1 Linear General Hard perfect
Theorem2 Linear General Soft uptoancestors
Theorem3 Linear Non-linear Soft perfectDAGand
mixingw.surrounding
models and prove identifiability up to ancestors under soft interventions. If the latent graph is
restricted to polytrees, they further prove perfect identifiability. Ahuja et al. (2023b) use multi-
targetdointerventionstoidentifythenon-intervenedvariablesfromtheintervenedvariables. Bing
etal.(2023)consideranon-linearlatentmodelunderlineartransformationandusemulti-targetdo
interventionstoproveidentifiabilityundercertainsparsityassumptions. JinandSyrgkanis(2023)
consideralineartransformandsufficientlynon-linearlatentmodelsunderalineartransformand
proveidentifiabilityuptosurroundingparentsusingsoftinterventions1. Theyalsoestablishsufficient
conditionsformulti-targetsoftinterventionstoensureidentifiability. Saengkyongametal.(2023)
takeadifferentapproachandconsiderthetaskofinterventionextrapolation. Intheirformulation,
interventions are upon exogenous action variables (e.g., instrumental variables) which affect the
latentvariableslinearly.
Onthefullynonparametricsetting,vonKügelgenetal.(2023)providethemostcloselyrelated
identifiabilityresultstoours. Specifically,theyshowthattwocoupled hardinterventionspernode
sufficeforidentifiabilityunderfaithfulnessassumptiononlatentcausalmodels. Ourresultshave
twomajordifferences: 1)Weaddressachievabilityviaaprovablycorrectalgorithmwhereasvon
Kügelgenetal.(2023)focusmainlyonidentifiability(e.g.,noalgorithmforrecoveryofthelatent
variables),2)wedispensewiththerestrictiveassumptionsonidentifiabilityresults,namely,wedo
notrequiretoknowwhichtwoenvironmentssharethesameinterventiontarget(hence,uncoupled
interventions),anddonotrequirefaithfulnessonthelatentmodels. Amongtheotherstudiesonthe
nonparametricsetting,JinandSyrgkanis(2023)provideanalogousresultsto(vonKügelgenetal.,
2023)byconsideringtwocoupledsoftinterventionsandidentifyinglatentvariablesuptomixing
withsurroundingvariables. JiangandAragam(2023)consideridentifyingthelatentDAGwithout
recoveringlatentvariables,whereitisshownthatarestrictedclassofDAGscanberecovered.
1.JinandSyrgkanis(2023)defineeffect-dominationambiguityintheirpaper,whichoverlapswithourdefinitionof
mixingwithsurroundingvariablesinDefinition2.
6SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Table 2: General(Non-parameteric)Settings. Comparisonoftheresultstopriorstudiesinthegeneral
setting. Formaldefinitions ofidentifiabilitymeasuresareprovidedinSection 3. Thesharedassumptions
(interventionaldiscrepancy)andadditionalassumptions(faithfulness)arediscussedinSection4.
Work Transformand Obs. Int.Data Faithfulness Identifiability Provable
LatentModel Data (env.pernode) result algorithm
vonKügelgenetal.(2023) General No 2coupledhard Yes perfect ✘
JinandSyrgkanis(2023) General No 2coupledsoft No perfectDAGand ✘
mixingw.surrounding
Theorem4 General Yes 2uncoupledhard No perfect ✔
Theorem5 General Yes 2coupledhard No perfect ✔
Theorem6 General No 2coupledhard Yes perfect ✔
Weaklysupervisedcausalrepresentationlearning. Acommonlyusedweaksupervisionsignalfor
provingidentifiabilityisassumingaccesstomulti-viewdata,i.e.,counterfactualpairsofobservations
arising from pre- and post-intervention observations (Locatello et al., 2020; von Kügelgen et al.,
2021;Brehmeretal.,2022;Ahujaetal.,2022b). Yaoetal.(2023)generalizethemulti-viewapproach
viaaunifiedframeworkwhichalsoallowspartialobservabilitywithnon-lineartransforms. Another
approach is using temporal sequences to identify latent causal representations (Lachapelle et al.,
2022;Yaoetal.,2022;Lippeetal.,2023). Finally,somestudiesusestrongersupervisionsignals
suchastheannotationsofthegroundtruthcausalvariablesorknowncausalgraph(Shenetal.,2022;
Liangetal.,2023). Amongthese,Liangetal.(2023)assumethatthelatentDAGisalreadyknown
andrecoverthelatentvariablesunderhardinterventions.
Identifiablerepresentationlearning. AsaspecialcaseofCRL,wherethelatentvariablesare
independent,thereisextensiveliteratureonidentifyinglatentrepresentations. Somerepresentative
approachesincludeleveragingtheknowledgeofthemechanismsthatgoverntheevolutionofthe
system (Ahuja et al., 2022a) and using weak supervision with auxiliary information (Shu et al.,
2019). Non-linearindependentcomponentanalysis(ICA)alsousessideinformation,intheformof
structuredtimeseriestoexploittemporalinformation(HyvärinenandMorioka,2017;Hälväand
Hyvärinen, 2020) or knowledge of auxiliary variables that renders latent variables conditionally
independent (Khemakhem et al., 2020a,b; Hyvärinen et al., 2019). On a related problem, the
identifiabilityofdeepgenerativemodelsisstudiedwithoutauxiliaryinformation(Kivvaetal.,2022).
Scorefunctionsforcausaldiscoverywithinobservedvariables. Scorematchinghasrecently
gained attraction in the causal discovery of observed variables. Rolland et al. (2022) use score
matching to recover non-linear additive Gaussian noise models. The proposed method finds the
topological order of causal variables but requires additional pruning to recover the full graph.
Montagnaetal.(2023b)focusonthesamesetting,recoverthefullgraphfromJacobianscores,and
dispense with the computationally expensive pruning stage. Montagna et al. (2023a) empirically
demonstratetherobustnessofscore-matching-basedapproachesagainsttheassumptionviolationsin
causaldiscovery. Also,Zhuetal.(2023)establishboundsontheerrorrateofscorematching-based
causaldiscoverymethods. Allofthesestudiesarelimitedtoobservedcausalvariables,whereasin
ourcase,wehaveacausalmodelinthelatentspace.
7VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
3. PreliminariesandDefinitions
Notations. Foravectora Rm,thei-theentryisdenotedbya . Matricesaredenotedbybold
i
∈
upper-caseletters,e.g.,A,whereA denotesthei-throwofAandA denotestheentryatrowiand
i i,j
columnj. FormatricesAandBwiththesameshapes,A ≼ Bdenotescomponent-wiseinequality.
We denote the indicator function by 1, and for a matrix A Rm×n, we use the convention that
∈
1 A 0,1 m×n, where the entries are specified by [1 A ] = 1 A = 0 . For a positive
i,j i,j
{ } ∈ { } { } { ̸ }
integern N,wedefine[n] ≜ 1,...,n . Thepermutationmatrixassociatedwithanypermutation
∈ { }
π of [n] is denoted by P , i.e., [π π ... π ]⊤ = P [1 2 ... n]⊤. The n-dimensional
π 1 2 n π
·
identitymatrixisdenotedbyI ,andtheHadamardproductisdenotedby . Givenafunction
n×n
⊙
f : Rr Rs thathasfirst-orderpartialderivativesonRr,wedenotetheJacobianoff atz Rs
→ ∈
by J (z). We use im(f) to denote the image of f and define rank of f as the number of linearly
f
independentvectorsinitsimageanddenoteitby (f) ≜ dim(im(f)). Accordingly, forasetof
R
functions ≜ f : i [k],f : Rr Rs wedefineim( ) ≜ k im(f ),anddenotetherank
F { i ∈ i → } F ∪i=1 i
of by ( ) ≜ dim(im( )).
F R F F
3.1 LatentCausalStructure
ConsiderlatentcausalrandomvariablesZ ≜ [Z ,...,Z ]⊤. Anunknowntransformationg : Rn
1 n
→
Rd generates the observable random variables X ≜ [X ,...,X ]⊤ from the latent variables Z
1 d
accordingto:
X = g(Z). (3)
Weassumethatd n,andtransformationgiscontinuouslydifferentiableandadiffeomorphismonto
≥
itsimage(otherwise,identifiabilityisill-posed). Wedenotetheimageofgby ≜ im(g) Rd. The
X ⊆
probabilitydensityfunctions(pdfs)ofZ andX aredenotedbypandp ,respectively. Weassume
X
thatpisabsolutelycontinuouswithrespecttothen-dimensionalLebesguemeasure. Subsequently,
p , which is defined on the image manifold im(g), is absolutely continuous with respect to the
X
n-dimensionalHausdorffmeasureratherthand-dimensionalLebesguemeasure2. Thedistribution
oflatentvariablesZ factorizeswithrespecttoaDAGthatconsistsofnnodesandisdenotedby .
G
Nodei [n]of representsZ andpfactorizesaccordingto:
i
∈ G
n
p(z) = p (z z ), (4)
i i pa(i)
|
i=1
(cid:89)
wherepa(i)denotesthesetofparentsofnodeiandp (z z )istheconditionalpdfofz given
i i pa(i) i
|
the variables of its parents. We use ch(i), an(i), and de(i) to denote the children, ancestors, and
descendantsofnodei,respectively. Accordingly,foreachnodei [n]wealsodefine
∈
pa(i) ≜ pa(i) i , ch(i) ≜ ch(i) i , an(i) ≜ an(i) i , and de(i) ≜ de(i) i . (5)
∪{ } ∪{ } ∪{ } ∪{ }
We denote the transitive closure and transitive reduction of by and , respectively3. The
tc tr
G G G
parentalrelationshipsinthesegraphsaredenotedbypa andpa ,andothergraphicalrelationships
tc tr
aredenotedsimilarly. Basedonthemodularityproperty,achangeinthecausalmechanismofnodei
2.Fordetailsofwherethishasbeenused,seeAppendixA.2
3.TransitiveclosureofaDAGG,denotedbyG ,isaDAGwithparentsdenotedbypa (i)=an(i)foreachnodei.
tr tr
ThetransitivereductionofaDAGGistheDAGwiththefewestedgesthathasthesamereachabilityrelationasG.
8SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Z<latexit sha1_base64="5uXesc7yCRnlkRqMtXEN/AviWBs=">AAAB6nicbVBNS8NAEJ3UrxqtVj16WSwFTyUpqD0WBPFY0X5gG8pmu2mXbjZhdyOU0J/gxYMiXsUf4k/w5r9x0/agrQ8GHu/NMDPPjzlT2nG+rdza+sbmVn7b3tkt7O0XDw5bKkokoU0S8Uh2fKwoZ4I2NdOcdmJJcehz2vbHl5nffqBSsUjc6UlMvRAPBQsYwdpIt/f9ar9YcirODGiVuAtSqhc+k/KV/dHoF796g4gkIRWacKxU13Vi7aVYakY4ndq9RNEYkzEe0q6hAodUeens1CkqG2WAgkiaEhrN1N8TKQ6VmoS+6QyxHqllLxP/87qJDmpeykScaCrIfFGQcKQjlP2NBkxSovnEEEwkM7ciMsISE23SsU0I7vLLq6RVrbjnlbMbk0YN5sjDMZzAKbhwAXW4hgY0gcAQHuEZXixuPVmv1tu8NWctZo7gD6z3H2UnkB4=</latexit> 2 Z<latexit sha1_base64="5uXesc7yCRnlkRqMtXEN/AviWBs=">AAAB6nicbVBNS8NAEJ3UrxqtVj16WSwFTyUpqD0WBPFY0X5gG8pmu2mXbjZhdyOU0J/gxYMiXsUf4k/w5r9x0/agrQ8GHu/NMDPPjzlT2nG+rdza+sbmVn7b3tkt7O0XDw5bKkokoU0S8Uh2fKwoZ4I2NdOcdmJJcehz2vbHl5nffqBSsUjc6UlMvRAPBQsYwdpIt/f9ar9YcirODGiVuAtSqhc+k/KV/dHoF796g4gkIRWacKxU13Vi7aVYakY4ndq9RNEYkzEe0q6hAodUeens1CkqG2WAgkiaEhrN1N8TKQ6VmoS+6QyxHqllLxP/87qJDmpeykScaCrIfFGQcKQjlP2NBkxSovnEEEwkM7ciMsISE23SsU0I7vLLq6RVrbjnlbMbk0YN5sjDMZzAKbhwAXW4hgY0gcAQHuEZXixuPVmv1tu8NWctZo7gD6z3H2UnkB4=</latexit> 2
Z<latexit sha1_base64="P8tdODjE+Ccd0sgUfyQHLZQF3qg=">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gV1BwDgniMaB6YLGF2MpsMmZldZmaFsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hwpk2nvftrKyurW9s5rbc7Z387l5h/6Ch41QRWicxj1UrxJpyJmndMMNpK1EUi5DTZji8nPjNB6o0i+WdGSU0ELgvWcQINla6ve/63ULRK3tToGXiz0mxmv9MS1fuR61b+Or0YpIKKg3hWOu27yUmyLAyjHA6djuppgkmQ9ynbUslFlQH2fTUMSpZpYeiWNmSBk3V3xMZFlqPRGg7BTYDvehNxP+8dmqiSpAxmaSGSjJbFKUcmRhN/kY9pigxfGQJJorZWxEZYIWJsem4NgR/8eVl0jgt++flsxubRgVmyMERHMMJ+HABVbiGGtSBQB8e4RleHO48Oa/O26x1xZnPHMIfOO8/Y6OQHQ==</latexit> 1 Z<latexit sha1_base64="njrnYAqehWzK+K46ngDB+0mv1pk=">AAAB6nicbVDLSsNAFL2prxqtVl26GSwFVyURH10WBHFZ0T6wDWUynbRDJ5MwMxFK6Ce4caGIW/FD/AR3/o2TtgttPXDhcM693HuPH3OmtON8W7mV1bX1jfymvbVd2Nkt7u03VZRIQhsk4pFs+1hRzgRtaKY5bceS4tDntOWPLjO/9UClYpG40+OYeiEeCBYwgrWRbu97p71iyak4U6Bl4s5JqVb4TMpX9ke9V/zq9iOShFRowrFSHdeJtZdiqRnhdGJ3E0VjTEZ4QDuGChxS5aXTUyeobJQ+CiJpSmg0VX9PpDhUahz6pjPEeqgWvUz8z+skOqh6KRNxoqkgs0VBwpGOUPY36jNJieZjQzCRzNyKyBBLTLRJxzYhuIsvL5PmScU9r5zdmDSqMEMeDuEIjsGFC6jBNdShAQQG8AjP8GJx68l6td5mrTlrPnMAf2C9/wBoL5Ag</latexit> 4 Z<latexit sha1_base64="P8tdODjE+Ccd0sgUfyQHLZQF3qg=">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gV1BwDgniMaB6YLGF2MpsMmZldZmaFsOQTvHhQxKv4IX6CN//GyeOgiQUNRVU33V1hwpk2nvftrKyurW9s5rbc7Z387l5h/6Ch41QRWicxj1UrxJpyJmndMMNpK1EUi5DTZji8nPjNB6o0i+WdGSU0ELgvWcQINla6ve/63ULRK3tToGXiz0mxmv9MS1fuR61b+Or0YpIKKg3hWOu27yUmyLAyjHA6djuppgkmQ9ynbUslFlQH2fTUMSpZpYeiWNmSBk3V3xMZFlqPRGg7BTYDvehNxP+8dmqiSpAxmaSGSjJbFKUcmRhN/kY9pigxfGQJJorZWxEZYIWJsem4NgR/8eVl0jgt++flsxubRgVmyMERHMMJ+HABVbiGGtSBQB8e4RleHO48Oa/O26x1xZnPHMIfOO8/Y6OQHQ==</latexit> 1
Z<latexit sha1_base64="Czw8gBdCbzBM614AatqUZ+FRK0U=">AAAB6nicbVBNS8NAEJ3UrxqtVj16WSwFTyVR1B4LgnisaD+wDWWz3bRLN5uwuxFK6E/w4kERr+IP8Sd489+4aXvQ1gcDj/dmmJnnx5wp7TjfVm5ldW19I79pb20XdnaLe/tNFSWS0AaJeCTbPlaUM0EbmmlO27GkOPQ5bfmjy8xvPVCpWCTu9DimXogHggWMYG2k2/veaa9YcirOFGiZuHNSqhU+k/KV/VHvFb+6/YgkIRWacKxUx3Vi7aVYakY4ndjdRNEYkxEe0I6hAodUeen01AkqG6WPgkiaEhpN1d8TKQ6VGoe+6QyxHqpFLxP/8zqJDqpeykScaCrIbFGQcKQjlP2N+kxSovnYEEwkM7ciMsQSE23SsU0I7uLLy6R5UnHPK2c3Jo0qzJCHQziCY3DhAmpwDXVoAIEBPMIzvFjcerJerbdZa86azxzAH1jvP2arkB8=</latexit> Z<latexit sha1_base64="Czw8gBdCbzBM614AatqUZ+FRK0U=">AAAB6nicbVBNS8NAEJ3UrxqtVj16WSwFTyVR1B4LgnisaD+wDWWz3bRLN5uwuxFK6E/w4kERr+IP8Sd489+4aXvQ1gcDj/dmmJnnx5wp7TjfVm5ldW19I79pb20XdnaLe/tNFSWS0AaJeCTbPlaUM0EbmmlO27GkOPQ5bfmjy8xvPVCpWCTu9DimXogHggWMYG2k2/veaa9YcirOFGiZuHNSqhU+k/KV/VHvFb+6/YgkIRWacKxUx3Vi7aVYakY4ndjdRNEYkxEe0I6hAodUeen01AkqG6WPgkiaEhpN1d8TKQ6VGoe+6QyxHqpFLxP/8zqJDqpeykScaCrIbFGQcKQjlP2N+kxSovnYEEwkM7ciMsQSE23SsU0I7uLLy6R5UnHPK2c3Jo0qzJCHQziCY3DhAmpwDXVoAIEBPMIzvFjcerJerbdZa86azxzAH1jvP2arkB8=</latexit>
3 3
Figure2: SamplelatentDAGs . (Left)Thevalidcausalordersare(1,2,3,4)and(1,3,2,4),and
G
onlysurroundednodeis4withsur(4) = 2,3 . (Right)Theonlyvalidcausalorderis(1,2,3),and
{ }
thesurroundednodesare = 2,3 withsur(2) = 1 ,sur(3) = 1,2 .
S { } { } { }
doesnotaffectthoseoftheothernodes. Wealsoassumethatallconditionalpdfs p (z z ) :
i i pa(i)
{ |
i [n] arecontinuouslydifferentiablewithrespecttoallz variablesandp(z) = 0forallz Rn.
∈ } ̸ ∈
Weconsiderthegeneralstructuralcausalmodels(SCMs)basedonwhichforeachi [n],
∈
Z = f (Z ,N ), (6)
i i pa(i) i
where f : i [n] aregeneralfunctionsthatcapturethedependenceofnodeionitsparentsand
i
{ ∈ }
N : i [n] accountfortheexogenousnoisetermsthatweassumetohavepdfswithfullsupport.
i
{ ∈ }
WespecializesomeoftheresultstoadditivenoiseSCMs,inwhich(6)becomes
Z = f (Z )+N . (7)
i i pa(i) i
Next, we provide a number of definitions that we will use frequently throughout the paper for
formalizingtheframeworkandanalyzingit.
Definition1(ValidCausalOrder) Werefertoapermutation(π ,...,π )of[n]asavalidcausal
1 n
order4 ifπ pa(π )indicatesthati < j.
i j
∈
Inthispaper,withoutlossofgenerality,weassumethat(1,...,n)isavalidcausalorder. Wealso
defineagraphicalnotionthatwillbeusefulforpresentingourresultsandanalysisonCRLundera
lineartransformation.
Definition2(SurroundedNode) Node i [n] in DAG is said to be surrounded if there exists
∈ G
anothernodej [n]suchthatch(i) ch(j). Wedenotethesetofnodesthatsurroundi [n]by
∈ ⊆ ∈
sur(i),andthesetofallnodesthataresurroundedby ,i.e.,
S
sur(i) ≜ j [n] : j = i , ch(i) ch(j) , and ≜ i [n] : sur(i) = . (8)
{ ∈ ̸ ⊆ } S { ∈ ̸ ∅}
3.2 ScoreFunctions
Thescorefunctionassociatedwithapdfisdefinedasthegradientofitslogarithm. Thescorefunction
associatedwithpisdenotedby
s(z) ≜ logp(z). (9)
z
∇
NotingtheconnectionX = g(Z), thedensityofX under 0, denotedbyp , issupportedonan
X
E
n-dimensional manifold embedded in Rd. Hence, specifying the score function of X requires
X
4.Itisalsocalledtopologicalorderingortopologicalsortintheliterature.
9VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
notions from differential geometry. For this purpose, we denote the tangent space of manifold
at point x by T . Tangent vectors v T are equivalence classes of continuously
x x
X ∈ X X ∈ X
differentiable curves γ: ( 1,1) Rd with γ(0) = x and γ′(0) = v. Furthermore, given a
− → X ⊆
functionf: R,denoteitsdirectionalderivativeatpointx alongatangentvectorv T
x
X → ∈ X ∈ X
byD f(x),whichisdefinedas
v
d
D f(x) ≜ (f γ)(t) , (10)
v
dt ◦
(cid:12)t=0
(cid:12)
(cid:12)
foranycurveγ inequivalenceclassv. Thedifferentialoff(cid:12) atpointx ,denotedbydf x,isthe
∈ X
linearoperatormappingtangentvectorv T toD f(x)(Simon,2014,p.57),i.e.,
x v
∈ X
df : T v D f(x) R. (11)
x x v
X ∋ (cid:55)→ ∈
LetB Rd×n beamatrixforwhichthecolumnsofBformanorthonormalbasisforT . Denote
x
∈ X
thedirectionalderivativeoff alongthei-thcolumnofBbyD f foralli [n]. Then,thedifferential
i
∈
operatorcanbeexpressedbythevector
Df(x) ≜ B D f(x) ... D f(x) ⊤ Rd , (12)
1 n
· ∈
(cid:2) (cid:3)
suchthat
df (v) = v⊤ Df(x), x , v T . (13)
x x
· ∀ ∈ X ∀ ∈ X
Notethatthedifferentialoperatordf isageneralizationofthegradient. Hence,wecangeneralize
thedefinitionofthescorefunctionusingthedifferentialoperatorbysettingf tothelogarithmofpdf.
Therefore,thescorefunctionofX under 0 isspecifiedasfollows:
E
s (x) ≜ Dlogp (x), x . (14)
X X
∀ ∈ X
3.3 InterventionMechanisms
Weconsidertwotypesofinterventions. Asoftinterventiononnodei(alsoreferredtoasimperfect
interventioninliterature),changestheconditionaldistributionp (z z )toadistinctconditional
i i pa(i)
|
distribution, which we denote by q (z z ). A soft intervention does not necessarily remove
i i pa(i)
|
the functional dependence of an intervened node on its parents and rather alters it to a different
mechanism. A stochastic hard intervention on node i (also referred to as perfect intervention) is
stricterthanasoftinterventionandremovestheedgesincidentoni. Ahardinterventiononnode
changesp (z z )toq (z )thatemphasizesthelackofdependenceofz onz . Finally,we
i i pa(i) i i i pa(i)
|
notethatinsomesettings,weassumetwohardinterventionspernode,inwhichcasethetwohard
interventionalmechanismsfornodeiaredenotedbytwodistinctpdfsq (z )andq˜(z ).
i i i i
Interventional environments. We consider atomic interventional environments in which each
environmentonenodeisintervenedin,asitiscustomarytothecloselyrelatedCRLliterature(Squires
et al., 2023; Ahuja et al., 2023a; Buchholz et al., 2023). In some settings (linear transformation),
wewillhaveoneinterventionalenvironmentpernodeanddenotetheinterventionalenvironments
by ≜ 1,..., n , where we call the atomic environment set. We denote the node in-
E {E E } E
tervened in environment m by Im [n]. For other settings (general transformation), we will
E ∈
havetwointerventionalenvironmentspernodeanddenotethesecondatomicenvironmentsetby
10SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
˜= ˜1,..., ˜n . Similarly,wedenotetheintervenednodein ˜m byI˜m foreachm [n]. Weas-
E {E E } E ∈
sumethatnode-environmentpairsareunspecified,i.e.,theorderedinterventionsets ≜ (I1,...,In)
I
and ˜ ≜ (I˜1,...,I˜n)aretwounknownpermutationsof[n]. Wealsoadopttheconventionthat 0
I E
istheobservationalenvironmentandI0 ≜ . Next,wedefinethenotionofcouplingbetweenthe
∅
environmentsets and ˜.
E E
Definition3(Coupled/UncoupledEnvironments) Thetwoenvironmentsets and ˜aresaidto
E E
be coupled if for the unknown permutations and ˜ we know that = ˜, i.e., the same node is
I I I I
intervenedinenvironments i and ˜i. Thetwoenvironmentsetsaresaidtobeuncoupledif ˜ isan
E E I
unknownpermutationof .
I
Next,wedefinepm asthepdfofZ inenvironment m. Hence,undersoftandhardinterventionfor
E
eachm [n],pm canbefactorizedasfollows.
∈
softinterventionin m : pm(z) = q (z z ) p (z z ), where ℓ = Im , (15)
ℓ ℓ pa(ℓ) i i pa(i)
E | |
i̸=ℓ
(cid:89)
hardinterventionin m : pm(z) = q (z ) p (z z ), where ℓ = Im . (16)
ℓ ℓ i i pa(i)
E |
i̸=ℓ
(cid:89)
Similarly, we define p˜m as the pdf of Z in ˜m, which can be factorized similarly to (16) with q˜
ℓ
E
replacedwithq . Hence,thescorefunctionsassociatedwithpm andp˜m arespecifiedasfollows.
ℓ
sm(z) ≜ logpm(z), and s˜m(z) ≜ logp˜m(z). (17)
z z
∇ ∇
We denote the score functions of the observed variables X under m and ˜m by sm and s˜m,
E E X X
respectively. Notethatthescorefunctionschangeacrossdifferentenvironments,whichisinduced
by the changes in the distribution of Z. We use Zm and Xm to denote the latent variables Z
and observed variables X in environment m, respectively. In Section 6.1, we investigate score
E
discrepanciesbetweensandsm (ors˜m)andcharacterizetherelationshipbetweenthescoresinthe
observationalandinterventionalenvironments.
3.4 IdentifiabilityandAchievabilityObjectives
TheobjectiveofCRListouseobservationsX generatedbytheobservationalandinterventional
environments and estimate the true latent variables Z and causal relations among them captured
by . Thefirstobjectiveisidentifiability,whichpertainstodeterminingalgorithm-agnosticsufficient
G
conditionsunderwhichZ and canberecovereduniquelyuptoapermutationandelement-wise
G
transform which is the strongest form of recovery in CRL from interventions as shown in (von
Kügelgenetal.,2023). Thesecondobjectiveisachievability,whichreferstodesigningalgorithms
thatareamenabletopracticalimplementationandgenerateprovablycorrectestimatesforZ and ,
G
foreseenbytheidentifiabilityguarantees. Inthissubsection,weprovidethedefinitionsneededfor
formalizingidentifiabilityandachievabilityobjectives.
WedenoteagenericestimatorofZ givenX byZˆ(X) : Rd Rn. Wealsoconsiderageneric
→
estimateof denotedby ˆ. InordertoassessthefidelityoftheestimatesZˆ(X)and ˆwithrespectto
G G G
thegroundtruthZ and ,weprovidethefollowingidentifiabilitymeasures. Westartwithspecifying
G
perfectidentifiability,whichisrelevantforassessingtheidentifiabilityandachievabilityresultsunder
hardinterventions.
11VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Definition4(PerfectIdentifiability) ToformalizeperfectidentifiabilityinCRLwedefine:
1. PerfectDAGrecovery: DAGrecoveryissaidtobeperfectif ˆisisomorphicto .
G G
2. Perfectlatentrecovery: GiventheestimatorZˆ(X),latentrecoveryissaidtobeperfectifZˆ(X)
is an element-wise diffeomorphism of a permutation of Z, i.e., there exists a permutation π
of[n]andasetoffunctions ϕ : i [n] suchthatϕ : R Randwehave
i i
{ ∈ } →
Zˆ(X) = P ϕ(Z), Z Rn , (18)
π
· ∀ ∈
whereϕ(Z) ≜ (ϕ (Z ),...,ϕ (Z ).
1 1 n n
3. Scalingconsistency: TheestimatorZˆ(X)issaidtomaintainscalingconsistencyifthereexists
apermutationπ of[n]andaconstantdiagonalmatrixC Rn×n suchthat
s
∈
Zˆ(X) = P C Z , Z Rn . (19)
π s
· · ∀ ∈
Wenotethatscalingconsistencyisaspecialcaseofperfectlatentrecoveryinwhichthediffeomor-
phismintheperfectlatentrecoveryisrestrictedtoanelement-wisescaling. Next,weprovidepartial
identifiability measures which will be useful to assess the identifiability and achievability results
undersoftinterventions.
Definition5(PartialIdentifiability) ToformalizepartialidentifiabilityinCRLwedefine:
1. Transitiveclosurerecovery: DAGrecoveryissaidtomaintaintransitiveclosureif ˆand
G G
havethesameancestralrelationships,i.e., ˆ isisomorphicto .
tr tr
G G
2. Mixingconsistencyuptoancestors: TheestimatorZˆ(X)issaidtomaintainmixingconsis-
tencyuptoancestorsifthereexistsapermutationπ of[n]andaconstantmatrixC Rn×n
an
∈
suchthat
Zˆ(X) = P C Z , Z Rn , (20)
π an
· · ∀ ∈
whereC hasnon-zerodiagonalentriesandforallj / an(i),C satisfies[C ] = 0.
an an an i,j
∈
3. Mixing consistency up to surrounding parents: The estimator Zˆ(X) is said to maintain
mixing consistency up to surrounding parents if there exists a permutation π of [n] and a
constantmatrixC Rn×n suchthat
sur
∈
Zˆ(X) = P C Z , Z Rn , (21)
π sur
· · ∀ ∈
where C has non-zero diagonal entries and for all i = j and j / sur(i), C satisfies
sur sur
̸ ∈
[C ] = 0.
sur i,j
3.5 Algorithm-relatedDefinitions
For formalizing the achievability results and designing the associated algorithms, generating the
estimates Zˆ(X) and ˆis facilitated by estimating the inverse of g based on the observed data X.
G
Specifically, an estimate of g−1, where g−1 denotes the inverse of g, facilitates recovering Z via
Z = g−1(X). Throughouttherestofthispaper,werefertog−1 asthetrueencoder. Toformalize
theproceduresofestimatingg−1,wedefine asthesetofpossiblevalidencoders,i.e.,candidates
H
12SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
forg−1. Afunctionh canbesuchacandidateifitisinvertible,thatis,thereexistsanassociated
∈ H
decoderh−1 suchthat(h−1 h)(X) = X. Hence,thesetofvalidencodersisspecifiedby
◦
≜ h : Rn : h−1 : Rn Rd suchthat (h−1 h)(X) = X , X . (22)
H { X → ∃ → ◦ ∀ ∈ X}
Next,correspondingtoanypairofobservationX andvalidencoderh ,wedefineZˆ(X;h)as
∈ H
anauxiliaryestimateofZ generatedbyapplyingthevalidencoderhonX,i.e.,
Zˆ(X;h) ≜ h(X) = (h g)(Z), h , X . (23)
◦ ∀ ∈ H ∈ X
The estimate Zˆ(X;h) inherits its randomness from X, and its statistical model is governed by
thatofX andthechoiceofh. Toemphasizethedependenceonh,wedenotethescorefunctions
associatedwiththepdfsofZˆ(X;h)underenvironments 0, m,and ˜m,respectively,by
E E E
s ( ;h), sm( ;h), and s˜m( ;h). (24)
Zˆ · Zˆ · Zˆ ·
Wewillbeaddressingbothgeneralandlineartransformationsg. Inthelineartransformationsetting,
thetruelineartransformationg isdenotedbymatrixG Rd×n. Accordingly, wedenoteavalid
∈
linearencoderbyH Rn×d. ForagivenvalidencoderH,theassociatedvaliddecoderisgivenby
∈
itsMoore-Penroseinverse,i.e.,H† ≜ H⊤ (H H⊤)−1.
· ·
4. IdentifiabilityandAchievabilityResults
Inthissection,wepresentthemainidentifiabilityandachievabilityresultsundervarioussettings.
Weprovideconstructiveproofsfortheseresults,whichserveastheresultsforbothidentifiability
andachievabilityaspects. TheconstructiveproofsarebasedontheCRLalgorithms,thespecificsof
which, theirproperties, andtheir optimality guaranteesare presentedin Section5. Theproofs of
theresultspresentedinthissectionareprovidedinAppendicesBandC.Ourresultsarecategorized
into two main settings based on the models of transformation from latent to observed variables
(i.e., function g): (i) linear transformation, and (ii) general (nonparametric) transformation. The
main difference between the assumptions made for both settings is the number of interventional
environments per node. Specifically, we investigate CRL under linear transformation with one
intervention per node and we address both soft and hard interventions. For CRL under a general
transformation, we consider two hard interventions per node, and we discuss both coupled and
uncoupledinterventions. Inparalleltoourscore-basedframeworkinitiallypresentedin(Varıcıetal.,
2023), there have been advances in identifiability results. We will also discuss the relevance and
distinctionsofourresultsvis-á-vistheresultsintheexistingliterature.
4.1 CRLunderLinearTransformations
Underalineartransformation,thegeneraltransformationmodelin(3)becomes:
X = G Z , (25)
·
where G Rd×n is an unknown full-rank matrix mapping the latent variables to the observed
∈
ones. We recall that the main purpose of using interventions for CRL is that interventions inject
properstatisticalvariationsintotheobserveddata. Subsequently,ourapproachtoCRLisbuilton
tracingsuchstatisticalvariationsviatrackingthevariationsofthescorefunctionsassociatedwiththe
13VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
interventionaldata. Wewilluseoneinterventionpernodeandthesetofinterventionalenvironments
is = 1,..., n . Wewillshowthatthekeysteptorecoveringthetruelatentrepresentationsis
E {E E }
identifyingthevalidencodersthatrenderminimalvariationsinthescores. Toensurethattheeffect
ofaninterventiononaparentofthetargetvariableisdifferentfromitseffectonthetargetvariable
itself,weadoptthefollowingassumptionforthecaseoflineartransformations. Thisassumption
holdsforawiderangeofmodelsandisdiscussedinmoredetailinSection4.3.
Assumption1 Foranyenvironment m ,andforallk pa(Im),wehave
E ∈ E ∈
[s sm]
k
− = 2, where ℓ = I . (26)
m
R (cid:32)(cid:34)[s sm] ℓ(cid:35)(cid:33)
−
Based on this assumption, our first result for the linear transformation setting establishes that
scalingconsistencyandperfectDAGrecoveryispossiblebyusingonehardinterventionpernode.
Furthermore,thescore-basedalgorithmsguaranteeachievingtheseperfectrecoveryobjectives.
Theorem1(Linear–OneHardIntervention) Under Assumption 1 for linear transformations,
usingobservationaldataandinterventionaldatafromonehardinterventionpernodesufficeto
(i) Identifiability: perfectlyrecoverthelatentDAG ;
G
(ii) Identifiability: ensurethescalingconsistencyofthelatentvariables;
(iii) Achievability: achievetheabovetwoguarantees(viaAlgorithm1).
Proof: SeeAppendixB.7.
WenotethattheexistingliteratureonCRLwithlineartransformationsandonestochastichard
interventionrestrictsthelatentcausalmodeltolinearGaussianmodels(Squiresetal.,2023;Buchholz
etal.,2023). Incontrast,Theorem1doesnotimposeanyrestrictiononthelatentcausalmodeland
showsthatonestochastichardinterventionpernodeissufficientfortheidentifiabilityofgeneral
latentcausalmodels.
Next,weextendtheresultstosoftinterventions. Themajorconsequenceofapplyinghardversus
softinterventionsisthatthevariationsinthelatentdistributionscausedbyhardinterventionsare,in
general,strongerthanthosecausedbysoftinterventions. Thereasonisthattheinterventiontargetis
notnecessarilyisolatedfromitsparentsunderasoftintervention. Consequently,theidentifiability
guaranteesforsoftinterventionsareexpectedtobeweaker. Inthenexttheorem,wepresentpartial
identifiabilityresults,definedinDefinition5,forsoftinterventionsunderthegenerallatentcausal
models.
Theorem2(Linear–OneSoftIntervention) UnderAssumption1forlineartransformations,us-
ingobservationaldataandinterventionaldatafromonesoftinterventionpernodesufficeto
(i) Identifiability: recoverthetransitiveclosureofthelatentDAG ;
G
(ii) Identifiability: ensurethemixingconsistencyofthelatentvariablesuptoancestors;
(iii) Achievability: achievetheabovetwoguarantees(viaAlgorithm1).
Proof: SeeAppendixB.7.
Similar to the restrictions in the existing results for hard interventions, the transitive closure
recovery results in the existing literature require the latent causal model to be either linear Gaus-
sian (Squires et al., 2023; Buchholz et al., 2023) or satisfy non-linearity conditions (Zhang et al.,
14SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
2023). In contrast, Theorem 2 achieves transitive closure recovery and mixing consistency up to
ancestorswithoutimposinganyrestrictionsonthelatentcausalmodel.
Forlineartransformations,finally,weinvestigatetheconditionsunderwhichsoftinterventions
are guaranteed to achieve identifiability results stronger than transitive closure and mixing up to
ancestors. In particular, we specify one condition on the rank of the score function differences
(s sm),formalizednext.
−
Assumption2(Full-rankScoreDifference) Forallinterventionalenvironments m wehave
E ∈ E
(s sm) = pa(Im) . (27)
R − | |
For insight into this assumption, it can be readily verified that for linear Gaussian latent models
(s sm) 2andontheotherhand,forsufficientlynon-linearcausalmodels, (s sm)ispa(Im).
R − ≤ R −
ThisassumptionisstrongerthanAssumption1sinceitimpliesthattheeffectsofaninterventionon
allparentsofthetargetvariablearedifferent. Wewillprovidemorediscussionsonthisassumptionin
Section4.3. ThenexttheoremtightenstheidentifiabilityandachievabilityguaranteesofTheorem2
underAssumption2.
Theorem3(Linear–OneSoftIntervention) UnderAssumption2forlineartransformations,us-
ingobservationaldataandinterventionaldatafromonesoftinterventionpernodesufficeto
(i) Identifiability: perfectlyrecoverthelatentDAG ;
G
(ii) Identifiability: ensurethemixingconsistencyofthelatentvariablesuptosurroundingparents;
(iii) Achievability: achievetheabovetwoguarantees(viaAlgorithm1).
Proof: SeeAppendixB.7.
Theorem3hastwoimportantimplications. First,thelatentDAGcanbeidentifiedusingonlysoft
interventionsundermildnon-linearityassumptionsonthelatentcausalmodel. Toourknowledge,
thisisthefirstresultintheliteratureforfullyrecoveringlatentDAGwithsoftinterventionswithout
restrictingthegraphicalstructure,e.g.,Zhangetal.(2023)requirelinearfaithfulnessassumptionto
achievesimilarresults,whichisonlyshowntoholdfornon-linearlatentmodelswithpolytreestruc-
ture. Secondly,theestimatedlatentvariablesrevealthetrueconditionalindependencerelationships
sincetheysatisfytheMarkovpropertywithrespecttotheestimatedlatentDAG,whichisisomorphic
tothetrueDAG.RecallingthatthemotivationofCRLislearningusefulrepresentationsthatpreserve
causalrelationships,ourresultshowsthatitcanbeachievedwithoutperfectidentifiabilityforalarge
classofmodels.
4.2 CRLunderGeneralTransformations
Inthissetting,weconsidergeneraltransformationswithoutanyparametricassumptionfortransfor-
mationg. Weusetwointerventionalenvironmentspernode,whichareexpectedtoprovidemore
informationcomparedtooneinterventionpernodesettingforlineartransformations. Thesetsof
interventionsare = 1,..., n and ˜= ˜1,..., ˜n . Forbeinginformative,weassumethat
E {E E } E {E E }
thetwointerventionmechanismspernodearesufficientlydistinct. Thisisformalizedbydefining
interventionaldiscrepancy(Liangetal.,2023)amongthecausalmechanismsofalatentvariable.
15VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Definition6(InterventionalDiscrepancy) Twointerventionmechanismswithpdfsp,q : R R
→
aresaidtosatisfyinterventionaldiscrepancyif
∂ p(u)
= 0, u R , (28)
∂uq(u) ̸ ∀ ∈ \T
where isanullset(i.e.,hasazeroLebesguemeasure).
T
Thisassumptionensuresthatthetwodistributionsaresufficientlydifferent. AsshownbyLiangetal.
(2023),evenwhenthelatentgraph isknown,foridentifiabilityviaoneinterventionpernode,itis
G
necessarytohaveinterventionaldiscrepancybetweenobservationaldistributionp andinterventional
i
distributionq ,forallz R|pa(i)|.
i pa(i)
∈
Our main result for general transformations establishes that perfect identifiability is possible
giventwoatomicenvironmentsets,evenwhentheenvironmentscorrespondingtothesamenodeare
notspecifiedinpairs. Thatis, notonlyisitunknownwhatnodeisintervenedinanenvironment,
additionallythelearneralsodoesnotknowwhichtwoenvironmentsinterveneonthesamenode.
Theorem4(General–UncoupledEnvironments) Using observational data and interventional
datafromtwouncoupledhardenvironmentsforwhicheachpairin p ,q ,q˜ satisfiesinterventional
i i i
{ }
discrepancy,sufficesto
(i) Identifiability: perfectlyrecoverthelatentDAG ;
G
(ii) Identifiability: perfectlyrecoverthelatentvariables;
(iii) Achievability: achievetheabovetwoguarantees(viaAlgorithm5).
Proof: SeeAppendixC.5.
Theorem 4 shows that using observational data enables us to resolve any mismatch between
theuncoupledenvironmentsetsandshowsidentifiabilityinthesettingofuncoupledenvironments.
This generalizes the identifiability result of von Kügelgen et al. (2023), which requires coupled
environments. Importantly, Theorem 4 does not require faithfulness whereas the study in (von
Kügelgen et al., 2023) requires that the estimated latent distribution is faithful to the associated
candidate graph for all h . Even though a faithfulness assumption does not compromise the
∈ H
identifiabilityresult,itisastrongrequirementtoverifyandposeschallengestodevisingrecovery
algorithms. Incontrast,weonlyrequireobservationaldata,whichisgenerallyaccessibleinpractice.
Basedonthis, wedesignascore-basedalgorithm, whichispresentedanddiscussedinSection5.
Next,iftheenvironmentsarecoupled,weproveperfectlatentrecoveryunderweakerassumptions
ontheinterventionaldiscrepancy.
Theorem5(General–CoupledEnvironments) Usingobservationaldataandinterventionaldata
fromtwouncoupledhardenvironmentsforwhichthepair(p ,q )satisfiesinterventionaldiscrepancy
i i
foralli [n],sufficesto
∈
(i) Identifiability: perfectlyrecoverthelatentDAG ;
G
(ii) Identifiability: perfectlyrecoverthelatentvariables;
(iii) Achievability: achievetheabovetwoguarantees(viaAlgorithm5).
Proof: SeeAppendixC.1.
IntheproofofTheorem5,weshowthattheadvantageofenvironmentcouplingisthatitrenders
interventional data sufficient for perfect latent recovery, and the observational data is only used
16SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
for recovering the graph. We further tighten this result by showing that for DAG recovery, the
observationaldatabecomesunnecessarywhenwehaveadditivenoisemodelsandaweakfaithfulness
conditionholds.
Theorem6(NoObservationalData) Using interventional data from two coupled hard environ-
mentsforwhichthepair(q ,q˜)satisfiesinterventionaldiscrepancyforalli [n],sufficesto
i i
∈
(i) Identifiability: perfectlyrecoverthelatentDAG ifthelatentcausalmodelhasadditivenoise,
G
p(Z)istwicedifferentiable,anditsatisfiestheadjacency-faithfulness5;
(ii) Identifiability: perfectlyrecoverthelatentvariables
(iii) Achievability: achievetheabovetwoguarantees.
Proof: SeeAppendixC.2.
Finally,weremarkthatthemainresultsinthissubsection(i.e.,Theorem4and5)holdforany
latentcausalmodel. TheadditivenoisemodelassumptionisrequiredonlyfortheDAGrecoverypart
ofTheorem6.
4.3 DiscussiononAssumptions1and2
Inthissubsection,weelaborateonAssumptions1and 2,whicharerelevanttoTheorems2and3,
respectively. Assumption1essentiallystatesthatscorechangesinthecoordinatesoftheintervened
node and a parent of the intervened node are linearly independent. This property holds for (but
is not limited to) the widely adopted additive noise models specified in (7) when we apply hard
interventions. Thispropertyisformalizedinthenextlemma.
Lemma1 Assumption1issatisfiedforadditivenoisemodelsunderhardinterventions.
Proof: SeeAppendixD.1.
When soft interventions are applied, we note that even partial identifiability is shown to be
impossiblewithoutmakingassumptionsabouttheeffectoftheinterventions. Specifically,forlinear
latentcausalmodels,Buchholzetal.(2023)proveimpossibilityresultsforpureshiftinterventions
andSquiresetal.(2023)showthatagenericityconditionisnecessaryforidentifyingthetransitive
closure of the latent DAG. Therefore, Assumption 1 can be interpreted as the counterpart of the
commonlyadoptedassumptionsintheliteratureonsoftinterventionsadaptedtothesettingofgeneral
latentcausalmodels.
Given the known result that perfect identifiability is impossible for linear Gaussian models
given soft interventions, the purpose of Assumption 2 is to get more insight into the extent of
identifiabilityguaranteesundersoftinterventions. Intuitively,thementionedimpossibilityresults
forlinearGaussianmodelsareduetotherankdeficiencyofscoredifferencesforlinearmodels–
specifically,weknowthat (s sm) 2forlinearGaussianmodels. Incontrast,forsufficiently
R − ≤
non-linear causal models, (s sm) can be as high as pa(Im). Assumption 2 ensures that this
R −
upperboundissatisfiedwithequalityforallnodes. Thisconditionholdsfortheclassofsufficiently
non-linearmodels,suchasquadraticcausalmodels. Inparticular,weshowthatthisconditionholds
for the two-layer neural networks (NNs) as a function class that can effectively approximate any
continuousfunction. Thisresultisformalizedinthenextlemma.
5.Adjacency-faithfulnessisaweakerversionofthefaithfulnessassumption(Ramseyetal.,2012). Itrequiresthatif
(cid:8) (cid:9)
nodesiandjareadjacentinG,thenZ andZ aredependentconditionalonanysubsetof Z :ℓ∈/ {i,j} .
i j ℓ
17VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Lemma2 Consider the additive model in (7) where f is a two-layer NN with sigmoid activa-
i
tionfunction,andweightmatricesWi andW¯ i forobservationalandinterventionalmechanisms,
respectively. Ifmax rank(Wi), rank(W¯ i) = pa(i) foralli [n],thenAssumption2holds.
{ } | | ∈
Wediscussthenon-linearityandtheproofofLemma2inAppendixD.2.
5. Score-basedAlgorithmsforCRL
Thissectionservesatwo-foldpurpose. First,itprovidestheconstructiveproofandthestepsinvolved
initforestablishingtheidentifiabilityresultsfor(i)lineartransformations(Theorems1and2)and
(ii)generaltransformations(Theorems4and5). Secondly,itprovidesachievabilityviadesigning
algorithmsthathaveprovableguaranteesfortheperfectrecoveryofthelatentvariablesandlatent
DAGfor(i)lineartransformations,inwhichperfectrecoveryoflatentvariablesimprovedtoscaling
consistency, and (ii) any general class of functions (linear and non-linear). These algorithms are
referredtoasLinearScore-basedCausalLatentEstimationviaInterventions(LSCALE-I)forthe
linearsetting,andGeneralizedScore-basedCausalLatentEstimationviaInterventions(GSCALE-
I) for the general setting. Various properties of these algorithms and the attendant achievability
guaranteeanalysesarepresentedinSection6.
Thekeyideaofthescore-basedframeworkisthattracingthechangesinthescorefunctionsof
thelatentvariablesguidesfindingreliableestimatesfortheinverseoftransformationg, whichin
turnfacilitatesestimatingZ. However,thescoresofthelatentvariablesarenotdirectlyaccessible.
Tocircumventthis,weestablishaconnectionbetweenthescorefunctionsofthelatentvariablesand
thoseoftheobservedvariablesX,whichareaccessible. Basedonthis,wefirstcomputethescore
functionsoftheaccessibleobservedvariablesX,andthenleveragetheestablishedconnectionto
determinetheneededscorefunctionsofthelatentvariables. Finally,wenotethatweareinterested
only in the changes in the score functions. Hence, we do not directly require the score functions
themselvesbutratherneedtheirdifferences. Lemma4inSection6establishesthatthechangesin
thescorefunctionsofthelatentvariablescanbetracedfromthechangesinthescorefunctionsofthe
observedvariables. Givenanyvalidencoderh,basedon(23),theestimatedlatentvariableZˆ(X;h)
andX arerelatedthroughZˆ(X;h) = h(X). Weusethisrelationshiptocharacterizetheconnection
betweenthescoredifferencesasfollows,whichisformalizedinLemma4inSection6.
between 0 and m : s (zˆ;h) sm(zˆ;h) = J (zˆ) ⊤ s (x) sm(x) , (29)
E E Zˆ − Zˆ h−1 · X − X
between 0 and ˜m : s (zˆ;h) s˜m(zˆ;h) = (cid:2)J (zˆ)(cid:3)⊤ (cid:2)s (x) s˜m(x)(cid:3), (30)
E E Zˆ − Zˆ h−1 · X − X
between m and ˜m : sm(zˆ;h) s˜m(zˆ;h) =(cid:2)J (zˆ)(cid:3)⊤ (cid:2)sm(x) s˜m(x)(cid:3) . (31)
E E Zˆ − Zˆ h−1 · X − X
Next,weprovidethealgorithmsforlinearandgeneraltran(cid:2)sformati(cid:3)ons(cid:2)ettings. (cid:3)
5.1 LSCALE-IAlgorithmforLinearTransformations
WeprovidethedetailsofthealgorithmLinearScore-basedCausalLatentEstimationviaInterventions
(LSCALE-I).ThealgorithmissummarizedinAlgorithm1andthestepsinvolvedaredescribednext.
Inputs: The inputs of LSCALE-I are the observed data from the observational environment, the
datafromoneinterventionalenvironmentpernode,andim(G). Wecanestimateim(G)correctly
fromnrandomsamplesofX usingsingularvaluedecompositionwithprobability1.
18SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Algorithm1LinearScore-basedCausalLatentEstimationviaInterventions(LSCALE-I)
1: Input: SamplesofX fromenvironment 0 andenvironmentset
E E
2: StepL1: Computescoredifferences: (s X −sm X)forallm
∈
[n]
3: StepL2: ApplyAlgorithm2: obtainacausalorderπ andinitialestimateforencoderH
4: StepL3: ApplyAlgorithm3: Minimizescorevariationstoobtainthetransitivereductionofthe
graphandupdatetheencoderestimate
5: iftheinterventionsarehardthen
6: StepL4: ApplyAlgorithm4: Resolvethemixingwithancestors
7: endif
8: Return ˆandZˆ
G
Algorithm2ObtainCausalOrder
1: Input: SamplesofX fromenvironment 0 andenvironmentset
2: InitializeA = 0 n×d E ▷AholE dsfortheestimateof H† ⊤
3: n = 1,...,n ▷remainingunorderedset
V { } (cid:0) (cid:1)
4: fort (n,...,1)do
∈
5: Ft = im(G) \span(A⊤ πt+1,...,A⊤ πn) ▷definethefeasibleset
6: fork t do
∈ V
c = min E a⊤ s (X) sm(X) (32)
X X
a∈Ft
m∈ (cid:88)Vt\{k} (cid:20)(cid:12)
(cid:12)
·
(cid:2)
−
(cid:3)(cid:12) (cid:12)(cid:21)
(cid:12) (cid:12)
7: ifc = 0then
8: Denoteaminimizerbya∗ andsetA⊤ = a∗ ▷theintermediateestimate
k
9: π t = k ▷Ik hasnoancestorsinIVt
10: t−1 = t π t ▷removetheidentifiednodefromunorderedset
V V \{ }
11: break
12: endif
13: endfor
14: endfor
15: Returnπ andA
StepL1-Scoredifferences: Westartbycomputingscoredifferences(s sm)forallm [n].
X − X ∈
StepL2-Obtainingacausalorder: The initial observation is that the image of (s sm) for
X − X
differentvaluesofmaresufficientlydifferent. Hence,anencodercannotsimultaneouslymake
theestimatedlatentscoredifferencesequaltozeroforallenvironments. However,thisbecomes
possible when a leaf node is excluded. Subsequently, the key idea in this step is searching for
the environment for which the latent score differences can be made zero for all the remaining
environments. Byfollowingthisroutine,Algorithm2findstheyoungestnodeamongtheremaining
ones at each step and returns a permutation π of the nodes. In Lemma 7, we show that the
permutationIπ ≜ (Iπ1,...,Iπn)isavalidcausalorder.
19VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Algorithm3MinimizeScoreVariations
1: Input: SamplesofX fromenvironment 0 andenvironmentset ,and π,A generatedby
E E { }
Algorithm2
2: Initialize ˆwithemptygraphovernodes π 1,...,π n
G { }
3: fort (n 1,...,1)do
∈ −
4: Ft = im(G) \span(A⊤ πt+1,...,A⊤ πn) ▷updatethefeasibleset
5: forj t+1,...,n do
∈ { }
6: ifAssumption2holdsthen ▷setfordeterminingwhetherπ t π j
→
7: t,j = j cˆh(π t) π t
M V \ ∪{ }
8: else
(cid:8) (cid:9)
9: t,j = j dˆe(π t) π t
M V \ ∪{ }
10: endif
(cid:8) (cid:9)
11:
c = min E a⊤ s (X) sm(X) (33)
X X
a∈Ft
m∈ (cid:88)Mt,j (cid:20)(cid:12)
(cid:12)
·
(cid:2)
−
(cid:3)(cid:12) (cid:12)(cid:21)
12: ifc = 0then (cid:12) (cid:12)
13: Denoteaminimizerbya∗ andsetA⊤ = a∗ ▷updatetheestimate
πt
14: else
15: Addπ t π j to ˆ
→ G
16: endif
17: endfor
18: endfor
19: Encoderestimate: H∗ = A⊤ †
20: Return ˆandH∗
G (cid:0) (cid:1)
StepL3-Identifyingancestorsviaminimizingscorevariations: Afterobtainingavalidcausal
orderfortheintervenednodesintheenvironments,weperformAlgorithm3toestimatechildren
ofanodeinthelatentDAGateachstepandconstructourencoderestimateH∗. InLemma8,we
showthattheoutputsofthisstepachieveidentifiabilityuptoancestors,thatis, ˆ isisomorphic
tr
G
to underpermutationπ,andZˆ isalinearfunctionof Z : j an(i) foralli [n].
Gtr πi
{
j
∈ } ∈
StepL4-Hardinterventionstoresolvemixingwithancestors: We have this additional step in
the case of hard interventions to further refine our estimates and achieve perfect identifiability.
Specifically,weusethefactthattheintervenedlatentvariablebecomesindependentofitsnon-
descendants. Algorithm4usesthiscriticalpropertytoconstructanunmixingmatrixVandrefines
ourencoderestimateasH∗ = V H∗. ForaperfectDAGrecovery,wecomputethelatentscore
·
differencesandconstructthegraph ˆbasedonthefollowingrule.
G
i pˆa(m) m E s (Zˆ;H∗) sm(Zˆ;H∗) = 0, (34)
∈ ∪{ } ⇐⇒ Zˆ − Zˆ i ̸
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:8) (cid:9) (cid:12)(cid:2) (cid:3) (cid:12)
InLemma10,weshowthat
ˆisisomorphict(cid:12)
o
andZˆ(X;H∗)satisfiess(cid:12)
calingconsistency.
G G
20SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Algorithm4UnmixingProcedure
1: Input: SamplesofX fromenvironment 0 andenvironmentset ,π fromAlgorithm2and
E E
H∗ fromAlgorithm3
2: InitializeV = I n×n
3: fork (1,...,n)do
∈
4: InitializeA = 0 (k−1)×(k−1)
5: Zˆπ k = H∗ Xπ k
·
6: forj 1,...,k 1 do
∈ { − }
7: Z˜ πj = V πj ·Zˆπ k
8: endfor
9: forj,ℓ 1,...,k 1 do
∈ { − }
10: A j,ℓ = Cov(Zˆ ππ jk,Z˜ π ℓ)
11: endfor
12: b = Cov Zˆ ππ kk,[Z˜ π1,...,Z˜ π k−1]
13: forj 1,...,k 1 do
∈ {(cid:0) − } (cid:1)
14: V π k,πj = −[A−1 ·b] j
15: endfor
16: endfor
17: Updatetheencoderestimate: H∗ = V H∗
·
18: LatentDAGrecovery: Construct ˆaccordingto(34).
G
19: return ˆandH∗
G
5.2 GSCALE-IAlgorithmforGeneralTransformations
IntheLSCALE-Ialgorithm,wehaveexploitedthetransformation’slinearityandrecoveredthetrue
encoder’s parameters sequentially using one intervention per node. For general transformations
(parametric or non-parametric), however, we cannot use the same parametric approach and rely
on the properties of linear transforms. To rectify these and design the general algorithm, we use
moreinformationintheformoftwointerventionspernode. Specifically,wedesigntheGSCALE-I
algorithm,whichdirectlyminimizesthescoredifferencesacrossinterventionalenvironmentpairs.
WeemphasizethatthestructureofthisalgorithmisdifferentfromtheLSCALE-Ialgorithm,and
feedingitwithdatafromoneinterventionpernodedoesnotreduceittoLSCALE-I.Specifically,
therearemajordifferencesinStepG2andStepL3,asStepG2ofGSCALE-Idirectlyminimizesthe
scoredifferencesforhardinterventions,whereasStepL3ofLSCALE-Iisdesignedtosequentially
minimizethescoredifferencesforsoftinterventionswhichsubsumehardinterventions.
ThecoretoolforidentifiabilityviatheGSCALE-Ialgorithmisthatamongallvalidencoders
h ,thetrueencoderg−1resultsintheminimumnumberofvariationsbetweenthescoreestimates
∈ H
sm(zˆ;h) and s˜m(zˆ;h) (see Lemma 11). To formalize these, corresponding to each valid encoder
Zˆ Zˆ
h ,wedefinescorechangematricesD (h),D(h),andD˜(h)asfollows. Foralli,m [n]:
t
∈ H ∈
[D (h)] ≜ E sm(Zˆ;h) s˜m(Zˆ;h) , (35)
t i,m Zˆ − Zˆ i
(cid:104)(cid:12) (cid:12)(cid:105)
[D(h)] ≜ E (cid:12)(cid:2)s (Zˆ;h) sm(Zˆ;h)(cid:3) (cid:12) , (36)
i,m (cid:12) Zˆ − Zˆ i(cid:12)
(cid:104)(cid:12) (cid:12)(cid:105)
[D˜(h)] ≜ E (cid:12)(cid:2)s (Zˆ;h) s˜m(Zˆ;h)(cid:3) (cid:12) , (37)
i,m (cid:12) Zˆ − Zˆ i(cid:12)
(cid:104)(cid:12) (cid:12)(cid:105)
(cid:12)(cid:2) (cid:3) (cid:12)
(cid:12) (cid:12)
21VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Algorithm5GeneralizedScore-basedCausalLatentEstimationviaInterventions(GSCALE-I)
1: Input: ,samplesofX fromenvironment 0 andenvironmentsets and ˜,is_coupled.
H E E E
2: StepG1: Computescoredifferences:(s X −sm X),(s X −s˜m X),and(sm X −s˜m X)forallm ∈ [n].
3: StepG2: Identifytheencoderbyminimizingscorevariations:
4: ifis_coupledthen
5: Solve 1 in(38),selectasolutionh∗.
P
6: else ▷searchforthecorrectcoupling
7: forallpermutationsπ of[n]do
8: Temporarilyrelabel ˜m to ˜πm forallm [n],andsolve 2 in(39)
E E ∈ P
9: Ifthereisasolution,selectasolutionh∗ andbreakfromtheloop.
10: endfor
11: endif
12: StepG3: Latentestimates: Zˆ = h∗(X).
13: StepG4: LatentDAGrecovery: ConstructlatentDAG ˆusing(40).
G
14: returnZˆ and ˆ.
G
where expectations are under the measures of latent score functions induced by the probability
measureofobservationaldata. Theentry[D (h)] willbestrictlypositiveonlywhenthereisaset
t i,m
ofsamplesX withastrictlypositivemeasurethatrendersnon-identicalscoressm(zˆ;h)ands˜m(zˆ;h).
Zˆ Zˆ
Similar properties hold for the entries of D(h) and D˜(h) for the respective score functions. The
GSCALE-IalgorithmissummarizedinAlgorithm5,andthestepsinvolvedaredescribednext. We
notethatthisalgorithmservesthetwopurposesofprovidingaconstructiveproofofidentifiabilityand
provablycorrectachievability. Wepresentthestepsinthemostgeneralformtoaddressbothaspects
andthenweelaboratemoreonthepracticalconsiderationspertinenttoachievabilityinRemark1.
Inputs: TheinputsofGSCALE-Iaretheobserveddatafromtheobservationalandtwointerventional
environments,whetherenvironmentsarecoupled/uncoupled,andasetofvalidencoders .
H
StepG1–Scoredifferences: Westartbycomputingscoredifferences(s sm),(s s˜m),and
X − X X − X
(sm s˜m)forallm [n].
X − X ∈
StepG2–Identifyingtheencoder: Thekeypropertyinthisstepisthatthenumberofvariations
oftheestimatedlatentscoredifferencesisalwaysnolessthanthenumberofvariationsofthetrue
latentscoredifferences. Wehavetwodifferentapproachesforcoupledanduncoupledsettings.
StepG2(a)–Coupledenvironments: Wesolvethefollowingoptimizationproblem
min D (h)
t 0
≜ h∈H ∥ ∥ (38)
1
P (cid:40) s.t. D (h) isadiagonalmatrix.
t
ConstrainingD (h)tobediagonalenforcesthatthefinalestimateZˆ andZ willberelated
t
bypermutation (theinterventionorder). Weselectasolutionof in(38)asourencoder
1
I P
estimateanddenoteitbyh∗.
StepG2(b)–Uncoupledenvironments: Inthissetting,additionally,weneedtodeterminethe
correctcouplingbetweentheinterventionalenvironmentsets and ˜. Tothisend,weiterate
E E
22SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
through permutations π of [n], and temporarily relabel ˜m to ˜πm for all m [n] within
E E ∈
eachiteration. Subsequently,wesolvethefollowingoptimizationproblem:
min D (h)
t 0
h∈H ∥ ∥
 s.t. D (h) isadiagonalmatrix
≜  t (39)
2 
P    1 D(h) = 1 D˜(h)
 { } { }
1 D(h) 1 D⊤(h) = I .
n×n
  { }⊙ { }


Theconstraint1 D(h) = 1 D˜(h) ensuresthatapermutationofthecorrectencoderisa
{ } { }
solutionto ifthecouplingiscorrect,andthelastconstraintensuresthatD(h)doesnot
2
P
contain2-cycles. Wewillshowthat isalwaysfeasibleand,morespecifically,admitsa
2
P
solutionifandonlyifπ isthecorrectcoupling(seeLemma13),inwhichcase,weselecta
solutionof asourencoderestimateanddenoteitbyh∗.
2
P
StepG3–Latentestimates: The latent causal variables are estimated using h∗ via Zˆ = h∗(X),
whereX istheobservationaldata.
StepG4–LatentDAGrecovery: We construct DAG ˆ from D(h∗) by assigning the non-zero
G
coordinatesofthei-thcolumnofD(h∗)astheparentsofnodeiin ˆ,i.e.,
G
pˆa(i) ≜ j = i : [D(h∗)] = 0 , i [n]. (40)
j,i
̸ ̸ ∀ ∈
(cid:8) (cid:9)
Remark1 Forthenonparametricidentifiabilityresults,havinganoraclethatsolvesthefunctional
optimization problems and in (38) and (39), respectively, is sufficient. Solving these two
1 2
P P
problemsintheirmostgeneralformrequirescalculusofvariations. Thesetwoproblems,however,
foranydesiredparameterizedfamilyoffunctions (e.g.,linear,polynomial,andneuralnetworks),
H
reducetoparametricoptimizationproblems.
6. PropertiesofLSCALE-IandGSCALE-I
Inthissection,weanalyzethepropertiesandstepsofthealgorithmsinSection5forbothlinearand
generaltransformations. Westartbypresentingthekeypropertiesofscorefunctionsthatwillbe
repeatedlyusedthroughouttheanalysis.
6.1 PropertiesofScoreFunctionsunderInterventions
Scorefunctionsandtheirvariationsacrossdifferentinterventionalenvironmentsplaypivotalroles
inourapproachtoidentifyinglatentrepresentations. Inthissection,wepresentthekeyproperties
of the score functions, and their proofs and additional discussion on assumptions are deferred to
AppendicesAandD.Wefirstinvestigatescorevariationsacrosspairsofenvironmentssuchasthe
observationalenvironmentandaninterventionalone(underbothsoftandhardatomicinterventions)
ortwointerventionalenvironments,eithercoupledoruncoupled. Thefollowinglemmadelineatesthe
setofcoordinatesofthescorefunctionthatareaffectedbytheinterventionsinallrelevantcases. The
keyinsightisthataninterventioncauseschangesinonlycertaincoordinatesofthescorefunction.
23VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Lemma3(ScoreChangesunderInterventions) Consider environments 0, m, and ˜m with
E E E
unknowninterventiontargetsIm andI˜m.
(i) Softinterventions: Iftheinterventionin m issoftandthelatentcausalmodelisanadditive
E
noisemodel,thenscorefunctionssandsm differintheiri-thcoordinateifandonlyifnodeior
oneofitschildrenisintervenedin m.
E
E s(Z) sm(Z) = 0 i pa(Im). (41)
− i ̸ ⇐⇒ ∈
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
(ii) Hardinterventions: If(cid:12)theinterventionin(cid:12) m (or ˜m)ishard,thenscorefunctionssandsm
E E
(ors˜m)differintheiri-thcoordinateifandonlyifnodeioroneofitschildrenisintervenedin
m (orin ˜m).
E E
E s(Z) sm(Z) = 0 i pa(Im), (42)
− i ̸ ⇐⇒ ∈
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
and E (cid:12) s(Z) s˜m(Z) (cid:12) = 0 i pa(I˜m). (43)
− i ̸ ⇐⇒ ∈
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
(iii) Coupled environments I(cid:12)m = I˜m: In the co(cid:12)upled environment setting, sm and s˜m differ in
theiri-thcoordinateifandonlyifiisintervened.
E sm(Z) s˜m(Z) = 0 i = Im . (44)
− i ̸ ⇐⇒
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
(iv) Uncoupledenvironmen(cid:12)tsIm = I˜m: Consi(cid:12)dertwointerventionalenvironments m and ˜m
̸ E E
withdifferentinterventiontargetsIm = I˜m,andconsideradditivenoisemodelsspecifiedin
̸
(7). Giventhatp(Z)istwicedifferentiable,thescorefunctionssm ands˜m differintheiri-th
coordinateifandonlyifnodeioroneofitschildrenisintervened.
E sm(Z) s˜m(Z) = 0 i pa(Im,I˜m). (45)
− i ̸ ⇐⇒ ∈
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
Proof: SeeAppendixA.1(cid:12). (cid:12)
Lemma3providesthenecessaryandsufficientconditionsfortheinvarianceofthecoordinatesof
thescorefunctionsoflatentvariables. ForLemma3tobeusefulforidentifiabilityviausingobserved
variablesX,weneedtounderstandtheconnectionbetweenscorefunctionsofX andZ. Inthenext
lemma,weestablishthisrelationshipforanyinjectivemappingf fromlatenttoobservedspace.
Lemma4(ScoreDifferenceTransformation) Consider random vectors Y ,Y Rr and W ,
1 2 1
∈
W Rs thatarerelatedthrough
2
∈
Y = f(W ), and Y = f(W ), (46)
1 1 2 2
such that r s, probability measures of W ,W are absolutely continuous with respect to the
1 2
≥
s-dimensionalLebesguemeasure,andf : Rs Rr isaninjectiveandcontinuouslydifferentiable
→
function. ThedifferenceofthescorefunctionsofY andY ,andthatofW andW arerelatedas
1 2 1 2
⊤
s (w) s (w) = J (w) s (y) s (y) , where y = f(w). (47)
W1
−
W2 f
·
Y1
−
Y2
(cid:2) (cid:3) (cid:2) (cid:3)
24SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Proof: SeeAppendixA.2.
WecustomizeLemma4totwospecialcases. First,weconsiderscoredifferencesofZˆ(X;h)
andX. Bysettingf = h−1,Lemma4immediatelyspecifiesthescoredifferencesofZˆ(X;h)and
X under different environment pairs presented in (29), (30), and (31). Next, we consider score
differences of Zˆ(X;h) and Z. Note that Zˆ(X;h) = h(X) = (h g)(Z). Hence, by defining
ϕ = h g andsettingf = ϕ−1,Lemma4yields ◦
h ◦ h
between 0 and m : s (zˆ;h) sm(zˆ;h) = [J (z)]−⊤ s(z) sm(z) , (48)
E E Zˆ − Zˆ ϕ h · −
between E0 and E˜m : s Zˆ(zˆ;h) −s˜m Zˆ(zˆ;h) = [J
ϕ
h(z)]−⊤ ·(cid:2)s(z) −s˜m(z)(cid:3), (49)
between Em and E˜m : sm Zˆ(zˆ;h) −s˜m Zˆ(zˆ;h) = [J
ϕ
h(z)]−⊤ ·(cid:2)sm(z) −s˜m(z(cid:3)) , (50)
inwhichJ (z)denotestheJacobianofϕ atpointz Rn. Equippedw(cid:2)iththeseresults,(cid:3)weanalyze
ϕ h
h ∈
themainalgorithmsteps.
6.2 AnalysisofLSCALE-IAlgorithm
Inthissubsection,wepresentinsightsintotherationalebehindthealgorithmstepsandthebuilding
blocksessentialforthealgorithmsteps’correctness.
6.2.1 PROPERTIES OF SCORE DIFFERENCES
AsdiscussedinSection5,thekeyideaofthescore-basedframeworkistousethechangesinthescore
functionsofthelatentvariablestorecoverthetrueencoder. Lemma3showsthatthelatentDAG
determinesthenumberandsitesofthechangesinscorefunctionsacrossenvironments. Specifically,
usingLemma3,foralli,m [n]wehave
∈
E s(Z) sm(Z) = 0 i pa(Im). (51)
− i ̸ ⇐⇒ ∈
(cid:20)(cid:12) (cid:12)(cid:21)
Hence,theexpectedvalueE(cid:12) (cid:12)(cid:2)
[s(Z)
sm(Z(cid:3) )](cid:12)
(cid:12)i iszeroforallenvironmentsinwhichneithernodei
−
orachildofnodeiisintervened. Then,takingthesummationofthisexpectedvalueoverallthese
(cid:2)(cid:12) (cid:12)(cid:3)
environments,weobtain (cid:12) (cid:12)
E s(Z) sm(Z) = 0. (52)
− i
{m∈[n]: (cid:88)i∈/pa(Im)} (cid:20)(cid:12)
(cid:12)(cid:2) (cid:3)
(cid:12) (cid:12)(cid:21)
Thisindicatesthat,forinstance,ifiisaleafno(cid:12) dein ,thenthesu(cid:12) mofE [s(Z) sm(Z)] over
i
G −
allenvironmentsexcept
Ii
iszero. Subsequently,thekeyideaforidentifyingthetrueencoderis
E
(cid:2)(cid:12) (cid:12)(cid:3)
enforcingtheestimatedlatentscoredifferences(s sm)tohavesimilarst(cid:12)ructuresasthetrue(cid:12) score
Zˆ − Zˆ
differences(s sm),e.g.,sufficientlymanyzeroentriesatappropriatecoordinates.
−
Scoredifferencesforlineartransformation. ForalineartransformationmatrixF,JacobianJ (z)
F
isindependentofz andequaltomatrixF. Hence,forlineartransformationX = G Z,wehave
·
s(z) sm(z) = G⊤ s (x) sm(x) . (53)
X X
− · −
Similarly,foravalidencoderHandZˆ(X;h) = H(cid:2)X = (H G) (cid:3)Z,wehave
· · ·
s (zˆ;H) sm(zˆ;H) = H† ⊤ s (x) sm(x) = (H G)−⊤ s(z) sm(z) . (54)
Zˆ − Zˆ · X − X · · −
(cid:0) (cid:1) (cid:2) (cid:3) (cid:2) (cid:3)
25VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Then,foravalidencoderHandset [n],thelatentscoredifferenceunderHbecomes
A ⊆
E s (Zˆ;H) sm(Zˆ;H) = E (H†)⊤ s (X) sm(X) (55)
Zˆ − Zˆ i i· X − X
m (cid:88)∈A (cid:20)(cid:12)
(cid:12)(cid:2) (cid:3)
(cid:12) (cid:12)(cid:21) m (cid:88)∈A (cid:20)(cid:12)
(cid:12)(cid:2) (cid:3) (cid:2)
(cid:3)(cid:12) (cid:12)(cid:21)
(cid:12) (cid:12) = E (cid:12) H G −⊤ s(Z) sm(Z) (cid:12) . (56)
· i · −
m (cid:88)∈A (cid:20)(cid:12)
(cid:12)(cid:2) (cid:3) (cid:2)
(cid:3)(cid:12) (cid:12)(cid:21)
(cid:12) (cid:12)
Subsequently, to find an encoder H that makes the estimated latent scores conform to a causal
structureasin(52),weinvestigatefindingtheassociateddecoderH† byanalyzingequation(56).
Observation. Considerthefollowingquestion: bychoosinganon-zerovectorb Rn,forhow
∈
manyenvironments m wecanmakeE b⊤ [s(Z) sm(Z)] = 0? Theanswerliesinthefollowing
E · −
observation,whichisanimmediateresultofLemma3. Toformalizetheanswer,forset [n],we
(cid:2) (cid:3) A ⊆
defineIA ≜ Im : m asthesetofnodesintervenedinanyenvironment m form .
{ ∈ A} E ∈ A
Lemma5(ScoreDifferenceRank) Foranyset [n],wehave
A ⊆
s sm : m pa IA . (57)
|A| ≤ R { − ∈ A} ≤
Furthermore,ifIA isancestrallyclo(cid:0) sed,i.e.,an(IA) I(cid:1)A,t(cid:12) (cid:12)hei(cid:0) neq(cid:1) u(cid:12) (cid:12)alitiesbecomeequalities,i.e.,
⊆
= s sm : m = pa IA . (58)
|A| R { − ∈ A}
(cid:0) (cid:1) (cid:12) (cid:0) (cid:1)(cid:12)
Proof: SeeAppendixB.1. (cid:12) (cid:12)
Lemma5impliesthatanon-zerovectorbcanmakeE b⊤ [s(Z) sm(Z)] = 0foratmost
· −
(n 1)valuesofm [n]. Alsonotethatifiisaleafnodein ,thenE [s(Z) sm(Z)] iszeroforall
i
− ∈ G(cid:2) − (cid:3)
m [n]exceptm = Ii. Hence,byfindingavectorb Rn thatmakesE b⊤ [s(Z) sm(Z)] = 0
∈ ∈ (cid:2) · −(cid:3)
forallexceptoneenvironmentm [n],wecaneffectivelyidentifyanenvironmentinwhichaleaf
∈ (cid:2) (cid:3)
nodeisintervened. Westartfromthissimpleobservationtoestablishthefollowingresult,which
servesasakeycomponentofthesubsequentanalysisinthissection.
Lemma6(ScoreDifferenceRankunderAncestralClosedness) Considerset [n]suchthat
A ⊆
IA ≜ Im : m isancestrallyclosed,i.e.,an(IA) IA. Then,underAssumption1wehave
{ ∈ A} ⊆
, if j k suchthat Ik pa(Ij)
s sm : m k = |A| ∃ ∈ A\{ } ∈ . (59)
R − ∈ A\{ } (cid:40) 1, otherwise
(cid:16) (cid:17) |A|−
(cid:8) (cid:9)
Proof: SeeAppendixB.2.
The importance of this lemma is that, given an ancestrally closed set, it guides us to identify
the youngest nodes that do not have any children within the set. We leverage this property in the
subsequentalgorithmsteps,startingwithobtainingacausalorder.
6.2.2 STEP L2 – OBTAINING A CAUSAL ORDER
Inouranalysis,findingavalidcausalorderamongtheenvironmentsiscrucialforrecoveringthe
latentvariablesandthelatentgraph. ThenextresultstatesthattheprocedureinAlgorithm2achieves
thisobjective.
26SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Lemma7(CausalOrder) UnderAssumption1,forpermutationπ determinedbyAlgorithm2,the
orderedsetIπ ≜ (Iπ1,...,Iπn)isavalidcausalorder.
Proof: SeeAppendixB.3.
Lemma 7 ensures that we can order environments as ( π1,..., πn) such that the intervened
E E
nodes (Iπ1,...,Iπn) form a valid causal order. This property guides estimating the rows of the
encodersequentially,startingwiththeleafandsequentiallyadvancingtotherootnode(s).
6.2.3 STEP L3 – MINIMIZING SCORE VARIATIONS
StepL2ensuresthatforeachnodei [n],wehaveasetthatcontainsalltheancestorsofianddoes
∈
notcontainanydescendantofi. Usingthisinformation,weaimtoformagraphestimate ˆsuchthat
G
ithasthesameancestralrelationshipsasthetruegraph . Dependingonthepropertiesofthelatent
G
causalmodel,wehavetwodifferentresultsinthisstep. First,weconsideranylatentcausalmodel
underAssumption1. ThenextresultshowsthatAlgorithm3achievestheobjectiveofrecovering
ancestralrelationshipsandsatisfiesmixingconsistencyuptoancestors.
Lemma8(IdentifiabilityuptoAncestors) UnderAssumption1,theoutputsofAlgorithm3achieve
identifiabilityuptoancestors. Specifically,
(C1) Transitiveclosuresof ˆand arerelatedthroughagraphisomorphismbypermutation .
G G I
(C2) H∗ satisfiesmixingconsistencyuptoancestors. Specifically,Zˆ(X;H∗) = P C Z such
I an
· ·
thatdiagonalentriesofC arenon-zeroandj / an(i)implies[C ] = 0.
an an i,j
∈
Proof: SeeAppendixB.3.
ThislemmashowsthatAlgorithm2returnsapermutationπ suchthatIπ isacausalorder. Given
such π, Lemma 8 shows that Algorithm 3 outputs satisfy transitive closure recovery and mixing
consistency up to ancestors. Hence, Algorithm 1 achieves the identifiability guarantees for soft
interventionsstatedinTheorem2. Wenotethattheseresultsaretightinthesensethattheycannotbe
improvedwithoutmakingadditionalassumptions. Specifically,thestudybySquiresetal.(2023)
showsthatinlinearGaussianmodelsunderlineartransformations,identifiabilitybeyondmixingup
toancestorsisimpossibleundersoftinterventions. Next,weconsiderthenon-linearlatentcausal
modelsthatsatisfyAssumption2. Specifically,ifthescoredifferencesareensuredtobefull-rank,
i.e.,Assumption2,weshowthatAlgorithm3achievessignificantlystrongeridentifiabilityresults
forsoftinterventionsthanTheorem2.
Lemma9(IdentifiabilityuptoSurroundingParents) UnderAssumption2,theoutputsofAlgo-
rithm3satisfy
(C3) ˆand arerelatedthroughagraphisomorphismbypermutation .
G G I
(C4) H∗ satisfies mixingconsistency up to surroundingparents. Specifically, Zˆ(X;H∗) = P
I
·
C Z suchthatdiagonalentriesofC arenon-zeroandj / sur(i)implies[C ] = 0.
sur sur sur i,j
· ∈
Furthermore,Zˆ(X;H∗)isMarkovwithrespectto ˆ.
G
Proof: SeeAppendixB.4.
ThislemmashowsthatunderAssumption2whichholdsforsufficientlynon-linearcausalmodels,
Algorithm3recoversthetruelatentDAGuptoanisomorphismandidentifiesthelatentvariablesup
tomixingwithonlysurroundingvariables. Hence,Algorithm1achievestheidentifiabilityguarantees
forsoftinterventionsstatedinTheorem3.
27VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
6.2.4 STEP L4 – HARD INTERVENTIONS
Softinterventionssubsumehardinterventions,andapplyinghardinterventionsisexpectedtoprovide
strongeridentifiabilityguarantees. Whenahardinterventionisapplied,nodeilosesitsfunctional
dependenceonpa(i). Thenextstatementisadirectresultofthisadditionalpropertyexclusiveto
hardinterventions.
Proposition1 Fortheenvironment m inwhichnodeℓ = Im ishardintervened,wehave
E
Zm Zm , j nd(ℓ), (60)
ℓ j
⊥⊥ ∀ ∈
wherend(ℓ)isthesetofnon-descendantsofℓin .
G
ThispropertycanbereadilyverifiedbynotingthatbasedontheMarkovproperty,eachvariableina
DAGisindependentofitsnon-descendants,givenitsparents. Whennodeℓishard-intervened,ithas
noparents,andthestatementfollowsdirectly. Motivatedbythispropertyofhardinterventions,the
mainideaofAlgorithm4istoensurethattheestimatedlatentvariablesconformtoProposition1.
Tothisend,weconsiderH∗,theencoderestimateofStepL3,andaimtolearnanunmixingmatrix
V Rn×nsuchthatV H∗wouldsatisfyscalingconsistency. ThenextresultshowsthatAlgorithm4
∈ ·
achievesthisobjectiveand,subsequently,aperfectDAGrecovery.
Lemma10(Unmixing) Under Assumption 1 and hard interventions, the outputs of Algorithm 4
satisfyscalingconsistencyandperfectDAGrecovery.
Proof: SeeAppendixB.5.
For some insight into this result, note that Proposition 1 provides us with random variable
pairsthataresupposedtobeindependent,andthecovarianceoftwoindependentrandomvariables
is necessarily zero. Hence, Algorithm 4 uses the covariance as a surrogate of independence and
estimatestherowsoftheunmixingmatrixbymakingcovariancesofthoserandomvariablepairszero.
Lemma10showsthatthisprocedureeliminatesmixingwithancestorsandachievesscalingconsis-
tency. RecallthatunderAssumption1,Lemmas7and8holdforbothsoftandhardinterventions.
Lemma10showsthat,underhardinterventions,Algorithm4outputssatisfyperfectDAGrecovery
andscalingconsistency. Hence,theLSCALE-IAlgorithm1achievesidentifiabilityguaranteesfor
hardinterventionsstatedinTheorem1.
6.3 AnalysisofGSCALE-IAlgorithm
SimilarlytoLSCALE-I,analyzingGSCALE-Iinvolvesminimizingscoredifferences. However,in
contrasttolineartransformations, estimatedandtruelatentscoredifferencesin(48)–(50)arenot
alwaysrelatedbyaconstantmatrixforgeneraltransformations. Tocircumventthisissue,analysis
ofGSCALE-Irelieson(44)inLemma3,i.e.,thescoredifferencebetweencoupledinterventional
environmentsisone-sparse. Subsequently,thekeyideaforidentifyingtheencoderisthatthenumber
ofvariancesbetweenthescoreestimatessm(zˆ;h)ands˜m(zˆ;h)isminimizedunderthetrueencoder
Zˆ Zˆ
h = g−1. Toshowthat,first,wedenotethetruescorechangematrixbyD ,whichisspecifiedas
t
[D ] ≜ E sm(Z) s˜m(Z) , i,m [n]. (61)
t i,m − i ∀ ∈
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
(cid:12) (cid:12)
28SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Westartbyconsideringcoupledenvironments. Sincetheonlyvaryingcausalmechanismacross m
E
and ˜m istheintervenednodeIm = I˜m,basedon(44)inLemma3wehave
E
E sm(Z) s˜m(Z) = 0 i = Im , (62)
− i ̸ ⇐⇒
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
(cid:12) (cid:12)
which implies that 1 D is a permutation matrix, specifically, 1 D = P⊤. We show that the
{ t } { t } I
number of variations between the score estimates sm(zˆ;h) and s˜m(zˆ;h), i.e., ℓ norm of D (h),
Zˆ Zˆ 0 t
cannotbelessthanthenumberofvariationsunderthetrueencoderg−1,thatisn = D .
∥ t ∥0
Lemma11(ScoreChangeMatrixDensity) Foreveryh , thescorechangematrixD (h)is
t
∈ H
atleastasdenseasthescorechangematrixD associatedwiththetruelatentvariables,
t
D (h) D = n. (63)
∥ t ∥0 ≥ ∥ t ∥0
Proof: SeeAppendixC.1.
TherestoftheproofofTheorem5buildsonLemma11andshowsthat,foranysolutionh∗ to
1
specifiedin(38),wehave1 J−1(z) = P⊤forallz Rn. Finally,usingLemma3(ii)weshowthP at
{ ϕ h∗ } I ∈
DAG ˆconstructedusingD(h∗)isisomorphictothetruelatentDAG . Next,weconsidertheuncou-
G G
pledenvironments. Theproofconsistsofshowingtwopropertiesoftheoptimizationproblemin
2
P
specifiedin(39): (i)itdoesnothaveafeasiblesolutionifthecouplingisincorrect,and(ii)ithasafea-
siblesolutionifthecouplingiscorrect,whicharegivenbyfollowingLemma12and13,respectively.
Lemma12(Feasibility) If the coupling is incorrect, i.e., π = , the optimization problem
2
̸ I P
specifiedin(39)doesnothaveafeasiblesolution.
Proof: SeeAppendixC.3.
The main intuition in the proof of Lemma 12 is that the constraints of cannot be satisfied
2
P
simultaneously under an incorrect coupling. We prove it by contradiction. We assume that h∗ is
asolution,hence,D (h∗)isdiagonaland1 D(h) = 1 D˜(h) . Then,byscrutinizingtheeldest
t
{ } { }
mismatchednode,weshowthatD(h∗) D⊤(h∗)cannotbeadiagonalmatrix,whichcontradictsthe
·
premisethath∗ isafeasiblesolution.
Lemma13 If the coupling is correct, i.e., π = , h = π−1 g−1 is a solution to specified in
2
I ◦ P
(39),andyields D (h) = n.
∥ t ∥0
Proof: SeeAppendixC.4.
Lemmas12and13collectivelyproveTheorem4identifiabilityasfollows. Wecansearchover
thepermutationsof[n]until admitsasolutionh∗. ByLemma12,theexistenceofthissolution
2
P
meansthatcouplingiscorrect. Notethatwhenthecouplingiscorrect, theconstraintsetof is
1
P
asubsetoftheconstraintsin . Furthermore,theminimumvalueof D (h) islowerbounded
P2 ∥ t ∥0
byn(Lemma11),whichisachievedbythesolutionh∗ (Lemma13). Hence,h∗ isalsoasolutionto
,andbyTheorem5,itsatisfiesperfectrecoveryofthelatentDAGandthelatentvariables.
1
P
29VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
7. EmpiricalEvaluations
Inthissection,weprovideempiricalassessmentsoftheachievabilityguarantees. Specifically,we
empiricallyevaluatetheperformanceoftheLSCALE-IandGSCALE-Ialgorithmsforrecovering
thelatentcausalvariablesZ andthelatentDAG . Forbrevity,inthissection,wepresentonlythe
G
resultsandtheirinterpretationsanddeferthedetailsoftheimplementationstoAppendixE.1(linear
transformations)andAppendixE.3(generaltransformations). Furthermore,theresultspresentedin
thissectionarefocusedonlatentcausalgraphswithn = 5nodes. Weprovideadditionalresultsfor
largergraphsinAppendicesE.2andE.4forlinearandgeneraltransformations,respectively. Finally,
we note that any desired score estimators can be modularly incorporated into our algorithms. In
Section7.3,weassessthesensitivityofourperformancetothequalityoftheestimators.6
Evaluationmetrics. Theobjectivesarerecoveringthegraph andthelatentvariablesZ. Weuse
G
thefollowingmetricstoevaluatetheaccuracyofLSCALE-IandGSCALE-Iforrecoveringthese.
• StructuralHammingdistance: ForassessingtherecoveryofthelatentDAG,wereportstructural
Hammingdistance(SHD)betweentheestimate ˆandtrueDAG . Thiscapturesthenumberof
G G
edgeoperations(add,delete,flip)neededtotransform ˆto . Dependingonthespecificsofthe
G G
modelandinterventions,wewillusetheSHDofthegraphicalmodelsrelevanttothatsetting.
• Normalized ℓ loss: We assess the recovery of the latent variables by the closeness of latent
2
variableestimatesZˆ totruelatentvariablesZ. Tothisend,wereportthenormalizedℓ loss
2
Z Zˆ
ℓ(Z,Zˆ) ≜ ∥ − ∥2 . (64)
Z
∥ ∥2
Again, depending on the specifics of the transformations and interventions, we will have more
specificvariationsofthislossfunction.
Scorefunctions. LSCALE-IandGSCALE-Ialgorithms,intheirfirststeps,computeestimatesof
thescoredifferencesintheobservationalenvironment. Thedesignsofouralgorithmsareagnosticto
howthisisperformed,i.e.,anyreliablemethodforestimatingthescoredifferencescanbeadopted
and incorporated into our algorithms in a modular way. In our experiments, we adopt two score
estimatorsnecessaryfordescribingthedifferentaspectsoftheidentifiabilityandachievabilityresults.
• Perfect score oracle for identifiability: Identifiability, by definition, refers to the possibility
ofrecoveringthecausalgraphandlatentvariablesunderallidealizedassumptionsforthedata.
AssessingtheidentifiabilityguaranteesformalizedinTheorems1–6requiresusingperfectestimates
for the score differences. Hence, we adopt a perfect score oracle for evaluating identifiability.
Specifically, We use a perfect score oracle that computes the score differences in Step L1 of
LSCALE-IandStepG1ofGSCALE-IbyleveragingLemma4andusingthegroundtruthscore
functionss,sm ands˜m (seeAppendixE.1fordetails).
• Data-drivenscoreestimatesforachievability: Achievability,ontheotherhand,isresponsiblefor
evaluatingtheaccuracyofthecausalgraphandlatentvariablesunderallrealisticassumptionsfor
thedata. Unlikeidentifiability,forachievability,weneedrealscoreestimates,whichareinevitably
6.The codebase for the algorithms and simulations are available at https://github.com/acarturk-e/
score-based-crl.
30SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
noisy. For this purpose, when the pdf p has a parametric form and score function s has a
X X
closed-formexpression,thenwecanestimatetheparameterstoformanestimatedscorefunction.
For instance, when X follows a linear Gaussian distribution, then we have s (x) = Θ x in
X
− ·
which Θ is the precision matrix of X and can be estimated from samples of X. In other cases
inwhichs doesnothaveaknownclosed-form,weadoptnon-parametricscoreestimators. In
X
particular,weusethestate-of-the-artslicedscorematchingwithvariancereduction(SSM-VR)for
scoreestimationduetoitsefficiencyandaccuracyfordownstreamtasks(Songetal.,2020).
7.1 LSCALE-IAlgorithmforLinearTransformations
Datageneration. Togenerate ,weuseErdo˝s-Rényimodelwithdensity0.5andn = 5nodes,
G
whichisgenerallythesizeofthelatentgraphsconsideredinCRLliterature. Fortheobservational
causalmechanisms,weadoptthelinearGaussianmodelwith
Z = A Z +N , i [n], (65)
i i i
· ∀ ∈
where A R1×n are the rows of the weight matrix A in which A = 0 if and only j pa(i).
i i,j
∈ ̸ ∈
Thenon-zeroedgeweightsaresampledfromUnif( [0.5,1.5]),andthenoisetermsarezero-mean
±
Gaussian variables with variances σ2 sampled from Unif([0.5,1.5]). For a hard intervention on
i
nodei,weset
Z = N¯ , (66)
i i
whereN¯ (0, σ i2 ). Forasoftinterventiononnodei,weset
i ∼ N 4
A
Z = i Z +N¯ . (67)
i i
2 ·
We consider target dimension values d 5,25,100 and generate 100 latent graphs for each d
∈ { }
value. Foreachgraph,wesamplen independentandidenticallydistributed(i.i.d.) samplesofZ
s
fromeachenvironment. Inthissetofexperiments,weconsidern [5 103, 5 104]toinvestigate
s
∈ × ×
theeffectofthenumberofsamplesontheperformanceofLSCALE-I.TheobservedvariablesX are
generatedaccordingtoX = G Z,inwhichG Rd×n israndomlysampledfull-rankmatrix.
· ∈
Implementationofthealgorithmsteps. Wesolvetheoptimizationproblems(32)ofAlgorithm2
and (33) of Algorithm 3 by finding null spaces of the covariance matrices of the observed score
differencefunctions. ThedetailsofthisimplementationareprovidedinAppendixE.1.
Evaluationmetrics. LSCALE-Ienjoysperfectandpartialrecoveryguaranteeswhenusinghard
andsoftinterventions,respectively. Hence,weuseproperevaluationmetricsforeachintervention
type.
1. Hardinterventions: Weassesstherecoveryofthelatentvariables,i.e.,scalingconsistency,by
theclosenessoflatentvariableestimatesdenotedbyℓ(Z,Zˆ). Forassessingtherecoveryofthe
latentDAG,wereporttheSHDbetweentheestimate ˆand .
G G
2. Softinterventions: WeassesstherecoveryofthelatentDAGbytheSHDbetweenthetransitive
closure of the estimate ˆ and that of the true graph . For assessing the recovery of latent
tc tc
G G
variables,wenotethatZ canberecovereduptomixingwithallitsancestors. Specifically,
i
Zˆ = (H G) Z , (68)
· ·
31VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Table3: LSCALE-Iforalinearcausalmodelwithonehardinterventionpernode(n = 5).
perfectscores noisyscores
n d n samples ℓ(Z,Zˆ) SHD( , ˆ) ℓ(Z,Zˆ) SHD( , ˆ)
s
G G G G
5 5 5 103 0.01 0.08 0.03 0.86
×
5 25 5 103 0.01 0.07 0.03 1.0
×
5 100 5 103 0.01 0.09 0.03 0.86
×
5 5 104 0.01 0.05 0.02 0.26
5 25 104 0.01 0.04 0.02 0.27
5 100 104 0.01 0.04 0.02 0.26
5 5 5 104 0.01 0.02 0.01 0.06
×
5 25 5 104 0.01 0.02 0.01 0.06
×
5 100 5 104 0.01 0.01 0.01 0.03
×
inwhich[H G] = 0forallj / an(i). Therefore,wereporttheaveragenumberofincorrect
Ii,j
· ∈
mixingcomponents,i.e.,thenumberof(i,j)pairssuchthatj / an(i)and[H G] = 0.
Ii,j
∈ · ̸
Observations. Table3andTable4showtheperformanceoftheLSCALE-Ialgorithmusingperfect
scores andnoisy scoresunder hard andsoft interventions, respectively. Theeffect ofvarying the
observed variable dimension d is shared for both intervention types. Specifically, the results for
bothperfectscoresandnoisyscoresremainconsistentwhilethedimensionofobservedvariables
increasesfromd = 5tod = 100. ThisconfirmsouranalysisthattheperformanceofLSCALE-Iis
agnostictothedimensionoftheobservations. Next,weconsidertheperformanceunderhardandsoft
interventionsintheirrespectiveevaluationmetricsforperfectandpartialidentifiability,respectively.
1. Hard interventions: For hard interventions, Table 3 shows that by using perfect scores from
asfewasn = 5 103 samplesperenvironment,wecanalmostperfectlyrecoverbothZ and
s
×
. This conforms to our results in Theorem 1 which states perfect identifiability under hard
G
interventions. Table3alsoshowsthatbyusingnoisyscorescomputedviaempiricalprecision
matricesofX,thelatentvariablescanberecoveredalmostaswellasinthecaseofperfectscores.
For instance, given n = 5 104 samples, the performance under noisy scores matches the
s
×
performanceunderperfectscoreswith5 103 samples.
×
2. Softinterventions: Forsoftinterventions,Table4showsthatbyusingperfectscoresfromasfew
asn = 5 103 samples,wecanachievethepartialidentifiabilityguaranteesinTheorem2for
s
×
softinterventions. Specifically,weensuremixingconsistencyuptoancestors(impliedbyzero
incorrectmixingcomponents)andalmostperfectlyrecoverthetrueancestralrelationships,i.e.,
. Similartothecaseofhardinterventions,weobservethatincreasingthenumberofsamples
tc
G
significantly improves the performance under noisy scores. For instance, given n = 5 104
s
×
samples,theaverageSHDbetweenthetransitiveclosures ˆ and isapproximately0.3,and
tc tc
G G
theaveragenumberofincorrectmixingcomponentsisapproximately0.2. Finally,wenotethat
theperformanceundersoftinterventionswithnoisyscoresisslightlyworsethanthatofthehard
interventionsintheirrespectivemetrics. Thisisalsoexpectedsincetheeffectofasoftintervention
onthescoredifferencesisweakerthantheeffectofahardintervention.
32SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Table4: LSCALE-Iforalinearcausalmodelwithonesoftinterventionpernode(n = 5).
perfectscores noisyscores
n d n samples incorrectmix. SHD( , ˆ ) incorrectmix. SHD( , ˆ )
s tc tc tc tc
G G G G
5 5 5 103 0 0.01 1.62 2.57
×
5 25 5 103 0 0.02 1.63 2.71
×
5 100 5 103 0 0.01 1.53 2.67
×
5 5 104 0 0.01 1.07 1.71
5 25 104 0 0.01 1.07 1.74
5 100 104 0 0.01 1.21 2.00
5 5 5 104 0 0 0.22 0.37
×
5 25 5 104 0 0 0.08 0.29
×
5 100 5 104 0 0 0.12 0.32
×
7.2 GSCALE-IAlgorithmforGeneralTransformations
Next,wefocusonshowcasingsettingsforwhichtheexistingliteraturelacksachievabilityresults
and provides only identifiability results. Hence, the achievability results in this subsection lack
counterpartsintheexistingliterature. Wefocusonanon-polynomialtransformg andanon-linear
latentcausalmodel.
Datageneration. Togenerate weusetheErdo˝s-Rényimodelwithdensity0.5andn = 5nodes.
G
Fortheobservationalcausalmechanisms,weadoptanadditivenoisemodelwith
Z = Z⊤ A Z +N , (69)
i pa(i)· i · pa(i) i
(cid:113)
where A : i [n] are positive-definite matrices, and the noise terms are zero-mean Gaussian
i
{ ∈ }
variableswithvariancesσ2 sampledrandomlyfromUnif([0.5,1.5]). Forthetwohardinterventions
i
on node i, Z is set to N (0,σ2 ) and N (0,σ2 ). We set σ2 = σ2 + 1 and
i q,i ∼ N q,i q˜,i ∼ N q˜,i q,i i
σ2 = σ2+2. Weconsidertargetdimensionvaluesd 5,25,100 . Foreachdvalue,wegenerate
q˜,i i ∈ { }
100latentgraphsandn samplespergraph,wherewesetn = 3 104. Asthetransformation,we
s s
×
considerageneralizedlinearmodel,
X = g(Z) = tanh(G Z), (70)
·
inwhichtanhisappliedelement-wise,andparameterG Rd×n isarandomlysampledfull-rank
∈
matrix.
Candidateencoderandlossfunction. Leveraging(70),weparameterizevalidencodersas
Zˆ = h(X) = H arctanh(X). (71)
·
TousegradientdescenttolearnparametersHofh,werelaxℓ normin(38)andinsteadminimize
0
theelement-wiseℓ norm D (h) computedwithempiricalexpectations. Wealsoaddproper
1,1 ∥ t ∥1,1
regularizationtermstoensurethattheestimatedparameterH∗ willbefull-rank.
33VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Table5: GSCALE-Iforaquadraticcausalmodel withtwocoupled hardinterventionsper node
(n = 5).
perfectscores noisyscores
n d n ℓ(Z,Zˆ) SHD( , ˆ) ℓ(Z,Zˆ) SHD( , ˆ)
s
G G G G
5 5 3 104 0.03 0.12 1.19 5.1
×
5 25 3 104 0.03 0.04 1.09 4.4
×
5 100 3 104 0.04 0.02 0.86 5.0
×
Evaluationmetrics. GSCALE-IensuresperfectlatentandDAGrecovery. WereportSHDbetween
ˆand toassesstherecoveryofthelatentDAG.Wereportnormalizedℓ lossbetweenZˆ andZ to
2
G G
recoverthelatentvariables.
Observations. Table 5 shows that by using perfect scores, we can almost perfectly recover the
latentvariablesandthelatentDAGforn = 5nodes. Wealsoobservethatincreasingdimensiondof
theobservationaldatadoesnotdegradetheperformance(aswasthecaseforLSCALE-I),confirming
ouranalysisthatGSCALE-Iisagnostictothedimensionofobservations.
The effect of the quality of score estimation. Table 5 shows that GSCALE-I’s performance
degrades significantly when using noisy scores. It is noteworthy that in LSCALE-I, the decrease
inperformancefromperfecttonoisyscoresisremarkablylesspronouncedthanthecorresponding
observationsinGSCALE-I.Thisdiscrepancyisattributedtothedistinctscoreestimationprocedures
adoptedinthetwoexperimentsettings. Specifically,weconsideredlinearGaussianlatentmodelsin
Section7.1,whichallowedustodirectlyestimatetheparametersoftheclosed-formscorefunctions .
X
However,inthissection,weusequadraticlatentmodelsunderanon-lineartransformation. Hence,
we rely on a nonparametric score estimation via SSM-VR (Song et al., 2020). This comparison
between two experiment settings and results underscores that the performance gap between the
theoreticalguaranteesandpracticalresultscanbesignificantlymitigatedthroughtheadvancesin
general score estimation techniques. Next, we provide a further empirical evaluation of how the
qualityofthescoreestimationaffectsthefinalperformance.
7.3 SensitivitytoScoreEstimationNoise
Asdiscussedearlier,scoreestimatorscanbemodularlyincorporatedintoouralgorithm. Forthecases
wherep isnotamenabletoparameterestimation,wecanusethenoisyscoreestimatesgenerated
X
bySSM-VRasachievablebaselines. InAppendixE.2,wefollowthisprocedurefortheLSCALE-I
algorithmwithaquadraticlatentcausalmodelspecifiedin(69)andreporttheresultsinTable9. In
thissubsection,weassesshowmuchperformanceimprovementwecanhaveasthescoreestimates
becomemoreaccurate. Tothisend,wetesttheLSCALE-Ialgorithmundervaryingscoreestimation
noiselevels. Specifically,werunLSCALE-Ialgorithmusingthescoresgeneratedbythefollowing
model:
sˆ (x;σ2) = s (x) 1+Ξ , where Ξ (0,σ2 I ). (72)
X X d×d
· ∼ N ·
Weconsiderhardinterventionsongrap(cid:0)hsizeo(cid:1)fn = 5,setd = 25,andvarythevalueofσ2 within
[10−4,10−2]. Werepeattheexperiments100timeswithn = 104 samplesfromeachenvironment.
s
WeplottheevaluationmetricsmeanZ errorandmeanSHDinFigure3withrespecttothesignal-
34SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
7.5
SSM SSM
0.2
5.0
0.1
2.5
0.0 0.0
0 20 40 60 0 20 40 60
Signaltonoiseratio(dB) Signaltonoiseratio(dB)
(a)Normalizedℓ lossversusSNR (b)SHDversusSNR
2
1 1
Figure 3: The performance of LSCALE-I under noisy scores with varying SNR for n = 5 and
d = 25. The dashed vertical lines correspond to SNR values that correspond to the performance
attainedbySSM-VR.
to-noise ratio (SNR). It is clear that when the score estimation error is small, indicated by a high
SNRvalue,LSCALE-Ialgorithmdemonstratesastrongperformanceinrecoveringbothlatentcausal
variablesandthelatentgraph. Wealsonotethatthebaselineresultswithnoisyscorescomputedvia
SSM-VRinTable9yieldsimilarsuccessatgraphrecoveryattheSNRofapproximately25dBand
latentrecoveryattheSRMofapproximately8dB.ThetrendofthecurvesinFigure3indicatesthat
ouralgorithmwouldgreatlybenefitfromabetterscoreestimator.
8. ConcludingRemarks
In this paper, we have proposed a score function-based CRL framework that uses stochastic in-
terventions to learn latent causal representations and the latent causal graph underlying them. In
this framework, by uncovering novel connectionsbetween score functionsand CRL,we havees-
tablishedidentifiabilityresultsforlinearandgeneraltransformationswithoutrestrictingthelatent
causalmodels,anddesignedLSCALE-IandGSCALE-Ialgorithmsthatachievetheseidentifiability
guarantees.
Thereareseveralexcitingdirectionsforfuturework. Currently,weconsideratomicinterventions
inwhichexactlyonecausalmechanismisalteredineachinterventionalenvironment. Establishing
thenecessaryandsufficientconditionsforasetofmulti-targetinterventionalenvironmentstosuffice
forperfectorpartialidentifiabilityisanimportantdirectionwithpracticalimplications. Next,we
notethatourLSCALE-Ialgorithmforlineartransformationsisagnostictotheinterventiontypeand
latentcausalmodel,meaningthatitcanbeusedforbothsoftandhardinterventionsanddifferent
causalmodels. DesigningsimilaruniversalCRLalgorithmsthatcanhandlegeneraltransformations
underdifferentsizesandtypesofinterventionalenvironmentsistheultimategoalforstudyingthe
identifiabilityofCRLfrominterventions. Finally,amissingcomponentofexistingCRLliteratureis
thefinite-sampleanalysis. Probabilisticidentifiabilityresultsforagivennumberofinterventional
data samples can be useful, especially in applications where performing interventions is costly.
Hence,itisanexcitingfuturedirectiontoinferpartialcausalinformationfromalimitednumberof
samplesandknowledgeaboutunseeninterventionsfromavailableinterventions.
35
ssol
ℓdezilamroN
2
DHSVARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
References
K.Ahuja,J.S.Hartford,andY.Bengio. Propertiesfrommechanisms: Anequivarianceperspective
onidentifiablerepresentationlearning. InProc.InternationalConferenceonLearningRepresenta-
tions,virtual,April2022a.
K. Ahuja, J. S. Hartford, and Y. Bengio. Weakly supervised representation learning with sparse
perturbations. InProc.AdvancesinNeuralInformationProcessingSystems,NewOrleansLA,
December2022b.
K.Ahuja,D.Mahajan,Y.Wang,andY.Bengio. Interventionalcausalrepresentationlearning. In
Proc.InternationalConferenceonMachineLearning,Honolulu,Hawaii,July2023a.
K. Ahuja, A. Mansouri, and Y. Wang. Multi-domain causal representation learning via weak
distributionalinvariances. arXiv:2310.02854,2023b.
S.Bing,U.Ninad,J.Wahl,andJ.Runge. Identifyinglinearly-mixedcausalrepresentationsfrom
multi-nodeinterventions. arXiv:2311.02695,2023.
W. M. Boothby. An introduction to differentiable manifolds and Riemannian geometry, Revised,
volume120. GulfProfessionalPublishing,2003.
J.Brehmer,P.DeHaan,P.Lippe,andT.S.Cohen. Weaklysupervisedcausalrepresentationlearning.
InProc.AdvancesinNeuralInformationProcessingSystems,NewOrleans,LA,December2022.
S.Buchholz,G.Rajendran,E.Rosenfeld,B.Aragam,B.Schölkopf,andP.Ravikumar. Learning
linearcausalrepresentationsfrominterventionsundergeneralnonlinearmixing. InProc.Advances
inNeuralInformationProcessingSystems,NewOrleans,LA,December2023.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signalsandsystems,2(4):303–314,1989.
H.HälväandA.Hyvärinen. HiddenMarkovnonlinearICA:Unsupervisedlearningfromnonsta-
tionarytimeseries. InProc.ConferenceonUncertaintyinArtificialIntelligence,virtual,August
2020.
A.HyvärinenandH.Morioka. NonlinearICAoftemporallydependentstationarysources. InProc.
InternationalConferenceonArtificialIntelligenceandStatistics,Ft.Lauderdale,FL,April2017.
A.HyvärinenandP.Pajunen. Nonlinearindependentcomponentanalysis: Existenceanduniqueness
results. NeuralNetworks,12(3):429–439,April1999.
A.Hyvärinen,H.Sasaki,andR.Turner. NonlinearICAusingauxiliaryvariablesandgeneralized
contrastivelearning. InProc.InternationalConferenceonArtificialIntelligenceandStatistics,
Naha,Japan,April2019.
Y.JiangandB.Aragam. Learningnonparametriclatentcausalgraphswithunknowninterventions.
InProc.AdvancesinNeuralInformationProcessingSystems,NewOrleans,LA,December2023.
J.JinandV.Syrgkanis. Learningcausalrepresentationsfromgeneralenvironments: Identifiability
andintrinsicambiguity. arXiv:2311.12267,2023.
36SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
I.Khemakhem,D.Kingma,R.Monti,andA.Hyvärinen. Variationalautoencodersandnonlinear
ICA: A unifying framework. In Proc. International Conference on Artificial Intelligence and
Statistics,virtual,August2020a.
I.Khemakhem,R.Monti,D.Kingma,andA.Hyvärinen. Ice-beem: Identifiableconditionalenergy-
baseddeepmodelsbasedonnonlinearICA. InProc.AdvancesinNeuralInformationProcessing
Systems,virtual,December2020b.
B.Kivva,G.Rajendran,P.Ravikumar,andB.Aragam. Identifiabilityofdeepgenerativemodels
under mixture priors without auxiliary information. In Proc. Advances in Neural Information
ProcessingSystems,NewOrleans,LA,December2022.
S.Lachapelle,P.Rodriguez,Y.Sharma,K.E.Everett,R.LePriol,A.Lacoste,andS.Lacoste-Julien.
Disentanglementviamechanismsparsityregularization: AnewprinciplefornonlinearICA. In
Proc.ConferenceonCausalLearningandReasoning,Eureka,CA,April2022.
T.E.Lee,J.A.Zhao,A.S.Sawhney,S.Girdhar,andO.Kroemer. Causalreasoninginsimulation
forstructure andtransferlearning ofrobotmanipulation policies. In Proc.IEEE International
ConferenceonRoboticsandAutomation,Xi’an,China,May2021.
W. Liang, A. Kekic´, J. von Kügelgen, S. Buchholz, M. Besserve, L. Gresele, and B. Schölkopf.
Causalcomponentanalysis. InProc.AdvancesinNeuralInformationProcessingSystems,New
Orleans,LA,December2023.
P.Lippe,S.Magliacane,S.Löwe,Y.M.Asano,T.Cohen, andE.Gavves. Causalrepresentation
learning for instantaneous and temporal effects in interactive systems. In Proc. International
ConferenceonLearningRepresentations,Kigali,Rwanda,May2023.
F.Locatello,S.Bauer,M.Lucic,G.Raetsch,S.Gelly,B.Schölkopf,andO.Bachem. Challenging
common assumptions in the unsupervised learning of disentangled representations. In Proc.
InternationalConferenceonMachineLearning,LongBeach,CA,June2019.
F.Locatello,B.Poole,G.Rätsch,B.Schölkopf,O.Bachem,andM.Tschannen. Weakly-supervised
disentanglementwithoutcompromises. InProc.InternationalConferenceonMachineLearning,
virtual,April2020.
F. Montagna, A. A. Mastakouri, E. Eulig, N. Noceti, L. Rosasco, D. Janzing, B. Aragam, and
F.Locatello. Assumptionviolationsincausaldiscoveryandtherobustnessofscorematching. In
Proc.AdvancesinNeuralInformationProcessingSystems,NewOrleans,LA,December2023a.
F.Montagna, N.Noceti, L. Rosasco, K. Zhang, and F.Locatello. Scalable causaldiscovery with
scorematching. InProc.ConferenceonCausalLearningandReasoning,Tübingen,Germany,
April2023b.
J.Pearl. Causality. CambridgeUniversityPress,Cambridge,UK,2009.
J.Ramsey,J.Zhang,andP.L.Spirtes. Adjacency-faithfulnessandconservativecausalinference.
arXiv:1206.6843,2012.
37VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
P.Rolland,V.Cevher,M.Kleindessner,C.Russell,D.Janzing,B.Schölkopf,andF.Locatello. Score
matching enables causal discovery of nonlinear additive noise models. In Proc. International
ConferenceonMachineLearning,Baltimore,MD,July2022.
S.Saengkyongam,E.Rosenfeld,P.Ravikumar,N.Pfister,andJ.Peters. Identifyingrepresentations
forinterventionextrapolation. arXiv:2310.04295,2023.
B.Schölkopf,F.Locatello,S.Bauer,N.R.Ke,N.Kalchbrenner,A.Goyal,andY.Bengio. Toward
causalrepresentationlearning. ProceedingsoftheIEEE,109(5):612–634,May2021.
X.Shen,F.Liu,H.Dong,Q.Lian,Z.Chen,andT.Zhang. Weaklysuperviseddisentangledgenerative
causalrepresentationlearning. JournalofMachineLearningResearch,23(1):10994–11048,2022.
R. Shu, Y. Chen, A. Kumar, S. Ermon, and B. Poole. Weakly supervised disentanglement with
guarantees. InProc.InternationalConferenceonLearningRepresentations,NewOrleans,LA,
May2019.
L.Simon. Introductiontogeometricmeasuretheory. TsinghuaLectures,2(2):3–1,2014.
Y.Song,S.Garg,J.Shi,andS.Ermon. Slicedscorematching: Ascalableapproachtodensityand
scoreestimation. InProc.UncertaintyinArtificialIntelligence,virtual,August2020.
C.Squires,A.Seigal,S.S.Bhate,andC.Uhler. Linearcausaldisentanglementviainterventions. In
Proc.InternationalConferenceonMachineLearning,Honolulu,Hawaii,July2023.
A.Tejada-Lapuerta,P.Bertin,S.Bauer,H.Aliee,Y.Bengio,andF.J.Theis. Causalmachinelearning
forsingle-cellgenomics. arXiv:2310.14935,2023.
B.Varıcı,E.Acartürk,K.Shanmugam,A.Kumar,andA.Tajer. Score-basedcausalrepresentation
learningwithinterventions. arXiv:2301.08230,2023.
J.vonKügelgen,Y.Sharma,L.Gresele,W.Brendel,B.Schölkopf,M.Besserve,andF.Locatello.
Self-supervisedlearningwithdataaugmentationsprovablyisolatescontentfromstyle. InProc.
AdvancesinNeuralInformationProcessingSystems,virtual,December2021.
J. von Kügelgen, M. Besserve, W. Liang, L. Gresele, A. Kekic´, E. Bareinboim, D. M. Blei, and
B.Schölkopf. Nonparametricidentifiabilityofcausalrepresentationsfromunknowninterventions.
InProc.AdvancesinNeuralInformationProcessingSystems,NewOrleans,LA,December2023.
E.W.Weisstein. Sigmoidfunction. https://mathworld.wolfram.com/,2002.
M.Yang,F.Liu,Z.Chen,X.Shen,J.Hao,andJ.Wang. CausalVAE:Disentangledrepresentation
learningvianeuralstructuralcausalmodels. InProc.IEEE/CVFConferenceonComputerVision
andPatternRecognition,virtual,June2021.
D.Yao,D.Xu,S.Lachapelle,S.Magliacane,P.Taslakian,G.Martius,J.vonKügelgen,andF.Lo-
catello. Multi-viewcausalrepresentationlearningwithpartialobservability. arXiv:2311.04056,
2023.
38SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
W.Yao, Y.Sun, A.Ho, C.Sun, andK.Zhang. Learningtemporallycausallatentprocessesfrom
generaltemporaldata. InProc.InternationalConferenceonLearningRepresentations,virtual,
April2022.
J.Zhang,C.Squires,K.Greenewald,A.Srivastava,K.Shanmugam,andC.Uhler. Identifiability
guarantees for causal disentanglement from soft interventions. In Proc. Advances in Neural
InformationProcessingSystems,NewOrleans,LA,December2023.
Z.Zhu,F.Locatello,andV.Cevher. Samplecomplexityboundsforscore-matching: Causaldiscovery
and generative modeling. In Proc. Advances in Neural Information Processing Systems, New
Orleans,LA,December2023.
39VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Appendix
Table of Contents
A ProofsofScoreFunctionPropertiesandTransformations 42
A.1 ProofofLemma3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
A.2 ProofofLemma4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
A.3 ProofofLemma14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
B ProofsoftheResultsforLinearTransformations 49
B.1 ProofofLemma5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
B.2 ProofofLemma6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
B.3 ProofsofLemma7andLemma8 . . . . . . . . . . . . . . . . . . . . . . . . . 51
B.4 ProofofLemma9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
B.5 ProofofLemma10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
B.6 ProofofLemma15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
B.7 ProofofTheorems1,2,and3 . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
C ProofsoftheResultsforGeneralTransformations 62
C.1 ProofofTheorem5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
C.2 ProofofTheorem6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
C.3 ProofofLemma12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
C.4 ProofofLemma13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
C.5 ProofofTheorem4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
C.6 ProofofProposition3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
D AnalysisoftheAssumptions 71
D.1 AnalysisofAssumption1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
D.2 AnalysisofAssumption2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
E EmpiricalEvaluations: DetailsandAdditionalResults 81
E.1 ImplementationDetailsofLSCALE-IAlgorithm . . . . . . . . . . . . . . . . . 81
E.2 AdditionalResultsforLSCALE-I . . . . . . . . . . . . . . . . . . . . . . . . . 83
E.3 ImplementationDetailsofGSCALE-IAlgorithm . . . . . . . . . . . . . . . . . 85
E.4 AdditionalResultsforGSCALE-I . . . . . . . . . . . . . . . . . . . . . . . . . 87
40SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Table6: Notation.
[n] : 1,...,n
{ }
1 : indicatorfunction
ground-truthvariables: : latentcausalgraphoverZ
G
: transitiveclosureof
tc
G G
: transitivereductionof
tr
G G
pa(i) : parentsofnodeiin
G
ch(i) : childrenofnodeiin
G
an(i) : ancestorsofnodeiin
G
de(i) : descendantsofnodeiin
G
X : [X ,...,X ]⊤ observedrandomvariables
1 d
Z : [Z ,...,Z ]⊤ latentrandomvariables
1 n
Z : vectorformedbyZ foralli pa(i)
pa(i) i
∈
g : truedecoder
h : avalidencoder
: thesetofvalidencodersh
H
G : truelineardecoder
H : avalidlinearencoder
interventionnotations: 0 : observationalenvironment
E
: ( 1,..., n) firstinterventionalenvironments
E E E
˜ : (˜1,..., ˜n) secondinterventionalenvironments
E E E
Im : theintervenednodesin m
E
I˜m : theintervenednodesin ˜m
E
: thesetofintervenednodes(I1,...,In)
I
˜ : thesetofintervenednodes(I˜1,...,I˜n)
I
statisticalmodels: Zˆ(X) : genericestimatorofZ givenX
Zˆ(X;h) : anauxiliaryestimatorofZ givenX andencoderh
ˆ : estimateof
G G
p,pm,p˜m : pdfsofZ in 0, m,and ˜m
E E E
p ,pm,p˜m : pdfsofX in 0, m,and ˜m
X X X E E E
s,sm,s˜m : scorefunctionsofZ in 0, m,and ˜m
E E E
s ,sm,s˜m : scorefunctionsofX in 0, m,and ˜m
X X X E E E
s ,sm,s˜m : scorefunctionsofZˆ in 0, m,and ˜m forencoderh
Zˆ Zˆ Zˆ E E E
pˆa(i) : parentsofnodeiin ˆ
G
matrixnotations: A† : Pseudo-inverseofmatrixA
A : rowiofmatrixA
i
A : entryofmatrixAatrowiandcolumnj
i,j
P : Permutationmatrixassociatedwithpermutationπ of[n]
π
(f) : dim(im(f))forafunctionf
R
( ) : dim(im( ))forasetoffunctions
R F F F
D,D˜,D : Truescorechangematrices
t
D(h),D˜(h),D (h) : Scorechangematricesunderencoderh
t
41VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
AppendixA. ProofsofScoreFunctionPropertiesandTransformations
Westartbyprovidingthefollowingfactsthatwillbeusedrepeatedlyintheproofs.
Proposition2 Considertwocontinuousfunctionsf,g : Rn R. Then,foranyα > 0,
→
z Rn f(z) = g(z) E f(Z) g(Z) α = 0. (73)
∃ ∈ ̸ ⇐⇒ − ̸
(cid:104)(cid:12) (cid:12) (cid:105)
Specifically,forα = 1,wehave (cid:12) (cid:12)
z Rn f(z) = g(z) E f(Z) g(Z) = 0. (74)
∃ ∈ ̸ ⇐⇒ − ̸
(cid:104)(cid:12) (cid:12)(cid:105)
Proof: Ifthereexistsz Rn suchthatf(z) = g(z),then(cid:12)f(z) g(z)is(cid:12)non-zerooveranon-zero-
∈ ̸ −
measuresetduetocontinuity. Then,E f(Z) g(Z) α = 0sincep(pdfofZ)hasfullsupport. On
| − | ̸
theotherdirection,iff(z) = g(z)forallz Rn,thenE f(Z) g(Z) α = 0. Thismeansthat
(cid:2) ∈ (cid:3) | − |
E f(Z) g(Z) α = 0impliesthatthereexistsz Rn suchthatf(z) = g(z).
| − | ̸ ∈ (cid:2) ̸ (cid:3)
(cid:2) (cid:3)
A.1 ProofofLemma3
Our score-based methodology builds on the changes in score functions under interventions. For
provingLemma3,weusethefollowingintermediateresultwhichformalizestheweakestpossible
requirementforameaningfulinterventionandshowsthatitisapropertyof(i)hardinterventions
underanycausalmodel,and(ii)additivenoisemodelundereithersoftorhardinterventions.
Lemma14(InterventionalRegularity) Causalmechanismsp andq ofnodeiaresaidtosatisfy
i i
interventionalregularityif
z Rn suchthat ∂ q i(z i | z pa(i)) = 0, k pa(i). (75)
∃ ∈ ∂z p (z z ) ̸ ∀ ∈
k i i pa(i)
|
Then,p andq satisfyinterventionalregularityifatleastoneofthefollowingconditionsistrue:
i i
1. Theinterventionishard,i.e.,q (z z ) = q (z ).
i i pa(i) i i
|
2. Thecausalmodelisanadditivenoisemodelinwhichthepdfsofthenoisevariablesareanalytic.
Proof: SeeAppendixA.3.
Case (i) and Case (ii). We give the proof for soft interventions on additive noise models, Case
(i). We will use interventional regularity since Lemma 14 shows that it is satisfied for additive
noise models. The proof for hard interventions, Case (ii), follows from similar arguments since
interventionalregularityisalsosatisfiedforhardinterventionsbyLemma14.
ProofofE [s(Z) sm(Z)] = 0 = i pa(Im): Letℓdenotethenodeintervenedin m,
i
− ̸ ⇒ ∈ E
i.e.Im = ℓ(cid:104). (cid:12)Following(4)and (cid:12)(cid:105)(15),thelatentscoress(z)andsm(z)aredecomposedas
(cid:12) (cid:12)
s(z) = logp (z z )+ logp (z z ), (76)
z ℓ ℓ pa(ℓ) z i i pa(i)
∇ | ∇ |
i̸=ℓ
(cid:88)
and sm(z) = logq (z z )+ logp (z z ). (77)
z ℓ ℓ pa(ℓ) z i i pa(i)
∇ | ∇ |
i̸=ℓ
(cid:88)
42SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Hence,s(z)andsm(z)differinonlythecausalmechanismofnodeℓ. Next,wecheckthederivatives
ofp (z z )andq (z z )intheiri-thcoordinates. NotethatthesetwodependonZ only
ℓ ℓ pa(ℓ) ℓ ℓ pa(ℓ)
| |
through Z : j pa(ℓ) . Therefore,ifi / pa(ℓ),
j
{ ∈ } ∈
∂ ∂
logp (z z ) = logq (z z ) = 0, (78)
ℓ ℓ pa(ℓ) ℓ ℓ pa(ℓ)
∂z | ∂z |
i i
whichindicatesthatifi / pa(ℓ),then[s(z)] = [sm(z)] forallz. This,equivalently,meansthatif
i i
∈
E [s(Z) sm(Z)] = 0,theni pa(ℓ).
i
− ̸ ∈
Pr(cid:2)o(cid:12) (cid:12)ofofi pa(Im)(cid:12) (cid:12)(cid:3) = E [s(Z) sm(Z)]
i
= 0: Notethatthetwoscorefunctionssand
∈ ⇒ − ̸
sm areequalintheircoordina(cid:104)te (cid:12)i pa(ℓ)onlyif (cid:12)(cid:105)
(cid:12) ∈ (cid:12)
∂logq ℓ(z ℓ z pa(ℓ)) ∂logp ℓ(z ℓ z pa(ℓ)) ∂ q ℓ(z ℓ z pa(i))
0 = | | = log | . (79)
∂z − ∂z ∂z p (z z )
i i i ℓ ℓ pa(ℓ)
|
However,(79)contradictswithinterventionalregularity. Therefore,ifi pa(ℓ),[s(z) ]and[sm(z)]
i i
∈
arenotidenticalandbyProposition2,E [s(Z) sm(Z)] = 0.
i
− ̸
Case(iii)Coupledenvironments. Sup(cid:2)p(cid:12)osethatIm = I˜m(cid:12)(cid:3)= ℓ. Following(16),wehave
(cid:12) (cid:12)
sm(z) = logq (z )+ logp (z z ), (80)
z ℓ ℓ z i i pa(i)
∇ ∇ |
i̸=ℓ
(cid:88)
and s˜m(z) = logq˜(z )+ logp (z z ). (81)
z ℓ ℓ z i i pa(i)
∇ ∇ |
i̸=ℓ
(cid:88)
Then,subtracting(81)from(80)andlookingati-thcoordinate,wehave
∂logq (z ) ∂logq˜(z )
sm(z) s˜m(z) = ℓ ℓ ℓ ℓ . (82)
− i ∂z − ∂z
i i
(cid:2) (cid:3)
If i = ℓ, the right-hand side is zero and we have sm(z) s˜m(z) = 0 for all z. On the other
̸ − i
hand, if i = ℓ, since q (z ) and q˜(z ) are distinct, there exists z Rn such that q (z ) = q˜(z ).
ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ
(cid:2) ∈(cid:3) ̸
Subsequently,byProposition2,wehaveE [sm(Z) s˜m(Z)] = 0.
i
− ̸
Case (iv) Uncoupled environments. Su(cid:2)p(cid:12)pose that Im = ℓ a(cid:12)n(cid:3)d I˜m = j, and ℓ = j. Following
(cid:12) (cid:12) ̸
(16),wehave
sm(z) = logq (z )+ logp (z z )+ logp (z z ), (83)
z ℓ ℓ z j j pa(j) z k k pa(k
∇ ∇ | ∇ |
k∈[n]\{ℓ,j}
(cid:88)
and s˜m(z) = logq (z )+ logp (z z )+ logp (z z ). (84)
z j j z ℓ ℓ pa(ℓ) z k k pa(k)
∇ ∇ | ∇ |
k∈[n]\{ℓ,j}
(cid:88)
Then,subtracting(84)from(83)wehave
sm(z) s˜m(z) = logq (z )+ logp (z z ) logq (z ) logp (z z ).
z ℓ ℓ z j j pa(j) z j j z ℓ ℓ pa(ℓ)
− ∇ ∇ | −∇ −∇ |
(85)
Scrutinizingthei-thcoordinate,wehave
sm(z) s˜m(z) = ∂logq ℓ(z ℓ) + ∂logp j(z j | z pa(j)) ∂logq j(z j) ∂logp ℓ(z ℓ | z pa(ℓ)) .
− i ∂z ∂z − ∂z − ∂z
i i i i
(cid:2) (cid:3) (86)
43VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
ProofofE [sm(Z) s˜m(Z) = 0 = i pa(ℓ,j): Supposethati / pa(ℓ,j). Then,none
− i ̸ ⇒ ∈ ∈
oftheterm(cid:104)s (cid:12)intheRHSof(86 (cid:3)) (cid:12)i(cid:105)safunctionofz i. Therefore,allthetermsintheRHSof(86)are
zero,andwe(cid:12)have sm(z) s˜m((cid:12)z) = 0forallz. ByProposition2,E [sm(Z) s˜m(Z)] = 0.
− i − i
This,equivalently,meansthatifE [sm(Z) s˜m(Z)] = 0,theni pa(ℓ,j).
(cid:2) (cid:3) − i ̸ ∈ (cid:2)(cid:12) (cid:12)(cid:3)
(cid:12) (cid:12)
ProofofE [sm(Z) s˜m(Z)] i (cid:2) =(cid:12) (cid:12) 0 = i pa(ℓ,(cid:12) (cid:12)j(cid:3) ): Weproveitbycontradiction. Assume
− ̸ ⇐ ∈
that sm(z)(cid:104)(cid:12) s˜m(z) = 0fora (cid:12)l(cid:105)lz. Withoutlossofgenerality,letℓ / pa(j).
−(cid:12) i (cid:12) ∈
If(cid:2)i = ℓ. Inthisca(cid:3)se,(86)issimplifiedto
0 = sm(z) s˜m(z) = ∂logq ℓ(z ℓ) ∂logp ℓ(z ℓ | z pa(ℓ)) . (87)
− ℓ ∂z − ∂z
ℓ ℓ
(cid:2) (cid:3)
If ℓ is a root node, i.e., pa(ℓ) = , (87) implies that (logq )′(z ) = (logp )′(z ) for all z .
ℓ ℓ ℓ ℓ ℓ
∅
Integrating, we get p (z ) = αq (z ) for some constant α. Since both p and q are pdfs, they
ℓ ℓ ℓ ℓ ℓ ℓ
both integrate to one, implying α = 1 and p (z ) = q (z ), which contradicts the premise that
ℓ ℓ ℓ ℓ
observationalandinterventionalmechanismsaredistinct. Ifℓisnotarootnode,considersome
k pa(ℓ). Then,takingthederivativeof(87)withrespecttoz ,wehave
k
∈
∂2logp (z z )
ℓ ℓ pa(ℓ)
0 = | . (88)
∂z ∂z
ℓ k
Recall the equation Z = f (Z )+N for additive noise models specified in (7). Denote
ℓ ℓ pa(ℓ) ℓ
the pdf of the noise term N by p . Then, the conditional pdf p (z z ) is given by
ℓ N ℓ ℓ pa(ℓ)
|
p (z z ) = p (z f (z )). Denotingthescorefunctionofp byr ,
ℓ ℓ pa(ℓ) N ℓ ℓ pa(ℓ) N p
| −
d
r (u) ≜ logp (u), (89)
p N
du
wehave
∂logp (z z ) ∂logp (z f (z ))
ℓ ℓ pa(ℓ) N ℓ ℓ pa(ℓ)
| = − = r (z f (z )). (90)
p ℓ ℓ pa(ℓ)
∂z ∂z −
ℓ ℓ
Substitutingthisinto(88),weobtain
∂r z f (z ) ∂f (z )
0 = p ℓ − ℓ pa(ℓ) = ℓ pa(ℓ) r′ z f (z ) , z Rn . (91)
p ℓ ℓ pa(ℓ)
∂z − ∂z · − ∀ ∈
(cid:0) k (cid:1) k
(cid:0) (cid:1)
Sincek isaparentofℓ,thereexistsafixedZ = z∗ realizationforwhich∂f (z∗ )/∂z
pa(ℓ) pa(ℓ) ℓ pa(ℓ) k
is non-zero. Otherwise, f (z ) would not be sensitive to z which is contradictory to k
ℓ pa(ℓ) k
being a parent of ℓ. Note that Z can vary freely after fixing Z . Therefore, for (91) to
ℓ pa(ℓ)
hold,thederivativeofr mustalwaysbezero. However,thescorefunctionofavalidpdfwith
p
fullsupportcannotbeconstant. Therefore, sm(z) s˜m(z) isnotalwayszero,andwehave
i − i
E [sm(Z) s˜m(Z)] = 0.
− i ̸ (cid:2) (cid:3)
If(cid:2)i(cid:12)= ℓ . Inthiscase,(cid:12)((cid:3)86)issimplifiedto
(cid:12)̸ (cid:12)
0 = sm(z) s˜m(z) = ∂logp j(z j | z pa(j)) ∂logq j(z j) ∂logp ℓ(z ℓ | z pa(ℓ)) . (92)
− i ∂z − ∂z − ∂z
i i i
(cid:2) (cid:3)
44SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Weinvestigatecasebycaseandreachacontradictionforeachcase. First,supposethati / pa(ℓ).
∈
Then,wehavei pa(j),and(92)becomes
∈
0 = sm(z) [s˜m(z) = ∂logp j(z j | z pa(j)) ∂logq j(z j) . (93)
− i ∂z − ∂z
i i
(cid:2) (cid:3)
Ifi = j,theimpossibilityof(93)directlyfollowsfromtheimpossibilityof(87). Theremaining
caseisi pa(j). Inthiscase,takingthederivativeoftheright-handsideof(93)withrespectto
∈
z ,weobtain
j
∂2logp (z z )
j j pa(j)
0 = | , (94)
∂z ∂z
i j
whichisarealizationof(88)fori pa(j)andj inplaceofk pa(ℓ)andℓ,whichweprovedto
∈ ∈
beimpossibleini = ℓcase. Therefore,i / pa(ℓ)isnotviable. Finally,supposethati pa(ℓ).
∈ ∈
Then,takingthederivativeoftheright-handsideof(92)withrespecttoz ,weobtain
ℓ
∂2logp (z z )
ℓ ℓ pa(ℓ)
0 = | , (95)
∂z ∂z
i ℓ
whichisagainarealizationof(88)fork = i,whichweprovedtobeimpossible.
Hence,weshowedthat sm(z) s˜m(z) cannotbezeroforallz values. Then,byProposition2we
− i
haveE [sm(Z) s˜m(Z)] = 0,andtheproofisconcluded.
− (cid:2) i ̸ (cid:3)
(cid:2)(cid:12) (cid:12)(cid:3)
A.2 Pr(cid:12)oofofLemma4 (cid:12)
Letusrecallthesetting. ConsiderrandomvectorsY ,Y Rr andW ,W Rs thatarerelated
1 2 1 2
∈ ∈
through Y = f(W ) and Y = f(W ) such that r s, probability measures of W ,W are
1 1 2 2 1 2
≥
absolutelycontinuouswithrespecttothes-dimensionalLebesguemeasureandf : Rs Rr isan
→
injectiveandcontinuouslydifferentiablefunction.
Inthissetting,therealizationsofW andY ,andthatofW andY ,arerelatedthroughy = f(w).
1 1 2 2
Since f is injective and continuously differentiable, volume element dw in Rs gets mapped to
det [J (w)]⊤ J (w) 1/2 dw onim(f). SinceW hasdensityp absolutelycontinuouswith
f
·
f 1 W1
respecttothes-dimensionalLebesguemeasure, using theareaformula(Boothby,2003), we can
(cid:12) (cid:0) (cid:1)(cid:12)
(cid:12)defineadensityforY ,d(cid:12)enotedbyp ,supportedonlyonmanifold ≜ im(f)whichisabsolutely
1 Y1
M
continuouswithrespecttothes-dimensionalHausdorffmeasure:
−1/2
p (y) = p (w) det [J (w)]⊤ J (w) , where y = f(w). (96)
Y1 W1
·
f
·
f
(cid:12) (cid:16) (cid:17)(cid:12)
(cid:12) (cid:12)
Densitiesp
Y2
andp
W2
ofY 2andW(cid:12) 2arerelatedsimilarly. S(cid:12)ubsequently,scorefunctionsof {W 1,W
2
}
and Y ,Y arespecifiedsimilarlyto(9)and(14),respectively. DenotetheJacobianmatrixoff at
1 2
{ }
pointw Rs byJ (w),whichisanr smatrixwithentriesgivenby
f
∈ ×
∂ f(w) ∂y
J (w) = i = i , i [r] ,j [s]. (97)
f i,j ∂w ∂w ∀ ∈ ∈
(cid:2) j (cid:3) j
(cid:2) (cid:3)
45VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Next,considerafunctionϕ: R. Sincethedomainofϕisamanifold,itsdifferential,denoted
M →
byDϕ,isdefinedaccordingto(12). Bynotingy = f(w),wecanalsodifferentiateϕwithrespectto
w Rs as(Simon,2014,p.57)
∈
⊤
ϕ(y) = (ϕ f)(w) = J (w) Dϕ(y). (98)
w w f
∇ ∇ ◦ ·
Next,giventheidentitiesin(96)and(98),wefindthe(cid:2)relation(cid:3)shipbetweenscorefunctionsofW
1
andY asfollows.
1
s (w) = logp (w) (99)
W1 ∇w W1
( =96) logp (y)+ log det [J (w)]⊤ J (w) 1/2 (100)
∇w Y1 ∇w f
·
g
( =98) J (w) ⊤ Dlogp (y)(cid:12) (cid:12)+ (cid:16) log det [J (w)(cid:17) ]⊤(cid:12) (cid:12) J (w) 1/2 (101)
f
·
Y1 (cid:12) ∇w f (cid:12)
·
f
= (cid:2) J (w)(cid:3)⊤ s (y)+ log det [J(cid:12) (cid:12) (w(cid:16) )]⊤ J (w) 1/2 .(cid:17)(cid:12) (cid:12) (102)
f
·
Y1 ∇w (cid:12)f
·
f (cid:12)
(cid:12) (cid:16) (cid:17)(cid:12)
Followingthesimilars(cid:2)tepstha(cid:3)tledto(102)forW 2 (cid:12)andY 2,weobtain (cid:12)
(cid:12) (cid:12)
s (w) = J (w) ⊤ s (y)+ log det [J (w)]⊤ J (w)
1/2
. (103)
W2 f
·
Y2 ∇w f
·
f
(cid:12) (cid:16) (cid:17)(cid:12)
Subtracting(103)from(1(cid:2)02),we(cid:3)obtainthedesiredres(cid:12)ult (cid:12)
(cid:12) (cid:12)
⊤
s (w) s (w) = J (w) s (y) s (y) . (104)
W1
−
W2 f
·
Y1
−
Y2
Corollary1 In Lemma 4, if f is a linear tr(cid:2)ansform(cid:3), th(cid:2)at is Y = F W(cid:3) for a full-rank matrix
·
F Rr×s,thenthescorefunctionsofY andW arerelatedthroughs (w) = F⊤ s (y),where
W Y
∈ ·
y = F w.
·
Proof: Forthegivenlineartransform,wehaveJ (w) = F,whichisindependentofw. Then,inthe
f
proofofLemma4,(102)reducestos (w) = F⊤ s (y). Finally,wenotethatthescoredifference
W Y
·
ofY canbesimilarlywrittenintermsofthescoredifferenceofW.
Corollary2 UnderthesamesettingandtheassumptionsasLemma4,wehave
⊤
†
s (y) s (y) = J (w) s (w) s (w) , where y = f(w). (105)
Y1
−
Y2 f
·
W1
−
W2
(cid:104) (cid:105)
Proof: Multiplying(104)fro(cid:2) mleftw(cid:3) ith [J(cid:2) (w)]† ⊤ ,weobtai(cid:3) n
f
⊤ ⊤
† (cid:2) (cid:3) † ⊤
J (w) s (w) s (w) = J (w) J (w) s (y) s (y) . (106)
f
·
W1
−
W2 f
·
f
·
Y1
−
Y2
(cid:104) (cid:105) (cid:104) (cid:105)
Note(cid:2)that (cid:3) (cid:2) (cid:3) (cid:2) (cid:3) (cid:2) (cid:3) (cid:2) (cid:3)
⊤
† ⊤ †
J (w) J (w) = J (w) J (w) . (107)
f f f f
· ·
Notethat,bypropertiesof(cid:104) (cid:2)theMoo(cid:3)re(cid:105) -Pen(cid:2)rosein(cid:3)verse,forany(cid:2)matrixA(cid:3) ,wehaveA A† A = A.
· ·
ThismeansthatA A† actsasaleftidentityforvectorsinthecolumnspaceofA. Bydefinition,s
·
Y1
ands havevaluesinT im(f),thetangentspaceoftheimagemanifoldf atpointw. Thisspace
Y2 w
isequaltothecolumnspaceofmatrixJ (w). Therefore,J (w) [J (w)]† actsasaleftidentityfor
f f f
·
s (y)ands (y),andwehave
Y1 Y2
†
J (w) J (w) s (y) s (y) = s (y) s (y). (108)
f
·
f
·
Y1
−
Y2 Y1
−
Y2
Substituting(107)and(108)(cid:2)into(10(cid:3)6)c(cid:2)ompletesthepro(cid:3)of.
46SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
A.3 ProofofLemma14
Wewillprovethatcausalmechanismsp andq satisfyinterventionalregularityfor(i)hardinterven-
i i
tionsand(ii)additivenoisemodels. Tothisend,wefirstdefine
q (z z )
ψ(z ,z ) ≜ i i | pa(i) . (109)
i pa(i)
p (z z )
i i pa(i)
|
We start by showing that ψ(z ,z ) varies with z . We prove it by contradiction. Assume the
i pa(i) i
contrary,i.e.,letψ(z ,z ) = ψ(z ). Byrearranging(109)wehave
i pa(i) pa(i)
q (z z ) = ψ(z )p (z z ). (110)
i i pa(i) pa(i) i i pa(i)
| |
Fixarealizationofz = z∗ andintegratebothsidesof(110)withrespecttoz . Sincebothp
pa(i) pa(i) i i
andq arepdfs,wehave
i
1 = q (z z∗ )dz = ψ(z∗ )p (z z∗ )dz dz (111)
R i i | pa(i) i R pa(i) i i | pa(i) i i
(cid:90) (cid:90)
= ψ(z∗ ) p (z z∗ )dz (112)
pa(i) R i i | pa(i) i
(cid:90)
= ψ(z∗ ). (113)
pa(i)
This identity implies that p (z z∗ ) = q (z z∗ ) for any arbitrary realization z∗ . This
i i | pa(i) i i | pa(i) pa(i)
contradictsthepremisethatobservationalandinterventionaldistributionsaredistinct. Asaresult,to
checkifamodelsatisfiesinterventionalregularityfornodei,itsufficestoinvestigatewhetherthe
functionψ isnotinvariantwithrespecttoz fork pa(i). Tothisend,from(109)weknowthat
k
∈
ψ(z ,z )varieswithz ifandonlyif
i pa(i) k
∂ ∂q i(z i z pa(i)) 1 ∂p i(z i z pa(i)) 1
logψ(z ,z ) = | | = 0.
i pa(i)
∂z ∂z · q (z z ) − ∂z · p (z z ) ̸
i i i i pa(i) i i i pa(i)
| |
(114)
Next,weinvestigatethesufficientconditionslistedintheLemma14.
A.3.1 HARD INTERVENTIONS
Underhardinterventions,notethatforanyk pa(i),
∈
∂ ∂
p (z z ) = 0, and q (z ) = 0. (115)
i i pa(i) i i
∂z | ̸ ∂z
k k
Then,itfollowsdirectlyfrom(114)that
∂ ∂q i(z i) 1 ∂p i(z i z pa(i)) 1
logψ(z ,z ) = | = 0. (116)
i pa(i)
∂z ∂z · q (z ) − ∂z · p (z z ) ̸
k k i i k i i pa(i)
|
=0 ̸=0
Thus,hardinterventionsonany(cid:124)la(cid:123)t(cid:122)ent(cid:125)causalmode(cid:124)lsatis(cid:123)fy(cid:122)interv(cid:125)entionalregularity.
47VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
A.3.2 ADDITIVE NOISE MODELS
Theadditivenoisemodelfornodeiisgivenby
Z = f (Z )+N , (117)
i i pa(i) i
asspecifiedin(7). Whennodeiissoftintervened,Z isgeneratedaccordingto
i
Z = f¯(Z )+N¯ , (118)
i i pa(i) i
inwhichf¯ andN¯ specifytheinterventionalmechanismfornodei. Then,denotingthepdfsofN
i i i
andN¯ byp andq ,respectively,(117)and(118)implythat
i N N
p (z z ) = p z f (z ) , and q (z z ) = q z f¯(z ) . (119)
i i pa(i) N i i pa(i) i i pa(i) N i i pa(i)
| − | −
Denotethescorefunctio(cid:0)nsassociatedwi(cid:1)thp andq by (cid:0) (cid:1)
N N
d p′ (u) d q′ (u)
r (u) ≜ logp (u) = N , and r (u) ≜ logq (u) = N . (120)
p N q N
du p (u) du q (u)
N N
Wewillprovethat(114)holdsbycontradiction. Assumethecontraryandlet
∂q i(z i z pa(i)) 1 ∂p i(z i z pa(i)) 1
| = | . (121)
∂z · q (z z ) ∂z · p (z z )
k i i pa(i) k i i pa(i)
| |
From(119)and(120),forthenumeratorsin(121)wehave,
∂p (z z ) ∂f (z )
i i | pa(i) = i pa(i) p′ z f (z ) , (122)
N i i pa(i)
∂z − ∂z · −
k k
∂q (z z ) ∂f¯(z ) (cid:0) (cid:1)
and i i | pa(i) = i pa(i) q′ z f¯(z ) . (123)
N i i pa(i)
∂z − ∂z · −
k k
(cid:0) (cid:1)
Hence,theidentityin(121)canbewrittenas
∂f (z ) ∂f¯(z )
i pa(i) r z f (z ) = i pa(i) r z f¯(z ) . (124)
p i i pa(i) q i i pa(i)
∂z · − ∂z · −
k k
(cid:0) (cid:1) (cid:0) (cid:1)
Define n and n¯ as the realizations of N and N¯ when Z = z and Z = z . By defining
i i i i i i pa(i) pa(i)
δ(z ) ≜ f (z ) f¯(z ),wehaven¯ = n +δ(z ). Then,(124)canrewrittenas
pa(i) i pa(i) i pa(i) i i pa(i)
−
∂f (z ) ∂f¯(z )
i pa(i) i pa(i)
r (n ) = r (n +δ(z )). (125)
p i q i pa(i)
∂z · ∂z ·
k k
Notethat ∂fi(z pa(i)) isanon-zerocontinuousfunction. Hence,thereexistsanintervalΦ R|pa(i)|
∂z k ⊆
over which
∂fi(z pa(i))
= 0. Likewise, r (n ) cannot be constantly zero over all possible intervals
Ω R. Thisis∂ bz ek caus̸ eotherwise,itwop uldi havetonecessarilybeaconstantzerofunction(since
⊆
itisanalytic),whichisaninvalidscorefunction. Hence,thereexistsanopenintervalΩ Rover
⊆
whichr (n )isnon-zeroforalln Ω. Then,wecanrearrange(125)as
p i i
∈
∂f¯ i(z pa(i))
r (n )
p i ∂z
= k , (n ,z ) Ω Φ. (126)
r q(n i+δ(z pa(i))) ∂fi(z pa(i)) ∀ i pa(i) ∈ ×
∂z
k
48SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Notethat theRHS of(126)isnot afunction ofn . Then, taking thederivativeof bothsideswith
i
respectton ,weget
i
r′(n ) r′(n +δ(z ))
p i q i pa(i)
= . (127)
r (n ) r (n +δ(z ))
p i q i pa(i)
Inthenextstep,weshowthatδ isnotaconstantfunction. Weprovethisbycontradiction. Suppose
thatδ(z ) = δ∗ isaconstantfunction. Then, thegradientsoff andf¯ areequal. From(126),
pa(i) i i
thisimpliesthat
r (n ) = r (n +δ∗), n Ω. (128)
p i q i i
∀ ∈
Since r (n ) and r (n +δ∗) are analytic functions that agree on an open interval of R, they are
p i q i
equal for all n R. This implies that p (n ) = ηq (n +δ∗) for some constant η R. Since
i N i N i
∈ ∈
p and q are pdfs, η = 1 is the only choice that maintains p and q are pdfs. Therefore,
N N N N
p (n ) = q (n +δ∗). However, using (119), this implies that p (z z ) = q (z z ),
N i N i i i pa(i) i i pa(i)
| |
whichcontradictsthepremisethataninterventionchangesthecausalmechanismoftargetnodei.
Thereforeδ isacontinuous,non-constantfunction,anditsimageoverz Φincludesanopen
pa(i)
∈
intervalΘ R. Withthisresultinmind,wereturnto(127). Considerafixedrealizationn = n∗
⊆ i i
anddenotethevalueoftheleft-handside(LHS)forn∗ byα. Bydefiningu ≜ δ(z ),weget
i pa(i)
r′(n∗+u)
q i
α = , u Θ. (129)
r (n∗+u) ∀ ∈
q i
Thisisonlypossibleifr isanexponentialfunction,i.e.,r (u) = k exp(αu)overintervalu Θ.
q q 1
∈
Sincer isananalyticfunction,itis,therefore,exponentialoverentireR. Then,theassociatedpdf
q
musthavetheformq (u) = k exp((k /α)exp(αu)). However,theintegralofthisfunctionover
N 2 1
the entire domain R diverges. Hence, it is not a valid pdf, rendering a contradiction. Hence, the
additivenoisemodelsatisfiesinterventionalregularity.
AppendixB. ProofsoftheResultsforLinearTransformations
B.1 ProofofLemma5
Considerenvironment m anddenotetheintervenednodebyℓ = Im. Lemma3(i)andLemma3(ii)
E
implythat,foranym [n],theimageofthefunction(s sm)containsavectorz Rn forwhich
∈ − ∈
z = 0andz = 0forj / pa(ℓ). Since(Z ,...,Z )aretopologicallyordered,thisimpliesz = 0
ℓ j 1 n j
̸ ∈
forj (ℓ+1). Then, fordifferentvaluesofm, thesespecifiedvectorsarelinearlyindependent.
≥
Hence,im( s sm : m )containsatleast linearlyindependentvectorsandwehave
{ − ∈ A} |A|
s sm : m . (130)
R { − ∈ A} ≥ |A|
Ontheotherhand,usingLemma3,(cid:0)theterms(Z) sm (cid:1)(Z)canhavenon-zeroentriesonlyatthe
−
coordinatespa(ℓ). Therefore,inanyvectorinim( s sm : m ),thej-thcoordinateiszeroif
{ − ∈ A}
j / pa IA . Subsequently,wehave
∈
(cid:0) (cid:1) s sm : m pa IA . (131)
R { − ∈ A} ≤
Finally,ifIA isancestrallyclosed(cid:0),wehavepa IA =(cid:1)IA.(cid:12)Th(cid:0)isim(cid:1)(cid:12)pliesthattheinequalitiesin(130)
(cid:12) (cid:12)
and(131)holdwithequalityandwehave
(cid:0) (cid:1)
s sm : m = = pa IA . (132)
R { − ∈ A} |A|
(cid:0) (cid:1) (cid:12) (cid:0) (cid:1)(cid:12)
(cid:12) (cid:12)
49VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
B.2 ProofofLemma6
Weconsideraset forwhichIA isancestrallyclosed,i.e.,an(IA) IA,andconsiderthescore
A ⊆
functiondifferences s sm : m k forsomek . First,weuseLemma5toshowthatthe
− ∈ A\{ } ∈ A
rankoftheimageofthesefunctionsiseither or 1. Notethatpa IA = pa(IA\{k}) pa(Ik).
(cid:8) (cid:9)|A| |A|− ∪
Subsequently,sinceIA isancestrallyclosed,wehave
(cid:0) (cid:1)
pa IA\{k} pa IA = IA = . (133)
≤ |A|
Then,usingLemma5,weha(cid:12)ve(cid:0) (cid:1)(cid:12) (cid:12) (cid:0) (cid:1)(cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
s sm : m k 1. (134)
|A| ≥ R − ∈ A\{ } ≥ |A|−
(cid:16) (cid:17)
Next,considertwocasesasfollows(cid:8). (cid:9)
Case(i): Ik doesnothaveachildinIA. Inthiscase,IA\{k} isancestrallyclosed,whichimplies
pa IA\{k} = IA\{k} = 1. (135)
|A|−
Then,usingLemma5and(134(cid:12)),w(cid:0)ehave(cid:1)(cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
s sm : m k = 1. (136)
R − ∈ A\{ } |A|−
(cid:16) (cid:17)
Case(ii): Ik hasachildinIA(cid:8) . SupposethatIj isth(cid:9) eeldestchildofIk inIA. Thismeansthat
ancestorsofIj andchildrenofIk donotintersect,an(Ij) ch(Ik) = . Letγ beapermutationof
∩ ∅
thenodesin suchthat Iγ1,...,Iγ |A| aretopologicallyordered. Thisimpliesthat
A { }
Iγj < Iγ ℓ , ℓ ch(k) j . (137)
∀ ∈ \{ }
Denotetheindexofk andj inγ byuandv,i.e.,γ = k andγ = j. Considertheset
u v
= γ ,...,γ ,γ ,...,γ . (138)
1 u−1 u+1 v−1
M { }
SinceIj istheeldestchildofIk in ,weknowthatIM isancestrallyclosed. Then,usingLemma5,
A
wehave
s sm : m = . (139)
R { − ∈ M} |M|
Note that the vectors in im( s s(cid:0)m : m ) have(cid:1)zeros in coordinates Ik and Ij since IM
{ − ∈ M}
doesnotcontainanychildrenofIk orIj. SinceIk pa(Ij),Assumption1impliesthatim(s sj)
∈ −
containstwolinearlyindependentvectorsa,b Rn suchthat
∈
a,b im(s sj) : a = 0 and b = 0. (140)
Ik Ij
∃ ∈ − ̸ ̸
Since pa(Ij) Ik , the set im s sm : m j contains + 2 linearly
\ { } ⊆ M { − ∈ M ∪ { }} |M|
independentvectors. Finally,weusethesameargumentasintheproofofLemma5asfollows. For
(cid:0) (cid:1)
eachm ( k,j ),wehave
∈ A\ M∪{ }
z im(s sm) : z Im = 0 and z j = 0, j (Im+1). (141)
∃ ∈ − ̸ ∀ ≥
Then,thesespecifiedvectorsarelinearlyindependentandarenotcontainedinim s sm : m
{ − ∈
j . Therefore, im s sm : m k containsatleast linearlyindependent
M∪{ }} { − ∈ A\{ }} |A| (cid:0)
vectors–twovectorsfromim(s sj)andonevectorfromim(s sm)foreachm k,j ,
(cid:1) (cid:0) − (cid:1) − ∈ A\{ }
andtheproofiscomplete.
50SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
B.3 ProofsofLemma7andLemma8
First,weprovidealinearalgebraicproperty,whichwillbeusedintheproofs.
Lemma15 Considerthelatentcausalgraph ,andamatrixL Rn×n.
G ∈
1. LetL beabinarymatrixthatdenotestheancestralrelationshipsin ,i.e.,
an
G
1, if i an(j),
[L ] ≜ ∈ . (142)
an i,j
(cid:40)0, otherwise
Then,ifI ≼ 1(L) ≼ L ,wealsohaveI ≼ 1(L−1) ≼ L .
n×n an n×n an
2. LetL beabinarymatrixthatdenotesthesurroundingrelationshipsin ,i.e.,
sur
G
1, if i = j ,
[L sur] i,j ≜ 1, if ch(j) ch(i), . (143)
⊆
 0, otherwise
Then,ifI
n×n
≼ 1(L) ≼ L sur,wealso haveI
n×n
≼ 1(L−1) ≼ L sur.
Proof: SeeAppendixB.6.
Reducing to equivalent optimization problems. We investigate the optimization problems in
Algorithms2and3. Notethatifa im(G),thereexistssomeb Rn suchthata = G b. Also,
∈ ∈ ·
recall(53)that
s(Z) sm(Z) = G⊤ s (X) sm(X) , m [n], whereX = G Z , (144)
X X
− · − ∀ ∈ ·
whichimpliesthat,bypre-mult(cid:2)iplyingbothsides(cid:3)byb,wehave
b⊤ s(Z) sm(Z) = (G b)⊤ s (X) sm(X) = a⊤ s (X) sm(X) . (145)
X X X X
· − · · − · −
Then,byd(cid:2)efiningB = A(cid:3) (G†)⊤fo(cid:2)rallj t+1,...(cid:3),n atste(cid:2)ptofAlgorithm2(cid:3),thefeasible
πj πj·
∈ { }
set correspondsto ˜ ≜ Rn span(B⊤ ,...,B⊤ ). Hence,foranyset [n],
Ft Ft
\
πt+1 πn
M ⊆
min E a⊤ s (X) sm(X) (146)
X X
a∈Ft
m (cid:88)∈M (cid:20)(cid:12)
(cid:12)
·
(cid:2)
−
(cid:3)(cid:12) (cid:12)(cid:21)
isequivalenttotheoptimizationproblem (cid:12) (cid:12)
min E b⊤ s(Z) sm(Z) . (147)
b∈F˜
tm (cid:88)∈M (cid:20)(cid:12)
(cid:12)
·
(cid:2)
−
(cid:3)(cid:12) (cid:12)(cid:21)
Furthermore, in the algorithm steps, we are(cid:12) only interested in w(cid:12)hether the optimal value of the
optimizationproblemsin(146)and(147)arezero. Next,usingProposition2,foranyb Rn and
∈
m [n],wehave
∈
z∗ Rn : b⊤ s(z∗) sm(z∗) = 0 E b⊤ s(z∗) sm(z∗) > 0. (148)
∃ ∈ · − ̸ ⇐⇒ · −
(cid:20)(cid:12) (cid:12)(cid:21)
Therefore,forany
(cid:2) [n],theterms(cid:3) in(146)and(147)a(cid:12)rezer(cid:2)
oifandonlyif
(cid:3)(cid:12)
(cid:12) (cid:12)
M ⊆
b ˜ : b⊤ s(z) sm(z) = 0, z Rn, m . (149)
t
∃ ∈ F · − ∀ ∈ ∀ ∈ M
(cid:2) (cid:3)
51VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
B.3.1 PROOF OF LEMMA 7
Weusetheequivalencyrelationshipsin(147)and(149)toprovethestatementsinLemma7. Recall
that denotesthesetofunorderednodesinsteptofAlgorithm2. Setting = k ,weknow
t t
V M V \{ }
thattheoptimalvalueoftheproblemspecifiedin(32)iszeroifandonlyif
b ˜ : b⊤ s(z) sm(z) = 0, z Rn, m k . (150)
t t
∃ ∈ F · − ∀ ∈ ∀ ∈ V \{ }
First,notethatthelemmastatem(cid:2)ent,Iπ = (Iπ1(cid:3) ,...,Iπn)isavalidcausalorder,isequivalenttothe
statement
de(Iπt) Iπi+1,...,Iπn , t [n], (151)
⊆ { } ∀ ∈
inwhichde(i)denotesthedescendantsofnodeiasspecifiedin(5). Weprovethestatementin(151)
byinduction. Asanauxiliaryresult,wealsodefineandprovethefollowingstatement:
0, t > j
1 A ·(G†)⊤
πt,Iπj
= 1 B πt,Iπj =
(cid:40)1, t = j
,
∀
t,j
∈
[n]. (152)
(cid:104) (cid:105)
(cid:8) (cid:9) (cid:8) (cid:9)
At the base case t = n, consider some k [n]. Note that
n
= IVn = 1,...,n . Hence, IVn
∈ V { }
is an ancestrally closed set, based on which we can use Lemma 6 as follows. If Ik has a child in
[n] Ik ,then
\{ }
s sm : m k = n. (153)
n
R − ∈ V \{ }
(cid:16) (cid:17)
Since ˜ = Rn span( ),(153)im(cid:8) pliesthat(150)doesno(cid:9) thold,andsolving(32)doesnotyielda
n
F \ ∅
zerovalue. Ontheotherhand,ifIk doesnothaveachildin[n] Ik ,then
\{ }
s sm : m k = n 1, (154)
n
R − ∈ V \{ } −
(cid:16) (cid:17)
(cid:8) (cid:9)
which implies that (150) has a unique solution b∗ (up to scaling) that has zeros in all coordinates
except Ik. Since b∗ contains at least one non-zero element, b∗ = 0. Next, as the induction
Ik ̸
hypothesis,assumethatforu t+1,...,n ,wehave
∈ { }
0, k π
u u
∈ V \{ }
de(Iπu) Iπu+1,...,Iπn , and 1 B = 1, k = π . (155)
⊆ { }
πu,Ik  u
 free, otherwise
(cid:8) (cid:9)

We will show that u = t also satisfies these conditions, andby induction, (155) will be satisfied
for all u [n], which implies that (151) and (152) are satisfied. To show this, note that IVt is
∈
ancestrallyclosedsince(155)holdsforallu t+1,...,n . Thisisbecauseanyk IVt cannot
∈ { } ∈
beadescendantofanodein Iπu+1,...,Iπn . Then,weuseLemma6asfollows. IfIk hasachild
{ }
inIVt,wehave
s sm : m k = t. (156)
t
R − ∈ V \{ }
(cid:16) (cid:17)
Alsonotethat,sinceIVt isancestra(cid:8)llyclosed,byLemma3,(cid:9)
s(Z) sm(Z) = 0, ℓ / IVt . (157)
− ℓ ∀ ∈
(cid:2) (cid:3)
52SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Then,anyvectorinim s sm : m
t
k cancontainnon-zerosonlyinthecoordinatesIVt.
{ − ∈ V \{ }}
Since the dimension of this image is t, and IVt = t, a solution of (150) must have zeros in
(cid:0) | (cid:1) |
coordinates IVt. However, such a vector falls into span(B⊤ ,...,B⊤ ) due to the induction
πt+1 πn
hypothesisthat(155)holdsforu t+1,...,n . Therefore,itisnotinthefeasibleset ˜. Hence,
t
∈ { } F
(150)doesnothold,whichmeansthatsolving(32)doesnotyieldazerovalue. Next,ifIk doesnot
haveachildinIVt,
s sm : m k = t 1. (158)
t
R − ∈ V \{ } −
(cid:16) (cid:17)
Since IVt\{k} is ancestrally clos(cid:8)ed, any vector in im (cid:9) s sm : m
t
k can contain
{ − ∈ V \ { }}
non-zerosonlyinthecoordinatesIVt\{k}, andasolutionof(150)musthavezerosincoordinates
(cid:0) (cid:1)
IVt\{k}. Therefore,anysolutionb∗ to(150)satisfiesthatb∗ = 0forℓ IVt\{k} andb∗ = 0. Note
ℓ ∈ Ik ̸
that(150)isvalidsincethereisasolutionb∗ forwhich
1 ifℓ = k ,
b∗ = . (159)
ℓ
(cid:40)0 otherwise
Subsequently,forπ = k,(155)issatisfiedforu = t. Therefore,byinduction,(155)iscorrectfor
t
allu 1,...,n ,whichcompletestheproofof(151)and(152),andconcludesthelemma.
∈ { }
B.3.2 PROOF OF LEMMA 8
Weusetheequivalencyrelationshipsin(147)and(149)toprovethestatementsinLemma8. Recall
that isdefinedas dˆe(π ) π inAlgorithm3. Setting = ,weknowthatthe
t,j j t t t,j
M V \{ ∪{ }} M M
optimalvalueoftheproblemspecifiedin(33)iszeroifandonlyif
b ˜ : b⊤ s(z) sm(z) = 0, z Rn, m . (160)
t t,j
∃ ∈ F · − ∀ ∈ ∀ ∈ M
First, note that the transitive clo(cid:2)sures of two g(cid:3)raphs are the same if and only if their transitive
reductionsarethesame. Then,(C1)inLemma8isequivalentto
π pˆa (π ) Iπt pa (Iπj), t,j [n], (161)
t ∈ tr j ⇐⇒ ∈ tr ∀ ∈
in which pˆa and pa denotes the parents in transitive reduction graphs ˆ and respectively.
tr tr Gtr Gtr
Wealsogivethefollowingstatement,whichwewillshowisequivalentto(C2)inLemma8.
0, Iπt / an(Iπj)
1 A ·(G†)⊤
πt,Iπj
= 1 B πt,Iπj =
(cid:40)1, t
=∈
j
, ∀t,j
∈
[n]. (162)
(cid:104) (cid:105)
(cid:8) (cid:9) (cid:8) (cid:9)
Weprovethedesiredresults(161)and(162)byinduction. Atthebasestep, considert = n 1,
−
andj = n. Then,weminimizethescorevariationsovertheset
= π = [n] π . (163)
n−1,n n n−1 n−1
M V \{ } \{ }
Since(Iπ1,...,Iπn)isavalidcausalorder,theonlypossiblepathfromIπn−1 toIπn isthepossible
edgebetweenthem. NotethatIVn isanancestrallyclosedset. Therefore,wecanuseLemma6as
follows. IfIπn−1 pa(Iπn),
∈
s sm : m = n. (164)
n−1,n
R { − ∈ M }
(cid:0) (cid:1)
53VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Subsequently,(160)doesnothold,whichmeansthatsolving(33)doesnotyieldazerovalue. On
theotherhand,ifIπn−1 / pa(Iπn),byLemma6,
∈
s sm : m = n 1, (165)
n−1,n
R { − ∈ M } −
which implies that (160) has a(cid:0)unique solution b∗ (up to s(cid:1)caling) that has zeros in all coordinates
except Iπn−1. Next, as the induction hypothesis, assume that for all v t + 1,...,n , and
∈ { }
j v+1,...,n ,theidentifiededgesandminimizerssatisfy
∈ { }
0, Iπv / an(Iπ k)
π v
∈
pˆa tr(π j)
⇐⇒
Iπv
∈
pa tr(Iπj), and 1 {B πv,Iπk
}
=
(cid:40)1, v
=∈
k
. (166)
Wewillshowthatv = tandj t+1,...,n alsosatisfytheseconditions,andbyinduction,(166)
∈ { }
will be satisfied for all t [n] and j t + 1,...,n , which implies that (161) and (162) are
∈ ∈ { }
satisfied.
Now,weprovethat(166)issatisfiedforv = tandj t+1,...,n byinductionasfollows.
∈ { }
At the base step, consider j = t + 1. Since Iπ1,...,Iπn is a valid causal order, we have
{ }
Iπt
∈
pa tr(Iπt+1)ifandonlyifIπt
∈
pa(Iπt+1). Algorithm3hasnotassignedanychildrentoπ
t
in
ˆyet. Hence,
t,t+1
= V
t+1
π
t
. SinceIVt+1 isancestrallyclosed,weuseLemma6asfollows.
G M \{ }
IfIπt pa(Iπt+1),
∈
s sm : m = = t+1. (167)
t,t+1 t+1
R { − ∈ M } |V |
Alsonotethat,sinceIVt+1 i(cid:0)sancestrallyclosed,byLe(cid:1)mma3,
s(Z) sm(Z) = 0, ℓ / IVt+1 . (168)
− ℓ ∀ ∈
Then, any vector in im( s (cid:2)sm : m (cid:3) ) can contain non-zeros only in the coordinates
t,t+1
{ − ∈ M }
IVt+1. Sincethedimensionofthisimageist+1and IVt+1 = t+1, thesolutionof(160)must
| |
havezerosincoordinatesIVt+1. However,suchavectorfallsintospan(B⊤ ,...,B⊤ )duetothe
πt+1 πn
inductionhypothesisthat(166)holdsforv t+1,...,t . Therefore,itisnotinthefeasibleset
∈ { }
˜. Hence,(160)doesnothold,whichmeansthatsolving(33)doesnotyieldazerovalue. Onthe
t
F
otherhand,ifIπt / pa(Iπt+1),byLemma6,wehave
∈
s sm : m = t. (169)
t,t+1
R { − ∈ M }
(169)impliesthat(160)hasauniqu(cid:0)esolutionb∗ (uptoscalin(cid:1)g)thathaszerosinIMt,t+1 coordinates.
Notethatb∗ musthaveatleastonenon-zeroelementincoordinatesIVt tobeinthefeasibleset ˜ t.
F
Sinceπ istheonlynodeinV ,wehaveb∗ = 0,whichconcludesthebasestep.
t t \Mt,t+1 Iπt
̸
Next,astheinductionhypothesis,assumethat(166)holdsforv = tandj t+1,...,u 1
∈ { − }
such that, if Iπt
∈
pa tr(Iπj), then we have π
t
∈
pˆa(π j). Note that by the induction hypothesis
and using the fact that all ancestral relationships can be read off from , we have identified all
tr
G
descendantsofIπt in Iπt+1,...,Iπu−1 . Then,weuseLemma6toprovethat(166)holdsforv = t
{ }
andj = uasfollows. IfIπt pa (Iπu), thenthereisnoothercausalpathfromIπt toIπu in .
∈ tr G
Hence,wehave
de(Iπt) an(Iπu) = , and an(Iπt) IMt,u , (170)
∩ ∅ ⊆
which implies that IMt,u Iπt is ancestrally closed. Since Iπt has a children in IMt,u, using
∪
Lemma6wehave
s sm : m = +1, (171)
t,u t,u
R { − ∈ M } |M |
(cid:0) (cid:1)
54SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
whichimpliesthatthesolutionof(160)haszerosincoordinatesIMt,u Iπt. However,thisisnota
∪
feasiblesolutionsincesuchavectorfallsintospan(B⊤ ,...,B⊤ ). Therefore,ifIπt pa (Iπu),
πt+1 πn ∈ tr
solving(33)doesnotyieldazerovalueandAlgorithm3addsπ π to ˆ. Thiscompletesthe
t u
→ G
inductionstepandwehaveconcludedthatthecondition(161)issatisfiedforallt [n]. Thatis,if
∈
Iπj
∈
pa tr(Iπt),thenπ
j
∈
pˆa(π t). Next,ifIπt ∈/ an(Iπu),weknowthatIMt,u ∪Iπu isancestrally
closed. SinceIπt hasnochildinIMt,u,usingLemma6wehave
s sm : m = . (172)
t,u t,u
R { − ∈ M } |M |
Thisimpliesthatthereexistsaso(cid:0) lutionb∗ to(160)andit(cid:1) satisfiesb∗ = 0forℓ IMt,u andb∗ = 0.
ℓ ∈ Iπt ̸
Thisconcludesthatthecondition(162)issatisfiedforallt,j. NotethatAlgorithm3doesnotaddan
edgeto ˆinthiscase.
G
Tosummarize,wehaveshownthat(i)ifIπt / an(Iπj),i.e.,Iπt / pa (Iπj),Algorithm3does
∈ ∈ tc
notaddtheedgeπ
t
→
π
j
to Gˆ,and(ii)ifIπt
∈
pa tr(Iπj),thealgorithmaddstheedgeπ
t
→
π
j
to
ˆ. Therefore,thetransitivereductions(orclosures)of and ˆarerelatedbypermutation . Finally,
G G G I
notethat(162)impliesthat
A G† ⊤ = P U, (173)
I
· ·
(cid:0) (cid:1)
forsomeuppertriangularmatrixUsuchthatitsdiagonalentriesarenon-zeroandforalli / an(j)
∈
we have U = 0. Subsequently, taking the inverse transpose of both sides and substituting
i,j
H∗ = (A⊤)†,weobtain
H∗ G = P L, (174)
I
· ·
forsomelowertriangularmatrixL. Notethat,usingLemma15,diagonalentriesofLarenon-zero
andj / an(i)alsoimpliesL = 0. Hence,H∗ satisfiesmixingconsistencyuptoancestorssince
i,j
∈
Zˆ(X;H∗) = H∗ G Z = P L Z , (175)
I
· · · ·
inwhichLsatisfiesthepropertiesofthematrixC inLemma8.
an
B.4 ProofofLemma9
Recall that is defined as cˆh(π ) π in Algorithm 3. Using (149), we know that
t,j j t t
M V \{ ∪{ }}
solving(33)yieldsazerovalueifandonlyif
b ˜ : b⊤ s(z) sm(z) = 0, z Rn, m . (176)
t t,j
∃ ∈ F · − ∀ ∈ ∀ ∈ M
(cid:2) (cid:3)
Next,considersomem . ByLemma3,
t,j
∈ M
s(Z) sm(Z) = 0, ℓ pa(Im). (177)
− ℓ ∀ ∈
(cid:2) (cid:3)
Thismeansthat,forallm ,
t,j
∈ M
b⊤ s(z) sm(z) = 0 b s(z) sm(z) = 0, z Rn . (178)
· − ⇐⇒ ℓ · − ℓ ∀ ∈
ℓ∈pa(Im)
(cid:2) (cid:3) (cid:88) (cid:2) (cid:3)
55VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Next,Assumption2ensuresthatimageof(s sm)contains pa(Im) linearlyindependentvectors.
− | |
Therefore,fortheright-handside(RHS)of(178)toholdforallm ,wemusthave
t,j
∈ M
b = 0, ℓ pa(IMt,j). (179)
ℓ
∀ ∈
Wewillinvestigatewhethersuchavectorbisinthefeasibleset ˜,andprovethedesiredresultby
t
F
induction. First,notethatthestatement(C3)inLemma9isequivalentto
π pˆa(π ) Iπt pa(Iπj), t,j [n], (180)
t j
∈ ⇐⇒ ∈ ∀ ∈
andthestatement(C4)inLemma9isequivalentto
0, Iπt / sur(Iπj)
1 A ·(G†)⊤
πt,Iπj
= 1 B πt,Iπj =
(cid:40)1, i
=∈
j
, ∀t,j
∈
[n], (181)
(cid:104) (cid:105)
(cid:8) (cid:9) (cid:8) (cid:9)
exceptfortheMarkovpropertyclaimwhichwewillproveintheend. Weprove(180)and(181)by
inductionasfollows. Atthebasecase,considert = n 1andj = n. Then,weminimizethescore
−
variationsovertheset
= π = [n] π . (182)
n−1,n n n−1 n−1
M V \{ } \{ }
IfIπn−1 pa(Iπn),usingLemma6,wehave
∈
s sm : m = n. (183)
n−1,n
R { − ∈ M }
Subsequently,(176)doesnothold(cid:0),whichmeansthatsolvin(cid:1)g(33)doesnotyieldazerovalue. On
theotherhand,ifIπn−1 / pa(Iπn),
∈
s sm : m = n 1, (184)
n−1,n
R { − ∈ M } −
which implies that (160) has a(cid:0)unique solution b∗ (up to s(cid:1)caling) that has zeros in all coordinates
exceptIπn−1. Inthiscase,sinceIπn isaleafnodein ,wehaveIπn−1 sur(Iπn). Next,asthe
G ∈
inductionhypothesis,assumethatforallv t+1,...,n ,andj v+1,...,n ,theidentified
∈ { } ∈ { }
edgesandminimizerssatisfy
0, Iπv / sur(Iπ k)
π v
∈
pˆa(π j)
⇐⇒
Iπv
∈
pa(Iπj), and 1 B πv,Iπk =
(cid:40)1, v
=∈
k
. (185)
(cid:8) (cid:9)
(185)impliesthatanyvectorb ˜
t
hasatleastonenon-zeroentryincoordinates Iπ1,...,Iπt .
∈ F { }
Otherwise,bfallsintospan(B⊤ ,...,B⊤ ),whichisexcludedfromthefeasibleset. Wewillshow
πt+1 πn
thatv = tandj t+1,...,n alsosatisfytheconditionsin(185),andbyinduction,(185)will
∈ { }
besatisfiedforallt [n]andj t+1,...,n ,whichimpliesthat(180)and(181)aresatisfied.
∈ ∈ { }
Now,weprovethat(185)issatisfiedforv = tandj t+1,...,n byinductionasfollows.
∈ { }
At the base step, consider j = t+1. Algorithm 3 has not assigned any children to π in ˆyet.
t
G
Hence,
t,t+1
=
t+1
π t. IfIπt pa(Iπt+1),wehave
M V \ ∈
pa(IMt,t+1) = IVt+1 . (186)
Thismeansthatasolutionof(176)mustsatisfy
b = 0, ℓ pa(IVt+1). (187)
ℓ
∀ ∈
56SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
However,suchavectorfallsintospan(B⊤ ,...,B⊤ )duetotheinductionhypothesisthat(185)
πt+1 πn
holdsforv t+1,...,t . Therefore,itisnotinthefeasibleset ˜. Hence,(176)doesnothold,
t
∈ { } F
whichmeansthatsolving(33)doesnotyieldazerovalueandwecorrectlyhaveπ π in ˆ. On
t t+1
→ G
theotherhand,ifIπt / pa(It+1),wehave
∈
pa(IMt,t+1) = IVt+1\{πt} . (188)
Thismeansthatasolutionof(176)existsanditsatisfies
b Iπt = 0, and b ℓ = 0, ℓ IVt+1\{πt} . (189)
̸ ∀ ∈
Next,astheinductionhypothesis,assumethat(185)holdsforv = tandj t+1,...,u 1
∈ { − }
such that, if Iπt pa(Iπj), then we have π
t
pˆa(π j). We determine whether Iπt pa(Iπu) as
∈ ∈ ∈
follows. NotethatIMt,u contains Iπ1,...,Iπt−1 bythedefinitionof t,u. IfIπt pa(Iπu),we
{ } M ∈
have
Iπt pa(IMt,t+1), (190)
∈
which implies that (176) does not hold. Hence, solving (33) does not yield a zero value and we
correctly have π
t
π
u
in ˆ. On the other hand, if Iπt / pa(Iπu), then Iπt is not contained in
→ G ∈
pa(IMt,u). Hence,asolutionof(176)existsanditsatisfies
b Iπt = 0, and b ℓ = 0, ℓ IMt,u . (191)
̸ ∀ ∈
NotethatAlgorithm3doesnotaddanedgeto ˆinthiscase. Tosummarize,wehaveshownthat(i)
G
ifIπt / pa(Iπj),thealgorithmdoesnotaddtheedgeπ
t
π
j
to ˆ,and(ii)ifIπt pa(Iπj),the
∈ → G ∈
algorithmaddstheedgeπ π to ˆ. Therefore, and ˆarerelatedthroughagraphisomorphism
t j
→ G G G
bypermutation ,hence(C3)inLemma9issatisfied.
I
Finally,weprove(181)tocompletetheinductionstep. Denotetheyoungestnon-childrenofπ
t
in ˆbyπ . SinceIπ isavalidcausalorder,thisimpliesthat
k
G
Iπt pa(Iπj), j k+1,...,n . (192)
∈ ∀ ∈ { }
Note that A , or equivalently B , is finalized in the step of j = k of Algorithm 3. Note that
πt πt
pa( t,k)containsallnon-childrenofIπt in . Furthermore,ifIπt pa(Iπj)butIπt / sur(Iπj),
M G ∈ ∈
then there exists some ℓ [n] for which Iπ ℓ ch(Iπj) and Iπ ℓ / ch(Iπt). In this case, Iπj
∈ ∈ ∈ ∈
Ipa(M t,k),andwehaveB πt,Iπj = 0,whichcompletestheproofof(181). Notethat(181)implies
A G† ⊤ = P U, (193)
I
· ·
forsomeuppertriangularmatrixUthatha(cid:0)snon(cid:1)-zerodiagonalentriesandforalli / sur(j)wehave
∈
U = 0. Subsequently,takingtheinversetransposeofbothsidesandsubstitutingH∗ = (A⊤)†,
i,j
weobtain
H∗ G = P L, (194)
I
· ·
forsomelowertriangularmatrixL. Notethat,usingLemma15,diagonalentriesofLarenon-zero
andj / sur(i)impliesL = 0. Hence,H∗ satisfiesmixingconsistencyuptosurroundingparents
i,j
∈
since
Zˆ(X;H∗) = H∗ G Z = P L Z , (195)
I
· · · ·
inwhichLsatisfiesthepropertiesofC inLemma9.
sur
57VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Markov property of the mixing with surrounding parents. We investigate the properties of
the mixing consistency up to surrounding parents further to prove that Zˆ is Markov with respect
to ˆ. Let ρ be the permutation that maps 1,...,n to , i.e., Iρi = i for all i [n]. Then, by
G { } I ∈
pre-multiplyingbothsidesof(195)byP⊤ = P ,andpluggingC forL,weobtain
I ρ sur
P Zˆ = C Z . (196)
ρ sur
· ·
RecallthefollowingSCMspecifiedin(6)
Z = f (Z ,N ). (197)
i i pa(i) i
Sincesur(i) pa(i),foreachi [n],wehave
⊆ ∈
Zˆ = [C ] Z + [C ] Z (198)
ρi sur i,i
·
i
·
sur i,j j
j∈sur(i)
(cid:88)
= [C ] f (Z ,N )+ [C ] Z , (199)
sur i,i i pa(i) i sur i,j j
· ·
j∈sur(i)
(cid:88)
whichisafunctionofZ andN . WeneedtospecifyZ intermsofZˆ toproveMarkov
pa(i) i ρi pˆa(ρi)
property. Toprovethis,itsufficestoshowthatforanyk pa(i),Z isafunctionofZˆ . Using
∈
k pˆa(ρi)
(196),wehave
Z = C−1 P Zˆ . (200)
sur· ρ ·
By Lemma 15, we know that [C−1] = 0 if i / sur(j) for distinct i and j. Subsequently, as
sur i,j ∈
counterpartof(198),wehave
Z = [C−1] Zˆ + [C−1] Zˆ . (201)
k sur k,k · ρ k sur k,j · ρj
j∈sur(k)
(cid:88)
Since k pa(i), we have ρ pa(ρ ) and Zˆ is in Zˆ . Note that if j sur(k), j is also in
∈ k ∈ i ρ k pˆa(ρi) ∈
pa(i). Therefore,everytermintheRHSof(201)belongstoZˆ andZ isafunctionofZˆ .
pˆa(ρi) k pˆa(ρi)
Then,using(199),weknowthatZˆ isafunctionofonlyZˆ andN ,whichconcludestheproof
ρi pˆa(ρi) i
thatZˆ isMarkovwithrespectto ˆ.
G
B.5 ProofofLemma10
Proofofthescalingconsistency. First,notethatfortheoutputofAlgorithm3,wehave
Zˆ(X;H∗) = P L Z , (202)
I
· ·
forsomelowertriangularmatrixLsuchthatthediagonalentriesofLarenon-zeroandj / an(i)
∈
impliesL = 0. OurgoalistofindanunmixingmatrixVsuchthat
i,j
Zˆ(V H∗) = V P L Z (203)
I
· · · ·
satisfiesthescalingconsistency. Tothisend,ourgoalistoconstructVthatsatisfies
1 V P L = P . (204)
I I
{ · · }
58SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
NotethatV = P L−1 P⊤ satisfies(204). Furthermore,sinceIπ isavalidcausalorder,weknow
I · · I
thatthedesiredmatrixV shouldhavenon-zerodiagonalentriesandsatisfyV = 0fori < j.
πi,πj
WewillprovethatAlgorithm4achievesthissolutionuptoscaling,andsubsequently,Lemma10
holds. Forthispurpose,weusethepropertiesofhardinterventionsasfollows. Weknowthat,under
hardinterventions,inenvironment m wehave
E
Zm Zm , j nd(Im), (205)
Im j
⊥⊥ ∀ ∈
in which nd(Im) denotes the set of non-descendants of Im. Also recall that (Iπ1,...,Iπn) is a
validcausalorderwhichimpliesthatnd(Iπ k)contains Iπ1,...,Iπ k−1 . Hence, forV tosatisfy
{ }
1 V P L = P ,thetermZ˜ ≜ Zˆ(V H∗)needstosatisfy
I I
{ · · } ·
Z˜π k Z˜π k, j 1,...,k 1 . (206)
π
k ⊥⊥
πj
∀ ∈ { − }
Thisindependencerelationimpliesthat
Cov Z˜π k,Z˜π k = 0, j [k 1]. (207)
π
k
πj
∀ ∈ −
(cid:0) (cid:1)
Weleveragethisproperty andusethe covarianceasa surrogateofindependence. Specifically, at
each step of the algorithm, we enforce the (k 1)-dimensional covariance vector corresponding
−
tonodeπ anditsnondescendantstobezero. Intherestoftheproof,weshowthatthisprocedure
k
eliminatesmixingandachievesscalingconsistency.
SinceweknowthatourtargetVhasnon-zerodiagonalentriesandsatisfiesV = 0fori < j,
πi,πj
we start with the initialization V = I . When learning the row vector V , we are effectively
n×n π
k
estimatingonlyentriesV
π k,j
forj
∈
{π 1,...,π
k−1
}. First,considerk = 1. UsingthefactthatIπ1
isarootnode,theinitializationV = I yields
n×n
1 if j = Iπ1 ,
1 V P L = . (208)
{ · I · } π1,j (cid:40)0 otherwise
(cid:2) (cid:3)
Next,assumethatforallℓ 1,...,k 1 ,wehave
∈ { − }
1 if j = Iπ ℓ ,
1 V P L = . (209)
{ · I · } π ℓ,j (cid:40)0 otherwise
(cid:2) (cid:3)
Wewillprovethat(209)alsoholdsforℓ = k. Considerstepk ofAlgorithm4,inwhichweupdate
rowvectorV whilekeepingthepreviouslylearnedrowvectorsV ,...,V fixed. Then,for
π
k
π1 π
k−1
anycolumnvectorv Rn×1,wehave
∈
Cov v⊤ Zˆπ k,Z˜ = V P L Cov(Zπ k) L⊤ P⊤ v , j 1,...,k 1 , (210)
·
πj πj
·
I
· · · ·
I
· ∀ ∈ { − }
(cid:16) (cid:17) ≜Bj
(cid:124) (cid:123)(cid:122) (cid:125)
in which B R1×n is a row vector. Then, using (210) and concatenating thevectors B : j
j j
∈ { ∈
[k 1] toformmatrixB R(k−1)×n,weobtain
− } ∈
Cov v⊤ Zˆπ k, Z˜ ,...,Z˜ = B v . (211)
·
π1 π
k−1 ·
(cid:16) (cid:17)
(cid:2) (cid:3)
59VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Wearesearchingforavectorv thatmakesB v azerovector. NotethatCov(Zπ k)isafull-rank
·
matrixsincecausalrelationshipsarenotdeterministic. Also,byconstruction,
i [k 1] : V = 0 j π ,...,π . (212)
∃ ∈ −
πi,j
̸ ⇐⇒ ∈ {
1 k−1
}
Therefore,B R(k−1)×n isanunknownfull-rankmatrix. NotethatwecanobtainentriesofBby
∈
computing
B = Cov Zˆπ k,Z˜ , j, ℓ [k 1]. (213)
j,ℓ πj π ℓ ∀ ∈ −
Thisprocedurecorrespondstotheinne(cid:0)rforloop(cid:1)inAlgorithm4forwhich
A = B , j,ℓ [k 1]. (214)
j,ℓ j,π ℓ ∀ ∈ −
Then, A is also a full-rank matrix, since it is a principal submatrix of the following symmetric
positivesemidefinitematrix
V (P L) Cov(Zπ k) (P L)⊤ V⊤ . (215)
I I
· · · · · ·
Wealsoobtainπ -thcolumnofB,denotedbybinAlgorithm4,asfollows
k
b ≜ Cov Zˆπ k,[Z˜ ,...,Z˜ ] . (216)
π k π1 π k−1
Recallthatwearesearchingforavectorv(cid:0) Rn×1 withtheprop(cid:1)ertiesv = 0,andv = 0forall
∈
π
k ̸
πj
j > k suchthatB v becomesazerovector. Theseconditionsrequirethat
·
v
π1
B v = A b  . . .  = 0(k−1)×1 . (217)
· ·
v
(cid:2) (cid:3)  π k
 
Theidentityin(217)hasauniquesolutionuptoscaling. Thatis,byfixingv = 1,wehave
π
k
v
π1
 . . .  = A−1 b. (218)
− ·
v
π
 k−1
 
NotethatAlgorithm4sets
V = A−1 b = v , j [k 1]. (219)
π k,πj
− ·
πj
∀ ∈ −
Hence,thealgorithmhasfoundauniquesolution(uptoscaling)
V = P L−1 P⊤ . (220)
π k I · · I π k
Therefore,byinduction,wehave (cid:2) (cid:3)
V = P L−1 P⊤ , (221)
I I
· ·
whichsatisfies1 V P L = P . Finally,using1 V P L = P in(203),weobtain
I I I I
{ · · } { · · }
Zˆ(V H∗) = C P Z , (222)
s I
· · ·
inwhichC isadiagonalmatrix,andtheproofiscomplete.
s
60SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
ProofoftheperfectDAGrecovery. Weshowthatthegraphconstructionprocessin(34)achieves
perfectDAGrecoveryasfollows. First,usingLemma4and(222),wehave
s (Zˆ;h) sm(Zˆ;h) = (C P )−⊤ s(Z) sm(Z) . (223)
Zˆ − Zˆ s · I · −
(cid:2) (cid:3)
Subsequently,usingthefactthatC isadiagonalmatrix,weobtain
s
s (Zˆ;h) sm(Zˆ;h) = 0 s(Z) sm(Z) = 0. (224)
Zˆ − Zˆ i ̸ ⇐⇒ − Ii ̸
(cid:2) (cid:3) (cid:2) (cid:3)
Then,usingLemma3,wehave
Ii pa(Im) E s(Z) sm(Z) = 0 (225)
∈ ⇐⇒ − ρi ̸
(cid:20)(cid:12) (cid:12)(cid:21)
s (cid:12)((cid:2) Zˆ;h) sm(Zˆ;(cid:3) h)(cid:12) = 0 (226)
⇐⇒ Zˆ(cid:12) − Zˆ (cid:12)i ̸
i(cid:2) pˆa(m) m , (cid:3) (227)
⇐⇒ ∈ ∪{ }
whichconcludestheproofthat ˆand arerelatedthroughagraphisomorphismbypermutation ,
G G I
whichdenotestheinterventionorder.
B.6 ProofofLemma15
First,notethatL ≼ L . Hence,westartwithconsideringagenericlowertriangularmatrixL
sur an
suchthatI ≼ 1(L) ≼ L . MatrixLcanbedecomposedas
n×n an
L = E (I +Λ), (228)
n×n
·
in which E is a diagonal matrix and Λ is a strictly upper triangular matrix that satisfies 1(Λ) ≼
1(L ). Subsequently,sinceEisadiagonalmatrix,wehave
an
L−1 = (I +Λ)−1 E−1 , and 1 L−1 = 1 (I +Λ)−1 . (229)
n×n n×n
· { }
(cid:8) (cid:9)
Note that Λ is a strictly upper triangular n n matrix, which implies that Λn is a zero matrix.
×
Therefore,theinverseof(I +Λ)canbeexpandedas
n×n
(I +Λ)−1 = I Λ+Λ2 +( 1)n−1Λn−1 . (230)
n×n n×n
− −··· −
If I + Λ −1 = 0 for some i,j [n] with i < j, by (230) we have [Λk] = 0 for some
n×n i,j ̸ ∈ i,j ̸
k [n 1]. ExpandingmatrixΛk yieldsthat[Λk] isequaltothesumoftheproductswithkterms,
(cid:2) (cid:3) i,j
∈ −
i.e.,
[Λk] = [Λ] [Λ] ...[Λ] . (231)
i,j i,a1 a1,a2 a k−1,j
i<a1<· (cid:88)··<a k−1<j
Therefore,if[Λk] = 0,thereexistsasequenceofentries [Λ] ,[Λ] ,...,[Λ] inwhich
i,j
̸
i,a1 a1,a2 a k−1,j
alltermsarenon-zero. Next,weprovethetwocasesasfollows.
(cid:0) (cid:1)
61VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Case1: Since1(Λ) ≼ 1(L ),wehave[L ] = [L ] = = [L ] = 1. Bythe
an an i,a1 an a1,a2
···
an a k−1,j
definitionofL ,thismeansthat
an
de(j) de(a ) de(a ) de(i), (232)
k−1 1
⊆ ⊂ ··· ⊂ ⊂
whichalsoimpliesthat[L ] = 1. Therefore,weconcludethat
an i,j
(I +Λ)−1 = 0 = [L ] = 1. (233)
n×n i,j ̸ ⇒ an i,j
Hence,I ≼ 1(L−1) ≼(cid:2) L . (cid:3)
n×n an
Case2: Since1(Λ) ≼ 1(L ),wehave[L ] = [L ] = = [L ] = 1. Bythe
sur sur i,a1 sur a1,a2
···
sur a k−1,j
definitionofL ,thismeansthat
sur
ch(j) ch(a ) ch(a ) ch(i), (234)
k−1 1
⊆ ⊂ ··· ⊂ ⊂
whichalsoimplies[L ] = 1sincech(j) ch(i). Therefore,weconcludethatif
sur i,j
⊆
(I +Λ)−1 = 0 = [L ] = 1. (235)
n×n i,j ̸ ⇒ sur i,j
Hence,I ≼ 1(L−1) ≼(cid:2) L . (cid:3)
n×n sur
B.7 ProofofTheorems1,2,and3
Proof of Theorem 1. Note that under Assumption 1, Lemmas 7 and 8 hold for both soft and
hard interventions. Continuing from the guarantees of Theorem 2, Lemma 10 shows that Algo-
rithm4outputssatisfyperfectDAGrecoveryandscalingconsistency. Hence,Algorithm1achieves
IdentifiabilityguaranteesforhardinterventionsstatedinTheorem1.
Proof of Theorem 2. Under Assumption 1 for linear transformations, Lemma 7 shows that
Algorithm2returnsapermutationπ suchthatIπ isacausalorder. Givensuchπ,Lemma8shows
thatAlgorithm3outputssatisfytransitiveclosurerecoveryandmixingconsistencyuptoancestors.
Hence,Algorithm1achievestheidentifiabilityguaranteesforsoftinterventionsstatedinTheorem2.
Proof of Theorem 3. Under Assumption 1 for linear transformations, Lemma 7 shows that
Algorithm2returnsapermutationπ suchthatIπ isacausalorder. Givensuchπ,Lemma9shows
thatAlgorithm3outputssatisfyperfectDAGrecoveryandmixingconsistencyuptosurrounding
parents. Hence,Algorithm1achievestheidentifiabilityguaranteesforsoftinterventionsstatedin
Theorem3.
AppendixC. ProofsoftheResultsforGeneralTransformations
Inthissection,wefirstproveidentifiabilityinthecoupledenvironmentsalongwiththeobservational
environmentcase(Theorem5). Then,weshowthattheresultcanbeextendedtocoupledenviron-
mentswithoutobservationalenvironment(Theorem6)anduncoupledenvironments(Theorem4).
Forconvenience,werecallthefollowingequationsfromthemainpaper. Foreachh we
∈ H
defineϕ ≜ h g. Then,Zˆ(X;h)andZ arerelatedas
h
◦
Zˆ(X;h) = h(X) = (h g)(Z) = ϕ (Z). (236)
h
◦
62SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Then,bysettingf = ϕ−1,Lemma4yields
h
between 0 and m : s (zˆ;h) sm(zˆ;h) = J (z) −⊤ s(z) sm(z) , (237)
E E Zˆ − Zˆ ϕ h · −
between 0 and ˜m : s (zˆ;h) s˜m(zˆ;h) = (cid:2)J (z)(cid:3)−⊤ (cid:2)s(z) s˜m(z)(cid:3), (238)
E E Zˆ − Zˆ ϕ h · −
between m and ˜m : sm(zˆ;h) s˜m(zˆ;h) = (cid:2)J (z)(cid:3)−⊤ (cid:2)sm(z) s˜m(z(cid:3)) . (239)
E E Zˆ − Zˆ ϕ h · −
(cid:2) (cid:3) (cid:2) (cid:3)
C.1 ProofofTheorem5
First,weinvestigatetheperfectrecoveryoflatentvariables.
Recoveringthelatentvariables. Werecoverthelatentvariablesusingonlythecoupledinterven-
tionalenvironments ( m, ˜m) : m [n] . Letρbethepermutationthatmaps 1,...,n to ,i.e.,
{ E E ∈ } { } I
Iρi = iforalli [n]andP
ρ
todenotethepermutationmatrixthatcorrespondstoρ,i.e.,
∈
1, m = ρ ,
i
[P ] = (240)
ρ i,m
(cid:40)0, else.
Sinceweconsidercoupledatomicinterventions,theonlyvaryingcausalmechanismbetween ρi
E
and ˜ρi isthatoftheintervenednodeinIρi = I˜ρi = i. Then,byLemma3(iii),wehave
E
E sm(Z) s˜m(Z) = 0 k = i. (241)
− k ̸ ⇐⇒
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
Werecallthedefinitionoftr(cid:12)uescorechangemat(cid:12)rixD
t
in(61)withentriesforalli,m [n],
∈
D ≜ E sm(Z) s˜m(Z) . (242)
t i,m − i
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:12)
Then,using(241),wehave
(cid:12) (cid:12)
1 D = P = P⊤ . (243)
t ρ I
{ }
Next, we show that the number of variations between the score estimates sm(zˆ;h) and s˜m(zˆ;h)
Zˆ Zˆ
cannotbelessthanthenumberofvariationsunderthetrueencoderg−1,thatisn = D . First,
∥ t ∥0
weprovidethefollowinglinearalgebraicproperty.
Proposition3 IfA Rn×n isafull-rankmatrix,thenthereexistsapermutationmatrixPsuchthat
∈
thediagonalelementsof P A arenon-zero.
·
Proof: SeeAppendixC.6.
Lemma11(ScoreChangeMatrixDensity) Foreveryh , thescorechangematrixD (h)is
t
∈ H
atleastasdenseasthescorechangematrixD associatedwiththetruelatentvariables,
t
D (h) D = n. (63)
∥ t ∥0 ≥ ∥ t ∥0
Proof: RecallthedefinitionofscorechangematrixD (h)in(35). Using(239),wecanwriteentries
t
ofD (h)equivalentlyas
t
D (h) = E J−⊤(Z) sm(Z) s˜m(Z) , i,m [n]. (244)
t i,m ϕ h i· − ∀ ∈
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
(cid:12) (cid:12)
63VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Since ϕ = h g is a diffeomorphism, J−⊤(z) is full-rank for all (h,z) Rn. Using
h ◦ ϕ h ∈ H ×
Proposition 3, for all (h,z), there exists a permutation π(h,z) of [n] with permutation matrix
(cid:2) (cid:3)
P (h,z)suchthat
1
P (h,z) J−⊤(z) = 0, i [n], where P (h,z) ≜ P . (245)
1 · ϕ h i,i ̸ ∀ ∈ 1 π(h,z)
(cid:104) (cid:105)
Next,recallthatinterventionaldiscrepancymeansthat,foreachi [n],thereexistsanullset R
i
∈ T ⊂
suchthat[sρi(z)]
i
= [s˜ρi(z)]
i
forallz
i i
(regardlessofthevalueofothercoordinatesofz).
̸ ∈ K\T
Then,thereexistsanullset Rn suchthat[sρi(z)]
i
= [s˜ρi(z)]
i
foralli [n]forallz Rn .
T ⊂ ̸ ∈ ∈ \T
WedenotethissetRn by asfollows:
\T Z
≜ z Rn : sρi(z) = s˜ρi(z) , i [n] . (246)
Z { ∈ i ̸ i ∀ ∈ }
Then,forallz ,h ,andi [n],(cid:2)wehav(cid:3)e (cid:2) (cid:3)
∈ Z ∈ H ∈
D (h) = E J−⊤(Z) sρi(Z) s˜ρi(Z) (247)
t πi(h,z),ρi ϕ h πi(h,z)· −
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
= E (cid:12) J−⊤(Z)] sρi(Z) s˜ρi(Z)(cid:12) , (248)
ϕ h πi(h,z),i · i − i
(cid:20)(cid:12) (cid:12)(cid:21)
inwhichπ (h,z)denotesthei-thelem(cid:12)e(cid:2) ntofthepermutatio(cid:2) nπ(h,z). Bythe(cid:3) de(cid:12)finitionofπ(h,z),
i (cid:12) (cid:12)
foranyz ,weknowthat J−⊤(z) = 0. Furthermore,bythedefinitionof ,wehave
∈ Z ϕ h πi(h,z),i ̸ Z
sρi(z) s˜ρi(z) = 0forz . Then,wehave D (h) = 0,whichimplies
− i ̸ ∈(cid:2) Z (cid:3) t πi(h,z),ρi ̸
(cid:2) (cid:3) 1 D (h) ≽ P⊤(h,z) P(cid:2) , h(cid:3) , z . (249)
t 1 · ρ ∀ ∈ H ∀ ∈ Z
Therefore, ∥D t(h) ∥0 ≥(cid:8) ∥P ρ ∥0(cid:9)= n for any h ∈ H, and the proof is concluded since we have
1 D = P .
t ρ
{ }
Thelowerboundforℓ normisachievedifandonlyif1 D (h) = P ,whichisanunknown
0 t ρ
{ }
permutation matrix. Since the only diagonal permutation matrix is I , the solution set of the
n×n
constrainedoptimizationproblemin(38)isgivenby
≜ h : 1 D (h) = I . (250)
1 t n×n
H ∈ H { }
Next,considersomefixedsolutionh∗(cid:8) . Dueto(249),wehav(cid:9)e
1
∈ H
1 D (h∗) = I ≽ P⊤(h∗,z) P , (251)
{ t } n×n 1 · ρ
whichimpliesthatwemusthaveP (h∗,z) = P forallz . Then,π (h∗,z) = ρ foralli [n].
1 ρ i i
∈ Z ∈
Wewillshowthatforalli = j,wehave
̸
J−1(z) = 0, z Rn (252)
ϕ h∗ j,ρi ∀ ∈
To show (252), first consider i =(cid:2)j, which(cid:3) implies [D (h∗)] = 0 since 1 D (h∗) = I .
̸
t ρi,ρj
{
t
}
n×n
Then,using(244),1 D (h∗) = I andLemma3(ii),wehave
t n×n
{ }
D (h∗) = E J−⊤(Z) sρj(Z) s˜ρj(Z) (253)
t ρi,ρj ϕ h∗ ρi · −
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
= E (cid:12) J−1(Z) sρj(Z) s˜ρj(Z)(cid:12) (254)
ϕ h∗ j,ρi · − j
(cid:20)(cid:12) (cid:12)(cid:21)
= 0.(cid:12)(cid:2) (cid:3) (cid:2) (cid:3) (cid:12) (255)
(cid:12) (cid:12)
64SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Notethat[sρj(z) −s˜ρj(z)]
j
̸= 0forallz
∈
Z. Hence,if[J ϕ− h1 ∗(z)]
j,ρi
isnon-zerooveranon-zero-
measuresetwithin ,then[D (h∗)] wouldnotbezero. Therefore, J−1(z) = 0onaset
Z t ρi,ρj ϕ h∗ j,ρi
ofmeasure1. SinceJ−1 isacontinuousfunction,thisimpliesthat
ϕ h∗ (cid:2) (cid:3)
J−1(z) = 0, z Rn . (256)
ϕ h∗ j,ρi ∈
(cid:2) (cid:3)
Toseethis,supposethat[J−1(z∗)] > 0forsomez∗ . Duetocontinuity,thereexistsanopen
ϕ h∗ j,ρi ∈ Z
setincludingz∗ forwhich[J−1(z∗)] > 0,andsinceopensetshavenon-zeromeasure,wereacha
ϕ h∗ j,ρi
contradiction. Therefore,ifi = j,[J−1(z)] = 0forallz Rn. SinceJ−1(z)mustbefull-rank
forallz Rn,wehave
̸ ϕ h∗ j,ρi ∈ ϕ h∗
∈ [J−1(z)] = 0, z Rn , i [n]. (257)
ϕ h∗ i,ρi ̸ ∀ ∈ ∀ ∈
Then,foranyh∗
∈
H1,[Zˆ(X;h∗)]
ρi
= [ϕ h∗(Z)]
ρi
isafunctionofonlyZ i,andwehave
[Zˆ(X;h∗)]
ρi
= ϕ h∗(Z i), ∀i
∈
[n], (258)
whichconcludestheproof.
Recoveringthelatentgraph Considertheselectedsolutionh∗ . Weconstructthegraph ˆ
1
∈ H G
asfollows. Wecreatennodesandassignthenon-zerocoordinatesofρ -thcolumnofD(h∗)asthe
j
parentsofnodeρ in ˆ,i.e.,
j
G
pˆa(ρ ) ≜ ρ = ρ : [D(h∗)] = 0 , j [n]. (259)
j i
̸
j ρi,ρj
̸ ∀ ∈
Using(36)and(237),wehave (cid:8) (cid:9)
pˆa(ρ ) ( =36) ρ = ρ : E s (Zˆ;h∗) sρj(Zˆ;h∗) = 0 (260)
j i ̸ j Zˆ − Zˆ ρi ̸
(cid:26) (cid:20)(cid:12) (cid:12)(cid:21) (cid:27)
(2 =37) ρ = ρ : E (cid:12) (cid:12)(cid:2) J−⊤(Z) s(Z) s˜(cid:3) ρj((cid:12) (cid:12)Z) = 0 (261)
i ̸ j ϕ h∗ ρi · − ̸
(cid:26) (cid:20)(cid:12) (cid:12)(cid:21) (cid:27)
(cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
= ρ = ρ : E (cid:12)J−⊤(Z) s(Z) s˜ρj(Z) (cid:12) = 0 . (262)
i ̸ j ϕ h∗ ρi,i· − i ̸
(cid:26) (cid:20)(cid:12) (cid:12)(cid:21) (cid:27)
(cid:12)(cid:2) (cid:3) (cid:2) (cid:3) (cid:12)
Since[J−⊤(z)] = 0forallz Rn,w(cid:12)ehave (cid:12)
ϕ h∗ ρi,i ̸ ∈
pˆa(ρ ) = ρ = ρ : E s(Z) s˜ρj(Z) = 0 . (263)
j i ̸ j − i ̸
(cid:26) (cid:20)(cid:12) (cid:12)(cid:21) (cid:27)
(cid:12)(cid:2) (cid:3) (cid:12)
From Lemma 3(i), E [s(Z) s˜ρj(Z)]
i
= 0(cid:12)if and only if i (cid:12)pa(j). Therefore, (259) implies
− ̸ ∈
that ρ pˆa(ρ ) if and only if i pa(j), which shows that and ˆare related through a graph
i ∈ j (cid:2)(cid:12) ∈ (cid:12)(cid:3) G G
isomorphismbypermu(cid:12)tationρ,whichwa(cid:12)sdefinedas −1.
I
C.2 ProofofTheorem6
IntheproofofTheorem5,weshowedthatcoupledhardinterventions(withoutusingobservational
environment)aresufficientforrecoveringthelatentvariables. Then,inthisproof,wejustfocuson
recoveringthelatentgraph. Specifically,wewillshowthatifp(pdfofZ)isadjacency-faithfulto
G
65VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
andthelatentcausalmodelisanadditivenoisemodel,thenwecanrecover withouthavingaccess
G
toobservationalenvironment 0. ByLemma3(iv),truelatentscorechangesacross ρi, ˜ρj gives
E {E E }
uspa(i,j)fori = j. First,weusetheperfectlatentrecoveryresulttoshowthatLemma3(iv)also
appliestoestima̸ tedlatentscorechanges. Specifically,using(239)and1 J−1 = P ,wehave
{ ϕ h∗} ρ
sρi(zˆ;h∗) s˜ρj(zˆ;h∗) = J−⊤(z) sρi(z) s˜ρj(z) (264)
Zˆ − Zˆ ρ k ϕ h∗ ρ k · −
(cid:2) (cid:3) = (cid:2)J−⊤(z)(cid:3) (cid:2) sρi(z) s˜ρj(z(cid:3)) . (265)
ϕ h∗ ρ k,k · − k
(cid:2) (cid:3) (cid:2) (cid:3)
Recallthatwehavefound J−⊤(z) = 0forallz Rn in(257). Then,wehave
ϕ h∗ ρ k,k ̸ ∈
(cid:2) (cid:3)
E sρi(Zˆ;h∗) s˜ρj(Zˆ;h∗) = 0 E sρi(Z) s˜ρj(Z) = 0. (266)
Zˆ − Zˆ ρ k ̸ ⇐⇒ − k ̸
(cid:20)(cid:12) (cid:12)(cid:21) (cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12) (cid:12)(cid:2) (cid:3) (cid:12)
Hence,b(cid:12)yLemma3(iv), (cid:12) (cid:12) (cid:12)
E sρi(Zˆ;h∗) s˜ρj(Zˆ;h∗) = 0 k pa(i,j). (267)
Zˆ − Zˆ ρ k ̸ ⇐⇒ ∈
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
Let us denote the g(cid:12)raph that is related to (cid:12)by permutation ρ, i.e., i pa(j) if and only if
ρ
G G ∈
ρ pa (ρ )forwhichpa (ρ )denotestheparentsofnodeρ in . Using(267),wehave
i ρ j ρ j j ρ
∈ G
E sρi(Zˆ;h∗) s˜ρj(Zˆ;h∗) = 0 ρ pa (ρ ,ρ ). (268)
Zˆ − Zˆ ρ k ̸ ⇐⇒ k ∈ ρ i j
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:12)
In the rest of the(cid:12)proof, we will show how to(cid:12)obtain pa (i) : i [n] using pa (i,j) : i,j
ρ ρ
{ ∈ } { ∈
[n], i = j . Since is a graph isomorphism of , this problem is equivalent to obtaining
ρ
̸ } G G
pa(i) : i [n] using pa(i,j) : i,j [n], i = j . NotethatZˆ (whichcorrespondstonodeiin
i
{ ∈ } { ∈ ̸ }
)isintervenedinenvironments i and ˜i. Wedenotethesetofrootnodesby
ρ
G E E
≜ i [n] : pa(i) = , (269)
K { ∈ ∅}
andalsodefine
≜ pa(i,j), i [n], and ≜ i : = 1 . (270)
i i
B ∀ ∈ B { |B | }
j̸=i
(cid:92)
Note that pa(i) . Hence, = 1 implies that i is a root node. We investigate the graph
i i
⊆ B |B |
recoveryinthreecases.
1. 3: Foranynodei [n],wehave
|B| ≥ ∈
pa(i) pa(i,j) = pa(i) j = pa(i). (271)
i
⊆ B ⊆ ∪ { }
j∈K (cid:92)\{i} (cid:110)j∈K (cid:92)\{i} (cid:111)
Note that, the last equality is due to j = since there are at least two root nodes
j∈K\{i}
∩ { } ∅
excludingi. Then, = pa(i)foralli [n]andwearedone.
i
B ∈
66SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
2. = 2: Thetwonodesin arerootnodes. Iftherewereatleastthreerootnodes, wewould
|B| B
haveatleastthreenodesin . Hence,thetwonodesin aretheonlyrootnodes. Subsequently,
B B
everyi / isalsonotin andwehave
∈ B K
pa(i) pa(i,j) = pa(i) j = pa(i). (272)
i
⊆ B ⊆ ∪ { }
j (cid:92)∈K (cid:110)j (cid:92)∈K (cid:111)
Hence, = pa(i)foreverynon-rootnodeiandwealreadyhavethetworootnodesin ,which
i
B B
completesthegraphrecovery.
3. 1: First,considerall(i,j)pairssuchthat pa(i,j) = 2. Forsuchan(i,j)pair,atleastone
|B| ≤ | |
ofthenodesisarootnode,otherwisepa(i,j)wouldcontainathirdnode. Usingthesepairs,we
identifyallrootnodesasfollows. NotethatahardinterventiononnodeimakesZ independent
i
of all of its non-descendants, and all conditional independence relations are preserved under
element-wisediffeomorphismssuchasϕ h∗. Then,usingtheadjacency-faithfulnessassumption,
weinferthat
• ifZˆ Zˆ in i andZˆ Zˆ in ˜j,thenbothiandj arerootnodes.
i j i j
⊥⊥ E ⊥⊥ E
• ifZˆ Zˆ in i,theni j andiisarootnode.
i j
̸⊥⊥ E →
• ifZˆ Zˆ in ˜j,thenj iandj isarootnode.
i j
̸⊥⊥ E →
This implies that by using at most two independence tests, we can determine whether i and j
nodesarerootnodes. Hence,byatmostnindependencetests,weidentifyallrootnodes. Wealso
knowthatthereareatmosttworootnodes. Ifwehavetworootnodes,then = pa(i)forall
i
B
non-rootnodes,andthegraphisrecovered. Ifwehaveonlyonerootnodei,thenforanyj = i
̸
wehave
pa(j) pa(i,j) = pa(j) i . (273)
j
⊆ B ⊆ ∪{ }
Finally,ifZˆ Zˆ Zˆ : ℓ i in ˜j,wehavei / pa(j)duetoadjacency-faithfulness.
j i ℓ j
⊥⊥ | { ∈ B \{ }} E ∈
Otherwise,weconcludethati pa(j). Hence,anadditional(n 1)conditionalindependence
∈ −
testsensuretherecoveryofallpa(j)sets,andthegraphrecoveryiscomplete.
C.3 ProofofLemma12
We will prove it by contradiction. Suppose that h∗ is a solution to the optimization problem
2
specifiedin(39). Usingthefactthat[J−⊤(z)]isfull-rankforallz Rn,andthescoredifferenP ce
vector [sρi(z) s˜ρi(z)] is not identicaϕ llh y∗ zero, (244) and Proposit∈ ion 2 imply that D (h∗) does
t
−
not have any zero columns. Subsequently, D (h∗) n, and since D (h∗) is diagonal, we
have 1 D (h∗) = I . We use J∗ ≜
J−∥⊤ at
s
sho∥ r0 tha≥
nd. If ρ = ρ˜
fort
some i [n], using
D
(h∗){ =t
I
} andLn e× mn
ma3(iii),forj =
i,ϕ h w∗
ehave
i i ∈
t n×n
̸
0 = D (h∗) (274)
t ρj,ρi
(cid:2) (cid:3)
= E J∗(Z)] sρi(Z) s˜ρi(Z) (275)
ρj
· −
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:2) (cid:3)(cid:12)
= E (cid:12) J∗(Z)] sρi(Z) s˜ρi(Z)(cid:12) . (276)
ρj,i · − i
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:2) (cid:3) (cid:12)
(cid:12) (cid:12)
67VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Recallthat,byinterventionaldiscrepancy,[sρi(z) s˜ρi(z)]
i
= 0exceptforanullset. Then,(274)
− ̸
impliesthatwehave[J∗(z)] = 0exceptforanullset. SinceJ∗ iscontinuous,thisimpliesthat
ρj,i
[J∗(z)] = 0forallz Rn. Furthermore,sinceJ∗(z)isinvertibleforallz,noneofitscolumns
ρj,i
∈
canbeazerovector. Hence,forallz Rn,[J∗(z)] = 0. Tosummarize,ifρ = ρ˜,wehave
∈
ρi,i i i
z Rn J∗(z) = 0 j = ρ . (277)
∀ ∈ j,i ̸ ⇐⇒ i
Now,considerthesetofmismatched(cid:2)nodes (cid:3)
≜ i [n] : ρ = ρ˜ . (278)
i i
A { ∈ ̸ }
Let a be a non-descendant of all the other nodes in . There exist nodes b,c , not
∈ A A ∈ A
necessarilydistinct,suchthat
ρ = ρ˜ , and ρ = ρ˜ . (279)
a b c a
Infoursteps,wewillshowthatD(h∗) = 0andD(h∗) = 0,whichviolatestheconstraint
ρa,ρc
̸
ρc,ρa
̸
1 D(h) 1 D⊤(h) = I and will conclude the proof by contradiction. Before giving the
n×n
{ }⊙ { }
steps,weprovidethefollowingargumentwhichwerepeatedlyuseintherestoftheproof. Forany
continuousfunctionf : Rn R,wehave
→
E f(Z) = 0 E f(Z) s(Z) sρa(Z) = 0, (280)
̸ ⇐⇒ · − a ̸
(cid:104)(cid:12) (cid:12)(cid:105) (cid:20)(cid:12)
(cid:12) (cid:2) (cid:3)
(cid:12) (cid:12)(cid:21)
and E (cid:12) f(Z)(cid:12) = 0 E (cid:12)f(Z) s(Z) s˜ρc(Z) (cid:12) = 0. (281)
̸ ⇐⇒ · − a ̸
First,supposethatE
(cid:104) f(cid:12)
(cid:12)(Z)
(cid:12) (cid:12)=(cid:105)
0.
Then,theree(cid:20) x(cid:12) (cid:12)istsano(cid:2)
pensetΨ
Rn(cid:3) f(cid:12) (cid:12)o(cid:21)
rwhichf(z) = 0for
(cid:12) (cid:12)
| | ̸ ⊆ ̸
all z Ψ. Due to interventional discrepancy between the pdfs p (z ) and q (z ), there exists an
a a a a
∈ (cid:2) (cid:3)
opensetwithinΨforwhich[sρa(Z) s(Z)]
a
= 0. Thisimpliesthat
− ̸
E f(Z) s(Z) sρa(Z) = 0. (282)
· − a ̸
(cid:20)(cid:12) (cid:12)(cid:21)
Fortheotherdirection, supposet(cid:12) (cid:12)hatE f(cid:2) (Z) [sρa(Z) (cid:3) s(cid:12) (cid:12)(Z)]
a
= 0, whichimpliesthatthere
· − ̸
existsanopensetΨforwhichbothf(z)and[sρa(z) s(z)]
a
arenon-zero. Then,E f(Z) = 0,
(cid:2)(cid:12) − (cid:12)(cid:3) | | ̸
andwehave(280). Similarly,duetoρ =(cid:12) ρ˜ andinterventionaldis(cid:12)crepancybetweenp andq˜ ,we
c a a a
(cid:2) (cid:3)
obtain(281).
Step1: ShowthatE [J∗(Z)] = 0. First,using(244)andLemma3(i),wehave
ρa,a
̸
(cid:104)(cid:12) (cid:12)(cid:105)
D(h(cid:12)∗) = E(cid:12) J∗(Z) s(Z) sρa(Z) (283)
ρa,ρa ρa · −
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
= E (cid:12) J∗(Z) s(Z) (cid:12)sρa(Z) . (284)
ρa,j · − j
(cid:20)(cid:12) (cid:12)j∈ (cid:88)pa(a)
(cid:2) (cid:3) (cid:2) (cid:3)
(cid:12) (cid:12)(cid:21)
Notethatpa(a) = a sinceais(cid:12)non-descendantoftheothernodesin .(cid:12)Considerj pa(a),
∩A { } A ∈
whichimpliesthatj / andρ = ρ˜ . By(277),wehave[J∗(Z)] = 0. Then,(284)becomes
∈ A
j j ρa,j
D(h∗) = E J∗(Z) s(Z) sρa(Z) = 0, (285)
ρa,ρa ρa,a· − a ̸
(cid:20)(cid:12) (cid:12)(cid:21)
sincediagonalentr(cid:2) iesofD(cid:3) (h∗)areno(cid:12)n(cid:2) -zerodu(cid:3) etoth(cid:2) elastconstraint(cid:3) in(cid:12)(39). Then,(280)implies
(cid:12) (cid:12)
thatE [J∗(Z)] = 0.
ρa,a
̸
(cid:2)(cid:12) (cid:12)(cid:3)
(cid:12) (cid:12)
68SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Step2: Showthat D˜(h∗) = 0. Next,weuseρ = ρ˜ andLemma3(i)toobtain
ρa,ρc ̸ c a
(cid:2) (cid:3)
D˜(h∗) = E J∗(Z) s(Z) s˜ρc(Z) (286)
ρa,ρc ρa · −
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
= E (cid:12) J∗(Z) s(Z) (cid:12)s˜ρc(Z) (287)
ρa,j · − j
(cid:20)(cid:12) (cid:12)j∈ (cid:88)pa(a)
(cid:2) (cid:3) (cid:2) (cid:3)
(cid:12) (cid:12)(cid:21)
(cid:12) (cid:12)
= E J∗(Z) s(Z) s˜ρc(Z) . (288)
ρa,a· − a
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:3) (cid:2) (cid:3) (cid:12)
Using(281)andStep1result,wehave(cid:12) D˜(h∗) = 0. (cid:12)
ρa,ρc ̸
Step3: ShowthatE [J∗(Z)] =(cid:2) 0. Us(cid:3) ing(244)andLemma3(i),wehave
ρc,a
̸
(cid:104)(cid:12) (cid:12)(cid:105)
D˜((cid:12) h∗) =(cid:12)E J∗(Z) s(Z) s˜ρc(Z) (289)
ρc,ρc ρc · −
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
= E (cid:12) J∗(Z) s(Z) (cid:12)s˜ρc(Z) (290)
ρc,j · − j
(cid:20)(cid:12) (cid:12)j∈ (cid:88)pa(a)
(cid:2) (cid:3) (cid:2) (cid:3)
(cid:12) (cid:12)(cid:21)
(cid:12) (cid:12)
= E J∗(Z)] s(Z) s˜ρc(Z) . (291)
ρc,a · − a
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12)(cid:2) (cid:2) (cid:3) (cid:12)
Since1 D(h∗) = 1 D˜(h∗) ,thedia(cid:12)gonalentry D˜(h∗) isnon(cid:12)-zero. Then,using(281)we
{ } { } ρc,ρc
haveE [J∗(Z)] = 0.
|
ρc,a
| ̸
(cid:2) (cid:3)
Step4:(cid:2)Showthat D(cid:3) (h∗) = 0. Next,weuseρ = ρ˜ andLemma3(i)toobtain
ρc,ρa ̸ c a
(cid:2) (cid:3)
D˜(h∗) = E J∗(Z) s(Z) sρa(Z) (292)
ρc,ρa ρc · −
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
= E (cid:12) J∗(Z) s(Z) (cid:12)sρa(Z) (293)
ρc,j · − j
(cid:20)(cid:12) (cid:12)j∈ (cid:88)pa(a)
(cid:2) (cid:3) (cid:2) (cid:3)
(cid:12) (cid:12)(cid:21)
(cid:12) (cid:12)
= E J∗(Z) s(Z) sρa(Z) . (294)
ρc,a· − a
(cid:20)(cid:12) (cid:12)(cid:21)
Using(280)andStep3result,wehave(cid:12) (cid:12)(cid:2) [D(h∗)(cid:3)
] ρc,ρa
̸=(cid:2)
0.
(cid:3) (cid:12)
(cid:12)
However, using the constraint 1 D(h∗) = 1 D˜(h∗) , we have [D(h∗)] = 0. Then,
{ } { }
ρa,ρc
̸
[D(h∗) D⊤(h∗)] = 0,whichviolatesthelastconstraintin(39). Therefore,ifthecouplingis
⊙
ρa,ρc
̸
incorrect,optimizationproblem hasnofeasiblesolution.
2
P
C.4 ProofofLemma13
Weconsiderthetrueencoderg−1 underthepermutationρ−1,thatish = ρ−1 g−1,andshowthat
◦
itisasolutionto specifiedin(39). First, notethatϕ = ρ−1 g−1 g = ρ−1, whichisjusta
2 h
permutation. HencP e,J−⊤ becomesthepermutationmatrixP⊤. Th◦ en,fo◦ ralli,m [n]wehave
ϕ h ρ ∈
D (h) = E P⊤ sm(Z) s˜m(Z) = E sm(Z) s˜m(Z) . (295)
t ρi,m ρ ρi · − − i
(cid:20)(cid:12) (cid:12)(cid:21) (cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12) (cid:12)(cid:2) (cid:3) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
69VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Then,byLemma3(ii),wehave[D (h)] = 0ifandonlyifi = Im,whichmeansm = ρ and
t ρi,m
̸
i
D (h) is a diagonal matrix. Hence, h satisfies the first constraint. Next, consider D(h). For all
t
i,j [n],wehave
∈
D (h) = E P⊤ s(Z) sρj(Z) = E sρj(Z) s˜ρj(Z) . (296)
t ρi,ρj ρ ρi · − − i
(cid:20)(cid:12) (cid:12)(cid:21) (cid:20)(cid:12) (cid:12)(cid:21)
(cid:2) (cid:3) (cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12) (cid:12)(cid:2) (cid:3) (cid:12)
ByLemma3(i),wehave[D(cid:12) (h)] = 0ifandonlyi(cid:12)fi = pa(cid:12)(j). Sinceρ = ρ˜,sim(cid:12)ilarly,wehave
ρi,ρj
̸
[D˜(h )] = 0ifandonlyifi pa(j). Therefore,wehave1 D(h) = 1 D˜(h) ,D(h)hasfull
ρ ρi,ρj
̸ ∈ { } { }
diagonalanditdoesnothavenon-zerovaluesinsymmetricentries. Hence,hsatisfiesthesecondand
thirdconstraints. Therefore,hisasolutionto sinceitsatisfiesallconstraintsandthediagonal
2
P
matrixD (h)has D (h) = n,whichisthelowerboundestablished.
t ∥ t ∥0
C.5 ProofofTheorem4
Recallthat ˜ = I˜1,...,I˜n isthepermutationofintervenednodesin ˜,socouplingπ considered
I { } E
in(39)isjustequalto ˜. Similarlytothedefinitionofρfor intheproofofTheorem5,letρ˜bethe
I I
permutationthatmaps 1,...,n to ˜,i.e.,Iρ˜i = iforalli [n]. Then,P ρ˜denotesthepermutation
{ } I ∈
matrixfortheinterventionorderoftheenvironments ˜1,..., ˜n ,i.e.,
{E E }
1, j = ρ˜ ,
i
P = (297)
ρ˜ i,j (cid:40)0, else.
(cid:2) (cid:3)
Lemma12showsthatifthecouplingisincorrect,i.e.,π = orequivalentlyρ = ρ˜,theoptimization
̸ I ̸
problem in (39) does not have a feasible solution. Next, Lemma 13 shows that if the coupling is
correct,i.e.,ρ = ρ˜,thereexistsasolutionto . Lemmas12and13collectivelyproveidentifiability
2
P
asfollows. Wecansearchoverthepermutationsof[n]until admitsasolutionh∗. ByLemma12,
2
P
theexistenceofthissolutionmeansthatcouplingiscorrect. Notethatwhenthecouplingiscorrect,
the constraint set of is a subset of the constraints in . Furthermore, the minimum value of
1 2
P P
D (h) islowerboundedbyn(Lemma11),whichisachievedbythesolutionh∗ (Lemma13).
∥ t ∥0
Hence,h∗ isalsoasolutionto ,andbyTheorem5,itsatisfiesperfectrecoveryofthelatentDAG
1
P
andthelatentvariables.
C.6 ProofofProposition3
Denotethesetofallpermutationsof[n]by . FromLeibnizformulaformatrixdeterminants,fora
n
S
matrixA Rn×n wehave
∈
n
det(A) = sgn(π) A (298)
·
i,πi
π (cid:88)∈Sn (cid:89)i=1
wheresgn(π)forapermutationπ of[n]is+1and 1forevenandoddpermutations,respectively.
−
Aisinvertibleifandonlyifdet(A) = 0,whichimpliesthat
̸
n
π : sgn(π) A = 0. (299)
∃ ∈
Sn
·
i,πi
̸
i=1
(cid:89)
70SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
BythedefinitionofP ,wehave[P A] = A . Then,
π π
·
i,i i,πi
n
π : sgn(π) [P A] = 0, (300)
n π i,i
∃ ∈ S · · ̸
i=1
(cid:89)
whichholdsifandonlyif[P A] = 0foralli [n].
π i,i
· ̸ ∈
AppendixD. AnalysisoftheAssumptions
D.1 AnalysisofAssumption1
Inthissubsection,weproveLemma1statement,i.e.,Assumption1issatisfiedforadditivenoise
modelsunderhardinterventions,asfollows.
Score decomposition. Consider a hard interventional environment m and let Im = i be the
E
intervenednode. Following(4)and(16),thelatentscoress(z)andsm(z)aredecomposedas
s(z) = logp (z z )+ logp (z z ), (301)
z i i pa(i) z j j pa(j)
∇ | ∇ |
j̸=i
(cid:88)
and sm(z) = logq (z )+ logp (z z ). (302)
z i i z j j pa(j)
∇ ∇ |
j̸=i
(cid:88)
Hence,s(z)andsm(z)differinonlythecausalmechanismofnodei. Subsequently,wehave
s(z) sm(z) = logp (z z ) logq (z ). (303)
z i i pa(i) z i i
− ∇ | −∇
Focusingontherelevanttwoentriesofthescoredifference,intervenednodeiandaparentk pa(i),
∈
[s sm] [ logp(z z )]
k z i pa(i) k
− ∇ |
= . (304)
 [s sm]   [ logp(z z )] [ logq(z )] 
i z i pa(i) i z i i
− ∇ | − ∇
   
Additivenoisemodel. Theadditivenoisemodelfornodeiisgivenby
Z = f (Z )+N , (305)
i i pa(i) i
asspecifiedin(7). Whennodeiishardintervened,Z isgeneratedaccordingto
i
Z = N¯ , (306)
i i
inwhichN¯ denotestheexogenousnoisetermfortheinterventionalmodel. Then,denotingthepdfs
i
ofN andN¯ byp andq ,respectively,(305)and(306)implythat
i i N N
p (z z ) = p (z f (z )), and q (z ) = q (z ). (307)
i i pa(i) N i i pa(i) i i N i
| −
Denotethescorefunctionsassociatedwithp andq by
N N
d p′ (u) d q′ (u)
r (u) ≜ logp (u) = N , and r (u) ≜ logq (u) = N . (308)
p N q N
du p (u) du q (u)
N N
Definen andn¯ astherealizationsofN andN¯ whenZ = z andZ = z . Then,wehave
i i i i i i pa(i) pa(i)
n¯ = n +f (z ). Using(307)and(308),wecanexpress(304)as
i i i pa(i)
[s −sm] k
=
−∂fi( ∂z zp ka(i)) ·r p(n i)
. (309)
 [s −sm] i  r p(n i) −r q(n i+f i(z pa(i)))
   
71VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Proofbycontradiction. Wewillprovethedesiredresultbycontradiction. Assumethat
[s sm]
k
− < 2, (310)
R (cid:32)(cid:34)[s sm] i(cid:35)(cid:33)
−
whichimpliesthatthereexistsaconstantκ Rsuchthat
∈
κ s(z) sm(z) = s(z) sm(z) , z Rn . (311)
· − i − k ∀ ∈
(cid:2) (cid:3) (cid:2) (cid:3)
Then,using(309),wehave
∂f (z ) r (n +f (z ))
i pa(i) q i i pa(i)
κ +1 = . (312)
· ∂z r (n )
k p i
Wescrutinize(312)intwocases.
Case 1: ∂fi(z pa(i)) is constant. In this case, let η = ∂fi(z pa(i)) for some η R for all z
∂z k ∂z k ∈ pa(i) ∈
R|pa(i)|. Then,theRHSof(312)isalsoconstantforallz R|pa(i)|andn R. Fixarealization
pa(i) i
∈ ∈
n = n∗ and note that f (z ) needs to be constant, denoted by δ∗, for all z R|pa(i)|.
i i i pa(i) pa(i) ∈
However,thisimpliesthat ∂fi(z pa(i)) = 0,andwehaver (n +δ∗) = r (n )foralln R. This
implies that p (n ) = η∗q (n∂z k +δ∗) for some constantq η∗i . Since p p andi q are pdi fs∈ , the only
N i N i N N
choice is η∗ = 1 and p (n ) = q (n +δ∗), then p (z z ) = q (z ), which contradicts the
N i N i i i pa(i) i i
|
premisethataninterventionchangesthecausalmechanismofthetargetnodei.
Case 2:
∂fi(z pa(i))
is not constant. In this case, note that LHS of (312) is not a function of n .
∂z i
k
Then,takingthederivativeofbothsideswithrespectton andrearranging,weobtain
i
r′(n ) r′(n +f (z ))
p i = q i i pa(i) , (n ,z ) R R|pa(i)| . (313)
i pa(i)
r (n ) r (n +f (z )) ∀ ∈ ×
p i q i i pa(i)
Next, considerafixedrealizationn = n∗, anddenotethevalueofLHSbyα. Sincef (z )is
i i i pa(i)
continuousandnotconstant,itsimagecontainsanopenintervalΘ R. Denotingu ≜ f (z ),
i pa(i)
⊆
wehave
r′(n∗+u)
q i
α = , u Θ. (314)
r (n∗+u) ∀ ∈
q i
Theonlysolutiontothisequalityisthatr (n∗+u)isanexponentialfunction,r (u) = k exp(αu)
q i q 1
over interval u Θ. Since r is an analytic function that equals to an exponential function
q
∈
over an interval, it is exponential over entire R. This implies that the pdf q (u) is of the form
N
q (u) = k exp((k /α)exp(αu)). However, this cannot be a valid pdf since its integral over R
N 2 1
diverges. Then,(310)cannotbetrueandwehave
[s sm]
k
− = 2, where i = I . (315)
m
R (cid:32)(cid:34)[s sm] i(cid:35)(cid:33)
−
72SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
D.2 AnalysisofAssumption2
Inthissection,weestablishthenecessaryandsufficientconditionsunderwhichAssumption2holds
foradditivemodels. Furthermore,weshowthatalargeclassofnon-linearmodelsinthelatentspace
satisfytheseconditions, includingthetwo-layerneuralnetworks. WehavefocusedonsuchNNs
sincetheyeffectivelyapproximatecontinuousfunctions(Cybenko,1989). Readily,thenecessary
andsufficientconditionscanbeinvestigatedforotherchoicesofnon-linearfunctions.
D.2.1 INTERPRETING ASSUMPTION 2
The implication of Assumption 2 is that the effect of an intervention is not lost in any linear
combinationofthevaryingcoordinatesofthescores. Toformalizethis,foreachnodei [n],we
∈
define
≜ c Rn : j pa(i) suchthat c = 0 , (316)
i j
C { ∈ ∃ ∈ ̸ }
andprovidethefollowingequivalentconditionwhichwillsimplifythesubsequentanalysis.
Assumption3 Consideranatomicenvironmentset . Then,forallc ,wehave
ℓ
E ∈ C
E c⊤ s(Z) sm(Z) = 0, where ℓ = Im . (317)
· − ̸
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12) (cid:2) (cid:3)(cid:12)
(cid:12) (cid:12)
Lemma16 Assumption2andAssumption3areequivalent.
Proof: First,notethatforarandomvariableX,E X = 0ifandonlyifE[X2] = 0ifandonlyif
| |
X = 0almostsurely. Considerenvironment m andletIm = ℓ. Then,(317)holdsifandonlyif
E (cid:2) (cid:3)
2
E c⊤ s(Z) sm(Z) = 0. (318)
· − ̸
(cid:20) (cid:21)
(cid:16) (cid:17)
(cid:2) (cid:3)
Next,notethat (s sm)isequaltotherankofthecovariancematrix
R −
Em ≜ E s(Z) sm(Z) s(Z) sm(Z) ⊤ . (319)
− · −
(cid:104) (cid:105)
(cid:2) (cid:3) (cid:2) (cid:3)
By Lemma 3, the only nonzero entries of Em lie in the submatrix with row and columns indices
pa(i). Since Em is a covariance matrix, it is symmetric and positive-semidefinite. Then, there
exists a matrix Fm such that Em = [Fm]⊤ Fm and j-th column of Fm is a zero vector for all
·
j / pa(i). Then,foranyc Rn,Fm cisalinearcombinationofthenonzerocolumnsofFm with
∈ ∈ ·
thecoefficients c : j pa(i) . Subsequently,wehave
j
{ ∈ }
Fm c = 0, c Em c = 0, c rank(Em) = pa(i) , (320)
i i
· ̸ ∀ ∈ C ⇐⇒ · ̸ ∀ ∈ C ⇐⇒ | |
whichconcludestheproofofLemma16.
73VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Necessaryandsufficientconditions. Considertheadditivenoisemodelfornodei
Z = f (Z )+N , (321)
i i pa(i) i
asspecifiedin(7). Whennodeiissoftintervened,Z isgeneratedaccordingto
i
Z = f¯(Z )+N¯ , (322)
i i pa(i) i
inwhichf¯ andN¯ specifytheinterventionalmechanismfornodei. Thefollowinglemmacharacter-
i i
izesthenecessaryandsufficientconditionsunderwhichAssumption3issatisfied. Inthissubsection,
weuseφastheshorthandforz .
pa(i)
Lemma17 Foreachnodei [n]considerthefollowingtwosetofequationsforc Rn:
∈ ∈
c c⊤ f (φ) = 0
i z i
− ·∇
, φ R|pa(i)| . (323)
 c c⊤ f¯(φ) = 0 ∀ ∈
 i z i
− ·∇
Assumption3holdsifandonlyiftheonlyallsolutionscto(323)satisfyc / i,orbasedon(316),
∈ C
equivalently
j pa(i) : c = 0. (324)
j
∀ ∈
Proof: SeeAppendixD.2.2.
ToprovidesomeintuitionabouttheconditionsinLemma17,weconsideranodei [n]and
∈
discuss the conditions in the context of a few examples. Note that by sweeping φ R|pa(i)| we
∈
generateacontinuumoflinearequationsoftheform:
c c⊤ f (φ) = 0, and c c⊤ f¯(φ) = 0. (325)
i z i i z i
− ·∇ − ·∇
Notethatforallj pa(i)wehave[ f (φ)] = [ f¯(φ)] = 0. Hence,infindingthesolutions
z i j z i j
̸∈ ∇ ∇
to(325)onlythecoordinates j pa(i) ofcarerelevant. Letusdefine
{ ∈ }
u(φ) ≜ f (φ), and u¯(φ) ≜ f¯(φ), (326)
φ i φ i
∇ ∇
which are the gradients of f and f by considering only the coordinates of z in j pa(i) .
i i
{ ∈ }
Accordingly, we also define b by concatenating only the coordinates of c with their indices in
j pa(i) . Next,considerw distinctchoicesofφanddenotethemby φt R|pa(i)| : t [w] .
{ ∈ } { ∈ ∈ }
Byconcatenatingthetwoequationsin(326)specializedtotheserealizations,wegetthefollowing
linearsystemwith2w equationsand pa(i) +1unknownvariables.
| |
[u(φ1)]⊤ 1
−
[u¯(φ1)]⊤ 1
 . − .  b
. . . .
c
= 0 2w . (327)
  i
[u(φw)]⊤ 1(cid:20) (cid:21)
 − 
[u¯(φw)]⊤ 1
 − 
 
≜V∈R2w×(|pa(i)|+1)
WhenV isfull-rank,i.e.,rank(V(cid:124)) = pa(cid:123)((cid:122)i) +1,(cid:125)thesystemhasonlythetrivialsolutionsc = 0
i
| |
andb = 0. Then,wemakethefollowingobservations.
74SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
1. If f and f¯ are linear functions, the vector spaces generated by u and u¯ have dimensions
i i
1. Subsequently, we always have rank(V) 2, rendering an underdetermined system when
≤
pa(i) 2. Hence,whenthemaximumdegreeof isatleast2,alinearcausalmodeldoesnot
| | ≥ G
satisfyAssumption3.
2. If f and f¯ are quadratic with full-rank matrices, i.e., f (φ) = φ⊤Aφ and f¯(φ) = φ⊤A¯φ
i i i i
where rank(A) = rank(A¯) = pa(i) , there is a choice of w pa(i) +1 and realizations
| | ≥ | |
φt R|pa(i)| : t [w] forwhichrank(V) = pa(i) +1andthesystemin(325)admitsonly
{ ∈ ∈ } | |
thetrivialsolutionsa = 0andb = 0. Hence,quadraticcausalmodelssatisfyAssumption2.
i
3. Iff andf¯ aretwo-layerNNswithasufficientlylargenumberofhiddenneurons,theyalsorender
i i
afullydeterminedsystem,andasaresult,theysatisfyAssumption3.
We investigate the last example in detail as follows. Assume that f and f¯ are two-layer NNs
i i
with pa(i) inputs, w and w¯ hidden nodes, respectively, and with sigmoid activation functions.
i i
Deno| tethe| weightmatricesbetweeninputandhiddenlayersinf
i
andf¯
i
byWi Rwi×|pa(i)| and
∈
W¯ i Rw¯i×|pa(i)|,respectively. Furthermore,defineν Rwi andν¯ Rwi astheweightsbetween
the h∈ idden layer and output in f and f¯, respectively.∈ Finally, defin∈ e ν and ν¯ as the bias terms.
i i 0 0
Hence,wehave
wi
f (φ) = ν⊤ σ(Wi φ)+ν = ν σ(Wi φ)+ν , (328)
i 0 j j 0
· · · ·
j=1
(cid:88)
w¯i
and f¯(φ) = ν¯⊤ σ(W¯ i φ)+ν = ν¯ σ(W¯ i φ)+ν¯ , (329)
i 0 j j 0
· · · ·
j=1
(cid:88)
inwhichactivationfunctionσ isappliedelement-wise.
Lemma2 ConsiderNNsf andf¯ specifiedin(328)and(329). Ifforalli [n]wehave
i i
∈
max rank(Wi) , rank(W¯ i) = pa(i) , (330)
{ } | |
thenAssumption3holds.
Proof: SeeAppendixD.2.3.
D.2.2 PROOF OF LEMMA 17
Weshowthatforeachnodei [n],thecondition
∈
E c⊤ s(Z) sm(Z) = 0, c , (331)
i
· − ̸ ∀ ∈ C
(cid:20)(cid:12) (cid:12)(cid:21)
(cid:12) (cid:2) (cid:3)(cid:12)
holdsifandonlyifthefollowi(cid:12)ngcontinuumofequat(cid:12)ionsadmittheirsolutionscinRn .
i
\C
c c⊤ f (φ) = 0
i z i
− ·∇
, φ R|pa(i)| , (332)
 c c⊤ f¯(φ) = 0 ∀ ∈
 i z i
− ·∇

75VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
inwhichshorthandφisusedforz . Let m betheenvironmentinwhichnodeiisintervened.
pa(i)
E
We first note that the condition in (331) can be equivalently stated as follows. Using (77), for all
c wehave
i
∈ C
E c⊤ logp (Z Z ) logq (Z Z ) = 0. (333)
z i i pa(i) z i i pa(i)
· ∇ | −∇ | ̸
(cid:20)(cid:12) (cid:12)(cid:21)
Thiscanbereadilyv(cid:12)erifie(cid:2)
dbynotingthat
(cid:3)(cid:12)
(cid:12) (cid:12)
c⊤ s(z) sm(z) (76 =),(77) c⊤ logp(z z ) logq (z z ) . (334)
z i pa(i) z i i pa(i)
· − · ∇ | −∇ |
(cid:104) (cid:105) (cid:104) (cid:105)
Also,usingProposition2,(333)isequivalentto
c , z : c⊤ logp (z z ) = c⊤ logq (z z ). (335)
i z i i pa(i) z i i pa(i)
∀ ∈ C ∃ ·∇ | ̸ ·∇ |
Theadditivenoisemodelfornodeiisgivenby
Z = f (Z )+N , (336)
i i pa(i) i
asspecifiedin(7). Whennodeiissoftintervened,Z isgeneratedaccordingto
i
Z = f¯(Z )+N¯ , (337)
i i pa(i) i
inwhichf¯ andN¯ specifytheinterventionalmechanismfornodei. Then,denotingthepdfsofN
i i i
andN¯ byp andq ,respectively,(336)and(336)implythat
i N N
p (z z ) = p z f (z ) , and q (z z ) = q z f¯(z ) . (338)
i i pa(i) N i i pa(i) i i pa(i) N i i pa(i)
| − | −
Denotethescorefunctio(cid:0)nsassociatedwi(cid:1)thp andq by (cid:0) (cid:1)
N N
d p′ (u) d q′ (u)
r (u) ≜ logp (u) = N , and r (u) ≜ logq (u) = N . (339)
p N q N
du p (u) du q (u)
N N
Define n and n¯ as the realizations of N and N¯ when Z = z and Z = z . By defining
i i i i i i pa(i) pa(i)
δ(z ) ≜ f (z ) f¯(z ), we have n¯ = n +δ(z ). Using (339) and (338), we can
pa(i) i pa(i) i pa(i) i i pa(i)
−
expresstherelevantentriesof logp (z z )and logq (z z )as
z i i pa(i) z i i pa(i)
∇ | ∇ |
r (n ), j = i,
p i
∇zlogp(z i | z pa(i)) j =  −∂fi( ∂z zp ja(i)) r p(n i), j ∈ pa(i), (340)

(cid:2) (cid:3) 0, j / pa(i),
∈
  r q(n¯ i), j = i,
and ∇zlogq i(z i | z pa(i)) j =  −∂f¯ i( ∂z zp ja(i)) r q(n¯ i), j ∈ pa(i), (341)

(cid:2) (cid:3) 0, j / pa(i).
∈
By substituting (340)–(341) in (335) and rearra nging the terms, the statement in (335) becomes

equivalenttofollowingstatement. Forallc ,thereexistn Randz R|pa(i)| suchthat
i i pa(i)
∈ C ∈ ∈
∂f (z ) ∂f¯(z )
i pa(i) i pa(i)
r (n ) c c = r (n +δ(z )) c c ,
p i i j q i pa(i) i j
· − · ∂z ̸ · − · ∂z
j j
(cid:18) j∈pa(i) (cid:19) (cid:18) j∈pa(i) (cid:19)
(cid:88) (cid:88)
(342)
76SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
which by using the shorthand φ for z can be compactly presented as follows. For all c ,
pa(i) i
∈ C
thereexistsn Randφ R|pa(i)| suchthat
i
∈ ∈
r (n ) c c⊤ f (φ) = r n +δ(φ) c c⊤ f¯(φ) . (343)
p i i z i q i i z i
· − ·∇ ̸ · − ·∇
(cid:104) (cid:105) (cid:104) (cid:105)
Hence,Assumption3isequivalenttothestateme(cid:0)ntin(343)(cid:1),whichweusefortherestoftheproof.
Sufficientcondition. Weshowthatif(332)admitsolutionsconlyinRn ,thenthestatement
i
\C
in (343) holds. By contradiction, assume that there exists c∗ such that for all n R and
i i
∈ C ∈
φ R|pa(i)|,
∈
r (n ) c∗ (c∗)⊤ f (φ) = r n +δ(φ) c∗ (c∗)⊤ f¯(φ) . (344)
p i i z i q i i z i
· − ·∇ · − ·∇
(cid:104) (cid:105) (cid:104) (cid:105)
Weshowthat c∗ is alsoa solutionto (332)(cid:0) , contradict(cid:1) ingthe premise. In orderto show that
i
∈ C
c∗ is also a solution to (332), suppose, by contradiction, that (344) holds, and there exists
i
∈ C
φ∗ R|pa(i)| correspondingtowhich
∈
(c∗)⊤ f¯(φ∗) c∗ = 0. (345)
z i i
·∇ − ̸
Note that f is a continuously differentiable function and also a function of z for all j pa(i).
i j
∈
Hence, there exists an open set Φ R|pa(i)| for φ for which (c∗)⊤ f (φ∗) c∗ is non-zero
⊆ ·∇z i − i
everywhereinΦ. Likewise,r cannotconstantlybezerooverallpossibleintervalsΩ. Thisisbecause
p
otherwise,itwouldhavetonecessarilybeaconstantzerofunction(sinceitisanalytic),whichisan
invalidscorefunction. Hence,thereexistsanopenintervalΩ Roverwhichr (n )isnon-zero
p i
⊆
for all n Ω. Subsequently, the left-hand side of (344) is non-zero over the Cartesian product
i
∈
(n ,φ) Ω Φ. Thismeansthatif(344)istrue,thenbothfunctionsonitsright-handsidemustbe
i
∈ ×
alsonon-zerooverΩ Φ. Hence,byrearrangingthetermsin(344)wehave
×
r q n i+δ(φ) = c∗ i −(c∗)⊤ ·∇zf i(φ) , (n ,φ) Ω Φ. (346)
r (n ) c∗ (c∗)⊤ f¯(φ) ∀ i ∈ ×
(cid:0) p i (cid:1) i − ·∇z i
Intwosteps,weshowthat(346)cannotbevalid.
Step1. First,weshowthatfunctionδcannotbeaconstantoverΦ(intervalspecifiedabove). Suppose
thecontraryandassumethatδ(φ) = δ∗ forallφ Φ. Hence,thegradientofδ iszero. Usingthe
∈
definitionofδ,thisimpliesthat
f (φ) = f¯(φ), φ Φ. (347)
z i z i
∇ ∇ ∀ ∈
Then,byleveraging(346),weconcludethatr (n ) = r (n +δ∗)foralln Ω. Sincer (n )and
p i q i i p i
∈
r (n +δ∗)areanalyticfunctionsthatagreeonanopenintervalofR,theyareequalforalln R
q i i
∈
aswell. Thisimpliesthatp (n ) = η q (n +δ∗)forsomeconstantη. Sincep andq arepdfs,
N i N i N N
·
the only choice is η = 1 and p (n ) = q (n +δ∗), then p (z z ) = q (z z ), which
N i N i i i pa(i) i i pa(i)
| |
contradictsthepremise.
Step 2. Finally, we will show that (346) cannot be true when δ is not a constant function over Φ.
Notethattheright-handsideof(346)isnotafunctionofn . Then,takingthederivativeofbothsides
i
withrespectton andrearranging,weobtain
i
r′(n ) r′(n +δ(φ))
p i q i
= , (n ,φ) Ω Φ. (348)
i
r (n ) r (n +δ(φ)) ∀ ∈ ×
p i q i
77VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Next,considerafixedrealizationn = n∗,anddenotethevalueofLHSbyα. Sinceδ iscontinuous
i i
andnotconstantoverΦ,itsimagecontainsanopenintervalΘ R. Denotingu ≜ δ(φ),weget
⊆
r′(n∗+u)
q i
α = , u Θ. (349)
r (n∗+u) ∀ ∈
q i
Theonlysolutiontothisequalityisthatr (n∗+u)isanexponentialfunction,r (u) = k exp(αu)
q i q 1
over interval u Θ. Since r is an analytic function that equals to an exponential function over
q
∈
aninterval,itisexponentialoverentireR. Thisimpliesthatthepdfq (u)isoftheformq (u) =
N N
k exp((k /α)exp(αu)). However, this cannot be a valid pdf since its integral over R diverges.
2 1
Hence,(346)isnottrue,andthepremisethatthereexistsc∗ andφ R|pa(i)| correspondingto
i
which(345)holdsisinvalid,concludingthatforallφ R|pa(∈ i)|C wehavec∈ ⊤ f¯(φ) = c . Proving
z i i
∈ ·∇
thecounterpartidentityc⊤ f (φ) = c followssimilarly. Therefore,(344)implies
z i i
·∇
c∗ (c∗)⊤ f (φ) = 0
i − ·∇z i
, φ R|pa(i)| , (350)
 c∗ (c∗)⊤ f¯(φ) = 0 ∀ ∈
 i − ·∇z i
whichmeansthatwehavefoundasolutionto(332)thatisnotinRn ,contradictingthepremise.
i
\C
Necessarycondition. AssumethatAssumption3,andequivalently,thestatementin(343)holds
but(332)hasasolutionc∗ in . Hence,
i
C
c∗ (c∗)⊤ f (φ) = c∗ (c∗)⊤ f¯(φ) = 0, φ R|pa(i)| . (351)
i z i i z i
− ·∇ − ·∇ ∀ ∈
Then,multiplyingleftsidebyr (n )andrightsidebyr (n +δ(φ)),weobtainthatforalln R
p i q i i
∈
andφ R|pa(i)|
∈
r (n ) c∗ (c∗)⊤ f (φ) = r (n +δ(φ)) c∗ (c∗)⊤ f¯(φ) = 0, (352)
p i i z i q i i z i
· − ·∇ · − ·∇
(cid:104) (cid:105) (cid:104) (cid:105)
whichimpliesthatforallc ,n Randφ R|pa(i)|
i i
∈ C ∈ ∈
r (n ) c c⊤ f (φ) = r (n +δ(φ)) c c⊤ f¯(φ) . (353)
p i i z i q i i z i
· − ·∇ · − ·∇
(cid:104) (cid:105) (cid:104) (cid:105)
ThiscontradictsAssumption3. Hence,theproofiscomplete.
D.2.3 PROOF OF LEMMA 2
Approach. WewillusethesameargumentasatthebeginningoftheproofofLemma17. Specifi-
cally,wewillshowthatforanynodeiandtwo-layerNNsf andf¯ withweightmatricesWi and
i i
W¯ i,thefollowingcontinuumofequationsadmittheirsolutionscinRn .
i
\C
c c⊤ f¯(φ) = 0
i z i
− ·∇
, φ R|pa(i)| , (354)
 ∀ ∈
c c⊤ f (φ) = 0
 i z i
− ·∇
inwhichshorthandφisusedforz . Hence,byinvokingLemma17,Assumption3holds.
pa(i)
78SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Definitions. Define d ≜ pa(i) . Since max rank(Wi),rank(W¯ i) = d , without loss of
i i
| | { }
generality,supposethatWi Rwi×di hasrankd i. Therestoftheprooffollowssimilarlyforthe
∈
caseofrank(W¯ i) = d . Weuseshorthand W,w for Wi,w whenitisobviousfromcontext.
i i
{ } { }
Parameterization. Notethatf canberepresentedbydifferentparameterizations,somecontaining
i
morehiddennodesthanothers. Withoutlossofgenerality,letw bethefewestnumberofnodesthat
canrepresentf . Thisimpliesthattheentriesofν arenon-zero. Otherwise,ifν = 0,wecanremove
i i
i-thhiddennodeandstillhavethesamef . Similarly,therowsofWaredistinct. Otherwise,ifthere
i
existrowsW = W fordistincti,j [w],removingj-thhiddennodeandusing(ν +ν )inplace
i j i j
∈
ofν resultsthesamefunctionasf with(w 1)hiddennodes. Similarly,wehaveW = 0forall
i i i
− ̸
i [w]. Otherwise,wehave
∈
ν
i
ν σ(W φ)+ν = (ν + ), (355)
i i 0 0
· · 2
andbyremovingi-thhiddennodeandusing(ν + νi)insteadofν ,wereachthesamefunctionas
0 2 0
f with(w 1)hiddennodes. Finally,wehaveW +W = 0. Otherwise,wehave
i i j
− ̸
ν σ(W φ)+ν σ(W φ)+ν = (ν ν ) σ(W φ)+(ν +ν ), (356)
i i j j 0 i j i 0 j
· · − · ·
andbyremovingj-thhiddennodeandusing(ν ν )insteadofν and(ν +ν )insteadofν ,we
i j i 0 j 0
−
reachthesamefunctionasf with(w 1)hiddennodes. Insummary,wehave
i
−
W = 0, ν = 0, i [w], and W W = 0, i,j [w] : i = j . (357)
i i i j
̸ ̸ ∀ ∈ ± ̸ ∀ ∈ ̸
Wewillshowthattheredoesnotexistc
i
suchthatc⊤ zf i(φ) = c
i
forallφ Rdi. Assume
∈ C ·∇ ∈
thecontrary,andassumethereexistsc∗ suchthat
i
∈ C
(c∗)⊤ f (φ) = c∗ , φ Rdi . (358)
z i i
·∇ ∀ ∈
Thisisequivalenttoshowingthatthereexistsnon-zerob∗ Rdi suchthat
∈
(b∗)⊤ f (ϕ) = c∗ , φ Rdi . (359)
φ i i
·∇ ∀ ∈
Basedon(328),thegradientoff (φ)is
i
f (φ) = W⊤ diag(ν) σ˙(W φ), (360)
φ i
∇ · · ·
wherediag(ν)isthediagonalmatrixwithν asitsdiagonalelements,andσ˙ isthederivativeofthe
sigmoidfunction,appliedelement-wisetoitsargument. Hence,basedon(358),thecontradiction
premiseisequivalenttohavinganon-zerob∗ Rdi suchthat
∈
σ˙(W φ) ⊤ diag(ν) W b∗ = c , φ Rdi . (361)
i
· · · · ∀ ∈
(cid:2) (cid:3)
We note that since W is full-rank and ν is non-zero for all i [w], the matrix diag(ν) W is
i
∈ ·
full-rank as well and it has a trivial null space. Subsequently, b∗ Rdi is non-zero if and only if
∈
(diag(ν) W b∗) Rw isnon-zero. Wewillusethefollowinglemmatoshowthat(361)cannotbe
· · ∈
true. Thisestablishesthatthecontradictionpremiseisnotcorrect,andcompletestheproof.
79VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Lemma18 Let u Rp have non-zero entries with distinct absolute values, i.e., u = 0 and
i
∈ ̸
u = u foralli = j, anda Rbeaconstant. Then, foreverynon-zerovectorc Rp, there
i j
| | ̸ | | ̸ ∈ ∈
existsα Rsuchthat[σ˙(αu)]⊤ c = a.
∈ · ̸
Proof: SeeAppendixD.2.4.
Let us define ξ ≜ W φ. We will show that there exists φ∗ Rdi such that ξ = W φ∗
· ∈ ·
satisfies the conditions in Lemma 18. Then, using Lemma 18 with the choice of u = W φ∗,
·
d = diag(ν) W b∗,anda = c ,wefindthatthereexistsα Rsuchthat
i
· · ∈
σ˙(α W φ∗) ⊤ diag(ν) W b∗ = c . (362)
i
· · · · · ̸
(cid:2) (cid:3)
Hence, (361) is false since it is violated for φ = αφ∗ and the proof is completed. We show the
existence of such φ∗ as follows. We first construct the set of φ values for which conditions of
Lemma 18 on w are not satisfied. The set in question is the union of the following cases: (i)
ξ = W φ = 0 for some i [w], (ii) ξ = ξ for some distinct i,j [w], or equivalently,
i i i j
· ∈ | | | | ∈
(W
i
W j)φ = 0. NotethatW
i
= 0andW
i
W
j
= 0by(357). Foranon-zerovectory Rdi,
± ̸ ± ̸ ∈
theset φ Rdi : y⊤φ = 0 isa(d
i
1)-dimensionalsubspaceofRdi. Then,thereiswnumberof
{ ∈ } −
(d 1)-dimensionalsubspacesthatfallundercase(i),andw(w 1)numberof(d 1)-dimensional
i i
− − −
subspacesthatfallundercase(ii). Therefore,therearew2 lower-dimensionalsubspacesforwhich
the conditions of Lemma 18 do not hold. However, Rdi cannot be covered by a finite number of
lower-dimensional subspaces of itself. Therefore, there exists φ∗ Rdi such that ξ = W φ∗
∈ ·
satisfiestheconditionsofLemma18,andtheproofiscompleted.
D.2.4 PROOF OF LEMMA 18
Assume the contrary and suppose that there exists a non-zero d and a for a given u. Define the
functiong (α) ≜ c⊤ σ˙(αu). Notethat
u
·
1
σ˙(x) = (363)
1+e−x+ex
is an even analytic function, and its Taylor series expansion at 0 has the domain of convergence
x R : x < π . Thus,forallα ( π , π ),
{ ∈ | | 2} ∈ −2maxi|ui| 2maxi|ui|
p ∞ p ∞ p
a = g (α) = c σ˙(αu ) = γ c (αu )2i = (γ c u2i)α2i . (364)
u j j i j j i j j
·
j=1 i=0 j=1 i=0 j=1
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Notethatg (α)isaconstantfunctionofα ( π , π ). Thus,itsTaylorcoefficients,
u ∈ −2maxi|ui| 2maxi|ui|
i.e., (γ p c u2i), are zeroforalli N+. However, thecoefficientsofevenpowersinTaylor
i j=1 j j ∈
expansionofσ˙,i.e.,γ ’s,arenon-zero(Weisstein,2002). Therefore,wehave p (c u2i) = 0for
(cid:80) i i=1 j j
alli N+. Next,constructthefollowingsystemoflinearequations,
∈ (cid:80)
u2 u4 ... u2w c
1 1 1 1
. . . .
 . . . . ... . .  . .  = 0. (365)
u2 u4 ... u2w c
 p p p  p 
  
80SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Thisisequivalentto
1 u2 ... u2(p−1) c
1 1 1
diag [u2 1,...,u2 p]⊤ ·. . . . . . ... . . .  · . . .  = 0. (366)
(cid:0) (cid:1) 1 u2 p ... u p2(p−1)  c p 
 
 
 
Vandermonde
NotethattheVandermondematrixin(366(cid:124))hasdeterm(cid:123)(cid:122)inant (cid:125) (u2 u2),whichisnon-zero
1≤i≤j<p i − j
since u = u for i = j. Multiplying an invertible matrix with a diagonal invertible matrix
i j
| | ̸ | | ̸ (cid:81)
generates another invertible matrix, and c must be a zero vector which is a contradiction. Hence,
theredoesnotexistsuchcforwhichc⊤ σ˙(αu)isconstantforeveryα R.
· ∈
AppendixE. EmpiricalEvaluations: DetailsandAdditionalResults
E.1 ImplementationDetailsofLSCALE-IAlgorithm
ScorefunctionofthelinearGaussianmodel. ForaGaussianrandomvectorY (µ,Σ),the
∼ N
scorefunctionofY isgivenby
s (y) = Σ−1 (y µ). (367)
Y
− · −
Usingthelinearmodelspecifiedin(65),inenvironment 0,
E
Z = (I A)−1 N . (368)
n i
− ·
SinceN isazeromeanGaussianrandomvectorand(I A)−1 isafullrankmatrix,Z isazero
i n
−
meanGaussianrandomvector. Hence,
−1
s(z) = Cov(Z) z . (369)
− ·
NotethatthecovarianceofZ hasthefollowin(cid:2)gform (cid:3)
Cov(Z) = (I A)−1 diag [σ2,...,σ2]⊤ (I A)−1 ⊤ . (370)
n − · 1 n · n −
Similarly,thescorefunctionofZ in m isgive(cid:0)nby (cid:1) (cid:0) (cid:1)
E
sm(z) = Cov(Zm) −1 z , m [n], (371)
− · ∀ ∈
in which Zm denotes the latent var(cid:2)iables Z in(cid:3)environment m. Finally, by setting f = G,
E
Corollary2specifiesthescoredifferencesofX intermsofZ underdifferentenvironmentpairsas
s (x) sm(x) = G† ⊤ s(z) sm(z) . (372)
X X
− · −
Score estimation for a linear Gaussian mo(cid:0)del.(cid:1) Fo(cid:2)r all environm(cid:3)ents m , given n i.i.d.
s
E ∈ E
samplesofX,wefirstcomputethesamplecovariancematrixdenotedbyΣˆm. Then,wecomputethe
sampleprecisionmatrixas
Θˆm ≜ (Σˆm)† , (373)
whichleadstothescorefunctionestimategivenby
sˆm(x) ≜ Θˆm x. (374)
X
− ·
81VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Preprocessing: Dimensionalityreduction. SinceX = G Z,wecancomputeim(G)usingn
·
random samples of X almost surely. Specifically, im(G) equals the column space of the sample
covariance matrix of n samples of X. In LSCALE-I, as a preprocessing step, we compute this
subspace and express samples of X and s in this basis. This procedure effectively reduces the
X
dimensionofX ands fromdton. Then,weperformstepsofLSCALE-Iusingthisndimensional
X
observed data. Next, we solve the optimization problems in (32) of Algorithm 2 and (33) of
Algorithm3byinvestigatingcertainnullspacesasfollows.
ImplementationofAlgorithm2(StepL2). Foreacht [n]andk ,weconsiderthesetof
t
∈ ∈ V
a im(G)whichresultsinc = 0in(32)
∈
null( ) ≜ a im(G) : E a⊤ s (X) sm(X) = 0 , (375)
t,k X X
V ∈ · −
(cid:26) m∈ (cid:88)Vt\{k} (cid:20)(cid:12)
(cid:12) (cid:2)
(cid:3)(cid:12) (cid:12)(cid:21) (cid:27)
(cid:12) (cid:12)
which we will shortly show that is a null space. In Algorithm 2, we check whether null( )
t,k
V
intersectswiththefeasibleset
= im(G) span(A⊤ ,...,A⊤ ). (376)
Ft
\
πt+1 πn
UsingProposition2,wecanwritenull( )as
t,k
V
null( ) = a im(G) : m k a⊤ Cor s (X) sm(X) a = 0 (377)
t,k t X X
V ∈ ∀ ∈ V \{ } · − ·
(cid:110) (cid:111)
(cid:0) (cid:1)
= a im(G) : a⊤ Cor s (X) sm(X) a = 0 , (378)
X X
∈ · − ·
(cid:110) (cid:18)m∈ (cid:88)Vt\{k}
(cid:0)
(cid:1)(cid:19) (cid:111)
whereCor(Y) ≜ E(Y Y⊤)denotesthecorrelationmatrixofarandomvectorY. Bynotingthat
·
Cor s (X) sm(X) isasymmetricmatrixforallm [n],(378)showsthatnull( )isanull
X − X ∈ Vt,k
space. Specifically,
(cid:0) (cid:1)
null( ) = im(G) null Cor s (X) sm(X) . (379)
t,k X X
V ∩ −
(cid:18)m∈ (cid:88)Vt\{k}
(cid:0)
(cid:1)(cid:19)
Theoptimizationproblemin(32)hassolutionc = 0ifandonlyif null( )isnotempty. Then,
t t,k
F ∩ V
using(376)and(379),(32)hassolutionc = 0ifandonlyif
null( ) = im(G) null Cor s (X) sm(X) span(A⊤ ,...,A⊤ ).
Vt,k
∩
X
−
X
̸⊆
πt+1 πn
(cid:18)m∈ (cid:88)Vt\{k}
(cid:0)
(cid:1)(cid:19)
(380)
Notethatboththeleftandright-handsidesofthisrelationaresubspacesofRd. Therefore, (380)
holdsifandonlyif
dim null( ) span(A⊤ ,...,A⊤ ) ⊥ = 0. (381)
Vt,k
∩
πt+1 πn
̸
(cid:16) (cid:17)
(cid:0) (cid:1)
82SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Table7: LSCALE-Iforalinearcausalmodelwithonehardinterventionpernode(n = 8).
perfectscores noisyscores
n d n samples ℓ(Z,Zˆ) SHD( , ˆ) ℓ(Z,Zˆ) SHD( , ˆ)
s
G G G G
8 8 5 103 0.02 0.73 0.04 3.79
×
8 25 5 103 0.02 0.83 0.04 3.73
×
8 100 5 103 0.02 0.70 0.04 4.24
×
8 8 104 0.01 0.43 0.03 2.15
8 25 104 0.01 0.47 0.03 1.99
8 100 104 0.01 0.39 0.03 2.31
8 8 5 104 0.01 0.20 0.01 0.40
×
8 25 5 104 0.01 0.16 0.01 0.41
×
8 100 5 104 0.01 0.20 0.01 0.36
×
ImplementationofAlgorithm3(StepL3). First,wedefine
null( ) ≜ a im(G) : E a⊤ s (X) sm(X) = 0 . (382)
t,j X X
M ∈ · −
(cid:26) m∈ (cid:88)Mt,j (cid:20)(cid:12)
(cid:12) (cid:2)
(cid:3)(cid:12) (cid:12)(cid:21) (cid:27)
(cid:12) (cid:12)
UsingthesameargumentsasforAlgorithm2,wehave
null( ) = im(G) null Cor s (X) sm(X) , (383)
t,j X X
M ∩ −
(cid:18)m∈ (cid:88)Mt,j
(cid:0)
(cid:1)(cid:19)
whichisasubspaceofRd. Then,theoptimizationproblem(33)hassolutionc = 0ifandonlyif
null( ) = (384)
t t,k
F ∩ V ̸ ∅
null( ) span(A⊤ ,...,A⊤ ) (385)
⇐⇒
Vt,k
̸⊆
πt+1 πn
dim null( ) span(A⊤ ,...,A⊤ ) ⊥ = 0. (386)
⇐⇒
Mt,j
∩
πt+1 πn
̸
(cid:16) (cid:17)
(cid:0) (cid:1)
E.2 AdditionalResultsforLSCALE-I
Inthissection,weextendthesimulationsettingsinSection7.1andinvestigatetheperformanceof
LSCALE-Iunderalargergraphsizeandquadraticlatentcausalmodels.
Increasinglatentdimensionn. WerepeattheexperimentsinSection7.1forn = 8andreport
theresultsforhardandsoftinterventionsinTables7and8. Themainobservationsaresimilarto
those from Tables 3 and 4. Specifically, LSCALE-I achieves close to perfect performance using
perfectscoresforbothhardandsoftinterventionsettings. Forthecaseofnoisyscores,givenenough
samples,LSCALE-IdemonstratesstrongperformanceatrecoveringbothZ and . Wenotethatthe
G
overallperformanceundern = 5variablesisslightlystrongerthanthatofundern = 8variables,
whichisexpectedduetotheincreasingproblemcomplexitywithn.
83VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Table8: LSCALE-Iforalinearcausalmodelwithonesoftinterventionpernode(n = 8).
perfectscores noisyscores
n d n samples incorrectmix. SHD( , ˆ ) incorrectmix. SHD( , ˆ )
s tc tc tc tc
G G G G
8 8 5 103 0 0 3.84 7.29
×
8 25 5 103 0 0.02 3.60 7.65
×
8 100 5 103 0 0.05 4.01 7.62
×
8 8 104 0 0.04 2.28 5.07
8 25 104 0 0.03 2.17 4.73
8 100 104 0 0.02 2.22 4.91
8 8 5 104 0 0 0.36 0.97
×
8 25 5 104 0 0.01 0.21 0.72
×
8 100 5 104 0 0.03 0.37 0.93
×
Quadraticlatentcausalmodels. Next,wereplacethelinearlatentmodelinSection7.1witha
quadraticlatentmodel. Specifically,weusethemodelspecifiedin(69)
Z = Z⊤ A Z +N , (387)
i pa(i)· i · pa(i) i
(cid:113)
where A : i [n] are positive-definite matrices, and the noise terms are zero-mean Gaussian
i
{ ∈ }
variableswithvariancesσ2sampledrandomlyfromUnif([0.5,1.5]). Forahardinterventiononnode
i
i,weset
Z = N¯ , (388)
i i
whereN¯ (0, σ i2 ). Forasoftinterventiononnodei,weset
i ∼ N 4
1
Z = Z⊤ A Z Z +N¯ . (389)
i 2 · pa(i)· i · pa(i) · i
(cid:113)
ThislatentmodelsatisfiesAssumption2. Therefore,weuseittoempiricallyevaluatetheguarantees
of Theorem 3. Another difference of this setting compared to Section 7.1 is that we estimate the
score functions of the observed variables using SSM-VR since p is not amenable to parameter
X
estimation.
• Forhardinterventions, Table9showsthatbyusingperfectscores, LSCALE-Iperformsnearly
perfectly. ThisresultagreeswithourobservationsinthelinearcausalmodelsettinginTables3
and7. Similarly,forsoftinterventions,Table10showsthatbyusingperfectscores,LSCALE-I
recoversthelatentgraphandensuresmixingconsistencyalmostperfectly,verifyingtheresultsof
Theorem3.
• WhenusingscoresestimatedbySSM-VR,Tables9and10showthatLSCALE-Idemonstratesa
mixedperformance. Fortherecoveryoflatentvariables,itperformsstronglyforhardinterventions,
asindicatedbyanormalizedℓ losslessthan0.1. Forsoftinterventions,thetotalnumberofentries
2
in an effective mixing matrix is 25. Therefore, Table 10 indicates a strong performance since
themeanincorrectmixingcomponentsislessthan6. Ontheotherhand,theperformanceatthe
84SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
graphrecoveryisrelativelyweaker. ThisresultcorroboratestheobservationsinSection7.2onthe
discrepancybetweenusingaparametricscoreestimationforlinearmodelsandanonparametric
scoreestimation(SSM-VR)forquadraticmodels.
Table9: LSCALE-Iforaquadraticcausalmodelwithonehardinterventionpernode(n = 5).
perfectscores noisyscores
n d n samples ℓ(Z,Zˆ) SHD( , ˆ) ℓ(Z,Zˆ) SHD( , ˆ)
s
G G G G
5 5 5 103 0.01 0.08 0.11 4.5
×
5 25 5 103 0.01 0.08 0.11 5.6
×
5 100 5 103 0.01 0.08 0.08 5.2
×
5 5 104 0.01 0.09 0.08 4.20
5 25 104 0.01 0.12 0.07 4.25
5 100 104 0.01 0.08 0.07 5.6
5 5 5 104 0.01 0.15 0.05 3.45
×
5 25 5 104 0.01 0.09 0.06 3.9
×
5 100 5 104 0.01 0.12 0.06 4.4
×
Table10: LSCALE-Iforaquadraticcausalmodelwithonesoftinterventionpernode(n = 5).
perfectscores noisyscores
n d n samples incorrectmix. SHD( , ˆ) incorrectmix. SHD( , ˆ)
s
G G G G
5 5 5 103 0.06 0.08 6.55 6.80
×
5 25 5 103 0.04 0.08 6.50 6.70
×
5 100 5 103 0.07 0.09 6.30 7.35
×
5 5 104 0.01 0.08 6.05 6.70
5 25 104 0.03 0.10 6.05 7.25
5 100 104 0.03 0.07 5.55 6.05
5 5 5 104 0.03 0.05 5.75 6.5
×
5 25 5 104 0.01 0.03 5.20 6.15
×
5 100 5 104 0.02 0.04 4.9 6.25
×
E.3 ImplementationDetailsofGSCALE-IAlgorithm
We perform experiments for the coupled interventions setting and use a regularized, ℓ -relaxed
1
versionoftheoptimizationproblemin(38). Specifically,inStepG2ofGSCALE-I,wesolvethe
followingoptimizationproblem:
min D (h) +λ E h−1(h(X)) X 2 +λ E h(X) 2 . (390)
h∈H∥ t ∥1,1 1 − 2 2 2
Inthissection,wedescribethecomputa(cid:104) ti(cid:13)onofthegroundtr(cid:13)uth(cid:105) scored(cid:104) if(cid:13)ference(cid:13)sf(cid:105)
orX,justification
(cid:13) (cid:13) (cid:13) (cid:13)
oftheoptimizationproblemin(390),andotherimplementationdetails.
85VARICI,ACARTÜRK,SHANMUGAM,KUMAR,ANDTAJER
Scorefunctionofthequadraticmodel. Following(16),scorefunctionssm ands˜m aredecom-
posedasfollows.
sm(z) = logq (z )+ logp (z z ), (391)
z ℓ ℓ z i i pa(i)
∇ ∇ |
i̸=ℓ
(cid:88)
and s˜m(z) = logq˜(z )+ logp (z z ). (392)
z ℓ ℓ z i i pa(i)
∇ ∇ |
i̸=ℓ
(cid:88)
Foradditivenoisemodels,thetermsin(391)and(392)haveclosed-formexpressions. Specifically,
using(90)anddenotingthescorefunctionsofthenoiseterms N : i [n] by r : i [n] ,we
i i
{ ∈ } { ∈ }
have
∂f (z )
i pa(j)
[s(z)] = r (n ) r (n ). (393)
i i i j j
− ∂z ·
i
j∈ch(i)
(cid:88)
RecallthatweconsideraquadraticlatentmodelinSection7.2with
f (z ) = z⊤ A z , and N (0,σ2), (394)
i pa(i) pa(i)· i · pa(i) i ∼ N i
(cid:113)
whichimplies
∂f j(z pa(j)) [A j] i z pa(j) n i
= · , and r (n ) = , i [n]. (395)
∂z i z p⊤ a(j)·A
j
·z
pa(j)
i i −σ i2 ∀ ∈
(cid:113)
Components of the score functions sm and s˜m can be computed similarly. Subsequently, using
Corollary2ofLemma4,wecancomputethescoredifferencesofobservedvariablesasfollows.
⊤
s (x) sm(x) = J (z) † s(z) sm(z) , (396)
X X g
− · −
s (x) s˜m(x) =
(cid:104)
(cid:2) J
(z)(cid:3)†(cid:105)⊤
(cid:2) s(z) s˜m(z)(cid:3) , (397)
X X g
− · −
sm(x) s˜m(x) =
(cid:104)
(cid:2) J
(z)(cid:3)†(cid:105)⊤
(cid:2) sm(z) s˜m(z(cid:3) ) . (398)
X X g
− · −
(cid:104) (cid:105)
(cid:2) (cid:3) (cid:2) (cid:3)
Implementationandevaluationsteps. Leveraging(70),weparameterizevalidencodershwith
parameterH Rn×d.
∈
Zˆ(X;h) = h(X) = H arctanh(X), (399)
·
Xˆ = h−1(Zˆ(X;h)) = tanh H† Zˆ(X;h) , (400)
·
(cid:16) (cid:17)
Notethatgiventhisparameterization,thefunctionϕ (z) = (h g)(z)isgivenby
h
◦
Zˆ = ϕ (Z) = H G Z . (401)
h
· ·
Subsequently,theonlyelement-wisediffeomorphismbetweenZ andZˆ isanelement-wisescaling.
Hence,wecanevaluatetheestimatedlatentvariablesagainstthescalingconsistencymetric. Tothis
end,weusenormalizedℓ lossspecifiedin(64).
2
86SCORE-BASEDCAUSALREPRESENTATIONLEARNINGFROMINTERVENTIONS
Weusen samplesfromtheobservationalenvironmenttocomputeempiricalexpectations. Since
s
encoderhisparameterizedbyH,weusegradientdescenttolearnthismatrix. Todoso,werelax
ℓ norm in (38) and instead minimize element-wise ℓ norm D (h) . Note that, scaling up
0 1,1 t 1,1
∥ ∥
Zˆ(h)byaconstantfactorscalesdownthescoredifferencesbythesamefactor. Hence,toprevent
thevanishingofthescoredifferencelosstrivially,weaddthefollowingregularizationtermtothe
optimizationobjective.
E Zˆ(h) 2 = E h(X) 2 . (402)
∥2 2
Wealsoaddthefollowingreconstru(cid:104) c(cid:13)tionloss(cid:105) toens(cid:104) u(cid:13)rethat(cid:13)hi(cid:105)
saninvertibletransform.
(cid:13) (cid:13) (cid:13)
E h−1(h(X)) X 2 . (403)
− 2
(cid:104)(cid:13) (cid:13) (cid:105)
Intheend,weminimizetheobjectivefu(cid:13)nction (cid:13)
D (h) +λ E h−1(h(X)) X 2 +λ E h(X) 2 . (404)
∥ t ∥1,1 1 − 2 2 2
Wedenotethefinalparameterestima(cid:104) t(cid:13) (cid:13)ebyH∗ andthee(cid:13) (cid:13)nc(cid:105) oderpar(cid:104) a(cid:13) (cid:13)meteriz(cid:13) (cid:13)ed(cid:105) byH∗ byh∗. Note
thatwedonotenforcethediagonalityconstraintuponD (h). Sincewelearnthelatentvariables
t
uptopermutation,wechangethisconstrainttoapost-processingstep. Specifically,wepermutethe
rowsofH∗ tomakeD (h∗)asclosetoasdiagonal,i.e., diag(D (h∗) I ) ismaximized.
t t n 1
∥ ⊙ ∥
InthesimulationresultsreportedinSection7.2,wesetλ = 10−4 andλ = 1,andsolve(390)
1 2
usingRMSpropoptimizerwithlearningrate10−3 for3 104 stepsforn = 5and4 104 stepsfor
× ×
n = 8. Recallthatthelatentgraphestimate ˆisconstructedusing1 D(h∗) . Weuseathreshold
G { }
λ toobtainthegraphfromtheuppertriangularpartofD(h∗)asfollows.
G
pˆa(i) = j : j < i and [D(h∗)] λ , i [n]. (405)
j,i G
{ ≥ } ∀ ∈
Wesetλ = 0.1forn = 5andλ = 0.2forn = 8.
G G
E.4 AdditionalResultsforGSCALE-I
Increasinglatentdimensionn. WerepeattheexperimentsinSection7.2forn = 8andreportthe
resultsinTable11. ThemainobservationsaresimilartothosefromTable5. Wenotethat with
G
n = 8nodesanddensity0.5hasanexpectednumberof14edges. Hence,havinganaverageSHDof
approximately1.5indicatesthatGSCALE-Iyieldsahighperformanceatrecoveringlatentcausal
relationshipsevenwhenthetransformationestimateisreasonablebutnotperfect.
Table11: GSCALE-Iforaquadraticcausalmodelwithtwocoupledhardinterventionspernode
(n = 8).
perfectscores noisyscores
n d n ℓ(Z,Zˆ) SHD( , ˆ) ℓ(Z,Zˆ) SHD( , ˆ)
s
G G G G
8 8 3 104 0.16 1.56 0.81 11.9
×
8 25 3 104 0.20 1.55 0.69 10.5
×
8 100 3 104 0.24 1.50 0.77 11.9
×
87