floZ: Evidence estimation from posterior samples with normalizing flows
Rahul Srinivasan1,2, Marco Crisostomi3,4, Roberto Trotta1,2,5,6, Enrico Barausse1,2, and Matteo Breschi1,2
1SISSA, Via Bonomea 265, 34136 Trieste, Italy and INFN Sezione di Trieste
2IFPU - Institute for Fundamental Physics of the Universe, Via Beirut 2, 34014 Trieste, Italy
3TAPIR, Division of Physics, Mathematics, and Astronomy,
California Institute of Technology, Pasadena, CA 91125, USA
4Dipartimento di Fisica, Universita` di Pisa, Largo B. Pontecorvo 3, 56127 Pisa, Italy
5Department of Physics, Imperial College London, United Kingdom and
6Italian Research Center on High Performance Computing, Big Data and Quantum Computing
(Dated: April 19, 2024)
We propose a novel method (floZ), based on normalizing flows, for estimating the Bayesian evi-
dence(anditsnumericaluncertainty)fromasetofsamplesdrawnfromtheunnormalizedposterior
distribution. Wevalidateitondistributionswhoseevidenceisknownanalytically,upto15parame-
terspacedimensions,andcomparewithtwostate-of-the-arttechniquesforestimatingtheevidence:
nested sampling (which computes the evidence as its main target) and a k-nearest-neighbors tech-
nique that produces evidence estimates from posterior samples. Provided representative samples
from the target posterior are available, our method is more robust to posterior distributions with
sharp features, especially in higher dimensions. It has wide applicability, e.g., to estimate the ev-
idence from variational inference, Markov-chain Monte Carlo samples, or any other method that
delivers samples from the unnormalized posterior density.
I. INTRODUCTION been proposed, which in the main are characterized by
the way the likelihood-constrained step is performed:
these include ellipsoidal [11], diffusive [12], dynamical
One of the most important scientific tasks is that of
[13] nested sampling; nested sampling with normalizing
discriminating between competing explanatory hypothe-
flows [14]; for a recent review, see [15]). One of the re-
ses for the data at hand. In classical statistics, this is
maining difficulties of nested sampling is its application
accomplished by means of a hypothesis test, in which a
to very large parameter spaces, where the curse of di-
null hypothesis (e.g., that the data do not contain a sig-
mensionality hobbles most algorithms. Progress in this
nal of interest) is rejected if, under the null, the proba-
direction is being made thanks to approaches such as
bilityofobservingdataasextremeormoreextremethan
PolyChord [16] and proximal nested sampling [17].
what has been observed is small (the so-called p-value).
An alternative –and often more fruitful– viewpoint is of- Other methods to compute the evidence range from
fered by a Bayesian approach to probability, in which the analytical (Laplace approximation and higher-order
the focus is shifted away from rejecting a null hypoth- moments [18], Savage-Dickey density ratio [19]) to the
esis to comparing alternative explanations (see [1] for a ones based on variations of density estimation, like par-
review). Thisisdonebymeansoftheposteriorodds(ra- allel tempering MCMC coupled with thermodynamic in-
tio of probabilities) between two (or several) competing tegration [20], and, more recently, on simulation-based
models. The central quantity for this calculation is the inferencewithneuraldensity(orneuralratio)estimation
Bayesian evidence, which gives the marginal likelihood and/or deep learning [21–23]. A promising approach is
forthedataundereachmodelonceallthemodelparam- that of evaluating the Bayesian evidence integral from a
eters have been integrated out, tensioning each model’s setofexistingposteriorsamples,previouslygatherede.g.
quality of fit against a quantitative notion of “Occam’s via MCMC. The interest lies in the ability of obtaining
razor”. Bayesian model comparison has received great the evidence from post-processing of posterior samples,
attention in cosmology (e.g. [2–4]), and is being adopted whichcanbeobtainedwithanysuitablealgorithm. Such
as a standard approach to model selection in the grav- a method, based on the distribution statistics of near-
itational waves community (e.g. [5–7]) as well as in the est neighbors of posterior samples, was presented in [24]
exoplanets one (e.g. [8, 9]). (see also [25] for a similar approach based on harmonic
reweighting of posterior samples).
The estimation of the Bayesian evidence is in general
achallengingtask, asitrequiresaveragingthelikelihood Here, we propose a new method based on normalizing
over the parameters’ prior over the entire model’s pa- flows. Normalizing flows are a type of generative learn-
rameter space. Various approaches have been proposed ing algorithms that aim to define a bijective transforma-
to this end. One that has gained particular prominence tionofasimpleprobabilitydistributionintoamorecom-
since its introduction by John Skilling [10] is nested plex distribution by a sequence of invertible and differ-
sampling – a method designed to transform the multi- entiablemappings. Normalizingflowshavebeeninitially
dimensionalintegraloftheBayesianevidenceintoaone- introduced in Ref. [26, 27] and then extended in various
dimensional integral. Since its invention, many differ- works, with applications to clustering classification [28],
entalgorithmicimplementationsofnestedsamplinghave densityestimation[29,30],andvariationalinference[31].
4202
rpA
81
]LM.tats[
1v49221.4042:viXra2
While normalizing flows have been used in several pa- independentsamplesandtheircorrespondingprobability
rameter estimation scenarios in cosmology and gravita- density.
tional wave physics [32–41], here we introduce them for
thefirsttimeasamethodtoevaluatetheevidence. This
paper is structured as follows: we introduce normalizing B. Evidence estimation
flows in Section II, and how a suitable loss can be de-
finedinordertoencodetheobjectiveofevidenceestima- If the target distribution p(x) is defined up to an un-
tion;inSectionIII,wevalidateourapproachontractable known normalization constant, Z, i.e. p(x) = pˆ(x)/Z
likelihood in up to 15 parameter space dimensions, and and since p(x) is normalized by definition, we have,
benchmarkitagainstdynesty[42](animplementationof
(cid:90)
nested sampling) and the k-nearest neighbors method of
Z = pˆ(x)dx. (3)
[24], demonstrating the superiority of our approach over
Rd
the latter. We conclude in Section IV with an outlook
on future applications. In the context of Bayesian analysis, p(x) represents the
posterior distribution of the parameters x, pˆ(x) is the
productoflikelihoodfunctionandpriordistribution(the
II. METHOD unnormalized posterior), and Z is the evidence. From
Eq. (3) we can write
A. Normalizing flows
pˆ(x)
dx=n(y)dy, (4)
Letx∈Rd beastochasticvariabledistributedaccord- Z
ing to a target probability – in our case, the posterior – from which it follows that
x ∼ p(x), which can be arbitrarily complex. Starting
f fl sr aoo ywm s ya a ∼lls oe nwt (yo o )f n ,es wa tm o hep m rl ee as p ne (t yx ht )era iv sc at are i nad b af l rr e bo xm itri anp rt( yox a) t, rl aan cto ter anm bt la esl pi bz a ai cn seg e, Z = pˆ( nf ϕ (y(y ))) (cid:12) (cid:12) (cid:12) (cid:12)det∂ ∂f yϕ(cid:12) (cid:12) (cid:12) (cid:12)= n(fpˆ − ϕ( 1x () x)) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)det∂ ∂f− ϕ x1(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)−1 . (5)
distribution. This transformation is performed training
Resorting to Eq. (1), the evidence in Eq. (5) can be
a neural network in order to construct a bijection map f
mapped using normalizing flows as
suchthatx=f (y),whereϕarethenetworkparameters
ϕ
that are defined by the form of the transformation and pˆ(x)
Z (cid:55)→ζ(x,ϕ)= . (6)
require to be optimized. Then, the target distribution
q (x)
can be mapped as ϕ
(cid:12) (cid:12) ∂f−1 (cid:12) (cid:12) Thiscomputationexploitsthecrucialpropertyofn(y)of
p(x)(cid:55)→q (x)=n(f−1(x)) (cid:12)det ϕ (x)(cid:12) , (1) being normalized by construction. Given an unnormal-
ϕ ϕ (cid:12) ∂x (cid:12)
(cid:12) (cid:12) ized statistical model pˆ(x) and a transformation f , the
ϕ
where f−1 is the inverse transformation of f , i.e. y = normalizationZ canbeexplicitlycomputedastheright-
ϕ ϕ hand side of Eq. (6). Note that this computation can be
f−1(x), and ∂f−1/∂x is its Jacobian. The transforma-
ϕ ϕ performedinthevariablexaswellasinthelatentspace
tion f can be arbitrarily complex, so that any distribu-
y, as shown in Eq. (5).
tion p(x) can be generated from any base distribution
Moreover, while the true value of Z is constant, the
n(y) under reasonable assumptions on the two distribu-
estimated ζ is a function of the variable x and of the
tions[43–45]. Inourimplementation,weconsidermasked
network parameters ϕ. In practice, for a given set of ϕ
autoregressive flows (MAFs) [46].
(aftertraining),theratioinEq.(6)willfluctuateaccord-
For our purposes, we focus on the amortized varia-
ing to the error of the trained flow f . We will address
ϕ
tional inference approach introduced in Ref. [31], where
and model this uncertainty in the following Section.
thebasedistributionn(y)isfixedtoasimpleprobability
function (generally taken to be normal with zero mean
and unit variance). Within this approach, when the tar-
C. Additional loss terms
get is to learn the unnormalized posterior, the network
parameters ϕ are usually optimized with a loss function
Assuming we have an arbitrarily flexible bijection and
definedasthecrossentropybetweenthetargetp(x)and
a combination of parameters ϕ∗ for which the transfor-
the reconstructed q (x), i.e.
ϕ mation f is exact, i.e. q (x) = p(x) ∀x; then, the
ϕ∗ ϕ∗
(cid:90) result of Eq. (6) is constant ζ(x,ϕ∗) = Z ∀x. However,
L (ϕ)=− p(x) log(q (x))dx
1 ϕ sincethefluxapproximationisneverexact,theestimated
Rd (2)
=−E (cid:2) log(q (x))(cid:3) , ζ i =ζ(x i,ϕ∗)ateachoftheposteriorsamples,x i,willbe
p(x) ϕ distributed around the true value of the evidence. While
whereE representstheexpectationvalueunderp(x), the minimum of Eq. (2) is expected to satisfy this condi-
p(x)
whichcanbecomputedbyasimpleaveragegivenasetof tion, we can additionally enforce this constraint as part3
of the loss to improve training. This means that we are
interested in the value of the network parameters ϕ∗ for
which the distribution of ζ, h(ζ|ϕ), results as close as L (ϕ) = |µ −1|≈|logµ |, (12)
3a g g
possible to a Dirac delta function. To this end, we de-
fineanadditionallossfunctionequaltothecross-entropy L (ϕ) = logσ . (13)
3b g
between the distribution h(ζ|ϕ) (estimated withnormal-
izing flows)1 and a δ function centered on the true value where µ and σ are estimated from the training sam-
g g
Z, i.e. ples via the sample average and standard deviation of ρ.
(cid:90) Notice that, |µ g−1|<σ g is required for the approxima-
L (ϕ)=− δ(ζ−Z)logh(ζ|ϕ)dζ. (7) tion (11) to hold. A distinguishing feature of L is that,
2 3
using pairs of samples for the evaluation, their number
Although Z is unknown and h(ζ|ϕ) cannot be evaluated is proportional to the square of the number of samples
point-wise, we can still estimate expectation values for used to evaluate L 1 and L 2. This introduces statistical
h(ζ|ϕ) from the training samples. robustness to the training, especially for small training
Assuming that h(ζ|ϕ) can be expanded in terms of sets.
its cumulants in a basis of Hermite polynomials, whose Inourimplementation,weexploitallthelossfunctions
leading-ordertermisanormaldistribution, fromEq.(7) described above, following a scheme that we present in
we obtain the next section (Sec. IID). This provides better results
than using any of the loss terms uniquely or in combina-
(µ −Z)2
L (ϕ)≃logσ + h +O(µ −Z)3, (8) tion.
2 h 2σ2 h
h
whereµ andσ2arerespectivelytheestimatesofthefirst
h h
and second moment of h(ζ|ϕ), derived from the training D. Algorithm
samples. Therefore,providedthat|µ −Z|<σ ,namely
h h
thattheground-truthofZ issufficientlyclosetothemean Givenasetofsamples{x },fori=1,...,N,extracted
i
oftheevidencedistribution,wecanneglectthequadratic fromp(x)andthecorrespondingunnormalizedprobabil-
termandapproximatethelossfunction(7)withthestan- ity values {pˆ(x )}, we proceed as follows:
i
darddeviationoftheestimatedevidencevaluesacrossthe
samples: L 2(ϕ)≃logσ h. 1. Pre-processing of input data: We prepare the sam-
L 2 helps to minimize the standard deviation of the ples for training and validation. The two datasets
evidence’s distribution, but does not provide any infor- contain independent samples. We whiten the sam-
mation about the mean, as Z is unknown. To improve ples to enhance the training process [24]. We do so
this feature, we consider the distribution of the ratio es- byprojectingthesamplesalongtheeigenvectorsof
timated for pairs of samples the covariance matrix of the samples and rescaling
bythesquarerootoftheeigenvaluestoensureunit
variance. Weassign80%ofthegeneratedwhitened
ζ(x ,ϕ)
ρ= i , (9) MCMC samples to the training dataset, and the
ζ(x ,ϕ)
j rest to the validation dataset.
which ideally would be a δ function centered at unity.
Thus, we can define a third loss function equal to the 2. Training the neural network: We train the normal-
cross-entropy between the distribution of the evidence izing flow on the training set employing gradient-
ratios g(ρ|ϕ) and δ(ρ−1): based optimization [47]. The loss function cycles
throughthefourpreviouslydiscussedlossfunctions
(cid:90)
L 3(ϕ)=− δ(ρ−1)logg(ρ|ϕ)dρ. (10) (L 1, L 2, L 3a, L 3b). As done in the framework of
transferlearning,thenetworkistrainedononeloss
before being adapted to minimize the next. Ini-
Applying the same considerations that led to Eq. (8),
tially, with L , the model is trained to learn the
we now obtain 1
generalfeaturesoftheposteriordistributiontopro-
(µ −1)2 vide a first estimate of the evidence – which is nec-
L (ϕ)≃logσ + g +O(µ −1)3, (11)
3 g 2σ g2 g essary to satisfy the assumptions on which L 2 re-
lies. Then, L incentivizes the pre-trained model
2
which can be broken into two separate losses toreducetheerrorintheevidenceestimation. L
3a
andL sequentiallyreducethedisparityinthenet-
3b
work’s evidence prediction from each sample. This
1 Noticethath(ζ|ϕ)isnotaprobabilitydistributionintheproper
sense; instead, it represents the histogram of the evidence ob- cyclictrainingregime,whereineachlossfunctionis
tainedateachposteriorsamplevaluefromtheflowapproxima- applied sequentially, repeats every N epoch. We
e
tion,q ϕ(xi). schedule the losses as follows:4
• Patience: If the optimizer does not find
 improvement in the loss of the validation
L (ϕ) e∈[0,0.25−t ),
αα
L
LL1
2(1
ϕ
(( ϕϕ
)
)) ++ (( 11 −− αα )) LL 2(ϕ (ϕ)
)
ee
e
∈∈
∈
[[
[
00
0
..
.
52 25
5
−,−
0
t.5t ,e
−
0,
.0e
5t.2
)e
,)5 ,),
d
t oh
va
e
et ra tts rre aat
ii
nna
ii
nf nt gge .r
t
Wea
r
emfi fiix
n
xe ad
tt hes
in s.u nm uTb mhe
i
br
s
ero isf toti ot 2e 0r
p
0a
r
.t ei vo en ns t,
L(ϕ)≡ 2 3a e
L (ϕ) e∈[0.5,0.75−t ), • Tolerance: As in the case of nested sampling,
α
L
αL
L3 3a
b3 (a
ϕ
(( ϕ)ϕ )) ++ (( 11 −− αα )) LL
3 (b
ϕ(ϕ )) e
e
e∈
∈ ∈[
[[
0
10.
.
−7 75
5
t,−
1 ,−
1t
e
), ,t0 e.
)7e
,5), i
a
it
nch
ti is
he
evp
e
eo
s
vs ias di epb nrle
e cd
et
e
eo
t se
trr ime mq
i
au
n
ti
e
ir ode
na
.t ch Wca ut
hra
et
c
nh ye
t(
hta eolg
l ee
ro
r
rr
a
oi nt rh
c
om
e n)
3b 1 e theestimatedevidenceislessthanthisthresh-
(14) old, the training is stopped. Given the ex-
wheree:=(epochmodN )/N representsthefrac- ploratory nature of this paper, we never used
e e
tional training epoch, t is the fractional tran- this condition.
e
sition period (0 < t < 0.25) that determines
e
3. Extracting evidence information: Given the opti-
the smoothness of the transition between one loss
mized set of network parameters ϕ∗, we estimate
term to the next, and α := min(1,max(0,(0.25−
ζ(x ,ϕ∗) according to Eq. (6), over a subset of
emod0.25)/t )). i
e
training samples {x }. Since q (x ) is more ac-
i ϕ∗ i
Each loss term uniquely contributes to 0.25 − t
e curate in the bulk of the distribution, following
fractional epochs before transitioning to the next
Ref. [48], we select the samples in the latent space
term. The transition is defined by the continuous
{y } within a sphere B centered about zero and
i
linear equation of the form αL +(1−α)L ,
prev next with Mahalanobis distance δ, which in the case of
where L , L are respectively the next and
next prev the Gaussian latent space simplifies as B = {y ∈
previous loss terms, and α ranges from 1 to 0 with √ B
y : ||y|| < δ}. We choose δ = d, i.e. containing
increasing epochs (as specified above). A sudden
the 1-σ region of the latent distribution.
transition (t ≪ 0.25) results in an oscillatory be-
e
havior, wherein the loss fluctuates between the lo-
cal minima of each loss term without net change.
III. VALIDATION AND SCALABILITY
However, a slow transition (t ∼ 0.25) results in
e
slow training, due to reduced training time allo-
InSecs.IIIAandIIIB,weillustrateexamplesofposte-
catedforeachlosstermseparately. Empirically,we
riorsampleswithtractable(i.e.analytic)andintractable
find the optimal training rate for a training cycle
(i.e. unknown analytically) evidence, respectively. Ta-
periodN =100,withatransitionperiodt =0.05
e e
ble I summarizes the different likelihood distributions
(corresponding to 5 epochs).
used to obtain the posterior samples. Using these ex-
As an alternative to the adaptive loss weighting,
amples, in Sec. IIIC we demonstrate the validity of our
we tested another formulation of the loss function,
methodandexploreitsdimensionalscalabilitybybench-
wherein the weights of the weighted sum of the in-
marking the results against existing evidence estimation
dividuallosstermsareoptimizedalongsidenetwork
techniques.
parameters ϕ:
L(ϕ,η,β,γ,δ)=ηL (ϕ)+βL (ϕ)
1 2
(15) A. Posterior samples with tractable evidence
+γL (ϕ)+δL (ϕ),
3a 3b
where η,β,γ,δ are optimizer-tuned weights with To validate our technique, we design the following un-
values that range from 0 to 1. However, the loss normalized posteriors, pˆ(x), with analytically tractable
scheduler was often more accurate and sometimes evidence. For simplicity, we choose a flat, rectangular
fastertoreachconvergencebywayofexceedingthe prior that defines the finite boundaries of the unnormal-
patience, especially for higher dimensions (>5). ized distribution.
The training terminates when at least one of the
• Truncated d-dimensional single Gaussian: We de-
following three conditions is satisfied:
fine a multivariate unnormalized Gaussian poste-
• Maximum iteration: The training stops rior in d dimensions, from which we draw samples
if the number of epochs exceeds a fixed truncated by the rectangular uniform prior. The
number of maximum iterations. We fix this distribution is defined in terms of a d-dimensional
number to 500 for all cases studied in this meanandad×dcovariancematrix. Giventhemul-
paper. However, more complex distributions tivariate Gaussian distribution of the latent vari-
orhigherdimensionscouldneedalargervalue. ables, this posterior is simple for the normalizing
flow to model, i.e. the normalizing flow obtained5
by minimizing the loss function is simply an affine C. Benchmarking
transformation.
• Truncated d-dimensional mixture of five Gaussian We evaluate the accuracy and efficiency of our flow-
distributions: Theunnormalizedposteriorisamix- based evidence estimation in comparison to the impor-
ture model of five different multivariate Gaussians, tance nested sampling technique discussed in Ref. [52],
withequalmixtureweights. EachofthefiveGaus- using the code in Ref. [42], and to a k-nearest neighbors
sians has a different mean and covariance matrix, (hereon, referred to as kNN) technique [24] to estimate
and the distribution is truncated by the rectangu- theevidencefromMCMCchainsofposteriorsamplesus-
lar prior. The resulting distribution is non-trivial ing the recommended k = 1 value. In the three panels of
to model, as discussed in Sec. IIIC. Figure 2, we show the comparison for the 2-dimensional
unnormalizedprobabilityfunctions,withaflatrectangu-
lar prior. We use 104 samples to evaluate the evidence
using our method and the kNN technique. The nested
B. Posterior samples with intractable evidence
sampling technique generates about 3·104 samples for
the evaluation. For d = 10 and d = 15, we evaluate
We also test the method with an unnormalized proba- theevidenceusing105 sampleswithourmethodandthe
bility distribution with analytically intractable evidence
kNN technique, whereas nested sampling uses between 5
fordimensionshigherthan2. Fordimensionshigherthan to9·104 samples. Weusethedefaultvaluesofthenested
3 the evidence is very expensive to compute even nu-
sampler’s tolerance and live points, which are 0.01 and
merically, therefore we do not provide a ground truth as
500 respectively.
reference.
InFigure3,weillustratethebehaviorofthelosssched-
• Truncated d-dimensional Rosenbrock distribution: ule for the Gaussian mixture case. The four individual
TheRosenbrockfunction[49]anditshigherdimen- loss terms are shown with different colors, the total loss
sional extensions described in Ref. [50] are often in thick/black; training losses are depicted with contin-
used to test the efficacy of MCMC sampling algo- uous lines and validation losses with dotted ones. As
rithms [51], as it is generally hard to probe the describedinSec.IID,weshowthateachlosstermsolely
maxima of the distribution. Moreover, the nor- contributes to 20 epochs before linearly transitioning to
malization constant is typically unknown. We use the next term in 5 epochs. In a span of 500 epochs, the
a d-dimensional Rosenbrock function as defined in network is trained on 5 cycles of the loss terms.
Ref. [50] and shown in Table I to describe the un-
normalized posterior. Figure 4 compares the evidence evaluated by floZ,
kNN, and nested sampling (labeled NS) techniques for
the three distributions, each simulated in 2, 10, and 15
Figure 1 shows examples of the three distributions in
dimensions. The single Gaussian scenario has the sim-
2-dimensions. The Gaussian distribution has mean
plest distribution and as a result, all methods perform
(cid:18) (cid:19) well. However, with increasing complexity and higher
23
µ= , dimensions, floZ and nested sampling are both more ac-
35
curate than kNN.
and covariance
The Rosenbrock likelihood provides an example with
(cid:18) (cid:19) intractable evidence. The ground truth of the 2-
299 31
σ = . dimensional case can be easily computed numerically,
31 284
however, for higher dimensions, the numerical computa-
tion becomes too expensive. We show that for 10 and 15
The mixture of five Gaussians has means
dimensions, nested sampling and floZ are in agreement
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) with each other within the estimated error bars.
39 30 18 46 28
µ = , , , , ,
j 19 38 12 44 28
Itisdifficulttofairlycomparethecomputationaleffort
required by floZ versus nested sampling, as the former
and covariance matrices
takes as input posterior samples previously generated.
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) Moreover, the efficiency of nested sampling depends on
29 8 250 15 152 4
σ = , , , the details of the algorithm used, as well as on its set-
j 8 118 15 171 4 32
tings(typically,thenumberoflivesamplesandtolerance
(cid:18) (cid:19) (cid:18) (cid:19)
173 12 198 17 adopted). Whileweleaveamoredetailedstudyofactual
, ,
12 107 17 468 scientific cases to future work, the d = 15 results of this
paper were obtained in less than 10 minutes on an A100
respectively. The Rosenbrock case has A = 100, B = 20. GPU.6
Functional form pˆ(x) Z
Gaussian exp(cid:2) −1(µ−x)Tσ−1(µ−x)(cid:3) I(x−x )I(x −x) (2π)d/2|σ|1/2[erf(x −µ,σ)−erf(µ−x ,σ)]
2 i f i f
Gaussian Mixture (cid:80)5 exp(cid:2) −1(µ −x)Tσ−1(µ −x)(cid:3) 1(cid:80)5 (2π)d/2|σ |1/2
j=1 2 j j j 5 j=1 j
I(x−x )I(x −x) [erf(x −µ ,σ )−erf(µ −x ,σ )]
i f i j j j f j
Rosenbrock exp{−Σd−1[A(x −x2)2+(1−x )2]/B}
j=1 j+1 j j
I(x−x )I(x −x)
i f
TABLEI.Summaryoftheunnormalizedposteriors(andtheirintegrals)usedtovalidateandbenchmarkourevidenceestima-
tion. The lower and upper bounds of the rectangular prior are given by x ,x , respectively, the indicator function I(z) = 1
i f
for z>0 and 0 elsewhere and erf(x,σ) is the multivariate Normal error function. The vectors µ,µ (j =1,...,5), covariance
j
matrices σ,σ (j =1,...,5)∈Rd×d and real scalars A,B are fixed.
j
FIG. 1. For each unnormalized posterior, we display 104 posterior samples for the case d = 2. For ease of comparison, the
unnormalizedposteriorpˆ(x)isscaledbyitsmaximumandshowninthecommoncolorbar. Theshadedgreyregionrepresents
the boundary of the rectangular prior.
FIG. 2. Evidence estimation in d=2 dimensions for (from left): multivariate Gaussian; finite multivariate Gaussian mixture;
Rosenbrock. Inallcases,floZandkNNemploy104 posteriorsamples. Thetruevalue,representedbythedashedline,hasbeen
rescaled to 0, and the shaded regions represent the 1-σ uncertainty.7
FIG. 3. The evolution of the network’s loss as a function of training epochs for the case of a 2-dimensional mixture of five
Gaussians, illustrates the loss schedule. The four loss terms are shown in color, the total loss in thick/black, and the training
(validation) losses are shown in solid (dotted) lines.
FIG. 4. The relative error in the log evidence estimation for different distributions in each panel as a function of the dimen-
sionality of parameter space, d. We compare the results of floZ and kNN using the same MCMC samples. We also show
the evidence from nested sampling (labeled as NS). Since the ground truth for the higher-dimensional Rosenbrock likelihoods
(grey-shaded region) is not numerically tractable, we compare the relative deviation from the floZ mean prediction (shown by
the secondary y-axis).8
IV. CONCLUSIONS of the distribution), which opens the door to potentially
exploiting the existence of a well-defined ‘typical set’ in
In this work, we have introduced floZ, a novel method high dimensions, a consequence of the concentration of
using normalizing flows for estimating the Bayesian ev- measure phenomenon. We shall explore this idea further
idence from posterior samples. Our approach computes in a dedicated future paper.
the Bayesian evidence and its numerical uncertainty us- Among the applications of floZ to gravitational-wave
ing pre-existing samples drawn from the target unnor- astronomy,itisworthmentioningtherecentevidencefor
malized posterior distribution, e.g. as obtained by stan- astochasticsignalreportedbypulsartimingarrayexper-
dard MCMC methods. iments [53–57]. Within a Bayesian framework, interpret-
Normalizing flows are built to map a complex target ingthenatureofthisstochasticsignalrequirescomparing
probability function (in our case, the unnormalized pos- differenthypothesesandtheirevidences,whichfloZcould
terior distribution, given by the product of a likelihood readilycomputefromtheexistingsamplesreleasedbythe
and prior) into a simple probability distribution (e.g. a experiments. Similarly, as another of the many possible
Gaussian). Thetransformationbetweentheoriginalvari- applications, floZ could be used to investigate the sta-
ablesandthelatentonesisbijectiveandcanbemodeled tisticalrobustnessoffeatures/peaksthatmaybepresent
with a neural network. Importantly for our method, the in the mass function of the astrophysical black holes de-
flows encode the volume of the real-space distribution, tected by the LIGO-Virgo-KAGRA collaboration [58].
i.e. the Bayesian evidence. Our method hinges on this Data availability: The floZ code and all the neces-
facttocomputetheevidencefromasetofposteriorsam- sary files to reproduce the results in this paper will soon
ples previously gathered by any suitable algorithm (e.g., be made available.
MCMC), while at the same time minimizing the scatter
of the estimates of the evidence across the samples by
adding suitable terms to the custom loss function.
ACKNOWLEDGMENTS
We have validated floZ against analytical benchmarks
in parameter spaces of up to 15 dimensions: multivari-
ate Gaussians, a finite multivariate Gaussian mixture, We thank Alan Heavens for insightful discussions.
as well as the Rosenbrock distribution. Our method E.B., R.S. and M.B. acknowledge support from the Eu-
demonstrates performance comparable to nested sam- ropean Union’s H2020 ERC Consolidator Grant “GRav-
pling(whichrequirestoberunadhoc)andsuperiortoa ity from Astrophysical to Microscopic Scales” (Grant
k-nearestneighborsmethodemployedonthesameposte- No. GRAMS-815673), thePRIN2022grant“GUVIRP-
riorsamples. WebelievethatfloZwilladdtothetoolbox Gravity tests in the UltraViolet and InfraRed with Pul-
of astronomers and cosmologists seeking a fast, reliable sar timing”, and the EU Horizon 2020 Research and In-
method to compute the evidence from existing sets of novation Programme under the Marie Sklodowska-Curie
posterior samples. floZ will be especially useful for cases Grant Agreement No. 101007855. M.C. is funded by
where the likelihood is expensive and a full nested sam- the European Union under the Horizon Europe’s Marie
plingrunisdifficulttoachieve,whileMCMCsamplescan Sklodowska-Curieproject101065440. R.T.acknowledges
be obtained more efficiently thanks to the ease of par- co-funding from Next Generation EU, in the context of
allelization of many MCMC algorithms (such as Gibbs the National Recovery and Resilience Plan, Investment
sampling). Pushing our approach to much larger param- PE1 – Project FAIR “Future Artificial Intelligence Re-
eter spaces is challenged by two limitations, both fun- search”. Thisresourcewasco-financedbytheNextGen-
damentally stemming from the curse of dimensionality: eration EU [DM 1555 del 11.10.22]. RT is partially sup-
first, obtaining posterior samples in high dimensions be- ported by the Fondazione ICSC, Spoke 3 “Astrophysics
comes typically more challenging (although some meth- and Cosmos Observations”, Piano Nazionale di Ripresa
ods, like Gibbs sampling and Hamiltonian Monte Carlo, e Resilienza Project ID CN00000013 “Italian Research
canshowmildscalingwithdimensionalityinsomefavor- Center on High-Performance Computing, Big Data and
able circumstances); second, training the flow in higher QuantumComputing”fundedbyMURMissione4Com-
dimensions suffers from increasing inaccuracies. How- ponente 2 Investimento 1.4: Potenziamento strutture di
ever, we note that we only need accurate densities in ricercaecreazionedi“campioninazionalidiR&S(M4C2-
latent space near the peak (as opposed to into the tails 19 )” - Next Generation EU (NGEU).
[1] R. Trotta, “Bayes in the sky: Bayesian inference and (2014) 039, arXiv:1312.3529 [astro-ph.CO].
model selection in cosmology,” Contemporary Physics [3] A. F. Heavens and E. Sellentin, “Objective Bayesian
49 no. 2, (Mar., 2008) 71–104, arXiv:0803.4089 analysis of neutrino masses and hierarchy,” JCAP 04
[astro-ph]. (2018) 047, arXiv:1802.09450 [astro-ph.CO].
[2] J. Martin, C. Ringeval, R. Trotta, and V. Vennin, “The [4] A. Heavens, Y. Fantaye, E. Sellentin, H. Eggers,
Best Inflationary Models After Planck,” JCAP 03 Z. Hosenie, S. Kroon, and A. Mootoovaloo, “No9
Evidence for Extensions to the Standard Cosmological [19] R. Trotta, “Applications of Bayesian model selection to
Model,” Phys. Rev. Lett. 119 no. 10, (Sept., 2017) cosmological parameters,” Mon. Not. Roy. Astron. Soc.
101301, arXiv:1704.03467 [astro-ph.CO]. 378 (2007) 72–82, arXiv:astro-ph/0504022.
[5] W. Del Pozzo, J. Veitch, and A. Vecchio, “Testing [20] P. M. Goggans and Y. Chi, “Using Thermodynamic
General Relativity using Bayesian model selection: Integration to Calculate the Posterior Probability in
Applications to observations of gravitational waves from Bayesian Model Selection Problems,” AIP Conference
compact binary systems,” Phys. Rev. D 83 (2011) Proceedings 707 no. 1, (04, 2004) 59–66.
082002, arXiv:1101.1391 [gr-qc]. https://doi.org/10.1063/1.1751356.
[6] S.J.VigelandandM.Vallisneri,“Bayesianinferencefor [21] K. Karchev, R. Trotta, and C. Weniger, “SimSIMS:
pulsar timing models,” Mon. Not. Roy. Astron. Soc. Simulation-based Supernova Ia Model Selection with
440 no. 2, (2014) 1446–1457, arXiv:1310.2606 thousands of latent variables,” arXiv e-prints (Nov.,
[astro-ph.IM]. 2023) arXiv:2311.15650, arXiv:2311.15650
[7] A. Toubiana, K. W. K. Wong, S. Babak, E. Barausse, [astro-ph.CO].
E. Berti, J. R. Gair, S. Marsat, and S. R. Taylor, [22] N. Jeffrey and B. D. Wandelt, “Evidence Networks:
“Discriminating between different scenarios for the simple losses for fast, amortized, neural Bayesian model
formation and evolution of massive black holes with comparison,” Mach. Learn. Sci. Tech. 5 no. 1, (2024)
LISA,” Phys. Rev. D 104 (2021) 083027, 015008, arXiv:2305.11241 [cs.LG].
arXiv:2106.13819 [gr-qc]. [23] S. T. Radev, M. D’Alessandro, U. K. Mertens, A. Voss,
[8] B. Lavie, J. M. Mendonc¸a, C. Mordasini, M. Malik, U. Ko¨the, and P.-C. Bu¨rkner, “Amortized Bayesian
M. Bonnefoy, B.-O. Demory, M. Oreshenko, S. L. model comparison with evidential deep learning,” arXiv
Grimm, D. Ehrenreich, and K. Heng, “Helios–retrieval: e-prints (Apr., 2020) arXiv:2004.10629,
An open-source, nested sampling atmospheric retrieval arXiv:2004.10629 [stat.ML].
code; application to the hr 8799 exoplanets and inferred [24] A. Heavens, Y. Fantaye, A. Mootoovaloo, H. Eggers,
constraints for planet formation,” The Astronomical Z. Hosenie, S. Kroon, and E. Sellentin, “Marginal
Journal 154 no. 3, (Aug., 2017) 91. Likelihoods from Monte Carlo Markov Chains,”
http://dx.doi.org/10.3847/1538-3881/aa7ed8. arXiv:1704.03472 [stat.CO].
[9] A. Lueber, D. Kitzmann, B. P. Bowler, A. J. Burgasser, [25] J. D. McEwen, C. G. R. Wallis, M. A. Price, and
and K. Heng, “Retrieval Study of Brown Dwarfs across A. Spurio Mancini, “Machine learning assisted Bayesian
the L-T Sequence,” ApJ 930 no. 2, (May, 2022) 136, model comparison: learnt harmonic mean estimator,”
arXiv:2204.01330 [astro-ph.EP]. arXiv e-prints (Nov., 2021) arXiv:2111.12720,
[10] J. Skilling, “Nested Sampling,” AIP Conference arXiv:2111.12720 [stat.ME].
Proceedings 735 no. 1, (11, 2004) 395–405. [26] E. Tabak and E. Vanden-Eijnden, “Density estimation
https://doi.org/10.1063/1.1835238. by dual ascent of the log-likelihood,” Communications
[11] F. Feroz, M. P. Hobson, and M. Bridges, in Mathematical Sciences 8 no. 1, (2010) 217–233.
“MULTINEST: an efficient and robust Bayesian [27] E. Tabak and C. Turner, “A family of nonparametric
inference tool for cosmology and particle physics,” density estimation algorithms,” Communications on
MNRAS 398 no. 4, (Oct., 2009) 1601–1614, Pure and Applied Mathematics 66 no. 2, (Feb., 2013)
arXiv:0809.3437 [astro-ph]. 145–164.
[12] B. J. Brewer, L. B. Pa´rtay, and G. Csa´nyi, “Diffusive [28] J. P. Agnelli, M. Cadeiras, E. G. Tabak, C. V. Turner,
Nested Sampling,” arXiv e-prints (Dec., 2009) and E. Vanden-Eijnden, “Clustering and classification
arXiv:0912.2380, arXiv:0912.2380 [stat.CO]. through normalizing flows in feature space,” Multiscale
[13] E. Higson, W. Handley, M. Hobson, and A. Lasenby, Modeling & Simulation 8 no. 5, (2010) 1784–1802,
“Dynamic nested sampling: an improved algorithm for https://doi.org/10.1137/100783522.
parameter estimation and evidence calculation,” https://doi.org/10.1137/100783522.
Statistics and Computing 29 no. 5, (Sept., 2019) [29] O. Rippel and R. P. Adams, “High-dimensional
891–913, arXiv:1704.03459 [stat.CO]. probability estimation with deep density models,” 2013.
[14] A. Moss, “Accelerated Bayesian inference using deep [30] P. Laurence, R. Pignol, and E. Tabak, Constrained
learning,” Mon. Not. Roy. Astron. Soc. 496 no. 1, density estimation, pp. 259–284. Springer-Verlag, 2014.
(2020) 328–338, arXiv:1903.10860 [astro-ph.CO]. [31] D. J. Rezende and S. Mohamed, “Variational inference
[15] J. Buchner, “Nested Sampling Methods,” Statistics with normalizing flows,” in Proceedings of the 32nd
Surveys 17 (Jan., 2023) 169–215, arXiv:2101.09675 International Conference on International Conference
[stat.CO]. onMachineLearning, vol.37 ofICML’15, p.1530–1538.
[16] W. J. Handley, M. P. Hobson, and A. N. Lasenby, JMLR.org, 2015.
“POLYCHORD: next-generation nested sampling,” [32] S. R. Green, C. Simpson, and J. Gair,
MNRAS 453 no. 4, (Nov., 2015) 4384–4398, “Gravitational-wave parameter estimation with
arXiv:1506.00171 [astro-ph.IM]. autoregressive neural network flows,” Phys. Rev. D 102
[17] X. Cai, J. D. McEwen, and M. Pereyra, “Proximal no. 10, (2020) 104057, arXiv:2002.07656
nested sampling for high-dimensional Bayesian model [astro-ph.IM].
selection,” arXiv e-prints (June, 2021) [33] S.R.GreenandJ.Gair,“Completeparameterinference
arXiv:2106.03646, arXiv:2106.03646 [stat.ME]. for GW150914 using deep learning,” Mach. Learn. Sci.
[18] E. Ruli, N. Sartori, and L. Ventura, “Improved Laplace Tech. 2 no. 3, (2021) 03LT01, arXiv:2008.03312
approximation for marginal likelihoods,” Electronic [astro-ph.IM].
Journal of Statistics 10 no. 2, (2016) 3986 – 4009. [34] M. Dax, S. R. Green, J. Gair, J. H. Macke,
https://doi.org/10.1214/16-EJS1218. A. Buonanno, and B. Scho¨lkopf, “Real-Time10
Gravitational Wave Science with Neural Posterior neural information processing systems 30 (2017) .
Estimation,” Phys. Rev. Lett. 127 no. 24, (2021) [47] D. P. Kingma and J. Ba, “Adam: A method for
241103, arXiv:2106.12594 [gr-qc]. stochastic optimization,” CoRR abs/1412.6980 (Dec.,
[35] M. Dax, S. R. Green, J. Gair, M. Deistler, B. Scho¨lkopf, 2014) arXiv:1412.6980, arXiv:1412.6980 [cs.LG].
and J. H. Macke, “Group equivariant neural posterior [48] T. J. DiCiccio, R. E. Kass, A. Raftery, and
estimation,” arXiv:2111.13139 [cs.LG]. L. Wasserman, “Computing bayes factors by combining
[36] M. Dax, S. R. Green, J. Gair, M. Pu¨rrer, J. Wildberger, simulation and asymptotic approximations,” Journal of
J. H. Macke, A. Buonanno, and B. Scho¨lkopf, “Neural the American Statistical Association 92 no. 439, (1997)
Importance Sampling for Rapid and Reliable 903–915. http://www.jstor.org/stable/2965554.
Gravitational-Wave Inference,” Phys. Rev. Lett. 130 [49] H. Rosenbrock, “An automatic method for finding the
no. 17, (Apr., 2023) 171403, arXiv:2210.05686 greatest or least value of a function,” Comput. J. 3
[gr-qc]. (1960) 175–184. https:
[37] J. Wildberger, M. Dax, S. R. Green, J. Gair, M. Pu¨rrer, //api.semanticscholar.org/CorpusID:62755334.
J. H. Macke, A. Buonanno, and B. Scho¨lkopf, [50] J. Goodman and J. Weare, “Ensemble samplers with
“Adapting to noise distribution shifts in flow-based affine invariance,” Communications in Applied
gravitational-wave inference,” arXiv:2211.08801 Mathematics and Computational Science 5 no. 1, (Jan.,
[gr-qc]. 2010) 65–80.
[38] U. Bhardwaj, J. Alvey, B. K. Miller, S. Nissanke, and [51] F. Pagani, M. Wiegand, and S. Nadarajah, “An
C. Weniger, “Peregrine: Sequential simulation-based n-dimensional rosenbrock distribution for mcmc
inference for gravitational wave signals,” testing,” Scandinavian Journal of Statistics 49 (04,
arXiv:2304.02035 [gr-qc]. 2021) .
[39] M. Crisostomi, K. Dey, E. Barausse, and R. Trotta, [52] J. S. Speagle, “DYNESTY: a dynamic nested sampling
“Neural posterior estimation with guaranteed exact package for estimating Bayesian posteriors and
coverage: The ringdown of GW150914,” Phys. Rev. D evidences,” MNRAS 493 no. 3, (Apr., 2020) 3132–3158,
108 no. 4, (2023) 044029, arXiv:2305.18528 [gr-qc]. arXiv:1904.02180 [astro-ph.IM].
[40] K. Leyde, S. R. Green, A. Toubiana, and J. Gair, [53] EPTA, InPTA: Collaboration, J. Antoniadis et al.,
“Gravitational wave populations and cosmology with “The second data release from the European Pulsar
neural posterior estimation,” Phys. Rev. D 109 no. 6, Timing Array - III. Search for gravitational wave
(2024) 064056, arXiv:2311.12093 [gr-qc]. signals,” Astron. Astrophys. 678 (2023) A50,
[41] D. Shih, M. Freytsis, S. R. Taylor, J. A. Dror, and arXiv:2306.16214 [astro-ph.HE].
N. Smyth, “Fast Parameter Inference on Pulsar Timing [54] P. Tarafdar et al., “The Indian Pulsar Timing Array:
Arrays with Normalizing Flows,” arXiv:2310.12209 First data release,” Publ. Astron. Soc. Austral. 39
[astro-ph.IM]. (2022) e053, arXiv:2206.09289 [astro-ph.IM].
[42] S. Koposov, J. Speagle, K. Barbary, G. Ashton, [55] NANOGrav Collaboration, G. Agazie et al., “The
E. Bennett, J. Buchner, C. Scheffler, B. Cook, NANOGrav 15 yr Data Set: Evidence for a
C. Talbot, J. Guillochon, P. Cubillos, A. A. Ramos, Gravitational-wave Background,” Astrophys. J. Lett.
B. Johnson, D. Lang, Ilya, M. Dartiailh, A. Nitz, 951 no. 1, (2023) L8, arXiv:2306.16213
A. McCluskey, and A. Archibald, “joshspeagle/dynesty: [astro-ph.HE].
v2.1.3,” Oct., 2023. [56] D. J. Reardon et al., “Search for an Isotropic
https://doi.org/10.5281/zenodo.8408702. Gravitational-wave Background with the Parkes Pulsar
[43] C. Villani, Topics in optimal transportation. Graduate Timing Array,” Astrophys. J. Lett. 951 no. 1, (2023)
studiesinmathematics.Americanmathematicalsociety, L6, arXiv:2306.16215 [astro-ph.HE].
Providence, Rhode Island, 2003. [57] H. Xu et al., “Searching for the Nano-Hertz Stochastic
[44] V. I. Bogachev, A. V. Kolesnikov, and K. V. Medvedev, Gravitational Wave Background with the Chinese
“Triangular transformations of measures,” Sbornik: Pulsar Timing Array Data Release I,” Res. Astron.
Mathematics 196 no. 3, (Apr, 2005) 309. https: Astrophys. 23 no. 7, (2023) 075024, arXiv:2306.16216
//dx.doi.org/10.1070/SM2005v196n03ABEH000882. [astro-ph.HE].
[45] K. Medvedev, “Certain properties of triangular [58] KAGRA, VIRGO, LIGO Scientific Collaboration,
transformations of measures,” Theory of Stochastic R. Abbott et al., “Population of Merging Compact
Processes 1 (01, 2008) . Binaries Inferred Using Gravitational Waves through
[46] G. Papamakarios, T. Pavlakou, and I. Murray, “Masked GWTC-3,” Phys. Rev. X 13 no. 1, (2023) 011048,
autoregressive flow for density estimation,” Advances in arXiv:2111.03634 [astro-ph.HE].