6Img-to-3D: Few-Image Large-Scale Outdoor
Driving Scene Reconstruction
Théo Gieruc1,2,∗, Marius Kästingschäfer2,∗, Sebastian Bernhard2, and Mathieu
Salzmann1
1 SDSC & CVLab, EPFL
{theo.gieruc, mathieu.salzmann}@epfl.ch
2 Continental Automotive Technologies GmbH, AI Lab Berlin
{marius.kaestingschaefer, sebastian2.bernhard}@continental.com
Fig.1:Qualitativeresultsof6Img-to-3D.Duringinferencetime,6Img-to-3Dtakes
six surround vehicle RGB images as input and returns a parameterized triplane from
which arbitrary views can be rendered.
Abstract. Current 3D reconstruction techniques struggle to infer un-
boundedscenesfromafewimagesfaithfully.Specifically,existingmeth-
odshavehighcomputationaldemands,requiredetailedposeinformation,
and cannot reconstruct occluded regions reliably. We introduce 6Img-
to-3D, an efficient, scalable transformer-based encoder-renderer method
for single-shot image to 3D reconstruction. Our method outputs a 3D-
consistentparameterizedtriplanefromonlysixoutward-facinginputim-
ages for large-scale, unbounded outdoor driving scenarios. We take a
step towards resolving existing shortcomings by combining contracted
custom cross- and self-attention mechanisms for triplane parameteriza-
tion, differentiable volume rendering, scene contraction, and image fea-
tureprojection.Weshowcasethatsixsurround-viewvehicleimagesfrom
asingletimestampwithoutglobalposeinformationareenoughtorecon-
struct 360◦ scenes during inference time, taking 395 ms. Our method
allows, for example, rendering third-person images and birds-eye views.
Ourcodeisavailablehere,andmoreexamplescanbefoundatourweb-
site https://6Img-to-3D.GitHub.io/.
Keywords: Scene Representation · Few-View Reconstruction · 3D Re-
construction · Autonomous Vehicles
*These authors contributed equally to this work.
4202
rpA
81
]VC.sc[
1v87321.4042:viXra2 T. Gieruc et al.
1 Introduction
Inferring the appearance and geometry of large-scale outdoor scenes from a few
camera inputs is a challenging, unsolved problem. The problem is character-
ized by the complexity inherent in outdoor scenes, namely a vast spatial extent,
diverse object textures, and ambiguity of the 3D geometry when resolving oc-
clusions. Robotics and autonomous driving systems both require methods that
processvision-centricfirst-personviewsfromafixedsetofcamerasasinputand
derivesuitablecontrolcommandsfromthese[80].Theabilitytoinstantaneously
perform 2D-to-3D scene reconstructions would be useful within such methods
and thus have a broad applicability within the robotics [2,51,60,89] and au-
tonomousdriving[8,15,61,111]domains.Instantaneouslygeneratingunoccluded
bird’s-eyeviewsindifficultparkingorslowdrivingscenarioswould,forexample,
greatly assist human drivers. For applying one-shot images to high-fidelity 3D
techniques in vehicles or robots, additional factors, such as the inference speed,
the quality of the inferred representation, the scalability of the approach, and
thenumberofcameraparametersrequiredforthetechniquetowork,areequally
important.Forthosesafety-criticaldomains,avoidingoverlycompressingthe3D
scene or altering scene details by adding or omitting road obstacles is crucial.
For those safety-critical domains, avoiding overly compressing the 3D scene or
altering scene details by adding or omitting road obstacles is crucial.
Despiterecentprogressin3Dreconstructionfromafeworsingle2Dimages,
many methods are either limited to single objects [27,45,46,76,96] or to gener-
ative indoor view synthesis [12]. Existing few-shot methods developed for single
objects are often not easily transferable to large-scale or unbounded outdoor
settings. This is because outdoor settings are more diverse due to their varying
structural composition, occlusions, and vast extent. Another limitation is that
many methods require a high degree of overlap between the input images, such
as found in spherically arranged and inward-facing camera setups focused on
a single object in the center [55,103]. This is opposed to the outward-facing
camerasetupessentialforautonomousdrivingwherethecameraoverlapismin-
imal [6,9,74]. The camera setups are contrasted in Fig. 2. Traditional methods
in autonomous driving have shown promising results with such camera setups
butlackvisualfidelitysincetheyonlypredictsemanticoccupancy[28,72,81,87]
and commonly rely on additional sensors such as LiDAR.
Given the above limitations, we present 6Img-to-3D, a novel transformer-
baseddecoder-renderermethodforsingle-shotimageto3Dreconstruction.6Img-
to-3D is fully differentiable and end-to-end trained on a single GPU. It simul-
taneously takes six outward-facing images (6 Img) as inputs and generates a
parameterized triplane (to 3D) from which novel scene views can be rendered.
Unlike previous methods, we do not focus on single objects but instead on com-
plex driving scenarios characterized by a high-depth complexity recorded only
by sparse multi-view outward-facing RGB cameras. Furthermore, unlike other
methods, we do not use a multi- or few-shot scheme where an initial reconstruc-
tionistestedandimprovediteratively.Instead,weprovidethe3Dreconstruction
in a single shot without iterative model refinement. To this end, we utilize pre-6Img-to-3D 3
Fig.2: View Overlap. Inward and outward-facing camera setups differ significantly
in their view overlap. Ourward-facing (inside-out) camera setups overlap minimally,
whereas inward-facing (outside-in) setups can overlap across multiple cameras. The
source of the excavator images is [54,55].
trainedResNetfeatures,cross-andself-attentionmechanismsforparameterizing
thetriplane,andimagefeatureprojectiontoconditiontherenderer.Ourmethod
has 61M parameters and is trained using roughly 11.4K input and 190K super-
visingimagesfrom1900scenes.WedonotrequiredepthorLiDARinformation.
Due to the absence of suitable datasets, including a large number of non-ego
vehicle views needed during training, we rely on synthetic data from Carla [19].
The contributions of this paper can be summarized as follows:
1. Visual Fidelity and Unboundedness. We present 6Img-to-3D, an effi-
cient, single-shot, few-image reconstruction pipeline for large-scale outdoor
environments.Ourmethodoutputsaparameterizedtriplaneofthe3Dscene
fromwhicharbitraryviewpointscanberendered.Wecombinetriplane-based
differentiable volume rendering, renderer conditioning on projected image
features, self- and custom cross-attention mechanisms, and an LPIPS loss.
2. Scaling and Hardware Efficiency. We demonstrate the favorable scaling
properties of our architecture regarding the amount of training data. Unlike
other large-scale 2D-to-3D methods, 6Img-to-3D trains and runs on a single
42GB GPU, allowing easier transfer of the method to embedded systems.
3. Testing and Abblations.Weperformextensiveadditionaltestsandabla-
tionstudiestoconfirmtheeffectofindividualmodelcomponentssuchasthe
LPIPS loss, projected image features conditioning, and scene contraction.
4. Self-Supervised and Runtime. We establish that our method trained
by using self-supervised learning generalizes well to novel scenes without
inputting additional camera poses other than those required to define the
novel views. Since our method only requires a single-forward pass during
inference, obtaining a parameterized triplane is fast and only takes 395ms.
Our code is available here.4 T. Gieruc et al.
2 Related Work
3D Representations. Traditional methods for representing 3D scenes include
voxels [24,68,75,88,91,97], point clouds [1,20,75,100,105,106], signed dis-
tance field (SDF) [38,84] and polygon meshes [32,48,57,75,83,94]. Recently,
numerousnewimplicitandexplicitrepresentationshaveemergedforlearning3D
scenes [23,79,92]. Implicit neural fields such as Neural Radiance Fields (NeRFs)
model the surrounding appearance and geometry using a continuously queri-
able function. They can be updated through differentiable volumetric render-
ing [23,49,55,59,77] and either utilize a single [17,55,108] or multiple [64,66]
multilayer perceptrons (MLPs). However, these models are slow to train, data-
intensive,andexpensivetorendersincequeryingtheimplicitneuralfieldiscom-
putationally demanding. Explicit approaches such as Plenoctrees [102], Plenox-
els [101], and NSVF [44] employ trilinear interpolation within a 3D grid to rep-
resent the scene. This significantly improves rendering and optimization time
but comes at the expense of limited scalability due to the curse of dimension-
ality when increasing the resolution or scene size. Hybrid models [56,73] adopt
an intermediary representation, storing values in a voxel-like grid. These val-
ues are then decoded using viewing direction through a shallow MLP to infer
color and density. Some models further refine this process by decomposing the
voxel grid into vector-matrix, using the CANDECOMP/PARAFAC (CP) de-
compositions [13] or into three or more orthogonal planes called triplanes or
K-Planes [5,11,21]. This enables the computation of features for each point in
space by aggregating the feature values of projected 3D points onto each plane,
transitioningfromanN-cubevoxelgridtoa3×N-squaredplanerepresentation.
Few-Image to 3D Representation. Single-scene 3D reconstruction methods
require many images to generalize towards novel views since the reconstruction
problemwouldbeunderconstrainedotherwise[55].Hence,few-imageto3Drep-
resentationmethodsincorporateadditionalglobalorlocalinformationtofurther
regularizeandconstraintheproblem.Globalregularizationmethodsrelyonad-
ditional model priors, either via applying further model regularization [58,98]
or by utilizing pre-trained image models to provide an extra guidance signal to
inform the optimization of the single-scene [18,42,45,53,62,63,69,93,96]. Local
regularization methods train a cross-scene multi-perspective aggregator. Such
models include PixelNeRF [103], IBRNet [85], MVSNeRF [14], VolRecon [67]
and others [31,40,47,82]. They retrieve image features via view projection and
aggregate resulting features using MLPs or transformers to obtain novel views.
We also rely on pixel feature conditioning, but unlike our method, many of
the mentioned local regularization methods rely on a large overlap between in-
put images ensured by using spherical inward-facing cameras. Another category
of methods does not train the representation on a per-scene basis but instead
trainsameta-networkthatoutputsthediscreteparameterizedscenerepresenta-
tion.Modelssuchas[3,22,25,71,86]usediffusiontogenerate3Drepresentations
of scenes. Many of those models use the triplane representation [3,22,25,71],
whereassomedirectlygenerateintheimage-space[86]orcanbeconditionedon
input images [3,25,86]. Other methods [7,11] use GANs to generate 3D repre-6Img-to-3D 5
sentations.ThemethodthatmostcloselyresemblesourworkisLRM[27],which
utilizes cross- and self-attention to parameterize a triplane using a few object
images. LRM focuses on single objects and does not handle unbound scenes. In
addition,wetrainonasingleA40GPU,whereasLRMistrainedon128NVIDIA
A100 GPUs and has seven times more parameters.
Large-Scale 3D Scene Representations. Extending NeRFs to unbounded
scenes requires spatially contracting the scene is usually done proportionally
to disparity as introduced in MipNeRF-360 [4] or using an inverted sphere
parametrization as in NeRF++ [109]. Architectures modeling outdoor driv-
ing scenes are BlockNeRF [78], Neural Scene Graphs [61], DisCoScene [95],
NeuralField-LDM [33] and Neural ground planes [70]. Those are, however, not
dealing with sparse input views. Neo360 [30] utilizes local image features to in-
feranimage-conditionaltriplanarrepresentation,wherebythemodeldissociates
foreground from background scene parts. Unlike our method, Neo360 is trained
on 8 A100 GPUs and focuses on inward-facing camera perspectives as input to
themodel.Duetothelargerviewoverlap,Neo360thussolvesaneasier2D-to-3D
lifting task. Large-scale scene generation methods such as Infinite Nature [43],
InfiniteNature-Zero [36], PathDreamer [35], CityDreamer [90], Infinicity [39],
SceneDreamer [16] and PersistentNature [10] often produce additional images
in an autoregressive fashion without the ability to input camera coordinates for
novel view synthesis control. In autonomous driving, 3D visual perception tasks
include 3D detection and map segmentation based on multi-camera images. Es-
tablished methods create a bird’s-eye-view (BEV) semantic representation of a
scene using either Inverse Perspective Mapping (IPM) [65] or attention mecha-
nisms [37,99]. TPVFormer [29], a work that our architecture is partially based
on, maps six outward-facing vehicle 2D images onto three orthogonal planes
using self- and cross-deformable attention mechanisms to obtain semantic occu-
pancy predictions. The method is, however, supervised with depth information
fromaLiDARsensorand,unlikeourmethod,onlypredictssemanticoccupancy.
3 6Img-to-3D
Given six outward-facing images I , their camera extrinsic (relative pose) ma-
ego
trices M and associated camera intrinsic matrices K with appropriate dimen-
sions, the goal is to reconstruct the surrounding 3D occupancy and appearance,
that is the scene S. Since the values of the scene S are not known, we use N
spherical multi-view images I with their associated extrinsic and extrinsic
sphere
matrices during training as a supervision signal to estimate the scene Sˆ. The
proposed architecture for lifting I consists of an image-to-triplane encoder.
ego
Afteraforwardpassthroughthisarchitecture,thediscretetriplanecontainsthe
approximationofthesceneSˆ.Thefollowingsectionsdescribethedifferentparts
of the pipeline visualized in Fig. 3 and the training objectives.6 T. Gieruc et al.
Fig.3: The overview of 6Img-to-3D.Givensixinputimages,wefirstencodethem
into feature maps using a pre-trained ResNet and an FPN (Sec. 3.1). The scene co-
ordinates are contracted to fit the unbounded scenes (Sec. 3.2). MLPs, cross-and self-
attention layers form the Image-to-Triplane Encoder of our framework (Sec. 3.3). Im-
ages can be rendered from the resulting triplane using our renderer (Sec. 3.4). We
additionally condition the rendering process on projected image features.
3.1 Image Encoder
SixRGBinputimagesI ∈R6×3×H×W areprocessedviaapre-trainedResNet
ego
[26] followed by a Feature Pyramid Network (FPN) [41], resulting in multi-scale
pixel-aligned image features F ∈R6×3×H/i×W/i, with i={8,16,32,64}.
I
3.2 Triplane Representation
The triplane consists of three pairwise orthogonal feature grids T , T and
HW HZ
T , each representing parts of the decomposed 3D space, a setup similar to
WZ
EG3D[11].Wechoosetriplanesbecausetheyallowgradient-basedoptimization,
are explicit representations and their parameterization is thus straightforward.
Additionally, they are lightweight and provide an easily queriable information
encoding [11,21]. The triplane T is composed of the three coordinate-centered
and axis-aligned planes T , T and T , respectively of size 200×200×
HW HZ WZ
F , 200 × 16 × F and 200 × 16 × F , where F is the number of feature
T T T T
channels of the triplane. Due to the concentration of relevant information near
thegroundplanein3Doutdoordrivingscenes,wedecidetoallocatemorespace
to the horizontal dimensions (HW) than to the vertical dimension (Z). The
grid coordinates x for each plane are normalized between -1 and 1. The
grid
contraction equation from world coordinates x to grid coordinates x is
w grid
given as element-wise operations by
x =x ·s
Ws W
(cid:40)
x =
xW 2s ∥x Ws∥≤1 (1)
grid (2− ∥xW1 s∥) 2∥x xW Ws
s∥
∥x Ws∥>1
In short, a scaling parameter s = [s ,s ,s ] is applied before contraction to
h w z
adapt to the nature of our scenes, and the scaled world coordinates x are
Ws
contractedusingthespatialdistortionintroducedinMip-NeRF360[4]toobtain
x , with ||·|| denoting the L2 norm.
grid6Img-to-3D 7
3.3 Image-to-Triplane Encoder
The Image-to-Triplane Encoder, adopted from TPVFormer [28], comprises de-
formable custom cross- and self-attention layers, both using residual connec-
tions. The first half of the encoder consists of three consecutive blocks of cross-
attention, self-attention, and MLPs, and the second half consists of two blocks
that only have self-attention layers and MLPs. The next sections will introduce
each component in more detail. Intermediate batch normalization (BN) layers
are applied within both blocks. The number of blocks is empirically motivated.
Wehypothesizethatincreasingthenumberofblockscouldfurtherimproveper-
formance;thecurrentdesignreflectsabalancechosenconsideringourcomputing
constraints. LRM [27], for example, uses 16 blocks in total.
Cross-Attention (Image-Triplane) We use cross-attention to incorporate as
much information as possible from input image features into the triplane grids.
Thisisfacilitatedbydeformableattentionmechanisms(DeformAttn)[112].Un-
like traditional attention mechanisms, this layer is designed to handle the high
dimensionality of both the image features and the triplanar grids by restricting
the attention of each query to a small set of keys, rather than to all available
keys as in traditional attention. This layer is applied to each plane separately.
Establishing the link between each query (triplane feature) and its keys (image
features)isnon-trivial.Wecomputethedeformableattentionthefollowingway:
CrossAttn(T )=DeformAttn(T ,pF,F ),k ∈{HW,HZ,WZ}
k k k I
whereby the grid features T are used as queries, the image features F as keys
k I
and the reference points pF links a selected number of keys to each query.
k
We establish the correspondence via reference points pF, computed as fol-
k
lows:wefirstestablishacorrespondencefromcameraindices(u,v) toworld
cami
coordinates x by sampling points along each input camera cam ray, as il-
W i
lustrated in Fig. 4a. The world coordinates x are then transformed into grid
W
coordinates x , employing Eq. (1), as shown in Fig. 4b. Given the obtained
grid
correspondences between camera indices (u,v) and 3D grid coordinates, for
cami
eachplaneT weconsideronlyasubsetofn fixedslicesofthecorrespondences
k k
to obtain the reference points pk, illustrated in Fig. 4c. For any (i,j) index in
F
theplaneT ,wenowhaveacorrespondenceton imagefeatureindexesviathe
k k
reference points pF. n is proportional to the dimension perpendicular to the
k k
plane T (4 for T , 32 for T and T ).
k HW HZ WZ
Self-Attention (Triplane) This layer is the same as the one within TPV-
Former. It allows the three planes to exchange information between and within
themselves. The self-attention layer also makes use of the deformable attention
described above:
SelfAttn(T )=DeformAttn(T ,pT,T),k ∈{HW,HZ,WZ}.
k k k
whereby T is the plane used as query, the triplane T is used as keys and the
k
query to keys correspondence is defined by the reference points pT. For any
k8 T. Gieruc et al.
Fig.4:EstablishingReferencePoints.a)Pointsareprojectedintotheworldspace
from the cameras. b) Those points are mapped into the grid space. c) For each plane
element, n points are selected to be used as reference points between the grid index
and image feature.
element in a plane, reference points are obtained by randomly sampling in their
neighborhood and in their perpendicular direction.
3.4 Triplane-to-Image Renderer
The renderer R, consisting of a shallow MLP, predicts the color c and density
σ ∈ [0,1] for a sampled 3D point x ∈ R3. As we are dealing with Lamber-
W
tian scenes, the view directions are discarded. The sampling point x is first
W
converted into grid coordinates x using Eq. (1). The triplane features f
grid T
are then obtained by projecting this point via bilinear sampling onto each plane
andapplyingtheHadamardproduct(elementwisemultiplication)onthoseplane
features: fijk =T (xij )·T (xik )·T (xjk ).
T HW grid HZ grid WZ grid
InspiredbyPixelNeRF[104],weextracttheProjectedImageFeaturesf
PIF
from the pixel-aligned input image features F by projecting x back into the
I W
image space of the input cameras: (u,v) = K M−1 x ,i ∈ {1,...,6}.
cami i i W
Since each point in space can be seen by a maximum of two cameras, the aggre-
gation function concatenates the valid features, with zero-padding when one or
no camera sees the point: f =aggregation(F (u,v) for i∈{1,...,6}).
PIF Ii cami
As the triplane features f and the projected image features f come
T PIF
fromtwodifferentdistributions,wefirstprocessf throughbatchnormaliza-
PIF
tionbeforegoingthroughtherenderer:c,σ =R(f ,BatchNorm(f )).Given
T PIF
the volume density σ and the color c , we follow the same volume rendering
i i
equations as NeRF [55], originating from [52]. Initially, multiple coarse points
are sampled uniformly along each ray r. Those coarse points are then fed into
the renderer to obtain densities, which are used for finer proportional sampling.
The renderer then accumulates contributions from these points along the ray,
which can be expressed as
N
Cˆ(r)=(cid:88)
T (1−exp(−σ δ ))c , (2)
i i i i
i=1
(cid:16) (cid:17)
where r represents a ray with origin o and direction d, T =exp
−(cid:80)i−1σ
δ
i j=1 j j
is the ray transmission upto sample i, 1−exp(−σ δ ) is the absorption from
i i6Img-to-3D 9
sample i, and δ =t −t is the distance between adjacent samples. Hereby t
i i+1 i i
is the accumulated distance from the ray origin to the start of the ith segment,
andCˆ(r)istheaccumulatedRGBcolor,resultinginapixelvaluecorresponding
to the origin o of the ray r. We can compute each pixel value using the volume
rendering equation to obtain a novel view I .
i
3.5 Training Objectives
Training is conducted using MSE and VGG-based L losses on randomly
LPIPS
sampled views, as well as the TV loss introduced by K-Planes on the triplanes
T [21] and a distortion loss from Mip-NeRF 360 [4]. This yields the objective
L=L (C(r),Cˆ(r))+λ L (T)+λ L (s,w)+λ L (x,xˆ). (3)
color t TV d dist l LPIPS
ThecolorlossisasimpleMSElosscomputedperrayoverbatchesofsizeN.
It can be written as
N
L (C(r),Cˆ(r))= 1 (cid:88) (C(r ),Cˆ(r ))2 . (4)
color N i i
i=1
Similar to [13,21,101], the total variation in space regularization is applied
to smoothen the triplanes. This is done via the loss
1 (cid:88)(cid:16) (cid:17)
L (T)= ∥Ti,j −Ti−1,j∥2+∥Ti,j −Ti,j−1∥2 (5)
TV 3n2 k k 2 k k 2
k,i,j
for T in {T ,T ,T }, where i,j are the grid indices of the plane. To
k HW HZ WZ
reduce floating artifacts and enforce volume rendering weights to be compact,
we regularize the scene using MipNeRF360’s [4] distortion loss, given by
(cid:12) (cid:12)
L dist(s,w)=(cid:88) w iw j(cid:12) (cid:12) (cid:12)s i+ 2s i+1 − s j + 2s j+q(cid:12) (cid:12) (cid:12)+ 1 3(cid:88) w i2(s i+1−s i), (6)
i,j i
wheresisasetofraydistancesandw arethevolumerenderingweightsparam-
eterizing each ray, computed following [4].
4 Experiments
4.1 Dataset
Sincedatasetswithbothegoandnon-egovehicleviewsareunavailable,weintro-
duceanoveldatasetforfew-viewimagereconstructioninanautonomousdriving
setting. Concerning 3D reconstruction, other approaches also rely on synthetic
data [30]. Our data is produced using the open-source CARLA Simulator [19].
Our dataset contains 2000 single-timestep complex outdoor driving scenes, each
offering six outward-facing vehicle images and 100 spherical images for super-
vision. We also produce five multi-timestep scenes, each with 200 consecutive10 T. Gieruc et al.
drivingstepscontainingsixoutward-facingvehiclecameras,onebird’s-eyeview,
and one camera following the vehicle from behind. The multi-timestep data is
only used for visualization. This results in 220K individual RGB images, each
with their associated intrinsic camera matrix and pose. The dataset represents
various driving scenes, vehicle types, pedestrians, and lighting conditions. To
show the generalizability of our method, we use CARLA [19] Town 1, Town 2
to 7, and 10 for generating training data, resulting in 1900 training scenes, and
leftthe100scenesfromTown2forvalidation.Weplantomakethedatasetand
the data generator publicly available in a separate future contribution.
4.2 Implementation Details
Model Inputs. The six input images have a resolution of 1600×928 each, and
the supervision images are downscaled to 64×48 pixels.
Architecture.OurtriplaneshavefeaturechannelsF ofsize128.Therenderer
T
Risimplementedasafive-layerMLPwith128neuronsperlayer,takingasinput
128 triplane features f and 2×128 projected image features f .
T PIF
Training. All models were trained on a single Nvidia A40 GPU with 42GB
of VRAM for 100 epochs, resulting in a training time of five days, with an
Adam optimizer [34], a learning rate of 5e-5, and a cosine scheduler [50] with
1000 warmup steps. One epoch consists of 1900 steps, each comprising a new
sceneandthreerandomlysampledviewsassupervision,scaledto64×48pixels.
Using more than three supervising views per scene leads to diminishing returns;
using only three views thus reduces the total training time, a fact also observed
by others [27]. Ray-based sampling allows our model trained on low-resolution
images to render high-resolution outputs during inference. We sample 64 coarse
followed by 64 fine points during training.
Inference. A forward pass to obtain the parameterized triplane is completed
in 395 ms. Rendering an image of size 400×300 from this triplane, using 128
uniformly-spaced points along each ray, takes 520 ms without the Projected
Image Features (PIFs) vs 1461 ms when using PIFs. To improve the quality,
a second pass based on the density of the first uniform sampling can be done
by resampling 128 points in denser regions. This brings the rendering speed
to 955ms without PIFs and 2853 ms with PIFs. Adding or removing the PIFs
and sampling in a single or double fashion thus offers a trade-off between visual
fidelity and speed.
4.3 Quantitative Results
We evaluate our method (6Img-to-3D) against several recent baselines such as
NeRF[55],K-Planes[21]andtherelatedfew-shotmethodPixelNeRF[103]which
Neo360 [30] also evaluates against.
Though Neo360’s approach only targets inward-facing camera setups with
largely overlapping cameras, it would be interesting to see how well Neo360 can
handleourmoredifficultuse-caseofoutward-facingcameraswithminimalover-
lap and compare against it. However, it requires a considerable computational6Img-to-3D 11
Table 1: Performance comparison.
Methods PSNR ↑SSIM↑LPIPS↓
TriPlane Overfitting 22.902 0.792 0.450
NeRF [55] 8.473 0.498 0.820
K-Planes [21] 9.307 0.437 0.773
PixelNeRF [104] 15.263 0.695 0.673
6Img-to-3D (Ours) 18.864 0.733 0.453
budget for training, i.e., 8 A100 GPUs for one day, which was not available in
theworkforthispaper.Sincethispaperfocusesonmethodsthatcanbetrained
efficiently on a single GPU and [30] does not allow straightforwardly to down
their model, we could not compare against it. Furthermore, Neo360’s approach
targets an inward-facing camera setup with widely overlapping camera views as
input, whereas we only consider ego-vehicle views with outward-facing cameras.
Their NERDS360 dataset [30] does not provide such a setup. Performance is
measured using the established metrics PSNR, LPIPS [110], and MSE on the
validation set. The results can be found in Tab. 1 and are visualized in Fig. 5.
We evaluated NeRF and K-Planes by training them on a subset of the eval-
uation set. The overfitted triplane was trained per evaluation scene on 80% of
thesupervisingsphericalimagestodeterminetheobtainableupperboundwhen
using the triplane representation. PixelNeRF was trained and evaluated under
conditions equivalent to our method. Due to the negligible camera view over-
lap, NeRF and K-Planes cannot reconstruct the scene, emphasizing the task’s
difficulty.
Fig.5: Visualization of the Comparison. PixelNeRF visibly struggles to recon-
structdetailsandcannotresembletextureproperly.Ourmethodrepresentsshapeand
appearance faithfully.12 T. Gieruc et al.
4.4 Ablation Study
Table 2: Ablation Results.
Method PSNR ↑ SSIM ↑ LPIPS ↓
Sampling Single DoubleSingleDoubleSingleDouble
w/o Scene Contraction 17.312 17.493 0.706 0.726 0.492 0.479
w/o L 18.835 18.953 0.723 0.736 0.555 0.538
LPIPS
w/o Proj. Image Features18.396 18.440 0.719 0.726 0.484 0.488
w SwinFIR Upscaler* - 19.188 - 0.746 - 0.444
6Img-to-3D (Ours) 18.762 18.864 0.719 0.733 0.453 0.453
* Not included in the full model since it visually impairs image quality.
We show the effectiveness of our model elements in Tab. 2 and visualize our
ablations in Fig. 6. All model variants are trained for 100 epochs and evaluated
ontheholdoutvalidationsetconsistingof100scenesfromtowntwo.Wefindthat
the scene contraction combined with the contracted cross-attention mechanism
substantiallyimprovesthemodelperformance.ThePIFsandLPIPShelpadjust
the color values further. While LPIPS leads to more human-appealing images,
thiscomesatthecostofreducedPSNRperformance.Doubleandsinglesampling
here refers to rendering the image with or without a more fine-grained second
sampling step.
Additionally, an off-the-shelf upscaler, SwinFIR [107], is trained and tested
ontheoutputofour6Img-to-3Dmodel.Whileslightlyimprovingonallmetrics,
avisualanalysisshowsthattheupscaleronlyoutputsasmoothedimageinstead
of adding otherwise missing details.
Fig.6:Visualizationoftheablation.Thefinalmodelparticularlybenefitsfromthe
LPIPS loss, the scene contraction, and the PIFs. When not using Scene Contraction,
theout-of-boundobjectsarenotrendered.WithoutthePIFs,themodelstruggleswith
finedetails.RemovingtheLPIPSlossleadstoincreasedsmearingartifactspotentially
caused by the projection of the PIFs along the camera rays.6Img-to-3D 13
4.5 Qualitative Results
Depth Values.Wevisualizetheexpectedterminationdepthofeachray,result-
inginthedepthmapsshowninFig.7.Whilenevertrainedondepthinformation
explicitly, 6Img-to-3D outputs reasonable depth maps.
Fig.7: Depth Visualization.6Img-to-3Dinfersafull3Drepresentationofthescene
and can thus also be queried for density only, resulting in reasonable depth maps.
Consistency Over Timesteps. Our model works deterministically and thus
obtains identical outputs for identical inputs. 6Img-to-3D also produces tem-
porally consistent images across multiple consecutive timesteps without being
explicitly trained to do so. A small change in the pose of the six input cam-
eras results in a small change in the resulting output representation, visualized
within the short trajectory in Fig. 8. When fed with six images per timestep,
our model can also visualize longer scenes for example from a bird’s-eye view or
a third-person perspective, see Fig. 8 long trajectory.
Fig.8: Trajectories. Consecutive timesteps are visualized with images from a BEV
camera and behind and above the ego vehicle.14 T. Gieruc et al.
4.6 Effects of Scaling
Since our framework is self-supervised, we investigated whether our pipeline
would benefit from a larger dataset. As shown in Fig. 9, the performance scales
with the number of scenes.
Fig.9: Effect of number of training scenes on performance.
5 Discussion
Conclusion. In this paper, we present 6Img-to-3D, an efficient and scalable
methodforreconstructing3Drepresentationsofunboundedoutdoorscenesfrom
a few images. Our evaluation demonstrates that 6Img-to-3D can faithfully re-
construct third-person vehicle perspectives and birds-eye views using only six
images with minimal overlap and without depth information. We achieve this
by leveraging a triplanar representation of space, deformable attention mech-
anisms, feature projection and scene contraction. Our pipeline benefits from
utilizing the features from the input images at multiple pipeline stages. A novel
cross-attentionsamplingmechanismforcontractedplanesessentiallydrivesper-
formance gains. Our model could be further improved by additional training
and using more data. We conclude this from the absence of overfitting during
training and from how the model scales with data. An increased model size al-
lows for a higher triplane resolution, which might help alleviate blurry texture
for fine-grained details. Our model has around 1 the amount of parameters of
7
LRM [27], so visual fidelity could certainly be improved by scaling the model.
We demonstrate the potential of our approach, particularly for embedded de-
vices within vehicles, given its computational requirements. Seamless 2D-to-3D
reconstructioncouldenhancedriverassistanceandautonomousdrivingsystems,
ultimately increasing road safety and navigation capabilities.
Future Directions. We see potential to improve the performance of our
model.Initialexperimentswithanout-of-the-boxupsamplertrainedonthedata
for post-rendering upscaling improved PSNR, SSIM, and LPIPS scores but in-
troduced blurring (see Fig. 6). Incorporating the six source image information
into the upscaling process represents a potentially valuable research direction
to improve upscaling performance. The rendered could be extended to incorpo-
rate the viewing direction if incorporating view-dependent effects is required.
This would allow the model to accurately recreate real-world materials, such as6Img-to-3D 15
shiny metals or wet surfaces, with specular or glossy properties. We made some
first attempts to run our trained model zero-shot on the nuScenes dataset [9].
Those zero-shot tests showed promising results that the model can be extended
to real-world data. We assume that real-world driving data could be utilized by
leveraging multi-timestep images for training. Using LiDAR for depth supervi-
sionorimplementingaseparatesegmentationheadwouldbenaturalextensions
of our work.
AcknowledgementsTheresearchleadingtotheseresultsispartiallyfundedby
theGermanFederalMinistryforEconomicAffairsandClimateActionwithinthe
project “NXT GEN AI METHODS". The authors wish to extend their sincere
gratitude to the creators of TPVformer, NeRFStudio, and the KPlanes paper
for generously open-sourcing their code.
References
1. Achlioptas,P.,Diamanti,O.,Mitliagkas,I.,Guibas,L.:Learningrepresentations
and generative models for 3d point clouds (2018), arXiv:1707.02392
2. Adamkiewicz, M., Chen, T., Caccavale, A., Gardner, R., Culbertson, P., Bohg,
J.,Schwager,M.:Vision-onlyrobotnavigationinaneuralradianceworld(2022),
arXiv:2110.00168
3. Anciukevicius,T.,Xu,Z.,Fisher,M.,Henderson,P.,Bilen,H.,Mitra,N.J.,Guer-
rero,P.:RenderDiffusion:ImageDiffusionfor3DReconstruction,Inpaintingand
Generation (Apr 2023), http://arxiv.org/abs/2211.09869, arXiv:2211.09869
4. Barron,J.T.,Mildenhall,B.,Verbin,D.,Srinivasan,P.P.,Hedman,P.:Mip-NeRF
360:UnboundedAnti-AliasedNeuralRadianceFields(Mar2022),http://arxiv.
org/abs/2111.12077, arXiv:2111.12077
5. Bautista,M.A.,Guo,P.,Abnar,S.,Talbott,W.,Toshev,A.,Chen,Z.,Dinh,L.,
Zhai, S., Goh, H., Ulbricht, D., Dehghan, A., Susskind, J.: GAUDI: A Neural
Architect for Immersive 3D Scene Generation (Jul 2022), http://arxiv.org/
abs/2207.13751, arXiv:2207.13751
6. Behley,J.,Garbade,M.,Milioto,A.,Quenzel,J.,Behnke,S.,Stachniss,C.,Gall,
J.: SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Se-
quences (Aug 2019), http://arxiv.org/abs/1904.01416, arXiv:1904.01416
7. Bhattarai, A.R., Nießner, M., Sevastopolsky, A.: TriPlaneNet: An Encoder
for EG3D Inversion (Mar 2023), http://arxiv.org/abs/2303.13497,
arXiv:2303.13497
8. Bogdoll,D.,Yang,Y.,Zöllner,J.M.:Muvo:Amultimodalgenerativeworldmodel
for autonomous driving with geometric representations (2023), arXiv:2311.11762
9. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krish-
nan, A., Pan, Y., Baldan, G., Beijbom, O.: nuScenes: A multimodal dataset
for autonomous driving (May 2020), http://arxiv.org/abs/1903.11027,
arXiv:1903.11027
10. Chai,L.,Tucker,R.,Li,Z.,Isola,P.,Snavely,N.:Persistentnature:Agenerative
model of unbounded 3d worlds (2023), arXiv:2303.13515
11. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo,
O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., Wetzstein, G.: Efficient
Geometry-aware3DGenerativeAdversarialNetworks(Apr2022),http://arxiv.
org/abs/2112.07945, arXiv:2112.0794516 T. Gieruc et al.
12. Charatan, D., Li, S., Tagliasacchi, A., Sitzmann, V.: pixelsplat: 3d gaussian
splats from image pairs for scalable generalizable 3d reconstruction (2023),
arXiv:2312.12337
13. Chen,A.,Xu,Z.,Geiger,A.,Yu,J.,Su,H.:TensoRF:TensorialRadianceFields
(Nov 2022), http://arxiv.org/abs/2203.09517, arXiv:2203.09517
14. Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: Mvsnerf:
Fast generalizable radiance field reconstruction from multi-view stereo (2021),
arXiv:2103.15595
15. Chen, L., Wu, P., Chitta, K., Jaeger, B., Geiger, A., Li, H.: End-to-end au-
tonomous driving: Challenges and frontiers (2023), arXiv:2306.16927
16. Chen,Z.,Wang,G.,Liu,Z.:Scenedreamer:Unbounded3dscenegenerationfrom
2dimagecollections.IEEETransactionsonPatternAnalysisandMachineIntelli-
gence45(12),15562–15576(Dec2023).https://doi.org/10.1109/tpami.2023.
3321857, http://dx.doi.org/10.1109/TPAMI.2023.3321857
17. Chen,Z.,Zhang,H.:Learningimplicitfieldsforgenerativeshapemodeling(2019),
arXiv:1812.02822
18. Deng, C., Jiang, C.M., Qi, C.R., Yan, X., Zhou, Y., Guibas, L., Anguelov, D.:
Nerdi:Single-viewnerfsynthesiswithlanguage-guideddiffusionasgeneralimage
priors (2022), arXiv:2212.03267
19. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: Carla: An open
urban driving simulator (2017), arXiv:1711.03938
20. Fan, H., Su, H., Guibas, L.: A point set generation network for 3d object recon-
struction from a single image (2016), arXiv:1612.00603
21. Fridovich-Keil, S., Meanti, G., Warburg, F., Recht, B., Kanazawa, A.: K-Planes:
Explicit Radiance Fields in Space, Time, and Appearance (Mar 2023), http:
//arxiv.org/abs/2301.10241, arXiv:2301.10241
22. Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Goj-
cic, Z., Fidler, S.: GET3D: A Generative Model of High Quality 3D Textured
Shapes Learned from Images (Sep 2022), http://arxiv.org/abs/2209.11163,
arXiv:2209.11163
23. Gao,K.,Gao,Y.,He,H.,Lu,D.,Xu,L.,Li,J.:Nerf:Neuralradiancefieldin3d
vision, a comprehensive review (2023), arXiv:2210.00379
24. Girdhar,R.,Fouhey,D.F.,Rodriguez,M.,Gupta,A.:Learningapredictableand
generative vector representation for objects (2016), arXiv:1603.08637
25. Gu, J., Trevithick, A., Lin, K.E., Susskind, J., Theobalt, C., Liu, L., Ra-
mamoorthi, R.: NerfDiff: Single-image View Synthesis with NeRF-guided Distil-
lationfrom3D-awareDiffusion(Feb2023),http://arxiv.org/abs/2302.10109,
arXiv:2302.10109
26. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition
(2015), arXiv:1512.03385
27. Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K.,
Bui,T.,Tan,H.:Lrm:Largereconstructionmodelforsingleimageto3d(2023),
arXiv:2311.04400
28. Huang,Y.,Zheng,W.,Zhang,Y.,Zhou,J.,Lu,J.:Tri-perspectiveviewforvision-
based 3d semantic occupancy prediction (2023), arXiv:2302.07817
29. Huang, Y., Zheng, W., Zhang, Y., Zhou, J., Lu, J.: Tri-Perspective View for
Vision-Based 3D Semantic Occupancy Prediction (Mar 2023), http://arxiv.
org/abs/2302.07817, arXiv:2302.07817
30. Irshad, M.Z., Zakharov, S., Liu, K., Guizilini, V., Kollar, T., Gaidon, A., Kira,
Z., Ambrus, R.: NeO 360: Neural Fields for Sparse View Synthesis of Outdoor
Scenes (Aug 2023), http://arxiv.org/abs/2308.12967, arXiv:2308.129676Img-to-3D 17
31. Johari,M.M.,Lepoittevin,Y.,Fleuret,F.:Geonerf:Generalizingnerfwithgeom-
etry priors (2022), arXiv:2111.13539
32. Kanazawa,A.,Tulsiani,S.,Efros,A.A.,Malik,J.:Learningcategory-specificmesh
reconstruction from image collections (2018), arXiv:1803.07549
33. Kim, S.W., Brown, B., Yin, K., Kreis, K., Schwarz, K., Li, D., Rombach, R.,
Torralba,A.,Fidler,S.:Neuralfield-ldm:Scenegenerationwithhierarchicallatent
diffusion models (2023), arXiv:2304.09787
34. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017),
arXiv:1412.6980
35. Koh,J.Y.,Lee,H.,Yang,Y.,Baldridge,J.,Anderson,P.:Pathdreamer:Aworld
model for indoor navigation (2021), arXiv:2105.08756
36. Li,Z.,Wang,Q.,Snavely,N.,Kanazawa,A.:Infinitenature-zero:Learningperpet-
ualviewgenerationofnaturalscenesfromsingleimages(2022),arXiv:2207.11148
37. Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Yu, Q., Dai, J.: BEV-
Former:LearningBird’s-Eye-ViewRepresentationfromMulti-CameraImagesvia
Spatiotemporal Transformers (Jul 2022), http://arxiv.org/abs/2203.17270,
arXiv:2203.17270
38. Liang,R.,Zhang,J.,Li,H.,Yang,C.,Guan,Y.,Vijaykumar,N.:Spidr:Sdf-based
neural point fields for illumination and deformation (2023), arXiv:2210.08398
39. Lin, C.H., Lee, H.Y., Menapace, W., Chai, M., Siarohin, A., Yang, M.H.,
Tulyakov, S.: Infinicity: Infinite-scale city synthesis (2023), arXiv:2301.09637
40. Lin, K.E., Yen-Chen, L., Lai, W.S., Lin, T.Y., Shih, Y.C., Ramamoorthi, R.:
Vision Transformer for NeRF-Based View Synthesis from a Single Input Image
(Oct 2022), http://arxiv.org/abs/2207.05736, arXiv:2207.05736
41. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection (2017), arXiv:1612.03144
42. Lin, Y., Han, H., Gong, C., Xu, Z., Zhang, Y., Li, X.: Consistent123: One
image to highly consistent 3d asset using case-aware diffusion priors (2023),
arXiv:2309.17261
43. Liu,A.,Tucker,R.,Jampani,V.,Makadia,A.,Snavely,N.,Kanazawa,A.:Infinite
nature: Perpetual view generation of natural scenes from a single image (2021),
arXiv:2012.09855
44. Liu, L., Gu, J., Lin, K.Z., Chua, T.S., Theobalt, C.: Neural sparse voxel fields
(2021), arXiv:2007.11571
45. Liu, M., Xu, C., Jin, H., Chen, L., T, M.V., Xu, Z., Su, H.: One-2-3-45: Any
single image to 3d mesh in 45 seconds without per-shape optimization (2023),
arXiv:2306.16928
46. Liu, R., Wu, R., Hoorick, B.V., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-
1-to-3: Zero-shot one image to 3d object (2023), arXiv:2303.11328
47. Liu,Y.,Peng,S.,Liu,L.,Wang,Q.,Wang,P.,Theobalt,C.,Zhou,X.,Wang,W.:
Neural rays for occlusion-aware image-based rendering (2022), arXiv:2107.13421
48. Liu, Z., Feng, Y., Black, M.J., Nowrouzezahrai, D., Paull, L., Liu, W.: Meshdif-
fusion: Score-based generative 3d mesh modeling (2023), arXiv:2303.08133
49. Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., Sheikh, Y.:
Neuralvolumes:learningdynamicrenderablevolumesfromimages.ACMTrans-
actionsonGraphics38(4),1–14(Jul2019).https://doi.org/10.1145/3306346.
3323020, http://dx.doi.org/10.1145/3306346.3323020
50. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts
(2017), arXiv:1608.03983
51. Maggio, D., Abate, M., Shi, J., Mario, C., Carlone, L.: Loc-nerf: Monte carlo
localization using neural radiance fields (2022), arXiv:2209.0905018 T. Gieruc et al.
52. Max, N.: Optical models for direct volume rendering. IEEE Transactions on Vi-
sualization and Computer Graphics 1(2), 99–108 (jun 1995). https://doi.org/
10.1109/2945.468400, https://doi.org/10.1109/2945.468400
53. Melas-Kyriazi, L., Rupprecht, C., Laina, I., Vedaldi, A.: Realfusion: 360deg re-
construction of any object from a single image (2023), arXiv:2302.10663
54. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R.,
Ng, R.: Synthetic nerf dataset, https://drive.google.com/drive/folders/
128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1
55. Mildenhall,B.,Srinivasan,P.P.,Tancik,M.,Barron,J.T.,Ramamoorthi,R.,Ng,
R.: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
(Aug 2020), http://arxiv.org/abs/2003.08934, arXiv:2003.08934
56. Müller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives
with a multiresolution hash encoding. ACM Transactions on Graphics 41(4), 1–
15 (Jul 2022). https://doi.org/10.1145/3528223.3530127, https://dl.acm.
org/doi/10.1145/3528223.3530127
57. Nash,C.,Ganin,Y.,Eslami,S.M.A.,Battaglia,P.W.:Polygen:Anautoregressive
generative model of 3d meshes (2020), arXiv:2002.10880
58. Niemeyer, M., Barron, J.T., Mildenhall, B., Sajjadi, M.S.M., Geiger, A., Rad-
wan,N.:RegNeRF:RegularizingNeuralRadianceFieldsforViewSynthesisfrom
SparseInputs(Dec2021),http://arxiv.org/abs/2112.00724,arXiv:2112.00724
59. Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: Differentiable volumetric
rendering: Learning implicit 3d representations without 3d supervision (2020),
arXiv:1912.07372
60. Ortiz, J., Clegg, A., Dong, J., Sucar, E., Novotny, D., Zollhoefer, M., Mukadam,
M.: isdf: Real-time neural signed distance fields for robot perception (2022),
arXiv:2204.02296
61. Ost, J., Mannan, F., Thuerey, N., Knodt, J., Heide, F.: Neural scene graphs for
dynamic scenes (2021), arXiv:2011.10379
62. Po, R., Yifan, W., Golyanik, V., Aberman, K., Barron, J.T., Bermano, A.H.,
Chan,E.R.,Dekel,T.,Holynski,A.,Kanazawa,A.,Liu,C.K.,Liu,L.,Mildenhall,
B.,Nießner,M.,Ommer,B.,Theobalt,C.,Wonka,P.,Wetzstein,G.:Stateofthe
art on diffusion models for visual computing (2023), arXiv:2310.07204
63. Raj, A., Kaza, S., Poole, B., Niemeyer, M., Ruiz, N., Mildenhall, B., Zada, S.,
Aberman, K., Rubinstein, M., Barron, J., Li, Y., Jampani, V.: Dreambooth3d:
Subject-driven text-to-3d generation (2023), arXiv:2303.13508
64. Rebain, D., Jiang, W., Yazdani, S., Li, K., Yi, K.M., Tagliasacchi, A.: DeRF:
Decomposed Radiance Fields (Nov 2020), http://arxiv.org/abs/2011.12490,
arXiv:2011.12490
65. Reiher,L.,Lampe,B.,Eckstein,L.:ASim2RealDeepLearningApproachforthe
TransformationofImagesfromMultipleVehicle-MountedCamerastoaSemanti-
callySegmentedImageinBird’sEyeView(May2020),http://arxiv.org/abs/
2005.04078, arXiv:2005.04078
66. Reiser,C.,Peng,S.,Liao,Y.,Geiger,A.:KiloNeRF:SpeedingupNeuralRadiance
FieldswithThousandsofTinyMLPs(Aug2021),http://arxiv.org/abs/2103.
13744, arXiv:2103.13744
67. Ren, Y., Wang, F., Zhang, T., Pollefeys, M., Süsstrunk, S.: VolRecon: Volume
Rendering of Signed Ray Distance Functions for Generalizable Multi-View Re-
construction (Apr 2023), http://arxiv.org/abs/2212.08067, arXiv:2212.08067
68. Seitz,S.M.,Dyer,C.R.:Photorealisticscenereconstructionbyvoxelcoloring.In:
Proceedingsofthe1997ConferenceonComputerVisionandPatternRecognition
(CVPR ’97). p. 1067. CVPR ’97, IEEE Computer Society, USA (1997)6Img-to-3D 19
69. Seo,J.,Jang,W.,Kwak,M.S.,Ko,J.,Kim,H.,Kim,J.,Kim,J.H.,Lee,J.,Kim,
S.: Let 2d diffusion model know 3d-consistency for robust text-to-3d generation
(2023), arXiv:2303.07937
70. Sharma,P.,Tewari,A.,Du,Y.,Zakharov,S.,Ambrus,R.,Gaidon,A.,Freeman,
W.T., Durand, F., Tenenbaum, J.B., Sitzmann, V.: Neural groundplans: Persis-
tent neural scene representations from a single image (2023), arXiv:2207.11232
71. Shue, J.R., Chan, E.R., Po, R., Ankner, Z., Wu, J., Wetzstein, G.: 3D Neural
Field Generation using Triplane Diffusion (Nov 2022), http://arxiv.org/abs/
2211.16677, arXiv:2211.16677
72. Sima, C., Tong, W., Wang, T., Chen, L., Wu, S., Deng, H., Gu, Y., Lu, L., Luo,
P., Lin, D., Li, H.: Scene as occupancy (2023), arXiv:2306.02851
73. Sun, C., Sun, M., Chen, H.T.: Direct Voxel Grid Optimization: Super-fast Con-
vergenceforRadianceFieldsReconstruction.In:2022IEEE/CVFConferenceon
Computer Vision and Pattern Recognition (CVPR). pp. 5449–5459. IEEE, New
Orleans, LA, USA (Jun 2022). https://doi.org/10.1109/CVPR52688.2022.
00538, https://ieeexplore.ieee.org/document/9879963/
74. Sun,P.,Kretzschmar,H.,Dotiwalla,X.,Chouard,A.,Patnaik,V.,Tsui,P.,Guo,
J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H.,
Timofeev,A.,Ettinger,S.,Krivokon,M.,Gao,A.,Joshi,A.,Zhang,Y.,Shlens,J.,
Chen,Z.,Anguelov,D.:Scalabilityinperceptionforautonomousdriving:Waymo
opendataset.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition (CVPR) (June 2020)
75. Szeliski, R.: Computer Vision: Algorithms and Applications. Springer-Verlag,
Berlin, Heidelberg, 1st edn. (2010)
76. Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Splatter image: Ultra-fast single-
view 3d reconstruction (2023), arXiv:2312.13150
77. Tagliasacchi, A., Mildenhall, B.: Volume rendering digest (for nerf) (2022),
arXiv:2209.02417
78. Tancik, M., Casser, V., Yan, X., Pradhan, S., Mildenhall, B., Srinivasan, P.P.,
Barron, J.T., Kretzschmar, H.: Block-NeRF: Scalable Large Scene Neural View
Synthesis (Feb 2022), http://arxiv.org/abs/2202.05263, arXiv:2202.05263
79. Tewari,A.,Thies,J.,Mildenhall,B.,Srinivasan,P.,Tretschk,E.,Yifan,W.,Lass-
ner,C.,Sitzmann,V.,Martin-Brualla,R.,Lombardi,S.,Simon,T.,Theobalt,C.,
Nießner, M., Barron, J.T., Wetzstein, G., Zollhöfer, M., Golyanik, V.: Advances
in Neural Rendering. Computer Graphics Forum (EG STAR 2022) (2022)
80. Thrun,S.,Burgard,W.,Fox,D.:ProbabilisticRobotics(IntelligentRoboticsand
Autonomous Agents). The MIT Press (2005)
81. Tian, X., Jiang, T., Yun, L., Mao, Y., Yang, H., Wang, Y., Wang, Y., Zhao, H.:
Occ3d:Alarge-scale3doccupancypredictionbenchmarkforautonomousdriving
(2023), arXiv:2304.14365
82. Trevithick, A., Yang, B.: Grf: Learning a general radiance field for 3d represen-
tation and rendering (2021), arXiv:2010.04595
83. Wang,N.,Zhang,Y.,Li,Z.,Fu,Y.,Liu,W.,Jiang,Y.G.:Pixel2mesh:Generating
3d mesh models from single rgb images (2018), arXiv:1804.01654
84. Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learn-
ing neural implicit surfaces by volume rendering for multi-view reconstruction.
NeurIPS (2021)
85. Wang, Q., Wang, Z., Genova, K., Srinivasan, P., Zhou, H., Barron, J.T.,
Martin-Brualla, R., Snavely, N., Funkhouser, T.: IBRNet: Learning Multi-
View Image-Based Rendering (Apr 2021), http://arxiv.org/abs/2102.13090,
arXiv:2102.1309020 T. Gieruc et al.
86. Watson,D.,Chan,W.,Martin-Brualla,R.,Ho,J.,Tagliasacchi,A.,Norouzi,M.:
NovelViewSynthesiswithDiffusionModels(Oct2022),http://arxiv.org/abs/
2210.04628, arXiv:2210.04628
87. Wei,Y.,Zhao,L.,Zheng,W.,Zhu,Z.,Zhou,J.,Lu,J.:Surroundocc:Multi-camera
3d occupancy prediction for autonomous driving (2023), arXiv:2303.09551
88. Wu,J.,Wang,Y.,Xue,T.,Sun,X.,Freeman,W.T.,Tenenbaum,J.B.:Marrnet:
3d shape reconstruction via 2.5d sketches (2017), arXiv:1711.03129
89. Wu,P.,Escontrela,A.,Hafner,D.,Goldberg,K.,Abbeel,P.:Daydreamer:World
models for physical robot learning (2022), arXiv:2206.14176
90. Xie,H.,Chen,Z.,Hong,F.,Liu,Z.:Citydreamer:Compositionalgenerativemodel
of unbounded 3d cities (2023), arXiv:2309.00610
91. Xie, H., Yao, H., Zhang, S., Zhou, S., Sun, W.: Pix2vox++: Multi-scale context-
aware 3d object reconstruction from single and multiple images. International
Journal of Computer Vision 128(12), 2919–2935 (Jul 2020). https://doi.org/
10.1007/s11263-020-01347-6, http://dx.doi.org/10.1007/s11263-020-
01347-6
92. Xie, Y., Takikawa, T., Saito, S., Litany, O., Yan, S., Khan, N., Tombari, F.,
Tompkin, J., Sitzmann, V., Sridhar, S.: Neural fields in visual computing and
beyond (2022), arXiv:2111.11426
93. Xu,D.,Jiang,Y.,Wang,P.,Fan,Z.,Wang,Y.,Wang,Z.:Neurallift-360:Lifting
anin-the-wild2dphototoa3dobjectwith360degviews(2023),arXiv:2211.16431
94. Xu, Q., Wang, W., Ceylan, D., Mech, R., Neumann, U.: Disn: Deep im-
plicit surface network for high-quality single-view 3d reconstruction (2021),
arXiv:1905.10711
95. Xu, Y., Chai, M., Shi, Z., Peng, S., Skorokhodov, I., Siarohin, A., Yang, C.,
Shen, Y., Lee, H.Y., Zhou, B., Tulyakov, S.: Discoscene: Spatially disentan-
gled generative radiance fields for controllable 3d-aware scene synthesis (2022),
arXiv:2212.11984
96. Xu, Y., Tan, H., Luan, F., Bi, S., Wang, P., Li, J., Shi, Z., Sunkavalli, K., Wet-
zstein, G., Xu, Z., Zhang, K.: Dmv3d: Denoising multi-view diffusion using 3d
large reconstruction model (2023), arXiv:2311.09217
97. Yagubbayli, F., Wang, Y., Tonioni, A., Tombari, F.: Legoformer: Transformers
for block-by-block multi-view 3d reconstruction (2022), arXiv:2106.12102
98. Yang, J., Pavone, M., Wang, Y.: Freenerf: Improving few-shot neural rendering
with free frequency regularization (2023), arXiv:2303.07418
99. Yang,W.,Li,Q.,Liu,W.,Yu,Y.,Ma,Y.,He,S.,Pan,J.:ProjectingYourView
Attentively: Monocular Road Scene Layout Estimation via Cross-view Transfor-
mation.In:2021IEEE/CVFConferenceonComputerVisionandPatternRecog-
nition(CVPR).pp.15531–15540.IEEE,Nashville,TN,USA(Jun2021).https:
//doi.org/10.1109/CVPR46437.2021.01528, https://ieeexplore.ieee.org/
document/9578824/
100. Yang,Y., Feng,C., Shen, Y.,Tian,D.: Foldingnet: Point cloudauto-encoder via
deep grid deformation (2018), arXiv:1712.07262
101. Yu,A.,Fridovich-Keil,S.,Tancik,M.,Chen,Q.,Recht,B.,Kanazawa,A.:Plenox-
els: Radiance Fields without Neural Networks (Dec 2021), http://arxiv.org/
abs/2112.05131, arXiv:2112.05131
102. Yu, A., Li, R., Tancik, M., Li, H., Ng, R., Kanazawa, A.: PlenOctrees for Real-
time Rendering of Neural Radiance Fields. In: 2021 IEEE/CVF International
Conference on Computer Vision (ICCV). pp. 5732–5741. IEEE, Montreal, QC,
Canada (Oct 2021). https://doi.org/10.1109/ICCV48922.2021.00570, https:
//ieeexplore.ieee.org/document/9711398/6Img-to-3D 21
103. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from
one or few images (2021), arXiv:2012.02190
104. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelNeRF: Neural Radiance Fields
from One or Few Images (May 2021), http://arxiv.org/abs/2012.02190,
arXiv:2012.02190
105. Yu, X., Rao, Y., Wang, Z., Liu, Z., Lu, J., Zhou, J.: Pointr: Diverse point cloud
completion with geometry-aware transformers (2021), arXiv:2108.08839
106. Zeng, X., Vahdat, A., Williams, F., Gojcic, Z., Litany, O., Fidler, S., Kreis,
K.: Lion: Latent point diffusion models for 3d shape generation (2022),
arXiv:2210.06978
107. Zhang, D., Huang, F., Liu, S., Wang, X., Jin, Z.: Swinfir: Revisiting the swinir
with fast fourier convolution and improved training for image super-resolution
(2023), arXiv:2208.11247
108. Zhang, K., Riegler, G., Snavely, N., Koltun, V.: NeRF++: Analyzing and Im-
provingNeuralRadianceFields(Oct2020),http://arxiv.org/abs/2010.07492,
arXiv:2010.07492
109. Zhang,K.,Riegler,G.,Snavely,N.,Koltun,V.:Nerf++:Analyzingandimproving
neural radiance fields (2020), arXiv:2010.07492
110. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric (2018), arXiv:1801.03924
111. Zheng,W.,Chen,W.,Huang,Y.,Zhang,B.,Duan,Y.,Lu,J.:Occworld:Learning
a 3d occupancy world model for autonomous driving (2023), arXiv:2311.16038
112. Zhu,X.,Su,W.,Lu,L.,Li,B.,Wang,X.,Dai,J.:DeformableDETR:Deformable
Transformers for End-to-End Object Detection (Mar 2021), http://arxiv.org/
abs/2010.04159, arXiv:2010.0415922 T. Gieruc et al.
6 Appendix
6.1 Dataset Visualization
Table 3: Samples from Our Dataset. Towns differ in buildings (residential and
commercial), types of bridges, amount of rivers, the traffic lights, and the kind of
junctions. All towns contain pedestrians.
Town Input(OutwardFacing) Supervision(SphericalImages)
1nwoT
2nwoT
3nwoT
4nwoT
5nwoT
6nwoT6Img-to-3D 23
6.2 Bounded Scene Paramterization
The transformation from world coordinates x to grid coordinates x can
W grid
also be done when a bounded scene is given.
Bounded scenes:Theworldcoordinatesx aretransformedintogridcoordi-
W
nates x by scaling the scene with the scaling parameter s=[s ,s ,s ] and
grid h w z
applying the offset o = [oo ,o ] according to Eq. (7) for element-wise opera-
, w z
tions.Whendepthmapsareavailable,out-of-boundpixelsofthetrainingimages
can be masked with a white background during an initial testing phase, so the
network learns only to reconstruct the inside of the bounded scene.
x −o
x = W (7)
grid s
6.3 Shape and Parameter Details
We follow the PyTorch convention of Batch × Channel × Height × Width
(B×C×H ×W) in our description of the model. The shape of 6Img-to-3D is
shown in Tab. 4.
Table4:Shapeandparametervaluesof6Img-to-3Dtrainedontheintroduceddataset.
Part Shape Parameters
Input Images 6×3×928×1600 n.a.
ResNet Maps 6×512×116×200 44M
6×1024×58×100
6×2048×29×50
Feature Maps 6×128×116×200 1M
6×128×58×100
6×128×29×50
6×128×15×25
Attention Mechanism n.a. 16M
Triplane 128×200×200 n.a.
128×200×16
128×16×200
MLP decoder input: 384 84K
output: 4
Output Image 3×600×800
6.4 Zero-shot nuScenes Performance visualization
We attempt an early analysis of our 6Img-to-3D applied zero-shot to nuScenes.
Without additional training, the model can display the appearance of nuscenes.
Unfortunately,thecamerasetupwithinourdatasetdoesnotmatchthenuScenes
dataexactly.Camerapositions,FOV,andscenestyle(sunnyvs.cloudyweather)
differ, making zero-shot transfer even more difficult. The promising results mo-
tivate to extent the model to real-world data.24 T. Gieruc et al.
Fig.10: 6Img-to-3D with nuScenes Data.
Fig.11: nuScenes Short Trajectory. Consecutive timesteps inferred during zero-
shot inference on nuScenes.
6.5 Failure Cases
In Fig. 12, we visualize the failure cases of our method. Our model struggles
with zero-shot reconstructing fine details, as seen in examples one and two.
Partially, model failures can be attributed to the dataset itself. Some of the
supervising images in the training set are located inside buildings. Within the
Carlasimulator,wallsaresometimestransparentfromwithinthebuilding,which
leads the model to pick up on this physically impossible artifact. Those failure
cases are shown in examples three and four. Example five shows an instance
where the model is unable to infer the structure of the building beyond what is
actually contained in the input images.
Fig.12: Visualization of Failure Cases.6Img-to-3D 25
6.6 Extended Qualitative Results Comparison
To give a better impression of our model’s performance, we here extend the
visualization from our ablations. The results are not cherry-picked.
Fig.13: Qualitative Results Comparison (I).26 T. Gieruc et al.
Fig.14: Qualitative Results Comparison (II).6Img-to-3D 27
Fig.15: Qualitative Results Comparison (III).