Lazy Diffusion Transformer for Interactive Image Editing
YotamNitzan1,2 ZongzeWu1 RichardZhang1 EliShechtman1
DanielCohen-Or2 TaesungPark1 Michae¨lGharbi1
1 AdobeResearch 2 Tel-AvivUniversity
https://lazydiffusion.github.io
mask: 46% mask: 26% mask: 38% mask: 7% mask: 7%
2.65s 1.50s 2.26s 0.96s 0.89s
(cid:147)blue sky with clouds(cid:148) (cid:147)distant mountain range(cid:148) (cid:147)brown grass field(cid:148) (cid:147)gazelle(cid:148) (cid:147)lion cub(cid:148)
Figure 1. Incremental image generation at 1024×1024 using LazyDiffusion with 20 diffusion steps. The model generates content
accordingtoatextpromptinanareaspecifiedbyamask. Eachupdategeneratesonlythemaskedpixels, witharuntimethatdepends
chieflyonthesizeofthemask,ratherthanthatoftheimage.
Abstract 10%oftheimage.
We introduce a novel diffusion transformer, LazyDiffu- 1.Introduction
sion, that generates partial image updates efficiently. Our
approach targets interactive image editing applications in Diffusionmodelshavehadremarkablesuccessesingener-
which, starting from a blank canvas or an image, a user atinghigh-qualityanddiverseimages. Theyarethepower-
specifiesasequenceoflocalizedimagemodificationsusing fulenginebehindexcitinglocalimageeditingapplications
binarymasksandtextprompts. Ourgeneratoroperatesin based on inpainting, where a user provides a mask and a
two phases. First, a context encoder processes the current text prompt describing a region to modify and the content
canvas and user mask to produce a compact global con- to generate, respectively [31, 42, 55]. While current ap-
texttailoredtotheregiontogenerate. Second,conditioned proaches yield impressive results, they are also slow and
onthiscontext,adiffusion-basedtransformerdecodersyn- wasteful. Invisible to the end user, the inpainting pipeline
thesizes the masked pixels in a “lazy” fashion, i.e., it only generatesanentireimageandthenselectivelyutilizesonly
generates the masked region. This contrasts with previous the few pixels located within the mask, discarding all oth-
works that either regenerate the full canvas, wasting time ers.Althoughthisapproachisgenerallycommonininpaint-
andcomputation,orconfineprocessingtoatightrectangu- ing pipelines [59, 60], its inefficiency is particularly pro-
larcroparoundthemask,ignoringtheglobalimagecontext nounced with diffusion models, due to their iterative sam-
altogether.Ourdecoder’sruntimescaleswiththemasksize, plingprocedure,precludingtheirusageininteractivework-
whichistypicallysmall,whileourencoderintroducesneg- flows. Practitioners[50,54]savetimeandcomputationby
ligibleoverhead.Wedemonstratethatourapproachiscom- cropping a small rectangular region around the mask, pos-
petitive with state-of-the-art inpainting methods in terms siblydownsamplingforprocessingwiththediffusion,then
of quality and fidelity while providing a 10× speedup for upsamplingandblendingtheresulttofillthehole. Indoing
typicaluserinteractions,wheretheeditingmaskrepresents so they compromise image quality and sacrifice the global
4202
rpA
81
]VC.sc[
1v28321.4042:viXraGenerate entire image Generate missing Ours - Generate missing baseline
Input with full context with crop context with full context
decoder
full-image input all tokens interact full generation
c“ aH ne da ielt sh wy ih tho m nue tm
s
a ad ne
d
19.48s 2.93s 1.90s
compressed context
dry fruits”
(a) (b) (c)
Figure2. Comparinginpaintingapproaches. (a)Mostworks[37,
42] generate the entire image, utilizing the full image context decoder
incremental
and fill the hole by discarding the non-masked regions. While
incremental noise only masked tokens interact generation
the outcome aligns well with the image, the process is time- ours
consuming. (b) generating only a lower resolution crop around
Figure 3. Our diffusion transformer decoder (bottom) reduces
themaskismoreefficientandstillseamlesslyblendswithnearby
synthesis computation using two strategies. First, we compress
pixels [50, 54]. However, the inpainted content is semantically
the image context using a separate encoder (not shown) outside
inconsistentwiththeoverallimagecontext. (c)ourapproachen-
thediffusionloop. Second,weonlygeneratetokenscorrespond-
suresbothglobalconsistencyandefficientexecution.
ing to the masked region to generate. In contrast, typical diffu-
siontransformers(top) [7,35]maintaintokensfortheentireim-
agethroughoutthediffusionprocess, topreserveglobalcontext.
image context, which often leads to spatially inconsistent
Whenperforminginpainting,suchmodelgeneratesafull-sizeim-
outputs(CompareFigs.2(a)and2(b)). age,mostofwhichisdiscardedinordertoin-filltheholeregion
Weproposeanewgenerativemodelarchitecture,which only. Existingconvolutionaldiffusionmodelsforinpainting[42]
we call LazyDiffusion. Our approach, illustrated in Fig. 1, sufferfromthesamedrawbacks.
generates partial image updates, strictly limited to the
masked region, and does so efficiently, with a cost com-
mensurate to the mask size. Yet, its output respects the age,throughthecompressedcontext,ensuringstrongcoher-
global context given by the observed canvas (Fig. 2(c)). ence. Theconditioningoncontextisefficientandaddsneg-
To achieve this, our key idea is to decouple the genera- ligiblecomputationaloverhead. Incontrast,previousmeth-
tive process into two distinct steps. First, an encoder pro- ods[1,42,55]achievespatialconsistencybyuniformlypro-
cessesthevisiblecanvasandmask,summarizingtheminto cessing all image regions, masked or not. Figure 3 illus-
a global context code. This encoder processes the entire trates the conceptual difference between our approach and
canvas,butitonlyrunsoncepermask.Second,conditioned abaselinediffusiontransformer.
on the global context and the user’s text prompt, a diffu- Our approach reduces computational cost significantly
siondecodergeneratesthenextpartialcanvasupdate. This for small masks, typical in interactive editing. We achieve
modelrunsmanytimesduringthediffusionprocess,butun- a speedup up to ×10 over methods processing the entire
likepreviousworks,itonlyoperatesonthemaskedregion. image,formaskcovering10%oftheimage. Additionally,
Since,inpractice,mostupdatescoversmallareas(10–20% our model produces results of comparable quality, indicat-
of the image), this yields significant computation savings, ingthatthecompressedcontextisrichandexpressive.Inan
thusmakingtheeditingexperiencemoreinteractive. interactiveimagegenerationcontext,ourmethodamortizes
Our encoder and diffusion decoder operate in a latent the overall synthesis cost over multiple user interactions,
space [42], for efficiency. Both use the transformer archi- improvinginteractionlatency. Italsoamortizestheencoder
tecture [13, 35, 53]. The transformer architecture is par- cost when generating multiple updates for a given mask,
ticularly appealing because splitting the image into small usingdifferentinputnoiseortextprompt(Fig.1,rightmost
enough patches (tokens) enables generating arbitrarily- panel).
shapedregionswithminimalwaste. Theencoderprocesses
theentireimageandmaskandproducesamask-dependent 2.RelatedWork
context. Wekeeponlythecontexttokenscorrespondingto
thelocationofthemaskedpatches. Thisensuresthedown- Speeding up diffusion models. Diffusion models [21, 46,
streamcomputationonlyscaleswiththesizeofthemasked 48] are a significant breakthrough in generative model-
region,andencouragesthecompressedcontexttorepresent ing [2, 11, 40, 42, 43] and editing [1, 29], producing im-
therelationshipofthemaskedregiontotherestoftheim- ages with unparalleled quality and diversity. But they re-
age. Ateachdenoisingstep,thedecoderonlyprocessesto- main costly to evaluate, due to the iterative nature of their
kenscorrespondingtomaskedpatches. Whilethedecoder samplingprocess. Numerousmethodshavebeendeveloped
generates only the masked region, it “sees” the entire im- toimprovetheirinferencetime,suchasbettersamplersanddedicated ODE solvers [22, 26, 27, 47], distillation tech- image models specifically for inpainting. Starting from an
niques [24, 28, 44, 49]. The gap between recent one-step imagegenerationarchitecture,GLIDE[31]andStableDif-
diffusionmodels[30,37,58]andtheirexpensivemulti-step fusion Inpaint [42] add mechanisms to additionally condi-
counterpartsisclosing.Ourapproachalsoseekstospeedup tiononthemaskandmaskedimageandfine-tunethemod-
theimagesynthesisprocessfordiffusion-basedmodels,but els to predict the masked pixels. Recent advancements in
ourcontributionislargelyorthogonalandcanbecombined thisdomaininvolvetraininginpaintingmodelswithobject-
with these optimizations: we reduce the amount of image levelmasks[55]ratherthanrandomonesandpossiblyalso
datatoprocess,ratherthantheatomicdiffusioniteration. object-level text captions [57], mirroring real-world usage
moreclosely. Theseworksretrofitimagegenerationarchi-
Transformer-based generative models. Early transformers
tecturesforlocalediting,butthesemodelsproducethefull
for image generation generate image autoregressively [8,
image, includingregionsthatshouldnotbechanged. This
14,41]inscanlineorder. CogView2[12]proposesahierar-
is inefficient in time and computing resources. Our archi-
chical transformer to improve generation speed and shows
tecture efficiently performs local edits by generating only
applicationtotext-guidedimageinpaintingwithrectangular
themaskedregion.
masks. Laternon-autoregressivemodelslikeMaskGIT[6]
generateimagesgradually,afewtokensatatime,butthey
3.Method
dosoiteratively,generatingalltokensateveryiterationand
discarding the unmasked ones, which is inefficient. They Our goal is to develop an efficient diffusion generator for
focusonsequentialgenerationtoimproveimagequality. text-guided image editing, whose generation cost scales
Our transformer-based model design is inspired by withthesizeoftheregiontogenerate,andwhichcanincor-
Masked Autoencoders (MAEs) [17], but we reverse their poratethecontextoftheentireimageforafixed,smallfrac-
asymmetricdesign. Ourencoderprocessesallthetokensto tionofitstotalcost. StartingfromanimageI ∈ Rh×w×3,
produce context at the masked locations, and our decoder theuserspecifiestheregiontobeeditedwithabinarymask
operates on the masked tokens. Our decoder is a power- M ∈ {0,1}h×w and text prompt c, indicating where and
ful diffusion transformer, recently proposed as an alterna- whatcontenttogenerate. Amaskvalue1specifiesaholeto
tive to the popular UNet design [35, 52]. Most relevant to inpaint,and0forcontextpixelstonottouch. Unlessstated
thiswork,DiT[35]wasproposedforclass-conditionedim- otherwise,weuseimagesofh=w =1024resolution.
age generation and was improved in PixArt-α [7] to sup- Following standard practice, we operate in latent
porttext-conditioning. Ourdiffusiondecoderisanadapta- space [42], a compressed version of the RGB domain
tion of PixArt-α that additionally conditions on the global (§ 3.1). Observing that the iterative diffusion process
context produced by the encoder. Masked diffusion trans- is the computational bottleneck in state-of-the-art genera-
formers were previously explored for representation learn- tors,ourgeneratorhasanovelasymmetricencoder-decoder
ing[16,56]orforminimizingtrainingcost[61]. Ourfocus transformer architecture, as illustrated in Fig. 4. The en-
is on speeding inference to improve interactivity. Recent coder (§ 3.2) compresses and summarizes the whole im-
trends indicate that the transformer architecture becoming age context and is only run once. The decoder (§ 3.3) is a
central to state-of-the-start image [15] and video genera- transformer-baseddiffusiondenoiserthatisiterativelyrun,
tors[3],forwhichourmethodwouldenablefasterinference butonlyonthemaskedarea.Assuch,computationcostand
andinteractiveapplications. latencyareproportionaltothenumberofpixelstosynthe-
size, ratherthantheentirecanvas[1,55,57]. Thissignifi-
Text-guided diffusion-based image editing. Text-to-image
cantlyreducescomputationsince,formostedits,themasks
diffusion models have become the de-facto foundation for
aresmall.
generativeimageeditingmethods. Withusereditstypically
spatiallylocalized,significantefforthasgoneintodevelop- 3.1.Latentspaceprocessing
ing techniques that allow precise modifications [4, 18, 34]
byselectivelymanipulatinginternalrepresentations,e.g.at- Following previous works of Latent Diffusion Models
tention maps, during the denoising process to affect only (LDM) [42], our model operates in an intermediate latent
certain local regions without undesirable side-effects. An- space of 8× lower resolution with c = 4 channels, which
other line of work adopts the formulation of inpainting, reducescomputationwithoutsignificantlyimpactingvisual
whereamaskisprovidedtolocalizetheedit. Blendeddif- quality. We use the pretrained latent VAE of Stable Dif-
fusion[1]andDiffEdit[9]usepre-trainedgenerationmod- fusion [42], denoting the encoder and decoder E and D,
elsandspatiallyblendnoisedversionsoftheinputintothe respectively. We encode the masked image as our latent
gradualdenoisingprocesstoenforcethepreservationofun- input[55]:
maskedregions. Thisindirectapproachoftenresultinarti-
facts, leading more recent approaches to fine-tune text-to- Z =E(I⊙(1−M)) ∈Rh 8×w 8×c, (1)masked input & mask input patches encoded patches global context output
context encoder
tokenize drop tokens
text prompt global context conditioning
(cid:147)jalapeæo pepper(cid:148)
blend
tokenize
decoder decoder decoder
input noise noise patches output patches
Figure4.Overview.Togenerateanincrementalimageupdate,ouralgorithmtakesasinputausermaskandatextprompt.(top)Westart
bytransformingthevisiblepixelsandbinarymaskintopatches,andpassthemtoavisiontransformer(ViT)encoder. Wethendropall
tokens,exceptthosecorrespondingtotheholeregion;thisisourglobalcontext.(bottom)Togeneratethemissingpixels,weinitializeaset
ofnoisepatchescorrespondingtothemaskedregionandpassthemthroughadiffusiontransformermodelforseveraldenoisingiterations,
untilweobtaindenoisedpatches.Unlikepreviousworks[7,35],whichprocesstheentireimage,ourdiffusiontransformeronlyprocesses
thepatchesrequiredtocoverthemissingregion. Wetrainourencoderanddiffusiondecoderjointlyusingadiffusiondenoisingobjective
on the missing patches. The generated patches are then blended back into the missing region to produce the final output. Our model
operatesinapretrainedlatentimagespace[42],butweillustrateourpipelinewithRGBimagesforsimplicity.
where⊙representselement-wisemultiplicationacrossthe As the self-attention layers in the encoder transformer
spatialdimensions. enable all the tokens to interact, each individual token has
the potential to encode the relevant context of the whole
3.2.Globalcontextencoder image. Assuch,wediscardthetokenscorrespondingtothe
visibleregion, keepingtheonescorrespondingtothehole.
EncoderE processesthewholeimage,withthegoalofef-
Dropping tokens outside the mask creates an information
ficiently encoding the information given by the visible re-
bottleneck that encourages E to summarize the input con-
gion, so that a downstream decoder can synthesize a visu-
textinacompactsetoftokensandensuresthedownstream
allyconsistentoutputwiththecontext. OurencoderE isa
computation only scales with the size of the masked area,
VisionTransformers(ViT)[13].Toproducetokens,wefirst
sincethedecoderwillthusonlyprocesstokenscoveringthe
downsamplethemaskM usingalearnedconvolutionlayer
hole. The tokens should also represent the relevant infor-
tomatchthelatentspatialdimensions,asdonebyWanget
mation for the given location; previous works visualizing
al.[55]. Then,weconcatenatethedownsampledmaskand
transformers [5] suggest that this location information can
latentcodeZ alongthechanneldimensionsandanddivide
be preserved. Patches with partial holes are also included,
theminto4×4patches,withanoverlapof1oneachside.
andthevisiblepixelsinthosepatchesareblendedinatthe
ThisyieldsN =64×64=4096patches. Then,following
output step. Formally, we maxpool mask M to a 64×64
standard practice, we linearly embed each patch and add
mapandvectorizeintoaset{m }4096,wherem ∈{0,1}.
positional embedding [53]. Finally, the tokens are passed i i=1 i
through the transformers and produce a new set of N to-
T ={τ |m =1}⊆T . (3)
kens. Insummary,theencodertransformstheinputZ and hole i i all
M intoasetofN tokensofdimensiond=1152.
The remaining set of N ≤ N tokens form our com-
hole
pressed global context. This design, along other architec-
turalchoices,areevaluatedinthesupplemental.
T ={τ ,τ ,...,τ }=E(Z,M), τ ∈Rd. (2)
all 1 2 N i
3.3.Incrementaldiffusiondecoder
Tokendropping. Thesetofoutputtokenscontaininforma- Wesynthesizethemissingpixels,usingatransformer-based
tion regarding the whole image, but using them all would diffusiondecoderD[7,35]. RatherthankeepingasetofN
causedownstreamcomputationtoscalewithrespecttothe tokens representing the whole image, we start with N
hole
inputsize.Canweinsteadkeeponlyasubsetoftokens,that tokenscorrespondingtothehole,X = {x }. Thediffu-
hole i
wouldholdtheinformationneededforgeneration? sionprocesscreatestime-conditionedtokensXt ={xt},
hole iwheret ∈ [0,...,T],startingattimeT withfeaturesdrawn as well. In the early stages of this research, we pri-
from a unit Gaussian. The decoder progressively denoises marily explored unconditional inpainting on the ImageNet
these tokens, conditioned on the T5-encoded text prompt dataset[10],whicharedetailedinAppendixB.
c[39]andtheglobalcontextproducedbytheencoderT :
hole Dataset.Wetrainourmodelat1024×1024resolutiononan
Xt−1 =D(cid:0) Xt ⊕T ;t,c(cid:1) , (4) internaldatasetcontaining220millionhigh-qualityimages,
hole hole hole coveringawidevarietyofobjectsandscenes. Weproduce
where⊕denotesconcatenationalongthehiddendimension masksandtextpromptsinaprocesssimilartothatproposed
ofcorrespondingelementsineachset. Wefindthiscondi- by Xie et al. [57]. Specifically, we use an entity segmen-
tioningmechanismsuperiortoseveralalternativesanalyzed tation model [38] to segment all objects in an image and
inAppendixB. thencaptioneachentitywithBLIP-2[23]. Tosimulatethe
Blending. The final tokens X0 are mapped back into the roughandinaccuratemaskscreatedbyusers,werandomly
hole
dilatetheentitymask(see AppendixDfordetails). During
latentimagedomainusingalinearlayer,andtheinverseof
training, werandomlysampletripletsofimage, mask, and
thepatch-splittingproceduretoobtainapartiallatentimage
Zˆ
hole
∈ Rh 8×w 8×c. The missing tokens, corresponding to caption.
visiblepixels,areleftuninitializedwithzeros. Wecombine Baselines. WecompareLazyDiffusionwithtwoinpainting
thisoutputwiththevisiblelatent,usingpointwisemasking, baselines (already shown in Fig. 2), which we refer to as
toobtainthefinallatentcomposite: RegenerateImage and RegenerateCrop. RegenerateImage,
Zˆ =(1−M)⊙Z+M ⊙Zˆ . (5) istheapproachfoundinmostacademicworks[37,42,55,
hole 57], and operates on the entire image. RegenerateCrop,
Finally,thisisdecodedbythelatentdecodertoproducethe usedinpopularsoftwareframeworks[50,54], operateson
finalRGBimageIˆ=D(Zˆ). a tight square crop around the masked region. The crop
Thesedecodedresultsoccasionallycontainsfaintlyvisi- is first resized to a fixed low-resolution before processing
ble seams. Previous works performing inpainting with la- and is upsampled back afterwards. Both approaches gen-
tent diffusion models observed this phenomenon and ad- erate as many pixels as their input contains (whether full-
dresseditwithadedicatedlatentdecoder[62]. Astheirde- canvas or local crop), unlike LazyDiffusion that generates
coder is computationally intensive, we opt to use a simple onlymaskedpatches.
Poisson blending postprocessing step [36] in RGB space. Toensureafaircomparison,weutilizethePixArt-αar-
We discuss this challenge in greater length in the supple- chitecture for both approaches. Since there is currently no
mental. publicly available PixArt-based inpainting models, we de-
signandtrainthemourselves. WeadaptPixArtforinpaint-
Training and implementation details. For the decoder, we
ing using the same procedure employed to transform Sta-
adoptthePixArt-α[7]architecture,andaddasinglelayerto
ble Diffusion [42] from generation to inpainting. Specifi-
supportourconditioningoncontext.Weinitializeallshared
cally, we incorporate the GLIDE [31] conditioning mech-
layersfromthepublicPixArt-αcheckpointtobenefitfrom
anism, where the generator operates on 9 latent channels:
theirpretraining. Theencoderontheotherhand,istrained
four channels for the latent being denoised, four channels
fromscratch. Thetwomodelsaretrainedjointlytorecon-
representing the latent of the masked input image, and the
struct masked (latent) pixels, using the Improved DDPM
lastchannelcontainingadownsampledversionofthemask.
objective[32].Wetrainourmodelfor100,000iterationson
WetraintwoPixArtmodelsat1024×1024and512×512
56NVIDIAA100GPUs,usingtheAdamWoptimizer[25],
forRegenerateImageandRegenerateCrop,respectively.
withaconstantlearningrate2×10−5,weightdecaysetto
WealsocomparewithStableDiffusionvariantsofthese
3×10−2 andglobalbatchsizeof224. WeuseT = 1000
two approaches for reference: SDXL [37] operates on the
diffusionstepsduringtraining. Wegenerateourresultsus-
entire1024×1024image,whileSD2-crop[42]operateson
ing the Improved DDPM sampler [32] with 50 steps, un-
a512×512crop. Itisimportanttonotethatthesemodels
lessspecifiedotherwise,andsettheclassifier-freeguidance
utilizedifferentarchitecturesandweretrainedondifferent
scale to 4.5. All running times are measured on a single
datasets,andhencearenotdirectlycomparable.Weinclude
A100GPU.WeprovidefurtherdetailsinAppendixD.
theminthiscomparisononlyasreferencesforstate-of-the-
4.Experiments artquality.
4.1.Experimentalsetup 4.2.Inferencetime
Themainpaperprimarilyfocusesonatext-conditionedset- We illustrate the overall runtime of all methods in Fig. 5.
ting, as do the experiments that follow. However, our ap- Thebaselinesrunisconstanttime,astheyoperateonfixed
proach is versatile and can be applied in other use cases size tensors derived from the fixed input size – full canvas“Croissant” “Chocolate fondue” “Cinnamon Roll” “Churros”
20
15
LazyDiffusion
RegenerateImage
10
RegenerateCrop
5
“huge fish” “colorful jellyfish” “sea turtle” “red corals”
0.2 0.4 0.6 0.8 1.0
Mask ratio
Figure5. ComparingLazyDiffusion’sruntimetothatofbaselines
regeneratingtheentire1024×1024imageorasmaller512×512
crop around the mask. LazyDiffusion is consistently faster than
RegenerateImage,especiallyforsmallmaskratiostypicaltointer-
activeedits,reachingaspeedupof10×. Similarly,LazyDiffusion “Desert cliff” “Lake” “Huge waterfall” “Flooded jungle”
isfasterthanRegenerateCropformaskratios< 25%. Formasks
greaterthanthat(dashed),RegenerateCropistechnicallyfasterbut
generates in low-resolution and naively upsamples to match the
desiredresolution,harmingimagequality.
forRegenerateImageandafixed-sizecropforRegenerate-
Crop. In contrast, LazyDiffusion’s runtime scales with the
“cute smiling cactus” “muscular arms” “sombrero” “spaghetti bowl”
mask size, because our decoder processes tensors with di-
mensionsproportionaltothemaskedregion. Thisleadsto
significantspeedupsforsmallmasks, typicalofinteractive
editing applications. For example, with a mask covering
10%oftheimageourmodelachievesa×10speedupover
RegenerateImage. Similarly, LazyDiffusion is also faster
thanRegenerateCropformaskssmallerthan25%. Atmask
ratio25%,bothmethodsgeneratethesamenumberofpix-
Figure 6. Progressive image editing (top) and image generation
els and have comparable running times. For larger masks,
(bottom)usingLazyDiffusion. Eachpanelillustratesagenerative
RegenerateCropisfasterbutgenerateslow-resolutioncrops
progression compared to the preceding state of the canvas to its
andnaivelyupsamplestonativeresolution,reducingsharp-
left. LazyDiffusion markedly accelerates local image edits (ap-
ness. Additionally, RegenerateCrop often fails to produce
proximately×10),renderingdiffusionmodelsmoreaptforuser-
outputsthatareconsistentwiththeregionoutsidethemask, in-the-loopapplications.
aswediscussbelow(Sec.4.4).
few-step[49],orevenone-stepmodels[58]. Weexpectthe
While there are additional networks in the pipeline, the
performancegainsprovidedbyourstrategytobeevenmore
diffusion decoder is the only component running multiple
striking on costlier applications like high-resolution image
times,andthusdominatestheruntime.Notably,ourcontext
editing,orvideosynthesis[3].
encoder adds a 73ms overhead, which is dwarfed by the
cost of the diffusion loop. The latent encoder and decoder 4.3.Progressivegeneration
take97msand176ms,respectively,andtheT5textencoder
Diffusion models are challenging to integrate into inter-
21ms. Thesearesharedbyallmethods.
active pipelines due to their high latency. There exists
Scaling laws. Our method essentially reduces the cost of an abundance of research on broadly accelerating diffu-
each denoising iteration at the price of a small overhead sion models [26, 49, 58], but in the context of this study,
forthecontextencoder,tobalancequalitywithcontextre- we highlight that individuals often tackle tasks incremen-
tention. Asa result, our performancegains aremost strik- tally, executingoperationsprogressivelyandconcentrating
ingforhighdiffusionstepcounts(typicallycorrelatedwith on local adjustments one at a time—whether it involves
higher image quality), and smaller mask sizes (most fre- adding or removing objects, refining, or retrying previous
quent in interactive applications). A single evaluation of attempts. LazyDiffusionsignificantlyacceleratessuchlocal
our decoder takes 374ms to generate full image, but only operations, making it well-suited for interactive pipelines
28msfor10%masks—a×13.4speedup,greaterthanthe withauser-in-the-loop.
encoder’soverhead. So,ourmethodremainsbeneficialfor In Fig. 6, we showcase a couple of iterations using
]ces[
emitnuRTable 1. Quantitative comparison of our method with the three croplackknowledgeoftheglobalimageandconsequently
baselines. We report zero-shot FID [20] and CLIPScore [19] produce objects that may seem reasonable in isolation but
on 10k images images from OpenImages [45]. Scores of SD2-
donotfitwellwithinthegreaterimagecontext(Fig.7(Bot-
crop[42]andSDXL[37]arenotdirectlycomparableandprovided
tom)). In contrast, SDXL and RegenerateImage utilize di-
onlyforreference.
rectandfullaccesstoallimagepixelstoconsistentlyyield
highlyrealisticresults,wherethegeneratedregionfitswell
Method CLIPScore(↑) FID(↓)
with the existing content. Notably, we find that LazyDif-
SD2-crop 0.21 6.95
fusion behaves similarly and produces comparable results
SDXL 0.21 6.88
eveninthesechallengingedgecases. Thissuggeststhatthe
RegenerateCrop 0.19 9.35 compressedimagecontextishighlyexpressiveandencodes
RegenerateImage 0.19 7.38 meaningfulsemanticinformation.
LazyDiffusion(Ours) 0.19 7.70
Userstudy. Wemeasurethemodels’capabilitytoproduce
highly-contextualinpaintingthroughauserstudy. Forthis,
we curate a specialized test set comprising scenarios that
LazyDiffusionforbothimageeditingandimagegeneration,
necessitate a high level of semantic image context for ef-
startingfromablankcanvas. Furthermore,weattachasup-
fectiveinpainting. Specifically, weselectimagesfeaturing
plemental video that showcases authentic user interactions
severalcloselyrelatedobjects,suchasasetofuniformbuns
withbothLazyDiffusionandourRegenerateImagebaseline,
on a tray. Subsequently, we evaluate all models based on
highlighting the discernible difference in running time be-
theirabilitytoregenerateoneoftheseobjectswhenmasked.
tweenthetwo.
In this scenario, the models must rely on visible pixels to
produceahigh-fidelityresult. Usersarepresentedwiththe
4.4.Inpaintingquality
maskedinputimage,atextprompt,andtworesults—ours
A distinctive feature of LazyDiffusion is its utilization of andabaseline. Theyarethenaskedto”selecttheoptionin
a compressed global context to aid inpainting. In con- whichtheinpaintedimage,asawhole,looksbest”.Wecol-
trast,RegenerateImageutilizesthecompleteglobalcontext, lectatotalof1778responsesfrom48uniqueusersandfind
whileRegenerateCropreliesonthecontextprovidedbypix- thatourmethodisstronglypreferredovermethodsoperat-
elsneighboringthemask. Wenowcomparetheresultspro- ingsolelyonacropandcompetitivewiththoseregenerating
ducedbytheseapproaches. the entire image. Specifically, LazyDiffusion is preferred
Forquantitativeevaluation,wereportzero-shotFID[20] over RegenerateCrop in 81% of cases, over SD2-crop in
and CLIPScore [19], which estimate similarity to real im- 82.5% of cases, over RegenerateImage in 46.1% of cases,
ages and text-image alignment, respectively. Additionally, and over SDXL in 48.5% of cases. These results indicate
we include scores for SDXL [37] and SD2-crop [42]. De- thatthecompressedencodercontextretainsthecoreseman-
spite not being directly comparable, because they use dif- ticinformationrequiredevenforchallengingusecases. In
ferent architectures and training data, they serve as refer- short, our model demonstrates competitive quality to our
encesforstate-of-the-artquality.InTable1,wereportmean conceptual upper-bound RegenerateImage, but runs up to
scoresoverarandomsampleof10,000imagesdrawnfrom tentimesfaster.
OpenImages [45]. Notably, text-image alignment (CLIP)
4.5.Sketch-guidedinpainting
remainsunaffectedbythemechanismtouseimagecontext.
OntheFIDmetric, LazyDiffusionexhibitsonlyamarginal
So far, our emphasis has been on generation guided solely
increasecomparedtoRegenerateImage(%4)andperforms
by the mask and a text prompt. However, in principle,
significantlybetterthanRegenerateCrop(%26).
our method is applicable to any localized generation task
WeshowqualitativecomparisonsinFig.7. Ourexami- andcanaccommodateotherformsofconditioning,suchas
nationrevealsasignificantdiscrepancyintheperformance sketches and edge maps. In Fig. 8, we briefly showcase
ofmodelsregeneratingacrop–RegenerateCropandSD2-
thisversatilitybyguidingthegenerationwithacoarsecolor
crop. Inmanyinstances, inpaintinginvolvesgeneratingan sketchprovidedbytheuser. FollowingtheSDEdit[29]ap-
objectthatisvisuallyindependentofotherconceptsinthe proach,weinitiatethegenerationprocessfromthepartially
image,suchasaddingasideoffriesnexttoaburger. Here, noisedinputimageinsteadofGaussiannoise.
models operating on a tight crop can produce reasonable-
looking objects and seamlessly blend them with the sur- 5.Conclusions,limitationsandfuturework
rounding pixels available in the crop (Fig. 7 (Top)). How-
ever,innumerousscenarios,thegoalistoaddanobjectthat We introduced a novel transformer-based encoder-decoder
is strongly related to the existing context, such as adding architectureforinteractiveimagegenerationandeditingus-
anotherbuntoatrayofbuns. Modelsoperatingsolelyona inga diffusionmodel. Our approachreducesthe diffusionRegenerate Crop Regenerate Image
LazyDiffusion
Input SD2 PixArt-㌵ SDXL PixArt-㌵
(Ours)
㌵ ㌵
Figure7.ComparingInpaintingResults:(Top)Inpaintingmostobjectsrequiresrelativelylittlesemanticcontext.Insuchcases,allmethods
producereasonablygoodresults,eventhoseprocessingonlyatightcrop. (Bottom)However,wheninpaintinganobjectcloselyrelatedto
others,suchasonebunoutofmany,theinpaintingmodelrequiresrobustsemanticunderstanding.Methodsprocessingonlyacropproduce
objectsthatmayseemreasonableinisolation,butdonotfitwellwithinthegreatercontextoftheimage.Incontrast,LazyDiffusionadeptly
leveragesthecompressedimagecontexttogeneratehigh-fidelityresults,comparableinqualitytomodelsregeneratingtheentireimage
andrunninguptotentimesslower.AdditionalresultsareprovidedinFigs.11to14.
”gorf
deR“
”seirF
hcnerF“
”redraobwonS“
”sesuohdriB“
”nuB“
”elaB“
”moorhsuM“
”yrrebwartS“input output input output better captions. Computer Science. https://cdn. openai.
com/papers/dall-e-3.pdf,2:3,2023. 2
[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,
Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-
man, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya
Ramesh. Video generation models as world simulators.
(cid:147)rabbit(cid:148) (cid:147)butterfly(cid:148) 2024. 3,6
[4] MingdengCao,XintaoWang,ZhongangQi,YingShan,Xi-
Figure 8. Our model readily supports additional forms of local
aohuQie,andYinqiangZheng. Masactrl: Tuning-freemu-
conditioning.Forexample,similartoSDEdit[29],ausercandraw
tualself-attentioncontrolforconsistentimagesynthesisand
asimplisticcoloredsketch,providingthemodelshapeandcolor
editing. arXivpreprintarXiv:2304.08465,2023. 3
information.
[5] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou,
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg-
ingpropertiesinself-supervisedvisiontransformers.InPro-
runtime by only generating the patches corresponding to
ceedingsoftheIEEE/CVFinternationalconferenceoncom-
thesmallregiontosynthesize,ratherthantheentireimage. putervision,pages9650–9660,2021. 4
Thisisachievedthroughaglobalcontextencoderthatsum- [6] HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamT
marizes the entire image once, outside the diffusion sam- Freeman. Maskgit: Masked generative image transformer.
plingloop,ensuringglobally-consistentoutputs. In Proceedings of the IEEE/CVF Conference on Computer
Ourmethodmaintainsthegenerationqualityofstate-of- VisionandPatternRecognition,pages11315–11325,2022.
the-artmodels,andreducesruntimecostsproportionallyto 3
thesizeoftheregiontogenerate. Thisreductioninlatency, [7] JunsongChen,JinchengYu,ChongjianGe,LeweiYao,Enze
Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,
particularly for small masks, transforms image generation
HuchuanLu,andZhenguoLi. Pixart-α:Fasttrainingofdif-
intoaninteractiveprocessbyspreadingthegenerationcost
fusiontransformerforphotorealistictext-to-imagesynthesis,
acrossmultipleuserinteractions.
2023. 2,3,4,5,12
Our architecture does have some weaknesses. Despite
[8] MarkChen,AlecRadford,RewonChild,JeffreyWu,Hee-
operating outside the diffusion loop, the context encoder
wooJun, DavidLuan, andIlyaSutskever. Generativepre-
processes the entire image, posing a potential bottleneck trainingfrompixels.InInternationalconferenceonmachine
forveryhigh-resolutionimagesduetoitsquadraticscaling learning,pages1691–1703.PMLR,2020. 3
ininputsize. Addressingthislimitationcouldenhancethe [9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,
scalability and applicability of our approach to larger and and Matthieu Cord. Diffedit: Diffusion-based seman-
more intricate visual content. We observed that occasion- tic image editing with mask guidance. arXiv preprint
ally,generatedresultshaveasubtlecolorshiftcomparedto arXiv:2210.11427,2022. 3
the visible image regions, leading to visible patch bound- [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
aries. WhilethePoissonblendingpost-processingmethods andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage
database. In2009IEEEconferenceoncomputervisionand
discussed in Section 3.3 effectively mitigates these issues,
patternrecognition,pages248–255.Ieee,2009. 5,12
futureresearchisneededtoidentifyamoreprincipledand
[11] PrafullaDhariwalandAlexanderNichol. Diffusionmodels
systematicsolution.
beatgansonimagesynthesis. Advancesinneuralinforma-
Acknowledgement. We are grateful to Minguk Kang, tionprocessingsystems,34:8780–8794,2021. 2
Tianwei Yin and Wei-An Lin for technical sugges- [12] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
tions, to Rotem Shalev-Arkushin for proofreading Cogview2: Faster and better text-to-image generation via
our draft and offering feedback, and to Yogev Nitzan hierarchicaltransformers. AdvancesinNeuralInformation
for his help running the user study. This work was ProcessingSystems,35:16890–16902,2022. 3
done while Yotam Nitzan was an intern at Adobe. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
References vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
[1] OmriAvrahami,DaniLischinski,andOhadFried. Blended arXiv:2010.11929,2020. 2,4,12
diffusionfortext-driveneditingofnaturalimages. InPro- [14] PatrickEsser,RobinRombach,andBjornOmmer. Taming
ceedingsoftheIEEE/CVFConferenceonComputerVision transformers for high-resolution image synthesis. In Pro-
andPatternRecognition,pages18208–18218,2022. 2,3 ceedings of the IEEE/CVF conference on computer vision
[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng andpatternrecognition,pages12873–12883,2021. 3
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim
Lee, Yufei Guo, et al. Improving image generation with Entezari, Jonas Mu¨ller, Harry Saini, Yam Levi, DominikLorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim [29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
Dockhorn, ZionEnglish, KyleLacey, AlexGoodwin, Yan- junWu,Jun-YanZhu,andStefanoErmon. Sdedit: Guided
nikMarek,andRobinRombach.Scalingrectifiedflowtrans- imagesynthesisandeditingwithstochasticdifferentialequa-
formersforhigh-resolutionimagesynthesis,2024. 3 tions. arXivpreprintarXiv:2108.01073,2021. 2,7,9
[16] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and [30] ThuanHoangNguyenandAnhTran. Swiftbrush: One-step
Shuicheng Yan. Masked diffusion transformer is a strong text-to-imagediffusionmodelwithvariationalscoredistilla-
imagesynthesizer. arXivpreprintarXiv:2303.14389,2023. tion. arXivpreprintarXiv:2312.05239,2023. 3
3 [31] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
Dolla´r,andRossGirshick.Maskedautoencodersarescalable MarkChen. Glide:Towardsphotorealisticimagegeneration
visionlearners. InProceedingsoftheIEEE/CVFconference andeditingwithtext-guideddiffusionmodels.arXivpreprint
on computer vision and pattern recognition, pages 16000– arXiv:2112.10741,2021. 1,3,5,12
16009,2022. 3 [32] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, denoising diffusion probabilistic models. In International
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im- ConferenceonMachineLearning,pages8162–8171.PMLR,
age editing with cross attention control. arXiv preprint 2021. 5
arXiv:2208.01626,2022. 3 [33] GauravParmar,RichardZhang,andJun-YanZhu.Onbuggy
[19] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras, resizinglibrariesandsurprisingsubtletiesinfidcalculation.
andYejinChoi. Clipscore:Areference-freeevaluationmet- arXivpreprintarXiv:2104.11222,2021. 14
ricforimagecaptioning. arXivpreprintarXiv:2104.08718, [34] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-
2021. 7,14 Elor, and Daniel Cohen-Or. Localizing object-level shape
[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, variations with text-to-image diffusion models. arXiv
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a preprintarXiv:2303.11306,2023. 3
twotime-scaleupdateruleconvergetoalocalnashequilib- [35] WilliamPeeblesandSainingXie. Scalablediffusionmodels
rium. Advances in neural information processing systems, with transformers. In Proceedings of the IEEE/CVF Inter-
30,2017. 7,13,14 nationalConferenceonComputerVision,pages4195–4205,
[21] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif- 2023. 2,3,4,12,13
fusionprobabilisticmodels. Advancesinneuralinformation [36] PatrickPe´rez,MichelGangnet,andAndrewBlake. Poisson
processingsystems,33:6840–6851,2020. 2 imageediting.InACMSIGGRAPH2003Papers,pages313–
[22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 318.2003. 5,14
Elucidating the design space of diffusion-based generative [37] Dustin Podell, Zion English, Kyle Lacey, Andreas
models. Advances in Neural Information Processing Sys- Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and
tems,35:26565–26577,2022. 3,14 Robin Rombach. Sdxl: Improving latent diffusion mod-
[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. els for high-resolution image synthesis. arXiv preprint
Blip-2: Bootstrapping language-image pre-training with arXiv:2307.01952,2023. 2,3,5,7,15
frozen image encoders and large language models. arXiv [38] Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, Hengshuang
preprintarXiv:2301.12597,2023. 5 Zhao, PhilipTorr, ZheLin, and JiayaJia. Openworld en-
[24] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and tity segmentation. IEEE Transactions on Pattern Analysis
Qiang Liu. Instaflow: One step is enough for high-quality andMachineIntelligence,2022. 5,19
diffusion-based text-to-image generation. arXiv preprint [39] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,
arXiv:2309.06380,2023. 3 SharanNarang, MichaelMatena, YanqiZhou, WeiLi, and
[25] IlyaLoshchilovandFrankHutter. Decoupledweightdecay Peter J Liu. Exploring the limits of transfer learning with
regularization. arXiv preprint arXiv:1711.05101, 2017. 5, a unified text-to-text transformer. The Journal of Machine
14 LearningResearch,21(1):5485–5551,2020. 5
[26] ChengLu,YuhaoZhou,FanBao,JianfeiChen,Chongxuan [40] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
Li,andJunZhu.Dpm-solver:Afastodesolverfordiffusion andMarkChen. Hierarchicaltext-conditionalimagegener-
probabilisticmodelsamplinginaround10steps. Advances ationwithcliplatents. arXivpreprintarXiv:2204.06125,1
in Neural Information Processing Systems, 35:5775–5787, (2):3,2022. 2
2022. 3,6 [41] AliRazavi,AaronVandenOord,andOriolVinyals. Gener-
[27] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx- atingdiversehigh-fidelityimageswithvq-vae-2. Advances
uanLi,andJunZhu. Dpm-solver++: Fastsolverforguided inneuralinformationprocessingsystems,32,2019. 3
sampling of diffusion probabilistic models. arXiv preprint [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
arXiv:2211.01095,2022. 3 Patrick Esser, and Bjo¨rn Ommer. High-resolution image
[28] Simian Luo, Yiqin Tan, LongboHuang, Jian Li, andHang synthesis with latent diffusion models. In Proceedings of
Zhao. Latent consistency models: Synthesizing high- theIEEE/CVFConferenceonComputerVisionandPattern
resolution images with few-step inference. arXiv preprint Recognition,pages10684–10695,2022. 1,2,3,4,5,7,13,
arXiv:2310.04378,2023. 3 14,15[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Yuille, and Christoph Feichtenhofer. Diffusion models as
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, masked autoencoders. arXiv preprint arXiv:2304.03283,
RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans, 2023. 3
etal.Photorealistictext-to-imagediffusionmodelswithdeep [57] ShaoanXie, ZhifeiZhang, ZheLin, TobiasHinz, andKun
language understanding. Advances in Neural Information Zhang.Smartbrush:Textandshapeguidedobjectinpainting
ProcessingSystems,35:36479–36494,2022. 2 withdiffusionmodel.InProceedingsoftheIEEE/CVFCon-
[44] Tim Salimans and Jonathan Ho. Progressive distillation ferenceonComputerVisionandPatternRecognition,pages
for fast sampling of diffusion models. arXiv preprint 22428–22437,2023. 3,5,15,19
arXiv:2202.00512,2022. 3 [58] Tianwei Yin, Michae¨l Gharbi, Richard Zhang, Eli Shecht-
[45] Christoph Schuhmann, Richard Vencu, Romain Beaumont, man,FredoDurand,WilliamTFreeman,andTaesungPark.
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo One-step diffusion with distribution matching distillation.
Coombes,JeniaJitsev,andAranKomatsuzaki.Laion-400m: arXivpreprintarXiv:2311.18828,2023. 3,6
Open dataset of clip-filtered 400 million image-text pairs. [59] JiahuiYu,ZheLin,JimeiYang,XiaohuiShen,XinLu,and
arXivpreprintarXiv:2111.02114,2021. 7 Thomas S Huang. Free-form image inpainting with gated
[46] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan, convolution. InProceedingsoftheIEEE/CVFInternational
and Surya Ganguli. Deep unsupervised learning using ConferenceonComputerVision,pages4471–4480,2019. 1,
nonequilibrium thermodynamics. In International confer- 12
enceonmachinelearning,pages2256–2265.PMLR,2015.
[60] ShengyuZhao,JonathanCui,YilunSheng,YueDong,Xiao
2 Liang,EricIChang,andYanXu.Largescaleimagecomple-
[47] Jiaming Song, Chenlin Meng, and Stefano Ermon. tionviaco-modulatedgenerativeadversarialnetworks.InIn-
Denoising diffusion implicit models. arXiv preprint ternationalConferenceonLearningRepresentations(ICLR),
arXiv:2010.02502,2020. 3 2021. 1
[48] YangSong,JaschaSohl-Dickstein,DiederikPKingma,Ab-
[61] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima
hishekKumar,StefanoErmon,andBenPoole. Score-based
Anandkumar. Fasttrainingofdiffusionmodelswithmasked
generative modeling through stochastic differential equa-
transformers. arXivpreprintarXiv:2306.09305,2023. 3
tions. arXivpreprintarXiv:2011.13456,2020. 2
[62] ZixinZhu, XueluFeng, DongdongChen, JianminBao, Le
[49] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Wang,YinpengChen,LuYuan,andGangHua. Designinga
Sutskever. Consistencymodels. 2023. 3,6
betterasymmetricvqganforstablediffusion. arXivpreprint
[50] stable-diffusion webui. stable-diffusion-webui.
arXiv:2306.04632,2023. 5,14
https://github.com/AUTOMATIC1111/stable-
[63] ZixinZhu, XueluFeng, DongdongChen, JianminBao, Le
diffusion-webui,2024. Accessed:Jan2024. 1,2,5
Wang,YinpengChen,LuYuan,andGangHua. Designinga
[51] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
betterasymmetricvqganforstablediffusion,2023. 14
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor
Lempitsky. Resolution-robust large mask inpainting with
fourier convolutions. arXiv preprint arXiv:2109.07161,
2021. 12
[52] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,
DanielCohen-Or,andAmitHBermano.Humanmotiondif-
fusionmodel. arXivpreprintarXiv:2209.14916,2022. 3
[53] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin. Attentionisallyouneed. Advancesinneural
informationprocessingsystems,30,2017. 2,4
[54] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro
Cuenca,NathanLambert,KashifRasul,MishigDavaadorj,
and Thomas Wolf. Diffusers: State-of-the-art diffusion
models. https://github.com/huggingface/
diffusers,2022. 1,2,5
[55] SuWang,ChitwanSaharia,CesleeMontgomery,JordiPont-
Tuset,ShaiNoy,StefanoPellegrini,YasumasaOnoe,Sarah
Laszlo, David J Fleet, Radu Soricut, et al. Imagen editor
and editbench: Advancing and evaluating text-guided im-
ageinpainting.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages18359–
18369,2023. 1,2,3,4,5,14,15
[56] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao
Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, AlanA.SupplementaryOverview number of tokens. Thus, the runtime of this architecture
scales as O(k2). In this section, we refer to this architec-
In Appendix B, we conduct an ablation study, comparing
tureasthe“ConcatHidden”variant.
our chosen architecture with possible alternatives. Then,
inAppendixC,weanalyzeourblendingapproachatpost- B.3.Alternativedesigns
processing and extend the qualitative evaluation from the
Wenextdescribealternativedesignswiththegoalofablat-
main paper. Finally, in Appendix D, we offer additional
ingthetwocorechoices–droppingvisibletokenstocom-
implementationdetails,completingthepaper.
presscontextandconditioningthroughconcatenation
B.ArchitectureDesignandAblation Fullcontextdesigns,utilizingthefullsetofN encoderto-
kensT ascontext:
all
Pivotaltoourarchitecturaldesigniscompressingthevisible
contexttofewertokensandutilizingitwithinthediffusion • RegenerateImage– As described in the paper, we adapt
decoder. In the following section, we describe the experi- DiT for inpainting using the GLIDE [31] conditioning
mentsleadingtooureventualdesign. approach. This model represents the common approach
in local editing literature – operates on the entire canvas
B.1.Setting thusseeingthefullcontextbutalsore-generatingtheen-
tireimage. Theruntimecomplexityofthisvariantscales
While text-based inpainting serves as the primary appli-
asO(N2). NotethatN >>k.
cation demonstrated in this paper, LazyDiffusion is read-
• Full-Context Cross-Attention – We add a cross-attention
ily applicable to a range of other local generation appli-
layer to the DiT block, between the self-attention and
cations. When designing our architecture in early stages
MLP layers. Other than the upstream activations, the
of this work, we applied our method to unconditional in-
cross-attentionlayergetsasinputthefullencodercontext
painting [51, 59] on ImageNet [10] at 256 × 256 resolu-
tokens T . Despite “seeing” the full context, the model
tion,asthissettingdemandssubstantiallylesstrainingtime all
generatesonlythek maskedpatches. It’sruntimescales
andresources. WeadoptthemaskingprotocolfromDeep-
asO(Nk).
FillV2[59]. WeusethesameViTXL/2[13]backbonefor
our context encoder and adopt DiT XL/2 [35] for the dif- Compressedcontextdesigns.Comparabletoourchosende-
fusiontransformer. NotethatthePixArt-α[7]architecture, sign–thefollowingmodelsutilizethemaskedtokensT
hole
used in the main paper, is a straight-forward adaptation of as context, generate only the masked region and have run-
DiT to support text conditioning. Consequently, the archi- times that scale with O(k2). They differ in their mecha-
tectureswedescribenextcanseamlesslyusebothasback- nism to condition on the context tokens. We experiment
bones. with simple conditioning approaches that are applied near
the input level. This prevents designs from being tightly
B.2.Chosendesignreview
coupledwiththespecificbackbonearchitecture, whichwe
Recall that in our proposed architecture, discussed in Sec. anticipatewouldfacilitateeasieradaptationtofuturediffu-
3, we selectively retain only encoder output tokens cor- siontransformers.
responding to the masked region, marked T . This en-
hole • ConcatLength–Thesetsoftokensareconcatenatedover
suresthatdownstreamdecodercomputationscaleswiththe
thesequencelength, ratherthanhiddendimension. This
mask size rather than the image size. At time t, the de-
requiresthetwosetsoftokenstohavethesamehiddendi-
coder denoises tokens Xt while conditioning on the re-
hole mension. Tothisend,wefirstlinearlyprojectthecontext
tained context tokens. We implement the conditioning by
tokenstothedecoder’shiddendimensiond. Formally, a
concatenating the context tokens to the noise tokens at the
singledenoisingstepisdoneby
decoder’s input. Omitted from the main paper for clarity,
weprependalinearprojectionlayertothediffusiontrans- Xt−1 =DiT(cid:0) [Xt ,linear(T )];t,c(cid:1) , (7)
hole hole hole
former backbone, projecting the concatenation of tokens
to the decoder’s hidden dimension d. Other than the first where[·,·]representsthesequence-lengthconcatenation.
layer, the diffusion transformer is then used as-is to gen- • WeightedSum–Anadditionalweightw ∈Rdislearned,
erate k = |T hole| tokens. Rewriting Eq. (4) from the main andtheinputtoDiTisaweightedsumofthetwosetsof
paperwithgreaterdetail,asingledenoisingstepreadsas tokens,formally
Xt−1 =DiT(cid:0) linear(Xt ⊕T );t,c(cid:1) , (6) Xt−1 =DiT(cid:0) Xt +w∗linear(T );t,c(cid:1) . (8)
hole hole hole hole hole hole
where ⊕ denotes concatenation along the hidden dimen- • Compressed-Context Cross-Attention – We again add a
sion. Transformers runtime scale quadratically with the cross-attention layer, but here it attends only to the re-Table 2. Hyperparameters configuration for all architecture de-
signs. StartingfromDiT’sXL/2configuration,weslightlyadapt 160 Co(k n2 c) at Hidden Cr( oN sk s) Attention
thehyperparameterstoensureFLOPcountsofO(k2)arecompa- Weighted Sum (N2)
140 Concat Length RegenerateImage
rable. Cross Attention
120
Runtime Hidden 100
Model Layers
Complexity Dimension 80
60
ConcatHidden 28 1152
40
WeightedSum 28 1152
O(k2)
20
ConcatLength 24 1024
CrossAttention 26 1152 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Mask Ratio
O(Nk) CrossAttention 28 1152
(a)
O(N2) RegenerateImage 28 1152
12 (k2) (Nk)
Concat Hidden Cross Attention
11 Weighted Sum (N2)
duced set of tokens T . To better resemble other de- Concat Length RegenerateImage
hole 10 Cross Attention
signsinthiscategory,incorporatingtheconditioningnear
9
theinput,weaddthecross-attentionlayeronlytothefirst
DiTblock. 8
7
B.4.Configurations
6
DiT’s FLOPs are strongly negatively correlated with FID, 5
100k 200k 300k 400k 500k
across different configurations [35]. To facilitate direct
Training Steps
comparison, we slightly adjust the XL/2 configuration for
(b)
the O(k2) variants so that their FLOP counts are similar.
Weprovidetheexacthyperparametersusedwitheachvari- Figure9. Comparingthevariousarchitecturedesignsintermsof
antinTab.2andtheresultingFLOPcountsasafunctionof (a)FLOPsand(b)quality,measuredviaFID[20].Solidlinesrep-
mask size are in Fig. 9a. As can be seen, Concat Hidden, resentvariantsofourapproach–theencoderoutputsacompressed
WeightedSumandtheCompressed-ContextCross-Attention contextandthedecodergeneratesonlythemaskedregion.Dashed
linesrepresentmechanismsinwhichthedecoderisconditionedon
have comparable FLOPs on the entire spectrum ranging
thefullimagecontextandeithergeneratesthemaskedregionor
from mask ratio of 10% to 100%. For full masks, the
theentireimage. Thelatteristheapproachtakenbyexistingin-
ConcatHidden,WeightedSumvariantsuse0.4%and0.6%
painting approaches [42]. The runtime complexities of different
moreFLOPsthanRegenerateImage,respectively. Thisim-
approachesisnotedinthelegend. Ascanbeseen, conditioning
plies that our conditioning introduces negligible overhead
each generated token directly on its corresponding compressed
and is well suited for using larger masks with no appar- context token, as done for the “Concat Hidden” and “Weighted
entdownside. Theotherthreevariantshavestrictlygreater Sum”variants,leadstosuperiorperformance,despiteusingfewer
FLOPcounts. FLOPsthancompetingapproaches.
B.5.Results
WetracktheFID[20]scoresacross500Ktrainingiterations yield better results. Specifically, in the two cross-attention
foralldecoderdesignsandpresenttheresultsinFig.9b. variants, the one that uses compressed context is superior
Initially, we observe that “Concat Hidden” and to the one using full context. Our attempts to improve the
“WeightedSum”notablyoutperformallothervariants. We performance of the RegenerateImage baseline by using a
attribute this superior performance to the explicit one-to- contextencoderanda“ConcatHidden”basedconditioning
one context provided by these approaches. In both cases, were futile; only dropping the visible context tokens was
eachnoisetokenisdirectlyconditionedonthecorrespond- effective. We speculate that incorporating the full context
ingcontexttoken.Incontrast,othermethodsrequirethede- imposes additional complexity on the decoder’s task. In
codertoextractcontextfromasetofencodertokens,which comparison,withLazyDiffusion,theinformationbottleneck
appearstobemorechallengingdespitetheuseofpositional encouragesthecontexttobeexpressivebutselective,allow-
embeddingandmoreexpressivemechanismssuchascross- ingthedecoderto“concentrate”onsynthesisonly.
attention. Interestingly, in the text-conditioned setting, LazyDiffu-
Furthermore,wenotethatthemorecomputationallyin- sionisnotsuperiorintermsofqualitytoRegenerateImage.
tensivebaselines,whichleverageadditionalcontext,donot This disparity might be explained by the lower level con-
spolfG
K05-DIFtextrequiredforunconditionalinpainting,whichprimarily Zero-pad Vanilla Zhu et al. Poisson
Input
Decoding Decoder Decoder Blending
involves continuing surrounding textures, compared to the
semanticcontextrequiredforgeneratingnovelobjects.
B.6.Implementationdetails
WetrainandsampleallmodelswiththeEDM[22]diffusion
formulation. We use Stable Diffusion’s [42] public latent
VAE.Wetraintheencoderanddecoderjointlyfromscratch,
on8NVIDIAA100GPUs,usingglobalbatchsizeof256,
usingtheAdamW[25]optimizerwithconstantlearningrate
of10−4.Wesampleusing40denoisingstepsandclassifier-
free guidance scale of 4.0. Other details are the same as
inthetext-conditionedsettingandaredetailedinthemain
paperorinAppendixD.
C.AdditionalExperimentsandResults
C.1.Blending
LazyDiffusiongeneratesonlythemaskedregionsofthela-
tent image. To achieve the final desired results, these re-
Figure10. Frompartiallatentgenerationtoinpaintedimage. The
gions must be composited with the visible image regions
“zero-pad decoding” column is produced by decoding the incre-
anddecodedintoanimage. Initially, wenaivelyblendthe
mentalgenerationwithzeropadding,demonstratingtheobjectin
generated latent with the latent of the input image, as de- isolation.Toproducethedesiredcompositedimage,weblendthe
scribedinEq.(5)inthemainpaper. However,weobserve incremental generation with the latent input. This occasionally
thatpassingtheblendedlatentthroughthelatentdecoderD leadstovisibleseamsandlackofcolorharmonizationasseenin
occasionally results in poorly harmonized images, charac- the“vanilladecoder”column. Thisissuecanbesolvedusingthe
terizedbyfaintlyvisibleseamsbetweenthegeneratedand latentdecoderproposedbyZhuetal.[62]orwithPoissonblend-
visibleregions. Thisphenomenonwaspreviouslynotedby ing[36]. Werecommendzoomingintobetterviewtheseamsor
lackthereof.
Zhu et al. [63] when performing local editing with Stable
Diffusion [42]. It is conjectured that the latent encoding
losessubtlecolorinformation,hinderingimageharmoniza-
Finally,inFigs.13and14weprovideanon-curatedset
tion. Inresponse, Zhuetal. proposedanalternativelatent
ofresults,withmasksandtextpromptsproducedautomati-
decoder that additionally conditions on the masked input
callybythesegmentationandcaptioningmodels. Themain
image I ⊙(1−M) itself and is also significantly larger.
challenge we observe from these results is that the model
Specifically,theirdecoderrunsfor800ms,4.5×longerthan
partiallyignoresthetextwhenitconflictswiththeshapeof
the“vanilla”StableDiffusionlatentdecoder.
themask. Forexample,thehamburgerinFig.13isgener-
Inourexperiments,wefindthatsimplyperformingPois-
atedwithoutahat.
son blending [36] in pixel space achieves comparable re-
sults, while running only for 35ms on average. Therefore,
D.AdditionalDetails
weintroduceaPoissonblendingpost-processingsteptoour
pipeline.Wedemonstratetheharmonizationissueandcom-
parethetwoapproachesinFig.10. Evaluation. WecomputeFID[20]usingclean-fid[33]. For
CLIPScore[19],wereportthe“local”versionthattakesas
C.2.AdditionalResults inputacroparoundthegeneratedobjectandthelocaltext,
describingtheobject. Thisapproachwaspreviouslyadvo-
In Figs. 11 and 12, we extend Fig. 7 of the main paper
cated by Wang et al. [55] and is more suitable for image
and provide more qualitative samples comparing LazyDif-
inpaintingthanusingthefullimageandtextcaptionforthe
fusionwiththefourbaselines–RegenerateCrop,SD2-crop,
entireimage.
RegenerateImage and SDXL. We find that LazyDiffusion
ismostlycomparabletoRegenerateImageandSDXLeven Architecture. Asdescribedinthemainpaper, weinitialize
wheninpaintingobjectsthatrequirehighsemanticcontext, ourdecoderwithPixArt-α’spubliclyreleasedweights. Our
despiteusingacompressedcontextandrunningupto10× decoder has an additional linear layer, introduced in Ap-
faster. pendix B.2, that projects the concatenation of context and
”yob
emosdnaH“
”ecuttel“
”thgin
ta
ytiC“
”effariG“
”rewolf
yppoP“Regenerate Crop Regenerate Image LazyDiffusion
Input
SD2 PixArt-㌵ SDXL PixArt-㌵ (Ours)
Figure 11. Comparing inpainting results on objects that require modest context, similar to Fig. 7(Top). All models usually produce
reasonablygoodresults.Occasionally,SDXL[37]andSD2[42]donotgenerateanything–aresultoftheirusageofrandommasksrather
thanobject-levelmasks[55,57].
edam
esuoH“
nootraC“
,gnittis
nosreP“
”yppuP“
”adnaP“
”taC“
”knab
yggiP“
”sserD“
”dloG
fo
”elidocorC
”weiv
kcabRegenerate Crop Regenerate Image LazyDiffusion
Input
SD2 PixArt-㌵ SDXL PixArt-㌵ (Ours)
Figure12. Comparinginpaintingresultsonobjectsthathaveclosesemanticrelationshipwiththeobservedcanvas,similartoFig. 7(Bot-
tom).Approachesthatonlyprocessacropmaygenerateobjectsthatappearreasonableontheirownbutlackcoherencewithinthebroader
contextoftheimage. Incontrast,LazyDiffusionproducesresultscomparabletothoseproducesbymethodsregeneratingtheentireimage.
Occasionally,LazyDiffusiondoesnotfullyutilizethevisiblecontext. Forinstance,our“sushi”resultaccuratelydepictstheorangewrap
andsesameseedsontop,consistentwithothersushiintheroll,butitfeaturesadifferentfilling.
rolocretaW“
rekciW“
samtsirhC“
”sgab
nottoC”
”noracaM“
”nroC“
”ihsuS“
”yrrebwartS“
”tnalpgge
”losarap
”snoitarocedInput Generations
Figure13.ArandomsetofresultsproducedbyLazyDiffusion.Foreachinputweproducethreeoutputsfromdifferentrandomseeds.
htiw
rac
gnicar
A“
regrubmah
a“
.moorhsuM“
egnarO“
rettiB“
dna
egnaro
na
”afoS“
”eeffoc
ettaL“
”seip
yrreB“
elprup
a
htiw
”.weiv
poT
”rewolf
”nolem
”boj
tniap
eulb
”ti
fo
pot
no
tahInput Generations
Figure14.ArandomsetofresultsproducedbyLazyDiffusion.Foreachinputweproducethreeoutputsfromdifferentrandomseeds.
htiw
dalaS“
,sneerg
tnarbiv
gniylf
nosrep“
moordeB“
wolley
decilS“
,seotamot
yciuj
”kcuD“
a
ni
etik
a
”elbaT“
”puos
toH“
”srekcepdooW“
”roiretni
”seotatop
delbmurc
dna
”yks
eulb
”eseehc
atefnoisetokenstothedecoder’shiddendimensiond. Weini-
tialize this layer such that it outputs the noise tokens in its
input and ignores the context. This ensures that at initial-
ization,ifgivenafullmaskandthusoperatesonalltokens,
ourresultsareexactlyequivalenttoPixArt-α’s.
Data.Asdiscussedinthepaper,weadoptadataprocessing
pipeline similar to that of SmartBrush [57]. Specifically,
our masks are originally produced by an entity segmenta-
tion model [38] and are dilated to simulate the rough and
inaccurate masks created by users. First, with probability
of20%wereplacethesegmentationmaskwitharectangu-
larmaskcorrespondingtoaboundingbox. Regardless,we
dilate the mask by first performing Gaussian Blurring and
thresholdingtheoutput. ThesizeoftheGaussiankernelis
sampleduniformlyfrom[imagesize/15,imagesize/5]and
itsstandarddeviationalongXandYissampleduniformly
and independently from [3,17]. The threshold is sampled
uniformlyfrom{10−1,10−2,10−3,10−4}.