[
    {
        "title": "Matching the Statistical Query Lower Bound for k-sparse Parity Problems with Stochastic Gradient Descent",
        "authors": "Yiwen KouZixiang ChenQuanquan GuSham M. Kakade",
        "links": "http://arxiv.org/abs/2404.12376v1",
        "entry_id": "http://arxiv.org/abs/2404.12376v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12376v1",
        "summary": "The $k$-parity problem is a classical problem in computational complexity and\nalgorithmic theory, serving as a key benchmark for understanding computational\nclasses. In this paper, we solve the $k$-parity problem with stochastic\ngradient descent (SGD) on two-layer fully-connected neural networks. We\ndemonstrate that SGD can efficiently solve the $k$-sparse parity problem on a\n$d$-dimensional hypercube ($k\\le O(\\sqrt{d})$) with a sample complexity of\n$\\tilde{O}(d^{k-1})$ using $2^{\\Theta(k)}$ neurons, thus matching the\nestablished $\\Omega(d^{k})$ lower bounds of Statistical Query (SQ) models. Our\ntheoretical analysis begins by constructing a good neural network capable of\ncorrectly solving the $k$-parity problem. We then demonstrate how a trained\nneural network with SGD can effectively approximate this good network, solving\nthe $k$-parity problem with small statistical errors. Our theoretical results\nand findings are supported by empirical evidence, showcasing the efficiency and\nefficacy of our approach.",
        "updated": "2024-04-18 17:57:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12376v1"
    },
    {
        "title": "Improving the interpretability of GNN predictions through conformal-based graph sparsification",
        "authors": "Pablo Sanchez-MartinKinaan Aamir KhanIsabel Valera",
        "links": "http://arxiv.org/abs/2404.12356v1",
        "entry_id": "http://arxiv.org/abs/2404.12356v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12356v1",
        "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nsolving graph classification tasks. However, most GNN architectures aggregate\ninformation from all nodes and edges in a graph, regardless of their relevance\nto the task at hand, thus hindering the interpretability of their predictions.\nIn contrast to prior work, in this paper we propose a GNN \\emph{training}\napproach that jointly i) finds the most predictive subgraph by removing edges\nand/or nodes -- -\\emph{without making assumptions about the subgraph structure}\n-- while ii) optimizing the performance of the graph classification task. To\nthat end, we rely on reinforcement learning to solve the resulting bi-level\noptimization with a reward function based on conformal predictions to account\nfor the current in-training uncertainty of the classifier. Our empirical\nresults on nine different graph classification datasets show that our method\ncompetes in performance with baselines while relying on significantly sparser\nsubgraphs, leading to more interpretable GNN-based predictions.",
        "updated": "2024-04-18 17:34:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12356v1"
    },
    {
        "title": "A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to Functional Conditional Moment Equations",
        "authors": "Yuchen ZhuYufeng ZhangZhaoran WangZhuoran YangXiaohong Chen",
        "links": "http://arxiv.org/abs/2404.12312v1",
        "entry_id": "http://arxiv.org/abs/2404.12312v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12312v1",
        "summary": "We study minimax optimization problems defined over infinite-dimensional\nfunction classes. In particular, we restrict the functions to the class of\noverparameterized two-layer neural networks and study (i) the convergence of\nthe gradient descent-ascent algorithm and (ii) the representation learning of\nthe neural network. As an initial step, we consider the minimax optimization\nproblem stemming from estimating a functional equation defined by conditional\nexpectations via adversarial estimation, where the objective function is\nquadratic in the functional space. For this problem, we establish convergence\nunder the mean-field regime by considering the continuous-time and\ninfinite-width limit of the optimization dynamics. Under this regime, gradient\ndescent-ascent corresponds to a Wasserstein gradient flow over the space of\nprobability measures defined over the space of neural network parameters. We\nprove that the Wasserstein gradient flow converges globally to a stationary\npoint of the minimax objective at a $\\mathcal{O}(T^{-1} + \\alpha^{-1} ) $\nsublinear rate, and additionally finds the solution to the functional equation\nwhen the regularizer of the minimax objective is strongly convex. Here $T$\ndenotes the time and $\\alpha$ is a scaling parameter of the neural network. In\nterms of representation learning, our results show that the feature\nrepresentation induced by the neural networks is allowed to deviate from the\ninitial one by the magnitude of $\\mathcal{O}(\\alpha^{-1})$, measured in terms\nof the Wasserstein distance. Finally, we apply our general results to concrete\nexamples including policy evaluation, nonparametric instrumental variable\nregression, and asset pricing.",
        "updated": "2024-04-18 16:46:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12312v1"
    },
    {
        "title": "floZ: Evidence estimation from posterior samples with normalizing flows",
        "authors": "Rahul SrinivasanMarco CrisostomiRoberto TrottaEnrico BarausseMatteo Breschi",
        "links": "http://arxiv.org/abs/2404.12294v1",
        "entry_id": "http://arxiv.org/abs/2404.12294v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12294v1",
        "summary": "We propose a novel method (floZ), based on normalizing flows, for estimating\nthe Bayesian evidence (and its numerical uncertainty) from a set of samples\ndrawn from the unnormalized posterior distribution. We validate it on\ndistributions whose evidence is known analytically, up to 15 parameter space\ndimensions, and compare with two state-of-the-art techniques for estimating the\nevidence: nested sampling (which computes the evidence as its main target) and\na k-nearest-neighbors technique that produces evidence estimates from posterior\nsamples. Provided representative samples from the target posterior are\navailable, our method is more robust to posterior distributions with sharp\nfeatures, especially in higher dimensions. It has wide applicability, e.g., to\nestimate the evidence from variational inference, Markov-chain Monte Carlo\nsamples, or any other method that delivers samples from the unnormalized\nposterior density.",
        "updated": "2024-04-18 16:16:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12294v1"
    },
    {
        "title": "Debiased Distribution Compression",
        "authors": "Lingxiao LiRaaz DwivediLester Mackey",
        "links": "http://arxiv.org/abs/2404.12290v1",
        "entry_id": "http://arxiv.org/abs/2404.12290v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12290v1",
        "summary": "Modern compression methods can summarize a target distribution $\\mathbb{P}$\nmore succinctly than i.i.d. sampling but require access to a low-bias input\nsequence like a Markov chain converging quickly to $\\mathbb{P}$. We introduce a\nnew suite of compression methods suitable for compression with biased input\nsequences. Given $n$ points targeting the wrong distribution and quadratic\ntime, Stein Kernel Thinning (SKT) returns $\\sqrt{n}$ equal-weighted points with\n$\\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\\mathbb {P}$. For\nlarger-scale compression tasks, Low-rank SKT achieves the same feat in\nsub-quadratic time using an adaptive low-rank debiasing procedure that may be\nof independent interest. For downstream tasks that support simplex or\nconstant-preserving weights, Stein Recombination and Stein Cholesky achieve\neven greater parsimony, matching the guarantees of SKT with as few as\n$\\operatorname{poly-log}(n)$ weighted points. Underlying these advances are new\nguarantees for the quality of simplex-weighted coresets, the spectral decay of\nkernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In\nour experiments, our techniques provide succinct and accurate posterior\nsummaries while overcoming biases due to burn-in, approximate Markov chain\nMonte Carlo, and tempering.",
        "updated": "2024-04-18 16:11:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12290v1"
    }
]