[
    {
        "title": "On the Content Bias in Fréchet Video Distance",
        "authors": "Songwei GeAniruddha MahapatraGaurav ParmarJun-Yan ZhuJia-Bin Huang",
        "links": "http://arxiv.org/abs/2404.12391v1",
        "entry_id": "http://arxiv.org/abs/2404.12391v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12391v1",
        "summary": "Fr\\'echet Video Distance (FVD), a prominent metric for evaluating video\ngeneration models, is known to conflict with human perception occasionally. In\nthis paper, we aim to explore the extent of FVD's bias toward per-frame quality\nover temporal realism and identify its sources. We first quantify the FVD's\nsensitivity to the temporal axis by decoupling the frame and motion quality and\nfind that the FVD increases only slightly with large temporal corruption. We\nthen analyze the generated videos and show that via careful sampling from a\nlarge set of generated videos that do not contain motions, one can drastically\ndecrease FVD without improving the temporal quality. Both studies suggest FVD's\nbias towards the quality of individual frames. We further observe that the bias\ncan be attributed to the features extracted from a supervised video classifier\ntrained on the content-biased dataset. We show that FVD with features extracted\nfrom the recent large-scale self-supervised video models is less biased toward\nimage quality. Finally, we revisit a few real-world examples to validate our\nhypothesis.",
        "updated": "2024-04-18 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12391v1"
    },
    {
        "title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
        "authors": "Xingyu FuYushi HuBangzheng LiYu FengHaoyu WangXudong LinDan RothNoah A. SmithWei-Chiu MaRanjay Krishna",
        "links": "http://arxiv.org/abs/2404.12390v1",
        "entry_id": "http://arxiv.org/abs/2404.12390v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12390v1",
        "summary": "We introduce Blink, a new benchmark for multimodal language models (LLMs)\nthat focuses on core visual perception abilities not found in other\nevaluations. Most of the Blink tasks can be solved by humans \"within a blink\"\n(e.g., relative depth estimation, visual correspondence, forensics detection,\nand multi-view reasoning). However, we find these perception-demanding tasks\ncast significant challenges for current multimodal LLMs because they resist\nmediation through natural language. Blink reformats 14 classic computer vision\ntasks into 3,807 multiple-choice questions, paired with single or multiple\nimages and visual prompting. While humans get 95.70% accuracy on average, Blink\nis surprisingly challenging for existing multimodal LLMs: even the\nbest-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only\n13.17% and 7.63% higher than random guessing, indicating that such perception\nabilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also\nhighlights that specialist CV models could solve these problems much better,\nsuggesting potential pathways for future improvements. We believe Blink will\nstimulate the community to help multimodal LLMs catch up with human-level\nvisual perception.",
        "updated": "2024-04-18 17:59:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12390v1"
    },
    {
        "title": "VideoGigaGAN: Towards Detail-rich Video Super-Resolution",
        "authors": "Yiran XuTaesung ParkRichard ZhangYang ZhouEli ShechtmanFeng LiuJia-Bin HuangDifan Liu",
        "links": "http://arxiv.org/abs/2404.12388v1",
        "entry_id": "http://arxiv.org/abs/2404.12388v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12388v1",
        "summary": "Video super-resolution (VSR) approaches have shown impressive temporal\nconsistency in upsampled videos. However, these approaches tend to generate\nblurrier results than their image counterparts as they are limited in their\ngenerative capability. This raises a fundamental question: can we extend the\nsuccess of a generative image upsampler to the VSR task while preserving the\ntemporal consistency? We introduce VideoGigaGAN, a new generative VSR model\nthat can produce videos with high-frequency details and temporal consistency.\nVideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN. Simply\ninflating GigaGAN to a video model by adding temporal modules produces severe\ntemporal flickering. We identify several key issues and propose techniques that\nsignificantly improve the temporal consistency of upsampled videos. Our\nexperiments show that, unlike previous VSR methods, VideoGigaGAN generates\ntemporally consistent videos with more fine-grained appearance details. We\nvalidate the effectiveness of VideoGigaGAN by comparing it with\nstate-of-the-art VSR models on public datasets and showcasing video results\nwith $8\\times$ super-resolution.",
        "updated": "2024-04-18 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12388v1"
    },
    {
        "title": "Moving Object Segmentation: All You Need Is SAM (and Flow)",
        "authors": "Junyu XieCharig YangWeidi XieAndrew Zisserman",
        "links": "http://arxiv.org/abs/2404.12389v1",
        "entry_id": "http://arxiv.org/abs/2404.12389v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12389v1",
        "summary": "The objective of this paper is motion segmentation -- discovering and\nsegmenting the moving objects in a video. This is a much studied area with\nnumerous careful,and sometimes complex, approaches and training schemes\nincluding: self-supervised learning, learning from synthetic datasets,\nobject-centric representations, amodal representations, and many more. Our\ninterest in this paper is to determine if the Segment Anything model (SAM) can\ncontribute to this task. We investigate two models for combining SAM with\noptical flow that harness the segmentation power of SAM with the ability of\nflow to discover and group moving objects. In the first model, we adapt SAM to\ntake optical flow, rather than RGB, as an input. In the second, SAM takes RGB\nas an input, and flow is used as a segmentation prompt. These surprisingly\nsimple methods, without any further modifications, outperform all previous\napproaches by a considerable margin in both single and multi-object benchmarks.\nWe also extend these frame-level segmentations to sequence-level segmentations\nthat maintain object identity. Again, this simple model outperforms previous\nmethods on multiple video object segmentation benchmarks.",
        "updated": "2024-04-18 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12389v1"
    },
    {
        "title": "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models",
        "authors": "Aitor OrmazabalChe ZhengCyprien de Masson d'AutumeDani YogatamaDeyu FuDonovan OngEric ChenEugenie LamprechtHai PhamIsaac OngKaloyan AleksievLei LiMatthew HendersonMax BainMikel ArtetxeNishant RelanPiotr PadlewskiQi LiuRen ChenSamuel PhuaYazheng YangYi TayYuqi WangZhongkai ZhuZhihui Xie",
        "links": "http://arxiv.org/abs/2404.12387v1",
        "entry_id": "http://arxiv.org/abs/2404.12387v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12387v1",
        "summary": "We introduce Reka Core, Flash, and Edge, a series of powerful multimodal\nlanguage models trained from scratch by Reka. Reka models are able to process\nand reason with text, images, video, and audio inputs. This technical report\ndiscusses details of training some of these models and provides comprehensive\nevaluation results. We show that Reka Edge and Reka Flash are not only\nstate-of-the-art but also outperform many much larger models, delivering\noutsized values for their respective compute class. Meanwhile, our most capable\nand largest model, Reka Core, approaches the best frontier models on both\nautomatic evaluations and blind human evaluations. On image question answering\nbenchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.\nMeanwhile, on multimodal chat, Core ranks as the second most preferred model\nunder a blind third-party human evaluation setup, outperforming other models\nsuch as Claude 3 Opus. On text benchmarks, Core not only performs competitively\nto other frontier models on a set of well-established benchmarks (e.g. MMLU,\nGSM8K) but also outperforms GPT4-0613 on human evaluation. On video question\nanswering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped\nin production at http://chat.reka.ai . A showcase of non cherry picked\nqualitative examples can also be found at http://showcase.reka.ai .",
        "updated": "2024-04-18 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12387v1"
    }
]