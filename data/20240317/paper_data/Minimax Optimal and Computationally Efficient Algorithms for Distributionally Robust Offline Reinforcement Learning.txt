Minimax Optimal and Computationally Efficient Algorithms for
Distributionally Robust Offline Reinforcement Learning
Zhishuai Liu∗ and Pan Xu†
Abstract
Distributionally robust offline reinforcement learning (RL), which seeks robust policy train-
ing against environment perturbation by modeling dynamics uncertainty, calls for function ap-
proximations when facing large state-action spaces. However, the consideration of dynamics
uncertainty introduces essential nonlinearity and computational burden, posing unique chal-
lenges for analyzing and practically employing function approximation. Focusing on a basic
setting where the nominal model and perturbed models are linearly parameterized,we propose
minimaxoptimalandcomputationallyefficientalgorithmsrealizingfunctionapproximationand
initiate the study on instance-dependent suboptimality analysis in the context of robust offline
RL. Our results uncover that function approximationin robust offline RL is essentially distinct
from and probably harder than that in standard offline RL. Our algorithms and theoretical re-
sults crucially depend on a variety of new techniques, involving a novelfunction approximation
mechanism incorporating variance information, a new procedure of suboptimality and estima-
tion uncertainty decomposition, a quantification of the robust value function shrinkage, and a
meticulously designed family of hard instances, which might be of independent interest.
1 Introduction
Offline reinforcement learning (RL) (Lange et al., 2012; Levine et al., 2020), which aims to learn
an optimal policy achieving maximum expected cumulative reward from a pre-collected dataset,
plays an important role in critical domains where online exploration is infeasible due to high cost
or ethical issues, such as precision medicine (Wang et al., 2018; Gottesman et al., 2019) and au-
tonomous driving (Pan et al., 2017; Sun et al., 2020). The foundational assumption of offline RL
(Levine et al., 2020; Jin et al., 2021; Xie et al., 2021) is that the offline dataset is collected from the
same environment where learned policies are intended to be deployed. However, this assumption
can be violated in practice due to temporal changes in dynamics. In such cases, standard offline
RL could face catastrophic failures (Farebrother et al., 2018; Packer et al., 2018; Zhao et al., 2020).
To address this issue, the robust offline RL (Morimoto and Doya, 2005; Nilim and El Ghaoui, 2005)
focuses on robust policy training against the environment perturbation, which serves as a promising
solution. Existing empirical successes of robust offline RL rely heavily on expressive function ap-
proximations (Pinto et al., 2017; Pattanaik et al., 2017; Mandlekar et al., 2017; Tessler et al., 2019;
Zhang et al.,2020;Kuang et al.,2022),astheomnipresenceofapplicationsfeaturinglargestateand
action spaces necessitates powerful function representations to enhance generalization capability of
decision-making in RL.
∗DukeUniversity;email: zhishuai.liu@duke.edu
†DukeUniversity;email: pan.xu@duke.edu
1
4202
raM
41
]GL.sc[
1v12690.3042:viXraTo theoretically understand robust offline RL with function approximation, the distributionally
robust Markov decision process (DRMDP) (Satia and Lave Jr, 1973; Nilim and El Ghaoui, 2005;
Iyengar, 2005) provides an established framework. In stark contrast to the standard MDP, DR-
MDP specifically tackles the model uncertainty by forming an uncertainty set around the nominal
model, and takes a max-min formulation aiming to maximize the value function corresponding
to a policy, uniformly across all perturbed models in the uncertainty set (Xu and Mannor, 2006;
Wiesemann et al., 2013; Zhang et al., 2021; Yang et al., 2022; Panaganti et al., 2022; Shi and Chi,
2022; Yang et al., 2023b; Shen et al., 2023). The core of DRMDPs lies in achieving an amenable
combination of uncertainty set design and corresponding techniques to solve the inner optimization
over the uncertainty set. However, this consideration of model uncertainty introduces fundamental
challengestofunctionapproximationintermsofcomputationalandstatisticalefficiency,particularly
giventheneedtomaximallyexploitessentialinformationintheofflinedataset. Forinstance,incases
wherethestateandactionspacesarelarge,thecommonlyused(s,a)-rectangularuncertaintysetcan
make the inner optimization computationally intractable for function approximation (Zhou et al.,
2023). Additionally, the distribution shifts, arising from the mismatch between the behavior policy
and the target policy, as well as the mismatch between the nominal model and perturbed models,
complicate the statistical analysis (Shi and Chi, 2022; Blanchet et al., 2023). Several recent works
attempt to conquer these challenges. Panaganti et al. (2022) studied the (s,a)-rectangularity, and
their algorithm may suffer from the above mentioned computational issue; Blanchet et al. (2023)
proposed a novel double pessimism principle, while their algorithm requires strong oracles, which
is not practically implementable. Meanwhile, a line of works study function approximation in the
online setting (Tamar et al., 2014; Roy et al., 2017; Wang and Zou, 2021; Badrinath and Kalathil,
2021; Liu and Xu, 2024) or with a simulator (Zhou et al., 2023), which are not applicable to offline
RL. Thus, the following question remains open:
Is it possible to design a computationally efficient and minimax optimal algorithm for
robust offline RL with function approximation?
Toanswertheabovequestion,wefocusonabasicsettingofd-rectangular linear DRMDP,wherethe
nominal model is a standard linear MDP, and all perturbed models are parameterized in a linearly
structured uncertainty set. We provide the first instance-dependent suboptimality analysis in the
DRMDP literature with function approximation, which offers insights into the problem’s intrinsic
characteristics and challenges. Concretely, our contributions are summarized as follows.
• We propose a computationally efficient algorithm, Distributionally Robust Pessimistic Value Iter-
ation (DRPVI), based on the pessimism principle (Jin et al., 2021; Xie et al., 2021; Shi and Chi,
2022) with a new function approximation mechanism explicitly devised for d-rectangular linear
DRMDPs. We show that DRPVI achieves the following instance-dependent upper bound on the
suboptimality gap:
H d
β 1
·
P∈s Uu ρp
(P0)
h=1Eπ⋆,P (cid:20)i=1kφ i(s h,a h)1 i kΛ− h1 |s 1 = s (cid:21)1,
X X
This bound resembles those established in offline RL within standard linear MDPs (Jin et al.,
2021; Zanette et al., 2021; Xiong et al., 2022). However, there are two significant differences in
1Here,disthefeaturedimension,H isthehorizonlength,β =O˜(√dH)isatunningparameterinDRPVI, ρ(P0)
1
is the uncertainty set with radius ρ, π⋆ is the optimal robust policy, φ(, ): Rd is the instance-depU endent
· · S×A→
feature vector, and Λ is thecovariance matrix defined in (4.3).
h
2our results. First, our bound depends on the supremum over the uncertainty set of transition
kernels instead of one single transition kernel. Second, our result relies on a diagonal-based nor-
malization, instead of the Mahalanobis norm of the feature vector, kφ(s h,a h) kΛ− h1. See Table 1
for a clearer comparison. These two distinctions are unique to DRMDPs with function approx-
imation, which we discuss in more details in Section 4. Moreover, our analysis provides a novel
pipeline for studying instance-dependent upper bounds of computationally efficient algorithms
under d-rectangular linear DRMDPs.
• We improve DRPVI by incorporating variance information into the new function approximation
mechanism, resulting in the VA-DRPVI algorithm, which achieves a smaller upper bound:
H d
β 2 · P∈s Uu ρp
(P0)
h=1Eπ⋆,P (cid:20)i=1kφ i(s h,a h)1 i kΣ h⋆−1 |s 1 = s (cid:21)2.
X X
This improves the result of DRPVI due to the fact that Σ⋆−1 H2Λ−1 by definition (Yin et al.,
h (cid:22) h
2022; Xiong et al., 2022). Furthermore, when the uncertainty level ρ = O(1), we show that the
robustvaluefunctionattainsaRange Shrinkageproperty, leadingtoanimprovement intheupper
bound by an order of H. This explicit improvement is new in variance-aware algorithms, and is
unique to DRMDPs. Our instance-dependent bound further implies a worst-case regret bound
O˜(d3/2H/(c √K))3 under an additional robust partial coverage assumption (Blanchet et al.,
·
2023), which achieves the best result in the literature of DRMDPs with function approximation.
• We further establish an information-theoretic lower bound. We prove that the upper bound
of VA-DRPVI matches the information-theoretic lower bound up to β , which implies that
2
VA-DRPVI is minimax optimal in the sense of information theory. Importantly, both DRPVI
and VA-DRPVI are computationally efficient and do not suffer from the high computational bur-
den, asdiscussedabove insettings withthe (s,a)-rectangular uncertainty set,duetoadecoupling
property of thed-rectangular uncertainty set(seeRemark 4.1 formore details). Thus, we confirm
that, for robust offline RL with function approximation, both the computational efficiency and
minimax optimality are achievable under the setting of d-rectangular linear DRMDPs.
Ouralgorithmdesignandtheoreticalanalysisdrawinspirationfromtwocrucialideasproposedin
standardlinearMDPs: thereference-advantagedecomposition(Xiong et al.,2022)andthevariance-
weighted ridge regression (Zhou et al., 2021a). However, the unique challenges in DRMDPs necessi-
tate novel treatments that go far beyond a combination of existing techniques. Specifically, existing
analysis of standard linear MDPs highly relies on the linear dependency of the Bellman equation
on the (nominal) transition kernel. This linear dependency is disrupted by the consideration of
model uncertainty, which induces essential nonlinearity that significantly complicates the statistical
analysis of estimation error. To obtain our instance-dependent upper bounds, we establish a new
theoretical analysis pipeline. This pipeline starts with a nontrivial decomposition of the suboptimal-
ity, and employs a new uncertainty decomposition that transforms the estimation uncertainty over
all perturbed models to estimation uncertainty under the nominal model.
The information-theoretic lower bound in our paper is the first of its kind in the linear DRMDP
setting, which could be of independent interest to the community. Previous lower bounds, which
are based on the commonly used Assoud’s method and established under the standard linear MDP,
2β = O˜(√d) is a tunning parameter in VA-DRPVI, Σ⋆ is the variance-weighted covariance matrix defined in
2 h
(5.3).
3c is theparameter in the robust partial coverage assumption defined in (4.7).
3Table 1: Summary of instance-dependent results in offline RL with linear function approximation.
Λ and Σ⋆ are the empirical covariance matrix defined in (4.3) and (5.3) respectively. Note that
h h
π⋆ means the optimal policy in standard MDPs and the optimal robust policy in DRMDPs. The
definition of Σ⋆ also depends on the corresponding definition of π⋆.
h
Algorithm Setting Instance-dependent upper bound on the suboptimality gap
PEVI (Jin et al., 2021) MDP dH
·
H h=1Eπ⋆,P
kφ(s h,a h) kΛ− h1 |s 1 =s
LinPEVI-ADV
(Xiong et al., 2022) MDP √dH ·P H h=1Eπ⋆,P(cid:2) kφ(s h,a h) kΛ− h1 |s 1 =s(cid:3)
(XL ii on nP gE eV tI a-A l.,D 2V 0+ 22) MDP √d · PH h=1Eπ⋆,P k(cid:2) φ(s h,a h) kΣ h⋆−1 |s 1 =s(cid:3)
DRPVI (ours) DRMDP √dH ·sup P∈Uρ(PP 0) H h=1Eπ(cid:2)⋆,P d i=1kφ i(s h,a h)1 i k(cid:3) Λ− h1 |s 1 =s
VA-DRPVI (ours) DRMDP √d ·sup P∈Uρ(P0) PH h=1Eπ⋆,P (cid:2)Pd i=1kφ i(s h,a h)1 i kΣ h⋆−1 |s 1 =s (cid:3)
P (cid:2)P (cid:3)
do not consider model uncertainty. In particular, one prerequisite for applying Assoud’s method
is switching the initial minimax objective to a minimax risk in terms of Hamming distance. The
intertwining of this prerequisite with the nonlinearity induced by the model uncertainty makes the
analysis significantly more challenging. To this end, we construct a novel family of hard instances,
carefully designed to (1) mitigate the nonlinearity caused by the model uncertainty, (2) fulfil the
prerequisite for Assoud’s method, and (3) be concise enough to admit matrix analysis.
Notations: We denote ∆( ) as the set of probability measures over some set . For any num-
S S
ber H Z , we denote [H] as the set of 1,2, ,H . For any function V : R, we
+
∈ { ··· } S →
denote [P hV](s,a) = E s′∼Ph(·|s,a)[V(s′)] as the expectation of V with respect to the transition
kernel P , [Var V](s,a) = [P V2](s,a) ([P V](s,a))2 as the variance of V, and [V V](s,a) =
h h h h h
−
max 1,[Var V](s,a) as the truncated variance of V. For a vector x, we denote x as its j-th entry.
h j
{ }
And we denote [x ] as a vector with the i-th entry being x . For a matrix A, denote λ (A) as the
i i∈[d] i i
i-theigenvalue ofA. Fortwomatrices AandB,wedenoteA B asthefactthatB Aisapositive
(cid:22) −
semidefinite matrix, and A B as the fact that A B is a positive semidefinite matrix. For any
(cid:23) −
function f : R, we denote f = sup f(s). Given any two probability distributions P and
S → k k∞ s∈S
Q on , the total variation divergence of P and Q is defined as D(P Q) = 1/2 P(s) Q(s)ds.
S || S| − |
R
2 Related Work
Offline Linear MDPs. Our work focuses on the offline linear MDP setting where the nominal
transition kernel, from which the offline dataset is collected, admits the linear MDP structure.
Numerous works have studied the provable efficiency and statistical limits of algorithms under
this setting (Jin et al., 2021; Zanette et al., 2021; Xie et al., 2021; Yin et al., 2022; Xiong et al.,
2022). The most relevant study to ours is the recent work of Xiong et al. (2022), which established
the minimax optimality of offline linear MDPs. At the core of their analysis is an advantage-
reference technique designed for offline RL under linear function approximation, together with a
variance aware pessimism-based algorithm. However, the offline linear MDP setting still remains
understudied in the context of DRMDPs.
4DRMDPs. The DRMDP framework has been extensively studied under different settings. The
works ofXu and Mannor (2006);Wiesemann et al.(2013);Yu and Xu(2015);Mannor et al.(2016);
Goyal and Grand-Clement (2023) assumed precise knowledge of the environment and formulated
the DRMDP as classic planning problems. The works of Zhou et al. (2021b); Yang et al. (2022);
Panaganti and Kalathil (2022); Xu et al. (2023); Shi et al. (2023); Yang et al. (2023a) assumed
access to a generative model and studied the sample complexities of DRMDPs. The works of
Panaganti et al. (2022); Shi and Chi (2022); Blanchet et al. (2023) studied the offline setting as-
suming access to only an offline dataset, and established sample complexities under data coverage
or concentrability assumptions. Wang and Zou (2021); Badrinath and Kalathil (2021); Dong et al.
(2022);Liang et al.(2023);Liu and Xu(2024)studiedtheonlinesettingwheretheagentcanactively
interact with the nominal environment to learn the optimal robust policy.
DRMDPs with linear function approximation. Tamar et al.(2014);Badrinath and Kalathil
(2021) proposed to use linear function approximation to solve DRMDPs with large state and action
spaces and established asymptotic convergence guarantees. Zhou et al. (2023) studied the natural
Actor-Critic with function approximation, assuming access to a simulator. Their function approxi-
mation mechanisms depend on two novel uncertainty sets, one based on double sampling and the
other on an integral probability metric. Ma et al. (2022) first combined the linear MDP with the
d-rectangular uncertainty set (Goyal and Grand-Clement, 2023), and proposed the setting dubbed
as the d-rectangular linear DRMDP, which naturally admits linear representations of the robust
Q-functions. Blanchet et al. (2023) studied the offline d-rectangular linear DRMDP setting, for
which the provable efficiency is established under a double pessimism principle. Liu and Xu (2024)
then studied the online d-rectangular linear DRMDP setting and pointed out that the intrinsic
nonlinearity of DRMDPs might pose additional challenges for linear function approximation. We
note that a similar instance-dependent upper bound is established in Theorem 4.1 of Liu and Xu
(2024).
3 Problem Formulation
Inthissection,weprovidetheframeworkofd-rectangularlinearDRMDP,introducesomeproperties
essential for the algorithm design and theoretical analysis, formulate the offline dataset collection
process and describe the goal of the robust offline RL.
Standard MDPs. We start with the standard MDP, which constitutes the basic of DRMDPs.
A finite horizon Markov decision process is denoted by MDP( , ,H,P,r), where and are the
S A S A
state and action spaces, H Z is the number of steps, P = P H denotes the set of probability
∈ + { h }h=1
transition kernels, r = r H denotes the reward functions. For any (h,s,a) [H] , we
{ h }h=1 ∈ ×S ×A
denote P ( s,a) asthetransition kerneland r : [0,1] as thedeterministic reward function
h h
·| S×A →
at step h. A sequence of deterministic decision rules is denoted as π = π H , where π :
{ h }h=1 h S → A
is the policy at step h. Given any policy π and transition P, for all (s,a,h) [H], the
corresponding value function Vπ,P (s) := Eπ,P H r (s ,a ) s = s and Q-fun∈ ctS io× n QAπ×,P (s,a) :=
h t=h t t t h h
Eπ,P H r (s ,a ) s = s,a = a characterize the expected cumulative rewards starting from
t=h t t t h h (cid:2)P (cid:12) (cid:3)
step h, and both of which are bounded in [0,H]. (cid:12)
(cid:2)P (cid:12) (cid:3)
(cid:12)
5Distributionally robust MDPs. A finite horizon distributionally robust Markov decision pro-
cessisdenotedbyDRMDP( , ,H, ρ(P0),r),whereP0 = P0 H isthesetofnominaltransition
kernels, ρ(P0)=
Sρ (PA 0)isU theuncertaintyset,usua{ llyh d} eh fi= n1
edasaballcenteredaroundP0
U h∈[H]Uh h
with radius (uncertainty level) ρ 0 based on some probability divergence measures (Iyengar, 2005;
N ≥
Yang et al., 2022; Xu et al., 2023). To account for the model uncertainty, the robust value function
Vπ,ρ : Rconsidersalltransitionkernelswithin ρ(P0)asfollowsVπ,ρ (s) = inf Vπ,P (s),
h S → U h P∈Uρ(P0) h
(h,s) [H] . In other words, the robust value function is the value function under the worst
∀ ∈ ×S
possible transition kernel within the uncertainty set. Similarly, the robust Q-function is defined as
π,ρ π,P
Q (s,a) = inf Q (s,a), for any (h,s,a) [H] . Further, we define the optimal
h P∈Uρ(P0) h ∈ ×S ×A
robust value function and optimal robust Q-function as
⋆,ρ π,ρ ⋆,ρ π,ρ
V (s) = supV (s), Q (s,a) = supQ (s,a), (h,s,a) [H] .
h h h h ∀ ∈ ×S ×A
π∈Π π∈Π
where Π is the set of all policies. The optimal robust policy π⋆ = π⋆ H is defined as the policy
that achieves the optimal robust value function as π⋆ = argsup
{ Vh π},ρh (= s1
), (h,s) [H] .
π∈Π h ∀ ∈ ×S
d-rectangular linear DRMDPs. A d-rectangular linear DRMDP is a DRMDP where the nom-
inal environment is a linear MDP (Jin et al., 2020) and the uncertainty set ρ (P0) is defined based
Uh h
on the linear structure of the nominal transition kernel P0.
h
Assumption3.1. (LinearMDP)Givenafeaturemappingφ : Rdsatisfying d φ (s,a) =
S×A → i=1 i
1, φ (s,a) 0, for any (i,s,a) [d] , we assume the reward function and nominal transition
i
≥ ∈ ×S×A P
kernels satisfy r (s,a) = φ(s,a),θ , and P0( s,a) = φ(s,a),µ0() , (h,s,a) [H]
h h h i h ·| h h · i ∀ ∈ × S × A
where θ √d, and µ H are unknown probability measures over .
k h k2 ≤ { h }h=1 S
With notations in Assumption 3.1, we define the factor uncertainty sets as ρ (µ0 ) = µ :
Uh,i h,i
µ ∆( ),D(µ µ0 ) ρ , (h,i) [H] [d], where D( ) is specified as the total variation (TV)
∈ S || h,i ≤ ∀ ∈ × ·||· (cid:8)
divergence in this work. The uncertainty set is defined as ρ (P0)= ρ (s,a;µ0), where
(cid:9) Uh h (s,a)∈S×AUh h
ρ (s,a;µ0) = d φ (s,a)µ () : µ () ρ (µ0 ), i [d] . A notable feature of this design
Uh h { i=1 i h,i · h,i · ∈ Uh,i h,i ∀ ∈ } N
is the factor uncertainty sets ρ (µ0 ) H,d are decoupled from the state-action pair (s,a) and
P {Uh,i h,i }h,i=1
also independent with each other. As demonstrated later, this decoupling property results in a
computationally efficient regime for function approximation.
RobustBellman equation. Underthesettingofd-rectangularlinearDRMDPs,therobustvalue
function and the robust Q-function satisfy the robust Bellman equations (Liu and Xu, 2024)
Qπ,ρ (s,a) = r (s,a)+ inf [P Vπ,ρ ](s,a), (3.1a)
h h Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1
Vπ,ρ (s) = E Qπ,ρ (s,a) , (3.1b)
h a∼πh(·|s) h
and the optimal robust policy π⋆ is determin(cid:2) istic. Thus(cid:3) , we can safely restrict the policy class Π to
the deterministic one. This leads to the robust Bellman optimality equations:
Q⋆,ρ (s,a) = r (s,a)+ inf [P V⋆,ρ ](s,a), (3.2a)
h h Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1
V⋆,ρ (s)= maxQ⋆(s,a). (3.2b)
h h
a∈A
6Offline Dataset and Goal. Let denote an offline dataset consisting of K i.i.d trajectories
D
generated from the nominal environment MDP( , ,H,P0,r) by a behavior policy πb = πb H .
S A { h}h=1
In concrete, for each τ [K], the trajectory (sτ,aτ,rτ) H satisfies that aτ πb( sτ), rτ =
∈ { h h h }h=1 h ∼ h ·| h h
r (sτ,aτ), and sτ P0( sτ,aτ) for any h [H].
h h h h+1 ∼ h ·| h h ∈
Thegoaloftherobustoffline RListolearntheoptimalrobustpolicyπ⋆ usingtheoffline dataset
. To evaluate the performance of an estimated policy πˆ, we define the suboptimality gap between
πD ˆ and the optimal robust policy π⋆ as SubOpt(πˆ,s ,ρ) := V⋆,ρ (s ) Vπˆ,ρ (s ). Then an agent aims
1 1 1 − 1 1
to learn a robust policy πˆ that minimizes the suboptimality gap SubOpt(πˆ,s,ρ), for any s .
∈ S
4 Warmup: Robust Pessimistic Value Iteration
Inthissection,wefirstproposeasimplealgorithminAlgorithm 1forrobustofflineRLwithfunction
approximationasawarmstart,andprovideaninstance-dependentupperboundonitssuboptimality
gap in Theorem 4.3.
The optimal robust Bellman equation (3.2) implies that the optimal robust policy π⋆ is greedy
⋆,ρ
withrespecttotheoptimalrobustQ-function. Therefore, itsufficestoestimateQ toapproximate
h
π⋆. Tothisend,weestimatetheoptimalrobustQ-functionandthetheoptimalrobustvaluefunction
by iteratively performing an empirical version of the optimal robust Bellman equation similar to
(3.2). Inconcrete,giventheestimatorsatsteph+1,denotedbyQ (s,a)andV (s),Liu and Xu
h+1 h+1
(2024) show that applying one step backward induction according to (3.2) leads to
b b
Q (s,a) := r (s,a)+ inf P V (s,a) = φ(s,a),θ +νρ , (4.1)
h h Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1 h h
(cid:2) (cid:3) (cid:10) (cid:11)
b
where V h+1(s) = max a∈AQ h+1(s,a), ν hρ
,i
:= max
α∈[0,H]
{z h,i(α)
−
ρ(α
−
min s′[V h+1(s′)] α)
}
and
z h,i(α) := Eµ0 h,i[V h+1(s′)] α, for any i [d]. To obtain the estimated Q h(s,a), it suffices to estimate
vectorsb z (α) = [z (α),...b ,z (α)]∈ and νρ as follows. b
h h,1 h,d h
• Estimate z (αb): note that [P0[V ] ](s,a) = φ(s,a),z (α) by Assumption 3.1, where the
h h h+1 α h h i
expectation is taken with respect to the nominal kernel P0( s,a). Given the estimator V (s),
h ·| h+1
it is natural to estimate z (α) by solving the following ridge regression on the offline dataset .
h
D
b
K
zˆ (α) = argmin V (sτ ) φτ⊤z 2 +λ z 2
h h+1 h+1 α− h k k2
z∈Rd
τ=1
X(cid:0)(cid:2) (cid:3) (cid:1)
K b
= Λ−1 φτ[V (sτ )] , (4.2)
h h h+1 h+1 α
τ=1
(cid:2)X (cid:3)
b
where λ > 0, φτ is a shorthand notation for φ(sτ,aτ), and Λ is the covariance matrix:
h h h h
K
Λ = φτ(φτ)⊤+λI. (4.3)
h h h
τ=1
X
• Estimate
νˆρ
: based on zˆ (α), we have
h h,i
νˆρ = max zˆ (α) ρ(α min[Vρ (s′)] , i [d], (4.4)
h,i α∈[0,H]{ h,i − − s′ h+1 α } ∀ ∈
b
7and immediately obtain the estimated robust Q-function at step h,
Q (s,a) = φ(s,a),θ +νˆρ (4.5)
h h h
(cid:10) (cid:11)
In the offline setting, we need tobadditionally account for the distribution shift caused by the
offline dataset . Hence, we propose to incorporate a penalty term in the estimator (4.5) based
D
on the pessimism principle in the face of uncertainty (Jin et al., 2021; Xie et al., 2021; Shi and Chi,
2022). The penalty term is carefully designed for the structure of d-rectangular linear DRMDPs,
upper bounding the robust estimation uncertainty. The algorithm is displayed in Algorithm 1.
Algorithm 1 Distributionally Robust Pessimistic Value Iteration (DRPVI)
Require: Input dataset and parameter β ; Vρ () = 0.
D 1 H+1 ·
1: for h = H, ,1 do
2: Λ h
←
K τ· =·· 1φτ hφτ h⊤+λI b
3: zˆ h(α) P= Λ− h1 K τ=1φτ
h
V hρ +1(sτ h+1)
α
4: νˆ hρ ,i ← max α∈(cid:0)[0P,H] zˆ h,i( (cid:2)α b) −ρ α −(cid:3)mi (cid:1)n s′ V hρ +1(s′) α , i ∈ [d]
5: Γ h( ·, ·)
←
β 1 d i=1(cid:8)φ i( ·, ·)1 i Λ(cid:0)− h1 (cid:2)
b
(cid:3) (cid:1)(cid:9)
6: Qρ h( ·, ·) ← φP( ·, ·)⊤ (cid:13) (cid:13)(θ h +νˆ hρ )(cid:13) (cid:13)−Γ h( ·, ·) [0,H−h+1]
ρ ρ ρ
7: πˆ bh( ·|·) ← a(cid:8)rgmax πh Q h( ·, ·),π h( ·|·) A,(cid:9)and V h( ·) ← hQ h( ·, ·),πˆ h( ·|·) iA
8: end for
(cid:10) (cid:11)
b b b
Remark 4.1. Algorithm 1 is essentially a value iteration algorithm based on the pessimism prin-
ciple in the face of uncertainty (Jin et al., 2021; Shi and Chi, 2022). The pessimism is achieved
by subtracting a robust penalty term, d i=1 φ i( ·, ·)1 i Λ− h1, from the robust Q-function estimation,
which is derived from bounding the function approximation uncertainty in d ridge regressions. In
P (cid:13) (cid:13)
c do sn ec pr aet re a, teat ris dt ge ep rh eg∈ re[H ssi] o, nd sen inot Lin ing eα 3⋆ i = toa or bg tm(cid:13) aia nx [ d0, iH ff] ere(cid:13)zˆ nh t,i( cα o) or− diρ naα te− sm ofin νˆs ρ′ iV nhρ + L1 in(s e′) 4.α Th, iw se des so il gv ne
(cid:8) (cid:0) h(cid:2) (cid:3) (cid:1)(cid:9)
is specifically tailored for the d-rectangular linear DRMDP, as we will see,bleading to a distinct
instance-dependent upper bound in Theorem 4.3. Notably, the optimization in Line 4 is decoupled
from the state-action pair, which stems from the decoupling property of d-rectangular uncertainty
set. SimilaralgorithmdesignshavealsoappearedinMa et al.(2022)forKullback-Leiblerdivergence
based DRMDPs and in Liu and Xu (2024) for online DRMDPs.
Before presenting the theoretical guarantee of DRPVI, we make the following feature cover-
age assumption, which is standard for offline linear MDPs (Wang et al., 2020; Duan et al., 2020;
Yin et al., 2022; Xiong et al., 2022).
Assumption 4.2. We assume κ := min λ (Eπb,P0 [φ(s ,a )φ(s ,a )⊤]) > 0 for the behavior
h∈[H] min h h h h
policy πb and the nominal transition kernel P0.
Assumption 4.2 requires the behavior policy to sufficiently explore the state-action space only
under the nominal environment P0. It implicitly assumes that the nominal and perturbed environ-
ments share the same state-action space, and that the full information of this space is accessible
through thenominal environment andthe behaviorpolicy πb. Assumption 4.2 rules outcases where
8new spaces (or simply new states) emerge in perturbed environments that can never be queried un-
der the nominal environment as a result of the distribution shift. Now we present the theoretical
guarantee for Algorithm 1.
Theorem4.3. UnderAssumptions 3.1and4.2,foranyK > max 512log(2dH2/δ)/κ2,20449d2H2/κ
{ }
and δ (0,1), if we set λ = 1 and β = O˜(√dH) in Algorithm 1, then with probability at least
1
∈
1 δ, for all s , the suboptimality of DRPVI satisfies
− ∈ S
H d
SubOpt(πˆ,s,ρ)
≤
β 1
·
P∈s Uu ρp
(P0)
h=1Eπ⋆,P (cid:20)i=1kφ i(s h,a h)1 i kΛ− h1 s 1 = s (cid:21). (4.6)
X X (cid:12)
The result in (4.6) resembles existing instance-dependent suboptimality(cid:12) bounds for standard
linear MDPs (Jin et al., 2021; Xiong et al., 2022), which both depend on the feature mapping φ
and the covariance matrix Λ (see Table 1 for a detailed comparison). However, there are two
h
major distinctions between those results. First, our result depends on the weighted sum of diagonal
elements d i=1kφ i(s h,a h)1 i kΛ− h1, dubbed as the d-rectangular robust estimation error, instead of
the Maha Planobis norm of the feature vector kφ(s h,a h) kΛ− h1. As discussed in Remark 4.1, this term
primarily arises due tothe necessity to solve ddistinct ridge regressions in each step, which presents
a unique challenge in our analysis. Second, we consider the supremum expectation of d-rectangular
robust estimation error with respect to all transition kernels in the uncertainty set, which measures
the worst case coverage of the covariance matrix Λ under the optimal robust policy π⋆.
h
Toconnectwithexistingliterature(Blanchet et al.,2023),wefurthershowthatunderAssumption 4.2,
the instance-dependent suboptimality bound can be simplified as follows.
Corollary 4.4. Underthesameassumptions andsettings asTheorem 4.3, withprobability atleast
1 δ, for all s , the suboptimality of DRPVI satisfies SubOpt(πˆ,s,ρ) = O˜(d3/2H2/(√κ K)).
− ∈ S ·
Remark 4.5. Note that the coverage parameter κ is upper bounded by 1/d (Wang et al., 2020;
Yin et al., 2022). If we assume there exist a constant 0 < c† < 1, such that κ = c†/d, then we have
SubOpt(πˆ,s,ρ) = O˜(d2H2/(c† √K)). This bound matches the state-of-the-art, Blanchet et al.
·
(2023, Theorem 6.3), which is derived under a distinct robust partial coverage assumption: there
exists some constant c> 0, such that for any (h,s,P) [H] ρ(P0), we have
∈ ×S ×U
Λ λI +K c Eπ⋆,P (φ (s,a)1 )(φ (s,a)1 )⊤ s = s . (4.7)
h i i i i 1
(cid:23) · · |
A closer examination of our Assumption 4.2(cid:2)reveals that, intuitively, Assum(cid:3)ption 4.2 guarantees a
weaker version of the robust partial coverage assumption: there exists some constant c† > 0, such
that for any (h,s,P) [H] ρ(P0), we have
∈ ×S ×U
Λ λI +K c†/d Eπ⋆,P (φ (s,a)1 )(φ (s,a)1 )⊤ s = s .
h i i i i 1
(cid:23) · · |
Nevertheless, Assumption 4.2 does not directly(cid:2) imply the robust partial cover(cid:3)age assumption (4.7).
Furthermore,ifweadditionallyassume(4.7)holds,thesuboptimalityofDRPVIsatisfiesSubOpt(πˆ,s,ρ) =
O˜(d3/2H2/(c √K)), which achieves a √d improvement over Blanchet et al. (2023, Theorem 6.3).
·
For more details, please refer to Appendix B.1 and Appendix B.2.
4.1 Proof of Theorem 4.3
Our analysis mainly deals with the challenges induced by the model uncertainty, inf , and
P∈Uρ(P0)
the need to maximally exploit the information in the offline dataset. The proof of Theorem 4.3
mainly constitutes of two steps.
9Step 1: suboptimality decomposition. We first decompose the suboptimality gap in the
following Lemma 4.6 to connect it with the estimation error, the full proof of which can be found
in Appendix D.1.
Lemma 4.6 (Suboptimality Decomposition for DRMDP). If the following holds
inf [P Vρ ](s,a) φ(s,a)νˆρ Γ (s,a), (s,a,h) [H], (4.8)
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 − h
(cid:12)
≤ h ∀ ∈S ×A×
(cid:12) (cid:12)
b
then we (cid:12) have SubOpt(πˆ,s,ρ) 2sup H E(cid:12) π⋆,P Γ (s ,a )s = s .
≤ P∈Uρ(P0) h=1 h h h | 1
ThemainchallengeinderivingLemma 4.6lPiesinthede(cid:2)pendencyofthero(cid:3)bustBellmanequation
(3.1) on the nominal kernel P0, which is not linear and does not even have an explicit form. It
should be noted that the term inf Ph(·|s,a)∈U hρ(s,a;µ0 h,i)[P hV hρ +1](s,a) −φ(s,a)νˆ hρ in condition (4.8)
stands for the estimation error of the estimated robust Q-function in (4.5), which we refer to as
(cid:12) (cid:12)
the robust estimation uncertain(cid:12)ty. Lemma 4.6 shows thbat under the conditio(cid:12)n that the robust
estimation uncertainty is bounded by Γ (s,a), the suboptimality gap can be upper bounded in
h
terms of Γ (s,a). To conclude the proof, it remains to derive Γ (s,a) and then substitute it back
h h
into the result in Lemma 4.6.
Step 2: bounding the robust estimation uncertainty. We now bound the robust estima-
tion uncertainty in Lemma 4.6 by the following result, the full proof of which can be found in
Appendix D.2.
Lemma 4.7 (Robust Estimation Uncertainty Bound). For any sufficiently large sample size K
satisfying K > max 512log(2dH2/δ)/κ2,20449d2H2/κ , and any fixed δ (0,1), if we set λ = 1
{ } ∈
in Algorithm 1, then with probability at least 1 δ, for all (s,a,h) [H], we have
− ∈ S ×A×
inf [P Vρ ](s,a) φ(s,a)νˆρ Γ (s,a), (4.9)
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 − h
(cid:12)
≤ h
(cid:12) (cid:12)
b
(cid:12) (cid:12)
where Γ h(s,a) = 4√dH√ι d i=1kφ i(s,a)1 i kΛ− h1 and ι= log(2dH2K/δ).
P
Γ (s,a) provides an explicit bound for the robust estimation uncertainty, which also serves as
h
the penalty term in Line 5 of Algorithm 1. The main challenge of deriving Lemma 4.7 lies in
inferring the worst-case behavior using information merely from the nominal environment. Our
idea is to first transform the robust estimation uncertainty to the estimation uncertainty of ridge
regressions (4.2) on the nominal model P0, where the samples are collected and statistical control
is available. We then adopt a reference-advantage decomposition technique, which is new in the
linear DRMDP literature, to further decompose the estimation uncertainty on the nominal model
into the reference uncertainty and the advantage uncertainty. The remaining proof is to bound the
reference uncertainty and advantage uncertainty respectively using concentration and union bound
arguments under an induction framework to address the temporal dependency. We highlight that
all these arguments are specifically designed for the unique problem of DRMDP, which is novel and
nontrivial.
105 Distributionally Robust Variance-Aware Pessimistic Value Itera-
tion
The instance-dependent bound in (4.6) has an explicit dependency on H, which arises from the
ρ
fact that Q (s,a) [0,H] for any (h,ρ) [H] (0,1] and the Hoeffding-type self-normalized
h ∈ ∈ ×
concentration inequality used in our analysis. We will show in this section that the range of any
robustvaluefunctioncouldbemuch smallerunderarefinedanalysis. Consequently, wecanleverage
variance information to improve Algorithm 1 and achieve a strengthened upper bound.
Intuition In the robust Bellman equation (3.1), the worst-case transition kernel would put as
π,ρ
much mass as possible on the minimizer of V (s), denoted by s . Based on this observation,
h+1 min
we conjecture that the robust Bellman equation (3.1) recursively reduces the maximal value of
robust value functions, and thus shrinks its range. To see this, we define µˇ = (1 ρ)µ0 +ρδ ,
h,i − h,i smin
π,ρ
where δ is the Dirac measure at s , and we assume V (s ) = 0 for any (π,h) Π [H]
just
fors im llin
ustration. It is easy to
verm ifyin
that µˇ
ρ (µ0h+ )1 anm din
is indeed the
worst-c∈
ase
f×
actor
h,i ∈ Uh,i h,i
kernel. Then by (3.1) we have Vπ,ρ (s) = E [r (s,a)+(1 ρ)[P0Vπ,ρ ](s,a)], which immediately
implies max s∈SV hπ,ρ (s)
≤
1+(1h −ρ)max s′a ∈∼ Sπ V hπh +,ρ 1(s′). This− justifih esh o+ u1 r conjecture that the range
of the robust value functions shrinks over stage. We dub this phenomenon as Range Shrinkage and
summarize it in the following lemma, with a more formal proof postponed to Appendix D.5.
Lemma 5.1 (Range Shrinkage). For any (ρ,π,h) (0,1] Π [H], we have max Vπ,ρ (s)
min Vπ,ρ (s) (1 (1 ρ)H−h+1)/ρ. ∈ × × s∈S h −
s∈S h ≤ − −
A similar phenomenon is first observed in infinite horizon tabular DRMDPs (Shi et al., 2023,
Lemma 7). One important implication of Lemma 5.1 is that the conditional variance of any value
function shrinks accordingly. In particular, when ρ = O(1), the range of any robust value function
would shrink to constant order, which leads to constant order conditional variances. This motivates
us to leverage the variance information in both algorithm design and theoretical analysis. Inspired
by the variance-weighted ridge regression in standard linear MDPs (Zhou et al., 2021a; Min et al.,
2021;Yin et al.,2022;Xiong et al.,2022), weproposetoimprove thevanillaridgeregressionin(4.2)
by incorporating variance weights. To this end, we first propose an appropriate variance estimator,
whose form is specifically motivated by our theoretical analysis framework, to quantify the variance
information.
Variance estimation We first run Algorithm 1 using an offline dataset ′ that is independent
′ρ D
of to obtain estimators of the optimal robust value functions V . By Assumption 3.1,
theD
variance of
[V′ρ
] under the nominal environment is [Var
[V{′ρ h
]
} ](h s∈ ,[H a)]
=
[P0[V′ρ
]2](s,a)
([P0[V′ρ
]
](s,a))h 2+ =1 α
φ(s,a),z ( φ(s,a),z )2. We
esth imah t+ eb1 zα
and z
h viah+ ri1 dα
ge
regre−
s-
h h+1 α b h h,2 i− h h,1 i b h,1 h,2 b
sion similarly as in (4.2):
b
K
z˜ (α) = argmin V′ρ (sτ ) 2 φτ⊤z 2 +λ z 2, (5.1a)
h,2 h+1 h+1 α− h k k2
z∈Rd
τ=1
X(cid:0)(cid:2) (cid:3) (cid:1)
K b
z˜ (α) = argmin V′ρ (sτ ) φτ⊤z 2 +λ z 2. (5.1b)
h,1 h+1 h+1 α− h k k2
z∈Rd
τ=1
X(cid:0)(cid:2) (cid:3) (cid:1)
b
11We construct the following truncated variance estimator
dH3
σ2(s,a;α) := max 1, φ(s,a)⊤z˜ (α) φ(s,a)⊤z˜ (α) 2 O˜ , (5.2)
h h,2 [0,H2]− h,1 [0,H]− √Kκ
n (cid:16) (cid:17)o
(cid:2) (cid:3) (cid:2) (cid:3)
wherebthe last term is a penalty to achieve pessimistic estimations of conditional variances.
Variance-Aware Function Approximation Mechanism Similar to the two-step estimation
procedureofAlgorithm 1,wefirstestimatez (α)bythefollowingvariance-weightedridgeregression
h
under the nominal environment:
K V (sτ ) φτ⊤z 2 K φτ[V (sτ )]
zˆ (α) = argmin h+1 h+1 α− h +λ z 2 = Σ−1(α) h h+1 h+1 α ,
h σ2(sτ,aτ;α) k k2 h σ2(sτ,aτ;α)
z∈Rd Xτ=1 (cid:0)(cid:2) b h h (cid:3)h (cid:1) hXτ=1 h b h h i
whereΣ (α) = K φτφτ⊤/σ2(sτ,aτ;α)+λI istheempiricalvariance-weightedcovariancematrix,
h τ=1 h h hb h h b
which can be deemed as an estimator of the following variance-weighted covariance matrix
P
b
K
Σ⋆ = φτφτ⊤/[V V⋆,ρ ](sτ,aτ)+λI. (5.3)
h h h h h+1 h h
τ=1
X
ρ
In the second step, we estimate ν , i [d] in the same way as (4.4). We then add a pessimism
h,i ∀ ∈
penalty based on Σ (α). We present the full algorithm details in Algorithm 2.
h
Algorithm 2Distributionally RobustandVarianceAwarePessimisticValueIteration(VA-DRPVI)
Require: Input dataset , ′ and β ; Vρ () =0
1 2:
:
R fou rn hA =lg Hor ,ithm ,11 dus
oinD
g
dD
ataset
D2
′ to
bH g+ et1 {·
V
h′ρ
}h∈[H]
···
3: Construct variance estimator σ2(, ;α) ubsing ′ by (5.1) and (5.2)
h · · D
4: Σ h(α) = K τ=1φτ hφτ h⊤/σ h2(sτ h,aτ h;α)+λI
5: zˆ h(α) = Σ P− h1(α) K τ=1φτ
h
V hρb +1(sτ h+1) α/σ h2(sτ h,aτ h;α)
6 7:
:
α
νˆ
hρi ,i= =ar zˆg hm ,i(a αx iα )∈ −[0(cid:16) , ρH (P] α{ izˆ −h,ib ( mα i) n(cid:2) − sb ′[Vρ( hρα +1− (sm ′)]i αn(cid:3) is )′ ,[V b b∀hρ i+ ∈1(s [d′) ]] α) },(cid:17) ∀i ∈ [d]
8: Γ h( ·, ·) ← β 2 d i=1kφ i( ·, ·)1 i kΣ b− h1(αi)
9: Qρ h( ·, ·) = {φ( P ·, ·)⊤(θ h +νˆ hρ ) −Γ h( ·, ·) }[0,H−h+1]
ρ ρ ρ
10: πˆ h( ·|·)
←
argmax πhhQ h( ·, ·),π h( ·|·) iA, V h( ·)
←
hQ h( ·, ·),πˆ h( ·|·) iA
11: endb for
b b b
Theorem 5.2. Under Assumptions 3.1 and 4.2, for K > max O˜(d2H6/κ),O˜(H4/κ2) and δ
{ } ∈
(0,1), if we set λ = 1/H2 and β = O˜(√d) in Algorithm 2, then with probability at least 1 δ, for
2
−
all s , the suboptimality of VA-DRPVI satisfies
∈ S
H d
SubOpt(πˆ,s,ρ) ≤ β 2 · P∈s Uu ρp
(P0)
h=1Eπ⋆,P (cid:20)i=1kφ i(s h,a h)1 i kΣ h⋆−1 s 1 = s (cid:21), (5.4)
X X (cid:12)
(cid:12)
where Σ⋆ is defined as in (5.3).
h
12Note that the bound in (5.4) does not explicitly depend on H anymore due to the choice of β .
2
A naive observation that [V V⋆,ρ ](s,a) [1,H2] implies Σ⋆−1 H2Λ−1 by definition. Thus the
h h+1 ∈ h (cid:22) h
upper bound of Algorithm 2 is never worse than that of Algorithm 1. This improvement brought
by variance information is similar to that in standard linear MDPs (Xiong et al., 2022, Theorem
2). However, thanks to the range shrinkage phenomenon, we can further show that VA-DRPVI is
strictly better than DRPVI when the uncertainty level is of constant order.
Corollary 5.3. Under the same assumptions and settings as Theorem 5.2, given the uncertainty
level ρ, we have with probability at least 1 δ, for all s , the suboptimality of VA-DRPVI
− ∈ S
satisfies
(1 (1 ρ)H) H d
SubOpt(πˆ,s,ρ)
≤
β 2
·
− ρ−
·
P∈s Uu ρp
(P0)
h=1Eπ⋆,P (cid:20)i=1kφ i(s h,a h)1 i kΛ− h1 |s 1 = s (cid:21).
X X
When ρ = O(1), the suboptimality of Algorithm 2 is smaller than that of Algorithm 1 by H.
With a similar argument as in Remark 4.5, if we further assume the robust partial coverage assump-
tion (4.7), the above instance-dependent upper bound can be simplified to O˜(d3/2H/(c √K)),
·
which improves the state-of-the-art (Blanchet et al., 2023, Theorem 6.3) by √dH.
6 Information-Theoretic Lower Bound
At the core of our instance-dependent upper bound in Theorem 5.2 is the following novel instance-
dependent uncertainty function
H d
Φ(Σ h⋆−1,s) := P∈s Uu ρp
(P0)
h=1Eπ⋆,P (cid:20)i=1kφ i(s h,a h)1 i kΣ h⋆−1 s 1 = s (cid:21).
X X (cid:12)
(cid:12)
We now establish an information-theoretic lower bound expressed in terms of Φ(Σ⋆−1,s), suggest-
h
ing its inevitability for d-rectangular linear DRMDPs. First, we define SubOpt(M,πˆ,s,ρ) as the
suboptimality gap specific to a DRMDP instance M.
Theorem 6.1. Given uncertainty level ρ (0,3/4], dimension d, horizon length H and sample size
K > O˜(d6), there exists a class of d-rectan∈ gular linear DRMDPs and an offline dataset of size
M D
K such that for all s , with probability at least 1 δ,
∈ S −
H d
in πˆf Msu ∈Mp SubOpt(M,πˆ,s,ρ) ≥ c · P∈s Uu ρp
(P0)
h=1Eπ⋆,P (cid:20)i=1kφ i(s h,a h)1 i kΣ h⋆−1 |s 1 = s (cid:21),
X X
where c is a universal constant.
Theorem 6.1 shows that the uncertainty function Φ(Σ⋆−1,s) is intrinsic to the information-
h
theoretic lower bound, and thus is inevitable. It is noteworthy that the lower bound presented
in Theorem 6.1 aligns with the upper bound demonstrated in Theorem 5.2 up to a factor of β ,
2
which implies that VA-DRPVI is minimax optimal in the sense of information theory, but with a
small gapof O˜(√d). Consequently, we can affirm that, in thecontext of robust offline reinforcement
learningwithfunctionapproximation, boththecomputationalefficiencyandminimaxoptimalityare
13achievable under the setting of d-rectangular linear DRMDPs with TV uncertainty sets. Moreover,
Theorem 6.1 also suggests that achieving a good robust policy necessitates the worst case coverage
of the offline dataset over the entire uncertainty set of transition models, which is significantly
different from standard linear MDPs where a good coverage under the nominal model is enough
(Jin et al., 2021; Yin et al., 2022; Xiong et al., 2022). Such a distinction indicates that learning in
linear DRMDPs may be more challenging in comparison to standard linear MDPs.
Further, we highlight that the hard instances we constructed in our proof of the lower bounds
also satisfy Assumption 4.2. It remains an interesting direction to explore what would happen if
the nominal and perturbed environments don’t share exactly the same state space. We conjecture
that since there could be absolutely new states emerging in perturbed environments that can never
be explored in the nominal environment, the policy learned merely using data collected from the
nominal environment could be arbitrarily bad.
Challenges and novelties in construction of hardinstances Existingtightlowerboundanal-
ysis in standard linear MDPs (Zanette et al., 2021; Yin et al., 2022; Xiong et al., 2022) generally
dependsontheAssoud’smethodandafamilyofhardinstances indexedbyξ 1,1 dH. However,
∈ {− }
they do not consider model uncertainty, which largely hinders the derivation of explicit formulas for
the robust value functions. Further, one prerequisite of the Assoud’s method is switching the ini-
tial minimax suboptimality inf πˆmax M∈MSubOpt(πˆ,s,ρ) to a risk of the form inf ξ′max ξD H(ξ,ξ′),
where D (, ) is the Hamming distance. The consideration of model uncertainty significantly com-
H
· ·
plicates this procedure, as the nonlinearity involved disrupts the linear dependency between the
value function and the index ξ. At the core of Theorem 6.1 is a novel class of hard instances .
M
The high-level ideas of the design are that the hard instances should (1) fulfill the d-rectangular
linear DRMDP conditions, (2) mitigate the nonlinearity caused by model uncertainty, (3) achieve
the prerequisite for Assoud’s method, and (4) be concise enough to admit matrix analysis. Thus,
the design of hard instances and analysis of lower bound is meticulous and nontrivial. We postpone
details on the construction of hard instances and the proof of Theorem 6.1 to Appendix C.
As a side product of Theorem 6.1, we show in the following corollary an information-theoretic
lower bound in terms of the instance-dependent uncertainty function Φ(Λ−1,s) in Theorem 4.3.
h
Corollary 6.2. Under the same setting in Theorem 6.1, the class of hard instances and offline
M
dataset in Theorem 6.1 also suggests that, with probability at least 1 δ,
D −
H d
in πˆf Msu ∈p MSubOpt(πˆ,s,ρ)
≥
c
·
P∈s Uu ρp
(P0)
h=1Eπ⋆,P (cid:20)i=1kφ i(s h,a h)1 i kΛ− h1 |s 1 = s (cid:21),
X X
where c is a universal constant.
With Corollary 4.4, we know that the instance-dependent uncertainty function Φ(Λ−1,s) in
h
Theorem 4.3 also arises from the information-theoretic lower bound. We note the lower bound in
Corollary 6.2 matches the upper bound in Theorem 4.3 up to β , thus DRPVI is also minimax
1
optimal in the sense of information theory, but with a larger gap of O˜(√dH). Moreover, the only
difference between Theorem 6.1 and Corollary 6.2 is the covariance matrix. Due to the fact that
Λ−1 Σ−1, the information-theoretic lower bound in Theorem 6.1 is indeed tighter than that in
h (cid:22) h
Corollary 6.2.
147 Conclusions
We studied robust offline RL with function approximation under the setting of d-rectangular linear
DRMDPs with TV uncertainty sets. We first proposed the DRPVI algorithm, accordingly built
up a theoretical analysis pipeline and established the first instance-dependent upper bound on the
suboptimality gap in the context of robust offline RL. Motivated by an interesting range shrinkage
phenomenon, we further proposed the VA-DRPVI algorithm, which leverages the conditional vari-
ance information of the optimal robust value function. Based on the analysis pipeline built above,
we show that the upper bound of VA-DRPVI achieves sharp dependence on the horizon length H.
Moreover, we found that new quantities in terms of two crucial ingredients, a supremum over uncer-
tainty set and a diagonal-based normalization, appear in the upper bounds. We further established
aninformation-theoretic lower boundtoprovidejustificationofthenewquantities, andconfirmthat
boththecomputational efficiency andminimaxoptimality areachievableforrobustofflineRLunder
the setting of d-rectangular linear DRMDPs. It remains an interesting future question whether the
computational and provable efficiency can be achieved in other settings for robust offline RL with
function approximation.
A A More Computationally Efficient Variant of VA-DRPVI
In this section, we propose a modified version of Algorithm 2, which reduces the computation in
the ridge regressions of Algorithm 2 and attains the same theoretical guarantees.
Variance Estimator. In Section 5, we estimate the variance of the truncated robust value func-
′ρ
tion [V ] . Thus, for different α, we need to establish different variance estimators, which signifi-
h+1 α
cantly increases the computational burden. The theoretical analysis of Algorithm 2 suggests that it
sufficebs to estimate the the variance of
V′ρ
, instead of the truncated one. In particular, we know
h+1
[Var
V′ρ
](s,a) =
[P0(V′ρ
)2](s,a)
([P0V′ρ
](s,a))2 = φ(s,a),z ( φ(s,a),z )2. Then
h h+1 h h+1 − b h h+1 h h,2 i− h h,1 i
we estimate z and z via ridge regression:
h,1 h,2
b b b
K
z˜ = argmin V′ρ (sτ ) 2 φτ⊤z 2 +λ z 2, (A.1a)
h,2 h+1 h+1 − h k k2
z∈Rd
τ=1
X(cid:0)(cid:0) (cid:1) (cid:1)
K b
z˜ = argmin V′ρ (sτ ) φτ⊤z 2 +λ z 2. (A.1b)
h,1 h+1 h+1 − h k k2
z∈Rd
τ=1
X(cid:0) (cid:1)
b
We construct the following truncated variance estimator:
dH3
σ2(s,a) := max 1, φ(s,a)⊤z˜ φ(s,a)⊤z˜ 2 O˜ . (A.2)
h h,2 [0,H2]− h,1 [0,H]− √Kκ
n (cid:16) (cid:17)o
(cid:2) (cid:3) (cid:2) (cid:3)
The modifiedbvariance-aware algorithm is presented in Algorithm 3 and the theoretical guarantee is
presented in Theorem A.1.
Theorem A.1. UnderAssumption 3.1 andAssumption 4.2, forK > max O˜(d2H6/κ),O˜(H4/κ2)
{ }
and δ (0,1), if we set λ = 1/H2 and β = O˜(√d) in Algorithm 3, then with probability at least
2
∈
15Algorithm 3 Modified VA-DRPVI
Require: Input dataset , ′ and β ; Vρ () =0
1: Run Algorithm 1
usinD
g
dD
ataset
2
′
toH g+ et1 · V′ρ
2: for h = H, ,1 do D b { h }h∈[H]
···
3: Construct variance estimator σ2(, ) usinbg ′ by (A.1) and (A.2)
h · · D
4: Σ h = K τ=1φτ hφτ h⊤/σ h2(sτ h,aτ h)+λI
5: zˆ h(α) P= Σ− h1 K τ=1φτ
h
V hρ +1(bsτ h+1) α/σ h2(sτ h,aτ h)
6 7:
:
α
νˆ
hρi ,i= =ar zˆg hm ,i(a αx iα(cid:16) )∈ −P[0, ρH (] α{ ib zˆ −h,i(cid:2)( mα b i) n− s′[Vρ( hρα +1− (s(cid:3)m ′)]i αn b is )′ ,[V b∀hρ i+ ∈1(s [d(cid:17)′) ]] α) }, ∀i ∈ [d]
8: Γ h( ·, ·) ← β 2 d i=1kφ i( ·, ·)1 i kΣ b− h1
9: Qρ h( ·, ·) = {φ( P ·, ·)⊤(θ h +νˆ hρ ) −Γ h( ·, ·) }[0,H−h+1]
ρ ρ ρ
10: πˆ h( ·|·)
←
argmax πhhQ h( ·, ·),π h( ·|·) iA, V h( ·)
←
hQ h( ·, ·),πˆ h( ·|·) iA
11: endb for
b b b
1 δ, for all s , the suboptimality of VA-DRPVI satisfies
− ∈ S
H d
SubOpt(πˆ,s,ρ) ≤ β 2 · P∈s Uu ρp
(P0)
Eπ⋆,P kφ i(s h,a h)1 i kΣ h⋆−1 |s 1 = s , (A.3)
Xh=1 hXi=1 i
where Σ⋆ = K φτφτ⊤/[V V⋆ ](sτ,aτ)+λI.
h τ=1 h h h h+1 h h
Remark A.P2. The computation cost of Algorithm 3 is much smaller than Algorithm 2, as the
variance estimators are not related to α anymore. Notably, Algorithm 3 shares the same upper
bound as Algorithm 2, thus is also minimax optimal.
B Proof of the Suboptimality Upper Bounds
Inthissection,weprovethemainresultsinCorollary 4.4,Remark 4.5,Theorem 5.2,andCorollary 5.3,
which give out the instance-dependent upper bounds of the proposed algorithms. Before the proof,
weintroducesomeusefulnotations. Foranyfunctionf : S → [0,H −1],defineinf Ph(·|s,a)∈U hρ(s,a;µ0 h,i)[P hf](s,a):=
φ(s,a)⊤νˆ hρ (f),whereforeachi
∈
[d],wehaveνˆ hρ ,i(f)= max
α∈[0,H]
{Eˆµ0 h,i[f(s)]
α
c−ρ(α −min s′∈S[f(s′)] α)
}
and Eˆµ0 h,i[f(s)]
α
= Λ− h1 K τ=1φτ h[f(sτ h+1)] α.
P
B.1 Proof of Corollary 4.4
The proof of Corollary 4.4 is straightforward given our result in Theorem 4.3.
Proof. We denote Λ˜ = Eπb,P0 [φ(s ,a )φ(s ,a )⊤]. By Assumption 4.2, we have Λ˜ κ I. We
h h h h h h h (cid:23) ·
further bound (4.6) as follows,
H d
P∈s Uu ρp
(P0)
Eπ⋆,P kφ i(s h,a h)1 i kΛ− h1 |s 1 =s
Xh=1 hXi=1 i
16H d
2
≤
P∈s Uu ρp
(P0)√K
Eπ⋆,P kφ i(s h,a h)1 i kΛ˜− h1 |s 1 = s (B.1)
Xh=1 hXi=1 i
H d
2
= sup Eπ⋆,P Tr φ (s ,a )1 φ (s ,a )1 ⊤ Λ˜−1 s = s
P∈Uρ(P0)√K
Xh=1 Xi=1 hq (cid:0)(cid:0)
i h h i
(cid:1)(cid:0)
i h h i
(cid:1)
h
(cid:1)(cid:12)
1
i
H d (cid:12)
2
sup Tr Eπ⋆,P φ (s ,a )1 φ (s ,a )1 ⊤ s = s Λ˜−1 (B.2)
≤ P∈Uρ(P0)√K
h=1i=1q
i h h i i h h i | 1 h
XX (cid:0) (cid:2)(cid:0) (cid:1)(cid:0) (cid:1) (cid:3) (cid:1)
2 H d (Eπ⋆,P[φ (s ,a )s = s])2
i h h 1
sup | (B.3)
≤ P∈Uρ(P0)√K s κ
h=1i=1
XX
H d
2 1
≤ √K κ
h=1i=1r
XX
2dH
,
≤ √κ K
·
where (B.1) is due to Lemma F.3, (B.2) is due to the Jensen’s inequality, (B.3) holds by the fact
that the only nonzero element of Eπ⋆,P φ (s ,a )1 φ (s ,a )1 ⊤ s = s is the i-th diagonal
i h h i i h h i 1
|
element,
Eπ⋆,P
[φ (s ,a )s = s]. We concludes the proof by invoking (4.6).
i h h | 1 (cid:2)(cid:0) (cid:1)(cid:0) (cid:1) (cid:3)
B.2 Proof of the Result in Remark 4.5
Next, we prove the result in Remark 4.5 under the additional robust partial coverage assumption
(4.2). We follow the argument in the proof of Theorem 6.3 in Blanchet et al. (2023) to derive the
suboptimality bound.
Proof. Recall (4.7), for any (h,s,P) [H] ρ(P0), we have
∈ ×S ×U
Λ λI +K c Eπ⋆,P (φ (s,a)1 )(φ (s,a)1 )⊤ s = s .
h i i i i 1
(cid:23) · · |
(cid:2) (cid:3)
First, we denote
ΛP = Eπ⋆,P φ (s ,a )1 φ (s ,a )1 ⊤ s = s , (h,i,P) [H] [d] ρ(P0). (B.4)
h,i i h h i i h h i 1 ∀ ∈ × ×U
h (cid:0) (cid:1)(cid:0) (cid:1) (cid:12) i
With this notation, the assumption in (4.7) can(cid:12) be concisely expressed as
Λ λI +K c ΛP . (B.5)
h (cid:23) · · h,i
Setting λ = 1 and leveraging (B.5), we further bound (4.6) as follows,
H d
P∈s Uu ρp
(P0)
Xh=1Eπ⋆,P hXi=1kφ i(s h,a h)1 i kΛ− h1 |s 1 = s
i
H d
= sup Eπ⋆,P Tr φ (s ,a )1 φ (s ,a )1 ⊤ Λ−1 s = s
i h h i i h h i h 1
P∈Uρ(P0)
Xh=1 Xi=1 hq (cid:0)(cid:0) (cid:1)(cid:0) (cid:1) (cid:1)(cid:12) i
(cid:12)
17H d
sup Tr Eπ⋆,P φ (s ,a )1 φ (s ,a )1 ⊤ s = s Λ−1 (B.6)
≤ i h h i i h h i | 1 h
P∈Uρ(P0)
h=1i=1q
XX (cid:0) (cid:2)(cid:0) (cid:1)(cid:0) (cid:1) (cid:3) (cid:1)
H d
sup Tr ΛP I +K c ΛP −1 (B.7)
≤ h,i· · · h,i
P∈Uρ(P0)
h=1i=1q
XX (cid:0) (cid:0) (cid:1) (cid:1)
H d (Eπ⋆,P[φ (s ,a )s = s])2
i h h 1
= sup | (B.8)
P∈Uρ(P0)
h=1i=1s1+c ·K ·(Eπ⋆,P[φ i(s h,a h) |s
1
= s])2
XX
H d
1
sup
≤ c K
P∈Uρ(P0) h=1i=1r ·
XX
dH
, (B.9)
≤ c √K
·
where (B.6) is due to the Jensen’s inequality, (B.7) holds by (B.5) and (B.4), (B.8) holds by the fact
thattheonlynonzeroelement ofΛP isthei-thdiagonalelement, (ΛP ) = Eπ⋆,P [φ (s ,a )s = s],
h,i h,i ii i h h | 1
and (B.9) is due to 0< c 1. We concludes the proof by invoking (4.6).
≤
B.3 Proof of Theorem 5.2
The proof idea is similar to that of Theorem 4.3, except that we additionally analyze the variance
estimation and apply the Bernstein-type self-normalized concentration inequality to bound the
reference uncertainty, which is the dominant term. We start from analyzing the estimation error of
conditional variances in the following lemma.
Lemma B.1. Under Assumption 3.1 and Assumption 4.2, when K O˜(H4/κ2), then with prob-
≥
ability at least 1 δ, for all (s,a,h) [H] and any fixed α, we have
− ∈ S ×A×
dH3
V [V⋆,ρ ] (s,a) O˜ σ2(s,a;α) V [V⋆,ρ ] (s,a).
h h+1 α − √Kκ ≤ h ≤ h h+1 α
(cid:16) (cid:17)
(cid:2) (cid:3) (cid:2) (cid:3)
The following lemma bounds the estimation errbor by reference-advantage decomposition.
Lemma B.2 (Variance-Aware Reference-Advantage Decomposition). There exist α , where
i i∈[d]
{ }
α [0,H], i [d], such that
i
∈ ∀ ∈
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
(cid:12)
(cid:12) d b b (cid:12)
(cid:12) (cid:12)
≤
λ kφ i(s,a)1 i kΣ− h1(αi)kEµ0 h[V h⋆ +,ρ 1(s)] αikΣ− h1(αi)
i=1
X
i
+| Xi=d 1kφ i(s,a)1 i kΣ− h1(αi{ )z (cid:13)XτK
=1
φ στ h h2η (hτ s( τ h[ ,V ah⋆ τ h+, ;ρ 1 α] α i)i) (cid:13)Σ} − h1(αi)
(cid:13) (cid:13)
(cid:13) ii (cid:13)
b
| {z }
18d
+λ Xi=1kφ i(s,a)1 i kΣ− h1(αi)
(cid:13)
(cid:13)Eµ0 h (cid:2)[V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi
(cid:3)(cid:13)
(cid:13)Σ− h1(αi)
b
(cid:13) iii (cid:13)
+ X| i=d 1kφ i(s,a)1 i kΣ− h1(αi) (cid:13)XτK
=1
φτ hη{hτ z([V bhρ + σ1 h2( (s s) τ h] ,α ai τ h− ;α[V i)h⋆ +,ρ 1(s)] αi) (cid:13)Σ− h} 1(αi),
(cid:13) (cid:13)
(cid:13) iv (cid:13)
b
where η hτ([f] αi) =| P0 h[f]
αi
(sτ h,aτ h) −[f(sτ h+1)]
αi
,{zfor any function f :
S →
[0,H −}1].
Now we are rea(cid:0)d(cid:2)y to pro(cid:3)ve Theorem 5.2 (cid:1)
Proof of Theorem 5.2. Toprovethistheorem,weboundtheestimationerrorbyΓ (s,a),theninvoke
h
Lemma 4.6 to get the result. First, we bound terms i-iv in Lemma B.2 to deduce Γ (s,a) at each
h
step h [H], respectively.
∈
Bound i and iii: We set λ = 1/H2 to ensure that for all (s,a,h) [H], we have
∈ S ×A×
d d
i+iii
≤
√λ√dH kφ i(s,a)1 i kΣ− h1(αi) = √d kφ i(s,a)1 i kΣ− h1(αi). (B.10)
i=1 i=1
X X
Bound ii: For all (s,a,α) [0,H], by definition we have σ (s,a;α) 1. Thus, for
h
all (h,τ,i) [H] [K] [d∈ ],S w× e hA av× e ητ([V⋆,ρ ] )/σ (sτ,aτ,α ) H. Not≥ e that V⋆,ρ is
∈ × × | h h+1 αi h h h i | ≤ H+1
independent of , we can directly apply Bernstein-type self-normalizebd concentration inequality
D
Lemma F.2 and a union bound to obtain the upper bobund. In concrete, we define the filtration
= σ( (sj ,aj ) τ sj τ−1). Since V⋆,ρ and σ (s,a;α) are independent of , thus
Fτ−1,h { h h }j=1 ∪ { h+1}j=1 h+1 h D
ητ([V⋆,ρ ] )/σ (sτ,aτ,α ) is mean-zero conditioned on the filtration . Further, we have
h h+1 αi h h h i Fτ−1,h
E ση hτ b (( s[V τ,h⋆ a+,ρ τ1 ;] α αi)
)
2 Fτ−1,h = [Var σ[V 2(h⋆ s+, τρ 1 ,] aα τi] ;( αsτ h ),aτ h) b (B.11)
h h h i h h h i
h(cid:16) (cid:17) (cid:12) i
(cid:12) [V[V⋆,ρ ] ](sτ,aτ)
(cid:12) h+1 αi h h
b
≤
σb2(sτ,aτ;α
)
h h h i
=
[V[V h⋆ +,ρ 1] αi](sτ h,aτ h) −O˜(dH3/√Kκ)
+
O˜(dH3/√Kκ)
b σ2(sτ,aτ;α ) σ2(sτ,aτ;α )
h h h i h h h i
O˜(dH3/√Kκ)
1+ (B.12)
≤ σ2(sτ,abτ;α ) O˜(dH3/√Kκ) b
h h h i −
dH3
1+2O˜ , (B.13)
≤ b √Kκ
(cid:16) (cid:17)
where (B.11) holds by the fact that σ2(, ; ) is independent of and (sτ,aτ) is measurable.
h · · · D h h Fτ−1,h
(B.12) holds by Lemma B.1, and (B.13) holds by setting K Ω˜(d2H6/κ) such that σ2(sτ,aτ;α )
≥ h h h i −
O˜(dH3/√Kκ) 1 O˜(dH3/√Kκ)b 1/2. Further, by (B.13), our choice of K also ensures that
≥ − ≥
E η hτ([V h⋆ +,ρ 1] αi) 2
|Fτ−1,h
= O(1). Then by Lemma F.2, we have b
(cid:2)(cid:0) (cid:1) (cid:3) K φτητ([V⋆,ρ ] )
h h h+1 αi O˜(√d).
(cid:13)Xτ=1
σ h2(sτ h,aτ h;α i) (cid:13)Σ− h1(αi) ≤
(cid:13) (cid:13)
(cid:13) (cid:13)
b 19This implies
d
ii
≤
O˜(√d) kφ i(s,a)1 i kΣ− h1(αi). (B.14)
i=1
X
Bound iv: Following the same induction analysis procedure, we have [Vρ ] [V⋆,ρ ]
k h+1 αi − h+1 αik ≤
O˜(√dH2/√Kκ). Then, using standard ǫ-covering number argument and Lemma F.1, we have
b
d3/2H2 d
iv
≤
O˜
√Kκ
kφ i(s,a)1 i kΣ− h1(αi). (B.15)
(cid:16) (cid:17)Xi=1
To make it non-dominant, we require K Ω˜(d2H4/κ). By Lemma B.1, for any α [0,H], we have
≥ ∈
σ2(sτ,aτ;α) [V [V⋆,ρ ] ](sτ,aτ) [V V⋆,ρ ](sτ,aτ),
h h h ≤ h h+1 α h h ≤ h h+1 h h
this implies that
b
K φτφτ⊤ −1 K φτφτ⊤ −1
h h +λI h h +λI := Σ⋆−1.
σ2(sτ,aτ;α ) (cid:22) [V V⋆,ρ ](sτ,aτ) h
(cid:16)Xτ=1 h h h i (cid:17) (cid:16)Xτ=1 h h+1 h h (cid:17)
Combining (B.10), (B.14) and (B.15), we have
b
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
(cid:12)
(cid:12) d b b (cid:12)
(cid:12) (cid:12)
≤
O˜(√d) kφ i(s,a)1
i kΣ
h⋆−1.
i=1
X
Define Γ h(s,a) = O˜(√d) d i=1kφ i(s,a)1
i kΣ
h⋆−1, we concludes the proof by invoking Lemma 4.6.
P
B.4 Proof of Corollary 5.3
In this section, we prove Corollary 5.3. We start with an interesting phenomenon, we call ‘range
shrinkage’, stated in the following lemma.
Lemma B.3 (Range Shrinkage). For any (ρ,π,h) (0,1] Π [H], we have
∈ × ×
1 (1 ρ)H−h+1
π,ρ π,ρ
maxV (s) minV (s) − − . (B.16)
s∈S h − s∈S h ≤ ρ
Proof of Corollary 5.3. By the fact that the variance of a random variable can be upper bounded
by the square of its range and Lemma B.3, for all (s,a,h) [H], we have
∈ S ×A×
1 (1 ρ)H−h+1 2 1 (1 ρ)H 2
[VV⋆ ](s,a) − − − − .
h+1 ≤ ρ ≤ ρ
(cid:16) (cid:17) (cid:16) (cid:17)
20Then we have
K φτφτ⊤ 1 K φτφτ⊤ 1
h h + I h h + I.
Xτ=1
[V hV h⋆ +1](sτ h,aτ h) H2 (cid:23)
Xτ=1
(1−(1 ρ−ρ)H )2 H2
Thus we have
K φτφτ⊤ 1 −1 1 (1 ρ)H 2 K 1 −1
Σ⋆−1 = h h + I − − φτφτ⊤+ I .
h [V V⋆ ](sτ,aτ) H2 (cid:22) ρ h h H2
(cid:16)Xτ=1 h h+1 h h (cid:17) (cid:16) (cid:17) (cid:16)Xτ=1 (cid:17)
By Theorem 5.2, we have
H d
SubOpt(πˆ,s,ρ) ≤ O˜(√d) · P∈s Uu ρp
(P0)
Xh=1Eπ⋆,P hXi=1kφ i(s h,a h)1 i kΣ h⋆−1 (cid:12)s 1 = s
i
1 (1 ρ)H H d (cid:12)
≤
O˜(√d)
·
− ρ− P∈s Uu ρp
(P0)
Eπ⋆,P kφ i(s h,a h)1 i kΛ− h1 s 1 = s .
Xh=1 hXi=1 (cid:12) i
(cid:12)
This concludes the proof.
C Proof of the Information-Theoretic Lower Bound
In this section, we prove the information-theoretic lower bound. We first introduce the construc-
tion of hard instances in Appendix C.1, then we prove Theorem 6.1 in Appendix C.2, and prove
Corollary 6.2 in Appendix C.3.
C.1 Construction of Hard Instances
We design a family of d-rectangular linear DRMDPs parameterized by a Boolean vector ξ =
ξ , where ξ 1,1 d. For a given ξ and uncertainty level ρ (0,3/4], the corresponding
h h∈[H] h
{ } ∈ {− } ρ ∈
d-rectangular linear DRMDP M has the following structure. The state space = x ,x and
ξ S { 1 2 }
the action space = 0,1 d. The initial state distribution µ is defined as
0
A { }
d+1 1
µ (x )= and µ (x )= .
0 1 0 2
d+2 d+2
The feature mapping φ: Rd+2 is defined as
S ×A →
d
a a a a
φ(x ,a)⊤ = 1 , 2 , , d ,1 i ,0
1
d d ··· d − d
(cid:16) Xi=1 (cid:17)
φ(x ,a)⊤ = 0,0, ,0,0,1 ,
2
···
which satisfies φ (s,a) 0 and d φ (s(cid:0) ,a) = 1. The fa(cid:1) ctor distributions µ are defined as
i ≥ i=1 i { h }h∈[H]
µ⊤P
= δ ,δ , ,δ ,δ ,δ , h [H],
h x1 x1 ··· x1 x1 x2 ∀ ∈
(cid:0) d+1 (cid:1)
| {z }
21ρ
1 x 1 x 2 1 1 ρ x 1 x 2 1
−
(a) The nominal environment. (b) The worst case transition at thefirst step.
Figure 1: The nominal environment and the worst case environment. The value on each arrow
represents the transition probability. The MDP has two states and H steps. For the nominal
environment, both x and x are absorbing states, which means that the state will always stay at
1 2
the initial state in the nominal environment. The worst case environment on the right is obtained
by perturbing the transition probability at the first step of the nominal environment, with others
remain the same.
so the transition is homogeneous and does not depend on action but only on state. The reward
parameters θ are defined as
h h∈[H]
{ }
ξ +1 ξ +1 ξ +1 1
θ⊤ =δ h1 , h2 , , hd , ,0 , h [H],
h · 2 2 ··· 2 2 ∀ ∈
(cid:16) (cid:17)
where δ is a parameter to control the differences among instances, which is to be determined later.
The reward r is generated from the normal distribution r (r (s ,a ),1), where r (s,a) =
h h h h h h
∼ N
φ(s,a)⊤θ . Note that
h
δ
r (x ,a) = φ(x ,a)⊤θ = ξ ,a +d 0 and r (x ,a) = φ(x ,a)⊤θ = 0, a ,
h 1 1 h h h 2 2 h
2d h i ≥ ∀ ∈ A
(cid:0) (cid:1)
which means that x is a worst state in terms of the mean reward. Thus, the worst case transition
2
kernel should have the highest possible transition probability to x . This construction is pivotal in
2
achieving aconciseexpressionofrobustvaluefunction. Further, weonlyconsidermodeluncertainty
in the first step. By the fact that x is the worse state, we know the worst case factor distribution
2
for the first step is
µˇ⊤ = (1 ρ)δ +ρδ ,(1 ρ)δ +ρδ , ,(1 ρ)δ +ρδ ,(1 ρ)δ +ρδ ,δ .
1 − x1 x2 − x1 x2 ··· − x1 x2 − x1 x2 x2
We illustrat(cid:0)e the designed d-rectangular linear DRMDP Mρ in Figure 1(a) and Figure 1(b).(cid:1)
ξ
Finally, we design the procedure for collecting the offline dataset. We assume the K trajectories
are collected by a behavior policy πb = πb defined as
{
h}h∈[H]
πb Unif e , ,e ,0 , h [H],
h ∼ { 1 ··· d } ∀ ∈
where e are the canonical basis v(cid:0)ectors in Rd. T(cid:1)he initial state is generated according to
i i∈[d]
{ }
µ . It is straightforward to check that the constructed hard instances satisfy Assumption 4.2. We
0
denote the offline dataset as .
D
C.2 Proof of Theorem 6.1
With this family of hard instances, we are ready to prove the information-theoretic lower bound.
First, we define some notations. For any ξ 1,1 dH, let Q denote the distribution of dataset
ξ
∈ {− }
collected from the MDP M . Denote the family of parameters as Ω = 1,1 dH and the family
ξ
D {− }
of hard instances as = M :ξ Ω .
ξ
M { ∈ }
22Proof of Theorem 6.1. The proof constitutes three steps. In the first step, we lower bound the
minimax suboptimality gap by testing error inthe following Lemma C.1, the fullproof of which can
be found in Appendix D.6.
Lemma C.1 (Reduction to testing). For the given family of d-rectangular linear DRMDPs, we
have
δdH
inf sup SubOpt(πˆ,x 1,ρ) (1 ρ) min inf Q ξ(ψ( ) = ξ)+Q ξ′(ψ( ) = ξ′) , (C.1)
πˆ M∈M ≥ − · 8d · ξ,ξ′∈Ω ψ D 6 D 6
DH(ξ,ξ′)=1 h i
where for fixed indices ξ and ξ′, ψ is any test function taking value in ξ,ξ′ .
{ }
In the second step, we lower bound the testing error on the right hand side of (C.1) in the
following Lemma C.2, the full proof of which can be found in Appendix D.7.
Lemma C.2 (Lowerboundontestingerror). Forthegivenfamilyofd-rectangularlinearDRMDPs,
let δ = d3/2/√2K, then we have
1
min inf Q ξ(ψ( ) = ξ)+Q ξ′(ψ( ) = ξ′) .
ξ,ξ′ ψ D 6 D 6 ≥ 2
DH(ξ,ξ′)=1 h i
By Lemma C.1 and Lemma C.2, we have
d3/2H
inf sup SubOpt(πˆ,x ,ρ) . (C.2)
1
πˆ M∈M ≥ 128√K
In the last step, we upper bound the uncertainty function Φ(Σ⋆,s) in the following Lemma C.3, the
h
full proof of which can be found in Appendix D.8.
Lemma C.3. For all M , when K O˜(d4), then with probability at least 1 δ, we have
ξ
∈M ≥ −
H d 4d3/2H
P∈s Uu ρp
(P0)
Xh=1Eπ⋆,P hXi=1kφ i(s h,a h)1 i kΣ h⋆−1 (cid:12)s 1 = x 1
i
≤ √K .
(cid:12)
By Lemma C.3 and (C.2), we know that with probability at least 1 δ, there exist a universal
−
constant c, such that
H d
in πˆf Msu ∈Mp SubOpt(πˆ,x 1,ρ) ≥ c · P∈s Uu ρp
(P0)
Xh=1Eπ⋆,P hXi=1kφ i(s h,a h)1 i kΣ h⋆−1 |s 1 = x 1 i.
This concludes the proof.
C.3 Proof of Corollary 6.2
Proof. TheresultinCorollary 6.2directlyfollows fromthefactshownin(D.38): fortheconstructed
hard instances, we have Σ⋆ = Λ . Thus, we complete the proof by directly substituting Σ⋆ in the
h h h
result of Theorem 6.1 by Λ .
h
23D Proof of Technical Lemmas
D.1 Proof of Lemma 4.6
Proof. First, we decompose SubOpt(πˆ,s,ρ) as follows
SubOpt(πˆ,s,ρ) =
Vπ⋆
(s)
Vρ (s)+Vρ
(s)
Vπˆ,ρ
(s),
1 − 1 1 − 1
I II
b b
then we bound term I and term II, respectively.
| {z } | {z }
Bounding term I: Note that
Vπ⋆,ρ (s) Vρ (s) = Qπ⋆,ρ (s,π⋆(s)) Qρ (s,πˆ (s))
h − h h h − h h
= Qπ⋆,ρ (s,π⋆(s)) Qρ (s,π⋆(s))+Qρ (s,π⋆(s)) Qρ (s,πˆ (s))
b h h − bh h h h − h h
Qπ⋆,ρ (s,π⋆(s)) Qρ (s,π⋆(s)). (D.1)
≤ h h − bh h b b
ρ
Where (D.1)holds by the fact that πˆ (s) is the greedy policy corresponding to Q (s,a), which leads
h b h
to Qρ (s,π⋆(s)) Qρ (s,πˆ (s)) 0. Further, by the robust Bellman equation (3.1), we have
h h − h h ≤
b
Qπ⋆,ρ (s,π⋆(s)) Qρ (s,π⋆(s))
bh h −b h h
= r (s,π⋆(s))+ inf [P Vπ⋆,ρ ](s,π⋆(s)) Qρ (s,π⋆(s))
h h Pb h(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h − h h
= r (s,π⋆(s))+ inf [P Vπ⋆,ρ ](s,π⋆(s)) rb(s,π⋆(s))
h h Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h − h h
inf [P Vρ ](s,π⋆(s))+r (s,π⋆(s))+ inf [P Vρ ](s,π⋆(s))
−Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h h h Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h
Qρ (s,π⋆(s)). b b
− h h
To proceed, we define the robust Bellman update error as follows
b
ζρ (s,a) = r (s,a)+ inf [P Vρ ](s,a) Qρ (s,a),
h h Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 − h
and denote the worst case transition kernel with respect tobthe estimatedbrobust value function as
P = {P h }h∈[H], where P h( ·|s,a) = arginf Ph(·|s,a)∈U hρ(s,a;µ0 h,i)[P hV hρ +1](s,a), ∀(s,a) ∈ S ×A, then we
have
b b b b
Qπ⋆,ρ (s,π⋆(s)) Qρ (s,π⋆(s))
h h − h h
= inf [P Vπ⋆,ρ ](s,π⋆(s)) inf [P Vρ ](s,π⋆(s))+ζρ (s,π⋆(s))
Ph(·|s,a)∈U hρ(s,a;µb0 h,i) h h+1 h −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h h h
P (Vπ⋆,ρ Vρ ) (s,π⋆(s))+ζρ (s,π⋆(s)). b (D.2)
≤ h h+1 − h+1 h h h
Combin(cid:2)ing (D.1) and (D.(cid:3)2), we have for any h [H],
b b ∈
Vπ⋆,ρ (s) Vρ (s) P (Vπ⋆,ρ Vρ ) (s,π⋆(s))+ζρ (s,π⋆(s)). (D.3)
h − h ≤ h h+1 − h+1 h h h
Recursively applying (D.3), we have (cid:2) (cid:3)
b b b
H
Vπ⋆,ρ
(s)
Vρ
(s)
Eπ⋆,Pb ζρ
(s ,a )s = s . (D.4)
1 − 1 ≤ h h h | 1
h=1
X (cid:2) (cid:3)
b
24Bounding term II: Note that Vρ (s) Vπˆ,ρ (s) = Qρ (s,πˆ (s)) Qπˆ,ρ (s,πˆ (s)), by the robust
h − h h h − h h
Bellman equation (3.1), we have
b b
ρ πˆ,ρ
V (s) V (s)
h − h
= Qρ (s,πˆ (s)) r (s,πˆ (s)) inf [P Vρ ](s,πˆ (s))
b h h − h h −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h
+b inf [P Vρ ](s,πˆ (s)) infb [P Vπˆ,ρ ](s,πˆ (s))
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h
= ζρ (s,πˆ (s))+ binf [P Vρ ](s,πˆ (s)) inf [P Vπˆ,ρ ](s,πˆ (s)).
− h h Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h
b
To proceed, we denote the worst case transition kernel with respect to the robust value function of
πˆ as Pπˆ = {P hπˆ }h∈[H], where P hπˆ( ·|s,a) = arginf Ph(·|s,a)∈U hρ(s,a;µ0 h,i)[P hV hπˆ +,ρ 1](s,a), then we have
Vρ
(s)
Vπˆ,ρ
(s)
ζρ
(s,πˆ (s))+
Pπˆ(Vρ Vπˆ,ρ
) (s,πˆ (s)). (D.5)
h − h ≤ − h h h h+1− h+1 h
Applying (D.5) recursively, we have (cid:2) (cid:3)
b b
H
Vρ
(s)
Vπˆ,ρ
(s)
Eπˆ,Pπˆ ζρ
(s ,a )s = s . (D.6)
1 − 1 ≤ − h h h | 1
h=1
X (cid:2) (cid:3)
b
ρ
Now it remains to bound the robust Bellman error ζ (, ). Next, we aim to show that for all
h · ·
(s,a,h) [H],
∈ S ×A× ρ
0 ζ (s,a) 2Γ (s,a).
≤ h ≤ h
Recall that ζ hρ (s,a) = r h(s,a)+inf Ph(·|s,a)∈U hρ(s,a;µ0 h,i)[P hV hρ +1](s,a) −Qρ h(s,a), by the definition of
ρ
Q (s,a) and the condition in (4.8), we have
h b b
b ζ hρ (s,a)
≥
Ph(·|s,a)∈i Un hρf
(s,a;µ0
h,i)[P hV hρ +1](s,a) −Ph(·|s,a)\ ∈i Un hρf
(s,a;µ0
h,i)[P hV hρ +1](s,a)+Γ h(s,a)
≥
0.
b b
On the other hand,
r (s,a)+ \ inf [P Vρ ](s,a) Γ (s,a)
h Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 − h
r (s,a)+ inf [bP Vρ ](s,a)
≤ h Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
H h+1. b
≤ −
ρ
Then by the definition of Q (s,a), we have
h
ζ hρ (s,a)
≤
Ph(·|s,a)∈i Un hρfb
(s,a;µ0
h,i)[P hV hρ +1](s,a) −Ph(·|s,a)\ ∈i Un hρf
(s,a;µ0
h,i)[P hV hρ +1](s,a)+Γ h(s,a)
2Γ (s,a). b b
h
≤
Thus, for all (s,a,h) [H], we have
∈ S ×A×
ρ
0 ζ (s,a) 2Γ (s,a). (D.7)
≤ h ≤ h
25Combining (D.4), (D.6) and (D.7), we have
H H
SubOpt(πˆ,s,ρ)
Eπ⋆,Pb
ζ (s ,a )s = s +
Eπˆ,Pπˆ ζρ
(s ,a )s = s
≤ h h h | 1 − h h h | 1
h=1 h=1
X (cid:2) (cid:3) X (cid:2) (cid:3)
H
Eπ⋆,Pb
ζρ
(s ,a )s = s
≤ h h h | 1
h=1
X (cid:2) (cid:3)
H
sup
Eπ⋆,P ζρ
(s ,a )s = s
≤ h h h | 1
P∈Uρ(P0)
h=1
X (cid:2) (cid:3)
H
2 sup
Eπ⋆,P
Γ (s ,a )s = s .
h h h 1
≤ |
P∈Uρ(P0)
h=1
X (cid:2) (cid:3)
This concludes the proof.
D.2 Proof of Lemma 4.7
In this section, we prove Lemma 4.7. Before the proof, we first present several auxiliary lemmas.
Lemma D.1 (Reference-Advantage decomposition). There exist α , where α [0,H], i
i i∈[d] i
{ } ∈ ∀ ∈
[d], such that
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
(cid:12)
(cid:12) d b d b (cid:12)K
(cid:12) (cid:12)
≤ λ kφ i(s,a)1 i kΛ− h1 kEµ0 h[V h⋆ +,ρ 1(s)] αikΛ− h1+ kφ i(s,a)1 i kΛ− h1 φτ hη hτ([V h⋆ +,ρ 1] αi) Λ−1
Xi=1 Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
i (cid:13) ii (cid:13)
d
+| λ kφ i(s,a)1 i kΛ{ − hz 1 Eµ0 h [V hρ +1(s)] αi } −[V| h⋆ +,ρ 1(s)] αi Λ−1 {z }
Xi=1 (cid:13)
(cid:13) (cid:2)
(cid:3)(cid:13)
(cid:13)
h
b
(cid:13) iii (cid:13)
d K
+| kφ i(s,a)1 i kΛ− h1 φτ hη{ hτz ([V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi)} Λ−1,
Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
b
(cid:13) iv (cid:13)
where ητ(|[f] ) = P0[f] (sτ,aτ) {[fz(sτ )] , for any function }f : [0,H 1].
h αi h αi h h − h+1 αi S → −
tL he em nm wa
ρ
sD at.2 isfi( eB sou(cid:0)n(cid:2)dofWe(cid:3)ights). Foranyh
∈
[H(cid:1)],denotetheweightw hρ = θ h+νˆ hρ inAlgorithm 1,
h
wρ
2H dK/λ.
k
hk2
≤
Lemma D.3. (Jin et al., 2021, Lemma B.2) Let fp: [0,R 1] be any fixed function. For any
S → −
δ (0,1), we have
∈
K
2 1 K
P φτ ητ(f) R2 2log +dlog 1+ δ.
h· h Λ−1 ≥ δ λ ≤
(cid:16)(cid:13)Xτ=1 (cid:13) h (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:17)
(cid:13) (cid:13)
(cid:13) (cid:13)
26Lemma D.4 (Covering number of function class ). For any h [H], let denote a class of
h h
V ∈ V
functions mapping from to R with the following parametric form
S
d
(s) = max φ(s,a)⊤θ β φ (s,a)1⊤Σ−1φ (s,a)1
Vh a∈A − i i h i i [0,H−h+1]
n Xi=1q o
where the parameters (θ,β,Σ ) satisfy θ L, β [0,B], λ (Σ ) λ. Assume φ(s,a)
h min h
k k ≤ ∈ ≥ k k ≤
1 for all (s,a) pairs, and let (ǫ) be the ǫ-covering number of with respect to the distance
h
N V
dist(V ,V )= sup V (x) V (x). Then
1 2 x| 1 − 2 |
log (ǫ) dlog(1+4L/ǫ)+d2log 1+8d1/2B2/(λǫ2) .
h
N ≤
(cid:2) (cid:3)
Lemma D.5. (Vershynin, 2018, Covering number of an interval) Denote the ǫ-covering number of
the closed interval [a,b] for some real number b > a with respect to the distance metric d(α ,α )=
1 2
α α as ([a,b]). Then we have ([a,b]) 3(b a)/ǫ.
1 2 ǫ ǫ
| − | N N ≤ −
Proof of Lemma 4.7. To prove this lemma, we bound terms i-iv in Lemma D.1at each steph [H],
∈
respectively. To deal with the temporal dependency, we follow the induction procedure proposed in
Xiong et al. (2022) and make essential adjustments to adapt to the robust setting.
The base case. We start from the last step H. By the fact that any robust value function is
upper bounded by H, then with λ = 1, for all (s,a) , we have
∈ S ×A
d
i+iii
≤
2H kφ i(s,a)1 i kΛ− H1. (D.8)
i=1
X
⋆,ρ
Next, we bound term ii. Note that V is independent of , we can directly apply Hoeffding-
H+1 D
type self-normalized concentration inequality Lemma F.1 and a union bound to obtain the upper
bound. In concrete, we define the filtration = σ( (sj ,aj ) τ sj τ−1). Since V⋆,ρ is
Fτ−1,h { h h }j=1 ∪{ h+1}j=1 H+1
independent of and is upper bounded by H, thus we have ητ ([V⋆,ρ ] ) is mean zero, i.e.,
E[ητ ([V⋆,ρ ] )D ] = 0 and H-subGaussian. By LemmaH F.1,H f+ or1 aα ni y|F fiτ x− e1 d,H index i [d], with
H H+1 αi |Fτ−1,H ∈
probability at least 1 δ/2dH2, we have
−
K 2 2dH2det(Λ )1/2
φτ ητ ([V⋆,ρ ] ) 2H2log H .
H H H+1 αi Λ−1 ≤ δdet(λI)1/2
(cid:13)Xτ=1 (cid:13) H (cid:16) (cid:17)
(cid:13) (cid:13)
(cid:13) (cid:13)
By the proof of Lemma B.2 in Jin et al. (2021), we know det(Λ ) (λ+K)d. Thus, we have
H
≤
K 2 d λ+K 2dH2 2dH2K
φτ ητ ([V⋆,ρ ] ) 2H2 log +log dH2log .
H H H+1 αi Λ−1 ≤ 2 λ δ ≤ δ
(cid:13)Xτ=1 (cid:13) H (cid:16) (cid:17)
(cid:13) (cid:13)
(cid:13) (cid:13)
Then by a union bound over i [d], with probability at least 1 δ/2H2, we have
∈ −
d
ii
≤
√dH√ι kφ i(s,a)1 i kΛ− H1, (D.9)
i=1
X
27where ι = log(2dH2K/δ) 1. As for the term iv, by construction we have V⋆,ρ = Vρ = 0 with
≥ H+1 H+1
probability 1. Thus, we trivially have
b
d
iv
≤
√dH√ι kφ i(s,a)1 i kΛ− H1. (D.10)
i=1
X
Combining (D.8), (D.9) and (D.10), for all (s,a) , with probability at least 1 δ/2H2, we
∈ S ×A −
have
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
(cid:12)PH(·|s,a)∈U Hρ(s,a;µ0 H,i) H H+1 −PH(·|s,a)∈U Hρ(s,a;µ0 H,i) H H+1
(cid:12)
(cid:12) d b b (cid:12)
(cid:12) (cid:12)
≤
4√dH√ι kφ i(s,a)1 i kΛ− H1. (D.11)
i=1
X
Thus, we define Γ H(s,a) := 4√dH√ι d i=1kφ i(s,a)1 i kΛ− H1. By the definition of Qρ H(s,a) in
Algorithm 1, we have
P
b
ρ ⋆,ρ
Q (s,a) = r (s,a) Γ (s,a) r (s,a) = Q (s,a),
H H − H [0,1] ≤ H H
(cid:8) (cid:9) ⋆,ρ ρ
which implies that bthe pessimism is achieved at step H, i.e., V (s) V (s), s . Next, we
H ≥ H ∀ ∈ S
⋆,ρ ρ
study V (s) V (s). The intuition is that given the estimation error bound in (D.11), with
H − H
⋆,ρ ρ b
sufficient data, the difference between V (s) and V (s) should be small. Specifically, we have
H H
b
V⋆,ρ (s) Vρ (s) = Q⋆,ρ (s,π⋆ (s)) Qρ (s,π⋆ (s))+Qρ (s,π⋆ (s)) Qρ (s,πˆ(s))
H − H H H − H H b H H − H
r (s,π⋆ (s))+ inf [P Vρ ](s,a)
b ≤ H H PHb(·|s,a)∈U Hρ(s,a;µ0 H,ib) H H+1 −b
r (s,π⋆ (s)) \ inf [P Vbρ ](s,a)+Γ (s,π⋆ (s)) (D.12)
H H −PH(·|s,a)∈U Hρ(s,a;µ0 H,i) H H+1 H H
2Γ (s,π⋆ (s)). b
≤ H H
where(D.12)holdsbytherobustBellmanequation(3.1)andthefactthatQρ (s,π⋆ (s)) Qρ (s,πˆ(s))
H H − H ≤
0. Then webound thepessimismterm Γ (s,a) interms of thesample sizeK.ByLemma F.3, when
H
K max 512log(2dH2/δ)/κ2,4/κ , with probability at least 1 δ/2H2b, we have b
≥ { } −
d 16√dH√ι d
2Γ H(s,a) = 8√dH√ι kφ i(s,a)1 i kΛ− H1
≤ √K
φ i(s,a) Λ˜− H1 1 ii/2 ,
i=1 i=1
X X (cid:0) (cid:1)
where Λ˜ = Eπb,P0 [φ(s ,a )φ(s ,a )⊤]. Note that for any positive definite matrix A, we know
H H H H H
λ (A) A λ (A). Thus, by Assumption 4.2, we have
min ii max
≤ ≤
16√dH 1√ι
2Γ (s,a) · := R . (D.13)
H H
≤ √Kκ
To summarize, we define the event
⋆,ρ ρ
= 0 V (s) V (s) R , s ,
EH ≤ H − H ≤ H ∀ ∈ S
then by aunion bound over (D.11)(cid:8)and (D.13), we know holds with(cid:9)probability at least 1 δ =
b EH − H
1 δ/H2. This is the base case.
−
28Inductive Hypothesis. Suppose with probability at least 1 δ , we have
h+1
−
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
(cid:12)Ph+1(·|s,a)∈U hρ +1(s,a;µ0 h+1,i) h+1 h+2 −Ph+1(·|s,a)∈U hρ +1(s,a;µ0 h+1,i) h+1 h+2
(cid:12)
(cid:12) d b b (cid:12)
(cid:12) (cid:12)
≤
4√dH√ι kφ i(s,a)1 i kΛ− h+1
1
:= Γ h+1(s,a), (D.14)
i=1
X
and
16√dH(H h)√ι
= 0 V⋆ (s) V (s) R := − , s . (D.15)
Eh+1 ≤ h+1 − h+1 ≤ h+1 √Kκ ∀ ∈ S
(cid:8) (cid:9)
b
Inductive Step. Next, we establish the result for step h. First, terms i, ii and iii at step h can
be similarly bounded as in the base case, i.e., we have
d
i+ii+iii
≤
3√dH√ι kφ i(s,a)1 i kΛ− h1, (D.16)
i=1
X
withprobability atleast1 δ/3H2. Itremainstoboundthetermivandensureitisnon-dominating.
− ρ ⋆,ρ
Here, we need to deal with the temporal dependency, as [V (s)] [V (s)] is correlated to
h+1 α − h+1 α
(sτ,aτ,sτ ) K , thus we need a uniform concentration argument. Consider the function class
{ h h h+1 }τ=1
b
(D,B,λ) = V (s;θ,β,Σ) : [0,H] with θ D,β [0,B],Σ λI ,
h h
V { S → k k≤ ∈ (cid:23) }
where V (s;θ,β,Σ) = max φ(s,a)⊤θ β d φ (s,a)1⊤Σ−1φ (s,a)1 . For sim-
h a∈A { − i=1 i i i i }[0,H−h+1]
plicity, we denote f (s):= [Vρ (s)] [V⋆,ρ (s)] ,qthen f (α ), where
αi h+1 αi − h+1P αi αi ∈Fh+1 i
ρ ⋆,ρ ρ
Fh+1(α) := b[V h+1(s)]
α
−[V h+1(s)]
α
: V h+1(s)
∈
Vh+1(D 0,B 0,λ) .
Note that for any fixed α, th(cid:8)e covering number of (α) is the same as that of(cid:9) (D ,B ,λ). By
b Fh+1b Vh 0 0
Lemma D.2,wehaveD = H Kd/λ. Bytheinductionassumption(D.14),wehaveB = 4√dH√ι.
0 0
Denote the ǫ-covering of the interval [0,H] with respect to the distance dist(α ,α ) = α α as
p 1 2 | 1 − 2 |
(ǫ), and its ǫ-covering number as (ǫ). For each α [0,H], we can find α (ǫ)
[0,H] [0,H] ǫ [0,H]
N |N | ∈ ∈ N
such that α α ǫ. For any fixed α [0,H], we denote the ǫ-covering of (α) with respect
ǫ h+1
| − | ≤ ∈ F
to the distance dist(f ,f ) = sup f (x) f (x) as (ǫ) (short for (ǫ;D,B,λ)) and its
1 2 x| 1 − 2 | Nh+1 Nh+1
ǫ-covering number as (ǫ). For each f (α), we can find fǫ (ǫ) such that
|Nh+1 | α ∈ Fh+1 α ∈ Nh+1
sup f (s) fǫ(s) ǫ. It follows that
s| α − α | ≤
K
2
φτητ(f ) 1 f R
h h αi Λ−1 · k αik∞ ≤ h+1
(cid:13) (cid:13)Xk=1 (cid:13)
(cid:13)
h
(cid:8) (cid:9)
(cid:13) K (cid:13) 2 K 2
2 φτητ(f ) 1 f R +ǫ +2 φτ ητ(f ) ητ(f ) .
≤ h h αiǫ Λ−1 · k αiǫk∞ ≤ h+1 h h αi − h αiǫ Λ−1
(cid:13) (cid:13)Xτ=1 (cid:13)
(cid:13)
h
(cid:8) (cid:9)
(cid:13) (cid:13)Xk=1
(cid:0)
(cid:1)(cid:13)
(cid:13)
h
Note that(cid:13) (cid:13) (cid:13) (cid:13)
K K
2 φτ ητ(f ) ητ(f ) 2 2ǫ2 φτΛ−1φτ′ 2ǫ2K2/λ.
h h αi − h αiǫ Λ−1 ≤ h h h ≤
(cid:13) (cid:13)Xk=1
(cid:0)
(cid:1)(cid:13)
(cid:13)
h τ, Xτ′=1
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:13) (cid:13)
29Then we have
K
2
φτητ(f ) 1 f R
h h αi Λ−1 · k αik∞ ≤ h+1
(cid:13) (cid:13)Xk=1 (cid:13)
(cid:13)
h
(cid:8) (cid:9)
(cid:13) K (cid:13) 2
4 φτητ(fǫ ) 1 fǫ R +2ǫ
≤ h h αiǫ Λ−1 · k αiǫk∞ ≤ h+1
(cid:13) (cid:13)Xτ=1 (cid:13)
(cid:13)
h
(cid:8) (cid:9)
(cid:13) K (cid:13) 2 2ǫ2K2
+4 φτ ητ(f ) ητ(fǫ ) +
h h αiǫ − h αiǫ Λ−1 λ
(cid:13) (cid:13)Xk=1
(cid:0)
(cid:1)(cid:13)
(cid:13)
h
(cid:13)K 2 (cid:13) 6ǫ2K2
4 φτητ(fǫ ) 1 fǫ R +2ǫ + ,
≤ h h αiǫ Λ−1 · k αiǫk∞ ≤ h+1 λ
(cid:13) (cid:13)Xτ=1 (cid:13)
(cid:13)
h
(cid:8) (cid:9)
where the last inequali(cid:13)ty holds by the (cid:13)fact that
K K
4 φτ ητ(f ) ητ(fǫ ) 2 4ǫ2 φτΛ−1φτ′ 4ǫ2K2/λ.
h h αiǫ − h αiǫ Λ−1 ≤ h h h ≤
(cid:13) (cid:13)Xk=1
(cid:0)
(cid:1)(cid:13)
(cid:13)
h τ, Xτ′=1
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:13) (cid:13)
With a union bound over (ǫ) and (ǫ) and by Lemma D.3, we have
h+1 [0,H]
N N
K
2
P sup φτητ(fǫ ) 1 fǫ R +2ǫ
( fα αǫi iǫ ǫ∈ ∈N N[0 h, +H 1]( (ǫ ǫ) )(cid:13)
(cid:13)
(cid:13)Xτ=1
h h αiǫ
(cid:13)
(cid:13)
(cid:13)Λ− h1 · (cid:8)k αiǫk∞ ≤ h+1
(cid:9)
> (R +2ǫ)2 2log
3dH2 |Nh+1(ǫ) ||N[0,H](ǫ)
| +dlog 1+
K δ
.
h+1 δ λ ≤ 3dH2
)
(cid:16) (cid:16) (cid:17)(cid:17)
Then with probability at least 1 δ/3dH2, for all f (α ), we have
−
αi
∈
Fh+1 i
K
2
φτητ(f ) 1 f R
h h αi Λ−1 · k αik∞ ≤ h+1
(cid:13)
(cid:13)
(cid:13)Xk= 41
inf (R
(cid:13)
(cid:13) (cid:13)
+h
2ǫ)2
(cid:8)
2log
3dH2 |Nh+(cid:9) 1(ǫ) ||N[0,H](ǫ)
| +dlog 1+
K
+
6ǫ2K2
.
h+1
≤ ǫ>0 δ λ λ
n (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17) o
By Lemma D.4 and Lemma D.5 together with D = H Kd/λ and B = 4√dH√ι, setting ǫ =
0 0
d3/2H2/(K3/2√κ)andK √dH/(32√κι),wehavelog (ǫ) 2d2log(512K3ι/d3/2H2). Thus,
≥
|Nph+1
| ≤
we have
K 2 512dH4ι 2dH2 512K3ι
φτητ(f ) 1 f R 2log +4d2log
h h αi Λ−1 · k αik∞ ≤ h+1 ≤ Kκ δ d3/2H2
(cid:13)
(cid:13)
(cid:13)Xk=1 (cid:13)
(cid:13)
(cid:13)
h
(cid:8) (cid:9)
20480d3H(cid:16)
4ι2
(cid:17)
.
≤ Kκ
Then, with a union bound over i [d], we have
∈
K 143d3/2H2ι
P sup φτητ [Vρ ] [V⋆,ρ ] >
(cid:18)i∈[d] (cid:13) (cid:13)Xτ=1
h h
(cid:0)
h+1 αi − h+1 αi
(cid:1)(cid:13)
(cid:13)Λ− h1 √Kκ
(cid:19)
b
(cid:13) (cid:13)
30K
P sup φτητ [Vρ ] [V⋆,ρ ] 1 [Vρ ] [V⋆,ρ ] R
≤ >(cid:18) 1i 4∈ 3[d d] 3(cid:13)
(cid:13)
(cid:13)/2Xτ H=1
2ι
h +h
(cid:0)
Pbh 1+1 α [i V−
ρ
]h+1 α [i V(cid:1)(cid:13)
(cid:13)
(cid:13)⋆,Λ ρ− h ]1
(cid:8)(cid:13) (cid:13)
>bh R+1 αi − h+1 αi
(cid:13)
(cid:13)∞ ≤ h+1
(cid:9)
√Kκ h+1 αi − h+1 αi ∞ h+1
(cid:19)
δ (cid:0) (cid:8)(cid:13) (cid:13) (cid:9)(cid:1)
+δ , (cid:13) b (cid:13)
≤ 3H2 h+1
which implies with probability at least 1 δ/3H2 δ , the term iv at step h can be bounded as
h+1
− −
143d3/2H2ι d
iv
≤ √Kκ
kφ i(s,a)1 i kΛ− h1. (D.17)
i=1
X
Then by a union bound over (D.16) and (D.17), if K > 20449d2H2/κ, then with probability at least
1 2δ/3H2 δ we have
h+1
− −
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
(cid:12)
(cid:12) d b b (cid:12)
(cid:12) (cid:12)
≤
4√dH√ι kφ i(s,a)1 i kΛ− h1 := Γ h(s,a). (D.18)
i=1
X
Further, when K > max 512log(3H2/δ)/κ2,4/κ , by Lemma F.3, with probability at least 1
{ } −
δ/3H2, we have
d 8√dH√ι
Γ h(s,a)
≤
4√dH√ι kφ i(s,a)1 i kΛ− h1
≤ √Kκ
. (D.19)
i=1
X
Then by a union bound over (D.18) and (D.19), under the event , with probability at least
h+1
E
1 δ/H2 δ , we have
h+1
− −
⋆,ρ ρ
V (s) V (s)
h − h
= Q⋆,ρ (s,π⋆(s)) Qρ (s,π⋆(s))+Qρ (s,π⋆(s)) Qρ (s,πˆ(s))
h b − h h − h
inf [P V⋆,ρ ](s,π⋆(s)) \ inf [P Vρ ](s,a)+Γ (s,π⋆(s))
≤ Ph(·|s,a)∈U hρ(s,a;µ0 h,ib ) h h+1 b −Ph(·|s,a)∈Ub hρ(s,a;µ0 h,i) h h+1 h
= inf [P V⋆,ρ ](s,π⋆(s)) inf [P Vbρ ](s,π⋆(s))
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
+ inf [P Vρ ](s,π⋆(s)) \ inf [PbVρ ](s,a)+Γ (s,π⋆(s))
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 h
R +2Γ (s,π⋆(s)) b b (D.20)
h+1 h
≤
16√dH(H h)√ι 16√dH√ι
− +
≤ √Kκ √Kκ
16√dH(H h+1)√ι
= − := R .
h
√Kκ
31where (D.20) holds by the following argument
inf [P V⋆,ρ ](s,π⋆(s)) inf [P Vρ ](s,π⋆(s))
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
[Pˆ V⋆,ρ ](s,π⋆(s)) [Pˆ Vρ ](s,π⋆(s)) b
≤ h h+1 − h h+1
⋆,ρ ρ
sup V (s) V (s)
≤ | h+1 − h+1 | b
s
R (D.21)
h+1 b
≤
where Pˆ h( ·|s,a) = arginf Ph(·|s,a)∈U hρ(s,a;µ0 h,i)[P hV hρ +1](s,a), ∀(s,a) ∈ S ×A, and (D.21) is due to the
induction assumption (D.15). Finally, denote
b
⋆,ρ ρ
= 0 V (s) V (s) R , s ,
Eh { ≤ h+1 − h+1 ≤ h ∀ ∈ S}
then we have P( h) δ h+1+δ/H2 := δ h. b
E ≤
Generalization. By induction and a union bound over h [H], setting
∈
d
Γ h(s,a) = 4√dH√ι kφ i(s,a)1 i kΛ− h1,
i=1
X
then with probability at least 1 (δ/H2+2δ/H2+ +Hδ/H2) = 1 dH(H +1)δ/2H2 > 1 δ,
− ··· − −
for all (s,a,h) [H], we have
∈S ×A×
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a) Γ (s,a).
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
(cid:12)
≤ h
(cid:12) (cid:12)
b b
This concl(cid:12)udes the proof. (cid:12)
D.3 Proof of Lemma B.1
Proof. Note that the conditional variance estimation does not involve any element of model uncer-
tainty, and thus the proof follows from Lemma 5 of Xiong et al. (2022). Recall that we estimate
[V [Vρ ] ](s,a) based on ′ as
h h+1 α D
√dH3
σ2(s,a;α) = max 1, φ(s,a)⊤β˜ (α) φ(s,a)⊤β˜ (α) 2 O˜ .
h h,2 [0,H2]− h,1 [0,H]− √Kκ
n (cid:16) (cid:17)o
(cid:2) (cid:3) (cid:2) (cid:3)
Note thbat
φ(s,a)⊤β˜ (α) φ(s,a)⊤β˜ (α) 2 [P [V′ρ ]2](s,a) ([P [V′ρ ] ](s,a))2
h,2 [0,H2]− h,1 [0,H]− h h+1 α − h h+1 α
(cid:12) (cid:12)(cid:2) φ(s,a)⊤β˜ ((cid:3)α) (cid:2) [P [V′ρ ]2](s,(cid:3)a) + φ(s,a)⊤β˜ (α) 2 ([P [V′ρ ] ](s,(cid:12) (cid:12)a))2
(cid:12)≤ h,2 [0,H2]− h h+1 α b h,1 [0,H]− b h h+1 α (cid:12)
(cid:12) (cid:12)(cid:2)φ(s,a)⊤β˜ (α) (cid:3) [P [V′ρ ]2](s,a) +2H (cid:12) (cid:12)φ(s(cid:12) (cid:12),(cid:2)a)⊤β˜ (α) [P(cid:3)[V′ρ ] ](s,a) (cid:12) (cid:12)
≤ (cid:12) h,2 − h h+1 αb (cid:12) (cid:12) h,1 − h h+1 α b (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) i (cid:12) (cid:12) ii (cid:12)
b b
(cid:12) (cid:12) (cid:12) (cid:12)
| {z } | {z }
32Note that the estimation error i and ii both come from regular ridge regressions with targets
[V′ρ
(s)]2 and
[V′ρ
(s)] , respectively. Thus, the analysis is standard and for simplicity we omit
h+1 α h+1 α
the details here and focus on the results: with probability at least 1 δ/2, we have
−
b b
φ(s,a)⊤β˜ (α) φ(s,a)⊤β˜ (α) 2 [P [V′ρ ]2](s,a) ([P [V′ρ ] ](s,a))2
h,2 [0,H2]− h,1 [0,H]− h h+1 α − h h+1 α
(cid:12) (cid:12) (cid:12)(cid:2) O˜ dH2 . (cid:3) (cid:2) (cid:3) b b ((cid:12) (cid:12) (cid:12)D.22)
≤ √Kκ
(cid:16) (cid:17)
Then by Theorem 4.3 and Lemma F.3, for all (s,a,h) [H], with probability at least
∈ S × A ×
1 δ/2, we have
−
′ρ ⋆,ρ
[Var [V ] ](s,a) [Var [V ] ](s,a)
h h+1 α − h h+1 α
(cid:12) [P [V′ρ ]2](s,a) [P [V⋆,ρ ]2](s,a) +(cid:12) [P [V′ρ ] ](s,a) 2 [P [V⋆,ρ ] ](s,a) 2
(cid:12)≤ hb h+1 α − h h+1 α (cid:12) h h+1 α − h h+1 α
≤
2(cid:12) (cid:12)H bP h([V hρ +1] α −[V h⋆ +,ρ 1] α) (s,a) +(cid:12) (cid:12)2H(cid:12) (cid:12)(cid:0)P h [ bV h⋆ +,ρ 1] α −[V h⋆ (cid:1) +,ρ 1] α(cid:0) (s,a) (cid:1) (cid:12)
(cid:12)
O˜ (cid:12)
(cid:12)√ (cid:2)dH3
b . (cid:3) (cid:12) (cid:12) (cid:12) (cid:12)(cid:2) (cid:0) (cid:1)(cid:3) (cid:12) (cid:12) (D.23)
≤ √Kκ
(cid:16) (cid:17)
By (D.22) and (D.23) and a union bound, we know that with probability at least 1 δ, we have
−
φ(s,a)⊤β˜ (α) φ(s,a)⊤β˜ (α) 2 [Var [V⋆,ρ ] ](s,a)
h,2 [0,H2]− h,1 [0,H]− h h+1 α
(cid:12) (cid:12)(cid:2) φ(s,a)⊤β˜ ((cid:3)α) (cid:2) φ(s,a)⊤β˜ ((cid:3)α) 2 [Var [V′ρ ] ](s,(cid:12) (cid:12)a)
(cid:12)≤ h,2 [0,H2]− h,1 [0,H]− h h+1 α (cid:12)
+(cid:12) (cid:12) (cid:12)(cid:2) [Var h[V h′ +ρ 1] α](s(cid:3),a) −[V(cid:2)ar h[V h⋆ +,ρ 1] α](s,a)(cid:3)
b
(cid:12) (cid:12)
(cid:12)
O˜(cid:12)
(cid:12)
dH3
b,
(cid:12)
(cid:12)
≤ √Kκ
(cid:16) (cid:17)
which implies that
dH3
φ(s,a)⊤β˜ (α) φ(s,a)⊤β˜ (α) 2 O˜ [Var [V⋆,ρ ] ](s,a).
h,2 [0,H2]− h,1 [0,H]− √Kκ ≤ h h+1 α
(cid:16) (cid:17)
(cid:2) (cid:3) (cid:2) (cid:3)
By the fact that the operator min 1, is order preserving, thus we have
{ ·}
σ2(s,a;α) [V [V⋆,ρ ] ](s,a).
h ≤ h h+1 α
Further, by the fact that the operator min 1, is a contraction map, (D.22) and (D.23), we have
b { ·}
σ2(s,a;α) V [V⋆,ρ ] (s,a)
h − h h+1 α
(cid:12)
(cid:12)≤b
σ h2(s,a;α)(cid:2)
−
V h[V h′ +ρ (cid:3) 1] α (s,(cid:12) (cid:12)a) + V h[V h′ +ρ 1] α (s,a)
−
V h[V h⋆ +,ρ 1] α (s,a)
≤
(cid:12) (cid:12)bφ(s,a)⊤β˜ h,2(cid:2) (α) [b 0,H2]−(cid:3) φ(s,(cid:12) (cid:12)a)⊤(cid:12) (cid:12)β(cid:2)˜ h,1(αb) 2 [0,H(cid:3) ]−O˜ √d (cid:2)H K3
κ
−[Va(cid:3) r h[V h′ +ρ(cid:12) (cid:12) 1] α](s,a)
+(cid:12) (cid:12) (cid:12)(cid:2) [Var h[V h′ +ρ 1] α](s(cid:3) ,a) −[V(cid:2) ar h[V h⋆ +,ρ 1] α](s,a)(cid:3) (cid:16) (cid:17) b (cid:12) (cid:12)
(cid:12)
O˜(cid:12)
(cid:12)
dH2
b+O˜
dH3
+O˜
√dH3
(cid:12)
(cid:12)
≤ √Kκ √Kκ √Kκ
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
dH3
= O˜ .
√Kκ
(cid:16) (cid:17)
This concludes the proof.
33D.4 Proof of Lemma B.2
Proof. Note that the reference-advantage decomposition is exactly the same as that in the proof of
Lemma D.1, thus we have
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
d b b
≤
φ i(s,a)1⊤
i
Eµ0 h[V h⋆ +,ρ 1(s)]
αi
−Eµ0 h[V h⋆ +,ρ 1(s)]
αi
i=1
X (cid:0) (cid:1)
b
reference uncertainty
d
| {z }
+ φ i(s,a)1⊤ i Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi −Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi .
i=1
X (cid:0) (cid:2) (cid:3) (cid:2) (cid:3)(cid:1)
b b b
advantage uncertainty
Next, we fur|ther decompose the reference uncertai{nzty and the advantage uncertainty, resp}ectively.
The Reference Uncertainty. Specifically, we have
d
φ i(s,a)1⊤
i
Eµ0 h[V h⋆ +,ρ 1(s)]
αi
−Eµ0 h[V h⋆ +,ρ 1(s)]
αi
i=1
X (cid:0) (cid:1)
d b K φτ P0[V⋆,ρ ] (sτ,aτ)
= φ i(s,a)1⊤
i
Eµ0 h[V h⋆ +,ρ 1(s)]
αi
−Σ− h1(α i) h σh 2(sh τ+ ,1 aα τi
;α
)h h
Xi=1 (cid:16) Xτ=1 (cid:2) h h h (cid:3) i
K φτ P0[V⋆,ρ ] (sτ,aτ) K φτ[V⋆,ρ (sτ )]
+Σ−1(α ) h h h+1 αi h h Σ−1(α ) b h h+1 h+1 αi
h i σ2(sτ,aτ;α ) − h i σ2(sτ,aτ;α )
Xτ=1 (cid:2) h h h (cid:3) i Xτ=1 h h h i (cid:17)
d d K φτητ([V⋆,ρ ] )
= λ φ i(s,a)1⊤
i
Σ− h1b(α i)Eµ0 h[V h⋆ +,ρ 1(s)]
αi
+ φ i(s,a)1⊤ ibΣ− h1(α i) σh 2(h sτ,ah τ+ ;1 αα )i
i=1 i=1 τ=1 h h h i
X X X
d
≤
λ kφ i(s,a)1 i kΣ− h1(αi)kEµ0 h[V h⋆ +,ρ 1(s)] αikΣ− h1(αi) b
i=1
X
i
+| Xi=d 1kφ i(s,a)1 i kΣ− h1(αi{ )z (cid:13)XτK
=1
φ στ h h2η (hτ s( τ h[ ,V ah⋆ τ h+, ;ρ 1 α] α i)i) (cid:13)Σ} − h1(αi)
(cid:13) (cid:13)
(cid:13) ii (cid:13)
b
| {z }
TheAdvantage Uncertainty. Similartotheargumentindecomposingthereferenceuncertainty,
we have
d
φ i(s,a)1⊤ i Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi −Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi
i=1
X (cid:0) (cid:2) (cid:3) (cid:2) (cid:3)(cid:1)
b b b
34d
≤ λ Xi=1kφ i(s,a)1 i kΣ− h1(αi)
(cid:13)
(cid:13)Eµ0 h (cid:2)[V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi
(cid:3)(cid:13)
(cid:13)Σ− h1(αi)
b
(cid:13) iii (cid:13)
+| Xi=d 1kφ i(s,a)1 i kΣ− h1(αi) (cid:13)XτK
=1
φ {τ hzη hτ([V bhρ + σ1 h2( (s s) τ h] ,α ai τ h− ;α[V i)h⋆ +,ρ 1(s)] αi) (cid:13)} Σ− h1(αi)
(cid:13) (cid:13)
(cid:13) iv (cid:13)
b
Put terms i-iv|together, we have {z }
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
d b b
≤
λ kφ i(s,a)1 i kΣ− h1(αi)kEµ0 h[V h⋆ +,ρ 1(s)] αikΣ− h1(αi)
i=1
X
i
+| Xi=d 1kφ i(s,a)1 i kΣ− h1(αi{ )z (cid:13)XτK
=1
φ στ h h2η (hτ s( τ h[ ,V ah⋆ τ h+, ;ρ 1 α] α i)i) (cid:13)Σ} − h1(αi)
(cid:13) (cid:13)
(cid:13) ii (cid:13)
b
d
+λ| Xi=1kφ i(s,a)1 i kΣ− h1(αi)
(cid:13)
(cid:13){ Ez µ0 h (cid:2)[V hρ +1(s)] αi −[V h⋆ +,ρ 1(} s)] αi
(cid:3)(cid:13)
(cid:13)Σ− h1(αi)
b
(cid:13) iii (cid:13)
+| Xi=d 1kφ i(s,a)1 i kΣ− h1(αi) (cid:13)XτK
=1
φτ hη{hτ z([V bhρ + σ1 h2( (s s) τ h] ,α ai τ h− ;α[V i)h⋆ +,ρ 1(s)] αi) (cid:13)Σ− h} 1(αi)
(cid:13) (cid:13)
(cid:13) iv (cid:13)
b
By similar argu|ment as Lemma D.1, we know t{hzere exist α˜ such that }
i i∈[d]
{ }
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
(cid:12)
(cid:12) d b b (cid:12)
(cid:12) (cid:12)
≤
λ kφ i(s,a)1 i kΣ− h1(α˜i)kEµ0 h[V h⋆ +,ρ 1(s)] αikΣ− h1(α˜i)
i=1
X
i
+| Xi=d 1kφ i(s,a)1 i kΣ− h1(α˜i{ )z (cid:13)XτK
=1
φ στ h h2η (hτ s( τ h[ ,V ah⋆ τ h+, ;ρ 1 α] α i)i) (cid:13)Σ} − h1(α˜i)
(cid:13) (cid:13)
(cid:13) ii (cid:13)
b
d
+λ| Xi=1kφ i(s,a)1 i kΣ− h1(αi)
(cid:13)
(cid:13){ Ez µ0 h (cid:2)[V hρ +1(s)] α˜i −[V h⋆ +,ρ 1(} s)] αi
(cid:3)(cid:13)
(cid:13)Σ− h1(α˜i)
b
(cid:13) iii (cid:13)
| {z }
35d K φτητ([Vρ (s)] [V⋆,ρ (s)] )
+ Xi=1kφ i(s,a)1 i kΣ− h1(α˜i)
(cid:13)Xτ=1
h h bh+ σ1 h2(sτ h,α˜ ai τ h− ;α i)h+1 αi (cid:13)Σ− h1(α˜i)
(cid:13) (cid:13)
(cid:13) iv (cid:13)
b
This concludes the|proof. {z }
D.5 Proof of Lemma B.3
Proof. By the robust bellman equation (3.1), we know
Vπ,ρ (s) = E [r(s,a)+ inf [P Vπ,ρ ](s,a)]. (D.24)
h a∼π(·|s) Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1
π,ρ
Then, we can trivially bound max V (s) as
s∈S h
maxVπ,ρ (s) max 1+ inf [P Vπ,ρ ](s,a) . (D.25)
s∈S h ≤ s,a
(cid:16)
Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1
(cid:17)
Further, by the definition of the d-rectangular uncertainty set, we have
d
inf [P Vπ,ρ ](s,a) = φ (s,a) inf E [Vπ,ρ (s)]. (D.26)
Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1
i=1
i µh,i∈U hρ ,i(µ0 h,i) s∼µh,i h+1
X
π,ρ π,ρ
Denoting s = argmax V (s)and s = argmin V (s), and for all i [d], we construct
max s∈S h+1 min s∈S h+1 ∈
a distribution µˇ = (1 ρ)µ +ρδ , where δ is the Dirac Delta distribution with mass on x.
Note that µˇ
h,i ρ (µ0−
),
thuh s,i
we
hs am vi en x
h,i ∈ Uh,i h,i
inf E [Vπ,ρ (s)] E [Vπ,ρ (s)] (1 ρ)maxVπ,ρ (s)+ρminVπ,ρ (s). (D.27)
µh,i∈U hρ ,i(µ0 h,i) s∼µh,i h+1 ≤ s∼µˇh,i h+1 ≤ − s∈S h+1 s h+1
Combining (D.25), (D.26) and (D.27), we have
π,ρ π,ρ π,ρ
maxV (s) (1 ρ)maxV (s)+ρminV (s)+1. (D.28)
s∈S h ≤ − s∈S h+1 s∈S h+1
π,ρ
On the other hand, by (D.24), we can trivially bound min V (s) as
s h
minVπ,ρ (s) min inf [P Vπ,ρ ](s,a). (D.29)
s h ≥ s,a Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1
By the fact that
inf E [Vπ,ρ (s)] minVπ,ρ (s), (D.30)
µh,i∈U hρ ,i(µ0 h,i) s∼µh,i h+1 ≥ s∈S h+1
combining (D.26), (D.29) and (D.30), we have
π,ρ π,ρ
minV (s) minV (s). (D.31)
s h ≥ s∈S h+1
For any h [H], by (D.28) and (D.31), we have
∈
π,ρ π,ρ
maxV (s) minV (s)
s∈S h − s∈S h
36π,ρ π,ρ π,ρ
1+(1 ρ)maxV (s) minV (s)+ρminV (s)
≤ − s∈S h+1 − s∈S h+1 s∈S h+1
π,ρ π,ρ
= 1+(1 ρ) maxV (s) minV (s) . (D.32)
− s∈S h+1 − s∈S h+1
(cid:2) (cid:3)
π,ρ
For step H, by the definition of the value function, we have 0 V (s) 1, s . Applying
π,ρ π,ρ ≤ H ≤ ∀ ∈ S
(D.32) with h = H 1 leads to max V (s) min V (s) 1+(1 ρ) 1. We finish the
− s∈S H−1 − s∈S H−1 ≤ − ·
proof by recursively applying (D.32).
D.6 Proof of Lemma C.1
Proof. The proof of Lemma C.1 consists of the following two steps:
Step 1: lower bound the suboptimality by Hamming distance. For any ξ 1,1 dH,
⋆,ρ ∈ {− }
denote V (s) as the optimal robust value function for the MDP instance M . For any function π,
ξ ξ
π,ρ
denote V as the robust value function corresponding to a policy π. Then by definition, we have
ξ
V⋆,ρ (x ) = max inf Eπ,P r (s ,a )+ +r (s ,a )s = x ,
ξ 1 π P∈Uρ(P0) 1 1 1 ··· H H H | 1 1
Vπ,ρ (x )= inf Eπ,P r (s(cid:2) ,a )+ +r (s ,a )s = x . (cid:3)
ξ 1 P∈Uρ(P0) 1 1 1 ··· H H H | 1 1
(cid:2) (cid:3)
For any given ξ, the optimal action at step h is
a⋆ = ((1+ξ )/2, ,(1+ξ )/2).
h h1 ··· hd
The worst case transition at the first step is known as
P (x x ,a) = (1 ρ), P (x x ,a) = ρ, P (x x ,a) = 1, a ,
1 1 1 1 2 1 1 2 2
| − | | ∀ ∈ A
and from the second step on, the state always stays at s . With these facts in mind, we have
2
d d d
1 1+ξ 1 1+ξ 1 1+ξ
⋆,ρ 1i 2i Hi
V (x ) = δ + +(1 ρ) + + +(1 ρ) +
ξ 1 2 4d − 2 4d ··· − 2 4d
nh Xi=1 i h Xi=1 i h Xi=1 io
d d d
δ 1+ξ 1+ξ 1+ξ
1i 2i Hi
= d+ +(1 ρ) d+ + +(1 ρ) d+ ,
2d 2 − 2 ··· − 2
nh Xi=1 i h Xi=1 i h Xi=1 io
and
d d d
δ
Vπ,ρ (x )= Eπ d+ ξ a +(1 ρ) d+ ξ a +(1 ρ) d+ ξ a .
ξ 1 2d 1i 1i − 2i 2i ··· − Hi Hi
nh Xi=1 i h Xi=1 i h Xi=1 io
Then we have
d H d
δ 1+ξ 1+ξ
V⋆,ρ (x ) Vπ,ρ (x )= 1i ξ Eπa +(1 ρ) hi ξ Eπa
ξ 1 − ξ 1 2d 2 − 1i 1i − 2 − hi hi
nhXi=1 i Xh=2 Xi=1(cid:16) (cid:17)o
H d
δ 1+ξ
(1 ρ) hi ξ Eπa
hi hi
≥ 2d − 2 −
Xh=1 Xi=1(cid:16) (cid:17)
37H d
δ 1 1
= (1 ρ) +ξ Eπ a
hi hi
2d − 2 2 −
Xh=1 Xi=1(cid:16) (cid:16) (cid:17)(cid:17)
H d
δ
= (1 ρ) (1 ξ Eπ(2a 1)). (D.33)
hi hi
4d − − −
h=1i=1
XX
Note that for any (h,i) [H] [d], by design we have 1= ξ2 , thus
∈ × hi
H d H d
δ δ
(1 ρ) (1 ξ Eπ(2a 1)) = (1 ρ) (ξ Eπ(2a 1))ξ
hi hi hi hi hi
4d − − − 4d − − −
h=1i=1 h=1i=1
XX XX
H d
δ
= (1 ρ) ξ Eπ(2a 1), (D.34)
hi hi
4d − | − − |
h=1i=1
XX
where (D.34) holds due to the fact that Eπ(2a 1) [ 1,1]. To continue, we have
hi
− ∈ −
H d
δ
(1 ρ) ξ Eπ(2a 1)
hi hi
4d − | − − |
h=1i=1
XX
H d
δ
(1 ρ) ξ Eπ(2a 1) 1 ξ = sign(Eπ(2a 1))
hi hi hi h,i
≥ 4d − | − − | { 6 − }
h=1i=1
XX
H d
δ
(1 ρ) 1 ξ = sign(Eπ(2a 1))
hi h,i
≥ 4d − { 6 − }
h=1i=1
XX
δ
(1 ρ)D (ξ,ξπ), (D.35)
H
≥ 4d −
where D (, ) is the Hamming distance, ξπ = ξπ , and ξπ := sign(Eπ(2a 1)), i [d].
H · · { h}h∈[H] hi hi − ∀ ∈
Combining (D.33), (D.34), (D.35) and the definition of the suboptimality gap, we have
δ
SupOpt(M ,x ,π,ρ) (1 ρ)D (ξ,ξπ). (D.36)
ξ 1 H
≥ 4d −
Step 2: lower bound the hamming distance by testing error. Applying Assouad’s method
(Tsybakov, 2009, Lemma 2.12), we have
dH
infsupE
ξ
D H(ξ,ξ′) min inf Q ξ(ψ( ) = ξ)+Q ξ′(ψ( ) = ξ′) , (D.37)
π ξ∈Ω ≥ 2 ξ,ξ′∈Ω ψ D 6 D 6
(cid:2) (cid:3)
DH(ξ,ξ′)=1 h i
where inf denotes the infimum over all test functions taking values in ξ,ξ′ . We conclude the
ψ
{ }
proof by combining (D.36) and (D.37).
D.7 Proof of Lemma C.2
Proof. By the Theorem 2.12 in Tsybakov (2009), we lower bound the testing error as follows
1 1/2
min inf Q ξ(ψ( ) = ξ)+Q ξ′(ψ( ) = ξ′) 1 max D KL Q ξ Q ξ′ ,
ξ,ξ′ ψ D 6 D 6 ≥ − 2 ξ,ξ′ ||
DH(ξ,ξ′)=1 h i (cid:16) DH(ξ,ξ′)=1
(cid:0)
(cid:1)(cid:17)
38where D KL( ) is the Kullback-Leibler divergence. Then it remains to bound D KL Q ξ Q ξ′ . Ac-
·||· ||
cording to the definition of Q ( ), we have
ξ
D (cid:0) (cid:1)
K H
Q ( ) = πb(ak sk)P (sk sk,ak)R(sk,ak;rk),
ξ D h h| h h h+1| h h h h h
k=1h=1
Y Y
whereR(sk,ak;rk)isthedensityfunctionof (r (sk,ak),1)atrk. Notethatthedifferencebetween
h h h N h h h h
the two distribution Q ξ( ) and Q ξ′( ) lies only in the reward distribution corresponding to the
D D
index where ξ and ξ′ differ. Then, by the chain rule of Kullback-Leibler divergence, we have
K
d+2 d+1 d 1 K δ2
D
KL
Q ξ( D) ||Q ξ′( D) = D
KL
N 2d
δ,1
N
2−
d
δ,1 = d+2d2.
(cid:0) (cid:1)
Xk=1 (cid:16) (cid:16) (cid:17)(cid:12) (cid:12)(cid:12)
(cid:12)
(cid:16) (cid:17)(cid:17)
Then by our choice of δ, we have (cid:12)(cid:12)
Kδ2 1/2 Kδ2 1/2 1
m ξ,ξin
′
in ψf Q ξ(ψ( D) 6= ξ)+Q ξ′(ψ( D) 6= ξ′)
≥
1
− 2(d+2)d2 ≥
1
− 2d3
= 2.
DH(ξ,ξ′)=1 h i (cid:16) (cid:17) (cid:16) (cid:17)
This completes the proof.
D.8 Proof of Lemma C.3
Proof. Recall that
K φτφτ⊤
Σ⋆−1 = h h +λI.
h [V V⋆,ρ ](sτ,aτ)
k=1 h h h h
X
We first show that with sufficiently large K, the clipped conditional variances of the optimal robust
⋆,ρ
value functions are always 1. Note that V (x )= 0, h [H], and
h 2 ∀ ∈
d
δ 1+ξ
⋆,ρ Hi
V (x ) = +d δ
H 1 2d 2 ≤
(cid:16)Xi=1 (cid:17)
d
δ 1+ξ
⋆,ρ H−1i ⋆,ρ
V (x ) = +d +V (x ) 2δ
H−1 1 2d 2 H 1 ≤
(cid:16)Xi=1 (cid:17)
···
d
δ 1+ξ
⋆,ρ 2i ⋆,ρ
V (x ) = +d +V (x ) (H 1) δ.
2 1 2d 2 3 1 ≤ − ·
(cid:16)Xi=1 (cid:17)
Then, when K Ω(H2d3), we have
≥
Var V⋆,ρ (x ,a) = P0(V⋆,ρ )2 (x ,a) P0(V⋆,ρ )2 (x ,a) 2 (1 ρ)ρH2δ2 1,
1 2 1 1 2 1 − 1 2 1 ≤ − ≤
and by (cid:2)design we h(cid:3)ave, (cid:2) (cid:3) (cid:0)(cid:2) (cid:3) (cid:1)
⋆,ρ ⋆,ρ
[Var V ](x ,a) = 0 and [Var V ](s,a) = 0, (s,a,h) [H]/ 1 .
1 2 2 h h+1 ∀ ∈ S ×A× { }
39Thus, we have [V V⋆,ρ ](sτ,aτ)= 1, which implies
h h h h
Σ⋆ = Λ . (D.38)
h h
Define
Λ˜ = Eπb,P0 [φ(s ,a )φ(s ,a )⊤],
h h h h h
then by definition we have
1 0 0 1(1 1) 0 0 0 0 0 0
d2 ··· d − d ···
0 0 0 0 0 0 1 0 1(1 1) 0
 ···   d2 ··· d − d 
. . . . . . . . . .
1 . . . . . 1 . . . . .
Λ˜ = . . . . . + . . . . . +
h    
d+2 0 0 0 0 0 d+20 0 0 0 0 ···
 ···   ··· 
1(1 1) 0 0 (1 1)2 0 0 1(1 1) 0 (1 1)2 0
d − d ··· − d   d − d ··· − d 
 0 0 0 0 0 0 0 0 0 0
 ···   ··· 
   
0 0 0 0 0 0 0 0 0 0
··· ···
0 0 0 0 0 0 0 0 0 0
 ···   ··· 
. . . . . . . . . .
1 . . . . . 1 . . . . .
. . . . . . . . . .
+ +
   
d+2 0 0 1 1(1 1) 0 d+2 0 0 0 0 0
 ··· d2 d − d   ··· 
0 0 1(1 1) (1 1)2 0 0 0 0 1 0
 ··· d − d − d   ··· 
0 0 0 0 0 0 0 0 0 0
 ···   ··· 
   
0 0 0 0 0
···
0 0 0 0 0
 ··· 
. . . . .
1 . . . . .
. . . . .
+
 
d+2 0 0 0 0 0
 ··· 
0 0 0 0 0
 ··· 
0 0 0 0 1
 ··· 
 1 0  0 1 (1 1) 0
d3 ··· d2 − d
0 1 0 1 (1 1) 0
 d3 ··· d2 − d 
. . . . .
d . . . . .
. . . . .
= .
 
d+2  0 0 1 1 (1 1) 0
 ··· d3 d2 − d 
 1 (1 1) 1 (1 1) 1 (1 1) (1 1)2+ 1 0
d2 − d d2 − d ··· d2 − d − d d 
 0 0 0 0 1
 ··· d
 
Denote
1 0 0 1 (1 1)
d3 ··· d2 − d
0 1 0 1 (1 1)
 d3 ··· d2 − d 
. . . .
D = . . . . . . . . ,
 
 0 0 1 1 (1 1) 
 ··· d3 d2 − d 
 1 (1 1) 1 (1 1) 1 (1 1) (1 1)2+ 1
d2 − d d2 − d ··· d2 − d − d d
 
40then by Gaussian elimination, we have
2d3 2d2 +d d3 2d2 +d d3 2d2 +d d d2
− − ··· − −
d3 2d2 +d 2d3 2d2 +d d3 2d2 +d d d2
 − − ··· − − 
D−1 = . . . . . . . . . . . . .
 
d3 2d2 +d d3 2d2 +d 2d3 2d2 +d d d2
 − − ··· − − 
 d d2 d d2 d d2 d 
 − − ··· − 
 
Note that
d D 0
Λ˜ = ,
h d+2 0 1
(cid:20) d(cid:21)
then we have
d+2 D−1 0
Λ˜−1 = .
h d 0 d
(cid:20) (cid:21)
Note that λ (D) = O(1/d3), thus Λ˜−1 = O(d3). Then when K > O˜(d6), for any (s,a,i,h)
min k h k ∈
[d] [H], with probability at least 1 δ, we have
S ×A× × −
2
kφ i(s,a)1 i kΛ− h1
≤
√Kkφ i(s,a)1 i kΛ˜− h1. (D.39)
With this in mind, we have
H d
P∈s Uu ρp
(P0)
Eπ⋆,P kφ i(s,a)1 i kΣ h⋆−1 |s 1 = x 1
Xh=1 hXi=1 i
H d
= P∈s Uu ρp
(P0)
Xh=1Eπ⋆,P hXi=1kφ i(s h,a h)1 i kΛ− h1 |s 1 = x 1
i
H d
2
≤
P∈s Uu ρp
(P0)
Xh=1Eπ⋆,P
h√K
Xi=1kφ i(s h,a h)1 i kΛ˜− h1 |s 1 = x 1
i
(D.40)
H d
2
= sup Eπ⋆,P φ (s ,a ) Λ˜−1 1/2 s = x
P∈Uρ(P0)
Xh=1
h√K
Xi=1
i h h
(cid:0)
h (cid:1)ii | 1 1
i
4Hd3/2
,
≤ √K
where (D.40) is due to (D.39). This concludes the proof.
E Proof of Supporting Lemmas
E.1 Proof of Lemma D.1
To prove Lemma D.1, we need the following proposition on the dual formulation under the TV
uncertainty set.
41Proposition E.1. (Strong duality for TV (Shi et al., 2023, Lemma 4)). Given any probabil-
ity measure µ0 over , a fixed uncertainty level ρ, the uncertainty set ρ(µ0) = µ : µ
S U { ∈
∆( ),D (µ µ0) ρ , and any function V : [0,H], we obtain
TV
S || ≤ } S →
inf E V(s) = max E [V(s)] ρ α min[V(s′)] , (E.1)
µ∈Uρ(µ0)
s∼µ
α∈[Vmin,Vmax]
s∼µ0 α
− − s′
α
(cid:8) (cid:0) (cid:1)(cid:9)
where [V(s)] = min V(s),α , V = min V(s) and V = max V(s). Notably, the range of α
α min s max s
{ }
can be relaxed to [0,H] without impacting the optimization.
Proof of Lemma D.1. By Assumption 3.1 and Proposition E.1, we have
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
d b b
= φ i(s,a) αm ∈[0a ,x H]{Eµ0 h,i[V hρ +1(s)]
α
−ρ(α −m si ′n[V hρ +1(s′)] α)
}
Xi=1 h
−αm ∈[0a ,x H]{Eµ0 h,i[V hρ +1(s)]
α
−b ρ(α −m si ′n[V hρ +1(s′)] α)b
}
.
i
b b b
Denote α
i
= argmax α∈[0,H]{Eµ0 h,i[V hρ +1(s)]
α
−ρ(α −min s′[V hρ +1(s′)] α) }, then we have
inf b[P Vρ ](s,a) \ ibnf [P Vρ ](s,a)
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
d b b
≤
φ i(s,a)
Eµ0
h,i[V
hρ
+1(s)]
αi
−Eµ0
h,i[V
hρ
+1(s)]
αi
i=1
X (cid:0) (cid:1)
d b b b
= φ i(s,a) 1⊤
i
Eµ0 h[V hρ +1(s)]
αi
−1⊤
i
Eµ0 h[V hρ +1(s)]
αi
.
i=1
X (cid:2) (cid:3)
b b b
Here we do reference-advantage decomposition by using the optimal robust value function as the
reference function. Specifically, we have
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
d b b
≤ φ i(s,a) 1⊤ i Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi +[V h⋆ +,ρ 1(s)] αi
i=1
X (cid:2) (cid:0) (cid:2) (cid:3)(cid:1)
−1⊤ i Eµ0 h [V hρ +1(s)] αib −[V h⋆ +,ρ 1(s)] αi +[V h⋆ +,ρ 1(s)] αi
d
(cid:0) (cid:2) (cid:3)(cid:1)(cid:3)
= φ i(bs,a)1b⊤
i
Eµ0 h[V h⋆ +,ρ 1(s)]
αi
−Eµ0 h[V h⋆ +,ρ 1(s)]
αi
i=1
X (cid:0) (cid:1)
b
reference uncertainty
d
| {z }
+ φ i(s,a)1⊤ i Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi −Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi . (E.2)
i=1
X (cid:0) (cid:2) (cid:3) (cid:2) (cid:3)(cid:1)
b b b
advantage uncertainty
| {z }
42The Reference Uncertainty. First, we bound the reference uncertainty. Specifically, we have
d
φ i(s,a)1⊤
i
Eµ0 h[V h⋆ +,ρ 1(s)]
αi
−Eµ0 h[V h⋆ +,ρ 1(s)]
αi
i=1
X (cid:0) (cid:1)
d b K
= φ i(s,a)1⊤
i
Eµ0 h[V h⋆ +,ρ 1(s)]
αi
−Λ− h1 φτ
h
P0 h[V h⋆ +,ρ 1]
αi
(sτ h,aτ h)
Xi=1 (cid:16) Xτ=1
(cid:2) (cid:3)
K K
+Λ−1 φτ P0[V⋆,ρ ] (sτ,aτ) Λ−1 φτ[V⋆,ρ (sτ )]
h h h h+1 αi h h − h h h+1 h+1 αi
Xτ=1
(cid:2) (cid:3)
Xτ=1 (cid:17)
d K
= φ i(s,a)1⊤
i
Eµ0 h[V h⋆ +,ρ 1(s)]
αi
−Λ− h1 φτ hφτ h⊤Eµ0 h[V h⋆ +,ρ 1(s)]
αi
Xi=1 (cid:16) Xτ=1
K
+Λ−1 φτ P0[V⋆,ρ ] (sτ,aτ) [V⋆,ρ (sτ )] .
h h h h+1 αi h h − h+1 h+1 αi
Xτ=1
(cid:0)(cid:2) (cid:3)
(cid:1)(cid:17)
For any function f : [0,H 1], we define ητ([f] ) = P0[f] (sτ,aτ) [f(sτ )] . Then,
S → − h αi h αi h h − h+1 αi
we have
(cid:0)(cid:2) (cid:3) (cid:1)
d
φ i(s,a)1⊤
i
Eµ0 h[V h⋆ +,ρ 1(s)]
αi
−Eµ0 h[V h⋆ +,ρ 1(s)]
αi
i=1
X (cid:0) (cid:1)
d b d K
= λ φ i(s,a)1⊤
i
Λ− h1Eµ0 h[V h⋆ +,ρ 1(s)]
αi
+ φ i(s,a)1⊤
i
Λ− h1 φτ hη hτ([V h⋆ +,ρ 1] αi)
i=1 i=1 τ=1
X X X
d d K
≤ λ kφ i(s,a)1 i kΛ− h1 kEµ0 h[V h⋆ +,ρ 1(s)] αikΛ− h1+ kφ i(s,a)1 i kΛ− h1 φτ hη hτ([V h⋆ +,ρ 1] αi) Λ−1 (E.3)
Xi=1 Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
i (cid:13) ii (cid:13)
| {z } | {z }
The Advantage Uncertainty. Next, we bound the advantage uncertainty. By similar argument
in bounding the reference uncertainty, we have
d
φ i(s,a)1⊤ i Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi −Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi
i=1
X (cid:0) (cid:2) (cid:3) (cid:2) (cid:3)(cid:1)
d b b b
≤ λ kφ i(s,a)1 i kΛ− h1 Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi Λ−1
Xi=1 (cid:13)
(cid:13) (cid:2)
(cid:3)(cid:13)
(cid:13)
h
b
(cid:13) iii (cid:13)
d K
+| kφ i(s,a)1 i kΛ− h1 φ{ τ hz η hτ([V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] α} i) Λ−1 (E.4)
Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
b
(cid:13) iv (cid:13)
Combining (E.2), (E.3) and (E.4), we have
| {z }
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
b b
43d d K
≤ λ kφ i(s,a)1 i kΛ− h1 kEµ0 h[V h⋆ +,ρ 1(s)] αikΛ− h1+ kφ i(s,a)1 i kΛ− h1 φτ hη hτ([V h⋆ +,ρ 1] αi) Λ−1
Xi=1 Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
i (cid:13) ii (cid:13)
d
+| λ kφ i(s,a)1 i kΛ{ − hz 1 Eµ0 h [V hρ +1(s)] αi } −[V| h⋆ +,ρ 1(s)] αi Λ−1 {z }
Xi=1 (cid:13)
(cid:13) (cid:2)
(cid:3)(cid:13)
(cid:13)
h
b
(cid:13) iii (cid:13)
d K
+| kφ i(s,a)1 i kΛ− h1 φτ hη{ hτz ([V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi)} Λ−1.
Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
b
(cid:13) iv (cid:13)
On the ot|her hand, we can similarly de{dzuce }
\ inf [P Vρ ](s,a) inf [P Vρ ](s,a)
Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
d b d b K
≤ λ kφ i(s,a)1 i kΛ− h1 kEµ0 h[V h⋆ +,ρ 1(s)] α′ ikΛ− h1+ kφ i(s,a)1 i kΛ− h1 φτ hη hτ([V h⋆ +,ρ 1] α′ i) Λ−1
Xi=1 Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
i (cid:13) ii (cid:13)
d
+| λ kφ i(s,a)1 i kΛ{ − hz 1 Eµ0 h [V hρ +1(s)] α′ i } −[V| h⋆ +,ρ 1(s)] α′ i Λ−1 {z }
Xi=1 (cid:13)
(cid:13) (cid:2)
(cid:3)(cid:13)
(cid:13)
h
b
(cid:13) iii (cid:13)
d K
+| kφ i(s,a)1 i kΛ− h1 φτ hη{ hτz ([V hρ +1(s)] α′
i
−[V h⋆ +,ρ 1(s)] α′ i)} Λ−1,
Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
b
(cid:13) iv (cid:13)
where α′
i
|= argmax α∈[0,H]{Eµ0 h,i[V hρ +1({sz)]
α
−ρ(α −min s′[V hρ +1(s′)] α)} }. Then for all i
∈
[d], there
exist α˜ α ,α′ , such that
i ∈{ i i}
b b b
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
(cid:12)
(cid:12) d b d b (cid:12)K
(cid:12) (cid:12)
≤ λ kφ i(s,a)1 i kΛ− h1 kEµ0 h[V h⋆ +,ρ 1(s)] α˜ikΛ− h1+ kφ i(s,a)1 i kΛ− h1 φτ hη hτ([V h⋆ +,ρ 1] α˜i) Λ−1
Xi=1 Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
i (cid:13) ii (cid:13)
d
+| λ kφ i(s,a)1 i kΛ{ − hz 1 Eµ0 h [V hρ +1(s)] α˜i } −[V| h⋆ +,ρ 1(s)] α˜i Λ−1 {z }
Xi=1 (cid:13)
(cid:13) (cid:2)
(cid:3)(cid:13)
(cid:13)
h
b
(cid:13) iii (cid:13)
d K
+| kφ i(s,a)1 i kΛ− h1 φτ hη{ hτz ([V hρ +1(s)] α˜i −[V h⋆ +,ρ 1(s)] α˜i)} Λ−1,
Xi=1 (cid:13)Xτ=1 (cid:13) h
(cid:13) (cid:13)
b
(cid:13) iv (cid:13)
This conc|ludes the proof. {z }
44E.2 Proof of Lemma D.2
The proof of Lemma D.2 will use the following fact.
Lemma E.2. (Jin et al., 2020, Lemma D.1) Let Λ = λI+ t φ φ⊤, where φ Rd and λ > 0.
t i=1 i i i ∈
Then:
P
t
φ⊤(Λ )−1φ d.
i t i ≤
i=1
X
Proof of Lemma D.2. The proof of Lemma D.2 is similar to that of Lemma E.1 in Liu and Xu
(2024). Denote α
i
= argmax α∈[0,H]{zˆ h,i(α)
−
ρ(α
−
min s′[V hρ +1(s′)] α) },i
∈
[d]. For any vector
v Rd, we have
∈
b
v⊤wρ = v⊤θ +v⊤ max zˆ (α) ρ(α min[Vρ (s′)] )
h h α∈[0,H]{ h,i − − s′ h+1 α } i∈[d]
(cid:12) (cid:12) (cid:12) h i (cid:12)
(cid:12) (cid:12)
≤
(cid:12) (cid:12)v⊤θ h + v⊤ αm ∈[0a ,x H]{zˆ h,i(α) −ρ(α −m si ′nb[V hρ +1(s′)] α)
}
i∈[(cid:12) (cid:12)
d]
(cid:12) (cid:12) (cid:12) h i (cid:12)
(cid:12) K (cid:12)
(cid:12) (cid:12) b
√d v +(cid:12) H v + v⊤ 1⊤ Λ−1 φτ[maxQρ (sτ ,a)](cid:12) (E.5)
≤ k k2 k k1 i h h a h+1 h+1 αi
(cid:12) (cid:20) (cid:18) τ=1 (cid:19)(cid:21)i∈[d](cid:12)
(cid:12) X (cid:12)
(cid:12) b (cid:12)
(cid:12) K K (cid:12)
√d v +H√d v + v⊤Λ−1v (φτ)⊤Λ−1φτ H (E.6)
≤ k k2 k k2 v h h h h ·
u(cid:20)τ=1 (cid:21)(cid:20)τ=1 (cid:21)
u X X
t
2H v dK/λ. (E.7)
2
≤ k k
p
We note that the term [(Λ−1 K φτ[max Qρ (sτ ,a)] ) ] in (E.5) is constructed by first
h τ=1 h a h+1 h+1 αi i i∈[d]
taking out the i-th coordinate of the ridge solution vector, Λ−1 K φτ[max Qρ (sτ ,a)]
P h τ=1 h a h+1 h+1 αi ∈
Rd, i [d], and then concatenating all d vab lues into a vector. Inequality (E.5) is due to the fact
that∀ ρ ∈ 1, (E.6) is due to the fact that Qρ H, and (E.7) P is due to Lemmba E.2 with t = K
≤ h+1 ≤
and the fact that the minimum eigenvalue of Λ is lower bounded by λ. The remainder of the proof
h
follows from the fact that wρ = max b v⊤wρ .
k hk2 v:kvk2=1 | h|
E.3 Proof of Lemma D.4
The proof of Lemma D.4 will use the following fact.
Lemma E.3. (Jin et al., 2020, Covering Number of Euclidean Ball) For any ǫ > 0, the ǫ-covering
number of the Euclidean ball in Rd with radius R > 0 is upper bounded by (1+2R/ǫ)d.
Proof of Lemma D.4. The proofis similartothe proofof Lemma E.3 in Liu and Xu(2024). Denote
A= β2Σ−1, so we have
h
d
() = max φ(s,a)⊤θ φ (s,a)1⊤Aφ (s,a)1 , (E.8)
Vh · a∈A − i i i i [0,H−h+1]
n Xi=1q o
45for θ L, A B2λ−1. For any two functions V ,V , let them take the form in (E.8)
1 2
k k ≤ k k ≤ ∈ V
with parameters (θ ,A ) and (θ ,A ), respectively. Then since both and max are
1 1 2 2 [0,H−h+1] a
{·}
contraction maps, we have
d
dist(V ,V ) sup θ⊤φ(x,a) φ (x,a)1⊤A φ (x,a)1
1 2 ≤ 1 − i i 1 i i
x,a (cid:12)(cid:20) i=1q (cid:21)
(cid:12) X
(cid:12) d
(cid:12) θ⊤φ(x,a) φ (x,a)1⊤A φ (x,a)1
− 2 − i i 2 i i
(cid:20) i=1q (cid:21)(cid:12)
X (cid:12)
d (cid:12)d
sup θ⊤φ φ 1⊤A φ 1 θ⊤φ (cid:12) φ 1⊤A φ 1
≤ 1 − i i 1 i i − 2 − i i 2 i i
φ:kφk≤1(cid:12)(cid:20) i=1q (cid:21) (cid:20) i=1q (cid:21)(cid:12)
(cid:12) X X (cid:12)
(cid:12) d (cid:12)
sup (cid:12) (θ θ )⊤φ + sup φ 1⊤(A A )φ 1 (cid:12) (E.9)
≤ 1 − 2 i i 1 − 2 i i
φ:kφk≤1 φ:kφk≤1
(cid:12) (cid:12)
Xi=1q
(cid:12) (cid:12) d
θ θ + A A sup φ 1
1 2 1 2 i i
≤ k − k k − k k k
φ:kφk≤1
i=1
p X
θ θ + A A , (E.10)
1 2 1 2 F
≤ k − k k − k
where (E.9) follows from triangularpinequlaity and the fact that √x √y x y , x,y 0.
| − | ≤ | − | ∀ ≥
For matrices, and denote the matrix operator norm and Frobenius norm respectively.
k·k k·kF p
Let be an ǫ/2-cover of θ Rd θ L with respect to the 2-norm, and be an ǫ2/4-
θ 2 A
C { ∈ |k k ≤ } C
cover of A Rd×d A d1/2B2λ−1 with respect to the Frobenius norm. By Lemma E.3, we
F
{ ∈ |k k ≤ }
know:
1+4L/ǫ d , 1+8d1/2B2/(λǫ2) d2 .
θ A
C ≤ C ≤
By (E.10), for any V (cid:12) ,(cid:12)the(cid:0)re exists θ(cid:1) (cid:12) an(cid:12)d A(cid:2) such that V(cid:3) parametrized by (θ ,A )
1 ∈(cid:12)V (cid:12) 2 ∈ C(cid:12)θ (cid:12) 2 ∈ CA 2 2 2
satisfies dist(V ,V ) ǫ. Hence, it holds that , which leads to
1 2 ǫ θ A
≤ N ≤ |C |·|C |
log ǫ log w +log A dlog(1+4L/ǫ)+d2log 1+8d1/2B2/(λǫ2) .
N ≤ |C | |C | ≤
This concludes the proof. (cid:2) (cid:3)
E.4 Proof of Theorem A.1
In this section, we give the proof of Theorem A.1, which largely follows the proof of Theorem 5.2,
only with minor modifications of the argument of the variance estimation.
The following lemma bounds the estimation error by reference-advantage decomposition.
LemmaE.4(ModifiedVariance-AwareReference-AdvantageDecomposition). Thereexist α ,
i i∈[d]
{ }
where α [0,H], i [d], such that
i
∈ ∀ ∈
inf [P Vρ ](s,a) \ inf [P Vρ ](s,a)
(cid:12)Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1 −Ph(·|s,a)∈U hρ(s,a;µ0 h,i) h h+1
(cid:12)
(cid:12) (cid:12)
≤ λ
d
kφ i(s,a)1 i kΣ−
hb
1 kEµ0 h[V h⋆ +,ρ 1(s)] αikΣ− h1+
d
kφ
i(s,b
a)1 i kΣ− h1
(cid:12) (cid:12)K φτ
h
ση 2hτ (( s[V
τh
,⋆
+
a,ρ τ1] )αi)
Σ−1
Xi=1 Xi=1 (cid:13)Xτ=1 h h h (cid:13) h
(cid:13) (cid:13)
i (cid:13) ii (cid:13)
b
| {z } | {z }
46d
+λ kφ i(s,a)1 i kΣ− h1 Eµ0 h [V hρ +1(s)] αi −[V h⋆ +,ρ 1(s)] αi Σ−1
Xi=1 (cid:13)
(cid:13) (cid:2)
(cid:3)(cid:13)
(cid:13)
h
b
(cid:13) iii (cid:13)
+|d
kφ i(s,a)1 i kΣ− h1
K φτ hη{hτ z([V hρ
+1
σ( 2s () s]
α τi ,−
aτ[ )V h⋆ +,ρ 1(s)] αi)}
Σ−1,
Xi=1 (cid:13)Xτ=1 b h h h (cid:13) h
(cid:13) (cid:13)
(cid:13) iv (cid:13)
b
where ητ(|[f] ) = P0[f] (sτ,aτ) {[fz(sτ )] , for any function f} : [0,H 1].
h αi h αi h h − h+1 αi S → −
Proof of Theorem (cid:0)A(cid:2).1. To p(cid:3)rove this theorem, w(cid:1)e bound the estimation error by Γ (s,a), then
h
invoke Lemma 4.6 to get the results. First, we bound terms i-iv in Lemma E.4 at each step h [H]
∈
respectively to deduce Γ (s,a).
h
Bound i and iii: We set λ = 1/H2 to ensure that for all (s,a,h) [H], we have
∈ S ×A×
d d
i+iii
≤
√λ√dH kφ i(s,a)1 i kΣ− h1 = √d kφ i(s,a)1 i kΣ− h1. (E.11)
i=1 i=1
X X
Bound ii: For all (s,a,α) [0,H], by definition we have σ (s,a) 1. Thus, for
h
all (h,τ,i) [H] [K] [d∈ ], S we× hA av× e ητ([V⋆,ρ ] )/σ (sτ,aτ) H. Note t≥ hat V⋆,ρ is in-
∈ × × h h+1 αi h h h ≤ H+1
dependent of , we can directly apply Bernstein-type self-normalized bconcentration inequality
D
Lemma F.2 and a union bound to obtain the upper bbound. In concrete, we define the filtra-
tion = σ( (sj ,aj ) τ sj τ−1). Since V⋆,ρ and σ (s,a) are independent of , thus
Fτ−1,h { h h }j=1 ∪{ h+1}j=1 h+1 h D
ητ([V⋆,ρ ] )/σ (sτ,aτ) is mean-zero conditioned on the filtration . By Lemma B.1 with
h h+1 αi h h h Fτ−1,h
α = H, we have b
b
dH3
V V⋆,ρ (s,a) O˜ σ2(s,a) V V⋆,ρ (s,a), (E.12)
h h+1 − √Kκ ≤ h ≤ h h+1
(cid:16) (cid:17)
(cid:2) (cid:3) (cid:2) (cid:3)
thus, for any α [0,H], we have b
i
∈
dH3 dH3
V [V⋆,ρ ] (s,a) O˜ V V⋆,ρ (s,a) O˜ σ2(s,a). (E.13)
h h+1 αi − √Kκ ≤ h h+1 − √Kκ ≤ h
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:2) (cid:3) (cid:2) (cid:3)
Further, we have b
E
η hτ([V h⋆ +,ρ 1] αi) 2
=
[Var[V h⋆ +,ρ 1] αi](sτ h,aτ h)
(E.14)
σ (sτ,aτ) Fτ−1,h σ2(sτ,aτ)
h h h h h h
h(cid:16) (cid:17) (cid:12) i
(cid:12) [V[V⋆,ρ ] ](sτ,aτ)
(cid:12) h+1 αi h h
b
≤
σb2(sτ,aτ)
h h h
=
[V[V h⋆ +,ρ 1] αi](sτ h,aτ h) −O˜(dH3/√Kκ)
+
O˜(dH3/√Kκ)
b σ2(sτ,aτ) σ2(sτ,aτ)
h h h h h h
O˜(dH3/√Kκ)
1+ (E.15)
≤ σ2(sτ,aτ)b O˜(dH3/√Kκ) b
h h h −
b 47dH3
1+2O˜ , (E.16)
≤ √Kκ
(cid:16) (cid:17)
where (E.14) holds by the fact that σ2(, ) is independent of and (sτ,aτ) is measur-
h · · D h h Fτ−1,h
able. (E.15) holds by (E.13), and (E.16) holds by setting K Ω˜(d2H6/κ) such that σ2(sτ,aτ)
≥ h h h −
O˜(dH3/√Kκ) 1 O˜(dH3/√Kκ) b1/2. Further, by (E.16), our choice of K also ensures that
≥ − ≥
E η hτ([V h⋆ +,ρ 1] αi) 2
|Fτ−1,h
= O(1). Then by Lemma F.2, we have b
(cid:2)(cid:0) (cid:1) (cid:3) K φτητ([V⋆,ρ ] )
h h h+1 αi O˜(√d).
σ2(sτ,aτ) Σ−1 ≤
(cid:13)Xτ=1 h h h (cid:13) h
(cid:13) (cid:13)
This implies (cid:13) (cid:13)
b
d
ii
≤
O˜(√d) kφ i(s,a)1 i kΣ− h1. (E.17)
i=1
X
Bound iv: Followingthesameinductionanalysisprocedure, weknowthat [Vρ ] [V⋆,ρ ]
k h+1 αi− h+1 αik≤
O˜(√dH2/√Kκ). Using standard ǫ-covering number argument and Lemma F.1, we have
b
d3/2H2 d
iv
≤
O˜
√Kκ
kφ i(s,a)1 i kΣ− h1. (E.18)
(cid:16) (cid:17)Xi=1
Tomakeitnon-dominant,werequireK Ω˜(d2H4/κ). By(E.12),wehaveσ2(sτ,aτ) [V V⋆ ](sτ,aτ),
≥ h h h ≤ h h+1 h h
which implies that
K φτφτ⊤ −1 K φτφτ⊤ b −1
h h +λI h h +λI .
σ2(sτ,aτ) (cid:22) [V V⋆ ](sτ,aτ)
(cid:16)Xτ=1 h h h (cid:17) (cid:16)Xτ=1 h h+1 h h (cid:17)
Combining (E.11), (E.17) and (E.18), we have
b
d
(cid:12)Ph(·|s,a)∈i Un hρf
(s,a;µ0
h,i)[P hV hρ +1](s,a) −Ph(·|s,a)\ ∈i Un hρf
(s,a;µ0
h,i)[P hV hρ +1](s,a)
(cid:12)
≤
O˜(√d) Xi=1kφ i(s,a)1
i kΣ⋆
h−1.
(cid:12) (cid:12)
D(cid:12)efine Γ h(s,a) = O˜(√db ) d i=1kφ i(s,a)1
i kΣ
h⋆−1, we conclub des the pr(cid:12)oof by invoking Lemma 4.6.
P
F Auxiliary Lemmas
Lemma F.1 (Concentration of Self-Normalized Processes). (Abbasi-Yadkori et al., 2011, Theorem
1) Let ǫ ∞ be a real-valued stochastic process with corresponding filtration ∞ . Let ǫ
{ t }t=1 {Ft }t=0 t |Ft−1
be mean-zero and σ-subGaussian; i.e. E[ǫ ] = 0, and
t t−1
|F
λ R, E[eλǫt ] eλ2σ2/2.
t−1
∀ ∈ |F ≤
Let φ ∞ be an Rd-valued stochastic process where φ is measurable. Assume Λ is a d d
posit{ ivet } dt= e1 finite matrix, and let Λ = Λ + t φ φ⊤t . TF het− n1 for any δ > 0, with pro0 bability× at
t 0 s=1 s s
least 1 δ, we have for all t 0:
− ≥ P
t 2 det(Λ )1/2det(Λ )−1/2
φ ǫ 2σ2log t 0 .
s s
(cid:13) (cid:13)Xs=1 (cid:13) (cid:13)Λ− t1 ≤ (cid:20) δ (cid:21)
(cid:13) (cid:13)
(cid:13) (cid:13)
48Lemma F.2 (Bernstein inequality for self-normalized martingales). (Zhou et al., 2021a, Theorem
2) Let η ∞ be a real-valued stochastic process. Let ∞ be a filtration, such that η is
{ t }t=1 {Ft }t=0 t
-measurable. Assume η also satisfies
t t
F
η R,E[η ] = 0,E[η2 ] σ2.
|
t
|≤
t |Ft−1 t|Ft−1
≤
Let x ∞ be an Rd-valued stochastic process where x is measurable and x L. Let
Λ ={ λIt }t +=1 t x x⊤. Then for any δ > 0, with probabit lityF att− l1 east 1 δ, for all tk >t 0k ,≤
t d s=1 s s −
P t tL2 4t2 4t2
x η 8σ dlog 1+ log +4Rlog .
s s
(cid:13) (cid:13)Xs=1 (cid:13) (cid:13)Λ− t1 ≤ r (cid:16) λd (cid:17)· (cid:16) δ (cid:17) (cid:16) δ (cid:17)
(cid:13) (cid:13)
Lemma F.3. (M(cid:13)in et al.,(cid:13)2021, Lemma H.5) Let φ : Rd satisfying φ(x,a) C for
all (x,a) . For any K > 0 and λ > 0, define G
S =×AK→
φ(x ,a )φ(x
,k
a )⊤
+k λI≤
where
∈ S ×A K k=1 k k k k d
(x ,a ) ’s are i.i.d. samples from some distribution ν over . Let G = E [φ(x,a)φ(x,a)⊤].
k k v
PS × A
Then, for any δ (0,1), if K satisfies that
∈
2d
K max 512C4 G−1 2 log ,4λ G−1 ,
≥ δ
n (cid:13) (cid:13) (cid:16) (cid:17) (cid:13) (cid:13)o
then with probability at least 1 δ, it holds(cid:13)simu(cid:13)ltaneously for (cid:13)all u (cid:13)Rd that
− ∈
2
ku kG− K1
≤
√Kku kG−1.
References
Abbasi-Yadkori, Y.,Pál, D.andSzepesvári, C.(2011). Improvedalgorithmsforlinearstochas-
tic bandits. Advances in neural information processing systems 24. (p. 48.)
Badrinath, K. P. and Kalathil, D. (2021). Robust reinforcement learning using least squares
policy iteration with provable performance guarantees. In International Conference on Machine
Learning. PMLR. (pp. 2 and 5.)
Blanchet, J., Lu, M., Zhang, T. and Zhong, H. (2023). Double pessimism is provably efficient
for distributionally robust offline reinforcement learning: Generic algorithm and robust partial
coverage. arXiv preprint arXiv:2305.09659 . (pp. 2, 3, 5, 9, 13, and 17.)
Dong, J., Li, J., Wang, B. and Zhang, J. (2022). Online policy optimization for robust mdp.
arXiv preprint arXiv:2209.13841 . (p. 5.)
Duan, Y., Jia, Z. and Wang, M. (2020). Minimax-optimal off-policy evaluation with linear
function approximation. In International Conference on Machine Learning. PMLR. (p. 8.)
Farebrother, J.,Machado, M. C.andBowling, M.(2018). Generalizationandregularization
in dqn. arXiv preprint arXiv:1810.00123 . (p. 1.)
Gottesman, O., Johansson, F., Komorowski, M., Faisal, A., Sontag, D., Doshi-Velez,
F. and Celi, L. A. (2019). Guidelines for reinforcement learning in healthcare. Nature medicine
25 16–18. (p. 1.)
49Goyal, V. and Grand-Clement, J. (2023). Robust markov decision processes: Beyond rectan-
gularity. Mathematics of Operations Research 48 203–226. (p. 5.)
Iyengar, G. N. (2005). Robust dynamic programming. Mathematics of Operations Research 30
257–280. (pp. 2 and 6.)
Jin, C.,Yang, Z.,Wang, Z. and Jordan, M. I. (2020). Provably efficient reinforcement learning
with linear function approximation. In Conference on Learning Theory. PMLR. (pp. 6 and 45.)
Jin, Y., Yang, Z. and Wang, Z. (2021). Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning. PMLR. (pp. 1, 2, 4, 8, 9, 14, 26, and 27.)
Kuang, Y.,Lu, M.,Wang, J.,Zhou, Q.,Li, B.andLi, H.(2022). Learningrobustpolicyagainst
disturbance in transition dynamics via state-conservative policy optimization. In Proceedings of
the AAAI Conference on Artificial Intelligence, vol. 36. (p. 1.)
Lange, S., Gabel, T. and Riedmiller, M. (2012). Batch reinforcement learning. In Reinforce-
ment learning: State-of-the-art. Springer, 45–73. (p. 1.)
Levine, S., Kumar, A., Tucker, G. and Fu, J. (2020). Offline reinforcement learning: Tutorial,
review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 . (p. 1.)
Liang, Z., Ma, X., Blanchet, J., Zhang, J. and Zhou, Z. (2023). Single-trajectory distribu-
tionally robust reinforcement learning. arXiv preprint arXiv:2301.11721 . (p. 5.)
Liu, Z. and Xu, P. (2024). Distributionally robust off-dynamics reinforcement learning: Provable
efficiency with linear function approximation. In International Conference on Artificial Intelli-
gence and Statistics. PMLR. (pp. 2, 5, 6, 7, 8, and 45.)
Ma, X., Liang, Z., Xia, L., Zhang, J., Blanchet, J., Liu, M.,Zhao, Q.and Zhou, Z.(2022).
Distributionally robust offline reinforcement learning with linear function approximation. arXiv
preprint arXiv:2209.06620 . (pp. 5 and 8.)
Mandlekar, A.,Zhu, Y.,Garg, A.,Fei-Fei, L.andSavarese, S.(2017). Adversarially robust
policy learning: Active construction of physically-plausible perturbations. In 2017 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). IEEE. (p. 1.)
Mannor, S., Mebel, O. and Xu, H. (2016). Robust mdps with k-rectangular uncertainty. Math-
ematics of Operations Research 41 1484–1509. (p. 5.)
Min, Y., Wang, T., Zhou, D. and Gu, Q. (2021). Variance-aware off-policy evaluation with
linear function approximation. Advances in neural information processing systems 34 7598–7610.
(pp. 11 and 49.)
Morimoto, J. and Doya, K. (2005). Robust reinforcement learning. Neural computation 17
335–359. (p. 1.)
Nilim, A. and El Ghaoui, L. (2005). Robust control of markov decision processes with uncertain
transition matrices. Operations Research 53 780–798. (pp. 1 and 2.)
50Packer, C., Gao, K., Kos, J., Krähenbühl, P., Koltun, V. and Song, D. (2018). Assessing
generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282 . (p. 1.)
Pan, Y., Cheng, C.-A., Saigol, K., Lee, K., Yan, X., Theodorou, E. and Boots, B.
(2017). Agile autonomous driving using end-to-end deep imitation learning. arXiv preprint
arXiv:1709.07174 . (p. 1.)
Panaganti, K. and Kalathil, D. (2022). Sample complexity of robust reinforcement learning
with a generative model. In International Conference on Artificial Intelligence and Statistics.
PMLR. (p. 5.)
Panaganti, K., Xu, Z., Kalathil, D. and Ghavamzadeh, M. (2022). Robust reinforcement
learning using offline data. Advances in neural information processing systems 35 32211–32224.
(pp. 2 and 5.)
Pattanaik, A., Tang, Z., Liu, S., Bommannan, G. and Chowdhary, G. (2017). Robust deep
reinforcement learning with adversarial attacks. arXiv preprint arXiv:1712.03632 . (p. 1.)
Pinto, L., Davidson, J., Sukthankar, R. and Gupta, A. (2017). Robust adversarial reinforce-
ment learning. In International Conference on Machine Learning. PMLR. (p. 1.)
Roy, A., Xu, H. and Pokutta, S. (2017). Reinforcement learning under model mismatch. Ad-
vances in neural information processing systems 30. (p. 2.)
Satia, J. K. and Lave Jr, R. E. (1973). Markovian decision processes with uncertain transition
probabilities. Operations Research 21 728–740. (p. 2.)
Shen, Y., Xu, P. and Zavlanos, M. M. (2023). Wasserstein distributionally robust policy
evaluation and learning for contextual bandits. arXiv preprint arXiv:2309.08748 . (p. 2.)
Shi, L.andChi, Y.(2022). Distributionally robustmodel-basedofflinereinforcement learningwith
near-optimal sample complexity. arXiv preprint arXiv:2208.05767 . (pp. 2, 5, and 8.)
Shi, L., Li, G., Wei, Y., Chen, Y., Geist, M. and Chi, Y. (2023). The curious price of
distributional robustness in reinforcement learning with a generative model. arXiv preprint
arXiv:2305.16589 . (pp. 5, 11, and 42.)
Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo,
J., Zhou, Y., Chai, Y., Caine, B. et al. (2020). Scalability in perception for autonomous
driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. (p. 1.)
Tamar, A.,Mannor, S.andXu, H.(2014). Scalinguprobustmdpsusingfunctionapproximation.
In International conference on machine learning. PMLR. (pp. 2 and 5.)
Tessler, C., Efroni, Y. and Mannor, S. (2019). Action robust reinforcement learning and
applications in continuous control. In International Conference on Machine Learning. PMLR. (p.
1.)
Tsybakov, A. B. (2009). Introduction to Nonparametric Estimation. Springer, New York. (p. 38.)
51Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data
science, vol. 47. Cambridge university press. (p. 27.)
Wang, L., Zhang, W., He, X. and Zha, H. (2018). Supervised reinforcement learning with
recurrent neural network for dynamic treatment recommendation. In Proceedings of the 24th
ACM SIGKDD international conference on knowledge discovery & data mining. (p. 1.)
Wang, R., Foster, D. P. and Kakade, S. M. (2020). What are the statistical limits of offline
rl with linear function approximation? arXiv preprint arXiv:2010.11895 . (pp. 8 and 9.)
Wang, Y. and Zou, S. (2021). Online robust reinforcement learning with model uncertainty.
Advances in Neural Information Processing Systems 34 7193–7206. (pp. 2 and 5.)
Wiesemann, W., Kuhn, D. and Rustem, B. (2013). Robust markov decision processes. Mathe-
matics of Operations Research 38 153–183. (pp. 2 and 5.)
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P. and Agarwal, A. (2021). Bellman-consistent
pessimism for offline reinforcement learning. Advances in neural information processing systems
34 6683–6694. (pp. 1, 2, 4, and 8.)
Xiong, W., Zhong, H., Shi, C., Shen, C., Wang, L. and Zhang, T. (2022). Nearly minimax
optimal offline reinforcement learning with linear function approximation: Single-agent mdp and
markov game. arXiv preprint arXiv:2205.15512 . (pp. 2, 3, 4, 8, 9, 11, 13, 14, 27, and 32.)
Xu, H.andMannor, S.(2006). Therobustness-performancetradeoff inmarkovdecisionprocesses.
Advances in Neural Information Processing Systems 19. (pp. 2 and 5.)
Xu, Z., Panaganti, K. and Kalathil, D. (2023). Improved sample complexity bounds for dis-
tributionally robust reinforcement learning. In International Conference on Artificial Intelligence
and Statistics. PMLR. (pp. 5 and 6.)
Yang, W., Wang, H., Kozuno, T., Jordan, S. M. and Zhang, Z. (2023a). Avoiding
model estimation in robust markov decision processes with a generative model. arXiv preprint
arXiv:2302.01248 . (p. 5.)
Yang, W.,Zhang, L.andZhang, Z.(2022). Toward theoreticalunderstandings ofrobustmarkov
decision processes: Sample complexity and asymptotics. The Annals of Statistics 50 3223–3248.
(pp. 2, 5, and 6.)
Yang, Z., Guo, Y., Xu, P., Liu, A. and Anandkumar, A. (2023b). Distributionally robust
policygradientforofflinecontextualbandits. InInternationalConference onArtificialIntelligence
and Statistics. PMLR. (p. 2.)
Yin, M.,Duan, Y.,Wang, M.andWang, Y.-X.(2022). Near-optimalofflinereinforcementlearn-
ing with linear representation: Leveraging variance information with pessimism. arXiv preprint
arXiv:2203.05804 . (pp. 3, 4, 8, 9, 11, and 14.)
Yu, P.and Xu, H.(2015). Distributionally robustcounterpart inmarkov decisionprocesses. IEEE
Transactions on Automatic Control 61 2538–2543. (p. 5.)
52Zanette, A., Wainwright, M. J. and Brunskill, E. (2021). Provable benefits of actor-critic
methods foroffline reinforcement learning. Advances in neural information processing systems 34
13626–13640. (pp. 2, 4, and 14.)
Zhang, H., Chen, H., Boning, D. and Hsieh, C.-J. (2021). Robust reinforcement learning on
state observations with learned optimal adversary. arXiv preprint arXiv:2101.08452 . (p. 2.)
Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning, D. and Hsieh, C.-J. (2020). Robust
deep reinforcement learning against adversarial perturbations on state observations. Advances in
Neural Information Processing Systems 33 21024–21037. (p. 1.)
Zhao, W., Queralta, J. P. and Westerlund, T. (2020). Sim-to-real transfer in deep rein-
forcement learning for robotics: a survey. In 2020 IEEE symposium series on computational
intelligence (SSCI). IEEE. (p. 1.)
Zhou, D., Gu, Q. and Szepesvari, C. (2021a). Nearly minimax optimal reinforcement learning
for linear mixture markov decision processes. In Conference on Learning Theory. PMLR. (pp. 3,
11, and 49.)
Zhou, R., Liu, T., Cheng, M., Kalathil, D., Kumar, P. and Tian, C. (2023). Natu-
ral actor-critic for robust reinforcement learning with function approximation. arXiv preprint
arXiv:2307.08875 . (pp. 2 and 5.)
Zhou, Z., Zhou, Z., Bai, Q., Qiu, L., Blanchet, J. and Glynn, P. (2021b). Finite-sample
regret bound for distributionally robust offline tabular reinforcement learning. In International
Conference on Artificial Intelligence and Statistics. PMLR. (p. 5.)
53