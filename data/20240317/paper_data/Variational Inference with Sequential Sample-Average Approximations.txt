VISA: Variational Inference with Sequential Sample-Average Approximations
HeikoZimmermann1 ChristianA.Naesseth1 Jan-WillemvandeMeent1
Abstract Rezendeetal.,2014)areoftenconsideredthegoldstandard
forgeneratinghighqualitysamplesfromtheposteriordistri-
We present variational inference with sequen-
bution. However,itisnotalwayspracticalorpossibletouse
tial sample-average approximations (VISA), a
adifferentiablemodel. Theimplementationofthesimulator
method for approximate inference in computa-
maynotsupportdifferentiation,orthemodelitselfmaynot
tionallyintensivemodels,suchasthosebasedon
bedifferentiable,forexamplebecauseitemploysdiscrete
numericalsimulations.VISAextendsimportance-
randomvariablesorstochasticcontrolflow. Insuchcases,
weightedforward-KLvariationalinferencebyem-
inferencefallsbackonmethodsbasedonscore-functiones-
ployingasequenceofsample-averageapproxima-
timators(Glynn,1990;Wingate&Weber,2013;Ranganath
tions, which are considered valid inside a trust
etal.,2014)orimportance-weightedforward-KLvariational
region. This makes it possible to reuse model
inference(IWFVI),whichderivesfromreweightedwake-
evaluationsacrossmultiplegradientsteps,thereby
sleepmethods(Bornschein&Bengio,2014;Leetal.,2018).
reducingcomputationalcost. Weperformexper-
Thesemethodsarelesscomputationallyefficient,butremain
iments on high-dimensional Gaussians, Lotka-
themostviableoptioninasubstantialnumberofusecases.
Volterradynamics,andaPickoverattractor,which
demonstratethatVISAcanachievecomparable Inthispaper,wepresentVISA,amethodthathasthepoten-
approximationaccuracytostandardimportance- tialtosubstantiallyimprovethecomputationalefficiencyof
weightedforward-KLvariationalinferencewith variationalinferenceformodelsthatarenon-differentiable
computationalsavingsofafactortwoormorefor and computationally intensive. The key idea is that eval-
conservativelychosenlearningrates. uation of the variational approximation will typically be
cheap relative to that of the model. This means that we
cansavecomputationbyreusingmodelevaluationsacross
1.Introduction
multiple updates of the variational posterior. To this end,
weadaptIWFVItoemployaseriesofSample-AverageAp-
Thedevelopmentofgeneral-purposemethodsforBayesian
proximations(SAA),whichuseafixedsetofsamplesthat
inference,suchasthoseprovidedbyprobabilisticprogram-
definesadeterministicsurrogatetotheobjective,ratherthan
mingsystems(vandeMeentetal.,2018),hasmadeitpossi-
generatingafreshsetofsamplesateachgradientstep.
bletoapplyBayesiananalysistosimulation-basedmodels
inthenaturalandphysicalsciences. Thereasonthatthese SAAmethodswererecentlystudiedinthecontextofrepa-
methods are so broadly applicable is that they generate rameterizedblack-boxvariationalinference(Giordanoetal.,
samplesbywayofrepeatedevaluationofthemodel. How- 2024;Burronietal.,2023)whichoptimizesthereverseKL-
ever,thisgeneralityoftencomesatsignificantcomputational divergence. Thesemethodsfixsamplesfromaparameter-
cost. Inferencemayrequirethousandsofmodelevaluations, free distribution, which are transformed to samples from
whereeachevaluationmayitselfinvolvecomputationally theapproximateposteriorusingadifferentiablemap,whose
intensiveoperationssuchasnumericalintegrationofadif- parametersareoptimizedtomaximizethevariationalbound.
ferentialequation,oracalltoanumericalsolver. VISA differs from these methods in that it optimizes a
forward-KLdivergenceanddoesnotrequireadifferentiable
Gradient-basedmethodshavebecometheworkhorseforin-
model. Concretely, VISA fixes samples from a parame-
ferenceinsimulation-basedmodels. Whenamodeldefines
terizedvariationaldistribution,ratherthansamplesfroma
a fully differentiable density, methods based on Hamilto-
parameter-freedistribution. Sincethevariationaldistribu-
nianMonteCarlo(Hoffman&Gelman,2014)andreparam-
tionwillchangeduringoptimization,weconstructanew
eterized variational inference (Kingma & Welling, 2013;
SAAwhenevertheoptimizationleavesatrustregion,which
1 AmsterdamMachineLearningLab,UniversityofAmster- we define in terms of the effective sample size. VISA is
dam,Amsterdam,TheNetherlands. Correspondenceto: Heiko a drop-in replacement for IWFVI, in which samples are
Zimmermann<h.zimmermann@uva.nl>. re-usedasmuchaspossible,therebysavingcomputation.
1
4202
raM
41
]LM.tats[
1v92490.3042:viXraVISA:VariationalInferencewithSequentialSample-AverageApproximations
WeevaluateVISAinthecontextofthreeexperiments. We 2012),whichtendtoexhibitahighdegreeofvariance,or
first consider high-dimensional Gaussians, where the ap- make use of the reparameterization trick, which is com-
proximationerrorcanbecomputedexactly. Wethencon- monly employed in variational autoencoders (Kingma &
siderinferenceinaLotka-VolterrasystemandaPickover Welling,2013;Rezendeetal.,2014).
Attractor,wherenumericalintegrationisperformedaspart
Reparameterizationisamethodforgeneratingz ∼ q by
oftheforwardsimulation. OurresultsshowthatVISAwith ϕ
wayofaparametricpushforwardofrandomvariablesξthat
aconservative(i.e.smallerthanneeded)stepsizecancon-
aredistributedaccordingtoaparameter-freedensityq ,
vergeinasmallernumberofmodelevaluationsthanIWFVI ξ
withamorecarefullytunedstepsize. Theseresultscome
z =T (ξ), ξ ∼q .
ϕ ξ
withthecaveatthatVISAismoresusceptibletobiasthan
IWFVI,especiallywhenusedwithaloweffectivesample InVI,reparameterizedsamplescanbeusedtocomputean
sizethreshold. Inourexperiments,savingsofafactortwo unbiasedestimateofthegradient,
ormorearerealizablewithconservativelychosenlearning
(cid:20) (cid:21)
rates,whileVISAperformsonparwithIWFVIformore d d p(y,T (ξ))
− L (ϕ)= E log ϕ
carefullytunedstepsizes. dϕ R qξ dϕ q ϕ(T ϕ(ξ))
2.Background ≈
1 (cid:88)N d logp(y,T ϕ(ξ(i)))
, ξ(i) ∼q .
N dϕ q (T (ξ(i))) ξ
i=1 ϕ ϕ
WefirstbrieflyreviewVIwithSGDandSAAs,beforewe
introduceVISAinSection3. Readersfamiliarwiththese
ReparameterizedVIrequiresamodelp(y,z)thatisdiffer-
topicscansafelyskipahead.
entiablewithrespecttozinordertocomputethepathwise
derivative(Rezendeetal.,2014),
2.1.VariationalInference (cid:12)
Variational Inference (VI) methods approximate an in-
dd ϕlogp(y,T ϕ(ξ))= ∂∂
z
logp(y,z)(cid:12) (cid:12)
(cid:12)
∂T ∂ϕ ϕ(ξ) .
z=Tϕ(ξ)
tractable target density with a tractable variational distri-
butionbysolvinganoptimizationproblem. Theobjectiveis Thismeansthatthemodelmustsupport(automatic)differen-
typicallytominimizeadivergencemeasureDbetweenthe tiationandmustnotmakeuseofdiscreterandomvariables
variationalapproximationq withparametersϕ ∈ Φand orstochasticcontrolflow,suchasifstatementsthatbranch
ϕ
thetargetdensityπ, onrandomvalues(vandeMeentetal.,2018).
min{L(ϕ):=D(q ,π)+const.}. (1) Importance-WeightedForward-KLVI. Whenminimiz-
ϕ
ϕ∈Φ
ingtheforwardKLdivergence,weapproximatethegradient
Inthisexposition,weassumethatthetargetdensityisthe d (cid:20) d (cid:21)
− L (ϕ)= E logq (z) .
posteriorofaprobabilisticmodelπ(z)=p(z |y)forwhich dϕ F dϕ ϕ
p(·|y)
weareabletopoint-wiseevaluatethejointdensityp(y,z).
Thisdoesnotrequiredifferentiabilityofthemodelp(y,z)
ThetwomostcommonapproachestoVIaretominimize
withrespecttoz,butdoesrequireapproximateinferenceto
thereverseorforwardKLdivergence,forwhichobjectives
generatesamplesfromtheposteriorp(z |y).
canbedefinedintermsofalowerandupperboundonthe
logmarginallogp(y), InIWFVI,thevariationaldistributionq isusedasapro-
ϕ
posal for a self-normalized importance sampler. This ap-
(cid:20) (cid:21)
p(y,z)
L (ϕ)=−E log =KL(q ||p(·|y))−logp(y), proachmakesuseofthefactthatwecanexpressthegradient
R qϕ q ϕ(z) ϕ asanexpectationwithrespecttoq ϕ,
(cid:20) (cid:21)
p(y,z)
L F(ϕ)= p(E
·|y)
log
q ϕ(z)
=KL(p(·|y)||q ϕ)+logp(y).
−
dd
ϕL F(ϕ)= qE
ϕ(cid:20)
w ϕ(z)
dd
ϕlogq
ϕ(z)(cid:21)
, w ϕ(z)=
p q( ϕz (| zy ))
.
WewillbrieflydiscussstandardreparameterizedVI,which Here the ratio w (z), known as the importance weight,
ϕ
maximizesthelowerbound−L R,andIWFVI,whichmini- needstobeapproximated,sincewecannotcomputep(z |y).
mizestheupperboundL .
F Todoso,wedefinetheself-normalizedweights
ReparameterizedVI. Whenmaximizingalowerbound w¯(i) p(y,z(i))
withstochasticgradientdescent,wecaneitheremployscore- wˆ(i) = ϕ , w¯(i) = ,
functionestimators(Ranganathetal.,2014;Paisleyetal.,
ϕ (cid:80)N j=1w¯ ϕ(j) ϕ q ϕ(z(i))
2VISA:VariationalInferencewithSequentialSample-AverageApproximations
leadingtothegradientestimate 3.SAAforForward-KLVariationalInference
d (cid:88)N d TheprimarymotivationbehindexistingSAA-basedmeth-
L (ϕ)≃ wˆ(i) logq (z(i)), z(i) ∼q . (2) odsforreparameterizedVI(Giordanoetal.,2024;Burroni
dϕ F dϕ ϕ ϕ
i=1 et al., 2023) is that fixing the noise realizations defines a
completelydeterministicsurrogateobjectiveLˆ (ϕ),which
Theresultingestimateisbiasedbutconsistent,meaningthat R
canthenbeusedwithanynumberofexistingoptimizers.
itconvergestothetruegradientasN →∞almostsurely.
Themainrequirementfromanimplementationpointofview
is that the model density p(y,z) is differentiable with re-
2.2.VIwithSample-AverageApproximations specttoz. Inthissettingitisalsonecessarytoevaluatethe
modelforeveryupdate,sinceanychangetoϕalsochanges
Inthestochasticoptimizationliterature,sample-averageap-
thevaluesofthetransformedsamplesz(i) =T (ξ(i)).
proximationsareusedtoapproximateanexpectedlosswith ϕ
asurrogatelossintheformofaMonteCarloestimate(see In developing VISA, both our motivation and implemen-
Kimetal.(2015)forareview). Importantly,thesamples tationrequirementsaresomewhatdifferent. Ourprimary
thattheSAAisbasedonremainfixedthroughouttheopti- interestisinminimizingthetotalnumberofmodelevalu-
mizationprocess. Thismeansthatthesurrogateobjective ationsatconvergence. Wealsowishtodevelopamethod
canbetreatedlikeanyotherdeterministicfunction,which thatisapplicablewhenthemodeldensityp(y,z)isnotdif-
can be optimized using second-order methods and other ferentiable,eitherbecausetheimplementationsimplydoes
standardoptimizationtools. notsupport(automatic)derivatives,orbecausethemodel
incorporates discrete variables or stochastic control flow,
Concretely,asample-averageapproximationappliestoan
whichintroducediscontinuitiesinthedensityp(y,z).
optimizationproblemoftheform
Tothisend,weproposeamethodthatoptimizesaforward
(cid:26) (cid:27)
min L(ϕ):=E[ℓ(z,ϕ)] , (3) KLwithanimportanceweightedobjectivethatincorporates
ϕ∈Φ ρ ideasfromSAA-basedapproaches. Inasettingwherewe
alreadyhaveaccesstosamplesfromtheposterior,wecould
inwhichthedensityρ(z)doesnotdependontheparameters
triviallydefineaSAAfortheupperboundL (ϕ),
ϕ. ThismeansthatwecancomputeasurrogatelossLˆ(ϕ) F
that is an unbiased estimate of the original loss L(ϕ) by
1 (cid:88)N p(y,z(i))
averagingoversamplesfromρ, Lˆ (ϕ)= log , z(i) ∼p(·|y).
F N q (z(i))
i=1 ϕ
N
1 (cid:88)
Lˆ(ϕ)= l(z(i),ϕ), z(i) ∼ρ. Inpractice,thisnaiveapproachisunlikelytobeusefulin
N
i=1 asettingwhereevaluationofp(y,z)iscomputationallyex-
pensive,sincewewouldstillneedtocarryoutapproximate
Undermildconditionsonℓandρ, asthenumberofsam-
inferencetogenerateasetofsamplesfromtheposterior.
plesN → ∞,theminimizerϕˆ = argmin Lˆ(ϕ)andthe
ϕ
minimalvalueLˆ(ϕˆ)convergealmostsurelytotheminima WethereforeadopttheapproachusedinIWFVI,whichuses
ϕ∗ =argminL (ϕ)andminimalvalueL(ϕ∗)oftheorigi- thevariationaldistributionasaproposalinaself-normalized
ϕ
nalproblem. importance-sampler. To define an SAA for the objective
inthissetting,weexpresstheobjectiveatparametersϕin
InthecontextofreparameterizedVI,asample-averageap-
termsofanexpectationwithrespecttoadistributionfrom
proximationcanbeconstructedbyfixingasetofsamples thesamefamilywithfixedparametersϕ˜,
{ξ(i) ∼q }N fromaparameter-freedistribution,
ξ i=1
(cid:20) (cid:21) (cid:20) (cid:21)
p(y,z) p(y,z)
Lˆ R(ϕ)=
N1 (cid:88)N logp q(cid:0) y (cid:0), TT
ϕ
(( ξξ (( ii )) )) (cid:1)(cid:1)
.
L F(ϕ)= p(E
·|y)
log q ϕ(z) = qE
ϕ˜
w ϕ˜(z)log q ϕ(z) .
i=1 ϕ ϕ AsinstandardIWFVI,theimportanceweightsw (z)are
ϕ˜
InanSAA-basedapproachtoreparameterizedVI(Giordano intractable,butwecandefineanSAAfortheobjectivein
etal.,2024;Burronietal.,2023),optimizationoftheparam- termsofself-normalizedweights,
etersϕwillmovethetransformedsamplesz(i) =T (ξ(i))
ϕ
to match the posterior density, whilst keeping the noise Lˆ (ϕ;ϕ˜)=(cid:88)N wˆ(i) logp(y,z(i)) , z(i) ∼q .
realizationsξ(i)fixed. Empiricalevaluationsshowthatcom- F ϕ˜ q (z(i)) ϕ˜
i=1 ϕ
biningtheSAAapproximationwithanoff-the-shelfsecond-
orderoptimizercanresultinsubstantialcomputationalgains Inthissurrogateobjective,whichwewilloptimizewithre-
aswellasmorereliableconvergencetotheoptimum. specttoϕ,thequalityoftheapproximationdependsonhow
3VISA:VariationalInferencewithSequentialSample-AverageApproximations
Φ
Algorithm1VISA S Z6,α(ϕ6) ...
Inp ϕ˜u ←t: ϕInitialparam. ϕ 0,tru ▷st Inre itg iaio lin zeth pr re os ph oo sld alα p, ad raa mta ey
ter
S Z3,α(ϕ3)
ϕ6
0
Z ←{z(i) ∼q ϕ˜}N
i=1
▷Initializesamples S Z0,α(ϕ0) ϕ3
fLˆ oF r(ϕ t=;ϕ˜ 1) ,= ..(cid:80) .,TN i= d1 owˆ ϕ( ˜i) logp q( ϕy (, zz (( ii )) )) ▷InitializeSAA ϕ0
Z6= {z 6(i) ∼qϕ6}N
i=1
ϕ t =optimizer-step(Lˆ F,ϕ t−1) Z3= {z 3(i) ∼qϕ3}N i=1
ifϕ ϕ˜t ←∈/ S ϕZ,α(ϕ˜)then ▷Noti ▷ns Uid pe datr tu es pt rr oe pg oio sn
al
Z0= {z 0(i) ∼qϕ0}N
i=1
t
Z ←{z(i) ∼q }N ▷Refreshsamples
ϕ˜ i=1 Figure1.Visualizationofparametertracesandtrustregionscorre-
Lˆ (ϕ;ϕ˜)=(cid:80)N wˆ(i) logp(y,z(i)) ▷RefreshSAA spondingtodifferentSAAs. Ifafteranupdateϕ∈/ S ,we
F i=1 ϕ˜ qϕ(z(i)) Z,α(ϕ˜)
endif setϕ˜←ϕtoconstructanewSAAandcorrespondingtrustregion.
endfor
which,foragiventhresholdαandsamplesZ ={z(i)}N ,
i=1
closelytheproposalwithparametersϕ˜matchestheposterior. mapseachparametertoacorrespondingtrustregionbased
Sinceourapproximationoftheposteriorwillimprovedur- onascoringfunction,whichwechoosetobetheESS,
ingoptimization,wewillupdateϕ˜tothecurrentparameter
S (ϕ˜)={ϕ∈Φ |s (ϕ˜,ϕ)>α}
valuesϕatsomeinterval,resultinginanapproachthatwe Z,α Z
willrefertoasasequentialsample-averageapproximation.
(cid:16)
(cid:80)N
v
(z(i))(cid:17)2
s (ϕ˜,ϕ)= i=1 ϕ˜,ϕ .
TodeterminewhenweneedtogenerateafreshSAA,we Z (cid:16) (cid:17)2
N(cid:80)N v (z(i))
willdefinethenotionofatrustregion. Thisdefinesanopti- i=1 ϕ˜,ϕ
mizationprocessinwhichtheSAAisrefreshedwhenever
Inotherwords,foragivenESSthresholdαwecanverify
theoptimizationtrajectoryleavesthecurrenttrustregion.
ϕ ∈ S (ϕ˜) by checking s (ϕ˜,ϕ) > α. We visualize
ThisoptimizationprocessisillustratedinFigure1andde- Z,α Z
hownewtrustregions,correspondingtodifferentSAAs,are
scribedschematicallyinAlgorithm1. Webeginbysetting
the proposal parameters ϕ˜ = ϕ to the initial variational constructedsequentiallyduringoptimizationinFigure1.
0
parameters,generatingasetofsamplesZ ={z(i)}N i=1and EffectoftheTrust-RegiononConvergence. Foralow
defining an SAA of the objective Lˆ (ϕ;ϕ˜) and a trust re- enoughthresholdαthealgorithmmightconvergetoanopti-
F
gionS Z,α(ϕ˜)basedonthesesamples. Wethenrepeatedly malparameterϕˆwithinthetrustregionofthecurrentSAA
updateϕ usinganoptimizeruntilthevalueϕ nolonger thatdoesnotyetsatisfyourglobalconvergencecriteria,i.e.
t t
liesinthetrustregion. Atthispoint,weupdatetheproposal sufficientlyminimizestheforwardKL-divergence. Inthese
parametersϕ˜=ϕ andgenerateafreshsampleset,which cases, if the variational approximation is not degenerate,
t
wethenusetoupdatetheSAAandthetrustregion. we can try recover by decreasing α such that ϕˆ ∈/ S (ϕ˜)
α
and continue optimization. As α → 1 the frequency of
DefiningTrustregions. Todefineanotionofatrustre-
sampleacquisitionincreasesand,forα=1VISAreduces
gion,wecomputeaneffectivesamplesize(ESS),whichis
tostandardIWFVI.Inthisworkwechoseαhighenough
aproxymeasureforthevarianceoftheimportanceweights.
thatthealgorithmdoesnotconvergeprematurelytoanop-
Notably,theimportanceweightscanbedecomposedinto
tima of an intermediate SAA. In these cases we find that
two parts, (1) the ratio of the variational density and the
convergenceofthetraininglosscanbeusedasindicatorfor
trustregiondensity,and(2)theratiobetweenposteriorand
convergenceoftheforwardKL-divergenceorcorrespond-
variationaldensity,
ingupperbound,whichweverifyinourexperiments. We
q (z) alsoexperimentedwithcachingpastsamplesetstocompute
w (z)=v (z)w (z), v (z)= ϕ .
ϕ˜ ϕ,ϕ˜ ϕ ϕ,ϕ˜ q (z) asecondarylossbasedonthelastM SAAs. Whilewedid
ϕ˜
notfindittoaddadditionalvalueinourexperiments,itbea
Thevarianceofw ϕisindependentofthefixedproposalpa- usefultooltoassessconvergenceinothersettings.
rametersanddecreasesduringoptimization.Thevarianceof
v measureshowsimilarq istoq andcanbecontrolled EfficientImplementation. Toavoidrecomputingdensity
ϕ,ϕ˜ ϕ˜ ϕ
valuesforoldsamplelocations,wecachebothsamplelo-
byupdatingtheproposalparametersϕ˜totheparametersof
cation z(i) and the corresponding log-joint density of the
thecurrentvariationalapproximationϕoncetheESSdrops
modellogp(z(i),y). Ifsamplingfromtheproposalischeap
belowacertainthreshold.
and memory is of concern, e.g. for large samples set or
Weformalizethisnotionbydefiningaset-valuedfunction ifpastsamplesetsarestoredtocomputeavalidationloss,
4VISA:VariationalInferencewithSequentialSample-AverageApproximations
we can store the random seed instead of the sample and complextargetdensities.
rematerializethesamplewhenneeded.
4.RelatedWork
5.Experiments
VIwithSAAs RecentworkthatstudiesSAAs(Giordano
We compare VISA to standard importance-weighted VI
etal.,2024;Burronietal.,2023)inthecontextofvariational
(IWFVI)intermsofinferencequalityandthenumberof
inferencefocusesonthereparameterizedblack-boxVIset-
model evaluations. We assess inference quality by mea-
tingandoptimizesareverseKL-divergence. Thesemethods
suringorestimatingtheforwardKL-divergencetothetrue
relyonreparameterizationtomovesamplestoareasofhigh
posterior,orwhensuchanestimateisnotavailablebyas-
posteriordensitywhilekeepingafixedsetofnoiserealiza-
sessingthelog-jointprobabilityofsamplesunderthemodel.
tionsfromthebasedistribution,whichdoesnotdependof
IntheGaussianexperimentwealsocomparetoblack-box
thevariationalparameters. Optimizingadeterministicob-
VI,whichoptimizesthereverseKL-divergence,andthere-
jectiveallowstheauthorstousesecond-orderoptimization
forereportinferencequalitybymeasuringthesymmetric
andlinearresponsemethods(Giordanoetal.,2015)tofitco-
KL-divergencetoallowforafaircomparison.
variances. Whilethesemethodsallowtorealizesubstantial
gainsintermsofinferencequalityandefficiency,incontrast
5.1.Gaussians
toVISA,theyrequiredifferentiabilityofthemodel.
TostudytheeffectofdifferentlearningratesandESSthresh-
Stochastic second-order optimization. There is also oldparameters,wefirstevaluateVISAonapproximating
work outside of the context of SAAs that aims to incor- medium-tohigh-dimensionalGaussiansandcomparethe
poratesecondorderinformationtoimprovestochasticop- inference performance over the number of model evalua-
timization and variational inference. Byrd et al. (2016) tionstoIWFVI,standardreparameterizedvariationalinfer-
proposebatched-L-BFGS,whichcomputesstablecurvature ence (BBVI-RP) and variational inference using a score-
estimatesbysub-sampledHessian-vectorproductsinstead functiongradientestimator(BBVI-SF).Notably,weinclude
ofcomputinggradientdifferencesateveryiteration. This BBVI-RPasareferenceonly,showcasingthatfastercon-
work has also been been adopted to the variational infer- vergencecanbeachievedbyleveragingthedifferentiability
ence setting by Liu & Owen (2021). Pathfinder (Zhang ofthemodel,anddonotcomparetoitdirectly. Toallowfor
etal.,2022)usesaquasi-Newtonmethodtofindthemode afaircomparisonbetweenmethodsthatoptimizeaforward
oftheatargetdensityandconstructnormalapproximations KL-divergence(VISA,baseline)andmethodsthatoptimize
tothedensityalongtheoptimizationpath. Theintermedi- areverseKL-divergence(BBVI),weevaluatetheinference
atenormalapproximationsareusedtodefineavariational qualifyintermsofthesymmetricKL-divergence.
approximation that minimizes an evidence lower bound.
We study two different target densities, a (1) D = 128
SimilartoSAA-basedmethods,pathfindercanreducethe
dimensionalGaussianwithadiagonalcovariancematrixand
numberofmodelevaluationsbyuptoanorderofmagnitude
(2)D =32dimensionalGaussianwithadensecovariance
comparedtoHMC,butrequiresadifferentiablemodel.
matrix. For the diagonal covariance matrix, we scale the
variancesoftheindividualdimensionfromσ = 0.1to
min
VIwithforwardKL-divergence. VISAisalsorelatedto
σ =1suchthatthecovariancematrixtakestheform
max
othermethodsthataimtooptimizeaforwardKL-divergence
or its stochastic upper bound. This includes reweighted- (cid:32)(cid:20) σ −σ (cid:21)D (cid:33)
C =diag σ +(i−1)∗ max min .
wakesleep(andwake-wake)methods(Bornschein&Ben- diag min D−1
i=1
gio,2014;Leetal.,2018)towhichwecompareVISAinthe
experimentsection,aswellastheirdoubly-reparameterized To create the dense covariance matrix we first sample a
variants(Tuckeretal.,2018;Finke&Thiery,2019;Bauer random positive semi-definite matrix M = AAT, where
&Mnih,2021),whicharenotdirectlycomparableasthey A ∼U(0,1)andthenconstructthecovariancematrix
ij
requireadifferentiablemodel.Whilethemethodsaboveuse
(cid:18) (cid:19)
asingleimportancesamplingstepusingthevariationalap- C = M +0.1I .
proximationasaproposal,othermethodsusemorecomplex dense ||M|| F
proposalincludingMCMCproposals(Naessethetal.,2020;
Zhangetal.,2023),approximateGibbskernels(Wuetal., Figure2showstheresultsfordifferentlearningrateslr∈
2020),orproposaldefinedbyprobabiliticprograms(Stites {0.001,0.005,0.01,0.05} and ESS threshold parameters
etal.,2021;Zimmermannetal.,2021). Whilethesemeth- α ∈ {0.9,0.95,0.99}. Wecomputegradientestimatesfor
odsdonotnecessarilyrequireadifferentiableproposalthey VISA,IWFVI,andBBVI-SFwithN = 10samples, and
arenotdesignedtobesampleefficientbuttoapproximate gradientestimatesforBBVI-RPusingasinglesample.
5VISA:VariationalInferencewithSequentialSample-AverageApproximations
Figure2.SymmetricKL-divergenceasafunctionofthenumberofmodelevaluationsforaGaussiantargetwithdiagonalcovariance
matrix(toprow)anddensecovariancematrix(bottomrow).Forsmalllearningrates(0.001,0.005,0.01)IWFVIandBBVI-SF,needa
largernumberofmodelevaluationstoconverge.VISAconvergesmuchfasterasitcompensatesforthesmallstepsizebyreusingsamples.
Foralearningrateof0.05VISAfailstoreliablyconverge,whileIWFVIstillconverges.Overall,VISAconvergesfasteroratthesame
rateasIWFVIandBBVI-SFwiththesameorhigherlearningrates.
WeobservethatVISAconvergessubstantiallyfasterthan 5.2.Lotka-Volterra
IWFVIandBBVI-SFatlowerlearningrates(0.001,0.005,
The Lotka-Vorterra predator-prey population dynamics
0.01)forboth,targetswithdiagonal-anddensecovariance
(Lotka,1925;Volterra,1927)aremodeledbyapairoffirst-
matrix. Thedifferenceintheconvergencerategetslesspro-
orderordinarydifferentialequations(ODEs),
nouncedasthelearningrateincreases. Forlargelearning
rates(0.05)VISAfailtoconvergereliably, whileIWFVI du dv
=(α−βv)u, =(−γ+δu)v,
stillconverges. Forevenhigherlearningratesallmethods dt dt
butBBVI-RPfailtoconverge. Ifwecompareconvergence
wherevdenotesthepredator-andudenotesthepreypopu-
acrosslearningrates,weobservethatVISAconvergesfaster
lation. Wewillinthefollowingdenotethepairofpredator-
oratthesamerateasIWFVIwiththesameorhigherlearn-
preypopulationsattimetwithz =(u ,v ). Thedynamics
t t t
ingrate. IntheGaussianexperiment,theoveralleffectof
oftheODEaregovernedbyitsrespectivepopulationgrowth
the threshold α on the convergence rate is minor, but we
andshrinkageparametersθ =(α,β,γ,δ),whichwewould
observeslightlyfasterconvergenceforlowerESSthresh-
liketoinfertogetherwiththeinitialconditionsofthesystem
olds. Wealsofindthat,especiallyforlowerESSthresholds,
givennoisyobservationsy =(y ,...,y ).
1:T 1 T
VISAisalsomoresusceptibletounderestimatingposterior
varianceandmightnotfullyconvergeinthefinalphaseof FollowingCarpenter(2018),weplacepriorsovertheinitial
training. Wehypothesizethatthisisduetothedeterministic populationsizez 0andsystemparametersθ
optimizationprocedure,whichoverfitstohighprobability zprey,zpred ∼LogNormal(log(10),1), (4)
samplesifthesamplesarenotrefreshedfrequentlyenough. 0 0
α,γ ∼Normal(1,0.5), (5)
ApossibleremedyforthisistoincreasetheESSthreshold
duringthefinalphaseoftraining,forα→1thisrecovers β,δ ∼Normal(0.05,0.05), (6)
thebehaviourofIWFVI.However,sincetheoveralleffect
andassumeafractionalobservationerror,
ofthethresholdontherateofconvergenceissmall, avi-
able strategy is to employ a fixed threshold value that is yprey,ypred ∼LogNormal(logz ,σ )
t t t t
somewhatconservative,suchasα=0.95orα=0.99.
σ ∼LogNormal(−1,1).
t
OverallwefindthatVISAconvergesfasteroratthesame
Given an initial population z , system parameters θ, and
0
rateasIWFVIandBBVI-SFforarangeofdifferentlearning
observations y we can solve the ODE numerically to
1:T
ratesandESSthresholds. MoreoverVISAissubstantially
obtain approximate population sizes z for time steps
1:T
more robust to the choice of learning rate. Using a de-
1,...,T which we use to compute the likelihood of the
terministicobjectiveallowsustodrawfreshsamplesless
observedpredator-preypopulations,
frequentlyandconsequentlyrequireslessevaluationsofa
potentiallyexpensivetoevaluatemodellog-jointdensity. (cid:89)T
p(y |z ,θ)= p(y |z ).
1:T 0 t t
t=0
6VISA:VariationalInferencewithSequentialSample-AverageApproximations
Figure3.ResultsforLotka-Volterramodelwithdifferentlearningrates.(Toprow)Trainingobjectiveovernumberofmodelevaluations.
(Middlerow)ApproximateforwardKL-divergencecomputedonreferencesamplesobtainedbyMCMC.Forsmallerstepsizes(0.001,
0.005)VISAachievescomparableforwardKL-divergencetoIWFVIwhilerequiringsignificantlylessmodelevaluationstoconverge(see
verticallines).Forlargerstepsizes(0.01)VISAonlyconvergeswithahighessthreshold(0.99)forwhichitrequiresapproximatelythe
samenumberofevaluationsasIWFVI.(Bottomrow)Gradientstepsovernumberofbatchevaluationsofthemodel,eachbatchevaluation
correspondstoevaluatingabatchofN =100samples.VISArequiresfewerevaluationspergradientstepcomparedtoIWFVI.
Our goal is to learn an approximation to the posterior whichweevaluatealongwiththetrainingobjectiveduring
p(θ,z |y)byminimizingtheevidenceupperbound optimizationtoassessconvergence.
0
WefindthatVISAisabletoobtainvariationaldistribution
(cid:20) (cid:21)
p(z ,y,θ)
L (ϕ):=E log 0 . (7) of similar quality to IWFVI while requiring significantly
F (z0,θ)∼p(·,·|y) q (z ,θ)
ϕ 0 fewermodelevaluationsforsmallerlearningrates(seeFig-
ure 3). Interestingly, VISA requires significantly fewer
Wemodelthevariationalapproximationq fortheinterac-
ϕ modelevaluationspergradientstepduringtheearlystages
tionparametersθandinitialpopulationsizesz asjointly
0 of training, while requiring slightly more evaluations per
log-normalandinitializeϕsuchthatthethemarginalover
gradientstepthereafter. Wehypothesisethatthisisagaina
z matches the prior (Equation 4) and the marginal over
0 resultofunderapproximatingposteriorvarianceinthelater
θ has similar coverage to the prior. We approximate the
stagesof training. As aresult, evensmall changesinthe
objectiveanditsgradientusingwithN =100samples. To
variationaldistributioncanleadtobigchangesintheESS,
specify a common convergence criteria, we compute the
whichtriggersthedrawingoffreshsamples. ForVISA,we
highest common test loss value for VISA with α = 0.99 alsofindamorepronounceddifferencebetweendifferent
andIWFVIthatisnotexceededbymorethan1natbyall
ESSthresholdsandtheirinfluenceonconvergence. Runs
consecutive test loss values. The convergence threshold
withahigherESSthresholdsconvergemorestablyandare
computedthiswayis−712.6nats.
abletoachievelowertestlossinthefinalstagesoftraining.
To evaluate the inference performance, we first generate
N =10000approximateposteriorsamplesusingaNo-U- 5.3.PickoverAttractor
Turn Sampler (NUTS) (Hoffman & Gelman, 2014) with
10000burn-instepsandwindow-adaption,whichgenerally
FollowingRainforthetal.(2016),wemodela3DPickover
provides good performance out of the box1. We use the
attractor(Pickover,1995)withparametersθ =(β,η),
approximateposteriorsamplestoapproximatean“oracle”
fortheupperboundinEquation7, x t+1,1 =sin(βx t,2)−cos(2.5x t,1)x t,3
x =sin(1.5x )x −cos(ηx )
t+1,2 t,1 t,3 t,2
1
(cid:88)N p(z(i),y,θ(i))
x =sin(x ).
LˆNUTS(ϕ)= log 0 , t+1,3 t,1
F N i=1 q ϕ(z 0(i),θ(i)) Due to its chaotic nature the system is sensitive to small
perturbationsinitsinitialstate, i.e. evensmallvariations
1NUTSisanadaptiveHamiltonianMonteCarlosamplerand
intheinitialstateleadtoexponentiallyfastdivergingtra-
usesthegradientinformationofthemodeltoguidethegeneration
ofproposals.Assuchitrequiresthelog-jointdensitymodeltobe jectories. Therefore, totracktheevolutionofthesystem,
differentiablewhichisnotrequiredbyVISAorIWFVI. weemployabootstrapparticlefilter(Gordonetal.,1993)
7VISA:VariationalInferencewithSequentialSample-AverageApproximations
Figure4.ResultsforPickoverattractor. (a)Approximatelog-jointdensityovernumberofbatch-evaluationsofmodel. (b)Log-joint
approximation plotted over domain of prior. The variational approximation capture the high density area containing the data. (c)
Visualizationofpickoverattractorwithgroundtruthparametersθ =[−2.3,1.25]. (d)Visualizationofattractorwithaveragesystem
parameterscomputedover10.000samplesfromthelearnedvariationalapproximation. Eachevaluationintheplotcorrespondsto
evaluatingabatchofN =10samples.
whichassumesnoisyobservationsy :=y andintroduces To obtain a tractable objective we replace the intractable
1:T
auxiliaryvariablesz :=z tomodelthelatentstateofthe marginallikelihoodp(y |θ)≈pˆ (y |θ)withthemarginal
1:T θ
system. Wedefinetheprioroversystemparameters likelihoodestimateobtainedbyrunningtheparticlefilter
(Naesseth et al., 2019), similar to pseudo-marginal meth-
(cid:40)
1/18 −3≤θ ≤3,0≤η ≤3 ods (Andrieu et al., 2010) and approximate the gradient
p(θ)= 1 ,
0 otherwise withN = 10samples. Asthelikelihoodestimateisnon-
differentiable due to the discrete ancestor choices made
andmodelthetransitionandobservationas, insidetheparticlefilter,wecannotrunNUTStoobtainap-
proximateposteriorsampleasbefore,butinsteadreportthe
z 0 ∼N(·|0 3,I 3), log-jointdensityofthevariationaldistribution.
z ∼N(·|h(z ,θ),σ ) fort>0,
t t−1 z We observe that VISA converges more stably with fewer
y t ∼N(·|z t,σ y) fort≥0, samples compared to IWFVI and find that attractors cor-
respondingtosamplesfromthevariationalapproximation
whereσ = 0.01,σ = 0.2,andhevolvesthesystemby
z y lookqualitativelysimilartothosebasedonthetrueparame-
onetimestepusingtheequationsofthePickoverattractor
ters. WesummarizetheresultinFigure. 4.
described above. The particle filter is used to simulate
T =100timestepswithM =500particles,whichrenders
6.DiscussionandLimitations
evaluatingthemodelexpensive.
To restrict the proposal to the same domain as the prior InthispaperwedevelopedVISA,amethodforapproximate
byfirstsamplingfromaNormal,whichwedenoteq¯ and inferenceforexpensivetoevaluatemodelsthatoptimizes
ϕ
parameterizedwithameanandlowerCholeskyfactor,and the forward KL-divergence through a sequence of SAAs.
thentransformthesampleby EachSAAisoptimizeddeterministicallyandrequiresand
fixesasinglesetofsamples,herebyrequiringnewmodel
f(θ)=(tanh(3·θ 1),tanh(1.5·θ 2+1.5)). evaluationsonlywhentheSAAisrefreshed. Totrackthe
approximationqualityofthecurrentSAA,VISAcomputes
The density of the transformed samples is q ϕ(θ) = theESSoftheratiobetweenthecurrentvariationaldistribu-
q¯ ϕ(θ)|detdf d( θθ)|−1,whichisappropriatelyrestrictedtothe tionandtheproposaldistributionthatwasusedtoconstruct
domainoftheprior. the SAA. If the ESS falls below a predefined threshold,
a new SAA approximation is constructed based on fresh
Weareinterestedinapproximatingthemarginalposterior
samples from the current variational distribution. We ob-
p(θ |y)oversystemparametersbyoptimizingtheevidence
servegainsofafactor2ormoreintermsofthenumberof
upperbound
requiredmodelevaluationsforconservativelychosenstep
E
(cid:20) logp θ(y,θ)(cid:21)
= E
(cid:34) logE p pf[pˆ(y |θ)](cid:35) s raiz ce ys, asw Ih Wile FVac Ih ,i te hv ein eg qus ii vm ai ll ea nr tp mos et te hr oio dr thap ap tr do ox ei sm na oti to en ma pc lc ou y-
pθ(θ|y)
q ϕ(θ)
pθ(θ|y)
q ϕ(θ)
thesequentialsample-averageapproximation.
(cid:20) (cid:20) (cid:21)(cid:21)
pˆ(y |θ)
≤ E E log .
pθ(θ|y) p pf q ϕ(θ)
8VISA:VariationalInferencewithSequentialSample-AverageApproximations
Underapproximationofposteriorvariance. Bothrepa- Burroni, J., Domke, J., and Sheldon, D. Sample Aver-
rameterizedVI,whichoptimizesthereverseKL-divergence, age Approximation for Black-Box VI. arXiv preprint
andIWFVI,whichoptimizestheforwardKL-divergence arXiv:2304.06803,2023.
viaimportancesampling,arepronetounderapproximating
posteriorvariance. InthecaseofreparameterizedVI,this Byrd, R. H., Hansen, S. L., Nocedal, J., and Singer, Y.
canoftenbeattributedtothemodeseekingbehaviourofthe A Stochastic Quasi-Newton Method for Large-Scale
reverseKL-divergence,whileinIWFVItheloweffective Optimization. SIAM Journal on Optimization, 26(2):
samplessizescanleadtooverfittingtoasmallnumberof 1008–1031, January 2016. ISSN 1052-6234. doi:
high-weightsamples. Wefoundthatkeepingthesamples 10.1137/140954362.
fixed for too long, i.e. using an ESS threshold that is too
Carpenter, B. Predator-Prey Population Dynamics:
low,canexacerbatethisproblem,astheoptimizercantake
The Lotka-Volterra model in Stan. https://mc-
multiplestepstowardsthesamehigh-weightsamples.
stan.org/users/documentation/case-studies/lotka-
Giordanoetal.(2024)andBurronietal.(2023)showedthat volterra-predator-prey.html#abstract,January2018.
whenapplyingSAAtoreparameterizedVI,itispossibleto
makeuseofsecond-ordermethods. Weexperimentedwith Finke,A.andThiery,A.H. Onimportance-weightedau-
optimizingSAAswithL-BGFS,whichisaquasi-Newton toencoders. arXivpreprintarXiv:1907.10477,2019.
methodwithlinesearch. However,wefoundthatintheset-
Giordano,R.,Ingram,M.,andBroderick,T. Blackboxvari-
tingofoptimizingaforwardKLwithrelativelyfewsamples,
ationalinferencewithadeterministicobjective: Faster,
L-BGFScanamplifytheproblemofoverfitting,oftenlead-
more accurate, and even more black box. Journal of
ingtoinstabilitiesandcollapsedvariationaldistributions.
MachineLearningResearch,25(18):1–39,2024.
Numberoflatentvariablesandparameters. Because Giordano, R. J., Broderick, T., and Jordan, M. I. Linear
VISA employs a relatively small number of samples and Response Methods for Accurate Covariance Estimates
doesnotrefreshsamplesateveryiteration,wefoundthatit fromMeanFieldVariationalBayes. InAdvancesinNeu-
isnotwell-suitedtomodelswithalargenumberoflatent ralInformationProcessingSystems,volume28.Curran
variablesorlargenumberofparameters. Thisagreeswith Associates,Inc.,2015.
theoretical findings by Giordano et al. (2024), who show
thatSAAsforafullcovarianceGaussianfailifthenumber Glynn, P. W. Likelihood ratio gradient estimation for
ofsamplesisnotatleastinthesameregimeasthenumber stochastic systems. Communications of the ACM, 33
oflatentdimensions. Burronietal.(2023)managetotrain (10):75–84, October 1990. ISSN 0001-0782. doi:
fullcovariancenormalapproximationbyusingasequence 10.1145/84537.84552.
of SAAs using increasingly large sample sizes, however,
Gordon,N.J.,Salmond,D.J.,andSmith,A.F.M. Novel
thisisdirectlyopposedtoourgoalofreducingthenumber
approachtononlinear/non-GaussianBayesianstateesti-
ofmodelevaluationsinexpensivetoevaluatemodels.
mation. IEEProceedingsF(RadarandSignalProcess-
ing),140(2):107–113,April1993. ISSN2053-9045. doi:
Acknowledgement
10.1049/ip-f-2.1993.0015.
TheauthorswouldliketothankTamaraBroderickforhelp-
Hoffman,M.D.andGelman,A. TheNo-U-TurnSampler:
fuldiscussionaboutsample-averageapproximationsinthe
AdaptivelySettingPathLengthsinHamiltonianMonte
contextofreparameterizedvariationalinference.
Carlo. JournalofMachineLearningResearch,15(47):
1593–1623,2014. ISSN1533-7928.
References
Kim,S.,Pasupathy,R.,andHenderson,S.G. AGuideto
Andrieu,C.,Doucet,A.,andHolenstein,R.ParticleMarkov SampleAverageApproximation.Handbookofsimulation
ChainMonteCarloMethods. JournaloftheRoyalSta- optimization,pp.207–243,2015.
tisticalSocietySeriesB:StatisticalMethodology,72(3):
269–342,June2010. Kingma,D.P.andWelling,M. Auto-EncodingVariational
Bayes. ICLR 2014 conference submission, December
Bauer,M.andMnih,A. GeneralizedDoublyReparameter- 2013.
izedGradientEstimators. InInternationalConferenceon
MachineLearning,pp.738–747.PMLR,2021. Le,T.A.,Kosiorek,A.R.,Siddharth,N.,Teh,Y.W.,and
Wood,F. RevisitingReweightedWake-Sleep. Interna-
Bornschein, J. and Bengio, Y. Reweighted wake-sleep. tionalConferenceonLearningRepresentations,Septem-
arXivpreprintarXiv:1406.2751,2014. ber2018.
9VISA:VariationalInferencewithSequentialSample-AverageApproximations
Liu,S.andOwen,A.B. Quasi-MonteCarloQuasi-Newton Wu, H., Zimmermann, H., Sennesh, E., Le, T. A., and
inVariationalBayes. JournalofMachineLearningRe- Meent,J.-W.V.D.AmortizedPopulationGibbsSamplers
search,22(243):1–23,2021. ISSN1533-7928. withNeuralSufficientStatistics. InProceedingsofthe
37thInternationalConferenceonMachineLearning,pp.
Lotka,A.J. Principlesofphysicalbiology. 1925.
10421–10431.PMLR,November2020.
Naesseth,C.,Lindsten,F.,andBlei,D. MarkovianScore
Zhang, L., Carpenter, B., Gelman, A., and Vehtari, A.
Climbing: VariationalInferencewithKL(p||q). InAd-
Pathfinder: Parallelquasi-Newtonvariationalinference.
vancesinNeuralInformationProcessingSystems, vol-
Journal of Machine Learning Research, 23(306):1–49,
ume33,pp.15499–15510.CurranAssociates,Inc.,2020.
2022. ISSN1533-7928.
Naesseth,C.A.,Lindsten,F.,andThomasB.,S. Elements
Zhang,L.,Blei,D.,andNaesseth,C.A. TransportScore
ofSequentialMonteCarlo. FoundationsandTrendsin
Climbing: VariationalInferenceUsingForwardKLand
MachineLearning,2019.
Adaptive Neural Transport. Transactions on Machine
Paisley,J.,Blei,D.,andJordan,M. Variationalbayesianin- LearningResearch,May2023. ISSN2835-8856.
ferencewithstochasticsearch. InProceedingsofthe37th
Zimmermann,H.,Wu,H.,Esmaeili,B.,andvandeMeent,
InternationalConferenceonMachineLearning,2012.
J.-W.NestedVariationalInference.InAdvancesinNeural
Pickover,C.A. ThePatternBook:Fractals,ArtAndNature. InformationProcessingSystems,volume34,pp.20423–
WorldScientific,June1995. ISBN978-981-4504-03-4. 20435.CurranAssociates,Inc.,2021.
Rainforth, T., Le, T. A., van de Meent, J.-W., Osborne,
M.A.,andWood,F.Bayesianoptimizationforprobabilis-
ticprograms.AdvancesinNeuralInformationProcessing
Systems,29,2016.
Ranganath, R., Gerrish, S., andBlei, D. BlackBoxVari-
ational Inference. In Proceedings of the Seventeenth
InternationalConferenceonArtificialIntelligenceand
Statistics,pp.814–822.PMLR,April2014.
Rezende, D. J., Mohamed, S., and Wierstra, D. Stochas-
ticBackpropagationandApproximateInferenceinDeep
GenerativeModels. InProceedingsofthe31stInterna-
tionalConferenceonMachineLearning,pp.1278–1286.
PMLR,June2014.
Stites,S.,Zimmermann,H.,Wu,H.,Sennesh,E.,andvande
Meent, J.-W. Learning proposals for probabilistic pro-
gramswithinferencecombinators. InProceedingsofthe
Thirty-Seventh Conference on Uncertainty in Artificial
Intelligence,pp.1056–1066.PMLR,December2021.
Tucker,G.,Lawson,D.,Gu,S.,andMaddison,C.J. Doubly
Reparameterized Gradient Estimators for Monte Carlo
Objectives. arXivpreprintarXiv:1810.04152,2018.
van de Meent, J.-W., Paige, B., Yang, H., and Wood, F.
An Introduction to Probabilistic Programming. arXiv
preprintarXiv:1809.10756,2018.
Volterra, V. Fluctuations in the Abundance of a Species
considered Mathematically. Nature, 119(2983):12–13,
January1927. ISSN1476-4687. doi: 10.1038/119012b0.
Wingate, D. and Weber, T. Automated variational in-
ference in probabilistic programming. arXiv preprint
arXiv:1301.1299,2013.
10