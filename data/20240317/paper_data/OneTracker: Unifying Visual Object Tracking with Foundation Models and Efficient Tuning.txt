OneTracker: Unifying Visual Object Tracking with
Foundation Models and Efficient Tuning
LingyiHong1* ShilinYan1* RenruiZhang2 WanyunLi1 XinyuZhou1 PinxueGuo3
KaixunJiang3 YitingChen1 JinglunLi3 ZhaoyuChen3 WenqiangZhang1,4††
1 ShanghaiKeyLabofIntelligentInformationProcessing,
SchoolofComputerScience,FudanUniversity,Shanghai,China
2 TheChineseUniversityofHongKong
3 ShanghaiEngineeringResearchCenterofAI&Robotics,
AcademyforEngineering&Technology,FudanUniversity,Shanghai,China
4 EngineeringResearchCenterofAI&Robotics,MinistryofEducation,
AcademyforEngineering&Technology,FudanUniversity,Shanghai,China
{honglyhly, tattoo.ysl}@gmail.com, wqzhang@fudan.edu.cn
Abstract video frame based on the initial bounding box in the first
frame. Ithasvariousapplications,suchasself-driving[10,
Visualobjecttrackingaimstolocalizethetargetobjectof 36, 119], visual surveillance [83, 96], and video compres-
eachframebasedonitsinitialappearanceinthefirstframe. sion [46]. In addition to the conventional RGB track-
Depending on the input modility, tracking tasks can be di- ing (Figure 1 (a)), there are various downstream tracking
vided into RGB tracking and RGB+X (e.g. RGB+N, and tasksthatincorporateadditionalinformationandboostper-
RGB+D) tracking. Despite the different input modalities, formance, including RGB+N, RGB+M, and RGB+D/T/E
thecoreaspectoftrackingisthetemporalmatching. Based tracking. InRGB+Ntracking[28,63,91,106],thenatural
onthiscommonground,wepresentageneralframeworkto linguisticdescriptionsoftargetareadditionallyprovidedto
unify various tracking tasks, termed as OneTracker. One- excludetheinterferenceofsimilarobjectsandenhancethe
Trackerfirstperformsalarge-scalepre-trainingonaRGB localization. InRGB+Mtracking[24,41,74,97,105],the
tracker called Foundation Tracker. This pretraining phase masks of the target in the first frame are offered instead
equipstheFoundationTrackerwithastableabilitytoesti- of bounding boxes. In RGB+D/T/E tracking, the depth,
matethelocationofthetargetobject. Thenweregardother thermal, and event maps are utilized as an extra to handle
modality information as prompt and build Prompt Tracker with the vulnerability of RGB trackers to complex scenar-
uponFoundationTracker. ThroughfreezingtheFoundation iosandimprovetherobustness. Thegoalofallthesedown-
Tracker and only adjusting some additional trainable pa- stream tasks is to localize the target with the assistance of
rameters, Prompt Tracker inhibits the strong localization multimodal information. Thus, we unify them as a whole,
ability from Foundation Tracker and achieves parameter- terming them as RGB+X tracking (Figure 1 (b)). Despite
efficient finetuning on downstream RGB+X tracking tasks. the diversity of tracking tasks, the core objective remains
To evaluate the effectiveness of our general framework the same: localizing the target in the search frame given
OneTracker, which is consisted of Foundation Tracker and itsinitialappearance,similartotheunderlyingprinciplesof
Prompt Tracker, we conduct extensive experiments on 6 humanattentionmechanisms[8]. Cognitivescientistshave
populartrackingtasksacross11benchmarksandourOne- discovered that the human vision system builds the corre-
Tracker outperforms other models and achieves state-of- spondence or motion [2] on the temporal dimension [49]
the-artperformance. todeterminetheobject’spositioninthecurrentframe[27],
1.Introduction regardless of the form of additional modalities in various
trackingtasks.
Object tracking [4, 13, 57, 93, 121, 122] is a foundation Currently, there is a prevailing trend where models are
visual task, that involves localizing a target object in each designedandtrainedforspecifictasksusingdatafromcer-
tain domains, offering convenience and yielding competi-
*EqualContribution
†CorrespondingAuthor tive results on individual tasks. However, this design phi-
4202
raM
41
]VC.sc[
1v43690.3042:viXraGround Truth Ground Truth
Template Frame
Box Supervise BoxInstSupervise Box Supervise BoxInstSupervise
Frozen
Box Prediction Seg Prediction Box Prediction Seg Prediction
RGB Tracker Trainable
Prediction
Box Head Seg Head Box Head Seg Head
Search Frame
(a) RGB Tracking
Template Frame Transformer Encoder Layer TTP Transformer Encoder Layer …
… …
CMT Prompter
RGB+X Tracker
Transformer Encoder Layer TTP Transformer Encoder Layer
Prediction CMT Prompter
Search Frame Linear Projection Linear Projection Prompt Embedding
Prompt
“Toy in hand.”
“Toy in hand.” … RGB+N TrackingRGB+T Tracking
…
RGB+N Tracking RGB+M Tracking RGB+T Tracking Template Frame Search Frame Template Frame Search Frame RGB+M Tracking
Prompt
(b) RGB+X Tracking (c) Foundation Tracker Training (d) Prompt Tracker Adaption
Figure1.(a)ThedefinitionofRGBtracking.(b)ThedefinitionofRGB+Xtracking.(c)OverviewofFoundationTrackertraining.
(d)Theparameter-efficientfinetuningofPromptTracker.
losophypresentscertainchallenges. (1)Independentmod- ure1(d)).Incontrasttoleveraginganextraparallelmodule
els require customized architectures, resulting in complex to fuse multimodal information, we propose Cross Modal-
trainingproceduresandredundantparameters. (2)Forcer- ityTrackingPrompters(CMTPrompter)tointroducemulti-
tain tracking tasks, the limited availability of large-scale modalfeaturesinaprompt-tuningmanner.CMTPrompters
data severely restricts performance potential. (3) The sep- learn semantic understanding of multimodal information
arate design approach falls short of accurately simulating and integrate it with RGB images. Furthermore, to en-
humanattentionmechanisms,whicharecrucialintracking. hance the adaptation to downstream task, we replace the
Although some previous works [1, 68, 87, 92, 101, 102] vanilla Transformer layers with Tracking Task Perception
have made attempts to address these problems in a uni- Transformer(TTPTransformer)layers. Becausethelinear
fied model, they still exhibit certain limitations. [1, 102] layerinTransformercontainsmostoftheknowledgeofspe-
are not specifically designed for tracking tasks, result- cifictasks,weonlyintroducefewtrainableparametersinto
ing in sub-optimal performance on tracking benchmarks. eachlinearlayerofTransformertobridgetheRGBtracking
[68, 87, 92, 101] only consider RGB images as input. and RGB+X tracking. By leveraging CMT Prompter and
[108, 123] attempt to utilize the multi-modal information, TTPTransformerlayer,PromptTrackerinheritsthestrong
but their applicability is limited to RGB+D/T/E tracking localization ability from Foundation Tracker and achieves
tasks. Moreover, these models fail to capture the unified competitiveperformanceondownstreamRGB+Xtracking
temporal attention mechanisms observed in human track- tasksafteraquickfine-tuningonasmallnumberofparam-
ing. eters. Giventheminimalnumberofadditionalparameters,
To address these challenges, we propose OneTracker, Prompt Tracker maintains a similar speed to Foundation
a general framework to unify RGB and RGB+X tracking Tracker.
within a consistent format. OneTracker firstly presents a Overall,ourcontributionsaresummariedasfollows:
FoundationTrackerforRGBtrackingtasks,andthenadapts • Wepresentaunifiedtrackingarchitecture,termedasOne-
ittoRGB+Xwithparameter-efficientstrategy. Indetail,we Tracker, which is consisted of Foundation Tracker and
pretrainaFoundationTracker[35]onseveralRGBtracking PromptTracker,totacklevariousformsoftrackingtasks,
datasets[28,45,69](Figure1(c)). i.e. bothRGBtrackingandRGB+N/M/D/T/Etracking.
After the large-scale pretraining, Foundation Tracker • WeproposeaFoundationTrackertrainedonseveralRGB
possessesstronglocalizationcapabilities,allowingittoac- trackingdatasets,whichownsstrongabilitytoaccurately
curately locate the target object in the search frame based localizetargetobjectsinsearchframe.
on its appearance in the template frame. Then, we pro- • To better adapt Foundation Tracker to downstream
ceedtofinetuneFoundationTrackeronspecificdownstream RGB+X tracking tasks efficiently, we propose CMT
RGB+X tracking tasks, referred as Prompt Tracker (Fig- Prompter and TTP Transformer layer, enhancing the“Toy in hand.” Language Encoder
RGB+T/D/E/M Add & Norm
RGB+N Tracking Tracking
Feed Forward Adapter
Linear Projection
Add & Norm
RGB+M Tracking Scaling
Template Frame Multi-headAttention
Linear Projection RGB+N ReLU
Tracking
Adapter Adapter
Linear Projection
Hidden States
Search Frame
RGB+T Tracking
(a) Unified Prompt Embedding (b) Cross Modality Tracking Prompters (c) Tracking Task Perception Transformer
Figure2. (a)UnifiedPromptEmbeddingstructure. (b)CrossModalityTracking(CMT)Prompters. (c)TrackingTaskPerception
(TTP)Transformerlayers.
model’s ability to incorporate additional modalities, 33, 118] are proposed to adjust ViT to downstream tasks.
termedasPromptTracker. ProTrack [108] and ViPT [123] attempt to introduce the
• OneTracker achieves state-of-the-art performance on 11 promptingconceptintotrakcingarea,whiletheyjustfocus
benchmarksfrom6trackingtasks. on RGB+D/T/E tracking. The question of how to transfer
large-scalepretrainingtrackertoothertrackingtasks, such
asRGB+MandRGB+Ntracking,remainsunanswered. In
2.RelatedWorks
this work, we propose Prompt Tracker based on Founda-
Large-scalePretrainingVisionModels. Large-scalepre- tionTracker. WeintroduceCMTPrompterandTTPTrans-
training models, or foundation models [7], have emerged formerlayertoperformtheparameter-efficientfinetuingon
as powerful models which are trained on broad data and RGB+Xtrackingtasks.
can be adapted to various downstream tasks. These mod- Visual Tracking. Visual object tracking is a funda-
els, initially popularized in Natural Language Processing mentaltask,includingRGBtrackingandRGB+Xtracking.
(NLP) by [23, 66, 77, 79], have extended their influence RGBtracking[29,70]utilizesrawRGBimagesforobject
to multiple domains. In the realm of computer vision, tracking.BecauseonlylevaragingpureRGBimageisprone
[113]isthefirsttoextendViT[25]to2billionparameters. to some complex scenarios, RGB+X tracking is proposed
[3,39,99]learnrepresentationsfromimagescorruptedby for robust tracking by incorporating multimodal informa-
masking. [30, 78, 82, 89, 98] explore the vision-language tion. RGB+T/E/D tracking [95, 104, 114, 115, 117, 124]
training strategy to align visual and text feature in a uni- take advantage of thermal or event flows or depth maps.
fiedspace.Theselarge-scalepretrainingvisionmodelshave RGB+N tracking [28, 63, 91] fuses language description
showntheirexceptionaltransferabilityacrossvariousdown- withRGBimages,andRGB+Mtracking[24,37,40–42,74,
stream tasks. Inspired by the success of large-scale pre- 97] provides the mask of target in the first frame. Despite
training strategies, we propose Foundation Tracker, which promisingperformanceoftask-specific[13,18,93,111]or
istrainedonacombinationofdiversetrackingdatasetsand multi-task [1, 68, 87, 92, 101, 102] trackers, these models
demonstratesstrongtemporalmatchingcapabilities. cannotsimulatethehumantemporalmatchingmechanism
Parameter-Efficient Transfer Learning. Parameter- well and lack the ability to handle multi-modal tracking
efficienttransferlearning(PETL)isintroducedtoserveasa tasks. In this work, we propose a general manner to unify
lightweightalternativetoaddressthislimitation, whichin- RGBandRGB+Xtrackingtasks.
volves freezing the pretrained language model and adding
a small number of extra trainable parameters to achieve 3.Methodology
quick adaptation to downstream tasks while maintaining
parameter efficiency [43, 44, 55, 62, 65, 75]. PETL also In this work, we propose OneTracker, consisting of Foun-
demonstrates its high efficiency in computer vision fields. dationTrackerandPromptTracker,toimplementaunified
VPT [47] inserts additional parameters to the input se- frameworkfortrackingtasks.TheoverallstructureofFoun-
quence before encoder. Diverse kinds of adapter [11, 14, dationTrackerandPromptTrackerisinFigure1(c)and(d).
Weight Share
raeniL
tpmorP
raeniL
raeniL
noisuF
raeniL
ssorC noitnettAWewillillustratetheunificationoftrackingtasks(Sec.3.1), as input to Foundation Tracker. The RGB frames I and
z
thestructureofFoundationTracker(Sec.3.2),thestructure I are divided into patches and flattened into 1D tokens
s
of Prompt Tracker (Sec. 3.3), and details of training and H Rz
GB
∈RNz×D andH Rs
GB
∈RNs×D,whereN
z
andN
s
finetuning(Sec.3.4). denotethetokennumberoftemplateandsearchframe,and
Disthedimensionoftokens. Thenthetokensareconcate-
3.1.TrackingUnification
nated into H0 = H0 = [Hz ,Hs ] and fed into
RGB RGB RGB
Thecoreaspectoftrackinginvolvesestimatingtheposition L-layertransformerencoderlayers. Theforwardprocessof
of moving objects in each video frame based on its initial thetransformerencoderlayerscanbewrittenas:
appearance.Dependingonthedifferentinputformat,track-
Hl =El(Hl−1),l={1,2,...,L}, (4)
ing tasks can be divided into two main categories: RGB
TrackingandRGB+XTracking(Figure1(a)and(b)). whereEl isthel-thtransformerencoderlayers,andHl−1
RGB Tracking. RGB tracking is a extensively studied is the input to El. The structure of transformer encoder
trackingtask, focusingontrackingobjectsusingRGBim-
layeristhesameasthevanillatransformerlayer[85]. We
ageinformation.Givenavideosequencewiththebounding
extract features and build the temporal matching between
boxoftargetobjectinthefirstframe, theformulaofRGB
template and search frame. Finally, a box head is lever-
trackingislike,
aged to convert the temporal correlation from transformer
encoder into localization coordinates. Moreover, an extra
B =FT(I,B ;θ), (1)
0 segmentationheadisleveragedtogeneratethemaskpredic-
tion,whosestructureisthesameas[110].WetrainFounda-
where I, B , B denote the RGB frames of a video, the
0
tionTrackeronseveralRGBtrackingbenchmarks, includ-
initialboxprediction,theboxpredictionsinthesubsequent
ingLaSOT[28],TrackingNet[69],andGOT-10K[45].The
frames. FTistheFoundationTrackerwithparameterθ.
segmentation head is optimized in a box-supervised man-
RGB+XTracking. Weintroduceanunifiedformatthat
ner [84] because of the absence of mask ground truth in
encompassesRGB+XtrackingandRGBtracking. Thefor-
trackingdatasets. SimilartothefoundationmodelinNLP,
mulaforRGB+Xtrackingcanbeexpressedas:
afterthelarge-scalepretraining,ourFoundationTrackerob-
′ tains the strong ability of temporal matching and transfer-
B =PT(I,B ,X;θ ), (2)
0
abilitytodownstreamRGB+Xtrackingtasks.
where X is the additional information input of specific
3.3.PromptTracker
RGB+X tracking task and PT is the Prompt Tracker with
parameterθ′. X variesdependingontheRGB+Xtracking Different from previous RGB+X works, which add an ad-
task. For RGB+N tracking, X is the language descrip- ditional module to fuse mutlimodal features, we regard
N
tion. For RGB+M tracking, X is the mask of the target the multimodal information as a kind of prompt and pro-
M
inthefirstframeandB isnotprovided. ForRGB+D/T/E videFoundationTrackerwithcomplementarityinaprompt-
0
tracking, X , X , and X correspond to the depth, ther- tuningmanner,termedasPromptTracker(Figure1(d)). To
D T E
mal,andeventmapofeachframe. Tofurtherillustrateour enableefficientadaptationtodownstreamtasks,wepropose
framework,wecanrewritetheEquation2asfollows: the Cross Modality Tracking Prompters (CMT Prompters)
andtheTrackingTaskPerceptionTransformer(TTPTrans-
B =PT(I,B 0,X;TTP(θ)) former)layers.
(3)
=FT(CMT(I,B ,X);TTP(θ)), Unified Prompt Embedding. With the general defini-
0
tion of RGB+X tracking, the Prompt Tracker leverages a
whereTTPdenotesthereplacementofthevanillaTrans- unified prompt embedding module (Figure 2 (a)) to trans-
former layers with our Tracking Task Perception (TTP) form different modality downstream information into to-
TransformerlayersandCMTistheCrossModalityTrack- kens P0 = P . The choice of prompt embedding strat-
X
ingPrompters(CMTPrompters). Throughourspecificde- egy depends on the specific downstream task’s modality.
sign, we succeed in unifying RGB tracking and RGB+X TodealwithlanguagedescriptioninRGB+Ntracking, we
trackinginageneralformat. adopt BERT [23] as a language encoder to extract the lin-
guistic feature P ∈ RL×D with a sequence length of L.
3.2.FoundationTracker N
For RGB+M tracking, a patch embed layer is utilized to
The structure of Foundation Tracker (Figure 1 (c)) is sim- project the mask of the target object into patches and flat-
ilar to ViT [25] with several transformer encoder layers, tentheminto1DtokensP
M
∈ RNz×D. ThesizeofP
M
is
which are responsible for processing the input frames and thesameasHz . ForRGB+Ttracking,thecorrespond-
RGB
capturing their spatial and temporal dependencies. To be- ingmultimodalmapsofthetemplateandsearchframeare
gin, the template frame I and search frame I are taken fedintoapatchembedlayerandthenflattenedinto1Dto-
z sRGBTracking
TransT STARK MixFormer OSTrack AiATrack SimTrack GRM UniTrack UTT Unicorn OmniTracker UNINEXT One
[12] [100] [17] [112] [34] [9] [35] [92] [68] [101] [87] [102] Tracker
AUC(↑) 64.9 66.4 69.2 69.1 69.0 69.3 69.9 35.1 64.6 65.3 69.1 69.2 70.5
LaSOT
[28]
PNorm(↑) 73.8 76.3 78.7 78.7 79.4 78.5 79.3 - - 73.1 77.3 77.1 79.9
P(↑) 69.0 71.2 74.7 75.2 73.8 74.0 75.8 32.6 67.2 68.7 75.4 75.5 76.5
AUC(↑) 81.4 81.3 83.1 83.1 82.7 82.3 84.0 - 79.7 79.0 83.4 83.2 83.7
Track [6in 9g ]Net PNorm(↑) 86.7 86.1 88.1 87.8 87.8 86.5 88.7 - - 82.0 86.7 86.9 88.4
P(↑) 80.3 78.1 81.6 82.0 80.4 - 83.3 - 77.0 77.4 82.3 83.3 82.7
RGB+NTracking
TNLS-III RTTNLD SiamRPN VITAL MDNet ATOM DiMP PrDIMP SiamRPN++ TNL2K-2 SNLT JointNLT One
[63] [31] [56] [81] [71] [21] [5] [22] [57] [91] [32] [120] Tracker
OTB99 AUC(↑) 55.0 61.0 61.2 65.2 64.6 67.6 67.3 68.3 65.8 68.0 66.6 65.3 69.7
[63] P(↑) 72.0 79.0 75.8 84.2 82.8 82.4 81.9 83.0 79.7 88.0 80.4 85.6 91.5
TNL2K AUC(↑) - 25.0 - - - - - - - 42.0 27.6 56.9 58.0
[91] P(↑) - 27.0 - - - - - - - 42.0 41.9 58.1 59.1
RGB+DTracking
ATOM LTDSEd DRefine keeptrack LTMUB DiMP DDiMP DeT OSTrack SPT ProTrack ViPT One
[20] [50] [52] [53] [51] [6] [51] [103] [111] [125] [107] [123] Tracker
F-score(↑) - 40.5 - - 46.0 - 48.5 53.2 52.9 53.8 57.8 59.4 60.9
DepthTrack
R(↑) - 38.2 - - 41.7 - 46.9 50.6 52.2 54.9 57.3 59.6 60.4
[104]
P(↑) - 43.0 - - 51.2 - 50.3 56.0 53.6 52.7 58.3 59.2 60.7
VOT EAO(↑) 50.5 - 59.2 60.6 - 54.3 - 65.7 67.6 65.1 65.1 72.1 72.7
RGBD2022 Accuracy(↑) 69.8 - 77.5 75.3 - 70.3 - 76.0 80.3 79.8 80.1 81.5 81.9
[53] Robustness(↑) 68.8 - 76.0 79.7 - 73.1 - 84.5 83.3 85.1 80.2 87.1 87.2
RGB+TTracking
SGT++ DAPNet HMFT FANet mfDiMP STARKS50 CAT APFNet OSTrack TransT ProTrack ViPT One
[58] [126] [117] [127] [116] [100] [60] [94] [112] [12] [107] [123] Tracker
LasHeR PR(↑) 36.5 43.1 43.6 44.1 44.7 44.9 45.0 50.0 51.5 52.4 53.8 65.1 67.2
[61] SR(↑) 25.1 30.9 31.3 31.4 31.4 34.3 36.1 36.2 39.4 41.2 42.0 52.5 53.8
RGBT234 MPR(↑) 64.6 72.0 79.6 72.9 78.7 79.0 80.4 79.0 82.3 82.7 79.5 83.5 85.7
[59] MSR(↑) 42.8 47.2 54.4 54.9 55.3 55.4 56.1 57.3 57.5 57.9 59.9 61.7 64.2
RGB+ETracking
MetaTracker ATOM STARKS50 ProTrack PrDIMP50 VITAL TransT LTMU SiamRCNN MDNet OSTrack ViPT One
[73] [20] [100] [107] [22] [81] [12] [19] [86] [71] [112] [123] Tracker
VisEvent MPR(↑) 49.1 60.8 61.2 63.2 64.4 64.9 65.0 65.5 65.9 66.1 69.5 75.8 76.7
[90] MSR(↑) 29.8 41.2 44.6 47.1 45.3 - 47.4 45.9 49.9 - 53.4 59.2 60.8
RGB+MTracking
STM CFBI AOT STCN XMem SiamMask SiamR-CNN UniTrack Unicorn TarVIS OmniTracker UNINEXT One
[72] [109] [110] [16] [15] [88] [86] [92] [101] [1] [87] [102] Tracker
J&F(↑) 89.3 89.4 91.1 91.6 92.0 69.8 - - 87.4 - 88.5 - 88.9
DAVIS16
J(↑) 88.7 88.3 90.1 90.8 90.7 71.7 - - 86.5 - 87.3 - 88.1
[74]
F(↑) 89.9 90.5 92.1 92.5 93.2 67.8 - - 88.2 - 89.7 - 89.7
J&F(↑) 81.8 81.9 84.9 85.4 86.2 56.4 70.6 - 69.2 82.0 71.0 81.8 82.5
DAVIS17 J(↑) 79.2 79.1 82.3 82.2 82.9 54.3 66.1 58.4 65.2 78.7 66.8 77.7 79.4
[76]
F(↑) 84.3 84.6 87.5 88.6 89.5 58.5 75.0 - 73.2 87.0 75.2 85.8 85.6
Table1.OverallperformanceonRGBtrackingandRGB+Xtracking.
kens P
T
∈ RNz×D and P
T
∈ RNs×D. The prompt em- CMTPromptersaredesignedtoextractthesemanticrepre-
beddingofRGB+DandRGB+Etrackingfollowsthesame sentationsofmultimodalinformationandprovideFounda-
procedureasRGB+Ttracking. Throughtheunifiedprompt tionTrackerwithcomplementarity. AsdepictedinFigure2
embeddingmodule,weeffectivelymapthemultimodalin- (b),CMTPromptersconsistofmultiplelinearlayersanda
formationintoacohesivetokenrepresentation. promptfusionmodule,whichcanbewrittenas:
Cross Modality Tracking Prompters. After unified
Pl+1 =CMTl(Hl,Pl),l={0,1,...,L−1}, (5)
promptembedding,weproposetheCrossModalityTrack-
ingPrompters(CMTPrompters)tofusetheextrainforma- whereCMTl−1 denotesthel-thCMTPrompter,andPl is
tion. Althoughafewworkshaveattemptedtoinsertsome theoutputofCMTl.ThepromptPlisaddedtotheoriginal
trainable parameters into pretrained models to bridge the matchingresultsHl inaformofresiduals:
upstreamanddownstreamtasks,howtointegratethecross-
modal information for tracking tasks is more challenging. Hl =Hl+Pl+1,l={0,1,...,L−1}. (6)CMT Prompters take the matching results Hl from l- where W ∈ Rd×r and W ∈ Rr×d is two mapping
down up
th transformer encoder layer and the prompt Pl as input. matrixes, ReLUisreluoperation, sistheconstantscaling
Firstly,theHl andPl aremappedtolower-dimensionalla- factor, and rank r ≪ min(d,k). During finetuning, we
tentspaceusingalinearlayer,respectively. Subsequently,a freeze the W and only update W and W . Through
down up
promptfusionmoduleisemployedtointegratethemodal- optimizingW andW ,weachievethehighlyefficient
down up
ities. For RGB+N tracking, cross-attention is utilized to learning of ∆θ. The TTP Transformer layers bridge the
merge the linguistic feature and temporal correlation. For RGBtrackingandRGB+Xtrackingwhilemaintainingthe
otherRGB+Xtracking,PlandHlareaddedandmergedby temporalmatchingknowledgeinFoundationTracker.
alinearlayer.Finally,anotherlinearlayerprojectsthefused
3.4.TrainingandInference
feature to the original dimension. The structure and for-
mat of the CMT Prompter remain consistent across differ- Training. The whole training process of OneTracker con-
entRGB+Xtrackingtasks. ByleveragingCMTPrompter, sists of two stages: Foundation Tracker pretraining and
we achieve the integration between RGB images and mul- PromptTrackerfinetuning. Inthefirstpretrainingstage,we
timodal information with high efficiency through prompt- pretrainourFoundationTrackeronacombinationofseveral
tuningtechniques. large-scale RGB tracking datasets, including LaSOT [28],
Tracking Task Perception Transformer. Although TrackingNet[69],andGOT-10K[45],whichisthesameas
CMT Prompter effectively complements auxiliary modali- previous trackers [18, 35, 112]. Following [18, 35, 112],
tiesasprompts,thePromptTrackerlacksspecializationfor we adopt the weighted focal loss [54] for classification, l
1
certaindownstreamtasks.Forexample,FoundationTracker loss and generalized IoU loss [80] for bounding box re-
excelsatlocalizingtargetsbasedonRGBimages,whilethe gression. Becausethereisnomaskannotationsintracking
lack of perception of linguistic features may result in sub- datasets,weleverageBoxInst[84]tosupervisethesegmen-
optimal performance in RGB+N tracking tasks. Given the tation head of Foundation Tracker. The total loss function
Foundation Tracker parametrazed by θ, θ may not be the canbeformulatedas:
optimal weights for downstream tasks. Suppose the best
weights on downstream tasks are θ′, the purpose of full L stage1 =L cls+λ iouL iou+λ L1L 1+λ LmaskLb mo ax sin kst (9)
finetuning is to learn difference ∆θ between θ and θ′ and
Inthesecondstage,wefinetuneourFoundationTracker
update Foundation Tracker to θ +∆θ. The drawbacks of
on RGB+X downstream tracking datasets. We freeze the
full finetuning are that we must learn a different set of pa-
parameters of Foundation Tracker and only train the CMT
rameters∆θ,whosedimension|∆θ|isequalto|θ|foreach
Prompters and adapters in TTP Transformer layers. For
different downstream task, and lack of large-scale data in
RGB+N/D/T/Etracking,thelossfunctionL isequal
specificdownstreamtasksmayleadtosuboptimalfinetun- stage2
toL . ForRGB+Mtracking,duetotheavailablemask
ingperformanceandresultincatastrophicforgetting. stage1
annotations, wedroptheBoxInstauxiliarylossandutilize
Thus, to bridge the gap between RGB tracking and
themaskannotationstooptimizethesegmentationhead.
downstream RGB+X tracking tasks, we propose Tracking
Inference. Becauseoftheslightdifferenceintheinput
TaskPerceptionTransformer(TTPTransformer)byadding
format of several tracking tasks, we adopt different infer-
someadapterstoFoundationTracker.Wheretoinserttrain-
ence manners. For RGB tracking and RGB+N tracking,
able parameters is a crucial question. The linear layer
Hanning window penalty is utilized to leverage the posi-
in transformer encoder contains the knowledge of specific
tional prior following previous works [18, 35, 112]. For
tasks, especiallythelinearlayerinFeedForwardNetwork
RGB+D/T/Etracking,themultimodalmapisalsocropped
(FFN) [38]. As shown in Figure 2 (c), we insert trainable
by using Hanning window penalty. For RGB+M track-
adapters with a small number of parameters to the linear
ing,thefirstframewiththemaskannotationandtheprevi-
projection operation in vanilla transformer encoder layers,
ousframewiththepredictedmaskarefedintothePrompt
i.e. thequery/key/valueprojectionmatrixesandtheoutput
Tracker to perform online target matching without crop-
layersinFFNtoenabletheefficientadaption.Thestructure
ping. DuetothespecificdesignofOneTracker,wecanap-
oftheadapterfollowsasimilarapproachwith [38]. Fora
pretrainedlinearlayerwithweightmatrixW ∈ Rd×k, the plythemtoseveraltrackingtaskswithoutanymodification
tothestructureofmodels.
formulacanbewrittenas:
h=Wx, (7) 4.Experiments
whereh∈Rd×tandx∈Rk×tdenotetheoutputandinput. 4.1.ImplementationDetails
kanddarethedimensionofhandx. tisthetokennumber
OneTracker is built on the encoder of ViT-B [26], which
ofx. Withanadapter,theprocessbecomes:
includes 12 sequential transformer layers. The box head
h=Wx+∆Wx=Wx+s·W ReLU(W x), (8) andsegmentationheadfollowthestructurein[18,35,111]
up downLaSOT DepthTrack LasHeR VisEvent OTB DAVIS17
Method #Params
AUC Pnorm P F R P PR SR PR SR AUC P J&F J F
FoundationTracker - 70.5 79.9 76.5 55.9 55.6 55.7 53.3 42.1 70.1 53.6 67.3 88.9 42.7 37.4 48.1
FullFinetune 99.83M - - - 57.2 56.9 57.1 65.4 52.5 75.6 59.8 68.5 89.6 77.8 75.4 80.2
PromptTracker 2.8M - - - 60.9 60.4 60.7 67.2 53.8 76.7 60.8 69.7 91.5 82.5 79.4 85.6
w/oCMTPrompters 2.55M - - - 56.5 55.4 56.7 60.7 47.1 74.0 54.5 68.7 89.9 80.4 78.6 82.2
w/oTTPTransFormer 0.25M - - - 59.2 58.8 59.1 65.6 52.3 75.3 59.0 69.3 90.8 81.7 79.3 84.0
Table2.AblationstudyonthePromptTracker.#Paramsdenotesthenumberoftrainableparameters.
DepthTrack LasHeR VisEvent OTB DAVIS17
Number #Params
F R P PR SR PR SR AUC P J&F J F
0 - 56.5 58.8 59.1 60.7 47.1 74.0 54.5 68.7 89.9 80.4 78.6 82.2
1 0.02M 57.6 57.2 57.3 61.5 48.7 75.7 59.2 68.9 90.0 82.5 79.4 85.6
2 0.04M 58.4 58.1 58.2 63.2 50.1 76.0 59.5 69.2 90.4 75.4 60.7 77.5
4 0.08M 59.1 58.9 59.3 64.1 51.0 76.1 59.7 69.3 90.4 67.1 73.3 73.5
6 0.12M 59.5 59.3 59.4 65.7 52.5 76.4 60.3 69.5 91.0 58.7 52.5 64.9
12 0.25M 60.9 60.4 60.7 67.2 53.8 76.7 60.8 69.7 91.5 48.3 44.5 52.1
Table3. AblationstudyonthenumberofCMTPrompters. #Paramsdenotesthenumberoftrainableparameters. Inthisexperiment,
wejustcountthenumberofparametersinCMTPrompters.
and [110], respectively. For RGB+N tracking tasks, we on LaSOT. GRM is one of the strongest tracking-specific
adoptBERT[23]astextencoder. Duringthefirstpretrain- model,andFoundationTrackeralsooutperformsit0.6AUC
ing stage, Foundation Tracker is optimized with AdamW onLaSOT.Moreover,ourFoundationTrackercangenerate
optimizer [67] for 300 epochs. The initial learning rate the mask of the target object due to an extra segmentation
is 4 × 10−5 for the ViT backbone and 4 × 10−4 for the head,whichisnotpossiblewithothertrackers.
heads. It decays by a factor of 10 after 240 epochs. For RGB+NTracking. Followingpreviousworks[120],we
thefinetuningofPromptTracker,wefreezetheparameters conduct experiments on OTB99 and TNL2K. Our Prompt
of Foundation Tracker and only adapt the CMT Prompter Tracker superpasses all existing RGB+N trackers at least
and TTP Transformer layers. We finetune Prompt Tracker 1.7 AUC and 2.5 precision on OTB99, although Prompt
for 60 epochs on the corresponding training data of each TrackerisnotspecificallydesignedforRGB+Ntracking.
downstreamtaskswithaninitiallearningrateof4×10−5
RGB+D/T/E Tracking. Following [123], we eva-
byusingAdamWoptimizer,andlearningratedecreasedby
lute our Prompt Tracker on DepthTrack [103] and VOT-
10after48epochs. Wesetλ as2,λ as5,λ as1,
iou L1 Lmask RGBD2022 [53] for RGB+D tracking, LasHeR [61] and
andras16. Moredetailsareinsupplementarymaterials.
RGBT234[59]forRGB+Ttracking,andVisEvent[90]for
RGB+E tracking. Our Prompt Trakcer greatly exceeds all
4.2.BenchmarkResultson6TrackingTasks
othertrackersintermsofperformance. Despitethefactthat
WeevaluateourOneTrackeron6trackingtasks,including ViPT [123] also adopts a similar prompt-tuning structure,
RGB tracking and RGB+X Tracking. We compare the re- our superior results demonstrate the effectiveness of CMT
sultswithtask-specificcounterpartsinTable1. PromptersandTTPTransformerlayers.
RGB Tracking. To show the strong temporal match- RGB+M Tracking. We choose DAVIS16 [74] and
ingabilityofourOneTracker,wecompareourFoundation DAVIS17[76]forRGB+Mtracking. DAVIS16isasingle-
Tracker on widely-used RGB tracking benchmarks: La- object benchmark with 20 evaluation splits and DAVIS17
SOT [28] and TrackingNet [69]. Area under the success isthemulti-objectexpansionofDAVIS16. Regionsimilar-
curve(AUC),normalizedprecision(P ),andprecision ityJ,contouraccuracyF,andtheiraveragedscoreJ&F
Norm
(P)areadoptedasmetrics. Ourmodelachieves70.5AUC are adopted as metrics. Our Prompt Tracker achieves the
and 69.7 AUC on LaSOT and TrackingNet, respectively, bestperformanceonitsmulti-taskcounterparts[1,102]and
outperformingallothertrackers. UniTrack,UTT,Unicorn, other unified tracking models [86–88, 92, 101] by a large
and OmniTracker are designed for multiple RGB tracking margin on both DAVIS16 and DAVIS17, despite its less
tasks, and UNINEXT is good at instance perception. Our trainingdataandtrainingcost. Therestillexistsasmallgap
FoundationTrackersurpassesthesemodelsatleast1.3AUC between Promp Tracker and specific models with memoryDepthTrack LasHeR VisEvent OTB DAVIS17
Task CMT TTP
F R P PR SR PR SR AUC P J&F J F
53.9 53.2 53.4 59.7 48.5 73.5 56.8 68.4 89.6 58.7 51.1 66.3
✓ 55.4 54.4 54.9 62.8 50.2 74.3 57.7 68.9 90.3 65.3 60.2 70.4
✓ 57.9 57.0 57.2 64.8 52.0 74.8 58.4 69.2 90.5 70.2 66.8 73.6
✓ ✓ 58.6 58.1 57.9 66.8 53.2 74.9 58.8 69.6 91.1 76.4 73.5 79.3
✓ ✓ ✓ 60.9 60.4 60.7 67.2 53.8 76.7 60.8 69.7 91.5 82.5 79.4 85.6
Table4.Ablationstudyonthetrainingstrategy.
Search Foundation Attn Template Search Foundation Attn Prompt Attn
Template
RGB Tracking RGB+D Tracking
Template Search Foundation Attn Prompt Attn Template Search Foundation Attn Prompt Attn
“white car
on the left.”
RGB+N Tracking RGB+T Tracking
TemplateFoundation Search Prompt Search Foundation Attn Prompt Attn Template Search Foundation Attn Prompt Attn
RGB+M Tracking RGB+E Tracking
Figure3. Visualizationresults. Theblue,red,andgreenboundingboxesdenotegroundtruth,FoundationTracker,andPromptTracker.
FoundationAttnandPromptAttndenotestheattentionmapofFoundationTrackerandPromptTracker.
mechanisms[15,16,110],butourPromptTrackeronlyre- of our Foundation Tracker and Prompt Tracker. Further-
lies on the first and previous frame, enabling it to handle more,weperformanablationstudyontheCMTPrompters
videoswithanylength. andTTPTransformerlayers. Theresultsinthefourthand
fifth rows illustrate the impact of these components in en-
4.3.AblationStudy. hancingthetrackingperformance.
Foundation Tracker and Prompt Tracker. To verify the CMTPrompterLayers. Weexploretheimpactonthe
strongtemporalmatchingabilityofFoundationTrackerand performanceofdifferentnumbersofCMTPrompterlayers
theeffectivenessofparameter-efficientfinetuning,weeval- inTable3. WeinsertCMTPromptersatdifferentpositions,
uatetheperformanceonRGBandRGB+XtrackinginTa- each 1, 2, 3, 6, 12 transformer blocks. A value of 0 for
ble 2. In the first row, we solely feed the RGB image into CMT Prompter layer denotes that we add the embedding
Foundation Tracker, and the results demonstrate its strong of multimodal information to RGB image embeddings di-
abilitytotrackbasedonvisualinformationalone. Then,we rectly. As the number of CMT Prompter layers increases,
conductfullfinetuningoftheFoundationTrackerondown- the performance of Prompt Tracker improves, suggesting
stream RGB+X tracking datasets, as well as parameter- the effectiveness of our CMT Prompters. However, inter-
efficientfinetuningusingourproposedCMTPromptersand estingly, the performance on RGB+M tracking shows the
TTP Transformer layers. The integration of multi-modal oppositetrend,withasignificantdropinperformanceasthe
information boosts the localization accuracy, while our number of CMT Prompter layers increases. This observa-
PromptTrackerachievesbetterperformancewhileonlyad- tiondemonstratesthatthemaskembeddingiseffectiveonly
justing2.8Mparameters,whichhighlightstheeffectiveness in capturing superficial features and does not provide sub-stantialbenefitswhenincorporateddeeplyintothemodel. References
Training Strategy. We analyze the training strategy of
[1] Ali Athar, Alexander Hermans, Jonathon Luiten, Deva
our Prompt Tracker on RGB+X tracking tasks in Table 4. Ramanan, and Bastian Leibe. Tarvis: A unified ap-
We investigate different approaches to jointly training the proachfortarget-basedvideosegmentation. arXivpreprint
PromptTrackeronmultipleRGB+Xdatasets.Fromthefirst arXiv:2301.02657,2023. 2,3,5,7,1
to the fourth row, we jointly train Prompt Tracker on the [2] ArminBahl,GeorgAmmer,TabeaSchilling,andAlexan-
combination of several RGB+X datasets. In the first row, der Borst. Object tracking in motion-blind flies. Nature
wetrainthePromptTrackerbyonlyseparatingtheembed- neuroscience,16(6):730–738,2013. 1
dinglayersfordifferentmodalities.. Inthesecondandthird [3] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit:
Bert pre-training of image transformers. arXiv preprint
rows,weseparatetheCMTPromptersandtheTTPTrans-
arXiv:2106.08254,2021. 3
former layers for different modalities, respectively. In the
[4] LucaBertinetto,JackValmadre,JoaoFHenriques,Andrea
fourthrow,weseparateboththeCMTPromptersandTTP
Vedaldi, andPhilipHSTorr. Fully-convolutionalsiamese
Transformer layers. By progressively separating the train-
networks for object tracking. In Computer Vision–ECCV
ingofeachmodule,weobservecontinuousimprovementin
2016 Workshops: Amsterdam, The Netherlands, October
performance.Thisphenomenoncanbeattributedtothelim-
8-10and15-16,2016,Proceedings,PartII14,pages850–
itedamountoftrainingdataavailableforeachdownstream 865.Springer,2016. 1
task,makingithardtotrainthePromptTrackerjointlyfor [5] GoutamBhat,MartinDanelljan,LucVanGool,andRadu
allRGB+Xtrackingtasks. Inthefifthrow,PromptTracker Timofte. Learning discriminative model prediction for
is trained on corresponding data for specific tasks, achiev- tracking. In Proceedings of the IEEE/CVF international
ingbetterperformance. conferenceoncomputervision,pages6182–6191,2019. 5
[6] GoutamBhat,MartinDanelljan,LucVanGool,andRadu
Temporal Matching Attention Visualization. We vi-
Timofte. Learning discriminative model prediction for
sualize the temporal matching attention map of Founda-
tracking. InICCV,2019. 5
tion Tracker and Prompt Tracker in Figure 3. The Foun-
[7] RishiBommasani,DrewAHudson,EhsanAdeli,RussAlt-
dationTracker,afterundergoingextensivelarge-scaletrain-
man,SimranArora,SydneyvonArx,MichaelSBernstein,
ing, demonstrates its strong temporal matching ability. It JeannetteBohg, AntoineBosselut, EmmaBrunskill, etal.
effectively captures the temporal dependencies and pro- Ontheopportunitiesandrisksoffoundationmodels. arXiv
vides accurate predictions for the target object. With preprintarXiv:2108.07258,2021. 3
the parameter-efficient finetuning on downstream RGB+X [8] Gyo¨rgyBuzsa´kiandRodolfoLlina´s. Spaceandtimeinthe
tracking datasets, the Prompt Tracker further improves the brain. Science,358(6362):482–485,2017. 1
tracking performance. By leveraging multimodal informa- [9] Boyu Chen, Peixia Li, Lei Bai, Lei Qiao, Qiuhong Shen,
tionandrefiningpredictions,PromptTrackerachievesmore Bo Li, Weihao Gan, Wei Wu, and Wanli Ouyang. Back-
bone is all your need: a simplified architecture for visual
preciseandaccurateresultsoncertaindatasets. Thesevisu-
objecttracking.InComputerVision–ECCV2022:17thEu-
alizationsdemonstratetheeffectivenessofourOneTracker
ropeanConference,TelAviv,Israel,October23–27,2022,
inestablishingtemporalcorrespondences.
Proceedings,PartXXII,pages375–392.Springer,2022. 5
[10] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong
Xiao. Deepdriving: Learningaffordancefordirectpercep-
5.Conclusion
tion in autonomous driving. In Proceedings of the IEEE
internationalconferenceoncomputervision,pages2722–
Weproposeageneralframework,OneTracker,tounifysev-
2730,2015. 1
eralRGBtrackingandRGB+Xtrackingtasks. OneTracker
[11] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
involves pretraining a Foundation Tracker on RGB track-
Yibing Song, Jue Wang, and Ping Luo. Adaptformer:
ing datasets and adapting it to downstream RGB+X track- Adapting vision transformers for scalable visual recogni-
ing tasks using prompt-tuning techniques. By leveraging tion. arXivpreprintarXiv:2205.13535,2022. 3
thestrengthsofpretrainingandparameter-efficientfinetun- [12] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun
ing mechanisms, our framework achieves state-of-the-art Yang,andHuchuanLu. Transformertracking. InProceed-
resultsinvarioustrackingscenarios. Superiorperformance ingsoftheIEEE/CVFconferenceoncomputervisionand
on11benchmarksof6tasksdemonstratestheeffectiveness patternrecognition,pages8126–8135,2021. 5
andpowerfulgenerationabilityofOneTracker. [13] ZeduChen,BinengZhong,GuorongLi,ShengpingZhang,
andRongrongJi. Siameseboxadaptivenetworkforvisual
Acknowledgments: This work was supported
tracking. InProceedingsoftheIEEE/CVFconferenceon
by National Natural Science Foundation of China computervisionandpatternrecognition,pages6668–6677,
(No.62072112), Scientific and Technological Innova- 2020. 1,3
tion Action Plan of Shanghai Science and Technology [14] ZheChen,YuchenDuan,WenhaiWang,JunjunHe,Tong
Committee(No.22511102202). Lu,JifengDai,andYuQiao.Visiontransformeradapterfordensepredictions. arXivpreprintarXiv:2205.08534,2022. SylvainGelly,JakobUszkoreit,andNeilHoulsby. Anim-
3 ageisworth16x16words: Transformersforimagerecog-
[15] HoKeiChengandAlexanderGSchwing. Xmem: Long- nitionatscale. ICLR,2021. 6
term video object segmentation with an atkinson-shiffrin [27] John Duncan. Selective attention and the organization of
memorymodel.InComputerVision–ECCV2022:17thEu- visual information. Journal of experimental psychology:
ropeanConference,TelAviv,Israel,October23–27,2022, General,113(4):501,1984. 1
Proceedings,PartXXVIII,pages640–658.Springer,2022.
[28] HengFan,LitingLin,FanYang,PengChu,GeDeng,Sijia
5,8
Yu,HexinBai,YongXu,ChunyuanLiao,andHaibinLing.
[16] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Re- Lasot:Ahigh-qualitybenchmarkforlarge-scalesingleob-
thinkingspace-timenetworkswithimprovedmemorycov- jecttracking. InProceedingsoftheIEEE/CVFconference
erageforefficientvideoobjectsegmentation. Advancesin on computer vision and pattern recognition, pages 5374–
NeuralInformationProcessingSystems,34:11781–11794, 5383,2019. 1,2,3,4,5,6,7
2021. 5,8
[29] HengFan,LitingLin,FanYang,PengChu,GeDeng,Si-
[17] YutaoCui,ChengJiang,LiminWang,andGangshanWu. jia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin
Mixformer:End-to-endtrackingwithiterativemixedatten- Ling. LaSOT: A high-quality benchmark for large-scale
tion.InProceedingsoftheIEEE/CVFConferenceonCom- singleobjecttracking. InCVPR,2019. 3
puterVisionandPatternRecognition,pages13608–13618,
[30] Rongyao Fang, Shilin Yan, Zhaoyang Huang, Jingqiu
2022. 5
Zhou, Hao Tian, Jifeng Dai, and Hongsheng Li.
[18] YutaoCui,ChengJiang,LiminWang,andGangshanWu.
Instructseq: Unifying vision tasks with instruction-
Mixformer:End-to-endtrackingwithiterativemixedatten-
conditioned multi-modal sequence generation. arXiv
tion.InProceedingsoftheIEEE/CVFConferenceonCom-
preprintarXiv:2311.18835,2023. 3
puterVisionandPatternRecognition,pages13608–13618,
[31] Qi Feng, Vitaly Ablavsky, Qinxun Bai, Guorong Li, and
2022. 3,6,1
StanSclaroff. Real-timevisualobjecttrackingwithnatu-
[19] Kenan Dai, Yunhua Zhang, Dong Wang, Jianhua Li,
rallanguagedescription. InProceedingsoftheIEEE/CVF
HuchuanLu,andXiaoyunYang. High-performancelong-
Winter Conference on Applications of Computer Vision,
term tracking with meta-updater. In Proceedings of the
pages700–709,2020. 5
IEEE/CVF conference on computer vision and pattern
[32] QiFeng, VitalyAblavsky, QinxunBai, andStanSclaroff.
recognition,pages6298–6307,2020. 5
Siamesenaturallanguagetracker: Trackingbynaturallan-
[20] MartinDanelljan,GoutamBhat,FahadShahbazKhan,and
guagedescriptionswithsiamesetrackers.InProceedingsof
Michael Felsberg. ATOM: Accurate tracking by overlap
theIEEE/CVFConferenceonComputerVisionandPattern
maximization. InCVPR,2019. 5
Recognition,pages5851–5860,2021. 5
[21] MartinDanelljan,GoutamBhat,FahadShahbazKhan,and
[33] PengGao, ShijieGeng, RenruiZhang, TeliMa, Rongyao
Michael Felsberg. Atom: Accurate tracking by overlap
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
maximization.InProceedingsoftheIEEE/CVFconference
Clip-adapter: Better vision-language models with feature
on computer vision and pattern recognition, pages 4660–
adapters. arXivpreprintarXiv:2110.04544,2021. 3
4669,2019. 5
[34] Shenyuan Gao, Chunluan Zhou, Chao Ma, Xinggang
[22] MartinDanelljan,LucVanGool,andRaduTimofte. Prob-
Wang,andJunsongYuan. Aiatrack: Attentioninattention
abilistic regression for visual tracking. In Proceedings of
fortransformervisualtracking.InComputerVision–ECCV
theIEEE/CVFconferenceoncomputervisionandpattern
2022: 17th European Conference, Tel Aviv, Israel, Octo-
recognition,pages7183–7192,2020. 5
ber23–27,2022,Proceedings,PartXXII,pages146–164.
[23] JacobDevlin,Ming-WeiChang,KentonLee,andKristina
Springer,2022. 5
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint [35] Shenyuan Gao, Chunluan Zhou, and Jun Zhang. Gener-
arXiv:1810.04805,2018. 3,4,7 alized relation modeling for transformer tracking. arXiv
preprintarXiv:2303.16580,2023. 2,5,6,1
[24] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang,
Philip HS Torr, and Song Bai. Mose: A new dataset [36] AndreasGeiger,PhilipLenz,andRaquelUrtasun. Arewe
for video object segmentation in complex scenes. arXiv readyforautonomousdriving? thekittivisionbenchmark
preprintarXiv:2302.01872,2023. 1,3 suite. In 2012 IEEE conference on computer vision and
patternrecognition,pages3354–3361.IEEE,2012. 1
[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [37] Pinxue Guo, Wei Zhang, Xiaoqiang Li, and Wenqiang
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Zhang. Adaptive online mutual learning bi-decoders for
Sylvain Gelly, et al. An image is worth 16x16 words: video object segmentation. IEEE Transactions on Image
Transformersforimagerecognitionatscale.arXivpreprint Processing,31:7063–7077,2022. 3
arXiv:2010.11929,2020. 3,4 [38] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Kirkpatrick,andGrahamNeubig. Towardsaunifiedview
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, of parameter-efficient transfer learning. arXiv preprint
Mostafa Dehghani, Matthias Minderer, Georg Heigold, arXiv:2110.04366,2021. 6[39] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr [52] Matej Kristan, Jiˇr´ı Matas, Alesˇ Leonardis, Michael
Dolla´r,andRossGirshick. Maskedautoencodersarescal- Felsberg, Roman Pflugfelder, Joni-Kristian Ka¨ma¨ra¨inen,
ablevisionlearners.InProceedingsoftheIEEE/CVFCon- HyungJinChang, MartinDanelljan, LukaCehovin, Alan
ferenceonComputerVisionandPatternRecognition,pages Lukezˇicˇ, et al. The ninth visual object tracking vot2021
16000–16009,2022. 3 challengeresults. InICCVW,pages2711–2738,2021. 5
[40] LingyiHong,WeiZhang,LiangyuChen,WenqiangZhang, [53] Matej Kristan, Alesˇ Leonardis, Jiˇr´ı Matas, Michael
andJianpingFan. Adaptiveselectionofreferenceframes Felsberg, Roman Pflugfelder, Joni-Kristian Ka¨ma¨ra¨inen,
forvideoobjectsegmentation.IEEETransactionsonImage Hyung Jin Chang, Martin Danelljan, Luka Cˇehovin Zajc,
Processing,31:1057–1071,2021. 3 Alan Lukezˇicˇ, et al. The tenth visual object tracking
[41] LingyiHong,WenchaoChen,ZhongyingLiu,WeiZhang, vot2022 challenge results. In ECCVW, pages 431–460.
PinxueGuo,ZhaoyuChen,andWenqiangZhang. Lvos:A Springer,2023. 5,7
benchmarkforlong-termvideoobjectsegmentation. arXiv [54] Hei Law and Jia Deng. Cornernet: Detecting objects as
preprintarXiv:2211.10181,2022. 1 pairedkeypoints. InProceedingsoftheEuropeanconfer-
[42] Lingyi Hong, Wei Zhang, Shuyong Gao, Hong Lu, and enceoncomputervision(ECCV),pages734–750,2018. 6
WenQiang Zhang. Simulflow: Simultaneously extracting [55] Brian Lester, Rami Al-Rfou, and Noah Constant. The
featureandidentifyingtargetforunsupervisedvideoobject powerofscaleforparameter-efficientprompttuning.arXiv
segmentation. In Proceedings of the 31st ACM Interna- preprintarXiv:2104.08691,2021. 3
tionalConferenceonMultimedia,pages7481–7490,2023.
[56] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.
3
Highperformancevisualtrackingwithsiameseregionpro-
[43] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,Bruna
posalnetwork. InProceedingsoftheIEEEconferenceon
Morrone, Quentin De Laroussilhe, Andrea Gesmundo,
computervisionandpatternrecognition,pages8971–8980,
Mona Attariyan, and Sylvain Gelly. Parameter-efficient
2018. 5
transfer learning for nlp. In International Conference on
[57] BoLi,WeiWu,QiangWang,FangyiZhang,JunliangXing,
MachineLearning,pages2790–2799.PMLR,2019. 3
and Junjie Yan. Siamrpn++: Evolution of siamese vi-
[44] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-
sualtrackingwithverydeepnetworks. InProceedingsof
Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.
theIEEE/CVFconferenceoncomputervisionandpattern
Lora:Low-rankadaptationoflargelanguagemodels.arXiv
recognition,pages4282–4291,2019. 1,5
preprintarXiv:2106.09685,2021. 3
[58] ChenglongLi,NanZhao,YijuanLu,ChengliZhu,andJin
[45] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k:
Tang. Weighted sparse representation regularized graph
Alargehigh-diversitybenchmarkforgenericobjecttrack-
learning for RGB-T object tracking. In ACMMM, pages
inginthewild. IEEEtransactionsonpatternanalysisand
1856–1864,2017. 5
machineintelligence,43(5):1562–1577,2019. 2,4,6,1
[59] ChenglongLi,XinyanLiang,YijuanLu,NanZhao,andJin
[46] LaurentItti.Automaticfoveationforvideocompressionus-
Tang. RGB-T object tracking: Benchmark and baseline.
inganeurobiologicalmodelofvisualattention.IEEEtrans-
PatternRecognition,96:106977,2019. 5,7
actionsonimageprocessing,13(10):1304–1318,2004. 1
[60] ChenglongLi,LeiLiu,AndongLu,QingJi,andJinTang.
[47] MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie,
Challenge-aware RGBT tracking. In ECCV, pages 222–
SergeBelongie,BharathHariharan,andSer-NamLim. Vi-
237.Springer,2020. 5
sual prompt tuning. In Computer Vision–ECCV 2022:
[61] Chenglong Li, Wanlin Xue, Yaqing Jia, Zhichen Qu, Bin
17thEuropeanConference,TelAviv,Israel,October23–27,
Luo, Jin Tang, and Dengdi Sun. Lasher: A large-scale
2022,Proceedings,PartXXXIII,pages709–727.Springer,
high-diversitybenchmarkforRGBTtracking.IEEETrans-
2022. 3
actionsonImageProcessing,31:392–404,2021. 5,7,2
[48] DiederikPKingmaandJimmyBa. Adam: Amethodfor
stochastic optimization. arXiv preprint arXiv:1412.6980, [62] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-
2014. 2 ing continuous prompts for generation. arXiv preprint
[49] ChristinKosseandDenisBurdakov. Naturalhypothalamic arXiv:2101.00190,2021. 3
circuitdynamicsunderlyingobjectmemorization. Nature [63] ZhenyangLi,RanTao,EfstratiosGavves,CeesGMSnoek,
communications,10(1):2505,2019. 1 andArnoldWMSmeulders. Trackingbynaturallanguage
[50] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Fels- specification. In Proceedings of the IEEE conference on
berg, RomanPflugfelder, Joni-KristianKamarainen, Luka computervisionandpatternrecognition,pages6495–6503,
Cˇehovin Zajc, Ondrej Drbohlav, Alan Lukezic, Amanda 2017. 1,3,5,2
Berg,etal.Theseventhvisualobjecttrackingvot2019chal- [64] Tsung-Yi Lin, Michael Maire, Serge J. Belongie,
lengeresults. InICCVW,pages0–0,2019. 5 LubomirD.Bourdev,RossB.Girshick,JamesHays,Pietro
[51] Matej Kristan, Alesˇ Leonardis, Jiˇr´ı Matas, Michael Fels- Perona,DevaRamanan,PiotrDolla´r,andC.LawrenceZit-
berg, Roman Pflugfelder, Joni-Kristian Ka¨ma¨ra¨inen, Mar- nick. Microsoft COCO: Common objects in context. In
tin Danelljan, Luka Cˇehovin Zajc, Alan Lukezˇicˇ, Ondrej ECCV,2014. 1
Drbohlav,etal. Theeighthvisualobjecttrackingvot2020 [65] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,
challenge results. In ECCVW, pages 547–601. Springer, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2:
2020. 5 Prompttuningcanbecomparabletofine-tuninguniversallyacrossscalesandtasks. arXivpreprintarXiv:2110.07602, ingtransferablevisualmodelsfromnaturallanguagesuper-
2021. 3 vision. In International conference on machine learning,
[66] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- pages8748–8763.PMLR,2021. 3
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke [79] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Zettlemoyer, and Veselin Stoyanov. Roberta: A ro- Lee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,
bustlyoptimizedbertpretrainingapproach. arXivpreprint and Peter J Liu. Exploring the limits of transfer learning
arXiv:1907.11692,2019. 3 withaunifiedtext-to-texttransformer. TheJournalofMa-
[67] IlyaLoshchilovandFrankHutter.Decoupledweightdecay chineLearningResearch,21(1):5485–5551,2020. 3
regularization. arXivpreprintarXiv:1711.05101,2017. 7 [80] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir
[68] FanMa,MikeZhengShou,LinchaoZhu,HaoqiFan,Yilei Sadeghian,IanReid,andSilvioSavarese. Generalizedin-
Xu, Yi Yang, and Zhicheng Yan. Unified transformer tersectionoverunion:Ametricandalossforboundingbox
trackerforobjecttracking.InProceedingsoftheIEEE/CVF regression. InProceedingsoftheIEEE/CVFconferenceon
Conference on Computer Vision and Pattern Recognition, computer vision and pattern recognition, pages 658–666,
pages8781–8790,2022. 2,3,5,1 2019. 6
[81] YibingSong,ChaoMa,XiaoheWu,LijunGong,Linchao
[69] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-
Bao, Wangmeng Zuo, Chunhua Shen, Rynson WH Lau,
subaihi,andBernardGhanem. Trackingnet: Alarge-scale
and Ming-Hsuan Yang. Vital: Visual tracking via adver-
datasetandbenchmarkforobjecttrackinginthewild. In
sariallearning. InProceedingsoftheIEEEconferenceon
Proceedings of the European conference on computer vi-
computervisionandpatternrecognition,pages8990–8999,
sion(ECCV),pages300–317,2018. 2,4,5,6,7,1
2018. 5
[70] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-
[82] Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li,
subaihi,andBernardGhanem. TrackingNet: Alarge-scale
Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, and
datasetandbenchmarkforobjecttrackinginthewild. In
Jifeng Dai. Towards all-in-one pre-training via maxi-
ECCV,2018. 3
mizing multi-modal mutual information. arXiv preprint
[71] Hyeonseob Nam and Bohyung Han. Learning multi-
arXiv:2211.09807,2022. 3
domainconvolutionalneuralnetworksforvisualtracking.
[83] Ying-LiTian,MaxLu,andArunHampapur.Robustandef-
InProceedingsoftheIEEEconferenceoncomputervision
ficientforegroundanalysisforreal-timevideosurveillance.
andpatternrecognition,pages4293–4302,2016. 5
In2005IEEEComputerSocietyConferenceonComputer
[72] SeoungWugOh,Joon-YoungLee,NingXu,andSeonJoo
Vision and Pattern Recognition (CVPR’05), pages 1182–
Kim. Videoobjectsegmentationusingspace-timememory
1187.IEEE,2005. 1
networks. In Proceedings of the IEEE/CVF International
[84] Zhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen.
ConferenceonComputerVision, pages9226–9235, 2019.
Boxinst: High-performance instance segmentation with
5
boxannotations. InProceedingsoftheIEEE/CVFConfer-
[73] EunbyungParkandAlexanderCBerg. Meta-tracker: Fast
ence on Computer Vision and Pattern Recognition, pages
androbustonlineadaptationforvisualobjecttrackers. In
5443–5452,2021. 4,6
ProceedingsoftheEuropeanConferenceonComputerVi-
[85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
sion(ECCV),pages569–585,2018. 5
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
[74] FedericoPerazzi,JordiPont-Tuset,BrianMcWilliams,Luc andIlliaPolosukhin. Attentionisallyouneed. Advances
VanGool,MarkusGross,andAlexanderSorkine-Hornung. inneuralinformationprocessingsystems,30,2017. 4
Abenchmarkdatasetandevaluationmethodologyforvideo
[86] Paul Voigtlaender, Jonathon Luiten, Philip HS Torr, and
object segmentation. In Proceedings of the IEEE confer-
BastianLeibe.Siamr-cnn:Visualtrackingbyre-detection.
ence on computer vision and pattern recognition, pages
In Proceedings of the IEEE/CVF conference on computer
724–732,2016. 1,3,5,7
visionandpatternrecognition,pages6578–6588,2020. 5,
[75] Jonas Pfeiffer, Aishwarya Kamath, Andreas Ru¨ckle´, 7
Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: [87] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo,
Non-destructive task composition for transfer learning. Xiyang Dai, Lu Yuan, and Yu-Gang Jiang. Omnitracker:
arXivpreprintarXiv:2005.00247,2020. 3 Unifyingobjecttrackingbytracking-with-detection. arXiv
[76] JordiPont-Tuset,FedericoPerazzi,SergiCaelles,PabloAr- preprintarXiv:2303.12079,2023. 2,3,5,1
bela´ez, Alex Sorkine-Hornung, and Luc Van Gool. The [88] QiangWang,LiZhang,LucaBertinetto,WeimingHu,and
2017davischallengeonvideoobjectsegmentation. arXiv PhilipH.S.Torr. Fastonlineobjecttrackingandsegmen-
preprintarXiv:1704.00675,2017. 5,7,2 tation:Aunifyingapproach. InCVPR,2019. 5,7
[77] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, [89] WenhuiWang,HangboBao,LiDong,JohanBjorck,Zhil-
Dario Amodei, Ilya Sutskever, et al. Language models iang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-
areunsupervisedmultitasklearners. OpenAIblog,1(8):9, hammed,SakshamSinghal,SubhojitSom,etal.Imageasa
2019. 3 foreignlanguage:Beitpretrainingforallvisionandvision-
[78] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya languagetasks. arXivpreprintarXiv:2208.10442,2022. 3
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, [90] XiaoWang,JianingLi,LinZhu,ZhipengZhang,ZheChen,
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- XinLi,YaoweiWang,YonghongTian,andFengWu. Vi-sevent: Reliableobjecttrackingviacollaborationofframe ception as object discovery and retrieval. arXiv preprint
andeventflows.arXivpreprintarXiv:2108.05015,2021.5, arXiv:2303.06674,2023. 2,3,5,7,1
7,2 [103] Song Yan, Jinyu Yang, Jani Ka¨pyla¨, Feng Zheng, Alesˇ
[91] XiaoWang,XiujunShu,ZhipengZhang,BoJiang,Yaowei Leonardis,andJoni-KristianKa¨ma¨ra¨inen.Depthtrack:Un-
Wang,YonghongTian,andFengWu. Towardsmoreflexi- veiling the power of RGBD tracking. In ICCV, pages
bleandaccurateobjecttrackingwithnaturallanguage:Al- 10725–10733,2021. 5,7
gorithmsandbenchmark. InProceedingsoftheIEEE/CVF [104] Song Yan, Jinyu Yang, Jani Ka¨pyla¨, Feng Zheng, Alesˇ
Conference on Computer Vision and Pattern Recognition, Leonardis,andJoni-KristianKa¨ma¨ra¨inen.Depthtrack:Un-
pages13763–13773,2021. 1,3,5,2 veilingthepowerofrgbdtracking. InProceedingsofthe
[92] Zhongdao Wang, Hengshuang Zhao, Ya-Li Li, Shengjin IEEE/CVF International Conference on Computer Vision,
Wang,PhilipTorr,andLucaBertinetto. Dodifferenttrack- pages10725–10733,2021. 3,5,2
ingtasksrequiredifferentappearancemodels? Advancesin [105] Shilin Yan, Xiaohao Xu, Lingyi Hong, Wenchao Chen,
NeuralInformationProcessingSystems,34:726–738,2021. WenqiangZhang,andWeiZhang. Panovos:Bridgingnon-
2,3,5,7,1 panoramicandpanoramicviewswithtransformerforvideo
[93] YiWu,JongwooLim,andMing-HsuanYang. Onlineob- segmentation. arXivpreprintarXiv:2309.12303,2023. 1
ject tracking: A benchmark. In Proceedings of the IEEE [106] ShilinYan,RenruiZhang,ZiyuGuo,WenchaoChen,Wei
conference on computer vision and pattern recognition, Zhang, HongyangLi, YuQiao, ZhongjiangHe, andPeng
pages2411–2418,2013. 1,3 Gao. Referred by multi-modality: A unified temporal
[94] Yun Xiao, Mengmeng Yang, Chenglong Li, Lei Liu, and transformerforvideoobjectsegmentation. arXivpreprint
Jin Tang. Attribute-based progressive fusion network for arXiv:2305.16318,2023. 1
RGBTtracking. InAAAI,2022. 5 [107] Jinyu Yang, Zhe Li, Feng Zheng, Ales Leonardis, and
[95] Yun Xiao, Mengmeng Yang, Chenglong Li, Lei Liu, and Jingkuan Song. Prompting for multi-modal tracking. In
Jin Tang. Attribute-based progressive fusion network for ACMMM,pages3492–3500,2022. 5
rgbt tracking. In Proceedings of the AAAI Conference on [108] Jinyu Yang, Zhe Li, Feng Zheng, Ales Leonardis, and
ArtificialIntelligence,pages2831–2838,2022. 3 Jingkuan Song. Prompting for multi-modal tracking. In
[96] Junliang Xing, Haizhou Ai, and Shihong Lao. Multi- Proceedingsofthe30thACMInternationalConferenceon
ple human tracking based on multi-view upper-body de- Multimedia,pages3492–3500,2022. 2,3,1
tection and discriminative learning. In 2010 20th Inter- [109] ZongxinYang,YunchaoWei,andYiYang. Collaborative
nationalConferenceonPatternRecognition, pages1698– videoobjectsegmentationbyforeground-backgroundinte-
1701.IEEE,2010. 1 gration. InComputerVision–ECCV2020: 16thEuropean
[97] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Conference, Glasgow, UK,August23–28, 2020, Proceed-
Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, ings,PartV,pages332–348.Springer,2020. 5
and Thomas Huang. Youtube-vos: Sequence-to-sequence [110] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating
video object segmentation. In Proceedings of the Euro- objects with transformers for video object segmentation.
pean conference on computer vision (ECCV), pages 585– Advances in Neural Information Processing Systems, 34:
601,2018. 1,3,2 2491–2502,2021. 4,5,7,8
[98] HongweiXue,YuchongSun,BeiLiu,JianlongFu,Ruihua [111] BotaoYe,HongChang,BingpengMa,ShiguangShan,and
Song,HouqiangLi,andJieboLuo.Clip-vip:Adaptingpre- Xilin Chen. Joint feature learning and relation modeling
trainedimage-textmodeltovideo-languagerepresentation for tracking: A one-stream framework. In ECCV, pages
alignment. arXivpreprintarXiv:2209.06430,2022. 3 341–357.Springer,2022. 3,5,6
[99] HongweiXue,PengGao,HongyangLi,YuQiao,HaoSun, [112] BotaoYe,HongChang,BingpengMa,ShiguangShan,and
HouqiangLi,andJieboLuo.Stareatwhatyousee:Masked XilinChen.Jointfeaturelearningandrelationmodelingfor
imagemodelingwithoutreconstruction. InProceedingsof tracking: A one-stream framework. In Computer Vision–
theIEEE/CVFConferenceonComputerVisionandPattern ECCV 2022: 17th European Conference, Tel Aviv, Israel,
Recognition,pages22732–22741,2023. 3 October23–27,2022,Proceedings,PartXXII,pages341–
[100] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and 357.Springer,2022. 5,6,1
HuchuanLu. Learningspatio-temporaltransformerforvi- [113] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and
sual tracking. In Proceedings of the IEEE/CVF interna- LucasBeyer. Scalingvisiontransformers. InProceedings
tionalconferenceoncomputervision,pages10448–10457, oftheIEEE/CVFConferenceonComputerVisionandPat-
2021. 5 ternRecognition,pages12104–12113,2022. 3
[101] BinYan, YiJiang, PeizeSun, DongWang, ZehuanYuan, [114] JiqingZhang,XinYang,YingkaiFu,XiaopengWei,Bao-
PingLuo,andHuchuanLu. Towardsgrandunificationof cai Yin, and Bo Dong. Object tracking by jointly ex-
objecttracking.InComputerVision–ECCV2022:17thEu- ploiting frame and event domain. In Proceedings of the
ropeanConference,TelAviv,Israel,October23–27,2022, IEEE/CVF International Conference on Computer Vision,
Proceedings,PartXXI,pages733–751.Springer,2022. 2, pages13043–13052,2021. 3
3,5,7,1 [115] Jiqing Zhang, Bo Dong, Haiwei Zhang, Jianchuan Ding,
[102] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, FelixHeide,BaocaiYin,andXinYang.Spikingtransform-
Zehuan Yuan, and Huchuan Lu. Universal instance per- ersforevent-basedsingleobjecttracking.InProceedingsoftheIEEE/CVFconferenceonComputerVisionandPattern
Recognition,pages8801–8810,2022. 3
[116] Lichao Zhang, Martin Danelljan, Abel Gonzalez-Garcia,
Joost Van De Weijer, and Fahad Shahbaz Khan. Multi-
modal fusion for end-to-end rgb-t tracking. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puterVisionWorkshops,pages0–0,2019. 5
[117] Pengyu Zhang, Jie Zhao, Dong Wang, Huchuan Lu, and
Xiang Ruan. Visible-thermal uav tracking: A large-scale
benchmark and new baseline. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages8886–8895,2022. 3,5
[118] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao,
Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.
Tip-adapter: Training-free clip-adapter for better vision-
language modeling. arXiv preprint arXiv:2111.03930,
2021. 3
[119] Ziyu Zhang, Sanja Fidler, and Raquel Urtasun. Instance-
level segmentation for autonomous driving with deep
denselyconnectedmrfs. InProceedingsoftheIEEECon-
ferenceonComputerVisionandPatternRecognition,pages
669–677,2016. 1
[120] LiZhou, ZikunZhou, KaigeMao, andZhenyuHe. Joint
visualgroundingandtrackingwithnaturallanguagespeci-
fication. arXivpreprintarXiv:2303.12027,2023. 5,7
[121] Xinyu Zhou, Pinxue Guo, Lingyi Hong, Jinglun Li, Wei
Zhang, WeifengGe, andWenqiangZhang. Readingrele-
vantfeaturefromglobalrepresentationmemoryforvisual
objecttracking. AdvancesinNeuralInformationProcess-
ingSystems,36,2024. 1
[122] Zechu Zhou, Xinyu Zhou, Zhaoyu Chen, Pinxue Guo,
Qian-YuLiu,andWenqiangZhang. Memorynetworkwith
pixel-levelspatio-temporallearningforvisualobjecttrack-
ing. IEEETransactionsonCircuitsandSystemsforVideo
Technology,2023. 1
[123] Jiawen Zhu, Simiao Lai, Xin Chen, Dong Wang, and
HuchuanLu. Visualpromptmulti-modaltracking. arXiv
preprintarXiv:2303.10826,2023. 2,3,5,7,1
[124] Xue-Feng Zhu, Tianyang Xu, Zhangyong Tang, Zucheng
Wu,HaodongLiu,XiaoYang,Xiao-JunWu,andJosefKit-
tler. Rgbd1k:Alarge-scaledatasetandbenchmarkforrgb-
dobjecttracking. arXivpreprintarXiv:2208.09787,2022.
3
[125] Xue-Feng Zhu, Tianyang Xu, Zhangyong Tang, Zucheng
Wu,HaodongLiu,XiaoYang,Xiao-JunWu,andJosefKit-
tler. RGBD1K: A large-scale dataset and benchmark for
RGB-Dobjecttracking. AAAI,2023. 5
[126] Yabin Zhu, Chenglong Li, Bin Luo, Jin Tang, and Xiao
Wang. Dense feature aggregation and pruning for rgbt
tracking. In Proceedings of the 27th ACM International
ConferenceonMultimedia,pages465–472,2019. 5
[127] YabinZhu,ChenglongLi,JinTang,andBinLuo. Quality-
awarefeatureaggregationnetworkforrobustrgbttracking.
IEEE Transactions on Intelligent Vehicles, 6(1):121–130,
2020. 5OneTracker: Unifying Visual Object Tracking with
Foundation Models and Efficient Tuning
Supplementary Material
6.Discussion separately. Thisimpliesthatifwewanttohandlemultiple
RGB+X tracking tasks, we need to adjust the parameters
To the best knowledge of ours, we are the first to unify
oftheCMTPromptersandTTPTransformerlayersaccord-
visual object tracking in a general framework. Although
ingly. Although the parameters of these two modules are
there exists some works [1, 68, 87, 92, 101, 102] which
iightweight and can be almost negligible, it still results in
tacklemultipletrackingtasksinasinglemodel,theseworks
inconvenience. Exploringmethodstohandlemultipletasks
only consider RGB modality and ignore multimodal in-
within a general model through joint training is an impor-
formtaion. Moreover, some methods [108, 123] take mul-
tant direction for future research. Secondly, although our
timodal information into consideration, but they only fo-
model is capable of handling 6 tracking tasks across vari-
cusonspecificmodalitiesandstilltreatRGBandRGB+X
ousmodalities,therearestillothermodalitiesthathavenot
tracking as separate entities. We consider these two tasks
beenconsidered. Wewillcontinuetoextendingourmodel
asaunifiedwhole. Ourworkunifiesseveraltrackingtasks,
to more downstream tasks. Thirdly, as the landscape of
RGBtrackingandRGB+N/D/T/E/Mtracking,andachieves
downstreamRGB+Xtasksevolves,itiscrucialtomakeour
competitive performance on 11 benchmarks across the 6
PromptTrackeradaptivetonewtaskswhilemaintainingits
tasks.
originalcapabilities. Ensuringtheflexibilityofourframe-
Diverging from conventional approaches that perform
worktoaccommodateemergingtaskswithoutsacrificingits
full finetuning on downstream datasets, we break the
performanceonexistingtasksisanimportantchallengethat
widely-used full finetuning manner, and introduce the
requiresfurtherinvestigation. Addressingtheselimitations
parameter-efficienttransferlearning(PETL),whichispop-
willcontributetothecontinuousdevelopmentandimprove-
ular in NLP, into tracking. In NLP, a large-scale founda-
mentofourframework,makingitmoreversatile,adaptable,
tionmodelistrainedonbroaddataandownsastronglog-
andeffectiveforabroaderrangeoftrackingtasks.
ical reasoning and generative ability. Then, PETL tech-
niques are adopted to transfer foundation model to down-
7.ExperimentDetails
streamtasksbyfreezingthepretrainedweightsandtraining
inserted parameters. Due to the similar temporal match-
7.1.FoundationTrackerTraining
ing mechanisms in RGB and RGB+X tracking tasks, we
follow the large-scale training and PETL manner in NLP. Foundation Tracker are trained on a combination of sev-
Our framework begins with the pretraining of Foundation eral RGB tracking and object detection datasets, includ-
Tracker on large-scale RGB tracking datasets, enabling it ing LaSOT [28], TrackingNet [69], GOT-10K [45], and
to acquire a strong temporal matching ability. After that, COCO [64], following [18, 35, 112]. We only used the
we incorporate multimodal information as prompt and in- training sets of these dataset for training.Data augmenta-
troduce CMT Prompters to enhance Prompt Tracker with tions, such as horizontal flip and brightness jittering, are
multimodalfeatures,boostingoverallperformance.Despite adoptedduringtraining.
similarstructureisdiscussedinProTrackandViPT,theydo Comparedtoprevioustrackers,thetrainingdatasetsand
not take language and mask into account. Besides, TTP setting,suchasthenumberoftrainingepochs,remaincon-
Transformerlayers areutilizedto adaptPromptTracker to sistent. Despite the same training setting, our Founda-
downstream tasks better. Through adjusting a set of ad- tionTrackerachievessuperiorperformance,outperforming
ditional parameters (about 2.8M), Prompt Tracker inhibits other trackers by at least 0.6 AUC on LaSOT. Models like
thestrongabilityfromFoundationTracker,andhavebetter UNINEXTandOmniTracker, whichaimtoaddressmulti-
adaptability than full finetuning. Importantly, the param- ple vision tasks, utilze a larger set of datasets in addition
eter efficiency makes it particularly suitable for resource- toRGNtrackingdataset6s. ThetrainingofUNINEXTand
constraineddeviceswhereonlyasmallnumberofparame- OmniTracker require significantly more time and GPU re-
ters need to be distributed to end-side deployments for the sources, typically taking several days and utilizing more
generalizationofdownstreamscenarios. GPUs.Incontrast,ourFoundationTrackercanbetrainedin
Limitations. Despite the high effectiveness and effi- aboutonedayon4NVIDIARTX3090GPUs. Compared
ciency, our framework still has some limitations. Firstly, to these models which required much more training data
fordifferenttrackingtaskswithintheRGB+Xdomain,our andtrainingcostthanourFoundationTracker,ourFounda-
PromptTrackerstillneedstobetrainedonspecificdatasets tionTrackerachievesbetterperformanceonRGBtrackingTable 5. Training setting for FoundationTable 6. Finetuning setting for PromptTable 7. Finetuning setting for Prompt
TrackeronRGBtrakingdatasets. TrackeronRGB+N/D/T/Etracking. TrackeronRGB+Mtrackingdatasets.
Config Value Config Value Config Value
optimizer AdamW[48] optimizer AdamW[48] optimizer AdamW[48]
leal re na ir nn gin rg atr ea it nei bn ach ke bad one 4 4× ×1 10 0− −4 5 wle ea ir gn hin tg der ca ate
y
4× 101 −0 4−5 baselearningrate 1×10−5
weightdecay 10−4 weightdecay 1×10−7
batchsize 128
batchsize 128 batchsize 8
epoch 60
epoch 300 Iterations 150,000
learningratedecayepoch 48
learningratedecayepoch 240
learningratedecayfactor 10 learningratedecayfactor 10 learningratedecayiteration 125,000
learningrateschedule steplr learningrateschedule steplr learningrateschedule steplr
maximumsamplingframegap 200 maximumsamplingframegap 200 maximumsamplingframegap 25
(atleast1.3AUConLaSOT).ConsideringthatourFounda-
tionTrackerachievesbetterperformanceonRGBtracking
whileutilizingthesameorsmalleramountoftrainingdata
andcomputationalresourcescomparedtoothermodels,the
comparisononLaSOTandTrackingNetbenchmarksisboth
fairandfavourabletoourapproach.
7.2.PromptTrackerFinetuning
RGB+N/D/T/Etracking. Fortheparameter-efficientfine-
tuningofPromptTrackerondownstreamRGB+Xtracking
tasks, wefinetunePromptTrackeroneachtaskseparately.
The size for template and search frame is 192×192 and
384×384. For RGB+N tracking, we adopt OTB99 [63],
LaSOT[28],andTNL2K[91]astrainingsets. ForRGB+D
tracking, DepthTrack [104] is chosen for training. For
RGB+Ttracking,LasHeR[61]isutilizedfortraining. For
RGB+E tracking, VisEvent [90] is leveraged for training.
Thehyper-parametersareinTable6.
RGB+M tracking. We choose the popular RGB+M
tracking datasets, DAVIS17 [76] and YouTube-VOS [97]
forfinetuning. Weselectthefirstframeandpreviousframe
astemplateframe,anddonotimplementanycroppingop-
eration on the template and search frames. The finetuning
detailsareinTable7