Pantypes: Diverse Representatives for Self-Explainable Models
RuneKjærsgaard1,Ahce`neBoubekki2,LineClemmensen1
1DepartmentofAppliedMathematicsandComputerScience,TechnicalUniversityofDenmark,Denmark
2MachineLearningandUncertainty,Physikalisch-TechnischeBundesanstalt,Germany
rdokj@dtu.dk
Abstract Transparency may be defined by two properties; (i) the
learned concepts are used in the decision making process
Prototypicalself-explainableclassifiershaveemergedtomeet without the use of a black-box model and (ii) the learned
thegrowingdemandforinterpretableAIsystems.Theseclas- conceptscanbevisualizedintheinputspace.
sifiersaredesignedtoincorporatehightransparencyintheir
Trustworthiness may be defined by three properties; (i)
decisionsbybasinginferenceonsimilaritywithlearnedpro-
thepredictiveperformanceofthemodelmatchesitsclosest
totypical objects. While these models are designed with di-
versity in mind, the learned prototypes often do not suffi- black-box counterpart, (ii) explanations are robust and (iii)
cientlyrepresentallaspectsoftheinputdistribution,particu- theexplanationsdirectlyrepresentthecontributionofthein-
larlythoseinlowdensityregions.Suchlackofsufficientdata putfeaturestothemodelpredictions.
representation,knownasrepresentationbias,hasbeenasso- Diversitymaybedefinedbyoneproperty;(i)theconcepts
ciatedwithvariousdetrimentalpropertiesrelatedtomachine
learnedbytheSEMarerepresentedbynon-overlappingin-
learningdiversityandfairness.Inlightofthis,weintroduce
formationinthelatentspace.
pantypes, a new family of prototypical objects designed to
While significant work has been put forth in the litera-
capture the full diversity of the input distribution through a
sparse set of objects. We show that pantypes can empower ture tocement the transparencyand trustworthiness axis of
prototypicalself-explainablemodelsbyoccupyingdivergent SEMs,onlylimitedeffortusingqualitativemeasuresexists
regionsofthelatentspaceandthusfosteringhighdiversity, forthediversityaxis.Similarly,therelationbetweenthedi-
interpretabilityandfairness. versityaxisandappropriateinferenceremainslargelyunex-
plored.Diversityistypicallyensuredbyintroducingmodel
regularization towards learning non-overlapping concepts
Introduction
(ViloneandLongo2020).However,thisconditionmaynot
Machine learning (ML) systems are increasingly affecting be strong enough, as non-overlapping concepts can still be
individuals across various societal domains. This has put learnedinasmallregionoftheinputspace,causingalackof
intoquestiontheblack-boxnatureofthesesystems,andfos- representativityforthefulldatadistribution,knownasrep-
teredthefieldofexplainableAI(XAI),whereinmodelinfer- resentationbias(Shahbazietal.2022).Representationbias
ence is corroborated with justifications and explanations in cancausesmallersub-populationstoremainhiddeninlow-
anefforttoincreasetransparencyandtrustworthiness.Inthis density regions and ultimately cause biased inference (Jin
line ofresearch twoapproaches have arisen;that of ad-hoc et al. 2020). To provide sufficient coverage and to mitigate
black-boxmodelexplanations(Selvarajuetal.2017;Yosin- theimpactofdatabiasduringmodelinference,itiscritical
skietal.2015),andthatofself-explainablemodels(SEMs) tocapturethefulldiversityofthedata,andtohavethisdiver-
(Chen et al. 2019a; Alvarez Melis and Jaakkola 2018). A sityberepresentedintheprototypeslearnedbytheSEM.To
popularapproachforSEMssubstitutestraditionalblack-box thisend,weintroducepantypes,anewfamilyofprototypi-
networkswithglass-boxcounterparts,whereclassrepresen- calobjectsdesignedtoempowerSEMsbysufficientlycov-
tativeprototypesaregeneratedandusedinthedecisionpro- eringthedataspace.Pantypegenerationispromotedusinga
cess(Chenetal.2019a)leadingtoincreasedtrustworthiness novel volumetric loss inspired by a probability distribution
andinterpretability. known as a Determinantal Point Process (DPP) (Kulesza,
Thevariousinitiativesemergingintheliteraturesharethe Taskar et al. 2012). This loss induces higher prototype di-
sameoverarchinggoals,butthereisstillalackofconsensus versity, enables more fine-grained diversity control, and at
ontheexactpropertiesaSEMsshoulddisplay(Gautametal. thesametimeallowsprototypepruningwhereinthenumber
2023). We adopt three prerequisites of a SEM outlined in of prototypes is determined dynamically dependent on the
(Gautametal.2022),namelytransparency,trustworthiness diversityexpressedwithineachclass.Prototypepruningen-
anddiversity. ablesthecapacitytolearnadditionalprototypesforcomplex
classesandtograspsimpleclassesthroughasparsersetof
Copyright©2024,AssociationfortheAdvancementofArtificial objects,improvingtheinterpretabilityoftheclassrepresen-
Intelligence(www.aaai.org).Allrightsreserved. tatives.
4202
raM
41
]LM.tats[
1v38390.3042:viXraOurcontributionscanbesummarizedasfollows: is the loss for a mixture of VAEs using the same network
each with a Gaussian prior distribution centered on one of
• Introduction of a volumetric loss, which promotes the
theprototypes(Gautametal.2022).HereI isad×diden-
generation of pantypes, a highly diverse set of proto- d
titymatrix.Finally,anorthonormalitylosstermisused:
types.
• Quantitativemeasuresforprototyperepresentativityand K
diversityinSEMs. L =(cid:88) ||Φ¯T Φ¯ −I ||2, (5)
orth k k M F
• Dynamicclass-specificprototypeselection. k=1
where Φ¯ is the mean subtracted prototype vector for all
PanVAE k
prototypesofclasskandI isanM ×M identitymatrix.
M
The modeling task at hand involves a classification setting
The orthonormality loss is included to foster intra-class
on visual image data, where the SEM learns to classify
prototype diversity and to uphold the diversity property of
K >0classesfromatrainingsetX={(x ,y )}N ,where
i i i=1 a SEM by inducing the learning of non-overlapping con-
x i ∈ RP istheith imageandy i ∈ {0,1}K isaone-hotla- cepts in the latent space and thus avoiding prototype col-
bel vector. We implement the pantypes1 on the foundation lapse (Wang et al. 2021; Jing et al. 2021). While this loss
ofawell-testedvariationalautoencoderbasedSEM,known causes the prototypes to be orthogonal, it does not explic-
as ProtoVAE (Gautam et al. 2022). This model uses an en- itlypreventtheprototypesfromoccupyingandrepresenting
coder function f : Rp → Rd ×Rd, to transform the input a small region (volume) of the full data-space. Moreover,
imagesintoaposteriordistribution(µ i,σ i).Alatentrepre-
prototype orthonormality is typically achieved early during
sentation z i of the ith image is then sampled from the dis- training,andfurtherscalingoftheorthonormalitylossdoes
tributionN(µ i,σ i)andpassedasinputtoadecoderfunc-
notsignificantlyalterthediversityoftheprototypes(seere-
tion g : Rd → Rp to generate the reconstructed image sultssection).
g(z ) = xˆ. To enable transparent predictions, the model
i i Poororskeweddatarepresentation,knownasrepresenta-
doesnotdirectlyusethefeaturevectorz duringinference,
i tionbias,hasbeenassociatedwithvariousdetrimentalprop-
but rather compares this vector to M prototypes per class
ertiesrelatedtoMLfairness,whereunderrepresentedminor-
Φ = {ϕ }k=1..K via a similarity function : Rd → RM.
kj j=1,,M itygroupsarenegativelyaffectedduringinference(Phillips
Theresultingsimilarityvectors ∈RK×M isthenusedina etal.2011).Tomitigatetheseissuesitisessentialtoachieve
i
glass-boxlinearclassifierh : RM → [0,1]K togeneratethe sufficient coverage of the full diversity represented in the
classpredictionh(s ) = yˆ.Thesimilarityfunction(Chen data(SureshandGuttag2019).Wedrawonthisideatoem-
i i
etal.2019b)isgivenby: powertheProtoVAEmodelbyexchangingitsclass-wiseor-
(cid:18)
||z −ϕ
||2+1(cid:19) thonormalitydiversitylosswithavolumetricdiversityloss,
s (k,j)=sim(z ,ϕ )=log i kj , which causes the model to learn prototypical objects with
i i kj ||z −ϕ ||2+ϵ
i kj various improved qualities, including an improved cover-
(1)
age of the embedding space. We call these learned objects
where 0 < ϵ < 1. This construction allows the similarity pantypes.Thelosstermstructureofourmodelis:
vector to not only capture the distances to the prototypes,
buttoalsoreflecttheinfluenceofeachprototypeonthefinal L =L +L +L , (6)
PanVAE pred VAE vol
prediction.
whereL isthevolumetricprototypeloss,whichnotonly
vol
LossTerms preventspreventsprototypecollapse,butcauseshigherpro-
totypediversity,enablesmorefine-graineddiversitycontrol,
To further enforce the properties of a SEM, we adopt the
and at the same time allows prototype pruning wherein the
samepredictionandVAElosstermstructureasProtoVAE:
numberofprototypesisdetermineddynamicallydependent
L =L +L +L , (2) onthediversityexpressedwithineachclass.
ProtoVAE pred VAE orth
where
Pantypes
N
1 (cid:88)
L pred = N CE(h(s i);y i) (3) Pantypes are prototypical objects learned in an end-to-end
i=1 mannerduringmodeltraining.Theyareinspiredbyaprob-
is a cross-entropy (CE) prediction loss term ensuring inter- abilitydistributionknownasaDeterminantalPointProcess
classdiversityintheprototypesand (DPP) (Kulesza, Taskar et al. 2012), which can be used
to sample from a population while ensuring high diversity.
N K M
1 (cid:88) (cid:88)(cid:88) DPPs have recently garnered attention in the ML commu-
L = ||x −xˆ||2+
VAE N i i nity, and have been used to draw diverse sets in a range of
i=1 k=1j=1 (4) MLapplicationsincludingdatafromvideos,images,docu-
y (k)
s i(k,j)
D (N(µ ,σ )||N(ϕ ,I ))
ments, sensors and recommendation systems (Gong et al.
i (cid:80)M
s (k,j)
KL i i kj d 2014; Kulesza, Taskar et al. 2012; Lin and Bilmes 2012;
l=1 i Zhouetal.2010;Krause,Singh,andGuestrin2008).DPPs
1OurcodeandtrainingdetailsarepubliclyavailableonGitHub describe a distribution over subsets, such that the sampling
athttps://github.com/RuneDK93/pantypes probability of a subset is proportional to the determinantof an associated sub-matrix (a minor) of a positive semi- DATASET PROTOPNET PROTOVAE PANVAE
definitekernelmatrix.Thekernelmatrixexpressessimilar-
MNIST 98.8±0.1 99.3±0.1 99.4±0.1
itybetweenfeaturevectorsofobservationsthroughakernel
FMNIST 89.9±0.5 91.6±0.1 92.2±0.1
function G = g(v ,v ). This global measure of similar-
ij i j QDRAW 58.7±0.0 85.6±0.1 85.5±0.1
ity is then used to sample such that similar items are un- CELEBA 98.2±0.1 98.6±0.0 98.6±0.0
likely to co-occur. The kernel can be constructed in vari-
ous ways including the radial basis function (RBF) kernel
G
ij
= e−γ||vi−vj||2 orthelinearkernel,leadingtoasimi- T toa Vb Ale E1 a: nP dre Pd ri oc tt oiv Pe Np ee tr ofo nr Mma Nn Ic Se T( ,a Fc Mcu Nra Ic Sy T) ,o Qf uP ia cn kV DA raE wP ar no d-
larityfunctionofinnerproductsknownastheGrammatrix
CelebA.Thevaluesarethemeanandstandarddeviationof
G = ⟨v ,v ⟩. When using the Gram matrix, a DPP is
ij i j threeruns.
equivalent to sampling with probability proportional to the
volumeoftheparalellotopeformedbythefeaturevectorsof
the sampled items. We utilize the linear kernel to construct
Results
avolumetriclossontheprototypesinthefollowingway:
Weperformexperimentsacrossvariousreal-worlddatasets
1 (cid:88)K 1 to monitor the transparency, diversity and trustworthiness
L vol = K k=1|G k|21, (7) o (Xf iP aa on ,V RA aE su. lT ,h ae ns de Vda ot la lgse rats fa 2r 0e 1F 7a ),sh Mio NnM ISN TIS (LT e( CF uM nN eI tS aT l)
.
where G ∈ RM×M is the Gram matrix given by G = 1998),QuickDraw(QDraw)(HaandEck2017)andCelebA
k k (Liuetal.2015).WedemonstratethetrustworthinessofPan-
ΦTΦ with Φ as column vectors in Φ and |G | is
k k kj k k VAEbyevaluatingthepredictiveperformanceoftheoverall
the Gramian (Gram determinant). |G k|21 measures the M- modelandassesthediversityandtransparencyusingquali-
dimensional volume of the parallelotope formed by the M tativeassessmentsfromvisualizationsoftheinputspace,as
columnsofΦ k embeddedind-dimensionalspace.Inother wellasquantitativemeasuresofprototypequalityandcover-
words,itexpressesthediversityoftheM prototypesofclass age.WecomparePanVAEtotheperformanceofProtoVAE
kthroughthevolumespannedbytheirfeaturevectors.This andProtoPNet.
loss not only prevents prototype collapse by causing the
feature vectors to diverge, but also directly encourages the PredictivePerformance
pantypes to occupy different sectors of the data domain to
The results for the predictive performance are shown in
expressalargevolume.
Tab. 1, which demonstrates that PanVAE, like ProtoVAE,
Prototype elimination Increasing the scaling on the vol- achieveshigherpredictiveperformancethanProtoPNETon
ume loss punishes pantypes that express a low volume and the four datasets. There is no significant predictive perfor-
thusdirectlyaltersthediversityofthelearnedobjects.With mancegapbetweenPanVAEandProtoVAEonthedatasets.
sufficient scaling, the volumetric loss forces pantypes out- ThisunderlinesthetrustworthinessofPanVAE.
of-distribution(OOD)iftheyarenotnecessarytorepresent
PrototypeRepresentationQuality
the observed diversity of a class. This allows natural prun-
ing, wherein the number of pantypes can be dynamically Firstly, we asses prototype representation quality using vi-
tuned by elimination of OOD pantypes. This is similar to sualinspectionofthelearnedprototypesandtheassociated
thedisciplineofhyperspectralendmemberunmixing,where latent space. This can be seen for the MNIST dataset on
anumberofendmembers(prototypes)aredisentangledfrom Fig.1,wheretheprototypesforProtoVAEandPanVAEare
a hyperspectral image and linear combinations of the end- shown. The diversity of PanVAE is higher than ProtoVAE.
members are used to reconstruct the input images. Follow- TheprototypesfromProtoVAEaremostlyorthogonalinla-
ingtraining,thelearnedendmemberscanbeassociatedwith tentspace,butonlyoccupyasmallregionofthespace.Con-
purityscores(Bermanetal.2004),whichexpressthequal- trarily,thevolumelossinPanVAEhaspushedthepantypes
ityoftheirexplanations.Thesescoresdescribethemaximal away from each other allowing them to occupy and repre-
responsibilityproportionofendmembersforreconstructing sent diverse regions of the dataspace. This is reflected in
theoriginalimages.Inotherwords,ahighpurityscoreindi- thedecodedprototypes,whichshowhighdiversitybyrepre-
cates that an endmember shares a high similarity with in- sentingvariousarchetypicalwaysofdrawingdigits.Forin-
dividual input images, while a low purity score indicates stance,thepantypescapturevariationsbetweenleft-handed
thatanendmemberiscapturingnoiseandshouldbepruned. andright-handeddigitsof”1”aswellasthearchetypical”1”
Such purity scores can be constructed from the similarity withahorizontalbase.Moreover,PanVAEhasfoundthatthe
scoresusedinthelinearclassifierinourSEM.Thus,aspro- digitsof”9”expresslessdiversityandhasthuspushedone
posed by (Berman et al. 2004), we can initiate the model ofthepantypesOOD(indicatedbyaredcrossinthefigure).
with a sufficiently large number of pantypes, and use the ThisformofprototypepruningbyPanVAEallowsthemodel
similarityscorestopruneindividualOODpantypes.Wepro- toassesandrepresenttheindividualdiversityexpressedby
poseaheuristicforpruning,whereapantypecanbepruned eachclass.
if it does not have the maximal similarity score for any of Fig.2demonstratesthediversitycontrolenabledbyPan-
the training images (i.e. it does not individually represents VAE by illustrating learned prototypes on the FMNIST
anyofthetrainingimagesmorethantheotherpantypes). datasetswithdifferentdiversitylossscalings.Theobjective(a)ProtoVAE. (b)PanVAE.
Figure 1: ProtoVAE (a) and PanVAE (b) visualizations of the latent space and decoded prototypes learned on MNIST after
30 epochs of training. Top: UMAP representations of the latent space with learned prototypes overlaid as squares. Bottom:
Decoded prototypes of class ’1’ and ’9’. One of the prototypes from PanVAE does not have the maximal similarity for any
training image, indicated by a red cross. PanVAE has captured variations in the digit ’1’ pertaining to right-handedness (first
’1’fromtheleft),left-handedness(second’1’fromtheleft)andatraditionalwritingstyle(third’1’fromtheleft).
sneaker sneaker
bag bag
ankle boot ankle boot
(a)ProtoVAEL 1. (b)PanVAEL 1.
orth vol
sneaker sneaker
bag bag
ankle boot ankle boot
(c)ProtoVAEL 100. (d)PanVAEL 100.
orth vol
Figure2:DiversitycontrolenabledbyProtoVAEandPanVAE.Thefigureshowsthechangeindecodedprototypeappearance
astherespectivediversityinducinglossesareincreased.TheprototypesareshownfortheFMNISTdataofclasses”sneaker”,
”bag” and ”ankle boot” after 10 epochs of training. Figs. 2a and 2c show the difference between ProtoVAE prototypes with
scalefactorof1and100onthediversitylossL .Figs.2band2dshowthedifferencebetweenPanVAEpantypeswithscale
orth
factorof1and100onthediversitylossL .
vol
of the orthonormalization loss in ProtoVAE is to enforce explainableclassifiersoftenonlyqualitativeassestheproto-
intra-class diversity, and hence that the prototypes capture type diversity axis (Gautam et al. 2022) (i.e. visual inspec-
differentconcepts.Whilethelossensuresthis,itonlydoes tion of the diversity prerequisite of non-overlapping proto-
soaftersufficienttrainingtime.Fig.2showsthatscalingthe types). We propose that self-explainable classifiers should
orthonormalizationlossinProtoVAEdoesnotsignificantly notonlybeassessedwithquantitativemeasuresonthetrust-
alter the diversity of the representation. On the other hand, worthinessaxis,butshouldalsobeevaluatedbyquantitative
thevolumetriclossinPanVAEallowsdirectcontroloverthe measuresonthediversityaxis.Thisincludesthorougheval-
diversityoftherepresentation. uationsofhowwelltheprototypesrepresentthedataspace.
In order to do this we make use of measures of prototype
Previous work in the literature on prototype based self-ProtoVAE ProtoVAE ProtoVAE
4.0 PanVAE 4.0 PanVAE 5.0 PanVAE
3.5 3.5 4.5
3.0 4.0
3.0
3.5
2.5 2.5
3.0
2.0 2.0
2.5
1.5 1.5 2.0
1.0 1.0 1.5
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Epochs Epochs Epochs
(a)MNIST. (b)FMNIST. (c)QuickDraw.
Figure 3: Evolution of prototype DB scores for PanVAE and ProtoVAE on MNIST, FMNIST and QuickDraw. Data points
indicatemeanvaluesandassociatedstandarddeviationsoverthreeruns.
quality and representativity by firstly measuring the proto- DATASET PROTOPNET PROTOVAE PANVAE
type quality using the Davies-Bouldin (DB) index (Davies
MNIST 2.20±0.18 1.21±0.00 1.13±0.03
andBouldin1979)andsecondlyevaluatingthediversityof
FMNIST 3.43±1.15 1.35±0.01 1.09±0.01
theclassrepresentativesbyassessingtheirdatacoverage.
QDRAW 2.52±0.62 2.57±0.01 1.82±0.01
Davies-BouldinIndex TheDBindexisameasureofclus-
CELEBA 27.09±27.23 1.58±0.15 1.37±0.01
terqualitydefinedbytheaveragesimilaritybetweencluster
C i fori = 1,...,k anditsmostsimilarclusterC j.Thesim- Table 2: Davies-Bouldin scores of prototypes from the dif-
ilarity measure R ij quantifies a balance between inter- and ferentmodelsonthedatasetsusedforourexperiments.The
intra-clusterdistances.Weadoptthismeasureandconsider valuesarethemeanandstandarddeviationoverthreeruns.
theprototypesinaSEMasclusterrepresentativesandassign
observationstotheirclosestprototypeinlatentspaceaccord-
ingtomaximalsimilarityscores.Theintra-clustersizes i is aspectsinthedataspacehasbeenfoundcriticalinobtaining
then measured as the average distance between prototype i unbiasedMLalgorithms(Jinetal.2020).
and each data point belonging to the prototype, while the Inordertoassesprototypedatacoverage,wecomparethe
theinter-clusterdistanced ij ismeasuredbythedistancebe- volume spanned by observations represented by the proto-
tween prototypes i and j. From this the cluster similarity typestothevolumeofthefulldatadistribution.Ideally,the
measureR ij canbeconstructedsuchthatitisnon-negative prototypes are diverse enough, that they sufficiently cover
andsymmetricby: a large volume of data they seek to represent. The cover-
agemaybeassessedthroughthevolumeoftheconvexhull
s +s
R = i j. (8) of the data. We evaluate our pantypes on this premise by
ij d
ij samplingthe100nearestobservationstoeachpantype.The
WiththesedefinitionsinplacetheDBindexmaybedefined proximity is measured in the full latent space in terms of
by: the similarity score (Eq. 1). We then compute the volume
spanned by the represented observations from their convex
1 (cid:88)k hull,andcomparethistothevolumeoftheoriginaldata.We
DB =
k
m i=a jxR ij, (9) illustrate the results of this procedure in Fig. 4 using a 2D
i=1 UMAP projection of the 256 dimensional latent space for
wherealowerDBscoresequatestoabetterrepresentationof the ”Bag” class in FMNIST. The increased diversity of the
theunderlyingdata.TheDBscoresforthedifferentmodels pantypesallowthemtooccupyandrepresentalargerregion
canbeseeninTab.2.PanVAEachievesthebestDBscores ofthedataspace.
inallcases,demonstratingtheabilityofthepantypestorep-
Demographic Diversity Sufficient representation of de-
resenttheunderlyingdataspaces.
mographic groups has been found critical in ensuring ML
InadditiontoachievinghigherfinalDBscores,PanVAE
fairness (Jin et al. 2020). Image data used to train facial
also does so using less training time. This is illustrated in
recognitionalgorithmshavehistoricallybeenbiasedtowards
Fig. 3, where the DB score evolution is shown for Proto-
Whiteindividuals,whichareoverrepresentedinthetraining
VAEandPanVAEover100epochsoftraining.PanVAEcon-
data, resulting in biased inference (Buolamwini and Gebru
vergesonalowerDBscoremuchquickerthanProtoVAE.
2018).Thelargestdisparityisfoundbetweenwhiteskinned
DataCoverage TheDBindexprovidesameasureofpro- anddarkskinnedindividuals.
totype quality in terms of prototype representation quality, Demographic diversity may be quantified using a mea-
butdoesnotsufficientlyasseshowwelltheprototypescover sure of combinatorial diversity, also known as diversity in-
thediversityinthedataspace.Sufficientcoverageofvarious dex(Simpson1949).Thecombinatorialdiversityisdefined
xednI
BD
xednI
BD
xednI
BD(a)BagprototypesfromProtoVAE. (b)BagpantypesfromPanVAE.
Figure4:PrototypecoverageinUMAPspacefrom20epochsoftrainingonFMNISTwith5prototypesforthe”bag”classfor
ProtoVAE(a)andPanVAE(b).Top:UMAPrepresentationsofthelatentspacewithlearnedprototypesoverlaidasredsquares.
TheprototypeconvexhullinUMAPspaceisshownasaredoutlinearoundtheprototypesandthefullclassdataspaceconvex
hullisshownasablueoutlinearoundthedata.Asampleofthe100closestobservationstoeachprototypeisshownasblack
datapoints.Theconvexhullofthesampledobservationsisshownasablackoutline.ThePanVAEsampleconvexhullcovers
77% of the volume of the full class convex hull, whereas the ProtoVAE sample convex hull covers 33%. Bottom: Decoded
prototypes.
(a)ProtoVAE.
(b)PanVAE.
Figure 5: Face prototypes learned on the UTK Face dataset. The learned prototypes are shown for ProtoVAE in (a) and for
PanVAE in (b). PanVAE has captured variations in race as well as other unseen features such as facial hair in males. The
ProtoVAE males all have somewhat neutral expressions with shut mouths while most of the females have slight smiles. The
PanVAEmalesandfemalesallexhibitlargevariationsinexpressionfromfullsmileswithvisibleteethtoneutralexpressions
withoutvisibleteeth.
as the information entropy of the distribution (Celis et al. groups.Ahighentropyequatestoamorediverse(fair)rep-
2016): resentation,whichisnotparticularlybiasedtowardsanyde-
k mographicgroup.
(cid:88)
H =− p logp , (10)
i i Weevaluatehowthevolumetriclossmayaidinmitigating
i=1 demographicdatabiasandenhancegroupleveldiversity.To
wherethecombinatorialdiversitymeasureH istheentropy, do this we train PanVAE on the UTK Face dataset (Zhang,
(cid:80)
p istheprobabilityofeventiand isthesumoverthepos- Song,andQi2017),whichcontainimagesofabout20,000
i
sible outcomes k. This measure quantifies the information individualswithassociatedsexandracelabels.Thedecoded
entropyofthedemographicdistributionoverkdemographic facialprototypesfromtrainingontheUTKFacedatasetcanMETRIC PROTOVAE PANVAE graphic diversity such as skin tone. To enforce high demo-
graphic diversity, the images would either have to be pose
ACCALL 95.08±0.11 95.42±0.37
alignedandbackgroundremoved(oratatleastbackground
ACCWHITEMALE 96.35±0.31 95.21±0.33
noise reduced) or the sensitive features would have to be
ACCBLACKFEMALE 91.67±0.53 94.90±0.39
ACCGAP 4.69±0.24 0.32±0.15 incorporated directly into the model, if possible. We have
DIVERSITY 1.26±0.06 1.43±0.07 trained PanVAE on the cropped and aligned version of the
UTK Face dataset to demonstrate that geometric and com-
binatorialdiversitycanbeobtainedsimultaneouslyinnoise
Table 3: UTK results. The values are the mean and stan-
reduceddatawiththevolumetricloss.Morebalanceddemo-
darddeviationofthreeruns.Theoverallaccuracyisreported
graphic representation can lead to better predictive perfor-
along with the individual accuracy and accuracy gap be-
mance for minority sub-populations in the data and conse-
tweenWhitemalesandBlackfemales.Apositivegapvalue
quentlylessdisparatepredictiveperformancebetweensub-
indicates that the mean accuracy is higher on White males
populations.However,thisusuallycomesattheexpenseof
comparedtoBlackfemales.Diversityistheinformationen-
areductioninperformanceforthemajoritygroup.Thus,the
tropy (demographic diversity) of the distribution of races
choice of representation should be carefully considered in
representedbytheprototypes.Therepresentedracesarede-
coherencewiththeaimandtargetpopulationofthetrained
terminedbythenearesttestimagetoeachprototype.
model.
Conclusion
beseeninFig.5.Toevaluatethedemographicdiversity,we
assestheraceofthenearesttestimagetoeachprototypeand We have introduced pantypes, a new family of prototypi-
use this to compute the combinatorial diversity of the race cal objects used in a SEM to capture the full diversity of
distribution. The overall accuracy and diversity results are the dataspace. Pantypes emerge by virtue of a volumet-
reportedinTab.3.Wealsoreporttheaccuracygapbetween ric loss and are easily integrated into existing prototypical
WhitemalesandBlackfemales.Thisaccuracygaphasbeen self-explainable classifier frameworks. The volumetric loss
identifiedasaubiquitousprobleminfacialrecognitionalgo- causes the pantypes to diverge early in the training process
rithms.Whitemalesaccountfor23percentoftheindividu- andtocapturevariousarchetypicalpatternsthroughasparse
als in the UTK Face data, while Black females account for setofobjectsleadingtoincreasedinterpretabilityandrepre-
9 percent. PanVAE achieves a lower accuracy gap between sentationqualitywithoutsacrificingaccuracy.
these demographics due to a better accuracy on Black fe-
males.However,thiscomesattheexpenseofaloweraccu- References
racyonthemajoritysub-populationofWhitemalesascom-
Alvarez Melis, D.; and Jaakkola, T. 2018. Towards robust
paredtoProtoVAE.
interpretability with self-explaining neural networks. Ad-
vancesinneuralinformationprocessingsystems,31.
Discussion
Berman,M.;Kiiveri,H.;Lagerstrom,R.;Ernst,A.;Dunne,
ThevolumetriclossinPanVAEpromotesthegenerationof
R.;andHuntington,J.F.2004. ICE:Astatisticalapproach
diverseprototypes,whichcapturetheunderlyingdataspace
to identifying endmembers in hyperspectral images. IEEE
andrepresentdistinctarchetypicalpatternsinthedata.This
transactions on Geoscience and Remote Sensing, 42(10):
leads to increased representation quality and data coverage
2085–2095.
andcanmitigatedatabias.However,pantypesaremostuse-
Buolamwini,J.;andGebru,T.2018. Gendershades:Inter-
ful when the diversity expressed by the input data aligns
sectionalaccuracydisparitiesincommercialgenderclassifi-
with the diversity a study aims to enforce. This is closely
cation. InConferenceonfairness,accountabilityandtrans-
relatedtotheconceptsofgeometricandcombinatorialdiver-
parency,77–91.PMLR.
sity(Celisetal.2016),wheregeometricdiversityexpresses
the volume spanned by a number of high-dimensional fea- Celis, L. E.; Deshpande, A.; Kathuria, T.; and Vishnoi,
ture vectors and combinatorial diversity is related to infor- N. K. 2016. How to be fair and diverse? arXiv preprint
mation entropy of discrete variables. This means that ge- arXiv:1610.07183.
ometric diversity is useful for ensuring what humans per-
Chen, C.; Li, O.; Tao, D.; Barnett, A.; Rudin, C.; and Su,
ceiveashighvisualdiversity,whilecombinatorialdiversity
J. K. 2019a. This looks like that: deep learning for inter-
is useful for ensuring high demographic diversity (or fair-
pretableimagerecognition. Advancesinneuralinformation
ness)ofhumanunderstandablesensitivevariablesthattake
processingsystems,32.
on a small number of discrete values (such as race). The
Chen, C.; Li, O.; Tao, D.; Barnett, A.; Rudin, C.; and Su,
volumetric loss in PanVAE exclusively ensures a large ge-
J. K. 2019b. This looks like that: deep learning for inter-
ometric diversity of the learned pantypes and as such only
pretableimagerecognition. Advancesinneuralinformation
enforces visually diversity. This may not necessarily align
processingsystems,32.
withthediversityinunseenprotectedattributessuchasrace
in facial image data. This misalignment can occur if fea- Davies, D. L.; and Bouldin, D. W. 1979. A cluster sepa-
tures like background color and pose in the facial images rationmeasure. IEEEtransactionsonpatternanalysisand
exhibitlargervisualvariationthanfeaturesrelatedtodemo- machineintelligence,224–227.Gautam, S.; Boubekki, A.; Hansen, S.; Salahuddin, S.; Vilone, G.; and Longo, L. 2020. Explainable artifi-
Jenssen,R.;Ho¨hne,M.;andKampffmeyer,M.2022. Proto- cial intelligence: a systematic review. arXiv preprint
vae: A trustworthy self-explainable prototypical variational arXiv:2006.00093.
model.AdvancesinNeuralInformationProcessingSystems, Wang,J.;Liu,H.;Wang,X.;andJing,L.2021.Interpretable
35:17940–17952. image recognition by constructing transparent embedding
Gautam,S.;Ho¨hne,M.M.-C.;Hansen,S.;Jenssen,R.;and space. InProceedingsoftheIEEE/CVFInternationalCon-
Kampffmeyer,M.2023. Thislooksmorelikethat:Enhanc- ferenceonComputerVision,895–904.
ingself-explainingmodelsbyprototypicalrelevancepropa- Xiao,H.;Rasul,K.;andVollgraf,R.2017. Fashion-mnist:
gation. PatternRecognition,136:109172. a novel image dataset for benchmarking machine learning
Gong,B.;Chao,W.-L.;Grauman,K.;andSha,F.2014. Di- algorithms. arXivpreprintarXiv:1708.07747.
versesequentialsubsetselectionforsupervisedvideosum- Yosinski, J.; Clune, J.; Nguyen, A.; Fuchs, T.; and Lipson,
marization. Advancesinneuralinformationprocessingsys- H.2015. Understandingneuralnetworksthroughdeepvisu-
tems,27. alization. arXivpreprintarXiv:1506.06579.
Ha,D.;andEck,D.2017. Aneuralrepresentationofsketch Zhang,Z.;Song,Y.;and Qi,H.2017. Age progression/re-
drawings. arXivpreprintarXiv:1704.03477. gressionbyconditionaladversarialautoencoder.InProceed-
Jin, Z.; Xu, M.; Sun, C.; Asudeh, A.; and Jagadish, H. ingsoftheIEEEconferenceoncomputervisionandpattern
2020. Mithracoverage: a system for investigating popula- recognition,5810–5818.
tion bias for intersectional fairness. In Proceedings of the
Zhou,T.;Kuscsik,Z.;Liu,J.-G.;Medo,M.;Wakeling,J.R.;
2020ACMSIGMODInternationalConferenceonManage-
and Zhang, Y.-C. 2010. Solving the apparent diversity-
mentofData,2721–2724.
accuracy dilemma of recommender systems. Proceedings
Jing,L.;Vincent,P.;LeCun,Y.;andTian,Y.2021. Under- oftheNationalAcademyofSciences,107(10):4511–4515.
standingdimensionalcollapseincontrastiveself-supervised
learning. arXivpreprintarXiv:2110.09348.
Acknowledgments
Krause,A.;Singh,A.;andGuestrin,C.2008. Near-optimal
Wewouldliketoacknowledgetheauthorsofthewell-tested
sensor placements in Gaussian processes: Theory, efficient
ProtoVAE. We have used the public code for this model as
algorithms and empirical studies. Journal of Machine
thefoundationofPanVAE.
LearningResearch,9(2).
Kulesza, A.; Taskar, B.; et al. 2012. Determinantal point
processes for machine learning. Foundations and Trends®
inMachineLearning,5(2–3):123–286.
LeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.
Gradient-based learning applied to document recognition.
ProceedingsoftheIEEE,86(11):2278–2324.
Lin,H.;andBilmes,J.A.2012. Learningmixturesofsub-
modularshellswithapplicationtodocumentsummarization.
arXivpreprintarXiv:1210.4871.
Liu,Z.;Luo,P.;Wang,X.;andTang,X.2015. Deeplearn-
ing face attributes in the wild. In Proceedings of the IEEE
internationalconferenceoncomputervision,3730–3738.
Phillips, P. J.; Jiang, F.; Narvekar, A.; Ayyad, J.; and
O’Toole, A. J. 2011. An other-race effect for face recog-
nitionalgorithms. ACMTransactionsonAppliedPerception
(TAP),8(2):1–11.
Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;
Parikh,D.;andBatra,D.2017. Grad-cam:Visualexplana-
tionsfromdeepnetworksviagradient-basedlocalization. In
Proceedings of the IEEE international conference on com-
putervision,618–626.
Shahbazi,N.;Lin,Y.;Asudeh,A.;andJagadish,H.2022. A
SurveyonTechniquesforIdentifyingandResolvingRepre-
sentationBiasinData. arXivpreprintarXiv:2203.11852.
Simpson, E. H. 1949. Measurement of diversity. nature,
163(4148):688–688.
Suresh, H.; and Guttag, J. V. 2019. A framework for un-
derstanding unintended consequences of machine learning.
arXivpreprintarXiv:1901.10002,2(8).