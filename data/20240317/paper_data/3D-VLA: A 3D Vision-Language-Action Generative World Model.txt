3D-VLA: A 3D Vision-Language-Action Generative World Model
HaoyuZhen12 XiaowenQiu1 PeihaoChen3 JinchengYang2 XinYan4
YilunDu5 YiningHong6 ChuangGan17
https://vis-www.cs.umass.edu/3dvla
Abstract ofthehumanbrain. Such2Dfoundationmodelsalsolay
thefoundationforrecentembodiedfoundationmodelssuch
Recentvision-language-action(VLA)modelsrely
asRT-2(Brohanetal.,2023)andPALM-E(Driessetal.,
on2Dinputs,lackingintegrationwiththebroader
2023a) that could generate high-level plans or low-level
realm of the 3D physical world. Furthermore,
actions contingent on the images. However, they neglect
theyperformactionpredictionbylearningadirect
thefactthathumanbeingsaresituatedwithinafarricher
mappingfromperceptiontoaction,neglectingthe
3Dphysicalworldbeyond2Dimages-theyreason,plan,
vastdynamicsoftheworldandtherelationsbe- andactbasedontheir3Dunderstandingoftheenvironment
tweenactionsanddynamics. Incontrast,human
(Palmer, 1975; Pylyshyn, 2003; Marr, 2010). It’s crucial
beingsareendowedwithworldmodelsthatdepict
thathuman-likeintelligentembodiedagentsareequipped
imaginationaboutfuturescenariostoplanactions
withthesame3Dunderstandingability.
accordingly. To this end, we propose 3D-VLA
byintroducinganewfamilyofembodiedfounda- Takingastepforward,recentworks(Huangetal.,2023b;
tionmodelsthatseamlesslylink3Dperception, Hong et al., 2024) develop embodied foundation models
reasoning,andactionthroughagenerativeworld thatcouldplanandactinthe3Denvironment. However,
model. Specifically, 3D-VLA is built on top of suchmodelsmainlylearnadirectmappingfromperception
a3D-basedlargelanguagemodel(LLM),anda toaction,devoidofabroaderunderstandingofthedynamics
setofinteractiontokensisintroducedtoengage oftheworld,andtherelationsbetweenactionsandworld
withtheembodiedenvironment. Furthermore,to dynamics. On the other hand, human beings are blessed
injectgenerationabilitiesintothemodel,wetrain withworldmodelsthatsimulatefutureeventsbasedon3D
aseriesofembodieddiffusionmodelsandalign internalrepresentations. Bydepictingtheimaginationand
themintotheLLMforpredictingthegoalimages anticipationaboutthefuturestates, onecouldbetterplan
andpointclouds.Totrainour3D-VLA,wecurate actionstowardthepredictedgoals.
alarge-scale3Dembodiedinstructiondatasetby
Challengesinevitablyexistforbuildingsuchhuman-like3D
extractingvast3D-relatedinformationfromexist-
worldmodels. Firstly,existingfoundationmodelsfocuson
ingroboticsdatasets. Ourexperimentsonheld-in
languagegeneration,unabletoimaginemodalitiesbeyond
datasetsdemonstratethat3D-VLAsignificantly
languageandsimulatefuturestatestofacilitateactiongener-
improvesthereasoning,multimodalgeneration,
ation,whichisacrucialaspectofworldmodels. Secondly,
and planning capabilities in embodied environ-
existing embodied datasets mainly contain 2D images or
ments,showcasingitspotentialinreal-worldap-
videos, lacking 3D-related annotations for reasoning and
plications.
planninginthe3Dspace.
To this end, we propose 3D-VLA by introducing a new
familyofembodiedfoundationmodelsthatseamlesslylink
1.Introduction
3Dperception,reasoning,andactionthroughagenerative
Nowadays,therehasbeenaproliferationofvision-language world model. Specifically, we build our 3D-VLA on top
models (Liu et al., 2023; Alayrac et al., 2022; Li et al., ofa3Dlargelanguagemodel(Hongetal.,2023)toequip
2023b)thatcantakeimagesasinputsandperformaseries themodelwith3Dunderstandingability. Sinceembodied
ofreasoningtasksinthe2Dspace,mirroringtheversatility taskscouldnotbeaccomplishedvialanguagegeneration
solelyandrequiredeeperdiggingintothedynamicscenes,
1UniversityofMassachusettsAmherst2ShanghaiJiaoTong
themanipulatedobjectsaswellasactionstointeractwith
University3SouthChinaUniversityofTechnology4WuhanUni-
versity5MassachusettsInstituteofTechnology6UniversityofCali- thescenes, weaddspecialinteractivetokenstotheLLM
fornia,LosAngeles 7MIT-IBMWatsonAILab. vocabulary(e.g.,scene,object,andactiontokens). These
1
4202
raM
41
]VC.sc[
1v13690.3042:viXra3D-VLA:A3DVision-Language-ActionGenerativeWorldModel
Reasoningand Localization
What-if Question Answering Embodied Question Answering Task Caption
By 4 steps:
Pick green can 1- Move the cart Pick up the
from middle 2- Place the pen apple fruit
drawer and place 3- Place the book
on counter 4-Adjust the chair
What will happen if <action> ? What should I do next? What happened?
Object Detection Task Caption with Grounding 3D Identification
A plastic,
shinny and
pink bowl in
the middle
Detect the knife with 3D B-box. [locationtokens] Describe the task with 3D B-box. Put carrotin pan This[locationtokens]is?
Multimodal Goal Generation
Goal Depth Generation GoalImage Generation Goal Point Cloud Generation
Goal Image
Generate the depth after closing What dose the picture look like What will the scene be like
<depth> <image> <pcd>
the bottom drawer. after the cloth is folded? when I pour out water?
Robot Planning
Planning on RLBench Planning on CALVIN
Pick up the red cup. Push the blue block into the drawer. Open the drawer.
Figure1. Examplesfromour3DEmbodiedInstructionTuningDataset.
addedtokensenableourmodeltoperformawiderrange actions. Fordatasetslackingdepthdata,weutilizeadepth
of embodied tasks and support interleaved 3D-text data. estimatortoappendnecessary3Ddetailsandprojectthem
Recognizingtheinadequacyofmultimodalgenerationabil- to3Dpointclouds. Additionally,wedesignapipelineto
ity in embodied foundation models, we propose to inject usetheoff-the-shelfmodelstoextract3D-relatedannota-
thegoalgenerationabilityinto3D-VLA.Wefirstpretrain tionsandenrichthelanguagedescriptions. Inthisway,we
asetofembodieddiffusionmodelsforRGBD-to-RGBD collect2M3D-language-actiondatapairs,coveringvarious
and point-to-point generation respectively. To efficiently taskssuchastaskcaptioning,actionprediction,localization,
bridgebetweenthediffusiondecodersofvariousmodalities multimodalgoalgeneration,etc,asshowninFigure1.
andtheLLMembeddingspace,weemployaprojectorthat
Tosumup,wehavethefollowingcontributions:
alignsmulti-modalgoalgenerationin3D-VLA.Itstrategi-
callyincorporatesmultimodalsignalstospecifythetypeof •Wepropose3D-VLA,anewfamilyof3Dvision-language-
modalityforageneration. actionembodiedfoundationmodelsthatunify3Dpercep-
tion,reasoning,andactionwithagenerativeworldmodel.
Another challenge for building such a generative world
model lies in the lack of data. The embodied datasets in •We create a large-scale 3D embodied instruction tuning
use(Padalkaretal.,2023;Brohanetal.,2022;Jangetal., datasetaddressingtheabsenceof3D-relatedinformation
2022)mainlyconsistof2Dimages,deficientin3D-related inexistingembodieddatasets.
information. Thus, we curate a large-scale 3D embodied
•Weaddinteractiontokenstobetterinteractwiththeenvi-
instructiontuningdataset. Specifically,wefirstgatheradi-
ronment. Wefurthertraindiffusionmodelsforgoalimage
versecollectionofdatasetsthatincludesrealandsynthetic
andpointcloudgeneration. Weutilizeaprojectortoeffi-
datafeaturingrobotmanipulationsandhuman-objectinter-
cientlyalignLLMoutputfeaturesanddiffusionmodels.
23D-VLA:A3DVision-Language-ActionGenerativeWorldModel
Goal Imagination Robot Control
Image / PointCloud
Diffusion Model
InitialState Goal State
Projector
Robot: Sure! I should <img pcd> pick up <obj> the chip bag </obj> [loc tokens] </img /pcd> Robot: Actionsare: [action tokens]
3D Vision-Language-Action Model
User: The initial scene is <scene>[embed]</scene> Findsome snacks for me. User: <scene>[embed]</scene> Executenow.
Q-Former Q-Former
Multi/Single View Images 3D Feature 3D Feature
Figure2.Overviewofour3D-VLApipeline.Theleftpartshowsourgoal-generationcapability.Ourmodelcanimaginethefinalstate
imageandpointcloudbasedontheuser’sinput.Thisgeneratedgoalstatecanthenbefedbacktoourmodeltoguidetherobotcontrol.
•Our3D-VLAcanconductaseriesoftasks,includinggoal 3D Foundation Models Our paper is closely related to
generation(intermsofimages,depths,andpointclouds), the 3D foundation models that integrate 3D features in
goal-basedplanning,andembodimentactionprediction. MLLMs(Hongetal.,2023;Chenetal.,2023b;Qietal.,
Itoutperformsbaselinemodelsbyalargemargininthese 2023;Xuetal.,2023;Huangetal.,2023a;Zhouetal.,2023;
novelembodiedtasks. Italsooutshinesbaselinemodelsin Guoetal.,2023;Lietal.,2024). Thesestudieshavesuc-
traditionallanguage-basedtasks. cessfully stepped forward to leverage foundation models
to comprehend 3D features. However, they primarily fo-
cusonanalyzingandreasoninginthecurrentobservable
2.RelatedWorks
state of the 3D scenes, thereby revealing a limitation in
MultimodalLanguageModelsRecentMultimodalLan- predictingfuturefeaturesthatextendbeyondimmediateper-
guageModelshavemaderemarkableadvancesinvarious ception. Contrastingwiththem,weaimtonotonlyunder-
domains,includingvisionandlanguageunderstanding(Li standtheperceivablescenesbutalsopredictimperceptible
etal.,2022;2023b;Liuetal.,2023;Huangetal.,2023c; multimodalfeaturesguidedbyspecificgoals. Thiscapabil-
Pengetal.,2023;Zhuetal.,2023),interleavedimageand ityenablesourmodeltofurthergenerateactiontokensto
textunderstanding(Alayracetal.,2022),interleavedimage interactwiththe3Dworld.
andtextgeneration(Dongetal.,2023). Somemoreunified
modelscanperceiveinputsandgenerateoutputsinarbitrary 3.3DEmbodiedInstructionTuningDataset
combinationsoftext,images,videos,andaudio(Wuetal.,
2023;Luetal.,2023). However,noneofthesemodelscan Recently, benefitingfrombillion-scaledatasetsonthein-
perceive3Dinputsoroutputactionsaccordingto3Dinput. ternet,VLMshavedemonstratedexceptionalproficiencyin
varioustasks. Similarly,million-leveldatasetscomprising
Vision-Language-ActionModelsPreviousvision-language
video-actionpairslaythefoundationforembodiedVLMs
models with action output have predominantly leveraged
forrobotcontrol. However,theymostlydon’tprovidedepth
2D features, thereby lacking the capability of 3D spatial
or3Dannotationsandprecisecontrolinrobotoperations
understanding (Driess et al., 2023b; Brohan et al., 2022;
thatnecessitatetheinclusionof3Dspatialreasoningand
2023). In contrast, our model is guided by 3D features,
interaction. Without3Dinformation,itischallengingfora
which are predicted in alignment with goal objectives in
robottocomprehendandexecutethecommandsthatrequire
ourgeneralworldmodel. Wearethefirsttoleverage3D
3Dspatialreasoning, suchas“placethefarthestcupinto
featuressuchaspointcloudsforactiontokengeneration,
themiddledrawer”.
significantlyimprovingactionplanningaccuracy. Addition-
ally,thispipelinepossessesthepotentialtobeextendedfor To bridge this gap, we build a large-scale 3D embodied
applicationsinreal-worldscenarios. instructiontuningdatasetthatprovidessufficient3D-related
informationaswellaspairedtextinstructionstotrainour
33D-VLA:A3DVision-Language-ActionGenerativeWorldModel
model. Wedesignapipelinetoextract3D-language-action 3D information and attend to the manipulated object for
pairs from existing embodied datasets, obtaining annota- betterdecision-making. Theembodieddatasetsthatserveas
tionsforpointclouds,depthmaps,3Dboundingboxes,the sourcesprovidetextinstructionstodescribethecommands
robot’s7Dactions,andtextualdescriptions. Thedetailsare executedbytherobots.WeusespaCy(Honnibal&Montani,
outlinedasfollows. 2017) to parse the instructions to obtain all noun chunks,
includingthemanipulatedobject. Weutilizeapre-trained
3.1.DatasetCollection groundingmodel(e.g.,Grounded-SAM(Renetal.,2024))
toobtainthe2Dmaskofeachobject.These2Dmasks,when
Ourdataarecuratedfromvarioussources. Weprovidean
liftedto3D,correspondtopartsofthepointcloud,allowing
overviewhere,withdetailsavailableintheAppendix:
us to obtain the 3D bounding boxes of all the objects in
RobotDatasets: Weselect12datasets(Brohanetal.,2022; space. When selecting masks, the manipulated object is
Jangetal.,2022;Walkeetal.,2023;Lynchetal.,2023;Feng chosen based on the highest confidence value in areas of
etal.,2023;Chenetal.,2023a;Dassetal.,2023;Mandlekar significantopticalflow. Sincewereconstructthedepthsand
etal.,2019;Meesetal.,2023;Shahetal.,2023;Sawhney pointclouds,wecoulduseimages,depths,andpointclouds
etal.,2021;Sermanetetal.,2023)fromtheOpen-XEmbodi- infutureframesasground-truthgoals. Foractions,weuse
mentDataset(Padalkaretal.,2023).Theyhavehigh-quality the7DoFactionsfromtheprovideddatasets.
imageswithlinguisticinstructionsintherealworldbutlack
more in-depth information and 3D annotations. We also 3.3.LanguageAnnotations
select datasets with excellent depth information, such as
Inspired by (Li et al., 2023a; Peng et al., 2023), we pro-
Dobb-E(Shafiullahetal.,2023)andRH20T(Fangetal.,
posetogeneratedenselanguageannotationsconsistingof
2023). Additionally, we use datasets collected from two
tokens(e.g.,<image></image>;<pcd></pcd>)thaten-
simulatorenvironments,RLBench(Jamesetal.,2020)and
compassthe3Dannotations(boundingboxes,goalimages
CALVIN(Meesetal.,2022).
/ depths / point clouds, actions) we generated before, as
HumanObjectInteractionDatasets: Human/hand-object showninthepromptsinFigure2.
interactionscouldprovidedemonstrationsthatbenefitrobot
Weusepre-definedlanguagetemplateswithtokenstocon-
decision-makingandimitation. Therefore,weutilizesev-
structthese3Dannotationsintopromptsandanswers. Fol-
eralhuman-objectinteractiondatasets,includingdatasets
lowing(Hongetal.,2023),weuseChatGPT-basedprompt-
withoutdepthinformation,suchasEpic-Kitchens(Damen
ingtodiversifyprompts. Specifically,weprovideinstruc-
etal.,2018),anddatasetswithbetter3Dannotations,such
tions to ChatGPT, as well as our annotated objects and
asHOI4D(Liuetal.,2022).
boundingboxes. Wealsogive2-3few-shothuman-written
demonstrationstoguidetheGPTonthetypeofdataitis
3.2.VisualAnnotations
instructedtogenerate. ChatGPTisaskedtosummarizethe
Estimatingdepthsandopticalflows. Giventhatover95% informationandrewritethetemplate-generatedpromptsinto
ofthevideodatasetsforembodiedtasksdonotprovide3D morediverseforms.Fortaskswithoutpre-definedtemplates,
information, we employ ZoeDepth (Bhat et al., 2023) on ChatGPTisalsoaskedtogeneratepromptsandanswersas
eachframeofthevideofromthesedatasets.Additionally,to language inputs and outputs of these tasks by itself. We
betterutilizevideodata,weuseRAFT(Teed&Deng,2020) show the detailed templates and prompts to generate all
for optical flow estimation. Optical flow aids in refining typesofdataintheAppendix.
thedatawegenerate. Thus,forvideosegmentswherethe
cameraposedoesnotchange,weuseopticalflowtoestimate 4.Methods
which pixels are the unmoved background. We align the
depthmapsofthesebackgroundsacrossdifferentframes 4.1.Overview
ofthesamevideo,multiplyingeachframe’sdepthmapby
Inthissection,weintroduce3D-VLA,aworldmodelfor
acoefficienttoensuredepthconsistency. Aftergettingthe
3Dreasoning,goalgeneration,anddecision-makinginem-
depthmaps,wecandirectlylifttheRGB-Dimagesinto3D
bodiedenvironments. AsshowninFigure2,wefirstbuild
pointcloudsusingcameraintrinsicsandposes.
ourbackboneontopof3D-LLM(Hongetal.,2023),and
Generating3Dannotations. Weaimtogenerateseveral furtherenhancethemodel’scapabilitiestointeractwiththe
3D-relatedannotations: 3Dboundingboxesoftheobjects, 3D world by adding a series of interaction tokens. Next,
goal images, depths, or point clouds as the imagination weinjectgoalgenerationabilityinto3D-VLAbyfirstpre-
outcomes,aswellasrobotactionsinthe3Dspace. Wefirst training the embodied diffusion models and employing a
extractthe3Dboundingboxesoftheobjectsinthescenes. projectorforaligningtheLLMandthediffusionmodels.
Suchinformationcouldbenefit3Dmodels’abilitytocapture
43D-VLA:A3DVision-Language-ActionGenerativeWorldModel
4.2.3D-VLA ing the ground-truth final states can enhance the model’s
reasoningandplanningcapabilities. However,trainingan
4.2.1.BACKBONE
MLLMtogenerateimages,depths,andpointcloudsisnon-
Inthefirststage,wedevelopthe3D-VLAbasemodelfol- trivial. Firstly,state-of-the-artvideodiffusionmodelsare
lowingthe methodologyof 3D-LLM(Hong etal., 2023). nottailoredforembodiedsetups. Forinstance,whenask-
Sincethedatasetwecollectedisnotatthebillion-levelscale ingRunway(Esseretal.,2023)togeneratefutureframes
requiredfortrainingamulti-modalLLMfromscratch,we giventheinstruction“openthedrawer”,theentiresceneis
followtheapproachof3D-LLMbyleveragingmulti-view alteredtoagreatextentwithregardtoviewchange,unex-
features to generate 3D scene features. This enables the pectedobjectdeformation,andweirdtexturereplacement,
seamless integration of visual features into a pre-trained aswellaslayoutdistortion. Similarly,usingthemethodof
VLMwithnoneedforadaptation. Meanwhile,thetraining DreamLLM(Dongetal.,2023)todirectlyfreezethesta-
datasetsfor3D-LLMmostlycompriseobjects(Deitkeetal., blediffusiontrainedoninternetdata,canleadtocollapsed
2022)andindoorscenes(Daietal.,2017;Ramakrishnan outputs. Secondly,howtoincorporatediffusionmodelsof
etal.,2021),whichdonotdirectlyalignwithourembod- variousmodalitiesintoasinglefoundationmodelremains
iedsetup. Therefore,wechoosenottoloadthe3D-LLM achallenge. Therefore,weproposetoinjecttheabilityto
pretrainedmodel. Instead,weutilizeBLIP2-FlanT5 (Li generateimages,depthsandpointcloudsinto3D-VLA.We
XL
etal.,2023b)asourpretrainedmodel. Duringtraining,we firstpretraintheembodieddiffusionmodelsintermsofdif-
unfreezeboththeinputandoutputembeddingsfortokens, ferentmodalitiessuchasimages,depthsandpointclouds,
aswellastheweightsoftheQ-Former. andthenalignthedecodersofthesediffusionmodelstothe
embeddingspaceof3D-VLAthroughanalignmentstage.
4.2.2.INTERACTIONTOKENS
4.3.1.PRETRAININGEMBODIEDDIFFUSIONMODELS
Toenhancethemodel’scomprehensionof3Dscenesand
FORGOALGENERATION
facilitateinteractionwithintheseenvironments, weintro-
duceanovelsetofinteractiontokens. Firstly,Weincorpo- Toaddressthelimitationsofcurrentdiffusionmodelsfor
rateobjecttokens<obj> </obj>thatenclosetheobject goalgenerationinanembodiedenvironment,wetrainRGB-
nounsintheparsedsentences(e.g.,<obj> a chocolate DtoRGB-Dandpoint-cloudtopoint-clouddiffusionmod-
bar </obj> [loc tokens] on the table) so that els. Weutilizeourcurated3D-languagevideodatatotraina
the model could better capture which objects are manip- conditionaldiffusionmodelthateditstheinitialstatemodal-
ulatedorreferredto. Secondly,tobetterrepresentspatial itybasedoninstructionstogeneratethecorrespondingfinal
information by language, we devise a set of location to- statemodality. Thespecifictrainingdetailsforthesemodels
kens <loc0-255> for grounding referred objects, which are as follows: For RGBD to RGBD generation, we em-
arerepresentedbysixtokensforthe3Dboundingboxinthe ployStableDiffusionV1.4(Rombachetal.,2022)asour
formofAABB.Thirdly,tobetterencodedynamicswithour pretrainedmodelduetotheefficiencyandqualityofimage
framework,weintroducethe<scene> </scene>tokens generationbylatentdiffusionwhenoperatinginthelatent
toenclosetheembeddingsofastaticscene. Bycomposing spaceofapretrainedVAE(Kingma&Welling,2013). We
overthescenetokens,3D-VLAcouldcomprehenddynamic concatenatetheRGBlatentanddepthlatentastheimage
scenesandmanageinputsthatinterleave3Dscenesandtext. condition. Similarly,forpoint-to-pointgeneration,weuse
Point-E (Nichol et al., 2022) as the pretrained model, to
Wefurtherenhancethearchitecturewithanexpandedset
whichweaddapointcloudconditioninput.
of specialized tokens that represent robotic actions. The
robot’sactions,with7degreesoffreedom,arerepresented
4.3.2.BRIDGINGLLMANDGOALGENERATION
bydiscretetokenssuchas<aloc0-255>,<arot0-255>,
and<gripper0/1>todenotethearm’sintendedabsolute After pretraining the diffusion models, we are equipped
location,rotation,gripperopenness. Theseactionsaresepa- withvariousdecodersthatcouldgenerategoalsbycondi-
ratedbytoken<ACT SEP>. tioning the latent spaces in their modalities. Challenges
remainastohowtoseamlesslyincorporatethepretrained
4.3.InjectingGoalGenerationAbilityinto3D-VLA decoders into the LLMs so that 3D-VLA could gener-
ate goals with regard to any pretrained modalities condi-
In this section, we introduce how our 3D-VLA performs
tioned on the input instructions. To bridge the gap be-
goalgenerationintermsofimages,depths,andpointclouds.
tweentheLLMandthediffusionmodelsofdifferentmodal-
Human beings pre-visualize the final states of the scenes ities,wedevelopanalignmentstageintoour3D-VLA.We
to facilitate action prediction or decision making, which firstintroduceadditionalspecialtokenssuchas<image>
is a key aspect in building world models. Moreover, dur- </image> and <pcd> </pcd>. These tokens are intri-
ingpreliminaryexperiments,wealsodiscoverthatprovid- cately designed to inform the decoder about the type of
53D-VLA:A3DVision-Language-ActionGenerativeWorldModel
Tasks Models BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGH-L EM@1
3D-LLM∗ 1.05 0.38 0.15 0.02 12.96 0.91 0.00
BLIP2OPT ∗ 7.39 3.17 0.03 0.02 3.87 7.40 3.03
2.7B
BLIP2FlanT5 ∗ 22.84 16.17 12.50 10.11 11.41 32.01 10.31
EmbodiedQA XL
OpenFlamingo ∗ 9.50 6.51 5.14 4.29 6.84 10.40 1.21
4B
LLaVA ∗ 11.66 8.06 6.01 4.58 12.59 14.17 5.67
7B
BLIP2FlanT5 37.31 27.20 20.32 15.48 17.80 38.92 15.35
XL
3D-VLA 48.34 38.55 31.72 26.80 23.72 49.33 24.53
3D-LLM∗ 0.78 0.16 0.07 0.05 0.57 1.33 0.00
BLIP2FlanT5 ∗ 8.50 2.07 0.35 0.00 3.40 8.45 0.00
XL
OpenFlamingo ∗ 7.61 1.64 0.37 0.00 4.74 9.36 0.00
TaskCaption 4B
LLaVA ∗ 2.63 0.69 0.16 0.00 2.63 4.65 0.00
7B
BLIP2FlanT5 22.05 11.40 5.72 3.16 8.72 26.12 7.75
XL
3D-VLA 55.69 45.88 39.39 34.88 27.57 62.01 29.34
BLIP2FlanT5 28.23 11.47 4.49 0.06 8.27 28.41 5.85
What-ifQA XL
3D-VLA 53.09 40.94 34.34 29.38 26.83 52.82 14.7
3D-LLM∗ 0.52 0.22 0.16 0.13 0.34 0.64 0.00
DenseCaption BLIP2FlanT5 36.17 24.72 18.06 13.96 17.83 40.56 13.10
XL
3D-VLA 51.90 42.83 38.11 34.62 25.25 55.91 39.49
Table1.Evaluationonreasoningabilityusingheld-indata.∗denoteszero-shottransferresultswithouttrainingonourpre-traindatasets.
modalcontenttooutput. Betweentheenclosingtokens,we Methods IoU Acc@25 Acc@50
supervise the LLM in generating instructions for a robot Kosmos-2(w/GTDepth) 10.92 12.73 3.85
toexecute,whichmayincludeobjecttokensandlocation CoVLM(w/GTDepth) 19.81 25.39 16.61
tokens, such as <image> pick up the <obj> apple 3D-VLA 29.33 42.26 27.09
</obj> [loc tokens] </image>. Based on this, we
Table2. Localizationresultsonheld-inroboticsdatasets.
canapplyatransformer-basedprojector,whichiscapable
ofmappingthedecoderfeaturesandembeddingsfromthe
Large Language Model (LLM) into the space of the DM
ofinteraction,whichrequireagreaterlevelofreasoningand
framework. Itplaysacrucialroleinenhancingthemodel’s
localizationabilities. Webuildseveraltaskson3Dembod-
capabilitytounderstandandgeneratemulti-modaldata,es-
iedinstructiontuningdatasetsforlearningtheseabilitiesin
tablishingaconnectionbetweenhigh-levellanguageunder-
theroboticsdomain. Thetasksinclude1)embodiedQAon
standingandmulti-modalgoalgeneration. Tomaketraining
RoboVQAdataset(Sermanetetal.,2023);2)taskcaption-
3D-VLAmoreefficientandtoavoidcatastrophicforgetting,
ingon11Open-Xdatasets(Padalkaretal.,2023),wherewe
weutilizeLoRA(Huetal.,2021)tofine-tunedifferentdif-
inputtheinitialandfinalscenesandasktheagenttoreason
fusionmodels. Atthesametime,weonlytrainthenewly
whathashappened;3)what-ifQAonRT-1dataset(Brohan
introducedspecialtokensembeddings,thecorresponding
etal.,2022),wheretheagentisaskedaquestionthatwhat
embeddingoutputlinearlayer,andtheentireprojector. We
willhappenifsomespecifiedactions(representedbyaction
minimizeboththeLLMandDMdenoisingloss.
tokens) are executed; 4) dense captioning on 11 Open-X
datasets,wheretheagentneedtocaptionthecontentspec-
5.Experiments ifiedbya3dboundingbox;5)localizationon11Open-X
datasets,wheretheagentistolocalizetheobjectmentioned
3D-VLA is a versatile 3D-based generative world model
intherobotmanipulationinstruction. Weevaluate3D-VLA
thatcanperformreasoningandgroundinginthe3Dworld,
onthesetasksusingheld-indatasets.
imaginemulti-modalgoalcontent,andgenerateactionsfor
robotmanipulation. Inthissection,weevaluate3D-VLAin Baselines. We compare 3D-VLA with 3D-LLM (Hong
threeaspects: 3Dreasoningandlocalization,multi-modal et al., 2023) and 2D vision-language models, including
goalgeneration,andembodiedactionplanning. BLIP2 (Li et al., 2023b), OpenFlamingo (Alayrac et al.,
2022),andLLaVA(Liuetal.,2023). Weimplementthese
5.1.3DReasoningandLocalization baselinesintwoways: 1)zero-shottransferwherewetest
thereleasedtrainedmodelonthesenewtasks; 2)held-in
Tasks. Ourprimaryfocusisonscenesinvolvingrobotsthat evaluationwherewetrainthereleasedmodelon2D-image-
arecharacterizedbygreaterdynamismandahigherdegree action-languagepairs(i.e.,,11datasetsselectedfromOpen-
63D-VLA:A3DVision-Language-ActionGenerativeWorldModel
Method PSNR↑ CLIPSim↑ SSIM↑ FID↓ Qualitativeresults. Theimagegoalgenerationresultsare
Instruct-P2P 14.41 0.909 0.389 0.309 showninTable3. Whencomparedwiththeexistinggenera-
SuSIE 15.20 0.898 0.549 0.182 tionmethodsthatdirectlyzero-shottransferstotherobotics
NeXT-GPT 8.86 0.199 0.153 0.432
domain(rows1,2,3inTable3),3D-VLAachievesapromis-
Instruct-P2P∗ 16.67 0.941 0.628 0.178
3D-VLAw/oPredBBox 17.02 0.919 0.632 0.173 ingperformanceintermsofmostmetrics. Thisunderscores
3D-VLA 17.21 0.920 0.636 0.177 the importance of training a world model using datasets
specificallydesignedforroboticsapplications. Evenina
Table3.RGBimagegoalgenerationresults.∗denotesthemodel
direct comparison with Instruct-P2P*, which was trained
istrainedonourpretraineddataset.
onthesameroboticsdatasetsweemployed(row4inthe
table),3D-VLAconsistentlyoutperformsit. Thishighlights
Models P-FID↓ Chamfer-L ↓
1 thattheintegrationofalargelanguagemodelinto3D-VLA
Point-E∗ 5.241 0.159
resultsinamorecomprehensiveandinsightfulcomprehen-
3D-VLAw/oPredBBox 4.914 0.143
sionofroboticsmanipulationinstructions,leadingtobetter
3D-VLA 4.796 0.139
goalimagegenerationperformance. Furthermore,whenwe
Table4.PointCloudgoalgenerationresults.∗denotesthemodel excludethepredictedboundingboxfromtheinputprompt
istrainedonourpretraineddataset. (row5),weobserveaslightdecreaseinperformance. This
observationconfirmstheeffectivenessofusingtheseinter-
mediatepredictedboundingboxesastheyassistthemodel
XandRoboVQAdataset).Forthelocalizationtask,wecom-
in comprehending the overall scene, allowing the model
parewith2DgroundingMLLM,namelyKosmos-2(Peng
toallocatemoreattentiontothespecificobjectmentioned
etal.,2023)andCoVLM(Lietal.,2023a). Specifically,we
inthegiveninstruction,ultimatelyenhancingitsabilityto
usethesemodelstodetect2Dboundingboxesinazero-shot
imaginethefinalgoalimages.
mannerandthentransferthemto3Dboundingboxesusing
depthprojection. ThepointcloudgenerationresultsarepresentedinTable4.
3D-VLAwithintermediatepredictedboundingboxesper-
Resultanalysis. InTables1,3D-VLAoutperformsall2D
forms the best. This outcome reinforces the significance
VLMmethodsonlanguagereasoningtasks. Weattribute
ofincorporatinglargelanguagemodelsandpreciseobject
ittotheleverageof3Dinformation,whichprovidesmore
localization in the context of comprehending both the in-
accuratespatialinformationforreasoning. Besides,since
structionandthescene.
ourdatasetcontainsabunchof3Dlocalizationannotations,
3D-VLAlearnstolocalizetherelevantobjects,whichhelps Quantitativeresults. InthefirstrowofFigure3,wevisu-
themodelfocusmoreonkeyobjectsforreasoning. More- alizethegeneratedRGB-Dgoalimagesonthetestsetof
over,wefindthat3D-LLMperformspoorlyontheserobotic RT-1(Brohanetal.,2022)andJacoPlay(Dassetal.,2023)
reasoningtasks,whichdemonstratesthenecessityofcollect- datasets. These samples are not seen in the training pro-
ingandtrainingonarobotics-related3Ddataset. InTable2, cess. Giventheinitialscenesandinstructions,the3D-VLA
3D-VLA demonstrates a marked superiority over the 2D modelconsistentlyexhibitsthecapabilitytomaintainthe
baselinemethodsintermsoflocalizationperformance. This backgroundelementsunchangedwhileaccuratelyidentify-
findingservesascompellingevidenceoftheefficacyofour ingthetargetobjectofinteractionandcorrectlymodifying
annotationprocess,whichsuppliesasubstantialquantityof thestatesoftheseidentifiedobjectsfollowingtheprovided
3Dannotations,therebyfacilitatingtheacquisitionofrobust instructions. The generated RGB-D goal images closely
3Dlocalizationcapabilitieswithinourmodel. alignbothintermsofvisualappearanceandsemanticcon-
tentwiththegroundtruthgoal. Inadditiontoourcontrolled
experimentalsettings,weextendedourtestingtoencompass
5.2.Multi-modalGoalGeneration
scenescapturedfromtheinternetoreverydaylife. Inthese
Tasks. WequantitativelyevaluatetheRGBgoalandpoint diverseanduncontrolledenvironments,our3D-VLAmodel
cloudgoalgenerationcapabilityof3D-VLAonOpen-Xtest consistentlyandrobustlydemonstrateditsefficacy.
sets. Werandomlysample4000episodesfromtheOpen-X
testsetwhich3D-VLAdoesnotseeinthetrainingprocess. 5.3.EmbodiedActionPlanning
Baselines. For image generation, we compare 3D-VLA
Tasks We evaluate the ability of 3D-VLA for robot
with three types of image generation methods: 1) image-
arm action prediction on two benchmarks, namely RL-
editingmethodsInstruct-P2P(Brooksetal.,2023);2)goal
Bench(Jamesetal.,2020)andCALVIN(Meesetal.,2022).
image/videogenerationmethodsSuSIE(Blacketal.,2023);
WeselectthreetasksfromRLBenchforevaluation.Besides,
3) LLMs with image generation ability NeXT-GPT (Wu
wealsoselectvar1fromthepick-up-cuptaskasanunseen
etal.,2023). Forpointcloudgeneration,wecomparewith
tasktotestthemodel’sgeneralizationability. ForCALVIN,
text-to-3DdiffusionmodelPoint-E(Nicholetal.,2022).
73D-VLA:A3DVision-Language-ActionGenerativeWorldModel
Input GT Goal Pred Goal Input GT Goal Pred Goal
RT-1
Place blue chip bag into top drawer (RT-1) Pick banana from white bowl (RT-1)
RT-1
JacoPlay
Move green chip bagnear water bottle (RT-1) Place the long bread on the table (JacoPlay)
Input Pred Goal Input Pred Goal Input Pred Goal
Novel Env
Remove the plastic bottle Knockthecokecanover Close middle drawer
Figure3.VisualizationofgeneratedRGB-Dgoalimages.Theresultsinthefirstrowaresampledfromthetestsetofheld-intrainingdata
whilethesecondrowistheunseenenvironmentsgatheredfromtheInternetordailylife.
Put Take Pickup Pickup Taskscompletedinarow
Knife Umbrella Cup Cup(unseen) 1 2 3 4 5
LanCon-Learn 28.8 45.6 23.2 -
MCIL 28.2 2.5 0.3 0 0
LanCon-Learnw/His. 32.2 50.8 44.2 -
3D-VLA 68 52 40 24 3D-VLA 44.7 16.3 8.1 1.6 0
Table5.EvaluationofactionplanningonRLBenchdataset. Table6.EvaluationofactionplanningonCALVINdataset.
we evaluate our model under the long-horizon multi-task
languagecontrolsetting,wheretheagentisrequiredtoexe-
cute5taskssequentially. WetraintheagentonscenesA,B, matchesthebaselineperformanceinmosttaskswithinthe
C,DandtestonsceneD. RLBenchactionprediction,showingitsplanningcapability.
It’sworthnotingthatthebaselineuseshistoryobservations,
Baselines. For RLBench, we compare our model 3D-
objectstates,andcurrentstateinformation,whereasweonly
VLA with LanCon-Learn (Silva et al., 2021), which is
executeviaopen-loopcontrol. Additionally,ourgeneraliza-
a multi-task approach that can predict actions based on
tioncapabilityisproveninthepick-up-cuptask. InTable6,
instruction-conditionedinputs. ForCALVIN,wecompare
3D-VLAalsoachievespromisingresultsinCALVIN.We
withMCIL(Lynch&Sermanet,2020), whichisacondi-
attributethesuperioritytotheabilitytolocalizetheobjects
tionalsequence-to-sequencevariationalautoencoder.
ofinterestandimaginethegoalstate,whichprovidesrich
Resultanalysis.AsshowninTable5,3D-VLAsurpassesor informationforinferringactions.
83D-VLA:A3DVision-Language-ActionGenerativeWorldModel
6.Conclusion Brooks,T.,Holynski,A.,andEfros,A.A. Instructpix2pix:
Learningtofollowimageeditinginstructions,2023.
In this paper, we introduce 3D-VLA, a generative world
modelthatcanreason,understand,generate,andplaninthe
Chen, L., Bahl, S., and Pathak, D. Playfusion: Skill ac-
embodiedenvironment. Wedeviseanoveldatageneration
quisitionviadiffusionfromlanguage-annotatedplay. In
pipelinetoconstructadatasetincluding2M3D-Language-
ConferenceonRobotLearning,pp.2012–2029.PMLR,
actiondatapairstotrainourmodel. Thesedataenableit
2023a.
toperformdiversetaskssuchastaskcaption,localization,
goal image/point cloud generation, action prediction, etc.
Chen, S., Chen, X., Zhang, C., Li, M., Yu, G., Fei, H.,
Ourmodeluses3D-LLMasthebackboneandintroduces
Zhu,H.,Fan,J.,andChen,T. Ll3da: Visualinteractive
interaction tokens to interact with the environment. We
instructiontuningforomni-3dunderstanding,reasoning,
trainaimagetoimageandpointtopointdiffusionmodel
andplanning,2023b.
forembodiedAI.Theyarefurtheralignedbyaprojector
withtheLLMtoenhancetheLLM’smultimodalgeneration Dai,A.,Chang,A.X.,Savva,M.,Halber,M.,Funkhouser,
capabilities. The experiment further shows that our 3D- T.,andNießner,M. Scannet: Richly-annotated3drecon-
VLAhasstrongercapabilitiesinembodiedtasksthanthe structionsofindoorscenes,2017.
2Dbaseline.
Damen, D., Doughty, H., Farinella, G. M., Fidler, S.,
ImpactStatement Furnari,A.,Kazakos,E.,Moltisanti,D.,Munro,J.,Per-
rett,T.,Price,W.,etal. Scalingegocentricvision: The
Thispaperintroducesresearchaimedatpushingthebound- epic-kitchens dataset. In Proceedings of the European
ariesofMachineLearningintherealmofrobotmanipula- conference on computer vision (ECCV), pp. 720–736,
tion. Giventhatrobotsoperateinthephysicalworld, the 2018.
potentialforcollisionswithobjectsandhumansariseswhen
therobotsystemisnotadequatelyconfigured. Tomitigate Dass, S., Yapeter, J., Zhang, J., Zhang, J., Pertsch,
thisissue,ourapproachinvolvesinitialtraininginasimula- K., Nikolaidis, S., and Lim, J. J. Clvr jaco
torenvironmentfollowedbyreal-worlddeploymentunder play dataset, 2023. URL https://github.com/
humansupervision,tominimizeanyadverseimpacts. clvrai/clvr_jaco_play_dataset.
Deitke,M.,Schwenk,D.,Salvador,J.,Weihs,L.,Michel,
References
O.,VanderBilt,E.,Schmidt,L.,Ehsani,K.,Kembhavi,
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., A.,andFarhadi,A. Objaverse: Auniverseofannotated
Hasson,Y.,Lenc,K.,Mensch,A.,Millican,K.,Reynolds, 3dobjects,2022.
M.,etal.Flamingo:avisuallanguagemodelforfew-shot
learning. Advances in Neural Information Processing Dong,R.,Han,C.,Peng,Y.,Qi,Z.,Ge,Z.,Yang,J.,Zhao,
Systems,35:23716–23736,2022. L.,Sun,J.,Zhou,H.,Wei,H.,Kong,X.,Zhang,X.,Ma,
K.,andYi,L.Dreamllm:Synergisticmultimodalcompre-
Bhat,S.F.,Birkl,R.,Wofk,D.,Wonka,P.,andMu¨ller,M. hensionandcreation. arXivpreprintarXiv:2309.11499,
Zoedepth: Zero-shottransferbycombiningrelativeand 2023.
metricdepth. arXivpreprintarXiv:2302.12288,2023.
Driess,D.,Xia,F.,Sajjadi,M.S.,Lynch,C.,Chowdhery,
Black,K.,Nakamoto,M.,Atreya,P.,Walke,H.,Finn,C., A.,Ichter,B.,Wahid,A.,Tompson,J.,Vuong,Q.,Yu,T.,
Kumar,A.,andLevine,S.Zero-shotroboticmanipulation etal. Palm-e: Anembodiedmultimodallanguagemodel.
withpretrainedimage-editingdiffusionmodels,2023. arXivpreprintarXiv:2303.03378,2023a.
Brohan,A.,Brown,N.,Carbajal,J.,Chebotar,Y.,Dabis,J., Driess,D.,Xia,F.,Sajjadi,M.S.M.,Lynch,C.,Chowdhery,
Finn,C.,Gopalakrishnan,K.,Hausman,K.,Herzog,A., A.,Ichter,B.,Wahid,A.,Tompson,J.,Vuong,Q.,Yu,T.,
Hsu,J.,etal. Rt-1: Roboticstransformerforreal-world Huang,W.,Chebotar,Y.,Sermanet,P.,Duckworth,D.,
controlatscale. arXivpreprintarXiv:2212.06817,2022. Levine,S.,Vanhoucke,V.,Hausman,K.,Toussaint,M.,
Greff,K.,Zeng,A.,Mordatch,I.,andFlorence,P.Palm-e:
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, Anembodiedmultimodallanguagemodel,2023b.
X., Choromanski, K., Ding, T., Driess, D., Dubey, A.,
Finn, C., et al. Rt-2: Vision-language-action models Esser,P.,Chiu,J.,Atighehchian,P.,Granskog,J.,andGer-
transferwebknowledgetoroboticcontrol. arXivpreprint manidis,A. Structureandcontent-guidedvideosynthesis
arXiv:2307.15818,2023. withdiffusionmodels,2023.
93D-VLA:A3DVision-Language-ActionGenerativeWorldModel
Fang, H.-S., Fang, H., Tang, Z., Liu, J., Wang, J., Zhu, Kingma,D.P.andWelling,M. Auto-encodingvariational
H.,andLu,C. Rh20t: Aroboticdatasetforlearningdi- bayes. arXivpreprintarXiv:1312.6114,2013.
verseskillsinone-shot.arXivpreprintarXiv:2307.00595,
Li,J.,Li,D.,Xiong,C.,andHoi,S. Blip: Bootstrapping
2023.
language-imagepre-trainingforunifiedvision-language
Feng,Y.,Hansen,N.,Xiong,Z.,Rajagopalan,C.,andWang, understandingandgeneration. InInternationalConfer-
X. Finetuning offline world models in the real world. ence on Machine Learning, pp. 12888–12900. PMLR,
arXivpreprintarXiv:2310.16029,2023. 2022.
Guo,Z.,Zhang,R.,Zhu,X.,Tang,Y.,Ma,X.,Han,J.,Chen, Li, J., Chen, D., Hong, Y., Chen, Z., Chen, P., Shen, Y.,
K.,Gao,P.,Li,X.,Li,H.,andHeng,P.-A. Point-bind& andGan,C. Covlm: Composingvisualentitiesandre-
point-llm: Aligningpointcloudwithmulti-modalityfor lationshipsinlargelanguagemodelsviacommunicative
3dunderstanding,generation,andinstructionfollowing, decoding. arXivpreprintarXiv:2311.03354,2023a.
2023.
Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Boot-
Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen,
strapping language-image pre-training with frozen im-
Z., and Gan, C. 3d-llm: Injecting the 3d world into
ageencodersandlargelanguagemodels. arXivpreprint
largelanguagemodels. arXivpreprintarXiv:2307.12981,
arXiv:2301.12597,2023b.
2023.
Li,Z.,Zhang,C.,Wang,X.,Ren,R.,Xu,Y.,Ma,R.,and
Hong, Y., Zheng, Z., Chen, P., Wang, Y., Li, J., andGan,
Liu, X. 3dmit: 3d multi-modal instruction tuning for
C. Multiply: A multisensory object-centric embod-
sceneunderstanding,2024.
ied large language model in 3d world. arXiv preprint
arXiv:2401.08577,2024. Liu, H., Li, C., Wu, Q., andLee, Y.J. Visualinstruction
tuning. arXivpreprintarXiv:2304.08485,2023.
Honnibal,M.andMontani,I. spaCy2: Naturallanguage
understanding with Bloom embeddings, convolutional Liu,Y.,Liu,Y.,Jiang,C.,Lyu,K.,Wan,W.,Shen,H.,Liang,
neural networks and incremental parsing. To appear, B.,Fu,Z.,Wang,H.,andYi,L. Hoi4d: A4degocentric
2017. datasetforcategory-levelhuman-objectinteraction. In
ProceedingsoftheIEEE/CVFConferenceonComputer
Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,
VisionandPatternRecognition,pp.21013–21022,2022.
S.,Wang,L.,andChen,W. Lora:Low-rankadaptationof
largelanguagemodels. arXivpreprintarXiv:2106.09685, Lu,J.,Clark,C.,Zellers,R.,Mottaghi,R.,andKembhavi,A.
2021. UNIFIED-IO:Aunifiedmodelforvision,language,and
multi-modaltasks. InTheEleventhInternationalConfer-
Huang,H.,Wang,Z.,Huang,R.,Liu,L.,Cheng,X.,Zhao,
enceonLearningRepresentations,2023. URLhttps:
Y.,Jin,T.,andZhao,Z. Chat-3dv2: Bridging3dscene
//openreview.net/forum?id=E01k9048soZ.
andlargelanguagemodelswithobjectidentifiers,2023a.
Lynch, C. and Sermanet, P. Language conditioned imi-
Huang,J.,Yong,S.,Ma,X.,Linghu,X.,Li,P.,Wang,Y.,Li,
tation learning over unstructured data. arXiv preprint
Q.,Zhu,S.-C.,Jia,B.,andHuang,S.Anembodiedgener-
arXiv:2005.07648,2020.
alistagentin3dworld. arXivpreprintarXiv:2311.12871,
2023b.
Lynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J.,
Huang,S.,Dong,L.,Wang,W.,Hao,Y.,Singhal,S.,Ma, Baruch,R.,Armstrong,T.,andFlorence,P. Interactive
S.,Lv,T.,Cui,L.,Mohammed,O.K.,Liu,Q.,etal. Lan- language: Talkingtorobotsinrealtime. IEEERobotics
guageisnotallyouneed: Aligningperceptionwithlan- andAutomationLetters,2023.
guagemodels. arXivpreprintarXiv:2302.14045,2023c.
Mandlekar,A.,Booher,J.,Spero,M.,Tung,A.,Gupta,A.,
James,S.,Ma,Z.,Arrojo,D.R.,andDavison,A.J.Rlbench: Zhu,Y.,Garg,A.,Savarese,S.,andFei-Fei,L. Scaling
Therobotlearningbenchmark&learningenvironment. robot supervision to hundreds of hours with roboturk:
IEEERoboticsandAutomationLetters,5(2):3019–3026, Roboticmanipulationdatasetthroughhumanreasoning
2020. anddexterity. In2019IEEE/RSJInternationalConfer-
enceonIntelligentRobotsandSystems(IROS),pp.1048–
Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F.,
1055.IEEE,2019.
Lynch, C., Levine, S., and Finn, C. Bc-z: Zero-shot
task generalization with robotic imitation learning. In Marr, D. Vision: A Computational Investigation into
Conference on Robot Learning, pp. 991–1002. PMLR, the Human Representation and Processing of Visual
2022. Information. The MIT Press, 07 2010. ISBN
103D-VLA:A3DVision-Language-ActionGenerativeWorldModel
9780262514620. doi: 10.7551/mitpress/9780262514620. Sawhney,A.,Lee,S.,Zhang,K.,Veloso,M.,andKroemer,
001.0001. URL https://doi.org/10.7551/ O. Playing with food: Learning food item representa-
mitpress/9780262514620.001.0001. tions through interactive exploration. In Experimental
Robotics: The17thInternationalSymposium,pp.309–
Mees,O.,Hermann,L.,Rosete-Beas,E.,andBurgard,W.
322.Springer,2021.
Calvin: A benchmark for language-conditioned policy
learningforlong-horizonrobotmanipulationtasks. IEEE Sermanet, P., Ding, T., Zhao, J., Xia, F., Dwibedi, D.,
RoboticsandAutomationLetters(RA-L),7(3):7327–7334, Gopalakrishnan,K.,Chan,C.,Dulac-Arnold,G.,Maddi-
2022. neni,S.,Joshi,N.J.,Florence,P.,Han,W.,Baruch,R.,
Lu,Y.,Mirchandani,S.,Xu,P.,Sanketi,P.,Hausman,K.,
Mees,O.,Borja-Diaz,J.,andBurgard,W. Groundinglan-
Shafran,I.,Ichter,B.,andCao,Y. Robovqa: Multimodal
guagewithvisualaffordancesoverunstructureddata. In
long-horizon reasoning for robotics. In arXiv preprint
Proceedings of the IEEE International Conference on
arXiv:2311.00899,2023.
RoboticsandAutomation(ICRA),London,UK,2023.
Shafiullah,N.M.M.,Rai,A.,Etukuru,H.,Liu,Y.,Misra,
Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., and Chen,
I.,Chintala,S.,andPinto,L. Onbringingrobotshome.
M. Point-e: A system for generating 3d point clouds
arXivpreprintarXiv:2311.16098,2023.
fromcomplexprompts.arXivpreprintarXiv:2212.08751,
2022.
Shah, R., Mart´ın-Mart´ın, R., and Zhu, Y. MUTEX:
Learning unified policies from multimodal task speci-
Padalkar,A.,Pooley,A.,Jain,A.,Bewley,A.,Herzog,A.,
fications. In7thAnnualConferenceonRobotLearning,
Irpan,A.,Khazatsky,A.,Rai,A.,Singh,A.,Brohan,A.,
2023. URLhttps://openreview.net/forum?
etal. Openx-embodiment: Roboticlearningdatasetsand
id=PwqiqaaEzJ.
rt-xmodels. arXivpreprintarXiv:2310.08864,2023.
Palmer,S. Theeffectsofcontextualscenesontheidentifi- Silva,A.,Moorman,N.,Silva,W.,Zaidi,Z.,Gopalan,N.,
cationofobjects. Memory&Cognition,3:519–526,01 andGombolay,M.Lancon-learn:Learningwithlanguage
1975. toenablegeneralizationinmulti-taskmanipulation.IEEE
RoboticsandAutomationLetters,7(2):1635–1642,2021.
Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma,
S., and Wei, F. Kosmos-2: Grounding multimodal Teed,Z.andDeng,J. Raft: Recurrentall-pairsfieldtrans-
large language models to the world. arXiv preprint formsforopticalflow. InComputerVision–ECCV2020:
arXiv:2306.14824,2023. 16thEuropeanConference,Glasgow,UK,August23–28,
2020, Proceedings, Part II 16, pp. 402–419. Springer,
Pylyshyn, Z. Seeing and Visualizing: It’s Not What You
2020.
Think. 012003. ISBN9780262316316. doi: 10.7551/
mitpress/6137.001.0001. Walke,H.R.,Black,K.,Zhao,T.Z.,Vuong,Q.,Zheng,C.,
Hansen-Estruch,P.,He,A.W.,Myers,V.,Kim,M.J.,Du,
Qi, Z., Fang, Y., Sun, Z., Wu, X., Wu, T., Wang, J., Lin,
M.,etal. Bridgedatav2: Adatasetforrobotlearningat
D., and Zhao, H. Gpt4point: A unified framework for
scale. InConferenceonRobotLearning,pp.1723–1736.
point-languageunderstandingandgeneration,2023.
PMLR,2023.
Ramakrishnan, S. K., Gokaslan, A., Wijmans, E.,
Wu, S., Fei, H., Qu, L., Ji, W., and Chua, T.-S. Next-
Maksymets,O.,Clegg,A.,Turner,J.,Undersander,E.,
gpt: Any-to-any multimodal llm. arXiv preprint
Galuba,W.,Westbury,A.,Chang,A.X.,Savva,M.,Zhao,
arXiv:2309.05519,2023.
Y.,andBatra,D. Habitat-matterport3ddataset(hm3d):
1000large-scale3denvironmentsforembodiedai,2021. Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., and Lin,
D. Pointllm: Empowering large language models to
Ren,T.,Liu,S.,Zeng,A.,Lin,J.,Li,K.,Cao,H.,Chen,J.,
understandpointclouds,2023.
Huang,X.,Chen,Y.,Yan,F.,Zeng,Z.,Zhang,H.,Li,F.,
Yang,J.,Li,H.,Jiang,Q.,andZhang,L. Groundedsam: Zhou,J.,Wang,J.,Ma,B.,Liu,Y.-S.,Huang,T.,andWang,
Assemblingopen-worldmodelsfordiversevisualtasks, X. Uni3d: Exploringunified3drepresentationatscale,
2024. 2023.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.
Ommer,B. High-resolutionimagesynthesiswithlatent
Minigpt-4: Enhancing vision-language understanding
diffusionmodels. InProceedingsoftheIEEE/CVFcon-
with advanced large language models. arXiv preprint
ferenceoncomputervisionandpatternrecognition,pp.
arXiv:2304.10592,2023.
10684–10695,2022.
113D-VLA:A3DVision-Language-ActionGenerativeWorldModel
A.ModelImplementationDetails
WeusepretrainedBLIP-2FlanT5asbackbone. Inthepretrainstage,wetrain3D-VLAsfor30epochson6×32V100s,
andvalidateeveryepoch. Thebatchsizeissetto4oneachnodeduringtraining. Additionally,weapplyalinearwarmupof
thelearningrateduringtheinitial1Ksteps,increasingfrom10−8 to10−5,followedbyacosinedecaywithaminimum
learningrateof10−6. Inthealignmentstage,wetrain3D-VLAsforamaximumofepochsof20on6×64V100s. The
batchsizeissetto2oneachnodefortraining. TheAdamWoptimizerisused,withbeta = 0.9,beta = 0.999,anda
1 2
weightdecayof0.05. WeuseDistributedDataParalleltotrainourmodels.
B.DatasetsDetails
B.1.DetailsonQuestionTemplates
Inthissection,weshowthequestiontemplatesfordatagenerationinTable7. Wedesignedcorrespondingtemplatesforsix
tasks. Wedesignthetemplatesforsixtasks,andwereplacetheINSTRUCTION,OBJECT,LOCATION,andACTIONin
eachtemplatewiththeinformationprocessedfromeachsample.
Tasks Templates
Verification Theinitialsceneis<scene></scene>andthecurrentsceneis<scene></scene>.
Instruction:INSTRUCTION.Finished?Answer:[yes/no]
TaskCaption Theinitialsceneis<scene></scene>andthefinalsceneis<scene></scene>.
Describethetask.Answer:INSTRUCTION.
Localization Thesceneis<scene></scene>.Locate:OBJECT.Answer:LOCATION
DenseCaption Thesceneis<scene></scene>.WhatislocatedatLOCATION?Answer:OBJECT
ImageorPoint Theinitialsceneis<scene></scene>.Instruction:INSTRUCTION.
CloudGeneration Generatethegoalimage(orpointcloud).Answer:<image>(<pcd>)INSTRUCTION</image>(</pcd>)
ActionPrediction <scene></scene>.INSTRUCTION.Predict{key/dense}actions.Answer:ACTION.
Table7. DetailedonQuestionTemplates.
B.2.DetailsonChatGPT-basedPrompting
Inthissection,weshowthepromptusedinChatGPT-baseddatagenerationinFigure4. TheChatGPTversionusedin
ourpaperisGPT-3.5-turbo-0125. Wegeneratedataforallseventasks, andweprovidealltheinformationintheform
oftext,suchastheinstructionsperformedbytherobot,totalexecutiontime,objectsandtheirlocationsinthescene,etc.
Additionally,foreachprompt,weprovidetwomanuallywrittensamplesasguidancetodirectChatGPTtowardsmore
naturaldatageneration.
B.3.DetailsonDatasetConstruction
WeshowthenumberoftheepisodesandhowweusethemintableTable8. Weutilizetwomaincategoriesofdatasets,
namelyroboticsdatasetsandhumanobjectinteraction(HOI)datasets. Fortheformer,wefilteredoutcomplexscenedatasets
topreventtheGrounded-SAMfromdetectingincorrectobjectlocations. However,withinthesameroboticsdataset,the
backgroundsettingsarelargelythesame. Therefore,intheGoalGenerationtasks,weincludedHOIdatasetstobetterallow
thediffusionmodeltolearndiversescenes,objectinteractionmethods,etc.
C.MoreVisualizationResultsaboutGoalGeneration
WeshowmorequalitativeexamplesinFigure5,6.
123D-VLA:A3DVision-Language-ActionGenerativeWorldModel
13
gnikaMnoisiceD
noitareneGlaoG
noitpecrePdnagninosaeR
noitpaCksaT
AQdeidobmE
sedosipEdesUfo#
tesataD
noitciderPnoitcA
duolCtnioP
htpeD
egamI
noitceteD
noitacfiireV
noitpaCesneD
)gnidnuorGtcejbO/w(
AQfi-tahW
✓
✓
✓
✓
✓
✓
✓
✓
✓
k503
stesataDscitoboR
✓
✓
✓
✓
✓
✓
✓
✓
✓
k04
Z-CB
✓
✓
✓
✓
✓
✓
✓
✓
✓
k52
egdirB
✓
✓
✓
✓
✓
✓
-
-
-
k01
NIVLAC
✓
✓
✓
✓
-
✓
-
✓
✓
k02
E-bboD
✓
✓
✓
✓
✓
✓
✓
✓
✓
k07
latcarF
✓
✓
✓
✓
✓
✓
✓
✓
✓
k9.0
yalPocaJ
✓
✓
✓
✓
-
-
-
✓
✓
k31
elbaTgnaL
✓
✓
✓
✓
✓
✓
✓
✓
✓
k5.1
xetuM
✓
✓
✓
✓
✓
✓
✓
✓
✓
k3.1
ecalP&kciP
✓
✓
✓
✓
✓
✓
✓
✓
✓
k5.0
noisuFyalP
✓
✓
✓
✓
-
✓
-
✓
✓
k2.4
dooFgniyalP
✓
✓
✓
✓
✓
✓
✓
✓
✓
k0.2
T02HR
✓
✓
✓
✓
✓
✓
-
-
-
k05
hcneBLR
✓
✓
✓
✓
-
✓
-
-
-
k0.2
krutoboR
-
-
-
-
-
-
-
-
✓
k16
AQVoboR
✓
✓
✓
✓
✓
✓
✓
✓
✓
k2.3
yalPocaT
-
✓
✓
✓
-
-
-
-
-
k11
stesataDIOH
-
✓
✓
✓
-
-
-
-
-
k6
nehctiKcipE
-
✓
✓
✓
-
-
-
-
-
k5
D4IOH
✓
✓
✓
✓
✓
✓
✓
✓
✓
k613
stesataDllA
.stesatadmooRdna,IOH,scitoboR:seirogetacruofotnimehtezirogetaceW.repapruonidesustesataD
.8elbaT3D-VLA:A3DVision-Language-ActionGenerativeWorldModel
messages=[{“role”: “system” , “content”: “
You are an AI visual assistant and a question-answeringgenerator capable of analyzing dynamic 3D scenes.
Suppose you have observed a robotic arm successfully executing an instruction: [instruction].
The scene's initial state is <initial scene> and <final scene>, where the final scene is the [num frame] frame, and we assumethat the task was
definitely not completed in the first 2/3 of the time.
You have the action sequence <action> of the robot arm.
In this instruction, the initial positions of these objects are [object + location]. Note that the location is the center pointsof objects
represented by a 3D coordinate (x, y, z) with units of meters.
Utilizing all the information above, you can choose to rewrite the instruction while retaining its original meaning.
Further, you need to generate multiple rounds of dialogue or a question answer pair, which should correspond to one of the following tasks:
1. Verification: Given the initial state and a mid-state frame, ask if the robot has completed the instruction.
2. Task Caption: Given the initial and final states, ask what task the robot performed.
3. Embodied QA: Please conduct some questions and answers about the current dynamic scene.
4. Localization: Detect where objects are, answer the location of the objects.
5. Dense Caption: Given the location of objects, answer with a description of those objects.
6. Image or Point Cloud Generation: Given the initial scene and instruction, generate an image or point cloud of the final state.
If choosing this task, enclose the instruction with the <image> </image> or <pcd> </pcd> token to represent generation.
7. Action Prediction: Given the initial scene, or having both initial and final scenes, predict actions. You can include a simple
task decomposition, but the length of the decomposition must not exceed 3.
”}]
Figure4. PromptforChatGPT-baseddatageneration.
Input GT Goal Pred Goal Input GT Goal Pred Goal
RT-1
JacoPlay
Open middle drawer (RT-1) Pick up the apple fruit (JacoPlay)
Input Pred Goal Input Pred Goal
Novel Env
Pick the coke can Pickupchipbag
Figure5.VisualizationofgeneratedRGBDgoalimages.Theresultsinthefirstrowaresampledfromthetestsetofheld-intrainingdata
whilethesecondrowaretheunseenenvironmentsgatheredfromdailylife.
143D-VLA:A3DVision-Language-ActionGenerativeWorldModel
Input RGBD Pred Goal Input PCD PredPCD
Pick up the red cup
Grasping the umbrella by its handle, lift it up and out of the stand
Figure6. VisualizationofgeneratedRGB-Dgoalimagesandgoalpointcloud.(RLBench)
15