Large Language Models and Causal Inference in Collaboration: A
Comprehensive Survey
Xiaoyu Liu1*, Paiheng Xu1*, JundaWu2, Jiaxin Yuan1, Yifan Yang1, YuhangZhou1, FuxiaoLiu1, Tianrui Guan1,
Haoliang Wang3, Tong Yu3, Julian McAuley2, WeiAi1, and Furong Huang1
1 University of Maryland, College Park
2 University of California San Diego
3 Adobe Research
* denotes equal contribution
Abstract
Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and
explainability of Natural Language Processing (NLP) models by capturing causal relationships among
variables. The emergence of generative Large Language Models (LLMs) has significantly impacted var-
ious NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on
evaluating and improving LLMs from a causal view in the following areas: understanding and improv-
ing the LLMs’ reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs
with explanations, and handling multimodality. Meanwhile, LLMs’ strong reasoning capacities can in
turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect
estimations. This review explores the interplay between causal inference frameworks and LLMs from
both perspectives, emphasizing their collective potential to further the development of more advanced
and equitable artificial intelligence systems.
1 Introduction
RecentlyLargeLanguageModels(LLMs)haveshowcasedremarkableversatilityacrossaspectrumofcritical
tasks. An LLM is adept at tasks such as copywriting, enhancing original sentences with their distinct
styleandvoice,respondingtoknowledgebasequeries,generatingcode,solvingmathematicalproblems,and
performingclassificationorgenerationtaskstailoredtouserrequirements. Moreover,therehasbeenarecent
expansionintomulti-modalvariants,suchasLargeVisualLanguageModels(LVLMs)orLargeMulti-modal
Language Models, which broaden their input/output capabilities to encompass various modalities. This
evolution has significantly enhanced both the potential and range of applications of these models.
In this survey, our primary focus is on Transformer-based Large Language Models (LLMs). The ca-
pability of LLMs is fundamentally rooted in their inference abilities, which dictates their proficiency in
comprehending, processing, and providing solutions to various inquiries, as well as their adaptability to so-
cietally impactful domains. Consequently, extensive research efforts have been dedicated to measuring and
enhancing these capabilities, ranging from assessing the reasoning abilities of LLMs to scrutinizing their
decision-making processes and addressing challenges such as concept alignment across different modalities
and mitigating hallucination. In addition, since LLMs are trained on extensive human knowledge with bil-
lions of parameters, they sometimes face challenges in appropriately prioritizing or downplaying what they
have learned in different scenarios. This can lead to issues such as domain shift, where the model’s perfor-
mance degrades on data that differ from the training set, and long-tail bias, where infrequent examples are
not handled as effectively.
Inmanyinstances,languagetasksrequirenotonlypredictingorgeneratingtextbasedonpatternsinthe
data but also understanding the underlying causalmechanisms driving these patterns. Causalinference has
shown great potential in improving predictive accuracy, fairness, robustness, and explainability of Natural
Language Processing (NLP) models [23]. With the advent of generative LLMs, a significant transformation
1
4202
raM
41
]LC.sc[
1v60690.3042:viXraModelUnderstanding [109,79,43,47,1,88,116]
ReasoningCapacity CommonsenseReasoning [27,98,114,123,62,17]
CounterfactualReasoning [11,70,50,60]
FairnessandBias [68,2,82,22,124,95,65,37]
CausalforLLMs Safety [15,9,122,118,69,56]
Explanation [35,10,33,119,55]
Multi-modality [74,45,49,85,86,121]
Benchmark [103,115,36,107,41,72,40,85,49,18,27]
TreatmentEffectEstimation [19,24,113]
LLMsforCausal
CausalRelationshipsDiscovery [44,71,4,5,61,92,7,91,8,7]
Figure 1: Structure Overview
has occurred across various NLP fields, attracting increased research interest in applying causal inference
to address LLM-related challenges and augment their functionality. Such transformation also motivates
this survey to outline causal methodologies and their implementation in LLMs, emphasizing their role in
enriching our comprehension and application of language models.
Meanwhile, this survey also aims to explore how LLMs can help with the causal inference framework.
Causalinferenceisformallydefinedasanintellectualdisciplinethatconsiderstheassumptions,studydesigns,
and estimation strategies that allow researchers to draw causal conclusions based on data [75]. Causal
inference has three main origins: potential outcomes, graphs, and structural equations, each serving unique
purposes[110]. Thepotentialoutcomesframework[75]focusesonestimatingcausaleffectsthroughstatistical
inference and treatment comparisons. Graphical models, meanwhile, excel in mapping out causal pathways
andvisualizingrelationships,withnodesrepresentingvariablesandedgesdenotingdirectionalinfluences. In
this survey, we mainly discuss Pearl’s formulation of causal graphs [76], which formalized causal graphical
models for presenting conditional independence relations among random variables using directed acyclic
graphs (DAGs).
We summarize how LLMs can help causal inference in its two important components, i.e., causal rela-
tionship discovery and treatment effect estimation. Determining causal relationships among variables is a
fundamentalstepinacausalinferenceframeworkbecausetheestimationofthecausaleffectofvariableAon
variableBrequirescausalassumptionsoncausalrelationshipsofothervariableswithAandB. Traditionally,
researchers rely on experts with subject matter knowledge to set the ground for such causal relationships.
Causaldiscoverymethods [108]provideanalternativeby discoveringcausalgraphsfromobservationaldata.
LLMs have demonstrated abilities to determine such causal relationships based on pre-trained knowledge
or given text. They can also be integrated with causal discovery methods to further increase the reliability
of the outcome. Estimating treatment effects is central to causal inference but is hindered by the absence
of counterfactual data in many cases. Leveraging the strong counterfactual reasoning abilities of LLMs,
researchers have developed various ways to generate high-quality counterfactuals to enable treatment effect
estimation.
The structure of the survey is given in Figure 1. We start with an introduction to recent advancements
in Large Language Models in Section 2. Then we provide an overview of causal inference methods that
are used and can be used to improve LLMs in Section 3. In the first half of the paper we go through
how these methods are used in various problems in the LLM community: Section 4.1 overviews how causal
methods are used to measure and improve the reasoning capacityof LLM, Section 4.2 and Section 4.3 focus
on the fairness and safety issues while Section 4.4 introduces how the explainability of LLM are approached
2
)sMLL(sledoMegaugnaLegraLdnalasuaCby causal inference methods. We also discuss the extension to construction and development of multi-
modality large models in Section 4.5. Finally, we list existing work on evaluation and benchmarking LLMs
from a causal perspective in Section 4.6. In the second half of the survey, we shift to how LLM extends
the boundary of causal inference. Section 5.1 explains current assumptions, limitations and bottlenecks of
causal inference. Section 5.3 and Section 5.2 states current works on improving treatment effect estimation
and causal discoveries. We highlight several future directions in Section 6.
2 Background of Large Language Models
Large Language Models (LLMs) have transformed the way we interact with and process language, opening
up new possibilities for naturallanguageunderstanding, generation,and communication[34]. These models
are in a constant state of evolution, continually expanding the limits of what is achievable in language
processingandartificialintelligence[73]. Inthis paper,wemainly focusedonTransformer-basedLLMs,and
we provide an overview of their recent progress in this session.
The majorbreakthroughof LargeLanguageModels (LLM)came in 2017when the Transformer[93]was
introduced. In a groundbreaking shift, the Transformer technology emerged, mastering the art of grasping
long-term linguistic connections. This innovation wasn’t just theoretical—it allowedfor simultaneous train-
ing on multiple GPUs, paving the way for the creation of significantly larger models [16]. Then came a
pivotal moment in 2018: the unveiling of OpenAI’s GPT-1 [78]. This was more than just another step in
naturallanguageprocessing(NLP);itwasaleap,thankstoitstransformer-basedarchitecture. Boasting117
million parameters, GPT-1 wasn’t merely crunching data; it was weaving contextually coherent sentences,
showcasing the transformative power of transformers in redefining NLP tasks [3]. Despite its initial limita-
tions, GPT-1didn’t just make a mark;it laidthe groundworkfora new waveof AI explorationand sparked
intense competition in the realm of LLMs.
In 2020, OpenAI released GPT-3 [20], which was able to generate highly coherent and natural-sounding
text [13]. It was a big deal because it showed just how awesome these huge language models could be at a
bunch of different language tasks [26]. Riding on the high from GPT-3’s success, OpenAI released the next
iteration of their language model, GPT-4 [2, 67, 52] with the ability to generate even more coherent and
natural-sounding text. Following GPT-4’s success, Google wasn’t far behind with their Bard [66]. Amazon
jazzed up Alexa [42] with some cool AI features, Huawei jumped in with their Pangu models [111] and
Alibaba proposed their QWEN models [6]. Then Meta brought out this thing called LLaMA [90], which
was all about the first open-source foundation models. Compared to LLaMA, LLaMA2 [90] has made more
explorations in reinforcement learning from human feedback (RLHF) and developed a chat-orientedversion
called LLaMA-chat, which generally outperforms existing open-source models across a range of helpfulness
and safety benchmarks. A large number of researchers have extended LLaMA models by either instruction
tuning[57]orcontinualpretraining. Alpaca[89]isthefirstopeninstruct-followingmodelfine-tunedbasedon
LLaMA.Inaddition,Vicuna[77]isanotherpopularLLaMAvariant,traineduponuser-sharedconversations
collected from ShareGPT [31].
As diverse versions of LLMs emerge,they encounter common challenges. In this survey, we demonstrate
that many of these challenges can be effectively addressed through causal methods. These methods encom-
pass enhancing the reasoning capacity of LLMs, tackling fairness concerns and mitigating potential biases,
ensuring safety, and enhancing the explainability of model outputs, and their extension to multi-modality
versions.
Building upon this progress, there is now a burgeoning interest in expanding the scope of these models
to encompass visual data, giving rise to the emergence of Large Visual Language Models (LVLMs). These
models aim to integrate the understanding of both textual and visual information, opening up new avenues
for more comprehensive AI systems capable of interpreting and generating content in multimodal formats.
One of the most prevalent approaches is inserting visual features as supplementary inputs to LLMs and
aligning them with textual features. This method has been adapted in several large vision-languagemodels
(LVLMs) such as MiniGPT-4 [127], LLaVA [59], Mplug-Owl [106] and so on [55, 57, 117, 96, 106, 58, 51].
In this survey, we also demonstrate that the causal methods could help concur challenges encountered in
existing LVLMs as above.
33 Brief Introduction of Causal Inference
In this section, we present the background knowledge of causal inference, including task descriptions, basic
concepts and notations, and general solutions.
Generally speaking, the task of causal inference is to estimate the causal relationship among variables.
The variables of interest are referred to as treatment, naturally, the effects of treatments are referred to as
treatment effects. For example, suppose two treatments can be applied to patients: Treatment Plan A and
B. When A is applied to a certain patient cohort, the recovery rate is 70% while when B is applied to the
same cohort, the recovery rate is 80%. The change of recovery rate is the effect of that treatment assets on
the recovery rate.
Ideally, the treatment effect can be measured as follows: applying different treatments to the same
cohort, and then the difference in the effect is the treatment effect. However, in real-world scenarios, this
ideal situation because it is impracticable for perfectly controlled experiments in most cases. For example,
in the above case, you can only apply one treatment to the same cohort at the same time. In reality, an
alternative is to conduct random controlled trials, in which the treatment assignment is controlled, such
as a completely random assignment. In this way, the groups receiving different treatments can be used to
measurethe differenceineffect. Unfortunately,evenperformingrandomizedexperimentsisexpensive,time-
consuming, and may cause ethical concerns in some cases. Therefore, estimating the treatment effect from
observational data has attracted growing attention due to the wide availability of observational data, and
methods are developed for the investigation of the causal effect of a certain treatment without performing
randomized experiments.
3.1 Potential Outcome Framework
One of the most influential frameworks in identifying and quantifying causal effects in observationaldata is
the potential outcomes framework[80]. The potential outcomes approachassociates causality with manipu-
lation applied to units, and compares causal effects of different treatments via their correspondingpotential
outcomes. Following [80], we state basic concepts in the potential outcome framework.
Unit. A unit is the atomic research object in the treatment effect study. A unit can be a physical object,
a firm, a patient, a person, or a collection of objects or persons, such as a classroom or a market, at a
particular time point [80]. Under the potential outcome framework,the atomic researchobjects at different
time points are different units.
Treatment. Treatment refers to the action that applies (exposes, or subjects) to a unit. For each unit-
treatment pair, the outcome of that treatment when applied to that unit is the potential outcome.With
N treatments T = {1,2,3,...,N}, the potential outcome of applying treatment T is denoted as Y(T =T ).
i i
Theobserved outcomeisthe outcomeofthe treatmentthatisactuallyapplied. Andthecounterfactual
outcome is the outcome if the unit had taken another treatment.
Treatment Effect The treatment effect can be quantitatively defined using the above definitions. The
treatment effect can be measured at the population, treated group, subgroup, and individual levels. At the
populationlevel, the treatment effectis estimatedas the AverageTreatmentEffect (ATE). Atthe subgroup
level, the treatment effect is called the Conditional Average Treatment Effect (CATE).
Definition3.1(BinaryAverageTreatmentEffect(ATE)). Supposewewanttomeasurethetreatmenteffect
of a treatment T =1. Then the average treatment effect is defined as:
E[Y(T =1)−Y(T =0)] (1)
where Y(T = 1) and Y(T = 0) denote the potential treated and control outcome of the whole population
respectively.
Definition 3.2 (Conditional Average Treatment Effect (CATE)).
E[Y(T =1)|X =x]−E[Y(T =0)|X =x] (2)
whereE[Y(T =1)|X =x],E[Y(T =0)|X =x]arethepotentialtreatedandcontroloutcomeofthesubgroup
with X =x.
At the individual level, the treatment effect is defined as Individual Treatment Effect (ITE). In some
literature, ITE is treated as the same as CATE [75].
43.2 Causal Graphical Models
The potential outcome framework is powerful in recovering the effect of causes. In a potential outcome
framework, causal effects are answered by specific manipulation of treatments. However, when it comes to
identifyingthecausalpathwayorvisualizingcausalnetworks,thepotentialoutcomemodelhasitslimitations.
Inthefrontofthechallenge,causalgraphicalmodelsutilizedirectededgestorepresentcausalitiesandencode
conditional independence among variables in the graphs.
3.2.1 Structural Equation Models (SEMs)
Oneofthemostwidely-spreadformulationsistheStructuralEquationModel[99,76],wherelinearstructural
equation models are used to present causal relationships by directed edges, which differentiate correlation
from causation when the graph structure is given. The linearity assumption was later been relaxed by [76]
and it formalized causal graphical models for presenting causal relations using Directed Acyclic Graphs
(DAGs).
Specifically, consider the random variable X ∈ RD×N = [X ,X ,...,X ], the linear SEM consists of a
1 2 N
set of equations of the form:
X i =β 0i+ X β jiX j +ǫ i, i=1,2,3,...,N (3)
j∈pa(Xi)
wherepa(X )denotesthesetofvariablesthataredirectparentsofX . ǫ ,ǫ ,...,ǫ aremutuallyindependent
i i 1 2 N
noise terms with zero mean, β are coefficients that quantify the causal effect of X on X .
ji j i
While the non-parametric SEM takes the form:
X =f (X ,ǫ ), i=1,2,3,...,N (4)
i i pa(i) i
The random variables X that satisfies the model structure of the form in Equation (3) or Equation (4)
can be represented by a directed acyclic graph (DAG) G=(V,E), where V is the set of associatedvertices,
each corresponding to one of a variable of interest X , and E is the corresponding edge set.
i
With pre-specified DAG and assumptions on the latent variables, the coefficients between the latent
variables are identifiable[46].
3.2.2 Bayesian Network
Causal inference can be naturally embedded in graphical model frameworks since the dependencies and
interactions between variables can be presented by graphs with probabilistic distributions, in which nodes
correspond to variables of interest and edges represent associations. One general solution except for SEMs
is to use a BayesianNetwork to represent the causal relationship.
In Bayesian networks, causalities among variables are represented in the form of DAGs with directed
edges carrying causal information.
A joint probability distribution P factorizes with respect to a DAG G if it satisfies:
f(X 1,X 2,...,X N)=Yf(X i|X pa(i)) (5)
i
In the next section, we show a comprehensive survey of how existing works help with the tasks and
challenges in LLMs in detail.
4 Causal Inference for Large Language Models
LLMs can significantly benefit from causal inference as it enhances their ability to understand and reason
about cause-and-effect relationships within data. In this section, we review how LLMs can benefit from
a causal view in the following perspectives, understanding and improving the LLMs’ reasoning capacity
(Section4.1),addressingfairness(Section4.2)andsafety(Section4.3)issuesinLLMs,complementingLLMs
with explanations (Section 4.4), and handling multimodality (Section 4.5). We then organize benchmark
datasets from these perspectives in Section 4.6.
54.1 Reasoning Capacity
4.1.1 Model Understanding
LLMs have demonstrated many emerging abilities in language generation and certain reasoning tasks [14,
44]. As the reasoning process is often associated with causal factors, it is logical to first understand and
evaluate LLMs’ reasoning ability from a causal lens. Zeˇcevi´c et al. [109] argued LLMs are not causal and
hypothesized that LLMs are simply trained on the data, in which causal knowledge is embedded. Thus,
in the inference stage, the LLMs can directly recite the causal knowledge without understanding the true
causality in the context. Similar behaviors are exhibited in a Causal Reasoning Assessment Benchmark,
CRAB, that consists of 1.8K causal frames and 352 causal chains in real-worldnarratives [79], where LLMs
arerequiredtooutputthecausalityclass(high,medium,low,ornocausality)betweenvariables. Theyshow
that LLMs can capture explicit causal statements in pre-training data, but face performance drop when
applying causalreasoning to new distributions, i.e., events that happened after the pre-trainingphase. Kim
etal. [43]examinedLLMs’abilitiesinunderstandingthecausalitiesofbothscientificpapersandnewspapers.
Their evaluation protocol is designed for the LLM to tell whether a statement is causal, conditional causal,
correlational, or no relationship. The results show that ChatGPT performs worse than a fine-tuned BERT
model in understanding causality. Abdali et al. [1] show the effectiveness of applying LLMs to diagnose
the cause of issues from Microsoft Windows Feedback Hub. Li et al. [47] showed that LLMs can identify
dynamical (spatio-temporal) effects. However, how to infer the relationship and interactions of them is still
challenging for LLMs, which are more emphasized as causal structures in causal inference.
Another important line of work is to understand LLMs’ hallucination and faithfulness in knowledge rea-
soningbyconsideringcausaleffects. Tangetal.[88]proposedamulti-agentsystem,CaCo-CoT,wheresome
LLMs are reasoners and others are evaluators. Reasoners try to provide causal solutions, while evaluators
try to challenge the reasoners with counterfactual candidates. With the cooperative reasoning framework,
CaCo-CoT helps to improve causal-consistency. Zhang et al. [116] identified the potential knowledge bias
pretrained in the LLMs as the confounder which causes incorrect answers and hallucinations. Zhang et
al. [116] proposed a chain-of-question framework to generate sub-questions necessary to answer a question,
and involve humans in the loop to provide the correct answers. With human annotation in the loop, the
confounding causal effect can be reduced, thus mitigating the spurious correlation.
4.1.2 Commonsense Reasoning
Commonsensereasoninginvolvestheabilitytoapplyeverydayknowledgeandintuitiveunderstandingsofthe
worldto makedecisionsordrawconclusions,whichisvitalforLLMs’contextualunderstandingandhuman-
like interactions[21, 84]. This sectionbriefly summarizes the commonsensereasoningability ofLLMs under
various settings [27, 98] and the employment of causally motivated methods in improving commonsense
causality reasoning [114, 123, 62, 17, 120].
ThroughtheevaluationofChatGPT’sperformanceoneventcausalityidentification,causaldiscovery,and
causal explanation generation,Gao et al. [27] have shown that ChatGPT is not a good reasoner but a good
causal explainer. ChatGPT, and even gpt-4 is outperformed by baseline methods based on fine-tuned small
pre-trainedlanguagemodelsoneventcausalityidentificationanddoespoorlyoncausaldiscovery. Theyalso
observeserioushallucinations oncausalreasoningunder In-ContextlearningandChain-of-Thoughtsettings
Asimilarconclusionhasalsobeenobtainedin[98],whichanalyzedthecausalquestionansweringcapabilities
of LLMs. It concluded that LLMs did not arrive at their answers through reasoning but through memo-
rization of the corresponding question and answer pair. Even if ChatGPT may not have causal reasoning
ability, it generates accurate and detailed causal explanations in some cases [27].
To improve the commonsense causality reasoning of LLMs that identify causes from effects in natural
language, ROCK (Reason O(A)bout Commonsense K(C)causality) [114] balances confounding effects using
temporal propensities. The centralquestion in this framework is the estimation of Average treatment effect
(ATE).GiventwoeventssuchthatE precedesE ,thestrengthofcausationfromE toE canbeestimated
1 2 1 2
by the change in the likelihood of E ’s occurrence due to intervening on E , denoted as ATE. While ROCK
2 1
adoptsapotential-outcomeframework,Chenetal. [17]usesaconversationcognitivemodelbasedonintuition
theories, andtransforms intuitive reasoninginto a structuralcausalmodel. Aiming to improve conversation
reasoning, the authors incorporate mental states such as desires, memory, experience and emotion, as an
6unobservable exogenous variable that implicitly affects the corresponding observable utterance:
H =AH +E, (6)
where H is the embedding of the utterance, E is the exogenous variable, and A is the adjacency matrix.
Other than facilitating reasoning ability of LLMs directly, Zheng et al. [123] use causal inference to pre-
servecommonsenseknowledgefromPre-trainedlanguagemodelsfor fine-tuning to preventcatastrophicfor-
getting;Luetal.[62]focusonimprovingLLM’sabilityingeneralizedproceduralplanningwithcommonsense-
infused prompts. In particular, Zheng et al. [123] abstract the fine-tuning process as a causal graph and
discovered that catastrophic forgetting is due to missing causal effects from the pre-trained data. To pre-
serve old knowledge from a pre-trained language model, an objective for fine-tuning (causal effect tuning)
is introduced. For procedural planning tasks, Lu et al. [62] proposed to learn causeeffect relations among
complex goals and stepwise tasks, and reduced spurious correlationamong them via front door adjustment.
4.1.3 Counterfactual Reasoning
Another potential use of LLMs’ generative capacity is to generate counterfactuals for data-augmentations
for small language models. Given a text x and a black-box classifier B, the counterfactual text x˜ of text x
should satisfy the following requirement [11, 70]:
1. x˜ has a different class than x, B(x)6=B(x˜).
2. x and x˜ differ only by minimal lexical changes.
3. x˜ is a feasible text and the commonsense constraint is satisfied.
This section discusses briefly the performance of LLMs in generating counterfactuals [50, 60] and the effort
of improving their qualities [70].
Li et al. in [50] examined the effectiveness of LLMs’ performance in generating counterfactuals under
four tasks: (1) sentiment analysis (SA) that alters sentiment polarity; (2) natural language inference (NLI)
that given a premise sentence and a hypothesis sentence, make a change to the hypothesis sentence to alter
the relationship between it and the premise sentence; (3) named entity recognition(NER) that changes the
entities in a sentence whose type is the same as the original entity type; (4) relation extraction (RE) that
changestherelationbetweentheheadandtailentity. IthasbeendemonstratedthatforsimpletaskslikeSA
andNLI,dataaugmentedviaLLMscanmitigatepotentialspuriousassociations. Formorecomplicatedtasks
like RE, LLMs may generate low-quality counterfactuals. In order to generate high-quality counterfactual,
detailed instructions are crucial. However, counter-intuitively, chain-of-thought doesn’t always help as it
may even lead to significant performance decreases under some settings. On the other hand, Liu et al. in
[60] evaluated abductive reasoning and counterfactual reasoning abilities for large language models of code
(Code-LLMs)andcomparedthemwithtextmodels. Withcode-promptsdesignedtotacklecausalreasoning
tasks, it has been shown in [60] that Code-LLMs achieve better results than text models.
Asshowninthe evaluation[50],thoughLLMsascounterfactualgeneratorscanenhancetheperformance
of small language models on simple tasks such as sentiment analysis and natural language inference, the
generated counterfactuals fail to have any significant effect on complex tasks like relation extraction. Miao
et al. in [70] claim that this is due to the failure of identifying causal terms correctly and ignoring the
commonsense constraint. To amend this, they proposed a framework to generate commonsense counterfac-
tuals for stable relation extraction via an intervention-based strategy. This framework is demonstrated to
have enhanced the stability on relation extraction tasks under various settings including the low-resource,
out-of-domain and adversarial-attackscenarios.
4.2 Fairness and Bias
Fairness and bias are pivotal factors in deploying language models effectively and ethically. Bias is common
in pretrained language models as they capture and potentially amplify undesired social stereotypes and
biases [68, 2, 126, 100, 96]. An example of bias in language models includes gender associations with
specific professions, such as male firefighters and female nurses [82]. Causality-based methodologies offer
7a promising approach for mitigating biases in language models by discerning the origins of bias through
a causal perspective. Bias mitigation is then followed by eliminating the unwanted spurious correlation
between generative factors though different types of causal intervention or causal invariant learning.
Dingetal. [22]introduceaproxyvariablerelatedtogenderbiasinthecausalgraph,andusetwodifferent
ways to eliminate the potential proxy bias and unresolved bias under the linear structural equation model.
Zhou et al. [124] believe that the backdoor path between the ground truth label and the non-causal factors
is the source of bias, and uses the Independent Causal Mechanism (ICM) principle to mitigate bias. Their
proposed method, Causal-Debias, achieves causal intervention and creates interventional distributions with
respect to different demographic groupsby augmenting and expanding the originaldata distribution. Wang
et al. [95] raise concerns over the precision of parameter estimation in existing causal models and introduce
several intermediate variables that are the causal children of raw text and the causal parent of the input to
language models. Under this assumption, they propose to eliminate bias by performing a ‘do’ operation on
theintermediatevariablesforbothwhite-boxLLMsandblack-boxLLMs. Madhavanetal. [65]considerthe
tokensgeneratedbygenerativelanguagemodelstrainedwithcausallanguagemodeling (CLM)objectivesas
a causalgraph, and analyze the bias under this model. Jenny et al. [37] use Activity Dependency Networks
(ADNs) to describe the causality effect between normative variables, such as clarity and authenticity, to
structure the cause of bias. The authors argue that using ADNs can better explain previously simplified
views of bias using just correlation, and display the complexity nature in identifying and mitigating biases
in large language models.
4.3 Safety
Withtheapplicationoflanguagemodelsinmultipledownstreamtasks,researchershaveobservedtheunreli-
abilityphenomenonofLLMsinknowledgeprobing[15]ordownstreaminferencetasks[9,122,118,69]. There
isincreasinginterestinapplyingthecausalinferencetechniquetoanalyzethecausalityofthenon-robustness
of the model and adjust the treatment to resolve the challenges [15, 118, 122, 56].
Knowledgeprobing ToelicittheknowledgeencodedinLLMs,previousworkusesprompt-basedprompt-
ing, that is, querying LMs with task-specific prompts. However, in this process, LLMs face challenges of
unreliability [15, 102], such as using shortcuts to complete the probing and generating different predictions
forthesemanticallyequivalentprompt. Toexplorethereasonbehindthenon-robustness,anempiricalstudy
from [15] constructs a structural causal model (SCM) containing 11 variables and identifies three types of
bias in the knowledge probing procedure: prompt preference bias, instance verbalization bias, and sample
disparity bias. Blocking the corresponding backdoor path for each type of bias effectively eliminates the
bias. This findingrevealsthepotentialofconstructinganSCMwithLLM-relatedvariablestoimproveLLM
robustness.
Downstream Inference Tasks In addition to knowledge probing, LLMs show vulnerability when en-
countering attacks on the prompts in downstream inference tasks [118, 122]. By simply translating text
tokens in input prompts to emojisequences, LLMs generatemore severe hallucinations [122]. Some neurons
withinLLMs alsohaveanunreasonablyhighcausaleffectonthe modelresponseand,by changingthe value
of that neuron, LLMs will produce meaningless responses [122].
The main reason why LLMs fail in prompt attacks is that it uses the spurious correlation to make
an inference [118]. Training LLMs to learn the causal relationship between input x and output y is an
intuitivemethodofbetterresistingpromptattacks. Therandomizedsmoothingtechnique[112,39]canmodel
the interventional distribution p(y|do(x)) by assuming discrete adversarial perturbations as the Gaussian
distribution [118]. The method of smoothing in the latent semantic space is more robust against known
attacks such as word substitutions, paraphrasing,and token position change [118].
Causal inference techniques are useful to induce robustness for LLM applications. For the setting with-
out prompt attacks, mitigating LLM unreliability can be achieved by identifying the causality and blocking
the backdoor path. Moreover, smoothing the latent semantic space is effective in resisting prompt attacks.
Despite the progress of causality analysis, there are some directions to explore, such as the causal relation-
ship between the pretraining corpus and model robustness and the evaluation method of robustness in the
generation task.
84.4 Explainability
Explainability in LLMs refers to the capacity to elucidate how these models arrive at their conclusions,
enhancing transparency and trustworthiness in AI decision-making processes [32, 54]. Many work have
tried to explain and understand the inner workings of LLMs [35, 10, 33, 119, 55]. We summarize research
efforts thatprobe the causalmechanisminLLMs fromthe followingthree directions: interveningthe inputs
or prompts, intervening inner components of LLMs, and abstracting the working mechanism into a causal
graph.
InputsorPromptIntervention Inputintervention,asadata-centricmethod,istocreatecounterfactual
input text by changing the treated feature in the text and to observe the model behavior on the original
and counterfactual texts. Recruiting individuals to produce counterfactual texts typically incurs significant
expenses. However, the advent of LLMs has shown that generating counterfactual inputs can be achieved
withlesscost. LLMscanfirstidentifythefeaturesininputtextscausallyassociatedwiththepredictionsand
arecapableofchangingtheidentifiedfeaturestocreatethecounterfactualtexts[12,28]. Thesecounterfactual
texts can be utilized to investigate the causality of the LLM and can serve as a training dataset to learn a
matching model, where the matched counterfactual pairs have similar embeddings [28].
Various works have developed different prompting methods and found whether the prompting methods
are causally associated with the final output of LLMs [81, 125, 97]. However,the causal effect of prompting
methods, such as chain-of-thought (CoT), and the final output is ambiguous. Prompt intervention, which
altersonlyoneparticularaspectofprompts,is proposedto understandthe contributionsofeachcomponent
of prompts on model behavior [64]. The experiments from [64, 38] first find that the linguistic features
and the grammar have a large effect on the LLM outputs. Then, intervention in intermediate variables in
prompts leads to consistent final answers with the expected output of the hypothesized causal model [87].
These findings suggest that LLMs realize the causal model suggested by their CoTs to a high extent, but
LLMs also utilize spurious correlations such as sentence length to make responses.
Input or prompt interventions are data-centric methods to probe the LLM mechanism, applied in both
open-source or black-box LLMs. However, the detailed information cascade inside the LLMs cannot be
discovered by such methods, so intervention on the inner model components is proposed.
Inner Component Intervention The attention mechanism and multilayer perceptron (MLP) layers
are the essential components in the structure of state-of-the-art (SoTA) LLMs. Existing work exchanges
the activation values in MLP and attention layers of different inputs to probe the function of MLPs and
attention mechanism in generating the answers [83]. The experimental results indicate that LLMs transfer
the information of inputs from midsequence early layers to the final token using the attention mechanism.
Due to the complexity of LLMs, current work focuses only on math word problems with four fundamental
arithmetic operators [83]. It is an interesting direction to generalize the component intervention to other
downstream tasks.
Causal Graph Abstraction An intuitive way to characterize causality within LLMs is to abstract the
working mechanism of LLMs into a causal graph. Boundless Distributed Alignment Search (DAS) [101]
by replacing brute-force searching the original DAS [29] with learnable parameters, has been effective on
the Alpaca model [89]. Given four pre-defined causal models, Boundless DAS extracts two of them as the
accurate hypotheses as the abstracted causal graph of the Alpaca model. However, the Boundless DAS
method is restricted by the given causal hypothesis, and the future direction can explore how to abstract
the causal graph in LLMs without prior causal graphs.
Currentexplainability work from a causalview use LLMs to interpret the causality in real-worldevents.
LLMs can generate high quality counterfactuals. By altering the inner values of LLMs and abstracting the
causality in LLMs, the currentwork has pointed out a direction to characterizethe inner causality of LLMs
on various tasks.
However,withthesementionedworksinLLMinterpretation,the scopeofinterpretationfocusesontasks
thathaveaclearcausalgraphbetweentaskinputsandoutputssuchassimplemathwordproblems. Probing
thecausalityinLLMsformorecomplexgenerationtasks,suchasquestionansweringorsummarizationtasks,
could provide more insight on the inner mechanism of LLMs.
94.5 Multi-modality
Largevision-languagemodelshavebeenpopularinmanyapplications[63,48,59,30]. Howtoconductcausal
reasoning on both images and texts can be crucial in correctly answer multimodal questions. Pawlowski et
al. [74] examined LLMs’ causal reasoning abilities and showed that the causal knowledge in the language
models can be too strong a prior which often causes the model to neglect visual evidence. Ko et al. [45]
proposed to alleviate the problem by adding self-consistent generation prediction, in which the three input
V, Q and A are individually predicted based on the other two inputs. Specifically, Li et al. [49] proposed
a image generation framework with causal reasoning and created a novel VQA datasets whose questions
requires causal explanations.
Another important question is to understand the spatial-temporal causal relationship of the visual el-
ements within the images and videos. Su et al. [85] proposed a framework, CaKE-LM, to leverage the
pretrained causal knowledge in the language model to understand the causal relations of the events in a
video. Basedonthe generatedcausalreasoningresults,CaKE-LMcanfurther generatethe question-answer
pairs and construct a new benchmark for causal video question-answering. Tai et al. [86] proposed the
link-context learning method to strengthen the LLM’s in-context learning abilities by instructing the model
to understand the underlying causal relationship between the demonstration data points. Zhao et al. [121]
suggested that there are two types of causal relationships in VQA. They proposed a prompting method,
causal context generation, to engage contextual information for better VQA precision.
4.6 Evaluation and Benchmark
In this section, we list existing evaluation metrics and benchmarks from a causal perspective for LLMs as
listed in Section 4.6. The causal evaluation mainly focuses on three aspects: the Model Understanding
(MU)ability,the CommonsenseReasoning(CR)ability,theCounterfactualReasoning(CF)ability,andthe
Fairness/Debias (FD) ability.
Reference MU CR CF FD Language Multimodal
ECHo [103] X X X
CREPE [115] X X
CLOMO [36] X X
IfQA [107] X X
Cladder [41] X X X
MoCa [72] X X
CORR2CAUSE[40] X X
CVidQA [85] X X
VQAI[49] X X
Chen et al. [18] X X
Gao et al. [27] X X
CRAB [79] X X
HELM [53] X X
Fair-Prism [25] X X
Biasasker [94] X X
Table1: Wesummarizetheexistingevaluationbenchmarks. Basedontheevaluationtasks,wecategorizethe
benchmarks into three categories: Model Understanding (MU), Commonsense Reasoning (CR), Counter-
factualReasoning(CF)andFairness/Debias(FD).Basedonthemodalitiesofthedatasamples,weidentify
the benchmarks with only textual inputs (Language) and those with multimodal inputs (Multimodal).
The benchmarks in model understanding (MU) focus on evaluating and understanding existing LLMs’
causal reasoning abilities in both natural language [41, 72, 40, 27, 79] and vison-language [85, 49, 103]. In
addition, some benchmarks [72, 27] also provide model understanding in comparison with human causal
reasoning and moral judgments. Commonsense reasoning benchmarks (CR) evaluate LLMs on tasks that
require extensive commonsense knowledge for both textual-only context [115, 41] and multimodal context
[103]. Contextswithcommonsensicalandanti-commonsensicalareconstructedin[41],tofurtherinvestigate
10whether LLMsuse averaged-outcausalreasoning. EvaluatingLLMs’counterfactualreasoning(CF)abilities
is essential in enabling explainable model reasoning and calibration of the generated rationales. Huang et
al. [36] introduce a specific task and benchmark for assessing LLMs’ logical counterfactual thinking. Yu et
al. [107] contribute a novel dataset to challenge LLMs in counterfactual reasoning in an open-domain QA
context. Chen et al. [18] investigate the ability of LLMs to provide explanations that aid in understanding
their reasoning process, particularly in the context of counterfactual scenarios. The fairness and bias (FD)
evaluations are particularly in addressing biases, fairness, and the overall transparency of language models.
HELM[53] is a comprehensiveevaluationbenchmark including previouslyneglectedareasfor fairness. Fair-
Prism [25] focuses specifically on fairness-related harms in models, which are identified and measured by
detailedhumanannotations. Biasasker[94]presentsanautomatedframeworktoidentifyandmeasuresocial
biases by probing the models with specially designed questions.
5 Large Language Model for Causal Inference
Causal inference, serving as a potent tool for addressing challenges in LLMs, heavily relies on world knowl-
edge. As previously mentioned, there are three primary origins of causal inference: the potential outcome
framework, graph-based causal methods, and the structural equations community. The potential outcome
frameworkreliessignificantlyonseveralassumptionstofacilitate thecomparisonoftreatmenteffectsamong
groups/individuals. One of the most challenging aspects of applying the potential outcome framework lies
in ensuring that these assumptions hold true in reality. In this section, we first examine these assumptions
andsubsequentlyillustratehowtheyarerelaxedinexistingliterature. Thegraph-basedcausalmethodsand
structural equation models also necessitate a certain level of understanding of the underlying causal graph.
For instance, Directed Acyclic Graphs (DAGs) serve as a fundamental assumption, and many structural
equation models assume a degree of linearity or that the input distribution adheres to specific probability
distributions. In our review, we also explore how existing methods verify the distribution in the input data
andextendcurrentmethodologiestoaccommodatemorecomplexdistributionswiththeassistanceofLLMs.
5.1 Fundamental Assumptions in Estimating Treatment Effect
In existing causal inference literature, several assumptions are adopted to estimate the treatment effect.
Here we discuss the three most commonly used assumptions and then show how the development of large
language models could help relax or challenge these fundamental assumptions.
Assumption 5.1 (Stable Unit Treatment Value Assumption ). The potential outcomes for any unit
do not vary with the treatment assigned to other units, and, for each unit, there are no different forms or
versions of each treatment level, which leads to different potential outcomes.
This assumption emphasizes the independence of each unit when estimating the treatment effect that
the units do not interact with each other. From a statistical perspective, it is equivalent to assuming each
treatment assignment subject is iid.
Assumption 5.2 (Ignorability/Unconfoundedness ). Given the observable background variable, X,
treatment assignment T is independent of the potential outcomes.
The assumption 5.2 states that if the background variable X is the same for two patients, then (1) the
treatment assignment should be the same. (2) the potential outcome should also be the same. In other
words, these two patients are treated as identical units, thus can be used to estimate the treatment effect
if they are assigned different treatments in the static dataset, as the treatment assignment is treated as
random.
Assumption 5.3 (Positivity ). For any value of X, treatment assignment is not deterministic. i.e.,
P(T =t|X =x)>0,∀t,x (7)
Thisassumptiontriestoguaranteethatthetreatmenteffectcanbeestimatedandthatwecanalwaysfind
a comparable sample. In the binary case, if the task is to estimate the performance of a specific treatment
T = 1, we would need to compare the potential outcome of patients receiving the treatment against those
who are not treated, requiring dataset points in both cases.
115.2 Treatment Effect Estimation
Onemainobstacleoftraditionalcausalmethods isthe lackofcounterfactualdata,makingthe estimationof
causaleffectsadifficultprobleminpractice. Chenetal.[19]proposedanewmethodforautomaticallygener-
ating high-quality counterfactual data at scale called DISCO (DIStilled COunterfactual Data). Specifically,
it prompts to generate of phrasal perturbations with a large general language model. Then, a task-specific
teachermodelfiltersthesegenerationstodistillhigh-qualitycounterfactualdata. Inaddition,Federetal.[24]
apply treatment effect estimation to alignknowledge for generalizationtowardsdifferentdomains. Zhang et
al. [113] try to optimize the treatmenteffect estimation onunlabeled datasets by performing self-supervised
causal learning through LLMs. Through exploring the primal-dual connection between optimal covariate
balancing and self-attention, their method facilitates zero-shot causal inference through the final layer of a
trained transformer-type architecture, contributing to a foundation model for treatment effect estimation.
5.3 Causal Relationships Discovery
Discoveringcausalrelationshipsbetweenvariablesisa fundamentalstepin causalinferenceas itenables the
identificationandestimationofcausaleffects. Inthis section,weintroducepapersdiscussinghowLLMscan
help discover causal relationships.
One line of this work focuses on casual relationship extraction or causality extraction which extracts
causal relationships between two variables from text directly. Traditional methods rely on linguistic cues
such as causal connectives (e.g., “cause”, “because”, and “lead to”) and grammatical patterns to identify
causalpairs[104]. Alaterworkutilizes the powerofstatisticalmachinelearninganddeeplearningto tackle
this taskinasupervisedlearningsetting[105]. AsLLMsshowpromisingpotentialwithreasoningcapacities
as introducedin Section4.1, many worksuse LLMs asa query toolto determine the edge directionbetween
two givenvariables. For example, Kıcımanet al. [44] show that LLMs canachieve competitive performance
indeterminingsuchpairwisecausalrelationshipswithaccuraciesupto97%. Analysesinthemedicaldomain
[71, 4, 5] exhibit similar observations. However, other studies highlight LLMs’ limitations of such pairwise
causal relationships. For example, sensitivity to prompt design leads to inconsistent results [61]; pairwise
judgments can lead to cycles in the full causal graph [92]; pairwise judgments require large computational
cost when applying to a large-scale dataset, N variables would require N prompts [7]; LLMs still provide
(cid:0)2(cid:1)
false information despite achieving strong results in most cases [61, 91]. Long et al. [61] propose strategies
for amending LLMs’ output based on consistency properties in causal inference [61].
To alleviate the impact of erroneous causal information from LLMs, previous works have integrated
LLMs with traditional causal discovery methods. Causal discovery or causal structure learning is the task
of recovering causal graphs from observational data whenever possible [108]. Traditional causal discovery
methodsmainlyincludeconstrained-basedmethodsthatexploitasequenceofstatisticaltestsandscore-based
methodsthatstructurearoundthemaximizationofthefitnessofagraphthroughaspaceofpossiblegraphs.
Vashishtha et al. propose two algorithms that combine LLMs with causal discovery methods: the first uses
causal order from an LLM to orient the undirected edges outputted by a constraint-based algorithm and
the second utilizes the LLM causal order as a prior for a score-basedalgorithm [92]. Ban et al. incorporate
LLM-drivencausalstatementsasqualitativeancestralconstraintsintheBayesiannetworkstructuretoguide
data-driven algorithms [8], which benefits smaller-scale problems, but encounters difficulties with larger
datasets due to inaccuracies in the LLM-derived constraints. They then propose an iterative framework
that employs LLMs to validate the accuracy of edges in the learned causal graph and fine-tune the causal
discovery process based on LLM feedback [7].
6 Future Directions
Theoretical understanding of LLM’s reasoning capacity. Causal inference methods offer a promis-
ing avenue for gaining deeper insights into the reasoning capacity of Large Language Models (LLMs). A
potential approach involves employing treatment effect estimation techniques to assess their performance
on specific tasks. The gold standard for treatment effect estimation is comparing potential outcomes across
various treatments in a controlled experiment. However, the practical challenges of conducting such ex-
periments in real-world scenarios make the interactive nature of LLMs an ideal candidate for investigation.
12Leveraging their inherent interactivity, researchers could explore quasi-experimental settings, utilizing the
natural variations in responses to infer causal relationships. It is crucial to acknowledge the complexities
associated with interpreting LLMs, potential biases in training data, and the intricate nature of language
understanding tasks. Additionally, considering ethical implications and addressing biases in both training
and evaluation data is essential when delving into the reasoning capacity of LLMs using causal inference
methods.
Efficient training and inference in LLMs. As model scales and training data continue to expand,
the process of knowledge updating and inference in Large Language Models (LLMs) becomes increasingly
resource-intensive. Consequently, it becomes imperative for us to devise methods that can efficiently and
judiciously update the knowledge base of pre-trained models. In this context, causal inference methods
can play a crucial role by offering guidance in quantifying efficiency. By establishing causal relationships
between methods of interest and the existing knowledge collection, these methods can help assess the im-
pact of different updating strategies. This approach not only addresses the resource challenges associated
with knowledge updating but also contributes to a more nuanced understanding of the evolving knowledge
landscape in LLMs.
LLM-based counterfactual estimation and augmentation. As a general expert, Large Language
Models (LLMs) can significantly contribute to overcoming the current limitations of causal inference meth-
ods. A common assumption in many causal methods, as indicated in Assumption 5.3, is the existence of
corresponding data points for every treatment. However, this assumption often proves untrue, particularly
when dealing with imbalanced minority data that may not support meaningful learning. LLMs, function-
ing as versatile experts, have the potential to address this challenge by aiding in data augmentation for
minority data. Through their comprehensive understanding of language and context, LLMs can enhance
the availability of diverse data points, facilitating more robust and effective causal inference in situations
where traditional methods may struggle due to data imbalances. Similarly, many methods operate under
the assumptionofunconfoundedness(Assumption5.2)withinthe potentialoutcomeframework,acondition
considered quite strong. Historically, this assumption has been accepted due to a lack of domain knowledge
regardingtheunderlyingcausalgraphoridentificationofpotentialconfounders. However,withtheadventof
LargeLanguageModels(LLMs),thereisanopportunitytoalleviatethisstringentlimitation. LLMscanact
as general experts, offering valuable information about potential causal graphs and knowledge. This trans-
formativecapabilityofLLMsopensavenuestoenhanceourunderstandingofcausalrelationships,addressing
a historical challenge where unconfoundedness assumptions were made due to limited domain knowledge.
7 Conclusion
Atitscore,alargelanguagemodel(LLM)islikeavastlibraryofknowledge. Oneoftheongoingchallengesis
figuringouthowtoextractanduse thisknowledgeeffectively. The keytoimprovingLLMs liesinenhancing
their ability to understand cause and effect – essentially, how things are connected. Causal reasoning is
crucial for making LLMs smarter. Looking at it from a causal inference perspective, we find a valuable
framework that helps boost the effectiveness of LLMs. Meanwhile, as keepers of human knowledge, LLMs
canevenhelpovercomelimitationsincausalinferencebyprovidingbroadexpertisethatgoesbeyondexisting
constraints, reshaping our understanding in this important area and bringing new vitality to this area.
In this survey, we offer a thorough examination of the current landscape where large language models
(LLM) intersect with causal inference. We delve into how causal inference contributes to LLM, enhancing
aspectssuchasreasoning,fairness,andsafety,alongwiththeexplainabilityofLLM.Additionally,weexplore
howLLM,inturn,broadensthehorizonsofcausalinference. Acrossthesecategories,oursurveyprovidesin-
depthdiscussions,comparisons,andconcisesummariesofthemethods scrutinized,offeringacomprehensive
overview of the current state of research at this intersection. The available benchmark datasets and open-
source codes of those methods are also listed.
Theexaminationofthecurrentadvancementsincausalinferenceandlargelanguagemodelsservesadual
purpose. Firstly, it enhances our comprehension of how these two fields mutually benefit from each other.
Secondly, it catalyzes the emergence of novel questions, propelling us closer to achieving Artificial General
13Intelligence. Moreover,this explorationhasthe potentialto extendinto diverserealmsandfindapplications
inreal-worldscenarios,showcasingthefar-reachingimplicationsofthesynergybetweencausalinferenceand
LLM.
References
[1] Sara Abdali et al. “Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and
In-context Learning”. In: arXiv preprint arXiv:2312.06820 (2023).
[2] Josh Achiam et al. “GPT-4 Technical Report”. In: arXiv preprint arXiv:2303.08774 (2023).
[3] Nur Arifin Akbar et al. “Deep Learning of a Pre-trained Language Model’s Joke Classifier Using
GPT-2”. In: Journal of Hunan University Natural Sciences 48.8 (2021).
[4] AlessandroAntonucci,GregorioPiqu’e,andMarcoZaffalon.“Zero-shotCausalGraphExtrapolation
from Text via LLMs”. In: 2023. url: https://api.semanticscholar.org/CorpusID:266521610.
[5] Vahan Arsenyan and Davit Shahnazaryan. “Large Language Models for Biomedical Causal Graph
Construction”. In: arXiv preprint arXiv:2301.12473 (2023).
[6] Jinze Bai et al. “Qwen technical report”. In: arXiv preprint arXiv:2309.16609 (2023).
[7] TaiyuBanetal.“CausalStructureLearningSupervisedbyLargeLanguageModel”.In:arXivpreprint
arXiv:2311.11689 (2023).
[8] TaiyuBanetal.“Fromquerytoolstocausalarchitects:Harnessinglargelanguagemodelsforadvanced
causal discovery from data”. In: arXiv preprint arXiv:2306.16902 (2023).
[9] RongzhouBao,JiayiWang, and Hai Zhao.“Defending pre-trainedlanguagemodels from adversarial
word substitutions without performance sacrifice”. In: arXiv preprint arXiv:2105.14553 (2021).
[10] Nora Belrose et al. “Eliciting latent predictions from transformers with the tuned lens”. In: arXiv
preprint arXiv:2303.08112 (2023).
[11] LorenzoBettietal.“Relevance-basedInfillingforNaturalLanguageCounterfactuals”.In:Proceedings
ofthe32ndACMInternationalConferenceonInformationandKnowledgeManagement.2023,pp.88–
98.
[12] Amrita Bhattacharjee et al. “LLMs as Counterfactual ExplanationModules: Can ChatGPT Explain
Black-box Text Classifiers?” In: arXiv preprint arXiv:2309.13340 (2023).
[13] Tom Brown et al. “Language models are few-shot learners”. In: Advances in neural information
processing systems 33 (2020), pp. 1877–1901.
[14] S´ebastien Bubeck et al. “Sparks of artificial general intelligence: Early experiments with gpt-4”. In:
arXiv preprint arXiv:2303.12712 (2023).
[15] Boxi Cao et al. “Can prompt probe pretrained language models? understanding the invisible risks
from a causal view”. In: arXiv preprint arXiv:2203.12258 (2022).
[16] Nicolas Carion et al. “End-to-end object detection with transformers”. In: European conference on
computer vision. Springer. 2020, pp. 213–229.
[17] Hang Chen et al. “Learning a Structural CausalModel for Intuition Reasoning in Conversation”.In:
arXiv preprint arXiv:2305.17727 (2023).
[18] Yanda Chen et al. “Do models explain themselves? counterfactual simulatability of natural language
explanations”. In: arXiv preprint arXiv:2307.08678 (2023).
[19] ZemingChenetal.“DISCO:distillingcounterfactualswithlargelanguagemodels”.In:Proceedings of
the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
2023,pp. 5514–5528.
[20] Robert Dale. “GPT-3: What’s it good for?” In: Natural Language Engineering 27.1 (2021), pp. 113–
118.
14[21] Ernest Davis and Gary Marcus. “Commonsense reasoning and commonsense knowledge in artificial
intelligence”. In: Communications of the ACM 58.9 (2015), pp. 92–103.
[22] LeiDingetal.“Wordembeddingsviacausalinference:Genderbiasreducingandsemanticinformation
preserving”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 11. 2022,
pp. 11864–11872.
[23] Amir Feder et al. “Causal Inference in Natural Language Processing: Estimation, Prediction, In-
terpretation and Beyond”. In: Transactions of the Association for Computational Linguistics 10
(2022). Ed. by Brian Roark and Ani Nenkova, pp. 1138–1158. doi: 10.1162/tacl_a_00511. url:
https://aclanthology.org/2022.tacl-1.66.
[24] Amir Feder et al. “Causal-structureDrivenAugmentations for Text OOD Generalization”.In: arXiv
preprint arXiv:2310.12803 (2023).
[25] Eve Fleisig et al. “Fair-Prism: Evaluating fairness-related harms in text generation”. In: Proceed-
ings of the 61st Annual Meeting of the Association for Computational Linguistics. Association for
Computational Linguistics. 2023.
[26] Luciano Floridi and Massimo Chiriatti. “GPT-3: Its nature, scope, limits, and consequences”. In:
Minds and Machines 30 (2020), pp. 681–694.
[27] JinglongGaoetal.“IsChatGPT aGoodCausalReasoner?AComprehensiveEvaluation”.In: arXiv
preprint arXiv:2305.07375 (2023).
[28] Yair Gat et al. “Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfac-
tuals”. In: arXiv preprint arXiv:2310.00603 (2023).
[29] AtticusGeigeretal.“Findingalignmentsbetweeninterpretablecausalvariablesanddistributedneural
representations”.In: arXiv preprint arXiv:2303.02536 (2023).
[30] Tianrui Guan et al. “LOC-ZSON: Language-driven Object-Centric Zero-Shot Object Retrieval and
Navigation”.In: arXiv preprint arXiv (2024).
[31] ArnavGudibandeetal.“Thefalsepromiseofimitatingproprietaryllms”.In:arXivpreprintarXiv:2305.15717
(2023).
[32] RiccardoGuidotti et al.“Asurveyofmethods for explainingblack boxmodels”.In: ACM computing
surveys (CSUR) 51.5 (2018), pp. 1–42.
[33] Wes Gurnee et al. “Finding Neurons in a Haystack: Case Studies with Sparse Probing”. In: arXiv
preprint arXiv:2305.01610 (2023).
[34] JordanHoffmannetal.“Trainingcompute-optimallargelanguagemodels”.In:arXivpreprintarXiv:2203.15556
(2022).
[35] Yifan Hou et al. “Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of
Language Models”. In: arXiv preprint arXiv:2310.14491 (2023).
[36] Yinya Huang et al. “CLOMO: Counterfactual Logical Modification with Large Language Models”.
In: arXiv preprint arXiv:2311.17438 (2023).
[37] DavidFJennyetal.“Navigatingthe OceanofBiases:PoliticalBiasAttributioninLanguageModels
via Causal Structures”. In: arXiv preprint arXiv:2311.08605 (2023).
[38] Zhenlan Ji et al. “Benchmarking and Explaining Large Language Model-based Code Generation: A
Causality-Centric Approach”. In: arXiv preprint arXiv:2310.06680 (2023).
[39] RobinJiaetal.“Certifiedrobustnesstoadversarialwordsubstitutions”.In:arXivpreprintarXiv:1909.00986
(2019).
[40] ZhijingJinetal.“CanLargeLanguageModelsInferCausationfromCorrelation?”In:arXiv preprint
arXiv:2306.05836 (2023).
[41] Zhijing Jinet al.“Cladder:Abenchmark to assesscausalreasoningcapabilities oflanguagemodels”.
In: arXiv preprint arXiv:2312.04350 (2023).
[42] Chandra Khatri et al. “Advancing the state of the art in open domain dialog systems through the
alexa prize”. In: arXiv preprint arXiv:1812.10757 (2018).
15[43] Yuheun Kim et al. “Can ChatGPT Understand Causal Language in Science Claims?” In: Proceed-
ings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media
Analysis. 2023, pp. 379–389.
[44] EmreKıcımanetal.“Causalreasoningandlargelanguagemodels:Openinganewfrontierforcausal-
ity”. In: arXiv preprint arXiv:2305.00050 (2023).
[45] Dohwan Ko et al. “Large Language Models are Temporal and Causal Reasoners for Video Question
Answering”. In: arXiv preprint arXiv:2310.15747 (2023).
[46] Manabu Kuroki and Judea Pearl. “Measurement bias and effect restoration in causal inference”. In:
Biometrika 101.2 (2014), pp. 423–437.
[47] Jia Li and Xiang Li. “Relation-Oriented: Toward Knowledge-Aligned Causal AI”. In: arXiv preprint
arXiv:2307.16387 (2023).
[48] Junnan Li et al. “Blip-2:Bootstrapping language-imagepre-training with frozenimage encoders and
large language models”. In: arXiv preprint arXiv:2301.12597 (2023).
[49] XiaochuanLietal.“ImageContentGenerationwithCausalReasoning”.In:arXivpreprintarXiv:2312.07132
(2023).
[50] Yongqi Li et al. “Large Language Models as Counterfactual Generator: Strengths and Weaknesses”.
In: arXiv preprint arXiv:2305.14791 (2023).
[51] Zhang Li et al. “Monkey:Image resolution and text label are important things for large multi-modal
models”. In: arXiv preprint arXiv:2311.06607 (2023).
[52] Zongxia Li et al. “Towards understanding in-context learning with contrastive demonstrations and
saliency maps”. In: arXiv preprint arXiv:2307.05052 (2023).
[53] Percy Liang et al. “Holistic evaluation of language models”. In: arXiv preprint arXiv:2211.09110
(2022).
[54] Zachary C Lipton. “The mythos of model interpretability: In machine learning, the concept of inter-
pretability is both important and slippery.” In: Queue 16.3 (2018), pp. 31–57.
[55] Fuxiao Liu et al. “Aligning Large Multi-Modal Model with Robust Instruction Tuning”. In: arXiv
preprint arXiv:2306.14565 (2023).
[56] Fuxiao Liu et al. “HallusionBench: You See What You Think? Or You Think What You See? An
Image-ContextReasoningBenchmarkChallengingforGPT-4V(ision),LLaVA-1.5,andOtherMulti-
modality Models”. In: arXiv preprint arXiv:2310.14566 (2023).
[57] Fuxiao Liu et al. “MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction
Tuning”. In: arXiv preprint arXiv:2311.10774 (2023).
[58] Fuxiao Liu et al. “Visual news: Benchmark and challenges in news image captioning”. In: arXiv
preprint arXiv:2010.03743 (2020).
[59] Haotian Liu et al. “Visual instruction tuning”. In: arXiv preprint arXiv:2304.08485 (2023).
[60] XiaoLiuetal.“TheMagicofIF:InvestigatingCausalReasoningAbilities inLargeLanguageModels
of Code”. In: arXiv preprint arXiv:2305.19213 (2023).
[61] StephanieLongetal.“Canlargelanguagemodelsbuildcausalgraphs?”In:arXivpreprintarXiv:2303.05279
(2023).
[62] Yujie Lu et al. “Neuro-Symbolic Procedural Planning with Commonsense Prompting”. In: arXiv
preprint arXiv:2206.02928 (2022).
[63] Muhammad Maaz et al. “Video-ChatGPT: Towards Detailed Video Understanding via Large Vision
and Language Models”. In: arXiv preprint arXiv:2306.05424 (2023).
[64] Aman Madaan, Katherine Hermann, and Amir Yazdanbakhsh. “What Makes Chain-of-Thought
Prompting Effective? A Counterfactual Study”. In: Findings of the Association for Computational
Linguistics: EMNLP 2023. 2023, pp. 1448–1535.
16[65] Rahul Madhavan et al. “CFL: Causally Fair Language Models Through Token-level Attribute Con-
trolled Generation”. In: arXiv preprint arXiv:2306.00374 (2023).
[66] James Manyika and Sissie Hsiao. “An overview of bard: an early experiment with generative ai”. In:
AI. Google Static Documents 2 (2023).
[67] Rui Mao et al. “GPTEval: A survey on assessments of ChatGPT and GPT-4”. In: arXiv preprint
arXiv:2308.12488 (2023).
[68] Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy. “An empirical survey of the effectiveness of
debiasing techniques for pre-trained language models”. In: arXiv preprint arXiv:2110.08527 (2021).
[69] ZhaoMengetal.“Self-SupervisedContrastiveLearningwithAdversarialPerturbationsforDefending
Word Substitution-based Attacks”. In: arXiv preprint arXiv:2107.07610 (2021).
[70] Xin Miao, Yongqi Li, and Tieyun Qian. “Generating Commonsense Counterfactuals for Stable Rela-
tion Extraction”.In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing. 2023, pp. 5654–5668.
[71] Narmada Naik et al. “Applying Large Language Models for Causal Structure Learning in Non Small
Cell Lung Cancer”. In: arXiv preprint arXiv:2311.07191 (2023).
[72] Allen Nie et al. “MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judg-
ment Tasks”. In: arXiv preprint arXiv:2310.19677 (2023).
[73] LiangmingPanetal.“Logic-lm:Empoweringlargelanguagemodelswithsymbolicsolversforfaithful
logical reasoning”.In: arXiv preprint arXiv:2305.12295 (2023).
[74] Nick Pawlowskiet al. “Answering Causal Questions with Augmented LLMs”. In: (2023).
[75] Judea Pearl. Causality. Cambridge university press, 2009.
[76] JudeaPearl.“Graphicalmodels for probabilisticandcausalreasoning”.In: Quantified representation
of uncertainty and imprecision (1998), pp. 367–389.
[77] Baolin Peng et al. “Instruction tuning with gpt-4”. In: arXiv preprint arXiv:2304.03277 (2023).
[78] Alec Radford et al. “Improving language understanding by generative pre-training”. In: (2018).
[79] AngelikaRomanouetal.“CRAB:AssessingtheStrengthofCausalRelationshipsBetweenReal-world
Events”. In: arXiv preprint arXiv:2311.04284 (2023).
[80] DonaldBRubin.“Estimatingcausaleffectsoftreatmentsinrandomizedandnonrandomizedstudies.”
In: Journal of educational Psychology 66.5 (1974), p. 688.
[81] Timo Schick and Hinrich Schu¨tze. “Exploiting cloze questions for few shot text classification and
natural language inference”. In: arXiv preprint arXiv:2001.07676 (2020).
[82] GabrielStanovsky,NoahA.Smith,andLukeZettlemoyer.“EvaluatingGenderBiasinMachineTrans-
lation”.In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.
Ed.by Anna Korhonen,DavidTraum,andLlu´ısMa`rquez.Florence,Italy:Associationfor Computa-
tionalLinguistics,July2019,pp.1679–1684.doi:10.18653/v1/P19-1164.url:https://aclanthology.org/P19-1164
[83] Alessandro Stolfo, Yonatan Belinkov, and Mrinmaya Sachan. A Mechanistic Interpretation of Arith-
meticReasoninginLanguageModelsusingCausalMediationAnalysis.2023.arXiv:2305.15054[cs.CL].
[84] Shane Storks, Qiaozi Gao, and Joyce Y Chai. “Commonsense reasoning for natural language under-
standing: A survey of benchmarks, resources, and approaches”. In: arXiv preprint arXiv:1904.01172
(2019), pp. 1–60.
[85] Hung-Ting Su et al. “Language Models are Causal Knowledge Extractors for Zero-shot Video Ques-
tion Answering”. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2023, pp. 4950–4959.
[86] Yan Tai et al. “Link-context learning for multimodal llms”. In: arXiv preprint arXiv:2308.07891
(2023).
[87] JuanheTJTan.“CausalAbstractionforChain-of-ThoughtReasoninginArithmeticWordProblems”.
In: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for
NLP. 2023,pp. 155–168.
17[88] Ziyi Tang et al. “Towards causalgpt: A multi-agent approach for faithful knowledge reasoning via
promoting causal consistency in llms”. In: arXiv preprint arXiv:2308.11914 (2023).
[89] Rohan Taori et al. “Alpaca: A strong, replicable instruction-following model”. In: Stanford Center
for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html 3.6(2023),
p. 7.
[90] Hugo Touvron et al. “Llama 2: Open foundation and fine-tuned chat models”. In: arXiv preprint
arXiv:2307.09288 (2023).
[91] Ruibo Tu, Chao Ma, and Cheng Zhang. “Causal-discovery performance of chatgpt in the context of
neuropathic pain diagnosis”. In: arXiv preprint arXiv:2301.13819 (2023).
[92] AniketVashishthaetal.“CausalInferenceUsingLLM-GuidedDiscovery”.In:arXivpreprintarXiv:2310.15117
(2023).
[93] Ashish Vaswani et al. “Attention is all you need”. In: Advances in neural information processing
systems 30 (2017).
[94] Yuxuan Wan et al. “Biasasker: Measuring the bias in conversational ai system”. In: Proceedings of
the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations
of Software Engineering. 2023, pp. 515–527.
[95] Fei Wang et al. “A Causal View of Entity Bias in (Large) Language Models”. In: arXiv preprint
arXiv:2305.14695 (2023).
[96] Xiyao Wang et al. “Mementos: A comprehensive benchmark for multimodal large language model
reasoning over image sequences”. In: arXiv preprint arXiv:2401.10529 (2024).
[97] JasonWeietal.“Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels”.In:Advances
in Neural Information Processing Systems 35 (2022), pp. 24824–24837.
[98] Moritz Willig et al. “Probing for correlations of causal facts: Large language models and causality”.
In: (2022).
[99] SewallWright.“Themethodofpathcoefficients”.In:Theannalsofmathematicalstatistics 5.3(1934),
pp. 161–215.
[100] Xiyang Wu et al. “On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the
Risks and Vulnerabilities”. In: arXiv preprint arXiv:2402.10340 (2024).
[101] Zhengxuan Wu et al. “Interpretability at scale: Identifying causal mechanisms in alpaca”. In: arXiv
preprint arXiv:2305.08809 (2023).
[102] Yijia Xiaoetal.“Largelanguagemodels canbe goodprivacyprotectionlearners”.In: arXiv preprint
arXiv:2310.02469 (2023).
[103] Yuxi Xie, Guanzhen Li, and Min-Yen Kan. “ECHo: Event Causality Inference via Human-centric
Reasoning”. In: arXiv preprint arXiv:2305.14740 (2023).
[104] JinghangXuetal.“Areviewofdatasetandlabelingmethodsforcausalityextraction”.In:Proceedings
of the 28th International Conference on Computational Linguistics. 2020, pp. 1519–1531.
[105] Jie Yang, Soyeon Caren Han, and Josiah Poon. “A survey on extraction of causal relations from
natural language text”. In: Knowledge and Information Systems 64.5 (2022), pp. 1161–1186.
[106] Qinghao Ye et al. “mplug-owl2: Revolutionizing multi-modal large language model with modality
collaboration”.In: arXiv preprint arXiv:2311.04257 (2023).
[107] Wenhao Yu et al. “IfQA: A Dataset for Open-domain Question Answering under Counterfactual
Presuppositions”.In: arXiv preprint arXiv:2305.14010 (2023).
[108] Alessio Zanga, Elif Ozkirimli, and Fabio Stella. “A survey on causal discovery:theory and practice”.
In: International Journal of Approximate Reasoning 151 (2022), pp. 101–129.
[109] Matej Zeˇcevi´cet al. “Causal parrots: Large language models may talk causality but are not causal”.
In: arXiv preprint arXiv:2308.13067 (2023).
18[110] JingyingZengandRunWang.“Asurveyofcausalinferenceframeworks”.In:arXivpreprintarXiv:2209.00869
(2022).
[111] Wei Zeng et al. “Pangua: Large-scale autoregressive pretrained Chinese language models with auto-
parallel computation”. In: arXiv preprint arXiv:2104.12369 (2021).
[112] RuntianZhaietal.“Macer:Attack-freeandscalablerobusttrainingviamaximizingcertifiedradius”.
In: arXiv preprint arXiv:2001.02378 (2020).
[113] Jiaqi Zhang et al. “Towards Causal Foundation Model: on Duality between Causal Inference and
Attention”. In: arXiv preprint arXiv:2310.00809 (2023).
[114] Jiayao Zhang et al. “Rock: Causal inference principles for reasoning about commonsense causality”.
In: International Conference on Machine Learning. PMLR. 2022, pp. 26750–26771.
[115] Li Zhang et al. “Causal Reasoning of Entities and Events in Procedural Texts”. In: arXiv preprint
arXiv:2301.10896 (2023).
[116] Shuo Zhang et al. “Mitigating Language Model Hallucination with Interactive Question-Knowledge
Alignment”. In: arXiv preprint arXiv:2305.13669 (2023).
[117] Yanzhe Zhanget al.“Llavar:Enhancedvisual instructiontuning for text-richimage understanding”.
In: arXiv preprint arXiv:2306.17107 (2023).
[118] Haiteng Zhao et al. “Certified robustness against natural language attacks by causal intervention”.
In: International Conference on Machine Learning. PMLR. 2022, pp. 26958–26970.
[119] Haiyan Zhao et al. “Explainability for large language models: A survey”. In: ACM Transactions on
Intelligent Systems and Technology (2023).
[120] Qinlin Zhao et al. “Competeai: Understanding the competition behaviors in large language model-
based agents”. In: arXiv preprint arXiv:2310.17512 (2023).
[121] Shitian Zhao et al. “Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-
modal Language Models”. In: arXiv preprint arXiv:2312.06685 (2023).
[122] Wei Zhao, Zhe Li, and Jun Sun. “Causality Analysis for Evaluating the Security of Large Language
Models”. In: arXiv preprint arXiv:2312.07876 (2023).
[123] Junhao Zheng et al. “Preserving Commonsense Knowledge from Pre-trained Language Models via
Causal Inference”. In: arXiv preprint arXiv:2306.10790 (2023).
[124] Fan Zhou et al. “Causal-debias: Unifying debiasing in pretrained language models and fine-tuning
via causal invariant learning”. In: Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers). 2023,pp. 4227–4241.
[125] YuhangZhou,SurajMaharjan,andBeiyeLiu.“Scalablepromptgenerationforsemi-supervisedlearn-
ing with language models”. In: arXiv preprint arXiv:2302.09236 (2023).
[126] Yuhang Zhou et al. “Explore Spurious Correlations at the Concept Level in Language Models for
Text Classification”. In: arXiv preprint arXiv:2311.08648 (2023).
[127] DeyaoZhuetal.“Minigpt-4:Enhancingvision-languageunderstandingwithadvancedlargelanguage
models”. In: arXiv preprint arXiv:2304.10592 (2023).
19