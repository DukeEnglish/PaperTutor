[
    {
        "title": "Trust AI Regulation? Discerning users are vital to build trust and effective AI regulation",
        "authors": "Zainab AlalawiPaolo BovaTheodor CimpeanuAlessandro Di StefanoManh Hong DuongElias Fernandez DomingosThe Anh HanMarcus KrellnerBianca OgboSimon T. PowersFilippo Zimmaro",
        "links": "http://arxiv.org/abs/2403.09510v1",
        "entry_id": "http://arxiv.org/abs/2403.09510v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09510v1",
        "summary": "There is general agreement that some form of regulation is necessary both for\nAI creators to be incentivised to develop trustworthy systems, and for users to\nactually trust those systems. But there is much debate about what form these\nregulations should take and how they should be implemented. Most work in this\narea has been qualitative, and has not been able to make formal predictions.\nHere, we propose that evolutionary game theory can be used to quantitatively\nmodel the dilemmas faced by users, AI creators, and regulators, and provide\ninsights into the possible effects of different regulatory regimes. We show\nthat creating trustworthy AI and user trust requires regulators to be\nincentivised to regulate effectively. We demonstrate the effectiveness of two\nmechanisms that can achieve this. The first is where governments can recognise\nand reward regulators that do a good job. In that case, if the AI system is not\ntoo risky for users then some level of trustworthy development and user trust\nevolves. We then consider an alternative solution, where users can condition\ntheir trust decision on the effectiveness of the regulators. This leads to\neffective regulation, and consequently the development of trustworthy AI and\nuser trust, provided that the cost of implementing regulations is not too high.\nOur findings highlight the importance of considering the effect of different\nregulatory regimes from an evolutionary game theoretic perspective.",
        "updated": "2024-03-14 15:56:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09510v1"
    },
    {
        "title": "Safe Road-Crossing by Autonomous Wheelchairs: a Novel Dataset and its Experimental Evaluation",
        "authors": "Carlo GrigioniFranca CorradiniAlessandro AntonucciJérôme GuzziFrancesco Flammini",
        "links": "http://arxiv.org/abs/2403.08984v1",
        "entry_id": "http://arxiv.org/abs/2403.08984v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08984v1",
        "summary": "Safe road-crossing by self-driving vehicles is a crucial problem to address\nin smart-cities. In this paper, we introduce a multi-sensor fusion approach to\nsupport road-crossing decisions in a system composed by an autonomous\nwheelchair and a flying drone featuring a robust sensory system made of diverse\nand redundant components. To that aim, we designed an analytical danger\nfunction based on explainable physical conditions evaluated by single sensors,\nincluding those using machine learning and artificial vision. As a\nproof-of-concept, we provide an experimental evaluation in a laboratory\nenvironment, showing the advantages of using multiple sensors, which can\nimprove decision accuracy and effectively support safety assessment. We made\nthe dataset available to the scientific community for further experimentation.\nThe work has been developed in the context of an European project named\nREXASI-PRO, which aims to develop trustworthy artificial intelligence for\nsocial navigation of people with reduced mobility.",
        "updated": "2024-03-13 22:19:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08984v1"
    },
    {
        "title": "Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning",
        "authors": "Peihong YuManav MishraAlec KoppelCarl BusartPriya NarayanDinesh ManochaAmrit BediPratap Tokekar",
        "links": "http://arxiv.org/abs/2403.08936v1",
        "entry_id": "http://arxiv.org/abs/2403.08936v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08936v1",
        "summary": "Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of\nefficient exploration due to the exponential increase in the size of the joint\nstate-action space. While demonstration-guided learning has proven beneficial\nin single-agent settings, its direct applicability to MARL is hindered by the\npractical difficulty of obtaining joint expert demonstrations. In this work, we\nintroduce a novel concept of personalized expert demonstrations, tailored for\neach individual agent or, more broadly, each individual type of agent within a\nheterogeneous team. These demonstrations solely pertain to single-agent\nbehaviors and how each agent can achieve personal goals without encompassing\nany cooperative elements, thus naively imitating them will not achieve\ncooperation due to potential conflicts. To this end, we propose an approach\nthat selectively utilizes personalized expert demonstrations as guidance and\nallows agents to learn to cooperate, namely personalized expert-guided MARL\n(PegMARL). This algorithm utilizes two discriminators: the first provides\nincentives based on the alignment of policy behavior with demonstrations, and\nthe second regulates incentives based on whether the behavior leads to the\ndesired objective. We evaluate PegMARL using personalized demonstrations in\nboth discrete and continuous environments. The results demonstrate that PegMARL\nlearns near-optimal policies even when provided with suboptimal demonstrations,\nand outperforms state-of-the-art MARL algorithms in solving coordinated tasks.\nWe also showcase PegMARL's capability to leverage joint demonstrations in the\nStarCraft scenario and converge effectively even with demonstrations from\nnon-co-trained policies.",
        "updated": "2024-03-13 20:11:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08936v1"
    },
    {
        "title": "Cultural evolution in populations of Large Language Models",
        "authors": "Jérémy PerezCorentin LégerMarcela Ovando-TellezChris FoulonJoan DussauldPierre-Yves OudeyerClément Moulin-Frier",
        "links": "http://arxiv.org/abs/2403.08882v1",
        "entry_id": "http://arxiv.org/abs/2403.08882v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08882v1",
        "summary": "Research in cultural evolution aims at providing causal explanations for the\nchange of culture over time. Over the past decades, this field has generated an\nimportant body of knowledge, using experimental, historical, and computational\nmethods. While computational models have been very successful at generating\ntestable hypotheses about the effects of several factors, such as population\nstructure or transmission biases, some phenomena have so far been more complex\nto capture using agent-based and formal models. This is in particular the case\nfor the effect of the transformations of social information induced by evolved\ncognitive mechanisms. We here propose that leveraging the capacity of Large\nLanguage Models (LLMs) to mimic human behavior may be fruitful to address this\ngap. On top of being an useful approximation of human cultural dynamics,\nmulti-agents models featuring generative agents are also important to study for\ntheir own sake. Indeed, as artificial agents are bound to participate more and\nmore to the evolution of culture, it is crucial to better understand the\ndynamics of machine-generated cultural evolution. We here present a framework\nfor simulating cultural evolution in populations of LLMs, allowing the\nmanipulation of variables known to be important in cultural evolution, such as\nnetwork structure, personality, and the way social information is aggregated\nand transformed. The software we developed for conducting these simulations is\nopen-source and features an intuitive user-interface, which we hope will help\nto build bridges between the fields of cultural evolution and generative\nartificial intelligence.",
        "updated": "2024-03-13 18:11:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08882v1"
    },
    {
        "title": "Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning",
        "authors": "Jing TanRamin KhaliliHolger Karl",
        "links": "http://arxiv.org/abs/2403.08879v1",
        "entry_id": "http://arxiv.org/abs/2403.08879v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08879v1",
        "summary": "The Intelligent Transportation System (ITS) environment is known to be\ndynamic and distributed, where participants (vehicle users, operators, etc.)\nhave multiple, changing and possibly conflicting objectives. Although\nReinforcement Learning (RL) algorithms are commonly applied to optimize ITS\napplications such as resource management and offloading, most RL algorithms\nfocus on single objectives. In many situations, converting a multi-objective\nproblem into a single-objective one is impossible, intractable or insufficient,\nmaking such RL algorithms inapplicable. We propose a multi-objective,\nmulti-agent reinforcement learning (MARL) algorithm with high learning\nefficiency and low computational requirements, which automatically triggers\nadaptive few-shot learning in a dynamic, distributed and noisy environment with\nsparse and delayed reward. We test our algorithm in an ITS environment with\nedge cloud computing. Empirical results show that the algorithm is quick to\nadapt to new environments and performs better in all individual and system\nmetrics compared to the state-of-the-art benchmark. Our algorithm also\naddresses various practical concerns with its modularized and asynchronous\nonline training method. In addition to the cloud simulation, we test our\nalgorithm on a single-board computer and show that it can make inference in 6\nmilliseconds.",
        "updated": "2024-03-13 18:05:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08879v1"
    }
]