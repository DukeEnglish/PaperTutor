[
    {
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
        "authors": "Akhil KediaMohd Abbas ZaidiSushil KhyaliaJungho JungHarshith GokaHaejun Lee",
        "links": "http://arxiv.org/abs/2403.09635v1",
        "entry_id": "http://arxiv.org/abs/2403.09635v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09635v1",
        "summary": "In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.",
        "updated": "2024-03-14 17:59:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09635v1"
    },
    {
        "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
        "authors": "Eric ZelikmanGeorges HarikYijia ShaoVaruna JayasiriNick HaberNoah D. Goodman",
        "links": "http://arxiv.org/abs/2403.09629v1",
        "entry_id": "http://arxiv.org/abs/2403.09629v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09629v1",
        "summary": "When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.",
        "updated": "2024-03-14 17:58:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09629v1"
    },
    {
        "title": "Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation",
        "authors": "Fangfu LiuHanyang WangWeiliang ChenHaowen SunYueqi Duan",
        "links": "http://arxiv.org/abs/2403.09625v1",
        "entry_id": "http://arxiv.org/abs/2403.09625v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09625v1",
        "summary": "Recent years have witnessed the strong power of 3D generation models, which\noffer a new level of creative flexibility by allowing users to guide the 3D\ncontent generation process through a single image or natural language. However,\nit remains challenging for existing 3D generation methods to create\nsubject-driven 3D content across diverse prompts. In this paper, we introduce a\nnovel 3D customization method, dubbed Make-Your-3D that can personalize\nhigh-fidelity and consistent 3D content from only a single image of a subject\nwith text description within 5 minutes. Our key insight is to harmonize the\ndistributions of a multi-view diffusion model and an identity-specific 2D\ngenerative model, aligning them with the distribution of the desired 3D\nsubject. Specifically, we design a co-evolution framework to reduce the\nvariance of distributions, where each model undergoes a process of learning\nfrom the other through identity-aware optimization and subject-prior\noptimization, respectively. Extensive experiments demonstrate that our method\ncan produce high-quality, consistent, and subject-specific 3D content with\ntext-driven modifications that are unseen in subject image.",
        "updated": "2024-03-14 17:57:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09625v1"
    },
    {
        "title": "Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning",
        "authors": "Zhishuai LiuPan Xu",
        "links": "http://arxiv.org/abs/2403.09621v1",
        "entry_id": "http://arxiv.org/abs/2403.09621v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09621v1",
        "summary": "Distributionally robust offline reinforcement learning (RL), which seeks\nrobust policy training against environment perturbation by modeling dynamics\nuncertainty, calls for function approximations when facing large state-action\nspaces. However, the consideration of dynamics uncertainty introduces essential\nnonlinearity and computational burden, posing unique challenges for analyzing\nand practically employing function approximation. Focusing on a basic setting\nwhere the nominal model and perturbed models are linearly parameterized, we\npropose minimax optimal and computationally efficient algorithms realizing\nfunction approximation and initiate the study on instance-dependent\nsuboptimality analysis in the context of robust offline RL. Our results uncover\nthat function approximation in robust offline RL is essentially distinct from\nand probably harder than that in standard offline RL. Our algorithms and\ntheoretical results crucially depend on a variety of new techniques, involving\na novel function approximation mechanism incorporating variance information, a\nnew procedure of suboptimality and estimation uncertainty decomposition, a\nquantification of the robust value function shrinkage, and a meticulously\ndesigned family of hard instances, which might be of independent interest.",
        "updated": "2024-03-14 17:55:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09621v1"
    },
    {
        "title": "Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training",
        "authors": "Yanlai YangMatt JonesMichael C. MozerMengye Ren",
        "links": "http://arxiv.org/abs/2403.09613v1",
        "entry_id": "http://arxiv.org/abs/2403.09613v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09613v1",
        "summary": "We explore the training dynamics of neural networks in a structured non-IID\nsetting where documents are presented cyclically in a fixed, repeated sequence.\nTypically, networks suffer from catastrophic interference when training on a\nsequence of documents; however, we discover a curious and remarkable property\nof LLMs fine-tuned sequentially in this setting: they exhibit anticipatory\nbehavior, recovering from the forgetting on documents before encountering them\nagain. The behavior emerges and becomes more robust as the architecture scales\nup its number of parameters. Through comprehensive experiments and\nvisualizations, we uncover new insights into training over-parameterized\nnetworks in structured environments.",
        "updated": "2024-03-14 17:51:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09613v1"
    }
]