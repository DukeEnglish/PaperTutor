[
    {
        "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
        "authors": "Piotr NawrotAdrian ŁańcuckiMarcin ChochowskiDavid TarjanEdoardo M. Ponti",
        "links": "http://arxiv.org/abs/2403.09636v1",
        "entry_id": "http://arxiv.org/abs/2403.09636v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09636v1",
        "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for on-line key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression rates in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to ~3.7x throughput increase in auto-regressive inference on a\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. We find\nthat DMC preserves the original downstream performance with up to 4x cache\ncompression, outperforming up-trained grouped-query attention (GQA). GQA and\nDMC can be even combined to obtain compounded gains. As a result DMC fits\nlonger contexts and larger batches within any given memory budget.",
        "updated": "2024-03-14 17:59:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09636v1"
    },
    {
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
        "authors": "Akhil KediaMohd Abbas ZaidiSushil KhyaliaJungho JungHarshith GokaHaejun Lee",
        "links": "http://arxiv.org/abs/2403.09635v1",
        "entry_id": "http://arxiv.org/abs/2403.09635v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09635v1",
        "summary": "In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.",
        "updated": "2024-03-14 17:59:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09635v1"
    },
    {
        "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
        "authors": "Haoyu ZhenXiaowen QiuPeihao ChenJincheng YangXin YanYilun DuYining HongChuang Gan",
        "links": "http://arxiv.org/abs/2403.09631v1",
        "entry_id": "http://arxiv.org/abs/2403.09631v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09631v1",
        "summary": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.",
        "updated": "2024-03-14 17:58:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09631v1"
    },
    {
        "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
        "authors": "Eric ZelikmanGeorges HarikYijia ShaoVaruna JayasiriNick HaberNoah D. Goodman",
        "links": "http://arxiv.org/abs/2403.09629v1",
        "entry_id": "http://arxiv.org/abs/2403.09629v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09629v1",
        "summary": "When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.",
        "updated": "2024-03-14 17:58:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09629v1"
    },
    {
        "title": "Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training",
        "authors": "Yanlai YangMatt JonesMichael C. MozerMengye Ren",
        "links": "http://arxiv.org/abs/2403.09613v1",
        "entry_id": "http://arxiv.org/abs/2403.09613v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09613v1",
        "summary": "We explore the training dynamics of neural networks in a structured non-IID\nsetting where documents are presented cyclically in a fixed, repeated sequence.\nTypically, networks suffer from catastrophic interference when training on a\nsequence of documents; however, we discover a curious and remarkable property\nof LLMs fine-tuned sequentially in this setting: they exhibit anticipatory\nbehavior, recovering from the forgetting on documents before encountering them\nagain. The behavior emerges and becomes more robust as the architecture scales\nup its number of parameters. Through comprehensive experiments and\nvisualizations, we uncover new insights into training over-parameterized\nnetworks in structured environments.",
        "updated": "2024-03-14 17:51:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09613v1"
    }
]