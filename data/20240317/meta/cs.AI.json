[
    {
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
        "authors": "Akhil KediaMohd Abbas ZaidiSushil KhyaliaJungho JungHarshith GokaHaejun Lee",
        "links": "http://arxiv.org/abs/2403.09635v1",
        "entry_id": "http://arxiv.org/abs/2403.09635v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09635v1",
        "summary": "In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.",
        "updated": "2024-03-14 17:59:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09635v1"
    },
    {
        "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
        "authors": "Haoyu ZhenXiaowen QiuPeihao ChenJincheng YangXin YanYilun DuYining HongChuang Gan",
        "links": "http://arxiv.org/abs/2403.09631v1",
        "entry_id": "http://arxiv.org/abs/2403.09631v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09631v1",
        "summary": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.",
        "updated": "2024-03-14 17:58:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09631v1"
    },
    {
        "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
        "authors": "Eric ZelikmanGeorges HarikYijia ShaoVaruna JayasiriNick HaberNoah D. Goodman",
        "links": "http://arxiv.org/abs/2403.09629v1",
        "entry_id": "http://arxiv.org/abs/2403.09629v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09629v1",
        "summary": "When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.",
        "updated": "2024-03-14 17:58:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09629v1"
    },
    {
        "title": "Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning",
        "authors": "Zhishuai LiuPan Xu",
        "links": "http://arxiv.org/abs/2403.09621v1",
        "entry_id": "http://arxiv.org/abs/2403.09621v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09621v1",
        "summary": "Distributionally robust offline reinforcement learning (RL), which seeks\nrobust policy training against environment perturbation by modeling dynamics\nuncertainty, calls for function approximations when facing large state-action\nspaces. However, the consideration of dynamics uncertainty introduces essential\nnonlinearity and computational burden, posing unique challenges for analyzing\nand practically employing function approximation. Focusing on a basic setting\nwhere the nominal model and perturbed models are linearly parameterized, we\npropose minimax optimal and computationally efficient algorithms realizing\nfunction approximation and initiate the study on instance-dependent\nsuboptimality analysis in the context of robust offline RL. Our results uncover\nthat function approximation in robust offline RL is essentially distinct from\nand probably harder than that in standard offline RL. Our algorithms and\ntheoretical results crucially depend on a variety of new techniques, involving\na novel function approximation mechanism incorporating variance information, a\nnew procedure of suboptimality and estimation uncertainty decomposition, a\nquantification of the robust value function shrinkage, and a meticulously\ndesigned family of hard instances, which might be of independent interest.",
        "updated": "2024-03-14 17:55:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09621v1"
    },
    {
        "title": "Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey",
        "authors": "Xiaoyu LiuPaiheng XuJunda WuJiaxin YuanYifan YangYuhang ZhouFuxiao LiuTianrui GuanHaoliang WangTong YuJulian McAuleyWei AiFurong Huang",
        "links": "http://arxiv.org/abs/2403.09606v1",
        "entry_id": "http://arxiv.org/abs/2403.09606v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09606v1",
        "summary": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
        "updated": "2024-03-14 17:47:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09606v1"
    }
]