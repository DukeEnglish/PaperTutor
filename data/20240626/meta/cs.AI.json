[
    {
        "title": "EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data",
        "authors": "Jesse ZhangMinho HeoZuxin LiuErdem BiyikJoseph J LimYao LiuRasool Fakoor",
        "links": "http://arxiv.org/abs/2406.17768v1",
        "entry_id": "http://arxiv.org/abs/2406.17768v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17768v1",
        "summary": "Most reinforcement learning (RL) methods focus on learning optimal policies\nover low-level action spaces. While these methods can perform well in their\ntraining environments, they lack the flexibility to transfer to new tasks.\nInstead, RL agents that can act over useful, temporally extended skills rather\nthan low-level actions can learn new tasks more easily. Prior work in\nskill-based RL either requires expert supervision to define useful skills,\nwhich is hard to scale, or learns a skill-space from offline data with\nheuristics that limit the adaptability of the skills, making them difficult to\ntransfer during downstream RL. Our approach, EXTRACT, instead utilizes\npre-trained vision language models to extract a discrete set of semantically\nmeaningful skills from offline data, each of which is parameterized by\ncontinuous arguments, without human supervision. This skill parameterization\nallows robots to learn new tasks by only needing to learn when to select a\nspecific skill and how to modify its arguments for the specific task. We\ndemonstrate through experiments in sparse-reward, image-based, robot\nmanipulation environments that EXTRACT can more quickly learn new tasks than\nprior works, with major gains in sample efficiency and performance over prior\nskill-based RL. Website at https://www.jessezhang.net/projects/extract/.",
        "updated": "2024-06-25 17:50:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17768v1"
    },
    {
        "title": "BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning",
        "authors": "Ercong NieBo ShaoZifeng DingMingyang WangHelmut SchmidHinrich Schütze",
        "links": "http://arxiv.org/abs/2406.17764v1",
        "entry_id": "http://arxiv.org/abs/2406.17764v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17764v1",
        "summary": "Large language models (LLMs) possess extensive parametric knowledge, but this\nknowledge is difficult to update with new information because retraining is\nvery expensive and infeasible for closed-source models. Knowledge editing (KE)\nhas emerged as a viable solution for updating the knowledge of LLMs without\ncompromising their overall performance. On-the-fly KE methods, inspired by\nin-context learning (ICL), have shown great promise and allow LLMs to be\ntreated as black boxes. In the past, KE was primarily employed in English\ncontexts, whereas the potential for cross-lingual KE in current English-centric\nLLMs has not been fully explored. To foster more research in this direction, we\nintroduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverse\nlanguages across three KE task types. We also propose a gradient-free KE method\ncalled Multilingual In-context Knowledge Editing (MIKE) and evaluate it on\nBMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in terms\nof reliability, generality, locality, and portability, offering valuable\ninsights and a framework for future research in cross-lingual KE. Our code and\ndata are publicly accessible via the anonymous repository at\nhttps://anonymous.4open.science/r/MIKE.",
        "updated": "2024-06-25 17:48:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17764v1"
    },
    {
        "title": "DiffusionPDE: Generative PDE-Solving Under Partial Observation",
        "authors": "Jiahe HuangGuandao YangZichen WangJeong Joon Park",
        "links": "http://arxiv.org/abs/2406.17763v1",
        "entry_id": "http://arxiv.org/abs/2406.17763v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17763v1",
        "summary": "We introduce a general framework for solving partial differential equations\n(PDEs) using generative diffusion models. In particular, we focus on the\nscenarios where we do not have the full knowledge of the scene necessary to\napply classical solvers. Most existing forward or inverse PDE approaches\nperform poorly when the observations on the data or the underlying coefficients\nare incomplete, which is a common assumption for real-world measurements. In\nthis work, we propose DiffusionPDE that can simultaneously fill in the missing\ninformation and solve a PDE by modeling the joint distribution of the solution\nand coefficient spaces. We show that the learned generative priors lead to a\nversatile framework for accurately solving a wide range of PDEs under partial\nobservation, significantly outperforming the state-of-the-art methods for both\nforward and inverse directions.",
        "updated": "2024-06-25 17:48:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17763v1"
    },
    {
        "title": "Solving Hard Mizar Problems with Instantiation and Strategy Invention",
        "authors": "Jan JakubůvMikoláš JanotaJosef Urban",
        "links": "http://arxiv.org/abs/2406.17762v1",
        "entry_id": "http://arxiv.org/abs/2406.17762v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17762v1",
        "summary": "In this work, we prove over 3000 previously ATP-unproved Mizar/MPTP problems\nby using several ATP and AI methods, raising the number of ATP-solved Mizar\nproblems from 75\\% to above 80\\%. First, we start to experiment with the cvc5\nSMT solver which uses several instantiation-based heuristics that differ from\nthe superposition-based systems, that were previously applied to Mizar,and add\nmany new solutions. Then we use automated strategy invention to develop cvc5\nstrategies that largely improve cvc5's performance on the hard problems. In\nparticular, the best invented strategy solves over 14\\% more problems than the\nbest previously available cvc5 strategy. We also show that different\nclausification methods have a high impact on such instantiation-based methods,\nagain producing many new solutions. In total, the methods solve 3021 (21.3\\%)\nof the 14163 previously unsolved hard Mizar problems. This is a new milestone\nover the Mizar large-theory benchmark and a large strengthening of the hammer\nmethods for Mizar.",
        "updated": "2024-06-25 17:47:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17762v1"
    },
    {
        "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages",
        "authors": "Shane AroraMarzena KarpinskaHung-Ting ChenIpsita BhattacharjeeMohit IyyerEunsol Choi",
        "links": "http://arxiv.org/abs/2406.17761v1",
        "entry_id": "http://arxiv.org/abs/2406.17761v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17761v1",
        "summary": "Large language models (LLMs) are commonly used for long-form question\nanswering, which requires them to generate paragraph-length answers to complex\nquestions. While long-form QA has been well-studied in English via many\ndifferent datasets and evaluation metrics, this research has not been extended\nto cover most other languages. To bridge this gap, we introduce CaLMQA, a\ncollection of 2.6K complex questions spanning 23 languages, including\nunder-resourced, rarely-studied languages such as Fijian and Kirundi. Our\ndataset includes both naturally-occurring questions collected from community\nweb forums as well as questions written by native speakers, whom we hire for\nthis purpose. Our process yields diverse, complex questions that reflect\ncultural topics (e.g. traditions, laws, news) and the language usage of native\nspeakers. We conduct automatic evaluation across a suite of open- and\nclosed-source models using our novel metric CaLMScore, which detects incorrect\nlanguage and token repetitions in answers, and observe that the quality of\nLLM-generated answers degrades significantly for some low-resource languages.\nWe perform human evaluation on a subset of models and see that model\nperformance is significantly worse for culturally specific questions than for\nculturally agnostic questions. Our findings highlight the need for further\nresearch in LLM multilingual capabilities and non-English LFQA evaluation.",
        "updated": "2024-06-25 17:45:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17761v1"
    }
]