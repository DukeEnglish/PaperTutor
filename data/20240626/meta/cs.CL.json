[
    {
        "title": "BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning",
        "authors": "Ercong NieBo ShaoZifeng DingMingyang WangHelmut SchmidHinrich Schütze",
        "links": "http://arxiv.org/abs/2406.17764v1",
        "entry_id": "http://arxiv.org/abs/2406.17764v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17764v1",
        "summary": "Large language models (LLMs) possess extensive parametric knowledge, but this\nknowledge is difficult to update with new information because retraining is\nvery expensive and infeasible for closed-source models. Knowledge editing (KE)\nhas emerged as a viable solution for updating the knowledge of LLMs without\ncompromising their overall performance. On-the-fly KE methods, inspired by\nin-context learning (ICL), have shown great promise and allow LLMs to be\ntreated as black boxes. In the past, KE was primarily employed in English\ncontexts, whereas the potential for cross-lingual KE in current English-centric\nLLMs has not been fully explored. To foster more research in this direction, we\nintroduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverse\nlanguages across three KE task types. We also propose a gradient-free KE method\ncalled Multilingual In-context Knowledge Editing (MIKE) and evaluate it on\nBMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in terms\nof reliability, generality, locality, and portability, offering valuable\ninsights and a framework for future research in cross-lingual KE. Our code and\ndata are publicly accessible via the anonymous repository at\nhttps://anonymous.4open.science/r/MIKE.",
        "updated": "2024-06-25 17:48:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17764v1"
    },
    {
        "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages",
        "authors": "Shane AroraMarzena KarpinskaHung-Ting ChenIpsita BhattacharjeeMohit IyyerEunsol Choi",
        "links": "http://arxiv.org/abs/2406.17761v1",
        "entry_id": "http://arxiv.org/abs/2406.17761v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17761v1",
        "summary": "Large language models (LLMs) are commonly used for long-form question\nanswering, which requires them to generate paragraph-length answers to complex\nquestions. While long-form QA has been well-studied in English via many\ndifferent datasets and evaluation metrics, this research has not been extended\nto cover most other languages. To bridge this gap, we introduce CaLMQA, a\ncollection of 2.6K complex questions spanning 23 languages, including\nunder-resourced, rarely-studied languages such as Fijian and Kirundi. Our\ndataset includes both naturally-occurring questions collected from community\nweb forums as well as questions written by native speakers, whom we hire for\nthis purpose. Our process yields diverse, complex questions that reflect\ncultural topics (e.g. traditions, laws, news) and the language usage of native\nspeakers. We conduct automatic evaluation across a suite of open- and\nclosed-source models using our novel metric CaLMScore, which detects incorrect\nlanguage and token repetitions in answers, and observe that the quality of\nLLM-generated answers degrades significantly for some low-resource languages.\nWe perform human evaluation on a subset of models and see that model\nperformance is significantly worse for culturally specific questions than for\nculturally agnostic questions. Our findings highlight the need for further\nresearch in LLM multilingual capabilities and non-English LFQA evaluation.",
        "updated": "2024-06-25 17:45:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17761v1"
    },
    {
        "title": "Accelerating Clinical Evidence Synthesis with Large Language Models",
        "authors": "Zifeng WangLang CaoBenjamin DanekYichi ZhangQiao JinZhiyong LuJimeng Sun",
        "links": "http://arxiv.org/abs/2406.17755v1",
        "entry_id": "http://arxiv.org/abs/2406.17755v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17755v1",
        "summary": "Automatic medical discovery by AI is a dream of many. One step toward that\ngoal is to create an AI model to understand clinical studies and synthesize\nclinical evidence from the literature. Clinical evidence synthesis currently\nrelies on systematic reviews of clinical trials and retrospective analyses from\nmedical literature. However, the rapid expansion of publications presents\nchallenges in efficiently identifying, summarizing, and updating evidence. We\nintroduce TrialMind, a generative AI-based pipeline for conducting medical\nsystematic reviews, encompassing study search, screening, and data extraction\nphases. We utilize large language models (LLMs) to drive each pipeline\ncomponent while incorporating human expert oversight to minimize errors. To\nfacilitate evaluation, we also create a benchmark dataset TrialReviewBench, a\ncustom dataset with 870 annotated clinical studies from 25 meta-analysis papers\nacross various medical treatments. Our results demonstrate that TrialMind\nsignificantly improves the literature review process, achieving high recall\nrates (0.897-1.000) in study searching from over 20 million PubMed studies and\noutperforming traditional language model embeddings-based methods in screening\n(Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpasses\ndirect GPT-4 performance in result extraction, with accuracy ranging from 0.65\nto 0.84. We also support clinical evidence synthesis in forest plots, as\nvalidated by eight human annotators who preferred TrialMind over the GPT-4\nbaseline with a winning rate of 62.5%-100% across the involved reviews. Our\nfindings suggest that an LLM-based clinical evidence synthesis approach, such\nas TrialMind, can enable reliable and high-quality clinical evidence synthesis\nto improve clinical research efficiency.",
        "updated": "2024-06-25 17:41:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17755v1"
    },
    {
        "title": "Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language",
        "authors": "Amalie Brogaard PauliIsabelle AugensteinIra Assent",
        "links": "http://arxiv.org/abs/2406.17753v1",
        "entry_id": "http://arxiv.org/abs/2406.17753v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17753v1",
        "summary": "We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive text - both when explicitly instructed to rewrite text\nto be more or less persuasive and when only instructed to paraphrase. To this\nend, we construct a new dataset, Persuasive-Pairs, of pairs each consisting of\na short text and of a text rewritten by an LLM to amplify or diminish\npersuasive language. We multi-annotate the pairs on a relative scale for\npersuasive language. This data is not only a valuable resource in itself, but\nwe also show that it can be used to train a regression model to predict a score\nof persuasive language between text pairs. This model can score and benchmark\nnew LLMs across domains, thereby facilitating the comparison of different LLMs.\nFinally, we discuss effects observed for different system prompts. Notably, we\nfind that different 'personas' in the system prompt of LLaMA3 change the\npersuasive language in the text substantially, even when only instructed to\nparaphrase. These findings underscore the importance of investigating\npersuasive language in LLM generated text.",
        "updated": "2024-06-25 17:40:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17753v1"
    },
    {
        "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon",
        "authors": "USVSN Sai PrashanthAlvin DengKyle O'BrienJyothir S VMohammad Aflah KhanJaydeep BorkarChristopher A. Choquette-ChooJacob Ray FuehneStella BidermanTracy KeKatherine LeeNaomi Saphra",
        "links": "http://arxiv.org/abs/2406.17746v1",
        "entry_id": "http://arxiv.org/abs/2406.17746v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17746v1",
        "summary": "Memorization in language models is typically treated as a homogenous\nphenomenon, neglecting the specifics of the memorized data. We instead model\nmemorization as the effect of a set of complex factors that describe each\nsample and relate it to the model and corpus. To build intuition around these\nfactors, we break memorization down into a taxonomy: recitation of highly\nduplicated sequences, reconstruction of inherently predictable sequences, and\nrecollection of sequences that are neither. We demonstrate the usefulness of\nour taxonomy by using it to construct a predictive model for memorization. By\nanalyzing dependencies and inspecting the weights of the predictive model, we\nfind that different factors influence the likelihood of memorization\ndifferently depending on the taxonomic category.",
        "updated": "2024-06-25 17:32:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17746v1"
    }
]