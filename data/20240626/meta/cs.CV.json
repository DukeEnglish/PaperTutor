[
    {
        "title": "Text-Animator: Controllable Visual Text Video Generation",
        "authors": "Lin LiuQuande LiuShengju QianYuan ZhouWengang ZhouHouqiang LiLingxi XieQi Tian",
        "links": "http://arxiv.org/abs/2406.17777v1",
        "entry_id": "http://arxiv.org/abs/2406.17777v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17777v1",
        "summary": "Video generation is a challenging yet pivotal task in various industries,\nsuch as gaming, e-commerce, and advertising. One significant unresolved aspect\nwithin T2V is the effective visualization of text within generated videos.\nDespite the progress achieved in Text-to-Video~(T2V) generation, current\nmethods still cannot effectively visualize texts in videos directly, as they\nmainly focus on summarizing semantic scene information, understanding, and\ndepicting actions. While recent advances in image-level visual text generation\nshow promise, transitioning these techniques into the video domain faces\nproblems, notably in preserving textual fidelity and motion coherence. In this\npaper, we propose an innovative approach termed Text-Animator for visual text\nvideo generation. Text-Animator contains a text embedding injection module to\nprecisely depict the structures of visual text in generated videos. Besides, we\ndevelop a camera control module and a text refinement module to improve the\nstability of generated visual text by controlling the camera movement as well\nas the motion of visualized text. Quantitative and qualitative experimental\nresults demonstrate the superiority of our approach to the accuracy of\ngenerated visual text over state-of-the-art video generation methods. The\nproject page can be found at https://laulampaul.github.io/text-animator.html.",
        "updated": "2024-06-25 17:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17777v1"
    },
    {
        "title": "Fast and Uncertainty-Aware SVBRDF Recovery from Multi-View Capture using Frequency Domain Analysis",
        "authors": "Ruben WiersmaJulien PhilipMiloš HašanKrishna MulliaFujun LuanElmar EisemannValentin Deschaintre",
        "links": "http://arxiv.org/abs/2406.17774v1",
        "entry_id": "http://arxiv.org/abs/2406.17774v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17774v1",
        "summary": "Relightable object acquisition is a key challenge in simplifying digital\nasset creation. Complete reconstruction of an object typically requires\ncapturing hundreds to thousands of photographs under controlled illumination,\nwith specialized equipment. The recent progress in differentiable rendering\nimproved the quality and accessibility of inverse rendering optimization.\nNevertheless, under uncontrolled illumination and unstructured viewpoints,\nthere is no guarantee that the observations contain enough information to\nreconstruct the appearance properties of the captured object.\n  We thus propose to consider the acquisition process from a signal-processing\nperspective. Given an object's geometry and a lighting environment, we estimate\nthe properties of the materials on the object's surface in seconds. We do so by\nleveraging frequency domain analysis, considering the recovery of material\nproperties as a deconvolution, enabling fast error estimation. We then quantify\nthe uncertainty of the estimation, based on the available data, highlighting\nthe areas for which priors or additional samples would be required for improved\nacquisition quality. We compare our approach to previous work and\nquantitatively evaluate our results, showing similar quality as previous work\nin a fraction of the time, and providing key information about the certainty of\nthe results.",
        "updated": "2024-06-25 17:59:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17774v1"
    },
    {
        "title": "MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning",
        "authors": "Xiangyu ZhaoXiangtai LiHaodong DuanHaian HuangYining LiKai ChenHua Yang",
        "links": "http://arxiv.org/abs/2406.17770v1",
        "entry_id": "http://arxiv.org/abs/2406.17770v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17770v1",
        "summary": "Multi-modal large language models (MLLMs) have made significant strides in\nvarious visual understanding tasks. However, the majority of these models are\nconstrained to process low-resolution images, which limits their effectiveness\nin perception tasks that necessitate detailed visual information. In our study,\nwe present MG-LLaVA, an innovative MLLM that enhances the model's visual\nprocessing capabilities by incorporating a multi-granularity vision flow, which\nincludes low-resolution, high-resolution, and object-centric features. We\npropose the integration of an additional high-resolution visual encoder to\ncapture fine-grained details, which are then fused with base visual features\nthrough a Conv-Gate fusion network. To further refine the model's object\nrecognition abilities, we incorporate object-level features derived from\nbounding boxes identified by offline detectors. Being trained solely on\npublicly available multimodal data through instruction tuning, MG-LLaVA\ndemonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide\nvariety of language encoders, ranging from 3.8B to 34B, to evaluate the model's\nperformance comprehensively. Extensive evaluations across multiple benchmarks\ndemonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code will be available at\nhttps://github.com/PhoenixZ810/MG-LLaVA.",
        "updated": "2024-06-25 17:55:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17770v1"
    },
    {
        "title": "DiffusionPDE: Generative PDE-Solving Under Partial Observation",
        "authors": "Jiahe HuangGuandao YangZichen WangJeong Joon Park",
        "links": "http://arxiv.org/abs/2406.17763v1",
        "entry_id": "http://arxiv.org/abs/2406.17763v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17763v1",
        "summary": "We introduce a general framework for solving partial differential equations\n(PDEs) using generative diffusion models. In particular, we focus on the\nscenarios where we do not have the full knowledge of the scene necessary to\napply classical solvers. Most existing forward or inverse PDE approaches\nperform poorly when the observations on the data or the underlying coefficients\nare incomplete, which is a common assumption for real-world measurements. In\nthis work, we propose DiffusionPDE that can simultaneously fill in the missing\ninformation and solve a PDE by modeling the joint distribution of the solution\nand coefficient spaces. We show that the learned generative priors lead to a\nversatile framework for accurately solving a wide range of PDEs under partial\nobservation, significantly outperforming the state-of-the-art methods for both\nforward and inverse directions.",
        "updated": "2024-06-25 17:48:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17763v1"
    },
    {
        "title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation",
        "authors": "Jianzong WuXiangtai LiYanhong ZengJiangning ZhangQianyu ZhouYining LiYunhai TongKai Chen",
        "links": "http://arxiv.org/abs/2406.17758v1",
        "entry_id": "http://arxiv.org/abs/2406.17758v1",
        "pdf_url": "http://arxiv.org/pdf/2406.17758v1",
        "summary": "In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth",
        "updated": "2024-06-25 17:42:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.17758v1"
    }
]