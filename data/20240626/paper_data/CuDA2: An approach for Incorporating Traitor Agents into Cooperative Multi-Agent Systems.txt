CuDA2: An approach for Incorporating Traitor
Agents into Cooperative Multi-Agent Systems
Zhen Chen, Yong Liao, Youpeng Zhao, Zipeng Dai, Jian Zhao
Abstract—Cooperative Multi-Agent Reinforcement Learning typically an opponent of the victim agents. However, these
(CMARL) strategies are well known to be vulnerable to ad- attack methods necessitate full access to and control over
versarial perturbations. Previous works on adversarial attacks
the environment or agents, requiring advanced hacking skills
haveprimarilyfocusedonwhite-boxattacksthatdirectlyperturb
to modify the server or the real/simulated environment. In
the states or actions of victim agents, often in scenarios with a
limited number of attacks. However, gaining complete access to light of these limitations, we propose a more practical attack
victimagentsinreal-worldenvironmentsisexceedinglydifficult. method: incorporating traitor agents into cooperative multi-
To create more realistic adversarial attacks, we introduce a agent systems. For instance, in a soccer game, an agent could
novel method that involves injecting traitor agents into the
be introduced that deliberately plays poorly, or in a mobile
CMARL system. We model this problem as a Traitor Markov
base station environment, a base station could be introduced
Decision Process (TMDP), where traitors cannot directly attack
thevictimagentsbutcaninfluencetheirformationorpositioning to interfere with the connection signals of other base stations.
throughcollisions.InTMDP,traitorsaretrainedusingthesame Thisapproachdoesnotrequiremodifyingordirectlyoperating
MARLalgorithmasthevictimagents,withtheirrewardfunction theenvironmentorthevictimagents,makingitamorefeasible
set as the negative of the victim agents’ reward. Despite this,
and realistic adversarial strategy.
the training efficiency for traitors remains low because it is
In response to this type of attack, we design a CMARL
challenging for them to directly associate their actions with
the victim agents’ rewards. To address this issue, we propose scenario involving traitors to indirectly target victim agents.
the Curiosity-Driven Adversarial Attack (CuDA2) framework. Specifically, we model the problem as a Traitor Markov
CuDA2 enhances the efficiency and aggressiveness of attacks on Decision Process (TMDP), where traitors and victim agents
thespecifiedvictimagents’policieswhilemaintainingtheoptimal
are on the same team but have opposing objectives. In this
policy invariance of the traitors. Specifically, we employ a pre-
setup, traitors cannot directly attack the victim agents but
trained Random Network Distillation (RND) module, where the
extra reward generated by the RND module encourages traitors can influence their observations by maneuvering or colliding
to explore states unencountered by the victim agents. Extensive with them. The success of adversarial policies stems from
experiments on various scenarios from SMAC demonstrate that the ability of an attacker to manipulate the victim agents’
ourCuDA2frameworkofferscomparableorsuperioradversarial
observations by taking unconventional actions, leading the
attack capabilities compared to other baselines.
game into unfamiliar states. This often causes the victim
agents to exhibit undesired, sub-optimal behaviors. Thus, the
I. INTRODUCTION
effectiveness of an attack may be significantly influenced by
Cooperative Multi-Agent Reinforcement Learning theattacker’scapabilitytoexploresuchstatesandexploitthese
(CMARL) has recently garnered significant attention [1], [2], vulnerabilities.
[3], finding applications in diverse areas such as autonomous This insight motivates us to propose a Curiosity-Driven
vehicle teams [4], multi-agent pathfinding [5], multi-UAV Adversarial Attack (CuDA2) framework, which employs a
control[6],anddynamicalgorithmconfiguration[7].Existing Random Network Distillation (RND) module to characterize
CMARL methods primarily address challenges like non- the novelty of the victim agents’ states. Specifically, we first
stationarity [8], credit assignment [9], and scalability, all pre-train an RND module in an environment where traitors
aimed at enhancing coordination in complex scenarios [10]. take random actions. This pre-training aims to provide an
Both value-based methods [11], [12] and policy gradient- intrinsic reward through the RND module when the traitors’
based methods [13], [14] have shown significant coordination actions cause significant displacement of the victim agents,
capabilities across a variety of tasks, such as SMAC [15] and thereby guiding the traitors to more effectively attack the vic-
Hanabi [16]. tim agents. To address the issue that additional rewards might
WhileCMARLhasdemonstratedremarkablesuccessacross alter the traitors’ optimal policy, we use the RND module
various domains, it shares a vulnerability with Single-Agent as a potential function and apply the dynamic potential-based
Reinforcement Learning (SARL) [17] to adversarial attacks. rewardshapingmethodtogenerateintrinsicrewardsduringthe
For example, studies such as [18], [19] explore imple- traitors’ training. We theoretically prove that this combination
menting attacks with a limited number of attempts by in- can ensure the invariance of the traitors’ optimal policy.
jecting adversarial samples at critical moments to cause the To evaluate the proposed method, we conducted extensive
most severe damage to the agent. Research by [20], [21] experiments on multiple SMAC maps with varying numbers
investigates poisoning attacks on multi-agent reinforcement of traitors and compared CuDA2 with several baselines.
learners, assuming the attacker controls one of the learners, Empirical results demonstrate that the CuDA2 framework
4202
nuJ
52
]GL.sc[
1v52471.6042:viXrasignificantlyenhancestheattackanddisruptioncapabilitiesof Learning (DRL), attackers manipulate input data during train-
the traitors. Additionally, we performed ablation studies and ing, introducing prediction biases into the model. The goal
visualizationexperiments.Theresultsindicatethatourmethod of adversarial machine learning is to enhance robustness and
more effectively reduces the win rate of the victim agents and security by examining potential attacks and threats. These
achieves curiosity-driven adversarial attacks more efficiently approaches are fundamentally similar to attacks on DRL.
compared to algorithms that solely use the RND module. We This paper specifically examines the vulnerability of the
provide the CMARL community with a new, more practical DRL model, primarily through adversarial machine learning
attack method, and defending against this type of attack can methods.
enhance the robustness and security of CMARL. Adversarial attacks in DRL can be categorized into
reward-based attacks [32], strategy-based attacks [33], [34],
II. RELATEDWORK
observation-based attacks [35], [36], [19], environment-based
A. Multi-Agent Reinforcement Learning (MARL) attacks [37], [38], and action-based attacks [39] according
In recent year, there exist significant research progress [1], to their algorithmic principles. Reward-based attacks involve
[22] in MARL. Numerous methods have emerged as effective altering the reward signal from the environment, either by
strategies for promoting coordination among agents, which changing the reward value’s sign or replacing the original
cangenerallybecategorizedintopolicy-basedandvalue-based rewardfunctionwithanadversarialone.Strategy-basedattacks
methods. Examples of policy gradient-based methods that fo- useadversarialagentstogeneratestatesandactionsbeyondthe
cusonoptimizingmulti-agentpoliciesincludeMADDPG[23], victim agent’s comprehension, causing disarray. Observation-
COMA[13],DOP[24],andMAPPO[16].MADDPGutilizes based attacks involve adding perturbations to the observed
the CTDE (Centralized Training Decentralized Execution) image, compelling the victim agent to take actions desired
paradigm to train policies and refine them via DDPG [25]. by the attacker, typically by perturbing the agent’s image
COMA also uses a centralized critic for policy optimization sensor. Environment-based attacks modify the agent’s training
but incorporates a counterfactual model to determine each environment directly, altering the dynamic model or adding
agent’s marginal contribution in a multi-agent system. DOP obstacles. Action-based attacks directly modify action outputs
advances this approach by employing a centralized linear by changing the action space in the training data.
mixingnetworktobreakdownglobalrewardsinacooperative In previous works on observation-based and action-based
system,significantlyenhancingtheperformanceofMADDPG attacks, attackers added appropriate perturbations to observa-
and COMA. Recently, MAPPO has applied the widely val- tion images or actions over a period of time to mislead the
idated proximal policy optimization technique from single- victimagentintomakingincorrectdecisions[40],[18].While
agent reinforcement learning to the MARL domain. ensuring the stealth of the attack, they aimed to minimize the
Another branch of MARL methods, called value-based cumulativerewardandreducetheoverallteam’sgains.Current
approaches, primarily concentrates on the factorization of the research mainly focuses on achieving more efficient attacks
value function. VDN [11] aims to break down the team underlimitedattackopportunities.However,theseworkshave
value function into individual agent values using a simple an important premise: the ability to obtain all permissions of
additive factorization. Adhering to the Individual-Global-Max the victim agent, including control over state actions. This
(IGM) principle [26], QMIX [12] enhances value function means their attack methods are white-box attacks, which are
decomposition by employing a non-linear mixing network known to be difficult to implement in real-world scenarios.
to approximate a monotonic function value decomposition. Therefore, in this work, we adopt traitor setting to attack
QPLEX [27] utilizes a duplex dueling network architecture to cooperative multi-agent scenarios. By training traitor agents,
factorize the joint value function, fully exploiting the expres- we aim to minimize the cumulative reward of the team.
sive power of IGM. Research by [9] conducted a theoretical
analysis of IGM by applying a multi-agent fitted Q-iteration III. BACKGROUND
algorithm. This paper primarily uses QMIX, MAPPO and
A. Markov Decision Process (MDP)
VDN as the main algorithms for the experiments.
AstandardMarkovDecisionProcess(MDP)canbedefined
B. Adversarial Attacks on MARL
as a tuple (S, A, P, R, γ) where s ∈ S is the state space,
Adversarial attacks involve the intentional manipulation of
a∈A is the action space, P(s′ |s,a):S×A×S →[0,1] is
machine learning models by attackers using specially crafted the transition probability, R(s,a):S×A→R is the reward
input samples to deceive or mislead the models. As deep
functionandγ ∈[0,1)isthediscountfactor,whichrepresents
learning rapidly advances, attackers are continuously devel-
the preference for immediate reward over long-term reward.
oping new attack methods, such as poisoning attacks [28],
The agent’s goal is to find the policy π∗ which at any given
adversarialmachinelearning[29],andothertechnologies[30],
s ∈S maximizes the expected discounted sum of rewards,
n
[31], making it increasingly challenging to detect and defend
against these attacks. Poisoning attacks typically occur during (cid:34) K (cid:35)
(cid:88)
the training phase, compromising performance and reliabil- π∗ =argmaxE γnR(s ,a ) , (1)
π n n
ity through harmful data. Similarly, in Deep Reinforcement π n=0where K is the number of time steps in each episode. K can A. Traitors Optimization Objective
be either finite or infinite, depending on whether we are using
We extend the MDP model to include an action selection
an environment with finite or infinite horizons. function for traitors P (a|π ,s).
T T
(cid:34) K (cid:35)
(cid:88)
Q(s,a)=E π γnR(s n,a n)|s 0 =s,a 0 =a . (2) Definition 1. A Traitor Markov Decision Process (TMDP)
n=0 is a tuple M˜ = (S,A,P ,P ,R,γ) where M =
V T
The Q-function Q(s,a) estimates how good it is to perform (S,A,P ,R,γ) is an MDP and the transition probability
V
an action in a state [41], given the policy π. for victim agents is the special case in which the policy is
applied without modification: P (a | π ,s) = π (a | s).
B. Potential Based Reward Shaping V V V
P (a | π ,s) = Pr(A = a | Π = π ,S = s) is the
T T n T n
Reward shaping involves enhancing the original reward probability that action a is selected in state s given a policy
function by incorporating domain-specific knowledge. This π. We also write M˜ =(M,P ).
A
process typically uses an additive form of reward shaping.
Formally,itcanbeexpressedasr′ =r+F,whererrepresents In the TMDP, π is a fixed, non-updating policy of the
V
theoriginalrewardfunction,F istheshapingrewardfunction, victim agents. R(s ,a ) is the reward obtained by the vic-
n n
and r′ denotes the modified reward function. Early studies tim agents in the environment. The traitors’ objective is to
on reward shaping [42], [43] focused on the design of the minimize the victim agents’ reward, so its reward function is
shaping reward function F but overlooked the possibility R = −R(s ,a ). An optimal traitor policy for a TMDP is
T n n
that these shaping rewards might alter the optimal policy. In one that maximizes the expected return:
other words, reward shaping might lead to the phenomenon
(cid:34) K (cid:35)
of reward hacking, causing the agent to develop suboptimal (cid:88)
π∗ =argmaxE −R(s ,a )γn , (6)
strategies. Although [44], [45] have attempted to mitigate this T π n n
π
n=0
issue by avoiding repeated extra rewards, these approaches
where actions are sampled according to P (a|π ,s).
tend to address the symptoms rather than the root cause. T T
Potential-Based Reward Shaping (PBRS) [46] was the first B. Pre-training
method to ensure the policy invariance property. Specifically,
Beforestartingtotrainthetraitors’policyπ ,wefirstneed
PBRS defines F as the difference between potential values: T
to pre-train a victim agents’ policy π to serve as our attack
V
F(s ,s )=γΦ(s )−Φ(s ), (3) target.Additionally,toachievemoreeffectiveattacks,wealso
n n+1 n+1 n
need to pre-train an RND module.
where Φ(s) : S → R is a potential function that provides
1) Victim Agents: As shown in Fig.1, the green box repre-
insights into the states. Notable variations of PBRS include
sents the pre-training process of the victim agents. The envi-
the potential-based advice approach:
ronmentusedis6m-vs-6mor8m-vs-8m,wherethenumberof
F(s ,a ,s ,a )=γΦ(s ,a )−Φ(s ,a ), (4) alliedagentsandenemiesisequal,andnotraitorisintroduced
n n n+1 n+1 n+1 n+1 n n
yet. After a period of training, we will obtain a well-trained
which extends Φ over the state-action space for action ad-
policy π which can defeat all the enemies while minimizing
V
vice [47], the dynamic PBRS approach:
its own losses and then we will save its network parameters.
F(s n,t n,s n+1,t n+1)=γΦ(s n+1,t n+1)−Φ(s n,t n), (5) (cid:34) K (cid:35)
(cid:88)
π∗ =argmaxE R(s ,a )γn . (7)
which incorporates a time parameter into Φ to allow for V π n n
π
dynamic potentials [48], and the dynamic potential-based n=0
advice approach that learns an auxiliary value function to The environment used is (6+N)m-vs-6m or (8+N)m-vs-8m,
transform any rewards into potentials [49]. whereNisthenumberoftraitors.Duringthetraitors’training,
we will load this policy, and the victim agents will generate
IV. METHOD
actions based on it. The traitors’ attack objective is to disrupt
Different from previous work based on observation and the victim agents’ policy, making it ineffective against the
action attacks, this paper introduces traitors into the training enemies and thus reducing its win rate.
framework of the existing agents. The traitors belong to 2) RND module: RND is a method used to measure the
the victim agents’ side but aims to minimize the team’s noveltyofstatesencounteredbyanagentduringtraining[51].
win rate. Under this setting, as shown in Fig.1, we propose The core idea involves two neural networks: a target network
theCuriosity-DrivenAdversarialAttacks(CuDA2)framework and a predictor network. The target network is randomly
to train the traitors. By pre-training the Random Network initialized at the beginning and remains fixed. It maps an
Distillation (RND) module [50], it can efficiently conduct observation from the environment to an embedding space,
adversarial attacks on the victim agents. Combining RND represented mathematically as f :O →Rk, where O denotes
with the dynamic PBRS method, we also theoretically prove thesetofobservationsandRk isthek-dimensionalembedding
that curiosity-driven adversarial attacks within the CuDA2 space. The predictor network, denoted by fˆ, is trained to ap-
framework do not alter the traitors’ optimal policy. proximatetheoutputofthetargetnetwork.ItisparameterizedN Traitors γR –R
i+1 i
RND Pretrain RND
Policy
Select Action Target_net N Traitors
Random Action
A -R
T
R (y_true –y_pred)2
i S Environment
Environment t
(6+N)m_vs_6m
Model_net
(6+N)m_vs_6m update
S and S
t t+1
S
t
A R and S
V Victim Agents Agents t
Load Model Environment
Policy Policy
6m_vs_6m
Select Action Select Action
A
t
Fig. 1: CuDA2 framework. First, as indicated by the green box in the figure, we need to define the target that the traitors intend to attack:
pre-training and saving a model of the victim agents. Second, as shown by the gray box in the figure, we also need to pre-train the RND
module within the strategy where the traitors take random actions. This can reduce the prediction error caused by the state changes of the
victim agents resulting from the traitors’ random actions. Finally, before training the traitors, we will load the victim agents model. During
the training process, we use the pre-trained RND module as a potential function to provide the traitors with intrinsic rewards through the
dynamic PBRS method.
by θ and maps observations to the same embedding space: noveltychangesdeterminedbytheRNDmodule.Specifically,
fˆ
fˆ:O →Rk. The predictor network is trained using gradient we get the corresponding outputs R i+1 and R i by inputting
descent to minimize the mean squared error (MSE) between the current state and the next state to the RND module and
its output and the target network’s output, formulated as: computeγR i+1−R iasintrinsicreward.Thetraitors’rewardis
shaped by adding the intrinsic reward to the negative victim’s
MSE=E[∥fˆ(x;θ)−f(x)∥2], (8) reward, promoting the exploration of states detrimental to
the victims. To be noted, classic RND techniques typically
where x represents the observations.
adopt R directly to encourage exploration, which may lead
The essence of RND is that the predictor network will i
to reward hacking and impede the learning of an optimal
perform poorly on novel states (states it has not encountered
policy. In contrast, we utilize a dynamic PBRS method to
before) because it has not had the chance to learn these states
ensure policy invariance, thus helping the traitor agents to
during training. This results in a higher prediction error for
grasp optimal policy.
novel states compared to familiar states. Consequently, the
prediction error from the RND can be used as an intrinsic The transition (s n,a T,s n+1,r T)(current state, traitors’ ac-
reward signal to encourage the agent to explore new and tion, next state, traitors’ reward) is stored in a replay buffer,
unseenareasofthestatespace.AsshowninFig.1,theyellow and random samples from this buffer are used to periodically
line within the gray box represents the RND pre-training updatethetraitors’policy,optimizingtheiractionstominimize
process. the victim agents’ rewards. At the end of each time step,
we will update the RND model with current state s . This
n
C. CuDA2 Framework approach enables traitors to learn strategies that maximize
As shown in Algorithm CuDA2 and Fig.1, the process of the impact and disruption of victim agents through intrinsic
our framework begins with pre-training the policy of victim reward-driven exploration and exploitation without changing
agents and the RND module. During the traitors’ training the optimal policy.
phase, each episode starts by resetting the environment to an
initialstate.Ateachtimestep,actionsa anda aresampled
V T D. Optimal Policy Invariance Theory Analysis
separately for victim agents and traitors from their respective
policiesπ andπ .Theseactionsarecombinedandexecuted, To prove that the reward shaping of the CuDA2 framework
V T
resulting in a new state and a reward for the victim agents. can maintain the optimal policy, let us consider the return U
i
The intrinsic reward for traitors is calculated based on state for any arbitrary agent i when experiencing sequence s¯ in aAlgorithm CuDA2 The return of the shaped agent U experiencing the same
i,F
Require: number of steps for each episode K, the policy of sequence s¯is:
victim agents π , the policy of traitors π , the intrinsic
V T K
(cid:88)
reward from random network distillation module RND, U (s¯)= γj−nr′
i,F j,i
replay buffer B
j=n
\\pre-train the RND module
K
(cid:88)
for each episode do = γj−n(r +F(s ,t ,s ,t ))
j,i j j j+1 j+1
for n←0→K do
j=n
sample a n ∼Uniform(a n) K
(cid:88)
sample s n+1 ∼p(s n+1 |s n,a n) = γj−n(r j,i+γRND(s j+1,t j+1)−RND(s j,t j))
update the model network of RND using s
n j=n
end for K K
(cid:88) (cid:88)
end for = γj−nr + γj−n+1RND(s ,t )
j,i j+1 j+1
\\train the policy of traitors j=n j=n
for each episode do K
(cid:88)
for n←0→K do − γj−nRND(s ,t )
j j
sample a V ∼π V(a V |s n) j=n
sample a T ∼π T(a T |s n) =U i(s¯)−RND(s n,t n)+γK−n+1RND(s K+1,t K+1)
s ,r ,done←env.step([a ,a ])
n+1 V V T =U (s¯)−∥fˆ(s ;θ )−f(s )∥2. (14)
if Not done then i n tn n
r shape ←γRND(s n+1,t n+1)−RND(s n,t n) In the above equation, it is important to note that if K
else approaches infinity, then γK−n+1RND(s ,t ) can be
K+1 K+1
r shape ←0−RND(s n,t n) ignored.However,ifthisisanepisodicreinforcementlearning
end if task (game, etc), where K is finite and s is a terminal
K+1
r T ←−r V +r shape state, the output of RND module (RND(s K+1,t K+1)) needs
store transition (s n,a T,s n+1,r T) in B to be replaced with 0. Otherwise, the potential function of the
update the model network of RND using s n terminal state will affect the policy learned by the agent [52].
sample random minibatch of transitions from B By combining Eq.(10) and Eq.(14) we know the shaped
update the policy of traitors π T Q-function is:
end for (cid:88)
Q∗(s ,a )= Pr(s¯|s ,a )U (s¯)
end for i n n n n i,F
s¯
=(cid:88) Pr(s¯|s ,a )(U (s¯)−∥fˆ(s ;θ )−f(s )∥2)
n n i n tn n
discounted framework without shaping. Formally: s¯
(cid:88)
= Pr(s¯|s ,a )U (s¯)
K n n i
(cid:88)
U (s¯)= γj−nr , (9) s¯
i j,i −(cid:88) Pr(s¯|s ,a )∥fˆ(s ;θ )−f(s )∥2
j=n n n n tn n
wherer istherewardreceivedattimej byagentifromthe s¯
j,i =Q (s ,a )−∥fˆ(s ;θ )−f(s )∥2, (15)
environment.Giventhisdefinitionofreturn,thetrueQ-values i n n n tn n
can be defined formally by: where t is the current time.
n
(cid:88) Therefore, any policy that optimizes Q (s ,a ) also op-
Q (s ,a )= Pr(s¯|s ,a )U (s¯). (10) i n n
i n n n n i timizes Q∗(s ,a ). Since ∥fˆ(s ;θ ) − f(s )∥2 does not
s¯ i n n n tn n
depend on the action chosen, which means the choice of
AccordingtoEq.(5),wenowconsiderthesameagentbutwith
the optimal action in the current state is not affected by the
a reward function modified by adding a dynamic potential-
valueofthisextrafunction.TherewardshapedbytheCuDA2
based reward function of the form given below:
framework will not change the optimal policy.
F(s ,t ,s ,t )=γRND(s ,t )
n n n+1 n+1 n+1 n+1 V. EXPERIMENTALSETUP
−RND(s ,t ), (11)
n n A. Environments
where the RND function is: We conduct our experiments on SMAC which is a widely
adopted environment for research in the field of cooperative
RND(s ,t )=∥fˆ(s ;θ )−f(s )∥2. (12)
n n n tn n multi-agent reinforcement learning based on Blizzard’s Star-
The shaped reward function r′ is: Craft II RTS game [15]. SMAC is composed of many combat
scenarios with pre-configured maps, where we train the ally
r′ =r +F(s ,t ,s ,t ). (13)
j,i j,i j j j+1 j+1 units to beat enemy units controlled by the built-in AI withx y input (s ) 2×L units
n
……
fc 128 units
ReLU
Fig. 2: (6+2)m-vs-6m Map in StarCraft II. We customize a map to
train the traitors, where the two traitors are circled in red, the six
victimagentsarecircledingreen,andthesixenemiesarecircledin
blue. The traitors’ goal is to reduce the win rate of victim agents.
……
an unknown strategy. At each timestep, agents can move or
attack any enemies and receive a global reward equal to the fc 128 units
total damage done to enemy units.
ReLU
Different from the original 8m-vs-8m, 5m-vs-6m and other
SMACmaps,wehavedesignedtwonewsetsofenvironments:
(6+N)m-vs-6m and (8+N)m-vs-8m, where N represents the
numberoftraitorsandcanbe1,2,or3.AsshowninFig.2,in
theoriginal6m-vs-6menvironment,wehaveinserted2traitors
…… output 64 units
whose allegiance is with the victim agents’ side. The traitors
cannot directly attack the victim agents but can disrupt their
formation through collisions. At each moment, the traitors Fig.3:DeepneuralnetworkarchitectureforRND.N isthenumber
receive a global reward equal to the negative reward of the of victim agents. This architecture is used in both our method and
the ablation experiments.
victim agents, meaning that the traitors’ goal is to minimize
the losses of the enemy.
D. RND Architecture
B. MARL Algorithms
The Random Network Distillation (RND) architecture is
We used three MARL algorithms for our experiments:
to develop a method that calculates curiosity but that is not
QMIX, MAPPO and VDN. Among these, QMIX and VDN
attracted by the stochastic elements of an environment. The
arevalue-basedMARLalgorithms,whileMAPPOisapolicy-
RND module used in this paper is shown in Fig.3. The input
based algorithm. QMIX and VDN are trained for 2,050,000
S is the xy-coordinate of the victim agents, which feeds
n
steps, and MAPPO is trained for 20,050,000 steps. The net-
into the first fully connected (fc) layer consisting of 128 units
workarchitectureandthehyperparametersofQMIX,MAPPO
and utilizing the ReLU activation function to introduce non-
and VDN are same as that in [53], which is a benchmark in
linearity. This is followed by a second fully connected layer,
cooperative MARL tasks. In VI-A1, we compare the results
also with 128 units and ReLU activation, further transforming
of our method with the baselines under these three different
the data. Finally, the processed data is passed through an
algorithms. The algorithm used for all other experiments is
output layer comprising 64 units, which produces the final
QMIX.
output.
C. Baseline Methods E. Training Details
Under the fixed MARL algorithm of the victim agents, we Before training the traitors in the CuDA2 framework, we
compared our CuDA2 framework’s adversarial attack method need to pre-train the RND module. We perform the pre-
with three baselines: stop, where traitors remain stationary training in the baseline where the traitors adopt a random
during the attack; random, where traitors perform random action strategy V-C. This pre-training offers two benefits:
actions; and minus r, where traitors use the same MARL first, it enhances the RND module’s sensitivity to unknown
algorithm as the victim agents but with a reward function states, and second, it reduces the extra reward generated
that is the negative of the victim agents’ reward function. by state changes caused by random actions. Additionally,
Our method builds on the minus r reward function by adding during the training process in the CuDA2 framework, we also
an additional reward through the RND module to incentivize update the parameters of the model network within the RND
traitors to develop more aggressive attack strategies. We run module, allowing it to evolve alongside the traitors’ strategy
each set of experiments five times. updates. The state obtained from the environment needs to be1.00 1.00 1.00
0.95 0.95 0.95
0.90 0.90 0.90
0.85 0.85 0.85
0.80 0.80 0.80
0.75 0.75 0.75
0.70 stop 0.70 stop 0.70 stop
random random random
0.65 minus_r 0.65 minus_r 0.65 minus_r
CuDA2 CuDA2 CuDA2
no_traitor no_traitor no_traitor
0.60 0.60 0.60
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 250 500 750 1000 1250 1500 1750
Frames(10^4) Frames(10^4) Frames(10^4)
(a) VDN (b) QMIX (c) MAPPO
Fig. 4: We test our method under different MARL algorithms in (6+1)m-vs-6m maps in comparison to the baseline method.
(a) stop. (b) random.
(c) minus r. (d) CuDA2.
Fig.5:Snapshotsofourmethodandbaselinemethod.(a)Thetraitorsremainstationary.(b)Thetraitorstakerandomactions.(c)Thetraitors
are trained using the same algorithm as the victim agents, with their reward function being the negative of the victim agents’ reward. (d)
the traitors receive extra rewards provided by the CuDA2 framework.
trimmedtoonlyretainthepositionalinformation.Thetrimmed VDN). The baselines include stop, random, and minus r, as
parts include the agents’ health and shield information, which defined in V-C. Then, to qualitatively analyze the impact of
gradually decrease over time. These values range from 0 the number of traitors and the ratio of traitors to allies on
to health or shield in any method, resulting in a the performance of our method and the baselines, we design
max max
consistent distribution across all methods with no variability. twoexperimentalenvironments:6m-vs-6mand8m-vs-8mand
For the RND module, this information constitutes noise. We evaluate the impact on the allies’ win rate and the number of
need to remove this extraneous information to make the RND allied deaths resulted from adding 1, 2, or 3 traitors.
module more sensitive to unknown states.
1) Different MARL Algorithms: As shown in Fig.4, after
VI. EXPERIMENTS
introducingatraitoragentintotheVDN,QMIX,andMAPPO
In this section, we first compare the results between our algorithms,wecomparedthedecreaseinthewinratesofallies
method and baselines. Then, we analyze the impact of each between our method and the baselines. The dashed line repre-
module within the CuDA2 framework on the performance sentsthehigheststablewinratethatalliescouldachievebefore
of traitor agents, providing additional details and potential addingthetraitor.Itcanbeseenthatallthreealgorithmscould
insights. achieve nearly 100% win rates in the 6m-vs-6m environment.
After adding the traitor, our method decreases the win rates
A. Comparison to Baselines
of allies to a more apparent degree compared to the baseline
To validate the effectiveness of our method under different methods. To be noted, in subsequent experiments where we
MARLalgorithms,wecomparetheproposedmethodwiththe test the number of traitors, we uniformly used QMIX to train
baselines across three MARL algorithms (QMIX, MAPPO, the policy of the victim agents.
etaR
niW
etaR
niW
etaR
niW1.00 1.0
3.00 5.5
0.95
2.75 0.9 5.0
0.90
2.50 0.8 4.5
0.85
2.25 4.0
0.80 0.7
2.00 3.5
0.75
1.75 0.6 3.0
0.70 stop stop stop stop
0.65 r m Ca uin n Dd u Ao s 2m _r 1.50 r m Ca uin n Dd u Ao s 2m _r 0.5 r m Ca uin n Dd u Ao s 2m _r 2.5 r m Ca uin n Dd u Ao s 2m _r
no_traitor 1.25 no_traitor no_traitor 2.0 no_traitor
0.60 0.4
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Frames(10^4) Frames(10^4) Frames(10^4) Frames(10^4)
(a) (6+1)m-vs-6m. (b) (6+1)m-vs-6m. (c) (8+1)m-vs-8m. (d) (8+1)m-vs-8m.
Fig. 6: Adding one traitor to the 6m-vs-6m and 8m-vs-8m environments. The number of Allied deaths includes the deaths of traitors. (a)
Addingonetraitortothe6m-vs-6menvironment,thecurveofalliedwinrate.(b)Addingonetraitortothe6m-vs-6menvironment,thecurve
of allied deaths. (c) Adding one traitor to the 8m-vs-8m environment, the curve of allied win rate. (d) Adding one traitor to the 8m-vs-8m
environment, the curve of allied deaths.
1.0
1.0
6 6
0.9 0.9
0.8 5 0.8 5
0.7
4
0.6 0.7 4
0.5 3 0.6
stop stop stop 3 stop
0.4 r ma in nd uo sm _r 2 r ma in nd uo sm _r 0.5 r ma in nd uo sm _r r ma in nd uo sm _r
0.3 CuDA2 CuDA2 CuDA2 CuDA2
no_traitor no_traitor no_traitor 2 no_traitor
1 0.4
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Frames(10^4) Frames(10^4) Frames(10^4) Frames(10^4)
(a) (6+2)m-vs-6m. (b) (6+2)m-vs-6m. (c) (8+2)m-vs-8m. (d) (8+2)m-vs-8m.
Fig. 7: Adding two traitors to the 6m-vs-6m and 8m-vs-8m environments. The number of Allied deaths includes the deaths of traitors. (a)
Adding two traitors to the 6m-vs-6m environment, the curve of allied win rate. (b) Adding two traitors to the 6m-vs-6m environment, the
curve of allied deaths. (c) Adding two traitors to the 8m-vs-8m environment, the curve of allied win rate. (d) Adding two traitors to the
8m-vs-8m environment, the curve of allied deaths.
1.0
1.0
8
7
0.9
7
0.8
6
6 0.8
stop 0.6 r ma in nd uo sm _r 5 0.7 5 CuDA2
no_traitor 4 4
0.4 0.6
3
s rato np dom s rato np dom 3 s rato np dom
0.2 2 minus_r 0.5 minus_r minus_r
CuDA2 CuDA2 CuDA2
no_traitor no_traitor 2 no_traitor
1 0.4
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Frames(10^4) Frames(10^4) Frames(10^4) Frames(10^4)
(a) (6+3)m-vs-6m. (b) (6+3)m-vs-6m. (c) (8+3)m-vs-8m. (d) (8+3)m-vs-8m.
Fig. 8: Adding three traitors to the 6m-vs-6m and 8m-vs-8m environments. The number of Allied deaths includes the deaths of traitors. (a)
Addingthreetraitorstothe6m-vs-6menvironment,thecurveofalliedwinrate.(b)Addingthreetraitorstothe6m-vs-6menvironment,the
curve of allied deaths. (c) Adding three traitors to the 8m-vs-8m environment, the curve of allied win rate. (d) Adding three traitors to the
8m-vs-8m environment, the curve of allied deaths.
etaR
niW
etaR
niW
etaR niW
shtaeD
deillA
shtaeD
deillA
shtaeD deillA
etaR
niW
etaR
niW
etaR niW
shtaeD
deillA
shtaeD
deillA
shtaeD deillA20 Position Heatmap 20 Position Heatmap Position Heatmap Position Heatmap
25.0 25.0
19 19
22.5 22.5
18 18
20.0 20.0
17 17
17.5 17.5
16 16
15.0 15.0
15 15
12.5 12.5
14 14 10.0 10.0
13 13 7.5 7.5
126 8 10 1X2 14 16 18 126 8 10 1X2 14 16 18 5.00.0 2.5 5.0 7.5 10X.0 12.5 15.0 17.5 20.0 5.00.0 2.5 5.0 7.5 10X.0 12.5 15.0 17.5 20.0
(a) Victim agents (RND). (b) Victim agents (CuDA2). (c) Traitors (RND). (d) Traitors (CuDA2).
Fig. 9: Position heatmaps of victim agents and traitors under different methods. (a) Position distribution of victim agents with traitors
receivingextrarewardsfromRND.(b)PositiondistributionofvictimagentswithtraitorsreceivingextrarewardsfromCuDA2.(c)Position
distribution of traitors with RND providing extra rewards. (d) Position distribution of traitors with CuDA2 providing extra rewards.
2) Number of Traitors: In this section, we compare the the traitors and victim agents by plotting heat maps of their
impact of introducing different numbers of traitors on the win positions. In the experimental scenario, the victim agents tend
rates and death counts of the victim agents. We set up two to start from the left side of the map and then spread out in a
environments: 6m-vs-6m and 8m-vs-8m, and add 1, 2, or 3 line in the middle to attack the enemies. As shown in Fig.9(a)
traitors to these environments. In Fig.5, we show the behavior and (b), the movement range of victim agents under RND is
of two traitors in the 6m-vs-6m environment using CuDA2 smaller compared to CuDA2, indicating that RND technique
and the baseline methods. In Fig.5(a), the traitors remain results in less effect on the victim agents’ policy. In Fig.9(c)
stationarythroughouttheprocess.InFig.5(b),thetraitorstake and (d), there is a significant difference in the behavior of
randomactionsandoneofthemmovestotheedgeofmapdue traitors under RND and CuDA2. RND traitors tend to move
to random movements, causing the enemies to be unable to near the central area of the map while CuDA2 traitors choose
detect and kill it. In Fig.5(c), traitors trained with the minus to move away from the combat area to avoid their own death
reward function of victim agents exhibit runaway behavior. after disrupting the victim agents in the center. Therefore,
In Fig.5(d), the traitors trained with CuDA2 collide with the in terms of both influencing the victim agents’ policy and
victimagentsattheinitialstageofeachepisodeandthenmove ensuring the traitors’ survival, CuDA2 performs better than
back and forth at the edge of the victim agents’ observation RND.
range,therebyinfluencingthevictimagents’decision-making.
As shown in Fig.6, Fig.7 and Fig.8, the win rates of the VII. CONCLUSIONANDFUTUREWORK
allies decreases more significantly as the number of traitors
This paper addresses the challenge of incorporating traitor
increases. Especially when the number of traitors is two or
agents into Cooperative Multi-Agent Reinforcement Learning
more, our method exhibits more significant impact on the
(CMARL).Unlikepreviousresearchaboutadversarialattacks,
allies’ win rate and death count compared to the minus r
the traitors in our setting cannot directly interfere with the
baseline. We assume that this is because traitors in CuDA2
statesandactionsofthevictimagents.Instead,wecontrolthe
framework disrupt the formation of victim agents through
behavior of the traitors to indirectly influence the formation
collisions, causing state-action pairs that victim agents have
and positions of the victim agents. We formalize this problem
never encountered during previous training, rendering their
as a Traitor Markov Decision Process (TMDP), where the
original strategies ineffective and leading to their defeat by
traitor agents aim to minimize the cumulative discounted
the enemies one by one. Except for the scenario with a single
reward of the victim agents when the policies of the victim
traitor, the attack effect is more pronounced in the 6m-vs-6m
agents are fixed. We then introduce the Curiosity-Driven Ad-
environmentcomparedtothe8m-vs-8menvironmentwhenthe
versarialAttack(CuDA2)framework.Bypre-trainingtheRan-
same number of traitors is added, due to the higher ratio of
domNetworkDistillation(RND)moduleandshapingintrinsic
traitors to victim agents in the 6m-vs-6m environment.
rewards using the dynamic Potential-Based Reward Shaping
(PBRS)method,CuDA2ensurestheinvarianceofthetraitors’
B. Ablation Study
optimal policy while guiding the traitors to perform more
Inthe8m-vs-8menvironmentwithtwotraitors,weevaluate aggressive attacks. Experimental results from comparisons
theresultsofusingRNDalonetogivetraitorsintrinsicrewards with baselines and ablation studies validate the effectiveness
and compared them with our method. To be specific, our of the CuDA2 framework from multiple perspectives. As
CuDA2frameworkincorporatesthedynamicPBRSmethodto our method is an efficient curiosity-driven adversarial attack
helptraitoragentslearnoptimalattackpolicycomparedtojust algorithm, we will focus on detecting the presence of traitors
using RND. We analyze the behavioral differences between and defending against their attacks in the future works.
Y Y Y YREFERENCES [20] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell,
“Adversarial policies: Attacking deep reinforcement learning,” arXiv
[1] S.GronauerandK.Diepold,“Multi-agentdeepreinforcementlearning: preprintarXiv:1905.10615,2019.
a survey,” Artificial Intelligence Review, vol. 55, no. 2, pp. 895–943, [21] W.Guo,X.Wu,S.Huang,andX.Xing,“Adversarialpolicylearningin
2022. two-playercompetitivegames,”inInternationalConferenceonMachine
[2] J. Zhao, M. Yang, Y. Zhao, X. Hu, W. Zhou, and H. Li, “Mcmarl:
Learning(ICML),2021,pp.3910–3919.
Parameterizing value function via mixture of categorical distributions [22] P.Hernandez-Leal,B.Kartal,andM.E.Taylor,“Asurveyandcritiqueof
formulti-agentreinforcementlearning,”IEEETransactionsonGames, multiagentdeepreinforcementlearning,”AutonomousAgentsandMulti-
2023. AgentSystems(AAMAS),vol.33,no.6,pp.750–797,2019.
[23] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch,
[3] J.Zhao,X.Hu,M.Yang,W.Zhou,J.Zhu,andH.Li,“Ctds:Central-
“Multi-agent actor-critic for mixed cooperative-competitive environ-
ized teacher with decentralized student for multi-agent reinforcement
learning,”IEEETransactionsonGames,2022.
ments,”AdvancesinNeuralInformationProcessingSystems(NeurIPS),
vol.30,2017.
[4] Z. Peng, Q. Li, K. M. Hui, C. Liu, and B. Zhou, “Learning to simu-
[24] Y.Wang,B.Han,T.Wang,H.Dong,andC.Zhang,“Dop:Off-policy
late self-driven particles system with coordinated policy optimization,”
multi-agentdecomposedpolicygradients,”inInternationalConference
AdvancesinNeuralInformationProcessingSystems(NeurIPS),vol.34,
onLearningRepresentations(ICLR),2020.
pp.10784–10797,2021.
[25] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
[5] N. Greshler, O. Gordon, O. Salzman, and N. Shimkin, “Cooperative
D.Silver,andD.Wierstra,“Continuouscontrolwithdeepreinforcement
multi-agentpathfinding:Beyondpathplanningandcollisionavoidance,”
learning,”arXivpreprintarXiv:1509.02971,2015.
in International Symposium on Multi-Robot and Multi-Agent Systems
[26] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi, “Qtran:
(MRS),2021,pp.20–28.
Learningtofactorizewithtransformationforcooperativemulti-agentre-
[6] W.J.Yun,S.Park,J.Kim,M.Shin,S.Jung,D.A.Mohaisen,andJ.-H.
inforcementlearning,”inInternationalConferenceonMachineLearning
Kim, “Cooperative multiagent deep reinforcement learning for reliable
(ICML),2019,pp.5887–5896.
surveillance viaautonomous multi-uavcontrol,” IEEE Transactionson
[27] J.Wang,Z.Ren,T.Liu,Y.Yu,andC.Zhang,“Qplex:Duplexdueling
IndustrialInformatics(TII),vol.18,no.10,pp.7086–7096,2022.
multi-agentq-learning,”arXivpreprintarXiv:2008.01062,2020.
[7] K. Xue, J. Xu, L. Yuan, M. Li, C. Qian, Z. Zhang, and Y. Yu,
[28] H. Chen and F. Koushanfar, “Tutorial: toward robust deep learning
“Multi-agent dynamic algorithm configuration,” Advances in Neural
againstpoisoningattacks,”ACMTransactionsonEmbeddedComputing
InformationProcessingSystems(NeurIPS),vol.35,pp.20147–20161,
Systems(TECS),vol.22,no.3,pp.1–15,2023.
2022.
[29] L.Huang,A.D.Joseph,B.Nelson,B.I.Rubinstein,andJ.D.Tygar,
[8] G.Papoudakis,F.Christianos,A.Rahman,andS.V.Albrecht,“Dealing
“Adversarial machine learning,” in ACM workshop on Security and
withnon-stationarityinmulti-agentdeepreinforcementlearning,”arXiv
ArtificialIntelligence,2011,pp.43–58.
preprintarXiv:1906.04737,2019.
[30] L.Song,R.Shokri,andP.Mittal,“Membershipinferenceattacksagainst
[9] J.Wang,Z.Ren,B.Han,J.Ye,andC.Zhang,“Towardsunderstanding
adversariallyrobustdeeplearningmodels,”inIEEESecurityandPrivacy
cooperativemulti-agentq-learningwithvaluefactorization,”Advancesin
Workshops(SPW),2019,pp.50–56.
NeuralInformationProcessingSystems(NeurIPS),vol.34,pp.29142–
[31] E. Wenger, J. Passananti, A. N. Bhagoji, Y. Yao, H. Zheng, and B. Y.
29155,2021.
Zhao,“Backdoorattacksagainstdeeplearningsystemsinthephysical
[10] F. Christianos, G. Papoudakis, M. A. Rahman, and S. V. Albrecht, world,”inProceedingsoftheIEEE/CVFConferenceonComputerVision
“Scaling multi-agent reinforcement learning with selective parameter andPatternRecognition(CVPR),2021,pp.6206–6215.
sharing,” in International Conference on Machine Learning (ICML),
[32] X. Zhang, Y. Ma, A. Singla, and X. Zhu, “Adaptive reward-poisoning
2021,pp.1989–1998.
attacksagainstreinforcementlearning,”inInternationalConferenceon
[11] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, MachineLearning(ICML),2020,pp.11225–11234.
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al., [33] B.VahidandH.William,“Adversarialexploitationofpolicyimitation,”
“Value-decomposition networks for cooperative multi-agent learning,” arXivpreprintarXiv:1906.01121,2019.
arXivpreprintarXiv:1706.05296,2017.
[34] K. Mo, W. Tang, J. Li, and X. Yuan, “Attacking deep reinforcement
[12] T.Rashid,M.Samvelyan,C.S.DeWitt,G.Farquhar,J.Foerster,and learning with decoupled adversarial policy,” IEEE Transactions on
S. Whiteson, “Monotonic value function factorisation for deep multi- Dependable and Secure Computing (TDSC), vol. 20, no. 1, pp. 758–
agentreinforcementlearning,”JournalofMachineLearningResearch, 768,2022.
vol.21,no.178,pp.1–51,2020. [35] L. Hussenot, M. Geist, and O. Pietquin, “Copycat: Taking control of
[13] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, neuralpolicieswithconstantattacks,”arXivpreprintarXiv:1905.12282,
“Counterfactual multi-agent policy gradients,” in Proceedings of the 2019.
AAAIConferenceonArtificialIntelligence(AAAI),vol.32,no.1,2018. [36] Y.Li,Q.Pan,andE.Cambria,“Deep-attackoverthedeepreinforcement
[14] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu, learning,”Knowledge-BasedSystems,vol.250,p.108965,2022.
“Thesurprisingeffectivenessofppoincooperativemulti-agentgames,” [37] X. Bai, W. Niu, J. Liu, X. Gao, Y. Xiang, and J. Liu, “Adversarial
AdvancesinNeuralInformationProcessingSystems(NeurIPS),vol.35, examples construction towards white-box q table variation in dqn
pp.24611–24624,2022. pathfinding training,” in International Conference on Data Science in
[15] M.Samvelyan,T.Rashid,C.S.deWitt,G.Farquhar,N.Nardelli,T.G.J. Cyberspace(DSC),2018,pp.781–787.
Rudner,C.-M.Hung,P.H.S.Torr,J.Foerster,andS.Whiteson,“The [38] T.Chen,W.Niu,Y.Xiang,X.Bai,J.Liu,Z.Han,andG.Li,“Gradient
StarCraftMulti-AgentChallenge,”CoRR,vol.abs/1902.04043,2019. band-basedadversarialtrainingforgeneralizedattackimmunityofa3c
[16] J.Guo,Y.Chen,Y.Hao,Z.Yin,Y.Yu,andS.Li,“Towardscomprehen- pathfinding,”arXivpreprintarXiv:1807.06752,2018.
sivetestingontherobustnessofcooperativemulti-agentreinforcement [39] X.Y.Lee,S.Ghadai,K.L.Tan,C.Hegde,andS.Sarkar,“Spatiotem-
learning,” in Proceedings of the IEEE/CVF Conference on Computer porallyconstrainedactionspaceattacksondeepreinforcementlearning
VisionandPatternRecognition(CVPR),2022,pp.115–122. agents,”inProceedingsoftheAAAIConferenceonArtificialIntelligence
[17] J. Zhao, W. Shu, Y. Zhao, W. Zhou, and H. Li, “Improving deep (AAAI),vol.34,no.04,2020,pp.4577–4584.
reinforcementlearningwithmirrorloss,”IEEETransactionsonGames, [40] X. Li, Y. Li, Z. Feng, Z. Wang, and Q. Pan, “Ats-o2a: A state-based
2022. adversarialattackstrategyondeepreinforcementlearning,”Computers
[18] L.Yuan,Z.Zhang,K.Xue,H.Yin,F.Chen,C.Guan,L.Li,C.Qian,and &Security,vol.129,p.103259,2023.
Y.Yu,“Robustmulti-agentcoordinationviaevolutionarygenerationof [41] R. Bellman, “A markovian decision process,” Journal of Mathematics
auxiliaryadversarialattackers,”inProceedingsoftheAAAIConference andMechanics,pp.679–684,1957.
on Artificial Intelligence (AAAI), vol. 37, no. 10, 2023, pp. 11753– [42] M.DorigoandM.Colombetti,“Robotshaping:Developingautonomous
11762. agentsthroughlearning,”ArtificialIntelligence,vol.71,no.2,pp.321–
[19] J. Sun, T. Zhang, X. Xie, L. Ma, Y. Zheng, K. Chen, and Y. Liu, 370,1994.
“Stealthy and efficient adversarial attacks against deep reinforcement [43] J.RandløvandP.Alstrøm,“Learningtodriveabicycleusingreinforce-
learning,” in Proceedings of the AAAI Conference on Artificial Intelli- ment learning and shaping.” in International Conference on Machine
gence(AAAI),vol.34,no.04,2020,pp.5883–5891. Learning(ICML),vol.98,1998,pp.463–471.[44] T.Carta,P.-Y.Oudeyer,O.Sigaud,andS.Lamprier,“Eager:Askingand
answering questions for automatic reward shaping in language-guided
rl,” Advances in Neural Information Processing Systems (NeurIPS),
vol.35,pp.12478–12490,2022.
[45] T. Zhang, H. Xu, X. Wang, Y. Wu, K. Keutzer, J. E. Gonzalez,
and Y. Tian, “Noveld: A simple yet effective exploration criterion,”
AdvancesinNeuralInformationProcessingSystems(NeurIPS),vol.34,
pp.25217–25230,2021.
[46] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward
transformations: Theory and application to reward shaping,” in Inter-
national Conference on Machine Learning (ICML), vol. 99, 1999, pp.
278–287.
[47] E. Wiewiora, G. W. Cottrell, and C. Elkan, “Principled methods for
advisingreinforcementlearningagents,”inInternationalConferenceon
MachineLearning(ICML),2003,pp.792–799.
[48] S.M.DevlinandD.Kudenko,“Dynamicpotential-basedrewardshap-
ing,”inInternationalConferenceonAutonomousAgentsandMultiagent
Systems(AAMAS),2012,pp.433–440.
[49] A. Harutyunyan, S. Devlin, P. Vrancx, and A. Nowe´, “Expressing
arbitrary reward functions as potential-based advice,” in Proceedings
oftheAAAIConferenceonArtificialIntelligence(AAAI),vol.29,no.1,
2015.
[50] Y. Burda, H. Edwards, A. Storkey, and O. Klimov, “Exploration by
randomnetworkdistillation,”arXivpreprintarXiv:1810.12894,2018.
[51] P. Xu, Q. Yin, J. Zhang, and K. Huang, “Deep reinforcement learning
withpart-awareexplorationbonusinvideogames,”IEEETransactions
onGames,vol.14,no.4,pp.644–653,2021.
[52] M.Grzes,“Rewardshapinginepisodicreinforcementlearning,”2017.
[53] G. Papoudakis, F. Christianos, L. Scha¨fer, and S. V. Albrecht,
“Benchmarking multi-agent deep reinforcement learning algorithms in
cooperativetasks,”inProceedingsoftheNeuralInformationProcessing
SystemsTrackonDatasetsandBenchmarks(NeurIPS),2021.[Online].
Available:http://arxiv.org/abs/2006.07869