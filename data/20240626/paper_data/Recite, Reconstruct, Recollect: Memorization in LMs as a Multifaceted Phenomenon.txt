Recite, Reconstruct, Recollect:
Memorization in LMs as a Multifaceted Phenomenon
USVSNSaiPrashanth*,1 AlvinDeng∗,1,4 KyleO’Brien∗,1,2 JyothirSV∗,1,3
MohammadAflahKhan1,6 JaydeepBorkar5
ChristopherA.Choquette-Choo7 JacobRayFuehne8 StellaBiderman1
TracyKe†,9 KatherineLee†,7 NaomiSaphra†,9,10
1EleutherAI 2Microsoft 3NewYorkUniversity 4DatologyAI 5NortheasternUniversity
6IndraprasthaInstituteofInformationTechnologyDelhi 7GoogleDeepMind
8UniversityofIllinoisatUrbana-Champaign 9HarvardUniversity 10KempnerInstitute
Correspondence:katherinelee@google.comandnsaphra@fas.harvard.edu
Abstract Ourtaxonomy,illustratedinFig. 1,definesthree
typesofLMmemorizationbasedoncolloquialde-
Memorizationinlanguagemodelsistypically scriptionsofhumanmemorization. Humansrecite
treatedasahomogenousphenomenon,neglect- directquotesthattheycommittomemorythrough
ing the specifics of the memorized data. We repeatedexposure,soLMsrecitehighlyduplicated
insteadmodelmemorizationastheeffectofa
sequences. Humansreconstructapassagebyre-
setofcomplexfactorsthatdescribeeachsam-
memberingageneralpatternandfillinginthegaps,
pleandrelateittothemodelandcorpus. To
so LMs reconstruct inherently predictable boiler-
buildintuitionaroundthesefactors,webreak
platetemplates. Humanssporadicallyrecollectan
memorization down into a taxonomy: recita-
tionofhighlyduplicatedsequences,reconstruc- episodicmemoryorfragmentafterasingleexpo-
tion of inherently predictable sequences, and sure,soLMsrecollectothersequencesseenrarely
recollectionofsequencesthatareneither. We duringtraining.
demonstrate the usefulness of our taxonomy
Weuseourtaxonomyinavarietyofexperiments
byusingittoconstructapredictivemodelfor
thathighlightthemultifacetednatureofmemoriza-
memorization. Byanalyzingdependenciesand
tion.2 Insummary:
inspectingtheweightsofthepredictivemodel,
wefindthatdifferentfactorsinfluencethelike-
lihoodofmemorizationdifferentlydepending • We introduce an intuitive taxonomy and
onthetaxonomiccategory. heuristicsforcategorizingmemorizeddata.
• Bycomparingmemorizedandunmemorized
1 Introduction
distributions, we assess how a variety of
The existing literature on Language Model (LM) corpus-wide statistics, datum-level metrics,
memorization1—the tendency to generate exact andrepresentationaldifferencesinfluencethe
copies of training samples at test time—varies likelihoodofagivensequencebeingmemo-
widely in stated motivation. Papers might focus rized. Ourdependencytestsconfirmexisting
oncopyright(Shietal.,2023;Karamolegkouetal., findingsthatlowperplexityisstronglyassoci-
2023; Meeus et al., 2024), privacy (Carlini et al., atedwithmemorization—thoughnotequally
2018, 2022b; Brown et al., 2022; Mireshghallah forallmemorizedexamples. Thisfactguides
et al., 2022), or scientifically understanding how ourheuristicforpartitioningmemorizeddata
interpolation (Mallinar et al., 2022) leads to gen- intoarecitationcategory.
eralization(Feldman,2021;Tirumalaetal.,2022;
• Westudyscalingfactorsinmemorizationby
Henighanetal.,2023a). Althoughtheseobjectives
monitoringeachtaxonomiccategoryoverthe
share commonalities, they also drive distinct and
courseoftrainingandacrossmodelsizes. The
sometimescontradictorynotionsofmemorization.
number of memorized sequences increases
Todisentanglethesemotivationsandtoarticulate
withtrainingtimeandmodelsize,regardless
thefactorsthatdetermineorsignalmemorization,
of taxonomic category. Recollection, how-
weproposeataxonomyinspiredbycolloquialdis-
ever, sees the fastest increase—and this out-
tinctionsofmemorizationbehaviorinhumans.
sizegrowthcannotbeattributedsolelytore-
* Equalcontribution
† Equalcontribution 2Code available at https://github.com/EleutherAI/
1Asdefinedbywww.genlaw.org/glossary.html. semantic-memorization.
1
4202
nuJ
52
]LC.sc[
1v64771.6042:viXraIs the sequence Is the continuation Is the sequence continuation solely a
32-extractable? duplicated > 5 boilerplate template: either incrementing
times? numerals or substring repetition?
Not Memorized Recitation Reconstruction Recollection
Figure1: Ourintuitivememorizationtaxonomyhasthreecategoriesdeterminedbysimpleheuristics.
peatedexposurestoraresequencesortoran- runs, enabling causal claims about the effect of
dommemorization. modelscaleonmemorization.
• Wetrainlogisticregressionstopredictmemo- Datasets Ourmemorizedsampleisapubliclist
rization for each category. This predictive of sequences memorized by Pythia, released by
model outperforms both a simple baseline Bidermanetal.(2023a). Unlikeotherworksthat
with no taxonomy and a model that uses a estimate whether a generation is from a model’s
taxonomyoptimizedbysearchingforthebest training set using predictive techniques (Carlini
set of mediating factors. These experiments etal.,2020;Shietal.,2023;Yangetal.,2024),this
showthattheintuitionsbehindourtaxonomy dataset contains all 32-extractable samples from
canimproveonmoregenericapproaches. the Pile, verified by referencing the training data
(Gao et al., 2020). We also collect a representa-
• Wehighlightdifferencesbetweencategories
tivesamplebytakingarandom3%subsetofThe
by exploring statistical dependencies, find-
Pile,retainingthefirst64tokensofeachsequence.
ing recitation is enabled by low-perplexity
Someanalysisalsoconsidersanunmemorizeddis-
promptsandrecollectionisconstrainedbythe
tributionestimatedbysubtractingthememorized
presenceofraretokens.
data distribution from the entire Pile, as inferred
fromtherepresentativesample.
2 Experiments
In this section, we detail the definitions and data 3 Potentialfactorsinmemorization
weusetoanalyzevaryingfactorsinmemorization.
We consider a number of possible factors in
DefiningMemorization Therearemultiplecom- whetheragivensequenceismemorized. Thesefac-
petingdefinitionsformemorization (Zhangetal., torsarebasedoncorpusstatistics,datumstatistics
2021; Ippolito et al., 2022). Because our experi- intrinsictothatsample,ormodelperplexity. Fea-
ments employ memorization data released by Bi- turesmaybecomputedoverthefirst32tokens(the
dermanetal.(2023a),weusetheirpreferreddefi- prompt); the last 32 tokens (the continuation);
nitionofk-extractablememorization(Carlinietal., and the full sequence of 64 tokens subsampled
2022a) with k = 32. A sample is k-extractable fromthetrainingdata. Implementationdetailsare
iftheLM,whenpromptedwiththefirstk tokens, providedinAppendixA.
generatesthefollowingk tokensverbatim. Manyofthesepropertieshavedifferentdistribu-
tionsformemorizedandunmemorizeddata. Fig.2
Language Models We study memorization
illustrates these differences, highlighting that for
across model scale and training timing using
some properties, the memorized distribution is
the deduplicated Pythia models (Biderman et al.,
moreconcentrated. Otherproperties—inparticular
2023b), which range in size from 70M to 12B
perplexityandnumberofduplicatesinthetraining
parameters3 trained on a deduped version of The
corpus—have memorized and unmemorized dis-
Pile(Gaoetal.,2020). Dataorderisfixedacross
tributions with visibly different medians. Where
3Excludingthe160Mparametermodel,asitsmemoriza- thedistributionsdiffer,thepropertyinquestionis
tiondatasetexhibitsoutlierbehaviorthatcouldbeeithera
likely to influence memorization, an assumption
buggydataartifactorarealphenomenon,butisregardless
outsideofthescopeofourwork. whichweemploypredictivelyinSection6.
230%
80% 40% 80% 12% 8%
8%
40% 20% 15% 40% 4% 6% 4%
0% 0% 0% 0% 0% 0% 0%
100 103 106 100 102 101 103 109 1010 108 109 1010 106 108 107 109
Duplicates Semantic Matches Textual Matches Max Token Freq. Mean Token Freq. Min Token Freq. Median Token Freq.
5% 30% 16% 4% 80% 8% 6%
2% 15% 8% 2% 40% 4% 3%
0% 0% 0% 0% 0% 0% 0%
105 107 109 107 109 3.0 4.5 6.0 100 101 100 101 100 101 102 2×1003×100
P25 Token Freq. P75 Token Freq. Huffman Length Prompt PPL Continuation PPL Sequence PPL Loss
Not Memorized Memorized
Figure2: Histogramofvariouspropertiesofinterest(describedinSection3)formemorizedandunmemorized
(estimatedbyassumingtherepresentativedataset’sstatisticsholdforthePile)samples.
3.1 Corpusstatistics focus on two common patterns defined by hand-
craftedheuristics:
Somefactorsrelateagivensequencetotheentire
trainingcorpus. Overall,thefollowingfeaturesil-
• Repeating: Consistingonlyofashortrepeat-
lustratehowmemorizationisinfluencedbyvarious
ingsequenceoftokens,e.g.,“GoGoGo...”.
typesofduplication.
Zhangetal.(2021)previouslydiscussedrepet-
Duplicates For each 32-token window in any
itivetemplatesasacommonfeatureofappar-
2049-token sequence seen during training, we
entlymemorizeddatawhichwasnotclassified
countthenumberofduplicatesinthePile.
ascounterfactuallymemorized.
SemanticMatchesToassesstheprevalenceof
semanticallysimilarsamplesintraining,wegener-
• Incrementing: Consisting of incrementing
ate document embeddings for each full sequence
numericalsequences. Forexample,consider
usingSBERTandcountthenumberofsequences
thesequence“23: 0xf1,24: 0xf2,25: 0xf3”,a
withcosinesimilarity≤ 0.8. Thesesequencesare
setofinterspersednumericalsequenceswith
semantically similar but may not be exact token-
repeatingseparators.
levelduplicates.
Textual Matches We filter the set of semantic
Compressibility WeuseHuffmanCoding(Huff-
matches for a given target sequence to identify
man, 1952) length to measure how easily a se-
thosewithalowLevenshteineditdistanceintheir
quenceiscompressed. Compressibilitygeneralizes
prompts(Levenshteinetal.,1966)fromthetarget
repeating templates to cases where minor varia-
sequence. Thesematchesflagslightvariationson
tions on repeating patterns must be memorized.
boilerplateprompts. Wecomputeeditdistanceat
The connection between learning, memorization,
thecharacterlevel,therebyaccountingfordifferent
andcompression isdrawn fromtheexistingliter-
tokenizationsofidenticalsequences.
ature: Carlini et al. (2020) attempts to filter out
Token frequency We also compute summary
sequences that are “easy” to produce by compar-
statistics about the corpus-wide frequency of in-
ing zlib compression with perplexity to identify
dividual tokens in the sequence: mean, median,
memorizedtrainingdata.
maximum, minimum, and 25th / 75th percentile
counts. 3.3 Perplexity
3.2 Sequenceproperties We compute average perplexity across tokens on
theprompt, continuation, andfullsequence. The
Because some sequences are inherently easier to
importance of perplexity is one of the most re-
encode, we also consider factors determined by
producedresultsinmemorizationresearch(Zhang
intrinsicmetricsonthesampleitself.
et al., 2021; Carlini et al., 2018) and we confirm
Templating Asampleisclassifiedastemplating that low perplexity sequences are far more likely
ifitfollowsapredictablepattern. Wedonotcom- to be memorized than high perplexity sequences
prehensively consider all possible templates, but (Fig. 2). Perplexityistheonlyfactorweconsider
3
egatnecreP
egatnecreP0.2 randomsamplesofnaturallanguagerecitation,in-
cludes all of these common cases. Recited code
0.1
text, asseeninAppendix4, islargelywebdevel-
0.0 opment(HTML,CSS,JavaScript,etc.) boilerplate
100 101 102 103
Duplicates that describes common elements or derives from
Figure3: KLdivergencebetweengenerationperplex- popularwebpagetemplates.
ity of memorized and non-memorized examples for
4.2 Reconstruction
Pythia12Bwithbootstrappedconfidenceintervals.Non-
memorized samples are treated as the reference dis- Areallperfectlyreproducedsequencestruly“mem-
tribution. Divergenceishighestforsequenceswith6
orized”? Weconsidercasesthatmaybespuriously
duplicates,whilehighlyduplicatedsequenceshavenear-
classifiedbydefinitionslikek-extraction. Rather
identicalmemorizedandunmemorizeddistributions.
thanencodingtheentiresequence,themodellearns
templatesandthenreconstructsthesamplebased
that relates to model behavior, rather than being on these more broadly applicable patterns. A se-
intrinsictothedata. quencecanthusbeperfectlyreproducedevenifit
neverappearedduringtraining.
4 MemorizationTaxonomy We consider a few templates—stereotyped se-
quencepatternswithasinglelogicalcontinuation—
Toanalyzethefundamentalcausesofk-extracted
to define reconstruction candidates. These tem-
memorization,wesubdividememorizedsamples
plates are not intended to be comprehensive, as
intothreetypes. Thefollowingrulescategorizea
anystereotypedpatternmaypermitreconstruction.
sample as a candidate for recitation, reconstruc-
Ourreconstructioncandidatesaresequencesclassi-
tion,orrecollection;candidatesmemorizedbythe
fiedasincrementingorrepeatingbytheheuristics
model are therefore respectively recited, recon-
describedinSection3.2. AsseeninAppendixE,
structed,orrecollected.
codeismorelikelytobereconstructedthannatural
languagetext. Whennaturallanguagetextisrecon-
4.1 Recitation
structed,asseeninAppendixF,itoftentakesthe
Theexistingmemorizationliteratureagreesthatthe formofachapterindexanditismorelikelythan
duplicationofasequenceacrossthetrainingcorpus code to contain cases of phrase repetition rather
isstronglycorrelatedwithitsmemorization(Lee thanarithmeticsequences.
et al., 2021; Kandpal et al., 2022). For example,
4.3 Recollection
LMs produce verbatim copies of bible quotes or
softwarelicensesthatarecommonlyduplicated. Afterexcludinghighlyduplicatedrecitationsand
We consider a sample to be a recitation candi- template-based reconstructions, what remains
dateifitishighlyduplicatedinthetrainingcorpus. memorized? Despiteonlyseeingasampleasmall
Model perplexity is a good predictor of memo- number of time, the model might still be able to
rizationonraresequencesbecausetheperplexity recollectagivensample,althoughthefactorsthat
distributionsaremoredifferentonmemorizedand leadtoinstantmemorizationarepoorlyunderstood.
unmemorized data with few duplicates (Fig. 3). We consider a sample to be a recollection candi-
Forhighlyduplicatedsequences,however,perplex- date if it is a candidate for neither recitation nor
ity is no longer a good predictor of whether the reconstruction.
sequence is memorized or not. We therefore de- Recollected code, seen in Table 4, is largely
fine a recitation candidate as a sequence with at madeupoftemplatingpatternsthatarenotstrictly
least 6 duplicates because the three-way relation- thecombinationofincrementingandrepetitionthat
shipbetweenperplexity,memorization,anddupli- weusetodefinetemplates. Theexamplesofnatu-
catecountdiffersbeforeandafterthatmaximum rallanguagerecollectioninTable3mightlikewise
divergencepoint. atfirstappeartobemisclassifiedrecitationcases.
LMs memorize a wide variety of highly dupli- Naturallanguagerecollectionfrequentlycomprises
catedtexts,asshownintheexampleofAppendix legalorliturgicaltexts,whichwouldbeexpected
F.Recitednaturallanguagetextlargelycomprises toappearfrequentlythroughoutthecorpus.
webpage boilerplate text, liturgy, and software li- One might conjecture that these sequences are
censesorotherlegalese. Table3,whichincludes cases of retokenization, i.e., the particular token
4
ecnegreviD
LKsequenceisrarebutthesamestringisheavilydu- data(Tirumalaetal.,2022). However,isthecumu-
plicatedinthecorpusunderdifferenttokenizations. lativeeffectduesolelytoexposuretomorememo-
However,thedependencytestsinAppendixBcon- rizablesequences? Duetorepeatedexposuretothe
tradictthishypothesis: thecorrelationbetweentex- sameheavilyduplicateddata? Orissomestructural
tualmatchcountandmemorizationisconsistently propertyofthelatermodelmoreamenabletoexact
neutralornegativeforrecollectioncandidates. In memorization? Tounderstandwhymemorization
otherwords,araretokensequenceislesslikelyto accumulatesthroughouttraining,wemeasureeach
bememorized,notmore,ifitisadifferenttokeniza- taxonomiccategoryinintermediatecheckpointsfor
tionofacommonstring. Weinsteadconjecturethat the12BparameterPythiamodel. Wefindthatac-
themodelappearstomemorizeslightdifferences cumulatedmemorizationcannotbeascribedsolely
in translation (liturgical text) or indexing (legal) tothenumberofavailablesamplestomemorizeor
betweeneachvariationonasequence. torepeatedexposuretohighlyduplicatedsamples.
First,inFigure4(c),weseethatmodelsdonot
5 DistributionAcrossScaleandTime
simplyaccumulatememorizedsampleswithauni-
formprobabilitythroughtrainingsincememoriza-
Larger models memorize more data (Biderman
tionincreasessub-linearly. Second, ifmemoriza-
et al., 2023a; Carlini et al., 2023; Tirumala et al.,
tionaccumulatessolelyduetorepeatedexposureto
2022), likelybecausetheyhavemoreparameters
eachduplicatedsample,recitationofthesehighly
with which to recreate those sequences. Recent
duplicatedsampleswouldbethemainsourceofin-
workondeduplication(Sorscheretal.,2022)has
creasingmemorization. Instead,theproportionof
argued that larger models are more distorted by
recitationdecreasesrelativetotheamountofmem-
duplication,potentiallybecauseheavilyduplicated
orization (Fig. 4(d)). Therefore, the additional
sequences are more likely to be memorized (Lee
memorizationcannotbeduetorepeatedexposure
etal.,2021).
torecitationcandidates. Instead,againthelargest
Likewise,modelsmemorizemoredataastrain-
proportionalincreaseamongallcategoriesisinthe
ingprogresses(Tirumalaetal.,2022),butitisnot
recollection category. This trend holds until ap-
known whether the accumulation of memorized
proximately86%oftotaltrainingtime,whichsees
examplesiscausedsolelybyincreasedexposureto
asuddenincreaseinreconstruction. Weconjecture
heavilyduplicatedsamplesorwhetherotherfactors
thatthisincreaserepresentsabreakthroughingen-
eventuallycausememorizationofraresequences.
eralizingmorecomplextemplatesbutleavefurther
Inthissection,westudytheimpactoftrainingtime
investigationtofuturework.
andmodelsizeoneachcategoryofmemorization.
Havingconsideredandrejectedbothexclusive
explanations,wemustpresumethatmemorization
5.1 Modelsize
continuestooccurlateintrainingthroughacom-
Fig. 4(a) reports the number of examples mem-
bination of repeated exposure, opportunities for
orized by each fully trained model, confirming
memorizingnewsequences,andotherunexplored
thatmemorizationincreaseswithparametercount.
factorsthatmaybethefocusoffuturework.
While all types of memorization increase with
modelsize,someincreasefasterthanothers. Rec-
6 Predictingmemorization
ollectiongrowsthemost(Fig. 4(b))from4.49%of
the examples memorized in the 70M model, to What makes a taxonomy useful, or a reflection
11.34% in the 12B model. This disproportion- of natural kinds? Our position is that categories
ate growth suggests larger models tend to mem- shoulddifferinthedependenciesbetweenfeatures
orize rarer sequences that cannot be trivially re- of interest. The most obvious example of vali-
constructed. Meanwhile, reconstruction barely datednaturalkindsisthecaseofSimpson’sPara-
increases, indicating the smallest models have dox(Simpson,1951),astatisticalphenomenonin
learnedtoextrapolaterepeatingandincrementing which a pair of variables are correlated across a
templatesalmostaseffectivelyasthelargest. population,butthedirectionofcorrelationreverses
whenconsideringeachsubpopulationcategorysep-
5.2 Time
arately. Simpson’sParadoxisonlythemostobvi-
Over the course of training, LMs are known ousevidencefornaturalkinds, butlargechanges
to memorize an increasing pool of the training incorrelationmaysupportcategoricaldifferences
5(a) (b) (c) (d)
100% 100%
106 106
90% 90%
105
105
80% 80%
..
.
104 ..
.
108 109 1010 70m 410m 1b 1.4b 2.8b 6.9b 12b 20% 40% 60% 80% 100% 16% 30% 44% 58% 72% 86% 100%
Parameters Parameters Training Time Training Time
Recitation Reconstruction Recollection
Figure4: Thequantityofmemorizeddatacategorizedbytaxonomyacrossparametersizeandtrainingtime. For
fullytrainedmodelsofvaryingparametersizes,wegive(a)totalcountsand(b)proportionofmemorizedsamples
bycategory. Forthe12Bparametermodel,weconsiderintermediatecheckpointsduringtraining,alsoprovidingfor
eachcheckpointthe(c)totalmemorizedcountsand(d)proportionofmemorizedsamplesbycategory. Notethat
theproportionalplotsaretruncatedat80%,asrecitationisconsistentlyamajorityoftheoverallmemorizeddata.
Figure5: Performanceofbaseline,proposedtaxonomyandoptimallypartitionedmodelsagainstvariousmetricson
subsetsoftestdataset. Confidenceintervalisstandarddeviationcomputedbybootstrapping.
evenifthatcorrelationdoesnotchangedirection. supportsmoreaccuratepredictions.
Wemeasureanumberofcategoricaldifferences
6.1 Models
in dependencies, including sign and significance
differences,inAppendixB.Ifourintuitivetaxon- EachmodelisalogisticregressiontrainedwithL2
omy did not reflect meaningful differences with regularization,abiasparameter,andbalancedclass
respect to the factors in Section 6.3, their depen- weights. Wesplittherepresentativesampleintotest
dencieswouldnotdiffersignificantly. Weinstead andtrainsets. Wethencombinethetrainsetwith
findsignificantdifferencesthroughstatisticaltests, thefullmemorizedsample,reservingaportionas
suggesting the taxonomy expresses some natural avalidationset. Foreachset,continuousfeatures
kinds. arenormalizedtozeromeanandunitvariance.
Notonlydothesedifferencessupportourtaxon-
Generic baseline model The generic baseline
omyasanontology,buttheysuggestourtaxonomy
is a logistic regression model trained to predict
canhelppredictmemorizationfromdependentfac-
whetherasampleismemorizedgiventhefeatures
tors. Wethereforetesttheapplicabilityofourtax-
fromsection3. Itistrainedonthetrainingsplitof
onomybycreatingapredictivemodelbasedonthe
theentirememorizeddatasetandtheentirerepre-
intuitive taxonomic model. We compare it with
sentativePilesample.
agenericbaselinemodellackingataxonomyand
with a model using an automatically selected op- Intuitivetaxonomicmodel(Ours) Thepredic-
timalpartition,findingthatourtaxonomicmodel tivemodelbasedonourintuitivetaxonomyismade
6
seiromeMupofasetofthreebinarylogisticregressionmod- they have no rare tokens. We posit that there is
els. Wedividesamplesintotaxonomicgroupsbe- moreresistancetomemorizingraretokenswithin
fore training a separate regression on each taxo- asequence,astheirpriorprobabilityislow.
nomiccategory. Meanwhile, the more duplicates a recollection
candidatehas,themorelikelyitistobememorized,
Optimally partitioned model To demonstrate
whereasrecitationcandidatesarehardlyaffectedby
that our intuitive taxonomic model is not sim-
duplicatecount. Theseresultssuggestthatbeyond
ply benefiting from having more degrees of free-
the5-duplicatethreshold,greaterexposurehardly
dom, we devise an equally complex—that is,
leadstomemorization.
with the same architecture of three binary logis-
Another notable difference is in the effect of
ticregressions—alternativetaxonomicmodel. To
perplexity: while predictable continuations are
provide a strong baseline, we search for a parti-
strongly associated with memorization across all
tion based on a set of possible feature-threshold
categories,unpredictablepromptsarestronglyas-
combinations. We train predictive models with
sociated with memorization except for cases of
thesamethree-regressionarchitectureasourintu-
reconstruction. Theclearexplanationisthathigh-
itive taxonomic model, but partitioning based on
perplexitypromptsoftenonlyoccuraspreludeto
eachfeature-thresholdcombination. Theoptimal
thesamecontinuation,providingauniqueindexfor
partitionisthatwhichsupportsthebestpredictive
thememorizedsequence,butthatalow-perplexity
model, which we find categorizes samples based
promptmayalsoinitiateacommontemplate, en-
onHuffmancodinglengthfollowedbysequence
ablingreconstruction.
duplicatecount.
Foragivenfeature,weconsiderthe25th,50th
7 Discussionandfuturework
and 75th percentiles of the value distribution dis-
tribution as potential thresholds. Each feature- Wehaveestablishedthatanintuitivetaxonomycan
threshold pair provides a possible partition split; beusedtoimproveunderstandingofmemorization.
weselecttheoptimalthree-categorypartitionbased Wenowrelateourmethodstotheexistingliterature
on F1 score on the aggregate representative test onmemorizationandtopossiblefuturedirections.
set. Notethatour“optimal”partitionmaynotex-
ploreourintuitivetaxonomyasanoptionbecause 7.1 Ontologiesofmemorization
the threshold search is limited to each feature’s
Ourworkisstronglyrelatedtoseveralrecentefforts
quartilevalues. Ourintuitivetaxonomymay—and
todevelopanontologyofmemorization. Dankers
does—thereforeoutperformtheoptimalpartition.
etal.(2023),studyingmachinetranslation,focus
primarily on the influence of a sequence during
6.2 Howgoodisourtaxonomy?
trainingratherthanonthesemanticsorproperties
To test our intuitions, we compare our proposed ofanindividualsequence. Likeus,theyinvestigate
taxonomytothehomogeneousbaselineandtoour thefactorsthatinfluencecounterfactualmemoriza-
optimal partition. As seen in Fig. 5, the greedy- tion,thecategorylikelytodominate“recollection”
optimalpartitionoutperformstheaggregatebase- cases. They find that rare tokens, long sequence
lineslightlyonmostmetrics,butourintuitivetax- lengths, and high BPE segmentation rate are cor-
onomy is better calibrated and more accurate ex- relatedwithcounterfactualmemorization (Zhang
ceptontherecollectionset,whereithaslowpreci- et al., 2021); of these, we only consider rare to-
sion. Weconcludethatourintuitionhasguidedus kens,whichweconfirmtopredictrecollectionin
toabettertaxonomythansearchingpossibledata particular.
partitions. Hartmannetal.(2023)considerwhatfacetsof
memorizationarelikelytoberelevanttodifferent
6.3 Categoricaldifferences
targets,justaswediscussthedifferencesbetween
Having confirmed the benefits of separately con- motivations grounded in copyright infringement
sidering these three taxonomic categories, Fig. 6 andprivacy. Bansaletal.(2023)considertwodif-
showshowtheydifferthroughthefeatureweights ferentkindsofmemorization: heuristicmemoriza-
fromourregressionmodels. tion,i.e.,shortcutlearning,andexamplememoriza-
Recollection candidates—that is, rare tion. Ourworkfocusesonwhattheycallexample
sequences—are more likely to be memorized if memorization,furtherdecomposingthatcategory.
7tionbyhighlightingthatraresequencescompose
thefastest-growingcategoryofmemorization.
7.3 Whichcategoriesdowecareabout?
The relevance of each category depends on our
motivationforstudyingmemorization.
1. Intellectual property violations: The con-
tentmostrelevanttoconcernsaboutintellec-
tualpropertymaybehighlyduplicateddata,
suchasfrequentlyexcerptedpassagesfroma
popularbook. However,someraresequences
mayalsobememorized,makingrecollection
potentiallyrelevanttoissuesofcopyrightin-
fringement.
2. Privacy: Iftheprimarymotivationisprevent-
ingthememorizationofpersonallyidentify-
inginformation,wemayfocusonrecollection,
asissuesmayariseifamodelgeneratessuch
information even after even a small number
ofexposures.
3. Scientificunderstandingofgeneralization:
WorklikeHenighanetal.(2023b)andBartlett
et al. (2020) points to eventual generaliza-
tion as a result of memorization dynamics.
Adeeperunderstandingofthesephenomena
mightfocusonreconstruction,whichexposes
adirectlinkbetweenapparentoverfittingand
generalpatternrecognition.
Figure 6: Feature weights from predictive models
7.4 Ontologiesandstatistics
trainedonthehomogeneousaggregatebaselineandthe
intuitivetaxonomycategories. This taxonomy may serve as an example for fu-
turemethodsofinterpretingcomplexphenomena,
in deep learning and elsewhere. We have, in par-
We do not test their result that high-entropy fea-
ticular, quantified the validity and usefulness of
turescanindicateexamplememorization,butlike
suchataxonomybycomparingpredictivemodels
us,theyusethisfactortodifferentiatebetweentheir
whichtreatmemorizationinaggregatetomodels
memorizationcategories.
which treat memorization as a multifaceted phe-
nomenonwithourtaxonomy. Weprovideevidence
7.2 Memorizationandtrainingtime
forthetaxonomicmodelbymeasuringtheimprove-
Our work fits into an existing literature on how mentinpredictivejudgmentswhenreflectingthe
timeandscaleaffectmemorization. Bidermanetal. dependentandnonlinearthresholdedrelationship
(2023b)findthatthepositionofasequenceintrain- betweenmemorizationandthepropertiesthatde-
ing does not affect its likelihood of being mem- fineeachtaxonomiccategory.
orized, and that smaller models fail to memorize In future work, we hope that interpretable and
even when repeatedly exposed to a term. Tiru- usefulontologiescanbevalidatedbyasimilarap-
malaetal.(2022)findthatlargermodelsmemorize proach. Ourproposalforwhatmakesagoodtaxo-
moretrainingdataandforgetlessduringtraining. nomicmodelisnotonlyapplicabletomemoriza-
They also observe that models memorize nouns tionoreventodeeplearningphenomena. Instead,
and numbers first, using these entities as unique bystudyinginteractionsandnonlinearitiesinarbi-
identifiersforindividualsamples. Ourworkfurther trarysettings,researchersmayfindcomplexdepen-
expandsourunderstandingofscaleinmemoriza- denciesandartifactslikeSimpson’sparadox.
8Limitations guage;engineeredthepredictivefeaturesfordupli-
catecount,tokenfrequency,andtemplating;helped
Our primary goal is to intuitively describe the
optimizeperplexitymeasurementforefficiency;de-
memorizationbehaviorwithataxonomyandcon-
bugged, expanded, and refactored the predictive
sequently use that taxonomy to investigate how
modeltrainingpipeline;generatedandmodifieda
severaldominantfactorsinmemorizationinteract
numberofplots;andhelpedwrite.
with each other. A secondary goal is to provide
Alvin Deng built the data processing pipeline,
anexampleofhowanontologycanbeconstructed
designedthepredictivemodeltrainingandevalua-
andtestedingeneral,astestedwithourpredictive
tionpipelines,analyzedclassifierperformanceon
models. However,thesepredictivemodelsarenot
code vs. natural language, generated exploratory
measurementsofstatisticaldependencyingeneral,
visualizations,andhelpedwrite.
instead only focusing on linear dependence. Al-
KyleO’Brienconductedtheinitialliteraturere-
though more general statistical dependencies are
view,implementedthepipelineforcalculatingper-
studied in the supplementary experiments of Ap-
plexity,designedseveralfiguresincludingtheex-
pendixB,theexperimentsinthemainbodyofthe
planatorydiagram,andhelpedcoordinateandman-
paper assume linear dependence and so the inter-
agetheproject.
actingfactorsshouldbeevaluatedinthecontextof
Jyothir S V conceived the initial idea while
our supplementary dependency experiments. We
conducting early experiments to investigate vari-
believeontologicalworkinspiredbyourapproach
ousmemorizationpatternsandtheircharacteristics.
couldimproveonourworkbyincorporatingmore
Healsoengineeredthecompressibility,semantic
generaldependencies.
match,andtextualmatchfeatures.
Anotherlimitationisourdefinitionofmemoriza-
MohammadAflahKhanhelpedengineerthetem-
tion. Thechoiceof32-elicitationhasanumberof
platingfeatures,visualizedandanalyzeddata,and
disadvantages, one of them being that we lose a
helpedwrite.
notionoffuzzyorpartialmemorization,whichis
Jaydeep Borkar conceived, coded, and visual-
consideredimportantinsomecontexts. Arguably,
izedpotentialpredictivefeaturesformemorization
underacounterfactualmemorizationdefinition,we
ChristopherA.Choquette-Chooranearlyexper-
maynotseesubstantialpatternsofeitherrecitation
imentsthatshapedtheproject.
orreconstruction. Themeasurementofmemoriza-
JacobRayFuehneassistedinexploringfeatures
tionisalargeareaofresearchwithmanypossible
bylabellingdataandfeatureengineering.
definitions to choose from (Carlini et al., 2022a;
StellaBidermanofferedhigh-levelguidance,ma-
Tirumalaetal.,2022;Kandpaletal.,2022;Zhang
terialresources,andhelpwithwriting.
et al., 2021;Zhao et al., 2022; Stocket al., 2022;
TracyKesupervisedandadvisedstatisticaltest-
Schwarzschildetal.,2024).4
ing,especiallythedependencytests.
KatherineLeesupervisedandadvisedtheproject
Acknowledgments
andhelpedwithpaperwriting.
This work was enabled in part by a gift from the
Naomi Saphra supervised and advised the
Chan Zuckerberg Initiative Foundation to estab-
projectandledpaperwriting.
lishtheKempnerInstitutefortheStudyofNatural
andArtificialIntelligence. Wewouldliketothank
EleutherAIandCoreWeaveforprovidingthecom- References
putingresourcesusedinthispaper.
Rachit Bansal, Danish Pruthi, and Yonatan Belinkov.
WethankDembaBafordiscussionthatinformed 2023. Measuresofinformationreflectmemorization
thiswork. patterns. Preprint,arxiv:2210.09404[cs,math].
Peter L Bartlett, Philip M Long, Gábor Lugosi, and
AuthorContributions
AlexanderTsigler.2020. Benignoverfittinginlinear
USVSNSaiPrashanthbuiltsamplinginfrastructure regression. ProceedingsoftheNationalAcademyof
Sciences,117(48):30063–30070.
forthememorizedandrepresentativedatasets;de-
signed and trained the classifier that determined StellaRoseBiderman,USVSNSaiPrashanth,Lintang
whetheragivensequencewascodeornaturallan- Sutawika,HaileySchoelkopf,QuentinG.Anthony,
ShivanshuPurohit,andEdwardRaf.2023a. Emer-
4For a discussion of definitions of memorization, see gentandpredictablememorizationinlargelanguage
https://genlaw.org/glossary.html#memorization. models. ArXiv,abs/2304.11158.
9StellaRoseBiderman,HaileySchoelkopf,QuentinG. Tom Henighan, Shan Carter, Tristan Hume, Nelson
Anthony, Herbie Bradley, Kyle O’Brien, Eric Hal- Elhage, Robert Lasenby, Stanislav Fort, Nicholas
lahan,MohammadAflahKhan,ShivanshuPurohit, Schiefer, andChristopherOlah.2023a. Superposi-
USVSNSaiPrashanth,EdwardRaff,AviyaSkowron, tion,memorization,anddoubledescent. Transformer
Lintang Sutawika, and Oskar van der Wal. 2023b. CircuitsThread.
Pythia: Asuiteforanalyzinglargelanguagemodels
acrosstrainingandscaling. ArXiv,abs/2304.01373. Tom Henighan, Shan Carter, Tristan Hume, Nelson
Elhage, Robert Lasenby, Stanislav Fort, Nicholas
Hannah Brown, Katherine Lee, FatemehSadat Schiefer,andChristopherOlah.2023b. Superposi-
Mireshghallah,R.Shokri,andFlorianTramèr.2022. tion,Memorization,andDoubleDescent.
Whatdoesitmeanforalanguagemodeltopreserve
privacy? Proceedingsofthe2022ACMConference DavidAHuffman.1952. Amethodfortheconstruction
onFairness,Accountability,andTransparency. ofminimum-redundancycodes. Proceedingsofthe
IRE,40(9):1098–1101.
NicholasCarlini,DaphneIppolito,MatthewJagielski,
DaphneIppolito,FlorianTramèr,MiladNasr,Chiyuan
KatherineLee,FlorianTramèr,andChiyuanZhang.
Zhang, MatthewJagielski, KatherineLee, Christo-
2022a. Quantifyingmemorizationacrossneurallan-
guagemodels. ArXiv,abs/2202.07646. pherAChoquette-Choo,andNicholasCarlini.2022.
Preventingverbatimmemorizationinlanguagemod-
NicholasCarlini,DaphneIppolito,MatthewJagielski, els gives a false sense of privacy. arXiv preprint
KatherineLee,FlorianTramer,andChiyuanZhang. arXiv:2210.17546.
2023. QuantifyingMemorizationAcrossNeuralLan-
NikhilKandpal,EricWallace,andColinRaffel.2022.
guageModels. arXivpreprint. ArXiv:2202.07646
Deduplicatingtrainingdatamitigatesprivacyrisksin
[cs].
languagemodels. ArXiv,abs/2202.06539.
NicholasCarlini,MatthewJagielski,NicolasPapernot,
Antonia Karamolegkou, Jiaang Li, Li Zhou, and An-
A.Terzis,FlorianTramèr,andChiyuanZhang.2022b.
dersSogaard.2023. Copyrightviolationsandlarge
Theprivacyonioneffect: Memorizationisrelative.
languagemodels. ArXiv,abs/2310.13771.
ArXiv,abs/2206.10469.
Katherine Lee, Daphne Ippolito, Andrew Nystrom,
NicholasCarlini,ChangLiu,ÚlfarErlingsson,Jernej
ChiyuanZhang,DouglasEck,ChrisCallison-Burch,
Kos, and Dawn Xiaodong Song. 2018. The secret
andNicholasCarlini.2021. Deduplicatingtraining
sharer: Evaluating and testing unintended memo-
datamakeslanguagemodelsbetter. InAnnualMeet-
rization in neural networks. In USENIX Security
ingoftheAssociationforComputationalLinguistics.
Symposium.
VladimirILevenshteinetal.1966. Binarycodescapa-
Nicholas Carlini, Florian Tramèr, Eric Wallace,
bleofcorrectingdeletions,insertions,andreversals.
Matthew Jagielski, Ariel Herbert-Voss, Katherine
InSovietphysicsdoklady,volume10,pages707–710.
Lee,AdamRoberts,TomB.Brown,DawnXiaodong
SovietUnion.
Song,ÚlfarErlingsson,AlinaOprea,andColinRaf-
fel. 2020. Extracting training data from large lan- NeilMallinar,JamesB.Simon,AmirhesamAbedsoltan,
guagemodels. InUSENIXSecuritySymposium. ParthePandit,MikhailBelkin,andPreetumNakkiran.
2022. Benign,tempered,orcatastrophic: Ataxon-
Verna Dankers, Ivan Titov, and Dieuwke Hupkes.
omyofoverfitting. Preprint,arXiv:2207.06569.
2023. Memorisation Cartography: Mapping
out the Memorisation-Generalisation Continuum MatthieuMeeus,IgorShilov,ManuelFaysse,andYves-
in Neural Machine Translation. arXiv preprint. AlexandredeMontjoye.2024. Copyrighttrapsfor
ArXiv:2311.05379[cs]. largelanguagemodels. ArXiv,abs/2402.09363.
VitalyFeldman.2021. DoesLearningRequireMem- FatemehsadatMireshghallah, ArchitUniyal, Tianhao
orization? A Short Tale about a Long Tail. arXiv Wang, David Evans, and Taylor Berg-Kirkpatrick.
preprint. ArXiv:1906.05271[cs,stat]. 2022. An empirical analysis of memorization in
fine-tunedautoregressivelanguagemodels. InCon-
LeoGao,StellaRoseBiderman,SidBlack,Laurence ferenceonEmpiricalMethodsinNaturalLanguage
Golding,TravisHoppe,CharlesFoster,JasonPhang, Processing.
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The pile: An Victor Sanh, Lysandre Debut, Julien Chaumond, and
800gbdatasetofdiversetextforlanguagemodeling. ThomasWolf.2020. Distilbert,adistilledversionof
ArXiv,abs/2101.00027. bert: smaller, faster, cheaperandlighter. Preprint,
arXiv:1910.01108.
Valentin Hartmann, Anshuman Suri, Vincent Bind-
schaedler, David Evans, Shruti Tople, and Robert Avi Schwarzschild, Zhili Feng, Pratyush Maini,
West.2023. SoK:Memorizationingeneral-purpose ZacharyC.Lipton,andJ.ZicoKolter.2024. Rethink-
largelanguagemodels. Preprint,arxiv:2310.18362 ingllmmemorizationthroughthelensofadversarial
[cs]. compression. Preprint,arXiv:2404.15146.
10Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo 1. Forevery32-gramwindowinthedatapoint
Huang,DaogaoLiu,TerraBlevins,DanqiChen,and S (comprisedof2049tokens)wecomputea
LukeZettlemoyer.2023. Detectingpretrainingdata
rollinghashandstoreit,alongwiththewin-
fromlargelanguagemodels. ArXiv,abs/2310.16789.
dow’sindexandoffsetinaparallelizedsetof
Edward H Simpson. 1951. The interpretation of in- dataframes.
teraction in contingency tables. Journal of the
RoyalStatisticalSociety: SeriesB(Methodological), 2. For every index position, we compute the
13(2):238–241.
same hash for all S[32: 64] (sequence con-
BenSorscher,RobertGeirhos,ShashankShekhar,Surya tinuations)andstoreall(index,offset,hash)
Ganguli,andAriMorcos.2022. Beyondneuralscal- tuples if their hash is one of the computed
inglaws: beatingpowerlawscalingviadatapruning. sequencescontinuationhashes.
AdvancesinNeuralInformationProcessingSystems,
35:19523–19536.
3. Now, for every sequence continuation, we
PierreStock,IgorShilov,IlyaMironov,andAlexandre look at all 32-gram windows with the same
Sablayrolles. 2022. Defending against reconstruc- hashandcomputetheirnumberofduplicates
tionattackswithrényidifferentialprivacy. Preprint, as the number of equivalent (same set of to-
arXiv:2202.07623.
kensinthesameorder)samples.
Kushal Tirumala, Aram H. Markosyan, Luke Zettle-
moyer,andArmenAghajanyan.2022. Memorization The hash function used is similar to Rabin-
WithoutOverfitting: AnalyzingtheTrainingDynam- Karp’s rolling hash algorithm. Specifically, con-
icsofLargeLanguageModels. arXiv:2205.10770
sideratokensequenceof32tokens.
[cs]. ArXiv: 2205.10770.
S = [c ,c ,c ,...c ]
Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, 1 2 3 32
DongsunKim,DonggyunHan,andDavidLo.2024.
Unveilingmemorizationincodemodels. InProceed- LetusdefinetwoprimesP = 60013andMOD =
ingsoftheIEEE/ACM46thInternationalConference 1018+3. Wedefinetheirhashfunctiontobe
onSoftwareEngineering,ICSE’24,NewYork,NY,
USA.AssociationforComputingMachinery.
H(S) = (c +c ∗P+c ∗P2+...+c ∗P31)%MOD
1 2 3 32
Chiyuan Zhang, Daphne Ippolito, Katherine Lee,
MatthewJagielski,FlorianTramèr,andNicholasCar- A.2 Tokenfrequency
lini. 2021. Counterfactual memorization in neural
languagemodels. ArXiv,abs/2112.12938. Token frequencies are calculated across the Pile.
Foreverysequencecontinuation,weconsiderthe
Xuandong Zhao, Lei Li, and Yu-Xiang Wang. 2022.
maximum,minimum,medianandquartilefrequen-
Provably confidential language modelling. In Pro-
ciesoftokens.
ceedingsofthe2022ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputational
Linguistics: HumanLanguageTechnologies,pages A.3 Compressibility
943–955, Seattle, United States. Association for
We use Huffman Coding length to measure how
ComputationalLinguistics.
easilyasequencecanbecompressed. Compress-
YukunZhu,RyanKiros,RichZemel,RuslanSalakhut- ibilityprovidesaroughgeneralizationofinternal
dinov,RaquelUrtasun,AntonioTorralba,andSanja
repetition, where only a few exceptions to some
Fidler.2015. Aligningbooksandmovies: Towards
simplerepetitionpatternmightneedtobememo-
story-like visual explanations by watching movies
andreadingbooks. InTheIEEEInternationalCon- rized. However,unlikestraightforwardrepetition
ferenceonComputerVision(ICCV). templates,compressiblesequencesmaynotbecon-
sideredtobereconstructedbythemodel. Instead,
A Implementationofmetrics
we include compressibility as a filter to evaluate
whetherLLMsmemorizesamplesthatareeasierto
Wenowprovidedetailsoftheimplementationfor
compressintotheirparameters.
metricsconsideredaspotentialfactorsformemo-
rization.
A.4 IncrementingandRepeatingtemplates
A.1 Numberofexactduplicatesamples A.4.1 IncrementingTemplates
We compute the number of exact duplicates as a Tocheckforanincrementingsequence,weperform
3-stepprocess: thefollowingsteps:
11• Splitthetextbywhitespaceandconvertany – If the current iteration has only numer-
splits which are numerals in non-decimal als, we consider current iteration to be
bases(e.g.,hexadecimal)intobase10. incrementingifallthenumeralsareinan
arithmeticprogression. Ifthedifference
• Removeescapesequences.
inAPis0,weconsiderittoberepeating
instead.
• Within each string, separate contiguous nu-
meric characters from anything else. If two
• Inputsplitsareconsideredasrepeatingifall
contiguousnumericcharactersareseparated
iterations for a given templating length are
byaperiod,combinethemintotheirfloating
repeating.
pointrepresentations.
• Inputsplitsareconsideredasincrementingif
• Discard if there are fewer than 3 potential
atleast one of the iterations for a given tem-
numeralsinthesequence.
platinglengthareincrementingandothers,for
• Check if the sequences are incrementing or the same templating length, are either incre-
repeating. menting(or)repeating.
A.4.2 Repeatingtemplates • For all templating lengths, if any length of
them has been found to be incrementing or
Weperformthefollowingstepstocheckforrepeat-
repeating, we return True (corresponding to
ingsequences:
thefactthatthetextisindeedatemplate)and
• Obtainasequencebysplittingthetextbychar- diff.
acter.
• Notethat,inthecaseofsequencesgenerated
• Check if the sequences are incrementing or while checking for a repeating template, we
repeating. donothaveanynumerals.
Weperformthefollowingstepstodetermineifa B DependencyTestsforInfluenceof
sequencegeneratedfromeitheroftheabovesteps
FeaturesonMemorization
isincrementingorrepeating.
Thissectioncontainsvisualizationsofvariousde-
• Foreverytemplatinglength,definedtobeless pendency tests between memorization likelihood
than half length of splits, and for every po- andourtargetfeatures. Welookatdependencieson
sitionlessthantemplatinglength,weiterate code(Fig. 9),naturallanguage(Fig. 8),andboth
through splits with start position as position (Fig. 7). These tests are more general and have
andstepsizesettotemplatinglength. Wethen strongerguaranteesthansimplylookingatregres-
determineifthecurrentiterationisrepeating sionweights, whichhaveanumberofflaws. For
orincrementing. example,weseeinFig. 6thatregressioncanreallo-
catebiastermstofeaturesthattakeonaconsistent
• For example, if position is 1 and templat-
value,givingthemspuriousweight.
ing length is 5, we iterate through positions
[1,6,11,16...]. ifourinputsplitslengthis10,
C Tablesforscalingexperiments
weiterateforalltemplatinglengths1through
5 and for all positions less than current tem- Figure 4 visualizes the count and proportion of
platinglength memorized samples by category across time and
scale. Wepresenttherawstatisticsforeachtaxo-
• Withineachiteration,wecheck: nomic category across model size in Table 1 and
trainingtimeinTable2.
– Ifthecurrentiterationhasbothtextsand
numerals,itisneitherincrementingnor
D Classifyingexamplesasnatural
repeating.
languageorcode
– Ifthecurrentiterationhasonlytexts,we
considerthecurrentiterationtoberepeat- To train a Natural Language vs Code classifier,
ingifallelementsintheiterationarethe wefine-tuneDistilBert(Sanhetal.,2020)onuni-
same. formly random sampled Bookcorpus (Zhu et al.,
12Figure7: Dependencymeasurementsbetweeninfluencefactorsandmemorization.
Figure8: Dependencymeasurementsbetweeninfluencefactorsandmemorizationfornaturallanguagesamples.
13Figure9: Dependencymeasurementsbetweeninfluencefactorsandmemorizationforcodesamples.
2015) and github-code datasets. We train it with This trend suggests that certain intrinsic factors
learning rate of 10−7 and batch size of 256 for a aboutcodemakeitmoresusceptibletomemoriza-
totalof1000stepsandobservevalidationf1score tion, even for recollection samples where memo-
of0.9950onaheldofevaluationset. rization cannot be attributed to obvious patterns
Toselectanoptimalthresholdforthisclassifier andhighduplication. BothcodeandNLbecome
onmemoriesdataset,werandomlysample500se- morelikelytobememorizedacrossscale,except
quences and manually label them. To make sure forreconstructionsamples,whichremaincompara-
that precision is high for our models, we choose tivelyunchanged.
≤ 0.4 as thresholdfor determining code samples
F Examplesofmemorizedcontinuation
andathresholdof≥ 0.525fordeterminingnatural
sequences
languagesamples,basedonthepointsmarkedin
Figure10,whichmarkpointsofnear100%preci-
Table 3 provides examples of memorized natural
sionforclassifyingeachcategory.
languagetextineachmemorizationcategoryand
Table 4 provides examples of memorized code.
E Likelihoodofmemorizationforcode
Samples are classified using the methodology in
andnaturallanguage
AppendixD.
Westudythelikelihoodthatasamplethathasbeen
confidentlyclassifierascodeorNL(AppendixD)
ismemorizedacrosstimeandscale. Forexample,
forallsamplesconfidentlyclassifiedascodeFig-
ure, what is the proportion of samples which are
memorized?
Figure 11 shows that code samples are more
likelytobememorizedthanNLacrosscategories.
14Figure10: FalsepositiveratesacrossvariousthresholdsonrandomlysampledsequencesofPile. Wechoose≤0.4
asthresholdfordeterminingcodesamplesandathresholdof≥0.525fordeterminingnaturallanguage
Code NL Code NL
101 101 101 101
102 102 102 102
103 103 103 103
104 104 104 104
105 105 105 105
108 109 1010 108 109 1010 0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
101 101 101 101
102 102 102 102
103 103 103 103
104 104 104 104
105 105 105 105
108 109 1010 108 109 1010 0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
101 101 101 101
102 102 102 102
103
103
103
103
104
104
104 104 105
105
105 105
108 109 1010 108 109 1010 0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
101 101 101 101
102 102 102 102
103 103 103 103
104 104 104 104
105
105
105
105
108 109 1010 108 109 1010 0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
Parameters Parameters Training Time Training Time
Figure11: WestudyhowlikelymodelsaretomemorizesamplesconfidentlyclassifiedascodeorNL.Wecalculate
thelikelihoodforeachdistribution(codevs. NL)separately. Figuresincludeprobabilityacrossamodelscaleand
trainingtime. ModelsmemorizeagreaterproportionofcodesamplesthanNLacrossallcategories,modelscale,
andtrainingtime.
15
)deziromeM(P
)noitaticeR(P
)noitcurtsnoceR(P
)noitcelloceR(P
)deziromeM(P
)noitaticeR(P
)noitcurtsnoceR(P
)noitcelloceR(PRecitation Reconstruction Recollection
Model Count Percent Count Percent Count Percent
70m 362,550.0 88.12% 30,430.0 7.40% 18,468.0 4.49%
410m 690,726.0 85.17% 46,076.0 5.68% 74,238.0 9.15%
1b 878,456.0 85.05% 49,253.0 4.77% 105,163.0 10.18%
1.4b 887,549.0 84.68% 49,435.0 4.72% 111,120.0 10.60%
2.8b 1,141,180.0 84.21% 52,416.0 3.87% 161,620.0 11.93%
6.9b 1,416,014.0 84.27% 53,968.0 3.21% 210,314.0 12.52%
12b 1,566,369.0 84.56% 55,114.0 4.10% 249,733.0 11.34%
Table1: Thenumberofmemorizedsamplesforeachtaxonomiccategoryacrossmodelsize. Theseresultsare
visualizedinFigure4(a)and4(b).
Recitation Reconstruction Recollection
Checkpoint Count Percent Count Percent Count Percent
16% 137,681.0 84.25% 7,960.0 4.87% 17,777.0 10.88%
30% 301,146.0 83.92% 15,379.0 4.29% 42,338.0 11.80%
44% 489,629.0 83.69% 23,333.0 3.99% 72,105.0 12.32%
58% 710,823.0 83.42% 31,260.0 3.67% 109,985.0 12.91%
72% 999,867.0 83.63% 38,999.0 3.26% 156,712.0 13.11%
86% 1,308,538.0 83.66% 47,145.0 3.01% 208,372.0 13.32%
100% 1,566,369.0 84.56% 55,114.0 4.10% 249,733.0 11.34%
Table2:ThenumberofmemorizedsamplesforeachtaxonomiccategoryacrosstrainingtimeforPythia12b. 14,000
isthefinalcheckpoint. TheseresultsarevisualizedinFigure4(c)and4(d).
16Category Text Count
-11NASB
Or do you not know that the unrighteous will not inherit the kingdom of God? Do not be
Recitation 175
deceived;neitherfornicators,noridolaters,noradulterers,noreffeminate,norhomosexuals,nor
thieves,northecovetous,nordrunk
2d1234,1238(8thCir.1990). Ontheotherhand,theFederalRulesofCivilProcedurehave
Recitation authorizedfornearly60years"motionsforsummaryjudgmentuponpropershowingsofthe 86
lackofagenuine,triableissueofmaterialfact."CelotexCorp.v.Catrett
viewoflife,food,cocktails,fitness,andfun.
Thisblogisjustaregularguy’sviewoflife,food,cocktails,fitness,andfun. Myopinions,
Recitation 52
musings,observations,rantings,ravings,foodieadventures,andoverallhumorouspontification
of
://www.harpercollins.ca>
NewZealand
HarperCollinsPublishers(NewZealand)Limited
P.O.Box1
Recitation 2303
Auckland,NewZealand
<http://www.harpercollins.co.nz>
UnitedKingdom
Har
000,
fromthetribeofSimeon12,000,
fromthetribeofLevi12,000,
Recitation 7
fromthetribeofIssachar12,000,
fromthetribeofZebulun12,000,
fromthetribeofJoseph12,000,
THEIREAGLEONANDTHEDIRTYBIRDSREADYASWELLDOWNANDGETTHEIR
Reconstruction EAGLEONANDTHEDIRTYBIRDSREADYASWELLDOWNANDGETTHEIREAGLE 4
ONANDTHEDIRTYB
181|182|183|184|185|186|187|188|189|190|191|192|193|194|195|196|197|198|199|200
Reconstruction 2
|201|202
4:14824:14834:14844:14854:14864:14874:14884:14894:14904:14914:14924:
Reconstruction 2
14934:14
1970–71TurkishThirdFootballLeagueseason
Promotionandrelegation:
1971–72TurkishThirdFootballLeagueseason
Reconstruction Promotionandrelegation: 1
1972–73TurkishThirdFootballLeagueseason
Promotionandrelegation:
1973–74TurkishThirdFootball
,28-63,28-64,28-65,28-66,28-67,28-68,28-69,28-70,28-71,28-72,28-73,28-74,28-75,
Reconstruction 3
28-76,28-77,28-78
affectthechild;
¶70(b)Thewishesofthechild,asexpresseddirectlybythechildorthroughthe
Recollection 2
child’sguardianadlitem,withdueregardforthematurityofthechild;
¶71(c)Thecustodialhistoryofthechild,
willbetoolate!
”24“Strivetoenterthroughthenarrowdoor.Formany,Itellyou,willseektoenterandwillnot
Recollection 4
beable. 25Whenoncethemasterofthehousehasrisenandshutthedoor,andyoubeginto
standoutsideandtoknockatthedoor,
,andfreedom.Revava
Revava(),isanOrthodoxJewishIsraelisettlementintheWestBank.LocatedbetweenBarkan
Recollection andKarneiShomron,itfallsunderthejurisdictionofShomronRegionalCouncil. Inithada 2
populationof.
TheinternationalcommunityconsidersIsraelisettlementsin
§2254(d)).
Astate-courtdecisionisconsidered“contraryto...clearlyestablishedFederallaw”ifthetwo
Recollection 5
are“diametricallydifferent,oppositeincharacterornature,ormutuallyopposed.”Williamsv.
Taylor,529U.S.362,405(
Table3: Randomexamplesofnaturallanguage(asclassifiedperAppendixD)fromeachcategoryofmemorization.
17Category Text Count
>-task"> <a class="nav-group-task-link" href="../Extensions/Int.html">Int</a> </li> <li
Recitation 5310
class="nav-group-task"><aclass="nav-group-task-link"href="../Extensions/
><widgetclass="GtkButton"id="entCleanBut"><propertyname="label"translatable="yes">...
Recitation :</strong></p> <ul> <li> <p>Must be one of: <code>true</code>, <code>false</code>, 3689
<code>1</code>,<code>0</code>.</p></li></ul>
= null, CancellationToken cancellationToken = default(CancellationToken)) { if
Recitation (Client.SubscriptionId==null){thrownewValidationException(ValidationRules.CannotBeNull, 227
"this.Client.SubscriptionId");}if
.Object</h3><code>equals,getClass,hashCode,notify,notifyAll,wait,wait,wait</code></li>
Recitation 18963
</ul></li></ul></li></ul></div><divclass="details">
FileEntry("/base1/dir1/",fe,age);fe.name="file3";fi->updateFileEntry("/base1/dir1/",fe,age);
Reconstruction 2
fe.name="file4";fi->updateFileEntry("/base1/
="time2[]"value="5"></td><td><inputtype="checkbox"name="time2[]"value="6"></td>
Reconstruction 2
<td><inputtype="checkbox"name="time2[]"value="7"></td><
XMM3 (1ULL « 28) #define DBG_CTX_EX_PART_FLAG_XMM4 (1ULL « 29) #define
Reconstruction 2
DBG_CTX_EX_PART_FLAG_XMM5(1ULL«30)#defineDBG_
" /> <Compile Include="Message\MFN_M06.cs" /> <Compile In-
Reconstruction clude="Message\MFN_M07.cs" /> <Compile Include="Message\_M08.cs" /> <Compile 3
Include="Message\MFN_
OFFSET + (2 * FPREG_SIZE)) #define PROBE_CPU_Q3_OFFSET
Recollection (PROBE_FIRST_FPREG_OFFSET+(3*FPREG_SIZE))#definePROBE_CPU_Q4_OFFSET 2
(PROBE_FIRST_FPREG_OFFSET+
); } uint16_t WS2812FX::mode_custom_5() { return customModes[5](); } uint16_t
Recollection 2
WS2812FX::mode_custom_6(){returncustomModes[6]();}uint16
XMMM128,__)/*0xDC*/NORMAL("paddusb",MM_XMM,MMM64_XMMM128,__)/*
Recollection 2
0xDD*/NORMAL("paddusw",MM_XMM,MMM64_X
DBF3B/*icon1.png*/;};651A5A7E177AE2D8003DBF3B/*icon2.pnginResources*/=
Recollection 2
{isa=PBXBuildFile;fileRef=651A5A7C177AE2D8003DBF
Table4: Randomexamplesofcode(asclassifiedperAppendixD)fromeachcategoryofmemorization.
18