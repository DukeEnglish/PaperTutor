Text-Animator: Controllable Visual Text Video Generation
LinLiu1 QuandeLiu2 ShengjuQian2 YuanZhou3
WengangZhou1 HouqiangLi1 LingxiXie4 QiTian4
1 EEISDepartment,UniversityofScienceandTechnologyofChina
2 Tencent 3NanyangTechnicalUniversity
4HuaweiTech.
A coffee mug on the office desk with the words ‘CAFE’ A raccoon stands in front of a street wall with the words ‘WELCOME’
A road sign on the road with the word ‘STOP’ On a street during the night, neon lights spell out ‘小吃街’
A bottle of perfume with the words 'CLASSIC' A girl wearing a blue T shirt with ‘beauty’, seaside background
Figure1. Givenasentencewithvisualizedwords, ourText-Animatorisabletoproduceawiderangeofvideosthatnotonlyshowthe
semanticinformationofgiventextprompts,butfurtheralignwiththevisualizedwords.Ourmethodisaonestagemethodwithoutfurther
tuning.
Abstract tual fidelity and motion coherence. In this paper, we pro-
pose an innovative approach termed Text-Animator for vi-
sual text video generation. Text-Animator contains a text
Videogenerationisachallengingyetpivotaltaskinvar- embedding injection module to precisely depict the struc-
ious industries, such as gaming, e-commerce, and adver- turesofvisualtextingeneratedvideos.Besides,wedevelop
tising. OnesignificantunresolvedaspectwithinT2Visthe a camera control module and a text refinement module to
effective visualization of text within generated videos. De- improve the stability of generated visual text by control-
spite the progress achieved in Text-to-Video (T2V) genera- ling the camera movement as well as the motion of visu-
tion, currentmethodsstillcannoteffectivelyvisualizetexts alized text. Quantitative and qualitative experimental re-
invideosdirectly,astheymainlyfocusonsummarizingse- sultsdemonstratethesuperiorityofourapproachtotheac-
manticsceneinformation,understanding,anddepictingac- curacy of generated visual text over state-of-the-art video
tions. Whilerecentadvancesinimage-levelvisualtextgen- generation methods. The project page can be found in
eration show promise, transitioning these techniques into
thevideodomainfacesproblems,notablyinpreservingtex-
4202
nuJ
52
]VC.sc[
1v77771.6042:viXralaulampaul.github.io/text-animator.html. textsthatshouldbegenerated.Ourmethodnotonlyreflects
thesemanticsofthecompletetext,butalsounderstandsthe
fine-grainsemanticsoftheinputvocabulary,andeffectively
1.Introduction aggregatesthetwointermsofcontentwhilemaintaininga
goodmotionassociation(unabletovisualizethemovement
Video generation has become an important cornerstone in
oftextandothercontent).
content-based generation, and has huge potential value in
To achieve these goals, we propose a novel method
various domains including e-commerce, advertising, the
calledText-Animator. DifferentfrompreviousT2Vmeth-
film industry, etc. For instance, in advertising scenarios,
ods, Text-Animator contains a text embedding injection
it is essential to display a manufacturer’s specific logo or
module to enhance its precise understanding and genera-
sloganinthegeneratedvideointheformoftext,whilealso
tioncapacityforvisualtext. Besides, aunifiedcontrolling
seamlesslyintegratingtextwiththeproductsfeaturedinthe
strategy with camera and text position control is designed
video(e.g. apieceofclothing). However,incurrentvideo
toimprovethestabilityofthemovementofvisualizedtext
generationapproaches,thevisualizationoftext/wordsinthe
and image content, thereby achieving unity and coordina-
generated video remains a challenging yet unresolved is-
tion of the text movements. Specifically, for camera con-
sue. For example, in the first example of Fig. 1, we need
trol, the control information is applied to the main body
to engrave the word ”CAFE” on the mug and ensure that
of the network by considering the features of the camera’s
themovementofthetextandthemugappearseamlessand
motion trajectories. The position control aims at control-
harmoniousinthevideo.
ling the specific position and size of visual text generated
CurrentT2Vmethodsareunsuitableforthesesettings,as
invideos. Owingtothecomprehensivecontrollingstrategy
theytypicallyfocusonunderstandingthesemantic-levelin-
overthedevelopedtextembeddinginjectionmodule,Text-
formationfromagivenpromptratherthaninterpretingspe-
Animatorshows asuperiorcapacity togeneratestable and
cific words themselves. For instance, given a text input as
accuratevisualtextcontentinvideos.
“apersonwalkingontheroad,”currentT2Vmodelscanin-
Insummary, thecontributionsofthispapercanbecon-
terpretthesceneandproduceacorrespondingvideoabouta
cludedbelow:
personwhowalksontheroad. However,thesemodelsfail
• We propose Text-Animator, a novel approach that can
to understand prompts at a more granular level. If the text
generate visual text in videos and maintain the structure
inputismodifiedto“apersonwalkingontheroad,wearing
consistencyofgeneratedvisualtexts. Toourknowledge,
aT-shirtwiththeword’HelloWorld’printedonit,”thegen-
thisisthefirstattemptatthevisualtextvideogeneration
erated results of current methods are far from satisfactory,
problem.
duetotheirinabilitytoaccuratelyinterpretthegenerationof
• WedevelopatextembeddinginjectionmoduleforText-
the texts ’Hello World’ and incorporate its associated mo-
Animator that can accurately depict the structural infor-
tioninformationeffectively.
mationofvisualtext. Besides,wealsoproposeacamera
Recently, some preliminary efforts have been made
controlandtextrefinementmoduletoaccuratelycontrol
in the field of visual text generation, specifically in the
thecameramovementandthemotionofthegeneratedvi-
paradigm of Text-to-Image (T2I) generation [17, 27, 37].
sualtext,toimprovethegenerationstability.
Thesetrialshaveshownpromisingresults,buttheyareonly
• Extensive experiments demonstrate that Text-Animator
limited to the image domain. When extending this task to
outperforms current text-to-video and image-to-video
videoscenarios,anintuitiveapproachistouseimagesgen-
generationmethodsbyalargemarginontheaccuracyof
eratedbythesemethodsasinputforcutting-edgeimage-to-
generatedvisualtext.
video(I2V)methods. However,mostcurrentI2Vmethods
either focus on learning motion patterns in simply natural
2.RelatedWork
scenes[2,6,29]ordeliberatelyomitdatathatincludevisual
textsduringdatasetcollection[2]. Asaresult,videosgen-
2.1.VisualTextGeneration
eratedbythesemethodsfallintoadilemmagenerallycalled
textcollapse,whichmeansthatasthenumberofframesin- The goal of visual text generation is to integrate user-
creases, the visualized text becomes increasingly blurry or specified texts into images or videos and produce well-
losesitsoriginalstructure(asdemonstratedinSec. 4ofthis formedandreadablevisualtext,thereforeeffectivelyensur-
paper). Therefore, it is difficult to directly extend visual ingthatthetextsfitwellwiththecorrespondingimagecon-
textgenerationmodelsfromtheimagedomaintothevideo tent. Currentresearchmainlyfocusesonhowtodesignan
domain. effective text encoder and considers the better guidance of
Based on the above observations, we propose an effec- text-conditionedcontrollinginformation. Fortextencoder,
tivesolutionforvisualtextvideogeneration,whichcanef- aslargelanguagemodelsdevelop[20,21],itisapromising
fectivelyundertextsforthedescriptionofvideosandvisual ideatodirectlyusethesemodelstoencodetext. However,thisroadmapinevitablyresultsinoverlookingthecharacter set, limiting their capability in controllable video genera-
featuresoftexts. Recently,someworkshaveoptimizedtext tion. Besides,VideoComposer[31]proposestousemotion
encodersforcharacterfeatures. GlyphDraw[17]fine-tuned vectors to control the video motion. MotionCtrl [33] de-
thetextencoderforChineseimagesforglyphembeddings. signs two control modules for camera motion and object
Chenetal.[4]trainedaglyph-extractedimageencoderfor motioncontrol. Drag-NUWA[38]usestrajectoriesandtext
imageediting. AnyText[27]utilizespretrainedrecognition promptsinajointwayforvideogenerationconditionedon
model,PP-OCRv3[5]forencodingtext. an initial image. Different from these approaches, a dual
To generate characters more accurately, control infor- controlvisualtextgenerationmodelisutilizedinourText-
mation of the text is required as additional input. Glyph- Animator,wherecameraposeinformationandpositiontra-
Draw[17]usesexplicitglyphimagesasconditionstoren- jectories can effectively control the motion of videos and
der characters. GlyphControl [37] and AnyText [27] em- makethegenerationprocessmorestable.
bed text conditions in the latent space by the combination
ofthepositionsoftextboxesandtherenderedglyphs. dif- 3.Method
ferentfrom[17,27,37],Yangetal.[37]usecharacter-level
In this section, we first introduce the pipeline of our Text-
segmentationmasksasconditionedcontrollinginformation,
AnimatorinSec. 3.1. Then, thedetailsofthekeycompo-
allowing for finer granularity control. To our knowledge,
nents are introduced in Sec. 3.2, Sec. 3.3, and Sec. 3.4
currentmethodsmainlyfocusonaddressingthevisualtext
respectively.
generationprobleminthetext-to-imagedomain,whichcan-
not be utilized to tackle text-to-video visual text genera- 3.1.Text-conditionedVideoGenerationPipeline
tion.Inthispaper,wefirstexplorethevisualtextgeneration
Firstly, we introduce the overall framework of our net-
taskinthevideodomain.
work, as shown in Fig. 2. Our method consists of four
2.2.VideoGeneration parts that are Text Embedding Injection Module, Cam-
era Control Module, Text Glyph and Position Refinement
Sora [3], a recent famous video generation model has at-
Module, and 3D-UNet Module. Given the integrated texts
tracted much attention from both the community of both
T , position map P , and Camera Pose Information
in 1,ori
industryandacademia. Beforetheemergenceofdiffusion-
(K ,E ,K ,E ,...,K ,E ),theglyphmapG isgen-
1 1 2 2 n n 1,ori
based models, lots of effort in this field have been paid on
eratedbyrenderingtextsT usingauniformfontstyleonto
in
methods based on GANs [9] or VQVAE [19, 28]. Among
an image based on their locations. Then, the video posi-
these methods, the pre-trained Text-to-Image (T2I) model
tionmapsP ,...,P andglyphmapsG ,...,G aregener-
1 n 1 n
CogView2 [7] is utilized in CogVideo [7] as the back-
ated by warping the P and G using camera pose
1,ori 1,ori
bone, to enable generating long sequence videos in an
information. CameraControlModuleandTextEmbedding
auto-regressive way. Based on autoregressive Transform-
InjectionModuleoutputmulti-scalecontrolfeaturescorre-
ers,NUWA[34]combinesthreetasks,whichareT2I,T2V,
spondingtotheircontrolinputrespectively. Thenoisez is
t
andvideoprediction.
fed into a 3D-UNet (the architecture of the 3D-UNet used
Currently,diffusionmodelshavebecomethemainstream
inourworkisassameasthatusedinAnimateDiffv3[10])
method in video generation. Make-A-Video [24] proposes
toobtainanoutputϵ . Intheinferencestage,thisoutputis
t
to learn visual-textual correlations and thus capture video
passed through the decoder of the VAE to obtain the final
motionfromunsuperviseddata. Somemethods[1,13,30,
outputvideos.
44] design effective temporal modules to reduce computa-
Recently, diffusion models have served as a primary
tional complexity and model temporal relationships effec-
framework for T2V generation and yielded promising re-
tively.Multi-stageapproaches[23,32,39]designmodelsto
sults. Current T2V methods are derived from the original
beusedinstagesforachievinghigh-definitionvideogener-
formulationsofdiffusionmodelsusedinimagegeneration.
ation. These methods highlight the versatility and efficacy
More specifically, we generate the latent representation z
0
ofdiffusionmodelsinadvancingvideogenerationcapabil-
byapplyingVariationalAutoencoder(VAE)[14]onthein-
ity.
put video x . Then, a sequence of N latent features of z
0 0
isgraduallyperturbedwithnoiseϵfromanormaldistribu-
2.3.Controllablevideogeneration
tion over T steps. Given the noisy input z , a neural net-
t
InadditiontoconventionalT2Vmodels,somemethodsfo- workϵˆ istrainedtopredicttheaddednoise. Inourwork,
θ
cusonmakingvideogenerationcontrollable.Inthesemeth- weinjectdualcontrolsignals(positioncontrolandcamera
ods,[12,35,36,42]turntorefertospecificvideotemplates control) into the denoising process, strengthening the sta-
for controlling motion. However, despite the effectiveness bility of video generation. Specifically, these two control
of these methods in motion controlling, they typically re- featuresarefirstfedintoadditionalControlNetN andN
p c
quire training new models on each template or template respectively, then injected into the generator through vari-Text Embedding Text Glyph and Position
Injection Expansion Size Refinement Module (Sec. 3.4)
Module (Sec. 3.2)
Video Position Video Position &
Refine Glyph Generation
Position Glyph
Text &
Camera Pose Information
Fuse Position Camera
ControlNet ControlNet
Feature
Camera Control Module (Sec. 3.3)
C
A delicate square cake
Prompt with the word 'HAPPY'
written on it VAE
C Cross Attention
2D U-Net Block
Temporal Attention Block T times Diffusion Pipeline
Figure2.FrameworkofText-Animator.Givenapre-trained3D-UNet,thecameraControlNettakescameraembeddingasinputandoutputs
camerarepresentations; thetextandpositionControlNettakesthecombinationfeaturez asinputandoutputspositionrepresentations
c
Thesefeaturesarethenintegratedintothe2DConvlayersandtemporalattentionlayersof3D-UNetattheirrespectivescales.
ousoperations.Hence,theobjectiveoftrainingourencoder FromthetopleftofFig. 2, wecanseethattheinputto
isshownbelow: the position and glyph control module is the position map
P ,P ,...,P and glyph map G1,G ,...,G generated by
1 2 n 2 n
the module in Sec. 3.4. We extract features of glyphs and
L=E [∥ϵ−ϵˆ (z ,c ,N (s ),N (s ),t)],
z0,ϵ,ct,sp,sct θ t t p p c c positionsseparatelyusingglyphconvolutionblocksandpo-
(1)
sition convolution blocks, respectively. Then, we merge
where c is the embeddings of the corresponding text
t thesefeaturesusingafusionconvolutionblock. Finally,af-
prompts, s is the set of the position maps and glyph
p ter combining these features with the noisy input Z , they
t
maps, and s is the set of camera pose information (s =
c c areinputtedintothetextandpositionControlNet. Thetext
K ,E ,K ,E ,...,K ,E ).
1 1 2 2 n n and position ControlNet output multi-scale feature maps
3.2.TextEmbeddingInjectionModule FP. FollowingtheControlNet[40],wefusethesefeatures
k
into the intermediate block and upsampling blocks of the
In the generation of videos with visual text, the first con-
UNet network, where they are directly added to the corre-
sideration is how to effectively embed the visual features
spondingfeatures.
of the required text into the base model (the pre-trained
UNet model). Inspired by previous methods of visualiz-
3.3.CameraControlforStableTextGeneration
ingtextinimages[27,37],weembedtextconditionsinthe
latent space by combining the positions of text boxes and After incorporating the text embedding injection module,
the rendered glyphs. Text boxes indicate the positions in ourmethodisnowcapableofgeneratingvisualtextvideos
the generated image where rendering should occur, while with text that moves following the scene. However, this
the rendered glyphs utilize existing font style (i.e., ‘Arial text movement can sometimes become disconnected from
Unicode’)topre-initializethestyleofthecharacters. Inad- themovementofobjectswithinthevideo. Forinstance,in
dition, unlike image generation, video generation involves theprompt‘Asignthatsays‘STOP’,’thetextpart”STOP”
processingfeaturesacrossmultipleframes. Toleveragethe might move to the right while the sign moves to the left.
pre-trained feature extractor used in image generation, we Togeneratemorestablevideos,additionalcontrolmodules
extractfeaturesfromeachframeusingaframe-wisefeature need to be designed. Therefore, we propose to use cam-
extractor, and then concatenate these features before feed- era pose information to control the movement of text and
ingthemintoapre-trainedUNetmodel. ensure consistency with the scene content. In this section,wewillprimarilydiscusshowtoembedcameraposeinfor- video,weobservedinexperimentsthattherelativeposition
mation into the underlying model. In the next section, we and size of the position feature map have a certain impact
will explore how to relate camera pose information to the on the final generation results. If the position feature map
positionandglyphmapsdiscussedinSection3.2. issmaller,itaffectsthediversityofgeneratedtext,resulting
To effectively embed the camera pose information in visual text that does not harmonize well with the con-
(K ,E ,K ,E ,...,K ,E ) into the camera ControlNet, tent in the video. Conversely, if the position feature map
1 1 2 2 n n
followed [11,25],weusethepluckerembedding. Andwe islarger, itmayleadtogeneratedtextcontainingincorrect
briefly introduce it as follows. A point (u,v) in the im- orrepeatedcharacters. Therefore,wedesignapositionre-
ageplaneisrepresentedasp = (o×d ,d ) ∈ R6, finement module. First, we extract the centroid of the ini-
u,v u,v u,v
where o ∈ R3 denotes the camera center in the world co- tialpositionmapP n,oriandrendertheglyphmapG n,oriat
ordinatespaceandd ∈ R3 representsadirectionalvec- specific positions. Then, we extract the convex hull of the
u,v
torinworldcoordinatespace,calculatedusingtheformula glyphmapandexpanditbyaddinganexpansionfactoreto
d
u,v
= RK−1[u,v,1]T +t. Randtrefertotherotation generateanewpositionmapP n.
matrix and the translation vector, respectively. Thus, the
embedding can be expressed as P ∈ R6×n×H×W, where 4.Experiments
H and W are the height and width for the frames and n
4.1.ImplementationDetails
representstheframenumber.
ThecameraControlNetconsistsoffourblocks, eachof We choose the AnimateDiffV3 [10] as the base text-to-
them comprising a residual-based convolutional block and video (T2V) model. The weights of the model’s mo-
atransformer-basedtemporalattentionblock, allowingthe tion module are initialized with AnimateDiffV3 [10]. The
network to learn temporal relationships within the camera weightsofotherpartsareinitializedwithDreamShaper[16]
poseinformation.Thenetworkoutputsmulti-scalefeatures, ororiginalSD1.5[22]. Cameracontrolnetandtextandpo-
Fc ∈ R(b×hk×wk)×n×ck. Afterobtainingmulti-scalecam- sition controlnet are trained using methods and datasets in
k
era features, it’s necessary to integrate these features into [11] and [27]. Finally, all the parts are aggregated and the
the3D-UNetarchitectureoftheT2Vmodel. Theimagela- parametersarefixedforinference. ImagedimensionsofG
tentfeaturesz kandthecameraposefeaturesF karedirectly andP aresettobe1024×1024and512×512,respectively.
fusedthroughpixel-wiseaddition. Theexpansionsizeeissetto1.2. Duringthesamplingpro-
cess, werandomlyselectedsomehintprompts (like‘these
3.4.AuxiliaryTextGlyphandPositionRefinement
textsarewrittenonit: xxx’)and concatenated themtothe
caption. The inference step and the guidance scale are set
To enable the collaboration between the camera control
to 25 and 7.5, respectively. Finally, the model outputs the
moduleandthetextembeddinginjectionmodule,itisnec-
videoswiththesize16×256×384.
essary to use the camera position information from videos
as guidance to generate the position map and glyph map
4.2.DatasetandMetrics
ofsubsequentframesbyconsideringtheguidancefromthe
firstframe. Thegenerationmethodisasfollows. Because of lacking the Text-to-video dataset for visual
Giventhefirstframe’smap(positionmaporglyphmap) text generation evaluation, we use the LAION subsets of
of the first frame, M, the intrinsic parameters K, and the AnyText-benchmark[27]forevaluatingtheeffectivenessof
transformation matrix T of the first frame, and the trans- visualtextvideogeneration. However,inthisdataset,some
1
formation matrix T of the n-th frame. We first calcu- images have text and main content separated, while others
n
late the transformation matrix T =T−1T from the consist only of text without any image content, which is
1to2 1 2
first frame to the second frame, and build the projection meaningless for video generation. Therefore, we selected
matrix P=KT . Next, the pixel coordinate matrix about90imagesfromthedatasettoformthetestset,which
1to2
of the first frame is converted to three-dimensional points isnamedtheLAIONsubset.
Point in the camera coordinate system. Here, due Firstly,weneedtoassesstheaccuracyandqualityoftext
cam13d
tolackingdepthinformation,weassumethatrenderedtexts generation. Accordingtothepaper[27], weemployedthe
on the same line is at the same depth. Then, the relative Sentence Accuracy (Sen. Acc) metric, where each gener-
transformation matrix T is used to transform to the atedtextlineiscroppedaccordingtothespecifiedposition
1to2
secondframecameracoordinatesystemandprojectitback andfedintoanOCRmodeltoobtainpredictedresults. Ad-
onto the pixel plane using P, followed by a normalization ditionally,theNormalizedEditDistance(NED)[18]isused
operation. After normalization, the projected coordinates to measure the similarity between two strings. To ensure
are constrained within the image boundary and filled into that our method has better video generation capabilities,
thesecondframeimage. we utilize the Fre´chet Inception Distance (FID) to assess
After generating the position and glyph maps from the thevideoappearancequalitybetweengeneratedvideosandreal-world videos. Moreover, we also adopted the Prompt erate specific texts and maintain textual consistency over
similarityandtheFramesimilaritymetric.Theformereval- time. Compared to SVD, our model not only accurately
uatesthesemanticsimilaritybetweentheinputdescription renders each character (ours: ‘HELLO’ vs SVD: ‘HELO’
andoutputvideo,whilethelatterevaluatesthecontinuityof or Pika: ‘HHLLLO’), but also maintains consistency over
thegeneratedvideos. time. SVDfailstolearnthemotioninformationofthetext,
causingthetexttobecomeincreasinglydisorderedastime
4.3.QuantitativeResults
passes.
ThequantitativeresultsareshowninTable1.Thecompared As for comparison with specific visual text generation
methodsaredividedintotwoparts.Thefirstpartisthecom- works,sincethereiscurrentlynoT2Vworkspecificallyde-
binationofthespecificimagevisualtextgenerationworks signedforvisualtextgeneration,wecontrastourapproach
(GlyphControl[37]andAnytext[27])+state-of-the-artI2V withmethodscombiningspecificT2Iworksforvisualtext
works (AnimateLCM [29], I2VGEN-XL [41]). The sec- generation (such as GlyphControl [37] and Anytext [27])
ondpartistheone-stagemethod. WeusetheAnimatediff- andstate-of-the-artI2Vworks(suchasAnimateLCM[29],
SDXL as the base model and two finetuned lora weight I2VGen-XL,andSVD[2]).AsshowninFig.4,ourmethod
from CIVIAI, denoted as Animatediff-SDXL (Text Lora shows superior integration of generated text with back-
A)1 and Animatediff-SDXL (Text Lora B)2 in Table 1 re- ground, while Anytext cannot generate the seaside back-
spectively. Thesetwoloraweightarefinetunedusingsome ground. WhenusingI2Vmethodstogeneratevideosfrom
imageswithvisualtext. FromTable1,wecanseethatthe reference frame images, the text parts are often blurred or
parameters of these methods are much larger than that of distorted. Our approach maintains the clarity of the text
ourmethod(over41%).Moreover,ourmethodsignificantly parts well and moves in coordination with the image con-
outperformsothermethodsintermsoftheaccuracyofgen- tent. Besides, in Fig. 5, we show one example of the
eratingvisualtext,asmeasuredbyevaluationmetricsSen. LAION-subsetdataset. Onlyourmethodcancorrectlydis-
ACCandNED(leadingby191.8%and30.4%respectively play the visual characters (CHRISTMAS) and the number
compared to the best method). This reflects the accuracy ofbags(two).
of the text generated by our method, and the text does not Atthesametime,wealsoconductedexperimentstover-
collapseinthegeneratedvideos. Asforthemetricmeasur- ifytherobustnessofourmethod. InFig.6,wedemonstrate
ingthesimilaritybetweenthegeneratedvideoandtheinput therobustnessofourmethodforlargemovementinthetext
text(FIDandPromptsimilarity), ourmethodachievedthe region. TheexistingSOTAmethodsdeformedthetextarea
second-best result. In terms of Prompt Similarity, the gap duringsmallmovements(asshownintheexampleabove),
withthebestmethodisonly0.6%. Inthemetricmeasuring sothevisualizationresultsofthesemethodsarenotshown
video stability and frame Similarity, our method achieved here. The texts for these two examples are ‘A coffee mug
the second-best result. We observed that the best method, withthewords‘cafe’ontheofficedesk’and‘Abottomof
Pika,tendstogeneratevideoswithsmallermovements,giv- milkwiththewords‘MILK’.Thedirectionofmovementis
ingthemanadvantageinthismetric. fromrighttoleft. Wecanseethatthestructureofourtext
Besides, in Table 2, we also compare with Open- can still be maintained even with a large range of camera
SORA[43]andthreerecentSOTAAPI,MorphStudio[26], movements. InFig.7,wedemonstratethatunderthesame
Pika [15], and Gen-2 [8]. Open-SORA and Morph Studio camerainformation,wecancontrolitsmovementspeedby
donothavetheSen. ACCscorebecausetheycannotgen- samplingthecamerainformationoftheintervalframes. At
eratecorrectsentencesorwords. Ourmethodsignificantly aspeedof4or6timestheoriginalspeed,ourmethodisstill
outperformsothermethodsintermsofSen. ACCandalso abletomaintainthestructureofthetext.
performsbetterthanothermethodsinNED.
4.5.AblationStudy
4.4.QualitativeResults
Inthispart,toillustratethecontributionsofourmethod,we
Inthissubsection,wefirstcomparedourmodelwithstate- conduct ablation studies on LAION-subset. The quantita-
of-the-artT2VmodelsorAPIsinthefieldoftext-to-video tivecomparisonsareshowninTable3.
generation (including ModelScope [30] SVD [2] (Stable Dual control: We conduct an ablation study to ana-
Video Diffusion), AnimatedDiff [10], Open-SORA [43], lyzetheeffectivenessofthedualcontroldesign. Generally
and Pika [15]) as shown in Fig. 3. These models show speaking,itisfeasibletouseonlypositionboxesforguid-
the ability on context understanding, but they fail to gen- ance without using camera poses. Therefore, we designed
the‘W/ocameracontrol’model,whichremovedthecamera
1This lora model is from https://civitai.com/models/
guidancemodulecomparedtotheoriginalmodel. Inaddi-
419492?\modelVersionId=467355
2This lora model is from https://civitai.com/models/ tion, weremovedthepositionblockandonlyusedcamera
221240/texta-generate-text-with-sdxl poseandglyphembedding,andnamedthismodel‘W/opo-Table 1. Quantitative comparison results on the LAION-subset dataset. The best results are shown in Bold and the second best are
underlined.
Method Parameters Sen.ACC↑ NED↑ FID↓ Promptsimilarity↑ Framesimilarity↑
Anytext[27]+AnimateLCM[29] 2726M 0.220 0.615 153.7 33.62 75.91
Anytext[27]+I2VGen-XL[41] 2785M 0.267 0.582 184.9 30.18 79.74
GlyphControl[37]+AnimateLCM[29] 2625M 0.139 0.303 182.3 34.00 76.03
GlyphControl[37]+I2VGen-XL[41] 2684M 0.197 0.298 186.1 32.26 79.98
Animatediff-SDXL(TextLoraA)[10] 2927M 0.209 0.555 262.1 32.72 74.22
Animatediff-SDXL(TextLoraB)[10] 2927M 0.197 0.528 275.2 32.51 78.10
Ours 1855M 0.779 0.802 180.6 33.78 92.66
ModelScope SVD AnimateDiff Open-SORA Pika Ours
Figure3. QualitativecomparisonofText-Animatorandstate-of-the-artT2VmodelsorAPIsinvisualtextgeneration. Thepromptis‘A
redpandaisholdingasignthatsays‘HELLO”.
Table 2. Quantitative comparison results on the LAION-subset Table3.AblationstudiesontheLAION-subsetdataset.
datasetwithsomeT2Vmethods.
Method Sen. ACC NED↑ FID↓
Method Sen.ACC↑ NED↑
W/ocameracontrol 0.755 0.786 183.2
Open-SORA[43] – 0.081 W/opositioncontrol 0.732 0.775 185.7
MorphStudio[26] – 0.255
W/opositionrefinement 0.755 0.763 180.9
Pika[15] 0.267 0.611
Expansionsize=0.9 0.779 0.804 181.9
Gen-2[8] 0.279 0.708
Expansionsize=1.4 0.767 0.791 181.3
Ours 0.779 0.802
Fullmodel 0.779 0.802 180.6
sitioncontrol’.InTable3,wecanseethatonthexxxmetric,
theperformanceofthe’W/ocameracontrol’modelhasde- mentisremoved,weusethedefaultpositionintheLAION
creasedby0.016onNEDcomparedtotheoriginalmodel, subset. Andwedenotethemodelas‘w/oPositionRefine-
and the performance of the ’W/o position control’ model ment’inTable3. Wecanseethattheoriginalpositionwill
has decreased by 0.027 on NED compared to the original decrease the accuracy. Besides, we conduct experiments
model. about the proper expansion size. We tried two expansion
Positionrefinementandexpansionsize: Wealsocon- coefficients:0.9(smallerthan1.2)and1.4(largerthan1.2).
duct experiments to analyze the effectiveness of our pro- Itcanbeobservedthatalthoughthesmallerexpansionco-
posedrefinementmodule. Whenthevideopositionrefine- efficient improves the accuracy of the text in the video, itGlyphControlimage Anytextimage Anytextimage Anytextimage
Video Video Video Video
GlyphControl+ Anytext+ Anytext+ Anytext+
Ours
AnimateLCM AnimateLCM I2VGEN-XL SVD
Figure4.QualitativecomparisonofText-Animatorandthecombinationofstate-of-the-artT2Ivisualtextgenerationmodels(GpyphCon-
trolandAnytext)andI2Vmodels(AnimateLCM[29],I2VGen-XL[41],andSVD).Thepromptis‘AgirlwearingablueT-shirtwiththe
words‘BEAUTY’,slightsmile,seasidebackground’.
AnimateDiff Anytext+ GlyphControl+ AnimateDiff+
Pika
+ Lora A i2vgen-xl i2vgen-xl Lora B
OpenSora Gen-2 Morph Studio Ours Ground Truth
Figure5. QualitativecomparisonofText-AnimatorandothersononeexampleoftheLAION-subsetdataset. Thepromptis‘Twobags
withtheword’CHRISTMAS’designedonit’.Othermethodscannotgeneratethecorrectword(Pleasezoomtoseetheresults).
negativelyimpactsthequalityofthevideogeneration. On the visual text video generation domain. Text-Animator
theotherhand,thelargerexpansioncoefficientcausessome emphasizes not only semantic understanding of text but
characterstoappearrepeatedlyinthevideo,therebyreduc- alsofine-grainedtextualsemantics,ensuringthatvisualized
ingtheaccuracyofthetext. text is dynamically integrated into video content while
maintaining motion coherence. Our approach introduces
5.Conclusion dual control mechanisms—camera and position control-
to synchronize text animation with video motion, thereby
In conclusion, this paper presents Text-Animator, an enhancingunityandcoordinationbetweentextualelements
innovativeapproachtoaddressthechallengeofintegrating andvideoscenes.Throughextensivequantitativeandvisual
textual elements effectively into generated videos withinUniversaltexteditingdiffusionmodel. AdvancesinNeural
InformationProcessingSystems,36,2024. 3
[5] Ruoyu Guo Xiaoting Yin Kaitao Jiang Yongkun Du Yun-
ing Du Lingfeng Zhu Baohua Lai Xiaoguang Hu Dianhai
YuYanjunMaChenxiaLi,WeiweiLiu. Pp-ocrv3:Moreat-
temptsfortheimprovementofultralightweightocrsystem.
arXivpreprintarXiv:2206.03001,2022. 3
[6] ZuozhuoDai,ZhenghaoZhang,YaoYao,BingxueQiu,Siyu
Frame 1 Frame 5 Frame 10 Frame 15
Zhu,LongQin,andWeizhiWang. Animateanything: Fine-
Figure6.Theexampleoflarge-areatextmovement,demonstrates grained open domain image animation with motion guid-
thatourmethoddoesnotcausedamagetothetextwhenmoving ance. arXive-prints,pagesarXiv–2311,2023. 2
textoveralargearea. [7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
Cogview2: Faster and better text-to-image generation via
frame 1 frame 5 frame 10 hierarchicaltransformers. AdvancesinNeuralInformation
ProcessingSystems,35:16890–16902,2022. 3
[8] Gen-2, September 25, 2023. https://research.
Speed=1.0x
runwayml.com/gen2. 6,7
[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and
Yoshua Bengio. Generative adversarial nets. Advances in
Speed=4.0x
neuralinformationprocessingsystems,27,2014. 3
[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao,DahuaLin, andBoDai. Animatediff: Animateyour
personalizedtext-to-imagediffusionmodelswithoutspecific
Speed=6.0x
tuning. arXivpreprintarXiv:2307.04725,2023. 3,5,6,7
[11] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo
Dai,HongshengLi,andCeyuanYang.Cameractrl:Enabling
Figure7.Thecomparisonofthesametextandcamerainformation
cameracontrolfortext-to-videogeneration. arXivpreprint
at different speeds. The prompt is ‘A delicious and square cake
arXiv:2404.02101,2024. 5
withthewords‘HAPPY”.
[12] XuanhuaHe,QuandeLiu,ShengjuQian,XinWang,TaoHu,
KeCao,KeyuYan,ManZhou,andJieZhang. Id-animator:
Zero-shotidentity-preservinghumanvideogeneration.arXiv
experiments, we have demonstrated that Text-Animator
preprintarXiv:2404.15275,2024. 3
outperforms existing T2V and hybrid T2I/I2V methods in
[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
termsofvideoqualityandfidelityoftextualrepresentation.
QifengChen.Latentvideodiffusionmodelsforhigh-fidelity
Our contributions not only address current challenges
long video generation. arXiv preprint arXiv:2211.13221,
but also inspire further exploration and innovation in this
2022. 3
rapidly evolving field of multimedia content generation.
[14] DiederikPKingmaandMaxWelling. Auto-encodingvaria-
tionalbayes. ICLR,2014. 3
References [15] Pikalabs,2023. https://www.pika.art/. 6,7
[16] Lykon, 2023. https://huggingface.co/Lykon/
[1] JieAn,SongyangZhang,HarryYang,SonalGupta,Jia-Bin dreamshaper-8. 5
Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu- [17] JianMa,MingjunZhao,ChenChen,RuichenWang,DiNiu,
sion with temporal shift for efficient text-to-video genera- Haonan Lu, and Xiaodong Lin. Glyphdraw: Learning to
tion. arXivpreprintarXiv:2304.08477,2023. 3 draw chinese characters in image synthesis models coher-
[2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel ently. arXivpreprintarXiv:2303.17870,2023. 2,3
Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, [18] AndresMarzalandEnriqueVidal. Computationofnormal-
ZionEnglish,VikramVoleti,AdamLetts,etal.Stablevideo ized edit distance and applications. IEEE transactions on
diffusion: Scaling latent video diffusion models to large pattern analysis and machine intelligence, 15(9):926–932,
datasets. arXivpreprintarXiv:2311.15127,2023. 2,6 1993. 5
[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, [19] ShengjuQian,HuiwenChang,YuanzhenLi,ZizhaoZhang,
Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh- JiayaJia, andHanZhang. Strait: Non-autoregressivegen-
man, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya eration with stratified image transformer. arXiv preprint
Ramesh. Video generation models as world simulators. arXiv:2303.00750,2023. 3
2024. 3 [20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[4] Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
ChanghuaMeng,HuijiaZhu,WeiqiangWang,etal.Diffute: AmandaAskell,PamelaMishkin,JackClark,etal.Learningtransferable visual models from natural language supervi- [33] ZhouxiaWang,ZiyangYuan,XintaoWang,TianshuiChen,
sion.InInternationalconferenceonmachinelearning,pages Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A
8748–8763.PMLR,2021. 2 unifiedandflexiblemotion controller forvideogeneration.
[21] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee, arXivpreprintarXiv:2312.03641,2023. 3
SharanNarang, MichaelMatena, Yanqi Zhou, WeiLi, and [34] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,
PeterJLiu. Exploringthelimitsoftransferlearningwitha Daxin Jiang, and Nan Duan. Nu¨wa: Visual synthesis pre-
unifiedtext-to-texttransformer.Journalofmachinelearning trainingforneuralvisualworldcreation. InEuropeancon-
research,21(140):1–67,2020. 2 ferenceoncomputervision,pages720–736.Springer,2022.
[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, 3
Patrick Esser, and Bjo¨rn Ommer. High-resolution image [35] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
synthesis with latent diffusion models. In Proceedings of Lei,YuchaoGu,YufeiShi,WynneHsu,YingShan,Xiaohu
theIEEE/CVFConferenceonComputerVisionandPattern Qie,andMikeZhengShou. Tune-a-video: One-shottuning
Recognition(CVPR),2022. 5 of image diffusion models for text-to-video generation. In
[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala ProceedingsoftheIEEE/CVFInternationalConferenceon
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, ComputerVision,pages7623–7633,2023. 3
RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans, [36] RuiqiWu,LiangyuChen,TongYang,ChunleGuo,Chongyi
etal.Photorealistictext-to-imagediffusionmodelswithdeep Li, and Xiangyu Zhang. Lamp: Learn a motion pat-
language understanding. Advances in neural information tern for few-shot-based video generation. arXiv preprint
processingsystems,35:36479–36494,2022. 3 arXiv:2310.10769,2023. 3
[24] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn, [37] Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, HaisongDing,HanHu,andKaiChen.Glyphcontrol:Glyph
OranGafni,DeviParikh,SonalGupta,andYanivTaigman. conditional control for visual text generation. Advances in
Make-a-video: Text-to-video generation without text-video NeuralInformationProcessingSystems,36,2024. 2,3,4,6,
data. 2022. 3 7
[25] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh [38] ShengmingYin,ChenfeiWu,JianLiang,JieShi,Houqiang
Tenenbaum,andFredoDurand. Lightfieldnetworks: Neu- Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained
ral scene representations with single-evaluation rendering. control in video generation by integrating text, image, and
Advances in Neural Information Processing Systems, 34: trajectory. arXivpreprintarXiv:2308.08089,2023. 3
19313–19325,2021. 5 [39] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,
[26] Morph studio, 2023. https://app.morphstudio. Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and
com/. 6,7 MikeZhengShou. Show-1: Marryingpixelandlatentdif-
[27] YuxiangTuo,WangmengXiang,Jun-YanHe,YifengGeng, fusion models for text-to-video generation. arXiv preprint
and Xuansong Xie. Anytext: Multilingual visual text gen- arXiv:2309.15818,2023. 3
erationandediting. arXivpreprintarXiv:2311.03054,2023. [40] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
2,3,4,5,6,7 conditional control to text-to-image diffusion models. In
ProceedingsoftheIEEE/CVFInternationalConferenceon
[28] AaronVanDenOord, OriolVinyals, etal. Neuraldiscrete
ComputerVision,pages3836–3847,2023. 4
representationlearning.Advancesinneuralinformationpro-
cessingsystems,30,2017. 3 [41] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,
Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and
[29] Fu-YunWang,ZhaoyangHuang,XiaoyuShi,WeikangBian,
Jingren Zhou. I2vgen-xl: High-quality image-to-video
Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm:
synthesis via cascaded diffusion models. arXiv preprint
Accelerating the animation of personalized diffusion mod-
arXiv:2311.04145,2023. 6,7,8
elsandadapterswithdecoupledconsistencylearning. arXiv
preprintarXiv:2402.00769,2024. 2,6,7,8 [42] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao
Zhang,JiaweiLiu,WeijiaWu,JussiKeppo,andMikeZheng
[30] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,
Shou. Motiondirector: Motion customization of text-to-
XiangWang,andShiweiZhang. Modelscopetext-to-video
video diffusion models. arXiv preprint arXiv:2310.08465,
technicalreport. arXivpreprintarXiv:2308.06571,2023. 3,
2023. 3
6
[43] ZangweiZheng,XiangyuPeng,andYangYou. Open-sora:
[31] XiangWang,HangjieYuan,ShiweiZhang,DayouChen,Ji-
Democratizingefficientvideoproductionforall,2024. 6,7
uniuWang,YingyaZhang,YujunShen,DeliZhao,andJin-
[44] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
grenZhou. Videocomposer: Compositionalvideosynthesis
withmotioncontrollability.AdvancesinNeuralInformation Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
ProcessingSystems,36,2024. 3 generation with latent diffusion models. arXiv preprint
arXiv:2211.11018,2022. 3
[32] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,
Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo
Yu, Peiqing Yang, et al. Lavie: High-quality video gener-
ationwithcascadedlatentdiffusionmodels. arXivpreprint
arXiv:2309.15103,2023. 3