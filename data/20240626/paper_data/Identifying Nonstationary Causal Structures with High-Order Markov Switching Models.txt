Identifying Nonstationary Causal Structures with
High-Order Markov Switching Models
CarlesBalsells-Rodas1 YixinWang2 PedroA.M.Mediano1,3 YingzhenLi1
1ImperialCollegeLondon
2UniversityofMichigan
3DivisionofPsychologyandLanguageSciences,UniversityCollegeLondon
Abstract
Intimeseries,theseapproachesfitaconstraineddynamic
modelwhichfavoursidentifiability[Petersetal.,2013].
Recent developments explore nonstationary time series
Causaldiscoveryintimeseriesisarapidlyevolv-
using time-dependent effects [Huang et al., 2015, 2019],
ingfieldwithawidevarietyofapplicationsinother
heterogeneous/history-dependentnoise[Huangetal.,2020,
areas such as climate science and neuroscience.
Gong et al., 2023], or leveraging contextual information
Traditionalapproachesassumeastationarycausal
across datasets [Günther et al., 2023]. Alternatively, non-
graph,whichcanbeadaptedtononstationarytime
stationaritycanbeaddressedbyassumingtime-dependent
series with time-dependent effects or heteroge-
causalstructures,whichisreferredtoascausalnonstation-
neousnoise.Inthisworkweaddressnonstation-
arity[Assaadetal.,2022].Regime-dependentcausaldis-
arityviaregime-dependentcausalstructures.We
covery[Saggioroetal.,2020]assumesdiscretelatentstates
firstestablishidentifiabilityforhigh-orderMarkov
(regimes)thatalterthetemporaldependencies,makingthe
SwitchingModels,whichprovidethefoundations
causal structure stationary within each regime. However, foridentifiableregime-dependentcausaldiscovery.
structureidentifiabilityinthisdomainremainsunexplored.
Ourempiricalstudiesdemonstratethescalability
ofourproposedapproachforhigh-orderregime- In this work, we establish identifiable regime-dependent
dependentstructureestimation,andweillustrate causaldiscoveryusinghigh-orderMarkovSwitchingMod-
itsapplicabilityonbrainactivitydata. els(MSMs;[Hamilton,1989]).TheyextendHiddenMarkov
Models(HMMs;[BaumandPetrie,1966])byincorporat-
ingautorregressivedependenciesontheobservationsgiven
1 INTRODUCTION discretelatentvariables.IdentifiabilityofstationaryHMMs
[Gassiat et al., 2016] is present in recent ICA literature
Identifyingcausalrelationshipsfromobservationaldatais [HälväandHyvarinen,2020,Hälväetal.,2021].However,
a challenging problem [Spirtes et al., 2000, Pearl, 2009], extensionstootherstructuredpriors,suchasMSMs,cannot
particularlyinthetimeseriesdomainwheretemporalde- includetransitionidentifiabilityduetotherequirementof
pendenciesmustbeconsidered[Assaadetal.,2022].Tradi- independentmixturecomponents[Allmanetal.,2009].
tionalapproachesassumetime-invariantstructures[Granger,
Recently, Balsells-Rodas et al. [2024] established identi-
1969,Petersetal.,2013],whereidentifiabilityextendsfrom
fiabilityforfirst-orderMSMs.Wegeneralisethistohigh-
non-temporalcausaldiscovery[Petersetal.,2017].Causal
orderdependenciesbyaddressingthechallengeofincreas-
discoveryencompassesadiversetaxonomy[Glymouretal.,
ing additional overlapping variables in the joint distribu-
2019].Constraint-basedmethodsuseconditionalindepen-
tion,whichrequiresstrengtheningtheassumptionsinthe
dencetesttorecovertheunderlyinggraphstructure,suchas
non-parametriccase.Wethenpresentparametrisationsvia
PC[Spirtesetal.,2000],FCI[Spirtes,2001],andextensions
conditionalGaussianswithanalyticmoments(Section3.1).
totimeseries[Runge,2018].Score-basedmethodsoptimise
This foundational result enables identifiability of regime-
scorefunctionsfromcausalgraphs[Chickering,2002];for
dependentcausalstructuresduetotheaccessibilityoftran-
timeseries,Dynotears[Pamfiletal.,2020]relaxesthecom-
sitionderivatives,whichwepresentinSection3.2.Finally,
binatorialsearchusingconstraint-basedoptimisationfrom
wedemonstratethescalabilityofouridentifiableMSMto
Zhengetal.[2018].Functionalcausalmodel-basedmethods
high-orderstructureestimation,andshowapplicationsto
representeffectsasfunctionsoftheircausesviastructural
neurosciencewithbrainactivitydata(Section5).
equationmodels[Shimizuetal.,2006,Petersetal.,2014].
AcceptedfortheCausalInferenceforTimeSeriesDataWorkshopatthe40thConferenceonUncertaintyinArtificialIntelligence (CI4TS2024).
4202
nuJ
52
]LM.tats[
1v89671.6042:viXra2 BACKGROUND approachallowstheincorporationofadditionalautoregres-
sivedependencieswithoutimposingfurtherrestrictionson
CausalDiscoveryinTimeSeries Causaldiscoveryas- theparametricfamily.Belowweprovideidentifiabilityanal-
sumesastructuralcausalmodel(SCM;[Pearl,2009])un- ysiswithfunctionsparameterisedbyneuralnetworks.Due
derlyingthedatagenerationprocess.Inthisworkwefocus to neural network symmetries, identifiability will instead
onstructuralequationmodels(SEMs;[Petersetal.,2017]), referonlytothefunctionalformoftheparameterisations.
whichgeneratedatagivenacausalgraphG∈G.NoteG
denotesacausalgraphspace.Considerasequenceofobser- 3.1 HIGH-ORDERMSMS
vationsx ∈Rd,witht∈{1,...,T},whereinthecontext
t
oftimeseries,thecorrespondingSEMattimet,withcausal Wedropthesubscriptθforsimplicity,andframetheMSM
graphG,isexpressedasfollows asamixtureofsequencesassuminglengthT <+∞.First,
wedefinethefamilyofinitialandtransitiondistributions
x(j) =f(j)({x(i) |x(i) ∈Pa(j)(τ), withautoregressiveorderM:
t t−τ t−τ
0≤τ ≤M},ε(j)), ε(j) ∼p , (1) (cid:110) (cid:111)
t t ε(j) ΠM := p (x )|a∈A , (3)
A a 1:M
for j ∈ {1,...,d}, where p denotes the distribution
ε(j)
(cid:110) (cid:111)
of the independent noise at each time t. We also denote PM := p (x |x ,...,x )|b∈B , (4)
Pa(j)(τ) as the parents of variable j with lag τ, which B b t t−1 t−M
are associated with the causal graph G. We distinguish where A and B are index sets satisfying mild measure
betweeninstantaneouseffects(τ = 0)andlaggedeffects theoretic conditions (see Appendix A). This formulation
(τ ≥1).Undernofurtherassumptions,thecausalgraphG connects with the notation presented in Eq. (2) as we
isunidentifiablefromdata.Toexemplify,Petersetal.[2013] have for each k ∈ {1,...,K }: p(x |s = k ) =
0 0 1:M M 0
proposesTiMINo,wherethecausalgraphisidentifiableif p (x ),forsomea ∈ A;andforeachk ∈ {1,...,K}:
a 1:M
theSEMbelongstoanidentifiablefunctionalmodelclass. p(x |x ,s = k) = p (x |x ) for some
t t−1:t−M t bt t t−1:t−M
b ∈BwithM <t≤T.Inotherwords,thefamiliesΠM
Markov SwitchingModels Again denote x t ∈ Rd ob- at nd PM contain infinitely many functions, and each thA e
B
servations of a sequence with t ∈ {1,...,T}. Markov distributionsintheMSMalignwithoneofsuchfunctions.
SwitchingModels(MSMs;[Hamilton,1989]),areaclass
Then, we can define a bijective path indexing function
of discrete latent variable models with latents s ∈
t φ:{1,...,C}→{1,...,K }×{1,...,K}T−M,witha
{1,...,K}.Theyconsiderautoregressiveconnectionsbe- 0
totalnumberofmixturepathsC = K ·K(T−M),where
tween the observed variables, conditioned on s . Given 0
t
c =p(s =φ(i))denotestheprobabilityofasequence
MSMsparametrisedbyθ,thelikelihoodp (x )canbe i M:T
θ 1:T
of states s = φ(i). Under the following notation we
computedexactlyasfollows: M:T
introduce the family of MSMs of order M, which is an
p (x )= (cid:88) p (s )p (x |s ) equivalentformulationofEq.(2)inthefinitemixturesense.
θ 1:T θ M:T θ 1:M M
sM:T
(cid:89)T
MT(ΠM,PM):=(cid:26) (cid:88)C
c p (x )
p (x |x ,...,x ,s ) (2) A B i ai 1:M
θ t t−1 t−M t
i=1
t=M+1 (cid:89)T (cid:12)
p (x |x ,...,x )(cid:12)p ∈ΠM,p ∈PM,
wherewedepictanMSMwithlagM.Forsimplicity,we bi t t t−1 t−M (cid:12) ai A bi t B
t=M+1
consider s ∈ {1,...,K }, and s ∈ {1,...,K} for
M 0 t t>M,ai ∈A,bi ∈B,(ai,bi )̸=(aj,bj ),
M < t ≤ T. Although the prior p θ(s M:T) in Eq (2) is t M+1:T M+1:T
notspecified,inourexperimentsweconsiderafirst-order C (cid:27)
(cid:88)
Markovchain.However,ourtheoreticalresultsallowany ∀i̸=j, c i =1,T >M . (5)
formforp (s )aslongasthestatesareindependentof i=1
θ M:T
theobservationsx 1:T. Given the requirement (ai,bi M+1:T) ̸= (aj,bj M+1:T)
for any i ̸= j, we have an injective mapping ϕ :
{1,...,C} → {1,...,K }×{1,...,K}T−M,suchthat
3 IDENTIFIABILITYANALYSIS 0
ϕ(i) = (ai,bi ,...,bi ). Therefore, we can combine
M+1 T
bothindexingmaps,ϕ◦φ−1tomapasetofstatess to
Inthissection,wefirstestablishtheoreticalresultsforhigh- M:T
someindicesa,b ,uniquely.
orderMarkovswitchingmodels.Then,wespecifytheidenti- M+1:T
ficationofthecorrespondingregime-dependentcausalstruc- TheabovenotationallowstheextensionofMSMtofinite
ture.Balsells-Rodasetal.[2024]alreadyprovidesresultsfor mixtures,wherewenowhaveamixtureofC trajectories
first-orderMSMsusingGaussiantransitionswherethemean indexedbysome(a,b ).Belowwedefinethenotion
M+1:T
andcovarianceareparameterisedbyanalyticfunctions.Our ofidentifiabilityofMT(ΠM,PM).
A B
2Definition 3.1. We say the family of Markov switching Theorem3.2. ConsidertheMarkovswitchingmodelfamily
modelswithorderM,whereM ∈Z+,isidentifiableupto with order M defined in Equation (5) under non-linear
permutations,ifforanyp,p˜∈MT(ΠM,PM), GaussianfamiliesM(IM,GM).Assume:
A B A B
p(x )= (i) UniqueindexingforthetransitionfamilyGM,definedby
1:T B
(cid:80)C i=1c ip ai(x 1:M)(cid:81)T t=M+1p
bi
t(x t|x t−1,...,x t−M), Eq.(7);andthefamilyI AM,definedbyEq.(9);
(ii) The transition distribution moments in GM, m(·,b) :
p˜(x 1:T)= RdM → Rd andΣ(·,b) : RdM → Rd×d,B areanalytic
(cid:80)C˜
c˜p (x
)(cid:81)T
p (x |x ,...,x ), functions,foranyb∈B;
i=1 i a˜i 1:M t=M+1 ˜bi t t−1 t−M
t
withp(x )=p˜(x ),∀x ∈RdT,wehaveK =K˜ , ThentheMarkovswitchingmodelfamilyisidentifiableas
1:T 1:T 1:T 0 0
K =K˜,andforeach1≤i≤C thereissome1≤j ≤C˜ definedin3.1.
suchthat:
Proofsketch:SeeAppendixBforthefullproof.Thestrategy
1. c =c˜ ; canbesummarisedinthefollowingsteps.
i j
2. ifbi =bi fort ,t >M,t ̸=t ,then˜bj =˜bj ;
t1 t2 1 2 1 2 t1 t2 1. Frommixturemodelsidentifiability[YakowitzandSpra-
3. p (x )=p (x ), ∀x ∈RdM; gins,1968],weshowlinearindependenceoverthejoint
ai 1:M a˜j 1:M 1:M
4. p bi(x t|x t−1,...,x t−M) = distributionfamilyP AM ,, BT := ΠM A ⊗(⊗T m=M+1P BM)is
p ˜bjt (x t|x t−1,...,x t−M),∀(x t,...,x t−M)∈Rd(M+1). sufficientforMSMidentifiability.
t 2. WeusesimilarinductiontechniquesfromBalsells-Rodas
etal.[2024]toshowlinearindependence.Wefirstfind
Giventhattheaboveframeworkisdesignedtoexploreiden-
conditionsforT =2M anduseinductiononT >2M
tifiability in deep generative models, we focus on estab-
(seefurtherdetailsbelow).Wethensolvetheremaining
lishing identifiability for parametric models, and refer to
cases(T ̸≡0 mod M).
AppendixBforthenon-parametricanalyses.Weformulate
our parametric model in terms of a non-linear Gaussian 3. We establish conditions for non-parametric PM
B
transitiongivenM previousobservations: and ΠM such that the product ΠM ⊗(⊗M PM) =
A A m=1 B
{p (x
)(cid:81)M
p (x |x ,...,x )}
(cid:110) ai 1:M m=1 bi M+m M+m m M−1+m
GM = p (x |x ,...,x )= containslinearindependentfunctions(caseT =2M).
B b t t−1 t−M
(cid:12) 4. WeshowthattheGaussianfamiliesI andG underas-
N(x |m(x ,...,x ,b),Σ(x ,...,x ,b)(cid:12) A B
t t−1 t−M t−1 t−M (cid:12) sumptions(i-ii)satisfythenon-parametriccasedefined
b∈B,(x ,...,x
)∈Rd(M+1)(cid:111)
, (6)
forΠM
A
andP BM respectively.
t t−M
Non-parametricidentifiability. Theaboveidentifiability
with m(·,b) : RdM → Rd and Σ(·,b) : RdM → Rd×d
result preserves the same model assumptions as the ones
beingnon-linearfunctions.SimilarlytoBalsells-Rodasetal.
presentedinBalsells-Rodasetal.[2024],whichestablish
[2024],wedefinethefollowinguniqueindexingassumption
MSM identifiability for M = 1. Key to their technique
fortheaboveGaussianfamily:
istheestablishmentoflinearindependencefromproducts
offunctionswithoneoverlappingvariable(LemmaB.1in
∀b̸=b′ ∈B, ∃(x ,...,x )∈RdM, s.t.
t−1 t−M Balsells-Rodasetal.[2024]).Toexemplify,thefollowing
m(x ,...,x ,b)̸=m(x ,...,x ,b′) first-orderjointdistributionfamily
t−1 t−M t−1 t−M
orΣ(x t−1,...,x t−M,b)̸=Σ(x t−1,...,x t−M,b′), (7) (cid:26) (cid:27)
Π1 ⊗P1⊗P1 = p (x )p (x |x )p (x |x ) , (10)
A B B a 1 b2 2 1 b3 3 2
whichassumestwoGaussiandistributionsonx fordiffer-
t
entb,b′ ∈B,given(x ,...,x ).Wealsointroduce
t−1 t−M foreverya∈Aandb ,b ∈B,containsexactlyoneover-
2 3
afamilyofinitialGaussiandistribution:
lappingvariableforproductsofconsecutivedistributions
(cid:110) (cid:111) (indicatedindifferentcolours).
IM := p (x )=N(x |µ(a),Σ (a))|a∈A ,
A a 1:M 1:M 1
Our strategy to generalise to any lag M < T focuses on
(8)
strengtheningtheassumptionsinthenon-parametriccase,
withequivalentuniqueindexingassumptions,
whicharestillsatisfiedwhenassuminganalyticGaussian
a̸=a′ ∈A⇔µ(a)̸=µ(a′)orΣ (a)̸=Σ (a′). (9) transition models. The challenge for identifiability when
1 1
increasingautoregressivedependenciesisduetotheincreas-
Wenowestablishthefollowingidentifiabilityresultforthe ing set of overlapping variables in the joint distribution
aboveparametricMSMoforderM. family. This complicates proving its linear independence
3(Proofsketch:step1),aswenolongerhavethesameover- variabless ∈{1,...,K}ateachtimestept.Therefore,the
t
lappingstructuresasinEq.(10).Therefore,theinduction correspondingcausalgraphbecomesaregime-dependent
techniqueusedinBalsells-Rodasetal.[2024]doesnothold. causal graph which we denote as G := {G }K ∈
1:K k k=1
Toillustrate,forM = 3,thejointdistributionfamilyhas (×KG).Itcanbedefinedasacollectionofcausalgraphs
thefollowingstructure: suchthatforeachk ∈ {1,...,K},G ∈ Gencodesthe
k
(cid:110) SEM at time t if s t = k. To establish connections with
Π3 A⊗(⊗3P B3)= p a(x 1,x 2,x 3)p b4(x 4|x 1,x 2,x 3) thepreviousparametrichigh-orderMSMs,weassumean
(cid:111) additivenoisemodel(ANM;[Hoyeretal.,2008])withab-
p (x |x ,x ,x )p (x |x ,x ,x ) , (11)
b5 5 4 3 2 b6 6 5 4 3 senceofinstantaneouseffects.Therefore,thecorresponding
regime-dependentSEMisexpressedasfollows
for every a ∈ A and b ,b ,b ∈ B, where we indicate
4 5 6
overlaping variables with different colours. Compared to x(j) =f(j)({x(i) |x(i) ∈Pa(j)(τ),
M = 1inEq.(10),whenmultiplyingtheinitialandtran-
t st t−τ t−τ st
sitiondistribution,p a(x 1:3)p b4(x 4|x 3,x 2,x 1),theoverlap 1≤τ ≤M})+ε( tj), ε( tj) ∼p ε(j), (13)
x shares some similarities. In fact, one can utilise the
1:3 wherePa(j)(τ)denotestheparentsofvariablej withlag
strategyinM =1toshowlinearindependenceforT =4 st
τ,associatedtoG fors =k.
intheabovefamily(seeLemmaB.3).ThenforT =5,the k t
conditioned variables in p (x |x ,x ,x ) will overlap Definition 3.3. We say that the regime-dependent graph
b5 5 4 3 2
withbothp (x )p (x |x ,x ,x ),andthustheprevi- ofaMSMwithorderM isidentifiableuptopermutations
a 1:3 b4 4 3 2 1
ousstrategycannotbeusedtoprovelinearindependence if for any p,p˜∈ MT(ΠM,PM), with respective regime-
A B
astheproductdoesnotdirectlyreducetoEq.(10).Similar dependent graphs G ∈ (×KG) and G˜ ∈ (×K˜G),
1:K 1:K˜
argumentsapplyforT =6,wherex 3overlapswithallthe such that p(x 1:T) = p˜(x 1:T); K = K˜ and there exists a
functionsinEq.(11). permutationσsuchthatG(k)=G˜(σ(k))fork ∈K.
Toensurelinearindependenceofthejointdistributionfam-
Weestablishidentifiabilityforregime-dependenttimeseries
ilyforanylagM,wetakeanaugmentedvariableapproach,
asfollows.
wherewegroupvariablestoforceoverlapsonlybetween
consecutive distributions (similar to Eq. (10)). Therefore, Corollary3.4. ConsidertheMarkovswitchingmodelfamily
thepreviousexamplewithM =3resultsasfollows with order M defined in Equation (5) under non-linear
Π3 A⊗(⊗3P
B3)=(cid:110)
p a(x 1:3)
G saa tiu ss fis ei da ;n families M(I AM,G BM). Assume (i) from 3.2 is
(cid:111)
p (x |x )| a∈A,b ∈(×3B) . (12) (ii) Independentnoise:thetransitiondistributionmeanin
b4:6 4:6 1:3 4:6
m(·,b) : RdM → Rd isanalytic,andΣ(·,b) := Σ(b)
Now, similar techniques used in the case M = 1 isdiagonalandconstantw.r.t(x t−1,...,x t−M);
should also apply, provided that the product distribution (iii) Minimality:ifx(i) ∈/ Pa(j)(τ),1≤τ ≤M,thenthei-
{p (x |x )}satisfiestheconditions(a4-a6)inLemma k
b4:6 4:6 1:3 thdimensionofm(x t−1,...,x t−M,k)isconstantw.r.t.
B.3.InAppendixB.2.2,weshowthatextendingassump-
x(j) .
tionstoproductdistributions(LemmasB.6andB.7)asthe t−τ
above requires to strengthen the assumptions on the non- Then,theregime-dependentgraphisidentifiableuptoper-
parametricfunctionfamilies.Morespecifically,thestronger mutations(Definition3.3).
assumption(b4)requireslinearindependenceofthefamily
SeeAppendixCfortheproof.Itsufficestoshowthatthe
PM toholdoneverysubsetofafullmeasuresetofRdM;
B regimedependentstructureisrecovereduptopermutations,
whereasforM =1,onlyanon-zeromeasuresetY ⊂Rd
giventheidentifiabletransitionderivativesfromTheorem
suffices.Intheexperiments(Sec.5.1,Figure1c),weillus-
3.2.Underassumptions(i-iii),wenotethatotherstructures
tratethisrequirementbycomparingestimationfornon-zero
intraditionalcausaltimeseriesanalysissuchasthefulltime
and full measure sets. The above strategy ensures linear
graph[Petersetal.,2017]arenotidentifiableinthecontext
independenceofproductfamilieswhenT isamultipleof
ofhigh-orderMSMs;aswehavenoaccesstothediscrete
thelagM.InTheoremB.9weusetheinductiontechnique
latentss ,butrathertheirdistributionuptopermutations.
forT =a·M,witha∈Z+toprovelinearindependence M:T
foranyT >M.
4 ESTIMATION
3.2 REGIME-DEPENDENTCAUSALDISCOVERY
WeuseExpectationMaximisation(EM)forefficientmix-
Inregime-dependenttimeseries[Saggioroetal.,2020],the turemodelestimation[Bishop,2006],assumingwehavea
SEMistime-dependent.However,itiscausallystationary datasetofN sequencesoflengthT.Belowprovideupdate
[Assaadetal.,2022]whenconditionedondiscretelatent rules for a sample x , but note that we use mini-batch
1:T
410 1.00 0.97 0.98 0.88 10 0.97 0.98 0.98 0.91 100
10 1 ReLU
5 1.00 1.00 1.00 1.00 5 0.99 1.00 0.99 1.00 Non-zero
Zero
10 2
3 1.00 1.00 1.00 0.99 3 1.00 1.00 1.00 0.98
10 3
2 5 10 20 2 5 10 20 1 3 5
Num. Lags. Num. Lags. Num. lags
(a)LowSparsity. (b)HighSparsity. (c)Modelassumptions.
Figure 1: Synthetic experiments on high-order MSMs: averaged F score on (a) low and (b) high sparsity settings; (c)
1
averagedL distanceusingdifferentmodelassumptions.
2
stochasticgradientascentwhenthenumberofsequences 5.1 SYNTHETICDATA
in the dataset is large. As mentioned, we assume a first-
orderMarkovchainforthestructureofthediscretestates, We generate N = 10000 sequences of length T = 200
wheretheirposteriordistributionattimet,{γ (x ) = andd = 5dimensionsusingMSMsdescribedinSection
t,k 1:T
p (s = k|x ) }, can be computed using forward and 2.Theestimatedfunctionsthatparameterisethetransitions
θ t 1:T
backwardmessages:{α (x )=p (x ,s =k)},and areevaluatedusingthepermutationwiththelowesterror.
t,k 1:t θ 1:t t
{β (x )=p (x |s =k)}respectively: We use diagonal, fixed covariance matrices and generate
t,k t+1:T θ t+1:T t
analytic transition means using random neural networks
α (x )β (x )
γ (x )= t,k 1:t t,k t+1:T , (14) withcosineactivations.Locallyconnectednetworks[Zheng
t,k 1:T (cid:80)K
α (x )β (x )
k=1 t,k 1:t t,k t+1:T etal.,2018]areused,withdatadependenciesfollowinga
whereα (x )andβ (x )arecomputedasfollows: sampledregime-dependentgraphwithapredefinedsparsity
t,k 1:t t,k 1:t
ratio.SeeAppendixDfordetailsondatageneration,model
α t,k(x 1:t)=p θ(x t|x t−1:t−M,s t =k) evaluationandtraining.
K
(cid:88)
× p θ(s t =k|s t−1 =k′)α t−1,k′(x 1:t−1), (15) Regime-dependentcausalstructureestimation Toeval-
k′=1 uate our approach, we generate data with different lags
M ∈ {2,5,10,20}, states K ∈ {3,5,10}, and sparsity
K
(cid:88)
β (x )= p (x |x ,s =k′) settings. The estimated causal structure is computed via
t,k t+1:T θ t+1 t:t+1−M t+1
thresholdingtheJacobianoftheestimatedtransitionfunc-
k′=1
×p (s =k′|s =k)β (x ). (16) tions.Weconsidermoderatesparsity(upto20parentsper
θ t+1 t t+1,k′ t+1:T
variable)andhighsparsity(5parentspervariable).Tomain-
Giventhatweparametrisethetransitiondistributionsusing tainsimilarsparsitylevelsacrossdifferentlags,thesparsity
analyticneuralnetworks,weadoptGeneralisedEM(GEM) ratio increases with M. Results are reported respectively
[Dempsteretal.,1977],wherethefollowinggradientascent inFigures1aand1brespectively,wherewecomputethe
stepisperformed: averaged F score across states after accounting for the
1
permutation.HighF scoresareconsistentlyobservedfor
T K 1
θnew ←θold+η (cid:88) (cid:88) γ (x ) increasingstatesandlags,demonstratingtheeffectiveness
t,k 1:T
ofouridentifiableMSMforregime-dependentcausaldis-
t=M+1k=1
coverywithhigh-ordertemporaldependencies.
×∇ logp (x |x ,s =k). (17)
θ θ t t−1:t−M t
Thegradienttermcanbecomputedusingback-propagation. Assumption violations We have only shown sufficient
This approach is well-established in the literature [Hälvä conditionsforidentifiabilityunderanalytictransitionfunc-
andHyvarinen,2020],andconvergencetoalocalmaximum tions(assumption(ii)inTheorem3.2).Toempiricallyverify
ofthelikelihoodisguaranteedunderthelargedatalimit. identifiability under weaker assumptions, we experiment
with:ReLUnetworks(fig.1c,ReLU);piece-wiseanalytic
5 EXPERIMENTS functionswithanon-zeromeasureintersection(fig.1c,Non-
zero); and analytic functions assumed in (ii), which have
Wefirstconductexperimentsonsyntheticdatato(i)explore zero-measureintersection(fig.1c,Zero).AlthoughReLU
thescalabilityofourapproachinlearningregime-dependent networksviolate(ii),identifiabilityshouldholdifassump-
graphsand(ii)empiricallyverifywhethertheassumptions tion(d3)inTheoremB.14holds(zero-measureintersection
inTheorem3.2arenecessaryforidentifiability.Furthermore, ofGaussianmoments).Therefore,wesamplerandomReLU
wemotivateourapproachusingrealbrainactivitydata. networkswiththesamecausalstructureacrossregimes.
5
setatS
.muN
setatS
.muN
)0m,m(tsid1e3 1e3 1 1.0 0.4
2
0.6 3 0.8
6 4
0.8 5 0.6
1 0.4
1.0 4 2
Awake 3 0.2
1.2 4
Anaesthetised
2 5 0.0
1 2 5 10 20 0.25 0.5 0.75 1.0 1.25 1.5
Num. lags Time (s)
(a) (b)
Figure2:(a)Testlog-likelihoodofECoGdatausingdifferentlags.(b)PosteriordistributionofanECoGepochonAwake
andAnaesthetisedconditionsusingK =5statesandM =2lags.
Figure 1c shows the results with averaged L distances Table1:Statetransitionfrequency(inHz)onvisualcortex
2
acrossregimesafteraccountingforpermutation.ReLUnet- ECoGdatawithM =2usingdifferentstates.
worksareestimatedwithhighererrorastheydirectlyvio-
lateassumption(ii)andtheequalcausalstructuresacross K
Condition
regimes reduce the chances of meeting (d3) in Theorem 3 5 10
B.14.Regardingpiece-wiseanalyticfunctions(Non-zero),
Awake 17.60 19.62 23.74
we observe an abrupt increase in L distance for higher
2 Anaesthetised 5.75 6.51 14.64
lags(M > 1)whencomparedtotheresultswithanalytic
functions.Thisconfirmsourtheoreticalfindingsdiscussed
inSection3.1,whereforM >1theassumptionsneedtobe entlags.The“awake”conditionlog-likelihoodsarelower,
strengthenedtoensurezeromeasureintersection,whereas whichillustratetheincreasedcomplexityindynamics.Our
forM =1anon-zeromeasureintersectionisallowed. identifiableMSMwithM =1performsbestonthe“anaes-
thetised”condition,whileM =2isbetteronthe“awake”
5.2 BRAINACTIVITYDATA condition.ThissuggestsM =1isbetterforsimplerdynam-
ics,whereashigher-ordersarebetterformorecomplexones.
Todemonstratethescalabilityofourmethod,weapplyit ForM > 1,thelog-likelihooddecreaseswithincreasing
tohigh-densityelectrocorticography(ECoG)brainactivity lags,possiblyduetotheincreasedtrainingcomplexity.Table
from the NeuroTycho database1, originally presented by 1presentsstatetransitionfrequencies(inHz),withconsis-
Yanagawa et al. [2013]. ECoG data consisting of signals tentlyhighervaluesinthe“awake”condition.Tosuppress
from128electrodesacrossthebrainwasrecordedfroma transitionartifacts,weconvolvetheposteriorsignalusing
macaque monkey under two conditions: normal wakeful- auniformkerneloflength3.Furthermore,theexamplese-
ness (Awake) and loss of consciousness induced through quenceinFigure2bshowsprolongedstatepreservationin
propofolanaesthesia(Anaesthetised).Ourfocusistoassess the “anaesthetised” condition. This result motivates iden-
whetherouridentifiablehigh-orderMSMcapturesdifferent tifiable MSMs for neuroscience, complementing existing
dynamicsacrossconditions,enablinghypothesistestingin methodologies[Medianoetal.,2023].
neuroscience.Basedonpriorwork[Medianoetal.,2023],
weexpecttoobservemorerapidlychangingdynamicsin
theawakecondition,capturedbymorefrequenttransitions 6 CONCLUSIONS
betweenthestatesinourMSM.
Inthiswork,weproveidentifiabilityofregime-dependent
The ECoG recording is sampled at 1kHz and comprises
causal discovery using identifiable high-order Markov
“awake”and“anaesthetised”segments,eachlasting929.8
SwitchingModels(MSMs).Ourkeycontributionisthegen-
and650.65secondsrespectively.Weselect21electrodes
eralisationofidentifiablefirst-orderMSMstohigher-order
approximately corresponding to the visual cortex of the
temporaldependenciesviastrengtheningtheassumptions
brain.Wefirstapplyasecond-orderButterworthnotchfilter
inthenon-parametriccase.Thisenablesmodelparametrisa-
at50Hztoeliminatelinenoise.Then,wedownsamplethe
tionsviaanalyticGaussiantransitions.Weverifyourtheo-
datato200Hz,standardiseeachchannelindependently,and
chunkeachsequenceintoepochsof2seconds(T =400). reticalfindingsempiricallythroughsyntheticexperiments
Thisyields464“awake”and325“anaesthetised”segments, anddemonstratetheapplicabilityofourapproachtorealistic
with50ofeachsavedfortesting. domains,suchashypothesistestinginneuroscience.Future
studies could be focused on incorporating instantaneous
Figure2ashowstestlog-likelihoodsforMSMswithdiffer- effects,includingfulltimegraphidentifiabilityviaconsis-
tentstateidentification,orleveragingidentifiablehigh-order
1http://neurotycho.org/ MSMsfornonstationarylatentcausalmodels.
6
ekawA
-
)T:1x(gol
desitehtseanA
- )T:1x(gol
tnenopmoC
tnenopmoC
desitehtseanA
ekawAReferences Hermanni Hälvä and Aapo Hyvarinen. Hidden markov
nonlinearica:Unsupervisedlearningfromnonstationary
ElizabethSAllman,CatherineMatias,andJohnARhodes. timeseries. InConferenceonUncertaintyinArtificial
Identifiability of parameters in latent structure models Intelligence,pages939–948.PMLR,2020.
withmanyobservedvariables. TheAnnalsofStatistics,
HermanniHälvä,SylvainLeCorff,LucLehéricy,Jonathan
37(6A):3099–3132,2009.
So,YongjieZhu,ElisabethGassiat,andAapoHyvarinen.
CharlesKAssaad,EmilieDevijver,andEricGaussier. Sur- Disentanglingidentifiablefeaturesfromnoisydatawith
veyandevaluationofcausaldiscoverymethodsfortime structurednonlinearica. AdvancesinNeuralInformation
series. Journal of Artificial Intelligence Research, 73: ProcessingSystems,34:1624–1633,2021.
767–819,2022.
JamesDHamilton. Anewapproachtotheeconomicanal-
Carles Balsells-Rodas, Yixin Wang, and Yingzhen Li. ysisofnonstationarytimeseriesandthebusinesscycle.
On the identifiability of switching dynamical systems. Econometrica:Journaloftheeconometricsociety,pages
In Forty-first International Conference on Machine 357–384,1989.
Learning,2024. URLhttps://openreview.net/
forum?id=Eew3yUQQtE. PatrikHoyer,DominikJanzing,JorisMMooij,JonasPeters,
andBernhardSchölkopf.Nonlinearcausaldiscoverywith
Leonard E Baum and Ted Petrie. Statistical inference additivenoisemodels. Advancesinneuralinformation
forprobabilisticfunctionsoffinitestatemarkovchains. processingsystems,21,2008.
Theannalsofmathematicalstatistics,37(6):1554–1563,
1966. BiweiHuang,KunZhang,andBernhardSchölkopf. Iden-
tification of time-dependent causal model: A gaussian
ChristopherM.Bishop. PatternRecognitionandMachine processtreatment. InTwenty-Fourthinternationaljoint
Learning(InformationScienceandStatistics). Springer- conferenceonartificialintelligence,2015.
Verlag,Berlin,Heidelberg,2006. ISBN0387310738.
BiweiHuang,KunZhang,MingmingGong,andClarkGly-
DavidMaxwellChickering. Optimalstructureidentification
mour. Causaldiscoveryandforecastinginnonstationary
withgreedysearch.Journalofmachinelearningresearch,
environmentswithstate-spacemodels. InInternational
3(Nov):507–554,2002.
conferenceonmachinelearning,pages2901–2910.Pmlr,
2019.
Arthur P Dempster, Nan M Laird, and Donald B Rubin.
Maximum likelihood from incomplete data via the em
Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey,
algorithm. Journaloftheroyalstatisticalsociety:series
Ruben Sanchez-Romero, Clark Glymour, and Bern-
B(methodological),39(1):1–22,1977.
hard Schölkopf. Causal discovery from heteroge-
ÉlisabethGassiat,AliceCleynen,andStephaneRobin. In- neous/nonstationarydata. JournalofMachineLearning
ferenceinfinitestatespacenonparametrichiddenmarkov
Research,21(89):1–53,2020.
modelsandapplications. StatisticsandComputing,26
DiederikP.KingmaandJimmyBa. Adam:Amethodfor
(1):61–71,2016.
stochasticoptimization. InYoshuaBengioandYannLe-
Clark Glymour, Kun Zhang, and Peter Spirtes. Review Cun,editors,3rdInternationalConferenceonLearning
ofcausaldiscoverymethodsbasedongraphicalmodels. Representations,ICLR2015,SanDiego,CA,USA,May
Frontiersingenetics,10:524,2019. 7-9,2015,ConferenceTrackProceedings,2015.
Wenbo Gong, Joel Jennings, Cheng Zhang, and Nick PedroA.M.Mediano,FernandoE.Rosas,AndreaI.Luppi,
Pawlowski. Rhino: Deep causal temporal relationship ValdasNoreika,AnilK.Seth,RobinL.Carhart-Harris,
learningwithhistory-dependentnoise. InTheEleventh Lionel Barnett, and Daniel Bor. Spectrally and tem-
InternationalConferenceonLearningRepresentations, porally resolved estimation of neural signal diversity.
2023. URLhttps://openreview.net/forum? eLife, March 2023. doi: doi.org/10.7554/eLife.88683.
id=i_1rbq8yFWC. 1. URL http://dx.doi.org/10.7554/eLife.
88683.1.
CliveWJGranger. Investigatingcausalrelationsbyecono-
metricmodelsandcross-spectralmethods.Econometrica: Boris Mityagin. The zero set of a real analytic function.
journaloftheEconometricSociety,pages424–438,1969. arXivpreprintarXiv:1512.07276,2015.
Wiebke Günther, Urmi Ninad, and Jakob Runge. Causal Roxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai,
discovery for time series from multiple datasets with PhilipPilgerstorfer,KonstantinosGeorgatzis,PaulBeau-
latentcontexts. InUncertaintyinArtificialIntelligence, mont,andBryonAragam. Dynotears:Structurelearning
pages766–776.PMLR,2023. from time-series data. In International Conference on
7Artificial Intelligence and Statistics, pages 1595–1605. Toru Yanagawa, Zenas C. Chao, Naomi Hasegawa, and
PMLR,2020. Naotaka Fujii. Large-scale information flow in con-
scious and unconscious states: an ecog study in mon-
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,
keys. PLOS ONE, 8(11):null, 11 2013. doi: 10.1371/
JamesBradbury,GregoryChanan,TrevorKilleen,Zem- journal.pone.0080845. URLhttps://doi.org/10.
ing Lin, Natalia Gimelshein, Luca Antiga, Alban Des- 1371/journal.pone.0080845.
maison,AndreasKopf,EdwardYang,ZacharyDeVito,
Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and
BenoitSteiner,LuFang,JunjieBai,andSoumithChin- EricPXing.Dagswithnotears:Continuousoptimization
tala. Pytorch: An imperative style, high-performance forstructurelearning. Advancesinneuralinformation
deep learning library. In Advances in Neural Informa- processingsystems,31,2018.
tion Processing Systems 32, pages 8024–8035. Curran
Associates,Inc.,2019.
Judea Pearl. Causal inference in statistics: An overview.
StatisticsSurveys,3(none):96–146,2009. doi:10.1214/
09-SS057. URL https://doi.org/10.1214/
09-SS057.
Jonas Peters, Dominik Janzing, and Bernhard Schölkopf.
Causal inference on time series using restricted struc-
turalequationmodels. Advancesinneuralinformation
processingsystems,26,2013.
JonasPeters,JorisM.Mooij,DominikJanzing,andBern-
hardSchölkopf. Causaldiscoverywithcontinuousaddi-
tivenoisemodels.JournalofMachineLearningResearch,
15(58):2009–2053,2014.
Jonas Peters, Dominik Janzing, and Bernhard Schölkopf.
Elementsofcausalinference:foundationsandlearning
algorithms. TheMITPress,2017.
Jakob Runge. Causal network reconstruction from time
series:Fromtheoreticalassumptionstopracticalestima-
tion. Chaos:AnInterdisciplinaryJournalofNonlinear
Science,28(7):075310,2018.
ElenaSaggioro,JanadeWiljes,MarleneKretschmer,and
JakobRunge. Reconstructingregime-dependentcausal
relationshipsfromobservationaltimeseries. Chaos:An
InterdisciplinaryJournalofNonlinearScience,30(11):
113115,2020.
Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Antti
Kerminen,andMichaelJordan. Alinearnon-gaussian
acyclicmodelforcausaldiscovery. JournalofMachine
LearningResearch,7(10),2006.
PeterSpirtes. Ananytimealgorithmforcausalinference. In
AISTATS,2001.
Peter Spirtes, Clark N Glymour, Richard Scheines, and
David Heckerman. Causation, prediction, and search.
MITpress,2000.
SidneyJYakowitzandJohnDSpragins. Ontheidentifi-
abilityoffinitemixtures. TheAnnalsofMathematical
Statistics,39(1):209–214,1968.
8Identifying Nonstationary Causal Structures with
High-Order Markov Switching Models (Supplementary Material)
CarlesBalsells-Rodas1 YixinWang2 PedroA.M.Mediano1,3 YingzhenLi1
1ImperialCollegeLondon
2UniversityofMichigan
3DivisionofPsychologyandLanguageSciences,UniversityCollegeLondon
A IDENTIFIABILITYINFINITEMIXTUREMODELS
OurtheoreticalframeworkusesfinitemixturemodelresultsfromYakowitzandSpragins[1968],whichshowidentifiability
offinitemixturesvialinearindependenceofthefamilyofmixingcomponents.Consideradistributionfamilywithfunctions
definedonx∈Rd,
F :={F (x)|a∈A}, (18)
A a
whereF (x)isad-dimensionalCDF.TheindexsetAisassumedtosatisfythatF (x),asafunctionof(x,a),ismeasurable
a a
onRd×A.WeintroducethenotionoflinearindependenceunderfinitemixturesofafamilyF .
A
DefinitionA.1. AfamilyoffunctionsF (Eq.(18))issaidtocontainlinearlyindependentfunctionsunderfinitemixtures,
A
ifforanyA ⊂Asuchthat|A |<+∞,thefunctionsin{f (x)|a∈A }arelinearlyindependent.
0 0 a 0
Theabovedefinitionisaweakerrequirementoflinearindependenceonfunctionfamiliesasitallowslineardependence
fromthelinearcombinationofinfinitelymanyotherfunctions.Considerthefollowingfinitemixturedistributionfamily:
L L
(cid:88) (cid:88)
H :={H(x)= c F (x)|L<+∞,a ∈A,a ̸=a ,∀i̸=j, c =1}, (19)
A i ai i i j i
i=1 i=1
whichisdefinedfromalinearcombinationofCFDsinF .Nowwespecifythedefinitionofidentifiablefinitemixture
A
familyfollowingYakowitzandSpragins[1968].
DefinitionA.2. ThefinitemixturefamilyHissaidtobeidentifiableuptopermutations,whenforanytwofinitemixtures
H(x) = (cid:80)L c F (x)andH˜(x) = (cid:80)L˜ c˜F (x),H(x) = H˜(x)forallx ∈ Rd,ifandonlyifL = L˜ andforeach
i=1 i ai i=1 i a˜i
1≤i≤Lthereissome1≤j ≤L˜ suchthatc =c˜ andF (x)=F (x)forallx∈Rd.
i j ai a˜j
Then,identifiabilityoffinitemixturemodelsisstatedasfollows.
PropositionA.3. [YakowitzandSpragins,1968] ThefinitemixturedistributionfamilyHisidentifiableuptopermutations,
ifandonlyiffunctionsinF arelinearlyindependentunderthefinitemixtures.
B PROOFOFTHEOREM3.2
Sketchoftheproof: Weorganisetheproofstrategyinto4steps.
1. Weshowtherequirementforidentifiabilityislinearindependenceofthejointdistributionfamily.
2. Weprovidelinearindependenceresultsforproductsofnon-parametricfunctions.First,westatethelineardependence
resultfromBalsells-Rodasetal.[2024],andwethenstrengthentheassumptionstoallowproductsofM functions.
AcceptedfortheCausalInferenceforTimeSeriesDataWorkshopatthe40thConferenceonUncertaintyinArtificialIntelligence (CI4TS2024).3. Weprovelinearindependenceofthejointdistributionfamilyfornon-parametrictransitionsoforderM.
4. Weshowtheparametricassumptions(i),(ii)satisfylinearindependenceofthejointdistributionfamily.
We note the strategy follows closely from Balsells-Rodas et al. [2024], where identifiability is shown for first-order
autoregressivedependencies.
B.1 LINEARINDEPENDECERENDERSIDENTIFIABILITY
Balsells-Rodasetal.[2024]showsthatPropositionA.3canbegeneralisedtoCFDsdefinedonx ∈RTd.Wenotethe
1:T
jointdistributionfamilyoftheMSMwithorderM PM,T :=ΠM ⊗(⊗T PM)haslinearindependentcomponentsif
A,B A t=M+1 B
andonlyifitsCDFalsocontainslinearindependentcomponents.WeincludethefollowingextensiontoPropositionA.3
whichisadaptedfromBalsells-Rodasetal.[2024].
Proposition B.1. (Adapted from [Balsells-Rodas et al., 2024]) Consider the distribution family given by Eq. 5. Then
thejointdistributionin MT(ΠM,PM)isidentifiableuptopermutationsifandonlyiffunctionsinPM,T arelinearly
A B A,B
independentunderfinitemixtures.
Wealsoadaptthefollowingresultfrom[Balsells-Rodasetal.,2024]andprovidetheproofforcompleteness.
TheoremB.2. (Adaptedfrom[Balsells-Rodasetal.,2024])AssumethefunctionsinPT,M arelinearlyindependentunder
A,B
finitemixtures,thenthedistributionfamilyMT(ΠM,PM)isidentifiableasdefinedinDef3.1.
A B
Proof. AssuminglinearindependenceunderfinitemixturesofPM,T,impliesidentifiabilityuptopermutationasdefinedin
A,B
DefinitionA.2(finitemixturemodelcase).Then,forp (x )andp (x )fromDefinition3.1,wehaveC =C˜,andfor
1 1:T 1 1:T
every1≤i≤C,thereexists1≤j ≤C˜ suchthatc =c˜ and:
i j
T T
(cid:89) (cid:89)
p (x ) p (x |x ,...,x )=p (x ) p (x |x ,...,x ), ∀x ∈RTd. (20)
ai 1:M bi
t
t t−1 t−M a˜j 1:M ˜bj
t
t t−1 t−M 1:T
t=M+1 t=M
GiventhatwehaveconditionalPDFs,ifthejointdistributionsareequalonx ,thenthedistributionsonx arealso
1:T 1:T−1
equal:
T−1 T−1
(cid:89) (cid:89)
p (x ) p (x |x ,...,x )=p (x ) p (x |x ,...,x ), ∀x ∈R(T−1)d. (21)
ai 1:M bi
t
t t−1 t−M a˜j 1:M ˜bj
t
t t−1 t−M 1:T−1
t=M+1 t=M
Therefore, we have p (x |x ,...,x ) = p (x |x ,...,x ) for all x ,...,x ∈ Rd. We can
bi
T
T T−1 T−M ˜bj
T
T T−1 T−M t t−M
follow the same reasoning for other time indices to have p (x |x ,...,x ) = p (x |x ,...,x ) for all
bi
t
t t−1 t−M ˜bj
t
t t−1 t−M
t>M,x ,...,x ∈Rd.Similarlogicappliestotheinitialdistribution,wherewehavep (x )=p (x )forall
t t−M ai 1:M a˜j 1:M
x ,...,x ∈Rd;andfromPropositionB.1,wehaveK =K˜ .GivenC =C˜ wealsohaveK =K˜.
1 M 0 0
Finally,ifthereexistst ̸=t suchthatbi =bi but˜bj ̸=˜bj ,wehaveforanyα∈RMd,β ∈Rd:
1 2 t1 t2 t1 t2
p (x =β|x =α)=p (x =β|x =α)
˜bj
t1
t1 t1−1:t1−M bi
t1
t1 t1−1:t1−M
=p (x =β|x =α)
bi
t2
t2 t2−1:t2−M
=p (x =β|x =α),
˜bj
t2
t2 t2−1:t2−M
which implies linear dependence of PM. We note this contradicts the assumption of linear independence of the joint
B
distributionPM,T,asweshouldhave
A,B
(cid:88)(cid:88)
γ p (x )p (x |x )=0, ∀x ∈R(T−1)d×Rd, (22)
ij bi
1:T−1
1:T−1 bj
T
T T−1:T−M 1:T
i j
withγ =0,1≤i≤C/K,1≤j ≤K.However,givenlineardependenceonPM,wecantherearangethetermsandset
ij B
γ =γ ̸=0,1≤j ≤K suchthattheaboveequationissatisfied.
i,j j
10ThenextstepistoshowconditionsunderwhichthejointdistributionfamilyPM,T islinearlyindependentunderfinite
A,B
mixtures.
B.2 LINEARINDEPENDENCEOFNON-PARAMETRICPRODUCTFAMILIES
Thestrategytoprovelinearindependenceonthenon-parametricjointdistributionfamilyistoshowlinearindependence
forconsecutiveproductsofdistributions.Asanexample,forM =1,wehave{p(x |x ,s )p(x |x ,s )}withone
t t−1 t t+1 t t+1
overlappingvariable.IncreasingM
increasesthenumberofoverlaps:{(cid:81)M
p(x |x ,s )}.Notably
m=1 t+m t−M+m:t−1+m t+m
forM >1,weobservetwotypesofoverlappingvariableswhencomputingtheabovejointprobabilityofM consecutive
observations:
1. Observed-conditioned overlaps: e.g. x ,...,x in the example, or the observed variables in the above joint
t+M t+1
probabilityproduct.
2. Conditioned-conditionedoverlaps:e.g.x ,...,x intheexample,ortheconditionedvariablesintheabovejoint
t−1 t−M
probabilityproduct.
Therefore,theincreaseofoverlappingvariablescomplicatestheverificationoflinearindependenceofthejointdistribution
family.
B.2.1 Preliminaries
WestartthefollowingresultfromBalsells-Rodasetal.[2024],whichshowstheconditionsunderwhichlinearindependence
canbepreservedforproductfunctionsofconsecutivevariableswithM =1overlap(y).
LemmaB.3. [Balsells-Rodasetal.,2024]ConsidertwofamiliesU := {u (y,x)|i ∈ I}andV := {v (z,y)|j ∈ J}
I i J j
withx∈X,y∈Rdy andz ∈Rdz.Wefurtherassumethefollowingassumptions:
(a1) Positivefunctionvalues:u i(y,x)>0foralli∈I,(y,x)∈Rdy ×X.Similarpositivefunctionvaluesassumption
appliestoV J:v j(z,y)>0forallj ∈J,(z,y)∈Rdz ×Rdy.
(a2) Uniqueindexing:forU ,i̸=i′ ∈I ⇔∃x,ys.t.u (x,y)̸=u (x,y).Similaruniqueindexingassumptionapplies
I i i′
toV ;
J
(a3) Linearindependenceunderfinitemixturesonspecificnon-zeromeasuresubsetsforU :foranynon-zeromeasure
I
subsetY ⊂Rdy,U
I
containslinearlyindependentfunctionsunderfinitemixtureson(y,x)∈Y ×X.
(a4) Linearindependenceunderfinitemixturesonspecificnon-zeromeasuresubsetsforV :thereexistsanon-zeromeasure
J
subsetY ⊂Rdy,suchthatforanynon-zeromeasuresubsetsY′ ⊂Y andZ ⊂Rdz,V
J
containslinearlyindependent
functionsunderfinitemixtureson(z,y)∈Z×Y′;
(a5) LineardependenceunderfinitemixturesforsubsetsoffunctionsinV
J
impliesrepeatingfunctions:foranyβ ∈Rdy,
anynon-zeromeasuresubsetZ ⊂Rdz andanysubsetJ
0
⊂J suchthat|J 0|<+∞,{v j(z,y=β)|j ∈J 0}contains
linearlydependentfunctionsonz ∈Z onlyif∃j ̸=j′ ∈J 0suchthatv j(z,β)=v j′(z,β)forallz ∈Rdz.
(a6) ContinuityforV J:foranyj ∈J,v j(z,y)iscontinuousiny∈Rdy.
Thenforanynon-zeromeasuresubsetZ ⊂Rdz,U
I
⊗V
J
:={v j(z,y)u i(y,x)|i∈I,j ∈J}containslinearindepedent
functionsunderfinitemixturesdefinedon(x,y,z)∈X ×Rdy ×Z.
Insummary,theresultverifiesthatunder(a1-a4),lineardependencecouldoccurforeveryvalueoftheoverlappingvariable
(y).However,thisisnotpossiblethanksto(a5-a6).
Balsells-Rodasetal.[2024]providea"proofbyinduction"techniqueusingLemmaB.3.Similarstrategiesarenotapplicable
for M > 1. Given some initial and transition distributions defined in ... respectively, the base case T = M +1 can
be proven using B.3. However, for T = τ, τ > M +1, we cannot directly use the results from Lemma B.3 as the
inductionhypothesisdoesnotsatisfy(a3),duetoyandzhavingdifferentsizes.Asimilar"proofbyinduction"technique
canbeusedwheny andz areforcedtohavethesamesize.Giventheincreasedsizeoftheconditionedvariables,this
requirestheinductiontechniquetoverifylinearindependencebygroupingthedistributionswithM consecutiveproducts,
{(cid:81)M
p(z |z ,s )}.Consequently,theresultingfamilymustsatisfy(a1),(a2),and(a4-a6)touse
m=1 t+m t−M+m:t−1+m t+m
LemmaB.3.
11B.2.2 Extendingassumptionsonproductfamilies
Belowweexploreassumptionsonnon-parametricfamilieswithM +1variables(alignedwithsometransitiondistribution
p(z |z ,s )),suchthattheproductofM consecutivesvariablessatisfies(a1),(a2),and(a4-a6).Wenoteassumption
t t−1:t−M t
(a2) is redundant, as if it holds for U and V , then (a3) and (a4) hold respectively. However, the converse is not true.
I J
Therefore,weremove(a2)fromourtheoreticalanalysisforsimplicity.Contraryto(a1)and(a6),wenotethattheextension
of(a4-a5)toM consecutiveproductsrequiresthemtobestrengthened.GivenafamilyV ={v (z,y ,...,y )|j ∈J}
J j M 1
withvariables(z,y ,...,y )definedon×M+1Rd,weprovidetheassumptionmodificationsfollowingtheenumeration
M 1 m=1
presentedinLemmaB.3.
(a1)→(b1) Positivefunctionvalues:v (z,y ,...,y )>0forallj ∈J,(z,y ,...,y )∈(×M+1Rd).
j M 1 M 1 m=1
(a4)→(b4) There exist a non-zero measure set Y ⊂ (×M Rd) with µ(Y) = µ(×M Rd) and such that for every
m=1 m=1
non-zeromeasuresetsY′ ⊂Y,andZ ⊂Rd,V containslinearlyindependentfunctionsunderfinitemixtures
J
on(z,y ,...,y )∈Z×Y′;
M 1
(a5)→(b5) For any (β ,...,β ) ∈ (×mRd), with 1 ≤ m ≤ M, any non-zero measure subsets Z ⊂ Rd, Y ⊂
m 1
(×M−mRd), and any subset J ⊂ J such that |J | < +∞, {v (z,y ,...,y ,y = β ,...,y =
0 0 j M m+1 m m 1
β )|j ∈ J }containslinearlydependentfunctionson(z,y ,...,y ) ∈ Z ×Y onlyif∃j ̸= j′ ∈ J
1 0 M m+1 0
suchthatv (z,y ,...,y ,y =β ,...,y =β )=v′(z,y ,...,y ,y =β ,...,y =β )
j M m+1 m m 1 1 j M m+1 m m 1 1
forall(z,y ,...,y )∈×M−m+1Rd.
M m+1
(a6)→(b6) ContinuityforV :foranyj ∈J,v (z,y ,...,y )iscontinuousin(y ,...,y )∈(×M Rd).
J j M 1 M 1 m=1
Weprovidethefollowingdefinitionswhichwillbeusedintheresultsbelow.
Definition B.4. Let A be a set with A ⊆ (×n Rd), and S = {s ,...,s }, where 1 ≤ k ≤ n and s ̸= s , s ,s ∈
m=1 1 k i j i j
{1,...,n}.Wedefineπ(A,S)theprojectionofAintothedimensionsofS as
π(A,S)={(x ,...,x )∈(×k Rd):(x ,...,x )∈A}⊆(×k Rd)
s1 sk m=1 1 n m=1
DefinitionB.5. LetAbeasetwithA⊆(×n Rd).Wedefinethereverseprojectiongiventhelastn−kcomponentsof
m=1
A:(x ,...,x )∈(×n Rd),with1≤k ≤n,as
n k m=k
A ={(x ,...,x )∈(×k−1 Rd):(x ,...,x )∈A}⊆(×k−1 Rd)
(xn,...,xk) 1 k−1 m=1 1 n m=1
Assumption(b4)extendstoconsecutiveproductfunctionsasfollows.
LemmaB.6. B4Extension.AssumeafamilyV ={v (z,y ,...,y )|j ∈J}withvariablesz,y ,...,y definedon
J j M 1 M 1
×M+1Rd,suchthatitsatisfies(b1),(b4),(b5),and(b6).
m=1
Then,foranynwith2≤n≤M,thereexistanon-zeromeasuresetY ⊂(×M Rd)withµ(Y)=µ(×M Rd)suchthat
m=1 m=1
foreverynon-zeromeasuresetsY′ ⊂Y,andZ ⊂(×n Rd),thefamily
m=1
(⊗n V ):={v (z ,...,z ,y ,...,y )...v (z ,y ,...,y )|(j ,...,j )∈(×n J)},
m=1 J jn n 1 M n j1 1 M 1 n 1 m=1
containslinearindependentfunctionsunderfinitemixturesin(z ,...,z ,y ,...,y )∈Z×Y′.
n 1 M 1
Proof. Weprovetheabovestatementbyinduction,wherewestartwiththebasecase(n=2)asfollows.
Casen=2. Assumethestatementisfalse.Then,foreverysetY ⊂(×M Rd)withµ(Y)=µ(×M Rd),thereexists
m=1 m=1
non-zeromeasuresetsZ ⊂(Rd×Rd)andY′ ⊂Y suchthatV ⊗V containslineardependentfunctions.Thismeansthe
J J
followinglineardependenceconditionmustbesatisfied
(cid:88)
γ v (z ,z ,y ,...,y )v (z ,y ,...,y )=0,
j1j2 j2 2 1 M 2 j1 1 M 1
(j1,j2)∈S0
∀(z ,z ,y ,...,y )∈(Z ×Y′), (23)
2 1 M 1
whereS ⊂ J ×J,|S | < +∞,and{γ ∈ R,(j ,j ) ∈ S }isasetofnon-zerovalueswhichmightdependonthe
0 0 j1j2 1 2 0
choiceofZ andY′,wherethelatterdependsonthechoiceoffullmeasureY.
12From(b1),thesetS containstwodifferentindices(j ,j )and(j′,j′),withj ̸=′ j .Toseethis,assumewehavej =j′.
0 1 2 1 2 2 2 2 2
Thenwecangroupv (z ,z ,y ,...,y )
j2 2 1 M 2
(cid:88)
v (z ,z ,y ,...,y ) γ v (z ,y ,...,y )=0,
j2 2 1 M 2 j1j2 j1 1 M 1
j1:(j1,j2)∈S0
∀(z ,z ,y ,...,y )∈(Z ×Y′), (24)
2 1 M 1
Giventhatwehaveatleasttwoindices(j ,j )and(j′,j )withj ̸=j′,theaboveequationcontradicts(b4),asthefamily
1 2 1 2 1 1
V containslinearindependentfunctionsunderfinitemixturesforeveryY′ ⊂Y,withµ(Y)=µ(×M Rd).
J m=1
NowwedefineJ :={j ⊂J|∃(j ,j )∈S },|J |<+∞.Then,lineardependencecanoccurforany(β ,...,β )∈
0 2 1 2 0 0 M 1
(π(Z,{1})×π(Y′,{M,...,2})),whereπdenotesetprojectionsdefinedinDef.B.4.
(cid:32) (cid:33)
(cid:88) (cid:88)
γ v (z =β ,y =β ,...,y =β ,y )
j1j2 j1 1 M M M−1 2 1 1
j2∈J0 j1:(j1,j2)∈S0
v (z ,z =β ,y =β ,...,y =β )=0,
j2 2 1 M M M−1 2 1
∀(z ,y )∈(Z ×Y′). (25)
2 1 2 1
WherebothZ =Z andY′ =Y′ arereverseprojections(Def.B.5)fromZ andY′ respectivelyandare
2 (βM) 1 (βM−1,...,β1)
dependentonβ ,...,β .Wenotethesesetsareneveremptysets.Now,wedefinethefollowingset.
M 1
(cid:40) (cid:12)
D 0 := (β M,...,β
1)∈(cid:16) π(Z,{1})×π(Y′,{M,...,2})(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:41)
(cid:88)
γ v (z =β ,y =β ,...,y =β ,y )=0,∀y ∈Y′ . (26)
j1,j2 j1 1 M M M−1 2 1 1 1 (βM−1,...,β1)
j1:(j1,j2)∈S0
From(b4)werequirelinearindependenceforeveryY′ ⊂Y,whereY hasfullmeasure;andeveryZ ⊂Rd.Therefore,linear
dependenceasdescribedabovecanhappenatmostin(Rd×(×M Rd)\Y),whichiszero-measuredas((×M Rd)\Y)
m=1 m=1
haszeromeasurein(×M Rd).Therefore,from(b4)weknowthesetD ⊂(π(Z,{1})×Y )haszeromeasuredueto
m=1 0 0
Y ⊂π(Y′,{M,...,2})havingzeromeasure.
0
NowdefineD := (π(Z,{1})×π(Y′,{M,...,2}))\D whichisnon-zeromeasured.Fromassumption(b1),wehave
0
∀(z =β ,y =β ,...,y =β )∈D,thereexistsy ∈Y′ suchthat
1 M M M−1 2 1 1 (βM−1,...,β1)
(cid:88)
γ˜ (β ,...,β ):= γ v (z =β ,y =β ,...,y =β ,y )̸=0
j2 M 1 j1j2 j1 1 M M M−1 2 1 1
j1:(j1,j2)∈S0
for at least two j ∈ J . This implies linear dependence of {v (z ,z = β ,y = β ,...,y = β )} on
2 0 j2 2 1 M M M−1 2 1
z ∈Z ,∀(z =β ,y =β ,...,y =β )∈D.
2 (βM) 1 M M M−1 2 1
Underassumption(b5),wecansplitJ intosubsetsindexedbyk ∈K(β ,...,β ),suchthatthefunctionswithineach
0 M 1
subsetJ (β ,...,β )areequal
k M 1
J =∪ J (β ,...,β ), J (β ,...,β )∩J (β ,...,β )=∅, ∀k ̸=k′ ∈K(β ,...,β ), (27)
0 k∈K(βM,...,β1) k M 1 k M 1 k′ M 1 M 1
j ̸=j′ ∈J (β ,...,β ) ⇔
k M 1
v (z ,z =β ,...,y =β )=v (z ,z =β ,...,y =β ), ∀z ∈Z . (28)
j 2 1 M 2 1 j′ 2 1 M 2 1 2 (βM)
Thenwecanrewritethelineardependenceconditionforany(β ,...,β )∈Das
M 1
(cid:32)
(cid:88) (cid:88) (cid:88)
γ v (z =β ,y =β ,...,y =β ,y )
j1j2 j1 1 M M M−1 2 1 1
k∈K(βM,...,β1) j2∈Jk(βM,...,β1)j1:(j1,j2)∈S0
(cid:33)
v (z ,z =β ,...,y =β ) =0, ∀(z ,y )∈,Z ×Y′ (29)
j2 2 1 M 2 1 2 1 (βM) (βM−1,...,β1)
13Recall from that v (z ,z = β ,...,y = β ) and v (z ,z = β ,...,y = β ) are the same functions on
j 2 1 M 2 1 j′ 2 1 M 2 1
z ∈Z iff.j ̸=j′ areinthesameindexsetJ (β ,...,β ).Thismeansiflinearindependenceholds,thenforany
2 (βM) k M 1
(β ,...,β )∈D,underassumptions(b1)and(b5),
M 1
(cid:88) (cid:88)
γ v (z =β ,y =β ,...,y =β ,y )=0,
j1j2 j1 1 M M M−1 2 1 1
j2∈Jk(βM,...,β1)j1:(j1,j2)∈S0
∀y ∈,Y′ , k ∈K(β ,...,β ). (30)
1 (βM−1,...,β1) M−1 1
DefineC(β ,...,β ) = min |J (β ,...,β )|theminimumcardinalitycountforj indicesintheJ (β ,...,β )
M 1 k k M 1 2 k M 1
subsets.
Choose(β∗ ,...,β∗)∈argmin C(β ,...,β ):
M 1 (βM,...,β1)∈D M 1
1. We have C(β∗ ,...,β∗) < |J | and |K(β∗ ,...,β∗)| ≥ 2. Otherwise for all j ̸= j′ ∈ J we have v (z ,z =
M 1 0 M 1 0 j 2 1
β ,...,y =β )=v (z ,z =β ,...,y =β )forallz ∈Z and(β ,...,β )∈D),sothattheyare
M 2 1 j′ 2 1 M 2 1 2 (βM) M 1
linearlydependenton(z ,z ,y ,...,y )∈Z˜ ×Dforsomenon-zeromeasureZ˜ ⊆π(Z,{2}),acontradictionto
2 1 M 2 2 2
assumption(b4)bysettingY′ =D,whichholdsnomatterthechoiceofZ asY′ ⊂Y whereY hasfullmeasure.
2. Now assume |J (β∗ ,...,β∗)| = C(β∗ ,...,β∗) w.l.o.g.. From assumption (b5), we know that for any j ∈
1 M 1 M 1
J (β∗ ,...,β∗)andj′ ∈ J \J (β∗ ,...,β∗),v (z ,z = β ,...,y = β ) = v (z ,z = β ,...,y = β )
1 M 1 0 1 M 1 j 2 1 M 2 1 j′ 2 1 M 2 1
onlyonazeromeasuresubsetofZ atmost.Thenas|J |<+∞andZ ⊂Rdhasnon-zeromeasure,there
(βM) 0 (βM)
existz ∈Z andδ >0suchthat
0 (βM)
|v (z =z ,z =β∗ ,...,y =β∗)−v (z =z ,z =β∗ ,...,y =β∗)|≥δ,
j 2 0 1 M 2 1 j′ 2 0 1 M 2 1
∀j ∈J (β∗ ,...,β∗),∀j′ ∈J \J (β∗ ,...,β∗) (31)
1 M 1 0 1 M 1
Underassumption(b6),thereexistsϵ(j)>0suchthatwecanconstructanϵ-ballB (β∗ ,...,β∗)usingℓ -norm,
ϵ(j) M 1 2
suchthat
|v (z =z ,z =β∗ ,...,y =β∗)−v (z =z ,z =β ,...,y =β )|≤δ/3,
j 2 0 1 M 2 1 j 2 0 1 M 2 1
∀(β ,...,β )∈B (β∗ ,...,β∗). (32)
M 1 ϵ(j) M 1
Choosing a suitable 0 < ϵ ≤ min ϵ(j) (note that min ϵ(j) > 0 as |J | < +∞) and constructing an ℓ -
j∈J0 j∈J0 0 2
norm-based ϵ-ball B (β∗ ,...,β∗) ⊂ Y , we have for all j ∈ J (β∗ ,...,β∗),j′ ∈ J \J (β∗ ,...,β∗), j′ ∈/
ϵ M 1 1 1 M 1 0 1 M 1
J (β ,...,β )forall(β ,...,β )∈B (β∗ ,...,β∗)dueto
1 M 1 M 1 ϵ M 1
|v (z =z ,z =β ,...,y =β )−v (z =z ,z =β ,...,y =β )|≥δ/3,
j 2 0 1 M 2 1 j′ 2 0 1 M 2 1
∀(β ,...,β )∈B (β∗ ,...,β∗). (33)
M 1 ϵ M 1
Sothismeansforthesplit{J (β ,...,β )}ofany(β ,...,β )∈B (β∗ ,...,β∗),wehaveJ (β ,...,β )⊂
k M 1 M 1 ϵ M 1 1 M 1
J (β∗) and therefore |J (β ,...,β )| ≤ |J (β∗ ,...,β∗)|. Now by definition of (β∗ ,...,β∗) ∈
1 1 M 1 1 M 1 M 1
argmin C(β ,...,β ) and |J (β∗ ,...,β∗)| = C(β∗ ,...,β∗), we have J (β ,...,β ) =
(βM,...,β1)∈Y M 1 1 M 1 M 1 1 M 1
J (β∗ ,...,β∗)forall(β ,...,β )∈B (β∗ ,...,β∗).
1 M 1 M 1 ϵ M 1
3. One can show that |J (β∗ ,...,β∗)| = 1, otherwise by definition of the splits and the above point, there exists
1 M 1
j ̸=j′ ∈J (β∗ ,...,β∗)suchthatv (z ,z =β ,...,y =β )=v (z ,z =β ,...,y =β )forallz ∈
1 M 1 j 2 1 M 2 1 j′ 2 1 M 2 1 2
Z and(β ,...,β )∈B (β∗ ,...,β∗),acontradictiontoassumption(b4)bysettingY′ =B (β∗ ,...,β∗).
(βM) M 1 ϵ M 1 ϵ M 1
Nowassumethatj ∈ J (β∗ ,...,β∗)istheonlyindexinthesubset,thenthefactprovedintheabovepointthat
1 M 1
J (β ,...,β )=J (β∗ ,...,β∗)forall(β ,...,β )∈B (β∗ ,...,β∗)means
1 M 1 1 M 1 M 1 ϵ M 1
(cid:88)
γ v (z =β ,y =β ,...,y =β ,y )=0,
j1j2 j1 1 M M M−1 2 1 1
j1:(j1,j2)∈S0
∀y ∈,Y′ , ∀(β ,...,β ))∈B (β∗ ,...,β∗), (34)
1 (βM−1,...,β1) M 1 ϵ M 1
againacontradictiontoassumption(b4)bysettingD =B (β∗ ,...,β∗).
ϵ M 1
14Theabove3pointsindicatethatlineardependencecannotholdforall(β ,...,β )∈D,andthusreachingacontradiction
M 1
withintheprojectionsetsfromthechosenZ andY′ ⊂Y.GiventhatthespaceinwhichwedefineY′isfinite-dimensional,
wecancovertheentirespacewithepsilonballsofcertainradiusϵ.Weapplythesamelogictothefullmeasuresetfrom(b4)
onthefamily{v (z ,z ,y ,...,y ):j ∈J}.Therefore,wecancovertheentirefullmeasuresetwithϵballssuchthat
j2 2 1 M 2 2
thepreviousargumentsholdforanynon-zeromeasuresets(π(Z,{1})×π(Y′,{M,...,2}))⊆Y exceptforsomezero
measuresetofpoints,nomatterthechoiceofZ orY′(Y isthefullmeasuresetin(b4)).
Finally,theaboveargumentneedstoholdforanyfullmeasuresetY ⊂(×M Rd).However,from(b4),theabovecondition
m=1
impliesπ(Y′,{M,...,2})canonlysatisfylineardependenceifitiszero-measured,whichisadirectcontradictionthe
statement.
Casen>2. Nowassumethestatementholdsforn−1,andagainprovethecasebycontradiction.Assumethestatement
is false for n > 2. Then, for every set Y ⊂ (×M Rd) with µ(Y) = µ(×M Rd), there exists non-zero measure sets
m=1 m=1
Z ⊂ (×n Rd)andY′ ⊂ Y suchthatthefamily(⊗n V )containslineardependentfunctions.Inotherwords,there
m=1 m=1 J
existsS ⊂(×n J),|S |<+∞suchthat
0 m=1 0
(cid:88)
γ v (z ,...,z ,y ,...,y )...v (z ,y ,...,y )=0,
jn,...,j1 jn n 1 M n j1 1 M 1
(jn,...j1)∈S0
∀(z ,...,z ,y ,...,y )∈(Z ×Y′), (35)
n 1 M 1
with{γ ∈ R,(j ,...,j ) ∈ S }asetofnon-zerovalueswhich,asbefore,mightdependonthechoiceofZ and
jn,...,j1 n 1 0
Y′ ⊂Y.Thepreviousequalitycanbearrangedasfollows
(cid:88)
γ v (z ,...,z ,y ,...,y )v (z ,...,z ,y ,...,y )=0,
jn,...,j1 jn n 1 M n jn−1,...,j1 n−1 1 M 1
(jn,...j1)∈S0
∀(z ,...,z ,y ,...,y )∈(Z ×Y′), (36)
n 1 M 1
where v := v (z ,...,z ,y ,...,y )...v (z ,y ,...,y ) for j ∈ J,1 ≤ i < n denotes the
jn−1,...,j1 jn−1 n−1 1 M n−1 j1 1 M 1 i
functionsonthefamily(⊗n−1V )andsatisfiesthefollowing:
m=1 J
(b1) Positive function values: v (z ,...,z ,y ,...,y ) > 0 for all (j ,...,j ) ∈
jn−1,...,j1 n−1 1 M 1 n−1 1
(×n−1J),(z ,...,z ,y ,...,y )∈(×M+n−1Rd).
m=1 n−1 1 M 1 m=1
(b4) Thereexistnon-zeromeasuresetsY ⊂(×M Rd)withµ(Y)=µ(×M Rd)suchthatforeverynon-zeromeasure
m=1 m=1
setsY′ ⊂Y,andZ ⊂(×n−1Rd),thefamily(⊗n−1V )containslinearindependentfunctionsunderfinitemixtures
m=1 m=1 J
in(z ,...,z ,y ,...,y )∈Z×Y′.
n−1 1 M 1
Thestrategyhereistoreducethiscasetotheabovebasecase(n=2),whereimportantlyweneedtoshow:
(1) S containsatleasttwo(j ,...,j )and(j′,...,j′)withj ̸=j′;and
0 n 1 n 1 n n
(2) theoverlappingvariablesbetweentheproductoffamilies,(z ,...,z ,y ,...,y )∈(π(Z,{n−1,...,1})×
n−1 1 M n
π(Y′,{M,...,n}))donotcauselineardependenceinnon-zeromeasuresetsforanychoiceofZ andY′ ⊂Y.
Wecansimplysee(i)holdsfrom(b1)and(b4).Therefore,wedefineJ := {j ∈ J|∃(j ,...,j ) ∈ S },|J | < +∞.
0 n n 1 0 0
Then,lineardependencecanoccurforany(β ,...,β )∈(π(Z,{n−1,...,1})×π(Y′,{M,...,n}))⊂(×M Rd),
M 1 m=1
whereπdenotesetprojectionsdefinedinDef.B.4.
(cid:88)
v (z ,z =β ,...,z =β ,y ,=β ...,y =β )
jn n n−1 M 1 M−n+2 M M−n+1 n 1
jn∈J0
(cid:32)
(cid:88)
γ v (z =β ,...,z =β ,y ,=β ,
jn,...,j1 jn−1,...,j1 n−1 M 1 M−n+2 M M−n+1
(jn−1,...,j1):(jn,...,j1)∈S0
(cid:33)
...,y =β ,y ,...,y ) =0,
n 1 n−1 1
(cid:16) (cid:17)
∀(z ,y ,...,y )∈ Z ×Y′ . (37)
n n−1 1 (βM,...,βM−n+2) (βM−n+1,...,β1)
15Whereasinthebasecase,bothZ ⊂ Rd andY′ ⊂ (×n−1Rd)arereverseprojections(Def.
(βM,...,βM−n+2) (βM−n+1,...,β1) m=1
B.5)fromZ andY′ respectivelyandaredependentonβ ,...,β .Asbefore,thesesetsareneverempty,andwecan
M 1
establishthefollowingequivalencebetweenvariablesandsets(giventhechoiceofZ andY′ ⊂Y)withrespecttothebase
case.
• z isequivalenttoz ,andthusZ ⊂RdisequivalenttoZ ⊂Rd.
n 2 (βM,...,βM−n+2) (βM,...,βM)
• z ,...,z areequivalenttoz ,andthusπ(Z,{n−1,...,1})⊂(×n−1Rd)isequivalenttoπ(Z,{1})⊂Rd.
n−1 1 1 m=1
• y ,...,y are equivalent to y ,...,y , and thus π(Y′,{M,...,n}) ⊂ (×M−n+1Rd) is equivalent to
M n M 2 m=1
π(Y′,{M,...,2})⊂(×M−1Rd).
m=1
• y ,...,y areequivalenttoy ,andthusY′ ⊂(×n−1Rd)isequivalenttoY′ ⊂Rd.
n−1 1 1 (βM−n+1,...,β1) m=1 (βM−1,...,β1)
Giventheaboveequivalences,theargumentsforcontradictiongivenlineardependenceforany(β ,...,β )∈(π(Z,{n−
M 1
1,...,1})×π(Y′,{M,...,n}))⊂(×M Rd)stillhold.Theonlydifferenceisthatthedimensionalityoftheprojections
m=1
andreverseprojectionswillchange(exceptforz ∈Z ).Therefore,theset
n (βM,...,βM−n+2)
(cid:40) (cid:12)
D 0 := (β M,...,β
1)∈(cid:16) π(Z,{n−1,...,1})×π(Y′,{M,...,n})(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:88)
γ v (z =β ,...,z =β ,y ,=β ,
jn,...,j1 jn−1,...,j1 n−1 M 1 M−n+2 M M−n+1
(jn−1,...,j1):(jn,...,j1)∈S0
(cid:41)
...,y =β ,y ,...,y )=0,∀(y ,...,y )∈Y′ (38)
n 1 n−1 1 n−1 1 (βM−n+1,...,β1)
has zero measure under assumption (b4), which implies that for any (z = β ,...,z = β ,y ,=
n−1 M 1 M−n+2 M
β ,...,y =β )∈D,withD :=(π(Z,{n−1,...,1})×π(Y′,{M,...,n}))\D ,thereexists(y ,...,y )∈
M−n+1 n 1 0 n−1 1
Y′ non-zero-measured,wehave
(βM−n+1,...,β1)
(cid:88)
γ˜ (β ,...,β ):= γ v (z =β ,...,
jn M 1 jn,...,j1 jn−1,...,j1 n−1 M
(jn−1,...,j1):(jn,...,j1)∈S0
z =β ,y ,=β ,...,y =β ,y ,...,y )̸=0 (39)
1 M−n+2 M M−n+1 n 1 n−1 1
foratleasttwoj ∈ J .Asbefore,thisimplieslineardependenceof{v (z ,z = β ,...,z = β ,y ,=
n 0 jn n n−1 M 1 M−n+2 M
β ...,y = β )} on z ∈ Z , ∀(z = β ,...,z = β ,y ,= β ...,y =
M−n+1 n 1 n (βM,...,βM−n+2) n−1 M 1 M−n+2 M M−n+1 n
β )∈D.Therefore,underassumptions(b5-b6)wecanshowlineardependencecannotholdforall(β ,...,β )∈D,no
1 M 1
matterthechoiceofZ andY′ ⊂Y,followingtheequivalentargumentsgiveninthecasewithn=2.
Regarding(b5),weobservethatlineardependenceonM productsoffunctionsgivenfixedsubsetsofvariablesdoesnot
implyrepeatingfunctionsanymore.Instead,weshowthatatleastonecomponentoftheproductfamilyislinearindependent
underfinitemixtures.
LemmaB.7. B5Extension.AssumeadistributionfamilyV ={v (z,y ,...,y )|j ∈J}withvariablesz,y ,...,y
J j M 1 M 1
definedon×M+1Rd,suchthatitsatisfies(b1),(b4),(b5),(b6).
m=1
Then, for any n with 2 ≤ n ≤ M, any (β ,...,β ) ∈ (×M Rd), any non-zero measure Z ⊂ (×n Rd),
M 1 m=1 m=1
and (×n J ) ⊂ (×n J), then {v (z ,...,z ,y = β ,...,y = β )...v (z ,y = β ,...,y =
m=1 m m=1 jn n 1 M M n n j1 1 M M 1
β )|(j ,...,j )∈(×n J )}containslinearlydependentfunctionsonlyif∃m≤nsuchthat{v (z ,...,z ,y =
1 n 1 m=1 m jm m 1 M
β ,...,y =β )|j ∈J }containslinearlydependentfunctionson(z ,...,z )∈π(Z,{m,...,1}).
M m m m m m 1
Proof. Weproofthestatementbyinductionstartingfromthebasecase(n=2)asfollows.
Casen=2. Assumetheabovenecessityassertionisfalse.FixZ ⊂Rd×Rd,andassumethatforany(β ,...,β )∈
M 1
(×M Rd),andS :=J ×J ⊂J ×J,thefamily{v (z ,y =β ,...,y =β )v (z ,z ,y =β ,...,y =
m=1 0 1 2 j1 1 M M 1 1 j2 2 1 M M 2
β )|(j ,j )∈(J ,J )}containslineardependentfunctions.From(b1,b4-b6),andLemmaB.3,bysettingu (y=z ,x=
2 1 2 1 2 i 1
∅) = v (z ,y = β ,...,y = β ),i = j ,andv = (z = z ,y = z ) = v (z ,z ,y = β ,...,y = β ),
j1 1 M M 1 1 1 j 2 1 j2 2 1 M M 2 2
j =j ,weknowthateitheroneofthefamiliescontainslineardependentfunctions(fromcontradictionsto(a3)or(a4)).
2
16Casen>2. Againassumethenecessityassertionisfalse.FixZ ⊂(×n Rd),andassumethatforany(β ,...,β )∈
m=1 M 1
(×M Rd), and (×n J ) ⊂ (×n J), the family {v (z ,...,z ,y = β ,...,y = β )...v (z ,y =
m=1 m=1 m m=1 jn n 1 M M n n j1 1 M
β ,...,y = β )|(j ,...,j ) ∈ (×n J )}containslinearlydependentfunctions.Asbefore,from(b1,b4-b6),and
M 1 1 n 1 m=1 m
LemmaB.3,weknowthatbysetting
u (y=(z ,...,z ),x=∅)=v (z ,...,z ,y =β ,...,y =β )...
i n−1 1 jn−1 n−1 1 M M n−1 n−1
v (z ,y =β ,...,y =β ), i=(j ,...,j ),
j1 1 M M 1 1 n−1 1
andv (z = z ,y = (z ,...,z )) = v (z ,...z ,y = β ,...,y = β ), j = j ,eitheroneofthefamilies
j n n−1 1 jn n 1 M M n n n
mustbelinearlydependentfromcontradictionsto(a3)or(a4).Therefore,either{v (z ,...,z ,y =β ,...,y =
jn n 1 M M n
β )|j ∈J )}containslinearlydependentfunctionson(z ,...,z )∈π(Z,{n,...,1}),orfromtheinductionhypothesis
n n n n 1
thereissomem≤n−1suchthat{v (z ,...,z ,y =β ,...,y =β )|j ∈J }containslinearlydependent
jm m 1 M M m m m m
functionson(z ,...,z )∈π(π(Z,{n−1,...,1}),{m,...,1})=π(Z,{m,...,1}).
m 1
B.2.3 LinearindependenceonM+1productsoffunctions
Given that assumption (b5) does not extend similarly as (b4), we adapt Lemma B.3 where we use B.7 to show linear
independencewhenV isdefinedasaproductofM functionswithconsecutivevariables.
J
LemmaB.8. LinearIndependenceonProductfunctionswithM consecutiveOverlappingvariables(LIPO-M).Assume
twofamiliesU
I
={u i(y,x)|i∈I}andV
J
={v j(z,y)|j ∈J},withx∈Rdx,y∈Rdy,andz ∈Rdz,withd
y
=d·M,
d =d·n,and1≤n≤M.Wefurtherassume:
z
(a) ThefamilyU satisfiesassumptions(a1)and(a3).
I
(b) EachelementinthefamilyV isdefinedasaproductofnfunctions,allofwhichbelongtothesamefamilyW =
J K
{w (z ,y ,...,y )|k ∈K},withz ∈Rd,(y ,...,y )∈(×M Rd),andisexpressedasfollows
k 1 M 1 1 M 1 m=1
V :=(⊗n W )={w (z ,...,z ,y )...w (z ,y ,...,y )|(k ,...,k )∈(×n K)}
J m=1 K kn n 1 M k1 1 M 1 n 1 m=1
whereJ :=(×n K),andz :=(z ,...,z ),y:=(y ,...,y )inV .ThefamilyW satisfiesassumptions(b1),
m=1 n 1 M 1 J K
(b4),(b5),and(b6).
Thenforanynon-zeromeasuresubsetZ ⊂Rdz,U
I
⊗V
J
:={u i(y,x)v j(z,y)|i∈I,j ∈J}containslinearindepedent
functionsunderfinitemixturesdefinedon(x,y,z)∈X ×Rdy ×Z.
Proof. Assumethissufficiencystatementisfalse,thenthereexistanon-zeromeasuresubsetZ ⊂Rdz,S
0
⊂I×J with
|S |<+∞andasetofnon-zerovalues{γ ∈R|(i,j)∈S },suchthat
0 ij 0
(cid:88)
γ v (z,y)u (y,x)=0, ∀(x,y,z)∈X ×Rdy ×Z. (40)
ij j i
(i,j)∈S0
NotethatthechoicesofS andγ areindependentofany(x,y,z)values,butmightbedependentonZ.FromLemma
0 ij
B.6weknowthatthefamilyV asdefinedin(b),satisfies(a4)fromLemmaB.3.Furthermore,italsosatisfies(a1)asit
J
isdefinedasaproductofnfunctionsthatsatisfy(b1),with1≤n≤M.Therefore,wefollowthesameargumentsasin
LemmaB.3,whichshowthattheindexsetS containsatleast2differentindices(i,j)and(i,j′)withj ̸=j′.Moreover,
0
wedefineJ ={j ∈A|∃(i,j)∈S }thesetofallpossiblejindicesthatappearinS ,where|J |<+∞andthefollowing
0 0 0 0
setY
0
:= {β ∈ Rdy|(cid:80) i:(i,j)∈S0γ iju i(y = β,x) = 0,∀x ∈ X}canonlyhavezeromeasureinRdy from(a3).Again
followingLemmaB.3,wehaveanon-zeromeasuresetY :=Y\Y ⊂Y wherewechooseY fromthenon-zeromeasure
1 0
(cid:80)
setin(b4).Then,wehaveforeachβ ∈Y ,thereexistsx∈X suchthat γ u (y=β,x)̸=0foratleasttwoj
1 i:(i,j)∈S0 ij i
indicesinJ .Thisimpliesforeachβ ∈Y ,{v (z,y=β)|j ∈J }containslinearlydependentfunctionsonz ∈Z.
0 1 j 0
From(b4-b6),LemmaB.7showsthattheproductofnfunctions(1≤n≤M)thatcomposethefamilyV implieslinear
J
dependentfunctionsofatleastoneofthecomponents.Thatis,forsomem≤n,{w (z ,...,z ,y =β ),k :
km m 1 M:m M:m m
(k ,...,k ) ∈ J },withy := (y ,...,y ),β := (β ,...,β ),andJ := (K(n)×···×K(1)),contains
n 1 0 M:m M m M:m M m 0 0 0
lineardependentfunctionson(z ,...,z )∈π(Z,{m,...,1}).Underassumption(b5),wecansplittheindexsetK(m)
m 1 0
17intosubsetsindexedbyl∈L(m)(β )asfollows,suchthatwithineachindexsubsetK(m)(β )thefunctionswith
M:m l M:m
thecorrespondingindicesareequal:
K(m) =∪ K(m)(β ), K(m)(β )∩K(m)(β )=∅,∀l̸=l′ ∈L(m)(β ),
0 l∈L(m)(βM:m) l M:m l M:m l′ M:m M:m
k ̸=k′ ∈K(m)(β ) ⇔ w (z ,...,z ,y =β )=w (z ,...,z ,y =β ), (41)
m m l M:m km m 1 M:m M:m k m′ m 1 M:m M:m
∀(z ,...,z )∈π(Z,{m,...,1}).
m 1
ThenwecanrewriteEq.(40)foranyβ ∈Y as
1
(cid:32)
(cid:88) (cid:88) (cid:88)
γ u (y=β,x)
i,kn,...,k1 i
l∈L(m)(βM:m) km∈K l(m)(βM:m)(i,kn,...,km+1,km−1,...,k1):(i,kn,...,k1)∈S0
(cid:33)
w (z ,...,z ,y =β )...w (z ,y =β ,...,y =β ) =0, ∀(x,z ,...,z )∈X ×Z. (42)
kn n 1 M M k1 1 M M 1 1 n 1
Note that we use γ = γ for any (i,j) ∈ S and (k ,...,k ) ∈ J , such that j = (k ,...,k ). Recall
i,kn,...,k1 ij 0 n 1 0 n 1
fromEq.(41)thatw (z ,...,z ,y = β )andw (z ,...,z ,y = β )arethesamefunctionson
km m 1 M:m M:m k m′ m 1 M:m M:m
(z ,...,z )∈π(Z,{m,...,1})iff.k ̸=k′ areinthesameindexsetK(1)(β).ThismeansifEq.(40)holds,thenfor
m 1 m m l
anyβ ∈Y ,underassumptions(b1)and(b5),
1
(cid:88) (cid:88)
γ u (y=β,x)w (z ,...,z ,y =β )
i,kn,...,k1 i kn n 1 M M
km∈K l(m)(βM:m)(i,kn,...,km+1,km−1,...,k1):(i,kn,...,k1)∈S0
...w (z ,...,z ,y =β )w (z ,...,z ,y =β )
km+1 m+1 1 M:m+1 M:m+1 km−1 m−1 1 M:m−1 M:m−1
...w (z ,y =β )=0, ∀(x,z ,...,z )∈X ×Z, l∈L(m)(β ). (43)
k1 1 M:1 M:1 n 1 M:m
DefineC(m)(β )= min |K(m)(β )|theminimumcardinalitycountfork indicesintheK(m)(β )subsets.
M:m l l M:m m l M:m
Chooseβ∗ ∈argmin C(m)(β ):
M:m βM:m∈π(Y1,{M,...,m}) M:m
1. We have C(m)(β∗ ) < |K(m)| and |L(m)(β∗ )| ≥ 2. Otherwise for all k ̸= k′ ∈ K(m) we have
M:m 0 M:m m m 0
w (z ,...,z ,y = β ) = w (z ,...,z ,y = β ) for all (z ,...,z ) ∈ π(Z,{m,...,1})
km m 1 M:m M:m k m′ m 1 M:m M:m m 1
andβ ∈π(Y ,{M,...,m}),sothattheyarelinearlydependenton(z ,...,z ,y )∈π(Z,{m,...,1})×
M:m 1 m 1 M:m
π(Y ,{M,...,m}),acontradictiontoassumption(b4).
1
2. Nowassume|K(m)(β∗ )|=C(m)(β∗ )w.l.o.g..Fromassumption(b5),weknowthatforanyk ∈K(m)(β∗ )
1 M:m M:m m 1 M:m
and k′ ∈ K(m)\K(m)(β∗ ), w (z ,...,z ,y = β ) = w (z ,...,z ,y = β ) only on
m 0 1 M:m km m 1 M:m M:m k m′ m 1 M:m M:m
zero measure subset of π(Z,{m,...,1}) at most. Then as |K(m)| < +∞ and π(Z,{m,...,1}) ⊂ (×mRd) has
0
non-zeromeasure,thereexist(z(0),...,z(0))∈π(Z,{m,...,1})andδ >0suchthat
m 1
|w (z =z(0) ,y =β∗ )−w (z =z(0) ,y =β∗ )|≥δ,
km m:1 m:1 M:m M:m k m′ m:1 m:1 M:m M:m
∀k ∈K(m)(β∗ ),∀k′ ∈K(m)\K(m)(β∗ ). (44)
m 1 M:m m 0 1 M:m
Underassumption(b6),thereexistsϵ(k ) > 0suchthatwecanconstructanϵ-ballB (β∗ )usingℓ -norm,
m ϵ(km) M:m 2
suchthat
|w (z =z(0) ,y =β∗ )−w (z =z(0) ,y =β )|≤δ/3, ∀β ∈B (β∗ ).
km m:1 m:1 M:m M:m km m:1 m:1 M:m M:m M:m ϵ(km) M:m
(45)
Choosing a suitable 0 < ϵ ≤ min ϵ(k ) (note that min ϵ(k ) > 0 as |K(m)| < +∞) and
km∈K 0(m) m km∈K 0(m) m 0
constructinganℓ -norm-basedϵ-ballB (β∗ )⊂π(Y ,{M,...,m}),wehaveforallk ∈K(m)(β∗ ),k′ ∈
2 ϵ M:m 1 m 1 M:m m
K(m)\K(m)(β∗ ),k′ ∈/ K(m)(β )forallβ ∈B (β∗ )dueto
0 1 M:m m 1 M:m M:m ϵ M:m
|w (z =z(0) ,y =β )−w (z =z(0) ,y =β )|≥δ/3, ∀β ∈B (β∗ ). (46)
km m:1 m:1 M:m M:m k m′ m:1 m:1 M:m M:m M:m ϵ M:m
18So this means for the split {K(m)(β )} of any β ∈ B (β∗ ), we have K(m)(β ) ⊂ K(m)(β∗ ) and
l M:m ϵ M:m 1 M:m 1 M:m
therefore|K(m)(β )|≤|K(m)(β∗ )|.Nowbydefinitionofβ∗ ∈argmin C(m)(β )
1 M:m 1 M:m M:m βM:m∈π(Y,{M,...,m}) M:m
and|K(m)(β∗ )|=C(m)(β∗ ),wehaveK(m)(β )=K(m)(β∗ )forallβ ∈B (β∗ ).
1 M:m M:m 1 M:m 1 M:m M:m ϵ M:m
3. Onecanshowthat|K(m)(β∗ )|=1,otherwisebydefinitionofthesplit(Eq.(41))andtheabovepoint,thereexists
1 M:m
k ̸=k′ ∈K(m)(β∗ )suchthatw (z ,...,z ,y =β )=w (z ,...,z ,y =β )forall
m m 1 M:m km m 1 M:m M:m k m′ m 1 M:m M:m
(z ,...,z )∈π(Z,{m,...,1})andβ ∈B (β∗ ),againacontradictiontoassumption(b4).
m 1 M:m ϵ M:m
Giventheabove3pointsandassumingk ∈K(m)(β∗ )istheonlyindexinthesubset,thenthefactprovedintheabove
m 1 M:m
pointthatK(m)(β )=K(m)(β∗ )means
1 M:m 1 M:m
(cid:88)
γ u (y=β,x)w (z ,...,z ,y =β )
i,kn,...,k1 i kn n 1 M M
(i,kn,...,km+1,km−1,...,k1):(i,kn,...,k1)∈S0
...w (z ,...,z ,y =β )w (z ,...,z ,y =β )
km+1 m+1 1 M:m+1 M:m+1 km−1 m−1 1 M:m−1 M:m−1
...w (z ,y =β )=0, ∀(x,z ,...,z )∈X ×Z,∀β ∈B (β∗ ); (47)
k1 1 M:1 M:1 n 1 M:m ϵ M:m
where the term w (z ,...,z ,y = β ) can be omitted from (b1). Now, by setting Y =
km m 1 M:m M:m
(B (β∗ ) × π(Y ,{m − 1,...,1})) \ Y , this implies linear dependence on the family {w (z ,...,z ,y =
ϵ M:m 1 0 kn n 1 M
β )...w (z ,...,z ,y = β )w (z ,...,z ,y = β )...w (z ,y =
M km+1 m+1 1 M:m+1 M:m+1 km−1 m−1 1 M:m−1 M:m−1 k1 1 M:1
β )|(k ,...,k ) ∈ (K(n) × ··· × K(m+1) × K(m−1)··· × K(1))}, and from Lemma B.7, we know there is at
M:1 n 2 0 0
leastonem′ ≤nwithm′ ̸=msuchthat{w (z ,...,z ,y =β ),k′ :(k ,...,k )∈J }containslinear
k m′ m′ 1 M:m′ M:m′ m n 1 0
independentfunctionsunderfinitemixtures.
ThepreviousargumentcanberepeatedlyappliedtoshowlineardependenceoneveryproductelementintheW family.
K
Finallywehave
(cid:88)
γ u (y=β,x)=0, ∀(x,y)∈X ×B (β∗), (48)
i,kn,...,k1 i ϵ
i:(i,kn,...,k1)∈S0
wherewesetβ∗ :=β .Foranym,andβ ∈B (β∗ ),wehave|K(m)(β )|≤|K(m)(β∗ )|(showninstep
M:1 M:m ϵ M:m 1 M:m 1 M:m
2above).Thereforebydefinitionofβ∗ ∈argmin C(m)(β ),wecanobtainalignmentofβ∗
M:m βM:m∈π(Y,{M,...,m}) M:m M:m
w.r.t.anyβ∗ wherem′ ̸=m.I.e.foranym̸=m′,with1≤m<m′ ≤nwlog.,wehavetwoβ∗(m),β∗(m′),suchthat
M:m′ M:m M:m′
β∗(m) =β∗(m′).Thisshowsacontradictionto(a3)byfixingY =B (β∗).
M:m′ M:m′ ϵ
B.2.4 Linearindependenceofnon-parametricjointdistributionfamily
Belowwepresentlinearindependencefornon-parametricjointdistributionfamilies.
TheoremB.9. Definethefollowingjointdistributionfamily
(cid:40) T (cid:41)
(cid:89)
p (x )=p (x ) p (x |x ,...,x ), p ∈ΠM,p ∈PM,t=M +1,...,T , (49)
a1:T 1:T a1:M 1:M at t t−1 t−M a1:M A at A
t=M+1
andassumeΠM andPM satisfythefollowingassumptions,
A A
(c1) ΠM satisfies(a1),(a3),and
A
(c2) PM satisfies(b4-b6).
A
Thenthefollowingstatementholds:ForanyT >M suchthatn=M ifT ≡0 mod M,orn=(T mod M),otherwise;
andanysubsetX ×···×X ⊂Rnd,thejointdistributionfamilycontainslinearlyindependentdistributionsunderfinite
(cid:124) (cid:123)(cid:122) (cid:125)
ntimes
mixturesfor(x ,x )∈R(T−n)d×X ×···×X.
1:T−n T−n+1:T
(cid:124) (cid:123)(cid:122) (cid:125)
ntimes
19Proof. WefirstprovethestatementforanyT >M suchthatT ≡0 mod M byinductionasfollows.
T =2M:TheresultcanbeprovedusingLemmaB.8bysettingintheproof,u (y=x ,x=x )=π (x ),i=
i 1:M 0 a1:M 1:M
a and v (z = x ,y = x ) =
(cid:81)M
p (x |x ),j = a . We observe that the
1:M j M+1:2M 1:M m=1 aM+m M+m m:M−1+m M+1:2M
Lemmaholdsusingassumptions(c1-c2)directlyasv (z =x ,y=x )isaproductofM functionsthatsatisfies
j M+1:2M 1:M
(b)inLemmaB.8.
T =aM,a∈Z+,a>2:AssumethestatementholdsforthejointdistributionfamilywhenT =τ −M.Notethatwecan
writep (x )as
a1:τ 1:τ
M
(cid:89)
p (x )=p (x ) p (x |x ). (50)
a1:τ 1:τ a1:τ−M 1:τ−M aτ−M+m τ−M+m τ−2M+m:τ−M−1+m
m=1
Then the statement for T = τ can be proved using Lemma B.8 by setting u (y = x ,x =
i τ−2M+1:τ−M
x ) = p (x ),i = a , and v (z = x ,y = x ) =
1:τ−2M a1:τ−M 1:τ−M 1:τ−M j τ−M+1:τ τ−2M+1:τ−M
(cid:81)M
p (x |x ),j = a . Note that the family spanned with
m=1 aτ−M+m τ−M+m τ−2M+m:τ−M−1+m τ−M+1:τ
p (x ),i = a satisfies (a1) from (c1), and (a3) from the induction hypothesis. For the family
a1:τ−M 1:τ−M 1:τ−M
(cid:81)M
p (x |x ),j = a , we have a product of M functions where (b1) and
m=1 aτ−M+m τ−M+m τ−2M+m:τ−M−1+m τ−M+1:τ
(b4-b6)aresatisfiedfrom(c2),whichimply(b)inLemmaB.8.
GiventheaboveresultweproceedtoprovethecasewhereT ̸≡0( mod M).
T ̸≡0 mod M:LetnbetheremainderofT/M:T ≡n mod M.Fromthepreviousresultweknowthestatementholds
forT −n,i.e.whenT isamultipleofM.Wecanwritep (x )asfollows
a1:T 1:T
n
(cid:89)
p (x )=p (x ) p (x |x ). (51)
a1:T 1:T a1:T−n 1:T−n aT−n+m T−n+m T−n+m−M:T−n+m−1
m=1
We can again prove the statement with Lemma B.8 by setting u (y = x ,x =
i T−n−M+1:T−n
x ) = p (x ),i = a , and v (z = x ,y = x ) =
(cid:81)1 n:T−n p−M
(x
a1:T− |n
x
1:T−n 1: )T ,− jn
= a
j
. The
famT i− lyn+ s1 p:T
anned with
pT−n−M (x+1:T−n
),i =
m=1 aT−n+m T−n+m T−n+m−M:T−n+m−1 τ−n+1:τ a1:T−n 1:T−n
a satisfies(a1)from(c1).(a3)issatisfiedasfollows:
1:T−n
• ForM <T <2M,wehaveT −n=M and(a3)holdsdirectlyfrom(c1).
• ForT >2M,wehaveT −n≡0 mod M and(a3)holdsfromthepreviousinductiontechnique.
Thefamily(cid:81)n
p (x |x )isaproductofnfunctionswhichsatisfy(b1)and(b4-b6)
m=1 aT−n+m T−n+m T−n+m−M:T−n+m−1
from(c2).Thisimplies(b)inLemmaB.8.
B.3 LINEARINDEPENDENCEUNDERASSUMPTIONS(I)AND(II)
Belowweexploretheparametricconditionsinwhichlinearindependenceofthejointdistribution(andthus,identifiability)
canbeachieved.WestartfromthefollowingresultfromBalsells-Rodasetal.[2024].
PropositionB.10. FunctionsinG arelinearlyindependentonvariables(x ,...,x )iftheuniqueindexingassumption
A t t−M
(Eq.(7))holds.
ThetransitionandinitialGaussiandistributionfamiliesdefinedinEqs.(6)and(8)respectivelyalignwithassumptions(a)
and(b)asfollows.
PropositionB.11. TheconditionalGaussiandistributionfamilyG (Eq.(6)),undertheuniqueindexingassumption(Eq.(7)),
A
satisfiesassumptions(b1)and(b5)inLemmasB.6,B.7,andB.8,ifwedefineV := G ,z := x and(y ,...,y ) :=
J A t M 1
(x ,...,x ).
t−1 t−M
PropositionB.12. TheinitialGaussiandistributionfamilyI (Eq.(8),undertheuniqueindexingassumption(Eq.(9),
A
satisfiesassumptions(a1),(a3)inLemmaB.8,ifwedefineU :=I ,y:=x andx=X =∅.
I A 1
20To see why (b5) holds under nonlinear Gaussians, we note from Prop. B.10 that linear dependence occurs only if the
uniqueindexingassumptiondoesnothold.Therefore,wecanfixasubsetofthe(x ,...,x )variables,suchthatthe
t−M t−1
resultingmeanandcovariancefunctionsviolatetheuniqueindexingassumption,whichwouldimplylineardependenceon
theresultingfunctionfamily.Toverify(b4),werequirethefollowingzero-measureintersectionofmomentsresult.
PropositionB.13. AssumeGaussianfamilytransitionsG underuniqueindexingdefinedbyEq.(7),withzero-measure
A
intersectionofmoments:ForAsuchthat|A|<+∞,andanya,a′ ∈Awitha̸=a′,thesetX :={(x ,...,x )∈
a,a′ t−1 t−M
RdM|m(x ,...,x ,a) = m(x ,...,x ,a′),Σ(x ,...,x ,a) = Σ(x ,...,x ,a′)} has zero
t−1 t−M t−1 t−M t−1 t−M t−1 t−M
measure.Then(b4)holdsifV :=G ,z :=x ,and(y ,...,y ):=(x ,...,x ).
J A t 1 M t−1 t−M
Proof. DefineV :=G ,z :=x ,and(y ,...,y ):=(x ,...,x )from(b4).WesetY
:=RdM\(cid:83)
X ,
J A t 1 M t−1 t−M a̸=a′∈A a,a′
where we have µ(Y) = µ(×M Rd). Thus, we are guaranteed unique indexing for any Y′ ⊂ Y, and from Gaussian
m=1
identifiabilityG containslinearindependentfunctionsunderfinitemixtureson(z,y ,...,y ) ∈ (Z ×Y′) ⊂ (Rd×
A 1 M
Y).
AsimilarassumptionispresentedinBalsells-Rodasetal.[2024],whereitisassumedwithinacertainnon-zeromeasureset.
NowwerestrictittoholdinafullmeasuresetofRdM.Finally,wepresentlinearindependenceofthejointdistributionin
theparametriccase.
TheoremB.14. Definethefollowingjointdistributionfamilyunderthenon-linearGaussianmodel
(cid:40) T (cid:41)
(cid:89)
PT = p (x )=p (x ) p (x |x ,...,x ), p ∈IM, p ∈GM, t=2,...,T ,
A a1:T 1:T a1:M 1:M at t t−1 t−M a1:M A at A
t=M+1
(52)
withGM,IM definedbyEqs.(6),(8)respectively.Assume:
A A
(d1) UniqueindexingforG andI :Eqs.(7), (9)hold;
A A
(d2) ThefunctionsinG arecontinuouswithrespectto(x ,...,x )∈RdM;
A t−1 M
(d3) Zero-measure intersection of moments: For A such that |A| < +∞, and any a,a′ ∈ A with a ̸= a′, the set
X :={(x ,...,x )∈RdM|m(x ,...,x ,a)=m(x ,...,x ,a′),Σ(x ,...,x ,a)=
a,a′ t−1 t−M t−1 t−M t−1 t−M t−1 t−M
Σ(x ,...,x ,a′)}haszeromeasure.
t−1 t−M
Then, the joint distribution family contains linearly independent distributions under finite mixtures for
(x ,x ,...,x )∈R(T−n)d×(×n Rd),wheren=M ifT mod M =0,orn=T mod notherwise.
1:T−n T−n+1 T m=1
Proof. Notethat(d2)and(b6)areequivalent.Assumptions(a1),(a3),(b1),and(b5)aresatisfiedduetoPropositionsB.11,
B.12.Assumption(b4)holdsduetoassumption(d3)viaPropB.13.Then,thestatementholdsbyTheoremB.9.
B.4 CONCLUSION
BelowweconcludetheproofofTheorem3.2
Proof. LinearindependenceunderfinitemixturesofthejointdistributionfamilyPM,T holdsfromTheoremB.14,andthe
A,B
statementisprovedbyB.2.Belowweclarifythealignmentfromassumptions(i-ii)to(d1-d3).(d1)and(i)areequivalent.
(d2)issatisfiedasanalyticfunctionsareC∞,andthereforecontinous.FromMityagin[2015],weknowthezerosetofan
analyticfunctioniszero-measured.Therefore,intersectionsbetweenonmeansorcovariancesforanya,a′ ∈A,witha̸=a′
arealsozero-measured,whichimplies(d3).
21C PROOFOFCOROLLARY3.4
Proof. AssumetwoMSMsp(x ),p˜(x )withcorrespondingregime-dependentgraphsG ,G˜ .Givenassumptions
1:T 1:T 1:K 1:K˜
(i-ii)andTheorem3.2,wehaveidentifiabilityuptopermutationsfromDef.3.1.Wefocusoncondition4whereforeach
1≤i≤C,thereissome1≤j ≤C˜ suchthat:
p (x |x )=p (x |x ), ∀(x ,...,x )∈Rd(M+1). (53)
bi
t
t t−1:t−M ˜bj
t
t t−1:t−M t t−M
Then, at each time step t there exists a permutation σ such that for each k ∈ {1,...,K}, p(x |x ,s = k) =
t t−1:t−M t
p˜(x |x ,s = σ(k)), ∀(x ,...,x ) ∈ Rd(M+1).Weknowfromcondition2thatσ isconstantfort > M.
t t−1:t−M t t t−M
SinceweknowthedistributionsareGaussian,fromGaussianidentifiability[YakowitzandSpragins,1968]wehave
m(x ,...,x ,k)=m˜(x ,...,x ,σ(k)) Σ(k)=Σ˜(σ(k)), k ∈{1,...,K}. (54)
t t−M t t−M
Now,giventhatthefunctionsm(·,k),k ∈ B areanalytic,theJacobianwillpreservesimilarpermutationequivalences.
Wlog.wefixτ,where1≤τ ≤M andcomputetheJacobianw.r.t.x ,denotedasJ .Wewillhavethefollowing
t−τ m(·,k),τ
equivalence
 
∂m(1)(xt−1,...,xt−M,k)
...
∂m(1)(xt−1,...,xt−M,k)

∂x( t1 −)
τ
∂x( td −)
τ 
 . 
J
m(·,k),τ
= .
.
=
 
∂m(d)(xt−1,...,xt−M,k)
...
∂m(d)(xt−1,...,xt−M,k)
∂x(1) ∂x(d)
t−τ t−τ
 
∂m˜(1)(xt−1,...,xt−M,σ(k))
...
∂m˜(1)(xt−1,...,xt−M,σ(k))

∂x( t1 −)
τ
∂x( td −)
τ 
 . 
 . . =J m(·,σ(k)),τ (55)
 
∂m˜(d)(xt−1,...,xt−M,σ(k))
...
∂m˜(d)(xt−1,...,xt−M,σ(k))
∂x(1) ∂x(d)
t−τ t−τ
wherem(i)(x ,...,x ,k)denotesthei-thdimensionofm(x ,...,x ,k).Giventheminimalityassumption
t−1 t−M t−1 t−M
(iii), for each k ∈ {1,...,K}, and 1 ≤ τ ≤ M, we have x(i) ∈ Pa(j)(τ) if ∂m(i)(xt−1,...,xt−M,k) ̸= 0. This implies
k ∂x(j)
t−τ
we can retrieve G and G˜ using the Jacobians; and from the above equation we have G = G˜ , for each
1:K 1:K˜ k σ(k)
k ∈{1,...,K}.
D EXPERIMENTDETAILS
D.1 DATAGENERATION
Asmentionedearlier,wesampleN =10000sequencesoflengthT =200.Weassumeafirst-orderMarkovchainwithK
states.Theinitialdistributionassignsequalprobabilityacrossstatesandthetrainsitionmatrixmaintainsthesamestatewith
90%probability,ortransitionstothenextstatewithprobability10%.TheinitialGaussiancomponentsaresampledfrom
N(0,0.72I).ThecovariancematricesoftheGaussiantransitionsarefixed:0.052I.Theassumptionsweexploreusethe
followingnetworksforthemeantransitions:
• ReLU:RandomReLUnetworks;
• Non-zero:Piece-wiseanalyticfunctionswithcosineactivations,whereweforcethesamefunctionacrossstatesifthe
L normoftheconditionedtrajectoryattimet(x ,...,x )isbetween3and5;and
2 t−1 t−M
• Zero:Randomnetworkswithcosineactivations.
In the main text, we already indicate the use of locally connected networks [Zheng et al., 2018] where the network
dependenciesfollowaregime-dependentcausalstructuresampledwithacertainsparsityratio.Thenetworksconsistof
two-layerMLPswith16hiddenunits.
22D.2 TRAININGSPECIFICATIONS
AlltheexperimentsareimplementedinPytorch[Paszkeetal.,2019]andperformedonNVIDIARTX2080TiGPUs.We
useexactbatchedM-stepupdatesforthediscretedistributionparameters,andsetabatchsizeof500(allsamplesforECoG
data). The batch size is reduced when increasing the number of lags to fit GPU memory requirements. We use Adam
optimiser[KingmaandBa,2015]withlearningrate7·10−3,andtrainforamaximumof100epochs(1000forECoG).We
decreasethelearningratebyafactorof0.5whenlikelihoodplateausupto2times.Similartorelatedapproaches[Hälväand
Hyvarinen,2020],weuserandomrestartstoachievebetterparameterestimates.Theestimatedtransitionmeansfollowthe
ground-truthstructureonthesyntheticexperiments,andweuse2-layerMLPswith32hiddenunitsontheECoGdata.We
setthecovariancematricesindependentofthevariables.
D.3 EVALUATION
L2 Distance Let d(f,g) denote the L distance of functions defined as f : Rd → Rd. We compute the L distance
2 2
approximatelyasfollows
(cid:90) (cid:113)
(cid:12)(cid:12) (cid:12)(cid:12)2
1(cid:88)I (cid:113)
(cid:12)(cid:12) (cid:0) (cid:1) (cid:0) (cid:1)(cid:12)(cid:12)2
d(f,g):= (cid:12)(cid:12)f(x)−g(x)(cid:12)(cid:12) dx≈ (cid:12)(cid:12)f x(i) −g x(i) (cid:12)(cid:12) , (56)
I
x∈Rd
i=1
where in our case, x(i) denote samples from a held-out dataset with size 1000. Given we have identifiability up to
permutations,wefirstcomputetheaverageddistancesacrossstatesK consideringallthepossiblepermutations.Then,we
selecttheonewhichgiveslowesterrorresultinginthefollowingexpression:
K
1 (cid:88)
err:= min d(m(·,i),m˜(·,k )), (57)
k=perm({1,...,K})K i
i=1
where the samples of the L metric are dM-dimensional, each of them consisting on M consecutive samples from a
2
Mth-orderMSMsequence.SinceaccountingforthepermutationhasacostofO(K!),forK >5wetakeagreedyapproach
withcostO(K2).Notethisalternativereturnsasuboptimalresultifestimationfails.
Causal structure computation Denote the Jacobian of m˜(·,k) at point (x ,...,x ) as J (x ,...,x ) As
1 M m˜(·,k) 1 M
mentioned,wecomputetheregime-dependentcausalgraphG˜ viathesholdingtheJacobianateachstatek ∈{1,...,K}.
1:K
SimilartoEq.(57),weuseM consecutivesamplesfromMSMheld-outsequences.Then,weclassifyeachgroupofsamples
tothecorrespondingstateusingtheposteriorp(s |x )toensuretheJacobiancapturestheeffectsoftheregimeofinterest.
t 1:T
Therefore,wehaveK setsofvariablesX ,k ∈ {1,...,K},eachwithsize|X | = N .Givenasequence,wecompute
k k k
theposteriordistribution,andset{x ,...,x }∈X ifk =argmaxp(s |x ).Thisyieldstothefollowingcausal
t−1 t−M k t 1:T
graphestimatorG˜ atregimek ∈{1,...,K}:
k
G˜
:=1(cid:32) 1 (cid:88)Nk (cid:12)
(cid:12)J
(cid:16) x(i),...,x(i)(cid:17)(cid:12) (cid:12)>τ(cid:33)
, (x(i),...,x(i))∈X , (58)
k N (cid:12) m˜(·,k) 1 M (cid:12) 1 M k
k
i=1
whereweuseτ =0.05inourexperiments,and1(·)denotestheindicatorfunction,whichequalsto1iftheargumentis
trueand0otherwise.Finally,wecomputetheaveragedF scoreacrosscomponentswithrespecttothegroundtruthcausal
1
graphG .
1:K
23