A New Perspective on Shampoo’s Preconditioner
DepenMorwani˚ ItaiShapira˚ NikhilVyas˚
SEAS SEAS SEAS
HarvardUniversity HarvardUniversity HarvardUniversity
dmorwani@g.harvard.edu itaishapira@g.harvard.edu nikhil@g.harvard.edu
EranMalach ShamKakade LucasJanson
KempnerInstitute KempnerInstitute DepartmentofStatistics
HarvardUniversity HarvardUniversity HarvardUniversity
emalach@g.harvard.edu sham@seas.harvard.edu ljanson@g.harvard.edu
Abstract
Shampoo,asecond-orderoptimizationalgorithmwhichusesaKroneckerproduct
preconditioner,hasrecentlygarneredincreasingattentionfromthemachinelearn-
ingcommunity. ThepreconditionerusedbyShampoocanbeviewedeitherasan
approximationoftheGauss–NewtoncomponentoftheHessianorthecovariance
matrixofthegradientsmaintainedbyAdagrad. Weprovideanexplicitandnovel
connectionbetweentheoptimalKroneckerproductapproximationofthesematri-
cesandtheapproximationmadebyShampoo. Ourconnectionhighlightsasubtle
but common misconception about Shampoo’s approximation. In particular, the
square of the approximation used by the Shampoo optimizer is equivalent to a
single step of the power iteration algorithm for computing the aforementioned
optimal Kronecker product approximation. Across a variety of datasets and ar-
chitecturesweempiricallydemonstratethatthisisclosetotheoptimalKronecker
product approximation. Additionally, for the Hessian approximation viewpoint,
we empirically study the impact of various practical tricks to make Shampoo
more computationally efficient (such as using the batch gradient and the empir-
icalFisher)onthequalityofHessianapproximation.
1 Introduction
Second-order optimization is a rich research area within deep learning that has seen multiple in-
fluential works over the past few decades. Recently, these methods have seen success in practical
largescaletrainingrunssuchasGemini1.5Flash(GeminiTeam,20024)andinacademicbench-
marks (Dahl et al., 2023). One of the primary challenges in this field arises from the substantial
memoryandcomputationaldemandsoftraditionalsecond-ordermethods,suchasAdagrad(Duchi
etal.,2011b)andNewton’smethod.Inthecontextofneuralnetworks,bothofthesemethodsrequire
storingandinvertinga|P|ˆ|P|dimensionalmatrixH (eithercovarianceofthegradientsforAda-
gradortheGauss–NewtoncomponentoftheHessianforNewton’smethod),where|P|represents
thenumberofparametersoftheneuralnetwork. Withmoderndeeplearningarchitecturescalingto
billionsofparameters,theserequirementsmakethedirectapplicationofthesemethodsimpractical.
Toaddressthisissue,variousapproacheshavebeenproposed,includingHessian-freeoptimization
(Martens et al., 2010) and efficient approximations of the matrix H (Gupta et al., 2018b; Martens
& Grosse, 2015b). These methods aim to leverage second-order information while mitigating the
computationalandmemoryoverhead.
˚Equalcontribution.RandomizedAuthorOrdering.
Preprint.Underreview.
4202
nuJ
52
]GL.sc[
1v84771.6042:viXraMNIST-2 CIFAR-5M ImageNet
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0 5 10 15 20 25 0.0 0 2000 4000 6000 8000 10000 0.0 0 10000 20000 30000 40000 50000
Steps Steps Steps
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.00 5 10 15 20 25 0.0 0 2000 4000 6000 8000 10000 0.00 10000 20000 30000 40000 50000
Steps Steps Steps
Optimal Kronecker Shampoo2 Shampoo
Figure 1: Top: Cosine similarity between different approximations of the Gauss–Newton (GN)
componentoftheHessiananditstruevaluefordifferentdatasetsandarchitectures.Bottom:Similar
plotshowingthecosinesimilaritybetweendifferentapproximationsoftheAdagradpreconditioner
matrixand its truevalue. Ascan beseen, Shampoo2 tracks theoptimal Kroneckerapproximation
much more closely than Shampoo does. MNIST-2 refers to a binary subsampled MNIST dataset.
Formoredetailsaboutdatasetsandarchitectures,pleaserefertoAppendixB.
The class of methods for efficiently approximating the matrix H predominantly involve either a
diagonaloralayer-wiseKroneckerproductapproximationofH.Thesechoicesaremotivatedbythe
factthat,comparedtomaintainingthematrixH,bothdiagonalandlayer-wiseKroneckerproducts
are significantly more memory-efficient to store and computationally efficient to invert. Two of
themostwell-knownmethodsthatutilizealayer-wiseKroneckerproductapproximationofH are
K-FAC(Martens&Grosse,2015b)andShampoo(Guptaetal.,2018b).
Inthiswork,weprimarilyfocusontheShampoooptimizer(Guptaetal.,2018b),whichhasrecently
gainedincreasingattentionfromtheresearchcommunity. Notably, inarecentbenchmarkofopti-
mization algorithms proposed for practical neural network training workloads (Dahl et al., 2023),
Shampoo appears to outperform all other existing methods. Another recent study, elucidating the
Google Adsrecommendation searchpipeline, revealed that the GoogleAds CTRmodel is trained
usingtheShampoooptimizer(Aniletal.,2022).Additionally,arecentwork(Shietal.,2023)imple-
mentedadistributeddataparallelversionofShampoo,demonstratingitssuperiorspeedintraining
ImageNetcomparedtoothermethods.
Previously, Shampoo’s approximation was shown to be an upper bound (in spectral norm) on the
matrix H (Gupta et al., 2018b). In this work, we make this connection much more precise. Prior
researchhasestablishedthenotionoftheoptimalKroneckerproductapproximation(inFrobenius
norm) of H (Koroko et al., 2023b), which can be obtained numerically using a power iteration
scheme. Theprimarycontributionofthisworkistotheoreticallyandempiricallydemonstratethat
the square of the approximation used by Shampoo is nearly equivalent to the optimal Kronecker
factoredapproximationofH.
Themaincontributionsoftheworkaresummarizedbelow:
• Wetheoreticallyshow(Proposition1)thatthesquareoftheShampoo’sapproximationof
H ispreciselyequaltooneroundofthepoweriterationschemeforobtainingtheoptimal
KroneckerfactoredapproximationofthematrixH. Informally,foranycovariancematrix
2
notweN–ssuaG
dargadA
ytiralimiS
enisoC
ytiralimiS
enisoCH “ ErggTswhereg P Rmn 2,wearguethattherightKroneckerproductapproximation
of H is ErGGJsbErGJGs while Shampoo proposes ErGGJs1{2 bErGJGs1{2, with
GPRmˆnrepresentingareshapedgintoamatrixofsizemˆn.
• Weempiricallyestablishthattheresultofoneroundofpoweriterationisveryclosetothe
optimalKroneckerfactoredapproximation(seeFigure1),andprovidetheoreticaljustifica-
tionforthesame.
• For the Hessian based viewpoint of Shampoo (Section 2.1.2), we empirically demon-
strate the impact on the Hessian approximation of various practical tricks implemented
to make Shampoo more computationally efficient such as averaging gradients over batch
(Section4.1)andusingempiricalFisherinsteadoftheactualFisher(Section4.2).
Remark. Previousworks(Ballesetal.,2020;Linetal.,2024)haveexploredthequestionofwhy
Adagrad-based approaches like Adam and Shampoo have an extra square root compared to the
Hessian inverse in their update. This alternative question is orthogonal to our contribution. For
details,referAppendixF.
Paperorganization. InSection2,wecoverthetechnicalbackgroundnecessaryforunderstanding
thiswork. InSection3,weprovideageneralpoweriterationschemeforobtainingtheoptimalKro-
neckerproductapproximationofthematrixH,andestablishthetheconnectionbetweenShampoo’s
approximation and the optimal Kronecker product approximation of H. In Section 4, we explore
theHessianapproximationviewpointofShampooandempiricallystudyhowvariouspracticaltricks
tomakeShampoomorecomputationallyefficientimpactthequalityoftheHessianapproximation.
In Section 5, we cover closely related works and conclude with discussing the limitations of the
workinSection6. InAppendixA,weincludeadditionalexperimentsontheViTarchitectureand
comparewiththeK-FACapproximationtotheHessian. Detailedrelatedwork,proofs,datasetand
architecturedetailshavebeendeferredtotheAppendix.
2 Technicalbackground
Weuselowercaseletterstodenotescalarsandvectors,anduppercaseletterstodenotematrices. For
asymmetricmatrixA,A ě 0(resp. A ą 0)denotesthatAispositivesemi-definite(resp. positive
definite). Similarly,forsymmetricmatricesAandB,A ě B (resp. A ą B)denotesA´B ě 0
(resp. A´B ą 0). We will use Mri,js refer tothe 0-indexed pi,jq entry ofthe matrix M. The
KroneckerproductoftwomatricesA P Rpˆq andB P Rrˆs isdenotedbyAbB P Rprˆqs. Itis
defined such that pAbBqrri`i1,sj `j1s “ Ari,jsBri1,j1s where 0 ď i ă p,0 ď j ă q,0 ď
i1 ă r,0 ď j1 ă s. VectorizationofamatrixA P Rmˆn,denotedbyvecpAq,isamn-dimensional
columnvectorobtainedbystackingthecolumnsofAontopofoneanother. Wewillusuallydenote
vecpAqbya.
FollowingisabasiclemmaaboutKroneckerproductsthatwillbeusedlater
Lemma1(Henderson&Searle(1981)). pAbBqvecpGq“vecpBGAJq.
2.1 Shampoo
TheoriginalShampoo(Guptaetal.,2018b)paperintroduceditsalgorithmasanapproximationof
anonlinelearningalgorithmAdagrad(Duchietal.,2011a). Shampoocanalsobeinterpreted(Anil
et al.,2020; Osawaet al.,2023a) asapproximating theGauss–Newton component ofthe Hessian.
BothoftheseperspectiveswillbediscussedinSection2.1.1and 2.1.2respectively. .
2.1.1 AdagradbasedperspectiveofShampoo
Adagrad: Thisisapreconditionedonlinelearningalgorithm,thatusestheaccumulatedcovariance
of the gradients as a preconditioner. Let θ
t
P Rp denote řthe parameters at time t and let g
t
P Rp
denotethegradient. ItmaintainsapreconditionerH “ T g gJ. Theupdatefortheparameter
Ada t“1 t t
forlearningrateηaregivenby
2Gauss–NewtoncomponentoftheHessiancanalsobeexpressedasacovariancematrix. Fordetails,refer
Section2.1.2
3θ “θ ´ηH´1{2g .
T`1 T Ada T
Shampoo is a preconditioned gradient method which maintains a layer-wise Kronecker product
approximationtofull-matrixAdagrad. Letthegradientforaweightmatrix3 W P Rmˆn attimet
t
begivenbyG PRmˆn. ThelemmabelowisusedtoobtaintheShampooalgorithmfromAdagrad:
t
Lemma 2 (Gupta et al. (2018b)). Assume that G ,...,G are matrices of rank at most r. Let
1 T
g “vecpG qforallt. Then,withďrepresentingtheforanyϵą0,
t t
˜ ¸ ˜ ¸
1
ÿT ÿT 1{2 ÿT 1{2
ϵI ` g gJ ď ϵI ` G GJ b ϵI ` GJG .
mn r t t m t t n t t
t“1 t“1 t“1
Basedontheabovelemma,ShampoomaintainstwopreconditionersL P Rmˆm andR P Rnˆn,
t t
which are initialized to ϵI and ϵI respectively. . The update for the preconditioners and the
m n
Shampooupdateforalearningrateηisgivenby
L “L `G GJ; R “R `GJG ; W “W ´ηL´1{4G R´1{4.
T T´1 T T T T´1 T T T`1 T T T T
ř
In Lemma 2 the matrix H “ T g gJ is approximated (ignoring ϵ and scalar factors) by the
´ Ada t“1¯ t t ´ ¯
ř 1{2 ř 1{2
the Kronecker product T G GJ b T GJG . Our main focus will be to study
t“1 t t t“1 t t
theoptimalKroneckerproductapproximationofthematrixH anditsconnectiontoShampoo’s
Ada
approximation(doneinSection3).
2.1.2 HessianbasedperspectiveofShampoo
InthissectionwedescribetheHessianapproximationviewpointofShampooexploredbyprevious
works(Aniletal.,2020;Osawaetal.,2023a)asanalternativetotheAdagradviewpointdescribed
above. Ourtheoreticalandempiricalresultsholdforbothviewpoints.
Gauss–Newton(GN)componentoftheHessian. Foradatapointpx,yq,letfpxqdenotetheoutput
ofaneuralnetworkandLpfpxq,yqrepresentthetrainingloss. LetW P Rmˆn representaweight
matrixintheneuralnetworkandD denotethetrainingdistribution. Then,forCEloss,theGauss-
Newton component of the Hessian of the loss with respect to W is given by (see Appendix D for
details)
« ff
Bf B2L Bf J “ ‰
H “ E “ E g gJ ,
GN px,yq„D BW Bf2 BW x„Dx x,s x,s
s„fpxq
where,forbrevity,fpxqdenotestheoutputdistributionoftheneuralnetworkandD representsthe
x
trainingdistributionofx(Pascanu&Bengio,2014). Theright-handsideoftheequation“isalsore‰-
ferredtointheliteratureastheFishermatrix,anditscounterpartforreallabels,E g gJ ,
px,yq„D x,y x,y
is referred to as the empirical Fisher. For brevity, goi“ng forwa‰rd, we will assume that x is drawn
fromD andrepresenttheFishermatrixasE g gJ . Similarly,whenbothxandy are
x x,s„fpxq x,s x,s
used,wewillassumetheyaredrawnfromD.
TheaimofalgorithmssuchasK-FACandShampoo(whenviewedfromtheHessianperspective)is
todoalayerwiseKroneckerproductapproximationoftheFishermatrixH .Thefollowinglemma
GN
establishestheapproximationmadebyShampoo:
Lemma3(AdaptedfromGuptaetal.(2018b);Aniletal.(2020)). AssumethatG arematrices
x,s
ofrankatmostr. Letg “vecpG q. Then,foranyϵą0,
x,s x,s
ˆ ˙ ˆ ˙
“ ‰ “ ‰ 1{2 “ ‰ 1{2
E g gJ ďr E G GJ b E GJ G . (1)
x,s x,s x,s x,s x,s x,s
x,s„fpxq x,s„fpxq x,s„fpxq
3Wewillfocusonweightsstructuredasmatricesthroughoutthispaper.
4InLemma2thematrixonthelefthandsideisequaltoH andtherighthandsiderepresentsthe
GN
H approximation made by Shampoo. However, computing this approximation at every step is
GN
expensive. So,inpractice,Shampoomakestwoadditionalapproximationsontop.
First, it replaces the per-input gradient by batch gradient, i.e, replaces E rG GJ s by
x,s„fpxq x,s x,s
E rG GJ s,whereB denotesthebatch,sistheconcatenationofs „ fpxqforallpx,yq P B
B,s B,s B,s ř
and G “ 1 G is the sampled batch gradient, with srxs representing the
B,s |B| px,yqPB,s“srxs x,s
sampledlabelcorrespondingtoxPB.
Second, it replaces sampled labels with real labels, i.e., it replaces E rG GJ s with
ř B,s B,s B,s
E rG GJs,whereG “ 1 G isthebatchgradient.
B B B B |B| px,yqPB x,y
Thus,ifG andW representthebatchgradientandweightmatrixatiterationj,andλisanexpo-
j j
nentialweightingparameter,thentheupdateofShampooisgivenby
L “λL `p1´λqG GJ; R “λR `p1´λqGJG ; W “W ´ηL´1{4G R´1{4,
j j´1 j j j j´1 j j j`1 j j j j
whereL andR representtheleftandrightpreconditionersmaintainedbyShampoo,respectively.
j j
Ourfocus(whenviewingShampoofromtheHessianperspective)willbetostudy
• The optimal Kronecker product approximation of the matrix H and its connection to
GN
Shampoo’sapproximation(doneinSection3).
• Theeffectoftheaforementionedtwoapproximationsontheapproximationquality(done
inSection4).
2.2 OptimalKroneckerproductapproximation
For Frobenius norm (or other “entry-wise” matrix norms), finding the optimal Kronecker product
approximationofamatrixH P Rmnˆmn isequivalenttofindingtheoptimalrank-oneapproxima-
tionofarearrangementofH. Wedefinetherearrangementoperatorreshapepq,appliedtoamatrix
H suchthat,
reshapepHqrmi`i1,nj`j1s“Hrmj`i,mj1`i1s,
whereti,i1u P r0,1,...,m´1s,tj,j1u P r0,1,...,n´1sandreshapepHq P Rm2ˆn2. Aproperty
ofreshapep)thatwillbeusefultousis:
H “AbB ðñ reshapepHq“abJ, (2)
whereA P Rmˆm,a “ vecpAq P Rm2,B P Rnˆn andb “ vecpBq P Rn2. Thispropertycanbe
usedtoprovethefollowingresultonoptimalKroneckerproductapproximation:
Lemma4(VanLoan&Pitsianis(1993)). LetH P Rmnˆmn beamatrixandletL P Rmˆn,R P
Rnˆm. Then, the equivalence of the Kronecker product approximation of H and the rank-one
approximationofreshapepHqisgivenby:
}H ´LbR} “}reshapepHq´vecpLqvecpRqJ} ,
F F
where}¨} denotestheFrobeniusnorm.
F
Since the optimal rank-1 approximation of a matrix is given by its singular value decomposition
(SVD),weconclude:
Corollary1. LetH P Rmnˆmn. IfthetopsingularvectorsandsingularvalueofreshapepHqare
representedbyu ,v andσ ,respectively,thenthematricesLPRmˆmandRPRnˆndefinedby
1 1 1
vecpLq“σ u , vecpRq“v ,
1 1 1
minimizetheFrobeniusnorm}H ´LbR} .
F
ObtainingSVDbypoweriteration. Poweriteration(Golub&VanLoan,1996)isawell-known
methodforestimatingthetopeigenvalueofamatrixM. Itcanalsobespecializedforobtainingthe
topsingularvectorsofamatrix. Thecorrespondingiterationsfortheleftsingularvectorℓandthe
rightsingularvectorraregivenby
ℓ ÐMr ; r ÐMJℓ , (3)
k k´1 k k´1
5wherekdenotestheiterationnumber.
Cosinesimilarity. Wewillbeusingcosinesimilaritybetweenmatricesasametricforapproxima-
tion. For two matrices M and M , this refers to TrpM MJq{p||M || ¨||M || q. A value of 1
1 2 1 2 1 F 2 F
indicatesperfectalignment,whileavalueof0indicatesorthogonality.
3 OptimalKroneckerproductapproximationandShampoo
Inthissection,wewillspecializethetheoryofSection2.2forfindingtheoptimalKroneckerprod-
uct approximation of a covariance matrix H “ E rggJs for g P Rmn. Both perspectives of
g„Dg
Shampoo described in Section 2.1 are concerned with Kronecker product approximations of H of
theformLbRwhereL P Rmˆm,R P Rnˆn,butfordifferentdistributionsD . FortheAdagrad
g
viewpoint, with D as the uniform distribution over g where 1 ď t ď T refers to the gradient at
g t
timestept,H “ H . FortheHessianviewpoint,withD asthedistributionovergradientswith
Ada g
batchsize1andwithsampledlabels,H “H (seeSection2.1.2forderivation).
GN
Since our results will hold for all distributions D , we will use ErggJs to refer to E rggJs to
g g„Dg
simplify notation. The main goal of this section will be to study the optimal Kronecker product
approximationtosuchagenericmatrixH,seeitsconnectiontoShampoo,andexperimentallyval-
idate our results for H “ H and H “ H , which are described in Section 2.1.1 and 2.1.2,
Ada GN
respectively.
Loan&Pitsianis(1993)describeanapproachtofindtheoptimalKroneckerproductapproximation
of a matrix (with respect to the Frobenius norm). Koroko et al. (2023b) use this approach to find
the optimal layer-wise Kronecker product approximation of the hessian matrix for networks with-
out weight sharing. We will now do a general analysis which would also be applicable to neural
networkswithweightsharing.
Since g P Rmn, each entry of g can be described as a tuple pi,jq P rms ˆrns. Consequently,
every entry of H can be represented by the tuple ppi,jq,pi1,j1qq. We now consider the matrix
Hˆ –reshapepHqPRm2ˆn2,whichisarearrangement(seeSection2)oftheentriesofH.
Byusingequation2wegetthat:
Hˆ “ErGbGs.
Further,byLemma4,wehavethatifLbRistheoptimalKroneckerproductapproximationofH,
thenℓrJ istheoptimalrank-1approximationofHˆ,whereℓ“vecpLqandr “vecpRq. Hence,the
problem reduces to finding the optimal rank-1 approximation of Hˆ. Applying the power iteration
scheme described in Equation 3 for estimating the top singular vectors of Hˆ and using Lemma 1
yields(wherekdenotesthekthstepofpoweriteration):
ℓ ÐHˆr “ErGbGsr “vecpErGR GJsq,
k k´1 k´1 k´1
r ÐHˆJℓ “ErGbGsJℓ “vecpErGJL Gsq.
k k´1 k´1 k´1
Reshapingvectorsonbothsidesintomatricesresultsin:
L ÐErGR GJs; R ÐErGJL Gs. (4)
k k´1 k k´1
3.1 Oneroundofpoweriteration
Our first and main approximation involves replacing the iterative power iteration scheme (Equa-
tion4)withjustasingleiteration. Thisleadstothemaincontributionofourwork:
Proposition1. Onestepofpoweriteration,startingfromtheidentity,forobtainingtheoptimalKro-
neckerproductapproximationofH ispreciselyequaltothesquareoftheShampoo’sapproximation
ofH
Proof. The initialization for the single iteration will use the identity matrix, i.e., I and I for L
m n
andR,respectively. Thus,wetransitionfromtheiterativeupdateequations:
L ÐErGR GJs; R ÐErGJL Gs,
k k´1 k k´1
6tothesimplifiedsingle-stepexpressions:
LÐErGGJs; RÐErGJGs.
With the above expression for L and R, LbR is precisely equal to the square of the Shampoo’s
approximationofH givenbytherighthandsideofEquation1.
As shown in Figure 1, for various datasets and architectures, this single step of power iteration is
veryclosetotheoptimalKroneckerproductapproximationforbothH “H (top)andH “H
GN Ada
(bottom).However,wecanseethattheupperboundproposedbytheoriginalShampoowork(Gupta
etal.,2018b)issignificantlyworse.
3.1.1 Whyinitializewiththeidentitymatrix?
MNIST-2 CIFAR-5M ImageNet
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0 5 10 15 20 25 0.0 0 2000 4000 6000 8000 10000 0.0 0 10000 20000 30000 40000 50000
Steps Steps Steps
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.00 5 10 15 20 25 0.0 0 2000 4000 6000 8000 10000 0.00 10000 20000 30000 40000 50000
Steps Steps Steps
Optimal Kronecker L R
Figure2: Comparing ?řσ1 and ?řα1σ1 forvariousdatasetsandarchitectures. Thetoprowis
σ2 α2σ2
i i i i i
forH “H GNwhilethebottomrowisforH “H Ada. TheLandRlegendsrepresent ?řα1σ α1
2σ2
for
i i i
theleftandrightsingularvectorrespectively. The“OptimalKronecker”legendrepresents ?řσ1
σ2
i i
(seeSection3.1.1). Asseen, ?řα1σ1 ismuchcloserto1ascomparedto ?řσ1 ,demonstrating
α2σ2 σ2
i i i i i
the role played by identity initialization in ensuring convergence of power iteration in one round.
SeeAppendixB.1fordetails.
ř ř
Suppose the SVD of Hˆ is given by Hˆ “ σ u vT , or equivalently, H “ σ U bV . The
i i i i i i i i
convergence of the power iteration in one step depends on the inner product of the initialization
vector with the top singular vector. Let us focus on the left side,4 i.e., the update L Ð E řrGGJs
whichasdeřscribedearlierisequivalenttostartingwiththeinřitializationI n. LetvecpI nq“ iα iv
i
iř.e. I
n
“ iα iV i. After one iteration, we obtain ℓ :“ iα iσ iu i, and correspondingly, L :“
α σ U . Weareinterestedinassessinghowcloselyℓapproximatestheleadingeigenvectoru .
i i i i 1
Thecosinesimilaritybetweenℓandu 1isgivenby ?řα1σ1 .
α2σ2
i i i
OnereasonwhythecosinesimilaritymightbelargeisthatHˆ isnearlyrank-1(σ islarge);thatis,
1
H iscloselyapproximatedbyaKroneckerproduct. AsillustratedinFigure1,thisassumptiondoes
notuniversallyhold. Instead,weproposeanalternativeexplanationforwhyasinglestepofpower
4Thediscussionfortheothersideisanalogous.
7
notweN–ssuaG
dargadA
ytiralimiS
enisoC
ytiralimiS
enisoCiterationistypicallysufficient:thecoefficientα isusuallylargerthanα foralliě2. Webeginby
1 i
providingatheoreticaljustificationforthis,followedbyempiricalevidencefromourexperiments.
We start by noting that α “ vecpI qT v “ TrpV q. Now, we will show that using the identity
i n i i
matrixasinitializationisagoodchoicesincea)showsithasthemaximaldotproductwithpossible
topcomponentsi.e.,PSDmatrices(Proposition2),andb)weexpectittohaveasmalldotproduct
withlatercomponents.
Lemma5(Loan&Pitsianis(1993)). V isaPositiveSemi-Definite(PSD)matrix.
1
SinceV isaPSDmatrixwewouldliketoinitializeourpoweriterationwithamatrixwhichisclose
1
toallPSDmatrices. Now,wewillshowthatidentityisthematrixwhichachievesthis,specifically
itmaximizestheminimumdotproductacrossthesetofPSDmatricesofunitFrobeniusnorm.
Proposition2. ConsiderthesetofPSDmatricesofunitFrobeniusnormofdimensionmdenoted
byS . Then
m
1
? I “argmax min xvecpMq,vecpM1qy.
m m MPSm M1PSm
The previous proposition argues that I maximizes the worst-case dot product with possible top
m
singularvectors. Now,wearguethatitsdotproductwithothersingularvectorsshouldbelower.
Lemma6. IfV ispositive-definite,thenV foriě2arenotPSD.
1 i
Therefore, the diagonal elements of V for i ě 2 need not be positive, and this might lead to can-
i
cellations(fori ě 2)inthetraceofV whichisequaltoα . Henceweexpectα ’sfori ě 2tobe
i i i
smallerthanα . Wenowshowexperimentstodemonstratethisinpractice. Toquantifythebenefit
1
ofα
1
usuallybeinglargerthanα
i
foriě2,wewillcompare ?řα1σ1 (forbothleftandrightsin-
α2σ2
i i i
gularvectors)and ?řσ1 . Thelattercanbeinterpretedasthecosinesimilarityifallα’swereequal
σ2
i i
orasameasureofhowcloseHˆ istobeingrank1sinceitisequaltothecosinesimilaritybetween
u 1v 1T andHˆ. Thus ?řσ1
σ2
isequaltothe“OptimalKronecker”cosinesimilarityusedinFigure1.
i i
InFigure2wetrackbothofthesequantitiesthroughtrainingandindeedobservethat ?řα1σ1 are
α2σ2
i i i
significantlycloserto1than ?řσ1
σ2
forbothH “H GN(top)andH “H Ada(bottom).
i i
3.1.2 ExactKroneckerproductstructureinH
“ ‰ “ ‰
ThepreviousdiscussionshowsthatE GGJ bE GJG isclosetotheoptimalKroneckerproduct
approximationofH.InthissectionwewillshowthatthisholdsexactlyifHisaKroneckerproduct.
Intuitively,thisholdssinceifH isaKroneckerproduct,thenHˆ isrank-1,andoneroundofpower
iterationwouldrecoverHˆ.Untilnow,wehavebeenfocusingonthedirectionoftopsingularvectors
of Hˆ, but with the assumption of Hˆ being rank 1, we can compute the explicit expression for Hˆ,
andhenceofH.
Corollary2. UndertheassumptionthatHˆ isrank-1,
` “ ‰ “ ‰˘ ` “ ‰˘
H “ E GGJ bE GJG {Tr E GGJ .
Proof. Let Hˆ “ σuvJ, i.e, H “ σU bV. Let I “ TrpUqU `R and I “ TrpVqV `R ,
m m n n
whereR andR aretheresidualmatrices. Now, afteroneroundofpoweriteration, theleftand
m n
rightestimatesprovidedbyShampooaregivenby
“ ‰ “ ‰
E GGJ “σTrpVqU, E GJG “σTrpUqV.
` “ ‰˘
Fromthis,wecanseethatTr E GGJ “σTrpUqTrpVq. Thus
` “ ‰ “ ‰˘ ` “ ‰˘
H “σU bV “ E GGJ bE GJG {Tr E GGJ .
8SinceH “Hˆ isanm2ˆ1matrixforbinomiallogisticregression,itisrank-1,sotheequalityin
GN
thecorollaryholds. Inotherwords,thesquareofShampoo’sH estimateperfectlycorrelateswith
GN
H forbinomiallogisticregression. ThisisdemonstratedinthefirstplotofFigure1.
GN ` “ ‰ “ ‰˘ ` “ ‰˘
Wenotethat E GGJ bE GJG {Tr E GGJ asanestimateofH wasalsoderivedbyRen
&Goldfarb(2021). Buttheirassumptionsweremuchstrongerthanours,specificallytheyassume
thatthegradientsfollowatensor-normaldistribution,whichimpliesthatHˆ isrank1. Instead,we
only make a second moment assumption on the gradients:“H “‰Ergg“Js is a‰n exact Kronecker
product. WealsonotethatourderivationofthedirectionE GGJ bE GJG beingclosetothe
optimalKroneckerproductapproximationholdsindependentlyofHˆ beingrank1.
3.1.3 Discussionaboutoptimization
LetusrefertoErGGJsbErGJGsbyH .AsmentionedinEquation1,theoriginalShampoopaper
1
used the approximation H used was H – ErGGJs1{2 bErGJGs1{2. In practice, when using
1{2
Shampooasanoptimizationalgorithm,thegradientstepistakeninthedirectionofH´p∇Lwhere
1{2
pistunedasahyperparameter(Aniletal.,2020;Shietal.,2023). SinceH´p “H´p{2,searching
1{2 1
over p in H´p yields the same search space as H´p. Therefore, the difference between H and
1{2 1 1
H doesnotmanifestpracticallyinoptimizationspeed,butityieldsasignificantdifferenceinour
1{2
understandingofhowShampooworks.
4 HessianApproximationofShampoo
FromtheHessianapproximationviewpoint,theprevioussectioncoversthecaseofusingbatchsize
1andsampledlabels,asdescribedinSection2.1.2. Tobeprecise,inFigure1top,weconsiderhow
well H is correlated with E rG GT sbE rGT G s, where s represents that the labels
GN x,s x,s x,s x,s x,s x,s
aresampledfromthemodel’soutputdistribution. Ontheotherhand,asdiscussedinSection2.1.2,
Shampooinpracticeisgenerallyusedwitharbitrarybatchsizesandreallabels. Wenowinvestigate
theeffectofthesetwofactorsontheHessianapproximation.
4.1 Averaginggradientsacrossthebatch
The next approximation towards Shampoo is to average the gradient across the batch, i.e., we go
from
LÐ E rG GJ s; RÐ E rGJ G s
x,s x,s x,s x,s
x,s„fpxq x,s„fpxq
to
LÐ|B| ErG GJ s; RÐ|B| ErGJ G s,
B,s B,s B,s B,s
B,s B,s
wheřre B denotes the batch, s is the concatenation of s „ fpxq for all x P B and G
B,s
“
1 G is the batch gradient, with srxs representing the sampled label correspond-
|B| xPB,s“srxs x,s
ingtoxPB.
Aspreviousworkshaveshown,thischangedoesnothaveanyeffectinexpectationduetoG being
x,s
meanzeroforallxwhenwetakeexpectationovers„fpxq(Bartlett,1953)i.e. E rG s“0.
s x,s
Lemma7(ImplicitlyinLiuetal.(2024);Osawaetal.(2023b)).
|B| ErG GJ s“ E rG GJ s.
B,s B,s x,s x,s
B,s x,s„fpxq
However,thisdoesleadtoasignificantimprovementincomputationalcomplexitybysavingupto
afactorofbatchsize.
4.2 Usingreallabelsinsteadofsampledlabels
Asourfinalapproximationwereplaceusingsampledlabelss „ fpxqtousingreallabelsy. This
approximation,denotedintheliteraturebyempiricalFisherwhenbatchsizeis1,hasbeendiscussed
atlengthbypriorworks(Osawaetal.,2023a;Kunstneretal.,2019). Themaintheoreticalargument
9MNIST-2 CIFAR-5M ImageNet
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0 5 10 15 20 25 0.0 0 2000 4000 6000 8000 10000 0.0 0 10000 20000 30000 40000 50000
Steps Steps Steps
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0 5 10 15 20 25 0.0 0 2000 4000 6000 8000 10000 0.0 0 10000 20000 30000 40000 50000
Steps Steps Steps
Optimal Kronecker Shampoo2, Sampled Labels Shampoo2, Real Labels
Figure3:CosinesimilaritybetweenapproximationsofH anditstruevalue. Firstrowisforbatch
GN
size1whilethesecondrowisforbatchsize256. Weobservedeteriorationinapproximationquality
atlargerbatchsize. Wenotethatthebatchsizedoesnotrefertothebatchsizeusedinoptimization,
ratheritreferstothebatchsizeusedforHessianapproximation.
forwhythisapproximationmayworkwellisthat, aswemovetowardsoptima, thetwoquantities
convergeinthepresenceoflabelnoise(Grosse,2021).
InFigure3(top),whenevaluatingH approximationwithbatchsize1,wesurprisinglyfindthat
GN
the approximation quality is good throughout the training. However, unlike the case of sampled
labels,theapproximationstartstodegradeatlargebatchsizesbecausethegradientswithreallabels
arenotmean0.Thelemmabelow(Grosse,2021)showshowthisestimatorchangeswithbatchsize.
ř
Lemma8(Grosse(2021)). LetB denotethebatchandG “ 1 G denotethebatch
B |B| px,yqPB x,y
gradient. Then
ˆ ˙
1 1
ErG GJs“ ErG GJ s` 1´ ErG s ErG sJ.
B B B |B|x,y x,y x,y |B| x,y x,y x,y x,y
The above lemma shows that, depending on the batch size, the estimator interpolates between
E rG GJ s (Empirical Fisher) and E rG sE rG sJ. As shown in Figure 3 (top), at
x,y x,y x,y x,y x,y x,y x,y
batchsize1,whenE rG GJsisequaltoE rG GJ s,itcloselytrackstheoptimalKronecker
B B B x,y x,y x,y
product approximation. In other words, approximating the empirical Fisher is nearly sufficient in
our experiments to recover the optimal Kronecker product approximation to H . However, with
GN
increasingbatchsize(Figure3,bottomrow),theapproximationqualitydegrades.
Wenotethatthisapproximationhasthecomputationalbenefitofnotrequiringanotherbackpropa-
gationwithsampledlabels;instead,thesecomputationscanbedonealongsideusualtraining.
5 Relatedwork
WediscusstherelatedworksindetailinAppendix E.Here,wediscusstwocloselyrelatedworks:
Ren&Goldfarb(2021)andKorokoetal.(2023a).
Ren&Goldfarb(2021)studytheHessianperspectiveofShampooandshowthat,undertheassump-
tionthatsampledgradientsfollowatensor-normaldistribution,thesquareoftheHessianestimate
of Shampoo is perfectly correlated with H . We also show the same result under much weaker
GN
conditions in Corollary 2. Moreover, in Proposition 1 we show that, in general, the square of the
10
1eziShctaB
652eziShctaB
ytiralimiS
enisoC
ytiralimiS
enisoCHessianestimateofShampooiscloselyrelatedtotheoptimalKroneckerproductapproximationof
H . WeadditionallyalsostudytheapproximationsusedbyShampootomakeitcomputationally
GN
efficient(Section4)andtheAdagradperspectiveofShampoo’spreconditioner.
Loan&Pitsianis(1993)developthetheoryofoptimalKroneckerproductapproximationofamatrix
(inFrobeniusnorm). Korokoetal.(2023a)useitforfindinglayer-wiseoptimalKroneckerproduct
approximation of H for a network without weight sharing. We extend their technique to net-
GN
workswithweight-sharing,andshowthatthesquareoftheHessianestimateofShampooisnearly
equivalenttotheoptimalKroneckerproductapproximationofH .
GN
6 Limitations
ThemaincontributionofourworkistoshowthatthesquareoftheShampoo’sapproximationofH
(whereHreferstoeitherH orH )isnearlyequivalenttotheoptimalKroneckerapproximation
Ada GN
of H. Although we verify this empirically on various datasets and provide theoretical arguments,
the gap between them depends on the problem structure. In some of our experiments with ViT
architecture(AppendixA),wefindthatthegapisrelativelylargercomparedtootherarchitectures.
Moreover,itremainsanopenquestiontounderstandtheconditions(beyondthosedescribedinK-
FAC Martens & Grosse (2015b)) under which H is expected to be close to a Kronecker product.
Again, in some of the experiments with ViTs (Appendix A), we find that the optimal Kronecker
productapproximationtoH ismuchworseascomparedtootherarchitectures.
Acknowledgements
NVandDMaresupportedbyaSimonsInvestigatorFellowship,NSFgrantDMS-2134157,DARPA
grant W911NF2010021, and DOE grant DE-SC0022199. This work has been made possible in
part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute
for the Study of Natural and Artificial Intelligence. SK and DM acknowledge funding from the
Office of Naval Research under award N00014-22-1-2377 and the National Science Foundation
Grantunderaward#IIS2229881. LJacknowledgesfundingfromtheNationalScienceFoundation
DMS-2134157.
11References
RohanAnil,VineetGupta,TomerKoren,KevinRegan,andYoramSinger.Towardspracticalsecond
orderoptimizationfordeeplearning. 2020.
RohanAnil, VineetGupta, TomerKoren, KevinRegan, andYoramSinger. Scalablesecondorder
optimizationfordeeplearning,2021.
Rohan Anil, Sandra Gadanho, Da Huang, Nijith Jacob, Zhuoshu Li, Dong Lin, Todd Phillips,
CristinaPop,KevinRegan,GilIShamir,etal.Onthefactoryfloor:Mlengineeringforindustrial-
scaleadsrecommendationmodels. arXivpreprintarXiv:2209.05310,2022.
Lukas Balles, Fabian Pedregosa, and Nicolas Le Roux. The geometry of sign gradient descent,
2020.
M. S. Bartlett. Approximate confidence intervals. Biometrika, 40(1/2):12–19, 1953. ISSN
00063444.
Minhyung Cho, Chandra Dhir, and Jaehyung Lee. Hessian-free optimization for learning deep
multidimensionalrecurrentneuralnetworks. InC.Cortes, N.Lawrence, D.Lee, M.Sugiyama,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran
Associates,Inc.,2015.
George E. Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli Shama Sastry,
Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan
Bae, Justin Gilmer, Abel L. Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan,
DanielSnider,EhsanAmid,KongtaoChen,ChrisJ.Maddison,RakshithVasudev,MichalBadura,
AnkushGarg,andPeterMattson. Benchmarkingneuralnetworktrainingalgorithms,2023.
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehier-
archicalimagedatabase. In2009IEEEConferenceonComputerVisionandPatternRecognition,
pp.248–255.Ieee,2009.
JohnDuchi,EladHazan,andYoramSinger. Adaptivesubgradientmethodsforonlinelearningand
stochasticoptimization. JournalofMachineLearningResearch,12(61):2121–2159,2011a.
JohnDuchi,EladHazan,andYoramSinger. Adaptivesubgradientmethodsforonlinelearningand
stochasticoptimization. JournalofMachineLearningResearch,12(61):2121–2159,2011b.
SaiSuryaDuvvuri,FnuDevvrit,RohanAnil,Cho-JuiHsieh,andInderjitSDhillon.Combiningaxes
preconditionersthroughkroneckerapproximationfordeeplearning. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024.
Runa Eschenhagen, Alexander Immer, Richard E Turner, Frank Schneider, and Philipp Hennig.
Kronecker-factored approximate curvature for modern neural network architectures. In Thirty-
seventhConferenceonNeuralInformationProcessingSystems,2023.
Kai-Xin Gao, Xiao-Lei Liu, Zheng-Hai Huang, Min Wang, Shuangling Wang, Zidong Wang,
DachuanXu,andFanYu. Eigenvalue-correctednaturalgradientbasedonanewapproximation,
2020.
KaixinGao,XiaoleiLiu,ZhenghaiHuang,MinWang,ZidongWang,DachuanXu,andFanYu. A
trace-restricted kronecker-factored approximation to natural gradient. Proceedings of the AAAI
Conference on Artificial Intelligence, 35(9):7519–7527, May 2021. doi: 10.1609/aaai.v35i9.
16921. URLhttps://ojs.aaai.org/index.php/AAAI/article/view/16921.
Jezabel R Garcia, Federica Freddi, Stathi Fotiadis, Maolin Li, Sattar Vakili, Alberto Bernac-
chia, and Guillaume Hennequin. Fisher-legendre (fishleg) optimization of deep neural net-
works. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=c9lAOPvQHS.
GoogleGeminiTeam. Gemini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokens
ofcontext. https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_
report.pdf,20024. [Online;accessed19-May-2024].
12Thomas George, Ce´sar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast
approximate natural gradient descent in a kronecker factored eigenbasis. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 31. Curran Associates,
Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/
48000647b315f6f00f913caa757a70b3-Paper.pdf.
Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University
Press,thirdedition,1996.
Roger Grosse. Adaptive gradient methods, normalization, and weight decay. https://www.cs.
toronto.edu/~rgrosse/courses/csc2541_2021/readings/L05_normalization.pdf,
2021.
Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor op-
timization. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pp. 1842–1850. PMLR, 10–15 Jul 2018a. URL https://proceedings.mlr.press/v80/
gupta18a.html.
Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor opti-
mization. InInternationalConferenceonMachineLearning,pp.1842–1850.PMLR,2018b.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778,2016.
HaroldVHendersonandShayleRSearle. Thevec-permutationmatrix, thevecoperatorandkro-
neckerproducts: Areview. Linearandmultilinearalgebra,9(4):271–288,1981.
Abdoulaye Koroko, Ani Anciaux-Sedrakian, Ibtihel Gharbia, Vale´rie Gare`s, Mounir Haddou, and
QuangHuyTran.Efficientapproximationsofthefishermatrixinneuralnetworksusingkronecker
product singular value decomposition. ESAIM: Proceedings and Surveys, 73:218–237, 2023a.
doi: 10.1051/proc/202373218. URLhttps://hal.science/hal-04266143.
Abdoulaye Koroko, Ani Anciaux-Sedrakian, Ibtihel Ben Gharbia, Vale´rie Gare`s, Mounir Haddou,
and Quang Huy Tran. Efficient approximations of the fisher matrix in neural networks using
kroneckerproductsingularvaluedecomposition. ESAIM:ProceedingsandSurveys,73:218–237,
2023b.
FrederikKunstner, PhilippHennig, andLukasBalles. Limitationsoftheempiricalfisherapproxi-
mation for natural gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche´-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_
files/paper/2019/file/46a558d97954d0692411c861cf78ef79-Paper.pdf.
YannLeCun,Le´onBottou,YoshuaBengio,andPatrickHaffner. Gradient-basedlearningappliedto
documentrecognition. ProceedingsoftheIEEE,86(11):2278–2324,1998.
Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E. Turner, and Alireza Makhzani.
Canweremovethesquare-rootinadaptivegradientmethods? asecond-orderperspective. arXiv
2402.03496,2024.
HongLiu, ZhiyuanLi, DavidLeoWrightHall, PercyLiang, andTengyuMa. Sophia: Ascalable
stochasticsecond-orderoptimizerforlanguagemodelpre-training. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024.
ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie.
Aconvnetforthe2020s. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pp.11976–11986,June2022.
C.F.VanLoanandN.Pitsianis. Approximationwithkroneckerproducts. InBartL.R.MoorMarc
S. Moonen, Gene H. Golub (ed.), Linear Algebra for Large Scale and Real-Time Applications,
pp.293–314.Springer,1993.
13JamesMartens. Deeplearningviahessian-freeoptimization. InJohannesFu¨rnkranzandThorsten
Joachims(eds.),Proceedingsofthe27thInternationalConferenceonMachineLearning(ICML-
10),June21-24,2010,Haifa,Israel,pp.735–742.Omnipress,2010. URLhttps://icml.cc/
Conferences/2010/papers/458.pdf.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approxi-
mate curvature. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
ConferenceonMachineLearning,volume37ofProceedingsofMachineLearningResearch,pp.
2408–2417,Lille,France,07–09Jul2015a.PMLR.URLhttps://proceedings.mlr.press/
v37/martens15.html.
JamesMartensandRogerGrosse.Optimizingneuralnetworkswithkronecker-factoredapproximate
curvature. InInternationalconferenceonmachinelearning,pp.2408–2417.PMLR,2015b.
James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free opti-
mization. In Proceedings of the 28th International Conference on International Conference
on Machine Learning, ICML’11, pp. 1033–1040, Madison, WI, USA, 2011. Omnipress. ISBN
9781450306195.
James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for
recurrentneuralnetworks. InInternationalConferenceonLearningRepresentations,2018. URL
https://openreview.net/forum?id=HyMTkQZAb.
JamesMartensetal. Deeplearningviahessian-freeoptimization. InIcml,volume27,pp.735–742,
2010.
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
onlinelearnersaregoodofflinegeneralizers. arXivpreprintarXiv:2010.08127,2020.
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka.
Large-scale distributed second-order optimization using kronecker-factored approximate curva-
turefordeepconvolutionalneuralnetworks. In2019IEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),pp.12351–12359,2019. doi: 10.1109/CVPR.2019.01264.
Kazuki Osawa, Satoki Ishikawa, Rio Yokota, Shigang Li, and Torsten Hoefler. ASDL: A unified
interfaceforgradientpreconditioninginpytorch. CoRR,abs/2305.04684,2023a. doi: 10.48550/
ARXIV.2305.04684. URLhttps://doi.org/10.48550/arXiv.2305.04684.
Kazuki Osawa, Satoki Ishikawa, Rio Yokota, Shigang Li, and Torsten Hoefler. Asdl: A unified
interfaceforgradientpreconditioninginpytorch,2023b.
Vardan Papyan. Measurements of three-level hierarchical structure in the outliers in the spectrum
of deepnet hessians. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the36thInternationalConferenceonMachineLearning,volume97ofProceedingsofMachine
Learning Research, pp. 5012–5021. PMLR, 09–15 Jun 2019. URL https://proceedings.
mlr.press/v97/papyan19a.html.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In Yoshua
BengioandYannLeCun(eds.),2ndInternationalConferenceonLearningRepresentations,ICLR
2014,Banff,AB,Canada,April14-16,2014,ConferenceTrackProceedings,2014. URLhttp:
//arxiv.org/abs/1301.3584.
Yi Ren and Donald Goldfarb. Tensor normal training for deep learning models. In M. Ran-
zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances
in Neural Information Processing Systems, volume 34, pp. 26040–26052. Curran Associates,
Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/
dae3312c4c6c7000a37ecfb7b0aeb0e4-Paper.pdf.
AdepuRaviSankar,YashKhasbage,RahulVigneswaran,andVineethNBalasubramanian.Adeeper
lookatthehessianeigenspectrumofdeepneuralnetworksanditsapplicationstoregularization.
Proceedings of the AAAI Conference on Artificial Intelligence, 35(11):9481–9488, May 2021.
doi: 10.1609/aaai.v35i11.17142. URLhttps://ojs.aaai.org/index.php/AAAI/article/
view/17142.
14Hao-Jun Michael Shi, Tsung-Hsien Lee, Shintaro Iwasaki, Jose Gallego-Posada, Zhijing Li,
KaushikRangadurai, DheevatsaMudigere, andMichaelRabbat. Adistributeddata-parallelpy-
torchimplementationofthedistributedshampoooptimizerfortrainingneuralnetworksat-scale,
2023.
CharlesFVanLoanandNikosPitsianis. ApproximationwithKroneckerproducts. Springer,1993.
15A Additionalexperimentalresults
MNIST-2 CIFAR-5M(ResNet) ImageNet
1.0 1.0 1.0
0.9
0.8 0.8 0.8
0.7 0.6 0.6
0.6
0.5 0.4 0.4
0.4
0.2 0.2
0.3
0.2 0 5 10 15 20 25 0.0 0 2000 4000 6000 8000 0.0 0 10000 20000 30000 40000 50000
Steps Steps Steps
Optimal Kronecker Shampoo2 Shampoo K-FAC
Figure 4: Cosine similarity between different approximations of the Gauss–Newton (GN) com-
ponent of the Hessian and its true value for different datasets and architectures. As can be seen,
Shampoo2 tracks the optimal Kronecker approximation much more closely than Shampoo. These
plots also include the K-FAC approximation, and we note that Shampoo2 always outperforms K-
FAC,thoughtheyarecloseinsomesettings.
A.1 ViTarchitecture
FFNLinearLayer1 FFNLinearLayer2 Q-KProjectionLayer
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000
Steps Steps Steps
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000
Steps Steps Steps
Optimal Kronecker Shampoo2 Shampoo K-FAC
Figure5: AnalogueofFigure1forViTarchitectureandtheCIFAR-5mdatasetfor3layersofthe
network. ForsomeofthefiguresweobserverelativelylargergapsbetweenShampoo2 andoptimal
Kroneckerapproximation.
Inthissubsection,wepresenttheresultsforaVisionTransformer(ViT)architecturetrainedonthe
CIFAR-5mdataset. Thisarchitecturefeaturesapatchsizeof4,ahiddendimensionof512,anMLP
dimensionof512,6layers,and8attentionheads.
Fortheseexperiments,weutilizethreelayersfromthefourthtransformerblock:twolayersfromthe
MLP(referredtoas’FFNLinearLayer1’and’FFNLinearLayer2’)andtheQKlayer5 (referred
toas’Q-KProjectionLayer’).
5TheQKlayerisseparatedfromtheVpartofthelayer,followingsimilardecompositionmethoddescribed
byDuvvurietal.(2024)
16
notweN–ssuaG
notweN–ssuaG
dargadA
ytiralimiS
enisoC
ytiralimiS
enisoC
ytiralimiS
enisoCB Experiments
DatasetsandArchitectures. Weconductedexperimentsonthreedatasets: MNIST(LeCunetal.,
1998),CIFAR-5M(Nakkiranetal.,2020),andImageNet(Dengetal.,2009),usinglogisticregres-
sion,ResNet18(Heetal.,2016),andConvNeXt-T(Liuetal.,2022)architectures,respectively. For
MNIST,wesubsampledtwodigits(t0,1u)andtrainedabinaryclassifier.
Table1: SummaryofExperimentalConfigurations. λdenotesweightdecayandβ indicatesmo-
1
mentum.
Dataset Architecture Optimizer BatchSize Steps lr λ β
1
MNIST LinearClassifier GD FullBatch 25 0.01 None 0
CIFAR-5M ResNet18 SGD 128 10000 .02 None .9
ImageNet ConvNeXt-T AdamW 2048 50000 3e-3 5e-3 0.9
For MNIST, we used the only layer, i.e, the first layer of the linear classifier for computing
the cosine similarities. For Resnet18 and Imagenet, we picked arbitrary layers. In particular,
for Resnet 18, we used one of the convolution layers within the first block (’layer1.1.conv1’ in
https://pytorch.org/vision/master/_modules/torchvision/models/resnet.html#
resnet18). ForImagenet,weusedthe1x1convolutionallayerwithinthe2ndblockofconvnext-
T (’stages.2.1.pwconv1’ in https://pytorch.org/vision/main/models/generated/
torchvision.models.convnext_tiny.html#torchvision.models.convnext_tiny).
Cosine similarity estimation for H . For estimating the Frobenius norm of H , we used the
GN GN
identity:
E rvJH2 vs“ E r}H v}2s“}H }2
GN GN 2 GN F
v„Np0,Idq v„Np0,Idq
Hessian-vector products with the Gauss–Newton component were performed using the Deep-
NetHessianlibraryprovidedbyPapyan(2019).
r
For estimating the cosine similarity between H and its estimator H , we used the following
GN GN
procedure:
r
1. Estimate}H } ,andcalculate}H } .
GN F GN F
2.
DefinescaledHr asSr
“
}HGN}FHr
.
GN GN Ă GN
}HGN}F
3. Cos-simpH
,Hr
q “ 1 ´
}HGN´Sr GN}2
F, where the numerator is again estimated via
GN GN 2}HGN}2
F
Hessian-vectorproducts.
r
Notethatintheaboveprocedure,wecanexactlycalculate}H } asitisgenerallyofaKronecker
GN F
productformwithbothtermsofsizemˆmornˆn,wheremˆnisthesizeofaweightmatrix.
CosinesimilarityestimationforH . Wefollowasimilarrecipeasbefore,butusingadifference
Ada ř
methodforcomputingtheproductH v. ForagiventimeT,H “ T g gJ. Thus,H v “
ř Ada Ada t“1 t t Ada
T pgJvqg . Wemaintainthisbykeepingarunningestimateofthequantityformultiplerandom
t“1 t t
vectorsvduringatrainingrun,anduseitforestimatingtheproductH v.
Ada
B.1 Figuredetails
OptimalKroneckermethod,whereverusedwascomputedwithfiveroundsofpoweriteration,start-
ingfromtheidentity. ForH “H ,theHessianapproximationsShampoo2,Shampoo,andK-FAC
GN
weredoneusingsampledlabelsandabatchsizeof1. ForH “ H andstept,weusedgradient
Ada
enocouteredduringthetrainingruninstepsďt.
K-FACwascomputedwiththe“reduce”variantfrom Eschenhagenetal.(2023).
In Figure 2, the Optimal Kronecker legend represents the cosine similarity between the optimal
KroneckerapproximationofH
GN
andH GN. Thisispreciselyequalto ?řσ1 σ2. Similarly,thelabel
i i
17L(resp. R)representsthecosinesimilaritybetweenthetopleft(resp. right)singularvectorofHˆ
GN
and the estimate obtained after one round of power iteration starting from I (resp. I ). This is
n m
preciselyequalto ?řα1σ1 .
α2σ2
i i i
In Figure 3 (top), the Hessian approximation is calculated with batch size 1, i.e, |B| “ 1 in Sec-
tion4.2. Similarly,inFigure3(bottom),|B|“256.
C Deferredproofs
Lemma6. IfV ispositive-definite,thenV foriě2arenotPSD.
1 i
Přroof. Consider two PřSD matrices M
1
and M
2
having the eigenvalue decomposition M
1
“
λ q qJ andM “ λ q qJ. Then
1i 1i 1i 2 2i 2i 2i
ÿ ` ˘
TrpM M q“ λ λ qJq 2
1 2 1i 2j 1i 2j
i,j
Thus,ifM andM haveunitfrobeniusnormandM ispositivedefinite,thenTrpM M qą0.
1 2 1 1 2
Thus, if V is positive definite, then by orthogonality of successive singular vectors, V for i ě 2
1 i
cannotbepositivesemi-definite.
Proposition2. ConsiderthesetofPSDmatricesofunitFrobeniusnormofdimensionmdenoted
byS . Then
m
1
? I “argmax min xvecpMq,vecpM1qy.
m m MPSm M1PSm
ř
Proof. ConsideřrtheeigendecompositionofanyM P S
q
givenby q i“1λ iv iv iJ. DenoteL “ ti :
λ
i
ď ?1 qu. As λ2
i
“1,therefore,|A|ě1. Consideranyj PA. Then
1
xVecpMq,Vecpv jv jJqyď ?
q
Asv isorthogonaltotheothereigenvectors. Thus,wecansee
j
1
max min xvecpMq,vecpM1qyď ?
MPSqM1PSq q
Moreover,forthematrix ?1 qI q,foranymatrixM1,
1 trpM1q
? qxI q,M1y“ ?
q
ř
řwhere trpM1q denotes the trace of the matrix M1. However, we know trpM1q “ λ
i
ě 1 as
λ2 “1. Thus
i
1 trpM1q 1
? qxI q,M1y“ ?
q
ě ?
q
Notethatthisistheonlymatrixwiththispropertyasanyothermatrixwillatleasthaveoneeigen-
valuelessthan ?1 . Thus
q
1
? qI q “ar Mg Pm Sqax Mm 1Pin SqxvecpMq,vecpM1qy
18Lemma7(ImplicitlyinLiuetal.(2024);Osawaetal.(2023b)).
|B| ErG GJ s“ E rG GJ s.
B,s B,s x,s x,s
B,s x,s„fpxq
Proof. EvaluatingG GT ,weget
B,s B,s
ÿ
1
G GT “ G GJ
B,s B,s |B|2 x,s x1,s1
x,x1PB,
s“srxs,s1“srx1s
TakingtheexpectationoversforagivenB,andbyusingE rG s“0weget
s x,s
ÿ
1 1
ErG GT s“ E rG GJ s“ E rG GJ s
s B,s B,s |B|2
x
s„fpxq x,s x,s |B|x„B,s„fpxq x,s x,s
Nowtakinganexpectationoverbatches,weget
|B| ErG GT s“ E rG GT s
B,s B,s x,s x,s
B,s x,s„fpxq
ř
Lemma8(Grosse(2021)). LetB denotethebatchandG “ 1 G denotethebatch
B |B| px,yqPB x,y
gradient. Then
ˆ ˙
1 1
ErG GJs“ ErG GJ s` 1´ ErG s ErG sJ.
B B B |B|x,y x,y x,y |B| x,y x,y x,y x,y
Proof. EvaluatingG GT,weget
B B
ÿ
1
G GT “ G GJ
B B |B|2 x,y x1,y1
px,yq,px1,y1qPB
TakingtheexpectationoverBonboththesides,weget
„ ȷ
“ ‰
1
E G GT “ |B| ErG GJ s`p|B|2´|B|q ErG s ErG sJ
B B B |B|2 x,y x,y x ˆ,y ˙ x,y x,y x,y x,y
“ ‰
1 1
ùñ E G GT “ ErG GJ s` 1´ ErG s ErG sJ
B B B |B|x,y x,y x,y |B| x,y x,y x,y x,y
D TechnicalBackgroundonHessian
Gauss–Newton(GN)componentoftheHessian. Foradatapointpx,yq,letfpxqdenotetheoutput
ofaneuralnetworkandLpfpxq,yqrepresentthetrainingloss. LetW P Rmˆn representaweight
matrixintheneuralnetworkandD denotethetrainingdistribution. Then, theHessianoftheloss
withrespecttoW isgivenby
„ ȷ « ff „ ȷ
B2L Bf B2L Bf J BL B2f
E “ E ` E .
px,yq„D
BW2
px,yq„D
BW Bf2 BW
px,yq„D
Bf BW2
The first component, for standard losses like cross-entropy (CE) and mean squared error (MSE),
is positive semi-definite and is generally known as the Gauss–Newton (GN) component (H ).
GN
PreviousworkshaveshownthatthispartcloselytrackstheoverallHessianduringneuralnetwork
training(Sankaretal.,2021),andthusmostsecond-ordermethodsapproximatetheGNcomponent.
Denoting BLpfpxq,yq byG PRmˆnandg “vecpG q,forCEloss,itcanalsobeshownthat
BW x,y x,y x,y
« ff
Bf B2L Bf J “ ‰
H “ E “ E g gJ ,
GN px,yq„D BW Bf2 BW x„Dx x,s x,s
s„fpxq
19E Relatedwork
Theliteraturerelatedtosecondorderoptimizationwithindeeplearningisveryrich,withmethods
thatcanbebroadlyclassifiedasHessian-freeandmethodsbasedonestimatingthepreconditionerH
(whichcouldrefertoeitherH orH ). Hessian-freemethods(Martens,2010)generallytendto
Ada GN
approximatethepreconditionedstep(forNewton’smethod)usingHessianvectorproducts, butdo
notmaintainanexplicitformoftheHessian. EstimatingH (Martens&Grosse,2015a;Guptaetal.,
2018a) methods maintain an explicit form of the preconditioner that could be efficiently stored as
wellasestimated.
E.1 Hessian-free
Oneoftheseminalworksrelatedtosecondorderoptimizationwithindeeplearningwastheintro-
ductionofHessian-freeoptimization(Martens,2010). Theworkdemonstratedtheeffectivenessof
usingconjugategradient(CG)forapproximatelysolvingtheNewtonsteponmultipleauto-encoder
and classifications tasks. Multiple works (Martens & Sutskever, 2011; Cho et al., 2015) have ex-
tendedthisalgorithmtootherarchitecturessuchasrecurrentnetworksandmultidimensionalneural
nets. Oneoftherecentworks(Garciaetal.,2023)alsotakesmotivationfromthislineofwork,by
approximatelyusingsinglestepCGforeveryupdate,alongwithmaintainingaclosedformforthe
inverseoftheHessian,forthesinglesteptobeeffective.
E.2 EstimatingPreconditioner
Given that it is costly to store the entire matrix H, various works have tried to estimate layer-
wise H. KFAC (Martens & Grosse, 2015a) was one of the first work, that went beyond diagonal
approximationandmadeaKroneckerproductapproximationtolayer-wiseH . Itshowedthatthis
GN
structureapproximatelycapturestheperlayerHessianforMLPs. Thisapproximationwasextended
toconvolutional(Osawaetal.,2019)andrecurrent(Martensetal.,2018)architectures. Subsequent
worksalsoimprovedtheHessianapproximation,byfurtherfixingthetrace(Gaoetal.,2021)aswell
asthediagonalestimates(Georgeetal.,2018;Gaoetal.,2020)oftheapproximation.Arecentwork
(Eschenhagenetal.,2023)alsodemonstratedthatK-FACcanbeextendedtolarge-scaletraining.
FromtheviewpointofapproximatingAdagrad(Duchietal.,2011b),Guptaetal.(2018a)introduced
Shampoo,thatalsomakesaKroneckerproductapproximationtoH . Oneofthesubsequentwork
Ada
(Ren & Goldfarb, 2021) introduced a modification of Shampoo, that was precisely estimating the
layer-wise H under certain distributional assumptions. Other works (Anil et al., 2021) intro-
GN
duced a distributed implementation of Shampoo, that has recently shown impressive performance
for training large scale networks (Shi et al., 2023). Recently, another paper (Duvvuri et al., 2024)
proposedamodificationofShampoo,empiricallyandtheoreticallydemonstratingthatthenewesti-
matorapproximatesH betterthanShampoo’sapproximation. Ourworkshowsthatthesquareof
Ada
Shampoo’sapproximationofH isnearlyequivalenttotheoptimalKroneckerapproximation.
Ada
F ComparisonwithextrasquarerootinAdagradbasedapproaches
Multiplepreviousworks(Ballesetal.,2020;Linetal.,2024)havetriedtoaddressthequestionof
whyAdagrad-basedapproacheslikeAdamandShampoo,haveanextrasquarerootintheirupdate
compared to Hessian inverse in their updates. This question is primarily concerned with the final
update to the weights being used in the optimization procedure, once we have approximated the
Hessian.
Theprimarycontributionofthisworkiscompletelyorthogonaltothisquestion. Weareaddressing
thequestionofoptimalKroneckerapproximationoftheHessian,anditsconnectiontoShampoo’s
Hessianapproximation. ThisisorthogonaltotheHessianpowerusedinthefinalupdate.
20