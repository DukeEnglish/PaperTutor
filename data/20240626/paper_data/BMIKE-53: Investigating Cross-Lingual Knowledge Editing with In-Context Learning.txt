BMIKE-53: Investigating Cross-Lingual Knowledge Editing with
In-Context Learning
ErcongNie1,2 BoShao3 ZifengDing1,2,4 MingyangWang1,2,5
HelmutSchmid1 HinrichSchütze1,2
1LMUMunich,Germany 2MunichCenterforMachineLearning,Germany
3TechnicalUniversityofMunich,Germany
4UniversityofOxford
5BoschCenterforArtificialIntelligence
nie@cis.lmu.de bo.shao@tum.de
Abstract Cross-Lingual Knowledge Editing
Edited(en)
Knowledge In which year was Darrell Spencer born? 1944
Largelanguagemodels(LLMs)possessexten- (th) เหตกุารณ์ประวตัศิาสตร์ใดที5ดาร์เรลสเปนเซอร์เกิด? ____
Test Input (In which historical event was Darrell Spencer born during?)
siveparametricknowledge,butthisknowledge
MIKE
isdifficulttoupdatewithnewinformationbe-
In what war did Jesse Orin Creech fight in? World War I
causeretrainingisveryexpensiveandinfeasi- เจสซี5 โอลนิ ครัชตอ่ส้ใูนสงครามใด สงครามโลกครัง+ท.หี น.ึง Reliability
example
bleforclosed-sourcemodels. Knowledgeedit- (In what war did Jesse Orin Creech fight? World War I)
ing(KE)hasemergedasaviablesolutionfor W เอh ิรa ์ลt ฮc คุa เกu อs รe ์d เส ยีth ชe ีว ิตd ดe ้วa ยt เรh ื5อ งo อf ะ ไE รa ?r วl ัณ H โo รo คker? tuberculosis G exe an mer pa lelity
(What did Earl Hooker die of? tuberculosis)
updatingtheknowledgeofLLMswithoutcom- Few-shot
Cross- What position does Malcolm Partridge play? forward Locality
promisingtheiroverallperformance. On-the- Lingual ใครตายใน แบทแมน ปะทะ ซเูปอร์แมน รุ่งอรุณแหง่ความยตุธิรรม?ซุปเปอร์แมน example
flyKEmethods,inspiredbyin-contextlearn- examples (Who dies in batman v superman dawn of justice? Superman)
ing (ICL), have shown great promise and al- W กาh รa เคt ล อื(a นr ไหc วh ทi าt งศe ลิc ปt ะ คd ือe สs ถาi ปg นn ิกe ขd อ ง T สo ถาo นd ีดy บัa เพy ลงิF ทi เูดr ยe ์ ทS ี(เกt ี(ยa วขt ้อi งo ?n ก? า รW กy าํเn รd ิบh ขa อm งก รL ะe แw สi นs ํา/วน P exo ar mta pb li elity
(What art movement is ToodyayFire Station's architect associated
low LLMs to be treated as black boxes. In with? Vorticism) ……
the past, KE was primarily employed in En- Edited(en)
Knowledge In which year was Darrell Spencer born? 1944
glishcontexts,whereasthepotentialforcross- (th) เหตกุารณ์ประวตัศิาสตร์ใดที5ดาร์เรลสเปนเซอร์เกิด? ____
lingual KE in current English-centric LLMs Test Input (In which historical event was Darrell Spencer born during?)
has not been fully explored. To foster more
Figure1: IllustrationofCross-LingualKnowledgeEdit-
research in this direction, we introduce the
ingandMIKE.th: Thai,en:English.
BMIKE-53 benchmark for evaluating cross-
lingualKEon53diverselanguagesacrossthree
KE task types. We also propose a gradient-
scaled up (Dai et al., 2022), and cannot be done
freeKEmethodcalledMultilingualIn-context
withclosed-sourcemodels. Inthiscontext,know-
Knowledge Editing (MIKE) and evaluate it
ledge editing (KE) has emerged as a promising
on BMIKE-53. Our evaluation focuses on
cross-lingual knowledge transfer in terms of technique for modifying the behaviour of LLMs
reliability,generality,locality,andportability, inatargetedmannerwithoutcompromisingtheir
offering valuable insights and a framework overallperformance(Zhangetal.,2024). Recent
for future research in cross-lingual KE. Our studieshavedrawninspirationfromthein-context
code and data are publicly accessible via the
learning (ICL) paradigm and explored gradient-
anonymousrepositoryathttps://anonymous.
freeKEmethodswhereLLMslearnfromprompts
4open.science/r/MIKE.
and demonstrations without parameter updating.
1 Introduction However,currentgradient-freeKEmethodshave
primarilybeenexploredinmonolingualcontexts.
Priorstudieshaveshownthatlargelanguagemod-
Incross-lingualKE(Fig.1),knowledgeedited
els (LLMs) acquire substantial knowledge in the
in a source language (e.g., English) is expected
pre-training stage and encode it in their parame-
togeneralizetoqueriesinatargetlanguage(e.g.,
ters(Minetal.,2022;Wangetal.,2023a;Zhang
Thai)withoutinterferingwithotherfacts. Ourwork
et al., 2023; Zhou et al., 2024). However, this
investigatestheefficacyofgradient-freemethods
knowledge is static and needs to be updated reg-
for cross-lingual KE. We introduce the BMIKE-
ularlytoremainrelevantandaccurate(Caoetal.,
53benchmark,whichevaluatescross-lingualKE
2021; Dhingra et al., 2022). Updating LLMs by
across 53 diverse languages and three KE task
fine-tuningthemontextswithnewknowledgebe-
types. We further propose MIKE, a Multilingual
comesincreasinglyexpensiveasthemodelsizeis
In-contextKnowledgeEditingmethod,andevalu-
∗Thefirstfourauthorscontributedequallytothiswork. ateitonBMIKE-53toshowthatthistaskcanbe
4202
nuJ
52
]LC.sc[
1v46771.6042:viXrasolved using gradient-free methods. A good KE 2.2 BenchmarkConstruction
method reliably feeds new knowledge to a LLM
Thebenchmarkconstructioninvolvestwosteps.
and enables it to correctly answer corresponding
questions(reliability). TheLLMisabletousethe UnifyingDataFormats First,weunifythefor-
new knowledge also for answering semantically mat of the three datasets to ensure consistency
similar queries (generality), and to transfer the andcompatibility. Eachunifiedentryincludesan
knowledgetorelatedcontent(portability). Finally, editedknowledgeitemandfourtestqueries. These
agoodKEmethodensuresthattheresponsetoun- queriesevaluatereliability,generality,locality,and
relatedqueriesremainsunchanged(locality)(Yao portability,respectively. Theportabilityevaluation
etal.,2023;Wangetal.,2023b). InspiredbyZheng datasets for zsRE and CounterFact were created
etal.(2023),wedevisefourtypesofcross-lingual byYaoetal.(2023). ForWDF,weextractaknowl-
in-contextlearning(ICL)demonstrations(Fig.1), edgegraphfromtheoriginalWFDdataset,andper-
eachalignedwithoneofthefourgoals,toenable form one-hop knowledge reasoning in this graph
MIKEtosimultaneouslyachieveallobjectives. toformulateportabilityqueries.
Insummary,ourcontributionsare: i)Wedevelop
ExpansiontoMultilingualDatasets Weusethe
BMIKE-53,amultilingualKEbenchmark,byuni-
Englishbasedatafromthefirststeptogeneratea
fyingtheformatofthreetasksinEnglishandauto-
multilingualbenchmarkbystructurallytranslating
maticallytranslatingtheminto52languages. ii)We
each data entry into its multilingual counterparts.
demonstratethefeasibilityofgradient-freemeth-
Followingpreviouswork(Nieetal.,2023;Wang
ods for cross-lingual KE by designing the MIKE
et al., 2023c; Beniwal et al., 2024), we use the
method. iii) We investigate experimentally how GoogleTranslateAPI1 forthemultilingualexpan-
factors such as demonstration formatting and lin-
sion. Tomaintaincomparabilitywithsimilarmulti-
guistic feature similarity influence cross-lingual
lingualknowledgedatasetslikeMLAMA(Kassner
knowledgetransferperformance.
etal.,2021)andBMLAMA(Qietal.,2023),we
optforthesamelanguageset.
Task #Training #Test Q-Len. A-Len. #Lang.
zsRE 10,000 743 9.02 2.02 3 MIKE
CounterFact 10,000 1,031 5.97 1.00 53*
WFD 4,360 784 4.71 2.55
OurproposedmethodMIKEusesin-contextlearn-
ingforknowledgeediting. Thepromptsentenceto
Table1: StatisticsofBMIKE-53. Q/A-Len.: Average
TextLengthofQuery/Answer. *: RefertoApp. A.1for the LLM includes a new fact and several demon-
moredetailsregardingthelanguagecoverage. strations that illustrate how to correctly use new
facts. TheMIKEmethodisdesignedtoaddressthe
followingtask.
2 BMIKE-53
3.1 TaskFormulation
2.1 DataSources
WearegivenasourcelanguagemodelM,aquery-
BMIKE-53isconstructedfromthreemonolingual
answerpairf = (x∗,y∗)representinganewfact,
KE datasets: zsRE (Levy et al., 2017), a Ques- s s
andasetofdemonstrationsC = {c ,··· ,c }for
tionAnswering(QA)datasetextendedandadapted 1 k
thefactf (seeSection3.2). AlsogivenisasetX∗
byDeCaoetal.(2021)and(Mitchelletal.,2021) s
whichcontainsx∗ aswellasothersourcelanguage
for KE evaluation; CounterFact (Meng et al., s
queries with identical semantics, and their trans-
2022), which updates the knowledge items with
lations X∗ = {I (x ) : x ∈ X∗} into the target
false facts; and WikiFactDiff (WFD) (Khodja t t s s s
languageusingatranslatorIt(·).
etal.,2024),whichcomprisesreal-worldchanges
Now we consider a target language query x .
t
to a knowledge base. Both Counterfact and
P (y|x ,f,C) is the probability which M as-
WFDarederivedfromWikiData(Vrandecˇic´ and M t
signs to answer y given the target query x , the
t
Krötzsch,2014),witheachentryoriginatingfrom
fact f, and the demonstrations C. P (y|x ) is
a knowledge triple. BMIKE-53 covers a diverse M t
theprobabilityassignedtoy giventhetargetquery
rangeofknowledge,spanningfromartificialtore-
x only. TheoriginalanswerofMtoqueryx is
t t
alistic. Altogether,BMIKE-53covers53languages
y = argmax P (y|x ).
acrossthreetasks(seeTab.1). Fig.4inApp.A.2 t y M t
showsexamplesfromthethreedatasets. 1https://cloud.google.com/translateGoal: ThemostlikelyanswerofMtothequery Reliability Generality Locality Portability
Task Baseline
F1 EM F1 EM F1 EM F1 EM
x givenf andC shouldbethetranslationofthe
t Prompt 45.2 11.41 44.99 11.21 22.15 1.31 30.75 0.36
factanswery∗ ifthequeryx hasthesameseman- zsRE MIKE-random 54.57 23.92 54.53 23.84 24.62 1.81 36.61 2.65
s t MIKE-search 51.1 15.65 51.04 15.62 25.48 2.24 36.27 1.7
ticsasthefactqueryx∗ and,otherwise,theanswer S M S M S M F1 EM
s
y obtainedwithoutknowledgeediting. Mathemat- Prompt 89.24 29.82 82.55 20.53 34.67 -7.02 27.18 0.02
t CounterFact MIKE-random 92.51 38.47 89.04 30.55 21.76 -18.23 29.97 0.03
ically: MIKE-search 84.75 32.4 80.48 26.12 36.75 -8.95 41.66 7.81
S M S M F1 EM F1 EM
(cid:26) It(y∗) ifx ∈ X∗ WFD MIKP Er -o ram np dt om 8 85 8. .0 21 8 2 38 5. .7 11 2 8 83 7. .5 89 3 2 34 1.6 .57 22 66 .. 82 15 00 .. 77 48 2 10 9.4 .68 0 0. .0 02 3
argmaxP (y|x ,f,C) = s t t MIKE-search 83.7 31.91 82.97 29.83 30.88 0.83 24.25 0.1
M t y otherwise
y t
Table2:Cross-lingualKEperformanceaveragingon52
M needs to understand the relationship between non-Englishlanguages. Eightdemonstrationsareused
the target language query x and the source lan- inthemainexperiment. Metricdetailsarein§3.3. Best
t
guagequeryx∗ topredictthecorrectanswer. resultsofeachtaskarebold.
s
3.2 MIKEDemonstrations
yo to the test input and the updated answer y∗
t t
DemonstrationType Eachdemonstrationccon- which takes the new fact into account, we fol-
sistsofanewfactf′ = (x′,y′)inthesourcelan- low Yao et al. (2023) and Zheng et al. (2023)
s s
guage, a query x′ in the target language and the and calculate the Perplexity Score (S) defined as
t
correctanswery′ inthetargetlanguage. Within- E([P (y∗) > P (yo)]) and the Magnitude (M)
t M t M t
context learning, the model can learn the proper definedasE(P (y∗)−P (yo)),where[·]isIver-
M t M t
relationshiptypesfromcorrespondingdemonstra- son’sbracketoperator. Ifadatasetdoesnotprovide
tions, as shown in Fig. 1. We provide four types theoriginalanswer,wecomparethepredictedan-
ofdemonstrationscoveringreliability, generality, swerswiththecorrectanswersusingtheF1score
locality,andportability. and Exact Match (EM) metrics, consistent with
priorwork(Wangetal.,2023b,c).
Demonstration Selection We explore two
demonstration selection strategies. The first ran-
4 Experiments
domlyselectsafixedsetofexamplesfromthetrain-
ing set (random). The second retrieves the most ByevaluatingtheMIKEmethodonourBMIKE-53
similarknowledgefactsfromthetrainingcorpusto benchmark, weinvestigatethreequestionsinour
constructthedemonstrations(search). Specifically, experiments. (i)Isitfeasibletoapplygradient-free
weusethepre-trainedsentenceencoder2 (Reimers ICLmethodssuchasMIKEforcross-lingualKE?
andGurevych,2019)toencodethenewfactf and (ii)HowdovariousICLdemonstrationsettingsin-
the records in the training corpus. Based on the fluencetheefficacyofcross-lingualKE?(iii)Does
cosinesimilarity,weretrievethetopksemantically thecross-lingualknowledgetransferperformance
most similar facts from the training set and rank varyacrosslanguages?
thembasedonthesimilarity.
4.1 MainResults
3.3 ExperimentalSetting Tab.2presentstheaveragecross-lingualKEresults
Weworkwiththeinstruction-tunedLLaMA3-8B across 52 target languages on three datasets. By
model. Over5%ofthepre-trainingdatasetofthis
comparingtwoMIKEsettings(randomvs. search
English-centricLLMconsistofhigh-qualitynon-
withthePromptbaseline,wefindthatincorporat-
English data. We use a zero-shot cross-lingual ingsemanticallysimilardemonstrationsenhances
prompt without demonstrations as our baseline the locality performance (e.g. 25.48 vs. 22.15
(prompt). Wesetthenumberoffew-shotdemon- and 30.88 vs. 26.25), while in terms of reliabil-
strations to 8 for both, MIKE-random and MIKE- ityandgenerality,therandomselectionyieldssu-
search. SeeApp.Bforimplementationdetails. perior KE performance. These results align with
resultsobtainedinmonolingualin-contextKEsce-
Evaluation Metrics We use two sets of met- narios(Zhengetal.,2023). Regardingportability,
ricstomeasurethecross-lingualKEperformance: MIKE-search is either on par with or superior to
If the dataset provides both, the original answer random. The lower performance wrt. reliability
and generality observed for search-based demon-
2https://huggingface.co/sentence-transformers/
all-mpnet-base-v2 strationscouldbeattributedtotheLLM’stendencyFigure2: Cross-lingualKEperformanceofasubsetoflanguageswithMIKE-random. (a)presentstheaveragescore
acrossreliability,generality,locality,andportability. (b-d)showallmetricsofeachlanguageonthreedatasets.
andtargetlanguagevectors(Tab.4),andconducta
correlationanalysis. AsindicatedinFig.3, most
linguistic feature similarities positively correlate
withthetransferperformanceinreliabilityandgen-
erality except for geographic similarity in WFD.
Conversely,thesesimilaritiesnegativelycorrelate
withtheperformanceinlocalityandportability.
Figure3: Correlationbetweenlinguisticfeaturesimilar- 5 RelatedWork
itiesandcross-lingualknowledgetransferperformance.
∗:p<0.05. Traditional KE methods are primarily gradient-
based. Theyeitherintroduceadditionalparameters
totrain–asexemplifiedbyMEND(Mitchelletal.,
tomisinterpretsemanticallysimilardemonstrations
2021)andSERAC(Mitchelletal.,2022)–orthey
asrelevantnewfacts. Therefore,itrespondswith
edit specific parameters of the original model, as
answersfromtheseexamples. Sincelocalityand
demonstrated by ROME (Meng et al., 2022) and
portability test queries require different answers
MEMIT(Mengetal.,2023). However,thesemeth-
fromthenewlyinjectedfact, theyareimmuneto
odshavehighcomputationaldemandsandarediffi-
confusionarisingfromsimilarKEexamples,and
culttoscale. Recently,gradient-freemethods,such
donottoadropinperformance.
asIKE(Zhengetal.,2023),MeLLo(Zhongetal.,
2023), and ICE (Cohen et al., 2024), have been
4.2 Cross-LingualKnowledgeTransfer
studied in KE for LLMs. Given the multilingual
Cross-LingualTransferVariance Fig.2shows
in-contextlearningcapabilitiesofEnglish-centric
the variance in cross-lingual knowledge transfer
LLMs (Lai et al., 2023; Nie et al., 2024), such
across various languages. Overall, certain lan-
asLLaMA2(Touvronetal.,2023),LLaMA3(AI,
guages,suchaskaandtaexhibitsuperiorperfor-
2024),andChatGPT,thepotentialforcross-lingual
mance,whereasotherslikelashowpoorerresults.
KE appears promising. Recent cross-lingual KE
Thedisparitiesinreliabilityandgeneralityareless
worklargelyemploysgradient-basedmethods(Xu
pronouncedthanthoseinlocalityandportability,
et al., 2023; Wang et al., 2023b; Beniwal et al.,
especiallyforCounterFactandWFD(Fig.2(c,d)).
2024; Wei et al., 2024). A notable gradient-free
work is ReMaKE (Wang et al., 2023c), a cross-
CorrelationAnalysis Toinvestigatethevariance
lingualretrieval-augmentedKEmethod. However,
incross-lingualknowledgetransfer,weassociate
theirmethodisspecificallyappliedtoaratherspe-
it with linguistic typological features. To quan-
cialKEscenario–batchedit. Inthissetting,multi-
tifythe linguisticsimilaritybetweenEnglish and
pleknowledgepieces,suchastheentireknowledge
the target languages, we use language vectors as
base,areeditedsimultaneously.
proposed by (Littell et al., 2017). Each language
can be encoded as five vectors that respectively
6 Conclusion
represent their syntactic, phonological, phonetic,
phylogenetic, and geographic features. Follow- We introduce the multilingual KE benchmark
ing Nie et al. (2023) and Ma et al. (2024), we BMIKE-53,anduseittodemonstratethepotential
calculatethecosinesimilaritiesbetweenthesource ofgradient-freeICLmethodsincross-lingualKE.Ourexperimentsrevealthatsearch-baseddemon- of the 59th Annual Meeting of the Association for
strations boost KE performance in locality and ComputationalLinguisticsandthe11thInternational
JointConferenceonNaturalLanguageProcessing
portability,whilerandomdemonstrationsenhance
(Volume1: LongPapers),pages1860–1874,Online.
reliabilityandgenerality. Moreover,weobserved
AssociationforComputationalLinguistics.
thatlinguisticsimilaritiespositivelyimpactcross-
lingual knowledge transfer performance in terms Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson,
andMorGeva.2024. EvaluatingtheRippleEffects
ofreliabilityandgenerality. Wehopethatourwork
ofKnowledgeEditinginLanguageModels. Transac-
andthisbenchmarkwillinspirefurtherresearchof
tionsoftheAssociationforComputationalLinguis-
multilingualKEinLLMs. tics,12:283–298.
Limitation DamaiDai,LiDong,YaruHao,ZhifangSui,Baobao
Chang,andFuruWei.2022. Knowledgeneuronsin
This research concentrates on the application of pretrainedtransformers. InProceedingsofthe60th
AnnualMeetingoftheAssociationforComputational
gradient-free KE methods. While this approach
Linguistics (Volume 1: Long Papers), pages 8493–
has yielded significant insights, additional explo-
8502,Dublin,Ireland.AssociationforComputational
rationofgradient-basedKEmethodologieswithin Linguistics.
the context of our multilingual KE benchmark,
NicolaDeCao,WilkerAziz,andIvanTitov.2021. Edit-
BMIKE-53,couldprovebeneficial. However,these
ingfactualknowledgeinlanguagemodels. InPro-
additional experiments are beyond the scope of ceedingsofthe2021ConferenceonEmpiricalMeth-
this work. Furthermore, our current study did ods in Natural Language Processing, pages 6491–
notspecificallyaddresstheenhancementofcross- 6506,OnlineandPuntaCana,DominicanRepublic.
AssociationforComputationalLinguistics.
lingual KE performance employing the MIKE
method. In the forthcoming research phases, our Bhuwan Dhingra, Jeremy R. Cole, Julian Martin
ambitionistodelvedeeperintooptimizingcross- Eisenschlos, Daniel Gillick, Jacob Eisenstein, and
WilliamW.Cohen.2022. Time-awarelanguagemod-
lingual KE performance. We aim to accomplish
elsastemporalknowledgebases. Transactionsofthe
this through the refinement and improvement of
AssociationforComputationalLinguistics,10:257–
theMIKEmethod,therebyfurtheradvancingour 273.
understandingofitspotentialincross-lingualKE.
Nora Kassner, Philipp Dufter, and Hinrich Schütze.
2021. Multilinguallama: Investigatingknowledgein
EthicStatement
multilingualpretrainedlanguagemodels. InProceed-
ingsofthe16thConferenceoftheEuropeanChap-
This research was conducted in accordance with
teroftheAssociationforComputationalLinguistics:
the ACM Code of Ethics. The datasets used in
MainVolume,pages3250–3258.
thispaperarepubliclyavailable. Wedonotintend
toshareanyPersonallyIdentifiableDatawiththis HichemAmmarKhodja,FrédéricBechet,QuentinBra-
bant,AlexisNasr,andGwénoléLecorvé.2024. Wik-
paper. Our project may raise awareness of these
ifactdiff: Alarge,realistic,andtemporallyadaptable
low-resource languages in the computational lin-
datasetforatomicfactualknowledgeupdateincausal
guisticscommunity. languagemodels. InProceedingsofthe2024Joint
InternationalConferenceonComputationalLinguis-
tics, Language Resources and Evaluation (LREC-
References COLING2024),pages17614–17624.
Meta AI. 2024. Introducing meta llama 3: The most VietLai,NghiaNgo,AmirPouranBenVeyseh,Hieu
capableopenlyavailablellmtodate. https://ai. Man, Franck Dernoncourt, Trung Bui, and Thien
meta.com/blog/meta-llama-3/. Nguyen.2023. ChatGPTbeyondEnglish: Towards
acomprehensiveevaluationoflargelanguagemod-
Himanshu Beniwal, Kowsik D, and Mayank Singh. els inmultilingual learning. In Findings ofthe As-
2024. Cross-lingualeditinginmultilinguallanguage sociation for Computational Linguistics: EMNLP
models. InFindingsoftheAssociationforCompu- 2023,pages13171–13189,Singapore.Association
tationalLinguistics: EACL2024,pages2078–2128, forComputationalLinguistics.
St. Julian’s, Malta. Association for Computational
Linguistics. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer.2017. Zero-shotrelationextractionvia
BoxiCao,HongyuLin,XianpeiHan,LeSun,Lingy- readingcomprehension. InProceedingsofthe21st
ongYan,MengLiao,TongXue,andJinXu.2021. Conference on Computational Natural Language
Knowledgeable or educated guess? revisiting lan- Learning(CoNLL2017),pages333–342,Vancouver,
guagemodelsasknowledgebases. InProceedings Canada.AssociationforComputationalLinguistics.PatrickLittell,DavidR.Mortensen,KeLin,Katherine NilsReimersandIrynaGurevych.2019. Sentence-bert:
Kairis,CarlisleTurner,andLoriLevin.2017. URIEL Sentenceembeddingsusingsiamesebert-networks.
andlang2vec:Representinglanguagesastypological, InProceedingsofthe2019ConferenceonEmpirical
geographical,andphylogeneticvectors. InProceed- MethodsinNaturalLanguageProcessingandthe9th
ingsofthe15thConferenceoftheEuropeanChap- InternationalJointConferenceonNaturalLanguage
teroftheAssociationforComputationalLinguistics: Processing(EMNLP-IJCNLP),pages3982–3992.
Volume2,ShortPapers,pages8–14,Valencia,Spain.
AssociationforComputationalLinguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bolei Ma, Ercong Nie, Shuzhou Yuan, Helmut
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Schmid,MichaelFärber,FraukeKreuter,andHinrich
Bhosale,DanBikel,LukasBlecher,CristianCanton
Schuetze.2024. ToPro: Token-levelpromptdecom-
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
positionforcross-lingualsequencelabelingtasks. In
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
Proceedingsofthe18thConferenceoftheEuropean
CynthiaGao,VedanujGoswami,NamanGoyal,An-
Chapter of the Association for Computational Lin-
thonyHartshorn,SagharHosseini,RuiHou,Hakan
guistics(Volume1: LongPapers),pages2685–2702,
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
St. Julian’s, Malta. Association for Computational
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Linguistics.
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
KevinMeng,DavidBau,AlexJAndonian,andYonatan
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Belinkov.2022. Locatingandeditingfactualassoci-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
ationsinGPT. InAdvancesinNeuralInformation
stein,RashiRungta,KalyanSaladi,AlanSchelten,
ProcessingSystems.
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
Kevin Meng, Arnab Sen Sharma, Alex J Andonian, nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Yonatan Belinkov, and David Bau. 2023. Mass- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
editing memory in a transformer. In The Eleventh ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
International Conference on Learning Representa- Melanie Kambadur, Sharan Narang, Aurelien Ro-
tions. driguez,RobertStojnic,SergeyEdunov,andThomas
Scialom.2023. Llama2: Openfoundationandfine-
SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe,
tunedchatmodels. Preprint,arXiv:2307.09288.
MikeLewis,HannanehHajishirzi,andLukeZettle-
moyer.2022. Rethinkingtheroleofdemonstrations:
DennyVrandecˇic´ andMarkusKrötzsch.2014. Wiki-
Whatmakesin-contextlearningwork? InProceed-
data: afreecollaborativeknowledgebase. Communi-
ingsofthe2022ConferenceonEmpiricalMethodsin
cationsoftheACM,57(10):78–85.
NaturalLanguageProcessing,pages11048–11064,
AbuDhabi,UnitedArabEmirates.Associationfor
BoshiWang,SewonMin,XiangDeng,JiamingShen,
ComputationalLinguistics.
You Wu, Luke Zettlemoyer, and Huan Sun. 2023a.
Towardsunderstandingchain-of-thoughtprompting:
EricMitchell,CharlesLin,AntoineBosselut,Chelsea
Anempiricalstudyofwhatmatters. InProceedings
Finn,andChristopherDManning.2021. Fastmodel
of the 61st Annual Meeting of the Association for
editing at scale. In International Conference on
ComputationalLinguistics(Volume1: LongPapers),
LearningRepresentations.
pages2717–2739,Toronto,Canada.Associationfor
EricMitchell,CharlesLin,AntoineBosselut,Christo- ComputationalLinguistics.
pherDManning,andChelseaFinn.2022. Memory-
basedmodeleditingatscale. InInternationalCon- JiaanWang,YunlongLiang,ZengkuiSun,YuxuanCao,
ferenceonMachineLearning,pages15817–15831. and Jiarong Xu. 2023b. Cross-lingual knowledge
PMLR. editing in large language models. arXiv preprint
arXiv:2309.08952.
ErcongNie,ShengLiang,HelmutSchmid,andHinrich
Schütze. 2023. Cross-lingual retrieval augmented
WeixuanWang,BarryHaddow,andAlexandraBirch.
promptforlow-resourcelanguages. InFindingsof
2023c. Retrieval-augmentedmultilingualknowledge
theAssociationforComputationalLinguistics: ACL
editing. arXivpreprintarXiv:2312.13040.
2023,pages8320–8340.
ZihaoWei,JingchengDeng,LiangPang,HanxingDing,
ErcongNie,ShuzhouYuan,BoleiMa,HelmutSchmid,
HuaweiShen,andXueqiCheng.2024. Mlake: Mul-
MichaelFärber,FraukeKreuter,andHinrichSchütze.
tilingualknowledgeeditingbenchmarkforlargelan-
2024. Decomposedprompting: Unveilingmultilin-
guagemodels. arXivpreprintarXiv:2404.04990.
guallinguisticstructureknowledgeinenglish-centric
largelanguagemodels. Preprint,arXiv:2402.18397.
YangXu,YutaiHou,WanxiangChe,andMinZhang.
JiruiQi,RaquelFernández,andAriannaBisazza.2023. 2023. Language anisotropic cross-lingual model
Cross-lingual consistency of factual knowledge in editing. InFindingsoftheAssociationforCompu-
multilinguallanguagemodels. InProceedingsofthe tational Linguistics: ACL 2023, pages 5554–5569,
2023ConferenceonEmpiricalMethodsinNatural Toronto,Canada.AssociationforComputationalLin-
LanguageProcessing,pages10650–10666. guistics.YunzhiYao,PengWang,BozhongTian,SiyuanCheng,
ZhouboLi,ShuminDeng,HuajunChen,andNingyu
Zhang.2023. Editinglargelanguagemodels: Prob-
lems, methods, and opportunities. In Proceedings
of the 2023 Conference on Empirical Methods in
NaturalLanguageProcessing,pages10222–10240.
Hanning Zhang, Shizhe Diao, Yong Lin, Yi R Fung,
Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji,
and Tong Zhang. 2023. R-tuning: Teaching large
languagemodelstorefuseunknownquestions. arXiv
preprintarXiv:2311.09677.
NingyuZhang,YunzhiYao,andShuminDeng.2024.
Knowledge editing for large language models. In
Proceedingsofthe2024JointInternationalConfer-
ence on Computational Linguistics, Language Re-
sourcesandEvaluation(LREC-COLING2024): Tu-
torialSummaries,pages33–41,Torino,Italia.ELRA
andICCL.
CeZheng,LeiLi,QingxiuDong,YuxuanFan,Zhiyong
Wu, Jingjing Xu, and Baobao Chang. 2023. Can
we edit factual knowledge by in-context learning?
In The 2023 Conference on Empirical Methods in
NaturalLanguageProcessing. Parameter Value
meta-llama/Meta-
Model
ZexuanZhong,ZhengxuanWu,ChristopherManning, Llama-3-8B-Instruct
ChristopherPotts,andDanqiChen.2023. MQuAKE: Temperature 0.6
Assessingknowledgeeditinginlanguagemodelsvia Top-p 0.9
multi-hop questions. In Proceedings of the 2023 Max.length 4096
Conference on Empirical Methods in Natural Lan- Num.ofdemonstration 8
guageProcessing, pages15686–15702, Singapore. Reliability,Generality,
Typeofdemonstration
AssociationforComputationalLinguistics. Locality,Portability
Proportionofdemo. 1:3:2:2
ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,
GPUType NVIDIAA100-SXM4-80GB
JiaoSun,YuningMao,XuezheMa,AviaEfrat,Ping
NumberofGPU 4
Yu,LiliYu,etal.2024. Lima: Lessismoreforalign-
Runninghours 72
ment. AdvancesinNeuralInformationProcessing
Systems,36.
Table3: ExperimentalImplementationDetails.
A BMIKE-53Details
A.1 LanguageCoverage
BMIKE-53 encompasses a total of 53 languages.
A comprehensive list of these languages can be
foundinTab.4. Additionally,thetableoutlinesthe
linguisticfeaturesimilaritiesbetweeneachtarget
languageandEnglish.
A.2 DataEntryExample
Fig.4showsthedataitemexamplesofBMIKE-53
forallthreedatasets.
B ExperimentImplementationDetails
C FullResults
Weshowthefullexperimentalresultsforallthree
tasksinTab.5(zsRE),Tab.6(CounterFact), and
Tab.7(WFD),respectively.lid language Family syn_sim pho_sim inv_sim gen_sim geo_sim
af Afrikaans Indo-European(Germanic) 84.94 81.83 69.10 50.46 86.84
ar Arabic Semitic 65.11 70.09 70.81 0.15 97.04
az Azerbaijani Turkic 52.00 81.83 67.86 0.19 96.96
be Belarusian Slavic 78.64 85.83 70.42 16.80 99.35
bg Bulgarian Slavic 85.78 85.83 68.38 13.73 99.01
bn Bengali Indo-European(Indo-Aryan) 58.36 76.30 74.38 12.71 88.96
ca Catalan Indo-European(Romance) 87.30 85.83 75.22 10.64 99.66
ceb Cebuano Austronesian 62.17 76.30 75.22 0.13 81.50
cs Czech Slavic 73.99 85.83 66.51 13.73 99.71
cy Welsh Indo-European(Celtic) 71.90 81.83 77.85 13.73 99.99
da Danish Indo-European(Germanic) 88.01 81.83 77.54 40.90 99.89
de German Indo-European(Germanic) 90.26 80.60 76.28 54.49 99.76
el Greek Indo-European(Hellenic) 78.31 95.35 64.76 15.03 98.96
es Spanish Indo-European(Romance) 82.16 85.83 63.83 9.71 99.59
et Estonian Uralic 77.35 85.83 66.94 0.23 99.45
eu Basque Isolate 62.36 85.29 56.88 3.33 99.76
fa Persian Indo-European(Iranian) 50.03 78.35 72.83 13.73 94.23
fi Finnish Uralic 71.08 87.05 70.00 0.19 99.19
fr French Indo-European(Romance) 81.18 75.28 74.09 9.71 99.93
ga Irish Indo-European(Celtic) 72.01 85.83 69.35 12.71 99.96
gl Galician Indo-European(Romance) 80.23 90.46 70.75 10.14 99.65
he Hebrew Semitic 75.15 72.55 64.37 0.13 97.16
hi Hindi Indo-European(Indo-Aryan) 61.63 78.35 70.91 12.71 91.10
hr Croatian Slavic 83.18 85.83 69.67 12.71 99.50
hu Hungarian Uralic 69.40 85.83 74.03 0.33 99.46
hy Armenian Indo-European(Satem) 63.03 69.66 68.73 19.39 97.23
id Indonesian Austronesian 72.66 90.92 75.58 0.12 79.16
it Italian Indo-European(Romance) 85.78 85.83 70.00 11.21 99.53
ja Japanese Isolate 50.03 66.77 65.40 0.19 85.65
ka Georgian Caucasian 68.50 66.93 62.93 0.19 97.09
ko Korean Isolate 55.29 74.65 70.94 0.33 86.93
la Latin Indo-European(Romance) 78.27 85.83 76.76 15.03 99.47
lt Lithuanian Indo-European(Baltic) 69.33 80.42 74.63 19.39 99.44
lv Latvian Indo-European(Baltic) 75.39 81.83 75.22 19.39 99.42
ms Malay Austronesian 70.49 90.92 72.49 0.15 80.49
nl Dutch Indo-European(Germanic) 92.43 81.83 72.24 44.51 99.96
pl Polish Slavic 78.64 85.83 65.29 15.03 99.63
pt Portuguese Indo-European(Romance) 84.24 90.46 78.68 10.14 99.68
ro Romanian Indo-European(Romance) 79.60 90.46 73.42 11.89 99.22
ru Russian Slavic 81.18 85.83 64.76 16.80 95.81
sk Slovak Slavic 82.16 85.83 70.66 15.03 99.55
sl Slovenian Slavic 80.59 85.83 75.58 15.03 99.62
sq Albanian Indo-European(Other) 79.60 87.05 72.49 33.48 99.19
sr Serbian Slavic 79.60 85.83 72.94 12.71 99.23
sv Swedish Indo-European(Germanic) 93.34 81.83 67.98 40.90 99.62
ta Tamil Dravidian 51.36 85.29 65.81 0.11 87.95
th Thai Kra-Dai 63.95 78.35 74.91 0.11 85.25
tr Turkish Turkic 50.68 81.83 66.59 0.14 98.25
uk Ukrainian Slavic 84.73 85.83 74.38 15.03 99.28
ur Urdu Indo-European(Indo-Aryan) 61.63 85.83 71.57 12.71 92.54
vi Vietnamese Austroasiatic 66.04 78.35 74.74 0.19 85.25
zh-cn Chinese Sino-Tibetan 71.08 72.55 69.73 0.33 88.42
Table4: DetailedinformationofthelanguagescoveredbyBMIKE-53. Therightfivecolumnsshowthelinguistic
featuresimilaritiesbetweenthetargetlanguageandEnglish. syn: syntax,pho: phonology,inv: phonetics,gen:
phylogenetic,geo: geographic,sim: similarity.zsRE
CounterFact
WFD
Figure4: DataItemExamplesofBMIKE-53.Table5: FullResultsonzsRE.
eRsz tpmorp
.gvA
hz
iv
ru
ku
rt
ht
at
vs
rs
qs
ls
ks
ur
or
tp
lp
ln
sm
vl
tl
al
ok
ak
aj
ti
di
yh
uh
rh
ih
eh
lg
ag
rf
if
af
ue
te
se
le
ed
ad
yc
sc
ec
ac
nb
gb
eb
za
ra
fa
02.5479.2547.9377.1593.6498.0411.5497.5769.7350.2473.0427.9364.8371.6452.3412.4435.448.0457.0446.0415.7367.1275.4425.0883.9496.2495.3449.9601.1408.0466.5556.5685.0450.2432.4480.1487.8354.5362.9313.2401.8482.3420.9304.2442.3446.5329.2425.0710.8446.4498.8394.5448.93
1f_ler
14.1104.4111.2157.5124.943.5185.472.4191.3196.956.2132.0196.9
00.056.2140.1165.9
17.1137.3116.8
04.0
00.0
92.924.0316.891.3104.4195.0238.909.0137.3163.0148.1172.4168.3113.1136.0138.983.2105.0177.0144.1109.0100.4175.1109.0117.1177.0165.9
92.936.0129.2131.41
me_ler
99.4446.1597.9315.1512.6445.0410.5425.5715.7360.2439.9309.9329.7337.5499.2423.4473.4465.0468.0455.0425.7376.1263.4416.9738.8415.2493.3461.9641.1475.0437.4520.5660.0412.2423.4493.1475.8300.5324.9362.2414.7490.3497.8351.2472.3453.5362.3463.0737.7420.5499.8395.5498.93
1f_neg
12.1129.2189.1184.5165.949.4185.476.4197.2196.9
11.2132.0165.9
00.0
17.1109.0165.9
71.1100.4116.8
31.0
00.0
92.937.7212.829.2104.4123.0296.932.0137.3132.0144.1172.4168.3171.1105.0165.952.2105.0132.0113.1177.0100.4148.1163.0175.1140.1165.9
92.909.0125.2104.41
me_neg
51.2294.3215.0285.0263.9118.6138.4235.7428.9187.7163.7171.9174.8125.2219.1224.3281.1290.1263.9184.8114.9187.927.7146.7570.1282.3280.0202.9416.2254.0265.4285.9241.0232.5112.2215.9169.6118.2179.6167.2201.3258.3233.0254.6167.9139.3198.0293.1486.3275.9175.6152.7183.91
1f_col
13.1
76.0
88.1
76.0
49.0
84.1
53.1
04.0
65.2
18.0
76.0
53.1
84.1
76.0
51.2
51.2
51.2
92.2
20.2
04.0
18.0
00.0
76.0
72.0
45.0
65.2
57.1
31.0
65.2
57.1
26.1
45.0
20.2
45.0
88.1
12.1
57.1
04.0
84.1
92.2
18.0
92.2
57.1
84.1
84.1
49.0
84.1
45.0
88.1
45.0
04.0
84.1
20.2
me_col
57.0308.6371.5275.3341.0361.6284.3338.0658.5263.8219.4244.7202.6257.7353.8257.0346.1367.9256.5240.5290.6206.4191.7299.7680.3378.9204.5227.6502.9293.7253.5352.5440.6280.4227.1368.6203.4268.0204.2280.9210.3325.0328.7220.223.0315.7124.9215.3588.3362.2327.4279.6295.52
1f_trop
63.0
80.1
04.0
45.0
04.0
04.0
31.0
72.0
04.0
04.0
04.0
04.0
76.0
00.0
04.0
45.0
72.0
45.0
31.0
72.0
00.0
31.0
04.0
18.0
00.0
04.0
04.0
76.0
04.0
04.0
04.0
31.0
72.0
72.0
04.0
72.0
04.0
31.0
72.0
31.0
31.0
04.0
72.0
72.0
76.0
04.0
04.0
04.0
04.0
72.0
76.0
72.0
04.0
me_trop
launam-EKIM
.gvA
hz
iv
ru
ku
rt
ht
at
vs
rs
qs
ls
ks
ur
or
tp
lp
ln
sm
vl
tl
al
ok
ak
aj
ti
di
yh
uh
rh
ih
eh
lg
ag
rf
if
af
ue
te
se
le
ed
ad
yc
sc
ec
ac
nb
gb
eb
za
ra
fa
75.4558.4608.9444.9565.7554.2564.1504.0866.9365.5545.2548.3595.9411.9583.2587.9471.1595.5596.6566.8430.4455.6335.9480.1877.5611.8421.5568.2781.2518.3535.8685.1773.3417.5488.6497.0544.4492.5484.5528.1519.6580.0637.2478.0596.1574.0528.1551.5773.9508.7523.8470.5508.35
1f_ler
29.3250.4315.6269.3290.4249.8288.800.1292.6112.2276.8295.7204.1287.9189.5231.1252.9130.2308.5388.5137.664.3129.2129.3363.8391.0228.0350.7205.4262.8209.1329.6228.6144.8192.6103.5224.6129.9122.1329.6250.7249.5305.7103.525.4270.9295.7257.5151.3257.2289.8109.4236.13
me_ler
35.4548.4699.9488.8597.7514.2560.2513.0844.9359.4519.2533.4506.9449.8538.1545.9439.0591.5518.6532.9403.4436.7365.9409.0871.6688.7474.4525.2772.2517.3512.8672.1709.2409.4494.6466.0520.5424.5424.5547.0520.7567.8554.3440.1548.1547.1546.1514.5723.9524.7502.8431.5560.45
1f_neg
48.3291.4350.7255.3230.5212.9292.972.1220.6104.1284.9235.8231.1287.9175.5223.0289.8136.1335.5324.6133.643.5129.2142.3375.9325.9110.0348.5296.3295.7236.1315.6288.5132.7151.6130.5269.6137.0263.1371.5250.7291.4375.8130.5267.4255.0387.6243.5110.3212.2252.9103.5203.23
me_neg
26.4219.9231.1214.2294.1235.9138.7209.0590.2274.9131.0294.0277.0234.5229.3219.4287.3277.2263.1289.0260.2285.0165.0279.9589.3279.3292.3226.1566.5278.1228.6261.4394.2259.8185.4234.2230.8135.6100.9106.4257.5263.6262.3288.7166.1285.5140.4245.3469.6299.2259.9165.9132.22
1f_col
18.1
63.3
20.2
80.1
84.1
20.2
26.1
12.1
65.2
12.1
53.1
12.1
20.2
26.1
51.2
65.2
92.2
1.3
51.2
80.1
88.1
00.0
26.1
20.2
12.1
51.2
51.2
57.1
69.2
20.2
84.1
26.1
20.2
88.1
24.2
20.2
26.1
53.1
53.1
24.2
20.2
01.3
51.2
84.1
88.1
76.0
92.2
45.0
92.2
80.1
49.0
88.1
20.2
me_col
16.6305.5432.0310.7351.7393.1337.7343.5641.9270.6350.2336.5324.1397.6442.3338.5305.7384.9334.1354.1334.1390.7174.1399.2764.9311.4336.1368.1635.4310.4350.3451.3575.0398.8265.6315.1320.7235.5209.0360.6311.0491.6399.2304.8211.6330.3246.4346.9570.3400.1434.7211.1330.43
1f_trop
56.2
45.7
01.3
88.1
77.3
63.3
31.0
53.1
04.0
89.4
69.2
77.3
20.2
58.4
53.1
57.1
92.2
76.7
88.1
88.1
80.1
84.1
12.1
31.7
89.4
84.1
65.2
88.1
26.1
85.4
38.2
53.1
80.1
45.0
76.0
26.1
80.1
12.1
38.2
36.3
65.2
36.3
51.2
12.1
5.3
20.2
65.2
57.1
58.4
89.4
80.1
80.1
17.4
me_trop
hcraes-EKIM
.gvA
hz
iv
ru
ku
rt
ht
at
vs
rs
qs
ls
ks
ur
or
tp
lp
ln
sm
vl
tl
al
ok
ak
aj
ti
di
yh
uh
rh
ih
eh
lg
ag
rf
if
af
ue
te
se
le
ed
ad
yc
sc
ec
ac
nb
gb
eb
za
ra
fa
01.1595.3583.3472.8561.3554.3462.7452.3897.1431.4510.5476.5458.4489.7464.7450.7420.9466.3411.5491.0544.1598.0334.4530.2867.9541.6440.7494.6758.6430.6405.3633.2783.7462.0537.6436.6458.4403.4490.6455.5430.5578.5495.3416.8465.5492.3449.6466.0838.4519.5573.6424.2561.54
1f_ler
56.5125.9112.5144.8123.3120.6185.488.5184.5100.4131.4160.3131.4180.184.5112.5172.4184.5116.5172.4175.1131.030.8179.6449.1216.5157.5150.1423.3184.5192.6132.7170.5112.5143.5188.5104.4149.4124.6157.5116.5143.5151.6143.5116.5143.5149.4151.6163.0148.1164.3192.6157.51
me_ler
40.1574.3581.3409.7542.3573.3451.7401.3838.1481.4539.4436.5406.4489.7463.7410.7491.946.3488.4472.0554.1536.0363.4596.1843.9511.6462.7407.6788.6430.6434.3613.2793.7429.9473.6457.6478.4419.3419.5485.5469.4588.5455.3476.8495.5452.3430.7455.0896.4517.5567.6454.2543.54
1f_neg
26.5183.9112.5171.8195.3188.5144.457.5184.5131.4172.4129.2131.4118.084.5170.5131.4184.5184.5176.4111.2131.003.8103.6400.1243.5120.6199.1423.3184.5124.6130.8170.5112.5143.5157.5104.4149.4192.6188.5184.5143.5120.6143.5157.5143.5170.5157.5132.0113.1160.3151.6157.51
me_neg
84.5250.8261.2290.4244.3210.0271.8241.1567.3256.9147.0262.2240.2233.4293.4243.5249.4274.3223.2261.1283.3204.0166.2226.0695.4254.5280.4200.1536.5277.2248.7285.5387.4299.9133.5255.2225.9163.8103.9127.5253.7295.6286.4292.9134.2230.7132.5258.4491.8205.3215.0232.1228.22
1f_col
42.2
01.3
96.2
20.2
57.1
20.2
51.2
80.1
77.3
84.1
92.2
24.2
65.2
49.0
32.3
65.2
24.2
1.3
96.2
18.0
20.2
00.0
32.3
38.2
51.2
38.2
65.2
12.1
01.3
20.2
88.1
84.1
69.2
20.2
69.2
92.2
92.2
51.2
26.1
32.3
92.2
36.3
05.3
57.1
20.2
57.1
65.2
76.0
01.3
84.1
80.1
24.2
24.2
me_col
72.6372.4401.8207.7340.5306.0376.6340.5609.9200.3390.0345.3360.1351.4446.3303.6330.7317.3327.1335.2316.2324.7122.3326.1796.7378.4366.3353.2661.4302.3323.3493.6579.2380.9273.7360.1354.7285.6251.9211.5371.0445.4368.3312.8288.4300.0266.5394.0689.0438.9333.8290.2378.33
1f_trop
07.1
60.6
53.1
12.1
45.0
57.1
00.0
80.1
49.0
80.1
57.1
53.1
57.1
04.0
12.1
49.0
12.1
80.1
49.0
80.1
18.0
04.0
51.2
84.8
24.2
24.2
09.3
13.4
04.0
18.0
51.2
63.3
53.1
45.0
84.1
76.0
12.1
49.0
12.1
88.1
12.1
26.1
96.2
45.0
76.0
45.0
24.2
49.0
80.1
05.3
57.1
18.0
71.4
me_tropTable6: FullResultsonCounterFact.
tcaFretnuoC
tpmorP
.gvA
hz
iv
ru
ku
rt
ht
at
vs
rs
qs
ls
ks
ur
or
tp
lp
ln
sm
vl
tl
al
ok
ak
aj
ti
di
yh
uh
rh
ih
eh
lg
ag
rf
if
af
ue
te
se
le
ed
ad
yc
sc
ec
ac
nb
gb
eb
za
ra
fa
.gvA
42.9891.4807.3934.2997.0917.3803.0904.3911.0971.8894.7896.0927.9821.6967.4944.5902.7881.4906.3902.7885.7867.1960.2842.2979.4843.5964.1944.2884.4889.0998.3923.3845.5944.2840.9828.2984.4819.3716.3846.5955.8821.6997.3983.4842.2901.1764.1943.5978.7994.7806.0872.1941.9842.98
slpp_ler
28.9206.885.7389.3312.5297.8166.0275.9243.4438.7204.9224.2281.5246.1346.7344.2424.0306.8473.2431.4217.2258.7198.812.0325.992.9366.9368.5265.4278.7275.8316.6211.2304.7102.3315.3308.1286.893.9101.6557.4229.4433.7461.2360.9279.9154.5353.6335.5487.8232.2141.1474.8328.92
edutingam_ler
55.2850.9716.6855.5874.1805.7744.2840.9813.0830.3820.0851.2806.0841.9818.6849.8820.0894.7802.7813.0883.1801.7805.7758.8898.0824.6823.3896.7781.8715.3888.7836.9700.7829.6777.1879.4814.0847.9603.7756.8844.2834.9846.2843.9708.3867.5669.1819.9864.1929.9747.2749.5814.0855.28
slpp_neg
35.0245.5
11.5229.3292.8125.0183.5139.3227.7293.1231.0224.6106.8117.3257.5243.8281.2290.9228.8200.6116.6162.2163.695.4270.781.6285.6245.1226.6163.9118.7263.2237.0265.1192.2244.2217.3141.646.2191.8366.6164.8247.0387.2254.8110.5180.3203.7210.3305.1282.635.6280.2235.02edutingam_neg
76.4375.9341.4319.1381.6369.9350.7320.5214.8325.1387.2328.4344.7325.1388.2327.1399.8373.6306.5340.4370.3325.8250.7314.5266.3397.5350.7373.6375.9380.6371.3356.0333.1358.3373.6372.3388.2393.2477.9327.4313.5387.2350.7395.2312.5300.5466.6323.5285.6203.2345.0433.1323.1476.43
slpp_col
20.7-24.1-92.7-46.8-06.5-31.2-12.5-54.11-26.7-50.01-45.8-31.5-89.5-40.8-88.8-95.8-64.6-10.8-84.8-05.6-38.6-55.3-12.2-46.11-26.2-37.7-85.6-47.9-19.4-87.5-15.9-36.21-80.7-63.5-15.7-06.7-01.5-05.1-40.4-77.01-69.5-06.8-89.8-39.7-40.5-12.4-88.6-66.21-82.21-58.9-73.1-87.8-46.5-20.7-
edutingam_col
81.7257.2336.0221.8200.9299.3213.2319.5564.2279.2273.0213.2248.3223.2326.4224.7214.7217.4247.0289.1234.3298.2106.9253.7652.5374.4255.1208.4597.5212.3285.6216.7332.3227.2298.3270.3288.2256.6121.0286.5253.1356.5229.4299.8132.5203.5182.2261.9471.9289.6210.3263.5272.1281.72
1f_trop
20.0
00.0
00.0
00.0
01.0
00.0
92.0
00.0
00.0
00.0
00.0
00.0
01.0
00.0
00.0
01.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
01.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
01.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
20.0
me_trop
launam-EKIM
.gvA
hz
iv
ru
ku
rt
ht
at
vs
rs
qs
ls
ks
ur
or
tp
lp
ln
sm
vl
tl
al
ok
ak
aj
ti
di
yh
uh
rh
ih
eh
lg
ag
rf
if
af
ue
te
se
le
ed
ad
yc
sc
ec
ac
nb
gb
eb
za
ra
fa
.gvA
15.2911.0916.6905.3913.3976.4967.1943.5981.4942.2950.2968.4965.1984.7915.6969.7902.0908.6909.6980.1971.1969.4931.6899.3925.9822.6973.4954.5811.0938.5969.4960.5809.6930.3804.0991.7994.7812.0896.0982.7934.9861.8982.7964.8843.5927.6707.3921.6948.8943.2926.9837.2997.3915.29
slpp_ler
74.8356.937.7489.6336.8244.9399.3285.4344.7582.2384.8446.1353.9273.5357.4575.2587.3330.0684.6564.1376.0325.9244.2158.5309.3156.5444.1571.9242.2326.8381.3488.8237.3464.3277.6305.4415.9224.8144.9240.8639.5222.4596.5695.3447.3362.1370.5455.2485.9530.5323.3208.9494.0674.83
edutingam_ler
40.9830.6813.3957.8819.9803.0971.8881.4904.0928.9879.7833.9892.7874.4904.3975.4949.5869.4904.3992.7825.9820.3975.1835.2900.7829.2972.1960.2892.7843.2989.0906.0837.2936.9732.6874.4930.6889.7779.4852.5949.5806.3942.2917.3858.1919.3770.8881.4909.6987.7852.2803.0965.1940.98
slpp_neg
55.0343.715.5396.9261.4223.7280.0251.1347.5469.7219.6347.7228.5287.9242.1458.9371.9263.5498.4495.4280.7293.4259.923.2312.1177.4350.0405.6263.7294.2304.4328.4211.2363.7151.8278.6377.0260.5140.3285.1552.0252.9329.8478.3324.7214.6275.4346.6336.7407.0322.5174.6326.6455.03edutingam_neg 67.1244.4267.0298.2280.0239.7222.5226.5120.2211.9157.7111.2251.4271.7132.8192.6153.7217.5194.6168.3297.9162.7185.9248.4152.4296.9196.9184.6252.4219.8167.3283.3279.6109.8281.3211.9123.5287.2330.8253.4183.3231.5165.0223.5273.0245.7322.5292.6115.997.9149.0335.8142.1267.12
slpp_col
32.81-84.4-01.02-26.41-73.61-08.01-53.21-24.91-41.72-35.91-89.72-00.51-18.41-89.81-66.42-06.32-43.61-18.52-35.82-32.51-04.71-63.41-83.5-10.02-37.5-09.91-97.32-58.61-45.51-79.91-02.71-61.81-02.91-45.01-90.81-92.22-05.11-11.8-82.21-32.23-03.31-35.42-87.72-11.91-55.61-81.41-97.81-56.12-25.23-31.22-28.5-05.22-37.42-32.81-
edutingam_col
79.9203.4305.2232.0398.2396.6255.5327.6549.3214.7258.3219.5236.7233.7380.8208.9276.0309.5294.2255.3271.6240.4142.1386.9630.8390.7267.3284.7587.6243.6237.6364.2430.6260.5234.6298.4206.4242.9125.1244.9210.5336.7238.5232.1291.9229.7167.6248.9406.3392.2349.4295.8241.3279.92
1f_trop
30.0
00.0
00.0
00.0
01.0
00.0
92.0
00.0
00.0
00.0
00.0
00.0
01.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
93.0
00.0
00.0
00.0
93.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
01.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
30.0
me_trop
hcraes-EKIM
.gvA
hz
iv
ru
ku
rt
ht
at
vs
rs
qs
ls
ks
ur
or
tp
lp
ln
sm
vl
tl
al
ok
ak
aj
ti
di
yh
uh
rh
ih
eh
lg
ag
rf
if
af
ue
te
se
le
ed
ad
yc
sc
ec
ac
nb
gb
eb
za
ra
fa
.gvA
57.4832.6839.2803.0913.0879.4884.4878.4868.1893.7808.3884.4885.4885.4862.5885.4805.0831.6893.7878.4878.4888.0925.6832.6896.0914.3854.5875.1869.1814.0892.7885.4817.6808.3890.4825.6886.4821.3832.6860.5883.4884.4817.3879.4815.3843.9730.3885.7857.8838.2822.3823.3883.4857.48
slpp_ler
04.2361.0333.0379.2457.5219.9281.0370.4242.4304.3387.5314.1307.0300.1368.2341.6326.7200.5315.4301.3358.3303.4316.3367.1207.9338.3359.1340.4210.1356.8254.0413.7230.5302.0320.2342.7331.3352.9291.7369.7323.9285.2356.5350.6348.8239.0330.2370.6288.8373.0344.2329.1369.7304.23
edutingam_ler
84.0839.2859.5717.6801.4746.2836.9700.4856.5723.3866.8742.9713.0813.0897.7743.9710.7736.9745.2820.0882.1819.6809.3821.3855.8828.6781.1844.9737.9774.8709.3808.0875.1829.9759.8746.2844.2812.0814.3843.9751.9736.9728.6712.0821.0841.6703.7700.4839.2866.8706.0874.8767.8784.08
slpp_neg
21.6255.4276.1294.7323.9160.3262.4230.2225.7264.7223.8255.4232.5283.5254.4262.7287.1244.5265.6253.7212.9262.7254.0332.9143.5366.3275.5227.1274.5204.3275.5377.3212.6266.4257.4219.0304.7213.5207.9265.9270.4247.5284.6221.8214.3290.5285.3250.3212.1306.6256.6279.3215.7221.62edutingam_neg 57.6340.4393.2469.6268.6347.0482.6361.7224.4425.1320.8383.9311.5352.7384.9301.2438.0417.1431.1405.5311.5326.1301.2378.9249.0315.1446.0487.9274.6383.9364.3323.8238.0425.1309.1499.8313.5328.4341.4354.3498.5323.1440.7466.6361.0481.6300.2453.7264.3327.1315.8351.7386.2457.63
slpp_col
59.8-39.8-09.2-91.02-27.8-81.4-60.11-49.31-10.4-25.41-65.01-67.6-66.9-76.9-87.5-96.5-23.6-49.3-49.3-32.21-08.11-44.01-24.11-53.01-05.41-45.5-10.5-24.31-74.8-72.6-67.41-78.41-42.6-58.21-68.4-84.9-99.9-29.9-48.21-03.4-07.9-09.2-37.1-66.9-60.5-04.9-11.5-60.41-13.21-44.41-85.8-47.7-24.4-59.8-
edutingam_col
66.1422.9387.1355.8387.8455.4396.2422.8690.6317.0467.2396.8300.9357.4557.0413.6415.2445.1480.4341.6397.9325.7136.1478.5738.3463.1421.6396.2640.8337.8338.9445.0617.3426.1385.9343.7392.2346.6294.2393.1428.7421.2480.3402.2344.1463.5217.0498.6662.2541.6415.2331.8382.9366.14
1f_trop
18.7
94.3
40.5
88.391.3134.5
87.3
05.6
08.9
42.8
40.5
42.8
38.819.5114.948.4112.9
60.1133.5
36.5
10.6
61.1
37.8
97.6
72.7
53.1198.6
03.3
27.5
13.9
07.9
12.921.2133.2
44.8
34.5
57.4
55.1
58.4
98.9
20.921.2161.4163.4
51.1189.3
14.9
51.830.5183.0159.4
05.6
12.9
18.7
me_tropTable7: FullResultsonWFD.
DFW tpmorP
.gvA
hz
iv
ru
ku
rt
ht
at
vs
rs
qs
ls
ks
ur
or
tp
lp
ln
sm
vl
tl
al
ok
ak
aj
ti
di
yh
uh
rh
ih
eh
lg
ag
rf
if
af
ue
te
se
le
ed
ad
yc
sc
ec
ac
nb
gb
eb
za
ra
fa
10.5897.6761.9810.8816.6855.3896.0952.1839.3859.4883.1888.7824.3864.5861.9893.8864.5867.7817.1917.5879.5819.6756.2812.9722.6892.9834.0931.5713.4887.8899.6801.0884.6838.8716.6899.0837.6861.3884.6893.8872.2814.9808.9817.5817.5843.9704.2850.0992.3856.2828.4860.4801.68
slpp_ler
17.8293.909.6427.3223.5204.5314.3439.5266.7381.0252.8172.7397.8277.9278.3479.5320.9134.4411.8478.2358.8225.0187.5172.6260.5130.5308.1447.1247.7209.5323.6223.2258.2281.6161.9350.3277.8121.6253.1305.4478.5280.2404.5444.6231.8102.7148.1217.2365.1250.1281.6258.2261.63
edutingam_ler
95.3846.5788.7842.7881.4860.4814.9847.0820.2855.3866.6753.6898.1876.3836.7880.5815.1879.5849.0998.1820.2871.7760.8735.6780.5810.8876.9800.5747.0853.6822.6841.2808.3820.2822.6832.0844.4867.1859.4864.5883.1899.6845.9896.4810.8832.0864.9756.8876.3821.1899.0861.3816.68
slpp_neg
76.4214.983.9379.2226.8167.3203.8302.5292.1389.7101.9120.1319.8118.4209.4311.7227.0272.8228.2424.2299.1296.6
74.867.1243.4156.1378.7374.0204.1261.8269.6247.5287.4260.7140.4323.4286.4174.3297.3255.2304.6221.8222.0423.5281.1306.6182.8181.0308.5214.8136.9145.2211.32edutingam_neg
52.6289.9295.2259.2221.6258.3221.1313.0588.2242.0241.1268.2289.9175.4233.7220.0371.4248.4205.4244.3236.3214.5117.3282.2504.5245.6240.5203.6407.6260.3214.6266.7369.4204.2277.5233.3296.1210.8103.0294.7235.7277.3284.4220.1226.2277.5197.4217.8437.6265.6214.4242.4261.52
1f_col
87.0
75.3
46.0
46.0
46.0
46.0
20.1
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
60.3
46.0
46.0
46.0
19.1
46.0
46.0
46.0
77.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
me_col
84.0265.5208.9150.3110.7182.0280.5229.2446.8196.2199.5191.8126.6178.7139.3216.1204.8171.2211.1266.2183.5148.996.7108.0591.2256.2263.0285.9368.7170.6107.6127.1366.9131.8113.2209.6131.3126.6179.2169.0280.0213.0248.0268.9178.6192.1110.3221.8320.2265.8150.6129.4128.71
1f_trop
20.0
83.0
00.0
00.0
00.0
00.0
31.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
31.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
31.0
31.0
31.0
00.0
00.0
00.0
00.0
00.0
31.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
me_trop
launam-EKIM
.gvA
hz
iv
ru
ku
rt
ht
at
vs
rs
qs
ls
ks
ur
or
tp
lp
ln
sm
vl
tl
al
ok
ak
aj
ti
di
yh
uh
rh
ih
eh
lg
ag
rf
if
af
ue
te
se
le
ed
ad
yc
sc
ec
ac
nb
gb
eb
za
ra
fa
82.8828.4865.0992.9892.9802.1974.2935.2816.6856.8839.3870.1953.6825.8849.0933.1987.8808.9873.3910.8896.0980.5845.9801.0876.9828.0953.2991.8745.9881.0909.8878.0808.9879.5892.9821.7830.9868.6854.1934.0916.6881.0989.2937.6865.0992.3893.8849.0922.6859.4872.8822.6848.19
slpp_ler
21.5337.3148.8447.6229.0353.8448.6409.9264.5403.6296.1204.1431.4377.5380.8438.3437.6212.8425.6568.5347.5300.7146.5257.6209.5125.0404.9469.5262.2345.9303.2307.3285.2355.6207.2479.1420.0316.2368.2434.7431.2336.5420.6530.2329.4381.4276.0389.4344.6211.5206.4362.4273.45
edutingam_ler
38.7824.3834.0981.0942.7849.0990.2967.1868.6809.8839.3896.0902.5873.7881.0970.1956.8845.9842.3980.5873.7853.6872.8840.7708.9802.1969.1980.9787.8808.9809.8844.4850.0942.7861.9895.5872.8868.6861.9845.9848.5825.8889.2917.5849.0975.4821.7813.0905.7844.4884.6801.6802.19
slpp_neg
05.1353.2145.2490.8244.4255.6317.3471.9277.1484.5285.1347.8357.5233.1356.2471.8340.7262.3356.1504.5256.8259.3127.5242.3268.5117.6355.5432.5264.8271.4373.4316.7247.2346.5225.7382.5371.8258.9296.0356.7331.2364.1339.9441.9271.7314.4212.7289.1360.1389.2236.0318.4216.83edutingam_neg 18.6209.0333.4265.3202.6266.4211.1305.1527.3264.1249.1265.3246.0218.5257.7211.1381.4219.3253.6251.4265.4219.5164.3226.1500.5235.6255.4249.5428.6208.3218.6273.7385.5290.4278.5296.3266.1214.8197.0265.7253.7281.4250.6282.1258.2226.7106.6223.9480.9259.7219.4255.3264.62
1f_col
47.0
39.2
46.0
46.0
46.0
46.0
20.1
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
97.1
77.0
46.0
46.0
51.1
46.0
46.0
46.0
20.1
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
98.0
46.0
46.0
me_col
06.9171.4247.8119.2177.5127.8187.3264.2499.6123.2112.7126.5172.5150.6154.2295.1245.5112.1248.8146.1113.4120.994.6146.0590.1226.1216.7109.8338.6102.5194.6140.2378.9119.5179.0289.5192.3154.6170.3172.9147.9163.0284.9118.7145.6106.2103.2226.8301.2265.6148.4117.4134.71
1f_trop
30.0
83.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
31.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
15.0
62.0
31.0
00.0
62.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
me_trop
hcraes-EKIM
.gvA
hz
iv
ru
ku
rt
ht
at
vs
rs
qs
ls
ks
ur
or
tp
lp
ln
sm
vl
tl
al
ok
ak
aj
ti
di
yh
uh
rh
ih
eh
lg
ag
rf
if
af
ue
te
se
le
ed
ad
yc
sc
ec
ac
nb
gb
eb
za
ra
fa
07.3895.9779.5895.5835.2868.6879.5819.2820.2868.6815.1822.6804.2892.3896.4880.5840.3833.5896.0920.2861.3815.1820.2863.0860.4844.4825.8803.7724.3864.5876.3823.8756.2808.3839.3813.4828.4835.2872.8896.4880.9775.4895.5824.3837.6859.8775.4821.7861.3841.2801.0878.0853.68
slpp_ler
19.1378.3220.8361.2353.8289.9345.8351.9254.6382.9269.2280.8340.2324.1318.6370.3311.7206.7312.5431.3328.2335.3298.4275.6234.0390.1358.8331.7252.9299.5300.0324.4213.9236.8281.3367.5387.9209.9286.9368.6301.5204.6317.1483.1393.4385.2273.8285.0377.9240.9211.0370.6203.24
edutingam_ler
79.2897.6779.5848.5847.0879.5801.6887.2867.1848.5804.2879.5898.1852.1856.2844.4835.2855.3809.8821.1819.2852.1804.2803.7792.3813.4821.7886.7776.3813.4860.4884.0841.2880.5856.2899.0860.4867.1810.8841.2864.9739.3844.4872.2801.6855.7719.2879.5872.2827.9779.9720.2817.58
slpp_neg
38.9256.0227.4323.1386.4272.3305.6375.8233.3337.8254.8259.5331.8215.7221.2394.1313.7225.9281.2477.6287.8296.0285.4277.2242.9237.1390.7344.6249.5238.9235.0332.7247.8256.1305.0331.0320.7207.1302.4392.2353.6261.1320.7340.0341.6385.2262.6237.8257.0361.5269.0387.6269.63edutingam_neg 88.0308.3365.6257.9265.9278.7228.4376.5545.7233.6281.6237.8261.5279.9273.1309.3334.8250.9233.9215.9208.8259.9164.0396.4548.0347.8236.7207.9405.3320.8235.1374.1494.9284.7270.8213.8219.4239.4201.7236.0356.0372.7224.9287.6288.6226.9165.9230.4587.1362.2314.0319.5282.13
1f_col
38.0
23.3
46.0
20.1
46.0
46.0
77.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
59.3
46.0
46.0
46.0
07.3
46.0
46.0
46.0
77.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
46.0
98.0
46.0
46.0
me_col
52.4297.6237.3222.6153.0225.3269.8217.5400.1258.4190.0265.2207.0258.1299.5244.5224.1289.4277.4204.5151.9116.4104.2224.5501.5248.4217.4236.2410.3231.0250.1259.5387.4256.2245.5285.0266.7137.1201.9165.4229.3260.4228.4280.4276.0289.5101.8268.1419.5293.1215.8123.8112.32
1f_trop
01.0
77.0
00.0
00.0
00.0
00.0
51.1
00.0
00.0
00.0
00.0
00.0
00.0
00.0
31.0
00.0
00.0
00.0
31.0
00.0
00.0
00.0
00.0
20.1
62.0
31.0
00.0
98.0
00.0
31.0
00.0
77.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
me_trop