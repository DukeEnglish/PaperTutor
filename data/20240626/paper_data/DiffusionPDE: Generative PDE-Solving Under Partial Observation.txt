DiffusionPDE: Generative PDE-Solving
Under Partial Observation
JiaheHuang1 GuandaoYang2 ZichenWang1 JeongJoonPark1
1UniversityofMichigan
2StanfordUniversity
{chloehjh, zzzichen, jjparkcv}@umich.edu
guandao@stanford.edu
Abstract
Weintroduceageneralframeworkforsolvingpartialdifferentialequations(PDEs)
usinggenerativediffusionmodels. Inparticular,wefocusonthescenarioswhere
wedonothavethefullknowledgeofthescenenecessarytoapplyclassicalsolvers.
MostexistingforwardorinversePDEapproachesperformpoorlywhentheob-
servationsonthedataortheunderlyingcoefficientsareincomplete, whichisa
commonassumptionforreal-worldmeasurements. Inthiswork,weproposeDiffu-
sionPDEthatcansimultaneouslyfillinthemissinginformationandsolveaPDEby
modelingthejointdistributionofthesolutionandcoefficientspaces. Weshowthat
thelearnedgenerativepriorsleadtoaversatileframeworkforaccuratelysolving
awiderangeofPDEsunderpartialobservation,significantlyoutperformingthe
state-of-the-artmethodsforbothforwardandinversedirections. Seeourproject
pageforresults: jhhuangchloe.github.io/Diffusion-PDE/.
1 Introduction
Partial differential equations (PDEs) are a cornerstone of modern science, underpinning many
contemporaryphysicaltheoriesthatexplainnaturalphenomena. TheabilitytosolvePDEsgrants
usthepowertopredictfuturestatesofasystem(forwardprocess)andestimateunderlyingphysical
propertiesfromstatemeasurements(inverseprocess).
To date, numerous methods [1, 2] have been proposed to numerically solve PDEs for both the
forwardandinversedirections. However,theclassicalmethodscanbeprohibitivelyslow,prompting
thedevelopmentofdata-driven,learning-basedsolversthataresignificantlyfasterandcapableof
handlingafamilyofPDEs. Theselearning-basedapproaches[3–6]typicallylearnadeterministic
mappingbetweeninputcoefficientsandtheirsolutionsusingdeepneuralnetworks.
Despitetheprogress,existinglearning-basedapproaches,muchlikeclassicalsolvers,relyoncomplete
observationsofthecoefficientstomapsolutions. However,completeinformationontheunderlying
physicalpropertiesorthestateofasystemisrarelyaccessible;inreality,mostmeasurementsare
sparseinspaceandtime. Bothclassicalsolversandthestate-of-the-artdata-drivenmodelsoften
overlook these scenarios and consequently fail when confronted with partial observations. This
limitationconfinestheiruseprimarilytosyntheticsimulations,wherefullsceneconfigurationsare
availablebydesign,makingtheirapplicationtoreal-worldcaseschallenging.
We present a comprehensive framework, DiffusionPDE, for solving PDEs in both forward and
inversedirectionsunderconditionsofhighlypartialobservations—typicallyjust1~3%ofthetotal
information. Thistaskisparticularlychallengingduetothenumerouspossiblewaystocomplete
missingdataandfindsubsequentsolutions. Ourapproachusesagenerativemodeltoformulatethe
jointdistributionofthecoefficientandsolutionspaces, effectivelymanagingtheuncertaintyand
simultaneouslyreconstructingbothspaces. Duringinference,wesamplerandomnoiseanditeratively
Preprint.Underreview.
4202
nuJ
52
]GL.sc[
1v36771.6042:viXraTraining
Joint Distribution Diffusion Model Predictions

Coefficient/Initial State: a Solution/Final State: u (on both a and u)
Generated a
...
... ...
Observations

Inference
(on either side or both)
Generated u
Sparse Ground Truth a Sparse Ground Truth u Diffusion Model
Noise PDE Guidance
Observation Guidance
Figure1: WeproposeDiffusionPDE,agenerativePDEsolverunderpartialobservations. Given
afamilyofPDEwithcoefficient(initialstate)aandsolution(finalstate)u,wetrainthediffusion
modelonthejointdistributionofaandu. Duringinference,wegraduallydenoiseaGaussiannoise,
guidedbysparseobservationandknownPDEfunction,torecoverthefullpredictionofbothaandu
thatalignwellwiththesparseobservationsandthegivenequation.
denoise it following standard diffusion models [7]. However, we uniquely guide this denoising
process with sparse observations and relevant PDE constraints, generating plausible outputs that
adheretotheimposedconstraints. Notably,DiffusionPDEcanhandleobservationswitharbitrary
densityandpatternswithasinglepre-trainedgenerativenetwork.
WeconductextensiveexperimentstoshowtheversatilityofDiffusionPDEasageneralPDE-solving
framework. We evaluate it on a diverse set of static and temporal PDEs, including Darcy Flow,
Poisson,Helmholtz,Burger’s,andNavier-Stokesequations. DiffusionPDEsignificantlyoutperforms
existingstate-of-the-artlearning-basedmethodsforsolvingPDEs[3–6,8]inbothforwardandinverse
directions with sparse measurements, while achieving comparable results with full observations.
Highlightingtheeffectivenessofourmodel,DiffusionPDEaccuratelyreconstructsthecompletestate
ofBurgers’equationusingtime-seriesdatafromjustfivesensors(Fig.4),suggestingthepotentialof
generativemodelstorevolutionizephysicalmodelinginreal-worldapplications.
2 RelatedWorks
Ourworkbuildsontheextensiveliteratureofthreeareas: forwardPDEsolvers,inversePDEsolvers,
anddiffusionmodels. Pleaseseerelevantsurveysformoreinformation[9–13].
ForwardPDESolvers. PDEsolverstakethespecificationofaphysicssystemandpredictitsstate
inunseenspaceandtimebysolvinganequationinvolvingpartialderivatives. SinceMostPDEs
areverychallengingtosolveanalytically, peopleresolvetonumericaltechniques, suchasFinite
ElementMethod[14,2]andBoundaryElementMethod[1,15]. Whilethesetechniquesshowstrong
performanceandversatilityinsomeproblems,theycanbecomputationallyexpensiveordifficult
to set up for complex physics systems. Recently, advancements in deep-learning methods have
inspiredanewsetofPDEsolvers. Raissietal. [16,6]introducePhysics-InformedNeuralNetworks
(PINNs),whichoptimizeaneuralnetworkusingPDEconstraintsasself-supervisedlossestooutput
thePDEsolutions. PINNshavebeenextendedtosolvingspecificfluid[17,18],Reynolds-averaged
Navier–Stokesequations[19],heatequations[20],anddynamicpowersystems[21]. WhilePINNs
cantackleawiderangeofcomplexPDEproblems,theyaredifficulttoscaleduetotheneedfor
networkoptimization. Analternativeapproach,neuraloperators[3,5],directlylearnthemapping
fromPDEparameters(e.g.initialandboundarycondition)tothesolutionfunction. Oncetrained,this
methodavoidsexpensivenetworkoptimizationandcaninstantlyoutputthesolutionresult. Thisidea
2hasbeenextendedtosolvePDEin3D[22,23],multiphaseflow[24],seismicwave[25,26],3D
turbulence[27,28],andsphericaldynamics[29]. Peoplehavealsoexploredusingneuralnetworksas
partofthePDEsolver,suchascompressingthephysicsstate[30–33]. Thesesolversusuallyassume
knownPDEparameters,andapplyingthemtosolvetheinverseproblemcanbechallenging.
PDEinverseproblem. TheinverseproblemreferstofindingthecoefficientsofaPDEthatcan
induce certain observations, mapping from the solution of a PDE solver to its input parameters.
Peoplehavetriedtoextendtraditionalnumericalmethodstothisinverseproblem[34–38],butthese
extensionsarenon-trivialtoimplementefficiently. Therearesimilarattemptstoinversedeep-learning
PDE solvers. For example, one can inverse PINNs by optimizing the network parameters such
thattheiroutputssatisfyboththeobserveddataandthegoverningequations. iFNO[39]andNIO
[40]triestoextendFNO[3]. Othermethods[41,42]directlylearntheoperatorfunctionsforthe
inverseproblem. PINO[4]furthercombinesneuraloperatorswithphysicsconstraintstoimprove
theperformanceofbothforwardandinverseproblems. Thesemethodsassumefullobservationsare
available. Toaddresstheinverseproblemwithpartialobservations,peoplehavetriedtoleverage
generativepriorswithGraphneuralnetworks[43,8]. Theseworkshavenotdemonstratedtheability
tosolvehigh-resolutionPDEs,possiblylimitedbythepowerofgenerativeprior. Wewanttoleverage
thestate-of-the-artgenerativemodel,diffusionmodels,todevelopabetterinversePDEsolver.
Diffusionmodels. Diffusionmodelshaveshowngreatpromiseinlearningthepriorwithhigher
resolutionsbyprogressivelyestimatingandremovingnoise. ModelslikeDDIM[44],DDPM[7],and
EDM[45]offerexpressivegenerativecapabilitiesbutfacechallengeswhensamplingwithspecific
constraints. Guideddiffusionmodels[46–49]enhancegenerationprocesseswithconstraintssuch
asimageinpainting,providingmorestableandaccuratesolutions. Priorworksondiffusionmodels
forPDEshighlightthepotentialofdiffusionapproachesbygeneratingPDEdatasetssuchas3D
turbulence[50,51]andNavier-Stokesequations[52]withdiffusionmodels. Diffusionmodelscan
also be used to model frequency spectrum and denoise the solution space [53], and conditional
diffusionmodelsareappliedtosolve2Dflowswithsparseobservation[54]. However,theapplication
ofdiffusionmodelstosolveinverseproblemsunderpartialobservationremainsunderexplored. In
thiswork,weaimtotaketheinitialstepstowardsaddressingthisgap.
3 Methods
3.1 Overview
Tosolvephysics-informedforwardandinverseproblemsunderuncertainty,westartbypre-traininga
diffusiongenerativemodelonafamilyofpartialdifferentialequations(PDEs).Thismodelisdesigned
to learn the joint distribution of the PDE coefficients (or the initial state) and its corresponding
solutions (or the final state). Our approach involves recovering full data in both spaces using
sparseobservationsfrom eitheror bothsides. Weachievethis throughthe iterative denoisingof
randomGaussiannoiseasinregulardiffusionmodelsbutwithadditionalguidancefromthesparse
observationsandthePDEfunctionenforcedduringdenoising. Theschematicdescriptionofour
approachisshowninFig. 1.
3.2 Prelimary: DiffusionModelsandGuidedDiffusion
DiffusionmodelsinvolveapredefinedforwardprocessthatgraduallyaddsGaussiannoisetothe
data and a learned reverse process that denoises the data to reconstruct the original distribution.
Specifically,Songetal. [55]proposeadeterministicdiffusionmodelthatlearnsanN-stepdenoising
processthateventuallyoutputsadenoiseddatax andsatisfiesthefollowingordinarydifferential
N
equations(ODE)ateachtimestept wherei∈{0,1,...,N −1}
i
(cid:0) (cid:1)
dx=−σ˙(t)σ(t)∇ logp x;σ(t) dt. (1)
x
(cid:0) (cid:1)
Here ∇ logp x;σ(t) is the score function [56] that helps to transform samples from a normal
x
distribution N(0,σ(t )2I) to a target probability distribution p(x;σ(t)). To estimate the score
0
function,Karrasetal. [45]proposetolearnadenoiserfunctionD(x;σ)suchthat
∇ logp(cid:0) x;σ(t)(cid:1) =(D(x;σ(t))−x)/σ(t)2 (2)
x
Toenablecontroloverthegenerateddata,guideddiffusionmethods[48]addguidancegradientsto
thescorefunctionduringthedenoisingprocess. Recently,diffusionposteriorsampling(DPS)[46]
3Algorithm1SparseObservationandPDEGuidedDiffusionSamplingAlgorithm.
1: inputDeterministicSamplerD θ(x;σ), σ(t i∈{0,...,N}),TotalPointCountm,ObservedPointCount
n,Observationy,PDEFunctionf,Weightsζ ,ζ
obs pde
2: samplex 0
∼N(cid:0)
0, σ(t
0)2I(cid:1)
▷Generateinitialsamplingnoise
3: fori∈{0,...,N −1}do
4: xˆi N ←D θ(x i;σ(t i)) ▷Estimatethedenoiseddataatstept i
5: d i ←(cid:0) x i−xˆi N(cid:1) /σ(t i) ▷Evaluatedx/dσ(t)atstept i
6: x i+1 ←x i+(σ(t i+1)−σ(t i))d i ▷TakeanEulerstepfromσ(t i)toσ(t i+1)
7: ifσ(t i+1)̸=0then
8: xˆi
N
←D θ(x i+1;σ(t i+1)) ▷Apply2ndordercorrelationunlessσ =0
9: d′ i ←(cid:0) x i+1−xˆi N(cid:1) /σ(t i+1) ▷Evaluatedx/dσ(t)atstept i+1
10: x i+1 ←x i+(σ(t i+1)−σ(t i))(cid:0) 21d i+ 1 2d′ i(cid:1) ▷Applythetrapezoidalruleatstept i+1
11: endif
12: L obs ← n1∥y−xˆi N∥2 2 ▷Evaluatetheobservationlossofxˆi N
13: L pde ← m1∥0−f(xˆi N)∥2 2 ▷EvaluatethePDElossofxˆi N
14: x i+1 ←x i+1−ζ obs∇ xiL obs−ζ pde∇ xiL pde ▷GuidethesamplingwithL obsandL pde
15: endfor
16: returnx N ▷Returnthedenoiseddata
madenotableprogressinguideddiffusionfortacklingvariousinverseproblems. DPSusescorrupted
measurementsyderivedfromxtoguidethediffusionmodelinoutputtingtheposteriordistribution
p(x|y). AprimeapplicationofDPSistheinpaintingproblem,whichinvolvesrecoveringacomplete
imagefromsparselyobservedpixels,whichsuitswellwithourtask. ThisapproachmodifiesEq. 1to
(cid:0) (cid:0) (cid:1) (cid:0) (cid:1)(cid:1)
dx=−σ˙(t)σ(t) ∇ logp x;σ(t) +∇ logp y|x;σ(t) dt. (3)
x x
DPS[46]showedthatunderGaussiannoiseassumptionofthesparsemeasurementoperatorM(·),
i.e.,y|x∼N(M(x),δ2I)withsomeS.D.δ,thelog-likelihoodfunctioncanbeapproximatedwith:
∇ logp(cid:0) y|x ;σ(t )(cid:1) ≈∇ logp(cid:0) y|xˆi ;σ(t )(cid:1) ≈− 1 ∇ ∥y−M(xˆi (x ;σ(t ))∥2, (4)
x i i xi N i δ2 xi N i i 2
wherexˆi :=D(x ;σ(t ))denotestheestimationofthefinaldenoiseddataateachdenoisingstepi.
N i i
ApplyingtheBaye’srule,thegradientdirectionoftheguideddiffusionistherefore:
∇ logp(x |y)≈s(x )−ζ∇ ∥y−M(xˆi )∥2, (5)
xi i i xi N 2
wheres(x)=∇ logp(cid:0) x(cid:1) istheoriginalscorefunction,andζ =1/δ2.
x
3.3 SolvingPDEswithGuidedDiffusion
OurworkfocusesontwoclassesofPDEs: staticPDEsanddynamictime-dependentPDEs. Static
systems(e.g.,DarcyFloworPoissonequations)aredefinedbyatime-independentfunctionf:
f(c;a,u)=0 in Ω⊂Rd, u(c)=g(c) in ∂Ω, (6)
whereΩisaboundeddomain, c ∈ Ωisaspatialcoordinate, a ∈ AisthePDEcoefficientfield,
andu∈U isthesolutionfield. ∂ΩistheboundaryofthedomainΩandu| =gistheboundary
∂Ω
constraint. Weaimtorecoverbothaandufromsparseobservationsoneitheraoruorboth.
Similarly,weconsiderthedynamicsystems(e.g.,Navier-Stokes):
f(c,τ;a,u)=0, inΩ×(0,∞)
u(c,τ)=g(c,τ), in∂Ω×(0,∞) (7)
u(c,τ)=a(c,τ), inΩ¯ ×{0}
whereτ isatemporalcoordinate,a = u ∈ Aistheinitialcondition,uisthesolutionfield,and
0
u| =gistheboundaryconstraint. Weaimtosimultaneouslyrecoverbothaandthesolution
Ω×(0,∞)
u :=u(·,T)ataspecifictimeT fromsparseobservationsoneithera,u ,orboth.
T T
Finally, we explore the recovery of the states across all timesteps u in 1D dynamic systems
0:T
governedbyBurger’sequation. OurnetworkD modelsthedistributionofall1Dstates,including
θ
theinitialconditionu andsolutionsu stackedinthetemporaldimension,forminga2Ddataset.
0 1:T
4Sparse observations on Sparse Observations on Sparse Observations on Both
coefficients Solutions Coefficients and Solutions Ground Truth Partial Observations
Error Rate: 2.8% Error Rate: 3.2% Error Rate: 2.6%
Relative Error: 2.5% Relative Error: 1.0% Relative Error: 0.9%
Figure2:DifferentfromforwardandinversePDEsolvers,DiffusionPDEcantakesparseobservations
oneitherthecoefficientaorthesolutionutorecoverbothofthem,usingonetrainednetwork. Here,
weshowtherecoveredaanduoftheDarcy’seqautiongivensparseobservationsona,u,orboth.
Comparedwiththegroundtruth,weseethatourmethodsuccessfullyrecoversthePDEinallcases.
GuidedDiffusionAlgorithm Inthedata-drivenPDEliterature,theabovetaskscanbeachievedby
learningdirectionalmappingsbetweenaandu(oru fordynamicsystems). Thus,existingmethods
T
typicallytrainseparateneuralnetworksfortheforwardsolutionoperatorF :A→U andtheinverse
solutionoperatorI :U →A.
Ourmethodunifiestheforwardandinverseoperatorswithasinglenetworkandanalgorithmusing
the guided diffusion framework. DiffusionPDE can handle arbitrary sparsity patterns with one
pre-traineddiffusionmodelD thatlearnsthejointdistributionofAandU,concatenatedonthe
θ
channeldimension,denotedX. Thus,ourdatax∈X,whereX :=A×U.Wefollowthetypical
diffusionmodelprocedures[45]totrainourmodelonafamilyofPDEs.
OncewetrainthediffusionmodelD ,weemployourphysics-informedDPS[46]formulationduring
θ
inferencetoguidethesamplingofx∈X thatsatisfiesthesparseobservationsandthegivenPDE,as
detailedinAlgorithm1. WefollowEq.5tomodifythescorefunctionusingthetwoguidanceterms:
(cid:0)
∇ logp(x |y ,f)≈∇ logp x )−ζ ∇ L −ζ ∇ L , (8)
xi i obs xi i obs xi obs pde xi pde
where x is the noisy data at denoising step i, y are the observed values, and f(·) = 0 is
i obs
the underlying PDE condition. L and L respectively represent the MSE loss of the sparse
obs pde
observationsandthePDEequationresiduals:
n
1 1 (cid:88)
L (x ,y ;D )= ∥y −xˆi ∥2 = (y (o )−xˆi (o ))2,
obs i obs θ n obs N 2 n obs j N j
j=1
(9)
1 1 (cid:88)(cid:88)
L (x ;D ,f)= ∥0−f(xˆi )∥2 = f(c ,τ ;uˆ ,aˆ )2,
pde i θ m N 2 m j k j j
j k
wherexˆi = D (x )isthecleanimageestimateatdenoisingtimestepi, whichcanbesplitinto
N θ i
coefficientuˆ andsolutionaˆ .Here,misthetotalnumberofgridpoints(i.e.,pixels),nisthenumber
i i
ofsparseobservationpoints. o representsthespatio-temporalcoordinateofjthobservation. Note
j
that,withoutlossofgenerality,L canbeaccumulatedforallapplicablePDEfunctionf inthe
pde
system,andthetimecomponentτ isignoredforstaticsystems.
k
4 Experiments
4.1 PDEProblemSettings
WeshowtheusefulnessofDiffusionPDEacrossvariousPDEsforinverseandforwardproblemsand
compareitagainstrecentlearning-basedtechniques. WetestonthefollowingfamiliesofPDEs.
5
stneiciffeoC
derevoceR
snoituloS
derevoceRGuided with  Guided with PDE Loss  Guided with  Guided with PDE Loss 
Ground Truth Observation Loss and Observation Loss Ground Truth Observation Loss and Observation Loss
Figure3: UsefulnessofPDEloss. Wevisualizetheabsoluteerrorsoftherecoveredcoefficientand
solutionoftheHelmholtzequationwithandw/oPDEloss. Wecomparehavingonlytheobservation
losswithapplyingtheadditionalPDEloss. TheerrorsdropsignificantlywhenusingPDEloss.
Darcy Flow. Darcy flow describes the movement of fluid through a porous medium. In our
experiment,weconsiderthestaticDarcyFlowwithano-slipboundary∂Ω
−∇·(a(c)∇u(c))=q(c), c∈Ω
(10)
u(c)=0, c∈∂Ω
Herethecoefficientahasbinaryvalues. Wesetq(c) = 1forconstantforce. ThePDEguidance
functionisthusf =∇·(a(c)∇u(c))+q(c).
InhomogeneousHelmholtzEquation. WeconsiderthestaticinhomogeneousHelmholtzEquation
withano-slipboundaryon∂Ω,whichdescribeswavepropagation:
∇2u(c)+k2u(c)=a(c), c∈Ω
(11)
u(c)=0, c∈∂Ω
The coefficient a is a piecewise constant function and k is a constant. Note 11 is the Poisson
equation when k = 0. Setting k = 1 for Helmholtz equations, the PDE guidance function is
f =∇2u(c)+k2u(c)−a(c).
Non-boundedNavier-StokesEquation. Westudythenon-boundedincompressiveNavier-Stokes
equationregardingthevorticity.
∂ w(c,τ)+v(c,τ)·∇w(c,τ)=ν∆w(c,τ)+q(c), c∈Ω,τ ∈(0,T]
t
(12)
∇·v(c,τ)=0, c∈Ω,τ ∈[0,T]
Herew =∇×visthevorticity,v(c,τ)isthevelocityatcattimeτ,andq(c)isaforcefield. We
settheviscositycoefficientν =10−3andcorrespondinglytheReynoldsnumberRe= 1 =1000.
ν
DiffusionPDE learns the joint distribution of w and w and we take T = 10 which simulates
0 T
1 second. Since T ≫ 0, we cannot accurately compute the PDE loss from our model outputs.
Therefore,giventhat∇·w(c,τ)=∇·(∇×v)=0,weusesimplifiedf =∇·w(c,τ).
Bounded Navier-Stokes Equation. We study the bounded 2D imcompressive Navier Stokes
regardingthevelocityvandpressurep.
1
∂ v(c,τ)+v(c,τ)·∇v(c,τ)+ ∇p=ν∇2v(c,τ), c∈Ω,τ ∈(0,T]
t ρ (13)
∇·v(c,τ)=0, c∈Ω,τ ∈(0,T].
Wesettheviscositycoefficientν =0.001andthefluiddensityρ=1.0. Wegenerate2Dcylinders
ofrandomradiusatrandompositionsinsidethegrid. Randomturbulenceflowsinfromthetopofthe
grid,withthevelocityfieldsatisfyingno-slipboundaryconditionsattheleftandrightedges,aswell
asaroundthecylinder∂Ω . DiffusionPDElearnsthejointdistributionofv andv
left,right,cylinder 0 T
atT =4,whichsimulates0.4seconds. Therefore,wesimilarlyusef =∇·v(c,τ)asbefore.
Burgers’Equation. WestudytheBurgers’equationwithperiodicboundaryconditionsona1D
spatialdomainofunitlengthΩ=(0,1). Wesettheviscositytoν =0.01. Inourexperiment,the
initialconditionu hasashapeof128×1,andwetake127moretimestepsaftertheinitialstateto
0
forma2Du ofsize128×128.
0:T
∂ u(c,τ)+∂ (u2(c,τ)/2)=ν∂ u(c,τ), c∈Ω,τ ∈(0,T]
t c cc
(14)
u(c,0)=u (c), c∈Ω
0
Wecanreliablycomputef =∂ u(c,τ)+∂ (u2(c,τ)/2)−ν∂ u(c,τ)withfinitedifferencesince
t c cc
wemodeldenselyonthetimedimension.
6
tneiciffeoC
rorrE
etulosbA
tneiciffeoC
noituloS
rorrE
etulosbA
noituloSTable1: Relativeerrorsofsolutions(orfinalstates)andcoefficients(orinitialstates)whensolving
forward and inverse problems respectively with sparse observations. Error rates are used for the
inverseproblemofDarcyFlow.
DiffusionPDE PINO DeepONet PINNs FNO
Forward 2.5% 35.2% 38.3% 48.8% 28.2%
DarcyFlow
Inverse 3.2% 49.2% 41.1% 59.7% 49.3%
Forward 4.5% 107.1% 155.5% 128.1% 100.9%
Poisson
Inverse 20.0% 231.9% 105.8% 130.0% 232.7%
Forward 8.8% 106.5% 123.1% 142.3% 98.2%
Helmholtz
Inverse 22.6% 216.9% 132.8% 160.0% 218.2%
Non-bounded Forward 6.9% 101.4% 103.2% 142.7% 101.4%
Navier-Stokes Inverse 10.4% 96.0% 97.2% 146.8% 96.0%
Bounded Forward 3.9% 81.1% 97.7% 100.1% 82.8%
Navier-Stokes Inverse 2.7% 69.5% 91.9% 105.5% 69.6%
4.2 DatasetPreparationandTraining
WefirsttestDiffusionPDEonjointlylearningtheforwardmappingF : A → U andtheinverse
mappingI :U →Agivensparseobservations. Inourexperiments,wedefineourPDEovertheunit
squareΩ = (0,1)2,whichwerepresentasa128×128grid. WeutilizeFiniteElementMethods
(FEM) to generate our training data. Specifically, we run FNO’s [3] released scripts to generate
DarcyFlowsandthevorticitiesoftheNavier-Stokesequation. Similarly,wegeneratethedatasetof
PoissonandHelmholtzusingsecond-orderfinitedifferenceschemes. Toaddmorecomplexboundary
conditions,weuseDifftaichi[57]togeneratethevelocitiesoftheboundedNavier-Stokesequation.
WetrainthejointdiffusionmodelforeachPDEonthreeA40GPUsforapproximately4hours,using
50,000datapairs. ForBurgers’equation,wetrainthediffusionmodelonadatasetof50,000samples
producedasoutlinedinFNO[3]. Werandomlyselect5outof128spatialpointsonΩtosimulate
sensorsthatprovidemeasurementsacrosstime.
4.3 BaselineMethods
WecompareDiffusionPDEwithstate-of-the-artlearning-basedmethods,includingPINO[4],Deep-
ONet[5],PINNs[6],andFNO[3]. However,notethatnoneofthesemethodsshowoperationon
partialobservations. Thesemethodscanlearnmappingsbetweenaanduoru andu withfull
0 1:T
observations, allowingthemtoalsosolvethemappingbetweenu andu . PINNsmapinputa
0 T
tooutputubyoptimizingacombinedlossfunctionthatincorporatesboththesolutionuandthe
PDEresiduals. DeepONetemploysabranchnetworktoencodeinputfunctionvaluessampledat
discretepointsandatrunknetworktohandlethecoordinatesoftheevaluatedoutputs. FNOmaps
fromtheparametricspacetothesolutionspaceusingFouriertransforms. PINOenhancesFNOby
integratingPDElossduringtrainingandrefiningthemodelwithPDElossfinetuning. Wetrainall
fourbaselinemethodsonbothforwardandinversemappingsusingfullobservationofaorufor
bothstaticanddynamicPDEs. Wetriedtrainingthebaselinemodelsonpartialobservations,butwe
noticeddegeneratetrainingoutcomes(seesupplementaryfordetails). Overall,theyareintendedfor
fullobservationsandmaynotbesuitableforsparsemeasurements.
Morecloselyrelatedtoourmethod,GraphPDE[8]demonstratestheabilitytorecovertheinitial
stateusingsparseobservationsonthefinalstate,ataskthatotherbaselinesstrugglewith. Therefore,
wecompareagainstGraphPDEfortheinverseproblemofboundedNavier-Stokes(NS)equation,
whichisthesetupusedintheirreport. GraphPDEusesatrainedlatentspacemodelandabounded
forwardGNNmodeltosolvetheinverseproblemwithsparsesensorsandthusisincompatiblewith
unboundedNavier-Stokes. WecreateboundedmeshesusingourboundedgridstotraintheGNN
modelandtrainthelatentpriorwithv forGraphPDE.
0:T
4.4 MainEvaluationResults
Werespectivelyaddresstheforwardproblemandtheinverseproblemwithsparseobservationsofa
oru. Fortheforwardproblem,werandomlyselectcoefficients(initialstates)assparseobservations
7Observations Ground Truth DiffusionPDE DeepONet PINO FNO PINNs
Forward
y
x 500 Random Points Relative Error: 6.9%Relative Error: 103.2%Relative Error: 101.4%Relative Error: 101.4%Relative Error: 142.7%
Inverse
y
x Relative Error: 10.4%Relative Error: 97.2%Relative Error: 96.0%Relative Error: 96.0%Relative Error: 146.8%
5 Random 
Sensors
x Relative Error: 2.68%Relative Error: 92.0%Relative Error: 85.7% Relative Error: 90.0%Relative Error: 92.4%
Figure4: WecompareDiffusionPDEwithstate-of-the-artneuralPDEsolvers[3–6]. Intheforward
Navier-Stokes problem, we give 500 sparse observations of the initial state to solve for the final
state. Intheinverseset-up,wetakeobservationsofthefinalstateandsolvefortheinitial. Forthe
Burgers’equation,weuse5sensorsthroughoutalltimestepsandwanttorecoverthesolutionat
alltimesteps. Notethatwetrainonneighboringsnapshotpairsforthebaselinesinordertoadd
continuousobservationsoftheBurgers’equation. Resultsshowthatexistingmethodsdonotsupport
PDEsolvingundersparseobservations,andwebelievetheyarenoteasilyextendabletodoso. We
referreaderstothesupplementaryforacompletesetofvisualresults.
andthencomparethepredictedsolutions(finalstates)withthegroundtruth. Specifically,weselect
500outof128×128points,approximately3%,onthecoefficientsofDarcyFlow,Poissonequation,
Helmholtzequation,andtheinitialstateofthenon-boundedNavier-Stokesequation. Forthebounded
Navier-Stokes equation, we use 1% observed points beside the boundary of the cylinder in 2D.
Similarly,fortheinverseproblem,werandomlysamplepointsonsolutions(finalstates)assparse
observations,usingthesamenumberofobservedpointsasintheforwardmodelforeachPDE.
WeshowtherelativeerrorsofallmethodsregardingbothforwardandinverseproblemsinTable
1. Since the coefficients of Darcy Flow are binary, we evaluate the error rates of our prediction.
Non-binarydataisevaluatedusingmeanpixel-wiserelativeerror. Wereporterrornumbersaver-
agedacross1,000randomscenesandobservationsforeachPDE.DiffusionPDEoutperformsother
methodsincludingPINO[4],DeepONet[5],PINNs[6],andFNO[3]forbothdirectionswithsparse
observations,demonstratingthenoveltyanduniquenessofourapproach. Fortheinverseproblemsof
thePoissonandHelmholtzequations,DiffusionPDEexhibitshighererrorratesduetotheinsufficient
constraintswithinthecoefficientspace,producedfromrandomfields. InFig. 4,wevisualizethe
resultsforsolvingboththeforwardandinverseproblemofthenon-boundedNavier-Stokes. Werefer
tothesupplementaryforadditionalvisualresults. Whileothermethodsmayproducepartiallycorrect
results,DiffusionPDEoutperformsthemandcanrecoverresultsveryclosetothegroundtruth.
FortheinverseproblemoftheboundedNavier-Stokesequation,wefurthercompareDiffusionPDE
withGraphPDE,asillustratedinFig. 5. OurfindingsrevealthatDiffusionPDEsurpassesGraphPDE
[8]inaccuracy,reducingtherelativeerrorfrom12.0%to2.7%withonly1%observedpoints.
WefurthershowwhetherDiffusionPDEcanjointlyrecoverbothaandubyanalyzingtheretrieved
aanduwithsparseobservationsondifferentsidesaswellasonbothsides. InFig. 2,werecoverthe
coefficientsandsolutionsofDarcyFlowbyrandomlyobserving500pointsononlycoefficientspace,
onlyspacesolutionspace,andboth. Bothcoefficientsandsolutionscanberecoveredwithlowerrors
foreachsituation. WethereforeconcludethatDiffusionPDEcansolvetheforwardproblemandthe
inverseproblemsimultaneouslywithsparseobservationsatanysidewithoutretrainingournetwork.
4.5 RecoveringSolutionsThroughoutaTimeInterval
We demonstrate that DiffusionPDE is capable of retrieving all time steps throughout the time
interval [0,T] from continuous observations on sparse sensors. To evaluate its ability to recover
u withsparsesensors,westudythe1DdynamicBurgers’equation,whereDiffusionPDElearns
0:T
8
noitauqE
sekotS-reivaN
dednuob-noN
noitauqE
’sregruB
etatS
laitinI
etatS
laniF
emiT
etatS
laniF
etatS
laitinI
emiTGround Truth Observed Final State Cylinder Boundary GraphPDE DiffusionPDE Ground Truth
Inverse
1% Random Points Relative Error: 12.0% Relative Error: 2.7%
Figure5: WecompareGraphPDE[8]andourmethodforsolvingtheinverseboundedNavier-Stokes
equation. Giventheboundaryconditionsand1%observationsofthefinalvorticityfield,wesolvethe
initialvorticityfield. Wesetthefliudstoflowinfromthetop,withboundaryconditionsattheedges
andamiddlecylinder. WhileGraphPDEcanrecovertheoverallpatternoftheinitialstate,itsuffers
fromnoisewhenthefluidpassesthecylinderandmissesthehighvorticitiesatthebottom.
the distribution of u using a 2D diffusion model. To apply continuous observation on PINO,
0:T
DeepONet,FNO,andPINNs,wetrainthemonneighboringsnapshotpairs. Ourexperimentresults
inatestrelativeerrorof2.68%,depictedinFig. 4,whichissignificantlylowerthanothermethods.
4.6 AdditionalAnalysis
WeexaminetheeffectsofdifferentcomponentsofouralgorithmsuchasPDElossandobservation
samplings. We strongly encourage readers to view the supplementary for more details of these
analysesaswellasadditionalexperiments.
PDELoss. ToverifytheroleofthePDEguidancelossofEq. 8duringthedenoisingprocess,we
visualizetheerrorsofrecoveredaanduofHelmholtzequationwithorwithoutPDEloss. Here,we
runourDPSalgorithmwith500sparseobservedpointsonboththecoefficientaandsolutionuand
studytheeffectoftheadditionalPDElossguidance. Therelativeerrorofureducesfrom9.3%to
0.6%,andtherelativeerrorofareducesfrom13.2%to9.4%. Therefore,weconcludethatPDE
guidancehelpssmooththepredictionandimprovetheaccuracy.
NumberofObservations. WeexaminetheresultsofDiffusionPDEinsolvingforwardandinverse
problemswhenthereare100,300,500,and1000randomobservationsona,u,orbothaandu. The
errorofDiffusionPDEdecreasesasthenumberofsparseobservationsincreases. DiffusionPDEis
capableofrecoveringbothaanduwitherrors1%∼10%withapproximately6%observationpoints
atanysideformostPDEfamilies. DiffusionPDEbecomesinsensitivetothenumberofobservations
andcansolvetheproblemswelloncemorethan3%ofthepointsareobserved.
ObservationSamplingPattern WeshowthatDiffusionPDEisrobusttodifferentsamplingpatterns
ofthesparseobservations,includinggridandnon-uniformlyconcentratedpatterns. Notethateven
when conditioned on the full observations, our approach performs on par with the current best
methods,likelyduetotheinherentresilienceofourguideddiffusionalgorithm.
5 ConclusionandFutureWork
Inthiswork,wedevelopDiffusionPDE,adiffusion-basedPDEsolverthataddressesthechallenge
ofsolvingPDEsfrompartialobservationsbyfillinginmissinginformationusinggenerativepriors.
Weformulateadiffusionmodelthatlearnsthejointdistributionofthecoefficient(orinitialstate)
spaceandthesolution(orfinalstate)space. Duringthesamplingprocess,DiffusionPDEcanflexibly
generateplausibledatabyguidingitsdenoisingwithsparsemeasurementsandPDEconstraints. Our
newapproachleadstosignificantimprovementsoverexistingstate-of-the-artmethods,advancing
towardageneralPDE-solvingframeworkthatleveragesthepowerofgenerativemodels.
Severalpromisingdirectionsforfutureresearchhaveemergedfromthiswork. Currently,Diffusion-
PDEislimitedtosolvingslicesof2DdynamicPDEs;extendingitscapabilitiestocoverfulltime
intervalsoftheseequationspresentsasignificantopportunity. Moreover,themodel’sstrugglewith
accuracyinspacesthatlackconstraintsisanothercriticalareaforexploration. DiffusionPDEalso
suffersfromaslowsamplingprocedure,andafastersolutionmightbedesired.
9
etatS
laniF
etatS
liatinIReferences
[1] FerriMHAliabadi. Boundaryelementmethods. InEncyclopediaofcontinuummechanics,
pages182–193.Springer,2020.
[2] PavelSˆolín. Partialdifferentialequationsandthefiniteelementmethod. JohnWiley&Sons,
2005.
[3] ZongyiLi,NikolaKovachki,KamyarAzizzadenesheli,BurigedeLiu,KaushikBhattacharya,
AndrewStuart,andAnimaAnandkumar. Fourierneuraloperatorforparametricpartialdifferen-
tialequations. arXivpreprintarXiv:2010.08895,2020.
[4] ZongyiLi,HongkaiZheng,NikolaKovachki,DavidJin,HaoxuanChen,BurigedeLiu,Kamyar
Azizzadenesheli,andAnimaAnandkumar.Physics-informedneuraloperatorforlearningpartial
differentialequations. ACM/JMSJournalofDataScience,2021.
[5] LuLu,PengzhanJin,GuofeiPang,ZhongqiangZhang,andGeorgeEmKarniadakis. Learning
nonlinearoperatorsviadeeponetbasedontheuniversalapproximationtheoremofoperators.
Naturemachineintelligence,3(3):218–229,2021.
[6] MaziarRaissi,ParisPerdikaris,andGeorgeEKarniadakis. Physics-informedneuralnetworks:
Adeeplearningframeworkforsolvingforwardandinverseproblemsinvolvingnonlinearpartial
differentialequations. JournalofComputationalphysics,378:686–707,2019.
[7] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advances
inneuralinformationprocessingsystems,33:6840–6851,2020.
[8] QingqingZhao,DavidBLindell,andGordonWetzstein. Learningtosolvepde-constrained
inverseproblemswithgraphnetworks. arXivpreprintarXiv:2206.00711,2022.
[9] LawrenceCEvans. Partialdifferentialequations,volume19. AmericanMathematicalSociety,
2022.
[10] Kenji Omori and Jun Kotera. Overview of pdes and their regulation. Circulation research,
100(3):309–327,2007.
[11] RyanPo,WangYifan,VladislavGolyanik,KfirAberman,JonathanTBarron,AmitHBermano,
EricRyanChan,TaliDekel,AleksanderHolynski,AngjooKanazawa,etal. Stateofthearton
diffusionmodelsforvisualcomputing. arXivpreprintarXiv:2310.07204,2023.
[12] WalterAStrauss. Partialdifferentialequations: Anintroduction. JohnWiley&Sons,2007.
[13] LingYang,ZhilongZhang,YangSong,ShendaHong,RunshengXu,YueZhao,WentaoZhang,
BinCui,andMing-HsuanYang. Diffusionmodels: Acomprehensivesurveyofmethodsand
applications. ACMComputingSurveys,56(4):1–39,2023.
[14] AlfioQuarteroniandAlbertoValli. Numericalapproximationofpartialdifferentialequations,
volume23. SpringerScience&BusinessMedia,2008.
[15] SergioRIdelsohn,EugenioOnate,NestorCalvo,andFacundoDelPin. Themeshlessfinite
elementmethod. InternationalJournalforNumericalMethodsinEngineering,58(6):893–912,
2003.
[16] MaziarRaissi, ParisPerdikaris, andGeorgeEmKarniadakis. Physicsinformeddeeplearn-
ing(parti): Data-drivensolutionsofnonlinearpartialdifferentialequations. arXivpreprint
arXiv:1711.10561,2017.
[17] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis.
Physics-informed neural networks (pinns) for fluid mechanics: A review. Acta Mechanica
Sinica,37(12):1727–1738,2021.
[18] ZhipingMao,AmeyaDJagtap,andGeorgeEmKarniadakis. Physics-informedneuralnetworks
forhigh-speedflows. ComputerMethodsinAppliedMechanicsandEngineering,360:112789,
2020.
10[19] HamidrezaEivazi,MojtabaTahani,PhilippSchlatter,andRicardoVinuesa. Physics-informed
neural networks for solving reynolds-averaged navier–stokes equations. Physics of Fluids,
34(7),2022.
[20] Shengze Cai, Zhicheng Wang, Sifan Wang, Paris Perdikaris, and George Em Karniadakis.
Physics-informed neural networks for heat transfer problems. Journal of Heat Transfer,
143(6):060801,2021.
[21] GeorgeSMisyris,AndreasVenzke,andSpyrosChatzivasileiadis. Physics-informedneural
networksforpowersystems. In2020IEEEpower&energysocietygeneralmeeting(PESGM),
pages1–5.IEEE,2020.
[22] Zongyi Li, Nikola Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Otta, Moham-
mad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, et al.
Geometry-informedneuraloperatorforlarge-scale3dpdes. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[23] LouisSerrano,LiseLeBoudec,ArmandKassaïKoupaï,ThomasXWang,YuanYin,Jean-Noël
Vittaut,andPatrickGallinari. Operatorlearningwithneuralfields: Tacklingpdesongeneral
geometries. AdvancesinNeuralInformationProcessingSystems,36,2024.
[24] GegeWen,ZongyiLi,KamyarAzizzadenesheli,AnimaAnandkumar,andSallyMBenson.
U-fno—anenhancedfourierneuraloperator-baseddeep-learningmodelformultiphaseflow.
AdvancesinWaterResources,163:104180,2022.
[25] FannyLehmann,FilippoGatti,MichaëlBertin,andDidierClouteau. Fourierneuraloperator
surrogatemodeltopredict3dseismicwavespropagation. arXivpreprintarXiv:2304.10242,
2023.
[26] BianLi, HanchenWang, ShihangFeng, XiuYang, andYouzuoLin. Solvingseismicwave
equations on variable velocity models with fourier neural operator. IEEE Transactions on
GeoscienceandRemoteSensing,61:1–18,2023.
[27] ZhijieLi,WenhuiPeng,ZelongYuan,andJianchunWang. Fourierneuraloperatorapproach
tolargeeddysimulationofthree-dimensionalturbulence. TheoreticalandAppliedMechanics
Letters,12(6):100389,2022.
[28] WenhuiPeng,ZelongYuan,ZhijieLi,andJianchunWang. Linearattentioncoupledfourier
neuraloperatorforsimulationofthree-dimensionalturbulence. PhysicsofFluids,35(1),2023.
[29] Boris Bonev, Thorsten Kurth, Christian Hundt, Jaideep Pathak, Maximilian Baust, Karthik
Kashinath, and Anima Anandkumar. Spherical fourier neural operators: Learning stable
dynamicsonthesphere. InInternationalconferenceonmachinelearning,pages2806–2823.
PMLR,2023.
[30] PeterYichenChen,JinxuXiang,DongHeonCho,YueChang,GAPershing,HenriqueTeles
Maia, Maurizio M Chiaramonte, Kevin Carlberg, and Eitan Grinspun. Crom: Continu-
ous reduced-order modeling of pdes using implicit neural representations. arXiv preprint
arXiv:2206.02607,2022.
[31] ZiluLi,GuandaoYang,XiDeng,ChristopherDeSa,BharathHariharan,andSteveMarschner.
Neuralcachesformontecarlopartialdifferentialequationsolvers. InSIGGRAPHAsia2023
ConferencePapers,pages1–10,2023.
[32] ThomasMüller,FabriceRousselle,JanNovák,andAlexanderKeller. Real-timeneuralradiance
cachingforpathtracing. arXivpreprintarXiv:2106.12372,2021.
[33] HongChulNam, JuliusBerner, andAnimaAnandkumar. Solvingpoissonequationsusing
neuralwalk-on-spheres. InICLR2024WorkshoponAI4DifferentialEquationsInScience,2024.
[34] MCho,BJadamba,RKahler,AAKhan,andMSama. First-orderandsecond-orderadjoint
methods for the inverse problem of identifying non-linear parameters in pdes. Industrial
MathematicsandComplexSystems: EmergingMathematicalModels,MethodsandAlgorithms,
pages147–163,2017.
11[35] ColinFoxandGeoffNicholls. Statisticalestimationoftheparametersofapde. Can.appl.
Math.Quater,10:277–810,2001.
[36] BastianHarrach. Anintroductiontofiniteelementmethodsforinversecoefficientproblemsin
ellipticpdes. JahresberichtderDeutschenMathematiker-Vereinigung,123(3):183–210,2021.
[37] KrishnaKumarandYonjinChoi. Acceleratingparticleandfluidsimulationswithdifferentiable
graph networks for solving forward and inverse problems. In Proceedings of the SC’23
WorkshopsofTheInternationalConferenceonHighPerformanceComputing,Network,Storage,
andAnalysis,pages60–65,2023.
[38] TristanvanLeeuwenandFelixJHerrmann. Apenaltymethodforpde-constrainedoptimization
ininverseproblems. InverseProblems,32(1):015007,2015.
[39] DaLongandShandianZhe. Invertiblefourierneuraloperatorsfortacklingbothforwardand
inverseproblems. arXivpreprintarXiv:2402.11722,2024.
[40] Roberto Molinaro, Yunan Yang, Björn Engquist, and Siddhartha Mishra. Neural inverse
operatorsforsolvingpdeinverseproblems. arXivpreprintarXiv:2301.11167,2023.
[41] Maarten V de Hoop, Matti Lassas, and Christopher A Wong. Deep learning architectures
fornonlinearoperatorfunctionsandnonlinearinverseproblems. MathematicalStatisticsand
Learning,4(1):1–86,2022.
[42] SamiraPakravan,PouriaAMistani,MiguelAAragon-Calvo,andFredericGibou. Solving
inverse-pdeproblemswithphysics-awareneuralnetworks. JournalofComputationalPhysics,
440:110414,2021.
[43] ValeriiIakovlev,MarkusHeinonen,andHarriLähdesmäki. Learningcontinuous-timepdes
fromsparsedatawithgraphneuralnetworks. arXivpreprintarXiv:2006.08956,2020.
[44] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. arXiv
preprintarXiv:2010.02502,2020.
[45] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-basedgenerativemodels. InProc.NeurIPS,2022.
[46] HyungjinChung,JeongsolKim,MichaelTMccann,MarcLKlasky,andJongChulYe. Diffu-
sionposteriorsamplingforgeneralnoisyinverseproblems. arXivpreprintarXiv:2209.14687,
2022.
[47] HyungjinChung,ByeongsuSim,DohoonRyu,andJongChulYe. Improvingdiffusionmodels
forinverseproblemsusingmanifoldconstraints. AdvancesinNeuralInformationProcessing
Systems,35:25683–25696,2022.
[48] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.
Advancesinneuralinformationprocessingsystems,34:8780–8794,2021.
[49] SihanXu,YidongHuang,JiayiPan,ZiqiaoMa,andJoyceChai. Inversion-freeimageediting
withnaturallanguage. arXivpreprintarXiv:2312.04965,2023.
[50] ChristianJacobsen,YilinZhuang,andKarthikDuraisamy. Cocogen: Physically-consistentand
conditionedscore-basedgenerativemodelsforforwardandinverseproblems. arXivpreprint
arXiv:2312.10527,2023.
[51] Marten Lienen, David Lüdke, Jan Hansen-Palmus, and Stephan Günnemann. From zero
to turbulence: Generative modeling for 3d flow simulation. In The Twelfth International
ConferenceonLearningRepresentations,2023.
[52] GefanYangandStefanSommer. Adenoisingdiffusionmodelforfluidfieldprediction. arXiv
preprintarXiv:2301.11661,2023.
[53] PhillipLippe,BasVeeling,ParisPerdikaris,RichardTurner,andJohannesBrandstetter. Pde-
refiner: Achievingaccuratelongrolloutswithneuralpdesolvers. AdvancesinNeuralInforma-
tionProcessingSystems,36,2024.
12[54] DuleShu,ZijieLi,andAmirBaratiFarimani. Aphysics-informeddiffusionmodelforhigh-
fidelityflowfieldreconstruction. JournalofComputationalPhysics,478:111972,2023.
[55] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,and
Ben Poole. Score-based generative modeling through stochastic differential equations. In
InternationalConferenceonLearningRepresentations,2021.
[56] AapoHyvärinenandPeterDayan. Estimationofnon-normalizedstatisticalmodelsbyscore
matching. JournalofMachineLearningResearch,6(4),2005.
[57] YuanmingHu,LukeAnderson,Tzu-MaoLi,QiSun,NathanCarr,JonathanRagan-Kelley,and
FrédoDurand. Difftaichi: Differentiableprogrammingforphysicalsimulation. arXivpreprint
arXiv:1910.00935,2019.
13Appendix
A Overview
Inthissupplementarymaterial,weprovidedetailedadditionalcontentthatcomplementsthemain
paper. SectionBelaboratesontheimplementationofourdatagenerationprocess. SectionCincludes
thedetailedimplementationofoursamplingprogress,whileSectionDdelvesintothereductionsin
errorachievedbyintegratingPDEloss. SectionEincludescomprehensivevisualresultsforboth
forwardandinversecomputationsusingsparseobservations,notfeaturedinthemaintext. InSection
F,wediscussresultsfromfullobservationscenariosforallmethods. SectionGjustifiesourchoice
oftrainingthebaselinesoncompleteobservationdata. ToevaluatethestabilityofDiffusionPDE,
SectionHincludesthestandarddeviationofDiffusionPDE,andSectionIexaminestherobustnessof
ourDiffusionPDEmodelagainstrandomnoiseordifferentobservationlocations. SectionJexplores
theeffectsofvaryingobservationnumbersonresultaccuracy,providingadeeperunderstandingof
ourmodel’sperformanceunderdifferentconditions. Finally,SectionKstudiestheperformanceof
DiffusionPDEacrossdifferentresolutions.
B DataGenerationDetails
Wegenerate50,000samplesforeachPDEandalldiffusionmodelsaretrainedonNvidiaA40GPUs.
B.1 StaticPDEs
Wederivedthe methodsof datagenerationfor staticPDEs from[3]. Wefirstgenerate Gaussian
randomfieldson(0,1)2sothatµ∼N(0,(−∆+9I)−2). ForDarcyFlow,weleta=f(µ)sothat:
(cid:26)a(x)=12, ifµ(x)≥0
a(x)=3, ifµ(x)<0
ForthePoissonequationandHelmholtzequation, weleta = µasthecoefficients. Wethenuse
second-order finite difference schemes to solve the solution u and enforce the no-slip boundary
conditionforsolutionsbymultiplyingamollifiersin(πx )sin(πx )forpointx=(x ,x )∈(0,1)2.
1 2 1 2
Bothaanduhaveresolutionsof128×128.
B.2 Non-boundedNavier-StokesEquation
We derived the method to generate non-bounded Navier-Stokes equation from [3]. The initial
conditionw isgeneratedbyGaussianrandomfieldN(0,71.5(−∆+49I)−2.5).Theforcingfunction
0
followsthefixedpatternforpoint(x ,x ):
1 2
1
q(x)= (sin(2π(x +x ))+cos(2π(x +x )))
10 1 2 1 2
Wethenusethepseudo-spectralmethodtosolvetheNavier-Stokesequationsinthestream-function
formulation. WetransformtheequationsintothespectraldomainusingFouriertransforms,solving
thevorticityequationinthespectraldomain,andthenusinginverseFouriertransformstocompute
nonlineartermsinphysicalspace.Wesimulatefor1secondwith10timesteps,andw hasaresolution
t
of128×128.
B.3 BoundedNavier-StokesEquation
We use Difftaichi [57] to generate data for the bounded Navier-Stokes equation. Specifically,
weapplytheMarker-and-Cell(MAC)methodbysolvingapressure-Poissonequationtoenforce
incompressibility and iterating through predictor and corrector steps to update the velocity and
pressurefields. Thegridisoftheresolution128×128andthecenterofthecylinderisatarandom
locationin[30,60]×[30,90]witharandomradiusin[5,20]. Thefluidflowsintothegridfromthe
upperboundarywitharandominitialverticalvelocityin[0.5,3]. Wesimulatefor1secondwith10
timestepsandstudysteps4to8whentheturbulenceispassingthecylinder.
B.4 Burgers’Equation
WederivedthemethodtogenerateBurgers’equationfrom[3]. Theinitialconditionu isgenerated
0
byGaussianrandomfieldN(0,625(−∆+25I)−2). WesolvethePDEwithaspectralmethodand
simulate1secondwith127additionaltimesteps. Thefinalu spacehasaresolutionof128×128.
0:T
14C GuidedSamplingDetails
Forexperimentswithsparseobservationsorsensors,wefindthatDiffusionPDEperformsthebest
whenweightsζ areselectedasshowninTable2. Duringtheinitial80%ofiterationsinthesampling
process,guidanceisexclusivelyprovidedbytheobservationlossL . Subsequently,after80%of
obs
theiterationshavebeencompleted,weintroducethePDElossL ,andreducetheweightingfactor
pde
ζ fortheobservationloss,byafactorof10. Thisadjustmentshiftstheprimaryguidinginfluence
obs
tothePDEloss,therebyaligningthediffusionmodelmorecloselywiththedynamicsgovernedby
thepartialdifferentialequations.
Table2: TheweightsassignedtothePDElossandtheobservationlossvarydependingonwhether
theobservationspertaintothecoefficients(orinitialstates)aortothesolutions(orfinalstates)u.
DarcyFlow Poisson Helmholtz Non-bounded Bounded Burgers’
Navier-Stokes Navier-Stokes equation
a 2.5×103 4×102 2×102 5×102 2.5×102 3.2×102
ζ
obs u 106 2×104 3×104 5×102 2.5×102 -
ζ 103 102 102 102 102 102
pde
D ImprovementinPredictionthroughPDELossTerm
DiffusionPDEperformsbetterwhenweapplythePDElosstermL inadditiontotheobservation
pde
losstermL asguidance,asshowninTable3. Theerrorsinboththecoefficients(initialstates)a
obs
andthesolutions(finalstates)usignificantlydecrease. Wealsovisualizetherecoveredaanduand
correspondingabsoluteerrorsofDarcyFlow,Poissonequation,andHelmholtzequationinFig. 6. It
isdemonstratedthatthepredictionbecomesmoreaccuratewiththecombinedguidanceofPDEloss
andobservationlossthanwithonlyobservationloss.
Table3: DiffusionPDE’predictionerrorsofcoefficients(initialstates)aandsolutions(finalstates)u
withsparseobservationonbothaandu,guidedbydifferentlossfunctions.
LossFunction Side DarcyFlow Poisson Helmholtz Non-bounded Bounded
Navier-Stokes Navier-Stokes
a 4.6% 12.1% 13.2% 8.2% 6.4%
L
obs u 4.8% 6.5% 9.3% 7.6% 3.3%
a 3.4% 10.3% 9.4% 4.9% 1.7%
L +L
obs pde u 1.7% 0.3% 0.6% 0.6% 1.4%
Guided with  Guided with PDE Loss  Guided with  Guided with PDE Loss 
Observation Loss and Observation Loss Ground Truth Observation Loss and Observation Loss
Error Rate: 3.3% Error Rate: 2.6%
Relative Error: 5.1% Relative Error: 0.9%
(a)Recoveredcoefficients,solutions,andcorrespondingabsoluteerrorsofDarcyFlow.
15
stneiciffeoC
derevoceR
snoituloS
derevoceR
rorrE
etulosbA
tneiciffeoC
rorrE
etulosbA
noituloSGuided with  Guided with PDE Loss  Guided with  Guided with PDE Loss 
Observation Loss and Observation Loss Ground Truth Observation Loss and Observation Loss
Relative Error: 12.1% Relative Error: 10.3%
Relative Error: 6.5% Relative Error: 0.3%
(b)Recoveredcoefficients,solutions,andcorrespondingabsoluteerrorsofPoissonequation.
Guided with  Guided with PDE Loss  Guided with  Guided with PDE Loss 
Observation Loss and Observation Loss Ground Truth Observation Loss and Observation Loss
Relative Error: 13.2% Relative Error: 9.4%
Relative Error: 9.3% Relative Error: 0.6%
(c)Recoveredcoefficients,solutions,andcorrespondingabsoluteerrorsofHelmholtzequation.
Figure6: Recoveredcoefficients,solutions,andtheircorrespondingvisualizedabsoluteerrorsfor
variousPDEfamilies.
E AdditionalResultsonAllPDEswithSparseObservation
WepresenttherecoveredresultsofanotherBurgers’equationinFig. 7. DiffusionPDEoutperforms
allothermethodswith5sensorsforcontinuousobservation. Wealsopresenttherecoveredresults
forboththeforwardandinverseproblemsofallotherPDEswithsparseobservations,asshownin
Fig. 8. Specifically,wesolvetheforwardandinverseproblemsfortheDarcyFlow,Poissonequation,
Helmholtzequation,andnon-boundedNavier-Stokesequationusing500randompointsobserved
ineitherthesolutionspaceorthecoefficientspace. Additionally,fortheboundedNavier-Stokes
equation,weobserve1%ofthepointsinthevelocityfield. OurfindingsindicatethatDiffusionPDE
outperformsallothermethods,providingthemostaccuratesolutions.
AdditionalDataSettingforDarcyFlow Tofurtherdemonstratethegeneralizationcapabilityof
ourmodel,weconductedadditionaltestsondifferentdatasettingsforDarcyFlow. InFig. 9,we
solvetheforwardandinverseproblemsofDarcyFlowwith500observationpoints,adjustingthe
binaryvaluesofato20and16insteadoftheoriginal12and3inSectionB,i.e.,
(cid:26)a(x)=20, ifµ(x)≥0
a(x)=16, ifµ(x)<0
Our results indicate that DiffusionPDE performs equally well under these varied data settings,
showcasingitsrobustnessandadaptability.
16
stneiciffeoC
derevoceR
snoituloS
derevoceR
stneiciffeoC
derevoceR
snoituloS
derevoceR
rorrE
etulosbA
tneiciffeoC
rorrE
etulosbA
noituloS
rorrE
etulosbA
tneiciffeoC
rorrE
etulosbA
noituloSObservations Ground Truth DiffusionPDE DeepONet PINO FNO PINNs
5 Random Sensors
x Relative Error: 5.1% Relative Error: 78.2% Relative Error: 96.2% Relative Error: 96.3% Relative Error: 81.0%
Figure7: ResultsofanotherBurgers’equationrecoveredby5sensorsthroughoutthetimeinterval.
Observations Ground Truth DiffusionPDE DeepONet PINO FNO PINNs
Forward
y
x 500 Random Points Relative Error: 2.5% Relative Error: 38.3%Relative Error: 35.2% Relative Error: 28.2% Relative Error: 48.8%
Inverse
y
x Error Rate: 3.2% Error Rate: 41.1% Error Rate: 49.2% Error Rate: 49.3% Error Rate: 59.7%
(a)ForwardandinverseresultsofDarcyFlowrecoveredby500observationpoints.
Observations Ground Truth DiffusionPDE DeepONet PINO FNO PINNs
Forward
y
x 500 Random Points Relative Error: 4.5%Relative Error: 155.5%Relative Error: 107.1%Relative Error: 100.9%Relative Error: 128.1%
Inverse
y
x Relative Error: 20.0%Relative Error: 105.8%Relative Error: 231.9%Relative Error: 232.7%Relative Error: 130.0%
(b)ForwardandinverseresultsofPoissonequationrecoveredby500observationpoints.
Observations Ground Truth DiffusionPDE DeepONet PINO FNO PINNs
Forward
y
x 500 Random Points Relative Error: 8.8%Relative Error: 123.1%Relative Error: 106.5%Relative Error: 98.2%Relative Error: 142.3%
Inverse
y
x Relative Error: 22.6%Relative Error: 132.8%Relative Error: 216.9%Relative Error: 218.2%Relative Error: 160.0%
(c)ForwardandinverseresultsofHelmholtzequationrecoveredby500observationpoints.
Observations Ground Truth DiffusionPDE DeepONet PINO FNO PINNs
Forward
y
x 500 Random Points Relative Error: 5.7%Relative Error: 100.1%Relative Error: 99.5%Relative Error: 99.1%Relative Error: 141.1%
Inverse
y
x Relative Error: 9.5% Relative Error: 96.9%Relative Error: 98.2%Relative Error: 98.1%Relative Error: 150.7%
(d)Forwardandinverseresultsofanothernon-boundedNavier-Stokesequationrecoveredby500observation
points.
17
emiT
stneiciffeoC
snoituloS
stneiciffeoC
snoituloS
stneiciffeoC
snoituloS
etatS
laitinI
etatS
laniF
snoituloS
stneiciffeoC
snoituloS
stneiciffeoC
snoituloS
stneiciffeoC
etatS
laniF
etatS
laitinIObservations Ground Truth DiffusionPDE DeepONet PINO FNO PINNs
Forward
y
x 1% Random Points Relative Error: 3.9% Relative Error: 97.7%Relative Error: 81.1% Relative Error: 82.8%Relative Error: 100.1%
Inverse
y
x Relative Error: 2.7% Relative Error: 91.9%Relative Error: 69.5%Relative Error: 69.6%Relative Error: 105.5%
(e)ForwardandinverseresultsofboundedNavier-Stokesequationrecoveredby1%observationpoints.
Ground Truth Observed Final State Cylinder Boundary GraphPDE DiffusionPDE Ground Truth
Inverse
1% Random Points Relative Error: 10.5% Relative Error: 5.6%
(f)InverseresultsofDiffusionPDEandGraphPDEofanotherboundedNavier-Stokesequationrecoveredby1%
observationpointsandtheknownboundaryofthecylinder.
Figure8:ResultsofforwardandinverseproblemsfordifferentPDEfamilieswithsparseobservation.
Observed Coefficients Recovered Solutions Ground Truth Observed Solutions Recovered Solutions Ground Truth
Forward Inverse
500 Random Points Relative Error: 2.0% 500 Random Points Error Rate: 4.9%
Figure9: ForwardandinverseresultsofDarcyFlowrecoveredby500observationpointsundera
differentdatasetting.
F SolvingForwardandInverseProblemswithFullObservation
Wehavealsoincludedtheerrorsofallmethodswhensolvingboththeforwardandinverseproblems
withfullobservation,asdisplayedinTable4.
Table4: Relativeerrorsofsolutions(orfinalstates)andcoefficients(orinitialstates)whensolving
forwardandinverseproblemswithfullobservations. Errorratesareusedfortheinverseproblemof
DarcyFlow.
DiffusionPDE PINO DeepONet PINNs FNO
Forward 2.2% 4.0% 12.3% 15.4% 5.3%
DarcyFlow
Inverse 2.0% 2.1% 8.4% 10.1% 5.6%
Forward 2.7% 3.7% 14.3% 16.1% 8.2%
Poisson
Inverse 9.8% 10.2% 29.0% 33.1% 13.6%
Forward 2.3% 4.9% 17.8% 18.1% 11.1%
Helmholtz
Inverse 4.0% 4.9% 28.1% 29.2% 5.0%
Non-bounded Forward 6.1% 1.1% 25.6% 27.3% 2.3%
Navier-Stokes Inverse 8.6% 6.8% 19.6% 27.8% 6.8%
Bounded Forward 1.7% 1.9% 13.3% 18.6% 2.0%
Navier-Stokes Inverse 1.4% 2.9% 6.1% 7.6% 3.0%
18
etatS
laitinI
etatS
laniF
etatS
laniF
etatS
laniF
etatS
laitinI
etatS
liatinIIngeneral,DiffusionPDEandPINOoutperformallothermethods,andDiffusionPDEperformsthe
bestforallstaticPDEs. DiffusionPDEiscapableofsolvingbothforwardandinverseproblemswith
errorsoflessthan10%forallclassesofdiscussedPDEsandiscomparabletothestate-of-the-art.
G TrainingBaselinesMethodsonPartialInputs
Forourmainexperiments,weopttotrainthebaselinemodels(PINO,DeepONet,PINNs,FNO)on
fullobservationsforseveralcompellingreasons: First,physics-informedmodelssuchasPINNsand
PINOareunabletoeffectivelycomputethePDElosswhenonlysparseobservationsareavailable.
Second,othermodelslikeDeepONetandFNOperformpoorlywithsparseobservations. Forinstance,
trainingtheDeepONetmodelon500uniformlyrandompointsforeachtrainingsampleinthecontext
of the forward problem of Darcy Flow leads to testing outcomes that are consistently similar, as
illustratedinFig. 10,regardlessofthetestinginput. Thispatternsuggeststhatthemodeltendsto
generateageneralizedsolutionthatminimizestheaverageerroracrossallpotentialsolutionsrather
thanconvergingbasedonspecificsamples. Furthermore,thepartial-input-trainedmodelexhibits
poorgeneralizationwhenfacedwithadifferentdistributionofobservationsfromtraining,indicating
thatitlacksflexibility—acriticalattributeofourDiffusionPDE.
500 Observed Points 1000 Observed Points Full Observation
Figure10: PredictedsolutionsobtainedusingtheDeepONetmodeltrainedwith500observation
pointsacrossdifferentnumbersofobservationpoints.
H StandardDeviationofDiffusionPDEExperimentResults
WefurtherassessthestatisticalsignificanceofourDiffusionPDEbyanalyzingthestandarddevia-
tionsforforwardandinverseproblemsunderconditionsof500sparseobservationpointsandfull
observation,respectively,asdetailedinTable5. Weevaluateourmodelusingtestsetscomprising
1,000samplesforeachPDE.Ourfindingsconfirmthatfullobservationenhancesthestabilityof
theresults,apredictableoutcomeasvariabilitydiminisheswithanincreaseinobservationpoints.
ThestandarddeviationsarenotablyhigherformorecomplexPDEs,suchastheinverseproblems
of the Poisson and Helmholtz equations, reflecting the inherent challenges associated with these
computations. Overall,DiffusionPDEdemonstratesconsiderablestability,evidencedbyrelatively
lowstandarddeviationsacrossvarioustests.
Table5: StandarddeviationofDiffusionPDEwhensolvingforwardandinverseproblemswithsparse
orfullobservations.
SparseObservations FullObservations
Forward 2.5±0.7% 2.2±0.1%
DarcyFlow
Inverse 3.2±0.9% 2.0±0.1%
Forward 4.5±0.9% 2.7±0.1%
Poisson
Inverse 20.0±1.8% 9.8±0.7%
Forward 8.8±1.0% 2.3±0.1%
Helmholtz
Inverse 22.6±1.7% 4.0±0.6%
Non-bounded Forward 6.9±0.9% 6.1±0.2%
Navier-Stokes Inverse 10.4±1.0% 8.6±0.3%
Bounded Forward 3.9±0.2% 1.7±0.1%
Navier-Stokes Inverse 2.7±0.2% 1.4±0.1%
19Figure11: RelativeerrorsofrecoveredDarcyFlowsolutionswithsparsenoisyobservation.
I RobustnessofDiffusionPDE
WefindthatDiffusionPDEisrobustagainstsparsenoisyobservation. InFig. 11,weaddGaussian
noisetothe500observedpointsofDarcyFlowcoefficients.OurDiffusionPDEcanmaintainarelative
error of around 10% with a 15% noise level concerning the forward problem, and the recovered
solutions are shown in Fig. 12. Baseline methods such as PINO also exhibit robustness against
randomnoiseundersparseobservationconditions;thisisattributedtotheirlimitedapplicabilityto
sparseobservationproblems,leadingthemtoaddresstheprobleminamorerandomizedmanner.
Clean Observations No Noise 1% Noise 3% Noise 5% Noise 10% Noise 15% Noise
Forward
Add Noise
Relative Error: 2.5% Relative Error: 6.3% Relative Error: 7.8% Relative Error: 7.4% Relative Error: 7.6% Relative Error: 10.1%
Figure12: RecoveredsolutionsforDarcyFlowwithnoisyobservations.
RobustnessonSamplingPatterns Moreover,asmentionedinthemaindocument,weinvestigate
the robustness of DiffusionPDE on different sampling patterns of the observation points. Here,
weaddresstheforwardproblemofDarcyFlowusing500observedcoefficientpoints, whichare
non-uniformlyconcentratedontheleftandrightsidesorareregularlydistributedacrossthegrid,as
depictedinFig.13.OurresultsdemonstratethatDiffusionPDEflexiblysolvesproblemswitharbitrary
sparseobservationlocationswithinthespatialdomain,withoutre-trainingtheneuralnetworkmodel.
Observed Coefficients Recovered Solutions Observed Coefficients Recovered Solutions Observed Coefficients Recovered Solutions
Forward Forward Forward
Relative Error: 6.2% Relative Error: 8.3% Relative Error: 2.3%
Figure 13: Recovered solutions for Darcy Flow with observations sampled using non-uniform
distributions.
20
stneiciffeoC
snoituloS
derevoceRJ SolvingForwardandInverseProblemswithDifferentNumbersof
Observations
WealsoinvestigatehowourDiffusionPDEhandlesvaryingdegreesofsparseobservation. Experi-
mentsareconductedontheDarcyFlow,Poissonequation,Helmholtzequation,andnon-bounded
Navier-Stokesequation. WeexaminetheresultsofDiffusionPDEinsolvingforwardandinverse
problemswhenthereare100, 300, 500, and1000randomobservationsona, u, orbothaandu,
as shown in Fig. 14. We have observed that the error of DiffusionPDE decreases as the number
ofsparseobservationsincreases. Overall,werecoverubetterthana. DiffusionPDEcanrecoveru
withapproximately2%observationpointsatanysideprettywell. DiffusionPDEisalsocapableof
recoveringbothaanduwitherrors1% ∼ 10%withapproximately6%observationpointsatany
sideformostPDEfamilies. WealsoconcludethatourDiffusionPDEbecomesinsensitivetothe
numberofobservationsoncemorethan3%ofthepointsareobserved.
(a)ErrorratesforDarcyFlowandrelativeerrorsforotherPDEsofrecoveredcoefficientsorinitialstatesa.
(b)Relativeerrorsofrecoveredsolutionsorfinalstatesu.
Figure14: Errorrateorrelativeerrorofbothcoefficients(orinitialstates)aandsolutions(orfinal
states)uwithdifferentnumbersofobservations.
21K SolvingForwardandInverseProblemsacrossVariedResolutions
ToevaluatethegeneralizabilityofDiffusionPDE,weimplementedthemodelonvariousresolutions,
including64×64and256×256,whilemaintainingthesamepercentageofobservedpoints. For
resolutionsof64×64,128×128,and256×256,weobserve125,500,and2000pointsonaoru
respectively,whichareapproximately3%foreachresolution. Overall,DiffusionPDEiscapableof
handlingdifferentresolutionseffectively. Forinstance,Table6presentstheforwardrelativeerrors
ofthesolutionuandinverseerrorratesofthecoefficientafortheDarcyFlow,demonstratingthat
DiffusionPDEperformsconsistentlywellwithsimilarerrorratesacrossvariousresolutions.
Table6: ForwardrelativeerrorsandinverseerrorratesofDarcyFlowacrossdifferentresolutions.
Resolution ForwardRelativeError InverseErrorRate
64×64 2.9% 4.3%
128×128 2.5% 3.2%
256×256 3.1% 4.1%
22