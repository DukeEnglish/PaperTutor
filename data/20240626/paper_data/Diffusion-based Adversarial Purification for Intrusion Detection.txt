Diffusion-based Adversarial Purification for
Intrusion Detection
Mohamed Amine Merzouk1,2, Erwan Beurier1,2, Reda Yaich2, Nora
Boulahia-Cuppens1, and Frédéric Cuppens1
1 Polytechnique Montréal, Montréal, Canada
{mohamed-amine.merzouk,erwan.beurier,
nora.boulahia-cuppens,frederic.cuppens}@polymtl.ca
2 IRT SystemX, Palaiseau, France
{mohamed-amine.merzouk,erwan.beurier,
reda.yaich}@irt-systemx.fr
Abstract. Theescalatingsophisticationofcyberattackshasencouraged
theintegrationofmachinelearningtechniquesinintrusiondetectionsys-
tems,buttheriseofadversarialexamplespresentsasignificantchallenge.
These crafted perturbations mislead ML models, enabling attackers to
evade detection or trigger false alerts. In response, adversarial purifica-
tion has emerged as a compelling solution, particularly with diffusion
models showing promising results. However, their purification potential
remains unexplored in the context of intrusion detection. This paper
demonstratestheeffectivenessofdiffusionmodelsinpurifyingadversar-
ial examples in network intrusion detection. Through a comprehensive
analysis of the diffusion parameters, we identify optimal configurations
maximizing adversarial robustness with minimal impact on regular per-
formance. Importantly, this study reveals insights into the relationship
between diffusion noise and diffusion steps, representing a novel con-
tribution to the field. Our experiments are carried out on two datasets
andagainstfiveadversarialattacks.Theimplementationcodeispublicly
available.
Keywords: Adversarial defense · Adversarial purification · Adversarial
examples · Diffusion models · Intrusion detection.
1 Introduction
Intrusion detection stands out as one of the most formidable challenges in cy-
bersecurity, especially with the increasing sophistication of cyberattacks. Unfor-
tunately, traditional signature-based approaches reach their limit against previ-
ouslyunknownthreats,alsocalledzero-days.Assuch,theintegrationofMachine
Learning (ML) techniques has emerged as a promising avenue for enhancing the
detection capabilities of intrusion detection systems.
However, the advent of adversarial examples [27,5] poses a severe obstacle
to the reliability of ML, specifically in critical tasks such as intrusion detection.
Theyaregeneratedfromregulardatainstancesbyaddingameticulouslycrafted
4202
nuJ
52
]RC.sc[
1v60671.6042:viXra2 Merzouk et al.
perturbation that misleads an ML model. Applied to network data, they enable
cyber attackers to either evade ML-based intrusion detection systems or flood
the network with false alerts.
In response to the escalating threat of adversarial attacks, research efforts
have been directed toward designing effective countermeasures. Among the var-
ious defensive approaches, adversarial purification has emerged as a compelling
solution to remove the adversarial perturbation from data before processing it.
This defense is particularly interesting for intrusion detection, as it can be inte-
grated upstream of the model without retraining. Recent work [18] has demon-
strated promising purification performance using diffusion models [22,8].
Diffusionmodelsaregenerativemodelsinspiredbythedynamicsofdiffusion
processes in physics [22]. They consist of a forward process that gradually adds
noise to initial data and a backward process that reconstructs that data using a
deepneuralnetwork.Becausediffusionmodelsaretrainedusingexamplesdrawn
fromtheoriginaldatadistribution,thereconstructeddataisexpectedtoadhere
to the same distribution, even when the initial data is an adversarial example.
Thus, they can be used for adversarial purification: removing the perturbation
fromadversarialexamplestoclassifythemcorrectly.Furthermore,sincetheyare
not explicitly trained on adversarial examples, their purification performance is
not limited to a specific adversarial attack.
Despite the recent attention given to adversarial purification with diffusion
models, there remains a notable gap in understanding their potential in the
contextofintrusiondetection.Thispaperfillsthatgapbystudyingthepurifica-
tioneffectsofdiffusionmodelsanddemonstratingtheireffectivenessonnetwork
intrusiondetection.Byconductingacomprehensiveanalysisofthediffusionpa-
rameters, we identify optimal configurations maximizing the adversarial robust-
ness with limited impact on the regular performance. We show the effectiveness
of our method on two network datasets (UNSW-NB15 [17] and NSL-KDD [29])
andagainststate-of-the-artadversarialattacks.Moreover,toourknowledge,this
isthefirstinvestigationoftherelationshipbetweenthenumberofdiffusionsteps
and the optimal amount of diffusion noise for adversarial purification. Our find-
ings demonstrate that the optimal amount of diffusion noise depends not on the
number of diffusion steps but rather on the amount of adversarial perturbation.
Section 2 introduces the background and related works in cybersecurity. In
Section 3, we describe the experiment methodology. We report the results in
Section 4, followed by a discussion in Section 5 and a conclusion in Section 6.
2 Background and Related Work
This section introduces the major approaches to adversarial defense, provides
some background on diffusion models, and reviews the literature on diffusion
models in intrusion detection systems and adversarial purification.
Adversarial Defenses. The following presents three dominant approaches to
defend against adversarial examples [34].Diffusion-based Adversarial Purification for Intrusion Detection 3
Adversarialtraining consistsoftrainingthemodelwithadversarialexamples
inadditiontothetrainingdata[5,14].Despiteitseffectiveness,thistechniquehas
several drawbacks: (i) it requires the retraining of the model and significantly
lengthens the training duration, and (ii) it protects only against adversarial
examples generated with the methods it was trained on.
Adversarial detection consists of a separate classifier deployed upstream of
the ML model that detects and discards adversarial examples before they are
fed to the model [1]. It is plug-and-play and does not require retraining. Several
supervisedandunsupervisedtechniquesexist;interestedreadersmayreferto[1]
for a detailed review. Unfortunately, adversarial detection also depends on the
adversarial attacks on which it was trained. Moreover, since it is a classifier, it
can be fooled to some extent [2].
Adversarial purification consists of a separate model deployed upstream of
the ML model that removes the perturbation from adversarial examples be-
fore they are fed to the model [24,26]. This approach is also plug-and-play, and
the purification models are typically trained independently of the ML model.
Adversarial purification does not require retraining, and it is, in many cases,
independent of the adversarial attacks. This paper focuses on the adversarial
purification approach using diffusion models [18].
DiffusionModels. Diffusionmodelsareaclassofgenerativemodelsthatlever-
age the diffusion processes used in physics to learn complex data distributions
andsamplesfromthem[22,8].InsteadofmodelingthedistributionlikeVAEs[10]
andGANs[4],theymodeltheprocessoftransformingasimpledistribution(e.g.,
Gaussian noise) into the target distribution through a sequence of steps. They
consist of two Markov processes: a forward and a reverse process.
In the forward process, each step involves adding Gaussian noise to the data
over T steps until no structure remains; it corresponds to a smooth transition
from the complex data distribution to a Gaussian distribution (latent space).
The amount of noise added at each step depends on the variance schedule β. In
the reverse process, each step reverts the corresponding forward step—removes
the diffusion noise—to reconstruct the original data distribution. Since the re-
verseprocessismathematicallyintractable,itisapproximatedwithdeepneural
networks. In this work, we use discrete diffusion steps, but the continuous case
can also apply; it requires solving stochastic differential equations [25].
In the following, we provide the intuition into the theory behind diffusion
models. We consider a dataset x with unknown distribution x ∼q(x ). For a
0 0 0
givennumberT ofsteps, weconsider theMarkovchain(x ) withtransitions
t t≤T
(cid:16) (cid:112) (cid:17)
q(x |x )=N x ; 1−β x ,β I , (1)
t+1 t t+1 t+1 t t+1 n
that is, we gradually add Gaussian noise with a given variance (β ) to the
t t≤T
data. If we define α¯ =
(cid:81)i=t
(1−β ), then the cumulative noise addition from
t i=1 i
the clean data to step t is written:
√
(cid:0) (cid:1)
q(x |x )≃N x ; α¯x ,(1−α¯ )I . (2)
t 0 t t 0 t n4 Merzouk et al.
Equation 2 describes the forward process of diffusion models. Note that to
ensure the diversity of generated data the variance schedule should guarantee
thatthedataresemblesaGaussiandistributionattheendoftheforwardprocess:
q(x |x )≃N (x ;0,I ). (3)
T 0 T n
The reverse process consists of generating examples from the original data
distribution using the reverse Markov chain; it starts from a Gaussian distribu-
tion with transitions
p(x |x )=N (x ;µ (x ,t),Σ (x ,t)), (4)
t t+1 t+1 θ t θ t
where θ represents the parameters of the deep neural network used to estimate
the diffusion noise to be removed.
Adversarial Purification with Diffusion Models. Adversarial purification
istheprocessofremovingtheperturbationfromadversarialexamplestoclassify
them correctly. This process can be seen as a generative task and approached
with diffusion models. The gradual addition of Gaussian noise in the forward
step submerses the adversarial perturbation, but the data becomes too noisy to
becorrectlyclassified.Thus,thereversestepreconstructsthedataintheoriginal
distribution without adversarial perturbations.
Furthermore, the forward process does not need to complete T steps and
reach a Gaussian distribution. There should be enough added noise to submerse
theadversarialperturbation,butnottoomuch,asitdamagesthedatastructure
and decreases the accuracy. The forward process should stop at the optimal
diffusion step t∗, where the diffusion noise suffices to remove the adversarial
perturbation while preserving the structure for the classification [18].
After this concept was introduced in [18], later work leveraged guided dif-
fusion models for adversarial purification [30,32]. Authors in [12] train a robust
guidance with an adversarial loss and apply it to the reverse process. Diffusion
models are also used to purify backdoors in poisoned models [21].
DiffusionModelsinIntrusionDetection. Intrusiondetectionsystemsbene-
fitgreatlyfromtheautomationprovidedbyML,includingdeeplearning[13,23,7].
However, network intrusion datasets are often imbalanced; benign traffic out-
weighsmalicioustraffic.Duetotheirgenerativecapabilities,diffusionmodelsare
successfullyappliedindataaugmentationforbalancingnetworkdatasets[35,28,6].
The diffusion model can also detect intrusion by learning the distribution of be-
nign traffic. The difference between the original and reconstructed data is then
used to detect malicious traffic [31,33].
However,toourknowledge,nopriorresearchhasinvestigatedtheadversarial
purification potential of diffusion models in the context of intrusion detection.
This paper represents the foremost initiative to address diffusion-based adver-
sarial purification in intrusion detection.Diffusion-based Adversarial Purification for Intrusion Detection 5
Diffusion model
Intrusion
Adversarial forward steps reverse step
detection
Dataset perturbation (adding noise) (reconstructing)
model
Fig.1: Methodology scheme: dataset instances x undergo adversarial perturbation,
0
the diffusion model’s purification, and then the intrusion detection classification.
3 Methodology
As shown in Figure 1, our methodology consists of a diffusion model deployed
upstream of the intrusion detection model. The diffusion model serves as a "fil-
ter"thatremovesadversarialperturbations.Itaddsnoisetothedataforseveral
steps t and reconstructs it through the reverse diffusion process involving the
diffusion neural network. The data, whether original or adversarial, are first pu-
rified by the diffusion model and then fed to the intrusion detection model to
determine if they are benign or malicious.
The intrusion detection model is a fully connected neural network with fixed
hyperparameters throughout the experiments. It comprises a fully connected
neural network with 5 hidden layers of 256, 512, 1024, 512, and 256 Rectified
Linear Units (ReLU). It is trained for 10,000 epochs, and the parameters are
optimized using Adam with a learning rate of 10−5.
The diffusion models are trained according to [8, Algorithm 1]. We consider T
discrete diffusion steps. Instead of having a separate neural network for each
timestep [22], we encode the timesteps as sinusoidal embeddings and add them
to each layer of the diffusion neural network [8].
The purification results are recorded across the diffusion steps. For each t∈
[1,T], we apply t forward diffusion steps to the data instance x to get x ; then
0 t
we apply t reverse diffusion steps to x to get xˆ , the reconstruction of x .
t 0 0
The variance schedule β is linearly distributed (T evenly spaced values between
β and β ). Across different experiments, the number of diffusion steps T is 100
1 T
or 1000, while β varies between 10−5 and 10−4, and β varies between 10−4
1 T
and 10−1.
The diffusion neural networks are fully connected neural networks with 10 hid-
denlayers;eachhiddenlayertypicallyconsistsof960ReLUunits,exceptforthe
experiments that compare neural network architectures. The diffusion neural
networks are trained for 200,000 epochs, where each epoch consists of predict-
ing a random step of the reverse process for each dataset instance. The loss
function is a Mean Squared Error (MSE), and the parameters are optimized
using AdamW with a learning rate of 10−4.6 Merzouk et al.
0.5 0.5
512 UPL MSE=0.026
960 UPL MSE=0.009
1024 UPL MSE=0.009
0.4 0.4 2048 UPL MSE=0.007
0.3 512 UPL MSE=0.362 0.3
960 UPL MSE=0.009
1024 UPL MSE=0.008
2048 UPL MSE=0.008 0.2 0.2
0.1 0.1
0.0 0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000
Epochs Epochs
(a)UNSW-NB15 (b)NSL-KDD
Fig.2: Reconstruction loss over training epochs for different neural network sizes
The metrics recorded during the experiments evaluate two aspects of diffusion
models: (i) the reconstruction performance by recording the reconstruction loss
(MSE between the original data and the reconstructed data) for adiffusionstep
t, and (ii) the adversarial purification performance by feeding the reconstructed
data to the intrusion detection model and recording its accuracy.
The optimal diffusion step t∗ ∈ [1,T] is the step that maximizes the intrusion
detection accuracy on adversarial examples [18]. It should be large enough to
dilute the adversarial perturbation with diffusion noise. However, the larger it
is, the more data structure it dilutes, which decreases the test accuracy.
All experiments are carried out on two prominent network datasets: NSL-
KDD [29], which is old but still widely used for benchmarking and comparison,
and UNSW-NB15 [17] which is more recent and representative of modern net-
work traffic [20]. For further details on the implementation of our experiments,
we make our code publicly available 3.
4 Results
In this section, we present the results of our experiments. We first analyze the
reconstruction loss throughout the training of the diffusion models to identify
optimal hyperparameters. Then, we study the reconstruction loss and the accu-
racy of the intrusion detection model on reconstructed data. We compare the
robustness of the intrusion detection model with respect to the level of diffusion
applied to the data. We aim to find the optimal diffusion step t∗ that removes
the adversarial perturbation and increases the robustness while minimizing the
repercussions on non-adversarial data.
3 https://github.com/mamerzouk/adversarial-purification
rorrE
derauqS
naeM
rorrE
derauqS
naeMDiffusion-based Adversarial Purification for Intrusion Detection 7
0.10 0.10
Training Training
Testing Testing
Adversarial Adversarial
0.08 0.08
0.06 0.06
0.04 0.04
0.02 0.02
0.00 0.00
0 200 400 600 800 1000 0 200 400 600 800 1000
Diffusion step (t) Diffusion step (t)
(a)UNSW-NB15 (b)NSL-KDD
Fig.3: Reconstruction loss over the diffusion steps t
Furthermore, we analyze the impact of several diffusion parameters on the
purificationperformance:thenumberofstepsT,theinitialvarianceβ ,andthe
1
final variance β . Finally, we study the impact of the adversarial perturbation
T
amplitudeϵontheoptimaldiffusionstept∗ andcompareourpurificationmodel
against five state-of-the-art adversarial attacks. The figures present the results
on both UNSW-NB15 and NSL-KDD; the values are the mean and standard
deviation over 10 randomly initialized runs.
Unless otherwise noted, the diffusion models in these experiments use the
standard diffusion parameters proposed in the previous work [8]: β = 10−4,
1
β =0.02, and T =1000.
T
Diffusion neural network size. Here, we analyze the training of diffusion
models on network data. We specifically compare the impact of the size of the
diffusion neural network on the training loss. We elaborate further on the im-
pact of the number of diffusion steps on the training loss in Appendix A. Figure
2 shows the reconstruction loss (in MSE) over the training epochs for different
UnitsPerLayer(UPL).Weobservethatthereconstructionlossdecreasesearlier
for larger diffusion neural networks. Furthermore, larger neural networks reach
lower MSE values when they stabilize, 0.008 and 0.007 with 2048 UPL against
0.362 and 0.026 with 512 UPL on UNSW-NB15 and NSL-KDD, respectively.
This is due to the capacity of larger neural networks to model more complex
patterns and learn better representations of the data. However, larger models
have drawbacks; they take significantly longer to train and require more com-
putational power. Larger models also display instabilities during the training;
Figure 2 shows how the 2048 UPL diffusion model loss peaks randomly between
150,000 and 200,000 epochs on UNSW-NB15. For the rest of the experiments,
rorrE
derauqS
naeM
rorrE
derauqS
naeM8 Merzouk et al.
1.0 1.0
Testing Testing
Training Training
Adversarial Adversarial
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0 200 400 600 800 1000 0 200 400 600 800 1000
Diffusion step (t) Diffusion step (t)
(a)UNSW-NB15 (b)NSL-KDD
Fig.4: Intrusion detection accuracy over the diffusion steps t
we will rely on diffusion neural networks with 10 hidden layers of 960 UPL
trained for 200,000 epochs.
Reconstructionlossandaccuracyovert. Figure3showsthereconstruction
lossoverthediffusionstepst,whileFigure4showstheaccuracyoftheintrusion
detection model on the same reconstructed data over the diffusion steps t.
Figure 3 shows that the reconstruction loss increases similarly for all three
sets,thelinesevenoverlaponUNSW-NB15.Indeed,asnoiseisaddedgradually,
the data structure is slowly destroyed. The more diffusion steps are applied, the
morenoiseisadded,andtheharderitistoreconstructpreciselythedata.After
600 steps, the reconstruction loss plateaued around 0.03 and 0.07 on UNSW-
NB15 and NSL-KDD, respectively. Moreover, we notice that the reconstruction
loss on adversarial examples is slightly superior to that on the original testing
set. The difference in the reconstruction losses can also be used as an indicator
fordetectingadversarialexamples.Thisavenueisnotinvestigatedinthispaper,
as adversarial detection approaches are out of our scope.
TheaccuracycurveinFigure4corroboratesthepreviousresults:thetraining
andtestingaccuracydecreaseasthediffusionstepincreasesduetothedamaged
datastructure.Itbecomesmorechallengingfortheintrusiondetectionmodelto
distinguish benign and malicious traffic, which decreases its accuracy. After 600
steps, the reconstructed data is too noisy to be classified correctly.
Purification performance. In order to evaluate the robustness of the intru-
siondetectionafterthediffusionmodel’spurification,wefocusontheredlinein
Figure 4, which represents the accuracy of the intrusion detection model on ad-
versarial examples. Those adversarial examples were generated from the testing
ycaruccA ycaruccADiffusion-based Adversarial Purification for Intrusion Detection 9
0.85 0.85
0.80 0.80
0.75 0.75
0.70 0.70
0.65 1=104 T=102 t*=46 [77.9%±0.5] 0.65 1=104 T=102 t*=54 [78.4%±0.5]
1=104 T=104 t*=43 [81.0%±0.1] 1=104 T=104 t*=23 [78.5%±0.3]
1=105 T=102 t*=43 [78.7%±0.6] 1=105 T=102 t*=35 [79.0%±0.6]
1=105 T=104 t*=38 [73.2%±0.9] 1=105 T=104 t*=18 [77.3%±0.5]
0.60 0.60
0 20 40 60 80 100 0 20 40 60 80 100
Diffusion step (t) Diffusion step (t)
(a)UNSW-NB15 (b)NSL-KDD
Fig.5: Intrusion detection accuracy over diffusion step t for different β . Continuous
1
linesforβ =10−2anddottedlinesforβ =10−4.Themarkerindicatesthemaximum
T T
accuracy reached at the optimal diffusion step t∗.
set using the targeted Fast Gradient Sign Method (FGSM) with a perturbation
amplitude ϵ = 0.03. At step 0, before purification, the accuracy on adversarial
data is 0.02, while it is 0.86 on the original test data, indicating that the ad-
versarial attack succeeded in misleading the intrusion detection model. After a
fewdiffusionsteps,theadversarialaccuracyincreasesdrastically.Theaddeddif-
fusion noise dilutes the adversarial perturbation that misleads the model while
preserving enough data structure for a good classification. After 44 steps, the
adversarialaccuracypeaksat78%whilethetestaccuracyis80%.Thisdiffusion
step t∗ is optimal as it maximizes the accuracy on adversarial examples while
minimizing the impact on the non-adversarial test data. This result empirically
shows the purification capabilities of diffusion models in intrusion detection.
After the peak, the structure damage due to the addition of diffusion noise
decreases the adversarial accuracy. Since the adversarial perturbation has been
removed, the difference between the test and adversarial accuracy disappears;
it is below 0.01 after t = 90. Both values decrease until they reach random
classification when the reconstructed data is too noisy.
Variance schedule. The variance of the Gaussian noise added at step t of the
diffusionprocessisdenotedβ .ItfollowsalineardistributionofT evenlyspaced
t
values between β and β . The variance schedule is an essential parameter of
1 T
the diffusion process; we hypothesize that it is also critical to the purification
capabilities of diffusion models. In the following, we study the impact of the
variancescheduleβonthepurificationcapabilitiesofthediffusionmodelthrough
theaccuracyoftheintrusiondetectionmodelonpurifiedexamples.Usingafixed
T =1000, we vary both β and β to find an optimal schedule.
1 T
ycaruccA ycaruccA10 Merzouk et al.
0.85 0.85
T=0.0001 t*=43 [81.0%±0.1] T=0.02 t*=46 [77.9%±0.5] T=0.0001 t*=23 [78.5%±0.3] T=0.02 t*=54 [78.4%±0.5]
T=0.001 t*=42 [79.9%±0.6] T=0.05 t*=54 [74.8%±0.5] T=0.001 t*=29 [78.5%±0.6] T=0.05 t*=53 [77.9%±0.5]
T=0.01 t*=44 [78.3%±0.8] T=0.1 t*=65 [71.3%±0.5] T=0.01 t*=34 [78.6%±0.5] T=0.1 t*=66 [75.7%±0.5]
0.80 0.80
0.75 0.75
0.70 0.70
0.65 0.65
0.60 0.60
0 20 40 60 80 100 0 20 40 60 80 100
Diffusion step (t) Diffusion step (t)
(a)UNSW-NB15 (b)NSL-KDD
Fig.6: Intrusion detection accuracy over diffusion step t for different β
T
Figure 5 shows the impact of β , the first value of the variance schedule. If
1
we focus on the continuous lines, corresponding to β = 10−2, we do not see a
T
significant difference between the two values of β . However, with a smaller β ,
1 T
the difference between β becomes significant. The dotted lines, corresponding
1
to β =10−4, show a large difference between the two values of β . As the final
T 1
β decreases, the difference between the accuracy of the two β values increases.
1
The impact of the initial variance β is therefore linked to the length of the
1
variance schedule β −β and becomes less significant as the interval increases.
T 1
This result suggests that β plays an influential role in the purification.
T
Figure 6 shows the impact of β , the final variance of the diffusion schedule.
T
Thefigureshowsanincreasingaccuracywithsmallerβ values.Theintuitionis
T
that as β decreases, the interval between successive variance values decreases,
T
which makes the noising more gradual and easier for the neural network to
reconstruct. On UNSW-NB15, the maximum accuracy value is 81% ± 0.1 at
t∗ = 43, it was reached with the smallest value β = 10−4, which represents
T
a constant variance schedule (since β = 10−4). On NSL-KDD, the maximum
1
accuracy value is very close between when β ≤ 10−2, but β ≤ 10−4 is the
1 1
earliest to achieve 78.5±0.3 after only t∗ =23.
NumberofdiffusionstepsT. Inadditiontotheinitialandfinalperturbation
valuesβ andβ ,thediffusionprocessischaracterizedbythenumberofdiffusion
1 T
stepsT.Thisparameterdeterminesthegranularityofthediffusionsincealarger
number of steps T makes the step size smaller.
In the following, we study how the number of diffusion steps T affects the
optimal diffusion steps t∗. We compare diffusion models with T =100 and T =
1000 with respect to the optimal diffusion step t∗ and identify how it translates
to an equivalent amount of noise. In Appendix A, we further investigate the
ycaruccA ycaruccADiffusion-based Adversarial Purification for Intrusion Detection 11
0.85 T T= =1 10 00
0 0
t t* *= =1 49
6
[ [7 778 .. 90 %% ±± 00 .. 55 ]] 0.85 T T= =1 10 00
0 0
* *= =0 0. .0 00 03 17
0
[ [7 778 .. 90 %% ±± 00 .. 55 ]] 0.85 T T= =1 10 00
0 0
2 2* *= =0 0. .0 03 25 47
9
[ [7 78 7. .0 9% %± ±0 0. .5 5]
]
0.80 0.80 0.80
0.75 0.75 0.75
0.70 0.70 0.70
0.65 0.65 0.65
0.60 0 20 40 60 80 100 0.6 00 .000 0.005 0.010 0.015 0.020 0.60 0.0 0.1 0.2 0.3 0.4 0.5
Diffusion step (t) Variance step (t) Composed variance (2t=1 t)
(a)UNSW-NB15
0.85 T T= =1 10 00
0 0
t t* *= =1 51
4
[ [7 77 8. .9 4% %± ±0 0. .6 5]
]
0.85 T T= =1 10 00
0 0
* *= =0 0. .0 00 02 11
2
[ [7 77 8. .9 4% %± ±0 0. .6 5]
]
0.85 T T= =1 10 00
0 0
2 2* *= =0 0. .0 01 32 31
3
[ [7 77 8. .9 4% %± ±0 0. .6 5]
]
0.80 0.80 0.80
0.75 0.75 0.75
0.70 0.70 0.70
0.65 0.65 0.65
0.60 0 20 40 60 80 100 0.6 00 .000 0.005 0.010 0.015 0.020 0.60 0.0 0.1 0.2 0.3 0.4 0.5
Diffusion step (t) Variance step (t) Composed variance (2t=1 t)
(b)NSL-KDD
Fig.7:Intrusiondetectionaccuracyovert,β ,andσ2 fordifferentnumberofdiffusion
t t
steps T
impactofthenumberofdiffusionstepsT onthetraininglossandthepurification
performance with the optimal variance schedule.
Figure 7 shows the impact of the number of diffusion steps T over three ref-
erences:thediffusionstept,thevariancestepβ ,andthecomposedvarianceσ2.
t t
This experiment uses the standard diffusion parameters to focus the analysis on
when the optimum is recorded rather than its value. With the optimal variance
interval recorded in Figure 6, β = β = 10−4, the 1000-step diffusion model
1 T
largely over-performs the 100-step one on UNSW-NB15, as shown in Figure 11
(Appendix A).
In the first part of Figure 7, we see the accuracy across the diffusion step
t. The T = 100 diffusion model reaches its optimum at t∗ = 19 and t∗ = 11,
whiletheT =1000reachesitsoptimumatt∗ =46andt∗ =54onUNSW-NB15
and NSL-KDD, respectively. This indicates a slower evolution when T = 1000,
whichiscoherentsincethevariancestepsaresmaller.Therefore,weplotthesame
values with respect to the variance steps β to find a similarity. In the second
t
partofFigure7,thex-axiscorrespondstoβ andcoversthewholeintervalfrom
t
0toβ =0.02.However,theoptimumisstillreachedatdistantvaluesofβ.For
T
T =100, it is reached with β∗ =0.0037 and β∗ =0.0021, while for T =1000, it
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA12 Merzouk et al.
0.85 0.85
=0 (no perturbation) =0.03 t*=54 [78.4%±0.5]
=0.01 t*=33 [79.5%±0.5] =0.04 t*=60 [78.0%±0.5]
=0.02 t*=41 [78.8%±0.5] =0.05 t*=57 [77.5%±0.5]
0.80 0.80
0.75 0.75
0.70 0.70
0.65 0.65
=0 (no perturbation) =0.03 t*=46 [77.9%±0.5]
=0.01 t*=39 [79.8%±0.5] =0.04 t*=55 [77.1%±0.4]
=0.02 t*=48 [78.8%±0.5] =0.05 t*=57 [76.3%±0.5]
0.60 0.60
0 20 40 60 80 100 0 20 40 60 80 100
Diffusion step (t) Diffusion step (t)
(a)UNSW-NB15 (b)NSL-KDD
Fig.8: Intrusion detection accuracy over the diffusion step t for different adversarial
perturbation amplitudes ϵ generated with FGSM
is reached with β∗ = 0.0010 and β∗ = 0.0012 in UNSW-NB15 and NSL-KDD,
respectively.
β is the amount of variance applied at the diffusion step t, but it does
t
not correspond to the total variance applied to the initial data x . Indeed, the
0
diffusionforwardprocessisacompositionofGaussiandistributionswithgradual
variances β . The total (composed) variance σ2 of such composition is the sum
t t
of the individual variances, σ2 =1−α¯ (Equation 2).
t t
The third part of Figure 7 shows the accuracy across the composed vari-
ance σ where the two lines overlap, indicating a similar accuracy regardless of
t
the number of diffusion steps T. The optimum is reached at σ2 = 0.0249 and
t
σ2 = 0.0333 for T = 1000, and σ2 = 0.0357 and σ2 = 0.0121 for T = 100 on
t t t
UNSW-NB15andNSL-KDD,respectively.Consideringthescaleofσ2 inFigure
T
7, the optimum values are relatively close. We note from this experiment that
the optimal noise added
σ2∗
approaches 0.03, which corresponds to the adver-
sarialperturbationamplitudeϵusedintheseexperiments.Thisresultsuggestsa
dependence between the optimal noise amount and the perturbation amplitude.
Adversarial perturbation amplitude ϵ. Beyond the diffusion parameters,
the purification performance of diffusion models depends on the amount of ad-
versarial perturbation ϵ added to the data. Figure 8 shows the accuracy of the
intrusiondetectionmodelonincreasingϵvalues.Itdemonstrateshowthepurifi-
cation performance decreases as the perturbation amplitude increases. The ac-
curacyatt∗ goesfrom79.8%±0.5and79.5%±0.5whenϵ=0.01to76.3%±0.5
and 77.5%±0.5 when ϵ = 0.05 on UNSW-NB15 and NSL-KDD, respectively.
Another pattern is that it takes more diffusion steps to reach an optimum as ϵ
increases.Thetwophenomenaarelinked:thediffusionmodelneedstoaddmore
ycaruccA ycaruccADiffusion-based Adversarial Purification for Intrusion Detection 13
0.85 0.85 No perturbation FGSM t*=54 [78.4%±0.5]
DeepFool t*=9 [80.4%±0.4] BIM t*=44 [78.4%±0.5]
JSMA t*=19 [79.8%±0.5] Carlini&Wagner t*=26 [75.8%±0.8]
0.80 0.80
0.75 0.75
0.70 0.70
0.65 0.65
No perturbation FGSM t*=46 [77.9%±0.5]
DeepFool t*=15 [81.3%±0.6] BIM t*=46 [78.2%±0.5]
JSMA t*=23 [80.2%±0.5] Carlini&Wagner t*=17 [73.9%±2.0]
0.60 0.60
0 20 40 60 80 100 0 20 40 60 80 100
Diffusion step (t) Diffusion step (t)
(a)UNSW-NB15 (b)NSL-KDD
Fig.9: Accuracy over the diffusion step t for different adversarial attacks
perturbation to dilute a larger ϵ, thus taking more diffusion steps. However, the
test accuracy (purple line) decreases as more noise is added. Since it represents
an upper bound on the adversarial accuracy, it causes the optimal adversarial
accuracy to decrease as ϵ increases.
Adversarial attacks. The efficiency of adversarial examples also depends on
the method used to generate them. Various methods exist, each optimizing dif-
ferent distance norms and criteria. While previous experiments show how dif-
fusion models considerably improve the adversarial accuracy of intrusion de-
tection models, they have only been tested on one adversarial attack (FGSM).
Here, we study how the purification performance is generalized to other adver-
sarialexamples’generationmethods.Wecomparefivewell-establishedmethods,
namely: DeepFool [16], Jacobian-based Saliency Map Attack (JSMA) [19], Fast
GradientSignMethod(FGSM)[5],BasicIterativeMethod(BIM)[11],andCar-
lini&Wagner’s L attack [3].
2
Figure 9 shows the adversarial accuracy of the intrusion detection model for
the adversarial attacks and the testing set baseline. We note that the diffusion
models successfully purify adversarial examples from all 5 generation methods.
However, the optimal adversarial accuracy and diffusion step vary from one
methodtotheother.Carlini&Wagner’sattackisthemostresistanttoadversarial
purification,withamaximumadversarialaccuracyof73.9%±2and75.8%±0.8
on UNSW-NB15 and NSL-KDD, respectively. The optimum is reached after
only 17 steps on UNSW-NB15, 2.7 times faster than FGSM. On NSL-KDD, the
adversarial accuracy is stable between 30 and 100, making t∗ less accurate.
FGSM and BIM achieve similar performance; FGSM reaches a maximum of
77.9%±0.5 and 78.4%±0.5, while BIM reaches a maximum of 78.2%±0.5 and
78.4%±0.5 on UNSW-NB15 and NSL-KDD, respectively. The optimal diffu-
ycaruccA ycaruccA14 Merzouk et al.
sion steps are also very close between the two. This similarity is explained by
the fact that BIM is based on FGSM; it applies small FGSM steps iteratively
(100 in our experiments) to optimize the adversarial examples. However, the it-
erative approach does not make BIM’s adversarial examples more robust to our
diffusion-basedadversarialpurification.Thediffusionmodelsachievethehighest
purification performance on DeepFool’s adversarial examples, with a maximum
accuracyof81.3%±0.6and80.4%±0.4onUNSW-NB15andNSL-KDD,respec-
tively, closely followed by JSMA with a maximum accuracy of 80.2%±0.5 and
79.8%±0.5 on UNSW-NB15 and NSL-KDD, respectively. DeepFool and JSMA
also had the fastest-growing adversarial accuracy values among other attacks,
almost reaching the testing accuracy upper bound.
5 Discussion
Diffusion neural network. The size of the neural network has a considerable im-
pact on the reconstruction loss of the diffusion model. Larger neural networks
converge faster and reach lower reconstruction loss values due to their capac-
ity to model more complex patterns and learn better representations. The main
obstacle to using larger neural networks is that they take longer to process the
data, making the training and reconstruction longer. The diffusion neural net-
workshouldbehyperparameterizedwithparticularattentiontodomain-specific
time constraints, especially in network intrusion detection, where the reaction
time is critical.
In addition to the size of the diffusion neural network, other hyperparame-
tersimpactthereconstructionlossand,potentially,thepurificationperformance.
Thechoiceofthelossfunction,theoptimizationalgorithm,andthelearningrate
affect the convergence time and help find better optimums. Finally, fully con-
nected neural networks can be replaced with more sophisticated neural network
architecturesthatmodelthediffusionprocessmoreprecisely.Insum,optimizing
diffusionneuralnetworksisapromisingavenueforimprovingthereconstruction
loss, the purification performance, and even the processing time of diffusion
models.
Variance schedule β. The variance schedule determines the amount of noise
added at each step of the forward diffusion process. In the experiments, the
schedule is linear—it consists of T evenly space values between β and β . This
1 T
scheduleisessentialforadversarialpurification,sinceaddingtoolittlenoisedoes
not remove the adversarial perturbation, and too much noise corrupts the data
structure.Theimportanceofβ ,thevarianceofthefirstdiffusionstep,depends
1
onthetotallengthofthediffusionscheduleβ −β .Whenthescheduleissmall,
T 1
thechoiceofthestartingvaluemakesaconsiderabledifferenceinthepurification
capabilities. As the difference between β and β increases, β is less significant
1 T 1
since it has little impact on the rest of the variance values.
On the other hand, the last value β has a more significant impact on the
T
variance schedule, which also extends to the purification performance. In theDiffusion-based Adversarial Purification for Intrusion Detection 15
standard setting, where T =1000, β =10−4, and β =0.02, a smaller value of
1 T
β leadstoabetteradversarialaccuracy.Thebestpurificationperformancewas
T
recorded with the smallest value β = β = 10−4, which describes a constant
T 1
variance schedule.
Asopposedtogenerativemodels,diffusionmodelsinadversarialpurification
donotrequiretheforwardprocesstoreachaGaussiandistributionN (x ;0,I )
T n
after T steps. Therefore, there is more freedom in choosing the diffusion param-
eters to optimize the adversarial accuracy.
Number of diffusion steps T. The number of diffusion steps affects multiple
aspectsofthediffusionprocess.Itdeterminesthesizeofthevariancescheduleβ
anditsvaluessincethereareT evenlyspacedvalues.Inthebest-performingcase
β = β = 10−4, the number of diffusion steps makes a considerable difference
T 1
in favor of a larger T. Having the same variance schedule divided into more
variancestepsallowstheneuralnetworktolearnmoredetailsabouttheapplied
noise, which are then helpful to accurately reconstruct the data in the reverse
process.
The number of diffusion steps also affects the optimal diffusion step t∗; it
happens earlier in the process when T is smaller. This is explained by the fact
that the diffusion steps t do not correspond to an equivalent variance step β .
t
However,evenwhenconsideringβ insteadoft,thereconstructioncurvedoesnot
t
match. This is because the β values are gradually added to the data (Equation
t
1)accordingtothevariancestep;theydonotrepresentthetotalvarianceapplied
to the data. Instead, we consider the variance of the composition, σ2 = 1−α¯,
t
whichmakesthevaluesofT =100andT =1000match.Thetotalvariancethat
maximizes the adversarial accuracy,
σ2∗
is very close between the two different
T values. Furthermore, the optimal variance
σ2∗
approaches the value of the
perturbation amplitude ϵ, demonstrating the dependence of the diffusion noise
on the adversarial perturbation.
Insum,diffusionmodelswithmorediffusionstepsachievebetteradversarial
accuracy with an optimal variance schedule. However, it takes more steps to
reach optimum, translating into a longer purification time. Thus, the choice of
T should also consider the time constraints that characterize the application
domain.
Adversarial perturbation amplitude. Thediffusion-basedadversarialpurification
effectivelyremovesadversarialperturbationsfromnetworktrafficdata.However,
the effectiveness of this method varies slightly depending on the nature of the
adversarialexamples.InthecaseofadversarialexamplesgeneratedwithFGSM,
the parameter ϵ determines the amount of perturbation added to the data. As
theϵincreases,morenoiseisrequiredtodilutetheperturbation,makingtheop-
timumoccurlaterintermsofdiffusionsteps.Anothersideeffectisthattheextra
required noise also dilutes some of the data structure, thus decreasing the test
accuracy, representing an upper bound to the adversarial accuracy. Therefore,
the maximum adversarial accuracy decreases as ϵ increases.16 Merzouk et al.
Adversarialattacks. Inadditiontotheperturbationamplitudeϵ,thepurification
process is sensitive to the adversarial examples’ generation method. The results
show a considerable gap in the adversarial accuracy between different methods.
In particular, DeepFool and JSMA are easier to purify and approach the test
accuracy upper-bound after a few diffusion steps, FGSM and BIM achieve close
performance due to their similarities, and Carlini&Wagner’s L attack is the
2
most resistant but our method still recovers up to 75% of the accuracy.
Adversary’s constraints. The adversarial examples’ generation method and the
adversarial perturbation amplitude are both parameters controlled by the ad-
versary when they generate the adversarial examples. However, they are con-
strained in the amount of perturbation they can add. A larger adversarial per-
turbation can increase the chances of being detected, cancel the purpose of the
data,orevenbreakitsconsistency,especiallyforhighlystructureddatalikenet-
work traffic [15]. In the experiments, we consider the worst-case scenario where
the attacker perturbs all the data features. However, the design of real-world
diffusion-based adversarial purification models should consider a realistic threat
model to optimize the diffusion parameters.
6 Conclusion
Diffusion models are a promising approach to adversarial purification. Their
seamlessintegrationwithexistingsystemsandgeneralizationacrossattackmeth-
ods make them particularly interesting in the context of intrusion detection.
Throughout this paper, we have demonstrated the effectiveness of diffusion
models in mitigating the threat of adversarial examples against intrusion detec-
tion. We have compared several diffusion neural network sizes, which show that
larger neural networks yield lower loss values despite their increased demands
in time and computational resources. Our analysis of the variance schedule in-
dicates the importance of the final variance β in determining the purification
T
performance, with smaller values achieving the highest accuracy. Furthermore,
wehaveshownthattheoptimalamountofdiffusionnoiseσ2∗
isnearlyconstant
regardless of the number of diffusion steps T and that it approaches the value
of the perturbation amplitude ϵ. However, in terms of purification performance,
diffusion models with a larger T display better adversarial accuracy despite re-
quiring more diffusion steps. Finally, we benchmarked our method against five
state-of-the-art adversarial attacks and an increasing perturbation amplitude.
While scalability and computational complexity remain the main challenges
for diffusion models in intrusion detection, especially for inline detection, we
envision future research endeavors to refine and optimize diffusion models for
practical deployment. As novel adversarial attacks emerge and challenge adver-
sarial defenses [9], our future work will focus on adapting diffusion-based purifi-
cation to these attacks. Ultimately, complementing diffusion models with other
defensive techniques remains necessary to prevent a single point of failure.Diffusion-based Adversarial Purification for Intrusion Detection 17
Acknowledgments
ThisworkwassupportedbyMitacsthroughtheMitacsAccelerateInternational
programandtheCRITiCALchair.Itwasenabledinpartbysupportprovidedby
CalculQuébec,ComputeOntario,theBCDRIGroup,andtheDigitalResearch
Alliance of Canada.
References
1. Aldahdooh, A., Hamidouche, W., Fezza, S.A., Déforges, O.: Adversarial example
detection for dnn models: A review and experimental comparison. Artificial Intel-
ligence Review (2022)
2. Carlini, N., Wagner, D.: Adversarial examples are not easily detected: Bypassing
ten detection methods. In: ACM workshop on artificial intelligence and security
(2017)
3. Carlini, N., Wagner, D.: Towards Evaluating the Robustness of Neural Networks.
In: IEEE Symposium on Security and Privacy (SP) (2017)
4. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S.,Courville,A.,Bengio,Y.:GenerativeAdversarialNets.In:AdvancesinNeural
Information Processing Systems (2014)
5. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. In: International Conference on Learning Representations (2015)
6. Han,F.,Ye,P.,She,C.,Duan,S.,Wang,L.,Liu,D.:Mmid-bench:Acomprehen-
sivebenchmarkformulti-domainmulti-categoryintrusiondetection.IEEETrans-
actions on Intelligent Vehicles (2024)
7. He,K.,Kim,D.D.,Asghar,M.R.:Adversarialmachinelearningfornetworkintru-
sion detection systems: A comprehensive survey. IEEE Communications Surveys
& Tutorials (2023)
8. Ho,J.,Jain,A.,Abbeel,P.:DenoisingDiffusionProbabilisticModels.In:Advances
in Neural Information Processing Systems (2020)
9. Kang, M., Song, D., Li, B.: Diffattack: Evasion attacks against diffusion-based
adversarial purification. In: Advances in Neural Information Processing Systems
(2023)
10. Kingma, D.P., Welling, M.: Auto-Encoding Variational Bayes (2022),
arXiv:1312.6114
11. Kurakin, A., Goodfellow, I.J., Bengio, S.: Adversarial examples in the physical
world. In: International Conference on Learning Representations (2017)
12. Lin, G., Tao, Z., Zhang, J., Tanaka, T., Zhao, Q.: Robust Diffusion Models for
Adversarial Purification (2024), arXiv:2403.16067
13. Liu, H., Lang, B.: Machine learning and deep learning methods for intrusion de-
tection systems: A survey. Applied Sciences (2019)
14. Madry,A.,Makelov,A.,Schmidt,L.,Tsipras,D.,Vladu,A.:Towardsdeeplearning
models resistant to adversarial attacks. In: International Conference on Learning
Representations (2018)
15. Merzouk, M.A., Cuppens, F., Boulahia-Cuppens, N., Yaich, R.: Investigating the
practicality of adversarial evasion attacks on network intrusion detection. Annals
of Telecommunications (2022)
16. Moosavi-Dezfooli, S.M., Fawzi, A., Frossard, P.: Deepfool: A simple and accurate
method to fool deep neural networks. In: IEEE Conference on Computer Vision
and Pattern Recognition (2016)18 Merzouk et al.
17. Moustafa,N.,Slay,J.:Unsw-nb15:acomprehensivedatasetfornetworkintrusion
detection systems (unsw-nb15 network data set). In: 2015 Military Communica-
tions and Information Systems Conference (MilCIS) (2015)
18. Nie, W., Guo, B., Huang, Y., Xiao, C., Vahdat, A., Anandkumar, A.: Diffu-
sionModelsforAdversarialPurification.In:InternationalConferenceonMachine
Learning (2022)
19. Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z.B., Swami, A.: The
LimitationsofDeepLearninginAdversarialSettings.In:IEEEEuropeanSympo-
sium on Security and Privacy (EuroS P) (2016)
20. Ring, M., Wunderlich, S., Scheuring, D., Landes, D., Hotho, A.: A survey of
network-based intrusion detection data sets. Computers & Security (2019)
21. Shi, Y., Du, M., Wu, X., Guan, Z., Sun, J., Liu, N.: Black-box backdoor defense
via zero-shot image purification. In: Advances in Neural Information Processing
Systems (2023)
22. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep Unsuper-
vised Learning using Nonequilibrium Thermodynamics. In: International Confer-
ence on Machine Learning (2015)
23. Sohn, I.: Deep belief network based intrusion detection techniques: A survey. Ex-
pert Systems with Applications (2021)
24. Song,Y.,Kim,T.,Nowozin,S.,Ermon,S.,Kushman,N.:Pixeldefend:Leveraging
generative models to understand and defend against adversarial examples (2017),
arXiv:1710.10766
25. Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.:
Score-Based Generative Modeling through Stochastic Differential Equations. In:
International Conference on Learning Representations (2021)
26. Srinivasan, V., Rohrer, C., Marban, A., Müller, K.R., Samek, W., Nakajima, S.:
Robustifyingmodelsagainstadversarialattacksbylangevindynamics.NeuralNet-
works (2021)
27. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
Fergus, R.: Intriguing properties of neural networks. In: International Conference
on Learning Representations (2014)
28. Tang,B.,Lu,Y.,Li,Q.,Bai,Y.,Yu,J.,Yu,X.:Adiffusionmodelbasedonnetwork
intrusion detection method for industrial cyber-physical systems. Sensors (2023)
29. Tavallaee,M.,Bagheri,E.,Lu,W.,Ghorbani,A.A.:Adetailedanalysisofthekdd
cup 99 data set. In: IEEE Symposium on Computational Intelligence for Security
and Defense Applications (2009)
30. Wang,J.,Lyu,Z.,Lin,D.,Dai,B.,Fu,H.:GuidedDiffusionModelforAdversarial
Purification (2022), arXiv:2205.14969
31. Wang, Y., Ding, J., He, X., Wei, Q., Yuan, S., Zhang, J.: Intrusion detection
methodbasedondenoisingdiffusionprobabilisticmodelsforuavnetworks.Mobile
Networks and Applications (2023)
32. Wu,Q.,Ye,H.,Gu,Y.:GuidedDiffusionModelforAdversarialPurificationfrom
Random Noise (2022), arXiv:2206.10875
33. Yang,C.,Wang,T.,Yan,X.:Ddmt:Denoisingdiffusionmasktransformermodels
for multivariate time series anomaly detection (2023), arXiv:2310.08800
34. Yuan, X., He, P., Zhu, Q., Li, X.: Adversarial Examples: Attacks and Defenses
forDeepLearning.IEEETransactionsonNeuralNetworksandLearningSystems
(2019)
35. Zhang, W., Chen, Z., Chen, D., Li, J., Pan, Y.: Did-ids: A novel diffusion-based
imbalanceddataintrusiondetectionsystem.In:IEEEInternationalConferenceon
Information, Communication and Networks (ICICN) (2023)Diffusion-based Adversarial Purification for Intrusion Detection 19
0.5 0.5
T=100 MSE=0.019±0.0 T=100 MSE=0.021±0.0
T=1000 MSE=0.008±0.0 T=1000 MSE=0.009±0.0
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0.0 0.0
0 50000 100000 150000 200000 0 50000 100000 150000 200000
Epochs Epochs
(a)UNSW-NB15 (b)NSL-KDD
Fig.10: Reconstruction loss over training epochs for T =100 and T =1000
A More on the number of diffusion steps T
Figure 10 shows the reconstruction loss over the training epochs. Both diffusion
models are trained with β = 10−4, β = 10−2, and a diffusion neural network
1 T
with10hiddenlayersof960ReLUunits.Weobservethatthereconstructionloss
forT =1000isalwayslowerthanforT =100.Also,thelosscurveforT =1000
stabilizes earlier (around 150,000 epochs as opposed to 190,000 epochs when
T = 100 on UNSW-NB15). Despite both models having the same β and β ,
1 T
a larger T divides the range into smaller steps, allowing for a more gradual
noising. As the added noise increases more slowly, the diffusion neural network
reconstructs better x at each step.
t−1
rorrE
derauqS
naeM
rorrE
derauqS
naeM20 Merzouk et al.
0.85 0.85
T=100 t*=14 [74.0%±1.6] T=100 t*=6 [78.5%±0.3]
T=1000 t*=43 [81.0%±0.1] T=1000 t*=23 [78.5%±0.3]
0.80 0.80
0.75 0.75
0.70 0.70
0.65 0.65
0.60 0.60
0 20 40 60 80 100 0 20 40 60 80 100
Diffusion step (t) Diffusion step (t)
(a)UNSW-NB15 (b)NSL-KDD
Fig.11:Accuracyoverthediffusionsteptfordifferentnumberofdiffusionstepstusing
the optimal variance interval recorded in Figure 6: β =β =10−4
1 T
ycaruccA ycaruccA