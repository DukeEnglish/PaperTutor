Accelerating Clinical Evidence Synthesis with Large Language Models
Zifeng Wang1, Lang Cao1, Benjamin Danek1, Yichi Zhang1, Qiao Jin2,
Zhiyong Lu2, Jimeng Sun1,3#
1 Department of Computer Science, University of Illinois Urbana-Champaign, Champaign,
IL
2 National Center for Biotechnology Information, National Library of Medicine, Bethesda,
MD
3 Carle Illinois College of Medicine, University of Illinois Urbana-Champaign, Champaign, IL
#Corresponding authors. Emails: jimeng@illinois.edu
Abstract
Automatic medical discovery by AI is a dream of many. One step toward that goal is
to create an AI model to understand clinical studies and synthesize clinical evidence from
theliterature. Clinicalevidencesynthesiscurrentlyreliesonsystematicreviewsofclinical
trials and retrospective analyses from medical literature. However, the rapid expansion
of publications presents challenges in efficiently identifying, summarizing, and updating
evidence. We introduce TrialMind, a generative AI-based pipeline for conducting medi-
calsystematicreviews,encompassingstudysearch,screening,anddataextractionphases.
Weutilizelargelanguagemodels(LLMs)todriveeachpipelinecomponentwhileincorpo-
rating human expert oversight to minimize errors. To facilitate evaluation, we also create
a benchmark dataset TrialReviewBench, a custom dataset with 870 annotated clini-
cal studies from 25 meta-analysis papers across various medical treatments. Our results
demonstrate that TrialMind significantly improves the literature review process, achiev-
inghighrecallrates(0.897-1.000)instudysearchingfromover20millionPubMedstudies
and outperforming traditional language model embeddings-based methods in screening
(Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpasses direct
GPT-4performanceinresultextraction,withaccuracyrangingfrom0.65to0.84. Wealso
supportclinicalevidencesynthesisinforestplots,asvalidatedbyeighthumanannotators
who preferred TrialMind over the GPT-4 baseline with a winning rate of 62.5%-100%
across the involved reviews. Our findings suggest that an LLM-based clinical evidence
synthesis approach, such as TrialMind, can enable reliable and high-quality clinical evi-
dence synthesis to improve clinical research efficiency.
1
4202
nuJ
52
]LC.sc[
1v55771.6042:viXraIntroduction
Clinical evidence is crucial for supporting clinical practices and advancing new drug de-
velopment. It is primarily gathered through retrospective analysis of real-world data or
through prospective clinical trials that assess new interventions on humans. Researchers
often conduct systematic reviews to consolidate evidence from various studies in the
literature.1 However, conducting a systematic literature review is expensive and time-
consuming, requiring five experts to analyze 195 publications over 67 weeks on average.2
Moreover, thefastgrowthofclinicalstudydatabasesmeans thattheinformationinthese
published clinical reviews becomes outdated rapidly.3 This situation underscores the ur-
gentneedtostreamlinethesystematicreviewprocessestoproducesystematicandtimely
clinical evidence from the extensive medical literature.4
Large language models (LLMs) excel at information processing and generating, show-
ing great promise in streamlining the clinical evidence synthesis process. Adapting LLMs
to new tasks can be done by providing them the task definition and examples as the
text inputs (namely ‚Äòprompts‚Äô) without the need of retraining the model.5 Researchers
have tried to adopt LLMs for individual tasks in literature review.6‚Äì10 For example, by
enhancing inputs with multiple papers, LLMs can summarize findings to answer medical
questions.9 This strategy helps reduce hallucinations but still faces challenges when the
input studies do not adequately support the posed questions, which requires more effort
in literature search and screening steps. Furthermore, LLMs often demonstrate limita-
tions in reasoning with numerical data found in clinical studies. The qualitative clinical
evidence generated using raw paper content via prompting can be overly generic, lack
critical information, or misinterpret results.11 Therefore, we propose developing an LLM-
driven pipeline to assist the entire workflow, including formulating research questions,
conducting literature mining, extracting information, and synthesizing clinical evidence.
This includes a comprehensive evaluation of LLMs in the entire process, which was not
well-explored.12
This study aims to fulfill the potential of AI in helping medical practitioners with
the entire clinical evidence synthesis process using AI. We demonstrate how our method
TrialMindisoptimizedfortheclinicalevidencesynthesistaskvia: (1)generatingboolean
queriestosearchfromtheliterature;(2)buildinginclusionandexclusioncriteriatoscreen
through the found studies; (3) extracting key information, including study protocols,
methods,participantbaselines,etc.,fromunstructureddocuments,perusers‚Äôrequest;and
(4) synthesizing high-quality clinical evidence. Unlike previous approaches, TrialMind
integrates LLMs into an AI pipeline by breaking down the task into multiple steps that
alignwithexpertpractices. Thisapproachmaintainsflexibilitybyinvolvinghumansinthe
loop to monitor, edit, and verify all intermediate steps and the final synthesized outputs.
ThispapercuratedabenchmarkdatasetTrialReviewBenchforacomprehensiveeval-
uation. Thedatasetincludes870involvedclinicalstudiesandmorethan50,000identified
studies from 25 meta-analyses. It also consists of manual annotations of 1,334 study
characteristics and 1,049 study results. We show that the TrialMind is able to 1) re-
trieveacompletelistoftargetstudiesfromtheliterature,2)followthespecifiedeligibility
criteria to rank the most relevant studies at the top, and 3) achieve high accuracy in
extracting information and clinical outcomes from unstructured documents based on user
requests. Additionally, the extracted clinical outcomes can be standardized as input for
meta-analysis (e.g., forest plots). We conducted a human evaluation of the synthesized
evidence to demonstrate the potential value of TrialMind in practice.
Results
Creating TrialReviewBench from medical literature
Asystematicunderstandingofcancertreatmentsiscrucialforoncologydrugdiscoveryand
development. Using a large list of cancer treatments from the National Cancer Institute
(NCI),13 we curated a dataset TrialReviewBench of systematic reviews. To ensure data
2Literature search Literature screening Data extraction Evidence synthesis
‚Ä¢ Eligibility criteria ‚Ä¢ Clinical outcome
‚Ä¢ Query generation generation ‚Ä¢ Information extraction extraction
‚Ä¢ Query augmentation ‚Ä¢ Eligibility prediction ‚Ä¢ Links to the sources ‚Ä¢ Clinical evidence
‚Ä¢ Study relevance ranking synthesis
Identified studies Ranked studies Study characteristics Clinical evidence
A Literature search C Data extraction
Population C I Tm -A cmR el- u lT n t, r o at nh se fr ea rp , ‚Ä¶y, 1 D exo tc ra p ca tir os ning and Tables
Treatment terms
Intervention Lymphoma, ùëÅ st uId de ien stified Texts Figures √óùëÅ
1 G ane dn erate L Me uu lk tie pm lei a, 3 Retrieve Identified Content of the involved studies
Comparison augment Myeloma, ‚Ä¶ studies
[
Condition terms {
Outcome Overall response, 3 Re sfe or u t ro c ethe " " "vn d aa e lm s uc ee " "" : :": 1" Nn 0u 0" m, , ber of samples", 2 Extraction
c reo sm pop nle st ee , ‚Ä¶ 2 Check, edit, and add } , "src": [0, 1]
...
Outcome terms ]
Extracted fields
B Literature screening D Evidence extraction and synthesis
‚Ä¢ Is a clinical study Check, Study #1 Study #2 Study #3
‚Ä¢ Report on safety edit, and
1 E gelig ni eb ri alit ty io c nriteria eleP mIC eO n ts ‚Ä¢ o n Ru e eut pc r oo o rm t to oe x nis c ei in t fy fc i, cl ‚Ä¶u ad ce y add 1 E o enfx t t dr aa prc ogt e in t th tc se li nre ics au ll ts
Generate outcomes include
overall response, ‚Ä¶
#Overall Survival #Overall Response
#Objective Response Rate#Complete Remission
List of eligibility criteria ‚Ä¶
C#1 C#2 C#3 ‚Ä¶
Standardize the
2 Eligibility prediction S1 2 r ae gs gu relt gs a a ten dd am na ak lye s is B Solo lio dd t utu mm oo rr
Scan and S2
Identified predict #Overall Response#Overall Survival studies
Criterion-level eligibility predictions
3 S rat nu kd iy n gselection & (i ndS et =u n 1d t Si 0i f te i 0 ues 0d
d
)
ies excluded (n =
8S 0ft siu ( 0r c nsd )r t =ei -e r e 2s o n 0 ua i 0nnft )gde r A c l per rg vi eg t ee dr le r
i
cig o ta in ot -e
n
sStu ad (t ni te h =s e 2r a t 0on )pked 3 Q a sunu mda l r mi eta s at u ri iv l zte a ta ion nalysis R p T a ‚Ä¶h nae e dts i eu p onl o vt t eos s rl: ) e a 2 w d l0 le o r s r evet seu i prnd a oci le nl lu s ss d e( u1 e r w0 vd e0 i. v r0 a e l C s t e chu f ao feg nin crg cac ae epl cs ru y y t ‚Ä¶s s h ai ato nh sn da : d t s O e aC mu fA er oR t s yn-t T isu n td r by a l t oe od d
‚Ä¢ Not eligible to C#1
‚Ä¢ Not eligible to C#2
‚Ä¢ ‚Ä¶
Figure 1: The TrialMind framework has four components: a) Utilizing input PICO elements,
TrialMind generates key terms to construct Boolean queries for retrieving studies from liter-
ature databases. b) TrialMind formulates eligibility criteria, which users can edit to provide
context for LLMs during eligibility predictions. Users can then select studies based on these
predictionsandranktheirrelevancebyaggregatingthem. c)TrialMindprocessesthedescrip-
tions of target data fields to extract and output the required information as structured data.
d) TrialMind extracts findings from the studies and collaborates with users to synthesize the
clinical evidence.
3
‚Ä¶
‚Ä¶quality, we crafted comprehensive queries with automatic filtering and manual screening.
Foreachreview,weobtainedthelistofstudieswiththeirPubMedIDs,retrievedtheirfull
content,andextractedstudycharacteristicsandclinicaloutcomes. WefollowedPubMed‚Äôs
usage policy and guidelines during retrieval. Further manual checks were performed to
correctinaccuracies, eliminateinvalidandduplicatepapers, andrefinethetextforclarity
(Methods). The final TrialReviewBench dataset consists of 870 studies involved in 25
reviews (Fig. 2a), including (1) Brachytherapy:14‚Äì16 3 reviews with 189 involved studies
and 5,811 identified studies; (2) Chemotherapy:17‚Äì19 3 reviews with 126 involved studies
and 5,874 identified studies; (3) Hyperthermia:20‚Äì22 3 reviews with involved 28 studies
and5,972identifiedstudies;(4)HormoneTherapy:23‚Äì25 3reviewswith30involvedstudies
and5,970identifiedstudies; (5)CancerVaccine:26 onereviewwith6involvedstudiesand
1,994relatedstudies;(6)ImmuneCheckpointInhibitors:27‚Äì29 3reviewswith187involved
studies and 5,813 identified studies; (7) Immune System Modulators:30 one review with 9
involved studies and 1,991 identified studies; (8) Monoclonal Antibodies:31‚Äì33 3 reviews
with 228 involved studies and 5,772 identified studies; (9) T-cell Transfer Therapy:34,35
2 reviews with 36 involved studies and 3,964 identified studies; (10) Stem Cell Trans-
plant:36‚Äì38 3 reviews with 31 involved studies and 5,969 identified studies. The detailed
characteristics of these studies are in Extended Table 1.
Build an LLM-driven system for clinical evidence synthesis
Large language models (LLMs) excel in adapting to new tasks when provided with task-
specific prompts while often struggling with complex tasks that require multi-step rea-
soning. Additionally, interacting and collaborating with LLMs can be problematic due
to their opaque nature and the complexity of debugging.39 In this study, we developed
TrialMind that decomposes the clinical evidence synthesis process into four main tasks
(Fig. 1 and Methods). Initially, using the provided research question enriched with pop-
ulation, intervention, comparison, and outcome (PICO) elements, TrialMind conducts a
comprehensive search from the literature. It also works with users to build the criteria
for study screening and rank the studies. Next, TrialMind browses the study details
to extract the study characteristics and pertinent findings. To ensure the accuracy and
integrity of the data, each output is linked to the sources for manual inspection. In the
final step, TrialMind standardizes the clinical outcomes for meta-analysis.
TrialMind can make a comprehensive retrieval of studies from the
literature
Finding relevant studies from medical literature like PubMed, which contains over 37
million entries, can be challenging. Typically, this requires the research expertise to
craft complex queries that comprehensively cover pertinent studies. The challenge lies in
balancingthespecificityofqueries: toostringent,andthesearchmaymissrelevantstudies;
too broad, and it becomes impractical to manually screen the overwhelming number of
results. Previous approaches propose to prompt LLMs to generate the searching query in
one pass,6 which can induce incomplete searching results due to the limited knowledge of
LLMs. In contrast, TrialMind is designed to produce comprehensive queries through a
pipeline that includes query generation, augmentation, and refinement. It also provides
users with the ability to make adjustments (Fig. 2b).
Thedatasetinvolvingclinicalstudiesspanningtencancertreatmentareaswasusedfor
evaluation (Fig. 2a). For each review, we collected the involved studies‚Äô PubMed IDs as
theground-truthandmeasuredtheRecall, i.e., howmanyground-truthstudiesarefound
in the search results. We created two baselines as the comparison: GPT-4 and Human.
The GPT-4 baseline makes a guided prompt for LLMs to generate the boolean queries.6
It represents the common way of prompting LLMs for literature search query generation.
The Human baseline represents a way where the key terms from PICO elements are
extracted manually and expanded, referring to UMLS,40 to construct the search queries.
Overall,TrialMindachievedaRecallof0.921onaverageforallreviewsinTrialReviewBench,
meaning it can capture all target studies most of the time. By contrast, the GPT-4 base-
4List of generated terms
a b subject to add, remove, or edit
c
Combine the Call to expand to
terms to build more terms
search queries
d
Figure 2: Literature search experiment results. a, Total number of involved studies and
participants of the review papers across different topics. b, The TrialMind‚Äôs interface for
users to retrieve studies. c, The Recall of the search results for reviews across four topics.
The bar heights indicate the Recall, and the star indicates the number of studies found. d,
Scatter plots of the Recall against the number of ground-truth studies. Each scatter indicates
the results of one review. Regression estimates are displayed with the 95% CIs in blue or
purple.
5line yielded Recall = 0.079, and the Human baseline yielded Recall = 0.230. We divided
the search results across four topics determined by the treatments studied in each review
(Fig. 2c). Our analysis showed that TrialMind can identify many more studies than
the baselines. For instance, TrialMind achieved Recall = 0.914 (N studies = 28,863)
for Immunotherapy-related reviews, while the GPT-4 baseline got Recall = 0.1 (N stud-
ies = 22), and the Human baseline got Recall = 0.154 (N studies = 153), respectively.
In Radiation/Chemotherapy, TrialMind achieved Recall = 0.897, the GPT-4 baseline
got Recall = 0.017, and the Human baseline got Recall = 0.304. In Hormone Therapy,
TrialMind achieved Recall=0.924, the GPT-4 baseline got Recall=0.100, and the Hu-
man baseline got Recall = 0.150. In Hyperthermia, TrialMind achieved Recall = 1.000,
the GPT-4 baseline got Recall = 0.133, and the Human baseline got Recall = 0.683.
These results demonstrate that regardless of the search task‚Äôs complexity, as indicated by
the variability in the Human baseline, TrialMind consistently retrieves nearly all target
studies from the PubMed database. This robust performance provides a solid foundation
for accurately identifying target studies in the screening phase.
Furthermore, we made scatter plots of Recall versus the number of target studies for
each review (Fig. 2d). The hypothesis was that an increase in target studies correlates
with the difficulty of achieving complete coverage. Our findings reveal that TrialMind
consistently maintained a high Recall, nearing 1 in many instances and never falling
below 0.7, significantly outperforming the best baselines across all 25 reviews. A trend
of declining Recall with an increasing number of target studies was confirmed through
regression analysis. For example, with fewer than 20 target studies, TrialMind achieved
perfect Recall for most reviews, while the GPT-4 baseline struggled, showing Recall close
to 0, and the Human baseline results varied between 0 and 0.85. As the number of target
studies increased, the Human and GPT-4 baselines‚Äô Recall decreased to nearly zero. In
contrast, TrialMind demonstrated remarkable resilience, showing minimal variation in
performance despite the increasing number of target studies. For instance, in a review
involving 141 studies, TrialMind achieved a Recall of 0.99, while the GPT-4 and Human
baselines obtained a Recall of 0.02 and 0, respectively.
TrialMind enhances literature screening and ranking
Typically, human experts manually sift through thousands of retrieved studies to select
relevant ones for inclusion in a systematic review. This process adheres to the PRISMA
statement,41 which involves creating a list of eligibility criteria and assessing each study‚Äôs
eligibility. TrialMindstreamlinesthistaskthroughathree-stepapproach: (1)itgenerates
a set of inclusion criteria, which are subject to user‚Äôs adjustments; (2) it applies these
criteria to evaluate the study‚Äôs eligibility, denoted by {‚àí1,0,1} where ‚àí1 and 1 represent
eligible and non-eligible, and 0 represents unknown/uncertain, respectively; and (3) it
ranksthestudiesbyaggregatingtheeligibilitypredictions,wheretheaggregationstrategy
can be specified by users (Fig. 3a). We took a summation of the criteria-level eligibility
predictionsasthestudy-levelrelevancepredictionscoresforranking. Assuch,TrialMind
providesarationalefortherelevancescoresbydetailingtheeligibilitypredictionsforeach
criterion.
WechoseMPNet42 andMedCPT43 asthegeneraldomainandmedicaldomainranking
baselines, respectively. These methods compute study relevance by the cosine similarity
between the encoded PICO elements as the query and the encoded study‚Äôs abstracts.
We also set a Random baseline that randomly samples from candidates. We created the
evaluation data based on the search results in the first stage. For each review, we mixed
the target studies with the other found studies to build a candidate set of 2,000 studies
for ranking. Discriminating the target studies from the other candidates is challenging
since all candidates meet the search queries, meaning they most probably investigate
the relevant therapies or conditions. We evaluated the ranking performance using the
Recall@20 and Recall@50 metrics. The concatenation of the title and abstract of each
study is used for all methods as inputs.
WefoundthatTrialMindgreatlyimprovedrankingperformances,withthefoldchanges
over the best baselines ranging from 1.4 to 24 across four topics (Table 3c). For in-
6a List of eligibility criteria subject b
to add, remove, or edit Eligibility prediction:
ineligible
Eligibility
prediction
Eligibility prediction:
eligible
Eligibility prediction:
unknown
c
d
Topic Metric TrialMind MPNet MedCPT Random Fold Change*
Recall@20 0.240 0.000 0.000 0.010 24.0
Hormone Therapy
Recall@50 0.439 0.042 0.000 0.025 10.5
Recall@20 0.246 0.037 0.037 0.010 6.6
Hyperthermia
Recall@50 0.306 0.037 0.120 0.025 2.5
Recall@20 0.230 0.056 0.048 0.010 4.1
Immunotherapy
Recall@50 0.383 0.101 0.101 0.025 3.8
Radiation/Chemother Recall@20 0.227 0.028 0.163 0.010 1.4
apy Recall@50 0.335 0.102 0.237 0.025 1.4
*TrialMind versus the best baseline (MPNet, MedCPT, or Random) across the row
0 5 10 15 >20 0 2 4 6 8 >10
Fold change compared to random performances Fold change compared to the best baseline
e
Figure 3: Literature screen experiment results. a, Streamline study screening using
TrialMind with human in the loop. b, Ranking performances for Recall@20/50 within across
therapeutic areas. c, Recall@20 and Recall@50 for TrialMind and selected baselines. d,
Effect of individual criterion on the ranking results. e, Ranking performance for Recall@K
with varying K in four topics. Shaded areas are 95% confidence interval.
7stance, for the Hormone Therapy topic, TrialMind obtained Recall@20 = 0.240 and
Recall@50 = 0.439. In the Hyperthermia topic, TrialMind obtained Recall@20 = 0.246
and Recall@50 = 0.306. In the Immunotherapy topic, TrialMind obtained Recall@20 =
0.230 and Recall@50 = 0.383. In the Radiation/Chemotherapy topic, TrialMind ob-
tained Recall@20 = 0.227 and Recall@50 = 0.335. TrialMind demonstrated the largest
advantage over baselines on the Hormone Therapy topic (fold change = 24 and 10.5 for
Recall@20 and Recall@50 compared to the best baselines). In contrast, other baselines
exhibit significant variability across different topics. The general domain baseline MPNet
was the worst as it performed similarly to the Random baseline in Recall@20. MedCPT
showed marginal improvement over MPNet in the last three topics, while both failed to
capture any target studies in Hormone Therapy.
Furthermore, TrialMind demonstrated significant improvements over the baselines
across various therapeutic areas (Fig. 3b). For example, in ‚ÄúCancer Vaccines‚Äù and ‚ÄúHor-
mone Therapy,‚Äù TrialMind substantially increased Recall@50, achieving 33.33-fold and
10.53-foldimprovements,respectively,comparedtothebest-performingbaseline. TrialMind
generally attained a fold change greater than 2 (ranging from 1.57 to 33.33). Despite the
challenge of selecting from a large pool of candidates (n=2,000) where candidates were
very similar, TrialMind identified an average of 43% of target studies within the top 50.
We compared TrialMind to MedCPT and MPNet for Recall@K (K in 10 to 200) to
gain insight into how K influences the performances (Fig. 3e). We found TrialMind can
capture most of the target studies when K = 200, as it obtained Recall@200 = 0.697 in
Immunotherapy, Recall@200=0.618 in Radiation/Chemotherapy, Recall@200=0.744 in
Hormone Therapy, Recall@200=0.830 in Hyperthermia, respectively. The improvement
over baselines is especially significant in Hormone Therapy and Hyperthermia, as the
other baselines did not outperform the Random performances all the time. For instance,
in Hormone Therapy, MedCPT‚Äôs Recall@200 = 0.122 and MPNet‚Äôs Recall@200 = 0.083
(Random‚Äôs Recall@200=0.100).
To thoroughly assess the quality of these criteria and their impact on ranking perfor-
mance, weconductedaleave-one-outanalysistocalculate‚àÜRecall@200foreachcriterion
(Fig. 3d). The ‚àÜRecall@200 metric measures the difference in ranking performance with
and without a specific criterion, with a larger value indicating superior criterion quality.
Ourfindingsrevealedthatmostcriteriapositivelyinfluencedrankingperformances,asthe
negative influence criteria are n=1 in Hormone Therapy, n=1 in Hyperthermia, n=5
in Radiation/Chemotherapy, and n = 7 in Immunotherapy. Additionally, we identified
redundanciesamongthegeneratedcriteria,asthosewith‚àÜRecall@200=0werethemost
frequently observed. This redundancy likely stems from some criteria covering similar
eligibility aspects, thus not impacting performance when one is omitted.
TrialMind scales data and result extraction from unstructured doc-
uments
TrialMind leverages LLMs to streamline extracting study characteristics such as target
therapies, studyarmdesign, andparticipants‚Äôbaselineinformationfrominvolvedstudies.
Specifically, TrialMind refers to the field names and the descriptions from users and use
the full content of the study documents in PDF or XML formats as inputs (Fig. 4a).
When the free full content is unavailable, TrialMind accepts the user-uploaded content
as the input. We developed an evaluation dataset by converting the study characteristic
tables from each review paper into data points. Our dataset comprises 1,334 target data
points, including696onstudydesign, 353onpopulationfeatures, and285onresults. We
assessed the data extraction performance using the Accuracy metric.
TrialMinddemonstratedstrongextractionperformanceacrossvarioustopics(Fig.4b):
it achieved an accuracy of ACC=0.78 (95% confidence interval (CI) = 0.75‚Äì0.81) in the
Immunotherapytopic,ACC=0.77(95%CI=0.72-0.82)intheRadiation/Chemotherapy
topic, ACC=0.72(95%CI=0.63-0.80)intheHormoneTherapytopic, andACC=0.83
(95% CI = 0.74-0.90) in the Hyperthermia topic. These results indicate that TrialMind
can provide a solid initial data extraction, which human experts can refine. Importantly,
8a b
Add the description of Fields to be extracted
the target study info
Field extraction
c
Predictions:Predictions: FP: Hallucinations
Null Not Null
Actuals:
Null
Actuals:
Not Null
L foin r k ee ad c hto e t xh te ra s co teu drc fe ies l d F inN fo: rM mi as ts ioin ng
Study Design Population Characteristics Study Results
d e
f g Define the target Define the target cohort, e.g., by
clinical endpoint conditions and treatments
Result
extraction
‚Ä¶
(results in tabular format)
Figure 4: Data and result extraction experiment results. a, Streamline study informa-
tion extraction using TrialMind. b, Data extraction accuracy within each field type across
four topics. c, Confusion matrix showing the hallucination and missing rates in the data
extraction results. d, Result extraction accuracy across topics. e, Result extraction accuracy
across clinical endpoints. f, Error analysis of the result extraction. g, Streamline result ex-
traction using TrialMind.
9each output can be cross-checked by the linked original sources, facilitating verification
and further investigation.
Diving deeper into the accuracy across different types of fields, we observed varying
performance levels. It performed best in extracting study design information, followed by
population details, and showed the lowest accuracy in extracting results (Fig. 4b). For
example, in the Immunotherapy topic, TrialMind achieved an accuracy of ACC = 0.95
(95% CI = 0.92-0.96) for study design, ACC=0.74 (95% CI = 0.67-0.80) for population
data, and ACC=0.42 (95% CI = 0.36-0.49) for results. This variance can be attributed
to the prevalence of numerical data in the fields: fields with more numerical data are
typically harder to extract accurately. Study design is mostly described in textual format
and is directly presented in the documents, whereas population and results often include
numerical data such as the number of patients or gender ratios. Results extraction is
particularly challenging, often requiring reasoning and transformation to capture values
accurately. Given these complexities, it is advisable to scrutinize the extracted numerical
data more carefully.
We also evaluated the robustness of TrialMind against hallucinations and missing
information (Fig. 4c). We constructed a confusion matrix detailing instances of halluci-
nations: false positives (FP) where TrialMind generated data not present in the input
document and false negatives (FN) where it failed to extract available target field infor-
mation. We observed that TrialMind achieved a precision of Precision=0.994 for study
design, Precision = 0.966 for population, and Precision = 0.862 for study results. Miss-
ing information was slightly more common than hallucinations, with TrialMind achiev-
ing recall rates of Recall = 0.946 for study design, Recall = 0.889 for population, and
Recall = 0.930 for study results. The incidence of both hallucinations and missing infor-
mation was generally low. However, hallucinations were notably more frequent in study
results; this often occurred because LLMs could confuse definitions of clinical outcomes,
for example, mistaking ‚Äòoverall response‚Äô for ‚Äòcomplete response.‚Äô Nevertheless, such hal-
lucinations are typically manageable, as human experts can easily identify and correct
them while reviewing the referenced material.
The challenges in extracting study results primarily stem from (1) identifying the lo-
cations that describe the desired outcomes from lengthy papers, (2) accurately extracting
relevant numerical values such as patient numbers, event counts, durations, and ratios
from the appropriate patient groups, and (3) performing the correct calculations to stan-
dardize these values for meta-analysis. In response to these complexities, we developed
a specialized pipeline for result extraction (Fig. 4g), where users provide the interested
outcome and the cohort definition. TrialMind offers a transparent extraction workflow,
documentingthesourcesofresultsalongwiththeintermediatereasoningandcalculations.
We compared TrialMind against two generalist LLM baselines, GPT-4 and Sonnet,
which were prompted to extract the target outcomes from the full content of the study
documents. Since the baselines can only make text extractions, we manually convert
them into numbers suitable for meta-analysis.44 This made very strong baselines since
theycombinedLLMextractionwithhumanpost-processing. Weassessedtheperformance
using the Accuracy metric.
TheevaluationconductedacrossfourtopicsdemonstratedthesuperiorityofTrialMind
(Fig.4d). Specifically,inImmunotherapy,TrialMindachievedanaccuracyofACC=0.70
(95% CI 0.62-0.77), while GPT-4 scored ACC = 0.54 (95% CI 0.45-0.62). In Radia-
tion/Chemotherapy, TrialMind reached ACC = 0.65 (95% CI 0.51-0.76), compared to
GPT-4‚Äôs ACC = 0.52 (95% CI 0.39-0.65). For Hormone Therapy, TrialMind achieved
ACC = 0.80 (95% CI 0.58-0.92), outperforming GPT-4, which scored ACC = 0.50 (95%
CI 0.30-0.70). In Hyperthermia, TrialMind obtained an accuracy of ACC = 0.84 (95%
CI 0.71-0.92), significantly higher than GPT-4‚Äôs ACC = 0.52 (95% CI 0.39-0.65). The
breakdownsofevaluationresultsbythemostfrequenttypesofclinicaloutcomes(Fig.4e)
showed TrialMind got fold changes in accuracy ranging from 1.05 to 2.83 and a me-
dian of 1.50 over the best baselines. This enhanced effectiveness is largely attributable
to TrialMind‚Äôs ability to accurately identify the correct data locations and apply logical
reasoning, while the baselines often produced erroneous initial extractions.
10a
Research question: Microwave ablation (MWA) compared with radiofrequency ablation (RFA) for the treatment of liver cancer: a
systematic review and meta-analysis
Local tumor progression (MWA v.s. RFA)
[Spiliotis et al., 2021, PMID: 34167181]
Local tumor progression (MWA v.s. RFA)
Method #1 (GPT-4 baseline) Weight
Study OR OddsRatio 95%‚àíCI(random)
Weight
Study OR OddsRatio 95%‚àíCI(random) Abdelaziz 2014 0.26 [0.06; 1.07] 5.7%
Vietti Violi 2018 0.50 [0.18; 1.39] 8.3%
Na√Øk Vietti Violi 0.50 [0.18; 1.39] 12.3% Qian 2012 1.26 [0.24; 6.47] 4.6%
AshrafAbdelaziz 0.26 [0.06; 1.07] 6.5% Takahashi2018 0.44 [0.22; 0.90] 11.3%
Guo‚àíJunQian 1.26 [0.24; 6.47] 4.8% Shady 2018 0.93 [0.47; 1.84] 11.7%
Hideo Takahashi 0.94 [0.26; 3.46] 7.6% Xu 2017 0.95 [0.50; 1.81] 12.0%
AukjeAJMvanTilborg 0.93 [0.47; 1.84] 28.0% vanTilborg2016 2.34 [1.17; 4.70] 11.5%
TheodoraAPotretzke 0.46 [0.19; 1.09] 17.4% Potretzke2016 0.46 [0.19; 1.09] 9.8%
Lei Zhang 0.87 [0.36; 2.12] 16.4% Zhang L 2013 0.85 [0.35; 2.07] 9.5%
YingjunLiu 0.37 [0.09; 1.42] 7.0% Liu 2013 0.37 [0.12; 1.13] 7.6%
Ding 2013 2.24 [0.78; 6.45] 8.0%
Commoneffectmodel 0.66 [0.46; 0.93] .
Randomeffectsmodel0.66 [0.46; 0.95] 100.0% Commoneffectmodel 0.82 [0.64; 1.06] .
Heterogeneity:I2=0%,ÔÅ£2 7= 5.58(p 0.1= 0.59)
0.5 1 2 10
R Hea tn ed roo gm ene eif tf ye :c I2ts =m 56o %d ,e ÔÅ£l
2
10 0. =7 9
22.85(p= 0.01)
[0.52; 1.19] 100.0%
0.1 0.5 1 2 10
Reference conclusion [Spiliotis et al., 2021, PMID: 34167181]
MWA showed promising results and demonstrated better oncological outcomes in terms
of LTP compared to RFA in patients with HCC. MWA can be utilized as the ablation
Local tumor progression (MWA v.s. RFA) method of choice in patients with HCC.
Method #2 (TrialMind)
Weight
Study OR OddsRatio 95%‚àíCI(random) To which degree do you think the forest plots made by the Method #1 aligned with the
forest plots from the source meta-analysis study?
Na√Øk Vietti Violi 0.50 [0.18; 1.39] 12.9%
AshrafAbdelaziz 0.26 [0.06; 1.07] 9.4% 1 2 3 4 5
Guo‚àíJunQian 1.26 [0.24; 6.47] 7.8% Not at all (worst) Very well (best)
Hideo Takahashi 0.44 [0.22; 0.90] 16.3%
AukjeAJMvanTilborg 3.02 [1.30; 7.02] 14.8%
TheodoraAPotretzke 0.46 [0.19; 1.09] 14.6% To which degree do you think the forest plots made by the Method #2 aligned with the
Lei Zhang 0.87 [0.36; 2.12] 14.3% forest plots from the source meta-analysis study?
YingjunLiu 0.37 [0.09; 1.42] 9.8%
1 2 3 4 5
Commoneffectmodel 0.67 [0.48; 0.94] . Not at all (worst) Very well (best)
Randomeffectsmodel0.67 [0.38; 1.19] 100.0%
Heterogeneity:I2=61%, ÔÅ£2 7= 17.97(p= 0.01)
0.1 0.5 1 2 10 Which results do you think are in general better?
Method #1 Method #2
b c d
Figure 5: Human evaluation of the synthesized clinical evidence. a, The study design
compares the synthesized clinical evidence from the baseline and TrialMind via human eval-
uation. b, Winning rate of TrialMind against the GPT-4+Human baseline across studies.
c, Violin plots of the ratings across studies. Each plot is tagged with the mean ratings (95%
CI) from all the annotators. d,Violin plots of the ratings across annotators with different
expertise levels. Each plot is tagged with the mean ratings (95% CI) from all the studies.
11We analyzed the error cases in our result extraction experiments and identified four
primaryerrortypes(Fig.4f). Themostcommonerrorwas‚ÄòInaccurate‚Äôextraction(n=36),
followed by ‚ÄòExtraction failure‚Äô (n=27), ‚ÄòUnavailable data‚Äô (n=10), and ‚ÄòHallucinations‚Äô
(n=3). ‚ÄòInaccurate‚Äô extractions often occurred due to multiple sections ambiguously de-
scribing the same field. For example, a clinical study might report the total number of
participants receiving CAR-T therapy early in the document and later provide outcomes
for a subset with non-small cell lung cancer (NSCLC). The specific results for NSCLC
patients are crucial for reviews focused on this subgroup, yet the presence of general data
can lead to confusion and inaccuracies in extraction. ‚ÄòExtraction failure‚Äô and ‚ÄòUnavail-
able data‚Äô both illustrate scenarios where TrialMind could not retrieve the information.
ThelattercaseparticularlyshowcasesTrialMind‚Äôsrobustnessagainsthallucinations,asit
failedtoextractdataoutsidethestudy‚Äôsmaincontent, suchasinappendices, whichwere
notincludedintheinputs. Furthermore,errorscausedbyhallucinationswereminor. The
outputs were easy to identify and correct through manual inspection since no references
were provided.
TrialMind synthesizes clinical evidence from extracted results
We engaged human annotators to assess the quality of synthesized clinical evidence pre-
sented in forest plots, a format commonly used in systematic reviews to report meta-
analysis results (Fig. 5a). We selected five systematic review studies as benchmarks and
referenced the clinical evidence reported in the target studies. The baseline used GPT-4
with a simple prompting to extract the relevant text pieces that report the target out-
come of interest (Methods). Manual calculations were necessary to standardize the data
for meta-analysis. In contrast, TrialMind automated the extraction and standardization.
Each annotator was asked to evaluate the evidence quality by comparing it against the
evidence reported in the target review and deciding which method, TrialMind or the
baseline, produced superior results. Additionally, they rated the quality of the synthe-
sizedclinicalevidenceonascaleof1to5. Theassignmentofourmethodandthebaseline
was randomized to ensure objectivity.
The evaluation highlighted TrialMind‚Äôs superior performance compared to the direct
use of GPT-4 for clinical evidence synthesis (Fig. 5b). We calculated the winning rate of
TrialMind versus the baseline across the five studies. The results indicate a consistent
preferencebyannotatorsfortheevidencesynthesizedbyTrialMindoverthatofthebase-
line. Specifically, TrialMind achieved winning rates of 87.5%, 100%, 62.5%, 62.5%, and
81.2%, respectively. Thebaseline‚Äôsprimaryshortcomingstemmedfromtheinitialextrac-
tion step, where GPT-4 often failed to identify the relevant sources without well-crafted
prompting. Therefore, thesubsequentmanualpost-processingwasunabletorectifythese
initial errors.
In addition, we illustrated the ratings of TrialMind and the baseline across studies
(Fig. 5c). We found TrialMind was competent as the GPT-4+Human baseline and out-
performed the baseline in many scenarios. For example, TrialMind obtained the mean
rating of 4.25 (95% CI 3.93-4.57) in Study #1 while the baseline obtained 3.50 (95% CI
3.13-3.87). In Study #2, TrialMind yielded 3.50 (95% CI 3.13-3.87) while the baseline
yielded 1.25 (95% CI 0.93-1.57). The performance of the two methods was comparable
in the remaining three studies. These results highlight TrialMind as a highly effective
alternativetoconventionalLLMusageinevidencesynthesis,streamliningdataextraction
and processing while maintaining the critical benefit of human oversight.
Finally,werequestedthatannotatorsself-assesstheirexpertiselevelinclinicalstudies,
classifying themselves into three categories: ‚ÄòBasic‚Äô, ‚ÄòFamiliar‚Äô, and ‚ÄòAdvanced‚Äô. The
typical profile ranges from computer scientists at the basic level to medical doctors at the
advanced level. We then analyzed the ratings given to both methods across these varying
expertise levels (Fig. 5d). We consistently observed higher ratings for TrialMind than
the baseline across all groups. Annotators with basic knowledge tended to provide more
conservative ratings, while those with more advanced expertise offered a wider range of
evaluations. Forinstance,the‚ÄòBasic‚Äôgroupprovidedaverageratingsof3.67(95%CI3.34-
3.39)forTrialMindcomparedto3.22(95%CI2.79-3.66)forthebaseline. The‚ÄòAdvanced‚Äô
12group rated TrialMind at an average of 3.40 (95% CI 3.16-3.64) and the baseline at 3.07
(95% CI 2.75-3.39).
Discussion
Clinical evidence forms the bedrock of evidence-based medicine, crucial for enhancing
healthcare decisions and guiding the discovery and development of new drugs. It often
comes from a systematic review of diverse studies found in the literature, encompassing
clinicaltrialsandretrospectiveanalysesofreal-worlddata. Yet,theburgeoningexpansion
of literature databases presents formidable challenges in efficiently identifying, summariz-
ing, and maintaining the currency of this evidence.
The rapid development of large language models (LLMs) and generative AI technolo-
gies has generated considerable interest in their potential applications. However, imple-
menting these models in a manner that is collaborative, transparent, and trustworthy
poses significant challenges, especially in critical areas such as medicine.45 For instance,
whenutilizingLLMstosummarizemultiplestudies, thesummariesoftenmerelyechothe
findings verbatim, omit crucial details, and fail to adhere to established best practices.9
ThisstudyintroducesaclinicalevidencesynthesispipelineenhancedbyLLMs,named
TrialMind. Thispipelineisstructuredinaccordancewithestablishedmedicalsystematic
review protocols, involving steps such as study searching, screening, data/result extrac-
tion, and evidence synthesis. At each stage, human experts have the capability to access,
monitor, and modify intermediate outputs. This human oversight helps to eliminate er-
rors and prevents their propagation through subsequent stages. Unlike approaches that
solely depend on the internal knowledge of LLMs, TrialMind integrates human expertise
throughin-contextlearningandchain-of-thoughtprompting. Additionally,TrialMindex-
tends external knowledge sources to its outputs through retrieval-augmented generation
and leveraging external computational tools to enhance the LLM‚Äôs reasoning and analyt-
ical capabilities. Comparative evaluations of TrialMind and traditional LLM approaches
havedemonstratedtheadvantagesofthissystemdesigninLLM-drivenapplicationswithin
the medical field.
This study also has several limitations. First, despite incorporating multiple tech-
niques, LLMs may still make errors at any stage. Therefore, human oversight and veri-
fication remain crucial when implementing TrialMind in practical settings. Second, the
promptsusedinTrialMindweredevelopedbasedonpromptengineeringexperience,sug-
gestingpotentialforperformanceenhancementthroughadvancedpromptoptimizationor
by fine-tuning the underlying LLMs to suit specific tasks better. Third, while TrialMind
demonstrated effectiveness in study search, screening, and information extraction, the
dataset used was limited in size due to the high costs associated with human labeling.
Future research could expand on these findings with larger datasets to further validate
themethod‚Äôseffectiveness. Fourth,thestudycoveragewasrestrictedtopubliclyavailable
sourcesfromPubMedCentral,whichprovidesstructuredPDFsandXMLs. Manyrelevant
studies are either not available on PubMed or are in formats that entail OCR algorithms
as preprocessing, indicating a need for further engineering to incorporate broader data
sources. Fifth, although TrialMind illustrated the potential of using advanced LLMs
like GPT-4 to streamline clinical evidence synthesis, developing techniques to adapt the
pipeline for use with other LLMs could increase its applicability. Finally, while the use
of LLMs like GPT-4 can accelerate study screening and data extraction, the associated
costs and processing times may present bottlenecks in some scenarios. Future enhance-
mentsthatimproveefficiencyorutilizelocalized,specializedsmallermodelscouldincrease
practical utility.
LLMshavemadesignificantstridesinAIapplications. TrialMindexemplifiesacrucial
aspect of system engineering in LLM-driven pipelines, facilitating the practical, robust,
and transparent use of LLMs. We anticipate that TrialMind will benefit the medical AI
communitybyfosteringthedevelopmentofLLM-drivenmedicalapplicationsandempha-
sizing the importance of human-AI collaboration.
13Methods
Description of the TrialReviewBench Dataset
TheoverallflowchartforthestudyidentificationandscreeningprocessinbuildingTrialReviewBench
is illustrated in Extended Fig. 1.
Database search and initial filtering We undertook a comprehensive search on
the PubMed database for meta-analysis papers related to cancer. The Boolean search
terms were specifically chosen to encompass a broad spectrum of cancer-related top-
ics. These terms included ‚Äúcancer‚Äù, ‚Äúoncology‚Äù, ‚Äúneoplasm‚Äù, ‚Äúcarcinoma‚Äù, ‚Äúmelanoma‚Äù,
‚Äúleukemia‚Äù, ‚Äúlymphoma‚Äù, and ‚Äúsarcoma‚Äù. Additionally, we incorporated terms related
to various treatment modalities such as ‚Äútherapy‚Äù, ‚Äútreatment‚Äù, ‚Äúchemotherapy‚Äù, ‚Äúradi-
ation therapy‚Äù, ‚Äúimmunotherapy‚Äù, ‚Äútargeted therapy‚Äù, ‚Äúsurgical treatment‚Äù, and ‚Äúhor-
mone therapy‚Äù. To ensure that our search was exhaustive yet precise, we also included
terms like ‚Äúmeta-analysis‚Äù and ‚Äùsystematic review‚Äù in our search criteria.
This initial search yielded an extensive pool of 46,192 results, reflecting the vast re-
search conducted in these areas. We applied specific filters to refine these results and
ensure relevance and quality. We focused on articles where PMC Full text was available
and specifically categorized under ‚ÄúMeta-Analysis‚Äù. Further refinement was done by re-
stricting the time frame of publications to those between January 1, 2020, and January
1, 2023. We also narrowed our focus to studies conducted on humans and those available
in English. This filtration process was critical in distilling the initial results into a more
manageable and focused collection of 2,691 papers.
Refinement Building upon our initial search, we employed further refinement tech-
niques using both MeSH terms and specific keywords. The MeSH terms were carefully
selected to target papers precisely relevant to various forms of cancer. These terms in-
cluded ‚Äúcancer‚Äù, ‚Äútumor‚Äù, ‚Äúneoplasms‚Äù, ‚Äúcarcinoma‚Äù, ‚Äúmyeloma‚Äù, and ‚Äúleukemia‚Äù. This
focused approach using MeSH terms effectively reduced our selection to 1,967 papers.
Tofurtherdiveinonpapersinvestigatingcancertherapies,weutilizedmanykeywords
derived from the National Cancer Institute‚Äôs ‚ÄúTypes of Cancer Treatment‚Äù list. This ap-
proachwasmulti-faceted,witheachsetofkeywordstargetingaspecificcategoryofcancer
therapy. Forchemotherapy,weincludedtermslike‚Äúchemotherapy‚Äù,‚Äúchemo‚Äù,andrelated
variations. In the realm of hormone therapy, we searched for phrases such as ‚Äùhormone
therapy‚Äù, ‚Äùhormonal therapy‚Äù, and similar terms. The keyword group for hyperther-
miaencompassedtermslike ‚Äúhyperthermia‚Äù, ‚Äúmicrowave‚Äù, ‚Äúradiofrequency‚Äù, andrelated
technologies. For cancer vaccines, we included keywords such as ‚Äúcancer vaccines‚Äù, ‚Äúcan-
cer vaccine‚Äù, and other related terms. The search for immune checkpoint inhibitors and
immune system modulators was comprehensive, including terms like ‚Äúimmune checkpoint
inhibitors‚Äù, ‚Äúimmunomodulators‚Äù, and various cytokines and growth factors. Lastly, our
search for monoclonal antibodies and T-cell transfer therapy included relevant terms like
‚Äúmonoclonal antibodies‚Äù, ‚Äút-cell therapy‚Äù, ‚Äúcar-t‚Äù, and other related phrases.
Thecarefulapplicationofkeywordfilteringplayedacrucialroleinnarrowingdownour
pool of research papers to a more focused and relevant set of 352. It represents a diverse
and meaningful collection of studies in cancer therapy, highlighting a range of innovative
and impactful research within this field.
Manual screening of titles and abstracts Then, we manually screened titles
and abstracts, applying a rigorous classification and sorting methodology. The remaining
papers were first categorized based on the type of cancer treatment they explored. We
then organized these papers by their citation count to gauge their impact and relevance
in the field. Our selection criteria aimed to enhance the quality and relevance of our final
dataset. We prioritized papers that focused on the study of treatment effects, such as
safety and efficacy, of various cancer interventions. We preferred studies that compared
individual treatments against a control group, as opposed to those examining the effects
14of combined therapies (e.g., Therapy A+B vs. A only). To build a list of representative
meta-analyses,weneededtoensurediversityinthetargetconditionsundereachtreatment
category.
Further, we favored studies that involved a larger number of individual studies, pro-
viding a broader base of evidence. However, we excluded network analysis studies and
meta-analyses that focused solely on prognostic and predictive effects, as they did not
align with our primary research focus. To maintain a balanced representation, we limited
our selection to a maximum of three papers per treatment category. This process culmi-
natedinafinaldatasetcomprising25papers. Thiscuratedcollectionformsthebackbone
of our analysis, ensuring a concentrated and pertinent selection of high-quality studies
directly relevant to our research objectives. The characterstics of the created dataset is
in Extended Table 1.
LLM Prompting
PromptingsteersLLMstoconductthetargettaskwithouttrainingtheunderlyingLLMs.
TrialMind proceeds clinical evidence synthesis in multiple steps associated with a series
of prompting techniques.
In-context learning LLMs exhibit a profound ability to comprehend input requests
and adhere to provided instructions during generation. The fundamental concept of in-
context learning (ICL) is to enable LLMs to learn from examples and task instructions
within a given context at inference time.5 Formally, for a specific task, we define T as the
taskprompt, whichincludesthetaskdefinition, inputformat, anddesiredoutputformat.
DuringasingleinferencesessionwithinputX,theLLMispromptedwithP(T,X),where
P(¬∑)isatransformationfunctionthatrestructuresthetaskdefinitionT andinputX into
the prompt format. The output XÀÜ is then generated as XÀÜ =LLM(P(T,X)).
Retrieval-augmented generation LLMs that rely solely on their internal knowl-
edge often produce erroneous outputs, primarily due to outdated information and hallu-
cinations. This issue can be mitigated through retrieval-augmented generation (RAG),
whichenhancesLLMsbydynamicallyincorporatingexternalknowledgeintotheirprompts
during generation.46 We denote R (¬∑) as the retriever that utilizes the input X to source
K
relevant contextual information through semantic search. R (¬∑) enables the dynamic
K
infusion of tailored knowledge into LLMs at inference time.
Chain-of-thought Chain-of-though (CoT) guides LLMs in solving a target task in a
step-by-step manner in one inference, hence handling complex or ambiguous tasks better
and inducing more accurate outputs.47 CoT employs the function P (¬∑) to structure
CoT
the task T into a series of chain-of-thought steps {S ,S ,...,S }. As a result, we obtain
1 2 T
{XÀÜ1,...,XÀÜT} = LLM(P (T,X)), all produced in a single inference session. This is
S S CoT
rather critical when we aim to elicit the thinking process of LLM and urge it in self-
reflection to improve its response. For instance, we may ask LLM to draft the initial
response in the first step and refine it in the second.
LLM-driven pipeline Clinical evidence synthesis involves a multi-step workflow as
outlined in the PRISMA statement.41 It can be generally outlined as identifying and
screening studies from databases, extracting characteristics and results from individual
studies, and synthesizing the evidence. To enhance each step‚Äôs performance, task-specific
prompts can be designed for an LLM to create an LLM-based module. This results in a
chainofpromptsthateffectivelyaddressesacomplexproblem, whichwecallLLM-driven
workflow. Specifically, this approach breaks down the entire meta-analysis process into
a sequence of N tasks, denoted as T = {T ,...,T }. In the workflow, the output from
1 N
one task, XÀÜ , serves as the input for the next, XÀÜ = LLM(P(T ,XÀÜ )). This modular
n n+1 n n
decompositionimprovesLLMperformancebydividingtheworkflowintomoremanageable
segments, increases transparency, and facilitates user interaction at various stages.
15Incorporating these techniques, the formulation of TrialMind for any subtask can be
represented as:
XÀÜ =LLM(P(T ,X ),R (X )), ‚àÄn=1,...,N, (1)
n+1 n n K n
where R (¬∑) are optional.
K
Implementation of TrialMind
All experiments were run in Python v.3.9. Detailed software versions are: pandas v2.2.2;
numpy v1.26.4; scipy v1.13.0; scikit-learn v1.4.1.post1; openai v1.23.6; langchain v0.1.16;
boto3 v1.34.94; pypdf v4.2.0; lxml v5.2.1 and chromadb v0.5.0 with Python v.3.9.
LLMs We included GPT-4 and Sonnet in our experiments. GPT-448 is regarded
as a state-of-the-art LLM and has demonstrated strong performances in many natural
language processing tasks (version: gpt-4-0125-preview). Sonnet49 is an LLM devel-
oped by Anthropic, representing a more lightweight but also very capable LLM (version:
anthropic.claude-3-sonnet-20240229-v1:0 on AWS Bedrock). Both models support long
context lengths (128K and 200K), enabling them to process the full content of a typical
PubMed paper in a single inference session.
Research question inputs TrialMind processes research question inputs using the
PICO (Population, Intervention, Comparison, Outcome) framework to define the study‚Äôs
topic and scope. In our experiments, the title of the target review paper served as the
general description. Subsequently, we extracted the PICO elements from the paper‚Äôs
abstract to detail the specific aspects of the research question.
Literature search TrialMind is tailored to adhere to the established guidelines41 in
conductingliteraturesearchandscreeningforclinicalevidencesynthesis. Intheliterature
searchstage,thekeyisformulatingBooleanqueriestoretrieveacomprehensivesetofcan-
didate studies from databases. These queries, in general, are a combination of treatment,
medication, and outcome terms, which can be generated by LLM using in-context learn-
ing. However,directpromptingcanyieldlowrecallqueriesduetothenarrowrangeofuser
inputsandtheLLMs‚Äôtendencytoproduceincorrectqueries,suchasgeneratingerroneous
MeSH(MedicalSubjectHeadings)terms.6 Toaddresstheselimitations,TrialMindincor-
porates RAG to enrich the context with knowledge sourced from PubMed, and employs
CoT processing to facilitate a more exhaustive generation of relevant terms.
Specifically, the literature search component has two main steps: initial query genera-
tion and then query refinement. In the first step, TrialMind prompts LLM to create the
initialbooleanqueriesderivedfromtheinputPICOtoretrieveagroupofstudies(Prompt
in Extended Fig. 3). The abstracts of these studies then enrich the context for refining
the initial queries, working as RAG. In addition, we used CoT to enhance the refinement
by urging LLMs to conduct multi-step reasoning for self-reflection enhancement (Prompt
in Extended Fig. 4). This process can be described as
{XÀÜ1,XÀÜ2,XÀÜ3}=LLM(P (T ,X,R (X))), (2)
S S S CoT LS K
whereX denotestheinputPICO;R (X)isthesetofabstractsofthefoundstudies; T
K LS
isthedefinitionofthequerygenerationtaskforliteraturesearch. Fortheoutput,thefirst
sub-step XÀÜ1 indicates a complete set of terms identified in the found studies; the second
S
XÀÜ2 indicates the subset of XÀÜ1 by filtering out the irrelevant; and the third XÀÜ3 indicates
S S S
the extension of XÀÜ2 by self-reflection and adding more augmentations. In this process,
S
LLMwillproducetheoutputsforallthreesubstepsinonepass,andTrialMindtakesXÀÜ3
S
as the final queries to fetch the candidate studies.
16Study screening TrialMind follows PRISMA to take a transparent approach for
study screening. It creates a set of eligibility criteria based on the input PICO as the
basis for study selection (Prompt in Extended Fig. 5), produced by
XÀÜ =LLM(P(T ,X)), (3)
EC EC
where XÀÜ = {E ,E ,...,E } is the M generated eligibility criteria; X is the input
EC 1 2 M
PICO;andT isthetaskdefinitionofcriteriageneration. Usersaregiventheopportunity
EC
to modify these generated criteria, further adjusting to their needs.
Based on XÀÜ , TrialMind embarks the parallel processing for the candidate studies.
EC
For i-th study F , the eligibility prediction is made by LLM as (Prompt in Extended
i
Fig. 6)
{I1,...,IM}=LLM(P(F ,X,T ,XÀÜ )), (4)
i i i SC EC
where T is the task definition of study screening; F is the study i‚Äôs content; Im ‚àà
SC i i
{‚àí1,0,1},‚àÄm = 1,...,M is the prediction of study i‚Äôs eligibility to the m-th criterion.
Here, ‚àí1 and 1 mean ineligible and eligible, 0 means uncertain, respectively. These
predictions offer a convenient way for users to inspect the eligibility and select the target
studies by altering the aggregation strategies. Im can be aggregated to offer an overall
i
relevance of each study, such as IÀÜ = (cid:80) Im. Users are also encouraged to extend the
i m i
criteria set or block the predictions of some criteria to make customized rankings during
the screening phase.
Data extraction Study data extraction is an open information extraction task that
requires the model to extract specific information based on user inputs and handle long
inputs, such as the full content of a paper. LLMs are particularly well-suited for this task
because (1) they can perform zero-shot learning via in-context learning, eliminating the
need for labeled training data, and (2) the most advanced LLMs can process extremely
longinputs. Assuch,theTrialMindframeworkisengineeredtostreamlinedataextraction
from structured or unstructured study documents using LLMs.
For the specified data fields to be extracted, TrialMind prompts LLMs to locate and
extract the relevant information (Prompt in Extended Fig. 7). These data fields include
(1) study characteristics such as study design, sample size, study type, and treatment
arms; (2) population baselines; and (3) study findings. In general, the extraction process
can be described as
{XÀÜ1 ,...,XÀÜK }=LLM(P(F,C,T )), (5)
EX EX EX
where F represents the full content of a study; T defines the task of data extraction;
EX
and C = {C ,C ,...,C } comprises the series of data fields targeted for extraction.
1 2 K
C is the user input natural language description of the target field, e.g., ‚Äúthe number
k
of participants in the study‚Äù. The input content F is segmented into distinct chunks,
each marked by a unique identifier. The outputs, denoted as XÀÜk = {Vk,Bk}, include
EX
the extracted values V and the indices B that link back to their respective locations in
the source content. Hence, it is convenient to check and correct mistakes made in the
extraction by sourcing the origin. The extraction can also be easily scaled by making
paralleled calls of LLMs.
Resultextraction Ouranalysisindicatesthatdataextractiongenerallyperformswell
for study design and population-related fields; however, extracting study results presents
challenges. Errors frequently arise due to the diverse presentation of results within stud-
ies and subtle discrepancies between the target population and outcomes versus those
reported. For instance, the target outcome is the risk ratios (treatment versus control)
regardingtheincidenceofadverseevents(AEs),whilethestudyreportsAEsamongmany
groupsseparately. Or, thetargetoutcomeistheincidenceofsevereAEs, whichimplicitly
correspond to those with grade III and more, while the study reports all grade AEs. To
overcomethesechallenges,wehaverefinedourdataextractionprocesstocreateaspecial-
ized result extraction pipeline that improves clinical evidence synthesis. This enhanced
pipeline consists of three crucial steps: (1) identifying the relevant content within the
17study (Prompt in Extended Fig. 8), (2) extracting and logically processing this content
to obtain numerical values (Prompt in Extended Fig. 9), and (3) converting these values
into a standardized tabular format (Prompt in Extended Fig. 10).
Steps (1) and (2) are conducted in one pass using CoT reasoning as
{XÀÜ1 ,XÀÜ2 }=LLM(P (X,O,F,T )), (6)
RE,S RE,S CoT RE
whereO isthenaturallanguagedescriptionoftheclinicalendpointofinterestandT is
RE
the task definition of result extraction. In the outputs, XÀÜ1 represents the raw content
RE,S
captured from the input content F regarding the clinical outcomes; XÀÜ2 represents
RE,S
the elicited numerical values from the raw content, such as the number of patients in
thegroup, theratioofpatientsencounteringoverallresponse, etc. Instep(3), TrialMind
writesPythoncodetomakethefinalcalculationtoconvertXÀÜ2 tothestandardtabular
RE,S
format.
XÀÜ =exec(LLM(P(X,O,T ,XÀÜ2 )),XÀÜ2 ). (7)
RE PY RE,S RE,S
In this process, TrialMind adheres to the instructions in T to generate code for data
PY
processing. Thiscodeisthenexecuted,usingXÀÜ2 asinput,toproducethestandardized
RE,S
resultXÀÜ . AnexamplecodesnippetmadetodothistransformationisshowninExtended
RE
Fig. 2. This approach facilitates verification of the extracted results by allowing for easy
backtracking to XÀÜ1 . Additionally, it ensures that the calculation process remains
RE,S
transparent, enhancing the reliability and reproducibility of the synthesized evidence.
Experimental setup
Literature search and screening Inourliteraturesearchexperiments,weassessed
performanceusingoverallRecall,aimingtoevaluatetheeffectivenessofdifferentmethods
in identifying all relevant studies from the PubMed database using APIs.50 For literature
screening, we measured efficacy using Recall@20 and Recall@50, which gauge how well
themethodscanprioritizetargetstudiesatthetopofthelist,therebyfacilitatingquicker
decisionsaboutwhichstudiestoincludeinevidencesynthesis. Weconstructedtheranking
candidatesetforeachreviewpaperbyinitiallyretrievingstudiesthroughTrialMind,then
refining this list by ranking the relevance of these studies to the target review‚Äôs PICO
elements using OpenAI embeddings. The top 2000 relevant studies were kept. We then
ensured all target papers were included in the candidate set to maintain the integrity of
our groundtruth data. The final candidate set was then deduplicated to be ranked by the
selected methods.
Inthecriteriaanalysisexperiment,weutilizedRecall@200toassesstheimpactofeach
criterion. This was done by first computing the relevance prediction using all eligibility
predictions and then recalculating it without the eligibility prediction for the specific cri-
terion in question. The difference in Recall@200 between these two relevance predictions,
denoted as ‚àÜRecall, indicates the criterion‚Äôs effect. A larger ‚àÜRecall suggests that the
criterion plays a more significant role in influencing the ranking results.
Data extraction and result extraction To evaluate performance, we measured
the accuracy of the values extracted by TrialMind against the groundtruth. We used the
study characteristic tables from the review papers as our test set. Each table‚Äôs column
namesservedasinputfielddescriptionsforTrialMind. Wemanuallydownloadedthefull
content for the studies listed in the characteristic table. To verify the accuracy of the
extracted values, we enlisted three annotators who manually compared them against the
data reported in the original tables.
We also measured the performance of result extraction using accuracy. The annota-
tors were asked to carefully read the extracted results and compare them to the results
reported in the original review paper. For the error analysis of TrialMind, the anno-
tators were asked to check the sources to categorize the errors for one of the reasons:
inaccurate, extraction failure, unavailable data, or hallucination. We designed a vanilla
18prompting strategy for GPT-4 and Sonnet models to set the baselines for the result ex-
traction. Specifically, the prompt was kept minimal, as ‚ÄúBased on the {paper}, tell me
the {outcome} from the input study for the population {cohort}‚Äù, where {paper} is the
placeholder for the paper‚Äôs content; {outcome} is the for the target endpoint; {cohort} is
the for the target population‚Äôs descriptions, including conditions and characteristics. The
responsesfromthesepromptsweretypicallyinfreetext,fromwhichannotatorsmanually
extracted result values to evaluate the baselines‚Äô performance.
Evidence synthesis In evidence synthesis, we processed the input data using R and
the ‚Äòmeta‚Äô package to make the forest plots and the pooled results based on the stan-
dardized result values. This is for both TrialMind and the baselines. Nonetheless, for
thebaseline,theannotatorsalsoneedtomanuallyextracttheresultvaluesandstandard-
ize the values to make them ready for meta-analysis, which forms the GPT-4+Human
baseline in the experiments.
Weengagedtwogroupsofannotatorsforourevaluation: (1)threecomputerscientists
with expertise in AI applications for medicine, and (2) five medical doctors to assess the
generatedforestplots. Eachannotatorwasaskedtoevaluatefivereviewstudies. Foreach
review,werandomlypresentedforestplotsgeneratedbyboththebaselineandTrialMind.
The annotators were required to determine how closely each generated plot aligned with
a reference forest plot taken from the target review paper. Additionally, they were asked
to judge which method, the baseline or TrialMind, produced better results in a win/lose
assessment. Fig.5ademonstratestheuserinterfaceforthisstudy,whichwascreatedwith
Google Forms.
19Boolean search query: ("cancer" OR "oncology" OR "neoplasm" OR "carcinoma" OR
Boolean search query from "melanoma" OR "leukemia" OR "lymphoma" OR "sarcoma") AND ("therapy" OR
the PubMed database "treatment" OR "chemotherapy" OR "radiation therapy" OR "immunotherapy" OR
(n = 46,192) "targeted therapy" OR "surgical treatment" OR "hormone therapy") AND ("meta-analysis"
OR "systematic review")
Chemotherapy: "chemotherapy", Immune modulators: "immune system
"chemo", "chemotherapies", "chemos‚Äú modulators", "immune system
(n = 149) modulator", "immunomodulators",
"immunomodulator", "cytokines",
"interferons", "interleukins", "BCG",
Applying filters: PMC Full Hormone therapy: "hormone therapy", "biological response modifiers",
"Erythropoietin", "IL-11", "Granulocyte-
text available, Meta- "hormonal therapy", "hormone
macrophage colony-stimulating factor",
Analysis, 2020/1/1 - treatment", "endocrine therapy" "granulocyte colony-stimulating factor",
2023/1/1, Humans, English (n = 17) "GM-CSF", "G-CSF"
(n = 2,691) (n = 7)
Hyperthermia: "hyperthermia",
Monoclonal antibodies: "monoclonal
"microwave", "radiofrequency", "lasers",
antibodies", "monoclonal antibody",
"ultrasound", "perfusion"
"antibodies", "antibody", "targeted
(n = 17)
therapy", "targetd cancer therapy",
"targeted cancer therapies"
(n = 23)
Checkpoint inhibitors: "immune
checkpoint inhibitors", "immune
Cancer vaccine: "cancer vaccines",
Mesh terms that contain checkpoint inhibitor", "checkpoint
"cancer vaccine", "cancer treatment
"cancer", "tumor", inhibitors", "checkpoint inhibitor‚Äú vaccines", "vaccine", "vaccines"
"neoplasms", "carcinoma", (n = 94) (n = 8)
"myeloma", or "leukemia"
(n = 1,967) T-cell transfer therapy: "t-cell therapy",
Stem cell transplant: "stem cell
"t cell therapy", "t-cell therapies", "t cell
transplant", "stem cell transplants",
therapies", "adoptive cell therapy",
"stem cell transplantation", "stem cell
"adoptive t-cell therapy", "adoptive t cell
transplantations", "bone marrow
therapy", "adoptive t-cell therapies",
transplant", "bone marrow transplants"
"adoptive t cell therapies", "adoptive cell
(n = 2)
therapies", "car-t", "TIL therapy", "tumor
infiltrating lymphocytes", "tumor-
infiltrating lymphocytes", "tumor
Brachytherapy: "brachytherapy", infiltrating lymphocyte", "tumor-infiltrating
Find meta-analyses focusing "brachytherapies", "brachytherapy‚Äú lymphocyte"
on cancer therapies by (n = 4) (n = 10)
keyword filtering
Keywords used to select meta-analyses on different types of cancer therapies
(n = 352)
Manual screening guidelines:
‚Ä¢ Focus exclusively on papers that study the treatment effects (such as safety, efficacy)
of cancer interventions.
‚Ä¢ Include only papers that compare a single treatment against a control group (e.g.,
placebo, standard of care), not multiple treatments.
‚Ä¢ Give preference to studies examining individual treatment vs. control over joint
therapies (e.g., Therapy A+B vs. A only).
‚Ä¢ Prioritize diversity in target conditions treated by each intervention.
Manual screening of titles
‚Ä¢ Favor studies that involve a larger number of individual studies.
and abstracts ‚Ä¢ Exclude papers that are network analyses.
(n = 25) ‚Ä¢ Omit meta-analyses focused solely on prognostic and predictive effects.
Extended Fig. 1: The flowchart of the screening process of meta-analyses involved in the
MetaSyns dataset.
20Make a
classification of
the input data
Consolidate the
raw data to the
target outcome
Make the final calculation
to get the standardized
outcomes
Extended Fig. 2: The example Python code made by TrialMind when converting the ex-
tracted result values to standardized tabular form.
21Prompt for initial query generation
You are a clinical specialist. You are conducting a clinical study meta-analysis.
The research is defined by the following PICO elements:
P (Patient, Problem or Population): {P}
I (Intervention): {I}
C (Comparison): {C}
O (Outcome): {O}
## Task
Your task is to identify the primary clinical term(s) in this research.
The clinical terms should be specific medical conditions, treatments, or procedures.
General terms such as 'patients', or 'therapy' should not be included.
## Reply Format
You should only reply with 1~3 primary term. Your output should be in JSON format,
like this:
{{
"terms": ["term1", "term2", "term3"]
}}
Extended Fig. 3: Prompt for generating initial search queries in the literature search.
22Prompt for query expansion and refinement
## Background
You are a clinical specialist. You are conducting a clinical meta-analysis.
The research is defined by the following PICO elements:
P (Patient, Problem or Population): {P}
I (Intervention): {I}
C (Comparison): {C}
O (Outcome): {O}
## Reference
You've already gathered these related papers:
{pubmed_reference_text}
## Task
Your task is to further your literature search by these 3 steps:
### Step 1
Extract related term in the reference papers.
Provide three lists of query terms: TREATMENTS, CONDITIONS, and OUTCOMES.
CONDITIONS: words about any conditions or disease that is related to this meta-analysis (referring to Problem section)
TREATMENTS: primary related clinical terms/keywords showed in these reference papers (referring to Intervention section)
OUTCOMES: clinical endpoints or outcome measurements that are related to this meta-analysis (referring to Outcome section)
### Step 2
Double-check these query terms, remove the terms that is not directly related to the PICO elements of this research.
Provide three lists of refined core terms: CORE_CONDITIONS, CORE_TREATMENTS, and CORE_OUTCOMES.
CORE_CONDITIONS: refined terms of conditions or disease
CORE_TREATMENTS: refined terms of primary related clinical terms/keywords
CORE_OUTCOMES: refined terms of clinical endpoints or outcome measurements
### Step 3
To expand the scope of query term searches, please extend each query term by:
1. Synonyms and other names/forms;
2. Possible abbreviations or full forms;
3. Split into elements for compound phrases.
Provide three lists of expanded query terms: EXPAND_CONDITIONS, EXPAND_TREATMENTS, EXPAND_OUTCOMES.
EXPAND_CONDITIONS: expanded terms of conditions or disease
EXPAND_TREATMENTS: expanded terms of primary related clinical terms/keywords
EXPAND_OUTCOMES: expanded terms of clinical endpoints or outcome measurements
## Reply format
There should be no overlap between each pair of lists
Your reply should be in a format like:
{{
"step 1": {{
"CONDITIONS": [condition1, condition2, ..] \\ (~10 items)
"TREATMENTS": [term1, term2 .. ] \\ (~10 items)
"OUTCOMES": [outcome1, outcome2, ..] \\ (~10 items)
}},
\\ Refine according to P (Patient, Problem or Population): {P} and I (Intervention): {I} and O (Outcome): {O}
"step 2": {{
"CORE_CONDITIONS": [condition1, condition2, ..] \\ (~5 items)
"CORE_TREATMENTS": [term1, term2, .. ] \\ (~5 items)
"CORE_OUTCOMES" : [outcome1, outcome2 ..] \\ (~5 items)
}},
\\ Augmentation
"step 3": {{
"EXPAND_CONDITIONS": [condition1, condition2, ..] \\ (~10 items)
"EXPAND_TREATMENTS": [term1, term2 ..] \\ (~10 items)
"EXPAND_OUTCOMES": [outcome1, outcome2 ..] \\ (~10 items)
}}
}}
Extended Fig. 4: Prompt for expanding and refining the initial search queries in the literature
search.
23Prompt for study eligibility criteria generation
You are a clinical specialist. You are conducting a clinical meta-analysis.
The research is defined by the following PICO elements:
P (Patient, Problem or Population): {P}
I (Intervention): {I}
C (Comparison): {C}
O (Outcome): {O}
## Task
Your task is to design the eligibility criteria for selecting studies for this meta-analysis
study following these 3 steps:
### Step 1
Based on the PRISMA guidelines and the PICO elements of this research, please
identify five eligibility criteria for the studies to be included in the meta-analysis.
Provide a rationale for each criterion.
ELIGIBILITY_ANALYSIS: your items and reasons here...
### Step 2
Next, create {num_title_criteria} binary questions that will help you select studies
based on their titles.
These questions should be designed so that a "YES" answer indicates the study
meets the criteria, while a "NO" answer means it doesn't.
The information required to answer these questions should be general and easily
found in the study title.
TITLE_CRITERIA n: ...
### Step 3
Finally, develop {num_abstract_criteria} more binary questions to further filter the
studies based on their content.
These questions should also be designed for a "YES" or "NO" answer,
but the information required to answer them will be more detailed and is expected to
be found within the main content of the study.
CONTENT_CRITERIA n: ...
## Reply Format
You should reply in a format like:
{{
"ELIGIBILITY_ANALYSIS": ["rationale1", "rationale12", ...] \\ the bullet points of
your analysis
"TITLE_CRITERIA": ["criterion1", "criterion2", "..."] \\ the {num_title_criteria} binary
title-based criteria
"CONTENT_CRITERIA": ["criterion1", "criterion2", "..."] \\ the
{num_abstract_criteria} binary content-based criteria
}}
Extended Fig. 5: Prompt for study eligibility criteria generation in the literature screen.
24Prompt for study eligibility assessment
# CONTEXT #
You are a clinical specialist tasked with assessing research papers for inclusion in a
meta-analysis based on specific eligibility criteria.
# OBJECTIVE #
Evaluate each criterion of a given paper to determine its eligibility for inclusion in the
meta-analysis. Provide a list of decisions ("YES", "NO", or "UNCERTAIN") for each
eligibility criterion. You must deliver exactly {num_criteria} responses.
# IMPORTANT NOTE #
If the information within the provided paper content is insufficient to conclusively
evaluate a criterion, you must opt for "UNCERTAIN" as your response. Avoid making
assumptions or extrapolating beyond the provided data, as accurate and reliable
responses are crucial, and fabricating information (hallucinations) could lead to
serious errors in the meta-analysis.
# PICO FRAMEWORK #
- P (Patient, Problem or Population): {P}
- I (Intervention): {I}
- C (Comparison): {C}
- O (Outcome): {O}
# PAPER DETAILS #
- Provided Paper: {paper_content}
# EVALUATION CRITERIA #
- Number of Criteria: {num_criteria}
- Criteria for Inclusion: {criteria_text}
# RESPONSE FORMAT #
You are required to output a JSON object containing a list of decisions for each of
the {num_criteria} eligibility criteria. Each decision should directly correspond to one
of the criteria and be listed in the order they are presented. Ensure to use
"UNCERTAIN" wherever the paper does not explicitly support a "YES" or "NO"
decision.
For example:
```json
{{
"evaluations": ["YES", "NO", "UNCERTAIN", "YES", "YES", ...] \\ List of
{num_criteria} decisions
}}
```
Extended Fig. 6: Prompt for study eligibility assessment in the literature screen.
25Prompt for study characteristics extraction
You are now the following python function: ```
def extract_fields_from_input_study(inputs: Dict[str, Any]) -> str:
\"\"\"
This function is tasked with analyzing clinical trial study reports or papers to extract specific information as
structured data
and provide citations for the extracted information.
The user will provide a list of fields they are interested in, along with a natural language description for each
field to guide you on what content to look for and from which parts of the report to extract it.
IMPORTANT:
For each field described by the user, you need to:
1. Identify and extract the relevant information from the report based on the provided description.
2. Generate a field name that accurately represents the content of the field based on its description.
3. Structure the extracted information into a standard format whenever possible (e.g., integer, numerical values,
dates, keywords, list of terms).
If standardization is not possible, the information should be presented in text format.
If the field is not found in the report, the extracted value should be "NP".
4. Provide a reference to the document ID from which this information was extracted.
This citation id should be restricted to be integers only.
You should NOT cite more than three sources for a single field.
You should try your best to provide the most relevant and specific citation for each field.
If two or more sources are equally relevant, you can just cite one of them.
The function returns a string representing a dictionary with each key representing a field and its extracted
value. The format should be as follows:
Returns: A syntactically correct JSON string representing a list of dictionary with three keys: name, value, and
source_id.
Format:
```json
[
{{
"name": \\ str, length <= 25 tokens
"value": \\ str, length <= 25 tokens
"source_id": \\ list[int], length <= 3 ids
}},
{{
"name":, \\ str, length <= 25 tokens
"value": \\ str, length <= 25 tokens
"source_id": \\ list[int], length <= 3 ids
}},
...
]
```
\"\"\"
```
Respond exclusively with the generated JSON string wrapped ```json and ```.
# User provided inputs
paper_content = \"\"\"{paper_content}\"\"\"
fields = \"\"\"{fields}\"\"\"
inputs = {{
"paper_content": paper_content,
"fields": fields
}}
Extended Fig. 7: Prompt for study characteristics extraction in the data extraction.
26Prompt for initial result extraction and localization
You are now the following python function: ```
def locate_evidence_in_study_about_the_request_for_results(inputs: Dict[str, Any]) -> str:
\"\"\"
This function is tasked with analyzing clinical trial study reports or papers to extract specific information as structured data.
Task Instructions:
1. Review the clinical trial paper, paying close attention to the sections discussing results related to the "{target_outcome}" for the
defined cohort "{cohort}".
2. Summarize the findings for each cohort, emphasizing the collective data and general trends observed. Individual patient data
should only be mentioned if highlighted as a significant exception or case study in the paper.
3. Present your summary in a table format with the following columns:
- 'Group Name': Name of the cohort.
- 'Number of Patients': Total participants in the cohort.
- 'Specified Outcome Measure': Key findings and metrics related to the "{target_outcome}".
Include aggregate values such as percentages, mean values, and other statistical summaries that reflect the overall results for the
group.
Must contain quantitative data, such as hazard ratios, odds ratios, mean differences, count of events, etc.
Do not include qualitative or descriptive data that cannot be quantified, such as "statistically significant improvement" without
specific values.
Here are the definitions of some common outcome measurements:
- overall survival/progression-free survival/etc.: usually defined by the hazard ratio or odds ratio, which is the ratio of hazard rate or
odds of an event occurring in the treatment group to that in the control group.
it can also be expressed as the number of events in the target group of patients.
- toxicity/adverse events/etc.: defined by the rate of occurrence of adverse events in the target groups of patients.
- objective response rate/overall response/etc.: defined by the proportion of patients who respond to the treatment in the target
groups, or number of patients with complete or partial response.
- disease control rate/relapse rate/etc.: defined by the proportion of patients who have stable disease or better in the target groups,
or number of patients with disease control.
it can also be expressed by the number of patients who have disease progression, so the disease control rate is 1 minus the
progression rate.
Returns:
A str representing a list of dictionary with three keys: Group Name, N, and Results.
Group Name: str - name of the cohort
N: int - number of participants in the cohort
Results: str - key findings and metrics related to the outcome measure, must be quantitative and concise
adhere to the input paper content.
Example format:
```json
[
{{
"Group Name": str, \\ the name of the cohort
"N": int, \\ the number of participants in the cohort
"Results": str \\ key findings and metrics related to the outcome measure, must be quantitative and concise (<= 50 tokens)
}},
{{
...
}},
...
]
```
\"\"\"
```
Respond exclusively with the generated JSON string wrapped ```json and ```.
# User provided inputs
paper_content = \"\"\"{paper_content}\"\"\"
cohort = \"\"\"{cohort}\"\"\"
target_outcome = \"\"\"{target_outcome}\"\"\"
inputs = {{
"paper_content": paper_content,
"cohort": cohort,
"target_outcome": target_outcome
}}
Extended Fig. 8: Prompt for the initial result extraction in the evidence synthesis.
27Prompt for the study result table formatting
You are now the following python function: ```
def format_study_result_table(inputs: Dict[str, Any]) -> str:
\"\"\"
This function is used to transform raw data (with the content described in texts) from a clinical study paper into a structured table format.
IMPORTANT: Organize the extracted data into a structured table format. Ensure that each column represents a crucial numerical data
point necessary for meta-analysis. This may include participant numbers (N), measurable outcomes,
and other quantifiable metrics related to the intervention or comparator.
IMPORTANT: The input groups are not mutually exclusive, you need to decide whether to combine them or
select the eligible groups based on the research question.
IMPORTANT: Drop the group if no outcome value is provided.
The function returns a string representing a list of dictionary, each dictionary has three keys:
```json
[
{{
Group: str \\ the name of the group
N: int \\ the number of participants in the group
Outcome Value: float or int \\ the outcome value for the group, must be float or int values
}},
{{
...
}},
...
]
```
Example output 1:
```json
[
{{
"Group": "Patient with irAEs",
"N": 100,
"Hazard Ratio": 0.5,
}},
{{
"Group": "Patient without irAEs",
"N": 150,
"Hazard Ratio": 0.3,
}},
]
```
[‚Ä¶ some more examples]
Returns:
A str representing a list of dictionary with three keys: Groups, N, and Outcome.
Groups: str - the name of the group
N: int - the number of participants in the group
{outcome}: float or int - the outcome value for the group, must be float or int
\"\"\"
```
Respond exclusively with the generated JSON string wrapped ```json and ```.
# User provided inputs
results = \"\"\"{results}\"\"\"
outcome = \"\"\"{outcome}\"\"\"
intervention = \"\"\"{intervention}\"\"\"
comparator = \"\"\"{comparator}\"\"\"
population = \"\"\"{population}\"\"\"
inputs = {{
"results": results,
"outcome": outcome,
"intervention": intervention,
"comparator": comparator,
"population": population
}}
Extended Fig. 9: Prompt for the result formatting in the evidence synthesis.
28Prompt for the study result standardization
You are now the following python function: ```
def generate_continuous_elegant_python_code(inputs: Dict[str, Any]) -> str:
\"\"\"
This function is used to generate python code to transform raw data into a structured format according to a specified schema.
The raw data is collected from a clinical study paper and needs to be organized to facilitate a meta-analysis study.
The target meta-analysis is driven by a specific research question, defined by the Population, Intervention, Comparator, and Outcome
(PICO) elements.
The function takes a dictionary of `inputs` as an argument, which contains the following keys:
- 'research_question': contains the population, intervention, comparator, and outcome (PICO) elements of the targeted meta-analysis.
Based on the research question's intervention and comparator, you need to classify which arms in the raw data belong to the targeted
experimental and control groups.
Based on the research question's population, you need to consider which parts of the observations in the raw data are relevant to the
targeted meta-analysis.
- 'raw_data': this is the markdown formatted input dataframe that contains the raw data to be transformed.
- 'desc': the description of the target dataframe the generated code needs to produce.
- 'target_output': the generated code needs produce a dataframe follows the structure of this target_output.
IMPORTANT: 'raw_data' does **NOT** need to be parsed or loaded into a dataframe. In your generated code,
just use `df` to represent the dataframe that contains the raw data.
IMPORTANT: NaN values should be treated with caution. If the raw data contains NaN values, ensure that your code handles them
appropriately,
either by removing the row with any NaN values or by filling all NaN with zeros.
IMPORTANT: Never use `pd.DataFrame.append` to add data to pd.DataFrame. This method has been deprecated, and it is
recommended to use `pd.concat` instead.
For example, to add a new row to a dataframe, you can use `df = pd.concat([df, new_row], ignore_index=True)`
Never use `df.append(new_row, ignore_index=True)`
The function returns a string of raw Python code, wrapped within <code> and </code> tags. For example:
<code>
# df
# Your generated python code here...
</code>
You should implemented three functions in your code, each of which takes a dataframe as input and returns a dataframe as output:
<code>
# Dataframe definition should be ignored and not show again
# df = ...
def classify_arms(df: pd.DataFrame):
# you need to define a new column in the dataframe to indicate which group each arm belongs to
# each arm should be classified as either experimental or control
# e.g., `df['Group'] = ['Experimental', 'Control', 'Experimental', ...]`
return df
def consolidate_data(df: pd.DataFrame):
# consolidate the data for each group
return df
def calculate_statistics(df: pd.DataFrame):
# calculate the statistics for each group
return df
# run these functions in sequence to get the final dataframe
df = classify_arms(df)
df = consolidate_data(df)
df = calculate_statistics(df)
</code>
Returns:
Executable Python code that will be used to transform the raw data into the structured format.
\"\"\"
```
Respond exclusively with the generated code wrapped <code></code>. Ensure that the code you generate is executable Python code that
can be run directly in a Python environment, requiring no additional string encapsulation.
# User provided inputs
research_question = \"\"\"Population: {population}, Intervention: {intervention}, Comparator: {comparator}, Outcome: {outcome}\"\"\"
raw_data = \"\"\"{raw_data}\"\"\"
desc = \"\"\"{desc}\"\"\"
target_output = \"\"\"{target_output}\"\"\"
inputs = {{
"research_question": research_question,
"raw_data": raw_data,
"desc": desc
"target_output": target_output
}}
Extended Fig. 10: Prompt for the result standardization in the evidence synthesis.
29Extended Table 1: The characteristics of involved meta-analyses papers in the MetaSyns
dataset. N : the number of identified studies; N : the number of involved studies after
1 2
screening; N : the number of involved participants in the studies; P: population; I/C: inter-
3
vention and comparison; O: outcome measurements.
Ref Topic N1 N2 N3 P I/C O
Valle,etal. Brachytherapy 2553 150 11322 men treated with a Salvageradicalprosta- 1. 2-yr and 5-yr
202132 number of salvage tectomy (RP), high- Relapse free survival
treatments for ra- intensity focused (RFS); 2. severe gen-
diorecurrent prostate ultrasound (HIFU), itourinary (GU) toxic-
cancer. cryotherapy,stereotac- ity;3. severegastroin-
tic body radiotherapy testinal(GI)toxicity
(SBRT),low‚Äìdose-rate
(LDR) brachytherapy,
and high-dose-rate
(HDR)brachytherapy
Li et al. Brachytherapy 11806 15 12773 prostatecancer 1. Brachytherapy 1. risk ratios (RRs)
202333 (BT); 2. External of genitourinary (GU)
beam radiother- toxicity;2. RRsofgas-
apy (EBRT); 3. trointestinal(GI)toxi-
BT+EBRT city
Hande et Brachytherapy 5499 24 5488 locallyadvancedcervi- 1. Volume-based 1. 3-year disease-
al. 202234 calcancer Brachytherapy (BT); free survival (DFS);
2. Point-AbasedBT 2. 3-yearlocalcontrol
(LC); 3. 3-year over-
allsurvival(OS);4. se-
vereGUtoxicity5. se-
vereGItoxicity
Wahyuhadi Cancer Vac- 326 6 1202 Glioblastoma multi- 1. activeimmunother- 1. Overall survival
et al. cines forme(GBM) apy: dendritic cell (OS); 2. Progression-
202244 vaccination, peptide free survival (PFS);
vaccination, DNA 3. post-treatment
vaccine, viral vector- Karnofsky perfor-
based vaccine, antigen mance scale (KFS); 4.
non-specific vaccine, serious adverse events
autologous tumor cell (AEs); 5. 2-year
therapy; 2. standard mortality
therapy (surgery,
chemotherapy, radio-
therapy)
Zhou et al. Checkpoint 2236 30 4971 patients with cancer 1. occurrence of 1. overall survival
202045 Inhibitors receiving immune immune-related AEs (OS); 2. progression-
checkpoint inhibitors (irAEs); 2. non- freesurvival(PFS)
(ICIs): nivolumab, occurrenceofirAEs
pembrolizumab,
atezolizumab, dur-
valumab,avelumab,or
ipilimumab
Leoneetal. Checkpoint 984 10 5257 advanced esophageal immunecheckpointin- 1. overallsurvivalben-
202246 Inhibitors squamous cell carci- hibitors(ICIs) efit; 2. progression-
noma(ESCC) freesurvival;3. overall
responserate
Song et al. Checkpoint 220 147 23761 cancerpatients 1. anti-PD-1 & anti- ICI-related AEs
202047 Inhibitors PD-L1 inhibitors; (Grade1-5,Grade3-5)
2. anti-CTLA-4
inhibitors
ABC Chemotherapy N/A 10 1183 participants with 1. adjuvant cisplatin- 1. overall survival;
Group, biopsy-proven tran- based chemotherapy 2. locoregional
202235 sitional cell carci- plus local treatment; recurrence-free sur-
noma with muscle- 2. local treatment; 3. vival; 3. metastasis-
invasive bladder local treatment then freesurvival;4. overall
cancer who had not adjuvant cisplatin- recurrence-free sur-
received neoadjuvant based chemotherapy vival
chemotherapy onreccurence
Lacasetal. Chemotherapy N/A 107 19805 squamous cell Head Q1: 1. loco-regional 1. overall survival;
202136 and Neck Cancer treatment (LRT); 2. 2. event-free survival;
(MACH-NC) LRT+CT; Q2: 1. in- 3. loco-regional fail-
duction CT + radio- ure (LRF); 4. dis-
therapy; 2. concomi- tant failure (DF); 5.
tantCT cancer and non-cancer
mortality
Xia et al. Chemotherapy N/A 9 36480 triple-negative breast 1. neoadjuvant 1. overall survival;
202037 cancer(TNBC) chemotherapy 2. disease-freesurvival
(NACT); 2. adju- (DFS)
vant chemotherapy
(ACT)
Liu et al. Hormone 7506 16 67616 high-riskprostatecan- 1. neoadjuvant hor- 1. overall sur-
202141 Therapy cer(HRPCa) mone therapy (NHT) vival; 2. biochem-
with radical prostate- ical progression-free
ctomy (RP); 2. RP survival; 3. cancer-
alone specific survival; 4.
disease-free survival;
5. risk rates (RRs)
of lymph node in-
volvement; 6. RRs
of pathological down-
staging; 7. RRs of
organ-confinement;
8. RRs of positive
surgical margins; 9.
RRs of seminal vesicle
invasion
30Piezzo et Hormone 685 8 4580 patients with HR- 1. CDK4/6 inhibitors 1. progression-freesur-
al. 202042 Therapy positive/HER2- withendocrinetherapy vival; 2. overall sur-
negative advanced (ET);2. ETonly vival; 3. objective re-
or metastatic breast sponserate(ORR)
cancer
Peleg Has- Hormone 113 6 35680 HR-positive/HER2- 1. adjuvant endocrine disease-freesurvival
son et al. Therapy positive early-stage therapy (tamox-
202143 breastcancer ifen); 2. aromatase
inhibitors
Kim et al. Hyperthermia 334 4 653 low-risk papillary thy- 1. thermalablation;2. 1. new tumor after
202140 roid microcarcinomas surgery treatment; 2. lymph
(PTMCs) node metastasis; 3.
complication
Wangetal. Hyperthermia 3060 9 1215 T1aN0M0 and ultrasound-guided 1. volume reduction
202239 T1bN0M0 papillary thermal ablation (LA, rate; 2. overall dis-
thyroidcarcinoma RFA,orMWA) easee progress rate; 3.
complicationrate
Spiliotis et Hyperthermia 716 15 2169 livercancer 1. microwaveablation; 1. complete ablation
al. 202138 2. radiofrequency ab- (CA); 2. local tu-
lation mor prgression (LTP);
3. intrahepaticdistant
(IDR); 4. complica-
tions
Li et al. Immune Sys- 682 9 1752 breastcancerreceiving 1. PEGylated gran- 1. risk ratios (RRs)
202048 tem Modula- chemotherapy ulocyte colony- of grade ¬ø= 3 / 4
tors stimulating factor neutropenia; 2. RRs
(G-CSF);2. G-CSF of febrile neutropenia
(FN); 3. time to ab-
soluteneutrophilcount
recovery; 4. grade 4
AEs; 5. RRs of skele-
taland/ormusclepain
Wu et al. Monoclonal 116 32 958 HER2-positive pa- HER2-targeted ther- 1. objective response
202249 Antibodies tientswithNSCLC apy rate(ORR);2. disease
control rate (DCR);
3. progression-freesur-
vival(PFS)
Li et al. Monoclonal 5861 27 15063 patients with solid tu- 1. anti-PD-1/PD-L1 RRofnephrotoxicity
202150 Antibodies mors mAbs; 2. anti-PD-
1/PD-L1 mAbs +
chemotherapy; 3.
standard chemother-
apy
Zhu et al. Monoclonal 2511 169 22492 cancer antibody‚Äìdrug conju- 1. RR of grade ¬ø= 3
202351 Antibodies gates(ADCs) AEs;2. RRofallgrade
AE
Masetti et Stem Cell 2141 9 1448 children with pedi- 1. allogeneic 1. overall survival;
al. 202254 Transplant atric acute myeloid hematopoietic stem 2. relapse rate; 3.
leukemia (AML) in cell transplanta- disease-freesurvival
first complete remis- tion (allo-HSCT);
sion(CR1) 2.chemotherapyalone
Zeng et al. Stem Cell 499 15 959 adult philadelphia 1. allogeneic 1. overall survival; 2.
202155 Transplant chromosome positive hematopoietic stem Relapse-free survival
acute lymphoblastic cell transplantation (RFS); 3. Odds Ratio
leukemia in post- (allo-HSCT); 2. tyro- of non-relapsed mor-
remission sine kinase inhibitor tality(NRM);4. Odds
(TKI) combined with Ratio of non-relapsed
chemotherapy survival(NRS)
Gagelmann Stem Cell 1050 7 680 patients with FLT3- tyrosine kinase in- 1. relapse-free sur-
et al. Transplant ITD-mutated acute hibitor (TKI) mainte- vival; 2. RRs of
202156 myeloid leukemia nancetherapy relapse; 3. overall
(AML) and received survival; 4. non-
allogeneic stem-cell replase mortality; 5.
transplation RRs of chronic graft
vs. chronic disease
(GVHD)
Yang et al. T-CellTrans- 661 23 350 patients with Relapse CAR-Ttherapy 1. Overall response
202152 ferTherapy or Refractory Multi- (OR); 2. complete
pleMyeloma(RRMM) response rate (CRR);
andtreatedwithCAR- 3. MRD negativity
Ttherapy within responders;
4. relapse rate; 5.
progression-free sur-
vival; 6. overall sur-
vival; 7. severe CRS
(sCRS); 8. Neurologic
toxicity(NT)
Shahzad et T-CellTrans- 677 13 57 relapsed/refractory CAR-Ttherapy 1. complete remission
al. 202353 ferTherapy acute myeloid (CR); 2. overall re-
leukemia(RR-AML) sponse rate; 3. inci-
denceofCRS;4. inci-
denceofICANs;5. in-
cidenceofgraft-versus-
hostdisease(GVHD)
31References
[1] Field,A.P.&Gillett,R. Howtodoameta-analysis. BritishJournalofMathematical
and Statistical Psychology 63, 665‚Äì694 (2010).
[2] Borah, R., Brown, A. W., Capers, P. L. & Kaiser, K. A. Analysis of the time and
workers needed to conduct systematic reviews of medical interventions using data
from the prospero registry. BMJ open 7, e012545 (2017).
[3] Hoffmeyer,B.D.,Andersen,M.Z.,Fonnes,S.&Rosenberg,J.Mostcochranereviews
have not been updated for more than 5 years. Journal of evidence-based medicine
14, 181‚Äì184 (2021).
[4] Marshall, I. J. & Wallace, B. C. Toward systematic review automation: a practical
guide to using machine learning tools in research synthesis. Systematic reviews 8,
1‚Äì10 (2019).
[5] Brown, T. et al. Language models are few-shot learners. Advances in Neural Infor-
mation Processing Systems 33, 1877‚Äì1901 (2020).
[6] Wang, S., Scells, H., Koopman, B. & Zuccon, G. Can chatgpt write a good boolean
queryforsystematicreviewliteraturesearch? InProceedingsofthe46thInternational
ACM SIGIR Conference on Research and Development in Information Retrieval,
1426‚Äì1436 (2023).
[7] Wadhwa, S., DeYoung, J., Nye, B., Amir, S. & Wallace, B. C. Jointly extracting
interventions,outcomes,andfindingsfromrctreportswithllms.InMachineLearning
for Healthcare Conference, 754‚Äì771 (PMLR, 2023).
[8] Syriani, E., David, I. & Kumar, G. Assessing the ability of chatgpt to screen articles
for systematic reviews. arXiv preprint arXiv:2307.06464 (2023).
[9] Shaib, C. et al. Summarizing, simplifying, and synthesizing medical evidence using
gpt-3 (with varying success). In The 61st Annual Meeting Of The Association For
Computational Linguistics (2023).
[10] Wallace,B.C.,Saha,S.,Soboczenski,F.&Marshall,I.J. Generating(factual?) nar-
rativesummariesofRCTs: Experimentswithneuralmulti-documentsummarization.
AMIA Summits on Translational Science Proceedings 2021, 605 (2021).
[11] Yun, H., Marshall, I., Trikalinos, T. & Wallace, B. C. Appraising the potential uses
and harms of llms for medical systematic reviews. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language Processing, 10122‚Äì10139 (2023).
[12] Christopoulou,S.C.Towardsautomatedmeta-analysisofclinicaltrials: Anoverview.
BioMedInformatics 3, 115‚Äì140 (2023).
[13] NationalCancerInstitute. Types of cancer treatment. https://www.cancer.gov/
about-cancer/treatment/types. Accessed: 2024-04-24.
[14] Valle, L. F. et al. A systematic review and meta-analysis of local salvage therapies
afterradiotherapyforprostatecancer(master).Europeanurology 80,280‚Äì292(2021).
[15] Li, X. et al. Comparison of chronic gastrointestinal and genitourinary toxicities
between brachytherapy and external beam radiotherapy for patients with prostate
cancer: A systematic review and meta-analysis. Technology and Health Care 1‚Äì16
(2023).
[16] Hande, V. et al. Point-a vs. volume-based brachytherapy for the treatment of cervix
cancer: A meta-analysis. Radiotherapy and Oncology 170, 70‚Äì78 (2022).
[17] Burdett, S. et al. Adjuvant chemotherapy for muscle-invasive bladder cancer: a
systematic review and meta-analysis of individual participant data from randomised
controlled trials. European urology 81, 50‚Äì61 (2022).
[18] Lacas, B. et al. Meta-analysis of chemotherapy in head and neck cancer (mach-nc):
Anupdateon107randomizedtrialsand19,805patients,onbehalfofmach-ncgroup.
Radiotherapy and Oncology 156, 281‚Äì293 (2021).
32[19] Xia,L.-Y.,Hu,Q.-L.,Zhang,J.,Xu,W.-Y.&Li,X.-S.Survivaloutcomesofneoadju-
vant versus adjuvant chemotherapy in triple-negative breast cancer: a meta-analysis
of 36,480 cases. World journal of surgical oncology 18, 1‚Äì8 (2020).
[20] Spiliotis, A. E. et al. Microwave ablation compared with radiofrequency ablation for
the treatment of liver cancer: a systematic review and meta-analysis. Radiology and
oncology 55, 247‚Äì258 (2021).
[21] Wang, M.-H., Liu, X., Wang, Q. & Zhang, H.-W. Safety and efficacy of ultrasound-
guided thermal ablation in treating t1an0m0 and t1bn0m0 papillary thyroid carci-
noma: A meta-analysis. Frontiers in Endocrinology 13, 952113 (2022).
[22] Kim,H.J.,Cho,S.J.&Baek,J.H. Comparisonofthermalablationandsurgeryfor
low-risk papillary thyroid microcarcinoma: a systematic review and meta-analysis.
Korean Journal of Radiology 22, 1730 (2021).
[23] Liu, W., Yao, Y., Liu, X., Liu, Y. & Zhang, G.-M. Neoadjuvant hormone therapy
for patients with high-risk prostate cancer: a systematic review and meta-analysis.
Asian Journal of Andrology 23, 429‚Äì436 (2021).
[24] Piezzo, M. et al. Progression-free survival and overall survival of cdk 4/6 inhibitors
plus endocrine therapy in metastatic breast cancer: a systematic review and meta-
analysis. International journal of molecular sciences 21, 6400 (2020).
[25] Hasson, S. P. et al. Adjuvant endocrine therapy in her2-positive breast cancer pa-
tients: systematic review and meta-analysis. ESMO open 6, 100088 (2021).
[26] Wahyuhadi,J.etal.Activeimmunotherapyforglioblastomatreatment: Asystematic
review and meta-analysis. Cancer Control 29, 10732748221079474 (2022).
[27] Zhou, X. et al. Are immune-related adverse events associated with the efficacy of
immunecheckpointinhibitorsinpatientswithcancer? asystematicreviewandmeta-
analysis. BMC medicine 18, 1‚Äì14 (2020).
[28] Leone, A. et al. Efficacy and activity of pd-1 blockade in patients with advanced
esophageal squamous cell carcinoma: a systematic review and meta-analysis with
focus on the value of pd-l1 combined positive score. ESMO open 7, 100380 (2022).
[29] Song, P., Zhang, D., Cui, X. & Zhang, L. Meta-analysis of immune-related adverse
events of immune checkpoint inhibitor therapy in cancer patients. Thoracic cancer
11, 2406‚Äì2430 (2020).
[30] Li,X.etal.Ispegylatedg-csfsuperiortog-csfinpatientswithbreastcancerreceiving
chemotherapy? a systematic review and meta-analysis. Supportive Care in Cancer
28, 5085‚Äì5097 (2020).
[31] Wu, H.-X., Zhuo, K.-Q. & Wang, K. Efficacy of targeted therapy in patients with
her2-positive non-small cell lung cancer: a systematic review and meta-analysis.
British Journal of Clinical Pharmacology 88, 2019‚Äì2034 (2022).
[32] Li,H.etal. Nephrotoxicityinpatientswithsolidtumorstreatedwithanti-pd-1/pd-l1
monoclonal antibodies: a systematic review and meta-analysis. Investigational new
drugs 39, 860‚Äì870 (2021).
[33] Zhu,Y.,Liu,K.,Wang,K.&Zhu,H. Treatment-relatedadverseeventsofantibody‚Äì
drug conjugates in clinical trials: A systematic review and meta-analysis. Cancer
129, 283‚Äì295 (2023).
[34] Yang, Q. et al. Efficacy and safety of car-t therapy for relapse or refractory multiple
myeloma: A systematic review and meta-analysis. International Journal of Medical
Sciences 18, 1786 (2021).
[35] Shahzad, M. et al. Outcomes with chimeric antigen receptor t-cell therapy in re-
lapsed or refractory acute myeloid leukemia: a systematic review and meta-analysis.
Frontiers in Immunology 14, 1152457 (2023).
[36] Masetti, R., Muratore, E., Gori, D., Prete, A. & Locatelli, F. Allogeneic hematopoi-
etic stem cell transplantation for pediatric acute myeloid leukemia in first complete
remission: a meta-analysis. Annals of Hematology 101, 2497‚Äì2506 (2022).
33[37] Zeng, Q., Xiang, B. & Liu, Z. Comparison of allogeneic hematopoietic stem cell
transplantation and tki combined with chemotherapy for adult philadelphia chromo-
some positive acute lymphoblastic leukemia: a systematic review and meta-analysis.
Cancer Medicine 10, 8741‚Äì8753 (2021).
[38] Gagelmann, N. et al. TKI maintenance after stem-cell transplantation for flt3-itd
positive acute myeloid leukemia: a systematic review and meta-analysis. Frontiers
in Immunology 12, 630429 (2021).
[39] Wu, T., Terry, M. & Cai, C. J. Ai chains: Transparent and controllable human-ai
interaction by chaining large language model prompts. In Proceedings of the 2022
CHI conference on human factors in computing systems, 1‚Äì22 (2022).
[40] Bodenreider,O. Theunifiedmedicallanguagesystem(umls): integratingbiomedical
terminology. Nucleic acids research 32, D267‚ÄìD270 (2004).
[41] Page, M. J. et al. The prisma 2020 statement: an updated guideline for reporting
systematic reviews. Bmj 372 (2021).
[42] Song, K., Tan, X., Qin, T., Lu, J. & Liu, T.-Y. Mpnet: Masked and permuted
pre-training for language understanding (2020). 2004.09297.
[43] Jin, Q. et al. Medcpt: Contrastive pre-trained transformers with large-scale pubmed
searchlogsforzero-shotbiomedicalinformationretrieval.Bioinformatics 39,btad651
(2023).
[44] Deeks, J. J. & Higgins, J. P. Statistical algorithms in review manager 5. Statistical
methods group of the Cochrane Collaboration 1 (2010).
[45] Zhang, G. et al. Leveraging generative ai for clinical evidence synthesis needs to
ensure trustworthiness. Journal of Biomedical Informatics 104640 (2024).
[46] Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.
Advances in Neural Information Processing Systems 33, 9459‚Äì9474 (2020).
[47] Wei, J.et al. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
Advances in neural information processing systems 35, 24824‚Äì24837 (2022).
[48] OpenAI. Gpt-4 technical report (2024). 2303.08774.
[49] Anthropic. Introducing the claude 3 family. https://www.anthropic.com/news/
claude-3-family (2023). Accessed: 2024-04-24.
[50] NationalCenterforBiotechnologyInformation(NCBI).Entrezprogrammingutilities
help. https://www.ncbi.nlm.nih.gov/books/NBK25501/ (2008). Accessed: 2024-
04-24.
34