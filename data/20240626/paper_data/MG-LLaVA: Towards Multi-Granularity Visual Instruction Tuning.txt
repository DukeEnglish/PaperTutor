MG-LLaVA: Towards Multi-Granularity Visual
Instruction Tuning
XiangyuZhao1,3∗,XiangtaiLi2,3,HaodongDuan3,HaianHuang3,
YiningLi3,KaiChen3†,HuaYang1†
1ShanghaiJiaotongUniversity2S-Lab,NanyangTechnologicalUniversity
3ShanghaiAILaboratory
ProjectPage: https://phoenixz810.github.io/MGLLaVA/
Email: xiangtai94@gmail.com,zxy2855064151@sjtu.edu.cn
Abstract
Multi-modal large language models (MLLMs) have made significant strides in
various visual understanding tasks. However, the majority of these models are
constrained to process low-resolution images, which limits their effectiveness
in perception tasks that necessitate detailed visual information. In our study,
wepresentMG-LLaVA,aninnovativeMLLMthatenhancesthemodel’svisual
processing capabilities by incorporating a multi-granularity vision flow, which
includeslow-resolution,high-resolution,andobject-centricfeatures. Wepropose
the integration of an additional high-resolution visual encoder to capture fine-
graineddetails,whicharethenfusedwithbasevisualfeaturesthroughaConv-Gate
fusion network. To further refine the model’s object recognition abilities, we
incorporateobject-levelfeaturesderivedfromboundingboxesidentifiedbyoffline
detectors. Being trained solely on publicly available multimodal data through
instructiontuning,MG-LLaVAdemonstratesexceptionalperceptionskills. We
instantiateMG-LLaVAwithawidevarietyoflanguageencoders,rangingfrom
3.8B to 34B, to evaluate the model’s performance comprehensively. Extensive
evaluationsacrossmultiplebenchmarksdemonstratethatMG-LLaVAoutperforms
existingMLLMsofcomparableparametersizes,showcasingitsremarkableefficacy.
Thecodewillbeavailableathttps://github.com/PhoenixZ810/MG-LLaVA.
1 Introduction
RecentworksonMultimodalLargeLanguageModels(MLLMs)[1,2,3,4,5,6]haveachievedrapid
developmentinvisionlanguageunderstanding,visualreasoning,visualinteraction,andlocalization.
MostMLLMsadoptpre-trainedLargeLanguageModels(LLMs)asthebasearchitecturetoprocess
concatenatedvisualandlanguageembeddings. Asonerepresentativework,LLaVA[3]adoptslow-
resolution(2242,3362,etc.) imagesasinputsandalignsvisualembeddingswiththetextmodalityvia
anMLPprojectorandthenperformsinstructiontuning. ThearchitectureofLLaVAhasbeenwidely
adoptedbysubsequentworks[7,8,9,10],andhasbeenappliedtovariousvisiontasks,including
detection,segmentation,andvideounderstanding.
Real-worldimagesexhibitawiderangeofresolutions,scales,andaspectratios,posingsignificant
challengesforMLLMswithlow-resolutioninputsinrobustlyprocessingthem.Totacklethisproblem,
recentworks[11,12,8,13,14,7,15]haveproposedvariousstrategiestoaugmentthecapabilitiesof
visualencodersinMLLMs,includingtrainingondiversedatasets,utilizinghigh-resolutionimage
∗TheworkwasdoneduringaninternshipatShanghaiAILaboratory.
†CorrespondingAuthor.
Preprint.Underreview.
4202
nuJ
52
]VC.sc[
1v07771.6042:viXraFigure1: MG-LLaVAoutperformsLLaVAacrossvariousvision-languagetasks,particularlyontasks
involvingobjectrecognition.
inputs,andemployingdynamicaspectratios. Mostoftheseapproachesinvolvetheintegrationof
additionalvisualtokensthroughvarioustechniques. Despitetheseadvancements,twocriticalissues
persist: (1)Althoughobject-levelfeaturesarecrucialinnearlyallvisualunderstandingtasks,they
arecurrentlyabsentinexistingvisionencoders;(2)NoneoftheexistingMLLMshaveintegrated
multi-granularityfeatures,aclassicconceptincomputervision,intotheirframeworks.
Motivatedbytheaforementionedanalysis,weintroduceMG-LLaVA,anovelMLLMdesignedto
effectivelyprocessmulti-granularityvisualinputs,includingobject-level,originimages,andhigh-
resolutioninputs.OurframeworkbuildsuponLLaVA[3]andisspecificallytailoredtoincorporateand
managemulti-granularityinputs. Forobject-levelinputs,weemployapre-trainedopen-vocabulary
detectortoidentifyobjectboundingboxesandexecuteRoIalignmenttoacquireregionvisualtokens.
In contrast to close-set detectors, open-vocabulary detectors offer enhanced generalizability and
robustness across diverse scenes. To handle fine-grained visual inputs, we utilize a convolution-
basedbackbone[16]toextractrichervisualfeatures. Subsequently,weproposeastraightforwardyet
effectivefusionstrategytointegratetheseinputsintotheoriginalvisualtokensinLLaVA.Specifically,
weinitiallymergethefine-grainedvisualtokenswiththeoriginalvisualtokensusingasimpleConv-
Gateconvolution,andthenweappendtheobject-leveltokenstothefusedtokens. Fig.2illustratethe
differencebetweenMG-LLaVAandexistingMLLMs. Experimentalresultsquantitativelyvalidate
theefficacyofthedesignofMG-LLaVA.
We perform extensive experiments with MG-LLaVA integrated with various language encoders,
rangingfrom3.8Bto34B,tosubstantiatetheeffectivenessofMG-LLaVA.Ourevaluationencom-
passes11popularmultimodalbenchmarksforbothimageandvideo. Additionally, wepresenta
comprehensivesetofablationstudiesthatillustratetheimpactofdifferentcomponentsinMG-LLaVA.
Benefitingfrommulti-granularityvisualfeatures,MG-LLaVAdemonstratesasignificantlyenhanced
capabilityinperceptionandvisualcomprehension,outperformingestablishedcounterpartsandno-
tablysurpassingGPT-4V[17]andGeminiPro-V[18]onvariousmultimodalbenchmarks,including
MMBench[19]andSEEDBench[20].
Thecontributionofthisworkcanbesummarizedasfollows:
• WeintroduceMG-LLaVA,anadvancedmulti-modalmodeladeptatprocessingvisualinputsof
multiplegranularities,includingobject-levelfeatures,original-resolutionimages,andhigh-resolution
data. ThisadvancementsignificantlyenhancesthecapabilitiesofMLLMsinvisualperceptionand
understanding.
• WeproposetheMulti-GranularityVisionFlow,astraightforwardyeteffectivemoduledesigned
for the integration of features across various granularities, thereby significantly improving the
performanceofourmodel. Theeffectivenessofourapproachissubstantiatedthroughempirical
experiments.
• Byemployingarangeoflanguagemodelsscalingfrom3.8Bto34B,ourmodelexhibitsclear
scalabilityandamarkedproficiencyinvisualcomprehension,outperformingestablishedcounterparts
andnotablysurpassingGPT-4VandGeminiPro-VonMMBenchandSEEDBench.
2Figure 2: Comparing Different MLLM Paradigms. MG-LLaVA effectively perceives multi-
granularityvisualinputsthatincludeobject-level,low,andhigh-resolutioninputs,therebyachieving
advancedmulti-modalunderstanding.
2 RelatedWork
LargeLanguageModels. Inrecentyears,privatelargelanguagemodels(LLMs)likeGPT-4[17]
and Llama [21] have gained remarkable performance. Concurrently, a multitude of open-source
research[22,23,24,25]hasembarkedontheexplorationofLLMs. LLMshowsstrongperformance
invariousNLPtasks. However,pureLLMscannothandleimageandvideoinputs. Ourworkfocuses
ondesigningnewmultimodallargelanguagemodels,whichjointlytakevisualandlanguagetokens
asinputs. Inthiswork,weengagedarangeofLLMs[22,26,27,28]scalingfrom3.8Bto34B.The
observedperformanceacrossthesemodelshasprovedtheeffectivenessofourdesign.
MultimodalLargeLanguageModels. Multi-modalLargeLanguageModels(MLLMs)[1,2,29,
30,24,31,32,10,33,34,35]haverecentlyshowcasedthepotentialtoendowLLMswithvisual
conversational abilities. Among these models, LLaVA [31] typically built a simple architecture
thatutilizesavision-languagecross-modaladaptertobridgethegapbetweenvisionandlanguage
tokens. Someresearch[36,37,11]triedtoincreaseperformancebyutilizinghigh-resolutioninputs.
LLaVA-UHD[7]cost-effectivelyincreasedinputresolutionbydividinghigh-resolutionimagesinto
smaller slices. Subsequently, LLaVA-HR [14] and Mini-Gemini [8], endeavor to incorporate an
additionalvisualencodertoenhancehigh-resolutiondetailswithoutincreasingthecountofvisual
tokens. However,theseworksconsistentlyoverlooktheimpactoffine-grainedobject-levelfeatures,
whichcompromisestheirpotentialforenhancedperception. Incomparison,MG-LLaVAexplores
thepotentialofmulti-granularityinputbysimultaneouslyleveraginghigh-resolutioninputs,low-
resolutioninputs,andobject-levelinputs. Byflexiblyintegratingvisualtokensofmultiplegranularity,
MG-LLaVAachievessuperiorperformanceonseveralbenchmarkswithamarginalincreaseincost.
Multi-GranularityModelinginVision. Inputsofmultiplegranularityhavebeenincorporatedinto
variousdownstreamvisiontasks. Inobjectdetectionandsegmentation,theefficacyofmulti-level
featureshasbeenwell-establishedindetectingobjectsofdifferentscales[38,39,40,41,42,43,44].
Forpanopticsegmentation,somemethods[45,46,47,48,49,50]appliedamulti-granularitynetwork
totraininstance,semantic,andpartsegmentationinparallel,andsomestudies[51,52,53,54,55]
have indicated that training on various levels of abstraction can improve the performance of the
segmentationnetwork. Forexample,SAM[56]presentsamulti-granularitymaskpredictionmethod
for handling various level masks, such as things, background stuff, and parts. Motivated by the
aboveworks,weaimtocaptureinputfromvariouslevelsofperceptionintoMLLM.Inparticular,
weconstructourmodelbydevelopingmultiplevisualbranchesfordifferentgranularity, thereby
augmentingitsperceptualcapabilities.
3 Method
Inthisstudy,weintroduceMG-LLaVA,whicheffectivelyharnessesboththehigh-resolutionand
object-levelfeaturesfortheimprovementofMLLMs. ThearchitectureofMG-LLaVAisillustrated
in Fig. 3. The model is composed of two key components: (1) Multi-Granularity Vision Flow
framework for extracting visual features with different resolutions and granularities, while also
3Figure3: TheillustrationofMG-LLaVA.Topleft: TheoverallframeworkofMG-LLaVA,whichin-
cludestheMulti-GranularityVisionFlowmoduleandaLLM.Right:IllustrationofMulti-Granularity
VisionFlow,whichaimstoextractmultiplevisualfeaturesandintegratedisparatefeaturestoensure
seamlessinteraction. Botttomleft: StructureofConv-GateFusionmodule.
effectivelyintegratingdisparatefeaturestoensureseamlessinteraction. (2)Alargelanguagemodel
dedicatedtogeneratingcoherentandcontextuallyrelevantresponses.
3.1 Preliminary
Asoneofthemostextensivelyadoptedmulti-modalLLMarchitectures,LLaVAconsistsofavision
encoderf ,anMLPprojectorf ,andalanguagemodelf . GivenavisualinputV andatextual
V p L
inputT,LLaVAcomputesthevisionandlanguageembeddingsasperEq.(1),wheref represents
T
theinputembeddinglayeroff . Theresultingembeddings,E andE ,arethenconcatenatedinto
L T V
asingletokensequence,servingastheinputtotheLLM.LLaVAutilizesEq.(2)tocalculatethe
probabilityofthetargetanswerX ,whereθrepresentsthetrainableparametersandListhelength
A
ofX . Themodelistrainedonvisualinstructiontuningdatatomaximizep(X |V,T).
A A
E =f (T),E =f (f (V)) (1)
T T V p V
L
p(X |V,T)=(cid:89) p (cid:16) X[i] |Concat(E ,E[1:i−1]),X[i−1](cid:17) (2)
A θ A V T A
i=1
Despitethepromisingresults,LLaVAstillrestrainsitselfinprocessingimagesatalowresolution
(2242, 3362, etc.), which significantly hinders the model’s ability, particularly in the recognition
of small objects. Scaling to high resolution without adapting the vision encoder directly would
dramaticallyincreasethenumberofvisualtokens,renderingtheapproachineffective. Furthermore,
thevisualinputcanalsobecomplexandcontainnumerousobjectswithinanimageorvideo,which
poseschallengesforMLLMsinidentifyingsomecriticalobjects. Empirically,incorporatingobject-
levelfeaturescansignificantlyenhancethemodel’sperceptualabilities. Therefore,weintroduce
MG-LLaVA,whicheffectivelyharnessesboththehigh-resolutionandobject-levelfeaturesforthe
improvementofMLLMs.
43.2 Multi-GranularityVisionFlow
Hybrid Vision Encoders As depicted in Fig. 3(b), MG-LLaVA initially processes images at
twodifferentresolutions: low-resolutionV andhigh-resolutionV . Inthelow-resolutionbranch,
L H
wefollowtheLLaVA-1.5[31]toutilizeaCLIP-pretrainedViT[57]denotedasfL toderivelow-
V
resolution features E ∈ RN×C. The ViT feature E benefits from an expanded receptive field,
L L
capturingamorecomprehensiveviewofglobalinformation.Inthehigh-resolutionbranch,weemploy
aCLIP-pretrainedConvNeXt[16]denotedbyfH toobtainhigh-resolutionfeaturesE ∈Rh×w×C.
V H
fH effectivelyextractsdetailedfeaturesfromhigh-resolutionimages,offeringdetailedlocalinsights.
V
fL andfH downsampletheinputresolutionwithstridesof14and32,respectively. Wetherefore
V V
adjustV andV toensurethatthenumberoftokensinE andE remainsthesame(N =h×w).
L H L H
Conv-GateFusion Combiningbothlowandhigh-resolutionfeaturesasinputsresultsinadoubling
ofthevisualtokenstobeprocessed,whichiscomputationallyineffective. Moreover,thedistinct
architecturesofViTandConvNeXtleadtoadiscrepancybetweenE andE ,requiringacareful
L H
fusion process. Inspired from [14], we implement a lightweight Conv-Gate fusion network that
facilitates feature aggregation while maintaining a single resolution’s token count, as shown in
Fig.3(c). Wefirstemploy1Dconvolutionstoalignthechannelwidthsofheterogeneousfeaturesand
subsequentlyuseagatinglayertomodulatethesemanticinformationacrosslowandhighresolutions,
asdescribedinEq.(3). Thefusionmoduleisappliedtotheoutputofbothvisionencoders,resulting
inonlyamarginalincreaseincomputationalcost.
E =E +G(Conv(E ),Conv(E ))×E (3)
F L L H H
IntegrationofObject-levelFeatures Giventhesetofkobjectboundingboxesderivedfromthe
image,denotedasB = {b ,b ,··· ,b },weemploytheRegionofInterest(RoI)Aligntoextract
1 2 k
object-levelfeaturesfromthevisionfeaturesofthehigh-resolutionencoderfH. Specifically,we
V
upsampleandconcatenatefeaturesfromdifferentconvolutionalstagestoascaleof1/4theinput
size,resultinginamulti-scalefeaturerepresentationfH′,whichprovidesafine-grainedperspective.
V
The object-level features are then aligned from fH′. To maintain computational efficiency, we
V
applyaveragepoolingtoeachobjectfeatureandsubsequentlyconcatenatethemintoasequence
E ∈Rk×C,asdetailedinEq.(4).
B
E =Concat(Avg(RoIAlign(fH′,B))) (4)
B V
Aftertheaggregationandextractionofobject-levelfeatures,E andE areprocessedindividually
F B
bytwoseparateprojectors(p andp )toalignwiththetextembeddingsE . Thealignedfeatures
F B T
arethenconcatenatedasinputforLLM.Wetrymultiplestrategiestomergeobject-levelfeatures
intovisualembeddingsandfindtheconcatenationoperationyieldsthemostbeneficialresults. The
experimentsarediscussedinSec.4.3. Duringtraining,weoptimizeEq.(5)onthevisualinstruction
tuningdatatoenhancethemulti-modalcomprehensioncapabilitiesofMG-LLaVA.Forvideotraining,
weexecutetheaforementionedoperationstoeachframeandthenconcatenatetheresultsintoan
extendedsequence.
L
p(X |V ,V ,B,T)=(cid:89) p (cid:16) X[i] |Concat(p (E ),p (E ),E[1:i−1]),X[i−1](cid:17) (5)
A L H θ A F F B B T A
i=1
3.3 Training&Inference
Recently,avarietyofpowerfultaggingmodelsandopen-vocabularydetectorshaveemerged,demon-
stratingremarkableefficacy. Byusingonespecifictaggingmodeltooutputlabels,whicharethen
usedbythedetectortogenerateboundingboxes,wecaneffectivelyavoidthegenerationofnumer-
ousirrelevantboxes,contrastingwiththedirectuseofclass-agnosticdetectors. Thedetailsofthe
inferencepipelineareillustratedinAppxD.Fortheacquisitionofobjectboundingboxes,weemploy
thewell-pretrainedRAM[58]asthetaggingmodelandOWL-ViTv2[59]asdetector. Thegenerated
boundingboxesarefilteredbyNMSandthenfedtomodelsfortrainingandinference. Itisimportant
tonotethatwhiletheRAMmodelaidsingeneratingtags,thesetagsservesolelyasinputsforthe
5open-vocabularydetectortodeterminetheboundingboxesandarenotintegratedintothetraining
phase.
FollowingLLaVA-1.5[31],weconductatwo-stagetrainingprocess. Duringthepretrainingstage,
wefreezeallvisualencodersandtheLLM,andonlytrainthefusionmodule,visualprojector,and
boxprojector. Thisaimstorefinethefusionmodule’scapabilitytoaggregatefeaturesoflowand
highresolutionsandtoenhancetheprojector’salignmentofvisualfeatureswiththetextembeddings.
During instruction tuning, we keep the visual encoders frozen to maintain the integrity of high-
qualityimagefeatureextractionandfinetunetheremainingcomponentstoenhancemulti-modality
comprehension.
4 Experiments
4.1 ImplementationDetails
ModelSettings. Inthiswork,allexperimentsareconductedbasedonXtuner[60]. Specially,we
chooseCLIPpre-trainedViT-Large-14-336[57]asalow-resolutionvisualencoderandtheLAION
pre-trained ConvNext-Large-320 [16] for high-resolution vision encoder. For the generation of
bounding boxes, we have selected RAM-Plus [58] as the tagging model and OWL-ViTv2-large-
patch14-ensemble[59]astheopen-vocabularydetector.
Datasets. Duringtheimage-basedtrainingstage,ourdatasetcomprises558Kimage-captionpairs
fromLAION-CCSBU[61]and708kimage-captionpairsfromALLaVA-4V-Captiondataset[62],
culminating in a total of 1.26M image-caption pairs for pretraining. The datasets employed for
instruction-tuningencompass665KmixturedatasetfromLLaVA-Instruct[31],692kinstructionsfrom
ALLaVA-4V-Instructiondataset[62],andanadditional25kinstructionsderivedfromacombination
of ShareGPT4V [63], DocVQA[64], DVQA [65] and AI2D [66], witha total number of more
than 1.3M image-text conversations. The superior quality of this dataset contributes to a swift
enhancementinperformance. Forvideotraining,followingVideo-LLaVA[10],wecombine558K
image-textpairsand703kvideo-textpairsforvideopertaining. Forinstruction-finetuning,weutilize
a665kimage-textinstructiondatasetfromLLaVAanda100kvideo-textinstructiondatasetfrom
Video-ChatGPT[9].
TrainingDetails. Wefixallseedsacrossthetrainingproceduresforfairness,whereweadoptthe
XTunercodebase[60]. Weestablishedthelow-resolutionparameterat336andthehigh-resolution
parameterat768. Forvideotraining,weuniformlyextract8framesfromeachvideo. Duringthe
pretrainingstage,weemployabatchsizeof32perdeviceandanaggregatebatchsizeof256. Inthe
instruction-tuningphase,wereducethebatchsizeto16perdeviceandanoverallbatchsizeof128.
Theinitiallearningrateissetto1e-3forthepretrainingstageand2e-5fortheinstruction-tuningstage.
Thenumberofboundingboxesperimageislimitedto100duringtraining.Theentiretrainingprocess
takesapproximately23hourswhenusingtheVicuna7B[22]modelusing8×A100GPUs. Forour
mostextensivemodel,theYi1.5-34B[28],weutilize32×A100GPUsandfinalizetheoptimization
processinroughlythreedaysbyemployingtheDeepSpeedZero3strategy.
4.2 MainResults
PerceptionBenchmarks. InTab.1,wecompareourMG-LLaVAwithpreviousleadingapproaches
acrossseveralsettingsonMulti-Modalbenchmarks,whichmainlyconcentrateonperceptioncapabil-
ity,includingMMBench-DevandMMBench-Test[19],SEEDBench-Image[20],andMMStar[71].
MMBenchisdedicatedtoadvancingtheunderstandingofmulti-modalperceptionandcognition,and
SEEDBenchprovidesacomprehensiveandobjectiveevaluationofMLLM.MMStarfurtherensures
each selected sample exhibits visual dependency. MG-LLaVA exhibits a significantly enhanced
perceptualcapabilitycomparedtoawiderangeofMLLMs. OurMG-LLaVAequippedwithphi3-
3.8B[26]showsuperiorperformancethanMiniCPMV2[70]of+3.8%onbothMMBenchDevand
Test,and+3.1%onSEEDBench. UtilizingVicuna-7B[22],MG-LLaVAoutperformsallmodelswith
vicuna-7Bandeven13BonMMBenchandSEEDBench,surpassingLLaVA-1.5-7Bbyanaverageof
5.1%acrossfourbenchmarks. Moreover,withYi1.5-34B[28],MG-LLaVAconsistentlyoutperforms
GPT-4VonMMBenchandSEEDBench. Concurrently,itmaintainsequivalentefficacytoGPT-4V
6Table1: Comparisonwithleadingmethodsonseveralpopularvisualbenchmarksthatconcentrateon
perception. Params. denotesthetotalnumberofparameterswithinthemodel. Res. referstothe
resolutionoftheinputimage,whichisassumedtobesquarebydefaultunlessotherwiseindicated.
Thenotation‘()’signifiesthepresenceofbothlow-resolutionandhigh-resolutioninputs,withthe
numberinsidetheparenthesesspecifyingthehigherresolution.
Method LLM Param. Res. MMBD MMBT SEEDI MMStar
PrivateModels
GPT-4V[17] - - - 75.1 77.0 72.3 49.7
GeminiProVision[18] - - - 75.2 73.6 70.7 38.6
Qwen-VL-Plus[24] - - - 66.2 67.0 65.7 39.7
Open-sourceModels
BLIP-2[67] Vicuna-13B 14.2B 224 - - 46.4 -
InstructBLIP[30] Vicuna-7B 8.2B 224 - 36 53.4 -
Shikra[68] Vicuna-13B 7.3B 224 58.8 60.2 - -
IDEFICS-9B[69] LLaMA-7B - 224 48.2 45.3 - -
IDEFICS-80B[69] LLaMA-65B - 224 - 54.6 - -
Qwen-VL[24] Qwen-7B 9.6B 448 38.2 32.2 56.3 -
Qwen-VL-Chat[24] Qwen-7B 9.6B 448 60.6 61.8 58.2 37.5
LLaVA-1.5[31] Vicuna-7B 7.2B 336 65.2 66.5 66.1 30.3
LLaVA-1.5[31] Vicuna-13B 13.4B 336 69.2 69.2 68.2 32.8
SPHINX[12] Vicuna-7B 10B 224 66.9 - 69.1 -
SPHINX-1k[12] Vicuna-7B 10B 448 67.1 - 71.6 -
Mini-Gemini[8] Vicuna-7B 7.4B 336(768) 69.3 - - -
MiniCPM-V2[70] MiniCPM-2.4B 2.8B 448 69.6 69.1 67.1 39.1
MOVA[13] Vicuna-7B 10B 576 70.4 - - -
LLaVA-HR[14] Vicuna-13B 13.4B 448(1024) - - 64.5 -
LLaVA-UHD[7] Vicuna-13B 13.4B 672×1008 68.0 - - -
OurModels
MG-LLaVA Phi3-3.8B 4.2B 336(768) 73.4 72.9 70.2 35.5
MG-LLaVA Vicuna-7B 7.4B 336(768) 72.1 71.9 69.4 35.1
MG-LLaVA LLaMA3-8B 8.4B 336(768) 76.5 76.6 71.5 36.9
MG-LLaVA Vicuna-13B 13.6B 336(768) 72.2 73.5 70.8 34.1
MG-LLaVA Yi1.5-34B 34.4B 336(768) 80.1 79.1 73.7 47.9
onMMStar. Incorporatingmulti-granularityvisualinputs,MG-LLaVAdevelopsitscapabilityof
capturingdetailswithintheimage. MorecasesareexhibitedinAppx.B.
VisualQuestionAnsweringBenchmarks. Inthissection,weanalyzeMLLM’scapabilityofvisual
conversation. Thebenchmarkscanbedividedintotwogroups: (1)Benchmarksrequireunderstanding
the text within images to provide answers, including TextVQA(VQAT) [72] and DocVQA [73].
Wereporttheaccuracyofbothvalidationsets. (2)Generalvisualquestionansweringbenchmarks
suchasVQA-V2[74],ScienceQA-Image(SQAI)[75],AI2D[66]. TheevaluationresultsonVQA
benchmarksareshowninTab.2. MG-LLaVAalsodemonstratesconsiderableproficiencyonVQA
benchmarks. When equipped with Vicuna-7B and 7.4B parameters, MG-LLaVA surpasses both
SPHINX-1k [12], which has 10B parameters, and Mini-Gemini with 7.4B parameters on these
benchmarks. Operating under identical parameter conditions, MG-LLaVA, with low-resolution
inputof336andhigh-resolutionof768,outperformsLLaVA-UHD[7],whichincorporatesaninput
resolutionof672×1008onVQAT,SQAI andAI2D.MG-LLaVAexhibitsitspotentialforexpansion
when integrated with larger LLM. With Yi1.5-34B [28], MG-LLaVA surpasses the majority of
establishedbaselinesacrossawidearrayofVQAbenchmarks.
VideoQuestionAnsweringBenchmarks. Todemonstratetheeffectivenessofourapproach,we
haveexpandedourmodeltoencompassvideocomprehension. WeevaluateourmodelsonMSVD
andMSRVTT,andresultsareshowninTab.3. MG-LLaVAoutperformsVideo-LLaVA[10]onboth
benchmarks,whichfurtherprovestheefficiencyofMG-LLaVA.Invideounderstanding,MG-LLaVA
demonstratesproficiencyinidentifyingthecriticalobjectinthevideo. Moreillustrativeinstancesare
depictedinAppx.B.
4.3 AblationExperiments
Inthissection,weconductcomprehensiveablationstudiesofourmodel. Theablationexperiments
arebasedonthetrainingdataprovidedbyLLaVA-1.5[31],withafixedseedprotocoltoensurethe
stabilityandcomparabilityoftheexperimentalconditions.
7Table2: ComparisonwithlaedingmethodsonpopularVQAvisualbenchmarks.
Method LLM Param. Res. VQAT DocVQA SQAI AI2D VQAv2
PrivateModels
GPT-4V - - - 78.0 42.3 82.1 - -
GeminiProVision - - - 74.6 - 81.4 - -
Qwen-VL-Plus - - - 78.9 82.2 73.4 - -
Open-sourceModels
BLIP-2 Vicuna-13B 14.2B 224 42.5 - 61.0 - 41.0
InstructBLIP Vicuna-7B 8.2B 224 50.1 10.9 60.5 40.6 -
Shikra Vicuna-13B 7.3B 224 - - - - -
IDEFICS-9B LLaMA-7B - 224 25.9 - - 42.2 50.9
IDEFICS-80B LLaMA-65B - 224 30.9 - - 54.8 60
Qwen-VL Qwen-7B 9.6B 448 63.8 62.1 67.1 57.7 78.8
Qwen-VL-Chat Qwen-7B 9.6B 448 61.5 57.1 68.2 63 78.2
LLaVA-1.5 Vicuna-7B 7.2B 336 58.2 21.5 66.8 55.5 78.5
LLaVA-1.5 Vicuna-13B 13.4B 336 61.3 24.1 71.6 61.1 80.0
SPHINX Vicuna-7B 10B 224 51.6 - 69.3 - 78.1
SPHINX-1k Vicuna-7B 10B 448 58.8 - 69.1 - 80.2
Mini-Gemini Vicuna-7B 7.4B 336(768) 65.2 - - - -
LLaVA-UHD Vicuna-13B 13.4B 672×1008 67.7 - 72.0 - 81.7
OurModels
MG-LLaVA Phi3-3.8B 4.2B 336(768) 65.3 49.3 73.9 75.1 77.9
MG-LLaVA Vicuna-7B 7.4B 336(768) 67.3 47.9 70.8 69.3 80.2
MG-LLaVA LLaMA3-8B 8.2B 336(768) 68.1 49.0 76.3 75.6 80.7
MG-LLaVA Vicuna-13B 13.6B 336(768) 69.6 52.1 74.7 73.4 81.2
MG-LLaVA Yi1.5-34B 34.4B 336(768) 70.0 56.1 77.0 81.1 82.0
Table3: ComparisonwithothermethodsonVideo-QAbenchmarks.
Method LLM MSVD-QA MSRVTT-QA
FrozenBiLM[76] - 32.2 16.8
VideoChat[32] Vicuna-7B 56.3 45.0
LLaMA-Adapter[77] - 54.9 43.8
Video-LLaMA[78] Vicuna-7B 51.6 29.6
Video-ChatGPT[9] Vicuna-7B 64.9 49.3
Video-LLaVA[10] Vicuna-7B 70.7 59.2
MG-LLaVA Vicuna-7B 71.5 59.8
Table4: AblationresultsonMMBench-DEV[19],SEEDBench[20]andTextVQA[72]. Weexecute
ourexperimentsbasedontheLLaVAmodelwithVicuna-7BandPhi3-3.8B.
Object-level Conv-Gate Vicuna-7B Phi3-3.8B
Features Fusion #TFLOPS Params. MMBD SEED VQAT #TFLOPS Params. MMBD SEED VQAT
× × 5.76 7.2B 68.2 64.6 56.7 3.3 4.0B 68.7 65.3 54.3
✓ × 6.20 7.4B 69.2(+1.0) 65.0(+0.4) 56.5(-0.2) 3.72 4.2B 70.4(+1.7) 66.2(+0.9) 54.5(+0.2)
✓ ✓ 6.21 7.4B 69.8(+1.6) 65.5(+0.9) 59.5(+2.8) 3.73 4.2B 71.0(+2.3) 66.4(+1.1) 56.8(+2.5)
Effect of Each Component. We first conduct ablation studies on object-level features and the
Conv-Gatefusionmoduleacrossmultipledatasets,includingMMBench-DEV[19],SEEDBench[20]
and TextVQA [72]. To validate the effectiveness of our method on different scales of LLM, the
baselineisbuiltonVicuna-7BandPhi3-3.8B.TheresultsareshowninTab.4.
Itisclearthatthemodelachievessignificantgainswiththeintegrationofobject-levelfeaturesand
Conv-GateFusionmodule. Whenaddingobject-levelfeatures,theperformanceofMMBench-Dev,
SEEDBench increases 1.0%, 0.4% separately with Vicuna-7B and 1.7%, 0.9% with Phi3. After
utilizingthefusionnetwork,theperformanceonthesetwobenchmarksfurtherincreasesby1.6%,
0.9%withVicuna-7Band2.3%,1.1%withPhi3. FortheTextVQAbenchmark,theincorporation
ofobject-levelfeaturesdoesnotmarkedlyenhanceperformanceduetothesuboptimaldetectionof
textualcontentwithinimagesbythedetector. Nevertheless,theintegrationofhigh-resolutionfeatures
mitigatesthislimitation,culminatinginanaccuracyincrementof3.0%onVicuna-7Band2.5%on
Phi3-3.8B.Theintegrationofbothmodulesincursamarginalincreaseincomputationalexpenseand
parametercount,yetitenhancestheefficacyofmodelsacrossvariousscales. Wefurtherenumerate
additionalcomparativeoutcomesacrossvarioussubsetsofMMBench-DevandSEEDBench,the
comparativeresultsareshowninAppx.A.
8Table5: Comparisonofdifferentfusionmodules, methodsofmergingobject-levelfeatures, and
taggingmodels.
(a)Fusionmodules. (b)Methodsofmergingobject-levelfeatures. (c)Taggingmodels.
Method MMBD MMStar Method MMBD MMStar Method MMBD MMStar
B wa /s Re eli sn ae mpler 6 59 5. .2 6 3 34 0. .1 5 B wa /s Fe -l ti on -e BCrossAttention 6 68 5. .2 7 3 32 3. .5 3 Baseline 68.2 32.5
w w w/ / /C P Cah ota nc vn h -n GIe nl afo tC eo M Fn i uc n sa i int og n 6 6 68 8 9. . .9 3 8 3 3 32 2 4. . .6 9 5 w w/ /B C- ot no c-F atCrossAttention 6 67 9. .7 8 3 34 4. .4 5 w w/ /C RAO MCO ta8 g0 s 6 68 9. .3 2 3 32 4. .9 5
FusionNetworkDesign. Wealsoexploreadiversedesignoffusionmodulesandperformablation
studiesonvariouscomponents: (1)ChannelConcat. Wesimplyconcatthelowandhigh-resolution
featuresinthechanneldimension. (2)PatchInfoMining. Wereplacethegated-fusionmodelwith
PatchInfoMiningin[8]. (3)Resampler. Wesubstitutethegated-fusionmodelwitharesampler
in[79]. TheresultsareshowninTab.5a. WefindourConv-Gatedfusionmoduleperformsbetter
throughthesemethods,whichconfirmsitsefficiency.
MethodofMergingObject-levelFeatures. Wefurtherexplorevariousmethodsforincorporating
object-levelfeatures:(1)F-to-BCross-Attention. Weaddacross-attentionblocktoenhancethefusion
featuresbyintegratingobject-levelfeaturesafterthefusionmodule,theenhancedfusionfeaturesare
thenfedintoLLM.(2)B-to-FCross-Attention. Followingthefusionmodule,anothercross-attention
blockisemployedtoenhancetheobject-levelfeaturesbyintegratingfusionfeatures. Thefusion
featuresandenhancedobject-levelfeaturesarethenconcatenatedasinputforLLM.Theframeworks
ofbotharedepictedinAppx.CandtheresultsarereportedinTab.5c. Ourobservationsindicatethat
cross-attentiondoesnotenhancetheintegrationofobject-levelfeaturesintovisualrepresentations.
Conversely,concatenatingobject-levelfeatureswithvisualtokensanddeferringthedecision-making
totheLLMyieldsmorefavorableoutcomes.
TaggingModel. Weinvestigatetheimpactofthetaggingmodelwithintheboundingboxgeneration
pipeline. Wecompareourmethodwithassigningfixedtagsbasedonthe80categoriesfromthe
COCO[80]datasettoopen-vocabularydetectorsforproducingboundingboxes. Thecomparative
resultsarepresentedinTab.5b. GiventhattheCOCOdataset’s80categoriesdonotcomprehensively
coverreal-worldobjects,thegeneratedboundingboxesfailtoencompassallobjectswithinanimage.
Thislimitationconsequentlydiminishestheimpactofobject-levelfeatures.
5 Discussions
Conclusions In this work, we propose MG-LLaVA, an expansive multi-modal model adept at
processingvisualinputsofmultiplegranularities,encompassingobject-levelfeatures,originalimages,
andhigh-resolutiondata. Toeffectivelyamalgamatefeaturesofvaryinggranularities,wepropose
theMulti-GranularityVisionFlowmodule,therebyequippingtheLLMwiththeabilitytodiscern
multi-modalinteractionsfromaconsolidatedvisualframework. UtilizingarangeofLLMsextending
from3.8Bto34Bparameters,ourmodelexhibitspronouncedscalabilityandremarkableperformance
invisualunderstanding,outperformingestablishedmodelsandsignificantlyoutperformingGPT-4V
and GeminiPro Vision on benchmarks such as MMBench and SEEDBench. The validity of our
methodologyissubstantiatedthroughrigorousempiricalstudies.
FutureWorks.WhileMG-LLaVAattainsremarkableresultsacrossvariousmulti-modalbenchmarks,
itencounterschallengesineffectivelyharnessingobjectfeaturescorrelatedwithtextualinputs. Our
MG-LLaVAmodelestablishesafoundationalbaselineforfutureexplorationsintomoresophisticated
techniquesofintegratinginputsofmultiplegranularities.
Broader Impacts. As a robust multi-modal language model, MG-LLaVA exhibits considerable
prowess in visual perception and comprehension, offering an innovative methodology to further
refineMLLMs. However, MG-LLaVA’spotentialsocietalimplicationsmeritattention, asitmay
facilitatethecreationofmultimodalapplications,includingthosewithpossibleadverseeffects. There
existsaconcernthatMG-LLaVAcouldbeutilizedinthedevelopmentofdeleteriousmultimodal
AI systems. We encourage users to carefully assess the potential impacts when deploying our
model.
9References
[1] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4:Enhancingvision-
languageunderstandingwithadvancedlargelanguagemodels. arXivpreprintarXiv:2304.10592,2023. 1,
3
[2] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,
Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with
multimodality. arXivpreprintarXiv:2304.14178,2023. 1,3
[3] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,2024.
1,2
[4] HaoZhang,HongyangLi,FengLi,TianheRen,XueyanZou,ShilongLiu,ShijiaHuang,JianfengGao,
LeiZhang,ChunyuanLi,etal. Llava-grounding: Groundedvisualchatwithlargemultimodalmodels.
arXivpreprintarXiv:2312.02949,2023. 1
[5] Fei Wei, Xinyu Zhang, Ailing Zhang, Bo Zhang, and Xiangxiang Chu. Lenna: Language enhanced
reasoningdetectionassistant. arXivpreprintarXiv:2312.02433,2023. 1
[6] JinjinXu,LiwuXu,YuzheYang,XiangLi,YanchunXie,Yi-JieHuang,andYaqianLi. u-llava:Unifying
multi-modaltasksvialargelanguagemodel. arXivpreprintarXiv:2311.05348,2023. 1
[7] RuyiXu,YuanYao,ZonghaoGuo,JunboCui,ZanlinNi,ChunjiangGe,Tat-SengChua,ZhiyuanLiu,
MaosongSun,andGaoHuang. Llava-uhd:anlmmperceivinganyaspectratioandhigh-resolutionimages.
arXivpreprintarXiv:2403.11703,2024. 1,3,7
[8] YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,ShaotengLiu,
andJiayaJia. Mini-gemini:Miningthepotentialofmulti-modalityvisionlanguagemodels. arXivpreprint
arXiv:2403.18814,2024. 1,3,7,9
[9] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:Towards
detailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprintarXiv:2306.05424,
2023. 1,6,8
[10] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunitedvisual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023. 1,3,6,7,8
[11] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-next:
Improvedreasoning,ocr,andworldknowledge,2024. 1,3
[12] ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,HanXiao,HanQiu,ChenLin,WenqiShao,
KeqinChen,etal. Sphinx: Thejointmixingofweights,tasks,andvisualembeddingsformulti-modal
largelanguagemodels. arXivpreprintarXiv:2311.07575,2023. 1,7
[13] ZhuofanZong,BingqiMa,DazhongShen,GuangluSong,HaoShao,DongzhiJiang,HongshengLi,and
YuLiu.Mova:Adaptingmixtureofvisionexpertstomultimodalcontext.arXivpreprintarXiv:2404.13046,
2024. 1,7
[14] GenLuo,YiyiZhou,YuxinZhang,XiawuZheng,XiaoshuaiSun,andRongrongJi. Feastyoureyes:
Mixture-of-resolutionadaptationformultimodallargelanguagemodels. arXivpreprintarXiv:2403.03003,
2024. 1,3,5,7
[15] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang,
HaodongDuan,WenweiZhang,YiningLi,etal. Internlm-xcomposer2-4khd:Apioneeringlargevision-
languagemodelhandlingresolutionsfrom336pixelsto4khd. arXivpreprintarXiv:2404.06512,2024.
1
[16] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b:Anopenlarge-scale
datasetfortrainingnextgenerationimage-textmodels. InNeurIPS,2022. 2,5,6
[17] ROpenAI. Gpt-4technicalreport.arxiv2303.08774. ViewinArticle,2(5),2023. 2,3,7
[18] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023. 2,7
[19] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,Jiaqi
Wang,ConghuiHe,ZiweiLiu,etal. Mmbench:Isyourmulti-modalmodelanall-aroundplayer? arXiv
preprintarXiv:2307.06281,2023. 2,6,8,16
[20] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench:Benchmarking
multimodalllmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023. 2,6,8,16
[21] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama:Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023. 3
10[22] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna:Anopen-sourcechatbotimpressinggpt-4
with90%*chatgptquality. Seehttps://vicuna.lmsys.org(accessed14April2023),2023. 3,6
[23] AiyuanYang,BinXiao,BingningWang,BorongZhang,CeBian,ChaoYin,ChenxuLv,DaPan,Dian
Wang,DongYan,etal. Baichuan2:Openlarge-scalelanguagemodels. arXivpreprintarXiv:2309.10305,
2023. 3
[24] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl: Afrontierlargevision-languagemodelwithversatileabilities. arXivpreprint
arXiv:2308.12966,2023. 3,7
[25] InternLMTeam. Internlm:Amultilinguallanguagemodelwithprogressivelyenhancedcapabilities,2023.
3
[26] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,AhmedAwadallah,HanyAwadalla,
NguyenBach,AmitBahree,ArashBakhtiari,HarkiratBehl,AlonBenhaim,MishaBilenko,JohanBjorck,
SébastienBubeck,MartinCai,CaioCésarTeodoroMendes,WeizhuChen,VishravChaudhary,Parul
Chopra,AllieDelGiorno,GustavodeRosa,MatthewDixon,RonenEldan,DanIter,AmitGarg,Abhishek
Goswami, SuriyaGunasekar, EmmanHaider, JunhengHao, RussellJ.Hewett, JamieHuynh, Mojan
Javaheripi,XinJin,PieroKauffmann,NikosKarampatziakis,DongwooKim,MahoudKhademi,Lev
Kurilenko,JamesR.Lee,YinTatLee,YuanzhiLi,ChenLiang,WeishungLiu,EricLin,ZeqiLin,Piyush
Madan,ArindamMitra,HardikModi,AnhNguyen,BrandonNorick,BarunPatra,DanielPerez-Becker,
ThomasPortet,ReidPryzant,HeyangQin,MarkoRadmilac,CorbyRosset,SambudhaRoy,Olatunji
Ruwase,OlliSaarikivi,AminSaied,AdilSalim,MichaelSantacroce,ShitalShah,NingShang,Hiteshi
Sharma,XiaSong,MasahiroTanaka,XinWang,RachelWard,GuanhuaWang,PhilippWitte,Michael
Wyatt,CanXu,JiahangXu,SonaliYadav,FanYang,ZiyiYang,DonghanYu,ChengruidongZhang,Cyril
Zhang,JianwenZhang,LiLynaZhang,YiZhang,YueZhang,YunanZhang,andXirenZhou. Phi-3
technicalreport:Ahighlycapablelanguagemodellocallyonyourphone,2024. 3,6
[27] AI@Meta. Llama3modelcard. 2024. 3
[28] AlexYoung,BeiChen,ChaoLi,ChengenHuang,GeZhang,GuanweiZhang,HengLi,JiangchengZhu,
JianqunChen,JingChang,etal. Yi:Openfoundationmodelsby01.ai. arXivpreprintarXiv:2403.04652,
2024. 3,6,7
[29] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,QinglongZhang,
XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligningforgeneric
visual-linguistictasks. arXivpreprintarXiv:2312.14238,2023. 3
[30] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleNFung,andStevenHoi. Instructblip:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. InNeurIPS,2024. 3,7
[31] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXivpreprintarXiv:2310.03744,2023. 3,5,6,7
[32] KunChangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,and
YuQiao. Videochat:Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,2023. 3,8
[33] TaoZhang,XiangtaiLi,HaoFei,HaoboYuan,ShengqiongWu,ShunpingJi,ChangeLoyChen,and
ShuichengYan. Omg-llava:Bridgingimage-level,object-level,pixel-levelreasoningandunderstanding.
arXivpreprint,2024. 3
[34] Kuan-ChihHuang,XiangtaiLi,LuQi,ShuichengYan,andMing-HsuanYang. Reason3d:Searchingand
reasoning3dsegmentationvialargelanguagemodel. arXiv,2024. 3
[35] ShengqiongWu,HaoFei,XiangtaiLi,JiayiJi,HanwangZhang,Tat-SengChua,andShuichengYan.
Towardssemanticequivalenceoftokenizationinmultimodalllm. arXivpreprintarXiv:2406.05127,2024.
3
[36] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,YuliangLiu,andXiang
Bai. Monkey:Imageresolutionandtextlabelareimportantthingsforlargemulti-modalmodels. arXiv
preprintarXiv:2311.06607,2023. 3
[37] PanZhang,XiaoyiDongBinWang,YuhangCao,ChaoXu,LinkeOuyang,ZhiyuanZhao,Shuangrui
Ding,SongyangZhang,HaodongDuan,HangYan,etal. Internlm-xcomposer:Avision-languagelarge
modelforadvancedtext-imagecomprehensionandcomposition. arXivpreprintarXiv:2309.15112,2023.
3
[38] QijieZhao,TaoSheng,YongtaoWang,ZhiTang,YingChen,LingCai,andHaibinLing. M2det: A
single-shotobjectdetectorbasedonmulti-levelfeaturepyramidnetwork. InAAAI,2019. 3
[39] RuiQian,YuxiLi,HuabinLiu,JohnSee,ShuangruiDing,XianLiu,DianLi,andWeiyaoLin. Enhancing
self-supervisedvideorepresentationlearningviamulti-levelfeatureoptimization. InICCV,2021. 3
11[40] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,JianweiYang,
HangSu,JunZhu,etal. Groundingdino:Marryingdinowithgroundedpre-trainingforopen-setobject
detection. arXivpreprintarXiv:2303.05499,2023. 3
[41] BoWan,DesenZhou,YongfeiLiu,RongjieLi,andXumingHe. Pose-awaremulti-levelfeaturenetwork
forhumanobjectinteractiondetection. InICCV,2019. 3
[42] XiangtaiLi,HaoboYuan,WeiLi,HenghuiDing,SizeWu,WenweiZhang,YiningLi,KaiChen,and
ChenChangeLoy. Omg-seg:Isonemodelgoodenoughforallsegmentation? InCVPR,2024. 3
[43] HaoboYuan,XiangtaiLi,ChongZhou,YiningLi,KaiChen,andChenChangeLoy. Open-vocabulary
sam:Segmentandrecognizetwenty-thousandclassesinteractively. arXivpreprint,2024. 3
[44] ChongZhou,XiangtaiLi,ChenChangeLoy,andBoDai. Edgesam:Prompt-in-the-loopdistillationfor
on-devicedeploymentofsam. arXivpreprintarXiv:2312.06660,2023. 3
[45] DaandeGeus,PanagiotisMeletis,andGijsDubbelman. Singlenetworkpanopticsegmentationforstreet
sceneunderstanding. InIV,2019. 3
[46] AlexanderKirillov,RossGirshick,KaimingHe,andPiotrDollár. Panopticfeaturepyramidnetworks. In
CVPR,2019. 3
[47] YanweiLi,XinzeChen,ZhengZhu,LingxiXie,GuanHuang,DalongDu,andXingangWang. Attention-
guidedunifiednetworkforpanopticsegmentation. InCVPR,2019. 3
[48] ShilinXu,XiangtaiLi,JingboWang,GuangliangCheng,YunhaiTong,andDachengTao. Fashionformer:
Asimple,effectiveandunifiedbaselineforhumanfashionsegmentationandrecognition. ECCV,2022. 3
[49] VigneshRamanathan,AnmolKalia,VladanPetrovic,YiWen,BaixueZheng,BaishanGuo,RuiWang,
AaronMarquez,RamaKovvuri,AbhishekKadian,etal. Paco:Partsandattributesofcommonobjects. In
CVPR,2023. 3
[50] Lu Qi, Yi-Wen Chen, Lehan Yang, Tiancheng Shen, Xiangtai Li, Weidong Guo, Yu Xu, and Ming-
HsuanYang. Generalizableentitygroundingviaassistanceoflargelanguagemodel. arXivpreprint
arXiv:2402.02555,2024. 3
[51] UmbertoMichieli,EdoardoBorsato,LucaRossi,andPietroZanuttigh. Gmnet:Graphmatchingnetwork
forlargescalepartsemanticsegmentationinthewild. InECCV,2020. 3
[52] YifanZhao,JiaLi,YuZhang,andYonghongTian. Multi-classpartparsingwithjointboundary-semantic
awareness. InICCV,2019. 3
[53] DaandeGeus,PanagiotisMeletis,ChenyangLu,XiaoxiaoWen,andGijsDubbelman.Part-awarepanoptic
segmentation. InCVPR,2021. 3
[54] Xiangtai Li, Shilin Xu, Yibo Yang, Guangliang Cheng, Yunhai Tong, and Dacheng Tao. Panoptic-
partformer:Learningaunifiedmodelforpanopticpartsegmentation. InECCV,2022. 3
[55] XiangtaiLi,ShilinXu,YiboYang,HaoboYuan,GuangliangCheng,YunhaiTong,ZhouchenLin,Ming-
HsuanYang,andDachengTao. Panopticpartformer++:Aunifiedanddecoupledviewforpanopticpart
segmentation. arXivpreprintarXiv:2301.00954,2023. 3
[56] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,
SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. InICCV,2023. 3
[57] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,2021. 5,6
[58] YoucaiZhang,XinyuHuang,JinyuMa,ZhaoyangLi,ZhaochuanLuo,YanchunXie,YuzhuoQin,Tong
Luo,YaqianLi,ShilongLiu,etal. Recognizeanything:Astrongimagetaggingmodel. arXivpreprint
arXiv:2306.03514,2023. 5,6
[59] MatthiasMinderer,AlexeyGritsenko,andNeilHoulsby. Scalingopen-vocabularyobjectdetection. In
NeurIPS,2024. 5,6
[60] XTunerContributors.Xtuner:Atoolkitforefficientlyfine-tuningllm.https://github.com/InternLM/
xtuner,2023. 6
[61] PiyushSharma, NanDing, SebastianGoodman, andRaduSoricut. Conceptualcaptions: Acleaned,
hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InProceedingsofthe56thAnnual
MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),2018. 6
[62] GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,Zhihong
Chen,JianquanLi,XiangWan,andBenyouWang. Allava:Harnessinggpt4v-synthesizeddataforalite
vision-languagemodel. arXivpreprintarXiv:2402.11684,2024. 6
[63] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
Sharegpt4v:Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprintarXiv:2311.12793,
2023. 6
12[64] RubènTito,DimosthenisKaratzas,andErnestValveny. Documentcollectionvisualquestionanswering.
InICDAR,2021. 6
[65] KushalKafle,BrianPrice,ScottCohen,andChristopherKanan. Dvqa:Understandingdatavisualizations
viaquestionanswering. InCVPR,2018. 6
[66] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAliFarhadi. A
diagramisworthadozenimages. InECCV.Springer,2016. 6,7
[67] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. InICML.PMLR,2023. 7
[68] KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,FengZhu,andRuiZhao. Shikra: Unleashing
multimodalllm’sreferentialdialoguemagic. arXivpreprintarXiv:2306.15195,2023. 7
[69] HugoLaurençon,LucileSaulnier,LéoTronchon,StasBekman,AmanpreetSingh,AntonLozhkov,Thomas
Wang,SiddharthKaramcheti,AlexanderRush,DouweKiela,etal. Obelics:Anopenweb-scalefiltered
datasetofinterleavedimage-textdocuments. InNeurIPS,2024. 7
[70] ShengdingHu,YugeTu,XuHan,ChaoqunHe,GanquCui,XiangLong,ZhiZheng,YeweiFang,Yuxiang
Huang,WeilinZhao,etal. Minicpm: Unveilingthepotentialofsmalllanguagemodelswithscalable
trainingstrategies. arXivpreprintarXiv:2404.06395,2024. 6,7
[71] LinChen,JinsongLi,XiaoyiDong,PanZhang,YuhangZang,ZehuiChen,HaodongDuan,JiaqiWang,
YuQiao,DahuaLin,etal. Areweontherightwayforevaluatinglargevision-languagemodels? arXiv
preprintarXiv:2403.20330,2024. 6
[72] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,and
MarcusRohrbach. Towardsvqamodelsthatcanread. InCVPR,2019. 7,8,16
[73] MineshMathew,DimosthenisKaratzas,andCVJawahar. Docvqa:Adatasetforvqaondocumentimages.
InWACV,2021. 7
[74] StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrenceZitnick,
andDeviParikh. Vqa:Visualquestionanswering. InICCV,2015. 7
[75] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,Peter
Clark,andAshwinKalyan.Learntoexplain:Multimodalreasoningviathoughtchainsforsciencequestion
answering. InNeurIPS,2022. 7
[76] AntoineYang,AntoineMiech,JosefSivic,IvanLaptev,andCordeliaSchmid. Zero-shotvideoquestion
answeringviafrozenbidirectionallanguagemodels. InNeurIPS,2022. 8
[77] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
HongshengLi, andYuQiao. Llama-adapter: Efficientfine-tuningoflanguagemodelswithzero-init
attention. arXivpreprintarXiv:2303.16199,2023. 8
[78] HangZhang,XinLi,andLidongBing. Video-llama:Aninstruction-tunedaudio-visuallanguagemodel
forvideounderstanding. arXivpreprintarXiv:2306.02858,2023. 8
[79] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelfor
few-shotlearning. InNeurIPS,2022. 9
[80] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,
andCLawrenceZitnick. Microsoftcoco:Commonobjectsincontext. InECCV,2014. 9
13A Appendix/Detailedresultsonsubsets
Inthissection,wecomparetheinfluenceofobject-levelfeaturesonseveralsubsetsofMMBench-Dev
andSeed-bench,asshowninFig.4. Itcanbeobservedthattheintegrationofobject-levelfeatures
significantlyenhancesthemodel’scapabilityinmultipleaspectsofperceptionincludingAttribute
Reasoning,Fine-grainedPerception,PhysicalRelationPerception,VisualReasoning,etc.
Figure4: AblationstudyonseveralsubsetsofMMBench-DEV-ENandSeed-bench. Fine-grained
Perception(I)denotesFine-grainedPerception(instance-level),PropertyReasoning(P)meansProp-
ertyReasoningPerceptionandSITUnderstandingdenotesStructuralizedImage-TextUnderstanding.
B Appendix/AdditionalShowcases
In this section, we present additional instances to substantiate the capability of MG-LLaVA. As
presented in Fig. 5 and Fig. 6, MG-LLaVA is proficient in addressing queries that necessitate
meticulousattentiontospecificsandincapturingfine-graineddetailswithinimageorvideo. These
furtherinstancesreinforcethesuperiorperformanceofourMG-LLaVAinvisualcomprehension.
Figure5: Morecasesofvideounderstanding.
14Figure6: Morecasesofimageunderstanding.
C Appendix/MethodofMergingObject-levelFeatures
TheillustrationofF-to-BCross-AttentionandB-to-FCross-AttentionisdepictedinFig.7.
Figure7: IllustrationofF-to-BCross-AttentionandB-to-FCross-Attention.
15D Appendix/InferencePipeline
TheinferencepipelineofMG-LLaVAisdisplayedinFig.8. Thetaggingmodelfirstprocessesthe
inputimagetoprovidetagswithintheimage,whicharesubsequentlyutilizedasthetextinputofthe
detectortoderiveboundingboxescorrespondingtothetaggedobejctswithintheimage.
Figure8: InferencepipelineofMG-LLaVA.
E Appendix/ComparisonofTaggingModels
Table6: AblationresultsonMMBench-DEV[19],SEEDBench[20]andTextVQA[72]. Weexecute
ourexperimentsbasedontheLLaVAmodelwithVicuna-7BandPhi3-3.8B.
NumberofBoxes
Method Images
0 1-10 12-20 21-30 30-50 >50
COCO80+OWL-ViTv2 389722 71118 245952 44059 28593 0 0
RAM+OWL-ViTv2 389722 43654 184706 91245 34648 22827 12645
16