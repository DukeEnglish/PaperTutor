FacePsy: An Open-Source Affective Mobile Sensing System –
Analyzing Facial Behavior and Head Gesture for Depression
Detection in Naturalistic Settings
RAHULISLAMandSANGWONBAE∗,CharlesV.Schaefer,Jr.SchoolofEngineeringandScience,
StevensInstituteofTechnology,USA
Depression,aprevalentandcomplexmentalhealthissueaffectingmillionsworldwide,presentssignificant
challengesfordetectionandmonitoring.Whilefacialexpressionshaveshownpromiseinlaboratorysettings
foridentifyingdepression,theirpotentialinreal-worldapplicationsremainslargelyunexploredduetothe
difficultiesindevelopingefficientmobilesystems.Inthisstudy,weaimtointroduceFacePsy,anopen-source
mobile sensing system designed to capture affective inferences by analyzing sophisticated features and
generatingreal-timedataonfacialbehaviorlandmarks,eyemovements,andheadgestures–allwithinthe
naturalisticcontextofsmartphoneusagewith25participants.Throughrigorousdevelopment,testing,and
optimization,weidentifiedeye-openstates,headgestures,smileexpressions,andspecificActionUnits(2,6,7,
12,15,and17)assignificantindicatorsofdepressiveepisodes(AUROC=81%).Ourregressionmodelpredicting
PHQ-9scoresachievedmoderateaccuracy,withaMeanAbsoluteErrorof3.08.Ourfindingsoffervaluable
insightsandimplicationsforenhancingdeployableandusablemobileaffectivesensingsystems,ultimately
improvingmentalhealthmonitoring,prediction,andjust-in-timeadaptiveinterventionsforresearchersand
developersinhealthcare.
CCS Concepts: • Human-centered computing → Ubiquitous and mobile computing design and
evaluationmethods.
AdditionalKeyWordsandPhrases:Affectivecomputing,Depression,MachineLearning,Mobilecomputing,
System,Empiricalstudythattellsusaboutpeople,ApplicationInstrumentation,FieldStudy
ACMReferenceFormat:
RahulIslamandSangWonBae.2024.FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–Analyzing
FacialBehaviorandHeadGestureforDepressionDetectioninNaturalisticSettings.InProceedingsofthe
ACMonHuman-ComputerInteraction:ACMMobileHCI,Sept30–Oct03,2024,Melbourne,Australia.ACM,New
York,NY,USA,32pages.
1 INTRODUCTION
Mentalhealthpertainstoemotional,psychological,andsocialwell-being,influencingdailythoughts,
feelings,andactions.Mentalillnessisaleadingcauseofdisability,withanestimated450million
peopleaffectedworldwide[71].Itisalsoasignificantpredictorofsuicide[67].Mentaldisorders
usually emerge in an individual’s early 20s [53], and their untreated presence can negatively
impactacademicsuccess,productivity,andsocialrelationships[54,97].InthecontextofCOVID-19,
theneedforsocialdistancinghasledtothewidespreadadoptionoftelehealthservicessuchas
telepsychiatry[22,72].Unfortunately,manyindividualsareexperiencingmentalhealthissues
duringthepandemic,withthehighestlevelsofpandemic-eraanxietyanddepressionobserved
∗Correspondingauthor.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeand
thefullcitationonthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACMmustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
©2024AssociationforComputingMachinery.
1
4202
nuJ
42
]CH.sc[
1v18171.6042:viXraMobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
in2020acrossallagegroups,whichbegantodeclineinearly2021[96].WhilemanyCOVID-19
restrictionshavebeenlifted,approximately5%oftheU.S.adultpopulation,orabout12million
Americans,arelivingwithco-occurringchronicpainandclinicallysignificantsymptomsofanxiety
and depression [50]. This ongoing situation underscores the importance of reconsidering how
todelivermentalhealthcareeffectivelyattherighttime.Personalizedpsychiatriccare,which
promotes preventive measures and offers tailored interventions, could help meet these needs,
thoughitsavailabilityremainslimited[1].
A growing body of psychological studies [27, 39, 40, 90] have suggested that depression is
characterizedbynonverbalsignalssuchasfacialmusclemovement,andheadgesture,whichcan
bedetectedautomaticallywithouttheneedforclinicalintervention.Werefertotheseas"facial
behaviorprimitives".Researchhasshownthatmentalillness,suchasdepression,leavesrecognizable
markersinthefacialpatternsofanindividual[81].Often,thesechangesmanifestinaperson’s
faceinvoluntarily.Creatinganautomaticsystem[17,86,93]basedonthesecuescanprovidean
objectiveandrepeatableevaluationandaddressproblemsrelatedtocostandtimerequirements.
Despitethevaluableinsightsgainedfromthesestudies,itshouldbenotedthattheywereconducted
incontrolledlabenvironmentsandrecordedvideosofindividuals’faces.Currently,thereal-life
implementationofsuchsystemsislimitedduetoprivacyconcerns[10,17,56,87,94],unrealistic
costs[74],andrequiredcomputationalpower[15,32,69].
Whilefacialactionshaveshownpromiseinlabsettingsforunderstandingdepression,their
application in real-world scenarios remains largely unexplored due to challenges in designing
efficient,deployablemobilesystems.Tobridgethisgap,weintroduceFacePsy,anopen-source
mobile sensing system capturing facial features, generating real-time data on facial behavior
landmarks,eyeopen,smile,andheadgestures,allthroughsmartphonesinnaturalsettings,while
preservinguserprivacy.Wehypothesizethatdigitalbiomarkersextractedfromfacialcuesoffer
valuable insights into an individual’s internal emotional and affective state, thereby enabling
algorithmstoinferdepression.Inthisfieldstudy,wegathereddatainreal-worldcontextstoassess
howandwhetherthedatacollectedthroughourframeworkcoulddemonstratethepotentialfor
predictingdepressiveepisodesinnaturalisticenvironments.
Whiletherearestudiesthatusemobilesensingtotrackpatternsinsocialandbehavioraldata
bytrackingcommunications,appusage,andGPSdata[15,69,99],thesemobilesensing-based
solutionsprimarilyfocusoncapturingsocialandbehavioraldatabutdisregardingaffectivesignals,
whichhavebeenshowntobeimportantindicatorsofdepression.Thesestudieshaveparticular
challengesandbarriers:(1)Mobilesensinglimitsmodelingusagebecauseitrequiresextensive
computationandpost-processing.Theburdenofcollectingsensors24/7andbatteryconsumption
mayleadtolowcompliance.Itmightnotbeusableforstakeholders.(2)Wangetal.[98]triedto
captureentirefaceimagesinthereal-worldsettingstounderstanddepressionbutreportedthat
theyfailedtovalidatetheeffectivenessofdataduetoinsufficientframestobuildamodel(one
framewhenunlockingthesmartphone);(3)Tsengetal.collectedandanalyzedeyepatchesin
detectingalertness[91]notdepression,butpartiallycapturedapartofthefaceonly(eye).Most
recently,MoodCapture[66]wasintroducedthatcapturesfacialimagesinnaturalenvironments
fordepressiondetection.Thisinvolvesanalyzingimageattributessuchasangle,dominantcolors,
location,objects,andlighting.TheutilityofMoodCapturefordevelopersseekingtoimplement
similarstudiesindifferentsettingsmaybesomewhatlimited,astheauthorshavenotmadetheir
mobilesystem,dataset,ormachinelearningpipelinepubliclyavailable.Ourstudycomplements
MoodCapture’sworkbyexploringtheincrementalutilityofmobilesensingfordepressiondetection
andadvocatingfornewavenuestodevelopmentalhealthassessmenttoolsbasedonin-the-wild
images.OurresearchadvancesfromMoodCaptureintermsofdatacollectionmechanisms,on-
device processing, privacy awareness, and the facial attributes collected. While MoodCapture
2FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
collectsfacialdatawhenauserrespondstosurveyquestions,ourstudyimplementsatrigger-based
datacollectionmotivatedbypriorliterature[92].Thismechanismactivatesbasedonuseractions,
suchasturningthescreenon/off,opening/closingapps,etc.,tostartorstopdatacollection.For
on-deviceprocessing,weonlysendthefinaldetectedfacialbehaviorprimitives(ActionUnits(AU),
smile,eyeopenstate,headEulerangles,andlandmarks)totheresearchserverforfurtheranalysis.
Thishelpsusensureuserprivacyandpreventtheleakageoffacialimagesbydiscardingimages
from user device after processing. Our work also advocates for privacy-aware data collection.
Thisapproachisinformedbypriorliteratureonnudging[5],informingusersaboutactivedata
collection[20,30],andnotifyinguserswhentheapprestartsitselfafterarebootorcrash[20].
WhileMoodCapturehasfirstintroducedtheuseoffacialimagesfordepressiondetectioninnatural
environments,ourapproachproposesanovelfacettothisfieldinseveralkeyaspects.Weare
amongthefirsttodevelopanopen-sourced,privacy-aware,trigger-basedaffectivemobilesystem
thatcapturesfacialdatafromusers’smartphonesandimmediatelydiscardstherawimagesafter
extractingessentialfeaturesinnearreal-time(within10seconds).Thismethodnotonlybuildsa
predictivemodelofdepressiveepisodesbutalsoensuresthatsensitivefacialdataisnotpermanently
storedondevices,addressingsignificantprivacyconcernsandawareness.
Toadvanceaffectivecomputingsystems,makingitapplicableinreal-worldsettings,wesyn-
thesizeasetofaffectivesignalsfromfacewhichhavebeenwell-validatedsuchasfacialmuscular
activities(AUs)aswellasproposednewfeaturesbeyondsimplefacialexpressionalgorithmsthat
havebeenunexploredandinvalidatedinauser’severydaysettings.Researchquestionsasfollows:
(RQ1)Whataretheimportantsignalsofaffectivebiomarkersondepressiveepisodedetectionbydif-
ferentiatingdepressiveandnon-depressiveepisodes,andhowcanthosekeyfeaturescontributeto
themodel’sperformance?and(RQ2)Whetherandhowcananaffectivemobilesystembeefficiently
designed,tested,anddevelopedtounderstandauser’smentalhealth,specificallyinpredicting
depressiveepisodes,inreal-worldsettings?Asaresult,weidentifiedspecificaffectiveindicatorsas
crucialfactorsfordistinguishingbetweenindividualsexperiencingdepressiveandnon-depressive
episodes.Theseindicatorsencompasstheeye-openstate,headpose,smileexpression,andspecific
ActionUnits(2,6,7,12,15,and17).Whenthesefeaturesarecombined,theyexhibitpredictive
potentialfordetectingdepressionepisodes,achievinganAUROCof67%foruniversalmodel,while
thehybridmodelhasanAUROCof81%.Itisworthnotingthatfurtherenhancementsinpredictive
accuracycanbeattainedthroughtheaccumulationofadditionaldataspanningsubsequentweeks.
Thesefindingsrepresentasignificantstrideinbridgingtheexistingdisparitybetweencontrolled
laboratory studies and the practical implementation of depression detection through affective
mobilesensingsystemsinreal-worldscenarios.
Assuch,wehavedevelopedadeployableandusableopen-sourced,lightweight,affectivemobile
systemfortheHCIcommunity1.Oursystemintegratesstate-of-the-artfacialbiomarkers,which
havebeenvalidatedtounderstandcomplexmentalstatesandworkloadsincontrolledlabsettings.
We have further expanded these features for the context of depression detection. The system
automatically extracts these features from a user’s smartphone in natural environments. This
systemhasthepotentialtocreatenewavenuesfordevelopingmentalhealthassessmenttoolsand
behaviormodelingbasedonin-the-wildimages.Moreover,ourFacePsysystemcanbedeployed
ineverydaysettings,anditisoptimizedwithasamplingrateof2.5FPSwithoutanydelayson
theuser’sownphones.Further,weprovideinsightswiththeexperimentswithdifferentsubset
offacialbehaviorprimitivesfeaturesindetectingdepressioninnaturalisticenvironments.Aswe
highlighttheimpactofeachsubsetoffeaturesvalidatedinnaturalisticenvironments,researchers
1Oursystemsourcecodeisavailableat:https://github.com/stevenshci/FacePsy
3MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
anddeveloperscanutilizeourmobilesystemtoconducttheirstudies,configureappsfortriggering
timeandfrequency,andbuildtheirowncomputationalmodels.
2 BACKGROUND
Inthissection,weintroducetheliteratureonfacialbehaviorprimitivesindevelopingdepression
inferencesandmachinelearningmodels(Section2.1)andprovidepriorworkondepressiondetec-
tionusingmobilesensingtechnologiesinthefieldsofmobileandaffectivecomputingcommunities
(Section2.2).
2.1 FacialBehaviorPrimitivesinDepression
Researchonnonverbalfacialbehavioroftenshowsthatindividualswithdepressionusuallyexhibit
fewerhappyfacialexpressions,reducedexpressiveness,andlessheadmovement.Thelessfrequent
displayofhappyfacialexpressionsbydepressedpatientsisacommonlyobservedfinding[14,37,
79,85].Variousstudiesalsolinkdepressionwithdecreasedgeneralfacialexpression[35,79]and
headmovement[3,33,51].Onestudyfoundthatparticipantswithmajordepressivedisorder(MDD)
hadasignificantlyreducedtransientpupillaryresponse[61],lendingsupporttothepotentialof
detectingdepressionthroughthesemeans.Typicalsymptomsofdepression,suchassorrowful
expressionsandalackofaffectiveexperience,havebeencharacterizedbyresearchersusingfacial
expressions[27].However,theprevalenceofnegativefacialexpressionsindepressedindividuals
isdisputed,withcontradictoryfindingspresentedinvariousstudies.Somearguethatdepression
ischaracterizedbyanincreaseinnegativefacialexpressions[78,84],whileotherssuggestthat
depressedpeoplemayactuallyexhibitmorepositivefacialexpressions[35,79].
To diagnose depression, nonverbal signals have been introduced by researchers in affective
computing.Forinstance,Cohnetal.[17]proposedvisualsignalsasnon-verbalbehavioralfeatures
– manually annotated facial action units (AUs) and active appearance model (AAM) features,
whicharemathematicallyderivedrepresentationsoffacialimagesthatcaptureshapeandtexture
variations.Theyfoundthatparticipantswithhighdepressionseveritydisplayedfewerassociative
facial expressions (AU12 – lip corner puller, and AU15 – lip corner depressor) and more non-
associativefacialexpressions(AU14–dimpler),indicatingthatthesetraitshelpedspotdepression.
Mostrecently,Valstaretal.[87,93]haveusedfacialandauditoryfeaturestodetectdepressioninpre-
recordedvideos.TheirfindingsuggeststhatAU4(browlowerer),AU12(lipcornerpuller),AU15(lip
cornerdepressor),andAU17(chinraiser)areusefulforestimatingdepressionseverity,supporting
existingevidence.Thesefindingshighlightthepotentialutilityofautomatedfacialbehavioranalysis
towardspredictingofdepression.Theseinitialstudieswerebasedonrecordedvideodatafrom
consentingparticipantsincontrolledenvironments.Incontrast,deployingsuchtechnologiesin
uncontrolled,everydaysettingsraisesvalidprivacyconcerns.However,ourapproachmitigates
theseconcernsbyprocessingdatadirectlyonthedevice.Byleveragingon-devicecomputationfor
featureextraction,wesignificantlyreducetheprivacyrisksassociatedwithtransmittingsensitive
facialdata.Thismethodensuresthatpersonaldatadoesnotleavetheuser’sdevice,aligningwith
privacy-preservingstrategiesessentialforreal-worldapplications.
ResearchersinHCIhaveexploredusingmobilecamerasensorstocaptureandanalyzeuser
dataformentalhealthassessment.Forinstance,Ruietal.[98]developedasmartphoneappthat
takes photos of users’ faces throughout the day, extracting facial expressions and landmarks.
However,thisapproachhadlimitedsuccess.Thecorrelationbetweenfacialexpressions,landmarks,
and mental health was difficult to establish due to the poor performance of facial expression
algorithms,landmarkdetectors,andimagequality.InaseparateHCIstudy,Vincentetal.[92]
usedpupilinformationtogaugeuseralertnessasanindicatorofmentalstates.Arecentstudy,
4FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
Table1. StudiesonPredictingDepressionUsingFacialBehaviorPrimitives
Study Part. StudyLength Data&FeatureTypes Validation PerformanceMetrics Research En-
Method vironment
Cohnetal.[17] 57 7-weekintervals Audio/video,17AUs,AAM Leave-one-out Accuracy:79% Lab
Valstaretal.[94] 292 Onevideoeach AVEC13,LPQ 5-fold MAE:10.88, Lab
RMSE:13.61
Songetal.[87] 84 Onevideoeach AVEC14,17AUs,Pose,Gaze 50/50split MAE:5.95, Lab
RMSE:7.15
Kongetal.[56] 102 N/A 10photos,Deeplearned 7:2:1split Accuracy:98.23% Lab
Casadoetal.[10] 376 Onevideoeach AVEC13/14,rPPG N/A AVEC13:MAE:6.43, Lab
AVEC14:MAE:6.57
Wangetal.[98] 37 10weeks Photos,Eigenfaces,landmarks N/A N/A Inthewild
Nepaletal.[66] 177 90days AU,Gaze,HeadPose,RigidityPa- 5-fold leave- BalancedAcc:61% Inthewild
rameters,andEye,2D&3DLand- subject-out
marks
Ourapproach 25 4weeks 12AUs,Smile,Eyeopen,HeadPose, Leave-One- Accuracy:51%, Inthewild
EAR,IVA,133landmarks Person-out AUC:67%
MAE:3.26
Leave-One-Day- Accuracy:69%,
out AUC:81%
MAE:3.08
MoodCapture[66],evaluateddepressionusingimagestakenautomaticallybysmartphonefront-
facingcamerasduringeverydayactivities.Thistoolanalyzesfeaturesintheimagessuchasangles,
dominant colors, locations, objects present, and lighting conditions. The study showed that a
randomforestalgorithmtrainedonfaciallandmarkscaneffectivelydistinguishbetweendepressed
andnon-depressedindividuals,andpredictrawPHQ-8scores.TheeffectivenessofMoodCapture
fordeveloperslookingtoconductsimilarstudiesinvariousenvironmentsmightbeconstrained
becausetheauthorshaven’treleasedtheirmobilesystem,dataset,ormachinelearningpipeline.Our
studyadvancesfromMoodCaptureintermsofdatacollectionmechanisms,on-deviceprocessing,
privacyawareness,andthefacialattributescollected.WhileMoodCapturecapturesimageswhen
participantsrespondtoEMAquestions,ourprotocolreliesonopportunisticdatacollection.We
gatherdatawhenparticipantsinteractwiththeirsmartphonesatspecifictriggers(SeeSection3.2).
Thisallowsustocollectinformationthatmorecomprehensivelyrepresentsparticipants’daily
livesandemotionalstates.Byusingabroaderdatacollectionframework,wecanconductaricher
analysisofbehavioralpatternsandemotionalnuancesthatoccurduringregularphoneusage,not
justduringspecificsurveyresponses.BothMoodCaptureandourstudyareprimarilydesignedfor
depressiondetection.However,usingsmartphonecamerastocaptureimagescouldalsoextend
toassessingothercognitivestates,suchasalertness,throughpupilimaging.Theseapproaches
[66, 92, 98], however, raises privacy concerns due to the transmission and processing of facial
imagesonexternalservers.Ourstudycomplementsthefindingsofthesestudiesbyimplementing
alldataprocessinglocallyontheuser’sdevice;weplantoopensourceoursystemtotheresearch
community,whichcanbeusedforfurtherstudiesinbehaviormodelingthroughaffectivesignals.
2.2 DetectingDepressionUsingMobileSensing
Significant progress has been made in detecting depression with mobile sensing. For instance,
Chikersaletal.[15]utilizedtheAWARE[32],anopen-sourcecontextinstrumentationframework.
It tracked behavioral data such as Bluetooth, calls, GPS, microphone, and screen status from
smartphonesandwearablefitnessdevicestodetectdepressionincollegestudents.Theirmethod
achieved85.4%accuracyinidentifyingchangesinindividuals’depressionand85.7%accuracyin
detectingpost-semesterdepressionoverasemester-long(16weeks)study.Inadifferentstudy,
Asareetal.[69]alsousedtheAWAREframeworktomonitorbehavioraldata.Theyfocusedonsleep,
physicalactivity,phoneusage,GPSlocation,anddailymoodratingsusingthecircumplexmodel
ofaffect(CMA)todetectdepression.Thisapproachresultedin81.43%accuracy.Thoughsensing
5MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
systemshaveproveneffectiveinvariousapplications,theirabilitytoprovidereal-timeinsightsand
interventionsisrelativelyunexplored.Thisismainlyduetotheresource-intensiverequirementsof
pre-processing,featureengineering,andmodeldevelopment,whichdemandfurtherscrutinyto
fullyutilizetheircapabilities.Itisworthnotingthatthesystemsexaminedinpreviousresearchdid
notsupportnear-real-timefacialfeatureextraction.Thisdistinctivefeatureseparatesouraffective
mobilesystemfromothers.WesummarizethefindingsofthesestudiesinTable2.
Table2. StudiesonPredictingDepressionUsingMobileSensing
Study SensorsUsed Results Findings
Chikersaletal.[15] Bluetooth,GPS,Screen,Calls,Sleep Accuracy:82.3% Identifiesdepressivesymptomsusingdatafromsmartphonesand
F1:78% fitnesstrackersofcollegestudents.Thestudyintroduceadvanced
featureextractiontechnique.
Opokuetal.[69] Sleep,Activity,GPS,PhoneUsage Accuracy:81.43% Classifiesindividualsasdepressed/non-depressedusingmood
AUC:82.31% scoresandsensordata.Significantdifferencesfoundinmood,
sleep,activity,phoneusage,GPSmobility.
Pedrellietal.[74] EDA,HeartRate,Accelerometer, MAE:3.88-4.74 Feasibilityofmonitoringdepressionseveritywithsmartphones
Sleep,Movements,Temperature andwearables,showingmoderatetohighcorrelationswith
clinician-assessedscores.
Farhanetal.[29] GPS,PhysicalActivity F1:55% Behavioraldatafromsmartphonespredictclinicaldepression.
CombiningwithPHQ-9scoresenhancesaccuracy.
Nepaletal.[66] AU,Gaze,HeadPose,RigidityPa- BalancedAcc:61% Usessmartphoneimagestodetectdepressionbyanalyzingfacial
rameters,Eye,2D&3DLandmarks expressionsandfeatures,demonstratingmachinelearningpoten-
tialinmentalhealthassessment.
IslamandBae[48] Pupil-IrisRatio Accuracy:76% Pupillaryresponseinnaturalsettingsvariesbetweenmorning
F1:64% andeveningandcandifferentiatebetweendepressiveandnon-
AUC:71% depressivestates.
Ourapproach 12AUs,Smile,Eyeopen,HeadPose, Accuracy:69% FacePsydetectsdepressiveepisodesbycollectingfacialbehaviors
EAR,IVA,133landmarks F1:67% andheadgesturesinreal-worldsettings,achievinghighpredictive
AUC:81% accuracywithkeyfeatureslikeeye-openstates,smileexpressions,
MAE:3.08 andspecificActionUnits.
Pedrellietal.[74]havecollecteddatastreamsfromawearabletracker,Empatica,andsmart-
phonetodetectchangesindepressionseverity.Theauthorsleveragedphysiologicaldatasuch
aselectrodermalactivity(EDA),peripheralskintemperature,heartrate,motionfromthe3-axis
accelerometer,sleepcharacteristics,socialinteractions,activitypatterns,andthenumberofapps
used,etc.Theyevaluatedtheirpredictivemodelsusingtwoevaluationmethods:user-splitand
time-split.Theyachievedanmeanabsoluteerror(MAE)rangingbetween3.88and4.74.However,
theirwork’slimitationisthatparticipantsmustweartwoE4Empatica,oneineachhand.Such
obtrusiveapproachesleadtodecreasedcomplianceofparticipantsandextracost($1,690perdevice)
forresearchersandusers.Inanotherstudy[29],theyhaveusedGPSandphysicalactivityassensor
featuresfordepression.Inanotherstudy[48],researchersusedpupillometrydataasaproxyfor
psychologicalstatetodetectdepression.Theyfoundpupillaryresponseinnaturalsettingsvaries
betweenmorningandeveningandcandifferentiatebetweendepressiveandnon-depressivestates.
Alloftheearlierstudieshavemostlyfocusedonbehavioralandsocialchangesinapersonduring
thedepressionbecauseopen-sourcedanddeployablemobileframeworksarelimitedintheHCIcom-
munity.Asweknow,depressionisamultifaceteddisorderthataffectsthebehavioral,physiological,
andsocialaspectsofpeople’slives.Thecurrentmobilesensing-basedapproaches[15,29,69]may
notbeabletocapturethephysiologicalsignalswithrichemotionalsignals,whichhavebeenshown
tobeimportantindicatorsofdepression.Whereaswearablesensing-basedphysiologicalsensing
solutionsareproposed,thecostisveryhighforsuchdeployment.Asmotivatedbytheworks
[10,17,56,87,94]inaffectivecomputinghaveshowngreatpotentialincapturingphysiological
signalswithrichemotionaldatainpersonswithdepressioninlabsettings.However,thesestudies
havebeenunexploredinreal-worlddeploymentwherecontinuoussymptommonitoringisessential
partofdeliveringanappropriatereal-timeintervention.Inthispaper,wewouldaddressthisspecific
issuebydevelopinganaffectivemobilesystemthatcanunobtrusivelyandopportunisticallytrack
6FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
individualfacialbehaviorprimitivestogiveinsightsintotheircomplexmentalstateinnear-real
timebyextractingvariousemotionaldatamotivatedbytheoriesinaffectivecomputing.Ashu-
manfaceservesasacrucialandnaturalmediumforconveyingemotionalandmentalstates[26].
Whilesomestudiesusefacialbehaviorprimitivestodetectdepression[87,93],attemptstodetect
depressionusingpassivelysensedfacialbehaviorprimitivesinanaturalisticenvironmenthave
beenunsuccessful[98].Theeffectivenessoftheexistingdepressiondetectionmodelsbasedon
facialbehaviorprimitivesinnaturalenvironmentsisalsounexplored.
Therefore, we aim to design, refine, and develop configurable triggering for data collection,
including when unlocking phones and app use, to capture users’ facial behavior data in their
everydaysettings.Ourmethodopportunisticallycapturesusers’facialbehaviorprimitivesunob-
trusivelybytriggeringdatacollectionwhenusersinteractwiththeirownsmartphones.Inaddition
todetectingdepression,weexploretheminimumnumberofdaysuserdatarequiredtoproduce
reliableperformance.Thefollowingsectionsdescribeourapproachindetail,startingwiththe
designofourpassivelyrunningmobileaffectivesystem.
3 DESIGNOPEN-SOURCEDAFFECTIVEMOBILESYSTEMFORRESEARCHERSINHCI
COMMUNITY
OurframeworkdesignisbuiltupontheHCItheoryandaffectivecomputingresearch.Thissection
introducesanoverviewofdesigningouraffectivemobilesystem,FacePsy(Section3.1,3.2,3.3,
3.4).,andevaluatesthefeasibilityofthesysteminapilotstudytorefinetheFacePsy(Section3.5).
FacePsyisopen-sourcedwithseveralkeyobjectives.Ourprimarygoalistoencouragewidespread
adoptionofthesystem,enablinguserstoderivevaluefromthegenerateddataandfacilitating
engagementwithtopicsrelatedtomentalhealthsensing,particularlyinthecontextofdepression.
Additionally,weaimtocultivateacommunityofcontributorswhocanenhancethesystemby
designingFacePsywithmodularityasacentralprinciple.ThecompletesourcecodeforFacePsy
canbeaccessedonGitHub:https://github.com/stevenshci/FacePsy.
3.1 TechnicalAspectsofFacePsy
FacePsy is designed to capture real-time facial behavior primitives as users interact with their
mobiledevices.Operatingwitharesponsetimeof2.5Hz,whichwasrobustlytestedacrosstwo
differentdevices,theappleveragesthefrontcameratogatherfacialdataduringspecifiedtriggers
opportunistically.Thisapproachenhancesdatarelevanceandoptimizesenergyconsumptionand
privacy.FacePsyintegratesadvancedmodulessuchasfaciallandmarkdetection[41],headpose
estimation [41], and facial action unit recognition [28], running these sophisticated processes
directlyonthedevice.Thison-deviceprocessingensuresprivacyandincreasesenergyefficiency
byeliminatingtheneedforcontinuousdatatransmission.Inresponsetothechallengesprevalent
intheHCIandUbicompdomainsconcerningthedeploymentofeverydayfacialbehaviorsensing
systems,FacePsyemphasizes:(1)Achievinghighperformanceinfacialimagecaptureandfeature
extractionwithoutcompromisingtheuserexperience.Thetestedresponsetimeensuresthatthe
appfunctions effectivelyinreal-time.(2)Prioritizing on-deviceimageprocessing tosafeguard
user privacy and improve battery efficiency. (3) Implementing trigger-based data collection to
refinemodelperformance,reducingtheneedforcontinuousdatamonitoringandprocessing.(4)
Enhancingsystemcontrollabilityandconfigurability,whichallowsresearcherstocustomizedata
collectionparametersaccordingtospecificresearchneeds.
Therestofthesectiondescribesthefacialbehaviorprimitivesdetectionimplementedinthe
FacePsysystemtoachievetherequiredresearchgoalindetail.
7MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
3.2 UnobtrusiveBackgroundSensingonAUser’sPhone
FacePsy has introduced functionalities, including configurable app triggers for data sampling.
Researcherscannowsetdatacollectiontriggerconditionsandsamplingrates,withadefaultsetto
10seconds,informedbystudiesindicatingpeakemotionalresponseswithinthistimeframe[24,64].
Theappsupportsreal-timefeatureextractionusingthesmartphone’scamera.Wequantizeda
CNNmodel[28]tointegrateintooursystem;itefficientlyextractsfacialbehaviorfeatures,with
processingtimevaryingbasedonthedevice’scapabilitiesandfeaturecomplexity.Processedimages
areautomaticallydiscardedfromtheuserdeviceafter20secondstoensureuserprivacyandmanage
storage.Noneoftheprocessedimagesleavetheuser’sdeviceorareprocessedoutsideofuser’s
device.Additionally,oursystemoffersresearchersflexibilityindefiningsamplingtriggersand
rates,enhancingitsapplicabilityfordiverseresearchneeds.
Uponinstallation,themobileappregistersasabackgroundservice,monitoringusereventslike
phonelock/unlockandapplicationusage(e.g.,WhatsApp,Twitter).Theseeventstriggera10-second
datacollectionsessionusingaphotoburst,adurationoptimizedforbatteryandcomputational
efficiencybasedonourfeasibilitystudy.FacePsycapturesfacialmarkers,suchasActionUnits[25],
andsecurelysyncsthisdatatoaresearchserverduringthissession.Tobalanceimageprocessing
demandsandresourceconsumption,theapprecordsatarateof2.5Hz,ensuringseamlessphone
usage.Thisdecisionwasinformedbyafeasibilitystudyinvolvingtwousers.Analternativefor
ahigherframeratecouldinvolverecordingasamediastreamandprocessingitframe-by-frame
usingacodec.
3.3 DesignRationaleBehindFacialBehaviorPrimitivesSelection
OurframeworkdesignisbasedonprinciplesofHCItheoryandisinformedbyextensiveresearchin
affectivecomputing.TheoreticalBackgroundsonhand-craftedfeatures(e.g.,HOG,LBP,etc.)[21,88]
ordeep-learned[49,101]featureshaveadoptedtorepresenteachframeorshortvideosegment
in lab settings. However, traditional hand-crafted features are not optimal for facial behavior
applicationsastheyarenotspecificallydesignedforthispurpose.Basedonpreviousstudies,which
suggestthatnon-verbalvisualcuescharacterizedepression,ourproposedapproachusesfacial
behavior attributes such as Action Units (AUs), face landmarks, and head pose as frame-wise
descriptors.Themachinelearningkit(MLKit)[41]isusedtoautomaticallydetectfacelandmarks,
smileandeye-openprobability,andheadpose,facilitatingdatacollection.Aconvolutionalneural
network(CNN)[28]isadaptedtodetecttheintensitiesof12differentAUs,resultingin151-channel
facialbehaviortime-seriesdata(12AU,1smileprobability,2eyeopenprobability,3headpose,and
133facelandmarks)foreachsession.
AUsfromthefacialactioncodingsystem(FACS)[25],whichtaxonomizeshumanfacialmove-
ments,specificallyAU4,AU12,AU15,andAU17,havebeenlinkedtodepressionseverity[36,87]
andmooddisorders[43,55].Additionally,133faciallandmarksthatlocalizekeyfacialregionsare
detectedusingMLKit[76].Featuresextractedfromtheselandmarks,suchasEye-aspectratio(EAR)
[31]andintervectorangles(IVA)[46],haveassociationswithhypervigilance[8],drowsiness[63],
andfacialexpressionanalysis[46].Moreover,they’vebeeninstrumentalinassessingmentalfatigue
[13].Wealsocomputedprobabilitiesforsmileandeye-openstates,whichhavebeenlinkedto
depression[37]andfatigue[57,100].Lastly,ourappcapturedheadEuleranglesrepresentinghead
movements.Headposefeatures,suchasslowerheadmovementsandspecificheadorientations,
havebeenidentifiedasindicatorsofdepression[3,87]andsuicidalideation[23,60].
Therationalebehindselectingthesefeaturesisrootedintheirestablishedassociationswith
mentalhealthindicators,especiallydepression.Byintegratingawiderangeoffacialattributes,
ourapproachaimstoprovideaholisticandunbiasedrepresentationoffacialbehaviors.Compared
8FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
topreviouslyemployedhand-craftedanddeep-learnedfeatures,thefacialbehaviordescriptors
providedinthisworkhaveseveraladvantages.Theyareimpartialsincetheirvaluesareunrelated
tothesubjects’identities,whichpreventsbiasbasedongender,age,ethnicity,etc.,frominfluencing
theresults,assuggestedbySongetal.[86].Second,Theyhaveaclear,comprehensiblemeaning,
whichmakesthemmoreinterpretable.
3.4 Configurability,PrivacyandUserAwareness
Toprovidehighconfigurabilityfortailoringtheapplicationasneeded,theFacePsyhelpsresearchers
adjustthedurationofdatacollection,whichbydefaultissetto10secondsuponanytriggerfordata
collection.Furthermore,itallowsfordistinctdatacollectiondurationsbasedondifferenttrigger
types,suchasappusage,andphoneunlocking.Lastly,thesystemsupportstheconfigurationof
variousappusagetriggerstoinitiatedatacollection,ensuringacomprehensiveandadaptable
researchtool.
Fig.1. ExamplesofTestImagesthatFailstoCaptureFeatures
Whendesigningoursystem,paramountimportancewasgiventouserprivacyandawareness.
DrawinginspirationfromHCIresearch,weinvestigateusernudging[5,30]inprivacysystems
[20]. This involves giving information about background data collection, especially in private
contextswherenoimagesarestored.Firstly,aclearnotificationisdisplayedinthenotificationbar,
indicating"Datacollectionisactive"toensureusersarealwaysawarewhendataisbeingcollected.
Additionally,agreenlightindicatorispositionednexttothecamera,signalingwhenthecamerais
activelycapturingdata.IntheeventFacePsyappautomaticallyrestartsitselfinthebackground,
eitherduetoaphonerestartoranappcrash,atoastnotificationispresentedtotheuser,stating
"FacePsyisrunningonbackground"Lastly,tofurthersafeguarduserdata,oncefacialbehavior
primitivesareextracted,theprocessedimagesareautomaticallydeletedfromtheuser’sdevice.
3.5 FeasibilityStudy
Prior to investigating if facial behavior primitives collected from FacePsy can assess a user’s
depression status, we decided to carry out a feasibility study to see whether the system could
preciselydetecttheuser’sfacialbehaviorsusingthefront-facingcameraofasmartphone.Todothis,
weenlistedthehelpof1volunteer(T2:tester2)andthefirstauthor(T1:tester1),whousedaGoogle
Pixel4and5asmartphonetogatherdatafortwodays.ParticipantscarriedFacePsyinstalledintheir
9MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
Fig.2. ExamplesofTestImagesThatSucceedstoCaptureFeatures
Table3. FacePsyAppResourceUsageonGoogle4&5a
Resource Day1(T1,T2) Day2 Avg(T1&T2)
Battery 37%,57% 58%,43% 48.75%
Memory(MB) 133,144 85,57 104.75
Datausage(MB) 9.25,22.23 16.13,6.3 13.48
Storage(MB) 175,155 177,177 171
phonesintotheirdailylives.FacePsycollecteddatawheneverparticipantsunlockedtheirphones.
Weevaluateoursystemforperceivedslownessintheirdevice,causinganydelay,interruption,or
disruptiontophoneusage.Ourfacialbehaviorsensingmodulesgeneratedpicturesannotatedaction
units,smileprobability,eyeopenprobability,headpose,andfacelandmarks.Aftergatheringthese
pictures,thefirstauthorconfirmedifthefacialbehaviorprimitiveswerecorrectlydetected.Todo
this,thefirstauthormanuallyverifiedthattheannotatedphotographsmatchedtheunannotated
photos.AnnotatedimagesoffacialbehaviormarkersthatwereidentifiedareshowninFigures2,
whileimageswereaccurate,theprocessingmodulesmadeerrorsonseveraloccasions(SeeFigure
1).Onaverage,theappconsumed48.75%batterywhilerunninginthebackgroundand104.75mb
memory.SeeTable3formoredetails.Intotal,wecollected834images.Ourfeaturesextraction
moduleprocessedonly817imageswhereafacewasdetectedwithanaveragefailurerateof2.04%
(SeeTable4).Thiswascalculatedbasedondailyend-of-dayreportsfromvolunteersregarding
resourceconsumptionbyourapp.Tooptimizeresourceconsumption,weimplementopportunistic
datacollection,whichallowstheapptocollectdatalessfrequently,thusreducingtheloadon
deviceresources.Furtherrefiningdatacollectiontriggerscanbedonetoensurethattheapponly
collectsdatawhenoptimalbehavioralsignalsarepresent,reducingunnecessaryresourceusage.
Theresultsofourappfeasibilityevaluationwereencouraging.Oursystemdetectedthefacial
behaviormakerswithouthinderingtheuserdeviceexperienceata10-secondphotoburstinthe
background.
10FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
Table4. FacePsyDataProcessingPerformance
Metric T1 T2
TotalImages 411 423
SuccessfulExtractions 401 416
NumberofFailures 10 7
OverallFailureRate(%) 2.43% 1.65%
FailureRateforAU(%) 10.97% 24.28%
FailureRateforLandmarks(%) 0.00% 0.00%
FailureRateforClassification(Eye,Smile) 0.00% 1.68%
FailureRateforHeadPose(%) 0.00% 0.00%
4 FIELDSTUDYDATACOLLECTION
Inthissection,wedescribeourstudyprotocolanddatacollectionprocessinnaturalisticenviron-
ments.
4.1 Participants
OfN=25participants(meanage27.88±8.87,range18-48)2,8werefemales,11weremales,and
6didnotspecifygenderonthedemographicsurvey.15participantswereAsian,4participants
wereCaucasian,and6participantsdidnotspecifytheirethnicityonthedemographicsurvey.1
participantindicatedthattheirhighesteducationwashighschool,8participantshadbachelor’s
degrees,10participantshadmaster’sdegrees,and6participantsdidnotindicatetheirhighest
educationonthedemographicsurvey.4participantsindicatedthattheyhadbeendiagnosedwitha
mentaldisorderinthepast,15indicatedtheyhadnot,and6didnotanswerthequestiononthe
demographicsurvey.DetaileddemographicdistributionisprovidedinTable5.
4.2 ParticipantsandStudyProcedure
ThisstudywasreviewedandapprovedbytheInstitutionalReviewBoard(IRB)attheUniversity.
Participantsinthisstudywereon-boardedremotelyacrossmultipletimezonesviaZoomConference
Meetings.Participantswereeligibletoparticipateinthestudyiftheywereabove18andowned
adataplan-enabledAndroidsmartphone.Theresearchteamadvertisedthestudythroughflyers
andpostsonFacebookandWhatsappgroups.Participantswereaskedtorespondtoascreening
questionnaireandselectapreferredtimefortheonboardingZoommeeting.Intheonboarding
meeting,theinterviewergaveparticipantsinformedconsentandaskedthemtorespondtothe
baselinequestionnaire.Afterthebaseline,theinterviewertookasemi-structuredinterviewto
understand the participant’s mental health, followed by installing a mobile application on the
participant’sdevicetotracksensordatafromtheirsmartphones.Thestudyquestionnaireswere
deliveredthroughemailandadministeredwithQualtrics,anonlinesurveyplatform.
Outof38participantswhowereinitiallyrecruited,only25participantscompletedthestudy.
Oneparticipantreportedhavingahighbatterydrainbecauseofourappanddroppedoutofthe
studyaftertwodays.Welaterfoundtheparticipanthadhighsocialmediausage,whichresultedin
frequentdatacollectiontriggers.Amongothers,3participantsdroppedoutforpersonalreasons,5
didn’tcompletesurveys,and4hadincompatibleAndroidversions,leadingtothefailureofthedata
collectiontriggermodule.Theparticipantswerecompensatedupto$135forfullcompliancewith
thestudy.Theparticipantswerecompensated$20forbaselineandinstallingthedatacollection
appandwerecompensated$25weeklyfor4weeks.
29participantsdidnotprovideanageinthedemographicsurvey.
11MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
Table5. DemographicDistribution
Attribute Unspecified Male Female Total
Gender 6 11 8 25
Age(Average) - 24.11 32.71 27.88
Ethnicity
-Unspecified 6 0 0 6
-Asian 0 9 6 15
-Caucasian 0 2 2 4
Education
-Unspecified 6 0 0 6
-HighSchool 0 0 1 1
-Bachelor’sDegree 0 5 3 8
-Master’sDegree 0 6 4 10
MentalHealthRate(Averageonascaleof1-10) - 6.73 6.88 6.79
DepressionState
-Unspecified 6 0 0 6
-Notatalloften 0 2 1 3
-Notsooften 0 4 4 8
-Somewhatoften 0 5 1 6
-Veryoften 0 0 2 2
MentalDisorderDiagnosis
-Unspecified 6 0 0 6
-No 0 9 6 15
-Yes 0 2 2 4
SmokingMarijuana
-Unspecified 6 0 0 6
-No 0 9 8 17
-Yes 0 2 0 2
4.3 Ground-Truth:Mentalhealthmeasures
Participants’ depression symptoms were assessed using a self-reported 9-item Patient Health
Questionnaire-9(PHQ-9)[58]atthreedistincttimes:uponjoiningthestudy(baseline),twoweeks
intothestudy(mid-point),andattheconclusionofthestudy(end-point).EachitemonthePHQ-9
is scored from 0 (not at all) to 3 (nearly every day), with the total scores ranging from 0 to 27.
Thisscoringcapturesthefrequencyofsymptomssuchasmood,sleepissues,fatigue,andchanges
inappetiteoverthepasttwoweeks.Monitoringasingleperson’smultiplePHQ-9scoresover
time can yield valuable insights into their mental health progression. The PHQ-9 categorizes
depressionseverityintofivelevels:scoresof0–4signifynodepressionsymptoms,5–9indicatemild
depressivesymptoms,10–14representmoderatedepressivesymptoms,15–19signifymoderately
severedepressivesymptoms,and20–27denoteseveredepressivesymptoms.Thismethodallows
researchersandclinicianstotracktheseverityandchangesindepressivesymptomseffectively.
Wecandefineadepressiveepisodeasaperiodcharacterizedbypersistentfeelingsofsadness,
hopelessness,andalackofinterestorpleasureinmostactivitiesobservedwithaPHQ-9score
duringatwo-weekobservationperiod.welabeltwoweeksofdatafromtheparticipantasdepressed
ornon-depressed(i.e.,adepressiveepisode)basedonthePHQ-9scoreoftheparticipantatthe
beginning and end of the two-week observation period, which falls within the range of mild
depressivesymptomsorworseaccordingtothePHQ-9severityscale[38].Welabelanindividual
12FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
ashavingadepressiveepisodeonlyifthePHQ-9scoreofthepersonisequaltoorgreaterthan5at
boththebeginningandendoftheobservationperiod;otherwise,itisconsideredanon-depressive
period.Thisapproachensuresconsistencyinlabelingperiodsasdepressiveornon-depressivebased
onestablishedthresholdsofdepressivesymptomseverity.Wecomplementourbinaryclassification
modelsbyincorporatingregressionmodelsdesignedtopredictthePHQ-9scores.It’simportant
to note that the PHQ is a versatile instrument, used both for screening for depression and for
monitoringchangesinclinicalsymptoms[59].Intotal,welabel14casesofdepressiveepisodes
and30non-depressiveepisodes(wheredepressiveepisodelengthistwoweeks).Weexcludedthe
lasttwoweeksofdatafrom6participantsduetonon-compliance,resultingintheexclusionof6
depressiveepisodes.
4.4 Facialbehaviordatacollection
TheFacePsyappactivatestocapturefacialdatainthreecircumstances:whentheuserunlockstheir
phonewhentheuseraccessesoneofapresetnumberoftriggerapps.Thephoneunlocktrigger
activatestheFacePsyappfor10secondsafterauserunlockstheirphone.FacePsyalsoactivatesfor
10secondswhentheuseropensoneofthirty-fivedifferentappsintotal,dividedintothecategories
ofcommunication,social,productivity,entertainment,andhealth.Forexample,Instagram,Google
Chrome,andAndroidMessagesareallconsideredtriggerapps.Theimagesprocessedweremostly
clearandhadhighresolution.However,someimageshadsomenoiseorblurrinessonwhichthe
modelcouldnotdetectfaces,whichisaprecursorforroutinessuchasAUdetection,landmark
detection,etc.Theseframesweredropped.
5 DATAPROCESSINGANDANALYSIS
5.1 FeatureEngineering
Sincemostofthefeaturesareextractedbyourbehavior-sensingsystemontheuserphoneitself,
weextractveryfewfeaturesaspartofpost-processing.Oursystempre-extractsfeaturessuchas
ActionUnits(AU1,2,4,6-7,10,12,14,17,23-24),SmilingandEyeopenprobability,facelandmarks,
andHeadposefeatures(yaw,pitchandroll)onuserdeviceitself.Weadditionallyextractfeatures
suchastheEye-aspectratioandInter-vectorangles.Moredetailsonthesefeaturesaredescribed
below.
Inter-vectorangle. Inter-VectorAngles(orIVA)arescale-invariantgeometricfeaturescomputedon
faciallandmarksforthepurposeoffacialshaperepresentation[46].Weconsiderthenosecenter
asthecentroidofthefaceforthepurposesofcomputingIVAfeatures.Wethensegmenttheface
into8regions(nosecenter,jawline,lefteyebrow,lefteye,righteyebrow,righteyebrow,mouth,
andcheeks)andcomputeintotal1439trianglesbytakingpermutationsofallpossibletriangles
fromthecentroidtotheremainingfaciallandmarks.WethenusePrincipleComponentAnalysisto
reducethenumberofIVAfeaturesdownto10.Wethencomputeangularvelocityandacceleration.
Eye-aspectratio. EyeAspectRatioisameasureoftheaspectratiooftheeyeregion,whichisused
asanestimateoftheeye-openingstate.WedefinedEARasthesumoftwoverticallengthsofthe
eyedividedbytwotimesthehorizontallengthoftheeye.
Wecollected12ActionUnits(AU),1SmileProbability,2EyeOpenProbabilities,3HeadEuler
Angles,2EyeAspectRatios,and20Inter-VectorAngles.Wesegmentedeachparticipant’sdayinto
fourepochs:midnight(12am-6am),morning(6am-12pm),afternoon(12pm-6pm),andevening(6pm-
12am),eachlastingsixhours.Foreachepoch,wecomputedstatisticalfeaturessuchasmin,max,
mean,median,sum,std,q1andq3tosummarizethefeaturesofthatepoch.Afterthisprocedure,
wehave320featuresforeachepochinourfinaldataset.Wethenclassifiedeachinstanceintoits
13MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
Fig.3. OverviewofOurAffectiveMobileSystem
respectivedepressionclass.Wenoticedvariationsinparticipationduration,withanaverageof3.36
daysmissingduetoparticipants’earlyexitfromthestudy.Thisresultedinatotaldatasetof616
participantdays.Intotal,wegathered544daysoffacialdatafrom25participantsoverafour-week
period.Notably,therewere55dayswithoutanyrecordeddata,asexplainedbyparticipants,such
asplannedholidaysorbreaks.Afurther17dayslackeddataduetoissueslikeimagequalityand
eye-openprobabilityinthefeatureextractionprocess.Asaresult,thetotalnumberofeffective
datapointsforanalysisis544.
5.2 StatisticalAnalysis
Theprimarytargetvariableofinterestinourstatisticalanalysiswasthepresenceorabsenceof
a depressive episode. We calculated Pearson’s correlation coefficient (r-value) for each feature
withinourdatasettoassessitsrelationshipwiththetargetvariable.Additionally,wedetermined
themeanandstandarddeviationforeachfeature,separatingthedatabygroup.Featureswere
thenorderedaccordingtotheabsolutevalueoftheirr-valuestopinpointthosewithatleasta
weakcorrelation(r-value>=|0.20|).Thisapproachallowedustofocusonthemostsignificant
relationships,improvingtheinterpretabilityandefficiencyofourmodels,minimizingunnecessary
complexity,andreducingthelikelihoodofoverfitting.
5.3 FeatureSelection
Our analysis used feature selection (FS) with a Decision Tree classifier, specifically the CART
(ClassificationandRegressionTrees)algorithmimplementedinscikit-learn’sDecisionTreeClassifier.
Tocomputetheimportancescoresforallfeaturesinthedataset,weusedtheGiniimportance,a
measurederiveddirectlyfromtheDecisionTreeitself.Wethenestablishedathresholdforfeature
selectionbased onthe mean valueof theseimportance scores, whichcalculated to0.00078125.
Featureswithimportancevaluesabovethisthresholdwereconsideredsignificantandretainedfor
furtheranalysis,whilethosebelowwerediscarded.Thisapproachensuresadata-driven,objective
criterionforfeatureselection,enhancingmodelinterpretivenessandefficiencybyfocusingon
featuresthatcontributetothepredictionofdepressiveepisodes.
14FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
5.4 PredictiveModelingwithMachineLearning
Inourpredictiveanalysisusingmachinelearning(SeeFigure3),wedevelopedaclassification
modeltodetectinstancesofdepressionandnon-depression,aswellasaregressionmodeltopredict
thePHQ-9score(SeeSection4.3).Weassignedlabelsofdepressionandnon-depressiontoeach
dayofdatabasedontheircorrespondingPHQ-9scores,whichservedastheground-truthvalues.
Our predictive modeling framework utilizes LightGBM (LGBM), a machine-learning library
thatimplementsagradient-boostingalgorithm.Thismethodhasdemonstratedrobustpredictive
capabilitiesinpreviousresearch[69,70]focusedondepressionprediction.Ourgoalistodevelop
bothauniversalmodelthatidentifiesgeneralpatternsandahybridmodelthatcapturesintricate
interactionsandtemporalsequenceswithinthedata.Toaddresstheissueofclassimbalance,where
depressiveinstancesarelessfrequent,weappliedSMOTE[11]onthetrainingdatasettoenhance
therepresentationoftheminorityclass,usedmeanimputationforhandlingmissingdata,and
performedstandardscalingonthefeatures.Hyperparametertuningwasconductedtomaximizethe
AUROCvalueforclassificationandMAEforregression.Weconstructedninesupervisedmodels,
eachtailoredtodifferentlearningschemesandutilizingsubsetsoffacialbehaviorfeaturessourced
fromvariousfacialregionstoassesstheirpredictivepowerfordepression.Theevaluationofthese
modelsprimarilyreliesontheAUROCscore[44].Thismetricisparticularlyeffectivefordepression
predictionasitevaluatesamodel’sabilitytodistinguishbetweendepressiveandnon-depressive
statesbyconsideringboththetruepositiverate(TPR)andfalsepositiverate(FPR).Theinsensitivity
ofAUROCtoclassimbalancemakesitespeciallyvaluable,andahigherAUROCscoresignifies
superiormodelperformance.
5.4.1 Universalmodel. Thislearningschemeutilizesastandardizedprocedurewhereasinglemodel
iscreatedforalluserstoidentifydepressiveepisodes.Itusesaleave-one-participant-out(LOPO),
a.k.aleave-one-out/leave-one-group-outcross-validationtechnique.Thisapproach,commonlyused
innumerousmobileinferencesystems,providesaclearunderstandingofmodelgeneralizability.
Oncethisuniversalmodelisinplace,itremainsunchanged.
5.4.2 Hybridmodel. Theidealmodelwouldblendthehighprecisionofindividualizedmodels
withtheeaseofuseofuniversalmodelsthatdon’trequireusertraining.Ourstudywasunable
to use individualized models due to a lack of data - only two labels per participant, which is
not enough for such intricate modeling. We experimented with a hybrid model, incorporating
asmallquantityofuser-specificdatawithabroadergeneraluserdataset.weimplementnested
cross-validationtominimizethelikelihoodofmodeloverfittingbyimplementingarobustML
modeltrainingstrategyrecommendedbyAsareetal.[69]forthepredictiveanalysisofdepression.
Weemployedstratifiedthree-foldcross-validationwithatime-seriesawareleave-one-participant-
day-out(LOPDO)cross-validationfortheouterandinnercross-validation.Inotherwords,one
participant’sdayischosenasthetestset,andtheremainingparticipant’sdatasetischosenasthe
trainingsetforeachiterationofthenestedcross-validation.Alltrainingsetsamplescapturedafter
thetestsetaresubtractedfortime-seriesawareness.Thisapproachcouldavoidtheunworkable
situationinwhichfuturedatasetsareusedtoforecastthepast.Consequently,theLOPDOmodel
effectivelycombinesuniqueaspectsofhowanindividual’sfacialbehaviordataislinkedtotheir
stateofdepressionwhilealsoidentifyinggeneralpatternsconsistentlyseenacrossdifferentpeople.
The classifiers’ hyperparameter optimization, feature scaling, oversampling, and missing data
imputationwerealladdressedbyinnercross-validation.Usinggridsearchacrossapredetermined
setofparameters,weoptimizedtheclassifiers’hyperparametersbymaximizingthemodelAUROC
score.
15MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
6 FIELDSTUDY:UNDERSTANDINGANDDETECTINGDEPRESSIVEEPISODES
ToobtainthefeasibilityofourproposedFacePsyframeworkinthefieldstudy,weaddressthe
followingquestion:(RQ1)canthefacialbehaviorfeaturescollectedbyourmobilesensingsystem
beeffectivelyutilizedtodetectdepressiveepisodesinanaturalisticenvironment?(Section6.2).To
understandsetsofdepression-relatedbiomarkersthatcouldbeusedinthewildweaddress(RQ2)
What’sthesignificanceofdifferentbiomarkersondepressiondetectiondifferentiatingdepressive
vs.non-depressiveepisodesinreal-worldsettings?Weintroducethetop27facialbehaviorfeatures.
Table6. SummaryofFeatureCorrelationswithDepressiveEpisodes
Feature p-value r-value DepressiveEpisode Non-DepressiveEpisode
(<0.05) Mean(SD) Mean(SD)
ear_right_sum_morning 0.00 0.35 23.34(28.87) 8.26(11.1)
ear_left_sum_morning 0.00 0.34 21.36(25.82) 7.99(10.83)
headEulerAngle_Y_sum_morning 0.00 -0.33 -404.03(719.02) -33.33(332.65)
leftEyeOpenProbability_sum_morning 0.00 0.33 55.41(69.64) 21.04(28.1)
rightEyeOpenProbability_sum_morning 0.00 0.31 48.74(62.6) 19.99(25.73)
AU15_min_afternoon 0.00 0.27 0.09(0.14) 0.04(0.04)
rightEyeOpenProbability_std_evening 0.00 0.26 0.24(0.07) 0.2(0.07)
smilingProbability_sum_morning 0.00 0.26 5.52(9.62) 1.98(3.52)
rightEyeOpenProbability_std_afternoon 0.00 0.23 0.24(0.07) 0.2(0.07)
AU17_sum_midnight 0.00 -0.23 6.36(13.6) 21.02(34.59)
AU12_median_morning 0.00 -0.22 0.29(0.21) 0.4(0.27)
AU07_std_evening 0.00 0.22 0.26(0.08) 0.22(0.07)
AU02_std_evening 0.00 0.21 0.21(0.07) 0.17(0.08)
smilingProbability_max_evening 0.00 0.21 0.35(0.2) 0.25(0.21)
AU07_median_morning 0.00 -0.21 0.59(0.24) 0.68(0.2)
smilingProbability_max_morning 0.00 0.21 0.33(0.2) 0.24(0.2)
smilingProbability_mean_midnight 0.00 0.21 0.14(0.12) 0.09(0.09)
AU12_q3_morning 0.00 -0.21 0.39(0.23) 0.5(0.27)
AU12_mean_morning 0.00 -0.21 0.31(0.19) 0.41(0.24)
leftEyeOpenProbability_std_evening 0.00 0.20 0.23(0.07) 0.2(0.07)
headEulerAngle_X_std_evening 0.00 0.20 3.86(1.4) 3.25(1.33)
AU06_max_morning 0.01 -0.20 0.56(0.27) 0.66(0.23)
smilingProbability_std_evening 0.00 0.20 0.09(0.05) 0.06(0.06)
smilingProbability_std_afternoon 0.00 0.20 0.08(0.06) 0.06(0.06)
smilingProbability_mean_afternoon 0.00 0.20 0.11(0.09) 0.07(0.09)
AU06_mean_morning 0.01 -0.20 0.29(0.19) 0.38(0.21)
smilingProbability_mean_evening 0.00 0.20 0.1(0.08) 0.07(0.07)
6.1 Statisticaldifferencebetweendepressiveandnon-depressiveepisodes
Theanalysisevaluatedthecorrelationofvariousfeatureswithdepressiveepisodes(SeeTable6).
Thefeatureswererankedbasedonthestrengthoftheircorrelation(r-value)withthetargetvariable,
whichindicatesthepresenceofadepressiveepisode.Outof1280only158featureshadap-value
lessthan0.05.Selectingfeatureswithatleastaweakcorrelation(r-value>=abs(0.20)),streamlining
analysisbyprioritizingmeaningfulrelationships,andenhancingmodelinterpretabilityandeffi-
ciencywhilereducingnoiseandtheriskofoverfitting.Temporaldynamicsofdepressiveepisodes
suggest features measured in the morning often show significant correlations, pointing to the
16FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
potentialimpactofdepressiononmorningroutinesorstates,suchasreducedfacialexpressiveness
orspecificeyemovementpatterns.ThefeatureheadEulerAngle_Y_sum_morninghasthestrongest
negativecorrelationwithdepressiveepisodes,withanr-valueof-0.33.Thissuggeststhatasthe
valueofthisfeaturedecreases,thelikelihoodofadepressiveepisodeincreases.TheYrotationof
theheadtranslatestotheyawoftheEulerangle.Otherfeatureswithnotablenegativecorrelations
includeAU17_sum_midnight,AU12_median_morning,AU12_q3_morningAU07_median_morning,
AU06_max_morningandAU06_mean_morning.TheabsenceofAU06,associatedwithexpressing
emotionsrelatedtohappinessorjoy,correlateswithdepression.Furthermore,AU07,AU12and
AU17 havebeenlinkedtodepressionseverity,supportingexistingevidence[36,87].
Thefeatureear_right_sum_morningandear_left_sum_morningshowsastrongpositivecorrela-
tionwithdepressiveepisodes,withanr-valueof0.35and0.34,respectively.Thisindicatesthatas
thevalueofthisfeatureincreases,thelikelihoodofadepressiveepisodealsoincreases.It’sveryim-
portanttoconsiderthetemporaldynamicsofthesefeatures.Otherfeatureswithsignificantpositive
correlationsincludeleftEyeOpenProbability_sum_morning,rightEyeOpenProbability_sum_morning,
rightEyeOpenProbability_std_evening and leftEyeOpenProbability_std_evening. The presence of
strongEAR,eyeopenprobabilityrelatedtohighalertness[2]inmorningandeveningcouldbe
explainedasthe“eveningness–morningness”dimensionindepression[12].Thepreferencefor
morningoreveningcanlargelybeattributedtothereductionofdepressivesymptomssuchaslow
energy,avoidanceofsocialinteraction,andlossofinterestinpreviouslypleasurableactivities[77].
Thepresenceofapositivecorrelation(r-valuesof0.26,0.21formorningsumandmaximum,
respectively; and similarly positive correlations for evening and midnight measures) between
smilingProbabilityanddepressiveepisodessuggeststhathighersmilingprobabilitiesareassociated
withanincreasedlikelihoodofdepressiveepisodes.Thisinterpretationmayseemcounterintuitive
sinceit’sexpectedthatdepressiveepisodeswouldbeassociatedwithlesssmiling.However,this
unexpectedpositivecorrelationdoesn’tnecessarilyimplythatsmilingmoreleadstodepressionor
viceversa.Itmightreflectcomplexunderlyingbehaviorsorcompensatorymechanisms,suchas
"smilingdepression,"whereindividualsmightsmileormaintainafacadeofhappinessinsocial
situationsdespiteexperiencingdepressivesymptomsinternally[95].However,asalimitationof
ourstudywearenotabletoconfirmifparticipantsaregoingsuchcases.Previousresearchsuggests
thatdepressionisnotonlyassociatedwithsadfacialexpressionsbutalsowith“atotallackof
facialexpressioncorrespondingtothelackofaffectiveexperience”[27].Sincewecollectshort
segments(10sec)ofdata,itcanbeinterpretedinvariousways,e.g.,asmilemaybearesultof
feelinghappyorfeelinghelpless,assuggestedbypriorresearch[87].Whileapositivecorrelation
betweensmilingProbabilityanddepressiveepisodesseemsparadoxical,ithighlightsthecomplexity
ofdepressivebehaviorsandtheimportanceofconsideringbroaderpsychologicalandsituational
contextswheninterpretingthesefindings.
6.2 Modeldevelopmentfromdatainthefieldstudy
6.2.1 Universalmodel. Table7summarizesthepredictiveperformanceofuniversalmodels.To
understandhowdifferentsubsetsoffacialbehaviorfeaturescontributetodetectingdepression,we
evaluatedninedifferentmodels,eachwithadifferentfacefeatureset,usingLightGBM.Themodel
usingthemostsignificantfeaturesfromthecorrelationanalysisperformedthebest,followedby
themodelthatincludedfeatureselection.Thisapproachenhancesthemodel’sinterpretability
andcomprehension.TheTSFmodelachieved51%accuracy,withaprecisionof40%,indicating
thatitcorrectlypredicteddepression40%ofthetime.Arecallof96%suggeststhatoutofallthe
depressiveepisodecasesinthedataset,themodelsuccessfullyidentifies96%ofthemaspositive.
Thisisparticularlyimportantindepressiondetection,wheremissingoutonpositivecasesleadsto
missingoutonopportunitiestointervene.Themodel’sreliabilityisalsoreflectedinanAUROC
17MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
scoreof0.67(Fig.4).Intermsofregressionmetrics,theMAEforeachmodelalsoprovidesinsights
intothequantitativeaccuracyofdepressionseverityestimation,showingthelowesterror(3.26)for
theActionUnitsmodel,whichsuggestsitssuperiorabilitytoestimatetheseverityofdepression
correctlycomparedtoothermodels,whereTSFmodelgotanMAEof5.13.Whilethisindicates
limitedabilitytodistinguishbetweendepressionandno-depressionclasses,itrepresentsbetter
agreementbetweenthemodel’spredictionsandactualobservationsthanarandomclassifier.
Fig.4. TheROCplotsshowtheuniversalmodelperformanceofeachfeaturetypemodel.
Table7. UniversalModelPerformance:WetrainedeightLGBMmodelsforpredictingdepression,includinga
differentfeaturesubset.Themodeltrainedusingallfeaturesshowedthebestresultsinpredictingdepression
Model MAE Accuracy Precision Recall F1 AUROC No. of Fea-
tures
EyeOpenProbability(EOP) 5.20 0.33 0.31 0.83 0.45 0.33 64
SmilingProbability(SP) 5.32 0.37 0.33 0.87 0.48 0.52 32
HeadEulerAngle(HEA) 4.71 0.29 0.28 0.71 0.40 0.27 96
ActionUnits(AU) 3.26 0.38 0.30 0.66 0.42 0.45 384
Eye-aspectratio(EAR) 5.31 0.32 0.30 0.80 0.44 0.35 64
Inter-vectorangle(IVA) 4.59 0.40 0.31 0.66 0.42 0.43 640
TopSignificantFeatures(TSF) 5.13 0.51 0.40 0.96 0.56 0.67 27
FeatureSelection(FS) 4.04 0.50 0.38 0.77 0.51 0.57 46
Allfeatures 3.77 0.40 0.28 0.51 0.36 0.40 1280
6.2.2 Hybridmodel. Table8summarizesthepredictiveperformanceofhybridmodels.Themodel
withthebestperformanceistheoneusingfeatureselectionwithLightGBM.Inthecontextof
detecting depressive episodes, the model demonstrated a commendable performance with an
accuracyof69%.Notably,whenpredictingadepressiveepisode,itwascorrect57%ofthetime,
asindicatedbyaprecisionforthedepressiveclass. Furthermore,itsuccessfullyidentified62%
of all actual depressive episodes, reflected by a recall. The F1-score, a measure of the model’s
balancebetweenprecisionandrecall,was0.67fordepressiveepisodes,suggestingaharmonized
performancedespitetheinherentclassimbalance.Themodel’sreliabilitywasalsounderscoredby
18FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
anAUROCof0.81,indicatingastrongabilitytodistinguishbetweenthetwoclassesandagood
agreementbetweenthemodel’spredictionsandactualobservations.Theregressionresultsfurther
enhanceourunderstanding,withthemodelachievinganMAEof3.08onthePHQ-9scale,which
rangesfrom0to27.Thisindicatesthatthemodel’sdepressionseveritypredictionsaretypically
withinapproximatelythreepointsoftheactualclinicalassessments,showingrelativelymoderate
accuracyinquantifyingtheseverityofdepressivesymptoms.
Fig.5. TheROCplotsshowthehybridmodelperformanceofeachfeaturetypemodel.
Table8. HybridModelPerformance:WetrainedeightLGBMmodelsforpredictingdepression,includinga
differentfeaturesubset.Themodeltrainedusingallfeaturesshowedthebestresultsinpredictingdepression
Model MAE Accuracy Precision Recall F1 AUROC No. of Fea-
tures
EyeOpenProbability(EOP) 3.16 0.67 0.50 0.48 0.49 0.66 64
SmilingProbability(SP) 3.26 0.64 0.46 0.43 0.44 0.63 32
HeadEulerAngle(HEA) 3.08 0.72 0.60 0.51 0.55 0.75 96
ActionUnits(AU) 3.02 0.67 0.50 0.35 0.41 0.67 384
Eye-aspectratio(EAR) 3.37 0.64 0.45 0.41 0.43 0.63 64
Inter-vectorangle(IVA) 3.57 0.59 0.35 0.28 0.31 0.55 640
TopSignificantFeatures(TSF) 3.18 0.70 0.55 0.55 0.55 0.77 27
FeatureSelection(FS) 3.08 0.69 0.57 0.62 0.67 0.81 46
Allfeatures 2.81 0.71 0.59 0.39 0.47 0.75 1280
Overall,theperformanceofthemodelsvaried,buttheoneusingselectedfeaturesshowedthe
bestresultsinpredictingdepression.FromtheAUROCplot(Fig.5),wecanobserveeventhough
modelwithHEAachievedbetterresultsintermsofaccuracyof72%,themodelitselfisstablewhen
combinedwithotherfeaturesitsyieldsmuchbetterresultswithmorepredictiveperformance
stabilization.
6.2.3 Minimumnumberofdaysneededtoproducereliabledetection. TheAUROCisametricusedto
evaluatetheperformanceofadiagnostictest,withvaluesrangingfrom0.5to1.Avaluegreaterthan
0.5isnecessaryforthetesttobemeaningful,andanAUROCof0.7oraboveisgenerallyconsidered
19MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
acceptable.Inthecontextofadepressiondetectionmodel,theperformancewasbetterthanrandom
guessingonday1,withanAUROCofgreatedthan0.5.Onday1,themodel’sperformanceimproved
toafairlevelwithanAUROCof62.4%.Remarkably,startingfromday7,themodelachievedan
acceptableperformancewithanAUROCof71.4%.Thisprogressionillustrates(Fig6)asignificant
enhancementinthemodel’sabilitytoaccuratelydetectdepressionovertheweeks.
Fig.6. Minimumnumberofdaysneededtoproducereliabledetection
7 LESSONSLEARNED:DISCUSSINGTHEDETECTIONOFDEPRESSIVEEPISODESON
ASCALE
In this section, we highlight our work that contributes to new insights into the detection of
depressiveepisodesineverydaysettingsbydesigninganddevelopinganopen-sourceaffective
mobilesystem.Wefoundeyeopenstate,headpose,smile,andactionunits(2,6,7,12,15,and17)
askeyaffectiveindicatorsindifferentiatingdepressiveandnon-depressiveepisodesthathavebeen
validatedinourfieldstudy.Thecombinedfeaturescanbeusedtopredictdepressionepisodes.The
universalmodelhasanAUROCof67%,whilethehybridmodelhasanAUROCof81%.Further
improvementcanachievedbycollectingmoredataforsubsequentweeks.Theseresultsserveas
abridgebetweencontrolledlaboratorystudiesandreal-worldapplications,demonstratingthe
feasibilityofdepressiondetectionusinganaffectivemobilesensingsystem.
7.1 ComparisontoPriorWork
Inthelandscapeofdepressiondetectionusingmobilesensing,ourapproachsignificantlyadvances
byleveragingin-the-wilddatacollectionthrougheverydaysmartphoneinteractions.Thismethod
contrasts with many previous studies that primarily utilize lab-controlled [10, 17, 56, 87, 94]
environmentsfordatacollectionusingcameramodality,thuslimitingthegeneralizabilityoftheir
findings.Forinstance,ourworkcomplementsthefindingsofNepaletal.[66],whichalsoemploys
anin-the-wildapproachbutfocusesondifferentfeaturesets,suchasgazeandheadpose,along
with 2D and 3D facial landmarks. While their study achieves a balanced accuracy of 61%, our
methodenhancesthemodel’ssensitivitytosubtlerindicatorsofdepressionthroughadetailed
analysisoffacialactionunits(AUs),eye-openstates,andsmileexpressions.Ourmethodachieves
anAUROCof81%andAccuracyof69%,andmaintainsaconsistentMAEacrossdifferentvalidation
strategies,highlightingtherobustnessofourfindingsandtheirpotentialforreal-worldapplication.
Thisdemonstratestheincrementalutilityofourapproach,particularlyintheseamlessintegration
ofmentalhealthmonitoringintodailytechnologyuse,thuscontributingvaluableinsightsinto
mobilehealth(mHealth)technologiesfordepressiondetection.
20FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
Incomparisontomobilesensing,studiesfocusedondepressiondetection,ourapproachcapi-
talizesondirectbehavioralmarkersaccessibleviaasmartphonecamera,distinguishingitfrom
studiesthatrelyonperipheralsensordatasuchasGPSorphysicalactivitymetrics.Forexample,
Chikersaletal.[15]andOpokuetal.[69]employavarietyofsensorstoinferdepressivestates
indirectlythroughchangesinmobilitypatternsandphoneusage.Whilethesestudiesachievehigh
accuracyandAUCscores,theymaynotcapturethenuancedemotionalstatesthatfacialbehavior
canindicate.Ourmethod’sutilizationofdetailedfacialactionunits,eye-openstates,andhead
gesturesoffersamoredirectandpotentiallyinsightfulmeasureofdepressiveepisodes,evidenced
byourcomparableAUCof81%.Ourmodeloutperformedcomparedtoamodelthatsolelyutilized
sensordatafromtheAWAREplatform[32],whichincludedBluetooth(Accuracy=69.3,F1=0.64),
Calls(Accuracy=68.5,F1=0.59),GPS(Accuracy=69.5,F1=0.62),andStepsCounter(Accuracy=63.6,
F1=0.53),asdetailedinresearchbyChikersaletal.[15].Ourapproachachievedanaccuracyof69%
andanF1scoreof0.67,surpassingindividualsensorresults.However,Chikersaletal.reporteda
higherF1scoreof0.78whencombiningallsensors,indicatingsuperiorperformancecomparedto
ourmodelinthatspecificsetup.Thisspecificityindetectingemotionalexpressionsoffersacritical
enhancementovertraditionalmobilesensingmethods,makingourapproachavaluableadditionto
thespectrumoftechnologiesformonitoringmentalhealthineverydaysettings.
7.2 Insights,ChallengesandOpportunitiesinPredictingDepressiveEpisodesinthe
NaturalisticEnvironments
Previouslab-controlledstudieshavedelvedintotheextractionoffacialbehaviorprimitivesfrom
faceimagesusingaffectsensingsystemslikeOpenFace[6].Thesestudies[10,17,56,87,94]have
achievedimpressiveperformanceindetectingdepression,duetotheirhighdatacollectionrate
(e.g.,processingvideo)andthecontrolledenvironmentinwhichthedatawascollected.Incontrast,
deployingthesesystemsinreal-worldscenariosusingsmartphonesresultsinseveralchallenges.
On-deviceresourcelimitationsoftenleadtoareducedframerate(2.5Hz),andtheunconstrained
datacollectionsettings–affectedbyfactorssuchasvariedlightingconditions,phoneorientation,
ongoingactivities,andwhethertheenvironmentisindoorsoroutdoors–canleadtopoorquality
ofdatacollection.This,inturn,canimpactthelackofsamplesthatlimitsthedevelopmentof
predictivemachinelearning.Mostrecently,MoodCapture[66]wasintroducedthatcapturesfacial
imagesinnaturalenvironmentsfordepressiondetection.Theirresearchdemonstratedthatusing
arandomforestalgorithmtrainedonfaciallandmarks,it’spossibletoidentifydepressionand
predict PHQ-8 scores among individuals. However, the utility of MoodCapture for developers
intendingtoreplicatesuchstudiesindifferentsettingsmaybelimitedduetothelackofaccess
tothemobilesystem,dataset,ormachinelearningframeworkusedbytheauthors.Ourresearch
buildsuponMoodCapture’swork;weadvancedatacollectionbyusingaliterature-basedtrigger
mechanismthatrespondstouserinteractionslikescreenactivityandappusage,enhancinguser
privacybyprocessingdataon-deviceanddiscardingrawimagespost-analysis.Ourstudyisnovel
indevelopinganopen-source,privacy-awaremobilesystemthatcapturesandprocessesfacialdata
innearreal-time,introducingsignificantimprovementsinprivacy-awareness,datacollection,and
on-deviceprocessing.
Thesimilaritiesanddifferencesobservedbetweenlab-controlledandreal-worldpredictivemodels
canbeattributedtotheinherentnatureoftheenvironmentsinwhichtheyoperate.Grounded
inHCI/affectivecomputingtheories[75],controlledenvironmentsallowforminimizingexternal
variables[9],ensuringthatthesubject’semotionalstateprimarilyinfluencesthedatacollected.
Insuchsettings,thesystemcanfocussolelyonthefacialbehaviorprimitiveswithoutinterfering
withexternalfactors.However,inreal-worldscenarios,themyriadofuncontrollablevariables,
fromlightingtopersonalactivities,introducesnoiseintothedata,whichcanmaskordistortthe
21MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
trueemotionalindicators.Fromanaffectivecomputingperspective,human-computerinteraction
isdynamicandmultifacetedinreal-worldsettings.Thesystemhastointerprettheemotionalstate
andaccountforthecontextinwhichtheinteractionistakingplace.Thiscontextcansignificantly
influencetheemotionalindicators,makingthemmorecomplextodecipher.Theseinsightsare
crucialforthedesignofanaffectivemobileframeworkformentalhealth.Recognizingthechallenges
ofreal-worlddatacollection,futureframeworksshouldincorporateadaptivealgorithmsthatcan
adjusttovaryingconditions,ensuringconsistentandaccuratepredictions.Additionally,leveraging
context-aware computing can help the system contextualize the data, distinguishing between
genuineemotionalindicatorsandthoseinfluencedbyexternalfactors.Thisapproachwouldlead
toamorerobustandreliableaffectivemobilesensingsystem,enhancingitspotentialinmental
healthapplications.
Our method demonstrates good performance in depression detection compared to previous
workbyOpokuetal.[69],whichusedasimilarlearningschemeforahybridmodel.However,
it’simportanttonotethatthiscomparisonmaynotbeentirelydirect.Additionally,ouruniversal
modelshowsfairperformancecomparedtopriorworkbyChikersaletal.[15]inpassivesensing
usingasimilarlearningscheme.Thereferencedstudyincorporatesacomprehensivesetoffeatures,
including sleep patterns, physical activity, phone usage, GPS location, and daily mood ratings,
tomodelbehavioralandsocialsignals.However,asextensiveresearchsuggests,depressionisa
multifaceteddisorderthataffectsvariousaspectsofanindividual’sbehavior,socialinteractions,and
physiology.Theexistingmobilesensing-basedapproaches[15,29,69]havelimitationsincapturing
affectivesignals[1]manifestedthroughinvoluntaryfacialmuscleandheadgestures,whichhave
beenestablishedascrucialindicatorsofdepression.Althoughwearablesensing-basedphysiological
solutionshavebeenproposed,theirhighdeploymentcostsincludingextracostforpurchasing
andwearingdevicespresentasignificantchallengeandlackofaffect/emotionalsignals.Previous
researchinaffectivecomputingconductedwithincontrolledlaboratorysettings[10,17,56,87,94]
hasdemonstratedsignificantpromiseincapturingemotionalsignalsinindividualswithdepression.
However,thesestudiesareconstrainedintheirapplicabilitytoreal-worlddeploymentsdueto
computational cost, cost of devices, and user efforts to wear extra devices, particularly when
continuousmonitoringisessentialfordeliveringtimelyinterventionsbasedonsignalscaptured
fromusers.Theresolutionoftheseissuesremainsunexplored.Therefore,ourstudyaimstobridge
thesegapsbyproposing,collecting,andevaluatingthefeasibilityofdeployingdatausingournovel
affectivemobilecomputingsysteminareal-world,naturalisticsetting.
7.3 PrivacyandEthicalConsiderationstoEnhanceFeasibilityinReal-WorldSettings
Becauseacameratemporarilycapturesauser’sfaceontheirsmartphoneforupto10seconds
beforedeletion,usersmayhaveconcernsduetothehumanperceptionof’usingacamera,’even
thoughthesystemdoesnotrecordanyvideos.Tobalanceprivacyconsiderationsanddataquality
formodeling,researchersintheHCIcommunityshouldconsiderdesigningusernudging[5,30].
Thesenudging,assuggestedbyresearchersinprivacy-preservingsystems[20],couldallowusers
topush/pullstatusaboutsystembehaviorwhenrunninginthebackgroundusingcontentthat
includesappropriateinformationaboutdatacollectionstatusinprivatecontextswheretheymay
feeluncomfortable,evenwhennoimagesarecollected/stored.
While our system automatically removes images after near-real-time feature extraction, the
conceptofpassivesensing[20]thatunderpinsouraffectivemobilesensingframeworkdoesnot
necessarilyrequireauserinterfacetoreduceusers’burden;instead,itrunsinthebackground.
Thisisincontrasttoactivesensing,typicallyusedformanualtasks.Nevertheless,itisessential
to empower users with the ability to enable or disable specific functionalities whenever they
perceivepotentialrisks,ratherthanrequiringthemtoexitthestudy.Whilethereisatrade-off
22FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
betweenprivacyandthequantityofdatacollected,itiscrucialforHCIresearcherstocollaborate
indesigningmentalhealthtrackingsystemsthatenhanceuserengagement[68],encourageself-
reflection[62],motivation[19],andtrust[52],ashasbeenwell-establishedinthefieldofpersonal
informatics.Forexample,ourindividualfacialbehaviorandheadposefeaturescanprovideusers
withdailyhappinesspercentagescoresusingfacialexpressionalgorithms.Thiscanserveasone
ofthemotivationalstrategiesthatcouldencouragethemtoreflectontheirmentalconditionand
enablethemtomaintaingoodmentalhealthpractice,potentiallycontributingvaluabledatafor
long-termhealthcareresearchinthefieldofmentalhealth.
Tobalanceprivacyconsiderationsanddataqualityduringsystemdesign,researchersshould
considerhowdatacouldbecollected,used,andstored,aswellaswhatimplicationsthiscould
havefortheprivacyoftheusers.Whiletheremaybediscussionoverhowdataprocessingcan
dictate/drivethelevelofinvasivenessoftheapplicationwhentheusersaregivenanoptionof
choosing which type of processing or filters (presenting blurry face images for facial feature
extraction)[20]theywouldallowthedeveloperstocarryout.Atthesametime,thisnecessary
allowingtheusertodosomayimpactthenumberofsignalscomingfromtheuser’sfacialdatafor
accuratebehaviormodeling.
Wehighlightthehighmonetarycostsassociatedwithwearable-basedphysiologicalmarkers
andthelackofrichemotionaldata.Whileweagreewiththeobservationregardingthefinancial
aspect,wewouldwanttoadequatelyaddressthepotentialprivacycostsofcamera-basedsensing,
whichcollectsuserdataopportunisticallythroughouttheday.Theprivacycostinthiscontext
referstothepotentialriskofunauthorizedaccessormisuseofpersonalandsensitivevisualdata
capturedbythecameras.Amethodtoreducethisimpactcouldinvolvediscardingtheuserimages
immediatelyaftercomputingthesensingvalues,therebyminimizingthestorageofpotentially
sensitiveinformationorperformingthedataprocessinginmemory.Further,wewouldreducethe
timeforautomaticfeatureextraction(currently10secondsperimage).Whilewealsoprovided
notificationtousersthatFacePsyiscollectingdatainthebackground,morecomprehensiveand
clearjustificationthatconsidersmonetaryandprivacycostsandstrategiestomitigatepotential
concerns.Facialdataofapersoncontainsvariouscharacteristicsofthefacesuchasshape(face
landmarks,smileprobability),eyeshape(eye-aspectratio),musclemovements(ActionUnits),and
faceorientation(headEulerangles).Whilethesecharacteristicsdescribethestateofthefaceatany
givenmomentbutdon’trevealaperson’sidentity.Itisimportanttoensurethatanyfacialdata
gatheredisencryptedandsecurelystored,aswellasensurethatauser’sidentityisnotrevealedin
anyway.Autilitarianapproachfromthenormativeethicspointofview[80]toprivacy-maintaining
interactionswithsuchdataistobalancethebenefitsthatcanbegainedfromfacialrecognition
technologywiththepotentialprivacy risks.Thisapproachinvolvesconsideringthetrade-offs
betweenthebenefitsofusingfacialrecognitiontechnologyandthepotentialriskstoprivacy.This
approachrequiresmakingdecisionsthatprioritizethegreatergood,suchasdecidingtocollect
onlytheminimumamountofdatanecessary,processingthedatainawaythatdoesnotrevealuser
identity,andencryptingandsecurelystoringthedata.Additionally,thisapproachmayinclude
takingstepstoensurethatfacialrecognitiontechnologyisusedresponsiblyandethically,suchas
providinguserswithinformationonhowtheirdataisbeingcollectedandusedandallowingthem
tooptoutofthesystemiftheychooseto.
7.3.1 UserFeedback. Wecollectedfeedback(questionnaireavailableinAppendixA.2)frompartic-
ipantsattheendofthestudy,providingvaluableinsightsintotheirexperiencesandperceptions
regardingtheFacePsyapp.Here,weexploretheinitialreactions,adjustmentstothedatacollection
processes,andtheevolvingacceptanceofprivacymeasuresthroughoutthestudy.Thisfeedback
isinstrumentalinunderstandingthereal-worldimplicationsofdeployingsuchtechnologyand
23MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
informspotentialenhancementstoimproveuserexperienceandtrust.Belowaredetailedreflections
acrossfivekeyareas,supportedbydirectquotesfromtheparticipants,illustratingthenuanced
reactionsandsuggestionsforfuturedevelopment.
• InitialPerceptionsandConsent:Participantsinitiallyhadmixedfeelingsaboutfacialdata
collection.Forinstance,P10stated,"Iwasalittleskepticalmyappsmighthungduetothis
newfunctionality." Despiteinitialhesitations,theconsentprocesswasgenerallyfoundto
bereassuring.P8shared,"Yes,everythingwasproperlyaddressedandIwaswellinformed,"
reflectingasentimentthattheprivacyanddatausageconcernswereadequatelymanaged.
Some participants (P23, P30) showed an initial discomfort with the app collecting their
sensitivefacialdataregardingprivacy.Thisdiscomfortmayariseduetotheintroductionof
newtechnologywhichtheyhaven’tusedpreviously.Whilediscomfortwasmentionedby
participants,theyshowedashiftintheircomfortlevelusingFacePsyoveraperiodofuse.
• ExperiencewithDataCollectionTriggers:Theexperienceoftheappactivatingonphone
unlockingoropeningtriggerappsvaried.Whilesomeparticipantsadjustedovertime,others
remainedconcerned.P35commented,"Itdefinitelybecamemoreacceptableovertime.Even
thoughIwasn’tparticularlyconcernedaboutdatacollection,Igotmoreusedtoitafterthefirst
fewdays."showingaquickadaptation,whileP24noted,"Itwasabitofanadjustmentbut
becamenormalwithtime",showingslowadoptionofappstriggerdatacollectionintheir
dailysmartphoneuse.WhereasP8noted,"ItwasaslightconcernatthestartbutIgotusedto
iteventually."
• ImpactonDailyUseandPrivacy:Changesinusagehabitsduetotheapp’sdatacollection
wereminimal.Thefeaturethatautomaticallydiscardedimagesafter20secondswaspositively
received.Forinstance,P24appreciatedthisfeature,saying,"Increasedcomfortlevel",P23
mentioned "A bit of comfort, but I am also aware that social media companies convert the
imagesintometadata.wheretheoriginalimageisnolongerneeded".Thisshowsparticipants
were educated about such data collection methods. While P35 mentioned, "It may have
subconsciouslyledmetousemyphonelessfrequentlyatbeginning.ButIgotusedtoovertime
andgotbacktomynormalphoneusagehabits.Itdidn’tmakemeavoidusinganyspecificapps."
• UnderstandingandTruston-deviceFeatureExtraction:Understandingofhowfacial
featureswereextractedvaried,withsomeparticipantsexpressingadesireformoreinfor-
mation.Trustwasgenerallyhighforthosewhofeltadequatelyinformed.Forexample,P30
affirmed,"Itrustitsinceitwasmadeclearbytheresearchersthatthiswasthecase.Ithink
asfarastrustatthedatacollection,thisisagreatapproach.Aslongastheimagedataisnot
leavingmyphone,Idonothaveanyissueswithit.".P35showedincreasedcomfortlevelwith
usingtheirphoneovertime,noting"ThiswasthemainreasonwhyIbecamemoreandmore
comfortableusingmyphoneaftertheinitialperiod.Ithinkthiswasthekeyfeaturethatmade
mestickwiththestudyuntiltheend.".P23advocatedforamechanismtocreateaprivate
repository,noting"Createaprivaterepositoryandalsoalertwhenthedataisaccessedwith
providingthereasonandbywhomitwasaccessed"–regardingtheownershipandcontrolof
theirpersonaldata.
• Long-termAcceptance:Perceptionsoftheappimprovedovertimeformanyparticipants.
Regardingsuggestedimprovements,P8recommended,"Iwouldmaketheactivationnotso
randombutintimedintervals" callingforenhancedusercontroloverthedatacollection
processes.AsimilarsentimentwasechoedbyP35,"Givinguserspersonalizationoptionsfor
whichappstoexcludefordatacollectionwouldbebeneficial.Someusersmaypreferexcluding
certainappsfrombeingmonitored,andIthinkhavingthisoptionwouldincreaseuseracceptance."
24FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
7.4 PotentialtoIntegrateAffectiveandCognitiveInferencesFromFacialBehavior
MarkerswithConversationalArtificialIntelligence(CAI)
As previous research has confirm that understanding facial behavior primitives can enhance
long-termsolutionsformanagingdepressionandprovideinsightsintoemotionalcommunication
[42],aggressionandnegativeaffectrecognition[34],psychologicaldistress[89],andautomatic
thoughtsperceptionduringcognitivebehavioraltherapy(CBT)[82,83],inferencesdrawnfrom
facialbehaviorprimitivesinourstudycouldenablemoreeffectiveaffectiveinteractions[18,47]
with a virtual CBT agent [82, 83], depending on an individual’s emotional and cognitive state.
Suchanapproachaimstocreatemoreaffect-sensitivemultimodalhuman–computerinteractions
[73],enhancingthehuman-likequalities,effectiveness,andefficiencyofvirtualagents.Real-time
interpretation of complex mental states from facial expressions and head gestures, indicating
concentration,fatigue,disagreement,interest,contemplation,uncertainty,andmore[26],could
providecontextualaffectdatatovirtualagents,facilitatingmorefluidinteractions.Thisaspectis
oftenlackingintext-basedandavatar-basedtherapyagents.
Inadditiontoitspotentialindepressiondetection,facialbehaviorprimitivescouldfindapplication
in real-time intoxication detection, as demonstrated in previous work on drunk face detection
usinganofflinedataset[65].Byincorporatingsuchcapabilities,theFacePsysystemcouldbroaden
its utilityand usecases for real-worldapplications. Moreover,the system’s applicabilitycould
extend to the detection of other forms of intoxication, such as marijuana intoxication [4, 16],
whereobservablefacialmuscleandheadmovementsareindicativeofpsychomotorretardation
andagitation.Recentdevelopments,likethecreationofanaugmentedrealityfeedbacksystem
forfacialparalysisinamobilesetting[7],hintatthepotentialutilityoftheFacePsysystemin
similardeployments.Beyondhealth-relatedapplications,theFacePsysystem’susefulnesscouldbe
exploredinmonitoringstudentengagementinsmartphone-basededucationorvirtualclasses[45]
whereinstructorscanadjustlecturematerialsbasedonstudents’behavioralfeedback.Indicators
suchasconcentration,interest,oruncertaintyarecrucialforenhancingclassroomengagementand
deliveringhigh-qualityeducation.TheFacePsysystemrepresentsasignificantstepintherealmof
mobileaffectsensinginhuman-computerinteraction,withfar-reachingimplicationsformental
healthwithvirtualtherapysessions,substanceabusedetection,andeducationalengagement.
8 LIMITATIONANDFUTUREWORK
Eventhoughwewereabletogetvaluableinsightsaboutmodelingdepressionandtheproposed
subsetoffacialandphysiologicalsignals,therearestillimprovementstobemadeforthissystem
tobeapplicableinclinicalsettings.Althoughwesuccessfullybuiltapopulationmodeltodetect
depression,however,theremightbeindividualpatternsthatourpopulationmodelcannotcapture,
andthusmaylimitthegeneralizabilityofourmodel.Inourfuturework,wewillcollectalarger
datasetperparticipantandinvestigatetheuseofmorepersonalizedindividualmodels.Whilewe
onlyregister11categoriesofappusetherecouldbemorecategoriesofappusethatcouldworkas
thebestavenuefordatacollection,furtherresearchshouldexamineifsuchcategoriesexist.The
currentlimitationofourapp,FacePsy,liesinitsinadvertenttriggeringofdatacollectionduring
intra-appnavigation,suchasmovingbetweenpageswithinthesameapp,leadingtomultipledata
capturesinasinglesession.Infuturework,weplantorefinetheapp’sarchitecturetodiscernand
limitdatacollectiontosignificantuserinteractions,therebyenhancingtheefficiencyandrelevance
ofthedatacollectionprocess.Furthermore,inourfuturework,wewanttointegratethepupillary
responsemeasurementmodule[48]inourprocessingpipelineforin-appmeasurementofpupillary
responsebyusingAndroidNativelibraries.Inaddition,weaimtoenrichFacePsybyintegrating
itwithsystemslikeAWAREandFitbit,combiningrichemotionalsignalsextractedfromvisual
25MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
datawithotherdatasuchasGPS,heartrate,andEDA.Wealsoplantoaddcontextuallayers,like
categorizingappsthattriggerdatacollectioninmodeldevelopmentwhichwecollectaspartofthe
FacePsytriggeringmechanism,todeepentheunderstandingoffacialbehaviorincontext.
Ourdepressionlabelingstrategymayraiseconcernsregardingthedata’sclarityandcharacter-
istics:Eachsessionrepresentsauniquedatapointwithspecificfeatures.It’simportanttonote
thatasingledaycouldencompassmultiplesessions.However,eachsessionlinkedtoaparticular
userwillpredictthedifferentdepressiveepisodelabelsspanovertwoweeksobservationperiod.
Thismethodologypresentstwopotentialchallenges.Firstly,thevariabilitybetweensessionsmight
makeitchallengingtoidentifyaconsistentpattern,whichcouldaffecttheaccuracyofpredicting
depressiveepisodes.Secondly,ifsessionsappeartoohomogenous,itcouldsuggestthatthemodel
mightbedetectingimplicitusercharacteristicsratherthantheiractualriskofdepression.
9 CONCLUSION
Depressionisamajormentalhealthdisorder.Assuch,detectingdepressioncanhavesignificant
impactsacrossseveraldomains.Towardsthisgoal,weproposeanaffectivemobilesystemthat
allows us to collect facial behavior primitives from faces by opportunistically capturing user
facesbyobservingtheuserinteractionwiththeirphoneinanaturalisticsetting.Tothisend,we
buildadepressiveepisodepredictionmodelthatachieves81%ofAUROC.Ourregressionmodel
that estimates PHQ-9 scores reached a moderate level of accuracy, exhibiting an MAE of 3.08.
Basedontheresultsfromourcross-validation,wefoundourmodelproducesreliableperformance
fromseveralweeksofdatatodetectdepressiveepisodes.Wehighlightkeybehaviorprimitives
differentiatingdepressiveandnon-depressiveepisodesandusecasescenariosregardinghowthe
system could be applicable in detecting mental and neurological disorders for researchers and
stakeholders.Lastly,wediscussprivacyandethicalconsiderationsindeployingsuchasystem.
10 ACKNOWLEDGMENTS
Wesincerelythankourresearchvolunteer,ShahnajLaila,forherkindassistanceinconducting
thefeasibilitystudy.Wearealsogratefultotheparticipantswhogenerouslyagreedtosharetheir
facialbehaviordataforthisstudy.
REFERENCES
[1] SaeedAbdullahandTanzeemChoudhury.2018.Sensingtechnologiesformonitoringseriousmentalillnesses.IEEE
MultiMedia25,1(2018),61–75.
[2] TakashiAbe.2023.PERCLOS-basedtechnologiesfordetectingdrowsiness:currentevidenceandfuturedirections.
SleepAdvances4,1(2023),zpad006.
[3] SharifaAlghowinem,RolandGoecke,MichaelWagner,GordonParkerx,andMichaelBreakspear.[n.d.].HeadPose
andMovementAnalysisasanIndicatorofDepression.In2013HumaineAssociationConferenceonAffectiveComputing
andIntelligentInteraction(2013-09).283–288. https://doi.org/10.1109/ACII.2013.53ISSN:2156-8111.
[4] SangWonBae,TammyChung,RahulIslam,BrianSuffoletto,JiamengDu,SerimJang,YuukiNishiyama,Raghu
Mulukutla,andAnindDey.2021.Mobilephonesensor-baseddetectionofsubjectivecannabisintoxicationinyoung
adults:Afeasibilitystudyinreal-worldsettings.Drugandalcoholdependence228(2021),108972.
[5] RebeccaBalebakoandLorrieCranor.2014.Improvingappprivacy:Nudgingappdeveloperstoprotectuserprivacy.
IEEESecurity&Privacy12,4(2014),55–58.
[6] TadasBaltrusaitis,AmirZadeh,YaoChongLim,andLouis-PhilippeMorency.2018.Openface2.0:Facialbehavior
analysistoolkit.In201813thIEEEinternationalconferenceonautomaticface&gesturerecognition(FG2018).IEEE,
59–66.
[7] GiulianaBarriosDell’OlioandMishaSra.2021.FaraPy:AnAugmentedRealityFeedbackSystemforFacialParalysis
usingActionUnitIntensityEstimation.InThe34thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology.
1027–1038.
[8] AlexandreBenoitandAliceCaplier.2005.Hypovigilenceanalysis:openorclosedeyeormouth?Blinkingoryawning
frequency?.InIEEEConferenceonAdvancedVideoandSignalBasedSurveillance,2005.IEEE,207–212.
26FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
[9] DonaldTCampbellandJulianCStanley.2015.Experimentalandquasi-experimentaldesignsforresearch.Ravenio
books.
[10] ConstantinoÁlvarezCasado,ManuelLageCañellas,andMiguelBordalloLópez.2023.DepressionRecognitionusing
RemotePhotoplethysmographyfromFacialVideos.IEEETransactionsonAffectiveComputing(2023).
[11] NiteshVChawla,KevinWBowyer,LawrenceOHall,andWPhilipKegelmeyer.2002.SMOTE:syntheticminority
over-samplingtechnique.Journalofartificialintelligenceresearch16(2002),321–357.
[12] IwonaChelminski,FRichardFerraro,ThomasVPetros,andJosephJPlaud.1999.Ananalysisofthe“eveningness–
morningness”dimensionin“depressive”collegestudents.Journalofaffectivedisorders52,1-3(1999),19–29.
[13] QianCheng,WuhongWang,XiaobeiJiang,ShanyiHou,andYongQin.2019.Assessmentofdrivermentalfatigue
usingfaciallandmarks.IEEEAccess7(2019),150423–150434.
[14] YuliaEChentsova-Dutton,JeanneLTsai,andIanHGotlib.2010.Furtherevidencefortheculturalnormhypothesis:
positiveemotionindepressedandcontrolEuropeanAmericanandAsianAmericanwomen.CulturalDiversityand
EthnicMinorityPsychology16,2(2010),284.
[15] PrernaChikersal,AfsanehDoryab,MichaelTumminia,DaniellaKVillalba,JanineMDutcher,XinwenLiu,Sheldon
Cohen,KaseyGCreswell,JenniferMankoff,JDavidCreswell,etal.2021.Detectingdepressionandpredictingits
onsetusinglongitudinalsymptomscapturedbypassivesensing:amachinelearningapproachwithrobustfeature
selection.ACMTransactionsonComputer-HumanInteraction(TOCHI)28,1(2021),1–41.
[16] TammyChung,SangWonBae,Eun-YoungMun,BrianSuffoletto,YuukiNishiyama,SerimJang,andAnindKDey.
2020.Mobileassessmentofacuteeffectsofmarijuanaoncognitivefunctioninginyoungadults:observationalstudy.
JMIRmHealthanduHealth8,3(2020),e16240.
[17] JeffreyFCohn,TomasSimonKruez,IainMatthews,YingYang,MinhHoaiNguyen,MargaraTejeraPadilla,Feng
Zhou,andFernandoDelaTorre.2009. Detectingdepressionfromfacialactionsandvocalprosody.In20093rd
InternationalConferenceonAffectiveComputingandIntelligentInteractionandWorkshops.IEEE,1–7.
[18] CristinaConati,StacyMarsella,andAnaPaiva.2005.Affectiveinteractions:thecomputerintheaffectiveloop.In
Proceedingsofthe10thinternationalconferenceonIntelligentuserinterfaces.7–7.
[19] SunnyConsolvo,KatherineEveritt,IanSmith,andJamesALanday.2006.Designrequirementsfortechnologiesthat
encouragephysicalactivity.InProceedingsoftheSIGCHIconferenceonHumanFactorsincomputingsystems.457–466.
[20] TamaraDenning,ZakariyaDehlawi,andTadayoshiKohno.2014.Insituwithbystandersofaugmentedrealityglasses:
Perspectivesonrecordingandprivacy-mediatingtechnologies.InProceedingsoftheSIGCHIConferenceonHuman
FactorsinComputingSystems.2377–2386.
[21] AbhinavDhallandRolandGoecke.2015.Atemporallypiece-wisefishervectorapproachfordepressionanalysis.In
2015Internationalconferenceonaffectivecomputingandintelligentinteraction(ACII).IEEE,255–259.
[22] SathyanarayananDoraiswamy,AmitAbraham,RavinderMamtani,andSohailaCheema.2020. UseofTelehealth
DuringtheCOVID-19Pandemic:ScopingReview.JMedInternetRes22,12(1Dec2020),e24087.
[23] NaomiEigbe,TadasBaltrusaitis,Louis-PhilippeMorency,andJohnPestian.2018.Towardvisualbehaviormarkersof
suicidalideation.In201813thIEEEInternationalConferenceonAutomaticFace&GestureRecognition(FG2018).IEEE,
530–534.
[24] PaulEkman.2003.Emotionsrevealed:recognizingfacesandfeelingstoimprovecommunicationandemotionallife.
NewYork.NY:Timesbooks(2003).
[25] PaulEkmanandWallaceVFriesen.1978.Facialactioncodingsystem.EnvironmentalPsychology&NonverbalBehavior
(1978).
[26] RanaElKalioubyandPeterRobinson.2005.Real-timeinferenceofcomplexmentalstatesfromfacialexpressionsand
headgestures.InReal-timevisionforhuman-computerinteraction.Springer,181–200.
[27] HeinerEllgring.2007.Non-verbalcommunicationindepression.CambridgeUniversityPress.
[28] ItirOnalErtugrul,JeffreyFCohn,LászlóAJeni,ZhengZhang,LijunYin,andQiangJi.2019. Cross-domainau
detection:Domains,learningapproaches,andmeasures.In201914thIEEEInternationalConferenceonAutomaticFace
&GestureRecognition(FG2019).IEEE,1–8.
[29] AsmaAhmadFarhan,ChaoqunYue,ReynaldoMorillo,ShwetaWare,JinLu,JinboBi,JayeshKamath,Alexander
Russell,AthanasiosBamis,andBingWang.2016.Behaviorvs.introspection:refiningpredictionofclinicaldepression
viasmartphonesensingdata.In2016IEEEwirelesshealth(WH).IEEE,1–8.
[30] AdriennePorterFelt,SergeEgelman,andDavidWagner.2012.I’vegot99problems,butvibrationain’tone:asurvey
ofsmartphoneusers’concerns.InProceedingsofthesecondACMworkshoponSecurityandprivacyinsmartphones
andmobiledevices.33–44.
[31] JialingFeng,ZhexiaoGuo,JunWang,andGuoDan.2020. Usingeyeaspectratiotoenhancefastandobjective
assessmentoffacialparalysis.Computationalandmathematicalmethodsinmedicine2020(2020).
[32] DenzilFerreira,VassilisKostakos,andAnindKDey.2015. AWARE:mobilecontextinstrumentationframework.
FrontiersinICT2(2015),6.
27MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
[33] Hans-UlrichFisch,SiegfriedFrey,andHans-PeterHirsbrunner.1983.Analyzingnonverbalbehaviorindepression.
Journalofabnormalpsychology92,3(1983),307.
[34] SiskaFitrianieandIuliaLefter.2023. OnHeadMotionforRecognizingAggressionandNegativeAffectduring
SpeakingandListening.InProceedingsofthe25thInternationalConferenceonMultimodalInteraction.455–464.
[35] WolfgangGaebelandWolfgangWölwer.2004. Facialexpressivityinthecourseofschizophreniaanddepression.
Europeanarchivesofpsychiatryandclinicalneuroscience254(2004),335–342.
[36] MihaiGavrilescuandNicolaeVizireanu.2019.Predictingdepression,anxiety,andstresslevelsfromvideosusingthe
facialactioncodingsystem.Sensors19,17(2019),3693.
[37] Jean-GuidoGehrickeandDavidShapiro.2000. Reducedfacialexpressionandsocialcontextinmajordepression:
discrepanciesbetweenfacialmuscleactivityandself-reportedemotion.PsychiatryResearch95,2(2000),157–167.
[38] SimonGilbody,DavidRichards,StephenBrealey,andCatherineHewitt.2007.Screeningfordepressioninmedical
settingswiththePatientHealthQuestionnaire(PHQ):adiagnosticmeta-analysis.Journalofgeneralinternalmedicine
22(2007),1596–1602.
[39] JeffreyMGirard,JeffreyFCohn,MohammadHMahoor,SeyedmohammadMavadati,andDeanPRosenwald.2013.
Socialriskanddepression:Evidencefrommanualandautomaticfacialexpressionanalysis.In201310thIEEE
InternationalConferenceandWorkshopsonAutomaticFaceandGestureRecognition(FG).IEEE,1–8.
[40] JeffreyMGirard,JeffreyFCohn,MohammadHMahoor,SMohammadMavadati,ZakiaHammal,andDeanP
Rosenwald.2014.Nonverbalsocialwithdrawalindepression:Evidencefrommanualandautomaticanalyses.Image
andvisioncomputing32,10(2014),641–647.
[41] Google.[n.d.].MLKit. https://developers.google.com/ml-kit
[42] ZakiaHammalandJeffreyFCohn.2014.Intra-andinterpersonalfunctionsofheadmotioninemotioncommunication.
InProceedingsofthe2014WorkshoponRoadmappingtheFutureofMultimodalInteractionResearchincludingBusiness
OpportunitiesandChallenges.19–22.
[43] Qian-BeiHong,Chung-HsienWu,Ming-HsiangSu,andChia-ChengChang.2019. ExploringMacroscopicand
MicroscopicFluctuationsofElicitedFacialExpressionsforMoodDisorderClassification. IEEETransactionson
AffectiveComputing12,4(2019),989–1001.
[44] JinHuangandCharlesXLing.2005.UsingAUCandaccuracyinevaluatinglearningalgorithms.IEEETransactions
onknowledgeandDataEngineering17,3(2005),299–310.
[45] MohammadRahulIslamandSangWonBae.2023.MicroFlow:AdvancingAffectiveStatesDetectioninLearningThrough
Micro-Expressions.TechnicalReport.EasyChair.
[46] RahulIslam,KaranAhuja,SandipKarmakar,andFerdousBarbhuiya.2016.SenTion:Aframeworkforsensingfacial
expressions.arXivpreprintarXiv:1608.04489(2016).
[47] RahulIslamandSangWonBae.2023. RevolutionizingMentalHealthSupport:AnInnovativeAffectiveMobile
FrameworkforDynamic,Proactive,andContext-AdaptiveConversationalAgents.Ubicomp,GenAI4PCSymposium
(2023).
[48] RahulIslamandSangWonBae.2024.PupilSense:DetectionofDepressiveEpisodesThroughPupillaryResponsein
theWild.InternationalConferenceonActivityandBehaviorComputing(2024).
[49] AsimJan,HongyingMeng,YonaFalinieBintiAGaus,andFanZhang.2017.Artificialintelligentsystemforautomatic
depressionlevelanalysisthroughvisualandvocalexpressions.IEEETransactionsonCognitiveandDevelopmental
Systems10,3(2017),668–680.
[50] SJennifer,BenjaminRBrady,MohabMIbrahim,KatherineEHerder,JessicaSWallace,AlyssaRPadilla,andToddW
Vanderah.2024.Co-occurrenceofchronicpainandanxiety/depressionsymptomsinUSadults:prevalence,functional
impacts,andopportunities.Pain165,3(2024),666–673.
[51] JyotiJoshi,RolandGoecke,GordonParker,andMichaelBreakspear.2013. Canbodyexpressionscontributeto
automaticdepressionanalysis?.In201310thIEEEInternationalConferenceandWorkshopsonAutomaticFaceand
GestureRecognition(FG).IEEE,1–7.
[52] PatrickGageKelley,JoannaBresee,LorrieFaithCranor,andRobertWReeder.2009.A"nutritionlabel"forprivacy.
InProceedingsofthe5thSymposiumonUsablePrivacyandSecurity.1–12.
[53] RonaldCKessler,PatriciaBerglund,OlgaDemler,RobertJin,KathleenRMerikangas,andEllenEWalters.2005.Life-
timeprevalenceandage-of-onsetdistributionsofDSM-IVdisordersintheNationalComorbiditySurveyReplication.
Archivesofgeneralpsychiatry62,6(2005),593–602.
[54] RonaldCKessler,CindyLFoster,WilliamBSaunders,andPaulEStang.1995.Socialconsequencesofpsychiatric
disorders,I:Educationalattainment.Americanjournalofpsychiatry152,7(1995),1026–1032.
[55] DimitriosKolliasandStefanosZafeiriou.2021.Affectanalysisin-the-wild:Valence-arousal,expressions,actionunits
andaunifiedframework.arXivpreprintarXiv:2103.15792(2021).
[56] XinruKong,YanYao,CuiyingWang,YuangengWang,JingTeng,andXianghuaQi.2022.AutomaticIdentificationof
DepressionUsingFacialImageswithDeepConvolutionalNeuralNetwork.MedicalScienceMonitor:International
28FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
MedicalJournalofExperimentalandClinicalResearch28(2022),e936409–1.
[57] DawnCKroencke,SharonGLynch,andDouglasRDenney.2000. Fatigueinmultiplesclerosis:relationshipto
depression,disability,anddiseasepattern.MultipleSclerosisJournal6,2(2000),131–136.
[58] KurtKroenke,RobertLSpitzer,andJanetBWWilliams.2001. ThePHQ-9:validityofabriefdepressionseverity
measure.Journalofgeneralinternalmedicine16,9(2001),606–613.
[59] KurtKroenke,RobertLSpitzer,JanetBWWilliams,andBerndLöwe.2010.Thepatienthealthquestionnairesomatic,
anxiety,anddepressivesymptomscales:asystematicreview.Generalhospitalpsychiatry32,4(2010),345–359.
[60] EugeneLaksana,TadasBaltrušaitis,Louis-PhilippeMorency,andJohnPPestian.2017.Investigatingfacialbehavior
indicatorsofsuicidalideation.In201712thIEEEInternationalConferenceonAutomaticFace&GestureRecognition(FG
2017).IEEE,770–777.
[61] ScottALaurenzo,RandyKardon,JohannesLedolter,PieterPoolman,AshleyMSchumacher,JamesBPotash,JanM
Full,OliviaRice,AnnaKetcham,ColeStarkey,etal.2016.Pupillaryresponseabnormalitiesindepressivedisorders.
Psychiatryresearch246(2016),492–499.
[62] IanLi,AnindDey,andJodiForlizzi.2010.Astage-basedmodelofpersonalinformaticssystems.InProceedingsofthe
SIGCHIconferenceonhumanfactorsincomputingsystems.557–566.
[63] CaioBezerraSoutoMaior,MárcioJosédasChagasMoura,JoãoMateusMarquesSantana,andIsisDidierLins.2020.
Real-timeclassificationforautonomousdrowsinessdetectionusingeyeaspectratio.ExpertSystemswithApplications
158(2020),113505.
[64] DavidMatsumotoandHyiSungHwang.2011.Evidencefortrainingtheabilitytoreadmicroexpressionsofemotion.
Motivationandemotion35(2011),181–191.
[65] VineetMehta,SaiSrinadhuKatta,DevendraPratapYadav,andAbhinavDhall.2019. DIF:Datasetofperceived
intoxicatedfacesfordrunkpersonidentification.In2019InternationalConferenceonMultimodalInteraction.367–374.
[66] SubigyaNepal,ArvindPillai,WeichenWang,TessGriffin,AmandaCCollins,MichaelHeinz,DamienLekkas,
ShayanMirjafari,MatthewNemesure,GeorgePrice,NicholasJacobson,andAndrewCampbell.2024.MoodCapture:
DepressionDetectionusingIn-the-WildSmartphoneImages.InProceedingsoftheCHIConferenceonHumanFactors
inComputingSystems(CHI’24).AssociationforComputingMachinery,NewYork,NY,USA,Article996,18pages.
[67] MatthewKNock,IrvingHwang,NancyASampson,andRonaldCKessler.2010. Mentaldisorders,comorbidity
andsuicidalbehavior:resultsfromtheNationalComorbiditySurveyReplication.Molecularpsychiatry15,8(2010),
868–876.
[68] HeatherLO’BrienandElaineGToms.2008.Whatisuserengagement?Aconceptualframeworkfordefininguser
engagementwithtechnology. JournaloftheAmericansocietyforInformationScienceandTechnology59,6(2008),
938–955.
[69] KennedyOpokuAsare,IsaacMoshe,YannikTerhorst,JulioVega,SimoHosio,HaraldBaumeister,LauraPulkki-Råback,
andDenzilFerreira.2022.Moodratingsanddigitalbiomarkersfromsmartphoneandwearabledatadifferentiates
andpredictsdepressionstatus::Alongitudinaldataanalysis.(2022).
[70] KennedyOpokuAsare,YannikTerhorst,JulioVega,EllaPeltonen,EemilLagerspetz,andDenzilFerreira.2021.
Predictingdepressionfromsmartphonebehavioralmarkersusingmachinelearningmethods,hyperparameter
optimization,andfeatureimportanceanalysis:exploratorystudy.JMIRmHealthanduHealth9,7(2021),e26540.
[71] WorldHealthOrganizationetal.2003.Investinginmentalhealth.(2003).
[72] M.O’BrienandF.McNicholas.2020. TheuseoftelepsychiatryduringCOVID-19andbeyond. IrishJournalof
PsychologicalMedicine37,4(2020),250–255.
[73] MajaPanticandLeonJMRothkrantz.2003. Towardanaffect-sensitivemultimodalhuman-computerinteraction.
Proc.IEEE91,9(2003),1370–1390.
[74] PaolaPedrelli,SzymonFedor,AsmaGhandeharioun,EstherHowe,DawnFIonescu,DarianBhathena,LaurenB
Fisher,CristinaCusin,MarenNyer,AlbertYeung,etal.2020. Monitoringchangesindepressionseverityusing
wearableandmobilesensors.Frontiersinpsychiatry11(2020),584711.
[75] RosalindWPicard.1999.Affectivecomputingforhci..InHCI(1).Citeseer,829–833.
[76] MaheshKrishnanandaPrabhuandDineshBabuJayagopi.2017.Realtimemultimodalemotionrecognitionsystem
usingfaciallandmarksandhandoverfacegestures.Int.J.ofMachineLearningandComputing7,2(2017),30–34.
[77] Arcady A Putilov. 2017. State-and trait-like variation in morning and evening components of morningness–
eveningnessinwinterdepression.Nordicjournalofpsychiatry71,8(2017),561–569.
[78] LawrenceIanReed,MichaelASayette,andJeffreyFCohn.2007. Impactofdepressiononresponsetocomedy:a
dynamicfacialcodinganalysis.Journalofabnormalpsychology116,4(2007),804.
[79] BabetteRenneberg,KatrinHeyn,RitaGebhard,andSilkeBachmann.2005.Facialexpressionofemotionsinborderline
personalitydisorderanddepression.Journalofbehaviortherapyandexperimentalpsychiatry36,3(2005),183–196.
[80] PekkaRuotsalainenandBerndBlobel.2020.Healthinformationsystemsinthedigitalhealthecosystem—problems
andsolutionsforethics,trustandprivacy. Internationaljournalofenvironmentalresearchandpublichealth17,9
29MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
(2020),3006.
[81] AvenSamareh,YanJin,ZhangyangWang,XiangyuChang,andShuaiHuang.2018.Detectdepressionfromcommu-
nication:howcomputervision,signalprocessing,andsentimentanalysisjoinforces.IISETransactionsonHealthcare
SystemsEngineering8,3(2018),196–208.
[82] KazuhiroShidara,HirokiTanaka,HiroyoshiAdachi,DaisukeKanayama,YukakoSakagami,TakashiKudo,andSatoshi
Nakamura.2020. Analysisofmoodchangesandfacialexpressionsduringcognitivebehaviortherapythrougha
virtualagent.InCompanionPublicationofthe2020InternationalConferenceonMultimodalInteraction.477–481.
[83] KazuhiroShidara,HirokiTanaka,HiroyoshiAdachi,DaisukeKanayama,YukakoSakagami,TakashiKudo,andSatoshi
Nakamura.2022.Automaticthoughtsandfacialexpressionsincognitiverestructuringwithvirtualagents.Frontiers
inComputerScience4(2022),762424.
[84] DeniseMSloan,MiltonEStrauss,StuartWQuirk,andMarthaSajatovic.1997.Subjectiveandexpressiveemotional
responsesindepression.Journalofaffectivedisorders46,2(1997),135–141.
[85] DeniseMSloan,MiltonEStrauss,andKatherineLWisner.2001.Diminishedresponsetopleasantstimulibydepressed
women.Journalofabnormalpsychology110,3(2001),488.
[86] SiyangSong,ShashankJaiswal,LinlinShen,andMichelValstar.[n.d.]. SpectralRepresentationofBehaviour
PrimitivesforDepressionAnalysis.([n.d.]),1–1. ConferenceName:IEEETransactionsonAffectiveComputing.
[87] SiyangSong,ShashankJaiswal,LinlinShen,andMichelValstar.2020.SpectralRepresentationofBehaviourPrimitives
forDepressionAnalysis.IEEETransactionsonAffectiveComputing(2020).
[88] SiyangSong,EnriqueSánchez-Lozano,ManiKumarTellamekala,LinlinShen,AlanJohnston,andMichelValstar.2019.
Dynamicfacialmodelsforvideo-baseddimensionalaffectestimation.InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVisionWorkshops.0–0.
[89] GiotaStratouandLouis-PhilippeMorency.2017.MultiSense—Context-awarenonverbalbehavioranalysisframework:
Apsychologicaldistressusecase.IEEETransactionsonAffectiveComputing8,2(2017),190–203.
[90] AnjaStuhrmann,ThomasSuslow,andUdoDannlowski.2011. Facialemotionprocessinginmajordepression:a
systematicreviewofneuroimagingfindings.Biologyofmood&anxietydisorders1,1(2011),1–17.
[91] VincentW.-S.Tseng,SaeedAbdullah,JeanCosta,andTanzeemChoudhury.[n.d.].AlertnessScanner:whatdoyour
pupilstellaboutyouralertness.InProceedingsofthe20thInternationalConferenceonHuman-ComputerInteraction
withMobileDevicesandServices(Barcelona,Spain,2018-09-03)(MobileHCI’18).AssociationforComputingMachinery,
1–11.
[92] VincentW-STseng,SaeedAbdullah,JeanCosta,andTanzeemChoudhury.2018.AlertnessScanner:whatdoyour
pupilstellaboutyouralertness.InProceedingsofthe20thInternationalConferenceonHuman-ComputerInteraction
withMobileDevicesandServices.1–11.
[93] MichelValstar,BjörnSchuller,KirstySmith,TimurAlmaev,FlorianEyben,JarekKrajewski,RoddyCowie,and
MajaPantic.2014.Avec2014:3ddimensionalaffectanddepressionrecognitionchallenge.InProceedingsofthe4th
internationalworkshoponaudio/visualemotionchallenge.3–10.
[94] MichelValstar,BjörnSchuller,KirstySmith,FlorianEyben,BihanJiang,SanjayBilakhia,SebastianSchnieder,Roddy
Cowie,andMajaPantic.2013.Avec2013:thecontinuousaudio/visualemotionanddepressionrecognitionchallenge.
InProceedingsofthe3rdACMinternationalworkshoponAudio/visualemotionchallenge.3–10.
[95] JessieMVanSwearingen,JeffreyFCohn,andAnuBajaj-Luthra.1999.Specificimpairmentofsmilingincreasesthe
severityofdepressivesymptomsinpatientswithfacialneuromusculardisorders.Aestheticplasticsurgery23(1999),
416–423.
[96] SarahCollierVillaume,ShantingChen,andEmmaKAdam.2023. Agedisparitiesinprevalenceofanxietyand
depressionamongUSadultsduringtheCOVID-19pandemic.JAMAnetworkopen6,11(2023),e2345073–e2345073.
[97] PhilipSWang,GregoryESimon,JerryAvorn,FranciscaAzocar,EvetteJLudman,JoyceMcCulloch,MariaZPetukhova,
andRonaldCKessler.2007.Telephonescreening,outreach,andcaremanagementfordepressedworkersandimpact
onclinicalandworkproductivityoutcomes:arandomizedcontrolledtrial.Jama298,12(2007),1401–1411.
[98] RuiWang,AndrewTCampbell,andXiaZhou.2015.Usingopportunisticfaceloggingfromsmartphonetoinfermental
health:challengesandfuturedirections.InAdjunctProceedingsofthe2015ACMInternationalJointConferenceon
PervasiveandUbiquitousComputingandProceedingsofthe2015ACMInternationalSymposiumonWearableComputers.
683–692.
[99] RuiWang,FanglinChen,ZhenyuChen,TianxingLi,GabriellaHarari,StefanieTignor,XiaZhou,DrorBen-Zeev,and
AndrewTCampbell.2014. StudentLife:assessingmentalhealth,academicperformanceandbehavioraltrendsof
collegestudentsusingsmartphones.InProceedingsofthe2014ACMinternationaljointconferenceonpervasiveand
ubiquitouscomputing.3–14.
[100] FangZhang,JingjingSu,LeiGeng,andZhitaoXiao.2017.Driverfatiguedetectionbasedoneyestaterecognition.In
2017InternationalConferenceonMachineVisionandInformationTechnology(CMVIT).IEEE,105–110.
30FacePsy:AnOpen-SourceAffectiveMobileSensingSystem–AnalyzingFacialBehaviorandHeadGestureforDepression
DetectioninNaturalisticSettings MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia
[101] XiuzhuangZhou,KaiJin,YuanyuanShang,andGuodongGuo.2018.Visuallyinterpretablerepresentationlearning
fordepressionrecognitionfromfacialimages.IEEETransactionsonAffectiveComputing11,3(2018),542–552.
A SURVEY
A.1 PHQ-9
Participants were asked: "Over the last two weeks, how often have you been bothered by the
followingproblems?"Thisquestionnaireispartofthestandardassessmenttogaugetheseverityof
depressivesymptoms.BelowisthePHQ-9questionnaire(Table9)usedinthestudy:
Table9. PHQ-9QuestionnaireItems
No. Question
1 Littleinterestorpleasureindoingthings
2 Feelingdown,depressed,orhopeless
3 Troublefallingorstayingasleep,orsleepingtoomuch
4 Feelingtiredorhavinglittleenergy
5 Poorappetiteorovereating
6 Feelingbadaboutyourself-orthatyouareafailureorhaveletyourselforyourfamilydown
7 Troubleconcentratingonthings,suchasreadingthenewspaperorwatchingtelevision
8 Movingorspeakingsoslowlythatotherpeoplecouldhavenoticed.Ortheopposite-beingso
fidgetyorrestlessthatyouhavebeenmovingaroundalotmorethanusual
9 Thoughtsthatyouwouldbebetteroffdead,orofhurtingyourself
A.2 StudyFeedback
Table10liststheuserfeedbackquestionsadministeredattheendofthestudytogaugeparticipants’
perceptionsoftheFacePsyapp,focusingonaspectsofconsent,datacollectiontriggers,impact
onprivacy,understandingoffeatureextraction,andlong-termacceptance.Thequestionswere
designedtounderstandtheparticipants’experiencesthroughouttheirinteractionwiththeappand
togathersuggestionsforfutureimprovements.
31MobileHCI’24,Sept30–Oct03,2024,Melbourne,Australia IslamandBae
Table10. UserFeedbackQuestions
No. Question
1 WhenyoufirststartedusingtheFacePsyapp,whatwereyourinitialthoughtsaboutthefacial
datacollection,especiallywhenunlockingyourphoneorusingspecificapps?
2 Howwereyouinformedaboutthedatacollectionprocess,anddidyoufeelthattheconsent
processadequatelyaddressedyourconcernsaboutprivacyanddatausage?
3 Canyoudescribehowyoufeltthefirstfewtimestheappactivateduponunlockingyourphone
oropeningtriggerapps?Diditbecomemoreacceptableovertime,ordiditremainaconcern?
4 WerethereanyparticulartriggerappsthatmadeyoumoreuncomfortablewhentheFacePsy
appactivated?Howdidthisaffectyourusageofthoseapps?
5 Didyounoticeanychangesinyourphoneusagehabitsduetotheapp’sdatacollection
methods?Forexample,didyouuseyourphonelessfrequentlyoravoidcertainapps?
6 Whatareyourthoughtsontheappautomaticallydiscardingimagesafter20seconds?Didthis
featureinfluenceyourcomfortlevelwiththeongoingdatacollection?
7 Howwelldoyouunderstandtheprocessoffacialfeatureextractionbytheapp?Wasthere
enoughinformationprovidedaboutwhatdataisextractedandhowitisused?
8 Doyoutrustthatthefacialdatacollectedremainsonyourdeviceandisnotuploaded
elsewhere?Whatcouldincreaseyourtrustinthesystem’shandlingofyourdata?
9 HowhasyourperceptionoftheFacePsyappanditsdatacollectionpracticeschangedduring
thecourseofthestudy?
10 Whatimprovementsorchangeswouldyousuggestfortheapp,especiallyregardinguser
controloverdatacollectionandprivacy?
32