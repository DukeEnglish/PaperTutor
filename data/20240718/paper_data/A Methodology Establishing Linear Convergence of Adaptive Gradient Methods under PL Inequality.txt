A Methodology Establishing Linear Convergence
of Adaptive Gradient Methods under PL Inequality
KushalChakrabartia,* andMayankBaranwala,b
aTataConsultancyServicesResearch, Mumbai,India
bDepartmentofSystems&ControlEngineering, IndianInstitute ofTechnology, India
ORCID(KushalChakrabarti): https://orcid.org/0000-0002-6747-8709,ORCID(MayankBaranwal):
https://orcid.org/0000-0001-9354-2826
Abstract. Adaptive gradient-descent optimizers are the standard to gradient-descent. One such theoretical difference between these
choicefortrainingneuralnetworkmodels.Despitetheirfastercon- twoclassesofmethodsislinearconvergence.Specifically,gradient-
vergencethangradient-descentandremarkableperformanceinprac- descentanditsacceleratedvariants,suchastheNesterovaccelerated
tice, the adaptive optimizers are not as well understood as vanilla gradientorHeavy-Ballmethod,areknowntoexhibitlinearconver-
gradient-descent.Areasonisthatthedynamicupdateofthelearning genceforaclassofcostfunctions[16].Ontheotherhand,mostof
rate that helps in faster convergence of these methods also makes theadaptivegradientoptimizerslacksuchaguarantee.
their analysis intricate. Particularly, the simple gradient-descent Linear convergence guarantees of gradient-descent, its acceler-
method converges at a linearratefor a classof optimization prob- atedvariants, coordinate descent, and AdaGrad-Norm [22] (among
lems,whereasthepracticallyfaster adaptivegradient methodslack the adaptive gradient methods) have been proved for the class of
suchatheoreticalguarantee.ThePolyak-Łojasiewicz(PL)inequality smooth and possibly non-convex cost functions that satisfy the
istheweakestknownclass,forwhichlinearconvergenceofgradient- Polyak-Łojasiewicz (PL) inequality [10]. The PL inequality is the
descent and itsmomentum variants has been proved. Therefore, in weakest condition among others, such as strong convexity, essen-
thispaper,weprovethatAdaGradandAdam,twowell-knownadap- tial strong convexity, weak strong convexity, and restricted secant
tive gradient methods, converge linearly when the cost function is inequality,thatleadstolinearconvergence ofgradient-descent and
smooth and satisfies the PL inequality. Our theoretical framework itsacceleratedvariantstothesolutionof(1)[10].Theobjectivefunc-
followsasimpleandunifiedapproach,applicabletobothbatchand tions in standard machine learning problems like linear regression
stochastic gradients, which can potentially be utilized inanalyzing and logistic regression satisfy the PL inequality. For solving over-
linearconvergenceofothervariantsofAdam. parameterized non-linear equations, [13] establishes a relation be-
tweenPLinequalityandtheconditionnumberofthetangentkernel,
and argued that sufficiently wide neural networks generally satisfy
1 Introduction
the PL inequality. Motivated by linear convergence guarantees of
Inthispaper,weconsidertheproblemofminimizingapossiblynon- gradient-descent anditsaforementionedvariantsunder thePLcon-
convexobjectivefunctionf :Rd R, dition and the applicability of PL inequality on a set of machine
→
learning problems, we investigate linear convergence of AdaGrad
minf(x). (1)
andAdamunderthePLinequality.
x Rd
∈ Topresentourresults,wedefinethefollowingnotationsandmake
Amongotherapplications,non-convexoptimizationappearsintrain- asetofassumptionsasstatedbelow.Letthegradientoff evaluated
ingneuralnetworkmodels.Itisastandardpracticetouseadaptive at x Rd be denoted by f(x) Rd, and its i-th element be
gradientoptimizersfortrainingsuchmodels.Comparedtogradient- denote∈ dby if(x)foreach∇ dimensi∈ oni 1,...,d .
descent,thesemethodshavebeenobservedtoconvergefasteranddo ∇ ∈{ }
notrequirelinesearchtodeterminethelearningrates.
Assumption 1. The minimum f of f exists and is finite, i.e.,
AdaGrad [7] ispossibly the earliest adaptive gradient optimizer. ∗
ToaddressthegradientaccumulationprobleminAdaGrad,theAdam
|min
x
∈Rdf(x) |< ∞.
algorithm [11] was proposed. Adam and its variants have been Assumption2. f istwicedifferentiableoveritsdomainRd andis
widelyusedtotraindeepneuralnetworksinthepastdecade.Despite
L-smooth,i.e., L> 0suchthat f(x) f(y) L x y
thesuccessoftheseadaptivegradientoptimizers,welackanunder-
forallx,y
R∃d. k∇ −∇ k≤ k − k
standingofwhythesemethodsworksowellinpractice.Thetheory ∈
of convergence of theadaptive optimizershasnot beencompletely
Assumption 3. f satisfies the Polyak-Łojasiewicz (PL) inequality,
developed. Since these methods work better than simple gradient-
i.e., l>0suchthat 1 f(x) 2 l(f(x) f )forallx Rd.
descent or its momentum variants, it is natural to expect a similar ∃ 2k∇ k ≥ − ∗ ∈
or better convergence guarantee of adaptive optimizers compared
Assumption 3hasbeen justifiedinthepreceding paragraph. As-
CorrespondingAuthor.Email:chakrabarti.k@tcs.com. sumption 1-2 are standard in the literature of gradient-based opti-
∗
4202
luJ
71
]GL.sc[
1v92621.7042:viXramization.AnimplicationofAssumption2isthat[2] estimatex ofthesolutionof(1)andtwomomentestimatesµ and
k k
ν thatdeterminethelearningrate.Theseestimatesareupdatedas
k
L
f(y) f(x) (y x)⊤ f(x)+ x y 2, x,y Rd. (2)
− ≤ − ∇ 2 k − k ∀ ∈ µ
k+1,i
=β 1kµ k,i+(1 β 1k) if(x k), (4a)
− ∇
Next,wereviewtheAdaGrad[7]algorithmanditsexistingcon- ν k+1,i=β 2ν k,i+(1 β 2) if(x k)2, (4b)
− |∇ |
vergenceguarantees. AdaGradandAdamareiterativemethods. At µ
x =x h k+1,i . (4c)
eachiterationk=0,1,...,AdaGradmaintainsanestimatex kofthe k+1,i k,i − √ν k+1,i+ǫ
solutionof(1)andanauxiliarystatey thatdeterminesthelearning
k
rate.x kandy kareupdatedas Here,β 1k,β
2
[0,1)aretwoalgorithmparameters.Fornon-convex
∈
optimization in stochastic settings, [23, 8] have proved (1/√K)
y k+1,i =y k,i+ if(x k)2, (3a) convergence rate of Adam and a family of its variantsO when the
|∇ |
x k+1,i =x k,i −h √∇ y ki +f 1( ,x ik +) ǫ. (3b) s ct oo nc vh ea rs gt eic ncg era rd ai te en ots f a dr oe ubu ln ei -f lo or om ply algb oo ru in thd med s. w[9 it] hp Aro dv ae md -sO ty( l1 e/K up) -
date in its inner loop in the stochastic settings under PL inequal-
Here, h > 0 isa stepsize, and ǫ > 0 is a small-valued parameter ity and bounded stochastic gradients. Without the bounded gradi-
to avoid division by zero. The original AdaGrad paper [7] proves entsassumption,[24]proved (log(K)/√K)convergencerateof
O
(√K)regretboundforconvexf,whereKisthenumberofitera- Adamfornon-convexoptimizationinstochasticsettings.[19]proved
O
tions,assumingthestochasticgradientsandtheestimatesofminima a (logK) regret bound of theSAdamalgorithm, withν re-
k+1,i
O
areuniformlybounded. Under thesameboundedness assumptions, placing√ν
k+1,i
inthedenominator ofAdam(4c)andavanishing
forstronglyconvexf,AdaGrad’sregretboundanalysis[6]implya ǫ ,whenf isstronglyconvexandthegradientsandtheestimatesof
k
(log(K)/K) convergence rate [4]. Following [7], there are sev- minimaareuniformlybounded.[1]provedexponentialconvergence
O
eralworksontheconvergenceofAdaGrad.Thenotableamongthem ofacontinuous-timeversionofAdamunderthePLinequality.How-
include [12, 5, 20, 18]. Among them, [18] most recently proved ever,[1]assumesthatthecontinuous-timeversionofthemomentum
(1/K) convergence rate of AdaGrad for non-convex f, without parameters β and β converge to one as the stepsize h converge
1 2
O
theboundedgradientassumption.Tothebestofourknowledge,lin- to zero. Also, the exponential convergence guarantee in [1] does
ear convergence of AdaGrad has not been proved in the literature. not extend to discrete-time, as discussed by the authors. From the
Ontheotherhand,thegradient-descentalgorithmisknowntohave aboveliteraturereview,wenotethatalinearconvergenceofAdam
linear convergence when f issmooth and satisfiesthePLinequal- indiscrete-timehasnot beenproved. Weprovelinearconvergence
ity[10].So, we askthe question of whether theadaptive gradient- of the discrete-time Adam algorithm (4a)-(4c) in the deterministic
descentmethods,suchasAdaGradandAdam,canhaveguaranteed settingswhenf isL-smoothandsatisfiesthePLinequality.
linearconvergenceforthesameclassofoptimizationproblems. WeaimtopresentaunifiedproofsketchforAdaGradandAdam
AmongthevariantsofAdaGrad, [22]proveslinearconvergence inthispaper,whichcanpotentiallybeutilizedforvariantsofAdam
oftheAdaGrad-Norm(normversionofAdaGrad)inthestochastic inthefuture.Thus,ourconvergenceanalysesofAdaGradandAdam
settingsunderstrongconvexityoff andinthedeterministicsetting both follow a similar approach. It is well known that the difficulty
underPLinequality.TheAdaGrad-Normalgorithm,describedas inobtainingaconvergencerate,orevenshowingasymptoticconver-
gence,oftheadaptivegradientmethodsstemsfrombothnumerator
y k+1=y k+ k∇f(x k) k2, anddenominatorofx
k+1
−x kbeingdependentonthegradient(and
f(x ) itshistory) and, thus, the firstorder component in(2)does not ad-
x k+1=x k −h √∇ y k+1k +ǫ, mitastraightforwarddescent direction,unlikethevanillagradient-
descent. The existing analyses of adaptive methods usually tackle
differsfromthecoordinate-wiseupdateinAdaGrad(3a)-(3b),which thischallengebyaddingandsubtractingasurrogateterm,leadingto
is more commonly used in practice [18]. So, the analysis in [22] anadditionalcomplicatedtermintheerror.
doesnottriviallyextendtoAdaGrad.Furthermore,inthedetermin- Ourapproachinvolvessplittingthedenominatorinx x
k+1,i k,i
−
istic settings, if y is initialized as a small value, AdaGrad-Norm intotwocases,dependingonwhetherthedenominatorisalwaysless
0
isshowntohaveasublinearrateuntilafinitenumberofiterations thanoneorcrossesoneafterafiniteiterations.Intheformercase,the
wherey crosses acertainthreshold [22].Inthesamesettings,we analysisfrom(2)becomesequivalenttothatofthevanillagradient-
k
provethatAdaGradhasalinearrateforeachiterationk 0.[15] descent oritsmomentum-variant, withoutanyadaptivestepsize. In
≥
proveda (logK)regretboundoftheSC-AdaGradalgorithm,with thelattercase,thechallengeisthatthedenominatorofx x
k+1,i k,i
O −
y
k+1,i
replacing√y
k+1,i
inthedenominator ofAdaGrad(3b)and is not apriori bounded and, hence, the argument used in gradient-
a coordinate-wise ǫ , when f isstrongly convex and the gradients descentdoesnottriviallyapply(ref.(6)later).Toaddressthischal-
k
andtheestimatesofminimaareuniformlybounded.TheSAdaGrad lengeinthiscase,ouranalysisinvolvestwobroadersteps.Inthefirst
algorithm,adouble-loopalgorithmwithAdaGradinitsinnerloop, step,theconditiononthedenominatorallowsustoprovedecrement
hasaconvergencerateof (1/K)forweaklystronglyconvexf[4]. inthecostfunctionovertheiterations,simplyforAdaGradandaf-
O
For two-layer networks with a positive definite kernel matrix, [21] teramoreinvolvedargumentforAdamduetoitsmomentuminthe
proves linear convergence of the AdaLoss algorithm if the width numeratorofx x .Inthenextstep,weusethetelescopic
k+1,i k,i
−
ofthehiddenlayerissufficientlylarge.Comparedtotheaforemen- sum, followed by a simple argument to prove the boundedness of
tionedworksontheanalysisofAdaGradanditsvariants,weprove thedenominator. Oncewehaveabounded denominator, theanaly-
linearconvergenceoftheoriginalAdaGrad(3a)-(3b)whenf isL- sisagainbecomessimilartotheformercase.Finally,inthegeneral
smoothandsatisfiesthePLinequality. case,wherethenumeratorofx x crossesoneonlyalong
k+1,i k,i
−
Next,wereviewtheAdamalgorithm[11]anditsexistingconver- asubsetofthedimensions 1,...,d andstayslessthanonealong
{ }
gence results. At each iteration k = 0,1,..., Adam maintains an therestof thedimensions, thelinear rateisproved bytheweakest(maximum)ofthelinearratesobtainedintheprevioustwocases. Case-I:First,weconsiderthecasewheny > (1 ǫ)2 forall
k+1,i
Tonotmakeourexpositiontooburdensome,weanalyzeAdaGrad i 1,...,d forallk T,whereT < .Recallt− hat y is
k,i
andAdamfirstinthedeterminsiticsetting,followedbyanalysisof a∈ non{ -decreasi} ng sequenc≥ eforalli.Consid∞ eranyiteration{ k } T.
AdaGrad with stochastic gradients. Due to space limit, we do not Then, √y k+1,i+ǫ 2 >√y k+1,i+ǫ.From(5),thenwehave≥
presenttheanalysisofAdamwithstochasticgradients,whichcanbe
(cid:12) (cid:12)
don Te heby kef yol clo ow nti rn ibg ut th ioe nte sc oh fn oiq uu re pap pre es re an rt eed asin fot lh lois wp sa .per. f(cid:12) (x k+1) −f((cid:12) x k) ≤−h (cid:18)1
−
L 2h (cid:19)i=d
1
√|∇ yi kf +( 1x ,ik +) |2 ǫ. (6)
Weprovelinearconvergenceofthediscrete-timeAdaGrad(3a)- X
• (3b)andtheAdamalgorithm(4a)-(4c)inthedeterministicsetting If 0 < h < L2, from above we have f(x k+1)
≤
f(x k). Since
when f is L-smooth and satisfiesthe PLinequality. Tothe best f(x k) : k T is a decreasing sequence and, under Assump-
{ ≥ }
ofourknowledge,thelinearconvergenceguaranteesofthesetwo tion 1, bounded from below by f , the sequence converges with
∗
methodsdonotexistintheliterature.Wedonotrequireconvexity lim k f(x k) < .Next,uponsummationfromt = T tot = k
→∞ ∞
or boundedness of the gradients. Our analysis of linear conver- onbothsidesof(6),duetotelescopiccancellationontheL.H.S.,
genceisexistentialinthesensethatweprovelinearconvergence
g
Oo ef unt rh ce
e
ano
e
axp lpt yi lm
si
eci siz tle oyr fs i,
n
Ab
t
du
e
at
r
Gmdo
rs
an
o
do ft
ath
nc eh da
p
Ar ra
o
dc
b
at le mer miz fe
oa
lnt lh
d
oe wac lgo aoef srfi
i it
mc hi
m
ie lan
p
rt ao arf pac pmo
re
on
t
av
e
ce
r
hr s-
,.
f(x k+1) −f(xT) ≤−h (cid:18)1
−
L 2h
(cid:19)t
X=k
T
Xi=d
1
√|∇ yi tf +( 1x ,it +) |2 ǫ,
•
with the difference lying in the additional arguments for Adam whichmeans
duetoitsmomentumtermofthegradients.Wesplittheconver-
genceanalysisintotwobroadcasesdependingonthemagnitude
0 lim h 1
Lh k d |∇if(xt) |2
o eaf ct hhe sud ce hno cm asi ena toto pr ri en sex nk t+ a1 s, ii
m−
px lek, ci oa nn vd erl ge ev ner ca eg ae nt ah le ysc io s.n Tdi ht uio sn ,of uo rr ≤k
→∞ (cid:18)
− 2
(cid:19)t X=T Xi=1
√yt+1,i+ǫ
proofsofAdaGradandAdamaresimplerthantheexistingworks.
lim (f(xT) f(x k+1)) f(xT) f .
≤k − ≤ − ∗
Moreover, the presented proof methodology, as outlined earlier, →∞
dp ir eo nv tid me es tu hs odw si .th Tha eu nn oifi ve ed ltyre oc fip oe urfo ter ca hn na il qy uz ein lg ieo st ih ner ada dd ra ep st siv ine gg tr ha e- So,lim k →∞ k t=T | √∇yi tf +( 1x ,t i) +|2 ǫ isbounded,whichimpliesthat
sd ie mno pm lerin wat ao yr ,o bf yx pk ro+ v1 i, ni g− itx sk b, oi, uw ndh ei dch nei ss sn .otboundedapriori,ina P lim |∇if(x k) |2 =0, i. (7)
Considering thepractical scenario, weprovelinearconvergence
k →∞√y k+1,i+ǫ ∀
•
inexpectationofAdaGradwithstochasticgradientstoaneighbor- Thus, either lim
k
if(x k) = 0 or lim
k
√y
k+1,i
= .
hoodoftheminima.Theanalysiswithstochasticgradientsfollow Consider the case→∞ lim|∇
k
√y|
k+1,i
= .→ F∞ rom (7), we ha∞ ve
thesameoutlinedescribedaboveinthedeterministicsetting. |∇if(x k) |2 = O(√y k→ +1∞ ,i + ǫ), which∞ implies √|∇ykif +( 1x ,ik +) |ǫ =
AlthoughweproveexistenceofalinearconvergencerateforAda- ((√y
k+1,i
+ ǫ)−0.5). So, if lim
k
√y
k+1,i
= , we have
tG er ra izd ea tn hd eA exd aa cm tc, oa el fi fim ci it ea nti to bn yo ef xo pu lr ica in tla yly resi ls ati is nt gha itt wit id thoe ths eno alt gc oh ra ir thac m- O lim k →∞ √|∇ykif +( 1x ,ik +) |ǫ = 0.Sinceboth→ l∞ im k →∞ √|∇yi kf +( 1x ,k i∞ ) +|2 ǫ = 0and
parameters and the problem constants. However, at least for Ada- lim k →∞ √|∇ ykif +( 1x ,ik +) | ǫ =0,itispossibleonlyiflim k →∞|∇if(x k) |=
Grad,suchexplicitrelationcanbeobtainedbyacloserlookintoour 0.So,wehaveprovedthatlim
k
if(x k) = 0istrue.Under
analysis,whichisdifficultforAdamduetotheadditionalmomentum Assumption 3, then we have
lim→∞|∇
f(x )
=|
f . The above ar-
k k
term.Duetolimitedspace,wedonotpresenttheanalysisofAdam gumentfurthershowsthatlim
k
→ √∞ y
k+1,i
= ∗ ispossibleonly
withstochasticgradients,asitcanbedonebyfollowingthestepsin in the trivial case where the fu→ nc∞ tion is at the m∞ inimum point. In
theproofsofTheorem2andTheorem3. the non-trivial case, therefore, √y
k+1,i
+ǫ is bounded. Then, for
0<h< L2, ∃M ∈(0, ∞)suchthat√y k+1,i+ǫ ≤M ∀i.From(6),
2 Linearconvergence ofAdaGrad
h Lh
f(x ) f(x ) 1 f(x ) 2.
Theorem1. ConsidertheAdaGradalgorithmin(3a)-(3b)withini- k+1 − k ≤−M − 2 k∇ k k
tializationy = 0 andx Rd andtheparameterǫ (0,1).If (cid:18) (cid:19)
0 d 0
Assumptions1-3hold,thent∈ hereexistsh > 0suchthat∈ ifthestep UnderAssumption3,fromabovewehave
size 0 < h < h, then (f(x ) f ) ρ(f(x ) f ) for all
k+1 k
− ∗ ≤ − ∗ h Lh
k 0,whereρ (0,1). f(x ) f(x ) 1 2l(f(x ) f ).
≥ ∈ k+1 − k ≤−M − 2 k − ∗
(cid:18) (cid:19)
Proof. Consideranarbitraryiterationk 0.UnderAssumption2,
≥
from(2)wehave Upondefiningc =h 1 Lh 2l,werewritetheaboveas
1 − 2 M
f(x k+1) −f(x k) ≤(x
k+1
−x k)⊤ ∇f(x k)+ L
2
kx
k+1
−x
k
k2. f(x k+1)(cid:0) −f(x k)(cid:1) ≤−c 1(f(x k) −f ∗),
Uponsubstitutingabovefrom(3b), whichmeans
f(x k+1) −f(x k) f(x k+1) −f
∗
≤(1 −c 1)(f(x k) −f ∗), ∀k ≥T. (8)
≤ Xi=d
1
−h √|∇ yi kf +( 1x ,ik +) |2 ǫ + L 2h2 √|∇ y ki +f 1( ,x ik +) | ǫ2 2 !. (5) tM hao tre co 1v <er, 10 .S< o,h (1< −L2 c 1im ) ∈pli (e 0s ,t 1h )at foc r1 0> <0 han <d0 m< in {h L2< ,M 2M 2 ll }.implies
(cid:12) (cid:12)
(cid:12) (cid:12)Case-II:Next,weconsiderthecasewheny
k+1,i
(1 ǫ)2forall Uponsubstitutingµ k+1,iabovefrom(4a)andrearrangingtheterms
i 1,...,d forallk 0.Then,√y k+1,i+ǫ ≤ 1.A− lso,y
0
=0
d
inthenumerator,
∈{ } ≥ ≤
and(3a)impliesthaty 0 k,i.From(5),thenwehave
k,i ≥ ∀ f(x k+1) f(x k)
−
f(x k+1) −f(x k) ≤−h 1
−
2L ǫh
2
k∇f(x k) k2.
h
d (1 −β 1k)(1 −hL 2(1 −β 1k)) |∇if(x k) |2 −hL 2β 12 k|µ k,i |2
(cid:18) (cid:19) ≤− √ν k+1,i+ǫ
UnderAssumption3,fromabovewehave
Xi=1
d
Lh h
β 1k(1 −hL(1 −β 1k))µ
k,i
∇if(x k)
f(x k+1) −f(x k) ≤−h (cid:18)1
− 2ǫ2
(cid:19)2l(f(x k) −f ∗). −
Xi=1
√ν k+1,i+ǫ
Upondefiningc 2 =h 1 − 2L ǫh 2 2l,weobtainthat = −h d (1 −θ k)(1 −β 1k)( √1 ν− kh +1L 2 ,i(1 +− ǫβ 1k)) |∇if(x k) |2
f(x k+1) f(cid:0) (1 (cid:1)c 2)(f(x k) f ), k 0. (9) Xi=1
Moreover,(1 −c
2−
)
∈∗ (0≤ ,1)f− or0<h<m− in∗
2
Lǫ2∀
,
21≥
l
. −h
i=d 1θ k(1 −β 1k)(1
−
√h νL
k2
+(1
1,−
i+β
1
ǫk)) |∇if(x k) |2
Case-III: Finally, we consider the case whnen, foroeach i X
{
a
sun1 cd, h. y.
tk
h.
+
a, td
1,
y} i, >∃T (i
1
>∈
−
([ ǫ0 1), 2∞
f
ǫo)
)r
2s ku foc
≥
rh at
T
lh lia i.t Ty hk e+ 1n1 ,,,i
.∃
.T≤
.,=
d(1 m−
foa
rǫ
x
a)
{
l2 lTf k1o ,r .k
.
T.<
, .T
FdT o∈ }i
r
−h Xi=d
1
−hL 2β 12 k|µ k,i |2+β 1 √k( ν1 k− +1h ,iL +(1 ǫ−β 1k))µ k,i ∇if(x k) ,
k+1,i
− ∈{ } ≥ for any θ (0,1). We define two set of indices I = i i
k T,theanalysisinCase-Idirectlyapplies.Fork < T,fromthe k ∈ { | ∈
an≥
alysisinCase-I-andCase-II,itfollowsthat
{1,...,d },µ
k,i
6= 0 }anditscomplementI′.Then,werewritethe
aboveinequalityas
f(x ) f max (1 c ),(1 c ) (f(x ) f ), (10)
k+1 1 2 k
− ∗ ≤ { − − } − ∗ f(x k+1) f(x k)
−
Sw ih ne cr ee ǫm <ax
1{
,( 21
Lǫ2−
<c 1) L2, .(1 −c 2) }for0<h<min {L2,M 2l,2 Lǫ2 , 21 l}.
≤−h
(1 −θ k)(1 −β 1k)( √1
ν−
kh +1L
2
,i(1
+−
ǫβ 1k)) |∇if(x k) |2
Weconcludethat,for0<h<min 2ǫ2 ,M, 1 , Xi ∈I
{ L 2l 2l}
h
θ k(1 −β 1k)(1 −hL 2(1 −β 1k)) |∇if(x k) |2
(f(x k+1) −f ∗) ≤max {(1 −c 1),(1 −c 2) }(f(x k) −f ∗), ∀k ≥0, −
i I
√ν k+1,i+ǫ
X∈
w ρh =er mem axax ({ 1(1 − cc )1 ,) (, 1(1 − cc 2 )) } a∈ nd( h0, =1). mT ih nep 2r ǫo 2o ,f Mis ,co 1m .pletewith
h
−hL 2β 12 k|µ k,i |2+β 1k(1 −hL(1 −β 1k))µ k,i ∇if(x k)
{ − 1 − 2 } { L 2l 2l} − √ν k+1,i+ǫ
i I
X∈
(f(S xinc )eρ f∈ )(0 o, f1 th) e,T Ah de ao Gre rm ad1 ai lm gop rl ii tc ha mtes (3t ah )a -t (t 3h be )s lu inb e- ao rp lt yim coa nli vty erg ga ep
s h
(1 −β 1k)(1 −hL 2(1 −β 1k)) |∇if(x k) |2
. (13)
tozerk oa− tth∗ eworst-caserateρ,forsmallenoughstepsizeh. − iX∈I′ √ν k+1,i+ǫ
Forβ < 1andh < 2 ,thelasttermontheR.H.S.in(13)
3 Linearconvergence ofAdam isneg1 ak tive.So,fori IL ,(1 w−eβ1 wk a) nttoshowthat
∈
T izh ate io or ne µm2 =. C νon =sid 0er ath ne dA xdama Rlg do ar nit dhm thi en p( a4 ra a) m-( e4 tc e) rsw βithi ,n βitial- θ k(1 −β 1k)(1 −hL 2(1 −β 1k)) |∇if(x k) |2
0 0 d 0 1k 2
∈ ∈
[ t0 ha,1 t) f, orǫ β∈ (0,1) [0. ,I βfA )ss thu em repti eo xn iss ts1- h3h >old 0, st uh ce hn ∃ thβ ak t i∈ f t( h0 e, s1 t) eps su ic zh e ≥hL 2β 12 k|µ k,i |2+β 1k(1 −hL(1 −β 1k)) |µ k,i ∇if(x k) |, (14)
1k k
∈
w0 h< ereh ρ< h (, 0t ,h 1e )n
.
(f(x k+1) −f ∗)
≤
ρ(f(x k) −f ∗)forallk
≥
0 ffo (r xsom )e θ
k f(∈
x( )0,1) 0a ,n fd ros mom (e 13h ).> Con0 s, idw eh ric ah nyw iould Ii .m Wpl ey nt oh ta et
∈ k+1 − k ≤ ∈
Proof. UnderAssumption2,from(2)wehave that(14)isequivalentto
L
f(x k+1) −f(x k) ≤(x
k+1
−x k)⊤ ∇f(x k)+ L
2
kx
k+1
−x
k
k2. θ k(1 −β 1k) |∇if(x k) |2 −β 1k |µ k,i ∇if(x k) |≥h 2β 12 k|µ k,i |2
L
Uponsubstitutingabovefrom(4c), +h 2(1 −β 1k) θ k(1 −β 1k) |∇if(x k) |2 −2β
1k
|µ
k,i
∇if(x k)
|
,
f(x k+1) f(x k) whichisimpliedi(cid:0)f (cid:1)
−
C 1a ,s
.e≤
.-I
.X :i ,=d dF1
ir s ft
o−
,
rwh
ae
lµ lk c√ k+ oν n1 k s,i i+∇
d
T1 e, ,i rif w+ t(
h
hx
e
eǫk rc) ea+
Tse
<L
ν
k2h +2
1(cid:12) (cid:12),
.i√ T>ν| hµ k e+k n(+ ,11,1 i √−,i +| ν2
ǫ
kǫ
) +(cid:12)
(cid:12)22 1f!
,o ir
+.
al
ǫl( 2i11 >∈) θ +k( h1 L 2− (β 11 −k) β|∇ 1ki )f (cid:0)( θx kk () 1|2 −− ββ 1k1 )k || ∇µ k if,i (∇ xi kf )( |2x k −) | β≥ 1kh |µL 2 k,β i1 ∇2 k i| fµ (k x,i k| ()2
1|
(cid:1)5).
{ } ≥ ∞
√ν k+1,i+ǫ.From(11),thenwehave (cid:12)
(cid:12)
(cid:12)
(cid:12)
For the case |µ k,i
|
= ∞, we choose β 1k = 0 and h < L2 so
f(x k+1) −f(x k) it nha (t 1( 51 )5 i) sph oo sld its i. veO ,t th he er nw (i 1s 5e, )f ho or ldth se foc rase |µ k,i
|
< ∞, if the L.H.S.
≤−h i=d 1(cid:18)µ k √+ ν1 k,i +∇ 1,i if +(x ǫk) − L 2h √| νµ kk ++ 11 ,, ii +|2 ǫ (cid:19). (12) h
≤
p
q
kk, (16)
Xwherewedenote UnderAssumption3anddefiningc = h(1 θ )(1 β )(1
1k k 1k
hL(1 β ))2l,fromaboveweget − − −
p
k
=θ k(1 β 1k) if(x k)2 β
1k
µ
k,i
if(x k) , 2 − 1k M
− |∇ | − | ∇ |
q
k
=
L
2(1 −β 1k) θ k(1 −β 1k) |∇if(x k) |2 −β
1k
|µ
k,i
∇if(x k)
|
f(x k+1) −f
∗
≤(1 −c 1k)(f(x k) −f ∗), ∀k ≥T. (20)
+ L 2β 12 k|µ k,i |2(cid:0) . (cid:1) M imo pr le ieo sve tr h, a0 t< c 1h< < L 1( .1 −2 Sβ o1 ,k) (1impli ce 1s )thatc 1 (k 0,> 1)0 fa on rd 00< <h< h M 2 <l
− ∈
min 2 ,M,h .
Now,theL.H.S.in(15)beingpositiveisequivalentto Case{ -L II( :1 − Nβ e1 xk t) ,w2 el con1 s} iderthecasewhenν (1 ǫ)2forall
k+1,i
≤ −
θ k > 1 β 1 βk
1k
| iµ fk (, xi | k) . (17) ai n∈ d{ (41 b, ).. im., pd li} esfo tr haa tll νk k,≥ i ≥0. 0T ∀h ken ,i, .√ Fν rok m+1 (, 1i 1+ ),ǫ th≤ en1. wA el hso av, eν 0=0 d
− |∇ |
Since µ < , we choose β < 1 so that d Lh2
β 1k
∈| k (,i 0|
,1)
an∞
d 1 −β1 βk 1k|∇| iµ fk (, xi k| )
|1k
<
1.1+ S| u∇ c| iµ hfk (, xi ak| ) |
choice of
Wf( ex nk o+ t1 e) th− atf th(x ek a) bo≤ veXi= in1 eq(cid:18) u− alh itµ yk i+ ss1 i, mi ∇ ili af r( tx ok (1) 2+ )w2 hǫ e2 re| √µ k ν+ k1 +,i 1| ,2 i+(cid:19) ǫ.
β 1k allows us to choose θ k ∈ (cid:18)1 −β1 βk 1k |∇| iµ fk (, xi k| ) |,1 (cid:19). So, for pin ro( o1 f2 a) bis ovr eep il nac Ce ad seb -y I,1 ana dnd inL stei as dre op fl (a 1c 8e )d ,wby eoǫL 2 bt. aS ino, thw ae
t
followthe
β th1 ak
t
t<
he
1 L+ .H|∇ .| Siµ1 f .k (, xi ik n| ) |(1a 5n )d isθ k po∈ siti(cid:18) ve1 .−β M1 βk 1 ok r| e∇ o| i vµ f ek ( r, x ,i k| s) | in, c1 e(cid:19), µwe hav <e f(x k+1) −f(x k)
| k,i | L d
∞
are, bo θth k(1num
β
1e kra )tor ifan (xd k)de 2no βm 1i kna µto kr ,iin if(t xh ke
)
R .. SH o. ,S t. heof R.H(1 .6 S)
.
≤−h(1 −θ k)(1 −β 1k)(1 −h 2ǫ2(1 −β 1k)) i=1|∇if(x k) |2
O − |∇ | − | ∇ | X
of (16) is positive. So, we can choose h > 0 satisfying (16) so L
(cid:0) (cid:1) = h(1 θ )(1 β )(1 h (1 β )) f(x ) 2. (21)
that (15) holds. We conclude that, β k,θ k (0,1) such that for − − k − 1k − 2ǫ2 − 1k k∇ k k
∃ ∈
β [0,β )andθ (θ ,1)thereexistsh (0, )suchthat
1k k k k 1
for Uh p∈ o< nh su1 b, s( t1 i5 tu) tih no gld fs r, o∈ w mh (i 1c 4h )im inp (l 1ie 3s ),th wa et h(1 a4 v) eh∈ oldsfo∞ ri ∈I. U
h
2n
L
ǫd 2e (r 1A −ss βu 1m kp )t )i 2o ln ,f3 roa mnd ad be ofi veni wng ec g2 ek
t
= h(1 −θ k)(1 −β 1k)(1 −
f(x k+1) −f(x k) f(x k+1) −f
∗
≤(1 −c 2k)(f(x k) −f ∗), ∀k ≥0. (22)
h
(1 −θ k)(1 −β 1k)(1 −hL 2(1 −β 1k)) |∇if(x k) |2
Therestoftheprooffollowsthesameargumentasintheproofof
≤− √ν k+1,i+ǫ Theorem1.
i I
X∈
h
(1 −β 1k)(1 −hL 2(1 −β 1k)) |∇if(x k) |2
Remark 1. Bias correction: Since µ = ν = 0, the moment
−
iX∈I′
√ν k+1,i+ǫ
estimates µ
k
and ν
k
are biased
towa0
rds
zero0
at the early stages
h
(1 −θ k)(1 −β 1k)(1 −hL 2(1 −β 1k)) |∇if(x k) |2 of iterations. Thus, in practice, Adam is implemented with a bias
≤− √ν k+1,i+ǫ correction that accounts for this initialization of the moment esti-
Xi ∈I matesat zero[11]. When biascorrection isactive, hin(4c) ises-
− =h −iX h∈ (I 1′ ( −1 θ− k)θ (k 1)( −1 β− 1kβ )1 (k 1)( −√1 ν h− k
L
2+h 1 (L ,2 1i( −+1 − βǫ 1kβ )1 )k)) d|∇ √|i ∇f ν(
i
kfx +(k 1x) ,i| k2
+) |2
ǫ
s olie fmn it nki i→a tl il a∞y lbr qe i1 ap 1− sl −βa
β
c1kc o2k k+e
+
rd 1
r1
eb cy = tioh 1 nq ,a11 t− n h− dβ eβ 1k 02 k rk + e+ 1 < s1 u. lqtS 1 p1−in r−β ec β1 ske 2k ek+
+
nβ 1 t11 ek d<, iβ n∞2 T∈ . hT eoh[0 ru e, s m1 ,) in 2, w t ih se e vh aca a liv s dee
,
≤0. Xi=1 (18) uponreplacinghwiththepositivequantityhmin k ≥0 q1 1− −β β1k 2k k+ +1 1.
UnderAssumption1,wefollowtheargumentintheproofofTheo- Remark 2. Likeourproof above, someoftheexistinganalyses of
rem1after(6),andobtain Adam require a decreasing β , including [3,11, 17]. Whilethere
1k
areconvergenceresultsofAdamwithconstantβ ,theydonotprove
1
lim
|∇if(x k) |2
=0, i. (19)
linearconvergence.
k →∞√ν k+1,i+ǫ ∀
Remark3. Wedonotutilizetheexplicitupdate(3a)or(4b)ofthe
Under Assumption 3, following the argument in the proof of denominatoryorν.Ourproofsholdaslongasy > 0orν >
k,i k,i
Theorem 1 after (7), we have lim k f(x k) = 0 d, and 0. The specific form of (3a) or (4b) only implicitlyappears in our
→∞∇
lim k f(x k) = f ,and M (0, )suchthat√ν k+1,i+ǫ analysisintermsofitsboundM.Therefore,Adam’sanalysistrivially
→∞ ∗ ∃ ∈ ∞ ≤
M i.From(18),thenweobtain extendstoAdaBelief[25]andAMSGrad[17].InRAdam[14], T <
∀ ∃
such that ρ(k) is increasing for k T and converges to ρ .
f(x k+1) f(x k) T∞ hen,bydefinition,0<r(k)< .The≥ n,followingtheargument∞ in
− ∞
h L Remark1,ouranalysisofAdameasilyextendstoRAdamfork T
(1 θ )(1 β )(1 h (1 β )) f(x ) 2. ≥
≤−M − k − 1k − 2 − 1k k∇ k k byreplacinghwithhmin k Tr(k).
≥4 AdaGradwithstochastic gradients Proof. Consideranarbitraryiterationk 0.UnderAssumption2,
≥
from(2)wehave
Inpractice,neuralnetworksaretrainedwithstochasticormini-batch
L
g pr ra ed seie nn tt os ua rt ae na ac lh ysi it ser oa fti Aon dao Gf rt ah de io npt ti hm ei sz te or c. hS ao s, ticin st eh tti is ns ge .c Fti oo rn s, iw me
-
f(x k+1) −f(x k) ≤(x
k+1
−x k)⊤ ∇f(x k)+
2
kx
k+1
−x
k
k2.
plicity,weconsideronlyonedatapointineachiteration.However, Upontaking conditional expectation onboth sidesandsubstituting
ourresultisapplicabletomini-batchwithmorethanonedatapoints. abovefrom(24a),
Foreachdatapoint ,wedefinethelossfunctionl : Rd Ras
l(, )
anditsgradieD
nt g(x; ) = xl(x;
).Ouraimis→
tomini-
E ζk[f(x k+1)] −f(x k)
· D D ∇ D
mizetheempiricalr misk in, f(x):=E ζ[l(x; ζ)]. (23) ≤ Xi=d 1 E ζk (cid:20)− √h yg ki ,, iζk +(k ǫ) (cid:21)∇if(x k)+ L 2h2 E ζk " √|g yi, kζk ,i( +k) ǫ|2 2 #!
x ∈Rd D = d h|∇if(x k) |2 + Lh2E ζk |g i,ζk(k) |2 (cid:12) (cid:12), ((cid:12) (cid:12)25)
Ateachiterationk
≥
0,AdaGradrandomlychoosesadatapoint
i=1
− √y k,i+ǫ 2 √(cid:2)y k,i+ǫ 2 (cid:3)!
,basedontherealizationζ oftherandomvariableζ,andcom- X
dpD euζ ntk e os teit ts hs eto i-c th has et li ec mg er na td oie fn gt ζg kζ (kk k( )k ,) fo= reg a( cx hk i; D ∈ζ {k 1). ,.W .e .,l de }t .g Ii n,ζ sk te(k ad) w tioh ner oe ft ch oe nl da is tt ioe nq au la vli aty riaf no cll eow ofs gf ir ,o ζm k((cid:12) (cid:12) kA )ss au nm dp Ati so s(cid:12) (cid:12)n um4. pF tir oo nm s4th -5e ,defini-
of(3a)-(3b),x kandy kareupdatedas E
ζk
|g i,ζk(k) |2 ≤V 1,i+(V 2,i+1) |∇if(x
k
|2
x =x h
g i,ζk(k)
, (24a)
(cid:2) (cid:3) ≤M+MG |∇if(x k) |2.
k+1,i k,i − √y k,i+ǫ
Uponsubstitutingfromabovein(25),
y =y + g (k)2. (24b)
k+1,i k,i | i,ζk | E [f(x )] f(x )
ζk k+1
−
k
Foreachiterationk ≥0wedefinethefollowing. d h|∇if(x k) |2
+
Lh2M+MG |∇if(x k) |2
. (26)
•
Let E ζk[ ·] denote the conditional expectation of a function the ≤ Xi=1 − √y k,i+ǫ 2 √y k,i+ǫ 2 !
randomvariablesζ ,giventhecurrentx andy .
•
Let E k[ ·] denote tk he total expectation ok f a funk ction of the ran- iCase- 1I ,: .F .i .r ,s dt, w foe rc ao ln ls kider Tth ,e wc ha es re eTwh <e(cid:12) (cid:12)n y k ., Ci o> nsi(cid:12) (cid:12)( d1 er− anǫ y)2 itef ro ar tia ol nl
d So pm eciv fia cr aia llb yl ,e Es k{ζ
[
·0 ], =... E, ζζ 0k
,.}
..,g ζkiv [e ·]n ,t khe ≥in 0it .ialconditionx 0andy 0. k∈ ≥{ T.Then,}
√y
k,i+ǫ≥2
>√y
k,i+ǫ.∞
From(26),thenwehave
For each i 1,...,d , define the conditional vari- E (cid:12)[f(x )](cid:12) f(x )
•
ance of g
(k∈
),
w{
hich is
}
a function of the random vari-
ζk(cid:12) k+1 (cid:12)− k
able ζ k, gi i, vζ ek n the current x
k
and y
k
as V ζk[g i,ζk(k)] =
h
d (1
−
LM 2Gh) |∇if(x k) |2
−
L 2Mh
. (27)
E
Eζk [|
gg
i,ζk
(( kk ))
]−
2.E ζk[g i,ζk(k)] |2 = E
ζk
|g i,ζk(k) |2
−
≤−
Xi=1
√y k,i+ǫ
| ζk(cid:2) i,ζk | (cid:3) (cid:2) (cid:3) Forh< 2 ,wehave(1 LMGh)>0.Now,exceptatthetrivial
LMG − 2
Wemaketwoadditionalstandardassumptions[2,18]forstochas- point f(x ) = 0 , there exists at least one j 1,...,d for
k d
∇ ∈ { }
ticgradientsasfollows.Assumption5isregardingboundedness of which jf(xt) = 0. Weconsider thenon-empty set I = i i
∇ 6 { | ∈
coordinate-wiseaffinenoisevariance[18]. 1,...,d , if(x k) = 0 and its complement I′. Then, we can
{ } ∇ 6 }
rewritetheR.H.S.aboveas
Assumption4. Ateachiterationk 0,thestochasticgradientis
anunbiasedestimateoftruegradient≥ ,i.e.,E ζk[g ζk(k)]= ∇f(x k). d (1
−
LM 2Gh) |∇if(x k) |2
−
L 2Mh
√y k,i+ǫ
Assumption 5. For each i 1,...,d , there exist two non- Xi=1
negativerealscalarvaluesV 1,i∈ an{ dV 2,isuc} hthat,foreachk ≥0,
=
(1
−
LM 2Gh) |∇if(x k) |2
−
L 2Mh
+
−L 2Mh
.
V ζk[g i,ζk(k)] ≤V 1,i+V 2,i |∇if(x k |2. Xi ∈I
√y k,i+ǫ
jX∈I′
√y k,j+ǫ
Uponrearrangingthetermsabove,wehave
WedefineM =maxiV 1,iandMG =maxi(V 2,i+1).
Theorem 3. Consider the AdaGrad algorithm in (24a)-(24b) with
d (1
−
LM 2Gh) |∇if(x k) |2
−
L 2Mh
>0
√y k,i+ǫ
initialization y
0
= 0
d
and the parameter ǫ (0,1). If Assump- Xi=1
sti to an tes m1 e- n5 tsho al rd e, tt rh ue en .thereexistsω ∈ (0, ∞)su∈ chthatthefollowing h< 2 i ∈I |∇ √i yf k( ,x ik +) ǫ|2 . (28)
(i) h > 0 such that if the step size 0 < h < h, then
⇐⇒ L i ∈I MG|∇ √iP yf k( ,x ik +) ǫ|2+M + j ∈I′ √ykM ,j+ǫ
∃E ζk[f(x k+1)] −f(x k) ≤ρ(f(x k) −f ∗)+ω,forallk ≥0, Since j ∈I′ √ykMP ,j+ǫ < ∞,boththenumeraPtoranddenominatorin
(( ii ii i) ) w Glimh ive k er → ne aρ ∞ r∈ bk i∇ t( r0 af, r(1 yx) cka h)n okd i2 ceω > s= od f2O t− hL e( LM M M inh G i) t. ih a. lx Rd, t ih se poR s. iHP tiv.S e. .o Tf h( e2 n8 ,) har >e O 0c(cid:16)a Pnbi ∈ eI ch|∇ o√ si y ef k n( ,x i tk + o) ǫ| s2 a(cid:17)ti. sfS yo (, 2t 8h )e ,R fo.H rw.S h. io cf h(28)
0
klim E k[f(x k+1)]
≤
1∈ ω ρ. i=d
1
(1 − LM 2Gh √) | y∇ k,i if +(x ǫk) |2 − L 2Mh >0. (29)
→∞ − XT {Ehe ζn k, [f((2 x7 k) +1)im
]
:pl kies
≥
t Th }at isE aζ dk e[ cf re( ax sk i+ ng1) s] eque< nceaf n( dx ,k u) n. deS ri Anc se
-
C
i
∈ase {- 1I ,I: ..N .e ,x dt }, w foe rc ao lln ksid ≥er 0t .h Te hc ea ns ,e √w yh ke ,n i+y k ǫ,i ≤≤ 1( .1 A− lsoǫ ,) y2 0fo =r a 0l dl
sumption1,boundedfrombelowbyf ,thesequenceconvergeswith and(24b)impliesthaty 0 k,i.From(26),thenwehave
k,i
lim E [f(x )]< .Next,u∗ ponsummationfromt=T to ≥ ∀
t=k k→ o∞ nboζ tk hsidesk+ o1 f(27),∞ duetotelescopiccancellation,weobtain E [f(x )] f(x )
ζk k+1
−
k
thetotalexpectation
= h 1
LMGh
f(x )2 +
LMdh2
.
E k[f(x k+1)] −f(xT) −
(cid:18)
− 2ǫ2
(cid:19)
(cid:13)∇ k
(cid:13)
2ǫ2
h
k d (1
−
LM 2Gh) |∇if(xt) |2
−
L 2Mh
.
UnderAssumption3,fromabove(cid:13) wehave (cid:13)
≤− √yt,i+ǫ
t X=T Xi=1 E [f(x )] f(x )
Combiningwith(29),wehave
ζk k+1
−
k
h 1
LMGh
2l(f(x ) f )+
LMdh2
.
0< lim
k d (1
−
LM 2Gh) |∇if(xt) |2
−
L 2Mh ≤− (cid:18) − 2ǫ2 (cid:19) k − ∗ 2ǫ2
k √yt,i+ǫ
≤klim→∞ (ft X (= xT TXi )= −1 E k[f(x k+1)]) ≤f(xT) −f ∗. Upondefiningc 2 =h (cid:16)1 − LM 2ǫG 2h (cid:17)2l,weobtainthat
→∞ E [f(x )] f (1 c )(f(x ) f )+ω. (33)
FollowingtheargumentasintheproofofTheorem1,theaboveim- ζk k+1 − ∗ ≤ − 2 k − ∗
pliesthat
Moreover,(1 c ) (0,1)for0<h<min 2ǫ2 , 1 .
lim
(1
−
LM 2Gh) |∇if(x k) |2
−
L 2Mh
=0, i. (30) Following
− the2 sam∈
e argument as in the
pnroL oM
f
G
of
2 Tl
hoeorem 1,
k →∞ √y k,i+ǫ ∀ for 0 < h < min 2ǫ2 ,M, 1 , h satisfying (29), and for
{LMG 2l 2l}
So, either lim k →∞(1
−
LM 2Gh) |∇if(x k) |2
−
L 2Mh = 0 or lim k →∞k∇f(x k) k2 >dg ∗,
lim
k
√y
k,i
= . Consider the case lim
k
√y
k,i
=
∞. F→ r∞ om (30), we h∞ ave (1
−
LM 2Gh) |∇if(x k) |→2 ∞
−
L 2Mh = E ζk[f(x k+1)] −f(x k) ≤ρ(f(x k) −f ∗)+ω, ∀k ≥0,
O(√y k,i + ǫ), which implies r(cid:12) (cid:12) (cid:12)(1 −LM 2Gh √) y|∇ k,i if +( ǫxk) |2 −L 2Mh (cid:12) (cid:12)
(cid:12)
= w iteh re ar te ingρ th=
e
am boa vx e{( f1 ro− mc k1) t, o(1 0,− byc 2 t) h} e. lS ai wnce ofρ to∈ tal( e0 x, p1 e) c, tau tip oo nn
,
lO im((√y k,i r+
(cid:12)(1
−ǫ) L− M 20. G5) h.
)
|∇So if, (xi kf
)
|l 2i −m Lk 2M→ h∞ (cid:12)√ =y k 0,i
.
Fo= llow∞ in,
g
tw he
e
ah ra gv ue
-
lim k →∞E k[f(x k+1)] ≤ 1 −ω ρ.Theproofiscomplete.
k →∞ (cid:12) (cid:12) √yk,i+ǫ (cid:12) (cid:12) According toTheorem3,AdaGradin(24a)-(24b)convergeslin-
epm
il
tie hen est rta ohf fate
t
tr
hl
ei( m7 c) aki s→n es∞th a(e
b1
op v−ro eo ,Lf wMo
2
ef
G
hT
ah
vh
)
ee |o ∇re im f(1 x, kl )i |m 2k −→∞
L
2M√ hy k =,i = 0.∞ Soi ,m in- ee na orl uy ghin ste ex pp se izc etat hio .n Thto
e
na ein ge hi bg oh rb ho or oh doo od
f
fo ∗f m toin wim hia chf E∗, kf [o fr (xs km +a 1ll
]
converges is (M), i.e., proportional to the variance of stochastic
O
LMh gradientsevaluatedattheminimumpoint.Furthermore,thegradient-
klim |∇if(x k) |2 = (1 2 LMGh) =:g ∗, ∀i. (31) norm k∇f(x k) k2convergestoalimitgreaterthanavalueof O(M).
→∞ − 2
Fromtheaboveargument,itfurtherfollowsthatlim
k
√y
k,i
= Remark 4. Following the steps in Theorem 2 and Theorem 3,
is possible only if lim f(x ) 2 =→∞ dg . For linear convergence in expectation to a neighborhood of min-
k k
0l∞ im
<k → h∞ <|∇
Lf M2(x
Gk
,) h|2 s> atisd fyg
i∗
n, gth (e 2n→ 9)√∞
,
ayk nk∇
d,i
f+ orǫ lii msk kb →ou ∞nd |e ∇d f. (T xh∗ ke )n |, 2fo >r pi lm
i lm
ia
ek
s→f
th∗
∞
atc Ea Akn
d[
afb me
(x
wp
k
ir
+
to h1v )e
s]
td
o≤
cf ho
1
ar
−ω
stρA icd aa gnm
rd al
diw
m
iei nt kh
t→
ss
∞
nt oo
k
tch
∇
na efs ct
(
ei xc
ssk
ag
)
rr
k
ia
l2
yd >i ce on
0
nt .s v,
I et
ri
i
g. me e. s-,
dg ∗, ∃M ∈(0, ∞)suchthat√y k,i+ǫ ≤B ∀i.Then,from(27),
totheminima.Thisimplicationofouranalysisisconsistentwiththe
E [f(x )] f(x ) result in [17] that Adam has non-zero regret at k for some
ζk k+1 − k onlineoptimizationproblems. → ∞
h
(1
LMGh)
f(x ) 2+
LMd
h2.
k
≤−B − 2 k∇ k 2B
UnderAssumption3,fromabovewehave 5 Summary
E [f(x )] f(x )
ζk k+1 − k We presented a framework that proves linear convergence of two
h
1
LMGh
2l(f(x ) f )+
LMd
h2.
adaptivegradientmethods,namelyAdaGradandAdamindiscrete-
≤−B − 2 k − ∗ 2B time,forminimizingsmoothobjectivefunctionsthatsatisfythePL
(cid:18) (cid:19)
inequality. Among the prior works on adaptive gradient methods,
Upon defining c = h 1 LMGh 2l and ω = LMdh2, we onlytheAdaGrad-Normalgorithmandacontinuous-timeversionof
1 B − 2 2B
rewritetheaboveas (cid:16) (cid:17) Adamhaveprovablelinearconvergence,foraclassofoptimization
problems.Thus,ourworkcontributestowardsreducingthetheoret-
E ζk[f(x k+1)] −f
∗
≤(1 −c 1)(f(x k) −f ∗)+ω, ∀k ≥T. (32) ical gap between vanilla gradient-descent and the more successful
adaptive gradient optimizers. The unifying approach in our frame-
Moreover,0<h< 2 impliesthatc >0and0<h< B im-
LMG 1 2l workcouldbeapplicableinrigorouslyanalyzingotheradaptivegra-
pliesthatc 1 <1.So,(1 −c 1) ∈(0,1)for0<h<min {LM2 G, 2B l}. dientoptimizers.References [25] J. Zhuang, T. Tang, Y. Ding, S. C. Tatikonda, N. Dvornek, X. Pa-
pademetris, and J. Duncan. Adabelief optimizer: Adapting stepsizes
[1] A.BarakatandP.Bianchi. Convergenceanddynamicalbehaviorofthe bythe belief in observed gradients. Advances inneural information
ADAMalgorithmfornonconvexstochasticoptimization.SIAMJournal processingsystems,33:18795–18806,2020.
onOptimization,31(1):244–274,2021.
[2] L.Bottou. Large-scalemachinelearningwithstochasticgradientde-
scent. InProceedings ofCOMPSTAT’2010:19thInternational Con-
ferenceonComputationalStatistics,ParisFrance,August22-27,2010
Keynote, Invited and Contributed Papers, pages 177–186. Springer,
2010.
[3] X.Chen,S.Liu,R.Sun,andM.Hong. Ontheconvergenceofaclass
ofAdam-typealgorithmsfornon-convexoptimization.InInternational
ConferenceonLearningRepresentations,2019.
[4] Z.Chen,Y.Xu,E.Chen,andT.Yang. SadaGrad:Stronglyadaptive
stochasticgradientmethods. InInternationalConferenceonMachine
Learning,pages913–921.PMLR,2018.
[5] A. Défossez, L.Bottou, F.Bach, and N.Usunier. A simple conver-
genceproofofAdamandAdagrad.TransactionsonMachineLearning
Research,2022.ISSN2835-8856.
[6] J.Duchi,E.Hazan,andY.Singer.Adaptivesubgradientmethodsforon-
linelearningandstochasticoptimization.In23rdConferenceonLearn-
ingTheory,COLT2010,pages257–269,2010.
[7] J.Duchi,E.Hazan,andY.Singer. Adaptivesubgradientmethodsfor
onlinelearningandstochasticoptimization.JournalofMachineLearn-
ingResearch,12(7),2011.
[8] Z. Guo, Y. Xu, W. Yin, R. Jin, and T. Yang. A novel conver-
gence analysis for algorithms of the Adam family. arXiv preprint
arXiv:2112.03459,2021.
[9] Z.Guo,Y.Xu,W.Yin,R.Jin,andT.Yang. Anovelconvergenceanal-
ysis for algorithms of the Adam family and beyond. arXiv preprint
arXiv:2104.14840,2022.
[10] H.Karimi,J.Nutini,andM.Schmidt. Linearconvergenceofgradient
andproximal-gradientmethodsunderthePolyak-łojasiewiczcondition.
InMachine LearningandKnowledge Discovery inDatabases:Euro-
peanConference,ECMLPKDD2016,RivadelGarda,Italy,September
19-23,2016,Proceedings,PartI16,pages795–811.Springer,2016.
[11] D.P.KingmaandJ.Ba. Adam:Amethodforstochasticoptimization.
arXivpreprintarXiv:1412.6980,2014.
[12] X.LiandF.Orabona. Ontheconvergence ofstochastic gradientde-
scentwithadaptivestepsizes. InThe22ndInternationalConferenceon
ArtificialIntelligenceandStatistics,pages983–992.PMLR,2019.
[13] C.Liu,L.Zhu,andM.Belkin. Towardatheoryofoptimization for
over-parameterizedsystemsofnon-linearequations:thelessonsofdeep
learning. arXivpreprintarXiv:2003.00307,7,2020.
[14] L.Liu,H.Jiang,P.He,W.Chen,X.Liu,J.Gao,andJ.Han. Onthe
varianceoftheadaptivelearningrateandbeyond.InInternationalCon-
ferenceonLearningRepresentations,2020.
[15] M.C.Mukkamala andM.Hein. Variants ofRMSPropandAdagrad
withlogarithmic regret bounds. InInternational Conference onMa-
chineLearning,pages2545–2553.PMLR,2017.
[16] I.Necoara,Y.Nesterov,andF.Glineur. Linearconvergenceoffirstor-
dermethodsfornon-stronglyconvexoptimization. MathematicalPro-
gramming,175:69–107,2019.
[17] S.J.Reddi,S.Kale,andS.Kumar.OntheconvergenceofAdamandbe-
yond.InInternationalConferenceonLearningRepresentations,2018.
[18] B.Wang,H.Zhang,Z.Ma,andW.Chen.ConvergenceofAdaGradfor
non-convexobjectives:Simpleproofsandrelaxedassumptions. InThe
ThirtySixthAnnualConference onLearningTheory,pages161–190.
PMLR,2023.
[19] G.Wang,S.Lu,Q.Cheng,W.-w.Tu,andL.Zhang. SAdam:Avariant
ofAdamforstronglyconvexfunctions.InInternationalConferenceon
LearningRepresentations,2019.
[20] R. Ward, X. Wu, and L. Bottou. Adagrad stepsizes: Sharp conver-
genceovernonconvexlandscapes. TheJournalofMachineLearning
Research,21(1):9047–9076,2020.
[21] X.Wu,Y.Xie,S.S.Du,andR.Ward. Adaloss:Acomputationally-
efficientandprovablyconvergentadaptivegradientmethod. Proceed-
ings of the AAAI Conference on Artificial Intelligence, 36(8):8691–
8699,2022.
[22] Y.Xie,X.Wu,andR.Ward.Linearconvergenceofadaptivestochastic
gradientdescent. InInternationalConferenceonArtificialIntelligence
andStatistics,pages1475–1485.PMLR,2020.
[23] M.Zaheer,S.Reddi,D.Sachan,S.Kale,andS.Kumar.Adaptivemeth-
odsfornonconvexoptimization. AdvancesinNeuralInformationPro-
cessingSystems,31,2018.
[24] Y. Zhang,C. Chen, N.Shi, R.Sun, andZ.-Q.Luo. Adam cancon-
vergewithoutanymodification onupdate rules. Advances inNeural
InformationProcessingSystems,35:28386–28399,2022.