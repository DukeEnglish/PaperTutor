ApplicationofPromptLearningModelsinIdentifyingtheCollaborative
ProblemSolvingSkillsinanOnlineTask
MENGXIAO ZHU,UniversityofScienceandTechnologyofChina,AnhuiProvinceKeyLaboratoryofScience
EducationandCommunication,China
XINWANG,UniversityofScienceandTechnologyofChina,China
XIANTAOWANG,UniversityofScienceandTechnologyofChina,China
ZIHANGCHEN,UniversityofScienceandTechnologyofChina,China
WEIHUANG,NationalEducationExaminationsAuthority,China
Collaborativeproblemsolving(CPS)competenceisconsideredoneoftheessential21st-centuryskills.Tofacilitatetheassessmentand
learningofCPScompetence,researchershaveproposedaseriesofframeworkstoconceptualizeCPSandexploredwaystomakesense
ofthecomplexprocessesinvolvedincollaborativeproblemsolving.However,encodingexplicitbehaviorsintosubskillswithinthe
frameworksofCPSskillsisstillachallengingtask.Traditionalstudieshavereliedonmanualcodingtodecipherbehavioraldatafor
CPS,butsuchcodingmethodscanbeverytime-consumingandcannotsupportreal-timeanalyses.Scholarshavebeguntoexplore
approachesforconstructingautomaticcodingmodels.Nevertheless,theexistingmodelsbuiltusingmachinelearningordeeplearning
techniquesdependonalargeamountoftrainingdataandhaverelativelylowaccuracy.Toaddresstheseproblems,thispaperproposes
aprompt-basedlearningpre-trainedmodel.Themodelcanachievehighperformanceevenwithlimitedtrainingdata.Inthisstudy,
threeexperimentswereconducted,andtheresultsshowedthatourmodelnotonlyproducedthehighestaccuracy,macroF1score,
andkappavaluesonlargetrainingsets,butalsoperformedthebestonsmalltrainingsetsoftheCPSbehavioraldata.Theapplication
oftheproposedprompt-basedlearningpre-trainedmodelcontributestotheCPSskillscodingtaskandcanalsobeusedforother
CSCWcodingtaskstoreplacemanualcoding.
CCSConcepts:•Human-centeredcomputing→Empiricalstudiesincollaborativeandsocialcomputing.
AdditionalKeyWordsandPhrases:collaborativeproblemsolving,prompt-basedlearning,automaticcoding,naturallanguage
processing
ACMReferenceFormat:
MengxiaoZhu,XinWang,XiantaoWang,ZihangChen,andWeiHuang.XXX.ApplicationofPromptLearningModelsinIdentifyingthe
CollaborativeProblemSolvingSkillsinanOnlineTask.In.ACM,NewYork,NY,USA,22pages.https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Collaborativeproblemsolving(CPS)referstotwoormorepeopleworkingtogethertosolveaproblemusingtheir
respectiveskillsthroughinformationsharingandeffectivecommunication[39].Asoneofthemostimportantskills
inthe21stcentury,CPScompetenceiswidelyrequiredinmanyscenarios,includinglearningenvironmentsandthe
workplace.Forexample,studentsareoftenaskedbyinstructorstoworkinteamstocompletecourseprojects.Inthe
workplace,anincreasingnumberoftaskscannolongerbeaccomplishedbyindividualsbutinsteadrequiremultiple
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenot
madeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforcomponents
ofthisworkownedbyothersthanACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversorto
redistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
©XXXAssociationforComputingMachinery.
ManuscriptsubmittedtoACM
1
4202
luJ
71
]CH.sc[
1v78421.7042:viXraCSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
teammemberstoworktogether.Onaglobalscale,peoplefromdifferentcountriescollaboratetosolvehealthcrises
(e.g.,COVID-19)andfindsolutionstootherseriousglobalproblems(e.g.,pollutionandglobalwarming)[44].Inrecent
years,CPShasattractedsignificantattentionfromresearchers.TheoreticalCPSframeworkshavebeenproposedto
conceptualizethisabstractconstruct[19,39],andassessmentapproacheshavealsobeendevelopedtomeasuretheCPS
competenceofindividuals[11,37,39].AlthoughCPScompetenceisimportant,manystudentsworldwidelackthis
skill,asreportedbytheProgrammeforInternationalStudentAssessment(PISA)[39].Consequently,itisnecessaryto
deepentheunderstandingofCPStoimprovetheCPSabilitiesofstudents.
Theoreticalframeworksandmeasurementapproachesformthefoundationforconductingin-depthCPSanalyses.
Asacompositecompetence,CPSencompassesvariousaspects,suchascriticalthinking,collaboration,communication,
andinnovation[51].Itisalsoamultimodal,dynamic,andsynergisticphenomenon[41],wherecollaborationand
problemsolvingoccursynergisticallyandinfluenceeachotherdynamically.Duetothecomplexityandabstractionof
CPS,researchershaveproposedtheoreticalframeworksforconceptualizingthisconstructanddividingitintomultiple
subskills(thedetailsaredescribedinSection2.1)thatcapturedifferentaspectsofCPScompetence.
TwotypesofassessmentsareoftenusedforCPS,thetraditionalmultiple-choicemethodsandsimulated-task-based
methods.Traditionalmultiple-choiceassessmentsusetextandsometimesimagestoproviderelevantsituationstothe
participants,whothenneedtochooseamongtheavailableoptionstosolveproblemsinimaginaryCPSscenarios[17].
Inthiscase,theCPSskilllevelsofindividualscanbeassessedaccordingtotheiranswers.However,thisapproachis
considereddeficient[2]becauseitonlycaptureslimitedinformation,andmoredetailedprocessdataoncollaboration
andproblemsolvingarenotavailable.Hence,researchershaveattemptedtodevelopvirtualenvironmentstosimulate
realisticoperatingspaces[2]basedontheevidence-centereddesignapproach(ECD;[38]).Inasimulatedenvironment,
thebehavioraltrajectoriesofteammembers,includingtheiractions(e.g.,clickingthemouseandpressingcertainkeys
onthekeyboard)andcommunications(e.g.,sendingmessagestootherteammates),canbetrackedbyaloggingsystem.
Byanalyzingthesebehavioraldata,researcherscanfurtherevaluateindividuals’masterylevelincertainCPSsubskills
[2,8],whichisbeneficialforcomprehensivelyunderstandingtheirCPScompetence.Suchbehavioraldatacanoffer
insightsintohowstudentsattempttoachievegoals.However,behavioraldatacomeinverylargequantitiesandare
usuallyunstructuredduetothevarietyofobservedbehaviorsandtheinconsistentformatsusedfordatacollection.
Toquantitativelyanalyzebehavioraldata,itisnecessarytofirstcodethecollectedbehavioraldataintospecificCPS
subskills[22].Nevertheless,codingbehavioraldataischallengingduetothedifficultyencounteredwhenattemptingto
makesenseofthelargenumberofoperationalbehaviorsofparticipants.
Generally,theexistingcodingapproachescanbecategorizedintotwotypes,manualcodingandautomaticcoding
approaches.Manualcodingreferstothetraditionalmethodoflabelingobserveddatawithhumancoders(e.g.,[1,2].
Thecodingprocessusuallyinvolvestwoormorecodersworkingtogetheronthetaskbasedonamutuallyagreed-upon
codingschemabuiltonarelatedtheoreticalframework.Aftertheinitialtrainingstage,thecodersareexpectedto
reachahighlevelofagreementregardingthecodingresults.Becauseoftherigorousmanualcodingprocess,results
withhighinterraterreliabilityareconsideredreliableandsuitableforfurtheranalysis.However,manualcodinghas
limitations.Ontheonehand,itisatime-consumingandlabor-intensiveprocess[44,48].Itisnotfeasibletorelyon
manualcodingforgeneratinglarge-scaleandreal-timecodesforCPSbehavioraldata.Ontheotherhand,ifthetask
scenarioortheoreticalframeworkchanges,theentirecodingprocessneedstoberestartedfromscratch.
ToefficientlycodeCPSbehaviors,automaticcodingisconsideredacrucialsteptowardscalinguptherelated
assessmentandlearningtasksinthecontextofCPS[22,29,39].Essentially,theautomaticcodingtaskcanberegarded
asaclassificationproblem,andresearchershavebeguntoexploreanddevelopmachinelearninganddeepneural
2ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
networkmodelsforcodingexplicitbehavioraldata[12,40,53].However,traditionalclassifiersrelyonthequantityand
qualityoftheutilizedtrainingsettoachieveacceptableperformance.Inreal-worldapplications,asignificantamount
ofhigh-qualitytrainingdatamaynotbereadilyavailable.Consequently,theperformanceoftheexistingmodelsshould
beimproved,andalternativeapproachesforbuildingclassifierswithlimitedtrainingdataneedtobeexplored.
Inthisstudy,weintroduceandadoptapromptlearningstrategyanddevelopaprompt-basedlearningpre-trained
modeltoenableautomaticcodingwithoutrelyingonlargetrainingsets.Prompt-basedlearning,asreviewedin[30],is
atechniquethatguidesapre-trainedmodeltogeneratespecifictypesofoutputsbyinsertingspecificprompttexts
intotheinput.Bydesigningeffectiveprompttexts,wecanguidethepre-trainedmodeltobetterunderstandand
handleautomaticCPScodingtaskswithhighaccuracy.Furthermore,byleveragingtheadvantagesofprompt-based
learningforlow-resourcedata[21,34,55],wecanobtainsatisfactoryclassificationresultswithfewertrainingdata.
Asastartingpoint,weexploreddifferentcombinationsofpromptgenerationmethods,includingprompttemplates,
mappingsbetweenoriginallabels(i.e.,thetargetedlabelsfortrueCPSsubskills),andlabelwords(i.e.,theoutput
wordsdownstreamofthepromptmodel),aswellasdifferentpre-trainedmodels.Thetemplatesandmappingscould
begeneratedeithermanuallyorautomatically.Ourresultsrevealedthatmanuallydesignedtemplates,alongwith
manuallydesignedmappings,outperformedotherpromptgenerationmethodswhenusingtheT5[45]pre-trained
model.Wealsoconductedacomparativeanalysiswithotherautomaticcodingmodelsproposedinpreviousstudies,
suchasKNN[13],RF[44],Linear[5],CNN[26],LSTM[25],GRU[7],andpre-trainedmodelsbasedonfine-tuning,such
asFinetune-BERT[3,10],Finetune-RoBERTa[31],andFinetune-T5[6].Theresultsshowedthatthemodeldeveloped
inourstudyoutperformedtheotherapproachesandachievedthehighestaccuracy,macroF1score,andkappavalues.
Finally,weassessedtheperformanceofourmodelonsmalltrainingsetsbyreducingtheamountofinputtrainingdata
andcomparedtheperformancesofdifferentmodelswiththatofourmodel.Theresultsdemonstratedthesuperior
performanceofourproposedmodelonsmalltrainingsets.Inconclusion,thisstudymakesthefollowingcontributions.
1)Weintroduceaprompt-basedlearningpre-trainedmodeltoaddresstheproblemofcodingprocessdatainCPS
tasks.
2)Theperformanceofourproposedmodeliscomparedwiththatofotherautomaticcodingmodelsonanempirical
datasetderivedfromaCPStasktodemonstratethesuperiorperformanceofourmodel.
3)Byusingpartialdatafromthetrainingset,wefindthattheperformanceofourmodelissatisfactorywhenlimited
trainingdataareavailable,andourmodeloutperformstheothermodels.
2 RELATEDWORK
Inthissection,wefirstreviewvariousCPSframeworksproposedinpreviousresearch.Next,weprovideanoverviewof
theprogressmadebycodingapproachesdevelopedforCPSprocessdata,includingbothmanualandautomaticcoding
methods.Finally,webrieflyreviewtheexistingpromptlearningmethods.
2.1 CPSFrameworks
Tocomprehendthebehaviorsandprocessesofindividualsincollaborativeproblemsolvingtasks,andtoassesstheir
CPScompetence,researchershavedevelopedCPSframeworksthatcanoperationalizethiscomplexconstruct(e.g.,
[15,19]).Mostframeworksshareacommonstructure,encompassingbothasocialaspectrelatedtocollaboration
andacognitiveaspectrelatedtoproblemsolving[18].IninternationalCPSassessments,thetwomostwidelyused
frameworksaretheAssessmentandTeachingof21stCenturySkills(ATC21s)andthePISAAssessment[18],andwe
3CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
brieflyreviewthesetwotheoreticalframeworks.Inaddition,wereviewathirdframework[1]developedforresearch
purposes,whichisalsoadoptedinthecurrentstudy.
AsaCPSframeworkdevelopedforassessment,ATC21s[19]identifiesbothsocialandcognitiveaspects.Thesocial
aspectcoversthreecomponents,includingparticipation,perspectivetaking,andsocialregulation.Participationisalong-
termprocessofbecomingacommunitymember,whichinvolvesinteraction,action,andtaskcompletion.Perspective
takingreferstounderstandingteammembers’knowledge,resources,andskills,andrespondingtoothers.Thefinal
component,socialregulation,pertainstothestrategiesandteamprocessesgroupmembersemploytofacilitateCPS,
includingnegotiating,takinginitiative,andassumingresponsibility.Thecognitiveaspectconsistsoftwodimensions,
includingtaskregulationandlearningandknowledgebuilding.Taskregulationisrelatedtoproblemsolvingcapabilities,
suchassettinggoals,managingresources,exploringproblems,andaggregatinginformation.And,learningandknowledge
buildingreferstotheabilitiestoplan,execute,reflect,andmonitorproblemsolving.
Asadistinctconstruct,thePISAframework[15]iscomposedoffourproblemsolving(alsoregardedascognitive)
competenciesandthreecollaborative(alsoregardedassocial)competencies.Specifically,thefoursubdimensionsofthe
problemsolvingdimensionincludeexploringandunderstanding,representingandformulating,planningandexecuting,
andmonitoringandreflecting.And,thethreesubdimensionsofthecollaborativedimensioncompriseestablishingand
maintainingsharedunderstanding,takingappropriateactionstosolvetheproblem,andestablishingandmaintaining
grouporganization.Thesetwodimensionsinteractandcross,formingamatrixwith12subskills.
Inadditiontotheabovetwoframeworks,Andrewsandcolleagues[1]alsoproposedaCPSontologyframeworkto
conceptualizetheCPSconstructmainlyforresearchpurposes.TheframeworkincludesnineCPSsubskillsacrosstwo
dimensions.Thefirstdimension,thecognitivedimension,involvesfivesubskills,includingexploringandunderstanding
(CEU),representingandformulating(CRF),planning(CP),executing(CE),andmonitoring(CM).Theseconddimension,
thesocialdimension,involvesfoursubskills,includingmaintainingcommunication(SMC),sharinginformation(SSI),
establishingsharedunderstanding(SESU),andnegotiating(SN).Inthecognitivedimension,exploringandunderstanding
involvesactionsforexploringproblem-relatedinformationandbuildingamutualunderstandingofthegivenproblem.
Representingandformulating referstoactionsandcommunicationthataimtobettervisualizeproblemsandform
hypotheses.Planningconcernscommunicationsthatareusedforthedeterminationoftasktargetsandsolutions,as
wellassubsequentrevisionsandrefinements.Executinginvolvesactionsandcommunicationsduringtheexecutionofa
taskcompletionplan.Finally,monitoringreferstotheactivitiesrelatedtodeterminingtaskcompletionprogress.Inthe
socialdimension,maintainingcommunicationisaboutcommunicatingcontentthatisirrelevanttotasks,whilesharing
informationisaboutcommunicatingcontentthatisrelevanttotasks.Establishingsharedunderstandingreferstogroup
memberstryingtounderstandeachother’sperspectives.Thelastsubskill,negotiating,involvescommunicationsused
tounderstandconflictsandproposesolutionstoreachaconsensus.Sincethetwosubskillsofexecutingandmonitoring
canoccurineitheractionsorchats,eachofthemisfurthersplitintotwocomponents,yieldingexecutingactions(CE),
executingchats(CEC),monitoringactions(CM),andmonitoringchats(CMC).Thus,theproposedCPSframeworkincludes
elevensubskills.Theframeworkprovidesatheory-drivenrelationshipoftheCPSsubskillsassociatedwithexplicit
behaviorswhenparticipantsperformtasks.Theempiricalanalysisconductedinthisstudyusedadatasetcollected
fromathree-resistortask[1],andthisCPSontologywasinitiallydevelopedforhumancoding.ThisCPSframework
wasthusadoptedforbuildingtheclassifiers.However,ourproposedmethodandmodeldonotdependonanyspecific
CPSframeworkoranyCPStaskandcanbegeneralizedtootherpracticesandapplicationsbeyondCPS.
4ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
2.2 CodingofCPSactivities
ToassociateconversationalandbehavioraldatafromCPSactivitieswithCPSskills,researchersrelyoncodingmethods,
whichcanberoughlydividedintomanualandautomaticcodingmethods.Wefirstreviewthemanualcodingapproaches
usedinthefield.Manualcodingprocessesusuallydependoncodingschemes[13].IngeneralCSCWcommunities,
researchershavedevelopedvariousmanualcodingschemestoservedifferentpurposes.Forinstance,inastudyof
computer-supportedcooperativelearningscenarios,[52]proposedamultidimensionalencodingmethodfordialectical
knowledgeconstruction.Asanotherexample,[4]developedtwocomplementaryencodingschemeswithdifferent
granularitiestoannotatedialogsinpeercollaborationscenarios.Additionally,[24]codedinteractivebehaviorssuchas
negotiationandelaborationbetweendifferentparticipants.
With the recent studies on CPS, researchers have also developed coding schemes that fit the CPS simulation
environment.Forexample,[49]proposedahierarchicalCPScodingschemethatcaneffectivelycaptureparticipants’
behavioralindicatorsandassociatethemwithCPSskills.[16]proposedanontologyframeworkforCPS,encoding
participants’chatsandactions(e.g.,changingtheirresistancevalues)into23CPSsubskills.Moreover,[28]proposeda
codingschemecombiningthe12CPSsubskillsclassifiedbythePISA2015andstudents’masterylevels.
Ingeneral,themanualcodingmethodisbasedonacertaintheoreticalframeworkthatmapsapieceofexplicit
behaviortoaspecificskill.However,thismethodhassignificantlimitations.Trainedratersneedtogothroughthe
inputdatamanually,checkalargenumberofcorpora,andthenmapthemtospecificCPSsubskills.Additionally,coders
needtoensureratingconsistencyamongthemselves,whichrequiresfrequentdiscussionstoproduceconsistentcoding
results.Thisprocessisundoubtedlytime-consumingandlabor-intensive[44,48].
Withthedevelopmentoftechnology,advancedmethodshavebeenappliedtoimplementautomaticcoding,thereby
facilitatingtaskssuchascodingtextandprovidingautomatedfeedback[22,56].Recently,researchersintheCPSfield
havealsoexploredautomaticcodingapproaches.Overall,automaticcodingapproachescanbedividedintotwotypes,
i.e.,machinelearninganddeeplearningmethods.Onemachinelearningmethodusedalinearchain-basedconditional
randomfield(CRF)toconstructthesequentialdependenciesofdialogcontent,andtheauthorsdevelopedanautomatic
codingsystemnamedCPS-rater[22].Thismethodwasprovedtobemoreeffectivethanthatof[14],whichtreated
differentdialogsindependently.Inanotherstudy[23],preselectedn-gramsandemotionswereusedtomodelfour
aspectsofCPS(i.e.,sharingideas,regulatingproblem-solvingactivities,negotiating,andmaintainingcommunication).
Additionally,theKNNclassifierwasalsousedforCPScoding[13]andwasfoundtobemoresatisfactorythannaive
Bayesclassificationandcomparabletomanualencoding.
Inadditiontotheaforementionedautomaticencodingmethods,[43,44]employedamoreadvanceddeeptransfer
learningapproachcalledbidirectionalencoderrepresentationsfromtransformers(BERT)toexplorethefeasibility
ofusingthismodeltoencodeCPSdataobtainedfromsimulatedindoorenvironmentsorrealscenes[44].Theyalso
analyzedthegeneralizabilityofseveraldifferentNLPmethods(BERT,n-grams,andwordcategories)forencodingtasks
[43].Asreviewedabove,automaticCPSdatasetcodingisanemergingresearchdirection,andmoreefficientautomatic
codingmodelsneedtobedeveloped.Additionally,sinceexistingautomaticcodingmodelsrelyonexistingmanual
codingdatasetsfortraining,generalizationoftheexistingmodelrequiresmanuallycodeddatasets.Toimprovethe
generalizabilityofanautomaticcodingmodel,weaimtodevelopamodelthatdependsonasmallamountofexisting
dataforautomaticcodingandcanalsoachievearelativelyhighaccuracy.
5CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
2.3 PromptLearningParadigm
Beforeintroducingpromptlearning,webrieflyreviewthepre-trainedlanguagemodel(PLM)concept,whichplaysa
vitalroleinfacilitatingthedevelopmentofpromptlearningmethods.Whentrainedonlarge-scaleopencorpora[35],
PLMachievessuperiorperformanceindiverseNLPtasks,suchassentimentanalysis[42,47]andmachinetranslation
[33],duetoitsabilitytoembedabundantsemanticandsyntacticinformation.Additionally,themodelcanbeadapted
todifferentdownstreamtasksbylearningdomain-specificknowledgeviafine-tuning[30].Nevertheless,fine-tuning
aPLMcanbechallengingduetotheneedforlarge-scaledatasetsandtheinvolvementofanenormousnumberof
parameters.Thischallengeisparticularlypronouncedinlow-resourcescenarios[21].Toaddressthislimitation,a
newparadigmcalled"prompt-basedlearning",whichallowsPLMstoprocessdownstreamtasksthroughpromptshas
emerged[30].
UnlikePLMfine-tuningforadownstreamtask,theprompt-basedmethodreformulatesadownstreamtaskusing
a textual prompt, effectively turning it into a masked word classification task [35]. We take text classification as
anexample.Giventheinputsentence,"Ilovethismovie",themodelisexpectedtooutput"positive"or"negative"
informationaboutthemeaningoftheinputsentence.However,PLMsdesignedfortextgenerationcannotdirectly
handleclassificationtasks.Byproperlytransformingtherawinputusingtheprompt-basedmethod,wecanenable
text-generatedPLMstoperformclassificationaswell.Utilizingtheaboveexample,theprompt-basedmethodinvolves
addinga[MASK]tokentotheinputsentence,structuringtheinputsentenceas"Ilovethismovie.Itisa[MASK]movie".
Themodelcanthengenerateoutputwordswiththeirassociatedprobabilities,suchas"funny","interesting",or"boring"
(referredtoaslabelwords).Thefirsttwowordsrepresentpositiveemotions,whilethethirdwordindicatesanegative
emotion.Theoutputwordscanthenbemappedtothecorrespondingemotionwordsforclassificationpurposes,and
thisstepisknownaslabel-wordmapping.Prompt-basedmodelsmodifytheinputtoadaptapre-trainedmodelto
variousdownstreamtasks,eliminatingtheneedtotrainaseparatemodelforeachtaskandreducingtherequirement
forencodinglarge-scaledatasets[30,35].Therefore,prompt-basedlearningmethodscanachieveexcellentperformance
infew-[9,20,34]andzero-shot[50,54,55]tasks.
Duetotheadvantagesofpromptlearning,ithasgarneredincreasingattentioninrecentyears.Forinstance,inthe
fieldofmentaldiseasediagnosis,aprompt-basedtopic-modelingmethodwasdevelopedtodetectdepressionbasedon
question-and-answerdatagatheredduringinterviews[21].Researchersutilizedthepromptlearningparadigmand
madetopic-wisepredictionsusingthecharacteristicsoftheinterviewdatatoconstructafusionmodelfordetecting
depression.Itisworthnotingthatthesamplesizeofpeoplewithmentalillnessisrelativelylimited,resultingineven
lessavailabledataavailable.Overall,thestudydemonstratedthattheprompt-basedmodeliswellsuitedforaddressing
thechallengeofinsufficienttrainingdata.Themodelwasalsoproventobeefficientinpersonalityandinterpersonal
reactivitypredictiontasks.Forexample,[27]employedaprompt-basedpre-trainedmodeltoparticipateinacompetition
involvingpersonalitypredictionandreactivityprediction,achieving1stplaceinbothsubtasks.Theadvantageof
thedesignedpromptisthatitprovidesadditionalpersonalizedinformationthatenhancestheperformanceofthe
pre-trainedmodel.Furthermore,theprompt-basedmethodhasbeenappliedinaffectivecomputing.Forexample,[35]
conductedanempiricalstudyonprompt-basedsentimentanalysisandemotiondetection.Theydemonstratedthe
biasesofPLMsinpromptingbycomparingtheperformancesofdifferentprompttemplates,label-wordforms,and
othercontrolvariables.Thisstudyhighlightedtheimportanceofpromptengineeringandlabel-wordselections.Itis
evidentfromtheaforementionedstudiesthatprompt-basedmodelsexcelinclassificationtasksandarealsoeffective
withsmalltrainingdatasets.Inthisstudy,theprompt-basedmodelisappliedtoautomaticallycodetheprocessdataof
6ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
CPStasks.Giventhepivotalroleofthepromptmethodandpre-trainedmodelsinpredictionperformance,thisstudy
aimstodeterminetheappropriatepromptgenerationmethodandpre-trainedmodel.
3 PRELIMINARYANALYSIS
In this section, we first introduce the process of collecting and building the utilized dataset, which encompasses
participants’behaviorsobservedduringCPSactivities.Next,wepresentavisualizationofthedataset,showcasingthe
proportionsofeachsubskill(referredtoaslabels)andthedistributionofchatdatalengths.Finally,wedelveintothe
datapreprocessingstepsconductedonthedatasettoprepareitforbeinginputintothemodel.
3.1 DataCollection
Task. Thedataforthisstudywerecollectedfromanonlinethree-resistortask.Thetaskinvolvesapplyingrelevant
physicalknowledgeofseriescircuitstoadjustresistorvalues,ensuringthatthevoltageacrosstheresistancesatisfies
therequirementsofthetask.Atotalof378participantswererecruited,andrandomlydividedinto126groups.Each
groupconsistedof3members,witheachmemberresponsibleforoneresistor.
TheoperationinterfaceisrepresentedinFig.1.Atthetopofthescreen,theknownconditionsandtargetsare
displayed.Thescreenpresentsacompletecircuitstructure.Thegoalofeachparticipantwastoadjusttheresistor
valuestoreachthetargetvoltage.Becauseeachmemberreceivedvaryinginformationandtheresistorsintheseries
circuitinfluencedeachother’svoltage,thegroupmembersneededtoengageindiscussionsandcollaboratetocomplete
thetask.Tofacilitatecommunicationamongthegroupmembers,achatboxwasprovided.Additionally,participants
canutilizethecalculatorintheupper-rightcornerofthescreenforcalculations.ToaccommodatetheCPScompetence
levelsofdifferentgroups,thetaskwasdividedintofourlevels,primarilydifferingintheknownconditionsandgoals,
asoutlinedinTable1.
Fig.1. Ascreenshotofthethree-resistortaskinthesimulatedenvironment.
7CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
Table1. Theconditionsettingsfordifferenttasklevels.
TaskLevel ExternalVoltage(E) ExternalResistance(R0) GoalVoltages
1 Knownbyallteammates Knownbyallteammates Sameforallteammates
2 Knownbyallteammates Knownbyallteammates Differentforallteammates
3 Unknownbyteammates Knownbyallteammates Differentforallteammates
4 Unknownbyteammates Unknownbyteammates Differentforallteammates
Dataset. Thedatawererecordedbyaloggingsystem,whichincludedparticipants’information,suchasstudent
IDsandgroupnames,aswellastaskinformationandparticipants’behaviorsduringtheactivities.Theparticipants’
behavioraldatacouldbeclassifiedintotwocategories.Thefirstcategoryinvolvedmanipulatingthesystem,suchas
changingtheresistororperformingcalculations,andthesecondcategoryincludedchattingwithothermembersin
thechatbox,suchas"Ithinkitwillmakeit",or"Alright,let’sdoabigone".Intotal,wecollected50,817piecesofdata,
comprising15,950chatrecordsand34,867manipulationrecords.
3.2 DatasetBuilding
ThecollectedexplicitbehavioraldataweremanuallycodedbythreecodersbasedontherubricoftheCPSontology
framework[1].Eachrecordcontainedinformationoneitheraninteractionwiththesimulatedtasksystemorasingle
chatmessagebetweenteammembers.Forexample,thechatmessage"weneed6.69𝑉 acrossourresistors" couldbe
classifiedasplanning(CP).Theinterraterreliabilitywassatisfactorywithkappa=.93forthe20%triple-codedsamples.
Eventually,50,817logentrieswereclassifiedinto11CPSsubskills,andthechatdatacovered8subskills.Table2displays
somecodingexamples.AsshowninTable2,themanipulationdatacouldbemappedtoaspecificsubskillsincetheyare
generallydeterministic.Itismorechallengingtoaddresschatdataduetotheirdiversityandirregularity.Toagreat
extent,chatscanbeassociatedwithallthesubskills,whichsignificantlyincreasesthecodingdifficultyofthemodel.
Thus,thisstudyfocusedprimarilyonautomaticchatdatacoding.
3.3 DataDescriptions
Weconductedafundamentalstatisticalanalysisofthesubskillcategoriesinthechatdata.Specifically,wecalculated
theproportionofeachtypeandcountedthenumberofrelatedwordsthatappearedineachchatmessagetobetter
understanditscharacteristics.
Table 3 shows the frequency and proportion of each classified subskill in the chat data. This reveals that the
chatdatawereunevenlydistributedacrosscategories.Morethan70%ofthechatdatapertainedtosocialsubskills.
Sharinginformation(SSI)appearedmostfrequently,followedbyestablishingsharedunderstanding(SESU).Conversely,
representingandformulating(CRF),andplanning(CP)weretheleastcommonlyusedsubskills.Inconclusion,social
subskillsareemployedmorefrequentlythancognitivesubskillsduringgroupcommunication.Theunevenproportions
ofsubskillsposechallengeswhenautomaticallycodingmodels.Themodelneededtoavoidshowingapreferencefora
specificcategoryduringthetrainingprocess.Thisensuredthateveniftheoverallpredictionaccuracywashigh,its
performanceintermsofpredictingfewerproportioncategorieswasnotextremelypoor.Thus,wetookthisfactorinto
accountwhenevaluatingthemodelperformance.
ThedistributionofthechatdatalengthisdepictedinFig.2.Thedistributionexhibitedaskewedpatternandwas
predominantlycomposedofshortsentences.Giventhat96.68%ofthechatdatacontained16wordsorfewerand
8ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
Table2. Examplesofcollecteddataandtheirencodingresults.
Dimension Subskill Label Example Type
Unsystematic/non-strategic use of
ExploringandUnderstanding CEU taskcomponentsorstrategydiscov- Manipulate
ery
"I feel like it’s the same problem
Cognitive when we had to find an unknown
RepresentingandFormulating CRF Chat
voltagesourceresistanceincircuit
analysis"
Planning CP "Weneed6.69vacrossourresistors" Chat
Engageinthebehaviorsconsistent
withthestatedplanforthelevel(e.g.,
ExecutingActions CE Manipulate
changeresistortothesuggestedre-
sistancevalue)
ExecutingChats CEC "Adjustyoursto300ohms" Chat
MonitoringActions CM Clicksubmit(submitvalues) Manipulate
Statewhereyouareorteamisabout
MonitoringChats CMC Chat
thegoalstate("I’mgood")
Off-topicconversationsnotrelated
MaintainingCommunication SMC tothetask(e.g.,tryingtodetermine Chat
thegroupmembers’realname)
Social
SharingInformation SSI "I’monboard1" Chat
Requestinformation:"Whatisyour
EstablishingSharedUnderstanding SESU Chat
resistance?"
Express disagreement: "That’s not
Negotiating SN Chat
right"
Table3. Thefrequencyandproportionofeachsubskillinallchatdata.
Label CRF CP CEC CMC SMC SSI SESU SN
Frequency(n) 356 1066 1348 1193 1292 6177 3317 1149
Proportion(%) 2.24 6.71 8.48 7.50 8.13 38.85 20.81 7.23
consideringthecomputationalefficiencyofthemodel,wechoseamaximumsentencelengthof16forthesubsequent
experimentalmodelsettings.
3.4 DataPreprocessing
Tofacilitatethesubsequentdataserializationprocess,weperformedtextreplacementsasoutlinedinTable4.We
appliedseveralstepstoprocessthechatdata.First,wereplacednounsrelatedtothethree-resistortaskwithspecial
tokens.Forexample,fortherelevantexpressionsofthefourresistorsR0-R3inthecircuit,wereplacedthemwith
[𝑅_𝑧𝑒𝑟𝑜−𝑅_𝑡ℎ𝑟𝑒𝑒]andaddedthemtothepre-trainedmodel.Similarly,wealsoreplacedtheexpressionsofthevoltage
values,currentvalues,andpurenumericalvalues.Inaddition,wereplacedcolloquialabbreviationsrelatedtovoltageor
resistance.Third,expressionsreferringtoteammembers’nicknames(e.g.,tiger,lion)werealsosubstitutedwiththe
commonnamesofpeopletohelpthelanguagemodelrecognizethemasdifferentmembersofateam.
9CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
Fig.2. Thedistributionofchatdatalength.
Table4. Thereplacementrulesforsomespecialdataduringpre-processing.
Content Target
Somenumericvalues(integer,float) [number]
𝑅0−𝑅3,𝑟0−𝑟3 [𝑅_𝑧𝑒𝑟𝑜]-[𝑅_𝑡ℎ𝑟𝑒𝑒]
Somevoltagevalues(integer,float)+Vorvolts [voltage]
Somecurrentvalues(integer,float)+aorA [current]
Voltorvoltage voltage
R,r,resorresistor resistor
4 METHODOLOGY
The proposed method consists of a data filtering module and an automatic coding module. The filtering module
preprocessestherawdata,whichisfollowedbymodelingtheinputdatausingtwokindsofclassificationmethods
basedontheircategories(chatormanipulationdata).Thisconstructisprimarilyinspiredbythedesignof[1].Inthe
automaticcodingmodule,wepresentaformalformulationanddetailedproblemdescriptionsasfollows.
4.1 ProblemFormulation
Inacollaborativeproblemsolvingactivity,ourgoalistopredictaCPSsubskillYcorrespondingtoaparticipant’s
explicitbehaviorXatacertaintime.
10ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
Y
Coding
CRF CP CEC CMC SMC SSI SESU SN
Mapping
…
Word Word Word Word Word Word Word Word Word Word Word Word Word
Prediction
…
[CLS] + xx 11 + xx 22 + + xx nn + , + it + is + [MASK] + [SEP]
Templating
…
X = xx 11 + xx 22 + + xx nn
Fig.3. Theprompt-basedfine-tuningprocesstocodethechatdata.
4.2 Prompt-BasedCodingMethodforChatData
Weutilizedaprompt-basedapproachtoenablethepre-trainedlanguagemodeltoautomaticallycodetheCPSchatdata.
ThisprocessisillustratedinFig.3.Specifically,foreachpieceofchatdata,wefirstconcatenateditwithamanually
definedtemplateasfollows,
𝑇(𝑋)= [𝐶𝐿𝑆]𝑋,𝑖𝑡𝑖𝑠[𝑀𝐴𝑆𝐾][𝑆𝐸𝑃] (1)
where𝑇 representsamodifiedvectorembeddingthatincorporatestheprompt(inthiscase,theprompttemplateis
“itis[MASK]”);𝑋 correspondstotheembeddingoftherawchatdata;[𝐶𝐿𝑆]and[𝑆𝐸𝑃]denotethebeginningandend
markersofasentenceinthepre-trainedlanguagemodel,respectively;and[𝑀𝐴𝑆𝐾]isthesymbolofthepositiontobe
predictedbythemodel.
Afterobtainingthetemplates,weusedthepre-trainedmodeltopredicttheprobabilityofgeneratingeachword
atthe[MASK]position.Toelaborate,weconstructedavocabulary𝑊 = 𝑤 1,𝑤 2,𝑤 3,...,𝑤 𝑛,andtheprobabilityof
generatingtheword𝑤 𝑡 is,
𝑃(𝑤 𝑡)=𝑝𝑟𝑒𝑑𝑖𝑐𝑡(𝑇(𝑥),𝑃𝐿𝑀,𝑤 𝑡) (2)
where𝑃𝐿𝑀representsthepre-trainedmodeland𝑝𝑟𝑒𝑑𝑖𝑐𝑡(·)denotestheuseofthe𝑃𝐿𝑀topredicttheprobabilityof
[𝑀𝐴𝑆𝐾]belongto𝑤 𝑡 intheembeddingof𝑇(𝑥).Thus,inthisequation,𝑃(𝑤 𝑡)representstheprobabilityofeachword
𝑤
𝑡
(i.e.,thewordinthevocabulary)beinggeneratedinthe[𝑀𝐴𝑆𝐾]positionbythepre-trainedmodel.
11CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
Afterpredictingtheprobabilityofeachword,themodelmappedthelabelwordstotheoriginallabelbycalculating
thetotalprobabilitiesofeachlabelwordassociatedwiththelabel.Specifically,ifthe𝑠thlabelisassociatedwith𝑘
words,thentheprobabilityofthefinalautomaticcodingofthe𝑠thlabelis,
𝑘
∑︁
𝑃(𝑌 =𝑠)= 𝑤 𝑖 (3)
𝑖
wherethepredictionprobability𝑃 isthesumofallprobabilitiesfortheassociatedlabelwords.Ultimately,theresult
ofautomaticcoding𝑌 correspondstothelabelwiththehighestprobability.Thus,theobjectivefunctioncanbedefined
asfollows,
𝑌 =𝑎𝑟𝑔𝑚𝑎𝑥 𝑠𝑃(𝑌 =𝑠) (4)
where𝑎𝑟𝑔𝑚𝑎𝑥 𝑠 isusedtofindtheargumentthatmaximizesagivenfunction.
4.3 Rule-BasedCodingMethodforManipulationData
Weusetherule-basedmodelforcodingmanipulationdata.Becausetheactiontypewasdefinite,wecouldcodethe
manipulationdatausingaone-to-onemappingstrategyinvolvingtherelevantCPSsubskills.Forexample,actionssuch
as"openZoom"and"viewboardinZoom"werecodedasmonitoringactions(CM).Anotherdirectlycorresponding
actionwas"performcalculatorwithXXX",whichcouldbecategorizedasexecutingactions(CE).However,concerning
actionsthatinvolvechangingthevalueofaresistorfromvalueAtovalueB,theycouldbeclassifiedaseitherexecuting
actions(CE)orexploringandunderstanding(CEU),dependingonthegroupstate.Ifagroupalreadyhadaplan,the
actionwaslabeledasCE.Ifagroupisinanexploringphase,theactionislabeledasCEU.Overall,mostmanipulation
datacanbedirectlycodedthroughone-to-onemapping,butsomemayalsorequirecodingtechniquesbasedonthe
specificproblemsolvingstagesofthegroupsduringtheirtasks.
5 EXPERIMENTANDEVALUATION
Inthissection,wepresenttheresultsofthreeexperimentsaimedatdemonstratingtheadvantagesoftheprompt-based
learningpre-trainedmodelinCPSbehavioraldataclassificationtasks,especiallyforcaseswithsmallsamplesizes.The
firstexperimentfocusesondeterminingthemosteffectivepromptmethodandpre-trainedmodelcombination,inwhich
casetheprompt-basedlearningpre-trainedmodelcanachievesuperiorperformance.Thesecondexperimentinvolves
acomparativeanalysis,pittingourmodelagainstdifferentclassificationmodelsproposedinpreviousautomaticCPS
codingstudies,includingbothmachinelearninganddeeplearningbasedmodels.Thefinalexperimentaimstoverify
thesuperiorityoftheprompt-basedlearningpre-trainedmodelintaskswithsmallsampledatasets.Weevaluatethe
performanceofthemodelusingaccuracy,themacroF1score,andkappa.Theformulasforthesemetricsareprovided
inequations(5)−(7)asfollows,
𝑇𝑃+𝑇𝑁
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦= (5)
𝑇𝑃+𝐹𝑃+𝑇𝑁 +𝐹𝑁
where𝑇𝑃 (truepositives)denotesthenumberofcorrectlyclassifiedpositivelabels;𝑇𝑁 (truenegatives)denotesthe
numberofcorrectlyclassifiednegativelabels;𝐹𝑃 (falsepositives)denotesthenumberofincorrectlyclassifiedpositive
labels;and𝐹𝑁 (falsenegatives)denotesthenumberofincorrectlyclassifiednegativelabels.
12ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
𝑀𝑎𝑐𝑟𝑜𝐹1𝑠𝑐𝑜𝑟𝑒 =
𝐹1𝑠𝑐𝑜𝑟𝑒 𝑐𝑙𝑎𝑠𝑠1+𝐹1𝑠𝑐𝑜𝑟𝑒 𝑐𝑙𝑎𝑠𝑠2+...+𝐹1𝑠𝑐𝑜𝑟𝑒
𝑐𝑙𝑎𝑠𝑠𝑁 (6)
𝑁
where𝑁 isthenumberofclassesorcategoriesintheclassificationproblem.
𝐾𝑎𝑝𝑝𝑎=
𝑃 0−𝑃
𝑒
(7)
1−𝑃
𝑒
where𝑃 0= 𝑇𝑃+𝑇𝑇 𝑁𝑃+ +𝑇 𝐹𝑁 𝑃+𝐹𝑁,and𝑃 𝑒 = (𝑇𝑃+𝐹𝑃)∗(𝑇 (𝑇𝑃 𝑃+ +𝐹 𝑇𝑁 𝑁)+ +𝐹(𝑇 𝑃𝑁 +𝐹+ 𝑁𝐹𝑃 )2)∗(𝑇𝑁+𝐹𝑁) .
Theformertwometrics,accuracyandmacroF1scorearecommonlyusedinclassificationtasks.Giventheimbalanced
categoriesofourdataset,weusedthemacroF1scoretoassesstheperformanceofthemodel.Additionally,weemployed
kappa to measure the consistency between the results of the model’s coding and manual coding, following the
guidelinesoutlinedin[36].Akappavalueof0.60indicatesacceptableconsistency,0.80representsarelativelyhigh
levelofconsistency,and0.90suggestsnearlyperfectconsistency[36].
5.1 Experiment1:ComparisonAmongDifferentPromptMethods
Inapromptmethod,theselectedpre-trainedmodeliscrucialtotheperformanceoftheresultingmodel.Atthesame
time,forpromptmethodsusingthesamepre-trainedmodel,differenttrainingstrategiescanalsoleadtosignificant
performancedifferences.ToachievethebestCPSclassificationperformance,wecomparedfourcommonpre-trained
models,namely,BERT[10],RoBERTa[32],GPT-2[46],andT5[6].Wedesignedfourdifferenttrainingconditionsand
describedthemasfollows.
Manual.Alltemplatesandmappingsbetweentheoriginallabelsandthelabelwordsinthevocabularyaremanually
defined.
TrainableVerbalizer(TV).Themappingsbetweentheoriginallabelsandlabelwordsaredeterminedthroughtraining,
whilethetemplatesaremanuallydefined.
TrainableTemplate(TT).Thetemplatesareobtainedthroughtraining,whilethemappingsbetweentheoriginal
labelsandlabelwordsaremanuallydefined.
TrainableTemplateandVerbalizer(TTV).Boththetemplatesandmappingsbetweentheoriginallabelsandlabel
wordsareobtainedthroughtraining.
ExperimentalSetup. Wedividethedatasetintoatrainingset,avalidationset,andatestsetwithproportionsof
0.70,0.15,and0.15,respectively.Thenumberoftrainingepochsissetto20,thelearningrateissetto1𝑒-5,andthe
maximumsentencelengthissetto16.Foreachmodelandtrainingenvironment,weconductedmultipleexperiments
byvaryingtheseeds,whicharesetto(0,1,2).OurmodelisimplementedinPyTorchandtrainedonanNVIDIARTX
3090GPUdevice.Toeffectivelyevaluatetheperformanceofthemodel,weusetheaccuracy,macroF1score,andkappa
valuesachievedonthetestsetandcalculatetheaveragescoresderivedfromdifferentseeds.
Results. Table5summarizestheoverallperformanceofdifferentpre-trainedmodelsundervarioustrainingcondi-
tions.Fromthistable,wecanobservethatundertheT5-manualcondition,themodelexhibitsthebestperformance,
achievinganaccuracyof0.802,amacroF1scoreof0.725,andakappavalueof0.743,whichindicatesacceptable
consistencywiththemanualresults.Overall,usingT5asourpre-trainedmodelwithmanuallydefinedtemplatesand
mappingscanyieldthebestclassificationresults.
13CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
Table5. Comparisonresultsofdifferentpromptmethods.
PLM Template Accuracy MacroF1Score Kappa
Manual 0.785 0.699 0.719
TV 0.791 0.712 0.728
BERT
TT 0.774 0.702 0.706
TTV 0.784 0.704 0.718
Manual 0.792 0.710 0.733
TV 0.800 0.714 0.741
RoBERTa
TT 0.790 0.706 0.726
TTV 0.790 0.725 0.727
Manual 0.782 0.695 0.720
TV 0.785 0.709 0.722
GPT-2
TT 0.771 0.657 0.705
TT 0.771 0.658 0.705
Manual 0.802 0.725 0.743
TV 0.792 0.720 0.730
T5
TT 0.782 0.714 0.716
TTV 0.780 0.710 0.715
5.2 Experiment2:ComparisonwithOtherTextClassificationModels
Next,wecomparetheperformanceofthepromptlearningmodelwiththatofothertextclassificationmodels.Weselect
ninebaselinetextclassificationmodelsbasedonpreviousstudiesconcerningCPSautomaticcoding,aswellasother
commonlyusedtextclassificationmodels.Thesebaselinemodelscanbeclassifiedintothreecategories,n-grambased
methods,deeplearningmethods,andfine-tuningbasedmethods.
N-grambasedmethods.Thisclassofmethodsusesann-grammodeltodeterminethefrequenciesofwordgroups
andappliesTF-IDFforfeatureengineeringtoprovideinputfordownstreamclassificationmodels.Forthedownstream
classificationmodels,wechooselinear,K-nearestneighbors(KNN),andrandomforests(RF)classifierstoperformthe
finalautomaticcodingtask.
Deeplearningmethods.Thisclassofmodelsusesdeeplearningmethodstoextracttextfeaturesandachievescoding
vialinearneuralnetworks.Inthefeatureextractionstage,wechoosetheGatedRecurrentUnit(GRU),LongShort-Term
Memory(LSTM),andConvolutionalNeuralNetwork(CNN)toprocessthetextdata.
Fine-tuningbasedmethods.Similartotheprompt-basedcodingmethodproposedinSection4.3,thistypeofmethod
alsousesapre-trainedmodel,withthedifferencebeingthatthesemethodsdirectlyusealinearneuralnetworkto
performautomaticcoding.
Experimentalsetup. ThegeneralsetupoftheexperimentsremainsthesameasthatdescribedinSection5.1but
with85%ofthetotaldataasthetrainingset.Moresetupdetailsregardingthecomparisonexperimentsareasfollows.
N-grambasedmethods.Inthen-grambasedmethods,wesetnto3andthemaximumnumberoffeaturesinTF-IDFto
10000.Forthedownstreamclassificationmodels,thesetupsareasfollows.1)Linearusesatwo-layerfullyconnected
neuralnetwork,andthenumberofneuronsissetto300.2)KNNcalculatesthedistancesbetweensamplestocomplete
theautomaticcodingtask,andthe𝐾valueissetto5.3)RFusesmultipleweakclassifiers(decisiontrees)forautomatic
encoding,andwesetthenumberofweakclassifiersto10.
14ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
Table6. Comparisonresultsofdifferentclassificationmodels.
Model Accuracy MacroF1Score Kappa
KNN 0.574 0.482 0.450
N-grambasedmethods Linear 0.621 0.487 0.498
RF 0.714 0.615 0.619
CNN 0.659 0.524 0.543
Deeplearningmethods GRU 0.724 0.632 0.639
LSTM 0.608 0.453 0.469
Finetune-BERT 0.782 0.726 0.716
Finetune-RoBERTa 0.797 0.733 0.738
Finetune
Finetune-T5 0.797 0.697 0.736
Prompt-BERT 0.795 0.734 0.733
Prompt-RoBERTa 0.801 0.728 0.741
Prompt
Prompt-T5 0.804 0.743 0.746
Deeplearningmethods.Inthedeeplearningmethods,wesetthemaximumtextlengthto20.Thefeatureextraction
methodsaresetasfollows.1)GRUissettobebidirectional,andthenumberoflayersofhiddenlayersissetto2.Each
hiddenlayerhas256neurons.2)LSTMissetinthesamewayastheGRU,alsowith2hiddenlayersconsistingof256
neurons.3)CNNissettohave20convolutionalkernelspossessingdifferentsizes,withthesizesoftheconvolutional
kernelsrangingfrom1to20.
Fine-tuningbasedmethods.WeuseBERT,RoBERTa,andT5asourpre-trainedmodelsandsetthemaximumtext
lengthto16.
Results. Table6summarizestheresultsproducedbytheprompt-basedpre-trainedmethodandtheothercomparative
models,fromwhichitcanbeseenthatourmodelachievesthebestperformanceconcerningallthreeevaluationcriteria.
Theaccuracyis0.804,themacroF1scoreis0.743,andthekappavalueis0.746.Thiscomparisondemonstratesthat
theproposedclassificationmethodbasedondeeplearningissuperiortothetraditionalmachinelearningmethods.
Moreover,themethodsthatusepre-trainedmodels,includingpromptsandfine-tuning,achievefarbetterperformance
thanothermethods,withourproposedprompt-basedmodeloutperformingallotherapproaches.
5.3 Experiment3:StudyonSmallTrainingSets
Inthissection,weexaminetheperformanceachievedbythepromptmodelonsmallsamples.Weconductedaseries
ofexperiments,inwhichwerandomlysampledaportionoftheoriginaltrainingsetforuseasanewtrainingset.
Subsequently,weretrainedallthemodelsdiscussedinSection5.2usingthesenewtrainingsetsandevaluatedtheir
performanceonthesametestset.Weemployallthreeevaluationmetrics,namelyaccuracy,themacroF1score,and
kappa,tocomprehensivelyexaminetheperformanceofthesemodelswithdifferenttrainingsetsizes.
Experimentalsetup. Werandomlysampledvariouspercentagesoftheoriginaltrainingdatasettocreatenew
trainingsets.Specifically,weusedthefollowingpercentagestodemonstratetheresults,6%,8%,11%,14%,18%,24%,
31%,41%,53%,69%,and85%.WethenretrainallthemodelsdiscussedinSection5.2usingthesevarioustrainingset
sizes.Subsequently,wetestedtheseretrainedmodelsontheoriginaltestsetandrecordedtheirperformanceintermsof
accuracy,macroF1score,andkappa.TheresultsoftheexperimentareshowninFig.4.Sincetheaccuracyandmacro
F1scorearethesame,wepresenttheminonefigure.
15CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
Fig.4. Theperformanceofdifferentmodelsondifferentscalesoftrainingsets,includingaccuracy,macroF1score,andKappa.
Results. AsshowninFig.4,overall,theprompt-basedmodels(exceptforPrompt-T5)performbetteronthesmall
trainingsamplesthandothefine-tuningmodels.Prompt-RoBERTaandPrompt-Bertarethetwobestmodels.To
reachthesatisfactorypredictionsindicatedbythehorizontalreddottedlineintheplots,theformermodelneedsonly
approximately6%oftheoriginaltrainingset(exceptforthemacroF1scoreindicator,whichneeds11%oftheoriginal
trainingsettoachieveasatisfactoryresult),whilethelattermodelneedsapproximately11%oftheoriginaltraining
set.ThePrompt-T5modelneedsapproximately16%oftheoriginaltrainingsettoreachthemetrictargets.Although
thismodeldoesnothaveanobviousadvantageonsmalltrainingsets,itcanachievesimilaraccuracy,macroF1score,
andkappavaluestothoseofthebestmodel,Prompt-RoBERTa,whenthetrainingsetproportionexceeds18%.For
thefine-tuningmodels,exceptforFinetune-RoBERTa,whichcanachievesatisfactoryresultsintermsoftheaccuracy
andkappaindicatorswithmorethan11%oftheoriginaltrainingset,Finetune-BertandFinetune-T5relyonlarger
trainingsamplestoachievegreatresults.Additionally,theexperimentalresultsdemonstratethesignificantadvantages
ofpre-trainedmodels.Methodsthatdonotutilizepre-trainedmodels(e.g.,GRUorCNN),donotperformaswellasthe
otherapproachesevenwhentheentiretrainingsetisused.Inaddition,wefindthattheselectedpre-trainedmodel
influencesthepredictionresultsobtainedonsmalltrainingsets.Specifically,RoBERTaissuperiortoBERT,andBERTis
superiortoT5.However,T5performsbetterwhenusingtheentiretrainingset.
6 CODINGRESULTSANALYSIS
Inadditiontotheaccuracy,macroF1score,andkappaindicatorsusedtoevaluatetheperformanceoftheautomatic
codingmodels,wealsoperformedaconfusionmatrixanalysisandanerroranalysistoobservethedetailedprediction
16ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
resultsyieldedbythemodelsforeveryCPSsubskill.Weemploythetwobest-performingmodelsinExperiment3,
Prompt-RoBERTaandPrompt-BERT,onthesmalltrainingsetwith11%oftheoriginaltrainingsetasexamplesto
demonstratetheanalysisresults.
6.1 ClassConfusionAnalysis
Fig.5representstheconfusionmatrixheatmapsoftheaccuraciesofthepredictionsproducedbyPrompt-RoBERTa
(thefigureontheleft)andPrompt-BERT(thefigureontheright)fortheeightsubskillsrelativetotheactuallabels
whenusing11%oftheoriginaltrainingset.ThePrompt-RoBERTamodelattainsthehighestpredictionaccuracy(0.85)
forthesharinginformation(SSI)subskill,whileithasthelowestpredictionaccuracy(0.33)fortherepresentingand
formulating(CRF)subskill,whichisconsistentwiththesubskillfrequencyresults(seeTable3).Thismodeltendsto
confuseCRF withSSI (0.36)andtoconfusemonitoringchats(CMC)withSSI (0.23).Thereasonforthismaybethat
representingandformulating(CRF),andmonitoring(CMC)chatsbothinvolvecommunicationrelatedtotasks,thus
requiringtheproblemortherolesofteammemberstobeunderstood.Thus,themodelmayincorrectlyregardthemas
sharinginformation(SSI).However,CRF andCMCbelongtothecognitivedimension,whereasSSI belongstothesocial
dimension,whichshowsthatthemodelmakesanincorrectpredictionintermsofdimensions.Thus,themodelmaybe
improvedbyfirstconsideringpredictingthehigh-leveldimensions,andthenproceedingtomoredetailedpredictions
concerningthesubskills.Thenextpairoffrequentlyconfusedsubskillsincludesmaintainingcommunication(SMC)
andSSI (0.16),whichmayresultfromthemodelhavingtroubledeterminingwhetherthecommunicationisrelatedto
thegiventask.Overall,themodelachieveshigherpredictionaccuracyforthesubskillsofthesocialdimension(with
allaccuraciesgreaterthan0.55)thanforthesubskillsofthecognitivedimension(withsomeaccuracieslowerthan
0.50).TheconfusionmatrixproducedbythePrompt-BERTmodelissimilartothatofthePrompt-RoBERTamodel.
Thedifferencesaremainlydisplayedinthepredictionsyieldforthenegotiating(SN),representingandformulating
(CRF),andexecutingchats(CEC)subskills.Specifically,thePrompt-BERTmodelhaslowerpredictionaccuracythanthe
Prompt-RoBERTamodel.Inconclusion,thepredictionresultsobtainedbythetwomodelsonthesmalltrainingset
with11%ofthedataarebothsatisfactory.Inaddition,theresultsshowthattheoverallpredictionaccuracymaybe
improvedbyimprovingthepredictionaccuraciesattainedforthesubskillsrelatedtocognition.
Fig.5. TheconfusionmatrixheatmapoftheaccuracyofPrompt-RoBERTamodelandPrompt-BERTmodelpredictionsonthe11%of
theoriginaltrainingset.
17CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
6.2 ErrorAnalysis
WeperformanerroranalysistoillustratetheerrorsinducedbythePrompt-RoBERTaandPrompt-BERTmodels.The
followinglistssomecases.
First,weshowexamplesthatareincorrectlypredictedbythePrompt-RoBERTamodel.Thechatmessageof“click
theleadsnexttothebuttpluglookingthing” concernsplanning(CP),butthemodelconsidersitasexecutingchats
(CEC).Planninggenerallyreferstotaskstobedonethathavenotyetoccurred,whileexecutingusuallyrefersto
theimplementationofaplan.Thus,thereisanobviousdifferencebetweenthetimingsofthesebehaviors.However,
themodeldoesnotdetectsuchadifference,leadingtoincorrectpredictions.Anotherfrequenterroroccursbetween
representingandformulating(CRF)andsharinginformation(SSI).Takethechatmessage“ifmineis[number],theother
twor’sshouldsumupto[number]” asanexample.ThechatmessageislabeledasCRF,butthemodelincorrectly
classifiesitasSSI.Theword“if”representsaconditionalassumption,i.e.,aninferencemadeinacertainsituation,so
thissentenceinvolvesrepresentingandformulating,insteadofsharinginformation,whichinvolvessharingsomething
basedonexistingknowledge.Themodelmaylacktheabilitytocapturekeywordsforclassificationpurposes.
Second,weshowtheerroneouscasespredictedbythePrompt-BERTmodel.Forinstance,thechatmessage“I’mon
themark”,isincorrectlypredictedassharinginformation(SSI)insteadofthetruelabelmonitoringchats(CMC).Onthe
onehand,thissentencedoesnotincludeusefulinformationaboutthetaskandonlyaimstoinformteammembersabout
theprogressoftheirmissions.Thisshowsthatthemodelmaynotaccuratelycapturetheinformationpresentedin
certaincases.Ontheotherhand,themodelcannoteffectivelydistinguishbetweenthecognitiveandsocialdimensions.
Themodelregardsthechatmessageof“luckyguess”,asestablishingsharedunderstanding(SESU),butthetruelabelis
maintainingcommunication(SMC).Thismessageaimstoexpressthejoyofmakingacorrectguessanddoesnotinclude
usefulinformation.Thus,themessageshouldbeencodedasSMC.Thisexampleagainshowsthatthemodelcannot
graspthesophisticatedinformationandmessageconveyedbythesentence,resultinginaninappropriateprediction.
Inconclusion,whenthemodelperformsencoding,itcannottakefulladvantageoftheinformationcontainedin
specialwords,e.g.,conditionalwordsandtensewords,tohelpmakemoreaccuratepredictions,whichmayleadto
deviationsinsentenceclassificationtasks.
7 DISCUSSION
Individualswithdifferentskillsandknowledgeareincreasinglyrequiredtoworkonateamtocollaborativelysolve
complexproblems[44].Tobetterunderstandandanalyzebehavioralpatternsobservedincollaborativeactivities,
recentstudieshavedesignedsimulatedCPSactivitiestocollectprocessdataduringtasks.Giventheunstructured
characteristicsofprocessdata,itisnecessarytoencodeandtransformthemintostructureddata.However,theexisting
automaticcodingmodelsforCPSprocessdatahaverelativelylowaccuraciesandsignificantdependenciesontraining
setswithlargenumbersofsamples.Toaddressthisproblem,thisstudyproposesanautomaticcodingmodelbased
onpromptlearningtolabelprocessdata,withaprimaryfocusonchatdata.Inthissection,wesummarizeourmain
findingsfromthethreeconductedexperimentsanddiscusstheapplicationsandlimitationsofthisstudy.
7.1 MainFindings
Experiment1explorestheinfluencesofpromptgenerationstrategiesandpre-trainedmodelsontheresultingclassifi-
cationperformance.WefindthatmanuallygeneratingpromptsandusingT5asthepre-trainedmodelcanachieve
thebestclassificationresults.Asnotedby[35],theclassificationresultsmaybebiasedduetotheselectedprompt
18ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
andlabelwordforms.Thus,itissuggestedthatwhenusingprompt-basedclassificationmodels,thepromptmethod
selectiontaskshouldbeconsidered.Inourstudy,wecomparedifferentpromptmethodsandfindthatboththeprompt
templatesandthemappingsbetweentheoriginalwordsandlabelwordsdisplaybetterperformancewhenusingthe
manualdesignapproach.ThisreflectstheuniquenessandcomplexityofthetaskofcodingCPSprocessdata,making
itdifficulttoobtainappropriatepromptmethodsonlythroughtraining.Instead,itisnecessarytodesignsuitable
promptsaccordingtotheactivitiesoftheparticipantsinCPStaskstoachieveimprovedclassificationperformance.
Additionally,inourstudy,weconsiderdifferentpre-trainedmodels,includingBERTanditsRoBERTavariant,aswell
asatransformer-basedmodel(T5),andfindthatT5achievesthebestperformance.ThismaybebecausetheT5model
hasalargernumberofmodelparametersandutilizesasuperiorrelativepositionalencodingapproachinsteadofthe
originalabsolutepositionalencodingapproach.
Experiment2comparesourprompt-basedlearningpre-trainedmodelwithnineotherwidelyusedclassification
models.Allthemodelscanbedividedintofourtypes,namely,n-gram-basedmethods,deeplearningmethods,fine-
tuningmethods,andpromptmethods,whichsharethecommonfeatureofconsideringthetask-specificinformation
derivedfromwordsinspeech[43].Overall,wefindthatacrossthethreeevaluationindicators,theperformance
rankingsofthedifferentmodelsareasfollows,prompt>fine-tuning>deeplearning>n-gram.Nevertheless,compared
tonon-pre-trainedmodelapproaches,methodsthatusepre-trainedmodelsperformbetterbecausethepretraining
taskimplementedonlarge-scaledataallowsthemtolearnricherandmorecomprehensivelanguagerepresentations.
Furthermore,comparedtofine-tuningmethods,promptmethodscanofferguidanceinformationtothemodel,helping
itbetterunderstandthetaskanddomain,ultimatelyleadingtoenhancedperformance.
Experiment3teststhesuperiorityoftheprompt-basedlearningpre-trainedmodelonsmalltrainingsets.Specifically,
basedonExperiment2,weexploredtheclassificationresultsproducedbydifferentmodelswithdifferenttraining
setsizes.Theseresultsareconsistentwithourhypothesis,asshowninFig.4.Prompt-basedlearningmodels,such
asPrompt-RoBERTaandPrompt-BERT,havethebestclassificationperformance.With85%ofthetotaldataasthe
trainingset,only11%ofthesedata(9.35%ofthetotaldata)areneededtoachievesatisfactoryresults.Inotherwords,it
ispossibletoachieveasatisfactoryautomaticcodingeffectforanewdatasetwithonlyapproximately10%ofitsdata
manuallycodedandusedasthetrainingset,whichcanalleviatetheproblemofdatascarcity[21].Interestingly,we
alsofindthatthepre-trainedmodelsbasedonRoBERTaandBERTperformbetterthanT5onsmalltrainingdatasets,
andthelatterrequiresapproximately16%oftheoriginaltrainingsettoreachacceptableperformance.However,T5has
anadvantagewhentrainedonthewholetrainingset.
7.2 Applications
Themainfindingsofitsworkhavepracticalimplications.First,performingautomaticcodingbasedonalanguage
modelcanreducethetimeandhumanresourcesrequiredformanualcoding.Thisstrategyreliesononlyaportionof
themanuallycodeddatatolearnmappingpatternsthroughtraining,enablingittoautomaticallycodetheremaining
data.ThismethodcanassistteachersinmonitoringgroupbehaviorsduringCPSactivities,enablingthemtoprovide
instantsupporttogroups.Additionally,itcanhelpteachersidentifystudents’strengthsandweaknessesincooperative
tasks,allowingthemtotakeappropriateactionstoimprovestudents’CPScompetencelevels[43].
Second,whencomparedwithothermodelsdevelopedinpreviousresearch[3,13,43],ourmodeldemonstrates
higherclassificationaccuracyandhaslowerrequirementsregardingthescaleoftheinputlabeledtrainingdata.Models
basedonpromptsmakepre-trainedmodelsdirectlyadaptabletodownstreamtasks[30].Thiscontextuallearning
paradigm,allowingamodeltolearnbyprovidingitwithhints,provestobemoreeffective.Thismaybethereasonwhy
19CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
prompt-basedlearningmodelsoutperformtheotherclassificationmodels(e.g.,KNN,LSTM,andGRU).[13]notedthat
improvingtheaccuracyofaclassifierwithoutincreasingtheamountofinputtrainingdataischallenging.Generally,
thelargerthetrainingdatasetis,thebettertheclassificationresults.However,forCPStasks,obtainingalargeamount
oftrainingdataistime-consumingandlaborious.Ourproposedmodelcanachievesatisfactoryresultswithfew-shot
learningtoaddressthisproblem.
7.3 Limitations
Aswithallotherresearch,ourstudyhassomelimitations.Thefirstlimitationisthattheimposeddatapreprocessing
requirementisrelativelyhigh.IndifferentCPStasks,thetask-relatedwordsorsymbolsshouldbeprocesseddifferently.
Additionally,abbreviationsandcolloquialwordsappearinginchatdatashouldalsobeprocessed.Duetothediversity
ofCPStasksandparticipants’behaviors,nouniformmethodisavailableforpreprocessingthetask-specificdata.Such
amethodshouldadapttothecharacteristicsofthedataandmodelinputs.Anotherlimitationisthatourcurrentmodel
considersutterancesindependently,withoutaccountingfortheconnectionsbetweensentences.Analyzingcontext
canhelpusmorepreciselyunderstandtheintentionofthecurrentutterance,andthemappingsbetweenanutterance
andtheCPSsubskillscanthenbedeterminedmoreaccurately.Finally,thegeneralizabilityoftheproposedmodelis
nottestedinthecurrentstudy.Weonlyverifytheeffectivenessofthepromptmodelononedatasetcollectedfrom
three-resistorCPStasks.Therefore,whetherthepresentedfindingscanbegeneralizedtootherCPStasksandcoding
scenarioswithdifferentCPSframeworksneedstobeexplored,andthisissuewillbeaddressedinourfuturestudies.
8 CONCLUSION
ThisstudyaimstodesignanautomaticcodingmodelforclassifyingCPSsubskillsbasedonloggingdatathatrecords
participants’explicitbehaviors.Toachievethisgoal,weconstructaprompt-basedlearningpre-trainedmodeland
conductthreeexperimentstoverifyitssuperiority.Experiment1comparesdifferentpromptgenerationmethodsand
pre-trainedmodelstodeterminethecombinationthatachievesthebestperformance.Experiment2comparesourmodel
withotherclassificationmodels,whileExperiment3assessestheperformanceattainedbydifferentmodelsonvarious
smalltrainingsets.Theresultsshowthatourmodelnotonlyhasthehighestaccuracy,macroF1score,andkappavalues
onlargetrainingsetsbutalsoperformsthebestonsmalltrainingsets.Overall,thisstudydemonstratestheeffectiveness
ofthedevelopedprompt-basedlearningpre-trainedmodelinCPSsubskillclassificationtasksinvolvinglow-resource
datasets.Inthefuture,weplantomodifyourmodeltoimplementcontext-basedclassification.Additionally,wewill
testthegeneralizabilityofthemodeltodifferentdatasets.Wehopethatsuchencodingmethodscanbeextendedto
moregeneralresearchfields,suchasotherhuman-computerinteractionsthatarecommonlystudiedintheCSCW
community,toachievethetextstream,audiostream,andvideostreamcoding.
REFERENCES
[1] JessicaAndrews-ToddandCarolMForsyth.2020.Exploringsocialandcognitivedimensionsofcollaborativeproblemsolvinginanopenonline
simulation-basedtask.Computersinhumanbehavior104(2020),105759.
[2] JessicaAndrews-ToddandDeirdreKerr.2019.Applicationofontologiesforassessingcollaborativeproblemsolvingskills.InternationalJournalof
Testing19,2(2019),172–187.
[3] JessicaAndrews-Todd,JonathanSteinberg,MichaelFlor,andCarolynMForsyth.2022.Exploringautomatedclassificationapproachestoadvance
theassessmentofcollaborativeproblemsolvingskills.JournalofIntelligence10,3(2022),39.
[4] ChristaSCAsterhanandBaruchBSchwarz.2009.Argumentationandexplanationinconceptualchange:Indicationsfromprotocolanalysesof
peer-to-peerdialog.Cognitivescience33,3(2009),374–400.
20ApplicationofPromptLearningModelsinIdentifyingtheCollaborativeProblemSolvingSkillsCinSCaWnO’2n0l2i4n,eNToavsekmber9-13,2024,SanJosé,CostaRica
[5] JingChen,JamesHFife,IsaacIBejar,andAndréARupp.2016.Buildinge-rater®scoringmodelsusingmachinelearningmethods.ETSResearch
ReportSeries2016,1(2016),1–12.
[6] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,EricLi,XuezhiWang,MostafaDehghani,SiddharthaBrahma,
AlbertWebson,ShixiangShaneGu,ZhuyunDai,MiracSuzgun,XinyunChen,AakankshaChowdhery,SharanNarang,GauravMishra,AdamsYu,
VincentZhao,YanpingHuang,AndrewDai,HongkunYu,SlavPetrov,EdH.Chi,JeffDean,JacobDevlin,AdamRoberts,DennyZhou,QuocV.Le,
andJasonWei.2022.ScalingInstruction-FinetunedLanguageModels. https://doi.org/10.48550/ARXIV.2210.11416
[7] JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.2014.Empiricalevaluationofgatedrecurrentneuralnetworksonsequence
modeling.arXivpreprintarXiv:1412.3555(2014).
[8] NationalResearchCounciletal.2011.Learningsciencethroughcomputergamesandsimulations.NationalAcademiesPress.
[9] GanquCui,ShengdingHu,NingDing,LongtaoHuang,andZhiyuanLiu.2022.Prototypicalverbalizerforprompt-basedfew-shottuning.arXiv
preprintarXiv:2203.09770(2022).
[10] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018. Bert:Pre-trainingofdeepbidirectionaltransformersforlanguage
understanding.arXivpreprintarXiv:1810.04805(2018).
[11] KEDiCerboandJTBehrens.2012.Implicationsofthedigitaloceanoncurrentandfutureassessment.Computersandtheirimpactonstateassessment:
Recenthistoryandpredictionsforthefuture1,1(2012),273–306.
[12] HeinrichDinkel,MengyueWu,andKaiYu.2019.Text-baseddepressiondetectiononsparsedata.arXivpreprintarXiv:1904.05154(2019).
[13] MichaelFlorandJessicaAndrews-Todd.2022. Towardsautomaticannotationofcollaborativeproblem-solvingskillsintechnology-enhanced
environments.JournalofComputerAssistedLearning38,5(2022),1434–1447.
[14] MichaelFlor,Su-YounYoon,JiangangHao,LeiLiu,andAlinavonDavier.2016.Automatedclassificationofcollaborativeproblemsolvinginteractions
insimulatedsciencetasks.InProceedingsofthe11thworkshoponinnovativeuseofNLPforbuildingeducationalapplications.31–41.
[15] OrganisationforEconomicCo-operationandDevelopment.2017.PISA2015assessmentandanalyticalframework:Science,reading,mathematic,
financialliteracyandcollaborativeproblemsolving.OECDPublishing.
[16] CarolForsyth,JessicaAndrews-Todd,andJonathanSteinberg.2020.AreYouReallyaTeamPlayer?ProfilingofCollaborativeProblemSolversinan
OnlineEnvironment.InternationalEducationalDataMiningSociety(2020).
[17] YanbinFu,PeidaZhan,QipengChen,andHongJiao.2023.Jointmodelingofactionsequencesandactiontimeincomputer-basedinteractivetasks.
BehaviorResearchMethods(2023),1–18.
[18] ArthurCGraesser,StephenMFiore,SamuelGreiff,JessicaAndrews-Todd,PeterWFoltz,andFriedrichWHesse.2018.Advancingthescienceof
collaborativeproblemsolving.psychologicalscienceinthepublicinterest19,2(2018),59–92.
[19] PatrickGriffinandEstherCare.2014.TheATC21Smethod.InAssessmentandteachingof21stcenturyskills:Methodsandapproach.Springer,3–33.
[20] YuxianGu,XuHan,ZhiyuanLiu,andMinlieHuang.2021.Ppt:Pre-trainedprompttuningforfew-shotlearning.arXivpreprintarXiv:2109.04332
(2021).
[21] YanrongGuo,JilongLiu,LeiWang,WeiQin,ShijieHao,andRichangHong.2023.APrompt-BasedTopic-ModelingMethodforDepressionDetection
onLow-ResourceData.IEEETransactionsonComputationalSocialSystems(2023).
[22] JiangangHao,LeiChen,MichaelFlor,LeiLiu,andAlinaAvonDavier.2017.CPS-Rater:Automatedsequentialannotationforconversationsin
collaborativeproblem-solvingactivities.ETSResearchReportSeries2017,1(2017),1–9.
[23] JiangangHao,LeiLiu,PatrickKyllonen,MichaelFlor,andAlinaAvonDavier.2019.Psychometricconsiderationsandageneralscoringstrategyfor
assessmentsofcollaborativeproblemsolving.ETSResearchReportSeries2019,1(2019),1–17.
[24] SteveHiggins,EmmaMercier,LizBurd,andAndrewJoyce-Gibbons.2012. Multi-touchtablesandcollaborativelearning. BritishJournalof
EducationalTechnology43,6(2012),1041–1054.
[25] SeppHochreiterandJürgenSchmidhuber.1997.Longshort-termmemory.Neuralcomputation9,8(1997),1735–1780.
[26] YoonKim.2014.Convolutionalneuralnetworksforsentenceclassification.arXivpreprintarXiv:1408.5882(2014).
[27] BinLi,YixuanWeng,QiyaSong,FuyanMa,BinSun,andShutaoLi.2022. Prompt-basedpre-trainedmodelforpersonalityandinterpersonal
reactivityprediction.InProceedingsofthe12thWorkshoponComputationalApproachestoSubjectivity,Sentiment&SocialMediaAnalysis.265–270.
[28] Cheng-HsuanLi,Pei-LingTsai,Zhi-YongLiu,Wen-ChiehHuang,andPei-JyunHsieh.2021.ExploringCollaborativeProblemSolvingBehavioral
TransitionPatternsinScienceofTaiwaneseStudentsatAge15AccordingtoMasteringLevels.Sustainability13,15(2021),8409.
[29] FinancialLiteracy.[n.d.].PISA2012AssessmentandAnalyticalFramework.([n.d.]).
[30] PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematic
surveyofpromptingmethodsinnaturallanguageprocessing.Comput.Surveys55,9(2023),1–35.
[31] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.
Roberta:Arobustlyoptimizedbertpretrainingapproach.arXivpreprintarXiv:1907.11692(2019).
[32] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.
Roberta:Arobustlyoptimizedbertpretrainingapproach.arXivpreprintarXiv:1907.11692(2019).
[33] AdamLopez.2008.Statisticalmachinetranslation.ACMComputingSurveys(CSUR)40,3(2008),1–49.
[34] RabeehKarimiMahabadi,LukeZettlemoyer,JamesHenderson,MarziehSaeidi,LambertMathias,VeselinStoyanov,andMajidYazdani.2022.Perfect:
Prompt-freeandefficientfew-shotlearningwithlanguagemodels.arXivpreprintarXiv:2204.01172(2022).
21CSCW’2024,November9-13,2024,SanJosé,CostaRica XXXandXXX,etal.
[35] RuiMao,QianLiu,KaiHe,WeiLi,andErikCambria.2022. Thebiasesofpre-trainedlanguagemodels:Anempiricalstudyonprompt-based
sentimentanalysisandemotiondetection.IEEETransactionsonAffectiveComputing(2022).
[36] MaryLMcHugh.2012.Interraterreliability:thekappastatistic.Biochemiamedica22,3(2012),276–282.
[37] RobertJMislevy,SethCorrigan,AndreasOranje,KristenDiCerbo,MalcolmIBauer,AlinavonDavier,andMichaelJohn.2016.Psychometricsand
game-basedassessment.Technologyandtesting:Improvingeducationalandpsychologicalmeasurement(2016),23–48.
[38] RobertJMislevy,LindaSSteinberg,andRussellGAlmond.2003. Focusarticle:Onthestructureofeducationalassessments. Measurement:
Interdisciplinaryresearchandperspectives1,1(2003),3–62.
[39] OECD.2017.PISA2015collaborativeproblemsolvingframework.
[40] AliceOthmani,DaoudKadoch,KamilBentounes,EmnaRejaibi,RomainAlfred,andAbdenourHadid.2021.Towardsrobustdeepneuralnetworks
foraffectanddepressionrecognitionfromspeech.InPatternRecognition.ICPRInternationalWorkshopsandChallenges:VirtualEvent,January10–15,
2021,Proceedings,PartII.Springer,5–19.
[41] FanOuyang,WeiqiXu,andMutluCukurova.2023. Anartificialintelligence-drivenlearninganalyticsmethodtoexaminethecollaborative
problem-solvingprocessfromthecomplexadaptivesystemsperspective.InternationalJournalofComputer-SupportedCollaborativeLearning18,1
(2023),39–66.
[42] BoPang,LillianLee,andShivakumarVaithyanathan.2002.Thumbsup?Sentimentclassificationusingmachinelearningtechniques.arXivpreprint
cs/0205070(2002).
[43] SamuelLPugh,ArjunRao,AngelaEBStewart,andSidneyKD’Mello.2022.Dospeech-basedcollaborationanalyticsgeneralizeacrosstaskcontexts?.
InLAK22:12thInternationalLearningAnalyticsandKnowledgeConference.208–218.
[44] SamuelLPugh,ShreeKrishnaSubburaj,ArjunRameshRao,AngelaEBStewart,JessicaAndrews-Todd,andSidneyKD’Mello.2021.SayWhat?
AutomaticModelingofCollaborativeProblemSolvingSkillsfromStudentSpeechintheWild.InternationalEducationalDataMiningSociety(2021).
[45] ChengweiQinandShafiqJoty.2021.LFPT5:Aunifiedframeworkforlifelongfew-shotlanguagelearningbasedonprompttuningoft5.arXiv
preprintarXiv:2110.07298(2021).
[46] AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever.2019.LanguageModelsareUnsupervisedMultitaskLearners.
(2019).
[47] RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherDManning,AndrewYNg,andChristopherPotts.2013.Recursivedeepmodels
forsemanticcompositionalityoverasentimenttreebank.InProceedingsofthe2013conferenceonempiricalmethodsinnaturallanguageprocessing.
1631–1642.
[48] AngelaEBStewart,HanaVrzakova,ChenSun,JadeYonehiro,CathlynAdeleStone,NicholasDDuran,ValerieShute,andSidneyKD’Mello.
2019.Isay,yousay,wesay:Usingspokenlanguagetomodelsocio-cognitiveprocessesduringcomputer-supportedcollaborativeproblemsolving.
ProceedingsoftheACMonHuman-ComputerInteraction3,CSCW(2019),1–19.
[49] ChenSun,ValerieJShute,AngelaStewart,JadeYonehiro,NicholasDuran,andSidneyD’Mello.2020.Towardsageneralizedcompetencymodelof
collaborativeproblemsolving.Computers&Education143(2020),103672.
[50] YiSun,YuZheng,ChaoHao,andHangpingQiu.2021.NSP-BERT:APrompt-basedZero-ShotLearnerThroughanOriginalPre-trainingTask–Next
SentencePrediction.arXive-prints(2021),arXiv–2109.
[51] BernieTrillingandCharlesFadel.2009.21stcenturyskills:Learningforlifeinourtimes.JohnWiley&Sons.
[52] ArminWeinbergerandFrankFischer.2006.Aframeworktoanalyzeargumentativeknowledgeconstructionincomputer-supportedcollaborative
learning.Computers&education46,1(2006),71–95.
[53] WenboZheng,LanYan,ChaoGou,andFei-YueWang.2020.Graphattentionmodelembeddedwithmulti-modalknowledgefordepressiondetection.
In2020IEEEInternationalConferenceonMultimediaandExpo(ICME).IEEE,1–6.
[54] RuiqiZhong,KristyLee,ZhengZhang,andDanKlein.2021.Adaptinglanguagemodelsforzero-shotlearningbymeta-tuningondatasetand
promptcollections.arXivpreprintarXiv:2104.04670(2021).
[55] ChuntingZhou,JunxianHe,XuezheMa,TaylorBerg-Kirkpatrick,andGrahamNeubig.2022.Promptconsistencyforzero-shottaskgeneralization.
arXivpreprintarXiv:2205.00049(2022).
[56] MengxiaoZhu,OuLydiaLiu,andHee-SunLee.2020.Usingclusteranalysistoexplorestudents’interactionswithautomatedfeedbackinanonline
Earthsciencetask.InternationalJournalofQuantitativeResearchinEducation5,2(2020),111–135.
22