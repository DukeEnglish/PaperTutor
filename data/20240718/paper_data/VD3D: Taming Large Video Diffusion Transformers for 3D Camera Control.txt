VD3D: Taming Large Video Diffusion Transformers
for 3D Camera Control
SherwinBahmani1,2,3 IvanSkorokhodov3 AliaksandrSiarohin3 WilliMenapace3
GuochengQian3 MichaelVasilkovsky3 Hsin-YingLee3 ChaoyangWang3
JiaxuZou3 AndreaTagliasacchi1,4 DavidB.Lindell1,2 SergeyTulyakov3
1UniversityofToronto 2VectorInstitute 3SnapInc. 4SFU
https://snap-research.github.io/vd3d
“Otters with chef hats, skillfully preparing
Camera “A dog wearing vr goggles on a boat” a miniature sushi feast on a lily pad”
Figure1: 3Dcameracontrolfortext-to-videogeneration. Weintroduceamethodthatcancontrol
camera poses for text-to-video generation using video diffusion transformers. (left) The method
takesasinputasetofcameraposesusedtogenerateeachframeofarenderedvideo. (center,right)
Applyingmultiplecameratrajectorieswiththesametextpromptenablessynthesisofcomplexscenes
fromvariedsetofviewpoints.
Abstract
Moderntext-to-videosynthesismodelsdemonstratecoherent,photorealisticgener-
ationofcomplexvideosfromatextdescription. However,mostexistingmodels
lackfine-grainedcontrolovercameramovement,whichiscriticalfordownstream
applicationsrelatedtocontentcreation,visualeffects,and3Dvision. Recently,
new methods demonstrate the ability to generate videos with controllable cam-
eraposes—thesetechniquesleveragepre-trainedU-Net-baseddiffusionmodels
thatexplicitlydisentanglespatialandtemporalgeneration. Still,noexistingap-
proachenablescameracontrolfornew,transformer-basedvideodiffusionmodels
thatprocessspatialandtemporalinformationjointly. Here,weproposetotame
videotransformersfor3DcameracontrolusingaControlNet-likeconditioning
mechanismthatincorporatesspatiotemporalcameraembeddingsbasedonPlucker
coordinates. Theapproachdemonstratesstate-of-the-artperformanceforcontrol-
lablevideogenerationafterfine-tuningontheRealEstate10Kdataset.Tothebestof
ourknowledge,ourworkisthefirsttoenablecameracontrolfortransformer-based
videodiffusionmodels.
1 Introduction
Text-to-videofoundationmodelsachieveunprecedentedvisualquality[7,52]. Theyaretrainedon
massivecollectionsofimagesandvideosandlearntosynthesizeremarkablyconsistentandphysically
plausible visualizations of the world. Yet, they lack built-in mechanisms for explicit 3D control
duringthesynthesisprocess,requiringuserstomanipulateoutputsthroughpromptengineeringand
Preprint.Underreview.
4202
luJ
71
]VC.sc[
1v18721.7042:viXra“Aspaceshuttlelaunching”
4D-fy
“Aspaceshuttlelaunching,camerazoom”
SnapVideo
“Aspaceshuttlelaunching”
VD3D
Figure2:Comparingtext-to-video,text-to-4D,andcamera-conditionedtext-to-videogeneration.
Text-to-4Dapproaches,suchas4D-fy[3](top)havefullcontrolovercamerathroughuseofa3D
representation,buttheylackphotorealism. (middle)Methodsfortext-to-videogeneration[41]create
realisticvideos,butdonotprovideexplicitcontrolovertheviewpoint.Incontrast,camera-conditioned
text-to-videogeneration(bottom)bridgesthegapbetweenthetwoparadigmsbyextendingtext-to-
videogeneratorswith3Dcameracontrolwithoutusinganexplicit3Drepresentation. Pleaseseethe
supplementarywebpageforthecorrespondingvideoresults.
trialanderror—aslow,laborious,andcomputationallyexpensiveprocess. Forexample,asFig.2
shows,state-of-the-artvideomodelsstruggletofollowevensimple“zoom-in”or“zoom-out”camera
trajectoriesusingtextpromptinstructions(seesupplementalwebpage). Thislackofcontrollability
limits interactivity and makes existing video generation techniques challenging to use for artists
or other end users. We augment 2D video generation models with control over the position and
orientationofthecamera,providingfiner-grainedcontrolcomparedtotextprompting,andfacilitating
useofvideogenerationmodelsfordownstreamapplications.
Several contemporary works [65, 19, 78] propose methods for camera control of state-of-the-art,
open-sourcevideodiffusionmodels. Thekeytechnicalinsightproposedbythesemethodsistoadd
cameracontrolbyfine-tuningthetemporalconditioninglayersofaU-Net-basedvideogeneration
modelonadatasetwithhigh-qualitycameraannotations. Whilethesetechniquesachievepromising
results,theyarenotapplicabletomorerecent,high-qualitytransformer-basedarchitectures[60,46],
suchasSora[7],SnapVideo[41],andLumina-T2X[15],astheselatestworkssimplydonothave
standalonetemporallayersamenabletocameraconditioning.
Largevideotransformersrepresentavideoasa(possiblycompressed)sequenceoftokens,applying
self-attention layers to all the tokens jointly [7, 41]. Consequently, they do not have standalone
temporallayers,whichareessentialforcurrentcameraconditioningapproachestointegrateviewpoint
information,makingtheminapplicabletosuchmodels. Asthecommunityshiftstowardslargevideo
transformerstojointlymodelspatiotemporaldependenciesinthedata,itiscriticaltodevelopmethods
thatprovidesimilarcapabilitiesforcameracontrol. Ourworkdesignsacameraconditioningmethod
tailoredtothejointspatiotemporalcomputationusedinlargevideotransformersandtakesastep
towardstamingthemforcontrollablevideosynthesis.
We develop our work on top of our implementation of SnapVideo [41], a state-of-the-art video
diffusionmodel,thatemploysFIT-blocks[11]forefficientvideomodelinginthecompressedlatent
space. We investigate various camera conditioning mechanisms in the fine-tuning scenario and
explore trade-offs in terms of visual quality preservation and controllability. Our findings reveal
2thatsimplyadaptingexistingapproachestovideotransformersdoesnotyieldsatisfactoryresults:
theyeitherenablesomelimitedamountofcontrolwhilereducingthevisualqualityoftheoutput
video,ortheyentirelyfailtocontrolcameramotion. Ourkeytechnicalinsightistoenablethecontrol
throughspatiotemporalcameraembeddings,whichwederivebycombiningPluckercoordinateswith
thenetworkinputthroughaseparatelytrainedcross-attentionlayer. Tothebestofourknowledge,
ourworkisthefirsttoexploreaControlNet-like[89]conditioningmechanismforspatiotemporal
transformers.
Weevaluatethemethodonacollectionofmanuallycraftedtextpromptsandunseencameratrajecto-
riesandcomparetobaselineapproachesthatincorporatepreviouscameracontrolmethodsintoa
videotransformer. Ourapproachachievesstate-of-the-artresultsintermsofcameracontrollability
andvideoquality,andalsoenablesdownstreamapplicationssuchasmulti-view,text-to-videogenera-
tion,asdepictedinFig.1. Incontrasttoexistingimage-to-3Dmethods(e.g.,[39,48,61]),which
arelimitedtoobject-centricscenes,ourapproachsynthesizesnovelviewsforrealinputimageswith
complexenvironments.
Overall,ourworkmakesthefollowingcontributions.
• Weproposeanewmethodtotamelargevideotransformersfor3Dcameracontrol. Our
approachusesaControlNet-likeconditioningmechanismthatincorporatesspatiotemporal
cameraembeddingsbasedonPluckercoordinates.
• Wethoroughlyevaluatethisapproach,includingcomparisonstopreviouscameracontrol
methods,whichweadapttothevideotransformerarchitecture.
• We demonstrate state-of-the-art results in controllable video generation by applying the
proposedconditioningmethodandfine-tuningschemetotheSnapVideomodel[41].
2 Relatedwork
Ourmethodisconnectedtotechniquesrelatedtotext-to-video,text-to-3D,andtext-to-4Dgeneration.
Asthisisapopularandfast-movingfield,thissectionprovidesonlyapartialoverviewwithafocus
onthemostrelevanttechniques;wereferreaderstoPoetal.[47]andYunusetal.[85]foramore
thoroughreviewofrelatedtechniques.
Text-to-videogeneration. Ourworkbuildsonrecentdevelopmentsin2Dvideogenerationmodels.
One such class of these techniques works by augmenting text-to-image models with layers that
operateonthetemporaldimensiontofacilitatevideogeneration[6,53,69,18,5]. Videogeneration
methods can be trained in a hybrid fashion on both images and video to improve the generation
quality[4,62,76,22,18,20,63,94]. Whilethesetechniquesareprimarilybasedonconvolutional,
U-Net-stylearchitectures,arecentshifttowardstransformer-basedmodelsenablessynthesisofmuch
longervideoswithsignificantlyhigherquality[7,40,41]. Still,theserecenttechniquesdonotenable
synthesiswithcontrollablecameramotion.
4Dgeneration. Previousmethodsalsotackletheproblemof4Dgeneration,i.e.,generatingvideosof
dynamic3Dscenesfromcontrollableviewpoints,usuallyfromaninputtextpromptorimage. Since
theinitialworkonthistopicusinglarge-scalegenerativemodels[54],significantimprovementsinthe
visualqualityandmotionqualityofgeneratedsceneshavebeenachieved[50,37,3,93,16,77,28,1,
90,72,42,34]. Whilethesemethodsgeneratescenesbasedontextconditioning,otherapproaches
convertaninputimageorvideotoadynamic3Dscene[50,92,81,45,93,37,16,86,12,70,79,
64,14,55,87,83,51,33,35,59,58,26,8,36,88]. Anotherlineofwork[2,75]extends3DGANs
into4Dbytrainingon2Dvideos,howeverthequalityislimitedandmodelsaretrainedonsingle
categorydatasets. Still,allofthesemethodsarefocusedonobject-centricgeneration,typicallybased
on3Dvolumetricrepresentations. Assuch,theytypicallydonotincorporatebackgroundelements,
andoverall,theydonotapproachthelevelofphotorealismdemonstratedbythestate-of-the-artvideo
generationmodelsusedinourtechnique.
Controllablegenerationwithdiffusionmodels.Methodsforcontrollablegenerationusingdiffusion
modelshavehadsignificantimpact,bothinthecontextofimageandvideogeneration. Forexample,
existingtechniquesallowcontrollableimagegenerationconditionedontext,depthmaps,edges,pose,
orothersignals[89,80]. Furthermore,thereisalineofworkfor3Dgenerationwhichconditions
diffusionmodelsoncameraposesforview-consistentmulti-viewgeneration[66,56,9,84,32,44,17].
Ourapproachismostsimilartorelatedtechniquesinvideogenerationthatseektocontrolthecamera
3"runninghorse" textenc.
joints.t.block compression decompression
noisestd.
framerate
resolution
time
XAttn+FF
FITBlock
videopixelstopatches
patchestopixels
Pluckerconditioning
Plucker
coords.
Pluckerconditioning XAttn+FF zeroconv.
XAttn+FF XAttn+FF
videopatchtokens
zeroconv.
MLP Pluckerpatchtokens
Pluckercoordstopatches
Figure3: Overviewofarchitecture. WeadapttheSnapVideoFITarchitecture[41]toincorporate
cameracontrol. Wetakeasinputthenoisyinputvideox˜,cameraextrinsicsC ,andcameraintrinsics
f
K for each video frame f. Using the camera parameters, we compute the Plücker coordinates
f
for each pixel within the video frames. Both the input video and Plücker coordinate frames are
convertedtopatchtokens,andweconditionthevideopatchtokensusingamechanismsimilarto
ControlNet[89](“Plückerconditioning”block). Then,themodelestimatesthedenoisedvideoxˆby
recurrentapplicationofFITblocks[11]. Eachblockreadsinformationfromthepatchtokensintoa
smallsetoflatenttokensonwhichcomputationisperformed. Theresultsarewrittentothepatch
tokensinaniterativedenoisingdiffusionprocess.
position. For example, MotionCtrl [65] designs camera and object control mechanisms for the
VideoCrafter1[10]andSVD[5]models.
Concurrent3Dcameracontrolmethods. Concurrentapproachesenablecameracontrolbycon-
ditioning the temporal layers of the network with camera pose information, e.g., using Plucker
coordinates[19,18,73,31]orotherembeddings[78]. Interestingly,itisalsopossibletoincorporate
cameracontrolintovideogenerationmodelswithoutadditionaltrainingthroughmanipulationand
maskingofattentionlayers,thoughthisrequiresadditionaltracking,segmentation,ordepthforeach
inputvideo[25,71,24]. Anotherrecentwork[38]transfersmotion,includingcameramotion,to
othergeneratedvideos.
AlthoughtheseapproachesdemonstratepromisingresultsforU-Net-basedvideodiffusionmodels,
thetechniquesarenotapplicabletomodernvideotransformersthatmodelspatio-temporaldynamics
jointly. Whileanotherconcurrentwork[67]usesatransformer-basedarchitectureforspaceandtime,
itdoesnottackletext-basedgenerationfordynamicscenesbutmainlynovelviewsynthesisfor3D
scenes. Inourwork,wedesignanefficientmechanismthatenablescameracontrolinvideodiffusion
transformersusingaControlNetinspiredmechanismwithoutsacrificingvisualquality.
3 Method
3.1 Largetext-to-videotransformers
Text-to-videogeneration. Diffusionmodelshaveemergedasthedominantparadigmforlarge-scale
videogeneration[23,22,7]. Thestandardsetupconsiderstheconditionaldistributionp(x|y)of
videosx∈RF×H×W (consistingofF framesofH×W resolution)giventheirtextualdescriptions
y ∈ YL, consisting of L (possibly padded) tokens from the alphabet Y. Following [29], our
4
FF+nttAS FF+nttAS FF+nttASvideo diffusion framework assumes a denoising model D : (x˜;y,σ) (cid:55)→ xˆ that predicts a clean
θ
videoxˆ fromthecorrespondingnoisedinputx˜ = x+σε,σ ∈ R ,ε ∼ N(0,I). Themodelis
+
parametrizedbyaneuralnetworkF (x˜;y,σ)asD (x˜;y,σ)=c (σ)F (c (σ)x˜;y,σ)+c (σ)x˜.
θ θ out θ in skip
Theoptimizationobjectiveisdefinedasfollows:
L(θ)≜ E (cid:2) ∥D (x+σε;y,σ)−x∥2(cid:3) −→min (1)
θ 2
(x,y),σ,ε θ
Wereferareaderto[41]and[29]forfurtherdetailsonthediffusionsetup,whichweadoptedwithout
modifications.
Spatiotemporal transformers. Following SnapVideo [41], our video generator consists of two
models: thebase4B-parametersgenerator,operatingon16-frames36×64resolutionvideos,and
a 288×512 upsampler. The latter, a diffusion model itself, is fine-tuned from the base model
and conditioned on the low-resolution videos. Each model uses FIT transformer blocks [11, 27]
for efficient self-attention operations (see Fig. 3). An FIT model consists of B blocks (we have
B = 6 in all the experiments) and first partitions each frame in an input video into patches [13]
of resolution h ×w (we use h = w = 4 in all the experiments). These video patches are
p p p p
then independently projected via a feedforward (FF) layer to obtain a sequence of video tokens
[v ]L ≜(v ,...,v )∈RL×doflengthL=F ×(H/h )×(W/w )anddimensionalityd. Next,
ℓ ℓ=1 1 L p p
each FIT block “reads” the information from this video sequence into a much shorter sequence
of latent tokens [z ]M ≜ (z ,...,z ) through a “read” cross-attention layer, followed by a
m m=1 1 L
feedforwardlayer. Thecoreprocessingwithself-attentionlayersisperformedinthislatentspace,and
thentheresultiswrittenbacktothevideotokensthroughacorresponding“write”cross-attention
layer(alsofollowedbyanFFlayer). ThelatenttokensineachnextFITblockareinitializedfromthe
previousone,whichhelpstopropagatethecomputationalresultsthroughoutthenetwork. Inthis
way,theentirecomputationoccursjointlyinbothspatialandtemporalaxes,whichyieldssuperior
scalabilityproperties[41].However,itabandonsthedecomposedspatial/temporalcomputationnature
ofmodernvideodiffusionU-Nets,whichisvitalformoderncameraconditioningtechniques[65,78]
toenforcecontrolwithoutcompromisingvisualquality.
3.2 Cameracontrolforspatiotemporaltransformers
Spatiotemporal camera representation. The standard way of representing camera parameters
foravideo(inthepinholemodel)isviaatrajectoryofextrinsicsandintrinsicscameraparameters
(C ,K )F for each f-th frame, where C = [R;t] ∈ R3×4, describes the camera rotation
f f f=1 f
R ∈ R3×3 andtranslationt×R3,andK ∈ R3×3 containsthefocallengthandprincipalpoint
f
(andalsohorizontal/verticalskewcoefficient, butitisalways0inoursetup). Tocontrolcamera
motion,existingmethodssuchas[65,78,25]conditionthetemporalattentionlayersofU-Net-based
videogeneratorsonembeddingscomputedfromthesecameraparameters. Suchapipelineprovides
agoodconditioningsignalforconvolutionalvideogeneratorswithdecomposedspatial/temporal
computation,butourexperimentsdemonstratethatitworkspoorlyforspatiotemporaltransformers:
theyeitherfailtopickupanycontrollability(whenbeingaddedastransformedresidualstothelatent
tokens),ordegradethevisualqualityoftheoutput(whentheoriginalnetworkparametersarebeing
fine-tuned). Thismotivatesustodesignabettercameraconditioningscheme,tailoredformodern
large-scalespatiotemporaltransformers.
First,weproposetonormalizethecameraparameterstotheveryfirstframe,forcingthefirstframe’s
rotation be an identity matrix R = I and translation be zero: t = 0. We then recompute the
1 1
extrinsicsforeachf-thframeasC =[R−1R ;t −t ],establishingaconsistentcoordinatesystem
f 1 f f 1
acrossvideos. Afterthat,wefounditessentialtoenrichtheconditioninginformationbyswitching
fromtemporalframe-levelcameraparameterstopixel-wisespatiotemporalcameraparameters. This
isachievedbycomputingthePlückercoordinatesforeachpixel,providingfine-grainedpositional
representation.
Plückercoordinatesprovideaconvenientparametrizationoflinesinthe3Dspace,andweusethem
tocomputefine-grainedpositionalrepresentationsofeachpixelineachframeofavideo. Giventhe
extrinsicandintrinsiccameraparametersR,t,K ofthef-thframe,weparametrizeeach(h,w)-th
f
pixelasaPlückerembeddingp¨ ∈R6fromthecamerapositiontothepixel’scenteras
f,h,w
d
p¨ =(t ×dˆ ,dˆ ), dˆ = , d =R K [w,h,1]⊤+t . (2)
f,h,w f f,h,w f,h,w f,h,w ∥d ∥ f,h,w f f f
f,h,w
5ThisapproachmirrorsthetechniqueusedinCameraCtrl[19],aconcurrentstudyfocusingoncamera
controlinU-Net-basedvideodiffusionmodels. ThemotivationforusingPlückercoordinatesisthat
geometricmanipulationsinthePlückerspacecanbeperformedthroughsimplearithmeticsonthe
coordinates,whichmakesiteasierforthenetworktousethepositionalinformationstoredinsucha
disentangledrepresentation.
ComputingPlückercoordinatesforeachpixelresultsinaP¨ ∈R6×F×H×W spatiotemporalcamera
representationforavideo. Toinputitintothemodel,wefirstperformtheequivalentViT-like[13]
h ×w patchificationprocedure. ItisfollowedbyalearnablefeedforwardlayerFF toobtainthe
p p p¨
Plückercameratokenssequence[c¨ ]L ∈RL×d ofthesamelengthL=F ×(H/h )×(W/w )
ℓ ℓ=1 p p
anddimensionalitydasthevideotokenssequence[v ]L . Thisspatiotemporalrepresentationcarries
ℓ ℓ=1
fine-grainedpositionalinformationabouteachpixelinavideo,makingiteasierforthegeneratorto
accuratelyfollowthedesiredcameramotion.
Cameraconditioning. ToinputtherichspatiotemporalcamerainformationintheformofPlücker
embeddingsintoourvideogenerator,wedesignanefficientControlNetlike[89]mechanismtailored
for large transformer models (see Fig. 3). This mechanism is guided by two main objectives: 1)
the model should be amenable to rapid fine-tuning from a small dataset with estimated camera
positions; and 2) the visual quality shouldn’t be compromised during the fine-tuning stage. We
foundthatmeetingtheseobjectivesismorechallengingforspatiotemporaltransformerscomparedto
U-Net-basedmodelswithdecomposedspatial/temporalcomputation,sinceevenminorinterventions
intotheirdesignquicklyleadtodegradedvideooutputs. Wehypothesizethatthecorereasonforitis
theentangledspatial/temporalcomputationofvideotransformers: anyattempttoalterthetemporal
dynamics(suchascameramotion)influencesspatialcommunicationbetweenthetokens,leading
tounnecessarysignalpropagationandoverfittingduringthefine-tuningstage. Tomitigatethis,we
inputthecamerainformationgraduallythroughreadcross-attentionlayers,zero-initializedfromthe
originalnetworkparametersofthecorrespondinglayers.
Specifically,ineachb-thFITblockofourvideogenerator,wereplaceitsstandardreadcross-attention
operation(see[27,41]fordetails):
[z(b)]M =FF(b)(XAttn(b)([z(b)]M ,[v(b)]L ]), (3)
m m=1 m m=1 ℓ ℓ=1
whereFF(·)andXAttn(·,·)denotefeed-forwardandcross-attentionlayersrespectively,with:
[z(b)]M =FF(b)(XAttn(b)([z(b)]M ,[v(b)]L ]) (4)
m m=1 m m=1 ℓ ℓ=1
+Conv(b)(FF(b)(XAttn(b)([z(b)]M ,Conv(b) ([c¨ ]L ]))), (5)
res cam cam m m=1 plück ℓ ℓ=1
where FF(b), and XAttn(b) are learnable layers, and Conv(b) ,Conv(b) are 1-dimensional convo-
cam cam plück res
lutionsthatprocesspluckercameratokensandtheirfinalrepresentationsinside,respectively. Itis
crucialtoinstantiatetheweightsoftheseconvolutionsfromzerostopreservethemodelinitialization.
Besides,weinitializetheweightsofFF(b) andXAttn(b) fromthecorrespondingparametersofthe
cam cam
originalnetwork. Thisapproachhelpstopreservevisualqualityatinitializationandfacilitatesrapid
fine-tuningonasmalldataset. Asaresult,weobtainthemethodforfine-grained3Dcameracontrol
inlargevideodiffusiontransformers. WenameitVD3DandvisualizeitsarchitectureinFig.3.
3.3 Trainingdetails
Optimizationdetails. Toensurecomparabilitywithpriorworksuchas[65], wetrainourvideo
generatoronthesameRealEstate10kdataset[95]. Weoptimizeonlythenewlyaddedparameters
FF and(α(b),FF(b),XAttn(b))B ,andkeeptherestofthenetworkfrozen. Wefoundthattraining
p¨ b=1
onlythebase36×64modelwassufficient,asthe288×512upsampleralreadyaccuratelyfollows
thecameramotionofalow-resolutionvideo. Weexperimentwithtwomodelvariants: asmaller
generatorwithapproximately700millionparametersforablationsandinitialexplorations,anda
larger 4 billion parameter model, which we use for the main results in this paper. Both models
weretrainedwithabatchsizeof256over50,000optimizationstepswiththeLAMBoptimizer[82].
Thelearningratewaswarmedupforthefirst10,000iterationsfrom0to0.005andthenlinearly
decreasedto0.0015oversubsequentiterations. Sincetheoriginalvideodiffusionmodelistrained
intheany-frame-conditioningpipeline[41],wecanproducevariablecameratrajectoriesfromthe
same starting frame. For text conditioning, we use the T5-11B [49] language model to encode
6text1024-dimensionalembeddingsinto128-lengthsequences. Fortrainingefficiency, theywere
precomputedfortheentiredataset. Therestofthetrainingdetailshavebeenadoptedfrom[41].
Computedetails.Asingletrainingrunforthesmaller700Mparametergeneratortakesapproximately
1dayonanodeequippedwith8×NVIDIAA10040GBGPUs,connectedviaNVIDIANVLink,
alongwith960GBofRAMand92IntelXeonCPUs. Thelarger4Bparametermodelwastrainedon
8suchnodesfor1,5days,totaling64×NVIDIAA10040GBGPUs. Intotal,weconducted≈150
trainingrunsforthesmallermodelduringthedevelopmentstageof4Bgenerator. Consequently,the
project’stotalcomputeutilizationamountedtoapproximately2700NVIDIAA10040GBGPU-days.
3.4 Dataset
We fine-tune the pre-trained text-to-video model (SnapVideo [41]), on RealEstate10K [95]. The
training split consists of roughly 65K video clips, and is the same as is used in concurrent work
(MotionCtrl [65] and CameraCtrl [19]). We evaluate our method using 20 camera trajectories
sampledfromtheRealEstate10Ktestsplitthatwerenotseenduringtrainingfortheuserstudy. For
theautomatedevaluations,weusethefulltestsplit.
3.5 Metrics
Weconductauserstudytoevaluateourapproachforcamera-controlledtext-to-videogeneration. The
studyparticipantsarepresentedwith20side-by-sidecomparisonsbetweentheproposedapproach
andthebaselinesaswellasareferencevideofromRealEstate10Kwiththesametrajectorytobetter
judgethecameraalignment. Weask20participantsforeachgeneratedvideosequencetoindicate
whichgeneratedvideotheypreferbasedonmultiplesubmetrics,namely,cameraalignment,motion
quality,textalignment,visualquality,andoverallpreference. Theuserstudyinvolvednegligiblerisk
totheparticipantsandwasconductedwithappropriateinstitutionalreviewboardandlegalapproval.
Participantswerecompensated0.30USDperquestion.
3.6 Baselines
WecompareourworktothetwoconcurrentworksMotionCtrl[65]andCameraCtrl[19]byadapting
theirpubliclyreleasedcodetotheSnapVideomodel[41]. Notethatbothbaselineswereoriginally
designedforspace-timedisentangledU-Netvideodiffusionmodels. Hence,theirapproachesarenot
directlyapplicabletothespatio-temporaltransformers,andsoweadaptthemtothissettingasfollows.
ForMotionCtrl,weomittheirobjectmotioncontrolmoduleandusetheirproposedcameramotion
control module to encode the camera parameters into a context vector used with the SnapVideo
model. Wefine-tuneboththecameramotioncontrolmoduleandthecrossattentionbetweenthe
latentvectorsandthepatches. WealsotrainoneversionofMotionCtrlwherewefreezetheoriginal
basemodeltopreventqualitydegradation. ForCameraCtrl,wefine-tunetheoriginalcameraencoder
moduleandusethistoproducethelatentvectorsintheSnapVideomodel. Duringfine-tuningthe
modelweightsarekeptfrozen—i.e.,thesameasinourproposedapproach.
4 Experiments
4.1 Assessment
We provide a qualitative and quantitative assessment of our approach compared to the baselines
in Fig. 4 and in Tab. 1. Following CameraCtrl [19], we also evaluate the camera pose accuracy
usingParticleSfM[91]ongeneratedvideosinTab.2. Weusinggenerationsfortextpromptsfrom
RealEstate10K[95]andMSR-VTT[74],testingbothin-andout-of-distributionprompts. Notethat
weadjusttheCameraCtrl[19]evaluationpipelinebynormalizingallcamerasintoaunifiedscaleas
COLMAPprovidesdifferentscalesacrossdifferentscenes. Thispreventssceneswithlargescaleto
haveahigherimpactontheerrors. Pleasealsorefertothesupplementarywebpageforadditional
videoresults.
InFig.4weobservethatadaptingthecameraconditioningmethodfromtheMotionCtrldegrades
visualqualityandtextalignment,likelybecausethisapproachadjuststheweightsofthebasevideo
model. Inthespace-timeU-Netforwhichthisapproachwasproposed,thetemporallayerscanbe
7“3 sheep enjoying spaghetti together” “Melting ice cream dripping down the cone”
MotionCtrl
CameraCtrl
Ours
Figure4: Camera-conditionedtext-to-videogeneration. Comparisonoftheproposedapproachto
MotionCtrlandCameraCtrlforthesamecameratrajectoryinput. MotionCtrlexhibitsworsequality
duetofine-tuningofexistinglayersandCameraCtrlisnotsensitivetocameraconditioning. Seethe
supplementarywebpageforvideoresults.
Table1: Quantitativeresults. WecompareourmethodtoMotionCtrlandCameraCtrlimplemented
onthesamebasevideomodelsasours.Themethodsareevaluatedinauserstudyinwhichparticipants
indicatetheirpreferencebasedoncameraaligntment(CA),motionquality(MQ),textalignment
(TA),visualquality(VQ),andoverallpreference(Overall). Thepercentagesindicatepreferencefor
VD3Dvs.thealternativemethod(ineachrow). Allresultsarestatisticallysignificantwithp<0.001
asevaluatedusingaχ2test.
HumanPreference
Method CA MQ TA VQ Overall
VD3Dvs.MotionCtrl 82% 81% 86% 81% 84%
VD3Dvs.CameraCtrl 78% 64% 63% 65% 66%
fine-tunedwithoutsacrificingvisualfidelity. Sincespatio-temporaltransformersdonotdecompose
temporal and spatial attributes in the same way, the model overfits to the small dataset used to
fine-tunethecross-attentionlayer. Whileweobservesomeagreementwiththecameraposesused
toconditionthemodel,thetextalignmentisgenerallylowinourexperiments(seesupplemental
webpage). In contrast, CameraCtrl keeps the pre-trained video model weights frozen and only
trains a camera encoder. This leads to strong visual quality, but the generated videos show little
agreementwiththeinputcameraposes. Wehypothesizethatthisisduerandominitializationofthe
newtemporalattentionlayersandconvolutions. Forfaircomparison,wetrainedallmodelsforthe
samenumberofiterations(describedinSec.3.3). WebelievethatCameraCtrlrequiressubstantially
longertrainingtimetoconvergeincomparisontoourControlNet-inspiredconditioningmechanism
(seethePluckerconditioningmoduleinFig.3),whichisinitializedusingacopyofthecross-attention
andfeed-forwardlayerandfine-tuned.
TheresultsoftheuserstudyinTab.1showthatmostparticipantspreferthegeneratedvideosusing
theproposedcameraconditioningmechanismacrossallevaluatedsub-metrics. Wealsoobserve
apronouncedpreferenceforthecameraalignmentoftheproposedmethodcomparedtotheother
baselines. Thatis,82%and78%ofparticipantspreferthecameraalignmentoftheproposedmethod
comparedtoourrespectiveadaptationsofMotionCtrlandCameraCtrltothevideotransformermodel.
Allresultsaresignificantatthep<0.001levelasevaluatedusingaχ2test.
4.2 Ablations
Pluckerembedding. WemotivateourPluckerembeddingconditioningmechanismbytraininga
variantusingtherawcameramatrices. Concretely,weflattenandconcatenateextrinsicsandintrinsics
matricesinthechanneldimensionandrepeatthevaluesinthespatialpatchdimensions. Weobserve
thatPluckerembeddingsprovideanessentialspatialconditioningmechanism,asshowninTab.2.
8Table2: Cameraposeevaluation. Weevaluateallmodelsusingreferencecameratrajectoriesfrom
theRealEstate10Ktestset. Wecomputetranslationandrotationerrorsbasedonestimatedcamera
posesfromgenerationsusingParticleSfM[91].
RealEstate10K MSR-VTT
Method
TransError(↓) RotError(↓) TransError(↓) RotError(↓)
BaseModel 0.616 0.207 0.717 0.216
MotionCtrl 0.518 0.161 0.627 0.148
MotionCtrl(frozen) 0.607 0.205 0.678 0.122
CameraCtrl 0.532 0.165 0.578 0.220
Ours 0.409 0.043 0.504 0.050
w/oPlucker 0.517 0.161 0.676 0.156
w/oControlNet 0.573 0.182 0.787 0.179
w/oweightcopy 0.424 0.044 0.513 0.063
ControlNetconditioning. OurControlNetinspiredconditioningmechanismensuresfastandprecise
learningoftheconditioningsignaldistribution. InsteadofusingaControlNetblock,wesimplyadd
zero-initializedPluckerembeddingfeaturestothepatchesandobserveclosetonocameracontrol.
Weobservetrainingcross-attentionlayersintheControlNetblockiskeytolearningcameracontrol
whilepreservingtheoriginalmodelweights. ThisisconfirmedbyourcameraevaluationinTab.2.
ControlNetweightcopy. WhiletheControlNetblockisessential,copyingthepre-trainedweights
intothenewcopyhasratherminorimpact,asshowninTab.2. Toverifythis,wetrainamodelwhere
werandomlyinitializethecross-attentionblockbetweenpatchesandlatentsinsteadofcopyingthe
weights. Weobservesimilarresults,showingthatratherthearchitectureandzero-initializationare
thekeycomponentofthisdesign.
4.3 Applications
Image-to-videogeneration. Wesynthesizecamera-controlledvideosbasedondifferentcamera
posesasshowninFig.1. ForthistaskweuseaversionoftheSnapVideomodelthatwefine-tune
onvideosequenceswherearandomsubsetoftheinputframesaremasked. Atinferencetime,we
canprovideimageguidanceforanyofthegeneratedframes,providinganadditionaldimensionof
controllabilitywhenpairedwithourproposedmethodforcameracontrol. Todemonstrateimage-to-
videogenerationinFig.1,weconditionthemodelusingcameraposesalongwithimageguidance
fromthefirstframeofageneratedvideosequence. Notethatourmethodprovidesnocontrolover
motionwithinthesceneitself;hence,scenemotioncandifferdependingontherandomseedorthe
providedcameraposes.
Image-to-multiviewgeneration. Wealsoexploremulti-viewgenerationforstaticscenesasshownin
Fig.5. Givenarealinputimageofacomplexsceneunseenduringtraining,ourcamera-conditioned
modelgeneratesview-consistentrenderingsofthatscenefromarbitraryviewpoints.Thesemulti-view
renderingscouldbedirectlyrelevanttodownstream3Dreconstructionspipelines, e.g., basedon
NeRF[43]or3DGaussianSplatting[30]. Weshowthepotentialofcamera-conditionedimage-to-
multiviewgenerationforcomplex3Dscenegeneration,butweleavemoreextensiveexplorationof
thistopicforfuturework.
5 Conclusion
Large-scalevideotransformermodelsshowimmensepromisetosolvemanylong-standingchal-
lenges in computer vision, including novel-view synthesis, single-image 3D reconstruction, and
text-conditionedscenesynthesis. Ourworkbringsadditionalcontrollabilitytothesemodels,enabling
ausertospecifythecameraposesfromwhichvideoframesarerendered.
Limitationsandfuturework. Thereareseverallimitationstoourwork,whichhighlightimportant
futureresearchdirections. Forexample,whilerenderingstaticscenesfromdifferentcameraview-
9Camera “A bedroom with a bed, lamps and a window” “A house sitting in the middle of a grassy field”
Figure5: Conditionalmulti-viewgenerationonarealimage. Wecangeneratearbitrarycamera
trajectoriesfromagivenrealimageformulti-viewsynthesis,pavingthewaytosingle-imagescene
reconstructionusingcamera-controlledvideomodels.
pointsproducesresultsthatappear3Dconsistent,dynamicscenesrenderedfromdifferentcamera
viewpointscanhaveinconsistentmotion(seesupplementalvideos). Weenvisionthatfuturevideo
generation models will have fine-grained control over both scene motion and camera motion to
address this issue. Further, our approach applies camera conditioning only to the low-resolution
SnapVideomodelandwekeeptheirupsamplermodelfrozen(i.e.,withoutcameraconditioning)—it
maybepossibletofurtherimprovecameracontrolthroughjointtraining,thoughthisbringsadditional
architecturalengineeringandcomputationalchallenges. Finally,ourapproachiscurrentlylimited
to generation of relatively short videos (16 frames), based on the design and training scheme of
the SnapVideo model. Future work to address these limitations will enable new capabilities for
applicationsincomputervision,visualeffects,augmentedandvirtualreality,andbeyond.
Broaderimpacts. Recentvideogenerationmodelsdemonstratecoherent,photorealisticsynthesisof
complexscenes—capabilitiesthatarehighlysoughtafterfornumerousapplicationsacrosscomputer
vision,graphics,andbeyond.Ourkeytechnicalcontributionsrelatetocameracontrolofthesemodels,
whichcanbeappliedtoawiderangeofmethods. Aswithallgenerativemodelsandtechnologies,
underlyingtechnologiescanbemisusedbybadactorsinunintendedways. Whilethesemethods
continuetoimprove, researchersanddevelopersshouldcontinuetoconsidersafeguards, suchas
outputfiltering,watermarking,accesscontrol,andothers.
References
[1] SherwinBahmani,XianLiu,WangYifan,IvanSkorokhodov,VictorRong,ZiweiLiu,Xihui
Liu,JeongJoonPark,SergeyTulyakov,GordonWetzstein,AndreaTagliasacchi,andDavidB.
Lindell. Tc4d: Trajectory-conditionedtext-to-4dgeneration. InProc.ECCV,2024.
[2] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Hao Tang, Gordon Wetzstein,
LeonidasGuibas, LucVanGool,andRaduTimofte. 3d-awarevideogeneration. InTMLR,
2023.
[3] SherwinBahmani,IvanSkorokhodov,VictorRong,GordonWetzstein,LeonidasGuibas,Peter
Wonka,SergeyTulyakov,JeongJoonPark,AndreaTagliasacchi,andDavidB.Lindell. 4d-fy:
Text-to-4dgenerationusinghybridscoredistillationsampling. InProc.CVPR,2024.
[4] MaxBain,ArshaNagrani,GülVarol,andAndrewZisserman. Frozenintime: Ajointvideo
andimageencoderforend-to-endretrieval. InProc.ICCV,2021.
[5] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Do-
minikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion:
Scalinglatentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023.
[6] AndreasBlattmann, RobinRombach, HuanLing, TimDockhorn, SeungWookKim, Sanja
Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent
diffusionmodels. InProc.CVPR,2023.
10[7] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,Joe
Taylor,TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh. Video
generationmodelsasworldsimulators. OpenAItechnicalreports,2024.
[8] ZenghaoChai,ChenTang,YongkangWong,andMohanKankanhalli.Star:Skeleton-awaretext-
based4davatargenerationwithin-networkmotionretargeting.arXivpreprintarXiv:2406.04629,
2024.
[9] EricRChan,KokiNagano,MatthewAChan,AlexanderWBergman,JeongJoonPark,Axel
Levy,MiikaAittala,ShaliniDeMello,TeroKarras,andGordonWetzstein. Generativenovel
viewsynthesiswith3d-awarediffusionmodels. InProc.ICCV,2023.
[10] HaoxinChen,MenghanXia,YingqingHe,YongZhang,XiaodongCun,ShaoshuYang,Jinbo
Xing,YaofangLiu,QifengChen,XintaoWang,ChaoWeng,andYingShan. Videocrafter1:
Opendiffusionmodelsforhigh-qualityvideogeneration. arXivpreprintarXiv:2310.19512,
2023.
[11] Ting Chen and Lala Li. Fit: Far-reaching interleaved transformers. arXiv preprint
arXiv:2305.12689,2023.
[12] Wen-HsuanChu,LeiKe,andKaterinaFragkiadaki. Dreamscene4d: Dynamicmulti-object
scenegenerationfrommonocularvideos. arXivpreprintarXiv:2405.02280,2024.
[13] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
etal. Animageisworth16x16words: Transformersforimagerecognitionatscale. Proc.ICLR,
2021.
[14] YutaoFeng,YintongShang,XiangFeng,LeiLan,ShandianZhe,TianjiaShao,HongzhiWu,
KunZhou,HaoSu,ChenfanfuJiang,etal. Elastogen: 4dgenerativeelastodynamics. arXiv
preprintarXiv:2405.15056,2024.
[15] PengGao,LeZhuo,ZiyiLin,DongyangLiu,RuoyiDu,XuLuo,LongtianQiu,YuhangZhang,
etal. Lumina-t2x: Transformingtextintoanymodality,resolution,anddurationviaflow-based
largediffusiontransformers. arXivpreprintarXiv:2405.05945,2024.
[16] QuankaiGao,QiangengXu,ZheCao,BenMildenhall,WenchaoMa,LeChen,DanhangTang,
andUlrichNeumann. Gaussianflow: SplattingGaussiandynamicsfor4Dcontentcreation.
arXivpreprintarXiv:2403.12365,2024.
[17] RuiqiGao,AleksanderHolynski,PhilippHenzler,ArthurBrussee,RicardoMartin-Brualla,
Pratul Srinivasan, Jonathan T Barron, and Ben Poole. Cat3d: Create anything in 3d with
multi-viewdiffusionmodels. arXivpreprintarXiv:2405.10314,2024.
[18] YuweiGuo,CeyuanYang,AnyiRao,YaohuiWang,YuQiao,DahuaLin,andBoDai. Ani-
matediff: Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning.
Proc.ICLR,2024.
[19] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan
Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint
arXiv:2404.02101,2024.
[20] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video
diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint
arXiv:2211.13221,2022.
[21] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Proc.
NeurIPS,2017.
[22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
DiederikPKingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: High
definitionvideogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
11[23] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi,andDavidJ
Fleet. Videodiffusionmodels. Proc.NeurIPS,2022.
[24] ChenHou,GuoqiangWei,YanZeng,andZhiboChen. Training-freecameracontrolforvideo
generation. arXivpreprintarXiv:2406.10126,2024.
[25] TengHu,JiangningZhang,RanYi,YatingWang,HongruiHuang,JieyuWeng,YabiaoWang,
andLizhuangMa. Motionmaster: Training-freecameramotiontransferforvideogeneration.
arXivpreprintarXiv:2404.15789,2024.
[26] TianyuHuang,YihanZeng,HuiLi,WangmengZuo,andRynsonWHLau. Dreamphysics:
Learningphysicalpropertiesofdynamic3dgaussianswithvideodiffusionpriors.arXivpreprint
arXiv:2406.01476,2024.
[27] AllanJabri,DavidFleet,andTingChen. Scalableadaptivecomputationforiterativegeneration.
arXivpreprintarXiv:2212.11972,2022.
[28] YanqinJiang, LiZhang, JinGao, WeiminHu, andYaoYao. Consistent4d: Consistent360
{\deg}dynamicobjectgenerationfrommonocularvideo. Proc.ICLR,2024.
[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-basedgenerativemodels. Proc.NeurIPS,2022.
[30] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3dgaussian
splattingforreal-timeradiancefieldrendering. Proc.ACMTOG,2023.
[31] ZhengfeiKuang,ShengquCai,HaoHe,YinghaoXu,HongshengLi,LeonidasGuibas,and
Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with
cameracontrol. arXivpreprintarXiv:2405.17414,2024.
[32] Nupur Kumari, Grace Su, Richard Zhang, Taesung Park, Eli Shechtman, and Jun-Yan
Zhu. Customizing text-to-image diffusion with camera viewpoint control. arXiv preprint
arXiv:2404.12333,2024.
[33] Yao-Chih Lee, Yi-Ting Chen, Andrew Wang, Ting-Hsuan Liao, Brandon Y Feng, and Jia-
Bin Huang. Vividdream: Generating 3d scene with ambient dynamics. arXiv preprint
arXiv:2405.20334,2024.
[34] Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard
Ghanem. Vivid-zoo: Multi-view video generation with diffusion model. arXiv preprint
arXiv:2406.08659,2024.
[35] RenjieLi,PanwangPan,BangbangYang,DejiaXu,ShijieZhou,XuanyangZhang,ZemingLi,
AchutaKadambi,ZhangyangWang,andZhiwenFan. 4k4dgen: Panoramic4dgenerationat4k
resolution. arXivpreprintarXiv:2406.13527,2024.
[36] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos N
Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d
generationviavideodiffusionmodels. arXivpreprintarXiv:2405.16645,2024.
[37] HuanLing,SeungWookKim,AntonioTorralba,SanjaFidler,andKarstenKreis. Alignyour
gaussians: Text-to-4dwithdynamic3dgaussiansandcomposeddiffusionmodels. InProc.
CVPR,2024.
[38] PengyangLing,JiaziBu,PanZhang,XiaoyiDong,YuhangZang,TongWu,HuaianChen,Jiaqi
Wang,andYiJin. Motionclone: Training-freemotioncloningforcontrollablevideogeneration.
arXivpreprintarXiv:2406.05338,2024.
[39] MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,MukundVarmaT,ZexiangXu,andHaoSu.
One-2-3-45: Anysingleimageto3dmeshin45secondswithoutper-shapeoptimization. Proc.
NeurIPS,2024.
[40] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian
Chen,andYuQiao. Latte: Latentdiffusiontransformerforvideogeneration. arXivpreprint
arXiv:2401.03048,2024.
12[41] WilliMenapace,AliaksandrSiarohin,IvanSkorokhodov,EkaterinaDeyneka,Tsai-ShienChen,
Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled
spatiotemporaltransformersfortext-to-videosynthesis. Proc.CVPR,2024.
[42] QiaoweiMiao,YaweiLuo,andYiYang. Pla4d: Pixel-levelalignmentsfortext-to-4dgaussian
splatting. arXivpreprintarXiv:2405.19957,2024.
[43] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,
andRenNg. Nerf: Representingscenesasneuralradiancefieldsforviewsynthesis. InProc.
ECCV,2020.
[44] NormanMüller,KatjaSchwarz,BarbaraRössle,LorenzoPorzi,SamuelRotaBulò,Matthias
Nießner, and Peter Kontschieder. Multidiff: Consistent novel view synthesis from a single
image. InProc.CVPR,2024.
[45] ZijiePan,ZeyuYang,XiatianZhu,andLiZhang. Fastdynamic3dobjectgenerationfroma
single-viewvideo. arXivpreprintarXiv:2401.08742,2024.
[46] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. Proc.ICCV,
2023.
[47] RyanPo,WangYifan,VladislavGolyanik,KfirAberman,JonathanTBarron,AmitHBermano,
EricRyanChan,TaliDekel,AleksanderHolynski,AngjooKanazawa,etal. Stateofthearton
diffusionmodelsforvisualcomputing. arXivpreprintarXiv:2310.07204,2023.
[48] GuochengQian,JinjieMai,AbdullahHamdi,JianRen,AliaksandrSiarohin,BingLi,Hsin-
YingLee,IvanSkorokhodov,PeterWonka,SergeyTulyakov,etal. Magic123: Oneimageto
high-quality3dobjectgenerationusingboth2dand3ddiffusionpriors. Proc.ICLR,2024.
[49] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. Proc.JMLR,2020.
[50] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu.
DreamGaussian4D:Generative4DGaussiansplatting. arXivpreprintarXiv:2312.17142,2023.
[51] JiaweiRen,KevinXie,AshkanMirzaei,HanxueLiang,XiaohuiZeng,KarstenKreis,ZiweiLiu,
AntonioTorralba,SanjaFidler,SeungWookKim,etal.L4gm:Large4dgaussianreconstruction
model. arXivpreprintarXiv:2406.10324,2024.
[52] Abhishek Sharma, Adams Yu, Ali Razavi, Andeep Toor, Andrew Pierson, Ankush Gupta,
AustinWaters,DanielTanis,DumitruErhan,EricLau,EleniShaw,GabeBarth-Maron,Greg
Shaw,HanZhang,HennaNandwani,HernanMoraldo,HyunjikKim,IrinaBlok,JakobBauer,
JeffDonahue,JunyoungChung,KoryMathewson,KurtisDavid,LasseEspeholt,Marcvan
Zee, Matt McGill, Medhini Narasimhan, Miaosen Wang, Mikołaj Bin´kowski, Mohammad
Babaeizadeh,MohammadTaghiSaffar,NickPezzotti,Pieter-JanKindermans,PoorvaRane,
RachelHornung,RobertRiachi,RubenVillegas,RuiQian,SanderDieleman,SerenaZhang,
SerkanCabi,ShixinLuo,ShlomiFruchter,SigneNørly,SrivatsanSrinivasan,TobiasPfaff,Tom
Hume,VikasVerma,WeizheHua,WilliamZhu,XinchenYan,XinyuWang,YelinKim,Yuqing
Du,andYutianChen. Veo,2024.
[53] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,
HarryYang,OronAshual,OranGafni,etal. Make-a-video: Text-to-videogenerationwithout
text-videodata. Proc.ICLR,2023.
[54] UrielSinger,ShellySheynin,AdamPolyak,OronAshual,IuriiMakarov,FilipposKokkinos,
NamanGoyal,AndreaVedaldi,DeviParikh,JustinJohnson,etal. Text-to-4ddynamicscene
generation. InProc.ICML,2023.
[55] QiSun,ZhiyangGuo,ZiyuWan,JingNathanYan,ShengmingYin,WengangZhou,JingLiao,
and Houqiang Li. Eg4d: Explicit generation of 4d object without score distillation. arXiv
preprintarXiv:2405.18132,2024.
13[56] Hung-YuTseng,QinboLi,ChangilKim,SuhibAlsisan,Jia-BinHuang,andJohannesKopf.
Consistentviewsynthesiswithpose-guideddiffusionmodels. InProc.CVPR,2023.
[57] ThomasUnterthiner,SjoerdvanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,
andSylvainGelly. Towardsaccurategenerativemodelsofvideo: Anewmetric&challenges.
arXivpreprintarXiv:1812.01717,2018.
[58] Lukas Uzolas, Elmar Eisemann, and Petr Kellnhofer. Motiondreamer: Zero-shot 3d mesh
animationfromvideodiffusionmodels. arXivpreprintarXiv:2405.20155,2024.
[59] BasileVanHoorick,RundiWu,EgeOzguroglu,KyleSargent,RuoshiLiu,PavelTokmakov,
AchalDave,ChangxiZheng,andCarlVondrick. Generativecameradolly: Extrememonocular
dynamicnovelviewsynthesis. arXivpreprintarXiv:2405.14868,2024.
[60] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Proc.NeurIPS,2017.
[61] VikramVoleti, Chun-HanYao, MarkBoss, AdamLetts, DavidPankratz, DmitryTochilkin,
ChristianLaforte,RobinRombach,andVarunJampani. Sv3d: Novelmulti-viewsynthesisand
3dgenerationfromasingleimageusinglatentvideodiffusion.arXivpreprintarXiv:2403.12008,
2024.
[62] WenjingWang,HuanYang,ZixiTuo,HuiguoHe,JunchenZhu,JianlongFu,andJiayingLiu.
Videofactory: Swapattentioninspatiotemporaldiffusionsfortext-to-videogeneration. arXiv
preprintarXiv:2305.10874,2023.
[63] XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang,Yujun
Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with
motioncontrollability. arXivpreprintarXiv:2306.02018,2023.
[64] YikaiWang,XinzhouWang,ZilongChen,ZhengyiWang,FuchunSun,andJunZhu. Vidu4d:
Singlegeneratedvideotohigh-fidelity4dreconstructionwithdynamicgaussiansurfels. arXiv
preprintarXiv:2405.16822,2024.
[65] ZhouxiaWang,ZiyangYuan,XintaoWang,TianshuiChen,MenghanXia,PingLuo,andYin
Shan. Motionctrl: A unified and flexible motion controller for video generation. In arXiv
preprintarXiv:2312.03641,2023.
[66] DanielWatson,WilliamChan,RicardoMartin-Brualla,JonathanHo,AndreaTagliasacchi,and
MohammadNorouzi. Novelviewsynthesiswithdiffusionmodels. Proc.ICLR,2023.
[67] DanielWatson,SaurabhSaxena,LalaLi,AndreaTagliasacchi,andDavidJFleet. Controlling
spaceandtimewithdiffusionmodels. arXivpreprintarXiv:2407.07860,2024.
[68] ChenfeiWu,LunHuang,QianxiZhang,BinyangLi,LeiJi,FanYang,GuillermoSapiro,and
NanDuan. Godiva: Generatingopen-domainvideosfromnaturaldescriptions. arXivpreprint
arXiv:2104.14806,2021.
[69] RuiqiWu,,LiangyuChen,TongYang,ChunleGuo,ChongyiLi,andXiangyuZhang. Lamp:
Learnamotionpatternforfew-shot-basedvideogeneration. arXivpreprintarXiv:2310.10769,
2023.
[70] ZijieWu,ChaohuiYu,YanqinJiang,ChenjieCao,FanWang,andXiangBai. Sc4d: Sparse-
controlledvideo-to-4dgenerationandmotiontransfer. arXivpreprintarXiv:2404.03736,2024.
[71] ZeqiXiao,YifanZhou,ShuaiYang,andXingangPan. Videodiffusionmodelsaretraining-free
motioninterpreterandcontroller. arXivpreprintarXiv:2405.14864,2024.
[72] DejiaXu,HanwenLiang,NeelPBhatt,HezhenHu,HanxueLiang,KonstantinosNPlataniotis,
andZhangyangWang. Comp4d:Llm-guidedcompositional4dscenegeneration. arXivpreprint
arXiv:2403.16993,2024.
[73] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vah-
dat. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint
arXiv:2406.02509,2024.
14[74] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for
bridgingvideoandlanguage. InProc.CVPR,2016.
[75] ZhongcongXu,JianfengZhang,JunHaoLiew,WenqingZhang,SongBai,JiashiFeng,and
MikeZhengShou. Pv3d: A3dgenerativemodelforportraitvideogeneration. Proc.ICLR,
2023.
[76] HongweiXue,TiankaiHang,YanhongZeng,YuchongSun,BeiLiu,HuanYang,JianlongFu,
andBainingGuo. Advancinghigh-resolutionvideo-languagerepresentationwithlarge-scale
videotranscriptions. InProc.CVPR,2022.
[77] QitongYang,MingtaoFeng,ZijieWu,ShijieSun,WeishengDong,YaonanWang,andAjmal
Mian. Beyondskeletons: Integrativelatentmappingforcoherent4dsequencegeneration. arXiv
preprintarXiv:2403.13238,2024.
[78] ShiyuanYang,LiangHou,HaibinHuang,ChongyangMa,PengfeiWan,DiZhang,Xiaodong
Chen,andJingLiao. Direct-a-video: Customizedvideogenerationwithuser-directedcamera
movementandobjectmotion. arXivpreprintarXiv:2402.03162,2024.
[79] ZeyuYang,ZijiePan,ChunGu,andLiZhang. Diffusion²: Dynamic3dcontentgenerationvia
scorecompositionoforthogonaldiffusionmodels. arXivpreprint2404.02148,2024.
[80] HuYe, JunZhang, SiboLiu, XiaoHan, andWeiYang. Ip-adapter: Textcompatibleimage
promptadapterfortext-to-imagediffusionmodels. arXivpreprintarXiv:2308.06721,2023.
[81] YuyangYin,DejiaXu,ZhangyangWang,YaoZhao,andYunchaoWei. 4dgen: Grounded4d
contentgenerationwithspatial-temporalconsistency. arXivpreprintarXiv:2312.17225,2023.
[82] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,
XiaodanSong,JamesDemmel,KurtKeutzer,andCho-JuiHsieh. Largebatchoptimizationfor
deeplearning: Trainingbertin76minutes. arXivpreprintarXiv:1904.00962,2019.
[83] HengYu,ChaoyangWang,PeiyeZhuang,WilliMenapace,AliaksandrSiarohin,JunliCao,
LaszloAJeni,SergeyTulyakov,andHsin-YingLee. 4real: Towardsphotorealistic4dscene
generationviavideodiffusionmodels. arXivpreprintarXiv:2406.07472,2024.
[84] JasonJYu,FereshtehForghani,KonstantinosGDerpanis,andMarcusABrubaker. Long-term
photometricconsistentnovelviewsynthesiswithdiffusionmodels. InProc.ICCV,2023.
[85] RazaYunus,JanEricLenssen,MichaelNiemeyer,YiyiLiao,ChristianRupprecht,Christian
Theobalt,GerardPons-Moll,Jia-BinHuang,VladislavGolyanik,andEddyIlg. Recenttrends
in3dreconstructionofgeneralnon-rigidscenes. InComputerGraphicsForum,2024.
[86] YifeiZeng,YanqinJiang,SiyuZhu,YuanxunLu,YoutianLin,HaoZhu,WeimingHu,Xun
Cao,andYaoYao. Stag4d: Spatial-temporalanchoredgenerative4dgaussians. arXivpreprint
arXiv:2403.14939,2024.
[87] HaiyuZhang,XinyuanChen,YaohuiWang,XihuiLiu,YunhongWang,andYuQiao.4diffusion:
Multi-viewvideodiffusionmodelfor4dgeneration. arXivpreprintarXiv:2405.20674,2024.
[88] Hao Zhang, Di Chang, Fang Li, Mohammad Soleymani, and Narendra Ahuja. Magic-
pose4d: Crafting articulated models with appearance and motion control. arXiv preprint
arXiv:2405.14017,2024.
[89] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusionmodels. Proc.ICCV,2023.
[90] TianyuanZhang,Hong-XingYu,RundiWu,BrandonYFeng,ChangxiZheng,NoahSnavely,
JiajunWu,andWilliamTFreeman. Physdreamer: Physics-basedinteractionwith3dobjects
viavideogeneration. arXivpreprintarXiv:2404.13026,2024.
[91] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm:
Exploitingdensepointtrajectoriesforlocalizingmovingcamerasinthewild. InProc.ECCV,
2022.
15[92] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. An-
imate124: Animating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603,
2023.
[93] YufengZheng,XuetingLi,KokiNagano,SifeiLiu,OtmarHilliges,andShaliniDeMello. A
unifiedapproachfortext-andimage-guided4dscenegeneration. InProc.CVPR,2024.
[94] DaquanZhou,WeiminWang,HanshuYan,WeiweiLv,YizheZhu,andJiashiFeng.Magicvideo:
Efficientvideogenerationwithlatentdiffusionmodels. arXivpreprintarXiv:2211.11018,2022.
[95] TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,andNoahSnavely. Stereomagnifi-
cation: Learningviewsynthesisusingmultiplaneimages. Proc.ACMTOG,2018.
16Table3: Multi-viewgeneration. Weevaluateallmodelsusingreferencecameratrajectoriesand
single-viewinputimagesoftheRealEstate10Ktestset. Wecomputereconstructionmetricsbasedon
thesubsequentframesforthelow-resolutionandupsampledhigh-resolutiongenerations.
Low-resolution High-resolution
Method
PSNR(↑) SSIM(↑) LPIPS(↓) PSNR(↑) SSIM(↑) LPIPS(↓)
BaseModel 14.74 0.320 0.334 13.23 0.459 0.572
MotionCtrl 15.07 0.348 0.308 13.42 0.467 0.560
MotionCtrl(frozen) 14.59 0.308 0.340 13.11 0.455 0.573
CameraCtrl 14.81 0.327 0.330 13.21 0.456 0.571
Ours 17.23 0.534 0.211 14.90 0.499 0.499
w/oPlucker 14.89 0.346 0.308 13.05 0.455 0.573
w/oControlNet 14.66 0.313 0.340 13.10 0.450 0.573
w/oweightcopy 16.96 0.509 0.220 14.75 0.495 0.504
Table 4: Quality metrics evaluation. We evaluate all models using text prompts from the
RealEstate10KandMSR-VTTtestsetsrespectively.
RealEstate10K MSR-VTT
Method
FID(↓) FVD(↓) CLIPSIM(↑) FID(↓) FVD(↓) CLIPSIM(↑)
BaseModel 8.22 160.37 0.2677 3.50 141.26 0.2774
MotionCtrl 1.50 52.30 0.2708 9.97 183.57 0.2677
MotionCtrl(frozen) 3.53 142.15 0.2772 8.19 165.48 0.2679
CameraCtrl 2.28 66.31 0.2730 8.47 181.90 0.2690
Ours 1.40 42.43 0.2807 7.80 165.18 0.2689
w/oPlucker 1.17 43.65 0.2715 9.84 152.91 0.2660
w/oControlNet 3.66 137.06 0.2766 8.34 185.79 0.2674
w/oweightcopy 1.38 42.00 0.2710 10.09 218.43 0.2647
A Quantitativeevaluation
Wefurtherevaluateallmodelsforthetaskofsingleimage-to-multiviewgeneration. Furthermore,we
provideresultsforestablished2Dvideogenerationmetrics.
A.1 Multi-viewgeneration
Weevaluateourmodelforimage-to-multiviewgeneration. Duetogroundtruthcorrespondencesfor
theRealEstate10K[95]dataset,wecanconditionourmodelonagivencameratrajectoryandassess
imagebasedmetrics,i.e.,PSNR,SSIM,andLPIPS.Weprovideresultsforthelow-resolutionbase
modelandtheupsampledhigh-resolutionresultsinTab.3.
A.2 Qualitymetrics
Moreover, we evaluate all models on established image and video generation metrics, namely,
FID [21], FVD [57], and CLIPSIM [68]. We provide results for RealEstate10K [95] and MSR-
VTT[74]inTab.4.
17