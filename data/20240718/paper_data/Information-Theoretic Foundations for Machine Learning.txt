Information-Theoretic Foundations for Machine Learning
Hong Jun Jeon1∗ and Benjamin Van Roy2,3
1Department of Computer Science, Stanford University
2Department of Electrical Engineering, Stanford University
3Department of Management Science and Engineering, Stanford University
Abstract
The staggering progress of machine learning in the past decade has been a sight to behold. In retrospect,
it is both remarkable and unsettling that these milestones were achievable with little to no rigorous theory to
guide experimentation. Despite this fact, practitioners have been able to guide their future experimentation
via observations from previous large-scale empirical investigations. However, alluding to Plato’s Allegory of
the cave, it is likely that the observations which form the field’s notion of reality are but shadows representing
fragmentsofthatreality. Inthiswork,weproposeatheoreticalframeworkwhichattemptstoanswerwhatexists
outside of the cave. To the theorist, we provide a framework which is mathematically rigorous and leaves open
many interesting ideas for future exploration. To the practitioner, we provide a framework whose results are
veryintuitive,general,andwhichwillhelpformprinciplestoguidefutureinvestigations. Concretely,weprovide
atheoreticalframeworkrootedinBayesianstatisticsandShannon’sinformationtheorywhichisgeneralenough
to unify the analysis of many phenomena in machine learning. Our framework characterizes the performance
ofanoptimalBayesianlearner,whichconsidersthefundamentallimitsofinformation. Unlikeexistinganalyses
that weaken with increasing data complexity, our theoretical tools provide accurate insights across diverse
machine learning settings. Throughout this work, we derive very general theoretical results and apply them to
derive insights specific to settings ranging from data which is independently and identically distributed under
an unknown distribution, to data which is sequential, to data which exhibits hierarchical structure amenable
to meta-learning. We conclude with a section dedicated to characterizing the performance of misspecified
algorithms. These results are exciting and particularly relevant as we strive to overcome increasingly difficult
machine learning challenges in this endlessly complex world.
∗Correspondencetohjjeon@stanford.edu.
1
4202
luJ
71
]LM.tats[
1v88221.7042:viXraContents
1 Introduction 4
2 Related Works 5
2.1 Frequentist and Bayesian Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 PAC Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3 A Framework for Learning 7
3.1 Probabilistic Framework and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.2 Data Generating Process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.3 Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.4 Achievable Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4 Requisite Information Theory 11
4.1 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.2 Conditional Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
4.3 Mutual Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.4 Differential Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.5 Requisite Results from Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5 Connecting Learning and Information Theory 18
5.1 Error is Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
5.2 Characterizing Error via Rate-Distortion Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6 Learning from iid Data 22
6.1 Theoretical Results Tailored for iid Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
6.2 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.2.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.2.2 Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.2.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
6.3 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
6.3.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
6.3.2 Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
6.3.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
6.4 Deep Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.4.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.4.2 Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
6.4.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
6.5 Nonparametric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
6.5.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
6.5.2 Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
6.5.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
7 Learning from Sequences 41
7.1 Data Generating Process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7.2 Binary AR(K) Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7.2.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7.2.2 Preliminary Theoretical Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7.2.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
7.3 Transformer Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
7.3.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
27.3.2 Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
7.3.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
8 Meta-Learning 52
8.1 Data Generating Process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
8.2 Meta-Learning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
8.3 Achievable Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
8.4 General Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
8.5 Linear Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
8.5.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
8.5.2 Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
8.5.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
8.6 In-Context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
8.6.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
8.6.2 Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
8.6.3 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
9 Misspecification 66
9.1 General Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
9.2 Linear Regression with Misspecification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
9.2.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
9.2.2 Misspecified Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
9.3 Neural Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
10 Conclusion 73
Appendix 75
A Lower Bounds for Linear Regression 75
B Theoretical Results for Dirichlet-Multinomial 77
B.1 General Combinatorics Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
B.2 Lemmas pertaining to Dirichlet Multinomial. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
31 Introduction
Inthepastdecade,thestaggeringprogressofmachinelearninghasbeenasighttobehold. Theresearchcommunity
has conquered games such as go, which were thought to require human-level learning and abstraction capabilities
[Silver et al., 2016]. We have produced systems which are capable of displaying common-sense and holding coher-
ent dialogues with humans around the globe [Achiam et al., 2023]. It is undeniable that these artifacts will be
remembered throughout the future of humanity’s pursuit of discovering and understanding intelligence.
In retrospect, it is both remarkable and unsettling that these milestones were achievable with little to no rigorous
theory to guide experimentation. While theorists have attempted to repurpose existing statistical tools to analyze
modern machine learning, the conclusions have largely contradicted the observations of practitioners. Zhang et al.
[2021] aptly demonstrated this point via a series of simple experiments which elucidated the fundamental incom-
patibility of empirically observed phenomena with existing notions of generalization. Despite this incoherence,
practitioners have been able to guide their future experimentation based on prior large-scale empirical investiga-
tions. However, without a clear idea of how these phenomena slot into a larger picture, many research efforts will
continue to be led astray. Alluding to Plato’s Allegory of the cave, it is likely that the observations which form the
field’s notion of reality are but shadows representing fragments of that reality.
In this work, we propose a theoretical framework which attempts to answer what exists outside of the cave. To
the theorist, we provide a framework which is mathematically rigorous and leaves open many interesting ideas for
futureexploration. Tothepractitioner,weprovideaframeworkwhoseresultsareveryintuitive,general,andwhich
will help form principles to guide future investigations. Concretely, we provide a theoretical framework rooted in
Bayesian statistics which is general enough to unify the analysis of many phenomena in machine learning.
Our theoretical framework draws inspiration from both Shannon’s theory of information and his maxim of “infor-
mation first, then computation”. The turn of the twentieth century brought a wave of interest in communications
research; work that would enable the transmission of signals across long distances. Much of the work in encod-
ing/decoding was approached heuristically, similarly to how deep learning is today. Shannon’s theory and maxim
redirected attention to characterizing what was fundamentally possible or impossible, in the absence of compu-
tational constraints. His theory guided the discovery of algorithms which achieved these fundamental limits and
eventually practical implementations as well.
The aforementioned staggering feats of machine learning and artificial intelligence have fueled optimism that any-
thingislearnablewithsufficientdataandcomputation. However,researchdirectionshavelargelybeeninformedby
informal reasoning supported by a plethora of empirical studies. While work in statistics provides some guidance,
the results for the most part lack the generality required to explain the continuing onslaught of novel empirical
findings. This monograph aims to provide a general framework to elucidate what is possible by studying how the
limitsofperformanceinmachinelearningdependontheinformationalcontentofthedata. Ourframeworkisbased
on Shannon’s information theory and characterizes the dependence of performance on information in the absence
of computational constraints. Concretely, we characterize the performance of an optimal Bayesian learner that
observes data generated by a suite of data generating processes of increasing complexity. By expressing what is
fundamentallypossibleinmachinelearning,weaimtodevelopintuitionthatcanguide fruitfulinvestigation.
Our framework characterizes the performance of an optimal Bayesian learner, which considers the fundamental
limits of information. Unlike existing analyses that weaken with increasing data complexity, our theoretical tools
provide accurate insights across diverse machine learning settings. For example, previous theories about learning
from sequential data rely on specific and rigid mixing time assumptions. However, Jeon et al. [2024] leverage our
framework to arrive at more general results which characterize the sample complexity of learning from sequences
autoregressively generated by transformers [Vaswani et al., 2017]. They further extend the techniques to analyze
hierarchical data generating processes which resemble meta-learning and in-context learning in large language
models (LLMs). That these analytic techniques remarkably apply whether data is exchangeable or exhibits more
complex structure points to the fundamental nature of our findings.
In recent years, we have observed that training larger machine learning models on more data continues to produce
significantly better performance. This continual improvement indicates that the data generating processes that
we study are more complex than the machine learning models which we fit. We refer to this phenomenon as
“misspecification” and it is prominently observed in natural language processing (NLP), where researchers have
tried to mathematically characterize this improvement in performance [Kaplan et al., 2020, Hoffmann et al., 2022].
The “neural scaling laws” of Kaplan et al. [2020] and Hoffmann et al. [2022] characterize the rate at which out-of-
4sample log-loss decreased with respect to more available compute and data. While these works provide extensive
empirical experimentation, their cursory mathematical analysis leave open many questions regarding how scaling
laws change depending on the complexity of the data-generating process. Jeon and Roy [2024] once again use the
theoretical tools from this monograph to rigorously characterize the error incurred by a misspecified algorithm.
They study a data-generating process which is identified by an single hidden-layer network of infinite width and
characterize how an algorithm with finite budget should optimally allocate between parameter count and dataset
size. Theseresultsnotablyareconsistent(uptologarithmicfactors)withthethefindingsofHoffmannetal.[2022],
where the optimal dataset size and parameter count exhibit a linear relationship.
Despitethefactthatourtheorydoesnotaddresscomputationalconstraints,empiricalstudieswithneuralnetworks
suggest that practical stochastic gradient algorithms suffice to attain the tradeoffs that our theory establishes
between information and performance [Zhu et al., 2022]. Throughout this work, we derive very general theoretical
results and apply them to derive insights specific to settings ranging from data which is iid under an unknown
distribution to data which is sequential to data which exhibits hierarchical structure amenable to meta-learning.
We conclude with a section dedicated to characterizing the performance of suboptimal algorithms that are based
on misspecified models, an exciting and relevant direction for future work.
2 Related Works
2.1 Frequentist and Bayesian Statistics
We begin with a discussion of frequentist statistics, the predominant framework which encompasses existing theo-
retical results. As its name would suggest, in frequentist statistics, probability describes how often an event occurs
ifaprocedureisrepeatedmanytimes. Forinstance, supposethereexistsanunknown parameterµ∈ℜandasam-
ple of size T: (X ,X ,...,X ) which is drawn iid N(µ,1). After observing the sample, the frequentist statistician
1 2 T
may construct a 95% confidence interval for the unknown µ. However, recall that in frequentism, probability is
assigned to how often an event occurs if a procedure is repeated many times. For our example, this entails that
if random samples of size T were drawn repeatedly and their corresponding confidence intervals constructed, then
95% of those confidence intervals would contain µ. Note that the unknown parameter µ is fixed and hence not
a random variable in the frequentist framework. As a result, the frequentist framework does not use the tools of
probability to model uncertainty pertaining to µ.
In contrast, Bayesian statistics treats all unknown quantities as random variables. A consequence is that these
quantities must be assigned subjective probabilities which reflect one’s prior beliefs pertaining to their values.
Returning to our example, the Bayesian may assign a prior distribution P(µ ∈ ·) = N(0,1) which reflects their
beliefspriortoobservingthesample. Afterobservingthesample(x ,x ,...,x ),theymayconstructa95%credible
1 2 T
interval for µ, an interval (a,b) for which P(µ ∈ (a,b)|X = x ,X = x ,...,X = x ) ≥ 0.95. The posterior
1 1 2 2 T T
distribution P(µ∈·|X =x ,X =x ,...,X =x ) is computed via Bayes rule. Note that unlike the frequentist
1 1 2 2 T T
confidence interval which pertains to repeated experimentation, the Bayesian credible interval states that for this
particular sample, with 95% probability, µ∈(a,b). However, we note that this probability is subjective as it relies
upon the prior subjective probability P(θ ∈ ·). While this subjectivism has been a topic of constant philosophical
debate, we note that in the realm of decision theory, it is well known that the choices of a decision maker which
abides by axioms of rationality can be explained as the result of a utility function and subjective probabilities
assigned to events [Savage, 1972].
While the debate surrounding these two schools of thought have continued for almost a century, it is prudent to
consider the purpose for such theory. We are reminded of Laplace’s prudent remark that “Probability theory is
nothing but common sense reduced to calculation”. Theory’s merit ought to stem from the results it can provide
for specific problems [Jaynes and Kempthorne, 1976]. Jaynes and Kempthorne [1976] espoused this viewpoint as
their background in physics led to their interest in the use of probability to describe and predict phenomena of
the physical world. Machine learning too is is rooted in predictions based on data produced by the physical world.
Therefore, we argue that the merits of machine learning theory also ought to stem from its ability to describe and
predict phenomena of data generated by the physical world. To this end, we believe that the results which we
derive via our framework both better reflect what is observed empirically and are also general enough to unify
many disparate areas of the field.
52.2 PAC Learning
The majority of existing theoretical results for the analysis of modern machine learning are set in the probably
approximately correct (PAC) learning framework [Valiant, 1984]. In PAC learning, an algorithm is presented with
a sample of data and is tasked with returning a hypothesis from a hypothesis set which can accurately perform
predictions out-of-sample. The probably approximately correct come from the detail that these results are phrased
as follows: “For any data distribution, with probability at least 1−δ over the randomness of an iid sample, the
excess out-of-sample error is ≤ ϵ”. “Probably” refers to the 1−δ probability and “approximately correct” the ϵ
tolerance of out-of-sample error. While this framework has facilitated the development of influential theoretical
concepts such as VC dimension [Vapnik, 2013] and Rademacher complexity [Bartlett and Mendelson, 2002], Zhang
etal.[2021]havedemonstratedthatthesetoolsareinherentlyinsufficienttoexplainmodernempiricalphenomena.
Namely, they demonstrate empirically that while the Rademacher complexity of a deep neural networks leads to
vacuoustheoreticalresults, theobservedout-of-sampleerrorofthesedeepneuralnetworksisactuallysmall.
Wepositthattheloosenessofthesetheoreticalresultsstemsfromthefactthattheyholdforany datadistribution
anduniformly overthehypothesisset. WhileitistruethatRademachercomplexitydependsonthedistributionof
theinputs,itdoesnot dependonthejointdistributionoftheinputand outputs. Meanwhile,datawhichweobserve
from the real world clearly contains inherent structure between input and output which facilitate sample-efficient
learning. Suppose we perform binary classification with input X ∼N(0,I ). In case 1, consider a data generating
d
process in which the corresponding class Y only depends on the first element of X. In case 2, consider a data
generating process in which Y depends on all d elements of X. Common sense would dictate that if we observed
an equal number of samples from each data generating process and considered the same hypothesis spaces, the
generalization error of case 1 ought to be lower than that of case 2. However, an analysis via VC dimension or
Rademachercomplexitywouldresultinthesame generalizationboundforcase1and2. Thisproblemexacerbated
by high dimensional input distributions and overparameterized hypothesis classes, both of which are prevalent
qualities of deep learning.
Several lines of analysis have attempted to ameliorate this via data dependent bounds. While these results also
hold for any data distribution, the choice of data distribution will impact the resulting error bound. Therefore,
such a result will be vacuous (as expected) for a problem instance with unstructured data, but potentially much
tighter for one which exhibits structure. The main frameworks for data dependent PAC results involve PAC Bayes
[McAllester, 1998] and the information-theoretic framework of Xu and Raginsky [2017]. Both frameworks involve
an algorithm which produces a predictive distribution of the hypothesis conditioned on the observed data (hence
they analyze a Bayesian algorithm under the PAC framework). The two only differ in that PAC Bayes results hold
with high probability over random draws of the data while the information-theoretic results hold in expectation
over random draws of the data. Therefore, each result upper bounds generalization error via the KL divergence or
the mutual information between the observed data and the hypothesis.
These data dependent results mark a significant step in understanding the puzzling empirical success of deep
learning. Notably, Dziugaite and Roy [2017] establish PAC-Bayes results for deep neural networks which result in
bounds that dramatically improve upon those based on parameter count or VC dimension. These results reflect
the importance that the data generating process has on the generalization error. However, a limitation is the
lack of theoretical tools which facilitate analytic derivations. Namely, the aforementioned KL divergence/mutual
information which bound generalization cannot be computed analytically outside of simple problem instances.
This is because these quantities involve the posterior distribution of the hypothesis conditioned on the data, which
cannot be expressed analytically outside of simple instances which exhibit a conjugate distribution. In contrast,
our results analyze these quantities in a Bayesian setting, allowing us to develop general tools to bound mutual
information analytically without needing to write down these complicated posterior distributions. The conciseness
and generality of these results lead us to believe that they are fundamental.
2.3 Information Theory
TheresultsofthisworkelucidatethetightrelationbetweenerrorinlearningandinformationmeasuresofShannon’s
theory [Shannon, 1948]. Prior work establishes connections between information theory and parameter estimation
notably through information-theoretic lower bounds. Namely, the global Fano method resembles a portion of the
techniques which we will cover in this monograph.
Awidelyknownframeworkinvolvinginformationtheoryandmachinelearningistheinformationbottleneckmethod
[Tishby et al., 2000]. On the surface, this work exhibits many similarities to ours as it draws a connection between
6information theory, rate-distortion theory and machine learning. However, the two works diverge in their purpose.
Theinformationbottleneckframeworkdescribesalearningobjectiverootedininformationtheoryandprescribesa
learningalgorithmtosolvethisoptimizationproblem. Whiletheyleveragedtheirframeworktoproduceearlywork
on generalization in deep neural networks [Tishby and Zaslavsky, 2015], the results remained very abstract. While
they devised metrics which they approximate empirically, they do not provide theoretical tools to analyze these
metrics analytically. In contrast, we present our framework with a collection of theoretical tools which facilitate
analytic solutions. This is an important property of a theoretical framework as it allows the researcher to forecast
what ought to be possible in practice.
Asalludedtoabove,thereexistnotableinformation-theoreticgeneralizationresultsinthePAClearningframework
introducedbyRussoandZou[2019]andexpandeduponbyXuandRaginsky[2017]. Whileourworksharessimilar
analytictechniques,weareabletostrengthentheresultsandprovidemorestreamlinedtheoreticaltoolsbyframing
resultsinaBayesiansetting. Inparticular,thisframingallowsustoupperandlowerboundthemutualinformation
between the data and a latent parameter via the rate-distortion function, which we can characterize analytically
for even complex data generating processes. We find the results of our framing to be fundamental and we hope
that the readers share this sentiment. We also hope that provides the reader with a new perspective on machine
learning.
The recent advances in LLMs have incited an interest in the connection between learning and compression. In
particular, researchers have posited that models which are better able to compress the observed data will achieve
lower out-of-sample error. This point is conveyed in [Deletang et al., 2024]. However, they do not provide a math-
ematically rigorous connection between learning and compression. In this work we establish a rigorous connection
between learning and optimal lossy compression. The loss incurred by an optimal Bayesian learner is upper and
lowerboundedbyappropriateexpressionscontainingtherate-distortionfunction(characterizationofoptimallossy
compression). For this community, we hope that our work provides clarity to this matter and informs future
experimentation and algorithm design.
3 A Framework for Learning
3.1 Probabilistic Framework and Notation
In our work, we define all random variables with respect to a common probability space (Ω,F,P). Recall that a
random variable θ is a measurable function Ω(cid:55)→Θ from the sample space Ω to a set Θ.
The probability measure P : F (cid:55)→ [0,1] assigns probabilities to events in the σ−algebra F. In particular, for any
event F ∈F, P(F) denotes the probability of the event. For events F,G ∈F for which P(G)>0, P(F|G) denotes
the probability of event F conditioned on event G.
For each realization z of a random variable Z, P(Z = z) is hence a function of z. We denote the value of this
functionevaluatedatZ byP(Z). Therefore,P(Z)isarandomvariable(sinceittakesrealizationsin[0,1]depending
on the value of Z). Likewise for realizations (y,z) of random variables Y,Z, P(Z =z|Y =y) is a function of (y,z)
and P(Z|Y) is a random variable which denotes the value of this function evaluated at (Y,Z).
IfrandomvariableZ :Ω(cid:55)→ℜK hasdensityp w.r.ttheLebesguemeasure,theconditionalprobabilityP(F|Z =z)
Z
is well-defined despite the fact that for all z, P(Z = z) = 0. If function f(z) = P(F|Z = z) and Y : Ω (cid:55)→ ℜK is
a random variable whose range is a subset of Z’s, then we use the ← symbol with P(F|Z ← Y) to denote f(Y).
Note that this is different from P(F|Z =Y) since this conditions on the event Z =Y while P(F|Z ←Y) indicates
a change of measure.
For any random variable θ, we use the notation P(θ ∈ ·) to denote the distribution of that random variable i.e.
P(θ ∈·)=P(θ−1(·)),whereθ−1(·)denotesthepre-imageof·(thepre-imagemustbe∈Fduetomeasurability). We
make a clear distinction between a random variable and its distribution in this way to provide accurate definitions
ofinformation-theoreticquantitieslaterinthiswork. Asmentionedintheintroduction,ourframeworkisBayesian,
and hence uses the tools of probability to model uncertainty about the unknown value of a variable of interest (for
instance θ). This involves treating θ as a random variable with a prior distribution P(θ ∈ ·) which encodes the
designer’s prior information about the value of this variable. The designer will often never directly observe θ,
but rather a stream of data which will contain information about θ. Machine Learning is therefore the process of
reducing uncertainty about θ in ways that are relevant for making better predictions about the future of this data
stream.
73.2 Data Generating Process
In machine learning, we are interested in discerning the relationship between input and output pairs (X,Y). Most
frameworks of machine learning focus on a static dataset of fixed size and hope to characterize the performance
of a predictive algorithm which leverages the information from the dataset for future predictions. However, any
practical system will continually have access to additional observations as it interacts with the environment. As a
result, it is prudent to consider a framework in which the data arrives in an online fashion and the objective is to
perform well across all time.
We consider a stochastic process which generates a sequence ((X ,Y ) : t ∈ Z ) of data pairs. For all t, we let
t t+1 +
H denote the history (X ,Y ,...,X ,Y ,X ) of experience. We assume that there exists an underlying latent
t 0 1 t−1 t t
variable θ such that (X ,X ,...)⊥θ and θ prescribes a conditional probability measure θ(·|H ) to the next label
0 1 t
Y . In the case of an iid data generating process, this conditional probability measure would only depend on
t+1
H via X . Furthermore, such a latent variable θ must exist under an infinite exchangability assumption on the
t t
sequence ((X ,Y ) : t ∈ Z ) by de Finetti’s Theorem. While we will first focus on this iid setting, we will also
t t+1 +
study learning setting in which the future data may be arbitrarily dependent on H even when conditioned on θ.
t
AsourframeworkisBayesian,werepresentouruncertaintyaboutθ bymodelingitasarandomvariablewithprior
distribution P(θ ∈·).
3.3 Error
Our framework focuses on a particular notion of error which facilitates analysis via Shannon-information theory.
For all t ∈ Z , our algorithm is tasked with providing a predictive distribution P of Y which may be derived
+ t t+1
from the history of data H which has already been observed. We denote such algorithm as π for which for all t,
t
P =π(H ). As aforementioned, an effective learning system ought to leverage data as it becomes available. As a
t t
result, for any time horizon T ∈Z , we are interested in quantifying the cumulative expected log-loss:
+
T−1
1 (cid:88)
L = E [−lnP (Y )].
T,π T π t t+1
t=0
As outlined in section 3.1, we take all random variables to be defined with respect to a common probability space.
As a result, the expectation E integrates over all random variables which we do not condition on. We use the
subscript π in E to specify that it is a function of π since for all t, π produces P . As Y is the random
π t t+1
variable which represents the next label that is generated by the underlying stochastic process, P (Y ) denotes
t t+1
the probability that our prediction P assigned to label Y .
t t+1
This loss function is commonly referred to in the literature as “log-loss” or “negative log-likelihood” and has
become a cornerstone of classification methods via neural networks. However, it is important to note that even for
an optimal algorithm, the minimum achievable log-loss is not 0. For instance, consider an omniscient algorithm
in the classification setting which produces for all t the prediction P =P(Y ∈·|θ,H ). Even this agent incurs a
t t+1 t
loss of:
T−1 T−1
1 (cid:88) 1 (cid:88)
E [−lnP(Y |θ,H )]= H(Y |θ,H )
T π t+1 t T t+1 t
t=0 t=0
H(Y |θ,X )
= 1:T 0:T−1 ,
T
whereourpointfollowsfromthefactthattheconditionalentropyofadiscreterandomvariableY isnon-negative.
1:T
As a result, we define the estimation error as:
H(Y |θ,X )
L =L − 1:T 0:T−1 .
T,π T,π T
Estimation error represents the error which is reducible via learning. As such, for a competent learning algorithm
tasked with a learnable task, L should decay to 0 as T goes to ∞.
T,π
3.4 Achievable Error
Since we are interested in characterizing the limits of what is possible via machine learning, a natural question
which arises is: For all T, which π minimizes L ? Since log-loss is a proper scoring rule, the optimal algorithm
T,π
8π is one such that for all t, P = P(Y ∈ ·|H ). This predictive distribution is often referred to as the Bayesian
t t+1 t
posterior and going forward we will denote it by Pˆ. The following result establishes optimality of Pˆ.
t t
Lemma 1. (Bayesian posterior is optimal) For all t∈Z ,
+
(cid:104) (cid:105)
E −lnPˆ(Y )|H a =.s. min E [−lnP (Y )|H ].
t t+1 t π t t+1 t
π
Proof.
E[−lnP t(Y t+1)|H t]a =.s.E(cid:34) −lnPˆ t(Y t+1)+lnP Pˆ t( (Y Yt+1) )(cid:12) (cid:12) (cid:12) (cid:12)H t(cid:35)
t t+1
(cid:104) (cid:105) (cid:16) (cid:17)
a =.s.E −lnPˆ(Y )|H +d Pˆ∥P .
t t+1 t KL t t
The result follows from the fact that KL-divergence is non-negative and the tower property.
This result is rather convenient since it prescribes that across all problem instances, the optimal prediction is Pˆ.
t
This is widely considered an advantage of the Bayesianism as opposed to the frequentism; the Bayesian need not
specify an ad-hoc algorithm to analyze/solve a problem. While in practice it may be intractable to compute Pˆ
t
exactly, for the purposes of characterizing achievable error, it provides immense utility. Going forward, we will
restrict our attention to the optimal achievable estimation error which we denote by:
L =
1 T (cid:88)−1 E(cid:104)
−lnPˆ(Y
)(cid:105)
−
H(Y 1:T|θ,X 0:T−1)
.
T T t t+1 T
t=0
The process of learning should result in L vanishing to 0 as T increase to ∞. While we have established that
T
the Bayesian posterior provides optimal predictions at each timestep, in its current form, it is unclear how to
characterizeL forproblemsinwhichtheposteriordistributiondoesnotexhibitananalyticform. Insection5,we
T
willestablishtheconnectionbetweenourlearningframeworkandinformationtheory. Thisconnectionwillfacilitate
the analysis of arbitrary learning problems, even those for which Pˆ cannot be expressed analytically.
t
In the following section, we overview requisite definitions and tools from information theory to establish the con-
nection between learning and information theory. For readers who are new to information theory, we provide the
followingsectionforcompleteness. Evenforreaderswhoarefamiliarwithinformationtheory, theremaybedetails
or results is the following section which may be worth revisiting.
Summary
• The algorithm’s observations through time t form a history H =(X ,Y ,...X ,Y ,X ).
t 0 1 t−1 t t
• The algorithm π produces for all t a predictive distribution P of Y given the history H .
t t+1 t
• For any horizon T ∈Z , we define estimation error of an algorithm π as
+
L =
1 T (cid:88)−1
E [−lnP (Y
)]−H(Y 1:T|θ,X 0:T−1)
.
T,π T π t t+1 T
t=0 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) irreducible error
log−loss
• The optimal algorithm assigns for all t,
P ←Pˆ =P(Y ∈·|H ).
t t t+1 t
9• We denote the estimation error incurred by the optimal algorithm by
L =
1 T (cid:88)−1
E
(cid:104)
−lnPˆ(Y
)(cid:105)
−
H(Y 1:T|θ,X 0:T−1)
.
T T π t t+1 T
t=0
104 Requisite Information Theory
In this section we outline definitions and results from information theory which we will refer to in later sections of
thismonograph. Foracomprehensiveoverviewofthetopic,wepointthereaderto[CoverandThomas,2012].
4.1 Entropy
In this text, H(X) denotes the entropy of a discrete random variable X :Ω(cid:55)→X. H is defined as follows:
(cid:88) 1
H(X)= P(X =x)ln .
P(X =x)
x∈X
Throughoutthismonograph,weusetheconventionthat0ln0=0. Whiletherearemanycolloquialinterpretations
of entropy which describe it as the expected “surprise” associated with outcomes of a random variable, we provide
a concrete motivation for entropy based on coding theory.
We begin our exposition of entropy with an introduction to coding theory. We first define a code for a random
variable.
Definition 2. A code C for random variable X : Ω (cid:55)→ X is a function which maps X (cid:55)→ {0,1}∗, where {0,1}∗
denotes the set of all binary strings.
When we send a text message to our friend, the characters that comprise of our message can be thought of as the
realizations of random variables. In many applications involving digital data transfer, a message (outcome of a
random variable) is encoded into a binary string which is passed through a communication channel, and decoded
atanendpoint. Sincethebinarystringarrivesinastream,itwouldbeconvenientifthemessagecouldbeuniquely
decoded as the data is arriving. A necessary and sufficient condition for this is online decodability is to design a
code C which is prefix-free i.e. no element in the image of C is a prefix of another element in the image of C. We
use C to denote the set of prefix-free codes for a random variable X
X
Since these codes are stored, transmitted, and decoded, the memory footprint becomes a significant design consid-
eration. Anaturalquestionwhicharisesis: “howdowedeviseoptimal prefix-freecodes?” Thenotionofoptimality
which gives rise to Shannon entropy is the following:
(cid:88) len(C(x))
argmin P(X =x)· ,
log (e)
C∈CX x∈X 2
wherelen(c)denotesthelengthofbinarystringc. Aprefix-freecodewhichminimizesthisobjectivewillon-average
requirethefewestnumberofbitstostore/transmitinformation. Anaiveprefix-freecodeisonewhichmapseachof
the |X| outcomes of X to a unique binary string of length ⌈log |X|⌉. While such a code would be reasonable if all
2
outcomes of X were equally likely, such a code would be highly suboptimal if some outcomes are much more/less
likelythanothers. Acompetentcodingschemeoughttomapmorelikelyoutcomestoshorter stringsandlesslikely
outcomes to longer strings.
110.14
0.12
0.1
0.08
0.06
0.04
0.02
0
e t a o i n s h r d l c umw f g y p b v k j x q z
Figure 1: The English alphabet consists of characters of varying frequency across common text corpuses. Notably,
thevowelsappearwithgreaterfrequency. Acodingschemeoughttomapthesemorefrequentlyappearingcharacters
to shorter strings.
The following result establishes the tight connection between the entropy of X and its optimal prefix-free code.
Theorem 3. (entropy characterizes optimal prefix-free code length) For all discrete random variables
X :Ω(cid:55)→X,
(cid:88) len(C(x)) 1
H(X) ≤ min P(X =x)· ≤ H(X)+ .
C∈CX
x∈X
log 2(e) log 2(e)
Thisresultdemonstratesthattheentropyofarandomvariabletightlycharacterizesthefundamentallimittowhich
itcanbelosslesslycompressed. Asaresult,theentropyreflectstheinherentcomplexityofarandomvariable. This
connectionisusefultokeepinmindasthereexistthefollowinganalogiesbetweenourBayesianandthefrequentist
frameworks:
frequentist(cid:55)→Bayesian
Θ(cid:55)→P(θ ∈·)
|Θ|(cid:55)→H(θ),
where θ :Ω(cid:55)→Θ and |Θ| denotes the cardinality of Θ.
4.2 Conditional Entropy
H(X|Y) denotes the conditional entropy of a discrete random variable X :Ω(cid:55)→X conditioned on another discrete
random variable Y :Ω(cid:55)→Y. The conditional entropy is defined as follows:
(cid:88) 1
H(X|Y)= P(X =x,Y =y)ln .
P(X =x|Y =y)
x∈X,y∈Y
Notethatunlike conditionalexpectation,conditionalentropyisanumber (andnotarandomvariable). Uponcloser
inspectionitisclearthatconditionalentropyis alsoalwaysnon-negativeandis 0onlywhenY fullydeterminesX.
On the other hand, when X ⊥Y, we have that H(X|Y)=H(X). We provide the following result which facilitates
mathematical manipulations involving the information-theoretic quantities outlined thus far.
Lemma 4. (chain rule of conditional entropy) For all discrete random variables X :Ω(cid:55)→X,Y :Ω(cid:55)→Y,
H(X,Y) = H(X)+H(Y|X) = H(Y)+H(X|Y).
12Proof.
(cid:88) 1 (cid:88) 1
H(X)+H(Y|X)= P(X =x)ln + P(X =x,Y =y)ln
P(X =x) P(Y =y|X =x)
x∈X x∈X,y∈Y
(cid:88) 1 (cid:88) P(X =x)
= P(X =x,Y =y)ln + P(X =x,Y =y)ln
P(X =x) P(X =x,Y =y)
x∈X,y∈Y x∈X,y∈Y
(cid:88) 1
= P(X =x,Y =y)ln
P(X =x,Y =y)
x∈X,y∈Y
=H(X,Y).
The second equality in the lemma statement follows from the same technique shown above.
If H(X,Y) denotes the average length of a prefix-free code for (X,Y) jointly, Lemma 4 establishes that H(X|Y)
reflects the average length of a prefix-free code for X after Y is already observed. Evidently if X ⊥ Y, then
observing Y does not provide any information which enables a shorter code for X (hence, H(X|Y) = H(X)).
a.s.
However, in the other extreme in which X = Y, observing Y means that X is also known. As a result, a trivial
code which maps every outcome of X to the null string ∅ will suffice (hence, H(X|Y)=0).
4.3 Mutual Information
I(X;Y) denotes the mutual information between two random variables X : Ω (cid:55)→ X and Y : Ω (cid:55)→ Y. Con-
cretely,
I(X;Y) = d (P((X,Y)∈·)∥P(X ∈·)⊗P(Y ∈·)),
KL
where P(X ∈ ·)⊗P(Y ∈ ·) denotes the outer product distribution. Note that KL divergence is a non-symmetric
functionwhichmapstwoprobabilitydistributions toℜ ∪{∞}. Fordiscreterandomvariables,wehavethefollowing
+
equivalence between mutual information and differences of (conditional) entropies:
I(X;Y) = H(X)−H(X|Y) = H(Y)−H(Y|X).
Notethatmutualinformationissymmetrici.e. I(X;Y)=I(Y;X)anditisalsoalwaysnon-negative(followsdirectly
as a consequence of Lemma 10). Intuitively, the mutual information I(X;Y) represents the amount of information
that X conveys about Y and vice versa. As such, if X fully determines Y, then I(X;Y) = H(X) = H(Y).
Meanwhile if X ⊥ Y, then I(X;Y) = 0 as the two random variables convey no information about each other. As
with conditional entropy, we provide the following result which facilitates mathematical analyses involving mutual
information:
Lemma 5. (chain rule of mutual information) For all random variables X :Ω(cid:55)→X,Y :Ω(cid:55)→Y,Z :Ω(cid:55)→Z,
I(Y;Z|X)+I(X;Z)=I(X,Y;Z).
Proof.
I(Y;Z|X)+I(X;Z)=H(Z|X)−H(Z|X,Y)+H(Z)−H(Z|X)
=H(Z)−H(Z|X,Y)
=I(X,Y;Z).
4.4 Differential Entropy
Whilewehavedefinedinformation-theoreticquantitiesfordiscreterandomvariables,outsideofmutualinformation
we have not broached a notion of information regarding continuous random variables. For a continuous random
variableX :Ω(cid:55)→X withdensityp (w.r.ttheLebesguemeasure),wedenotethedifferentialentropyofX by
X
(cid:90) 1
h(X) = p (x)ln dµ(x),
X p (x)
x∈X X
13Figure 2: This venn diagram illustrates the relationships between the introduced information-theoretic quantities.
whereµ(·)denotestheLebesguemeasure. Whiledifferentialentropyostensiblyresemblesdiscreteentropy, thetwo
are different in almost all regards. For instance, while the discrete entropy of a random variable is always non-
negative, the differential entropy can often be negative. Look no further than X =Uniform([0,1/2]). In this case,
h(X)=−ln(2). Furthermore, whilediscreteentropyisinvariantunderone-to-onetransformations, thedifferential
entropy is not. For instance, h(2X) = h(X)+ln2. A measure of information should not be negative nor should
it be dependent on units used. For these reasons, unlike discrete entropy, differential entropy is not a meaningful
informational quantity by itself. The correct extension of discrete entropy to continuous random variables exists
via rate-distortion theory, which we will present in the following section.
While differential entropy itself is not a meaningful measure of information, differences in (conditional) differential
entropies are still equal to mutual information. Concretely, for continuous random variables X,Y with finite
(conditional) differential entropies,
I(X;Y) = h(X)−h(X|Y) = h(Y)−h(Y|X).
4.5 Requisite Results from Information Theory
We now present an amalgamation of widely known and requisite results from information theory. Various proofs
throughout this monograph will refer to the results of this section.
Lemma 6. (log-sum inequality) For all n∈Z, if a ,...,a ≥0, b ,...,b ≥0, and a=(cid:80)n a ,b=(cid:80)n b ,
1 n 1 n i=1 i i=1 i
then
n
(cid:88) a lna i ≥alna .
i b b
i
i=1
Proof.
n n
(cid:88) a lna i =(cid:88) b · a i lna i
i b i b b
i i i
i=1 i=1
n
=b(cid:88)b
i ·
a
i
lna
i
b b b
i i
i=1
(cid:32) n (cid:33) (cid:32) n (cid:33)
( ≥a)
b
(cid:88)b
i ·
a
i ln
(cid:88)b
i ·
a
i
b b b b
i i
i=1 i=1
a a
=b· ln
b b
a
=aln ,
b
where (a) follows from Jensen’s inequality applied to the function xlnx.
Lemma 7. (conditioning reduces entropy) For all discrete random variables X :Ω(cid:55)→X,Y :Ω(cid:55)→Y,
H(X)≥H(X|Y).
14Proof.
(cid:88) 1
H(X)= P(X =x)ln
P(X =x)
x∈X
(cid:88) (cid:88) 1
= P(X =x,Y =y)ln
P(X =x)
x∈Xy∈Y
(a) (cid:88) (cid:88) P(Y =y)
≥ P(X =x,Y =y)ln
P(X =x,Y =y)
x∈Xy∈Y
(cid:88) (cid:88) 1
= P(X =x,Y =y)ln
P(X =x|Y =y)
x∈Xy∈Y
=H(X|Y),
where (a) follows from negating the log-sum inequality and setting a = P(X = x,Y = y),b = P(Y = y) and
i i
a=P(X =x),b=1.
Lemma 8. (conditioning reduces differential entropy) For all continuous random variables X :Ω(cid:55)→X,Y :
Ω(cid:55)→Y, if h(X),h(X|Y) exist and are both finite, then
h(X)≥h(X|Y).
Proof. The proof follows from the same reasoning as in Lemma 7 by constructing a sequence of partitions of X,Y
and taking the associated limits.
Lemma 9. (equivalence of mutual information and KL-divergence) For all random variables X : Ω (cid:55)→
X,Y :Ω(cid:55)→Y,
I(X;Y)=E[d (P(Y ∈·|X) ∥ P(Y ∈·))].
KL
Proof. We prove the result for discrete random variables X,Y. With appropriate technical assumptions, the result
can also be extended to continual random variables which exhibit density functions.
(cid:88) (cid:88) P(X =x,Y =y)
I(X;Y)= P(X =x,Y =y)ln
P(X =x)·P(Y =y)
x∈Xy∈Y
(cid:88) (cid:88) P(Y =y|X =x)
= P(X =x) P(Y =y|X =x)ln
P(Y =y)
x∈X y∈Y
=E[d (P(Y ∈·|X) ∥ P(Y ∈·))].
KL
Lemma 10. (non-negativity of KL-divergence) For all probability distributions P(·) : F (cid:55)→ [0,1],Q(·)F (cid:55)→
[0,1],
d (P(·) ∥ Q(·))≥0.
KL
Furthermore,
d ((P(·) ∥ Q(·))=0 ⇐⇒ for all ν ∈F,(P(ν)>0 =⇒ P(ν)=Q(ν)).
KL
15Proof.
(cid:88) P(ν)
d (P(·) ∥ Q(·))= P(ν)ln
KL Q(ν)
ν∈F
(cid:88) Q(ν)
= −P(ν)ln
P(ν)
ν∈F
(a) (cid:88) Q(ν)
≥ −ln P(ν)·
P(ν)
ν∈F
(cid:88)
=−ln Q(ν)
ν∈F
=0.
where(a)followsfromJensen’sinequality. Toprovethesecondresult,considerthecaseinwhichJensen’sinequality
(cid:80)
holds with equality. This occurs iff Q(ν)/P(ν)= P(ν)·Q(ν)/P(ν)=1. This occurs only when Q(ν)=P(ν)
ν∈F
for all ν ∈F for which P(ν)>0.
Lemma 11. (maximum differential entropy) For all density functions q : ℜd (cid:55)→ ℜ , for all i,j ∈ [d], let
+
(cid:82)
Σ = x x q(x)dx. If X ∼N(0,Σ), then
i,j x∈ℜd i j
(cid:90) 1
h(X)≥ q(x)ln dx.
q(x)
x∈ℜd
Proof. Let p denote the probability density function of X.
(cid:90) (cid:18) 1 x⊤Σ−1x(cid:19)
h(X)= −p(x) ln(2πe|Σ|)− dx
2 2
x∈ℜd
1 (cid:90) x⊤Σ−1x
=− ln(2πe|Σ|)+ p(x)· dx
2 2
x∈ℜd
(a) 1 (cid:90) x⊤Σ−1x
= − ln(2πe|Σ|)+ q(x)· dx
2 2
x∈ℜd
(cid:90) 1
= q(x)ln dx
p(x)
x∈ℜd
(cid:90) 1
= q(x)ln dx+d (q(·) ∥ p(·))
q(x) KL
x∈ℜd
(b)(cid:90) 1
≥ q(x)ln dx
q(x)
x∈ℜd
where (a) follows from the equivalence of covariances assumption and (b) follows from Lemma 10.
Lemma 12. (data processing inequality) Let X,Y,Z be random variables for which X ⊥Z|Y, then
I(X;Z)≤I(Y;Z), I(X;Z)≤I(X;Y).
I(X;Z)≤I(X,Y;Z)
=I(Y;Z)+I(X;Z|Y)
( =a)I(Y;Z),
where (a) follows from the independence assumption. Similarly,
I(X;Z)≤I(X;Y,Z)
=I(X;Y)+I(X;Z|Y)
=I(X;Y).
16Summary
• The entropy of a discrete random variable X :Ω(cid:55)→X is
(cid:88) 1
H(X)= P(X =x)ln .
P(X =x)
x∈X
• The conditional entropy of a discrete random variable X : Ω (cid:55)→ X conditioned on another discrete
random variable Y :Ω(cid:55)→Y is
(cid:88) 1
H(X|Y)= P(X =x,Y =y)ln .
P(X =x|Y =y)
x∈X,y∈Y
• The mutual information between two random variables X :Ω(cid:55)→X and Y :Ω(cid:55)→Y is
I(X;Y) = d (P((X,Y)∈·)∥P(X ∈·)⊗P(Y ∈·)),
KL
where P(X ∈·)⊗P(Y ∈·) denotes the outer product distribution. For discrete random variables,
I(X;Y) = H(X)−H(X|Y) = H(Y)−H(Y|X).
• (chain rule of mutual information) For all random variables X :Ω(cid:55)→X,Y :Ω(cid:55)→Y,Z :Ω(cid:55)→Z,
I(Y;Z|X)+I(X;Z)=I(X,Y;Z).
• Thedifferentialentropy ofacontinuousrandomvariableX :Ω(cid:55)→X withdensityp (w.r.ttheLebesgue
X
measure µ(·)) is
(cid:90) 1
h(X) = p (x)ln dµ(x).
X p (x)
x∈X X
• Differential entropy can be negative and is unit-dependent.
• For continuous random variables X :Ω(cid:55)→X,Y :Ω(cid:55)→Y,
I(X;Y) = h(X)−h(X|Y) = h(Y)−h(Y|X).
175 Connecting Learning and Information Theory
In this section, we will leverage the requisite information-theoretic results of section 4 to derive general upper and
lower bounds for the estimation error of the Bayesian posterior (L ). The results of this section will facilitate the
T
analysis of concrete problem instances in the following sections.
5.1 Error is Information
We now establish the elegant connection between L and mutual information.
T
Theorem 13. (optimal error equals total information) For all T ∈Z ,
+
I(H ;θ)
L = T .
T T
Proof.
L =
1 T (cid:88)−1 E(cid:104)
−lnPˆ(Y
)(cid:105)
−
H(Y 1:T|θ,X 0:T−1)
T T t t+1 T
t=0
=
1 T (cid:88)−1 E(cid:34) lnP(Y t+1|θ,H t)(cid:35)
T Pˆ(Y )
t=0 t t+1
T−1
1 (cid:88)
= E[d (P(Y ∈·|θ,H ) ∥ P(Y ∈·|H ))]
T KL t+1 t t+1 t
t=0
T−1
1 (cid:88)
= I(Y ;θ|H )
T t+1 t
t=0
T−1
1 (cid:88)
= I(Y ;θ|H )+I(X ;θ|H ,Y )
T t+1 t t t−1 t
t=0
I(H ;θ)
= T .
T
The estimation error incurred by an optimal algorithm over horizon T is exactly equal to the total information
acquired about θ from the observing the data H . Every nat of information about θ can only be acquired via
T
incurring error on a prediction which relied on that information. Evidently, an optimal algorithm never makes
the same mistake twice, hence resulting in the equality between total loss incurred and total information acquired.
Results of a similar flavor appear in the global Fano’s method from the frequentist analysis of minimax lower
bounds. However, the following connections to rate-distortion theory are novel.
A natural question which may arise when inspecting Theorem 13 is: “Does L decay to 0 in T and if so, at what
T
rate?”. Ostensibly, the numerator I(θ;H ) appears to be growing in T, so it is not immediately obvious that even
T
an optimal learner will experience vanishing error. A simple instance to initially consider is θ which is a discrete
random variable. In such an instance, we can always provide the upper bound:
H(θ)
L ≤ .
T T
Therefore, for any θ for which H(θ)<∞, we have that L =O(1/T).
T
However, what about a more realistic scenario in which θ is a continuous random variable? A naive idea would
be to simply supplant the discrete entropy H(θ) with the differential entropy h(θ). However, while differences
in conditional differential entropy equal mutual information (just as with discrete entropy), differential entropy
does not upper bound mutual information. This is because differential entropy can be negative (as discussed in
section 4.4). The appropriate extension of discrete entropy to continuous random variables can be establish via
rate-distortion theory.
185.2 Characterizing Error via Rate-Distortion Theory
We begin with the definition of the rate-distortion function.
Definition 14. (rate-distortion function) Let ϵ ≥ 0,θ : Ω (cid:55)→ Θ be a random variable, and ρ a distortion
function which maps θ and another random variable θ˜ to ℜ . The rate-distortion function evaluated for random
+
variable θ at tolerance ϵ is defined as:
inf I(θ;θ˜),
θ˜∈Θ˜ ϵ(cid:124) (cid:123)(cid:122) (cid:125)
rate
where
 

(cid:104) (cid:105)

Θ˜ = θ˜:E ρ(θ,θ˜) ≤ϵ .
ϵ

(cid:124) (cid:123)(cid:122) (cid:125)

distortion
Intuitively, one can think of θ˜as the result of passing θ through a noisy channel, resulting in a lossy compression.
TheobjectiveI(θ;θ˜),referredtoastherate,characterizesthenumberofnatsthatθ˜retainsaboutθ. meanwhile,the
distortionfunctionρcharacterizeshowlossythecompressionis. Therate-distortionfunctionreturnstheminimum
number of nats necessary to achieve distortion at most ϵ. In many ways this generalizes the concept of an ϵ-cover
in frequentist statistics. For the application of rate-distortion theory to the analysis of estimation error in machine
learning, we consider the following distortion function:
(cid:104) (cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
E ρ (θ,θ˜) =E d P(Y ∈·|θ,H ) ∥ P(Y ∈·|θ˜,H )
t KL t+1 t t+1 t
=I(Y ;θ|θ˜,H ),
t+1 t
where the second equality follows from the fact that θ˜ ⊥ Y |(θ,H ). This restriction ensure that θ˜ does not
t+1 t
contain exogenous information about Y such as aleatoric noise which cannot be determined from (θ,H ). We
t+1 t
use the notation H (θ) to denote the following rate-distortion function:
ϵ,T
H (θ)= inf I(θ,θ˜),
ϵ,T
θ˜∈Θ˜
ϵ,T
where
(cid:40) (cid:41)
I(H ;θ|θ˜)
Θ˜ = θ˜:θ˜⊥H |θ; T ≤ϵ .
ϵ,T T T
The following result upper and lower bounds the optimal estimation error in terms of the rate-distortion func-
tion:
Theorem 15. (rate-distortion estimation error bounds) For all T ∈Z ,
+
(cid:26)H (θ) (cid:27) H (θ)
sup min ϵ,T ,ϵ ≤ L ≤ inf ϵ,T +ϵ.
ϵ≥0 T T ϵ≥0 T
19Proof. We begin with a proof of the upper bound.
I(H ;θ)
L = T
T T
T−1
1 (cid:88)
= I(Y ;θ|H )
T t+1 t
t=0
T−1
= 1 (cid:88) I(Y ;θ,θ˜|H )
T t+1 t
t=0
T−1
= 1 (cid:88) I(Y ;θ˜|H )+I(Y ;θ|θ˜,H )
T t+1 t t+1 t
t=0
=
I(H T;θ˜)
+
1 T (cid:88)−1
I(Y ;θ|θ˜,H )
T T t+1 t
t=0
( ≤a)
inf inf
I(θ;θ˜)
+
1 T (cid:88)−1
I(Y ;θ|θ˜,H )
ϵ≥0θ˜∈Θ˜
ϵ,T
T T
t=0
t+1 t
I(θ;θ˜) I(H ;θ|θ˜)
= inf inf + T
ϵ≥0θ˜∈Θ˜
ϵ,T
T T
H (θ)
≤ inf ϵ,T +ϵ
ϵ≥0 T
where (a) follows from the data processing inequality applied to the markov chain θ˜⊥H |θ.
T
We now proceed with the lower bound. Suppose that I(H ;θ)<H (θ). Let θ˜=H˜ ∈/ Θ˜ where H˜ is another
T ϵ,T T ϵ,T T
history sampled in the same manner as H .
T
T−1
(cid:88)
I(H ;θ)= I(Y ;θ|H )
T t+1 t
t=0
T−1
( ≥a) (cid:88) I(Y ;θ|H˜ ,H )
t+1 t t
t=0
T−1
= (cid:88) I(Y ;θ|θ˜,H )
t+1 t
t=0
(b)
≥ ϵT,
where (a) follows from the fact that conditioning reduces entropy and that Y ⊥H˜ |(θ,H ) and (b) follows from
t+1 t t
the fact that θ˜∈/ Θ˜ . Therefore, for all ϵ≥0, I(H ;θ)≥min{H ,ϵT}. The result follows.
ϵ,T T ϵ,T
Theorem 15 establishes the tight relation between estimation error and the rate-distortion function. The result
is very general and facilitates the analysis of concrete problem instances in supervised learning. In the following
section, we will study 3 concrete instances of increasing complexity to demonstrate how Theorem 15 facilitates
analysis.
We also note the qualitative similarity between Theorem 15 and classical results from PAC learning which all
characterize estimation error as a fraction involving a complexity function of the hypothesis space and the dataset
size (VC-dimension, Rademacher complexity, log-covering number). In our framework, the rate-distortion function
serves as the “complexity” function which characterizes the difficulty of learning θ for the purposes of prediction.
In the following section, we will use this general result to derive concrete error bounds for a suite of problems
involving data pairs which are iid when conditioned on θ.
20Summary
• (optimal estimation error equals total information) For horizon T ∈Z , the estimation error of
+
the optimal algorithm is denoted as L and is
T
I(θ;H )
L = T .
T T
• For all T ∈Z ,
+
H(θ)
L ≤ .
T T
Therefore, for a discrete random variable θ with finite entropy, L =O(1/T).
T
• The extension of this result to continuous random variables can be made via rate-distortion theory.
• (rate-distortion function) Let ϵ ≥ 0,θ : Ω (cid:55)→ Θ be a random variable, and ρ a distortion function
which maps θ and another random variable θ˜to ℜ . The rate-distortion function evaluated for random
+
variable θ at tolerance ϵ is defined as:
inf I(θ;θ˜),
θ˜∈Θ˜
ϵ
where
(cid:110) (cid:104) (cid:105) (cid:111)
Θ˜ = θ˜:E ρ(θ,θ˜) ≤ϵ .
ϵ
• We use the notation H (θ) to denote the following rate-distortion function:
ϵ,T
H (θ)= inf I(θ,θ˜),
ϵ,T
θ˜∈Θ˜
ϵ,T
where
(cid:40) (cid:41)
I(H ;θ|θ˜)
Θ˜ = θ˜:θ˜⊥H |θ; T ≤ϵ .
ϵ,T T T
• (rate-distortion estimation error bounds) For all T ∈Z ,
+
(cid:26)H (θ) (cid:27) H (θ)
sup min ϵ,T ,ϵ ≤ L ≤ inf ϵ,T +ϵ.
ϵ≥0 T T ϵ≥0 T
216 Learning from iid Data
Inthissection, werestrictourattentiontotheanalysisoflearningunderindependentlyandidenticallydistributed
(iid) data. Concretely, we assume that the random process representing the inputs: (X ,X ,...) is iid. Each input
0 1
X is associated with a label Y and we assume infinite exchangeability of the sequence ((X ,Y ),(X ,Y ),...).
t t+1 0 1 1 1
By de Finetti’s theorem, there exists a latent random variable, which we denote by θ, for which conditioned on θ,
the above sequence is iid. We further make the standard assumption that the sequence of inputs (X ,X ,...) is
0 1
independent of θ. As a result, θ is a random variable which represents for all t the conditional probability measure
θ(·|X ) of Y . The process of learning involves the reduction of uncertainty about θ in ways which are relevant
t t+1
for future predictions.
In the following section, we provide several results which follow as a consequence of the above iid assumption.
We dedicate the remaining sections to studying 4 concrete problem instances of increasing complexity: 1) linear
regression, 2) logistic regression, 3) deep neural networks, 4) non-parametric learning. We hope that this suite of
examples provide the reader with enough intuition and tools to analyze their own problems of interest.
6.1 Theoretical Results Tailored for iid Data
We begin with several intuitive theoretical results which follow as a result of the iid assumptions on the data. The
first result establishes monotonicity of estimation error:
Lemma 16. (monotonicity of per-timestep estimation error) If ((X ,Y : t ∈ Z ) is an iid stochastic
t t+1 +
process when conditioned on θ, then for all t∈Z ,
+
I(Y ;θ|H ) ≤ I(Y ;θ|H ).
t+2 t+1 t+1 t
Proof. We have
I(θ;Y |H )=h(Y |H )−h(Y |θ,H )
t+2 t+1 t+2 t+1 t+2 t+1
(a)
= h(Y |H )−h(Y |θ,H ,Y ,X )
t+2 t+1 t+2 t−1 t t+1
(b)
≤ h(Y |H ,Y ,X )−h(Y |θ,H ,Y ,X )
t+2 t−1 t t+1 t+2 t−1 t t+1
=I(Y ;θ|H ,Y ,X )
t+2 t−1 t t+1
( =c)I(Y
;θ|H ,Y ,X )
t+1 t−1 t t
=I(Y ;θ|H ),
t+1 t
where (a) follows since Y ⊥(X ,Y )|(θ,X ), (b) follows from the fact that conditioning reduces differential
t+2 t t+1 t+1
entropy, and (c) follows from the fact that (X ,Y ) and (X ,Y ) are identically distributed conditioned on
t t+1 t+1 t+2
(H ,Y ).
t−1 t
The above result establishes that when the data generating process is iid conditioned on θ, the optimal per-
timestep estimation error is monotonically non-increasing. This is intuitive, if the data is iid, the future sequence
is exchangeable and hence, conditioning on additional information (H as opposed to H ) should only improve
t+1 t
our ability to make predictions. A corollary of this result is that for iid data, we can upper bound distortion via
an expression which is much simpler to analyze.
Corollary 17. (distortion upper bound) If ((X ,Y : t ∈ Z ) is an iid stochastic process when conditioned
t t+1 +
on θ and for all t∈Z , Y ⊥θ˜|(θ,X ), then for all T ∈Z ,
+ t+1 t ++
I(H ;θ|θ˜)
T ≤ I(Y ;θ|θ˜,X ).
T 1 0
22Proof.
I(H T;θ|θ˜)
=
1 T (cid:88)−1
I(X ,Y ;θ|θ˜,H ,Y )
T T t t+1 t−1 t
t=0
T−1
( =a) 1 (cid:88) I(Y ;θ|θ˜,H )
T t+1 t
t=0
(b)
≤ I(Y ;θ|θ˜,X )
1 0
where (a) follows from the assumption that X ⊥ θ and (b) follows from the same proof technique as in Lemma
t
16.
Ingeneral,theexpressionI(H ;θ|θ˜)ismuchmorecumbersometodealwithincomparisontoI(Y ;θ|θ˜,X ). Corol-
T 1 0
lary 17 established a suitable relationship between the two quantities. However, to facilitate the analysis of rate-
distortion lower bounds, it is fruitful to consider analysis via a modified rate-distortion function tailored for iid
data generating processes. For all ϵ≥0, we let
H (θ)= inf I(θ;θ˜),
ϵ
θ˜∈Θ˜
ϵ
where
Θ˜ ={θ˜:θ˜⊥H |θ, I(Y ;θ|θ˜,X )≤ϵ}.
ϵ ∞ 1 0
Note that in contrast to the general rate-distortion formulation of section 5, this modified rate-distortion does not
depend on the time horizon T. We now provide an estimation error lower bound in terms of H (θ) which applies
ϵ
for learning from iid data.
Lemma 18. (estimation error lower bound for iid data) If ((X ,Y : t ∈ Z ) is an iid stochastic process
t t+1 +
when conditioned on θ, then for all T ∈Z ,
++
(cid:26)H
(θ)
(cid:27)
sup min ϵ ,ϵ ≤ L .
T T
ϵ≥0
Proof. Fix T ∈ Z . Let θ˜ = (H˜ ,Y˜ ) be independent from but distributed identically with (H ,Y )
+ T−2 T−1 T−2 T−1
when conditioned on θ.
Fix ϵ≥0. If L <H (θ)/T then
T ϵ
L <H (θ)/T =⇒ I(H ;θ)<H (θ) =⇒ I(θ˜;θ)<H (θ).
T ϵ T ϵ ϵ
Since the rate of θ˜is lower than the rate-distortion function H (θ), θ˜∈/ Θ˜ . As a result,
ϵ ϵ
L
( =a) I(H T;θ)
T T
T−1
( =b) 1 (cid:88) I(Y ;θ|H )
T t+1 t
t=0
(c)
≥ I(Y ;θ|H )
T T−1
=I(Y ;θ|θ˜,X )
T T−1
(d)
> ϵ,
where (a) follows from Lemma 13, (b) follows from the chain rule of mutual information, (c) follows from Lemma
16, and (d) follows from the fact that θ˜∈/ Θ˜ . Therefore,
ϵ
L ≥min{H (θ)/T, ϵ}.
T ϵ
Since this holds for any ϵ≥0, the result follows.
In the following 4 sections we will apply our general results to concrete problem instances. We begin with linear
regression.
236.2 Linear Regression
Figure 3: We depict our linear regression data generating process above. It consists of an input vector X of
dimension d, an unknown weight vector θ of dimension d, and a final output Y which is the sum of θ⊤X and
independent Gaussian noise Z.
6.2.1 Data Generating Process
In linear regression, the variable that we are interested in estimating is a random vector θ ∈ℜd. As our analytical
toolsaredevelopedinaBayesianframework,weassumeaknownpriordistributionP(θ ∈·)tomodelouruncertainty
over the value of θ. For simplicity and concreteness, in this example, we assume that P(θ ∈ ·) = N(0,I /d). For
d
all t∈Z , inputs and outputs are generated according to a random vector X i ∼id N(0,I ) and
+ t d
Y =θ⊤X +W ,
t+1 t t+1
where W i ∼id N(0,σ2) for known variance σ2. We note that the results and techniques developed in this section
t
certainly extend to input and prior distributions which are not Gaussian with slight modifications. We study the
Gaussian case as it minimizes ancillary clutter without compromising generality. In the following section, we will
establish a series of smaller results which will allow us to streamline our error analysis using the tools established
in section 5.
6.2.2 Theoretical Building Blocks
In this section we will establish 3 relatively simple results which will enable us to directly apply Theorem 15 and
arrive at estimation error bounds for linear regression. As such, the results will all involve characterizing the
rate-distortion function for this data generating process. We begin our analysis with a result which establishes the
threshold ϵ at which the rate-distortion function is trivially 0.
Lemma 19. (linear regression 0 rate-distortion threshold) For all d ∈ Z ,σ2 ∈ ℜ , if for all t ∈ Z ,
++ ++ +
(X ,Y ) are generated according to the linear regression process and ϵ≥ 1ln(1+1/σ2), then for all T ∈Z ,
t t+1 2 ++
H (θ)=0.
ϵ,T
24Proof. Let θ˜=∅. Then,
I(H ;θ|θ˜) (a)
T ≤ I(Y ;θ|θ˜,X )
T 1 0
=h(Y |θ˜,X )−h(Y |θ,θ˜,X )
1 0 1 0
=h(Y |X )−h(W)
1 0
(cid:20) (cid:21)
( ≤b) E 1 ln(cid:0) 2πe(cid:0) σ2+Var[θ⊤X|X](cid:1)(cid:1) − 1 ln(cid:0) 2πeσ2(cid:1)
2 2
( =c)E(cid:20) 1 ln(cid:18) 2πe(cid:18) σ2+ ∥X∥2 2(cid:19)(cid:19)(cid:21) − 1 ln(cid:0) 2πeσ2(cid:1)
2 d 2
( ≤d) 1 ln(cid:32) 2πe(cid:32) σ2+ E(cid:2) ∥X∥2 2(cid:3)(cid:33)(cid:33) − 1 ln(cid:0) 2πeσ2(cid:1)
2 d 2
(cid:18) (cid:19)
1 1
= ln 1+
2 σ2
≤ϵ,
where (a) follows from Corollary 17, (b) follows from Lemma 11, (c) follows from the fact that θ has covariance
matrix I /d, and (d) follows from Jensen’s inequality. The above establishes that θ˜∈ Θ˜ for all T. The result
d ϵ,T
follows from the fact that I(θ;θ˜)=0.
Intuitively, there should be a threshold of ϵ for which values greater than ϵ result in a rate-distortion value of
0. Evidently, a 0-bit quantization that solely relies on the prior distribution will result in a suitable distortion
valuewhenthethresholdϵislargeenough. Thisresultcharacterizestheedge-caseconditionfortherate-distortion
function. The following result will provide an upper bound on the rate-distortion function for the more interesting
scenario in which the threshold ϵ<1/2ln(1+1/σ2).
Lemma 20. (linear regression rate-distortion upper bound) For all d ∈ Z , σ2 ∈ ℜ , and ϵ ∈ ℜ , if
++ ++ ++
for all t∈Z , (X ,Y ) are generated according to the linear regression process, then for all T,
+ t t+1
(cid:18) (cid:18) (cid:19)(cid:19)
d 1
H (θ)≤ ln .
ϵ,T 2 σ2(e2ϵ−1)
+
Proof. Fix σ2 ∈ℜ . Let θ˜=θ+V, where V ∼N(0,(δ2/d)I ) for δ2 = σ2(e2ϵ−1) and V ⊥θ. Note that δ2 ≥0
++ d 1−σ2(e2ϵ−1)
25for all 0<ϵ< 1ln(cid:0) 1+ 1 (cid:1) . We begin by showing that θ˜∈Θ˜ for all T.
2 σ2 ϵ,T
I(H ;θ|θ˜)
T ≤I(Y ;θ|θ˜,X )
T 1 0
=h(Y |θ˜,X )−h(Y |θ,θ˜,X )
1 0 1 0
=h(W +θ⊤X|θ˜,X )−h(W )
1 0 1
=h(cid:32) W 1+θ⊤X 0− 1θ˜⊤ +X δ0 2(cid:12) (cid:12) (cid:12) (cid:12)θ˜,X 0(cid:33) −h(W 1)
(cid:32) (cid:18) δ2 1 (cid:19)⊤ (cid:12) (cid:33)
=h W + θ+ V X (cid:12)θ˜,X −h(W )
1 1+δ2 1+δ2 0(cid:12) 0 1
(cid:32) (cid:18) δ2 1 (cid:19)⊤ (cid:12) (cid:33)
≤h W + θ+ V X (cid:12)X −h(W )
1 1+δ2 1+δ2 0(cid:12) 0 1
( ≤a) E(cid:20) 1 ln(cid:18) 2πe(cid:18) σ2+(cid:18) δ4
+
δ2 (cid:19)
∥X
∥2(cid:19)(cid:19)(cid:21)
−
1 ln(cid:0) 2πeσ2(cid:1)
2 d(1+δ2)2 d(1+δ2)2 0 2 2
(cid:20) 1 (cid:18) δ2∥X ∥2 (cid:19)(cid:21)
=E ln 1+ 0 2
2 d(1+δ2)σ2
(b) 1 (cid:18) δ2 (cid:19)
≤ ln 1+
2 (1+δ2)σ2
=
1 ln(cid:0) e2ϵ(cid:1)
2
=ϵ,
where (a) follows from Lemma 11 and (b) follows from Jensen’s inequality. Next, we upper bound the rate of θ˜:
I(θ;θ˜)=h(θ˜)−h(θ˜|θ)
=h(θ˜)−h(V)
(a) d
(cid:18) (cid:18) δ2+1(cid:19)(cid:19)
d
(cid:18) (cid:18) δ2(cid:19)(cid:19)
≤ ln 2πe − ln 2πe
2 d 2 d
(cid:18) (cid:19)
d 1
= ln 1+
2 δ2
(cid:18) (cid:19)
d 1
= ln ,
2 σ2(e2ϵ−1)
where (a) follows from Lemma 11. The result follows from Lemma 19 and the fact that θ˜∈Θ˜ for all T ∈Z
ϵ,T ++
and ϵ<1/2ln(1+1/σ2).
Note that above, we never explicitly relied on the assumption that P(θ ∈ ·) = N(0,I /d). We simply needed the
d
fact that the elements of θ are independent with variances which sum to 1 (for inequality (a) above). To establish
a lower bound on the rate-distortion function in the linear regression setting, we rely more on the Gaussian prior
assumption. Concretedetailsofthemostgeneralassumptionsrequiredtoarriveatsuchlowerboundscanbefound
in Appendix A.
Lemma 21. (linear regression rate-distortion lower bound) For all d ∈ Z s.t. d > 2, σ2 ∈ ℜ , and
++ ++
ϵ∈ℜ , if for all t∈Z , (X ,Y ) are generated according to the linear regression process, then
++ + t t+1
(cid:32) (cid:32) (cid:33)(cid:33)
d 1
H (θ) ≥ ln .
ϵ 2 (8+ d σ2)ϵ
d−2 +
26Proof. Fix σ2 ∈ℜ , ϵ∈Z T ∈Z , and θ˜∈Θ˜ . Then,
++ + ++ ϵ,T
(a)
ϵ ≥ I(Y;θ|θ˜,X)
(b) (cid:20) ∥X∥2 (cid:21) (cid:104) (cid:105)
≥ E 2 E ∥θ−E[θ|θ˜]∥2
2(4∥X∥2+dσ2) 2
2
 
1 (cid:104) (cid:105)
=E

2(4∥X∥2+dσ2)E ∥θ−E[θ|θ˜]∥2
2
2
∥X∥2
2
(cid:34) (cid:35)
1 (cid:104) (cid:105)
=E E ∥θ−E[θ|θ˜]∥2
8+ dσ2 2
∥X∥2
2
(c) 1 (cid:104) (cid:105)
≥ E ∥θ−E[θ|θ˜]∥2
(cid:104) (cid:105) 2
E 8+ dσ2
∥X∥2
2
( =d) 1 E(cid:104) ∥θ−E[θ|θ˜]∥2(cid:105)
8+ dσ2 2
d−2
1 (cid:104) (cid:105)
≥ E ∥θ−E[θ|θ˜]∥2
8+ d σ2 2
d−2
where (a) follows from the fact that θ˜∈ Θ˜ , (b) follows from Lemma 63, (c) follows from Jensen’s inequality, and
ϵ
(d) follows from the fact that E[1/χ2(d)]=1/(d−2).
Since the above condition is an implication that holds for arbitrary θ˜∈ Θ˜ , minimizing the rate I(θ;θ˜) over the
ϵ
set of proxies that satisfy E[∥θ−E[θ|θ˜]∥2]≤(8+ d σ2)ϵ will provide a lower bound. However, this is simply the
2 d−2
rate-distortion problem for a multivariate source under squared error distortion. For this problem there exists a
well known lower bound (Theorem 10.3.3 of [Cover and Thomas, 2006]). The lower bound follows as a result.
6.2.3 Main Result
The rate-distortion bounds which we established in the previous section allow us to directly apply Theorem 15 to
arrive at a bound for estimation error.
Theorem 22. (linear regression estimation error bounds) For all d ∈ Z , σ2 ≥ 0, if for all t, (X ,Y )
++ t t+1
are generated according to the linear regression processes, then for all T,
(cid:32) (cid:33) (cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
d 2T d T 1 d
W ≤ L ≤ ln + ln 1+ ,
2T d(8+ d σ2) T 2T σ2d 2T T
d−2 +
where W is the Lambert W function.
Proof.
(a) H (θ)
L ≤ inf ϵ,T +ϵ
T ϵ≥0 T
(cid:16) (cid:16) (cid:17)(cid:17)
dln 1
(b) σ2(e2ϵ−1)
≤ inf + +ϵ
ϵ≥0 2T
(cid:18) (cid:18) (cid:19)(cid:19)
dln 1
(c)
σ2(cid:16) (1+d)T−1(cid:17)
1
(cid:18) d(cid:19)
≤ T + + ln 1+
2T 2T T
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
d T 1 d
≤ ln + ln 1+ ,
2T σ2d 2T T
+
27where (a) follows from Theorem 15, (b) follows from Lemma 20, and (c) follows from setting ϵ= 1 ln(cid:0) 1+ d(cid:1) .
2T T
(cid:26)H
(θ)
(cid:27)
L ≥sup min ϵ ,ϵ
T T
ϵ≥0
(cid:40)(cid:32) (cid:32) (cid:33)(cid:33) (cid:41)
d 1
≥sup min ln , ϵ
2T (8+ d σ2)ϵ
ϵ≥0 d−2 +
(cid:32) (cid:32) (cid:33)(cid:33)
d 1
=ϵ s.t. ln =ϵ
2T (8+ d σ2)ϵ
d−2 +
−W(cid:32) 8+d−d 2σ2(cid:33)
2
(a) e
=
2T
d(8+ d σ2)
d−2
(b) e−ln(8+ d−d 2σ2)
≥
2T
d(8+ d σ2)
d−2
d
= ,
2T
wherein(a),W denotestheLambertW functionand(b)followsfromthefactthatforallx≥e,W(x)≤ln(2x).
Notably, this bound is consistent with classical results in statistics which dictate that the estimation error grows
linearly in the problem dimension d and decays linearly in the number of samples observed T. In the following
section, we will observe that qualitatively similar results also hold for logistic regression.
6.3 Logistic Regression
Figure 4: We depict our logistic regression data generating process above. It consists of an input vector X of
dimension d, an unknown weight vector θ of dimension d, and a final binary output Y ∈ {0,1}. Y is sampled
according to probabilities generated by the sigmoid function applied to θ⊤X.
6.3.1 Data Generating Process
We next study logistic regression to demonstrate the application of our tools to classification. Just as in linear
regression,inlogisticregression,thevariablethatweareinterestedinestimatingisarandomvectorθ ∈ℜd. Again
for simplicity and concreteness, we assume that P(θ ∈ ·) = N(0,I /d). For all t ∈ Z , inputs and outputs are
d +
iid
generated according to a random vector X ∼ N(0,I ) and
t d
(cid:40)
1 w.p. 1
Y = 1+e−θ⊤Xt .
t+1
0 otherwise
286.3.2 Theoretical Building Blocks
We begin with the following result which upper bounds the binary KL-divergence of a sigmoidal output via the
squared difference between the logits.
Lemma 23. (squared error upper bounds binary KL-divergence) For all x,y ∈ℜ,
1 1+e−y 1 1+ey (x−y)2
ln + ln ≤
1+e−x 1+e−x 1+ex 1+ex 8
Proof.
This upper bound facilitates the following upper bound on the rate-distortion function.
Lemma 24. (logistic regression rate-distortion upper bound) For all d ∈ Z and ϵ ∈ ℜ , if for all
++ ++
t∈Z ,(X ,Y ) are generated according to the logistic regression process, then for all T,
+ t t+1
(cid:18) (cid:19)
d 1
H ≤ ln 1+ .
ϵ,T 2 8ϵ
Proof. Let θ˜=θ+Z where Z ⊥θ and Z ∼N(0,8ϵ/d). Then,
I(H ;θ|θ˜)
T ≤I(Y ;θ|θ˜,X )
T 1 0
(cid:104) (cid:16) (cid:17)(cid:105)
=E d P(Y ∈·|θ,X ) ∥ P(Y ∈·|θ˜,X )
KL 1 0 1 0
(a) (cid:104) (cid:16) (cid:17)(cid:105)
≤ E d P(Y ∈·|θ,X ) ∥ P(Y ∈·|θ ←θ˜,X )
KL 1 0 1 0
 ln(cid:16) 1+e−θ˜⊤X0(cid:17) ln(cid:16) 1+eθ˜⊤X0(cid:17)
=E 1+e−θ⊤X0 + 1+eθ⊤X0 
 1+e−θ⊤X0 1+eθ⊤X0 
(cid:20)(cid:16) (cid:17)2(cid:21)
E θ⊤X −θ˜⊤X
(b) 0 0
≤
8
(cid:104) (cid:105)
E ∥θ−θ˜∥2
2
=
8
=ϵ,
where (a) follows from Lemma 57 and (b) follows from Lemma 23.
We now upper bound the rate.
I(θ;θ˜)=h(θ˜)−h(θ˜|θ)
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
(a) d 1 8ϵ d 8ϵ
≤ ln 2πe + − ln 2πe
2 d d 2 d
(cid:18) (cid:19)
d 1
= ln 1+ ,
2 8ϵ
where (a) follows from Lemma 11. The result follows.
6.3.3 Main Result
With the rate-distortion upper bound in place, we establish the following upper bound on estimation error.
Theorem 25. (logistic regression estimation error upper bound) For all d ∈ Z , if for all t, (X ,Y )
++ t t+1
is generated according to the logistic regression process, then for all T,
(cid:18) (cid:18) (cid:19)(cid:19)
d T
L ≤ 1+ln 1+ .
T 2T 4d
29Proof.
(a) H (θ)
L ≤ inf ϵ,T +ϵ
T ϵ≥0 T
(b)
dln(cid:0)
1+
d(cid:1)
≤ inf 2 8ϵ +ϵ
ϵ≥0 T
(cid:18) (cid:19)
(c) d T d
≤ ln 1+ + ,
2T 4d 2T
where (a) follows from Theorem 15, (b) follows from Lemma 24, and (c) follows from setting ϵ= d .
2T
We observe that just as in linear regression, we observe an error bound which is O˜(d/T). In the following section,
we consider a much more complex deep neural network data generating process.
6.4 Deep Neural Networks
Figure 5: We depict our deep neural network data generating process above. It consists of input dimension d,
width N, depth L, with output dimension 1, and ReLU activation units. We denote the weights at layer ℓ by A(ℓ)
and the output of layer ℓ by U(ℓ). We assume that the final output Y is the sum of the final network output U(L)
and independent Gaussian noise Z.
6.4.1 Data Generating Process
For a deep neural network, we are interested in estimating a collection of random matrices (A(1),...,A(L)) which
representtheweightsofthenetwork. WeassumeaknownpriordistributionP(A(1:L) ∈·)tomodelouruncertainty
over the network weights. For simplicity, we assume that the width of every hidden layer is identically set to a
positive integer N and that the network has input dimension d and output dimension 1. We further assume the
following prior distribution on the weights of the network.
 N(0,IN×d) if ℓ=1
 d
P(A(ℓ))= N(0,IN×N) if 2≤ℓ≤L−1.
N
N(0,IN)
if ℓ=L
N
We further make the assumption that the weights across layers are independent. These Gaussian assumptions are
again not necessary for the following results (only the specified covariance structure is required), but we provide
the above instance for concreteness.
30For all t∈Z , inputs and outputs are generated according to a random vector X i ∼id N(0,I ) and
+ t d
U(0) =X
t t
(cid:16) (cid:17)
U(ℓ+1) =ReLU A(ℓ+1)U(ℓ)
t t
Y =U(L)+W ,
t+1 t t+1
where W i ∼id N(0,σ2) for known variance σ2.
t
6.4.2 Theoretical Building Blocks
In this section, we will cover several helpful lemmas which will allow us to streamline the error analysis for neural
networks. Lemma 29 establishes an upper bound on the distortion function in the deep neural network setting.
This result allows us to easily derive upper bounds for the rate-distortion function in Theorem 30. However, we
begin with the following three Lemmas (26, 27, 28) which facilitate the derivation of distortion upper bound in
Lemma 29. We begin with an initial result which decomposes the total distortion I(Y;A(1:L)|A˜(1:L),X) into a sum
of simpler terms.
Lemma 26. (distortion decomposition) If (X,Y) are generated by the deep neural network process and
A˜ ,...,A˜ are independent random variables such that, Y ⊥A˜(1:L)|A(1:L),X, then
1 L
L
I(Y;A(1:L)|A˜(1:L),X) = (cid:88) I(Y;A(ℓ)|A(ℓ+1:L),A˜(1:ℓ),X).
ℓ=1
Proof.
L
I(Y;A(1:L)|A˜(1:L),X)( =a)(cid:88) I(Y;A(ℓ)|A(ℓ+1:L),A˜(1:L),X)
ℓ=1
L
( =a)(cid:88) I(Y;A(ℓ)|A(ℓ+1:L),A˜(1:ℓ),X),
ℓ=1
where (a) follows from the chain rule and (b) follows from conditional independence assumptions.
With this decomposition in mind, we can derive a suitable upper bound for the total distortion by deriving an
upper bound for each individual term of the decomposition. The following two results establish such an upper
bound for the individual terms.
Lemma27. (moreinformationwiththetrueinput)If(X,Y)aregeneratedbythedeepneuralnetworkprocess
and A˜ ,...,A˜ are independent random variables such that, Y ⊥A˜(1:L)|A(1:L),X, then for all ℓ∈{1,...,L},
1 L
I(Y;A(ℓ)|A(ℓ+1:L),A˜(1:ℓ),X) ≤ I(Y;A(ℓ)|A(ℓ+1:L),A˜(ℓ),U(ℓ−1)).
Proof.
I(Y;A(ℓ)|A(ℓ+1:L),A˜(1:ℓ),X)=I(Y,X,A˜(1:ℓ−1);A(ℓ)|A(ℓ+1:L),A˜(ℓ))−I(X,A˜(1:ℓ−1);A(ℓ)|A(ℓ+1:L),A˜(ℓ))
=I(Y,X,A˜(ℓ−1:1);A(ℓ)|A(ℓ+1:L),A˜(ℓ))
=I(Y;A(ℓ)|A(ℓ+1:L),A˜(ℓ))+I(X,A˜(1:ℓ−1);A(ℓ)|Y,A(ℓ+1:L),A˜(ℓ))
(a)
≤ I(Y;A(ℓ)|A(ℓ+1:L),A˜(ℓ))+I(X,A(1:ℓ−1);A(ℓ)|Y,A(ℓ+1:L),A˜(ℓ))
=I(Y,X,A(1:ℓ−1);A(ℓ)|A(ℓ+1:L),A˜(ℓ))
( =b)I(Y;A(ℓ)|X,A(1:ℓ−1),A(ℓ+1:L),A˜(ℓ))
( =c)I(Y;A(ℓ)|U(ℓ−1),A(ℓ+1:L),A˜(ℓ)),
where (a) follows from the fact that A(ℓ) ⊥A˜(1:ℓ−1)|(X,Y,A(1:ℓ−1)) and the data processing inequality, (b) follows
fromthefactthatI(A(ℓ);A(1:ℓ−1),X|A(ℓ+1:L),A˜(ℓ))=0,and(c)followsfromthefactthatY ⊥(A(1:ℓ−1),X)|U(ℓ−1).
31This result states that more information is extracted about A(ℓ) when we condition on the true input to layer ℓ
(U(ℓ−1)) as opposed to the approximate input based on (A(1:ℓ−1),X). This is a rather intuitive result as having
access to the true input U(ℓ−1) which generated label Y should allow us to extract more information about the
parametersA(ℓ). ThisallowsustosimplifyouranalysissincetheRHSofLemma27onlyconsistsofoneapproximate
termA˜(ℓ) asopposedtoℓofthemintheLHS.Thefollowingresultleveragesthissimplifiedformtoderiveanupper
bound for each individual term in the decomposition of Lemma 26.
Lemma 28. For all d,N,L∈Z ,σ2 ∈ℜ , if (X,Y) are generated according to the deep neural network process
++ ++
and A˜ ,...,A˜ are independent random variables such that for all Y ⊥A˜(1:L)|(A(1:L),X), then for all ℓ∈[L],
1 L
 (cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13)(A(ℓ)−A˜(ℓ))U(ℓ−1)(cid:13)
I(Y;A(ℓ)|A(ℓ+1:L),A˜(1:ℓ),X) ≤ 1 ln 1+ (cid:13) (cid:13) 2  .
2  σ2N 
Proof. In the proof below, we use the notation f to denote the depth L+1−ℓ MLP with ReLU activation
A(ℓ:L)
units and weights parameterized by A(ℓ:L).
(a)
I(Y;A(ℓ)|A(ℓ+1:L),A˜(1:ℓ),X) ≤ I(Y;A(ℓ)|A(ℓ+1:L),A˜(ℓ),U(ℓ−1))
=h(Y|A(ℓ+1:L),A˜(ℓ),U(ℓ−1))−h(Y|A(ℓ+1:L),U(ℓ−1))
=h(Y|A(ℓ+1:L),A˜(ℓ),U(ℓ−1))− 1 ln(cid:0) 2πeσ2(cid:1)
2
  E(cid:20)(cid:16)
Y −f
(cid:0)
U(ℓ−1)(cid:1)(cid:17)2(cid:12) (cid:12)A(ℓ+1:L),A˜(ℓ),U(ℓ−1)(cid:21)
( ≤b) E 1 ln

A˜(ℓ),A(ℓ+1:L) (cid:12)  

2  σ2 
  (cid:16)
f
(cid:0) U(ℓ−1)(cid:1)
−f
(cid:0) U(ℓ−1)(cid:1)(cid:17)2
≤E1
ln1+
A(ℓ:L) A˜(ℓ),A(ℓ+1:L)

2  σ2 
  (cid:16) (cid:17)2
A(L)A(L−1)...A(ℓ+1)(A(ℓ)−A˜(ℓ))U(ℓ−1)
(c) 1
≤ E ln1+ 
2  σ2 
 (cid:20)(cid:16) (cid:17)2(cid:21)
E A(L)A(L−1)...A(ℓ+1)(A(ℓ)−A˜(ℓ))U(ℓ−1)
(d) 1  
≤ ln1+ 
2  σ2 
 (cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13)(A(ℓ)−A˜(ℓ))U(ℓ−1)(cid:13)
(e) 1  (cid:13) (cid:13) 2 
= ln1+ 
2  σ2N 
where(a)followsfromLemma27, (b)followsfromLemma11, (c)comesfromthefactthatforallnandx,y ∈ℜn,
∥x−y∥2 ≥ ∥ReLU(x)−ReLU(y)∥2, (d) follows from Jensen’s inequality, and (e) follows from the independence
2
and variance assumptions of the deep neural network data generating process.
With this upper bound in place, it becomes trivial to derive an upper bound for the total distortion. We present
this result now whose proof follows as a direct application of the above established lemmas.
Lemma 29. (distortion upper bound) For all d,N,L ∈ Z ,σ2 ∈ ℜ , if (X,Y) are generated accord-
++ ++
ing to the deep neural network process and A˜ ,...,A˜ are independent random variables such that for all Y ⊥
1 L
32A˜(1:L)|(A(1:L),X), then
 (cid:20)(cid:13)(cid:16) (cid:17) (cid:13)2(cid:21)
E (cid:13) A(ℓ)−A˜(ℓ) U(ℓ−1)(cid:13)
I(Y;A(1:L)|A˜(1:L),X) ≤ (cid:88)L 1 ln 1+ (cid:13) (cid:13) 2  .
2  σ2N 
ℓ=1
Proof.
L
I(Y;A(1:L)|A˜(1:L),X)( =a)(cid:88) I(Y;A(ℓ)|A(ℓ+1:L),A˜(1:ℓ),X)
ℓ=1
 (cid:20)(cid:13)(cid:16) (cid:17) (cid:13)2(cid:21)
E (cid:13) A(ℓ)−A˜(ℓ) U(ℓ−1)(cid:13)
(b)(cid:88)L 1  (cid:13) (cid:13) 2 
≤ ln1+ ,
2  σ2N 
ℓ=1
where (a) follows from Lemma 26 and (b) follows from Lemma 28.
We note that remarkably, the error in each term of the sum in the RHS does not depend on L. This is a clear
improvement upon the results based on VC dimension [Bartlett et al., 1998, 2019] for which the term within the
log would contain the product of the operator norms of the matrices A(ℓ+1:L). The average-case framework allows
us to not incur such a penalty since for all ℓ, E[A(ℓ)⊤A(ℓ)]∝I. Therefore, the resulting rate-distortion bound will
depend only linearly on the parameter count of the network (as opposed to linearly in the product of parameter
count and depth). With an upper bound on the total distortion in place, we can easily derive an upper bound for
the rate-distortion function for the deep neural network data generating process.
Theorem 30. (deep neural network rate-distortion upper bound) For all d,N,L ∈ Z ,σ2,ϵ ∈ ℜ , if
++ ++
(X,Y) are generated according to the deep neural network process, then
 
(cid:18) (L−2)N2+N +dN(cid:19) 1
H ϵ(A(1:L)) ≤
2
ln1+
σ2(cid:16)
e2 Lϵ
−1(cid:17).
Proof. For all ℓ ∈ [L], let A˜(ℓ) = A(ℓ) +Z(ℓ) where Z(ℓ) ⊥ A(ℓ) and each element of Z(ℓ) is i ∼id N(0,δ2), where
δ2 =σ2(e2ϵ/L−1)/d(ℓ−1) and d(ℓ−1) denotes the dimension of U(ℓ−1). Then,
 (cid:20)(cid:13)(cid:16) (cid:17) (cid:13)2(cid:21)
E (cid:13) A(ℓ)−A˜(ℓ) U(ℓ−1)(cid:13)
I(Y;A(1:L)|A˜(1:L),X)≤(cid:88)L 1 ln
1+
(cid:13) (cid:13) 2 

2  σ2N 
ℓ=1
(cid:88)L
1
 E(cid:104)(cid:13) (cid:13)Z(ℓ)U(ℓ−1)(cid:13) (cid:13)2 2(cid:105)
= ln1+ 
2 σ2N
ℓ=1
=(cid:88)L 1 ln(cid:32)
1+
σ2N(e2 Lϵ −1)E(cid:2) ∥U(ℓ−1)∥2 2(cid:3)(cid:33)
2 σ2Nd(ℓ−1)
ℓ=1
=ϵ,
33where (a) follows from Lemma 29, and (b) follows from the fact that E[Z(ℓ)⊤Z(ℓ)]= σ2N(e2ϵ/L−1)I .
d(ℓ−1) d(ℓ−1)
L
I(A(1:L);A˜(1:L))=(cid:88) I(A(ℓ);A˜(ℓ))
ℓ=1
L
=(cid:88) h(A˜(ℓ))−h(A˜(ℓ)|A(ℓ))
ℓ=1
( ≤a)(cid:88)L d(ℓ−1)d(ℓ) ln(cid:18) 2πe(cid:18) δ2+ 1 (cid:19)(cid:19) − d(ℓ−1)d(ℓ) ln(cid:0) 2πeδ2(cid:1)
2 d(ℓ−1) 2
ℓ=1
(cid:88)L d(ℓ−1)d(ℓ) (cid:18) 1 (cid:19)
= ln 1+
2 δ2d(ℓ−1)
ℓ=1
(cid:88)L d(ℓ−1)d(ℓ) (cid:18) 1 (cid:19)
= ln 1+
2 σ2(e2 Lϵ −1)
ℓ=1
 
(cid:18) (L−2)N2+N +dN(cid:19) 1
= ln1+
(cid:16)
(cid:17),
2 σ2 e2 Lϵ −1
where (a) follows from Lemma 11. The result follows from the definiton of the rate-distortion function.
Note that the bound in Theorem 30 is only linear in the parameter count of the network. In a setting in which
we assume an independent prior on the weights of the network, such a result is the best that one could expect. In
the following subsection, we will leverage this rate-distortion upper bound and Theorem 15 to arrive at an upper
bound for the estimation error for the deep neural network setting.
6.4.3 Main Result
Withthetheoreticaltoolsdevelopedintheprevioussection,wenowestablishthemainresult,whichupperbounds
the estimation error of an optimal agent learning from data generated by the deep neural network process.
Theorem 31. (deep neural network estimation error upper bound) For all d,N,L ∈ Z ,σ2 ∈ ℜ , if
++ ++
for all t, (X ,Y ) are generated according to the deep neural network process, then for all T,
t t+1
(cid:18) (cid:18) (cid:19)(cid:19)
P 2LT
L ≤ 1+ln 1+ ,
T 2T σ2P
where P =(L−2)N2+N +dN denotes the total parameter count of the network.
Proof.
(a) H (A(1:L))
L ≤ inf ϵ +ϵ
T ϵ≥0 T
(cid:32) (cid:33)
P ln 1+ 1
(cid:16) 2ϵ (cid:17)
(b) σ2 eL−1
≤ inf +ϵ
ϵ≥0 2T
  
P 1+ln1+ (cid:18) 1 (cid:19)
P
(c) σ2 e2LT−1
≤
2T
(cid:18) (cid:18) (cid:19)(cid:19)
(d) P 2LT
≤ 1+ln 1+ ,
2T σ2P
34where (a) follows from Theorem 15, (b) follows from Theorem 30, (c) follows by setting ϵ=P/2T, and (d) follows
from the fact that for all x∈ℜ ,
+
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
ln 1+ ≤ln 1+ .
σ2(ex−1) σ2x
Notably, Theorem 31 establishes an upper bound which is only linear in the total parameter count of the network
(O(P)). This notably improves upon existing results from the frequentist line of analysis [Bartlett et al., 1998,
Harvey et al., 2017] which derive an upper bound which is O˜(PL). As mentioned in the previous section, we
are able to arrive at these stronger results by leveraging an expectation with respect to the prior distribution as
opposed to a worst-case assumption over the hypotheses in a set.
Since we observe empirically that deep neural networks are able to effectively learn even in the presence of limited
data, our error analysis in the Bayesian framework provides results which are closer to qualitative observations of
empirical studies. However, the results of this section are not sufficient to explain how learning may be possible
whenthedatasetsizeissmaller thantheparametercountofthemodelwhichgeneratedthedata. Suchresultswill
require stronger assumptions surrounding the dependence between weights in the neural network. In the following
section, we explore this phenomenon in a nonparametric setting in which the neural network which generated the
data may consist of infinitely many parameters, but good performance can be obtained with relatively modest
amounts of data.
6.5 Nonparametric Learning
Figure 6: We depict our nonparametric data generating process above. It consists of input dimension d, an infinite
numberofhiddenunitswithReLUactivations,andoutputdimension1. Wedenotetheweightsofthefirstlayervia
aninfinitedimensionalmatrixAandtheweightsoftheoutputlayerbyaninfinitedimensionalvectorθ. Toenforce
structure which enables learning, we assume an appropriate prior distribution which results in a concentration of
the weights of θ as depicted by the solid (as opposed to dotted) lines in the above diagram. We assume that the
final output Y is the sum of the final network output and independent Gaussian noise Z.
In this section, we study a nonparameteric data generating process which can be parameterized by a two-layer
neural network of infinite width. While the results we present in this section are limited to a two-layer example,
the techniques derived in this section are general enough to be applied to appropriate instances of nonparameteric
deep neural networks as well.
6.5.1 Data Generating Process
For a nonparametric neural network, we are interested in estimating a function which can be uniquely identified
by an infinite-dimensional matrix A ∈ ℜ∞×d which represents the first-layer weights, and an infinite-dimensional
vectorθ ∈ℜ∞ whichdenotestheoutput-layerweights. Asinpriorexamples,weassumeaknownpriordistribution
35P((A,θ)∈·). The neural network has input dimension d and output dimension 1. In this analysis, we restrict our
attention to a particular prior distribution on the weights of the network. This prior distribution is describe by a
Dirichlet process which detail next.
A Dirichlet process is a stochastic process whose realizations are probability mass functions over a countable
(often infinite) support. The Dirichlet process takes as input a scale parameter K, and a base distribution P. Its
realizations are hence probability mass functions with support on a countable subset of the set defined by the base
distribution P. In our problem, we will take this base distribution to be Unif(Sd−1), the uniform distribution over
the unit sphere of dimension d. As a result, realizations of the Dirichlet process will be probability mass functions
over a countable subset of Sd−1.
As such, a realization of a Dirichlet process exhibits a parameterization via a vector in θ¯ ∈ ℜ∞ which denotes
the frequencies of each outcome along with the collection of vectors in A ∈ Sd−1 for n ∈ Z , which comprise
n ++
of the support. With this parameterization, we can construct the following data generating process. For all t, let
iid
X ∼ N(0,I ) and
t d
√ ∞
(cid:88)
Y =W + K· θ ReLU(A⊤X ),
t+1 t+1 n n t
n=1
(cid:40)
θ¯ w.p. 0.5
θ = n ,
n −θ¯ w.p. 0.5
n
where θ¯ denotes the frequency associated with the outcome denoted by A and for all t, W i ∼id N(0,σ2) denotes
n n t
independent additive Gaussian noise of known variance σ2.
WenowspeakaboutthescaleparameterK. Evidently,θ¯,thoughinℜ∞,islimitedincomplexitybythefactthatfor
all n, θ¯ ≥0 and (cid:80)∞ θ¯ =1. The scale parameter K induces additional structure in the form of concentration.
n n=1 n
The Dirichlet process inherently assumes a degree of concentration from the fact that its realizations are in a
countable subset of Sd−1. However, the scale parameter induces further concentration to create a sparsity-like
structureintheoutcomes. Withoutsuchstructure,effectivelearninginthepresenceoffinitecomputeandmemory
may be infeasible. Therefore, despite the fact that this neural network is parameterized by an infinite number of
parameters, thestructure inducedbyDirichletpriorand finitescaleparameterK willallow usto reasonaboutthe
achievable performance of an optimal learning algorithm.
6.5.2 Theoretical Building Blocks
In prior sections, we often derived the appropriate rate-distortion result by simply devising a compression which
adds independent Gaussian noise to the parameters. This will no longer suffice as the parameters of interest are
infinite-dimensional and would hence result in an infinite rate. We introduce a proof technique derived from early
resultsbyBarron[1993]inhisseminalworkontheapproximationratesofneuralnetworks. Thekeyinsightisthat
if one samples m times from the categorical distribution induced by θ¯,A and simply construct a function which
averages the observed outcomes, the approximation error (in our case, distortion) would decay linearly in m. The
following theoretical result concretely establishes this idea.
Lemma 32. (approximation via multinomial) For all K,m ∈ Z , let (A,θ¯) ∼ DP(K,Unif(Sd−1)) and for
++
all n∈Z ,
++
(cid:40)
θ¯ w.p. 0.5
θ = n .
n −θ¯ w.p. 0.5
n
If for all i∈[m], c i ∼id Categorical(θ¯) and A˜ =A , then
i i ci
 2
√
E      √ K·(cid:88)∞ θ nReLU(A⊤ nX t)− mK (cid:88)m sign(θ ci)·ReLU(A˜⊤
i
X t)  

  

≤ K m.
 n=1 i=1  
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
true function approximation
36Proof.
E (cid:32) √ K·(cid:88)∞
θ nReLU(A⊤ nX t)−
√ mK (cid:88)m
sign(θ ci)·ReLU(A˜⊤ i X
t)(cid:33)2

n=1 i=1
( =a)E (cid:32)√ mK (cid:88)m sign(θ ci)·ReLU(A˜⊤ i X t)(cid:33)2 −(cid:32) √ K·(cid:88)∞ θ nReLU(A⊤ nX t)(cid:33)2 
i=1 n=1
(cid:32)√
m
(cid:33)2
≤E  mK (cid:88) sign(θ ci)·ReLU(A˜⊤ i X t) 
i=1
m
( =b) K (cid:88) E(cid:104) (ReLU(A˜⊤X ))2(cid:105)
m2 i t
i=1
m
≤ K (cid:88) E(cid:104) (A˜⊤X )2(cid:105)
m2 i t
i=1
K
= ,
m
where(a)followsfromthefactthatthetwoexpressionsinthedifferencehavethesameexpectationand(b)follows
from the fact that the c ’s are independent and E[sign(θ )]=0.
i ci
This result establishes that despite the fact that this data generating process ostensibly has infinite complexity,
there exist good approximations which only require finite information about the data generating process. The
above result will allow us to establish bounds on the distortion of a suitable compression. The following result will
allow us to establish favorable bounds on the rate of the compression.
Lemma 33. (Dirichlet-multinomial concentration) For all K,m∈Z , let (A,θ¯)∼DP(K,Unif(Sd−1)). If
++
for all i ∈ [m], c i ∼id Categorical(θ¯) and N denotes the random variable which represents the number of unique
i m
categories draw in (c ,...,c ), then
1 m
(cid:16) n(cid:17)
E[N ] ≤ Kln 1+ .
m K
Proof. As the proof of this result requires significant mathematical machinery, we refer the reader to Appendix B
for the result.
Lemma 33 establishes that the approximation studied above also has favorable informational complexity. Suppose
we construct a compression of (θ,A) as in the statement of Lemma 32, however, instead of setting A˜ = A , we
i ci
instead use a quantization of Sd−1. Let A denote an ϵ-cover of Sd−1 with respect to ∥·∥2. Furthermore, let
ϵ 2
A¯ =argmin ∥A −a∥2.
i,ϵ i 2
a∈Aϵ
Suppose we construct a compression for which A˜ = A¯ . Then, the collection (A˜ ,...,A˜ ) will have finite
i ci,ϵ 1 m
entropy since it consists of a finite collection of discrete random variables defined on the finite set A . However, a
ϵ
naive calculation of the entropy will result in a sub-optimal bound on the rate. We arrive at a tighter bound by
considering a loss-less compression of (A˜ ,...,A˜ ) and leveraging the insight of Theorem 3 that entropy is upper
1 m
bounded by the average length of an optimal prefix-free code.
Theorem 34. (Dirichlet process rate-distortion upper bound) For all d,K ∈ Z and σ2,ϵ ∈ ℜ , if
++ ++
(X,Y) are generated according to the Dirichlet neural network process with parameters θ,A, then
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
2 2K 2 4
H (θ,A) ≤ Kln 1+ ln +2dKln 1+ ln 1+ .
ϵ σ2ϵ σ2ϵ σ2ϵ ϵ
Proof. Supposewesetϵ′ =mσ2(e2K/(mσ2)−1)/2K−1≥0andm=⌈ K ⌉. Foralli∈[m],letc i ∼id Categorical(θ¯).
σ2ϵ i
Let A denote an ϵ′-cover of Sd−1 with respect to ∥·∥2 and for all i∈Z , let
ϵ 2 ++
A¯ =argmin ∥A −a∥2.
i,ϵ i 2
a∈Aϵ
37Recall that N denotes the number of unique outcomes in (c ,...,c ). Let I = (I ,...,I ) denote an ordered
set which consm ists of the unique outcomes. Let (θ˜,A˜)=(θ˜ ,A˜1 ,...,θ˜m ,A˜ ) be a1 collectN iom n of random variable
1 1 Nm Nm
such that
m
A˜ =A¯ ; θ˜ =(cid:88) sign(θ )·1[c =I ].
i Ii,ϵ i Ii j i
j=1
Therefore, θ˜consists of N integers in ±[m] and A˜ consists of N outcomes from the set A . We begin by upper
m m ϵ
bounding the distortion of (θ˜,A˜).
I(Y;θ,A|θ˜,A˜,X)=h(Y|θ˜,A˜,X)−h(Y|θ,A)
(cid:32) √ (cid:12) (cid:33)
=h Y − mK (cid:88) θ˜ i·ReLU(A˜⊤
i
X)(cid:12) (cid:12) (cid:12)θ˜,A˜ −h(W)
i∈I
 (cid:16) √ (cid:17)2
Y − K (cid:80) θ˜ ·ReLU(A˜⊤X)
( ≤a) E1
ln
m i∈I i i

2  σ2 
 (cid:20)(cid:16) √ (cid:17)2(cid:21)
E Y − K (cid:80) θ˜ ·ReLU(A˜⊤X)
(b) 1  m i∈I i i 
≤ ln 
2  σ2 
 (cid:16)√ √ (cid:17)2
K·(cid:80)∞ θ ReLU(A⊤X)− K (cid:80)m sign(θ )·ReLU(A˜⊤X)
=
1
ln1+
n=1 n n m i=1 ci i

2  σ2 
 (cid:20)(cid:16) (cid:17)2(cid:21)
2K + 2KE (cid:80)m sign(θ )ReLU(A⊤X)−(cid:80)m sign(θ )·ReLU(A˜⊤X)
(c) 1  m m2 i=1 ci i i=1 ci i 
≤ ln1+ 
2  σ2 
 2K + 2K (cid:80)m E(cid:104) (ReLU(A⊤X)−ReLU(A˜⊤X))2(cid:105)
1 m m2 i=1 i i
= ln1+ 
2 σ2
 (cid:104) (cid:105)
2K + 2KE (A⊤X−A˜⊤X)2
(d) 1 m m i i
≤ ln1+ 
2 σ2
(e) 1
(cid:18) 2K(1+ϵ′)(cid:19)
≤ ln 1+
2 σ2m
K
=
σ2m
≤ϵ,
where (a) follows from Theorem 11 and the fact that conditioning reduces differential entropy, (b) follows from
Jensen’sinequality, (c)followsfromLemma32andthefactthatforallx,y,z ∈ℜ, (x−y)2 ≤2(x−z)2+2(z−y)2,
(d)followsfromthefactthatforallx,y ∈ℜ, (x−y)2 ≥(ReLU(x)−ReLU(y))2, and(e)followsfromthefactthat
A˜ is the closest element in an ϵ′ cover of Sd−1.
i
38We now upper bound the rate of (θ˜,A˜)
I(θ,A;θ˜,A˜)≤H(θ˜,A˜)
(a)
≤ E[N ·(ln(2m)+ln(|A |))]
m ϵ′
(b) (cid:16) m(cid:17)(cid:18) 3 (cid:19)
≤ Kln 1+ ln2m+dln
K ϵ′2
(cid:18) (cid:19)(cid:32) (cid:32) √ (cid:33)(cid:33)
(c) 2 2K ϵ 3
≤ Kln 1+ ln +2dln
σ2ϵ σ2ϵ eϵ−1−ϵ
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
(d) 2 2K 2 4
≤ Kln 1+ ln +2dKln 1+ ln 1+ ,
σ2ϵ σ2ϵ σ2ϵ ϵ
where (a) follows from Theorem 3 and the fact that a realization (θ˜,A˜) can be mapped to a codeword of length
N (ln(2m) + ln(|A |)) by using ln(2m) nats to encode each of θ˜ ,...,θ˜ , and ln(|A |) to encode each of
m ϵ′ 1 Nm ϵ′
A˜ ,...,A˜ , (b) follows from Lemma 33 and the fact that |A | ≤ (3/ϵ′2)d, (c) follows by upper bounding
1 Nm ϵ′
the quantity via m = 2K/(σ2ϵ) as opposed to ⌈K/(σ2ϵ)⌉, and (d) follows from the fact that for all ϵ ≥ 0,
√
ln(ϵ 3/(eϵ−1−ϵ))≤ln(1+4/ϵ).
6.5.3 Main Result
With the theoretical tools derived in the previous section, we now establish the following upper bound on the
estimation error of an optimal agent learning from data generated by the Dirichlet process.
Theorem 35. (Dirichlet process estimation error upper bound) For all d,K ∈Z , if for all t, (X ,Y )
++ t t+1
are generated according to the Dirichlet neural network process, then for all T,
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
K T T 2dK T T
L ≤ ln 1+ ln + 1+ln 1+ ln 1+ .
T T σ2d σ2d T σ2d dK
Proof. The result follows from Theorems 15 and 34 and setting ϵ=2dk/T.
Notably, this result is O˜(dK/T) despite the fact that the Dirichlet neural network process is described by a neural
network with infinite width. The scale parameter K determines the degree of concentration which occurs in the
output-layer weights of the network and hence controls the difficult of learning. This example of learning under
data generated by a nonparametric process further demonstrates the flexibility and ingenuity of proof techniques
which are encompassed in our framework. We hope that the suite of examples provided in this section enables the
reader to analyze whichever complex processes they may have in mind.
Summary
• (monotonicity of per-timestep estimation error) If for all s∈Z , (X ,Y ) is sampled iid from
+ s s+1
some distribution P(·|θ), then for all t∈Z ,
+
I(Y ;θ|H ) ≤ I(Y ;θ|H ).
t+2 t+1 t+1 t
• For all ϵ≥0, we let
H (θ)= inf I(θ;θ˜),
ϵ
θ˜∈Θ˜
ϵ
where
Θ˜ ={θ˜:θ˜⊥H |θ, I(Y ;θ|θ˜,X )≤ϵ}.
ϵ ∞ 1 0
Notethatincontrasttothegeneralrate-distortionformulationofsection5,thismodifiedrate-distortion
does not depend on the time horizon T.
39• (linear regression estimation error bounds) For all input dimensions d ∈ Z and noise variance
++
σ2 ≥0, if for all t, (X ,Y ) are generated according to the linear regression processes, then for all T,
t t+1
(cid:32) (cid:33) (cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
d 2T d T 1 d
W ≤ L ≤ ln + ln 1+ ,
2T d(8+ d σ2) T 2T σ2d 2T T
d−2 +
where W is the Lambert W function.
• (logistic regression estimation error upper bound) For all input dimensions d∈Z , if for all t,
++
(X ,Y ) is generated according to the logistic regression process, then for all T,
t t+1
(cid:18) (cid:18) (cid:19)(cid:19)
d T
L ≤ 1+ln 1+ .
T 2T 4d
• (deep neural network estimation error upper bound) For all input dimensions d, widths N, and
depths L each ∈ Z , and noise variances σ2 ∈ ℜ , if for all t, (X ,Y ) are generated according to
++ ++ t t+1
the deep neural network process, then for all T,
(cid:18) (cid:18) (cid:19)(cid:19)
P 2LT
L ≤ 1+ln 1+ ,
T 2T σ2P
where P =(L−2)N2+N +dN denotes the total parameter count of the network.
• (Dirichlet process estimation error upper bound) For all input dimensions d, scale parameters
K, each ∈ Z , if for all t, (X ,Y ) are generated according to the Dirichlet neural network process,
++ t t+1
then for all T,
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
K T T 2dK T T
L ≤ ln 1+ ln + 1+ln 1+ ln 1+ .
T T σ2d σ2d T σ2d dK
407 Learning from Sequences
In the previous section, we focused on the special case of learning from an (infintely) exchangeable sequence of
data (iid when conditioned on θ). However, in general, machine learning systems will have to reason about data
which does not obey this rigid structure. For instance, suppose that X ,X ,X ,... describes a sequence of text
0 1 2
tokens from a book. It’s clear that such a sequence would not exhibit exchangeability as the order of the tokens
playsacriticalroleinderivingmeaning. Existingframeworksforanalyzingmachinelearningcanonlyreasonabout
learning from sequential data under rigid and contrived notions of mixing time for the data generating process.
However, in our framework, Theorem 15 does not make any explicit assumptions on the structure of the data.
In this section, we will demonstrate that Theorem 15 seamlessly extends to the analysis of learning from data
generated by an auto-regressive process.
7.1 Data Generating Process
Let X ,X ,X , be a sequence of random variables representing observations. We assume that this sequence is
0 1 2
generated by an autoregressive model which is parameterized by a random variable θ. As a result, for all t, X is
t+1
drawn according to a probability distribution which depends on θ and the history H = (X ,X ,...,X ). In this
t 0 1 t
section, we will analyze learning under two concrete autoregressive data generating processes. The first involves a
binary AR(K) process and the second a transformer model with context length K.
7.2 Binary AR(K) Process
7.2.1 Data Generating Process
We begin with a simple AR(K) process over a binary alphabet. We aim to demonstrate that our framework can
easily adapt to machine learning from sequential data. In this problem, we are interested in estimating random
vectorsθ ,...,θ ∈ℜd fromdatawhichisgeneratedinthefollowingway. Let(X ,X ...,X )∼Unif({0,1}K).
1 K 0 1 K−1
Furthermore, consider known vectors Φ ,Φ ∈ℜd of L2 norm equal to 1 which correspond to vector embedding of
0 1
the binary outcomes 0 and 1 respectively. For brevity of notation, we use ϕ to denote Φ .
t Xt
For all t≥K, let
(cid:40) 1 w.p. σ(cid:16) (cid:80)K θ⊤ϕ (cid:17)
X = k=1 k t−k+1 ,
t+1
0 otherwise
whereσdenotesthesigmoidfunction. Forallk,weassumetheindependentpriordistributionsP(θ )=N(0,I /(K)).
k d
7.2.2 Preliminary Theoretical Results
As in all prior examples, our strategy is to leverage rate-distortion theory to arrive at an estimation error upper
bound. We begin with the following result which upper bounds the rate-distortion function in the binary AR(K)
problem setting.
Lemma 36. (binary AR(K) rate-distortion upper bound) For all d,K ∈ Z and ϵ ∈ ℜ , if for all
++ ++
t∈Z ,X is generated according to the binary AR(K) process, then for all T,
+ t
(cid:18) (cid:19)
dK 1
H (θ ) ≤ ln 1+ .
ϵ,T 1:K 2 8ϵ
Proof. For k ∈{0,1,...,K−1}, let θ˜ =θ +Z where Z ⊥θ and Z ∼N(0,I ·8ϵ/K). For all t, let H denote
k k k k k k d t
41(X ,X ,...,X ). Then,
0 1 t
I(H ;θ |θ˜ )
T 1:K 1:K
T
T−1
= 1 (cid:88) I(X ;θ |θ˜ ,H )
T t+1 1:K 1:K t
t=0
T−1
≤ 1 (cid:88) I(X ;θ |θ˜ ,X )
T t+1 1:K 1:K t:t−K+1
t=0
T−1
≤ 1 (cid:88) E(cid:104) d (cid:16) P(X ∈·|θ ,X ) ∥ P(X ∈·|θ˜ ,X )(cid:17)(cid:105)
T KL t+1 1:K t:t−K+1 t+1 1:K t:t−K+1
t=0
T−1
( ≤a) 1 (cid:88) E(cid:104) d (cid:16) P(X ∈·|θ ,X ) ∥ P(X ∈·|θ ←θ˜ ,X )(cid:17)(cid:105)
T KL t+1 1:K t:t−K+1 t+1 1:K 1:K t:t−K+1
t=0
 (cid:18) (cid:19) (cid:18) (cid:19)
ln
1+e−(cid:80)K k=1θk⊤ϕt−k+1
ln
1+e(cid:80)K k=1θk⊤ϕt−k+1
=
1 T (cid:88)−1 E

1+e−(cid:80)K k=1θk⊤ϕt−k+1
+
1+e(cid:80)K k=1θk⊤ϕt−k+1 

T
t=0
 1+e−(cid:80)K k=1θ k⊤ϕt−k+1 1+e(cid:80)K k=1θ k⊤ϕt−k+1 
(cid:20)(cid:16) (cid:17)2(cid:21)
E (cid:80)K θ⊤ϕ −θ˜⊤ϕ
(b) 1 T (cid:88)−1 k=1 k t−k+1 k t−k+1
≤
T 8
t=0
(cid:104) (cid:105)
T−1E (cid:80)K ϕ⊤ (θ −θ˜ )(θ −θ˜ )⊤ϕ
1 (cid:88) k=1 t:t−K+1 k k k k t:t−K+1
=
T 8
t=0
(cid:104) (cid:105)
ϵ·E (cid:80)K ϕ⊤ ϕ
k=1 t−k t−k+1
=
K
=ϵ,
where (a) follows from Lemma 57 and (b) follows from Lemma 23.
We now upper bound the rate.
I(θ ;θ˜ )=h(θ˜ )−h(θ˜ |θ )
1:K 1:K 1:K 1:K 1:K
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
(a) dK 1 8ϵ dK 8ϵ
≤ ln 2πe + − ln 2πe
2 K K 2 K
(cid:18) (cid:19)
dK 1
= ln 1+ ,
2 8ϵ
where (a) follows from Lemma 11. The result follows.
7.2.3 Main Result
We now present the main result of this section which upper bounds the estimation error of learning under the
binary AR(K) data generating process. The result follows directly as a result of Lemma 36.
Theorem 37. (binary AR(K) estimation error upper bound) For all d,K ∈ Z , if for all t ∈ Z ,X is
++ + t
generated according to the binary AR(K) process, then for all T,
(cid:18) (cid:18) (cid:19)(cid:19)
dK T
L ≤ 1+ln 1+ .
T 2T 4dK
42Proof.
(a) H (θ )
L ≤ inf ϵ,T 1:K +ϵ
T ϵ≥0 T
(b)
dKln(cid:0)
1+
1(cid:1)
≤ inf 8ϵ +ϵ
ϵ≥0 2T
(cid:18) (cid:18) (cid:19)(cid:19)
(c) dK T
≤ 1+ln 1+ ,
2T 4dK
where (a) follows from Theorem 15, (b) follows from Lemma 36, and (c) follows by setting ϵ=(dK)/(2T).
An interesting aspect of this result is that both the proof techniques and final result are hardly impacted by the
factthatthesequenceisnot iid whenconditionedonθ . Existingtoolsforstatisticalanalysiscanstruggleinthe
1:K
setting without the appropriate averages of iid quantities. However, our analytical tools involving rate-distortion
theory allow us to handle such problem instances with relative ease and produce reasonable upper bounds on
estimation error in learning settings involving sequences of data. In the following section, we extend these tools to
analyze a more complex transformer data generating process.
7.3 Transformer Process
7.3.1 Data Generating Process
LetX ,X ,...beasequenceoverafinitevocabulary{1,...,d}. Eachofthedoutcomesisassociatedwithaknown
0 2
embedding vector denoted as Φ for j ∈{1,...,d}. We assume that for all j, ∥Φ ∥ =1. For brevity of notation,
j j 2
we let ϕ =Φ i.e. the embedding associated with token X .
t Xt t
LetK denotethecontextlengthofthetransformer,Ldenoteit’sdepth,andr denotetheattentiondimension. We
assume that the first token X is sampled from an arbitrary pmf on {1,...,d} but subsequent tokens are sampled
0
basedonthepreviousK tokenswithinthecontextwindowandtheweightsofadepthLtransformermodel.
Just as in the deep neural networks section, we use U(ℓ) to denote the output of layer ℓ at time t. As a result, for
t
ℓ=0, U =ϕ (the embeddings associated with the past K tokens). For all t≤T,ℓ<L, let
t,0 t−K+1:t
(cid:32) (cid:33)
(cid:16) (cid:17) U(ℓ−1)⊤A(ℓ)U(ℓ−1)
Attn(ℓ) U(ℓ−1) =Softmax t √ t
t r
denotetheattentionmatrixoflayer(ℓ)whereSoftmaxdenotesthesoftmaxfunctionappliedelementwisealongthe
columns. ThematrixA(ℓ) ∈ℜr×r canbeinterpretedastheproductofthekeyandquerymatricesandwithoutloss
of generality, we assume that the elements of the matrices A(ℓ) are distributed iid N(0,1) (Gaussian assumption is
not crucial but unit variance is).
Subsequently, we let
(cid:16) (cid:16) (cid:17)(cid:17)
U(ℓ) =Clip V(ℓ)U(ℓ−1)Attn(ℓ) U(ℓ−1) ,
t t t
where Clip ensures that each column of the matrix input has L norm at most 1. The matrix V(ℓ) resembles
2
the value matrix and we assume that the rows of V(ℓ) are distributed iid Unif(Sd−1). For ℓ < L, we have that
V(ℓ) ∈ℜr×r, whereas V(L) ∈ℜd×r.
Finally, the next token is generated via sampling from the softmax of the final layer:
(cid:16) (cid:17)
X ∼Softmax U(L)[−1] ,
t+1 t
where U(L)[−1] denotes the right-most column of U(L). At each layer ℓ, the parameters θ(ℓ) consist of the matrices
t t
A(ℓ),V(ℓ).
437.3.2 Theoretical Building Blocks
As with deep neural networks, our strategy is to drive a rate-distortion bound by deriving a suitable bound for the
distortion function. Recall that by the chain rule, for all t,
L
I(X ;θ(1:L)|θ˜(1:L),H ) = (cid:88) I(X ;θ(ℓ)|θ(ℓ+1:L),θ˜(1:ℓ),H ).
t+1 t t+1 t
ℓ=1
In the deep neural network setting, the above RHS could be further simplified via an upper bound which replaces
the conditioning on θ˜(1:ℓ) to (θ˜(ℓ),θ(1:ℓ−1)). However, this result relied heavily on the iid structure of the data. In
the sequential data setting, we have the following weaker result which still facilitates our analysis.
Lemma 38. For all t,L∈Z and ℓ∈{1,...,L}, if θ(i) ⊥θ(j), θ˜(i) ⊥θ˜(j), and θ(i) ⊥θ˜(j) for i̸=j, then
++
I(X ;θ(ℓ)|θ(ℓ+1:L),θ˜(1:ℓ),H ) ≤ I(H ;θ(ℓ)|θ(ℓ+1:L),θ(1:ℓ−1),θ˜(ℓ),X ).
t+1 t t+1 0
Proof.
I(X ;θ(ℓ)|θ(ℓ+1:L),θ˜(1:ℓ),H )( =a)I(H ,θ˜(1:ℓ−1);θ(ℓ)|θ(ℓ+1:L),θ˜(ℓ))−I(H ,θ˜(1:ℓ−1);θ(ℓ)|θ(ℓ+1:L),θ˜(ℓ))
t+1 t t+1 t
(b)
≤ I(H ,θ˜(1:ℓ−1);θ(ℓ)|θ(ℓ+1:L),θ˜(ℓ),X )
t+1 0
(c)
≤ I(H ,θ(1:ℓ−1);θ(ℓ)|θ(ℓ+1:L),θ˜(ℓ),X )
t+1 0
( =d)I(H ,θ(1:ℓ−1);θ(ℓ)|θ(ℓ+1:L),θ˜(ℓ),X )−I(θ(1:ℓ−1);θ(ℓ)|θ(ℓ+1:L),θ˜(ℓ),X )
t+1 0 0
( =e)I(H ;θ(ℓ)|θ(ℓ+1:L),θ(1:ℓ−1),θ˜(ℓ),X )
t+1 0
where (a) follows from the chain rule of mutual information, (b) follows from the independence assumptions, (c)
follows from the data processing inequality applied to the markov chain θ ⊥θ˜(1:ℓ−1)|(H ,θ(ℓ+1:L),θ(1:ℓ−1),XK),
i t+1 0
(d) follows from the fact that I(θ(1:ℓ−1);θ |θ(ℓ+1:L),θ˜,X ) = 0, and (e) follows from the chain rule of mutual
i i 0
information.
Note that in the RHS of Lemma 38 is H as opposed to X which we would have desired. Nonetheless, this
t+1 t+1
factor of t will eventually only appear as a logarithmic factor in the final bound. To account for the influence of
conditioningonthefuturelayersθ(ℓ+1:L), weestablishthefollowingresultonthesquaredLipschitzconstant. Note
that we use ∥·∥ to denote the operator norm of a matrix and ∥·∥ to denote the frobenius norm.
σ F
Lemma 39. (transformer layer Lipschitz) For all d,r,K,ℓ∈Z , if
++
(cid:16) (cid:16) (cid:17)(cid:17)
U˜(ℓ) =Clip V(ℓ)U˜(ℓ−1)Attn(ℓ) U˜(ℓ−1) ,
then
(cid:13) (cid:13)2 (cid:18) 4K∥A(ℓ)∥2(cid:19) (cid:13) (cid:13)2
(cid:13)U(ℓ)−U˜(ℓ)(cid:13) ≤ 2K∥V(ℓ)∥2 1+ σ ·(cid:13)U(ℓ−1)−U˜(ℓ−1)(cid:13) .
(cid:13) (cid:13) F σ r (cid:13) (cid:13) F
44Proof.
(cid:13) (cid:13)2
(cid:13)U(ℓ)−U˜(ℓ)(cid:13)
(cid:13) (cid:13)
F
(cid:13) (cid:13) (cid:18) (cid:18) U(ℓ−1)⊤A(ℓ)U(ℓ−1)(cid:19)(cid:19) (cid:32) (cid:32) U˜(ℓ−1)⊤A(ℓ)U˜(ℓ−1)(cid:33)(cid:33)(cid:13) (cid:13)2
=(cid:13)Clip V(ℓ)U(ℓ−1)σ √ −Clip V(ℓ)U˜(ℓ−1)σ √ (cid:13)
(cid:13) r r (cid:13)
(cid:13) (cid:13)
F
(a)(cid:13) (cid:13) (cid:18) U(ℓ−1)⊤A(ℓ)U(ℓ−1)(cid:19) (cid:32) U˜(ℓ−1)⊤A(ℓ)U˜(ℓ−1)(cid:33)(cid:13) (cid:13)2
≤ (cid:13)V(ℓ)U(ℓ−1)σ √ −V(ℓ)U˜(ℓ−1)σ √ (cid:13)
(cid:13) r r (cid:13)
(cid:13) (cid:13)
F
(
=b)(cid:88)K
∥V(ℓ)∥2
(cid:13) (cid:13) (cid:13)U(ℓ−1)σ(cid:32) U(ℓ−1)⊤ √A(ℓ)U k(ℓ−1)(cid:33) −U˜(ℓ−1)σ(cid:32) U˜(ℓ−1)⊤ √A(ℓ)U˜ k(ℓ−1)(cid:33)(cid:13) (cid:13) (cid:13)2
σ(cid:13) r r (cid:13)
(cid:13) (cid:13)
k=1 2
≤(cid:88)K
2∥V(ℓ)∥2
(cid:13) (cid:13) (cid:13)U(ℓ−1)σ(cid:32) U(ℓ−1)⊤ √A(ℓ)U k(ℓ−1)(cid:33) −U˜(ℓ−1)σ(cid:32) U(ℓ−1)⊤ √A(ℓ)U k(ℓ−1)(cid:33)(cid:13) (cid:13) (cid:13)2
σ(cid:13) r r (cid:13)
(cid:13) (cid:13)
k=1 2
+(cid:88)K
2∥V(ℓ)∥2
(cid:13) (cid:13) (cid:13)U˜(ℓ−1)σ(cid:32) U(ℓ−1)⊤ √A(ℓ)U k(ℓ−1)(cid:33) −U˜(ℓ−1)σ(cid:32) U˜(ℓ−1)⊤ √A(ℓ)U˜ k(ℓ−1)(cid:33)(cid:13) (cid:13) (cid:13)2
σ(cid:13) r r (cid:13)
(cid:13) (cid:13)
k=1 2
K
( ≤c)(cid:88)
2∥V(ℓ)∥2∥U(ℓ−1)−U˜(ℓ−1)∥2
σ F
k=1
+(cid:88)K 2K
∥V(ℓ)∥2
(cid:13) (cid:13)U(ℓ−1)⊤A(ℓ)U(ℓ−1)−U˜(ℓ−1)⊤A(ℓ)U˜(ℓ−1)(cid:13) (cid:13)2
r σ(cid:13) k k (cid:13) 2
k=1
( ≤d)
2K∥V(ℓ)∥2
(cid:13) (cid:13)U(ℓ−1)−U˜(ℓ−1)(cid:13) (cid:13)2
+
4K (cid:88)K
∥V(ℓ)∥2
(cid:13) (cid:13)U(ℓ−1)⊤A(ℓ)U(ℓ−1)−U(ℓ−1)⊤A(ℓ)U˜(ℓ−1)(cid:13) (cid:13)2
σ(cid:13) (cid:13) F r σ(cid:13) k k (cid:13) 2
k=1
+
4K (cid:88)K
∥V(ℓ)∥2
(cid:13) (cid:13)U(ℓ−1)⊤A(ℓ)U˜(ℓ−1)−U˜(ℓ−1)⊤A(ℓ)U˜(ℓ−1)(cid:13) (cid:13)2
r σ(cid:13) k k (cid:13) 2
k=1
( ≤e)
2K∥V(ℓ)∥2
(cid:13) (cid:13)U(ℓ−1)−U˜(ℓ−1)(cid:13) (cid:13)2
+
4K2 (cid:88)K
∥V(ℓ)∥2∥A(ℓ)∥2
(cid:13) (cid:13)U(ℓ−1)−U˜(ℓ−1)(cid:13) (cid:13)2
σ(cid:13) (cid:13) F r σ σ(cid:13) k k (cid:13) 2
k=1
+
4K (cid:88)K
∥V(ℓ)∥2∥A(ℓ)∥2
(cid:13) (cid:13)U(ℓ−1)−U˜(ℓ−1)(cid:13) (cid:13)2
r σ σ(cid:13) (cid:13) F
k=1
(cid:13) (cid:13)2 8K2 (cid:13) (cid:13)2
=2K∥V(ℓ)∥2 (cid:13)U(ℓ−1)−U˜(ℓ−1)(cid:13) + ∥V(ℓ)∥2∥A(ℓ)∥2 (cid:13)U(ℓ−1)−U˜(ℓ−1)(cid:13)
σ(cid:13) (cid:13) F r σ σ(cid:13) (cid:13) F
(cid:18) 4K∥A(ℓ)∥2(cid:19) (cid:13) (cid:13)2
=2K∥V(ℓ)∥2 1+ σ ·(cid:13)U(ℓ−1)−U˜(ℓ−1)(cid:13) ,
σ r (cid:13) (cid:13) F
where (a) follows from the fact that Clip is a contraction mapping, where in (b), U(ℓ−1) denotes the kth column of
k
U(ℓ−1) ∈ℜd×K, (c) follows from the fact ∥U˜(ℓ−1)∥2 ≤K and the fact that softmax is 1-Lipschitz, (d) follows from
σ
the fact that (x+z)2 ≤2(x−y)2+2(y+z)2, and (e) follows from the fact that ∥U(ℓ−1)U(ℓ−1)⊤∥ ≤K.
σ
note that again, unlike the standard deep neural network setting, the Lipschitz constant is not 1, but rather scales
with K, the context length. The intricacies of the softmax self-attention mechanism complicate the process of
arriving at a tight characterization. However, for the purposes of deriving an estimation error bound, this result
will suffice.
With this result in place, we now upper bound the expected squared difference between an output generated by a
transformerlayerwiththecorrectweightsA,V andanoutputgeneratedbyslightlyperturbedweightsA˜,V˜.
Lemma 40. For all d,r,K ∈Z and ϵ≥0, if V ∈ℜr×r consists of elements distributed iid N(0,1/r), A∈ℜr×r
++
consists of elements distributed N(0,1), for all ℓ∈[L],
U˜(ℓ)
=Clip(cid:16) V˜(ℓ)U(ℓ−1)At˜tn(ℓ)(cid:16) U(ℓ−1)(cid:17)(cid:17)
,
45where for all ℓ<L, E[∥V(ℓ)−V˜(ℓ)∥2]≤ϵ, E[∥A(ℓ)−A˜(ℓ)∥2]≤ϵ, and E[∥V(L)−V˜(L)∥2]≤ϵ, E[∥A(L)−A˜(L)∥2]≤
F F F F
rϵ/d, then
(cid:40)
(cid:104) (cid:105) 2K2(1+K)ϵ if ℓ<L
E ∥U(ℓ)−U˜(ℓ)∥2 ≤ .
F 2K(1+K)ϵ if ℓ=L
Proof.
(cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13)U(ℓ)−U˜(ℓ)(cid:13)
(cid:13) (cid:13)
F
≤E(cid:20)
sup
(cid:13)
(cid:13)V(ℓ)uAttn(ℓ)(u)−V˜(ℓ)uAt˜tn(ℓ)
(u)(cid:13) (cid:13)2(cid:21)
(cid:13) (cid:13)
u∈U F
 (cid:13) (cid:13) (cid:18) u⊤Au(cid:19) (cid:32) u⊤A˜u(cid:33)(cid:13) (cid:13)2
=E sup(cid:13) (cid:13)Vuσ √ r −V˜uσ √ r (cid:13) (cid:13) 
u∈U(cid:13) (cid:13)
F
(a)  (cid:13) (cid:13)(cid:16) (cid:17) (cid:32) u⊤A˜u(cid:33)(cid:13) (cid:13)2  (cid:13) (cid:13) (cid:32) (cid:18) u⊤Au(cid:19) (cid:32) u⊤A˜u(cid:33)(cid:33)(cid:13) (cid:13)2
≤ 2E sup(cid:13) (cid:13) V −V˜ uσ √ r (cid:13) (cid:13) +2E sup(cid:13) (cid:13)Vu σ √ r −σ √ r (cid:13) (cid:13) 
u∈U(cid:13) (cid:13) u∈U(cid:13) (cid:13)
F F
(b)  (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:32) u⊤A˜u(cid:33)(cid:13) (cid:13)2  (cid:13) (cid:13) (cid:18) u⊤Au(cid:19) (cid:32) u⊤A˜u(cid:33)(cid:13) (cid:13)2
≤ 2E  usu ∈p U(cid:13) (cid:13)V −V˜(cid:13) (cid:13) F (cid:13) (cid:13) (cid:13)uσ √ r (cid:13) (cid:13) (cid:13) +2E  usu ∈p U∥V∥2 F (cid:13) (cid:13) (cid:13)uσ √ r −uσ √ r (cid:13) (cid:13) (cid:13) 
F F
(c) (cid:13) (cid:13) (cid:32) u⊤A˜u(cid:33)(cid:13) (cid:13)2  (cid:13) (cid:13) (cid:18) u⊤Au(cid:19) (cid:32) u⊤A˜u(cid:33)(cid:13) (cid:13)2
≤ 2ϵ·sup∥u∥2 F ·(cid:13) (cid:13)σ √ r (cid:13) (cid:13) +2E ∥V∥2 F ·sup∥u∥2 F (cid:13) (cid:13)σ √ r −σ √ r (cid:13) (cid:13) 
u∈U (cid:13) (cid:13) u∈U (cid:13) (cid:13)
F F
 (cid:13) (cid:13)2
(d) (cid:13)u⊤Au u⊤A˜u(cid:13)
≤ 2ϵK2+2E Kr·sup(cid:13) (cid:13) √ r − √ r (cid:13) (cid:13) 
u∈U(cid:13) (cid:13)
F
 
K K
( =e) 2ϵK2+2K·E sup(cid:88)(cid:88)(cid:16) u⊤
i
(A−A˜)u j(cid:17)2

u∈U
i=1j=1
 
≤2ϵK2+2K·E
(cid:88)K (cid:88)K (cid:13)
(cid:13)
(cid:13)A−A˜(cid:13)
(cid:13)
(cid:13)2

F
i=1j=1
=2K2ϵ+2K3ϵ,
where (a) follows from the fact that ∥a+b∥2 ≤2∥a∥2 +2∥b∥2 for all matrices a,b, (b) follows from the fact that
F F F
(cid:104) (cid:105)
∥ab∥2 ≤ ∥a∥2∥b∥2 and ∥a∥2 ≤ ∥a∥2 for all matrices a,b, (c) follows from the fact that E ∥V −V˜∥2 = ϵ, (d)
F σ F σ F F
follows from the fact that E[∥V∥2]=r, and the fact that softmax is 1-Lipschitz, and where in (e), x denotes the
F i
ith column of matrix x.
46For ℓ=L,
(cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13)U(L)−U˜(L)(cid:13)
(cid:13) (cid:13)
F
≤E(cid:20)
sup
(cid:13)
(cid:13)V(L)uAttn(L)(u)[−1]−V˜(L)uAt˜tn(L)
(u)[−1](cid:13) (cid:13)2(cid:21)
(cid:13) (cid:13)
u∈U F
 (cid:13) (cid:13) (cid:18) u⊤A(L)u[−1](cid:19) (cid:32) u⊤A˜(L)u[−1](cid:33)(cid:13) (cid:13)2
=E sup(cid:13) (cid:13)V(L)uσ √ r −V˜(L)uσ √ r (cid:13) (cid:13) 
u∈U(cid:13) (cid:13)
F
 (cid:13) (cid:32) (cid:33)(cid:13)2
(a) (cid:13)(cid:16) (cid:17) u⊤A˜(L)u[−1] (cid:13)
≤ 2E sup(cid:13) (cid:13) V(L)−V˜(L) uσ √ r (cid:13) (cid:13) 
u∈U(cid:13) (cid:13)
F
 (cid:13) (cid:13) (cid:32) (cid:18) u⊤A(L)u[−1](cid:19) (cid:32) u⊤A˜(L)u[−1](cid:33)(cid:33)(cid:13) (cid:13)2
+2E sup(cid:13) (cid:13)V(L)u σ √ r −σ √ r (cid:13) (cid:13) 
u∈U(cid:13) (cid:13)
F
 (cid:13) (cid:32) (cid:33)(cid:13)2
(b) (cid:13) (cid:13)2 (cid:13) u⊤A˜(L)u[−1] (cid:13)
≤ 2E  usu ∈p U(cid:13) (cid:13)V(L)−V˜(L)(cid:13) (cid:13) F (cid:13) (cid:13) (cid:13)uσ √ r (cid:13) (cid:13) (cid:13) 
F
 (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:18) u⊤A(L)u[−1](cid:19) (cid:32) u⊤A˜(L)u[−1](cid:33)(cid:13) (cid:13)2
+2E  usu ∈p U(cid:13) (cid:13)V(L)(cid:13) (cid:13) F (cid:13) (cid:13) (cid:13)uσ √ r −uσ √ r (cid:13) (cid:13) (cid:13) 
F
(cid:13) (cid:32) (cid:33)(cid:13)2
(c) (cid:13) u⊤A˜(L)u[−1] (cid:13)
≤ 2ϵ·sup∥u∥2 ·(cid:13)σ √ (cid:13)
F (cid:13) r (cid:13)
u∈U (cid:13) (cid:13)
F
 (cid:13) (cid:13) (cid:18) u⊤A(L)u[−1](cid:19) (cid:32) u⊤A˜(L)u[−1](cid:33)(cid:13) (cid:13)2
+2E ∥V(L)∥2 F ·sup∥u∥2 F (cid:13) (cid:13)σ √ r −σ √ r (cid:13) (cid:13) 
u∈U (cid:13) (cid:13)
F
 (cid:13) (cid:13)2
(d) (cid:13)u⊤A(L)u[−1] u⊤A˜(L)u[−1](cid:13)
≤ 2ϵK+2E Kd·sup(cid:13) (cid:13) √ r − √ r (cid:13) (cid:13) 
u∈U(cid:13) (cid:13)
F
(cid:34) K (cid:35)
( =e) 2ϵK+ 2Kd ·E sup(cid:88)(cid:16) u⊤(A(L)−A˜(L))u[−1](cid:17)2
r i
u∈U
i=1
≤2ϵK+
2Kd ·E(cid:34) (cid:88)K (cid:13) (cid:13)A(L)−A˜(L)(cid:13) (cid:13)2(cid:35)
r (cid:13) (cid:13) F
i=1
=2Kϵ+2K2ϵ,
where (a) follows from the fact that ∥a+b∥2 ≤2∥a∥2 +2∥b∥2 for all matrices a,b, (b) follows from the fact that
F F F
(cid:104) (cid:105)
∥ab∥2 ≤ ∥a∥2∥b∥2 and ∥a∥2 ≤ ∥a∥2 for all matrices a,b, (c) follows from the fact that E ∥V −V˜∥2 = ϵ, (d)
F σ F σ F F
follows from the fact that E[∥V∥2]=d, and the fact that softmax is 1-Lipschitz, and where in (e), u denotes the
F i
ith column of matrix u.
With these preliminary results in place, we establish the following upper bound on the distortion of a single layer
ℓ of our transformer.
Lemma 41. (transformer layer distortion upper bound) For all d,r,t,K,L∈Z , 0≤ϵ, and ℓ∈[L], if
++
θ˜(ℓ) = (V˜(ℓ),A˜(ℓ))=(V(ℓ)+Z(ℓ),A(ℓ)+B(ℓ)),
where
(cid:40)
N(0,ϵ/r2) if ℓ<L
(Z(ℓ),B(ℓ))⊥(V(ℓ),A(ℓ)), Z(ℓ) i ∼id ; B(ℓ) i ∼id N(0,ϵ/r2),
N(0,ϵ/(rd) if ℓ=L
then
I(H ;θ(ℓ)|θ(ℓ+1:L),θ(1:ℓ−1),θ˜(ℓ),X ) ≤ ϵ(t+1)K(8K(1+16K))L−ℓ+1.
t+1 0
47Proof. We begin with the base cases ℓ=L
(cid:104) (cid:16) (cid:17)(cid:105)
I(X ;θ(L)|θ(1:L−1),θ˜(L),H )=E d P(X ∈·|θ(1:L),H ) ∥ P(X ∈·|θ(1:L−1),θ˜(L),H )
t+1 t KL t+1 t t+1 t
(cid:104) (cid:16) (cid:17)(cid:105)
≤E d P(X ∈·|θ(1:L),H ) ∥ P(X ∈·|θ(1:L−1),θ(L) ←θ˜(L),H )
KL t+1 t t+1 t
≤E(cid:104)(cid:13)
(cid:13)f θ(1:L)(X t:t−K+1)−f θ˜(L)(f θ(1:L−1)(X
t:t−K+1))(cid:13) (cid:13)2 2(cid:105)
≤2K(1+K)ϵ,
For ℓ<L, we have:
I(X ;θ(ℓ)|θ(1:ℓ−1),θ(ℓ+1:L),θ˜(ℓ),H )
t+1 t
(cid:104) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:105)
=E d P X ∈·|θ(1:L),H ∥ P X ∈·|θ(1:ℓ−1),θ(ℓ+1:L),θ˜(ℓ),H
KL t+1 t t+1 t
(a) (cid:104) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:105)
≤ E d P X ∈·|θ(1:L),H ∥ P X ∈·|θ(1:ℓ−1),θ(ℓ+1:L),θ(ℓ) ←θ˜(ℓ),H
KL t+1 t t+1 t
( ≤b) E(cid:104)(cid:13)
(cid:13)f θ(1:L)(X
t:t−K+1)−(cid:0)
f
θ(ℓ+1:L)
◦f
θ˜(ℓ)
◦f
θ(1:ℓ−1)(cid:1)
(X
t:t−K+1)(cid:13) (cid:13)2 2(cid:105)
( ≤c) E(cid:20) 2K∥V(L)∥2 σ(cid:18) 1+ 4K∥A r(L)∥2 σ(cid:19) ·(cid:13) (cid:13)f θ(1:L−1)(X t:t−K+1)−(cid:0) f
θ(ℓ+1:L−1)
◦f
θ˜(ℓ)
◦f θ(1:ℓ−1)(cid:1) (X t:t−K+1)(cid:13) (cid:13)2 2(cid:21)
( ≤d) E(cid:34)(cid:32) (cid:89)L 2K∥V(l)∥2 σ(cid:18) 1+ 4K∥A r(l)∥2 σ(cid:19)(cid:33) ·(cid:13) (cid:13)f θ(1:ℓ)(X t:t−K+1)−(cid:0) f
θ˜(ℓ)
◦f θ(1:ℓ−1)(cid:1) (X t:t−K+1)(cid:13) (cid:13)2 2(cid:35)
l=ℓ+1
(e)
(cid:34)(cid:32) (cid:89)L (cid:18) 4K∥A(l)∥2(cid:19)(cid:33) (cid:35)
≤ E 2K∥V(l)∥2 1+ σ ·2K2(1+K)ϵ
σ r
l=ℓ+1
(f)
≤ (8K(1+16K))L−ℓ·2K2(1+K)ϵ
≤ϵK(8K(1+16K))L−ℓ+1
where (a) follows from Lemma 57, (b) follows from Lemma 23, (c) and (d) follow from Lemma 39, (e) follows from
Lemma40andthefactthat(V(i),A(i) ⊥(V(j),A(j)))and(V(i),A(i) ⊥(V˜(j),A˜(j)))fori̸=j, and(f)followsfrom
the fact that A(L) ⊥V(L) and E(cid:2) ∥A(L)∥2(cid:3) ≤4r and E(cid:2) ∥V(L)∥2(cid:3) ≤4.
σ σ
With this inequality in place, we have the following.
t
I(H ;θ(ℓ)|θ(ℓ+1:L),θ(1:ℓ−1),θ˜(ℓ),X )=(cid:88) I(X ;θ(ℓ)|θ(ℓ+1:L),θ(1:ℓ−1),θ˜(ℓ),H )
t+1 0 k+1 k
k=0
t
( ≤a)(cid:88)
ϵK(8K(1+16K))L−ℓ+1
k=0
=ϵ(t+1)K(8K(1+16K))L−ℓ+1,
where (a) follows from the above result.
With Lemma 41 in place, we establish the final upper bound on the distortion for the full deep transformer.
Lemma 42. (transformer distortion upper bound) For all d,r,t,K,L∈Z , and ℓ∈[L], if
++
θ˜(ℓ) = (V˜(ℓ),A˜(ℓ))=(V(ℓ)+Z(ℓ),A(ℓ)+B(ℓ)),
where
(cid:40)
N(0,ϵ/r2) if ℓ<L
(Z(ℓ),B(ℓ))⊥(V(ℓ),A(ℓ)), Z(ℓ) i ∼id ; B(ℓ) i ∼id N(0,ϵ/r2),
N(0,ϵ/(rd) if ℓ=L
then
I(X ;θ(1:L)|θ˜(1:L),H ) ≤ ϵKL(t+1)(8K(1+16K))L.
t+1 t
48Proof.
L
I(X ;θ(1:L)|θ˜(1:L),H )=(cid:88) I(X ;θ(ℓ)|θ˜(1:L),θ(ℓ+1:L)H )
t+1 t t+1 t
ℓ=1
L
( ≤a)(cid:88)
I(H ;θ(ℓ)|θ(ℓ+1:L),θ(1:ℓ−1),θ˜(ℓ),X )
t+1 0
ℓ=1
(b)
≤ ϵKL(t+1)(8K(1+16K))L,
where (a) follows from Lemma 38, and (b) follows from Lemma 41.
Lemma43. (transformerrate-distortionupperbound)Foralld,r,t,K,L∈Z ,ifforallt,X isgenerated
++ t
by the transformer process, then
(cid:18) r·max{r,d}KLT(8K(1+16K))L(cid:19)
H (θ(1:L)) ≤ r·max{r,d}Lln 1+ .
ϵ,T ϵ
Proof. For all ℓ∈[L], let
θ˜(ℓ) = (V˜(ℓ),A˜(ℓ))=(V(ℓ)+Z(ℓ),A(ℓ)+B(ℓ)),
where
(cid:40)
N(0,ϵ′/r2) if ℓ<L
(Z(ℓ),B(ℓ))⊥(V(ℓ),A(ℓ)), Z(ℓ) i ∼id ; B(ℓ) i ∼id N(0,ϵ′/r2).
N(0,ϵ′/(rd) if ℓ=L
Let ϵ′ = ϵ . Then,
KLT(8K(1+16K))L
I(θ(1:L);θ˜(1:L))=h(θ˜(1:L))−h(θ˜(1:L)|θ(1:L))
L
=(cid:88) h(θ˜(ℓ))−h(θ˜(ℓ)|θ(ℓ))
ℓ=1
L
=(cid:88)(cid:16) h(V˜(ℓ))−h(V˜(ℓ)|V(ℓ))+h(A˜(ℓ))−h(A˜(ℓ)|A(ℓ))(cid:17)
ℓ=1
L (cid:88)−1(cid:18) r2 (cid:18) (cid:18) 1 ϵ′(cid:19)(cid:19) r2 (cid:18) (cid:18) ϵ′(cid:19)(cid:19)(cid:19)
≤ ln 2πe + − ln 2πe
2 r r2 2 r2
ℓ=1
(cid:18) rd (cid:18) (cid:18) 1 ϵ′ (cid:19)(cid:19) rd (cid:18) (cid:18) ϵ′ (cid:19)(cid:19)(cid:19)
+ ln 2πe + − ln 2πe
2 r rd 2 rd
(cid:88)L (cid:18) r2 (cid:18) (cid:18) ϵ′(cid:19)(cid:19) r2 (cid:18) (cid:18) ϵ′(cid:19)(cid:19)(cid:19)
+ ln 2πe 1+ − ln 2πe
2 r2 2 r2
ℓ=1
Lr·max{r,d}
(cid:18) max{r,d}KLT(8K(1+16K))L)(cid:19)
≤ ln 1+
2 ϵ
Lr2 (cid:18) r2KLT(8K(1+16K)))L(cid:19)
+ ln 1+
2 ϵ
(cid:18) r·max{r,d}KLT(8K(1+16K))L(cid:19)
≤Lr·max{r,d}ln 1+ ,
ϵ
We now verify that the distortion is ≤ϵ.
I(H T;θ(1:L)|θ˜(1:L))
=
1 T (cid:88)−1
I(X ;θ(1:L)|θ˜(1:L),H )
T T t+1 t
t=0
T−1
≤ 1 (cid:88) ϵ′KL(t+1)(8K(1+16K))L
T
t=0
≤ϵ′KLT (8K(1+16K))L
=ϵ.
497.3.3 Main Result
With the results established in the previous section, we now present the following result which upper bounds the
estimation error of an optimal agent which learns from data generated by the transformer process.
Theorem 44. (transformer estimation error upper bound) For all d,r,L,K,T ∈Z , if for all t∈Z , X
++ + t
is generated according to the transformer process, then
(cid:16) (cid:17)
r·max{r,d}L2ln(8eK(1+16K)) r·max{r,d}Lln 2K LT2
L ≤ + .
T T T
Proof.
(a) H (θ(1:L))
L ≤ inf ϵ,T +ϵ
T ϵ≥0 T
(cid:16) (cid:17)
r·max{r,d}Lln 1+
r·max{r,d}KLT(8K(1+16K))L
(b) ϵ
≤ inf +ϵ
ϵ≥0 T
(cid:18) (cid:19)
r·max{r,d}Lln 1+
r·max{r,d}KLT(8K(1+16K))L
(c) r·max{r,d}L2 r·max{r,d}L2
≤ T +
T T
(cid:16) (cid:17)
r·max{r,d}Lln
2KT2(8K L(1+16K))L
r·max{r,d}L2
≤ +
T T
(cid:16) (cid:17)
r·max{r,d}L2ln(8eK(1+16K)) r·max{r,d}Lln 2K LT2
= + ,
T T
where (a) follows from Theorem 15, (b) follows from Lemma 43, (c) follows by setting ϵ=r·max{r,d}L2/T
Notably, the result scales linearly in the product of the parameter count of the transformer and the depth of the
transformer. Unlike in the standard feed-forward network setting, we are unable to eliminate the quadratic depth
dependence. Thisisduetofactthatitisunknownwhethersoftmaxattentionobeystheconditionthattheexpected
squared lipschitz constant is ≤1. In this work we upper bounded this lipschitz value by (2K+8K2) which results
in the additional dependence on depth.
Summary
• In this section, we demonstrate that the analytic tools which we developed seamlessly transfer to the
setting in which data is no longer iid under some unknown distribution. To demonstrate concrete use
of our tools, we derive estimation error upper bounds for 2 settings involving learning from sequential
data: 1) a simple binary AR(K) process, 2) a transformer process.
• (binaryAR(K)estimationerrorupperbound)Forallembeddingdimensionsdandcontextlengths
K, if for all t∈Z ,X is generated according to the binary AR(K) process, then for all T,
+ t
(cid:18) (cid:18) (cid:19)(cid:19)
dK T
L ≤ 1+ln 1+ .
T 2T 4dK
• Consider a sequence which is generated by a transformer with vocabulary size d, embedding dimension
r, depth L, and context length K. The following result holds:
• (transformer estimation error upper bound) For all vocabulary sizes d, embedding dimensions r,
depths L, and context lengths K, if for all t∈Z , X is generated according to the transformer process,
+ t
50then for all T,
(cid:16) (cid:17)
r·max{r,d}L2ln(8eK(1+16K)) r·max{r,d}Lln 2K LT2
L ≤ + .
T T T
518 Meta-Learning
Abstract, Recent, work, has,
demonstrated, substantial, gains, …
Article, From, Wikipedia, the, free,
Bibliotik
Pile-CC encyclopedia, …
PubMed Central Arxiv
PG-19 BC2
Subtitles
Stack
Exchange Github import, torch, import, torch, .nn, as, nn,
PMA OpenWebText2 IRC EP
FreeLaw USPTO import, torch, .nn, .functional, as, F …
Phil NIH Wikipedia DM Math HN YT
Figure 7: The above diagram depicts the pre-training of large language models as a meta-learning problem. The
meta-parameters ψ specify a distribution over document types (tasks). Each document type (task) θ specifies
m
an autoregressive random process. Each document (X(m),X(m),X(m),...), is represented by a sequence of tokens
0 1 2
which is distributed according to the random process designated by the document type.
8.1 Data Generating Process
In meta-learning assume a further underlying structure in the data that we observe. Concretely, let the random
process(X(m) :m,t∈Z )denoteobservations. Foranyfixedm,(X(m),X(m),...)representsthedataassociated
t ++ 0 1
with a particular task indexed by m. In meta-learning, we assume that there exists a shared structure across the
tasks. Learning the structure will often accelerate learning on subsequent tasks. We assume that the sequence
(X(1) ,X(2) ,...) is exchangeable i.e. any finite permutation of the sequence does not alter the joint distribution.
0:∞ 0:∞
De Finetti’s theorem then states that there exists a latent random variable, which we will denote by ψ such that
conditioned on ψ, the above sequence is iid. Note that this iid assumption does not pertain to the observations
within a given task, but rather to the entire collections X(m) across tasks.
0:∞
This latent variable ψ, which we refer to as the meta-parameters, represents the aforementioned information which
issharedacross tasks. However,typicallyinmeta-learning,foranyparticulartask,thereexistsinformationbeyond
ψ which must be learned to produce accurate predictions. As a result, we let (θ ,θ ,...) be a sequence which is
1 2
alsoiidwhenconditionedonψ andweassumethatforallm,X(m) ⊥ψ|θ . Foreachtaskm,θ maysubsumethe
0:∞ m m
information contained in ψ and contains all other relevant information which can be learned from the intra-task
observations.
We again outline that, we do not make the assumption that the intra-task observations are iid when conditioned
on θ . This allows us to characterize more realistic data generating processes such as natural language for which
m
the “tasks” may represent document types and the “observations” the sequence of tokens which comprise of a
document of said type (Figure 7).
8.2 Meta-Learning Error
We define notions of error for meta-learning which directly parallel the definitions of section 3.3. For purposes
of analysis, we consider learning under a setting for which there exist M tasks and T observations per task. A
52learning algorithm produces for each (m,t)∈[M]×[T], a predictive distribution P of X(m) after observing the
m,t t+1
concatenated history which we denote by
(cid:16) (cid:17)
H = X(1),X(2),...,X(m−1),X(m) .
m,t 0:T 0:T 0:T 0:t
H consists of all observations from tasks 1,...,m−1 and up to the t-th observation of task m. We express our
m,t
meta-learning algorithm in terms of a function π for which P = π(H ). For all M,T ∈ Z , we measure the
m,t m,t ++
error realized by our predictions P in terms of the average cumulative expected log-loss:
m,t
M T−1
L =
1 (cid:88) (cid:88)
E
(cid:104)
−lnP
(cid:16) X(m)(cid:17)(cid:105)
.
M,T,π MT π m,t t+1
m=1 t=0
Since this error is not baselined at 0, we elect to study the following estimation error:
H(H |ψ,θ )
L = L − M,T 1:M .
M,T,π M,T,π MT
Note that while we introduce this new notation to facilitate exposition of meta-learning, the above notions of error
are subsumed by those established in section 3.3.
8.3 Achievable Error
We are again interested in characterizing the limits of performance via machine learning. As a result, it is prudent
to answer: For all (M,T), what π minimizes L ? As one would expect, the optimal algorithm assigns for all
M,T,π
(m,t), P =P(X ∈·|H ). We will denote this Bayesian posterior which we will denote as Pˆ .
m,t t+1 m,t m,t
Lemma 45. (Bayesian posterior is optimal) For all m,t∈Z ,
+
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
E −lnPˆ X(m) = min E −lnP X(m) .
m,t t+1 π m,t t+1
π
Proof.

Pˆ
(cid:16) X(m)(cid:17)
(cid:12)

E(cid:104) −lnP m,t(cid:16) X t( +m 1)(cid:17) |H m,t(cid:105) a =.s.E −lnPˆ m,t(cid:16) X t( +m 1)(cid:17) +ln Pm,t (X(t+ m1
))
(cid:12) (cid:12) (cid:12)H m,t
m,t t+1
(cid:104) (cid:105) (cid:16) (cid:17)
a =.s.E −lnPˆ (X(m))|H +d Pˆ ∥P .
m,t t+1 m,t KL m,t m,t
The result follows from the fact that KL-divergence is non-negative and the tower property.
Goingforward, wewillrestrictourattentiontotheoptimalachievableestimationerrorwhichwedenoteby:
L =
1 T (cid:88)−1 (cid:88)M E(cid:104)
−lnPˆ
(cid:16) X(m)(cid:17)(cid:105)
−
H(H M,T|ψ,θ 1,...,θ M)
.
M,T MT m,t t+1 MT
t=0 m=1
The process of learning should result in L vanishing to 0 as MT increases to ∞. We are interested in char-
M,T
acterizing the rate at which the estimation error decays to 0. In previous sections, we established that even if
the observations are not iid, we can observe error which decays linearly in the number of observations. In the
following section, we will establish general results which bound the estimation error of meta-learning and facilitate
the analysis of concrete problem instances.
8.4 General Theoretical Results
We begin with a general result which expresses the estimation error of meta-learning in terms of two very intuitive
quantities:
53Theorem 46. (estimation error) For all M,T ∈Z ,
+
L =
I(H M,T;ψ)
+
(cid:88)M I(X 0(m :T);θ m|ψ)
.
M,T MT MT
(cid:124) (cid:123)(cid:122) (cid:125) m=1(cid:124) (cid:123)(cid:122) (cid:125)
meta intra-task
estimation estimation
error error
Proof.
L =
1 (cid:88)M T (cid:88)−1 E(cid:104)
−lnPˆ
(X(m))(cid:105)
−
H(H M,T|ψ,θ 1,...,θ M)
M,T MT m,t t+1 MT
m=1 t=0
M T−1
= 1 (cid:88) (cid:88) I(X(m);ψ,θ |H )
MT t+1 m m,t
m=1 t=0
M
( =a) 1 (cid:88) I(X(m);ψ,θ |H )
MT 0:T m m−1,T
m=1
M M
( =b) 1 (cid:88) I(X(m);ψ|H )+ 1 (cid:88) I(X(m);θ |ψ,H )
MT 0:T m−1,T MT 0:T m m−1,T
m=1 m=1
M M
( =c) 1 (cid:88) I(X(m);ψ|H )+ 1 (cid:88) I(X(m);θ |ψ)
MT 0:T m−1,T MT 0:T m
m=1 m=1
( =d) I(H M,T;ψ)
+
(cid:88)M I(X 0(m :T);θ m|ψ)
,
MT MT
m=1
where (a),(b), and (d) follow from the chain rule of mutual information, and (c) follows from the meta-learning
conditional independence assumptions.
The meta-estimation error term refers to the error which is attributed to learning about the meta-parameters ψ.
Since all of the observations contain information about ψ, it is intuitive that the numerator would include the
mutual information between the full history H and the meta-parameters ψ. Likewise, it is also to be expected
M,T
that the error decays linearly in the product MT since each observation contributes information about ψ. The
second term consists of the error which is incurred from learning about the task specific parameters θ ,...,θ
1 M
conditioned on ψ. Note that due to the conditional iid assumptions, this term simplifies to I(X(m);θ |ψ)/T.
0:T m
Notably, this term decays linearly in T but not M since the data from other tasks is independent of θ(m) when
conditioned on ψ. Intuitively, this error captures the remaining information to be extracted for a particular task
beyond what is present in the meta-parameters.
While this result is useful for conceptual understanding, we require further tools to streamline theoretical anal-
ysis. We again leverage rate-distortion theory to establish suitable upper and lower bounds for the estimation
error of meta-learning. Concretely, we define two rate-distortion functions. The first characterizes the meta-
parameters:
H (ψ) = inf I(ψ;ψ˜),
ϵ,M,T
ψ˜∈Ψ˜
ϵ,M,T
where
(cid:40) (cid:41)
I(H ;ψ|ψ˜)
Ψ = ψ˜:ψ˜⊥H ; M,T ≤ϵ .
ϵ,M,T M,T MT
The second characterizes the intra-task parameters conditioned on ψ:
H (θ |ψ) = inf I(θ ;θ˜ |ψ),
ϵ,T m m m
θ˜ m∈Θ˜
ϵ,T
where
(cid:40) (cid:41)
(cid:12) I(X(m);θ |θ˜,ψ)
Θ˜ = θ˜:θ˜⊥H (cid:12)θ ; 0:T m ≤ϵ .
ϵ,T M,T(cid:12) m T
54With these definitions in place, we establish upper and lower bounds on the estimation error in terms of the above
rate-distortion functions.
Theorem 47. (rate-distortion estimation error bound) For all M,T ∈Z , and m∈{1,...,M},
+
H (ψ) H (θ |ψ)
L ≤ inf ϵ,M,T +ϵ + inf ϵ′,T m +ϵ′,
M,T ϵ≥0 MT ϵ′≥0 T
and
(cid:26)H
(ψ)
(cid:27) (cid:26)H
(θ |ψ)
(cid:27)
L ≥ supmin ϵ,M,T ,ϵ + supmin ϵ′,T m ,ϵ′ .
M,T MT T
ϵ≥0 ϵ′≥0
Proof. We begin by showing the upper bound:
I(H ;ψ) I(X(m);θ |ψ)
L = M,T + 0:T m
M,T MT T
I(H ;ψ,ψ˜) I(X(m);θ ,θ˜ |ψ)
= M,T + 0:T m m
MT T
I(H ;ψ˜) I(H ;ψ|ψ˜) I(X(m);θ ,θ˜ |ψ)
= M,T + M,T + 0:T m m
MT MT T
(a) I(ψ;ψ˜) I(H ;ψ|ψ˜) I(X(m);θ ,θ˜ |ψ)
≤ + M,T + 0:T m m
MT MT T
I(ψ;ψ˜) I(H ;ψ|ψ˜) I(X(m);θ˜ |ψ) I(X(m);θ |θ˜ ,ψ)
= + M,T + 0:T m + 0:T m m
MT MT T T
(b) I(ψ;ψ˜) I(H ;ψ|ψ˜) I(θ ;θ˜ |ψ) I(X(m);θ |θ˜ ,ψ)
≤ + M,T + m m + 0:T m m
MT MT T T
(c) H (ψ) H (θ˜ |ψ)
≤ ϵ,M,T +ϵ+ ϵ′,M,T m +ϵ′,
MT T
where(a)and(b)followfromthedataprocessinginequalityand(c)followsfromthedefinitionoftherate-distortion
functions. The upper bound follows from the fact that inequality (c) holds for all ϵ≥0.
We now prove the lower bound. Suppose that I(H ;ψ) < H (ψ) Let ψ˜ = H˜ ∈/ Ψ˜ where H˜ is
M,T ϵ,M,T M,T ϵ,M,T M,T
another history sampled in the same manner as H .
M,T
M T−1
I(H ;ψ)= (cid:88) (cid:88) I(X(m);ψ|H )
M,T t+1 m,t
m=1 t=0
M T−1
( ≥a) (cid:88) (cid:88) I(X(m);ψ|H˜ ,H )
t+1 M,T m,t
m=1 t=0
M T−1
= (cid:88) (cid:88) I(X(m);ψ|ψ˜,X(m),...,X(m))
t+1 1 t
m=1 t=0
(b)
≥ ϵMT,
where (a) follows from the fact that conditioning reduces entropy and that X(m) ⊥H˜ |(ψ,H ) and (b) follows
t+1 M,T m,t
from the fact that ψ˜∈/ Ψ˜ . Therefore, for all ϵ≥0, I(H ;ψ)≥min{H (ψ),ϵMT}.
ϵ,M,T M,T ϵ,M,T
SupposethatI(H(m);θ |ψ)<H (θ |ψ). Letθ˜ =D˜ ∈/ Θ˜ whereD˜ isanotherhistorysampledinthesame
T m ϵ,T m m m ϵ,T m
55manner as X(m).
0:T
T−1
I(X(m);θ |ψ)= (cid:88) I(X(m);θ |X(m),...,X(m),ψ)
0:T m t+1 m 1 t
t=0
T−1
( ≥a) (cid:88) I(X(m);θ |D˜ ,X(m),...,X(m),ψ)
t+1 m m 1 t
t=0
T−1
= (cid:88) I(X(m);θ |θ˜ ,H ,ψ)
t+1 m m m,t
t=0
(b)
≥ ϵT,
where (a) follows from the fact that conditioning reduces entropy and that X(m) ⊥ D˜ |(ψ,X(m),...,X(m)) and
t+1 m 1 t
(b) follows from the fact that θ˜ ∈/ Θ˜ . Therefore, for all ϵ ≥ 0, I(X(m);θ |ψ) ≥ min{H (θ ),ϵT}. The
m ϵ,T 0:T m ϵ,M,T m
lower bound follows as a result.
Theorem 47 establishes an intimate connection between the estimation error of meta learning and the aforemen-
tioned rate-distortion functions via upper and lower bounds. This result will allow us to streamline the analysis
of learning under various meta-learning problems. In the subsequent sections, we will study two such instances.
The first is a simple linear representation learning setting which is provided as exposition to acclimate the reader
to the styles of analysis required for meta-learning. The second setting involves a mixture of transformers which
resembles and provides insights about pre-training and in-context learning in LLMs.
8.5 Linear Representation Learning
Figure 8: The above diagram depicts the linear representation learning problem. The matrix ψ ∈ℜd×r represents
the information which is shared across the tasks. The intra-task information θ is the product ψξ(m), where
m
ξ(m) ∈ℜr. Theresultingvectorθ ispassedthroughasoftmaxtoproduceacategoricaldistributionoverdclasses.
m
Each corresponding sequence X(m),X(m),X(m),... is produce via iid sampling of the aforementioned categorical
0 1 2
distribution.
8.5.1 Data Generating Process
Weintroduceasimplelinearrepresentationlearningproblemasaconcreteexampleofmeta-learningtodemonstrate
our method of analysis. In this example, the intra-task observations are iid, but we begin with such an example
for simplicity and to demonstrate this as a special case of general meta-learning under our framework.
56For all d,r ∈Z , we let ψ :Ω(cid:55)→ℜd×r be distributed uniformly over the set of d×r matrices with orthonormal
++
columns. We assume that d ≫ r. For all m, let ξ(m) : Ω (cid:55)→ ℜr be distributed iid N(0,I /r). We let θ = ψξ(m).
r m
As for the observable data, for each (m,t), let X(m) be drawn as according to the following probability law:
t+1

1 w.p. σ(θ )
2
w.p.
σ(θm )1
X(m) = m 2 ,
t+1 . d..
w.p. σ(θ )
m d
where σ(θ m)
j
= eθm,j/(cid:80)d k=1eθm,k. For each task m, the algorithm is tasked with estimating a vector θ
m
from
sampled observations (X(m),X(m),...). By reasoning about data from previous tasks, the algorithm can estimate
1 2
ψ, which reduces the burden of estimating θ to just estimating ξ(m) for each task. This is significant given the
m
assumption that d≫r.
8.5.2 Theoretical Building Blocks
We begin by establishing several lemmas which streamline our analysis. We begin with the following upper bound
on the meta-estimation error term:
Lemma 48. (meta rate-distortion upper bound) For all d,r,M,T ∈Z , if for all (m,t)∈[M]×[T], X(m)
++ t
is generated according to the linear representation learning process, then
 
dr d
H M,T,ϵ(ψ) ≤
2MT
ln1+
r(cid:16)
e2ϵ rT
−1(cid:17).
Proof. Let ψ˜ = ψ +Z where Z ∈ ℜd×r is independent of ψ and consists of elements which are distributed iid
N(0,ϵ′) where ϵ′ =(e2ϵ rT −1)/d We begin by upper bounding the rate.
I(ψ;ψ˜) h(ψ˜)−h(ψ˜|ψ)
=
MT MT
(a) dr ln(cid:0) 2πe(cid:0) ϵ′+ 1(cid:1)(cid:1) − dr ln(2πeϵ′)
≤ 2 r 2
MT
drln(cid:0) 1+ 1 (cid:1)
= rϵ′
2MT
 
dr d
= ln1+
(cid:16)
(cid:17),
2MT r e2ϵ rT −1
where(a)followsfromthemaximumdifferentialentropyofarandomvariableoffixedvariancebeingupperbounded
by a Gaussian random variable.
57We now upper bound the distortion.
I(H ;ψ|ψ˜) H(H |ψ˜)−H(H |ψ)
M,T = M,T M,T
MT MT
(cid:80)M H(X(m)|ψ˜,H )−H(X(m)|ψ,H )
= m=1 0:T m−1,T 0:T m−1,T
MT
H(X(1)|ψ˜)−H(X(1)|ψ)
≤ 0:T 0:T
T
I(X(1);ψ|ψ˜)
= 0:T
T
I(θ ;ψ|ψ˜)
≤ 1
T
(cid:104) (cid:105)
E d (P(θ ∈·|ψ)∥P(θ ∈·|ψ˜))
KL 1 1
=
n
(cid:104) (cid:105)
E d (P(θ ∈·|ψ)∥P(θ ·|ψ ←ψ˜))
(a) KL 1 ∈
≤
T
(cid:104) (cid:105)
E d (lim P(θ ∈·|ψ)∥lim P(θ |ψ ←ψ˜))
KL δ→0 δ δ→0 δ
≤
T
( =b) T1 E
 δli →m
01 2ln (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)δ δI Id+
+
ψ ψ˜ψ ψr˜⊤ ⊤(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12) −d+Tr (cid:32)
δI d+
ψ˜ψ r˜⊤(cid:33)−1(cid:18)
δI d+
ψψ r⊤(cid:19) 

(cid:12) d r (cid:12)
 (cid:12)
(cid:12)δI +
π˜ψ˜⊤(cid:12) (cid:12)
( ≤c) 1 E lim 1 ln(cid:12)(cid:12) d r (cid:12) (cid:12)
T δ→02 (cid:12)δI + ψψ⊤(cid:12)
(cid:12) d r (cid:12)
 
|δI
|·(cid:12)
(cid:12)I +
ψ˜⊤ψ˜(cid:12) (cid:12)
( =d) 1 E lim 1 ln d (cid:12) (cid:12) r rδ (cid:12) (cid:12)
T δ→02 |δI |·(cid:12)I + ψ⊤ψ(cid:12)
d (cid:12) r rδ (cid:12)
 (cid:12)
(cid:12)I +
ψ˜⊤ψ˜(cid:12) (cid:12)
= T1 E  δli →m 01 2ln(cid:12) (cid:12) (cid:12)r
I r+
r rIrδ δ(cid:12) (cid:12)(cid:12) 
(cid:12)
(cid:12)
E[ψ˜⊤ψ˜](cid:12) (cid:12)
(e) 1
(cid:12) (cid:12)I r+
rδ
(cid:12)
(cid:12)
≤ δli →m 02T ln  (cid:12) (cid:12)I r+ rIr δ(cid:12) (cid:12)  
(cid:12) (cid:12)
1
(cid:12)
(cid:12) (cid:12)I r+
E[Ir+ rδdϵ′Ir](cid:12)
(cid:12)
(cid:12)
= δli →m 02T ln  (cid:12) (cid:12)I r+ rIr δ(cid:12) (cid:12)  
r
(cid:32)
1+
1+dϵ′(cid:33)
= lim ln rδ
δ→02T 1+ 1
rδ
r
= ln(1+dϵ)
2T
=ϵ,
where (a), (b) follows from continuity of the KL-divergence between two multivariate normal distributions w.r.t
the covariance matrix, (c) follows from the fact that the trace term is upper bounded by d, (d) follows from the
matrix determinant lemma, ϵ= 1, and (e) follows from Jensen’s inequality. The result follows.
m
Recall that for meta-learning, the total estimation error is upper bounded by the sum of the meta rate-distortion
58function and the intra-task rate-distortion function. The following result establishes an upper bound on the intra-
task rate-distortion function.
Lemma 49. (intra-task rate-distortion upper bound) For all r,T ∈Z ,
++
(cid:18) (cid:19)
r 1
H (θ |ψ) ≤ ln 1+ .
T,ϵ m 2 ϵ
Proof. Let ξ˜=ξ(m)+Z where Z ⊥ξ(m) and Z ∼N(0,ϵI /r). Let θ˜=ψξ˜. We begin by upper bounding the rate.
r
I(θ ;θ˜|ψ)≤I(ξ;ξ˜|ψ)
m
=h(ξ˜|ψ)−h(ξ˜|ψ,ξ)
=h(ξ˜)−h(ξ˜|ξ)
=h(ξ˜)−h(Z)
(cid:18) (cid:19) (cid:18) (cid:19)
r ϵ 1 r 2πeϵ
= ln 2πe( + ) − ln
2 r r 2 r
(cid:18) (cid:19)
r 1
= ln 1+ .
2 ϵ
We now upper bound the distortion:
I(X(m);θ |θ˜,ψ) I(X(m);θ |θ˜,ψ)
0:T m ≤ 0:T m
T T
≤I(X(m);θ |θ˜)
1 m
(cid:104) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:105)
=E d P X(m) ∈·|θ ∥P X(m) ∈·|θ˜
KL 1 m 1
(a) (cid:104) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:105)
≤ E d P X(m) ∈·|θ ∥P X(m) ∈·|θ ←θ˜
KL 1 m 1 m
(b (cid:104) (cid:105)
≤E ∥θ˜−θ ∥2
m 2
(cid:20)(cid:16) (cid:17)⊤ (cid:16) (cid:17)(cid:21)
=E ξ(m)−ξ˜ ψ⊤ψ ξ(m)−ξ˜
(cid:20)(cid:16) (cid:17)⊤(cid:16) (cid:17)(cid:21)
=E ξ(m)−ξ˜ ξ(m)−ξ˜
=E(cid:2) Z⊤Z(cid:3)
=ϵ
where (a) follows from Lemma 1, and (b) follows from Lemma 23.
With these two results in place, we present the main result which upper bounds the estimation error of linear
representation learning.
8.5.3 Main Result
With the rate-distortion upper bounds established from the previous section, we present the following reslt which
upper bounds the estimation error of linear representation learning.
Theorem 50. (linear representation learning estimation error bound) For all d,r,M,T ∈Z , if for all
++
(m,t), X(m) is generated according to the linear representation learning process, then
t
drln(cid:0) e(cid:0)
1+
M(cid:1)(cid:1) rln(cid:0) e(cid:0)
1+
2T(cid:1)(cid:1)
L ≤ r + r .
M,T 2MT 2T
59Proof.
(a) H (ψ) H (θ |ψ)
L ≤ inf M,Tϵ +ϵ+ inf T,ϵ′ m +ϵ′
M,T ϵ≥0 MT ϵ′≥0 T
 
(cid:18) (cid:19)
(b) dr d r 1
≤ inf ln1+
(cid:16)
(cid:17)+ϵ+ inf ln 1+ +ϵ′
ϵ≥0 2MT r e2ϵ rT −1 ϵ′≥02n rϵ′
 
(c) dr d dr rln(1+ 2T) r
≤ ln1+
(cid:16)
(cid:17)+ + r +
2MT r eMdϵ −1 2MT 2T 2T
drln(cid:0) 1+ M(cid:1) dr rln(1+ 2T) r
≤ r + + r +
2MT 2MT 2T 2T
drln(cid:0) e(cid:0)
1+
M(cid:1)(cid:1) rln(cid:0) e(cid:0)
1+
2T(cid:1)(cid:1)
= r + r ,
2MT 2T
where (a) follows from Theorem 46, (b) follows directly from Lemmas 48 and 49, and (c) follows from setting
ϵ= dr and ϵ′ = r .
2MT 2T
The first term describes the error which is incurred in the process of estimating the meta-parameters ψ. Notably,
this term is linear in dr, the parameter count of ψ and decays linearly in MT, the total number of observations.
This is both intuitive and desirable since each observation ought to provide information about ψ. Meanwhile,
the second term describes the error which is incurred in the process of estimating the intra-task parameters θ
m
conditionedonψ. Inourprobleminstance,θ =ψξ forξ ∈ℜr,sotheremaininguncertaintyinθ whenconditioning
on ψ is simply ξ. Therefore, the intra-task error is linear in r, the parameter count of ξ and decays linearly in T,
the number of observations pertinent to task m.
In the following section, we will characterize the learning performance under a much more complex process which
resembles pre-training and in-context learning in LLMs.
8.6 In-Context Learning
Abstract, Recent, work, has demonstrated
Article, From, Wikipedia, the free
import, torch, import, torch .nn
Figure9: Theabovediagramdepictsthein-contextlearningproblem. ψ designatesamixtureofmanytransformer
models, each which is responsible for producing documents of a particular type. Each document is generated
by sampling a transformer model θ from the mixture ψ, and generating a sequence (X(m),X(m),X(m),...)
m 0 1 2
autoregressively.
608.6.1 Data Generating Process
In the ICL process, for all documents m, we let its tokens (X(m),X(m),...) be a sequence in {1,...,d}, where d
1 2
denotes the size of the vocabulary. Each of the d outcomes is associated with a known embedding vector which we
denote as Φ for j ∈{1,...,d}. We assume that for all j, ∥Φ ∥ =1. For brevity of notation, we let ϕ(m) =Φ
j j 2 t X(m)
t
i.e. the embedding associated with token X(m).
t
Each document is generated by a transformer model which is sampled iid from a mixture. We assume that
sampling is performed according to a categorical distribution parameterized by α with prior distribution P(α ∈
·)=Dirichlet(N,[R/N,...,R/N]) for a scale parameter R≪N. Realizations of this prior distributions are hence
probabilitymassfunctionontheclasses{1,2,...,N}. IfwesampleM timesfromthispmf, werecoveraDirichlet-
multinomial for which the expected number of unique classes grows linearly in R and only logarithmically in M.
This may be representative of the documents which comprise of an internet corpus. The vast majority will belong
to one of several prominent classes such as academic literature, articles, question-answer, etc. However, there may
be several documents which are rather spurious in relation to the remainder of the corpus. Therefore, we permit
the number of document types N to potentially be exponentially large, but we assume that the scale parameter R
remains modest.
EachoftheN elementsofthemixturecorrespondstoatransformernetworkasoutlinedinSection7.3. LetK denote
the context lengths of the transformers, L denote their depths, and r their attention dimensions. Let ψ ,...,ψ
1 N
denote the weights of the transformer mixture elements. As a result, the meta-parameters ψ =(α,ψ ,...,ψ ) i.e.
1 N
the mixture sampling weights and the mixture model weights.
Forallt,thegenerationoftokenX(m)willdependonthesampledmixtureindexi ∈{1,...,N},thecorresponding
t+1 m
transformerweights ψ andthepreviousK tokens X(m) ,...,X(m). Weuse U(m) todenotetheoutputoflayer
im t−K+1 t t,ℓ
ℓ at time t for document m. For all m,t, we let (U(m) = ϕ ) be the embeddings associated with the past
t,0 t−K+1:t
K tokens. For ℓ > 0, we let U(m) denote the output of layer ℓ of the transformer with input U(m). For all
t,ℓ t,0
t≤T,i<L,m≤M, let
(cid:16) (cid:17)
(cid:32) U(m)⊤A(m)U(m) (cid:33)
Attn(ℓ) U(m) =Softmax t,ℓ−1 √ℓ t,ℓ−1
t,ℓ−1 r
denotetheattentionmatrixoflayerℓfordocumentmwherethesoftmaxfunctionisappliedelementwisealongthe
columns. ThematrixA(m) ∈ℜr×r canbeinterpretedastheproductofthekeyandquerymatricesandweassume
ℓ
that the elements of the matrices A(m) are distributed iid N(0,1).
ℓ
Subsequently, we let
(cid:16) (cid:16) (cid:17)(cid:17)
U(m) =Clip V(m)U(m) Attn(ℓ) U(m) ,
t,ℓ ℓ t,ℓ−1 t,ℓ−1
where Clip ensures that each column of the matrix input has L norm at most 1. The matrix V(m) resembles the
2 ℓ
value matrix and we assume that the elements of V(m) are distributed iid N(0,1/d).
ℓ
Finally, the next token is generated via sampling from the softmax of the final layer:
(cid:16) (cid:17)
X(m) ∼Softmax U(m)[−1] ,
t+1 t,L
where U(m)[−1] denotes the right-most column of U(m). At each layer ℓ, the parameters consist of the matrices
t,L t,L
A(m),V(m). Asaresult, theintra-taskparametersconsistofθ =(i ,ψ )i.e. themixtureindexandtheweights
ℓ ℓ m m im
of the corresponding transformer model.
8.6.2 Theoretical Building Blocks
To obtain the tightest bounds for this problem instance, instead of consider two separate rate-distortion functions
as in Theorem 47, we must instead analyze the combined rate-distortion function defined below:
H (ψ,θ ) = inf I(ψ,θ ;θ˜),
M,T,ϵ 1:M 1:M
θ˜∈Θ˜
M,T,ϵ
61where
(cid:40) (cid:41)
I(H ;ψ,θ |θ˜)
Θ˜ = θ˜:θ˜⊥H |θ ; M,T 1:M ≤ϵ .
M,T,ϵ M,T 1:M MT
Under this rate-distortion function, we obtain the following bounds on estimation error:
Theorem 51. (rate-distortion estimation error bound 2) For all M,T ∈Z ,
+
H (ψ,θ )
L ≤ inf M,T,ϵ 1:M +ϵ,
M,T ϵ≥0 MT
and
(cid:26)H
(ψ,θ )
(cid:27)
L ≥ sup min M,T,ϵ 1:M , ϵ .
M,T MT
ϵ≥0
Proof. We begin by establishing the upper bound.
I(H ;ψ,θ )
L = M,T 1:M
M,T MT
I(H ;ψ,θ ,θ˜)
= inf M,T 1:M
θ˜∈Θ˜
M,T,ϵ
MT
I(H ;θ˜) I(H ;θ |θ˜)
= inf M,T + M,T 1:M
θ˜∈Θ˜
M,T,ϵ
MT MT
H (ψ,θ )
≤ M,T,ϵ 1:M +ϵ.
MT
We now establish the lower bound. Suppose that I(H ;ψ,θ ) < H (ψ,θ ). Let θ˜ = H˜ ∈/ Θ˜
M,T 1:M M,T,ϵ 1:M M,T M,T,ϵ
where H˜ is another independent history sampled in the same manner as H .
M,T M,T
I(H ;ψ,θ )=H(H )−H(H |ψ,θ )
M,T 1:M M,T M,T 1:M
( =a)H(H )−H(H |ψ,θ ,θ˜)
M,T M,T 1:M
(b)
≥ H(H |θ˜)−H(H |ψ,θ ,θ˜)
M,T M,T 1:M
=I(H ;ψ,θ |θ˜)
M,T 1:M
(c)
≥ ϵMT
where (a) follows from conditional independence assumptions, (b) follows from the fact that conditioning re-
duces entropy, and c) follows from the fact that θ˜ ∈/ Θ˜ . Therefore, for all ϵ ≥ 0, I(H ;ψ,θ ) ≥
M,T,ϵ M,T 1:M
min{H (ψ,θ ),ϵMT}. The result follows.
M,T,ϵ 1:M
With this rate-distortion bound in place, we established that in order to bound the estimation error of ICL, we
simply must bound the rate-distortion function. In the following result, we establish an upper bound on the
rate-distortion function for the ICL problem.
Lemma 52. (in-context learning rate-distortion upper bound) For all d,r,K,L,M,T, if for all (m,t) ∈
[M]×[T], X(m) is generated by the ICL process, then
t
(cid:18) M(cid:19) (cid:18) r·max{r,d}KLT(8K(1+16K))L(cid:19)
H (ψ,θ ) ≤ Mln(N)+Rln 1+ ·r·max{r,d}Lln 1+ .
M,T,ϵ 1:M R ϵ
Proof. FortheICLdatageneratingprocess,wewillconstructacompressionwhichincrementallybuildsupelements
of the mixture. Concretely, let θ˜ = (i ,i ,...,i ,ψ˜) where ψ = {ψ˜ : j ∈ {i ,i ,...,i }} and ψ˜ represents a
1 2 M j 1 2 M j
noisy version of ψ . We will use I to denote the set of unique indices from {i ,i ,...,i }.
j M 1 2 M
Let ψ˜ =(V˜ ,A˜ :ℓ∈[L]) where
j j,ℓ j,ℓ
(cid:16) (cid:17)
V˜ ,A˜ =(V +Z , A +B )
j,ℓ j,ℓ j,ℓ j,ℓ j,ℓ j,ℓ
62(cid:40)
N(0,ϵ′/r2) if ℓ<L
(Z ,B )⊥(V ,A ), Z i ∼id ; B i ∼id N(0,ϵ′/r2),
j,ℓ j,ℓ j,ℓ j,ℓ j,ℓ N(0,ϵ′/(rd) if ℓ=L j,ℓ
and ϵ′ = ϵ .
KLT(8K(1+16K))L
We begin by upper bounding the rate.
I(ψ,θ ;θ˜)=I(ψ,θ ;(i ,...,i ),ψ˜)
1:M 1:M 1 M
=I(ψ,(i ,...,i );(i ,...,i ),ψ˜)
1 M 1 M
=H(i ,...,i )+I(ψ;ψ˜|(i ,...,i ))
1 M 1 M
(cid:34) N (cid:35)
≤Mln(N)+E (cid:88) 1 ·I(ψ ;ψ˜)
[i∈IM] i i
i=1
(a)
(cid:18) M(cid:19) (cid:18) r·max{r,d}KLT(8K(1+16K))L(cid:19)
≤ Mln(N)+Rln 1+ ·r·max{r,d}Lln 1+ ,
R ϵ
where (a) follows from Lemmas 33 and 43.
We now upper bound the distortion:
I(H ;ψ,θ |θ˜) I(H ;θ |θ˜)
M,T 1:M = M,T 1:M
MT MT
=
(cid:88)M I(X 0(m :T);θ 1:M|θ˜,H m−1,T)
MT
m=1
I(X(1);θ |θ˜)
≤ 0:T 1:M
T
I(X(1);θ |ψ˜ )
= 0:T 1 i1
T
(a)
≤ ϵ,
where (a) follows from Lemma 43.
8.6.3 Main Result
With the rate-distortion results established from the previous section, we establish the following upper bound on
the estimation error of in-context learning.
Theorem 53. (in-context learning estimation error upper bound) For all d,r,L,K,R,N,M,T ∈Z , if
++
for all (m,t)∈[M]×[T], X(m) is generated according to the ICL process, then
t
L ≤
rmax{r,d}RL2ln(cid:0) 1+ M R(cid:1) ln(8Ke(1+16K))
+
rmax{r,d}RLln(cid:0) 1+ M R(cid:1) ln(cid:16) 2KM LT2(cid:17)
+
ln(N)
.
M,T MT MT T
Proof.
H (ψ,θ )
L ≤ inf M,T,ϵ 1:M +ϵ
M,T ϵ≥0 MT
rmax{r,d}RLln(cid:0)
1+
M(cid:1) ln(cid:16)
1+
rmax{r,d}KLT(8K(1+16K))L(cid:17)
ln(N) R ϵ
≤ inf + +ϵ
ϵ≥0 T MT
ln(N) rmax{r,d}RLln(cid:0) 1+ M R(cid:1) ln(cid:16) 1+ KMT2(8K L(1+16K))L(cid:17) rmax{r,d}RL2
≤ + +
T MT MT
ln(N) rmax{r,d}RLln(cid:0) 1+ M R(cid:1) ln(cid:16) 2KMT2(8K L(1+16K))L(cid:17) rmax{r,d}RL2
≤ + +
T MT MT
≤
ln(N)
+
rmax{r,d}RL2ln(cid:0) 1+ M R(cid:1) ln(8Ke(1+16K))
+
rmax{r,d}RLln(cid:0) 1+ M R(cid:1) ln(cid:16) 2KM LT2(cid:17)
T MT MT
63Notably,thetermwhichdecayslinearlyinT indicatesthatoncethemixtureislearned,theonlyinformationwhich
must be deduced from each document is its mixture index. As a result, the error grows logarithmically in N (the
size of the mixture) and decays linearly in T (the length of the intra-task sequence). Notably, the first term scales
linearly in the product of the parameter count and depth of the transformer model. Furthermore it also scales
linearly in Rln(1+M/R), the expected number of unique mixture elements from M documents. Notably, this
component of the error decays linearly in MT, so it will decay both as the number of documents and the number
of tokens per document grow. As a result, for a pre-training dataset with sufficiently large M, this portion of the
error could become negligible, which would indicate that the loss incurred by ICL would eventually be ≈ln(N)/T.
This is intuitive as once the relevant mixture elements have been learned, the algorithm just has to disambiguate
which index the next document belongs to.
Summary
• Meta-learning consists of a random process (X(m) : m,t ∈ Z ) of observations. For any fixed m,
t ++
(X(m),X(m),...) represents the data associated with a particular task indexed by m.
0 1
• The meta-parameters ψ, represent information which is shared across tasks.
• The intra-task parameters (θ ,θ ,...) are iid when conditioned on ψ and for all m, X(m) ⊥ψ|θ .
1 2 0:∞ m
• A meta-learning algorithm produces for each (m,t) ∈ [M]×[T], a predictive distribution P of X(m)
m,t t+1
after observing the concatenated history which we denote by
(cid:16) (cid:17)
H = X(1),X(2),...,X(m−1),X(m) .
m,t 0:T 0:T 0:T 0:t
• For all M,T ∈ Z , we measure the error realized by our predictions P in terms of the average
++ m,t
cumulative expected log-loss:
M T−1
L =
1 (cid:88) (cid:88)
E
(cid:104)
−lnP
(cid:16) X(m)(cid:17)(cid:105)
.
M,T,π MT π m,t t+1
m=1 t=0
Since this error is not baselined at 0, we elect to study the following estimation error:
H(H |ψ,θ )
L = L − M,T 1:M .
M,T,π M,T,π MT
• (Bayesian posterior is optimal) For all m,t∈Z ,
+
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
E −lnPˆ X(m) = min E −lnP X(m) .
m,t t+1 π m,t t+1
π
• (estimation error) For all M,T ∈Z ,
+
L =
I(H M,T;ψ)
+
(cid:88)M I(X 0(m :T);θ m|ψ)
.
M,T MT MT
(cid:124) (cid:123)(cid:122) (cid:125) m=1(cid:124) (cid:123)(cid:122) (cid:125)
meta intra-task
estimation estimation
error error
• We define the rate-distortion function for meta-parameters ψ as
H (ψ) = inf I(ψ;ψ˜),
ϵ,M,T
ψ˜∈Ψ˜
ϵ,M,T
64where
(cid:40) (cid:41)
I(H ;ψ|ψ˜)
Ψ = ψ˜:ψ˜⊥H ; M,T ≤ϵ .
ϵ,M,T M,T MT
• We define the rate-distortion function for intra-task parameters θ conditioned on ψ as
m
H (θ |ψ) = inf I(θ ;θ˜ |ψ),
ϵ,T m m m
θ˜ m∈Θ˜
ϵ,T
where
(cid:40) (cid:41)
(cid:12) I(X(m);θ |θ˜,ψ)
Θ˜ = θ˜:θ˜⊥H (cid:12)θ ; 0:T m ≤ϵ .
ϵ,T M,T(cid:12) m T
• (rate-distortion estimation error bound) For all M,T ∈Z , and m∈{1,...,M},
+
H (ψ) H (θ |ψ)
L ≤ inf ϵ,M,T +ϵ + inf ϵ′,T m +ϵ′,
M,T ϵ≥0 MT ϵ′≥0 T
and
(cid:26)H
(ψ)
(cid:27) (cid:26)H
(θ |ψ)
(cid:27)
L ≥ supmin ϵ,M,T ,ϵ + supmin ϵ′,T m ,ϵ′ .
M,T MT T
ϵ≥0 ϵ′≥0
• (linear representation learning estimation error bound)Foralld,r,M,T ∈Z , ifforall(m,t),
++
X(m) is generated according to the linear representation learning process, then
t
drln(cid:0) e(cid:0)
1+
M(cid:1)(cid:1) rln(cid:0) e(cid:0)
1+
2T(cid:1)(cid:1)
L ≤ r + r .
M,T 2MT 2T
• We define the combined rate-distortion function of (ϕ,θ ) as
1:M
H (ψ,θ ) = inf I(ψ,θ ;θ˜),
M,T,ϵ 1:M 1:M
θ˜∈Θ˜
M,T,ϵ
where
(cid:40) (cid:41)
I(H ;ψ,θ |θ˜)
Θ˜ = θ˜:θ˜⊥H |θ ; M,T 1:M ≤ϵ .
M,T,ϵ M,T 1:M MT
• (rate-distortion estimation error bound) For all M,T ∈Z ,
+
H (ψ,θ )
L ≤ inf M,T,ϵ 1:M +ϵ,
M,T ϵ≥0 MT
and
(cid:26)H
(ψ,θ )
(cid:27)
L ≥ sup min M,T,ϵ 1:M , ϵ .
M,T MT
ϵ≥0
• (in-context learning estimation error upper bound)Forallvocabularysizesd,embeddingdimen-
sions r, depths L, context lengths K, scale parameters R, mixture size N, and M,T ∈ Z , if for all
++
(m,t)∈[M]×[T], X(m) is generated according to the ICL process, then
t
rmax{r,d}RL2ln(cid:0)
1+
M(cid:1)
ln(8Ke(1+16K))
L ≤ R
M,T MT
rmax{r,d}RLln(cid:0)
1+
M(cid:1) ln(cid:16) 2KMT2(cid:17)
R L ln(N)
+ + .
MT T
659 Misspecification
All of the results presented in this text have involved bounding the estimation error of an optimal agent with a
correctly specified prior distribution. While in general little can be said about the performance of an arbitrarily
suboptimalagent,inthissection,wefirstprovidesomegeneraltheoreticalresultswhichmayallowonetostudythe
error of a suboptimal agent under certain problem settings. Afterwards, we apply the results to study the impact
of misspecification in the Bayesian linear regression setting.
9.1 General Theoretical Results
We begin with the following general result which exactly characterizes the loss incurred by an arbitrary prediction
P˜ which may depend on the previous data H . Recall that P∗(·) denotes P(Y ∈ ·|θ,H ) and Pˆ(·) denotes
t t t t+1 t t
P(Y ∈·|H ).
t+1 t
Theorem 54. (loss of an arbitrary algorithm) For all T ∈Z , if for all t∈[T], P˜ is a predictive distribution
+ t
which may depend on the previous data H , then
t
T1 T (cid:88)−1 E(cid:104) d KL(cid:16) P t∗(·)∥P˜ t(·)(cid:17)(cid:105) = I(H TT;θ) + T1 T (cid:88)−1 E(cid:104) d KL(cid:16) Pˆ t(·)(cid:13) (cid:13)P˜ t(·)(cid:17)(cid:105) .
t=0 t=0
(cid:124) (cid:123)(cid:122) (cid:125)
misspecification error
Proof.
1 T (cid:88)−1 E(cid:104)
d
(cid:16) P∗(·)∥P˜(·)(cid:17)(cid:105)
=
1 T (cid:88)−1 E(cid:20) lnP t∗(Y t+1)(cid:21)
T KL t t T P˜(Y )
t=0 t=0 t t+1
=
1 T (cid:88)−1 E(cid:34) lnP t∗(Y t+1)(cid:35) +E(cid:34) lnPˆ t(Y t+1)(cid:35)
T Pˆ(Y ) P˜(Y )
t=0 t t+1 t t+1
= I(H TT;θ) + T1 T (cid:88)−1 E(cid:104) d KL(cid:16) Pˆ t(·)(cid:13) (cid:13)P˜ t(·)(cid:17)(cid:105) .
t=0
This result states that for a suboptimal prediction P˜, the additional error incurred (misspecification error) is
t
characterized by the KL divergence between the optimal prediction P˜ = P(Y ∈ ·|H ) and P˜. We note that
t t+1 t t
under many circumstances, this KL divergence may be difficult to study. However, if we restrict our attention to
predictions which are suboptimal due to a misspecified prior distribution P˜(θ ∈ ·), we have the following upper
bound:
Theorem 55. (misspecified prior error bound) For all T ∈ Z , if for all t ∈ [T], P˜(·) is the Bayesian
++ t
posterior of Y conditioned on H under the prior P˜(θ ∈·), then
t+1 t
(cid:16) (cid:17)
T−1 d P(θ ∈·)∥P˜(θ ∈·)
T1 (cid:88) E(cid:104) d KL(cid:16) Pˆ t(·) (cid:13) (cid:13) P˜ t(·)(cid:17)(cid:105) ≤ KL
T
.
t=0
Proof.
T−1
T1 (cid:88) E(cid:104) d KL(cid:16) Pˆ t(·) (cid:13) (cid:13) P˜ t(·)(cid:17)(cid:105)( =a) T1 d KL(cid:16) P(H T ∈·) (cid:13) (cid:13) P˜(H T ∈·)(cid:17)
t=0
(cid:16) (cid:17)
d P(θ ∈·) ∥ P˜(θ ∈·)
(b) KL
≤ ,
T
where (a) follows from the chain rule of KL divergence and (b) follows from the data processing inequality of KL
Divergence.
66We note that in instances for which the numerator of the RHS is finite, this provides strong guarantees. This
stipulates that if P˜ never assigns probability 0 to an event A for which P(θ ∈ A) > 0, then the misspecification
error will decay linearly in T. However, it may often be the case P˜(θ ∈ ·) may only have support on a subset of
the support of P(θ ∈·). In such instances, our upper bound is vacuous as the KL divergence will be ∞.
For such instances with mismatched supports, it is prudent to reframe the behavior of the agent as learning and
making inferences with respect to a misspecified learning target. Such agents learn about a random variable
θ˜: Ω (cid:55)→ Θ˜ for which Θ˜ ⊆ Θ and behave as if θ = θ˜. This may reflect machine learning algorithms in the non-
realizable setting which is becoming more and more relevant with the increasing complexity of problems (natural
language, robotics, etc). Concretely, for all t, we assume that this agent produces a prediction
P˜(·) = (cid:88) P(cid:16) θ˜=ν|H (cid:17) ·P(Y ∈·|θ =ν,H ).
t t t+1 t
ν∈Θ˜
Note that this prediction integrates over realizations of θ˜and posits a predictive distribution for Y as if θ were
t+1
the realization. While we present the above for discrete random variable θ˜, natural extensions exist by replacing
the sum with an integral and the probability measures with their Radon-Nikodymn derivatives w.r.t the Lebesgue
measure. If Θ˜ ⊂ Θ, then it’s evident that the error incurred by P˜ will not vanish to 0 if P(θ ∈ Θ\Θ˜) > 0. The
t
following result provides a very intuitive upper bound on the error incurred by P˜.
t
Theorem 56. (misspecified learner error bound) For all T ∈Z and random variables θ˜:Ω(cid:55)→Θ˜ for which
++
Θ˜ ⊆Θ, if
P˜(·) = (cid:88) P(cid:16) θ˜=ν|H (cid:17) ·P(Y ∈·|θ =ν,H ),
t t t+1 t
ν∈Θ˜
then
1 T (cid:88)−1 E(cid:104)
d
(cid:16) P∗(·)∥P˜(·)(cid:17)(cid:105)
≤
I(H T;θ˜)
+
1 T (cid:88)−1 E(cid:104)
d
(cid:16) P∗(·)∥P(cid:16)
Y ∈·|θ ←θ˜,H
(cid:17)(cid:17)(cid:105)
.
T KL t t T T KL t t+1 t
t=0 (cid:124) (cid:123)(cid:122) (cid:125) t=0
estimation error (cid:124) (cid:123)(cid:122) (cid:125)
misspecification error
67Proof.
T−1
1 (cid:88) E(cid:104)
d
(cid:16) P∗(·)∥P˜(·)(cid:17)(cid:105)
T KL t t
t=0
  
T (cid:13)
= T1 (cid:88) E d KLP t∗(·)(cid:13) (cid:13) (cid:13)(cid:88) P(Y t+1 ∈·|θ =ν,H t)·P(θ˜=ν|H t)
t=0 ν∈Θ˜
  
T−1 (cid:13)
= T1 (cid:88) I(X t+1;θ|H t)+E d KLP(Y t+1 ∈·|H t)(cid:13) (cid:13) (cid:13)(cid:88) P(Y t+1 ∈·|θ =ν,H t)·P(θ˜=ν|H t)
t=0 ν∈Θ˜
T−1
1 (cid:88)
= I(X ;θ|H )
T t+1 t
t=0
 
+ T1 T (cid:88)−1 E (cid:88) (cid:88) P(Y t+1 =y|H t)·P(θ˜=ν|Y t+1 =y,H t)ln (cid:80) P(Y P =(Y yt |+ θ1 == νy ,| HH t )) ·P(θ˜=ν|H )
t=0 ν∈Θ˜y∈Y ν∈Θ˜ t+1 t t
T−1
1 (cid:88)
= I(X ;θ|H )
T t+1 t
t=0
 
+ T1 T (cid:88)−1 E (cid:88) (cid:88) P(Y t+1 =y|H t)·P(θ˜=ν|Y t+1 =y,H t)ln(cid:80) (cid:80)ν∈Θ˜ P( PY (t Y+1 = =y| yH |θt) =·P ν( ,θ˜ H= )ν ·| PY (t θ+ ˜1 == ν|y H,H )t) 
t=0 ν∈Θ˜y∈Y ν∈Θ˜ t+1 t t
T−1
(a) 1 (cid:88)
≤ I(X ;θ|H )
T t+1 t
t=0
 
+ T1 T (cid:88)−1 E (cid:88) (cid:88) P(Y t+1 =y|H t)·P(θ˜=ν|Y t+1 =y,H t)lnP( PY (t Y+1 = =y| yH |θt) =·P ν( ,θ˜ H= )ν ·| PY (t θ+ ˜1 == ν|y H,H )t) 
t=0 y∈Yν∈Θ˜ t+1 t t
T−1
1 (cid:88)
= I(X ;θ|H )
T t+1 t
t=0
T−1
+ 1 (cid:88) E(cid:104) d (cid:16) P(θ˜∈·|Y ,H )∥P(θ˜∈·|H )(cid:17)(cid:105) +E(cid:104) d (cid:16) P(Y ∈·|H )∥P(Y ∈·|θ ←θ˜,H )(cid:17)(cid:105)
T KL t+1 t t KL t+1 t t+1 t
t=0
T−1
1 (cid:88)
= E[d (P(Y ∈·|θ,H )∥P(Y ∈·|H ))]
T KL t+1 t t+1 t
t=0
T−1
+ 1 (cid:88) I(Y ;θ˜|H )+E(cid:104) d (cid:16) P(Y ∈·|H )∥P(Y ∈·|θ ←θ˜,H )(cid:17)(cid:105)
T t+1 t KL t+1 t t+1 t
t=0
T−1
= 1 (cid:88) I(Y ;θ˜|H )+E(cid:104) d (cid:16) P(Y ∈·|θ,H )∥P(Y ∈·|θ ←θ˜,H )(cid:17)(cid:105)
T t+1 t KL t+1 t t+1 t
t=0
=
I(H T;θ˜)
+
1 T (cid:88)−1 E(cid:104)
d
(cid:16)
P(Y ∈·|θ,H )∥P(Y ∈·|θ ←θ˜,H
)(cid:17)(cid:105)
,
T T KL t+1 t t+1 t
t=0
where (a) follows from the log-sum inequality.
This result is rather intuitive as it involves an estimation error term which grows with the number of nats that θ˜
contains about H . Note that the estimation error will always decay to 0 as T → ∞ so long as I(H ;θ)/T also
T T
decays to 0 as T →∞. Meanwhile, the misspecification error signifies the error which persists even with access to
infinite data. Note that when ((X ,Y ):t∈Z ) is an iid process conditioned on θ, each term in the sum will be
t t+1 +
equal and hence reduce to E[d (P(Y ∈ ·|θ,X )∥P(Y ∈ ·|θ ← θ˜,X ))]. We note that often this quantity can
KL t+1 t t+1 t
be simple to bound for concrete problem instances and was used as an intermediate step in the derivation of many
earlier proofs in this paper.
68Inregardstothisquantity,wenotethefollowingresultwhichstatesthatthepredictionP(Y ∈·|θ˜,H )experiences
t+1 t
less error than the prediction which results from the change of measure: P(Y ∈·|θ ←θ˜,H ).
t+1 t
Lemma 57. If θ˜:Ω(cid:55)→Θ˜ is a random variable such that for all t∈Z , Y ⊥θ˜|θ,H and Θ˜ ⊆Θ, then
+ t+1 t
(cid:104) (cid:105) (cid:104) (cid:105)
E d (P(Y ∈·|θ,H )∥P(Y ∈·|θ˜,H )) ≤ E d (P(Y ∈·|θ,H )∥P(Y ∈·|θ ←θ˜,H )) .
KL t+1 t t+1 t KL t+1 t t+1 t
Proof.
(cid:104) (cid:105)
E d (P(Y ∈·|θ,H )∥P(Y ∈·|θ ←θ˜,H ))
KL t+1 t t+1 t
(cid:20) P(Y |θ,H ) (cid:21)
=E ln t+1 t
P(Y |θ ←θ˜,H )
t+1 t
(cid:20) P(Y |θ,H )(cid:21) (cid:34) P(Y |θ˜,H ) (cid:35)
=E ln t+1 t +E ln t+1 t
P(Y |θ˜,H ) P(Y |θ ←θ˜,H )
t+1 t t+1 t
=E(cid:104) d KL(P(Y t+1 ∈·|θ,H t)∥P(Y t+1 ∈·|θ˜,H t))(cid:105) +E(cid:34) E(cid:34) ln P(YP(Y t+ |θ1| ←θ˜,H θ˜,t H) )(cid:12) (cid:12) (cid:12) (cid:12)θ˜,H t(cid:35)(cid:35)
t+1 t
(cid:104) (cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
=E d (P(Y ∈·|θ,H )∥P(Y ∈·|θ˜,H )) +E d P(Y ∈·|θ˜,H )∥P(Y ∈·|θ ←θ˜,H )
KL t+1 t t+1 t KL t+1 t t+1 t
(a) (cid:104) (cid:105)
≥ E d (P(Y ∈·|θ,H )∥P(Y ∈·|θ˜,H )) ,
KL t+1 t t+1 t
where (a) follows from the fact that KL-divergence is non-negative.
In the section to follow, we run through two concrete examples in the linear regression setting which demonstrate
use of Theorems 55 and 56.
9.2 Linear Regression with Misspecification
In this section, we will apply Theorems 54 and 55 to study the impact of misspecification in the linear regression
setting from earlier in the text. We select this setting as the existence of a conjugate prior allows us to draw
additional insights about the misspecification error.
9.2.1 Data Generating Process
We restate the linear regression setting here for the reader’s convenience. In linear regression, the variable that we
are interested in estimating is a random vector θ ∈ℜd. We assume a prior distribution P(θ ∈·)=N(0,I /d). For
d
all t∈Z , inputs and outputs are generated according to a random vector X i ∼id N(0,I ) and
+ t d
Y =θ⊤X +W ,
t+1 t t+1
where W i ∼id N(0,σ2) for known variance σ2.
t
9.2.2 Misspecified Algorithms
In this section, we will study the estimation error of two misspecified linear regression algorithms. The first
algorithm will be misspecified in that it will assume a prior distribution P˜(θ ∈ ·) = N(µ,I ), where µ ̸= 0. The
d
second algorithm will make predictions while failing to model the impact of one of the features.
The following result upper bounds the misspecification error incurred by an algorithm which performs Bayesian
inference with respect to a misspecified prior distribution P˜(θ ∈·)=N(µ,I ).
d
Theorem 58. (mean misspecification error upper bound) For all d,T ∈Z , if for all t∈[T], P˜ returns
++ t
a predictive distribution of Y conditioned on H under prior distribution P˜(θ ∈·)=N(µ,I), then
t+1 t
1 (cid:88)T E(cid:104)
d
(cid:16) Pˆ∥P˜(cid:17)(cid:105)
≤
∥µ∥2
2
T KL t t 2T
t=0
69Proof.
(cid:16) (cid:17)
T d P(θ ∈·) ∥ P˜(θ ∈·)
1 (cid:88) E(cid:104)
d
(cid:16) Pˆ∥P˜(cid:17)(cid:105)( ≤a) KL
T KL t t T
t=0
( =b)
∥µ∥2
2,
2T
where (a) follows from Theorem 55 and (b) follows from the formula of KL divergence between two multivariate
normal distributions.
We notice that in this setting, the misspecification error decays to 0 as T →∞. This is intuitive as in this setting,
the algorithm, though suboptimal, does not assign 0 probability mass to any events with truly have non-zero
probability. Inthefollowingexample,wewillstudywhatoccurswhenthisisnotthecaseandratherthealgorithm
is blind to certain aspects of the underlying data generating process.
We now study the misspecification error of an algorithm which ignores the influence of the final dimension of θ.
Conceretely, if θ denotes the ith dimension of θ ∈ℜd, then θ˜=(θ ,θ ,...,θ ,0). We assume that for all t, the
i 1 2 d−1
algorithm produces a prediction
(cid:90)
P˜ (·)= P(Y ∈·|θ =ν,X )·p (ν|H ) dµ(ν),
t t+1 t θ˜ t
ν∈ℜd−1
where p (·|H ) is the Radon-Nikodym derivative of P(θ˜∈·|H ) w.r.t. the Lebesgue measure µ. We now derive an
θ˜ t t
upper bound for the error incurred by this algorithm.
Theorem59. (misspecificationerrorofmissingfeatureagent)Foralld,T ∈Z ,ifforallt∈{0,1,...,T−
++
1}, (X ,Y ) is generated by the linear regression process and
t t+1
(cid:90)
P˜(·) = P(Y ∈·|θ =ν,X )·p (ν|H ) dµ(ν),
t t+1 t θ˜ t
ν∈ℜd−1
then,
1 T (cid:88)−1 E(cid:104)
d
(cid:16) P∗(·)∥P˜(·)(cid:17)(cid:105)
≤
d−1(cid:18) 1 +ln(cid:18)
1+
T(cid:19)(cid:19)
+
1
.
T KL t t 2T σ2 d dσ2
t=0
70Proof. Let θ¯=θ˜+Z where Z ⊥θ˜and Z ∼N(0,ϵI ), Z =0 for ϵ=1/T. Then,
1:d−1 d−1 d
I(H T;θ˜)
=
1 T (cid:88)−1
I(Y ;θ˜|H )
T T t+1 t
t=0
T−1
= 1 (cid:88) I(Y ;θ¯|H )+I(Y ;θ˜|θ¯,H )
T t+1 t t+1 t
t=0
T−1
( ≤a) 1 (cid:88) I(Y ;θ¯|H )+I(Y ;θ|θ¯,H )
T t+1 t t+1 t
t=0
=
I(θ¯;H T)
+
1 T (cid:88)−1
I(Y ;θ|θ¯,H )
T T t+1 t
t=0
I(θ¯;θ) 1 (cid:34) (cid:32) (cid:0) θ⊤X −θ¯⊤X (cid:1)2(cid:33)(cid:35)
≤ + E ln 1+ t t
T 2 σ2
(cid:18) (cid:19)
 E(cid:104)(cid:0)
Z⊤X
(cid:1)2(cid:105)
+
1
(b) d−1 1 1 t,1:d−1 d
≤ ln 1+ + ln1+ 
2T dϵ 2 σ2
d−1 (cid:18) 1 (cid:19) 1 (cid:18) (d−1)ϵ+ 1(cid:19)
= ln 1+ + ln 1+ d
2T dϵ 2 σ2
d−1 (cid:18) T(cid:19) 1 (cid:32) (d−1) + 1(cid:33)
= ln 1+ + ln 1+ T d
2T d 2 σ2
(cid:18) (cid:19)
d−1 T d−1 1
≤ ln 1+ + + ,
2T d 2Tσ2 2dσ2
where (a) follows from the data processing inequality and (b) follows from Jensen’s inequality.
T−1
1 (cid:88) E(cid:104) d (cid:16) P∗(·)∥P(cid:16) Y ∈·|θ ←θ˜,H (cid:17)(cid:17)(cid:105) =E(cid:104) d (cid:16) P∗(·)∥P(cid:16) Y ∈·|θ ←θ˜,X (cid:17)(cid:17)(cid:105)
T KL t t+1 t KL t t+1 t
t=0
(cid:16) (cid:17)2
θ⊤X −θ˜⊤X
t t
=
2σ2
1
= .
2dσ2
Therefore, the result follows:
1 T (cid:88)−1 E(cid:104)
d
(cid:16) P∗(·)∥P˜(·)(cid:17)(cid:105)
≤
I(H T;θ˜)
+
1 T (cid:88)−1 E(cid:104)
d
(cid:16) P∗(·)∥P(cid:16)
Y ∈·|θ ←θ˜,H
(cid:17)(cid:17)(cid:105)
T KL t t T T KL t t+1 t
t=0 t=0
(cid:18) (cid:19)
d−1 T d−1 1 1
≤ ln 1+ + + +
2T d 2Tσ2 2dσ2 2dσ2
(cid:18) (cid:18) (cid:19)(cid:19)
d−1 1 T 1
= +ln 1+ + ,
2T σ2 d dσ2
where (a) follows from Theorem 56.
Note that the first term describes the error which reduces to 0 as T →∞. Meanwhile, the second term represents
irreducible error which comes from the fact that θ˜ignores the influence of the final feature.
9.3 Neural Scaling Laws
More broadly, we believe that results such as Theorem 56 can allow researchers in the area to more carefully
consider the trade-offs which occur in the process of machine learning. This is already becoming apparent in the
71phenomenon of neural scaling laws [Hoffmann et al., 2022, Kaplan et al., 2020] which aim to optimally balance the
error due to misspecification and estimation for a fixed FLOP budget. The FLOP count is roughly the product of
the parameter count of the trained model and the size of the training dataset. Evidently, increasing the parameter
countwouldreducemisspecificationerror,potentiallyatthecostofgreaterestimationerror. Meanwhile,increasing
the dataset will reduce the estimation error. As a result, for a fixed FLOP count, it is not immediately clear how
much should be allocated to parameter count vs dataset size to achieve the smallest loss. We hope that the results
provided in this section provide the groundwork for future theoretical study of how compute-bound algorithms
ought to effectively trade-off these two sources of error in learning.
Summary
• Recall that P∗(·)=P(Y ∈·|θ,H ), Pˆ(·)=P(Y ∈·|H ).
t t+1 t t t+1 t
• (loss of an arbitrary algorithm) For all T ∈ Z , if for all t ∈ [T], P˜ is a predictive distribution
+ t
which may depend on the previous data H , then
t
T1 T (cid:88)−1 E(cid:104) d KL(cid:16) P t∗(·)∥P˜ t(·)(cid:17)(cid:105) = I(H TT;θ) + T1 T (cid:88)−1 E(cid:104) d KL(cid:16) Pˆ t(·)(cid:13) (cid:13)P˜ t(·)(cid:17)(cid:105) .
t=0 t=0
(cid:124) (cid:123)(cid:122) (cid:125)
misspecification error
• (misspecified prior error bound) For all T ∈ Z , if for all t ∈ [T], P˜(·) is the Bayesian posterior
++ t
of Y conditioned on H under the prior P˜(θ ∈·), then
t+1 t
(cid:16) (cid:17)
T−1 d P(θ ∈·)∥P˜(θ ∈·)
T1 (cid:88) E(cid:104) d KL(cid:16) Pˆ t(·) (cid:13) (cid:13) P˜ t(·)(cid:17)(cid:105) ≤ KL
T
.
t=0
• For instances in which P˜(θ ∈·) assigns 0 probability mass to a set A for which P(θ ∈A)>0, the above
bound is vacuous.
• In such instances, it is prudent to instead consider the result of the following learner which performs
inference on a separate learning target θ˜and performs predictions as if θ =θ˜:
• (misspecified learner error bound) For all T ∈ Z and random variables θ˜ : Ω (cid:55)→ Θ˜ for which
++
Θ˜ ⊆Θ, if
P˜(·) = (cid:88) P(cid:16) θ˜=ν|H (cid:17) ·P(Y ∈·|θ =ν,H ),
t t t+1 t
ν∈Θ˜
then
1 T (cid:88)−1 E(cid:104)
d
(cid:16) P∗(·)∥P˜(·)(cid:17)(cid:105)
≤
I(H T;θ˜)
+
1 T (cid:88)−1 E(cid:104)
d
(cid:16) P∗(·)∥P(cid:16)
Y ∈·|θ ←θ˜,H
(cid:17)(cid:17)(cid:105)
.
T KL t t T T KL t t+1 t
t=0 (cid:124) (cid:123)(cid:122) (cid:125) t=0
estimation error (cid:124) (cid:123)(cid:122) (cid:125)
misspecification error
• Resultsliketheoneabovecanprovidetheoreticalinsightsintohowestimationerrorandmisspecification
error ought to be traded off with access to limited data or compute.
7210 Conclusion
Summary. In this paper, we framed continual learning as computationally constrained reinforcement learning.
Under this perspective, we formally introduced an objective for continual learning. This objective is to maximize
the infinite-horizon average reward under computational constraints. We formalized the concepts of agent state,
information capacity, and learning targets, which play key roles for thinking about continual learning and distin-
guishing it from traditional vanishing-regret learning. Leveraging information theory, we decomposed prediction
errorintoforgettingandimplasticitycomponents. Weconcludedwithcasestudiesthatstudiedimplicationsofour
objective on behaviors of performant agents.
Future Research. In our case studies, we discussed how different agent capabilities contribute to performance
under our objective. These capabilities include balancing forgetting with fast relearning, seeking out durable
information when exploring, modeling environment dynamics, and meta-learning hyperparameters. We hope that
this work inspires researchers to design agents that exhibit such capabilities and to study the trade-offs between
themundercomputationalconstraints. Inparticular, wehopeourholisticobjectivehelpsresearchersreasonabout
these trade-offs and reveal additional desired capabilities.
Acknowledgements
Financial support from the NSF GRFP fellowship and the Army Research Office (ARO) Grant W911NF2010055
is gratefully acknowledged.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint
arXiv:2303.08774, 2023.
AndrewRBarron. Universalapproximationboundsforsuperpositionsofasigmoidalfunction. IEEE Transactions
on Information Theory, 39(3):930–945, 1993.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.
Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC-dimension bounds for piecewise polynomial
networks. Neural computation, 10(8):2159–2173, 1998.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and pseu-
dodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research, 20(63):1–17,
2019.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and
Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954.
Thomas M Cover and Joy A Thomas. Elements of Information Theory. John Wiley & Sons, 2012.
Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern,
Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness.
Languagemodelingiscompression. InThe Twelfth International Conference on Learning Representations,2024.
URL https://openreview.net/forum?id=jznbgiynus.
GintareKarolinaDziugaiteandDanielMRoy. Computingnonvacuousgeneralizationboundsfordeep(stochastic)
neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear
neural networks. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on Learning
Theory,volume65ofProceedings of Machine Learning Research,pages1064–1068.PMLR,07–10Jul2017. URL
https://proceedings.mlr.press/v65/harvey17a.html.
73Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego
de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican,
GeorgevandenDriessche,BogdanDamoc,AureliaGuy,SimonOsindero,KarenSimonyan,ErichElsen,JackW.
Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.
Edwin T Jaynes and Oscar Kempthorne. Confidence intervals vs Bayesian intervals. In Foundations of Probability
Theory, Statistical Inference, and Statistical Theories of Science: Proceedings of an International Research Col-
loquium held at the University of Western Ontario, London, Canada, 10–13 May 1973 Volume II Foundations
and Philosophy of Statistical Inference, pages 175–257. Springer, 1976.
Hong Jun Jeon and Benjamin Van Roy. Information-theoretic foundations for neural scaling laws, 2024. URL
https://arxiv.org/abs/2407.01456.
Hong Jun Jeon, Jason D. Lee, Qi Lei, and Benjamin Van Roy. An information-theoretic analysis of in-context
learning, 2024.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361, 2020.
David A McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference on Compu-
tational learning theory, pages 230–234, 1998.
DanielRussoandJamesZou. Howmuchdoesyourdataexplorationoverfit? controllingbiasviainformationusage.
IEEE Transactions on Information Theory, 66(1):302–323, 2019.
Leonard J Savage. The foundations of statistics. Courier Corporation, 1972.
Claude E Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423,
1948.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with
deep neural networks and tree search. Nature, 529(7587):484–489, 2016.
NaftaliTishbyandNogaZaslavsky.Deeplearningandtheinformationbottleneckprinciple.In2015ieeeinformation
theory workshop (itw), pages 1–5. IEEE, 2015.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint
physics/0004057, 2000.
Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and
Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms.
Advances in neural information processing systems, 30, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
(still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.
Yifan Zhu, Hong Jun Jeon, and Benjamin Van Roy. Is stochastic gradient descent near optimal?, 2022.
74Appendix
A Lower Bounds for Linear Regression
We now introduce a lemma relating expected KL divergence to mean-squared error, a distortion measure that is
prevalent in the literature. This relation will allow us to derive a lower bound for the rate-distortion function in
the Gaussian linear regression setting.
Lemma 60. For all d ∈ Z and σ2 ≥ 0, if θ : Ω (cid:55)→ ℜd has iid components that are each ν2-subgaussian and
++
symmetric, X ∼ N(0,I ), and Y ∼ N(θ⊤X,σ2), then for all proxies θ˜ ∈ Θ˜, Y −E[Y|θ˜,X] is 4ν2∥X∥2 +σ2-
d 2
subgaussian conditioned on X.
Proof.
E(cid:104) eλ(Y−E[Y|θ˜,X])(cid:12) (cid:12)X(cid:105)( =a)E(cid:104) eλ(Y−E[Y|θ,X])(cid:12) (cid:12)X(cid:105) ·E(cid:104) eλ(E[Y|θ,X]−E[Y|θ˜,X])(cid:12) (cid:12)X(cid:105)
=eλ2 2σ2 ·E(cid:104) eλ((θ−E[θ|θ˜])⊤X)(cid:12) (cid:12)X(cid:105)
( ≤b) eλ2 2σ2 ·E(cid:104) e−λ(θ⊤X)·E(cid:104) e−λ(θ⊤X)|θ˜,X(cid:105)(cid:12) (cid:12)X(cid:105)
( ≤c) eλ2 2σ2E(cid:104) E[e−λ(θ⊤X)|θ˜,X]2(cid:12) (cid:12)X(cid:105)
≤eλ2 2σ2E(cid:104) E[e−2λ(θ⊤X)|θ˜,X](cid:12) (cid:12)X(cid:105)
=eλ2 2σ2E(cid:104) e−2λ(θ⊤X)(cid:12) (cid:12)X(cid:105)
( =d) eλ2 2σ2 e2λ2ν2∥X∥2
2
λ2(σ2+4ν2∥X∥2 2)
=e 2 ,
where (a) follows from Y −E[Y|θ,X] = W which is independent from E[Y|θ,X]−E[Y|θ˜,X], (b) follows from
the fact that θ⊤X is symmetric conditioned on X and Jensen’s inequality, (c) follows from the fact that eθ⊤X =
E[eθ⊤X|θ,θ˜,X], and (d) follows from the fact that the components of θ are ν2-subgaussian.
Lemma 61. If Y −E[Y|θ˜,X] is ν2-subgaussian conditional on X, then for all α > 1, Y −E[Y|θ˜,X] is αν2-
subgaussian conditional on (θ˜,X).
Proof. Assumethatforsomeα>1,thereexistsaneventS s.t. P(θ˜∈S)>0andθ˜∈S impliesthatY −E[Y|θ˜,X]
is not αν2-subgaussian conditioned on (θ˜,X). We have that
E(cid:104) eλ(Y−E[Y|θ˜,X])(cid:12) (cid:12)X(cid:105) ≥P(θ˜∈S)·E(cid:104) eλ(Y−E[Y|θ˜,X])|θ˜∈S,X(cid:105)
( >a) elnP(θ˜∈S)·eαλ 22ν2
=eλ2(αν2+ λ2
2
2lnP(θ˜∈S))
=eλ2(ν2+(α−1)ν2 2+ λ2 2lnP(θ˜∈S))
,
where (a) holds for all λ s.t. |λ| ≥ |λ∗| for some λ∗. Such λ∗ exists because of the fact that θ˜∈ S implies that
Y −E[Y|θ˜,X]isnotαν2-subgaussianconditionedon(θ˜,X). Asaresult,forλsuchthatλ2
>max(cid:110) 2lnP(θ˜∈S),λ2(cid:111)
,
(1−α)ν2 ∗
we have that
E(cid:104) eλ(Y−E[Y|θ˜,X])(cid:12) (cid:12)X(cid:105) >eλ2 2ν2
,
which is a contradiction since Y −E[Y|θ˜,X] is ν2-subgaussian conditional on X. Therefore the assumption that
there exists α>1 and θ˜s.t. is not αν2-subgaussian conditional on (X,θ˜) cannot be true. The result follows.
75Lemma 62. If Y −E[Y|θ˜,X] is ν2-subgaussian conditioned on (θ˜,X), then
(cid:16) (cid:17)2
E[Y|θ,X]−E[Y|θ˜,X]
(cid:104) (cid:105)
E ≤E d (P(Y ∈·|θ,X)∥P(Y ∈·|θ˜,X)) .
 2ν2  KL
Proof. We begin by stating the Donsker-Varadhan variational form of the KL-divergence. For all probability
distributions P(·) and Q(·) over ℜ such that P is absolutely continuous with respect to Q, for densities dP,dQ
w.r.t the Lebesgue measure,
(cid:18)(cid:90) (cid:90) (cid:19)
d (P(·)∥Q(·))= sup g(y)dP(y)−ln eg(y)dQ(y) ,
KL
g:ℜ→ℜ y∈ℜ y∈ℜ
(cid:82)
wherethesupremumistakenovermeasurablefunctionsforwhich g(y)dP(y)iswell-definedandtheexpression
y∈ℜ
on the right (cid:82) eg(y)dQ(y) is finite.
y∈ℜ
Let P(·) = P(Y ∈ ·|θ,X ),Q(·) = P(Y ∈ ·|θ˜,X), and Z = Y −E[Y|θ˜,X]. Then, for arbitrary λ ∈ R, applying the
t
variational form of KL-divergence with g(Y)=λZ gives us
d (P(Y ∈·|θ,X)∥P(Y ∈·|θ˜,X))( =a) d (P(Y ∈·|θ,θ˜,X)∥P(Y ∈·|θ˜,X))
KL KL
(cid:104) (cid:105) (cid:104) (cid:105)
≥λE Z|θ,θ˜,X −lnE eλZ|θ˜,X
(b) (cid:16) (cid:17) λ2ν2
≥ λ E[Y|θ,X]−E[Y|θ˜,X] − ,
2
where (a) follows from Y ⊥θ˜|(θ,X) and (b) follows from Z being ν2-subgaussian conditioned on (θ˜,X). Since the
above holds for arbitrary λ, maximizing the RHS w.r.t λ give us:
(cid:16) (cid:17)2
E[Y|θ,X]−E[Y|θ˜,X]
d (P(Y ∈·|θ,X)∥P(Y ∈·|θ˜,X))≥ .
KL 2ν2
The result follows from taking an expectation on both sides.
We establish a lower bound by first finding a suitable lower bound for the distortion function. For subgaussian
randomvectors,thefollowinglemmaallowsustolowerboundtheexpectedKL-divergencedistortionbyamultiple
of the mean squared error.
Lemma 63. For all θ˜∈ Θ˜, d ∈ Z and σ2 ∈ ℜ , if θ : Ω (cid:55)→ ℜd consists of iid components each of which are
++ ++
1/d-subgaussian and symmetric, P(X ∈·)∼N(0,I ), and if Y ∼N(θ⊤X,σ2), then
d
(cid:20) ∥X∥2 (cid:21) (cid:104) (cid:105)
E 2 E ∥θ−E[θ|θ˜]∥2 ≤I(Y;θ|θ˜,X).
2(4∥X∥2+dσ2) 2
2
Proof. θ⊤X is∥X∥2/d-subgaussianconditionedonX andbyLemma60,Y−E[Y|θ˜,X]is4∥X∥2/d+σ2-subgaussian
2 2
conditioned on ∥X∥2. Lemma 61 then states that Y −E[Y|θ˜,X] is is α(4∥X∥2/d+σ2)-subgaussian conditioned on
2 2
76(θ˜,X) for all α>1. Therefore,
(cid:16) (cid:17)2
(θ−E[θ|θ˜])⊤X
(cid:104) (cid:105)(a)
E d (P(Y ∈·|θ,X)∥P(Y ∈·|θ˜,X)) ≥ limE 
KL α↓1  2α(cid:16) 4∥X∥2 2 +σ2(cid:17) 
d
(cid:16) (cid:17)2
(θ−E[θ|θ˜])⊤X
=E 
 2(cid:16) 4∥X∥2
2
+σ2(cid:17) 
d
 (cid:16) (cid:17)2 
(θ−E[θ|θ˜])⊤X (cid:12)
(cid:12)
=EE (cid:12)∥X∥2
  2(cid:16) 4∥X∥2
2
+σ2(cid:17) (cid:12)
(cid:12)
2
d
∥X∥2 2E(cid:104) ∥(θ−E[θ|θ˜]∥2(cid:105)
=E

d
(cid:16) (cid:17)
2

2
4∥X∥2
2 +σ2
d
( =b)E(cid:20) ∥X∥2 2 (cid:21) E(cid:104) ∥θ−E[θ|θ˜]∥2(cid:105)
2(4∥X∥2+dσ2) 2
2
where (a) follows from Lemma 62 and (b) follows from the fact that X ∼N(0,I ).
d
B Theoretical Results for Dirichlet-Multinomial
B.1 General Combinatorics Results
We begin with the folloiwing general combinatorics results that we will eventually apply to bound the estimation
error of the finite N Dirichlet-Multinomial process.
Lemma 64. For all m,j,n,K,N ∈Z s.t. j <n, if
++
n−1 K n−1 K n−1 K
(cid:88) (cid:88) (cid:88)
C (j)= N · N ·...· N ,
m K+i K+i K+i
i=j i=j+1 i>j+m−1
(cid:124) (cid:123)(cid:122) (cid:125)
m
then
1 Km (cid:18) K+n (cid:19) 1 Km (cid:18) K+n (cid:19)
lnm ≤C (j)≤ lnm .
m!Nm K+m−1+j m m!Nm K−1+j
Proof. We first prove the upper bound via induction. Base Case: n=1
n−1 K
(cid:88)
C (j)= N
1 K+i
i=j
K (cid:90) n 1
≤ dx
N K+x
j−1
(cid:18) (cid:19)
K K+n
= ln
N K−1+j
77Assume inductive hypothesis is true for m=k.
n−1 K
(cid:88)
C (j)= N ·C (i+1)
k+1 K+i k
i=j
K (cid:90) n 1
≤ ·C (x+1)
N K+x k
j−1
Kk+1 1 (cid:90) n 1 (cid:18) K+n(cid:19)
≤ ·lnk dx
Nk+1k! K+x K+x
j−1
≤−Kk+1 1 ·lnk+1(cid:18) K+n(cid:19)(cid:12) (cid:12) (cid:12)n
Nk+1(k+1)! K+x (cid:12)
j−1
Kk+1 1 (cid:18) K+n (cid:19)
= lnk+1 .
Nk+1(k+1)! K−1+j
We now prove the lower bound also via induction. Base Case: m=1
n−1 K
(cid:88)
C (j)= N
1 K+i
i=j
K (cid:90) n 1
≥ dx
N K+x
j
(cid:18) (cid:19)
K K+n
= ln
N K+j
Assume inductive hypothesis is true for m=k.
n−1 K
(cid:88)
C (j)= N ·C (i+1)
k+1 K+i k
i=j
K (cid:90) n 1
≥ ·C (x+1)
N K+x k
j
Kk+1 1 (cid:90) n 1 (cid:18) K+n (cid:19)
≥ ·lnk dx
Nk+1k! K+x K+k+x
j
≥−Kk+1 1 ·lnk+1(cid:18) K+n (cid:19)(cid:12) (cid:12) (cid:12)n
Nk+1(k+1)! K+k+x (cid:12)
j
Kk+1 1 (cid:18) K+k+n(cid:19)
= lnk+1
Nk+1(k+1)! K+k+j
Kk+1 1 (cid:18) K+n (cid:19)
≥ lnk+1 .
Nk+1(k+1)! K+k+j
The result follows.
√
Lemma 65. For all i,n,K,N ∈Z , if 2≤K ≤ N and n≤N, then
++
1 K2i (cid:18) K+n(cid:19) 1 K2i+1 (cid:18) K+n(cid:19)
ln2i − ln2i+1 ≥0.
(2i)!N2i K−1 (2i+1)!N2i+1 K+2i
78Proof.
(cid:18) (cid:19)
(a) 1 K K+n
0 ≤ 1− ln
2i+1N K−1
(cid:18) (cid:19) (cid:18) (cid:19)
K+n 1 K K+n
=ln2i − ln2i+1
K−1 2i+1N K−1
(cid:18) (cid:19) (cid:18) (cid:19)
K+n 1 K K+n
≤ln2i − ln2i+1
K−1 2i+1N K+2i
1 K2i (cid:18) K+n(cid:19) 1 K2i+1 (cid:18) K+n(cid:19)
= ln2i − ln2i+1
(2i)!N2i K−1 (2i+1)!N2i+1 K+2i
where (a) follows for all n≤(K−1)e3 KN −K which is implied by n≤N for K ≥2.
√
Lemma 66. For all n,K,N ∈Z , if 2≤K ≤ N and n≤N, then
++
n (cid:89)−1(cid:32) K (cid:33) K (cid:16) n(cid:17)
1− N ≥1− ln 1+ .
K+i N K
i=0
Proof.
n (cid:89)−1(cid:32) K (cid:33) (a) ⌊ (cid:88)n− 21⌋ 1 K2i+1 (cid:18) K+r (cid:19)
1− N ≥ 1− ln2i+1
K+i (2i+1)!N2i+1 K+2i
i=0 i=0
⌈ (cid:88)n− 21⌉ 1 K2i (cid:18) K+r(cid:19)
+ ln2i
(2i)!N2i K−1
i=1
(b) K (cid:16) n(cid:17)
≥ 1− ln 1+
N K
⌊ (cid:88)n− 21⌋ 1 K2i (cid:18) K+n(cid:19) 1 K2i+1 (cid:18) K+n(cid:19)
+ ln2i − ln2i+1
(2i)!N2i K−1 (2i+1)!N2i+1 K+2i
i=1
(c) K (cid:16) n(cid:17)
≥ 1− ln 1+ ,
N K
where (a) follows from Lemma 64, and (b) follows from Lemma 65
B.2 Lemmas pertaining to Dirichlet Multinomial
The following result upper bounds the expected number of unique classes drawn from a Dirichlet-multinomial
distribution with n draws and α=[K/N,...,K/N]∈ℜN.
√
Lemma67. Foralln,K,N ∈Z s.t. K ≤ N andn≤N,ifθ˜′ ∼DirMult(n,α)forα=[K/N,...,K/N]∈ℜN,
++
then
(cid:34) N (cid:35)
(cid:88) (cid:16) n(cid:17)
E 1 ≤Kln 1+ .
[θ˜ i′>0] K
i=1
79Proof.
(cid:34) N (cid:35)
E (cid:88) 1 =N ·P(θ˜′ >0)
[θ˜′>0] i
i
i=1
( =a) N ·(cid:16) 1−P(θ˜′ =0,θ˜′ +···+θ˜′ =n)(cid:17)
1 2 N
(cid:32) Γ(K)Γ(n+1) Γ(cid:0)K(cid:1) Γ(cid:0) n+ K(N −1)(cid:1) (cid:33)
=N · 1− · N · N
Γ(n+K)
Γ(cid:0)K(cid:1)
Γ(1)
Γ(cid:0)K(N −1)(cid:1)
Γ(n+1)
N N
(cid:32)
Γ(K)
Γ(cid:0)
n+K−
K(cid:1)(cid:33)
=N · 1− · N
Γ(n+K)
Γ(cid:0)
K−
K(cid:1)
N
(cid:32) (cid:81)n−1(cid:0) K− K +i(cid:1)(cid:33)
=N · 1− i=0 N
(cid:81)n−1K+i
i=0
(cid:32) n−1(cid:32)
K
(cid:33)(cid:33)
(cid:89)
=N · 1− 1− N
K+i
i=0
(b) (cid:18) (cid:18) K (cid:16) n(cid:17)(cid:19)(cid:19)
≤ N · 1− 1− ln 1+
N K
(cid:16) n(cid:17)
=Kln 1+ .
K
where (a) follows from the aggregation property of the Dirichlet-multinomial distribution and (b) follows from
Lemma 66.
Note that this upper bound is independent of N. In the next section, we will apply the dominated convegence
theorem to bound the number of unique basis functions drawn by our learning model.
The extention to a Dirichlet Process (N →∞) follows trivially.
Lemma 68. For all n,K ∈ Z , if θ is distributed according to a Dirichlet process with base distribution
++
Uniform(Sd−1) with scale parameter K and θ˜∼Multinomial(θ), then
(cid:34) (cid:35)
(cid:88) (cid:16) n(cid:17)
E 1 ≤Kln 1+ .
[θ˜ w>0] K
w∈W
Proof.
(cid:34) (cid:35) (cid:34) (cid:34) (cid:12) (cid:35)(cid:35)
E (cid:88) 1 [θ˜ w>0] =E E (cid:88) 1 [θ˜ w>0](cid:12) (cid:12) (cid:12)W
w∈W w∈W
  
(cid:12)
( =a)E E  Nl →im
∞
(cid:88) 1 [θ˜ w′ >0](cid:12) (cid:12) (cid:12)W
w∈W˜
  
(cid:12)
( =b) Nl →im ∞E E (cid:88) 1 [θ˜ w′ >0](cid:12) (cid:12) (cid:12)W
w∈W˜
(c) (cid:16) n(cid:17)
≤ lim Kln 1+
N→∞ K
(cid:16) n(cid:17)
=Kln 1+ ,
K
wherein(a),W isasubsetofthefirstN elementsofW andX˜ isDirMult(n,α )whereα =[K/N,...,K/N]∈
N N N N
ℜN and W is the set of classes, (b) follows from the dominated convergence theorem since |(cid:80) 1 |≤n,
N w∈W˜ [θ˜′ >0]
w
and (c) follows from Lemma 67.
80