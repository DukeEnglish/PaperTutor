GroundUp: Rapid Sketch-Based 3D City Massing
Gizem Esra Ünlü1 Mohamed Sayed2 Yulia Gryaditskaya3 Gabriel Brostow1
1University College London 2Niantic 3PAI and CVSSP, University of Surrey
Abstract. We propose GroundUp, the first sketch-based ideation tool
for3Dcitymassing ofurbanareas.Wefocusonearly-stageurbandesign,
where sketching is a common tool and the design starts from balancing
buildingvolumes(masses)andopenspaces.WithHuman-CenteredAIin
mind,weaimtohelparchitectsquicklyrevisetheirideasbyeasilyswitch-
ingbetween2Dsketchesand3Dmodels,allowingforsmootheriteration
and sharing of ideas. Inspired by feedback from architects and existing
workflows, our system takes as a first input a user sketch of multiple
buildings in a top-down view. The user then draws a perspective sketch
of the envisioned site. Our method is designed to exploit the comple-
mentarityofinformationinthetwosketchesandallowsuserstoquickly
previewandadjusttheinferred3Dshapes.Ourmodelhastwomaincom-
ponents.First,weproposeanovelsketch-to-depthpredictionnetworkfor
perspectivesketchesthatexploitstop-downsketchshapes. Second,weuse
depthcuesderivedfromtheperspectivesketchasaconditiontoourdif-
fusion model, which ultimately completes the geometry in a top-down
view. Thus,ourfinal3Dgeometryisrepresentedasaheightfield,allow-
inguserstoconstructthecity“fromthegroundup”. Thecode,datasets,
and interface are available at visual.cs.ucl.ac.uk/pubs/groundup.
1 Introduction
Urban design has a deep impact on people’s lives, and it epitomizes the op-
portunities to bring Human-Centered AI for Computer Vision [96] into iterative
design[37].Theloopofdrawinganddiscussingbuildings,andspecificallysketch-
ing the buildings’ masses, i.e. coarse shapes, is the crucial first stage of urban
planning[63].“Architecturaldesignbeginswithamassingstudy” [43],wherethe
term “massing” is used for this stage because it locks in the long-term balance
between constructed mass versus open space. In pilot interviews, architects said
that existing 3D software for urban modeling is too cumbersome for ideation
and does not support beginners.
Presently,theeaseofsketchingishardtobeat.3Dmodelprecisionisnotthe
toppriorityinmassing.Rather,urbandesignaimstosatisfytheconstraintsand
desires of whole teams of stakeholders. For example, an architect will play with
many massing alternatives, often changing their mind mid-sketch. Currently,
theyiteratefurtherin2Dwithfellowarchitectsonashortlistoffavorites,before
re-doing just one or a few designs in 3D software (e.g. Rhino or Sketchup), to
test out the idea. Our work aims to facilitate the design process by providing
the means to quickly preview designs in 3D.
4202
luJ
71
]VC.sc[
1v93721.7042:viXra2 Ünlü et al.
(0) Select underlay (1) Sketch (2) Sk etch
(optional) Top-down Perspective view
(3) In-browser Render
Fig.1: An illustrative example of our method. (0) Users of our web-based
GroundUp system can optionally load registered maps, satellite images, or perspec-
tive photographs as underlay layers. These give context for the “massing” process. A
blankunderlayisusedinthisexample.(1)(bottom)Theusersketchestheinitialfoot-
prints of multiple buildings in a top-down view. These strokes are projected into the
perspective-viewcanvas(top).(2)Theusersketchesaperspectiveviewofthesite,and
then(3)theytriggerourtrainedmodeltoinferthe3Dshapeofthesketchedbuildings.
The user can then refine their ideas, iterating between 2D sketching and 3D visuals.
We propose GroundUp, a sketch-based 3D modeling tool for city massing.
AsshowninFig.1andthevideo,itworksbygettingtheusertodrawandrefine
their ideas in two views: a top-down “plan” sketch and a perspective sketch. In
bothviews,userscanoptionallysketchontopofbackprojectedlinesandselected
underlay photos. This helps to iterate or when remodeling of an existing site
is required. Tightly coupled with this interface, our algorithm quickly infers
3D massing-quality geometry. Such 3D geometry, once approved, can be refined
outsideofGroundUpandusedinthedownstreamstagesofarchitecturaldesign.
The model intertwined with this interface faces multiple challenges. Com-
pared to photos, sketch lines only provide a sparse signal about the scene. Be-
tween a top-down and perspective sketch, it is hard to expect texture regions to
match in appearance, making off-the-shelf approaches targeting 3D reconstruc-
tion from multi-view images [20,51,70] inapplicable to our problem. Addition-
ally,urbanareasareinherentlycomplexscenes,soperspectiveviewsthatconvey
building heights and roof shapes also suffer from extreme self-occlusions. With
many unobserved or partially-observed regions, we turn to diffusion as a gener-
ative formulation that could help our method to reconstruct plausible building
shapes (Fig. 1(3)).
Critically,themodelupdatesmustberesponsiveforthesystemtobeusable,
imposing trade-offs between interactivity and the geometric quality from our
adapted latent diffusion model.
Our proposed solution to these challenges offers the following contributions:
– GroundUp is the first system for quick 2D sketch-based iteration on 3D
massing design of city blocks.
– Our novel sketch-to-depth prediction network for perspective sketches ex-
ploits the top-down view’s cues, and necessitated a bespoke training data
process for this important domain.GroundUp 3
– We carefully design our top-down diffusion model to handle multiple condi-
tions, integrating cues from both top-down and perspective sketches.
2 Related Work
Sketch-based 3D shape modeling systems greatly facilitate the creation of 3D
content,andthefirstproposedsystemscameinthe1970s[13,36].Foranin-depth
review of existing systems, we refer the reader to the comprehensive reviews [2,
5,7]. Here, we focus on works related to our overall goal of urban reconstruction
and papers most related to our method.
2.1 3D Building and City Reconstruction
Theexcitingworksrelatedtotheproblemsofurbanreconstructioncanbeclassi-
fiedintotwosetsofproblems[22]:firstoflayoutgeneration[1,17,35]andsecond,
ofcitymodelingandrendering[40–42,49,82].Inourwork,weaimtoprovidethe
user with direct control of the layout via sketching in a top-down view, rather
thangeneratingitautomatically.Next,manyalgorithms[46,73,79]forarchitec-
turalmodelingtakeasinputpointcloudsorDigitalSurfaceModels(DSMs)that
containbuildingheightinformation,obtainedwithLiDAR(LightDetectionand
Ranging)orphotogrammetry[90].Incontrast,wepursueadifferentgoalofhow
to obtain buildings’ height information from sparse user-provided sketches.
Multiple works utilize convolutional neural networks (CNNs) for monocular
depth estimation from a satellite image [9,10,27,56,58] and building segmenta-
tioninasatelliteimage[8,47,55],orboth[47,55].Inthefirststageofourmethod,
we also rely on a CNN to obtain a segmentation of a top-down sketch into in-
dividual buildings. We then propose to inject this information into a monocular
depth estimation network that takes a perspective sketch as an input – the step
that we show is paramount in the context of sparse sketch inputs.
2.2 3D from Sketches
3D representations: Sketch to 3D inference has been based on voxel-based rep-
resentations [15], point clouds [78,93], implicit functions [12,32,94], and 3D
diffusion models [3]. Existing methods have a restricted ability to reconstruct
details and to scale to larger scenes (e.g. multiple objects). We aim for the
prompt reconstruction of multiple object shapes within an interactive interface.
Our method controls for computational complexity and reduces memory foot-
print by regressing only 2.5D information, which is subsequently converted to a
3D mesh. We leverage depth and normal maps as intermediate representations.
Usingintermediaterepresentationssuchasdepthandnormalmapsisacommon
approach in sketch-based 3D reconstruction [26,74,81]. Just as we leverage a
U-Net architecture [67], several works do so to predict multi-view depth and
normal maps [45,53,95]. These methods then fuse the maps to a 3D shape. In
contrast, aiming at complex scenes with multiple occlusions, we predict only4 Ünlü et al.
one perspective view map and rely on a diffusion model to predict a plausible
heightfiled, matching perspective and top-down views. Recent work targeting
lifting sketches of machine-made shapes to 3D [64], similar to us, first predicts
depth.Theirfullmethodfocusesonthereconstructionofsharpedges.However,
ittakesabouttwominutesforsingleobjectinferenceonaverage.Incomparison,
our method runs end-to-end in under 2.7 seconds on multi-building scenes.
Ambiguity of 3D reconstruction: For single-view and even sparse multi-view
reconstruction, unobserved regions create uncertainty, on top of the shape am-
biguity of the observed geometric surfaces. Learning of shape category priors
is one of the most prominent approaches to dealing with sparse sketch in-
puts [3,11,32,78,88,92,93]. To further alleviate uncertainty, in single object
modeling, symmetry can be leveraged [33,84]. Other approaches regress param-
eters of predefined procedural programs [60,62], or assume availability of addi-
tional information [44,91]. For our task of modeling 3D building shape masses
in city neighborhoods, we have more pronounced uncertainty from the multi-
ple layers of occlusions in any perspective view. Additionally, 3D buildings and
theirgroupsalsohaveirregularshapescoveringdiversegeometricconfigurations.
Therefore, we leverage a generative diffusion model that allows us to obtain
plausible-looking building masses from two sparse sketch constraints (Fig. 1).
Urban modeling: Nishidaet al.[60]exploreddata-driveninferenceofprocedural
grammars for individual building reconstruction. The reconstruction ability of
such methods is limited to what is possible to represent with the considered
grammar. Also, their approach cannot infer the shape from a complete draw-
ing and assumes a specific drawing order, matching the grammar used. Liu et
al. [52] extends procedural modeling to VR sketch inputs. Vitruvio [76] targets
individual3Dbuildingreconstructionfrominputsketches.Thepaperstressesthe
importance of perspective 2D sketch-based modeling in architectural applications
forearlyideadevelopment.Theirmethodadoptsanoccupancynetwork[57]that
is either fine-tuned or trained from scratch on synthetic sketches of individual
buildings.However,theirresultsshowblobbyreconstructionswithsomefloating
geometry pieces, typical in implicit 3D shape representations such as occupancy
grids and signed distance fields [54,59,61]. Our heightfield representation allows
us to obtain higher quality reconstructions of multiple buildings in one scene.
2.3 Depth Estimation from RGB Images
Sketches are harder for shape inference than RGB images, but we draw lessons
nonetheless. For calibrated stereo pairs [14,38,87] or unstructured views with
known poses [25,31,71], cost volumes reveal metric depth by matching pho-
tometric appearance between views. Unfortunately, the winning disparities are
misleading with our textureless sketches. For depth from a single image, recent
methods rely on a learned prior for depth estimation [21,30,89]. Follow-ups uti-
lize 3D point networks [86] to combat scale ambiguity, dataset mixing [65] for
more generalizable models, classification heads [24] for improved accuracy, orGroundUp 5
generative models [19,39,69] for sharper depth maps. Recent methods combine
the two: cost volumes and strong image priors, to produce sharp metric depths
from multiple views [20,51,70]. Rather than relying on photometric matching,
we utilizea top-downsketchin anoccupancy volumeto resolvescale ambiguity.
3 Method
Oursupervisedmodelistightlycoupledwiththeuser-facing2Dand3Dinterface
describedinSec.1andinFig.1.Themodelhasseveralcomponents,shownand
summarised in Fig. 2.
ResNet-50
Backproject to a
3D Point Cloud Mul�-C ondi�on
Diffusion
- top-down - perspective - perspective
I. Input sketches UNet++ view depth top- 3D mesh
UNet++ down depth
Point Cloud com−pl etion
Feature - foreground Render onto the
occupancy buildings inRiv �eo saNl lu izem at�5e 0o n mask ground plane depth int itio ap l- id zo aw tion n skeP tce hrs ap le igcti nmve ent
⋆ −
II. Top-down Mask Predic�on III. Perspec�ve Depth Predic�on IV. Top-down Height Condi�on V. Top-down Building Depth Comple�on VI. Final 3D Model
Fig.2: Reconstruction pipeline overview. (I.) From input sketches, (II.) we es-
timate the segmentation of the top-down sketch into individual buildings (as detailed
in Sec. 3.1). (III.) We then inject the volumetric information about the spaces not oc-
cupied by buildings (based on the segmentation result and using a known perspective
camerafromourinterface)intothenetworkthatpredictsdepthandaforegroundmask
fortheperspectivesketchview(furtherdetailedinSec.3.2).(IV.)Fromthepredicted
depth values, we obtain a partial 3D point cloud of the user-envisioned 3D city block.
(V) By projecting a sparse 3D prediction into a top-down view, we obtain an initial
guess for a top-down view heightfield. Finally, we rely on a diffusion model to obtain
a plausible 3D reconstruction that aligns with the perspective and top-down sketches
(as shown in V-VI. and detailed in Sec. 3.3).
3.1 Building occupancy mask estimation for top-down sketches
First, given a top-down sketch S we aim to obtain building occupancy M , and
t t
instancesegmentationM⋆ maps(Fig.2II.):Weuseasubscriptttodenotemaps
t
of the top-down views. Our top-down occupancy prediction network follows a
UNet++ architecture: an encoder-decoder network with dense and nested skip
connections introduced in [97]. As an encoder, we use ResNet-50 [34], initialized
withtheweightsofthemodelpre-trainedonImageNet[16].Wetrainournetwork
with a weighted binary-cross entropy (BCE) loss
N
1 (cid:88)(cid:104) (cid:105)
L =− λ [y log(p )]+λ [(1−y )log(1−p )] , (1)
mask N 1 i i 0 i i
i=16 Ünlü et al.
wherey andp aretheground-truthandpredictedmaskvaluesfortheith pixel,
i i
respectively.λ andλ aretheweightsforgroundandbuildingclasspredictions,
0 1
respectively.Weempiricallyfoundthatabiggerweightλ forthebuildingpixels
1
improves mask prediction performance, accounting for class imbalance as build-
ings occupy a smaller area in the image. We provide further implementation
details in the supplemental.
We then segment into individual buildings, M⋆, by applying Connected-
t
Component Labeling [68] (Fig. 2 II.). We use this building-level segmentation
M⋆ for visualization of the 3D reconstruction results in our UI (Fig. 2 VI).
t
3.2 Depth prediction from perspective sketches
Given a perspective sketch S and a top-down building mask M , we aim to
p t
predict perspective depth maps D and foreground masks M (Fig. 2 III.) – We
p p
use a subscript p to denote maps of the perspective views. In contrast to the
masks in Sec. 3.1, M labels both the building and ground pixels as foreground,
p
with the background being sky pixels.
Network design Predicting depth from a single sparse sketch is an ill-posed
problem. Moreover, in our scenario, each sketch can be quite complex with mul-
tiple buildings and occlusions. We design our perspective view depth predictor
to handle such complex urban scenes.
Our architecture is inspired by a multi-view depth estimation method [70].
ThebackboneofthisnetworkisaUNet++architecture,identicaltotheonewe
introduced in Sec. 3.1. To reduce ambiguity in a perspective view, we leverage
top-downviewinformation.However,applyingthemulti-viewstereoasin[70,83]
is not feasible, as our views have little visual overlap so it is infeasible to per-
form meaningful feature matching between such views. Instead, we exploit the
fact that the top-down building occupancy mask, M , provides information on
t
whether a location in 3D space is free. We construct a 3D occupancy volume,
whichisalignedwiththeperspectiveviewfrustum.Weconstructitbyslicingthe
3D view frustum of the perspective camera with n depth planes at equidistant
intervals between the near d and far planes d . We populate the occupancy
near far
volume by setting all voxels that fall above non-occupied regions to −ν and all
voxels above occupied regions to ν. We discuss the choice of ν in detail in the
supplemental material. Intuitively, we pick ν to be sufficiently large, but within
therangeofourencoderfeatures.Wefeed3Doccupancyfeaturesasinputtothe
UNet++ encoder. Namely, the 3D occupancy features are of shape D×H×W,
whereD isthenumberofdepthplanes.Whenfeedingthesefeaturesintothe2D
encoder in the UNet++, we consider depth planes as image feature channels C.
Then,similarlyto[70],wepasstheinputsketchthroughaResNet-50encoderto
obtainmulti-levelfeatures. StartingfromthefirstlayeroftheUNet++,atevery
secondlayer,weconcatenateoutputfeatureswithcorrespondingencodedsketch
features. This network design allows us to efficiently inject top-down sketch in-
formation,resultinginmoreaccurateperspectivedepthpredictions.Weprovide
the ablation study of our design in Sec. 4.GroundUp 7
Training During training, we use ground-truth M building occupancy masks.
t
We train our depth predictor with a weighted sum of four loss terms, so
L =ω L +ω L +ω L +ω L , (2)
D d depth g grad n p,norm m mask
whereω denotestheweightofthecorrespondinglosscomponent.Weintroduce
∗
each term below.
L is a multi-scale loss on depth predictions, that was shown to provide
depth
sharperdepthmapsatdepthdiscontinuitiesthanalossappliedonlyatthefinal
depthmapresolution[21,28,29,70].Followingpreviousworks,wepredictdepths
at four resolutions from different levels of our UNet++ decoder, such that at
each of the subsequent scales the spatial resolution is doubled. It is defined as
S
L depth =(cid:88)(cid:13) (cid:13)(D p) s−(D pgt) s(cid:13) (cid:13) 1 (3)
s=1
(cid:13) (cid:13)
where (cid:13)·(cid:13)
1
is the L 1-norm and (D ps,D pg st) are the predicted and ground-truth
depth maps at the sth scale.
Similarly, inspired by [48,70], to encourage smoother gradient changes and
sharper depth discontinuities in predicted depth maps, we use a multi-scale loss
L that penalizes differences in depth gradients between the predicted and
grad
ground-truth depth map:
S
(cid:88)(cid:13) (cid:13) (cid:13) (cid:13)
L grad = (cid:13)∇ xR s(cid:13) 1+(cid:13)∇ yR s(cid:13) 1, (4)
s=1
where R =(D ) −(Dgt) .
s p s p s
Following Yin et al. [85], who showed that a geometric constraint on nor-
mal maps improves monocular depth estimation, we use a loss L between
p,norm
ground-truth Ngt and predicted N normal maps:
p p
N
(cid:88)
L = (1−(N ) ·(Ngt) ), (5)
p,norm p i p i
i=1
wherewesumoverthedotproductsofnormalvectors(N∗) ∈R3 incorrespond-
p i
ingnormalmaplocationsi.Weobservedthatthislossimprovestheperformance
in our setting as well. Both N and Ngt are computed on the fly from their cor-
p p
responding depth maps.
Finally,L isaweightedBCEloss,definedsimilarlytotheoneinEq.(1).
mask
We use it to segment out building and ground pixels.
3.3 Conditional diffusion model for 3D building reconstruction
In the previous section, we described how we obtain a depth estimation for a
perspective sketch view. As the next step, we backproject the depth map to8 Ünlü et al.
obtain a 3D point cloud. From this point cloud, we initialize a heightfield of
the city block aligned with the top-down user sketch. To account for possible
inaccuracies in the depth prediction network, we leverage a mask M , predicted
t
with our building occupancy mask estimation network as described in Sec. 3.1.
We set all heightfield predictions that fall outside the occupied regions to a
constant ground-level value. We then use a diffusion model conditioned on the
inputsketchandtheinitial heightfieldfromthe perspectivesketch viewtocom-
plete missing depth regions in the top-down view. Since our conditioning relies
on both views, the model predicts plausible 3D buildings that align with user
sketches.Notethat,duringtraining,weinitializeheightfieldsusingground-truth
perspective view depth maps.
Networkarchitecture WebuildonthelatentspacediffusionmodelbyDuanet
al. [19], adapting it to handle multiple conditions. We chose a latent diffusion
modelduetoitsmemoryefficiencyandinferencespeedcomparedtoimage-space
diffusion models.
We map ground-truth depth maps to a latent space using a depth encoder:
z =E (Dgt). Additionally, we encode sketch and depth conditions: c =
depth t sketch
E (C ) and c = E (C ), respectively. We initialize E and
sketch St depth depth Dt sketch
E with pre-trained weights. Specifically, for E , we employ a ResNet-
depth sketch
50 architecture pre-trained on ImageNet. For the depth encoder, we employ
the one from the Stable Diffusion [66]. We pre-train the autoencoder following
theirstrategyandsuperviseusingground-truthtop-downdepthmapsDgt using
t
KL-regularization in the latent space. We fine-tune both latent encoders when
training the full model.
Toconstructthefinalinputtothedenoisingnetwork,wecombinethesketch,
c , and depth, c , conditions with the noisy depth latent z , for a given
sketch depth k
noise level k. To align features, we pass latent representations c and z
depth k
through two separate CNNs, consisting of two convolutional layers. The final
denoisingnetworkinputiscreatedbycombiningsketchlatentfeaturesanddepth
conditions with the noisy depth latent through an element-wise summation.
Training The training objective for the diffusion process is defined as
L =E [∥ϵ −ϵ (z ,c ,c ,k)∥]2, (6)
diff k∼[1,T],zk,ϵk k θ k St Dt
where ϵ and ϵ are the ground-truth and predicted noise maps, at timestep k.
t θ
Additionally,weuseauxiliarypixel-basedlossestohelptraintheconditioning
process. Firstly, L and L losses on predicted D and ground-truth Dgt depth
1 2 t t
maps are used, defined as
L L1
=(cid:13)
(cid:13)D t−D
tgt(cid:13)
(cid:13) 1 and L L2
=(cid:13)
(cid:13)D t−D
tgt(cid:13)
(cid:13) 2. (7)
WealsousealossonnormalmapsL ,definedsimilarlytotheoneinEq.(5).
t,norm
We find that this loss results in sharper, more uniform depth predictions. We
ablate its effect in Sec. 4.GroundUp 9
The complete objective loss of our top-down heightfield completion diffusion
model is defined as
L =L +L +L +L . (8)
total diff L1 L2 t,norm
3D mesh: From the ground up Finally,toobtaina3Dmesh,wecreatea3D
mesh grid M3D ∈ RN×N×3 with N ×N vertices, where N is the width/height
of the top-down depth map where the horizontal x and vertical y axes map to
pixel coordinates. We obtain the height of each vertex v in M3D as
ij
vz =d −(D ) , (9)
ij ground t ij
where d is the depth value of the ground plane and (D ) is the predicted
ground t ij
top-downdepthatpixellocation(i,j).Weassignd tothemaximumdepth
ground
value in D .
t
4 Experiments
In this section, we evaluate our method on synthetic sketches. We first evaluate
our perspective depth prediction network and discuss the importance of various
designchoices.Wethenassessourcompletemethod,byevaluatingourtop-down
completion network on inputs predicted by the perspective depth network. We
compare with a few alternative baselines and ablate our design choices. The
details of data generation and splits are provided in the supplemental.
Perspective depth prediction In Tab. 1, we assess our design choices for
the perspective depth prediction network and compare against several baselines
using standard depth metrics [21]. Briefly, Abs Diff is the absolute difference
between ground-truth and predicted depth maps, Abs Rel is the absolute differ-
encenormalizedbytheground-truthdepthmap,SqRel isthesquareofAbsRel,
RMSE is the root mean square error between both depth maps, Log RMSE is
therootmeansquareerroronloggeddepths,anda5 istheratioofpixelswhose
depth values have a relative depth error lower than 5%.
Baselines: We train a naive monocular depth predictor baseline from Sayed et
al. [70] without a cost volume (no source views for multi-view stereo), which we
refer to as Mono, and compare two image encoder backbones. In Tab. 1, lines
[1-2] refer to Mono for a smaller (EfficientNet [75]) and Mono for a larger
S L
encoder (ResNet-50 [34]). A large image encoder leads to superiority across all
depth metrics, with a minimal increase in inference speed – 0.16s on average
per sample. Given this, we use this larger backbone for all other experiments.
Ablations: In Tab. 1, OV represents our model with the occupancy volume
obtained as described in Sec. 3.2. We empirically found ν = 50 to give the
best results. This value is close to the mid-point of the range of multi-scale10 Ünlü et al.
Model Abs Diff ↓Abs Rel↓Sq Rel↓RMSE↓Log RMSE↓ a5↑
1 Mono 6.64 5.33 0.79 10.06 9.41 66.13
S
2 Mono 5.57 4.33 0.53 8.58 7.44 68.82
L
3 OV 6.23 5.24 0.74 9.28 9.55 67.80
S
4 OV 3.49 2.13 0.21 6.54 3.4389.20
L
Table 1: Quantitative evaluation of the perspective depth estimation.Mono
stands for a monocular depth predictor baseline by Sayed et al. [70], where subscripts
S and L define a smaller and larger encoder backbones, respectively. OV represents
our model with the occupancy volume obtained as described in Sec. 3.2. Please see
Sec. 4 for the details. All metric values apart from a5 are scaled up by 102.
image features. We hypothesize that this setting allows the network to leverage
the occupancy information most beneficially. We provide a detailed analysis of
the choice of ν in the supplemental. Lines [4] vs. [3] show the advantage of
the larger encoder backbone. Our complete model then comprises a ResNet-50
encoderbackbone,andanoccupancyvolumewithvoxelsassignedusingν =50.
Comparison: InTab.1,lines[3-4]vs[1-2]showthattheOV modelsoutperform
Monomodels.WeshowaqualitativecomparisonoftheMonobaselinewithour
OV methodinFig.3,showingtheimportanceoftheproposedoccupancyfeature
volume for correcting for spatial ambiguity from single-view depth estimation.
TopView Mono (Baseline) TopView OV (Ours)
Fig.3: Qualitative evaluation of the perspective depth estimation. Mono
stands for a monocular depth predictor baseline by Sayed et al. [70]. OV represents
our model with the occupancy volume, obtained as described in Sec. 3.2. Grey mesh
corresponds to the geometry obtained from the ground-truth heightfield. Point clouds
represent the estimated depth values from a perspective sketch. Colors encode the
distance from a camera. Our prediction visually aligns better with the ground-truth.
Top-down depth completion Our final goal is to infer plausible building
geometriesfromtop-downS andperspectiveS sketches(Fig.5[a,b]).Namely,
t p
werelyonthetop-downsketchtorecoverbuildinglayoutsandontheperspective
sketchtoestimatebuildings’heights.Weobtainheightcueswiththeperspective
depth prediction network. Then, the aim of our diffusion model, introduced in
Sec. 3.3, is to produce top-down depth maps faithful to the top-down sketch S
t
and height cues C .
DtGroundUp 11
Model Abs Diff↓Abs Rel↓Sq Rel↓RMSE↓
S +C (fs) 0.1090 0.0221 0.0042 0.1246
t Dt
S +C (pt) 0.1079 0.0218 0.0040 0.1221
t Dt
S +C (pt) + L 0.1056 0.0214 0.0039 0.1200
t Dt t,norm
HeightFields [80] 0.1099 0.0225 0.0046 0.1341
Table 2: Quantitative analysis of top-down depth prediction. (fs) denotes
training sketch and depth encoders from scratch jointly with the diffusion model. (pt)
referstopre-trainedencodersforsketchanddepthconditions,asdescribedinSec.3.3.
S +C denotesthatweusetwoconditions:atop-downsketchandapartialtop-down
t Dt
depth prediction based on the perspective sketch view. The numbers in the first two
lines represent diffusion models trained with the losses defined by equations Eqs. (6)
and (7), while the last line represents the model trained with the full loss Eq. (8).
Model Acc↓Compl↓Chamfer↓Precision↑Recall↑F Score↑
S +C (fs) 3.89 2.91 3.40 81.6 87.7 84.2
t Dt
S +C (pt) 3.79 2.90 3.34 81.8 88.1 84.4
t Dt
S +C (pt) + L 4.35 3.54 3.94 76.1 82.0 78.3
t Dt t,norm
HeightFields [80] 5.35 6.99 6.17 70.4 63.9 66.0
Table 3: Quantitative 3D evaluation of the final reconstructed meshes. The
metrics in this table account for the visibility of 3D geometry in a perspective sketch
view. Please see Sec. 4 for details. The notation in this table matches the caption of
Tab.2.Wedetailsonthemetrics:Completion,Accuracy,ChamferDistance,Precision,
Recall, and F-Score can be found in [6].
We first ablate the design of our network and then compare it with a deter-
ministicbaseline.Forevaluations,weusemetricsin2D(Tab.2)and3D(Tab.3),
comparingagainsttheground-truth.For2Devaluation,weusemetricssimilarto
the ones in Tab. 1. Since we focus on buildings and not the terrain, we compute
all2Dmetricsonlywithinbuildings’ground-truthregions,usingbuildingmasks
M .Weevaluate3Dmetricsonlyforthepartsofgeometriesobservedintheper-
t
spective sketch viewpoints. This allows us to focus the evaluation on regions for
which the perspective sketches provide explicit control of the buildings’ heights.
Beforecomputingsampledpointclouddistancesbetweenpredictedandground-
truth meshes, we remove points not in the region around the back-projected
ground-truth perspective depth map.
Role of pre-training: Tab. 2 demonstrates the importance of pretraining sketch
and depth encoders, E and E , respectively. (fs) refers to training the
sketch depth
encoders from scratch jointly with the diffusion model and (pt) refers to pre-
training latent encoders for sketch and depth conditions.
Role of normal loss: We show the qualitative evaluation of the role of the nor-
mal loss in Fig. 4. It shows that the normal loss yields building geometries with
sharper corners and flat building tops. Computed on all building regions, 2D
losses in Tab. 2 show that the normal loss L , defined with Eq. (8), sig-
t,norm12 Ünlü et al.
a. Ground-truth b. With L c. Without L
norm norm
Visibility View 1 View 2 View 1 View 2
Fig.4:Roleofthenormalloss.a)Visibilityregions(redpoints)arecomputedbased
on ground-truth geometry and the perspective sketch viewpoint. b) Prediction when
thenormallossisused:Theredpointcloudisridingslightlyabovethegreenprediction.
Asshowninview2 theheightisslightlyunderestimatedinthevisibleregions,butthe
loss results in more even roofs overall. c) Prediction when the normal loss is not used:
the model produces blobby building geometry outside the visible regions.
nificantly improves the accuracy of top-down depth-predictions – reflecting on
the overall appearance of the buildings. 3D metrics in Tab. 3, computed only
on visible regions from the perspective sketch viewpoint, highlight a slight ge-
ometry shrinkage, visible in View 2 in Fig. 4. While adding a normal loss hurts
quantitative 3D metrics, we advocate its usage as it produces much sharper and
smoother surfaces, as shown in Fig. 4 and supported with Tab. 2.
Comparison with a deterministic baseline: Qualitativeresultsofourmethodare
shown in Fig. 5 (d) and (g): We can infer realistic building geometries follow-
ing input sketches that closely resemble the ground-truth – Fig. 5 (e) and (h).
We compare our generative approach against the HeightFields [80] baseline – a
deterministic model designed for heightfield completion from multi-frame RGB
sequences. We train and test it on the same input as our model and visualize
theresultsinFig.5(c)and(f).Inparticular,theHeightFields model’stesttime
input is the output of our first step: the predicted partial point cloud from a
perspective view. This model is not a suitable stand-alone method for the task.
Totrainthismodel,wealsoaddedL ,aswefoundittoresultinbetterper-
t,norm
formance. However, even with this additional loss, the HeightFields model fails
to produce buildings with correct heights, and produces less plausible building
geometries. In particular, it fails to capture sharp details and flat rooftops.
Tabs. 2 and 3 show quantitative comparison of our full model with Height-
Fields [80] baseline. They show the superiority of our diffusion model in all
settings, confirming the visual observations.
5 User Study
Modeling Interface: To validate our contributions, we built an interactive user
interface in HTML, JavaScript, and Python. The 3D massing system runs real-
time on a Titan X and can be used on any touch-screen device thorough aGroundUp 13
a. Top-down b. Perspectivec. HeightFields d. Ours e. Groundtruth f. HeightFields g. Ours h. Groundtruth
sketch sketch Perspective Perspective Perspective Top-down Top-down Top-down
Fig.5: Qualitative evaluation on synthetic sketches. (a) and (b) show example
top-down and perspective sketches. (c) and (f) show reconstruction results obtained
withtheHeightFields [80]method,whichistrainedandtestedonthesamedataasour
method. (d) and (g) show reconstruction results by our method. (e) and (h) show the
heightfieldoftheground-truthtop-downdepthmap.Notethatthecolorsareassigned
according to the ground-truth segmentation of buildings. Please zoom in to better see
the alignment of predicted geometries with the ground-truth buildings’ areas.
browser, ideally with a stylus. Broadly, the UI lets users sketch perspective and
top-downviewson2Dcanvases,editstrokes,projectatop-downsketchintothe
perspectivecanvastoaligntheirsketches,andcontainsa3Dviewer.Anoverview
of the UI is in Fig. 1 and is described in greater detail in the supplemental.
Evaluation: To validate our system, we run a proof-of-concept user study. For
thestudy,wecollaboratedwithoneoftheworld-leadingschoolsinurbandesign,
the Bartlett School of Architecture, at University College London. We engaged
5 urban design architects: 2 undergraduate students, advanced in their studies,
and3postgraduateswithvaryingyearsofprofessionalpractice.Additionally,to
test how friendly our system is for users with limited modeling and sketching
experience, we engaged 5 further volunteers. All users watched a short video
tutorial and had 5 minutes to play with the interface before starting the task.
To have a concrete qualitative goal in our main study, we chose to provide
participants with reference top-down and perspective renderings as underlays
(the example screenshot is provided in the supplemental). We selected 9 scenes,
randomly distributed between participants. Each participant drew two scenes.
In a post-study questionnaire, all architects indicated that they were able
to recreate the building from the reference in under 5 minutes. As expected, it
was more challenging for novices, yet, 2/5 were satisfied with the outcome. On
a 5-point Likert scale, architects (novices) gave an average score of 0.8 (1.2) on
how well the results match the reference, with +2 for matching the reference
well and −2 for failing completely. On a 5-point Likert scale, architects gave an
average 1.4 score on how likely they are to use such an interface: where −2 for
highly unlikely and +2 for highly likely. This analysis shows that overall our14 Ünlü et al.
Freehand sketches Reconstruction
a. Top-down b. Perspective c. Top-down d. Perspective
Fig.6: Freehandsketchesandcorresponding3Dreconstructionsinouruserinterface,
made by urban design architects in our study. (e) shows automatically post-processed
results, rendered with an offline rendered, as described in the supplemental.
system achieves a set goal of fast prototyping of building masses, while future
work could aim to further improve the reconstruction accuracy. The detailed
statistics for the post-study questionnaire are provided in the supplementary.
In our pilot study, architects consistently indicated that it would take them
about10minutesinRhinoforscenescomparabletotheoneswetarget.Thepilot
studyonSketchUp,documentedinthesupplemental,similarlyshowedthatitis
notsuitableforfastprototyping.This,inparticular,showsthelackofconvenient
tools for early design stages and reinforces the motivation for our work.
Twourbandesignarchitectsalsodidfreehand modeling afterthemainstudy
and completed post-study questionnaires. These sketches are shown in Fig. 6.
6 Conclusion and discussion
We have presented the first sketch-based method for early-stage urban design,
aligning it with the Human-Centered AI philosophy [72]. Taking into account
design workflows that commonly start from top-down city layouts, we proposed
models that, while working in image space, efficiently leverage information from
bothperspectiveandtop-downsketchviews.GroundUpaddressestheespecially
challenging(butnotunique)aspectsofourproblem:complexityanddiversityof
scene geometries, sparsity of sketch inputs, and incomplete depth cues in user-
provided views. While we only show the results for a single perspective sketch,
our system is trivially extended to a multi-view setting: by projecting point
clouds inferred from extra perspective sketches into the top-down views passed
to our diffusion model. We provide numerical experiments in the supplemental.
With this work, we have taken a step toward quick building massing. To propel
the integration of our tool into design workflows, future work might focus on
directly predicting editable mesh representations and supporting finer details.
Additionally, it could be interesting to extended this work to trees and terrain,
for example, by sketching trunks and contour lines for the terrain.
1
tcetihcrA
2
tcetihcrAGroundUp 15
Acknowledgements
We thank Prof. Tobias Ritschel for his invaluable feedback and help; Natalia
Laskovaya for an inspiring and detailed early discussion on design processes in
architecture; Sharon Betts for her huge help in making our user study possi-
ble. We also thank Kening Guo and all the anonymous participants of the user
studies. Gizem Esra Ünlü is funded by a Niantic PhD scholarship.
References
1. Benes, B., Zhou, X., Chang, P., Cani, M.P.R.: Urban brush: Intuitive and con-
trollable urban layout editing. In: The 34th Annual ACM Symposium on User
Interface Software and Technology (2021)
2. Bhattacharjee,S.,Chaudhuri,P.:Asurveyonsketchbasedcontentcreation:from
thedesktoptovirtualandaugmentedreality.ComputerGraphicsForum39,757–
780 (05 2020)
3. Binninger, A., Hertz, A., Sorkine-Hornung, O., Cohen-Or, D., Giryes, R.: Sens:
Sketch-based implicit neural shape modeling. Arxiv preprint -(-) (06 2023)
4. Blender Online Community: Blender - a 3D modelling and rendering package.
BlenderFoundation,BlenderInstitute,Amsterdam(2022),http://www.blender.
org
5. Bonnici, A., Akman, A., Calleja, G., Camilleri, K., Fehling, P., Ferreira, A., Her-
muth, F., Israel, J., Landwehr, T., Liu, J., Padfield, N., Sezgin, T., Rosin, P.:
Sketch-based interaction and modeling: where do we stand? Artificial Intelligence
for Engineering Design, Analysis and Manufacturing 33, 1–19 (11 2019)
6. Bozic,A.,Palafox,P.,Thies,J.,Dai,A.,Nießner,M.:TransformerFusion:Monoc-
ular RGB scene reconstruction using transformers. NeurIPS (2021)
7. Camba, J.D., Company, P., Naya, F.: Sketch-based modeling in mechanical en-
gineering design: Current status and opportunities. Computer-Aided Design 150,
103283 (2022)
8. Chen, S., Ogawa, Y., Zhao, C., Sekimoto, Y.: Large-scale individual building ex-
tractionfromopen-sourcesatelliteimageryviasuper-resolution-basedinstanceseg-
mentationapproach.ISPRSJournalofPhotogrammetryandRemoteSensing195
(2023)
9. Chen, S., Shi, Y., Xiong, Z., Zhu, X.X.: Htc-dc net: Monocular height estimation
fromsingleremotesensingimages.IEEETransactionsonGeoscienceandRemote
Sensing 61 (2023)
10. Chen, Z., Zhang, Y., Qi, X., Mao, Y., Zhou, X., Niu, L., Wu, H., Wang, L.,
Ge, Y.: Heightformer: A multilevel interaction and image-adaptive classification-
regression network for monocular height estimation with aerial images. arXiv
preprint arXiv:2310.07995 (2023)
11. Cheng, Z., Chai, M., Ren, J., Lee, H.Y., Olszewski, K., Huang, Z., Maji, S.,
Tulyakov, S.: Cross-modal 3D Shape Generation and Manipulation, pp. 303–321.
Springer (11 2022)
12. Chowdhury, P.N., Wang, T., Ceylan, D., Song, Y.Z., Gryaditskaya, Y.: Garment
ideation: Iterative view-aware sketch-based garment modeling. In: 2022 Interna-
tional Conference on 3D Vision (3DV). pp. 22–31 (2022)
13. Clowes, M.B.: On seeing things. Artificial intelligence 2(1), 79–116 (1971)16 Ünlü et al.
14. Collins, R.T.: A space-sweep approach to true multi-image matching. In: CVPR
(1996)
15. Delanoy, J., Aubry, M., Isola, P., Efros, A.A., Bousseau, A.: 3d sketching using
multi-viewdeepvolumetricprediction.Proc.ACMComput.Graph.Interact.Tech.
1(1) (Jul 2018)
16. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. IEEE (2009)
17. Deng,J.,Chai,W.,Guo,J.,Huang,Q.,Hu,W.,Hwang,J.N.,Wang,G.:Citygen:
Infiniteandcontrollable3dcitylayoutgeneration.arXivpreprintarXiv:2312.01508
(2023)
18. Du,R.,Chang,D.,Hospedales,T.,Song,Y.Z.,Ma,Z.:Demofusion:Democratising
high-resolution image generation with no $$$ (2024)
19. Duan, Y., Zhu, Z., Guo, X.: Diffusiondepth: Diffusion denoising approach for
monocular depth estimation. CoRR abs/2303.05021 (2023)
20. Duzceker, A., Galliani, S., Vogel, C., Speciale, P., Dusmanu, M., Pollefeys, M.:
Deepvideomvs: Multi-view stereo on video with recurrent spatio-temporal fusion.
In: CVPR (2021)
21. Eigen,D.,Puhrsch,C.,Fergus,R.:Depthmappredictionfromasingleimageusing
amulti-scaledeepnetwork.In:Ghahramani,Z.,Welling,M.,Cortes,C.,Lawrence,
N.D.,Weinberger,K.Q.(eds.)AdvancesinNeuralInformationProcessingSystems
27:AnnualConferenceonNeuralInformationProcessingSystems2014,December
8-13 2014, Montreal, Quebec, Canada. pp. 2366–2374 (2014)
22. Feng,T.,Fan,F.,Bednarz,T.:Areviewofcomputergraphicsapproachestourban
modelingfromamachinelearningperspective.FrontiersofInformationTechnology
& Electronic Engineering 22(7) (2021)
23. Fifty, C., Amid, E., Zhao, Z., Yu, T., Anil, R., Finn, C.: Efficiently identifying
taskgroupingsformulti-tasklearning.AdvancesinNeuralInformationProcessing
Systems 34 (2021)
24. Fu, H., Gong, M., Wang, C., Batmanghelich, K., Tao, D.: Deep ordinal regression
network for monocular depth estimation. In: Proceedings of the IEEE conference
on computer vision and pattern recognition (2018)
25. Furukawa, Y., Hernández, C., et al.: Multi-view stereo: A tutorial. Foundations
and Trends® in Computer Graphics and Vision 9(1-2), 1–148 (2015)
26. Gao, C., Yu, Q., Sheng, L., Song, Y., Xu, D.: Sketchsampler: Sketch-based 3d
reconstruction via view-dependent depth sampling. In: ECCV (2022)
27. Ghamisi, P., Yokoya, N.: Img2dsm: Height simulation from single imagery using
conditionalgenerativeadversarialnet.IEEEGeoscienceandRemoteSensingLet-
ters 15(5) (2018)
28. Godard, C., Aodha, O.M., Brostow, G.J.: Unsupervised monocular depth estima-
tion with left-right consistency. In: 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. pp.
6602–6611. IEEE Computer Society (2017)
29. Godard,C.,Aodha,O.M.,Firman,M.,Brostow,G.J.:Diggingintoself-supervised
monocular depth estimation. In: 2019 IEEE/CVF International Conference on
Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,
2019. pp. 3827–3837. IEEE (2019)
30. Godard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocular depth esti-
mation with left-right consistency. In: CVPR (2017)
31. Goesele,M.,Curless,B.,Seitz,S.M.:Multi-viewstereorevisited.In:CVPR(2006)GroundUp 17
32. Guillard, B., Remelli, E., Yvernay, P., Fua, P.: Sketch2mesh: Reconstructing and
editing 3d shapes from sketches. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision (2021)
33. Hähnlein, F., Gryaditskaya, Y., Sheffer, A., Bousseau, A.: Symmetry-driven 3d
reconstructionfromconceptsketches.In:ACMSIGGRAPH2022ConferencePro-
ceedings. pp. 1–8 (2022)
34. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
(2016)
35. He, L., Aliaga, D.: Globalmapper: Arbitrary-shaped urban layout generation.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV). pp. 454–464 (October 2023)
36. Huffman, D.A.: Impossible objects as nonsense sentences. Machine intelligence 6,
295–323 (1971)
37. Jacoby, S.: Drawing Architecture and the Urban. Wiley (2016)
38. Kang, S.B., Szeliski, R., Chai, J.: Handling occlusions in dense multi-view stereo.
In: CVPR (2001)
39. Ke, B., Obukhov, A., Huang, S., Metzger, N., Daudt, R.C., Schindler, K.: Re-
purposingdiffusion-basedimagegeneratorsformonoculardepthestimation.arXiv
preprint arXiv:2312.02145 (2023)
40. Kelly,T.,Femiani,J.,Wonka,P.,Mitra,N.J.:Bigsur:Large-scalestructuredurban
reconstruction. ACM Transactions on Graphics 36(6) (November 2017)
41. Kelly, T., Guerrero, P., Steed, A., Wonka, P., Mitra, N.J.: Frankengan: Guided
detailsynthesisforbuildingmassmodelsusingstyle-synchonizedgans.ACMTrans.
Graph. 37(6), 1:1–1:14 (2018)
42. Kim,S.,Kim,D.,Choi,S.:Citycraft:3dvirtualcitycreationfromasingleimage.
The Visual Computer 36 (2020)
43. Leyton, M.: A generative theory of shape. vol. 2145, p. p366. Springer Berlin /
Heidelberg, Germany (2001)
44. Li, C., Pan, H., Bousseau, A., Mitra, N.J.: Free2cad: Parsing freehand drawings
into cad commands. ACM TOG (2022)
45. Li,C.,Pan,H.,Liu,Y.,Tong,X.,Sheffer,A.,Wang,W.:Robustflow-guidedneural
prediction for sketch-based freeform surface modeling. ACM Trans. Graph. 37(6)
(2018)
46. Li, L., Song, N., Sun, F., Liu, X., Wang, R., Yao, J., Cao, S.: Point2roof: End-to-
end 3dbuilding roof modeling fromairbornelidar pointclouds. ISPRSJournal of
Photogrammetry and Remote Sensing 193, 17–28 (2022)
47. Li, X., Wen, C., Wang, L., Fang, Y.: Geometry-aware segmentation of remote
sensing images via joint height estimation. IEEE Geoscience and Remote Sensing
Letters 19 (2021)
48. Li,Z.,Snavely,N.:Megadepth:Learningsingle-viewdepthpredictionfrominternet
photos. In: 2018 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR2018,SaltLakeCity,UT,USA,June18-22,2018.pp.2041–2050.Computer
Vision Foundation / IEEE Computer Society (2018)
49. Lin,C.H.,Lee,H.Y.,Menapace,W.,Chai,M.,Siarohin,A.,Yang,M.H.,Tulyakov,
S.:Infinicity:Infinite-scalecitysynthesis.In:ProceedingsoftheIEEE/CVFInter-
national Conference on Computer Vision (ICCV) (October 2023)
50. Lin, L., Liu, Y., Hu, Y., Yan, X., Xie, K., Huang, H.: Capturing, reconstructing,
andsimulating:Theurbanscene3ddataset.In:Avidan,S.,Brostow,G.J.,Cissé,M.,
Farinella,G.M.,Hassner,T.(eds.)ComputerVision-ECCV2022-17thEuropean18 Ünlü et al.
Conference,TelAviv,Israel,October23-27,2022,Proceedings,PartVIII.Lecture
Notes in Computer Science, vol. 13668, pp. 93–109. Springer (2022)
51. Lipson, L., Teed, Z., Deng, J.: Raft-stereo: Multilevel recurrent field transforms
for stereo matching. In: 2021 International Conference on 3D Vision (3DV). pp.
218–227. IEEE (2021)
52. Liu,Z.,Zhang,F.,Cheng,Z.:Buildingsketch:Freehandmid-airsketchingforbuild-
ingmodeling.In:IEEEInternationalSymposiumonMixedandAugmentedReality
(ISMAR). IEEE (2021)
53. Lun,Z.,Gadelha,M.,Kalogerakis,E.,Maji,S.,Wang,R.:3dshapereconstruction
from sketchesvia multi-view convolutional networks.In: InternationalConference
on 3D Vision (3DV) (2017)
54. Luo, L., Chowdhury, P.N., Xiang, T., Song, Y.Z., Gryaditskaya, Y.: 3d vr sketch
guided 3d shape prototyping and exploration. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision (2023)
55. Mahdi,E.,Ziming,Z.,Xinming,H.:Aerialheightpredictionandrefinementneural
networks with semantic and geometric guidance. arXiv preprint arXiv:2011.10697
(2020)
56. Mahmud, J., Price, T., Bapat, A., Frahm, J.M.: Boundary-aware 3d building re-
constructionfromasingleoverheadimage.In:ProceedingsoftheIEEE/CVFCon-
ference on Computer Vision and Pattern Recognition (2020)
57. Mescheder, L.M., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy
networks: Learning 3d reconstruction in function space. In: IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,
June 16-20, 2019. pp. 4460–4470. Computer Vision Foundation / IEEE, Long
Beach, CA, USA (2019)
58. Mou, L., Zhu, X.X.: Im2height: Height estimation from single monocular im-
agery via fully residual convolutional-deconvolutional network. arXiv preprint
arXiv:1802.10249 (2018)
59. Nam, G., Khlifi, M., Rodriguez, A., Tono, A., Zhou, L., Guerrero, P.: 3d-ldm:
Neural implicit 3d shape generation with latent diffusion models. arXiv preprint
arXiv:2212.00842 (2022)
60. Nishida, G., Garcia-Dorado, I., Aliaga, D.G., Benes, B., Bousseau, A.: Interactive
sketching of urban procedural models. ACM Transactions on Graphics (TOG)
35(4) (2016)
61. Park,J.J.,Florence,P.,Straub,J.,Newcombe,R.,Lovegrove,S.:Deepsdf:Learning
continuous signed distance functions for shape representation. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition (2019)
62. Pearl,O.,Lang,I.,Hu,Y.,Yeh,R.A.,Hanocka,R.:Geocode:Interpretableshape
programs. arXiv preprint arXiv:2212.11715 (2022)
63. Pitts,G.,Luther,M.:Aparametricapproachto3dmassinganddensitymodelling.
In: DigitalPhysicality: Proceedings ofthe30th eCAADeConference. pp.157–165
(2012)
64. Puhachov,I.,Martens,C.,Kry,P.G.,Bessmeltsev,M.:Reconstructionofmachine-
made shapes from bitmap sketches. ACM Trans. Graph. 42(6) (2023)
65. Ranftl, R., Lasinger, K., Hafner, D., Schindler, K., Koltun, V.: Towards robust
monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer.
IEEE transactions on pattern analysis and machine intelligence 44(3), 1623–1637
(2020)
66. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: IEEE/CVF Conference on Com-GroundUp 19
puterVisionandPatternRecognition,CVPR2022,NewOrleans,LA,USA,June
18-24, 2022. pp. 10674–10685. IEEE, New Orleans, Louisiana, USA (2022)
67. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: Medical Image Computing and Computer-Assisted
Intervention–MICCAI2015:18thInternationalConference,Munich,Germany,Oc-
tober 5-9, 2015, Proceedings, Part III 18. Springer (2015)
68. Rosenfeld,A.,Pfaltz,J.L.:Sequentialoperationsindigitalpictureprocessing.Jour-
nal of the ACM (JACM) 13(4), 471–494 (1966)
69. Saxena, S., Kar, A., Norouzi, M., Fleet, D.J.: Monocular depth estimation using
diffusion models. CoRR abs/2302.14816 (2023)
70. Sayed, M., Gibson, J., Watson, J., Prisacariu, V., Firman, M., Godard, C.: Sim-
plerecon:3dreconstructionwithout3dconvolutions.In:ComputerVision-ECCV
2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Pro-
ceedings, Part XXXIII. Lecture Notes in Computer Science, vol. 13693, pp. 1–19.
Springer (2022)
71. Schönberger, J.L., Zheng, E., Frahm, J.M., Pollefeys, M.: Pixelwise view selection
for unstructured multi-view stereo. In: Computer Vision–ECCV 2016: 14th Euro-
peanConference,Amsterdam,TheNetherlands,October11-14,2016,Proceedings,
Part III 14. pp. 501–518. Springer (2016)
72. Shneiderman, B.: Human-centered AI. Oxford University Press (2022), https:
//books.google.co.uk/books?id=YS9VEAAAQBAJ
73. Stucker, C., Schindler, K.: Resdepth: A deep residual prior for 3d reconstruction
from high-resolution satellite images. ISPRS Journal of Photogrammetry and Re-
mote Sensing 183 (2022)
74. Su, W., Du, D., Yang, X., Zhou, S., Fu, H.: Interactive sketch-based normal map
generation with deep neural networks. Proceedings of the ACM on Computer
Graphics and Interactive Techniques 1(1) (2018)
75. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural
networks. In: International conference on machine learning. PMLR (2019)
76. Tono, A., Huang, H., Agrawal, A., Fischer, M.: Vitruvio: 3d building meshes via
single perspective sketches. arXiv preprint arXiv:2210.13634 (2022)
77. Ünlü, G., Sayed, M., Brostow, G.J.: Interactive sketching of mannequin poses.
In: International Conference on 3D Vision, 3DV 2022, Prague, Czech Republic,
September 12-16, 2022. pp. 700–710. IEEE (2022). https://doi.org/10.1109/
3DV57658.2022.00080, https://doi.org/10.1109/3DV57658.2022.00080
78. Wang, J., Lin, J., Yu, Q., Liu, R., Chen, Y., Yu, S.X.: 3d shape reconstruction
from free-hand sketches. In: Karlinsky, L., Michaeli, T., Nishino, K. (eds.) ECCV
Workshops (2022)
79. Wang, Y., Zorzi, S., Bittner, K.: Machine-learned 3d building vectorization from
satellite imagery. In: IEEE Conference on Computer Vision and Pattern Recogni-
tionWorkshops,CVPRWorkshops2021,virtual,June19-25,2021.pp.1072–1081.
Computer Vision Foundation / IEEE, Virtual (2021)
80. Watson, J., Vicente, S., Aodha, O.M., Godard, C., Brostow, G.J., Firman, M.:
HeightfieldsforefficientscenereconstructionforAR.In:IEEE/CVFWinterCon-
ference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA,
January 2-7, 2023. pp. 5839–5849. IEEE (2023)
81. Wu,J.,Zhang,C.,Zhang,X.,Zhang,Z.,Freeman,W.T.,Tenenbaum,J.B.:Learn-
ingshapepriorsforsingle-view3dcompletionandreconstruction.In:ECCV(2018)
82. Xie,H.,Chen,Z.,Hong,F.,Liu,Z.:Citydreamer:Compositionalgenerativemodel
of unbounded 3d cities. arXiv preprint arXiv:2309.00610 (2023)20 Ünlü et al.
83. Yao,Y.,Luo,Z.,Li,S.,Fang,T.,Quan,L.:MVSNet:Depthinferenceforunstruc-
tured multi-view stereo. In: ECCV (2018)
84. Yao, Y., Schertler, N., Rosales, E., Rhodin, H., Sigal, L., Sheffer, A.: Front2back:
Singleview3dshapereconstructionviafronttobackprediction.In:Proceedingsof
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)
85. Yin,W.,Liu,Y.,Shen,C.,Yan,Y.:Enforcinggeometricconstraintsofvirtualnor-
mal for depth prediction. In: 2019 IEEE/CVF International Conference on Com-
puter Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019.
pp. 5683–5692. IEEE (2019)
86. Yin, W., Zhang, J., Wang, O., Niklaus, S., Mai, L., Chen, S., Shen, C.: Learning
to recover 3d scene shape from a single image. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 204–213 (2021)
87. Žbontar,J.,LeCun,Y.:Stereomatchingbytrainingaconvolutionalneuralnetwork
to compare image patches. JMLR (2016)
88. Zhang, S.H., Guo, Y.C., Gu, Q.W.: Sketch2model: View-aware 3d modeling from
single free-hand sketches. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. pp. 6012–6021 (2021)
89. Zhao, C., Sun, Q., Zhang, C., Tang, Y., Qian, F.: Monocular depth estimation
based on deep learning: An overview. Science China Technological Sciences 63(9)
(2020)
90. Zhao, L., Wang, H., Zhu, Y., Song, M.: A review of 3d reconstruction from high-
resolution urban satellite images. International Journal of Remote Sensing 44(2)
(2023)
91. Zheng, J., Zhu, Y., Wang, K., Zou, Q., Zhou, Z.: Deep learning assisted opti-
mization for 3d reconstruction from single 2d line drawings. arXiv e-prints pp.
arXiv–2209 (2022)
92. Zheng, X.Y., Pan, H., Wang, P.S., Tong, X., Liu, Y., Shum, H.Y.: Locally atten-
tionalsdfdiffusionforcontrollable3dshapegeneration.ACMTrans.Graph.42(4)
(2023)
93. Zhong, Y., Gryaditskaya, Y., Zhang, H., Song, Y.: Deep sketch-based modeling:
Tips and tricks. In: Struc, V., Fernández, F.G. (eds.) International Conference on
3D Vision (3DV). IEEE (2020)
94. Zhong,Y.,Gryaditskaya,Y.,Zhang,H.,Song,Y.Z.:Astudyofdeepsinglesketch-
basedmodeling:View/styleinvariance,sparsityandlatentspacedisentanglement.
Computers & Graphics 106, 237–247 (2022)
95. Zhong, Y., Qi, Y., Gryaditskaya, Y., Zhang, H., Song, Y.Z.: Towards practical
sketch-based 3d shape generation: The role of professional sketches. IEEE Trans-
actions on Circuits and Systems for Video Technology (2020)
96. Zhou, B., Russakovsky, O., Fong, R., Hoffman, J.: CVPR Tutorial on Human-
CenteredAIforComputerVision(2022),https://human-centeredai.github.io/
97. Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J.: Unet++: A nested
u-net architecture for medical image segmentation. In: Deep Learning in Medical
Image Analysis and Multimodal Learning for Clinical Decision Support: 4th In-
ternational Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS
2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20,
2018, Proceedings 4. Springer (2018)GroundUp 21
7 Synthetic data
7.1 Data generation
To train our networks, we use the UrbanScene3D dataset [50] which contains
large-scale3Dmodelsofsixreal-worldcities.WeselectedNewYorkandChicago
fortrainingandvalidationrespectively,andSanFranciscofortesting.Ourtrain-
ing set contains 40K samples, our validation set contains 2K samples, and our
test set comprises 1K samples. For training samples, we perform random aug-
mentation on the heights of individual buildings by scaling each building along
the vertical axis to increase the diversity of our scenes. We generate synthetic
sketchesofbuildingsinperspectiveviewsandtheirground-truthdepthandseg-
mentation maps, using Blender Freestyle [4].
7.2 View Selection in 3D cities
AswementionedinSec.7.1ofthemainpaper,wegeneratesyntheticsketchesof
buildings in perspective views and their ground-truth depth and segmentation
maps, using Blender Freestyle [4]. We place two cameras for each scene: one
top-down orthographic Cam and one aerial perspective Cam . We start by
t p
samplingCam ’slocationinthesceneandconsequentlysetCam inthepositive
p t
look-at direction of the former at the midpoint between near and far planes. To
avoidplacingCam withinbuildinggeometry,wepre-processeachcityandlabel
p
traversable regions on the ground plane. Moreover, each camera sits at a pre-
determined height above the ground, selected so that most of the buildings are
observed from above. This implies that since our system was trained with fixed
settings for top-down S and perspective S sketches, it expects that the inputs
t p
shouldadheretotheserenderingsettings.Werecognizethatthislimitsthechoice
of viewpoints, and in full-featured applications, the urban designer may want to
chooseadifferentviewpoint,suchasastreet-levelsketch,oruseanaxonometric
projection. However, we believe that robustness to such representation changes
isonlyamatteroftrainingonadatasetthatincludesawiderrangeofrendering
settings.
7.3 Representative samples
In Fig. 7, we provide samples from our training dataset, showing: perspective
syntheticsketchesS ,foregroundmasksforperspectiveviewsM ,depthinper-
p p
spective views D , top-down synthetic sketches S , building-level segmentation
p t
in top-down views M⋆ and top-down depth maps D .
t t
8 User study: Modeling interface
To validate our contributions, we built an interactive user interface in HTML,
JavaScript, and Python. The 3D massing system works in real-time on a Titan22 Ünlü et al.
S
p
M
p
D
p
S
t
M
t
D
t
Fig.7: Ground-truth maps from our synthetic dataset: perspective synthetic sketches
S , foreground masks for perspective views with visualized segmentation of buildings
p
(in the method we only use the binary foreground mask), depth in perspective views
D ,top-downsyntheticsketchesS ,building-levelsegmentationintop-downviewsM⋆
p t t
and top-down depth maps D . Please see Sec. 7.1 of the main paper and Sec. 7.2 for
t
details on data generation and view selection.GroundUp 23
Fig.8: Screenshot of our interface, as seen by participants in our user study. Note
thatbothsketchingviews(top-downandperspective)havebeenloadedwithreference
images. This means user study participants were mostly modeling existing massing,
instead of inventing new designs.24 Ünlü et al.
X,andcanbeusedonanytouch-screendevicethroughabrowser.Whilemouse,
touch, and stylus inputs are allowed, we recommend users use a stylus, because
it is easier and results in higher-quality sketches.
The interface is split into three main regions: a 3D viewport for interac-
tion with a predicted 3D scene, and two sketching canvases for perspective and
top-down views. Fig. 8 and Fig. 9 show our sketching interface, with and with-
out a reference underlay in the sketch canvases, respectively. For sketching, our
tool supports standard capabilities such as erasing and undoing. Strokes are
treated as vector data. We support two levels of zoom for the sketch views.
The integrated 3D viewer is simple, and generated meshes can be exported to
downstream 3D tools, e.g. for adding details or vectorized rendering like Fig. 1
(4) of the main paper. An important capability that was added in response to
pilot users was a sketch-to-sketch projection. Users can project their top-down
sketches to the perspective canvas, allowing them to see the building layouts as
akindoffoundationFig.9.Thissupportsusersinaligningtheirmassesbetween
top-down and perspective sketches, which can be hard to do otherwise.
Fig.9: Sketching interface. The interface is split into 3 major components: a 3D view
for interaction with a predicted 3D scene, and two sketching canvases for perspective
and top-down views. The buildings are generated here using only layout information.
9 User study: Additional details and feedback
Userfeedback,especiallyfromthepost-studyquestionnaire,isprovidedinTab.4.
Additionally,welistherequotesfromalltheusersinourstudy,groupedbysen-
timent: good, bad, and neutral.GroundUp 25
9.1 Quotes with positive sentiment
– Very cool system!
– Make it easy to iterate on designs. I can adapt it as I go. Deterministic
behavior, so I feel that I have control over the output.
– I think that could be a very useful tool. Even if I sketched it really really well
onpaper,I’llsubconsciouslyconvincemyselfitworks,evenifin3Ditdoesn’t
work. (e.g. gaps in their Snake-tunnel model)
– Formassing,wealwaysstartdesigningfromthetopdownandinsketchform.
I usually have an idea in mind for what the design would look like in per-
spective. But just from the topdown it is hard to visualize how the buildings
wouldlooklikeinperspective.Thistoolisgreatforvisualizingtheperspective
view quickly.
– This was FAST! In Rhino I’d need at least 10 min for basic meshforming,
and then 30 minutes to make that more accurate.
– Ibelievethatthefirststepofdesignshouldbeginwithfreehand.Thesoftware
tools are limit my creativity. That’s why mostly l was doing on paper sketch
after that l import to revit or sketchup. I believe that style would be super
useful.
– I feel like if you get the rough shape + layout from your sketch, then you can
easily import and get full 3D, vs. just sketching on paper and then you just
have a flat sketch.
– (good to) start to visualize geometry in the plan into 3d prespective
– I don’t tend to design cities/buildings, so this particular implementation
likely wouldn’t be useful for me personally, but a more general 3D-model-
from-sketch (e.g. for random objects in a room, like a couch, table, etc.),
could be useful for rapidly creating AR/VR spaces.
– Speed of making model and quick modify is useful
– This is too fun!
– (onsketching2views,andthepossibilityofsketchingmore)IfIhadtosketch
a 2nd perspective, I wouldn’t think it’s worth it.
– If I had to sketch a 2nd perspective, I wouldn’t think it’s worth it.
– Nice to use.
– Theplantoperspectiveprojectionfromperspectivecanvastotopdownisvery
useful when doing freefrom sketching.
9.2 Quotes with negative sentiment
– The shape of the roof can (sic) be chosen. (likely meant can’t)
– Only concern is how accurate it can be - I need details for only some situa-
tions
– It would be nice if I could edit the heights on the 3D model to what I wanted
them to be (i.e. refine the 3D model by clicking and dragging on the tops of
the buildings). I feel like it could also be useful to be able to quickly place
trees and roads (things that aren’t just buildings).26 Ünlü et al.
– Finer details are hard to sketch.
– Tried to draw pitched roof in the top-down: bad result.
– I wish I could reduce the opacity of top-down projected sketch lines in the
Perspective View. They’re obscuring my reference image.
– Just like working in my sketchbook, but you also see the 3D even if it’s not
perfect
– Depending on the design scenario, I would want to sketch from different
viewpoints (for the perspective sketch). For some scenarios, a street-level
sketch would be better. But for massing, a higher perspective is better. But
depending on the scene I am designing, I would like to change my sketch to
matchthescene:IwishIcouldchangetheviewpointforperspectivesketching
in this tool.
9.3 Quotes with neutral sentiment
– (Please add) Zoom in, zoom out tool
– (Please add) Line weight to differentiate elements in sketching
– To write text on it, e.g. overlay window, like a post-it note on the 3D mesh.
Like annotation to show where the wind goes.
– keyboard shortcut to switch between canvases
– Maybe would want an image-to-sketch converter, so I can just pull in the
image and then edit lines.
– Would be cool to also use sketches to define building details, e.g. door and
window. Could be nice to use a prompt to texture the building.
– I like the melty thing it created - like gipsum - I couldn’t do that in Rhino
really. Rhino says: “your line is this, follow it!”
– Details in the facade
– I think it would be nice to have a quick way to get a 3D representation to
then get a more precise building. I could see myself tracing over with a cube
[in the 3D view] - depends on the level of detail I’m going for. Normally in
Blender, I’d start with a cube and position things relative to it. Could have
a concrete wrapping of initial shape with a sharp convex hull. But could skip
it if it’s already sharp enough.
UI just needs small refinements
– If I want details, I’ll just do it in Rhino.GroundUp 27
9.4 Summary of short-answer responses to the post-study
questionnaire
Tab. 4 provides extended statistics supporting the discussion in Sec. 5 of the
main paper.
Architect non-Architect
cohort cohort
Post-study Question
(5 respon- (5 respon-
dents) dents)
Yes: 2/5
Wereyouabletorecreatethebuildingsintherefer- Yes: 5/5
No: 3/5
ence image in 5 minutes?
+1: 4/5 +2: 1/5
Howaccuratelywereyouabletorecreatethebuild-
0: 1/5 +1: 4/5
ings in the reference images using the sketching in-
terface? Scale: [-2 -1 0 +1 +2] where +2: matches
reference well
+2: 3/5 +1: 1/3
How likely are you to use the sketching interface in +1: 1/5 0: 2/3
this study in the future for 3D building massing in 0: 1/5 (2 non-responses)
the early design/ideation stage? Scale: [-2 -1 0 +1
+2], where -2: highly unlikely, +2: highly likely
Yes: 1/3
Yes: 4/5 Conditional Yes:
Would you consider using the sketching interface in
No: 1/5 2/3
thisstudyaspartofyour3Dmodelcreationprocess?
(2 non-responses)
Forexample,insteadofusing3Dmodelingsoftware
only(e.g.Rhino),ideatinginthissketchinginterface,
beforeimportingtheoutput3Dmassbuildingmodel
into Rhino and continuing there?
Sketch: 3/5
Sketch: 3/5
Inthefuture,whichonecouldyouseeyourselfusing Rhino: 1/5
Rhino: 2/5
for making 3D mass models? Blender: 1/5
Yes: 4/5
Wouldseeinga3Dmodelfromyoursketchprojected Yes: 5/5
No: 1/5
to 2D help you refine your sketch? (overlaid in the
3D canvas)
Table 4: Summaryofshort-answerresponsestothepost-studyquestionnaire.Despite
the sketch-based web interface being new for everyone, architects performed the task
moreswiftly onaverage. Itis encouragingthat threeout offivearchitectswerehighly
likely to use this sketching interface for massing, though non-architects were less en-
thusiastic.
10 User study: Comparison with SketchUp
We tested two more architects, one of whom specializes in urban design. One
uses SketchUp regularly; the other routinely works with similar software. Both28 Ünlü et al.
were asked to model two scenes, first in our interface and then in SketchUp. In
both systems, we provided top-down and perspective references. For SketchUp,
wesavedtheoutputafter5minutesand10minutesofmodeling.After5minutes
in SketchUp, architects were able to only complete a flat outline of buildings.
After10minutes,theywerestillnotdonewithfixingtheheightsofthebuildings,
as shown in Fig. 10. Meanwhile, with our system, architects were able to obtain
3D geometry in under 5 minutes. After massing, our models can be exported to
detail-oriented modeling tools.
User Sketch Ours(5min) SketchUp(5min.) SketchUp(10min)
Fig.10: OneoffourscenesmodeledinGroundUpvsSketchUpbyanarchitect.Qual-
itatively and quantitatively, quick progress is better in ours.
11 Perspective depth prediction: Additional analysis and
Visualizations
Choice of ν In this section, we analyze the effect of different settings of ν values
to construct occupancy feature volume.
The 3D occupancy features are of shape D×H ×W, where D is the num-
ber of depth planes. When feeding these features into the 2D encoder in the
UNet++, we consider depth planes as image feature channels C. We generate
theoccupancyfeaturesbysettingallvoxelsthatfallabovenon-occupiedregions
to −ν and all voxels above occupied regions to ν.
Weexperimentwith5differentsettings:ν ∈{1,25,50,75,100}.Tab.5shows
that using ν = 50 performs better than the other settings. To understand the
reason behind this, we observe the range of the multi-scale image features from
the image encoder backbone. At the beginning of training, the range of these
features is between 0 and 100 for the first few training batches. We believe that
keepingν closetothemid-pointofthatrangeallowsthenetworktoleveragethe
occupancy information most beneficially.
Sparse Height Information Fig.11showsheightinformationourdiffusionmodel
gets as well as the baseline.
12 Implementation details
AllourmodelsandbaselinesweretrainedusingPyTorch.Forperspectivedepth
prediction,weusedabatchsizeof16acrossallmodelsandablationexperiments,GroundUp 29
Abs Abs Sq Log
Model RMSE↓ a5↑
Diff↓Rel↓Rel↓ RMSE↓
1 OV 4.64 3.39 0.31 7.04 5.15 75.1
L−ν1
2 OV 4.8 3.29 0.37 8.41 5.15 74.6
L−ν25
3 OV 3.49 2.13 0.21 6.54 3.4389.2
L−ν50
4 OV 4.71 3.2 0.33 8.05 4.84 74.5
L−ν75
5 OV 4.22 2.68 0.29 7.74 4.2 81.2
L−ν100
Table 5: The effect of the choice of ν for the Occupancy features Volume (OV) in
the perspective depth prediction network. All models are trained using the ResNet-50
encoder. All metric values apart from a5 are scaled up by 102.
a) b) c)
Fig.11:QualitativecomparisonofHeightFields[80]vsourmodel.a)showstheground-
truthmeshwithacameramarkerfortheperspectiveviewandthevisualizationofwhat
thatviewseesoverlaidingreen.b)istheHeightFields [80]outputandc)isourmodel.
withafixedlearningrateof1e-4andweightdecay.Wetrainedallmodelsfor25K
iterations on four RTX 2080 GPUs. Our top-down mask model is trained with
similar parameters to the depth predictor except we train it for 5K iterations.
For building segmentation, we use Pytorch’s BCEWithLogitsLoss function and
set pos_weight as 20 for balancing the building pixels against the ground pixels
in the mask image. For our depth completion diffusion model, we set a batch
size to 32 and a learning rate to 3e−4. We trained all models for 35 epochs, on
amachinewithRTX2080GPUs.Forthedepthcompletionbaselineinthemain
paper, HeightFields [80], we used a batch size of 12, a learning rate of 1e−4,
and trained it for 35 epochs on an NVIDIA RTX 3090 GPU. Please note that
we added a normal loss L to this baseline, as we found that results in more
norm
accurate reconstructions with sharper features. During training for our models
and HeightFields [80] baseline, we augment both the top-down and perspective
sketches, following the strategy proposed by Ünlü et al. [77].
12.1 Multi-conditional top-down diffusion model
Inthemainpaper,wedescribehowweconditionthediffusionmodelinSec.3.3.
Here, we provide additional details of the CNNs that we use to align features
of the sketch and depth encoders. The latent features c and z are passed
depth k
through separate CNN heads: each head contains two convolutional blocks with
a Conv2d layer followed by GroupNorm and Relu layers.30 Ünlü et al.
13 Post-processing heightfields for visualization
In the user interface, we use a quick (real-time) meshing algorithm. We elevate
each grid point of an initial 2D flat mesh using the predicted height values, as
we described in Sec. 3.3.
Toobtainreal-timeperformance,ourpredictedheightfieldshavelimitedspa-
tial resolution, which results in some jagged aliased geometry on the vertical
surface of the buildings. This effect can be observed for example in Fig. 11, in
both the ground-truth and our predictions, which are both obtained from the
same resolution heightfields.
Toprovideuserswithanoptiontoworkwithhigher-qualitymeshatthenext
stage oftheirdesign pipeline, we explored automaticoffline post-processing. We
first vectorize the predicted heightfields using Adobe Illustrator’s Image Trace
tool. We then export it as a high-resolution raster image (300dpi). We use the
following settings for Image Trace:
– Preset: custom
– Mode: Grayscale
– Threshold: Between [8-20] (depending on the depth map, the threshold may
vary.)
– Paths: 75 / Corners: 75 / Noise: 25
For rendering the vectorized high-resolution output, we used Blender’s Ren-
derEngine.Weusedthisapproachtogeneratevisualizationsintheteaserinthe
main paper (Fig. 1), the supplementary video, and for the visualization of the
results of the freehand modeling sessions (Fig. 6 in the main paper).
Potentially, some superresolution approaches that do not require training,
such as [18], can also be used to reduce the jaggedness of the reconstructed
meshes.
14 Controllable geometry generation in occluded areas
Assketchingistypicallythefirststepinanydesignprocess,ourprimarygoalwas
toenableatoolthatcombinesthebenefitsofsketchingand3Dshapeexploration
for large-scale city scenes. Depending on the use case, the user interface could
be modified to fit different modeling scenarios. Our UI could be extended to
allow modeling buildings 1-1 – the strategy chosen during sketching by one of
our participants in the user study, Novice-User-2. For another scenario, the UI
could evolve to support multi-view perspective sketches. Furthermore, one user
asked for camera-angle control (see Sec. 9.4).
While we leave a thorough exploration of multi-view iterative editing to fu-
turework,wehaveconductedapreliminarystudy.Totestthis,weused250test
scenes with 2 perspective views 45◦ apart. We projected point clouds inferred
from extra perspective sketches into the top-down representation passed to our
diffusion model. Without any finetuning, the reconstruction is improved on all
metrics, e.g. by .0020 points on absolute difference of top-down view, compared
to a single perspective sketch, as seen in Tab. 6.GroundUp 31
15 Effect of normals loss
Model Abs Diff↓Abs Rel↓Sq Rel↓RMSE↓
GroundUp (single camera) 0.1158 0.0249 0.0073 0.1619
GroundUp (two cameras) 0.1138 0.0244 0.0069 0.1557
Table 6: Quantitative evaluation on multi-view input. GroundUp with multi-view
input improves metrics.
In Tab. 3 of the main paper, we noticed a drop in performance when the
normal loss is used. We tracked this down through visualizations - please see
Fig. 4. This loss causes geometry to shrink slightly in all directions - especially
in the areas occluded in the perspective sketch. In Fig. 4-a, the red point cloud
accounts for both visibility and actual building height. For L , Fig. 4-b
t,norm
shows the red point cloud is riding slightly above the green prediction, meaning
theheightisunderestimated.Incontrast,thepredictionwithoutthenormalloss
does not suffer from underestimated heights within the visibility region, albeit
producing uneven surfaces (Fig. 4-c). L produces nice building geometry
t,norm
withevensurfaceswithinandoutsidethevisibilityregion;withoutit,themodel
produces unrealistic buildings, deviating a lot from real building geometry, es-
pecially outside the visibility region (Fig. 4-c).
The 3D metric in Tab. 3 masks for visibility, so this metric is sensitive to
shrinkage while ignoring defects outside the perspective view. The 2D metrics
are computed for the full buildings’ geometries and reflect on the quality of
buildings’ rooftops outside of areas visible in the perspective views.
We think the reason for the geometry shrinkage in the visible regions could
be explained with the aid of multi-task learning literature. Training a neural
network with an auxiliary task could affect the performance on the main task,
e.g. that of depth and normals estimation [23].