Scalable Monte Carlo for Bayesian
Learning
PaulFearnhead,ChristopherNemeth,ChrisJ.OatesandChrisSherlock
4202
luJ
71
]LM.tats[
1v15721.7042:viXraContents
Preface 1
1 Background 4
1.1 MonteCarloMethods 5
1.1.1 WhatisMonteCarloIntegration? 5
1.1.2 ImportanceSampling 7
1.1.3 MonteCarloorQuadrature? 7
1.1.4 ControlVariates 9
1.1.5 MonteCarloIntegrationandBayesianStatistics 11
1.2 ExampleApplications 14
1.2.1 LogisticRegression 15
1.2.2 BayesianMatrixFactorisation 15
1.2.3 BayesianNeuralNetworksforClassification 16
1.3 MarkovChains 17
1.3.1 ReversibleMarkovchains 18
1.3.2 Convergence,Averages,andVariances 19
1.4 StochasticDifferentialEquations 22
1.4.1 TheOrnstein–UhlenbeckProcess 23
1.4.2 TheInfinitesimalGenerator 24
1.4.3 LangevinDiffusions 26
1.5 TheKernelTrick 28
1.5.1 Finite-DimensionalInnerProductSpaces 28
1.5.2 KernelsinaFinite-DimensionalInnerProductSpace 31
1.5.3 ANewInnerProductandtheKernelTrickinFiniteDimensions 34
1.5.4 GeneralKernels 35
1.5.5 ThePoweroftheKernelTrick 38
1.6 ChapterNotes 40
2 ReversibleMCMCanditsScaling 42
2.1 TheMetropolis–HastingsAlgorithm 43
2.1.1 Component-wiseupdatesandGibbsmoves 48
2.1.2 TheMetropolis–HastingsIndependenceSampler 49
2.1.3 TheRandomWalkMetropolisAlgorithm 50
iiiiv Contents
2.1.4 TheMetropolis-AdjustedLangevinAlgorithm 53
2.2 HamiltonianMonteCarlo 57
2.3 ChapterNotes 63
3 StochasticGradientMCMCAlgorithms 65
3.1 TheUnadjustedLangevinAlgorithm 65
3.2 Approximatevs.ExactMCMC 67
3.3 StochasticGradientLangevinDynamics 69
3.3.1 ControllingStochasticityintheGradientEstimator 72
3.3.2 Example:TheValueofControlVariates 78
3.3.3 ConvergenceResultsforStochasticGradientLangevinDynamics 80
3.4 AGeneralFrameworkforstochasticgradientMCMC 85
3.5 GuidanceforEfficientScalableBayesianLearning 90
3.5.1 ExperimentsonaLogisticRegressionModel 92
3.5.2 ExperimentsonaBayesianNeuralNetworkModel 97
3.6 GeneralisationsandExtensions 100
3.6.1 ScalableInferenceforModelsinConstrainedSpaces 100
3.6.2 ScalableInferencewithTimeSeriesData 102
3.7 ChapterNotes 105
4 Non-ReversibleMCMC 106
4.1 TheBenefitsofNon-Reversibility 106
4.2 HamiltonianMonteCarloRevisited 109
4.3 LiftingSchemesforMCMC 112
4.3.1 Non-ReversibleHMC 112
4.3.2 Gustafson’sAlgorithmandMultidimensionalGeneralisations 113
4.4 ImprovingNon-reversibility:DelayedRejection 119
4.4.1 TheDiscreteBouncyParticleSampler 121
4.5 ChapterNotes 124
5 Continuous-TimeMCMC 126
5.1 Continuous-TimeMCMCastheLimitofNon-ReversibleMCMC 126
5.2 PiecewiseDeterministicMarkovProcesses 129
5.2.1 WhatisaPDMP? 129
5.2.2 SimulatingPDMPs 130
5.2.3 TheGeneratorandInvariantDistributionofaPDMP 134
5.2.4 TheLimitingProcessofSection5.1asaPDMP 136
5.3 Continuous-timeMCMCviaPDMPs 139
5.3.1 DifferentSamplers 140
5.3.2 UseofPDMPOutput 154
5.3.3 ComparisonofSamplers 155
5.4 EfficientSimulationofPDMPSamplers 159
5.4.1 SimulatingPDMPs 159
5.4.2 ExploitingModelSparsity 165Contents v
5.4.3 DataSubsamplingIdeas 168
5.5 Extensions 175
5.5.1 DiscontinuousTargetDistribution 176
5.5.2 ReversibleJumpPDMPSamplers 179
5.5.3 MoreGeneralVelocityModels 184
5.6 ChapterNotes 191
6 AssessingandImprovingMCMC 192
6.1 DiagnosticsforMCMC 192
6.1.1 ConvergenceDiagnostics 193
6.1.2 BiasDiagnostics 194
6.1.3 ImprovedBiasDiagnosticsviatheKernelTrick 197
6.2 ConvergenceBoundsforMCMC 200
6.2.1 BoundsonIntegralProbabilityMetrics 200
6.2.2 ChoiceofAuxiliaryMarkovProcess 203
6.2.3 KernelSteinDiscrepancy 205
6.2.4 ConvergenceControl 210
6.2.5 StochasticGradientSteinDiscrepancy 216
6.3 OptimalWeightsforMCMC 218
6.4 OptimalThinningforMCMC 223
6.5 ChapterNotes 224
References 229Preface
Atthetimeofwriting,science,industry,andsocietyarebeingtransformed
by the emergence of a new generation of powerful machine learning and
artificialintelligence(AI)methodologies.Thesafeuseofsuchalgorithms
demands a probabilistic viewpoint, enabling reasoning in settings where
data are noisy or limited, and endowing predictions with an appropriate
degree of confidence for downstream decision-making and mitigation of
risk.Yet,itremainstruethatfundamentalprobabilisticoperations,suchas
conditioning on an observed dataset, are not easily performed at the scale
required.
The aim of this book is to provide a graduate-level introduction to ad-
vancedtopicsinMarkovchainMonteCarlo(MCMC),asappliedbroadlyin
theBayesiancomputationalcontext.Most,ifnotallofthesetopics(stochas-
ticgradientMCMC,non-reversibleMCMC,continuoustimeMCMC,and
new techniques for convergence assessment) have emerged as recently as
thelastdecade,andhavedrivensubstantialrecentpracticalandtheoretical
advances in the field. A particular focus is on methods that are scalable
withrespecttoeithertheamountofdata,orthedatadimension,motivated
by the emerging high-priority application areas in machine learning and
AI.Throughoutthisbook,theclearpresentationofideasisprioritisedover
arigoroustechnicaltreatmentofallmathematicaldetails;appropriateref-
erences for further reading are provided in the end-notes of each chapter.
In particular, we will limit the use of measure theory; the reader should
assumethatallsetsandfunctionsaremeasurablewithrespecttoanappro-
priatesigma-algebra,andallcontinuousdistributionsshouldbeassumedto
beabsolutelycontinuouswithrespecttoLebesguemeasureandalldensities
shouldbeassumedtobedensitieswithrespecttoLebesguemeasure.
Thisbookhasbeenindirectlyshapedbytheresearchersandcolleagues
– too numerous to name individually – who have contributed to recent
progress in the field. Special gratitude must go to Rebekah Fearnhead,
Heishiro Kanagawa, Tama´s Papp and Lorenzo Rimella, for proof-reading
12 Preface
themanuscript,toRichardHoweyfortypesettingthefigures,andtoNatalie
TomlinsonandAnnaScrivenfortheirencouragementandtypesettingsup-
port.TheauthorsaregratefulforfinancialsupportfromtheEngineeringand
PhysicalSciencesCouncil(throughgrantsEP/W019590/1,EP/R018561/1,
EP/R034710/1,EP/V022636/1andEP/Y028783/1),theAlanTuringInsti-
tute,andtheLeverhulmeTrust.
PaulFearnhead ChrisJ.Oates
ChristopherNemeth NewcastleUniversity,UK
ChrisSherlock
LancasterUniversity,UKPreface 3
CommonNotation
𝑛 total number of iterations of an algorithm or Monte Carlo
samplesize.
𝑑 dimension(ofparameterspace).
𝑁 numberofelementsinthedataset.
D thedataset{y 1,...,y𝑁},.
𝜽 theparameter.
𝐿(𝜽;D) thelikelihoodfunction.
ℓ(𝜽;D) thelog-likelihoodfunction.
𝜋 (𝜽) thepriordensity.
0
𝜋(𝜽|D) theposteriordensity,oftenabbreviatedto𝜋(𝜽).
I theindicatorfunction.
I𝑑 the𝑑×𝑑identitymatrix.
𝑥
𝑖
the𝑖thcomponentofthevectorx.
x𝑘 the𝑘thvectorinasequence(x𝑘)𝑘=1,2,....
𝑥 𝑘(𝑖) the𝑖thcomponentofthevectorx𝑘.
i.i.d. independentandidenticallydistributed.
=D
equalindistribution.
D
→ convergesindistribution.
𝛿 theDiracdistribution,whichplacesallmassatx.
x
N(·;𝝁,V) the density of a normal random variable with mean 𝝁 and
covarianceV.
U𝑑(·) the density for a uniform random variable on the 𝑑-
dimensionalsphere,S𝑑−1 ⊂R𝑑.
L𝑝(𝜋) thesetofmeasurablefunctions 𝑓 with∫ |𝑓(x)|𝑝 d𝜋(x) <∞.
𝐶𝑠(R𝑑,R𝑝) the set of functions 𝑓 : R𝑑 → R𝑝 for which continuous
derivativesexistofordersupto𝑠 ∈ {0,1,...}∪{∞}.
𝑥 𝑛 =𝑂(𝑎 𝑛) thereexist𝑛 0and𝑀 suchthatforall𝑛 ≥𝑛 0,|𝑥 𝑛| ≤ 𝑀𝑎 𝑛.
𝑋 𝑛 =𝑂 𝑝(𝑎 𝑛) forany𝜖 > 0thereexist𝑛 0 and 𝑀 suchthatforall𝑛 ≥ 𝑛 0,
P(|𝑋 𝑛| > 𝑀𝑎 𝑛) <𝜖.1
Background
This book describes some recent developments in scalable Monte Carlo
algorithms and their applications within Bayesian learning: what exactly
doesthismean?
MonteCarlomethodsareaclassofcomputationalmethodsthatinvolve
repeated sampling to numerically approximate quantities of interest. We
specificallyfocusonMonteCarlointegrationmethods,whicharesampling-
basedmethodsforevaluatingorapproximatingthevalueofintegrals.Such
methods are widely used across science and engineering, but our motiva-
tioncomesparticularlyfromBayesianstatistics.Oneofthekeyquantities
in Bayesian statistics is the posterior distribution, which encapsulates our
belief regarding unknown parameters of a model given our prior belief
and an observed dataset. We can then obtain estimates of the parameters,
orquantifyouruncertaintyabouttheparameters,intermsofexpectations
withrespecttotheposteriordistribution.Forexample,acommonestimate
ofaparameteristheposteriorexpectationofthatparameter;thepredictive
probability of future observations is the expectation of the density/mass
functionofthefutureobservationtakenwithrespecttotheposteriordistri-
bution.Calculatingtheseexpectationsinvolvesevaluatinganintegral,and
the idea of Monte Carlo is to use samples from the posterior to estimate
suchintegrals.
ThemainchallengewithusingMonteCarloinBayesianstatisticsisoften
inderivinganefficientalgorithmtosamplefromtheposteriordistribution.
Markov chain Monte Carlo is a general and widely-used class of methods
for sampling from a distribution, based on simulating a Markov process
thathastheposteriordistributionasitsstationarydistribution.
Inrecentyears,therehasbeeninterestinapplyingMarkovchainMonte
Carlotoever-increasinglycomplexandchallengingproblems.Forexample,
the dimension, 𝑑 say, of the parameter space of the models we wish to fit
to data, or the number of data points, 𝑁 say, in our data set can be large.
As either 𝑑 or 𝑁 increases, the efficiency of Markov chain Monte Carlo
41.1 MonteCarloMethods 5
methods may reduce. For example, as 𝑑 increases we may need to have
moreiterationsofourMarkovchainMonteCarloalgorithmtoachievethe
required level of accuracy, while as 𝑁 increases, the computational cost
periterationofastandardalgorithmwillincrease.ScalableMarkovchain
MonteCarlomethodsarespecificallythosemethodswhichcanscalewell
aseitherorbothof𝑑 and 𝑁 increase.
The remainder of this introductory chapter will cover background rele-
vanttoscalableMarkovchainMonteCarlo.Thenextsectionwillintroduce
MonteCarlomethods,explainwhyMonteCarlointegrationiswidely-used,
and explain how it is relevant to Bayesian statistics. This will be followed
by an introduction to some of the statistical models and applications that
willbeusedtodemonstratethemethodsinthisbook,aswellasaninformal
and brief introduction to some of the concepts from stochastic processes
that will be used in later chapters. Finally, the chapter ends with a short
introduction to kernel methods in preparation for a deeper exposition in
Chapter6.
1.1 MonteCarloMethods
1.1.1 WhatisMonteCarloIntegration?
Assume we have a distribution of interest. For simplicity of presentation,
hereandfortheremainderofthischapter,weassumethatthedistribution
iscontinuousonR𝑑.LetXdenotearandomvariablewiththisdistribution
and let 𝜋(x) denote the corresponding probability density function for X;
wewillalsouse 𝜋 torefertothedistributionitselfwhenthatisnecessary.
ThentheexpectationofsomefunctionℎofXisanintegral
∫
𝐼 =E[ℎ(X)] = ℎ(x)𝜋(x) dx.
This expectation is well-defined, that is, ℎ is integrable with respect to 𝜋,
if∫ |ℎ(x)|𝜋(x) dx < ∞.Weabbreviatethisto ℎ ∈ L1(𝜋) andthroughout
this section we assume that this holds true. If we can sample from 𝜋(·)
thenwecanestimatethisexpectation/integralby(i)drawing𝑛independent
realisations, x 1,...,x𝑛, from 𝜋(·) and (ii) calculating the sample average
ofthevaluesℎ(x 1),...,ℎ(x𝑛).Thisgivesanestimateof 𝐼,namely
𝑛
1 ∑︁
𝐼ˆ=
𝑛
ℎ(x𝑘).
𝑘=1
ThisiscalledaMonteCarloestimateof𝐼,asitisobtainedfromindependent,
randomsamplesfrom𝜋(·).6 Background
TheMonteCarloestimatorcanbeinterpretedasbeingbasedon𝑛inde-
pendentrandomvariablesX 1,...,X𝑛,ofwhichx 1,...,x𝑛arerealisations.
Isitagoodestimator?Thisisimpossibletoansweringenerality,butwecan
at least describe some good properties that the estimator can admit. First,
sincetheX𝑖 arei.i.d,E(cid:2)𝐼ˆ(cid:3) = E[ℎ(X 1)] = 𝐼,sotheestimatorisunbiased.
Secondly, and more importantly, the strong law of large numbers tells us
that we can make our estimate arbitrarily accurate, with high probability,
if we choose 𝑛 large enough. Formally, provided 𝐼 is well defined, that is
ℎ ∈ L1(𝜋),andoursamplesfrom𝜋(·) areindependent,thenas𝑛 →∞,
𝑛
1 ∑︁
𝑛
ℎ(X𝑘) → 𝐼 almostsurely. (1.1)
𝑘=1
Almostsureconvergencemeansthatthecollectionofoutcomeswherethe
convergencedoesnotoccurhasacombinedprobabilityof0.
Thus,withhighprobability,ourMonteCarloestimatorwillbeaccurate
if we choose 𝑛 large enough, but the result does not tell us how large 𝑛
needs to be, nor how accurate the estimator will be for a given value of
𝑛. However, provided that ∫ ℎ(x)2𝜋(x) dx < ∞, which we abbreviate to
ℎ ∈ L2(𝜋),wecanusethecentrallimittheoremtoanswerthesequestions.
Againassumethatoursamplesfrom𝜋(·) areindependent,anddefine
∫
𝑉 = {ℎ(x)−𝐼}2𝜋(x) dx.
Then,thecentrallimittheoremstatesthat
√ 𝑛(cid:32) 𝑛1 (cid:205)𝑛 𝑘=1√ℎ(X𝑘)−𝐼(cid:33)
→D N(0,1),
𝑉
as𝑛 →∞.Heretheconvergenceisindistribution,andwehaveconvergence
toastandardnormaldistributioninthelimit.
One way of interpreting this result is that, for large enough 𝑛, then
approximately
1
∑︁𝑛 (cid:18) 𝑉(cid:19)
𝑛
ℎ(X𝑘) ∼N 𝐼,
𝑛
.
𝑘=1
That is our estimator will be approximately normally distributed, with
meanequaltotheintegral,𝐼,andavariancethatis𝑉/𝑛.Thisshowsthatthe
quantity𝑉 governshoweasyitistoestimate𝐼 viaMonteCarlointegration,
and the accuracy depends on both 𝑉 and 𝑛. The order of the error of a
Monte Carlo estimator is
√︁𝑉/𝑛
and, thus, the Monte Carlo error decays
withsamplesizeatarateof𝑛−1/2.1.1 MonteCarloMethods 7
1.1.2 ImportanceSampling
What if we are interested in calculating or approximating a more general
integral, 𝐼 = ∫ 𝑔(x)dx, of some function 𝑔 over a region Ω? We can use
Ω
MonteCarlosamplingtoestimatethisintegralbyre-writingtheintegralas
anexpectationwithrespecttosomedensityfunction 𝑞(·) definedonΩas
follows,
∫ 𝑔(x)
𝐼 = 𝑞(x) dx=E[ℎ(X)],
𝑞(x)
Ω
where ℎ(x) = 𝑔(x)/𝑞(x).If 𝐼 iswell-defined,thatis∫ |𝑔(x)|dx < ∞,then
ℎ ∈ L1(𝑞),and𝐼 canbeestimatedusingMonteCarlointegrationasabove,
basedonindependentrealisedsamplesx 1,...,x𝑛 from𝑞(·) bycalculating
thearithmeticmeanofℎ(x 1),...,ℎ(x𝑛).ThisprocessiscalledImportance
Sampling,and𝑞(·) isknownastheproposaldistribution.
For this Monte Carlo estimator to be feasible, we have two constraints
on 𝑞. First, we need 𝑞(x) > 0 whenever 𝑔(x) > 0, in order for ℎ(x) to be
well-defined. Second, we need to be able to easily sample from 𝑞(·). The
choiceof𝑞(·)willaffecttheaccuracyoftheestimator,withthevarianceof
ourestimatorforaMonteCarlosampleofsize𝑛being𝑉/𝑛where
∫ (cid:18)𝑔(x) (cid:19)2
𝑉 = −𝐼 𝑞(x) dx.
𝑞(x)
This variance will be small if 𝑔(x)/𝑞(x) is roughly constant, and one can
show that the optimal choice of 𝑞(·) in terms of minimising𝑉 is 𝑞(x) ∝
|𝑔(x)|. If 𝑔(x) is non-negative everywhere (or non-positive everywhere)
then such a choice of 𝑞 will give an estimator that has zero-variance, that
is an exact estimator. More generally the variance𝑉 will be large if there
arevaluesofxforwhich𝑔(x)/𝑞(x) islarge.Thisleadstoarule-of-thumb
that,ifΩisunbounded,onewants𝑞(x) tohaveheaviertailsthan |𝑔(x)| to
avoidthisratioblowingupas ∥x∥ →∞.
1.1.3 MonteCarloorQuadrature?
It is natural to ask why one should use Monte Carlo integration when
therearealternativenumericalintegrationmethods,suchasquadrature.To
seethepotentialbenefitsofMonteCarlomethods,considerestimatingan
integral on the unit hyper-cube [0,1]𝑑. We can then compare quadrature
withMonteCarlointegrationbasedonsamplesfromauniformdistribution
on [0,1]𝑑.
First,consider 𝑑 = 1.Inthiscase,quadraturemethodstendtobemuch8 Background
   
   
   
   
   
   
                       
x
Figure1.1 Exampleoftrapezoidrule.Wecanestimatethe
integral,by(i)setting𝑥 1,...,𝑥 𝑛 tobeevenlyspacedpointson
[0,1];(ii)creating𝑛−1trapezoidsbasedonjoiningupthepoints
(𝑥 𝑘,ℎ(𝑥 𝑘))(shadedinregions);and(iii)estimatingtheintegral
bythesumoftheareasofthetrapezoids.
more accurate than Monte Carlo methods. We have seen that the Monte
Carlovariance,ifwehave𝑛MonteCarlosamples,is𝑂(1/𝑛),whichmeans
thattheerrorofourMonteCarloestimatorwillbe𝑂 𝑝(𝑛−1/2).
Bycomparison,asimplenumericalmethodisthetrapezoidalrule.This
involves evaluating the integrand, ℎ(𝑥) at a set of equally spaced points,
𝑥 1,...,𝑥 𝑛,on [0,1],andapproximatingtheintegralusingthetotalareaof
thetrapezoidsformedbyjoiningupthepoints(𝑥 𝑘,ℎ(𝑥 𝑘))for𝑘 =1,...,𝑛,
see Figure 1.1. Assuming our integrand has a bounded second derivative
|ℎ′′(𝑥)| < 𝐿 for some 𝐿, then we can bound the error in the estimate of
theintegralas𝐿𝛿2/12,where𝛿 =1/(𝑛−1) isthewidthofeachtrapezoid.
Thisgivesanerrorthatdecayslike𝑂(1/𝑛2),whichismuchbetterthanthe
MonteCarlomethod.Furthermore,higher-orderquadraturemethods,such
as Simpson’s rule, can obtain even faster decay of the approximate error
with𝑛,iftheintegrandissufficientlysmooth.
So,quadraturemethodscanbemoreaccuratefor1-dimensionalintegrals,
at least for functions whose second derivatives are bounded. However,
)x(h1.1 MonteCarloMethods 9
now consider higher-dimensional integrals involving functions ℎ(x), the
only information about which we have is that the second-order (partial)
derivativesarebounded.Thenwecanapplyacubaturerulebasedonagrid
of 𝑚 +1 equally spaced points in each dimension. The spacing of these
points will be 𝛿 = 1/𝑚 and there will be 𝑛 = (𝑚 + 1)𝑑 points in total.
If we have a cubature whose error decays like 𝛿𝑟, for some power 𝑟, for
example, 𝑟 = 2 for the trapezoidal rule, then the error decays at a rate of
𝑚−𝑟 ≈ 𝑛−𝑟/𝑑. For large 𝑑, this convergence will be slower than the 𝑛−1/2
rate of Monte Carlo integration, explaining why Monte Carlo is often the
defaultmethodfornumericallyapproximatinghigh-dimensionalintegrals.
Toovercomethiscurseofdimensionincubature,itisusuallynecessaryto
identifyasenseinwhichtheintegrandℎ(x)iseffectivelylow-dimensional,
whichcanbedifficultorimpossibledependingontheappliedcontext.
1.1.4 ControlVariates
Letusreturntotheproblemofestimatingtheexpectationofsomefunction
ofarandomvariable,
∫
𝐼 =E[ℎ(X)] = ℎ(x)𝜋(x) dx,
where𝜋(x)isthedensityofX.Wehaveseenhowwecanestimatethisusing
asamplefrom 𝜋(·),andthattheaccuracyofthisestimatorisproportional
to
∫ ∫
𝑉 = {ℎ(x)−𝐼}2𝜋(x) dx= ℎ(x)2𝜋(x) dx−𝐼2.
The latter expression is just the standard expression for the variance of
ℎ(X).Thisshowsthatitiseasiertoestimateexpectationsoffunctionsthat
varylesswhenevaluatedatX.
Assume that we know the expectation of a set of random variables
𝑔 1(X),...,𝑔 𝐽(X),eachatransformationofX.Withoutlossofgenerality,
wecanassumethattheserandomvariableshavemeanzero,i.e.,
E(cid:2)𝑔 𝑗(X)(cid:3) =0, for 𝑗 =1,...,𝐽,
as, if this is not the case, we can define new random variables equal to
theoldrandomvariablesminustheirexpectations.Then,foranyconstants
𝛾 1,...,𝛾 𝑛,
𝐽 (cid:34) 𝐽 (cid:35)
𝐼 =E[ℎ(X)]−∑︁ 𝛾 𝑗E(cid:2)𝑔 𝑗(X)(cid:3) =E ℎ(X)−∑︁ 𝛾 𝑗𝑔 𝐽(X) . (1.2)
𝑗=1 𝑗=110 Background
     
     
                 
x x x
Figure1.2 ExampleofcontrolvariatesforestimatingE[sin(𝑋)],
where 𝑋 hasastandardnormaldistributionN(0,1).Eachplot
showsthefunctionwhoseexpectationisbeingestimatedand50
valuesusedintheMonteCarloestimate(dots).Fromlefttoright
thefunctionsarerespectively:ℎ(𝑥) =sin(𝑥),ℎ(𝑥) =sin(𝑥)−𝑥,
andℎ(𝑥) =sin(𝑥)−𝜋𝑥/2+(𝑥2−1)/2.Theexpectationofeach
functionisconstructedtobethesame.Theeffectofintroducing
controlvariatesinthemiddleandright-handplotistoflattenout
thefunctionweareintegrating–inthemiddleplot,thishappens
for𝑥 ≈0andfortheright-handplotfor𝑥 ≈𝜋/2.Thevariability
ofthefunctionvalues,i.e.thedots,issmallestforthemiddleplot
andlargestfortheright-handplot.
Bysuitablechoiceoftheconstants𝛾 1,...,𝛾 𝐽,thevariabilityoftherandom
variable ℎ(X)−(cid:205)𝐽 𝑗=1𝛾 𝑗𝑔 𝑗(X) canbemadesmallerthanthatof ℎ(X),and
thus a Monte Carlo estimate of 𝐼 based on (1.2) will have smaller Monte
CarlovariancethanthebasicMonteCarloestimator.Wecall(cid:205)𝐽 𝑗=1𝛾 𝑗𝑔 𝑗(X)
a control variate for ℎ(X). Heuristically, we want to choose 𝛾 1,...,𝛾 𝐽 so
thatℎ(X) ≈ 𝛾 0+(cid:205)𝐽 𝑗=1𝛾 𝑗𝑔 𝑗(X),whichmeansthatℎ(X)−(cid:205)𝐽 𝑗=1𝛾 𝑗𝑔 𝑗(X) is
approximatelyconstant.
Asasimpleexample,considerestimatingtheexpectationofsin(𝑋)where
𝑋hasastandardnormaldistributionN(0,1).Weknowthatthisexpectation
is0asthedistributionof 𝑋 issymmetricabout0andsin(−𝑥) = −sin(𝑥).
We will compare the simple Monte Carlo estimator of the expectation
with estimates using control variates with the functions 𝑔 (𝑥) = 𝑥 and
1
𝑔 (𝑥) = 𝑥2 −1. By using a Taylor expansion of sin(𝑥) at 𝑥 = 0 we have
2
sin(𝑥) ≈𝑥 forsmall𝑥,andthusasimplechoiceofcontrolvariateis𝑔 (𝑥).
1
We show pictorially the benefit of using this control variate in Figure
1.2,whereweseethatsin(𝑥) −𝑥 ≈ 0formost𝑥 valuessampledfromthe
standardnormaldistribution.ThisreducestheMonteCarlovarianceofthe
estimateofE[ℎ(𝑋)] byclosetoafactorof2.
Care must be taken with control variates, however. For example, if we1.1 MonteCarloMethods 11
performaTaylorexpansionofsin(𝑥) at𝑥 = 𝜋/2wegetsin(𝑥) ≈ 1− (𝑥−
𝜋/2)2/2, which suggests using −𝑔 (𝑥)/2+𝜋𝑔 (𝑥)/2 as a control variate.
2 1
However, this choice leads to an increase in the Monte Carlo variance by
overafactorof3.Figure1.2showsthatthefunctionsin(𝑥)−𝜋𝑥/2+(𝑥2−1)/2
is roughly constant for 𝑥 ≈ 𝜋/2, but overall it is more variable across the
range𝑥 ∈ [−2,2],wheremostoftheprobabilitymassofN(0,1) lies.
This example shows that the choice of 𝛾 1,...,𝛾 𝐽 is important when
using control variates. In some situations, there may be a natural way of
choosingthese–forexample,basedonaTaylorexpansionofthefunction
of interest around the mode of the distribution of X. However, it is also
possible to choose these values based on simulation. Ideally, we would
choose𝛾 1,...,𝛾 𝐽 tominimisetheMonteCarlovariance
∫ (cid:40) 𝐽 (cid:41)2
∑︁
ℎ(x)− 𝛾 𝑗𝑔 𝑗(x) 𝜋(x) dx−𝐼2,
𝑗=1
andwecanobtainaMonteCarloestimateofthis.Ifx 1,...,x𝑛 arerealised
samplesfrom𝜋(·),thenwecanchoose𝛾 1,...,𝛾 𝐽 tominimise
𝑛 (cid:32) 𝐽 (cid:33)2
∑︁ ∑︁
ℎ(x𝑘)− 𝛾 𝑗𝑔 𝑗(x𝑘) ,
𝑘=1 𝑗=1
which just involves minimising a sum of squares criterion. If we let h be
the 𝑛×1 vector whose𝑖th entry is ℎ(x𝑖), 𝜸 be the 𝐽 ×1 vector whose𝑖th
entryis 𝛾 𝑖,andZbethe 𝑛×𝐽 matrixwhose (𝑖, 𝑗)thentryis 𝑔 𝑗(x𝑖),then,
assumingZisoffullrank,theleast-squaresestimateof𝜸is
𝜸ˆ = (Z⊤Z)−1Z⊤h.
These estimates 𝜸ˆ depend on the Monte Carlo samples, and thus for the
MonteCarloestimateof𝐼tobeunbiasedweneedtouseanewsetofMonte
CarlosamplesfromXforestimating 𝐼 usingthe𝜸ˆ.
While we have presented the idea of control variates for estimating ex-
pectationsoffunctions,similarideascanbeusedwithimportancesampling
forestimatinggeneralintegrals.
1.1.5 MonteCarloIntegrationandBayesianStatistics
One of the most important applications of Monte Carlo methods occurs
withinBayesianstatistics.Toexplainwhy,considertheproblemofmaking
inferences, from data, about the parameter of a statistical model. We will12 Background
use the notation D to denote data in general. In some situations, we will
need to distinguish individual data points, and in those settings, we will
assumeD = {y 1,...,y𝑁},withy𝑖 beingthe𝑖thdatapointand𝑁 beingthe
sizeofourdataset.
We further assume that we have a model for the data. Let the model
depend on a parameter 𝜽, and denote the likelihood of the data under
our model by 𝐿(𝜽;D). The likelihood is the probability, or probability
density, of observing data D under our model if the parameter is 𝜽. In
Bayesianstatistics,werepresentbeliefs,oruncertainty,abouttheparameter,
𝜽,throughprobabilitydistributions.Ourbeliefsabout 𝜽 beforeseeingthe
dataaregivenbyaprior,𝜋 (𝜽),and,onceweobservedata,Bayes’Theorem
0
providestheupdatetotheposteriordistribution:
𝜋(𝜽 | D) ∝ 𝜋 (𝜽)𝐿(𝜽;D). (1.3)
0
Where it will not cause confusion, we may drop the explicit conditioning
onthedataintheposterior,andwrite𝜋(𝜽) ratherthan𝜋(𝜽 | D).
Assuming the correctness of our model, the posterior distribution con-
tainsallinformationabouttheparameter, 𝜽,thatcanbelogicallydeduced
from our prior belief and the dataset. From it, we can then obtain a point
estimatefor𝜽,suchasitsposteriormean,andquantifyuncertaintyinterms
oftheposteriorprobabilityof𝜽 lyinginagivensetofvalues.However,in
most applications, the posterior distribution is intractable, meaning that it
cannot be explicitly calculated. The central challenge is that the posterior
density 𝜋(𝜽 | D) isknown,viaBayes’Theorem,onlyuptoanormalising
constant.
The intractability of the posterior distribution is a key motivator for
Monte Carlo methods. If we can draw samples from 𝜋(𝜽 | D), then we
can obtain simple, and often accurate, Monte Carlo estimates of posterior
quantitiesofinterest.Givenrealisations𝜽 1,...,𝜽𝑛sampledfrom𝜋(𝜽 | D),
andafunctionℎ(𝜽) whoseexpectation
∫
𝐼 :=E 𝜋 [ℎ(𝜽)] = ℎ(𝜽)𝜋(𝜽 | D) d𝜽
isofinterest,define
𝑛
𝜇 (cid:98)ℎ(𝑛) := 𝑛1 ∑︁ ℎ(𝜽𝑘). (1.4)
𝑘=1
As mentioned earlier, for any function ℎ ∈ L1(𝜋), the strong law of large
numbers (1.1) tells us that we can estimate E 𝜋 [ℎ(𝜽)] as accurately as1.1 MonteCarloMethods 13
we desire using Monte Carlo integration, and provided enough samples
are taken: 𝜇 (cid:98)ℎ(𝑛) → E 𝜋 [ℎ(𝜽)] almost surely as 𝑛 → ∞. Moreover, if ℎ ∈
L2(𝜋) then the central limit theorem states that the Monte Carlo error,
𝜇 (cid:98)ℎ(𝑛) −E
𝜋
[ℎ(𝜽)] is𝑂 𝑝(𝑛−1/2).
Forexample,thevectorofposteriormeanscanbeestimatedby
𝑛
1 ∑︁
𝜽ˆ =
𝑛
𝜽𝑘,
𝑘=1
andtheposteriorprobabilityof𝜽 ∈ B forsomesetB canbeestimatedby
theproportionofMonteCarlosamplesinB
𝑛
1 ∑︁
Pˆ (𝜽 ∈ B) =
𝑛
I{𝜽𝑘 ∈ B},
𝑘=1
whereI{𝜽𝑘 ∈ B}istheindicatorfunctionoftheevent𝜽𝑘 ∈ B.
ThechallengewiththisMonteCarloapproachtoBayesianstatisticsisthe
difficultyinsamplingfrom 𝜋(𝜽),particularlyif 𝜽 ishigh-dimensional.Of
theMonteCarlointegrationmethodswehavementionedsofar,importance
samplingoffersanalternativewhenweareunabletosamplefrom𝜋directly.
Consider estimating the posterior expectation for some function ℎ(𝜽), so
ℎ(𝜽) = 𝜽 would give us the posterior mean of 𝜽 and ℎ(𝜽) = I{𝜽 ∈ B}
would give us the posterior probability of 𝜽 ∈ B. Let 𝑞(𝜽) be a proposal
distributionwiththesamesupportastheposterior.Thenwehave
∫ ∫ ℎ(𝜽)𝜋(𝜽)
E[ℎ(𝜽) | D] = ℎ(𝜽)𝜋(𝜽) d𝜽 = 𝑞(𝜽) d𝜽.
𝑞(𝜽)
Itiscommontodefineweights𝑤(𝜽) := 𝜋(𝜽)/𝑞(𝜽).Thengivenanindepen-
dentsample𝜽 1,...,𝜽𝑛from𝑞(𝜽),wecanestimatetheposteriorexpectation
bytheimportancesamplingestimator
𝑛
1 ∑︁
𝑛
𝑤(𝜽𝑘)ℎ(𝜽𝑘).
𝑘=1
Therearetwoissueswiththisestimator.Thefirstisthatasweonlyknow
theposterioruptoaconstantofproportionality,weonlyknowtheweights
up to a constant of proportionality. However, the constant of proportion-
ality can be estimated by setting ℎ(𝜽) = 1, whence E[ℎ(𝜽)] = 1 as the
expectationofaconstantistheconstant.Thuswecanusetheunnormalised
posteriordensityinthedefinitionoftheweights,andestimatethenormal-
ising constant as (1/𝑛)(cid:205)𝑛 𝑘=1𝑤(𝜽𝑘). The posterior expectation of ℎ(𝜽) is14 Background
thenestimatedas
∑︁ 𝑘𝑛
=1
(cid:205)𝑛
𝑗𝑤 =1( 𝑤𝜽𝑘 ()
𝜽𝑗)ℎ(𝜽𝑘),
whichrequiresknowingtheposteriordensityonlyuptoaconstantofpropor-
tionality.Oftenwedefinenormalisedweights𝑤∗(𝜽𝑘) = 𝑤(𝜽𝑘)/(cid:205)𝑛 𝑗=1𝑤(𝜽𝑗),
andwecanthenviewtheweightedsamples(𝜽𝑘,𝑤∗(𝜽𝑘)),for𝑘 =1,...,𝑛,
asadiscreteapproximationtotheposterior.
The second issue is that the Monte Carlo variances of our estimators
of posterior expectations depend on the variability of the weights. Often
this will be large if 𝜽 is high-dimensional. To see this, consider a toy
example where the posterior has independent components. Assume each
component is normal with mean 0 and variance 𝜎2, and the importance-
sampling proposal distribution is also independent over components, but
withastandardnormaldistribution,i.e.,withmeanzeroandunitvariance,
for each component. The importance sampling weight for a realisation
𝜽 = (𝜃 1,...,𝜃 𝑑) is
(cid:40) 𝜎2−1∑︁𝑑 (cid:41)
𝑤(𝜽) = 𝜎−𝑑 exp 𝜃2 .
2𝜎2 𝑖
𝑖=1
Now(cid:205)𝑑 𝜃2hasa𝜒2distributionundertheproposal,andusingthemoment
𝑖=1 𝑖 𝑑
generatingfunctionofa𝜒2distribution,weobtaintheMonteCarlovariance
𝑑
oftheweight:
var{𝑤(𝜽)} = 𝜎−𝑑 (cid:0) 2−𝜎2(cid:1)−𝑑/2−1.
√
Writing 𝜎2 = 1+𝜖, for some 𝜖 > 0, this variance is (1/ 1−𝜖2)𝑑 −1,
which increases exponentially with 𝑑. The focus of Markov chain Monte
Carlo(MCMC)methodsthatweintroduceinthenextchapteristoproduce
samplingalgorithmsthatavoidthisexponentialcurseofdimensionality.
1.2 ExampleApplications
Inlaterchapters,wewilldemonstratetheMonteCarlomethodsonsomeex-
amplemodelswhichwenowintroduce.Whilstthesemodelsaresomewhat
simple to describe, their posteriors exhibit many of the features of more
challenging posterior distributions, in particular, with respect to scalable
sampling.1.2 ExampleApplications 15
1.2.1 LogisticRegression
Logisticregressionmodelstherelationshipbetweenabinaryresponseand
asetofcovariates.Denotetheresponsesby 𝑦 1,...,𝑦 𝑁 andthecovariates
by 𝑑-dimensionalvectorsx 1,...,x𝑁.Then,logisticregressionmodelsthe
data (the responses) as conditionally independent, given a 𝑑-dimensional
parameter𝜽 andthecovariates,andthat
exp{x⊤𝜽}
P(cid:0)𝑌 = 𝑦 𝑗|x𝑗,𝜽(cid:1) = 1+exp{𝑗 x⊤𝜽}.
𝑗
Anintercepttermcanbeincludedinthemodelbysettingthefirstcoordinate
ofeachofx 1,...,x𝑁 tobe1.
Ourinterestwillbeinsamplingfromtheposteriordistributionof𝜽.To
definetheposterior,weneedtospecifyaprior 𝜋 (𝜽),andwewillassume
0
that our prior is Gaussian with mean 0 and variance 𝚺 . This leads to a
𝜽
posterior distribution, 𝜋(𝜽|D), which can be written succinctly up to a
multiplicativeconstantas
𝜋(𝜽|D)
∝exp(cid:26) −1 𝜽⊤𝚺−1𝜽(cid:27) (cid:214)𝑁 exp{𝑦 𝑗x⊤ 𝑗𝜽}
.
2 𝜽 1+exp{x⊤𝜽}
𝑗=1 𝑗
This is a canonical, albeit relatively simple, test problem for sampling
methodologies. When we consider sampling methods for this model, we
will drop the explicit conditioning on data D and use 𝜋(𝜽) to denote the
targetdistributionofthesampler.Thesamplersweconsiderwilloftenuse
gradientinformationabouttheirtargetdistribution,andwehave
𝜕lo 𝜕g 𝜃𝜋 𝑖(𝜽) =−(cid:2) 𝜽⊤𝚺 𝜽−1(cid:3) 𝑖 +∑︁ 𝑗𝑁
=1
𝑥( 𝑗𝑖) (cid:40) 𝑦 𝑗 − 1+ex ep x{ px {⊤ 𝑗 x𝜽 ⊤ 𝑗} 𝜽}(cid:41) , (1.5)
where𝑥( 𝑗𝑖) indicatesthe𝑖thcomponentofx𝑗.
1.2.2 BayesianMatrixFactorisation
Bayesian matrix factorisation attempts to find a representation of a high-
dimensionalmatrixastheproductoftwolower-dimensionalmatrices.Con-
sider an 𝑛 × 𝑚 matrix Y, and let 𝜽 = {U,V} where U and V are 𝑛 × 𝑑
and 𝑑 ×𝑚 matrices respectively. Then the approximation is Y ≈ UV. If
𝑑 ≪ min{𝑚,𝑛} then this can lead to a substantial reduction in dimen-
sion, and the model can be viewed as attempting to find low-dimensional
structureinY.16 Background
TheinterpretationofthismodelisthateachrowofVisafactor,andwe
aimtoapproximateeachrowofYasalinearcombinationofthesefactors.
TheentriesinUarecalledfactorloadings,andgivetherelativeweightof
eachfactorforeachrowofY.
One common approach to fitting these models is to use a Gaussian
workingmodel,thusuptoadditiveconstants,thelog-likelihoodis
 𝑛 𝑚 (cid:32) 𝑑 (cid:33)2
𝐿(𝜽;D) =−𝑛𝑚log𝜎− 2𝜎1
2
 ∑︁∑︁ 𝑌
𝑖,𝑗
−∑︁ 𝑈 𝑖,𝑘𝑉
𝑘,𝑗
 ,
 𝑖=1 𝑗=1 𝑘=1  
 
where𝜎2isthevarianceofthedifferencebetweenentriesofYandUV.In
Bayesianmatrixfactorisation,wethenintroduceapriorontheparameters
U and V. Often, the prior for each entry is Gaussian, or is a mixture of a
Gaussianandapoint-massatzero,asthisencouragessparsityinthefactors
which potentially aids the interpretation of U and V. It is also possible to
introduceaprioroverthenumberoffactors,𝑑,withthepriorsfortheentries
ofUandVpotentiallydependingon𝑑.
1.2.3 BayesianNeuralNetworksforClassification
Artificial neural networks are a flexible and popular class of models used
in machine learning for solving supervised learning problems, such as
regressionandclassificationtasks.Inthecaseofclassification,assumethat
𝑦 1,𝑦 2,...,𝑦
𝑁
areobserveddata,whereeach𝑦 𝑗representsoneof𝐺classes,
i.e. 𝑦 𝑗 ∈ {1,2,...,𝐺}. Assuming 𝑑−dimensional vectors x 1,x 2,...,x𝑁
forthecovariates,thenunderasimpletwo-layerneuralnetworkmodel,the
probabilityofaparticularclass 𝑦 𝑗 is
P(𝑌 = 𝑦 𝑗|x𝑗,𝜽) ∝exp(A⊤ 𝑦 𝑗𝜎(B⊤x𝑗 +b)+𝑎 𝑦 𝑗), (1.6)
where b is a 𝑑 ℎ-dimensional vector, B is a 𝑑 × 𝑑 ℎ matrix, with 𝑑 ℎ the
dimension of the variables in the hidden layer. The function 𝜎 : R𝑑 ℎ →
(0,1)𝑑 ℎ isavectorsoftmaxfunctionwith𝜎(z) 𝑖 = exp(𝑧 𝑖)/{(cid:205)𝑑 𝑗=ℎ 1exp(𝑧 𝑗)}
for𝑖 = 1,...,𝑑 ℎ. The notation A𝑖 refers to the𝑖-th column of the 𝑑 ℎ ×𝐺
matrixA.Theparametersofthemodel𝜽 =vec(a,A,b,B) arerepresented
byvectorsa,b,commonlyreferredtoasbiases,andmatricesA,B,which
arecommonlyreferredtoasweights.
TakingaBayesianapproachtoparameterestimation,wecanplaceinde-
pendentGaussianpriorsoneachoftheelementsofthebiasesandweights
in𝜽.MonteCarloalgorithmscanbeusedtosamplefromtheposterior,1.3 MarkovChains 17
𝑁
(cid:214)
𝜋(𝜽|D) ∝ 𝜋 0(𝜽) P(𝑦 𝑗|x𝑗,𝜽). (1.7)
𝑗=1
For Bayesian neural network models, the dataset sizes tend to be very
large and approximating the posterior distribution requires Monte Carlo
methods which are scalable to large datasets. In Chapter 3, we will use
stochastic gradient Markov chain Monte Carlo algorithms to approximate
theBayesianneuralnetworkposteriordistribution.
1.3 MarkovChains
This section describes discrete-time Markov chains, focusing on the con-
cepts that will be required to understand the Markov chain Monte Carlo
method and its efficiency: the stationary distribution, reversibility, con-
vergence to the stationary distribution, ergodic averages, integrated auto-
correlationtimeandeffectivesamplesize.
Definition 1.1 A discrete-time Markov chain on a state space X is a
collectionofrandomvariables{𝑋 𝑘}∞ 𝑘=0witheach𝑋
𝑘
∈ X,suchthatforany
A ⊆ X,
P(𝑋 𝑘+1 ∈ A | 𝑋 𝑘 =𝑥 𝑘,...,𝑋 0 =𝑥 0) =P(𝑋 𝑘+1 ∈ A | 𝑋 𝑘 =𝑥 𝑘); (1.8)
conditional on the current state, the distribution of the next state is inde-
pendentofallpreviousstates.
Inthischapter,wewillonlyconsiderhomogeneousMarkovchains,where
thedistributionofthenextstategiventhecurrentstatedoesnotdependon
thevalueof 𝑘.Suchachainhasastationarydistribution,𝜈,if 𝑋 𝑘 ∼ 𝜈 =⇒
𝑋
𝑘+1
∼ 𝜈.Ifthechainalsohasauniquelimitingdistribution,thenthismust
be 𝜈 since, by repeated induction, if 𝑋 𝑗 ∼ 𝜈 then 𝑋 𝑘 ∼ 𝜈 for all 𝑘 > 𝑗,
includingas 𝑘 →∞.
ThefollowingtwoexamplesofMarkovchainsontheverticesofan 𝑚-
sided polygon illustrate different ways that a chain can be stationary. We
labeltheverticesofthepolygonfrom0to𝑚−1,increasinginaclockwise
direction;thus,X = {0,1,...,𝑚−1}.
Example1.2 (SeeFigure1.3,left.)Let{𝑋 𝑘}∞ 𝑘=0beaMarkovchainonthe
verticesofan𝑚-sidedpolygonwherethestateattime𝑘+1isobtainedfrom
thestateattime 𝑘 bymovingtothenextvertexinaclockwisedirection.If
attime𝑘 thechainisequallylikelytobeateachofthevertices,thenthisis18 Background
𝐴 𝐴
1/3 1/3
𝐼 𝐵 𝐼 𝐵
1/3
𝐻 𝐶 𝐻 𝐶
𝐺 𝐷 𝐺 𝐷
𝐹 𝐸 𝐹 𝐸
Figure1.3 9-sidedpolygonwheretheMarkovchainonlymoves
clockwise(leftfigure),asinExample1.2ormoveseithera
clockwiseoranti-clockwisedirectionwithprobability1/3(right
figure),asinExample1.3.
stillthecaseattime𝑘+1.ThestationarydistributionhasP(𝑋
𝑘
=𝑥) =1/𝑚
for𝑥 ∈ X.
Example 1.3 (See Figure 1.3, right.) Let {𝑋 𝑘}∞
𝑘=0
be a Markov chain on
theverticesofan𝑛-sidedpolygonwherethestateattime 𝑘 +1isobtained
fromthestateattim.e𝑘 byperformingoneofthefollowingmoves,eachof
whichhasaprobabilityof1/3:movetothenextvertexinananti-clockwise
direction; do not move; move to the next vertex in a clockwise direction.
AswithExample1.2thestationarydistributionhasP(𝑋 𝑘 =𝑥) = 1/𝑚 for
𝑥 ∈ X.
1.3.1 ReversibleMarkovchains
Example1.2hasaclearflowinaclockwisedirectionand,becauseofthis,
isanexampleofanon-reversibleMarkovchain;thesewillbediscussedin
detailinChapter4.Bycontrast,inExample1.3,consideranytwoadjacent
vertices: at stationarity, the probability of being at the first and moving to
thesecondisthesameastheprobabilityofbeingatthesecondandmoving
tothefirst.Indeed,thisistrueofanypairofvertices,withtheprobability
being0iftheyarenotadjacent.ThisisanexampleofareversibleMarkov
chain.
Definition 1.4 A Markov chain {𝑋 𝑘}∞
𝑘=1
with a state space of X is re-
versiblewithrespecttoadistribution 𝜈 when,foranytwosets B,C ⊆ X,
if 𝑋 𝑘 ∼ 𝜈thenP(𝑋 𝑘 ∈ B,𝑋 𝑘+1 ∈ C) =P(𝑋 𝑘 ∈ C,𝑋 𝑘+1 ∈ B).1.3 MarkovChains 19
Considerthedecomposition
P(𝑋 ∈ B,𝑋 ∈ C) =P(𝑋 ∈ B)P(𝑋 ∈ C|𝑋 ∈ B).
𝑘 𝑘+1 𝑘 𝑘+1 𝑘
Thefirsttermontheright-handsideistheamountofprobabilitymassinB
attime𝑘 andthesecondtermisthefractionofthatmasswhichmovestoC
attime𝑘+1,sotheproductistheamountofprobabilitymassmovingfrom
B to C.Ifthechainisreversiblewithrespectto 𝜈 and 𝑋 𝑘 ∼ 𝜈,thenthisis
alsotheamountofmassmovingfromC toB.Giventhisbalance,referred
to as detailed balance, we would expect the total amount of probability
massinanysettoremainconstant.Indeed,settingC =XinDefinition1.4,
weseethatreversibilityimpliesthatif𝑋
𝑘
∼ 𝜈,P(𝑋
𝑘
∈ B) =P(X
𝑘+1
∈ B).
SincethisisalsotrueforallB, 𝑋
𝑘+1
∼ 𝜈.
1.3.2 Convergence,Averages,andVariances
In Example 1.3, whatever the value or distribution of 𝑋 , as 𝑘 → ∞ the
0
distribution of 𝑋 𝑘 converges to the stationary distribution. For simplicity
of presentation, we show this when 𝑚 = 2𝑚′ +1 is odd. For all 𝑥 ,𝑥 ∈
0
X, P(𝑋 𝑚′ =𝑥|𝑋 0 =𝑥 0) ≥ 1/3𝑚′ since it takes at most 𝑚′ moves in a
single direction to reach 𝑥, and if the chain arrives earlier, we include the
probabilityofitstayingat𝑥 untiltime𝑚′.Thus,thetransitionprobability
after𝑚′ stepscanbewrittenasamixture:
P(𝑋 𝑚′ =𝑥|𝑋 0 =𝑥 0) = 𝛿𝜈(𝑥)+(1−𝛿)𝑞(𝑥|𝑥 0), (1.9)
forsomeconditionalprobabilitymassfunction𝑞andwith𝛿 =𝑚/3𝑚′.The
distribution at the start of a given iteration can always be thought of as a
mixtureof𝜈andsomeotherdistribution,wherethemixtureprobabilityfor
𝜈couldbe0.Wecanimaginethatthereisahiddencoin,andifitisshowing
”heads” then the distribution of the chain is 𝜈. Since 𝜈 is the stationary
distribution,ifthecoiniscurrentlyshowing”heads”itwillstillbeshowing
headsafterafurther𝑚′moves.Ifthecoinisshowing”tails”then(1.9)tells
usthatthereisaprobabilityofatleast𝛿thatitwillbeshowingheadsafter
thenext𝑚′ moves.Equivalently,themixtureprobabilityofthecomponent
thatisnot𝜈hasbeenmultipliedby1−𝛿orless.After𝑘𝑚′ iterations,itis,
thereforeatmost (1−𝛿)𝑘 →0as 𝑘 →∞.
However,convergencetoastationarydistributiondoesnotoccurforall
Markovchains.ThechaininExample1.2isdeterministic:if 𝑋 = 0,then
0
𝑋 𝑘𝑚 = 0 for all integers, 𝑘. The following examples illustrate two further
cases.20 Background
𝐴
1/2 1/2
𝐼 𝐵
𝐻 𝐶
𝐺 𝐷
𝐹 𝐸
Figure1.4 9-sidedpolygonwithMarkovtransitionsdescribedin
Example1.5.
Example1.5 (SeeFigure1.4.)AlterExample1.3sothatthechaincannot
remainatitscurrentvertexbutmustmoveeitherclockwiseoranticlockwise
by a single vertex, each with a probability of 1/2. As with the Examples
1.2and1.3,thestationarydistributionhasP(𝑋
𝑘
=𝑥) =1/𝑚 for𝑥 ∈ X.
If𝑛isanevennumber,and𝑋 iseventhenthechaininExample1.5only
0
visits even-numbered states at even-numbered times, and odd-numbered
statesatodd-numberedtimes.Suchchainsaretermedperiodicandclearly
donotconvergetotheirstationarydistribution.
Example 1.6 Consider a Markov chain of the form in Example 1.3, but
on two separate 𝑚-sided polygons with no movement between the two. A
chain with separate regions between which there can be no movement is
termedreduciblebecauseitcanbereducedtosimplercomponentparts.
Areduciblechaindoesnotevenhaveasinglestationarydistribution.In
Example1.6forany 𝛽 ∈ [0,1] thedistributionwithprobabilities 𝛽/𝑚 for
eachvertexonthefirstpolygonand(1−𝛽)/𝑚foreachvertexonthesecond
polygonisstationary.
Achainwhichisnotreducibleistermedirreducible,andachainwhich
isnotperiodicistermedaperiodic.
ErgodicAverages
TheergodictheoremforaMarkovchainonageneralstate-space,X,states
thatprovidedthechainsatisfiesnaturalgeneralisationsofirreducibilityand
aperiodicity, and has a proper stationary distribution, 𝜈, then as 𝑘 → ∞,
thedistributionof𝑋 𝑘convergestothatstationarydistribution.Furthermore,
subjecttothesameconditions,foranyℎ ∈ L1(𝜈),samplesfromtheMarkov1.3 MarkovChains 21
chainsatisfyastronglawoflargenumbers:
𝑛
1 ∑︁
(cid:98)𝐼 𝑛(ℎ) := 𝑛 ℎ(𝑋 𝑘) →E 𝜈 [ℎ(𝑋)] (1.10)
𝑘=1
almostsurelyas𝑛 →∞.
IntegratedAuto-CorrelationTimeandEffectiveSampleSize
Let us assume that 𝑋 is, in fact, drawn from the stationary distribution.
0
Define 𝜎 ℎ2 := Var𝜈 [ℎ(𝑋)] andassume 𝜎 ℎ2 < ∞.For 𝑘 ∈ {0,1,2,...},the
lag-𝑘auto-correlationis𝜌
𝑘
:=Cor[ℎ(𝑋 0),ℎ(𝑋 𝑘)] =Cor(cid:2)ℎ(𝑋 𝑗),ℎ(𝑋 𝑗+𝑘)(cid:3)
since the Markov chain is time-homogeneous. If the 𝑋 𝑗 were independent
(cid:104) (cid:105)
samples from 𝜈 then 𝑛Var (cid:98)𝐼 𝑛(ℎ) = 𝜎 ℎ2. For a stationary Markov chain
with
∞
∑︁
|𝜌 𝑘| < ∞, (1.11)
𝑘=1
itholdsthat
(cid:104) (cid:105)
lim 𝑛Var (cid:98)𝐼 𝑛(ℎ) = 𝜎 ℎ2 IACTℎ, (1.12)
𝑛→∞
where
∞
∑︁
IACTℎ :=1+2 𝜌 𝑘, (1.13)
𝑘=1
is the integrated auto-correlation time. To see why this is the case, firstly,
without loss of generality, take ℎ to have E 𝜈 [ℎ(𝑋)] = 0; if this is not
true initially, we subtract off the expectation: the variance properties are
unchanged.Then
(cid:104) (cid:105) 1 (cid:34) ∑︁𝑛 ∑︁𝑛 (cid:35) 𝜎2 ∑︁𝑛 ∑︁𝑛
𝑛Var (cid:98)𝐼 𝑛(ℎ) = 𝑛E ℎ(𝑋 𝑗)ℎ(𝑋 𝑘) = 𝑛ℎ 𝜌 |𝑘−𝑗|. (1.14)
𝑘=1 𝑗=1 𝑘=1 𝑗=1
But(cid:205)𝑛 𝑘=1(cid:205)𝑛 𝑗=1𝜌 |𝑘−𝑗| =𝑛𝜌 0+2(cid:205)𝑛 𝑘=1(𝑛−𝑘)𝜌 𝑘,so
𝑛 (cid:104) (cid:105) ∑︁𝑛 (cid:18) 𝑘(cid:19) ∑︁∞ (cid:18) 𝑘(cid:19)
𝜎2Var (cid:98)𝐼 𝑛(ℎ) =1+2 1− 𝑛 𝜌 𝑘 =1+2 max 0,1− 𝑛 𝜌 𝑘.
ℎ 𝑘=1 𝑘=0
Given(1.11),thedominatedconvergetheorempermitsustoexchangethe
ordering of the limit as 𝑛 → ∞ and the sum over 𝑘, which provides the
limit(1.12).22 Background
Thepracticalconsequenceof (1.12)isthat,forfinite𝑛,
(cid:104) (cid:105) 𝜎2
Var (cid:98)𝐼 ℎ ≈ 𝑛/IACℎ Tℎ, (1.15)
thesameasthevarianceif𝑛/IACTℎi.i.d.samplesfrom𝜈hadbeenused.The
quantity 𝑛/IACTℎ is, therefore, known as the effective sample size. Since
they relate directly to the inverse variance of (cid:98)𝐼 𝑛(ℎ), effective sample size
andtheinverseoftheintegratedauto-correlationtimeareusefulmeasures
oftheefficiencyofaMarkovchainforestimatingE
𝜈
[ℎ(𝑋)].
1.4 StochasticDifferentialEquations
TheLangevinstochasticdifferentialequationisthebasisfortheMetropolis
Adjusted Langevin Algorithm (Section 2.1.4) and for stochastic gradient
Langevin methods (Chapter 3). It is also key to understanding the effi-
ciencyofvariousMetropolis–Hastingsalgorithmswhenthedimension,𝑑,
ishigh(seeChapter2).Westartwithaheuristicintroductiontostochastic
differential equations before considering a special case of the Langevin
diffusionknownastheOrnstein–Uhlenbeckprocessandthenmovingonto
thegeneralLangevindiffusion.
Consideradifferentialequationoftheform
d𝑥
𝑡 = 𝑎(𝑥 ,𝑡),
d𝑡 𝑡
withaknowninitialvaluefor𝑥 .DiscretisingtimeleadstothesimpleEuler
0
approximation
𝛿𝑥 ≈ 𝑎(𝑥 ,𝑡)𝛿𝑡,
𝑡 𝑡
where𝛿𝑥 𝑡 = 𝑥 𝑡+𝛿𝑡 −𝑥 𝑡.Setting𝛿𝑡 =𝑇/𝑚,startingwith𝑥 0,andrecursively
applying the Euler update 𝑚 times leads to an approximation (cid:98)𝑥 𝑇, which
approachesthetruevalue𝑥
𝑇
as𝑚 →∞.
Insteadofdeterministicupdates,wemightwishtoallowfortheaddition
of random noise with scale proportional to 𝑏(𝑥 𝑡,𝑡). The initial value, 𝑋 0,
may now be random and setting 𝛿𝑋 𝑡 := 𝑋 𝑡+𝛿𝑡 − 𝑋 𝑡 leads to one possible
update
𝛿𝑋
𝑡
≈ 𝑎(𝑋 𝑡,𝑡)𝛿𝑡+𝑏(𝑋 𝑡,𝑡)𝜖 𝑡, 𝜖
𝑡
∼N(0,𝛿𝑡),
where the Gaussian noise terms 𝜖 𝑡 are independent of all previous ran-
domness, and 𝑋 𝑡 has become a random variable. A noise distribution of
theformN(0,𝛿𝑡) ischosenbecauseitisself-consistent.Forexample,with1.4 StochasticDifferentialEquations 23
𝑎(𝑋 𝑡,𝑡) = 𝑎and𝑏(𝑥 𝑡,𝑡) = 𝑏,aftertwotimestepsinitialisedat 𝑋
0
=𝑥 0,we
have
𝑋
2𝛿𝑡
≈𝑥 0+𝑎𝛿𝑡+𝑏𝜖
𝛿𝑡
+𝑎𝛿𝑡+𝑏𝜖
2𝛿𝑡
=𝑥 0+2𝑎𝛿𝑡+𝑏𝜖˜
2𝛿𝑡
where𝜖˜ 2𝛿𝑡 ∼N(0,2𝛿𝑡),sincethetwonoiseterms𝜖 𝛿𝑡,𝜖 2𝛿𝑡 areindependent.
However,theright-handsideofthisexpressionisexactlyofthesameform
wewouldgetfromasingletimestepofsize2𝛿𝑡 toobtain 𝑋 2𝛿𝑡 from 𝑋 0.
Theprocesswith𝑎 =0,𝑏 =1and𝑋 =0consistsofasequenceofmean-
0
zeroGaussianincrements,eachwithavarianceof𝛿𝑡.Thisisadiscretisation
ofaprocessknownasBrownianmotion,whichisoftendenotedby𝑊 𝑡.In
particular,wehavethat
𝛿𝑊
𝑡
=𝑊
𝑡+𝛿𝑡
−𝑊
𝑡
∼N(0,𝛿𝑡),
and 𝑊 𝑡 ∼ N(0,𝑡). From the definition of 𝑊 𝑡, we may rewrite the noisy
updateas
𝛿𝑋 𝑡 ≈ 𝑎(𝑋 𝑡,𝑡)𝛿𝑡+𝑏(𝑋 𝑡,𝑡)𝛿𝑊 𝑡. (1.16)
Considerthisprocessonsomeinterval [0,𝑇],with𝛿𝑡 =𝑇/𝑚 and 𝑋 =𝑥 ,
0 0
forsomeinitialvalue𝑥 .Subjecttosomeregularityconditions,thelimitas
0
𝑚 →∞existsandiswritten:
d𝑋
𝑡
= 𝑎(𝑋 𝑡,𝑡)d𝑡+𝑏(𝑋 𝑡,𝑡)d𝑊 𝑡.
Thisisknownasastochasticdifferentialequation(SDE),and(1.16)isthe
Euler–Maruyama approximation to it. Subject to the initial condition, the
solutiontothisSDEisthestochasticprocess{𝑋 𝑡} 𝑡∈[0,𝑇] obtainedfromthe
limit𝛿𝑡 →0ofthediscrete-timeprocessdefinedthrough(1.16).
The above heuristic describes a one-dimensional SDE and its Euler–
Maruyama discretisation; however, it is straightforward to extend these to
higherdimensionswithX𝑡 ∈ R𝑑,a : R𝑑 ×[0,∞) → R𝑑,W𝑡 ∈ R𝑘 andthe
𝑑×𝑘 matrixb:R𝑑 ×[0,∞) →R𝑑𝑘.
A stochastic process that satisfies an SDE is called a diffusion. For the
mostpart,wewilldealwithtime-homogeneousdiffusions,where𝑎 and 𝑏
havenoexplicittimedependence;however,time-inhomogeneousdiffusions
willbeusedinChapter3.
1.4.1 TheOrnstein–UhlenbeckProcess
ConsidertheSDE
1
d𝑋
𝑡
=− 2𝜎2𝑏2𝑋 𝑡d𝑡+𝑏d𝑊 𝑡.24 Background
TheEuler–Maruyamadiscretisationgives
(cid:18) 𝑏2 (cid:19)
𝑋
𝑡+𝛿𝑡
≈ 𝑋
𝑡
+𝛿𝑋
𝑡
= 1− 2𝜎2𝛿𝑡 𝑋
𝑡
+𝑏𝛿𝑊 𝑡.
Since𝛿𝑊 𝑡isGaussiandistributedandindependentof𝑋 𝑡,if𝑋 𝑡isGaussianso
is 𝑋 𝑡+𝛿𝑡.Moreover,ifE[𝑋 𝑡] = 0,thenE[𝑋 𝑡+𝛿𝑡] = 0.Finally,ifVar[𝑋 𝑡] =
𝜎2 then
(cid:18) 𝑏2 (cid:19)2 1
Var[𝑋 𝑡+𝛿𝑡] = 1− 2𝜎2𝛿𝑡 𝜎2+𝑏2𝛿𝑡 = 𝜎2+ 4𝜎4𝑏4𝛿𝑡2.
In the limit 𝑚 → ∞, as the number of increments is increased, with
𝛿𝑡 = 𝑇/𝑚 ↓ 0, the term in 𝛿𝑡2 becomes irrelevant: the variance does not
change.Thus,if 𝑋 0 ∼ N(0,𝜎2) then 𝑋 𝑡 ∼ N(0,𝜎2) forall𝑡 > 0;theSDE
is stationary. Shifting the coordinate system by 𝑚 we see that the slightly
moregeneralSDE
1
d𝑋 𝑡 =− 2𝜎2𝑏2(𝑋 𝑡 −𝑚)d𝑡+𝑏d𝑊 𝑡 (1.17)
hasastationarydistributionofN(𝑚,𝜎2).TheprocessarisingfromtheSDE
(1.17) is known as the Ornstein–Uhlenbeck (OU) process. Substituting
𝑠 = 𝑏2𝑡, the SDE becomes d𝑋 𝑠 = −(𝑋 𝑠 − 𝑚)/(2𝜎2)d𝑠 + d𝑊 𝑠, which
explains why 𝑏2 is termed the speed of the diffusion. Figure 1.5 presents
threerealisationsofOUprocesseswithstationarydistributionN(𝑚,1)and
started from the corresponding 𝑚/2. Each diffusion has a different speed,
andtheeffectofthisontheconvergenceto,andmixingwithin,thestationary
distributionisclearlyvisible.
1.4.2 TheInfinitesimalGenerator
The infinitesimal generator (or, simply, generator) of a continuous-time
stochasticprocessactsonafunctionℎoftheprocess:
(Lℎ)(x) := 𝜕𝜕 𝑡E[ℎ(X𝑡)|X 0 =x](cid:12) (cid:12) (cid:12)
(cid:12)
𝑡=0
= l 𝛿i 𝑡m ↓0E[ℎ(X𝛿 𝛿𝑡) 𝑡]−ℎ(x) . (1.18)
Thesetoffunctionsforwhichthelimitexistsforallxiscalledthedomain
oftheinfinitesimalgenerator.Subjecttoregularityconditions,thisincludes
the set of compactly supported functions with a second derivative that is
continuous,denoted𝐶2.
0
For processes defined by an SDE, we can gain some insight into their1.4 StochasticDifferentialEquations 25
 
 
 
 
 
            
t
Figure1.5 ThreerealisationsoftheOrnstein–Uhlenbeck
processes,allwith𝜎 =1,andonthetimeinterval [0,10].Other
parametersettingsare𝑥 =2,𝑚 =4and𝑏 =3;𝑥 =𝑚 =0and
0 0
𝑏 =1;𝑥 =−2,𝑚 =−4and𝑏 =1/3.
0
generatorbyconsideringaTaylorexpansion.Forsimplicityofpresentation,
weconsider𝑥 ∈ R:
(cid:20) (cid:21)
1 1 1
E[ℎ(𝑋 )−ℎ(𝑥)] = E (𝑋 −𝑥)ℎ′(𝑥)+ (𝑋 −𝑥)2ℎ′′(𝑥)+... .
𝛿𝑡 𝛿𝑡 𝛿𝑡 𝛿𝑡 2 𝛿𝑡
The Euler–Maruyama approximation of the SDE is 𝑋 𝛿𝑡 − 𝑥 ≈ 𝑎(𝑥)𝛿𝑡 +
𝑏(𝑥)𝛿𝑊 𝑡. Thus E[𝑋 𝛿𝑡 −𝑥] ≈ 𝑎(𝑥)𝛿𝑡 and E(cid:2) (𝑋 𝛿𝑡 −𝑥)2(cid:3) ≈ 𝑏(𝑥)2𝛿𝑡 +
𝑎(𝑥)2[𝛿𝑡]2, with all higher order expectations at most 𝑜(𝛿𝑡). Thus, we
mightexpectthat
dℎ 1 d2ℎ
(Lℎ)(𝑥) = 𝑎(𝑥) + 𝑏(𝑥)2 ,
d𝑥 2 d𝑥2
andthisisindeedthecase.Foramultivariatediffusion,thegeneratoris
(Lℎ)(x) =∑︁ 𝑖=𝑑
1
𝑎 𝑖𝜕𝜕 𝑥ℎ 𝑖(cid:12) (cid:12) (cid:12)
(cid:12)
x+ 1 2∑︁ 𝑖=𝑑
1
∑︁ 𝑗𝑑
=1
(𝑏𝑏⊤) 𝑖,𝑗𝜕𝑥𝜕 𝑖2 𝜕ℎ
𝑥
𝑗(cid:12) (cid:12) (cid:12)
(cid:12)
x. (1.19)
Generatorsofdiffusionprocessesareusedinthenextsubsectiontoderive
x26 Background
thestationarydensityoftwoclassesofdiffusionthatappearrepeatedlyin
Chapters 2 and 3. Generators of diffusions are also used in Chapter 6 for
theassessmentandimprovementofalgorithms.Finally,Chapter5employs
the generators of another class of continuous-time stochastic processes to
determinetheprocesses’stationarydistributions.
1.4.3 LangevinDiffusions
Wenowdescribetwoclassesofdiffusion,theoverdampedandunderdamped
Langevindiffusions,wherethestationarydensityformsanexplicitpartof
theSDEformulation.
TheOverdampedLangevinDiffusion
Consider a positive, differentiable density function 𝑓(x) for x ∈ R𝑑, and
thefollowingSDE:
1
dX𝑡 = ∇log 𝑓(X𝑡)𝑏2d𝑡+𝑏dW𝑡. (1.20)
2
AsolutiontothisSDEisknownasanoverdampedLangevindiffusion.The
OUprocess(1.17)with 𝑓(𝑥) = N(𝑥;𝑚,1/𝑎) isaspecialcaseofthisclass
ofdiffusions,andinthiscase,asseeninSection1.4.1, 𝑓 isthedensityofthe
stationaryprocess.Infact,thisistrueingeneral:thestationarydensityofthe
overdamped Langevin diffusion (1.20) is 𝑓. To see this in one dimension,
considertheinfinitesimalgeneratorofthediffusion:
1 𝑓′(𝑥) 1
(Lℎ)(𝑥) = 𝑏2 ℎ′(𝑥)+ 𝑏2ℎ′′(𝑥).
2 𝑓(𝑥) 2
Thisistherateofchangeoftheexpectationofℎ(𝑋 𝑡) at𝑡 =0,whenstarted
from 𝑋 = 𝑥. Suppose instead that 𝑋 has a density of 𝑓. Then, the rate
0 0
of change of the expectation of 𝑋 𝑡 at 𝑡 = 0 can be calculated by taking
expectationswithrespectto 𝑋 .Thisis
0
1 ∫ (cid:26) 𝑓′(𝑥) (cid:27) 1 ∫
𝑏2 ℎ′(𝑥)+ℎ′′(𝑥) 𝑓(𝑥) d𝑥 = 𝑏2 {𝑓(𝑥)ℎ′(𝑥)}′ d𝑥 =0
2 𝑓(𝑥) 2
for all sufficiently smooth ℎ with compact support. Thus, if 𝑋 ∼ 𝑓,
0
dd 𝑡E[ℎ(𝑋 𝑡)](cid:12) (cid:12)
𝑡=0
= 0. This is true for all ℎ ∈ 𝐶 02, and so the distribution
of𝑋
𝑡
doesnotchangeas𝑡increasesfrom0.Thedistributionattime0must,
therefore, be the stationary distribution of the Langevin diffusion (1.20),
and 𝑓 isthecorrespondingstationarydensity.
WhenLangevindiffusionsareemployedinaBayesiansetting, 𝑓(x)isof-
tenaposteriordensitywhosenormalisingconstantis,typically,intractable.1.4 StochasticDifferentialEquations 27
Thefactthatthecalculationof∇log 𝑓(X𝑡)doesnotrequirethisnormalising
constantiscrucialtothepracticaluseofthesediffusions.
TheUnderdampedLangevinDiffusion
The underdamped Langevin diffusion extends the state space to include a
velocitycomponent,P𝑡:
dX𝑡 =P𝑡d𝑡, (1.21)
√︁
dP𝑡 =−𝛾P𝑡d𝑡+𝑐∇log 𝑓(X𝑡)d𝑡+ 2𝛾𝑐dW𝑡 (1.22)
Intuitively, dividing (1.22) through by 𝛾 and taking the limit as 𝛾 → ∞
and 𝑐 → ∞ with 𝑐/𝛾 = 𝑏2/2 fixed, we obtain the overdamped Langevin
diffusion,sothelatterisalimitingcaseoftheunderdampeddiffusion.
TheunderdampedLangevindiffusiontargets 𝑓(x)𝑔(p),where
(cid:18) (cid:19)
1 1
𝑔(p) = √ exp − ∥p∥2 .
2𝜋𝑐 2𝑐
To see this we, again, restrict ourselves to the one-dimensional case to
simplifythepresentationand,again,westartfromthegenerator:
𝑓′(𝑥)
(Lℎ)(𝑥,𝑝) = 𝑝ℎ (𝑥,𝑝)−𝛾𝑝ℎ (𝑥,𝑝)+𝑐 ℎ (𝑥,𝑝)+𝛾𝑐ℎ (𝑥,𝑝),
𝑥 𝑝 𝑓(𝑥) 𝑝 𝑝,𝑝
where we have used subscripts to denote differentiation of ℎ with respect
to𝑥 or 𝑝.Thequantity (Lℎ)(𝑥,𝑝) istherateofchangeoftheexpectation
of ℎ(𝑋 𝑡,𝑃 𝑡) at 𝑡 = 0, when started at 𝑋
0
= 𝑥 and 𝑃
0
= 𝑝. Thus if 𝑋
0
and
𝑃 have respective densities of 𝑓(𝑥) and 𝑔(𝑝), then the rate of change of
0
E[ℎ(𝑋 𝑡,𝑃 𝑡)] at𝑡 =0is
∬ (cid:26) 𝑓′(𝑥) (cid:27)
𝑝ℎ
𝑥
−𝛾𝑝ℎ
𝑝
+𝑐
𝑓(𝑥)
ℎ
𝑝
+𝛾𝑐ℎ
𝑝,𝑝
𝑓(𝑥)𝑔(𝑝)d𝑝d𝑥.
In the manipulations that follow, we will twice use the fact that 𝑔′(𝑝) =
−𝑝𝑔(𝑝)/𝑐.Firstly,integrationbypartsgives
∫ ∫
𝛾𝑐ℎ 𝑝,𝑝𝑔(𝑝) d𝑝 = 𝑝𝛾ℎ 𝑝𝑔(𝑝) d𝑝,
sothesecondandfourthtermscancel.Secondly,twointegrationsbyparts,
firstwithrespectto 𝑝 andthenwithrespectto𝑥,give
∬ ∬
𝑐𝑓′(𝑥)ℎ 𝑝𝑔(𝑝) d𝑝d𝑥 = 𝑓′(𝑥)ℎ𝑝𝑔(𝑝) d𝑝d𝑥
∬
=− 𝑓(𝑥)ℎ 𝑥𝑝𝑔(𝑝) d𝑝d𝑥,28 Background
sothefirstandthirdtermscancel.Theargumentiscompletedanalogously
tothatfortheoverdampedLangevindiffusion.
In Chapter 3, we explore further the overdamped and underdamped
Langevin diffusions as practical algorithms for scalable Monte Carlo in-
ference in the large-data setting and show that the discretisation of these
diffusion processes leads to important special cases of the general frame-
workforstochasticgradientMCMCalgorithms.
1.5 TheKernelTrick
Chapter 6 introduces the kernel Stein discrepancy and uses it to measure
the discrepancy between a sample of points and a distribution of interest.
Practical use of the methodology is made feasible by the ability to reduce
whatappearstobeaninfiniteamountofcomputation–maximisingaquan-
tityoveranuncountablyinfinitesetofpossiblefunctions–toonlyafinite
numberofarithmeticoperations.Thekeymechanismforthissimplification
is often called the kernel trick, and the setting for its use is a reproducing
kernelHilbertspace.
This section first explains the kernel trick in the more familiar setting
of a finite-dimensional inner-product space, before extending to the more
general setting required for Chapter 6. Whilst many of the concepts in-
troduced are much more general, our presentation focuses on the specific
setting of relevance: the vectors of our inner-product space are functions,
theassociatedfieldisRandtheinnerproductisanintegralwithrespectto
aprobabilitydistribution.
Throughout, 𝑓(·), 𝑔(·) etc. are functions from X → R, where X is R𝑑
or some closed or open subset of R𝑑; 𝑓(x), 𝑔(x) etc denote the function
evaluated at x ∈ X. The probability distribution 𝜈 is assumed to have a
density𝜈(x) onX.
1.5.1 Finite-DimensionalInnerProductSpaces
Let 0(·) be the function such that 0(x) = 0 for all x ∈ X. A set, V, of
functionsfromX →RisavectorspaceoverRifthefollowingaxiomsare
satisfied:
1. 0(·) ∈ V.
2. 𝑓(·) ∈ V =⇒ −𝑓(·) ∈ V.
3. 𝑓(·),𝑔(·) ∈ V =⇒ 𝑓(·)+𝑔(·) ∈ V.
4. 𝑓(·) ∈ Vand𝑎 ∈ R =⇒ 𝑎𝑓(·) ∈ V.1.5 TheKernelTrick 29
Aside: The associativity, commutativity and distributativity axioms of
a general vector space are satisfied automatically when the elements are
functionsandfromX toRandthefieldisR.
Everyfinite-dimensionalvectorspacehasadimension,𝑛,suchthatthere
isasetof𝑛vectors{𝑏 1(·),...,𝑏 𝑛(·)}whichsatisfytwoproperties:
1. Linearindependence:Ifthereare𝑎 1,...,𝑎 𝑛 ∈ Rsuchthat(cid:205) 𝑖𝑛 =1𝑎 𝑖𝑏 𝑖(·) =
0then𝑎
𝑖
=0forall𝑖 ∈ {1,...,𝑛}.
2. Spanning V: for each 𝑓(·) ∈ V there are 𝑎 1,...,𝑎 𝑛 ∈ R such that
𝑓(·) =(cid:205) 𝑖𝑛 =1𝑎 𝑖𝑏 𝑖(·).
Theset{𝑏 1(·),...,𝑏 𝑛(·)}iscalledabasis.
Example1.7 Itisstraightforwardtocheckthattheset
V= {𝑓(·) : 𝑓(𝑥) = 𝑐sin(𝑥+𝜃) : 𝑐 ∈ R,𝜃 ∈ [0,2𝜋)}
= {𝑓(·) : 𝑓(𝑥) = 𝑎sin𝑥+𝑏cos𝑥;𝑎,𝑏 ∈ R}
satisfies Axioms 1–4, whatever the domain, X ⊆ R. We may take 𝑏 (·) =
1
sin(·)and𝑏 (·) =cos(·).However,wemayalsotake𝑏 (·) =sin(·)+3cos(·)
2 1
and𝑏 (·) =cos(·),forexample.
2
For any vector space V of functions from X → R and any distribution
𝜈 with a probability density function on X of 𝜈(𝑥), we define the inner
product
∫
⟨𝑓(·),𝑔(·)⟩ = 𝑓(x)𝑔(x)𝜈(x) dx, (1.23)
𝜈
wherehereandthroughoutthissection,iftheintegralrangeisnotspecified
thenitisX.Werefertothisinnerproductas ⟨·,·⟩ .
𝜈
The inner product defined by (1.23) clearly satisfies two of the three
defining properties of an inner product: ⟨𝑓(·),𝑔(·)⟩ = ⟨𝑔(·), 𝑓(·)⟩ and
⟨𝑓(·)+𝑔(·),ℎ(·)⟩ = ⟨𝑓(·),ℎ(·)⟩ + ⟨𝑔(·),ℎ(·)⟩. However, we have only
that ⟨𝑓(·), 𝑓(·)⟩ = 0 ⇔ 𝑓(x) = 0(x) 𝜈-almost everywhere, rather than
⟨𝑓(·), 𝑓(·)⟩ = 0 ⇔ 𝑓(·) = 0(·). Each 𝑓 belongs to an equivalence class
of functions that are equal 𝜈-almost everywhere. This set of equivalence
classes forms a vector space and (1.23) defines an inner product on this
space,notonthespaceoffunctions,V.Tokeepthepresentationinthissec-
tionasstraightforwardaspossibleourwordingignoresthisdistinction,but
themorerigorousreadermaywishtoreplaceanyvectorspaceoffunctions
and inner product between these functions with the corresponding vector
spaceofequivalenceclassesoffunctionsandinnerproductsbetweenthese
equivalenceclasses.30 Background
Theinnerproductprovidesanorm,calledtheinducednorm,thesquare
ofwhichis
∫
∥𝑓(·)∥2 = ⟨𝑓(·), 𝑓(·)⟩ = 𝑓(x)2𝜈(x) dx.
𝜈 𝜈
Example 1.8 (Example 1.7 continued) Let X = [0,2𝜋] and let 𝜈 be the
uniformdistributionon [0,2𝜋].Forany 𝑓(·),𝑔(·) ∈ V,
1
∫ 2𝜋
1
∫ 2𝜋
⟨𝑓(·),𝑔(·)⟩ = 𝑓(𝑥)𝑔(𝑥) d𝑥 and ∥𝑓(·)∥2 = 𝑓(𝑥)2 d𝑥.
𝜈 2𝜋 𝜈 2𝜋
0 0
Example1.9 ForageneralvectorspaceVoffunctionsoftheformX →R,
letV
𝜈
betheelementsofVwhichhaveafinitenorminducedby𝜈:
(cid:26) ∫ (cid:27)
V 𝜈 = 𝑓(·) ∈ V: 𝑓(x)2𝜈(x) dx < ∞ .
ThenV 𝜈isalsoavectorspace,sinceAxioms1,2and4aresatisfiedtrivially,
andAxiom3issatisfiedsinceforany 𝑓(·),𝑔(·) ∈ V 𝜈,
∥𝑓(·)+𝑔(·)∥2 = ⟨𝑓(·)+𝑔(·), 𝑓(·)+𝑔(·)⟩
𝜈 𝜈
= ∥𝑓(·)∥2 +2⟨𝑓(·),𝑔(·)⟩ +∥𝑔(·)∥2
𝜈 𝜈 𝜈
≤ ∥𝑓(·)∥2
𝜈
+2∥𝑓(·)∥ 𝜈∥𝑔(·)∥
𝜈
+∥𝑔(·)∥2
𝜈
< ∞,
wherethethirdlineusestheCauchy–Schwarzinequality,which,inthiscase,
isthefamiliarinequalityE[𝑓(X)𝑔(X)]2 ≤ E[𝑓(X)2]E(cid:2)𝑔(X)2(cid:3) ,whereX
hasadensity𝜈onX.
Henceforth, for narrative simplicity, we will assume that V is a finite-
dimensionalvectorspacewithdimension𝑛.Section1.5.4extendsthenar-
rativetopotentiallyinfinite-dimensionalspaces.
When considering the inner product ⟨·,·⟩ , two vectors 𝑓(·),𝑔(·) ∈
𝜈
V are said to be orthogonal if ⟨𝑓(·),𝑔(·)⟩ = 0 and the basis vectors,
𝜈
𝑒 1(·),...,𝑒 𝑛(·) aresaidtobeorthonormaliftheyareorthogonalandeach
hasanormof1:foreach 𝑗,𝑘 ∈ {1,...,𝑛},
∥𝑒 𝑗(·)∥ 𝜈 =1 and ⟨𝑒 𝑗(·),𝑒 𝑘(·)⟩ 𝜈 =0
whenever 𝑗 ≠ 𝑘. We will reserve the symbols {𝑒 𝑘(·)}𝑛
𝑘=1
for any set of 𝑛
orthonormalbasisfunctions.
Therepresentationof 𝑓(·) intermsofanorthonormalbasis
𝑛
∑︁
𝑓(·) = 𝑓 𝑒 (·)
𝑗 𝑗
𝑗=11.5 TheKernelTrick 31
is termed an orthonormal decomposition of 𝑓(·). Since the 𝑒 𝑖(·) are or-
thonormal,theprojectionof 𝑓(·) onto𝑒 𝑘(·) is 𝑓 𝑘:
(cid:42) 𝑛 (cid:43) 𝑛
∑︁ ∑︁
⟨𝑓(·),𝑒 (·)⟩ = 𝑓 𝑒 (·),𝑒 (·) = 𝑓 ⟨𝑒 (·),𝑒 (·)⟩
𝑘 𝜈 𝑗 𝑗 𝑘 𝑗 𝑗 𝑘 𝜈
𝑗=1 𝜈 𝑗=1
= 𝑓 𝑘. (1.24)
Furthermore, the squared norm of 𝑓(·) is the sum of the squares of the
orthonormalprojections:
(cid:42) 𝑛 𝑛 (cid:43) 𝑛 𝑛
∑︁ ∑︁ ∑︁∑︁
∥𝑓(·)∥2 = 𝑓 𝑒 (·), 𝑓 𝑒 (·) = 𝑓 𝑓 ⟨𝑒 (·),𝑒 (·)⟩
𝑗 𝑗 𝑘 𝑘 𝑗 𝑘 𝑗 𝑘
𝑗=1 𝑘=1 𝑗=1 𝑘=1
𝑛
∑︁
= 𝑓2. (1.25)
𝑗
𝑗=1
Example1.10 InExample1.7,since∫ 2𝜋 sin2𝑥 d𝑥 = ∫ 2𝜋 cos2𝑥 d𝑥 = 𝜋
0 0
and∫ 2𝜋 sin𝑥cos𝑥 d𝑥 =0,
0
√ √
𝑒 (·) = 2sin(·) and 𝑒 (·) = 2cos(·)
1 2
form an orthonormal basis for V when 𝜈 is the uniform distribution on
[0,2𝜋].Anyfunction 𝑓(·) ∈ Vcanbewrittenas 𝑓(·) = 𝑓 𝑒 (·)+ 𝑓 𝑒 (·).
1 1 2 2
Forexampleset
√
3 1
𝑓(𝑥) =sin(𝑥+𝜋/6) = sin𝑥+ cos𝑥. (1.26)
2 2
√ √ √
So 𝑓 = 3/(2 2) and 𝑓 =1/(2 2).Also
1 2
3 1 1 1
∫ 2𝜋
∥𝑓(·)∥2 = 𝑓2+ 𝑓2 = + = = sin2(𝑥+𝜋/6) d𝑥.
𝜈 1 2 8 8 2 2𝜋
0
1.5.2 KernelsinaFinite-DimensionalInnerProductSpace
As in the previous subsection, let V be an 𝑛-dimensional vector space of
functions from X to R and let 𝜈 be a probability distribution on X with a
probabilitydensityof 𝜈(x),x ∈ X.Finally,let {𝑒 𝑘(·)}𝑛
𝑘=1
beasetofbasis
functionswhichisorthonormalwithrespecttotheinnerproduct(1.23).
Let𝜆 1,...,𝜆 𝑛beasetofnon-negativescalarsandconsiderthefollowing
real-valuedfunctiononX×X:
𝑛
∑︁
k(x,y) = 𝜆 𝑗𝑒 𝑗(x)𝑒 𝑗(y). (1.27)
𝑗=132 Background
Clearly,k(·,·)issymmetric:k(y,x) =k(x,y).Moreover,k(·,·)ispositive
semidefinite:foranyfinite𝐽 < ∞,𝑐 1,...,𝑐
𝐽
∈ Randx 1,...x𝐽 ∈ R𝑑,
𝐽 𝐽 𝐽 𝐽 𝑛
∑︁∑︁ ∑︁∑︁ ∑︁
𝑐 𝑗𝑐 𝑘k(x𝑗,x𝑘) = 𝑐 𝑗𝑐
𝑘
𝜆 𝑙𝑒 𝑙(x𝑗)𝑒 𝑙(x𝑘)
𝑗=1 𝑘=1 𝑗=1 𝑘=1 𝑙=1
𝑛 𝐽 𝐽
∑︁ ∑︁∑︁
= 𝜆
𝑙
𝑐 𝑗𝑐 𝑘𝑒 𝑙(x𝑗)𝑒 𝑙(x𝑘)
𝑙=1 𝑗=1 𝑘=1
𝑛 (cid:40) 𝐽 (cid:41)2
∑︁ ∑︁
= 𝜆
𝑙
𝑐 𝑗𝑒 𝑙(x𝑗) ≥ 0.
𝑙=1 𝑗=1
Anyfunctionk(·,·) :X×X →Rwhichisbothsymmetricandpositive
semidefiniteiscalledakernel.
Example1.11 ContinuingExample1.7,letk : [0,2𝜋]×[0,2𝜋] →Rbe
1 3
k(𝑥,𝑦) = 𝑒 (𝑥)𝑒 (𝑦)+ 𝑒 (𝑥)𝑒 (𝑦) =sin𝑥sin𝑦+3cos𝑥cos𝑦
2 1 1 2 2 2
=2cos(𝑦−𝑥)+cos(𝑦+𝑥).
Thisissymmetricandpositivedefinitebyconstruction.
Giventhedefinitionofk(·,·) in(1.27),define
𝑛
∑︁
k(x,·) = 𝜆 𝑗𝑒 𝑗(x)𝑒 𝑗(·), (1.28)
𝑗=1
andk(·,x) =k(x,·).Since𝑒 𝑗(x) ∈ R,k(x,·) ∈ V.Furthermore,for 𝑓(·) ∈
Vdefinetheoperator𝑇 via
k
∫
𝑇 𝑓(·) = k(·,y)𝑓(y)𝜈(y) dy. (1.29)
k
Then𝑇 isalinearoperator,sinceforany𝑎,𝑏 ∈ Rand 𝑓(·),𝑔(·) ∈ V,
k
𝑇 {𝑎𝑓(·)+𝑏𝑔(·)} = 𝑎𝑇 𝑓(·)+𝑏𝑇 𝑔(·).
k k k1.5 TheKernelTrick 33
Now,writing 𝑓(·) =(cid:205)𝑛
𝑘=1
𝑓 𝑘𝑒 𝑘(·),
∫
(𝑇 𝑓(·))(x) = k(x,y)𝑓(y)𝜈(y)dy
k
(cid:42) 𝑛 𝑛 (cid:43)
∑︁ ∑︁
= ⟨k(x,·), 𝑓(·)⟩
𝜈
= 𝜆 𝑗𝑒 𝑗(x)𝑒 𝑗(·), 𝑓 𝑘𝑒 𝑘(·)
𝑗=1 𝑘=1 𝜈
𝑛 𝑛 𝑛
∑︁∑︁ ∑︁
= 𝜆 𝑗𝑒 𝑗(x)𝑓
𝑘
⟨𝑒 𝑗(·),𝑒 𝑘(·)⟩
𝜈
= 𝜆 𝑘𝑓 𝑘𝑒 𝑘(x).
𝑗=1 𝑘=1 𝑘=1
So𝑇 k𝑓(·) = (cid:205)𝑛 𝑘=1𝜆 𝑘𝑓 𝑘𝑒 𝑘(·) and, hence,𝑇 k𝑓(·) ∈ V, too. Moreover, con-
sidering 𝑓(·) = 𝑒 𝑗(·), we see that 𝑇 k𝑒 𝑗(·) = 𝜆 𝑗𝑒 𝑗(·); each 𝑒 𝑗(·) is an
eigenfunctionof𝑇
k
withacorrespondingeigenvalueof𝜆 𝑗.
Example 1.12 Continuing Example 1.7, with the kernel from Example
1.11,
k(𝑥,·) =sin𝑥sin(·)+3cos𝑥cos(·) =2cos(·−𝑥)+cos(·+𝑥).
Let 𝑓(·) beasdefinedin(1.26).Then,usingthedefiniteintegralsatthe
startofExample1.10,
(cid:40)√ (cid:41)
1
∫ 2𝜋
3 1
𝑇 𝑓(·) = {sin(·)sin𝑦+3cos(·)cos𝑦} sin𝑦+ cos𝑦 d𝑦
k 2𝜋 2 2
0
1
∫ 2𝜋√
= 3sin(·)sin2𝑦+3cos(·)cos2𝑦 d𝑦
4𝜋
0
1 (cid:110)√ (cid:111)
= 3sin(·)+3cos(·) .
4
Since
√
√ √ 3 1
𝑒 (·) = 2sin(·), 𝑒 (·) = 2cos(·), 𝑓 = √ , 𝑓 = √ ,
1 2 1 2
2 2 2 2
𝜆 =1/2and𝜆 =3/2,𝑇 𝑓(·) is,therefore,
1 2 k
𝜆 𝑓 𝑒 (·)+𝜆 𝑓 𝑒 (·),
1 1 1 2 2 2
aswewouldhope.34 Background
1.5.3 ANewInnerProductandtheKernelTrickinFinite
Dimensions
Let {𝑒 𝑗(·)}𝑛
𝑗=1
beanorthonormalbasisforVandletk bedefinedthrough
(1.27)withrespecttothisbasis.For 𝑓(·),𝑔(·) ∈ Vwith
𝑛 𝑛
∑︁ ∑︁
𝑓(·) = 𝑓 𝑗𝑒 𝑗(·) and 𝑔(·) = 𝑔 𝑗𝑒 𝑗(·), (1.30)
𝑗=1 𝑗=1
theinnerproductwithrespectto𝜈isthesumoftheproductsoftheorthog-
onalprojections:
(cid:42) 𝑛 𝑛 (cid:43) 𝑛 𝑛
∑︁ ∑︁ ∑︁∑︁
⟨𝑓(·),𝑔(·)⟩ = 𝑓 𝑒 (·), 𝑔 𝑒 (·) = 𝑓 𝑔 ⟨𝑒 (·),𝑒 (·)⟩
𝜈 𝑗 𝑗 𝑘 𝑘 𝑗 𝑘 𝑗 𝑘 𝜈
𝑗=1 𝑘=1 𝜈 𝑗=1 𝑘=1
𝑛
∑︁
= 𝑓 𝑔 .
𝑗 𝑗
𝑗=1
Wenowdefineanewinnerproduct
𝑛 𝑓 𝑔
⟨𝑓(·),𝑔(·)⟩ =∑︁ 𝑗 𝑗, (1.31)
k 𝜆
𝑗
𝑗=1
where the {𝜆 𝑗}𝑛 𝑗=1, are exactly those from the definition of k and are the
eigenvaluesoftheoperator𝑇 .
k
This inner product may be rephrased in terms of a set of eigenfunc-
tionswhichareorthonormalwithrespectto ⟨·,·⟩ : {𝑒′(·)}𝑛 with𝑒′(·) =
k 𝑗 𝑗=1 𝑗
√︁𝜆 𝑗𝑒 𝑗(·).Withrespecttothisbasis,thevector
𝑛
∑︁
𝑓(·) = 𝑓′𝑒′(·)
𝑗 𝑗
𝑗=1
with 𝑓 𝑗′ = 𝑓 𝑗/√︁𝜆 𝑗.Usingananalogousdecompositionfor𝑔(·),
𝑛
∑︁
⟨𝑓(·),𝑔(·)⟩ = 𝑓′𝑔′,
k 𝑗 𝑗
𝑗=1
asexpected.Finally,
𝑛
∑︁
k(x,y) = 𝑒′(x)𝑒′(y).
𝑗 𝑗
𝑗=11.5 TheKernelTrick 35
√
Example 1.13 In Example 1.11, 𝑒′(·) = sin(·) and 𝑒′(·) = 3cos(·).
1 2
Clearly,
k(𝑥,𝑦) =sin𝑥sin𝑦+3cos𝑥cos𝑦 = 𝑒′(𝑥)𝑒′(𝑦)+𝑒′(𝑥)𝑒′(𝑦).
1 1 2 2
For 𝑓(·) asinExample1.26,
√ √ √
3 1 3 3√
𝑓(·) = sin(·)+ cos(·) = sin(·)+ 3cos(·),
2 2 2 6
√ √
so 𝑓′ = 3/2and 𝑓′ = 3/6.Thus
1 2
3 3 5
∥𝑓(·)∥2 = + = .
k 4 36 6
TheKernelTrick
Fromthedefinition(1.28)andwith 𝑓(·) decomposedasin(1.30),
⟨k(x,·), 𝑓(·)⟩
k
=
(cid:42) ∑︁𝑛
𝜆 𝑗𝑒 𝑗(x)𝑒
𝑗(·),∑︁𝑛
𝑓 𝑘𝑒
𝑘(·)(cid:43) =∑︁𝑛 𝜆 𝑗𝑒 𝜆𝑗(x)𝑓
𝑗
𝑗
𝑗=1 𝑘=1 𝑗=1
k
= 𝑓(x). (1.32)
Moreover,choosing 𝑓(·)tobek(y,·), 𝑓 𝑗 =𝜆 𝑗𝑒 𝑗(y)from(1.28),and,hence,
𝑛
∑︁
⟨k(x,·),k(y,·)⟩
k
= 𝜆 𝑗𝑒 𝑗(x)𝑒 𝑗(y) =k(x,y). (1.33)
𝑗=1
Together,(1.32)and(1.33)enabletheevaluationofinnerproductsin⟨·,·⟩
k
without needing to know the original basis functions 𝑒 1(·),...,𝑒 𝑛(·) nor
the associated values𝜆 1,...,𝜆 𝑛. Indeed, we do not even need to know 𝜈.
Thisisknownasthekerneltrick,andwewillexemplifyitsuseinSection
1.5.5.First,wegeneralisetoamuchbroaderclassofkernels.
1.5.4 GeneralKernels
InSection1.5.2wecreatedakernelvia(1.27)usingaknownorthonormal
basis for the inner-product space V, with the inner product specifed by
(1.23)accordingtothedensity𝜈.However,akernelisanypositive-definite
symmetricfunctionandweareinterestedinkernelsk :X×X →R.
Example1.14 TheGaussiankernelis
k(x,y) =exp(cid:0) −∥y−x∥2(cid:1),36 Background
where∥·∥representsthestandardEuclideannorm.Thisisclearlysymmet-
ric.ToseethatkisalsopositivesemidefiniteonX =R𝑑,notethat
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1 1 1
Z∼N𝑑 x, I𝑑 and Y|Z∼N𝑑 Z, I𝑑 =⇒ Y∼N𝑑 x, I𝑑 ,
4 4 2
fromwhich
∫
exp(cid:0) −∥y−x∥2(cid:1) = 𝛾 exp(cid:0) −2∥y−z∥2(cid:1) exp(cid:0) −2∥x−z∥2(cid:1) dz,
where𝛾 =2𝑑/𝜋𝑑/2.Hence(cid:205)𝐽 𝑗=1(cid:205) 𝑘𝐽 =1𝑐 𝑗𝑐 𝑘k(x𝑗,x𝑘) is
𝐽 𝐽 ∫
𝛾∑︁∑︁ 𝑐 𝑗𝑐 𝑘 exp(cid:0) −2∥x𝑗 −z∥2(cid:1) exp(cid:0) −2∥x𝑘 −z∥2(cid:1) dz
𝑗=1 𝑘=1
∫ 𝐽 𝐽
= 𝛾 ∑︁∑︁ 𝑐 𝑗𝑐 𝑘exp(cid:0) −2∥x𝑗 −z∥2(cid:1) exp(cid:0) −2∥x𝑘 −z∥2(cid:1) dz
𝑗=1 𝑘=1
∫ (cid:40) 𝐽 (cid:41)2
= 𝛾 ∑︁ 𝑐 𝑗exp(cid:0) −2∥x𝑗 −z∥2(cid:1) dz
𝑗=1
≥ 0.
When specifying k in Example 1.14, we have not specified a vector
space,noradensity𝜈,noranassociatedinnerproduct.However,sincekis
akernel,wemighthopethatifwedospecify𝜈andtheinnerproduct⟨·,·⟩
𝜈
in(1.23),thentheremightbeavectorspacewithabasisthatisorthonormal
withrespectto⟨·,·⟩ suchthatkhasthedecomposition(1.27).Ifthiswere
𝜈
thecasethenwewouldknowthattherewasanewinnerproduct⟨·,·⟩ such
k
that (1.32) and (1.33) held. Hence we could evaluate inner products with
respecttokwithoutknowingthebasisitselfnortheeigenvaluesof𝑇 ,nor,
k
even,thedetailsabout𝜈.
The decomposition in (1.23) does not hold in general, but Mercer’s
Theoremandgeneralisationsofittellusthatananalogousdecomposition
butwith𝑛potentiallyinfiniteholdswidely.
Specifically, let X be R𝑑 or a closed or open subset of R𝑑, k(·,·) :
X ×X → R be a kernel and 𝜈(x), x ∈ X, be a probability density on X.
Then,providedk(x,y) isacontinuousfunctionofxandy,and
∫
k(x,y)2𝜈(y) dy < ∞ forallx ∈ X, (1.34)
thelinearoperator𝑇 definedin(1.29)hasatmostcountablymanypositive
k
(andnonegative)eigenvalues𝜆 ,𝜆 ,... withcorrespondingeigenfunctions
1 21.5 TheKernelTrick 37
𝑒 (·),𝑒 (·),... which are orthonormal with respect to the inner product
1 2
⟨·,·⟩ definedin(1.23).Furthermore,kcanbedecomposedas
𝜈
∞
∑︁
k(x,y) = 𝜆 𝑘𝑒 𝑗(x)𝑒 𝑗(y)
𝑗=1
and the set {√︁𝜆 𝑗𝑒 𝑗(·)}∞
𝑗=1
forms an orthonormal basis with respect to the
innerproduct
∞ 𝑓 𝑔
⟨𝑓(·),𝑔(·)⟩ =∑︁ 𝑗 𝑗,
k 𝜆
𝑗
𝑗=1
where
∞ ∞
∑︁ ∑︁
𝑓(·) = 𝑓 𝑗𝑒 𝑗(·) and 𝑔(·) = 𝑔 𝑗𝑒 𝑗(·). (1.35)
𝑗=1 𝑗=1
The space in which 𝑒 (·),𝑒 (·),... lie is a generalisation of the vector
1 2
spaceofExample1.9totheHilbertspace,H 𝜈,offunctions 𝑓(·) :X →R
withtheinnerproduct⟨·,·⟩ andsuchthat∥𝑓(·)∥2 =∫ 𝑓(x)2𝜈(x) dx < ∞.
𝜈 𝜈
Likewisetheorthonormalbasis{√︁𝜆 𝑗𝑒 𝑗(·)}∞ 𝑗=1liesinthereproducingkernel
Hilbert space, H , of functions with the inner product ⟨·,·⟩ and such
k k
that ∥𝑓(·)∥ < ∞. A Hilbert space H is an inner product space with
k
a potentially infinite set of basis vectors that is complete; informally, it
containsno”holes”,sothatforanysequence 𝑓 , 𝑓 ,... with(cid:205)∞ 𝑓2 < ∞
1 2 𝑗=1 𝑗
then(consideringH k,forexample) 𝑓(·) =lim𝑛→∞(cid:205)𝑛
𝑗=1
𝑓 𝑗𝑒′ 𝑗(·)exists,with
distance measured through the norm induced by the inner product, and is
inH .
k
Thus,thesimplificationsoftheinnerproductsin(1.32)and(1.33)con-
tinuetohold;ingeneral,theintermediatestepsmustreplace𝑛with∞.
Example1.15 FortheGaussiankernelofExample1.14,k(x,·) =exp(−∥x−
·∥2) and
(cid:10) exp(−∥x−·∥2),exp(−∥y−·∥2)(cid:11) =exp(−∥y−x∥2).
k
Also,forany 𝑓(·) ∈ H ,
k
(cid:10) exp(−∥x−·∥2), 𝑓(·)(cid:11) = 𝑓(x).
k
Trace-ClassKernels
Akernelwhere∫ k(x,x)𝜈(x) dx= 𝑐 < ∞isreferredtoastraceclass.This
propertyhasimportantconsequencesforthesetofeigenvalues,𝜆 ,𝜆 ,...,
1 238 Background
of𝑇 ,since
k
∫ ∫ ∞
∑︁
k(x,x)𝜈(x) dx= 𝜆 𝑘𝑒 𝑘(x)𝑒 𝑘(x)𝜈(x) dx
𝑘=1
∞ ∫ ∞
∑︁ ∑︁
= 𝜆
𝑘
𝑒 𝑘(x)2𝜈(x) dx= 𝜆 𝑘.
𝑘=1 𝑘=1
Thus(cid:205)∞ 𝑘=1𝜆
𝑘
= 𝑐.Sinceeach𝜆
𝑘
≥ 0,wehavelim𝑘→∞𝜆
𝑘
=0.
The Gaussian kernel of Example 1.14 is trace class with 𝑐 = 1 since
k(x,x) = 1forallx ∈ R𝑑.ThekernelwewillmeetininChapter6isalso
oftraceclass,followingasimilarreasoning.
Withoutlossofgenerality,welabeltheeigenvalues𝜆 ,𝜆 ... inorderof
1 2
decreasingsize(choosinganyoneofthepossibilitiesifsomeofthe𝜆 𝑗 are
notunique).Withthedecompositionof 𝑓(·) in(1.35),
∞ 𝑓2 ∞
∥𝑓(·)∥2 =∑︁ 𝑗 ≥ 1 ∑︁ 𝑓2 = 1 ∥𝑓(·)∥2.
k 𝜆 𝜆 𝑗 𝜆 𝜈
𝑗=1 𝑗 1 𝑗=1 1
Thus∥𝑓(·)∥
k
< ∞ =⇒ ∥𝑓(·)∥
𝜈
< ∞andhenceH
k
⊆ H 𝜈.Ingeneral,H
k
isstrictlysmallerthanH 𝜈 andthemorequicklytheeigenvaluesof𝑇 kdecay
thesmallerthespaceH .
k
1.5.5 ThePoweroftheKernelTrick
Supposewehavevaluesx 1,...,x𝑚 ∈ X andweareinterestedin
(cid:40) 𝑚 (cid:41)
∑︁
V∗ = 𝑔(·) :𝑔(·) = 𝑔 𝑗k(x𝑗,·), 𝑔 1,...,𝑔
𝑚
∈ R .
𝑗=1
Firstly,forany𝑔(·) =(cid:205)𝑚 𝑗=1𝑔 𝑗k(x𝑗,·),
(cid:42) 𝑚 𝑚 (cid:43) 𝑚 𝑚
∑︁ ∑︁ ∑︁∑︁
∥𝑔(·)∥2 = 𝑔 𝑗k(x𝑗,·), 𝑔 𝑘k(x𝑘,·) = 𝑔
𝑗
⟨k(x𝑗,·),k(x𝑘,·⟩𝑔
𝑘
k
𝑗=1 𝑘=1 𝑗=1 𝑘=1
k
𝑚 𝑚
∑︁∑︁
= 𝑔 𝑗k(x𝑗,x𝑘)𝑔 𝑘 < ∞. (1.36)
𝑗=1 𝑘=1
So,V∗ ⊆ H .Secondly,forany 𝑓(·) ∈ H ,
k k
𝑚 𝑚
∑︁ ∑︁
⟨𝑓(·),𝑔(·)⟩ k = 𝑔 𝑗 ⟨𝑓(·),k(x𝑗,·)⟩ k = 𝑔 𝑗𝑓(x𝑗). (1.37)
𝑗=1 𝑗=1
Suppose there is a particular function of interest, 𝑓(·) ∈ H , and we
k1.5 TheKernelTrick 39
wouldliketoconstructthefunction𝑔(·) ∈ V∗ thatmostcloselyresembles
𝑓(·) in shape. We could find the unit vector in V∗ which has the largest
componentinthe 𝑓(·) direction:
𝑔(·) = argmax ⟨𝑓(·),𝑔(·)⟩ .
(cid:98) k
𝑔(·)∈V∗:∥𝑔(·)∥=1
Thesizeoftheinnerproduct,⟨𝑓(·),𝑔(·)⟩ ,isameasureoftheabilityofV∗
(cid:98) k
torepresent 𝑓(·).
Definef = [𝑓(x 1),..., 𝑓(x𝑚)]⊤ andg = [𝑔 1,...𝑔 𝑚]⊤ andletKbethe
matrixwithelements𝐾
𝑖,𝑗
=k(x𝑖,x𝑗).Then(1.36)and(1.37)become
⟨𝑓(·),𝑔(·)⟩ =g⊤f and ∥𝑔(·)∥2 =g⊤Kg.
k k
Tofind𝑔(·)wemustfindthevectorgthatmaximisesg⊤fsubjecttog⊤Kg=
(cid:98) (cid:98)
1.
Let A be a square matrix such that AA⊤ = K and set h = A⊤g. Then,
equivalently,wewishtomaximiseh⊤A−1fsuchthat∥h∥ =1.Wemustfind
theunit 𝑚-vectorwiththelargestcomponentintheA−1f direction,which
is
A−1f A−⊤A−1f K−1f
(cid:98)h=
√︁ (A−1f)⊤A−1f
=⇒ (cid:98)g= √
f⊤A−⊤A−1f
= √ f⊤K−1f,
since (cid:98)g=A−⊤ (cid:98)h.Theinnerproduct (cid:98)g⊤f is
f⊤K−1f √
√ = f⊤K−1f.
f⊤K−1f
Thiscalculationonlyrequiresustobeabletoevaluate 𝑓(x𝑗) andk(x𝑗,x𝑘)
for 𝑗,𝑘 = 1,...,𝑚.Wedonotneedtoknowtheeigenfunctions 𝑒 (·),...
1
noreigenvalues𝜆 ,... of𝑇 .Indeed,wedonotevenneedtoknow𝜈;only
1 k
that(1.34)issatisfied.
Example 1.16 Let X = R and let k be the one-dimensional case of
the Gaussian kernel in Example 1.14. We find the approximations to the
function
1
𝑓(𝑥) = ,
1+𝑥2
usinggraduallymoreandmorekernelfunctionsk(𝑥 𝑗,𝑥).Forpoints,𝑥 1,...,𝑥 𝐽,
K is the matrix with elements 𝐾 𝑖,𝑗 = exp[−(𝑥 𝑖 −𝑥 𝑗)2], and f is the vec-
tor with 𝑓 𝑗 = 𝑓(𝑥 𝑗). We set (𝑥 1,𝑥 2,𝑥 3,𝑥 4,𝑥 5,𝑥 6,𝑥 7) = (−3,...,3) and
approximate 𝑓(𝑥) using just 𝑥 then 𝑥 ,...,𝑥 , then 𝑥 ,...,𝑥 and finally
1 1 3 1 5
𝑥 ,...,𝑥 . Figure 1.6 compares the four approximations with the truth.
1 7
Eachtimenewpointsareaddedtotheset,theapproximationimproves,but40 Background
   
 7
 $
     %
 &
 '
   
   
   
   
         
x
Figure1.6 Thefunction 𝑓(𝑥) =1/(1+𝑥2)(T)andkernel-based
approximationsto 𝑓(𝑥)fromExample1.16.CurvesuseA:
𝑥 =−3,B:𝑥 =−3,−2,−1,C:𝑥 =−3,...,1andD:𝑥 =−3,...,3.
itmatterswherethepointsareadded;somebasisvectorsaremorehelpful
thanothers.
1.6 ChapterNotes
Therearemanytextswhichcovertheintroductorymaterialfromthischapter
inmoredepthandrigourthanwehaveallowed;wesuggestafewoneach
topic.
BasicMonteCarloandimportancesamplingiscoveredinRipley(2009)
andRubinsteinandKroese(2008).ForanintroductiontoBayesianstatistics
and the use of Monte Carlo methods for Bayesian analysis, see Bernardo
andSmith(2009),Robert(2007)andRobertandCasella(1999).
Norris(1998)providesagentleintroductiontoMarkovchainsondiscrete
statespaces,whileMeynandTweedie(2012)givesathoroughtreatmenton
generalstatespaces;alessthoroughbutmorereadilyaccessibletreatment
for general state spaces is given in Roberts and Rosenthal (2004). Geyer
(1992) describes methods for estimating the integrated auto-correlation
 Q R L W F Q X I1.6 ChapterNotes 41
time from a sample of the chain when the Markov chain is reversible; for
thenon-reversiblechainsofChapter4theintegratedauto-correlationcanbe
estimatedbyfittinganauto-regressiveprocesstothetimeseries{ℎ(𝑋 𝑘)}𝑛
𝑘=1
orbyestimatingthespectraldensityoftheseriesatafrequencyof0(e.g.
HeidelbergerandWelch,1981).
Stochastic differential equations and diffusions are the subject of Ok-
sendal (2013), Rogers and Williams (2000a) and Rogers and Williams
(2000b). An alternative to simple Monte Carlo, which attempts to obtain
betterconvergencerateswiththeMonteCarlosamplesize𝑛,isquasi-Monte
Carlo. See, for example, Caflisch (1998) for an introduction and L’Ecuyer
andLemieux(2002)forworkonrandomisedquasi-MonteCarlo.
Chapter 1 of Conway (2010) introduces Hilbert spaces in general, and
kernelsandreproducingkernelHilbertspacesarecoveredinChapter6of
RasmussenandWilliams(2005).Mercer’sTheoremisusuallystatedfora
compactX;wehaveusedthegeneralisationtonon-compactspacesinSun
(2005).2
Reversible MCMC and its Scaling
BuildingontheintroductionstoBayesianstatistics,MonteCarlomethods
and Markov chains in Chapter 1, this section introduces Markov chain
MonteCarloalgorithmsasagenericcomputationalsolutiontothechallenge
ofusingMonteCarlomethodstosamplefromtheposteriordistributionand,
hence,estimateposteriorexpectationsofquantitiesofinterest.
As described in Chapter 1, if it is possible to sample directly from the
posterior, 𝜋(𝜽) := 𝜋(𝜽|D) (see equation (1.3)) then for any function ℎ
with E 𝜋 (cid:2)ℎ2(𝜽)(cid:3) < ∞, it is possible to estimate E 𝜋 [ℎ(𝜽)] via the Monte
Carlo average (1.4), the typical error of which is of size 𝑛−1/2, where 𝑛 is
thenumberofsamples.Unfortunately,itisusuallynotpossible,oriscom-
putationallyinfeasible,togenerateindependentandidenticallydistributed
samplesfrom 𝜋.Importancesamplingprovides,perhaps,themostnatural
alternativetodirectsampling;however,asexemplifiedinSection1.1.5the
variance of importance sampling estimators typically degrades exponen-
tiallyquicklywithdimension.
Markov chain Monte Carlo (MCMC) is a generalisation of the Monte
Carlo method that, as we will see, has several favourable properties when
itcomestofacilitatingcomputationinBayesianstatisticsproblems.Inthis
context,theaimofMCMCistoconstructaMarkovchain, {𝜽𝑘}∞
𝑘=1
whose
limitingdistributionistheposteriordistributionofinterest,sothatsamples
from a sufficiently long chain, except, perhaps, those near the beginning,
arise approximately from the posterior and can be used to create Monte
Carlo approximations to expectations as in (1.4), via the ergodic average
definedin(1.10).
The workhorse of MCMC is the Metropolis–Hastings algorithm. We
describe the general Metropolis–Hastings algorithm and show that the
resulting chain satisfies detailed balance with respect to 𝜋. We then in-
vestigate particular special cases: the independence sampler, the random
walk Metropolis algorithm, the Metropolis-adjusted Langevin algorithm,
andHamiltonianMonteCarlo.Foreachofthesecases,weoverviewthebe-
422.1 TheMetropolis–HastingsAlgorithm 43
haviourasthedimension 𝑑 → ∞,motivatingtheneedforfurtherscalable
methods.
Throughout this chapter, we denote the support of 𝜋 by Θ; for example
ΘmightbeR𝑑 forsome𝑑 ∈ N.
2.1 TheMetropolis–HastingsAlgorithm
TheideaoftheMetropolis–Hastingsalgorithmistodefinethedynamicsof
aMarkovchainbyspecifyinganarbitraryproposaldistributionforthenext
stateoftheMarkovchain,andthenhavingamechanismwherethisproposal
iseitheracceptedorrejected.Ifitisrejected,thestateoftheMarkovchainis
unchanged.Aswewillsee,itisgenerallypossibletochoosetheacceptance
probabilitytodependonthetargetdistribution,sothattheresultingMarkov
chainwillhavethetargetdistributionasitsstationarydistribution.
TheMetropolis–HastingsalgorithmisgiveninAlgorithm1.Theposte-
rior density, 𝜋, appears in both the numerator and denominator of the ac-
ceptanceprobability,𝛼(𝜽𝑘,𝜽′),sotermsinvolvingthetypicallyintractable
density 𝜋(𝜽) can be replaced with the tractable product of the prior and
the likelihood, 𝜋 (𝜽)𝐿(𝜽;D); we do not need to know the normalising
0
constant, 𝑝(D) =∫ 𝜋 (𝜽)𝐿(𝜽;D)d𝜽,asitwillcancelintheratio.
Θ 0
As well as 𝜋(𝜽), and an initial value for the parameter vector, the
Metropolis–Hastingsalgorithmrequiresaproposaldensity,𝑞(𝜽′|𝜽).Com-
monchoicesofthedensity𝑞 includethefollowing:
Metropolis–Hastingsindependencesampler(MHIS) 𝑞(𝜽′ | 𝜽) := 𝑞′(𝜽′)
forsomedensity 𝑞′.Theproposaldoesnotdependonthecurrent
state; for example, 𝑞′ could be the same as a sensible importance
samplingproposaldistribution(seeSection1.1.5).
RandomwalkMetropolis(RWM) 𝑞(𝜽′ | 𝜽) = 𝑞′(𝜽′ − 𝜽), where 𝑞′ is
a density such that for any vector v ∈ Θ, 𝑞′(v) = 𝑞′(−v). For
example 𝜽′|𝜽 ∼ N(𝜽,𝜆2I𝑑),whereI𝑑 isthe 𝑑 ×𝑑 identitymatrix
and𝜆 > 0.
Metropolis-adjustedLangevinalgorithm(MALA) addsaspecificform
ofoffsettoaGaussianRWMproposal.Forexample,𝜽′|𝜽 ∼N(𝜽+
1𝜆2∇log𝜋(𝜽),𝜆2I𝑑).
2
HamiltonianMonteCarlo(HMC) starting from 𝜽 and with a random
momentum,suchasp∼N(0,𝑀I𝑑),Hamiltoniandynamicsareap-
proximatelyintegratedforwardsonapotentialsurfaceof−log𝜋(𝜽).
Theproposal,𝜽′,isthepositionaftersomefixedtime𝑇.44 ReversibleMCMCanditsScaling
Laterinthischapterwedescribeandinvestigatetheseclassesofproposals
inmoredetailandexaminetheirrelativeefficiencies.
Algorithm1:Metropolis–Hastingsalgorithm
Input:Density𝜋(𝜽),initialvalue𝜽 andproposaldensity𝑞(𝜽′|𝜽).
0
for 𝑘 ∈ 0,...,𝑛−1do
Propose𝜽′ from𝑞(𝜽′|𝜽𝑘)
Calculatetheacceptanceprobability:
𝛼(𝜽𝑘,𝜽′) :=min(cid:18) 1, 𝜋𝜋 (( 𝜽𝜽 𝑘′) )𝑞 𝑞( (𝜽 𝜽𝑘 ′|| 𝜽𝜽 𝑘′) )(cid:19) . (2.1)
Withaprobabilityof𝛼(𝜽𝑘,𝜽′) accepttheproposal,𝜽𝑘+1 ← 𝜽′;
otherwiserejectit,𝜽𝑘+1 ← 𝜽𝑘.
end
That the Metropolis–Hastings algorithm has a stationary density of 𝜋,
follows directly from the fact that it is reversible with respect to 𝜋 (see
Section1.3).Wenowshowthatthisisthecase.First,noticethat
𝜋(𝜽)𝑞(𝜽′|𝜽)𝛼(𝜽,𝜽′) = 𝜋(𝜽′)𝑞(𝜽|𝜽′)𝛼(𝜽′,𝜽),
sincebotharemin(𝜋(𝜽)𝑞(𝜽′|𝜽),𝜋(𝜽′)𝑞(𝜽|𝜽′)).Nowsupposethat𝜽𝑘 ∼ 𝜋,
letB,C ⊆ ΘandletAbetheeventthattheproposalisaccepted.Then
∫ ∫
P(A,𝜽𝑘 ∈ B,𝜽𝑘+1 ∈ C) = 𝜋(𝜽𝑘)𝑞(𝜽′ | 𝜽𝑘)𝛼(𝜽𝑘,𝜽′) d𝜽′d𝜽𝑘
𝜽𝑘∈B 𝜽′∈C
∫ ∫
= 𝜋(𝜽′)𝑞(𝜽𝑘 | 𝜽′)𝛼(𝜽′,𝜽𝑘) d𝜽𝑘d𝜽′
𝜽𝑘∈B 𝜽′∈C
∫ ∫
= 𝜋(𝜽𝑘)𝑞(𝜽′ | 𝜽𝑘)𝛼(𝜽𝑘,𝜽′) d𝜽𝑘d𝜽′
𝜽′∈B 𝜽𝑘∈C
=P(A,𝜽𝑘 ∈ C,𝜽𝑘+1 ∈ B)
where,onthepenultimatelinewehaveswitchedthelabels.Since𝜽𝑘 = 𝜽𝑘+1
onarejection,wealsohavethat
(cid:16) (cid:17) (cid:16) (cid:17)
P A∁,𝜽𝑘 ∈ B,𝜽𝑘+1 ∈ C =P A∁,𝜽𝑘 ∈ C,𝜽𝑘+1 ∈ B .
SummingthetwoequalitiesaboveforAandA∁ gives
P(𝜽𝑘 ∈ B,𝜽𝑘+1 ∈ C) =P(𝜽𝑘 ∈ C,𝜽𝑘+1 ∈ B),
asrequired.2.1 TheMetropolis–HastingsAlgorithm 45
Burn-In,Mixing,EstimatorsandtheirVariance
Typically, the initial value for the Markov chain is not sampled from 𝜋,
sinceifitwerepossibletodothisthentherewouldbenoneedforMCMC.
Hence,themarginaldistributionsofearlypointsintheMarkovchainmight
notbesufficientlycloseto𝜋.Inpractice,wediscardsuchearlypointsfrom
the sample; the terms warm-up or burn-in are applied to both this initial
period and the samples that arise from it. Here, we imagine that there are
𝑏burn-insamples,𝜽 −𝑏+1,...𝜽
0
andthattheremainingsamples,whichare
deemedtobefromachainthathasapproximatelyconverged,are𝜽 1,...,𝜽𝑛.
The expectation of any function ℎ(𝜽) with respect to the posterior is then
estimatedvia:
𝑛
1 ∑︁
(cid:98)𝐼 𝑛(ℎ) :=
𝑛
ℎ(𝜽𝑘). (2.2)
𝑘=1
FollowingtheexpositioninSection1.3.2,subjecttoconditions,including
thatE
𝜋
(cid:2)ℎ(𝜽)2(cid:3) < ∞,thevarianceofthisestimatormaybeapproximated
as in (1.15); this is an approximation both because the Markov chain has
notfullyconvergedafterthe𝑏burn-initerations,andbecause𝑛isfinite.As
withstandardMonteCarloestimates,thestandarderrordecreasesas𝑛−1/2;
however,theconstantofproportionalityis(typically)higher,reflectingthe
factthatthesamplesare(typically)positivelycorrelated.
In most MCMC algorithms, the positive correlation arises from two
separatesources:firstly,aproposalmayberejected,inwhichcasethenew
position of the chain is the same as the old position; secondly most types
ofMetropolis-Hastingsalgorithmarelocal:theproposalis,insomesense,
closetothecurrentvaluewhencomparedtothesizeofthemainposterior
mass. Consequently, the chain can take many iterations to move from one
part of the posterior to another. The act of moving around the posterior
is termed mixing and in this book we informally refer to the number of
iterations taken to move substantially within the context of the posterior
distributionasthemixingtime.
RunningExample
The following running example of an isotropic Gaussian target distribu-
tion serves to demonstrate some properties of the Metropolis–Hastings
algorithm in practice. In the next section, it will be used to illustrate the
different measures of efficiency of a Metropolis–Hastings Markov chain
algorithm.46 ReversibleMCMCanditsScaling
Example2.1 Given𝑑 ∈ N,and𝜽 = (𝜃 1,...,𝜃 𝑑)⊤,let
(cid:32) 𝑑 (cid:33) (cid:18) (cid:19)
1 1∑︁ 1 1
𝜋 𝑑(𝜽) =N(𝜽;0,I𝑑) ≡
(2𝜋)𝑑/2
exp −
2
𝜃 𝑖2 ≡
(2𝜋)𝑑/2
exp − 2∥𝜽∥2 ,
𝑖=1
where ∥·∥ denotestheEuclideannorm,andI𝑑 the𝑑×𝑑 identitymatrix.
Fornow,weexploretheabovetargetusingtheRWMalgorithmdescribed
above:
(cid:18) (cid:19)
1 1
𝑞(𝜽′|𝜽) =N(cid:0) 𝜽′;𝜽,𝜆2I𝑑(cid:1) ≡
(2𝜋)𝑑/2𝜆𝑑
exp − 2𝜆2∥𝜽′−𝜽∥2 .
Figure2.1showsplotsfrom𝑛 =1000iterationsofthealgorithminExample
2.1 with 𝑑 = 1. The top plot starts from 𝜽 = 20 while the other two start
0
from 𝜽 = 1.Morethan99.7%oftheposteriormassliesbetween 𝜽 = −3
0
and𝜽 =3,andsothechainthatwasstartedoutsideofthisregionfirstheads
towardsthemainmass.Onceithasarrived,itthenexplorestheregionfor
the remainder of the time, 𝑛. The exploration is slow because the scale of
theproposedjumps,𝜆 =0.2,issmallcomparedwiththesizeoftheregion.
With larger proposed jumps, 𝜆 = 2, the exploration is much more rapid.
However, with jumps of size𝜆 = 20, most of the proposals are outside of
thehigh-densityregion,sotheacceptanceratioissmallandtheproposals
arerejected.Thus,eventhoughtheproposedjumpsarelarge,thealgorithm
doesnotexploretherangeofposteriorvaluesquickly.
λ=0.2 λ=2 λ=20
    
 
      
 
 
                                
k k k
Figure2.1 TraceplotsfromthreeRWMrunson𝜋 𝑑 from
Example2.1with𝑑 =1,usinginitialvaluesof20,1and1,and
scalingsof0.2,2and20respectively.
Theory tells us that the distribution of 𝜽𝑘 converges to 𝜋 as 𝑘 → ∞.
For a finite 𝑘, 𝜽𝑘 will not be an exact draw from 𝜋 but it might be close.
In Figure 2.1 (left) we might deem the distribution sufficiently close after2.1 TheMetropolis–HastingsAlgorithm 47
approximately300iterationsandsowemightdiscard𝜽 ,...,𝜽 asburn-
0 299
inandtake{𝜽 ,...,𝜽 }tobeanapproximate,correlated,samplefrom
300 1000
𝜋foruseinaMonteCarloaverage,𝜇 (cid:98)ℎ,oftheform(1.4).Therunsillustrated
inFigure2.1(middleandright)startedfromasensiblevalueintheposterior
andso{𝜽 ,...,𝜽 }mightreasonablybeused.
1 1000
Let us now ignore the need for burn-in when 𝜽 = 20 and 𝜆 = 0.2, or
0
considerathoughtexperimentwherethisalgorithmwasalsostartedfrom
𝜽 = 1. From Figure 2.2, the sample obtained when 𝜆 = 2 appears to
0
represent 𝜋,whichissymmetricaboutasinglemodeat0andhassupport
beyond ±2, much better than either of the other two samples. Thus, we
mightexpectestimates, 𝜇 (cid:98)ℎ,obtainedfromthealgorithmwhen𝜆 =2tobe,
insomesense,moreaccurate.
   
       
   
   
   
   
           
                   
θ θ θ
Figure2.2 Histogramsofthesamplesobtainedfromthe
algorithmsinFigure2.1.Theleftplot(correspondingto𝜆 =0.2,
𝜽 =20)wascreatedafterdiscarding{𝜽 ,...,𝜽 }asburn-in;
0 0 299
fortheothertworuns(centre:𝜆 =2;right:𝜆 =20)only𝜽 was
0
discarded.
The empirical acceptance rate for a Metropolis–Hastings algorithm is
the fraction of the 𝑛 proposals that were accepted. For the three RWM
algorithms, these were respectively 0.881, 0.485 and 0.059; the smaller
the proposed jumps, the closer 𝜋(𝜽′) typically is to 𝜋(𝜽𝑘) and so the
higher the acceptance rate. For a stationary Metropolis–Hastings Markov
chain,theempiricalacceptancerateapproximatesthetrueacceptancerate
atstationarity:
𝛼 =E
𝜽∼𝜋,𝜽′∼𝑞(·|𝜽)
[𝛼(𝜽,𝜽′)]
AspectsoftheproposalfortheRWM,MALAandHMCareoftentunedby
targetinganempiricalacceptanceratethatisneithertoohighnortoolow.In
latersections,theacceptanceratewillprovideuswithanintuitiveentrance
intothebehaviourofthecanonicalMetropolis–Hastingsalgorithmsasthe
 \ W L V Q H '  \ W L V Q H '  \ W L V Q H '48 ReversibleMCMCanditsScaling
dimension of the parameter vector increases. Here it will be helpful to
define the acceptance ratio: 𝜌(𝜽,𝜽′) := 𝜋(𝜽′)𝑞(𝜽|𝜽′)/{𝜋(𝜽)𝑞(𝜽′|𝜽)}, so
that𝛼(𝜽,𝜽′) =min[1,𝜌(𝜽,𝜽′)].
2.1.1 Component-wiseupdatesandGibbsmoves
Algorithm1,theMetropolis–Hastingsalgorithm,andallofthespecialcases
that we will examine in this chapter, are written so that a single iteration
consistsofaproposaltochangetheentire𝜽vectorandadecisiononwhether
ornottoacceptthisproposal.However,itisalsopossible,andsometimes
helpful,tosequentiallyupdatesubsetsofthecomponentsof𝜽.Indeed,each
iterationoftheveryfirstMetropolis–Hastingsalgorithm(Metropolisetal.,
1953) cycled through pairs of components (𝑥 and 𝑦 coordinates of each
particleinalatticeofalargenumberofparticles),applyingarandomwalk
Metropolisupdateonepairatatime.
Denote the set of components to be updated by
𝜽(𝑖)
and the remaining
componentsby𝜽(−𝑖).Bywritingthetargetas𝜋(𝜽) = 𝜋(𝜽(−𝑖))𝜋(𝜽(𝑖)|𝜽(−𝑖)),
and the proposal as 𝑞 𝑖(𝜽′(𝑖)|𝜽(𝑖),𝜽(−𝑖)), essentially the same argument as
foraproposalthatchangesallcomponentsshowsthatthecomponent-wise
propose/accept-rejectstepwithanacceptanceprobabilityof
min(cid:18)
1,
𝜋(𝜽′(𝑖)|𝜽(−𝑖))𝑞 𝑖(𝜽(𝑖)|𝜽′(𝑖),𝜽(−𝑖))(cid:19)
𝜋(𝜽(𝑖)|𝜽(−𝑖))𝑞 𝑖(𝜽′(𝑖)|𝜽(𝑖),𝜽(−𝑖))
satisfiesdetailedbalancewithrespecttothefullposterior, 𝜋.Ofcourse,if
only that move were used, some components would never be updated, the
algorithm would be reducible, and ergodic averages would, therefore, not
convergetothecorrespondingtrueexpectations.Thecompositionofmany
such moves over different components, typically, does not satisfy this de-
tailedbalancecondition,but,sinceeachmovepreserves𝜋,thecomposition
does,too.
Inthespecialcasewheretheproposalforthe𝑖thblockofcomponentsis
𝑞 𝑖(𝜽′(𝑖)|𝜽𝑘) := 𝜋(𝜽′(𝑖)|𝜽 𝑘(−𝑖)),
theacceptanceprobabilityis1andthemoveiscalledaGibbsmove.Sucha
moveisonlyfeasiblewhenitispossibletosamplefrom𝜋(𝜽(𝑖)|𝜽(−𝑖))which
𝑘
most usually occurs when, conditional on
𝜽(−𝑖),
the prior for
𝜽(𝑖)
is con-
𝑘
jugate with its likelihood. For example, when 𝑦 1,...,𝑦 𝑁 are independent
realisations from a N(𝜇,1/𝜏) distribution and 𝜇 and 𝜏 have independent
priors with 𝜇 following a Student-t distribution and 𝜏 ∼ Gam(𝑎,𝑏), then
a posteriori 𝜏|𝜇 ∼ Gam(𝑎 + 𝑛/2,𝑏 + 1 2(cid:205)𝑛 𝑗=1(𝑦 𝑖 − 𝜇)2); this property is2.1 TheMetropolis–HastingsAlgorithm 49
sometimes called conditional conjugacy. Gibbs moves offer a further ad-
vantagewhencomparedwithmanyotherMetropolis-Hastingsmoves,over
andabovethefactthattheacceptanceprobabilityis1:theupdatedparam-
etercomponentissampledfromthefullrangeofitsconditionalposterior.
Whencomponentsareclosetoindependent,thiscontrastswithalgorithms
suchastherandomwalkMetropolisandMALA,where,inmoderatetohigh
dimensions,themovesarelocal–theproposedvalueisclosetothecurrent
value.However,when,asistypicallythecase,componentsarecorrelated,
the conditional posterior for a component can have a much smaller range
thanitsmarginalposterior,andsotheGibbsmoves,too,are,ineffect,local.
WhilsttheyareausefultoolintheMCMCarmoury,Gibbsmovesarenot
the focus of this book and for further information, we refer the interested
readertothegeneraltextscitedinSection2.3.
2.1.2 TheMetropolis–HastingsIndependenceSampler
Consider the Metropolis–Hastings independence sampler (MHIS) in the
case where 𝑞(𝜽) = 𝜋(𝜽). In this case 𝛼(𝜽𝑘,𝜽′) = 1 and every proposal is
accepted.Sinceproposalsarefrom 𝜋,theMHISprovidesuswithani.i.d.
samplefrom 𝜋.Ofcourse,inpractice,wearetypicallynotabletosample
from 𝜋, but this suggests a heuristic for the MHIS: choose a proposal so
thattheacceptancerateisascloseaspossibleto1.
Forunimodalposteriorsinlowdimensionsareasonableapproximation
canoftenbeobtainedbyfirstusinganumericalmethodtofindtheposterior
modeandthenchoosingaproposalthatmatchesthemodeandthecurvature
ofthelog-posterioratthispoint.Thisstrategydoesnotscalefavourablyto
highdimensions,however,asthefollowingsimpleexampleshows.
ConsidertheisotropicunitGaussianposteriorfromExample2.1anduse
thefollowingMHISproposal:
(cid:18) (cid:19)
1 1
𝑞(𝜽′|𝜽) =N(0,𝜎2I𝑑) ≡
(2𝜋)𝑑/2𝜎𝑑
exp − 2𝜎2∥𝜽′∥2 .
Theacceptanceratio, 𝜌(𝜽,𝜽′),is
exp(−1∥𝜽′∥2)exp(− 1 ∥𝜽∥2) (cid:26) 1 (cid:18) 1 (cid:19) (cid:27)
2 2𝜎2 =exp 1− (cid:0) ∥𝜽∥2−∥𝜽′∥2(cid:1) ,
exp(−1∥𝜽∥2)exp(− 1 ∥𝜽′∥2) 2 𝜎2
2 2𝜎2
so
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1 1
log𝜌(𝜽,𝜽′) = 1− ∥𝜽∥2− ∥𝜽′∥2 .
𝑑 2 𝜎2 𝑑 𝑑50 ReversibleMCMCanditsScaling
Ifthechainisstationary,then∥𝜽∥2 =(cid:205) 𝑖𝑑 =1𝜃 𝑖2 ∼ 𝜒 𝑑2since𝜃
𝑖
𝑖 ∼𝑖𝑑 N(0,1).Thus
E(cid:2) ∥𝜽∥2/𝑑(cid:3) = 1 and Var(cid:2) ∥𝜽∥2/𝑑(cid:3) = 2/𝑑, and the same properties hold
for ∥𝜽′/𝜎∥2/𝑑. Thus, in high dimensions, to a first-order approximation,
∥𝜽∥2/𝑑 ≈1and ∥𝜽′∥2/𝑑 ≈ 𝜎2 and
(cid:18) (cid:19)
1 1 1
log𝜌(𝜽,𝜽′) ≈ 1− (cid:0) 1−𝜎2(cid:1).
𝑑 2 𝜎2
Thisgivesafirst-orderapproximationtotheacceptancerateof
(cid:18) (cid:26) 𝑑 (cid:27)(cid:19)
min 1,exp − (cid:0)𝜎2−1(cid:1)2 ,
2𝜎2
which grows exponentially small with dimension unless 𝜎 = 1. Alterna-
√
tively,stabilisingtheacceptancerateabovezerorequires𝜎2 =1+𝑂(1/ 𝑑);
theapproximationmustbecomemoreandmoreaccurateas𝑑 →∞.
The exponential decrease in acceptance rate with dimension is closely
linked with the exponential increase in the variance of the weights with
dimensionintheimportancesamplingexampleattheendofSection1.1.5.
Inhighdimensions,asufficientlyaccurateandtractableapproximation,𝑞,
israrelyavailable;consequentlyimportancesamplingandMHISarerarely
used,exceptinrelativelysimple,low-dimensionalscenarios.
2.1.3 TheRandomWalkMetropolisAlgorithm
TherandomwalkMetropolis(RWM)algorithmwasthefirstMCMCalgo-
rithmtoeverbeused.Unliketheindependencesampler,itdoesnotrequire
anaccurateglobalapproximationtotheposteriorandcanbetunedsothat
itworkseveninveryhighdimensions.Furthermore,unlikethealgorithms
that we shall explore subsequently, it does not require the gradient of the
logposterior.
ThemostfrequentlyusedRWMproposal,theso-calledpreconditioned
RWM, has the form 𝜽′|𝜽 = 𝜽 + N(0,𝜆2V), where V is an estimate of
the posterior variance matrix and 𝜆 is a tunable scaling parameter. This
enhancement can increase the efficiency of the algorithm by many orders
ofmagnitudewhenthecomponentsof𝜽 arehighlycorrelatedand/orvary
onverydifferentlengthscales.However,whatevertheproposal,theRWM
constraint, that 𝑞(𝜽′|𝜽) = 𝑞(𝜽|𝜽′), means that the acceptance probability
simplifiestomin{1,𝜋(𝜽′)/𝜋(𝜽)}.2.1 TheMetropolis–HastingsAlgorithm 51
ScalingofRWMwithDimension
We again consider the isotropic Gaussian posterior of Example 2.1 and
showhowtheRWMalgorithmusingaN(𝜽,𝜆2I𝑑)proposalcanbemadeto
worknomatterwhatthedimension.
Writetheproposalas𝜽′ = 𝜽 +𝜆Z,whereZ ∼ N(0,I𝑑).Thelogaccep-
tanceratioisthen
1 1 1
log𝜌(𝜽,𝜽′) =− ∥𝜽 +𝜆Z∥2+ ∥𝜽∥2 =−𝜆∥𝜽∥(cid:98)𝜽 ·Z− 𝜆2∥Z∥2
2 2 2
1
=D −𝜆∥𝜽∥ 𝑍′− 𝜆2∥Z∥2, (2.3)
2
where 𝑍′ ∼ N(0,1) and (cid:98)𝜽 = 𝜽/∥𝜽∥. Now ∥Z∥2 ∼ 𝜒 𝑑2 and ∥𝜽∥2 ∼ 𝜒 𝑑2,
and by the same argument as used for the MHIS, we might make a first
√
approximation of ∥Z∥2 ≈ 𝑑 and ∥𝜽∥ ≈ 𝑑, from which it appears that
the acceptance ratio must decay exponentially quickly with dimension.
However,thisneednotbethecase,sincewecancontrolthescaling,𝜆.The
√
factthat ∥𝑍∥2/𝑑 ≈1and ∥𝜽∥/ 𝑑 ≈1suggestssetting
ℓ
𝜆 = √ (2.4)
𝑑
forsomefixedℓ > 0.Inthiscase
(cid:20) (cid:26) ∥𝜽∥ 1 1 (cid:27)(cid:21)
𝛼(𝜽,𝜽′) =D min 1,exp −ℓ𝑍′ √ − ℓ2 ∥Z∥2
𝑑 2 𝑑
(cid:20) (cid:26) (cid:27)(cid:21)
1
≈min 1,exp −ℓ𝑍′− ℓ2 .
2
Thisquantityisstableawayfromzeroanddoesnotdependondimension.
Takingexpectations,elementarycalculusgives
(cid:18) (cid:19)
1
E
𝜽∼𝜋,𝜽′∼𝑞(·|𝜽)
[𝛼(𝜽,𝜽′)] ≈2Φ − 2ℓ ,
whereΦisthecumulativedistributionfunctionofastandardnormalrandom
variable. This equation describes how, for a high-dimensional Gaussian
target,theacceptanceratedecreasesasthe(dimensionally-adjusted)scaling,
ℓ,increases.
Indeed, much more is true. Figure 2.3 shows trace plots for 𝜃 , the first
1
component of 𝜽, when 𝑑 = 50 and when 𝑑 = 500 and using a scaling of
√
𝜆 = ℓ/ 𝑑 with ℓ = 2. The behaviours of the trace plots appear similar,
except that when 𝑑 = 500 the time scale over which the process explores
theposterioristentimesthatwhen𝑑 =50.52 ReversibleMCMCanditsScaling
d=50
 
 
 
 
 
 
 
                              
k
d=500
 
 
 
 
 
 
 
                                    
k
Figure2.3 Traceplotsforthefirstcomponent,𝜃 ,of𝜽 fora
1
RWMHon𝜋(𝜽) ∝exp(−1∥𝜽∥2)in𝑑 =50(top)and𝑑 =500
2
(bottom).Bothalgorithmswereinitialisedusingasamplefrom𝜋
√
andeachusedascalingof𝜆 =ℓ/ 𝑑 withℓ =2.
√
As dimension goes to infinity, with a scaling of ℓ/ 𝑑 and with time
sped up by a factor of 𝑑 (essentially running for 𝑛𝑑 iterations rather than
𝑛),thepathofthefirstcomponentapproaches(indistribution)thepathof
thestochasticdifferentialequation(2.5),below.Forsimplicityofnotation,
wedenotethefirstcomponentby𝜃 anddenoteitsmarginaldistributionby
𝑓(𝜃) ∝exp(−𝜃2/2).
d𝜃 𝑡 = 1 [log 𝑓(𝜃 𝑡)]′ ℎ(ℓ)d𝑡+√︁ ℎ(ℓ)d𝑊 𝑡, (2.5)
2
where ℎ(ℓ) = ℓ2 ×2Φ(−ℓ/2). This is the OU process defined in (1.17),
θ
θ2.1 TheMetropolis–HastingsAlgorithm 53
with 𝑏 = √︁ℎ(ℓ);ithasaN(0,1) stationarydistribution.Here, ℎ(ℓ) canbe
thoughtofasthespeed ofthediffusion,withalargervaluecorresponding
to a diffusion that will converge to stationarity more quickly, and can be
maximised with respect to ℓ, giving ℓ ≈ 2.38. This corresponds to an
opt
acceptancerateof2Φ(−ℓ /2) ≈0.234,andleadstothewell-knownadvice
opt
to choose the RWM scaling so that the acceptance rate is approximately
1/4.
Ofmoreimportanceforusisthatthelimitingprocessisapproachedby
√
letting𝜆 =ℓ/ 𝑑andspeedinguptimebyafactorof𝑑.Reversingthislogic,
in dimension 𝑑, the first component moves 𝑑 times more slowly than the
diffusion.Inotherwordsthetimeor,equivalently,thenumberofiterations
takenbytheRWMtoexploretheposteriorindimension 𝑑 isproportional
to𝑑.
Figure2.4emphasisesthislineardependenceondimensionbycontinu-
ingtheexampleinFigure2.3.Indimension𝑑 =50,thealgorithmisrunfor
𝑛 =10000iterations,andforeachcomponent,theauto-correlationsarecal-
culateduptoalagof300.Thedottedbluelineshowsthecomponent-wise
averageofeachauto-correlation.For 𝑑 = 500,𝑛 = 100000iterationswere
usedandauto-correlationsuptoalagof3000werecalculated.Thedashed
red line shows the component-wise averages plotted against lag/10. The
curves are almost indistinguishable and the resulting estimated integrated
auto-correlation times are, respectively, 70 and 718. The corresponding
effectivesamplesizesare,therefore,almostidentical,eventhoughtheex-
perimentwith𝑑 =500usedtentimesthenumberofiterations.
The above arguments have been made rigorous and applied to more
complextargetssuchas𝜋(𝜽) =(cid:206) 𝑖𝑑 =1𝐶 𝑖𝑓(𝐶 𝑖𝜃 𝑖),foralargeclassofdensity
functions 𝑓 (see Roberts and Rosenthal, 2001, for example). The limiting
process for the first coordinate becomes a Langevin diffusion (1.20) with
a stationary density of 𝐶 𝑓(𝐶 𝜃 ), and in all cases the time taken by the
1 1 1
RWMtoexplorethetargetisproportionalto𝑑.
2.1.4 TheMetropolis-AdjustedLangevinAlgorithm
The Metropolis-adjusted Langevin algorithm (MALA) differs from the
RWM proposal of 𝜽′|𝜽 ∼ N(𝜽,𝜆2I𝑑) through an additional determinis-
ticoffsetof 1𝜆2∇log𝜋(𝜽).Wemotivatethisproposalandthengeneraliseit
2
toallowpreconditioningviaapositivedefinitevariancematrix,V;aswith
theRWM,thiscanbringdramaticefficiencyimprovementsinpractice.
The deterministic offset can be seen as an additional movement in the
“uphill” direction, that is biasing the proposal to move to areas of higher54 ReversibleMCMCanditsScaling
    d=50
d=500
   
   
   
   
   
             
 O D J (d=50)  R U  O D J    (d=500)
Figure2.4 Component-wiseaverageauto-correlationplotsfora
RWMon𝜋(𝜽) ∝exp(−1∥𝜽∥2)in𝑑 =50and𝑑 =500.Both
2
algorithmswereinitialisedusingasamplefrom𝜋andeachuseda
√
scalingof𝜆 =ℓ/ 𝑑 withℓ =2.
posteriordensity;however,thereisadeepermotivation.Theproposalcan
bewrittenas
1
𝜽′|𝜽 = 𝜽 + 𝜆2∇log𝜋(𝜽)+𝝐 (2.6)
2
where𝝐 ∼N(0,𝜆2I𝑑).
Substituting 𝜆2 = 𝛿𝑡, we see that the proposal is exactly the Euler–
Maruyama discretisation of the Langevin diffusion that has a stationary
distributionof𝜋,(1.20)(with𝑏 =1).Inparticular,inthehypotheticallimit
as 𝜆 ↓ 0 the algorithm should require no accept-reject step to target 𝜋. In
thissense,itisanaturalformfortheproposal.
WenowderivethepreconditionedMALAproposal.Forageneralpositive-
definite V, let A be a square matrix such that AA⊤ = V, and consider
𝝍 =A𝜽.Multiplying(2.6)byAgives
1
𝝍′|𝝍 =𝝍+ 𝜆2A∇ log𝜋(𝜽)+A𝝐,
2 𝜽
where we have made explicit that the gradient is with respect to 𝜽. The
ρ2.1 TheMetropolis–HastingsAlgorithm 55
densityfor𝝍 is (cid:101)𝜋(𝝍) ∝ 𝜋(A−1𝝍) = 𝜋(𝜽).Further,∇ 𝜃 =A⊤∇ 𝜓,so
1
𝝍′|𝝍 =𝝍+ 𝜆2V∇ log𝜋(𝝍)+A𝝐.
2 𝝍 (cid:101)
Since A𝝐 ∼ N(0,𝜆2V), this corresponds to the preconditioned MALA
proposal:
(cid:18) (cid:19)
1
𝜽′|𝜽 ∼N 𝜽 + 𝜆2V∇log𝜋(𝜽),𝜆2V .
2
Whentheposteriorisunimodal,preconditionedMALAisoftenmosteffi-
cientwhenVisanapproximationtotheposteriorvariance.
WenowexplorethescalingofMALAwithdimensionandthesensitivity
tolargegradients.Inbothoftheseanalysesthefollowingsimplificationof
partofthelogacceptanceratiowillbehelpful.FortheMALAproposalin
(2.6),andwritingg(𝜽) forthegradientat𝜽,
𝑞(𝜽|𝜽′) 1 𝜆2 1 𝜆2
log = ∥𝜽′−𝜽 − g(𝜽)∥2− ∥𝜽 −𝜽′− g(𝜽′)∥2
𝑞(𝜽′|𝜽) 2𝜆2 2 2𝜆2 2
= 1 [g(𝜽)+g(𝜽′)]⊤ (cid:2)𝜆2g(𝜽)−𝜆2g(𝜽′)+4(𝜽 −𝜽′)(cid:3)
8
=−1 [g(𝜽)+g(𝜽′)]⊤ (cid:2)𝜆2g(𝜽)+𝜆2g(𝜽′)+4𝝐(cid:3). (2.7)
8
ScalingofMALAwithDimension
IntheisotropicGaussianrunningexample,Example2.1,thelogacceptance
ratioforMALAis,using(2.7),
1 1 𝜆2 1
log𝜌(𝜽,𝜽′) = ∥𝜽∥2− ∥𝜽′∥2− ∥𝜽 +𝜽′∥2+ (𝜽 +𝜽′)⊤Z.
2 2 8 2
Substituting𝜽′ = (1−1𝜆2)𝜽+𝜆Zfrom(2.6)andcollectingterms,weobtain
2
log𝜌(𝜽,𝜽′) =−𝜆3 (cid:20) 𝜆(cid:8) ∥Z∥2−∥𝜽∥2(cid:9) + 1 𝜆3∥𝜽∥2+(2−𝜆2)𝜽⊤Z(cid:21) , (2.8)
8 4
whereZ ∼ N(0,I𝑑).Since ∥Z∥2 and ∥𝜽∥2 areboth 𝜒 𝑑2,eachhasanexpec-
tationof𝑑 andtheirdifferenceis𝑂 𝑝(𝑑1/2);further,𝜽⊤Z∼N(0,∥𝜽∥2).
Tounderstandtherelativesizesofthetermsweinformallywrite:
(cid:104) (cid:16) (cid:17) (cid:16) (cid:17)(cid:105)
log𝜌(𝜽,𝜽′) =𝜆3 𝜆𝑂
𝑝
𝑑1/2 +𝜆3𝑂 𝑝(𝑑)+(2−𝜆2)𝑂
𝑝
𝑑1/2 .
Thus,with𝜆 = ℓ/𝑑1/6,thefirsttermvanishesandthesecondandthirdare
𝑂 𝑝(1),leadingtoanacceptanceratiothatis𝑂 𝑝(1).56 ReversibleMCMCanditsScaling
AsfortheRWM,itispossibletoobtainalimitingdiffusionoftheform
(2.5) for the first component of 𝜽 as the dimension goes to infinity. For
MALA, however, the required scaling is 𝜆 = ℓ/𝑑1/6, time is sped up by a
factor of 𝑑1/3 (essentially running for 𝑛𝑑1/3 iterations rather than 𝑛) and,
fortheGaussiantarget,thespeedofthediffusionis ℎ(ℓ) = 2ℓ2Φ(−ℓ3/4).
Optimising the speed with respect to the scaling leads to a recommended
acceptance rate of approximately, 57.4%. As with the RWM, the more
important point for us is that the limiting OU process mixes in a time of
𝑂(1), so the original process, before it has been sped up, mixes in a time
of𝑂(𝑑1/3).Thisisconsiderablyfasterthanthe𝑂(𝑑) mixingoftheRWM.
AsfortheRWM,theaboveresult,whichisspecifictoaN(0,I𝑑) target,
has been generalised to targets of the form 𝜋(𝜽) = (cid:206) 𝑖𝑑 =1𝐶 𝑖𝑓(𝐶 𝑖𝜃 𝑖), again
leading to a Langevin diffusion for the first component in the limit as
𝑑 →∞,anoptimalacceptancerateof57.4%,andrequiringtimetobesped
upbyafactorof𝑑1/3;seeRobertsandRosenthal(2001).
The above product results for MALA rely on the existence and good
behaviour of all derivatives of 𝑓 up to the 7th order, and that the process
was started from stationarity. Christensen et al. (2005) investigates the
behaviourofMALAonahigh-dimensionalisotropicGaussiantargetwhen
the algorithm is started close to the mode. When a scaling of 𝜆 = ℓ/𝑑1/6
is used, in the limit as 𝑑 → ∞ the process, sped up by a factor of 𝑑1/3,
does not move. Substituting 𝜽 = 0, for example, into (2.8), we see that
log𝜌 = −𝜆4∥Z∥2/8. Since ∥Z∥2 = 𝑂 𝑝(𝑑), the acceptance probability is
approximately exp[−ℓ4𝑑1/3]. If, instead, a scaling of 𝜆 = ℓ/𝑑1/4 is used
then then acceptance rate remains 𝑂 𝑝(1) as 𝑑 → ∞. Christensen et al.
(2005) shows that this new process, sped up by a factor of 𝑑1/2, moves
deterministicallytowardstheregionofthemainposteriormass.Reductions
inefficiencycanalsooccurifonlylowerderivativesofthetargetarewell-
behaved.
Sensitivitytogradients
WhilstthescalingpropertiesofMALAarefavourablecomparedwiththose
oftheRWMandMHIS,theperformanceofMALAisnotoriouslysensitive
tolargegradients.Weillustratethiswithasimpleexampleinonedimension.
Example2.2 Let 𝜃 ∈ Randforsome 𝑎 > 0let 𝜋(𝜃) ∝ exp(cid:0) −1|𝜃|𝑎(cid:1) ,so
𝑎
∇log𝜋(𝜃) =−𝜃|𝜃|𝑎−2 and |∇log𝜋(𝜃)| = |𝜃|𝑎−1.
When𝑎 > 2,whateverthe(fixed)valueof𝜆,forlargeenough𝜃,E[𝜃′|𝜃]
is dominated by the term 1𝜆2∇log𝜋(𝜃), so that (with a very high proba-
22.2 HamiltonianMonteCarlo 57
bility) the proposal has the opposite sign to the current value and a much
largermagnitude,𝜆2|𝜃|𝑎−1/2.
Writing𝜖 =𝜆𝑍,where𝑍 ∼N(0,1),thelogacceptanceratioforMALA
is 𝜌(𝜃,𝜃′) =log𝜋(𝜃′)−log𝜋(𝜃)+𝐵(𝜃,𝜃′),where,from(2.7),
1
𝐵(𝜃,𝜃′) = {𝜃|𝜃|𝑎−2+𝜃′|𝜃′|𝑎−2}{4𝜆𝑍 −𝜆2𝜃|𝜃|𝑎−2−𝜆2𝜃′|𝜃′|𝑎−2}. (2.9)
8
The highest order term in (2.9) arises from the product of 𝜃′ terms, so
it is negative and of order |𝜃|2(𝑎−1)2. The difference in log posteriors is
dominated by log𝜋(𝜃′) ∼ −|𝜃|𝑎(𝑎−1), which is, again, large and negative.
Hence, the acceptance probability is almost certainly very close to 0. Un-
surprisingly, since the proposal is even further from the main mass than
the current value is, the proposal is almost certainly rejected. As 𝜃 moves
furtherandfurtherintothetailofthetarget,theaverage(overtheproposal
distribution) acceptance probability for MALA becomes arbitrarily small
andthealgorithmconvergesincreasinglyslowly.
In Example 2.2, similar poor behaviour occurs even with 𝑎 = 2 (a
Gaussianposterior),provided𝜆2 > 2.Moregenerally,MALAcanbecome
”stuck” anywhere that ∥∇log𝜋∥ is large. In particular, the basic MALA
algorithmshouldbeusedwithcautioniftheusersuspectsthattheposterior
hastailswhicharelighterthanGaussian.Mitigationsagainstthisbehaviour
arebrieflydiscussedintheChapterNotes.
2.2 HamiltonianMonteCarlo
We have seen that when the dimension 𝑑 is high, MALA can maintain a
highacceptanceratewithascalingofℓ/𝑑1/6,whereastheRWMrequiresa
scalingofℓ/𝑑1/2.Inotherwords,MALAcanproposemuchlargersensible
jumps than the RWM. As we shall see, Hamiltonian Monte Carlo allows
evenlargerjumpsthanMALA,whilstmaintainingahighacceptancerate.
Hamiltonian Monte Carlo (HMC) can be viewed as using a Metropolis–
Hastings algorithm, but with a more intricate proposal mechanism than
thoseseensofar.
One may consider −log𝜋(𝜽) as a potential energy surface. Intuitively
onemaythinkofthisasaphysicalsurfaceonwhicha“particle”withmass𝑀
currentlysitsata“height”(strictly,potentialenergy)of−log𝜋(𝜽) abovea
currentparametervalue,𝜽 ∈ R𝑑.Toobtaintheproposal,theparticleisgiven
a random momentum, p ∈ R𝑑 drawn from a symmetric distribution. The
truefrictionlessmotionthattheparticlewouldundergoalongthepotential58 ReversibleMCMCanditsScaling
surface according to Hamiltonian dynamics is approximated numerically.
Theproposalistheposition𝜽′ ∈ R𝑑 afteratime𝑇,atuningparameter.
Asweshallsee,thelog-acceptanceratioforthealgorithmcanbewritten
as
(cid:26) (cid:27)
1 1
log𝜌(𝜽,𝜽′) =−log𝜋(𝜽)+ p⊤p− −log𝜋(𝜽′)+ p′⊤p′ ,
2𝑀 2𝑀
where p′ is the momentum at time𝑇. The term, p⊤p/(2𝑀) is the kinetic
energy of the particle, and −log𝜋(𝜽) is the potential energy, so the ac-
ceptancerateismin[1,exp(−𝛿𝐸)],where𝛿𝐸 isthechangeintotalenergy
over time 𝑇. Frictionless motion conserves the total energy so that under
the exact dynamics the acceptance probability is 1. Numerical integration
approximates the dynamics, using an integration step size, 𝜖. A smaller 𝜖
gives a more accurate numerical scheme and a higher average acceptance
rate, but for a given𝑇 it also requires more numerical steps and, hence, a
largercomputationalcost.
Wenowprovideamorerigorousdescriptionofastandardversionofthe
algorithm,includinganexplanationoftheacceptanceprobabilitythatleads
toastationarydistributionof𝜋.
Thefirstcomponentofthealgorithmisapositive-definitemassmatrix,
M,theinverseofwhichplaysasimilarroletothepreconditioningmatrix
V used in the RWM and MALA. The mass of an object, as used in the
intuitive explanation above, is the ratio between the magnitude of a force
that is applied and the magnitude of the acceleration that results and is a
scalar. For additional generality, in HMC, we imagine that this ratio can
bedifferentalongeachofasetof 𝑑 orthogonalprincipalaxesleadingtoa
massmatrixratherthanascalar.
ThecorecomponentoftheHMCalgorithmisthenumericalintegration
scheme,whichrepeatedlyusestheleapfrogsteptodeterministicallyevolve
the position and momentum from a time 𝑡 to a time 𝑡 +𝜖: (𝜽𝑡+𝜖,p𝑡+𝜖) =
Leap(𝜽𝑡,p𝑡;𝜖,M),where
1 1
p
∗
=p𝑡+ ∇log𝜋(𝜽𝑡), 𝜽𝑡+𝜖 = 𝜽𝑡+𝜖M−1p ∗, p𝑡+𝜖 =p ∗+ ∇log𝜋(𝜽𝑡+𝜖).
2 2
HMCusestheleapfrogschemeratherthan,forexample,theEulerorRunge–
Kutta schemes because the leapfrog scheme possesses two key properties
thatwillbediscussedshortly.
HMC repeats the leapfrog step 𝐿 times, where 𝐿𝜖 = 𝑇, to obtain the
proposal, 𝜽′ = 𝜽𝑇 as depicted in Figure 2.5. The proposed momentum is,
in fact, p′ = −p𝑇 and we denote the transformation: (𝜽,p) → (𝜽′,p′) by
𝐿
Leap .
−2.2 HamiltonianMonteCarlo 59
 
 F X U U H Q W
 S U R S R V H G
 
 
 
 
 
 
             
Figure2.5 Initialpoint𝜽 =𝜽 0 =+andfinalpoint𝜽′ =𝜽 2.5 =×
after𝐿 =25leapfrogstepsusingatimeintervalof𝜖 =0.1anda
massmatrixofM=I .Themomentumatthecurrentand
2
proposedpoint(beforethemomentflip)isproportionaltothesize
ofthecorrespondingarrowandintermediatepointsappearas
smallsolidcircles.
Standard HMC proposes p from a N(0,M) distribution, and can be
viewed as targeting a joint density of (𝜽,p) that is the product of the
densityforpandtheposterior:
𝜋(𝜽,p) = 𝜋(𝜽)(2𝜋)−𝑑/2det(M)−1/2exp(cid:2) − 1 p⊤M−1p(cid:3)
(cid:101)
2
At the end of each iteration, we discard p, and the marginal for 𝜽 is 𝜋,
as required. Algorithm 2 details the standard version of the Hamiltonian
MonteCarloalgorithm.
HMC combines a momentum refresh with a Metropolis–Hastings step
whichusesadeterministicproposal(𝜽,p) ← (𝜽′,p′) ≡Leap𝐿(𝜽,p;𝜖;M);
−
finallythenewmomentumisdiscarded.Themomentumrefreshmentpre-
serves𝜋asitsamplesdirectlyfromthecorrectconditional.Wenowexplain
(cid:101)
whytheaccept-rejectstepwithadeterministicproposalalsopreserves𝜋.
(cid:101)
Theleapfrogsteppossessestwokeyproperties:60 ReversibleMCMCanditsScaling
Algorithm2:HamiltonianMonteCarlo
Input:Density𝜋(𝜽),initialvalue𝜽 ,massmatrixM,timeinterval
0
𝑇,numberofleapfrogsteps 𝐿.
𝜖 ←𝑇/𝐿.
for 𝑘 ∈ 0,...,𝑛−1do
Samplep∼N(0,M).
(𝜽′,p′) ←Leap −𝐿(𝜽𝑘,p).
Calculatetheacceptanceprobability:
(cid:18) 𝜋(𝜽′,p′)(cid:19)
𝛼(𝜽𝑘,p;𝜽′,p′) :=min 1, (cid:101)
(cid:101)𝜋(𝜽𝑘,p)
.
Withaprobabilityof𝛼(𝜽𝑘,p;𝜽′,p′) accepttheproposal,
𝜽𝑘+1 ← 𝜽′;otherwiserejectit,𝜽𝑘+1 ← 𝜽𝑘.
end
Property1 LeaphasaJacobianof1.
Property2 IfLeap(𝜽,p) = (𝜽′,p′) thenLeap(𝜽′,−p′) = (𝜽,−p).
Property1 arisesbecause theLeapfrogis acomposition ofthreetransfor-
mations each of which has a Jacobian of 1. Property 2 is straightforward
to verify, and emulates frictionless dynamics in that if after moving for
sometimethemomentumofanobjectissuddenlyreversed,afterthesame
amountoftimeagaintheobjectwillendupbackwhereitstarted,moving
with the same speed as when it started but in the opposite direction. The
composition of 𝐿 leapfrog steps possesses the same property: in Figure
2.5, starting at the × but with a momentum given by the reverse of the
corresponding arrow, and proceeding for 25 leapfrog steps leads to the +
position, but with a momentum of exactly the reverse of the true initial
momentumthatthecorrespondingarrowrepresents.
Naturally,Leap𝐿 ,thecompositionof 𝐿 leapfrogsteps,combinedwitha
−
𝐿
momentumflip,alsohasaJacobianof1.Moreover,Leap isself-inverse:
−
Leap𝐿(Leap𝐿(𝜽,p)) = (𝜽,p); equivalently, from (𝜽′,p′) we would pro-
− −
pose (𝜽,p).
AsinSection2.1,thedetailedbalanceconditionistrivialunderrejection
sowefocusonacceptances.LetAbetheeventofanacceptanceandwrite
(𝜽𝑘,p𝑘) and (𝜽𝑘+1,p𝑘+1) for the position and momentum before and after
the acceptance step (and before the momentum is discarded). Then, for
B ∈ R2𝑑 and C ∈ R2𝑑, and writing Leap𝐿(A) for the image of a set A
−2.2 HamiltonianMonteCarlo 61
𝐿
underLeap ,
−
P(A,(𝜽𝑘,p𝑘) ∈ B,(𝜽𝑘+1,p𝑘+1) ∈ C)
∬
= 𝜋(𝜽,p)𝛼(𝜽,p;𝜽′,p′) d(𝜽,p)
(cid:101)
B∩Leap−𝐿(C)
∬
= 𝜋(𝜽′,p′)𝛼(𝜽′,p′;𝜽,p)) d(𝜽′,p′)
(cid:101)
Leap−𝐿(B)∩C
=P(A,(𝜽𝑘,p𝑘) ∈ C,(𝜽𝑘+1,p𝑘+1) ∈ B),
𝐿
where on both intermediate lines we have used that Leap is self inverse
−
andthepenultimatelineusesthattheJacobianof (𝜽,p) → (𝜽′,p′) is1.
ScalingofHMCwithDimension
Givenaparticularintegrationtime,𝑇,thesmallerthestepsize,𝜖,themore
accurate the leapfrog scheme, and the closer the acceptance rate is to 1.
Atthesametime,thecomputationalcostisproportionaltothenumberof
leapfrog steps, 𝐿 = ⌈𝑇/𝜖⌉. Alarge 𝜖 leads tomany rejections anda small
𝜖 leads to a high computational cost, suggesting that there is an optimal
choiceof𝜖 betweenthesetwoextremes.
Inthisanalysis,weconsiderageneralproducttarget,𝜋(𝜽) =(cid:206) 𝑖𝑑
=1
𝑓(𝜃 𝑖),
andassumeanidentitymassmatrix,M = I𝑑.Inthiscase,theevolutionof
each (𝜃 𝑖,𝑝 𝑖) by Leap −𝐿 does not depend on any of the other components.
TheacceptanceratioforHMCis
𝜌(𝜽,p;𝜽′,p′) =
(cid:101)𝜋(𝜽′,p′) =(cid:214)𝑑
𝜌(𝑖)
𝜋(𝜽,p) 1
(cid:101) 𝑖=1
where (𝜽′,p′) =Leap𝐿(𝜽,p),adeterministicfunction,and
−
𝑓(𝜃′)N(𝑝′;0,1)
𝜌(𝑖) = 𝑖 𝑖 .
1 𝑓(𝜃 𝑖)N(𝑝 𝑖;0,1)
𝐿
At stationarity, after cancellations, and using the unit Jacobian of Leap ,
−
wehave
(cid:104) (cid:105) ∫
E
𝜃 𝑖∼𝑓,𝑝 𝑖∼N(0,1)
𝜌 1(𝑖) = 𝑓(𝜃 𝑖′)N(𝑝 𝑖′;0,1) d𝜃 𝑖d𝑝
𝑖
∫
= 𝑓(𝜃′)N(𝑝′;0,1) d𝜃′d𝑝′ =1. (2.10)
𝑖 𝑖 𝑖 𝑖
Moreover,the𝜌(𝑖)
arei.i.d.,so,bythecentrallimittheorem,approximately,
1
𝑑
∑︁
log𝜌 = log𝜌
𝑖
∼N(𝑑E[log𝜌 1],𝑑Var[log𝜌 1]),
𝑖=162 ReversibleMCMCanditsScaling
fromwhichweseethat𝜌hasapproximatelyalognormaldistribution.From
(2.10), and the component-wise independence, E[𝜌] = 1, so E[log𝜌] =
−1Var[log𝜌]and,hence,E[log𝜌 ] ≈−1Var[log𝜌 ].Thisgivesthesame
2 1 2 1
distributionforthelog-acceptanceratioaswefoundfortheRWMin(2.3)
withthesamescalingoftheexpectationandvariancewithdimensionif𝜆
(fortheRWM)or𝜖 (forHMC)iskeptfixed.FortheRWM,thisnecessitated
taking𝜆2 ∝ 1/𝑑;however,forHamiltoniandynamicsapproximatedbythe
leapfrog integrator with step size 𝜖 over a time period𝑇, the error in the
totalenergyis𝑂(𝜖2);i.e.,E[|log𝜌 |] =𝑂(𝜖2).Thus
1
1
Var[log𝜌 ]+ Var[log𝜌 ]2
1 4 1
=Var[log𝜌 ]+E[log𝜌 ]2 =E(cid:2) (log𝜌 )2(cid:3) =𝑂(𝜖4).
1 1 1
Setting 𝜖 = 𝑂(𝑑−1/4) gives Var[log𝜌 ] = 𝑂(1/𝑑), so both E[log𝜌] and
1
Var[log𝜌]are𝑂(1),asrequiredfortheacceptanceratiotobewell-behaved.
Taking 𝜖 ∝ 𝑑−1/4, and 𝑇 fixed as dimension increases, implies that for a
given amount of movement in each component, the number of leapfrog
steps, and hence the computational cost, increases in proportion to 𝑑1/4.
Contrasting this with a cost of 𝑑1/3 for MALA and 𝑑 for the RWM shows
whyHMCisoftenthealgorithmofchoiceforhigh-dimensionaltargets.
TuningHMC
Afteramorerigorousscalinganalysisthanourheuristicexplanation,Beskos
etal.(2013)concludesthatgiven𝑇,inthehigh-dimensionallimit,𝜖 should
bechosensothattheacceptancerateisaround65%;thislimitisapproached
slowly,however,andinpractice,itisoftenfoundthatahigheracceptance
rateisoptimal.
ThemaindifficultywithtuningHMCisinchoosingtheintegrationtime,
𝑇.Forexample,foraN(0,𝜎2)target,𝜋,usingamomentumof𝑝 ∼N(0,1),
itisstraightforwardtoshowthatif𝜃 ∼ 𝜋thenunderthetrueHamiltonian
0
dynamics,Cor[𝜃 0,𝜃 𝑇] =cos(𝑇/𝜎).IfthetargetisaproductofGaussians,
each with a different length scale, then the auto-correlations between the
currentvaluesandtheproposalsforeachcoordinatehavedifferentperiods.
The periodicity means that increasing𝑇 does not monotonically decrease
theauto-correlationandthedifferentperiodsmeanthattheminimumcorre-
lationoverallcomponents,whichupperboundstheminimumofthelag-1
auto-correlations,isanerraticfunctionof𝑇.Hence,theoverallefficiency
can,andoftendoes,behaveerraticallyas𝑇 isvaried.Thisisillustratedin
Figure2.6wheretheoptimalchoiceof𝑇isaround8–9,butslightdeviations
fromthisrangeleadtosubstantialdecreasesinefficiency.2.3 ChapterNotes 63
θ N(0,i2),i=1, ,5
i∼
    
    
    
    
    
    
    
    
    
                  
T
Figure2.6 Cor(𝜃 0,𝜃 𝑇)against𝑇 forall5componentsof𝜽 when
𝜋(𝜽) ∝exp[−1(cid:205)5 𝜃2/𝑖2],𝜽 ∼𝜋andp∼N(0,I )(non-solid
2 𝑖=1 𝑖 0 5
lines).Thethicksolidlineisthepointwisemaximumover
components.
2.3 ChapterNotes
This chapter has only touched the surface on many established aspects
of MCMC and variations on the Metropolis–Hastings algorithm of Hast-
ings(1970).ThefirstMCMCalgorithmwastherandom-walkMetropolis-
within-GibbsalgorithmofMetropolisetal.(1953),whilsttheMALAwas
suggestedandstudiedinBesag(1994)andRobertsandTweedie(1996)and
HMC was proposed in Duane et al. (1987). There are many texts and re-
viewarticlesdevotedtoMarkovchainMonteCarlo,includingGeyer(1992),
RobertandCasella(1999),GamermanandLopes(2006)andBrooksetal.
(2011).
The first high-dimensional RWM scaling result showing a limiting dif-
fusion appears in Roberts et al. (1997) and applies to a product target,
𝜋(𝜽) ∝ (cid:206) 𝑖𝑑
=1
𝑓(𝜃 𝑖); this is extended to MALA in Roberts and Rosen-
thal (1998) and, for both the RWM and MALA, to targets of the form
(cid:206) 𝑖𝑑 =1𝐶 𝑖𝑓(𝐶 𝑖𝜃 𝑖) in Roberts and Rosenthal (2001). Sherlock and Roberts
(2009) tackles the RWM on spherical and elliptical targets, showing that
insomesituationstheoptimalacceptanceratecanbelessthan0.234,and
)
θ,
θ(roC
T
064 ReversibleMCMCanditsScaling
Sherlocketal.(2015)extendstheanalysisforproducttargetstothepseudo-
marginalRWM.Ofthemanyotherscalingresultsforthesetwoalgorithms,
wehighlightthefollowing:Christensenetal.(2005)examinesthetransient
phasesofthealgorithms,Beskosetal.(2009)considersachangeofmea-
surefromaproductlawand,finally,Kamatani(2020)considerstheRWM
onsphericallysymmetricscale-mixturesofGaussiansandshowsthatwhen
the tails are heavier than exponential, although the optimal scaling is still
ℓ/𝑑, the norm of the process, ∥𝜽∥, mixes in a time of 𝑂(𝑑2) rather than
𝑂(𝑑),suggestingthattheRWMmaybetoocostlyonsome,morerealistic
heavy-tailedtargets.
Example 2.2 illustrated the poor behaviour of MALA in the tails of
targets with tails that are lighter than Gaussian. A simple solution is to
truncate the gradient term (Roberts and Tweedie, 1996). Livingstone and
Zanella(2022)providesanalternativeuseofgradientswithintheproposal
that leads to the same limiting behaviour as MALA, but is automatically
robusttoissueswithlight-tails.
A single iteration of HMC approximates Hamiltonian dynamics over a
finite time 𝑇, whatever the dimension. Thus, if, as in the earlier scaling
analysis, 𝑇 is kept fixed, there can be no limiting diffusion for a product
target. A similar scaling analysis to ours appears in Neal (2011), which
itselfisbasedonCreutz(1988);amorerigorousanalysisisgiveninBeskos
etal.(2013).
RecentworkshavemitigatedagainsttheerraticdependenceoftheHMC
efficiency on the integration time,𝑇. Techniques include introducing ran-
domnessintothelengthofthepath(Neal,2011;Bou-RabeeandSanz-Serna,
2017;Hoffmanetal.,2021),randomisingthechoiceofproposalpointfrom
those along the path (Hoffman and Gelman, 2014; Sherlock et al., 2023,
the former also automatically choosing 𝑇 at each iteration and the latter
adjusting𝑇 accordingtoanaturallengthscaleofthetarget)orbyjittering
themomentumaftereachleapfrogstep(Riou-DurandandVogrinc,2023).
FurthervariationsontheHMCalgorithmincludethetrulynon-reversible
Horowitz(1991),whichisdiscussedinmoredetailinSection4.3,andSohl-
Dickstein et al. (2014), which tries to mitigate one of the key issues with
Horowitz(1991).
AseparatestrandofmethodologicaldevelopmentsforreversibleMCMC
starts with position-dependent preconditioning of MALA and extends to
Riemann manifold Hamiltonian Monte Carlo (Girolami and Calderhead,
2011)andRiemannmanifoldMALA(Xifaraetal.,2014)whichitselffeeds
intotheStochasticGradientRiemannianLangevinDynamicsdescribedin
Section3.4.3
Stochastic Gradient MCMC Algorithms
Chapter2introducedMarkovchainMonteCarloalgorithmsasasimulation-
basedapproachtoapproximatedistributionsofinterest.Adrawbackofthe
algorithmsintroducedinChapter2isthattheircomputationaltimescales
poorlywithlargedatasets.Inthischapter,wewillexploreaclassofalgo-
rithmsthatcanbeviewedasapproximationsofthealgorithmsintroducedin
Chapter2.WeintroducethestochasticgradientLangevinalgorithm,andex-
tensionsofthisalgorithm,whicharepopularBayesianinferencemethodsin
thefieldofmachinelearning.ComparedtotraditionalMCMCalgorithms,
wewillnowreplacethegradientofthelogdensityofthetargetdistribution
withastochasticapproximation.Thisstochasticapproximationisgenerated
using a subsample of the full dataset to produce an approximate MCMC
algorithm. This class of stochastic gradient MCMC algorithms is com-
putationally faster than standard MCMC algorithms but at the expense of
introducingasmallasymptoticbiasthatcanbecorrectedpost-hoc.Through
this chapter, we will discuss the motivation behind these algorithms, and
theirextensions,andprovideempiricalcomparisonstotraditionalMCMC
algorithms.
3.1 TheUnadjustedLangevinAlgorithm
Recallthatweaimtosamplefromaposteriordistributionwithdensity𝜋(𝜽),
where for this chapter, 𝜽 is a 𝑑−dimensional vector in R𝑑. It is assumed
forthemethodswediscussinthischapterthatlog𝜋(𝜽) iscontinuousand
differentiable almost everywhere. Simulating a stochastic process that has
𝜋 as its stationary distribution is a well-established method for generating
samples approximately from 𝜋(𝜽). By sampling from such a process for
anextendedperiod,anddiscardingtheinitialburn-insamples,weobtaina
set of samples that approximate 𝜋(𝜽). The accuracy of the approximation
depends on how quickly the stochastic process converges to its stationary
distributionfromtheinitialpoint,relativetothelengthoftheburn-inperiod,
6566 StochasticGradientMCMCAlgorithms
aswellasonthetimeforthechaintomixwithinthestationarydistribution.
The Markov Chain Monte Carlo (MCMC; see Chapter 2) method is the
mostwidelyusedtechniqueforsamplinginthismanner
With𝑏 =1,theoverdampedLangevindiffusionfirstintroducedin(1.20)
ofSection1.4.3is
1
d𝜽𝑡 = ∇log𝜋(𝜽𝑡)d𝑡+d𝑊 𝑡, (3.1)
2
where 1∇log𝜋(𝜽𝑡) isthedrifttermand𝑊 𝑡 denotes 𝑑-dimensionalBrow-
2
nian motion. In this chapter we sometimes refer to the solution to this
stochastic differential equation (SDE) simply as the Langevin diffusion.
Under mild regularity conditions, the Langevin diffusion has 𝜋 as its sta-
tionarydistribution.AsdetailedinSection1.4and,inparticular(1.16),this
equationcanbeinterpretedasdefiningthedynamicsofaMarkovprocess
over infinitesimally small time intervals. That is, for a small time-interval
𝛿 > 0, the Langevin diffusion has a discrete-time analogue given by the
Euler–Maruyamaapproximation,
𝛿 √
𝜽𝑡+𝛿 = 𝜽𝑡 + ∇log𝜋(𝜽𝑡)+ 𝛿Z, 𝑡 ≥ 0 (3.2)
2
whereZisavectorof𝑑 independentstandardGaussianrandomvariables.
This discrete-time update equation is commonly known as the unadjusted
Langevin algorithm (ULA) or the Langevin Monte Carlo algorithm. The
discrete-timesequence{𝜽𝑡} 𝑡≥0generatedby(3.2)differsfromthesequence
producedbytheprocessin(3.1).Theupdateequationgivenin(3.2)provides
astraightforwardandpracticallyimplementablemethodforgeneratingap-
proximate samples from the overdamped Langevin diffusion. To generate
samplesoveraduration𝑇 = 𝑛𝛿,where𝑛isaninteger,webeginbysetting
theinitialstateoftheprocessto𝜽 ,andthenrepeatedlysimulatetheprocess
0
using(3.2)toobtainvaluesattimes𝛿,2𝛿,...,𝑛𝛿.Wewillusethenotation
𝜽𝑘 torefertothestateoftheprocessattime 𝑘𝛿.AswiththeMCMCalgo-
rithms discussed in Chapter 2, an estimate of any expectation is obtained
viaaMonteCarloaverage:E
𝜋
[ℎ(𝜽)] ≈ 𝑛1 (cid:205)𝑛 𝑘=1ℎ(𝜽𝑘).
To improve the accuracy of the Euler–Maruyama discretisation (3.2)
when sampling from the Langevin diffusion at a fixed time 𝑇, we can
decrease 𝛿. As 𝛿 becomes smaller, the discretisation error decreases and
the approximation becomes more accurate. In theory, we can achieve any
desired degree of accuracy in approximating the SDE (3.1) by selecting
𝛿 small enough. However, for a fixed𝑇, the computational cost increases
in proportion to 1/𝛿. Alternatively, given a fixed computational budget,
𝑇 decreases in proportion to 𝛿. The longer 𝑇 is, the more information3.2 Approximatevs.ExactMCMC 67
about the diffusion’s stationary distribution we collect, and hence, for a
fixed computation budget, the variance of any estimate from the samples
increasesasthebiasdecreases.Inpractice,therefore,thechoiceof𝛿requires
acompromisebetweenthebiasandthevarianceofourestimators.
3.2 Approximatevs.ExactMCMC
The overdamped Langevin diffusion has 𝜋 as its stationary distribution
and therefore it is natural to consider this stochastic process as the basis
for an MCMC algorithm. In fact, if it were possible to simulate exactly
the dynamics of the Langevin diffusion, then we could use the resulting
realisationsatasetofdiscretetimepointsasourMCMCoutput.However,
for general 𝜋(𝜽), the Langevin dynamics are intractable, and therefore it
is necessary to resort to using samples generated by the Euler–Maruyama
approximation(3.2).
This is most commonly seen with the Metropolis-adjusted Langevin
Algorithm (MALA) (see Section 2.1.4). This algorithm uses the Euler–
Maruyamaapproximation(3.2)overanappropriatelychosentime-interval,
𝛿, to define the proposal distribution of a standard Metropolis–Hastings
sampler (see Algorithm 1). Simulated values are then either accepted or
rejected based on the Metropolis–Hastings acceptance probability (2.1).
Such an algorithm has good theoretical properties, and in particular, can
scale better to high-dimensional problems than the simpler random walk
MCMCalgorithm.SeeSection2.1.4foramoredetaileddescriptionofthe
MALAalgorithmanditsdimensionalscaling.
AsimpleralgorithmisthejustdescribedunadjustedLangevinalgorithm
(3.2), which simulates from the Euler–Maruyama approximation of the
Langevin diffusion but does not use a Metropolis–Hastings accept-reject
step, and so the stationary distribution of the resulting Markov chain is
not 𝜋. Hence, even once the Markov chain has essentially converged, the
Monte Carlo samples are from an approximation to 𝜋 rather than from 𝜋
itself.Becauseofthis,estimatorsofexpectationsaretypicallybiased,even
as the number of samples, 𝑛, grows to infinity. Computationally, such an
algorithmisquickerperiteration,butoftenthissavingissmallasthecostof
calculating∇log𝜋(𝜽),whichisrequiredforonestepoftheULAalgorithm,
typicallyscalesatleastlinearlywiththedatasetsize.IftheMALAalgorithm
isoptimallytuned,thenapproximately40%ofthesampleswillberejected,
which leads to wasted computation compared to ULA where all samples,
albeitbiased,areaccepted.However,thisiscounteractedbythelargerstep
sizesthatarepossiblewithMALA.68 StochasticGradientMCMCAlgorithms
Example:SamplingfromaGaussianPosteriorDistribution
To illustrate the computational and statistical accuracy trade-offs between
the ULA and MALA schemes, we consider a simple bivariate Gaussian
posteriordistribution,whichweshalluseasarunningexamplethroughout
this chapter. We assume that data arise as realisations from a Gaussian
location model with mean parameter 𝜽 assumed to be unknown and the
varianceVisknown.WeselectaconjugateGaussianpriorfortheunknown
𝜽 whichleadstothegenerativemodel
y𝑗|𝜽 ∼N(𝜽,V), 𝜽 ∼N(0,I 2), for 𝑗 =1,...,𝑁, (3.3)
(cid:18) (cid:19)
1 0
where we set V = and I is a 2-dimensional identity matrix. For
0 10 2
thissimplemodel,itispossibletoderiveatractableposteriordistribution
𝜽|y∼N(𝝁 𝑁,𝚺 𝑁),where𝚺
𝑁
= (𝑁V−1+I 2)−1and𝝁
𝑁
= 𝚺 𝑁(V−1(cid:205)𝑁 𝑗=1y𝑗).
We can use both ULA and MALA schemes to sample from the posterior
distribution and compare the Monte Carlo accuracy of both algorithms
against the known ground-truth posterior distribution. We measure the
distributional accuracy between the true posterior 𝜋 and a Monte Carlo
approximation𝜋˜ usingtheWasserstein-2distance,
∫
d2 (𝜋,𝜋˜) = inf ∥𝜽 −𝜽′∥2d𝜁(𝜽,𝜽′), (3.4)
𝑊
2 𝜁∈Γ(𝜋,𝜋˜) R𝑑×R𝑑
wheretheinf istakenwithrespecttoalljointdistributions𝜁 whichhave𝜋
and𝜋ˆ astheirmarginaldistributions.Inthesettingwhereboth𝜋and𝜋ˆ are
Gaussian,thereisatractableclosed-formexpressionfortheWasserstein-2
distance,
d2
𝑊
2(N(𝝁 𝑎,𝚺 𝑎),N(𝝁 𝑏,𝚺 𝑏)) = ∥𝝁 𝑎−𝝁 𝑏∥2 2+trace(𝚺 𝑎+𝚺 𝑏−2(𝚺1 𝑎/2𝚺 𝑏𝚺1 𝑎/2)1/2).
In Figure 3.1, we calculated the approximate Wasserstein-2 distance
betweenthetrueposteriorandamoment-matchedGaussianapproximation
to the Monte Carlo samples generated by the ULA/MALA algorithms.
We simulated 𝑁 = 1000 (left panel) and 𝑁 = 10000 (right panel) data
pointsfromthemodel(3.3)andranULA/MALAfor 𝑛 = 1000iterations.
For each 𝑁, MALA and ULA used the same step size, 𝛿 = 1/𝑁. Since
the true posterior is Gaussian, we expect the moment-matched Gaussian
approximation for the MALA sampler to get more and more accurate as
the number of iterations increases. Since the ULA algorithm update is a
conditionalGaussian,thestationarydistributionforULAisalsoGaussian,
sothemoment-matchedGaussianapproximationtothiswillalsogetmore
andmoreaccurateasthenumberofiterationsincreases.3.3 StochasticGradientLangevinDynamics 69
ComparingthecomputationaltimeforULAandMALA,theperiteration
costofULAiscomparabletoMALAinitially,ifnotslightlybetter.However,
withalargercomputationalbudget,i.e.moreMonteCarloiterations,ULA
is less accurate due to the asymptotic bias from discretising the Langevin
diffusion.WhentakingintoaccountthereducedcomputationalcostofULA,
thismeansthatULAisbetterforsmallcomputationalbudgets,whereasfor
moderate to large computational budgets, MALA is better. Note that the
computational budget required for MALA to display improved statistical
efficiencyoverULAisdependentonthedatasetsize.Thisishighlightedin
Figure 3.1, where 𝑁 = 1000 in the left panel and 𝑁 = 10000 in the right
panel.
The reason that ULA is competitive with MALA only for very small
computational budgets is that the computational gain per iteration is only
roughlytwo-fold(i.e.notcalculatingtheaccept-rejectratioroughlyhalves
thecostasthegradientstillneedstobecalculated),andthisisonlyasmall
gainrelativetothebiasthatisintroduced.If,ontheotherhand,therewas
a way of implementing ULA, or something like ULA, which was 𝑂(𝑁)
faster,thenthecomputationalbenefitcomparedtoMALAwouldbemore
significant. To achieve such a speed-up, this would require an algorithm
where the cost of calculating or approximating the gradient is only 𝑂(1)
– this is the key idea behind the stochastic gradient Langevin dynamics
algorithmwhichwillbeexploredindetailintheremainderofthischapter.
3.3 StochasticGradientLangevinDynamics
WehaveseenhowtheULAalgorithmiscomputationallyfasterthanMALA,
attheexpenseofintroducingabiaswhichproducessamplesnotfromthe
desiredinvariantdistribution𝜋,butadistributioncloseto𝜋.Evenwithout
the Metropolis-Hastings acceptance probability, the ULA algorithm still
incurs a cost in calculating the gradient of the log-posterior density and
under common assumptions, this computational cost scales linearly with
thedatasetsize.
Recent interest in Bayesian analysis has considered the challenge of
scalableinferenceinthepresenceoflargedatasets,wherethelog-posterior
density is defined as a sum over data points. For instance, if we consider
data y 1,...,y𝑁 that are conditionally independent given 𝜽, then 𝜋(𝜽) ∝
𝜋 0(𝜽)(cid:206)𝑁
𝑗=1
𝑓(y𝑗|𝜽). Here, 𝜋 0(𝜽) is the prior density, and 𝑓(y𝑗|𝜽) is the
likelihoodforthe 𝑗thobservation.Inthiscontext,wedefinethelog-posterior
densityas70 StochasticGradientMCMCAlgorithms
ULA ULA
0.8
MALA MALA
0.8
0.7
0.7
0.6
0.6 0.5
0.4
0.5
0.3
0 2 4 6 0 2 4 6 8
Computational time Computational time
Figure3.1 Wassersteindistancebetweenthetrue𝜋and
approximateposteriordistributionagainstcomputationaltime(in
seconds),wheretheapproximateposterior𝜋˜ isgeneratedusing
theULAandMALAschemes.Leftpanel𝑁 =1000andright
panel𝑁 =10000.
𝑁
∑︁ 1
log𝜋(𝜽) = log𝜋 𝑗(𝜽), where log𝜋 𝑗(𝜽) =log 𝑓(y𝑗|𝜽)+
𝑁
log𝜋 0(𝜽).
𝑗=1
(3.5)
ThecomputationalbottleneckforULAisincalculating∇log𝜋(𝜽),which
canbeexpensiveifwehavealargedataset.Foradatasetwith𝑁independent
observations,wherethelog-posteriordensityisasumover 𝑁 independent
terms(3.5),thecomputationalcostofevaluatingthelog-posteriordensity,
oritsgradient,is𝑂(𝑁) foreachiterationofULA.
AsolutiontothisproblemistousestochasticgradientLangevindynamics
(SGLD,WellingandTeh,2011),whichavoidscalculating∇log𝜋(𝜽),and
instead uses an unbiased estimator of the gradient at each iteration. It is
trivial to obtain an unbiased estimate using a random subsample of the
terms in the sum. The simplest implementation is to choose 𝑚 ≪ 𝑁 and
estimate∇log𝜋(𝜽) with
𝑁 ∑︁
∇(𝑚)log𝜋(𝜽) =
𝑚
∇log𝜋 𝑗(𝜽), (3.6)
𝑗∈S𝑚
where S 𝑚 is a random sample of size 𝑚 taken without replacement from
{1,...,𝑁}. We call this the simple estimator of the gradient and use the
ecnatsid
nietsressaW
ecnatsid
nietsressaW3.3 StochasticGradientLangevinDynamics 71
superscript (𝑚) todenotethesubsamplesizeusedinconstructingouresti-
mator.TheresultingSGLDalgorithmisgiveninAlgorithm3.Essentially,
theSGLDalgorithmisthesameasULA(3.2)andissimulatinganEuler–
MaruyamadiscretisationoftheLangevindiffusion.Theonlydifferenceis
thatthetruegradientisreplacedwiththeestimatedgradient(3.6).
Usinganestimatorforthegradientaddsadditionalnoise,withavariance
of𝑂(𝛿2),andthereforethestochasticdynamicsnolongerfollowtheULA
update equation (3.2); instead the SGLD algorithm targets a distribution
that is close to a tempered version of 𝜋. However, for sufficiently small
𝛿,thisadditionalvariancebecomesnegligiblecomparedwiththeinjected
Gaussiannoiseof(3.2),whichhasavarianceof𝛿.Itispossibletogeneralise
Algorithm 3 to the setting of adaptive step sizes 𝛿 𝑘 which are dependent
on the iteration number 𝑘. This is not commonly used in practice and for
simplicity,weworkwiththeconstantstepsizeversiongiveninAlgorithm
3. A justification for using the SGLD algorithm with a decaying step size
canbegivenbyaninformalargumentalongthelinesthatifthestepsizeis
𝛿 𝑘 ↓ 0 for 𝑘 → ∞, then the process will converge to the true overdamped
Langevindiffusion,andhencetheMonteCarlosampleswillbeexactinthe
limit(seeSection3.3.3).
Algorithm3:StochasticGradientLangevinDynamics(SGLD)
Input:𝜽 ,𝛿.
0
for 𝑘 ∈ 1,...,𝑛do
DrawasubsetS
𝑚
⊂ {1,...,𝑁}
Estimate∇(𝑚)log𝜋(𝜽) using(3.6)
DrawZ𝑘 ∼N(0,𝛿I)
Update𝜽𝑘+1 ← 𝜽𝑘 + 2𝛿∇(𝑚)log𝜋(𝜽)+Z𝑘
end
TheadvantageofSGLDisthat,ifthesubsamplesize𝑚ismuchsmaller
than the full dataset size 𝑁, the per-iteration cost of the algorithm can be
muchsmallerthanthatofeitherMALAorULA.Forlargedataapplications,
SGLDhasbeenempiricallyshowntoperformbetterthanstandardMCMC
when there is a fixed computational budget (Ahn et al., 2015; Li et al.,
2016).Inchallengingexamples,performancehasbeenbasedonmeasures
of predictive accuracy on a held-out test dataset, rather than based on
how accurately the samples approximate the true posterior distribution.
Furthermore,theconclusionsfromsuchstudieswillclearlydependonthe
computationalbudget,withlargerbudgetsfavouringexactmethodssuchas72 StochasticGradientMCMCAlgorithms
MALA;seethetheoreticalresultsinSection3.3.3andempiricalresultsin
Section3.2.
The SGLD algorithm is closely related to stochastic gradient descent
(SGD) (Robbins and Monro, 1951), an efficient algorithm for finding the
local maxima of a function. The only difference is the inclusion of the
additive Gaussian noise at each iteration of SGLD. Without the noise,
but with a suitably decreasing step size, SGD would converge to a local
maximum of the density 𝜋(𝜽). Again, SGLD has been shown empirically
to out-perform stochastic gradient descent (Chen et al., 2014), at least in
termsofpredictiveaccuracy–intuitivelythisisbecauseSGLDwillproduce
samplesfromtheregionaroundtheestimateobtainedbySGD,andthuscan
average over the uncertainty in the parameters. This strong link between
SGLD and SGD may also explain why the former performs well when
comparedtoexactMCMCmethods,atleastintermsofpredictiveaccuracy.
3.3.1 ControllingStochasticityintheGradientEstimator
ThekeyingredientofSGLDisfoundinreplacingthetruegradientwithan
unbiasedestimator.Themoreaccuratethisestimatoris,thelowerthebias
will be for the same computational cost, and thus it is natural to consider
alternativestothesimpleestimator(3.6).Onewayofreducingthevariance
ofaMonteCarloestimatoristousecontrolvariates(seeSection1.1.4fora
detailedexplanation),whichinoursettinginvolveschoosingasetofsimple
functions𝑔 𝑗, 𝑗 =1,...,𝑁,whichwerefertoascontrolvariates,andwhose
sum(cid:205)𝑁 𝑗=1𝑔 𝑗(𝜽) canbeevaluatedforany𝜽.
Wecanrewritethefull-datagradientofthelog-posteriordensityas
𝑁 𝑁 𝑁
∑︁ ∇log𝜋 𝑗(𝜽) =∑︁ 𝑔 𝑗(𝜽)+∑︁(cid:0) ∇log𝜋 𝑗(𝜽)−𝑔 𝑗(𝜽)(cid:1),
𝑗=1 𝑗=1 𝑗=1
andfromthis,wecanobtainanunbiasedestimator
∑︁𝑁 𝑁 ∑︁
𝑔 𝑗(𝜽)+
𝑚
(∇log𝜋 𝑗(𝜽)−𝑔 𝑗(𝜽)), (3.7)
𝑗=1 𝑗∈S𝑚
where again S 𝑚 is a random sample of size 𝑚 drawn from {1,...,𝑁}.
Theintuitionbehindthisideaisthatifeach𝑔 𝑗(𝜽) ≈ ∇log𝜋 𝑗(𝜽),thenthis
estimator can have a much smaller variance than the simple subsampled
gradientestimator(3.6).
Oneapproachtochoosingthecontrolvariatefunction𝑔 𝑗(𝜽)thatisoften
usedinpractice,isto(i)useSGDtofindanapproximation,(cid:98)𝜽,tothemode3.3 StochasticGradientLangevinDynamics 73
of the distribution 𝜋; and (ii) set 𝑔 𝑗(𝜽) = ∇log𝜋 𝑗((cid:98)𝜽). This leads to the
followingcontrolvariateestimator,
∑︁𝑁 𝑁 ∑︁ (cid:16) (cid:17)
∇(𝑚)log𝜋 cv(𝜽) = ∇log𝜋 𝑗((cid:98)𝜽)+
𝑚
∇log𝜋 𝑗(𝜽)−∇log𝜋 𝑗((cid:98)𝜽) .
𝑗=1 𝑗∈S𝑚
(3.8)
Implementingsuchanestimatorinvolvesaninitialup-frontcostforfinding
a suitable (cid:98)𝜽 and then calculating, storing, and summing ∇log𝜋 𝑗((cid:98)𝜽) for
𝑗 = 1,...,𝑁.Forthesetypesofcontrolvariateapproaches,themaincost
isfromfindingasuitable(cid:98)𝜽.Although,oncefound,wecanthenuse(cid:98)𝜽 asa
startingvaluefortheSGLDalgorithm,replacing𝜽 0with(cid:98)𝜽 inAlgorithm3,
whichcansignificantlyreducetheburn-inphase.
The advantage of using the control variate-based estimator can be seen
ifwecomparethevarianceboundsofthisestimatoragainstthesimpleesti-
mator.Ifweassumethateachlog𝜋 𝑗(𝜽)istwicecontinuouslydifferentiable
on R𝑑 and has Lipschitz-continuous gradients, then there exist positive
constants 𝐿 𝑗 > 0forall 𝑗 =1,...,𝑁,suchthat
∥∇log𝜋 𝑗(𝜽)−∇log𝜋 𝑗(𝜽′)∥ ≤ 𝐿 𝑗∥𝜽 −𝜽′∥. (3.9)
Lemma 3.1 and several subsequent results provide bounds on the trace
ofthevariancematrixofeachestimatorof∇log𝜋(𝜽).Sincevariancesare
non-negative,thisboundsthevariancesofeachindividualcomponent.Also,
since all eigenvalues of a variance matrix are non-negative, it bounds the
largest of these; i.e. the variance of the worst-behaved linear combination
ofcomponents.Finally,forany𝑑-vectorrandomvariable𝝃 withE[𝝃] =0,
(cid:34) 𝑑 (cid:35)
tr(Var[𝝃]) =E ∑︁ 𝜉2 =E(cid:2) ∥𝝃∥2(cid:3) ≥ Var[∥𝝃∥],
𝑖
𝑖=1
so the bounds also apply to the variance of the Euclidean norm of the
gradient.Foranyrandomvector𝝃withE[𝝃] =0,werefertotheimportant
quantityofE(cid:2) ∥𝝃∥2(cid:3)
=tr(Var[𝝃]) asitspseudo-variance.
Lemma3.1 Assumecondition(3.9),thenthereareconstants𝐶 ,𝐶 > 0
1 2
where the pseudo variances of the simple gradient estimator (3.6) and
controlvariate-basedgradientestimator(3.8)havethefollowingbounds:
(cid:16) (cid:17) 𝑁2
tr Var(cid:2) ∇(𝑚)log𝜋(𝜽)(cid:3) ≤ 𝐶 , (3.10)
1 𝑚
(cid:16) (cid:17) 𝑁2
tr Var(cid:2) ∇(𝑚)log𝜋 cv(𝜽)(cid:3) ≤ 𝐶 2∥𝜽 −(cid:98)𝜽∥2
𝑚
, (3.11)74 StochasticGradientMCMCAlgorithms
Proof Weprovethisresultforthecontrolvariate-basedgradientestimators
(3.8);theresultforthesimpleSGLDestimator(3.6)followsanalogously.
Wefirstdefine𝝃 := ∇(𝑚)log𝜋 (𝜽)−∇log𝜋(𝜽),sothat𝝃 measuresthe
cv
noiseinthegradientestimateandhasmeanzero.Thetraceofthevariance
inthenoiseisthengivenby
E(cid:2) ∥𝝃∥2(cid:3) =E(cid:104)(cid:13) (cid:13)∇(𝑚)log𝜋 cv(𝜽)−∇log𝜋(𝜽)(cid:13) (cid:13)2(cid:105)
=E   (cid:13) (cid:13) (cid:13) (cid:13)𝑚𝑁 ∑︁ (cid:16) ∇log𝜋 𝑗(𝜽)−∇log𝜋 𝑗((cid:98)𝜽)(cid:17) − (cid:16) ∇log𝜋(𝜽)−∇log𝜋((cid:98)𝜽)(cid:17)(cid:13) (cid:13) (cid:13) (cid:13)2  

(cid:13) 𝑗∈S𝑚 (cid:13) 
 
=E   (cid:13) (cid:13) (cid:13) (cid:13)𝑚𝑁 ∑︁ (cid:26) (cid:16) ∇log𝜋 𝑗(𝜽)−∇log𝜋 𝑗((cid:98)𝜽)(cid:17) − 𝑁1 (cid:16) ∇log𝜋(𝜽)−∇log𝜋((cid:98)𝜽)(cid:17)(cid:27)(cid:13) (cid:13) (cid:13) (cid:13)2  

(cid:13) 𝑗∈S𝑚 (cid:13) 
 
≤ 𝑚𝑁 22 E(cid:34) ∑︁ (cid:13) (cid:13) (cid:13) (cid:13)(cid:16) ∇log𝜋 𝑗(𝜽)−∇log𝜋 𝑗((cid:98)𝜽)(cid:17) − 𝑁1 (cid:16) ∇log𝜋(𝜽)−∇log𝜋((cid:98)𝜽)(cid:17)(cid:13) (cid:13) (cid:13) (cid:13)2(cid:35) .
𝑗∈S𝑚
where the final line follows from the triangle inequality and due to inde-
pendencebetweenthe∇log𝜋 𝑗(𝜽)termsinthesettingofsubsamplingwith
replacement. For subsampling without replacement, the sampled indices
willbenegativelycorrelatedandthuswillleadtolowervariance.Forany
random variable X, we have
E(cid:2) ∥X−E[X]∥2(cid:3)
≤
E(cid:2) ∥X∥2(cid:3)
. Using this
result,andtheLipschitzassumption(3.9),leadsto
E(cid:2) ∥𝝃∥2(cid:3) ≤ 𝑚𝑁 22 ∑︁ E(cid:20)(cid:13) (cid:13) (cid:13)∇log𝜋 𝑗(𝜽)−∇log𝜋 𝑗((cid:98)𝜽)(cid:13) (cid:13) (cid:13)2(cid:21)
𝑗∈S𝑚
𝑁2 (cid:20)(cid:13) (cid:13)2(cid:21)
≤
𝑚
E (cid:13) (cid:13)∇log𝜋 𝑗(𝜽)−∇log𝜋 𝑗((cid:98)𝜽)(cid:13)
(cid:13)
𝑁2 1 ∑︁𝑁 (cid:16) (cid:13) (cid:13)(cid:17)2 𝑁2 (cid:13) (cid:13)2 1 (cid:32) ∑︁𝑁 (cid:33)
≤
𝑚 𝑁
𝐿 𝑗(cid:13) (cid:13)𝜽 −(cid:98)𝜽(cid:13)
(cid:13)
=
𝑚
(cid:13) (cid:13)𝜽 −(cid:98)𝜽(cid:13)
(cid:13) 𝑁
𝐿2
𝑗
,
𝑗=1 𝑗=1
where the second line follows from the exchangeability of the indices 𝑗
and on that line, 𝑗, is a single index sampled uniformly at random from
1,...,𝑁.Thisprovesthestatedresultandgives𝐶 = 1 (cid:205)𝑁 𝐿2. □
2 𝑁 𝑗=1 𝑗
Ifwemakethefurtherassumptionthatforall 𝑗 = 1,...,𝑁 thereexists
apositiveconstant 𝐿 > 0suchthat
∥∇log𝜋 𝑗(𝜽)−∇log𝜋 𝑗(𝜽′)∥ ≤ 𝐿∥𝜽 −𝜽′∥ (3.12)
holds.Then,itisstraightforwardtoshowthatwehavethefollowingLips-3.3 StochasticGradientLangevinDynamics 75
chitzboundonthegradientofthelog-posterior,
∥∇log𝜋(𝜽)−∇log𝜋(𝜽′)∥ ≤ 𝐿𝑁∥𝜽 −𝜽′∥, (3.13)
whichnowleadstoanupdatedconstant𝐶 = 𝐿2 in(3.11)ofLemma3.1.
2
By comparing the upper bounds on the variance of the gradients in
(3.10)and(3.11),wecanseethatwhen𝜽 iscloseto(cid:98)𝜽 wewouldexpectthe
variance in the control variate estimator to be smaller than for the simple
estimator. Furthermore, in many big data settings where 𝑁 is large, we
would expect by the Bernstein–von Mises theorem (e.g. LeCam, 1986)
that a value of 𝜽 drawn from the posterior distribution to be of distance
𝑂(𝑁−1/2)fromthemodeofthedistribution(cid:98)𝜽,i.e.weexpect∥𝜽−(cid:98)𝜽∥2tobe
𝑂(𝑁−1). Therefore, compared to the 𝑂(𝑁2/𝑚) variance from the simple
estimator(3.10),wewouldexpecttoseeareduced𝑂(𝑁/𝑚)variance(3.11)
for the control variate gradient estimator. This simple argument suggests
that,forthesamelevelofaccuracy,wecanreducethecomputationalcost
ofSGLDby𝑂(𝑁)ifweusecontrolvariate-basedgradientestimators.This
issupportedbyanumberoftheoreticalresultswhichshowthatifweignore
the pre-processing cost of finding(cid:98)𝜽, the computational cost per effective
sample of SGLD with control variates is𝑂(1), rather than the𝑂(𝑁) cost
forSGLDwiththesimplegradientestimator(3.6).
Afurtherconsequenceoftheseboundsonthevarianceisthattheysug-
gest that if 𝜽 is far from(cid:98)𝜽, then the variance when using control variates
canbelarger,potentiallysubstantiallylargerthanthatofthesimpleestima-
tor. This point is illustrated in the top-left panel of Figure 3.2, where the
variance in the simple gradient estimator (3.6) is approximately constant
for all 𝜽. However, the variance in the control variate gradient estimator
increases as 𝜽 moves away from (cid:98)𝜽. Two natural ways of addressing this
issue have been proposed in the literature. One option is to only use the
control variate estimator when 𝜽 is close enough to (cid:98)𝜽 (Fearnhead et al.,
2018),thoughitisuptotheusertodefinewhatis“closeenough”inprac-
tice. The second approach, a Langevin interpretation of the optimisation
algorithmSAGA,whichwasproposedinDubeyetal.(2016),istoupdate
(cid:98)𝜽whilstrunningtheSGLDalgorithm.Thiscanbedoneefficientlybyusing
𝑔 𝑗(𝜽) = ∇log𝜋 𝑗(𝜽𝑘 ),where𝜽𝑘 isthevalueof𝜽 atthemostrecentitera-
𝑗 𝑗
tionoftheSGLDalgorithmwhere∇log𝜋 𝑗(𝜽)wasevaluated.Thisinvolves
updatingthestorageof𝑔 𝑗(𝜽) anditssumateachiteration;importantlythe
lattercanbedonewithan𝑂(𝑚) cost.
An alternative approach to reducing the variance, for both the simple
andcontrolvariate-basedgradientestimators,istousenon-uniformlygen-
eratedsubsampleswithinthegradientestimator.Thisapproach,introduced76 StochasticGradientMCMCAlgorithms
in Putcha et al. (2023) and known as preferential subsampling, generates
subsamplesS 𝑚 ⊂ {1,...,𝑁}ofsize𝑚 withreplacement,wheretheprob-
abilityofdrawingthe 𝑗thdatapointis𝑝 𝑗 andtheexpectednumberoftimes
thatthe 𝑗thdatapointappearsinthesubsampleis𝑚𝑝 𝑗.Thisthenleadsto
anewsimple,asopposedtoCV-based,unbiasedgradientestimator
∇(𝑚)log𝜋 (𝜽) =
1 ∑︁ ∇log𝜋 𝑗(𝜽)
, (3.14)
ps 𝑚 𝑝
𝑗
𝑗∈S𝑚
where 𝑝 𝑗 > 0 for all 𝑗 = 1,...,𝑁 and (cid:205)𝑁 𝑗=1 𝑝 𝑗 = 1. The simple gradient
estimator(3.6)isgivenasaspecialcasewhen 𝑝 𝑗 =1/𝑁 forall 𝑗.
Ifwedefinetheerrorinthestochasticgradientas𝝃 = ∇(𝑚)log𝜋 (𝜽)−
ps
∇log𝜋(𝜽), then taking expectations over the random subsample, whose
distributiondependsontheweightsp= (𝑝 1,...,𝑝 𝑁),leadstothepseudo-
varianceof∇(𝑚)log𝜋 (𝜽),
ps
(cid:16) (cid:17)
E(cid:2) ∥𝝃∥2(cid:3) =tr Var[∇(𝑚)log𝜋 (𝜽)] .
ps
Lemma3.2 Theoptimalweightsp∗ whichminimisethepseudo-variance
(cid:16) (cid:17)
min tr Var[∇(𝑚)log𝜋 (𝜽)]
p:𝑝 𝑗∈[0,1],(cid:205) 𝑗𝑝 𝑗=1 ps
areequivalentlyfoundbyminimising
𝑁
min∑︁ 𝑝1 (cid:13) (cid:13)∇log𝜋 𝑗(𝜽)(cid:13) (cid:13)2,
p 𝑗
𝑗=1
resultinginoptimalweightsoftheform
𝑝∗ =
∥∇log𝜋 𝑗(𝜽)∥
for 𝑗 =1,...,𝑁. (3.15)
𝑗 (cid:205) 𝑖𝑁 =1∥∇log𝜋 𝑖(𝜽)∥
Proof A full proof of this result is given in Putcha et al. (2023). In
brief, this result follows from two steps. Firstly, a straightforward expan-
sionoftr(cid:0) Var[∇(𝑚)log𝜋 (𝜽)](cid:1) withthegradientestimatorgivenin(3.14)
ps
leads to 𝑚1 (cid:205)𝑁
𝑗=1
𝑝1 𝑗(cid:13) (cid:13)∇log𝜋 𝑗(𝜽)(cid:13) (cid:13)2 + 𝐶, where 𝐶 > 0 is a constant that
is independent of p. Minimising this term with respect to the constraint
{p: 𝑝
𝑗
∈ [0,1],(cid:205)
𝑗
𝑝
𝑗
=1}followsbyusingLagrangemultipliers.
□
Inpractice,theoptimalweights 𝑝∗,whichwouldminimisethegradient
𝑗
variance, are dependent on the current iterate 𝜽𝑘 of the SGLD algorithm.
This means that for each iteration 𝑘 in Algorithm 3, every 𝑝 𝑗 for all 𝑗 =3.3 StochasticGradientLangevinDynamics 77
1,...,𝑁 would need to be updated, which is an 𝑂(𝑁) calculation and
woulddefeatthepurposeofusingsubsamplesofsize𝑚 ≪ 𝑁 intheSGLD
algorithm. We could instead replace the optimal weights 𝑝∗ (3.15) with
𝑗
approximateweights 𝑝 (cid:98)𝑗,wherein(3.15)wereplace 𝜽 withanestimateof
the posterior mode(cid:98)𝜽. As discussed in the case of control variate gradient
estimators,finding(cid:98)𝜽 requiresaone-offpre-processingcost.
Weighted sampling can be combined with the control variate estimator
(3.7) with a natural choice of weights that are increasing with the size of
the derivative of ∇log𝜋 𝑗(𝜽) at(cid:98)𝜽. We can also use stratified sampling to
try to ensure each subsample is representative of the full data. However,
regardlessofthechoiceofgradientestimator,animportantquestionis:how
large should the subsample size 𝑚 be? Taking one iteration of SGLD, the
varianceofthenoisefromthegradienttermisdominatedbythevariance
of the injected noise. As the former is proportional to 𝛿2, and the latter to
𝛿,𝑚 willbe𝑂(1) as 𝑁 increasesifwechoose𝛿 = 𝑂(1/𝑁) (thesquareof
thetargetsize).ThechoiceofstepsizeisdiscussedfurtherinSection3.5.
Subsample size could also be dynamically adjusted whilst running the
SGLDalgorithm.Theidea,whichisparticularlyrelevantwhenusingcon-
trol variates, is that the accuracy of the estimator of the gradient can vary
considerably with 𝜽. To counteract this, it may be more efficient to have
a larger subsample size when the variance would be larger. One simple
approachistospecifyanupperbound,say𝑉,onthevariancethatwewould
liketoachieve,andconsiderhowtovarysubsamplesizetoachievethis.
An extension of the result in Lemma 3.1 can be derived for weighted
gradientsifthecontrolvariategradientestimator(3.7)inLemma3.1isre-
placedwiththepreferentialsamplinggradientestimator(3.14).Thisresults
inanewupperboundforthevarianceofthegradientestimator,
tr(cid:16) Var(cid:2) ∇(𝑚)log𝜋 cv(𝜽)(cid:3)(cid:17) ≤ 𝑚1 ∥𝜽 −(cid:98)𝜽∥2∑︁𝑁 𝐿 𝑝2 𝑗, (3.16)
𝑗
𝑖=1
where 𝑝 𝑗 are subsample weights (3.15) and 𝐿 𝑗 are Lipschitz constants on
thegradientcomponents(3.9).Asimilarresultcanbederivedforthesimple
gradientestimator(3.6).AswiththecontrolvariateboundinLemma3.1,
theboundin(3.16)isalso𝑂(𝑁).The𝑂(1/𝑁) term ∥𝜽−(cid:98)𝜽∥2 cancelswith
thesummationover 𝑁 terms.However,eachofthe 𝑁 termsis𝑂(𝑁) since
each𝑝 𝑗 is𝑂(1/𝑁).Choosingappropriateweights𝑝 𝑗 foreach 𝑗 =1,...,𝑁
reducesthemultiplierof 𝑁.
Givenourspecifiedupperbound𝑉 > 0,wecanplugthisinto(3.16).By
rearrangingtheinequalityandusingtheoptimalweights 𝑝∗ (3.15),wecan
𝑗78 StochasticGradientMCMCAlgorithms
showthatthesubsamplesizeshouldbeatleast
(cid:18) 𝑁 𝐿2(cid:19)
𝑚 > 1 ∥𝜽 −(cid:98)𝜽∥2 ∑︁ 𝑗 .
𝑉 𝑝∗
𝑖=1 𝑗
Forafixedbound𝑉,fixed 𝑁 andfixedweights 𝑝∗,theoptimalsubsample
𝑗
sizeis𝑚 ∝ ∥𝜽−(cid:98)𝜽∥2,whichsuggeststhatforSGLDwithcontrolvariates,the
subsamplesizeshouldincreaseataratewhichisquadraticinthedistance
between the current iterate of the SGLD chain 𝜽𝑘 and the mode of the
posteriordistribution(cid:98)𝜽.Whilsttheconstantofproportionalitymaybehard
to calculate, a user can choose a constant based on a reasonable average
subsamplesizetheywanttoachieve–andthiswouldstillenforcethatwe
have similar accuracy for the estimate of the gradient for all iterations of
SGLD.
3.3.2 Example:TheValueofControlVariates
Recall the tractable Gaussian posterior example 𝜋(𝜽) = N(𝝁 𝑁,𝚺 𝑁) in
(3.3), where 𝚺 𝑁 = (𝑁V−1 + I 2)−1 and 𝝁 𝑁 = 𝚺 𝑁(V−1(cid:205)𝑁 𝑗=1y𝑗). From
this posterior distribution, we can calculate the full-data gradient, as well
as the simple stochastic (3.6) and control variate-based gradients (3.8).
The gradient for the 𝑗th component is ∇log𝜋 𝑗(𝜽) = ∇log 𝑓(y𝑗|𝜽) +
(1/𝑁)∇log𝜋 (𝜽) and for this example we can easily derive the poste-
0
rior mode (cid:98)𝜽 = 𝚺 𝑁(V−1(cid:205)𝑁 𝑗=1y𝑗) and use this within the control variate
gradientestimator.Forthefull-data,simplestochastic,andcontrolvariate
gradientestimatorswehavethefollowing,
𝑁 𝑁
∑︁ ∑︁
∇log𝜋(𝜽) = ∇log𝜋 𝑗(𝜽) =− V−1y𝑗 −(𝑁V−1+I 2)𝜽,
𝑗=1 𝑗=1
∇(𝑚)log𝜋(𝜽) = 𝑚𝑁 ∑︁ ∇log𝜋 𝑗(𝜽) =− 𝑚𝑁 ∑︁ V−1y𝑗 − (cid:0)𝑁V−1+I 2(cid:1) 𝜽,
𝑗∈S𝑚 𝑗∈S𝑚3.3 StochasticGradientLangevinDynamics 79
∑︁𝑁 𝑁 ∑︁ (cid:16) (cid:17)
∇(𝑚)log𝜋 cv(𝜽) = ∇log𝜋 𝑗((cid:98)𝜽)+
𝑚
∇log𝜋 𝑗(𝜽)−∇log𝜋 𝑗((cid:98)𝜽)
𝑗=1 𝑗∈S𝑚
𝑁
=−∑︁ V−1y𝑗 −(𝑁V−1+I 2)(cid:98)𝜽 + (cid:0)𝑁V−1+I 2(cid:1) (cid:16) (cid:98)𝜽 −𝜽(cid:17)
𝑗=1
𝑁
∑︁
=− V−1y𝑗 −(𝑁V−1+I 2)𝜽.
𝑗=1
ForthisGaussianexample,withthecontrolvariateestimatorsettothepos-
teriormode,thecontrolvariate-basedgradientresultsinthesamegradient
estimatorasthefull-datagradient.Thesimplegradientestimator(3.6)gives
anunbiasedestimateofthefull-datagradient,however,forsmallsubsam-
plesizes,thisestimatorcanleadtopoorposteriorapproximations.Forthis
particularmodel,itispossibletoseparatethedatay𝑗 fromtheparameters
𝜽 in such a way that the gradient estimator can be updated at each Monte
Carloiterationwithoutre-evaluatingthedata,i.e.thedatacomponentofthe
gradient could be pre-computed and stored. Therefore, for this particular
model,thesimpleSGLDgradientestimatorwouldnotberecommendedas
themodelstructureeasilyleadstomoreefficientgradientestimators.How-
ever,derivingbettergradientestimators,suchasforthisGaussianposterior
model,isusuallyonlypossibleforsimplemodelsandthereforethesimple
unbiasedgradientestimator(3.6)isstillapopularchoicewithintheSGLD
scheme(Algorithm3)forgeneralmodels.
Figure 3.2 shows the posterior approximation for the Gaussian model
using the three gradient estimators, where stochastic gradients are calcu-
latedusing1%ofthefulldataset.TheULAsampler(top-right)withouta
Metropolis–Hastingscorrectionproducesanaccurate,albeitslightlybiased,
approximationsimilartotheSGLDwithcontrolvariates(SGLD-CV)algo-
rithm(bottom-right),whichhasthesamegradientestimatorforthismodel.
TheSGLDalgorithm(bottom-left)hasthecorrectposteriormeanbutpro-
ducesanover-dispersedapproximationtotheposteriorvariance.Reducing
the stochasticity in the gradient estimator, for example, by increasing the
subsample size, will lead to improved empirical performance. This is a
well-known feature of the SGLD algorithm and its theoretical properties
arediscussedinthenextsection.80 StochasticGradientMCMCAlgorithms
 9 D U L D Q F H  R I  V W R F K D V W L F  J U D G L H Q W V  8 / $  V D P S O H U
   
  
   
      
 6 * / '
   
 6 * / '  & 9   
   
  
   
 
                                            
θ θ
 6 * / '  V D P S O H U     G D W D  6 * / '  & 9  V D P S O H U     G D W D
  
  
  
  
  
  
     
   
                                       
θ θ
Figure3.2 Topleft:Varianceoftheestimatedgradientstaken
overarangeof𝜃(1) forthefirstcomponentof𝜽.Thevariancefor
theSGLDestimatorisstableacross𝜃(1),whereasthevariancein
theSGLD-CVgradientestimatorincreasesas|𝜃(1) −(cid:98)𝜃(1)|
increases.Theremainingplotsgivetheposteriorapproximationto
thefirstmarginalcomponent𝜃(1) forULA,SGLDand
SGLD-CV,withthesolidblacklinerepresentingthetruedensity.
3.3.3 ConvergenceResultsforStochasticGradientLangevin
Dynamics
TheSGLDalgorithmprovidesasimpleandefficientapproachforsampling
fromaposteriordistribution 𝜋.However,akeyquestioniswhethererrors
can accumulate over many SGLD iterations, leading to poor approximate
samples.Fortunately,undersuitableregularityconditionson𝜋,theoretical
resultsindicatethatSGLDcanavoidpersistenterroraccumulation.Akey
    J R O   H F Q D L U D 93.3 StochasticGradientLangevinDynamics 81
assumptionisthatthedriftintheunderlyingLangevindiffusionpushesthe
state 𝜽 toward regions of high probability under 𝜋. This ensures that the
diffusion is geometrically ergodic - i.e. it forgets its initial position at an
exponentialrate.Asaresult,theSGLD-generatedsamplestendtoremain
intheregionsofhighprobabilityunder𝜋.
Therearetwomaintheoreticalapproachesforanalyzingtheaccuracyof
SGLD.(i)Wecanconsidertheaccuracyofestimatingexpectationsofthe
formE
𝜋
[ℎ(𝜽)],forsometestfunctionℎ(𝜽)undertheposteriordistribution
𝜋.Thisinvolvestakingtheaverageofℎ(𝜽𝑘)over𝑛SGLDiterates{𝜽𝑘}𝑛 𝑘=1.
Themeansquarederror(MSE)betweenthisMonteCarloaverageandthe
trueexpectationE
𝜋
[ℎ(𝜽)]providesonemeasureofSGLDaccuracy.Tehet
al.(2016)studiedthiserrormetricinthesettingwheretheSGLDstepsize
𝛿 𝑘 decaysoveriterations.(ii)Alternatively,wecanboundtheerrorbetween
thedistributionoftheSGLDiteratesandtheposterior𝜋after𝑛steps.This
involvesanalysingthetotalvariation,orWassersteindistance,betweenthe
marginaldistributionoftheSGLDchainatiteration𝑛(i.e.𝜋˜𝑛)and𝜋.Since
SGLDisbasedontheLangevindiffusion,itsergodicitypropertiescanbe
leveragedtoprovethemarginaldistributionsconvergeto𝜋.
Consider approach (i). The MSE of the SGLD estimator can be upper
boundedby𝐶 𝑁(𝛿2+1/𝑛𝛿),where𝐶 𝑁 isaconstantdependentonthedataset
size𝑁.The𝛿2termreflectsthesquaredbiasand1/𝑛𝛿isthevarianceterm.
For a fixed computational budget 𝑛, the bias increases if the step size 𝛿
increases,whilethevariancedecreaseswithincreasing𝛿.Tehetal.(2016)
showedthatinthesettingofadecreasingstepsize𝛿 𝑘,inordertominimise
the asymptotic MSE, the optimal 𝛿 decays at rate of 𝑛−1/3. This yields an
MSE rate of 𝑛−2/3, which is slower than the 𝑛−1 rate for standard Monte
Carlo methods. The slower convergence arises from controlling both bias
andvariance,whichissimilartootherasymptoticallybiasedMonteCarlo
methods. On the other hand, for larger computational budgets (i.e. larger
𝑛),exactMCMCwilloutperformSGLD,becauseunlikeforexactMCMC
methods,thebiasfromSGLDisnon-vanishing.
Forapproach(ii),onequantifiestheaccuracyofSGLDviatheaccuracy
of the marginal distribution of the SGLD samples, 𝜽𝑘, at iteration 𝑘. We
denote this marginal distribution by 𝜋˜𝑘. Accuracy is commonly measured
bytheWassersteindistance(3.4)between𝜋˜𝑘 andtheposteriordistribution
𝜋,asthismakestheanalysismoretractable.However,careisneededwhen
interpretingtheWassersteindistanceasitisnotscale-invariant,i.e.chang-
ingtheunitsscalesthedistance.Additionally,forafixedlevelofaccuracy
ineachmarginal,thedistancegrowsas𝑑1/2 withdimension𝑑.
MuchofthetheoryforULAandSGLDassumesthelogposteriordensity,82 StochasticGradientMCMCAlgorithms
log𝜋(𝜽),issmoothandstronglyconcave.Thekeyassumptionsforanalysis
of these algorithms are that log𝜋(𝜽) is 𝑙-convex (3.18) and log𝜋(𝜽) is
continuously differentiable with 𝐿-Lipschitz gradients (3.17). This means
thereexistconstants0 < 𝑙 ≤ 𝐿 suchthatforall𝜽 and𝜽′:
∥∇log𝜋(𝜽′)−∇log𝜋(𝜽)∥ ≤ 𝐿∥𝜽 −𝜽′∥, and (3.17)
𝑙
−⟨∇log𝜋(𝜽)−∇log𝜋(𝜽′),𝜽 −𝜽′⟩ ≥ ∥𝜽 −𝜽′∥2, (3.18)
2
where we define 𝜅 = 𝐿/𝑙 as the condition number. The first condition
(3.17) bounds how fast the Langevin drift can change, and thus controls
the step size (which should be 𝛿 < 1/𝐿). The second assumption (3.18)
ensuresthatthedrifttermpushes 𝜽 towardshigh-densityregions,making
theLangevindiffusiongeometricallyergodic.Together,theseassumptions
implyanupperandlowerboundonthedirectionalderivativesofthelogpos-
teriordensity.Theboundsenablestablediscretisationandpreventpersistent
erroraccumulationintheSGLDalgorithm.Theyalsoimplylog𝜋(𝜽)isuni-
modal. By leveraging strong concavity, the resulting theory provides step
size conditions and rates of convergence for ULA/SGLD. When log𝜋(𝜽)
is non-strongly concave, alternative assumptions are required (Raginsky
etal.,2017;Majkaetal.,2020),forinstancethatlog𝜋(𝜽)isdissipative,i.e.
⟨𝜽,∇log𝜋(𝜽)⟩ ≥ 𝑎∥𝜽∥2−𝑏,forsome𝑎 > 0and𝑏 ≥ 0.
Under the above assumptions (3.17)-(3.18), Dalalyan and Karagulyan
(2019)showedthatrunningtheSGLDalgorithmwithastepsizeparameter
𝛿 ≤ 2/(𝑙 + 𝐿), the Wasserstein-2 distance d𝑊 (𝜋˜𝑛,𝜋) between the SGLD
2
marginal𝜋˜𝑛 atiteration𝑛andtheposterior𝜋canbeboundedas:
d𝑊 2(𝜋˜𝑛,𝜋) ≤ (1−𝑙𝛿)𝑛 d𝑊 2(𝜋˜ 0,𝜋)+𝐶 1(𝛿𝑑)1/2+𝐶 2𝜎(𝛿𝑑)1/2, (3.19)
where𝑙,𝐶 ,𝐶 areconstants,𝑑isthedimension,and𝜎2isanupperbound
1 2
on the variance of any individual component of the stochastic gradient
(3.10). In the case of ULA, 𝜎2 = 0. The first term in the upper bound
decays exponentially, controlling bias from the initial distribution of the
algorithm 𝜋˜ wheretheMarkovchainisinitialisedfrom.Thesecondterm
0
represents the error that results from the Euler–Maruyama discretisation
of the Langevin diffusion. The final term relates to the variance in the
stochastic gradient. In the case of the simple gradient estimator (3.6) the
variance is 𝑂(𝑁2/𝑚) and the final term in the bound is 𝑂(𝑁√︁ (𝛿𝑑/𝑚)),
whichisthenthedominatingtermintheupperbound.
GiventhatthemainmotivationofSGLDistoperformBayesianinference
overlarge-scaledata,anaturalquestionistoaskhowdoesSGLDscaleas
datasize𝑁 increases?Onewayofaddressingthisistoask,as𝑁 increases,3.3 StochasticGradientLangevinDynamics 83
what is the computational cost of running SGLD so that we have a fixed
level of approximation? We need to define our measure of approximation
appropriately to account for the fact that 𝜋 will change as 𝑁 increases:
undercertainassumptions,suchastheBernstein–vonMises(LeCam,1986)
assumption, the variance will decay as 1/𝑁. This has been investigated
by Nagapetyan et al. (2017) and Baker et al. (2019) who consider using
control variates as a pre-processing step, which has a computational cost
thatislinearin 𝑁.Ignoringthecostofthispre-processingstepforSGLD,
usingcontrolvariatesasymptoticallyhasacomputationalcostpereffective
samplethatisconstant.Bycomparison,thecomputationalcostpereffective
sampleofSGLDwiththesimpleestimatorofthegradient(3.6)islinearin
𝑁.
Example:TheoreticalPropertiesonaGaussianTargetDistribution
WecangaininsightintothepropertiesoftheSGLDalgorithmbyreturning
toourrunningGaussianexample(3.3).Recallthattheposteriorunderthis
model is a bivariate Gaussian distribution 𝜽|y ∼ N(𝝁 𝑁,𝚺 𝑁). In (3.3), we
assumed that the covariance matrix 𝚺 𝑁 was diagonal, however, we will
now instead consider a general symmetric positive semi-definite matrix.
We can express the variance matrix in terms of some rotation matrix P
andadiagonalmatrixD,whoseentriessatisfythecondition𝜎2 ≥ 𝜎2,i.e.
1 2
𝚺 𝑁 = P⊤DP. Under this Gaussian posterior model, the drift term of the
Langevindiffusionis
∇log𝜋(𝜽) =−𝚺−1(𝜽 − 𝝁 ) =−P⊤D−1P(𝜽 − 𝝁 ).
𝑁 𝑁 𝑁
The 𝑘 +1thiterationoftheSGLDalgorithmis
𝛿 √
𝜽𝑘+1 = 𝜽𝑘 − 2P⊤D−1P(𝜽𝑘 − 𝝁 𝑁)+𝛿𝝂𝑘 + 𝛿Z𝑘, (3.20)
whereZ𝑘isavectoroftwoindependentstandardGaussianrandomvariables
and 𝝂𝑘 istheerrorinducedbyourstochasticestimateof ∇log𝜋(𝜽𝑘).The
entriesofD−1correspondtotheconstantsthatappearinassumptions(3.17)-
(3.18),with𝑙 =1/𝜎2and𝐿 =1/𝜎2,whicharediscussedfurtherinSection
1 2
3.5intermsoftheirimpactonthestepsizeparameter𝛿.
We can simplify the SGLD algorithm by considering updates on the
transformedstate𝜽˜ =P(𝜽 − 𝝁 ),whichgivesSGLDupdates,
𝑁
𝛿 √
𝜽˜
𝑘+1
=𝜽˜
𝑘
− 2D−1𝜽˜
𝑘
+𝛿P𝝂+ 𝛿PZ
(cid:18) 1−𝛿/(2𝜎2) 0 (cid:19) √
=
0
1
1−𝛿/(2𝜎2)
𝜽˜
𝑘
+𝛿P𝝂𝑘 + 𝛿PZ𝑘,
284 StochasticGradientMCMCAlgorithms
wherethevarianceofPZ𝑘 isstilltheidentityasPisarotationmatrix.
TheSGLDupdateisavectorauto-regressiveprocess.Thisprocesswill
have a stationary distribution if 𝛿 < 4𝜎2 = 4/𝐿, otherwise the process
2
will produce trajectories which will tend to infinity in at least one of the
two components. A similar assumption on the step size is required to es-
tablish the upper bound on d𝑊 (𝜋˜𝑛,𝜋) in (3.19) for log-concave posterior
2
distributions.
For simplicity in the manner of convergence, we choose 𝛿 < 2𝜎2 and
2
then define 𝜆 𝑗 = 𝛿/(2𝜎2 𝑗) < 1. This then leads to the following SGLD
dynamicsforeachcomponent, 𝑗 =1,2
𝜃˜ 𝑘( +𝑗)
1
= (1−𝜆 𝑗)𝑘𝜃˜ 0(𝑗)
+∑︁𝑘
(1−𝜆 𝑗)𝑘−𝑖
(cid:16)
𝛿P𝝂𝑖
+√ 𝛿PZ𝑖(cid:17)(𝑗)
, (3.21)
𝑖=1
where 𝜃˜ 𝑘(𝑗) is the 𝑗th component of 𝜽˜ 𝑘. From this, we immediately see
from the first term on the right-hand side of (3.21) that the SGLD algo-
rithm forgets its initial condition exponentially quickly. However, the rate
ofexponentialdecayisslowerforthecomponentwiththelargermarginal
variance, i.e. 𝜎2. Furthermore, as the step size 𝛿 is constrained by the
1
smaller marginal variance 𝜎2, this rate will have to be slow if 𝜎2 ≪ 𝜎2;
2 2 1
thissuggeststhattherearebenefitsfromre-scalingtheposteriorsothatthe
marginalvariancesofdifferentcomponentsareapproximatelyequal.
Taking the expectation of (3.21), with respect to 𝝂 and Z, and letting
𝑘 → ∞, results in SGLD dynamics that have the correct limiting mean,
but with an inflated variance. This is most easily seen if we assume that
the variance of P𝝂 is independent of position. In this case, the stationary
distributionofSGLDwillhaveavarianceof
Var𝜋˜ (cid:2) 𝜽˜(cid:3) = (cid:18) (1−(1− 0𝜆 1)2)−1 (1−(1−0 𝜆 )2)−1 (cid:19) (𝛿2𝚺 𝑁 +𝛿I 2),
2
where I is a 2-dimensional identity matrix. The marginal variance for
2
component 𝑗 isthus
𝜎2
𝑗
1−1+ 𝛿/𝛿 (Σ
4𝜎𝑗𝑗
2)
= 𝜎2
𝑗
(cid:0) 1+𝛿Σ 𝑗𝑗(cid:1) +
4𝛿
+𝑂(𝛿2).
𝑗
The inflation in variance comes both from the noise in the estimate of
∇log𝜋(𝜽), which is the 𝛿Σ 𝑗𝑗 factor, and the Euler approximation, which
suppliestheadditiveconstant,𝛿/4.Formoregeneralposteriordistributions,
the mean of the stationary distribution of the SGLD algorithm will not
necessarilybecorrect,butwewouldexpectthemeantobemoreaccurate3.4 AGeneralFrameworkforstochasticgradientMCMC 85
thanthevariance,withthevarianceofSGLDbeinggreaterthanthatofthe
trueposterior.Thisanalysisfurthersuggeststhat,forposteriordistributions
whichareclosetoGaussian,itmaybepossibletoperformabettercorrection
to compensate for the inflation of the variance. For example, we could
replaceZ𝑘 withGaussianrandomvariableswher √ethecovariancematrixis
chosensuchthatthecovariancematrixof𝛿𝝂𝑘 + 𝛿Zbecomestheidentity
matrix.
3.4 AGeneralFrameworkforstochasticgradientMCMC
Stochastic gradient MCMC is not limited to approximating the Langevin
diffusion.Wecanconstructotherdiffusionprocessesthatalsohave𝜋astheir
stationarydistribution.Byapproximatingthedynamicsofthesealternative
diffusions, we can develop stochastic gradient versions of many popular
MCMCalgorithmsbeyondtheLangevinmethod.
Ma et al. (2015) proposed a general framework for stochastic gradient
MCMC that extends the approach beyond the Langevin diffusion. This
frameworkallowsustodevelopstochasticgradientanaloguesofalgorithms
likeHamiltonianMonteCarlo(HMC)(asintroducedinSection2.2),which
leveragesHamiltoniandynamics.stochasticgradientHMC(SGHMC)has
beenshowntodisplayimprovedmixingpropertiescomparedtostochastic
gradientLangevindynamics.
BeyondbothSGLDandSGHMC,stochasticgradientMCMCapproaches
provideageneralandflexibleframeworktoadaptmanyMCMCalgorithms
tolargedatasetswherefull-dataMCMCisinfeasible.Byapproximatingthe
dynamics of various diffusions with the correct stationary distribution, it
ispossibletodevelopfast,scalableMCMCalgorithmstailoredtodifferent
posteriorgeometriesanddatasets.
Weintroduceageneralclassofdiffusionmodels,thatmayalsoinclude
auxiliary variables, and we denote the full state by 𝝑 ∈ R𝑑 𝝑. This state
contains our variable of interest 𝜽, as well as an auxiliary variable p.
For example, in the case of the overdamped Langevin diffusion, 𝝑 = 𝜽,
and extending the state space to include a velocity variable p allows for
the underdamped Langevin diffusion of Section 1.4.3, which relates to
HamiltonianMCMC,giving𝝑 = (𝜽,p).
Wedefinethegeneralstochasticdifferentialequationfor𝝑as
1 √︁
d𝝑 = b(𝝑)d𝑡+ D(𝝑)d𝑊 𝑡, (3.22)
2
whereb(𝝑)isthedriftterm,D(𝝑)isapositivesemidefinitediffusionmatrix86 StochasticGradientMCMCAlgorithms
withmatrixsquareroot√︁ D(𝝑) and𝑊 𝑡 denotes 𝑑 𝝑-dimensionalBrownian
motion. Ma et al. (2015) provide a general framework for how to choose
b(𝝑)andD(𝝑)toachieveadesiredstationarydistribution.Givenageneral
function𝐻(𝝑),whereexp{−𝐻(𝝑)}isintegrable,simulatingfromanSDE
withdriftterm,
b(𝝑) =−[D(𝝑)+Q(𝝑)]∇𝐻(𝝑)+Γ(𝝑), where (3.23)
∑︁𝑑 𝜕
Γ 𝑖(𝝑) = 𝜕𝝑𝑗(D𝑖𝑗(𝝑)+Q𝑖𝑗(𝝑)),
𝑗=1
ensuresthatthestationarydistributionof(3.22)isproportionaltoexp{−𝐻(𝝑)}.
The matrix Q(𝝑) is a skew-symmetric curl matrix which controls the de-
terministic traversing effects of the SDE sampler, whereas D(𝝑) controls
the diffuse dynamics of the process. The general SDE (3.22) can be de-
composed into (i) Riemannian-Langevin dynamics which are reversible
andcontrolledbyD(𝝑)and(ii)deterministicHamiltoniandynamicswhich
areirreversibleandcontrolledbyQ(𝝑).FortheLangevin-typealgorithms,
where Q(𝝑) = 0, it can be shown that they are able to quickly converge
towardsamodeofthedistributionanddiffusivelyexploretheregionaround
themode.ForthedeterministicHamiltonianMonteCarloalgorithm,where
D(𝝑) = 0, the algorithm excels at deterministically traversing along level
setsoftheHamiltonian.
ToapproximatelysamplefromtheSDEwediscretisethedynamicsusing
the same Euler–Maruyama scheme used for the Langevin diffusion (3.1),
i.e.,
𝛿 √
𝝑𝑘+𝛿 = 𝝑𝑘 − [(D(𝝑𝑘)+Q(𝝑𝑘))∇𝐻(𝝑𝑘)+Γ(𝝑𝑘)]+ 𝛿Z, 𝑘 ≥ 0,
2
(3.24)
where Z ∼ N(0,D(𝝑𝑘)). If 𝝑 = 𝜽, the posterior distribution 𝜋 is the
stationarydistributionforthechoice 𝐻(𝝑) = −log𝜋(𝜽).Ifweincludean
auxiliary variable, i.e. 𝝑 = (𝜽,p), and we choose 𝐻(𝝑) = −log𝜋(𝜽) +
𝐾(p) for some user-chosen function 𝐾(·), then 𝜋 is the 𝜽-marginal of the
stationarydistribution.
FromthegeneralSDEframework(3.22),wecanobtainstochasticgradi-
entMCMC(SGMCMC)algorithmsbyreplacing∇𝐻(𝝑𝑘)in(3.24)withan
unbiasedestimate(cid:98)∇𝐻(𝝑𝑘) thatisevaluatedonasubsampleofthedataset.
As shown in Section 3.3.3, and Figure 3.2, the statistical efficiency of
stochastic gradient algorithms is strongly tied to the variance of the gra-
dient estimate. We can view that the variability of the gradient estimate
inflatesthenoiseinputinto(3.24)–whichwillleadtoastationarydistribu-3.4 AGeneralFrameworkforstochasticgradientMCMC 87
tion whose variance is also inflated. Therefore, where feasible, we should
(cid:104) (cid:105)
correctforthis.IfwedefineV(𝝑𝑘) =Var (cid:98)∇𝐻(𝝑𝑘) asthevarianceofthe
stochasticgradient,thentheconditionalvarianceof𝝑𝑘+𝛿 given𝝑𝑘 willbe
inflatedby𝛿2B(𝝑𝑘),where
1
B(𝝑𝑘) = (D(𝝑𝑘)+Q(𝝑𝑘))V(𝝑𝑘)(D(𝝑𝑘)+Q(𝝑𝑘))⊤. (3.25)
4
Correcting for this inflation in the SGMCMC setting leads to a modified
discrete-timealgorithmin(3.24),where∇𝐻(𝝑𝑘)isreplacedbythestochas-
ticapproximation(cid:98)∇𝐻(𝝑𝑘)butwealsoreducethevarianceofthenoiseterm,
soZ ∼ N(0,D(𝝑𝑘) −𝛿B(𝝑𝑘)).Thechallengewiththisideainpracticeis
howdoweestimateB(𝝑𝑘)?
Through different choices of 𝐻(𝝑), D(𝝑) and Q(𝝑), we can derive
several SGMCMC algorithms as special cases of the general discretised
SDE(3.24).Someofthesespecialcaseswillbeintroducedinthefollowing
sections.
StochasticGradientLangevinDynamics
TheSGLDalgorithm(introducedinSection3.3)followsfromthedynamics
ofthegeneralSDE(3.24)bysetting
𝝑 = 𝜽, 𝐻(𝝑) =−log𝜋(𝜽), D(𝝑) =I, Q(𝝑) =0
togivedynamics
𝛿 √
𝜽𝑘+𝛿 = 𝜽𝑘 + [∇log𝜋(𝜽𝑘)]+ 𝛿Z, 𝑘 ≥ 0,
2
which is the same Euler–Maruyama approximation of the Langevin dif-
fusion introduced in (3.2), but where in practice ∇log𝜋(𝜽𝑘) would be
replaced with a stochastic gradient estimator, using, for example, (3.6),
(3.8)or(3.14).
StochasticGradientHamiltonianMonteCarlo
ThepopularHMCalgorithmintroducedinSection2.2canalsobederived
as a special case of the general SDE dynamics (3.24). As discussed in
Section 2.2, the HMC algorithm introduces a velocity component p to
improve the mixing of the Markov chain and a mass matrix M, where
the Hamiltonian dynamics are used to update the position 𝜽 and velocity
componentsp.Inpractice,theHMCalgorithmusestheleapfrognumerical
integrationschemetominimisenumericalerrors,however,forthepurpose
of illustration, we shall consider the simpler Euler integration scheme for
creatingastochasticgradientHMCalgorithm.88 StochasticGradientMCMCAlgorithms
TheEuler-discretisedHamiltoniandynamicsforthestate𝝑 = (𝜽,p) are
(cid:18) 𝜽𝑘+𝛿(cid:19)
=
(cid:18) 𝜽𝑘(cid:19)
+
𝛿 (cid:20) M−1p𝑘 (cid:21)
, 𝑘 ≥ 0, (3.26)
p𝑘+𝛿 p𝑘 2 ∇log𝜋(𝜽𝑘)
whichfitsintothegeneralSDEframeworkof (3.22)and(3.23)bysetting
(cid:18) (cid:19)
1 0 −I
𝐻(𝝑) =−log𝜋(𝜽)+ p⊤M−1p, D(𝝑) =0 and Q(𝝑) = .
2 I 0
If the gradient ∇log𝜋(𝜽) in (3.26) is replaced by a stochastic gradient
∇ˆ log𝜋(𝜽) = ∇log𝜋(𝜽) +N(0,V(𝜽)), where V(𝜽) is the variance of the
stochasticgradient,thenunderthisstochasticsettingthedynamicsin(3.26)
willbecome
𝛿
p𝑘+𝛿 =p𝑘 + ∇log𝜋(𝜽)+N(0,𝛿V(𝜽𝑘)),
2
whichisknownasthenaivestochasticgradientHMC(SGHMC)algorithm
(Chenetal.,2014;Maetal.,2015).ItwasprovedinChenetal.(2014)that
thenaiveSGHMCalgorithmdoesnotworkwellastheerrorfromestimating
the gradient will accumulate over iterations and cannot be controlled. To
overcome this, the authors suggest adding a friction term for the velocity,
thisisequivalenttousingastochasticgradientversionoftheunderdamped
Langevindynamics,ofSection1.4.3.Theintuitionisthattheintroductionof
frictionmeansthaterrorsfrompreviousiterationswilldecaygeometrically
sothattheoverallerrorfromusingastochasticgradientcanbecontrolled.
Wecanfurtherimproveaccuracybyusingtheideaofcorrectingthevariance
oftheinjectednoisethatisintroducedateachiteration.
ReturningtothegeneralSDEframework(3.22),thecorrectedstochastic
gradientHMCalgorithmfollowsbysetting
1
𝝑 = (𝜽,p), 𝐻(𝝑) =−log𝜋(𝜽)+ p⊤M−1p,
2
(cid:18) (cid:19) (cid:18) (cid:19)
0 0 0 −I
D(𝝑) = , Q(𝝑) = ,
0 C I 0
whereCisasagenericmatrix,sometimesknownasthefrictionterm,andis
chosensuchthatC ⪰ 𝛿V(𝜽)isapositivesemi-definitematrix.Discretising
this general form SDE with this particular D(𝝑) and Q(𝝑) leads to the
dynamics,
(cid:18) 𝜽𝑘+𝛿(cid:19)
=
(cid:18) 𝜽𝑘(cid:19)
+
𝛿 (cid:20) M−1p𝑘 (cid:21)
+
(cid:20) √0 (cid:21)
, 𝑘 ≥ 0. (3.27)
p𝑘+𝛿 p𝑘 2 ∇ˆ log𝜋(𝜽𝑘)−CM−1p𝑘 𝛿Z3.4 AGeneralFrameworkforstochasticgradientMCMC 89
Thegradient∇log𝜋(𝜽𝑘)isreplacedbyastochasticestimator∇ˆ log𝜋(𝜽𝑘)
andZ∼N(0,C−𝛿Bˆ),whereBˆ isanestimateofV(𝜽𝑘).
TheefficiencywithwhichanMCMCalgorithmcanexploreaposterior
distribution is heavily tied to the geometry of the posterior. If the 𝜽 com-
ponents of the posterior distribution are strongly correlated, then the step
size will need to be optimised for the component with the smallest vari-
abilityinordertoensurethatthealgorithmdoesnotdiverge(asillustrated
in Section 3.3.3 for the case of a Gaussian target distribution). This will
significantly reduce the mixing time of the other components unless the
posterior distribution is reparameterised so that the components of 𝜽 are
uncorrelatedandhavesimilarmarginaldistributions.Withinthecontextof
SGMCMCdynamics,itispossibletodevelopalgorithmsthatincorporate
reparameterisationbypreconditioningthegradientswithapositive-definite
matrix G(𝜽). If G(𝜽) is the expected Fisher information of the posterior
distribution, then the SGMCMC dynamics will be locally adapted to the
posteriorcurvaturebyexploitingtheRiemanniangeometryoftheposterior
distribution.
StochasticGradientRiemannianLangevinDynamics
Riemannian versions of SGLD (stochastic gradient Riemannian Langevin
dynamics;SGRLD)andSGHMC(stochasticgradientRiemannianHamil-
tonianMonteCarlo;SGRHMC)alsofollowasspecialcasesofthegeneral-
formSDE(3.23).WecanderiveSGRLDbysetting
𝝑 = 𝜽, 𝐻(𝝑) =−log𝜋(𝜽), D(𝝑) =G(𝜽)−1, Q(𝝑) =0
whichleadstothediscrete-timedynamics
𝛿 √
𝜽𝑘+𝛿 = 𝜽𝑘 + (cid:2) G(𝜽𝑘)−1∇ˆ log𝜋(𝜽𝑘)+Γ(𝜽𝑘)(cid:3) + ℎZ, 𝑘 ≥ 0,
2
with Z ∼ N(0,G(𝜽𝑘)−1) and ∇ˆ log𝜋(𝜽𝑘) is a stochastic estimator for
∇log𝜋(𝜽𝑘). The term G(𝜽𝑘)−1∇log𝜋(𝜽𝑘) is the natural gradient of the
posteriordistributionwhichgivesthedirectionofsteepestascentbytaking
into account the geometry implied by G(𝜽𝑘). If G(𝜽) = I, then the direc-
tion of steepest ascent would be given in the Euclidean space. The term
Γ 𝑖(𝜽) = (cid:205)
𝑗
𝜕(G 𝜕(𝜽 𝜽) 𝑗−1)𝑖𝑗 accounts for the curvature of the manifold defined
byG(𝜽).AstochasticgradientHMCimplementationthatutilisestheRie-
manniangeometryoftheposteriordistributioncanalsobederivedwithin
the general SDE framework. Taking the curl matrix Q(𝜽) from SGHMC90 StochasticGradientMCMCAlgorithms
andreplacingtheidentitymatriceswithG(𝜽)−1/2 leadstoanSGHMCal-
gorithmthatisadaptivetothelocalposteriorgeometry.
TheoreticalcomparisonofSGMCMCalgorithms
WhencomparingthevarietyofSGMCMCalgorithmsitisnaturaltocon-
siderwhichalgorithmisthemostaccurateandcomputationallyefficient.It
ispossibletocomparethetheoreticalconvergenceratesforsomeofthese
algorithms.InthecaseofSGHMCandSGLD,andinthecontextofsmooth
andstronglylog-concaveposteriors,itispossibletoderiveboundsonthe
Wasserstein-2 distance between the posterior and the SGMCMC sample
distribution.Theseresultsarenon-asymptoticandboundtheWasserstein-2
error for some 𝑘 iterations of the SGMCMC algorithm using optimally
tuned step sizes. These results show that if the stochastic gradients have
lowvariance,thenSGLDrequires𝑂(𝑑2/𝜖2)iterationsforagivenaccuracy
𝜖, while SGHMC needs only𝑂(𝑑/𝜖). So SGHMC is generally preferred,
withincreasingbenefitsinhigherdimensions(seeFigure3.4foranumer-
ical illustration). However, there is a phase transition in SGHMC as the
gradient noise variance grows, above a threshold SGHMC behaves like
SGLD,requiring𝑂(𝑑2/𝜖2) iterations.
3.5 GuidanceforEfficientScalableBayesianLearning
Each of the SGMCMC algorithms introduced has tuning parameters that
needtobechosenbytheuserwhenrunningthesealgorithms.SomeSGM-
CMC algorithms, such as SGLD with control variates, require additional
tuningparameters,e.g.choosingacontrolvariate,butcommontoallSGM-
CMC algorithms is the step size parameter 𝛿, the subsample size 𝑚, and
thenumberofMonteCarloiterations𝑛.
For MCMC algorithms with Metropolis–Hastings corrections, such as
MALA (Section 2.1.4) and HMC (Section 2.2), and which do not utilise
data subsampling, the main tuning parameter is the step size 𝛿. Existing
theoretical results have established that the optimal 𝛿 should be chosen
suchthattheMetropolis–Hastingsacceptancerateis57.4%(inthecaseof
MALA)oratleast65%(inthecaseofHMC).Undercertainassumptions
on the posterior distribution, this choice minimises the integrated auto-
correlation time of the Markov chain produced by these algorithms (see
Section1.3.2forfurtherdetails).
InthecaseofSGMCMCalgorithms,whichdonotincludeaMetropolis–
Hastings acceptance step, minimising the auto-correlation of the Markov3.5 GuidanceforEfficientScalableBayesianLearning 91
chain is not an appropriate objective for optimising 𝛿. Intuitively, this is
becausetheauto-correlationcanbereducedbysimplyletting𝛿 →∞,butin
thecaseoftheULAandSGMCMCalgorithms,thiswillincreasethebiasin
thediscretiseddiffusionprocessandleadtopoorposteriorapproximations.
This is not an issue for MALA and HMC as the Metropolis–Hastings
acceptanceratewilltendtozeroas𝛿 →∞,andsointhissetting,minimising
the auto-correlation time will balance between making large jumps in the
posterior space (i.e large 𝛿) against having a reasonable acceptance rate.
Alternative metrics are required to minimise the variance in the Markov
chainandaccountfortheasymptoticbiaspresentinSGMCMCalgorithms.
ApopularchoiceisthekernelSteindiscrepancy(KSD)metric,akernelised
versionoftheSteindiscrepancydiscussedindetailinChapter6.TheKSD
provides a measure of discrepancy between the true posterior distribution
and the Monte Carlo approximation generated by an SGMCMC or other
MCMCalgorithm.UsingtheKSDmetric,itispossibletooptimise𝛿(and
otherSGMCMCtuningparameters)byassessingvarioustuningparameters
over a grid of possible values and selecting the tuning parameters which
minimise the KSD (Coullon et al., 2023). Related ideas, based on Stein’s
methodandexploredinChapter6,canbeusedtooptimallythintheMarkov
chaintomaximisetheinformationabouttheposteriorcontainedbyasmaller
setofMonteCarlosamples.
Thestepsizeparameter𝛿inSGLDcanbechosenusingtheconvergence
results in Section 3.3.3. The upper bound on the Wasserstein-2 distance
from Dalalyan and Karagulyan (2019), under assumptions 3.17 and 3.18,
requires 𝛿 ≤ 2/(𝑙 + 𝐿) in order to establish convergence for the SGLD
algorithm. Setting 𝛿 to be too small leads to slow convergence of the
Markovchain,butastepsizethatistoolargecancausetheSGLDalgorithm
to diverge. If 𝑙 and 𝐿 are known, then this information can be used to set
𝛿. For example, returning to the running Gaussian example (3.3) where
theposteriordistributionis 𝜽|y ∼ N(𝝁 𝑁,𝚺 𝑁),weknowthat∇log𝜋(𝜽) =
−𝚺−1(𝜽−𝝁 )andthattheLipschitzconstant𝐿measuresthelargestchange
𝑁 𝑁
inthegradient.TakingtheHessiani.e.,∇∇log𝜋(𝜽) =−𝚺−1,theLipschitz
𝑁
constant is equal to the spectral norm of the inverse covariance matrix
𝐿 = ∥∇log𝜋(𝜽)∥ ≤ ∥∇∇log𝜋(𝜽)∥ ≤ ∥𝚺− 𝑁1∥ = 1/𝜆 min(𝚺 𝑁), which is
equaltothereciprocalofthesmallesteigenvalueofthecovariancematrix.
As for the smoothness parameter 𝑙, this is the largest eigenvalue of the
Hessian𝑙 = ∥∇∇log𝜋(𝜽)∥ ≥ 𝜆 max(𝚺 𝑁).Therefore,if𝛿 ≤ 2/(𝑙 +𝐿),then
𝛿 ≈𝜆 min(𝚺 𝑁)/𝜆 max(𝚺 𝑁),whichisequaltotheconditionnumber𝜅 = 𝐿/𝑙.
We can assess the relationship between the step size parameter and the
propertiesoftheGaussianmodelbyconsideringtwocovariancefunctionsΣ92 StochasticGradientMCMCAlgorithms
(cid:18) (cid:19) (cid:18) (cid:19)
1 0 1 3
fromtheGaussianmodel(3.3),where𝚺(𝑖) = and𝚺(𝑖𝑖) = .
0 10 3 10
Under covariance 𝚺(𝑖), the variables 𝜽 are uncorrelated whereas for 𝚺(𝑖𝑖)
thereisimposedcorrelationbetweenthecomponentsof𝜽.Thisleadstocon-
ditionnumbers𝜅(𝑖) ≈10−3and𝜅(𝑖𝑖) ≈10−4for𝚺(𝑖) and𝚺(𝑖𝑖),respectively.
InFigure3.3,weplottheWasserstein-2distancebetweenthetrueGaussian
posterior distribution with 𝑁 = 1000 data points with the approximation
generated from 𝑛 = 10000 iterations of the SGLD algorithm, where the
stepsizeparameter𝛿 isvariedoveragridofvalues𝛿 ∈ {10−5,...,10−1}.
TheleftpanelofFigure3.3isforthemodelwithuncorrelatedcovariance
matrix𝚺(𝑖) andtherightpaneluses𝚺(𝑖𝑖).Forbothexperiments,thereisa
valuefor 𝛿 whichminimisestheWasserstein-2distanceandinthecaseof
uncorrelatedvariables,i.e.𝚺(𝑖),theoptimal𝛿islargerthaninthecorrelated
case𝚺(𝑖𝑖).
Thedotindicatesthestepsizerecommendedbythetheoretical
results, i.e. 𝛿 = 𝐿/𝑙, which closely aligns with the optimal grid search
opt
result.
In general settings, however, it is not possible to calculate the optimal
stepsizeparameterasthisdependsonpropertiesoftheunknownposterior
distribution. Under certain conditions on the posterior distribution, and
assuming that 𝑁 is sufficiently large, then by the Bernstein–von Mises
theorem,thevarianceoftheposteriordistributionwillbeoforder𝑂(𝑑/𝑁).
Therefore,settingthestepsizeparametertobeproportionalto1/𝑁,i.e.𝛿 ∝
1/𝑁,givesasimpleheuristicstepsizewhichisoftenusedbypractitioners.
Note that, for the Gaussian posterior example given above, this heuristic
wouldleadtoastepsize𝛿 = 1/𝑁 = 10−3,whichmatchestheoptimalstep
sizeparameterforthesettingwithuncorrelated𝜽components,i.e.𝚺(𝑖)
(see
theleftpanelinFigure3.3).However,thisstepsizewouldbetoolargefor
thecorrelatedsettingwithcovarianceΣ(𝑖𝑖) (seerightpanelinFigure3.3).
AnalternativeperspectivediscussedinSection3.3.1isthatfortheSGLD-
type algorithms, the variance of the gradient component of the Langevin
dynamics is 𝑂(𝛿2) and therefore dominated by the 𝑂(𝛿) variance from
the injected noise of the process. For the SGLD algorithm with a simple
unbiasedgradientestimator,thevarianceofthegradientis𝑂(𝑁2),andso
a natural choice for 𝛿 to control the variance of the stochastic gradient is
𝛿 =1/𝑁.
3.5.1 ExperimentsonaLogisticRegressionModel
The logistic regression model, first introduced in Section 1.2.1, is used to
predict the probability of binary outcomes 𝑦 𝑗 ∈ {0,1} given covariates3.5 GuidanceforEfficientScalableBayesianLearning 93
       
   
   
   
   
   
       
 6 W H S  V L ] H   O R J     6 W H S  V L ] H   O R J   
Figure3.3 TheWasserstein-2distancebetweenthetrueand
approximateposteriordistributionswhenvaryingthestepsize
parameter𝛿.LeftpanelisforthemodelwithcovarianceΣ(𝑖).
RightpanelisforthemodelwithcovarianceΣ(𝑖𝑖).
x𝑗 ∈ R𝑑 𝑥. Assuming a Gaussian prior for the regression coefficients 𝜽 ∼
N(0,Σ 𝜽),theposteriordistribution,conditionalonthedataD = {𝑦 𝑗,x𝑗}𝑁 𝑗=1,
isgivenbytheunnormaliseddensity
𝜋(𝜽) := 𝜋(𝜽|D)
∝exp(cid:26) −1 𝜽𝑇Σ−1𝜽(cid:27) (cid:214)𝑁 exp{𝑦 𝑗x⊤ 𝑗𝜽}
.
2 𝜽 1+exp{x𝑇𝜽}
𝑗=1 𝑗
Metropolis–Hastings-basedMCMCalgorithms,suchasMALA(Section
2.1.4) and HMC (Section 2.2), can be used to sample from the posterior
distributionofthelogisticregressionmodel.However,inthelargedataset-
ting,thesealgorithmswillconvergeslowlyduetothehighercomputational
costofevaluatingtheposteriordensity,anditsgradient,onthefulldataset.
Whereas SGMCMC algorithms are faster, per Monte Carlo iteration, but
introduceanasymptoticbiasintotheposteriorapproximation.
To assess the statistical accuracy of SGMCMC against exact MCMC
approaches, consider a logistic regression model with 𝑁 = 10000 obser-
vations, which is small enough to allow MALA and HMC approaches to
be computationally feasible. The dataset is split into a training and test
datasetwitha80/20split.Dataaresimulatedfromthelogisticregression
model where the dimension of the parameter vector 𝜽 ∈ R𝑑 is varied,
𝑑 ∈ {100,200,300,400,500}.Thestatisticalaccuracyoftheposteriorap-
proximationoftheSGLDalgorithm(Alg.3)andSGHMCalgorithm(3.27),
 H F Q D W V L '    Q L H W V U H V V D :  H F Q D W V L '    Q L H W V U H V V D :94 StochasticGradientMCMCAlgorithms
withandwithoutcontrolvariates(3.7),iscomparedagainstalongMonte
CarlorunoftheNUTSalgorithm(HoffmanandGelman,2014)usingthe
PythonpackageBlackJax(Cabezasetal.,2024),whichprovidesaground-
truthMonteCarloapproximationtotheposteriordistribution.Asnotedin
Section 3.5, standard MCMC diagnostics are not applicable for assessing
the convergence of SGMCMC algorithms, and so as a proxy for posterior
accuracy, the mean squared error (MSE) in the estimate of the posterior
meanandvariancegivenbytheSGMCMCalgorithmsiscomparedagainst
theposteriormeanandvariancetakenfromtheNUTSsamples.TheNUTS
and SGMCMC algorithms are each run for 𝑛 = 10000 iterations and for
theSGMCMCalgorithms,asubsamplesizeof10%isused.Notethatthe
stepsizeparameterisfixedusingtheheuristic𝛿 =1/𝑁 forallexperiments.
Notethatimprovednumericalresultscouldbeachievedbyoptimisingthe
stepsizeparameterforeachdimension𝑑.
Figure3.4showstheMSEintheestimateofE
𝜋
[𝜽]andVar𝜋 [𝜽],where
the true expectation and variance are given by the NUTS sampler. The
plotted results are presented as the MSE averaged over the parameter di-
mension.TheresultsshowthattheSGHMCalgorithmsaremorerobustto
higherdimensionsthantheSGLD-typealgorithms.Thiscoincideswiththe
established theory that compared to SGHMC, SGLD requires more itera-
tionstoachieveasimilarlevelofaccuracy.Notethatforhigher-dimensional
problems,itmaybenecessarytoruntheSGDoptimiserforlongertofind
themodeofthedistributionthatisusedincontrolvariate-basedSGMCMC
algorithms.
The posterior accuracy results in Figure 3.4 suggest that without in-
creasingthe numberof MonteCarloiterations, theSGMCMCalgorithms
willproducepoorerposteriorapproximationswithincreasingparameterdi-
mension.TheresultsshowthatSGMCMCalgorithmscanproducehighly
accurate approximations for the first and second moment of the posterior
distribution,andinthecaseofSGHMC,thefirstmomentisverysimilarto
the first moment given by the NUTS sampler. As illustrated previously in
Figure 3.2, SGLD can produce good approximations to the first posterior
moment, but for small data subsamples it tends to produce overestimates
of the second posterior moment. The reason for the poorer posterior ap-
proximationoftheSGLD-basedsamplerscomparedtotheSGHMC-based
samplerscanbeseenintheMonteCarlotraceplotsinFigure3.5.Forthe
higher-dimensionalsetting(𝑑 =500),wecanseethatforposteriorcompo-
nents 𝜃 and 𝜃 , the mixing is worse for SGLD and SGLD-CV compared
1 2
toSGHMCandSGHMC-CV.ThisisthenreflectedintheMonteCarloap-
proximationfortheposteriormeanandvariance(Figure3.4),whereSGLD3.5 GuidanceforEfficientScalableBayesianLearning 95
  H  
 6 * / '       6 * / '
      
 6 * / '  & 9  6 * / '  & 9
    
 6 * + 0 &  6 * + 0 &
      
 6 * + 0 &  & 9       6 * + 0 &  & 9
      
    
      
    
           
               
 ' L P H Q V L R Q  ' L P H Q V L R Q
Figure3.4 Themeansquarederror(MSE)ofE 𝜋 [𝜽] (leftpanel)
andVar𝜋 [𝜽] (rightpanel)comparedtothesamemoments
calculatedfromtheNUTSposteriorsamples,whicharetreatedas
theground-truth.TheresultsplottedarefortheaverageMSE
takenoveralldimensionsofthemeanandmarginalvarianceof
𝜋(𝜽).
and SGLD-CV are not as accurate as SGHMC and SGHMC-CV when
𝑑 = 500, but display similar levels of accuracy for 𝑑 = 100 and 𝑑 = 200.
ThemixingoftheSGLD-basedsamplerscouldbeimprovedbyhand-tuning
the step-size parameter 𝛿, or preconditioning the gradients to account for
thecorrelationstructureoftheposteriordistribution.
Beyond posterior accuracy, we can also assess the predictive accuracy
of SGMCMC algorithms against exact full-data MCMC algorithms, in
this case using the NUTS sampler. Figure 3.6 illustrates that SGMCMC
algorithms are competitive against slower Metropolis–Hastings-based al-
gorithms when assessed against predictive accuracy. Figure 3.6 plots the
percentageimprovementinlog-posteriorpredictiveaccuracyforeachSGM-
CMC algorithm over the log-posterior predictive accuracy of the NUTS
sampler, which is treated as the gold standard approach. The results are
given for the logistic regression model on a test dataset using posterior
samplesoverarangeofparameterdimensions𝑑.Theresultshighlightthat
SGMCMC algorithms are competitive and potentially superior to slower,
full-data, MCMC algorithms in terms of predictive accuracy, displaying
only a small decrease in efficiency but with a significant computational
advantage.
 ( 6 0  ( 6 096 StochasticGradientMCMCAlgorithms
SGLD SGLD
0.0 1.2
0.1
1.0
0.2
0.8
0.3
0.4
0.6
0.5
0 5000 10000 0 5000 10000
Iterations Iterations
SGLD-CV SGLD-CV
0.2 1.2
0.0
1.0
0.2 0.8
0.4
0.6
0.6
0 5000 10000 0 5000 10000
Iterations Iterations
SGHMC SGHMC
0.0
1.2
0.2 1.0
0.4 0.8
0.6
0.6
0 5000 10000 0 5000 10000
Iterations Iterations
SGHMC-CV SGHMC-CV
0.2 1.4
0.0 1.2
0.2 1.0
0.4 0.8
0.6
0.6
0 5000 10000 0 5000 10000
Iterations Iterations
NUTS NUTS
1.4
0.0
1.2
0.2
1.0
0.4 0.8
0.6 0.6
0 2000 4000 0 2000 4000
Iterations Iterations
Figure3.5 Traceplotsforthefirsttwocomponentsof𝜽 with
𝑑 =500.
θ
θ
θ
θ
θ
1
1
1
1
1
θ
θ
θ
θ
θ
2
2
2
2
23.5 GuidanceforEfficientScalableBayesianLearning 97
SGLD
3
SGLD-CV
SGHMC
2
SGHMC-CV
1
0
1
2
3
4
100 200 300 400 500
Dimension
Figure3.6 Percentageimprovementinthelog-predictivedensity
ofeachSGMCMCalgorithmrelativetothelogpredictivedensity
oftheNUTSsampleronthelogisticregressionmodelcalculated
onatestdataset.Thedimensionoftheparameterofinterestis
𝑑 ∈ {100,200,300,400,500}.
3.5.2 ExperimentsonaBayesianNeuralNetworkModel
A Bayesian neural network model for multi-class classification was intro-
duced in Section 1.2.3, where the dataset D = {𝑦 𝑗,x𝑗}𝑁 𝑗=1is comprised of
a collection of 𝐺 classes 𝑦 𝑗 ∈ {1,...,𝐺} and covariates x𝑗 ∈ R𝑑 𝑥. The
unnormalisedposteriordensityis
𝑁
(cid:214)
𝜋(𝜽) := 𝜋(𝜽|D) ∝ 𝜋 0(𝜽) exp(A⊤ 𝑦 𝑗+1𝜎(B⊤x𝑗 +b)+𝑎 𝑦 𝑗+1), (3.28)
𝑗=1
where 𝜎(·) isasoftmaxactivationfunction(seeSectionSection1.2.3for
details).Themodelparameters𝜽 =vec(A,B,a,b)aretheweightsA,Band
biases a,b of the network model. We shall assume independent standard
Gaussianpriorsforeachparameter,i.e.𝜽 ∼N(0,I).
Neural networks are commonly used for image classification tasks.
One of the most fundamental and widely used datasets in image clas-
sification is the MNIST handwritten digit dataset. The MNIST dataset
consists of images of handwritten digits, ranging from zero to nine, i.e.
tnemevorpmi
egatnecrep
evitciderp-goL98 StochasticGradientMCMCAlgorithms
Figure3.7 AselectionofdigitsfromtheMNISTdataset.Image
source-Wikipedia.
𝑦 𝑗 ∈ {0,1,2,3,4,5,6,7,8,9} (see Figure 3.7 for a subsample of the
dataset). Each image is represented by a small square of 28 pixels by
28pixelswhicharetreatedascovariates.Eachx𝑗 ∈ R784 isavectorisation
ofamatrixmadeupof28rowsand28columns,witheachpixelcontaining
grayscale information representing the darkness of that specific point in
theimage.Abrighterpixelwouldhaveahighervalue,whileadarkerone
wouldhavealowervalue.
TheBayesianneuralnetworkforthisexample(3.28)hastwolayers:an
inputlayerthatreceivestheinformationfromthe28×28image,andahidden
layer containing 100 hidden variables that act as intermediate processing
units.Theparametersoftheneuralnetwork𝜽 areoftheformA ∈ R10×100,
B ∈ R784×100,a ∈ R1×10,andb ∈ R1×100.
The MNIST dataset contains a large collection of 60000 images in the
trainingset.Eachimagehasacorrespondinglabel,indicatingwhichdigit
(0 − 9) it represents. Using SGMCMC algorithms, we can approximate
the posterior distribution of the Bayesian neural network using subsam-
ples of the labelled images and pixel values to train the Bayesian neural
networktorecognisepatternsandrelationshipsbetweenthepixelsandthe
correspondingdigits.
We use the SGLD and SGHMC algorithms from the Python package
SGMCMCJax(CoullonandNemeth,2022),andtheircontrol-variatecoun-
terparts,todrawsamplesfromtheBayesianneuralnetworkposterior(3.28).
Weruneachalgorithmfor2000iterationsandretainevery10thiterationof
theMarkovchain.Asubsamplesizeof1%ofthefulldatasetisusedforall3.5 GuidanceforEfficientScalableBayesianLearning 99
0.9
0.8
0.7
0.6
SGLD
0.5 SGLD-CV
SGHMC
SGHMC-CV
0.4
0 50 100 150 200
Number of Iterations
Figure3.8 Averageposteriorpredictiveaccuracyoverallclasses
foreachoftheSGMCMCsamplers.
SGMCMC samplers. For the control-variate-based algorithms, a stochas-
tic gradient descent algorithm is used to find the posterior mode and the
Markovchainisinitialisedattheposteriormode.
There is a separate set of 10000 unseen images, also with correspond-
ing labels but hidden from the training process. This test set allows us to
evaluate how well our neural network performs on new data. By feeding
these new images into the network, we can see if the posterior network is
able to accurately classify the images into one of the ten-digit categories
(0-9). Figure 3.8 shows the posterior predictive accuracy for each SGM-
CMC sampler over the number of Monte Carlo iterations (storing every
10thposteriorsample).Theresultsshowthatallofthesamplersconverge
toapproximately93%accuracyinclassifyingtheMNISTtestsetdigits.The
SGLD and SGHMC samplers converge at a similar rate (in terms of pre-
dictiveaccuracy).Thecontrolvariate-basedSGLDandSGHMCsamplers
also converge at a similar rate to each other but achieve higher predictive
accuracywithfewerMonteCarloiterationsasthesesamplersareinitialised
attheposteriormodeandthusremovetheburn-inphaseoftheMonteCarlo
sampler.
ycarucca
evitciderP100 StochasticGradientMCMCAlgorithms
3.6 GeneralisationsandExtensions
TheSGMCMCframeworkoutlinedinSection3.4canbeextendedbeyond
the SGLD algorithm to improve Markov chain mixing. However, two key
assumptionslimittheapplicabilityofcurrentSGMCMCalgorithms:(i)the
parameters𝜽 ∈ R𝑑existinanunconstrainedspace,and(ii)thelog-posterior
densitylog𝜋(𝜽) isasumoverconditionallyindependentterms.
Assumption(i)precludesestimating 𝜽 onaconstrainedspace(e.g. 𝜽 ∈
[0,1]𝑑). Assumption (ii) requires data y 1,...,y𝑁 to be independent or
have limited dependence, restricting the applicability of SGMCMC for
timeseriesorspatialmodels.
OngoingresearchaimstorelaxtheseassumptionsandexpandSGMCMC
tobroadermodelclasses.Somepromisingdirectionsinclude:
• Transformationtechniquestoenablesamplingonconstrainedparameter
spaces(Brosseetal.,2017;Bubecketal.,2018;Hsiehetal.,2018).
• Exploitingshort-rangedependenciesandothermodelstructurestoallow
subsamplingfortimeseriesandnetworkdata(Lietal.,2016;Maetal.,
2017;Aicheretal.,2023).
• Leveragingalternativestochasticprocesseswithdesiredinvariantdistri-
butionsassamplersformodelswithcomplexdataandparameterstruc-
tures(Bakeretal.,2018).
Bydevelopingspecialisedsubsamplingschemesandtransformations,it
ispossibletomakeSGMCMCalgorithmsmoreapplicabletoawiderrange
ofBayesianmodelswhileretainingcomputationalefficiency.
3.6.1 ScalableInferenceforModelsinConstrainedSpaces
Manystatisticalmodelscontainparameterswithinherentconstraints,such
as the variance parameter 𝜏2 in a Gaussian distribution (𝜏 ∈ R+) or the
success probability 𝑝 in a Bernoulli model (𝑝 ∈ [0,1]). Simulating these
constrainedparametersusingstandardLangevindynamics(3.2)willoften
producesamplesviolatingtheconstraints.Forinstance,ifatiteration 𝑘 of
theSGLDalgorithm𝜽𝑘 = 𝜏 𝑘2isclosetozero,thenwithhighprobability,the
nextiterate𝜽𝑘+1 islikelytobenegative,breakingthepositivityconstraint.
One solution is to shrink the step size 𝛿 → 0 as 𝜏2 → 0, but this leads to
poormixingneartheboundary.
A natural approach is to transform the Langevin dynamics so sampling
occursinanunconstrainedspace,butthechoiceoftransformationgreatly3.6 GeneralisationsandExtensions 101
impacts mixing near the boundary. Alternatively, we can project the dy-
namicsintotheconstrainedspace,however,thisyieldspoorerconvergence
compared to the unconstrained setting. The mirrored Langevin algorithm
(Hsiehetal.,2018)wasproposedtoaddressthisissue.Itbuildsonthemir-
rored descent algorithm (Beck and Teboulle, 2003) from the optimisation
literaturetotransformconstrainedsamplingintoanunconstrainedproblem
usingamirrormap.Comparedtoagenericmappingfunction,mirrormaps
haveadditionalproperties,suchasstrictconvexity,differentiabilityanddi-
verginggradientsattheboundaryofthedomain,whichmakesmirrormap
algorithmswell-suitedtoconstrainedsamplingandoptimisationproblems.
Ifweassumethat𝜋(𝜽)isthedensityofaconstraineddistribution,namely
thatlog𝜋(𝜽)hasaboundedconvexdomain,thenassumingthatthereexists
a mirror map 𝜓(·) which is closed and proper, we can map the variable
𝜽 ∼ 𝜋 from the constrained space (primal space) to the unconstrained
space (dual space), where 𝝑 := ∇𝜓(𝜽) and 𝝑 ∼ 𝜈. Under the assumption
that 𝜓 is a convex function that is closed, proper, and twice continuously
differentiable, with Fenchel dual noted as 𝜓∗, then Theorem 1 of Hsieh
etal.(2018)showsthat∇𝜓∗(𝝑) ∼ 𝜋.Thisresultimpliesthatitispossible
to transform the problem of sampling from a constrained distribution 𝜋
to the simpler problem of sampling from an unconstrained distribution 𝜈.
Usingthealgorithmshighlightedinthischapter,wecansimulateaMarkov
chain 𝝑 ∼ 𝜈 and apply the mapping ∇𝜓∗(𝝑) to produce samples from the
desiredposteriordistribution𝜋.InthecaseoftheLangevinsampler,thisis
achievedbymodifyingtheoriginalLangevindiffusion(3.1)toamirrored
Langevindiffusion,
1
d𝝑 = (∇log𝜈◦∇𝜓)(𝜽)d𝑡+d𝑊 𝑡 (3.29)
2
𝜽 = ∇𝜓∗(𝝑). (3.30)
Inpractice,asnotedearlierinthischapter,itisnotpossibletosimulateex-
actlyfromthemirrorLangevindiffusion.UsingthesameEuler–Maruyama
discretisationschemeitispossibletocreateapracticaldiscrete-timealgo-
rithm.StochasticgradientimplementationsofthemirrorLangevindynam-
icsareeasilyderivedinthedualspaceandfollowdirectlyfromtheSGLD
algorithm.
One popular model that requires sampling from a constrained domain
is the latent Dirichlet allocation (LDA) model (Blei et al., 2003) which is
used for topic modelling. Here, the model parameters are constrained to
the probability simplex, i.e. 𝜃 𝑖 ≥ 0,𝑖 = 1,...,𝑑 and (cid:205) 𝑖𝑑 =1𝜃 𝑖 = 1. Mir-
roredLangevindynamics(3.29)canbeusedtosimulatefromthesimplex102 StochasticGradientMCMCAlgorithms
distribution by mapping the parameters to R𝑑 and running the Langevin
dynamicsalgorithmontheunconstrainedspace.Theentropicmirrormap
(Beck and Teboulle, 2003) satisfies the required assumptions for a valid
mapfunctionunderthemirroredLangevindynamics,
𝑑 (cid:32) 𝑑 (cid:33) (cid:32) 𝑑 (cid:33)
∑︁ ∑︁ ∑︁
𝜓(𝜽) = 𝜃 𝑖log𝜃 𝑖 + 1− 𝜃 𝑖 log 1− 𝜃 𝑖 , (3.31)
𝑖=1 𝑖=1 𝑖=1
where0log0:=0.Thetransformedlog-posteriordensityisthengivenby
𝑑
∑︁
log𝜈(𝝑) =log𝜋(𝜽)◦∇𝜓∗(𝝑)− 𝜗 𝑖 +(𝑑+1)𝜓∗(𝝑), (3.32)
𝑖=1
where 𝜓∗(𝝑) = log(1+(cid:205) 𝑖𝑑 =1exp(𝜗 𝑖)) is the Fenchal dual of 𝜓(·) and is
strictly convex with Lipschitz gradients. Aside from transforming a con-
strainedsamplingproblemintoanunconstrainedsamplingproblem,mirror
mapscanalsoleadtosimplerposteriordistributionsinthedualspace.For
example, the Dirichlet posterior distribution introduced above, leads to a
posteriordistributiononthedualspace(3.32)whichisstrictlylog-concave.
Insteadofusingmirrormapstosamplefromtheposteriordistributionof
the LDA model, we could instead use the stochastic gradient Riemannian
Langevin dynamics algorithm (see Section 3.4). Under the SGRLD algo-
rithm, the constrained parameters 𝜽 can be transformed to R𝑑 via several
alternative reparameterisations (see Patterson and Teh (2013) for a list).
However, this can induce asymptotic biases dominating the boundary re-
gions. An alternative approach is to recognise that the LDA posterior can
beexpressedasatransformationofindependentgammarandomvariables.
Therefore, rather than simulating from the simplex distribution via the
Langevindiffusion,onecouldinsteadutilisetheCox–Ingersoll–Ross(CIR)
process (Cox et al., 1985), which is invariant with respect to the gamma
distribution. This CIR-based approach (Baker et al., 2018) avoids bound-
ary biases. More broadly, leveraging alternative stochastic processes with
desired invariant distributions can enable specialised samplers for models
withcomplexstructures.
3.6.2 ScalableInferencewithTimeSeriesData
A key requirement for developing stochastic gradient algorithms is the
abilitytogenerateunbiasedestimatesof∇log𝜋(𝜽)usingdatasubsampling,
as in (3.6). This is straightforward when the log-posterior density, and
its gradient, are expressed as a sum of log𝜋 𝑖(𝜽) and ∇log𝜋 𝑖(𝜽) terms,3.6 GeneralisationsandExtensions 103
respectively, i.e. log𝜋(𝜽) = (cid:205) 𝑖𝑁 =1log𝜋 𝑖(𝜽). Randomly subsampling these
termsprovidesanunbiasedlog-densityandgradientestimate.
However, for models where data are not conditionally independent, for
example,networkdata,timeseries,orspatialdata,thelog-posteriordensity
cannotbeexpressedasasimplesum.Naivelysubsamplingwillyieldbiased
estimatesoflog𝜋(𝜽)and∇log𝜋(𝜽).Capturingbothshort-andlong-range
dependencies in spatial data with subsampling remains an open challenge
forSGMCMC.Fornetworkdata,Lietal.(2016)developedanSGMCMC
algorithm for the mixed-membership stochastic block model using block
structureandstratifiedsubsamplingtoobtainunbiasedgradientestimates.
RecentworkinSGMCMCfortemporallycorrelateddatahasfocusedon
hiddenMarkovmodels(Maetal.,2017)withfinitestates,lineardynamical
systems(Aicheretal.,2019)andgeneralnonlinearhiddenMarkovmodels
(Aicheretal.,2023).Underthismodellingframework,thehiddenMarkov
modelconsistsoftwostochasticprocesses:i)alatentstateprocess{X𝑡}𝑇 𝑡=0,
whichisaMarkovchainthatevolvesovertime𝑡 =1,...,𝑇,withX𝑡depend-
ingonlyonX𝑡−1 and𝜽,withtransitiondensitygivenby 𝑝(x𝑡|x𝑡−1,𝜽);and
ii)anobservedprocess{Y𝑡}𝑇
𝑡=0
thatisconditionallyindependentgiventhe
latentstatesand𝜽,whichareobservedwithprobabilitydensity𝑝(y𝑡|x𝑡,𝜽).
Assumingmodelparameters𝜽,thefullgenerativemodel(Figure3.9)is
X𝑡|(X𝑡−1 =x𝑡−1,𝜽) ∼ 𝑝(x𝑡|x𝑡−1,𝜽) (3.33)
Y𝑡|(X𝑡 =x𝑡,𝜽) ∼ 𝑝(y𝑡|x𝑡,𝜽). (3.34)
The latent Markov chain X𝑡 captures the dynamics and temporal depen-
dence, while the observations Y𝑡 depend only on the current state of the
latent process. Hidden Markov models are useful for modelling complex
time series data byaugmenting the observables with latent states. Itis of-
tencommontodistinguishbetweenhiddenMarkovmodelsandstate-space
models,whereinthecaseoftheformerthelatentprocessX𝑡 isdiscreteand
iscontinuousinthecaseofthelatter.However,forthesakeofconvenience,
weshallusethetermhiddenMarkovmodeltocoverbothmodeltypes.
UsingSGMCMCmethods,itispossibletoestimatethemodelparameters
𝜽 for general hidden Markov models which exhibit temporal dependency
in the observations. As before, the goal is to sample from the posterior
distribution 𝜋(𝜽) := 𝑝(𝜽|y) where y = {y 1,...,y𝑇} is the observed data
sequence, which is proportional to the product of the likelihood 𝑝(y|𝜽)
and the prior 𝜋 (𝜽). The likelihood 𝑝(y|𝜽) typically cannot be evaluated
0
exactly,asitrequiressumming(discretesetting)orintegrating(continuous
setting)overthelatentstatesX.Focusingonthecontinuousvariablesetting,104 StochasticGradientMCMCAlgorithms
X
0
X𝑡−2 X𝑡−1 X𝑡 X𝑡+1 X𝑡+2 X𝑇
y
0
y𝑡−2 y𝑡−1 y𝑡 y𝑡+1 y𝑡+2 y𝑇
S S∗
Figure3.9 GraphicalrepresentationofthehiddenMarkovmodel
withlatentvariablesX𝑖 andobservationsy𝑖.Thesubsequenceis
capturedinthesolidboxSandthebufferregionishighlightedby
thedottedboxS∗.
the latent states can be integrated out numerically using particle filtering
techniques(Doucetetal.,2009).UsingaparticleapproximationofFisher’s
identity(Nemethetal.,2016),
∇ 𝜽log𝜋(𝜽) = ∇ 𝜽log𝑝(y 1:𝑇|𝜽) =E X|Y,𝜽 [∇ 𝜽log𝑝(X 1:𝑇,y 1:𝑇|𝜽)] (3.35)
𝑇
∑︁
= E
X|Y,𝜽
[∇ 𝜽log𝑝(X𝑡,y𝑡|x𝑡−1,𝜽)]
𝑡=1
it is possible to unbiasedly approximate ∇ log𝜋(𝜽) by replacing the pos-
𝜽
terior distribution of the latent states 𝑝(x 1:𝑇|y 1:𝑇,𝜽) with a numerical ap-
proximationrepresentedbyasetof𝑃particles{X(𝑝)}𝑃
.
1:𝑇 𝑝=1
CalculatingFisher’sidentitycanbecomputationallyexpensiveforlarge
𝑇 andsoanSGMCMCapproximationtoFisher’sidentitycanbeusedtore-
placethefulldatagradient(3.35)withastochasticapproximationestimated
from a random subset of the data. This allows each gradient evaluation to
be cheaper, enabling more MCMC iterations and better convergence for
large 𝑇. However, naively subsampling the data randomly (3.6) induces
bias since it breaks temporal dependencies. To address this issue, Aicher
etal.(2019,2023)proposetosubsamplecontiguoussubsequences.Figure
3.9providesagraphicalrepresentationofthehiddenMarkovmodelwhich
alsohighlightsthesubsampleddataasacontiguoussubsequenceS ofsize
𝑚.ThestochasticgradientfollowingFisher’sidentity(3.35)isthen
∇ 𝜽(𝑚)log𝑝(y 1:𝑇|𝜽) =∑︁ Pr(𝑡 ∈ S)−1·E
X|yS∗,𝜽
[∇ 𝜽log𝑝(X𝑡,y𝑡|x𝑡−1,𝜽)].
𝑡∈S
(3.36)
However, the stochastic gradient estimator (3.36) is biased because the
expectation is taken over the latent state which is dependent only on y
S3.7 ChapterNotes 105
and not y 1:𝑇. This bias can be reduced by extending the subsequence S
to include a buffered region S∗. The expectation in (3.35) is then taken
overthewiderbufferedrange 𝑝(x𝑡|y S∗,𝜽) ratherthan 𝑝(x𝑡|y S,𝜽).Results
from Aicher et al. (2019) show that under Lipschitz assumptions on the
model(3.33),anditsgradients,theerrorinthestochasticgradientsdecays
geometricallywiththebuffersize |S∗|.
3.7 ChapterNotes
MarkovchainMonteCarloalgorithmshavebeenthecornerstoneofBayesian
inferencesincethe1990s.However,asthesizeofdatasetshasgrown,the
requirement that each MCMC update uses all of the data to approximate
theposteriordistributionhascreatedacomputationalchallenge.Therehas
beensignificantrecentinterestintheStatisticsandMachineLearningcom-
munitiestocreatenewMonteCarlo-basedalgorithmsforscalableBayesian
inferenceinthepresenceoflargedatasets(Bardenetetal.,2014).Broadly
speaking,scalableMCMCalgorithmstendtofallintotwocategorieswhich
either (i) subsample the data (Welling and Teh, 2011; Chen et al., 2014;
NemethandFearnhead,2021),ascoveredinthischapter,or(ii)useparallel
computingtodistributethecomputationalcostacrossmultipleCPUs(Scott
etal.,2016;NemethandSherlock,2018;Vyneretal.,2023).
In the context of data subsampling, the per iteration cost of the Monte
Carloalgorithmisreduced.However,thisreductionincomputationalcostis
onlybeneficialifthesubsamplingschemeleadstoposteriorapproximations
with high statistical accuracy. Chatterji et al. (2018) gives results on the
numberofiterations,andresultingcomputationalcost,requiredfordifferent
stochasticgradientalgorithmstoproducesamplesfromadistributionwhich
iswithinaspecified“distance”of 𝜋.Otherworks(Bierkensetal.,2019b;
HugginsandZou,2017;Pollocketal.,2020)havestudiedthecomputational
andstatisticaltrade-offsthatresultfromapproximateandscalableMCMC
schemes,whichareoftenreferredtoasexact-approximatealgorithms.Itis
oftenthecasewithscalableMCMCalgorithmsthatthereisnofreelunch
and simple naive subsampling alone does not lead to statistically efficient
algorithms,seeforexampleJohndrowetal.(2020).Inthecaseofcontrol-
variatesforsubsampling,anumberoftheoreticalresults(e.g.Nagapetyan
etal.,2017;Bakeretal.,2019;Brosseetal.,2018)showthatifweignore
the pre-processing cost of finding (cid:98)𝜽, the computational cost per-effective
sample of SGLD with control variates is𝑂(1), rather than the𝑂(𝑁) cost
forSGLDwiththesimplegradientestimator(3.6).4
Non-Reversible MCMC
AreversibleMarkovchainisanyMarkovchainthatsatisfiesdetailedbal-
ance;seeSection1.3.Remember,thisconditionstatesthat,atstationarity,
theprobabilityofthechainstartinginasetB andmovingtosetC isequal
totheprobabilityofitstartinginthesetCandmovingtoB.Thismeansthat
thedynamicsoftheprocessarethesamebackwardsintimeasforwardsin
time.OneconsequenceofreversibilityisthattheMarkovchaincanexhibit
random-walkbehaviour,whereitcanreturntostatesorregionsofthestate
spacewhereithasrecentlybeen.
Anon-reversibleMarkovchainisanyMarkovchainthatdoesnotsatisfy
detailed balance. As we will see, the potential benefit of non-reversibility
isthattheMarkovchaincanmorequicklyexplorethestatespaceasitcan
suppress the random walk behaviour. However, designing non-reversible
Markovchainswiththerequiredstationarydistribution,orevendetermining
thestationarydistributionofanon-reversibleMarkovchain,ismuchmore
challenging than for a reversible chain. This chapter will describe one
approachtodesigningnon-reversibleMCMCsamplersbasedontheideaof
lifting–whichinvolvestakingareversibleMCMCschemeandthenlifting
it to a higher-dimensional state-space to enable the use of non-reversible
moves. These ideas naturally motivate the non-reversible continuous-time
MCMCsamplersofChapter5.
4.1 TheBenefitsofNon-Reversibility
To see the benefits of non-reversible Markov chains, we will consider the
followingsimpleexample.
Example4.1 Let 𝑋 =0and
0
𝑋
𝑘
= (𝑋 𝑘−1+𝐽 𝑘)(mod𝑆),
sothat𝑋 𝑘followsarandomwalkon{0,...,𝑆−1}.Here𝐽 𝑘isthejumpsize,
1064.1 TheBenefitsofNon-Reversibility 107
whichcanhaveanydistributionon{−ℎ,...,ℎ}thatissymmetricabout0,
and (mod𝑆) meanswetaketheremainderafterdividingby𝑆.Weinclude
(mod𝑆),soarandomwalkthatmovestonegativevalues,orvaluesequal
to or above 𝑆, gets mapped back to {0,...,𝑆 −1}, with 𝑆 mapping to 0
and−1to𝑆−1andsoon.Forsimplicity,weconsider𝐽 𝑘 tohaveauniform
distribution on {−ℎ,−ℎ+1,...,ℎ}. It is straightforward to show that the
resulting Markov chain is reversible and has the uniform distribution on
{0,...,𝑆−1}asitsstationarydistribution.
First,considerℎ =1andlookatthebehaviourofthechainasweincrease
𝑆.InthetoprowofFigure4.1,weshowtraceplotsofthechainfor𝑆 =100,
𝑆 = 200 and 𝑆 = 400. In each case, we show the path of the chain over
40𝑆 iterations. What we observe is that as 𝑆 increases the chain becomes
muchsloweratexploringthestate-space.Wecanseethismoreclearlyifwe
look at the empirical marginal distribution of the chain after 𝑛 time steps.
Let 𝑏 be a chosen burn-in period, then for 𝑛 > 𝑏 the empirical marginal
distribution,whichisournaturalestimatorofthestationarydistributionof
thechain,is
𝑛
1 ∑︁
(cid:98)𝜋 𝑛(𝑖) = 𝑛−𝑏 I{𝑋 𝑘 =𝑖}. (4.1)
𝑘=𝑏+1
Thisisjusttheproportionoftime,afterburn-in,thatthechainwasinstate
𝑖.Wecanthencomparethisestimatewiththetruestationarydistribution,
by calculating the total variation distance between the two. This is just
(cid:205) 𝑖𝑆 =− 01| (cid:98)𝜋 𝑛(𝑖) −1/𝑆|, and is shown in the top-row of Figure 4.2, where we
showthetotalvariationdistanceagainst𝑛/𝑆andagainst𝑛/𝑆2 for𝑆 =100,
𝑆 = 200and𝑆 = 400.Weseeevidencethat,asweincrease 𝑆,thetimewe
needtorunourMarkovchainmustincreaseproportionallywith𝑆2tohave
thesamedegreeofaccuracy.
It is interesting to compare this with the performance of the following
non-reversible Markov chain, which we construct by making the random
walk biased; i.e., choosing a jump distribution 𝐽 𝑘 whose expectation is
non-zero.
Example4.2 InExample4.1,keep𝐽 𝑘 takingvaluesin{−1,0,1},butset
thejumpprobabilitiestobe {2/9,1/3,4/9} respectivelysothatapositive
jumpistwiceaslikelyasanegativeone.
WeshowtheresultingtraceplotsoftheMarkovchaininthebottomrow
ofFigure4.1.Qualitativelythetraceplotslookverydifferenttothoseinthe
toprow,asthepathstendtomoveupwardsateachiteration.Thisislinked108 Non-ReversibleMCMC
Figure4.1 TraceplotsoftwoMCMCalgorithmsforsampling
fromauniformdistributionon{0,...,𝑆−1}fordifferentvalues
of𝑆.Arandomwalkofstepsizeuniformon{−1,0,1}(Example
4.1,top),andanon-reversible(biased)randomwalkwhichis
twiceaslikelytohaveastepof1thanastepof-1(Example4.2,
bottom).Columnsarefor𝑆 =100(left),𝑆 =200(middle)and
𝑆 =400(right).WeshowthestateoftheMCMCalgorithmfor
40𝑆iterations(toprow)or4𝑆iterations(bottomrow).Forthis
scalingofiterations,weseethereversibleMCMCalgorithm
mixesmoreslowlyas𝑆increases,whereasqualitativelythe
mixingofthenon-reversiblealgorithmremainssimilar.
to the non-reversibility of the chain, as a realisation of the chain forward
intimewillnowlookverydifferentfromarealisationbackwardsintime.
Furthermore,therealisationsfordifferent 𝑆 looksimilar.Thatis,oncewe
scale the number of iterations by 𝑆, chains with different 𝑆 mix similarly.
This is shown quantitatively in the bottom left plot of Figure 4.2, where
we plot the total variation distance of the empirical marginal distribution
ofourchain(4.1)fromtheuniformdistribution,against𝑛/𝑆:thisisalmost
identicalforthethreevaluesof𝑆.
Why does the non-reversible chain have better mixing properties? In-
tuitively, the poor performance of the reversible chain is because it has
random-walk behaviour: it will often move up on one iteration and then
movedownonthenext.Thenon-reversiblechainsuppressesthisrandom-4.2 HamiltonianMonteCarloRevisited 109
walk behaviour as its bias means it will tend to move in the same direc-
tion.Thequalitativedifferencebetweenthesereversibleandnon-reversible
chains that we have demonstrated empirically, can be shown theoretically
(Diaconisetal.,2000).
For a simple heuristic of this behaviour, consider 𝑋 = ⌊𝑆/2⌋ and 𝑛 ≤
0
⌊𝑆/2⌋.Forthesymmetricrandomwalk,E[𝑋 𝑛−𝑋 0] =0and,sincethetotal
movementbyiteration𝑛isasumof𝑛independentmoves,Var[𝑋 𝑛−𝑋 0] ∝
𝑛,sothetypicalamountofmovementinthefirst𝑛iterationsisproportional
√
to 𝑛. However, for the biased random walk, E[𝑋 𝑛−𝑋 0] ∝ 𝑛 and so the
amountofmovementisroughlyproportionalto𝑛.
Importantly, when we measured the performance of the non-reversible
Markov chain we looked at the accuracy of (4.1), which is the proportion
oftimeitspendsineachstateaveragedovertime.If,instead,welookatthe
convergence of P(𝑋 𝑛 =𝑖) to 1/𝑆, we would obtain a very different result
tothatshowninFigure4.2,asthebiasoftherandomwalkinExample4.2
meansthatthecentreofthedistributionof𝑋 𝑛changeswith𝑛:thebenefitof
thenon-reversiblechainisonlyrealisedaswetaketheergodicaverageover
different time-points. This is most easily seen for the extreme case where
𝐽 𝑘 = 1 with probability 1. In that case, we find that the distribution of 𝑋 𝑛
isapointmassatasinglevalueforeach𝑛,butbyaveragingovertimewe
stillhavethat(4.1)convergestotheuniformdistributionatarateof1/𝑛.
Finally,asanaside,weobservethatthepoorperformanceofthereversible
chain is also linked to the fact that the size of the moves at each iteration
is small – this can be quantified in terms of the variance of 𝑋 𝑘 − 𝑋 𝑘−1
relativetothevarianceofthestationarydistribution.Anditisthefactthat
this ratio increased as we increased 𝑆 that meant that the reversible chain
performedrelativelypoorlyforlarger 𝑆.Toseethiswecanimplementthe
reversiblerandomwalk,butsetthemaximumstepsize ℎ = 𝑆/100soitis
proportionalto𝑆.Thetotalvariationdistancebetween(4.1)andtheuniform
distributionforsuchachainisshowninthebottomrightplotofFigure4.2,
anddemonstratesbetterscalingwith𝑆:infact,likethenon-reversiblechain,
as𝑆increaseswenowobtainthesameaccuracyprovidingwescale𝑛tobe
proportionalto𝑆.
4.2 HamiltonianMonteCarloRevisited
TheHamiltonianMonteCarlo,orHMC,algorithmofSection2.2isoften
viewed as a non-reversible MCMC algorithm. However, strictly it is a
reversiblealgorithm.
Remember that the HMC algorithm for sampling from a density 𝜋(𝜽),110 Non-ReversibleMCMC
       
S=400 S=400
S=200 S=200
    S=100     S=100
       
       
       
                             
Iterations/S Iterations/S2
       
S=400 S=400
S=200 S=200
    S=100     S=100
       
       
       
                           
Iterations/S Iterations/S
Figure4.2 Empiricaltotalvariationdistance(TVD)between
(4.1)andtheuniformdistributionagainstthenumberofiterations
anddifferentvaluesof𝑆:𝑆 =100(blue),𝑆 =200(red)and
𝑆 =400(black).TVDfortherandomwalkofExample4.1with
stepsize1(toprow)with𝑥-axisscaledby𝑆(topleft)or𝑆2(top
right).TVDforthenon-reversibleExample4.2(bottomleft),and
fortheExample4.1withstepsizescaledby𝑆(bottomright);in
bothcasethe𝑥-axisisscaledby𝑆.Allresultsareaveragedover
10realisationsofthechains.
involvesaugmentingthestateofourMCMCalgorithmwithamomentum
variableofthesamedimensionas𝜽.Denotethemomentumvariablebyp,
andthestateofourMarkovchainby(𝜽,p).Inthefollowing,forsimplicity,
we will assume that the mass matrix is the identity. We introduce a target
 ' 9 7
 ' 9 7
 ' 9 7
 ' 9 74.2 HamiltonianMonteCarloRevisited 111
densityfor (𝜽,p),
(cid:26) (cid:27)
1
𝜋˜(𝜽,p) ∝ 𝜋(𝜽)exp − p⊤p ,
2
whichhasindependentcomponents,with𝜽 from𝜋andphavingastandard
Gaussiandistribution.AnHMCalgorithmwhichhas𝜋˜(𝜽,p)asitsstation-
ary density interleaves the following three moves at each iteration. Given
currentstate (𝜽𝑘,p𝑘)
(1) SimulateanewmomentumpfromastandardGaussiandistribution.
(2)(i) Obtaintheproposedstate (𝜽′,p′) byrunningtheleapfrogdynamics,
for some number, 𝐿, of steps with some step length, 𝜖, starting from
(𝜽𝑘,p),andflipthefinalmomentum.
(ii) Accepttheproposedstate, (𝜽𝑘+1,p′′) = (𝜽′,p′) withprobability
(cid:18) 𝜋(𝜽′,p′)(cid:19)
min 1, ,
𝜋(𝜽𝑘,p)
otherwise (𝜽𝑘+1,p′′) = (𝜽𝑘,p).
(3) Flipthemomentum, (𝜽𝑘+1,p𝑘+1) = (𝜽𝑘,−p′′).
In the notation of Section 2.2, the proposal in step (2i) is denoted by
Leap
−𝐿(𝜽𝑘,p).
Move (2) involves simulating non-reversible Hamiltonian
dynamicstoproduceapotentiallylargeproposedmove,butthemoveitself
is reversible. As, trivially, are moves (1) and (3). The move (3) has no
net effect on the algorithm since the momentum is discarded at the next
iteration, and it is not included in the description in Section 2.2. It does,
however,ensurethat,iftheproposalisaccepted,thefinalmomentumisthe
sameasthatwhichwasobtainedthroughtheleapfrogapproximationtothe
Hamiltoniandynamicsratherthanitsopposite.Thiswillbehelpfulinthe
nextsection.
Interleaving three different reversible moves does not necessarily result
in a reversible Markov chain (see the next section, for example). But in
this case, it is straightforward to show that the marginal process for 𝜽
is a reversible Markov chain. The benefit of HMC is in the use of the
non-reversibledeterministicleap-frogdynamicstoproducelargeproposed
movesthathaveahighchanceofbeingaccepted.Aswesawintheprevious
section,theissueswithreversibleMCMCalgorithmsoccurwhenthestep
size is small – and HMC gets around this by being able to propose large
moves rather than being non-reversible. This fact is seen in the scaling
resultsforHMCasthedimensionofthestate-spaceincreases(seeSection
2.2andtheliteratureinSection2.3).112 Non-ReversibleMCMC
4.3 LiftingSchemesforMCMC
Whilst HMC is a reversible algorithm when viewed as a Markov chain
on 𝜽,someoftheideasbehindtheHamiltoniandynamicsarecommonto
non-reversibleMCMCalgorithms.Furthermore,itispossibletoadaptthe
HMCalgorithmsothatitisnon-reversible.
Mostnon-reversibleMCMCalgorithmsinvolvetheideaof“lifting”,that
is,definingtheMarkovchainonahigher-dimensionalstatespacethanyou
wish to sample from. Moreover, they tend to do this in a way similar to
HMC, in that if you wish to sample from some target distribution, 𝜋(𝜽),
you work with a Markov chain with state (𝜽,p), where p is of the same
dimension, 𝑑, as 𝜽 and can be interpreted as the momentum or velocity
that is describing the direction and speed with which the Markov chain is
currentlymovingthrough𝜽 space.Thenon-reversibilityofthealgorithmis
basedontryingtoencourageMarkovchainmovesthatcontinueinroughly
thesamedirectionoversuccessiveiterations.
4.3.1 Non-ReversibleHMC
Oneofthefirsttrulynon-reversiblealgorithmsisanextensionofHMCdue
toHorowitz(1991).Theideaistoadapthowthemomentumisrefreshedat
eachiterationsothatthemomentumatthecurrentiterationwillbesimilar
tothatatthepreviousiteration.Thiscanbeachievedbyreplacingstep(1)
oftheHMCalgorithmwiththeupdate
p′ = 𝛾p+(1−𝛾2)1/2𝜻,
where 𝜻 is a realisation of a 𝑑-dimensional standard Gaussian random
variable, and 0 < 𝛾 < 1. If p has a standard Gaussian distribution, then
so does p′: it is Gaussian as it is a linear combination of two independent
Gaussianrandomvariables,theexpectationoftheright-handsideis0,and
thevarianceis𝛾2I𝑑+(1−𝛾2)I𝑑 =I𝑑.If𝛾iscloseto1thenp′willbeclose
top.TheoverallalgorithmisgiveninAlgorithm4.
This algorithm has the required stationary distribution, as each step
satisfiesdetailedbalancewithrespecttotheextendedposterior,𝜋.However,
(cid:101)
whilst each step of the algorithm is a reversible Markov chain move, by
interleavingthemovesweobtainanon-reversiblealgorithm.
WhilstthisalgorithmgeneralisesHMC,inpracticeitisrarelynoticeably
moreefficient(seeforexampleNeal,2011).Thereasonisthemomentum
flipthatoccursifwerejectourproposal,as,if𝛾islargethissendsthechain
backinthedirectionfromwhenceitcame.Thusweneedtoeitherchoosethe4.3 LiftingSchemesforMCMC 113
Algorithm4:Non-reversibleHMCofHorowitz(1991)
Input:Density𝜋(𝜽),initialvalue𝜽 andmomentump sampled
0 0
fromN(0,I𝑑),andrefreshrate𝛾 ∈ [0,1).
for 𝑘 ∈ 0,...,𝑛−1do
Simulate𝜻 ∼N(0,I𝑑) andsetp′ = 𝛾p𝑘 +(1−𝛾2)1/2𝜻.
(𝜽′,p′′) ←Leap −𝐿(𝜽𝑘,p′).
Calculatetheacceptanceprobability:
(cid:18) 𝜋(𝜽′,p′′)(cid:19)
𝛼(𝜽𝑘,p′;𝜽′,p′′) :=min 1, (cid:101)
(cid:101)𝜋(𝜽𝑘,p′)
.
Withaprobabilityof𝛼(𝜽𝑘,p′;𝜽′,p′′) accepttheproposal,
(𝜽𝑘+1,p′′′) ← (𝜽′,p′′);otherwiserejectit,
(𝜽𝑘+1,p′′′) ← (𝜽𝑘,p𝑘).
Flipthevelocity: (𝜽𝑘+1,p𝑘+1) ← (𝜽𝑘+1,−p′′′).
end
leapfrogstepsize,𝜖,tobesmallenoughthattheprobabilityofacceptance
after 𝐿 stepsisusuallyverycloseto1,orweneedtochoose𝛾 tobesmall
so that the new momentum is relatively unrelated to the old momentum.
The first comes at a high computational cost while the second leads to
an algorithm that is very similar to standard HMC (which corresponds to
𝛾 = 0). Later, in Section 4.4, we will present some alternative ideas that
canalleviatetheproblemsofmomentumflipsinalgorithmssuchasthis.
4.3.2 Gustafson’sAlgorithmandMultidimensionalGeneralisations
It is possible to use similar ideas to obtain non-reversible versions of a
random walk Metropolis–Hastings algorithm, known as guided random
walks. This was first proposed by Gustafson (1998) for component-wise
updates, though it has a natural extension to multivariate updates which
we will also describe. First, consider the univariate case and a Gaussian
random walk proposal with a variance of 𝛿2 and 𝜁 ∼ N(0,𝛿2). Given a
currentvalue,𝜃,wecanwritethisproposalas
𝜃′ = 𝜃+ 𝑝|𝜁|,
where 𝑝 is uniformly sampled from {−1,1}. We can think of 𝑝 as the
directionofthemoveand|𝜁|asthesizeofthemove.TheideaofGustafson
(1998) is to augment the state of the chain with 𝑝 and to simulate a chain114 Non-ReversibleMCMC
suchthatthedirectionofthemoveislikelytobethesameoversuccessive
timesteps.Thiscanbeachievedbyiteratingthefollowingtwosteps
(1) Simulate𝜁,arealisationofaGaussianrandomvariable,andset(𝜃′,𝑝′) =
(𝜃 𝑘 + 𝑝 𝑘|𝜁|,−𝑝 𝑘) withprobability
(cid:26) 𝜋(𝜃′)(cid:27)
min 1,
𝜋(𝜃 )
𝑘
otherwiseset (𝜃′,𝑝′) = (𝜃 𝑘,𝑝 𝑘).
(2) Flipthedirection,so (𝜃 𝑘+1,𝑝 𝑘+1) = (𝜃′,−𝑝′).
Thestationarydistributionofthisalgorithmhasindependentdistributions
for 𝜃 and 𝑝, with 𝜃 from the distribution whose density is proportional to
𝜋(𝜃),and 𝑝 havingauniformdistributionon{−1,1}.Thisfollowsasboth
steps(1)and(2)arereversiblemovesthatkeepsuchadistributioninvariant.
To see the behaviour of this algorithm, and compare it to a standard
random walk Metropolis–Hastings algorithm, we show results of both
algorithms when sampling from a Gaussian target distribution in Figures
4.3 and 4.4. For Figure 4.3, we implement both algorithms using a small
proposal variance, 𝛿2. Here we see the poor mixing of the random walk
Metropolis due to its reversibility and the random walk behaviour of its
output.Thismeansthatittakesnearly1000stepstoreachthemodeofthe
target distribution. By comparison, Gustafson’s algorithm suppresses this
random walk behaviour, with large periods of time spent moving in the
samedirection.Aswearesamplingfromauni-modaltarget,thequalitative
dynamics of the algorithm are as follows: when it is moving towards the
mode the acceptance probability is 1 and the algorithm keeps moving in
thatdirection.Itisonlywhenitismovingawayfromthemodethatithas
anychanceofrejectingaproposalandswitchingthedirectionofitsmove.
The behaviourof thetwo algorithmswhenwe usea largerstep size,as
shown in Figure 4.4, is very different. In this case, both trace plots look
qualitativelysimilar,andbothsamplershavesimilarlygoodperformanceas
shownbytheauto-correlationplots.Thereasonisthatforalargestepsize,
the chances of rejecting the proposal and switching the direction are now
muchhigher,closerto0.5onaverage.Thisalsotiesinwiththeobservation
from Section 4.1 that reversibility is only an issue when the step sizes are
small.
If we wish to sample from a multivariate target 𝜋(𝜽), we can do so
byapplying thisupdate component-wise.Thatis, weaugment thestateto
(𝜽,p) where p is 𝑑-dimensional and each entry of p is either 1 or −1 and
specifiesthedirectionofthenextupdateofthecorrespondingcomponent4.3 LiftingSchemesforMCMC 115
 5 : 0 +  * X V W D I V R Q
   
   
   
   
   
   
   
                     
 N k
       
       
       
       
       
       
       
               
 / D J  / D J
Figure4.3 ComparisonofrandomwalkMetropolis–Hastings
(leftcolumn)andtheguidedrandomwalk(rightcolumn)when
samplingfroma1-dGaussian.TheproposalisGaussianwitha
standarddeviationof0.1inbothcases.Toprowshowstraceplots,
andbottomrowshowstheestimatedauto-correlationofthe
chains.
of 𝜽. Then we have 𝑑 parts to each iteration of the algorithm where each
partusestheabovealgorithmtoupdateadifferentcomponentof𝜽.
One can see the similarity with the algorithm of Horowitz (1991). We
haveaugmentedthestatetoincludeacomponent,ofthesamedimensionas
𝜽,thatgovernsthedirectionoftheupdateoftheMarkovchain.OurMarkov
chain update is based on interleaving two reversible moves, but with the
resultingMarkovchainbeingnon-reversible.Finally,eachreversiblemove
involvesaflipofdirection;providingtheacceptanceprobabilityinstep(1)
 ) & $
θ k
 ) & $
θ k116 Non-ReversibleMCMC
 5 : 0 +  * X V W D I V R Q
   
   
   
   
   
   
                 
k k
       
       
       
       
       
       
       
                     
 / D J  / D J
Figure4.4 ComparisonofrandomwalkMetropolis–Hastings
(leftcolumn)andtheguidedrandomwalkalgorithm(right
column)whensamplingfroma1-dGaussian.Theproposalis
Gaussianwithastandarddeviationof2.38inbothcases.Toprow
showstraceplots,andbottomrowshowstheestimated
auto-correlationofthechains.
is high, then these cancel out and the chain is likely to move in the same
directionovermultipletime-steps.
Finally, one can implement the idea of Gustafson (1998) in a way that
jointly updates the 𝑑-dimensional state. This can be achieved by letting p
be a 𝑑-dimensional unit vector. Define a target distribution on (𝜽,p) that
istheproductof 𝜋(𝜽) andtheuniformdensityforponthe 𝑑-dimensional
sphere; we denote the density by U𝑑(p) and the surface of the sphere as
S 𝑑.Thistargetwillbekeptinvariantbythefollowingalgorithm,whereto
 ) & $
θ
k
 ) & $
θ
k4.3 LiftingSchemesforMCMC 117
aidthepresentationofthealgorithminSection4.4wereplacetherandom
|𝜁| withafixed,user-prescribed 𝛿 > 0.Hereandfortheremainderofthis
chaptertheproposalisadeterministic,1-1map,ratherthanadensity,and
weusethesymbolqratherthan𝑞.
Algorithm 5: Multi-dimensional guided random walk, with fixed
direction.
Input:Density𝜋(𝜽),initialvalue𝜽 andspeed𝛿 > 0,unitvector
0
p
0
sampledfromU𝑑.
for 𝑘 ∈ 0,...,𝑛−1do
Propose (𝜽′,p′) =q 1(𝜽𝑘,p𝑘) = (𝜽𝑘 +𝛿p𝑘,−p𝑘).
Withprobability
(cid:26) 𝜋(𝜽′)(cid:27)
𝛼 1(𝜽𝑘,p𝑘;𝜽′,p′) :=min 1,
𝜋(𝜽𝑘)
accepttheproposal, (𝜽𝑘+1,p′′) ← (𝜽′,p′);otherwiserejectit,
(𝜽𝑘+1,p′′) ← (𝜽𝑘,p𝑘).
Flipthevelocity: (𝜽𝑘+1,p𝑘+1) = (𝜽𝑘+1,−p′′).
end
The velocity flip does not update 𝜽, and p ∈ S 𝑑 ⇔ −p ∈ S 𝑑, so the
flip is reversible with respect to 𝜋(𝜽)U𝑑(p), which must, therefore, be
the stationary density. It is helpful to explore exactly why the first step
is also reversible with respect to this density. Suppose that (𝜽,p) has a
density of 𝜋(𝜽)U𝑑(p) and let B and C be disjoint sets in R𝑑 ×S 𝑑. Then
P((𝜽,p) ∈ B,(𝜽′,p′) ∈ C) is
∫ (cid:18) 𝜋(𝜽′)(cid:19)
𝜋(𝜽)U𝑑(p)min 1,
𝜋(𝜽)
1 B(𝜽,p)1 C(𝜽′,p′) d𝜽dp
∫ (cid:18) 𝜋(𝜽) (cid:19)
= 𝜋(𝜽′)U𝑑(p′)min 1,
𝜋(𝜽′)
1 B(𝜽,p)1 C(𝜽′,p′) d𝜽dp
∫ (cid:18) 𝜋(𝜽) (cid:19)
= 𝜋(𝜽′)U𝑑(p′)min 1,
𝜋(𝜽′)
1 B(𝜽,p)1 C(𝜽′,p′) d𝜽′dp′,
as required. Here, the second line follows by rearrangement and because
the algorithm forces p ∈ S 𝑑 ⇔ p′ ∈ S 𝑑. The third line follows from the
unitJacobianofthetransformationq .
1
While this algorithm keeps the target on (𝜽,p) invariant, the algorithm
is reducible; p can only take two values: p and −p , whilst 𝜽 is confined
0 0
to a discrete grid on the vector 𝜽 +𝛿p . It is straightforward to make the
0 0118 Non-ReversibleMCMC
   
   
   
   
   
                   
Figure4.5 Traceplotfor50000iterationsofthealgorithmof
Gustafson(1998)samplingfroma2-𝑑 densityconcentratedon
theunitcircle.Theheatmapshowsthetargetdensity(redisthe
regionofhighdensity),andtheblacklineshowsthetraceofthe
algorithm.
algorithmirreducibleindimension2andabovebyaddingoccasionalmoves
thatupdatep,forexample,thatsampleanewvalueofpfromU𝑑.
ComparisononaRing-shapedTarget
To see the benefits of these non-reversible algorithms, we compared the
guidedrandomwalkalgorithmwiththatofastandardrandomwalkMetropo-
lisalgorithmforsamplingfromadensitythatconcentratesontheperimeter
of a circle in 2-𝑑. This mimics a common challenge of sampling from
a density that concentrates near a lower-dimensional manifold within a
higher-dimensionalspace.WeimplementedAlgorithm5witharefreshof
thedirectionevery10iterations.
Trace plots for the non-reversible and reversible algorithms are shown
inFigures4.5and4.6,respectively.Eachalgorithmusesthesameaverage
step size. For good mixing in the manifold scenario, the step size needs
to be of the order of the width of the density orthogonal to the circle –
and thus is small relative to the size of the circle itself. As we have seen4.4 ImprovingNon-reversibility:DelayedRejection 119
   
   
   
   
   
                   
Figure4.6 Traceplotfor50000iterationsoftherandom-walk
Metropolisalgorithmsamplingfroma2-𝑑 densityconcentrated
ontheunitcircle.Theheatmapshowsthetargetdensity(redis
theregionofhighdensity),andtheblacklineisthetraceofthe
algorithm.
previously, using such a small step size leads to random-walk behaviour
for the reversible algorithm. By comparison, the non-reversible algorithm
is able to suppress this. The overall effect is much better mixing for the
non-reversible algorithm, which takes 50000 iterations to explore the full
extentofthering.Bycomparison,overthesamenumberofiterations,the
reversiblealgorithmhasonlyexploredthebottomhalfofthering;infact,
ittakessixtimesthisnumberofiterationstoexploretheentirering.
4.4 ImprovingNon-reversibility:DelayedRejection
The major source of inefficiency in Algorithm 5 is the net reversal of
momentum whenever the proposed new position and momentum (𝜽′,p′)
arerejected.Thesubsequentmomentumflipisdesignedtokeeptheprocess
movinginthesamedirectionasitwasatthestartoftheiteration;however,
it has the opposite effect when there is a rejection, causing the chain to
retrace its steps. The delayed-rejection method of Green and Mira (2001)120 Non-ReversibleMCMC
canbeappliedtoanyreversiblestepandprovidesanaturalmechanismfor
reducingtheprobabilityofrejectionasfollows:whenevertheoriginalstep
wouldhaverejectedtheproposal,anewvalueisproposed;theacceptance
probabilityforthisnewproposalisdesignedexactlysothatthecombination
of the rejected existing step and the potential new step satisfies detailed
balancewithrespecttotheintendedposterior.
The validity of the propose-accept/reject step of Algorithm 5 relies
on the Jacobian of q
1
being 1 and the fact that q 1(𝜽′,p′) ≡ q 1(𝜽𝑘 +
𝛿p𝑘,−p𝑘) = (𝜽𝑘,p𝑘). Following analogous constraints, we incorporate a
delayed-rejectionmoveasfollows:If(𝜽′,p′)isrejected,wecanmakeanew
proposalbasednotonlyonthecurrentstate,(𝜽𝑘,p𝑘)butalsoontheinitial,
rejectedproposal,(𝜽′,p′).Inthiscase,wenowconsiderthecurrentstateto
betheoriginalcurrentstateandtheinitial,rejectedproposal:(𝜽𝑘,p𝑘,𝜽′,p′);
we call this the full state. We then make a proposal for this full state;
i.e., we propose a new original state and a new initial, rejected proposal:
(𝜽′′,p′′,𝜽′′′,p′′′) =q 2(𝜽𝑘,p𝑘,𝜽′,p′),where(𝜽′′′,p′′′) = (𝜽′′+𝛿p′′,−p′′).
Tosimplifythenotationinthefollowing,wedefine
z𝑘 = (𝜽𝑘,p𝑘), z′ = (𝜽′,p′), z′′ = (𝜽′′,p′′) and z′′′ = (𝜽′′′,p′′′).
Wewilleitheracceptorrejecttheproposal(z′′,z′′′) =q 2(z𝑘,z′)forthefull
stateandwechoosetheprobabilityofacceptanceexactlysothatthemove
satisfies detailed balance with respect to the density of the current state
(whichimpliestheinitialproposal),includingthefactthatitwasrejected:
𝜋˜˜(z𝑘,z′) := 𝜋(𝜽𝑘)U𝑑(p𝑘)I[z′ =q 1(z𝑘)]{1−𝛼 1(z𝑘;z′)}.
By analogy with the Metropolis–Hastings algorithm we might expect this
acceptanceprobabilitytobe
(cid:20) {1−𝛼 (z′′;z′′′)}𝜋(𝜽′′)(cid:21)
𝛼 2(z𝑘,z′;z′′,z′′′) :=min 1, {1−𝛼1
1(z𝑘;z′)}𝜋(𝜽𝑘)
,
where for simplicity of presentation we have not included the indicator
functions I[z′′′ =q 1(z′′)] and I[z′ =q 1(z𝑘)] in the numerator and de-
nominator of the fraction, respectively, since by design these are both 1.
Indeed, subject to conditions on q , 𝛼 is correct. For the proposal and
2 2
accept/rejectsteptobevalid,q musthaveaJacobianof1,andmustsatisfy
2
q 2(z′′,z′′′) = (z𝑘,z′). The probability of being at z𝑘 with a deterministic
proposalofz′ thathasbeenrejected,andthenmovingtothisnewfullstate
is,therefore,
𝜋˜˜(z𝑘,z′)𝛼 2(z𝑘z′;z′′,z′′′),4.4 ImprovingNon-reversibility:DelayedRejection 121
which,bydesign,isequalto
𝜋˜˜(z′′,z′′′)𝛼 2(z′′,z′′′;z𝑘,z′).
Usingthetwoconstraintsonq andananalogousargumenttothatusedfor
2
Algorithm5,detailedbalanceis,therefore,satisfied.
4.4.1 TheDiscreteBouncyParticleSampler
Thecurrentstateisz𝑘,giventhis,z′ isfixedviaq 1.The1-1mapq
1
simi-
larly fixes the relationship between z′′ and z′′′ so the additional flexibility
this delayed-rejection formulation allows is the freedom to choose z′′ or,
equivalently,z′′′.
To make 𝛼 close to 1, a sensible aim is to choose (z′′,z′′′) such that
2
𝜋(𝜽′′) ≈ 𝜋(𝜽𝑘) and 𝛼 1(z′′;z′′′) ≈ 𝛼 1(z𝑘;z′). If we set 𝜽′′′ = 𝜽′, both of
theseconditionswillbesatisfiedif𝜋(𝜽′)/𝜋(𝜽) ≈ 𝜋(𝜽′)/𝜋(𝜽′′);i.e.
log𝜋(𝜽′)−log𝜋(𝜽′′) ≈log𝜋(𝜽′)−log𝜋(𝜽𝑘).
Taylorexpandingabout𝜽′,werequire𝛿g·p≈ 𝛿g·p′′,whereg= ∇log𝜋(𝜽′);
i.e. both p′′ and p should have approximately the same component in the
gradientdirection.Perhapsthemostnaturalwaytoachievethisisbysetting
p′′ = Ψ g(p) :=−p𝑘 +2(p𝑘 · (cid:98)g) (cid:98)g,
whereg=g/∥g∥,isthedirectionofthegradientoflog𝜋at𝜽′.Inthiscase,
(cid:98)
Ψ g(p′′) =p𝑘 −2(p𝑘 · (cid:98)g) (cid:98)g+2[(cid:8) −p𝑘 +2(p𝑘 · (cid:98)g) (cid:98)g(cid:9) · (cid:98)g] (cid:98)g=p𝑘.
Further,sinceΨ (·) isself-inverse,theabsolutevalueofitsJacobianmust
g
be1.Thefullproposalis,therefore,
(z′′,z′′′) =q 2(z𝑘,z′) = (cid:0) 𝜽′−Ψ g(p𝑘),Ψ g(p𝑘),𝜽′,−Ψ g(p𝑘)(cid:1).
However, since z′′′ plays no part in any subsequent movement and since
momenta do not figure in the acceptance probabilities, it can simplify
notationtothinkoftheproposalas
(𝜽′′,p′′) =q∗ 2(𝜽𝑘,p𝑘,𝜽′) := (cid:0) 𝜽′−Ψ g(p𝑘),Ψ g(p𝑘)(cid:1),
theinitialacceptanceprobabilityas
(cid:26) 𝜋(𝜽′)(cid:27)
𝛼 1∗(𝜽𝑘,𝜽′) :=min 1,
𝜋(𝜽𝑘)
,122 Non-ReversibleMCMC
andthesecondacceptanceprobabilityas
(cid:20) {1−𝛼∗(𝜽′′,𝜽′)}𝜋(𝜽′′)(cid:21)
𝛼∗(𝜽,𝜽′,𝜽′′) =min 1, 1 .
2 {1−𝛼∗(𝜽𝑘,𝜽′)}𝜋(𝜽𝑘)
1
ThefullalgorithmisgiveninAlgorithm6andillustratedinFigure4.7.
Algorithm6:DiscreteBouncyParticleSampler
Input:Density𝜋(𝜽),initialvalue𝜽 andspeed𝛿,unitvectorp
0 0
sampledfromU𝑑.
for𝑡 ∈ 0,...,𝑇 −1do
Propose (𝜽′,p′) =q 1(𝜽𝑘,p𝑘).
Withaprobabilityof𝛼∗(𝜽𝑘,𝜽′) accepttheproposal:
1
(𝜽𝑘+1,p𝑘+1) ← (𝜽′,p′).
Iftheproposalisnotacceptedthenpropose
(𝜽′′,p′′) =q∗(𝜽𝑘,p𝑘,𝜽′).
2
Withaprobabilityof𝛼∗(𝜽𝑘,𝜽′,𝜽′′) acceptthisproposal:
2
(𝜽𝑘+1,p𝑘+1) ← (𝜽′′,p′′),otherwise (𝜽𝑘+1,p𝑘+1) = (𝜽𝑘,p𝑘).
Flipthevelocity: (𝜽𝑘+1,p𝑘+1) = (𝜽𝑘+1,−p𝑘+1).
end
As with Algorithm 5, we can ensure that Algorithm 6 is irreducible by
refreshing the unit vector p. This could involve sampling p𝑘 ∼ U𝑑 every
𝑚 iterations for some integer 𝑚, or with probability 𝑝 on any iteration;
however,thisstillallowsforrejectionscausingthesamplertoexactlyretrace
the recent past. More usually, therefore, after every velocity flip step, the
followingupdateismade:
p𝑘+1 = ∥𝛾𝛾p p𝑘 𝑘++ 11 ++ √︁√︁ 1 1− −𝛾 𝛾2 2𝝃 𝝃∥,
where𝝃 ∼N(0, 𝑑1I𝑑) and0 ≤ 𝛾 < 1.
Considering the combined effect of the initial proposal, the delayed-
rejection step and the momentum flip on a hypothetical particle at 𝜽𝑘
with a velocity of p𝑘 moving along a level, frictionless surface provides
a useful insight into the behaviour of Algorithm 6. In the following we
defineR (p) := −Ψ (p) = p−2(p·g) g,whichisareflectionofpinthe
g g (cid:98) (cid:98)
hyperplaneperpendiculartog.
• If the initial proposal is accepted then the net effect is (𝜽𝑘+1,p𝑘+1) =
(𝜽𝑘 +𝛿p𝑘,p𝑘);theparticlemovesexactlyasitshouldoveratime𝛿.4.4 ImprovingNon-reversibility:DelayedRejection 123
θ
k 1
−
p
k 1
−
θ k θ00
p
k
p00
−
θ0
Figure4.7 Heuristicofconsecutivestepsfromthediscrete
bouncyparticlesampler.From(𝜽𝑘−1,p𝑘−1)theinitialproposalis
acceptedandthemomentumisflipped,leadingto(𝜽𝑘,p𝑘).The
initialproposalfromthispoint(𝜽′isshownbutp′ =−p𝑘 isnot)is
rejected.Thethicksolidlineshowsacontourof𝜋at𝜽′ withthe
tangentlineat𝜽′ showndashed.Thefullproposalincludes
(𝜽′′,p′′);thefigureshows−p′′ toemphasisehow,iftheproposal
isaccepted,withthesubsequentmomentumflippedthe
movementisanalogoustoabounceoffthetangenthyperplane.
• Iftheinitialproposalisrejected,butthedelayed-rejectionproposalisac-
cepted,thentheneteffectis(𝜽𝑘+1,p𝑘+1) = (cid:0) 𝜽𝑘 +𝛿p𝑘 +𝛿R g(p𝑘),R g(p𝑘)(cid:1) ;
theparticlemovesforwardforatime𝛿 to𝜽′ thenreflectsoffthehyper-
planeperpendiculartogandmovesforwardforanothertime𝛿.
• If both the initial step and the delayed-rejection step are rejected then
(𝜽𝑘+1,p𝑘+1) = (𝜽𝑘,−p𝑘);theparticlereversesdirection.
If it were not for the occasional full rejection, the path of the points
outputted from the algorithm would resemble a time discretisation of the
smoothpathofaparticlemovingalongafrictionlesssurfaceandoccasion-
allybouncingoffahardbarrierinthehyperplaneperpendiculartothelocal
gradient. For this reason, the algorithm is known as the Discrete Bouncy
ParticleSampler.Inthelimit,as𝛿↓0andthenumberofstepsisincreased
in proportion to 1/𝛿, this becomes a continuous-time algorithm known as
theBouncyParticleSampler,whichweshallmeetinSection5.3.1.124 Non-ReversibleMCMC
       
       
       
       
       
           
Figure4.8 Trajectoriesafter5000iterations(left)and50000
iterations(right)ofthediscretebouncyparticlesampleronthe
two-dimensionalringtargetofFigures4.5and4.6,andusingthe
samestepsize.
PerformanceontheRing-shapedTarget
We exemplify the improved trajectories of the discrete bouncy particle
samplerbyimplementingitonthesametwo-dimensionalring-shapedtarget
for which the guided random walk with a directional update every 10
iterations took 50000 iterations to explore (Figure 4.5) and the random
walkMetropoliswiththesamestepsizetook300000iterationstoexplore
(Figure4.6showsthefirst50000iterations).
Ourdiscretebouncyparticlesamplerusesthesamestepsizeastheearlier
algorithmsandsets𝛾sothatthedirectionoftravelisonlypartiallyforgotten
fromjustafteronebouncetojustbeforethenext(seeSherlockandThiery,
2022). Figure 4.8 shows the path of the discrete bouncy particle sampler
after5000(left)and50000(right)iterations,onthetwo-dimensionalring
target. Exploration of the full circle takes only a tenth of the number of
iterationsthattheguidedrandomwalkrequiresandasixtiethofthenumber
of iterations required by the RWM. The coverage after the full 50000
iterationsisalsomorecompletethanwhenusingtheguidedrandomwalk.
4.5 ChapterNotes
Perhaps the earliest theoretical results showing that non-reversible chains
havebettermixingpropertiesweregiveninDiaconisetal.(2000).There-4.5 ChapterNotes 125
sultsfromapre-printofthisworkwereextendedinChenetal.(1999).See
alsoNeal(2004)andSunetal.(2010)whichshowthatnon-reversiblesam-
plersreduceasymptoticvariance.MorerecentresultsaregiveninBierkens
(2016).
Thediscretebouncyparticlesampler,describedinSection4.4,isgiven
in Sherlock and Thiery (2022). Similar algorithms appear in Neal (2003)
andVanettietal.(2017).Thekeydifferenceistheuseof”externalbounces”
ratherthan”internalbounces”:reflectionhappensinthecurrentpointrather
than the proposed point, so that: (𝜽′′,p′′,𝜽′′′,p′′′) = (𝜽𝑘,Ψ g(p𝑘),𝜽𝑘 +
Ψ g(p𝑘),−Ψ g(p𝑘)), where g = ∇log𝜋(𝜽𝑘). Use of the bounce move that
is key to the success of the discrete bouncy particle sampler will be seen
within the continuous-time bouncy particle sampler described in the next
chapter. It has also been used in other discrete-time MCMC algorithms.
For example, the Hug move in Ludkin and Sherlock (2022) uses repeated
bouncesinthesamewaythatHMCusesrepeatedleapfrogsteps,withthe
resultthatthepathtotheproposalstaysclosetothecontourof𝜋onwhichit
started,andjustaswithHMC,foragivenintegrationtime,theacceptance
probabilitycanbemadeascloseto1asdesiredbyreducingthestepsize.
Analternativetotheliftingschemesdescribedinthischapteraremethods
thatadaptareversibleMarkovchaintoanon-reversibleonewithoutadding
additional states. The general approach is to find a loop of states, and
then adapt the probability flow around this loop such that the net flow of
probabilityateachstateispreserved.Forexample,ifwehavestates𝑖, 𝑗 and
𝑘 thenwecanreduceeachofthethreeprobabilitiesofmovingfrom𝑖to𝑖, 𝑗
to 𝑗 and 𝑘 to 𝑘 bytheminimumofthethreeandincreaseeachofthethree
probabilitytransitionsfrom𝑖to 𝑗, 𝑗 to𝑘 and𝑘 to𝑖bythesameamount,so
thattheinvariantdistributionofthechainisunchanged.Suchchangeshave
been described as adding vortices. These ideas are described in Sun et al.
(2010) and Turitsyn et al. (2011). See Suwa and Todo (2010) for related
ideas. These constructions are hard to adapt to general Markov chains,
particularly chainswithout discretestates, though seeBierkens (2016)for
an approach to adapt the Metropolis–Hastings acceptance probability to
introducenon-reversibility.5
Continuous-Time MCMC
The previous chapter introduced the idea of non-reversible MCMC, and
demonstrated that non-reversibility may help improve the Markov chain’s
mixing by suppressing random-walk behaviour. In this chapter, we now
presentaclassofnon-reversibleMCMCalgorithmsthatcantargetageneral
distribution, 𝜋(𝜽). These algorithms are different from standard MCMC
algorithmsinthattheyarebasedonsimulatingacontinuous-timeMarkov
chain. Furthermore, specific examples of these continuous-time MCMC
algorithms can be derived as continuous-time limits of the non-reversible
algorithmsweintroducedinthepreviouschapter.
As a way of motivating these algorithms, we will first look at this
continuous-time limit. The resulting continuous-time process is from a
class of processes called piecewise deterministic Markov processes. We
willintroducesomebackgroundonthisclassofprocesses,includingsome
detailsaroundhowwecansimulatetheircontinuous-timetrajectories,be-
foreintroducingexampleMCMCalgorithmsandvariousextensions.These
algorithms use gradient information, and unless stated otherwise, we will
assume that the target distribution 𝜋(𝜽) is differentiable everywhere (in
practice being continuous and differentiable almost everywhere is suffi-
cient).
Withinthischapter,wewillneedtorefertobothcomponentsofavector
andpossiblythestateofthevectoratdifferenttimes.Todistinguishbetween
these, we will use the convention that when both time and component are
needed, we subscript by time and superscript by the component. So, for
example,𝜃(𝑖),willbethe𝑖thcomponentofthestate,𝜽,attime𝑡.
𝑡
5.1 Continuous-TimeMCMCastheLimitofNon-Reversible
MCMC
To help build intuition for continuous-time MCMC, and to see how it
linkstodiscrete-timeMCMCalgorithms,wewillshowwecanderivethe
1265.1 Continuous-TimeMCMCastheLimitofNon-ReversibleMCMC127
continuous-time algorithm as the limit of a discrete-time algorithm as we
let the step size in the discrete-time algorithm to tend to 0. Here, we will
consider this limiting argument at an informal level, before presenting a
moreformaljustificationforcontinuous-timeMCMCmethods.
Wewillconsidersamplingfroma1-dimensionaltargetdistributionusing
a simplified version of the non-reversible guided random walk algorithm
thatwasintroducedinSection4.3.Oursimplificationistoassumethatthe
stepsizeateachiterationisfixed.Ourstatewillstillbe(𝜃,𝑝),with𝑝either
1or−1andspecifyingthedirectionofthenextproposedmove,andwewill
let the size of the move be equal to some fixed value 𝛿. Such an MCMC
algorithm would not be irreducible, as it could only explore values of the
state that are integer multiples of 𝛿 away from its initial value. However,
the algorithm will still have the correct invariant distribution; i.e. if we
simulate the initial state from the target distribution for 𝜃 and a uniform
distributionfor 𝑝,thenthiswillalsobethedistributionofthestateatany
future iteration. The issue of lack of irreducibility vanishes in the limit as
𝛿 →0.
Asareminder,ifthetargetdistributionofinterestis𝜋(𝜃)thentheMCMC
algorithmwilliteratethefollowingmoves
(1a) Proposeamovefrom(𝜃,𝑝)to(𝜃+𝛿𝑝,−𝑝).Acceptthiswiththestandard
Metropolis–Hastingsacceptanceprobability,whichsimplifiesto
(cid:26) 𝜋(𝜃+𝛿𝑝)(cid:27)
min 1,
𝜋(𝜃)
(1b) Movefrom (𝜃′,𝑝) to (𝜃′,−𝑝).
In step (1a), we propose a move of size 𝛿 in direction 𝑝. If we accept this
move,thenwewillflipthedirection 𝑝 inbothsteps(1a)and(1b)–sothe
directionwillbethesameatthenextiteration.Ifwerejectthemove,then
we will only flip 𝑝 in step (1b) and thus the direction of the move will be
flippedforthenextiteration.
Underthisframework,wecanthenconsiderletting𝛿 →0whileincreas-
ingthenumberofiterations𝑛.Thatiswefixavalue𝑠andforanynumber
ofiterations,𝑛 willset 𝛿 = 𝑠/𝑛.Wewillscaletimesothatthe𝑖thMCMC
transition will occur at time 𝑖𝛿, and define (𝜃 𝑡,𝑝 𝑡) to be the value of the
stateafterthe𝑖thMCMCtransitionfor𝑖𝛿 ≤ 𝑡 < (𝑖+1)𝛿.128 Continuous-TimeMCMC
Now,foreachmoveinstep(1a)therejectionprobabilityforsmall𝛿is
max{0,1−exp[log𝜋(𝜃+𝛿𝑝)−log𝜋(𝜃)]}
(cid:26) (cid:20) dlog𝜋(𝜃) (cid:21)(cid:27)
=max 0,1−exp 𝛿+𝑜(𝛿)
d𝜃
(cid:26) dlog𝜋(𝜃)(cid:27)
=max 0,−𝑝 𝛿+𝑜(𝛿),
d𝜃
assumingthat,forexample,thederivativeof𝜋(𝜃) iscontinuous.
Thusinourlimitas𝛿 →0,rejectionsinstep(1a)willoccuraseventsin
aPoissonprocessofrate
𝜆(𝜃 𝑡,𝑝 𝑡)
=max(cid:26)
0,−𝑝
𝑡dlog d𝜋 𝜃(𝜃 𝑡)(cid:27)
.
The dynamics between these events will be deterministic, with 𝑝 𝑡 being
constant and 𝜃 𝑡 changing as per a constant velocity model with velocity
𝑝 𝑡. At each event, the velocity will just flip. While the process is moving
to areas of higher probability density, as defined by 𝜋(𝜃), the rate of the
Poisson process will be 0. Thus events will only occur if the process is
movingtoareasoflowerprobabilitymass.
Howdoesthislimiting,continuous-timealgorithmcomparetothealgo-
rithmofGustafson(1998)?Wewillcomparewiththestandard,irreducible
versionofGustafson(1998)wherethestepsizeateachiterationisrandom.
We show trace plots for both algorithms for sampling from a mixture of
two Gaussians in Figure 5.1. The target distribution is an equal mix of a
Gaussianwithmean2andvariance1andwithmean0andvariance0.12.
Thiswaschosensothatwehavetwomodeswheredifferentstepsizeswould
beoptimal,whilststillallowingforsufficientoverlapofthemodesthatthe
chainswouldmixbetweenthem.
Wecanseethatqualitativelythetwotrace-plotsaresimilar.Bothmethods
produce zig-zag-like traces, as they will continue to move in the same
directionwhenmovingtoareasofhigherprobabilitydensity.However,the
continuous-timeprocesshasanumberofpotentialadvantages:
• If we could simulate the continuous-time trajectory directly, then it has
thebenefitofhavingfewereventswherethevelocitychanges(andwhere
the state needs to be updated) than iterations of Gustafson’s algorithm
– in our example, there are around 200 events in the continuous-time
algorithmascomparedto1000iterationsofGustafson’salgorithm.
• Italsohasthebenefitoflesstuning,asnostepsizeneedstobespecified.5.2 PiecewiseDeterministicMarkovProcesses 129
 
 
 
 
   
 
 
   
                   
 , W H U D W L R Q  k   7 L P H  t 
Figure5.1 TraceplotsofGustafson’salgorithm(left)andthe
continuous-timelimit(right)forsamplingfromamixtureof
Gaussians.
• Finallyithastheadvantagethatthefullcontinuous-timesamplepathcan
beusedtocalculateMonteCarloaverages.
However,directlysimulatingthecontinuous-timetrajectoryisnotnormally
possible – and it is both the difficulty with simulating the continuous-
time process and the additional computational overhead of doing so that
arethemaindisadvantages.Furthermore,accountingfortheverydifferent
computationalcosts,periterationversusperunitofcontinuoustime,makes
thealgorithmshardtocomparetheoretically.Wewillreturntoissuesaround
simulatingcontinuous-timeMarkovprocesseslikethisbelow.
5.2 PiecewiseDeterministicMarkovProcesses
Thecontinuous-timelimitingprocesswederivedintheprevioussectionis
anexampleofapiecewisedeterministicMarkovprocess,orPDMP.These
areMarkovprocessesthatevolvedeterministicallybetweenasetofrandom
events.Wewillnowintroducetheseprocessesbeforeconsideringhowthey
canbedesignedandusedtosamplefromatargetdistributionofinterest.
5.2.1 WhatisaPDMP?
WewilldenotethestateofaPDMPattime𝑡 byz𝑡.APDMPisdefinedby
thefollowingproperties:
x k x t130 Continuous-TimeMCMC
(i) The deterministic dynamics. The PDMP evolves deterministically be-
tweenasetofeventtimes.WeconsiderPDMPswherethedeterministic
dynamicsarespecifiedthroughanordinarydifferentialequation
d dz 𝑡𝑡 = 𝝓(z𝑡), (5.1)
for some gradient field 𝝓(·). We will also define 𝚽 to be the transition
function, or flow map, for these dynamics, so for 𝑠 > 0 the solution to
thedifferentialequationsatisfiesz𝑠+𝑡 =𝚽(z𝑡,𝑠).
(ii) Theeventrate.Randomeventsoccuratarate𝜆(z𝑡) thatdependsonthe
currentstate.
(iii) Thetransitionkernelatevents.Ateachevent,attime𝜏,thestatechanges
accordingtosomeMarkovtransitionkernelQ(Z𝜏 ∈ ·|z𝜏−),where
z𝜏− =limz𝜏−𝜖
𝜖↓0
isthevalueofthestateimmediatelybeforetheevent.ThatisQ(Z𝜏 ∈ ·|z𝜏−)
definestheconditionalprobabilityofmovingtoset·,giventhestateim-
mediatelybeforetheevent.
Tosimplifyexposition,andbecausethisisconsistentwiththePDMPswe
willuseforMCMC,wewillprimarilyfocusondiscretetransitionkernelsat
events,andlet𝑞(·|z) denotetheprobabilitymassfunctionassociatedwith
thetransitionkernel.Thoughtheideasbelowextendnaturallytocontinuous
transitionkernels.
AsthedeterministicdynamicsareMarkov,andtheeventrateandtransi-
tiondensitydependjustonthecurrentstate,thentheresultingprocesswill
beMarkov.
5.2.2 SimulatingPDMPs
To be able to use a PDMP as the basis of a sampling algorithm, we will
needtobeabletosimulateandstorerealisationsofthePDMP.First,wecan
store a realisation of the continuous-time path of a PDMP by storing just
theinitialstateofthePDMP,andthetimeandeventimmediatelyaftereach
event.Wewillcallthesepointstheskeletonoftherealisation.Ifwesimulate
aPDMPuptotime𝑇 thenitsskeletonwillbeoftheform{𝑡 𝑘,z𝑡 𝑘}𝑛 𝑘=0,where
𝑡 0 = 0, and 𝑡 1,...,𝑡 𝑛 are the event times of the PDMP prior to 𝑇. Given
this skeleton, we can fill in the continuous-time path using the transition
functionforthedeterministicdynamics:
z𝑡 =𝚽(z𝑡 ,𝑡−𝑡 𝑘),where𝑡 𝑘 isthelargestskeletontimelessthan𝑡.
𝑘5.2 PiecewiseDeterministicMarkovProcesses 131
ThussimulatingaPDMPwilljustrequiretheabilitytosimulatetheskeleton
points.Ifweassumethatsimulatingfromthetransitiondensityateventsis
straightforward,thentheonlychallengewillbesimulatingtheeventtimes
themselves.Wecandothisbyusingthefollowingargumentthatshowsthat
thetimeuntilthenexteventcanberecastasthetimeuntilthefirsteventin
atimeinhomogeneousPoissonprocess.
To simplify notation we will consider simulating the time of the first
event, and denote the initial state as Z = z. By the Markov property, the
0
same approach can then be applied to simulating subsequent events: as if
the current state is Z𝑡 = z then the subsequent time until the next event
is the same as the time until the first event if we start the process at state
Z =z.
0
If there has not been an event by time 𝑡, then due to the deterministic
dynamics between events we know that the state at time 𝑡 will be z𝑡 =
𝚽(z,𝑡). The instantaneous rate of an event at time 𝑡, if there has been no
event before 𝑡, is thus 𝜆(z𝑡) = 𝜆(𝚽(z,𝑡)). Thus, the rate of the first event
of the PDMP is equal to the rate of the first event in an inhomogeneous
Poissonprocesswithrate
𝜆˜ (𝑡) =𝜆(𝚽(z,𝑡)).
z
Here,weusethetildesymboltodistinguishthisratefromtheratefunction,
𝜆, that depends on the state. We also subscript the rate by z, the initial
state.Asdescribedabove,bytheMarkovproperty,ifwehavesimulatedthe
PDMPuntiltime𝑠andthecurrentstateisz𝑠,thentherateofthenextevent
asafunctionofthefurthertime,𝑡,untilthenexteventwillbe𝜆˜ (𝑡).
z𝑠
Asaresult,wehavetransformedtheproblemofsimulatingaPDMPto
thatofsimulatingthefirsteventofatimeinhomogeneousPoissonprocess.
There are several methods for simulating such an event time (see e.g.
LewisandShedler,1979;Bouchard-Coˆte´ etal.,2018)andwewilloutline
threegeneralstrategiesfordoingso.Whetherthesecanbeimplementedin
practice depends on the form of 𝜆˜ , and we will return to this with some
z
exampleslater.
DirectSimulationbyInversion
In theory, one can simulate directly the time to the next event. Let 𝜏 be
therandomvariablethatisthetimeuntilthefirst(ornext)event.Standard
propertiesofaPoissonprocessgivethattheprobabilityofnoeventbytime
𝑡 is
(cid:26) ∫ 𝑡 (cid:27)
P(𝜏 > 𝑡) =exp − 𝜆˜ (𝑠)d𝑠 .
z
0132 Continuous-TimeMCMC
Foracontinuousrandomvariable, 𝑋,withdistributionfunction 𝐹 𝑋(·),we
have that the transformed random variable 𝐹 𝑋(𝑋) has a standard uniform
distribution. From this, we can simulate 𝑋 by simulating 𝑢, a realisation
ofastandarduniformdistributionandsetting𝑥 = 𝐹−1(𝑢).Thedistribution
𝑋
functionfor𝜏is1−P(𝜏 > 𝑡),thuswecansimulate𝜏from𝑢bysolving
(cid:26) ∫ 𝜏 (cid:27)
𝑢 =1−exp − 𝜆˜ (𝑠)d𝑠 .
z
0
Thiscanberearrangedto
∫ 𝜏
− 𝜆˜ (𝑠)d𝑠 =log(1−𝑢).
z
0
In practice, it is common to further simplify this expression using that if
𝑈 isastandarduniformrandomvariablethen−log(1−𝑈) hasastandard
exponentialdistribution.Thusif𝑤isarealisationofastandardexponential
randomvariablethen𝜏canbesimulatedasthesolutionof
∫ 𝜏
𝜆˜ (𝑠)d𝑠 = 𝑤. (5.2)
z
0
AsummaryofthisapproachisgiveninAlgorithm7.
Algorithm7:Directsimulationofeventtime
Input:Ratefunction𝜆˜ .
z
Simulate𝑤,fromastandardexponentialdistribution.
Find𝜏 ≥ 0the(smallest)solutionto
∫ 𝜏
𝜆˜ (𝑠)d𝑠 = 𝑤.
z
0
Output:𝜏.
Whether we can implement direct simulation depends on whether we
can solve (5.2). This is possible if 𝜆˜ (𝑠) is constant, linear, or piecewise
z
linear in 𝑠. It is also possible if it is some other simple function, such as
proportionaltotheexponentialofalinearfunctionof𝑠.
PoissonThinning
Whatifwecannotsimulatetheeventdirectlybyinversion?Inthiscase,the
most common approach to simulation is based on Poisson thinning. This
approach is based on the following property of Poisson processes: If we
have a Poisson process with rate 𝜆+(𝑠), and we simulate points from this5.2 PiecewiseDeterministicMarkovProcesses 133
Poissonprocessandthenaccepteachpointwithprobability𝛼(𝑠),thenthe
acceptedpointshavethesamedistributionaspointsfromaPoissonprocess
with rate 𝜆+(𝑠)𝛼(𝑠). As we are only keeping, i.e. accepting, some of the
simulatedpoints,thisiscalledathinnedpointprocess.
Poisson thinning inverts this property. For any rate function 𝜆+(𝑠) that
upperbounds𝜆˜ (𝑠),i.e.where𝜆+(𝑠) ≥ 𝜆˜ (𝑠)forall𝑠 ≥ 0,wecansimulate
z z
thetimeuntilthenexteventasthefirsteventofathinnedPoissonprocess.
ThisleadstoAlgorithm8
Algorithm8:SimulationofeventtimeviaPoissonthinning
Input:Ratefunctions𝜆+ and𝜆˜ ,with𝜆+(𝑠) ≥ 𝜆˜ (𝑠) forall𝑠 ≥ 0.
z z
Set𝑠 =0and𝜏 =0
while𝜏 =0do
Simulate𝑡,thetimeofthefirsteventafter𝑠inaPoissonprocess
withrate𝜆+.
Set𝑠 =𝑡.
Withprobability𝜆˜ (𝑡)/𝜆+(𝑡),set𝜏 =𝑡.
z
end
Output:𝜏,thefirsteventinaPoissonprocesswithrate𝜆˜ .
z
For this to work we need to be able to upper bound𝜆˜ by a simple rate
z
function,forwhichwecandirectlysimulateevents.Inpractice,thiswould
oftenbealinearorpiecewiselinearratefunction.TheefficiencyofPoisson
thinning depends on how close the upper bound rate is to 𝜆˜ . In practice,
z
we can improve on the simple Poisson thinning algorithm with adaptive
thinning.Thatis,ifwesimulateaneventandthenrejectit,wecanusethe
information from evaluating 𝜆˜ to improve our upper bound. We will see
z
someexamplesofsuchadaptivethinningbelow.
Superposition
ThefinalpropertyofPoissonprocessesthatcanhelpwithsimulatingtheir
events is that of superposition. This says that if we have two Poisson pro-
cesses, one with rate 𝜆(1)(𝑠) and one with rate 𝜆(2)(𝑠), and we indepen-
dentlysimulateeventsfromeachprocessandtaketheunionofevents,then
these have the same distribution as events in a Poisson process with rate
𝜆(1)(𝑠)+𝜆(2)(𝑠).
In terms of simulating the first event in a Poisson process, this can be
re-expressed as if 𝜏(1) is the first event time for a Poisson process with
rate𝜆(1)(𝑠),and 𝜏(2) isthefirsteventtimeforaPoissonprocesswithrate134 Continuous-TimeMCMC
𝜆(2)(𝑠),thenmin{𝜏(1),𝜏(2)}isdistributedasthefirsteventtimeinaPoisson
processwithrate𝜆(1)(𝑠)+𝜆(2)(𝑠).
By induction, superposition trivially applies if we consider more than
two independent Poisson processes. That is, the time of the first event in
anyoftheprocessesisdistributedasthetimeofthefirsteventofaPoisson
process whose rate is the sum of the rates. Superposition can be useful
as it allows us to split a complex rate function into a sum of simpler rate
functions.Ifwecansimulateeventsfromprocesseswitheachofthesimpler
rates,thenitallowsustosimulateeventsfromtheprocesswiththecomplex
ratefunction.
5.2.3 TheGeneratorandInvariantDistributionofaPDMP
InordertouseaPDMPtosamplefromatargetdistribution,wefirstneed
to be able to determine what the stationary distribution of a given PDMP
is. Here, we give an informal derivation of how to calculate the invariant
distribution of a PDMP. (Assuming the PDMP satisfies some regularity
conditions, and in particular is irreducible, then this invariant distribution
will be its stationary distribution.) In the next section, we will invert this
characterisationoftheinvariantdistributiontoconstructsimplerecipesfor
thedynamicsofaPDMPtohaveagiventargetdistributionasitsstationary
distribution.IntermsofunderstandingthedevelopmentofPDMPsamplers
in the rest of this chapter, the key result is (5.4) below – and those not
interested in the intuition behind this result could skip the intervening
materialinthissubsection.
First,weneedtoconsiderthegeneratorofourPDMP.Thisisrigorously
derived in Davis (1984). Remember from Section 1.4.2 that the generator
of a continuous-time Markov process is an operator that gives the time
derivativeoftheexpectedvalueofafunctionofthestate,asafunctionof
itscurrentvalue.If L isthegenerator,and ℎ asuitabletestfunctionfrom
thedomainofthegenerator,then
(Lℎ)(z)
=limE[ℎ(Z𝑡)|Z
0
=z]−ℎ(z)
.
𝑡↓0 𝑡
Informally we can calculate the generator by considering the two types
ofdynamicsofthePDMP.First,ifweconsidersolelythedeterministicdy-
namics(5.1),thenthechangeinℎ(z𝑡) isdeterministicandthecontribution
to the generator is just the time-derivative of ℎ(z𝑡) at 𝑡 = 0, which by the5.2 PiecewiseDeterministicMarkovProcesses 135
productruleis
dℎ(z𝑡)(cid:12)
(cid:12) (cid:12) = 𝝓(z)·∇ℎ(z).
d𝑡 (cid:12)
𝑡=0
Second,wehavethecontributionfromtherandomevents.Theprobability
of an event in [0,𝑡] is𝜆(z)𝑡 +𝑜(𝑡). If an event occurs, the change in ℎ(z)
is E Q(·|z) [ℎ(Z′)] − ℎ(z) + 𝑜(𝑡), where the expectation is with respect to
Z′ ∼ Q(·|z), the transition kernel at an event. This gives a contribution to
thegeneratorthatis
𝜆(z) (cid:2)E
Q(·|z)
[ℎ(Z′)]−ℎ(z)(cid:3).
Thusthegeneratoris
(Lℎ)(z) = 𝝓(z)·∇ℎ(z)+𝜆(z) (cid:2)E
Q(·|z)
[ℎ(Z′)]−ℎ(z)(cid:3).
The domain of the generator is given in Davis (1984). One extension of
PDMPs that will be relevant later is that we can introduce boundaries,
with additional specified, possibly random, behaviour if the PDMP hits
a boundary. In such a case, the behaviour at the boundaries affects the
generator solely through its domain. Essentially, the domain is reduced
to include only those functions whose expectation is unaffected by the
dynamicsontheboundary.
If we start the PDMP with an initial distribution 𝜋(z) for Z , then the
0
derivative of E[ℎ(Z𝑡)] at 𝑡 = 0 is equal to the average of the generator
appliedtoℎ(z) withrespectto𝜋(z).Thisisequalto
∫
(Lℎ)(z)𝜋(z)dz.
Foranyℎinthedomainofthegenerator,if𝜋(z)issufficientlywell-behaved,
thenthisintegralwillbeequalto
∫
ℎ(z)(L∗𝜋)(z)dz, (5.3)
whereL∗ isadifferentoperator,calledtheadjointofthegenerator.
We can attempt to define the invariant distribution of the PDMP by
the property that, if we draw Z from this invariant distribution, then the
0
expectation of any function of the state will be constant over time – as
if started from the invariant distribution, the marginal distribution of the
PDMPwillnotchange.Thus(5.3)mustbeequalto0if 𝜋 istheinvariant
distribution.Asthismusthappenforanyfunction ℎofthestate,forwhich
theexpectationexists,thissuggeststhat𝜋mustsatisfy (L∗𝜋)(z) =0.
It is possible to derive the adjoint L∗ using integration by parts. From136 Continuous-TimeMCMC
this,wehavethat(L∗𝜋)(z) =0impliesthattheinvariantdistributionmust
satisfy
−∑︁𝑑 𝜕𝜙 𝑖(z)𝜋(z) +∑︁
𝜋(z′)𝜆(z′)𝑞(z|z′)−𝜋(z)𝜆(z) =0, (5.4)
𝜕𝑧
𝑖
𝑖=1 z′
where𝑞(z|z′)istheprobabilitymassfunctionassociatedwiththetransition
kernel Q(·|z′). This equation has a natural interpretation. The first term
on the left-hand side quantifies the change in probability mass due to the
deterministicdynamics,thesecondtermisthechangeduetoeventsmoving
intostatezandthelastisthechangeduetoeventsthatmovethestateoutof
z.If 𝜋 isaninvariantdistribution,thenthenetchangeinprobabilitymass
iszero.
In the following, we will use (5.4) to define the invariant distribution
of our PDMP. Though this requires inverting the informal argument we
have made – see Chevallier et al. (2021) for results that give relatively
weakconditionsunderwhichyoucaninvertthisargumentandwhere(5.4)
impliesthat𝜋(z) istheinvariantdistributionofourPDMP.
5.2.4 TheLimitingProcessofSection5.1asaPDMP
We can now recognise the limiting process derived in Section 5.1 as a
PDMP. Remember that we want to sample from a distribution 𝜋(𝜃) for
some scalar 𝜃. To do this we introduced a velocity or momentum, 𝑝, and
defined a state z = (𝜃,𝑝) – strictly this is z = (𝜃,𝑝)⊤, but we will use the
shorthand (𝜃,𝑝) in the following. Henceforth, we will use the notation z,
or z𝑡, and (𝜃,𝑝), or (𝜃 𝑡,𝑝 𝑡) interchangeably – as viewed most helpful for
thegivencontext.Forreasonsthatwillbecomeapparent,wewillcall𝜃 the
positioncomponentofthestate,and 𝑝 thevelocitycomponent.
The limiting process of Section 5.1 was a PDMP with state z = (𝜃,𝑝),
with𝜃 ∈ Rand 𝑝 ∈ {−1,1},definedbythefollowingproperties:
(U1) Deterministicdynamics.Thedeterministicdyanamicsareaconstantve-
locitymodel:
d d𝜃 𝑡𝑡 = 𝑝 𝑡, d d𝑝 𝑡𝑡 =0.
(U2) Eventrate.Therateofeventsis
(cid:26) dlog𝜋(𝜃)(cid:27)
𝜆(𝜃,𝑝) =max 0,−𝑝 .
d𝜃5.2 PiecewiseDeterministicMarkovProcesses 137
(U3) Transition kernel at events. At an event the velocity component of the
stateflips,i.e. 𝑝 𝜏 =−𝑝 𝜏−.
WewillcallthisPDMPtheunivariatePDMP.
It is possible to show, using (5.4), that the invariant distribution of this
PDMP is 𝜋˜(z) = 𝜋(𝜃)𝜋 𝑝(𝑝), where 𝜋 𝑝 is the probability mass function
for a uniform distribution on {−1,1}. That is the 𝜃-marginal is 𝜋(𝜃), the
distribution we wish to sample from. Furthermore, under the invariant
distribution, 𝑝 is independent of 𝜃 and has a uniform distribution. For
this simple example, the invariant distribution will also be the stationary
distribution,unlesswehavereducibilitycausedbyaregionwhere𝜋(𝜃) =0
thatseparatestworegionswithpositiveprobabilityunder𝜋.
As we will be considering more general PDMP samplers, it is helpful
to consider a slightly different question. For PDMPs with the given deter-
ministicdynamicsandtransitionsatevents,howwouldwecalculateevent
ratesthatwouldgiveaninvariantdistributionwhose𝜃marginalis𝜋(𝜃)?In
answeringthisquestion,wewillcoverthestepsforshowingthattheevent
rategivenaboveleadstothestatedinvariantdistribution.
Todothiswewilluse(5.4).Ifwesubstituteinadistribution𝜋˜(𝜃,𝑝),and
the deterministic dynamics and transition kernel at events, this illustrates
thatfor𝜋˜(𝜃,𝑝) tobeaninvariantdistribution,itmustsatisfy
d𝜋˜(𝜃,𝑝)
−𝑝 +𝜆(𝜃,−𝑝)𝜋˜(𝜃,−𝑝)−𝜆(𝜃,𝑝)𝜋˜(𝜃,𝑝) =0. (5.5)
d𝜃
Here,thefirsttermcomesfromtheconstantvelocitydeterministicdynam-
ics,andthesecondcomesfromthereonlybeingonepossibletransitionto
state (𝜃,𝑝) atanevent,andthisisfromastate (𝜃,−𝑝).
So what event rate would lead to an invariant distribution with the cor-
rection𝜃-marginal?Inansweringthisquestion,wefirstseethattheremay
bearangeofdifferenteventratesthatwouldleadtoavalidinvariantdistri-
bution.Notleastbecausemanypossibleinvariantdistributionswouldhave
a𝜃-marginalas𝜋(𝜃).So,ourfirststepistoattempttofindaratesuchthat
theinvariantdistributionhas𝜃independentof𝑝.Denotesuchadistribution
by 𝜋˜(z) = 𝜋(𝜃)𝜋 𝑝(𝑝),where 𝜋 𝑝 canbeanydistributionon {−1,1}.Then
substitutingthisinto(5.5),andusing
d𝜋(𝜃) dlog𝜋(𝜃)
= 𝜋(𝜃)
d𝜃 d𝜃
gives
dlog𝜋(𝜃)
−𝑝
d𝜃
𝜋(𝜃)𝜋 𝑝(𝑝)+𝜆(𝜃,−𝑝)𝜋(𝜃)𝜋 𝑝(−𝑝)−𝜆(𝜃,𝑝)𝜋(𝜃)𝜋 𝑝(𝑝) =0.138 Continuous-TimeMCMC
Ifweconsider𝜃 forwhich𝜋(𝜃) > 0thenthisstates
dlog𝜋(𝜃)
𝜆(𝜃,−𝑝)𝜋 𝑝(−𝑝)−𝜆(𝜃,𝑝)𝜋 𝑝(𝑝) = 𝑝
d𝜃
𝜋 𝑝(𝑝). (5.6)
Ifweconsiderthesameargument,butforthestate (𝜃,−𝑝),weget
dlog𝜋(𝜃)
𝜆(𝜃,𝑝)𝜋 𝑝(𝑝)−𝜆(𝜃,−𝑝)𝜋 𝑝(−𝑝) =−𝑝
d𝜃
𝜋 𝑝(−𝑝). (5.7)
Adding(5.6)to(5.7)gives
dlog𝜋(𝜃)
0=
d𝜃
(cid:0)𝑝𝜋 𝑝(𝑝)− 𝑝𝜋 𝑝(−𝑝)(cid:1).
From this, we can conclude that the distribution 𝜋 𝑝 must satisfy 𝜋 𝑝(𝑝) =
𝜋 𝑝(−𝑝), i.e. be the uniform distribution on {−1,1}. This makes sense
intuitively. The transition at the events only changes the velocity. Thus
invarianceforthe𝜃-componentcomesfromaveragingoutthedynamicsfor
different 𝑝,andthisrequiresthattheinvariantdistributionfor 𝑝hasamean
of zero.As 𝑝 can onlytake two values,this means itmust be theuniform
distribution. For the PDMPs that we consider later, that only change the
velocityateventsandhaveconstantvelocitydynamics,asimilarargument
holds that the invariant distribution for the velocity component must have
zeroasthemean.
If we now return to our question of what rates will lead to an invariant
distribution with 𝜃-marginal equal to 𝜋(𝜃), and substitute in (5.7) that
𝜋 𝑝(𝑝) = 𝜋 𝑝(−𝑝) = 1/2 for 𝑝 ∈ {−1,1} then, by removing this common
factor,weget
dlog𝜋(𝜃)
𝜆(𝜃,𝑝)−𝜆(𝜃,−𝑝) =−𝑝 .
d𝜃
Asolutiontothisequationistheratewespecifiedabove,
(cid:26) dlog𝜋(𝜃)(cid:27)
𝜆(𝜃,𝑝) =max 0,−𝑝 . (5.8)
d𝜃
However, this is not the only solution. In fact, for any positive function
𝛾(𝜃) ≥ 0,therate
(cid:26) dlog𝜋(𝜃)(cid:27)
max 0,−𝑝 +𝛾(𝜃),
d𝜃
willalsoleadtothesameinvariantdistribution.
Theratein(5.8)isthesmallestratethatwillleadtotherequiredinvariant
distribution. This is often called the canonical rate. Intuitively, there are
twoadvantagesofusingthecanonicalrate,asopposedtoalargerrate.The5.3 Continuous-timeMCMCviaPDMPs 139
firstisthatalargerratewillleadtomoreevents,andthusislikelytohavea
largercomputationalcostforsimulatingtheresultingPDMP.Thesecondis
thatalargerratewillleadtomorechangesinvelocityandwillre-introduce
therandomwalkbehaviourthatweweretryingtoavoidwithnon-reversible
MCMC. Thus, we would expect that using the canonical rate will lead to
bettermixing.
Afinalcommentontherateswearerequiredtouse,suchasthecanonical
rate, is that they depend on the target distribution 𝜋(𝜃) only through the
derivativeof∇log𝜋(𝜃).Thisisimportantasitmeansthat𝜋(𝜃)onlyneeds
tobeknownuptoaconstantofproportionality,asiscommonlythecasefor
samplingfromtheposteriordistributioninBayesianstatistics,seeSection
1.1.5.
5.3 Continuous-timeMCMCviaPDMPs
In practice, we will want to use MCMC to sample a target density in R𝑑.
VariousPDMPsgeneralisetheprocessintroducedintheprevioussectionto
𝑑 > 1.WewilldescribethreesuchfamiliesofPDMPs,allofwhichreduce
totheunivariatePDMPofSection5.2.4if𝑑 =1butdifferfor𝑑 > 1.They
eachsharesomecommonfeatures,whichwewilldescribefirst.
Assumewewishtosamplefrom𝜋(𝜽)where𝜽is𝑑-dimensional.Thestate
ofourPDMPwillbez𝑡 = (𝜽⊤
𝑡
,p⊤
𝑡
)⊤,wherep𝑡 isalso𝑑-dimensional.Aswe
usetheconventionthatvectorsarecolumnvectors,whendefiningz𝑡wehave
had to transpose these vectors to concatenate 𝜽𝑡 and p𝑡. In the following,
to simplify notation, we will abuse this and just write z𝑡 = (𝜽𝑡,p𝑡). As
before,wecanthinkof 𝜽 asthepositioncomponentof thestate,andpas
thevelocity.
ThedeterministicdynamicsofthethreefamiliesofPDMPswillbethe
same:
(CV) Deterministic dynamics. The process evolves according to a Constant
Velocity(CV)model.
d d𝜽 𝑡𝑡 =p𝑡, d dp 𝑡𝑡 =0, (5.9)
orinthenotationweusedtodefinePDMPs,𝝓 = (p,0).Thesolutionof
thedeterministicdynamicsare
𝚽(z,𝑡) =𝚽((𝜽,p),𝑡) = (𝜽 +𝑡p,p). (5.10)
Furthermore, they also only allow the velocity component to change at
events.140 Continuous-TimeMCMC
ThethreefamiliesofPDMPswilldifferintermsofthepossiblevalues
for the velocity component, the event rate, and the transition kernel for
the velocity at events. In each case, these are chosen so that the invariant
distributionofthePDMPwillhavea𝜽-marginalthatis𝜋(𝜽),andforwhich
the velocity component, p, is independent of the position 𝜽. Throughout,
wewilldenotetheinvariantdistributionas𝜋˜(𝜽,p) = 𝜋(𝜽)𝜋 (p),thoughas
p
mentioned,theformof𝜋 willdifferbetweendifferentfamiliesofPDMPs.
p
Beforewedescribethethreefamiliesindetail,itishelpfultointroduce
some notation. We will use ∇ to denote the gradient vector with respect
𝜽
to 𝜽 only. This is the 𝑑-dimensional column vector whose entries are the
partial derivatives with respect to the components of 𝜽. The first term in
the equation for the invariant distribution of the PDMP (5.4) will be the
same for all three families as they share the same deterministic dynamics
andformoftheinvariantdistribution.Ignoringtheminussign,thistermis
∑︁2𝑑 𝜕𝜙 𝑖( 𝜕z 𝑧)𝜋˜(z) =∑︁𝑑 𝜕𝑝 𝑖𝜋 𝜕˜( 𝜃𝜽,p)
= 𝜋
p(p)∑︁𝑑
𝑝
𝑖𝜕 𝜕𝜋 𝜃(𝜽)
𝑖 𝑖 𝑖
𝑖=1 𝑖=1 𝑖=1
∑︁𝑑 𝜕log𝜋(𝜽)
= 𝜋 p(p) 𝜋(𝜽)𝑝
𝑖 𝜕𝜃
𝑖
𝑖=1
= 𝜋˜(𝜽,p) (p·∇ log𝜋(𝜽)). (5.11)
𝜽
Here,wehavefirstusedthatonlythe𝜃 𝑖 componentsarechanging,andthen
used𝜋˜(𝜽,p) = 𝜋(𝜽)𝜋 (p).Thethirdstepcomesfromthedefinitionofthe
p
derivativeoflog𝜋(𝜽) intermsofthederivativeof𝜋(𝜽),andthefinalstep
fromusing𝜋(𝜽)𝜋 (p) = 𝜋˜(𝜽,p).
p
5.3.1 DifferentSamplers
TheCoordinateSampler
PossiblythesimplestextensionofourunivariatePDMPtoonethatsamples
from a multi-dimensional distribution is the Coordinate Sampler of Wu
andRobert(2020).Forthissampler,thesetofpossiblevelocitiesisV =
cs
{±e𝑖} 𝑖𝑑 =1wheree𝑖isthe𝑖thunitvector.Thatis,e𝑖istheunitvectorwhose𝑖th
componentis1,andallothercomponentsare0.Thus,thepossiblevelocities
correspond to moving in either a positive or negative direction along one
of the coordinate axes in R𝑑. It can be viewed as a sampler which applies
the univariate PDMP dynamics along each coordinate in turn – though
the order in which different coordinate directions are chosen is random.
Introducingaconstantrefreshrate𝜆 r ≥ 0,thedynamicsofthecoordinate5.3 Continuous-timeMCMCviaPDMPs 141
samplerinvolvetheconstantvelocity(CV)deterministicdynamicstogether
with
(CS1) Eventrate.Eventsoccurwiththerate
𝜆 cs(𝜽,p) =max{0,−p·∇ 𝜽log𝜋(𝜽)}+𝜆 r.
(CS2) Transitionkernelatevents.Atanevent,theprobabilityofswitchingtoa
newvelocityp′ ∈ V is
cs
1
𝑞 cs((𝜽,p′)|(𝜽,p)) =
𝐶(𝜽)
(max{0,p′·∇ 𝜽log𝜋(𝜽)}+𝜆 r),
wherethenormalisingconstantis
𝐶(𝜽) = ∑︁ (max{0,p′·∇ 𝜽log𝜋(𝜽)}+𝜆 r) =2𝑑𝜆 r+∑︁𝑑 (cid:12) (cid:12) (cid:12) (cid:12)𝜕lo 𝜕g 𝜃𝜋 𝑖(𝜽)(cid:12) (cid:12) (cid:12) (cid:12).
p′∈V
cs
𝑖=1
The refresh rate 𝜆 r introduces additional random velocity switches. As
discussedabove,intuitivelyalargerrefreshratewillleadtomorerandom
walk behaviour and thus worse mixing. However, choosing𝜆 r > 0 allows
forstrongertheoreticalresultsaboutthesampler,includingthatthesampler
will be irreducible unless e.g. 𝜋(𝜽) has disconnected regions where there
ispositiveprobability.
The invariant distribution of the Coordinate Sampler is given by the
followingresult.
Theorem 5.1 For any 𝜆 r ≥ 0, the Coordinate Sampler, whose dynam-
ics are defined by (CV), (CS1) and (CS2), has an invariant distribution
𝜋˜(𝜽,p) = 𝜋(𝜽)𝜋 (p) where𝜋 istheuniformdistributionoverV .
p p cs
Proof We show this result by showing that (5.4) holds. To simplify ex-
pressions slightly, we will use the notation that for any scalar 𝑎 we have
{𝑎} =max{0,𝑎}.
+
Substitutingintheeventrateandtransitionkernelanddistribution𝜋˜,for
p ∈ V ,theleft-handsideof(5.4)is
cs
𝜋˜(𝜽,p)(−p·∇ 𝜽log𝜋(𝜽))−𝜋˜(𝜽,p)({−p·∇ 𝜽log𝜋(𝜽)} ++𝜆 r)
∑︁
+ ({−p′·∇ 𝜽log𝜋(𝜽)} ++𝜆 r)𝜋˜(𝜽,p′)𝑞 cs((𝜽,p)|(𝜽,p′)),
p′∈V
cs
wherewehaveused(5.9)tosimplifythefirstterm.Bythedefinitionof 𝜋˜,142 Continuous-TimeMCMC
wehave𝜋˜(𝜽,p) = 𝜋˜(𝜽,p′) forallp,p′ ∈ V .Thusthissimplifiesto
cs
−p·∇ 𝜽log𝜋(𝜽)−({−p·∇ 𝜽log𝜋(𝜽)} ++𝜆 r)
∑︁
+ ({−p′·∇ 𝜽log𝜋(𝜽)} ++𝜆 r)𝑞 cs((𝜽,p)|(𝜽,p′)).
p′∈V
cs
Nowusingthedefinitionof𝑞 ,wegetthatthefinalterminthisexpression
cs
is
∑︁
({−p′·∇ 𝜽log𝜋(𝜽)} ++𝜆 r)𝑞 cs((𝜽,p)|(𝜽,p′))
p′∈V
cs
= ∑︁ ({−p′·∇ 𝜽log𝜋(𝜽)} ++𝜆 r) 𝐶(1
𝜽)
(cid:0) {p·∇ 𝜽log𝜋(𝜽)} ++𝜆 r(cid:1)
p′∈V
cs
= (cid:0) {p·∇ 𝜽log𝜋(𝜽)} ++𝜆 r(cid:1) 𝐶(1
𝜽)
∑︁ ({−p′·∇ 𝜽log𝜋(𝜽)} ++𝜆 r)
p′∈V
cs
= (cid:0) {p·∇ 𝜽log𝜋(𝜽)} ++𝜆 r(cid:1),
whereforthelastlineweuse
∑︁
𝐶(𝜽) = ({−p′·∇ 𝜽log𝜋(𝜽)} ++𝜆 r).
p′∈V
cs
Substitutingintoourexpressionfortheleft-handsideof(5.4)gives
−p·∇ 𝜽log𝜋(𝜽)−({−p·∇ 𝜽log𝜋(𝜽)} ++𝜆 r)+ (cid:0) {p·∇ 𝜽log𝜋(𝜽)} ++𝜆 r(cid:1)
=−p·∇ log𝜋(𝜽)−({−p·∇ log𝜋(𝜽)} )+ (cid:0) {p·∇ log𝜋(𝜽)} (cid:1).
𝜽 𝜽 + 𝜽 +
By considering separately the cases where p · ∇ log𝜋(𝜽) ≥ 0 and p ·
𝜽
∇ log𝜋(𝜽) < 0, it is simple to see that this expression for the left-hand
𝜽
sideof(5.4)is0,asrequired. □
TheZig–ZagSampler
We now present the Zig–Zag algorithm of Bierkens et al. (2019b). This
algorithmhasvelocitiesinV = {±1}𝑑,andthestatemovessimultaneously
zz
alongeachcoordinateaxis,andthevelocitydetermineswhichdirectionit
moves for each axis. There are 2𝑑 possible velocities, and if for example
𝑑 = 2,thesewillbe (1,1), (1,−1), (−1,1) and (−1,−1).Atanevent,one
component of the velocity will change signs. The sampler gets its name
fromtheresultingdynamicsconsistingofzig-zagginglines.
TodefinethedynamicsoftheZig–ZagSampleritishelpfultointroduce5.3 Continuous-timeMCMCviaPDMPs 143
coordinate-specificrates
(cid:26) 𝜕log𝜋(𝜽)(cid:27)
𝜆 𝑖(𝜽,p) =max 0,−𝑝
𝑖 𝜕𝜃
,
𝑖
whichisofthesameformasthecanonicalrateoftheunivariatePDMPif
we just vary that𝑖th component of 𝜽. We also introduce the functions 𝐹 𝑖,
for𝑖 =1,...,𝑑,whichflipsthesignofthe𝑖thcomponentofavector.Soif
p′ = 𝐹 𝑖(p) then 𝑝 𝑖′ =−𝑝 𝑖 and,for 𝑗 ≠𝑖, 𝑝′ 𝑗 = 𝑝 𝑗.
ThePDMPprocessisdefinedbyCVdynamicstogetherwith
(ZZ1) Eventrate.Eventsoccurwiththerate
𝑑
∑︁
𝜆 zz(𝜽,p) = 𝜆 𝑖(𝜽,p).
𝑖=1
(ZZ2) Transitionkernelatevents.Ataneventtheprobabilitymassfunctionof
thetransitionis
𝑞 zz(𝜽,p′|𝜽,p) =
𝜆𝜆 𝑖( (𝜽 𝜽, ,p p)
), forp′ = 𝐹 𝑖(p).
zz
Thusthepositionisunchanged,andweflipcomponent𝑖 ofthevelocity
withprobabilityproportionalto𝜆 𝑖(𝜽,p).
Here we have presented the dynamics in terms of the rate of an event
and a transition probability for that event. However, by the superposition
property of Poisson processes that was discussed above, one can equiva-
lentlyrepresentthedynamicsintermsof𝑑possibleeventtypes.Eventtype
𝑖 corresponds to flipping the𝑖th component of the velocity, and this event
occurs,independentlyofotherevents,withrate𝜆 𝑖.Thisviewofthedynam-
icsoftheZig–Zagalgorithmisoftenusedinalgorithmicimplementations
tosamplerealisationsoftheprocess.
We can relate the Zig–Zag Sampler to a limiting version of the guided
randomwalkalgorithmofGustafson(1998)inasimilarwaytotheargument
presented in Section 5.1. The Zig–Zag Sampler is the limit of an MCMC
algorithmthatrepeatedlyappliesoneiterationoftheguidedrandomwalk
algorithmtoeachcomponentof𝜽 inturn.
ThefollowingresultgivestheinvariantdistributionofthePDMPprocess.
Theorem 5.2 The Zig–Zag Sampler, defined by (CV), (ZZ1) and (ZZ2)
has invariant distribution 𝜋˜(𝜽,p) = 𝜋(𝜽)𝜋 (p) where 𝜋 is the uniform
p p
distributionoverV .
zz144 Continuous-TimeMCMC
Proof Again,weshowthisresultbyshowingthat(5.4)holds.Bythesame
argument as in the first step of the proof of Theorem 5.1, if we substitute
the form of 𝜋˜ and the definition of the dynamics of the Zig–Zag Sampler
intotheleft-handsideof(5.4)wegetthatthisisproportionalto
𝑑 𝑑
∑︁ ∑︁
−p·∇ 𝜽log𝜋(𝜽)− 𝜆 𝑖(𝜽,p)+ 𝜆 𝑖(𝜽,𝐹 𝑖(p)). (5.12)
𝑖=1 𝑖=1
The first term relates to the change in probability mass due to the deter-
ministicdynamicsandisthesametermasappearedinthecalculationsfor
the Coordinate Sampler. The second term is the rate of leaving the state
(𝜽,p),andthethirdistherateofmovingtothestate(𝜽,p)whichhastobe
fromastateoftheform (𝜽,𝐹 𝑖(p)).AsintheargumentfortheCoordinate
Sampler,wehaveremovedthe𝜋˜ termsasthesearethesamefor (𝜽,p) and
(𝜽,𝐹 𝑖(p)) forall𝑖.
Tosimplifythisexpressionweuse
𝜕log𝜋(𝜽)
−𝜆 𝑖(𝜽,p)+𝜆(𝜽,𝐹 𝑖(p)) = 𝑝
𝑖 𝜕𝜃
,
𝑖
and
∑︁𝑑 𝜕log𝜋(𝜽)
p·∇ 𝜽log𝜋(𝜽) = 𝑝
𝑖 𝜕𝜃
.
𝑖
𝑖=1
Thus(5.12)becomes
∑︁𝑑 (cid:18) 𝜕log𝜋(𝜽) 𝜕log𝜋(𝜽)(cid:19)
−𝑝
𝑖 𝜕𝜃
+ 𝑝
𝑖 𝜕𝜃
=0,
𝑖 𝑖
𝑖=1
asrequired. □
BouncyParticleSampler
ThethirdsamplerthatweintroduceistheBouncyParticleSampler,which
was first introduced as a way of simulating particle systems in statistical
mechanics(PetersanddeWith,2012),butwasthenproposedasageneral
sampling algorithm by Bouchard-Coˆte´ et al. (2018). It can be derived as
a continuous-time limit of the Discrete Bouncy Particle Sampler that was
introducedinSection4.4.Likethatalgorithm,thetransitionsateventsare
reflectionsofthevelocityinthecontoursoflog𝜋(𝜽).
Fora𝑑-dimensionalvectorg,letg=g/(g·g)1/2betheunitvectorinthe
(cid:98)
directionofg.AsinSection4.4,definethefunctionR (p) =p−2(p·g)g,
g (cid:98) (cid:98)
tobethereflectionofpinthehyperplaneperpendiculartog.Animportant5.3 Continuous-timeMCMCviaPDMPs 145
propertyofareflection,thatwewillusebelow,isthatitpreservesthesize
ofthevector.Thatis
(cid:0) (cid:1) (cid:0) (cid:1)
R (p)·R (p) = p−2(p·g) g · p−2(p·g) g
g g (cid:98) (cid:98) (cid:98) (cid:98)
=p·p−4(p·g) p·g+4(p·g)2g·g
(cid:98) (cid:98) (cid:98) (cid:98) (cid:98)
=p·p−4(p·g)2+4(p·g)2 =p·p,
(cid:98) (cid:98)
whereforthepenultimateequalitywehaveusedthatgisaunitvector.Also,
(cid:98)
reflectionisaninvolution,thatisR (R (p)) =p.Toseethis
g g
(cid:0) (cid:1)
R (R (p)) = R p−2(p·g) g
g g g (cid:98) (cid:98)
=p−2(p·g) g−2{(p−2(p·g) g)·g}g
(cid:98) (cid:98) (cid:98) (cid:98) (cid:98) (cid:98)
=p−2(p·g) g−2{p·g−2(p·g)}g
(cid:98) (cid:98) (cid:98) (cid:98) (cid:98)
=p−2(p·g) g+2(p·g) g=p.
(cid:98) (cid:98) (cid:98) (cid:98)
TherearetwoversionsoftheBouncyParticleSampler,thatdifferonlyin
thesetofpossiblevelocities.Wewillmainlyworkwiththeversionwhere
the velocities take values in R𝑑, and have an invariant distribution that is
standardnormal.Foranyrefreshrate𝜆 r ≥ 0,theBouncyParticleSampler
inthiscaseisaPDMPwithconstantvelocitydynamics(CV)and
(BPS1) Eventrate.Eventsoccuratarate
𝜆 BPS(𝜽,p) =max{0,−p·∇ 𝜽log𝜋(𝜽)}+𝜆 r.
(BPS2) Transition at events. At an event with probability 1 − 𝜆 r/𝜆 BPS(𝜽,p),
reflect the velocity in the hyperplane perpendicular to ∇ log𝜋(𝜽), that
𝜽
isthenewvelocityis
p′ = R (p), withg= ∇ log𝜋(𝜽);
g 𝜽
otherwisesampleanewvelocity,p′fromastandardnormaldistribution.
Thepositionisunchangedatanevent.
AswiththeZig–ZagSampler,wecaninterpretthedynamicsintermsof
events of different types. In this case, we have reflection events that occur
withratemax{0,−p·∇ 𝜽log𝜋(𝜽)},andrefresheventsthatoccurwithrate𝜆 r.
If𝜆 r > 0thenBouchard-Coˆte´ etal.(2018)provethattheresultingprocess
isirreducible,assumingweakconditionson 𝜋(𝜽).Furthermore,theygive
anexamplewhereif𝜆 r = 0,thesamplerwillbereducible,andthisoccurs
ifweweretousetheBouncyParticleSamplertosamplefromaGaussian
distribution.AsmanytargetdistributionscanbeclosetoGaussian,wemay
haveareduciblesampler,oronewhichmixesslowlyif𝜆 r =0.Inpractice,
tuning𝜆 r isimportant,asnotonlycanthesamplermixpoorlyfor𝜆 r ≈ 0,146 Continuous-TimeMCMC
butifwechoose𝜆 rtoolargeitwillintroducerandomwalkbehaviourwhich
willalsoleadtopoormixing.Wewillreturntothisissuelaterinthissection
andinSection5.3.3
Thealternativeversionofthealgorithmhasvelocitiesthatlieontheunit
𝑑-dimensional hypersphere. The only difference in terms of the dynamics
is that at a refresh event, we sample a new velocity from the uniform
distributiononthesphereratherthanfromastandardnormaldistribution.
Furthermore,thereareextensionsoftheBouncyParticleSamplerthatonly
partially refresh the velocity. That is at a refresh event we sample a new
velocity from a Markov kernel which has a standard normal distribution
(or for the alternative version a uniform distribution on the sphere) as its
stationarydistribution.
ThefollowingresultgivestheinvariantdistributionoftheBouncyParti-
cleSampler.
Theorem5.3 Forany𝜆 r ≥ 0,theBouncyParticleSampler,whosedynam-
icsaredefinedby(CV),(BPS1)and(BPS2),hasaninvariantdistribution
𝜋˜(𝜽,p) = 𝜋(𝜽)𝜋 (p) where 𝜋 isthedensityofa 𝑑-dimensionalstandard
p p
normaldistribution.
Proof AsourpresentationofPDMPshasfocussedondiscretetransitions
atevents,wewillprovetheresultfor𝜆 r = 0only.Theextensionto𝜆 r > 0
isstraightforward,astheadditionalrefreshratestriviallykeep𝜋˜ invariant.
Asshownabove,areflectiondoesnotchangethelengthofavector.As
fortheproposedinvariantdistribution,𝜋 (p)dependsonponlythroughits
p
length,wehave𝜋 (p) = 𝜋 (R (p)),foranydirectionofreflectiong.Thus
p p g
areflectioneventdoesnotchangethevalueof𝜋˜.Thismeanswecanusethe
same argument as at the start of the proofs of Theorem 5.1 and Theorem
5.2togetthattheleft-handsideof(5.4)isproportionalto
−p·∇ log𝜋(𝜽)+𝜆 (𝜽,R (p))−𝜆 (𝜽,p), (5.13)
𝜽 BPS g BPS
wheretosimplifynotationwehaveusedg= ∇ log𝜋(𝜽).Themiddleterm
𝜽
is the rate of transitioning to state (𝜽,p) and uses, as shown above, that
reflectionsareinvolutions,soitisthestate(𝜽,R (p))thatwilltransitionto
g
(𝜽,p) atareflectionevent.Substitutingintheformof𝜆 ,andusingthe
BPS
definitionofg,wehave
𝜆 (𝜽,R (p))−𝜆 (𝜽,p) =max{0,R (p)·g}−max{0,p·g}.
BPS g BPS g
Now
R (p)·g= (p−2(p·g)g)·g=p·g−2(p·g)(g·g) =p·g−2(p·g)(g·g),
g (cid:98) (cid:98) (cid:98) (cid:98) (cid:98) (cid:98)5.3 Continuous-timeMCMCviaPDMPs 147
wherethelastequalityfollowsasgisproportionaltog.Asgisaunitvector,
(cid:98) (cid:98)
wehaveR (p)·g=−p·g.Substitutinggives
g
𝜆 (𝜽,R (p))−𝜆 (𝜽,p) =max{0,−p·g}−max{0,p·g} =−p·g.
BPS g BPS
By definition of g this is just p·∇ log𝜋(𝜽). Substituting this into (5.13)
𝜽
weseethattheleft-handsideof(5.4)is0asrequired.
□
Example:SamplingfromaGaussianTarget
To gain an initial understanding of these algorithms in practice, how they
differ from each other and how they compare to HMC, we will consider
their implementation for sampling from a Gaussian distribution. This is
an example where all methods can be implemented exactly and enables a
simplecomparisonofthedynamicsofthedifferentsamplers.
TosimplifynotationwewillassumeourtargetGaussiandistributionhas
ameanzero.Thiscanbeassumedwithoutlossofgeneralityintermsofthe
behaviour of the samplers, as we can re-centre any Gaussian distribution
withnon-zeromeanandthiswouldnotchangethesamplers’dynamics.A
mean-zeroGaussiandistributioniscommonlyparameterisedbyitscovari-
ance matrix, 𝚺 say, but in terms of its density function, it is easier to use
theprecisionmatrix,whichistheinverseofthecovariancematrix.Wewill
denotetheprecisionmatrixbyQ = 𝚺−1.Weassumethat𝚺,andhenceQ,
ispositive-definite.Thenuptoanadditiveconstant,wehave
1
log𝜋(𝜽) =− 𝜽⊤Q𝜽.
2
TheratesofthePDMPsamplersdependon𝜋through−∇ log𝜋(𝜽),which
𝜽
fortheGaussiantargetisQ𝜽.
Before showing the output from the different PDMP samplers for this
model, we will describe an approach to simulating the PDMPs. Each of
the three PDMPs introduced in the previous section can be viewed as
having multiple types of event, with each event having a deterministic
transition.FortheZig–ZagSampler,wehaveoneeventassociatedwitheach
componentof𝜽,andthatflipstheassociatedcomponentofthevelocity.For
the Bouncy Particle Sampler, and the Coordinate Sampler, there are two
events,oneofwhichisarefreshofthevelocity.Ourapproachtosimulating
these PDMPs is to use the idea of superposition, that is we will simulate
the time for each of the possible events, find which occurs first, and then
thistypeofeventwithitsassociatedtimeisthenexteventforourPDMP.
As described in Section 5.2.2, to simulate times of events we need,148 Continuous-TimeMCMC
foreachtypeofevent,tocalculatetherateofthetimeuntilthenextevent,
giventhecurrentstate.Wewilldescribehowtocalculatethisforthebounce
event of the Bouncy Particle Sampler, and for a flip event in the Zig–Zag
Sampler.Simulatingtherefresheventsistrivial,andsimulatingtheevents
oftheCoordinateSamplerfollowsbysimilararguments.
LetthecurrentstateofourPDMPbez= (𝜽,p),andlet𝜆˜ (𝑡)betherate
z
atwhichaneventoccursintermsofthefuturetime𝑡.Wewillfirstconsider
thebounceeventoftheBouncyParticleSampler.Letthecurrenttimebe𝑠,
soz𝑠 =z,then
𝜆˜ z(𝑡) =max{0,p𝑠+𝑡 ·(−∇ 𝜽log𝜋(𝜽𝑡+𝑠)} =max{0,p⊤ 𝑠+𝑡Q𝜽𝑠+𝑡}
=max{0,p⊤Q(𝜽 +𝑡p)} =max{0,p⊤Q𝜽 +𝑡p⊤Qp}.
Herewehaveusedthedefinitionof𝜆˜ z(𝑡),substitutedinlog𝜋(𝜽𝑡+𝑠)forour
targetandthenusedthefactthatupuntilthenextevent,p𝑠+𝑡 =p𝑠 =pand
𝜽𝑠+𝑡 = 𝜽𝑠 +𝑡p𝑠 = 𝜽 +𝑡p.
Weareviewing𝜆˜ (𝑡) asafunctionofthefurthertimeuntiltheevent,𝑡.
z
We can see that 𝜆˜ is the maximum of zero and a linear function of 𝑡, and
defining 𝑎 = p⊤Q𝜽 and 𝑏 = p⊤Qp, the linear function is equal to 𝑎 +𝑏𝑡.
Furthermore,asQispositive-definitewehave𝑏 > 0.Forthisrate,wecan
simulate event times directly using Algorithm 7. To do this, we need to
solve∫ 𝑡 𝜆˜ (𝑢)d𝑢 = 𝑤.Therearetwocasesfortheintegral.First,if𝑎 > 0,
0 z
then𝜆˜ (𝑢) = 𝑎+𝑏𝑢forall𝑢 > 0,so
z
∫ 𝑡 ∫ 𝑡 𝑏𝑡2
𝜆˜ (𝑢)d𝑢 = (𝑎+𝑏𝑢)d𝑢 = 𝑎𝑡+ ,
z 2
0 0
√
andthisisequalto𝑤 > 0if𝑡 =−𝑎/𝑏+ 𝑎2+2𝑤𝑏/𝑏.If𝑎 < 0then𝑎+𝑏𝑡
isonlypositivefor𝑡 > |𝑎|/𝑏,thusforsuch𝑡
∫ 𝑡 ∫ 𝑡 ∫ 𝑡−|𝑎|/𝑏 𝑏(𝑡−|𝑎|/𝑏)2
𝜆˜ (𝑢)d𝑢 = (𝑎+𝑏𝑢)d𝑢 = 𝑏𝑢′2d𝑢′ = .
z 2
0 |𝑎|/𝑏 0
Thisisequalto𝑤 > 0when𝑡 = |𝑎|/𝑏+√︁ 2𝑤/𝑏.
TheresultingalgorithmforoneiterationoftheBouncyParticleSampler
isgiveninAlgorithm9.Thealgorithmsimulatesthetimeofarefreshevent
and a bounce event. It sets the time of the next event to be the smaller of
thesetwotimes,andupdatesthepositionofthestate.Then,dependingon
which type of event occurred first, it updates the velocity. For a refresh
event,thisinvolvessimulatingfromthesampler’sinvariantdistributionfor
thevelocity,whichdependingonthetypeofBouncyParticleSamplercan
be either a standard Gaussian distribution or the uniform distribution on5.3 Continuous-timeMCMCviaPDMPs 149
the unit hyper-sphere. For a bounce event, the velocity is reflected in the
hyperplaneperpendicularto−∇log𝜋(𝜽) atthecurrentposition 𝜽′,which
forourmodelisQ𝜽′.
Algorithm9:BouncyParticleSampler:GaussianTarget
Input:PrecisionMatrix,Q,currentstate (𝜽,p),refreshrate𝜆 r > 0
Calculate𝑎 =p⊤Q𝜽 and𝑏 =p⊤Qp.
Simulate𝑤 and𝑤 ,independentrealisationsofastandard
1 2
exponentialrandomvariable.
Calculatetimeuntilarefreshevent𝜏
1
= 𝑤 1/𝜆 r.
Calculatetimeuntilabounceevent:if𝑎 < 0𝜏 =√︁ 2𝑤 /𝑏+|𝑎|/𝑏,
2 2
√︁
otherwise𝜏 =−𝑎/𝑏+ 𝑎2+2𝑤 𝑏/𝑏.
2 2
Calculateeventtime𝑡 =min{𝜏 ,𝜏 }.
1 2
Updateposition𝜽′ = 𝜽 =𝑡p.
Decidedoneventtypeandupdatevelocity:
if 𝜏 < 𝜏 then
1 2
Refreshevent.Simulatep′ fromitsinvariantdistribution.
else
Bounceevent.Set
Q𝜽′
p′ =p−2(p⊤Q𝜽′) .
√︁
𝜽′⊤Q⊤Q𝜽′
end
Output:Timetonextevent𝑡,andnewstate (𝜽′,p′)
A similar derivation is possible for the Zig–Zag Sampler. The only
differenceisthatthefliprateofthe𝑖thcomponentof 𝑝 𝑖 is
max{0,−𝑝 𝑖(∇ 𝜽log𝜋(𝜽)) 𝑖} =max{0,𝑝 𝑖(Q𝜽) 𝑖},
where we write, for example, (Q𝜽) 𝑖 to denote the 𝑖th component of Q𝜽.
Thus,theassociatedrateasafunctionoftimeuntilthisevent,ifthecurrent
stateofz= (𝜽,p),is
𝜆˜ z(𝑖)(𝑡) =max{0,𝑝 𝑖[Q(𝜽 +𝑡p)] 𝑖} =max{0,𝑝 𝑖(Q𝜽)
𝑖
+𝑡𝑝 𝑖(Qp) 𝑖}.
This again is the maximum of 0 and a linear function of 𝑡 and can be
simulatedasdescribedabove.Asabove,wecanset𝜆˜(𝑖)(𝑡) =max{0,𝑎+𝑏𝑡}
z
butnowwith𝑎 = 𝑝 𝑖(Q𝜽) 𝑖 and 𝑏 = 𝑝 𝑖(Qp) 𝑖.Theonlydifferenceisthatin
thiscase, 𝑏 canbenegative.If 𝑏 < 0and 𝑎 < 0,thenthiseventcannever150 Continuous-TimeMCMC
happen.If𝑎 > 0,thenaneventcanhappenfor𝑡 < 𝑎/|𝑏|.Inthiscase
∫ 𝑡 𝑏𝑡2
𝑤 = (𝑎+𝑏𝑢)d𝑢 ⇒ 𝑤 = 𝑎𝑡+ .
2
0
This has a solution for 𝑡 > 0 only if 𝑤 ≤ 𝑎2/(2|𝑏|), in which case 𝑡 =
√
−𝑎/𝑏 + 𝑎2+2𝑤𝑏/𝑏. If 𝑤 > 𝑎2/(2|𝑏|) then this event does not happen.
If an event cannot or does not happen, then algorithmically we set the
associatedeventtimetoinfinity.
AnalgorithmforoneiterationoftheZig–ZagSamplerisgiveninAlgo-
rithm10.IthasasimilarformastheBouncyParticleSampler.Wecalculate
theeventtimeforeachtypeofevent–thoughduetothefourpossiblecases
we have not given the formulae for calculating the event times within the
algorithm. We then set the event time to the smallest of these times and
update the position. Finally, the event type is calculated and we apply the
appropriate transition to the velocity, remembering that 𝐹 𝑖(p) flips the𝑖th
componentofthevectorp.
Algorithm10:Zig–ZagSampler:GaussianTarget
Input:PrecisionMatrix,Q,currentstate (𝜽,p),refreshrate𝜆 r > 0
for𝑖 =1,...,𝑑 do
Calculate𝑎 𝑖 = 𝑝 𝑖(Q𝜽) 𝑖 and𝑏 𝑖 = 𝑝 𝑖(Qp) 𝑖.
Simulate𝑤 𝑖,arealisationofastandardexponentialrandom
variable.
Calculate𝜏 𝑖,theeventtimeforaflipofthe𝑖thcomponentofp.
end
Calculateeventtime𝑡 =min𝑖=1,...,𝑑{𝜏 𝑖}.
Updateposition𝜽′ = 𝜽 =𝑡p.
Decideoneventtype,𝑖∗ =argmin{𝜏 𝑖}.
Updatevelocityp′ = 𝐹 𝑖∗(p).
Output:Timetonextevent𝑡,andnewstate (𝜽′,p′)
To gain some intuition of the properties of these PDMP algorithms,
wewillfirstlookqualitativelyattheoutputofrunningthealgorithmsfora
bivariateGaussian–aswecanplottherealisationsfromthepathsgenerated
bythepositioncomponentofthePDMPs.First,welookattheimportance
of the refresh rate with the Bouncy Particle Sampler. To most clearly see
thepotentialissuesforthissampler,itishelpfultoobserveitsamplingfrom
a target with uncorrelated, equal variance components. See Figure 5.2 for
outputfromtheBouncyParticleSamplerfordifferentrefreshrates.5.3 Continuous-timeMCMCviaPDMPs 151
   
   
   
   
   
   
   
           
θ(1) θ(1)
   
   
   
   
   
   
   
           
θ(1) θ(1)
Figure5.2 Plotsofrealisationsofthetrajectoryorpathofthe
BouncyParticleSamplerwhensamplingfromastandard(i.e.
uncorrelated,equalvariance)bivariateGaussiandistribution.
Trajectoriesshownfornorefreshevents(topleft),𝜆
r
=0.1(top
right),𝜆 r =1(bottomleft)and𝜆 r =10(topright).Theheatmap
showsthelogposteriordensityofthetargetineachcase.
Thetop-leftplotshowsthetrajectoryifwedonotuseanyrefreshevents.
Wecanclearlyseeevidenceofthesamplerbeingreducible–asthesampler
doesnotenteralargeregionaroundthemode.Bouchard-Coˆte´etal.(2018)
provethattheBouncyParticleSamplerisinfactreducibleforthisexample.
Furthermore,theyshowthatthisisavoidedifweuseanynon-zerorefresh
rate, and if we do so the sampler will converge to the target distribution.
However,wegetverydifferentbehaviourfordifferentvaluesoftherefresh
rate,asshownintheremainingplotsofFigure5.2.Asmallrateproducesa
)2(θ
)2(θ
)2(θ
)2(θ152 Continuous-TimeMCMC
sampler, that whilst irreducible, has poor mixing properties (see top-right
plot).Thesamplerhaslongperiodsbetweenrefreshevents,andforeachof
these periods, there are regions of the state space that the sampler cannot
reach.Toolargearefreshratemeansthatthesamplershowsrandom-walk
behaviour, which can also adversely affect its mixing (see bottom-right
plot). Tuning of the refresh rate to a good intermediate value can result
in a sampler that mixes well and avoids this random-walk behaviour (see
bottom-leftplot).Wewillreturntohowtotunetherefreshratelater.Finally,
whilsttheCoordinateSampleralsohasrefreshevents,itdoesnotsufferfrom
thesameproblemsastheBouncyParticleSampler,andwillbeirreducible
evenif𝜆 r =0.
We now compare the outputs of the three different PDMP samplers,
withtheBouncyParticleSamplerusinganappropriatelytunedrefreshrate,
𝜆 r =1.TheseareshowninFigure5.3togetherwiththeoutputfromHMC.
At this stage, there are two main points we wish to make. First, one can
seethequalitativesimilaritiesanddifferencesbetweenthedifferentPDMP
samplers.Eachsamplerexplores𝜽spacewithstraight-linetrajectories,and
they all have the property that they continue in the same direction whilst
moving to areas of higher probability density – though for the Zig–Zag
Sampler, this has to be interpreted separately for each axis component.
However,thetrajectoriesthemselvesareverydifferentduetothedifferent
possible velocities and transitions. The Coordinate Sampler explores the
posterior by exploring a single component of 𝜽 at a time. This has some
similarities with a Gibbs sampler (see Section 2.1.1), and intuition from
resultsonmixingofGibbssamplerssuggestthatthissamplerwillperform
best when there is no strong correlation between the components of 𝜽.
TheZig–ZagSamplerhastrajectoriesconsistingofdiagonallines,andthe
transitions that flip a single component of the velocity lead to trajectories
that look like zig-zags, which gives the sampler its name. Finally, the
BouncyParticleSamplercanhavetrajectoriesthatexplorethespaceinany
direction.
Second, it is interesting to compare PDMP samplers with HMC (see
bottom-rightplot).Forthismodel,wecansolvetheHamiltoniandynamics
foreachproposaloftheHMCalgorithmexactly,andthuswealwayswillac-
ceptaproposal.ThetrajectoriesoftheHamiltoniandynamicsareelliptical,
ascomparedtothestraight-linesegmentsofourPDMPsamplers.However,
the main difference is that the output of a PDMP sampler is a continuous
path, whilst for HMC, we obtain a set of points. For non-Gaussian target
distributions,theHMCsamplerwillnotalwaysaccepttheproposalwhich
canleadtoworsemixing.Insuchcases,whilstwecannotdirectlysample5.3 Continuous-timeMCMCviaPDMPs 153
   
   
   
   
   
   
   
           
θ(1) θ(1)
   
   
   
   
   
   
   
           
θ(1) θ(1)
Figure5.3 ComparisonofthreePDMPalgorithmsandHMCfor
samplingfromabivariateGaussianwithunitmarginalvariances
andcorrelationof0.5.Realisationsofthetrajectories(bluelines)
oftheCoordinateSampler(topleft),theZig–ZagSampler(top
right)andtheBouncyParticleSamplerwith𝜆 r =1(bottomleft).
TrajectoriesofHamiltoniandynamics(line)andthesampled
points(dots)fromHMC(bottomright).Theheatmapshowsthe
logposteriordensityofthetargetineachcase.
thetrajectoriesofthePDMPsamplers,thisonlyimpactsthecomputational
cost of simulation, rather than the output or the mixing properties of the
sampler.
)2(θ
)2(θ
)2(θ
)2(θ154 Continuous-TimeMCMC
5.3.2 UseofPDMPOutput
SimulatingaPDMP,asdescribedinSection5.2.2,producesaskeletonof
thesamplepathoftheprocess.Assumethatwehavesimulatedtheprocess
uptosometime𝑇.Wewilldenotethisskeletonbytheset{𝜏 𝑖,(𝜽𝜏 𝑖,p𝜏 𝑖)}𝑛 𝑘=+1
0
that gives the initial state of the process, with 𝜏 = 0, the time and state
0
aftereachevent,for 𝑘 = 1,...,𝑛,andthefinalstateoftheprocessattime
𝜏
𝑛+1
=𝑇.Howdoweusethisoutputtoapproximatethetargetdistribution?
For any Monte Carlo method, an approximation to the target distribu-
tion, 𝜋(𝜽) comes from the ability to estimate the expectation for arbitrary
functions of 𝜽 with respect to 𝜋. Thus consider estimating E 𝜋 [ℎ(𝜽)] for
some function ℎ for which this expectation exists. First, we describe how
wedonotestimatethisexpectation!Wecannotjustusethesampleaverage
of ℎ(·) at the skeleton points for 𝜽, even after allowing some burn-in. In
general, the skeleton points will not be sampled from 𝜋 at stationarity, as
theywillbebiasedtowardsvalueswherethereisalargeaveragerateofan
eventoccurring.
Instead,weneedtoestimatetheexpectationwithrespecttothecontinuous-
timesamplepath,afterallowingforasuitableburn-in.Therearetwoways
ofdoingthis.Assumewechoosetheburn-intobesometime𝑆.Thenone
approach is to calculate the average value of the integral of ℎ(𝜽𝑡) for our
sample path for 𝑆 < 𝑡 ≤ 𝑇. This can be calculated from the skeleton as
follows. First, we work out the value of 𝜽𝑆 for our sample path. This is
possiblebyfinding𝑙 suchthat𝜏 𝑙 ≤ 𝑆 < 𝜏 𝑙+1,andusinglinearinterpolation
𝜏 −𝑆 𝑆−𝜏
𝜽𝑆 = 𝜏𝑙+1
−𝜏
𝜽𝑙 +
𝜏
−𝑙
𝜏
𝜽𝑙.
𝑙+1 𝑙 𝑙+1 𝑙
Thenourestimatorcanbecalculatedas
Eˆ 𝜋 [ℎ(𝜽)] = 𝑇 −1 𝑆 (cid:18)∫ 𝜏 𝑙+1 ℎ(cid:18) 𝜽𝑆 +(𝑡−𝑆)𝜽 𝜏𝜏 𝑙+1 − −𝜽 𝑆𝑆(cid:19) d𝑡
𝑆 𝑙+1
+ ∑︁𝐾 ∫ 𝜏 𝑘+1 ℎ(cid:18) 𝜽𝜏
𝑘
+(𝑡−𝜏 𝑘)𝜽 𝜏𝜏 𝑘+1 − −𝜽 𝜏𝜏 𝑘(cid:19) d𝑡(cid:33)
𝑘=𝑙+1 𝜏 𝑘 𝑘+1 𝑘
Thisestimatorisonlypracticalifwecananalyticallycalculatetheintegrals
alongthelinearsegmentsofthepath.Amoregeneral,andarguablysimpler
approachistoevaluate 𝜽𝑡 at 𝑁 evenlyspacedpointsbetween 𝑆 and𝑇 and
then use standard Monte Carlo averages with respect to this set of values.
Thatis,let𝛿 = (𝑇 −𝑆)/𝑁 andusinglinearinterpolationasaboveevaluate5.3 Continuous-timeMCMCviaPDMPs 155
𝜽𝑆+𝑗𝛿 for 𝑗 =1,...,𝑁.ThenourestimatorofE 𝜋 [ℎ(𝜽)] wouldnowbe
𝑁
1 ∑︁
Eˆ
𝜋
[ℎ(𝜽)] =
𝑁
ℎ(𝜽𝑆+𝑗𝛿).
𝑗=1
One advantage of this approach is the final output is similar to that for
standardMCMC,whicheasescomparisonandenablesustousemethodsfor
assessingtheaccuracyofstandardMCMCestimatorssuchastheintegrated
auto-correlationtimeandeffectivesamplesize(seeSection1.3.2).
5.3.3 ComparisonofSamplers
Bierkensetal.(2022)examinesthemixingpropertiesoftheBouncyParticle
Sampler and the Zig–Zag Sampler in the limit as the dimension of the
space, 𝑑 → ∞, for the special case where the posterior of interest is a 𝑑-
dimensional standard normal distribution. In related work, Bierkens et al.
(2023a)investigatesfinite-dimensionalnormaltargetswheresomeprincipal
components have a much smaller length scale than others. We summarise
thefindingsofthesetwopapersandprovidesomeintuitionforthem.
Tocomparethesamplers,weneedtoconsiderboththeircomputational
cost for simulating a trajectory of fixed duration, and how the mixing of
the process depends on time. First, consider the computational cost, and
the intensity of bounce events on a 𝑑-dimensional standard normal target,
so that at stationarity ∇log𝜋(𝜽) = −𝜽. The momentum for the Zig–Zag
Samplerisp ∈ {−1,+1}𝑑.FortheBouncyParticleSampler,wedescribe
zz
thecasewherepissampledfromU𝑑,theuniformdistributionontheunit
hypersphere.If,instead,p
BPS
∼N(0, 𝑑1I𝑑),forlarge𝑑,∥p BPS∥2 ≈1andthe
analysisisthe √sameasforthesettingwherep
BPS
∼ U𝑑.Speedinguptime
byafactorof 𝑑leadstotheversionoftheBPSthatsamplesp∼N(0,I𝑑),
butmakesnodifferencetotheoverallefficiencyintermsofmixingperunit
ofcomputationaleffort.
Foreithersampler,let 𝑝 𝑖 bethe𝑖thcomponentofitsmomentum.Forthe
BouncyParticleSampler,theintensityis𝜆 BPS(𝜽,p) =max{0,(cid:205) 𝑖𝑑
=1
𝑝 𝑖𝜃 𝑖} =
𝑂(1),sinceeachterminthesumhasthesameexpectationof0andavari-
ance of𝑂(1/𝑑). Thus,there are𝑂(1) events perunit oftime. Incontrast,
for the Zig–Zag Sampler, the intensity is𝜆 zz(𝜽,p) = (cid:205) 𝑖𝑑 =1max{0,𝑝 𝑖𝜃 𝑖} =
𝑂(𝑑),sinceeachterminthesumhasthesamepositiveexpectation.Hence,
thereare𝑂(𝑑) eventsperunittime.Ingeneral,foreachsampler,thecom-
putational cost of performing a bounce is 𝑂(𝑑): for the Bouncy Particle
Sampler,thisistheorderofthecostofcalculatingthegradientrequiredfor156 Continuous-TimeMCMC
boththeeventrateandforcalculatingthenewvelocityatabounceevent;
forZig-Zag,afterabounceeventwewillneedtoupdatetheratesforeach
ofthe𝑑 possibleevents(thoughseeSection5.4.2forsituationswherethis
can be reduced). Thus the total cost per unit time is𝑂(𝑑) for the Bouncy
ParticleSamplerand𝑂 (cid:0)𝑑2(cid:1) fortheZig–ZagSampler.
The above costing generalises to any reasonably well-behaved target.
However, because it only updates a component at a time, in cases where
components have a sparse conditional dependence graph the cost of per-
formingaZig–Zagbouncecanbereducedto𝑂(1) (seeSection5.4.2).
Nowweturntothemixingproperties.Inthisspecialcaseofanisotropic
target, the state of the Markov process can be encapsulated by the radial
component, ∥𝜽∥, and the angle between the radius and the momentum,
whichisproportionalto𝜽 ·p.
For Bouncy Particle Samplers, the radial component mixes in 𝑂(𝑑)
time,whereastheangularcomponentmixesin𝑂(1) time.Toseewhy,for
simplicity,weignoreanyrefreshevents.Considerasinglestraight-linepath
betweenbounces,whichwewillcallasegment:becausethecontoursona
N(0,I𝑑) targetarespherical,log𝜋 increasesmonotonicallytoamaximum
alongthesegmentandthendecreasesmonotonicallyuntilthenextbounce.
Let 𝐸 be the size of the drop in log𝜋 from its maximum until the next
bounce.Startaclockatatimewhenlog𝜋isatamaximumandlet𝑡 bethe
timesincetheclockstarted.Sincetherearenorefreshevents,iftherehas
beennobouncesincetheclockstarted,thesizeofthetotaldropinlog𝜋by
time𝑡 is
∫ 𝑡
𝐷(𝑡) =− p·∇log𝜋(𝜽𝑠)d𝑠.
0
Now,for𝑡 > 0,𝜋(𝜽𝑡) decreasesuntilabounceoccurs,so
𝐸 > 𝐷(𝑡) ⇔”Nobouncebytime𝑡.”
However,whilelog𝜋isdecreasing,bounceeventsfollowaninhomogeneous
Poisson process with a rate at time 𝑠 of 𝜆(𝑠) = −p · ∇log𝜋(𝜽𝑠), so the
probabilitythattherehasbeennoeventbytime𝑡 is
(cid:20) ∫ 𝑡 (cid:21)
exp − 𝜆(𝑠)d𝑠 =exp[−𝐷(𝑡)].
0
Thus P(𝐸 > 𝐷(𝑡)) = exp[−𝐷(𝑡)], so 𝐸 has an exponential distribution
withrateparameter1.
Overthe𝑂(1)timebetweenbounces,theangle𝜽·pmovesmonotonically
fromitsmostnegativeextent(justafterabounce)toitsmostpositiveextent5.3 Continuous-timeMCMCviaPDMPs 157
(justbeforethenextbounce).Thus,𝜽 ·pmixesin𝑂(1) time.However,
1 1
log𝜋(𝜽) =constant− ∥𝜽∥2 =constant− 𝜒2
𝑑
2 2
√
atstationarity,anda 𝜒2 randomvariablehasastandarddeviationof 2𝑑.
𝑑
So,tomix,thelog𝜋 processneedstomoveby𝑂(𝑑1/2),yetitonlymoves
by𝑂(1) in𝑂(1) time.Inananalogousmannertothelimitfortherandom
walk Metropolis in Section 2.1.3, speeding up time by a factor of 𝑑 leads
toalimitingdiffusionfor ∥𝜽∥;thus ∥𝜽∥ mixesin𝑂(𝑑) time.
By maximising the speed of the limiting diffusion for ∥𝜽∥, the same
analysisadvisesontuningtherefreshratefortheBouncyParticleSampler,
𝜆 r.Thissuggestschoosing𝜆 r sothattheratioofrefreshtobounceevents
is≈0.78.
Inthecaseofanisotropicnormaltarget,theZig–ZagSamplersimplifies
to𝑑independentone-dimensionalinstancesofGustafson’salgorithm(Sec-
tion 4.3). Thus, each individual component mixes in 𝑂(1) time, so both
∥𝜽∥ and𝜽 ·pmixin𝑂(1) time.
Multiplyingthemixingtimesbythecomputationalcosts,wecandefine
mixingcosts.ForBouncyParticleSamplers,theseare𝑂(𝑑)fortheangular
component and 𝑂(𝑑2) for the radial component, whereas the costs for
the Zig–Zag Sampler are 𝑂(𝑑2) for both components. If one follows the
adage that a sampler is only as good as its worst-mixing component, this
suggests that, at least for well-behaved targets, both algorithms have a
similarefficiency.
WhataboutthemixingoftheBouncyParticleSamplerforotherfunctions
ofthestate?Bierkensetal.(2022)showthatithasthesame𝑂(𝑑2) ratefor
marginal components of the state – i.e. if we are interested in 𝜃 𝑖 for some
𝑖.ThiscomparestoresultsinDeligiannidisetal.(2021)whichsuggestthat
mixingcostsformarginalcomponentsare𝑂(𝑑).Thedifferenceinresults
comesfromdifferentchoicesofhowtherefreshratedependson𝑑.Bierkens
etal.(2022)haveaconstantrate,whereasDeligiannidisetal.(2021)assume
that the rate decays like 𝑂(𝑑−1/2). The latter choice improves mixing for
marginalcomponentsbutworsensthemixingoftheradialcomponent,so
thatas𝑑 →∞,thelimitingprocessfortheradialcomponentisdegenerate.
To see this in practice, we compared the performance of Zig–Zag and
theBouncyParticleSampleratsamplingfromaGaussianwith𝑑 =20and
𝑑 = 1000. Results are shown in Figure 5.4, where we scale the number
of events simulated to be proportional to the dimension, 𝑑. The theory
statesthatforthisscalingwewouldexpectsimilarmixingfortheZig–Zag
sampleroverthelengthofthesimulation.Weobservethisqualitativelyfor158 Continuous-TimeMCMC
6 6 6
5 5 5
4 4 4
3 3 3
0 1 0 1 0 1
Proportion of run Proportion of run Proportion of run
2 2 2
0 0 0
2 2 2
0 1 0 1 0 1
Proportion of run Proportion of run Proportion of run
33 33 33
32 32 32
31 31 31
30 30 30
0 1 0 1 0 1
Proportion of run Proportion of run Proportion of run
2 2 2
0 0 0
2 2 2
0 1 0 1 0 1
Proportion of run Proportion of run Proportion of run
Figure5.4 TraceplotsforZig–Zag(left-handcolumn),Bouncy
ParticleSamplerwith𝜆
r
=1.5√︁𝑑/20(middlecolumn)and
BouncyParticleSamplerwith𝜆
r
=1.5(right-handcolumn)fora
Gaussiantargetwith𝑑 =20(toptworows)and𝑑 =1000(bottom
tworows).Ineachcaseweshowtraceplotsfortheradial
componentofthestate,∥𝜽∥,andthefirstcomponentofthestate,
𝜃 .Weranallsamplersfor20𝑑bounceevents,andscaledthetime
1
axisbytheproportionoftheresultingsimulationtime.
θ
θ
θ
θ
1
k
k
1
k
k
θ
θ
θ
θ
1
k
k
1
k
k
θ
θ
θ
θ
1
k
k
1
k
k5.4 EfficientSimulationofPDMPSamplers 159
both ∥𝜽∥ and 𝜃 . The theory also suggests similar qualitative behaviour
1 √
for the Bouncy Particle Sampler with a refresh rate that scales with 𝑑,
so that the proportion of refresh events is roughly similar for different 𝑑;
we observe this in the middle column of the plot. By comparison, if we
use a fixed refresh rate, as in Deligiannidis et al. (2021), then we observe
better mixing for 𝜃 but worse mixing for ∥𝜽∥ as we increase 𝑑 – see the
1
right-handcolumnofFigure5.4.
One important consequence of the theoretical results, and that is seen
in the results in Figure 5.4, is that care is needed when we assess mixing
and convergence of, in particular, the Bouncy Particle Sampler – as we
cangetsubstantiallydifferentmeasuresofmixing,suchasauto-correlation
time, depending on which function of the state we consider. Observing
a fast mixing chain for one component may mask that other functions of
thestatearemixingveryslowly(seeSection6.2.4formorediscussionon
summarising convergence in multivariate settings and measures that can
givedifferingimportancetodifferentcoordinates).
Bierkens et al. (2023a) investigate fixed, finite-dimensional normal tar-
getswherebetween1and 𝑑 −1principalcomponentshavealengthscale
of 𝜖, while the remainder have a length scale of 1. Both algorithms have
𝑂(𝜖−1)eventsperunittimebecausethemomentumis𝑂(1)butsomelength
scales are 𝑂(𝜖). The Bouncy Particle Sampler mixes in 𝑂(1) time; how-
ever,thealignmentoftheZig–ZagSampleriscrucialtoitsmixingtime:if
theprincipalaxesofthetargetarealignedwiththe𝑑 Zig–Zagmomentum
componentsthenitalsomixesin𝑂(1) time,butinalmostallotherscenar-
ios its mixing time is 𝑂(𝜖−1). Preconditioning is usually advised for any
MCMCalgorithm;thisanalysishighlightsthattheneedforpreconditioning
the Zig–Zag Sampler is even more marked than it is for Bouncy Particle
Samplers.
5.4 EfficientSimulationofPDMPSamplers
We now consider various approaches for simulating the PDMP samplers.
As a running example that we will use to help explain some of the ideas,
wewillconsiderthelogisticregressionmodelwithaGaussianprior,which
wasintroducedinSection1.2.1.
5.4.1 SimulatingPDMPs
WhensamplingfromtheGaussiantarget,theratesthatdeterminethetime
untilthenexteventwerelinearandthuswecouldsimulatetheeventtimes160 Continuous-TimeMCMC
of the PDMP exactly. What happens for more complicated targets where
thisisnotpossible?Herewedescribethreepossiblemethodsforsimulating
theseevents.
ThemostcommonapproachesforsimulatingeventsofaPDMParebased
ontheideaofPoissonthinning(seeSection5.2.2).Rememberthisinvolves
upper-bounding the event rate, simulating events with this bounding rate,
andthenacceptingthesimulatedeventswiththeratioofthetrueratetothe
boundingrate. Thechallengewith Poissonthinningis findinggoodupper
bounds that are simple enough that we can simulate events analytically,
and,ideally,closetothetruerate,asthecomputationalefficiencyofPoisson
thinningdependsonhowclosetheboundingrateistothetruerate.Thefirst
twomethodswedescribearebasedonthisideabutdifferintheassumptions
theymakeonthetargetandhowtheboundingratesareconstructed.
ThefirstapproachcomesfromBierkensetal.(2019b)andassumesthat
the Hessian of the minus log-target is bounded. To simplify the notation,
definethevectorU(𝜽) =−∇log𝜋(𝜽),soU= (𝑈 1(𝜽),...,𝑈 𝑑(𝜽)) with
𝜕log𝜋(𝜽)
𝑈 𝑗(𝜽) =−
𝜕𝜃
.
𝑗
NowtheHessianof−log𝜋(𝜽) isa𝑑×𝑑 matrixH(𝜽) definedas
H(𝜽) = (cid:0) ∇𝑈 1(𝜽) ··· ∇𝑈 𝑑(𝜽) (cid:1).
ThenweassumethatthereissomematrixJsuchthatforanyvector,w,and
any𝜽,
w⊤H(𝜽)w ≤ w⊤Jw.
ThisholdsforGaussiantargetdistributions,asH(𝜽) =Hisaconstant.More
importantly, it holds for targets which are heavier-tailed than Gaussian,
includingtheposteriordistributionforlogisticregressionormanyversions
of robust regression if we have e.g. Gaussian priors on the parameters of
themodel.
Now,asintroducedbefore,considerarateforthenextevent,orthenext
specifictypeofevent,inaPDMP,𝜆˜ (𝑡),intermsofthefurthertimeuntil
z
theevent𝑡.Assumethat
𝜆˜ (𝑡) =max{0,w·U(𝜽 +p𝑡)},
z
whereU = −∇log𝜋 asdefined above,the currentstate is (𝜽,p),and wis
somevectorthatdependsonthesampler.FortheBouncyParticleSampler
or the Coordinate Sampler, if we are considering time until the next non-
refresh event, then w = p, whereas for the Zig–Zag Sampler if we are5.4 EfficientSimulationofPDMPSamplers 161
considering the next flip of component 𝑗 then w will be either the unit
vectorwith1or−1inthe 𝑗thecomponentandzeroelsewhere.
Nowconsiderthetermw·U(𝜽 +p𝑡).Thiscanberewrittenas
w·U(𝜽 +p𝑡)
=w·U(𝜽)+∑︁𝑑
𝑤
𝑖∫ 𝑡 d𝑈 𝑖(𝜽 d𝑠+p𝑠)
d𝑠
𝑖=1 0
𝑑 ∫ 𝑡
∑︁
=w·U(𝜽)+ 𝑤
𝑖
p·∇𝑈 𝑖(𝜽 +p𝑠)d𝑠
𝑖=1 0
∫ 𝑡
=w·U(𝜽)+ w⊤H(𝜽 +p𝑠)pd𝑠.
0
Thefirstequalitycomesfromwritingthevalueofafunctionattime𝑡asits
valueattime0plustheintegralofitsderivativefromtime0totime𝑡.We
thenusethechainruletogetthederivativeof𝑈 𝑖(𝜽+p𝑠) withrespectto𝑠,
andfinallythedefinitionoftheHessianmatrix.
Now,byCauchy–Schwarzforvectors,w⊤Hp ≤ ∥w∥∥Hp∥,where∥w∥ =
((cid:205) 𝑖𝑑 =1𝑤 𝑖)1/2isthe𝐿 2normofthevectorw.This,togetherwithourassump-
tionontheboundoftheHessian,gives
w⊤H(𝜽 +p𝑠)p ≤ ∥w∥∥H(𝜽 +p𝑠)p∥ ≤ ∥w∥∥Jp∥
whichisaconstant.Thuswegetthelinearbound
w·U(𝜽 +p𝑡) ≤ w·U(𝜽)+∥w∥∥Jp∥𝑡,
orequivalentlythepiecewiselinearboundontherate
𝜆˜ (𝑡) ≤ max{0,w·U(𝜽)+∥w∥∥Jp∥𝑡}.
z
We can simulate events from this upper-bounding rate analytically, in an
equivalentwaytowhichwesimulatetheeventsfortheGaussiantarget.If
w≠p,wecanuseinsteadw⊤Hp ≤ ∥p∥∥Hw∥ togetthealternativebound
𝜆˜ (𝑡) ≤ max{0,w·U(𝜽)+∥p∥∥Jw∥𝑡}.
z
Oursecondapproach(SuttonandFearnhead,2023)isbasedonadifferent
assumptionforthetarget,namelythatwecandecompose−∇log𝜋(𝜽+p𝑡),
as a function of 𝑡 for any 𝜽 and p into the sum of convex and concave
functions,andassumetheconcavefunctionisdifferentiableeverywhere.A
function 𝑓 (𝑡) for𝑡 ≥ 0 is a convex function if for any 0 ≤ 𝑟 < 𝑠 < 𝑡, we
∪
have
𝑡−𝑠 𝑠−𝑟
𝑓 (𝑠) ≤ 𝑓 (𝑟)+ 𝑓 (𝑡),
∪ 𝑡−𝑟 ∪ 𝑡−𝑟 ∪
whileafunction 𝑓 (𝑡) for𝑡 ≥ 0isaconcavefunctionif−𝑓 (𝑡) isconvex.
∩ ∩162 Continuous-TimeMCMC
Thatisifwepickanytwopointsonthefunctionandjointhembyastraight
line,thenthefunctionliesbelowthelineifitisconvex,andabovetheline
ifitisconcave.SeeFigure5.5foranexample.
Wewillassumethatwecandecomposetherateuntilthenexteventas
𝜆˜ (𝑡) =max{0, 𝑓 (𝑡)+ 𝑓 (𝑡)},
z ∪ ∩
i.e. in terms of a single convex and single concave function – though the
ideasbelowapplytriviallyifourdecompositioninvolvesmultipleconcave
andconvexfunctions.(Infact,thesumofconvexfunctionsisconvex,and
the sum of concave functions is concave, so we can immediately simplify
thedecompositiontothecaseweareconsidering.)Ourstartingpointisthe
bound
𝜆˜ (𝑡) =max{0, 𝑓 (𝑡)+ 𝑓 (𝑡)} ≤ max{0, 𝑓 (𝑡)}+max{0, 𝑓 (𝑡)}. (5.14)
z ∪ ∩ ∪ ∩
Sutton and Fearnhead (2023) then use the fact that we can bound 𝑓 (𝑡)
∪
and 𝑓 (𝑡) by piecewise linear functions just by evaluating the functions
∩
at a set of grid points. Once we have these piecewise linear bounds, they
immediatelygiveusapiecewiselinearboundon𝜆˜ (𝑡)bysubstitutingthem
z
into(5.14).
Howdowegetpiecewiselinearboundson 𝑓 (𝑡)and 𝑓 (𝑡)?Itissimplest
∪ ∩
to see this through a picture – see Figure 5.5. For the convex function,
the bound comes immediately from the definition: if we evaluate 𝑓 (𝑡) at
∪
𝑡 =0and𝑡 =𝑡 ,thenthestraightlinethatjoinsthesepointsgivesanupper
1
boundon [0,𝑡 ].Foraconcavefunction,weneedtoevaluate 𝑓 (𝑡) andits
1 ∩
derivative at 𝑡 = 0 and 𝑡 = 𝑡 . We then construct the tangents to 𝑓 (𝑡) at
1 ∩
𝑡 =0and𝑡 =𝑡 ,andthefunctionliesbelowbothtangents.
1
The above approach gives upper bounds on some interval [0,𝑡 ], and
1
wecanproceedbyusingtheseupperboundstosimulateevents,ifany,on
[0,𝑡 ]. If there are no events, we then choose some 𝑡 > 𝑡 and calculate
1 2 1
an upper bound on [𝑡 ,𝑡 ] and repeat the process. In practice, Sutton and
1 2
Fearnhead (2023) suggest choosing 𝑡 ,𝑡 ,... to be equally spaced, and
1 2
suggestwaysofchoosingthespacinginanadaptivewaythatbalancesthe
number of times we do not simulate an event from the bounding process
on an interval, against the number of times we simulate events from the
boundingprocessthatarenotaccepted.Theideaisthatiftheintervalsare
toolow,wewastetimebyhavingtoosmallanintervalandthushavingto
calculatetheupperbound,whereasiftheintervalistoolargethentheupper
bound can become loose and we waste time by simulating lots of events
that are rejected. It is also possible to recycle calculations used to decide
whethertoacceptaneventtoimprovethebounds.5.4 EfficientSimulationofPDMPSamplers 163
       
       
   
   
   
   
   
   
   
                       
t t
Figure5.5 Exampleofaconvexfunction(left)andaconcave
function(right).Foraconvexfunction,thestraightline(shownin
grey)thatjoinsanytwopointswillupper-boundthefunction
betweenthosetwopoints.Foraconcavefunction,thestraightline
(showningreydashed)thatjoinsanytwopointswilllower-bound
thefunctionbetweenthosetwopoints.Foraconcavefunction,we
canupper-boundthefunctionbyanytangenttothefunction
(showningrey).Exampleboundsfor [0,0.5] and [0.5.1] are
shownbythegreyline(left-handplot)andminimumofthegrey
lines(right-handplot).
The final set of methods we will overview is based on simulating the
eventsusingnumericalmethods.Therehavebeentwodistinctapproaches
that have been suggested. The first is based on the equation for directly
simulatingtheeventtimes.Rememberthatwecansimulatethenextevent
timeforaprocesswithrate𝜆˜ (𝑡)bysimulating𝑢,arealisationofastandard
z
uniformrandomvariable,andthenfinding𝜏thesolutionto
∫ 𝜏
− 𝜆˜ (𝑠)d𝑠 =log(1−𝑢).
z
0
The smallest solution, 𝜏, of this equation, is the time until the next event.
Paganietal.(2020)suggestsolvingthisequationnumericallyusingBrent’s
method(Pressetal.,2007).Theotherapproachistousenumericalmethods
tofindanupperbound.Corbellaetal.(2022)suggestsuchamethod,where
theyuseBrent’salgorithmtofindthemaximumof𝜆˜ (𝑡) onsomeinterval
z
[0,𝑡 ], then propose points from a constant rate set to this maximum and
1
usePoissonthinning.Ifnoeventissimulatedovertheinterval [0,𝑡 ] they
1
repeattheprocessonthenextinterval.
f
u
f
u164 Continuous-TimeMCMC
The advantage of using such numerical methods is that they are fully
general, that is they can be applied to any model in an automatic manner.
The disadvantages are two-fold. First, computationally they can be slow,
depending on the numerical methods used. The second is that numerical
errors may lead to errors in the simulation of the dynamics of the PDMP,
so that the resulting PDMP may no longer target the correct distribution.
The hope is that any numerical error is small so that the PDMP will have
a stationary distribution that is still close to the target distribution. Pagani
etal.(2020)giveresultsonhownumericalerrorwillimpactthedistribution
thatthePDMPissamplingfrom.
Example:BoundedHessianforLogisticRegression
Logisticregression isone examplewherewe havea boundedHessian.To
seethis,let𝜋(𝜽)betheposteriorforthelogisticregressionmodelofSection
1.2.1withaGaussianprior.Thendifferentiating(1.5)gives
−𝜕2log𝜋(𝜽) = (cid:2) 𝚺−1(cid:3) +∑︁𝑁 𝑥(𝑖)𝑥(𝑙) (cid:40) exp{x⊤ 𝑗𝜽} (cid:41)(cid:40) 1 (cid:41) ,
𝜕𝜃 𝑖𝜕𝜃
𝑙
𝜽 𝑖,𝑙
𝑗=1
𝑗 𝑗 1+exp{x⊤ 𝑗𝜽} 1+exp{x⊤ 𝑗𝜽}
where the subscripts 𝑖,𝑙 denote the (𝑖,𝑙)th element of the corresponding
matrix.Now,foranyprobability𝑞 wehave𝑞(1−𝑞) ≤ 1/4,so
𝜕2log𝜋(𝜽)
≤
(cid:2) 𝚺−1(cid:3)
+
1∑︁𝑁
𝑥(𝑖)𝑥(𝑙).
𝜕𝜃 𝑖𝜕𝜃 𝑙 𝜽 𝑖,𝑙 4 𝑗 𝑗
𝑗=1
Ifweintroducea𝑁×𝑑matrixXwhose(𝑗,𝑙)thentryis𝑥(𝑙),thenthisgives
𝑗
aboundontheHessianof−log𝜋(𝜽) thatisJ= 𝚺−1+(1/4)X⊤X.
FortheBouncyParticleSampler,therateofthenextbounceevent,ifthe
currentstateis (𝜽,p),is
max{0,p·∇(−log𝜋(𝜽 +𝑡p)} ≤ max{0,−p·∇(log𝜋(𝜽)+∥p∥∥Jp∥𝑡},
by the above argument. For the Zig–Zag Sampler, the rate of the next flip
ofcomponent𝑖 ofthevelocityisboundedaboveby,forexample,
(cid:26) 𝜕log𝜋(𝜽) √ (cid:27)
max 0,−𝑝 𝑖 𝜕𝜃 + 𝑑∥Je𝑖∥𝑡 .
𝑖
Example:Concave–convexSamplingforBayesianMatrixFactorisation
ConsidertheBayesianmatrixfactorisationmodelofSection1.2.2.Tosim-
plifynotationwewillassumeanimproperuniformpriorfortheparameters5.4 EfficientSimulationofPDMPSamplers 165
𝜽 = {U,V}andset𝜎2 =1.Theresultinglog-posterioris
 𝑛 𝑚 (cid:32) 𝑑 (cid:33)2
log𝜋(U,V|Y) =−1  ∑︁∑︁ 𝑌
𝑖𝑗
−∑︁ 𝑈 𝑖𝑘𝑉
𝑘𝑗
 .
2
 𝑖=1 𝑗=1 𝑘=1  
 
Aswewillsee,theeventratesforthismodelfortheZig–ZagSamplerorthe
BouncyParticleSamplerarepolynomialsofthetimetoanevent,andsuch
events can be simulated by concave–convex sampling. We will show this
fortheZig–ZagSampler,buttheextensiontotheBouncyParticleSampler
issimple.
Considertherateforupdate𝑈 𝑖,𝑙.Thisdependsonthederivativeofminus
log𝜋,whichis
𝜕log𝜋(U,V|Y) ∑︁𝑚 (cid:32) ∑︁𝑑 (cid:33)
− = 𝑌 − 𝑈 𝑉 𝑉 .
𝜕𝑈 𝑖,𝑗 𝑖,𝑘 𝑘,𝑗 𝑙,𝑗
𝑖,𝑙
𝑗=1 𝑘=1
Ifthecurrentstateisgivenbyaposition (U,V) andavelocity (U(cid:164),V(cid:164)) then
therateofaneventasafunctionofthetimetothenextevent𝑡 is
𝜕log𝜋(U+𝑡U(cid:164),V+𝑡V(cid:164)|Y)
−𝑈(cid:164)
𝑖,𝑙 𝜕𝑈
𝑖,𝑙
𝑚 (cid:32) 𝑑 (cid:33)
∑︁ ∑︁
=𝑈(cid:164) 𝑌 − (𝑈 +𝑡𝑈(cid:164) )(𝑉 +𝑡𝑉(cid:164) ) (𝑉 +𝑡𝑉(cid:164) ).
𝑖,𝑙 𝑖,𝑗 𝑖,𝑘 𝑖,𝑘 𝑘,𝑗 𝑘,𝑗 𝑙,𝑗 𝑙,𝑗
𝑗=1 𝑘=1
Thisiscubicin𝑡,anditiseasytoobtainaconcave–convexdecomposition
of this rate. The convex function will be the sum of terms in the cubic
expressionthathavepositivecoefficients,andtheconcavefunctionwillbe
thesumoftermswithnegativecoefficients.
5.4.2 ExploitingModelSparsity
One advantage of PDMP samplers is that they can take advantage of a
certaintypeofsparsityinthetargetdistributiontospeedupcomputation.
ThisismosteasilydescribedfortheZig–ZagSampler,thoughsimilarideas
canbeusedforanadaptedversionoftheBouncyParticleSampler(seethe
section on the Local Bouncy Particle Sampler in Bouchard-Coˆte´ et al.,
2018).
WehavedescribedhowwecansimulatetheZig–ZagSamplerbysimu-
lating𝑑eventtimes,oneforeachpossibletransitionofthevelocity.Wethen
implementtheeventthatoccursfirst,andthenresimulatethefurthertimes
foreachofthe 𝑑 possibleevents.Werepeatthisprocessmultipletimesin166 Continuous-TimeMCMC
ordertosimulatetheskeletonofarealisationoftheprocess.However,we
canimproveonthisimplementationiftheeventthatoccursdoesnotaffect
theratesofmanyoftheothertypesofevents.Thiscanhappenifthemodel
hasaformofsparsityintermsof
𝜕log𝜋(𝜽)
(5.15)
𝜕𝜃
𝑖
onlydependingonasmallnumberofthecomponentsof𝜽.
To describe the idea formally, it is helpful to introduce some notation.
For𝑖 =1,...,𝑑letS 𝑖 ⊂ {1,...,𝑑}denotethecomponentsof𝜽 that(5.15)
depends on. So if 𝑗 is not in S 𝑖, then (5.15) will not change as we vary
𝜃 𝑗 ifwekeepallothercomponentsof 𝜽 fixed.Thismeansthatifwehave
an event that changes the 𝑖th component of 𝜽, then this will only affect
the future rate of events that change the 𝑗th component of 𝜽 for 𝑗 ∈ S 𝑖.
By the Markov property of the PDMP, if the rates are unchanged, we can
re-use the simulated event times for 𝑗 ∉ S 𝑖 if 𝑗 ≠ 𝑖. We obviously need
tore-simulatetheeventthatflipsthe𝑖thcomponentofthevelocityevenif
𝑖 ∉S 𝑖.TheresultingalgorithmisshowninAlgorithm11,withthekeypart
beingthataftereacheventweonlyre-simulatesomeoftheeventtimes,for
othereventswejustupdateandre-usethepreviouslysimulatedtimes.
As one example of the potential advantage of this algorithm, consider
simulating a Gaussian target where the precision matrix is tri-diagonal.
That is, the (𝑖, 𝑗) entry of the precision matrix is zero if |𝑖− 𝑗| > 1. Such
a model occurs if there is some form of Markov or AR(1) structure to the
components of 𝜽. In this case, S 𝑖 = {𝑖 −1,𝑖,𝑖 +1} for 𝑖 = 2,...,𝑑 −1,
withS
1
= {1,2}andS
𝑑
= {𝑑−1,𝑑}.TounderstandtheideaofAlgorithm
11,imagine 𝑑 = 5say,andthatwehavesimulatedthatthefurthertimeto
the five possible events is 0.3, 0.7, 1.3, ∞ and 0.5. The event that occurs
firstcorrespondstoflippingthefirstcomponentofthevelocity.Thischange
only affects the rate at which future events that affect the first and second
components of the velocity occur. So we need to re-simulate the further
timetothenexteventofthesetypes.Fortheotherthreetypesofevents,we
justupdatethefurthertimetotakeaccountofthefactthatatimeoflength
0.3 has passed. Thus the events at which the third to fifth components of
thevelocitychangewillnowoccurafterafurthertimeperiodof1.0,∞and
0.2,respectively.
Intermsofthecomputationaladvantageofthisscheme,inthisexample,
if 𝑑 is large then the computational cost per iteration involves resampling
atmost3eventtimesratherthan𝑑eventtimes.Itisalsopossibletofurther
improveonAlgorithm11byusingthefactthattheorderofoccurrenceof5.4 EfficientSimulationofPDMPSamplers 167
Algorithm11:Zig–ZagSampler:ExploitingSparsity
Input:Eventratesforeachtypeofevent𝜆 𝑖,initialstate (𝜽,p),
simulationtime𝑇.
Set𝑠 =0and 𝑘 =0.
for𝑖 =1,...,𝑑 do
Simulate𝑡
𝑖
thetimeuntilthenexteventthatflipsthe𝑖th
componentofthevelocity.
end
whiles¡T do
Calculatefurthertimetonextevent𝑡 =min𝑖=1,...,𝑑{𝑡 𝑖}.
Updateposition𝜽𝑠+𝑡 = 𝜽𝑠 +𝑡p.
Decideoneventtype,𝑖∗ =argmin{𝑡 𝑖}.
Updatevelocityp𝑠+𝑡 = 𝐹 𝑖∗(p).
Updatetime𝑠 = 𝑠+𝑡.
Storeskeletonpoints:set 𝑘 = 𝑘 +1,𝜏 𝑘 = 𝑠and𝜽𝜏 = 𝜽𝑠.
𝑘
Updatefurthertimetoevents:
for𝑖 =1,...,𝑑 do
if𝑖 ∈ S
𝑖∗
∪{𝑖∗}then
Simulate𝑡
𝑖
thetimeuntilthenexteventthatflipsthe𝑖th
componentofthevelocity.
end
else
Set𝑡
𝑖
=𝑡
𝑖
−𝑡.
end
end
end
Output:Skeletonofevents{(𝜏 𝑘,𝜽𝜏 𝑘)}𝑛
𝑘=0
eventsthatwedonotre-simulatewillnotchange(seeBouchard-Coˆte´etal.,
2018)–andthiscanreducethecostperiterationtobeoftheorderofthe
numberofeventtimesthatweneedtore-simulate.
Example:LogisticRegression
When would this idea be useful for sampling from the posterior of our
logisticregressionmodel?Therateofaneventthatflipscomponent𝑖ofthe
velocitydependsonthe𝜃
𝑖
derivativeoflog𝜋(𝜽),whichfrom(1.5),is
−(cid:2) 𝜽⊤𝚺 𝜽−1(cid:3)
𝑖
+∑︁𝑁
𝑥( 𝑗𝑖)
(cid:40)
𝑦
𝑗
−
1+ex ep x{ px {⊤
𝑗
x𝜽 ⊤} 𝜽}(cid:41)
.
𝑗=1 𝑗168 Continuous-TimeMCMC
Weneedthistodependononlyasmallsetofcomponentsof𝜽.Thiswould
requiretwothings.Firstthat𝚺−1issparsesoonlyasmallnumberofentries
𝜽
of the 𝑖th row or column of 𝚺−1 are non-zero. Second, we would require
𝜽
that the observations for which
𝜃(𝑖)
≠ 0 would, combined, only have a
𝑗
smallnumberofcomponentsofthecovariatesthatarenon-zero.Formally,
we can define the set of rates that we would need to update after a flip of
component𝑖 of𝜽 as
(cid:110) (cid:111)
S 𝑖 = 𝑘 : (𝚺−1) 𝑖,𝑘 ≠0, or∃ 𝑗 suchthatx( 𝑗𝑖)x( 𝑗𝑘) ≠0 .
This can happen for models with random effects which are included
within 𝜽. If we set the random effects to be 𝜃 1,...,𝜃 𝑁, then for 𝑗 ∈
{1,...,𝑁}, x(𝑗) = 1 and x(𝑖) = 0 for 𝑖 ≠ 𝑗. If further, we have that the
𝑗 𝑗
randomeffectsareindependentofeachotherandtheotherparameters,S
𝑖
willonlyincludetheparametersofthefixedeffects.
5.4.3 DataSubsamplingIdeas
OnepotentialadvantageofPDMPsamplersinBayesianstatisticsisthatthey
can use subsampling ideas to reduce the computational cost per iteration.
ThiswasfirstsuggestedfortheZig–ZagSamplerbyBierkensetal.(2019b),
thoughtheideasapplymorewidely.
Thestartingpointisamoregeneralobservationthatwecanpotentially
simulate from a target distribution if we have an unbiased estimator of
∇log𝜋. This is most easily seen for the Zig–Zag Sampler, and we will
focus just on this case for simplicity. See Fearnhead et al. (2018) and
relatedideasforthelocalBouncyParticleSamplerinBouchard-Coˆte´ etal.
(2018)forhowthisisgeneralisedtootherPDMPs.
The Zig–Zag Sampler has 𝑑 possible types of event. Consider the 𝑖th
suchevent.Ifthecurrentpositionis𝜽andthe𝑖thcomponentofthevelocity
is 𝑝 𝑖,thenthiscomponentflipswitharate
(cid:26) 𝜕log𝜋(𝜽)(cid:27)
max 0,−𝑝 𝑖 𝜕𝜃 .
𝑖
The key property of this rate that means that the sampler targets 𝜋(𝜽) is
thatthedifferenceinratebetweenaflipfrom 𝑝 𝑖 to−𝑝 𝑖 andtherateofthe
reverseeventis
(cid:26) 𝜕log𝜋(𝜽)(cid:27) (cid:26) 𝜕log𝜋(𝜽)(cid:27) 𝜕log𝜋(𝜽)
max 0,−𝑝 𝑖 𝜕𝜃 −max 0,𝑝 𝑖 𝜕𝜃 =−𝑝 𝑖 𝜕𝜃 .
𝑖 𝑖 𝑖
In practice, for Zig–Zag, one of the two rates is equal to 0, and this is the5.4 EfficientSimulationofPDMPSamplers 169
most efficient choice and corresponds to the Zig–Zag Sampler using the
canonicalrates(seeSection5.3.1).
Nowimaginewehaveafamilyofvector-valuedrandomvariablesG(𝜽),
such that E[G(𝜽)] = −∇log𝜋(𝜽) for all 𝜽. Then if we implement the
same dynamics as the Zig–Zag Sampler, but with the rate of flipping the
𝑖thcomponentofthevelocityequalto
E[max{0,𝑝 𝑖𝐺 𝑖(𝜽)}],
then this will also produce a PDMP sampler that targets 𝜋. To see this, as
above,considerthedifferenceintherateofflipping 𝑝 𝑖 to−𝑝 𝑖 andtherate
ofthereverseevent.Thisis
E[max{0,𝑝 𝑖𝐺 𝑖(𝜽)}]−E[max{0,−𝑝 𝑖𝐺 𝑖(𝜽)}]
=E[max{0,𝑝 𝑖𝐺 𝑖(𝜽)}−max{0,−𝑝 𝑖𝐺 𝑖(𝜽)}]
𝜕log𝜋(𝜽)
=E[𝑝 𝑖𝐺 𝑖(𝜽)] = 𝑝 𝑖E[𝐺 𝑖(𝜽)] =−𝑝
𝑖 𝜕𝜃
,
𝑖
where we have used the standard result max{0,𝑥} − max{0,−𝑥} = 𝑥,
linearityofexpectation,andthedefinitionoftheexpectationofG.Thisis
preciselytheconditionweneedontheratesforaPDMPSamplerwiththe
Zig–Zag dynamics to target 𝜋, the only difference is the rates being used
arenolongerthecanonicalrates.
InordertousetheZig–ZagSamplerwiththeseratesweneedawayof
simulatingtheevents.ThisismorechallengingthanforstandardZig–Zag
asweneedtodealwiththeratesbeingdefinedimplicitlybyanexpectation.
Todothis,thestandardapproachistofindabounding 𝑏 𝑖(𝜽) suchthatfor
any realisation of G(𝜽) we have 𝑝 𝑖𝐺 𝑖(𝜽) ≤ 𝑏 𝑖(𝜽). If we can find such a
boundthenwecanstillusePoissonthinningtosimulatetheevents.Letthe
timeuntilthenexteventwhichflips 𝑝 𝑖 be
𝜆˜ z(𝑖)(𝑡) =E[max{0,𝑝 𝑖𝐺 𝑖(𝜽 +𝑡p)}],
and the corresponding bounding rate, which is used to simulate potential
events, be 𝑏 𝑖(𝜽 +𝑡p). Then Poisson thinning for events of rate 𝜆˜ z(𝑖)(𝑡) is
possiblebyusingthefollowingsteps:
(T0) Setcurrenttimeto𝑠 =0
(T1) Simulate the time 𝜏 > 𝑠 of the next event a process with rate 𝜆¯(𝑡) =
𝑏 𝑖(𝜽 +𝑡p).
(T2) Simulate𝑔 𝑖,arealisationof𝐺 𝑖(𝜽 +𝜏p).
(T3) Accepttheeventtimewithprobabilitymax{0,𝑝 𝑖𝑔 𝑖}/𝑏 𝑖(𝜽 +𝑡p).Other-
wiseset𝑠 = 𝜏andreturnto(T1).170 Continuous-TimeMCMC
ToseethatthisisavalidPoissonthinningalgorithmtosimulateeventswith
rate𝜆˜(𝑖)(𝑡),wejustneedtocalculatetheprobabilityofacceptinganevent
z
in step (T3). By averaging over the possible realisation of 𝑔 𝑖 in step (T2)
andusingthefactthatbydefinitionforany𝑔 𝑖,theprobabilityinstep(T3)
islessthanorequalto1,thisis
E[max{0,𝑝 𝑖𝐺 𝑖}/𝑏 𝑖(𝜽 +𝑡p)] =
E[m 𝑏a 𝑖(x 𝜽{0 +, 𝑡𝑝 p𝑖𝐺
)
𝑖}]
=
𝑏
𝑖𝜆 (˜ 𝜽z(𝑖) +(𝑡 𝑡)
p),
asrequired.
Howdoesthisidearelatetotheuseofsubsampling?Consider𝜋beinga
posteriordistribution,andsupposethatlog𝜋canbewrittenasasum
𝑁
∑︁
log𝜋(𝜽) = log𝜋 𝑗(𝜽),
𝑗=1
where log𝜋 𝑗 for 𝑗 = 1,...,𝑁 is 1/𝑁 times the log-prior plus the log-
likelihood contributions from the 𝑗th data point. Then this gives a simple
wayofconstructinganunbiasedestimatorof−∇log𝜋(𝜽),bysimulating 𝐼
uniformlyon{1,...,𝑁}andsettingG(𝜽) to−𝑁∇log𝜋 𝐼(𝜽).
The advantageof using suchan unbiased estimatorwithin theZig–Zag
SampleristhatateachiterationofthePoissonthinningalgorithmusedto
simulate an event, i.e. step (T2) and (T3) above, we need to process only
one data point. This gives a per-iteration saving of a factor of 𝑁 over the
standardZig–ZagSamplerwhichrequirescalculatingderivativesoflog𝜋.
However, there are additional costs to using this subsampling idea. First,
often the bounds that we use for Poisson thinning will be larger if we use
subsampling – as they have to bound the rate for all possible realisations
of 𝑔 𝑖. This will lead to more iterations of the Poisson thinning algorithm
to simulate the PDMP for the same amount of (stochastic process) time.
Second, as we are no longer using the canonical rates we will introduce
more events, and this will lead to more random-walk-like behaviour and
slower mixing. Empirical results in Bierkens et al. (2019b) suggest that
the overall effect of these is to counteract the factor of 𝑁 improvement in
per-iterationcost.
SocansubsamplingideaswithinPDMPsbebeneficial?Itturnsoutthey
can,butwemustuseabetter,i.e.lowervariance,estimatorforG.Thiscan
bedoneusingcontrolvariateideasthatarecommoninSGLD(seeSection
3.3.1). We first run an optimisation algorithm, such as SGD, to find the
mode or a value close to the mode of log𝜋. Denote this value by(cid:98)𝜽. Then5.4 EfficientSimulationofPDMPSamplers 171
wecanwrite
𝑁
∑︁
log𝜋(𝜽) =log𝜋((cid:98)𝜽)+ {log𝜋 𝑗(𝜽)−log𝜋 𝑗((cid:98)𝜽)}.
𝑗=1
Soanunbiasedestimatorcanbeobtainedbyfirstsampling 𝐼 uniformlyon
{1,...,𝑁}andthensettingG(𝜽) to
−log𝜋((cid:98)𝜽)−𝑁{∇log𝜋 𝐼(𝜽)−∇log𝜋 𝐼((cid:98)𝜽)}.
Importantlytheterm−log𝜋((cid:98)𝜽) isaconstant,sorequiresasingleup-front
𝑂(𝑁) cost to calculate it. Then evaluating a realisation of this estimator
has 𝑂(1) cost. If the Hessian of −log𝜋 is bounded, then Bierkens et al.
(2019b)showthatwecanobtaintheboundsneededtosimulatetheZig–Zag
Samplerusingthisunbiasedestimatorusingthelinearboundsdescribedfor
boundedHessiantargetsinSection5.4.1.Inthiscase,forafixedaccuracy
ofthefinalMonteCarlosample,wecanobtainaspeed-upbyafactorof𝑁,
afterfinding(cid:98)𝜽 andcalculating−log𝜋((cid:98)𝜽),relativetothestandardZig–Zag
Sampler.
Example:SubsamplingforLogisticRegression
To further explain how to implement the Zig–Zag Sampler with subsam-
pling,wewillconsiderthelogisticregressionmodel.Tosimplifyexposition
wewillassumethatwehaveanimproperflatprior,sointhenotationabove
𝜋 (𝜽) ∝1.Thuswecandropthecontributionofthepriorto𝜋andwehave
0
𝜋(𝜽) ∝(cid:206)𝑁 𝑗=1𝜋 𝑗(𝜽) with
𝜋 𝑗(𝜽) = (cid:32) 1ex +p e{ x𝑦 p𝑗 {x x⊤ 𝑗 ⊤𝜽 𝜽} }(cid:33) ,
𝑗
where 𝑦 𝑗 is the binary response and x𝑗 is the vector of covariates for the
𝑗thobservation.
Takingthefirstderivativesof−log𝜋(𝜽) gives
(cid:40) (cid:41)
−𝜕log𝜋 𝑗(𝜽)
=𝑥(𝑖)
exp{x⊤ 𝑗𝜽}
−𝑦 .
𝜕𝜃
𝑖
𝑗 1+exp{x⊤ 𝑗𝜽} 𝑗
Using 0 ≤ exp(𝑎)/(1 + exp(𝑎)) ≤ 1, we have that the modulus of this
derivativeisboundedby𝑥(𝑖).Thusifweusetheunbiasedestimator
𝑗
𝐺 𝑖(𝜽)
=−𝑁𝜕log 𝜕𝜃𝜋 𝐼(𝜽)
, 𝐼 uniformlydistributedon{1,...,𝑁},
𝑖172 Continuous-TimeMCMC
thenwecanboundtherateofaneventby
𝑏 𝑖(𝜽) = 𝑁 max |𝑥( 𝑗𝑖)|.
𝑗=1,...,𝑁
Inthefollowing,wewillcalltheresultingsamplerZig–Zagwithsubsam-
pling.
Howaboutifweusecontrolvariates?Fix(cid:98)𝜽,andconsidertheestimator
ofthegradient
(cid:32) (cid:33)
𝐺(𝐶𝑉)(𝜽)
=−𝜕log𝜋((cid:98)𝜽)
−𝑁
𝜕log𝜋 𝐼(𝜽)
−
𝜕log𝜋 𝐼((cid:98)𝜽)
,
𝑖 𝜕𝜃 𝜕𝜃 𝜕𝜃
𝑖 𝑖 𝑖
withagain,𝐼 isuniformlydistributedon{1,...,𝑁}.Toobtainappropriate
boundsforp(𝑗)G(𝑗)(𝜽),considerthesecondderivatives
𝐶𝑉
(cid:40) (cid:41)
−𝜕2log𝜋 𝑗(𝜽)
=𝑥(𝑖)x(𝑙)
exp{x⊤ 𝑗𝜽}
.
𝜕𝜃 𝑖𝜕𝜃
𝑙
𝑗 𝑗 (1+exp{x⊤ 𝑗𝜽)})2
As before using that 0 ≤ exp(𝑎)/(1 + exp(𝑎))2 ≤ 1/4, we can bound
the modulus of this second derivative by
(1/4)|x(𝑖)x(𝑙)|.
This gives the
𝑗 𝑗
followingbound
(cid:12) (cid:12)
(cid:12)𝜕log𝜋 𝑗(𝜽 +𝑡p) 𝜕log𝜋 𝑗((cid:98)𝜽)(cid:12)
(cid:12) − (cid:12)
(cid:12) 𝜕𝜃 𝜕𝜃 (cid:12)
(cid:12) 𝑖 𝑖 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)𝜕log𝜋 𝑗(𝜽 +𝑡p) 𝜕log𝜋 𝑗(𝜽)(cid:12)
(cid:12)
(cid:12)𝜕log𝜋 𝑗(𝜽) 𝜕log𝜋 𝑗((cid:98)𝜽)(cid:12)
≤ (cid:12) − (cid:12)+(cid:12) − (cid:12)
(cid:12) 𝜕𝜃 𝑖 𝜕𝜃 𝑖 (cid:12) (cid:12) (cid:12) 𝜕𝜃 𝑖 𝜕𝜃 𝑖 (cid:12) (cid:12)
1 (cid:16) (cid:17)
≤ 4|𝑥( 𝑗𝑖)|∥x𝑖∥ ∥𝜽 −(cid:98)𝜽∥+𝑡∥p∥ , (5.16)
where∥·∥denotestheEuclideannorm.Thelatterinequalitycomesfrom(i)
boundingthechangeinafunctionbythesizeofthechangeintheargument
timesaboundonthegradientinthedirectionofthechange;and(ii)asthe
gradient 𝜕log𝜋 𝑗/𝜕𝜃 𝑖 isboundedby (1/4)|x( 𝑗𝑖)x( 𝑗𝑙)| inthe𝑙thdirection,we
canboundthedot-productofthegradientwithavectorvby
𝑑 𝑑
∑︁1 4|𝑥( 𝑗𝑖)𝑥( 𝑗𝑙)𝑣 𝑙| = 1 4|𝑥( 𝑗𝑖)|∑︁ |𝑥( 𝑗𝑙)𝑣 𝑙| ≤ 1 4|𝑥( 𝑗𝑖)|∥x𝑖∥∥v∥,
𝑙=1 𝑙=1
withthelaststepusingCauchy–Schwarz.
Using (5.16), we get the following linear bound on the rate of events5.4 EfficientSimulationofPDMPSamplers 173
whenweusecontrolvariates.
max{0,𝑝 𝑖𝐺 𝑖(𝐶𝑉)(𝜽 +𝑡p)}
(cid:40) (cid:41)
𝜕log𝜋((cid:98)𝜽) 1 (cid:16) (cid:17)
≤ max 0,−𝑝 𝑖 𝜕𝜃
𝑖
+ 4𝑁𝑀 𝑖 ∥𝜽 −(cid:98)𝜽∥+𝑡∥p∥ ,
where 𝑀 𝑖 = max𝑗=1,...,𝑁(|𝑥( 𝑗𝑖)|∥x𝑗∥). In the following, we will call the
resultingsamplerZig–Zagwithcontrolvariates.
To demonstrate how subsampling works in practice and the scaling of
the methods with sample size we will compare three PDMP samplers for
logisticregressionwithsamplesizesof𝑁 =100and𝑁 =900.Thesearethe
standard Zig–Zag Sampler, Zig–Zag with subsampling and Zig–Zag with
controlvariates.Inparticular,wewanttogiveintuitionaboutthedifferent
impacts of the computational cost for estimating gradients, the efficiency
ofthePoissonthinningboundsonsimulatingevents,andthemixingofthe
PDMPonthethreealgorithms.
Results are shown in Figure 5.6. There are a number of points to draw
out. First, we see the potential advantage of subsampling in reducing the
cost per iteration of the samplers. If this is dominated by accessing and
calculatinggradientsforeachdatapoint,thenthesubsamplingversionsof
Zig–Zagareabletoproposemanymoreeventtimes(asseenbythelarger
numberofbluedotsforthemiddleandbottomrows).However,thisinitself
doesnotleadtoamoreaccurateMCMCmethodforthesamecomputational
cost–asthedrawbackofsubsamplingisthattheboundsusedforPoisson
thinning are worse. About two proposed events are needed for one actual
event with standard Zig–Zag, but this reduces to over 10 proposed events
forthetwosamplerswhichusesubsampling.
One impact of this is, if we consider 𝑁 = 100 then the standard Zig–
Zag process is simulated for stochastic process time of 80 time units, and
this is only increased to 130 time units for Zig–Zag with subsampling
and 160 time units for Zig–Zag with control variates. Moreover, the Zig–
Zagprocesseswhichusesubsamplinghaveahigheroverallrateofevents,
particularlywhencontrolvariatesarenotused.Thisleadstomorerandom-
walk behaviour for Zig–Zag with subsampling (see middle row) which
worsens the mixing of the sampling. This is less of an issue when we use
controlvariates.
Next,considerthescalingofthealgorithmsbycomparing𝑁 =100with
𝑁 = 900. First, the posterior for 𝑁 = 900 is more concentrated, but if we
re-scaleaxes(ashasbeendoneintheplots)thentheposteriorcontoursand
thedynamicsofstandardZig–Zagaresimilarforthetwocases.However,if174 Continuous-TimeMCMC
Figure5.6 ExampleoutputofZig–ZagSamplersforlogistic
regression:standardZig–Zag(toprow),Zig–Zagwith
subsampling(middlerow)andZig–Zagwithcontrolvariates
(bottomrow).Foreachplot,theheatmapshowsthecontoursof
log𝜋,theblacklineshowsthetrajectoryofZig–Zagandtheblue
dotsshowthepointswhereweproposeapossibleevent.Inall
cases,weranZig–Zagforthesamenumberofdata-pointgradient
calculations.Asthetoprowdoesnotinvolvesubsampling,this
meanswepropose𝑁-timesasmanyeventsforthemiddleand
bottomrowsasforthetoprow.Resultsfor𝑁 =100(left-hand
column)and𝑁 =900(right-handcolumn).5.5 Extensions 175
wehavefixedthecomputationalresource,thentheimpactonstandardZig–
Zag is that we can only simulate the process for a shorter period – in this
casesimulating9timesfeweractualevents.ForZig–Zagwithsubsampling,
we are able to propose the same number of events, but the higher rate of
events and the looser bound for Poisson thinning means that we are only
abletosimulateatrajectorythatisonly4timesaslongasforthestandard
Zig–Zag, despite proposing 900 times as many events. Moreover, we see
thatthetrajectoryisincreasinglydiffusiveandthustheoverallexploration
of the state-space of this sampler qualitatively looks no better than for
standard Zig–Zag. This property is shown more rigorously by Bierkens
et al. (2019b), who show that the scaling with 𝑁 of the standard Zig–Zag
andZig–Zagwithsubsamplingissimilar.Moreover,theirempiricalresults
suggest that for the same computational cost, standard Zig–Zag is more
accurate.
Bycomparison,weseebettermixingbehaviourforZig–Zagwithcontrol
variates.Thesamplerisabletosimulateatrajectorythatisapproximately
ten times as long as the standard Zig–Zag. Moreover, the mixing looks
qualitatively similar to that of the algorithm for 𝑁 = 100 and to that of
thestandardZig–Zagfor 𝑁 =900.Thisisagainshownmorerigorouslyin
Bierkensetal.(2019b),whereresultssuggestthattheaccuracyasmeasured,
say,byeffectivesamplesizeperCPUcostscalesbyafactorof𝑁 betterfor
Zig–Zag with control variates than for the other two samplers. However,
Zig–Zagwithcontrolvariatesdoesneedanadditionalpre-processingstep
tofindthemode,oravaluenearthemode,oftheposteriorandtocalculate
thegradientofthelogposterioratthisestimateofthemode.Thisimproved
scaling has been termed super-efficiency, and the fact that we can only
achieve super-efficiency after a pre-processing step is shown theoretically
byJohndrowetal.(2020).
5.5 Extensions
ThePDMPsamplerswehaveconsideredsofarareappropriateforsampling
fromcontinuousdensitiesthataredifferentiablealmosteverywhereandare
basedonspecificconstantvelocitydynamics.Wenowdescriberecentwork
atgeneralisingthesamplers:toallowsamplingfromdiscontinuoustargets;
introduce reversible-jump moves to allow sampling from targets defined
acrossspacesofdifferentdimension;andgeneralisethevelocityspaceand
theconstantvelocitydynamics.176 Continuous-TimeMCMC
5.5.1 DiscontinuousTargetDistribution
The PDMP samplers we have described can sample from target densities
thataredifferentiableeverywhere.Itisalsoeasytoseethattheyaresuitable
for densities that are non-differentiable, providing the set of points where
the density is not differentiable is a null set. However, as described, they
cannotbeusedfordensitiesthatarenotcontinuouseverywhere.
It is possible to extend PDMP samplers so they are suitable for many
discontinuoustargetdensities.TheideaistousestandardPDMPdynamics
inregionswherethetargetiscontinuous,andthenaddadditionaldynamics
wheneverthePDMPsamplerreachesapointofdiscontinuity.Thisapproach
has been suggested by Bierkens et al. (2018) for continuous densities,
but defined only on a compact region, and by Chevallier et al. (2021) in
moregenerality.Wewilloutlinethebasicideaandgivesomeexamplesof
appropriatedynamicsatpointsofdiscontinuityfordifferentsamplers.
Assume our target density 𝜋(𝜽) can be defined in terms of a set of
continuous densities, 𝜋(𝑖), each constrained to a set of open regions 𝐸 𝑖 of
R𝑑,for𝑖 =1,...,𝐾 forsome𝐾.LetΓbethesetof𝜽 pointsthatlieonthe
boundaryofoneormoreregions,andassumethisisanull-setwithrespect
toLebesguemeasureofR𝑑.Weassumethat𝐸
𝑖
andΓpartitionR𝑑,sothat
any𝜽 ∈ R𝑑 liesinpreciselyoneof𝐸 1,...,𝐸 𝐾,Γ.For𝜽 ∉ Γ,ourtargetis
𝜋(𝜽) = 𝜋(𝑖)(𝜽) for𝜽 ∈ 𝐸 𝑖.
The simplest example of such a density will be for 𝜽 constrained to some
compactregion,𝐸 .InthiscasewehaveΓastheboundaryof𝐸 and𝐸 is
1 1 2
thecomplementof𝐸 ∪Γ,with𝜋(2)(𝜽) =0for𝜽 ∈ 𝐸 .
1 2
The idea is that we can define a PDMP sampler that is appropriate for
each 𝜋(𝑖),butneedtonowdefinewhathappenswhenthe 𝜽 componentof
thestatetriestoleave𝐸 𝑖.Toexplainthis,whilstkeepingthenotationsimple,
wewillconsiderthecaseof𝐾 =2regions,anddescribeconditionsforthe
PDMP sampler on the boundary between 𝐸 and 𝐸 that are sufficient for
1 2
it to target 𝜋(𝜽) as defined above. Extending this to 𝐾 > 2 is trivial as
we just applythe same conditions toeach boundary between tworegions.
(We do not need to consider behaviour at points that lie on the boundary
betweenthreeormoreregionsasthesamplerwillnothitsuchpointswith
probability1.)
For 𝜽 ∈ Γ, let n(𝜽) be the normal to the boundary and assume that
thenormalisdefinedtopointoutofregion 𝐸 .AssumewehaveaPDMP
1
samplerwithvelocitysetVandwithstationarydistributionforthevelocity
componentthatisindependentof𝜽 andisdenotedby 𝜋 .Oncewehitthe
p5.5 Extensions 177
boundary,thevelocitywilldeterminewhetherthestateismovingoutof𝐸
1
andinto 𝐸 orvice-versa.Todistinguishthesetwopossibilities,definefor
2
each𝜽 ∈ Γ
V+ = (cid:8) p ∈ V :n(𝜽)⊤p > 0(cid:9), and V− = (cid:8) p ∈ V :n(𝜽)⊤p < 0(cid:9).
𝜽 𝜽
So,forexample,ifthestateis(𝜽,p)for𝜽 ∈ Γandp ∈ V+thenthesampler
𝜽
wasinregion𝐸 andismovinginto𝐸 .
1 2
Nowdefineafamilyofprobabilitydensity,orprobabilitymass,functions
forp ∈ V,as
(cid:26) |n(𝜽)⊤p|𝜋 (p)𝜋(2)(𝜽) ifp ∈ V+
ℓ (p) = p 𝜽
𝜽 |n(𝜽)⊤p|𝜋 (p)𝜋(1)(𝜽) otherwise
p
This is just proportional to the density 𝜋 (p) weighted by the size of the
p
velocityinthedirectionofthenormaln(𝜽) andweightedbythedensityat
𝜽 intheregionthatthesamplerismovingto.
Finally,defineafamilyoftransitionkernelsforthevelocitycomponent,
Q𝑏(p′ ∈ ·|p)foreach𝜽 ∈ Γ.Thefollowingtheorem,takenfromChevallier
𝜽
et al. (2021), gives appropriate dynamics for our PDMP sampler on the
boundary.
Theorem 5.4 Assume 𝜋 is symmetric, so 𝜋 (p) = 𝜋 (−p), and that for
p p p
each 𝜽 ∈ Γ the transition kernel Q𝑏 has ℓ as its invariant distribution.
𝜽 𝜽
ThenaPDMPsamplerwith:
(i) dynamics for 𝜽 ∈ 𝐸 𝑖, for 𝑖 = 1,2, that have invariant distribution
𝜋(𝑖)(𝜽)𝜋 (p);and
p
(ii) for𝜽 ∈ Γhasatransitionthatkeeps𝜽 unchangedbutthat:
(B1) flipsthevelocityp′ =p;
(B2) updatesthevelocityaccordingtoQ𝑏,i.e.p′′ ∼Q𝑏(·,p′);and
𝜽 𝜽
(B3) updatesthestateofthePDMPto (𝜽,p′′);
willhaveinvariantdistribution𝜋(𝜽)𝜋 (p).
p
Steps (B1) – (B3) of the theorem give appropriate dynamics for the
velocitywhenwehittheboundary,with(B2)statedintermsofatransition
kernelthathasℓ asitsinvariantdistribution.
𝜽
Therearevariouspossiblechoicesofdynamicsduetodifferentchoices
forthetransitionkernel.AtrivialchoiceforQ𝑏 istheidentitymap.Inthis
𝜽
case, the transition at the boundary is to reverse the sign of the velocity,
whichmeansthatthesamplerwillneverleavetheregionthatitstartswith.
Thusthischoiceisonlysuitableforthecasewherewestartthesamplerin
𝐸 andwhere 𝜋(2)(𝜽) = 0,i.e.thereisnoprobabilitymassin 𝐸 .Evenin
1 2178 Continuous-TimeMCMC
thiscase,thischoicemaynotbeagoodone,asitwillforcethesamplerto
retraceitsstepsonceithitstheboundary,whichwillslowdownmixing.
An alternative choice is to define Q𝑏 to be independent of the current
𝜽
velocity,andjusttoinvolvesamplingfromℓ .Theproblemwiththisisthat
𝜽
sampling from ℓ may be difficult. For both the Coordinate Sampler and
𝜽
the Zig–Zag Sampler, ℓ is a discrete distribution, and can be calculated
𝜽
exactly. For the Coordinate Sampler, p can take 2𝑑 possible values, and
this approach can be reasonable. However, for the Zig–Zag Sampler, p
cantake2𝑑 possiblevalues,andthuscalculatingandsimulatingfromℓ is
𝜽
prohibitiveunless𝑑issmall.Incaseswhereℓ isdifficulttosamplefromone
𝜽
can instead use Metropolis–Hastings to define a transition kernel that has
therequiredinvariantdistribution.Thisinvolvesproposinganewvelocity
fromanarbitraryproposaldistributionandthenacceptingorrejectingthat
proposal.Theproblemwiththisisthatiftheacceptanceprobabilityisnot
high then we are likely to reject the proposal, and our new velocity will
justbeminusthevelocitywithwhichwehittheboundary,fromstep(B1),
which will inhibit mixing. A partial solution is to define Q𝑏 to involve
𝜽
𝐿 > 1 Metropolis–Hastings steps, though this comes with an increased
computationalcostofsamplingfromQ𝑏.
𝜽
FortheBouncyParticleSampler,thereisasimpleandverynaturalchoice
ofdynamicsattheboundarywhichsatisfiestheconditionofTheorem5.4.
Ifp ∈ V+,sowearecurrentlymovingoutof𝐸 thenthedynamicsare:
𝜽 1
(R1) Withprobabilitymin{1,𝜋(2)(𝜽)/𝜋(1)(𝜽)}thevelocityisunchanged,i.e.
p′′ =p;
(R2) Otherwise,wereflectthevelocityinthetangenttotheboundaryat𝜽,i.e.
usingthenotationintroducedforreflectionsp′′ = R (p).
n(𝜽)
If p ∈ V−, then the dynamics are as above but with 𝜋(1) and 𝜋(2) inter-
𝜽
changedin(R1).Underthesedynamics,ifthesamplerismovingtoaregion
ofhigherprobabilitydensity,itcontinues.Ifnot,thenwithsomeprobability
itcontinues,otherwiseitreflectsback.Aspecialcasewherewehave𝜋(𝜽)
definedonlyonthecompactregion 𝐸 ,sothat 𝜋(2)(𝜽) = 0,inwhichcase
1
thesamplerwillalwaysreflectifithitstheboundary.
We cannot apply similar dynamics for the Zig–Zag Sampler unless the
boundary aligns appropriately with the velocity axes, as the reflected ve-
locity in step (R2), R (p), may no longer be a valid velocity. However,
n(𝜽)
Chevallieretal.(2021)showthattheabovedynamicsfortheBouncyParti-
cleSamplercanbeviewedasthebehaviouroftheBouncyParticleSampler
ifweapproximatethediscontinuoustargetbyacontinuousone,bysmooth-
ing out the discontinuity, but then consider the limit where we allow the5.5 Extensions 179
transitionbetween 𝜋(1) and 𝜋(2) tooccurmorequickly.Bythesamestrat-
egy, we can construct an appropriate transition kernel for Zig–Zag, see
Chevallieretal.(2021)formoredetails.
Example:LogisticRegressionwithConstraints
Asanexampleapplicationforthisalgorithm,considerthelogisticregres-
sion model but with constraints on the parameters. There are two natural
constraints,oneisthatwemayknowthattheeffectofacovariateissuchthat
largervalueswillincrease,say,theprobabilityofobservingaresponseof
1.Ifthe𝑖thcomponentofthex𝑗sissuchacovariate,then𝜃 𝑖 ≥ 0.Similarly,
ifanincreaseinthe𝑖thcomponentofthecovariatevectorisknowntolead
to a decrease in the probability, then 𝜃 𝑖 ≤ 0. Alternatively, we may have
someconstraintsonthesizeoftheeffect,e.g.𝜃 𝑖 ≥ 𝜃 𝑙.
For the Bouncy Particle Sampler, the simplest way of including such a
constraintistocalculatethetimewhenthecurrentdeterministicdynamics
will first violate the constraint. If an event does not happen by this time,
thenwejustreflectthevelocityofftheboundaryof𝜽 spaceimpliedbythe
constraint. For the constraint 𝜃 𝑖 ≥ 0 or 𝜃 𝑖 ≤ 0 this would involve flipping
the𝑖th component of the velocity. For the constraint 𝜃 𝑖 ≥ 𝜃 𝑙, this involves
thereflectionR
g
withgdefinedso𝑔
𝑖
= 1,𝑔
𝑙
= −1andallotherentriesset
tozero.
Inthisexample,wecanusethesameadaptiontotheZig–ZagSampler,
as for each constraint the reflection at the boundary will produce a new
velocity that is valid for the sampler. However, this would not be the case
if,forexample,wewantedtoenforceaconstraintsuchas𝜃 > 2𝜃 .
1 2
5.5.2 ReversibleJumpPDMPSamplers
Another class of distributions that the standard PDMP samplers are not
suitableforaredistributionsdefinedonasetofspacesofdifferentdimen-
sions. In the following, we think of this in terms of a distribution over a
discrete set of models, with a continuous density for each model. Whilst
PDMPscanbeusedtoexplorethedistributiondefinedforeachmodel,we
wouldneedawayofmovingbetweenthesemodels–whichwouldleadto
atypeofreversiblejumpversionofPDMPsamplers.
Oneapproachtodothisistoaddadditionaleventsthatintroducediscrete
jumpsfromonespacetoanother.Suchmoveshavebeenproposedforother
continuous-time samplers, see Grenander and Miller (1994), Phillips and
Smith(1996)andStephens(2000).However,theycanbehardtoimplement
duetochallengeswithsimulatingthesemoveswiththecorrectrate.180 Continuous-TimeMCMC
Acomputationallymoreefficientprocedureexistsifthedifferentmodels
aredefinedintermsofsomecommon𝑑-dimensionalparameter𝜽,buteach
model fixes one or more components of 𝜽 to a specific value. The most
common example of such a distribution is the posterior distribution for
the coefficients of a linear or generalised linear regression model, where
different models correspond to including different sets of covariates in
the model. Thus we can define 𝜽 to be the coefficients for the full set of
covariates, and a given model will fix the coefficients of covariates not in
themodeltozero.
TomotivatetheformofthePDMPsamplerswewillintroduce,consider
first the case where we have a single covariate, so 𝑑 = 1. Also, ignore
havinganyinterceptinthelinearorgeneralisedlinearmodel.Wenowhave
twomodels,dependingonwhetherweincludethecovariateornot.Ifℓ(𝜃)
denotes the likelihood as a function of 𝜃 and if we have a prior that is a
mixture of point mass at 0, which we will denote by 𝛿 (𝜃), and a prior
0
definedonR,𝜈(𝜃),thentheposteriordistributionis
𝜋(𝜃) ∝ 𝑤𝛿 (𝜃)ℓ(𝜃)+(1−𝑤)𝜈(𝜃)ℓ(𝜃),
0
where𝑤isthepriorprobabilityofexcludingthecovariate.Whilstwecannot
use a standard PDMP to sample from this target, we can approximate the
prior by replacing the point mass at zero with a distribution concentrated
aroundzero.IfweuseaGaussiandensitywithvariance𝜎2for𝜎 ≈0,then
wehavethetarget
𝜋 𝜎(𝜃) ∝ 𝑤N(𝜃;0,𝜎2)ℓ(𝜃)+(1−𝑤)𝜈(𝜃)ℓ(𝜃), (5.17)
whereN(𝜃;0,𝜎2) denotesthedensityofaGaussianwithmean0andvari-
ance𝜎2.WecannowuseaPDMPtosamplefromthisposteriordistribution.
Alsobyletting𝜎 →0wehavethatthisposteriorwilltend,insomesense,
totheposteriorwiththepointmassat0.Soanaturalapproachistoconsider
the dynamics of the PDMP targeting 𝜋 𝜎 and whether we get well-defined
dynamicsinthelimitof𝜎 →0.
Figure5.7showsthedynamicsforthreedifferentvaluesof𝜎.Weseethat
in each case the sampler will spend periods of time in the neighbourhood
of0.Aswereduce𝜎,theseneighbourhoodsconcentratecloserto0butthe
time the sampler spends in them seems to be similarly distributed. Away
fromtheneighbourhood,thedynamicsarethoseofsamplingfromadensity
proportionalto𝜈(𝜃)ℓ(𝜃).Thissuggeststhelimitingbehaviourwouldbethat
ofaPDMPtargeting𝜈(𝜃)ℓ(𝜃)butthatif𝜃 𝑡 hitszerothenthesamplerstays
atzeroforarandomamountoftime.
Chevallieretal.(2023)proposesuchaPDMPsampler,andshowthatif5.5 Extensions 181
     
     
     
              
t t t
Figure5.7 ExampleoutputfromPDMPsamplerforatargetthat
isanequalmixtureofaN(0,1)distributionanda𝑁(0,𝜎2)
distributionfor𝜎 =0.1(left),𝜎 =0.01(middle)and𝜎 =0.001
(right).As𝜎 →0thetrajectoriesqualitativelyconvergeto
periodsat,orcloseto0,andperiodswherethetrajectoryis
governedbytheN(0,1)distribution.Thislimitistheformforthe
reversiblejumpPDMPsampler.Thecomputationaladvantageof
usingthelimitingdynamicsisthatitavoidssimulatingthelarger
numberofeventscloseto0–whichwasoftheorderof10000for
theright-handplot.
wespecifythedynamicsasgivenbelowitwilltargetthecorrectdistribution
across models. To describe the sampler for a 𝑑-dimensional parameter 𝜽,
assumethat
2𝑑
∑︁
𝜋(𝜽) = 𝜋 𝑘(𝜽),
𝑘=1
whereweassumethateachmodel 𝑘 correspondstoadifferentsetofcom-
ponents of 𝜽 being set to zero. Here, 𝜋 𝑘(𝜽) is, up to proportionality, the
densityof𝜽associatedwiththe𝑘thmodel.Thesedensitiesmustbedefined
uptoacommonconstantofproportionalityacrossallmodels.
Formodel𝑘,letS
𝑘
betheactivesetofmodel𝑘,thatisthesetofindices
ofcomponentsof 𝜽 thatarenon-zeroformodel 𝑘.TheideaofthePDMP
isthatifwearecurrentlyexploringmodel𝑘 wewillsimulateaPDMPthat
targets𝜋 𝑘(𝜽),andthathasnon-zerovelocityonlyforcomponentsof𝜽 that
areinS 𝑘.Butinaddition,weallowtwofurtherevents.Thefirstisthatifa
componentof𝜽hitszero,andwedenotethiscomponentby𝑖,thenwemove
tothemodelwithactivesetS 𝑘\{𝑖}.Thesecondisthatforeach 𝑗 ∉ S 𝑘 we
have a rate of moving to the model with active set S 𝑘 ∪{𝑗}. If we move
tosuchamodelwedonotchange𝜽,butsimulateanewvelocityp,which
willhaveanon-zerovelocityforcomponent 𝑗.
To define such a sampler we just need to specify the rate of adding
θ t θ t θ t182 Continuous-TimeMCMC
a component to the model, and the distribution of the velocity after the
transition.WewilldescribethesefortheZig–ZagSamplerandtheBouncy
Particle Sampler. For general results, and generalisations of this sampler
whichdoesnotalwaysmovebetweenmodelswhenacomponentof𝜽 hits
zero,seeChevallieretal.(2023).
FortheZig–ZagSampler,assumingweareinmodel𝑘 withcurrentstate
(𝜽,p),wehavethefollowingprocessforaddingavariabletothemodel.
(Add–ZZ) For each 𝑗 ∉ S 𝑘, move to model 𝑘′ with active set S 𝑘 ∪ {𝑗} at rate
𝜋 𝑘′(𝜽)/𝜋 𝑘(𝜽). Set the new velocity, p′, such that 𝑝 𝑖′ = 𝑝 𝑖 for𝑖 ≠ 𝑗 and
𝑝′ isdrawnuniformlyatrandomfrom{−1,1}.
𝑗
For the Bouncy Particle Sampler, with standard Gaussian distribution for
the velocity, again assuming we are in model 𝑘, with current state (𝜽,p),
wehavethefollowing
(Add–BPS) Foreach 𝑗 ∉S 𝑘,movetomodel 𝑘′ withactivesetS 𝑘 ∪{𝑗}atrate
√2 𝜋 𝑘′(𝜽)
.
2𝜋 𝜋 𝑘(𝜽)
Set the new velocity, p′, such that 𝑝 𝑖′ = 𝑝 𝑖 for 𝑖 ≠ 𝑗 and 𝑝′ 𝑗 = 𝑥 is
simulatedfromadistributionwithdensityfunctionproportionalto
(cid:26) (cid:27)
1
|𝑥|exp − 𝑥2 ,
2
thisisastandardnormaldensityfunctionscaledby |𝑥|.
TheintuitionforthedensityofthenewvelocitycomponentfortheBouncy
ParticleSampleristhatitisskewed,relativetoitsinvariantdistribution,to
largerabsolutevaluesofthevelocityasthesecorrespondtovelocitiesthat
wouldhitzeromorequickly.
Importantly for both samplers, the rates at which we add components
will often be simple. If our target distribution is defined as a posterior
distribution, with common likelihood for each model, then the likelihood
componentsoftheposteriorswillcancelandtherateswilljustdependon
theratioofpriors.Formanypriors,thedistributionofeachcomponentof
𝜽 willbeindependent,inwhichcasetheseratesbecomeconstant.
FortheZig–ZagSampler,onecanimproveonthissamplerbyremember-
ing the velocity of each inactive component prior to it becoming inactive.
Then,whenthatcomponentisre-introducedtothemodelwere-usethesame
velocity.ThisistheStickyZig–ZagSamplerofBierkensetal.(2023b).It5.5 Extensions 183
can mix better as it ensures that the dynamics of each component of 𝜽
reflectslessoften.
Example:LogisticRegressionwithModelChoice
AnextensiontothelogisticregressionmodelofSection1.2.1istoinclude
a choice as to which covariates to include in the model. We will consider
twoexamplepriors,andcalculatetherateofaddingacovariatetothemodel
fortheZig–ZagSamplerineachcase.
Acommonpriorwouldbetoassumeindependenceacrosscovariates,so
𝜃 𝑖 = 0 with probability 𝑤 𝑖 and is drawn from a normal distribution with
mean0andvariance𝜎 𝑖2withprobability1−𝑤 𝑖.Inthiscase,becauseofthe
independence, if covariate 𝑖 is not in the current model, the rate at which
weadditwillnotdependonthevalueof𝜃
𝑙
for𝑙 ≠𝑖.Thus,thisratewillbe
constantandequalto
(1−𝑤 𝑖) 1
,
𝑤 √︃
𝑖 2𝜋𝜎2
𝑖
the ratio of the prior probability of a model which includes covariate 𝑖 to
thepriorprobabilityofthesamemodelwithoutcovariate𝑖,timestheprior
probabilitydensityof𝜃
𝑖
=0undertheformermodel.
What about when we have a prior under which components of 𝜽 are
dependent? Assume we are currently in model 𝑘 which does not include
the𝑖thcovariate,andthataddingthiscovariatewillproducemodel 𝑘′.Let
𝑞 𝑘 and𝑞 𝑘′ bethepriorprobabilityofthetwomodelsandassumethatthey
both have Gaussian priors on 𝜽 with mean 0 and covariance matrices on
theactivecomponentsof 𝜽 denotedby 𝚺 𝑘 and 𝚺 𝑘′ respectively.Let 𝑎 𝑘 be
the number of active components of model 𝑘, with 𝑎 𝑘′ = 𝑎 𝑘 +1. We can
introducematricesA𝑘 andA𝑘′ sothatthepriorfortheactivecomponents
of𝜽 underourpriorformodel 𝑘 is
(cid:18)
1
(cid:19)𝑎 𝑘/2 (cid:26)
1
(cid:27)
2𝜋
det(𝚺 𝑘)−1/2exp − 2𝜽⊤A𝑘𝜽 .
This is possible by padding A with zeroes, so if covariate 𝑙 is not in the
modelthen 𝐴 𝑙𝑗 = 𝐴 𝑗𝑙 =0,for 𝑗 =1,...,𝑑.
Withthisdefinitionoftheprior,therateofmovingfrommodel 𝑘 to 𝑘′
asafunctionof𝜽 becomes
𝑞 𝑞𝑘 𝑘′ (cid:18) 21 𝜋(cid:19)1/2 (cid:18) dd ee tt (( 𝚺𝚺 𝑘𝑘 ′) )(cid:19)1/2 exp(cid:26) − 21 𝜽⊤(A𝑘′ −A𝑘)𝜽(cid:27) .184 Continuous-TimeMCMC
Define𝐶tobetheconstantbeforetheexponentialterm.Ifourcurrentstate
is (𝜽,p) thentherateuntilwemovetomodel 𝑘′ is
(cid:26) (cid:27)
1
𝐶exp − (𝜽⊤+𝑡p)(A𝑘′ −A𝑘)(𝜽 +𝑡p) =
2
(cid:26) (cid:27) (cid:26) (cid:27)
1 1
𝐶exp − 𝜽⊤(A𝑘′ −A𝑘)𝜽 exp −𝜽⊤(A𝑘′ −A𝑘)p𝑡− p⊤(A𝑘′ −A𝑘)p𝑡2 .
2 2
Thisisoftheform 𝑎exp{𝑏𝑡 +𝑐𝑡2} forsomeconstants 𝑎, 𝑏 and 𝑐.Wecan
simulatethetimeofthenexteventwiththisrateif𝑐 ≤ 0astheintegralof
therateisanalyticfor𝑐 =0,andcanbeexpressedintermsofprobabilities
ofanormaldistributionfor𝑐 < 0.For𝑐 > 0,wecanusePoissonthinning
withe.g.boundsoftheform 𝐴exp{𝐵𝑡}oversuitableintervalsfor𝑡.
5.5.3 MoreGeneralVelocityModels
Another possible way of extending PDMP samplers is to consider more
general models for the dynamics. There are two simple ways of doing
this, the first is to alter the distribution of the velocities for the Bouncy
Particle Sampler or the Zig–Zag Sampler so that they are not spherically
symmetric.Theotheristoconsidernon-constantvelocitymodels.Wewill
brieflydescribeeachoftheseinturn.
First,wewillfocusontheZig–ZagSampler.Ifwelete𝑖 bethe𝑖thunit
vector, i.e. the vector whose𝑖th component is 1 and all other components
are0,thenthesetofvelocitiesoftheZig–Zigarethevelocitiesoftheform
(cid:205) 𝑖𝑑 =1𝑎 𝑖e𝑖, where 𝑎 𝑖 ∈ {−1,1} for𝑖 = 1,...,𝑑. That is, they are the set of
velocities that one obtains by adding plus or minus each unit vector. The
rateofflipping𝑎 𝑖 isequaltothemaximumof0andminusthedotproduct
of𝑎 𝑖e𝑖 with∇log𝜋(𝜽).
To generalise this we just need to replace the unit vectors with another
set of vectors that span R𝑑. Denote this set by p 1,...,p𝑑. Importantly for
Zig–Zag, the change to the process is trivial – as the event rates are of a
similar form but with e 1,...,e𝑑 replaced with p 1,...,p𝑑. There are two
naturalapproachestochoosingp 1,...,p𝑑.Oneistojustchangethespeed
in each direction, so p𝑖 = 𝑐 𝑖e𝑖 for some set of positive scalars 𝑐 1,...,𝑐 𝑑.
This can be helpful if different components of 𝜽 under 𝜋 are on different
scales.Anaturalchoiceistoset𝑐 𝑖tobeanestimateofthemarginalstandard
deviationof𝜽𝑖 under𝜋.Theotherapproachistoalsochangethedirections
ofthevelocitiesaswell.Ifwehaveanestimateofthevariance-matrixof𝜽
under 𝜋,say𝚺,thenonechoiceistochoosethep𝑖stobetheeigenvectors
of𝚺.5.5 Extensions 185
To see why this is a natural choice, consider 𝜋(𝜽) being a Gaussian
distributionwithvariance𝚺.Centrethisdistributionsoithasameanof0.
Ifweusep 1,...,p𝑑 asourbasisforthevelocities,andp=(cid:205) 𝑖𝑑 =1𝑎 𝑖p𝑖,then
therateatwhichweflip𝑎 𝑖 isequalto
max{0,−𝑎 𝑖p⊤
𝑖
∇log𝜋(𝜽 +p𝑡)} =max(cid:8) 0,−𝑎 𝑖p⊤
𝑖
(cid:0) −𝚺−1(𝜽 +p𝑡)(cid:1)(cid:9).
But using that p𝑖 is an eigenvector of 𝚺, and hence also of 𝚺−1, and if we
assumethecorrespondingeigenvalueof𝚺is𝛾 𝑖,wehavethat
(cid:32) 𝑑 (cid:33)
−𝑎 𝑖p⊤
𝑖
(cid:0) −𝚺−1(𝜽 +p𝑡)(cid:1) = 𝑎 𝑖𝛾1 p⊤
𝑖
𝜽 +𝑡∑︁ 𝑎 𝑗p𝑗 𝑡 = 𝑎 𝑖𝛾1 (cid:0) p⊤
𝑖
𝜽 +𝑎 𝑖𝑡(cid:1).
𝑖 𝑖
𝑗=1
Inthiscase,theeventratedoesnotdependonthevelocityinothercompo-
nentsand essentiallyZig–Zagwill reduceto independentprocessesalong
eachcomponent,p⊤𝜽,of𝜽.
𝑖
AsimilarideacanbeusedtogeneralisetheBouncyParticleSampler.It
issimplesttodescribethisforthecasewheretheinvariantdistributionfor
p is Gaussian, as the generalisation is to allow a non-identity covariance
matrixforthis invariantdistribution.Inthefollowing, wewillassumethe
invariantdistributionisGaussianwithmean0andvariance𝚺.
Aswechange𝚺,wehavetochangethereflectioneventsofthesampler.
Toseewhy,notethatakeypropertyofthereflectioneventofthestandard
BouncyParticleSampler,whereinstep(BPS2)
p′ = R (p), withg= ∇ log𝜋(𝜽),
g 𝜽
wasthat||p′||2 = ||p||2,sothistransitiondoesnotchangethedensityofthe
2 2
state under 𝜋 . This is no longer the case if the variance of p under 𝜋 is
p p
notamultipleoftheidentity.
Soweneedtogeneralisethereflectionsothatitdependson 𝚺.Itturns
outthattheappropriatereflectionis
2g⊤p
R𝚺(p) =p− 𝚺g.
g (g⊤𝚺g)
Importantly,ifp′ = R𝚺(p) foranyg,then
g
p′⊤𝚺−1p′ =p⊤𝚺−1p,
so it does not change the density under a Gaussian with variance 𝚺. Fur-
thermore,westillhavethatifg= ∇log𝜋(𝜽) then
p′·∇log𝜋(𝜽) =−p·∇log𝜋(𝜽),
whichistheotherkeyrequirementofthetransitionneededforthevalidity186 Continuous-TimeMCMC
ofthesampler.Usingthesetwopropertiesitisstraightforwardtoshowthat
theBouncyParticleSamplerwith(BPS2)replacedby(BPS2’)belowwill
have 𝜋(𝜽)𝜋 (p) asitsinvariantdistribution,where 𝜋 (p) isthedensityof
p p
aGaussiandistributionwithmean0andvariance𝚺.
(BPS2’) Transition at events. At an event with probability 1 − 𝜆 r/𝜆 BPS(𝜽,p),
reflectthevelocity
p′ = R𝚺(p), withg= ∇ log𝜋(𝜽);
g 𝜽
otherwise sample a new velocity, p′ from a normal distribution with
mean0andvariance𝚺.Thepositionisunchangedatanevent.
Asabove,anaturalchoiceof𝚺touseinthedistributionforthevelocityis
tochooseittobeanestimateofthevarianceof𝜽 under𝜋.Furthermore,for
boththeBouncyParticleSamplerandZig–Zagonecanrelatethechoiceof
distributiononthevelocitytorunningthecanonicalversionofthePDMP
but after applying a linear reparameterisation to the random variable of
thedistributionwewishtosamplefrom.Wewilldescribethelinkforthe
BouncyParticleSampler,thoughasimilarargumentappliestootherPDMP
samplers.
ConsidertheBouncyParticleSamplerfor (𝜽,p) withtargetdistribution
𝜋(𝜽)andastandardGaussiandistributionforp.Forsomeinvertiblematrix
L define 𝝍 = L𝜽, and consider the dynamics of the PDMP but viewed in
termsof𝝍.If𝜽 isdrawnfrom 𝜋(𝜽),and𝝍 = L𝜽,thenthedensityof𝝍 is
𝜋 (𝝍) ∝ 𝜋(L−1𝝍),astheJacobianofthetransformationisconstant.Ifwe
𝝍
considerderivativesthen
𝜕log𝜋 𝝍(𝝍)
=
𝜕log𝜋(L−1𝝍) =∑︁𝑑 𝜕log𝜋(L−1𝝍)
(cid:0) L−1(cid:1) .
𝜕𝜓 𝜕𝜓 𝜕𝜃 𝑗𝑖
𝑖 𝑖 𝑗
𝑗=1
Thisisjustthe𝑖thentryofL−⊤∇ log𝜋(L−1𝝍),whichgivesthat
𝜽
∇ 𝜋 (𝝍) =L−⊤∇ log𝜋(L−1𝝍). (5.18)
𝝍 𝝍 𝜽
NowletusconsiderthedynamicsoftheBouncyParticleSamplerin𝝍
space.Wewillconsidereachaspectofthedynamicsinturn:
Deterministic Dynamics: If we transform the constant velocity dynamics
into𝝍 spacewehave
d𝝍 d𝜽
=L =Lp,
d𝑡 d𝑡
so these are still constant velocity dynamics but with velocity w = Lp.5.5 Extensions 187
Furthermore, if p has a Gaussian distribution with an identity covariance
matrix,thenwisGaussianwithcovarianceLL⊤.
Rate of Bounce Events: If the current state is (𝜽,p) then the rate of a
bounceeventismax{0,p·∇log𝜋(𝜽)}.Now
p·∇log𝜋(𝜽) =p⊤∇log𝜋(𝜽) =w⊤(L−1)⊤∇log𝜋(L−1𝝍) =w⊤∇ 𝜋 (𝝍),
𝝍 𝝍
where we have transformed (𝜽,p) to (𝝍,w) and used (5.18). This is the
ratefortheBouncyParticleSamplertargeting𝜋 (𝝍).
𝝍
ReflectionatBounceEvents:Ifthecurrentstateis(𝜽,p)thenatabounce
eventthenewvelocityis
∇ log𝜋(𝜽)
p′ =p−2(p·∇ log𝜋(𝜽)) 𝜽 .
𝜽 (∇ log𝜋(𝜽)⊤∇ log𝜋(𝜽))1/2
𝜽 𝜽
Soifweconsiderthevelocityforthe𝝍process,w′ =Lp′,anduse𝚺 =LL⊤,
thisis
L∇ log𝜋(𝜽)
w′ =Lp−2(p·∇ log𝜋(𝜽)) 𝜽
𝜽 (∇ log𝜋(𝜽)⊤∇ log𝜋(𝜽))1/2
𝜽 𝜽
=w−2(w⊤L−𝑇∇ log𝜋(L−1𝝍))
𝜽
𝚺L−𝑇∇ log𝜋(L−1𝝍)
× 𝜽
(∇ log𝜋(L−1𝝍)⊤L−1𝚺L−𝑇∇ log𝜋(L−1𝝍))1/2
𝜽 𝜽
𝚺∇ log𝜋 (𝝍)
=w−2(w⊤∇ log𝜋 (𝝍)) 𝝍 𝝍 .
𝝍 𝝍 (∇ log𝜋 (𝝍)⊤𝚺∇ log𝜋 (𝝍))1/2
𝝍 𝝍 𝝍 𝝍
ThisisjustR𝚺(w)withg= ∇log (𝝍),thereflectionoftheBouncyParticle
g 𝝍
Samplerwithcovariancematrix𝚺.
RefreshEvents:Theseeventsoccurataconstantrate,whichisunaffected
bythetransformationto𝝍.Atarefreshevent,wesimulatepfromastandard
Gaussian,whichcorrespondstosimulatingw = LpfromaGaussianwith
covariance𝚺 =LL⊤.
Thustheprocessin𝝍spaceisaBouncyParticleSamplerwithcovariance
matrix𝚺 =LL⊤ forthevelocity.
Asecondgeneralisationistoaltertheconstantvelocitydynamics.This
hasbeen suggestedinparticular asa wayofgeneralising theBouncyPar-
ticleSamplerwithcovariancematrix 𝚺 forthevelocity,withtheresulting
algorithm called the Boomerang Sampler (Bierkens et al., 2020), though
similar ideas also appear under the name of Hamiltonian-BPS in Vanetti
etal.(2017).
Consideravelocitymodelwithmarginaldistributionsuchthatlog𝜋 (p) =
p
−(1/2)p⊤𝚺−1p. Write log𝜋(𝜽) = 𝑈(𝜽) − (1/2)(𝜽 −𝜽∗)⊤𝚺−1(𝜽 −𝜽∗) for188 Continuous-TimeMCMC
somefunction𝑈(𝜽) andconstantvector 𝜽∗.Theideaistohavedetermin-
isticdynamicsthatmovealongcontoursof−(1/2)(𝜽−𝜽∗)⊤𝚺−1(𝜽−𝜽∗)−
(1/2)p⊤𝚺−1p in (𝜽,p) space. Such dynamics are given by Hamiltonian
dynamics,whicharetractableinthiscase,andare
d𝜽 dp
=p, = 𝜽 −𝜽∗.
d𝑡 d𝑡
The solution of these dynamics are 𝜽𝑡 = 𝜽∗ + (𝜽
0
−𝜽∗)cos(𝑡) +p 0sin(𝑡)
and p𝑡 = p 0cos(𝑡) − (𝜽
0
− 𝜽∗)sin(𝑡). The rate of bounce events for the
Boomerang Sampler is just max{0,p𝑡 ·∇𝑈(𝜽𝑡)}, with bounces as per the
Bouncy Particle Sampler when the velocity has covariance matrix 𝚺. As
before,wecanalsointroducerefreshevents.
If 𝑈(𝜽) = 0, so we are targeting a Gaussian distribution for 𝜽 with
mean 𝜽∗ and covariance 𝚺, then this sampler just undergoes Hamiltonian
dynamics.Inthiscase,astrictlypositiverefreshrateisneededtoavoidthe
samplerbeingreducible,andtheresultingprocessisaformofrandomised
Hamiltoniandynamics,thatisHMCbutwitharandomrefreshtimeforthe
velocity.Fornon-Gaussiantargets,thissamplerwillhaveadditionalbounce
events,butthehopeisthatifthetargetisclosetoGaussianwithmean 𝜽∗
and covariance 𝚺, then the rate of bounce events will be much lower than
forthestandardBouncyParticleSampler.
Care is needed with one aspect of simulating the Boomerang Sampler,
as the different dynamics require slightly different approaches to simulate
the event times. If the current state is (𝜽 ,p ) then the rate until the next
0 0
eventisnow
𝜆˜ (𝜽0,p0)(𝑡) =max{0,p𝑡 ·∇𝑈(𝜽𝑡)} =max{0,
(p cos(𝑡)+(𝜽 −𝜽∗)sin(𝑡))·∇𝑈(𝜽∗+(𝜽 −𝜽∗)cos(𝑡)+p sin(𝑡))},
0 0 0 0
where we have substituted in the definitions of p𝑡 and 𝜽𝑡. Bierkens et al.
(2020)givesomegeneralapproachestoboundingthisrate,whichusesthe
property that the deterministic dynamics of the Boomerang Sampler are
such that |𝜽𝑡 −𝜽∗|2 + |p𝑡|2 is a constant. To keep the notation simple, we
willshowthisfor𝜽∗ =0,butthesameargumentappliesmoregenerally.In
thiscase
|𝜽𝑡|2+|p𝑡|2 = (𝜽 0cos(𝑡)+p 0sin(𝑡))·(𝜽 0cos(𝑡)+p 0sin(𝑡))
+(p cos(𝑡)−𝜽 sin(𝑡))·(p cos(𝑡)−𝜽 sin(𝑡))
0 0 0 0
= |𝜽 |2cos2(𝑡)+|p |2sin2(𝑡)+2𝜽 ·p sin(𝑡)cos(𝑡)
0 0 0 0
+|p |cos2(𝑡)+|𝜽 |2sin(𝑡)−2𝜽 ·p sin(𝑡)cos(𝑡)
0 0 0 0
= |𝜽 |2(sin2(𝑡)+cos2(𝑡)+|p |2(sin2(𝑡)+cos2(𝑡)) = |𝜽 |2+|p |2.
0 0 0 05.5 Extensions 189
Howisthisuseful?Thispropertymeansthatwecanboundthedistance
from 𝜽∗ that the current deterministic trajectory can take. Thus if we can
bound the Hessian of𝑈, which is the derivative of ∇𝑈, then this enables
us to bound ∇𝑈 for the current trajectory based on the value of ∇𝑈 at 𝜽∗
plus a term that depends on the bound on the Hessian and the distance
the trajectory can be from 𝜽∗. One such bound, that we will use below
is that if the spectral norm of the Hessian of 𝑈 is bounded by 𝑀, so
that ∥∇2𝑈(𝜽)x∥ ≤ 𝑀∥x∥ for any vector x with ∥ · ∥ denoting Euclidean
2
distance, then for the current deterministic trajectory, we have a constant
bound:
1
𝜆(𝜽𝑡,p𝑡) ≤ |∇𝑈(𝜽∗)|(|𝜽 0−𝜽∗|2+|p 0|2)1/2+ 2𝑀(|𝜽 0−𝜽∗|2+|p 0|2). (5.19)
Example:BoomerangforLogisticRegression
Asanexample,consideragainthelogisticregressionmodel.Therearetwo
natural choices for the centring value and covariance of the Boomerang
Sampler.Thefirstistosetthemtothepriormeanandcovariance, 𝜽∗ = 0
and 𝚺 = 𝚺 𝜽. The second is to estimate the mode of log𝜋,(cid:98)𝜽𝑀𝐴𝑃 say, and
theinverseoftheHessianof−log𝜋 at(cid:98)𝜽𝑀𝐴𝑃.Wewillcomparethesetwo
options.
Ifwesetthemtothepriorvaluesthen𝑈(𝜽) isminusthelog-likelihood.
As described in Section 5.4.1 we can bound the Hessian of minus the
log-likelihoodby (1/4)X⊤X,whereXisthe 𝑁 ×𝑑 matrixofcovariates.
What about if we set 𝜽∗ and 𝚺 based on the estimate of the mode of
log𝜋 and the inverse of the Hessian of −log𝜋 at the mode? Denoting the
log-likelihoodofthelogisticmodelbyℓ(𝜽;D),andtheHessianofminus
thelog-likelihoodbyH(𝜽).Thischoicegives
1
𝑈(𝜽) =−ℓ(𝜽;D)− (𝜽 −(cid:98)𝜽𝑀𝐴𝑃)⊤H((cid:98)𝜽𝑀𝐴𝑃)(𝜽 −(cid:98)𝜽𝑀𝐴𝑃),
2
wherewehaveusedthefactthatthecontributionfromthepriorwillcancel.
Taking second derivatives, the Hessian of this at 𝜽 will be the difference
betweentwomatrices,H(𝜽)−H((cid:98)𝜽𝑀𝐴𝑃).Thesematricesarebothpositive
semi-definite,withspectralnormboundedby (1/4)X⊤X,thusthespectral
norm of the difference is also bounced by (1/4)X⊤X. This is because the
eigenvalues of H(𝜽) are bounded between 0 and 𝑀 for some constant 𝑀,
andtheeigenvaluesof−H((cid:98)𝜽𝑀𝐴𝑃) areboundedbetween−𝑀 and0,sothe
eigenvaluesofH(𝜽)−H((cid:98)𝜽𝑀𝐴𝑃) arebetween−𝑀 and 𝑀.
ThuswecanimplementtheBoomerangSamplerforbothchoicesof𝜽∗
and 𝚺 with the constant bound given by (5.19) using the same value for190 Continuous-TimeMCMC
       
       
       
       
                               
θ(1) θ(1)
Figure5.8 TraceplotsoftheBoomerangSamplerforthe
posteriorofalogisticregressionmodel.Theheatplotshowsthe
contoursoftheposterior.Examplerealisationwhen𝚺 =𝚺 and
𝜽
𝜽∗isthepriormean(left)andwhen𝜽∗isanestimateofthe
posteriormodeand𝚺isbasedontheHessianoflog𝜋at𝜽∗
(right).
𝑀. The bounds will differ though due to the different values for 𝜽∗ and
𝑈 and hence for |∇𝑈(𝜽∗)|. In particular, this will be 0 for 𝜽∗ = (cid:98)𝜽𝑀𝐴𝑃, or
at least close to 0 if we have a reasonable approximation for the mode of
log𝜋. Example realisations for the two samplers are shown in Figure 5.8,
fordatawith 𝑁 = 100and 𝑑 = 2andwithapriorcovarianceof2foreach
componentof𝜽.
ThemaindifferencebetweentheBoomerangSamplerandtheprevious
PDMPsamplersismostobviouslyseenintheright-handplotofFigure5.8,
aswehaveellipticaltrajectoriesbetweenevents.Thisisreminiscentofthe
trajectoriesforHMC.Fortheleft-handplot,whereΣ islargecomparedto
thecurvatureoftheposterior,thecontoursareellipticalbutwithlargerradii
andthustheoutputlooksmoresimilartoourpreviousPDMPsamplers.In
thisexample,oneofthemainadvantagesofbasing 𝜽∗ andΣ onthemode
and Hessian at the mode is that the computational cost of simulating the
Boomerang Sampler is lower. Both have been simulated with roughly the
samelengthoftrajectory,butusingthepriorvalueshasrequiredproposing
fourtimesasmanyevents.Thisisbecauseofthelooserboundontheevent
ratethatwehaveinthiscase.
)2(θ )2(θ5.6 ChapterNotes 191
5.6 ChapterNotes
The initial idea of using PDMP processes for sampling comes from the
physical sciences, see for example Turitsyn et al. (2011), Peters and de
With(2012)andMicheletal.(2014).Thesewereintroducedintostatistics
byBouchard-Coˆte´etal.(2018)andBierkensandRoberts(2017),andoneof
theearlypaperstodescribethelinktoPDMPswasFearnheadetal.(2018).
Thelatterpaperalsogivesanexamplewhereavoidingrefresheventsinthe
Bouncy Particle Sampler can lead to slow mixing. How the subsampling
ideasofSection5.4.3extendtosamplersotherthantheZig–ZagSampler
isalsodiscussedinFearnheadetal.(2018),withrelatedideasforthelocal
BouncyParticleSamplerinBouchard-Coˆte´ etal.(2018).
As well as the PDMP samplers mentioned in the chapter, there have
beenvariouspaperssuggestingextensionstoPDMPmethods.Forexample
Vanettietal.(2017),WuandRobert(2017)andMicheletal.(2020).The
continuous-timemethodscanberelatedtodiscrete-timeMCMCmethods
such as reflective slice sampling (Neal, 2003) and the Discrete Bouncy
ParticleSampler(SherlockandThiery,2022).
The theoretical analysis of PDMP samplers has been active, including
showingergodicity(Deligiannidisetal.,2019;Bierkensetal.,2019a)and
exploring limiting behaviour of the Bouncy Particle Sampler and the Zig
Zagsampler(Deligiannidisetal.,2021;Bierkensetal.,2022;Andrieuetal.,
2021).Particularlystrongresultsareavailablefortheone-dimensionalcase
(BierkensandDuncan,2017;BierkensandVerduynLunel,2022).
ComplementaryresultsonscalingoftheDiscreteBouncyParticleSam-
pler to those shown in Section 5.3.3 are given in Sherlock and Thiery
(2022), which shows similar scaling to the Bouncy Particle Sampler and
alsoprovidessupportingtheorytohelpchoosetherefreshrate.6
Assessing and Improving MCMC
Thedevelopmentofmoresophisticatedand,especially,approximatesam-
plingalgorithms,aimedatimprovingscalabilityinoneormoreofthesenses
already discussed in this book, raises important considerations about how
a suitable algorithm should be selected for a given task, how its tuning
parameters should be determined, and how its convergence should be as-
sessed.Thischapterpresentsrecentsolutionstotheaboveproblems,whose
starting point is to derive explicit upper bounds on an appropriate dis-
tance between the posterior and the approximation produced by MCMC.
Further, we explain how these same tools can be adapted to provide pow-
erfulpost-processingmethodsthatcanbeusedretrospectivelytoimprove
approximationsproducedusingscalableMCMC.
6.1 DiagnosticsforMCMC
TheapproximationsprovidedbyMCMCareonlyusefulifwecanbecon-
fident that the samples collectively form a reasonable approximation to
the intended target. This, however, appears to be a circular requirement –
how can we verify that MCMC has worked without access to the limiting
targettocheck?Severaldiagnosticshaveemergedaspragmaticsolutions,
enablingapractitionertodetectcertainfailuremodesofMCMC.Inparticu-
lar,wehighlightconvergencediagnostics,whichaimtodeterminewhether
the Markov chain has converged to some stationary distribution, and bias
diagnostics,whichaimtodetectwhetherthestationarydistributionofthe
Markovchainisindeedthetargetdistributionofinterest.Forcontext,both
classes of diagnostic will be briefly discussed. Throughout this Chapter,
we shall restrict attention to distributions defined on R𝑑 for simplicity of
presentation.
1926.1 DiagnosticsforMCMC 193
6.1.1 ConvergenceDiagnostics
To limit the scope, here we describe the convergence diagnostics that are
most widely used. The Gelman–Rubin diagnostic is based on realisations
of 𝐿 independentMarkovchains,eachoflength𝑛,wherepracticalconsid-
erations typically restrict 𝐿 to be a small number, such as 3, 4 or 5. For
aunivariatetargetdistribution,theGelman–Rubindiagnosticisdefinedas
thesquarerootoftheratiooftwoestimatorsofthevariance𝜎2ofthetarget
distribution
√︂
𝜎2
𝑅 (cid:98):= (cid:98) , (6.1)
𝑠2
(cid:98)
where 𝑠2 is the (arithmetic) mean of the sample variances 𝑠2 along the 𝐿
(cid:98) 𝑙
samplepaths,
𝐿
1 ∑︁
𝑠2 := 𝑠2,
(cid:98) 𝐿 𝑙
𝑙=1
which typically provides an underestimate of 𝜎2, since it is possible that
oneormoreofthe 𝐿 chainshasnotexploredtheposteriorwell,while 𝜎2
(cid:98)
isconstructedas
𝑛−1 1 ∑︁𝐿 (cid:32) 1 ∑︁𝐿 (cid:33)2
(cid:98)𝜎2 := 𝑛 (cid:98)𝑠2+ 𝐿−1 𝑚 𝑙 − 𝐿 𝑚 𝑙′ , (6.2)
𝑙=1 𝑙′=1
where 𝑚 𝑙 is the sample mean from the 𝑙th sample path, which typically
provides an overestimate of the target variance. Indeed, the second term
in (6.2) is an estimate of the asymptotic variance of the sample mean
of the Markov chain, which is typically larger than the variance 𝜎2 we
𝑛
would obtain if our samples were truly independent; recall the discussion
ofeffectivesamplessizesand(1.15)fromChapter1.ForanergodicMarkov
chain, 𝑅 (cid:98)converges to 1 as 𝑛 → ∞. In practice, it is common to discard
a burn-in period of length 𝑛, where 𝑛 is the smallest integer for which
2
𝑅 (cid:98)< 1+𝛿,where𝛿isasuitablethreshold.Thesomewhatarbitrarychoices
of𝛿 =0.1and𝛿 =0.01areoftenused.
Convergence diagnostics are widely and successfully used. However,
it remains the case that the performance of 𝑅 (cid:98) and related convergence
diagnosticsdependsstronglyonhowtheindependentrealisationsofMCMC
areinitialised.Indeed,considerthetaskofgeneratingapproximatesamples
fromthemixturedistribution
1 1
𝜋(𝑥) = N(𝑥;−2,0.52)+ N(𝑥;2,0.52). (6.3)
2 2194 AssessingandImprovingMCMC
To avoid the situation where all chains are confined to the same local
high-probabilityregionduetochance,standardpracticeistoinitialisethe
Markov chains by sampling their initial state from a distribution that is
over-dispersed with respect to the target. Thus, we may initialise Markov
chains by sampling from, say, N(0,52). Running 𝐿 = 3 chains of length
𝑛 =1000leadstothetwosetsofsamplepathsshowninFigure6.1.Inboth
sets of sample paths, the length 𝑛 of the sample paths was insufficient to
enabletheMarkovchainstoexplorebothcomponentsof𝜋,andeachofthe
chains remained in the component in which it was initialised. On the left
side of the figure, one of the sample paths is clearly qualitatively distinct
fromtheothertwo,sincetheMarkovchainsexploreddifferentcomponents
of 𝜋, and the Gelman–Rubin diagnostic correctly detects that the Markov
chainshavenotconverged.Unfortunately,ontherightsideofthefigure,it
so happenedthat eachof the chainswas initialisedin the highprobability
region of the same component. As a result, the Gelman–Rubin diagnostic
appears to be converging to values below the commonly used thresholds
𝛿 = 0.1 and 𝛿 = 0.01, and fails to detect that the Markov chains have
exploredonlyoneofthecomponentsof𝜋.
Whatwentwrongwiththeconvergencediagnostic(6.1)inthisexample?
Well, the Markov processes exhibited a form of quasi-stationarity; transi-
tions from one component of the mixture to the other is a rare event, and
conditionalonsuchatransitionnotoccurringthebehaviouroftheMarkov
chainsisarguablyexcellent.Therarityoftransitionsbetweencomponents
makes it fundamentally difficult to distinguish between quasi-stationarity
and convergence of a Markov chain when the sample paths are confined
tothesamecomponent;someknowledgeoftheinvariantdistribution 𝜋 is
required. This motivates the discussion of an alternative diagnostic which
does indeed leverage information about 𝜋, a bias diagnostic, which we
describenext.
6.1.2 BiasDiagnostics
Eveninfavourablesituations,suchasthecaseofauni-modaltarget,con-
vergencediagnosticsdonotprovideaguaranteethatMarkovchainsamples
constituteafaithfulapproximationofthetarget.Indeed,convergencediag-
nosticsarenotcapableofdetectingbiasinsampleroutput,forexampleas
introduced when using stochastic gradient methods (Chapter 3), or intro-
duced when a coding error has occurred. Instead, bias diagnostics can be6.1 DiagnosticsforMCMC 195
   
   
   
                     
 L W H U D W L R Q  L W H U D W L R Q
   
     
   
     
         
         
   
     
   
     
                     
 L W H U D W L R Q  L W H U D W L R Q
Figure6.1 ConvergencediagnosticsforMCMC.Three
independentMarkovchainsweresimulatedtogeneratesamples
fromtheGaussianmixturetarget𝜋in(6.3).Inthefirstscenario
(leftpanels)thechainsexploredifferentcomponentsof𝜋,andthe
Gelman–Rubindiagnostic𝑅 (cid:98)correctlydetectsthattheMarkov
chainshavenotconverged.Inthesecondscenario(rightpanels)
thechainsexplorethesamecomponentof𝜋,andthe
Gelman–RubindiagnosticdoesnotdetectthattheMarkovchains
havenotconverged.[StarsindicatetheinitialstateofeachMarkov
chain.Thedensity𝜋isshaded.Dottedlinesindicatethetwo
commonlyusedthresholds𝛿 =0.1and0.01for𝑅 (cid:98)−1.]
usedtoidentifysuchsituations,asimpleexampleofwhichis
(cid:13) 𝑛 (cid:13)
(cid:13)1 ∑︁ (cid:13)
𝐵 (cid:98):=(cid:13)
(cid:13)𝑛
(∇log𝜋)(𝑋 𝑘)(cid:13) (cid:13). (6.4)
(cid:13) 𝑘=1 (cid:13)
1
R
x
−
c
1
R
x
−
c196 AssessingandImprovingMCMC
Providedthat∇log𝜋 ∈ L1(𝜋),whichwerecallmeansthat∫ ∥∇log𝜋∥ d𝜋 <
∞ from Section 1.1.1, from the strong law of large numbers for Markov
chainstheseriesin(6.4)almostsurelyconvergestothelimit
∫ ∫ (∇𝜋)(𝑥) ∫
∇log𝜋 d𝜋 = 𝜋(𝑥) d𝑥 = (∇𝜋)(𝑥) d𝑥 =0
𝜋(𝑥)
whenever the Markov chain is ergodic and 𝜋-invariant. The final equality
is integration by parts; a special case of Lemma 6.3 in the sequel. On
the other hand, just as the passing of a convergence diagnostic test does
not guarantee that the MCMC has converged, the convergence of (6.4) to
0 does not guarantee that the Markov chain preserves the correct target
distribution. Surprisingly, bias diagnostics are not widely used, at least
comparedtoconvergencediagnostics,whichmaybedueto(inourexample)
therequirementtocomputeagradient,ormaysimplybebecausetheyhave
beenhistoricallyoverlooked.
Consideragainthemixturedistribution𝜋 in(6.3)andsupposethat,due
to a coding error, we have implemented a Markov chain whose stationary
distributionisN(𝜇,0.52).Running 𝐿 = 3chainsoflength𝑛 = 1000leads
to the two sets of sample paths shown in Figure 6.2 for 𝜇 = 2 (left) and
𝜇 =0(right).Inbothsetsofsamplepaths,theGelman–Rubinconvergence
diagnostictestispassed,despitetheMarkovchainsfailingtobe𝜋-invariant.
On the left side of the figure, the bias diagnostic (6.4) clearly does not
convergetozero,andthusthebiasintheMarkovchainoutputisdetected.
Unfortunately,ontherightsideofthefigure,thebiasdiagnosticappearsto
be decreasing for all chains as the number 𝑛 of samples is increased, and
wedonotdiagnosethefailureofMCMC.
What went wrong with the bias diagnostic (6.4) in this example? Well,
informationaboutafinite-dimensionalgeneralisedmoment∫ ∇log𝜋 d𝜈 ∈
R𝑑 isinsufficienttocharacteriseaprobabilitydistribution;thereareanin-
finitudeofdistributions𝜈forwhichall𝑑componentsofthisgeneralisedmo-
mentare0.Thissuggestsapotentialsolution;findaninfinite-dimensional
generalisedmomentthatfullydetermineswhetherornot𝜋and𝜈areequal.
Surprisingly,thiscanbeachievedwithoutneedingtoexplicitlydealwithan
infinite-dimensionalgeneralisedmoment,duetothekerneltrickfromma-
chinelearning,whichwasintroducedinSection1.5.3forfinite-dimensional
inner-productspaces,andwillnowbeexploredintheinfinite-dimensional
setting.6.1 DiagnosticsforMCMC 197
   
   
   
                     
   
     
   
     
   
     
   
     
               
                       
 L W H U D W L R Q  L W H U D W L R Q
Figure6.2 BiasdiagnosticsforMCMC.Threeindependent
biasedMarkovchainsweresimulated,sothattheinvariant
distributiondiffersfromtheGaussianmixturetarget𝜋in(6.3).In
thefirstscenario(leftpanels)thechainsexploreaGaussian
centredat𝑥 =2,andthebiasdiagnostic𝐵 (cid:98)correctlydetectsthat
theMarkovchainshavenotconverged.Inthesecondscenario
(rightpanels)thechainsexploreaGaussiantargetcentredat
𝑥 =0,andthebiasdiagnosticdoesnotdetectthattheMarkov
chainshavenotconverged.[Starsindicatetheinitialstateofeach
Markovchain.Thedensity𝜋isshaded.]
6.1.3 ImprovedBiasDiagnosticsviatheKernelTrick
Though finite-dimensional bias diagnostics can be misled, the same may
notbetrueofabiasdiagnosticthatisinfinite-dimensional.Theaimofthis
sectionistoindicate,atahighlevel,howsuchaninfinite-dimensionalbias
B
x
c B
x
c198 AssessingandImprovingMCMC
diagnosticcanbeconstructed.Amorerigorousmathematicaltreatmentis
thenprovidedinSection6.2.
Supposethatweareabletowritedownacountableset {𝜙 ,𝜙 ,...} of
1 2
functions 𝜙 𝑗 : R𝑑 → Rsuchthateachofthemoments∫ 𝜙 𝑗(x)𝜋(x) dxof
𝜋canbeanalyticallyevaluated;withoutlossofgeneralitywemaysuppose
thateachgeneralisedmomentof𝜋isequalto0(sinceifnot,wemaysimply
redefine 𝜙 𝑗 as 𝜙 𝑗 −∫ 𝜙 𝑗(x)𝜋(x) dx). We have already seen examples of
functions𝜙 𝑗 thatcanbeused;wecouldtake
𝜕log𝜋(x)
𝜙 𝑗(x) =
𝜕𝑥
𝑗
for 𝑗 =1,...,𝑑.Moregenerally,wecanusethegeneratorofany𝜋-invariant
Markov process to construct such functions; the details are deferred to
Section6.2.Assumingthatthe𝜙 𝑗arelinearlyindependentandappropriately
normalised,wecanconstructaHilbertspace
(cid:40) (cid:41)
∞ ∞
∑︁ ∑︁
H = ℎ = 𝑐 𝑗𝜙 𝑗 : 𝑐2 𝑗 < ∞
𝑗=1 𝑗=1
whoseelementsarefunctionsℎ :R𝑑 →R,equippedwithaninnerproduct
⟨ℎ,ℎ′⟩ H = (cid:205)∞ 𝑗=1𝑐 𝑗𝑐′ 𝑗, where here ℎ = (cid:205)∞ 𝑗=1𝑐 𝑗𝜙 𝑗 and ℎ′ = (cid:205)∞ 𝑗=1𝑐′ 𝑗𝜙 𝑗. The
inducednormis ∥ℎ∥ := ⟨ℎ,ℎ⟩1/2.BypickingelementsfromthisHilbert
H H
space we can construct an infinitude of bias diagnostics, and the question
isthenwhichdiagnostictopick?
Onesolutionistoadoptanadversarialperspective,whereweselectan
element ℎ ∈ H thatmaximallydiscriminatesbetween 𝜋 andtheempirical
approximation to 𝜋 produced from MCMC. The bias diagnostic obtained
inthiswaycanbewrittenas
(cid:12) 𝑛 (cid:12)
(cid:12)1 ∑︁ (cid:12)
𝐵 (cid:101):= sup (cid:12)
(cid:12)𝑛
ℎ(X𝑘)(cid:12) (cid:12),
∥ℎ∥H≤1(cid:12) 𝑘=1 (cid:12)
where the supremum is taken over the unit ball of H, to ensure that the
supremumiscomputedoveraboundedset.Furtherre-writingintermsof
thebasisfunctions,wehave
(cid:40)(cid:12) (cid:12)1 ∑︁𝑛 ∑︁∞ (cid:12) (cid:12) ∑︁∞ (cid:41)
𝐵 (cid:101):=sup (cid:12)
(cid:12)𝑛
𝑐 𝑗𝜙 𝑗(X𝑘)(cid:12)
(cid:12)
: 𝑐2
𝑗
≤ 1 , (6.5)
(cid:12) 𝑘=1 𝑗=1 (cid:12) 𝑗=1
whenceweseethemaximisationisequivalenttofindingthepointconthe
surfaceofthe(infinite-dimensional)unithyperspherethatmaximisesthedot6.1 DiagnosticsforMCMC 199
productwiththe(infinite-dimensional)vectorc′ with𝑐′
𝑗
= 𝑛1 (cid:205)𝑛 𝑘=1𝜙 𝑗(X𝑘).
The solution of this maximisation problem is to align c to c′, and upon
properlynormalisingweobtain
𝑐 =
𝑛1 (cid:205)𝑛 𝑘=1𝜙 𝑗(X𝑘)
.
𝑗 √︃
(cid:205)∞
𝑗′=1
(cid:0) 𝑛1 (cid:205)𝑛 𝑘′=1𝜙 𝑗′(X𝑘′)(cid:1)2
Inserting this expression back into (6.5) and rearranging, we obtain the
explicitbiasdiagnostic
(cid:118)(cid:117)
(cid:116) 𝑛 𝑛 (cid:32) ∞ (cid:33)
1 ∑︁∑︁ ∑︁
𝐵 (cid:101)=
𝑛2
𝜙 𝑗(X𝑘)𝜙 𝑗(X𝑘′) .
𝑘=1 𝑘′=1 𝑗=1
Atthispointonecanraiseareasonableobjectionthatevaluating𝐵 (cid:101)appears
torequireaninfinitecomputationalbudget,duetothesummationoverthe
functions 𝜙 𝑗. Remarkably, there are situations where such infinite series
admitclosed-formanalyticexpressions
∞
∑︁
k𝜋(x,x′) := 𝜙 𝑗(x)𝜙 𝑗(x′),
𝑗=1
asituationknowninmachinelearningasthekerneltrick.SeeSection1.5for
aprimeronthekerneltrick.Providedthatwehaveaccesstoakerneltrick,
which of course depends on the precise choice we make for the functions
𝜙
𝑗
toensurethat∫ 𝜙 𝑗(x)𝜋(x) dx=0,wecanhopetoobtainaclosed-form
expressionforthebiasdiagnostic(6.5),namely
(cid:118)(cid:116)
𝑛 𝑛
1 ∑︁∑︁
𝐵 (cid:101)=
𝑛2
k𝜋(X𝑘,X𝑘′). (6.6)
𝑘=1 𝑘′=1
One would hope that (6.6) converges to 0 as 𝑛 → ∞ if and only if the
Markovchainis 𝜋-invariant.Itturnsoutthatsuchanideacanbemadeto
work,aswewillexplaininSection6.2.
Tosummarise,wehaveseenthatbothconvergencediagnosticsandcon-
ventionalfinite-dimensionalbiasdiagnosticscanprovideausefulpractical
tool to detect the failure of MCMC, but even taken together they are in-
sufficient to guarantee that output from the sampler provides an accurate
approximation of the intended target distribution. In the next section, we
turnourattentiontotheconstructionofinfinite-dimensionalbiasdiagnos-
tics, of the form (6.6), which attempt to solve the problem of assessing200 AssessingandImprovingMCMC
MCMCoutputbyestablishingexplicitupperboundsonanappropriatedis-
tancebetweentheposteriorandtheapproximationproducedbyMCMCin
termsofdiagnosticsoftheform(6.6).
6.2 ConvergenceBoundsforMCMC
Incontrasttoconvergencediagnosticsandconventionalfinite-dimensional
biasdiagnostics,whichmayfailtodetectinstanceswhereMCMChasfailed,
theaimofthissectionistoseekexplicitandcomputableupperboundson
theerrorbetweentheMCMCoutputandthetargetdistribution.Thistopic
hasreceivedconsiderablerecentinterestfollowingthepioneeringworkof
GorhamandMackey(2015).Tosetthescene,wefirstexplainhowtheuseof
asuitablediffusionprocessenablesexplicitboundsonintegralprobability
metrics,focusingonthe Wasserstein-1metricforclarityinSections6.2.1
and6.2.2.However,theWasserstein-1metricisnotfavourableforcompu-
tationinthiscontext,andweinsteadconsiderintegralprobabilitymetrics
basedonreproducingkernelHilbertspacesinSection6.2.3,notingthatthe
associated kernel Stein discrepancies can also provide valid convergence
bounds in Section 6.2.4. Lastly, in Section 6.2.5 we connect kernel Stein
discrepanciestothestochasticgradientmethodsfromChapter3.
6.2.1 BoundsonIntegralProbabilityMetrics
Our aim here is to arrive at an explicit and computable upper bound on
an appropriate metric between the target distribution 𝜋 and the empirical
distributionproducedbyMCMC.Let P(R𝑑) denotethesetofprobability
distributions on R𝑑 and consider a metric d : P(R𝑑) ×P(R𝑑) → [0,∞].
Asausefulconvention,wehaveextendedtherangeofametrictoinclude
the value ∞, to avoid the need to specify the subset of distributions on
which the metric is defined. Let 𝜋 ∈ P(R𝑑) be the distributional target
of MCMC. For our theoretical development, we will now introduce an
auxiliarydiscretetimeergodicMarkovchain,whichneednotberelatedto
theMarkovprocess(es)underpinningtheMCMCmethod(s)beingassessed.
The role of this auxiliary chain is limited to being a theoretical device in
whatfollows,andwedenoteitstransitionkernelas𝑇 𝜋,meaningthat𝑇 𝜋𝜈is
thedistributionafteronestepoftheauxiliaryMarkovchaininitialisedfrom
X ∼ 𝜈.Thisauxiliarychainisrequiredtosatisfythecontractionproperty
0
d(𝑇 𝜋𝜋,𝑇 𝜋𝜈) ≤ 𝜌 d(𝜋,𝜈) (6.7)6.2 ConvergenceBoundsforMCMC 201
for some 𝜌 ∈ [0,1) and all 𝜈 ∈ P(R𝑑). From the triangle inequality,
d(𝜋,𝜈) ≤ d(𝜋,𝑇 𝜋𝜈)+d(𝑇 𝜋𝜈,𝜈),combinedwiththecontractiond(𝜋,𝑇 𝜋𝜈) =
d(𝑇 𝜋𝜋,𝑇 𝜋𝜈) ≤ 𝜌 d(𝜋,𝜈), it follows that (1− 𝜌)d(𝜋,𝜈) ≤ d(𝑇 𝜋𝜈,𝜈). The
discrepancy
1
𝐷 𝜋(𝜈) :=
1−𝜌
d(𝑇 𝜋𝜈,𝜈)
thereforeconstitutesanupperboundonthemetricd(𝜋,𝜈),whichcouldin
principlebeusedtoquantifyhowwellagivendistribution𝜈approximates𝜋
insituationswherewedonothavedirectaccessto𝜋,butwheretheauxiliary
Markovchaincanbesimulated.Further,sincedisametricandtheMarkov
chain has a unique invariant distribution, 𝐷 𝜋(𝜈) = 0 if and only if 𝜈 = 𝜋.
An ideal scenario would be a fast mixing auxiliary Markov chain, so that
𝜌 ≪ 1,𝑇 𝜋𝜈 ≈ 𝜋,and 𝐷 𝜋(𝜈) ≈ d(𝜋,𝜈).Ontheotherhand,iftheauxiliary
Markovchainmixesslowlythenthevaluestakenbythediscrepancycould
fail to provide a meaningful indication of whether or not 𝜈 is an accurate
approximation to 𝜋. The utility of this upper bound therefore depends on
themixingpropertiesoftheauxiliaryMarkovchainonwhichitisbased.
Tomovetowardsacomputablebound,letussupposethatdisanintegral
probabilitymetric,meaningthatforany𝜋,𝜈 ∈ P(R𝑑)
∫ ∫
d(𝜋,𝜈) =sup 𝑔(x) 𝜋(dx)− 𝑔(x) d𝜈(x) (6.8)
𝑔∈G
for a suitable symmetric set1 of test functions G. Introducing the linear
operator
∫
(𝐿 𝜋𝑔)(·) = 𝑔(x′)𝑇 𝜋(·,dx′)−𝑔(·),
andobservingthat
∫ ∫ ∫
(𝐿 𝜋𝑔)(x) d𝜈(x) = 𝑔(x′)𝑇 𝜋(x,dx′) d𝜈(x)− 𝑔(x) d𝜈(x)
∫ ∫
= 𝑔(x) d𝑇 𝜋𝜈(x)− 𝑔(x) d𝜈(x),
thediscrepancycanbeexpressedas
∫
1
𝐷 𝜋(𝜈) =
1−𝜌
s 𝑔u ∈p
G
(𝐿 𝜋𝑔)(x) d𝜈(x). (6.9)
However,toactuallyevaluatethisdiscrepancywearerequiredtocompute
1 ThesetGissymmetricif−𝑔 ∈ Gwhenever𝑔 ∈ G;thisallowsustoavoidtaking
absolutevaluesinthedefinitionoftheintegralprobabilitymetric.202 AssessingandImprovingMCMC
expressions involving 𝐿 𝜋, which in effect requires simulating all possi-
ble realisations of one step of the auxiliary Markov chain, and is there-
fore intractable in general. To circumvent this issue, we will move from a
discrete-timeauxiliaryMarkovchaintoacontinuous-timeauxiliaryMarkov
process,withtime𝑡 transitionkernel𝑇𝑡 andassociatedlinearoperator 𝐿𝑡
𝜋 𝜋
anddiscrepancy 𝐷𝑡 .Thecontractionproperty(6.7)inthiscasereads
𝜋
d(𝑇 𝜋𝑡𝜋,𝑇 𝜋𝑡𝜈) ≤ 𝜌 𝑡 d(𝜋,𝜈). (6.10)
Consideringthe𝑡 ↓0limitwemay,iftheauxiliaryMarkovprocessmixes
rapidly enough, obtain an expression for the discrepancy in terms of the
generatorL 𝜋oftheauxiliaryMarkovprocess,whichwerecallfromSection
5.2.3isdefinedthroughitsactiononsuitablyregulartestfunctions𝑔 :R𝑑 →
Ras
1
(L 𝜋𝑔)(·) :=l 𝑡i ↓m
0
𝑡
𝐿𝑡 𝜋𝑔(·).
Indeed, assume that 𝜌 𝑡 = 𝑒−𝑐𝑡 for some 𝑐 > 0. Then, in a purely formal
manipulation,
𝐷 𝜋(𝜈) :=lim𝐷𝑡 𝜋(𝜈) (6.11)
𝑡↓0
∫ ∫
1 1
=s 𝑔u ∈p
G
l 𝑡i ↓m
0
1−𝜌
𝑡(𝐿𝑡 𝜋𝑔)(x) d𝜈(x) =
𝑐
s 𝑔u ∈p
G
(L 𝜋𝑔)(x) d𝜈(x),
wherethefinalstepusesthedefinitionofthegeneratorL 𝜋 andthefactthat
𝑒−𝑐𝑡 =1−𝑐𝑡+𝑜(𝑡) when𝑡 issmall.Intriguingly,thisformofdiscrepancy
may be computable, up to the rate constant 𝑐, which will be unknown in
general. The remaining challenges appear to be the selection of a suitable
auxiliary Markov process, for which the contraction property (6.10) is
satisfied,andthesolutionoftheoptimisationproblemoverG.Theseissues
areaddressed,respectively,inSections6.2.2and6.2.3.
Remark (Stein discrepancies) The discrepancy that we have introduced
in (6.11) is an instance of Stein discrepancy, as defined in the pioneering
work of Gorham and Mackey (2015). A Stein discrepancy refers to any
discrepancyoftheform
∫
sup (A 𝜋𝑔)(x) d𝜈 (6.12)
𝑔∈G′
wherethetheSteinoperatorA
𝜋
andtheSteinclassG′areselectedinsuch
a way that (6.12) is zero if and only if 𝜋 and 𝜈 are equal. The concept of
a Stein discrepancy is more general than the discrepancy 𝐷 𝜋(𝜈) we have6.2 ConvergenceBoundsforMCMC 203
constructed, since the Stein operator A 𝜋 need not arise from considera-
tion of a continuous-time Markov process; see for example the review of
Anastasiouetal.(2023).
6.2.2 ChoiceofAuxiliaryMarkovProcess
To make this argument useful we require a metric d and an auxiliary
continuous-timeMarkovprocessforwhichthecontractionproperty(6.10)
is satisfied. For the auxiliary Markov process, we will consider the over-
dampedLangevindiffusionfromSection1.4:
√
dX𝑡 = ∇log𝜋(X𝑡) d𝑡+ 2dW𝑡 (6.13)
whose infinitesimal generator is the second order differential operator
(L 𝜋𝑔)(x) = (Δ𝑔)(x)+⟨(∇log𝜋)(x),(∇𝑔)(x)⟩,whereΔdenotestheLapla-
cian differential operator for R𝑑. To simplify the presentation, we initially
make the assumption that 𝜋 is strongly log-concave, meaning that (3.18)
holds for some 𝑙 > 0, and we recall that a sufficient condition for strong
log-concavity is that −∇∇log𝜋(x) ≻ 𝜖I𝑑 for some 𝜖 > 0 and all x ∈ R𝑑,
where∇∇denotestheHessiandifferentialoperatorandthenotationA ≻ B
is used to mean that A−B is a symmetric positive definite matrix. This
assumptionwillberelaxedinSection6.2.4.Let𝐶𝑠(R𝑑,R𝑝) denotetheset
offunctions 𝑓 :R𝑑 →R𝑝 forwhichcontinuousderivativesexistoforders
upto𝑠 ∈ {0,1,...}∪{∞}.For𝑔 ∈𝐶0(R𝑑,R𝑝),let
∥𝑔(x)−𝑔(x′)∥
𝑀 (𝑔) := sup (6.14)
1 ∥x−x′∥
x,x′∈R𝑑
x≠x′
denote its (possibly infinite) Lipschitz constant. Recall that a function 𝑔
is called Lipschitz whenever 𝑀 (𝑔) < ∞. The following is a well-known
1
contractionresultfortheoverdampedLangevindiffusion,whoseproofcan
befoundine.g.vonRenesseandSturm(2005),orseeRemark1inEberle
(2016):
Theorem6.1(ContractionoftheoverdampedLangevindiffusion) Let 𝜋
bestronglylog-concaveandlet∇log𝜋beLipschitz.Thentheoverdamped
Langevin diffusion (6.13) satisfies the contraction property (6.10) in the
Wasserstein-1metric
∫ ∫
d𝑊 (𝜋,𝜈) := sup 𝑔(x) d𝜋(x)− 𝑔(x) d𝜈(x) (6.15)
1
𝑔∈𝐶0(R𝑑,R)
𝑀 1(𝑔)≤1
with 𝜌 𝑡 = 𝑒−𝑐𝑡 forsome𝑐 > 0andall𝑡 ∈ [0,∞).204 AssessingandImprovingMCMC
The infinitesimal generator L 𝜋 of the overdamped Langevin diffusion
requires∇𝑔andΔ𝑔toexist,buttheWasserstein-1integralprobabilitymetric
containsnon-differentiablefunctionsinthetestfunctionsetG.Thisappears
topreventusfromrunningtheformalargumentin(6.11).However,itturns
outthatwemay,withoutlossofgenerality,imposeadditionalsmoothness
ontheWasserstein-1testfunctionset:
Lemma6.2(SmoothertestfunctionsforWasserstein-1) For𝜋,𝜈 ∈ P(R𝑑),
∫ ∫
d𝑊 (𝜋,𝜈) = sup 𝑔(x) d𝜋(x)− 𝑔(x) d𝜈(x). (6.16)
1
𝑔∈𝐶∞(R𝑑,R)
𝑀 1(𝑔)≤1
Proof Since the supremum is being computed over a subset of the test
functions in (6.15), it is immediate that the right-hand side of (6.16) is
upper-bounded by d𝑊 (𝜋,𝜈). To prove the corresponding lower bound,
1
let 𝜖 ∈ (0,1). From the definition of d𝑊 in (6.15), there exists 𝑔 𝜖 with
𝑀 1(𝑔 𝜖) ≤ 1 such that ∫ 𝑔 𝜖(x)d𝜋(x) − ∫1 𝑔 𝜖(x)d𝜈(x) > d𝑊 1(𝜋,𝜈) − 𝜖.
Let 𝛿 > 0 and Z ∼ N(0,I𝑑). Set 𝑔 𝜖,𝛿(x) = E[𝑔 𝜖(x+𝛿Z)]. Then 𝑔 𝜖,𝛿 ∈
𝐶∞(R𝑑,R)andtheLipschitzconstantof𝑔 𝜖,𝛿 isnotgreaterthanthatof𝑔 𝜖,
sinceforallx,x′ ∈ R𝑑,
|𝑔 𝜖,𝛿(x)−𝑔 𝜖,𝛿(x′)| = |E[𝑔 𝜖(x+𝛿Z)−𝑔 𝜖(x′+𝛿Z)]|
≤ 𝑀 1(𝑔 𝜖)∥x−x′∥.
Thus𝑔 𝜖,𝛿isanelementofthetestsetoverwhichthesupremumiscomputed
ontherighthandsideof (6.16).From
|𝑔 𝜖,𝛿(x)−𝑔 𝜖(x)| = |E[𝑔 𝜖(x+𝛿Z)−𝑔 𝜖(x)]| ≤ 𝛿E[∥Z∥]𝑀 1(𝑔 𝜖), (6.17)
itfollowsthat 𝑔 𝜖,𝛿 distinguishesbetween 𝜋 and 𝜈 almostaswellas 𝑔 𝜖,in
thesensethat
∫ ∫
𝑔 𝜖,𝛿(x) d𝜋(x)− 𝑔 𝜖,𝛿(x) d𝜈(x)
∫ ∫
≥ 𝑔 𝜖(x) d𝜋(x)− 𝑔 𝜖(x) d𝜈(x)−2𝛿E[∥Z∥]𝑀 1(𝑔 𝜖)
> {d𝑊 (𝜋,𝜈)−𝜖 −2𝛿E[∥Z∥]},
1
whichcanbemadearbitrarilyclosetod𝑊 (𝜋,𝜈) bytaking𝜖,𝛿 → 0.Thus
1
thesupremumin(6.16)coincideswithd𝑊 (𝜋,𝜈),asclaimed. □
16.2 ConvergenceBoundsforMCMC 205
Tosummarise,ourformalargumenthasledtoabound
∫
1
𝐷 𝜋(𝜈) =
𝑐
sup Δ𝑔(x)+⟨∇log𝜋(x),∇𝑔(x)⟩ d𝜈(x) (6.18)
𝑔∈𝐶∞(R𝑑,R)
𝑀 1(𝑔)≤1
on the Wasserstein-1 distance between 𝜋 and 𝜈 that holds in the strongly
log-concave setting of Theorem 6.1. The route to obtaining this bound
is instructive, and the lessons that we learned will be exploited in the
subsequent sections, but unfortunately, the evaluation of this discrepancy
requiresachallengingoptimisationproblemtobesolved.Inthecasewhere
𝜈 has finite support, the objective function depends on 𝑔 only through its
derivatives at the nodes in the support. This observation enabled Gorham
and Mackey (2015) to cast a closely related optimisation problem as a
collection of linear programmes, which then can be numerically solved.
TheinterestedreaderisreferredtoGorhamandMackey(2015)forfurther
detail.However,therelianceonnumericalmethodstoevaluate(6.18)limits
the utility of (6.18). Instead, we will proceed in Section 6.2.3 to consider
alternativesetsoftestfunctionsforwhichthecorrespondingoptimisation
problemcanbeanalyticallysolved.
6.2.3 KernelSteinDiscrepancy
The aim of this section is to consider alternatives to the Wasserstein-1
distance,correspondingtoalternativesets G oftestfunctionsdefiningthe
integralprobabilitymetric(6.8),forwhichtheoptimisationproblemin(6.9)
can be explicitly solved using the kernel trick advertised in Section 6.1.3.
However,theuseofalternativemetricsleadsustodepartfromtheargument
of Section 6.2.1, which was based on the Wasserstein-1 contraction result
of Theorem 6.1, raising the question of whether the resulting discrepancy
is still a meaningful convergence bound. This question will be answered
positivelyinSection6.2.4.
To simplify the discussion, we start by considering vector fields as test
functions, allowing us to reduce the order of the differential operators
involved.Thus,inthegeneralnotationof (6.12),weconsider
(A 𝜋g)(x) = (∇·g)(x)+⟨(∇log𝜋)(x),g(x)⟩, (6.19)
which is a first order differential operator and the elements g are now
vector fields g : R𝑑 → R𝑑. The discussion in Section 6.2.2 corresponds
to g(x) = (∇𝑔)(x) for twice-differentiable 𝑔 : R𝑑 → R. Here, and in
thesequel,foreaseofpresentation,wehavesubsumedtheconstantfactor206 AssessingandImprovingMCMC
1/𝑐 into the definition of the vector fields g. Now, if we are to consider
alternative test functions g, the minimum requirement on g is that A 𝜋g
integratesto0withrespectto𝜋,toensurethatthediscrepancyweconstruct
vanisheswhen𝜋and𝜈areequal.Tothisend,wehavethefollowingresult:
Lemma 6.3 Let g : R𝑑 → R𝑑 satisfy g ∈ L1(𝜋) and A 𝜋g ∈ L1(𝜋),
whereA
𝜋
isdefinedin(6.19).Then∫ (A 𝜋g)(x) d𝜋(x) =0.
Proof Firstnoticethat
∫ ∫ ∫
1
(A 𝜋g)(x) d𝜋(x) = 𝜋(x)(∇·(𝜋g))(x) d𝜋(x) = (∇·(𝜋g))(x) dx,
whichsuggestsusingthedivergencetheoremtocalculatethisintegral.To
avoid the explicit calculation of surface integrals, which would otherwise
berequiredwhenusingthedivergencetheorem,wewillfirstapproximate
the vector field 𝜋g using another vector field with compact support. Let
𝜑 𝑚 : R𝑑 → R denote the 𝑚th term in a sequence of compactly supported
functions with 𝜑 𝑚(x) = 1 for ∥x∥ ≤ 𝑚, sup x∥∇𝜑 𝑚(x)∥ < 𝑚−1 for each
𝑚 ∈ N, and 𝜑 𝑚(x) ↑ 1 for each x ∈ R𝑑. From the divergence theorem
appliedtoavectorfieldwithcompactsupport,
∫
0= (∇·(𝜑 𝑚𝜋g))(x) dx
∫ ∫
= ⟨∇𝜑 𝑚(x),(𝜋g)(x)⟩ dx+ 𝜑 𝑚(x)(∇·(𝜋g))(x) dx.
Since 𝜑 𝑚 ↑ 1 pointwise and ∇ · (𝜋g) ∈ L1(R𝑑), from the dominated
convergencetheorem
∫ ∫
𝜑 𝑚(x)(∇·(𝜋g))(x) dx→ (∇·(𝜋g))(x) dx.
On the other hand, using Cauchy–Schwarz and the assumption that 𝜋g ∈
L1(R𝑑),
(cid:12)∫ (cid:12) (cid:18) (cid:19)∫
(cid:12) (cid:12) ⟨∇𝜑 𝑚(x),(𝜋g)(x)⟩ dx(cid:12) (cid:12) ≤ sup∥∇𝜑 𝑚(x)∥ ∥(𝜋g)(x)∥ dx→0.
(cid:12) (cid:12)
x
Thuswehaveshownthat
∫ ∫
(A 𝜋g)(x) d𝜋(x) = (∇·(𝜋g))(x) dx=0,
asclaimed. □
Our attention now turns to selecting a set of vector fields g for which
Lemma 6.3 holds and for which the optimisation problem in (6.18) can6.2 ConvergenceBoundsforMCMC 207
be explicitly solved. One approach to this task is to use a matrix-valued
reproducingkernel,meaningafunctionK :R𝑑 ×R𝑑 →R𝑑×𝑑 thatis
1. transpose-symmetric;K(x,x′) =K(x′,x)⊤ forallx,x′ ∈ R𝑑
2. positivesemi-definite;
𝑛 𝑛
∑︁∑︁
⟨c𝑘,K(x𝑘,x𝑘′)c𝑘′⟩ ≥ 0
𝑘=1 𝑘′=1
forallx 1,...,x𝑛 ∈ R𝑑,allc 1,...,c𝑛 ∈ R𝑑,andall𝑛 ∈ N.
For clarity, we emphasise that ⟨c,c′⟩ = c⊤c′ is the usual Euclidean inner
product on R𝑛; in the sequel we will use subscripts to distinguish other
innerproductsastheyareintroduced.LetK = K(·,x),sothatK : R𝑑 →
x x
R𝑑×𝑑 is matrix-valued. For vector-valued functions g = (cid:205)𝑛 𝑘=1K x𝑘c𝑘 and
g′ =(cid:205) 𝑙𝑚 =1K
x
𝑙′c 𝑙′,defineaninnerproduct
𝑛 𝑚
∑︁∑︁
⟨g,g′⟩
H(K)
= ⟨c𝑘,K(x𝑘,x 𝑙′)c 𝑙′⟩. (6.20)
𝑘=1 𝑙=1
ThereisauniqueHilbertspacereproducedbyK,denotedH(K);seePropo-
sition2.1ofCarmelietal.(2006).Thisspaceischaracterisedas
H(K) =span{K c:x,c ∈ R𝑑}
x
whereheretheclosureistakenwithrespecttotheinnerproductin(6.20).
TheresultingHilbertspacesatisfiesthereproducingproperty
⟨g,K c⟩ = ⟨g(x),c⟩
x H(K)
forallg ∈ H(K) andx,c ∈ R𝑑,whichisaparticularinstanceofthekernel
trickdiscussedinSection6.1.3.Inwhatfollows,itisconvenienttooverload
notation,suchthatthereproducingpropertybecomes⟨g,K ⟩ =g(x)in
x H(K)
aninformalshorthand.
Assumingsufficientregularitythat
𝐹
𝜈
:H(K) →R
∫
g↦→ (A 𝜋g)(x) d𝜈(x)
is a bounded linear functional, the Riesz representation theorem tells us
that there is a unique element 𝜇 𝜈 such that 𝐹 𝜈(·) = ⟨𝜇 𝜈,·⟩ H(K). Using our
reproducingpropertyshorthand,
∫
𝜇 𝜈(x′) = ⟨𝜇 𝜈,K x′⟩
H(K)
= 𝐹 𝜈(K x′) = Ax 𝜋K(x,x′)d𝜈(x),208 AssessingandImprovingMCMC
wherethesuperscriptinAx 𝜋 indicatestheactionofA 𝜋 onthexargument,
collapsing the matrix-valued function K into the vector-valued function
x
AxK .Itfollowsthat,ifweconsiderthecollectionofvectorfieldsgwithin
𝜋 x
theunitballofH(K),ouroptimisationproblemcanbeexplicitlysolved:
∫
sup (A 𝜋g)(x) d𝜈(x) = sup ⟨g,𝜇 𝜈⟩
H(K)
= ∥𝜇 𝜈∥ H(K), (6.21)
∥g∥H(K)≤1 ∥g∥H(K)≤1
where,againfromthereproducingpropertyandtheassumptionthat 𝐹 𝜈 is
aboundedlinearfunctional,
(cid:28)∫ ∫ (cid:29)
∥𝜇 𝜈∥2
H(K)
= Ax 𝜋K
x
d𝜈(x), Ax 𝜋′ K
x′
d𝜈(x′)
H(K)
∬
= AxAx′⟨K ,K ⟩ d𝜈(x)d𝜈(x′)
𝜋 𝜋 x x′ H(K)
∬
= AxAx′ K(x,x′) d𝜈(x)d𝜈(x′).
𝜋 𝜋
It is convenient to introduce the shorthand 𝑘 𝜋(x,x′) := Ax 𝜋Ax 𝜋′K(x,x′),
whenceweobtainthediscrepancy
√︄
∬
D 𝑘 (𝜈) := 𝑘 𝜋(x,x′) d𝜈(x)d𝜈(x′), (6.22)
𝜋
which isexactly ofthe formwe soughtin (6.6).This wastermed akernel
Stein discrepancy in Chwialkowski et al. (2016); Liu et al. (2016), due to
its dependence on a reproducing kernel and its characterisation as a Stein
discrepancy(6.1).Asecondconsequenceof (6.21)isthatwecanviewthe
kernelSteindiscrepancyasageneralisedmoment
(cid:13) 𝑛 (cid:13)
D 𝑘 𝜋(𝜈 𝑛) =(cid:13) (cid:13) (cid:13) (cid:13)𝑛1 ∑︁
𝑘=1
Ax 𝜋K x(cid:12) (cid:12) x=x𝑘 d𝜈(x)(cid:13) (cid:13) (cid:13)
(cid:13)
H(K),
whichtakesasimilarformto(6.4)fromSection6.1.2,albeitthegeneralised
momentcannowbeinfinite-dimensionalbyvirtueoftakingvaluesinH(K).
The function AxK is indeed a member of H(K) due to the differential
𝜋 x
reproducing property (see Barp et al., 2022, Appendix C6). The kernel
Stein discrepancy has the potential to be a useful bias diagnostic, but first
weneedtoestablishthatithasourbasicdesiredfunctionality,suchasbeing
equal to 0 if and only if 𝜋 and 𝜈 are identical. Clearly, then the choice of
kernelKwillbeimportant,soweaddressthispointnext.
Oneofthesimplestformsofmatrix-valuedreproducingkernelisK(x,x′) =6.2 ConvergenceBoundsforMCMC 209
k(x,x′)I𝑑,wherekisascalar-valuedreproducingkernel.Thischoiceleads
totheexplicitformula,duetoOatesetal.(2017):
k𝜋(x,x′) = ∇ x·∇ x′k(x,x′)+⟨∇ xlog𝜋(x),∇ x′k(x,x′)⟩
+⟨∇ log𝜋(x′),∇ k(x,x′)⟩
x′ x
+⟨∇ log𝜋(x),∇ log𝜋(x′)⟩k(x,x′) (6.23)
x x′
The function k𝜋 is automatically a scalar-valued reproducing kernel (see
Barpetal.,2022,Theorem2.6),andk𝜋(x,x′) = Ax 𝜋g x′(x)whereg x′(x) :=
Ax′K(x,x′) ∈ H(K). Thus, if the matrix-valued reproducing kernel K is
𝜋
selectedsuchthattheconditionsg ∈ L1(𝜋) andA 𝜋g ∈ L1(𝜋) ofLemma
6.3 are satisfied for each g ∈ H(K), it follows that ∫ k𝜋(x,x′)d𝜋(x) =
∫ Axg (x) d𝜋(x) = 0forallx′ ∈ R𝑑.Sufficientconditionsforsatisfying
𝜋 x′
thepreconditionsofLemma6.3willshortlybediscussed.
Inthecasewhere𝜈 =(cid:205)𝑛 𝑘=1𝑤 𝑘𝛿 x𝑘isadiscretedistribution,(6.22)reduces
tothedoublesum
(cid:118)(cid:116)
𝑛 𝑛
∑︁∑︁
D k𝜋(𝜈) = 𝑤 𝑘𝑤 𝑘′k𝜋(x𝑘,x𝑘′). (6.24)
𝑘=1 𝑘′=1
For the degenerate reproducing kernel with k(x,x′) = 1 for all x,x′ ∈ R𝑑
and uniform weights 𝑤 𝑘 = 𝑛−1, the kernel Stein discrepancy in (6.24)
reducestothesimpleform
(cid:13) 𝑛 (cid:13)
(cid:13)1 ∑︁ (cid:13)
D k𝜋(𝜈) =(cid:13)
(cid:13)𝑛
∇ xlog𝜋(x𝑘)(cid:13) (cid:13),
(cid:13) 𝑘=1 (cid:13)
whichispreciselythebiasdiagnosticfrom(6.4).Inthiscase,H(K) isthe
Hilbert space of constant vector fields on R𝑑 with norm ∥g∥ = ∥𝜷∥
H(K)
where g(x) = 𝜷 for all x ∈ R𝑑, which is insufficiently rich to determine
whetherornot𝜋 and𝜈 arecloseorequal.However,withasuitablechoice
ofreproducingkernelthekernelSteindiscrepancycandistinguishbetween
differentdistributionsandindeedprovideaformofconvergencecontrol,as
weexplaininSection6.2.4.
First,however,wemustensuretheconditionsofLemma6.3aresatisfied,
sothatD (𝜈) =0when𝜋and𝜈areequal.Thiscanbeachievedusingthe
k𝜋
followingresult:
Lemma 6.4 If K(x,x′) = k(x,x′)I𝑑 with k(x,x′) = 𝜙(x−x′) for some
𝜙 ∈𝐶2(R𝑑,R),then∇log𝜋 ∈ L1(𝜋)impliesg ∈ L1(𝜋)andA 𝜋g ∈ L1(𝜋)
forallg ∈ H(K).210 AssessingandImprovingMCMC
Proof Thereproducingproperty,followedbyCauchy–Schwarz,gives
∫ ∫
∥g(x)∥ d𝜋(x) = ∥⟨g,K ⟩ ∥ d𝜋(x)
x H(K)
∫
√︁ √︁
≤ ∥g∥ tr(K(x,x)) d𝜋(x) = ∥g∥ 𝑑𝜙(0),
H(K) H(K)
whichisfiniteforallg ∈ H(K).Similarly,
∫ ∫
|(A 𝜋g)(x)| d𝜋(x) = |⟨g,Ax 𝜋K x⟩ H(K)| d𝜋(x)
∫
≤ ∥g∥ √︁ AxAx′K(x,x′)| d𝜋(x).
H(K) 𝜋 𝜋 x′=x
Here the assumption 𝜙 ∈ 𝐶2(R𝑑,R) ensures the application of AxAx′ to
𝜋 𝜋
K(x,x′) iswell-defined.Specialisingtothetranslation-invariantreproduc-
ingkernelinthestatement,wehave
AxAx′ K(x,x′)| =−(Δ𝜙)(0)+𝜙(0)∥(∇log𝜋)(x)∥2 (6.25)
𝜋 𝜋 x′=x
whichshowsthatA 𝜋g ∈ L1(𝜋)whenever∇log𝜋 ∈ L1(𝜋),andcompletes
theargument. □
ThekernelSteindiscrepancieswehavejustconstructedarewell-defined
andcomputable,butwehavenotyetaddressedthequestionofifandhow
thevaluesofthediscrepancy D (𝜈) arerelatedtotheclosenessof 𝜋 and
k𝜋
𝜈.Indeed,sincewehaveusedalternativetestfunctionscomparedto(6.18),
wecannotexpect D (𝜈) toprovideanupperboundontheWasserstein-1
k𝜋
distancebetween 𝜋 and 𝜈.Thenextsectionexplainstowhatextentkernel
Steindiscrepanciesrelatetotheclosenessof𝜋and𝜈.
6.2.4 ConvergenceControl
The aim of this section is to establish whether kernel Stein discrepancies
canprovidecontroloverintegralprobabilitymetrics.Atthesametime,we
willweakenthestronglog-concavityassumptionfromSection6.2.2toan
assumptionthat𝜋isdistantlydissipative,meaningthat
(cid:26) ⟨(∇log𝜋)(x)−(∇log𝜋)(x′),x−x′⟩(cid:27)
liminf inf − > 0,
𝑟→∞ x,x′∈R𝑑 ∥x−x′∥2
∥x−x′∥=𝑟
forwhichWasserstein-1contractionoftheoverdampedLangevindiffusion
(6.13) can still be established (see Lindvall and Rogers, 1986; Eberle,
2016).ThenextLemmademonstratesthatdistantdissipativityisindeeda
generalisationofstronglog-concavity:6.2 ConvergenceBoundsforMCMC 211
Lemma 6.5 If ∇log𝜋 is bounded on a compact set 𝑆 ⊂ R𝑑 and 𝜋 is
stronglylog-concaveontheboundaryofandoutsideoftheset𝑆,then𝜋 is
distantlydissipative.
Proof Let b(x) := (∇log𝜋)(x), let 𝑆 be the compact set in the state-
ment,andletint(𝑆)denotetheinteriorof𝑆.Fromthestronglog-concavity
assumption,thereexists𝑐 > 0suchthatforallx,x′ ∉int(𝑆) wehave
⟨b(x)−b(x′),x−x′⟩
− > 𝑐. (6.26)
∥x−x′∥2
Since b is bounded on 𝑆, we may pick 𝐵 > sup ∥b(x)∥. Since 𝑆 is
x∈𝑆
compact, 𝑆 is contained in {x : ∥x∥ ≤ 𝑟/2} for some sufficiently large
𝑟 > 0,andwemaysupposethat𝑟 > 2𝐵/𝑐,so𝑐′ := 𝑐−(2𝐵/𝑟) > 0.
Consider arbitrary x,x′ such that ∥x−x′∥ > 𝑟. If x,x′ ∉ 𝑆, condition
(6.26) is satisfied. Thus consider the other case, where without loss of
generality x ∈ 𝑆 (and thus x′ ∉ 𝑆). Let x′′ be the closest point to x that
belongsto𝑆andiscolinearwithxandx′.Then
−⟨b(x)−b(x′),x−x′⟩ =−⟨b(x)−b(x′′),x−x′⟩−⟨b(x′′)−b(x′),x−x′⟩
(cid:28) x−x′′ (cid:29)
=−∥x−x′∥ b(x)−b(x′′),
∥x−x′′∥
∥x−x′∥
− ⟨b(x′′)−b(x′),x′′−x′⟩
∥x′′−x′∥
> −∥x−x′∥·2𝐵 + 1·𝑐∥x′′−x′∥2
whereinthefinallinetheCauchy–Schwarzandtriangleinequalitieswere
usedtoboundthefirstterm,andforthesecondterm(6.26)wasappliedto
x′,x′′ ∉int(𝑆).Thus
⟨b(x)−b(x′),x−x′⟩ 2𝐵 2𝐵
− > − +𝑐 > − +𝑐 = 𝑐′.
∥x−x′∥2 ∥x−x′∥ 𝑟
Combiningthesetworesults,wehaveshownthatforallx,x′with∥x−x′∥ >
𝑟,
⟨b(x)−b(x′),x−x′⟩
− > min(𝑐,𝑐′) > 0
∥x−x′∥2
andthusthedistantdissipativityof𝜋isestablished. □
ThemostcommonlyusedkernelSteindiscrepanciesdonotoffercontrol
oftheWasserstein-1distance,thoughitispossible,throughacarefulchoice
ofkernel,toobtainWasserstein-1control;wereturntothispointattheend212 AssessingandImprovingMCMC
ofthepresentsection.Rather,themostcommonkernelSteindiscrepancies
offercontrolofthe(weaker)Dudleymetric
∫ ∫
d𝐷(𝜋,𝜈) := sup 𝑔(x) d𝜋(x)− 𝑔(x) d𝜈(x)
𝑔∈𝐶0(R𝑑,R)
𝑀 0(𝑔)+𝑀 1(𝑔)≤1
on P(R𝑑), at least in certain scenarios where 𝜋 is distantly dissipative
andthereproducingkernel 𝑘 iscarefullyselected.Foraboundedfunction
𝑔 : R𝑑 → R𝑝, let 𝑀 (𝑔) = sup ∥𝑔(x)∥ and recall that 𝑀 (𝑔) denotes
0 x∈R𝑑 1
theLipschitzconstant,from(6.14).
Theorem 6.6 (Weak convergence control; Theorem 8 of Gorham and
Mackey, 2017) Let 𝜋 be distantly dissipative and ∇log𝜋 be Lipschitz.
LetK(x,x′) = 𝜙(x−x′)I𝑑 where 𝜙(z) = (1+∥z/𝜎∥2)−𝛽 forsome 𝜎 > 0,
𝛽 ∈ (0,1).ThenD k𝜋(𝜈 𝑛) →0impliesthatd𝐷(𝜋,𝜈 𝑛) →0.
Ofcourse,if∇log𝜋 isLipschitz,then∇log𝜋 isautomaticallybounded
onanycompactset.ThesetoftestfunctionsthatdefinetheDudleymetric
issmallerthanthatfortheWasserstein-1metric,andasaresulttheDudley
metricisweakerthantheWasserstein-1metric.Indeed,theDudleymetric
actuallymetrisestheso-calledweakconvergenceofdistributions,meaning
that𝜈 𝑛convergesweakly(orindistribution)to𝜋ifandonlyifd𝐷(𝜋,𝜈 𝑛) →
0.ThekernelSteindiscrepancyitselfdoesnotprovideanupperboundon
d𝐷 in this context, but an explicit nonlinear transformation of the kernel
Steindiscrepancydoesstillconstituteanexplicitupperbound.SeeGorham
andMackey(2017)forfulldetails.
The reproducing kernel appearing in Theorem 6.6 is called the inverse
multi-quadricreproducingkernel,anditsusewasnotaccidental.Thecare-
fulanalysisinTheorem6ofGorhamandMackey(2017)demonstratesthat
reproducingkernelswithlightertailscanfailtocontrolweakconvergence,
at least in dimension 𝑑 ≥ 3. Moreover, the inverse multi-quadric repro-
ducing kernel is computationally straightforward, and in fact the property
of weak convergence control extends to the parametric family of inverse
multi-quadricreproducingkernelsoftheform
K IMQ(x,x′) = (cid:0) 1+∥𝚺−1/2(x−x′)∥2(cid:1)−𝛽 I𝑑, 𝚺 ≻ 0, 𝛽 ∈ (0,1), (6.27)
where 𝚺 is a symmetric positive definite matrix, with the former case
recovered when 𝚺 = I𝑑; see Theorem 4 in Chen et al. (2019). For the
extendedfamilyofinversemulti-quadricreproducingkernelsin(6.27),the6.2 ConvergenceBoundsforMCMC 213
explicitformin(6.23)becomes
4𝛽(𝛽+1)∥𝚺−1(x−x′)∥2
k𝜋(x,x′) =−
(1+∥𝚺−1/2(x−x′)∥2)𝛽+2
(cid:20) tr(𝚺−1)+⟨(∇log𝜋)(x)−(∇log𝜋)(x′),𝚺−1(x−x′)⟩(cid:21)
+2𝛽
(1+∥𝚺−1/2(x−x′)∥2)𝛽+1
⟨(∇log𝜋)(x),(∇log𝜋)(x′)⟩
+ ,
(1+∥𝚺−1/2(x−x′)∥2)𝛽
which can be readily computed provided that the gradient ∇log𝜋 can be
pointwiseevaluated.
To illustrate the performance of kernel Stein discrepancies, consider
again the Gaussian mixture distribution 𝜋 from (6.3). This distribution
is distantly dissipative2 and has a log-density that is Lipschitz, meaning
we are in the setting of Theorem 6.6. Proceeding with the inverse multi-
quadric reproducing kernel (6.27) with parameters Σ = 1, 𝛽 = 0.5, we
thereforehaveaguaranteethatconvergenceofthekernelSteindiscrepancy
D k𝜋(𝜈 𝑛) to0impliestheweakconvergenceof𝜈
𝑛
to𝜋.Inwhatfollowswe
let 𝜈 𝑛 = 𝑛1 (cid:205)𝑛 𝑘=1𝛿 𝑋
𝑘
be the empirical distribution associated to a Markov
chainsamplepath(𝑋 𝑘) 1≤𝑘≤𝑛andusekernelSteindiscrepancytodetermine
whetherornot𝜈 𝑛convergesto𝜋.TheleftpanelofFigure6.3displaystypical
realisations of the Markov chain sample path (top), while underneath the
associated kernel Stein discrepancy (as a function of 𝑛) is displayed. In
addition,thefigureincludescorrespondingresultsforaMarkovchainthat
leaves only the first component of 𝜋 invariant (right), and thus does not
provide a consistent approximation of 𝜋. Asymptotically, it can be seen
that the kernel Stein discrepancy correctly distinguishes between the two
scenarios,inwhichtheMarkovchaindoesanddoesnotleave 𝜋 invariant.
However, focusing on the biased Markov chain, at small sample sizes the
discrepancy does not detect that the chain has only explored one mixture
component, and the discrepancy appears to decrease smoothly as more
samples are collected. It is only once a sufficient number of samples have
been collected that the failure of the Markov chain to explore the second
mixture component is detected, and the discrepancy ceases decreasing to
reflectthat.
Thesmall𝑛 behaviourofthekernelSteindiscrepancyobservedinFig-
ure 6.3has been termedblindness tomixing proportionsin Wenliangand
Kanagawa (2021), and provides an important note of caution that, when
2 TheclassofdistantlydissipativedistributionsincludesallfiniteGaussianmixtureswith
commoncovariancematrix;seeGorhametal.(2019).214 AssessingandImprovingMCMC
   
   
   
                         
 L W H U D W L R Q  L W H U D W L R Q
   
     
   
     
   
     
       
           
 L W H U D W L R Q  L W H U D W L R Q
Figure6.3 PerformanceofkernelSteindiscrepancy.Three
unbiased(left)andbiased(right)Markovchainsweresimulated.
Intheunbiasedcase,thekernelSteindiscrepancyD k𝜋(𝜈 𝑛)
correctlydetectsthattheMarkovchainsareconvergingtothe
target.Inthebiasedscenario,thekernelSteindiscrepancydetects
thattheMarkovchainshavenotconvergedtothecorrecttarget,
butthisbecomesclearonlyafterasufficientnumberofiterations
havebeenperformed.[Starsindicatetheinitialstateofeach
Markovchain.Thedensity𝜋isshaded.]
using kernel Stein discrepancies to assess MCMC output, the failure of
the Markov chain to explore distant high-probability regions may only be
detectedifthelength𝑛oftheMarkovchainoutputislargeenough.Ourfor-
mal argument in Section 6.2.1 provides insight into this phenomenon; the
Wasserstein-1contractionrateconstantoftheoverdampedLangevindiffu-
)
ν(
x
n
πk
D
)
ν(
x
n
πk
D6.2 ConvergenceBoundsforMCMC 215
sion,denoted𝑐in(6.11),canbeextremelysmallfordistributionssuchas𝜋
forwhichtheblindnessphenomenonisencountered,sinceforthisdiffusion
a move between the effective support of the distinct mixture components
isarareevent.Asaconsequence,althoughkernelSteindiscrepancydoes
provide convergence control, in unfavourable settings it may provide only
alooseformofcontrol.
In multi-dimensional settings, an appropriate choice of the matrix 𝚺
appearing in the inverse multi-quadric reproducing kernel (6.27) can be
important. In situations where 𝜈 𝑛 can be interpreted as an approximation
of 𝜋, the present authors continue to recommended the use of 𝚺 = I𝑑,
followingtheapplicationofadata-dependenttransformation
(x𝑖,∇log𝜋(x𝑖)) ↦→ (𝚪 𝑛−1x𝑖,𝚪 𝑛∇log𝜋(x𝑖)), (6.28)
where 𝚪 𝑛 is the diagonal matrix whose diagonal entries are the mean
absolutedeviationofthecorrespondingcoordinatesof(x𝑘) 1≤𝑘≤𝑛,thestates
on which 𝜈 𝑛 is supported. This transformation amounts to performing the
change of variables x ↦→ x := 𝚪−1x prior to computing the kernel Stein
(cid:101) 𝑛
discrepancywith𝚺 =I𝑑.Indeed,denotingby (cid:101)𝜋thetransformedprobability
densityfunction,thechange-of-variablesformulagivesthat
∇ (cid:101)xlog (cid:101)𝜋( (cid:101)x) = ∇ (cid:101)xlog[det(𝚪 𝑛)𝜋(x)]
= ∇ log𝜋(x)
(cid:101)x
= ∇ (cid:101)xlog𝜋(𝚪 𝑛(cid:101)x) = 𝚪 𝑛∇log𝜋(𝚪 𝑛(cid:101)x) = 𝚪 𝑛∇log𝜋(x).
In this recommendation, the mean absolute deviation is used to provide a
robust estimate for the unknown scale of the standard deviation of each
coordinatein𝜋.Onemaybetemptedtoconsiderextendingthisrecommen-
dation to the more general class of invertible linear transformations, but
theauthorsofRiabizetal.(2022)cautionedthatifconsiderableadditional
sample-based variability is introduced in estimating a general invertible
lineartransform,thiscanactasanundesirableconfoundingfactorwhenthe
resultingdiscrepanciesaretobeinterpretedforassessmentofMCMC.Ina
similarspirit,so-calledslicedkernelSteindiscrepancieshaverecentlybeen
developedforhigh-dimensionalapplications(Gongetal.,2020);however,
at the time of writing the convergence control of these discrepancies has
yettobeestablished.
Asidefromthespecificlimitationsjustdiscussed,thereareamyriadof
statisticalapplicationswherekernelSteindiscrepanciescanandhavebeen
successfullyapplied.Twodistinctuseswillbediscussedinthischapter;op-
timalweightingofMarkovchainoutput(Section6.3),andoptimalthinning216 AssessingandImprovingMCMC
of Markov chain output (Section 6.4). To close this section, we highlight
thatstrongermodesofconvergencecanalsobecontrolledbykernelStein
discrepancies. The following result, which is a special case of Kanagawa
et al. (2024), indicates how a suitable tilting of the reproducing kernel
enforcesmomentconvergencecontrol:
Theorem 6.7 (Moment convergence control; Corollary 3.4 in Kanagawa
etal.(2024)) Let 𝜋 bedistantlydissipativeand ∇log𝜋 beLipschitz.Let
𝑞 ∈ N,x
0
∈ R𝑑,andadopttheshorthand𝑤 𝑟(x) := (1+∥x−x 0∥2)(𝑟−1)/2.
Let
K(x,x′) = 𝑤 𝑞(x)𝑤 𝑞(x′)K IMQ(x,x′)+𝑤 𝑞−1(x)𝑤 𝑞−1(x′)(1+⟨x−x 0,x′−x 0⟩)I𝑑
whereK istheinversemulti-quadricreproducingkernelfrom(6.27).Let
IMQ
(𝜈 𝑛) 𝑛∈N beasequenceofdistributionswhosemomentsuptoorder𝑞 exist.
Then D k𝜋(𝜈 𝑛) → 0 implies that both d𝐷(𝜋,𝜈 𝑛) → 0 and the moments of
𝜈
𝑛
uptoorder𝑞 convergetothoseof𝜋.
In other words, the kernel Stein discrepancies constructed in this manner
have control over the Wasserstein-𝑞 distance, which is equivalent to weak
convergenceplustheconvergenceofmomentsupto𝑞thorder.Theprincipal
requirement for making use of the kernel Stein discrepancies in Theorem
6.7 is to pick a location x ∈ R𝑑. Theoretical guidance tells us that this
0
kernelSteindiscrepancyprovidestightestcontrolovermomentswhenx is
0
inaregionofhighprobabilityfor𝜋,sinceotherwisetheweightings𝑤 𝑞 and
𝑤 𝑞−1 become approximately constant and we recover the standard kernel.
Thedifficultyoffindingsuchalocationx willbecontext-dependent.
0
6.2.5 StochasticGradientSteinDiscrepancy
The aim of this section is to discuss how kernel Stein discrepancies may
be extended to the so-called tall data setting, where algorithms such as
stochasticgradientMCMCfromChapter3areused.Theprincipalchallenge
inthissettingisthatcomputationofthegradient∇log𝜋isassociatedwith
ahighcomputationalcost𝑂(𝑁) duetotheformofthelikelihood
𝑁
(cid:214)
𝐿(x;D) = 𝐿(x;y𝑗)
𝑗=1
asaproductofalargenumber𝑁oftermsthateachneedtobedifferentiated.
PerformingBayesianinferencewithaprior𝜋 (x),theposteriordistribution
06.2 ConvergenceBoundsforMCMC 217
takestheform
𝑁
(cid:214)
𝜋(x) ∝ 𝜋 𝑗(x), 𝜋 𝑗(x) ∝ 𝜋 0(x)1/𝑁𝐿(x;y𝑗),
𝑗=1
whereweassumeeach𝜋
𝑗
canbeproperlynormalised.Let𝜈
𝑛
= 𝑛1 (cid:205)𝑛 𝑘=1𝛿
x𝑘
define a sequence (𝜈 𝑛) 𝑛∈N ⊂ P(R𝑑) of discrete distributions in terms
of a sequence (x𝑘) 𝑘∈N ⊂ R𝑑. Fix a batch size 𝑚 ≪ 𝑁 and, for each
𝑘, independently select a uniformly random subset S 𝑚(𝑘) of size 𝑚 from
{1,...,𝑁}.Then
𝑁 ∑︁
(cid:98)b𝑘 :=
𝑚
∇log𝜋 𝑗(x𝑘)
𝑗∈S𝑚(𝑘)
is a stochastic approximation to the gradient b(x𝑘) = ∇log𝜋(x𝑘) that
can be computed at a relatively lower 𝑂(𝑚) cost. It is then tempting to
replacetheexactgradientsb(x𝑖)withtheirstochasticcounterparts(cid:98)b𝑖within
the construction of kernel Stein discrepancy. For reproducing kernels of
the form K(x,x′) = k(x,x′)I𝑑, this construction leads to the following
stochasticapproximationof (6.23):
(cid:68) (cid:69)
(cid:98)k𝜋(x𝑖,x𝑗) := ∇ x·∇ x′k(x,x′)|
x=x𝑖,x′=x𝑗
+ (cid:98)b𝑖, ∇ x′k(x𝑖,x′)|
x′=x𝑗
+(cid:68) (cid:98)b𝑗, ∇ xk(x,x𝑗)(cid:12) (cid:12) x=x𝑖(cid:69) +(cid:68) (cid:98)b𝑖, (cid:98)b𝑗(cid:69) k(x𝑖,x𝑗)
Gorhametal.(2020)definedthestochastickernelSteindiscrepancyinthis
contextas
(cid:118)(cid:116)
𝑛 𝑛
1 ∑︁∑︁
D
kˆ
𝜋(𝜈 𝑛) =
𝑛2
kˆ 𝜋(x𝑘,x𝑘′).
𝑘=1 𝑘′=1
An immediate question is whether or not the introduction of stochastic-
ity into the gradients jeopardised the weak convergence control property
established in the case of exact gradients in Theorem 6.6. It turns out
that, provided each 𝜋 1,...,𝜋 𝑁 satisfies the conditions of Theorem 6.6,
a form of weak convergence control continues to hold. Specifically, if
each 𝜋 𝑖 is distantly dissipative, and each ∇log𝜋 𝑖 is Lipschitz, then with
k(x,x′) = (1+∥(x−x′)/𝜎∥2)−𝛽,𝜎 > 0, 𝛽 ∈ (0,1),Gorhametal.(2020,
Theorem4)showsthat
D
kˆ
𝜋(𝜈 𝑛) →0 =⇒ d D(𝜋,𝜈 𝑛) →0
almost surely. This result justifies the use of stochastic kernel Stein dis-
crepanciesintheirownright,notmerelyasapproximationstokernelStein218 AssessingandImprovingMCMC
discrepancy that becomes exact when 𝑚 = 𝑁. Indeed, in principle, only a
batchsizeof𝑚 =1isrequired.
Figure 6.4 displays stochastic kernel Stein discrepancies computed for
the sequence of empirical distributions 𝜈 𝑛 produced using stochastic gra-
dient Langevin dynamics applied to the logistic regression example from
Section 3.5.1. It can be seen that, even for a batch size 𝑚 = 100, which is
muchlessthanthesize 𝑁 = 104 ofthedataset,thestochastickernelStein
discrepancyiscapableofprovidingsimilarinformationontheperformance
of the sampler compared to when larger batch sizes, such as 𝑚 = 103 are
used.Thepredictabledecreaseofthediscrepancyindicatesthattheintrinsic
bias of stochastic gradient Langevin dynamics is negligible relative to the
errorincurredbyusingonly𝑛ofthesesamplestoconstruct 𝜈 𝑛.Neverthe-
less,atthetimeofwriting,thereremainsscopetoimprovethesestochastic
discrepancies,notleastthroughtheuseofreduced-variancestochasticap-
proximationstothegradient.
6.3 OptimalWeightsforMCMC
At this point we have seen how computable discrepancies may be con-
structedandusedtopassivelyassesstheperformanceofMCMC.Nowwe
turn to how such discrepancies might be used to actively improve output
fromMCMC.Specifically,inthissectionweexplorehow,givenarealisa-
tion(x𝑘) 1≤𝑘≤𝑛ofaMarkovchainandatargetdistribution𝜋,wemayexploit
thekernelSteindiscrepancytoassignaweight𝑤 𝑘toeachx𝑘insuchaman-
ner that the discrepancy between the weighted empirical distribution and
𝜋 is minimised. This is loosely analogous to importance sampling (c.f.
Section 1.1.5), except here the analogue of the importance distribution is
thedistributionoftheMCMCsamplepathwhich,liketheposterioritself,
is implicitly defined. As such, the methods we will discuss were termed
Black Box Importance Sampling in Liu and Lee (2017). Surprisingly, we
willseethatsuchretrospectivere-weightingcanbeusedtoremovethebias
of approximate sampling algorithms, such as stochastic gradient MCMC
fromChapter3.
Letk𝜋 : R𝑑 ×R𝑑 → Rbeascalar-valuedreproducingkernelforwhich
∫ k𝜋(x,y) d𝜋(x) =0forally ∈ R𝑑;anexamplebeing(6.23).Theweights6.3 OptimalWeightsforMCMC 219
 
  
m=1,000
m=100
 
  
 
  
       
           
 L W H U D W L R Q
Figure6.4 StochasticgradientSteindiscrepancies.Stochastic
gradientLangevindynamicswasusedtogenerateapproximate
samplesfromtheposteriordistribution𝜋inthelogisticregression
examplefromSection3.5.1,andstochasticgradientStein
discrepancieswereusedtomeasurethediscrepancybetweenthe
empiricaldistribution𝜈
𝑛
oftheapproximatesamplesand𝜋.Here
𝑚indicatesthesizeofthedatasubsetsthatwereusedto
approximatethegradient.
thatweconsiderarethesolutionofthefollowingoptimisationproblem:
𝑤★
w★ :=(cid:169) (cid:173)
(cid:173)
(cid:171)
𝑤. . . ★1
𝑛
(cid:170) (cid:174)
(cid:174)
(cid:172)
∈ 𝑤𝑤a
11
+r ,.g
·. ·. ·,
+m
𝑤
𝑤𝑛i 𝑛n
≥ =0
1D
k𝜋
(cid:32) ∑︁ 𝑘𝑛 =1𝑤 𝑘𝛿 x𝑘(cid:33) (6.29)
Let𝜈 𝑛bethegeneralweightedempiricaldistributionappearingontheright-
handsideof(6.29).From(6.22)wehaveD k𝜋(𝜈 𝑛)2 = ⟨w,K𝜋w⟩,whereK𝜋is
the𝑛×𝑛matrixwithentries[K𝜋]
𝑖,𝑗
=k𝜋(x𝑖,x𝑗).IfthematrixK𝜋ispositive
definite then w★ is unique and, although not available in closed form, w★
can be computed by solving a linearly-constrained quadratic optimisation
problemoverthepositiveorthantofR𝑛.Theoptimallyweighteddistribution
willbedenoted𝜈★inthesequel.
𝑛
A natural first question is whether the weighted approximation to 𝜋,
)
ν(
n
πk
D
b220 AssessingandImprovingMCMC
obtained by retrospectively assigning weights to output from MCMC, is
consistent. This turns out to be true, under appropriate assumptions, and
moreover,optimalweightscanprovidebiascorrectioninsettingswherethe
Markov chain is not 𝜋-invariant. Indeed, the recent work of Riabiz et al.
(2022) established that, in the case of a 𝜇-invariant, time-homogeneous,
ergodicMarkovchain(X𝑘)
𝑘∈N
⊂ R𝑑,thentheexistenceofcertainmoments
oftheratio𝜋/𝜇canbeusedtodeducethat
(cid:32) 𝑛 (cid:33)
∑︁
D 𝑤★𝛿 →0
k𝜋 𝑘 X𝑘
𝑘=1
almost surely as 𝑛 → ∞. Importantly, the biased target 𝜇 of the Markov
chain(X𝑘) 𝑘∈NdoesnotneedtobeknowntoperformBlackBoxImportance
Samplingin(6.29).
Itcansometimesbeconvenienttorelaxthenon-negativityconstraint,to
consider
𝑤★
w (cid:101)★ :=(cid:169) (cid:173)
(cid:173)
(cid:171)
𝑤
(cid:101)(cid:101) . . . ★1
𝑛
(cid:170) (cid:174)
(cid:174)
(cid:172)
∈ 𝑤a 1+r wg
··∈
·+m
R
𝑤𝑛i 𝑛n =1D
k𝜋
(cid:32) ∑︁ 𝑘𝑛 =1𝑤 𝑘𝛿 x𝑘(cid:33) . (6.30)
The weights w★ may be negative, and thus the associated 𝜈˜★ is a signed
(cid:101) 𝑛
measureingeneral.Signedmeasuresmaynotposeaproblemifthegoalof
computationistoapproximateposteriorexpectationsofinterest,butifthe
goalistoapproximate𝜋itselfthenaproperprobabilitydistributionmaybe
preferred.Themainadvantageoftheformulationin(6.30)isthat,provided
K𝜋 ≻ 0,therelaxedoptimisationproblemhasauniqueandexplicitsolution
K−11
w★ = 𝜋 . (6.31)
(cid:101) 1⊤K−11
𝜋
This formulation is closely related to kernel cubature (also known as
Bayesianquadrature),andspecificallycoincideswiththenormalisedkernel
cubatureofKarvonenetal.(2018).From(6.31),wededucethatthecompu-
tationalcomplexityofobtainingoptimalweightsisingeneral𝑂(𝑛3).This
canprecludetheuseofoptimalweightsondesktopcomputationalhardware
when𝑛 islargerthanafewthousand.However,wewillseeinSection6.4
howaccuratesparseapproximationstotheoptimallyweighteddistribution
canbeefficientlyconstructed.
Todemonstratetheeffectofre-weighting,considerthefollowingRosen-
brocktarget
𝜋(𝑥,𝑦) ∝exp(−(𝑥−𝑎)2−𝑏(𝑦−𝑥2)2)6.3 OptimalWeightsforMCMC 221
where here we take 𝑎 = 0, 𝑏 = 3. The distribution 𝜋 might be referred to
as a horseshoe distribution, due to the curved shape of its level sets. To
represent output from a biased sampler, we generate sequence (X𝑘) 𝑘∈N of
independent samples from 𝜈 = N(0,I 2) and assign a weight 𝑤 𝑖 to each
stateX𝑖 accordingtoeither(6.29)or(6.30).Fortheseexperimentswetook
𝚺 = I , 𝛽 = 0.5, and the transformation in (6.28) was applied. Figure 6.5
2
displaysthequalitativepropertiesoftheweightsw★definedby(6.29)(left)
andtheweightsw★ definedby(6.30)(right).Inbothcases,itcanbeseen
(cid:101)
thatstatesX𝑘forwhichtheprobabilityunder𝜋islowaretypicallyassigned
a small weight. On the other hand, optimal weights are not independent,
and the over-representation of states in a local region due to Monte Carlo
samplingvariabilityispartiallymitigated.ComparisonofthekernelStein
discrepanciesD (𝜈★)and𝐷 (𝜈★)indicatesthatthenon-negativeweights
k𝜋 𝑛 k𝜋 (cid:101)𝑛
w★ perform nearly as well as the signed weights w★, while both sets of
(cid:101)
weightsleadtoasubstantialdecreaseinkernelSteindiscrepancycompared
totheuseof(inconsistent)uniformweightsinthisexperiment.
Of course, in this toy example, one has access to the sampling density
oftheX𝑘 andself-normalisedimportancesamplingcouldtriviallybeused.
Thatis,toeachsampleX𝑘weassignweightsproportionalto𝜋(X𝑘)/𝜈(X𝑘),
and we normalise these weights to sum to 1. The performance of self-
normalisedimportancesamplingisdisplayedinthelowerpanelofFigure
6.5,whereitisseentobeinferiortothediscrepancy-basedmethodswhich
we have discussed. Where has this performance gap come from? Well,
inadditiontobiascorrection,thediscrepancy-basedmethodsadditionally
perform variance reduction, in the sense that the random vectors w★ and
w★eachcontaincomponentsthatarestronglyinter-dependent.Thismeans
(cid:101)
thatifaregionisover-representedwithsamples,thentheoverallweightof
thesesamplescanbecollectivelyreducedtobetterapproximatethetarget
𝜋.Incontrast,self-normalisedimportancesamplinghastorelyonthelong-
run frequency of independent sampling to ensure that different regions of
the domain are assigned an equal amount of probability mass, and this
negativelyaffectsitsfinitesampleperformance.
Thesatisfactoryperformanceofoptimalweightsisobservedinsettings
suchasFigure6.5,wherepathologicalbehavioursofkernelSteindiscrep-
ancy (such as blindness to mixing proportions; see Section 6.2.4) are not
encountered. However, outside this setting the use of optimal weights can
fail.Further,ifthekernelSteindiscrepancyhasweakconvergencecontrol
butnotmomentcontrol,thenthereisnoguaranteethatmomentscomputed
usingtheweightedempiricalapproximations𝜈★or𝜈★willbeconvergentin
𝑛 (cid:101)𝑛222 AssessingandImprovingMCMC
w w <0
i i
w >0
fi
f
 
  
     ν=ν
n
ν=ν
n
  ν=ν
   n
 6 H O I  Q R U P D O L V H G  ,  6 
e
     
        
 Q X P E H U  R I  V D P S O H V
Figure6.5 OptimalweightsforMCMC.Samplesfroma
standardGaussiandistributionwereassignedeithertheoptimal
sign-constrainedweightsw★
(left)ortheoptimalunconstrained
weightsw★ (right),toobtainconsistentapproximations𝜈★ and𝜈★
(cid:101) 𝑛 (cid:101)𝑛
respectivelyofthehorseshoedistribution𝜋(shaded).These
approximationseachdemonstrateconvergenceinthesenseof
kernelSteindiscrepancyasthenumberofsamplesisincreased,
whilethedistribution𝜈oftheoriginalsamplesdoesnotprovidea
consistentapproximationof𝜋.
the𝑛 →∞limit.Theseremarksemphasisethatacertaindegreeofcaution
isneededwhenoptimalweightsareemployed.
)ν(
πk
D6.4 OptimalThinningforMCMC 223
6.4 OptimalThinningforMCMC
Theoutputfromasamplingalgorithmisoftenusedforsubsequentcompu-
tation,forexample,toapproximatetheposteriorexpectationofaquantityof
interest.Inscenarioswherethissubsequentcomputationincursanon-trivial
computationalcost,itisusuallydesirabletoworkwithassmallanumber𝑛
ofsamplesaspossible,providedthatthesecontinuetoprovideanaccurate
approximation to the posterior target. Standard practice for MCMC is to
retainthesubsetofstatesvisitedalongthesamplepathwhoseindicesare
(𝜎(𝑖)) 1≤𝑖≤𝑚,where𝜎(𝑖) = 𝑏+𝑐𝑖,𝑏isthedurationofaburn-inperiodand
𝑐isthethinningperiod.However,thisdoesnotdirectlyattempttoarriveat
acompressedrepresentationoftheposteriortarget.Theaimofthissection
is to discuss how one might select indices (𝜎(𝑖)) 1≤𝑖≤𝑚 to optimally ap-
proximatethetarget.Indoingso,wewillalsoarriveataconvenientsparse
approximation to the optimally weighted distributions studied in Section
6.3.
Given output (X𝑘) 1≤𝑘≤𝑛 from a 𝜋-invariant MCMC algorithm, we aim
toconstructanapproximation𝜈
𝑛,𝑚
= 𝑚1 (cid:205)𝑚 𝑘=1𝛿
X𝜎(𝑘)
to𝜋,whichwerequire
is sparse, meaning that 𝑚 ≪ 𝑛. For concreteness, we consider the setting
wheretheindexsequence𝜎 isgreedilydeterminedaccordingto
(cid:32) 𝑗−1 (cid:33)
1 1∑︁
𝜎(𝑗) ∈ argmin D 𝛿 + 𝛿
k𝜋 𝑗 X𝑘 𝑗 X𝜎(𝑗′)
𝑘=1,...,𝑛
𝑗′=1
foreach 𝑗 ∈ N.Using(6.22),andignoringtermsthatdonotdependonX𝑘,
thegreedyalgorithmisequivalentto
𝜎(𝑗) ∈ argmin
k𝜋(X𝑘,X𝑘) +∑︁𝑗−1
k𝜋(X𝑘,X𝜋(𝑗′))
𝑘=1,...,𝑛 2
𝑗′=1
where,intheeventofatie,itdoesnotmatterhowastateisselected.The
approximation 𝜈 𝑛,𝑚, under appropriate regularity conditions, converges to
𝜈★inthe𝑚 →∞limit;seeTheorem1ofRiabizetal.(2022).Thus,afinite
𝑛
runofthisgreedyalgorithmcouldinprinciplebeusedasasparsealternative
tooptimalweightingofstatesfromSection6.3.Further,thecomputational
complexity of this greedy algorithm is 𝑂(𝑛𝑚2), which would improve on
the𝑂(𝑛3) oftheoptimalweightsfromSection6.3when𝑚 ≪ 𝑛.Buthow
large should 𝑚 be for 𝜈 𝑛,𝑚 to be a sufficiently accurate approximation of
𝜈★tobeuseful?Thisquestionwasansweredwithatheoreticalargumentin
𝑛
Riabizetal.(2022),whoestablishedthat
D k𝜋(𝜈 𝑛,𝑚)−D k𝜋(𝜈★ 𝑛) →0224 AssessingandImprovingMCMC
almost surely as 𝑚,𝑛 → ∞, under conditions that include requiring 𝑚 to
increaseatleastasfastas(log𝑛)2/𝛽 forsome𝛽 ∈ (0,1).Thisisarelatively
mildconstrainton𝑚,andinthissensetherelativesizeof𝑚comparedto𝑛
canbesmall.
Toperformanempiricalcomparisonofoptimalweightingandthegreedy
algorithm just described, we return to the Rosenbrock example from Sec-
tion 6.3. Using the same MCMC output, we contrast the approximations
producedusingtheweightsw★withtheapproximationproducedusingthe
greedyalgorithmjustdescribed.Figure6.6demonstratestheconvergence
of the sparse approximation to the optimally weighted approximation as
𝑚 is increased. This convergence occurs reasonably quickly, suggesting
thatafaster,sparseapproximationmayoftenbepreferredcomparedtothe
weightedapproximationsfromSection6.3.Fortheseexperiments,wetook
𝚺 = I , 𝛽 = 0.5, and the transformation in (6.28) was applied. Although
2
inthistoyexample,samplingwasnotacomputationalbottleneck,inmore
challenging examples the use of these greedy algorithms can provide an
automaticmethodtobothidentifyandremoveaninitialburn-inperiod,and
tocompressthesampleroutput.
6.5 ChapterNotes
The development of sophisticated sampling algorithms, including those
described in this book, should be guided by a qualitative assessment of
theirempiricalperformanceoveravarietyofrealisticdistributionaltargets.
The purpose of this Chapter was to demonstrate how one can construct
explicit upper bounds on the “closeness” of the sampler output to the tar-
get.Inparticular,kernelSteindiscrepanciesemergedasacomputationally
convenient performance measure, which can be computed provided that
the gradient of the target log-density can be evaluated pointwise. Except
forscenarioswheretheposteriorcontainsdistanthigh-probabilityregions,
thekernelSteindiscrepancycanprovideanaccurateindicationofsampler
performance. Further, we described two different scenarios in which out-
putfromMCMCcanbeactivelyimprovedusingthesetechniques;optimal
weightingandoptimalthinningofMCMCoutput.
The literature on diagnostic checks for MCMC is almost as old as the
literatureonMCMC.OurbriefdiscussioninSection6.1barelyscratched
thesurfaceofthistopic,andwerefertheinterestedreadertomoredetailed
treatmentssuchasCowlesandCarlin(1996).Theconvergencediagnostics
we presented are due to Gelman and Rubin (1992); Brooks and Gelman
(1998);Gelmanetal.(2014).Thesomewhatarbitrarychoicesof𝛿 =0.1and6.5 ChapterNotes 225
    ν=ν
n,m
 0 D U N R Y  & K D L Q
ν=ν
n
 6 H O H F W H G  6 W D W H V
ν=ν
n
 
  
       
           
m
Figure6.6 OptimalthinningforMCMC.Asubsetofsize𝑚was
selectedfromthesamplepath(X𝑘) 1≤𝑘≤𝑛 ofa𝜋-invariantMarkov
chaininsuchawaythatthekernelSteindiscrepancybetweenthe
associatedempiricaldistribution(circles)and𝜋(shaded)was
greedilyminimised.Theselectedstatesareshownontheleft
panel,whileontherightpanelthekernelSteindiscrepancyofthe
resultingempiricaldistribution𝜈 𝑛,𝑚isseentoconvergetothatof
theoptimallyweightedempiricaldistribution𝜈★
thatusesthefull
𝑛
Markovchainoutput.Here𝑛=103.
𝛿 =0.01wereused,respectively,inGelmanetal.(2014);VatsandKnudson
(2021) and Vehtari et al. (2021). Generalisations of these convergence
diagnostics to the case of a multivariate target, and other improvements,
canbefoundine.g.BrooksandGelman(1998);VatsandKnudson(2021);
Vehtarietal.(2021).
Theconstructionofcomputableconvergenceboundshasreceivedlimited
historicalattention,fromauthorsthatincludeMeynetal.(1994);Rosenthal
(1995);RobertsandTweedie(1999);JonesandHobert(2001).Theconver-
genceboundwepresentedinSection6.2wassomewhatnovel,inthesense
thatearlierworkhastendedtomotivateandderivesuchboundsasaconse-
quence of Stein’s method. Introduced in Stein (1972), this technique from
appliedprobabilityhasbeenextensivelyusedtostudyvariousinstancesof
approximationamongrandomvariables.However,thelastdecadehasseen
an explosion of research investigating the computational uses of Stein’s
method, sparked by the formalisation of the Stein discrepancy in Gorham
and Mackey (2015). A myriad of computational applications of Stein dis-
)ν(
πk
D226 AssessingandImprovingMCMC
crepancies have now been explored, and a recent overview is provided in
Anastasiouetal.(2023).
The use of reproducing kernels led us to a discrepancy that could be
explicitly computed. However, there are some technical challenges asso-
ciated with the use of the resulting kernel Stein discrepancies. First, the
computational complexity of evaluating the kernel Stein discrepancy be-
tween 𝜋 and a distribution 𝜈 𝑛 supported on 𝑛 discrete states is𝑂(𝑛2); c.f.
(6.24).However,thiscomplexitycaninfactbereducedtonear-linearusing
the random features approach developed in Huggins and Mackey (2018),
whose discussion was beyond the scope of this book. Second, one must
ensure that the required properties of the discrepancy hold in the relevant
appliedcontext.Ourdiscussionfocusedonweakconvergencecontrol,but
other relevant properties include separation, meaning that 𝐷 𝜋(𝜈) = 0 if
and only if 𝜋 = 𝜈, and convergence detection, meaning that 𝐷 𝜋(𝜈 𝑛) → 0
whenever𝜈 𝑛 convergesto𝜋 inanappropriatesensetobespecified.Adis-
crepancyforwhichbothconvergencecontrolandconvergencedetectionare
satisfiedmaybeusedtocompareandselectbetweencompetingsampling
algorithms, as investigated in Gorham and Mackey (2015, 2017). To this
end, a rigorous technical presentation of kernel Stein discrepancies and
theirtheoreticalpropertiescanbefoundinBarpetal.(2022).
The use of isotropic reproducing kernels can lead to a curse of dimen-
sion, meaning that differences between probability distributions become
more difficult to detect as the dimension 𝑑 of the state space is increased.
A generalisation that replaces the overdamped Langevin in (6.13) with a
moregeneralclassof𝜋-invariantdiffusionprocessesonR𝑑 wasstudiedin
Gorham et al. (2019), and in the case of kernel Stein discrepancy, this is
equivalenttotheuseofcertainnon-isotropicreproducingkernels,however,
the selection of a suitable diffusion to address the curse of dimension has
notbeenexplored.Inaconstructiveattempttoimprovetheperformanceof
Steindiscrepancyinthehigh-dimensionalcontext,Grathwohletal.(2020)
proposedtosubstitutethereproducingkernelHilbertspaceintheintegral
probabilitymetric(6.8)withthesetoftestfunctionsspannedbyanappro-
priatelydifferentiableparametricneuralnetwork.Suchanapproachtrades
the potentially better detection properties of the discrepancy with both a
lackoftheoreticalguaranteesandtheadditionalcomputationalcomplexity
involved in the adversarial training of a neural network. Further research
willberequiredtounderstandthistrade-offindetail.
Tolimitthescope,wediscussedonlyalgorithmsforoptimalweighting
andoptimalthinning,ineachcaseforprobabilitydistributionsdefinedon
R𝑑.OptimalweightingwasintroducedinLiuandLee(2017)anditscon-6.5 ChapterNotes 227
sistencywasfirstestablishedinHodgkinsonetal.(2020).Optimalthinning
was introduced and analysed in Riabiz et al. (2022), and mini-batching
strategieswereproposedandstudiedtofurtherreducethe𝑂(𝑛𝑚2) costin
Teymuretal.(2021).Bothalgorithmscanbegeneralisedtonon-Euclidean
domains X through the identification of a suitable Markov process on X
withanexplicitgeneratorL 𝜋;someMarkovprocessessuitablefordiscrete
domains X are described in e.g. Shi et al. (2022). In related work, Fisher
and Oates (2024) demonstrated how consistent approximation using opti-
mal weights and optimal thinning can even be achieved without access to
gradientsofthetarget,providedthatgradientsofasuitableapproximating
distributioncanbeobtained.References
Ahn, Sungjin, Korattikara, Anoop, Liu, Nathan, Rajan, Suju, and Welling, Max.
2015. Large-scale distributed Bayesian matrix factorization using stochastic gra-
dientMCMC. Pages9–18of:Proceedingsofthe21thACMSIGKDDinternational
conferenceonknowledgediscoveryanddatamining. ACM.
Aicher,Christopher,Ma,Yi-An,Foti,NicholasJ,andFox,EmilyB.2019. Stochastic
gradient MCMC for state space models. SIAM Journal on Mathematics of Data
Science,1(3),555–587.
Aicher, Christopher, Putcha, Srshti, Nemeth, Christopher, Fearnhead, Paul, and Fox,
Emily.2023.StochasticgradientMCMCfornonlinearstatespacemodels.Bayesian
Analysis,1(1),1–23.
Anastasiou,Andreas,Barp,Alessandro,Briol,Franc¸ois-Xavier,Ebner,Bruno,Gaunt,
Robert E, Ghaderinezhad, Fatemeh, Gorham, Jackson, Gretton, Arthur, Ley,
Christophe,Liu,Qiang,Mackey,Lester,Oates,ChrisJ.,Reinert,Gesine,andSwan,
Yvik.2023. Stein’smethodmeetscomputationalstatistics:Areviewofsomerecent
developments. StatisticalScience,38(1),120–139.
Andrieu, Christophe, Durmus, Alain, Nu¨sken, Nikolas, and Roussel, Julien. 2021.
HypocoercivityofpiecewisedeterministicMarkovprocess-MonteCarlo.TheAnnals
ofAppliedProbability,31(5),2478–2517.
Baker,Jack,Fearnhead,Paul,Fox,Emily,andNemeth,Christopher.2018. Large-Scale
StochasticSamplingfromtheProbabilitySimplex. Pages6721–6731of:Advances
inNeuralInformationProcessingSystems.
Baker,Jack,Fearnhead,Paul,Fox,EmilyB,andNemeth,Christopher.2019. Control
variatesforstochasticgradientMCMC. StatisticsandComputing,29(3),599–615.
Bardenet,Re´mi,Doucet,Arnaud,andHolmes,Chris.2014.TowardsscalingupMarkov
chainMonteCarlo:anadaptivesubsamplingapproach. Pages405–413of:Interna-
tionalConferenceonMachineLearning(ICML).
Barp,Alessandro,Simon-Gabriel,Carl-Johann,Girolami,Mark,andMackey,Lester.
2022. Targetedseparationandconvergencewithkerneldiscrepancies. In:NeurIPS
2022WorkshoponScore-BasedMethods.
Beck,Amir,andTeboulle,Marc.2003.Mirrordescentandnonlinearprojectedsubgra-
dientmethodsforconvexoptimization.OperationsResearchLetters,31(3),167–175.
Bernardo,Jose´M,andSmith,AdrianFM.2009.BayesianTheory.JohnWiley&Sons.
Besag,Julian.1994.Commentson”Representationsofknowledgeincomplexsystems”
byU.GrenanderandM.I.Miller. JournaloftheRoyalStatisticalSocietySeriesB,
56,591–592.
229230 References
Beskos,Alex,Roberts,Gareth,andStuart,Andrew.2009. Optimalscalingsforlocal
Metropolis-Hastings chains on non-product targets in high dimensions. Annals of
AppliedProbability,19(3),863–898.
Beskos,Alexandros,Pillai,Natesh,Roberts,Gareth,Sanz-Serna,Jesus-Maria,andStu-
art,Andrew.2013. OptimaltuningofthehybridMonteCarloalgorithm. Bernoulli,
19(5A),1501–1534.
Bierkens,Joris.2016. Non-reversibleMetropolis-Hastings. StatisticsandComputing,
26(6),1213–1228.
Bierkens,Joris,andDuncan,Andrew.2017. Limittheoremsforthezig-zagprocess.
AdvancesinAppliedProbability,49(3),791–825.
Bierkens, Joris, and Roberts, Gareth. 2017. A piecewise deterministic scaling limit
of lifted Metropolis–Hastings in the Curie–Weiss model. The Annals of Applied
Probability,27,846–882.
Bierkens,Joris,andVerduynLunel,SjoerdM.2022. Spectralanalysisofthezigzag
process. Pages827–860of:Annalesdel’InstitutHenriPoincare(B)Probabiliteset
statistiques,vol.58. InstitutHenriPoincare´.
Bierkens,Joris,Bouchard-Coˆte´,Alexandre,Doucet,Arnaud,Duncan,AndrewB,Fearn-
head,Paul,Lienart,Thibaut,Roberts,Gareth,andVollmer,SebastianJ.2018.Piece-
wisedeterministicMarkovprocessesforscalableMonteCarloonrestricteddomains.
Statistics&ProbabilityLetters,136,148–154.
Bierkens, Joris, Roberts, Gareth O, and Zitt, Pierre-Andre´. 2019a. Ergodicity of the
zigzagprocess. TheAnnalsofAppliedProbability,29(4),2266–2301.
Bierkens,Joris,Fearnhead,Paul,andRoberts,GarethO.2019b. TheZig-Zagprocess
andsuper-efficientsamplingforBayesiananalysisofbigdata. Annalsofstatistics,
47(3),1288–1320.
Bierkens,Joris,Grazzi,Sebastiano,Kamatani,Kengo,andRoberts,Gareth.2020. The
boomerangsampler.Pages908–918of:InternationalConferenceonMachineLearn-
ing. PMLR.
Bierkens, Joris, Kamatani, Kengo, and Roberts, Gareth O. 2022. High-dimensional
scalinglimitsofpiecewisedeterministicsamplingalgorithms.TheAnnalsofApplied
Probability,32(5),3361–3407.
Bierkens,Joris,Kamatani,Kengo,andRoberts,GarethO.2023a. ScalingofPiecewise
DeterministicMonteCarloforAnisotropicTargets.
Bierkens,Joris,Grazzi,Sebastiano,Meulen,Frankvander,andSchauer,Moritz.2023b.
StickyPDMPsamplersforsparseandlocalinferenceproblems. StatisticsandCom-
puting,33(1),8.
Blei,DavidM,Ng,AndrewY,andJordan,MichaelI.2003.Latentdirichletallocation.
JournalofmachineLearningresearch,3(Jan),993–1022.
Bou-Rabee,Nawaf,andSanz-Serna,Jesu´sMar´ıa.2017. RANDOMIZEDHAMILTO-
NIANMONTECARLO. TheAnnalsofAppliedProbability,27(4),2159–2194.
Bouchard-Coˆte´, Alexandre, Vollmer, Sebastian J, and Doucet, Arnaud. 2018. The
bouncyparticlesampler:Anonreversiblerejection-freeMarkovchainMonteCarlo
method. JournaloftheAmericanStatisticalAssociation,113(522),855–867.
Brooks,StephenP,andGelman,Andrew.1998. Generalmethodsformonitoringcon-
vergenceofiterativesimulations. Journalofcomputationalandgraphicalstatistics,
7(4),434–455.References 231
Brooks,Steve,Gelman,Andrew,Jones,Galin,andMeng,Xiao-Li.2011. Handbookof
MarkovchainMonteCarlo. CRCpress.
Brosse,Nicolas,Durmus,Alain,Moulines,E´ric,andPereyra,Marcelo.2017.Sampling
fromalog-concavedistributionwithcompactsupportwithproximalLangevinMonte
Carlo. Pages319–342of:Conferenceonlearningtheory. PMLR.
Brosse,Nicolas,Durmus,Alain,andMoulines,E´ric.2018.Thepromisesandpitfallsof
StochasticGradientLangevinDynamics. Pages8278–8288of:AdvancesinNeural
InformationProcessingSystems.
Bubeck, Se´bastien, Eldan, Ronen, and Lehec, Joseph. 2018. Sampling from a log-
concavedistributionwithProjectedLangevinMonteCarlo. Discrete&Computa-
tionalGeometry,59(4),757–783.
Cabezas,Alberto,Corenflos,Adrien,Lao,Junpeng,andLouf,Re´mi.2024. BlackJAX:
ComposableBayesianinferenceinJAX. arXivpreprintarXiv:2402.10797.
Caflisch,RusselE.1998.MonteCarloandQuasi-MonteCarloMethods.ActaNumerica,
7,1–49.
Carmeli,Claudio,DeVito,Ernesto,andToigo,Alessandro.2006.Vectorvaluedrepro-
ducingkernelHilbertspacesofintegrablefunctionsandMercertheorem. Analysis
andApplications,4(04),377–408.
Chatterji,Niladri,Flammarion,Nicolas,Ma,Yian,Bartlett,Peter,andJordan,Michael.
2018.OnthetheoryofvariancereductionforstochasticgradientMonteCarlo.Pages
764–773of:InternationalConferenceonMachineLearning. PMLR.
Chen,Fang,Lova´sz,La´szlo´,andPak,Igor.1999. LiftingMarkovchainstospeedup
mixing. Pages275–281of:Proceedingsofthethirty-firstannualACMsymposium
onTheoryofcomputing.
Chen,Tianqi,Fox,Emily,andGuestrin,Carlos.2014. StochasticgradientHamiltonian
MonteCarlo. Pages1683–1691of:InternationalConferenceonMachineLearning.
Chen,WilsonYe,Barp,Alessandro,Briol,Franc¸ois-Xavier,Gorham,Jackson,Girolami,
Mark,Mackey,Lester,andOates,Chris.2019.SteinpointMarkovchainMonteCarlo.
Pages1011–1021of:InternationalConferenceonMachineLearning. PMLR.
Chevallier,Augustin,Power,Sam,Wang,AndiQ,andFearnhead,Paul.2021. PDMP
MonteCarlomethodsforpiecewise-smoothdensities. arXiv:2111.05859.
Chevallier, Augustin, Fearnhead, Paul, and Sutton, Matthew. 2023. Reversible jump
PDMPsamplersforvariableselection. JournaloftheAmericanStatisticalAssocia-
tion,118(544),2915–2927.
Christensen,OleF.,Roberts,GarethO.,andRosenthal,JeffreyS.2005. ScalingLimits
for the Transient Phase of Local Metropolis-Hastings Algorithms. Journal of the
RoyalStatisticalSociety.SeriesB(StatisticalMethodology),67(2),253–268.
Chwialkowski,Kacper,Strathmann,Heiko,andGretton,Arthur.2016. Akerneltestof
goodnessoffit.Pages2606–2615of:Internationalconferenceonmachinelearning.
PMLR.
Conway,JohnB.2010. ACourseinFunctionalAnalysis.Secondedn. Springer.
Corbella,Alice,Spencer,SimonEF,andRoberts,GarethO.2022. AutomaticZig-Zag
samplinginpractice. StatisticsandComputing,32(6),107.
Coullon,Jeremie,andNemeth,Christopher.2022. SGMCMCJax:alightweightJAX
library for stochastic gradient Markov chain Monte Carlo algorithms. Journal of
OpenSourceSoftware,7(72),4113.232 References
Coullon, Jeremie, South, Leah, and Nemeth, Christopher. 2023. Efficient and gener-
alizabletuningstrategiesforstochasticgradientMCMC. StatisticsandComputing,
33(3),66.
Cowles,MaryKathryn,andCarlin,BradleyP.1996. MarkovchainMonteCarlocon-
vergence diagnostics: a comparative review. Journal of the American Statistical
Association,91(434),883–904.
Cox,John,IngersollJr,JonathanE,andRoss,StephenA.1985. ATheoryoftheTerm
StructureofInterestRates. Econometrica,53(2),385–408.
Creutz, Michael. 1988. Global Monte Carlo algorithms for many-fermion systems.
Phys.Rev.D,38(Aug),1228–1238.
Dalalyan, Arnak S, and Karagulyan, Avetik. 2019. User-friendly guarantees for the
Langevin Monte Carlo with inaccurate gradient. Stochastic Processes and their
Applications,129(12),5278–5311.
Davis,MarkHA.1984. Piecewise-deterministicMarkovprocesses:Ageneralclassof
non-diffusionstochasticmodels. JournaloftheRoyalStatisticalSociety:SeriesB
(Methodological),46(3),353–376.
Deligiannidis, George, Bouchard-Coˆte´, Alexandre, and Doucet, Arnaud. 2019. Ex-
ponential ergodicity of the bouncy particle sampler. The Annals of Statistics, 47,
1268–1287.
Deligiannidis,George,Paulin,Daniel,Bouchard-Coˆte´,Alexandre,andDoucet,Arnaud.
2021. RandomizedHamiltonianMonteCarloasscalinglimitofthebouncyparticle
sampleranddimension-freeconvergencerates. TheAnnalsofAppliedProbability,
31(6),2612–2662.
Diaconis,Persi,Holmes,Susan,andNeal,RadfordM.2000.Analysisofanonreversible
Markovchainsampler. AnnalsofAppliedProbability,10(3),726–752.
Doucet,Arnaud,Johansen,AdamM,etal.2009. Atutorialonparticlefilteringand
smoothing:Fifteenyearslater. Handbookofnonlinearfiltering,12(656-704),3.
Duane,Simon,Kennedy,A.D.,Pendleton,BrianJ.,andRoweth,Duncan.1987.Hybrid
MonteCarlo. PhysicsLettersB,195(2),216–222.
Dubey, Kumar Avinava, Reddi, Sashank J, Williamson, Sinead A, Poczos, Barnabas,
Smola,AlexanderJ,andXing,EricP.2016. Variancereductioninstochasticgra-
dient Langevin dynamics. Pages 1154–1162 of: Advances in Neural Information
ProcessingSystems.
Eberle,Andreas.2016. Reflectioncouplingsandcontractionratesfordiffusions. Prob-
abilitytheoryandrelatedfields,166(3),851–886.
Fearnhead,Paul,Bierkens,Joris,Pollock,Murray,Roberts,GarethO,etal.2018.Piece-
wisedeterministicMarkovprocessesforcontinuous-timeMonteCarlo. Statistical
Science,33(3),386–412.
Fisher, Matthew, and Oates, Chris J. 2024. Gradient-free kernel Stein discrepancy.
AdvancesinNeuralInformationProcessingSystems,36.
Gamerman,Dani,andLopes,HedibertF.2006. MarkovchainMonteCarlo:stochastic
simulationforBayesianinference. CRCpress.
Gelman,Andrew,andRubin,DonaldB.1992.Inferencefromiterativesimulationusing
multiplesequences. Statisticalscience,7(4),457–472.
Gelman, Andrew, Carlin, John B, Stern, Hal S, Dunson, David B, Vehtari, Aki, and
Rubin,DonaldB.2014. BayesianDataAnalysis. Vol.2. CRCpress.References 233
Geyer,CharlesJ.1992. PracticalMarkovchainMonteCarlo. StatisticalScience,7(4),
473–483.
Girolami,Mark,andCalderhead,Ben.2011. RiemannmanifoldLangevinandHamil-
tonian Monte Carlo methods. Journal of the Royal Statistical Society: Series B
(StatisticalMethodology),73(2),123–214.
Gong,Wenbo,Li,Yingzhen,andHerna´ndez-Lobato,Jose´Miguel.2020.SlicedKernel-
izedSteinDiscrepancy. In:InternationalConferenceonLearningRepresentations.
Gorham, Jackson, and Mackey, Lester. 2015. Measuring sample quality with Stein’s
method. Pages226–234of:AdvancesinNeuralInformationProcessingSystems.
Gorham,Jackson,andMackey,Lester.2017. Measuringsamplequalitywithkernels.
Pages1292–1301of:Proceedingsofthe34thInternationalConferenceonMachine
Learning. PMLR.
Gorham,Jackson,Duncan,AndrewB,Vollmer,SebastianJ,andMackey,Lester.2019.
Measuringsamplequalitywithdiffusions.TheAnnalsofAppliedProbability,29(5),
2884–2928.
Gorham,Jackson,Raj,Anant,andMackey,Lester.2020.StochasticSteindiscrepancies.
AdvancesinNeuralInformationProcessingSystems,33,17931–17942.
Grathwohl, Will, Wang, Kuan-Chieh, Jacobsen, Jo¨rn-Henrik, Duvenaud, David, and
Zemel, Richard. 2020. Learning the stein discrepancy for training and evaluating
energy-basedmodelswithoutsampling. Pages3732–3747of:InternationalConfer-
enceonMachineLearning. PMLR.
Green, Peter J, and Mira, Antonietta. 2001. Delayed rejection in reversible jump
Metropolis–Hastings. Biometrika,88(4),1035–1053.
Grenander,Ulf,andMiller,MichaelI.1994. Representationsofknowledgeincomplex
systems. JournaloftheRoyalStatisticalSociety:SeriesB(Methodological),56(4),
549–581.
Gustafson,Paul.1998.AguidedwalkMetropolisalgorithm.StatisticsandComputing,
8(4),357–364.
Hastings, W Keith. 1970. Monte Carlo sampling methods using Markov chains and
theirapplications. Biometrika,57,97–109.
Heidelberger, Philip, and Welch, Peter D. 1981. A Spectral Method for Confidence
IntervalGenerationandRunLengthControlinSimulations.Communicationsofthe
ACM,24(4),233–245.
Hodgkinson,Liam,Salomone,Robert,andRoosta,Fred.2020. ThereproducingStein
kernelapproachforpost-hoccorrectedsampling. arXivpreprintarXiv:2001.09266.
Hoffman, Matthew, Radul, Alexey, and Sountsov, Pavel. 2021. An Adaptive-MCMC
SchemeforSettingTrajectoryLengthsinHamiltonianMonteCarlo. Pages3907–
3915of:Banerjee,Arindam,andFukumizu,Kenji(eds),ProceedingsofThe24th
International Conference on Artificial Intelligence and Statistics. Proceedings of
MachineLearningResearch,vol.130. PMLR.
Hoffman,MatthewD,andGelman,Andrew.2014.TheNo-U-Turnsampler:adaptively
setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning
Research,15(1),1593–1623.
Horowitz,AlanM.1991.AgeneralizedguidedMonteCarloalgorithm.PhysicsLetters
B,268(2),247–252.
Hsieh,Ya-Ping,Kavis,Ali,Rolland,Paul,andCevher,Volkan.2018.MirroredLangevin
Dynamics.Pages2883–2892of:AdvancesinNeuralInformationProcessingSystems.234 References
Huggins, Jonathan, and Mackey, Lester. 2018. Random feature Stein discrepancies.
AdvancesinNeuralInformationProcessingSystems,31.
Huggins,Jonathan,andZou,James.2017. Quantifyingtheaccuracyofapproximate
diffusionsandMarkovchains.Pages382–391of:ArtificialIntelligenceandStatistics.
PMLR.
Johndrow, James E, Pillai, Natesh S, and Smith, Aaron. 2020. No free lunch for
approximateMCMC. arXiv:2010.12514.
Jones,GalinL,andHobert,JamesP.2001.Honestexplorationofintractableprobability
distributionsviaMarkovchainMonteCarlo. StatisticalScience,16(4),312–334.
Kamatani,Kengo.2020. RandomwalkMetropolisalgorithminhighdimensionwith
non-Gaussian target distributions. Stochastic Processes and their Applications,
130(1),297–327.
Kanagawa,Heishiro,Barp,Alessandro,Simon-Gabriel,Carl-Johann,Gretton,Arthur,
andMackey,Lester.2024. ControllingMomentswithKernelSteinDiscrepancies.
arXivpreprintarXiv:2211.05408v4.
Karvonen,Toni,Oates,ChrisJ,andSarkka,Simo.2018.ABayes-Sardcubaturemethod.
AdvancesinNeuralInformationProcessingSystems,31.
LeCam,Lucien.1986.Asymptoticmethodsinstatisticaldecisiontheory.Springerseries
instatistics. Springer.
Lewis,PAW,andShedler,GeraldS.1979. SimulationofnonhomogeneousPoisson
processesbythinning. NavalResearchLogisticsQuarterly,26(3),403–413.
Li,Wenzhe,Ahn,Sungjin,andWelling,Max.2016.ScalableMCMCformixedmember-
shipstochasticblockmodels.Pages723–731of:ArtificialIntelligenceandStatistics.
Lindvall,Torgny,andRogers,LCrisG.1986.Couplingofmultidimensionaldiffusions
byreflection. TheAnnalsofProbability,860–872.
Liu,Qiang,andLee,Jason.2017. Black-boximportancesampling. Pages952–961of:
ArtificialIntelligenceandStatistics. PMLR.
Liu, Qiang, Lee, Jason, and Jordan, Michael. 2016. A kernelized Stein discrepancy
forgoodness-of-fittests. Pages276–284of:InternationalConferenceonMachine
Learning. PMLR.
Livingstone,Samuel,andZanella,Giacomo.2022. TheBarkerProposal:Combining
RobustnessandEfficiencyinGradient-BasedMCMC.JournaloftheRoyalStatistical
SocietySeriesB:StatisticalMethodology,84(2),496–523.
Ludkin,M,andSherlock,C.2022.Hugandhop:adiscrete-time,nonreversibleMarkov
chainMonteCarloalgorithm. Biometrika,110(2),301–318.
L’Ecuyer,Pierre,andLemieux,Christiane.2002.Recentadvancesinrandomizedquasi-
MonteCarlomethods. In:Dror,Moshe,L’Ecuyer,Pierre,andSzidarovszky,Ferenc
(eds), Modeling Uncertainty: An Examination of Stochastic Theory, Methods, and
Applications. Springer.
Ma, Yi-An, Chen, Tianqi, and Fox, Emily. 2015. A complete recipe for stochastic
gradientMCMC. Pages2917–2925of:AdvancesinNeuralInformationProcessing
Systems.
Ma, Yi-An, Foti, Nicholas J, and Fox, Emily B. 2017. Stochastic gradient MCMC
methodsforhiddenMarkovmodels. Pages2265–2274of:InternationalConference
onMachineLearning. PMLR.References 235
Majka,MateuszB,Mijatovic´,Aleksandar,andSzpruch, Lukasz.2020.Non-asymptotic
boundsforsamplingalgorithmswithoutlog-concavity.TheAnnalsofAppliedProb-
ability,30(4),1534–1581.
Metropolis, Nicholas, Rosenbluth, Arianna W, Rosenbluth, Marshall N, Teller, Au-
gustaH,andTeller,Edward.1953. Equationofstatecalculationsbyfastcomputing
machines. Thejournalofchemicalphysics,21(6),1087–1092.
Meyn,SeanP,andTweedie,RichardL.2012. MarkovChainsandStochasticStability.
SpringerScience&BusinessMedia.
Meyn, Sean P, Tweedie, Robert L, et al. 1994. Computable bounds for geometric
convergenceratesofMarkovchains. TheAnnalsofAppliedProbability,4(4),981–
1011.
Michel, Manon, Kapfer, Sebastian C, and Krauth, Werner. 2014. Generalized event-
chainMonteCarlo:Constructingrejection-freeglobal-balancealgorithmsfromin-
finitesimalsteps. TheJournalofChemicalPhysics,140(5).
Michel, Manon, Durmus, Alain, and Se´ne´cal, Ste´phane. 2020. Forward event-chain
Monte Carlo: Fast sampling by randomness control in irreversible Markov chains.
JournalofComputationalandGraphicalStatistics,29(4),689–702.
Nagapetyan,Tigran,Duncan,AndrewB,Hasenclever,Leonard,Vollmer,SebastianJ,
Szpruch, Lukasz, and Zygalakis, Konstantinos. 2017. The true cost of stochastic
gradientLangevindynamics. arXiv:1706.02692.
Neal,RadfordM.2003. Slicesampling. TheAnnalsofStatistics,31(3),705–767.
Neal,RadfordM.2004. ImprovingasymptoticvarianceofMCMCestimators:Non-
reversiblechainsarebetter. arXivpreprintmath/0407281.
Neal, Radford M. 2011. MCMC using Hamiltonian dynamics. Pages 113–162 of:
Brooks,Steve,Gelman,Andrew,Jones,GalinL,andMeng,Xiao-Li(eds),Handbook
ofMarkovchainMonteCarlo. CRCPress.
Nemeth, Christopher, and Fearnhead, Paul. 2021. Stochastic gradient markov chain
montecarlo. JournaloftheAmericanStatisticalAssociation,116(533),433–450.
Nemeth,Christopher,andSherlock,Chris.2018.MergingMCMCsubposteriorsthrough
Gaussian-processapproximations. BayesianAnalysis,13(2),507–530.
Nemeth, Christopher, Fearnhead, Paul, and Mihaylova, Lyudmila. 2016. Particle ap-
proximationsofthescoreandobservedinformationmatrixforparameterestimation
instate–spacemodelswithlinearcomputationalcost.JournalofComputationaland
GraphicalStatistics,25(4),1138–1157.
Norris,JamesR.1998. MarkovChains. CambridgeUniversityPress.
Oates, Chris J, Girolami, Mark, and Chopin, Nicolas. 2017. Control functionals for
MonteCarlointegration.JournaloftheRoyalStatisticalSociety:SeriesB(Statistical
Methodology),79(3),695–718.
Oksendal,Bernt.2013. StochasticDifferentialEquations:AnIntroductionwithAppli-
cations. SpringerScience&BusinessMedia.
Pagani,Filippo,Chevallier,Augustin,Power,Sam,House,Thomas,andCotter,Simon.
2020. NuZZ:numericalZig-Zagsamplingforgeneralmodels. arXiv:2003.03636.
Patterson,Sam,andTeh,YeeWhye.2013. StochasticgradientRiemannianLangevin
dynamics on the probability simplex. Pages 3102–3110 of: Advances in Neural
InformationProcessingSystems.
Peters, Elias A J F, and de With, G. 2012. Rejection-free Monte Carlo sampling for
generalpotentials. PhysicalReviewE,85(2),026703.236 References
Phillips,DavidB,andSmith,AdrianFM.1996. Bayesianmodelcomparisonviajump
diffusions.Pages215–240of:Gilks,WallyR,Richardson,Sylvia,andSpiegelhalter,
David(eds),MarkovchainMonteCarloinpractice. Chapman&Hall,CRC.
Pollock,Murray,Fearnhead,Paul,Johansen,AdamM,andRoberts,GarethO.2020.
Quasi-stationaryMonteCarloandtheScaLEalgorithm.JournaloftheRoyalStatis-
ticalSocietySeriesB:StatisticalMethodology,82(5),1167–1221.
Press,WilliamH,Teukolsky,SaulA,Vetterling,WilliamT,andFlannery,BrianP.2007.
NumericalrecipesinC++:Theartofscientificcomputing. CambridgeUniversity
Press.
Putcha,Srshti,Nemeth,Christopher,andFearnhead,Paul.2023. PreferentialSubsam-
plingforStochasticGradientLangevinDynamics.Pages8837–8856of:International
ConferenceonArtificialIntelligenceandStatistics. PMLR.
Raginsky,Maxim,Rakhlin,Alexander,andTelgarsky,Matus.2017.Non-convexlearn-
ing via stochastic gradient langevin dynamics: a nonasymptotic analysis. Pages
1674–1703of:ConferenceonLearningTheory. PMLR.
Rasmussen,CarlEdward,andWilliams,ChristopherK.I.2005. GaussianProcesses
forMachineLearning. TheMITPress.
Riabiz,Marina,Chen,WilsonYe,Cockayne,Jon,Swietach,Pawel,Niederer,StevenA,
Mackey, Lester, and Oates, Chris J. 2022. Optimal thinning of MCMC output.
Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4),
1059–1081.
Riou-Durand,Lionel,andVogrinc,Jure.2023.MetropolisAdjustedLangevinTrajecto-
ries:arobustalternativetoHamiltonianMonteCarlo.
Ripley,BrianD.2009. StochasticSimulation. JohnWiley&Sons.
Robbins,Herbert,andMonro,Sutton.1951. Astochasticapproximationmethod. The
annalsofmathematicalstatistics,400–407.
Robert,ChristianP.2007. TheBayesianChoice:fromDecision-TheoreticFoundations
toComputationalImplementation. Springer.
Robert, Christian P, and Casella, George. 1999. Monte Carlo Statistical Methods.
Springer.
Roberts,GarethO,andRosenthal,JeffreyS.1998. Optimalscalingofdiscreteapprox-
imationstoLangevindiffusions. JournaloftheRoyalStatisticalSociety:SeriesB
(StatisticalMethodology),60(1),255–268.
Roberts, Gareth O, and Rosenthal, Jeffrey S. 2001. Optimal scaling for various
Metropolis-Hastingsalgorithms. Statisticalscience,16(4),351–367.
Roberts,GarethO,andRosenthal,JeffreyS.2004. GeneralstatespaceMarkovchains
andMCMCalgorithms. ProbabilitySurveys,1,20–71.
Roberts,GarethO,andTweedie,RichardL.1996.ExponentialconvergenceofLangevin
distributionsandtheirdiscreteapproximations. Bernoulli,2(4),341–363.
Roberts,GarethO,andTweedie,RichardL.1999. Boundsonregenerationtimesand
convergenceratesforMarkovchains. StochasticProcessesandTheirApplications,
80(2),211–229.
Roberts,GarethO.,Gelman,Andrew,andGilks,WalterR.1997. Weakconvergence
andoptimalscalingofrandomwalkMetropolisalgorithms. TheAnnalsofApplied
Probability,7,110–120.
Rogers,LeonardCG,andWilliams,David.2000a. Diffusions,MarkovProcesses,and
Martingales:Volume1,Foundations. CambridgeUniversityPress.References 237
Rogers,LeonardCG,andWilliams,David.2000b. Diffusions,MarkovProcesses,and
Martingales:Volume2,ItoCalculus. CambridgeUniversityPress.
Rosenthal,JeffreyS.1995. MinorizationconditionsandconvergenceratesforMarkov
chainMonteCarlo. JournaloftheAmericanStatisticalAssociation,90(430),558–
566.
Rubinstein,RY,andKroese,DP.2008.SimulationandtheMonteCarloMethod.John
Wiley&Sons.
Scott,StevenL,Blocker,AlexanderW,Bonassi,FernandoV,Chipman,HughA,George,
EdwardI,andMcCulloch,RobertE.2016.Bayesandbigdata:TheconsensusMonte
Carlo algorithm. International Journal of Management Science and Engineering
Management,11(2),78–88.
Sherlock,C.,Thiery,A.H.,Roberts,G.O.,andRosenthal,J.R.2015.Ontheefficiency
ofpseudo-marginalrandomwalkMetropolisalgorithms. AnnalsofStatistics,43(1),
238–275.
Sherlock, Chris, and Roberts, Gareth. 2009. Optimal scaling of the random walk
Metropolisonellipticallysymmetricunimodaltargets. Bernoulli,15(3),774–798.
Sherlock,Chris,andThiery,AlexandreH.2022. Adiscretebouncyparticlesampler.
Biometrika,109(2),335–349.
Sherlock,Chris,Urbas,Szymon,andLudkin,Matthew.2023. Theapogeetoapogee
pathsampler.JournalofComputationalandGraphicalStatistics,32(4),1436–1446.
Shi,Jiaxin,Zhou,Yuhao,Hwang,Jessica,Titsias,Michalis,andMackey,Lester.2022.
GradientestimationwithdiscreteSteinoperators. Advancesinneuralinformation
processingsystems,35,25829–25841.
Sohl-Dickstein,Jascha,Mudigonda,Mayur,andDeWeese,Michael.2014.Hamiltonian
MonteCarlowithoutdetailedbalance. Pages719–726of:InternationalConference
onMachineLearning. PMLR.
Stein,Charles.1972. Aboundfortheerrorinthenormalapproximationtothedistri-
butionofasumofdependentrandomvariables. Pages583–603of:Proceedingsof
the6thBerkeleySymposiumonMathematicalStatisticsandProbability,Volume2:
ProbabilityTheory,vol.6. UniversityofCaliforniaPress.
Stephens,Matthew.2000.Bayesiananalysisofmixturemodelswithanunknownnumber
ofcomponents-analternativetoreversiblejumpmethods.AnnalsofStatistics,40–74.
Sun, Hongwei. 2005. Mercer theorem for RKHS on noncompact sets. Journal of
Complexity,21(3),337–349.
Sun,Yi,Schmidhuber,Ju¨rgen,andGomez,Faustino.2010. Improvingtheasymptotic
performanceofMarkovchainMonte-Carlobyinsertingvortices. Pages2235–2243
of:Lafferty,J.,Williams,C.K.I.,Shawe-Taylor,J.,Zemel,R.S.,andCulotta,A.(eds),
AdvancesinNeuralInformationProcessingSystems,vol.23.
Sutton,Matthew,andFearnhead,Paul.2023. Concave-convexPDMP-basedsampling.
JournalofComputationalandGraphicalStatistics,32(4),1425–1435.
Suwa,Hidemaro,andTodo,Synge.2010. MarkovchainMonteCarlomethodwithout
detailedbalance. PhysicalReviewLetters,105(12),120603.
Teh, Yee Whye, Thiery, Alexandre H, and Vollmer, Sebastian J. 2016. Consistency
andfluctuationsforstochasticgradientLangevindynamics.TheJournalofMachine
LearningResearch,17(1),193–225.238 References
Teymur, Onur, Gorham, Jackson, Riabiz, Marina, and Oates, Chris. 2021. Optimal
quantisationofprobabilitymeasuresusingmaximummeandiscrepancy.Pages1027–
1035of:InternationalConferenceonArtificialIntelligenceandStatistics. PMLR.
Turitsyn, Konstantin S, Chertkov, Michael, and Vucelja, Marija. 2011. Irreversible
MonteCarloalgorithmsforefficientsampling. PhysicaD:NonlinearPhenomena,
240(4-5),410–414.
Vanetti,Paul,Bouchard-Coˆte´,Alexandre,Deligiannidis,George,andDoucet,Arnaud.
2017. Piecewise-deterministicMarkovchainMonteCarlo. arXiv:1707.05296.
Vats,Dootika,andKnudson,Christina.2021.RevisitingtheGelman–Rubindiagnostic.
StatisticalScience,36(4),518–529.
Vehtari,Aki,Gelman,Andrew,Simpson,Daniel,Carpenter,Bob,andBu¨rkner,Paul-
Christian.2021. Rank-normalization,folding,andlocalization:Animproved 𝑅ˆ for
assessingconvergenceofMCMC. BayesianAnalysis,1(1),1–28.
von Renesse, Max-K, and Sturm, Karl-Theodor. 2005. Transport inequalities, gradi-
ent estimates, entropy and Ricci curvature. Communications on pure and applied
mathematics,58(7),923–940.
Vyner, Callum, Nemeth, Christopher, and Sherlock, Chris. 2023. SwISS: A scalable
MarkovchainMonteCarlodivide-and-conquerstrategy. Stat,12(1),e523.
Welling,Max,andTeh,YeeW.2011.BayesianlearningviastochasticgradientLangevin
dynamics. Pages681–688of:Proceedingsofthe28thInternationalConferenceon
MachineLearning(ICML-11).
Wenliang, Li K, and Kanagawa, Heishiro. 2021. Blindness of score-based methods
to isolated components and mixing proportions. In: Proceedings of the NeurIPS
Workshop“YourModelisWrong:RobustnessandMisspecificationinProbabilistic
Modeling”’.
Wu, Changye, and Robert, Christian P. 2017. Generalized bouncy particle sampler.
arXiv:1706.04781.
Wu, Changye, and Robert, Christian P. 2020. Coordinate sampler: a non-reversible
Gibbs-likeMCMCsampler. StatisticsandComputing,30(3),721–730.
Xifara,T.,Sherlock,C.,Livingstone,S.,Byrne,S.,andGirolami,M.2014. Langevin
diffusionsandtheMetropolis-adjustedLangevinalgorithm. Statistics&Probability
Letters,91,14–19.