Jigsaw Game: Federated Clustering
Jinxuan Xu jinxuan.xu@rutgers.edu
Department of Electrical and Computer Engineering
Rutgers University
Hong-You Chen chen.9301@osu.edu
Department of Computer Science and Engineering
The Ohio State University
Wei-Lun Chao chao.209@osu.edu
Department of Computer Science and Engineering
The Ohio State University
Yuqian Zhang yqz.zhang@rutgers.edu
Department of Electrical and Computer Engineering
Rutgers University
Abstract
Federated learning has recently garnered significant attention, especially within the domain
of supervised learning. However, despite the abundance of unlabeled data on end-users,
unsupervised learning problems such as clustering in the federated setting remain under-
explored. In this paper, we investigate the federated clustering problem, with a focus on
federated k-means. We outline the challenge posed by its non-convex objective and data
heterogeneity in the federated framework. To tackle these challenges, we adopt a new per-
spective by studying the structures of local solutions in k-means and propose a one-shot
algorithm called FeCA (Federated Centroid Aggregation). FeCA adaptively refines local
solutions on clients, then aggregates these refined solutions to recover the global solution of
the entire dataset in a single round. We empirically demonstrate the robustness of FeCA
under various federated scenarios on both synthetic and real-world data. Additionally, we
extend FeCA to representation learning and present DeepFeCA, which combines Deep-
Cluster and FeCA for unsupervised feature learning in the federated setting.
1 Introduction
Federatedlearning(FL)hasemergedasapromisingframework,enablingmodeltrainingacrossdecentralized
data. This approach addresses data privacy concerns by allowing data to remain on individual clients. The
goal of FL is to collaboratively train a model across multiple clients without directly sharing data. Within
this context, FedAvg McMahan et al. (2017) has been considered the standard approach in FL, designed to
obtain a centralized model by averaging the models trained independently on each client’s data.
Although FL has seen widespread applications in the domain of supervised learning, particularly in tasks
like classification (Oh et al., 2021; Jiménez-Sánchez et al., 2023), its utilization in the unsupervised learning
sphere is still largely unexplored, even though it holds significant potential and applicability in numerous
practical situations. A notable example is the large collections of unlabeled photographs owned by most
smartphone users. In such instances, federated unsupervised learning can be a powerful paradigm, enabling
theuseofunsupervisedlearningapproachestoleveragethe“collectivewisdom”oftheseunlabeleddatawhile
safeguarding user privacy.
Inthispaper,weinvestigatefederatedunsupervisedlearning,particularlyfocusingonthepopularclustering
problem of k-means. In prior studies, clustering methods have been applied in FL mainly focusing on prob-
1
4202
luJ
71
]GL.sc[
1v46721.7042:viXralemssuchasclientselection(Ghoshetal.,2020;Longetal.,2023)andprivacyenhancement(Lietal.,2022;
Elhussein & Gürsoy, 2023), without a deep investigation into the unsupervised learning aspect. Moreover,
existingdistributedclusteringmethodsoverlooktheuniquechallengesinFL,suchasdataheterogeneityand
communicationefficiency,makingitdifficulttoapplyinthefederatedsetting. Ourstudyextendstofederated
clustering, incorporating unsupervised clustering on individual clients within a federated framework.
One key challenge of federated clustering is the inherent non-convexity of clustering problems, presenting
multiple equivalent global solutions and potentially even more local solutions. Standard algorithms like
Lloyd’s algorithm Lloyd (1982) can only find a local solution of the k-means problem, without guaranteeing
global optimum. We note that the term “local solution” in this context refers to a local optimal in optimiza-
tion, not the solution learned from a client1. This challenge is amplified in the federated setting, where each
client’sdataisadistinctsubsetoftheentiredataset. EvenundertheIIDdatasamplescenario,eachclient’s
clustering results might be suboptimal local solutions containing spurious centroids far from the true global
centroids. And this issue could become even more pronounced under non-IID scenarios.
To this end, we propose a one-shot federated k-means algorithm: Federated Centroid Aggregation (FeCA),
offeringanewapproachbyexploitingstructuredlocalsolutions. Inthek-meansproblem,localsolutionscarry
valuable information from the global solution. The proposed algorithm resolves these local solutions and
leveragestheirbenignpropertieswithinthefederatedclusteringframework. FeCAisbuiltupontheoretical
studies (Qian et al., 2021; Chen et al., 2024) derived in a centralized setting, which suggests that every local
solution is structured and contains nontrivial information about the global solution. Specifically, a local
solution consists of estimates of the k ground truth centers, with a subset of these estimates being accurate.
One common concern of FL lies in the potential decrease in performance compared to centralized models
due to data heterogeneity across clients. However, from the perspective of local solutions, federated clus-
tering could benefit from the decentralized framework. Each client’s solution, whether a local optimum or
not, carries partial information about the global solution of the entire dataset. By incorporating multiple
clients’solutions, thecentralservercouldpotentiallyrecovertheglobaloptimalsolutioninoneshot, akinto
assembling a jigsaw puzzle of clients’ solutions. For instance, if a true centroid is missing from one client’s
solution, it might be identified in the solutions of other clients.
Therefore, FeCA is designed to recover the global solution for k-means clustering in a federated setting by
refining and aggregating solutions from clients. First, Lloyd’s algorithm for k-means is performed on each
client’s data. Then, FeCA adaptively refines spurious centroids using their structural properties to obtain
a set of refined centroids for each client. Then refined centroids are sent to the central server, where FeCA
aggregates them to recover the global solution of the entire dataset. By exploiting the structure in local
solutions, FeCA is able to accurately identify the true k centroids of the entire dataset in one shot.
We further extend FeCA beyond a pre-defined feature space to the modern deep feature framework (Liu
et al., 2021). Specifically, we present DeepFeCA, a federated representation learning algorithm from de-
centralized unlabeled data. Concretely, we pair FeCA with clustering-based deep representation learning
models such as DeepCluster Caron et al. (2018; 2020), which assign pseudo-labels according to k-means
clusteringand then train theneural network in a supervised manner. Theresulting algorithm, DeepFeCA,
alternates between applying FeCA to the current features and using DeepCluster for further training.
Thisiterativeprocessenhancesthemodel’sabilitytolearnmeaningfulrepresentationsfromthedecentralized
data.
WeevaluatebothFeCAandDeepFeCAonbenchmarkdatasets,includingS-setsFränti&Sieranoja(2018),
CIFAR Krizhevsky et al. (2009), and Tiny-ImageNet Le & Yang (2015). FeCA consistently outperforms
baselines in various federated settings, demonstrating its effectiveness in recovering the global solution.
Furthermore, DeepFeCA shows promising performance in federated representation learning.
1Forclarity,throughoutthispaper,weusetheclient’ssolutionfortheresultobtainedfromaclient. Ifthesolutionhappens
tobealocalsolution,wenameittheclient’slocalsolution.
22 Related Work
Federated learning. Mainstream FL algorithms (McMahan et al., 2017; Khaled et al., 2020; Haddadpour
& Mahdavi, 2019) adopt coordinate-wise averaging of the weights from clients. However, given the limited
performance of direct averaging, other approaches have been proposed: Yurochkin et al. (2019); Wang et al.
(2020); Zhang et al. (2023b); Tan et al. (2023) identify the permutation symmetry in neural networks and
then aggregate after the adaptive permutation; Lin et al. (2020); He et al. (2020); Zhou et al. (2020); Chen
& Chao (2021); Zeng et al. (2023) replace weight average by model ensemble and distillation. These studies
enhancetheperformanceofthesynchronizationschemebutoverlooktheimpactoflocalsolutionsonclients.
Federated Clustering. Many distributed clustering methods (Balcan et al., 2013; Bachem et al., 2018;
Kargupta et al., 2001; Januzaj et al., 2004; Hess et al., 2022) have been proposed, but they overlook the
heterogeneous challenge in FL. For synchronizing results returned from different clustering solutions, con-
sensus clustering has been studied widely (Monti et al., 2003; Goder & Filkov, 2008; Li et al., 2021). But it
works on the same dataset, unlike FL. In the context of FL, Qiao et al. (2021); Li et al. (2023); Xia et al.
(2020); Lu et al. (2023); Li et al. (2022) focus on communication efficiency or privacy-preserving. A recent
federated clustering study Stallmann & Wilbik (2022) proposes weighted averaging for Fuzzy c-means but
requiresmultiplerounds. Thestudymostrelevanttooursintroducesk-FEDDennisetal.(2021),aone-shot
federated clustering algorithm, under a rather strong assumption that each client only has data from a few
true clusters. It is still underexplored for federated clustering and usage of local solutions.
Federated representation learning. FedRep Collins et al. (2021) studies supervised representation
learning by alternating updates between classifiers and feature extractors. Jeong et al. (2020); Zhang et al.
(2020)studyfederatedsemi-supervisedlearningwiththeserverholdingsomelabeleddataandclientshaving
unlabeled data. For federated unsupervised learning, Zhuang et al. (2021) proposes self-supervised learning
innon-IIDsettingswithadivergence-awareupdatestrategyformitigatingnon-IIDchallenges, distinctfrom
our clustering focus. Zhang et al. (2023a) adopts the contrastive approach for model training on clients. A
recentframeworkLubanaetal.(2022)introducesfederatedunsupervisedlearningwithconstrainedclustering
for representation learning, while our focus lies on exploring federated clustering via local solutions.
3 Background
Clustering. Given a d-dimensional dataset D = {x ∈ Rd,...,x ∈ Rd}, the goal of k-means problem is
1 N
to identify k centroids C ={c ∈Rd,...,c ∈Rd} that minimize the following objective
1 k
N
. X
G(C)= min∥x −c ∥2. (1)
n j 2
j∈[k]
n=1
Federated clustering. In the federated setting, the dataset D is decentralized across M clients. Each
client m∈[M] possesses a distinct subset D of the entire dataset D. Despite different data configurations,
m
the goal of federated clustering remains the same – to identify k centroids C ={c ,...,c } for D =∪ D .
1 k m m
Under this federated framework, the optimization problem in Equation 1 can be reformulated as
M
X
min G(C)= G (C), (2)
m
C
m=1
where G is the k-means objective computed on D . Due to privacy concerns that restrict direct data
m m
sharing among clients, the optimization problem described in Equation 2 cannot be solved directly. Thus,
theproposedalgorithmFeCAutilizesacollaborativeapproachbetweenclientsandacentralserver. Initially,
each client m independently minimizes G (C) to obtain a set of k centroids C(m) = {c(m),...,c(m)} from
m 1 k
their dataset D . Then the server aggregates centroids ∪ C(m) to find a set of k centroids C for D.
m m
We note that when clients perform standard Lloyd’s algorithm for k-means clustering, they usually end up
with local solutions C(m), resulting in suboptimal performance even with IID distributed data D . These
m
local solutions can significantly complicate the aggregation process on the central server. Thus, the key
3Figure 1: Clustering results. (Left): global and local solutions on centralized/IID client’s data; (Right):
global solutions for non-IID client’s data sharing similar structures.
challenge in federated clustering lies in effectively resolving the client’s local solutions and appropriately
aggregating them on the central server.
3.1 Structure of Local Solutions
To better resolve the federated clustering problem, we propose to take a deeper look at local solutions in
k-means, which often significantly differ from the global minimizer. Recent theoretical works by Qian et al.
(2021); Chen et al. (2024) have established a positive result that under certain separation conditions, all
the local solutions share a common geometric structure. More formally, suppose a local solution identifies
centroids {c ,...,c }. Then, there exists a one-to-one association between these centroids and the true
1 k
centers {c∗,...,c∗} from the global solution. This association ensures that each centroid c belongs to
1 k i
exactly one of the following cases with overwhelming probability2:
• Case1(one-fit-manyassociation): centroidc isassociatedwiths(s>1)truecenters{c∗ ,...,c∗ }.
i j1 js
• Case 2 (one/many-fit-one association): t (t≥1) centroids {c ,...,c } are all associated with one
i1 it
true center c∗.
j
Namely, a centroid c in a local solution is either a one-fit-many centroid that is located in the middle of
i
multipletruecenters(case1, whens>1), oraone/many-fit-one centroidthatisclosetoatruecenter(case
2). Notably, when c is the only centroid near a true center (case 2, when t=1), it is considered a correctly
i
identified centroid that closely approximates a true center. An illustration is provided in Figure 1. Next, we
will introduce how our algorithm utilizes such local solution structures to obtain unified clustering results in
the federated setting.
4 Jigsaw Game – FeCA
Algorithm 1 Federated Centroid Aggregation (FeCA)
1: input cluster number k
2: for each client m=1,...,M do
3: C(m),D(m) ← ClientUpdate (k)
4: R(m) ← RadiusAssign (C(m),D(m))
5: end for
6: C ←SM C(m)
m=1
7: R←SM R(m)
m=1
8: C∗ ← ServerAggregation (k,C,R)
9: return C∗
The proposed federated clustering algorithm FeCA is built upon the collaboration between clients and a
centralserver. EachclientmsharesitsrefinedcentroidsolutionC(m) withtheserver,whereeachC(m) carries
partial information of the global solution, similar to pieces of a jigsaw puzzle. The server then aggregates
2Suchstructureoflocalsolutionsholdsevenwhenk̸=k∗,wherek∗ isthenumberoftrueclustersinthedataset.
4Client 1 Client 1 Client 1
Client M Client M Client M
Centralizeddataset Server aggregation
Clustering results Removingone-fit-many Sending to the server
Figure 2: FeCA roadmap. 1st column: The centralized dataset distributed to clients. 2nd column: The
k-meansclusteringresultsondifferentclientsundernon-IIDdatasamplescenario,whereblacktrianglesand
squares represent centroids. 3rd column: Eliminating one-fit-many centroids in Algorithm 2, indicated by
hollowsquaresandtriangles. 4thcolumn: Centroidssenttotheserver. 5thcolumn: Aggregationofreceived
centroids on the server where red crosses represent recovered centroids.
received centroids ∪ C(m) to obtain a unified complete solution C∗, akin to assembling puzzle pieces in a
m
jigsaw game. FeCA only requires one communication between clients and the server, thanks to its adaptive
refinement of local solutions on the client side. The detailed procedure is presented in Algorithm 1 and
illustrated in Figure 2.
Privacy concern. While privacy is crucial in FL, it is not our main focus. However, an advantage of
our one-shot algorithm is its minimal information exchange compared to standard iterative approaches like
distributed clustering. In FeCA, sending refined centroids to the server is viewed as no more privacy risk
thanmainstreamFLapproachesofsendingmodelswithclassifiers,whichmoreorlessconveyclassorcluster
information.
4.1 Client Update Algorithm
Algorithm 2 FeCA- ClientUpdate
1: input cluster number k
2: Apply Lloyd’s algorithm on the data of client m to obtain k clusters D(m) = {D(m),...,D(m)} and
1 k
corresponding k centroids C(m) ={c(m),...,c(m)}.
1 k
3: while C(m) ̸=⊘ do
4:
Detectone-fit-manycentroidc(m) whoseclusterD(m) hasthelargeststandarddeviationandcalculate
i i
the objective value G(m) of the cluster D(m).
i i
5:
Detectmany-fit-onecentroidsc( pm),c( qm) ofclustersD p(m) andD q(m) withthesmallestpairwisedistance.
6: Merge D p(m) and D q(m) into one cluster D j(m) to obtain a new centroid c( jm) and calculate the objective
value G(m) of the merged cluster D(m).
j j
7: if G(m) ≥G(m) then
i j
8: C(m) ←C(m)\{c(m)}
i
9: D(m) ←D(m)\{D(m)}
i
10: else
Terminate the loop.
11:
12: end if
13: end while
14: return centroids C(m), clusters D(m)
5
… … …This step aims to refine the spurious local solutions of k-means clustering on clients. Each client m first
performsstandardLloyd’salgorithmtoobtainasetofk centroidsC(m) ={c(m),...,c(m)}, andthissolution
1 k
is only guaranteed to be a local solution. As discussed in subsection 3.1, despite the variations in solutions
across different clients, each C(m) always possesses some centroids (one/many-fit-one) that are proximate to
a subset of ground truth centers. To facilitate the aggregation process on the server, we propose retaining
only centroids from C(m) that are positioned close to true centers.
The client update step in FeCA focuses on refining the solution C(m) by eliminating one-fit-many centroids
that are distant from any true center. Specifically, a one-fit-many centroid is always located in the middle
of multiple nearby true clusters, making it distant from most data points in those clusters and leading to a
high standard deviation for its cluster. Conversely, many-fit-one centroids, which fit the same true center,
are close to each other and thus have a small pairwise distance. As presented in Algorithm 2, we first use
these properties to detect the candidate one-fit-many c(m) and many-fit-one c(m),c(m) centroids, which are
i p q
likely from a spurious local solution.
Next, for further refinement, we need to confirm if these candidate centroids indeed originate from a local
solution. To this end, for detected one-fit-many centroid c(m), we first calculate the objective value G(m) of
i i
its cluster D(m) as
i
G(m) =X ∥x−c(m)∥2. (3)
i x∈D(m) i 2
i
Fordetectedmany-fit-onecentroids,wemergetheirclustersD(m),D(m) toformanewclusterD(m) withthe
p q j
corresponding centroid c(m). And then we calculate the objective value G(m) as
j j
G(m) =X ∥x−c(m)∥2. (4)
j x∈{Dp(m)∪Dq(m)} j 2
If the current solution C(m) is only locally optimal, D(m) should contain data from multiple true clusters
i
with a large G(m), while D(m) only contains data from one true cluster with a small G(m). Therefore, if
i j j
G(m) is greater than G(m), it confirms that these candidate centroids stem from a local solution. In such
i j
cases, Algorithm 2 removes c(m) from C(m) for not being close to any true center. Otherwise, if G(m) is less
i i
than G(m), these centroids are regarded as the correct portion with no need for further refinement.
j
Notably,theremaybemultiplegroupsofcentroidsthatpossessthelocalstructure. Thealgorithmisdesigned
to iteratively identify and refine the local solution. Our theoretical analysis (Lemma A.1 in the appendix)
demonstrates that Algorithm 2 effectively removes all one-fit-many centroids from local solutions under the
Stochastic Ball Model. In this model, we assume that each client’s data is sampled independently and
uniformly from one of k disjoint balls centered at the ground truth centers. A formal definition is provided
in Appendix A.
4.2 Radius Assign Algorithm
After removing one-fit-many centroids in the ClientUpdate phase, only centroids near true centers
(one/many-fit-one) would be sent to the server. This RadiusAssign step prepares these centroids for server-
side aggregation by assigning a specific radius to each. This setup allows the server to utilize these radii
for effective aggregation. The primary goal of this step is to determine the radius that best approximates
the true cluster radius of the entire dataset. In this section, we present two algorithmic variants for the
RadiusAssign step. The first variant Algorithm 3 is designed for theoretical validation purposes, while the
second variant Algorithm 4 is tailored for empirical experimentation.
Through the theoretical variant, we establish Theorem 4.1 that characterizes the performance of our al-
gorithm FeCA under the Stochastic Ball Model. This theoretical variant generates a tentative solution
C˜ (m) by discarding any potential many-fit-one centroids within C(m). Following this, a radius r(m) is then
calculated according to the minimum pairwise distance among centroids in C˜ (m), and this radius is assigned
to every centroid in the original solution C(m) from Algorithm 2. We note that the method for identifying
many-fit-onecentroidsutilizedinthistheoreticalvariantisonlyapplicableundertheStochasticBallModel.
6Algorithm 3 FeCA- RadiusAssign (Theoretical)
1: input centroids C(m), clusters D(m)
2: Initialize a new set of centroids C˜ (m) =C(m).
3: Determine r′ for each centroid c˜(m) ∈ C˜ (m) as the maximum distance from any data point in D(m) to
i i i
c˜(m): r′ =max ∥x(m)−c˜(m)∥ .
i i x(m)∈D(m) i i 2
4: for each centroidi c˜(m)i ∈C˜ (m) do
i
5: Identify the nearest centroid c˜( jm ̸=i) ∈C˜ (m) to c˜( im) with the pairwise distance denoted as d ij.
6: if r i′+r j′ >d ij then
7: C˜ (m) ←C˜ (m)\{c˜(m),c˜(m)}
i j
8: end if
9: end for
10: Calculate the minimal pairwise distance between centroids in C˜ (m): ∆˜( mm in) =min c˜i,c˜j∈C˜(m),i̸=j∥c˜ i−c˜ j∥ 2.
11: Determine the uniform radius r(m) = 1∆˜(m) and assign it to each centroid c(m) ∈C(m).
2 min i
(cid:16) (cid:17)
12: R(m) ←S c(m),r(m)
i i
13: return R(m)
Algorithm 4 FeCA- RadiusAssign (Empirical)
1: input centroids C(m), clusters D(m)
2: Determine r′ for each centroid c(m) ∈ C(m) as the maximum distance from any data point in D(m) to
i i i
c(m): r′ =max ∥x(m)−c(m)∥ .
i i x(m)∈D(m) i i 2
i i
3: for each centroid c(m) ∈C(m) do
i
4: Identify the nearest centroid c( jm ̸=i) ∈ C(m) to c( im) with the pairwise distance denoted as d ij, and then
calculate r′′ = 1d .
i 2 ij
5:
Determine the unique radius r(m) =min(r′,r′′) and assign it to the centroid c(m).
i i i i
6: end for
(cid:16) (cid:17)
7: R(m) ←S c(m),r(m)
i i i
8: return R(m)
However, in real-world applications, especially under non-IID data sample scenarios, it is both challenging
and unnecessary to eliminate all many-fit-one centroids from clients’ solutions, as they often align closely
with true centers. Accordingly, we develop an empirical variant, Algorithm 4, which assumes only one-fit-
manycentroidsareexcludedandassignsauniqueradiusr(m) toeachcentroidc(m) ∈C(m). Asforremaining
i i
many-fit-onecentroids, theirradiiareestimatedashalfoftheirpairwisedistances, whicharetypicallymuch
smaller compared to those of correct centroids. The server then groups all received centroids based on these
radii, prioritizing the largest ones first. This ensures that the smaller radii associated with many-fit-one
centroids minimally impact the aggregation process. An in-depth analysis of Algorithm 4 is provided in
Appendix B, showcasing its effectiveness across a variety of experimental settings, including those with high
data heterogeneity.
It is worth noting that the theoretical variant is designed for theoretical analysis under the Stochastic Ball
Model assumption. This assumption enables easy identification of many-fit-one centroids for clearer cluster
separationapproximationandaccurateradiusassignment. Incontrast,theempiricalvariantdoesnotneedto
removemany-fit-onecentroids, astheyareclosetotruecentersandaidinreconstructingtheglobalsolution
on the server side. This approach allows the empirical variant to assign distinct radii to each remaining
centroid, enhancing the algorithm’s effectiveness and practicality without relying on limited assumptions. A
detailed comparison of the theoretical and empirical variants is provided in subsection C.4.
74.3 Server Aggregation Algorithm
Algorithm 5 FeCA- ServerAggregation
1: input cluster number k, centroids C, radius R
2:
n=1
3: while C ̸=⊘ do
4: Pick (c i,r i)∈R with the largest r i.
5: Let S n ={c t :c t ∈C,∥c t−c i∥ 2 ≤r i}
6: Set C ←C\S n
7:
n=n+1
8: end while
9: Select top k sets S n containing the largest number of elements.
10: C∗ ←mean(S j),j ∈[k]
11: return C∗
Attheserverstage, thegoalistoaggregateallreceivedcentroidsC ={C(1),...,C(M)}fromM clientsintoa
unified set of k centroids C∗. This task presents apparent challenges: due to the preceding refinement stage,
clientsmaycontributevaryingnumbersofcentroids,andtheindicesofthesecentroidsoftenlackconsistency
across clients. However, assuming the refinement phase in Algorithm 2 effectively removes spurious one-
fit-many centroids far from true centers, the returned centroids on the server would be closely grouped
around true centers. This phenomenon enables a straightforward classification of all returned centroids into
k distinct groups, each aligned with one of the k true centers, as presented in Algorithm 5. Equivalently,
this is another clustering problem based on returned centroids under a high Signal-to-Noise Ratio (SNR)
separation condition. Finally, the server calculates the means of centroids within each group to obtain C∗.
In some extreme cases where the number of groups n might be less than k, such as when all clients converge
to the same local solution. In such cases, Algorithm 2 removes one-fit-many centroids associated with the
same true clusters from all clients. This renders it impossible for Algorithm 5 to reconstruct corresponding
truecenterswithoutreceivinganyassociatedcentroidsfromclients. Itisimportanttonotethatthisscenario
is trivial within the federated framework, where all clients share the same local solutions. Essentially, it is
akin to having only one client encountering a local solution. Further discussion on cases when n < k is
provided in the Appendix E.
4.4 Theoretical Analysis
We now state our main theorem, which characterizes the performance of FeCA under the Stochastic Ball
Model. Assume the data x(m) of client m is sampled independently and uniformly from one of k disjoint
balls B with radius r, each centered at a true center θ∗, s∈[k]. Each ball component under the Stochastic
s s
Ball Model has a density
1
f s(x)= Vol(B )1 B s(x). (5)
s
Additionally, we define the maximum and minimum pairwise separations between the true centers {θ∗}
s s∈[k]
as
∆ :=max∥θ∗−θ∗∥ , ∆ :=min∥θ∗−θ∗∥ .
max s s′ 2 min s s′ 2
s̸=s′ s̸=s′
Theorem 4.1. (Main Theorem) Under the Stochastic Ball Model, for some constants λ≥3 and η ≥5, if
p
∆ ≥4λ2k4r and ∆ ≥10ηλk2 r∆ ,
max min max
then by utilizing the radius determined by Algorithm 3, any output centroid c∗ from Algorithm 1 is close to
s
some ground truth center:
4
∥c∗−θ∗∥ ≤ ∆ . (6)
s s′ 2 5η min
8Theorem4.1characterizestheperformanceofourmainalgorithmFeCA,utilizingtheradiusfromAlgorithm
3. The proof, provided in Appendix A, builds on the infinite-sample and high SNR assumptions established
in Qian et al. (2021) which characterizes local solutions of centralized k-means. Next, we will provide a
discussion of both conditions.
• Separation Condition: the separation between true centers ∆ and ∆ cannot be too small
min max
is generally necessary for a local solution to bear the structural properties described in subsec-
tion 3.1 Qian et al. (2021). Additionally, the ratio ∆max indicates how evenly spaced the true
centers are, with the ratio approaching 1 when the
true∆mceinnters
are nearly evenly spaced.
• Technical Assumptions: our main theorem heavily depends on the Stochastic Ball Model and
infinity sample assumptions. We would love to note that the local solution structure also holds
when the data follows the Gaussian mixture model or has finite data samples Chen et al. (2024).
Weviewthesetechnicalassumptionsaslessimportantthantheaboveseparationconditionandwill
corroborate using both synthetic and real clustering data to demonstrate the effectiveness of our
algorithm.
Notethattheaboveassumptionsareoftennotmetinpractice. Thus,wedevelopanothervariant,Algorithm
4, which does not require the elimination of many-fit-one centroids and assigns a unique radius to each
returned centroid from the client. A detailed empirical evaluation of these radii determined by Algorithm 4
is presented in Appendix B, showcasing their effectiveness in supporting our algorithm FeCA.
4.5 Discussions on Heterogeneity
We assume that the client’s local solution for its dataset D is also a local solution of the entire dataset
m
D = ∪ D , which allows us to leverage the structures discussed in subsection 3.1. This assumption holds
m m
whenD isanIID-sampledsubsetfromD. Ourexperimentsshowcaseouralgorithm’srobustnessevenunder
m
non-IIDconditions. HereweprovideanexplanationinFigure1,whererightplotsillustratetwoclients’non-
IID sampled data and the corresponding global solutions (achieve a global optimum when k = k∗3). We
found that despite the data heterogeneity, the clients’ global solutions share similar structures as described
in subsection 3.1. We attribute these observations to the fact that under non-IID conditions, clients’ data
tend to concentrate on some of the true clusters. This increases the chance that clients’ global solutions
contain many-fit-one centroids for those true clusters, which can be aggregated together on the server by
our algorithm FeCA. This scenario also suggests that even if a client can recover the global solution on its
non-IID data, such a global solution coincides with a local solution on IID data and we still need to deploy
FeCA to produce the final solution.
5 DeepFeCA
With FeCA,wecanlearncentroidsC∗ inthepre-definedfeaturespacecollaborativelywithmultipleclients.
In this section, we extend FeCA to unsupervised representation learning Liu et al. (2021), aiming to learn
a feature extractor f parameterized by θ from the unlabeled data set D, such that the extracted feature
θ
f (x) of data x can better characterize its similarity or dissimilarity to other data instances.
θ
Somestudies(Asanoetal.,2020;Caronetal.,2020;2018)integrateclusteringapproacheswithunsupervised
representationlearning. Forinstance, Caronetal.(2018)proposedDeepCluster, whichlearnsf (x)from
θ
an unlabeled dataset D ={x }N by repeating two steps:
n n=1
• Perform Lloyd’s algorithm on {f (x )}N to obtain a set of k centroids C ={c }k ;
θ n n=1 j j=1
• Create a pseudo-labeled set D ={(x ,yˆ )}N where yˆ =argmin ∥x −c ∥ , and learn f with a
n n n=1 n j n j 2 θ
linear classifier in a supervised fashion for multiple epochs.
3k∗ indicatesthenumberoftruecentersintheentiredataset.
9Algorithm 6 DeepFeCA for Representation Learning
1: Input: cluster number k, total number of clients M, round number T
2: Initialization: server model θ¯
3: for round t=1,··· ,T do
4: Perform FeCA with M clients to obtain k aggregated centroids C∗ (k-means clustering is performed
on features extracted by f on each client).
5: Broadcast θ¯and C∗ to
allθ¯
clients.
6: for each client m∈[M] do
7: Create D m ={(x( nm),yˆ n(m))}N n=m 1 where yˆ n(m) =argmin j∥x( nm)−c j∥ 2 and c j ∈C∗.
8: Train θ¯together with a linear classifier on D m for multiple epochs to obtain the client model θ(m).
9: end for
10:
θ¯ ←PM |Dm|θ(m)
m=1 |D|
11: end for
12:
Return the model θ¯
DeepCluster is known for its simplicity and has been shown to perform on par with other more advanced
self-supervised learning methods for representation learning Ericsson et al. (2021). In this paper, we extend
DeepClustertothefederatedsettingandproposeDeepFeCA,whichintegratesDeepClusterwithinthe
FedAvgMcMahanetal.(2017)framework. Namely,DeepFeCAiteratesbetweenlocaltrainingoff (x)on
θ
eachclientandglobalmodelaggregationformultiplerounds. Attheendofeachround,DeepFeCAapplies
FeCA to update the centroids that are used to assign pseudo labels. Algorithm 6 outlines our approach,
where the red text corresponds to FeCA, the blue text corresponds to DeepCluster, and the green text
corresponds to the element-wise weight average of FedAvg.
6 Experiments
WefirstevaluateFeCAonbenchmarksyntheticdatasets,whichhavewell-establishedtruecenters. Thenwe
extendourevaluationtofrozenfeaturesofreal-worldimagedataextractedfrompre-trainedneuralnetworks.
Additionally, we assess the representation learning capabilities of DeepFeCA by training a deep feature
extractor network from scratch in the federated framework.
Tosimulatethenon-IIDdatapartitions,wefollowHsuetal.(2019)tosplitthedatadrawnfromDirichlet(α)
for multiple clients. Smaller α indicates that the split is more heterogeneous. We also include the IID
setting, in which clients are provided with uniformly split subsets of the entire dataset. Furthermore, we
have standardized the number of clients to M =10 for all experiments in this section. A detailed discussion
on the impact of varying the number of clients is provided in Appendix D.
Baselines. We mainly compare three baselines:
• Match Averaging (M-Avg): matches different sets of centroids from clients by minimizing their
ℓ -distances and returns the means of matched centroids.
2
• k-FED: the one-shot federated clustering method Dennis et al. (2021) designed for heterogeneous
√
data, assuming that each client’s data originates from k′ ≤ k∗ true clusters. The method utilizes
a small k′ for k-means clustering on clients. In the following experiments, we select a single k′ for
each dataset, with detailed tuning experiments provided in subsection C.5.
• FFCM: Stallmann & Wilbik (2022) focuses on fuzzy c-means clustering and presents two versions
of aggregation algorithms, which are weighted averaging centroids (v1) and applying k-means on
centroids (v2). Since this method is not designed for a one-shot setting, we report its results for
both round 1 and round 10 in the following experiments.
Additionally, we include a centralized benchmark, representing the performance of k-means clustering on
the entire dataset without federated splits. In the following experiments, we set k = k∗ for all methods
except for k-FED.
10Evaluation metric. For synthetic datasets with known true centers, we assess recovered centroids by
calculatingtheℓ -distancebetweenoutputcentroidsandtruecenters. Incontrast,forrealdatasetswheretrue
2
centers are unknown, we adopt the standard clustering measures including Purity and Normalized Mutual
Information (NMI). The average Purity for all clusters is reported. NMI measures the mutual information
shared between clustering assignments X and true labels Y defined as NMI(X,Y) = 2I(X;Y)/(H(X)+
H(Y)), where I denotes the mutual information and H is the entropy.
6.1 FeCA Evaluation
On synthetic datasets. We evaluate FeCA on benchmark datasets in Fränti & Sieranoja (2018) with
knowntruecenters. Specifically,wefocusonS-sets,comprisingsyntheticdatacharacterizedbyfourdifferent
degrees of separation. S-sets includes four sets: S1, S2, S3, and S4, each consisting of 15 Gaussian clusters
in R2. Visualizations of S-sets are provided in Appendix C.
We assess recovered centroids by calculating the ℓ -distance to ground truth centers, with mean results
2
and standard deviation from 10 runs reported in Table 1. Additionally, the Purity and NMI of clustering
assignment quality are presented in subsection C.1. We investigate three data sample scenarios in the
federated setting: IID, Dirichlet(0.3), and Dirichlet(0.1). And we select k′ = 5 for k-FED after careful
tuning (detailed in subsection C.5).
ResultsinTable1indicatethatouralgorithmFeCAconsistentlyoutperformsallbaselinesinrecoveringthe
globalsolutionacrossallexperimentalsettings. Figure4providesavisualizationoftheseresults,demonstrat-
ing that even under the challenging non-IID scenario – Dirichlet(0.3), FeCA’s recovered centroids closely
approximate the true centers.
Table 1: ℓ -distance (scaled by 104) between recovered centroids and true centers on S-sets
2
under three data sample scenarios. Dirichlet(0.3) and Dirichlet(0.1) are denoted as (0.3) and (0.1),
respectively. Rd indicates rounds for FFCM. We report mean±std from 10 random runs.
ℓ -distance ×104 S-sets (S1) S-sets (S2)
2
Method IID (0.3) (0.1) IID (0.3) (0.1)
M-Avg 7.2±4.3 45.8±5.0 63.0±2.8 14.7±7.1 51.4±8.9 64.0±11.1
FFCMv1(Rd=1) 12.2±15.8 80.7±13.3 81.1±15.7 16.7±20.4 70.9±13.3 89.7±16.0
FFCMv1(Rd=10) 3.3±1.4 50.3±8.8 66.1±12.4 9.6±19.1 56.9±12.3 60.3±18.2
FFCMv2(Rd=1) 23.1±19.4 75.7±16.8 77.0±14.4 16.1±16.9 71.9±13.5 79.9±12.6
FFCMv2(Rd=10) 2.6±0.4 42.9±14.8 54.2±14.6 32.9±0.5 58.1±13.5 59.4±25.8
k-FED(k′=5) 84.6±15.8 59.4±12.3 59.8±16.9 75.9±17.7 64.1±13.1 63.9±11.6
FeCA 1.0±0.1 6.8±3.7 22.3±15.6 1.9±0.1 13.6±16.1 38.8±3.4
Centralized 14.3±18.7 8.4±14.7
ℓ -distance ×104 S-sets (S3) S-sets (S4)
2
Method IID (0.3) (0.1) IID (0.3) (0.1)
M-Avg 16.4±6.8 35.5±5.7 46.7±5.3 14.2±3.7 28.7±3.3 40.1±6.4
FFCMv1(Rd=1) 16.6±15.2 58.1±11.9 77.4±8.9 11.4±9.5 45.0±8.9 63.1±10.8
FFCMv1(Rd=10) 11.4±15.5 45.9±8.8 66.6±12.9 4.8±1.3 45.0±12.3 52.0±11.9
FFCMv2(Rd=1) 18.2±12.6 62.7±12.6 79.1±10.2 13.0±11.4 48.9±9.7 60.4±9.6
FFCMv2(Rd=10) 4.5±0.7 49.4±19.3 56.7±16.7 4.9±0.6 44.6±9.1 49.9±7.3
k-FED(k′=5) 73.1±14.3 65.6±15.2 68.8±12.8 52.7±10.8 43.4±6.3 47.5±8.6
FeCA 3.6±0.7 23.6±8.8 33.2±6.7 4.7±0.7 24.5±7.2 31.5±5.8
Centralized 25.4±19.1 20.1±10.0
Remark 6.1. Note that Table 1 suggests that FeCA (and some other federated clustering methods) can
even outperform the centralized k-means. From the perspective of federated learning, this may look odd.
11Figure 3: Illustrations of ℓ -distance results in Table 1 with 10 random seeds.
2
Figure4: Visualizations of S-sets (S1&S4) and recovered centroids by different methods. Results
are showcased under the Dirichlet(0.3) data sample scenario. Blue dots represent recovered centroids, and
red crosses indicate the ground truth centers.
But it makes perfect sense from the local solution point of view of k-means. Solving centralized k-means
likely leads to a local solution with suboptimal performance. However, with multiple clients independently
solving k-means, their solutions together have a higher chance to collaboratively recover the global solution.
To explore this advantage of FeCA, we conduct experiments on the impact of varying numbers of clients
on the synthetic dataset, as detailed in Appendix D.
On frozen features. WeevaluateFeCAonfeaturesextractedfrompre-trainedneuralnetworksusingreal
datasets–CIFAR10/100Krizhevskyetal.(2009). AndthefrozenfeaturesaregeneratedfromtheImageNet
pre-trained ResNet-50 He et al. (2016) model. For k-FED algorithm, we select k′ = 3 for CIFAR10 and
√
k′ = 10 for CIFAR100, following the suggestion k′ ≤ k∗ from their paper. In Table 2, we present the
average Purity and NMI calculated from three random runs. Our algorithm FeCA demonstrates robust
performance across different data sample scenarios. And it outperforms all baseline models in most cases.
Undertheextremenon-IIDscenario–Dirichlet(0.1),thek-FEDalgorithmtendstooutperformothers. This
√
isbecausek-FEDisdesignedforhighheterogeneity, assumingeachclientpossessesdatafromk′ ≤ k∗ true
clusters. Without considering local solution structures in k-means, k-FED relies on an accurate selection of
k′ to reduce the chance of occurrence of spurious centroids in scenarios of high heterogeneity, as illustrated
in Figure 1(right). This strategy makes its performance highly sensitive to the choice of k′, and it tends
to decrease rapidly in less heterogeneous cases. This situation highlights the importance of addressing local
solutions in federated clustering.
6.2 DeepFeCA Evaluation
We validate DeepFeCA on the CIFAR10/100 and the Tiny-ImageNet dataset with 64×64 resolution. We
randomly initialize a ResNet-18 model and train it for 150 rounds, with all 10 clients fully participating in
the process. Each round we train 5 local epochs for clients’ models with batch size 128. We modified the
official implementation of DeepCluster-v2 Caron et al. (2020) and followed their training details.
12Table 2: Purity and NMI on CIFAR-10/100. We present mean results from three random runs under
four data sample scenarios. For k-FED method, we select k′=3 on CIFAR-10 and k′=10 on CIFAR-100.
CIFAR-10 CIFAR-100
Purity NMI Purity NMI
Method IID (1.0)(0.3)(0.1) IID (1.0)(0.3)(0.1) IID (1.0)(0.3)(0.1) IID (1.0)(0.3)(0.1)
M-Avg 0.48 0.45 0.41 0.39 0.43 0.41 0.35 0.34 0.20 0.19 0.19 0.18 0.37 0.37 0.36 0.36
FFCMv1(Rd=1) 0.24 0.32 0.37 0.36 0.33 0.37 0.38 0.36 0.06 0.07 0.07 0.07 0.24 0.25 0.24 0.25
FFCMv1(Rd=10) 0.20 0.20 0.23 0.24 0.31 0.31 0.32 0.31 0.03 0.03 0.04 0.03 0.13 0.13 0.15 0.14
FFCMv2(Rd=1) 0.32 0.44 0.53 0.56 0.27 0.43 0.48 0.50 0.10 0.11 0.11 0.12 0.27 0.30 0.30 0.32
FFCMv2(Rd=10) 0.32 0.43 0.53 0.56 0.27 0.43 0.48 0.50 0.10 0.10 0.10 0.12 0.26 0.28 0.29 0.31
k-FED 0.28 0.41 0.47 0.51 0.30 0.39 0.47 0.52 0.10 0.22 0.34 0.48 0.27 0.37 0.49 0.62
FeCA 0.61 0.60 0.58 0.57 0.53 0.51 0.49 0.50 0.37 0.37 0.37 0.34 0.49 0.49 0.49 0.47
Centralized 0.64 0.53 0.39 0.50
The test accuracy is reported using linear evaluation, with the mean results of three runs presented in
Table 3. Since each round of FFCM requires one communication between the server and clients, we only
perform FFCM(Rd=1) given the computation resource constraint. And we set k′ =20 for k-FED on Tiny-
ImageNet. We first confirm that the centralized training of DeepCluster-v2 reaches reasonable accuracy
on three datasets. Also, we see that the performance in federated framework drops sharply compared to
centralized learning, showing the challenges of learning deep representation on decentralized data. One
possible reason could be the limited performance of k-means clustering results on noisy features, especially
in the early rounds, leading to unreliable pseudo labels in the following supervised training step.
However, our method shows encouraging improvements. The DeepFeCA outperforms the baselines signif-
icantly and consistently across all settings on three datasets. From experiments, we demonstrate: (1) it is
challengingforapplicationsof DeepCluster-v2infederatedsettingstomatchitscentralizedperformance;
(2) it is possible for current baselines to learn meaningful features in federated representation learning; (3)
the proposed DeepFeCA serves as a strong baseline, which outperforms current algorithms by a notable
gain.
Table 3: Top-1 test accuracy (%) with linear evaluation. We present mean results from 3 runs on
three datasets.
Accuracy (%) CIFAR-10 CIFAR-100 Tiny-ImageNet
Method IID (0.3)(0.1) IID (0.3)(0.1) IID (0.3)(0.1)
M-Avg 53.6 48.2 45.1 25.2 24.8 23.6 16.4 15.1 13.5
FFCMv1(Rd=1) 44.6 48.5 45.5 22.5 25.0 25.9 12.1 16.9 12.5
FFCMv2(Rd=1) 47.7 50.1 49.1 21.9 22.9 22.9 14.9 14.1 14.0
k-FED(k′=20) 46.8 50.0 47.1 27.5 28.9 29.7 15.3 18.9 23.4
DeepFeCA 55.3 63.7 59.9 35.3 34.9 35.2 26.7 26.4 26.1
Centralized 80.9 50.1 36.6
137 Conclusions and Future Works
We investigate federated clustering, an important yet under-explored area in federated unsupervised learn-
ing, and propose a one-shot algorithm, FeCA, by leveraging structures of local solutions in k-means. We
also adopt FeCA for representation learning and propose DeepFeCA. Through comprehensive experiments
on benchmark datasets, both FeCA and DeepFeCA demonstrate superior performance and robustness, out-
performing established baselines across various settings.
Towardsotherchallengingsettings. Throughoutthewholepaper,weconsidereithertheinfinitesample
scenario (for theory) or the large sample scenario (for experiments), where a considerable large data sample
size is still required for the local solution to have the desired structure. This corresponds to the cross-silo
federated learning as introduced in the review paper Kairouz et al. (2021), where the number of clients is
limitedbutsufficientdataisavailableoneachclient. Theothercross-devicefederatedlearningsetting,where
limiteddataisavailableoneachclient,canbeadaptedtotentativelymimicthecross-silosetting. Onecould
group all the clients into a few groups to guarantee sufficient data samples in each group, and then apply
FedAvg McMahan et al. (2017) or other federated algorithm over the clients within each group, and at last
deploy our federated k-means algorithm on solutions returned by different groups.
Acknowledgments
J. Xu and Y. Zhang acknowledge support from the Department of Electrical and Computer Engineering at
Rutgers University. H.-Y. Chen and W.-L. Chao are supported in part by grants from the National Science
Foundation (IIS-2107077 and OAC-2112606) and Cisco Research.
References
Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering
and representation learning. In ICLR, 2020.
Olivier Bachem, Mario Lucic, and Andreas Krause. Scalable k-means clustering via lightweight coresets. In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,
pp. 1119–1127, 2018.
Maria-Florina F Balcan, Steven Ehrlich, and Yingyu Liang. Distributed k-means and k-median clustering
on general topologies. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger
(eds.), Advances in Neural Information Processing Systems. Curran Associates, Inc., 2013.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised
learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp.
132–149, 2018.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu-
pervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882,
2020.
Hong-You Chen and Wei-Lun Chao. Fedbe: Making bayesian model ensemble applicable to federated
learning. In ICLR, 2021.
Yudong Chen, Dogyoon Song, Xumei Xi, and Yuqian Zhang. Local minima structures in gaussian mixture
models. IEEE Transactions on Information Theory, 2024.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations
for personalized federated learning. 2021.
Don Kurian Dennis, Tian Li, and Virginia Smith. Heterogeneity for the win: One-shot federated clustering.
arXiv preprint arXiv:2103.00697, 2021.
14AhmedElhusseinandGamzeGürsoy. Privacy-preservingpatientclusteringforpersonalizedfederatedlearn-
ings. In Machine Learning for Healthcare Conference, pp. 150–166. PMLR, 2023.
Linus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models transfer? In
CVPR, pp. 5414–5423, 2021.
Pasi Fränti and Sami Sieranoja. K-means properties on six clustering benchmark datasets, 2018. URL
http://cs.uef.fi/sipu/datasets/.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered
federated learning. Advances in Neural Information Processing Systems, 33:19586–19597, 2020.
Andrey Goder and Vladimir Filkov. Consensus clustering algorithms: Comparison and refinement. In 2008
Proceedings of the Tenth Workshop on Algorithm Engineering and Experiments (ALENEX), pp. 109–117.
SIAM, 2008.
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated
learning. arXiv preprint arXiv:1910.14425, 2019.
Chaoyang He, Murali Annavaram, and Salman Avestimehr. Group knowledge transfer: Federated learning
of large cnns at the edge. In NeurIPS, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR, 2016.
Tom Hess, Ron Visbord, and Sivan Sabato. Fast distributed k-means with a small number of rounds. arXiv
preprint arXiv:2201.13217, 2022.
Jiazhen Hong, Wei Qian, Yudong Chen, and Yuqian Zhang. A geometric approach to k-means. arXiv
preprint arXiv:2201.04822, 2022.
Tzu-MingHarryHsu,HangQi,andMatthewBrown. Measuringtheeffectsofnon-identicaldatadistribution
for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.
Eshref Januzaj, Hans-Peter Kriegel, and Martin Pfeifle. Dbdc: Density based distributed clustering. In
International Conference on Extending Database Technology, pp. 88–105. Springer, 2004.
WonyongJeong, JaehongYoon, EunhoYang, andSungJuHwang. Federatedsemi-supervisedlearningwith
inter-client consistency & disjoint learning. 2020.
Amelia Jiménez-Sánchez, Mickael Tardy, Miguel A González Ballester, Diana Mateus, and Gemma Piella.
Memory-aware curriculum federated learning for breast cancer classification. Computer Methods and
Programs in Biomedicine, 229:107318, 2023.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open
problems in federated learning. Foundations and trends® in machine learning, 14(1–2):1–210, 2021.
HillolKargupta,WeiyunHuang,KrishnamoorthySivakumar,andErikJohnson. Distributedclusteringusing
collective principal component analysis. Knowledge and Information Systems, 3(4):422–448, 2001.
AKhaled,KMishchenko,andPRichtárik. Tightertheoryforlocalsgdonidenticalandheterogeneousdata.
In AISTATS, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 2015.
Guangrui Li, Guoliang Kang, Yi Zhu, Yunchao Wei, and Yi Yang. Domain consensus clustering for uni-
versal domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 9757–9766, 2021.
15Songze Li, Sizai Hou, Baturalp Buyukates, and Salman Avestimehr. Secure federated clustering. arXiv
preprint arXiv:2205.15564, 2022.
Yiwei Li, Shuai Wang, Chong-Yung Chi, and Tony QS Quek. Differentially private federated clustering over
non-iid data. arXiv preprint arXiv:2301.00955, 2023.
Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion
in federated learning. In NeurIPS, 2020.
Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-supervised
learning: Generative or contrastive. IEEE Transactions on Knowledge and Data Engineering, 2021.
Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129–137,
1982.
Guodong Long, Ming Xie, Tao Shen, Tianyi Zhou, Xianzhi Wang, and Jing Jiang. Multi-center federated
learning: clients clustering for better personalization. World Wide Web, 26(1):481–500, 2023.
LinLu,YaoLin,YuanWen,JinxiongZhu,andShengwuXiong. Federatedclusteringforrecognizingdriving
styles from private trajectories. Engineering Applications of Artificial Intelligence, 118:105714, 2023.
Ekdeep Singh Lubana, Chi Ian Tang, Fahim Kawsar, Robert P Dick, and Akhil Mathur. Orchestra: Unsu-
pervised federated learning via globally consistent clustering. arXiv preprint arXiv:2205.11506, 2022.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and
statistics, pp. 1273–1282. PMLR, 2017.
Stefano Monti, Pablo Tamayo, Jill Mesirov, and Todd Golub. Consensus clustering: a resampling-based
method for class discovery and visualization of gene expression microarray data. Machine learning, 52(1):
91–118, 2003.
Jaehoon Oh, Sangmook Kim, and Se-Young Yun. Fedbabu: Towards enhanced representation for federated
image classification. arXiv preprint arXiv:2106.06042, 2021.
Wei Qian, Yuqian Zhang, and Yudong Chen. Structures of spurious local minima in k-means. IEEE
Transactions on Information Theory, 68(1):395–422, 2021.
Cheng Qiao, Kenneth N Brown, Fan Zhang, and Zhihong Tian. Federated adaptive asynchronous clustering
algorithm for wireless mesh networks. IEEE Transactions on Knowledge and Data Engineering, 2021.
Morris Stallmann and Anna Wilbik. Towards federated clustering: A federated fuzzy c-means algorithm
(ffcm). arXiv preprint arXiv:2201.07316, 2022.
Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang. Federated learning on
non-iid graphs via structural knowledge sharing. In Proceedings of the AAAI conference on artificial
intelligence, volume 37, pp. 9953–9961, 2023.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated
learning with matched averaging. In ICLR, 2020.
Chang Xia, Jingyu Hua, Wei Tong, and Sheng Zhong. Distributed k-means clustering guaranteeing local
differential privacy. Computers & Security, 90:101699, 2020.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In ICML, 2019.
Dun Zeng, Siqi Liang, Xiangjing Hu, Hui Wang, and Zenglin Xu. Fedlab: A flexible federated learning
framework. Journal of Machine Learning Research, 24(100):1–7, 2023.
16Fengda Zhang, Kun Kuang, Long Chen, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Fei
Wu, Yueting Zhuang, et al. Federated unsupervised representation learning. Frontiers of Information
Technology & Electronic Engineering, 24(8):1181–1193, 2023a.
Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Fedala:
Adaptivelocalaggregationforpersonalizedfederatedlearning. InProceedings of the AAAI Conference on
Artificial Intelligence, volume 37, pp. 11237–11244, 2023b.
Zhengming Zhang, Yaoqing Yang, Zhewei Yao, Yujun Yan, Joseph E Gonzalez, and Michael W Mahoney.
Improvingsemi-supervisedfederatedlearningbyreducingthegradientdiversityofmodels. arXiv preprint
arXiv:2008.11364, 2020.
YanlinZhou,GeorgePu,XiyaoMa,XiaolinLi,andDapengWu. Distilledone-shotfederatedlearning. arXiv
preprint arXiv:2009.07999, 2020.
Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang, and Shuai Yi. Collaborative unsupervised visual
representationlearningfromdecentralizeddata.InProceedingsoftheIEEE/CVFInternationalConference
on Computer Vision, pp. 4912–4921, 2021.
A Proof of Theorem 4.1
In this section, we prove Theorem 4.1. Under the Stochastic Ball Model and high Signal-to-Noise Ratios
(SNR)condition, wewilldemonstrate: (1)alltheclientsreturnedone/many-fit-onecentroidscorresponding
to the same ground truth center are bounded within a ball of some radius (determined by Algorithm 3);
(2) for the final output centroids C∗ = {c∗,...,c∗} from Algorithm 1, the distance between any centroid
1 k
c∗ ∈ C∗,s ∈ [k] and its corresponding true center is upper bounded. Specifically, the proof is composed of
s
the following three steps:
• Step 1 (Proof of the effectiveness of removing one-fit-many in Algorithm 2): On the
client end, we prove the effectiveness of removing all one-fit-many centroids by Algorithm 2;
• Step 2 (Proof of the effectiveness of assigning radius in Algorithm 3): On the server end,
weprovethattheradiusassignedbyAlgorithm3effectivelyenclosesallthecentroids(returnedfrom
the clients) associated with the same true center;
• Step 3 (Proof of Theorem 4.1): On the server end, under the assumption that there does not
exist any one-fit-many centroid (proved in Step 1), we first prove Algorithm 5 correctly classifies all
thereturnedcentroidsC ={C(1),...,C(M)}fromM clientsusingradiiassignedbyAlgorithm3,and
then derive the error bound between recovered centroids and their associated ground truth centers.
For completeness, we outline some notations used in the following proof and a formal description of the
Stochastic Ball Model below.
Stochastic Ball Model and Notations. Let θ∗,...,θ∗ ∈ Rd represent k distinct true cluster centers,
1 k
and f be the density of a distribution with mean θ∗ for each s∈[k]. We assume each data point x(m) ∈Rd
s s
of client m∈[M] is sampled independently and uniformly from a mixture f of distributions {f } , with
s s∈[k]
the density
1 k
X
f(x)= f (x). (7)
k s
s=1
The Stochastic Ball Model is the mixture f where each ball component has density
1
f s(x)= Vol(B )1 B s(x), s∈[k], (8)
s
17where B denotes a ball component centered at θ∗ with radius r. In the context of the k-means problem,
s s
to identify a set of k centroids C(m) = {c(m),...,c(m)} on client m, we consider the goal as minimizing the
1 k
following objective:
Z 1 k Z
G(C(m))=N min∥x(m)−c(m)∥2f(x(m))dx(m) = X min∥x(m)−c(m)∥2f (x(m))dx(m). (9)
i∈[k] i 2 k i∈[k] i 2 s
s=1
The above objective function represents the infinite-sample limit of the objective (1) on client m. In the
following proof, we denote the objective G(C(m)) in (9) on client m as G(m). Given a set of k centroids
C(m), we denote the associated Voronoi set as {V(m),...,V(m)}, where V(m) is the region consisting of all
1 k j
the points closer to c(m) than any other centroid in C(m). Formally, for each j ∈[k], we define
j
V(m) ={x:∥x−c(m)∥ ≤∥x−c(m)∥ ,∀l̸=j,l∈[k]}. (10)
j j 2 l 2
In addition, we define the maximum and minimum pairwise separations between true centers {θ∗} as
s s∈[k]
∆ :=max∥θ∗−θ∗∥ and ∆ :=min∥θ∗−θ∗∥ . (11)
max s s′ 2 min s s′ 2
s̸=s′ s̸=s′
A.1 Step 1 (Proof of the effectiveness of removing one-fit-many in Algorithm 2)
In this section, we prove the effectiveness of Algorithm 2 in eliminating all one-fit-many centroids under the
Stochastic Ball Model. Specifically, on each client m ∈ [M], after applying Lloyd’s algorithm, we obtain a
set of k centroids C(m). If C(m) is a non-degenerate local minimum that is not the global optimum, then it
must contain both one-fit-many and many-fit-one centroids, as discussed in subsection 3.1.
Next, the algorithm identifies a candidate one-fit-many centroid c(m) ∈ C(m) whose corresponding Voronoi
i
set V(m) contains data points with the largest standard deviation of distances to c(m). In this proof, with
i i
the infinite-sample limit, we derive the objective G(m) in Equation 3 as
i
Z
G(m) = ∥x−c(m)∥2f(x)dx. (12)
i i 2
V(m)
i
In addition, the algorithm pinpoints two candidate many-fit-one centroids c(m) and c(m) from C(m), charac-
p q
terized by the minimal pairwise distance. Then we tentatively merge the respective Voronoi sets V(m) and
p
V(m) toformanewregionD(m) andsubsequentlyobtainacorrespondingcentroidc(m). TheobjectiveG(m)
q j j j
in Equation 4 is then calculated as
Z
G(m) = ∥x−c(m)∥2f(x)dx, where D(m) =V(m)∪V(m). (13)
j j 2 j p q
D(m)
j
To test if these candidate centroids c(m) and {c(m),c(m)} are one-fit-many and many-fit-one centroids, re-
i p q
spectively, Algorithm 2 compares G(m) and G(m). If G(m) ≥G(m), then c(m) is confirmed as a one-fit-many
i j i j i
centroidtoberemoved,and{c(m),c(m)}aremany-fit-onecentroidstobekept. Thisisprovedinthefollowing
p q
Lemma A.1.
Lemma A.1. Under the Stochastic Ball Model, for some constants λ≥3 and η ≥5, if
p
∆ ≥4λ2k4r and ∆ ≥10ηλk2 r∆ , (14)
max min max
then Algorithm 2 eliminates all the one-fit-many centroids in a local minimizer C(m) on client m.
Proof. If C(m) is a non-degenerate local minimum that is not globally optimal on client m, then it must
contain both one-fit-many and many-fit-one centroids. Without loss of generality, assume that c(m) ∈ C(m)
i
18is associated with multiple true centers θ∗,...,θ∗, where t ≥ 2. Additionally, let {c(m),c(m)} ∈ C(m)
1 t p q
(potentially along with other centroids) are associated with the same true center θ∗ . Then the objective
t+1
G(m) of C(m) is
Z
G(m) =G(m)+T +B, where T = min{∥x−c(m)∥2,∥x−c(m)∥2}f(x)dx, (15)
i 1 1 p 2 q 2
Vp(m)∪Vq(m)
and G(m) is defined in (12). B denotes the objective value contributed by the Voronoi set other than
i
{V(m),V(m),V(m)}.
i p q
We construct a hypothetical solution C(m) by: (1) merging Voronoi sets V(m) and V(m) into a new region
h p q
D(m) with the centroid c(m); (2) dividing the Voronoi set V(m) into two regions D(m) and D(m), with new
j j i i1 i2
centroids c(m) and c(m) respectively:
i1 i2
D(m) ={x∈V(m) :∥x−c(m)∥ ≤∥x−c(m)∥ }, D(m) ={x∈V(m) :∥x−c(m)∥ ≤∥x−c(m)∥ };
i1 i i1 2 i2 2 i2 i i2 2 i1 2
(3) keeping the remaining centroids in C(m). Thus, we have
C(m) =C(m)\{c(m),c(m),c(m)}∪{c(m),c(m),c(m)}. (16)
h i p q i1 i2 j
The objective G(m) for this hypothetical solution C(m) is
h h
Z
G(m) =G(m)+T +B, where T = min{∥x−c(m)∥2,∥x−c(m)∥2}f(x)dx, (17)
h j 2 2
V(m)
i1 2 i2 2
i
and G(m) is defined in (13). By selecting centroids c(m) = c(m) and c(m) = argmax ∥x−c(m)∥ and
applyinj
g Lemma A.2, we have
i1 i i2 x∈V i(m) i
∆2
G(m)−G(m) ≥ min. (18)
h 36k
This inequality implies that C(m) is a local solution with a suboptimal objective value G(m). In Algorithm
2, instead of comparing G(m) and G(m), we evaluate G(m) and G(m) to determine whether C(m) is a local
h i j
solution. The difference between objective values G(m) and G(m) is
h
G(m)−G(m) =G(m)−G(m)−(T −T ). (19)
h i j 2 1
Following the selection of centroids c(m) and c(m) as above, we have the proved claim that ∥c(m)−c(m)∥≥
i1 i2 i1 i2
∆m 2in −r from the proof of Lemma A.2 in Hong et al. (2022). For the term T 2, it follows that
Z Z
T = ∥x−c(m)∥2f(x)dx+ ∥x−c(m)∥2f(x)dx (20)
2
D(m)
i1 2
D(m)
i2 2
i1 i2
Z (cid:16) (cid:17)2
≥ ∥c(m)−c(m)∥ −∥x−c(m)∥ f(x)dx (21)
D(m)
i1 i2 2 i1 2
i2
Z (cid:18)∆ (cid:19)2
≥ min −r−2r f(x)dx (22)
2
D(m)
i2
1 (cid:18)∆ (cid:19)2
≥ min −3r (23)
k 2
1 (cid:18)2∆ (cid:19)2
≥ min (24)
k 5
19Equation(20)followsfromV(m) =D(m)∪D(m). Inequality(21)usesthetriangleinequality,and(22)follows
i i1 i2
fromthechoiceofc(m) andc(m). Inequality(23)stemsfromtheballcomponent’svolumebeing 1. Inequality
i1 i2 k
(24) follows the claim that r ≤ ∆min ≤ ∆min, which is derived from the separation assumption in (14).
20ηλ2k4 30
For the term T , we have
1
Z Z
T = ∥x−c(m)∥2f(x)dx+ ∥x−c(m)∥2f(x)dx (25)
1 p 2 q 2
Vp(m) Vq(m)
Z (cid:16) (cid:17)2 Z (cid:16) (cid:17)2
≤ ∥x−θ∗ ∥ +∥θ∗ −c(m)∥ f(x)dx+ ∥x−θ∗ ∥ +∥θ∗ −c(m)∥ f(x)dx (26)
t+1 2 t+1 p 2 t+1 2 t+1 q 2
Vp(m) Vq(m)
Z (cid:16) p (cid:17)2 Z (cid:16) p (cid:17)2
≤ r+8λk2 r∆ f(x)dx+ r+8λk2 r∆ f(x)dx (27)
max max
Vp(m) Vq(m)
=Z (cid:16) r+8λk2p
r∆
(cid:17)2
f(x)dx≤
1 (cid:18)
r+
4∆ min(cid:19)2
≤
1 (cid:18)∆ min(cid:19)2
(28)
D(m) max k 5η k 3
j
Equation(25)followsfromthedefinitionofthetermT . Inequality(26)followsfromthetriangleinequality.
1
Inequality(27)utilizestheerrorboundfromTheorem1(many/one-fit-oneassociation)inQianetal.(2021),
witheachballcomponent’sradiusasr. Inequality(28)isbasedonthevolumeofeachballcomponentbeing
1, the proved claim r ≤ ∆min, and the assumption that η ≥5.
k 30
Combining the above inequalities, we have T −T ≥0. Thus, the equation (19) can be derived as
2 1
∆2
G(m)−G(m) =G(m)−G(m)+(T −T )≥G(m)−G(m) ≥ min ≥0. (29)
i j h 2 1 h 36k
Therefore, if G(m) ≥ G(m), it implies that C(m) is a local solution with a suboptimal objective value G(m).
i j
Subsequently, by comparing G(m) and G(m) for each candidate centroid c(m), Algorithm 2 can effectively
i j i
eliminates all one-fit-many centroids from C(m).
Lemma A.2. Under the Stochastic Ball Model, for some constants λ≥3 and η ≥5, if
p
∆ ≥4λ2k4r and ∆ ≥10ηλk2 r∆ , (30)
max min max
then when c(m) =c(m) and c(m) =argmax ∥x−c(m)∥, the following holds:
i1 i i2 x∈V i(m) i
∆2
G(m)−G(m) ≥ min. (31)
h 36k
Proof. The difference between objective values of the solution C(m) and the hypothetical solution C(m) is
h
G(m)−G(m) =(G(m)−T )−(G(m)−T ). (32)
h i 2 j 1
Lemma A.2 in Hong et al. (2022) establishes that by choosing centroids c(m) = c(m) and c(m) =
i1 i i2
argmax ∥x−c(m)∥, we have
x∈V(m) i
i
∆2 4r2
G(m)−T ≥ min, and G(m)−T ≤ , (33)
i 2 18k j 1 k
which follows from the volumes of the ball components under the Stochastic Ball Model, each equating to
1. Then, we derive
k
∆2 4r2 ∆2
G(m)−G(m) ≥ min − ≥ min. (34)
h 18k k 36k
Thesecondinequalityin(34)followstheclaimthatr ≤ ∆min ≤ ∆min,whichisderivedfromtheseparation
20ηλ2k4 30
assumption in (30).
20A.2 Step 2 (Proof of the radius assignment in Algorithm 3)
In this section, we present a theoretical analysis demonstrating the effectiveness of the radius assignment
in Algorithm 3, ensuring coverage of all centroids associated with one true center. On one hand, following
the removal of all one-fit-many centroids by Algorithm 2 (proved in step 1), all returned centroids C =
{C(1),...,C(M)} on the server end are concentrated around true centers {θ∗} . Thus, this is equivalently
s s∈[k]
anotherclusteringproblemoncentroidsC with(extremely)highSNRseparationcondition. Let{S∗,...,S∗}
1 k
be the ground truth clustering sets of all returned centroids C, where for each s ∈ [k], centroids within S∗
s
are all associated with the one true center θ∗. Algorithm 5 classifies these returned centroids C into k sets
s
{S ,...,S }, using the radius in R={R(1),...,R(M)} determined by Algorithm 3.
1 k
On each client m∈[M], Algorithm 3 first generates a new set of centroids C˜ (m) by discarding any potential
many-fit-one centroid from C(m). It then identifies the minimal pairwise distance ∆˜(m) in C˜ (m) aiming to
min
approximate ∆ , formulated as:
min
∆˜(m) = min ∥c˜ −c˜ ∥ . (35)
min i j 2
c˜i,c˜j∈C˜(m),i̸=j
Subsequently, Algorithm 3 calculates a uniform radius r(m) = 1∆˜(m), and assigns it to every centroid in
2 min
C(m). These centroid-radius pairs are then sent to the server.
Lemma A.3. Under the Stochastic Ball Model, for some constant λ≥3 and η ≥5, if
p
∆ ≥4λ2k4r and ∆ ≥10ηλk2 r∆ , (36)
max min max
then for centroids in S∗ which are associated with the true center θ∗,s∈[k], we have the following inequality
s s
holds on all clients m∈[M]:
1
max ∥c −c ∥ ≤ ∆˜(m), ∀s∈[k]. (37)
j i 2 2 min
ci,cj∈S s∗,i̸=j
Proof. For centroids in S∗ which are associated with one true center θ∗,s ∈ [k], we upper bound their
s s
maximum pairwise distance using the triangle inequality:
max ∥c −c ∥ ≤ max (∥c −θ∗∥ +∥θ∗−c ∥ )
j i 2 j s 2 s i 2
ci,cj∈S s∗,i̸=j ci,cj∈S s∗,i̸=j
≤ 2 max ∥c −θ∗∥ . (38)
i s 2
ci∈S s∗
UndertheStochasticBallModelwithradiusr,weapplytheerrorboundfromTheorem1(many/one-fit-one
association) in Qian et al. (2021) to the above inequality, and for some constant λ≥3 we have
p
max ∥c −c ∥ ≤2 max ∥c −θ∗∥ ≤16λk2 r∆ . (39)
j i 2 i s 2 max
ci,cj∈S s∗,i̸=j ci∈S s∗
√
By combining the above inequality and the assumption ∆ ≥10ηλk2 r∆ in (36), we obtain
min max
8
max ∥c −c ∥ ≤ ∆ , (40)
ci,cj∈S s∗,i̸=j
j i 2 5η min
where the constant η ≥5. Next, we derive the approximation error between ∆˜(m) and ∆ as:
min min
|∆˜(m) −∆ |≤ max ∥c −θ∗∥ + max ∥c −θ∗∥ , s∈[k],s′ ∈[k],s̸=s′
min min cs∈S s∗ s s 2 c s′∈S s∗
′
s′ s′ 2
≤ 2 max ∥c −θ∗∥ , s∈[k]. (41)
s s 2
cs∈S s∗
21Given that all clients follow the same mixture distributions under the Stochastic Ball Model, the above
inequality holds for all clients m ∈ [M]. Similar to inequality (38), we again utilize the error bound from
Theorem 1 (many/one-fit-one association) in Qian et al. (2021), and obtain:
8
|∆˜(m) −∆ |≤ ∆ , ∀m∈[M]. (42)
min min 5η min
Reorganizing the terms in inequality (42) gives
5η
∆˜(m) ≤∆ ≤
5η
∆˜(m), ∀m∈[M]. (43)
5η+8 min min 5η−8 min
Then by combining inequalities (40) and (43), for each s∈[k], we obtain
8 1
max ∥c −c ∥ ≤ ∆˜(m) ≤ ∆˜(m), ∀m∈[M], (44)
ci,cj∈S s∗,i̸=j
j i 2 5η−8 min 2 min
where the last inequality follows from the assumption that η ≥5.
A.3 Proof of Theorem 4.1
In this section, we complete the proof of our main theorem by demonstrating: (1) Algorithm 5 correctly
classifies all returned centroids in alignment with their corresponding true centers, utilizing the radius as-
signed by Algorithm 3; (2) we derive the error bound between the final output centroids C∗ from Algorithm
1 and the corresponding true centers.
Proof. Let cluster labels be s = 1,...,k. During the grouping process of all returned centroids C on the
server end, Algorithm 5 first selects a centroid in C with the largest radius. Without loss of generality, we
assume that this selected centroid c ∈ C is returned by the client m and associated with the true center
s
θ∗. Thus, this centroid belongs to the ground truth clustering set as c ∈ S∗ and its assigned radius is
s s s
r = 1∆˜(m). Algorithm 5 then groups the centroids located within the ball centered at c with radius r ,
s 2 min s s
resulting in the formation of the grouped cluster S ={c:c∈C,∥c−c ∥≤r }.
s s s
On one hand, Lemma A.3 implies that for each s ∈ [k], the maximum pairwise distance between centroids
in S∗ is bounded by 1∆˜(m) for any client m. Consequently, S∗ ∈ S can be readily inferred based on the
s 2 min s s
definition of S .
s
On the other hand, for other centroids c ∈S∗,s′ ∈[k],s̸=s′, we have
s′ s′
∥c −c ∥ ≥∥θ∗ −θ∗∥ −∥c −θ∗∥ −∥c −θ∗∥
s′ s 2 s′ s 2 s′ s′ 2 s s 2
≥∆ −∥c −θ∗∥ −∥c −θ∗∥ . (45)
min s′ s′ 2 s s 2
Utilizing the error bound from Theorem 1 (many/one-fit-one association) in Qian et al. (2021) gives
8
p
∥c −c ∥ ≥∆ −16λk2 r∆ ≥∆ − ∆ , (46)
s′ s 2 min max min 5η min
√
where the last step follows from the assumption ∆ ≥10ηλk2 r∆ . Applying the lower bound in (43)
min max
to the above inequality, for any client m∈[M], it follows that
∥c −c ∥ ≥
5η−8
∆˜(m) >
1
∆˜(m), (47)
s′ s 2 5η+8 min 2 min
where the constant η ≥ 5. Thus, following the definition of S , the above inequality implies that centroids
s
c ∈ S∗,s ̸= s′ do not belong to S . Combining the proved claim that S∗ ∈ S , this suggests that
s′ s′ s s s
S = S∗,s ∈ [k], up to a permutation of cluster labels. This further implies that Algorithm 5 correctly
s s
classifies all returned centroids in C according to their associated true centers.
22For each s∈[k], Algorithm 5 computes the mean of S , denoted as c∗ =mean(S ). The collection of these
s s s
mean centroids, C∗ = {c∗,...,c∗}, constitutes the final set of centroids output by Algorithm 1. Then the
1 k
proximity of c∗ to its associated true center θ∗ can be bounded as:
s s
∥c∗−θ∗∥ ≤max∥c−θ∗∥ ≤max∥c−θ∗∥ , ∀s∈[k] (48)
s s 2 s 2 s 2
c∈Ss c∈S s∗
4
p
≤8λk2 r∆ ≤ ∆ , (49)
max 5η min
for some constants η ≥ 5. The inequality (48) follows from the proved statement S = S∗, s ∈ [k]. The
s s
inequality (49) first utilizes the error bound from Theorem 1 (many/one-fit-one association) in Qian et al.
√
(2021),followedbytheapplicationoftheseparationassumption∆ ≥10ηλk2 r∆ . Thus,anyoutput
min max
centroid c∗ ∈C∗ from Algorithm 1 is close to some true center θ∗ as:
s s′
4
∥c∗−θ∗∥ ≤ ∆ , (50)
s s′ 2 5η min
thereby proving Theorem 4.1.
B Evaluation on the radius assigned by Algorithm 4 (Empirical).
This section evaluates the radius produced by the empirical algorithm variant, Algorithm 4. While the
proof of our main theorem characterizes the performance of the Algorithm 3, it is important to note that
most real-world scenarios do not satisfy a homogeneous data sample assumption. Consequently, we have
implemented an empirical procedure that assigns a unique radius to each returned centroid. In this context,
it is also not necessary to remove many-fit-one centroids on clients, as these centroids typically concentrate
aroundtruecentersandwillbegroupedtogetherviatheaggregationalgorithmontheserver. Thisempirical
algorithm variant is specifically designed to adapt our main algorithm FeCA for more general scenarios.
In this section, we empirically assess the radius assigned by Algorithm 4 under both IID and non-IID
scenarios, demonstrating its effectiveness for the proceeding aggregation step in Algorithm 5. Specifically,
our objective is to empirically show that utilizing the selected centroid-radius pair (c ,r ),s∈[k] (assigned
s s
by Algorithm 4), Algorithm 5 can effectively group all returned centroids corresponding to the same true
centerθ∗ ontheserverend. Recallthatwedenoteasetofreturnedcentroidsassociatedwithonetruecenter
s
θ∗ as S∗.
s s
Essentially,weaimtovalidatethatthedistancebetweenanycentroidc∈S∗ toc isboundedbytheassigned
s s
radius r . Our goal is thus formulated as follows:
s
max ∥c −c ∥ ≤r . (51)
i s 2 s
ci∈S s∗
The left side of the above inequality indicates the maximum distance between any two centroids in S∗, and
s
it can be further elaborated as
max ∥c −c ∥ ≤ max (∥c −θ∗∥ +∥θ∗−c ∥ )≤2 max ∥c −θ∗∥ . (52)
i s 2 i s 2 s s 2 i s 2
ci∈S s∗ cs∈S s∗ ci∈S s∗
Then our goal in (51) can be reformulated as
∥c −θ∗∥ 1
max i s 2 ≤ . (53)
ci∈S s∗ r s 2
Next, we present empirical results on the synthetic dataset, S-sets (S1), with known ground truth centers
{θ∗} . These results demonstrate that the inequality (53) holds across both IID and non-IID cases. For
s s∈[k]
this purpose, we define a new parameter σ as
∥c −θ∗∥
σ := i s 2, c ∈S∗ s∈[k]. (54)
i r i s
s
23This parameter σ represents the distance between the returned centroid c and its fitted true center θ∗
i i s
scaledbytheradiusr . Ourempiricalresultsdemonstratethatvaluesofσ remainbelow0.5forallreturned
s i
centroids in C ={C(1),...,C(M)}, in accordance with our goal inequality (53).
Results. Figure 5 illustrates the evaluation of σ , as determined using the radius assigned by Algorithm
i
4, in varied inhomogeneous settings on S-sets (S1). This figure presents σ values for all returned centroids
i
across three random runs, categorized according to their respective true centers in different colors. Results
consistently indicate that σ values stay below 0.5, thereby empirically substantiating the validity of the
i
inequality(53)inouranalysis. Consequently, itdemonstratestheefficacyofaggregatingcentroidsusingthe
radius assigned by Algorithm 4 on the server end.
Figure 5: Evaluation of σ on S-sets (S1) across three data sample scenarios. σ for k clusters is
i
representedindifferentcolors. Thevaluesofσ forallreturnedcentroidsc arereportedover3randomruns,
i i
with the red star marking the maximum σ observed in three runs. A σ value below 0.5 indicates that, the
i i
server effectively groups centroid c utilizing the radius r assigned by Algorithm 4.
i s
24We note that the number of returned centroids associated with each true center may vary. It is because we
selectively remove one-fit-many centroids on the client side, while it is possible for many-fit-one centroids
to be present. In some extreme non-IID cases, assuming a client only contains a few secluded data points
from one true cluster but they all far deviate from the true center, it may occur that a returned centroid is
not covered by the radius. Then it will be considered noisy and discarded by Algorithm 5. Concretely, the
recovered centroids of this cluster will be contributed by returned centroids from other clients.
C Supplementary experiments on the synthetic dataset
This section presents additional experimental results on the synthetic dataset S-sets Fränti & Sieranoja
(2018). The S-sets comprise four sets: S1, S2, S3, and S4, each consisting of 15 Gaussian clusters in 2-
dimensional data with varying degrees of overlap, specifically 9%,22%,41%, and 44%. For the visualization
of S-sets, refer to Figure 6 from their paper Fränti & Sieranoja (2018).
Figure 6: Visualizations of S-sets.
C.1 Evaluations on the clustering assignments
ThissectionshiftsfocustotheevaluationofclusteringassignmentsonthesyntheticdatasetS-sets,diverging
from the analysis of recovered centroids. While Table 1 in the paper assesses the ℓ -distance between
2
recovered centroids and known ground truth centers, we herein present the average results of Purity and
Table 4: Purity and NMI on S-sets under three data sample scenarios.
Purity S-sets(S1) S-sets(S2) S-sets(S3) S-sets(S4)
Method IID (0.3) (0.1) IID (0.3) (0.1) IID (0.3) (0.1) IID (0.3) (0.1)
M-Avg 0.99 0.79 0.69 0.93 0.74 0.68 0.80 0.66 0.61 0.75 0.64 0.57
FFCMv1(Rd=1) 0.97 0.69 0.66 0.95 0.68 0.58 0.83 0.60 0.54 0.79 0.60 0.53
FFCMv1(Rd=10) 0.98 0.78 0.68 0.96 0.74 0.71 0.85 0.66 0.57 0.80 0.61 0.55
FFCMv2(Rd=1) 0.95 0.69 0.66 0.95 0.63 0.61 0.83 0.58 0.52 0.78 0.57 0.54
FFCMv2(Rd=10) 0.99 0.80 0.76 0.97 0.70 0.72 0.86 0.65 0.61 0.80 0.60 0.59
k-FED(k′=5) 0.62 0.76 0.75 0.60 0.71 0.71 0.55 0.61 0.60 0.55 0.62 0.58
FeCA 0.99 0.98 0.96 0.97 0.95 0.90 0.86 0.80 0.78 0.80 0.73 0.65
Centralized 0.97 0.96 0.83 0.77
NMI S-sets(S1) S-sets(S2) S-sets(S3) S-sets(S4)
Method IID (0.3) (0.1) IID (0.3) (0.1) IID (0.3) (0.1) IID (0.3) (0.1)
M-Avg 0.98 0.85 0.79 0.92 0.80 0.78 0.77 0.71 0.69 0.70 0.65 0.62
FFCMv1(Rd=1) 0.98 0.80 0.78 0.93 0.77 0.71 0.78 0.67 0.64 0.72 0.63 0.60
FFCMv1(Rd=10) 0.98 0.85 0.80 0.94 0.80 0.78 0.79 0.71 0.66 0.72 0.64 0.61
FFCMv2(Rd=1) 0.97 0.80 0.78 0.93 0.75 0.72 0.78 0.67 0.63 0.71 0.62 0.60
FFCMv2(Rd=10) 0.99 0.87 0.85 0.95 0.78 0.79 0.79 0.70 0.68 0.72 0.63 0.63
k-FED(k′=5) 0.80 0.84 0.83 0.75 0.80 0.80 0.66 0.69 0.68 0.62 0.65 0.63
FeCA 0.99 0.96 0.95 0.95 0.94 0.90 0.80 0.77 0.75 0.72 0.69 0.66
Centralized 0.98 0.94 0.79 0.71
25NMIacross10randomrunsinTable4underthreedifferentdatasamplescenarios. Thefindingsconsistently
demonstrate that our algorithm FeCA surpasses all baseline algorithms in performance across every tested
scenario, underlining its effectiveness in federated clustering tasks.
C.2 More visualizations of recovered centroids by different methods on S-sets
To further demonstrate the superior performance of our algorithm, we present more visualizations corre-
sponding to the results detailed in Table 1 for S-sets(S2) and S-sets(S3). Figure 7 displays the centroids
recovered by various federated clustering algorithms under the non-IID condition – Dirichlet(0.3). Our al-
gorithm’s ability to resolve and leverage the structures of local solutions enables it to outperform other
baseline methods that fail to address these critical aspects, especially in challenging non-IID settings. This
emphasizes the critical role of resolving local solutions for enhanced algorithmic performance.
Figure7: Visualizations of S-sets (S2&S3) and recovered centroids by different methods. Results
are showcased under the Dirichlet(0.3) data sample scenario. Blue dots represent recovered centroids, and
red crosses indicate the ground truth centers.
C.3 Ablation study on eliminating one-fit-many centroids in Algorithm 2
Removing one-fit-many centroids in Algorithm 2 plays a crucial role in enhancing the algorithm’s perfor-
mance. Thesecentroidsaretypicallyfarfromanytruecenters. Byeliminatingone-fit-manycentroidsatthe
clientend,weeffectivelypreventthetransmissionoftheseproblematiccentroidstotheserver. Itsignificantly
simplifies the task of Algorithm 5 on the server side, which involves grouping received centroids close to the
same true center.
In this section, we conduct an ablation study on eliminating one-fit-many centroids in Algorithm 2. In
the following experiments, one-fit-many centroids are not removed on clients and then sent to the server.
We present mean square errors between recovered centroids and ground truth centers in Table 5. The
comparative results clearly demonstrate a performance degradation when these centroids are not removed,
underscoring the significance of eliminating the one-fit-many step in Algorithm 2.
Table 5: Mean square errors between recovered centroids and true centers on S-sets(S1) under
IID data sample scenario. Values of MSE are scaled by 106. We report mean results from 10 random
runs.
Method MSE×106
FeCA(notremovingone-fit-manycentroids) 12977.2
FeCA(removingone-fit-manycentroids) 6.7
Not enough output centroids. Not removing one-fit-many centroids can lead to a scenario where the
number of reconstructed centroids is less than k. This occurs because the cluster of one-fit-many centroid
typically contains data points from multiple true clusters, resulting in a significantly larger radius than that
assigned to the true cluster. Consequently, the server may prioritize these centroids with large radii during
the grouping process, forming a large group erroneously containing centroids associated with different true
centers. In Figure 8, we provide visualizations of reconstructed centroids without removing one-fit-many,
26demonstrating a notable decrease in performance. This emphasizes the necessity of their removal in our
algorithm.
output13 centroids output14 centroids output14 centroids output15 centroids
Figure8: Visualizations of recovered centroids by FeCA (not removing one-fit-many centroids).
Results are showcased under the IID condition on S-sets(S1) with k∗ = 15. Blue dots represent recovered
centroids, and red crosses indicate the ground truth centers.
C.4 Comparison between Algorithm 3 (theoretical) and Algorithm 4 (empirical)
Algorithm 3 (theoretical) is designed for theoretical analysis only under a strict setup, specifically the
Stochastic Ball Model assumption. This assumption allows for the straightforward identification and re-
moval of many-fit-one centroids. However, in practical scenarios, eliminating many-fit-ones is challenging
and unnecessary, as they often carry crucial information about the global solution.
Inthissection,weexploretheapplicabilityofAlgorithm3(theoretical)beyonditsconstraintsbyconducting
experiments on S-sets under various heterogeneous conditions. Table 6 presents results, revealing that
the performance of Algorithm 3 is suboptimal when assumptions of Stochastic Ball Model are not met,
particularly in non-IID cases. This suboptimal performance is due to its reliance on specific assumptions
for identifying many-fit-one centroids. Consequently, in practical scenarios, this approach may erroneously
eliminate true centroids, leading to less effective outcomes.
Table 6: ℓ -distance (mean±std) between recovered centroids and true centers on S-sets(S1)
2
under different data sample scenarios. Values of ℓ -distance are scaled by 104.
2
Method IID (0.3) (0.1)
FeCA(theoretical) 1.1±0.1 31.8±23.8 58.9±15.1
FeCA(empirical) 1.0±0.1 6.8±3.7 22.3±15.6
C.5 Tuning k′ for k-FED on the synthetic dataset
In this section, we provide the results of experiments conducted to select k′ for k-FED Dennis et al. (2021)
onthesyntheticdataset. GiventhatallfoursubsetsofS-setshavethesamenumberoftrueclustersk∗ =15,
hereweutilizeS-sets(S1)fortuningk′,whichhasthelargestdegreeofseparation. Thischoiceisbasedonthe
separation condition mentioned in their paper. We present the ℓ -distance between recovered centroids and
2
true centers for k′ values ranging from 2 to 15. And results (mean±std) from 10 random runs are reported
in Table 7. Additionally, we evaluate clustering assignments generated from the recovered centroids using
Purity and NMI metrics. The mean results from 10 random runs are included in Table 7.
Consideringk-FEDisdesignedforheterogeneouscases,weadoptaDirichlet(0.3)datasamplescenarioinour
experiments. It is also important to note that when k′ = k∗, the aggregation step in the k-FED algorithm
essentially becomes redundant, and the recovered centroids are equivalent to the set of centroids returned
by one randomly chosen client.
27Table7: ℓ -distance(mean±std)betweenrecoveredcentroidsandtruecenters, Purity, andNMI
2
on S-sets(S1) under Dirichlet(0.3) data sample scenario. Values of ℓ -distance are scaled by 104.
2
k-FED k′=2 k′=3 k′=4 k′=5 k′=6 k′=7 k′=8
ℓ2-distance 63.3±7.0 65.9±11.9 60.8±16.8 58.1±10.7 60.3±15.0 59.4±11.1 62.5±11.0
Purity 0.71 0.72 0.79 0.79 0.78 0.79 0.76
NMI 0.83 0.83 0.89 0.88 0.87 0.88 0.87
k-FED k′=9 k′=10 k′=11 k′=12 k′=13 k′=14 k′=15
ℓ2-distance 77.3±11.2 69.4±13.8 77.7±15.2 76.4±10.7 72.1±16.1 69.8±15.7 67.4±9.5
Purity 0.72 0.74 0.71 0.72 0.74 0.75 0.78
NMI 0.83 0.84 0.83 0.85 0.85 0.86 0.85
Figure 9: Illustrations of ℓ -distance means, Purity and NMI detailed in Table 7.
2
C.6 Presence of local solutions in k-means
Inthissection,weperformcentralizedk-meanson50%,75%,and100%ofdatafromS-sets. InFigure10,we
illustrate objective values calculated as Equation 1 using 10 random seeds. As depicted, Lloyd’s algorithm
frequentlyconvergestolocalsolutionswithobjectivevaluessignificantlylargerthanthatofglobalsolutions.
This empirical result demonstrates the presence of local solutions with poor performance is independent of
the data sample size. Also, it highlights the necessity of our algorithm, which is specifically designed to
address and resolve these suboptimal local solutions.
S-sets (S1) S-sets (S2) S-sets (S3) S-sets (S4)
100% 75% 50% 100% 75% 50% 100% 75% 50% 100% 75% 50%
1.5E+13 2.0E+13 2.5E+13 2.0E+13
1.5E+13 2.0E+13 1.5E+13
1.0E+13
1.5E+13
1.0E+13 1.0E+13
1.0E+13
5.0E+12
5.0E+12 5.0E+12 5.0E+12
0 0 0 0
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 2 4 6 8 10
10 seeds 10 seeds 10 seeds 10 seeds
Figure10: Objective values of centralized k-means using 100%, 75%, and 50% of data from S-sets.
To better understand the structures of local solutions, we visualize some local solutions of centralized k-
means on S-sets, shown in Figure 11. This visualization reveals that despite variations in data separations,
spurious local solutions exhibit structures as discussed in subsection 3.1, containing both one-fit-many and
many-fit-onecentroids. Thisemphasizesthenecessityofthestepsinouralgorithmthatdiscardone-fit-many
and aggregate many-fit-one centroids.
Almost-empty cases. As outlined in Qian et al. (2021), local solutions of k-means can be composed of
one/many-fit-one, one-fit-many and almost-empty centroids. The first two have been discussed in detail
28
seulaV
evitcejbOFigure 11: Visualizations of structured local solutions of centralized k-means on S-sets.
in subsection 3.1. Addressing almost-empty cases involves identifying centroids that are far from any true
centersanditsclusterisalmostemptywithasmallmeasure. Thistypicallyoccurswhenthedatasetcontains
isolated points that are significantly far from the true centers. It is worth noting that almost-empty cases
are more theoretical than practical, with rare occurrences in empirical experiments.
However, if such a case does occur, our algorithm can handle it in the aggregation step by Algorithm
5. Centroids from almost-empty clusters can be treated as noisy data and discarded during the grouping
process. Since they are distant from true centers and other received centroids, they do not contribute
meaningfully to the final grouping.
D Discussion on varying numbers of clients
In this section, we explore the impact of varying client numbers on our federated approach. We conduct ex-
perimentsbyallocatingafixeddatasetportion(5%randomlysampledfromtheentiredataset)toeachclient
and then perform our algorithm FeCA across varying numbers of clients. We first evaluate the recovered
centroids by calculating ℓ -distance between these centroids and the ground truth centers. Subsequently,
2
we apply a one-step Lloyd’s algorithm using the recovered centroids for initialization and then evaluate the
clustering assignments by calculating Purity and NMI. Results for the S-sets (S1) are presented in Table 8.
It is noteworthy that only centralized k-means clustering is performed when the number of clients is one.
In such cases, centralized k-means often results in large ℓ -distance due to convergence to local optima with
2
suboptimal performance.
The findings presented in Table 8 are visually depicted in Figure 12 (left), where a trend of decreasing
ℓ -distance is observed as the number of clients M increases. This trend indicates that collaboration among
2
multiple clients can significantly mitigate the negative impact of local solutions. Specifically, when a client
encounters a local minimum, integrating benign results from other clients can help alleviate this issue. This
collaborative mechanism underscores the effectiveness of federated approaches in improving performance by
leveraging the distributed nature of client contributions.
Additionally, we assess the impact of varying client numbers in a standard federated setting, particularly
under IID data sample scenario for S-sets(S1). In the following experiments, with M denoting the number
of clients, each client is allocated 1 of the data points from S-sets(S1). We note that centralized k-means
M
is performed when M = 1 on the entire dataset. We evaluate the performance of our algorithm FeCA by
reporting the ℓ -distance between recovered centroids and true centers, alongside the Purity and NMI of
2
clustering assignments, detailed in Table 9. Moreover, Figure 12 (right) features visual representations of
theℓ distanceresults,emphasizingtherobustnessofourfederatedalgorithmFeCAacrossvaryingnumbers
2
of clients.
29Table8: ℓ -distance(mean±std)betweenrecoveredcentroidsandtruecenters, Purity, andNMI
2
with different numbers of clients M. Values of ℓ -distance are scaled by 104. Each client possesses 5%
2
data of S-sets(S1).
FeCA M =1 M =2 M =3 M =4 M =5 M =6 M =7 M =8 M =9 M =10
ℓ2-distance 10.7±14.3 3.1±0.6 2.6±0.4 2.1±0.3 2.0±0.3 1.7±0.3 1.8±0.2 1.6±0.4 1.4±0.2 1.4±0.3
Purity 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
NMI 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
FeCA M =11 M =12 M =13 M =14 M =15 M =16 M =17 M =18 M =19 M =20
ℓ2-distance 1.3±0.2 1.4±0.3 1.3±0.2 1.4±0.2 1.2±0.2 1.2±0.1 1.1±0.1 1.1±0.1 1.0±0.1 1.0±0.1
Purity 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
NMI 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
Table9: ℓ -distance(mean±std)betweenrecoveredcentroidsandtruecenters, Purity, andNMI
2
with different numbers of clients M. Values of ℓ -distance are scaled by 104. Each client possesses 1
2 M
data of S-sets(S1).
FeCA M =1 M =2 M =3 M =4 M =5 M =6 M =7 M =8 M =9 M =10
ℓ2-distance 14.3±18.7 1.1±0.3 1.2±0.3 1.1±0.2 1.1±0.2 1.2±0.2 1.1±0.1 1.1±0.1 1.0±0.1 1.0±0.1
Purity 0.97 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
NMI 0.98 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
FeCA M =11 M =12 M =13 M =14 M =15 M =16 M =17 M =18 M =19 M =20
ℓ2-distance 1.1±0.1 1.1±0.1 1.1±0.2 1.1±0.1 1.1±0.1 1.0±0.2 1.1±0.2 1.1±0.1 1.1±0.1 1.1±0.1
Purity 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
NMI 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
Figure 12: Illustrations of ℓ -distance in Table 8 and Table 9.
2
E Discussion on varying k
Theselectionofk isanimportantaspectofthek-meansproblem. Inthissection, wepresentsupplementary
experiments investigating the performance of our algorithm with varying k. For clarity, in this study, we
use k to denote the number of recovered centroids desired by our algorithm, which is also the number of
output centroids by Algorithm 1. k′ denotes the parameter used to perform k-means clustering on clients in
Algorithm 2, while k∗ indicates the number of true centers. In the following, we present experiments with
varying values of k′ and k, respectively.
Varying values of k′. We perform FeCA on S-sets(S1) under three data sample scenarios, varying values
of k′. We chose k′ = 10 (undershoot case) and k′ = 20 (overshoot case) with k∗ = 15. Mean square errors
betweenrecoveredcentroidsandgroundtruthcentersareshowninTable10. Notethatforundershootcases,
30thenumberofrecoveredcentroidsk mightbelessthank∗. Intheseinstances,weidentifythetopk matching
centroids with recovered centroids from k∗ true centers and calculate the mean square error between them.
Visualizations of clustering results on different clients and FeCA’s recovered centroids are illustrated in the
following figures.
Table10: Mean square errors between recovered centroids and true centers on S-sets(S1) under
three data sample scenarios. Values of MSE are scaled by 106. Dirichlet(0.3) and Dirichlet(0.1) are
denoted as (0.3) and (0.1), respectively. We report mean results from 10 random runs.
Methods k k′ k∗ MSE-IID MSE-(0.3) MSE-(0.1)
FeCA-Varyingk′ (undershootcase) 15 10 15 34599.4 21055.7 19311.6
FeCA-Varyingk′ (overshootcase) 15 20 15 9.9 5960.4 8738.6
FeCA-Varyingk 10 15 15 8.4 551.2 7532.7
FeCA 15 15 15 6.7 308.3 3315.3
When k′ ̸=k∗, even if clients converge to the global solution, this global solution exhibits similar structures
(one-fit-many and many-fit-one) to those observed in local solutions when k′ = k∗. As shown in Figure 13,
when k′ = 10, clients’ clustering results demonstrate the presence of one-fit-many, while k′ = 20 showcases
many-fit-one structures illustrated in Figure 15. Therefore, this underscores the necessity of our algorithm,
as it effectively addresses such structured solutions.
In the undershoot case (k′ = 10 < k∗), clients’ clustering results might contain multiple one-fit-many
but no many-fit-one centroids. This complicates the elimination of one-fit-many centroids in Algorithm 2,
because identifying one-fit-many becomes challenging without many-fit-one centroids for reference. Under
non-IID conditions, this might occur in extreme undershoot cases with rather small k′, as clients’ data
may concentrate on partial true clusters. As discussed in subsection C.3, if one-fit-many centroids are not
eliminated on clients, the number of output centroids k may be less than k∗, shown in Figure 13.
Incontrast,intheovershootcase(k′ =20>k∗),theclients’clusteringresultsincludemultiplemany-fit-one
centroids but no one-fit-many centroids. Since our algorithm effectively addresses many-fit-one centroids in
theaggregationstepontheserver,FeCAcanstillaccuratelyapproximatethetruecenters,asdemonstrated
in Figure 15. This is because the elimination of many-fit-one centroids on the clients’ side is unnecessary;
these centroids can contribute meaningfully to the grouping process on the server side, as discussed in
subsection 4.2.
Varying values of k. We perform FeCA on S-sets(S1) under three data sample scenarios, selecting
k′ = k∗ = 15, and k = 10. Mean square errors between recovered centroids and ground truth centers are
reported in Table 10. When k <k∗, Algorithm 5 outputs the mean centroids of the top k groups containing
the largest number of elements. As illustrated in Figure 17 and Figure 18, with k < k∗, the recovered
centroids can still accurately approximate partial true centers.
31Figure 13: Clients’ clustering results and FeCA’s outputs under IID condition.
Clustering results on different clients when 𝑘! = 10.
Recovered centroids when 𝑘 = 𝑘∗ = 15.
output 12 centroids output 13 centroids output 14 centroids output 15 centroids
Figure 14: Clients’ clustering results and FeCA’s outputs under non-IID condition - (0.3).
32Figure 15: Clients’ clustering results and FeCA’s outputs under IID condition.
Clustering results on different clients when 𝑘! = 20.
Recovered centroids when 𝑘 = 𝑘∗ = 15.
output 15 centroids output 15 centroids output 15 centroids output 15 centroids
Figure 16: Clients’ clustering results and FeCA’s outputs under non-IID condition - (0.3).
33Figure 17: Clients’ clustering results and FeCA’s outputs under IID condition.
Clustering results on different clients when 𝑘! = 𝑘∗ = 15.
Recovered centroids when 𝑘 = 10.
.
output 10 centroids output 10 centroids output 10 centroids output 10 centroids
Figure 18: Clients’ clustering results and FeCA’s outputs under non-IID condition - (0.3).
34