LMMs-Eval: Reality Check on the Evaluation of
Large Multimodal Models
KaichenZhang∗,1,2 BoLi∗,1,2 PeiyuanZhang∗,1,2 FanyiPu∗,1,2
JoshuaAdrianCahyono1,2 KairuiHu1,2 ShuaiLiu1,2
YuanhanZhang1,2 JingkangYang1,2 ChunyuanLi1 ZiweiLiu1,2,(cid:66)
1LMMs-LabTeam 2S-Lab,NTU,Singapore
{zhan0564, libo0013, peiyuan.zhang, fpu001, ziwei.liu}@ntu.edu.sg
https://github.com/EvolvingLMMs-Lab/lmms-eval
Abstract
The advances of large foundation models necessitate wide-coverage, low-cost,
andzero-contaminationbenchmarks. Despitecontinuousexplorationoflanguage
modelevaluations,comprehensivestudiesontheevaluationofLargeMulti-modal
Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a
unified and standardized multimodal benchmark framework with over 50 tasks
and more than 10 models to promote transparent and reproducible evaluations.
Although LMMS-EVAL offers comprehensive coverage, we find it still falls
short in achieving low cost and zero contamination. To approach this evalua-
tion trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation
toolkit that emphasizes both coverage and efficiency. Additionally, we present
MultimodalLIVEBENCHthatutilizescontinuouslyupdatingnewsandonlinefo-
rumstoassessmodels’generalizationabilitiesinthewild,featuringalow-costand
zero-contaminationevaluationapproach. Insummary,ourworkhighlightstheim-
portanceofconsideringtheevaluationtrilemmaandprovidespracticalsolutionsto
navigatethetrade-offsinevaluatinglargemulti-modalmodels,pavingthewayfor
moreeffectiveandreliablebenchmarkingofLMMs. Weopensourceourcodebase
andmaintainleaderboardofLIVEBENCHatGithubandLiveBench.
1 Introduction
Good benchmarks guide AI development. Current large foundational models such as GPT-
4[59],Gemini[69],Claude[2],andmanyothers[71,60,57,14]havedemonstratedtransformative
capabilities,approachingorsurpassinghuman-levelperformancesinmanytasks. Inthiscontext,
benchmarksbecomebothchallengingandcrucialtodifferentiateamongthemodelsanddetecttheir
weaknesses.
Inthefieldoflanguagemodels,exemplaryworkssuchas[38,68,19]aimedtocomprehensively
assessmodelsacrossawiderangeofdimensions. AsgenerativeAIevolvesfromlanguage-centricto
multimodal,aunifiedevaluationframeworkandacloserlookatexistingbenchmarksareneeded.
Transparent,standardized,andreproducibleevaluationsarecrucial. Weidentifythatthereis
so far no unified evaluation protocol in the field of LMM. Model publishers [42, 71, 16, 87, 33]
comeupwithcustomevaluationpipelines,whichoftendiffersignificantlyindatapreparation,output
postprocessing,andmetricscalculation,hinderingtransparencyandreproducibility. Tothisend,we
buildastandardizedandreliablebenchmarksuitetoassessmultimodalmodelsintheirentiretywith
*Equalcontribution.(cid:66)Correspondingauthor.
Preprint.Underreview.
4202
luJ
71
]LC.sc[
1v27721.7042:viXraWide
LLaVA-W GQA MMMU Coverage
Ferret MME MathVista LLaVA QwenVL Coreset
POPE AI2D
MMBench CoZ ne tr ao m-
i
Low LMMs-Eval Selection LMM Lis t- eEval
DocVQA VQAv2 H Bal elu ns ci hon InstructBLIP ChatGPT nation Cost . LIVE (b) LMMs-Eval-Lite
ChartQA ScienceLiveBench
QA Gemini Idefics2
… … EvaluationTrilemma
It’s hard to News&Forum LiveBench
simultaneously achieve Webistes 2024-05
50+ Tasks 10+Models wide-coverage, low- 2024-06
cost, and zero- …
(a) LMMs-Eval contamination. (c) LiveBench
Figure1: TobestnavigatethetrilemmainLMMevaluationbenchmarking,wecontribute(1)LMMS-
EVAL: aunifiedandstandardizedmultimodalbenchmarksuitethatencompassesover50tasksand
morethan10models,ensuringwidecoverage;(2)LMMS-EVALLITE: anefficientbenchmarkset
withreliableandalignedresultswiththetime-consumingfull-setevaluation,addressinglow-cost
concerns;(3) LIVEBENCH: theevaluationbenchmarkwiththelatestinformationfromnewsand
forumwebsites,aimingtoevaluatemodel’szero-shotgeneralizationabilityonmostrecentevents,
therebypreventingcontaminationduringevaluations.
LMMS-EVAL. LMMS-EVALcoversover50tasksinvariousscenariostothoroughlyassessmore
than 10 multimodal models with around 30 variants. It offers a standardized evaluation pipeline
toensuretransparencyandreproducibility. Italsocomeswithaunifiedinterfacetofacilitatethe
integrationofnewmodelsanddatasets.
Wide-coverage,low-cost,andzero-contaminationbenchmarkishardtoachievesimultaneously.
Webelieveitisanimpossibletriangletoevaluatemodelswithwidecoverageandlowcostwithout
making the benchmarks susceptible to contamination, as shown in Figure 1. For instance, the
HuggingFaceOpenLLMleaderboard[72]providesaneconomicalwaytoevaluatelanguagemodels
acrossawiderangeoftasks,butitisalsopronetooverfittingandcontamination. TheLMSysChatbot
Arena[13]andAI2WildVision[50]offerrobustandnon-contaminatedevaluationthroughrealuser
interactions. However, it is expensive to gather tens of thousands of human preferences. In this
work,wedonotbreakthisimpossibletriangle. Instead,wecomplementtheevaluationlandscape
ofLMMsbyintroducingLMMS-EVALLITE andLIVEBENCH. Bycoveringdiversesetsoftasks
andpruningunnecessarydatainstances,LMMS-EVALLITE featuresalow-costandwide-coverage
LMMevaluation. Ontheotherhand,LiveBenchgathersthelatestinformationfromnewsandonline
forumstoconstructthetestdata,targetinganeconomicalandgeneralizablewaytodobenchmarks.
Insummary,weaimtoofferacomprehensiveviewoftheevaluationsonmultimodalmodelswhile
presentingourobservationsandsolutions. Ourpapermakesthefollowingcontributions:
(1) LMMS-EVAL: a unified multimodal models evaluation suite that covers over 50 tasks and
morethan10modelswitharound30sub-variants. WithLMMS-EVAL,weaimtostreamlineand
standardizetheevaluationprocessofmultimodalmodelstoensurestandardizedcomparisonsbetween
models.
(2)LMMS-EVALLITE: anefficientevaluationsetthatprovidesreliableandalignedresultswiththe
time-consumingfull-setevaluation. LMMS-EVALLITE prunesunnecessarydatainstancestoreduce
theevaluationcostwhilemaintainingtheevaluationquality.
(3) LIVEBENCH: an evaluation benchmark that gathers the latest information from news and
forum websites to evaluate models’ zero-shot generalization ability on the most recent events.
LIVEBENCH aimstoprovidealow-costandgeneralizablewaytoevaluatemultimodalmodels.
2 LMMS-EVAL: AUnifiedMultimodalModelsEvaluationSuite
Evaluationhasoftentakenasignificantamountoftimeinthemodeldevelopmentcycle.InSection2.1
wearguethatexistingevaluationpipelinesinLMMcontainmuchoverheadandarenotstandardized.
2ByintroducingLMMS-EVAL,wereducethisoverheadandscaleuptheevaluation. However,aswe
noteinSection2.2,thereisstillatrilemmainLMMevaluationthatwecannotfullyresolvebutonly
findabettertrade-off.
2.1 ScalingEvaluationswithaStandardizedFramework
Table1: Anoverviewofselectedresultson LMMS-EVAL, achievedthroughastandardizedand
transparentlyreproduciblepipeline.
Models Parameters AI2D ChartQA DocVQA LLaVAW Mathvista MME MMMU RealworldQA
LLaVA-1.5-7B 7B 54.8 18.2 28.1 59.6 26.7 1859.0 35.3 55.8
LLaVA-NeXT-Vicuna-7B 7B 66.6 54.8 74.4 72.3 34.4 1841.8 35.1 57.8
LLaVA-NeXT-Mistral-7B 7B 60.8 38.8 72.2 71.7 37.4 1823.4 33.4 59.3
Qwen-VL-Chat 7B 45.9 60.1 66.3 21.2 24.6 1890.8 27.7 1.7
InstructBLIP-Vicuna-7B 7B 33.8 12.5 13.9 55.2 23.4 1508.7 28.4 37.4
LLaVA-NeXT-LLaMA3-8B 8B 71.6 69.5 78.2 80.1 37.5 1971.5 41.7 60.0
Xcomposer4K-HD 8B 78.1 80.6 90.8 74.2 57.3 2189.8 42.6 62.6
Idefics2-8B 8B 69.2 26.4 73.4 43.7 48.0 1792.1 39.7 25.5
LLaVA-1.5-13B 13B 59.5 18.2 30.3 66.1 26.4 1818.3 34.8 54.9
LLaVA-NeXT-Vicuna-13B 13B 70.0 62.2 77.5 72.3 35.1 1891.9 35.9 58.7
InstructBLIP-Vicuna-13B 13B 36.8 12.7 13.6 54.4 25.0 1529.6 33.7 42.4
InternVL-1.5 26B 79.0 83.8 92.4 90.2 61.5 2183.6 43.1 65.0
LLaVA-NeXT-34B 34B 74.9 68.7 84.0 88.8 46.0 2030.4 46.7 62.0
LLaVA-NeXT-72B 72B 77.4 77.0 84.4 89.2 46.6 2158.9 46.4 65.4
LLaVA-NeXT-110B 110B 80.4 79.7 85.7 90.4 49.0 2200.4 49.1 63.1
ReducingtheoverheadExistingevaluationsinLMMsareoftendoneonamodel-by-modeland
dataset-by-datasetbasis[42,71]. Researcherscreatecustominferencescriptsfortheirmodelsacross
differentbenchmarks. Whilemanageableforasinglemodelandafewbenchmarks, thisprocess
becomeshighlyinefficientwhenevaluatingmultiplecheckpointsacrosstenormoredatasets. Users
needtomanuallylauncheachindividualscripttopreprocessthedatasets, inferencemodels, and
calculatefinalscoresbasedontheoutputs. Boilerplatesarealsoabundantinthecode. Toaddressthis,
LMMS-EVALfollowstheframeworkdesignofLM-EVAL-HARNESS[19]toallowforaone-command
evaluationofmultiplemodelsanddatasets. Wepreprocessandhandleallthedataneededduring
evaluation,ensuringasingledatasourceisusedacrossdifferentmodelsforastandardizedevaluation.
Furthermore,detailedmodeloutputsandresultswillbeloggedforfutureanalysis.
StandardizedevaluationCustomevaluationscriptsalsoleadtoanotherissue: thescoresreportedin
differentplacesarenotdirectlycomparable. Forinstance,[35]extractsmodelanswersbycomparing
theoutputprobabilitiesamongthechoices. Itiscountedcorrectsolongastheground-truthanswer
hasthelowestperplexityamongthechoices(PPL-based). However,[40]usethegeneration-based
evaluation. An answer is counted as correct only if the model’s generation matches the option
letter. Tothisend,wedesignaunifiedframeworkin LMMS-EVAL coveringdifferentevaluation
setups. Webelievethereisnobestsetupbutoneneedstofixonewhencomparingresultsacross
differentmodels. Forafaircomparison,wealsorespectthechattemplateofthemodelsiftheyare
instruction-tuned. Forreproducibilityandtransparency,adetailedlogcontainingtheevaluationsetup,
modelgenerations,andscorebreakdownwillbeautomaticallylogged. Sincewedesignedaunified
interface,newmodelsanddatasetscanalsobequicklyaddedintoLMMS-EVAL.
Equippedwiththesetwocoredesigns,wesuccessfullyscaledupourevaluationtoover10modelsand
morethan50datasets. WepresentpartialresultsinTable1andthefullsupportedmodels,datasets,
andscorescanbefoundinAppendixEandAppendixF.Webelievethatlarge-scaleevaluations
arecrucial. Theyenableacomprehensivecomparisonacrossvariousaspectsofmodelperformance,
revealingwhetheramodelisaversatileperformerorexcelsonlyinspecifictasks. Additionally,large-
scale,reproducible,andstandardizedevaluationsareessentialinablationexperimentstoenhanceour
understandingofmodelarchitecturesandtrainingdata.
2.2 TheEvaluationTrilemma
Ourultimategoalistofindawide-coverage,low-cost,andzero-contaminationwaytoevaluateLMMs.
However,evenwithLMMS-EVAL,wefindittobehardorevenimpossible. Specifically,oncewe
scaletheevaluationdatasetsto50+,itbecomestime-consumingtoperformafullevaluationrunon
thosedatasets. Besides,thosebenchmarksarealsosusceptibletocontaminationduringthetraining
3time[79]. AsshowninFigure1,webelievethereisatrilemmainmodelevaluation. Onecannot
achievethethreegoalssimultaneouslybutonlyfindatrade-off. TheLMSysChatbotArena[13]and
AI2WildVision[50]arefoundationalworksinstressingwidecoverageandanti-contamination. We
presentoursolutiontobalancetheothertwosidesofthetriangleinSection3andSection4.
3 LMMS-EVAL LITE: AffordableEvaluationwithBroadDomainCoverage
WeestimatethetimetoevaluatevariousLLaVAmod-
els on all LMMS-EVAL datasets in Figure 2. These
evaluationswereconductedusing8×A100GPUswith
flashattentionenabled. Wereplicatethemodelweights
acrossGPUsandusedataparallelbydefault. Formod-
elslargerthan72B,weusepipelineparallelism[26]to
loadasinglemodelacrossdifferentGPUs.
Weaimtoconstructalitebenchmarksetthatcanprovide
usefulandfastsignalsduringthemodeldevelopment.
If we can identify a subset of the benchmark where
theabsolutescoresandrelativerankingsamongmodels
remainsimilartothefullset,wecanconsiderittobesafe Figure 2: Evaluation cost demonstration
to prune the datasets. We thus present LMMS-EVAL onFullandLiteset.
LITE tocomplementthefulldatasetsinLMMS-EVAL.
Table 2: Overview of datasets in LMMS-EVAL LITE. In addition to reducing the size of large
evaluationdatasets,wealsoretainthecompleteversionsofcertaindatasetstoensurecomprehensive
coverage.
TaskDomain Dataset Split FullSize LiteSize
ChartQA test 2500 400
Doc&InfographicUnderstanding DocVQA val 5349 400
InfoVQA val 2801 200
Flickr30k val 31784 400
NoCaps val 4500 400
ImageUnderstanding&Captioning
TextCaps val 3166 300
RefCOCO val 8811 500
VisualQuestionAnswering TextVQA val 5000 300
MathVista testmini 1000 1000
Math&Science
AI2D test 3088 300
VisualDialogue LLaVA-W test 60 60
MME cog.&percep. 2374 2374
MMMU val 900 900
Multi-discipline
CMMMU val 900 900
Seed-Bench test 17990 700
- Total - 90223 9134
LitesetselectionLetthebenchmarkberepresentedasD ={(x ,y )}n andthescoringfunction
i i i=1
underlyingthebenchmarksystembedenotedasS. Givenamodelf,lettheresponseofthemodel
toaparticularquestioninthedatasetbedenotedasf(x ) = y . Weaimtoselectasubsetofthe
i (cid:98)i
benchmarkV ∈Dsuchthat
(cid:12) (cid:12)
(cid:12) |D| |V| (cid:12)
(cid:12) 1 (cid:88) 1 (cid:88) (cid:12)
V:|m V|i ≤n |D|(cid:12)
(cid:12)|D|
S(y i,y (cid:98)i)−
|V|
S(y i,y (cid:98)i)(cid:12)
(cid:12)
(1)
(cid:12) i=1 i=1 (cid:12)
Thisobjectivefunctionhasbeenproventobeequivalenttosolvingthek-Centerproblem[63]and
canbeviewedasfindingasubsetofdatapointsthatcancoverthefullset. Thiscorrespondstoour
motivationtofindasubsetthatserveasaproxyofthefullbenchmarks. However,findingtheexact
solutiontothek-CenterproblemisNP-hard[15]. Consequently,wechoosetouseagreedyalgorithm,
toefficientlycomputetheresults. Thegreedyalgorithmiscapableofachievinga2-OPTsolution.
Thedetailofthealgorithmcanbefoundin AppendixH.
To perform k-center clustering, an embedding needs to be extracted for each data point. In [63],
image features were extracted by the CNN for k-center clustering. We employed CLIP [62] for
4imageembeddingsandBGE-M3[8]fortextembeddings,andconcatenatedthemtoproduceafinal
embedding.
Toensurethatourselectedsubsetmaintainssomebasictestingabilitiescomparedtotheoriginal
benchmarks,weassessourfindingsbyexaminingthecorrelationbetweentheoriginalscoresandthe
litesetscoresacrosssixversionsofLLaVA[40]. WepresentsomeofourresultsinFigure3where
alltheresultsachieverlargerthan0.9. Resultswithallthedatasetswechoosecanbefoundinthe
AppendixD.
Lite benchmark construction We refer to the datasets used in works such as [58, 69, 2, 40] to
constructLMMS-EVALLITEandselect15datasetsacrossdifferenttaskdomainsforwidecoverage.
Tomaintainalowcostduringevaluation,weapplytheselectionmethodtopickrepresentativepoints
fordatasetscontainingmorethan1500datapoints. Thecorrelationbetweentheoriginalscoresand
thelitesetscoresislowforMME[18],sowedecidedtokeepthefullversionofit. Inaddition,we
curateanewversionofLMMS-EVALLITE inAppendixGthatcontainsmoredatasets.
(a)AI2D (b)Flickr30k (c)InfoVQA
Figure3: CorrelationGraphbetweenscoresforourlitesetandoriginalscores
Figure4: ResultsofLMMS-EVALLITEacrossdifferentmodels. Thex-axisrepresenttheweighted
averagepercentageofscoresthatthemodelgetacrossallthedataset.
Score Aggregation To provide an overall signal to guide model development, we designed a
strategytoaggregatethescoresacrossdifferentbenchmarksinLMMS-EVALLITE. Sincedifferent
datasetsandbenchmarkscomeupwiththeirownmetrics,itisnotreasonabletosimplycalculatethe
averagescore. Instead,wefirstnormalizethescoresfromeachdatasetwithinarangeof100andthen
calculatetheaveragetobethefinalaggregatedscore. Wereporttheaggregatedscorebeforeandafter
thelitesetpruninginFigure4todemonstratetheeffectivenessofourselectionmethod. Notethat
LMMS-EVAL LITEisnotdesignedtofullycomparetheperformanceofdifferentmodelfamilies.
Instead,itservedasatooltoprovideusefulandlow-costsignalsduringmodeltrainingandablations.
4 LIVEBENCH: FromStatictoLiveEvaluation
4.1 ProbingintoMultimodalDataContamination
LMMsaretrainedonmassiveamountsofdata. Forinstance, Qwen-VL[3]leverages1.4billion
pretrainingdataandCogVLM[75]uses1.5billion. However,researchinbothLLMs[86,76]and
LMMs [9] has indicated that data contamination can significantly skew benchmark scores. This
highlightstheneedforcarefuldatamanagementandvalidationtoensureaccurateandfairevaluations.
5WeexploremultimodaltrainingwithintheLLaVAframeworks,utilizingtwoprimarydatatypes:
(1) pretraining data to align visual and textual embeddings and train the vision encoder, and (2)
high-quality,supervisedfinetuningdatatoimprovediverseinstruction-followingcapabilities. The
re-annotationandconversionoflargewebandacademicdatasetsintotrainingmaterialsfrequently
leadtoissuesofoverlapandcontamination. Toaddressthis,wedevelopedananalyticaltooltoassess
theoverlapbetweentrainingandbenchmarkdata,showcasingourfindingswithdatafrom[40]with
userdataremovedinit.
Figure 5: Contamination analysis in current evaluation benchmarks and LLaVA’s training data.
Among the datasets with an overlap proportion exceeding 20%, including ChartQA, VQAv2,
COCO2014, and GQA, it has been confirmed that their training sets are included in LLaVA’s
trainingdata.
Text Overlap To measure text overlap, we use a string matching technique similar to those by
GPT-4[59],PaLM[70],andLLaMA[74]. Typically,an8∼13n-gramsrangeisused[6],butwe
consistentlyuse8n-gramsforsimiplicity. Weexcludeanyn-gramappearingmorethan10times
inthetrainingdata,labelingtheseasmeaninglessn-grams. Wealsocalculateanoverlapratiofor
eachnewn-gramcandidateagainstoursetofmeaninglessn-grams,excludingthoseexceedinga
predefinedthreshold.
Image Overlap Contrary to text overlap, determining image overlap is a more challenging task.
Whileitiscommonpracticetocomputeimageembeddingsandthencalculatetheircosinesimilarity,
selectinganappropriatethresholdapplicabletoalldatasetsisdifficult.Insteadofcomputingsimilarity
intheembeddingspace,weempiricallyfindthatusingthepretrainedSEED-tokenizer[20]leadsto
meaningfulseparationindetectingtheoverlap. Wefirsttokenizeeachimageintoa1-Dsequenceof
32tokens. Similartotext,an8-gramlookuptablewasconstructedfromthoseimagetokenstodetect
imagecontamination. Theoccurrenceof8-gramoverlapcanbeinterpretedasapproximately1/4of
theimageoverlapping.
4.1.1 Results&AnalysisonDecontamination
Toevaluatethepotentialcontaminationofcurrentbenchmarks,weselectedover20benchmarks,
includingAI2D[29],ChartQA[54],NoCaps[1],VQAv2[21],andLLaVA-in-the-wild[42]. We
report the percentages of image and text overlap in Figure 5 for our selected datasets and more
qualitativeresultsqualitativeresultsinFigure6. Ourexaminationofbothimageandtextoverlaps
hasrevealedthreeprimarytypesofdatacontaminationacrossvariousbenchmarks.
DuplicateImagesInstancesofcompletelyidenticalimagesbetweenthetrainingsetandbenchmark
datasets were observed. This issue is exemplified by two identical images in ChartQA [54] and
MM-Vet[83].
SimilarImagesOurimagen-gramanalysishassuccesfullyidentifiedtheoccurrenceofvisuallysim-
ilarimagesinboththetrainingandbenchmarkdatasets. Suchsimilaritiescouldleadtosemantically
similarquestions,asdemonstratedinexamplesfromNoCaps[1],ChartQA[54]andMM-Vet[83].
SimilarQuestionsWealsoobserverecurringquestionstructuresinthetrainingdatathatmirrorthose
inthebenchmarkdataset. Althoughthecorrespondingimagesmaydiffer,thesimilarityinquestion
structurecouldadvantagethemodelinrespondingtobenchmarkqueries.
6Figure6: WepresentseveralcasesofpossibledataoverlappinginLLaVA-NeXTpretrainingand
supervised-finetuningdata. Weobservedthreetypesofdatacontamination(1)duplicateimages(2)
similarimages(3)similarquestions.
Figure7: Overviewpipelinefor LIVEBENCH . We collectthelatest informationfromthe lively
updatedwebsites,organizetheQ&Abasedontheinformationwiththeassistanceofmultimodal
models,verifytheQ&Awithhumanannotators,evaluatethemodelswiththeQ&Acorpususing
differentjudgemodels,includinghumanjudges,andfinallyreporttheproblemset.
4.2 MultimodalLiveBench
Traditionalbenchmarksfocusonstaticevaluationsusingfixedquestionsandanswers. Asmultimodal
researchprogresses,open-sourcemodelsoftenoutperformcommercialoneslikeGPT4Vinbench-
marks,yettheylaginrealuserexperience. Dynamic,user-orientedpublicarenaslikeLMSysand
WildVisionaregainingpopularityformodelevaluationbutstrugglewithpromptqualitycontrol,
difficulty,distribution,andnoisytraffic,makingconsistentcomparisonsdifficult. Additionally,they
requirecollectingtensofthousandsofuserpreferences,whichmakestheevaluationextremelycostly.
RecentbenchmarkssuchasVibe-Eval[61]andLLaVA-Wilder[32]usereal-worlddataformore
authentic testing models abilities in the wild. However, as current foundational models training
dataiscontinuouslycrawledandupdatedfromtheweb,thetrainedmodelmayinevitablyseeand
contaminatetheevaluationbenchmarks.
To address this issue, we propose a new evaluation framework, LIVEBENCH. The key idea of
LIVEBENCH is to evaluate the model’s performance on a lively updated dataset to achieve zero
contamination while maintaining low cost. We collect the evaluation dataset from the web, and
7buildapipelinetoautomaticallygatherthelatestglobalinformationfromwebsitessuchasnewsand
communityforums. Thedetailedspecificsareasfollows.
4.2.1 DataCollectionFromtheWeb
Toensurethetimelinessandauthenticityofourinformation,weselectsourcesfromover60news
outlets,includingCNN,BBC,Japan’sAsahiShimbun,andChina’sXinhuaNewsAgency,aswellas
insightsfromforumslikeReddit. AdetailedlistofthesesourcesisprovidedinAppendixI.1.
Webeginbycapturingscreenshotsofhomepagesandthenrefinetheseimagesbyremovingwhite
margins and other non-news elements to ensure the content focuses on news information, not
advertisementsorerrorsduetowebsiteblocking.Foranalysis,weselectaquizmodelfromourpoolof
currentmostpowerfulcommercialmultimodalmodels,suchasGPT4-V,Claude-3-Opus,andGemini-
1.5-Pro. Wethenguidethequizmodeltoprogressivelyaskquestionsacrossmultipledimensions,
including(1)basicunderstanding(2)contextualanalysis(3)deeperandbroaderimplications(4)
furtherinsights. ThemodelsdesignaQ&Asettoaddressthesedimensions. Subsequently,another
modelfromourpoolreviewsandrevisesthequestionsforaccuracyandrelevance.
ThefinalQ&Asarethenreviewedbyhumansforultimatevalidation. Tobalancedatacollection
costsanduserevaluation,weaimtogatherabout500questionsmonthly,selecting100-300forour
finalLIVEBENCH problemset,taggedwithidentifierslikeLiveBench-2024-05.
4.2.2 EvaluationMetrics&ResultsonLIVEBENCH
WeadoptthescoringcriteriafromLLaVA-Wilder[32]andVibe-Eval[61]. Thejudgemodelassigns
ascorefrom[1,10]basedontheprovidedground-truthanswer,detailedin Section4.2.3. Weuse
GPT-4o as the default judge model due to its popularity and high throughput API. Additionally,
Claude-3-OpusandGemini1.5Proareimplementedasalternativejudgemodels. Thefinalreport
resultswillbescaledtoanaccuracymetricfrom0to100basedonthescores.
Table3: LiveBench-2024-06Results. Weincludetheoverallaccuracyandtheaccuracyofeach
dimension. WeuseDI,BI,FIastheabbreviationforDeeperImplications, BoarderImplications
and Further Insights. We keep our monthly maintaination and publish the results of SOTA-level
multimodalmodelsonLiveBenchLeaderboard.
Model LLM Overall Basic Contextual DI BI FI
Idefics-2-8B Mistral-v0.1-7B 36.1 41.4 29.6 35.6 45.4 28.6
InstructBLIP-7B Vicuna-1.1-7B 40.4 16.0 32.8 44.2 60.4 48.8
InstructBLIP-13B Vicuna-1.1-13B 42.9 24.6 32.6 48.8 66.6 41.8
LLaVA-1.5-7B Vicuna-1.5-7B 45.6 19.0 36.4 56.2 69.2 47.4
LLaVA-1.5-13B Vicuna-1.5-7B 48.9 23.2 37.4 56 72.2 55.8
GPT-4-Turbo(wo/vision) - 51.9 8.4 36.4 72.0 76.8 66.0
InternVL-2-2b InternLM-2-1.8B 51.9 49 44.6 48.4 61.8 55.8
LLaVA-NeXT-8B LLaMA-3-8B 67.8 50.9 62.7 74.7 80.0 70.0
InternVL-2-4b Phi-3-3.8B 68.2 71.2 60.2 66.6 76.4 66.4
XComposer-4KHD InernLM-2-7B 70.7 76.8 65.4 70.0 72.8 68.4
InternVL-2-8B InternLM-2.5-7B 73.4 81.2 68.6 71.0 76.6 69.6
InternVL-2-26B InternLM-2-20B 77.2 75.8 72.0 80.4 78.6 79.2
LLaVA-NeXT-34B Nous-Hermes-2-Yi-34B 78.4 73.0 72.4 82.4 87.8 76.2
InternVL-1.5-26B InternLM-2-20B 80.1 80.6 80.8 79.2 80.6 79.4
LLaVA-NeXT-72B Qwen-1.5-72B 80.2 76.2 72.8 84.8 86.2 80.8
Gemini-1.5-Flash - 85.7 86.8 83.0 84.6 87.8 86.2
Gemini-1.5-Pro - 85.8 92.4 81.0 84.0 92.2 79.6
Claude-3-5-Sonnet - 92.3 93.2 90.8 91.4 95.2 91.0
GPT-4o - 92.4 91.0 89.8 92.8 96.4 92.0
GPT-4-Turbo - 93.0 91.6 89.4 90.8 99.0 94.0
WepresenttheresultsoftheLIVEBENCH evaluationinTable3. Theresultsclearlyindicatethat
bothGPT-4seriesmodels,includingGPT-4-TurboandGPT-4-Omni,areamongthetopperformers.
Incontrast,theGeminiandClaudeseriesstilllagbehindopen-sourcemodels.
8Althoughmanyopen-sourcemodelsoutperformthesecommercialmodelsinstaticacademicbench-
marks(e.g.MME[18]andMMBench[44]),ourfindingssupportthehypothesisthatcommercial
multimodalmodelslikeGPT-4Vpossessrobustcapabilitiesthatexistingbenchmarksfailtofully
capture. Specifically,LIVEBENCH requiresmodelstodemonstratestrongzero-shotgeneralization
abilitiestointerpretconstantlyupdatedcontentfromnewsandforumwebsites.
WemaystillbefarfromreachingthelevelofGPT-4V.Thecurrentsurpassinginbenchmarks
ismerelyduetotheconsideredscenariosbeingtoosimple,fixed,oralreadycontaminated. These
findings, despite appearing as a setback for competitors, actually illuminate the limitations of
conventionalevaluationbenchmarks. Theyemphasizethenecessityformorethoroughevaluations
toaccuratelygaugemodelperformance. BenchmarkingservesasacompassforadvancingAI,and
theseresultsoffervaluableinsightsforprospectivechallengersseekingtoenhancetheirmodels.
4.2.3 CaseAnalysisonLIVEBENCH
The evaluation results on LIVEBENCHshow a different trend. In many existing benchmarks, the
performanceofopen-sourcedmultimodalmodelshassurpassedcommercialmodelslikeGPT-4V,
Gemini,andClaude. However,inLIVEBENCH,thecommercialmodelsstilloutperformtheopen-
sourcedmodels. Herewelistsomeofthehallucinationcasesintheopen-sourcedmodelsthatcaused
thepoorperformance. Formoredetails,pleaserefertoLiveBenchDetails.
Q:Based on the imagery and headlines presented, what are two contrasting potential
consequences for US foreign policy based on the outcome of the Biden-Trump debate?
LLaVA-NeXT-72B:The image shows two contrasting headlines related to the outcome of the
Biden-Trump debate: 1. "Why an Israel-Hezbollah war would be far more dangerous today than
the last time around" -This headline suggests that there is a potential for increased tensions and
conflict in the Middle East, which could have significant consequences for US foreign policy. If the
debate leads to a change in policy or rhetoric that exacerbates existing conflicts, it could lead to a
more volatile international environment. 2. "Julian Assange returns home as free man to Australia"
-This headline implies a different outcome where the focus on international relations might shift
towards issues of freedom of information and the treatment of journalists and whistle-blowers. If
the debate results in a change of policy towards individuals like Julian Assange, it could signal a
shift in how the US approaches global information sharing and the role of journalism in
international affairs. These headlines represent two very different potential directions for US
foreign policy based on the outcome of the debate, one focused on regional conflict and the other
on issues of press freedom and international justice.
GPT4oScore:0
GPT4oRationale:The assistant's response does not directly address the potential consequences
for US foreign policy based on the outcome of the Biden-Trump debate. Instead, it focuses on the
implications of the headlines themselves, which are not directly related to the debate. The ground
truth response, on the other hand, clearly outlines two contrasting potential consequences for US
foreign policy based on the debate outcome, providing specific examples of policy areas that
could be impacted. Therefore, the assistant's response does not meet the criteria for identifying
clear contrasting consequences or providing plausible specific examples.
Q:Analyzethe image accompanying the article "Lynas Bets on New Rare Earths Products,
Breaking China Stranglehold". What symbolic elements are used and how do they relate to the
article's headline?
InternVL-2-26B:The image accompanying the article "Lynas Bets on New Rare Earths Products,
Breaking China Stranglehold" features a person in a suit, which could symbolize business or
corporate activity. The person is holding a small, glowing object that resembles a rare earth
element, which directly relates to the article's headline about Lynas, an Australian company,
investing in new rare earths products. The glowing object could represent the value and
importance of rare earths in the global market, and the person's suit suggests the corporate aspect
of the industry. The image visually supports the article's theme of Lynas aiming to break away
from China's dominance in the rare earths market.
GPT4oScore:0
GPT4oRationale:The assistant's response incorrectly describes the image. The image actually
showsa worker excavating minerals, which directly relates to rare earth mining. The assistant's
response mentions a person in a suit holding a glowing object, which is not present in the image.
Therefore, the assistant's response does not correctly identify the symbolic elements or their
connection to the headline.
Figure8: AcaseanalysisofhallucinationinLLaVA-NeXT-72BandInternVL-2-26Bmodels. The
redpartindicatesthehallucination.
InFigure8,wepresentacaseanalysisofhallucinationinLLaVA-NeXT-72BandInternVL-2-26B
models. In the first case, the question is about the Biden-Trump debate, but LLaVA-NeXT-72B
hallucinatesbyinterpretingtheheadlinesWhyanIsrael-Hezbollahwarwouldbefarmore... and
Julian Assange... as indicating contrasting consequences. However, these headlines are neither
directlyrelatedtothedebate’soutcomenorsuggestbroaderinternationalissues. Inthesecondcase,
InternVL-2-26BincorrectlydescribestheimageaccompanyingthearticleLynasBetsonNewRare
EarthsProducts,BreakingChinaStrangleholdbutfocusesontheimagenexttothearticle.
Bothopen-sourcedmodelsshowhallucinationbymisplacingthecontexttonear-placeheadlines
orimages. Thismaysuggestthatthemodelsarenotwell-trainedtounderstandthecontextofthe
newsarticlesandthelayoutofamodernwebsite. Meanwhile,wedidnotobservesuchcommon
hallucinationincommercialmodels.
95 Conclusions
Inthiswork,weconductedathoroughrealitycheckonthecurrentevaluationpipelineandbenchmarks
forLMMs. Werecognizethedifficultiesintheevaluationduetotheevaluationtrilemma. Although
we cannot break this trilemma, we present three key contributions to find a better trade-off: 1)
LMMS-EVAL, a unified evaluation suite for a standardized and large-scale LMM evaluation, 2)
LMMS-EVAL LITE to balance low-cost evaluation with wide coverage, and 3) LIVEBENCH, a
benchmarkthattransformstraditionalstaticevaluationintoadynamicformattoaddresspotentialdata
contaminationinLMMsevaluation.WehopeourLMMS-EVALfamilymakesavaluablecontribution
tothecommunitytowardstheholisticevaluationofLMMs.
Limitation&FutureWorkThroughrealitycheck,weexplorethefieldofevaluationinLMMsand
re-examinetheevaluationprocess. Throughoutourpapers,weassumethattheevaluationtrilemma
cannotberesolved. Thissuggestsfutureworkthatgoesdeeperintofindingabettertrade-offamong
thesidesofthetrilemmaorpotentiallyovercomingit. Additionally,weaddresstheissueofdata
contaminationusingarelativelysimplemethodthatrequiresaccesstothetrainingdata,whilemost
researchdoesnotopen-sourcetheirdata. Futureworkmayfocusonmethodsthatrelysolelyonthe
modelanddevelopmoreefficientapproaches.
10References
[1] HarshAgrawal,KaranDesai,YufeiWang,XinleiChen,RishabhJain,MarkJohnson,Dhruv
Batra,DeviParikh,StefanLee,andPeterAnderson. nocaps: novelobjectcaptioningatscale.
InProceedingsoftheIEEEInternationalConferenceonComputerVision,pages8948–8957,
2019.
[2] Anthropic. Introducingthenextgenerationofclaude. AnthropicNews,March2024.
[3] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,
localization,textreading,andbeyond,2023.
[4] RohanBavishi,ErichElsen,CurtisHawthorne,MaxwellNye,AugustusOdena,ArushiSomani,
andSag˘nakTas¸ırlar. Introducingourmultimodalmodels,2023.
[5] AliFurkanBiten,RubenTito,AndresMafla,LluisGomez,MarçalRusiñol,ErnestValveny,
C.V.Jawahar,andDimosthenisKaratzas. Scenetextvisualquestionanswering,2019.
[6] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,Alec
Radford,IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners,2020.
[7] RizhaoCai, ZiruiSong, DayanGuan, ZhenhaoChen, XingLuo, ChenyuYi, andAlexKot.
Benchlmm: Benchmarkingcross-stylevisualcapabilityoflargemultimodalmodels,2023.
[8] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-
embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through
self-knowledgedistillation,2024.
[9] LinChen,JinsongLi,XiaoyiDong,PanZhang,YuhangZang,ZehuiChen,HaodongDuan,
JiaqiWang,YuQiao,DahuaLin,andFengZhao. Areweontherightwayforevaluatinglarge
vision-languagemodels?,2024.
[10] XingyuChen,ZihanZhao,LuChen,DanyangZhang,JiabaoJi,AoLuo,YuxuanXiong,and
KaiYu. Websrc: Adatasetforweb-basedstructuralreadingcomprehension,2021.
[11] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl:Scalingupvisionfoundationmodelsandaligning
forgenericvisual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[12] KanzhiCheng,QiushiSun,YougangChu,FangzhiXu,YantaoLi,JianbingZhang,andZhiyong
Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint
arXiv:2401.10935,2024.
[13] Wei-LinChiang,LianminZheng,YingSheng,AnastasiosNikolasAngelopoulos,TianleLi,
DachengLi,HaoZhang,BanghuaZhu,MichaelJordan,JosephE.Gonzalez,andIonStoica.
Chatbotarena: Anopenplatformforevaluatingllmsbyhumanpreference,2024.
[14] Cohere. Introducingcommandr+: Ascalablellmbuiltforbusiness,2024.
[15] W.Cook. CombinatorialOptimization. AWiley-Intersciencepublication.Wiley,1997.
[16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose
vision-languagemodelswithinstructiontuning,2023.
[17] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,Songyang
Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: A
pioneeringlargevision-languagemodelhandlingresolutionsfrom336pixelsto4khd. arXiv
preprintarXiv:2404.06512,2024.
11[18] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,
Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive
evaluationbenchmarkformultimodallargelanguagemodels,2024.
[19] LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,Charles
Foster,LaurenceGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,Niklas
Muennighoff,ChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,
LintangSutawika,EricTang,AnishThite,BenWang,KevinWang,andAndyZou.Aframework
forfew-shotlanguagemodelevaluation,122023.
[20] YuyingGe,SijieZhao,ZiyunZeng,YixiaoGe,ChenLi,XintaoWang,andYingShan. Making
llamaseeanddrawwithseedtokenizer. arXivpreprintarXiv:2310.01218,2023.
[21] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. MakingtheV
inVQAmatter: ElevatingtheroleofimageunderstandinginVisualQuestionAnswering. In
ConferenceonComputerVisionandPatternRecognition(CVPR),2017.
[22] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang,
LichangChen,FurongHuang,YaserYacoob,DineshManocha,andTianyiZhou. Hallusion-
bench: Anadvanceddiagnosticsuiteforentangledlanguagehallucination&visualillusionin
largevision-languagemodels,2023.
[23] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,
andJeffreyPBigham. Vizwizgrandchallenge: Answeringvisualquestionsfromblindpeople.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
3608–3617,2018.
[24] ChaoqunHe,RenjieLuo,YuzhuoBai,ShengdingHu,ZhenLengThai,JunhaoShen,JinyiHu,
XuHan,YujieHuang,YuxiangZhang,etal. Olympiadbench: Achallengingbenchmarkfor
promotingagiwitholympiad-levelbilingualmultimodalscientificproblems. arXivpreprint
arXiv:2402.14008,2024.
[25] JinyiHu,YuanYao,ChongyiWang,ShanWang,YinxuPan,QianyuChen,TianyuYu,Hanghao
Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and
MaosongSun. Largemultilingualmodelspivotzero-shotmultimodallearningacrosslanguages.
arXivpreprintarXiv:2308.12038,2023.
[26] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen,
HyoukJoongLee,JiquanNgiam,QuocV.Le,YonghuiWu,andZhifengChen. Gpipe: Efficient
trainingofgiantneuralnetworksusingpipelineparallelism,2019.
[27] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual
reasoningandcompositionalquestionanswering. InProceedingsoftheIEEE/CVFconference
oncomputervisionandpatternrecognition,pages6700–6709,2019.
[28] SaharKazemzadeh,VicenteOrdonez,MarkMatten,andTamaraBerg. ReferItGame: Referring
to objects in photographs of natural scenes. In Alessandro Moschitti, Bo Pang, and Walter
Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural
LanguageProcessing(EMNLP),pages787–798,Doha,Qatar,October2014.Associationfor
ComputationalLinguistics.
[29] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAli
Farhadi. Adiagramisworthadozenimages,2016.
[30] GeewookKim,TeakgyuHong,MoonbinYim,JeongYeonNam,JinyoungPark,JinyeongYim,
Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document
understandingtransformer. InEuropeanConferenceonComputerVision(ECCV),2022.
[31] HugoLaurençon,LéoTronchon,MatthieuCord,andVictorSanh. Whatmatterswhenbuilding
vision-languagemodels?,2024.
[32] BoLi,KaichenZhang,HaoZhang,DongGuo,RenruiZhang,FengLi,YuanhanZhang,Ziwei
Liu,andChunyuanLi. Llava-next: Strongerllmssuperchargemultimodalcapabilitiesinthe
wild,May2024.
12[33] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter: A
multi-modalmodelwithin-contextinstructiontuning,2023.
[34] BohaoLi,YuyingGe,YixiaoGe,GuangzhiWang,RuiWang,RuimaoZhang,andYingShan.
Seed-bench-2: Benchmarkingmultimodallargelanguagemodels,2023.
[35] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench:
Benchmarkingmultimodalllmswithgenerativecomprehension,2023.
[36] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-
bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint
arXiv:2307.16125,2023.
[37] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluating
objecthallucinationinlargevision-languagemodels,2023.
[38] PercyLiang,RishiBommasani,TonyLee,DimitrisTsipras,DilaraSoylu,MichihiroYasunaga,
Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of
languagemodels. arXivpreprintarXiv:2211.09110,2022.
[39] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Dollár,andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InComputer
Vision–ECCV2014: 13thEuropeanConference,Zurich,Switzerland,September6-12,2014,
Proceedings,PartV13,pages740–755.Springer,2014.
[40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning,2023.
[41] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,2024.
[42] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning,2023.
[43] JunpengLiu,YifanSong,BillYuchenLin,WaiLam,GrahamNeubig,YuanzhiLi,andXiang
Yue. Visualwebbench: Howfarhavemultimodalllmsevolvedinwebpageunderstandingand
grounding? arXivpreprintarXiv:2404.05955,2024.
[44] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. Mmbench: Isyourmulti-modal
modelanall-aroundplayer?,2024.
[45] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
[46] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu
Liu,MingruiChen,ChunyuanLi,LianwenJin,etal. Onthehiddenmysteryofocrinlarge
multimodalmodels. arXivpreprintarXiv:2305.07895,2023.
[47] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao
Cheng,Kai-WeiChang,MichelGalley,andJianfengGao. Mathvista: Evaluatingmathematical
reasoningoffoundationmodelsinvisualcontexts. InInternationalConferenceonLearning
Representations(ICLR),2024.
[48] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. InAliceH.Oh,AlekhAgarwal,DanielleBelgrave,and
KyunghyunCho,editors,AdvancesinNeuralInformationProcessingSystems,2022.
[49] PanLu,LiangQiu,JiaqiChen,TonyXia,YizhouZhao,WeiZhang,ZhouYu,XiaodanLiang,
andSong-ChunZhu. Iconqa: Anewbenchmarkforabstractdiagramunderstandingandvisual
languagereasoning,2022.
[50] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Wang, Yejin Choi, and Bill Yuchen Lin.
Wildvisionarena: Benchmarkingmultimodalllmsinthewild,February2024.
13[51] JunhuaMao,JonathanHuang,AlexanderToshev,OanaCamburu,AlanL.Yuille,andKevin
Murphy. Generationandcomprehensionofunambiguousobjectdescriptions. InProceedings
oftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),June2016.
[52] KennethMarino,MohammadRastegari,AliFarhadi,andRoozbehMottaghi. Ok-vqa: Avisual
questionansweringbenchmarkrequiringexternalknowledge. InProceedingsoftheIEEE/cvf
conferenceoncomputervisionandpatternrecognition,pages3195–3204,2019.
[53] KennethMarino,MohammadRastegari,AliFarhadi,andRoozbehMottaghi. Ok-vqa: Avisual
question answering benchmark requiring external knowledge. In Conference on Computer
VisionandPatternRecognition(CVPR),2019.
[54] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A
benchmarkforquestionansweringaboutchartswithvisualandlogicalreasoning,2022.
[55] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C.V.
Jawahar. Infographicvqa. InProceedingsoftheIEEE/CVFWinterConferenceonApplications
ofComputerVision(WACV),pages1697–1706,January2022.
[56] MineshMathew,DimosthenisKaratzas,RManmatha,andCVJawahar. Docvqa: Adatasetfor
vqaondocumentimages.corrabs/2007.00398(2020). arXivpreprintarXiv:2007.00398,2020.
[57] Mistral. Mixtral8x22b: Cheaper,better,faster,stronger,2024.
[58] OpenAI. Gpt-4v(ision)systemcard,2023.
[59] OpenAI. Gpt-4technicalreport,2024.
[60] AitorOrmazabal,CheZheng,CypriendeMassond’Autume,DaniYogatama,DeyuFu,Dono-
vanOng,EricChen,EugenieLamprecht,HaiPham,IsaacOng,etal. Rekacore,flash,and
edge: Aseriesofpowerfulmultimodallanguagemodels. arXivpreprintarXiv:2404.12387,
2024.
[61] PiotrPadlewski, MaxBain, MatthewHenderson, ZhongkaiZhu, NishantRelan, HaiPham,
DonovanOng, KaloyanAleksiev, AitorOrmazabal, SamuelPhua, etal. Vibe-eval: Ahard
evaluation suite for measuring progress of multimodal language models. arXiv preprint
arXiv:2405.02287,2024.
[62] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgar-
wal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlya
Sutskever. Learningtransferablevisualmodelsfromnaturallanguagesupervision,2021.
[63] OzanSenerandSilvioSavarese. Activelearningforconvolutionalneuralnetworks: Acore-set
approach. InInternationalConferenceonLearningRepresentations,2018.
[64] OleksiiSidorov,RonghangHu,MarcusRohrbach,andAmanpreetSingh. Textcaps: adataset
forimagecaptioningwithreadingcomprehension,2020.
[65] OleksiiSidorov,RonghangHu,MarcusRohrbach,andAmanpreetSingh. Textcaps: adataset
forimagecaptioningwithreadingcomprehension. 2020.
[66] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi
Parikh,andMarcusRohrbach. Towardsvqamodelsthatcanread,2019.
[67] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi
Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages8317–8326,2019.
[68] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.
Beyondtheimitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguagemodels.
arXivpreprintarXiv:2206.04615,2022.
[69] GeminiTeam. Gemini: Afamilyofhighlycapablemultimodalmodels,2024.
14[70] PaLMTeam. Palm2technicalreport,2023.
[71] QwenTeam. Introducingqwen-vl,2024.
[72] TheHuggingFaceH4Team. Openllmleaderboard-ahuggingfacespacebyhuggingfaceh4,
2023.
[73] RubènTito,DimosthenisKaratzas,andErnestValveny. Hierarchicalmultimodaltransformers
formulti-pagedocvqa,2023.
[74] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,
ArmandJoulin,EdouardGrave,andGuillaumeLample. Llama: Openandefficientfoundation
languagemodels,2023.
[75] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,Zhuoyi
Yang,LeiZhao,XixuanSong,JiazhengXu,BinXu,JuanziLi,YuxiaoDong,MingDing,and
JieTang. Cogvlm: Visualexpertforpretrainedlanguagemodels,2024.
[76] TianwenWei,LiangZhao,LichangZhang,BoZhu,LijieWang,HaihuaYang,BiyeLi,Cheng
Cheng,WeiweiLü,RuiHu,ChenxiaLi,LiuYang,XilinLuo,XuejieWu,LunanLiu,Wenjun
Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma,
ChuanhaiDong,YanqiSun,YifuChen,YongyiPeng,XiaojuanLiang,ShuichengYan,Han
Fang,andYahuiZhou. Skywork: Amoreopenbilingualfoundationmodel,2023.
[77] HaoningWu,ZichengZhang,ErliZhang,ChaofengChen,LiangLiao,AnnanWang,Chunyi
Li,WenxiuSun,QiongYan,GuangtaoZhai,etal. Q-bench: Abenchmarkforgeneral-purpose
foundationmodelsonlow-levelvision. arXivpreprintarXiv:2309.14181,2023.
[78] xAI. Grok-1.5visionpreview,2024.
[79] ShuoYang,Wei-LinChiang,LianminZheng,JosephE.Gonzalez,andIonStoica. Rethinking
benchmarkandcontaminationforlanguagemodelswithrephrasedsamples,2023.
[80] HaoxuanYou,HaotianZhang,ZheGan,XianzhiDu,BowenZhang,ZiruiWang,Liangliang
Cao,Shih-FuChang,andYinfeiYang. Ferret: Referandgroundanythinganywhereatany
granularity. arXivpreprintarXiv:2310.07704,2023.
[81] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions
tovisualdenotations: Newsimilaritymetricsforsemanticinferenceovereventdescriptions.
TransactionsoftheAssociationforComputationalLinguistics,2:67–78,2014.
[82] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions
tovisualdenotations: Newsimilaritymetricsforsemanticinferenceovereventdescriptions.
TransactionsoftheAssociationforComputationalLinguistics,2:67–78,2014.
[83] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,
andLijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabilities,
2023.
[84] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,
DongfuJiang,WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,
MingYin,BoyuanZheng,ZhenzhuYang,YiboLiu,WenhaoHuang,HuanSun,YuSu,and
WenhuChen. Mmmu: Amassivemulti-disciplinemultimodalunderstandingandreasoning
benchmarkforexpertagi,2023.
[85] GeZhang,XinrunDu,BeiChen,YimingLiang,TongxuLuo,TianyuZheng,KangZhu,Yuyang
Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan,
YizhiLi,ZekunWang,YudongLiu,Yu-HsuanTsai,FengjiZhang,ChenghuaLin,Wenhao
Huang,WenhuChen,andJieFu. Cmmmu: Achinesemassivemulti-disciplinemultimodal
understandingbenchmark,2024.
15[86] HughZhang,JeffDa,DeanLee,VaughnRobinson,CatherineWu,WillSong,TiffanyZhao,
Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and
SummerYue. Acarefulexaminationoflargelanguagemodelperformanceongradeschool
arithmetic,2024.
[87] PanZhang,XiaoyiDong,BinWang,YuhangCao,ChaoXu,LinkeOuyang,ZhiyuanZhao,
HaodongDuan,SongyangZhang,ShuangruiDing,WenweiZhang,HangYan,XinyueZhang,
Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and
Jiaqi Wang. Internlm-xcomposer: A vision-language large model for advanced text-image
comprehensionandcomposition,2023.
[88] RenruiZhang,DongzhiJiang,YichiZhang,HaokunLin,ZiyuGuo,PengshuoQiu,AojunZhou,
PanLu,Kai-WeiChang,PengGao,andHongshengLi. Mathverse: Doesyourmulti-modalllm
trulyseethediagramsinvisualmathproblems?,2024.
16A BroaderImpacts
A comprehensive evaluation framework can help identify the limitations of existing multimodal
models,preventingpotentialAImisuse. Ontheotherhand,benchmarkscanalsointroducebiasesthat
maynotreflectreal-worldscenarios. Ifthebenchmarksarenotrepresentativeofdiverseapplications
and contexts, there is a risk that models optimized for these benchmarks may perform poorly
in practical settings. Besides, automatic evaluations cannot replace expert human assessment in
specializedfieldssuchasmedicalimaging. TheconstructionofLIVEBENCHusesreal-worlddata
crawledfromtheweb. Itcouldpotentiallyleadtoconcernsregardingdataprivacy. Thebenchmarks
weprovidearemeantforresearchpurposesonlyandshouldbeusedwithcaution.
B DataContamination
Table4: Detailedimageoverlapandtextoverlapstatisticsaccrossdifferentdataset
Imageoverlap(%) Textoverlap(%)
Dataset Split LLaVA-NeXTData LLaVA-NeXTData
Math&Science
AI2D[29] test 6.09 25.97
MathVista[47] testmini 9.90 7.70
ScienceQA[48] img 0.35 1.54
Doc&Inforgraphic
ChartQA[54] test 68.64 26.52
DocVQA[56] val 36.08 4.06
InfoVQA[56] test 0.14 0.39
Caption
COCO2014[39] val 46.05 22.19
Flickr30k[81] test 2.97 0.00
NoCaps[1] val 2.53 19.98
TextCaps[64] val 3.79 0.00
VQA
GQA[27] testdev-balanced 13.91 9.50
TextVQA[66] val 3.90 2.00
VQAv2[21] val 46.21 2.90
Multi-taskbenchmark
CMMMU[85] val 2.89 1.11
MMBench[44] cn-dev 2.77 0.81
MMBench[44] en-dev 2.77 7.97
MME[18] test 1.60 1.39
MMMU[84] val 2.67 3.56
MMVet[83] val 4.13 3.21
SEED-Bench[35] all 1.11 13.84
Others
LLaVA-W[42] test 5.00 1.67
POPE[37] val 42.20 0.00
WepresentthedetailsoftheimageoverlappinginTable4. DatasetssuchasChartQA[54],DocVQA
[56],COCO[39],andVQAv2[21]wereincludedintheLLaVA-NeXT[40]trainingdataandthus
suffered the most from data contamination. Most of the benchmarks maintain a relatively low
contaminationproportion,withimageandtextoverlapbelow10%. POPE[37]wasdetectedtohave
ahighimageoverlappingratiobecauseitusesimagesourcesfromCOCO[39].
17C MoreQualitativeExamples
Figure9: Morequalitaiveresultswefoundusingourdecontaminationtools
We present more qualitative results here to demonstrate the data contamination problem in the
dataset. WeobservemoreidenticalimagesinbenchmarkssuchasLLaVAW [42],MathVista[47],
andInfoVQA[56]. Similarimageshavealsobeenanotherissueindifferentdatasets;wepresenttwo
moreexamplesinNoCaps[1]andMM-Vet[83]. Textoverlappingcanhelpusdetectquestionswith
similarsentencestructure. Thoughtheimagesmightnotbesimilarenough,thesesimilarquestions
mightalsobemarkedasin-domainquestions. Forexample,wepresenttwocasesinMathVista[47].
Thoughnotnecessarilycontaminationoroverlappingcases,thetwoimagesarebothtestingsimilar
domainknowledgeandmayhelpthemodeltoanswerquestionsinthebenchmarks.
D CoresetSelectioncorrelation
We compare the original scores and the selected dataset scores between the Lite version and the
originaldatasets,calculatingthecorrelationscoresbetweenthem. Wetriedtwodifferentembeddings
toperformk-centerclustering. InadditiontousingCLIP[62]andBGE[8]embeddings,wealso
trainedaLLaVA-Qwen1.8Bmodelfollowingthetrainingrecipeof[40]toembedimageandtext
pairssimultaneously. ForLLaVAembeddings,thelasthiddenstatesforalltokenswereaveragedinto
asinglevectortoserveasthefeaturevectorforeachdatapoint. Wereportthecorrelationresultsfor
bothembeddingsinTable5.
E LMMS-EVAL SuiteInformation
Datasets on LMMs-Eval In previous research, benchmarks such as AI2D [29], TextVQA [66],
TextCaps [64], Flickr30k [81], and OK-VQA [53] among many others, have been employed to
assess a model’s performance in tasks such as captioning, optical character recognition (OCR),
and visual QA. With the advent of Large Multimodal Models (LMMs), these have increasingly
focusedonbroadercapabilitiesspanningbothvisionandlanguage,includingreasoning[48]and
visualinstructionfollowing[42]. Consequently,newbenchmarksfeaturingincreasinglychallenging
tasksandmorecomprehensiveevaluationshavebeenproposed. Forexample,ScienceQA[48]and
MathVista [47] assess mathematical and scientific competencies, while benchmarks like SEED-
18Table5: Thefullcorrelationresultsweachieveusingourselectionmethods
Correlation
Dataset Split LiteSize OriginalSize LLaVAEmbedding CLIP+BGEEmbedding
Math&Science
AI2D [29] test 300 3088 0.94 0.98
Doc&Inforgraphic
ChartQA [54] test 400 2500 0.96 0.97
DocVQA [56] val 400 5349 0.99 0.99
InfoVQA [56] val 200 2801 0.94 0.94
Caption
Flickr30k [81] test 400 31784 0.99 0.91
NoCaps [1] val 400 4500 0.99 0.98
TextCaps [64] val 300 3166 0.98 0.96
RefCOCO [28] val 500 8811 0.99 0.99
VQA
TextVQA [66] val 300 5000 0.99 0.99
Multi-taskbenchmark
SeedBench [35] test 700 17990 0.77 0.87
Table6: DatasetStatisticsinLMMS-EVAL. Thistablecategorizestheinitialsetoftasks,detailing
theirtaskdomains, ground-truthtypes, instancecounts, andsplits. Weprovideacomprehensive
overviewofthediversedatasetsemployed,whichcovervarioustaskdomainsandevaluationmetrics.
Datasets TaskDomains Ground-TruthTypes Instances Splits
AI2D[29] Science,Diagram Muiti-Choice 3088 test
BenchLMM [7] CrossStyleUnderstanding ShortAnswer/Muiti-Choice 102 test
ChartQA[54] Chart ShortAnswer 2500 test
CMMMU[85] Multi-task,WorldKnowledge Free-form/Muiti-Choice 900/11000 val/test
COCO2014Caption[39] Captioning ShortAnswer 40775/40504 test/val
COCO2017Caption[39] Captioning ShortAnswer 40670/5000 test/val
DocVQA[56] Document ShortAnswer 5349 test
Ferret[80] ReferringorGroundingActions Free-formAnswer 120 test
Flickr30k[82] VisualUnderstanding Captioning 31783 test
GQA[27] Real-World/CompositionalQA ShortAnswer 12578 test/dev
Hallusion-Bench[22] MultimodalImage-ContextReasoning YesorNo 951 image
IconQA[49] AbstractDiagrams Muiti-Choice/ShortAnswer 21489/21488 test/val
InfoVQA[55] Infographicsunderstanding Extractive/Numerical 2801 val
LLaVA-COCO[42] Conversation,Reasoning Free-formAnswer 90 test
LLaVA-W[42] Conversation,Reasoning Free-formAnswer 60 test
LLaVA-Wilder[41] Conversation,Reasoning Free-formAnswer 210/1020 test
LiveBench(Ours) WebpageUnderstanding/LivelyUpdated Free-form dynamic test
MathVista[47] MathematicalReasoning/Understanding Free-form/Muiti-Choice 1000 testmini
MathVerse[88] MathematicalReasoning/Understanding Free-form/Muiti-Choice 3940 testmini
MMBench[45] Reasoning/Perception Muiti-Choice 6666/4329 test/dev
MME[18] Perception,Cognition YesorNo 2374 test
MMMU[84] Multi-task,WorldKnowledge Free-form/Muiti-Choice 10500/900 test/val
MM-Vet[83] Multi-task Free-form 218 test
Multilingual-LLaVA-W Multi-lingualConversation,Reasoning Free-formAnswer 60 test
MultiDocVQA[73] Document ShortAnswer 5019/5187 test/val
NoCaps[1] NovelObjectCaptioning ShortAnswer 4500 val
OCRBench[46] TextRecognition ShortAnswer 1000 test
OKVQA[52] knowledge-basedvisualQA ShortAnswer 5046 val
OlympiadBench[24] Reasoning ShortAnswer 2126/6351 test-en/test-cn
POPE[37] Hallucination YesorNo 9000 test
Q-Bench [77] ImageQualityAssessment ShortAnswer/Muiti-Choice 2990 test
RealWorldQA [78] RealworldscenariosQA Muiti-Choice 765 test
Refcoco[28,51] ReferringExpression ShortAnswer 5000/1975/1810/8811 bbox-test/A/B/val
Refcoco[28,51] ReferringExpression ShortAnswer 5000/1975/1810/8811 seg-test/A/B/val
Refcoco+[28,51] ReferringExpression ShortAnswer 1975/1798/3805, bbox-testA/B/val
Refcoco+[28,51] ReferringExpression ShortAnswer 1975/1798/3805 seg-testA/B/val
Refcocog[28,51] ReferringExpression ShortAnswer 5023/7573 bbox-testB/val,
Refcocog[28,51] ReferringExpression ShortAnswer 5023/7573 seg-test/val
ScienceQA[48] Science,WorldKnowledge,Reasoning Muiti-Choice 4241 test
ScreenSPOT[12] GUIUnderstanding/Navigation ShortAnswer/Coordinates 1272 test
SEED-Bench[36] SpatialandTemporalUnderstanding Muiti-Choice 17990 test
SEED-Bench-2[34] Multi-disciplinaryKnowledge Muiti-Choice 24371 test
ST-VQA[5] HighlevelSemanticInformationUnderstanding ShortAnswer 4070 test
SynthDoG[30] TextUnderstanding Free-form 500/500 val-en/val-zh
TextCaps[65] TextUnderstanding Captioning 21953/3166/3289 train/val/test
TextVQA[67] TextUnderstanding ShortAnswer 5000/5734 val/test
VisualWebBench[43] WebpageUnderstanding/OCR/Reasoning ShortAnswer/Muiti-Choice 1536 test
VizwizVQA[23] LowQualityImageUnderstanding ShortAnswer 8000/4319 test/val
VQAv2[21] VisualQA Free-form 447793/214354 test/val
WebSRC[10] StructureofWebpage ShortAnswer/YesorNo 40357/52826 test/dev
19Table7: DetailedStatisticsoftheInitialSetofModelsinLMMS-EVAL. Themodelsarecategorized
bytheirmodelfamily, withtheirinferenceparameters, modeltypes(indicatingwhethertheyare
open-sourcedoraccessedviaAPI),andparalleltypes,whichdenotethestrategyleveragedduringthe
modelinference.
ModelFamily ModelVersion Parameters ModelType ParallelType
InstructBLIP-Vicuna-7B 7B Open-sourced Data
InstructBLIP
InstructBLIP-Vicuna-13B 13B Open-sourced Data
Fuyu Fuyu-8B 8B Open-sourced Data
Idefics Idefics-2-8B 8B Open-sourced Data
MiniCPM MiniCPM-V2.8B 2.8B Open-sourced Data
XComposer XComposer-4KHD 8B Open-sourced Data
InternVL InternVL-1.5 26B Open-sourced Data
LLaVA-1.5-7B 7B Open-sourced Data
LLaVA-1.5-13B 13B Open-sourced Data
LLaVA-NeXT-Vicuna-7B 7B Open-sourced Data
LLaVA-NeXT-Vicuna-13B 13B Open-sourced Data
LLaVA LLaVA-NeXT-Mistral-7B 7B Open-sourced Data
LLaVA-NeXT-Yi-34B 34B Open-sourced Data
LLaVA-NeXT-LLaMA-3-8B 8B Open-sourced Data
LLaVA-NeXT-Qwen-72B 72B Open-sourced Model
LLaVA-NeXT-Qwen-110B 110B Open-sourced Model
Qwen-VL-Chat-7B 7B Open-sourced Data
Qwen-VL Qwen-VL-Plus N/A Close-sourced,API Data
Qwen-VL-MAX N/A Close-sourced,API Data
Gemini-1.0-Pro N/A Close-sourced,API Data
Gemini Gemini-1.5-Flash N/A Close-sourced,API Data
Gemini-1.5-Pro N/A Close-sourced,API Data
GPT-4V N/A Close-sourced,API Data
GPT4
GPT-4O N/A Close-sourced,API Data
Claude-3-Haku N/A Close-sourced,API Data
Claude Claude-3-Sonnet N/A Close-sourced,API Data
Claude-3-Opus N/A Close-sourced,API Data
Bench[35],CMMMU[85],MMMU[84],andMM-Bench[44]evaluatethemultifaceteddimensions
ofmultimodalmodels.
ModelsonLMMs-EvalToenablecomparisonsonnewbenchmarksfordifferentmodelsandto
understandtheircapabilitiesacrossmultipletasks,wehavesupportedover10modelssuchasFuyu[4],
LLaVA[42],Instruct-BLIP[16],InternVL[11],XComposer[17],Qwen-VL[3],MiniCPM[25],
Idefics[31]andclosed-sourcemodelssuchasGPT-4V[58],Gemini[69],Qwen-VL-Max[71]and
Claude[2].
20F UnifiedEvaluationResultswith LMMS-EVAL
Table8: MoreresultsusingLMMS-EVAL
Split Metric #Num LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-NeXT-mistral-7B LLaVA-NeXT-vicuna-7B LLaVA-NeXT-13B LLaVA-NeXT-34B
COCO-Cap cococap_val_2014 CIDEr 40,504 108.66 113.88 107.66 96.98 99.45 103.16
COCO-Cap cococap_val_2017 CIDEr 5,000 110.38 115.61 109.22 99.93 101.99 105.89
DocVQA val ANLS 5,349 28.08 30.29 72.16 74.35 77.45 83.98
GQA testdev_balanced_instructions Acc 12,578 61.97 63.24 54.98 64.23 65.36 67.08
MultidocVQA val Anls/acc 5,187 16.65/7.21 18.25/8.02 41.4/27.89 44.42/31.32 46.28/32.56 50.16/34.93
NoCaps nocaps_eval CIDEr 4,500 105.54 109.28 96.14 88.29 88.27 91.94
OKVQA val Acc 5,046 53.44 58.22 54.77 44.25 46.27 46.84
POPE test F1Score 9,000 85.87 85.92 86.79 86.4 86.26 87.77
ScienceQA scienceqa-full Acc. 4,114 70.41 74.96 28.84 73.21 75.85 85.81
Refcoco all CIder 17,596 29.76 34.26 9.47 34.2 34.75 33.56
Refcoco+ all CIder 7,578 28.92 31.01 9.05 31.82 32 30.66
Refcocog all CIder 12,596 57.76 59.23 19.35 52.18 58.02 59.26
ScienceQA scienceqa-img Acc 2,017 70.43 72.88 28.56 70.15 73.57 81.85
SEED-Bench Seed-1 Image-Acc 17,990 60.49 67.06 65.97 64.74 65.64 69.55
SEED-Bench-2 Seed-2 Acc 24,371 57.89 59.88 60.83 59.88 60.72 64.98
TextCaps val CIDEr 3,166 98.15 103.92 70.39 71.79 67.39 67.11
TextVQA val exact_match 5,000 46.07 48.73 65.76 64.85 66.92 69.31
VizWiz(val) val Acc 4,319 54.39 56.65 63.79 60.64 63.56 66.61
VQAv2 val Acc 214,354 76.64 78.26 80.32 80.06 80.92 82.07
WepresentadditionalresultsusingLMMS-EVAL here. Duetolimitedcomputationalresources,we
areonlyabletoprovideaholisticviewofmodelsfromtheLLaVA[40]series. Thisdemonstratesthat
achievingbothwidecoverageandlow-costevaluationsimultaneouslyisnotfeasible,necessitatinga
balancebetweenthesetwoaspects.
G Curatingmoredatasetsin LMMS-EVAL LITE
Table9: LMMS-EVAL LITE withmoredatasets,wherewefixedthesizeoftheLiteversionand
includemorefieldsanddatasetsforamorewholisticanddiverseevaluationforswiftdevelopment
TaskDomain Dataset Split FullSize LiteSize
ChartQA test 2500 500
Doc&InfographicUnderstanding DocVQA val 5349 500
InfoVQA val 2801 500
Flickr30k val 31784 500
NoCaps val 4500 500
ImageUnderstanding&Captioning TextCaps val 3166 500
RefCOCO val 8811 500
COCO val 5000 500
GQA test 12578 500
OKVQA val 5046 500
VisualQuestionAnswering VizWiz-VQA val 4319 500
VQA-V2 val 214354 500
TextVQA val 5000 500
MathVista testmini 1000 1000
Math&Science
AI2D test 3088 500
VisualDialogue LLaVA-W test 60 60
MM-Bench cn-dev 4329 500
MM-Bench en-dev 4377 500
MME cog.&percep. 2374 2374
Multi-discipline
MMMU val 900 900
CMMMU val 900 900
Seed-Bench test 17990 500
- Total - 340226 13734
Weappliedthesamealgorithmtoadditionaldatasetstodevelopamorecomprehensiveanddiverse
Liteversion. IncontrasttotheoriginalLMMS-EVALLITE,ourversionincorporatesmoredatasets,
includingCOCO[39]andVQA[21].
21H K-CenterGreedyalgorithm
Algorithm1k-Center-Greedy
Input: datax and|V|=n
i
Initializes=ϕ
while|s|<ndo
u=argmax min ∆(x ,x )
i∈D\s j∈s i j
s=s∪{u}
endwhile
returns
Thegreedyalgorithmweusefork-centerclusteringisdetailedinAlgorithm1. Ink-centerclustering,
theobjectiveistoselectkpointsamongV verticessuchthatthemaximumdistancefromanypoint
inV toitsnearestclustercenterisminimized. Intheemployedgreedyalgorithm,arandompoint
isinitiallychosenasacenter. Subsequently, thedistancefromthiscentertoeveryotherpointis
updated. Thepointwiththemaximumdistancefromthecurrentcentersisthenselectedandaddedto
thecenterlist. Thisprocessisrepeateduntilkcenterpointshavebeenidentified.
I LiveBenchDetails
I.1 WebsiteCandidatesforLiveBench
To evaluate the performance and reliability of various news and information sources, a diverse
set of websites has been selected for LIVEBENCH. We present the websites in Table 10. These
websitesspanmultiplecategories,ensuringcomprehensivecoverageofdifferentdomainssuchas
general news, business, technology, and international affairs. The list of candidate websites for
LIVEBENCHincludesprominentsourceslikeBBC,CNN,Bloomberg,WSJ,andReuters,among
others. Each of these websites has been categorized based on its primary content focus. This
categorizationaidsinthesystematicevaluationofthecontentqualityandtheimpactofimageryand
reportingstylesacrossdifferentdomains. Itshouldbenotedthatthisisainitialsetofcandidate
websitesandtheremaybechangesdependingonthesituationsofthesewebsites.
I.2 ExamplesfromLiveBench-2024-06
Figures10and11illustrateselectedexamplesfromtheLiveBench-2024-06evaluation. These
figurescategorizeresultsintothreedistincttypes: BasicUnderstanding,ContextualAnalysis,and
BroaderImplications.
22Table10: ListofwebsitesselectedforLIVEBENCH.
Name URL Category
BBCMain https://www.bbc.com/ GeneralNews
BBCNews https://www.bbc.com/news News
BBCSport https://www.bbc.com/sport Sports
BBCBusiness https://www.bbc.com/business Business
BBCInnovation https://www.bbc.com/innovation Innovation
BBCCulture https://www.bbc.com/culture Culture
BBCTravel https://www.bbc.com/travel Travel
BBCFuturePlanet https://www.bbc.com/future-planet Environment
CNNMain https://edition.cnn.com/ GeneralNews
CNNPolitics https://edition.cnn.com/politics Politics
CNNEntertainment https://edition.cnn.com/entertainment Entertainment
CNNStyle https://edition.cnn.com/style Style
BloombergEconomics https://www.bloomberg.com/economics Economics
BloombergIndustries https://www.bloomberg.com/industries Industries
BloombergTechnology https://www.bloomberg.com/technology Technology
BloombergPolitics https://www.bloomberg.com/politics Politics
BloombergOpinion https://www.bloomberg.com/opinion Opinion
WSJMain https://www.wsj.com/ GeneralNews
WSJAfrica https://www.wsj.com/world/africa?mod=nav_top_subsection Africa
WSJAmericas https://www.wsj.com/world/americas?mod=nav_top_subsection Americas
WSJAsia https://www.wsj.com/world/asia?mod=nav_top_subsection Asia
WSJChina https://www.wsj.com/world/china?mod=nav_top_subsection China
WSJEurope https://www.wsj.com/world/europe?mod=nav_top_subsection Europe
WSJMiddleEast https://www.wsj.com/world/middle-east?mod=nav_top_subsection MiddleEast
WSJIndia https://www.wsj.com/world/india?mod=nav_top_subsection India
WSJOceania https://www.wsj.com/world/oceania?mod=nav_top_subsection Oceania
WSJRussia https://www.wsj.com/world/russia?mod=nav_top_subsection Russia
WSJUK https://www.wsj.com/world/uk?mod=nav_top_subsection UK
WSJScience https://www.wsj.com/science?mod=nav_top_subsection Science
WSJArchaeology https://www.wsj.com/science/archaeology?mod=nav_top_subsection Archaeology
WSJBiology https://www.wsj.com/science/biology?mod=nav_top_subsection Biology
WSJEnvironment https://www.wsj.com/science/environment?mod=nav_top_subsection Environment
WSJPhysics https://www.wsj.com/science/physics?mod=nav_top_subsection Physics
WSJSpace https://www.wsj.com/science/space-astronomy?mod=nav_top_subsectionSpace
WSJCentralBanking https://www.wsj.com/economy/central-banking?mod=nav_top_subsectionCentralBanking
WSJConsumers https://www.wsj.com/economy/consumers?mod=nav_top_subsection Consumers
WSJHousing https://www.wsj.com/economy/housing?mod=nav_top_subsection Housing
WSJJobs https://www.wsj.com/economy/jobs?mod=nav_top_subsection Jobs
WSJTrade https://www.wsj.com/economy/trade?mod=nav_top_subsection Trade
WSJGlobal https://www.wsj.com/economy/global GlobalEconomy
WSJAI https://www.wsj.com/tech/ai?mod=nav_top_subsection AI
WSJBiotech https://www.wsj.com/tech/biotech Biotech
WSJCybersecurity https://www.wsj.com/tech/cybersecurity?mod=nav_top_subsection Cybersecurity
WSJPersonalTech https://www.wsj.com/tech/personal-tech?mod=nav_top_subsection PersonalTech
ReutersMain https://www.reuters.com/ GeneralNews
ReutersAerospaceandDefense https://www.reuters.com/business/aerospace-defense/ AerospaceandDefense
ReutersAutosandTransportationhttps://www.reuters.com/business/autos-transportation/ AutosandTransportation
ReutersDavos https://www.reuters.com/business/davos/ Davos
ReutersEnergy https://www.reuters.com/business/energy/ Energy
ReutersEnvironment https://www.reuters.com/business/environment/ Environment
ReutersFinance https://www.reuters.com/business/finance/ Finance
ReutersHealthcare https://www.reuters.com/business/healthcare-pharmaceuticals/ Healthcare
ReutersMediaandTelecom https://www.reuters.com/business/media-telecom/ MediaandTelecom
ReutersRetailandConsumer https://www.reuters.com/business/retail-consumer/ RetailandConsumer
ReutersFutureofHealth https://www.reuters.com/business/future-of-health/ FutureofHealth
ReutersFutureofMoney https://www.reuters.com/business/future-of-money/ FutureofMoney
ReutersTakeFive https://www.reuters.com/business/take-five/ Analysis
ReutersWorldatWork https://www.reuters.com/business/world-at-work/ WorldatWork
ReutersBreakingviews https://www.reuters.com/breakingviews/ Opinion
ReutersTechnology https://www.reuters.com/technology/ Technology
ReutersCybersecurity https://www.reuters.com/technology/cybersecurity/ Cybersecurity
ReutersSpace https://www.reuters.com/technology/space/ Space
ReutersDisrupted https://www.reuters.com/technology/disrupted/ Disruption
ReutersMomentum https://www.reuters.com/technology/reuters-momentum/ Technology
ReutersInvestigations https://www.reuters.com/investigations/ Investigations
AndreessenHorowitz https://a16z.com/news-content/#latest Technology
HackerNews https://news.ycombinator.com/ Technology
Reddit https://www.reddit.com/?rdt=48006 SocialMedia
CrunchbaseNews https://news.crunchbase.com/ Startups
CCTV https://www.cctv.com/ InternationalNews
23Figure10: ExamplesonLiveBench. ThisfigureillustratesqualitativeresultscategorizedintoBasic
Understanding,ContextualAnalysis,andBroaderImplications. Eachcategorypresentsaquestion
relatedtoanarticleandthecorrespondingground-truthanswer.
24Figure11: ExamplesonLiveBench. ThisfigureillustratesqualitativeresultscategorizedintoBasic
UnderstandingandComparativeAnalysis. Eachcategorypresentsaquestionrelatedtoanarticleand
thecorrespondingground-truthanswer.
25