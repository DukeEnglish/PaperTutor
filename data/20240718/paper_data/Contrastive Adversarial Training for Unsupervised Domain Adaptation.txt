Contrastive Adversarial Training for Unsupervised
Domain Adaptation
JiahongChen1,ZhilinZhang1,XinLi1,BehzadShahrasbi1,andArjunMishra1
Amazon
{jiahoc, zzhilin, lilc, behzadsb, misarjun}@amazon.com
Abstract. Domainadversarialtraininghasshownitseffectivecapabilityforfind-
ing domain invariant feature representations and been successfully adopted for
variousdomainadaptationtasks.However,recentadvancesoflargemodels(e.g.,
vision transformers) and emerging of complex adaptation scenarios (e.g., Do-
mainNet)makeadversarialtrainingbeingeasilybiasedtowardssourcedomain
and hardly adapted to target domain. The reason is twofold: relying on large
amountoflabelleddatafromsourcedomainforlargemodeltrainingandlacking
oflabelleddatafromtargetdomainforfine-tuning.Existingapproacheswidely
focusedoneitherenhancingdiscriminatororimprovingthetrainingstabilityfor
thebackbonenetworks.Duetounbalancedcompetitionbetweenthefeatureex-
tractorandthediscriminatorduringtheadversarialtraining,existingsolutionsfail
tofunctionwelloncomplexdatasets.Toaddressthisissue,weproposedanovel
contrastiveadversarialtraining(CAT)approachthatleveragesthelabeledsource
domainsamplestoreinforceandregulatethefeaturegenerationfortargetdomain.
Typically,theregulationforcesthetargetfeaturedistributionbeingsimilartothe
sourcefeaturedistribution.CATaddressedthreemajorchallengesinadversarial
learning:1)ensurethefeaturedistributionsfromtwodomainsasindistinguish-
ableaspossibleforthediscriminator,resultinginamorerobustdomain-invariant
feature generation; 2) encourage target samples moving closer to the source in
thefeaturespace,reducingtherequirementforgeneralizingclassifiertrainedon
thelabeledsourcedomaintounlabeledtargetdomain;3)avoiddirectlyaligning
unpairedsourceandtargetsampleswithinmini-batch.CATcanbeeasilyplugged
intoexistingmodelsandexhibitssignificantperformanceimprovements.Wecon-
ductextensiveexperimentsonlargedomainadaptationbenchmarkdatasets,and
theresultsshowthatCATsignificantlyimprovesthebaselineadversarialtraining
methods from +0.5% to +4.4% on Visda-2017, from +1.0% to +2.7% on Do-
mainNet,andfrom+0.3%to+1.8%onOffice-Home,respectively.
Keywords: DomainAdaptation·AdversarialLearning
1 Introduction
Deep neural networks have achieved impressive performance in various computer vi-
siontasks.However,suchsuccessoftenreliesonalargeamountoflabeledtrainingdata,
ifwedirectlyadoptlargepretrainedmodelforindividualtasks,theperformancewillbe
degradedduetomis-alignmentbetweensourceandtargetdomain.Ontheotherhand,
itisinfeasibleandcostlytoobtainlabelleddatafortrainingdedicatedlargemodelsin
4202
luJ
71
]GL.sc[
1v28721.7042:viXra2 J.Chenetal.
mostreal-worldscenarios.UnsupervisedDomainAdaptation(UDA)addressedthela-
belmissingchallengesbytransferringknowledgefromalabel-richsourcedomaintoa
separateunlabeledtargetdomain[2].Overthepastyears,manyadversarialUDAadap-
tationmethodshavebeenproposed,whichcommonlyleveragestheideaofadversarial
trainingtolearnthedomain-invariantfeaturerepresentationbyusingadomaindiscrim-
inatortocompetewithfeaturegenerator[7,15,20].TheseUDAmethodsareusuallyap-
pliedinconjunctionwithapretrainedConvolutionalNeuralNetwork(CNN)backbone
(e.g.,ResNet[9])andworkwellforsmalltomediumsizedimagesclassificationtasks,
such as Office-Home [26] and VisDA [18]. However, the performance of CNN-based
adversarialapproachonlarge-scaledatasetslikeDomainNet[17]isunfavorable.Toad-
dresssuchissue,recentresearcheshavewidelyleveragethevisiontransformers(ViT)
toresolvethechallengingcross-domainadaptionproblems,i.e.,unsuperviseddomain
adaptation (UDA) task. Due to the success of transformer architecture, ViT-based ap-
proachesshownsubstantialimprovementsovertheCNN-basedframeworks[29,30,32].
The above existing adversarial training methods widely consist of two parts: 1) a
standardfeed-forwardneuralnetwork(usuallyincludingafeaturegeneratorandalabel
predictor,i.e.,classifier);and2)adiscriminator.Thediscriminatorconnectstothefea-
ture extractor via a gradient reversal layer (GRL). During the back-propagation, GRL
forcesthefeaturedistributionsoverthetwodomainstobesimilarthroughmultiplying
thegradientsbyacertainnegativeconstant.Inthisway,sourceandtargetdomainsbe-
comeindistinguishableforthediscriminator.Ontheotherhand,theclassifierforcesthe
featuregeneratortoextractrobustdiscriminativefeaturesforthelabeledsourcedomain.
Throughsuchadversariallearning,thediscriminatorcompeteswiththefeaturegener-
atorforextractingthedomain-invariantfeaturesacrossthetwodomains.It,therefore,
enablesasmoothknowledgeadaptationfromsourcedomaintotargetdomain.
Consideringbothbackbonenetworksandadaptationscenariosbecomemorecom-
plex over time, traditional adversarial training methods becomes less effective. Re-
searchers have invested a lot of efforts in improving the domain adversarial training
by improving discriminator [15], or improving the training stability for the backbone
network [20,23]. However, existing adversarial training approaches purely relies on
GRLstoforcesourceandtargetdomainsappearingsimilarinordertobettercompete
with discriminator. It limits the capability of adversarial training between the feature
extractorandthediscriminator.Typically,theimprovementofthefeatureextractorwill
leadtofurtherbiasedfeaturegenerationtowardlabeledsourcedata,whichinthemean-
time, makes the features from two data domains dis-similar and makes the domain
discrimination takes become easier. As a result, it degrades the performance for tar-
getdomainclassification.Additionally,duetorandomsampling,thesourceandtarget
samplesareunpairedwithineachmini-batch.Withouttheknowledgeoftargetlabels,
itisinfeasibletodirectlyalignsourceandtargetfeaturestoimprovethedifficultyfor
discriminator[27].Therefore,asmorepowerfulbackbonenetworksanddiscriminators
aredeveloped,thebalancedcompetitionbetweenthefeatureextractorandthediscrim-
inatorduringtheadversarialtrainingiseasilydestroyed.
To tackle such imbalanced competition between the feature extractor and the dis-
criminator,weproposeanovelContrastiveAdversarialTraining(CAT)plug-inforad-
versarialUDAs(seeFig.1(a)).CATexplicitlyforcesthefeaturesfromtwodatadomainCAT 3
(a) (b)
Fig.1: Overview of Contrastive Adversarial Training (CAT). (a) highlights the location of our
CAT component in the model, which serves as a reinforcement for the GRL process to make
the domain classification more difficult for the discriminator; (b) illustrates that contrastively
aligning target samples to it’s source anchor can simultaneously increase the competitiveness
oftheadversarialtrainingforamorerobustdomaininvariantfeatureextractionandreducethe
domaindiscrepancytomoreeasilydrawdecisionboundariesforunlabeledtargetdomain.
becomesimilartoincreasethedifficultyofthediscriminatorfordistinguishingthesam-
plesfromtwodifferentdomains,whileperversetheperformanceofthesourceclassifier.
Specifically, CAT mitigates the divergence between source and target in a contrastive
way:sourcedomainsamplesservesastheanchors,whilesimilartargetdomainsamples
arepulledclosertotheanchorsanddissimilartargetsamplesarepushedaway.Consid-
eringthattherearenogroundtruthlabelsforthetargetdomainsamplestoalignwith
anchors,weleverageafeaturebanktofindthesimilaranddissimilartargetsamplesfor
the source domain samples at the global scale by gradually updating the feature bank
with newly seen samples in each mini-batch. To this end, the target samples and the
sourcesamplesfromthesameclassareareclusteredtogether,whichleadstotwoben-
efitsasshowninFig.1(b):1)Thedecisionboundarieslearntfromthesourcedomain
canalsobeeasilyappliedtothetargetduetothetarget-to-sourcealignment(intraclass
alignment);2)Itbecomesmoredifficultforthediscriminatortotelltheoriginaldomain
ofthesamplesduetothecross-domaininterclassalignment,improvingtherobustness
forbothfeatureextractoranddiscriminatorduringtheadversarialtraining.
To the best of our knowledge, CAT is the first adversrial UDA approach to lever-
agetheContrastiveLearningprocessbetweensourceandtargetdomainforfacilitating
a more effective adversarial learning. Due to its universality and simplicity, CAT can
bestraightforwardlyintegratedintoadversarialUDAmethodsacrossdifferentdatasets,
makingithighlyaccessibleinpractice.Additionally,weextensivelyverifytheempir-
ical efficacy of CAT across large UDA datasets for image classification with Vision
Transformer(ViT)backbones[6].Insummary,wemakethefollowingcontributions:
– WeuncoverthattheadversarialtrainingforUDApartneedstobebalancedbyrein-
forcingGRLtocompetewithdiscriminator,leadingtoamorecompetitiveprocess
foradversrialtraining.
– Weproposeanovelcontrastivelossthatavoidstheunpairproblemindomainalign-
ment.Itusesthelabeledsourcedomainsamplesasanchorthatpullssimilarsam-
ples in target domain while pushing dis-similar target samples away, ensuring a
easydecisionboundaryandaharddomainclassification.4 J.Chenetal.
– The proposed model, CAT, can serve as a plug-in for adversarial UDAs and be
integratedintoexistingadversarialtrainingmodelseasily.
– Extensiveexperimentsareconductedonwidelytestedbenchmarks.Ourproposed
CATachievesthebestperformances,including93.3%onVisDA-2017,85.1%on
Office-Home,and58.0%onDomainNet.Itimprovesperformanceoftheadversar-
ial UDA models from +0.5% to +4.4% on Visda-2017, from +1.0% to +2.7% on
DomainNet,andfrom+0.3%to+1.8%onOffice-Home,respectively.
2 RelatedWork
UnsupervisedDomainAdaptation:UDAhasbeenproposedtoaddressthechallenges
inlearningthefeaturerepresentationsfromthesourcedomain’slabeleddataandgener-
alizeswellontheunseendatafromthetargetdomain[11,12].Ganinetal.firstproposed
Domain-AdversarialTrainingofNeuralNetworks(DANN),whichleveragethepower
ofadversarialtrainingtoaddressthecross-domaindiscrepancy[7].Then,Conditional
AdversarialDomainAdaptation(CDAN)isproposedtoimprovethediscriminatorwith
conditionalGAN[15].AdversarialDiscriminativeDomainAdaptation(ADDA)ispro-
posed to learn the mapping between source and target mappings to minimize the do-
maindivergence,makingthesourceclassifiercanbedirectlyappliedtothetargetfea-
ture [24]. Additionally, multiple methods focusing on improving the training stability
forthebackbonenetworkaredeveloped.SmoothDomainAdversarialTraining(SDAT)
addresses the problem of sub-optimal performance by leveraging the smoothness en-
hancing formulation for adversarial loss [20]. SSRT [23] adopts a safe training [14]
strategy to reduce the risk of model collapse and improve the effectiveness of knowl-
edgetransferbetweendomainswithlargegaps.
ContrastiveLearning:ContrastiveLearning(CL)isapopulartechniqueforself-
supervisedlearningofvisualrepresentations[3,10].Itusespairsofexamplestoextract
meaningful representations by contrasting positive and negative pairs of instances by
leveraging the assumption that similar samples should be in close proximity within
thelearnedfeaturespace,whiledissimilarinstancesshouldbepositionedfartherapart
[31]. CL has also been used in non-adversarial UDAs. Contrastive Adaptation Net-
work (CAN) is a non-adversarial UDA approach that explicitly models the intraclass
and interclass domain discrepancy [13]. Similarly, cross domain contrastive learning
(CDCL)[28]furtherimprovesthecontrastivelearningforUDAwithInfoNCE[8,16],
which minimizes the distance of a positive pair relative to all other pairs [22]. How-
ever,bothCANandCDCLfocusonnon-adversarialsetting,whichhelpstheclassifier
to accurately draw decision boundaries for unlabeled target domain, but ignored the
effectiveness of incorporating adversarial training to further extract domain invariant
loss.
3 Methods
3.1 ProblemFormulationforUDA
ThegeneralsettingofUDAconsistsofalabeledsourcedomainD ={(x(i),y(i))}N
s s s i=1
and a unlabeled target domain with D = {x(j)}M , where x, y denotes data sam-
t t j=1 · ·CAT 5
plesandlabelsrespectively;M,N representsthenumberofsamplesineachdomain.
UDA aims to learn a model h = g ◦ f, where g(·;θ ) : X → Z denotes the fea-
g
tureextractorthatprojectstheinputsamplesintoitsfeaturespace,f(·;θ ) : Z → Y
f
denotes the classifier that projects the features Z into logits. In the mean time, an ad-
versarialadaptationlearnsdomain-invariantfeatureviaabinarydomaindiscrimination
d(;θ ):Z →{0,1}thatmapsfeaturestodomainlabels.
d
ThemainlearningobjectiveforadversarialUDAconsistsofstandardsourceclas-
sificationlossL andanddomainadversariallossL :
cls d
L =−E L(f(g(x )),y ), (1)
cls (xs,ys)∈Ds s s
L =−E [logd(f(x ))]−E [log(1−d(f(x )))]. (2)
d xs∼Ds s xt∼Dt t
whereListhestandardcross-entropylossforclassificationthatguaranteelowersource
risk.Theoverallobjectivefunctionservingasthebackboneforadversarialtrainingis
asfollows:
minL=minL +L . (3)
cls d
3.2 ContrastiveSource-TargetAlignment
Toexplicitlyencouragecloserdistancefortwodatadomainsinfeaturespaces,wepro-
pose to mitigate the divergence between source and target in a contrastive way. Typi-
cally,sourcedomainsamplesactastheanchors,followedbypullingsimilartargetdo-
mainsamplestowardsthemandpushingdissimilaronesaway.Tofacilitatethisprocess,
weintroducefeatureclustering,aimingtogrouphighlyrelatedtargetfeaturestowards
thesourcesampleswhileconcurrentlycreatingseparationamongdissimilarones.
Inordertocalculatethesample-wisesimilarity,ourapproachdrawsinspirationfrom
theprinciplesofunsupervisedrepresentationlearning’sneighborhooddiscovery,asex-
emplifiedinpriorworkssuchas[21],[25],and[31].Tokeeptrackofunlabeledtarget
domain features, we build a feature bank (B) to help the contrastive learning process.
Typically,wecollectthefeaturesofthetargetdomainsamples,B ={z =g(x )|x ∈
t t t
D }, and continuously update B during training. This iterative update mechanism al-
t
lows all target domain samples’ features to be continuously improved throughout the
trainingprocess.
Foreachsourcesamplex(i) inthemini-batch,wecalculatethesimilarity,p ,be-
s ij
tweenitssourcefeaturez(i) andthetargetfeaturez(j) ofatargetsamplex(j) interms
s t t
ofsoftmaxfunctiontocaptureshowmuchagivenanchorsourcefeaturez(i)alignswith
s
thetargetfeaturez(j)
t
ez( si)·z t(j)
p = . (4)
ij (cid:80)M k=1ez( si)·z( tk)
Equation4canbeinterpretedastheprobabilityindicatingthesimilaritybetweena
targetfeaturez(j) iscomparedtoasourcefeaturez(i).Then,thelikelihoodofsource-
t s
target similarity can be denoted as the product of probabilities, following the work of6 J.Chenetal.
Yangetal.[31]:
(cid:89) (cid:89)
P(z (i)|C ,θ )= p , P(z (i)|D ,θ )= p
s i g ij s i g ij (5)
j∈Ci j∈Di
whereC istheclosesetforsourceanchorz(i)andD denotesthedistantset.Theclose
i s i
set C consists of features of the target domain’s samples that are considered close to
i
sourceanchorz(i),whilethedistantsetD includesfeaturesoftargetdomainsamples
s i
that are considered distant from the anchor. To define the neighbor set C (z(i)) for a
i s
querysamplex(i),weleveragetheconceptofK-NearestNeighbors(KNN)[4]from
s
thetargetdomainbasedonthecosinesimilarityinthefeaturespace:
C ={argminK [g(x(i))·g(x(n)])}. (6)
i n=1,g(x(n))∈B s t
t
Similarly, the distant set D (z(i)) for a query sample x(i) is defined by its K-
i s s
Farthest Neighbors from the target domain based on the cosine similarity, as shown
below:
D ={argmaxK [g(x(i))·g(x(m))]}. (7)
i m=1,g(x(m))∈B s t
t
Intuitively, for each source sample x (i), the corresponding target features in D
s i
should be less similar to its feature z (i) than those in C . This process involves mov-
s i
ing target features in C towards z(i) while pushing away target features in D , en-
i s i
abling contrastive learning to identify and cluster discriminative features from unla-
beled data within the target domain. Our goal is to maximize the likelihood of target
featureclusteringinacontrastivemannerbysimultaneouslymaximizingthelikelihood
of the neighbor set and minimizing the likelihood of the distant set. The optimization
objectivecanbeformulatedasfollows:
(cid:20) P(z (i)|C ,θ )(cid:21)
maxlog s i g , (8)
P(z (i)|D ,θ )
s i g
whichcanbeequivalentlyexpressedastheminimizationoftheproductofprobabilities
intheawaysetD dividedbytheproductofprobabilitiesintheneighborset:
i
(cid:34) (cid:81) (cid:35)
p
maxlog n∈Ci in .
(cid:81)
p
m∈Di im
(cid:34) (cid:35)
(cid:89) (cid:89)
=max log p in−log p im (9)
n∈Ci m∈Di
(cid:34) (cid:35)
(cid:88) (cid:88)
=max log(p )− log(p )
in im
n∈Ci m∈DiCAT 7
WecanthenrewriteEqn.9usingEqn.4:
(cid:34) (cid:35)
(cid:88)
ez( si)·z( tn)
(cid:88)
ez( si)·z( tm)
max log( )− log( )
n∈Ci
(cid:80)M k=1ez( si)·z( tk)
m∈Di
(cid:80)M k=1ez( si)·z( tk)
(10)
(cid:34) (cid:35)
=max (cid:88) z(i)·z(n)− (cid:88) z(i)·z(m) .
s t s t
n∈Ci m∈Di
MaximizingEqn.10canbeachievedbytheoptimizationinacontrastiveway,i.e.,
maximizingdotproductionbetweensourceanchorandtargetfeaturesinthecloseset,
whileminimizingtheonesforthedistantset.FurtherexpandingEqn.10canderiveit
intheformoftripletcontrastiveloss[1]:
(cid:34) (cid:35)
max (cid:88) z(i)·z(n)− (cid:88) z(i)·z(m)
s t s t
n∈Ci m∈Di
(cid:34) (cid:35)
∝min 2· (cid:88) z(i)·z(m)−2· (cid:88) z(i)·z(n) (11)
s t s t
m∈Di n∈Ci
(cid:34) (cid:35)
≤min (cid:88) ||z(i)−z(n)|| − (cid:88) ||z(i)−z(m)|| ,
s t 2 s t 2
n∈Ci m∈Di
where ||z(i) −z(n)|| can be regarded as the l -norm between the source anchor and
s t 2 2
target features in the close set C , ||z(i) −z(m)|| is the l -norm between the source
i s t 2 2
anchorandtargetfeaturesinthedistantsetD .Therefore,minimizingthelossfunction
i
inEqn.11willpullthesimilartargetfeaturesintheclosesettowardsthesourceanchor,
whilepushingdis-similartargetfeaturesinthedistantsetawayfromtheanchor.
Additionally,optimizingEqn.11willresultinalowerboundofEqn.8,revealing
that the maximum likelihood can be achieved by simultaneously minimizing the loss
amongsourceanchor,closetargetfeatures,anddistanttargetfeatures.
This optimization objective effectively encourages the contrastive model’s feature
representation, learned through the feature generator g, to bring target samples in the
sameclassasthesourceanchorclosertoeachotherinthefeaturespace,whilesimulta-
neouslydistancingdis-similartargetsamplesofdifferentclasses.Tothisend,thesource
andtargetfeaturesbecomemoredistinguishable,makingit1)harderforthediscrimi-
natortoidentifythedomainofinputsamples;2)easierfortheclassifiertobeapplied
intheunlabeledtargetfeatures.
The contrastive loss from Equation 11 can be regarded as the backbone for our
newcontrastiveadversarialtraining.Itcanbedirectlyintegratedintootheradversarial
training methods such as CDAN, SDAT, SSRT. The detailed implementation for the
proposedadversarialtrainingplug-in,CAT,issummarizedinAlgorithm1.
4 Experiments
We evaluate our proposed method, CAT, on three datasets: DomainNet, VisDA-2017,
andOffice-Home,byintegratingittoseveralstate-of-theartalgorithmswithViTback-8 J.Chenetal.
Algorithm1:TheimplementationofcontrastiveadversarialtrainingforCAT
Input:datasamples/labelsD ,D ,neighborhoodsizeK,batchsizeN′,
s t
hyper-parameterγ,networkstructuresg,f,coefficientλ.
Output:networkparametersθ ,θ .
g f
1 Initializenetworkparametersθ g,θ f;
2 forsampledminibatch X s,Y s,X tdo
3 #getmodeloutputs
4 Z s ←g(X s;θ g),Z t ←g(X t;θ g);
5 Yˆ s ←f(Z s;θ f);
6 #pairwisesimilarity
7 fori∈{1,··· ,N},j ∈{1,...,M}do
8 D cos(z( si),z( tj))=z( si)·z t(j)/||z( si)||·||z t(j)||
9 end
10 #generateneighborsetandawayset
11 C i ={argminK j=1[D cos(z( si),z t(j))]};
12 D i ={argmaxK j=1[D cos(z( si),z( tj))]};
13 #calculatecontrastiveloss
(cid:34) (cid:35)
N′
14 L con = N1 ′ (cid:80) (cid:80) ||z( si)−z( tn)|| 2− (cid:80) ||z( si)−z( tm)|| 2 ;
i=1 n∈Ci m∈Di
15 #calculateoverallloss
16 L=L cls+L d+λL con;
17 UpdatemodelparameterswithGRL→θ g,θ f.
18 end
bone.TheexperimentswereranonAWSSageMakerP3instanceswithNVIDIATesla
V100 GPUs. All experiments were conducted 5 times and the average accuracy isre-
ported.
4.1 Datasets
DomainNet [17]: DomainNet is the largest dataset to date for domain adaptation,
whichconsistsof 600Kimagesfor345differentclasses.Intheexperiment,wefollow
thesetupinSDATtotest20differentadaptationscenariosacross5differentdomains:
clipart(clp),infograph(inf),painting(pnt),real(rel),andsketch(skt).
VisDA-2017 [19]: VisDA is a large domain adaptation dataset for the transition
fromsimulationtorealworld,whichcontains280Kimagesfor12differentclasses.
Office-Home[26]:Office-Homeisacommonbenchmarkdatasetfordomainadap-
tation which contains 15,500 images from 65 classes and 4 domains: Art (A), Clipart
(C),Product(Pr)andRealWorld(R).CAT 9
4.2 ImplementationDetails
ForUDAtraining,wefollowSDATandMIC[11,20]byadoptingtheTransferLearn-
ing Library1 for implementation and using the same training parameters: SGD with a
learning rate of 0.002, a batch size of 32. For DomainNet, we follow SSRT [23] with
thesametrainingparameters(SGDwithalearningrateof0.004,abatchsizeof32).We
usedViT-B/16[6]pretrainedonImageNet[5]asthebackboneacrossallexperiments.
4.3 Results
As shown in Tables 1 to 3, CAT achieves significant and consistent performance im-
provements for existing adversarial UDA approaches over various datasets and do-
mains.
VisDA-2017:FortheVisDA-2017dataset,weaddourproposedadversarialtraining
plug-in,CAT,toexistingadversarialUDAalgorithmsincludingCDAN,MCC,SDAT,
andMICandshowtheresultsinTable1,wheregreennumbersinthebucketssuggest
out-performwhilerednumberssuggestunder-perform.Wecanseethattheadditionof
CATimprovestheperformanceonallbaselineadversrialUDAapproachesacrossma-
jority of source-to-target adaptation scenarios. MIC+CAT improves the SoTA adver-
sarial UDA (MIC) performance on the VisDA-2017 by 0.5% to 93.3%. Additionally,
CAT also enhances the performance of other listed adversarial UDA approaches by a
marginfrom1.2%to4.4%.ThisindicatesthattheproposedCATcanconsistentlyand
effectivelyimprovetheadversarialtrainingandincreasetheoverallmodelperformance.
Table1:Imageclassificationacc.in%onVisDA-2017forUDA.
Method Pln Bik Bus Car Hrs Knf Mcy Per Plt Skb Trn Trk Avg.
TVT 92.9 85.6 77.5 60.5 93.6 98.2 89.3 76.4 93.6 92 91.7 55.7 83.9
CDTrans 97.1 90.5 824 77.5 96.6 96.1 93.6 88.6 97.9 869 90.3 62.8 88.4
PMTrans 98.9 93.7 84.5 73.3 99.0 98.0 96.2 67.8 94.2 98.4 96.6 49.0 87.5
SSRT 98.9 87.6 89.1 84.8 98.3 98.7 96.3 81.1 94.8 97.9 94.5 43.1 88.8
CDAN 94.3 53.0 75.7 60.5 93.9 98.3 96.4 77.5 91.6 81.8 87.4 45.2 79.6
w.CAT 96.2 81.0 83.8 55.2 91.7 97.6 93.3 78.2 95.5 93.0 90.3 52.0 84.0(4.4)↑
MCC 96.2 80.2 78.5 68.9 96.9 98.9 94.1 79.8 96.1 94.7 92.4 60.5 86.4
w.CAT 97.9 88.3 79.9 85.3 97.4 96.5 90.2 82.1 95.1 97.2 92.6 58.1 88.3(1.9)↑
SDAT 98.4 90.9 85.4 82.1 98.5 97.6 96.3 86.1 96.2 96.7 92.9 56.8 89.8
w.CAT 98.4 92.3 88.2 82.8 98.6 98.6 94.4 86.5 97.8 98.4 92.7 62.7 91.0(1.2)↑
MIC 99.0 93.3 86.5 87.6 98.9 99.0 97.2 89.8 98.9 98.9 96.5 68.0 92.8
w.CAT 99.0 94.3 90.1 87.0 99.0 99.5 95.5 90.4 98.9 98.9 95.8 71.1 93.3(0.5)↑
DomainNet:Table2showstheexperimentresultsonthelargestandmostchalleng-
ingDomainNetdatasetacrossfivedomains.Weconductedacomprehensiveexperiment
1https://github.com/thuml/Transfer-Learning-Library10 J.Chenetal.
overseveraladversarialUDAapproachesandreportedtheirperformancewiththepro-
posedCATplug-in.Overall,CATimprovesnearlyalloftheadaptationscenariossig-
nificantlywithonlytwominorunder-perform(-0.1%and-0.2%).Specifically,withthe
help of CAT, the average performance of the SoTA algorithm, SSRT, is improved by
1.0%to58.0%,whichisasignificantliftconsideringthelargenumberofclasses(345
classes) and images present in DomainNet. The performance of DANN and SDAT is
alsoimprovedby1.6%and2.7%,respectively.Onspecificsource-to-targetadaptation
scenarioslikeinf→skt,theSSRT+CATperformanceincreaseisupto5.1%.
Table2:Imageclassificationacc.in%onDomainNetforUDA.
DANN
DANN clp inf pnt rel skt Avg. clp inf pnt rel skt Avg
+CAT
- 30.9 53.3 72.7 55.4 53.1 - 31.5 55.6 73.6 57.0 54.4
clp clp
- (0.6) (2.3) (0.9) (1.6) (1.4)
43 - 40.8 56.4 35.9 44.0 47.3 - 45.6 57.9 40.7 47.9
inf inf
(4.3) - (4.8) (1.5) (4.8) (3.9)
55.7 28.6 - 70.5 48.3 50.8 56.6 29.4 - 70.5 49.8 51.6
pnt pnt
(0.9) (0.8) - (0.0) (1.5) (0.8)
62.3 32.5 62.5 - 50.7 52.0 64.1 33.9 63.1 - 51.8 53.2
rel rel
(1.8) (1.4) (0.6) - (1.1) (1.2)
66.4 30.6 58 70.1 - 56.3 67.0 31.1 59.1 70.1 - 56.8
skt skt
(0.6) (0.5) (1.1) (0.0) - (0.5)
56.9 30.7 53.7 67.4 47.6 51.2 58.8 31.5 55.9 68.0 49.8 52.8
Avg. Avg.
(1.9) (0.8) (2.2) (0.6) (2.3) (1.6)
SDAT
SDAT clp inf pnt rel skt Avg. clp inf pnt rel skt Avg.
+CAT
- 28.2 51.5 68.6 55.9 51.1 - 31.5 55.5 72.7 58.5 54.6
clp clp
- (3.3) (4.0) (4.1) (2.6) (3.5)
40.3 - 41.7 53.9 35.4 42.8 43.7 - 45.4 57.6 37.8 46.1
inf inf
(3.4) - (3.7) (3.7) (2.4) (3.3)
50.9 27.5 - 64.8 45.3 47.1 53.4 28.4 - 67.0 47.3 49.0
pnt pnt
(2.5) (0.9) - (2.2) (2.0) (1.9)
59.8 32.1 61.3 - 49.0 50.6 62.7 33.2 63.4 - 50.7 52.5
rel rel
(2.9) (1.1) (2.1) - (1.7) (2.0)
62.3 29.2 53.6 62.8 - 52.0 65.6 30.7 56.8 65.2 - 54.6
skt skt
(3.3) (1.5) (3.2) (2.4) - (2.6)
53.3 29.3 52.0 62.5 46.4 48.7 56.4 31.0 55.3 65.6 48.6 51.4
Avg. Avg.
(3.1) (1.7) (3.3) (3.1) (2.2) (2.7)
SSRT
SSRT clp inf pnt rel skt Avg. clp inf pnt rel skt Avg.
+CAT
- 33.8 60.2 75.8 59.8 57.4 - 34.6 60.7 75.7 60.6 57.9
clp clp
- (0.8) (0.5) (-0.1) (0.8) (0.5)
55.5 - 54.0 68.2 44.7 55.6 57.3 - 55.7 68 49.8 57.7
inf inf
(1.8) - (1.7) (-0.2) (5.1) (2.1)
61.7 28.5 - 71.4 55.2 54.2 62.2 31.0 - 71.9 56.1 55.3
pnt pnt
(0.5) (2.5) - (0.5) (0.9) (1.1)
69.9 37.1 66.0 - 58.9 58.0 70.9 37.2 66.4 - 60.1 58.7
rel rel
(1.0) (0.1) (0.4) - (1.2) (0.7)
70.6 32.8 62.2 73.2 - 59.7 71.4 32.9 63.3 73.4 - 60.3
skt skt
(0.8) (0.1) (1.1) (0.2) - (0.6)
64.4 33.1 60.6 72.2 54.7 57.0 65.5 33.9 61.5 72.3 56.7 58.0
Avg. Avg.
(1.1) (0.8) (0.9) (0.1) (2.0) (1.0)
Office-Home:AsseeninTable3-thequantitativeresultsonOffice-Homedataset,
our proposed CAT plug-in achieves notable performance gains on CDAN, MCC, and
SDAT approach, as well as other Vit-based approaches. When comparing the perfor-CAT 11
Table3:Imageclassificationacc.in%onOffice-HomeforUDA.
Method AC AP AR CA CP CR PA PC PR RA RC RP Avg.
CDTrans 68.8 85.0 86.9 81.5 87.1 87.3 79.6 63.3 88.2 82 66 90.6 80.5
TVT 74.9 86.8 89.5 82.8 87.9 88.3 79.8 71.9 90.1 85.5 74.6 90.6 83.6
CDAN 62.6 82.9 87.2 79.2 84.9 87.1 77.9 63.3 88.7 83.1 63.5 90.8 79.3
w.CAT 65.1 84.3 88.2 82.6 85.0 87.5 81.2 64.6 89.0 85.3 69.0 91.2 81.1(1.8)↑
MCC 67.0 84.8 90.2 83.4 87.3 89.3 80.7 64.4 90.0 86.6 70.4 91.9 82.2
w.CAT 67.1 88.9 89.7 84.1 87.4 89.5 81.8 64.6 90.2 85.2 70.1 91.3 82.5(0.3)↑
SDAT 70.8 87.0 90.5 85.2 87.3 89.7 84.1 70.7 90.6 88.3 75.5 92.1 84.3
w.CAT 75.6 87.5 90.4 87.7 88.6 89.6 83.5 72.8 90.2 87.0 76.4 92.4 85.1(0.8)↑
mance improvement across different domain datasets, CAT particularly gains bigger
performance improvement for the domain adaptation scenarios such as AC, PC, RC
that have exhibits less promising results by the base algorithms. When comparing the
performance improvement across different base algorithms, the CAT plug-in obtains
bigger performance improvement over less complex algorithms, indicating CAT’s ef-
fectivenessonincreasingthedifficultyforthediscriminator.Overall,theresultsshow
thatourproposedmethod,CAT,canexplicitlygeneralizedomaininvariantfeaturesby
improvingthedomainadversarialtrainingprocess.
5 AblationStudyandDiscussions
EffectivenessofCAT:AsshowninTables1-3,existingadversarialUDAapproaches
receivedsubstantialperformanceimprovementsbyintegratingwithCATinalldatasets.
Especially in DomainNet dataset, which is the largest UDA dataset to date, CAT im-
proves58outof60comparedalgorithmandadaptationscenariocombinations.Itshows
theeffectivenessofincreasingthecompetitivenessofadversarialtraining.
Additionally, to validate the effectiveness of CAT’s feature alignment capability,
wecompareitsperformancewithanotherfeaturealignmentapproach,KL-divergence
(KLD),inTable4.TheresultsforCDANandMICshowthatusingKLDcanslightly
increasetheperformancefortheoriginalmodelsbutlesseffectivecomparingtoCAT,
indicating that the adversarial training is improved due to the increased difficulty for
domainclassification(i.e.,thediscriminator).However,forMCCandSDAT,duetothe
unpaired issue of unlabeled data samples, samples from different class were aligned,
which damages the their generalization and the performance of the classifier also de-
creases.
Table4:EffectiveofCATvsKLD.
CDAN MCC SDAT MIC
Baseline 79.6 86.4 89.8 92.8
+KLD 81.1 86.1 89.5 92.9
+CAT 84.0 88.3 91.0 93.312 J.Chenetal.
ImpactofcontrastivecoefficientλandneighborsizeK:Westudytheimpactof
hyperparametersofCAT,λandK,onVisDA-2017withMICbackbone.Wevaryλin
[0.1,1,5,10] and K in [1,2,5,15]. The results are presented in Fig. 2. Generally, our
model is less sensitive to the change of λ, where the model performance is peaked at
93.3whenwechooseλ=1,5.TheperformanceofCATcontinuouslyoutperformsthe
baselinemethodbeforeλbecometoolarge.ThereasonisthatalargeλmayletL
con
dominatethelossfunctionandmakethemodelonlyfocusofsource-to-targetalignment
without making accurate source domain classification and domain discrimination. As
for the impact of neighbor size K, the model is more sensitive to it, where the best
modelperformance93.3isachievedwithK =5.Generally,alargerKleadstoabetter
modelperformance,indicatingthecontrastivelearningbetweenthesourceanchorand
similar target samples is more effective when we select more samples from the target
domain.However,whenthenumberofneighborsistoolarge,noisysamplesfromother
classesmightbeintroduced,hencetheperformancestartstodrop.
K
93.3 93.3
93.2
93.2
93.1
93.0 93.1
92.9 93.0
92.8
92.9
92.7
92.6 92.8
101 100 101 2 4 6 8 10 12 14
Fig.2:Impactofcontrastivecoefficientλandneighborsize.
Visualization: We compare our proposed method, CAT, with the model trained
withoutittoverifytheeffectivenessofintroducingthecontrastiveadversarialtraining.
Thevisualizationresultsusingt-SNEareshowninFig.3,wheredotsdenotessamples
fromsourcedomain,and+denotestheonesintargetdomain.Inthisvisualization,300
samples were randomly selected from each class in the VisDA-2017 dataset. We can
clearly see that the target features and the source features of the same class are fully
overlapped and collapse into one another, which indicates a desirable alignment be-
tween source and target domains. In contrast, for the methods without CAT, although
thesamplesfromthesameclassbutdifferentdomainsarealsomappedintoneighbor-
ingregion,thereisaclearboundarybetweenthetwoclustersofsamplesthatarefrom
differentdomains,whichshowsalesseffectivefeaturealignmentsbetweensourceand
targetdomains.ThecomparisonsuggeststhattheproposedCATcanexplicitlyreduce
thebiastowardsthelabeledsourcedomainandimprovetarget-to-sourcefeaturealign-
ment,whichattributestotheimproveddomainadaptationresultsshowninFig.3.
Additionally, compared to Fig. 3 (a) and (c), the features from different classes in
Fig.3(b)and(d)arebetterseparatedfromeachother,especiallyforclasseswithsimilar
objects such as Car, Truck, Bus, and Train. This suggests that the proposed CAT can
)%(
ycaruccACAT 13
(a)SDAT (b)SDAT+CAT
(c)MIC (d)MIC+CAT
Fig.3:Visualizationwitht-SNEforadaptationmethodsw/woCAT(bestviewedincolor).
effectivelyencourageinterclassseparationbyleveragethecontrastivetraining,leading
toaeasierdecisionboundaryfortheclassifier.
Effectiveness of cross-domain intraclass alignment: We also calculate the A-
distance[2]toquantitativelymeasurethedomaindistributiondiscrepancyforthedata
samplesusedint-SNEvisualization:
dist =2(1−2ϵ), (12)
A
where ϵ is the test error of a binary classifier trained 10 epochs to discriminate the
source samples from the target samples. The results show that before applying CAT,
theA-distanceis1.84and1.77(lowerthebetter)forSDATandMIC,respectively.In
contrast, the A-distance for SDAT+CAT (1.30) and MIC (1.29) is signifcantly lower,
suggestingabetterdomainalignment.
Limitations of using Contrastive Training: From Fig. 3, we observe that many
data samples from different classes are closely clustered in the t-SNE visualization,
especiallyinclassesCar,Truck,Bus.Somesamplesareevenclusteredtowrongclasses
after contrastive training. To find the root cause of such deficiency for our proposed
contrastivetrainingmethod,therawimagesfromthetargetdomainoftheVisDA-2017
datasetareillustratedinFig.4,wherethefirstrowshowsthecommonobjectsinthese
classesfromthesourcedomainwhilethesecondrowshowssomespecialobjectsfrom14 J.Chenetal.
thetargetdomain.AsshowninFig.4(d)-(f),therearesomespecialimagesthatcontain
theobjectsfromotherclassesandtheseobjectsevendominatetheareasoftheparticular
images.Thisisbecausemostofthetargetdomainimagesfortheseclassesweretaken
from street, which does not guarantee that a single object is contained in each image.
Hence, during the contrastive training, the non-target objects in such images lead to
theimagesbeingmappedtothewrongclasses(typically,theclassesthatarerelatedto
thenon-targetobjects).Nevertheless,suchspecialadaptationcasesfurthervalidatethe
effectiveness of CAT in pulling similar target samples to the source domain anchors,
whilepushingthedis-similartargetdomainsamplesawayfromtheanchors.
(a)Commoncar (b)Commontruck (c)Commonbus
(d)Specialcar (e)Specialtruck (f)Specialbus
Fig.4:HardtoclassifycasesinDomainNet.
6 Conclusions
In this paper, we presented Contrastive Adversarial Training (CAT) for unsupervised
domainadaptation,aplug-inmoduletoimprovethelearningofdomaininvariantfea-
turebyincreasingthecompetitivenessduringadversarialtraining.Throughincreasing
thedifficultyofthediscriminatorfordomainclassification,CATforcesthefeatureex-
tractor to generate more robust domain invariant features. In the meantime, thanking
to the target sample features being clustered closely with their source anchor sam-
ples, the classifier trained on the labeled source domain data can be easily adopted
into the unlabelled target domain. The comprehensive experiments shown that CAT
can effectively align the source and target feature distributions, and substantially im-
provetheperformanceofexistingadversarialapproachesoverlargeandcomplexUDA
datasets.
ecruoS
tegraTCAT 15
References
1. Balntas, V., Riba, E., Ponsa, D., Mikolajczyk, K.: Learning local feature descriptors with
tripletsandshallowconvolutionalneuralnetworks.In:Bmvc.vol.1,p.3(2016) 7
2. Ben-David,S.,Blitzer,J.,Crammer,K.,Pereira,F.:Analysisofrepresentationsfordomain
adaptation.Advancesinneuralinformationprocessingsystems19(2006) 2,13
3. Chen,T.,Kornblith,S.,Norouzi,M.,Hinton,G.:Asimpleframeworkforcontrastivelearning
ofvisualrepresentations.In:Internationalconferenceonmachinelearning.pp.1597–1607.
PMLR(2020) 4
4. Cover,T.,Hart,P.:Nearestneighborpatternclassification.IEEEtransactionsoninformation
theory13(1),21–27(1967) 6
5. Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:Imagenet:Alarge-scalehierarchi-
calimagedatabase.In:2009IEEEconferenceoncomputervisionandpatternrecognition.
pp.248–255.Ieee(2009) 9
6. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,T.,De-
hghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is
worth16x16words:Transformersforimagerecognitionatscale.ICLR(2021) 3,9
7. Ganin,Y.,Ustinova,E.,Ajakan,H.,Germain,P.,Larochelle,H.,Laviolette,F.,March,M.,
Lempitsky,V.:Domain-adversarialtrainingofneuralnetworks.Journalofmachinelearning
research17(59),1–35(2016) 2,4
8. He,K.,Fan,H.,Wu,Y.,Xie,S.,Girshick,R.:Momentumcontrastforunsupervisedvisual
representation learning. In: Proceedings of the IEEE/CVF conference on computer vision
andpatternrecognition.pp.9729–9738(2020) 4
9. He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.In:Pro-
ceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.pp.770–778
(2016) 2
10. Ho, C.H., Nvasconcelos, N.: Contrastive learning with adversarial examples. Advances in
NeuralInformationProcessingSystems33,17081–17093(2020) 4
11. Hoyer, L., Dai, D., Wang, H., Van Gool, L.: Mic: Masked image consistency for context-
enhanced domain adaptation. In: Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition.pp.11721–11732(2023) 4,9
12. Jin,Y.,Wang,X.,Long,M.,Wang,J.:Minimumclassconfusionforversatiledomainadap-
tation.In:ComputerVision–ECCV2020:16thEuropeanConference,Glasgow,UK,August
23–28,2020,Proceedings,PartXXI16.pp.464–480.Springer(2020) 4
13. Kang,G.,Jiang,L.,Yang,Y.,Hauptmann,A.G.:Contrastiveadaptationnetworkforunsuper-
viseddomainadaptation.In:ProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition.pp.4893–4902(2019) 4
14. Li,Y.F.,Zhou,Z.H.:Towardsmakingunlabeleddataneverhurt.IEEEtransactionsonpattern
analysisandmachineintelligence37(1),175–188(2014) 4
15. Long,M.,Cao,Z.,Wang,J.,Jordan,M.I.:Conditionaladversarialdomainadaptation.Ad-
vancesinneuralinformationprocessingsystems31(2018) 2,4
16. Oord,A.v.d.,Li,Y.,Vinyals,O.:Representationlearningwithcontrastivepredictivecoding.
arXivpreprintarXiv:1807.03748(2018) 4
17. Peng,X.,Bai,Q.,Xia,X.,Huang,Z.,Saenko,K.,Wang,B.:Momentmatchingformulti-
source domain adaptation. In: Proceedings of the IEEE/CVF international conference on
computervision.pp.1406–1415(2019) 2,8
18. Peng, X., Usman, B., Kaushik, N., Hoffman, J., Wang, D., Saenko, K.: Visda: The visual
domainadaptationchallenge.arXivpreprintarXiv:1710.06924(2017) 2
19. Peng, X., Usman, B., Kaushik, N., Hoffman, J., Wang, D., Saenko, K.: Visda: The visual
domainadaptationchallenge(2017) 816 J.Chenetal.
20. Rangwani, H., Aithal, S.K., Mishra, M., Jain, A., Radhakrishnan, V.B.: A closer look at
smoothnessindomainadversarialtraining.In:InternationalConferenceonMachineLearn-
ing.pp.18378–18399.PMLR(2022) 2,4,9
21. Saito,K.,Kim,D.,Sclaroff,S.,Saenko,K.:Universaldomainadaptationthroughselfsuper-
vision.Advancesinneuralinformationprocessingsystems33,16282–16292(2020) 5
22. Sohn,K.:Improveddeepmetriclearningwithmulti-classn-pairlossobjective.Advancesin
neuralinformationprocessingsystems29(2016) 4
23. Sun,T.,Lu,C.,Zhang,T.,Ling,H.:Safeself-refinementfortransformer-baseddomainadap-
tation.In:ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecog-
nition.pp.7191–7200(2022) 2,4,9
24. Tzeng,E.,Hoffman,J.,Saenko,K.,Darrell,T.:Adversarialdiscriminativedomainadapta-
tion.In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.pp.
7167–7176(2017) 4
25. Van Gansbeke, W., Vandenhende, S., Georgoulis, S., Proesmans, M., Van Gool, L.: Scan:
Learningtoclassifyimageswithoutlabels.In:Europeanconferenceoncomputervision.pp.
268–285.Springer(2020) 5
26. Venkateswara,H.,Eusebio,J.,Chakraborty,S.,Panchanathan,S.:Deephashingnetworkfor
unsuperviseddomainadaptation.In:ProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition.pp.5018–5027(2017) 2,8
27. Wang,J.,Chen,J.,Lin,J.,Sigal,L.,deSilva,C.W.:Discriminativefeaturealignment:Im-
provingtransferabilityofunsuperviseddomainadaptationbygaussian-guidedlatentalign-
ment.PatternRecognition116,107943(2021) 2
28. Wang,R.,Wu,Z.,Weng,Z.,Chen,J.,Qi,G.J.,Jiang,Y.G.:Cross-domaincontrastivelearn-
ingforunsuperviseddomainadaptation.IEEETransactionsonMultimedia(2022) 4
29. Xu,T.,Chen,W.,Wang,P.,Wang,F.,Li,H.,Jin,R.:Cdtrans:Cross-domaintransformerfor
unsuperviseddomainadaptation.ICLR(2022) 2
30. Yang,J.,Liu,J.,Xu,N.,Huang,J.:Tvt:Transferablevisiontransformerforunsupervised
domainadaptation.In:ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsof
ComputerVision.pp.520–530(2023) 2
31. Yang,S.,Wang,Y.,Wang,K.,Jui,S.,etal.:Attractinganddispersing:Asimpleapproach
forsource-freedomainadaptation.In:AdvancesinNeuralInformationProcessingSystems
(2022) 4,5,6
32. Zhu, J., Bai, H., Wang, L.: Patch-mix transformer for unsupervised domain adaptation: A
game perspective. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition.pp.3561–3571(2023) 2