Generalizable Human Gaussians
for Sparse View Synthesis
Youngjoong Kwon1, Baole Fang1∗, Yixing Lu1∗, Haoye Dong1, Cheng Zhang1,
Francisco Vicente Carrasco1, Albert Mosella-Montoro1, Jianjin Xu1,
Shingo Takagi2, Daeil Kim2, Aayush Prakash2, and Fernando De la Torre1
1Carnegie Mellon University 2Meta Reality Labs
https://humansensinglab.github.io/Generalizable-Human-Gaussians/
Abstract. Recent progress in neural rendering has brought forth pio-
neering methods, such as NeRF and Gaussian Splatting, which revolu-
tionizeviewrenderingacrossvariousdomainslikeAR/VR,gaming,and
content creation. While these methods excel at interpolating within the
training data, the challenge of generalizing to new scenes and objects
from very sparse views persists. Specifically, modeling 3D humans from
sparseviewspresentsformidablehurdlesduetotheinherentcomplexity
of human geometry, resulting in inaccurate reconstructions of geometry
and textures. To tackle this challenge, this paper leverages recent ad-
vancementsinGaussianSplattingandintroducesanewmethodtolearn
generalizable human Gaussians that allows photorealistic and accurate
view-renderingofanewhumansubjectfromalimitedsetofsparseviews
inafeed-forwardmanner.Apivotalinnovationofourapproachinvolves
reformulating the learning of 3D Gaussian parameters into a regression
process defined on the 2D UV space of a human template, which allows
leveragingthestronggeometrypriorandtheadvantagesof2Dconvolu-
tions.Inaddition,amulti-scaffoldisproposedtoeffectivelyrepresentthe
offset details. Our method outperforms recent methods on both within-
dataset generalization as well as cross-dataset generalization settings.
1 Introduction
Recent advancements in neural rendering techniques, such as Neural Radiance
Fields (NeRF) [30], 3D Gaussian Splatting [17], and point-based graphics [2],
have unveiled a multitude of captivating applications spanning virtual avatars,
asset content creation, or cinematic production. While these methods excel at
interpolatingasinglescene/objectfrommanyinputviews,itisverychallenging
to generalize to new scenes and objects with few samples, and extrapolating
outside the captured views. This limitation is particularly pronounced in the
task of photorealistic rendering of humans, a subject of widespread interest and
applications.Thetaskofmodeling3Dhumansfromsparseviewpointsiscompli-
cated by the inherent complexities of human geometry, including articulations,
* Equal contribution
4202
luJ
71
]VC.sc[
1v77721.7042:viXra2 Kwon et al.
Vanilla 3D-GS [17] NHP [20] GPS-Gaussian [58] GHG (Ours)
Per-subject optimization Generalizable methods
Fig.1: Generalizable Human Gaussian (GHG). Our method can perform accu-
rateandphotorealisticnovelviewrenderingsofanewhumansubjectgivenverysparse
inputs (e.g., 3 views) without involving any test-time optimization or fine-tuning. In
thesparse-viewsetup,ourGHGapproachexhibitssuperiorrenderingqualitycompared
to other generalizable methods such as NHP [20] and GPS-Gaussian [58].
self-occlusions,andcomplexsurfacegeometrieslikehair.Thesefactorsoftenlead
to significant inaccuracies in the reconstruction of both geometry and textures,
posing substantial hurdles to generating photorealistic digital humans.
Recent advances in human rendering incorporate implicit neural representa-
tions (e.g. NeRF) with human template models to facilitate generalizable and
robustsynthesisundersparseviewsettings[7,8,20,21,33,57].WhileNeRF-based
methods have made sigificant progress in generalizable human rendering, they
are limited by their slow runtime, mainly due to their computationally intesive
per-pixel volume rendering process. Additionally, in sparse-view setting, lever-
aging recent advances in inpainting models holds promiss for capturing details
absentintheinputviews.However,integratingthesemodulespresentspractical
challenges as it would further burden the already heavy NeRF-based system.
Recently, explicit representation methods such as 3D Gaussians have gained
popularity for their efficient rasterization-based rendering speed. This fast ren-
dering capability enables seamless integration with other models, such as gen-
erative [44] or depth estimation models [58], to achieve high-quality novel view
rendering. However, these methods encounter difficulties when dealing with hu-
man subjects particularly when only sparse input views (e.g., 2-3 views) are
available. The inherent challenges in rendering human subjects, such as articu-
lations, self-occlusions, and complex surface geometries, worsen the difficulties
in such sparse-view setting (see Figure 1).
To this end, we propose Generalizable Human Gaussians (GHG), a
method for accurate and photorealistic novel-view renderings of human sub-
jects. GHG enables rendering of a novel human subject given very sparse input
views, without requiring any test-time optimization or fine-tuning. To improve
performance in the sparse-view setting, our key insight is to leverage human
geometry prior by reformulating the optimization of 3D Gaussian parameters
within the 2D UV space derived from a human template model. By anchoring
theGaussianparametersontothesurfaceofthe3Dhumantemplatemodel,eachGeneralizable Human Gaussians (GHG) 3
location in the template space can be mapped to each foreground pixel in the
corresponding 2D UV map space. Our UV map-based Gaussian representation
significantly improves the reconstruction of complex human geometries. Oper-
ating on the 2D UV map space enables us to utilize 2D CNNs for the Gaussian
optimization which can incorporate information from neighboring pixels unlike
MLPs. Additionally, this approach makes our model compatible with inpainting
models [53,54], facilitating seamless integration.
While our human UV map-based representation brings significant advance-
ment in generalization and robustness in rendering, we aim to improve its effec-
tiveness further. Given the inherent disparity between the template body model
and real human geometry (e.g. clothing or hair), we present a method to bridge
this gap. Toachieve this, we propose to generate multiple offset meshes through
dilating the human template mesh, both at the input and output spaces. These
meshes serve as scaffolds, enabling effective encoding of input geometry infor-
mation, as well as facilitating a richer representation of displacements beyond
that can be captured by a single template mesh at the output. Leveraging these
multi-scaffold meshes enables more faithful capturing of real human geometries,
whichoftencannotbeaccuratelyrepresentedbyasingletemplatemeshsurface.
WeevaluatetheefficacyofourGeneralizableHumanGaussiansontwomulti-
view human capture datasets: THuman 2.0 [55] and RenderPeople [38]. Exist-
ing generalizable human rendering approaches that allow sparse-view input (3
views) are primarily NeRF-based methods [7,20]. We compare with these meth-
ods and demonstrate superior rendering quality in both in-domain and cross-
datasetevaluationsettings.Additionally,wecompareourapproachwithexisting
3DGaussians-basedmethods,whicheithernecessitatesmoreinputviews[58]or
per-subject optimization [17], showcasing distinct benefits of our approach.
In summary, our main contributions are as follows:
– Weproposeanewfeed-forwardmethodforaccurateandphotorealisticnovel-
viewrenderingsofnewhumansfromverysparseinputviews.Thisisachieved
by integrating human geometry prior with 3D Gaussians. Specifically, we re-
formulate the optimization of 3D Gaussian parameters into a task of gen-
erating a Gaussian parameter map within the 2D human UV space derived
from the human template model.
– Weproposeamulti-scaffoldrepresentationaimedatminimizingthedisparity
betweenthetemplatemodelandrealhumangeometry.Thisapproachenables
theGaussianparameterstobelearnedacrossmultiplescaffoldspaces,allow-
ing for a more comprehensive representation of displacements that surpasses
the capacity of a single template mesh space.
2 Related Work
Generalizable NeRF for Human Rendering. Neural Radiance Fields has
demonstrated its powerful capability to render 3D scenes with photorealistic4 Kwon et al.
quality. However, they can be only optimized on a single scene, and require im-
ages taken from densely sampled cameras to train. To generalize to new scenes
without optimization at inference time, some works condition the generation on
thepixel-alignedfeatures[41,45,52],cost-volumes[5,47],orimage-basedrender-
ing[26,46].Althoughtheyhavedemonstratedhigh-qualitygeneralizationability
on general objects and scenes, directly applying those methods to human sub-
jects is non-trivial due to the complicated human geometries (i.e., articulations
and self-occlusions). To effectively address the generalization to humans, a line
of works utilize 3D human prior. Specifically, SMPL surface [27] is leveraged
as the tool for aggregating the relevant features while preserving its geometric
structure [6–10,20,21,57]. Skeletal keypoints are also utilized [29]. Despite their
detailed output, their rendering speed is very slow due to the volume rendering
process which requires heavy computations to render a single pixel. This deters
them from combining with other modules to further improve the performance
(e.g. inpainting). In our work, thanks to the fast rasterization-based rendering,
our model can be combined with 2D-based inpainting module [53,54] to com-
pensate for the unobserved regions inevitable under sparse view settings.
3D Gaussian Splatting. 3D Gaussian Splatting is a method to represent a
scene with a set of 3D Gaussians [17,39]. By utilizing GPU-parallelized rasteri-
zation, they achieve fast rendering speed and have presented impressive ability
in novel view synthesis tasks. Some concurrent works utilize human template as
the 3D prior and combine it with 3D Gaussians to create animatable represen-
tations [14,16,19,24,31,34,51,59,60]. However, they are not generalizable and
requirenewtrainingprocessforeverynewsubject.Zhengetal.[58]achievegen-
eralization to novel humans by incorporating a stereo-depth estimation module,
whichservesasapartialgeometryprior.However,theysufferwhengivensparse
views with few overlappings and thus depth could not be estimated. Therefore,
they can only interpolate between very close views. In this work, we aim for a
feed-forward generalizable human rendering method that can work when given
verysparseinputswithfewornocorrespondencesbyleveraging3Dhumanprior.
Multi-surfacerepresentations.Whileutilizing3Dhumanpriorhasprovenits
effectiveness in the human rendering task [11,25,35,36,43], representing the ge-
ometrygapbetweenhumantemplateandtherealgeometry(e.g.looseclothing,
hair)isstillchallenging.Somerecentliterature[1,22,32,49]utilizemulti-surface
(shell)[37]torepresentthegeometrydisplacement.However,theideafromthese
works is not directly applicable to our generalization task because they either
can be only optimized on a single subject or cannot be conditioned on the in-
put subject information (i.e., unconditional generation from noise) [1]. In this
paper, we propose multi-scaffold, a multi-surface-based representation that can
effectively represent the human geometric details in the generalization setting.
3 Generalizable Human Gaussians (GHG)
Given a set of multi-view images of a subject (that is not in the training set),
along with their camera position and fitted human template (i.e., SMPL [27]),Generalizable Human Gaussians (GHG) 5
Geometry Cue
Inner Outer
𝑀∆𝒑,𝑖𝑖=1…𝑆
ℰgeo
𝒟d𝑒𝑐
Appearance Cue ℰappr
𝑀𝑖𝑖=0…𝑆
Gaussian Maps
(a) Sparse Inputs (b) Multi-scaffold 𝑀𝒄,𝑖𝑖=0…𝑆 (c) Gaussian Maps (d) Gaussians Anchored on
Construction Generation Multi-scaffold
Fig.2:OverviewofGHG.(a)Wefocusongeneralizablehumanrenderingundervery
sparse view setting. (b) We first construct the multi-scaffolds by dilating the human
template surface. The 2D UV space of each scaffold serves to collect the geometry
and appearance information from the corresponding 3D locations. (c) The aggregated
multi-scaffoldinputisfedintothenetwork,whichgeneratesmulti-Gaussianparameter
maps.(d)Finally,Gaussiansareanchoredonthecorrespondingsurfaceofeachscaffold,
and rasterized into novel views.
our goal is to render photo-realistic novel views. To address this challenge, we
proposeGeneralizableHumanGaussian(GHG),afeed-forwardarchitecturethat
doesnotrequireanyfine-tuning.SeeFigure2foranillustrationofGHC.Inthis
section, we first review the 3D Gaussian Splatting and discuss the motivation of
GHG(Section3.1).InSection3.2,weintroducethemainideaofGHG,whichre-
formulatestheGaussiansplatfittingasaregressionprobleminthe2DUVspace
of the human template. Next, we present our multi-scaffold representation that
allowsencodingandmodelingofthecomplicatedgeometricdetails(Section3.3).
Finally, we describe the end-to-end training objective in Section 3.4.
3.1 Background and Motivation
Notation.Functions(e.g.,neuralnetworkmapping)aredenotedwithuppercase
calligraphicletters(e.g.,F).Vectorsaredenotedwithboldlowercaseletters(e.g.,
p).Matricesaredenotedwithuppercaseletters(e.g.,M).Setsaredenotedwith
bold uppercase letters (e.g., Θ).
3D Gaussian Splatting (3D-GS).Thekeyideaof3D-GS[17]istorepresent
a scene with a set of 3D Gaussians, each of which is characterized by a 3D
covariance matrix Σ and a center (mean) position p:
G(x) =e− 21(x−p)TΣ−1(x−p). (1)
The means of the 3D Gaussians can be initialized by a point cloud using Struc-
ture from Motion [42] computed from C images. Each Gaussian G is parame-
terized by Θ = {p,q,s,α,η} where p ∈ R3 is the center position, q ∈ R4 is
the rotation quaternion, s ∈ R3 is the scaling factor, α ∈ R1 is the opacity,
and η ∈ R(l+1)2 represents the coefficients of the spherical harmonics (SH)
of order l. The covariance matrix is decomposed as Σ = RSS⊤R⊤, where6 Kwon et al.
S = diag(s) ∈ R3×3 is the scaling matrix and R ∈ R3×3 is a rotation matrix
derived from the quaternion q.
TherenderingofaGaussiansetintoanimageplaneisdonebyapproximating
the projection of a 3D Gaussian into pixel coordinates along the depth dimen-
sion [17]. Specifically, for each pixel, the final rendered color c is obtained
pixel
by the α-blending of K overlapping Gaussians that are depth-ordered:
i−1
(cid:88) (cid:89)
c = c α (1−α ), (2)
pixel i i j
i∈K j=1
where α is the opacity and c is the RGB color extracted from SH coefficients.
i i
Motivation.ApplyingGStoourtask(i.e.,generalizablehumanrenderingfrom
sparseviews)isnottrivialforthefollowingreasons:First,theoriginalGSwasde-
signedforsingle-sceneoptimization,makingitdifficulttoadapttogeneralization
tasks — specifically to reconstruct unseen human subjects without model fine-
tuning.Second,accuratepointcloudinitializationrequiresasubstantialnumber
of input images for Structure from Motion. With the number of input images
reduced to as few as 3, vanilla 3D-GS struggles to accurately reconstruct the
complex geometry and texture of the human body. See Fig. 1-Vanilla 3D-GS as
anexampleoftheview-reconstructionachievedwiththeoriginalGS.Therefore,
in this paper, we focus on adapting the 3D Gaussian Splatting for generalizable
human rendering from sparse inputs.
3.2 Learning 3D Gaussians in 2D Human UV Space
UV space of human template. Our goal is to model a generalizable funtion
F({I } ) = {Θ } that estimates the parameters of N Gaussians condi-
c C n NG G
tionedontheinputimages{I } .However,duetothecomplexnatureofhuman
c C
geometry that involves articulations and occlusions, it is challenging to regress
the parameters only given few sparse observations. Therefore, we propose to in-
corporatea3Dgeometryprior(i.e.,ahumantemplatemodelsuchasSMPL[27])
byattachingtheGaussiansonthetemplatesurfaceandregressingtheirparame-
tersinthe2DUVspaceofthehumantemplate.Specifically,foreveryforeground
pixeloftheUVmap,weattachaGaussianonthecorresponding3Dhumansur-
facepointdefinedbytheUVmapping.Then,weregressandstoreitsparameters
in the set of 2D UV maps M = {M ,M ,M ,M ,M }. M ,M ,M ,M ,M
p q s α c p q s α c
denotes the map for the position, rotation, scaling, opacity, and RGB color, re-
spectively. Each parameter map has the resolution of H ×W ×D, where D is
the dimension of each parameter.
2D CNN-based parameter regression. To model the function F, we adopt
a2DConvolutionalNetworkthatprovidesseveralbenefits.Firstofall,2DCNN
naturallyaggregatestheinformationfromneighboringpixels.Thishelpsoursys-
temtoconsiderthelocalcontextandthusmaintaintheconsistencybetweenthe
adjacent Gaussian parameters, which both contribute to better reconstruction
accuracy. In addition, it facilitates integrating other image-based enhancementGeneralizable Human Gaussians (GHG) 7
models, in our case the inpainting module to hallucinate unobserved regions. In
practice, F is modeled with a U-Net as follows:
(cid:0) (cid:1)
F =D E({I } ) =M, (3)
dec c C
where E, D is the U-Net-based encoder and decoder, respectively. {I } de-
dec c C
notes the input images, and M is the set of Gaussian parameter maps.
Reformulation. In our formulation, F regresses the set of 2D parameter maps
M = {M ,M ,M ,M ,M }. Since the Gaussian positions are fixed on the
p q s α c
human template surface, the position map M is computed by rasterizing the
p
vertex position of the human template on the 2D UV space. The RGB map M
c
is computed as the weighted average of corresponding pixels from all observed
views. Specifically, for each pixel in M , we find their projections to all visible
c
source images and average the source RGB values weighted by visibility:
C
(cid:88) (cid:0) (cid:1)
M = W (P)·Π I ,Proj (P) . (4)
c c c c
c=1
P is the Gaussian center positions (i.e., foreground pixel values of the position
map M ). W (P) is the normalized visibility of 3D positions P for the c-th
p c
camera. C is thenumberof totalinput views. I is the c-th inputview image. Π
c
denotes the bilinear sampling operator. Proj denotes the 3D to 2D projection
c
(cid:0) (cid:1)
with respect to the c-th camera. Π I ,Proj (P) returns an image that is the
c c
result of the of interpolating I in the projected coordinates of P.
c
Since M and M are already computed, the regression of the parameter
p c
map M is reduced to M = {M ,M ,M }. To effectively regress the Gaussian
q s α
parameter maps, we provide F with the geometry and appearance cue which
have complementary attributes.
The appearance cue provides information of geometric details and how they
should look like. The geometry cue facilitates the optimization of the Gaussian
parameters to match the appearance details. To provide the geometry cue, we
encode the position map M using the geometry encoder E . The appearance
p geo
cue is obtained by encoding the RGB map M using the appearance encoder
c
E .Now,weadapttheEquation3toconditiontheparametermapgeneration
appr
on the geometry and appearance cue, as:
(cid:0) (cid:1)
D E (M ),E (M ) =M. (5)
dec geo p appr c
Inpainting. It is inevitable to have unobserved regions under very sparse view
settings. This results in blurriness or missing texture in some areas. To address
this issue we incorporate into our architecture a 2D inpainting method. In par-
ticular, we create a set of pseudo ground truth texture maps by transferring
the ground truth mesh texture map into the human template UV space (see
Appx-Fig.10). On this dataset, we train an attention-based generative model
G [53,54] to inpaint the missing regions present in the human template
inpaint
UV space RGB map. At the inference time, we inpaint the RGB map M with
c
G . We would like to note that this is possible because our 2D CNN-based
inpaint8 Kwon et al.
Inner Outer
Combined
Fig.3:I𝑽𝑽l𝟎𝟎lustration𝑽𝑽𝟏𝟏ofmulti-sc𝑽𝑽a𝟐𝟐ffoldrepres𝑽𝑽e𝟑𝟑ntation.Ea𝑽𝑽c𝟒𝟒hcolumnshowsdifferent
scaffold levels, with the last column illustrating their combined effect. The top part
showstheRGBrepresentation,whilethebottomparthighlightsaffectedregions,with
grey indicating unaffected areas.
system facilitates the combination with a 2D-based inpainting module. Our pa-
rameter map regression is again adapted to:
(cid:16) (cid:0) (cid:1)(cid:17)
D E (M ),E G (M ) =M. (6)
dec geo p appr inpaint c
Pleaserefertothesupplementarymaterialsfordetailsoftheinpaintingnetwork.
3.3 Modeling Geometric Details with Multi-scaffolds
The utilization of a human template model helps to reconstruct the shape and
appearancewithsparseviews.However,thisisnotenoughtoeffectivelyrepresent
accurately detailsthat areoffseted fromthe human surfacesuchas hairor loose
clothingduetothefollowingreasons:(1)Theappearancedetailsdeviatingfrom
the template surface (e.g., ponytail) cannot be accurately represented with the
input appearance cue (i.e., RGB map M in Eq. (6)).
c
Therefore, to narrow this geometry gap, we propose to utilize multiple scaf-
folds constructed through dilation of the human template mesh. These multi-
scaffold representation facilitates the effective encoding of the geometry gap in-
formationintotheinput,andallowsformoreversatileoutputGaussianstorepre-
sentthedisplacementdetailsmoreaccurately.Specifically,wecreatethemultiple
scaffolds{V } byoffsettingthehumantemplateverticesV ={v }along
i i=1...S 0 0,j
its outward vertex normal direction:
V ={v |v =v +i·d·nˆ }, (7)
i i,j i,j 0,j j
where v is the j-th vertex of i-th outer scaffold, nˆ is the j-th vertex normal,
i,j j
d defines the offset between scaffolds, S is the number of outer scaffolds. We use
d=1cm and S =4 in the experiments.
Input. We adapt the geometry and appearance cues to aggregate information
from the entire level of scaffolds. We redefine the geometry cue as the featureGeneralizable Human Gaussians (GHG) 9
extracted from the concatenation of offset maps which record the displacement
between each scaffold:
E (M ⊕...⊕M ), where M =M −M . (8)
geo ∆p,1 ∆p,S ∆p,i p,i p,i−1
M ,M istheoffsetmapandpositionmapofthei-thscaffold,respectively.
∆p,i p,i
⊕ is the concatenation operation. The appearance cue is redefined as:
C
(cid:88)
E (M ⊕...⊕M ), where M = W (P )·Π(I ,Proj (P )). (9)
appr c,0 c,S c,i c i c c i
c=1
Here M , P is the RGB map and the Gaussian center positions corresponding
c,i i
to the i-th level scaffold, respectively. Note that inpainting is done only to the
RGB map of 0-th level scaffold (i.e., M =G (M ))
c,0 inpaint c,0
Output.Therearenumerouspossibledesignchoicestomodelthedisplacement
details such as hair. For example, the scaling can be enlarged to cover the gap,
orwecoulddirectlylearnGaussianmeanoffsets.However,weempiricallyfound
out that these lead to unstable training and hinder the system from converging
because of the high degree-of-freedom (see Fig. 6). Therefore, we attach Gaus-
sians on each scaffold, and regress their parameters within each scaffold. This is
realizedbyconfiningthemaximumscalingoftheGaussiansastheoffsetbetween
scaffolds. Our final formulation is defined as:
(cid:0) (cid:1)
D E (M ⊕...⊕M ),E (M ⊕...⊕M ) ={M } . (10)
dec geo ∆p,1 ∆p,S appr c,0 c,S i i=0...S
where{M }isthesetofparametermapscorrespondingtothei-thlevelscaffold.
i
3.4 Training and Optimization
Gaussian parameter map regressor. To train our Gaussian parameter map
regressorF (i.e.,E ,E ,D ),weemploymulti-viewRGBandmasksuper-
geo appr dec
vision. Specifically, we sample N target views from the positions in between the
input views and generate the RGB and mask predictions. The predictions are
supervisedbyminimizingthelossobjectiveL= 1(λ ·L +λ ·L +λ ·
N 1 1 ssim ssim mask
L ),whereL ,L areL andSSIMloss[48]computedbetweentheground
mask 1 ssim 1
truth and predicted RGB images, respectively. L is the Binary Cross En-
mask
tropy loss computed between the ground truth and predicted foreground mask.
We use N =3, λ =0.8, λ =0.2, λ =0.02 in our experiments. We used
1 ssim mask
asingleGPUwith20Gmemoryduringtraining.AdamWoptimizer[28]withan
initiallearningrateof2e−4 wasused.Wetraintheparameterregressorfor100k
iterations with a single batch size, which takes around 10 hours.
Inpaintingnetwork.WhentrainingtheinpaintingnetworkG ,L =λ ·
inpaint G rec
L +λ ·L is minimized, where L , L are L loss and adversarial loss
rec adv adv rec adv 1
computedbetweentheinpaintedresultsandpseudogroundtruth.λ =10and
rec
λ =1areusedintraining.Thelossobjectivefortheinpaintingdiscriminator
adv
D isdefinedthediscriminatorlossbetweentheinpaintedimageandpseudo
inpaint10 Kwon et al.
Table 1: Comparison with NeRF-based methods for (a) in-domain and (b)
cross-domain sparse view synthesis. For all the methods, we use 3 views during
both training and testing. GHG achieves competitive results for both settings. TH:
THuman [55]. RP: RenderPeople [38]. See Figure 4 and 5 for qualitative results.
(a) In-domain: TH → TH (b) Cross-domain: TH → RP
Method PSNR↑ LPIPS↓ FID↓ PSNR↑ LPIPS↓ FID↓
NHP [20] 23.32 184.69 136.56 22.34 172.56 137.23
NIA [21] 23.20 181.82 127.30 22.45 168.15 124.80
GHG (ours) 21.90 133.41 61.67 21.02 137.73 60.85
groundtruth.G andD aretrainedalternativelyfor40epochs.Weuse
inpaint inpaint
Adamoptimizer[18]withaninitiallearningrateof1e−4 anddecaythelearning
rate by half every 10 epoch. It takes around 4 hours to train the inpainting
moduleonasingleGPUwithabatchsizeof1.NotethatG istrainedonly
inpaint
onceseparatelyfromtheGaussianmapregressor,anditisageneralmodelthat
works for different new subjects at inference.
4 Experiments
4.1 Baselines, Datasets, and Metrics
Baselines.Webenchmarkourmethodagainststate-of-the-artgeneralizablehu-
man rendering techniques from two categories: 3D human template-conditioned
NeRF methods NHP [20] and NIA [21], and depth-based 3D Gaussian method
GPS-Gaussian[58].Additionally,wecomparedwiththeoriginalvanilla3DGaus-
sians [17], which are optimized per subject.
Datasets. We conducted experiments on two datasets: the THuman [55] and
RenderPeople [38] dataset. The THuman dataset comprises 526 high-quality
3D scans, texture maps, and corresponding SMPL-X parameters. 100 subjects
werereservedfortheevaluation,followingGPS-Gaussian[58].TheRenderPeople
datasetencompasses3Dhumanscansrepresentingdiverseclothingstyles,races,
andages,totaling956subjectssplitinto756trainand200testsubjects.SMPL-
X parameters were estimated using off-the-shelf methods [3,4].
Metrics. Wegeneratedimages at aresolution of 1024×1024 for evaluating our
results.Toassessthequalityofourresults,weemployedseveralmetrics.Initially,
we utilized the peak signal-to-noise ratio (PSNR), a standard metric. However,
PSNR may not fully reflect human perception, as it can assign a low error
to very blurry and unrealistic results [56]. Therefore, we also incorporated the
learnedperceptualimagepatchsimilarity(LPIPS)[56]andtheFréchetinception
distance (FID) [12], which better align with human perceptions.
4.2 Comparison with NeRF-based methods
We compare with NHP [20] and NIA [21], which are the two competitors that
are most similar to our settings in that (1) they focus on generalizable humanGeneralizable Human Gaussians (GHG) 11
Input NHP NIA Vanilla-GS GPS-Gaussian* GHG (Ours) GT
Fig.4: Qualitative comparisons. All methods are trained and tested on THuman
dataset[55].†Unliketheothermethods,Vanilla-GS[17]isper-subjectoptimizedonthe
testingsubjects.*GPS-Gaussian[58]istrainedandtestedwith5inputviews,whereas
NHP [20], NIA [21] and our method are trained and tested with 3 input views.
rendering from very sparse (i.e., 3) input views and (2) use the human template
as3Dprior.Ours,NHP,andNIAaretrained/evaluatedontheTHumandataset
withthesameNHPprotocol,wherethreerandomlychoseninputviewsareused
during training and the same three canonical views are used during evaluation.
In-domain generalization. Table 1-(a) shows the in-domain generalization
result where we evaluate on test subjects from THuman dataset. We achieve
the best performance on the perception-based metrics LPIPS and FID, and
comparablePSNR.AsshowninFigure4,thesingle-layerrepresentationofNHP
and NIA where the features are aggregated on a single surface of a naked body
leads to the mixture of visual details and produce blurry results. On the other
hand, our method collects visual information from the multi-scaffold and thus
recovers sharp and high-frequency details including hair, wrinkles, and logos.
Cross-domain generalization. To confirm the cross-dataset generalizability
of our approach, we train a model on the THuman dataset and evaluate it on
the challenging RenderPeople dataset without any test-time optimization. The
RenderPeopledatasetexhibitsamorediversedatadistributioncomparedtothe
training dataset (THuman), encompassing variations in race, age, and apparel.
Yet, our GHG significantly outperforms NHP and NIA on the perception-based
metrics LPIPS and FID. In Figure 5, our method recovers fine details such as
facialexpressions,clothingpatterns,andtextures.However,sincePSNRispixel-
wisecomputed,aslightdeviationfromthegroundtruthcanleadtoalowerscore.12 Kwon et al.
Table 2: Comparison with Gaussian Splatting-based methods on the THu-
man dataset [55]. Due to GPS-Gaussian [58] requiring at least 5 input views for
reasonable results, we train and test all methods with 5 views.
Method PSNR↑ LPIPS↓ FID↓
Vanilla-GS (per-subject) [17] 17.62 220.30 210.03
GPS-Gaussian [58] 20.69 123.30 46.26
GHG (ours) 22.06 132.42 37.10
Input NHP NIA GHG (Ours) GT
Fig.5:Qualitativeresultsoncross-domaingeneralization.Wetrainthemodels
onTHumandataset[55]andtestonRenderpeopledataset[38]withoutmodelfinetun-
ing.GHGcanrenderhigh-frequentdetailsandaccurategeometryofthenovelsubject.
This can explain our lower performance in terms of PSNR in Table 1-(a),(b),
while NHP with blurry results achieves the highest performance.
4.3 Comparison with Gaussian Splatting-based methods
We show the comparison with GPS-Gaussian [58] and the original vanilla 3D
Gaussians [17]. Although the original GPS-Gaussian does not focus on sparse
view synthesis as ours, we include comparison with them because we are both
3DGaussian-basedmethodsandexploregeneralizationontounseenhumansub-
jects. In our exploration of GPS Gaussian, we observed that GPS-Gaussian re-
quires a substantial overlap between inputs for stereo-depth computation and
cannot perform adequately with fewer than five views. Therefore, for compari-
son purposes in Table 2, we employ five uniformly distributed input views for
trainingandevaluationofGPS-Gaussian,vanilla3DGaussian,andourmethod.
Unlike the other methods, vanilla Gaussian is per-subject optimized and thus
trained on the testing human subjects. As presented in Table 2, our approach
achieves comparable results to GPS-Gaussian [58] with better PSNR and FID
scores,whilesignificantlyoutperformsthevanillaGaussianmethod.Visualcom-
parisons in Figure 4 reveal that our model, trained and tested with 3 input
views, exhibits more accurate geometries and finer detail reconstruction com-
pared to GPS-Gaussian, trained and tested with 5 input views. Particularly,
GPS-Gaussian suffers from inaccurate geometry and missing contents, possibly
due to its inherent high demand for larger overlap between input views.Generalizable Human Gaussians (GHG) 13
Table 3: Ablation study on the multi-scaffold representation. S: single scaf-
fold.S⋆:singlescaffoldwithalargescale.S†:singlescaffoldwithalearnableoffset.✓:
multiple scaffolds.
Input Scaffold Output Scaffold
Geometry Appearance Gaussian Map PSNR↑ LPIPS↓ FID↓
a S S S 22.30 145.74 84.38
b S S ✓ 22.44 142.84 73.91
c S ✓ ✓ 22.96 136.81 72.43
d ✓ S ✓ 22.60 144.27 81.31
e ✓ ✓ S⋆ 23.11 145.03 90.16
f ✓ ✓ S† 23.39 145.55 87.49
g ✓ ✓ ✓ 21.90 133.41 61.67
4.4 Ablation Studies and Analyses
We conducted ablation studies on the THuman dataset, evaluating variants of
our GHG model on unseen subjects in Figure 6 and Table 3, 4, and 5.
Effectofmulti-scaffoldrepresentation.Weexaminetheimpactofthemulti-
scaffoldrepresentationfordifferentconfigurationsofinputs/outputs,seeTable3.
First, we trained three input variants where the multi-scaffold input is either
partially (c, d) or not used at all (b). We retained the multi-scaffold output
(i.e., multi-Gaussian map generation). The lack of multi-layer geometry and
appearance information leads to perceptual performance degradation. Second,
wedesignedtwooutputvariantswhereonlyasingleGaussianmapisgenerated,
where we maintained the multi-scaffold input. In our original model, scale of
each Gaussian map is confined up to the distance between its next scaffold (i.e.,
1cm). To represent the displacement between the template model and the real
geometry, either scale is allowed to grow up to 4cm (Table 3-(e)) or scale is
still confined to 1cm but we additionally learn a Gaussian center offset that
can move up to 4cm (Table 3-(f)). However, as shown in Figure 6-(2),(3), the
lackofregularizationgeneratesblurryresults.Thelowestperformanceofoutput
variants without multi-scaffold representation in Table 3-(e),(f) again validates
our design choice where we build 3D Gaussians on multiple scaffolds.
Importance of geometry and appearance cue. To study the effect of inte-
grationofthegeometryandappearancecue,wetrainavariantwiththegeometry
cue completely removed (Table 4-first row, Figure 6-4) and a variant with ap-
pearance cue removed (Table 4-second row, Figure 6-5). Removing one of them
leads to visual artifacts and failure of recovering geometry gap such as hair.
Effect of inpainting. Under our sparse-view setting, it is inevitable to have
insufficient observations (Figure 6-6). Our 2D architecture allows us to easily
combine with the 2D-based inpainting module and hallucinate the unobserved
regions, thus leads to improved quality (Figure 6-7, Table 5).14 Kwon et al.
1.Single-scaffold 2. Larger scale 3. Learnable offset 4. w/o Geometry 5. w/o Appearance 6. w/o Inpaint 7.Ours full
Table 3 Table 4 Table 5
Fig.6: Ablation studies. 1) Result only using the template mesh. 2) Illustrates the
result of using a larger scale. 3) Depicts the result of learning the Gaussian offset.
4) Shows the model devoid of geometry information. 5) Illustrates the model without
appearance cues. 6) Shows the model without inpainting. 7) Presents our model.
Table4: Ablationstudyonthegeom- Table 5: Ablation study on the tex-
etry cue and appearance cue.✗/✓indi- tureinpaintingnetwork.✗/✓indicates
catescompletelyremove/keeptheencoding without/with the inpainting network on
branch. See Figure 2 for an illustration. the2DUVspace,respectively.Pleasesee
Figure 6 for a comparison result.
Geo. App. PSNR↑ LPIPS↓ FID↓
✗ ✓ 22.74 138.35 69.12 Inpainting PSNR↑ LPIPS↓ FID↓
✓ ✗ 21.83 146.05 78.56 ✗ 21.65 135.42 67.15
✓ ✓ 21.90 133.41 61.67 ✓ 21.90 133.41 61.67
5 Conclusion
WepresentGeneralizableHumanGaussians(GHG),afeed-forwardarchitecture
capable of synthesizing novel views of new humans using sparse input views,
without the need for test-time optimization. Our key insight is the reformula-
tion of 3D Gaussian parameter optimization into the generation of parameter
maps within the 2D human UV space. This allows us to leverage the human
geometry prior, addressing challenges such as articulations and self-occlusions.
Additionally, by framing the task as a 2D problem, we can exploit local neigh-
boring information and integrate 2D-based inpainting modules to hallucinate
unobserved regions. Finally, we propose a multi-scaffold approach to effectively
represent and bridge the geometry gap between the human template and real
humangeometry.Experimentalresultsshowthatourmethodcangeneratehigh-
quality renderings surpassing state-of-the-art approaches.Generalizable Human Gaussians (GHG) 15
References
1. Abdal, R., Yifan, W., Shi, Z., Xu, Y., Po, R., Kuang, Z., Chen, Q., Yeung,
D.Y.,Wetzstein,G.:Gaussianshellmapsforefficient3dhumangeneration.arXiv
preprint arXiv:2311.17857 (2023) 4
2. Aliev, K.A., Sevastopolsky, A., Kolos, M., Ulyanov, D., Lempitsky, V.: Neural
point-based graphics. In: Computer Vision–ECCV 2020: 16th European Confer-
ence,Glasgow,UK,August23–28,2020,Proceedings,PartXXII16.pp.696–712.
Springer (2020) 1
3. Bhatnagar, B.L., Sminchisescu, C., Theobalt, C., Pons-Moll, G.: Combining im-
plicit function learning and parametric models for 3d human reconstruction. In:
European Conference on Computer Vision (ECCV). Springer (aug 2020) 10
4. Bhatnagar, B.L., Sminchisescu, C., Theobalt, C., Pons-Moll, G.: Loopreg: Self-
supervised learning of implicit surface correspondences, pose and shape for 3d
humanmeshregistration.In:AdvancesinNeuralInformationProcessingSystems
(NeurIPS) (December 2020) 10
5. Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: Mvsnerf: Fast
generalizableradiancefieldreconstructionfrommulti-viewstereo.In:Proceedings
oftheIEEE/CVFInternationalConferenceonComputerVision.pp.14124–14133
(2021) 4
6. Chen, J., Yi, W., Ma, L., Jia, X., Lu, H.: Gm-nerf: Learning generalizable
model-basedneuralradiancefieldsfrommulti-viewimages.In:Proceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.20648–
20658 (2023) 4
7. Chen, M., Zhang, J., Xu, X., Liu, L., Cai, Y., Feng, J., Yan, S.: Geometry-guided
progressive nerf for generalizable and efficient neural human rendering. In: Euro-
pean Conference on Computer Vision. pp. 222–239. Springer (2022) 2, 3, 4
8. Cheng, W., Xu, S., Piao, J., Qian, C., Wu, W., Lin, K.Y., Li, H.: Generalizable
neural performer: Learning robust radiance fields for human novel view synthesis.
arXiv preprint arXiv:2204.11798 (2022) 2, 4
9. Gao, Q., Wang, Y., Liu, L., Liu, L., Theobalt, C., Chen, B.: Neural novel actor:
Learning a generalized animatable neural representation for human actors. IEEE
Transactions on Visualization and Computer Graphics (2023) 4
10. Gao,X.,Yang,J.,Kim,J.,Peng,S.,Liu,Z.,Tong,X.:Mps-nerf:Generalizable3d
human rendering from multiview images. IEEE Transactions on Pattern Analysis
and Machine Intelligence (2022) 4
11. Habermann, M., Liu, L., Xu, W., Pons-Moll, G., Zollhoefer, M., Theobalt, C.:
Hdhumans:Ahybridapproachforhigh-fidelitydigitalhumans.Proceedingsofthe
ACM on Computer Graphics and Interactive Techniques 6(3), 1–23 (2023) 4
12. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.Advancesin
neural information processing systems 30 (2017) 10
13. Ho, H.I., Song, J., Hilliges, O.: Sith: Single-view textured human reconstruction
withimage-conditioneddiffusion.In:ProceedingsoftheIEEEConferenceonCom-
puter Vision and Pattern Recognition (CVPR) (2024) 20
14. Hu,L.,Zhang,H.,Zhang,Y.,Zhou,B.,Liu,B.,Zhang,S.,Nie,L.:Gaussianavatar:
Towards realistic human avatar modeling from a single video via animatable 3d
gaussians. arXiv preprint arXiv:2312.02134 (2023) 4
15. Huang,Y.,Yi,H.,Xiu,Y.,Liao,T.,Tang,J.,Cai,D.,Thies,J.:Tech:Text-guided
reconstruction of lifelike clothed humans. arXiv preprint arXiv:2308.08545 (2023)
2016 Kwon et al.
16. Jena,R.,Iyer,G.S.,Choudhary,S.,Smith,B.,Chaudhari,P.,Gee,J.:Splatarmor:
Articulatedgaussiansplattingforanimatablehumansfrommonocularrgbvideos.
arXiv preprint arXiv:2311.10812 (2023) 4
17. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-timeradiancefieldrendering.ACMTransactionsonGraphics42(4)(2023) 1,
3, 4, 5, 6, 10, 11, 12
18. Kingma,D.P.,Ba,J.:Adam:Amethodforstochasticoptimization.arXivpreprint
arXiv:1412.6980 (2014) 10
19. Kocabas, M., Chang, J.H.R., Gabriel, J., Tuzel, O., Ranjan, A.: Hugs: Human
gaussian splats. arXiv preprint arXiv:2311.17910 (2023) 4
20. Kwon, Y., Kim, D., Ceylan, D., Fuchs, H.: Neural human performer: Learning
generalizableradiancefieldsforhumanperformancerendering.AdvancesinNeural
Information Processing Systems 34, 24741–24752 (2021) 2, 3, 4, 10, 11, 20, 21
21. Kwon, Y., Kim, D., Ceylan, D., Fuchs, H.: Neural image-based avatars: General-
izable radiance fields for human avatar modeling. In: International Conference on
Learning Representations (2023) 2, 4, 10, 11, 20, 21
22. Kwon, Y., Liu, L., Fuchs, H., Habermann, M., Theobalt, C.: Deliffas: Deformable
light fields for fast avatar synthesis. arXiv preprint arXiv:2310.11449 (2023) 4
23. Lazova,V.,Insafutdinov,E.,Pons-Moll,G.:360-degreetexturesofpeopleincloth-
ing from a single image. In: 2019 International Conference on 3D Vision (3DV).
pp. 643–653. IEEE (2019) 24
24. Li, Z., Zheng, Z., Wang, L., Liu, Y.: Animatable gaussians: Learning pose-
dependentgaussianmapsforhigh-fidelityhumanavatarmodeling.arXivpreprint
arXiv:2311.16096 (2023) 4
25. Liu,L.,Habermann,M.,Rudnev,V.,Sarkar,K.,Gu,J.,Theobalt,C.:Neuralactor:
Neural free-view synthesis of human actors with pose control. ACM transactions
on graphics (TOG) 40(6), 1–16 (2021) 4
26. Liu,Y.,Peng,S.,Liu,L.,Wang,Q.,Wang,P.,Christian,T.,Zhou,X.,Wang,W.:
Neural rays for occlusion-aware image-based rendering. In: CVPR (2022) 4
27. Loper,M.,Mahmood,N.,Romero,J.,Pons-Moll,G.,Black,M.J.:Smpl:Askinned
multi-person linear model. In: Seminal Graphics Papers: Pushing the Boundaries,
Volume 2, pp. 851–866 (2023) 4, 6
28. Loshchilov,I.,Hutter,F.:Decoupledweightdecayregularization.In:ICLR.Open-
Review.net (2019), https://openreview.net/forum?id=Bkg6RiCqY7 9
29. Mihajlovic,M.,Bansal,A.,Zollhoefer,M.,Tang,S.,Saito,S.:Keypointnerf:Gener-
alizingimage-basedvolumetricavatarsusingrelativespatialencodingofkeypoints.
In: European conference on computer vision. pp. 179–197. Springer (2022) 4
30. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.Commu-
nications of the ACM 65(1), 99–106 (2021) 1
31. Moreau, A., Song, J., Dhamo, H., Shaw, R., Zhou, Y., Pérez-Pellitero, E.: Hu-
mangaussiansplatting:Real-timerenderingofanimatableavatars.arXivpreprint
arXiv:2311.17113 (2023) 4
32. Ouyang, H., Zhang, B., Zhang, P., Yang, H., Yang, J., Chen, D., Chen, Q., Wen,
F.: Real-time neural character rendering with pose-guided multiplane images. In:
European Conference on Computer Vision. pp. 192–209. Springer (2022) 4
33. Pan, X., Yang, Z., Ma, J., Zhou, C., Yang, Y.: Transhuman: A transformer-based
humanrepresentationforgeneralizableneuralhumanrendering.In:Proceedingsof
theIEEE/CVFInternationalconferenceoncomputervision.pp.3544–3555(2023)
2Generalizable Human Gaussians (GHG) 17
34. Pang,H.,Zhu,H.,Kortylewski,A.,Theobalt,C.,Habermann,M.:Ash:Animatable
gaussian splats for efficient and photoreal human rendering (2023) 4
35. Peng,S.,Dong,J.,Wang,Q.,Zhang,S.,Shuai,Q.,Zhou,X.,Bao,H.:Animatable
neural radiance fields for modeling dynamic human bodies. In: Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision.pp.14314–14323(2021)
4
36. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural
body: Implicit neural representations with structured latent codes for novel view
synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 9054–9063 (2021) 4
37. Porumbescu, S.D., Budge, B., Feng, L., Joy, K.I.: Shell maps. ACM Transactions
on Graphics (TOG) 24(3), 626–633 (2005) 4
38. RenderPeople. http://renderpeople.com (2018) 3, 10, 12, 19
39. Robertini, N., Casas, D., Rhodin, H., Seidel, H.P., Theobalt, C.: Model-based
outdoor performance capture. In: Proceedings of the 2016 International Confer-
ence on 3D Vision (3DV 2016) (2016), http://gvv.mpi-inf.mpg.de/projects/
OutdoorPerfcap/ 4
40. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition.pp.10684–10695(2022) 19
41. Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., Li, H.: Pifu:
Pixel-aligned implicit function for high-resolution clothed human digitization. In:
Proceedings of the IEEE/CVF international conference on computer vision. pp.
2304–2314 (2019) 4
42. Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition.pp.4104–4113
(2016) 5
43. Su, S.Y., Yu, F., Zollhöfer, M., Rhodin, H.: A-nerf: Articulated neural radiance
fields for learning human shape, appearance, and pose. Advances in Neural Infor-
mation Processing Systems 34, 12278–12291 (2021) 4
44. Tang,J.,Ren,J.,Zhou,H.,Liu,Z.,Zeng,G.:Dreamgaussian:Generativegaussian
splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)
2
45. Wang, P., Chen, X., Chen, T., Venugopalan, S., Wang, Z., et al.: Is attention all
nerf needs? arXiv preprint arXiv:2207.13298 (2022) 4
46. Wang,Q.,Wang,Z.,Genova,K.,Srinivasan,P.P.,Zhou,H.,Barron,J.T.,Martin-
Brualla,R.,Snavely,N.,Funkhouser,T.:Ibrnet:Learningmulti-viewimage-based
rendering.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 4690–4699 (2021) 4
47. Wang, S., Wang, Z., Schmelzle, R., Zheng, L., Kwon, Y., Sengupta, R., Fuchs,
H.:Learningviewsynthesisfordesktoptelepresencewithfewrgbdcameras.IEEE
Transactions on Visualization and Computer Graphics (2024) 4
48. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. IEEE TIP 13(4), 600–612 (2004) 9
49. Wang, Z., Shen, T., Nimier-David, M., Sharp, N., Gao, J., Keller, A., Fidler, S.,
Müller,T.,Gojcic,Z.:Adaptiveshellsforefficientneuralradiancefieldrendering.
arXiv preprint arXiv:2311.10091 (2023) 4
50. Xiu,Y.,Yang,J.,Cao,X.,Tzionas,D.,Black,M.J.:Econ:Explicitclothedhumans
optimizedvianormalintegration.In:ProceedingsoftheIEEE/CVFconferenceon
computer vision and pattern recognition. pp. 512–523 (2023) 2018 Kwon et al.
51. Ye, K., Shao, T., Zhou, K.: Animatable 3d gaussians for high-fidelity synthesis of
human motions. arXiv preprint arXiv:2311.13404 (2023) 4
52. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from
one or few images. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 4578–4587 (2021) 4
53. Yu,J.,Lin,Z.,Yang,J.,Shen,X.,Lu,X.,Huang,T.S.:Free-formimageinpainting
with gated convolution. arXiv preprint arXiv:1806.03589 (2018) 3, 4, 7, 24
54. Yu,J.,Lin,Z.,Yang,J.,Shen,X.,Lu,X.,Huang,T.S.:Generativeimageinpainting
with contextual attention. arXiv preprint arXiv:1801.07892 (2018) 3, 4, 7
55. Yu,T.,Zheng,Z.,Guo,K.,Liu,P.,Dai,Q.,Liu,Y.:Function4d:Real-timehuman
volumetric capture from very sparse consumer rgbd sensors. In: IEEE Conference
onComputerVisionandPatternRecognition(CVPR2021)(June2021) 3,10,11,
12, 19
56. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 586–595 (2018) 10
57. Zhao, F., Yang, W., Zhang, J., Lin, P., Zhang, Y., Yu, J., Xu, L.: Humannerf:
Efficiently generated human radiance field from sparse inputs. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
7743–7753 (2022) 2, 4
58. Zheng, S., Zhou, B., Shao, R., Liu, B., Zhang, S., Nie, L., Liu, Y.: Gps-gaussian:
Generalizablepixel-wise3dgaussiansplattingforreal-timehumannovelviewsyn-
thesis.ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition (2024) 2, 3, 4, 10, 11, 12, 20
59. Zhou,Z.,Ma,F.,Fan,H.,Yang,Y.:Headstudio:Texttoanimatableheadavatars
with 3d gaussian splatting. arXiv preprint arXiv:2402.06149 (2024) 4
60. Zielonka, W., Bagautdinov, T., Saito, S., Zollhöfer, M., Thies, J., Romero, J.:
Drivable 3d gaussian avatars. arXiv preprint arXiv:2311.08581 (2023) 4Generalizable Human Gaussians (GHG) 19
A Appendix - Overview
Thisappendixisorganizedasfollows:Sec.Bdiscussesthelimitationsandfutureworks;
Sec.Cpresentsthesocietalimpactsourworkcanhave;Sec.Dshowsadditionalresults
including video results, comparison with single-view methods, ablations on number of
outerscaffolds,ablationstudywithdifferentlosssupervision,ablationsonthenumber
ofinputviewsatinferencetime,andruntimeatinference.Sec.Eprovidesinformation
regarding reproducibility, which includes implementation details.
B Limitations and Future Works
Although our method achieves state-of-the-art results in terms of visual quality and
runtime, it is not free from limitations. (1) While our method effectively compensates
for minor inaccuracies in SMPL-X estimations through the use of multi-scaffolds, sig-
nificant deviations in SMPL-X from the input images could compromise the quality
of our results, as our Gaussians are anchored to the SMPL-X surface. (2) Currently,
the number of scaffolds is determined empirically. It would be an interesting direction
toexploreadaptivescaffoldsbasedonsubjectattributes(e.g.,looseortightclothing).
(3) The performance of our inpainting network is constrained by the small number of
groundtruthtexturemapsavailableduringtraining,whichinturnlimitsitsabilityto
generatedetailedhallucinationswhengivenasingle-viewinput.Therefore,integrating
andfine-tuninggenerativemodelstrainedonextensivedatasets(e.g.,StableDiffusion
model [40]) could substantially improve our network’s hallucination capabilities and
generalizability, which is a promising direction for future work.
C Societal Impacts
Ourproposedmethodcanpushimmersiveentertainmentandcommunicationtoamore
affordablesetting.Forexample,ourworkhasthepotentialtoenhancetheaccessibility
of telepresence experiences by facilitating the creation of avatars from minimal RGB
images. Moreover, the technology presents benefits to film and game production by
enabling efficient synthesis of large-scale 3D human avatars with low costs.
However, our work might also introduce potential challenges, primarily related to
the accessible creation of realistic human images. This could lead to deep-fake human
avatars on social media, with implications for misinformation and the degradation of
trustindigitalcontent.Tomitigatesuchrisks,itisurgenttopromoteethicalguidelines
and regulations on synthetic media. We strongly appeal transparent use of such tech-
nologyasitshouldalignwithsocietalinterestsandfostertrustratherthanskepticism.
D Additional results
D.1 Video results
Videoresultsofcomparisonwiththestate-of-the-artbaselinesonthein-domaingener-
alization task (i.e., trained and tested on THuman 2.0 dataset [55]) and cross-dataset
generalizationtask(i.e.,trainedonTHuman2.0andtestedonRenderPeople[38])can20 Kwon et al.
Fig.7: Comparison with single-view reconstruction methods: ECON [50], TeCH [15],
and SiTH [13]. Our method outperforms the baselines in terms of faithfulness to the
given observation.
be found in the project website*. For the in-domain generalization task, we compare
ourGHGwith(1)humantemplate-conditionedNeRF,generalizationfromsparseview
methods NHP [20] and NIA [21], and (2) generalizable 3D Gaussian Splatting for hu-
man rendering method GPS-Gaussian [58]. Note that GPS-Gaussian is trained and
tested with 5 input views due to the rectification requirement. NHP, NIA, and ours
are trained and tested with 3 input views. For the cross-dataset generalization task,
we show comparison with our main baselines NHP and NIA. Our method can recover
sharp and fine details compared to human template-conditioned NeRF baselines. Due
to the lack of full 3D prior, GPS-Gaussian suffers in maintaining multi-view consis-
tency between the novel views generated using different input views. On the other
hand, ours maintains robust and accurate geometry reconstruction utilizing the 3D
human template.
D.2 Comparison with single-view methods
Fig. 7 shows comparisons with SOTA single-view reconstruction methods that are
based on 3D human prior: ECON [50], TECH [15], and SiTH [13]. We used their offi-
cially released implementation for the comparison. Our sparse-view work outperforms
in terms of accuracy and faithfulness to the observed data, as can be seen in Fig. 7.
Also, the single-view methods either require per-subject optimization (ECON, TeCH)
or run at relatively slow speed (e.g., ECON 3 min / TeCH 4 hr / SiTH 2 min). On
theotherhand,oursisafeed-forwardmethodthatrunsat4fps,whichis×480faster
than SiTH.
D.3 Ablations
Ablation on the number of scaffolds. In Tab. 6, we study the impact of number
of outer scaffolds. Variants with different number of outer scaffolds are trained and
tested.Theperformanceincreaseissaturatedasmorethan5outerscaffoldsareused.
Therefore, we use 4 outer scaffolds as our final model. In Fig. 8, we show how the
* https://humansensinglab.github.io/Generalizable-Human-GaussiansGeneralizable Human Gaussians (GHG) 21
Table 6: Ablation study on the number of outer scaffolds used. We trained
and tested variants with different numbers of scaffolds that are outside the original
SMPL-X surface. The variant with only the base template is denoted as “0 scaffold”.
The performance increase is saturated as more than 5 outer scaffolds are used.
# Out scaffolds. PSNR↑ LPIPS↓ FID↓
0 22.30 145.74 84.38
1 22.77 139.16 75.66
2 22.28 137.65 73.54
3 21.87 136.38 65.19
4 (Ours full) 21.90 133.41 61.67
5 22.13 134.73 63.80
6 22.09 135.52 64.81
Table 7: Ablation study on the supervision. ✗/✓indicates completely re-
move/keep the loss supervision. Our L -only supervision result (a) still outperforms
1
thehumantemplate-conditionedNeRFmethodsNHPandNIA,whicharealsotrained
withL -onlysupervision.Thisvalidatestheeffectivenessofourproposedmulti-scaffold.
1
L SSIM Mask Multi-view PSNR↑ LPIPS↓ FID↓
1
NHP ✓ ✗ ✗ ✗ 23.32 184.69 136.56
NIA ✓ ✗ ✗ ✗ 23.20 181.82 127.30
a ✓ ✗ ✗ ✗ 23.05 142.57 71.97
b ✓ ✓ ✗ ✗ 22.69 136.44 69.50
c ✓ ✓ ✗ ✓ 22.03 134.82 62.04
Ours full ✓ ✓ ✓ ✓ 21.90 133.41 61.67
number of scaffolds affects the reconstruction of offset details such as hair (a,c) and
loose clothing (b,c).
Ablation on the supervision.Tab.7showstheimpactofdifferentlosssupervision
employed during training. Note that our variant with L -only supervision (Tab. 7-
1
a) already outperforms the human template-conditioned generalizable NeRF methods
NHPandNIA,whicharealsotrainedwithL -onlysupervision,intermsofperceptual
1
metrics LPIPS and FID. This validates that our gain is not only from the different
supervision but also from our proposed multi-scaffold. Our full model that leverages
multi-viewsupervisionwithL ,SSIM,andmasklossachievesthehighestperformance
1
ontheperception-basedmetrics.Notethatmulti-viewsupervisionispossiblebylever-
aging the fast 3D Gaussian splatting.
Ablation on the number of input views at inference. We trained our model
using 3 input views and tested with different number of input views at inference time
in Tab. 8. The performance improves as more observations are available. However,
notethatourperformancewhenonlygiventwoviewsisstillcomparabletothe3-view
results. This demonstrates the effectiveness of our method under sparse view setting.
Performance on the randomly selected input views. During evaluation, we
followedtheconventionofprevioussparseview3Dhumanreconstructionworks[20,21]
that use 3 uniformly distributed inputs. However, we additionally ran the evaluations
given 3 random views 10 times and computed the mean metrics. We verified that the22 Kwon et al.
Fig.8: Multi-scaffoldhelpsreconstructhairandlooseclothing.S denotesthenumber
of outer scaffolds.
Table 8: Ablation study on the number of input views at inference. We
trained our model using 3 input views, and tested with different numbers of input
viewsatinferencetime.Theperformanceimprovesasmoreobservationsareavailable.
# Inputs. PSNR↑ LPIPS↓ FID↓
1 20.08 152.54 99.13
2 21.79 132.61 78.56
3 21.90 133.41 61.67
4 22.01 133.68 53.40
5 22.07 131.80 35.00
performancedifferencebetweentheuniformlyandrandomlysampledinputsisminimal
– PSNR is 1.5%, and LPIPS is 0.3%.
D.4 Runtime at inference
Our GHG runs at 4fps for rendering a single 1K (1024×1024) image on a single
NVIDIA RTX A4500 GPU. However, note that inpainting network takes most of our
runtime (74%). Without the inpainting network, ours runs at 15fps. More efficient
inpainting model can be explored to further reduce the runtime.
The detailed breakdown of runtime is as follows. Our pipeline can be divided into
three stages: (1) constructing multi-scaffold (2) Gaussian parameter map generation
(3) rasterization. (1) Constructing multi-scaffold: RGB map for each scaffold is
aggregated on the UV space of human template. Our inpainting network inpaints theGeneralizable Human Gaussians (GHG) 23
missing regions of the innermost scaffold RGB map in 180.89 ms. (2) Gaussian pa-
rameter map generation: Multi-Gaussian parameter maps are generated in 57.97
ms. (3) Rasterization: Rasterization takes 5.78 ms. In total, GHG takes 244.65 ms
to render a single 1K image.
We would like to highlight that our method runs faster than the sparse-view gen-
eralizabl human NeRF methods NHP and NIA (0.01fps to render a single 1K image)
while outperforming their visual quality.
E Implementation details
ℰ𝑎𝑝𝑝𝑟
2 2 2
𝐼𝑐𝐶 𝑀𝒄
5) v2 n3
o( C5 × kco
lB
s)2
e3
(×
kco
lB
s)8
e4
(×
kco
lB
s)6
e9
(×
𝑀𝒄
R R R
+
+ + 3) v2 n3 o( C3 × 3 v) n4 o( C3 × 𝑀𝒒
2 2 2 2 2 2
𝑀𝒑 5) v2 n3 o( C5 × kco
lB
s)2 e3 (× kco
lB
s)8 e4 (× kco
lB
s)6 e9 (× kco
lB
s)6 e9 (× kco
lB
s)8 e4 (× kco
lB
s)2 e3 (× 3) v2 n3 o( C3 × 3 v) n3 o( C3 × 𝑀𝒔 𝑴
R R R R R R
ℰ𝑔𝑒𝑜 𝐷𝑑𝑒𝑐 3) v2 n3 o( C3 × 3 v) n1 o( C3 × 𝑀𝛼
𝑀𝒑
Fig.9: Network architecture for Gaussian parameter map generation.
E.1 Gaussian parameter map generation
ThearchitecturedesignofourGaussianparametermapgenerationnetworkispresented
in Fig. 9. Our network is composed of two encoders E , E and one decoder D .
appr geo dec
The feature maps extracted by E and E are added together before being fed
appr geo
intoD .Moreover,M andM aresentintoSoftplus andSigmoid activationlayers,
dec s α
respectively,aftertheconvolutionlayers.Notethatinthefigure,thenumberfollowing
each layer name and sitting in the bracket denotes its output channel size.24 Kwon et al.
(a) (b) (c)
(d) (e) (f)
SMPL-X GT mesh Transferred
SMPL-X texture map
Fig.10: Illustration of texture transfer on to the SMPL-X UV space. For
each point on the SMPL-X model (a), the nearest point on the scanned mesh (b) is
found. Then, we get the corresponding position of this point on the scan’s UV map
(e), which will be mapped to the matching location on the SMPL-X’s UV map (d).
Resulting on the transferred texture map (f) and the colored mesh (c).
E.2 Inpainting
Pseudo ground truth generation To create the pseudo ground truth texture map
on the SMPL-X UV space, we follow the approach proposed in Lazova et al [23]. The
processisillustratedinFig.10.ForeachpointontheSMPL-Xmodel,weidentifythe
nearestpointonthescannedobject.Next,wedeterminethecorrespondingpositionof
thispointonthescan’sUVmap.Wethentransferthecolorfromthispositiononthe
scan’s UV map to the corresponding location on the SMPL-X’s UV map.
NetworkarchitectureFig.11showstheinpaintingmodulearchitecture.Theinpaint-
ing network follows the DeepFillv2 design [53]. The inpainting network is composed
of a generator G and a discriminator D . In the generator, all convolutions
inpaint inpaint
are gated convolutions with a kernel size of 3×3 if not specified, where GatedConv,
DilateGatedConv,GatedConvDown,GatedConvUp haveastrideof1,1,2,0.5,respec-
tively. The four DilateGatedConv layers in DilatedBlock have a dilation of 2, 4, 8, 16,
respectively.TheAttention layerisaself-attentionlayer.Inthediscriminator,allcon-
volutions are common 2D convolutions, where Conv, ConvDown have a stride of 1, 2,
respectively. Besides, all convolution layers are followed by ELU activation. Note that
in the figure, the number following each layer name and sitting in the bracket denotes
its output channel size.Generalizable Human Gaussians (GHG) 25
2 4 8 16
× 5vn o C d)8 e4 ta( G5 )84( vn o C d etaG )69( n w o D vn o C d etaG )69( vn o C d etaG )291( n w o D vn o C d etaG )291( vn o C d etaG vn o C d etaG d e) t2 a9 li1 D( vn o C d etaG d e) t2 a9 li1 D( vn o C d etaG d e) t2 a9 li1 D( vn o C d etaG d e) t2 a9 li1 D( vn o C d) e2 t9 a1 G( p U vn o C d e) t6 a9 G( vn o C d e) t6 a9 G( p U vn o C d e) t8 a4 G( vn o C d e) t4 a2 G( vn o C d et) a3 G( )3( h n aT
Encoder DilatedBlock Decoder
𝓖 𝒊𝒏𝒑𝒂𝒊𝒏𝒕Stage 2
MI )291( red o cn
E𝓖 𝒊𝒏
vn o C d) e2 t9 a1
G(𝒑𝒂𝒊𝒏
kco lB d e) t2 a9 li1
D(𝒕 Sta
vn o C d) e2 t9 a1
G(ge 1
)291( red o ceD
) )2 29 91 1(
(
r re ed do
oc cn nE
E
kc
vo nl oB Cd
de) et2
)a
t9
8
ali1
4
GD(
( )291(
n
o itn
ettA
vn
o C
d) e2 t9 a1 G(
vn
o C
d) e2 t9 a1 G(
etan etacn o
c
vn o C d) e2 t9 a1 G( )3( red o ceD
× 7)8 vn4 o( C7 × 4n w o D)6 vn9 o( C4 × 4n w o) D29 vn1 o( C4 × 4n w o) D29 vn1 o( C4 × 4n w o) D29 vn1 o( C4 × 4n w o D v) n1 o( C4 Real or Fake tensor of shape ( 3𝐻 2, 3𝑊 2,256)
𝓓
𝒊𝒏𝒑𝒂𝒊𝒏𝒕
Fig.11: Inpainting network.