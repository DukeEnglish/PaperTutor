Why Do You Grok?
A Theoretical Analysis of Grokking Modular Addition
MohamadAminMohamadi†‡ ZhiyuanLi† LeiWu§ DanicaJ.Sutherland‡¶
Abstract
Wepresentatheoreticalexplanationofthe“grokking”phenomenon(Poweretal.,2022),
whereamodelgeneralizeslongafteroverfitting,fortheoriginally-studiedproblemofmodular
addition. First,weshowthatearlyingradientdescent,whenthe“kernelregime”approximately
holds,nopermutation-equivariantmodelcanachievesmallpopulationerroronmodularaddition
unlessitseesatleastaconstantfractionofallpossibledatapoints. Eventually,however,models
escapethekernelregime. Weshowthattwo-layerquadraticnetworksthatachievezerotraining
losswithboundedℓ normgeneralizewellwithsubstantiallyfewertrainingpoints,andfurther
∞
showsuchnetworksexistandcanbefoundbygradientdescentwithsmallℓ regularization. We
∞
furtherprovideempiricalevidencethatthesenetworksaswellassimpleTransformers,leavethe
kernelregimeonlyafterinitiallyoverfitting. Takentogether,ourresultsstronglysupportthecase
forgrokkingasaconsequenceofthetransitionfromkernel-likebehaviortolimitingbehaviorof
gradientdescentondeepnetworks.
1 Introduction
Understandingthegeneralizationpatternsofmodernover-parameterizedneuralnetworkshasbeena
long-standinggoalofthedeeplearningcommunity. Poweretal.(2022)demonstratedanintriguing
phenomenontheycalled“grokking”whenlearningtransformersonsmallalgorithmictasks: neural
networks can find a generalizing solution long after overfitting to the training dataset with poor
generalization. This observation has lead to a stream of recent works aimed at uncovering the
mechanisms that can lead a network to “grok,” and properties of the final solutions, on various
algorithmic tasks. Later, it was discovered that grokking can happen in tasks beyond modular
arithmetic: inlearningsparseparities(Baraketal.,2022;Bhattamishraetal.,2023),imageclassifiers
(Liuetal.,2023),greatestcommondivisors(Charton,2024),matrixcompletion(Lyuetal.,2024),
andk-sparselinearpredictors(Lyuetal.,2024).
Grokkinghasbeenvariouslyattributedtodifficultyofrepresentationlearning(Liuetal.,2022),
the“slingshot”mechanism(Thilaketal.,2022),weightnorm(Liuetal.,2023;Varmaetal.,2023),
propertiesofthelosslandscape(Notsawoetal.,2023),simplicityofthelearnedsolution(Nandaetal.,
2023)andotherfeaturelearningmechanisms(Levietal.,2024;Rubinetal.,2024). Theoretically,
Gromov(2023)presentedananalyticalconstructionforatwo-layerMLPthatsolvesmodularaddition
iscompatiblewithagrokkingpattern.1 Kumaretal.(2024)demonstratedgrokkingwhentraininga
two-layerMLPonapolynomialregressionproblem,asdidXuetal.(2024)forXORdata. Thenotion
ofdelayedgeneralizationwasperhapsearlierobservedbyLietal.(2022)whentrainingdiagonal
†ToyotaTechnologicalInstituteatChicago ‡UniversityofBritishColumbia §PekingUniversity ¶Amii
Correspondenceto{mohamadamin,zhiyuanli}@ttic.edu,dsuth@cs.ubc.ca.
1Gromov(2023)claimsthissolutionistheonefoundbygradientdescent,butthisdidnotseemtobethecaseinour
experience.
1
4202
luJ
71
]GL.sc[
1v23321.7042:viXraChange of NTK Scale of init Sample Complexity
1.00
1.0 1.0
60000
50000 0.8 0.8 0.80
=10
40000 0.6 0.6 =7.5 0.60 p=113
30000 =5 p=233
0.4 0.4 =2.5 0.40 p=313
20000 ×109 p=433
10000 Train 0.2 0.2 0.20 p=547
Test p=613
0 0.0 0.0 0.00
102 103 104 5 104 102 103 5 103 2.5 104 2 p1.5 3 p1.5 4 p1.5 2 p5/3
Step Step Train Size
Figure1: Empiricalinvestigationintogrokkingmodularadditionontwo-layernetworksintheclassification
taskwithcross-entropyloss. Left: ChangeofempiricalNTK1(∥Θˆ −Θˆ ∥ )isnegligiblebeforefittingthe
t 0 F
trainingdata. NTKchangesdrasticallyafteroverfitting,implyingthatthedelayedgeneralizationmightbe
causedbydelayedatransitioningfromkerneltorichregime. Middle: Reducinginitializationscalecan
mitigategrokking,tothepointofcompletelyeliminatingthegapbetweentrainandtestcurves. αdenotes
scalemultipliedbyθ ,theinitialweightsaccordingtodefaultPyTorchinitialization(Heetal.,2015). The
0
dashed lines indicate train set statistics, and the solid lines correspond to the test set. Right: Empirical
evaluationssupportasamplecomplexityofO˜(p5/3)ontheclassificationtaskwithcross-entropyloss. More
detailsinSection4.
linearnetworkswithlabelnoiseSGDandthroughsharpnessminimization,beforeitwasknownas
grokking(Poweretal.,2022).
Lyuetal.(2024)presentarigoroustheoreticalframeworkinwhichgrokkingcanbeprovably
demonstratedthroughadichotomyofearlyandlateimplicitbiasesofthetrainingalgorithm. More
specifically,theyattributetheoverfittingstageofgrokkingtotheinitialbehaviorofgradientdescent
beingsimilartoakernelpredictor(Jacotetal.,2018;Aroraetal.,2019b;Leeetal.,2019),andthe
generalizationstageistodifferentlate-phaseimplicitbiasessuchassharpnessminimization(Blanc
etal.,2020;Lietal.,2022;Damianetal.,2021;HaoChenetal.,2020),marginmaximization(Soudry
etal.,2018;Nacsonetal.,2019;Weietal.,2019;LyuandLi,2020),orparameternormminimization
(Gunasekaretal.,2017;Gunasekaretal.,2018;Aroraetal.,2019a). Thistransitionfromkernelto
richregimehasbeenwidelyobserved(Chizatetal.,2019;Moroshkoetal.,2020;Geigeretal.,2020;
Telgarsky,2022;Lyuetal.,2024). Consistentwiththisframework,Kumaretal.(2024)hypothesized
thatgrokkingcanbeexplainedthroughthetransitionfromthekernelregimetothe“rich”regime,as
longasthesizeofthetrainingdatasetisneithertoosmall(wheregeneralizationwouldbeimpossible)
nortoolarge(wheregeneralizationwouldbeeasy). Theyprovidedempiricalsupportbyconsidering
scalingthemodeloutput,whichisaroughproxyfortherateoffeaturelearninginmodularaddition
ontwo-layerMLPs.
Thisdichotomybetweenearlykernelregimeandlatefeaturelearningtriggeredbyweakimplicit
orexplicitregularization(i.e.thetransitionfromlazytorichregime)seemstobethemostpromising
theorytoexplaingrokking. Evenso,twofundamentalquestionsastowhygrokkingoccursonthe
originalproblemof modularadditionhaveremainedunanswered:
1. Whydoawidevarietyofarchitecturesallfailtogeneralizeintheinitialphaseoftraining,i.e.
inthekernelregime? Isitbecausetheirkernelsaccidentallysharesomecommonproperty,or
itisindeedapropertyofthemodularadditiontaskitself?
2. Howdoesweakregularizationencouragethenetworktolearngeneralizablefeatureslaterin
1Θˆ is the NTK of the model output on the training data using the parameter at step t. When the model output
t
f isavector,weusetheNTKofitsfirstcoordinateasanapproximation,following(Mohamadietal.,2023),where
Θˆ ≜[∇ f (θ ;X )][∇ f (θ ;X )]⊤.
t θ 1 t train θ 1 t train
2
F||0
t
||
ycaruccA ycaruccA
ycaruccA
tseT
tseBtraining?
OurContributions. Inthiswork,weaddressthesequestionswithrigoroustheoreticalanalyses
oflearningmodularadditionwithgradientdescentontwo-layerMLPs.
Wespecificallyfocusontheproblem
a+b ≡ c (mod p) (1.1)
wherea,b,c ∈ Z forafixedp ∈ N. Weuse(regularized)gradientdescentonatwo-layerMLPwith
p
quadraticactivations,thesamearchitectureasGromov(2023). Weconsidertwodifferenttasks: it
issomewhateasiertoanalyzea“regression”taskwhereweusesquarelosstolearnafunctionof
(a,b,c)thatindicateswhether(1.1)holds,butwealsostudythe“classification”taskinwhichwe
usecross-entropylosstolearnap-wayclassifiertopredictwhichcvaluesatisfies(1.1)foragiven
(a,b). Thisdiscrete,noiselesssettinghasonlyafinitenumberofpossibledistinctdatapoints: p3 for
regression,p2 forclassification.
To address the first question, we prove in Sections 3.1 and 4.1 that this task is fundamentally
hard forpermutation-equivariantkernelmethods,duetoinherentsymmetries. Thus,permutation-
equivariant networks which are well-approximated by their neural tangent kernel approximation
cannotgeneralizewell. Wealsoprovethat,althoughnosuchmethodcangeneralize,neuraltangent
kernelapproachesbasedonourarchitecturewithrealisticwidthscanachievezerotrainingerror. This
resultishighlysuggestiveofwhydrasticoverfittingwithpoorgeneralizationhasbeenempirically
observedonthisproblemacrossawidevarietyofarchitectures,losses,andlearningalgorithms(e.g.
Poweretal.,2022;Liuetal.,2022;Gromov,2023).
Toprovethegeneralizationlowerbounds,wedevelopanovelgeneraltechniquetoanalyzethe
populationℓ lossoflearninggeneralfunctionclasseswithpredictorsofintrinsicdimensionn(for
2
instancekernelpredictorswithntrainingpoints),presentedinAppendixD.4. Thisframeworkallows
ustoprovelowerboundsonpopulationℓ lossforthegeneralcaseoflearningmodularadditionon
2
msummands(ratherthan2or3)withkernels,andmightbeofindependentinterest.
To address the second question – why this occurs – we identify ℓ norm as an effective and
∞
practically-relevantcomplexitymeasureforgeneralization,whichiscloselyrelatedtotheimplicit
biasofAdam(XieandLi,2024;Zhangetal.,2024),andshowthatnetworkswithsmallℓ norm
∞
intheregressionsetting(Section3.2)andlargeℓ -normalizedmarginintheclassificationsetting
∞
(Section4.2)canbothprovablygeneralizewithfarfewersamplesthanrequiredinthekernelregime.
Thus,modelswithcorrespondingimplicitbiasescangeneralizewell,answeringthesecondquestion.
Inregression,ourproofsarebasedon“optimisticrates”(Srebroetal.,2010)forthesmoothℓ lossin
2
termsoftheRademachercomplexityofnetworkswithboundedℓ parameternorm. Inclassification,
∞
ourproofappliesthePAC-BayesianframeworkofMcAllester(2003)tonetworkswithboundedℓ
∞
parameternorm.
Insummary,ourmaincontributionsareasfollows:
1. We prove that networks in the kernel regime can only generalize if trained on at least a
constantfractionofallpossibledatapoints,i.e.anΩ(1)portion,forregression(Section3.1)
andclassification(Section4.1).
2. Weprovethatnetworkswithappropriateregularizationcangeneralizewithmanyfewersamples:
O(1/p)portionofallpossibledatapointsforsquarelossgeneralizationontheregressiontask
(Section3.2),andO˜(1/p1/3)forzero-onelossgeneralizationonclassification(Section4.2).
32 Preliminaries and Setup
Notations. Weuse[p]todenotetheset{1,...,p}. Weusee todenotetheistandardbasisvector,
i
i.e.,thevectorwith1initsithcomponentand0elsewhere. Forvectora,weusea⊙2 denotesthe
element-wisesquareofthevectoraForanynonemptysetX,asymmetricfunctionK : X ×X → R
iscalledapositivesemi-definitekernel(p.s.d.) kernelonX ifforalln ∈ N,allx ,...,x ∈ X,and
1 n
allλ ,...,λ ∈ R,itholdsthat(cid:80)n (cid:80)n λ λ K(x ,x ) ≥ 0. Giventwonon-negativefunctions
1 n i=1 j=1 i j i j
f,g,wesaythatf(n) = O(g(n))(resp. f(n) = Ω(g(n)))iff. thereexistsabsoluteconstantC > 0,
suchthatforalln ≥ 0,f(n) ≤ Cg(n)(resp. f(n) ≥ Cg(n)). Weusef(n) = ω(g(n))todenote
thatforallC > 0,thereexistsn > 0suchthatforalln ≥ n ,f(n) > Cg(n).
0 0
Setup. Wefocusonlearningmodularaddition,(1.1),withatwo-layernetworkwithnobiasesand
quadraticactivation,followingGromov(2023). Morespecifically,givenparametersθ = (W,V),the
modelf mapsthepairofintegers(a,b)–representedasthevector(e ,e ) ∈ R2p –toavectorin
a b
Rp. Weusetheform
f(θ;(e ,e )) = V (W(e ,e ))⊙2, (2.1)
a b a b
whereW ∈ Rh×2p,V ∈ Rp×h forsomeintegerh. Wecallhthewidthofthehiddenlayeranditwill
besetlater.
Intheclassificationsetting(Section4),wetrainf withcross-entropylosstoidentifythecsuch
thata+b ≡ c (mod p)inamulti-classificationsetting. LettingZ = {e : i ∈ [p]},thesetofall
i
possibleinputsisX = Z ×Z andoutputsisY = Z;thereareN = p2 distinctdatapoints.
Intheregressionsetting(Section3),weinsteadaimtolearnforeachtuple(a,b,c),ifa+b ≡
c (mod p), that is, map x = (e ,e ,e ) to the scalar y = p1(a + b ≡ c (mod p)) using
a b c
g(θ;(e ,e ,e ))) ≜ e⊤f(θ;(e ,e )).2 HereX = Z3,Y = {0,p},andN = p3;wetrainthemodel
a b c c a b
g(θ;x) = ⟨e ,f(θ;(e ,e ))⟩tominimizethesquareloss.
c a b
Ineithersetting,weuseD todenotethedistributionoverX ×Y whichisuniformoverallN
possible input-output pairs, while D = {(x ,y )}n is the training dataset of size n. Given
train i i i=1
parameters θ, we use Ψ(θ;·) : X → Yˆ to denote the predictor (g(θ;·) for regression, f(θ;·) for
classification), we train the model with gradient descent with tiny ℓ regularization,3 which is
∞
intendedtoemulatetheimplicitbiasofAdam(AdamW)whichmightleadtogrokking. Ingeneral,
wedefinelossas
1 (cid:88)
Lλ(Ψ,θ,D ) ≜ ℓ(Ψ(θ;x),y)+λ∥θ∥ , (2.2)
ℓ train n ∞
(x,y)∈Dtrain
where ℓ : Yˆ ×Y → R denotes either square (ℓ ) or cross-entropy loss and λ ≥ 0 controls the
≥0 2
strengthofregularization. Omittingλreferstothecasewherenoregularizationisusedandomitting
D meansthewholepopulationisusedinevaluatingtheloss. Fornon-parametricfunctions(like
train
Ψ denotedasthepredictorwhichreturnszeroonanyinput)wedropθ aswell.
0
Connectionbetweenℓ -normRegularizationandGrokking. Poweretal.(2022)showsthat
∞
grokkingphenomenonhappensforAdamandgetsmoreprominentwithdecoupledweightdecay
(AdamW).RecentworksbyXieandLi(2024)andZhangetal.(2024)showthatAdamimplicitly
regularizestheℓ normoftheweights. Indetail,XieandLi(2024)showsthatAdamWcanonly
∞
convergetoKKTpointsofℓ -normconstrainedoptimization. Zhangetal.(2024)showsthatAdam
∞
converges to max-margin solutions w.r.t. ℓ norm for linear models on separable datasets and
∞
2ThisscalingimpliesthatthepredictorΨ (·)=0haspopulationsquarelossp;thisscalingallowsbounded∥θ∥ .
0 ∞
3MoredetailsabouttheregularizationstrengthcanbefoundinAppendixA.
4conjecturethisresultcouldbegeneralizedtogeneralhomogeneousmodels(includingourmodelf,
whichis3-homogeneous). Thisconnectionmotivatesustousetheexplicitℓ -normregularization
∞
ontopofgradientdescenttounderstandgrokkinginthemodularadditionsetting.
Definition2.1. AdeterministicsupervisedlearningalgorithmAisamappingfromasequenceof
trainingdata,D ∈ (X ×Y)n,toahypothesisA(D ) : X → Yˆ. ThealgorithmAcouldalso
train train
berandomized,inwhichcasetheoutputA(D )isadistributionoverhypotheses. Tworandomized
train
algorithmsAandA′ arethesameifforanyinput,theiroutputshavethesamedistributioninfunction
space,writtenasA(D ) =d A′(D ).
train train
We further define equivariance of learning algorithms which is the main component of our
analysis in deriving lower bounds. This definition is used in Theorem 3.3 and Proposition 4.2 to
proveequivarianceofGDinlearningmodularaddition.
Definition 2.2 (Equivariant Algorithms). A learning algorithm is equivariant under group G
X
(or G -equivariant) if for any dataset D ∈ (X × Y)n and for all T ∈ G , it holds that
X train X
A({(T(x ),y )}n ) ◦ T =d A({(x ,y )}n ). For deterministic learning algorithms, this is
i i i=1 i i i=1
equivalenttosayingthatforallx ∈ X,A({(T(x ),y )}n )(T(x)) = [A({(x ,y )}n )](x).
i i i=1 i i i=1
Definition2.3(KernelMethods). ForYˆ ⊆ R,wesayalearningalgorithmAisakernelmethod ifit
firstpicksa(potentiallyrandom)positivesemi-definitekernelK onX beforeseeingthedata,4 and
thenoutputssomehypothesisΨsuchthatthereexist{λ }n ∈ RforwhichΨ(·) = (cid:80)n K(·,x )λ ,
i i=1 i=1 i i
whereλ candependonthetrainingdatasetD .
i train
In particular, when λ(D ) = K†y, where K = (K(x ,x ))n , y = (y )n , the kernel
train i j i,j=1 i i=1
methodiscalleda(ridgeless)kernelregressionmethod.
Theorem 2.4. For any p.s.d. kernel K : X ×X → R and transformation group G X, kernel
g
regression (Definition 2.3) with respect to kernel K is G -equivariant if and only if kernel K is
X
equivarianttoG X,i.e.,K(T(x),T(x′)) = K(x,x′)foranyT ∈ G X andx,x′ ∈ X.
g g
ProofofTheorem2.4. ForanyD = {(x ,y )}n andtransformationT ∈ G ,letDT bethe
train i i i=1 X train
transformeddataset{(T(x ),y )}n ,andA bethekernelregressionalgorithmw.r.t.K. Forany
i i i=1 K
x,wehavethat
n n
(cid:88) (cid:88)
A(T(D ))(T(x)) = K(T(x),T(x ))λ (DT ) = K(x,x )λ (D ) = A(D )(x),
train i i train i i train train
i=1 i=1
wherethesecondequalityfollowsfromtheequivarianceofthekernelK w.r.t. G .
X
3 Regression Task
Wewillfirstpresentourtheoreticalanalysisofthegrokkingphenomenononmodulararithmeticwith
two-layerquadraticnetworksintheregressionsetting,wherewelearnafunctionfrom(e ,e ,e )to
a b c
p1(a+b = c (mod p)). Althoughthisisperhapsalessnaturalwaytomodelmodulararithmetic
thantheclassificationtask,itadmitssomeusefultheoreticaltools.
Inthissection,weshowthatnetworksinthekernelregimewillprovablyfailtogeneralizeaslong
astheydonothaveaccesstonearlyallthepointsinthedataset(Theorem3.4),althoughtheycan
achievezerotrainingerror(Theorem3.1). However,thenetworkeventuallyleavethekernelregime
4Ourdefinitionofkernelmethodsdoesnotcoverlearningalgorithmsthatchoosethekernelbasedonthetrainingdata.
Anylearningalgorithmcouldbeframedasakernelmethodwithadata-dependentkernelK(x,x′)=Ψ(x)Ψ(x′).
5Scale of init Change of NTK
1.0 =0.1 30000 1.0
=0.05
0.8 =0.01 25000 0.8
=0.005 20000
0.6 0.6
15000
0.4 0.4
10000
0.2 5000 Train 0.2
Test
0.0 0 0.0
102 103 104 105 102 103 104 105
Step Step
Figure 2: Empirical evidence for kernel regime in early training. Left: Train (dashed) and test
(solid) accuracy while training in the regression setting with various initialization scales α and a
fixedp = 47. Shrinkingthescaleofinitializationcanmitigategrokkingintheregressiontask,
eventuallyeliminatingofthegapbetweentrainandtestaccuracies(atthecostofslowerimprovement
ineach). Right: eNTKcontinuestosignificantlychangeafteroverfitting.
becauseoftheweakℓ normregularization. Wefurtherestablishthatifthenetworkmanagesto
∞
achievezerotrainingerrorwithsmall∥θ∥ ,itwillgeneralizewithonlya n = ω˜(1/p)portionof
∞ N
theoveralldataset,aslongasthewidthofthenetworkislargerthan4p(Theorem3.6). Wealsoprove
thatsuchnetworksexist(Proposition3.8),anddemonstratethatgradientdescentcanfindthemwitha
smallamountofexplicitregularization(Figure3).
3.1 KernelRegime
A recent line of work on the neural tangent kernel (NTK) framework (Jacot et al., 2018; Arora
et al., 2019b; Lee et al., 2019; Chizat et al., 2019; Yang and Hu, 2021) has shown that with
typical initialization schemes, gradient descent in over-parameterized neural networks locally
approximates the behavior of a kernel model using the empirical neural tangent kernel (eNTK)
K (x,x′) ≜ ∇g(θ;x)∇g(θ;x′)T. Inthe“kernelregime,”thechangeinθoverthecourseofgradient
θ
descentdoesnotsubstantiallychangetheeNTKK ,andhencetheneuralnetworkbehavessimilarly
θ
toakernelpredictortrainedwithK . Withsquareloss,ashere,thesekernelpredictorsfollowa
θ0
particularlysimpleoptimizationpathforwhichaclosedform(correspondingtokernelregression)is
available.
For networks of finite width (and in certain infinite-width cases), the eNTK will stay roughly
constantandthenetworkwillcloselytrackthekernelmodelforthefirstphaseofoptimization,but
thetinyregularizationwilleventuallyleadittodepartthekernelregime. Thus,boundsonkernel
modelsareinformativeaboutdeepnetworksinthefirstpartofoptimization.
Inthefirstphaseofgrokking,themodeloverfitstothetrainingdata,achievingverylowtraining
loss but retaining high loss on test points.We establish that both phenomena occur in the kernel
regime.
3.1.1 EmpiricalNeuralTangentKernelsCanAchieveZeroTrainingError
Ourfirstresultshowsthatkernelregressionwiththeempiricalneuraltangentkernelofourquadratic
network achieves zero training loss if the network is mildly wide, for instance, n = Θ˜(p2) and
widthh = Θ˜(p). Thisimplies,forexample,thatina“lazytraining”setting(Chizatetal.,2019),or
alternativelywhenthefirstlayerisinitializedwithhugeweightsandthesecondlayerwithtinyones,
gradientdescentcanachievezerotrainingloss. Informally,thisalsostronglysuggeststhatnetworks
6
ycaruccA F||0
t
||
ycaruccAwithmoretypicalinitializationscanachieveverysmalltrainingerrorwithoutneedingtoleavethe
“kernelregime.”
Theorem 3.1. Initialize the network of Section 2 with any values for V, and entries of W all
independentlyUniform([−s ,s ])forsomes > 0. Letthemostfrequentvalueofc ∈ [p]appear
W W W
ρ times. Then, if h > 36ρ log(n/δ), it holds with probability at least 1−δ over the random
c c
initializationofW thatkernelregressionusingthenetwork’sempiricalneuraltangentkernelcan
achieve zero training loss for any target labels such that if x = x then y = y . Conversely, if
i j i j
h < n/(3p), there exist target labels (with y = y when x = x ) for which this method cannot
i j i j
achievezerotrainingloss.
Uniforminitializationisthedefaultbehaviore.g.inPyTorch(Anseletal.,2024). Notethatρ
c
is always between n/p and n. Moreover, in the usual setting with randomly selected D and
train
n = Ω(cid:0) plogp(cid:1)–recallthatfullknowledgeofasingle(a,b)pairrequiresn = pintheregression
setting–wehavethatρ = Θ(n/p)withhighprobability(RaabandSteger,1998,Theorem1). Thus
c
inthisusualsetting,thethresholdforinterpolationisatanetworkwidthh = Θ˜(n/p).
ProofsketchofTheorem3.1. Kernelregressioncanachieveallpossibletargetlabelsiffitskernel
matrixisstrictlypositivedefinite(LemmasB.1andB.2). Evaluatingtheexpectedneuraltangent
kernel, we can see it is strictly positive definite under only mild assumptions on the weights
(PropositionB.3). Withboundedweights,amatrixChernoffboundcontrolstheconvergenceofthe
lowesteigenvalueofthekernelmatrixtothatoftheexpectedkernelmatrix(PropositionB.4). The
lowerboundfollowsfromtherankofthekernelmatrix(PropositionB.6).
3.1.2 Permutation-EquivariantKernelMethodsCannotGeneralize
Wenowshowthatforanypermutation-equivariantkernelmethod(Definition2.3)(andhencefor
networkstrainedbygradientdescentcloseenoughtoinitialization),generalizationispossibleonly
whentrainingonn = Ω(p3)samples,i.e.theportionofallpossibledatapointsis n = Ω(1).
N
Thekeycomponentofouranalysisisthepermutationequivarianceoflearningmodularaddition
inthissetting. Wefirstdefinethepermutationgrouponthemodularadditiondata:
Definition3.2. LetS denotethesetofallpermutationson[p]. WedefinethepermutationgroupG
p X
onX asthegroup
(cid:8) (e ,e ,e ) (cid:55)→ (e ,e ,e ) : σ ,σ ,σ ∈ S (cid:9) ,
a b c σ1(a) σ2(b) σ3(c) 1 2 3 p
withthegroupoperationbeingcompositionofeachpermutation.
Thefollowingtheoremestablishesthatourlearningalgorithmisequivariant(Definition2.2)under
thispermutationgroup,whichwecallpermutation-equivariantforshort. Notethatthisresultapplies
totheactualprocessofgradientdescentonourneuralnetwork,notonlytoitsNTKapproximation.
(ThisresultisnotparticularlyspecifictothearchitecturedefinedinSection2;itholdsbroadly.)
Theorem3.3(PermuationEquivarianceinRegression). Trainingatwo-layerneuralnetworkwith
quadraticactivations(suchasg(θ;x))initializedwithrandomi.i.d. parametersusinggradient-based
updaterules(suchasgradientdescent)onthemodularadditionproblemintheregressionsetting
isanequivariantlearningalgorithm(asdefinedinDefinition2.2)withrespecttothepermutation
groupofDefinition3.2appliedontheinputdata.
7MoredetailsandthefullproofareinAppendixC.
Roughlyspeaking,thisindicatesthatlearningthemodularadditiontaskonthissetupisexactly
thesamedifficultyaslearninganypermutedversionofthedataset. Followingthesameargument,we
canshowthatthekernelmethodcorrespondingtotheneuralnetsintheearlyphaseoftrainingisalso
permutation-equivariant.
Further, as the distribution D is uniform and thus invariant under permutation, Theorem 3.3
showsthatthedifficultyoflearningtheoriginalground-truthisthesameassimultaneouslylearning
allthepermutedversionsoftheground-truths,whichturnsouttobedifficultforanykernelmethod.
Thefollowingtheoremformalizesthisidea,establishingthatanysuchkernelpredictorneedsatleast
n = Ω(p3), or equivalently n = Ω(1), to generalize on the modular addition task; otherwise it
N
cannotdosubstantiallybetterthanthetrivialall-zeropredictor.
Theorem3.4(LowerBound). ThereexistsaconstantC > 0suchthatforanyp ≥ 2,trainingdata
sizen < Cp3,andanypermutation-equivariantkernelmethodA,itholdsthat
w p
E EL (A({(x ,y )}n )) ≥ L (Ψ ) = ,
(xi,yi)n i=1∼DnA ℓ2 i i i=1 2 ℓ2 0 2
whereE takesthemeanovertherandomnessinalgorithmA.
A
AfullproofofthisresultisdeferredtoAppendixD.
Theorem3.4isinfactaspecialcaseofamoregeneraltheoremthatlowerboundsthepopulationℓ
2
lossoflearningmodularadditionwithm-summandsusingpermutation-equivariantkernels,showing
thatpoorgeneralizationisinevitable. Wepresentaninformalversionofthisresultbelowandrefer
thereadertoCorollaryD.8fortheformalversion.
Theorem3.5(Informal). Considertheproblemoflearningmodularadditionoverm-summands
withatrainingsetofsizenusingapermutationequivariantkernelmethodA. Foranyp ≥ 2and
trainingdatasizen < Cpm theexpectedpopulationℓ lossisatleastΩ(p),whichisofthesame
2
magnitudeasthetrivialall-zeropredictor.
Poorpopulationlossisthusinevitableformodelswhicharewell-approximatedbyapermutation-
equivariant kernel; since our quadratic network can also achieve zero training error in the kernel
regime,thisstronglysuggestsdrasticoverfittinginearlytraining. Fortunately,however,regularized
gradientdescentonfinitenetworkswilleventuallyleavethekernelregimeandbeginlearningfeatures.
3.2 RichRegime
Whilethetransientbehaviorofgradientdescentafterleavingthekernelregimemaybecomplicated,it
isoftenthecasethatthelimitingbehaviorcanbebetter-understoodbasedonanalysesofimplicitbias.
Motivatedbythis,wepresentageneralizationanalysisofnetworksthatachievezerotrainingerror
(asweexpectinthelong-termoptimizationlimit)withbounded∥θ∥ . Thisassumptionismotivated
∞
byrecentworkofXieandLi(2024),whoshowthatfull-batchAdamWcanonlyconvergetoKKT
pointsoftheℓ norm-constrainedoptimizationproblem,min L(Ψ,θ,D ),whereλis
∞ ∥θ∥ ≤1 train
theweightdecaycoefficient. Wewilldiscussthefeasibilityofthis∞assuλmptionmoreafterwards.
Theorem 3.6 (Upper Bound). For any width h ≥ 8p, with probability at least 1 − δ over the
randomnessoftrainingdatasetD ofsizen,definethesetofinterpolatingsolutionsasΘ˜∗ = {θ |
train
L (Ψ,θ,D ) = 0}. Foranyinterpolatingsolutionθ∗ ∈ Θ˜∗ withsmallℓ norm,i.e.,satisfying
ℓ2 train ∞
that∥θ∗∥ = O(min ∥θ∥ ),itholdsthat
∞ θ∈Θ˜∗ ∞
(cid:18) p2 (cid:18) 1 1(cid:19)(cid:19)
L (g,θ∗) = O log3n+ log ·L (Ψ ).
ℓ2
n p δ
ℓ2 0
8Accuracy norm of parameters
1.0 p=113 5 p=113
0.8 p p= =2 33 13 3 4 p=233
p=313
p=433
0.6 3 p=433
0.4 2
0.2 1
0.0 0
102 103 104 105 102 103 104 105
Step Step
Figure3: Empiricalverificationofourtheoreticalexplanationforgeneralization. Wetrainanetwork
ofwidthh = 4pwithgradientdescentonℓ lossand10−4 ℓ -regularizationon2×p2.25 training
2 ∞
samples(outofp3)intheregressionsetting. Left: Generalizationhappenswhenthenumberof
samplesaremorethanΩ(p2),aspredictedbyProposition3.7. Thedashedandsolidlinesindicate
trainandtestsetstatisticsrespectively. Right: ℓ normofparametersaftergrokkingremainsthe
∞
samefordifferentproblemdimensionsp,aspredictedby
ComparingTheorem3.6toTheorem3.4,weseethatwhenn = ω˜(p2)andh = Ω(p),thereis
a strict separation between generalization in the kernel and rich regimes. Note that the classifier
ϕ(e ,e ) ≜ argmax f (θ∗;(e ,e )) has population error rate at most 2L (ϕ) (as shown in
a b c c a b p ℓ2
Proposition E.7) and thus, its population error goes to zero when n = ω˜(p2). The proof of
Theorem3.6consistsofshowingallnetworkswithsmalltrainingerrorandsmallℓ normsgeneralize
∞
(Proposition3.7),andatleastonesuchnetworkexists(Proposition3.8).
Proposition 3.7. For any R > 0,δ ∈ (0,1), D of size n, and θ∗ ∈ {θ = (W,V) :
train
L (g,θ,D ) = 0 ∧ ∥θ∥ ≤ R}, there exists a positive constant C > 0 such that with
ℓ2 train ∞
probabilityatleast1−δ overtherandomnessofD ,
train
CR6h2 (cid:18) 1(cid:19)
L (g,θ∗) ≤ plog3n+log .
ℓ2
n δ
ProofsketchofProposition3.7. WeboundtheRademachercomplexityofthesetofnetworkswith
smallℓ weights,andthenapplyTheoremE.6,duetoSrebroetal.(2010),whichgivesan“optimistic”
∞
boundontheexcessriskofsmoothlossfunctions. DetailsinAppendixE.
Proposition3.8. LetthesetofmodelswithzeropopulationlossbeΘ∗ ≜ {θ | L (g,θ) = 0}. For
ℓ2
(cid:106) (cid:107)−1
anyp ≥ 2andh ≥ 8p,Θ∗ isnonemptyandmin ∥θ∥ ≤ h 3.
θ∈Θ∗ ∞ 8p
ProofsketchofProposition3.8. The main difficulty is a manual Fourier-based construction of a
zero-losssolutionwithh = 8pandℓ normatmostone;thisisinspiredbysimilarconstructions
∞
ofGromov(2023)andresultsofNandaetal.(2023). Inaconcurrentwork,Morwanietal.(2024)
presented a similar construction. Once we have that, because our model is 3-homogeneous in
its parameters, we can easily reduce the ℓ norm by duplicating neurons, without changing the
∞
input-outputfunction. DetailsinAppendixF.
Weknowthataninterpolatingsolutionwithsmallℓ normexist,andthatanysuchsolutionwill
∞
generalize. Doesgradientdescentfindsuchasolution? InFigure3,weempiricallyevaluatetheℓ
∞
normofweightslearnedbyrunninggradientdescentontheregressiontask,withvaryingp,anda
9
||
||verysmallℓ regularizer.5 Consistentwithourmanualconstructionofweights,theℓ normofthe
∞ ∞
networkweightsdoesnotgrowwiththeproblemdimensionp. Thissupportstheapplicabilityof
Theorem3.6andafarbettersamplecomplexitycomparedtothekernelregime.
EmpiricalEvidencethatKernelLowerBoundsLeadtoOverfitting: Onewaytomitigatethe
grokkingeffectinlearningmodularadditionisbypreventingthenetworkfromoverfittingthetraining
setinthekernelregime. Todoso,onecanreducethescaleofinitialization(e.g.smallervariance
scales for weight initializations in the scheme of He et al. (2015)). Roughly speaking, networks
initializedwithaverysmallscaleofparametersneedtoundergoasignificantgrowthinthenormof
parameterstobeabletofitthetrainingdata. Thisnormgrowthwilleventuallyleadthenetworkto
leavethekernelregimeandbeginlearningfeaturesbeforeoverifttingthedata. Figure2confirmsthat
inoursetting: usingaverysmallweightinitializationcansubstantiallymitigatethegrokkingeffect
bypreventingthenetworkfromoverfittinginthekernelregime,incidactingthatgrokkingisindeed
causedbyadelayedtransitionfromkernel(overiftting)torich(generalizing)regime. However,this
comesatacost: trainingbecomesincreasinglymoredifficultasthescaleofinitializationdecreases
(Chizatetal.,2019;Moroshkoetal.,2020;Telgarsky,2022).
InFigure2,wealsoevaluatethechangeoftheempiricalNTKduringtraining,bycomputingthe
differenceineNTKmatricesthroughtraining. Tomakethisempiricalinvestigationcomputationally
feasible,weevaluatedtheeNTKapproximationofMohamadietal.(2023)on20,000randomdata
points,similartopreviousschemes(Fortetal.,2020;Weietal.,2019;Mohamadietal.,2023). We
seethatthechangeinempiricalNTKisordersofmagnitudelargerafteroverfittingthetrainingset,
implying that most feature learning happens only later, supporting our hypothesis that the initial
overfittingoccursroughlyinthekernelregime.
4 Classification Task
Wenowmoveontothemulti-classclassificationsettingestablishedinSection2,wherewetrainwith
cross-entropy loss. Similar to the regression problem, we first analyze the early stage of training
where the network operates like a kernel machine, and then move onto the rich regime and focus
ontheweightslearnedthroughmargin-maximizationimplicitbiasofgradientdescent. Weprove
thatasamplecomplexitygapbetweenkernel(Theorem4.3)andrich(Theorem4.6)regimeexists
when learning a two-layer neural network with gradient descent on modular addition modeled as
amulti-classclassificationtask. Ourresultsimplythataslongasthemax-marginimplicitbiasof
gradient descent on exponential-type loss functions drives the net to leave the kernel regime and
Ωˆ(p5/3)samplesareusedfortraining,generalizationtothewholepopulationisguaranteed.
4.1 KernelRegime
Inthemulti-classsetting,theoutputisp-dimensional,andthustheeNTKforeachpairofpointsisa
p×ppsdmatrix(seeÁlvarezetal.,2012). Ournotionofkernelmethods(Definition4.1)slightly
changesduetoaccountformulti-outputfunctions,andthemainlowerboundresultissimilartothat
intheregressioncase: kernelmethodsmustseeaconstantfractionofdatabeforelearning.
Definition4.1. ForY ⊆ Rp,wesayalearningalgorithmAisakernelmethod ifbeforeseeingthe
dataitfirstpicksa(potentiallyrandom)kernelK onX suchthatforanyx,x′ ∈ X,K(x,x′)isap
5Weusedaregularizationweightof10−4. Withoutexplicitregularization,asmallnumberofnetworkweightsdogrow
withp,butwebelievethisphenomenonisnotimportanttotheoverallbehaviorofgradientdescentonthistask.
10Accuracy Loss
1.0
p=113
15.0
p=233
0.8 12.5 p=313
p=433
0.6 10.0
7.5
0.4
p=113
5.0
p=233
0.2
p=313 2.5
p=433
0.0 0.0
102 103 104 5 104 102 103 104 5 104
Step Step
Figure 4: Empirical investigation of grokking in the classification setting with tiny ℓ regu-
∞
larization on different problem dimensions p. Networks are trained with normalized gradient
descent8 oncross-entropylossandanℓ regularizationof10−20 strength. Thedashedlinesinthe
∞
indicatetrainsetstatistics,andthesolidlinescorrespondtothetestset.
by p positive semi-definite matrix, and then outputs some hypothesis h based on D such that
train
thereexist{λ }n ∈ Rp forwhichh(·) = (cid:80)n K(·,x )λ .
i i=1 i=1 i i
We next establish the permutation-equivariance of gradient-based learning algorithms in the
classificationsetting. Theykeydifferenceisthatinthemulti-outputsetting,gradient-basedlearning
algorithmsareequivarianttopermutationsonbothinputdataandoutputlabels. Moredetailsare
availableinAppendixC.
Proposition4.2(PermuationEquivarianceinClassification). WedefinethepermutationgroupG
X,Y
onX ×Y (definedinSection2)asthegroup
(cid:8) (e ,e ),e (cid:55)→ (e ,e ),e : σ ,σ ,σ ∈ S (cid:9) .
a b c σ1(a) σ2(b) σ3(c) 1 2 3 p
Training a two-layer neural network with quadratic activations (such as f(θ;x)) initialized with
randomi.i.d. parametersusinggradient-basedupdaterules(asdefinedinDefinitionC.1)6 onthe
modularadditionproblemintheclassificationsettingisanequivariantlearningalgorithm(asdefined
inDefinition2.2)withrespecttoG .7
X,Y
Wesayalearningalgorithmisinput-outputpermutation-equivariantitisequivariantwithrespect
toG . Thefollowingtheoremestablishesthatanysuchkernellearningalgorithmneedsatleast
X,Y
n = Ω(p2),orequivalently n = Ω(1),togeneralizeonthemodularadditiontaskintheclassification
N
setting.
Theorem4.3(KernelLowerBound). ThereexistsaconstantC > 0suchthatforanytrainingdata
sizen < Cp2 andanykernelmethodAwhichisinput-outputpermutation-equivariant(withrespect
toG ),itholdsthat
X,Y
1
E EL (A({x ,y }n )) ≥ L (Ψ ),
(xi,yi)n i=1∼DnA ℓ2 i i i=1 2 ℓ2 0
whereE takesexpectationovertherandomnessinalgorithmA.
A
Intuitively,theclassificationsettingisnotverydifferentfromtheregressionsetting. Theyshare
the same goal and the amount of knowledge contained in each data in the classification setting is
exactlyequaltopdataintheregressionsetting,wherethesepdatasharethesamefirsttwocoordinates
andonlydifferinthelastcoordinate.
6GDisanexampleofsuchalgorithm.
7AsimilarresultholdsforadaptivealgorithmslikeAdamaswell,asdiscussedinCorollaryC.7.
8WereferthereadertoAppendixAforthedefinitionofnormalizedGD.
11Moreformally,wecandefinethefollowingcorrespondence: givenonedatapointinclassification
(x,y) ∈ R2p ×Rp, we can view it as p data points in regression, {((x,e ),y )}p , denoted by
i i i=1
F(x,y). Similarly,anyfunctionΨmappingfromx = {(e ,e )}toRp canbeviewedasafunction
i j
mapping {(e ,e ,e )} to R by defining Ψ′(x,e ) = [Ψ(x)] . Moreover, these two functions Ψ
i j k k k
and Ψ′ share the same population ℓ loss. Under this view, matrix-valued kernel learning with n
2
classificationdatapointsisexactlythesameasscalar-valuedkernellearningwithnpregressiondata
points.
TheonlyobstacletodirectlyapplyingTheorem3.4isthatthedatadistributionisdifferent. Forthe
regressionsetting,eachdataissampledindependentlyanduniformly,andthenumberofdatapoints
canbeanyinteger;intheclassificationsetting,thedatapointsaresampledinindependentgroups
andthenumberofdatamustbeamultipleofp. However,itiseasytoseethisnewdistributionisstill
invariantunderpermutationsasinDefinition3.2. Thus,wecandirectlyapplyTheorem3.4onthis
newdistributiontogetTheorem4.3. AformalversionofthisargumentisavailableinAppendixD.3.
Therefore,fornetworksoperatinginthekernelregime,generalizationtounseendatainℓ loss
2
is impossible unless n/N = Ω(1), i.e. we have observed a constant fraction of all the N = p2
possible data points. It is worth emphasizing, however, that a large ℓ loss does not necessarily
2
imply a large classification error. It remains open to prove a classification error lower bound for
permutation-equivariantkernelmethods.
4.2 RichRegime
Toanalyzethebehaviourofthenetworkintherichregime,wefirstintroducethenotionofmargin
and elaborate on the margin-maximization implicit bias of gradient descent. For a multi-class
classificationproblemwithpclassesandafixednetworkf,themarginofadatapoint(x,y)is
q(θ;x,y) ≜ f (θ;x)−maxf (θ;x).
y y′
y′̸=y
ThemarginforadatasetD isdefinedastheminimummarginofallpointsonthedataset:
q (θ;D) ≜ min q(θ;x,y).
min
(x,y)∈D
Whenthenetworkf ishomogeneouswithrespecttoitsparameters(asisthecaseinoursetup),
onecanobservethataslongasθ linearlyseparatesthedatasetD ,i.e.,q (θ;D) > 0,itispossible
min
toarbitraryscaletheminimummargin,throughscalingtheparametersofthenetwork. Hence,in
suchhomogenousnetworks,oneisusuallyconcernedwithanormalizedmarginq (θ/∥θ∥;D)
min
accordingtosomenorm.
LyuandLi(2020)provedthatgradientdescentonhomogeneousmodelswiththecross-entropy
(orsimilar)lossesondatasetD,intheabsenceofexplicitregularization,maximizesthenormalized
margin. Specifically, although ∥θ∥ → ∞ as t → ∞, θ/∥θ∥ converges to a solution (or more
2 2
generally,aKKTpoint)ofthefollowingproblemwhenoneexists:
1
min ∥θ∥2 s.t. q (θ;D) ≥ 1. (4.1)
2 2 min
Toestablishourresultsintheclassificationsetting,weborrowthefollowingtheoremfromWei
etal.(2019),whoprovethatwhenthestrengthoftheregularizationusedin(2.2)issmallenough,the
maximumnormalizedmarginoftheregularizedlossconvergestothatoftheunregularizedloss.
Proposition 4.4 (Wei et al., 2019, Theorem 4.1). Consider a positively homogeneous function f
with respect to parameters θ and a dataset D separable with f. Let ∥·∥ be any norm. Let γ∗
train
12bethemaximumnormalizedmarginoftheunregularizedlossandγλ benormalizedmarginofthe
minimizeroftheregularizedlosswithstrengthλ. Asλ → 0,γλ → γ∗.
Thatis,ifweuseasmallenoughregularizationweightλwithanynormin(2.2),wewillobtain
approximately the same solution as the unregularized problem. This confirms that training our
two-layernetworkusinggradientdescentwithasmallenoughℓ regularizercanleadtoweights
∞
closetothesolutionofthemaxℓ -normalizedmarginproblem. Nextwepresentthemainresult
∞
ofthissubsection,aupperboundontesterror,showingthatallparameterswhoseℓ -normalized
∞
margin is close enough to the max ℓ -normalized margin solution will generalize with a sample
∞
complexityofO˜(p5/3)
Theorem 4.5 (Upper Bound). Let δ ∈ (0,1) be positive constants such that 8p ≤ h = O(p)
independent of p and n. For any n representing the size of the training set D it holds with
train
probabilityatleast1−δoverrandomnessofD ,forallinterpolatingsolutionsθwithnormalized
train
ℓ -marginΩ(p),itholdsthat
∞
(cid:32)(cid:114) (cid:33)
p5/3
L (f,θ,D) ≤ O˜ . (4.2)
0
n
In other words, any interpolating solution θ with approximately maximal normalized ℓ -margin,
∞
i.e.,q (θ/∥θ∥ ;D ) ≥ Ω(max q (θ′;D )),hasasamplecomplexityofO˜(p5/3),
min ∞ train ∥θ′∥∞≤1 min train
sinceProposition3.8showsthatthemaximalnormalizedℓ -marginisatleastΩ(p).
∞
Our proof of Theorem 4.5 is based on the PAC-Bayesian framework (McAllester, 2003),
specificallyusingLemma1ofNeyshaburetal.(2018). Thisresultprovidesamargin-basedhigh
probabilitygeneralizationboundforanypredictorbasedonthemarginloss,whichcountsaprediction
ascorrectonlyifitpredictswithamarginatleastγ:
(cid:104) (cid:105)
L (f,θ,D) ≜ Pr q(θ,x,y) > γ . (4.3)
γ
(x,y)∼D
L issimplythemisclassificationrate,or0-1error. Thefollowingstatementusesthisresulttoprove
0
anupperboundonthepopulation0-1errorofnetworksusingmarginlossonthetrainingdataset.
Theorem4.6. Foranyp ≥ 2, trainingsetsizen ≥ 1,δ ∈ (0,1), normr > 0, widthh′ > 4log 2
√ δ
andnormalizedmarginγ/r3 = Ω˜( h)itholdswithprobabilityatleast1−δ overtherandomness
ofthetrainingsetD thatforanyθ′ ofwidthh′ with∥θ′∥ ≤ r
train ∞
(cid:32)(cid:114) (cid:115) (cid:33)
p h2
L (f,θ′,D) ≤ L (f,θ′,D )+O˜ · 3 (4.4)
0 γ train n γ/r3
whereD denotesthepopulationandf,LaredefinedinEquations(2.1)and (4.3)respectively.
ProofSketchofTheorem4.6. Throughaseriesofconcentrationinequalities,weshowthataslongas
theassumptionsofTheorem4.6aremet,theoutputlogitsundergaussianperturbationonparameters,
f(θ′+θ˜′;·),whereθ˜′ isentry-wiseGaussiannoiseofmeanzeroandvarianceσ2,areclosetothatof
√
f(θ′;·)uptoanabsolutedifferenceofO˜( hσ3). TheproofiscompletedbyapplyingaPAC-bayesian
generalization bound, e.g., using Lemma 1 of Neyshabur et al. (2018) and setting σ based on the
valueofℓ -normalizedmarginγ/r3 andwidthh. ThefullproofisavailableinAppendixG.
∞
13Accuracy Loss Change of NTK
1.0 p=113 78 p=113 40000 1.0
p=233 p=233
0.8 p=313 6 p=313 0.8
30000 5
0.6 0.6
4
20000
0.4 3 0.4
0.2 2 10000 0.2
1 Train
Test
0.0 0 0 0.0
102 103 104 5 104 102 103 104 5 104 102 103 104 5 104
Step Step Step
Figure 5: Grokking in transformers happens after a delayed transition from kernel to rich
regime. Aone-layertransformeristrainedwithgradientdescentusingcross-entropylossandatiny
ℓ regularizationof10−20 strengthon2×p5/3 trainingsamplesfromthemodularadditionproblem
∞
withvariousps. ChangeofeNTKuptothepointoffittingthetrainingsetisnegligible. TheeNTK
hasadrasticchangeonlyafterfittingthewholetrainingset,implyingminimalfeaturelearninguntil
pastoverfitting. Thedashedlinesinthemiddleandleftfiguresindicatetrainsetstatistics,andthe
solidlinescorrespondtothetestset.
NowweshowhowtoderiveTheorem4.5usingTheorem4.6. BecauseTheorem4.6onlyrelies
onthenormalizedmarginandourmodelis3-homogeneous,withoutlossofgenerality,wecanfix
normboundr = 1. Thenitsufficestoset8p < h = O(p)andγ = Ω(p)inEquation(4.4),wherethe
firsttermbecomes0becauseTheorem4.5assumesθ hasnormalizedmarginΩ(p)andthesecond
(cid:113)
termbecomesO˜( p5/3 ).
n
Theorem 4.5 confirms and explains previous observations (e.g. Power et al., 2022; Gromov,
2023;Nandaetal.,2023;Liuetal.,2023)ontheminimumthresholdforthefractionofdatausedto
achievegeneralization. IncombinationwithTheorem4.2ofLyuandLi(2020),thisshowsthatwith
enoughtrainingdata,gradientdescentwilleventuallyfindageneralizingsolutionforthissettingof
themodularadditionproblem.
Empirical Verification of Our Theory: In terms of whether gradient descent finds a model
satisfying these conditions: Figure 4 empirically evaluates the ℓ norm of the learned weights
∞
throughgradientdescentontheclassificationtask,withtinyℓ regularization(weightof10−20),
∞
acrossdifferentvaluesofp. Again,itcanbeseenthattheℓ normoftheweightsofthenetworkdo
∞
notgrowwiththeproblemdimension. Theseexperimentusen = 2p5/3 datapointsforeachp,with
alearningrateof10.
Similartotheregressionsetting,Figure1presentsempiricalevidenceontheimpactofkernel
regimeonthepoorgeneralizationcapabilitiesintheearlyphaseoftrainingbyshowingtheminimal
changeofempiricalNTKuntilafteroverfittingintheclassificationsetting.
Throughchangingthescaleofinitialization,inFigure1wedemonstratethatbydecreasingthe
scaleofinitializationintheclassificationtaskonecanmitigategrokkingsuchthatthegapbetween
overfittingandgeneralizationbecomeslargerorsmaller. Thisfurthersupportsthataslongasthe
networkliesinthekernelregime,generalizationwithouthavingaccesstotheaconstantfractionof
thedatasetisimpossible.
GrokkingModularAdditioninTransformers: Figure5suggeststhat,similarlytothetwo-layer
network,grokkingintheoriginalTransformerstudiedbyPoweretal.(2022)mightbeexplainable
throughthesamemechanism. Infact,Theorem3.3,albeitwithsmallmodificationstoincorporate
thesharedembeddingsinthetransformerarchitecture,appliestotransformersaswell.
14
F||0
t
||
ycaruccA5 Additional Related Works
LimitationsimposedbyEquivarianceofLearningAlgorithms. AbbeandBoix-Adsera(2022)
studytheimpactofequivarianceofthetrainingalgorithmsontheefficiencyoflearningdifferent
functionsondifferentnetworks. Inparticular,theyconsidertwomainsetups: a)learningwithFCNs
usingnoisyGDwithclippedgradientsthroughoutthetraining,andb)learningaspecificinstanceof
themodularadditiontask(p = 2withnoisyinputs)withFCNsusingSGD.Althoughtheirapproach
instudyinglowerboundsforefficientlearningsharessomehighlevelsimilaritywithoursinusing
equivariance of the training algorithm, the settings considered are significantly differs from ours.
Moreover,inAppendixD.4wepresentanovelabstractframeworkforanalyzinglowerboundson
populationℓ lossforgeneralfunctionclasses. MalachandShalev-Shwartz(2022)presentanother
2
techniqueinanalysisofpopulationlosslowerbounds,whichsharessomehigh-levelsimilaritywith
ourframework,albeittheiranalysisismorerestrictiveonfunctionclasses. Ng(2004)alsodiscusses
rotationalequivarianceofmanylearningalgorithmsandpresentsagenerallowerboundonthe0-1
populationerrorofsuchalgorithmsinthegeneralcase.
Margin Maximization as the Late Phase Implicit Bias. Morwani et al. (2024) present an
analyticalsolutionforthemax-marginsolutionoflearningmodularadditioninaclassificationsetting
similar to Section 4 when using all of the dataset in training the network. Similar to our analysis
of the rich regime in this setting, they face difficulties in proving results under the assumption of
boundedℓ normforweightsofthenetwork,andassumeanℓ boundinstead.
2 2,3
6 Conclusion
Inthiswork,westudiedthephenomenonofgrokkinginlearningmodularadditionwithgradient
descent ontwo-layer networks, modeled as regression or classification. Weshowedthat learning
modularadditionaspresentedisfundamentallyadifficulttaskforkernelmodels(forexampleneural
networksinkernelregime)duetotheinherentsymmetryandpermutation-equivarianceofthetask.
Wetheoreticallyestablishedthisdifficultybypresentingsamplecomplexitylowerboundsoforderof
constantfractionofthewholedataset. Wefurthershowedthatnetworkssatisfyingcertainconditions
generalizefarbetterthanthoseinthekernelregime,thatsuchnetworksexist,andshowedempirical
evidencethatsimpleregularizedgradientdescentcaneventuallyfindthem,onceitescapesthekernel
regime. Theseresults,incombination,attempttoaddresswhygrokkingisobservedwhenlearning
modular addition. We provide strong evidence to the hypothesis (Lyu et al., 2024; Kumar et al.,
2024)that,onthisimportantproblem,itisindeedduetoaseparationbetweenkernelandnon-kernel
behaviorofgradientdescent.
FutureWork. Wehaveguaranteedlargeℓ populationlossfornetworksinkernelregimeon
2
bothregressionandclassificationsettings. Itispossible,however,fornetworkstohavearbitrarily
highℓ losswhilehavingperfectclassificationaccuracy. Weconjecturethatimpossibilityresults
2
foraccuracy-basedgeneralizationmayalsobepossiblebasedonpermutationequivarianceinthe
earlyphaseoftraining,butleaveitasfuturework. Moreover,weonlystudythecauseofgrokkingin
thesesettings,butdonotanalyzepossibletrainingtechniquestoenablequickgeneralizationonthis
task. Althoughweareabletoeliminategrokkingthroughchangingthescaleofinitialization,doing
soactuallyslowsdownthetimetofinalgeneralization; findingpracticalmethodstoenablequick
generalizationwouldbemoreuseful.
15Acknowledgments
WewouldliketothankKaifengLyuandWeiHuforhelpfuldiscussions. Thisworkwasenabled
inpartbysupportprovidedbytheNaturalSciencesandEngineeringResearchCouncilofCanada,
theCanadaCIFARAIChairsprogram,AdvancedResearchComputingattheUniversityofBritish
Columbia,CalculQuébec,theBCDRIGroup,andtheDigitalResearchAllianceofCanada.
References
EmmanuelAbbeandEnricBoix-Adsera(2022).“Onthenon-universalityofdeeplearning:quantifying
thecostofsymmetry.”NeurIPS.arXiv:2208.03113.
JasonAnsel,EdwardYang,HoraceHe,NataliaGimelshein,AnimeshJain,MichaelVoznesensky,
Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will
Constable,AlbanDesmaison,ZacharyDeVito,EliasEllison,WillFeng,JiongGong,Michael
Gschwind,BrianHirsh,SherlockHuang,KshiteejKalambarkar,LaurentKirsch,MichaelLazos,
Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, C. K. Luk, Bert Maher, Yunjie Pan,
ChristianPuhrsch,MatthiasReso,MarkSaroufim,MarcosYukioSiraichi,HelenSuk,Shunting
Zhang,MichaelSuo,PhilTillet,XuZhao,EikanWang,KerenZhou,RichardZou,Xiaodong
Wang,AjitMathews,WilliamWen,GregoryChanan,PengWu,andSoumithChintala(2024).
“PyTorch2:FasterMachineLearningThroughDynamicPythonBytecodeTransformationand
GraphCompilation.”ArchitecturalSupportforProgrammingLanguagesandOperatingSystems.
SanjeevArora,NadavCohen,WeiHu,andYupingLuo(2019a).“ImplicitRegularizationinDeep
MatrixFactorization.”NeurIPS.arXiv:1905.13655.
SanjeevArora,SimonSDu,WeiHu,ZhiyuanLi,RussRSalakhutdinov,andRuosongWang(2019b).
“Onexactcomputationwithaninfinitelywideneuralnet.”NeurIPS.arXiv:1904.11955.
BoazBarak,BenjaminEdelman,SurbhiGoel,ShamKakade,EranMalach,andCyrilZhang(2022).
“Hiddenprogressindeeplearning:SGDlearnsparitiesnearthecomputationallimit.”NeurIPS.
arXiv:2207.08799.
Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom (2023). “Simplicity Bias in
TransformersandtheirAbilitytoLearnSparseBooleanFunctions.”ACL.arXiv:2211.12316.
GuyBlanc,NehaGupta,GregoryValiant,andPaulValiant(2020).“Implicitregularizationfordeep
neuralnetworksdrivenbyanOrnstein-Uhlenbecklikeprocess.”arXiv:1904.09080.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin,GeorgeNecula,AdamPaszke,JakeVanderPlas,SkyeWanderman-Milne,andQiao
Zhang(2018).“JAX:composabletransformationsofPython+NumPyprograms.”Version0.3.13.
url:http://github.com/google/jax.
FrançoisCharton(2024).“Learningthegreatestcommondivisor:explainingtransformerpredictions.”
ICLR.arXiv:2308.15594.
Lenaic Chizat, Edouard Oyallon, and Francis Bach (2019). “On Lazy Training in Differentiable
Programming.”NeurIPS.arXiv:1812.07956.
RichardCourantandDavidHilbert(1953).MethodsofMathematicalPhysics.Vol.1.Interscience
Publishers.
AlexDamian,TengyuMa,andJasonD.Lee(2021).“LabelNoiseSGDProvablyPrefersFlatGlobal
Minimizers.”NeurIPS.arXiv:2106.06530.
StanislavFort,GintareKarolinaDziugaite,MansheejPaul,SepidehKharaghani,DanielM.Roy,and
SuryaGanguli(2020).“Deeplearningversuskernellearning:anempiricalstudyoflosslandscape
geometryandthetimeevolutionoftheNeuralTangentKernel.”NeurIPS.arXiv:2010.15110.
16Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart (Nov. 2020). “Disentangling
featureandlazytrainingindeepneuralnetworks.”JournalofStatisticalMechanics:Theoryand
Experiment2020.11,p.113301.arXiv:1906.08034.
AndreyGromov(2023).“Grokkingmodulararithmetic.”arXiv:2301.02679.
SuriyaGunasekar,JasonLee,DanielSoudry,andNathanSrebro(2018).“CharacterizingImplicit
BiasinTermsofOptimizationGeometry.”ICML.arXiv:1802.08246.
SuriyaGunasekar,BlakeWoodworth,SrinadhBhojanapalli,BehnamNeyshabur,andNathanSrebro
(2017).“ImplicitRegularizationinMatrixFactorization.”NeurIPS.arXiv:1705.09280.
JeffZ.HaoChen,ColinWei,JasonD.Lee,andTengyuMa(2020).“ShapeMatters:Understanding
theImplicitBiasoftheNoiseCovariance.”COLT.arXiv:2006.08680.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2015). “Delving deep into rectifiers:
Surpassinghuman-levelperformanceonImageNetclassification.”ICCV.arXiv:1502.01852.
ArthurJacot,FranckGabriel,andClémentHongler(2018).“Neuraltangentkernel:Convergenceand
generalizationinneuralnetworks.”NeurIPS.arXiv:1806.07572.
TanishqKumar,BlakeBordelon,SamuelJ.Gershman,andCengizPehlevan(2024).“Grokkingas
theTransitionfromLazytoRichTrainingDynamics.”ICLR.arXiv:2310.06110.
JaehoonLee,LechaoXiao,SamuelSchoenholz,YasamanBahri,RomanNovak,JaschaSohl-Dickstein,
andJeffreyPennington(2019).“Wideneuralnetworksofanydepthevolveaslinearmodelsunder
gradientdescent.”NeurIPS.arXiv:1902.06720.
NoamLevi,AlonBeck,andYohaiBar-Sinai(2024).“GrokkinginLinearEstimators–ASolvable
ModelthatGrokswithoutUnderstanding.”ICLR.arXiv:2310.16441.
Zhiyuan Li, Tianhao Wang, and Sanjeev Arora (2022). “What Happens after SGD Reaches Zero
Loss?–AMathematicalFramework.”ICLR.arXiv:2110.06914.
ZhiyuanLi,YiZhang,andSanjeevArora(2021).“Whyareconvolutionalnetsmoresample-efficient
thanfully-connectednets?”ICLR.arXiv:2010.08515.
ZimingLiu,OuailKitouni,NiklasSNolte,EricMichaud,MaxTegmark,andMikeWilliams(2022).
“Towards understanding grokking: An effective theory of representation learning.” NeurIPS.
arXiv:2205.10343.
ZimingLiu,EricJMichaud,andMaxTegmark(2023).“Omnigrok:Grokkingbeyondalgorithmic
data.”ICLR.arXiv:2210.01117.
KaifengLyu,JikaiJin,ZhiyuanLi,SimonS.Du,JasonD.Lee,andWeiHu(2024).“Dichotomyof
EarlyandLatePhaseImplicitBiasesCanProvablyInduceGrokking.”ICLR.arXiv:2311.18817.
KaifengLyuandJianLi(2020).“Gradientdescentmaximizesthemarginofhomogeneousneural
networks.”ICLR.arXiv:1906.05890.
EranMalachandShaiShalev-Shwartz(2022).“WhenHardnessofApproximationMeetsHardness
ofLearning.”JournalofMachineLearningResearch23.91,pp.1–24.arXiv:2008.08059.
DavidA.McAllester(2003).“SimplifiedPAC-BayesianMarginBounds.”COLT.
Mohamad Amin Mohamadi, Wonho Bae, and Danica J Sutherland (2023). “A fast, well-founded
approximation to the empirical neural tangent kernel.” International Conference on Machine
Learning.PMLR,pp.25061–25081.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talkwalkar (2018). Foundations of Machine
Learning.2nded.MITPress.url:https://cs.nyu.edu/~mohri/mlbook/.
EdwardMoroshko,SuriyaGunasekar,BlakeWoodworth,JasonD.Lee,NathanSrebro,andDaniel
Soudry (2020). “Implicit Bias in Deep Linear Classification: Initialization Scale vs Training
Accuracy.”NeurIPS.arXiv:2007.06738.
Depen Morwani, Benjamin L. Edelman, Costin-Andrei Oncescu, Rosie Zhao, and Sham Kakade
(2024). “Feature emergence via margin maximization: case studies in algebraic tasks.” ICLR.
arXiv:2311.07568.
17MorShpigelNacson,SuriyaGunasekar,JasonD.Lee,NathanSrebro,andDanielSoudry(2019).
“Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep
Models.”ICML.arXiv:1905.07325.
Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt (2023). “Progress
measuresforgrokkingviamechanisticinterpretability.”ICLR.arXiv:2301.05217.
BehnamNeyshabur,SrinadhBhojanapalli,andNathanSrebro(2018).“APAC-BayesianApproach
toSpectrally-NormalizedMarginBoundsforNeuralNetworks.”ICLR.arXiv:1707.09564.
AndrewYNg(2004).“Featureselection,L vs.L regularization,androtationalinvariance.”ICML.
1 2
PascalJr.TikengNotsawo,HattieZhou,MohammadPezeshki,IrinaRish,andGuillaumeDumas
(2023).“PredictingGrokkingLongBeforeitHappens:Alookintothelosslandscapeofmodels
whichgrok.”arXiv:2306.13253.
AletheaPower,YuriBurda,HarriEdwards,IgorBabuschkin,andVedantMisra(2022).“Grokking:
GeneralizationBeyondOverfittingonSmallAlgorithmicDatasets.”arXiv:2201.02177.
Martin Raab and Angelika Steger (1998). “‘Balls into Bins’ — A Simple and Tight Analysis.”
RandomizationandApproximationTechniquesinComputerScience.
NoaRubin,InbarSeroussi,andZoharRingel(2024).“GrokkingasaFirstOrderPhaseTransitionin
TwoLayerNetworks.”ICLR.arXiv:2310.03789.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro (2018).
“TheImplicitBiasofGradientDescentonSeparableData.”JMLR19.70.arXiv:1710.10345.
NathanSrebro,KarthikSridharan,andAmbujTewari(2010).“Smoothness,lownoiseandfastrates.”
arXiv:1009.3896.
MatusTelgarsky(2022).“Featureselectionwithgradientdescentontwo-layernetworksinlow-rotation
regimes.”arXiv:2208.02789.
Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, and Joshua Susskind
(2022).“Theslingshotmechanism:Anempiricalstudyofadaptiveoptimizersandthegrokking
phenomenon.”arXiv:2206.04817.
Joel A. Tropp (2015). “An Introduction to Matrix Concentration Inequalities.” Foundations and
Trends®inMachineLearning8.1-2,pp.1–230.arXiv:1501.01571.
VikrantVarma,RohinShah,ZacharyKenton,JánosKramár,andRamanaKumar(2023).“Explaining
grokkingthroughcircuitefficiency.”arXiv:2309.02390.
MartinJ.Wainwright(2019).High-DimensionalStatistics:ANon-AsymptoticViewpoint.Cambridge
UniversityPress.
ColinWei,JasonD.Lee,QiangLiu,andTengyuMa(2019).“RegularizationMatters:Generalization
andOptimizationofNeuralNetsv.s.theirInducedKernel.”NeurIPS.arXiv:1810.05369.
ShuoXieandZhiyuanLi(2024).“ImplicitBiasofAdamW:ℓ NormConstrainedOptimization.”
∞
ICML.arXiv:2404.04454.
Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu (2024). “Benign Overfitting and
GrokkinginReLUNetworksforXORClusterData.”ICLR.arXiv:2310.02541.
Greg Yang and Edward J. Hu (2021). “Tensor Programs IV: Feature Learning in Infinite-Width
NeuralNetworks.”ICML.arXiv:2011.14522.
ChenyangZhang,DifanZou,andYuanCao(2024).“TheImplicitBiasofAdamonSeparableData.”
arXiv:2406.10650.
Mauricio A Álvarez, Lorenzo Rosasco, and Neil D Lawrence (2012). “Kernels for vector-valued
functions:Areview.”FoundationsandTrends®inMachineLearning4.3,pp.195–266.arXiv:
1106.6251.
18A Experimental Setup
Inthissection,webrieflyexplainthesetupusedforourexperimentalevaluations.
A.1 Regression
Weusevanillagradientdescentwithsquaredlossandtinyℓ regularizationfor50,000,100,000
∞
or200,000stepsforeachexperiment. Inregression,ourlearningratehasbeenfixedto1,andthe
regularizationstrengthhasbeensetto10−4. ThenetworkhasbeeninitializedaccordingtoHeetal.
(2015). Theamountofdatausedfortraininginregressiontaskhasbeensetto2×p2.25.
A.2 Classification
Weusevanillagradientdescentwithcross-entropylossandtinyℓ regularization,forupto100,000
∞
steps. The learning rate in the presented experiments was set to 10 and was kept constant during
thetraining. Theregularizationstrengthofℓ regularizerhasbeensetto10−20. Toacceleratethe
∞
trainingwithcross-entropyloss,weusethe"normalized"GDtrick,wherethelearningrateofeach
stepisscaledbytheinverseofthenormofthegradient:
∇ ℓ(θ )
θ = θ −η θ t (A.1)
t+1 t
∥∇ ℓ(θ )∥
θ t 2
whereℓdenotesthelossfunctionandη denotesthelearningrate. Thenetworkhasbeeninitialized
accordingtoHeetal.(2015). Theamountofdatausedfortraininginregressiontaskhasbeensetto
2×p5/3.
A.3 Transformer
To train the one-layer transformer we have used full-batch gradient descent with a learning rate
of η = 0.25 and a tiny ℓ regularization with the strength of 1.0e−20. The network has been
∞
initializedaccordingtodefaultPyTorchinitialziation(migratedotJAX).Wehaveused2×p5/3 of
thedatafortraining.
A.4 Logistics
We used the JAX framework (Bradbury et al., 2018) to implement and run the experiments on
machinesusingNVIDIAV100orA100GPUs.
B Ability of Neural Tangent Kernel Models to Interpolate
Inthissection,weproveTheorem3.1,thatneuraltangentkernelmodelsforourone-hidden-layer
quadraticnetworkareabletoexactlyinterpolatetheirtrainingdata,intheregressionsetting.
Ifthetrainingsetcontainsduplicatex,sayx = x fori ̸= j,thenKcannotbefullrank–the
i j
ithandjthrowsarenecessarilyidentical. Ofcourse,therealsoexistunattainabletargets; justset
y ̸= y . Callalabelingconsistentifforalliandj suchthatx = x ,y = y . Alearningalgorithm
i j i j i j
canachieveanyconsistentlabelingifandonlyifthatalgorithmappliedtothelargestdistinctsubset
(e.g.removingj butkeepingi)canachieveanylabeling. Thus,weassumewithoutlossofgenerality
intheremainderofthissectionthatthetrainingsetcontainsnoduplicates.
19B.1 Full-RankKernelsareExpressive
Recallingthatneuraltangentkernelmodelscorrespondto“ridgeless”kernelregression(e.g.Jacot
etal.,2018;Leeetal.,2019),wefirstnoticethatthismethodcaninterpolateanysetoftraininglabels
whenthekernelisstrictlypositivedefinite.
Specifically,empiricalneuraltangentkernelregressioncorrespondstoridgelessregressionwith
apriormeancorrespondingtothenetworkatinitializationf ,
0
n
(cid:88)
argmin ∥f −f ∥ = f + argmin ∥f∥ = f + K(·,x )λ forλ = K†y′,
0 K 0 K 0 i i
f:∀i,f(xi)=yi f:∀i,f(xi)=yi−f0
i=1
whereK† istheMoore-Penrosepseudoinverseofthen×nkernelmatrix[K(x ,x )] ,andy′ has
i j ij
ithentryy −f (x ). Bychangingvariablesbetweeny′ andy,weneednotexplicitlyconsiderf in
i 0 i 0
thefollowingtworesults.
Lemma B.1. Let D = {(x ,y )} , and let K be a kernel such that the kernel matrix
train i i i∈[n]
K = [K(x ,x )] isstrictlypositivedefinite. Thenkernelridgelessregressionachieveszerotraining
i j ij
erroronD .
train
Proof. Thiswell-knownresultfollowsfromthefactthat[fˆ(x )] = KK†y. IfKisstrictlypositive
i i
definite,K† = K−1,andso[fˆ(x )] = y.
i i
Thefollowingresultisapartialconverse.
Lemma B.2. Let {x } and K be a kernel such that the kernel matrix K = [K(x ,x )] is
i i∈[n] i j ij
singular. Thenthereexistsanassignmentofy ∈ Rsuchthatkernelridgelessregressionachieves
i
nonzerotrainingerroronD = {(x ,y )} .
train i i i∈[n]
Proof. Lety beanynonzerovectorinthenullspaceofK; suchvectorsexistsinceKissingular.
ThenyisalsointhenullspaceofK†,andthetrainingsetpredictionsareKK†y = 0 ̸= y.
B.2 ExpectedNTKisFull-Rank
Wenextshowthat,intheregressionsetting,theexpected neuraltangentkernelisstrictlypositive
definite. Whileperhapsofinterestofitsownaccord,thiswillbeakeycomponentinouranalysisof
finite-widthnetworksthatfollows.
PropositionB.3. LettheentriesofW followadistributionP ,allmutuallyindependent. AssumeP
W W
hasmeanzero,varianceσ2 > 0,skewnesszero,andkurtosisκ suchthatE w4 = κ σ4 .
W W w∼PW W W
TheentriesofV canbearbitrary. Thentheexpectedneuraltangentkernelfortheregressionnetwork
with any finite h ≥ 1 is strictly positive definite on any set of distinct inputs {x : i ∈ [n]}, with
i
minimumeigenvalueatleast4hσ4 .
W
We first note that if P = N(0,σ2 ), κ = 3. If P = Unif([−s ,s ]), σ2 = 1s2 and
W W V W W W W 3 W
κ = 9. Thesetwodistributionscoverthevastmajorityofinitializationschemesusedinpractice.
W 5
Proof. It will be more convenient in this proof to give different names to the sub-matrix of the
parametervectorW thatactontheainputsandthebinputs; wewillwriteW = (cid:2) Q R(cid:3), where
Q,Rareeachh×pmatrices. Thenwecanwritethefullmodelas
h
(cid:88)
g(θ;(e ,e ,e )) = eTV(Qe +Re )⊙2 = V (Q +R )2.
a b c c a b ck ka kb
k=1
20The empirical neural tangent kernel between two inputs x = (e ,e ,e ) and x′ = (e ,e ,e ) is
a b c a′ b′ c′
thengivenby
(cid:88)h (cid:34) (cid:88)p ∂g(θ;x)∂g(θ;x′)
K (x,x′) =
θ
∂V ∂V
c′′k c′′k
k=1 c′′=1 (B.1)
(cid:88)p ∂g(θ;x)∂g(θ;x′) (cid:88)p ∂g(θ;x)∂g(θ;x′)(cid:35)
+ + .
∂Q ∂Q ∂R ∂R
ka′′ ka′′ kb′′ kb′′
a′′=1 b′′=1
EvaluatingtheV derivatives,
(cid:40)
∂g(θ;x) (Q +R )2 ifc = c′′
ka kb
=
∂V c′′k 0 otherwise
(cid:88)p ∂g(θ;x)∂g(θ;x′) (cid:40) (Q ka+R kb)2(Q
ka′
+R kb′)2 ifc = c′
=
∂V c′′k ∂V c′′k 0 otherwise.
c′′=1
WhileitwouldnotbedifficulttoanalyzetheQandRderivativesaswell,theirexactformwill
notbeimportant. Instead,weonlyneedtowrite
h
(cid:88)
K (x,x′) = J (x,x′)+ L (x,x′) (B.2)
θ θ θ
k
k=1
(cid:88)h (cid:88)p ∂g(θ;x)∂g(θ;x′) (cid:88)p ∂g(θ;x)∂g(θ;x′)
J (x,x′) = +
θ
∂Q ∂Q ∂R ∂R
ka′′ ka′′ kb′′ kb′′
k=1a′′=1 b′′=1
L (x,x′) = (Q +R )2(Q +R )21(c = c′). (B.3)
θ
k
ka kb ka′ kb′
ThefunctionJ hasanexplicitfeaturemapcorrespondingtotherelevantgradients; thus, forany
θ
set of inputs {x : i ∈ [n]} and any value of θ, the kernel matrix J = [J (x ,x )] is positive
i θ θ i j ij
semi-definite. Thisimplies,e.g.viaWeyl’sinequality,thatE J isalsopositivesemi-definite.
θ θ
For any fixed set of inputs {x : i ∈ [n]}, the kernel matrices L = [L (x ,x )] are
i θ θ i j ij
k k
independentandidenticallydistributed,andinparticularhavethesamemeanEL foranyarbitrary
θ
k
choiceofk ∈ [p]. Thustheexpectedneuraltangentkernelmatrixcanbewrittenas
[EK (x ,x )] = EJ +hEL .
θ i j ij θ θ
k
WewillshowthatEL hasminimumeigenvalueatleast4σ4 ,fromwhichtheresultfollows.
θ k W
Todoso,wewillevaluate
(cid:104) (cid:105)
EL (x,x′) = E (Q +R )2(Q +R )2 1(c = c′)
θ
k
ka kb ka′ kb′
forarbitrarynonrandomx = (e ,e ,e )andx′ = (e ,e ,e ),whereallexpectationswillbeover
a b c a′ b′ c′
therelevantparameters{Q : a ∈ [p]}∪{R : b ∈ [p]}.
ka kb
Ifa = a′ andb = b′,
E(cid:2) (Q +R )4(cid:3) = EQ4 +4EQ3 ER +6EQ2 ER2 +4EQ ER3 +ER4
ka kb ka ka kb ka kb ka kb kb
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
κWσ W4 0 0 σ W2 σ W2 0 0 κWσ W4
= (2κ +6)σ4 .
W W
21Ifinsteada = a′ butb ̸= b′,then
E(cid:2) (Q +R )2(Q +R )2(cid:3) = E(cid:2) (Q2 +2Q R +R2 )(Q2 +2Q R +R2 )(cid:3)
ka kb ka kb′ ka ka kb kb ka ka kb′ kb′
= EQ4 +2EQ3 ER +EQ2 ER2
ka ka kb′ ka kb′
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
κWσ W4 0 0 σ W2 σ W2
+2EQ3 ER +4EQ2 ER ER +2EQ ER ER2
ka kb ka kb kb′ ka kb kb′
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
0 0 σ2 0 0 0 0 σ2
W W
+ER2 EQ2 +2ER2 EQ ER +ER2 ER2
kb ka kb ka kb′ kb kb′
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
σ2 σ2 σ2 0 0 σ2 σ2
W W W W W
= (κ +3)σ4 ;
W W
thecasewherea ̸= a′ butb = b′ isthesamebysymmetry.
Finally,whena ̸= a′ andb ̸= b′,wehavebyindependencethat
E(cid:2) (Q +R )2(Q +R )2(cid:3) = E(cid:2) (Q +R )2(cid:3)E(cid:2) (Q +R )2(cid:3) =
(cid:16)
E(cid:2) (Q +R
)2(cid:3)(cid:17)2
ka kb ka′ kb′ ka kb ka′ kb′ ka kb
(cid:16) (cid:17)2
= EQ2 +2EQ ER +ER2 = (2σ2 )2 = 4σ4 .
ka ka kb kb W W
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
σ2 0 0 σ2
W W
Combiningthecases,itholdsingeneralthat

4 ifa ̸= a′,b ̸= b′


 κ +3 ifa = a′,b ̸= b′
EL (x,x′) = σ4 1(c = c′) W
θ k W  κ W +3 ifa ̸= a′,b = b′

 2κ +6 ifa = a′,b = b′
W
= 4σ4 1(c = c′)
W
+(κ −1)σ4 1(a = a′,c = c′)
W W
+(κ −1)σ4 1(b = b′,c = c′)
W W
+4σ4 1(a = a′,b = b′,c = c′).
W
Thekurtosisofanyprobabilitydistributionisatleast1byJensen’sinequality,soallthecoefficients
inthislastformarenonnegative. Wecanthenusethistoconstructanexplicitfeaturemapforeach
terminEL ,becauseifγ ≥ 0,
θ
k
√ γ1(c = 1)T√ γ1(c′ = 1)
√ √
 γ1(c = 2)  γ1(c′ = 2)
γ1(c = c′) =  .   .  (B.4)
 . .   . . 
   
√ √
γ1(c = p) γ1(c′ = p)
correspondstoexplicitfeaturesinRp. Theotherindicatorfunctionscanbeimplementedinthesame
way,inRp2 orRp3;theirsumcanthenbeobtainedbyconcatenatingtheindividualfeaturestogether.
Thisconstructionmakesitclearthatthefunction
M(x,x′) = 4σ4 1(c = c′)+(κ −1)σ4 1(a = a′,c = c′)+(κ −1)σ4 1(b = b′,c = c′)
W W W W W
22isapositivesemi-definitekernel,soM = [M(x ,x )] isapositivesemi-definitematrixforanyset
i j ij
ofinputs{x }.
i
Itremainstoshowthatthefinalcomponentofthekernelisstrictlypositivedefinite. Noticingthat
1(a = a′,b = b′,c = c′) = 1(x = x′),ifthe{x : i ∈ [n]}aredistinct,theexpectedkernelmatrix
i
is EL = M+4σ4 I. Since M is positive semi-definite, this has minimum eigenvalue at least
θ k W
4σ4 > 0,asdesired.
W
B.3 EmpiricalNTKsareLikelyFull-Rank
Now,forbounded initializationschemes,weusematrixconcentrationinequalitiestoshowthatthe
empiricalneuraltangentkernelisalsolikelytobefull-rankwhenhislargeenough.
PropositionB.4. InthesettingofPropositionB.3,furtherassumethatPr (|w| ≤ s ) = 1.
w∼PW W
Letthesetofinputs{x : i ∈ [n]}bedistinctandsuchthatthemostcommoncvalueisseenρ times.
i c
Thentheempiricalneuraltangentkernelfortheregressionnetworkisstrictlypositivedefinitewith
probabilityatleast1−δ overthechoiceofrandomparametersθ aslongas
4s4 n
h > Wρ log .
σ4 c δ
W
IfP isuniformon[−s ,s ]fors > 0,thisconditionisequivalentto
W W W W
n
h > 36ρ log .
c
δ
Proof. RecallthedecompositionofK (x,x′) = J (x,x′)+(cid:80)h L (x,x′)from(B.2),andthe
θ θ k=1 θ k
correspondingn×nmatricesK = [K (x ,x )] ,J = [J (x ,x )] ,andL = [L (x ,x )] ,
θ θ i j ij θ θ i j ij θ θ i j ij
k k
sothatK = J +(cid:80)h L . AsshownintheproofofPropositionB.3,J ispositivesemi-definite;
θ θ k=1 θ k θ
wenowwishtoshowthatthesumofhiidmatricesL ,whosemeanhEL hasminimumeigenvalue
θ θ
k k
µ := 4hσ4 ,islikelytobefull-rank. WecandosobyapplyingLemmaB.5,adirectcorollaryof
min W
matrixChernoffbounds,ifweadditionallyhaveanalmostsureupperboundontheoperatornorm
∥L ∥.
θ
k
Notatetheinputx as(e ,e ,e )fora ,b ,c ∈ [p]. Using(B.3),wecanwrite
i ai bi ci i i i
L = diag(w )Cdiag(w ), (B.5)
θ θ θ
k k k
wherediag : Rn → Rn×n constructsadiagonalmatrixwiththegivenvectoronitsdiagonaland
(C) = 1(c = c )
ij i j
(w ) = (Q +R )2.
θ
k
i kai kbi
Using∥AB∥ ≤ ∥A∥∥B∥and∥diag(v)∥ = max |v |,weobtainthat
i i
∥L ∥ ≤ (2s )2∥C∥(2s )2 = 16s4 ∥C∥.
θ k W W W
Using(B.4),wecanwriteC = Φ ΦT sothat∥C∥ = ∥Φ ∥2,whereΦ ∈ Rn×p isgivenby
c c c c
 
1(c = 1) ··· 1(c = p)
1 1
Φ =  . . . ... . . . .
c  
1(c = 1) ··· 1(c = p)
n n
23Wecanevaluatethisoperatornormwith
p
(cid:88)
(Φ x) = 1(c = ℓ)x = x
c i i ℓ ci
ℓ=1
n p (cid:32) n (cid:33)
(cid:88) (cid:88) (cid:88)
∥Φ x∥2 = x2 = x2 1(c = ℓ)
c ci ℓ i
i=1 ℓ=1 i=1
p (cid:32) n (cid:33) n
(cid:88) (cid:88) (cid:88)
∥Φ ∥2 = sup x2 1(c = ℓ) = max 1(c = ℓ) = ρ .
c ℓ i i c
∥x∥=1 ℓ∈[p]
ℓ=1 i=1 i=1
Thusitholdsalmostsurelythat
∥L ∥ ≤ 16s4 ρ =: L.
θ k W c
PlugginginLemmaB.5,wehaveshownthat
(cid:18) 4hσ4 (cid:19)
Pr(λ (K ) > 0) ≥ 1−nexp − W ,
min θ 16s4 ρ
W c
andsoK isfullrankwithprobabilityatleast1−δ aslongas
θ
4s4 n
h > Wρ log .
σ4 c δ
W
IftheentriesofW areiidUnif([−s ,s ])fors > 0,σ2 = 1s2 ,givingthecondition
w w w W 3 W
n
h > 36ρ log .
c
δ
LemmaB.5. Considerafinitesequenceofindependentpositivesemi-definited×drealrandom
matricesX ,...,X . Assumethat∥X ∥ ≤ Lalmostsurely,andthatµ = λ ((cid:80)m EX ) >
1 m k min min k=1 k
0. Then
(cid:32) (cid:32) m (cid:33) (cid:33)
(cid:88) (cid:16) µ min(cid:17)
Pr λ X > 0 ≥ 1−dexp − .
min k
L
k=1
Proof. AmatrixChernoffbound(Tropp,2015,Theorem5.1.1)ingivesthatforeachϵ ∈ [0,1),
(cid:32) (cid:32) (cid:88)h (cid:33) (cid:33) (cid:18) e−ϵ (cid:19)µmin/L
Pr λ X > (1−ϵ)µ ≥ 1−d . (B.6)
min k min (1−ϵ)1−ϵ
k=1
If the smallest eigenvalue is strictly positive, it must exceed some δ > 0, which corresponds to
some ϵ < 1. Thus the event of being full-rank is the limit of the events of exceeding each ϵ, and
since the right-hand side of (B.6) is continuous for ϵ < 1, we can take the limit as ϵ ↗ 1; since
e−ϵ/(1−ϵ)1−ϵ → 1/e,thisgivesthedesiredresult.
Thisthresholdisindeedtightuptologarithmicfactors,inthesensethattherearesomelabels
whichkernelridgelessregressioncannotachievewhenh = o(n/p).
Proposition B.6. In the setting of Proposition B.3, the empirical neural tangent kernel of the
regressionnetworkK isguaranteedtobesingularwhenh < n/(3p).
θ
24Proof. Expandingon(B.1),wewilladditionallyneedtoevaluatetheQandRderivatives:
(cid:40)
∂g(θ;x) 2V (Q +R ) ifa = a′′
ck ka kb
=
∂Q ka′′ 0 otherwise
(cid:88)p ∂g(θ;x)∂g(x′) (cid:40) 4V ckV c′k(Q ka+R kb)(Q ka+R kb′) ifa = a′
=
∂Q ka′′ ∂Q ka′′ 0 otherwise
a′′=1
(cid:40)
∂g(θ;x) 2V (Q +R ) ifb = b′′
ck ka kb
=
∂R kb′′ 0 otherwise
(cid:88)p ∂g(x)∂g(x′) (cid:40) 4V ckV c′k(Q ka+R kb)(Q
ka′
+R kb) ifb = b′
=
∂R kb′′ ∂R kb′′ 0 otherwise.
b′′=1
Writingasin(B.2)and(B.5),wehavethat
h
(cid:88)
K = diag(w )Cdiag(w )+diag(v )Adiag(v )+diag(v )Bdiag(v ),
θ θ θ θ θ θ θ
k k k k k k
k=1
where(v ) = 2V (Q +R ),A = 1(a = a ),andB = 1(b = b ). By(B.4),eachofthe
θ i ck ka kb ij i j ij i j
k
matricesA,B,andChaverankatmostp;thusrank(K ) ≤ 3ph. If3ph < n,thisn×nmatrix
θ
cannotbefull-rank.
C Permutation-Equivariance of Gradient-Based Training
Wefirstdefinethenotionofpermutationequivariance. Towardsthis,weborrowthefollowingproof
fromAppendixCofLietal.(2021):
DefinitionC.1(Gradient-BasedAlgorithmA). WeborrowthedefinitionofAlgorithm1inLietal.
(2021)withtheslightmodificationofrestrictingtheupdateruleF(U,M,D )whereU denotes
train
theparametersandM : U → (X → R)denotesthemodelmappingparameterstoafunction. We
restricttheupdateruletoonlyallowgradient-basedupdaterules,suchthat
F(U,M,D ) = U −η∇ L(M(U),D )
train U train
forsomeη ∈ RandlossfunctionLmappingafunctionandadatasettoaloss(X → R)×D → R.
Theorem C.2. Suppose G is a group acting on X. The gradient-based iterative algorithm A
X
(definedinDefinitionC.1)isG -equivariantif:
X
1. ThereexistsagroupG actingonparametersU andagroupisomorphismτ : G → G such
θ X θ
thatforallx ∈ X,T ∈ G ,U wehavethatg(U;x) = g(τ(T)(U);T(x)).
X
2. The gradient update rule is invariant under joint group action (T,τ(T)) for all T ∈ G :
X
τ(T)(∇ g(U;x)) = ∇ g(τ(T)(U);T(x)).
U U
3. TheinitializationdistributionP isinvariantunderG .
init X
Wenowpresentourpermutation-equivarianceresults.
25DefinitionC.3. WedefineG asagroupofactionstobeappliedonU = (W,V)as
θ
G ≜ {π | σ ,σ ,σ ∈ S }
θ σ1,σ2,σ3 1 2 3 p
where π is defined as π (W,V) ≜ (W′,V′), where ∀i ∈ [h],j ∈ [p],W′ =
σ1,σ2,σ3 σ1,σ2,σ3 ij
W ,W′ = W ,V′ = V . This means the concatenation of the row is per-
mui, tσ e1 d(j i) nthei( sj+ amp) ewayai s,σ t2 h(j e) datσ a3( dj o), ei
s.
Furj t, hi
ermore,therowsofV alsogetpermutedaccordingto
σ .
3
RemarkC.4. Thereisaone-to-onemappingτ betweenG andG ,whichisτ(σ ,σ ,σ ) = π .
θ X 1 2 3 σ1,σ2,σ3
LemmaC.5. Foreveryx = (i,j,k)wherei,j,k ∈ [p],T ∈ G wehave
X
g(U;x) = g(τ(T)(U);T(x)).
Proof. Foreachx = (i,j,k) ∈ X;σ ,σ ,σ ∈ S ;U = (W,V)wehavethat
1 2 3 p
 W′⊤(σ (i),σ (j))⊙2
1 1 2
(cid:42) (cid:43)
g(τ(σ ,σ ,σ )(U);(σ ,σ ,σ )(x)) = e ,V′
  W 2′⊤(σ 1( .i),σ 2(j)) 

1 2 3 1 2 3 σ3(k) σ3 . . 
 
W′⊤(σ (i),σ (j))
h 1 2

W′ +W′
⊙2
(cid:42)
W1 ′,σ1(i) +W1 ′,σ2(j)+p
 (cid:43)
= e k,V
 

2,σ1(i)
. .
2,σ2(j)+p 

 . 
 
W′ +W′
h,σ1(i) h,σ2(j)+p
 ⊙2
W +W
1,i 1,j+p
(cid:42) (cid:43)
W 2,i+W 2,j+p
= e ,V  . 
k  . . 
 
W +W
h,i h,j+p
=
(cid:10)
e
,V(Wx)⊙2(cid:11)
k
= g(U;x).
LemmaC.6. Foreveryx = (i,j,k)wherei,j,k ∈ [p],T ∈ G wehave
X
τ(T)(∇ g(U;x)) = ∇ g(τ(T)(U);T(x)).
U U
Proof. Wefirstconsiderthegradientofthesecondlayer. Foralla,b ∈ [p]andσ ,σ ,σ ∈ S :
1 2 3 p
τ−1(π )(∇ g(τ(σ ,σ ,σ )(U);(σ ,σ ,σ )(x)))
σ1,σ2,σ3 V
b
1 2 3 1 2 3
= I(σ (k) = σ (b))∇ g(τ(σ ,σ ,σ )(U);(σ ,σ ,σ )(x))
3 3 V 1 2 3 1 2 3
b
= I(k = b)(W′(σ ,σ )(x))⊙2
1 2
= I(k = b)(Wx)⊙2
= ∇ g(U;x).
V
b
Forthegradientsofthefirstlayerwehave:
26τ−1(π )(∇ g(τ(σ ,σ ,σ )(U);(σ ,σ ,σ )(x)))
σ1,σ2,σ3 W 1 2 3 1 2 3
(cid:16) (cid:17)
= τ−1(π ) 2V ⊙(W′ (e ,e )⊤) (e ,e )
σ1,σ2,σ3 k (σ1,σ2) σ1(i) σ2(j) σ1(i) σ2(j)
(cid:16) (cid:17)
= τ−1(σ ,σ ,σ ) 2V ⊙(W(e ,e )) (e ,e )⊤
1 2 3 k i j σ1(i) σ2(j)
= 2V ⊙(Wx) (σ ,σ
)−1(cid:0)
(e ,e
)(cid:1)
k 1 2 σ1(i) σ2(j)
= 2V ⊙(W(e ,e )⊤) (e ,e )
k i j i j
= ∇ g(U;x).
W
Theorem3.3(PermuationEquivarianceinRegression). Trainingatwo-layerneuralnetworkwith
quadraticactivations(suchasg(θ;x))initializedwithrandomi.i.d. parametersusinggradient-based
updaterules(suchasgradientdescent)onthemodularadditionproblemintheregressionsetting
isanequivariantlearningalgorithm(asdefinedinDefinition2.2)withrespecttothepermutation
groupofDefinition3.2appliedontheinputdata.
Proof. We show that training a neural network with gradient descent in our setting satisfies the
three conditions proposed in Theorem C.2 and thus, this theorem applies to the training process.
Equivarianceoftheforwardpassandthebackwardpass(updaterule)aresettledthroughLemmasC.5
andC.6. Finally,itisstraightforwardtoseethattheinitializationisinvariantunderG definedin
θ
DefinitionC.3. Sincethedistributionofinitializationissymmetricandeachparameterisinitialized
independentlyandidenticallyfromthesamedistribution,theactionofswappingrowsorcolumns
doesn’tchangethedistribution.
Proposition4.2(PermuationEquivarianceinClassification). WedefinethepermutationgroupG
X,Y
onX ×Y (definedinSection2)asthegroup
(cid:8) (e ,e ),e (cid:55)→ (e ,e ),e : σ ,σ ,σ ∈ S (cid:9) .
a b c σ1(a) σ2(b) σ3(c) 1 2 3 p
Training a two-layer neural network with quadratic activations (such as f(θ;x)) initialized with
randomi.i.d. parametersusinggradient-basedupdaterules(asdefinedinDefinitionC.1)8 onthe
modularadditionproblemintheclassificationsettingisanequivariantlearningalgorithm(asdefined
inDefinition2.2)withrespecttoG .9
X,Y
ProofSketch. Consider the permutation group G defined in Definition C.3. Note that there is a
θ
one-to-onemappingκmappingG definedinDefinition3.2toG andanotherone-to-onemapping
X,Y X
τ mappingG toG whereτ(σ ,σ ,σ ) = π . Sincef andg sharetheparameters,it’sclear
X θ 1 2 3 σ1,σ2,σ3
thatthedistributionofinitializationforf isequivariantunderG . Toseetheequivarianceofforward
θ
andbackwardpassesonf(U;·)itsufficestoseethatLemmasC.5andC.6holdfor(G ,G )and
X,Y θ
functionf undertheisomorphismτ ◦κsinceforanyparametersU andinputsi,j,k ∈ [p]itholds
thatf (U;(i,j)) = g(U;(i,j,k)).
k
Corollary C.7 (Equivariance of Adam). Theorem 3.3 (and similarly Proposition 4.2) applies to
othergradient-basedtrainingalgorithmsthatneedmemory,suchasAdam.
8GDisanexampleofsuchalgorithm.
9AsimilarresultholdsforadaptivealgorithmslikeAdamaswell,asdiscussedinCorollaryC.7.
27ProofSketchforCorollaryC.7. Toprovethatothergradient-basedalgorithms,particularlyAdam,
arealsoequivarianttothepermutationgroupG wejustneedtoensurethattheupdateruleofthese
X
algorithmsisequivariantunderthejointgroupaction(T,τ(T)). First,notethatlinearoperations
(suchasweightdecay)ongradientsandparametersequivariant. Totrackthemomentumatdifferent
steps of the algorithm we can apply an induction on equivariance of these variables on the step
number. Att = 0they’rebothzero. m isalinearcombinationm andg whichbothareequivarint
t t−1 t
under the joint action (T,τ(T)). Since the gradient is equivariant, the coordinate-wise squared
gradientisalsoequivariantandthelinearcombinationofitwithv isalsoequivariant. Thissettles
t1
theequivarianceoftheupdateruleofAdamandothersimilargradient-basedalgorithmsthatneed
memoryandbuffers.
D Lower Bound of Population Loss for Kernel Methods
InthissectionwepresenttheformalversionofTheorem3.4alongsideaproofofit. Notethatakernel-
based predictor h on a training data {(x ,y )}n can be expressed as h(x) = (cid:80)n λ K(x ,x)
i i i=1 i=1 i i
where λ ;i∈[n] are constants. Assuming that the kernel’s feature maps are of dimension d, the
i
predictionsarelinearcombinationsofd-dimensionalfeaturemaps. Wefirstpresentageneralproof
thateverypermutationinvariantkernelrequiresΩ(p2)trainingpointstooutperformthenullpredictor
intermsofℓ loss,andthenshowthatthistheoremappliestothedistributionofempiricalNTKsat
2
initialization.
D.1 Notation
Weuse[p]todenotetheset{1,...,p}. WeuseS todenotethepermutationgroupsoverpelements,
p
S aspermutationgroupovermelementsandidistheidentitymapping. ForanynonemptysetX,a
m
symmetricfunctionK : X ×X → Riscalledapositivesemi-definitekernel(p.s.d)kernelonX if
foranyn ∈ N,anyx ,...,x andλ ,...,λ ∈ R,itholdsthat(cid:80)n (cid:80)n λ λ K(x ,x ) ≥ 0.
1 n 1 n i=1 j=1 i j i j
ForasubspaceV ofRn andvectorx ∈ Rn,wedefinedist(x,V) ≜ min ∥x−v∥ .
v∈V 2
For any pm-dimensional vector v ∈ Rpm we denote by v(x) the vector v indexed by an
m-dimensionalindexvectorx ∈ [p]m. Wedefinethevectors ∈ Rpm whoseentriesare
i,a
(cid:40)
1, x[i] = a;
s (x) ≜ (D.1)
i,a 0, otherwise.
whichhelpsusdenotetheall-onceslicesinthisvectorspace,V ≜ span{s } . Wefurther
s i,a i∈[m],a∈[p]
define∆(x,x′)wherex,x′ aretwoindexvectorsofsizemasthenumberofequalindicesbetween
them,formally:
m
(cid:88)
∆(x,x′) = 1 .
x[i]=x′[i]
i=1
WealsodefineX asthesetofallx,x′ ∈ [p]b indexvectorpairssuchthatdist(x,x′) = a,formally:
a,b
X ≜ {(x,x′) : x,x′ ∈ [p]b∧∆(x,x′) = a}.
a,b
Whenb = m,wedropthesecondindexandwriteX (insteadofX )forsimplicity. It’sclearthat
a a,m
thecollectionofallX for0 ≤ d ≤ misapartitioningofthesetofallpairsofindexvectors. Unless
d
statedotherwise,xreferstothem-dimensionalindexvector. Finally,wedefineU ≜ [Unif(S )]m
m p
astheproductofmUniformdistributiononS .
p
28D.2 LossLowerBound: RegressionSetting
For convenience of notation, we will use the (i,j,k) and e + e + e interchangebly for
i j+p k+2p
i,j,k ∈ [p]. WedenotethefunctionK(x ,·) : [p]×[p]×[p] → RasatensoronRp×p×pbyv (·)for
t t
(cid:18) (cid:19)
eacht ∈ [n]. WealsodefinefunctionΨ (i,j,k) ≜ 1 σ (i)+σ (j) ≡ σ (k)(mod p) . We
σ1,σ2,σ3 1 2 3
canviewafunctionmappingfrom[p]×[p]×[p] → Rasavectorofsizep3anddefineinnerproducts
anddistonthefunctionspace,i.e.,⟨Ψ,Ψ′⟩ ≜ (cid:80) Ψ(i,j,k)Ψ′(i,j,k)and∥h∥2 ≜ ⟨h,h⟩.
i,j,k∈[p] 2
TheoremD.1. Foranyintegersn ≥ 1,p ≥ 2andkernelK : ([p]×[p]×[p])×([p]×[p]×[p]) → R,
foranyx = (i ,j ,k ) ∈ [p]3 foreacht ∈ [n],itholdsthat
t t t t
(cid:13) (cid:13)(cid:88)n (cid:13) (cid:13)2 (cid:18)
1 n
(cid:18)
2
(cid:19)(cid:19)
min E inf (cid:13) λ K(x ,·)−Ψ (·)(cid:13) ≥ p2 1− − exp
x1,...,xnσ1,σ2,σ3∼Unif(S p)λ1,...,λn∈R(cid:13) (cid:13)
t=1
t t σ1,σ2,σ3 (cid:13) (cid:13)
2
p p3 p−1
(D.2)
Inotherwords,ifn ≤ (1−Ω(1))p3,thentheexpectedpopulationℓ lossisatleastΩ(p2),which
2
isofthesamemagnitudeasthetrivialall-zeropredictor.
ProofofTheoremD.1. ThisTheoremisadirectresultofCorollaryD.8forthecasewherem = 3.
Theorem3.4(LowerBound). ThereexistsaconstantC > 0suchthatforanyp ≥ 2,trainingdata
sizen < Cp3,andanypermutation-equivariantkernelmethodA,itholdsthat
w p
E EL (A({(x ,y )}n )) ≥ L (Ψ ) = ,
(xi,yi)n i=1∼DnA ℓ2 i i i=1 2 ℓ2 0 2
whereE takesthemeanovertherandomnessinalgorithmA.
A
ProofofTheorem3.4. BecauseAispermutation-equivariant,wehavethat
E EL (A({(x ,y )}n ))
(xi,yi)n i=1∼DnA
ℓ2 i i i=1
= E E∥A({(x ,y )}n )−p·Ψ ∥2/p3
i i i=1 id,id,id
(xi,yi)n i=1∼DnA
= E E∥A({(x ,y )}n )/p−Ψ ∥2/p
i i i=1 id,id,id
(xi,yi)n i=1∼DnA
= E E E∥A({(x ,y )}n )/p−Ψ ∥2/p.
(xi,yi)n i=1∼Dnσ1,σ2,σ3∼Unif(S p)A
i i i=1 σ1,σ2,σ3
BecauseAisakernelmethod,wehaveforany(x ,y )n andσ ,σ ,σ ∈ S
i i i=1 1 2 3 p
(cid:13)
n
(cid:13)2
(cid:13)(cid:88) (cid:13)
∥A({(x ,y )}n )/p−Ψ ∥2 ≥ inf (cid:13) λ K(x ,·)−Ψ (·)(cid:13) .
i i i=1 σ1,σ2,σ3 λ1,...,λn∈R(cid:13)
(cid:13)
t=1
t t σ1,σ2,σ3 (cid:13)
(cid:13)
2
ApplyingTheoremD.1completestheproof.
D.3 LossLowerBound: ClassificationSetting
Forthissubsection,wedefinethemodularadditionfunctionΨ : [p]×[p] → [p]as(cid:2) Ψσ3 (i,j)(cid:3) ≜
(cid:16) (cid:17)
σ1,σ2 k
1 σ (i)+σ (j) ≡ σ (k) (mod p) forallk ∈ [p],σ ,σ ,σ ∈ S .
1 2 3 1 2 3 p
29Theorem D.2. For any integers n > 1,p ≥ 2 and input-output permutation equivariant kernel
K : ([p]×[p])×([p]×[p]) → Rp×p (according to Proposition 4.2), suppose x = (i ,j ) i. ∼i.d.
t t t
Unif([p]×[p])foreacht ∈ [n]itholdsthat
(cid:88)
(cid:13) (cid:13)(cid:88)n (cid:13) (cid:13)2 (cid:18)
1 n
(cid:18)
2
(cid:19)(cid:19)
E E inf (cid:13) K(x ,x)λ −Ψσ3 (x)(cid:13) ≥ p2 1− − exp .
x1,...,xnσ1,σ2σ3∼Unif(S p)λ1,...,λn∈Rp x∈[p]×[p](cid:13) (cid:13)
t=1
t t σ1,σ2 (cid:13) (cid:13)
2
p p2 p−1
Inotherwords,ifn ≤ (1−Ω(1))p2,thentheexpectedpopulationℓ lossisatleastΩ(p2),whichis
2
ofthesamemagnitudeasthetrivialall-zeropredictor.
Proof. Notethat
(cid:13)
n
(cid:13)2
(cid:88) (cid:13)(cid:88) (cid:13)
E E inf (cid:13) K(x ,x)λ −Ψσ3 (x)(cid:13)
x1,...,xnσ1,σ2,σ3∼Unif(S p)λ∈Rn×p
x∈[p]×[p](cid:13)
(cid:13)
i=1
i i σ1,σ2 (cid:13)
(cid:13)
2
p (cid:32) n (cid:33)2
= E E inf (cid:88) (cid:88) (cid:88)(cid:68) [K(x ,x)] ,λ (cid:69) −(cid:2) Ψσ3 (x)(cid:3)
x1,...,xnσ1,σ2∼Unif(S p)λ∈Rn×p
x∈[p]×[p]j=1 i=1
i j,: i σ1,σ2 j
p (cid:32) n p (cid:33)2
= E E inf (cid:88) (cid:88) (cid:88)(cid:88) λ [K(x ,x)] −(cid:2) Ψσ3 (x)(cid:3) .
x1,...,xnσ1,σ2,σ3∼Unif(S p)λ∈Rn×p
x∈[p]×[p]j=1 i=1 k=1
i,k i j,k σ1,σ2 j
(D.3)
Wedefine
K′(cid:0) (i,j,k),(i′,j′,k′)(cid:1) ≜ K(cid:0) (i,j),(i′,j′)(cid:1) ,
k,k′
Ψ′ (cid:0) (i,j,k)(cid:1) ≜ (cid:2) Ψσ3 (cid:0) (i,j)(cid:1)(cid:3) ,
σ1,σ2,σ3 σ1,σ2 k
λ′ ≜ λ ,
t ⌊t/p⌋,tmodp
x′ ≜ (i ,j ,t mod p) (D.4)
t ⌊t/p⌋ ⌊t/p⌋
foranyi,j,k,i′,j′,k′ ∈ [p],σ ,σ ,σ ∈ S ,t ∈ [np],λ ∈ Rn×p andλ′ ∈ Rnp. Itcanbeseenthat
1 2 3 p
p (cid:32) n p (cid:33)2
E inf (cid:88) (cid:88) (cid:88)(cid:88) λ [K(x ,x)] −(cid:2) Ψσ3 (x)(cid:3)
x1,...,xn λ∈Rn×p i,k i j,k σ1,σ2 j
σ1,σ2,σ3∼Unif(S p) x∈[p]×[p]j=1 i=1 k=1
p (cid:32) np (cid:33)2
E inf (cid:88) (cid:88) (cid:88) λ′K′(cid:0) x′,(x[0],x[1],i mod p)(cid:1) −Ψ′ (x) .
x′,...,x′ λ′∈Rnp
i i σ1,σ2,σ3
σ1,σ2,σ1 3∼Un np
if(S p)
x∈[p]×[p]j=1 i=1
(D.5)
ThisissimilartoTheoremD.1exceptthattheunderlyingdataisgeneratedthroughsamplingfrom
independentgroupseachhavingasizeofp(wesamplei.i.dfromUnif([p]×[p])andforeachsample
weconsiderthesetofallpossibleresponses). ApplyingtheresultofTheoremD.1yields
p (cid:32) np (cid:33)2
E inf (cid:88) (cid:88) (cid:88) λ′K′(cid:0) x′,(x[0],x[1],i mod p)(cid:1) −Ψ′ (x)
x′,...,x′ λ′∈Rnp
i i σ1,σ2,σ3
σ1,σ2,σ1 3∼Un np
if(S p)
x∈[p]×[p]j=1 i=1
(cid:18) (cid:18) (cid:19)(cid:19)
1 n 2
≥ p2 1− − exp (D.6)
p p2 p−1
whichcompletestheproof.
30Theorem4.3(KernelLowerBound). ThereexistsaconstantC > 0suchthatforanytrainingdata
sizen < Cp2 andanykernelmethodAwhichisinput-outputpermutation-equivariant(withrespect
toG ),itholdsthat
X,Y
1
E EL (A({x ,y }n )) ≥ L (Ψ ),
(xi,yi)n i=1∼DnA ℓ2 i i i=1 2 ℓ2 0
whereE takesexpectationovertherandomnessinalgorithmA.
A
ProofofTheorem4.3. BecauseAisinput-outputpermutation-equivariant,wehavethat
E EL (cid:0) A(cid:0) {(x ,y )}n (cid:1)(cid:1)
(xi,yi)n i=1∼DnA
ℓ2 i i i=1
= E E (cid:88) (cid:13) (cid:13)A(cid:0) {(x ,y )}n (cid:1) (x)−p·Ψid (x)(cid:13) (cid:13)2 /p2
(cid:13) i i i=1 id,id (cid:13)
(xi,yi)n i=1∼DnA
x∈[p]×[p]
2
= E E (cid:88) (cid:13) (cid:13)A(cid:0) {(x ,y )}n (cid:1) (x)/p−Ψid (x)(cid:13) (cid:13)2
(cid:13) i i i=1 id,id (cid:13)
(xi,yi)n i=1∼DnA
x∈[p]×[p]
2
= (xi,yi)E
n
i=1∼Dnσ1,σ2,σ3E
∼Unif(S
p)E
A
x∈(cid:88) [p]×[p](cid:13) (cid:13)A(cid:0) {(x i,y i)}n i=1(cid:1) (x)/p−Ψσ σ3 1,σ2(x)(cid:13) (cid:13)2 2.
BecauseAisakernelmethod,wehaveforany(x ,y )n andσ ,σ ,σ ∈ S
i i i=1 1 2 3 p
(cid:13)
n
(cid:13)2
x∈(cid:88) [p]×[p](cid:13) (cid:13)A(cid:0) {(x i,y i)}n i=1(cid:1) (x)/p−Ψσ σ3 1,σ2(x)(cid:13) (cid:13)2
2
≥ λ1,..i .n ,λf
n∈R
x∈(cid:88) [p]×[p](cid:13) (cid:13)
(cid:13)
(cid:13)(cid:88) t=1λ tK(x t,x)−Ψσ σ3 1,σ2(x)(cid:13) (cid:13)
(cid:13)
(cid:13)
2.
ApplyingTheoremD.2completestheproof.
D.4 LossLowerBound: GeneralTheoremForArbitraryFunctions
LemmaD.3. ForanysubspaceV ofRn andvectorx ∈ Rn,let{v }m beanorthonormalbasisof
i i=1
V. Itholdsthatdist2(x,V) = ∥x∥2−(cid:80)m ⟨x,v ⟩2.
2 i=1 i
TheproofofLemmaD.3isstraightforwardandthusomitted.
LemmaD.4. ForanydistributionD overfunctionsmappingfromX → Randnfunctions{k }n
i i=1
wherek : X → Rforeachi ∈ [n]itholdsthat
i
 
(cid:32) n (cid:33)2 |X|
1 (cid:88) (cid:88) 1 (cid:88)
E min Ψ(x)− α ik i(x)  ≥ λ i(Σ).
h∼D α∈Rn |X| |X|
x∈X i=1 i=n+1
where Σ (x,x′) ≜ E [Ψ(x)Ψ(x′)] and λ denotes the i’th largest eigenvalue function. For
D Ψ∼D i
notationalconvenience,wealsoviewΣ asaR|X|×|X| matrix.
D
ProofofLemmaD.4. ForeachΨ,wedefiner ∈ R|X| whoseentriesarerealizationofthefunction
Ψ
Ψoninputsx ∈ X. Itsufficestoshowthat
|X|
E (cid:2) dist2(r ,V)(cid:3) ≥ (cid:88) λ (Σ )
Ψ i D
h∼D
i=n+1
31for any subspace V = span{v ,v ,··· ,v } ⊂ R|X|×|X| where v for i ∈ [n] are orthonormal
1 2 n i
vectors. Notethat
(cid:34) n (cid:35)
E (cid:2) dist2(r ,V)(cid:3) = E ∥r ∥2−(cid:88) ⟨v ,r ⟩2
Ψ Ψ 2 t Ψ
Ψ∼D Ψ∼D
t=0
(cid:18) (cid:19) n (cid:18) (cid:19)
(cid:104) (cid:105) (cid:88) (cid:104) (cid:105)
= Tr E ΨΨ⊤ − v⊤ E ΨΨ⊤ v
t t
Ψ∼D Ψ∼D
t=1
n
(cid:88)
≥ Tr(Σ )− λ (Σ )
D i D
i=1
|X|
(cid:88)
= λ (Σ ) (D.7)
i D
i=n+1
where the inequality in the second to last line is due to the min-max theorem (also called
Courant–Fischer–Weyl min-max principle) (Courant and Hilbert, 1953). This completes the
proof.
(cid:104) (cid:105)
ThefollowingCorollaryD.5isadirectconsequenceofLemmaD.4,notingthatE 1 (cid:80) Ψ(x)2 =
Ψ∼D |X| x∈X
Tr(Σ ).
D
CorollaryD.5. ForanydistributionDoverfunctionsmappingfromX → Randnfunctions{k }n
i i=1
wherek : X → Rforeachi ∈ [n],if(cid:80)n λ (Σ ) ≤ 1 Tr(Σ )thenitisguaranteedthat
i i=1 i D 2 D
 
(cid:32) n (cid:33)2 (cid:34) (cid:35)
1 (cid:88) (cid:88) 1 1 (cid:88)
E min Ψ(x)− α ik i(x)  ≥ E Ψ(x)2
Ψ∼D α∈Rn |X| 2 Ψ∼D |X|
x∈X i=1 x∈X
wheretheright-handsidedenotestheexpectedlossoftheall-0predictor.
LemmaD.6. ForanymatrixΣ ∈ Rd×d,subspaceV andprojectionmatrixP ∈ Rd×dcorresponding
V
toV,itholdsthat
n (cid:18) (cid:19)
(cid:88)
λ (Σ) ≤ n·λ (I −P )Σ(I −P ) +Tr(P ΣP ).
i 1 V V V V
i=1
Proof. Let V = span{α }n where α ∈ Rd is the i’th eigenvector of Σ and let P be its
n i i=1 i Vn
correspondingprojectionmatrix. LetP betheprojectionontothesumoftwosubspacesV +V,
Vn+V n
itholdsthat
n
(cid:88)
λ (Σ) = Tr(P ΣP )
i Vn Vn
i=1
≤ Tr(P ΣP )
Vn+V Vn+V
(cid:0) (cid:1)
= Tr(P ΣP )+Tr (P −P )Σ(P −P )
V V Vn+V V Vn+V V
(cid:0) (cid:1)
≤ Tr(P ΣP )+n·λ (P −P )Σ(P −P )
V V 1 Vn+V V Vn+V V
≤ Tr(P ΣP )+n·λ (cid:0) (I −P )Σ(I −P )(cid:1) . (D.8)
V V 1 V V
Here the second to last inequality is because P −P is at most rank-n and so is (P −
Vn+V V Vn+V
P )Σ(P −P ). ThelastinequalityisbecauseP ≤ I andthus(P −P )Σ(P −
V Vn+V V Vn+V Vn+V V Vn+V
P ) ⪯ (I −P )Σ(I −P ).
V V V
32D.5 LossLowerBound: ModularAdditionwithmSummands
LemmaD.7. LetD betheuniformdistributionover
(cid:40) (cid:18) (cid:88)m (cid:19)(cid:12) (cid:12) (cid:41)
H ≜ Ψ(x) = 1 σ (x ) ≡ 0 (mod p) (cid:12) σ ∈ S foralli ∈ [m] (D.9)
i i (cid:12) i p
(cid:12)
i=1
andΣ (x,x′) = E [Ψ(x)Ψ(x′)]. Itholdsthat
D Ψ∼D
n (cid:18) (cid:19)
(cid:88) n m−1
λ (Σ) ≤ pm−2+ exp .
i
p p−1
i=1
ProofofLemmaD.7. Consider the vector space V defined in Equation (D.1) and the projection
s
matrixP ∈ R|X|×|X| ontoV . TofindTr(cid:0) P ΣP (cid:1)wecanfirstderiveTr(cid:0) P ΨΨ⊤P (cid:1)where
Vs S Vs Vs Vs Vs
Ψ ∈ R|X| isasamplefromD. Notethatsince(I −P )hshouldbeorthogonaltoV (thesumof
Vs s
eachsliceoftheprojectedvectorshouldbezero)wehavethat
(cid:40)
(cid:104) (cid:105) −1 Ψ(x) = 0
(I −P )Ψ (x) = p .
Vs p−1 Ψ(x) = 1
p
Hence,P Ψ(x) = 1 forallx ∈ X. Thus,
Vs p
(cid:18) (cid:19) (cid:20) (cid:18) (cid:19)(cid:21)
Tr P ΣP = E Tr P ΨΨ⊤P
Vs Vs
Ψ∼D
Vs Vs
(cid:20) (cid:18) (cid:19)(cid:21)
1
= E Tr 1
Ψ∼D p2 |X|×|X|
|X|
=
p2
= pm−2 (D.10)
where1 denotestheall-onesquarematrixofsize|X|. Moreover,byLemmaD.9wehavethat
|X|×|X|
(cid:18) (cid:19) (cid:18) (cid:19)
n m−1
n·λ (I −P )Σ(I −P ) = sup v⊤Σ v ≤ exp . (D.11)
1 Vs Vs D
p p−1
∥v∥ 2≤1,v⊥Vs
Combiningthetwoequationsabove,wecanseethat
n (cid:18) (cid:19)
(cid:88) n m−1
λ (Σ) ≤ pm−2+ exp .
i
p p−1
i=1
Thisconcludestheproof.
CorollaryD.8. ConsiderthefunctionclassHdefinedinEquation(D.9)andtheuniformdistribution
overitdenotedbyD. Foranyintegersn ≥ 1,p ≥ 2,1 ≤ m < p,permutation-equivariantkernel
K : [p]m×[p]m → Ritholdsthat
(cid:13) (cid:13)(cid:88)n (cid:13) (cid:13)2 (cid:18)
1 n
(cid:18) m−1(cid:19)(cid:19)
min E inf (cid:13) λ K(x ,·)−Ψ(cid:13) ≥ pm−1 1− − exp .
x1,x2,···xnΨ∼Dα∈Rn(cid:13)
(cid:13)
t t (cid:13)
(cid:13)
p pm p−1
t=1 2
foranyx ,x ,··· ,x ∈ [p]m. Inotherwords,ifn < (1−Ω(1))pm,thentheexpectedpopulation
1 2 n
ℓ lossisatleastΩ(pm−1),whichisofthesamemagnitudeasthetrivialall-zeropredictor.
2
33ProofofCorollaryD.8. Itsufficestoshowthatforanyn-dimensionalsubspaceV ⊂ Rpm itholds
that
(cid:18) (cid:18) (cid:19)(cid:19)
1 n m−1
E dist2(V,Ψ) ≥ pm−1 1− − exp .
Ψ∼D p pm−1 p−1
CombiningLemmaD.7andCorollaryD.5yieldsthisstatement.
LemmaD.9. Foranyv ∈ Rpm suchthat∥v∥ = 1andv ⊥ V (definedinEquation(D.1)),itholds
s
that
(cid:18) (cid:19)
(cid:104) (cid:105) 1 m−1
E ⟨Ψ ,v⟩2 ≤ exp .
σ
σ∼U
m
p p−1
The main implication of this Lemma D.9 is that for any n-dimensional space V ⊂ Rpm can
not "cover" the vector space of all Ψ functions for different permutations σ ∈ S . To prove this
σ p
Lemma, we decompose the inner product ⟨v,h ⟩ to d+1 sums using the Multinomial Theorem.
σ
Thisdecompositionisenabledthroughobservingthefactthatthereared+1equivalencegroups
in the possible set of indices of v. Based on Lemma D.10, we can use the Binomial Theorem to
decomposetheexpectationoftheinnerproductasfollows
 
(cid:32) (cid:33)2
(cid:104) (cid:105) (cid:88)
E ⟨v,Ψ σ⟩2 = E  v(x)Ψ σ(x) 
σ∼U
m
σ∼U
m x
 
m
= E  (cid:88) (cid:88) Ψ σ(x)Ψ σ(x′)v(x)v(x′) 
σ∼U m
d=0 x,x′

dist(x,x′)=d
m
(cid:88) (cid:88)
= C v(x)v(x′) (D.12)
d
d=0 x,x′
dist(x,x′)=d
whereC d ≜ E σ∼U m[Ψ σ(x)Ψ σ(x′)]forany(x,x′) ∈ X d.
To complete the proof, we now have to bound two terms. First, we need to show that the
expectationE σ∼U m[Ψ σ(x)Ψ σ(x′)]foreach(x,x′)inthesameequivalencegroupisbounded. Next,
we need to show that for each set X , the sum (cid:80) v(x)v(x′) is also bounded. These are
d (x,x′)∈X
correspondinglyshowninLemmasD.10andD.11. Basedond thesetwoLemmas,wecannowpresent
theproofofLemmaD.9.
ProofofLemmaD.9.
m
(cid:104) (cid:105) (cid:88) (cid:88)
E ⟨v,Ψ ⟩2 = C v(x)v(x′)
σ d
σ∼U
m d=0 x,x′
dist(x,x′)=d
m (cid:18) (cid:19) (cid:18) (cid:19)
(cid:88) 1 1 m
= 1− (−1)d
p2 (1−p)d−1 d
d=0
1
(cid:18)
1
(cid:19)m−1
= 1+
p p−1
(cid:18) (cid:19)
1 m−1
≤ exp . (D.13)
p p−1
34wherethesecondtolaststepisduetothebinomialTheoremandthelaststepisduetothefactthat
forallx ∈ R,1+x ≤ exp(x).
LemmaD.10. Foranyindexvectorpair(x,x′) ∈ X itholdsthat
d
(cid:18) (cid:19)
E (cid:2) Ψ (x)Ψ (x′)(cid:3) = 1 1− 1 .
σ∼U
m
σ σ p2 (1−p)d−1
ProofofLemmaD.10. Letusre-iteratethedefinitionofh:
(cid:18) m (cid:19)
(cid:88)
Ψ (x) = 1 σ (x ) ≡ 0 (mod p) .
σ i i
i=1
Foreachpair(x′,x′)wedefineI(x,x′) ≜ {i : x[i] ̸= x′[i]}andE(x,x′) ≜ {i : x[i] = x′[i]}. We
alsodefine
C ≜ E (cid:2) Ψ (x)Ψ (x′)(cid:3)
d σ σ
σ∼U
m
whereforeach(x,x′) ∈ X . Notethatford = 0,wehavethat
d
C = E (cid:2) Ψ (x)Ψ (x′)(cid:3)
0 σ σ
σ∼U
m
= E [Ψ (x)]
σ
σ∼U
m
(cid:34) (cid:18) m (cid:19)(cid:35)
(cid:88)
= E 1 σ (x ) ≡ 0 (mod p)
i i
σ∼U
m
i=1
= E [a ≡ 0 (mod p)]
a∼Unif([p])
= 1/p. (D.14)
Forany2 ≤ d ≤ mitholdsthat
(cid:34) (cid:18) m m (cid:19)(cid:35)
(cid:88) (cid:88)
C = E 1 σ (x ) ≡ σ (x′) ≡ 0 (mod p)
d i i i i
σ∼U
m
i=1 i=1
 
(cid:18) (cid:19)
(cid:88) (cid:88) (cid:88) (cid:88)
= E 1 σ i(x i)+ σ i(x i) ≡ σ i(x′ i)+ σ i(x i) ≡ 0 (mod p) 
σ∼U
m
i∈I(x,x′) i∈E(x,x′) i∈I(x,x′) i∈E(x,x′)
 
(cid:18) (cid:19)
(cid:88) (cid:88) (cid:88)
= E 1 σ i(x i) ≡ σ i(x′ i) ≡ − σ i(x i) (mod p) 
σ∼U
m
i∈I(x,x′) i∈I(x,x′) i∈E(x,x′)
 
(cid:18) (cid:19)
(cid:88) (cid:88)
= E 1 σ i(x i) ≡ σ i(x′ i) ≡ ζ (mod p) 
σ∼U
m
ζ∼Unif([p]) i∈I(x,x′) i∈I(x,x′)
(cid:34) (cid:18) d d (cid:19)(cid:35)
(cid:88) (cid:88)
= E 1 y ≡ y ≡ ζ (mod p) (D.15)
i i
(y,y′)∼Unif(X )
d,d i=1 i=1
ζ∼Unif([p])
(cid:104) (cid:105)
where in the second to last line, we replaced E σ∼U
m
−(cid:80) i∈E(x,x′)σ i(x i) with E ζ∼Unif([p])[ζ]
(assumingd ≥ 1). Weaimtofindtheclosedformformulaforthegenerald ≥ 2. Asζ ∼ Unif([p])
35isindependentofthetwosums,wecanabsorbitandwrite(fromhereuntiltherestoftheproofwe
drop(mod p)fromequivalencesforeaseofpresentation)
(cid:34) (cid:32) d d (cid:33)(cid:35)
(cid:88) (cid:88)
Q ≜ Pr 1 y ≡ y′ = p·C . (D.16)
d i i d
(y,y′)∼Unif(X )
d,d i=1 i=1
Notethatford ≥ 2wehavethat
(cid:34) (cid:32)d−1 d−1 (cid:33) (cid:35)
(cid:88) (cid:88)
Q = Pr 1 y ≡ y′ ·1(y = y′)
d i i d d
(y,y′)∈Unif(X )
d,d i=1 i=1
(cid:34) (cid:32)d−1 d−1 (cid:33) (cid:32) d−1 d−1 (cid:33)(cid:35)
(cid:88) (cid:88) (cid:88) (cid:88)
+ Pr 1 y ̸≡ y′ ·1 y ≡ y′ + y′ − y
i i d d i i
(y,y′)∈Unif(X )
d,d i=1 i=1 i=1 i=1
1
= (1−Q )· . (D.17)
d−1
p−1
Henceford ≥ 2wehavethat
1
C = (1−p·C )
d d−1
p(p−1)
1 C
d−1
= −
p(p−1) p−1
(cid:18) (cid:19)
1 1
= 1− . (D.18)
p2 (1−p)d−1
Thiscompletestheproof.
LemmaD.11. ForanyX whered ∈ [m]andv ∈ Rpm suchthat∥v∥ = 1andv ⊥ V (definedin
d 2 s
Equation(D.1))itholdsthat
(cid:18) (cid:19)
(cid:88) m
v(x)v(x′) = (−1)d .
d
(x,x′)∈X
d
ProofofLemmaD.11. LetusdefineG(d)as
1 (cid:88)
G(d) ≜ v(x)v(x′)
(cid:0)m(cid:1)
d (x,x′)∈X
d
= E (cid:88) (cid:88) v(cid:0) σ(y∥t)(cid:1) v(cid:0) σ(y∥t′)(cid:1) . (D.19)
σ∈S
m
y∈[p]m−d(t,t′)∈X
d,d
Sincev ⊥ V wehavethat
s
   
E

(cid:88) (cid:88) (cid:88) v(cid:0) σ(y∥s∥t)(cid:1) (cid:88) v(cid:0) σ(y∥s∥t′)(cid:1)

σ∈S
m
y∈[p]d(t,t′)∈X
d−1,d−1
s∈[p] s∈[p]
 
= E  (cid:88) (cid:88) v(cid:0) σ(y∥s∥t)(cid:1) v(cid:0) σ(y∥s∥t′)(cid:1) 
σ∈S
m
(y∥s)∈[p]m−d+1(t,t′)∈X
d−1,d−1
 
+ E  (cid:88) (cid:88) v(cid:0) σ(y∥s∥t′)(cid:1) v(cid:0) σ(y∥s′∥t′)(cid:1) 
σ∈S
m
y∈[p]m−d(s∥t,s′∥t′)∈X
d,d
= G(d−1)+G(d) = 0. (D.20)
36NotethatG(0) = (cid:80) v(x)2 = ∥v∥2 = 1. Hence,G(d) = (−1)d. Thiscompletestheproof.
x 2
E Generalization Upper Bound for Regression
(cid:16) (cid:17) (cid:16) (cid:17)
Theoriginalmodelisf (cid:0) e ,e (cid:1)⊤ = V(W (cid:0) e ,e (cid:1)⊤ )⊙2. Weconsiderthemodelg (cid:0) e ,e ,e (cid:1)⊤ =
i j i j i j k
(cid:68) (cid:16) (cid:17)(cid:69)
e ,f (cid:0) e ,e (cid:1)⊤ andthefunctionclassHisdefinedoverg withdifferentweightsθ = (W,V)
k i j
where W ∈ Rh×2p and V ∈ Rp×h. For an input x ∈ R3p, we define two slices x′ ≜ x[:2p] and
x′′ ≜ x[2p:]. We also define W ≜ {W ∈ Rh×2p : ∥W∥ ≤ r} and V ≜ {V ∈ Rp×h :
h,r ∞ h,r
∥V∥ ≤ r} which we will use later to denote parameters of our function. We further define
∞
D = {x ,x ,··· ,x } such that for alla ∈ [n] wehavex = (cid:0) e ,e ,e (cid:1)⊤ for somei,j,k ∈ [p].
n 1 2 n a i j k
For this section, we fix the set {x ,x ,··· ,x } ∼ Unif(X) and denote by R (H) the empirical
1 2 n n
RademachercomplexityofthefunctionclassHdefinedas
(cid:34) n (cid:35)
1 (cid:88)
R (H) ≜ E sup Ψ(x )σ (E.1)
n i i
σ∼Unif({±1}n) Ψ∈H n
i=1
whereHmapsX → Randn ∈ N.
LemmaE.1. Considerthefunctionclasses
(cid:110) (cid:104) (cid:105) (cid:68) (cid:69)(cid:111)
Hw ≜ Ψ : R3p → R | ∃ W⊤ ∈ W ∧V ∈ V s.t. Ψ(x) = x′′,V (cid:10) W,x′(cid:11)2 (E.2)
r,r′ w,r w,r′
and
(cid:40) 4 (cid:41)
G ≜ g : R3p → R | ∃(cid:2) U ∈ R4×3p∧∥U∥ ≤ r(cid:3) s.t. g(x) = (cid:88) ⟨U⊤,x⟩3 (E.3)
r ∞ i
i=1
whereU denotesthei’throwofU. ThefunctionclassH1 iscontainedinG (andhence
i r,r′ max(r,r′)
R (H ) ≤ R (G )).
n r,r′ n max(r,r′)
ProofofLemmaE.1. We prove this lemma by showing that for each pair of matrices W,V of
Ψ ∈ H1 , we can construct a matrix U of g ∈ G such that for all x ∈ R3p, h(x) = g(x).
r,r′ max(r,r′)
(cid:68) (cid:69)
Consider an arbitrary parameterization W,V of h such that Ψ(x) = x′′,V ⟨W,x′⟩2 . We can
(cid:113)
constructU = 3 2 (cid:0) Q ,Q ,Q ,Q (cid:1)⊤ where
9 1 2 3 4
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
W −W −W/2 W/2
Q = , Q = , Q = , Q = . (E.4)
1 V 2 V 3 V 4 −V
Observethat
4
(cid:88)(cid:68) (cid:69)3
g(x) = U⊤,x
i
i=1
2 (cid:104) (cid:105)
= ⟨U ,x⟩3+⟨U ,x⟩3+⟨U ,x⟩3+⟨U ,x⟩3
1 2 3 4
9
(cid:34) (cid:35)
=
2 (cid:0)(cid:10) W,x′(cid:11) +(cid:10) V,x′′(cid:11)(cid:1)3 −(cid:0)(cid:10) W,x′(cid:11) −(cid:10) V,x′′(cid:11)(cid:1)3 −(cid:18) ⟨W,x′⟩ +(cid:10) V,x′′(cid:11)(cid:19)3 +(cid:18) ⟨W,x′⟩ −(cid:10) V,x′′(cid:11)(cid:19)3
9 2 2
(cid:68) (cid:69)
= x′′,V (cid:10) W,x′(cid:11)2
= Ψ(x). (E.5)
37Itisstraightforwardtoseethat∥U∥ = max(∥W∥ ,∥V∥ ),whichcompletestheproof.
∞ ∞ ∞
LemmaE.2. ConsiderthefunctionclassG definedinEquation(E.3). Itholdsthat
r
√
324r3 p
R (G ) ≤ √ .
n r
n
ProofofLemmaE.2. WecanderivetheRademachercomplexityofG as
r
(cid:34) n 4 (cid:35)
1 (cid:88) (cid:88)
R (G ) = E sup σ ⟨U⊤,x ⟩3
n r σ∼Unif({±1}n) U∈R4×3p,∥U∥ ∞≤r n a=1 a c=1 c a
(cid:34) n (cid:35)
4 (cid:88)
≤ E sup σ ⟨U,x ⟩3
a a
σ∼Unif({±1}n) U∈R3p,∥U∥ ∞≤r n a=1
(cid:34) 108r2 (cid:88)n (cid:35)
≤ E sup σ ⟨U,x ⟩ (E.6)
a a
σ∼Unif({±1}n) U∈R3p,∥U∥ ∞≤r n a=1
where we used Talagrand’s contraction lemma (Lemma E.3) using the fact that f(x) = x3 is
27r2-lipschitznessin[−3r,3r]andthat|⟨U,x ⟩| ≤ 3r forall∥U∥ ≤ r,andwecancontinue:
a ∞
(cid:34) 108r2 (cid:13) (cid:13)(cid:88)n (cid:13) (cid:13) (cid:35)
≤ E sup ∥U∥ (cid:13) σ x (cid:13)
σ∼Unif({±1}n) U∈R3p,∥U∥ ∞≤r n 2(cid:13) (cid:13) a=1 a a(cid:13) (cid:13) 2
108r3√
3p
(cid:34)(cid:13) (cid:13)(cid:88)n (cid:13)
(cid:13)
(cid:35)
≤ E (cid:13) σ x (cid:13)
n σ∼Unif({±1}n) (cid:13) (cid:13) a a(cid:13) (cid:13)
a=1 2
√
324r3 p
≤ √ . (E.7)
n
√
wherethelaststepfollowsfromRademachercomplexityoflinearmodelandthefactthat∥x∥ = 3
2
(cid:104)(cid:13) (cid:13) (cid:105) √
forallx ∈ D andE (cid:13)(cid:80)N σ (cid:13) ≤ n.
n σ∼Unif({±1}n) (cid:13) a=1 a(cid:13)
2
LemmaE.3(Talagrand’sContractionLemma). LetHbeanarbitraryfunctionclassandg bean
L-Lipschitzfunction. Itholdsthat
R (g◦H) ≤ L·R (H).
n n
Aproofofthisstandardresultcanbefound,forinstance,asLemma5.7ofMohrietal.(2018).
Remark E.4. Lemma E.2 directly implies the following Rademacher complexity bound for the
functionclassH1 definedinEquation(E.2):
r,r′
√
324 p
R (H1 ) ≤ √ max(r,r′)3.
n r,r′ n
LemmaE.5. √
324h p
R (Hh ) ≤ √ max(r,r′)3.
n r,r′ n
38ProofofLemmaE.5.
 
n
1 (cid:88)
R n(H rh ,r′) = σ∼UnifE ({±1}n) Ψ∈su Hp
rh
,r′
n
i=1σ iΨ(x i)
(cid:34) n (cid:42) (cid:18) (cid:18) (cid:19)(cid:19)⊙2(cid:43)(cid:35)
1 (cid:88) e
= E sup σ e ,V W ia
σ∼Unif({±1}n) W∈W h,r,V∈V h,r′ n a=1 a ka e ja
(cid:34) n (cid:42) h (cid:18) (cid:18) (cid:19)(cid:19)⊙2(cid:43)(cid:35)
1 (cid:88) (cid:88) e
= E sup σ e , V W ia
σ∼Unif({±1}n) W∈W h,r,V∈V h,r′ n a=1 a ka b=1 :,b b e ja
(cid:34) n (cid:42) (cid:28) (cid:18) (cid:19)(cid:29)2(cid:43)(cid:35)
h (cid:88) e
≤ E sup σ e ,V W, ia
σ∼Unif({±1}n) W∈W1,r,V∈V 1,r′ n a=1 a ka e ja
= hR (H1 ) (E.8)
n r,r′
whereinthesecondtolastlineweusedthefactthattheRademachercomplexityofatwo-layerNN
with h hidden neurons is bounded by h times that of a single-hidden-neuron counterpart. Thus,
applyingRemarkE.4wecanconcludethat
√
324h p
R (Hh ) ≤ √ max(r,r′)3.
n r,r′ n
Wearenowreadytopresenttheproofofthesamplecomplexityupperboundforone-hidden-layer
networksintheregressiontask. WefirstpresentthefollowingTheoremfromSrebroetal.(2010)on
boundingtheexcessriskofH-smoothlossfunctions.
TheoremE.6(Theorem1fromSrebroetal.(2010)). ForanH-smoothnon-negativelossℓsuch
thatforallx,y,Ψ,|ℓ(Ψ(x),y)| ≤ b,foranyδ > 0wehavewithprobabilityatleast1−δ overa
randomsamplesizeofn,foranyΨ ∈ H,
(cid:32) (cid:32) (cid:114) (cid:33) (cid:33)
(cid:113) √ blog(1/δ) blog(1/δ)
L(Ψ) ≤ Lˆ(Ψ)+K Lˆ(Ψ) Hlog1.5nR (H)+ +Hlog3nR2(H)+
n n n n
where K is a positive constant, L(Ψ) denotes the population loss of Ψ according to ℓ and Lˆ(Ψ)
denotesthelossofΨonthementionedsampleofsizenaccordingtoℓ.
WenowpresenttheproofofProposition3.7.
Proposition 3.7. For any R > 0,δ ∈ (0,1), D of size n, and θ∗ ∈ {θ = (W,V) :
train
L (g,θ,D ) = 0 ∧ ∥θ∥ ≤ R}, there exists a positive constant C > 0 such that with
ℓ2 train ∞
probabilityatleast1−δ overtherandomnessofD ,
train
CR6h2 (cid:18) 1(cid:19)
L (g,θ∗) ≤ plog3n+log .
ℓ2
n δ
Proof. ConsiderthefunctionclassHh forwhomwehavealreadyprovedaRademachercomplexity
R,R
upperbound. As∥θ∥ ≤ R,andalltheinputsareone-hot,forallx = (e ,e ,e )andg ∈ Hh
∞ i j k R,R
39it holds that g(x) ≤ 4hR3. This boundedness accordingly implies smoothness of ℓ loss on this
2
functionclasswithH = 1. Hence,TheoremE.6directlyappliestoourfunctionclass,yielding:
CR6h2(cid:18) (cid:19)
L (g(θ,·)) ≤ plog3n+log(1/δ)
l2
n
forsomepositiveconstantC independetofothervalues.
Finally,weremarkthatiftheℓ lossissmallenough,thenthemisclassificationerrorisguaranteed
2
tobezero.
Proposition E.7. Consider a predictor g : X → R. The population misclassification error is
upper-boundedby2L (g)/p.
ℓ2
Proof. Notethateach(x,y) ∈ X ×Y thatismisclassifiedinducesanℓ lossofatleastp2/2. Tosee
2
thatwhy,foreachpair(e ,e )tobemisclassifiedwhileattainingminimumpossibleℓ lossweneed
a b 2
p
g((e ,e ,e )) <
a b c
2
p
g((e ,e ,e )) >
a b d
2
g((e ,e ,e )) = 0 forallk ̸∈ {c,d}
a b k
where c = a+b (mod p), d ∈ [p] ̸= c and k ∈ [p]. Hence, each of (e ,e ,e ) and (e ,e ,e )
a b c a b d
introduceanℓ lossofatleastp2/4intheregerssiontask.
2
F Construction of Interpolating Solution with Small ℓ Norm
∞
Inthissection,weproveProposition3.8. Wepresentaconstructionofweightsthatinterpolatesthe
datasetforh = 8p. Thenwegeneralizethisresulttoanyh ≥ 8pbyduplicatingthefirst8pneurons
(cid:106) (cid:107) (cid:106) (cid:107)−1
h times,whereeachcopyis h 3 timessmallerinmagnitude.
8p 8p
Proposition3.8. LetthesetofmodelswithzeropopulationlossbeΘ∗ ≜ {θ | L (g,θ) = 0}. For
ℓ2
(cid:106) (cid:107)−1
anyp ≥ 2andh ≥ 8p,Θ∗ isnonemptyandmin ∥θ∥ ≤ h 3.
θ∈Θ∗ ∞ 8p
ProofofProposition3.8. Webeginbyconstructing8matricesofsizep×2pdenotedbyW(i). For
everyn,m ∈ [p],wehavethat
40(cid:18) (cid:19) (cid:18) (cid:19)
2πk 2πk
(1) (1)
W = cos n W = +cos m
k,n p k,m+p p
(cid:18) (cid:19) (cid:18) (cid:19)
2πk 2πk
(2) (2)
W = cos n W −cos m  (cid:16) (cid:17)⊤
k,n p k,m+p= p +cos 2πkq
p
W
k( ,3 n)
=
sin(cid:18) 2π pk n(cid:19)
W
k( ,3 m)
+p
=
+sin(cid:18) 2π pk m(cid:19)  

−cos(cid:16) (cid:16)2π pkq(cid:17) (cid:17) 


(cid:18) 2πk (cid:19) (cid:18) 2πk (cid:19)  −cos 2π pkq  
W(4) = sin n W(4) = −sin m  (cid:16) (cid:17)
k,n p k,m+p p +cos 2πkq 
 p 
(5) (cid:18) 2πk (cid:19) (5) (cid:18) 2πk (cid:19) V q,8k:8(k+1) =  +sin(cid:16) 2πkq(cid:17) 
W = sin n W = +cos m  p 
k,n p k,m+p p  (cid:16) (cid:17)
−sin 2πkq 
(cid:18) 2πk (cid:19) (cid:18) 2πk (cid:19)  p 
W(6) = sin n W(6) = −cos m  (cid:16) (cid:17)
k,n p k,m+p p  +sin 2π pkq  
(cid:18) (cid:19) (cid:18) (cid:19)  (cid:16) (cid:17)
(7) 2πk (7) 2πk −sin 2πkq
W = cos n W = +sin m p
k,n p k,m+p p (F.2)
(cid:18) (cid:19) (cid:18) (cid:19)
2πk 2πk
(8) (8)
W = −cos n W = sin m
k,n p k,m+p p
(F.1)
EachW(i) for1 ≤ i ≤ 8isap×2pmatrix,whoseelementsaregivenbytheequationspresented
above. Hence,ineachequationk,n,m ∈ {0,1,··· ,p}. Theconstructionofthefirstlayerisbased
onstackingW(i) for1 ≤ i ≤ 8toconstructW ∈ R8p×2p. Theweightsofthesecondlayeraregiven
inEquation(F.2),whereV presentsasliceofthesecondlayerandq,k ∈ {0,1,··· ,p−1}.
q,8k:8(k+1)
To show that this construction solves the modular addition problem analytically, we will
analyticallyperformtheinferencestepfortwoarbitraryinputsn,mwherex = (e ,e ). Wedenote
n m
h = (Wx)⊙2 ∈ R8p asthepost-activationsofthefirstlayer,whichisgivenby
 (cid:16) (cid:17) (cid:16) (cid:17)2
cos 2πkn +cos 2πkm
p p
 (cid:16) (cid:17) (cid:16) (cid:17)
cos 2πkn −cos 2πkm 
 p p 
 (cid:16) (cid:17) (cid:16) (cid:17)
sin 2πkn +sin 2πkm 
 p p 
 (cid:16) (cid:17) (cid:16) (cid:17)
sin 2πkn −sin 2πkm 
h 8k:8(k+1) =   (cid:16) p (cid:17) (cid:16) p (cid:17)  . (F.3)
sin 2πkn +cos 2πkm 
 p p 
 (cid:16) (cid:17) (cid:16) (cid:17)
sin 2πkn −cos 2πkm 
 p p 
 (cid:16) (cid:17) (cid:16) (cid:17)
cos 2πkn +sin 2πkm 
 p p 
 (cid:16) (cid:17) (cid:16) (cid:17)
cos 2πkn −sin 2πkm
p p
Notethatforeachk,wehavethat(afterdropping(e ,e )forsimplicity)
n m
(cid:18) (cid:19) (cid:18) (cid:19)
2πk 2πk
h −h = 2cos (n+m) +2cos (n−m) (F.4)
8k 8k+1
p p
and
(cid:18) (cid:19) (cid:18) (cid:19)
2πk 2πk
h −h = 2cos (n−m) −2cos (n+m) (F.5)
8k+2 8k+3
p p
and
(cid:18) (cid:19) (cid:18) (cid:19)
2πk 2πk
h −h = 2sin (n+m) +2sin (n−m) (F.6)
8k+4 8k+5
p p
41and
(cid:18) (cid:19) (cid:18) (cid:19)
2πk 2πk
h −h = 2sin (n+m) −2sin (n−m) . (F.7)
8k+6 8k+7
p p
Hence,
(cid:18) (cid:19)
2πk
h −h −h +h = 4cos (n+m) (F.8)
8k 8k+1 8k+2 8k+3
p
and
(cid:18) (cid:19)
2πk
h −h +h −h = 4sin (n+m) . (F.9)
8k+4 8k+5 8k+6 8k+7
p
Usingthefactthatcos(a−b) = cos(a)cos(b)−sin(a)sin(b),wecanseethat
p−1 (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
(cid:88) 2πk 2πk 2πk 2πk
[f(e ,e )] = ⟨V ,h⟩ = 4 cos q cos (n+m) −sin q sin (n+m)
n m q q,:
p p p p
k=0
p−1 (cid:18) (cid:19)
(cid:88) 2πk
= 4 cos (m+n−q)
p
k=0
(cid:0) (cid:1)
= 4p1 (m+n−q) mod p = 0
(F.10)
wherethelastequalityfollowsfromEuler’sidentityandneedsptobeodd.
Remark F.1. Assuming p is odd, we need at most 4p hidden neurons to interpolate the modular
additiontask.
Observingthefactthatcos(2π−a) = cos(a),wecanseethat
p−1
(cid:88)p−1 (cid:18) 2πk (cid:19) (cid:88)2 (cid:18) 2πk (cid:19)
cos (m+n−q) = 1+2 cos (m+n−q) (F.11)
p p
k=0 k=1
(cid:16) (cid:17)
wherewereplacedcos 2π0(m+n−q) with1. BasedonEquation(F.11),wecancutouthalfof
p
theweightsofthefirstandsecondlayer,andonlyconstructthefrequenciesupto p−1,whichresults
2
inonlyneeding4phiddenneuronstoconstructtheinterpolatingsolution.
G Margin-Based Generalization Bound for Classification
Webeginbyprovidingsomebackgroundandnotationonsub-exponentialrandomvariables,which
willbelaterusedintheproofofourmargin-basedgeneralizationbound.
G.1 Backgroundonsub-exponentialvariables
Thefollowingproofsrelyheavilyonconcentrationinequalitiesforsub-exponentialrandomvariables;
wewillfirstreviewsomebackgroundonthesequantities.
Areal-valuedrandomvariableX withmeanµiscalledsub-exponential(seee.g.Wainwright,
2019)iftherearenon-negativeparameters(ν,α)suchthat
E[eλ(X−µ)] ≤
eν2 2λ2
forall|λ| <
1
. (G.1)
α
42WeuseX ∼ SE(ν,α)todenotethatX isasub-exponentialrandomvariablewithparameters(ν,α),
butnotethatthisisnotaparticulardistribution.
One famous sub-exponential random variable is the product of the absolute value of two
standard normal distributions, z ∼ N(0,1), such that the two factors are either independent
i
(X = |z ||z | ∼ SE(ν ,α )withmean2/π)orthesame(X = z2 ∼ SE(2,4)withmean1). We
1 1 2 p p 2
nowpresentafewlemmasregardingsub-exponentialrandomvariablesthatwillcomeinhandyinthe
latersubsectionsoftheappendix.
LemmaG.1. AssumeX issub-exponentialwithparameters(ν,α). Itholdsthattherandomvariable
sX wheres ∈ R+ isalsosub-exponential,butwithparameters(sν,sα).
Proof. Consider X ∼ SE(ν,α) and X′ = sX with E[X′] = sE[X]. Based on the definition of
sub-exponentialrandomvariables
ν2λ2 1
E[exp(λ(X −µ))] ≤ exp( ) forall|λ| <
2 α
(cid:20) (cid:18)
λ
(cid:19)(cid:21) ν2s2λ2
λ 1
=⇒ E exp (sX −sµ) ≤ exp( s2 ) forall| | < (G.2)
s 2 s sα
=λ =′= =⇒λ s E(cid:2) exp(cid:0) λ′(X′−µ′)(cid:1)(cid:3) ≤ exp(ν2s2 λ′2 ) forall|λ′| < 1
2 sα
Definingα′ = sαandν′ = sν weseethatX′ ∼ SE(sν,sα).
PropositionG.2. IfalloftherandomvariablesX fori ∈ [N]forN ∈ N+ aresub-exponentialwith
i
(cid:113)
parameters (ν ,α ), and all of them are independent, then (cid:80)N X ∈ SE( (cid:80)N ν2,max α ),
i i i=1 i i=1 i i i
(cid:18) (cid:113) (cid:19)
and N1 (cid:80)N i=1X
i
∼ SE √1
N
N1 (cid:80)N i=1ν i2, N1 max iα
i
.
Proof. Thisisasimplificationofthediscussionpriortoequation2.18ofWainwright(2019).
Proposition G.3. For a random variable X ∼ SE(ν,α), the following concentration inequality
holds:
(cid:18) (cid:18) t2 t (cid:19)(cid:19)
Pr(|X −µ| ≥ t) ≤ 2exp −min , .
2ν2 2α
Proof. TheproofisstraightforwardfrommultiplyingtheresultderivedinEquation2.18ofWainwright
(2019)byascalar.
CorollaryG.4. ConsiderX ∼ SE(ν,α),thefollowingboundholdswithprobabilityatleast1−δ:
(cid:32) (cid:114) (cid:33)
2 2
|X −µ| < max ν 2log ,2αlog .
δ δ
Asub-Gaussianrandomvariable,SG(ν),isonewhichsatisfies(G.1)forallλ,i.e.itisthelimit
ofSE(ν,α)asα → 0.
PropositionG.5(Chernoffbound). IfX isSG(ν),thenwithprobabilityatleast1−δ,|X −µ| ≤
(cid:113)
ν 2log 2.
δ
PropositionG.6(Hoeffding’sinequality). IfX ,...,X areindependentvariableswithmeansµ
1 n i
(cid:113)
andeachSG(ν ),then|(cid:80)n X −(cid:80)n µ | ≤ 2(cid:0)(cid:80)n ν2(cid:1) log 2 withprobabilityatleast1−δ.
i i=1 i i=1 i i=1 i δ
43G.2 GeneralizationBound
Wearenowreadytostatethemaintheoremforprovinganupperboundonthenumberoftraining
pointsneededtogeneralize. Webeginbydefiningsomenotationsandoperatorsthatwillbeusefulin
themainproof. First,wedefine0 tobethezeromatrix(orvectorincaseitsone-dimensional)of
a×b
shapea×b.
DefinitionG.7. Assumep ≥ 2isaninteger. WedefineΘh,b asthesetofpossibleparametersof
r
one-hiddenlayerquadraticnetworksofwidthhwhoseℓ normisboundedbyr andhaveboutput
∞
logits. Formally,
(cid:26) (cid:12) (cid:27)
Θh,b ≜ (W,V) (cid:12) (cid:12) W ∈ Rh×2p,V ∈ Rb×h,∥V∥ ≤ r,∥W∥ ≤ r .
r (cid:12) ∞ ∞
(cid:110) (cid:111)
WealsodefineMh ≜ (W,V) ∈ Θh | ∀i ∈ [p]; (cid:80)h V = 0 astheparameterswhosesecond
r r j=1 ij
layerweights’rowshaveazerosum.
DefinitionG.8. Wenextdefineanoperatorforaddinggaussiannoisetoinputmatricesorvectors:
Λ (A) = A+A˜
σ
whereAisanymatrixorvectorandA˜hasthesameshapeasAexceptitsentriesarei.i.dsampled
fromtheGaussiandistributionN(0,σ2).
OurproofreliesonLemma1fromNeyshaburetal.(2018). Forconvenience,were-iteratethis
lemmahere.
Lemma G.9 (Lemma 1 from (Neyshabur et al., 2018)). Let f(θ;·) be a predictor X → RK
for some integer K > 0 with parameters θ and P be any distribution on parameters that
is independent of the training data. For any γ,δ > 0, it holds with probability at least
1 − δ over randomness of training for any θ and any distribution on parameters P such that
θ
(cid:2) (cid:12) (cid:12) (cid:3)
Pr
u∼P
θ
max x∈X,k∈[K](cid:12)f k(θ+u;x)−f k(θ;x)(cid:12) < γ/4 > 1/2
(cid:115)
KL(θ+u∥P)+log 6m
L (f,θ,D) ≤ L (f,θ,D )+4 δ
0 γ train
m−1
whereD denotesthepopulation.
ThefollowingismainTheoremforthissection.
Theorem4.6. Foranyp ≥ 2, trainingsetsizen ≥ 1,δ ∈ (0,1), normr > 0, widthh′ > 4log 2
√ δ
andnormalizedmarginγ/r3 = Ω˜( h)itholdswithprobabilityatleast1−δ overtherandomness
ofthetrainingsetD thatforanyθ′ ofwidthh′ with∥θ′∥ ≤ r
train ∞
(cid:32)(cid:114) (cid:115) (cid:33)
p h2
L (f,θ′,D) ≤ L (f,θ′,D )+O˜ · 3 (4.4)
0 γ train n γ/r3
whereD denotesthepopulationandf,LaredefinedinEquations(2.1)and (4.3)respectively.
44Proof. Let us first construct θ = (W,V) ∈ Mh,p where h = 2h′ from θ′ = (W′,V′) such that
r
W =
(cid:20) W′(cid:21)
and V = (cid:2) V′ −V′(cid:3). This network has the same outputs as the original one with
W′
parameters θ′, while each row in V has a zero sum (and hence θ ∈ Mh,p). Since the outputs of
r
the network with parameters θ are the same as those of θ′, any generalization bound applying to
parametersθ alsoappliestotheparametersθ′. Notethat
f
(cid:16)
(cid:0) Λ (V),Λ (W)(cid:1) ,(e ,e
)(cid:17)
= (V +V˜
)⊤(cid:16)
(cid:0) W +W˜ (cid:1) (e ,e
)(cid:17)⊙2
c σ σ a b c c a b
(cid:16) (cid:17)
= f (θ,(e ,e ))+V Q˜⊙2+2Q⊙Q˜ +V˜ (Q+Q˜)⊙2. (G.3)
c a b c c
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) LemmaG.13
LemmaG.11
where we denoted V˜ = Λ (V)−V,W˜ = Λ (W)−W, Q = (W +W ) and Q˜ = (W˜ +W˜ ).
σ σ a b a b
Asnotedintheinequality,wecanapplyLemmasG.11andG.13toshowthatforanyδ ∈ (0,1)with
1
probabilityatleast1−δ overtherandomnessofperturbationitholdsthat
1
(cid:12) (cid:16) (cid:17) (cid:12)
(cid:12)f (cid:0) Λ (V),Λ (W)(cid:1) ,(e ,e ) −f (cid:0) θ,(e ,e )(cid:1)(cid:12)
(cid:12) c σ σ a b c a b (cid:12)
≤ 16(cid:114) 2hlog 2 max(cid:0) r2σ,rσ2(cid:1) +32√ 2h(cid:18) log 2(h+1)(cid:19)3/2 max(σ3,rσ2,r2σ)
δ δ
1 1
√ (cid:18) 2(h+1)(cid:19)3/2
≤ 64 2h log max(σ3,rσ2,r2σ). (G.4)
δ
1
Applyaunionboundonalldifferentc ∈ [p]toseethatforanyδ ∈ (0,1)withprobabilityatleast
2
1−δ overrandomnessofperturbation
2
(cid:12) (cid:12)
(cid:12) (cid:16) (cid:17) (cid:12)
max(cid:12)f (cid:0) Λ (V),Λ (W)(cid:1) ,(e ,e ) −f (cid:0) θ,(e ,e )(cid:1)(cid:12) (G.5)
(cid:12) c σ σ a b c a b (cid:12)
c∈[p](cid:12) (cid:12)
√ (cid:18) 2p(h+1)(cid:19)3/2
≤ 64 2h log max(σ3,rσ2,r2σ). (G.6)
δ
2
Hence,we’dwant
64√ 2h(cid:18)
log
2p(h+1)(cid:19)3/2 max(cid:0) (σ/r)3,(σ/r)2,σ/r(cid:1)
≤
γ
δ 4r3
2
=⇒ max(cid:0) (σ/r)3,(σ/r)2,σ/r(cid:1) ≤
γ/r3
(G.7)
√ (cid:16) (cid:17)3/2
2(h+1)
256 2h log
δ1

(cid:32) (cid:33)1/3

   γ/r3 r (*)
=⇒ σ =
  256√ 2h(cid:16) log2(h δ+ 11)(cid:17)3/2
(G.8)
     256√ 2h(cid:16) lγ o/ gr 23 (h+1)(cid:17)3/2r (**)
δ1
where γ/r3 > 1 decides the event (*) and γ/r3 ≤ 1 decides the
256√ 2h(cid:16) log2(h+1)(cid:17)3/2 256√ 2h(cid:16) log2(h+1)(cid:17)3/2
δ1 δ1
event(**). Assumingwe’reintheregimewhereσ > r, wecanchooseδ < 1/2toseethatwith
2
45probabilityatleast1/2
(cid:12) (cid:12)
(cid:12) (cid:16) (cid:17) (cid:12)
max(cid:12)f (cid:0) Λ (V),Λ (W)(cid:1) ,(e ,e ) −f (cid:0) θ,(e ,e )(cid:1)(cid:12) ≤ γ/4. (G.9)
(cid:12) c σ σ a b c a b (cid:12)
c∈[p](cid:12) (cid:12)
Note that KL(Λ (A)∥N(0,σ2)) ≤
∥A∥2
F for any matrix A. Apply Lemma G.9 to see that with
σ 2σ2
probabilityatleast1−δ overrandomnessofD ofsizenwehavethat
train
(cid:118)
(cid:117)
(cid:117)
(cid:117)3hplog2(h δ+ 11)
+log 6n
(cid:117) (cid:16) γ/r3 (cid:17)2/3 δ
L (f,θ,D) ≤ L (f,θ,D )+4(cid:116) 256√ 2h
0 γ train
n−1
(cid:32)(cid:114) (cid:115) (cid:33)
p h2
≤ L (f,θ,D )+O˜ · 3 (G.10)
γ train n γ/r3
foranyθ ∈ Mh,p.
r
LemmaG.10. Chooseσ,δ > 0andintegerp ≥ 2. Foranyr > 0andintegersh ≥ 1,a,b ∈ [p]it
holdswithprobabilityatleast1−δ that
(cid:114)
(cid:12) (cid:12)Λ (V)⊤(cid:0) W(e ,e )(cid:1)⊙2 −V⊤(cid:0) W(e ,e )(cid:1)⊙2(cid:12) (cid:12) ≤ 4r2σ hlog 2
(cid:12) σ a b a b (cid:12) δ
where(W,V) ∈ Θh,1 .
r
Proof. Denote by V˜ = Λ (V) − V and Q = (cid:0) W(e ,e )(cid:1)⊙2. We can expand the target as
σ a b
V⊤Q+V˜⊤Qwherethefirstsummandisconstantandthesecondsummandisdistributedaccording
(cid:16) (cid:17)
to N 0,σ2∥Q∥2 . Note that since ∥W∥ ≤ r, we have that ∥Q∥2 ≤ 16hr4. Applying
2 ∞ 2
PropositionG.5onthisGaussianrandomvariable,onecanseethatwithprobabilityatleast1−δ
overrandomnessofperturbation
(cid:114)
(cid:12) (cid:12)Λ (V)⊤(cid:0) W(e ,e )(cid:1)⊙2 −V⊤Q(cid:12) (cid:12) ≤ 4r2σ hlog 2 . (G.11)
(cid:12) σ a b (cid:12) δ
LemmaG.11. Chooseσ,δ > 0andintegerp ≥ 2. Foranyr > 0andintegersh ≥ 8log 2,a,b ∈ [p]
δ
itholdswithprobabilityatleast1−δ that
(cid:12) (cid:12) (cid:114)
(cid:12) (cid:12)V⊤(cid:0)
Λ σ(W)(e a,e
b)(cid:1)⊙2 −V⊤(cid:16)
W(e a,e
b)(cid:17)⊙2(cid:12)
(cid:12) ≤ 16 hlog
2 max(cid:0) r2σ,rσ2(cid:1)
(cid:12) (cid:12) δ
where(W,V) ∈ Mh,1 .
r
Proof. Denote by W˜ = Λ (W)−W, Q = (cid:0) W(e ,e )(cid:1)⊙2 and Q˜ = (cid:0) W˜ (e ,e )(cid:1)⊙2. Note that
σ a b a b
eachcoordinateofQ˜ issub-exponentialwithparametersSE(4σ2,8σ2)andmean2. Wecanexpand
(cid:16) (cid:17)
V⊤(cid:0) Λ (W)(e ,e )(cid:1)⊙2 = V⊤Q+V⊤Q˜ +2V⊤ (W +W )⊙(W˜ +W˜ ) .
σ a b a b a b
46√
Note that V⊤Q˜ is sub-exponential with parameters SE(4rσ2 h,8rσ2) 10 and mean 0 (due to
(cid:80)h V = 0andlinearityofexpectation). WecanapplyCorollaryG.4toseethatwithprobability
i=1 i
atleast1−δ/2
(cid:32)(cid:114) (cid:33)
(cid:12) (cid:12) 2 2
(cid:12)V⊤Q˜(cid:12) ≤ 8rσ2max 2hlog ,4log .
(cid:12) (cid:12) δ δ
(cid:16) (cid:17)
Moreover,Since2V⊤ (W +W )⊙(W˜ +W˜ ) isdistributedaccordingtoN(0,8σ2∥V ⊙(W +W )∥2)
a b a b a b 2
and ∥V ⊙(W +W )∥2 ≤ 4hr4, applying Proposition G.5 reveals that with probability at least
a b 2
1−δ/2overrandomnessofperturbation
(cid:114)
(cid:12) (cid:16) (cid:17)(cid:12) 2
(cid:12)2V⊤ (W +W )⊙(W˜ +W˜ ) (cid:12) ≤ 8r2σ hlog .
(cid:12) a b a b (cid:12) δ
Combiningthetwoequationsaboveshowsthatwithprobabilityatleast1−δ overrandomnessof
perturbationitholdsthat
(cid:114)
(cid:12) (cid:12)V⊤(cid:0) Λ (W)(e ,e )(cid:1)⊙2 −V⊤Q(cid:12) (cid:12) ≤ 16 hlog 2 max(cid:0) r2σ,rσ2(cid:1) .
(cid:12) σ a b (cid:12) δ
LemmaG.12. Chooseσ,δ > 0andintegerp ≥ 2. Foranyr > 0andintegersh ≥ eδ,a,b ∈ [p]it
holdswithprobabilityatleast1−δ overrandomnessofperturbationthat
(cid:12) (cid:12) (cid:12)Λ σ(0 h)⊤(cid:16) Λ σ(0 h×2p)(e a,e b)(cid:17)⊙2(cid:12) (cid:12) (cid:12) ≤ (cid:18) 2σ2+8√ 2σ2log 2(h+1)(cid:19) σ(cid:114) hlog 2(h+1) .
(cid:12) (cid:12) δ δ
(cid:16) (cid:17)⊙2
Proof. Denote by V˜ = Λ (0 ) and Q˜ = Λ (0 )(e ,e ) . It’s easy to see that each
σ h σ h×2p a b
√ √
coordinateofQ˜ issub-exponentialwithparametersSE(2σ2 2,4σ2 2)andmean2σ2. Tobound
V˜⊤Q˜,weemploythefollowingstrategy: sinceeachcoordinateofQ˜ isasub-exponentialrandom
variable, we can use a union bound in combination with Corollary G.4 to derive a bound on the
maximumvalueofthem. Next,wepullthismaximumvalueoutofthesum,andapplyPropositionG.6
toboundthesumofremainingindependentGaussians. Combiningthesetwohighprobabilityevents,
wepresentahighprobabilityboundonV˜⊤Q˜ beingbounded. Formally,forarbitraryδ ,δ > 0:
1 2
(cid:34) (cid:32) (cid:114) 2h √ 2h(cid:33) (cid:35)
Pr Q˜ ≤ 2σ2+σ2max 4 log ,8 2log foralli ∈ [h] ≥ 1−δ
i 1
δ δ
1 1
(cid:34) (cid:12) (cid:12) (cid:18) √ 2h(cid:19)(cid:12) (cid:12)(cid:88)h (cid:12) (cid:12)(cid:35)
=⇒ Pr (cid:12)V˜⊤Q˜(cid:12) ≤ 2σ2+8 2σ2log (cid:12) V˜(cid:12) ≥ 1−δ
(cid:12) (cid:12) δ (cid:12) i(cid:12) 1
1 (cid:12) (cid:12)
i=1
(cid:20)(cid:12) (cid:12) (cid:18) √ 2h(cid:19) (cid:114) 2 (cid:21)
=⇒ Pr (cid:12)V˜⊤Q˜(cid:12) ≤ 2σ2+8 2σ2log σ hlog ≥ 1−δ −δ
(cid:12) (cid:12) δ δ 1 2
1 2
(cid:34) (cid:12) (cid:12) (cid:18) √ 2(h+1)(cid:19) (cid:114) 2(h+1)(cid:35)
=⇒ Pr (cid:12)V˜⊤Q˜(cid:12) ≤ 2σ2+8 2σ2log σ hlog ≥ 1−δ (G.12)
(cid:12) (cid:12) δ δ
whereforthelaststeptobecorrectwechoseδ = h δ andδ = 1 δ.
1 h+1 2 h+1
10Notethattheseparametersarenottight,butthisdoesn’taffectthecorrectnessofthisargument. Forexample,arandom
variablethatisSE(a,b)isalsoSE(2a,2b),orifit’sSG(a),thenitisalsoSG(2a).
47LemmaG.13. Chooseσ,δ > 0andintegerp ≥ 2. Foranyr > 0andintegersh ≥ 8log 2,a,b ∈ [p]
δ
itholdsthat
(cid:12) (cid:12) (cid:12)Λ σ(0 h)⊤(cid:16) Λ σ(W)(e a,e b)(cid:17)⊙2(cid:12) (cid:12) (cid:12) ≤ 32√ h(cid:18) log 2(h+1)(cid:19)3/2 max(σ3,rσ2,r2σ,σ).
(cid:12) (cid:12) δ
4
whereW ∈ Rh×p suchthat∥W∥ ≤ r.
∞
Proof. DenotebyV˜ = Λ (0 ),W˜ = Λ (W)−W,Q = W(e ,e )andQ˜ = W˜ (e ,e ). Wehave
σ h σ a b a b
that
(cid:16) (cid:17)⊙2 (cid:16) (cid:17)⊙2
Λ (0 )⊤ Λ (W)(e ,e ) = V˜⊤ W +W +W˜ +W˜
σ h σ a b a b a b
(cid:16) (cid:17)
= V˜⊤ Q⊙2+Q˜⊙2+2Q⊙Q˜ (G.13)
InLemmaG.10wehavealreadyshownthatforanyδ > 0withprobabilityatleast1−δ over
1 1
randomnessofperturbationitholdsthat
(cid:114)
(cid:12) (cid:12) 2
(cid:12)V˜⊤Q⊙2(cid:12) ≤ 4r2σ hlog . (G.14)
(cid:12) (cid:12) δ
1
Denoteξ = V˜⊙Q˜. ξ,thevectorofproductoftwoindependentGaussians,issub-exponentialwith
√ √ √ √
parametersSE(2σ2 2,4σ2 2)andmean0(andhencesumofitscoordinatesisSE(2σ2 2h,4σ2 2)).
Since ∥W∥ ≤ r, applying Corollary G.4 yields that for any δ > 0, it holds with probability at
∞ 2
least1−δ that
2
(cid:12) (cid:12)2V˜⊤(cid:0) Q⊙Q˜(cid:1)(cid:12) (cid:12) ≤ 4√ 2rσ2max(cid:18)(cid:114) 2hlog 2 ,4log 2 (cid:19)
(cid:12) (cid:12) δ δ
2 2
(cid:114)
2
≤ 16rσ2 hlog . (G.15)
δ
2
Finally,weemployLemmaG.12toshowthatforanyδ > 0itholdswithprobabilityatleast
3
1−δ that
3
(cid:115)
(cid:12) (cid:12) (cid:18) √ 2(h+1)(cid:19) 2(h+1)
(cid:12)V˜⊤Q˜⊙2(cid:12) ≤ 2σ2+8 2σ2log σ hlog . (G.16)
(cid:12) (cid:12) δ δ
3 3
Applying a union bound on Equations (G.14) to (G.16) and choosign δ = δ /3 = δ /3 =
4 1 2
δ /(3h+3)revealsthatwithprobabilityatleast1−δ
3 4
(cid:12) (cid:12)
(cid:12)Λ σ(0
h)⊤(cid:16)
Λ σ(W)(e a,e
b)(cid:17)⊙2(cid:12) (cid:12)
(cid:12) ≤
16(cid:114)
hlog
h max(cid:0) rσ2,r2σ(cid:1) +16√ 2hσ3(cid:18)
log
2(h+1)(cid:19)3/2
(cid:12) (cid:12) δ δ
4 4
√ (cid:18) 2(h+1)(cid:19)3/2
≤ 32 h log max(σ3,rσ2,r2σ). (G.17)
δ
4
48