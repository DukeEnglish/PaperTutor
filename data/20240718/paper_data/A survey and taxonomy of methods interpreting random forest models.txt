A survey and taxonomy of methods interpreting random
forest models
Maissae Haddouchi and Abdelaziz Berrado
Maissae Haddouchi
Mohammed V University in Rabat, Ecole Mohammadia d'Ingénieurs (EMI), Rabat,
Morocco. E-mail: maissaehaddouchi@research.emi.ac.ma
ORCID iD: 0000−0002−4851−7979.
Abdelaziz Berrado
Mohammed V University in Rabat, Ecole Mohammadia d'Ingénieurs (EMI), Rabat,
Morocco. E-mail: berrado@emi.ac.ma
Conflict of Interest
On behalf of all authors, the corresponding author states that there is no conflict of interest.
Abstract
The interpretability of random forest (RF) models is a research topic of growing interest in the
machine learning (ML) community. In the state of the art, RF is considered a powerful learning
ensemble given its predictive performance, flexibility, and ease of use. Furthermore, the inner process
of the RF model is understandable because it uses an intuitive and intelligible approach for building
the RF decision tree ensemble. However, the RF resulting model is regarded as a “black box”
because of its numerous deep decision trees. Gaining visibility over the entire process that induces
the final decisions by exploring each decision tree is complicated, if not impossible. This complexity
limits the acceptance and implementation of RF models in several fields of application.
Several papers have tackled the interpretation of RF models. This paper aims to provide an
extensive review of methods used in the literature to interpret RF resulting models. We have
analyzed these methods and classified them based on different axes. Although this review is not
exhaustive, it provides a taxonomy of various techniques that should guide users in choosing the
most appropriate tools for interpreting RF models, depending on the interpretability aspects sought. It
should also be valuable for researchers who aim to focus their work on the interpretability of RF or ML
black boxes in general.
Keywords: Interpretability, Explainability, Random Forest, Taxonomy, Literature Review.A survey and taxonomy of methods interpreting random forest models
1. INTRODUCTION
The growing progress in computational capabilities and recent advances in computer science
developments have made machine learning (ML) algorithms a new kind of powerful “brains” capable
of processing massive data and learning the predictive structure of problems in a way that humans
cannot. The operational use of these algorithms solves various daily life problems and influences the
decision-making process in several critical areas. However, the use of ML algorithms is expected not
only to produce accurate predictions but also to provide insights into the predictive structure of the
data (Louppe, 2014).
For users (decision makers, analysts, experts, project stakeholders, etc.), the interpretability of ML
models is often as important as their predictive performance. When building an ML project, the team
needs to understand and trust the ML model, validate the knowledge it provides, and feel reassured
during its deployment. The model should be proven faithful and accurate through intelligible
explanations. The project team also needs to identify the model’s strengths and weaknesses to
monitor and consider them when applying ML decisions or leading analysis when problems occur.
Moreover, the aim of using ML models in scientific disciplines is often to discover new knowledge
that can help experts in the field better understand the phenomenon under study, and interact with
the parameters that influence it.
Furthermore, owing to the ethical, moral, or legal impact that an ML decision can produce on one’s
life, the ML model should argue its decisions through comprehensible explanations. In addition, ML
models are constrained in many regulated fields of application to be interpretable so as to provide
explanations regarding individual contestations (Goodman and Flaxman, 2017).
The topic of interpretability and explainability of ML models is increasingly gaining importance in the
academic community and becoming a crucial aspect for the public acceptance of ML models.
Interpretability conventionally refers to the degree to which a human can understand the model
decision structure. The terms “interpretability” and “explainability” are generally used synonymously
in the literature, although the ML community seems to prefer the former (Preece, 2018; Emmert-
Streib et al., 2020; Adadi and Berrada, 2018; Hakkoum et al., 2022). In the rest of this paper, we will
use the two terms interchangeably.
In practical application, an algorithm that does not provide, for nonexperts in ML, enough information
2A survey and taxonomy of methods interpreting random forest models
about the learner process and the learned model can be merely discarded in favor of less accurate
and more interpretable approaches (Haddouchi and Berrado, 2018). In the report published in
Kaggle’s 2021 edition of the State of Machine Learning and Data Science, the most commonly used
algorithms were, as in 2020, linear and logistic regression, followed closely by decision trees and
random forests. The linear regression algorithm is considered an interpretable algorithm because it
provides a clear explanation and a simple graphical representation of the mathematical basis of the
prediction structure. However, the linear regression model is generally biased and does not compete
in predictive performance with other more complex ML models. It is possible to improve its
performance by including polynomial, exponential, or other terms, but this model tweaking will be
done at the expense of interpretability. Decision trees are also well-known for their intelligibility in
that they mimic, in some sense, the natural reasoning of humans. The popular classification and
regression trees (CART) method (Breiman et al., 1884) greedily splits the descriptive variable space
into a number of simple and non-overlapping regions, ending up with a diagram describing the logic
behind the decision-making process. Each split is performed by comparing the gain in predictive
performance based on all the splitting possibilities over all the descriptive variables. This approach
obviously provides a simple interpretation of the model (if shallow tree) but generally suffers from
overfitting (thus predictive accuracy) compared to other supervised learning methods.
The random forest (RF) model (Breiman, 2001) overcomes the overfitting problem of the individual
decision trees by aggregating multiple deep decision trees on different bootstrapped samples and
using a random selection of descriptive variables at each splitting step. Thus, an RF ensemble can
be seen as a committee of decision-makers (trees) who make decisions through a consensus. Then
extracting and interpreting information shared by different points of view can lead to a solid
knowledge about the structure of the model. However, this task may seem difficult to achieve given
the number and depth of RF trees.
A key claim of this work is that the issue of interpreting the RF model is scientifically interesting. On
the one hand, the building process of the RF model is intelligible for human thinking compared to
other black-box models. On the other hand, the RF algorithm has proven its efficiency in many
practical problems. In one study (Fawagreh et al., 2014; Biau and Scornet, 2016), the authors
exposed and referenced several successful applications of RF models in ecology, medicine,
agriculture, astronomy, autopsy, bioinformatics and computational biology, chemoinformatics, traffic
3A survey and taxonomy of methods interpreting random forest models
and transport planning, and 3D object recognition, just to list a few. Also, the algorithm is user-
friendly since its tuning parameters are easily understandable (such as the number of trees, the
number of candidate variables for splitting, the number of leaf nodes, etc.). Its default
hyperparameters are generally sufficient to provide satisfactory results, which results in time-saving.
Moreover, the RF algorithm is versatile enough to deal with small sample sizes or be applied to high-
dimensional feature spaces (Biau and Scornet, 2016). It can handle big data via parallelization as
well (Chen et al., 2017).
The fact that the RF model is categorized as a black-box model (because of its number and depth of
trees) restricts its deployment in many fields of application. Hence, providing insight into this model’s
predictions becomes necessary for its applicability in the real world. Providing a firm basis from the
literature about the different works proposing methods to interpret the RF resulting model should
guide, in practice, the selection of the most useful tools for the interpretation and deep analysis of
the model results, with regard to the interpretability aspects sought. This should also be useful for
researchers who focus their studies on the interpretability of RF models or ML in general.
Different methods in the literature look inside the RF model. The increasing number of publications
on this topic produces extensive knowledge from various perspectives. The existing reviews on
explainable models examine several classes of methods (generally focusing on neural network
models). Two works are particularly interested in the review of methods interpreting RF models
(Haddouchi and Berrado, 2019; Aria et al., 2021). The two papers present a comprehensive
background of the literature on this topic, following a traditional (narrative) review. They essentially
analyze the methodological aspects based on the taxonomy proposed by Haddouchi and Berrado
(2019). In addition, the later paper (Aria et al., 2021) presents a comparative analysis between two
interpretative methods over different benchmarking datasets.
This paper, in contrast, provides an in-depth review of RF interpretation methods following a
systematic methodology. It rigorously reviews these methods, providing a bibliographic analysis of
the surveyed papers and a literature classification of the different techniques based on different axes
(stage of explanation, objective of interpretability, type of the problem solved, its input and output
formats, methodology used for providing explanations, and programming language used in the
implementation of the method). To achieve the aim of this study, we have set four main objectives:
• To study the evolution, during the last two decades, of the number of methods interpreting RF
4A survey and taxonomy of methods interpreting random forest models
models and to check whether there is any tendency or patterns that organize the reviewed
papers.
• To conduct a linguistic analysis of the most occurrent terms in the surveyed papers to bring out
information that can aid in the classification of the studied methods.
• To set up a classification grid that considers the different aspects that come into play when
choosing and exploiting an interpretative method.
• To establish a taxonomy of the methods reviewed and present the distribution of these methods
according to the different classification criteria.
• To analyze the usability of these methods in practical applications
The remainder of this paper is organized as follows. In Section 2, we describe the review
methodology followed. In Section 3, we present the bibliographic analysis of the reviewed papers. In
Section 4, we report the literature taxonomy and discuss the distribution of the surveyed methods
according to the different classification criteria. In Section 5, we discuss usability studies related to RF
interpretative methods. Finally, in Section 6, we present the conclusion of this work.
2. REVIEW METHODOLOGY
We conducted a thorough literature review by examining journal papers from five academic
databases publishing works in the computer science domain: ScienceDirect, SCOPUS, ACM Digital
Library, IEEExplore, and the dblp computer science bibliography, in addition to well-cited preprints
published on the arXiv database. We have used a keyword-based search to select relevant papers. It
consisted of searching for the association of the “interpret*” or “explain*” terms with the set of terms
“random forest”. The search was done within the title, abstract, and keywords search fields (except
for the dblp database, where the search was done within the text field). The literature search was
refined by limiting the literature search’s timeframe to the period from 2001 to 2021 and the language
to “English”. In addition, the search was narrowed to the computer science domain in each of the
SCOPUS and arXiv databases. The extraction of the results was done between 16/02/2022 and
18/02/2022.
At first, a total of 974 papers were selected. After removing redundant papers, a total of 751 papers
were screened based on the titles and abstracts to determine relevant articles for further analysis. 73
papers were then considered for text screening and verification of the inclusion and exclusion
criteria. We considered only papers that propose original methods interpreting the RF model that can
5A survey and taxonomy of methods interpreting random forest models
be applied to large purposes. 10 papers applying existing interpretative methods to specific issues
and 4 papers reporting methods tailored to specific problems were excluded. 7 other papers were
excluded for being oriented toward prediction improvement, feature selection, or missing values
processing. Finally, 3 papers were removed because they were based on a modified version of RF, 4
papers were not considered because they do not include an illustration of the interpretable
explanations for RF models, and 2 papers were excluded for being reduced versions of other
considered papers. A list of 43 selected papers was then completed with 16 additional papers based
on exploring the reference and citation lists of the selected papers (snowballing). These 16 papers
included 11 journal papers and 5 reports (approved by a scientific committee).
The final list included 59 papers. Figure 1 shows a step-by-step flow diagram of this selection
process. We provide in the following section a bibliographic analysis and a literature classification
based on these 59 papers.
Figure 1: Flowchart outlining the selection process of the review of methods interpreting random
forest models
6A survey and taxonomy of methods interpreting random forest models
3. BIBLIOGRAPHIC ANALYSIS
Figure 2 reports the evolution of the number of publications per year. It shows an increasing interest
in the research community on the topic of interpreting/explaining RF models, especially in the last 5
years. A similar trend was reported by Adadi and Berrada (2018) regarding XAI in general. The
rising interest in the topic of interpretability/explainability in ML is the consequence of the increasing
improvement of the ML model’s efficiency in solving real-world issues that affect the decision-
making process in various critical areas; without being able to provide sufficient information about
the key parameters that explain their results (Adadi and Berrada, 2018). However, the more critical
an ML model decision is, the more important it is for the model to explain its results. It is therefore
foreseeable that the development of more interpretable models will be as important as the
development of more accurate models over the next few years. Figure 3 reports the number of
citations per paper, clustered based on the paper categories: peer-reviewed journal article, preprint,
or report. The citation counts were extracted from Google Scholar on June 19, 2022. The most cited
reference is the paper of Liaw and Wiener (2007) (Corpus ID: 3093707 in Semantic Scholar), which
introduced two extra pieces of information produced by the RF model: the measure of variable
importance and the RF proximity measure. Figure 3 also shows that the highly cited papers are
published as preprints and that there was a rising interest in publishing methods interpreting the RF
model during the last 3 years.
Figure 2: Evolution of the number of publications interpreting random forest models
per year
7A survey and taxonomy of methods interpreting random forest models
Figure 3: Number of citations per paper, clustered based on the paper categories (papers
interpreting random forest models)
Figure 4 provides a literature map of authors where the nodes represent the papers and the edges
represent their citation relationships (the map was plotted using the Litmaps1 research platform). The
nodes are sized by citation count. This map shows how the papers relate to one another in an
ordered timeline (papers published earlier on the left and the latest papers on the right). The most
connected papers are, as expected, highly cited papers such as that of Liaw and Wiener (2007). The
connection network is globally dense, especially since 2014. Also, we do not notice any apparent
clustering in this network. This finding suggests that there is no confirmed direction for this research
topic and that most papers considered various previous techniques proposed in the literature.
We performed a linguistic search based on titles and abstracts to discover relevant terms related to
RF interpretability. This analysis aims to identify some main concepts that could help classify the
existing literature. We provide in Figure 5 a term co-occurrence map based on title and abstract data
in the surveyed papers. This map was produced using the VOSviewer2 software, which scanned the
surveyed publications’ text data and allowed filtering of irrelevant terms. We have filtered general
terms such as “method”, “approach”, “new”, and “result” and technical terms such as the names of
the cited algorithms, including “deep learning”, “decision tree”, “intrees”, etc.
1 https://www.litmaps.com/
2 https://www.vosviewer.com//
8A survey and taxonomy of methods interpreting random forest models
Figure 4: Literature map of authors where the nodes represent the papers and the edges
represent their citation relationships (for papers interpreting random forest models)
In Figure 5, the nodes represent the relevant terms, and the edges represent their co-occurrence
relationship. The nodes are sized by occurrence count. VOSviewer uses modularity-based clustering
to identify sets of closely related terms. A detailed description of VOSviewer processing can be found
in the paper published by van Eck and Waltman (2010). This map highlights important concepts
related to RF interpretability, such as global and local explanation, and surrogate models. It also
clusters the methods into different classes of methods, such as rule-based methods, methods
producing sub forests, methods based on features’ importance, methods based on the similarity
among instances, and methods based on interactive visualization. In the following, we propose a
literature classification of the surveyed papers based on the concepts above and other additional
features. We classified these methods according to seven axes (stage, objective, methodology, issue
type, input, output, and code language) to present a comprehensive and holistic analysis of the RF
interpretability literature.
9A survey and taxonomy of methods interpreting random forest models
Figure 5: Term co-occurrence map based on title and abstract data in the surveyed papers (for
papers interpreting random forest models)
4. LITERATURE CLASSIFICATION
According to the XAI reviews provided by Vilone and Longo (2020) and Islam et al. (2022), XAI
methods can be classified based on the stage of explanation, the type of the problem solved, and
its input and output formats. Considering the surveyed articles reported in Section 3 and inspired by
the term co-occurrence map (Figure 5), we have added three additional criteria: the objective of
interpretability, the methodology used for providing the explanation, and the implementation
language used in the implementation of the method. Table 1 lists the methods used in this
classification. The name, abbreviation, and reference are provided for each one. The classification
details about each method are not provided in this paper because of space restrictions. Instead,
readers can consult the GitHub repository reserved for this study, for further information (see
Section Declarations).
Figure 6 summarizes the distribution of the surveyed methods over the classification criteria. A
detailed analysis of this distribution is provided in the following sections.
10A survey and taxonomy of methods interpreting random forest models
Table 1 List of the selected methods for classification
Name
Name
Ref. Name Name abrev. Ref.
abrev.
Explaining
Classifications for explainVis
(Robnik-Sikonja and Tree-Ensemble Representer-
TREX
(Brophy and Lowd,
Kononenko, 2008) Point Explanations 2020)
Individual Instances
Combined Rule (Smith and
Extraction and Feature CRF (Liu et al., 2012) Not mentioned smit21 Alvarez,
Elimination 2021)
Not mentioned joha12
(Johansson et al., Discretized Interpretable
RF-DIMLP (Bologna, 2021)
2012) Multi-Layer Perceptron
Not mentioned webb14 (Webb et al., 2014) Not mentioned blan21 (Blanchart, 2021)
Conclusive Local
Feature Contributions for
rfFC
(Palczewska et al.,
Interpretation Rules for LionForests
(Mollas et al.,
RF Classification 2014) 2021)
Random Forests
Consistent Sufficient
Not mentioned liu14 (Liu et al., 2014) Explanations and Minimal ACV
(Amoukou and
Brunel, 2021)
Local Rules
(Utkin and
Self-Organising Maps SOM
(Płonski and Ensembles of Random
ER-SHAP Konstantinov,
Zaremba, 2014) SHAPs
2021)
Active Learning-Based
Pedagogical Rule ALPA
(Junque De Fortuny
Explainable Matrix ExMatrix
(M. P. Neto and F.
and Martens, 2015) V. Paulovich, 2021)
Extraction
Local Outlier Factor LOFB-DRF
(Fawagreh et al.,
Not mentioned verd21
(Verdinelli and
2015) Wasserman, 2021)
(Izza and
Not mentioned paul15
(Paul and Dupont, Random Forests
RFxpl Marques-Silva,
2015) Explanations
2021)
Local Interpretable
Model-Agnostic LIME
(Ribeiro et al., Partial Dependence through
StratPD
(Parr and Wilson,
2016) Stratification 2021)
Explanations
Shapely Values
SVC (Hur et al., 2017)
Random Forest Similarity
RFMap
(Mazumdar et al.,
Contribution Map 2021)
Model Extraction
Modelextra
(Bastani et al., 2017)
Visual Analytics for
VERONICA
(Rostamzadeh et
ction Identifying Feature Groups al., 2021)
Intervention in Prediction
IPM (Epifanio, 2017)
Knowledge Discovery from
ForEx
(Adnan and Islam,
Measure Decision Forests 2017)
SHapley Additive
SHAP
(Lundberg et al., Variable Importance/ Partial VIP+PDP+P (Liaw and Wiener,
exPlanation 2018) Dependency /Proximy Matrix M 2007)
Model Agnostic
Supervised Local MAPLE (Plumb et al., 2019)
Interactive Random Forests
irfplot (Quach, 2012)
Plots
Explanations
SHapley Additive
ADD-Lib (Gossen et al., 2019)
Exploring a Random Forest ggRandomFo
(Ehrlinger, 2015)
exPlanation for Regression rests
Tree Explainer
TreeExplai (Lundberg et al., Exploratory Data Analysis
edarf
(Jones and Linder,
ner 2019) using Random Forests 2016)
Interpreting Random
iForest (Zhao et al., 2019)
Interactive Visualization for
Rfviz (Beckett, 2018)
Forests Random Forests
Interpretable Trees inTrees (Deng, 2019) Random Forest Explainer
randomFores
(Paluszy, 2017)
tExplainer
Single Sample Feature
SSFI (Gatto et al., 2019) Tree Space Prototypes SG and SM (Tan et al., 2020)
Importance
Surrogate Minimal Depth SMD (Seifert et al., 2019)
Relational Interpretable
RISM
(Van Assche and
Single Model Single , 2008)
Random Forest-based
IRFRE (Wang et al., 2020) Sub-Forests sub-forests
(Wang and Zhang,
Rule Extraction 2009)
Explainer Explainer (Kopp et al., 2020) Forest Floor forestFloor
(Welling et al.,
2016)
Collection of High
Importance Random CHIRPS (Hatwell et al., 2020)
Local Rule-Based
LORE
(Guidotti et al.,
Explanations 2018)
Path Snippets
Rule Extraction Based on
Improved Conditional
permimp
(Debeer and Strobl,
Heuristic Search and Sparse
RF+DHC/SG (Mashayekhi and
Permutation Importance 2020) L/MSGL Gras, 2017)
Group Lasso Methods
Forest Interpretable Tree
forest_bas (Sagi and Rokach,
Not mentioned FAB
(Hara and Hayashi,
ed_tree 2020) 2017)
Explainable AI for Trees
rfVarImpO
(Loecher, 2020)
Optimal Explanations for
OptExplain
(Zhang et al.,
OB Ensemble 2021)
Random Forest Optimal
Counterfactual Set RF-OCSE
(Fernandez et al., Predictive Learning via Rule
RuleFit
(Friedman and
2020) Ensembles Popescu, 2008)
Extractor
Rule Covering for
Interpretation and MIRCO (Birbil et al., 2020)
Boosting
11A survey and taxonomy of methods interpreting random forest models
Figure 6: Distribution of the surveyed methods over the classification criteria (methods
interpreting random forest models)
4.1 Taxonomy and distribution analysis of the surveyed papers
4.1.1 Stage
The stage of explainability designs the time at which the explanation is processed with regard to the
processing period of the RF algorithm. The stage can be ante hoc or post hoc (Vilone and Longo,
2020). Around 97% of the surveyed papers propose post hoc methods.
Ante hoc methods consider generating the explanation using the internal processing of the RF model.
These explanations concern the basic extra information learned using RF processing, namely,
variable importance, partial dependency, and the proximity matrix (Breiman, 2002). Variable
importance plots (VIPs) inform about the statistical significance of the effect of each descriptive
variable on the resulting prediction (Breiman, 2001). VIPs help users identify which features are
important for predictions. Partial dependency plots (PDPs) report the marginal effect of a descriptive
variable or multiple variables on the output target (Friedman, 2000). They help users explore how
predictions respond to input variables’ variations. The VIP and PDP aim to depict the relative influence
12A survey and taxonomy of methods interpreting random forest models
of each variable (or small group of variables) on the predictions but do not consider existing variable
interactions and can be biased if the variables are correlated. The proximity matrix (PM) (Liaw and
Wiener, 2007) computes the proportion of times every two observations end up in the same terminal
node across all the terminal nodes of the RF model. The idea behind this measure is that similar
observations should be found more often in the same terminal nodes than in different nodes. The PM
can be plotted using multi-dimensional scaling methods to help users discover data clusters and
outliers. However, proximity plots often appear similar or irrespective of the data, raising uncertainty
about their usefulness (Hastie et al., 2009).
Post hoc methods generate explanations externally to the RF model. These methods try to mimic the
RF predictions to provide an intelligible explanation to users and decision-makers. Post hoc methods
are divided into two subcategories: model-agnostic and model-specific. Model-agnostic methods treat
the original model as a black box and are designed to be generally applicable (Angelov et al., 2021;
Dieber and Kirrane, 2020). In contrast, model-specific methods take advantage of the intrinsic
architecture of the original model (Angelov et al., 2021) and can only be applied to a particular type of
one or several models. In this survey, we consider an intrinsic model any model that can only be
applied to RF or RF and decision trees in general. Only 13% of the surveyed post hoc methods are
model-agnostic. Other agnostic methods can probably be applied to RF models. However, the key
search used in this survey has limited the results to papers including the term “random forest”. For
more examples related to agnostic models, we suggest, for instance, the “Interpretable Machine
Learning” book (Molnar, 2022), which describes several agnostic techniques along with application
examples, advantages, disadvantages, and implementation references.
4.1.2 Objective
The objective of interpretability qualifies the extent of the perception provided through explanation.
The explanation can enable a global overview, a local explanation, a pattern discovery, or a hybrid
objective.
The global overview provides insights into the general behavior of the ML model, such as the most
important variables influencing predictions. A global overview that respects human cognitive
limitations is the first step to understanding the essential aspects learned by the model. The local
explanation focuses on arguing each specific prediction and consists of approximating the original
model in a small area around an instance of interest (Angelov et al., 2021). Explaining a specific
13A survey and taxonomy of methods interpreting random forest models
prediction is challenging, especially if the prediction is subject to moral or legal constraints. The
pattern discovery objective aims to unveil patterns and prototypes in the data. For instance, let us
consider that we want to predict churn in the telecommunication domain. It would be interesting to
discover the prototypes of predicted churners and explain their behaviors to plan focused marketing
campaigns. Finally, a hybrid objective combines two or three of the objectives above.
Based on Figure 7, displaying the distribution of the surveyed methods according to the objective
south through interpretability, we can see a total overlap of 55%. This result shows that a method can
have more than one objective. For instance, rule extraction methods, when model-specific, can
enable global overview, local explanation, and pattern discovery. Generating a simple set of rules that
approximate the RF model can provide concise information about patterns and prototypes in the data,
important variables, and their relationships. For example, in the case of predicting a particular disease
(based on information from historical patient data), experts would appreciate discovering a set of rules
that explain the different conditions inducing the disease. They would have a global overview of the
different prototypes in the data and explain each new prediction depending on the prototype to which
it belongs (each rule or reduced set of rules would represent a prototype of patients). They would also
be able to understand and analyze each set of conditions and design an adequate treatment
accordingly. Based on this analysis, they could continuously improve the predictive model, for
example, by adjusting RF hyperparameters or enriching feature inputs.
Figure 7: Distribution of the surveyed methods according to the objective south through the
interpretability of random forest models
14A survey and taxonomy of methods interpreting random forest models
4.1.3 Issue solved and input/output formats
Based on the 59 selected papers in this review, the issue requiring interpretability is either a
regression or a classification problem (binary or multiclass classification). The inputs are
continuous, categorical, or mixed data, and the outputs are texts, statistics, or visual graphs. Text
outputs concern rule-based explanation, while statistics discuss data distribution, rules metrics, or
any numerical information providing insights about the behavior of the RF model. Visual graphs
include any plot-based output providing visual explanations, such as scatter plots, box plots, or
more sophisticated plots.
Figure 8 illustrates the data flow of the methods interpreting RF, from inputs to outputs passing
through the solved issue. The link between two successive nodes is drawn with a width proportional
to the flow quantity between the two nodes. The visualization of the different flows does not reveal
any evident pattern among inputs, outputs, and solved issues. As shown in this plot, 90% of the
surveyed methods process input features that can be either categorical or continuous.
However, several methods use discretization as a preprocessing step to handle continuous
variables, such as in (Robnik-Sˇikonja and Kononenko, 2008), (Ribeiro et al., 2016), (Deng, 2019),
and (Debeer and Strobl, 2020). Also, there are more choices for methods interpreting RF
classification models than methods interpreting RF regression models. About 88% of the methods
can be applied to classification issues, whereas 44% can process regression (by considering hybrid
methods). Finally, the outputs’ format is distributed in an almost equitable way among the different
classes of outputs.
Figure 8: Data flow of the surveyed methods interpreting random forest models, from inputs to
outputs passing through the solved issue
15A survey and taxonomy of methods interpreting random forest models
4.1.4 Implementation
The implementation of the proposed methods in the selected papers is mostly done using the R and
Python programming languages. Around 48% of the published implementation used Python, 39%
used R, and 8% used Java. The remaining 5% used Matlab, the Pyro web application, and the ACE-
hipP system. Figure 9 illustrates the evolution of code implementation from 2007 to 2021. It shows
that Python was the programming language of choice for developers during the last 5 years.
Figure 9: Distribution of code implementation of methods interpreting random forest models
4.1.5 Methodology
In this section, we classify the selected surveyed approaches based on the methodology used to
interpret the RF model. We identified the five following methodologies:
• Size reduction: reduces the number of trees in the forest to produce a simpler model.
• Rule extraction: extracts a representative set of rules for approximating the RF model.
• Features oriented: explains how changing the values of the descriptive features affects the predictions.
• Sample similarity-based: looks at the similarity among instances to explain predictions.
• Template/web interface: framework or web interface developed to facilitate the exploration and
explanation of the RF model.
• Hybrid methods: can combine more than one method from the methodologies above.
Figure 10 reports the distribution of the surveyed methods according to the methodology used and the
objective of interpretation. This figure shows that almost 40% of the methods use a rule extraction
methodology. Around 60% of them allow a global overview of the model, a local explanation of the
predictions, and the discovery of patterns in the data. Feature-oriented methods represent about 30%
of the studied approaches and are most often used for a global overview. Around 14% of the
surveyed methods propose templates or web interfaces that enable a global overview or the
16A survey and taxonomy of methods interpreting random forest models
discovery of patterns, while 7% of the methods use a sample similarity-based methodology and are
generally focused on pattern discovery. The remaining approaches (less than 10%) use a size
reduction or hybrid methodology. The following section describes in more details the methodologies
introduced in this section.
Figure 10: Distribution of the surveyed methods according to the methodology used and the objective
of interpretation
4.2 Review of the surveyed papers according to the interpretative methodologies used
We present in this section a detailed review of the methods interpreting RF according to the
methodologies introduced in Section 4.1.5.
4.2.1 Size reduction
Size reduction methods aim to build a reduced subset of decision trees that is competitive with a large
RF model in its predictive performance. Latinne et al. (2001) reduced the number of trees using the
McNemar test of significance on the tree predictions. Bernard et al. (2008) used a sub-optimal
classifier selection method to obtain a more performant subset of decision trees. Zhang et al. (2021)
considered three measures to assess the prediction performance of the trees and to form a sub-forest
that achieves better accuracy than the large RF model. Yang et al. (2012) used a margin optimization-
based pruning approach to reduce the RF size while improving its performance. Khan et al. (2016)
formed an optimal RF sub-ensemble by considering the individual performance of the trees (based on
prediction errors) as well as their collective performance (using the Brier score). Finally, Van Assche
and Blockeel (2008) used the class distribution estimated from the different decision trees to build a
single decision tree that approximates the decisions made by the initial tree ensemble.
The approaches above generally achieve a predictive performance comparable to RF performance. In
17A survey and taxonomy of methods interpreting random forest models
addition, reducing the size of the forest would make it easier to explore the tree paths. They, however,
could remain black boxes depending on the number and the depth of their tree members.
4.2.2 Rule extraction
This class of methods builds a rule ensemble to approximate the RF model predictions while
considering the complexity of the rule ensemble. This complexity is generally expressed in terms of
the number of rules, their lengths, and their coverage. Rule extraction methodology in model-specific
methods consists of extracting rules from the RF trees and reducing their number considering the
trade-off between their complexity and predictive performance.
In most papers, the authors expressed a rule as the intersection of the conditions defining a path from
a root node to a leaf node in a tree. Different methods were used to tackle the trade-off between
predictive performance and complexity. Deng (2019) used a complexity-guided condition selection
method to select a compact set of rules. Phung et al. (2015) proposed a greedy approach to generate
a set of ranked and weighted rules. Mashayekhi and Gras (2017) applied a hill-climbing technique to
produce a rule ensemble significantly reduced in size. Hara and Hayashi (2017) used a bayesian
model selection algorithm that optimizes the rule ensemble complexity while maintaining the
prediction performance. Adnan and Islam (2017) considered different metrics to select a high-quality
rule ensemble. Be´nard et al. (2020) selected the most relevant rules from an adapted version of the
RF model based on their probability of occurrence. Birbil et al. (2020) applied a mathematical
programming approach to minimize the number of selected rules and their total impurity. Zhang et al.
(2021) based their selection of rules on logical reasoning, sampling, and optimization.
Otherwise, some authors looked at the other possible paths in the trees (not only those from the root
to the leaf nodes). Friedman and Popescu (2008), Meinshausen (2010), and Mashayekhi and Gras
(2017) selected from the RF nodes the most important rules predicting the outcomes. In the first
paper, the authors selected the set of rules based on averaging a weighted rule list. In the second
paper, the authors applied a regularized linear regression, and in the third paper, the authors used a
heuristic search method and a sparse group lasso method.
In model agnostic methods, the rule extraction methodology consists of constructing a tree or a rule
list that mimics the relationship between the inputs and outputs of the RF model, such as in
(Johansson et al., 2012) and (Ribeiro et al., 2016).
The if-then semantic of the rules makes the outputs of the rule extraction methods probably the most
18A survey and taxonomy of methods interpreting random forest models
comprehensible for natural human thinking, provided that the length and the number of rules are
acceptable.
4.2.3 Features oriented
This class of methods provides explanations based on feature relevance. We can break down this
class of methods into 3 sub-categories: VIPs, Dependency plots (DPs), and local decomposition
techniques.
The VIP, constructed for RF, shows the importance of the contribution of each descriptive variable on
the model prediction performance (Breiman, 2001). Two basic ways are used to measure variables’
importance for RF models (Hastie et al., 2009). The first technique measures how much the accuracy
decreases (on average) on the out-of-bag samples when each variable is randomly permuted. Debeer
and Strobl (2020) and Loecher (2020) developed revisited versions of this technique to increase the
interpretability and stability of the outputs and decrease the required execution time. The second
approach reports the accumulated improvement in the split criterion (using measures such as the Gini
index or entropy) attributed to each splitting variable (Hastie et al., 2009). In recent works, alternative
methods were proposed to measure the importance of the variables on the predictive performance.
For Hur et al. (2017), Utkin and Konstantinov (2021), and Lundberg et al. (2018), the importance of
the features was assessed using the Shapley feature importance measure from game theory, which
distributes the model performance among the variables according to their marginal contribution.
Vilone and Longo (2020) and Epifanio (2017) proposed a different perspective called the intervention
in prediction measure (IPM) to assess variable importance for RFs. IPM computation does not require
knowing the prediction performance; it only exploits the structure of the trees forming the RF model.
The IPM is a case-wise technique. For each instance, the percentage of times a variable is used in
the prediction of the instance is calculated (for each tree). Then, the IPM is averaged for each class in
the case of RF classification or globally. Conversely, Ishwaran et al. (2010) and Seifert et al. (2019)
proposed another way to assess the importance of the features. The minimal depth (MD) variable
importance score of a variable A is defined as the average level of the first split based on the variable
A across all the RF trees with at least one split based on the variable A. The surrogate minimal depth
(SMD) variable importance is an extension of MD variable importance. It applies the definition of MD
not only according to the first appearance in RF splits. It computes surrogate variables for RF and
applies the definition of MD to RF primary splits and surrogate splits.
19A survey and taxonomy of methods interpreting random forest models
DPs offer another perspective for analyzing the influence of the descriptive variables on the model
predictions. PDPs inform users about the global relationship between a feature (or a set of features)
and the predicted outcomes. PDPs show the marginal effect of an input variable (or a set of input
variables) on the prediction of the fitted model (Friedman, 2000). Parr and Wilson (2021) proposed a
revisited approach that computes partial dependencies directly from the training data without relying
on the predictions of the fitted model. Their work is motivated by the claim that the same partial
dependency algorithm can provide different shapes for different supervised models, which makes it
difficult to differentiate between model artifacts and true relationships in the data.
The key disadvantage of PDPs is that they only capture the main effect of the features and ignore
possible feature correlations. SHAP DPs (Lundberg et al., 2018) are an alternative solution to the
traditional PDPs that better capture interaction effects among variables.
The local decomposition of a prediction by feature relevance is another useful way used to inspect
specific predictions. The feature contribution (Palczewska et al., 2014) and forest floor (Welling et al.,
2016) methods quantify the contribution of each descriptive variable in each instance prediction, for
random forests. Forest floor also provides graphical visuals of the prediction decomposition in a 2D-
3D features space, which enables exploring the fitted model structure in a 2D or 3D space.
4.2.4 Sample similarity-based
Sample similarity-based methods look at the similarity among instances when providing explanations.
The proximity matrix inherent to the RF model reports the proportion of time; every two observations
end up in the same terminal nodes across all the terminal nodes of the RF trees. The motivation
behind this measure is that similar observations should belong to the same terminal nodes more often
than dissimilar ones. The proximity matrix can be projected into a 2D space using multi-dimensional
scaling (MDS) methods to identify data clusters and outliers (Liaw and Wiener, 2007). Płon´ski and
Zaremba (2014) presented an alternative method that builds supervised self-organizing maps (SOMs)
based on the RF proximity matrix (for classification). Their experiments revealed that visualizing the
RF proximity matrix with a SOM offers a better understanding of the relationships in the data than
MDS visualization. In addition, the SOM learned using the RF proximity matrix achieved better
predictive performance than a SOM learned with Euclidean distances. In the same vein, Tan et al.
(2020) introduced an adaptive prototype selection method to reveal prototypes according to a
distance function derived from the RF proximity. On the other hand, Lundberg et al. (2018) proposed
20A survey and taxonomy of methods interpreting random forest models
to find clusters in the data based on a supervised clustering of the SHAP feature attributions, which
means that the instances are clustered based on explanation similarity.
While the methods above aim for a global overview, other methods consider instance similarities for a
local explanation. According to Ribeiro et al. (2016), the local interpretable model-agnostic
explanations (LIME) model aims to locally approximate a black box-model by an interpretable model
(linear regression as an example). First, LIME creates synthetic samples in the neighborhood of the
instance to explain. Next, it computes a similarity index between this instance and the sampled
instances. It then fits the original and interpretable models to these samples. Lastly, LIME minimizes a
loss function (which measures the loss in prediction between the original model and an interpretable
model) weighted by the proximity to the instance being explained, plus a complexity term (which
measures the complexity of the interpretable model). The best interpretable model is the solution to
this optimization problem.
On the other hand, Iforest (Zhao et al., 2019), a model-specific method, explores the similarities
among the decision paths that generate the same predictions in the RF trees. It uses the t-distributed
stochastic neighbor embedding (t-SNE) technique to project all the decision paths onto a 2D space so
that users can easily visualize and examine the properties of path clusters that induced the decision
of a specific input instance. According to Brophy and Lowd (2020), the tree-ensemble representer-
point explanations (TREX) method uses the structure of the trees to build a tree ensemble kernel,
which acts as a similarity measure among instances. By using this kernel in a surrogate model that
approximates the original tree ensemble, TREX computes the global or local importance of each
training example in the prediction of each specific instance, resulting in an instance-attribution
explanation of positive and negative training points. Finally, Blanchart (2021) proposed to characterize
the model into pure decision regions (regions over which the model makes a constant prediction over
all classes) under the form of a collection of multi-dimensional intervals. Using counterfactual (CF)
reasoning, they determine the closest CF example associated with a specified instance and the
geometrical characterization of its decision region. The CF explanation describes the slightest change
to the feature values that changes the prediction value to another predefined value.
4.2.5 Template/web interface
This class of approaches covers methods that provide web interfaces or templates ready to apply to
21A survey and taxonomy of methods interpreting random forest models
interpret RF predictions based on existing published approaches. Such tools should help researchers
easily apply/monitor the implemented methodologies to their research problems. Many researchers
provided interactive visualization tools based on the existing methodologies, allowing the users to
manipulate the visible results via input devices such as a keyboard or mouse.
The randomForestExplainer package (Paluszy, 2017) computes and plots various statistics
measuring the importance of variables and their interactions in RF models. Although the package
uses existing methodologies (except for some simple extensions), it provides various graphical
capabilities that facilitate visualizing and interpreting the results. The ggRandomForests (Ehrlinger,
2015) package is also devoted to visually understanding RF models. It allows for examining variable
importance and minimal depth measures. It also enables investigating variable associations with the
variable dependencies plots or using minimal depth interaction plots. Along the way, the package
offers commands for modifying and customizing results. Similarly, the edarf (Exploratory Data
Analysis using Random Forests) (Jones and Linder, 2016) package contains useful functions for
computing and visualizing the partial dependencies of features, the permutation importance of
covariates, and the proximity between data points according to the fitted model. In the same vein,
Smith and Alvarez (2021) published a template for computing and visualizing Shapely values for
binary classification problems.
Alternatively, Quach (2012) developed the irfplot package (interactive Random Forest Plot) and
combined it with the iPlots eXtreme tool (Urbanek, 2011) to offer an interactive visualization of parallel
variable coordinates, variable importance, and proximities plots. Rfviz is another sophisticated
interactive visualization tool designed for interpreting RF predictions in a user-friendly way. The Rfviz
tool allows visualizing and interacting with the parallel coordinate plot of the descriptive variables, the
parallel coordinate plot of the variable importance scores, and the rotational scatterplot of the
proximities.
Some tools are devoted to specific domains of applications. Intended for processing electronic health
records (EHRs), the VERONICA (Rostamzadeh et al., 2021) system incorporates a visual analytic
module that utilizes the natural classification of features in EHRs to identify the group of features with
the strongest predictive power, integrating different sampling strategies, supervised ML techniques
(including RF), analytics algorithms, visualization techniques, and human-data interaction.
While the tools above generally employ already published methodologies, their main advantage is
22A survey and taxonomy of methods interpreting random forest models
that they help researchers and users easily and effectively apply/monitor the implemented
methodologies to their research problems.
Figure 11 classifies the different methods listed in Table 1 according to the objective of interpretability,
the methodology used, and the issue solved. The detailed taxonomy of the 59 studied methods can
be found on GitHub (see Section Declarations). This figure offers to users a visual tool for the
identification/selection of the appropriate interpretative methods given the objective of interpretability,
the methodology used, and the nature of the issue under study.
class
egression
classM ulticlass
Binary F
Hybrid class
WebTe me pSe la ait tzu er e /r s eS a dm uple cF t i o o nsir mi ile arn it ted Re a t Bur ines a ry Hybrid
oriented
Multiclass interface yoriented R
ule
-based+
Multiclass
+Global Extr
R e gression + LP ocalatternoverview action
erview disco
explanation
Binary class Fe atures ov ery v
Hybrid Classification
Size
M H Mu y Bl bt uini rc lai tl d c a
r
iyl cs a s ls
a
sr T see inm sd tW e pu
R
urelc fa lb a et t cei o e/ n G L Mlo o ucb lta ia el c ll xa ps ls
a
nas m tuo ioerf nv t t h eh oye de d s Glob va d PelP i s ra vevct ro + it eye
wa
vr Sen atmtr pey
ler n
os
ri
i eS m nb ta ei a dlm +a s sr ie mp i bt id all y
a
se
r
e
i-
dt
y-
FeaB turi en sary c lass
Global o+e dm iscp ol ate/interface Hybrid
overview Regression
Extrt i
Figure 11: Classification of the surveyed methods according to the objective of interpretability, the
methodology used, and the issue solved
23A survey and taxonomy of methods interpreting random forest models
In practice, it is recommended to experiment with various interpretative methods, which should
ideally lead to similar or complementary conclusions. Moreover, the provided explanations should be
adapted to users' cognitive capacities and preferences (Haddouchi and Berrado, 2018; Kovalerchuk
et al., 2020; Lahav et al., 2019). In an ML project applied to a specialized domain, explanations may
differ depending on the user trying to understand the predictive model (developer, data analyst,
technical user, domain specialist, lay users, etc.).
The usability of the provided explanations should be validated before deploying an RF model.
Usability experiments should verify that the presented RF model is accepted for deployment based
on the diverse explanations examined by the different users (including the team project and the end-
users). Alternatively, it may occur that a user rejects the model due to misleading explanations
(Lipton, 2018). Such deficient explanations, also called quasi-explanations (Kovalerchuk et al.,
2020), are generally a result of using concepts or terms foreign to the user domain or scientific
background.
We investigated the inclusion of usability studies in the surveyed papers. We provide in the next
section the obtained results. We also present the expected advantages and disadvantages of the
different methodologies used to interpret RF based on the usability studies reported in the surveyed
papers and several other papers tackling the usability issue.
5. USABILITY STUDIES
5.1 Importance of usability studies
It is important to remember that ML interpretation is not a machine activity; it fundamentally involves
human perception and interaction (Kovalerchuk et al., 2020). As related by Lahav et al. (2019) and
Poursabzi-Sangdeh et al. (2021), many ML interpretability studies focus on developing methods
producing explanations that are assumed to be easily understandable to humans. Through
experimental studies, these works revealed that there is a divergence between ML interpretability
designers' expectations and the practical results in real-world applications.
In the same vein, Kovalerchuk et al. (2020) provided many examples of explanations that can be
easily rejected by users without a background in ML. Explanations based on weights, distances,
layers, or deep trees could be meaningless in real-world applications. For instance, a weighted
summation of the blood pressure and temperature is foreign to the medical language. Also, a deep
24A survey and taxonomy of methods interpreting random forest models
tree or a large ordered rule list could be difficult to trace and summarize by a clinician, especially if
they use non-conventional terms or split values (Huysmans et al., 2011).
Moreover, some visualizations that are assumed to facilitate understandability and communication
can instead decrease the usability of the explanations to end-users. Experiments with users not
related to the field of data science showed that there is no evidence that visualizations increase the
level of perceived ML model usability compared to the absence of visualizations (Haas, 2021).
Thus, designing meaningful explanations for users is challenging. It should consider users'
backgrounds, knowledge, and preferences, which can vary considerably (Nourani et al., 2018; Ali et
al., 2023; Molnar, 2022).
It is, therefore, necessary to validate the designer's assumptions about the usability of an
interpretative model based on interactive experiments with users. The designer should verify that
interpretations are acceptable by the different users, leading to the deployment of the model in
practice (Lahav et al., 2019).
It also may be interesting to consider dynamic formats of explanations based on users' experimental
feedback (Nourani et al., 2018) because although the main objective of users' experiments is to
produce meaningful explanations, different sub-objectives should be defined in consideration of the
different classes of users and applications (Ali et al., 2023).
Limited work on RF interpretative methods has reported usability studies. Out of the 59 surveyed
works, only eight papers report usability experiments. Their authors proposed different testing
protocols based on subjective and objective measures for quantifying the understandability and utility
of the provided explanations. The experiment participants were either human subjects on Amazon
Mechanical Turk without a background in ML or users familiar with the ML field.
5.2 Usability studies related to RF interpretative methods
Zhao et al. (2019), Neto and Paulovich (2021), Mazumdar et al. (2021), and Bastani et al. (2017)
performed user studies over respectively 10, 13, 15, and 46 participants with a background in ML.
Their objective was to verify that their model explanations can help technical users, such as data
scientists, understand and validate the predictions of an RF model.
Alternatively, Lundberg et al. (2019), Lundberg et al. (2018), and Tan et al. (2020) aimed to validate
25A survey and taxonomy of methods interpreting random forest models
through user studies, including respectively, 33, 34, and 42 Amazon Mechanical Turk without a
background in ML, that their methods allow the best agreement with human intuition in predicting
outcomes compared to other existing methods.
Finally, Ribeiro et al. (2016) evaluated their agnostic method with both technical and non-technical
human subjects, by setting three main objectives. The authors recruited a hundred of Amazon
Mechanical Turk without a background in ML to check the two first objectives. They first verified that
the provided explanations can help users decide from a set of classifiers, which classifier generalizes
better. They also verified that their explanations can help non-expert users improve the predictive
model by modifying the set of attributes used in the model. The third objective required users familiar
with the ML domain (27 graduate students with knowledge in ML). The authors aimed to demonstrate
that explaining individual predictions helps users know when and why the model will likely fail in
predicting outputs. It should be noted that the experiments with real users in (Ribeiro et al., 2016),
were conducted using a set of classifiers that, unfortunately did not include the RF model. Since the
experimented model is an agnostic model (that could be applied to any classifier), the conclusions are
assumed to be generalizable to other classifiers. However, The findings should be verified for other
classifiers and real-world applications.
Based on the different usability works reported in this paper, it is clear that user studies depend on
their designers' choices. The designers' choices regarding the targeted audience, number of users,
study objectives, predictive issue nature, data complexity, and questionnaires' content and score
assignment vary greatly over the different studies. Although the literature proposes various user
experiment designs, there is still no consensus about how the usability of the explanations can be
assessed effectively (Ali et al., 2023; Saeed and Omlin, 2023), especially for end-users without a
background in ML.
Liao et al. (2020) investigated the explanation needs of end-users. They proposed a Question Bank
(QB), including a set of prototypical questions that should help designers identify and address
explanation needs. They also mapped the proposed questions to explanation methods available in
the literature. Sipos et al. (2023) studied the applicability of this QB in a specific usage context. They
proposed an extension of this QB with other questions. They also enriched the existing questions,
aiming that their work will provide a basis for future studies on the identification of explanation needs
26A survey and taxonomy of methods interpreting random forest models
in different contexts. In real-world ML projects, the authors suggested using the QB as a starting point
and customizing it based on team discussions.
In the same vein, Jin et al. (2022) suggested considering users' feedback while designing post-hoc
explanations. The authors conceived the EUCA framework, which provides a prototyping protocol
(including tools and methodologies) that can guide the creation of solutions that consider the context-
specific explainability needs. They listed several end-user-friendly explanatory forms that can be used
as building blocks in a participative design, including technical and non-technical users. They also
detailed these explanatory forms based on several criteria (such as their applicable explanation goals,
visual representations, advantages and disadvantages, and examples of algorithmic
implementations), and provided supporting material for their practical use (including prototyping cards,
templates, and examples of application). While the EUCA protocol includes a limited number of
explanatory forms and lacks the quantitative evaluation of usability, it provides a practical protocol
example for designing feasible collaborative solutions seeking explainability.
User studies in the literature employ diverse metrics to quantify usability. We refer to (Rong et al.,
2022) work, which presents a survey of metrics used in recent works, hence providing ideas on how
to collect and evaluate human feedback on explanations. The authors in this work deeply analyzed
several user studies' experimental designs, participants, and measures. They also proposed a
general guideline for human-centered user studies.
Works such as (Liao et al., 2020), (Sipos et al., 2023), (Jin et al., 2022), (Rong et al., 2022), and
(Molnar, 2022) can be used as starting points for the design of practical solutions for interpreting RF
models, including diverse explanatory forms. Our survey of methods interpreting the RF model is not
exhaustive but provides a good basis for the diverse exploratory forms available for the RF model. We
summarize in Table 2 the expected advantages and pitfalls of the methodologies presented in Section
4.2, based on the different works addressing the usability topic reported in this paper. This table along
with the taxonomy presented in this paper and the detailed classification provided on GitHub (see
Section Declarations), could be used as supporting material for designing a user-friendly solution that
considers the different roles, needs, and preferences of the users requiring explanations.
27A survey and taxonomy of methods interpreting random forest models
Table 2: Advantages and disadvantages of the RF interpretative methodologies
Methodologies Description Advantages Disadvantages
Size reduction Build a reduced subset of Reducing the size of the forest would The subset of decision trees generally remains a black box
methods decision trees that is make it easier to explore the tree paths depending on the number and the depth of its tree members.
competitive with a large RF for a local explanation.
model in its predictive
performance.
Rule extraction Build a rule ensemble that The if-then semantic of the rules is Determining the best tradeoff between the complexity of the rule
methods approximates the RF intuitive for human thinking (if rules are ensemble and the predictive performance is challenging (tackled
model predictions while built with intelligible features) differently among papers) and generally time-consuming. It is
considering the complexity Rules facilitate communication about the sometimes not aligned with user's needs.
of the rule ensemble. This model predictions. The process of building the rule ensemble does not consider the
complexity is generally Rules can allow for local explanation or meaningfulness of the features and splits employed. In addition, by
expressed in terms of the data clustering, where each cluster is choosing the least complex rules, this process generates sometimes
number of rules, lengths, explained by one or a few rules. counter-intuitive rules for domain experts.
and coverage. Rules inform about interactions between Rule ensemble building process often gives more importance to the
features. improvement of the predictive performance compared to the
interpretability coverage (many instances remain unexplained).
The rule ensemble is sometimes made up of multiple overlapping
rules, which can blur the overall overview of the final model.
Feature-oriented Provide explanations VIPs help users identify which features PDPs and VIPs do not consider variable interactions and can be
methods based on feature are globally important for predictions. biased if the variables are correlated.
relevance. Can be divided DPs help users explore how predictions SHAP DPs better capture interaction effects among variables but
into three sub-categories: respond to input variable variations. are generally more computationally consuming.
VIPs, DPs, and Local Local decomposition based on feature The realistic number of features to represent in PDPs is limited. It is
decomposition. relevance helps users to understand and hard for Humans to imagine more than three dimensions.
compare individual predictions. Local decomposition (such as the summation of weighed
heterogeneous features) could be not meaningful and foreign to the
application domain language.
Sample Look at the similarity Visualizations based on the RF proximity Visualizations based on RF proximity often appear similar or
similarity-based among instances when matrix or SHAP explanation similarity irrespective of the data, raising uncertainty about their usefulness.
methods providing explanations. allow the identification of data clusters Similar or contrafactual examples could cause confusion if they are
and outliers. counter-intuitive or poorly described (via non-meaningful features).
Local explanations based on similarity
(similar or counterfactual examples) help
verify and compare decisions.
Template/web Cover methods that Help researchers and users easily Interactive and sophisticated visualizations generally require
interfaces provide web interfaces or apply/monitor existing methodologies to supplementary text explanations and tutorials.
templates ready to apply to their research problems. Non-conventional visualization forms can be misleading and often
interpret RF predictions Allow users to easily manipulate the require extra effort for users compared to text or tables.
based on existing visible results via input devices such as a
approaches. keyboard or mouse.
28A survey and taxonomy of methods interpreting random forest models
6. SUMMARY AND CONCLUSIONS
As reported in this paper, RF has the advantage of being an intelligible algorithm regarding its building
process. However, gaining visibility over the entire process that induces the final decisions by
exploring each RF decision tree is complicated, if not impossible. This complexity limits the
acceptance and deployment of RF models in several fields, such as health care, biology, and security.
The interpretability of RF models within these fields is required because the more transparency the
model gains, the more users will trust it and take action based on its results. Otherwise, if the
decision-making process is not clearly explained and understood, errors may occur and go unnoticed
until they cause technical, financial, legal, moral, or ethical damage that may be difficult or impossible
to repair. Thus, more and more approaches are proposed in the literature to interpret RF models,
especially in the last 3 years.
This paper provides a literature review of methods interpreting RF models, following a systematic
methodology. This review presented at first a bibliographic analysis of the surveyed papers through
the analysis of the number of publications, the author’s literature map, and the terms’ co-occurrence
map. It then proposed a taxonomy and a distribution analysis of the surveyed methods based on the
main properties that could help users choose the most suitable methods for the issue under study.
The taxonomy criteria include the stage of explanation, the objective of interpretability, the type of the
problem solved, its input and output formats, the methodology used for providing explanations, and
the programming language used in the implementation of the method.
The analysis of the surveyed methods showed that around 97% of the papers propose post hoc
methods (3% ante hoc). About 87% of them are model-specific methods, thus taking advantage of the
intrinsic architecture of RFs or decision trees in general. The remaining 13% are model-agnostic. This
tendency suggests that the RF predictive structure enables the discovery of diverse kinds of
explanations that are worthwhile to investigate.
The analysis also revealed that about 90% of the surveyed methods can process mixed data
(categorical and continuous inputs). It showed, in addition, that the output format of explanations
varies equitably among texts, statistics, or visual graphs format.
The distribution of the surveyed methods depending on the nature of the issue solved showed that
there is more choice for methods that can be applied to classification issues (around 88%) than for
methods that can be used for regression (around 44%). In addition, it has been noticed that Python is
29A survey and taxonomy of methods interpreting random forest models
the programming language of choice used to develop the surveyed methods during the last five years.
The comparison of the surveyed methods based on the objective of interpretability (global overview,
local explanation, pattern discovery, or a hybrid objective) exhibited about 55% of overlap, meaning
that most methods enable more than one objective.
We have analyzed the methodologies used to interpret the RF model and structured them into five
classes. Size reduction methods reduce the number of RF trees to produce a simpler model. Rule
extraction methods extract a representative set of rules for approximating the RF model. Features-
oriented approaches explain how changes in the values of the descriptive features affect the
predictions. Sample similarity-based methods investigate the similarity among instances to explain
predictions. Finally, template/web interface methods offer frameworks or web interfaces to facilitate
the exploration and explanation of the RF model using already published methodologies. Hybrid
methods can combine more than one method from the methodologies above.
We have shown that almost 40% of the methods use a rule extraction methodology, and that around
60% of them allow a global overview of the model, a local explanation of the predictions, and the
discovery of patterns in the data. Feature-oriented methods represent about 30% of the studied
approaches and are most often used for a global overview. Around 14% of the surveyed methods
propose templates or web interfaces that enable a global overview or the discovery of patterns, while
7% of the methods use a sample similarity-based methodology and are generally focused on pattern
discovery. The remaining approaches (less than 10%) use a size reduction or hybrid methodology.
We have provided a visual tool for the identification/selection of appropriate interpretive methods
given the interpretability objective, the methodology used, and the nature of the problem under
consideration.
We recommend, in practice, comparing and combining different methods of interpretation to extract
various explanations, which should ideally lead to similar or complementary conclusions. These
conclusions should be validated based on rigorous usability studies with technical and non-technical
users. We reported in this work that only around 13% of the surveyed methods presented usability
studies, suggesting that the field of RF interpretability is still far from the level of the required scientific
rigor. We thus reported several works tackling the usability issue, aiming that they could help design
practical end-user solutions that gather diverse RF explanatory forms.
30A survey and taxonomy of methods interpreting random forest models
In future work, we plan to deeply analyze the literature related to usability studies in ML. We are
interested in designing a practical protocol and toolbox that could be used in usability experiments.
These tools are aimed at guiding the choice of the most effective solutions for interpreting RF models
depending on the problem under study, the concerned users, and the objectives of interpretability.
We also plan to focus on interpreting the RF model via the rule extraction methodology. We consider
this class of methods a key to efficient and natural understanding and communication about the
model learned by RFs. As reported in this survey, rule extraction methods can allow for a global
overview of the model, a local explanation of the predictions, and the discovery of patterns. The main
challenge for this kind of method is to find the best trade-off between the complexity and the
predictive performance of the final rule ensemble. We intend in future work to analyze the existing rule
extraction methods to figure out the main characteristics that come into play when addressing this
trade-off. We aim to determine the existing shortcomings and propose possible solutions to resolve
them.
DECLARATIONS
Funding Information
The authors did not receive support from any organization for the submitted work.
Conflict of interest/Competing interests
On behalf of all authors, the corresponding author states that there is no conflict of interest.
Code availability
The implementation and the computational work are done using the R language and environment for
statistical computing. The code and data files used are available via
https://github.com/HMAISSAE/RF_SLR.
Availability of data and materials
All data used to perform this review is available via https://github.com/HMAISSAE/RF_SLR.
REFERENCES
Adadi A, Berrada M (2018) Peeking Inside the Black-Box: A Survey on Explainable Artificial
Intelligence (XAI). IEEE Access 6:52,138–52,160. https://doi.org/10.1109/ACCESS.2018.2870052
Adnan MN, Islam MZ (2017) ForEx++: A New Framework for Knowledge Discovery from Decision
Forests. Australasian Journal of Information Systems 21. https://doi.org/10.3127/ajis.v21i0.1539
31A survey and taxonomy of methods interpreting random forest models
Ali S, Abuhmed T, El-Sappagh S, et al. (2023) Explainable Artificial Intelligence (XAI): What we know
and what is left to attain Trustworthy Artificial Intelligence. Information Fusion 99, 101805.
https://doi.org/10.1016/j.inffus.2023.101805
Amoukou SI, Brunel NJB (2021) Consistent Sufficient Explanations and Minimal Local Rules for
explaining regression and classification models https://doi.org/ arxiv.org/abs/2111.04658
Angelov PP, Soares EA, Jiang R, et al. (2021) Explainable artificial intelligence: an analytical review.
WIREs Data Mining and Knowledge Discovery. https://doi.org/10.1002/widm.1424
Aria M, Cuccurullo C, Gnasso A (2021) A Comparison among Interpretative Proposals for Random
Forests. Machine Learning with Applications 6:100,094.
https://doi.org/10.1016/j.mlwa.2021.100094
Bastani O, Kim C, Bastani H (2017) Interpreting Blackbox Models via Model Extraction
https://doi.org/arxiv.org/abs/1705.08504
Beckett C (2018) Rfviz: An Interactive Visualization Package for Random Forests in R
Bernard S, Heutte L, Adam S (2008) On the selection of decision trees in Random Forests. In: IEEE
International Joint Conference on Neural Networks (IJCNN), Atlanta, United States, pp 302–307,
https://doi.org/10.1109/IJCNN.2009.5178693
Biau G, Scornet E (2016) A random forest guided tour. TEST 25(2):197–227.
https://doi.org/10.1007/s11749-016-0481-7
Birbil SI, Edali M, Yuceoglu B (2020) Rule Covering for Interpretation and Boosting URL
http://arxiv.org/abs/2007.06379
Blanchart P (2021) An exact counterfactual-example-based approach to tree-ensemble models
interpretability https://doi.org/arxiv.org/abs/2105.14820
Bologna G (2021) A rule extraction technique applied to ensembles of neural networks, random
forests, and gradient-boosted trees. Algorithms 14(12), MDPI. https://doi.org/10.3390/a14120339
Breiman L (2001) Random Forests. Machine Learning 45(1):5–32. https://doi.org/
10.1023/A:1010933404324
Breiman L (2002) WALD LECTURE II LOOKING INSIDE THE BLACK BOX
Breiman L, Friedman JH, Olshen RA, et al. (1884) CLASSIFICATION AND REGRESSION TREES,
the wadsworth statistics/probability series edn.Monterey, CA : Wadsworth & Brooks/Cole
Advanced Books & Software, 1984. - 358 p.
Brophy J, Lowd D (2020) TREX: Tree-Ensemble Representer-Point Explanations
https://doi.org/arxiv.org/abs/2009.05530
Be´nard C, Biau G, da Veiga S, et al. (2020) SIRUS: Stable and Interpretable RUle Set for
Classification. Tech. Rep. arXiv:1908.06852, arXiv
Chen J, Li K, Tang Z, et al. (2017) A Parallel Random Forest Algorithm for Big Data in a Spark Cloud
Computing Environment. IEEE Transactions on Parallel and Distributed Systems 28(4):919–933.
https://doi.org/10.1109/TPDS.2016. 2603511
Debeer D, Strobl C (2020) Conditional permutation importance revisited. BMC Bioinformatics 21(1).
https://doi.org/10.1186/s12859-020-03622-2, publisher: BioMed Central
Deng H (2019) Interpreting tree ensembles with inTrees. International Journal of Data Science and
Analytics 7(4):277–287. https://doi.org/10.1007/ s41060-018-0144-8, publisher: Springer Science
and Business Media Deutschland GmbH
32A survey and taxonomy of methods interpreting random forest models
Dieber J, Kirrane S (2020) Why model why? Assessing the strengths and limitations of LIME.
https://doi.org/arxiv.org/abs/2012.00093, arXiv:2012.00093 [cs]
Eck NJ, Waltman L (2010) Software survey: VOSviewer, a computer program for bibliometric
mapping. Scientometrics 84(2):523–538. https://doi.org/10.1007/ s11192-009-0146-3
Ehrlinger J (2015) ggRandomForests: Visually Exploring a Random Forest for Regression
https://doi.org/arxiv.org/abs/1501.07196
Emmert-Streib F, Yli-Harja O, Dehmer M (2020) Explainable artificial intelligence and machine
learning: A reality rooted perspective. WIREs Data Mining and Knowledge Discovery 10(6):e1368.
https://doi.org/10.1002/widm.1368
Epifanio I (2017) Intervention in prediction measure: A new approach to assessing variable
importance for random forests. BMC Bioinformatics 18(1). https://doi. org/10.1186/s12859-017-
1650-8, publisher: BioMed Central Ltd.
Fawagreh K, Gaber MM, Elyan E (2014) Random forests: from early developments to recent
advancements. Systems Science & Control Engineering 2(1):602–609.
https://doi.org/10.1080/21642583.2014.956265
Fawagreh K, Gaber MM, Elyan E (2015) An Outlier Detection-based Tree Selection Approach to
Extreme Pruning of Random Forests https://doi.org/arxiv. org/abs/1503.05187
Ferna´ndez R, Mart´ın de Diego I, Acen˜a V, et al. (2020) Random forest explainability using
counterfactual sets. Information Fusion 63:196–207, publisher: Elsevier B.V.
https://doi.org/10.1016/j.inffus.2020.07.001
Friedman JH (2000) Greedy Function Approximation: A Gradient Boosting Machine. Annals of
Statistics 29:1189–1232
Friedman JH, Popescu BE (2008) Predictive learning via rule ensembles. The Annals of Applied
Statistics 2(3). https://doi.org/10.1214/07-AOAS148
Gatto J, Lanka R, Iwashita Y, et al. (2019) Single Sample Feature Importance: An Interpretable
Algorithm for Low-Level Feature Analysis https://doi.org/arxiv.org/ abs/1911.11901
Goodman B, Flaxman S (2017) European Union Regulations on Algorithmic Decision-Making and a
”Right to Explanation”. AI Magazine 38(3):50. https://doi.org/10.1609/aimag.v38i3.2741
Gossen F, Murtovi A, Zweihoff P, et al. (2019) ADD-Lib: Decision Diagrams in Practice
https://doi.org/arxiv.org/abs/1912.11308
Guidotti R, Monreale A, Ruggieri S, et al. (2018) Local Rule-Based Explanations of Black Box
Decision Systems URL http://arxiv.org/abs/1805.10820
Haas C. de (2021) Usability Study of an Explainable Machine Learning Risk Model for Predicting
Illegal Shipbreaking (Thesis). https://studenttheses.uu.nl/handle/20.500.12932/211
Haddouchi M, Berrado A (2018) Assessing interpretation capacity in Machine Learning: A critical
review. In: Proceedings of the 12th International Conference on Intelligent Systems: Theories and
Applications. Association for Computing Machinery, New York, NY, USA, SITA’18, pp 1–6,
https://doi.org/10.1145/ 3289402.3289549
Haddouchi M, Berrado A (2019) A survey of methods and tools used for interpreting Random Forest.
In: 2019 1st International Conference on Smart Systems and Data Science (ICSSD), pp 1–6,
https://doi.org/10.1109/ ICSSD47982.2019.9002770
Hakkoum H, Abnane I, Idri A (2022) Interpretability in the medical field: A systematic mapping and
review study. Applied Soft Computing 117:108,391. https://doi.org/10.1016/j.asoc.2021.108391
33A survey and taxonomy of methods interpreting random forest models
Hara S, Hayashi K (2017) Making Tree Ensembles Interpretable: A Bayesian Model Selection
Approach URL http://arxiv.org/abs/1606.09066
Hastie T, Tibshirani R, Friedman J (2009) The elements of statistical learning: data mining, inference
and prediction. Springer, 2nd edition.
Hatwell J, Gaber MM, Azad RMA (2020) CHIRPS: Explaining random forest classification. Artificial
Intelligence Review 53(8):5747–5788. https://doi.org/10. 1007/s10462-020-09833-6
Hur JH, Ihm SY, Park YH (2017) A variable impacts measurement in random forest for mobile cloud
computing. Wireless Communications and Mobile Computing 2017, publisher: Hindawi Limited.
https://doi.org/10.1155/2017/6817627
Huysmans J, Dejaeger K, Mues C, et al. (2011) An empirical evaluation of the comprehensibility of
decision table, tree and rule based predictive models. Decision Support Systems 51, 141–154.
https://doi.org/10.1016/j.dss.2010.12.003
Ishwaran H, Kogalur UB, Gorodeski EZ, et al. (2010) High-Dimensional Variable Selection for
Survival Data. Journal of the American Statistical Association 105(489):205–217
Islam MR, Ahmed MU, Barua S, et al. (2022) A Systematic Review of Explainable Artificial
Intelligence in Terms of Different Application Domains and Tasks. Applied Sciences 12(3):1353.
https://doi.org/10.3390/app12031353
Izza Y, Marques-Silva J (2021) On Explaining Random Forests with SAT pp 2584–2591.
https://doi.org/10.24963/ijcai.2021/356, publisher: International Joint Conferences on Artificial
Intelligence
Jin W, Fan J, Gromala D, et al. (2022) EUCA: the End-User-Centered Explainable AI Framework.
https://doi.org/10.48550/arXiv.2102.02437
Johansson U, So¨nstro¨d C, Lo¨fstro¨m T, et al. (2012) Obtaining accurate and comprehensible
classifiers using oracle coaching. Intelligent Data Analysis 16(2):247–263.
https://doi.org/10.3233/IDA-2012-0522
Jones ZM, Linder FJ (2016) edarf: Exploratory Data Analysis using Random Forests. Journal of
Open Source Software 1(6):92. https://doi.org/10.21105/joss.
Junque De Fortuny E, Martens D (2015) Active Learning-Based Pedagogical Rule Extraction. IEEE
Transactions on Neural Networks and Learning Systems 26(11):2664–2677.
https://doi.org/10.1109/TNNLS.2015.2389037
Khan Z, Gul A, Perperoglou A (2016) An Ensemble of Optimal Trees for Classification and
Regression (OTE). Pattern Recognition p 25
Kopp M, Pevny´ T, Holenˇa M (2020) Anomaly explanation with random forests. Expert Systems with
Applications 149, publisher: Elsevier Ltd. https://doi.org/10.1016/j.eswa.2020.113187
Kovalerchuk B, Ahmad M.A., Teredesai A (2020) Survey of explainable machine learning with visual
and granular methods beyond quasi-explanations. https://doi.org/10.48550/arXiv.2009.10221
Lahav O, Mastronarde N, van der Schaar M (2019) What is Interpretable? Using Machine Learning
to Design Interpretable Decision-Support Systems. https://doi.org/10.48550/arXiv.1811.10799
Latinne P, Debeir O, Decaestecker C (2001) Limiting the Number of Trees in Random Forests. In:
Goos G, Hartmanis J, van Leeuwen J, et al. (eds) Multiple Classifier Systems, vol 2096. Springer
Berlin Heidelberg, Berlin, Heidelberg, p 178–187, https://doi.org/10.1007/3-540-48219-9_18
Liaw A, Wiener M (2007) Classification and Regression by randomForest Liu S, Patel R, Daga P, et
al. (2012) Combined rule extraction and feature elimination in supervised classification. IEEE
34A survey and taxonomy of methods interpreting random forest models
Transactions on Nanobioscience 11(3):228–236. https://doi.org/10.1109/TNB.2012.2213264
Liao Q.V., Gruen D, Miller S (2020) Questioning the AI: Informing Design Practices for Explainable
AI User Experiences, in: Proceedings of the 2020 CHI Conference on Human Factors in
Computing Systems. https://doi.org/10.1145/3313831.3376590
Lipton Z.C. (2018) The Mythos of Model Interpretability: In machine learning, the concept of
interpretability is both important and slippery. Queue 16, 31–57.
https://doi.org/10.1145/3236386.3241340
Liu S, Dissanayake S, Patel S, et al. (2014) Learning accurate and interpretable models based on
regularized random forests regression. BMC Systems Biology 8(3), publisher: BioMed Central Ltd..
https://doi.org/10.1186/1752-0509-8-S3-S5
Loecher M (2020) From unbiased MDI Feature Importance to Explainable AI for Trees
https://doi.org/arxiv.org/abs/2003.12043
Louppe G (2014) Understanding Random Forests: From Theory to Practice
https://doi.org/10.13140/2.1.1570.5928
Lundberg SM, Erion GG, Lee SI (2018) Consistent Individualized Feature Attribution for Tree
Ensembles https://doi.org/arxiv.org/abs/1802.03888
Lundberg SM, Erion G, Chen H, et al. (2019) Explainable AI for Trees: From Local Explanations to
Global Understanding https://doi.org/arxiv.org/abs/1905.04610
M. P. Neto, F. V. Paulovich (2021) Explainable Matrix - Visualization for Global and Local
Interpretability of Random Forest Classification Ensembles. IEEE Transactions on Visualization
and Computer Graphics 27(2):1427–1437. https://doi.org/10.1109/TVCG.2020.3030354
Mashayekhi M, Gras R (2017) Rule Extraction from Decision Trees Ensembles: New Algorithms
Based on Heuristic Search and Sparse Group Lasso Methods. International Journal of Information
Technology & Decision Making 16(06):1707–1727. https://doi.org/10.1142/S0219622017500055
Mazumdar D, Neto M, Paulovich F (2021) Random forest similarity maps: A scalable visual
representation for global and local interpretation. Electronics (Switzerland) 10(22) , publisher:
MDPI. https://doi.org/10.3390/electronics10222862
Meinshausen N (2010) Node Harvest. The Annals of Applied Statistics 4(4):2049–2072.
https://doi.org/10.1214/10-AOAS367
Mollas I, Bassiliades N, Tsoumakas G (2021) Conclusive Local Interpretation Rules for Random
Forests https://doi.org/arxiv.org/abs/2104.06040
Molnar C. (2022) Interpretable Machine Learning. A Guide for Making Black Box Models Explainable
(2nd ed.). christophm.github.io/interpretable-ml-book/
Nourani M, Kabir S, Mohseni S, et al. (2019) The Effects of Meaningful and Meaningless
Explanations on Trust and Perceived System Accuracy in Intelligent Systems. Proceedings of the
AAAI Conference on Human Computation and Crowdsourcing 7, 97–105.
https://doi.org/10.1609/hcomp.v7i1.5284
Palczewska A, Palczewski J, Robinson RM, et al. (2014) Interpreting random forest classification
models using a feature contribution method. Advances in Intelligent Systems and Computing
263:193–218, publisher: Springer Verlag. https://doi.org/10.1007/ 978-3-319-04717-1_9
Paluszy A (2017) Structure mining and knowledge extraction from random forest with applications to
The Cancer Genome Atlas project. Tech. rep., Faculty of Mathematics, Informatics and Mechanics
Parr T, Wilson JD (2021) Partial dependence through stratification. Machine Learning with
35A survey and taxonomy of methods interpreting random forest models
Applications 6:100,146. https://doi.org/10.1016/j.mlwa.2021.100146
Paul J, Dupont P (2015) Inferring statistically significant features from random forests.
Neurocomputing 150(PB):471–480. https://doi.org/10.1016/j.neucom. 2014.07.067, publisher:
Elsevier B.V.
Phung LTK, Chau VTN, Phung NH (2015) Extracting Rule RF in Educational Data Classification:
From a Random Forest to Interpretable Refined Rules. In: 2015 International Conference on
Advanced Computing and Applications (ACOMP). IEEE, Ho Chi Minh City, Vietnam, pp 20–27,
https://doi.org/10.1109/ACOMP. 2015.13
Plumb G, Molitor D, Talwalkar A (2019) Model Agnostic Supervised Local Explanations.
arXiv:180702910 [cs, stat] ArXiv: 1807.02910
Poursabzi-Sangdeh F, Goldstein D.G., Hofman J.M., et al. (2021) Manipulating and Measuring
Model Interpretability. http://arxiv.org/abs/1802.07810
Preece A (2018) Asking ‘Why’ in AI: Explainability of intelligent systems – perspectives and
challenges. Intelligent Systems in Accounting, Finance and Management 25(2):63–72.
https://doi.org/10.1002/isaf.1422
Płon´ski P, Zaremba K (2014) Visualizing random forest with self-organising map. Lecture Notes in
Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in
Bioinformatics) 8468 LNAI(PART
2):63–71. https://doi.org/10.1007/978-3-319-07176-3 6
Quach AT (2012) Interactive Random Forests Plots. Tech. rep., Utah State University
Ribeiro MT, Singh S, Guestrin C (2016) “why should i trust you?” explaining the predictions of any
classifier. NAACL-HLT 2016 - 2016 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Proceedings of the
Demonstrations Session pp 97–101, publisher: Association for Computational Linguistics (ACL).
https://doi.org/10.18653/v1/n16-3020
Robnik-Sˇikonja M, Kononenko I (2008) Explaining classifications for individual instances. IEEE
Transactions on Knowledge and Data Engineering 20(5):589–600.
https://doi.org/10.1109/TKDE.2007.190734
Rong Y, Leemann T, Nguyen T, et al. (2022) Towards Human-centered Explainable AI: User Studies
for Model Explanations. http://arxiv.org/abs/2210.11584
Rostamzadeh N, Abdullah S, Sedig K, et al. (2021) Veronica: Visual analytics for identifying feature
groups in disease classification. Information (Switzerland) 12(9) , publisher: MDPI.
https://doi.org/10.3390/info12090344
Saeed W, Omlin C (2023) Explainable AI (XAI): A systematic meta-survey of current challenges and
future opportunities. Knowledge-Based Systems 263, 110273.
https://doi.org/10.1016/j.knosys.2023.110273
Sagi O, Rokach L (2020) Explainable decision forest: Transforming a decision forest into an
interpretable tree. Information Fusion 61:124–138. publisher: Elsevier B.V. https://doi.org/
10.1016/j.inffus.2020.03.013,
Seifert S, Gundlach S, Szymczak S (2019) Surrogate minimal depth as an importance measure for
variables in random forests. Bioinformatics 35(19):3663–3671, publisher: Oxford University Press.
https://doi.org/10.1093/bioinformatics/btz149
Sipos L, Schäfer U, Glinka K, Müller-Birn C (2023) Identifying Explanation Needs of End-users:
Applying and Extending the XAI Question Bank. Mensch und Computer 2023.
https://doi.org/10.1145/3603555.3608551
36A survey and taxonomy of methods interpreting random forest models
Smith M, Alvarez F (2021) A machine learning research template for binary classification problems
and shapley values integration[Formula presented]. Software Impacts 8, publisher: Elsevier B.V.
https://doi.org/10.1016/j.simpa.2021.100074
Tan S, Soloviev M, Hooker G, et al. (2020) Tree Space Prototypes: Another Look at Making Tree
Ensembles Interpretable. arXiv URL http://arxiv.org/abs/1611. 07115
Urbanek S (2011) iPlots eXtreme: next-generation interactive graphics design and implementation of
modern interactive graphics. Computational Statistics 26(3):381–393.
https://doi.org/10.1007/s00180-011-0240-x
Utkin LV, Konstantinov AV (2021) Ensembles of Random SHAPs.
https://doi.org/arxiv.org/abs/2103.03302
Van Assche A, Blockeel H (2008) Seeing the Forest Through the Trees. In: Blockeel H, Ramon J,
Shavlik J, et al. (eds) Inductive Logic Programming. Springer, Berlin, Heidelberg, Lecture Notes in
Computer Science, pp 269–279, https://doi.org/10.1007/978-3-540-78469-2 26
Verdinelli I, Wasserman L (2021) Forest Guided Smoothing https://doi.org/arxiv. org/abs/2103.05092
Vilone G, Longo L (2020) Explainable Artificial Intelligence: a Systematic Review. arXiv:200600093
[cs]
Wang M, Zhang H (2009) Search for the smallest random forest. Statistics and Its Interface
2(3):381–388. https://doi.org/10.4310/SII.2009.v2.n3.a11
Wang S, Wang Y, Wang D, et al. (2020) An improved random forest-based rule extraction method
for breast cancer diagnosis. Applied Soft Computing Journal 86, publisher: Elsevier Ltd.
https://doi.org/10.1016/j.asoc.2019.105941
Webb S, Hanser T, Howlin B, et al. (2014) Feature combination networks for the interpretation of
statistical machine learning models: Application to Ames mutagenicity. Journal of Cheminformatics
6(1), publisher: Gas Turbine Society of Japan. https://doi.org/10.1186/1758-2946-6-8
Welling SH, Refsgaard HHF, Brockhoff PB, et al. (2016) Forest Floor Visualizations of Random
Forests. http://arxiv.org/abs/1605.09196
Yang F, Lu Wh, Luo Lk, et al. (2012) Margin Optimization Based Pruning for Random Forest.
Neurocomputing 94:54–63. https://doi.org/10.1016/j.neucom. 2012.04.007
Zhang G, Hou Z, Huang Y, et al. (2021) Extracting Optimal Explanations for Ensemble Trees via
Logical Reasoning URL http://arxiv.org/abs/2103.02191
Zhao X, Wu Y, Lee DL, et al. (2019) iForest: Interpreting Random Forests via Visual Analytics. IEEE
Transactions on Visualization and Computer Graphics 25(1):407–416.
https://doi.org/10.1109/TVCG.2018.2864475
37