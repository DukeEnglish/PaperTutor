[
    {
        "title": "SMooDi: Stylized Motion Diffusion Model",
        "authors": "Lei ZhongYiming XieVarun JampaniDeqing SunHuaizu Jiang",
        "links": "http://arxiv.org/abs/2407.12783v1",
        "entry_id": "http://arxiv.org/abs/2407.12783v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12783v1",
        "summary": "We introduce a novel Stylized Motion Diffusion model, dubbed SMooDi, to\ngenerate stylized motion driven by content texts and style motion sequences.\nUnlike existing methods that either generate motion of various content or\ntransfer style from one sequence to another, SMooDi can rapidly generate motion\nacross a broad range of content and diverse styles. To this end, we tailor a\npre-trained text-to-motion model for stylization. Specifically, we propose\nstyle guidance to ensure that the generated motion closely matches the\nreference style, alongside a lightweight style adaptor that directs the motion\ntowards the desired style while ensuring realism. Experiments across various\napplications demonstrate that our proposed framework outperforms existing\nmethods in stylized motion generation.",
        "updated": "2024-07-17 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12783v1"
    },
    {
        "title": "Contrastive Adversarial Training for Unsupervised Domain Adaptation",
        "authors": "Jiahong ChenZhilin ZhangLucy LiBehzad ShahrasbiArjun Mishra",
        "links": "http://arxiv.org/abs/2407.12782v1",
        "entry_id": "http://arxiv.org/abs/2407.12782v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12782v1",
        "summary": "Domain adversarial training has shown its effective capability for finding\ndomain invariant feature representations and been successfully adopted for\nvarious domain adaptation tasks. However, recent advances of large models\n(e.g., vision transformers) and emerging of complex adaptation scenarios (e.g.,\nDomainNet) make adversarial training being easily biased towards source domain\nand hardly adapted to target domain. The reason is twofold: relying on large\namount of labelled data from source domain for large model training and lacking\nof labelled data from target domain for fine-tuning. Existing approaches widely\nfocused on either enhancing discriminator or improving the training stability\nfor the backbone networks. Due to unbalanced competition between the feature\nextractor and the discriminator during the adversarial training, existing\nsolutions fail to function well on complex datasets. To address this issue, we\nproposed a novel contrastive adversarial training (CAT) approach that leverages\nthe labeled source domain samples to reinforce and regulate the feature\ngeneration for target domain. Typically, the regulation forces the target\nfeature distribution being similar to the source feature distribution. CAT\naddressed three major challenges in adversarial learning: 1) ensure the feature\ndistributions from two domains as indistinguishable as possible for the\ndiscriminator, resulting in a more robust domain-invariant feature generation;\n2) encourage target samples moving closer to the source in the feature space,\nreducing the requirement for generalizing classifier trained on the labeled\nsource domain to unlabeled target domain; 3) avoid directly aligning unpaired\nsource and target samples within mini-batch. CAT can be easily plugged into\nexisting models and exhibits significant performance improvements.",
        "updated": "2024-07-17 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12782v1"
    },
    {
        "title": "VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control",
        "authors": "Sherwin BahmaniIvan SkorokhodovAliaksandr SiarohinWilli MenapaceGuocheng QianMichael VasilkovskyHsin-Ying LeeChaoyang WangJiaxu ZouAndrea TagliasacchiDavid B. LindellSergey Tulyakov",
        "links": "http://arxiv.org/abs/2407.12781v1",
        "entry_id": "http://arxiv.org/abs/2407.12781v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12781v1",
        "summary": "Modern text-to-video synthesis models demonstrate coherent, photorealistic\ngeneration of complex videos from a text description. However, most existing\nmodels lack fine-grained control over camera movement, which is critical for\ndownstream applications related to content creation, visual effects, and 3D\nvision. Recently, new methods demonstrate the ability to generate videos with\ncontrollable camera poses these techniques leverage pre-trained U-Net-based\ndiffusion models that explicitly disentangle spatial and temporal generation.\nStill, no existing approach enables camera control for new, transformer-based\nvideo diffusion models that process spatial and temporal information jointly.\nHere, we propose to tame video transformers for 3D camera control using a\nControlNet-like conditioning mechanism that incorporates spatiotemporal camera\nembeddings based on Plucker coordinates. The approach demonstrates\nstate-of-the-art performance for controllable video generation after\nfine-tuning on the RealEstate10K dataset. To the best of our knowledge, our\nwork is the first to enable camera control for transformer-based video\ndiffusion models.",
        "updated": "2024-07-17 17:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12781v1"
    },
    {
        "title": "Generalizable Human Gaussians for Sparse View Synthesis",
        "authors": "Youngjoong KwonBaole FangYixing LuHaoye DongCheng ZhangFrancisco Vicente CarrascoAlbert Mosella-MontoroJianjin XuShingo TakagiDaeil KimAayush PrakashFernando De la Torre",
        "links": "http://arxiv.org/abs/2407.12777v1",
        "entry_id": "http://arxiv.org/abs/2407.12777v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12777v1",
        "summary": "Recent progress in neural rendering has brought forth pioneering methods,\nsuch as NeRF and Gaussian Splatting, which revolutionize view rendering across\nvarious domains like AR/VR, gaming, and content creation. While these methods\nexcel at interpolating {\\em within the training data}, the challenge of\ngeneralizing to new scenes and objects from very sparse views persists.\nSpecifically, modeling 3D humans from sparse views presents formidable hurdles\ndue to the inherent complexity of human geometry, resulting in inaccurate\nreconstructions of geometry and textures. To tackle this challenge, this paper\nleverages recent advancements in Gaussian Splatting and introduces a new method\nto learn generalizable human Gaussians that allows photorealistic and accurate\nview-rendering of a new human subject from a limited set of sparse views in a\nfeed-forward manner. A pivotal innovation of our approach involves\nreformulating the learning of 3D Gaussian parameters into a regression process\ndefined on the 2D UV space of a human template, which allows leveraging the\nstrong geometry prior and the advantages of 2D convolutions. In addition, a\nmulti-scaffold is proposed to effectively represent the offset details. Our\nmethod outperforms recent methods on both within-dataset generalization as well\nas cross-dataset generalization settings.",
        "updated": "2024-07-17 17:56:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12777v1"
    },
    {
        "title": "OMG-Net: A Deep Learning Framework Deploying Segment Anything to Detect Pan-Cancer Mitotic Figures from Haematoxylin and Eosin-Stained Slides",
        "authors": "Zhuoyan ShenMikael SimardDouglas BrandVanghelita AndreiAli Al-KhaderFatine OumlilKatherine TreversThomas ButtersSimon HaefligerEleanna KaraFernanda AmaryRoberto TiraboscoPaul CoolGary RoyleMaria A. HawkinsAdrienne M. FlanaganCharles-Antoine Collins Fekete",
        "links": "http://arxiv.org/abs/2407.12773v1",
        "entry_id": "http://arxiv.org/abs/2407.12773v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12773v1",
        "summary": "Mitotic activity is an important feature for grading several cancer types.\nCounting mitotic figures (MFs) is a time-consuming, laborious task prone to\ninter-observer variation. Inaccurate recognition of MFs can lead to incorrect\ngrading and hence potential suboptimal treatment. In this study, we propose an\nartificial intelligence (AI)-aided approach to detect MFs in digitised\nhaematoxylin and eosin-stained whole slide images (WSIs). Advances in this area\nare hampered by the limited number and types of cancer datasets of MFs. Here we\nestablish the largest pan-cancer dataset of mitotic figures by combining an\nin-house dataset of soft tissue tumours (STMF) with five open-source mitotic\ndatasets comprising multiple human cancers and canine specimens (ICPR, TUPAC,\nCCMCT, CMC and MIDOG++). This new dataset identifies 74,620 MFs and 105,538\nmitotic-like figures. We then employed a two-stage framework (the Optimised\nMitoses Generator Network (OMG-Net) to classify MFs. The framework first\ndeploys the Segment Anything Model (SAM) to automate the contouring of MFs and\nsurrounding objects. An adapted ResNet18 is subsequently trained to classify\nMFs. OMG-Net reaches an F1-score of 0.84 on pan-cancer MF detection (breast\ncarcinoma, neuroendocrine tumour and melanoma), largely outperforming the\nprevious state-of-the-art MIDOG++ benchmark model on its hold-out testing set\n(e.g. +16% F1-score on breast cancer detection, p<0.001) thereby providing\nsuperior accuracy in detecting MFs on various types of tumours obtained with\ndifferent scanners.",
        "updated": "2024-07-17 17:53:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12773v1"
    }
]