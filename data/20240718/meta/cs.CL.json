[
    {
        "title": "LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models",
        "authors": "Kaichen ZhangBo LiPeiyuan ZhangFanyi PuJoshua Adrian CahyonoKairui HuShuai LiuYuanhan ZhangJingkang YangChunyuan LiZiwei Liu",
        "links": "http://arxiv.org/abs/2407.12772v1",
        "entry_id": "http://arxiv.org/abs/2407.12772v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12772v1",
        "summary": "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.",
        "updated": "2024-07-17 17:51:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12772v1"
    },
    {
        "title": "The Role of Network and Identity in the Diffusion of Hashtags",
        "authors": "Aparna AnanthasubramaniamYufei ZhuDavid JurgensDaniel Romero",
        "links": "http://arxiv.org/abs/2407.12771v1",
        "entry_id": "http://arxiv.org/abs/2407.12771v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12771v1",
        "summary": "Although the spread of behaviors is influenced by many social factors,\nexisting literature tends to study the effects of single factors -- most often,\nproperties of the social network -- on the final cascade. In order to move\ntowards a more integrated view of cascades, this paper offers the first\ncomprehensive investigation into the role of two social factors in the\ndiffusion of 1,337 popular hashtags representing the production of novel\nculture on Twitter: 1) the topology of the Twitter social network and 2)\nperformance of each user's probable demographic identity. Here, we show that\ncascades are best modeled using a combination of network and identity, rather\nthan either factor alone. This combined model best reproduces a composite index\nof ten cascade properties across all 1,337 hashtags. However, there is\nimportant heterogeneity in what social factors are required to reproduce\ndifferent properties of hashtag cascades. For instance, while a combined\nnetwork+identity model best predicts the popularity of cascades, a network-only\nmodel has better performance in predicting cascade growth and an identity-only\nmodel in adopter composition. We are able to predict what type of hashtag is\nbest modeled by each combination of features and use this to further improve\nperformance. Additionally, consistent with prior literature on the combined\nnetwork+identity model most outperforms the single-factor counterfactuals among\nhashtags used for expressing racial or regional identity, stance-taking,\ntalking about sports, or variants of existing cultural trends with very slow-\nor fast-growing communicative need. In sum, our results imply the utility of\nmulti-factor models in predicting cascades, in order to account for the varied\nways in which network, identity, and other social factors play a role in the\ndiffusion of hashtags on Twitter.",
        "updated": "2024-07-17 17:51:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12771v1"
    },
    {
        "title": "HDLCopilot: Hardware Design Library Querying with Natural Language",
        "authors": "Manar AbdelattySherief Reda",
        "links": "http://arxiv.org/abs/2407.12749v1",
        "entry_id": "http://arxiv.org/abs/2407.12749v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12749v1",
        "summary": "Hardware design engineers routinely work with multiple Process Design Kits\n(PDKs) from various fabrication labs, each containing several standard cell\nlibraries, optimized for specific metric such as speed, power, or density.\nThese libraries include multiple views such as liberty files for timing\ninformation, LEF files for abstract layout details, and technology LEF for\nprocess design rules. Navigating this complex landscape to retrieve specific\ninformation about gates or design rules is often time-consuming and\nerror-prone. To address this, we present HDLCopilot, an LLM-powered PDK query\nsystem that allows engineers to streamline interactions with PDKs in natural\nlanguage format, making information retrieval accurate and more efficient.\nHDLCopilot achieves an accuracy of 94.23\\% on an evaluation set comprised of\ndiverse and complex natural language queries. HDLCopilot positions itself as a\npowerful assistant in the hardware design process, enhancing productivity and\nreducing potential human errors.",
        "updated": "2024-07-17 17:11:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12749v1"
    },
    {
        "title": "A LLM Benchmark based on the Minecraft Builder Dialog Agent Task",
        "authors": "Chris MadgeMassimo Poesio",
        "links": "http://arxiv.org/abs/2407.12734v1",
        "entry_id": "http://arxiv.org/abs/2407.12734v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12734v1",
        "summary": "In this work we proposing adapting the Minecraft builder task into an LLM\nbenchmark suitable for evaluating LLM ability in spatially orientated tasks,\nand informing builder agent design. Previous works have proposed corpora with\nvarying complex structures, and human written instructions. We instead attempt\nto provide a comprehensive synthetic benchmark for testing builder agents over\na series of distinct tasks that comprise of common building operations. We\nbelieve this approach allows us to probe specific strengths and weaknesses of\ndifferent agents, and test the ability of LLMs in the challenging area of\nspatial reasoning and vector based math.",
        "updated": "2024-07-17 16:52:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12734v1"
    },
    {
        "title": "Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?",
        "authors": "Ben YaoYazhou ZhangQiuchi LiJing Qin",
        "links": "http://arxiv.org/abs/2407.12725v1",
        "entry_id": "http://arxiv.org/abs/2407.12725v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12725v1",
        "summary": "Elaborating a series of intermediate reasoning steps significantly improves\nthe ability of large language models (LLMs) to solve complex problems, as such\nsteps would evoke LLMs to think sequentially. However, human sarcasm\nunderstanding is often considered an intuitive and holistic cognitive process,\nin which various linguistic, contextual, and emotional cues are integrated to\nform a comprehensive understanding of the speaker's true intention, which is\nargued not be limited to a step-by-step reasoning process. To verify this\nargument, we introduce a new prompting framework called SarcasmCue, which\ncontains four prompting strategies, $viz.$ chain of contradiction (CoC), graph\nof cues (GoC), bagging of cues (BoC) and tensor of cues (ToC), which elicits\nLLMs to detect human sarcasm by considering sequential and non-sequential\nprompting methods. Through a comprehensive empirical comparison on four\nbenchmarking datasets, we show that the proposed four prompting methods\noutperforms standard IO prompting, CoT and ToT with a considerable margin, and\nnon-sequential prompting generally outperforms sequential prompting.",
        "updated": "2024-07-17 16:42:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12725v1"
    }
]