[
    {
        "title": "GroundUp: Rapid Sketch-Based 3D City Massing",
        "authors": "Gizem Esra UnluMohamed SayedYulia GryaditskayaGabriel Brostow",
        "links": "http://arxiv.org/abs/2407.12739v1",
        "entry_id": "http://arxiv.org/abs/2407.12739v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12739v1",
        "summary": "We propose GroundUp, the first sketch-based ideation tool for 3D city massing\nof urban areas. We focus on early-stage urban design, where sketching is a\ncommon tool and the design starts from balancing building volumes (masses) and\nopen spaces. With Human-Centered AI in mind, we aim to help architects quickly\nrevise their ideas by easily switching between 2D sketches and 3D models,\nallowing for smoother iteration and sharing of ideas. Inspired by feedback from\narchitects and existing workflows, our system takes as a first input a user\nsketch of multiple buildings in a top-down view. The user then draws a\nperspective sketch of the envisioned site. Our method is designed to exploit\nthe complementarity of information in the two sketches and allows users to\nquickly preview and adjust the inferred 3D shapes. Our model has two main\ncomponents. First, we propose a novel sketch-to-depth prediction network for\nperspective sketches that exploits top-down sketch shapes. Second, we use depth\ncues derived from the perspective sketch as a condition to our diffusion model,\nwhich ultimately completes the geometry in a top-down view. Thus, our final 3D\ngeometry is represented as a heightfield, allowing users to construct the city\n`from the ground up'.",
        "updated": "2024-07-17 16:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12739v1"
    },
    {
        "title": "The Future of Learning: Large Language Models through the Lens of Students",
        "authors": "He ZhangJingyi XieChuhao WuJie CaiChanMin KimJohn M. Carroll",
        "links": "http://arxiv.org/abs/2407.12723v1",
        "entry_id": "http://arxiv.org/abs/2407.12723v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12723v1",
        "summary": "As Large-Scale Language Models (LLMs) continue to evolve, they demonstrate\nsignificant enhancements in performance and an expansion of functionalities,\nimpacting various domains, including education. In this study, we conducted\ninterviews with 14 students to explore their everyday interactions with\nChatGPT. Our preliminary findings reveal that students grapple with the dilemma\nof utilizing ChatGPT's efficiency for learning and information seeking, while\nsimultaneously experiencing a crisis of trust and ethical concerns regarding\nthe outcomes and broader impacts of ChatGPT. The students perceive ChatGPT as\nbeing more \"human-like\" compared to traditional AI. This dilemma, characterized\nby mixed emotions, inconsistent behaviors, and an overall positive attitude\ntowards ChatGPT, underscores its potential for beneficial applications in\neducation and learning. However, we argue that despite its human-like\nqualities, the advanced capabilities of such intelligence might lead to adverse\nconsequences. Therefore, it's imperative to approach its application cautiously\nand strive to mitigate potential harms in future developments.",
        "updated": "2024-07-17 16:40:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12723v1"
    },
    {
        "title": "AudienceView: AI-Assisted Interpretation of Audience Feedback in Journalism",
        "authors": "William BrannonDoug BeefermanHang JiangAndrew HeywardDeb Roy",
        "links": "http://arxiv.org/abs/2407.12613v1",
        "entry_id": "http://arxiv.org/abs/2407.12613v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12613v1",
        "summary": "Understanding and making use of audience feedback is important but difficult\nfor journalists, who now face an impractically large volume of audience\ncomments online. We introduce AudienceView, an online tool to help journalists\ncategorize and interpret this feedback by leveraging large language models\n(LLMs). AudienceView identifies themes and topics, connects them back to\nspecific comments, provides ways to visualize the sentiment and distribution of\nthe comments, and helps users develop ideas for subsequent reporting projects.\nWe consider how such tools can be useful in a journalist's workflow, and\nemphasize the importance of contextual awareness and human judgment.",
        "updated": "2024-07-17 14:41:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12613v1"
    },
    {
        "title": "Abstraction Alignment: Comparing Model and Human Conceptual Relationships",
        "authors": "Angie BoggustHyemin BangHendrik StrobeltArvind Satyanarayan",
        "links": "http://arxiv.org/abs/2407.12543v1",
        "entry_id": "http://arxiv.org/abs/2407.12543v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12543v1",
        "summary": "Abstraction -- the process of generalizing specific examples into broad\nreusable patterns -- is central to how people efficiently process and store\ninformation and apply their knowledge to new data. Promisingly, research has\nshown that ML models learn representations that span levels of abstraction,\nfrom specific concepts like \"bolo tie\" and \"car tire\" to more general concepts\nlike \"CEO\" and \"model\". However, existing techniques analyze these\nrepresentations in isolation, treating learned concepts as independent\nartifacts rather than an interconnected web of abstraction. As a result,\nalthough we can identify the concepts a model uses to produce its output, it is\ndifficult to assess if it has learned a human-aligned abstraction of the\nconcepts that will generalize to new data. To address this gap, we introduce\nabstraction alignment, a methodology to measure the agreement between a model's\nlearned abstraction and the expected human abstraction. We quantify abstraction\nalignment by comparing model outputs against a human abstraction graph, such as\nlinguistic relationships or medical disease hierarchies. In evaluation tasks\ninterpreting image models, benchmarking language models, and analyzing medical\ndatasets, abstraction alignment provides a deeper understanding of model\nbehavior and dataset content, differentiating errors based on their agreement\nwith human knowledge, expanding the verbosity of current model quality metrics,\nand revealing ways to improve existing human abstractions.",
        "updated": "2024-07-17 13:27:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12543v1"
    },
    {
        "title": "Application of Prompt Learning Models in Identifying the Collaborative Problem Solving Skills in an Online Task",
        "authors": "Mengxiao ZhuXin WangXiantao WangZihang ChenWei Huang",
        "links": "http://arxiv.org/abs/2407.12487v1",
        "entry_id": "http://arxiv.org/abs/2407.12487v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12487v1",
        "summary": "Collaborative problem solving (CPS) competence is considered one of the\nessential 21st-century skills. To facilitate the assessment and learning of CPS\ncompetence, researchers have proposed a series of frameworks to conceptualize\nCPS and explored ways to make sense of the complex processes involved in\ncollaborative problem solving. However, encoding explicit behaviors into\nsubskills within the frameworks of CPS skills is still a challenging task.\nTraditional studies have relied on manual coding to decipher behavioral data\nfor CPS, but such coding methods can be very time-consuming and cannot support\nreal-time analyses. Scholars have begun to explore approaches for constructing\nautomatic coding models. Nevertheless, the existing models built using machine\nlearning or deep learning techniques depend on a large amount of training data\nand have relatively low accuracy. To address these problems, this paper\nproposes a prompt-based learning pre-trained model. The model can achieve high\nperformance even with limited training data. In this study, three experiments\nwere conducted, and the results showed that our model not only produced the\nhighest accuracy, macro F1 score, and kappa values on large training sets, but\nalso performed the best on small training sets of the CPS behavioral data. The\napplication of the proposed prompt-based learning pre-trained model contributes\nto the CPS skills coding task and can also be used for other CSCW coding tasks\nto replace manual coding.",
        "updated": "2024-07-17 11:12:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12487v1"
    }
]