Preprintunderreview
HOW DISCRETE AND CONTINUOUS DIFFUSION MEET:
COMPREHENSIVE ANALYSIS OF DISCRETE DIFFUSION
MODELS VIA A STOCHASTIC INTEGRAL FRAMEWORK
YinuoRen HaoxuanChen
ICME ICME
StanfordUniversity StanfordUniversity
yinuoren@stanford.edu haoxuanc@stanford.edu
GrantM.Rotskoff LexingYing
DepartmentofChemistry DepartmentofMathematics
StanfordUniversity StanfordUniversity
rotskoff@stanford.edu lexing@stanford.edu
ABSTRACT
Discrete diffusion models have gained increasing attention for their ability to
model complex distributions with tractable sampling and inference. However,
theerroranalysisfordiscretediffusionmodelsremainslesswell-understood. In
this work, we propose a comprehensive framework for the error analysis of dis-
crete diffusion models based on Lévy-type stochastic integrals. By generalizing
thePoissonrandommeasuretothatwithatime-independentandstate-dependent
intensity,werigorouslyestablishastochasticintegralformulationofdiscretedif-
fusion models and provide the corresponding change of measure theorems that
are intriguingly analogous to Itô integrals and Girsanov’s theorem for their con-
tinuouscounterparts. Ourframeworkunifiesandstrengthensthecurrenttheoret-
ical results on discrete diffusion models and obtains the first error bound for the
τ-leaping scheme in KL divergence. With error sources clearly identified, our
analysis gives new insight into the mathematical properties of discrete diffusion
modelsandoffersguidanceforthedesignofefficientandaccuratealgorithmsfor
real-worlddiscretediffusionmodelapplications.
1 INTRODUCTION
Diffusion and flow-based models designed for discrete distributions have gained significant atten-
tion in recent years due to their versatility and wide applicability across various domains. These
models have been proposed and refined in several key works (Sohl-Dickstein et al., 2015; Austin
et al., 2021; Floto et al., 2023; Hoogeboom et al., 2021a;b; Meng et al., 2022; Richemond et al.,
2022;Sunetal.,2022;Santosetal.,2023). Theappealofsuchmodelsstemsfromtheirpotentialto
addresschallengingproblemsinfieldslikecomputationalbiology,wheretheyhaveshownpromise
in tasks such as molecule, protein, and DNA sequence design (Seff et al., 2019; Alamdari et al.,
2023;Avdeyevetal.,2023;Emamietal.,2023;Freyetal.,2023;Watsonetal.,2023;Yangetal.,
2023b;Campbelletal.,2024;Starketal.,2024;Kerby&Moon,2024;Yietal.,2024).Additionally,
these approaches have proven effective in combinatorial optimization (Li et al., 2024e), modeling
retrosynthesis (Igashov et al., 2023), image synthesis (Lezama et al., 2022; Gu et al., 2022), text
summarization (Dat et al., 2024) along with the generation of graph (Niu et al., 2020; Shi et al.,
2020; Qin et al., 2023; Vignac et al., 2022), layout (Inoue et al., 2023; Zhang et al., 2023), mo-
tion (Chi et al., 2024; Lou et al., 2023), sound (Yang et al., 2023a), image (Hu et al., 2022; Zhu
etal.,2022),speech(Wuetal.,2024)andtext(Heetal.,2022;Wuetal.,2023;Gongetal.,2023;
Zhengetal.,2023;Zhouetal.,2023;Shietal.,2024;Sahooetal.,2024). Discretediffusionmodels
alsosynergizewithothermethodologies,includingtensornetworks(Causeretal.,2024),enhanced
guidancemechanisms(Gruveretal.,2024;Nisonoffetal.,2024;Lietal.,2024d),structuredprefer-
entialgeneration(Rissanenetal.,2024),andalternativemetrics,e.g. theFisherinformationmetric
1
4202
tcO
4
]GL.sc[
1v10630.0142:viXraPreprintunderreview
(Davisetal.,2024). Thesedevelopmentshighlightthegrowingimportanceofdiscretemodelingin
advancingboththeoreticalunderstandingandefficientimplementations.
PartlyduetotheabsenceofadiscreteequivalenttoGirsanov’stheorem,theerroranalysisfordis-
cretediffusionmodelsremainsunderdevelopedcomparedtotheircontinuouscounterparts. (Camp-
belletal.,2022)conductsaMarkovchain-basederroranalysisforτ-leapingintotalvariationdis-
tance,withfurtheradvancementsfortheparticularstatespaceX={0,1}dbyChen&Ying(2024)
andlatertoX=[S]dbyaconcurrentwork(Zhangetal.,2024).Inthiswork,ourgoalistoestablish
a comprehensive framework for analyzing discrete diffusion models through a stochastic analysis
perspective,whichismotivatedbythetheoryofcontinuousdiffusionmodelsandcompletelydiffer-
entfrompreviousworks. DrawingontoolsfromLévyprocessesandmethodologiesforanalyzing
thesimulationsofchemicalreactions(Li,2007;Andersonetal.,2011),weextendPoissonrandom
measurestothosewithevolvingintensities,i.e. bothtime-inhomogeneousandstate-dependentin-
tensities(Protter,1983),introduceLévy-typestochasticintegrals(Applebaum,2009),andarticulate
correspondingchangeofmeasuretheorems,whichareanalogoustotheItôintegralsandGirsanov’s
theoremincontinuoussettings.
We furtherdemonstrate that discretediffusion modelsimplemented via eitherthe τ-leaping orthe
uniformization scheme can be formulated as stochastic integrals w.r.t. Poisson random measures
with evolving intensity, which leads to a unified framework for error analysis. This new stochas-
ticintegral-basedframework,markingafirstfordiscretediffusionmodels,isespeciallyconvenient
and straightforward for decomposing inference error into three parts: truncation, approximation,
and discretization, drawing satisfying parallels with state-of-the-art theories for continuous diffu-
sion model (Chen et al., 2022; 2023a; Benton et al., 2023a). Our approach thus provides intuitive
explanations for the loss design and unifies the error analysis across both schemes, enhancing the
comparativeunderstandingoftheirconvergencecharacteristics. Notably,weachievestrongercon-
vergenceresultsinKLdivergenceandrelaxsomeofthestringentassumptionspreviouslyrequired
forthestatespace,theratematrix,theregularity,andtheestimationofscorefunctions,etc.,thereby
pavingthewayfortheanalysisofabroaderclassofdiscretediffusionmodelsofinterestaswellas
providingvaluableinsightsandtoolsfordesigningefficientandaccuratealgorithmstailoredtothe
practicaldemandsofdiscretediffusionmodelsinreal-worldapplications.
1.1 CONTRIBUTIONS
Ourmaincontributionsaresummarizedasfollows:
• WedeveloparigorousframeworkfordiscretediffusionmodelsusingLévy-typestochasticinte-
gralsbasedonthePoissonrandommeasurewithevolvingintensity,whichincludesformulating
discrete diffusion models into stochastic integrals and establishing change of measure theorems
thatfacilitateexplicitlog-likelihoodratiocalculations;
• Our framework extends to a comprehensive, continuous-time analysis for error decomposition
in discrete diffusion models, drawing clear parallels with the methodologies used in continuous
modelsandenablingmoreeffectiveadaptationsoftechniquesacrossdifferentmodeltypes;
• We unify and fortify existing research on discrete diffusion models by deriving the first error
boundforτ-leapingintermsofKLdivergence,provideacomparativestudyofτ-leapinganduni-
formizationimplementations,andshedlightonestablishingconvergenceguaranteesforabroader
spectrumofdiscretediffusionmodels.
1.2 RELATEDWORKS
Continuous Diffusion Models. Continuous diffusion models have been one of the most active
researchareasingenerativemodeling. Earlierworkoncontinuousdiffusionmodelsandprobability
flow-basedmodelsinclude(Sohl-Dicksteinetal.,2015;Zhangetal.,2018;Song&Ermon,2019;
Hoetal.,2020;Songetal.,2020;2021;Lipmanetal.,2022;Liuetal.,2022;Albergo&Vanden-
Eijnden,2022;Albergoetal.,2023). Ithasshownstate-of-the-artperformanceinvariousfieldsof
science andengineering. For some recentwork and comprehensivereview articles, one may refer
to(Xuetal.,2022;Yangetal.,2023c;Chan,2024;Wangetal.,2023;Alakhdaretal.,2024;Chen
etal.,2024b;Fanetal.,2024;Guoetal.,2024;Rieseletal.,2024;Zhuetal.,2024).
2Preprintunderreview
TheoryofContinuousDiffusionModels. Inadditiontothehugesuccessachievedbydiffusion
models in empirical studies, many works have also tried to establish sampling guarantees for dif-
fusion and probability flow-based models, such as (Tzen & Raginsky, 2019; Block et al., 2020;
Benton et al., 2023b; Chen et al., 2023b; Mbacke & Rivasplata, 2023; Liang et al., 2024). Re-
garding theoretical analysis of continuous diffusion models, (Lee et al., 2022) provided the first
samplingguaranteeunderthesmoothnessandisoperimetryassumptions. Follow-upworkremoved
such assumptions (Chen et al., 2022; 2023a; Lee et al., 2023) and obtained better convergence re-
sults (Benton et al., 2023a; Pedrotti et al., 2023; Li et al., 2023; 2024a;b). For the probability
flow-based implementation, sampling guarantee was also established and further refined in many
recentwork(Chenetal.,2024c;Gao&Zhu,2024;Huangetal.,2024;Lietal.,2024c)
2 PRELIMINARIES
In this section, we introduce the basic concepts of both continuous and discrete diffusion models
and then roughly outline the error analysis for continuous diffusion models, which will serve as a
referencefortheerroranalysisfordiscretediffusionmodels.
2.1 CONTINUOUSDIFFUSIONMODELS
Indiffusionmodels, theforwardprocessisdesignedasanItôprocess(x ) inRd satisfying
t 0≤t≤T
thefollowingstochasticdifferentialequation(SDE):
dx =b (x )dt+g dw , with x ∼p , (2.1)
t t t t t 0 0
where (w ) is a standard Brownian motion. The probability distribution of x is denoted by
t t≥0 t
p , and the distribution p at time t = 0 is the target distribution for sampling. The time-reversal
t 0
(x ) of(2.1)satisfiesthebackwardprocess:
s 0≤s≤T
(cid:104) (cid:105)
dx = −b (x )+g g⊤∇logp (x ) ds+g dw , (2.2)
s s s s s s s s s
where∗ denotes∗ ,withp =p andp =p .
s T−s 0 T T 0
Oneofthecommonchoicesforthedriftb andthediffusioncoefficientg isb (x) = −1β x and
√ t t 2 t t
g = σ β I, underwhich(2.1)isanOrnstein-Uhlenbeck(OU)processconvergingexponentially,
t
i.e. p ≈ p := N(0,σ2I),andtheforwardprocess(2.1)andthebackwardprocess(2.2)reduce
T ∞
tothefollowingform:
1 (cid:112) (cid:20) 1 (cid:21) (cid:113) dx =− β x dt+σ β dw , anddx =β x +σ2∇logp (x ) ds+σ β dw . (2.3)
t 2 t t t t s s 2 s s s s s
Inpractice,thescorefunctions (x ):=∇logp (x )isoftenestimatedbyaneuralnetworksθ(x ),
t t t t (cid:98)t t
whereθdenotestheparameters,andtrainedviadenoisingscore-matching(Vincent,2011):
θ =argmin(cid:90) T ψ tE xt∼pt(cid:104)(cid:13) (cid:13)∇logp t(x t)−s (cid:98)θ t(x t)(cid:13) (cid:13)2(cid:105) dt
θ 0 (2.4)
=argmin(cid:90) T ψ tE x0∼p0(cid:104) E xt∼pt|0(xt|x0)(cid:104)(cid:13) (cid:13)∇logp t|0(x t|x 0)−s (cid:98)θ t(x t)(cid:13) (cid:13)2(cid:105)(cid:105) dt,
θ 0
wherep (x |x )isthetransitiondistributionfromx tox under(2.3)withanexplicitformas
t|0 t 0 0 t
N(µ t,σ t2I), where µ
t
=x 0e−1 2(cid:82) 0tβtdt and σ t2 =σ2(cid:16) 1−e−(cid:82) 0tβtdt(cid:17) , (2.5)
andψ isaweightingfunctionforthelossattimet. AfterobtainingtheNN-basedscorefunction
t
sθ(x ),thebackwardprocessin(2.3)isapproximatedas:
(cid:98)t s
(cid:20) (cid:21)
1
dy = y +sθ(y ) ds+dw , withy ∼q =N(0,σ2I). (2.6)
s 2 s (cid:98)s s s 0 0
3
⃗
⃗
⃗
⃗
⃗ ⃗
⃗
⃗
⃗
⃗
⃗ ⃗
⃗ ⃗
⃗
⃗
⃗ ⃗Preprintunderreview
2.2 DISCRETEDIFFUSIONMODELS
Indiscretediffusionmodels,oneturnstoconsideracontinuous-timeMarkovchain(x ) ina
t 0≤t≤T
spaceXoffinitecardinalityastheforwardprocess. Wedenotetheprobabilitydistributionofx by
t
avectorp ∈∆|X|,where∆|X|denotestheprobabilitysimplexinR|X|.Giventhetargetdistribution
t
p ,theMarkovchainsatisfiesthefollowingmasterequation:
0
dp
dtt =Q tp t, where Q t =(Q t(y,x)) x,y∈X ∈R|X|×|X| (2.7)
istheratematrixattimet. TheratematrixQ satisfiesthefollowingtwoconditions:
t
(cid:88)
(i)Q (x,x)=− Q (y,x), ∀x∈X; (ii)Q (x,y)≥0, ∀x̸=y ∈X.
t t t
y̸=x
In the following, we will use a shorthand notation Q(cid:101)t to denote the matrix Q
t
with the diagonal
elementssettozero. Itcanbeshownthatthecorrespondingbackwardprocessisofthesameform
butwithadifferentratematrix(Kelly,2011):
(cid:40)
dp p s(y)Q (x,y), ∀x̸=y ∈X,
dss =Q sp s, where Q s(y,x)= −p s( (cid:80)x) s
Q (y′,x), ∀x=y ∈X.
(2.8)
y′̸=x s
TheratematrixQ isoftenchosentopossesscertainsparsestructuressuchthattheforwardprocess
t
convergestoasimpledistributionthatiseasytosamplefrom. Severalpopularchoicesincludethe
uniformandabsorbingtransitions(Louetal.,2024).
Thecommonpracticeistodefinethescorefunction(orratherthescorevector)as
p
s t(x)=(s t(x,y)) y∈X :=
p
(t x), ∀x∈X,
t
andestimateitbyaneuralnetworksθ(x),wheretheneuralnetworkθistrainedbyminimizingthe
(cid:98)t
scoreentropy(Bentonetal.,2022;Louetal.,2024):
θ
=argmin(cid:90) T
ψ E
(cid:20) (cid:88)(cid:18) −logs (cid:98)θ t(x,y)
−1+
s (cid:98)θ t(x,y)(cid:19)
s (x,y)Q
(x,y)(cid:21)
dt
t xt∼pt s (x,y) s (x,y) t t
θ 0 y̸=x t t
(2.9)
=argmin(cid:90) T
ψ E
(cid:20) (cid:88)(cid:18) −p t(y|x 0) logsθ(x,y)+sθ(x,y)(cid:19)
Q
(x,y)(cid:21)
dt.
t x0∼p0 p (x|x ) (cid:98)t (cid:98)t t
θ 0 y̸=x t 0
Similar to the continuous case, the backward process is approximated by the continuous-time
θ
Markovchainwiththefollowingmasterequationwithq =p andratematrixQ(cid:98) :
0 ∞ s
dq s =Q(cid:98)θ q , where Q(cid:98)θ (y,x)=sθ(x,y)Q (x,y), ∀x̸=y ∈X. (2.10)
ds s s s (cid:98)s s
and sampling is accomplished by first sampling from the distribution p and then evolving the
∞
Markovchainaccordingly.
2.3 ERRORANALYSISOFCONTINUOUSDIFFUSIONMODELS
Beforeweproceedtotheerroranalysisofdiscretediffusionmodels, wefirstreviewthattheerror
analysis of continuous diffusion models, which is often conducted by considering the following
threeerrorterms:
• Truncation Error: The error caused by approximating p by p , which is often of the order
T ∞
O(dexp(−T))duetoexponentialergodicity;
• ApproximationError: Theerrorcausedbyapproximatingthescorefunction∇logp (x )bya
t t
neuralnetworksθ(x ),whichisoftenassumedtobeoforderO(ϵ),whereϵisasmallthreshold,
(cid:98)t t
givenathoroughtrainingprocess;
• Discretization Error: The error caused by numerically solving the SDE (2.6) with Euler-
Maruyamaschemeorotherschemes,e.g. exponentialintegrator(Zhang&Chen,2022).
4
⃗
⃗
⃗⃗
⃗
⃗
⃗Preprintunderreview
Thetotalerrorisobtainedfromthesethreeerrortermswithproperchoicesoftheorderofthetime
horizon T and the design of the numerical scheme. We extract the following theorem from the
state-to-the-arttheoreticalresult(Bentonetal.,2023a)forlatercomparison:
Theorem 2.1 (Error Analysis of Continuous Diffusion Models). Suppose the time discretization
scheme(s ) withs =0ands =T−δsatisfiess −s ≤κ(T−s )fork ∈[0:N−1].
i i∈[0,N] 0 N k+1 k k+1
Assumecov(p )=I,andthescorefunction∇logp (x )isestimatedbytheneuralnetworksθ(x )
0 t t (cid:98)t t
withϵ-accuracy,i.e.
N (cid:88)−1 (cid:20)(cid:13) (cid:13)2(cid:21)
(s −s )E (cid:13)∇logp (x )−sθ (x )(cid:13) ≤ϵ.
k+1 k xsk∼p sk (cid:13) sk sk (cid:98)sk sk (cid:13)
k=0
Thenunderthefollowingchoiceoftheorderofparameters
T =O(log(dϵ−1)), κ=O(d−1ϵ2log−1(dϵ−1)), N =O(dϵ−1log(dϵ−1)),
wehaveD (p ∥q )≤ϵ,whereq isthedistributionoftheapproximatebackwardprocess(2.6)
KL δ (cid:98)tN (cid:98)tN
implementedwithexponentialintegratorafterN steps.
3 STOCHASTIC INTEGRAL FORMULATION OF DISCRETE DIFFUSION
MODELS
In this section, we introduce the stochastic integral formulation of discrete diffusion models. The
goal is to establish a path evolution equation analogous to Itô integral (or equivalently, stochastic
differential equations), with the master equation (2.7) and (2.8) analogous to the Fokker-Planck
equation,inthecontinuouscase.
3.1 POISSONRANDOMMEASUREWITHEVOLVINGINTENSITY
Inthefollowing,thePoissondistributionwithexpectationλisdenotedbyP(λ).
Definition3.1(PoissonRandomMeasurewithEvolvingIntensity). Let(Ω,F,P)beaprobability
spaceand(X,B,ν)beameasurespaceandλ (y)isanon-negativepredictableprocessonR+×
t
X×ΩsatisfyingforanyT >0,(cid:82)T (cid:82) 1∨|y|∨|y|2λ (y)ν(dy)dt<∞, a.s..Therandommeasure
0 X t
N[λ](dt,dy)onR+×XiscalledaPoissonrandommeasurewithevolvingintensityλ (y)if
t
(cid:16) (cid:17)
(cid:82)t(cid:82)
(i) ForanyB ∈Band0≤s<t,N[λ]((s,t]×B)∼P λ (y)ν(dy)dτ ;
s B τ
(ii) For any t ≥ 0 and disjoint sets {B } ⊂ B, {N [λ](B ):=N[λ]((0,t]×B )} are
i i∈[n] t i i i∈[n]
independentstochasticprocesses.
Well-definednessofthePoissonrandommeasurewithevolvingintensityisnon-trivial.Theclassical
Poissonrandommeasureiswell-studiedbythetheoryofLévyprocesses(Applebaum,2009), and
theextensiontothestate-dependentintensityisproposedandanalyzedin(Glasserman&Merener,
2004). Notably, Li (2007) establishes the stochastic integral formulation for the chemical master
equation with the Poisson random measure with state-dependent intensity, which is a special case
oftheevolvingintensity,andsubsequentlyshowstheweakandstrongconvergenceoftheτ-leaping
scheme. Poisson random measures with evolving intensity are first discussed by the technique in-
troducedin(Protter,1983),andfurtherdetailsareprovidedinAppendixA.1.
In the following, we will denote the filtration generated by the Poisson random measure
N[λ](dt,dy) by (F ) . The Poisson random measure defined above admits similar properties
t t≥0
asthestandardPoissonrandommeasureandtheBrownianmotion. Inparticular,onecanextendthe
ItôintegraltotheLévy-typestochasticintegralw.r.t. Poissonrandommeasurewithevolvinginten-
sityforpredictableprocesses. ItalsoadmitsItôisometry,Itô’sformula(TheoremA.9),andLévy’s
characterizationtheorem(TheoremA.8),forwhichwereferreaderstoAppendixA.2fordetails.
Nowweturntothesettingofdiscretediffusionmodels, wherethestatespaceXisfiniteendowed
withthenaturalσ-algebraB =2Xandthecountingmeasureν =(cid:80) δ .
y∈X y
5
⃗ ⃗
⃗ ⃗ ⃗Preprintunderreview
Proposition3.2(StochasticIntegralFormulationofDiscreteDiffusionModels). Theforwardpro-
cessindiscretediffusionmodels(2.7)canberepresentedbythefollowingstochasticintegral:
(cid:90) t(cid:90)
x
t
=x 0+ (y−x t−)N[λ](dt,dy), withλ t(y)=Q(cid:101)t(y,x t−), (3.1)
0 X
and the backward process in discrete diffusion models (2.8) can be represented by the following
stochasticintegral:
(cid:90) s(cid:90)
x
s
=x 0+ (y−x s−)N[µ](ds,dy), withµ s(y)=s s(x s−,y)Q(cid:101)s(x s−,y), (3.2)
0 X
whereX denotestheleftlimitofacàdlàgprocessX attimet.
t− t
The proof of Proposition 3.2 is provided in
Appendix A.4. We would like to remark that 5 1
the stochastic integral formulation in Proposi-
tion 3.2 is tantalizingly close to the Itô inte- 4
gralincontinuousdiffusionmodelsintheform
of stochastic differential equations (cf. (2.1)
3 0.5
and (2.2)). Recalling that in the continuous
case, Girsanov’s theorem is applied to SDEs
for deriving the score-matching loss (2.4) and 2
also for the error analysis by associating the
loss with the KL divergence, one may wonder 1 0
0.0 0.2 0.4 0.6 0.8 1.0
ifsimilartechniquescanbeappliedtodiscrete
t
diffusion models. The following section gives
Figure 1: Example trajectories of stochastic in-
an affirmative answer to this question by pro-
tegrals (3.1) w.r.t. Poisson random measure with
viding a change of measure for Poisson ran-
differentevolvingintensities.Theintensityischo-
dom measures with evolving intensity, which senasλ (y)=50f if|y−x |=1orotherwise
t t t−
is the theoretical foundation of our stochastic 0, as shown in dashed lines. Intuitively, λ con-
t
integral-basederroranalysisframeworkfordis- trolstherateofjumpsattimetandlocationy.
cretediffusionmodels.
3.2 CHANGEOFMEASURE
Thefollowingtheoremprovidesachange-of-measureargumentforstochasticintegralsw.r.t. Pois-
sonrandommeasureswithevolvingintensity,analogoustoGirsanov’stheoremforItôintegralsw.r.t.
Brownianmotions.
Theorem 3.3 (Change of Measure for Poisson Random Measure with Evolving Density). Let
N[λ](dt,dy) be a Poisson random measure with evolving intensity λ (y) in the probability space
t
(Ω,F,P), and h (y) be a positive predictable process on R+ × X × Ω. Suppose the following
t
exponentialprocessisalocalF -martingale:
t
(cid:18)(cid:90) t(cid:90) (cid:90) t(cid:90) (cid:19)
Z [h]:=exp logh (y)N[λ](dt×dy)− (h (y)−1)λ (y)ν(dy) , (3.3)
t t t t
0 X 0 X
andQisanotherprobabilitymeasureon(Ω,F)suchthatQ ≪ PwithRadon-Nikodymderivative
dQ/dP| = Z [h]. Then the Poisson random measure N[λ](dt,dy) under the measure Q is a
Ft t
Poissonrandommeasurewithevolvingintensityλ (y)h (y).
t t
Remark 3.4. One may verify that the exponential process Z in (3.3) is a local F -martingale
t t
byApplebaum(2009,Corollary5.2.2)undermildassumptionsonthefunctionh (y)(cf. Novikov’s
t
conditionintheGirsanov’stheoremforItôintegrals).
Then it is straightforward to derive the following corollary, which was derived in (Benton et al.,
2022) with a different technique with Feller processes and adopted in (Lou et al., 2024; Chen &
Ying,2024)inthedesignoflossfunctionsfortheneuralnetworktraining:
Corollary3.5(EquivalencebetweenKLDivergenceandScoreEntropy-basedLossFunction). Let
p and q be the path measures of the backward process (2.8) and the approximate backward
0:T 0:T
6
⃗
⃗ ⃗ ⃗
tx
⃗ ⃗ ⃗
tfPreprintunderreview
process(2.10),thenitholdsthat
D (p ∥q )≤D (p ∥q )
KL T T KL 0:T 0:T
(cid:34) (cid:32) (cid:33) (cid:35)
(cid:90) T (cid:90) sθ(x ,y) (3.4)
=D KL(p 0∥q 0)+E K (cid:98) ss (xs−
,y)
s s(x s−,y)Q(cid:101)s(x s−,y)ν(dy)dt ,
0 X s s−
whereK(x) = x−1−logx ≥ 0,andtheexpectationistakenw.r.t. pathsgeneratedbytheback-
wardprocess(3.2). Consequently,minimizingthelossfunction(2.9)fordiscretediffusionmodelsis
equivalenttominimizingtheKLdivergencebetweenthepathmeasuresofthegroundtruthandthe
approximatebackwardprocess.
Proofsofthechangeofmeasure-relatedargumentsabovewillbeprovidedinAppendixA.3. Intu-
itively,Theorem3.3depictstherelationsbetweenthelikelihoodratio(Z [h])oftwopathsgenerated
t
bytwoPoissonrandommeasureswithdifferentintensities. Oneshouldrecallthatinthecontinuous
casewithItôintegrals,theproximityoftwopathsinKLdivergenceonlyrequiresasmalldifference
betweenthedrifttermsbyGirsanov’stheorem,andtherefore,thescorefunctioncanbetrainedwith
the mean squared error loss (2.4) (Song et al., 2020), while in the discrete case, the proximity of
twopathsrequiresthelikelihoodratiotobeclosetoone,accountingforamorecomplicatedscore
entropydesignintheloss(2.9)(Bentonetal.,2022;Louetal.,2024).
4 ERROR ANALYSIS OF DISCRETE DIFFUSION MODELS
In this section, we firstly review two different implementations of the discrete diffusion models,
namely τ-leaping (Gillespie, 2001) and uniformization (Van Dijk, 1992), derive their stochastic
integralformulationsasinProposition3.2,andprovideourmainresultsfortheirerroranalysis.
4.1 ALGORITHMS
4.1.1 τ-LEAPING.
Astraightforwardalgorithmforsimulatingthebackwardprocessistodiscretizetheintegralin(3.2)
withanEuler-Maruyamascheme;thisleadstotheτ-leapingalgorithmsummarizedinAlgorithm1.
Algorithm1:τ-LeapingAlgorithmforDiscreteDiffusionModelInference
Input: y ∼q ,timediscretizationscheme(s ) withs =0ands =T −δ,intensity
(cid:98)0 0 i i∈[0:N] 0 N
functionµθ definedinProposition4.1,andneuralnetwork-basedscorefunction
(cid:98)s
estimationsθ.
(cid:98)t
Output: Asampley ∼q .
(cid:98)sN (cid:98)tN
forn=0toN −1do
1
(cid:88)
2 y ← (y−y )P(µθ(y )(s −s )); (4.1)
(cid:98)sn+1 (cid:98)sn (cid:98)s (cid:98)sn n+1 n
y∈X
end
3
Asshowninthefollowingproposition,τ-leapingcanbeformulatedasastochasticintegral.
Proposition 4.1 (Stochastic Integral Formulation of τ-Leaping). The τ-leaping algorithm (Algo-
rithm1)isequivalenttosolvingthefollowingstochasticintegralequation:
(cid:90) s(cid:90)
y =y + (y−y )N[µθ ](ds,dy), (4.2)
(cid:98)s (cid:98)0 (cid:98)⌊s⌋− (cid:98)⌊·⌋
0 X
wheretheevolvingintensityµθ(y)isgivenby
(cid:98)s
µ (cid:98)θ ⌊s⌋(y)=s (cid:98)θ ⌊s⌋(y (cid:98)⌊s⌋−,y)Q(cid:101)⌊s⌋(y (cid:98)⌊s⌋−,y)=µ (cid:98)θ sn(y),
inwhichweusedthesymbol⌊s⌋=s fors∈[s ,s ). Wewillcalltheprocessy theinterpolat-
n n n+1 (cid:98)s
ingprocessoftheτ-leapingalgorithmanddenotethedistributionofy byq .
(cid:98)s (cid:98)s
7
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗ ⃗ ⃗Preprintunderreview
4.1.2 UNIFORMIZATION
Another algorithm considered for simulating the backward process in discrete diffusion models is
uniformization. The algorithm is summarized in Algorithm 2, in which σ denotes the m-th
(m)
orderstatisticoftheM uniformrandomvariableson[0,1],andtherandomnessin(4.3)shouldbe
understoodassamplingacategoricaldistributionandupdatingthestateaccordingly.
Algorithm2:UniformizationAlgorithmforDiscreteDiffusionModelInference
Input: y ∼q ,timediscretizationscheme(s ) withs =0ands =T −δ,intensity
(cid:98)0 0 b b∈[0,N] 0 B
upperboundprocessλ ,intensityfunctionµθ definedinProposition4.1,andneural
s (cid:98)s
network-basedscorefunctionestimationsθ.
(cid:98)t
Output: Asamplex ∼q .
sB tB
forb=0toB−1do
1
M ∼P(λ (s −s )),σ ∼Unif([0,1])form∈[M];
2 sb+1 b+1 b m
form=1toM do
3
(cid:40)
4 y, withprob. µθ (y)/λ , fory ∈X,
y ←
(cid:98)sb+σ(m) sb+1
(4.3)
(cid:98)sb+σ(m) y , withprob. 1−(cid:80) µθ (y)/λ ;
(cid:98)sb y∈X(cid:98)sb+σ(m) sb+1
end
5
end
6
ThemainideaistosimulatethebackwardprocessbyaPoissonrandommeasurewithapiecewise
constantintensityupperboundprocessandthensamplethebehaviorofeachjumpaccordingtothe
intensityµθ(y)attimes.Theuniformizationalgorithmalsoadmitsastochasticintegralformulation,
(cid:98)s
asshowninthefollowingproposition.
Proposition4.2(StochasticIntegralFormulationofUniformization). Undertheblockdiscretization
scheme (s ) with s = 0 and s = T − δ, and for any s ∈ (s ,s ], we define the
b b∈[0,B] 0 B b b+1
piecewiseconstantintensityupperboundprocessbyλ = sup (cid:82) µθ(y)ν(dy).Thenthe
s s∈(sb,sb+1] X(cid:98)s
uniformization algorithm (Algorithm 2) is equivalent to solving the following stochastic integral
equationintheaugmentedmeasurespace(X×[0,λ],B⊗B([0,λ]),ν⊗m):
(cid:90) s(cid:90) (cid:90)
y s =y 0+
0 X
R(y−y s−)1 0≤ξ≤(cid:82) Xµ(cid:98)θ s(y)ν(dy)N[µ (cid:98)θ](ds,dy,dξ), (4.4)
wheretheevolvingintensityµ (cid:98)θ s(y)isgivenbyµ (cid:98)θ s(y)=s (cid:98)θ s(y (cid:98)s−,y)Q(cid:101)s(y (cid:98)s−,y).
BasedonProposition4.2, onecanshowthattheuniformizationalgorithmsimulatesthebackward
process in discrete diffusion models accurately (cf. Theorem A.11), and the proofs of the claims
abovewillbeprovidedinAppendixA.4.
4.2 ASSUMPTIONS
Weneedthefollowingassumptionstoensurethewell-definednessofdiscretediffusionmodels. For
simplicity, we assume the rate matrix Q is time-homogeneous and symmetric, i.e. Q = Q for
t t
anyt ≥ 0. Infact,theresultscanbeeasilyextendedtothetime-inhomogeneouscaseofthefamily
Q =β Qwitharescalingfactorβ ,andasymmetriccaseswillbeleftforfutureworks.
t t t
Assumption4.3(RegularityoftheRateMatrix). TheratematrixQsatisfiesthefollowingcondi-
tions:
(i) Foranyx,y ∈X,Q(x,y)≤C andD ≤−Q(x,x)≤DforsomeconstantsC,D,D >0;
(ii) The modified log-Sobolev constant ρ(Q) of the rate matrix Q (cf. Definition B.6) is lower
boundedbyρ>0.
Statement (i) assumes the regularity of the rate matrix, which is often trivially satisfied in many
applications, while Statement (ii) ensures the exponential convergence of the forward process in
discretediffusionmodels. Ingeneral,ρ(Q)maydependontheconnectivityandotherstructuresof
8
⃗Preprintunderreview
thecorrespondinggraphG(Q)(cf.DefinitionB.1).Suchlowerboundhasbeenobtainedforspecific
graphs(e.g. ExampleB.10andB.11),andgeneralresultsareinactiveresearch(Saloff-Coste,1997;
Bobkov&Tetali,2006). WereferreaderstoAppendixBforfurtherdiscussionsontheliteratureof
themodifiedlog-Sobolevconstant,aswellasitsrelationtothespectralgap,themixingtime,etc..
Assumption4.4(BoundedScore). Thetruescorefunctionsatisfiess (x,y) ≲ 1∨t−1, whilethe
t
learnedscorefunctionsatisfiessθ(x,y)∈(0,M],foranyx,y ∈X.
(cid:98)s
ThefirstpartontheasymptoticbehaviorofthetruescorecorrespondstotheestimationE[∥s ∥2]∼
t
E[∥x −µ ∥2/σ2]∼1∨t−1inthecontinuouscase(Chenetal.,2023a,Assumption1)andfurther
t t t
justificationisprovidedinRemarkB.3. Theboundontheestimatedscorecanbeeasilysatisfiedby
addingtruncationinpost-processingintheimplementationoftheNN-basedscoreestimator.
Assumption4.5(ContinuityofScoreFunction). Foranyt>0andy ∈XsuchthatQ(x ,y)>0,
t−
(cid:12) (cid:12) (cid:12) (cid:12)
wehave(cid:12)µ t+(y)(cid:12):=(cid:12)pt(x t−)Q(xt,y) −1(cid:12)≲1∨t−γ,forsomeexponentγ ∈[0,1].
(cid:12) µt(y) (cid:12) (cid:12)pt(xt)Q(x t−,y) (cid:12)
Assumption4.5correspondstotheLipschitzcontinuityofthescorefunction(cf.(Chenetal.,2022,
Assumption1),(Chenetal.,2023a,Assumption3))forcontinuousdiffusionmodels,andisinlight
of the postulation that adjacent vertices should have close score function and intensity values. In
(cid:12) (cid:12)
theworsecase,assumeQ(x,y) = Θ(1),thenanaïveboundwouldbe(cid:12) (cid:12)µ µt+ t(( yy ))(cid:12)
(cid:12)
≲ |s t(x t,x t−)| ≲
1∨t−1withγ =1. However,whentheinitialdistributionisbothupperandlowerbounded,γ may
beassmallas0,andweplantoinvestigatehowthis(local)continuityofthescorefunctionaffects
theoverallperformanceofdiscretediffusionmodels.
Assumption4.6(ϵ-accurateScoreEstimation). Thescorefunctions (x )isestimatedbytheneural
t t
networksθ(x )withϵ-accuracy,i.e.
(cid:98)t t
N n(cid:88) =− 01 (s n+1−s n)E(cid:34) (cid:90) XK(cid:32) ss (cid:98) sθ s nn ((x xs s− n
−
n, ,y y) )(cid:33) s sn(x
s−
n,y)Q(cid:101)(x
s−
n,y)ν(dy)(cid:35) ≤ϵ.
This assumption assumes the expressive power and sufficient training of the NN-based score esti-
mator and is standard in diffusion model-related theories (Chen et al., 2022; 2023a; Benton et al.,
2023a;Chenetal.,2024c).
4.3 ERRORANALYSIS
OurmainresultsarepresentedbelowforeachalgorithmintroducedinSection4.1.
4.3.1 τ-LEAPING
Theorem 4.7 (Error Analysis of τ-Leaping). Suppose the time discretization scheme (s )
i i∈[0,N]
withs =0ands =T −δsatisfiesfork ∈[0:N −1],s −s
≤κ(cid:0)
1∨(T −s
)1+γ−η(cid:1)
,
0 N k+1 k k+1
where the exponent η satisfies γ < η ≲ 1−T−1 when γ < 1, and η = 1 when γ = 1. Under
Assumptions4.3,4.4,4.5,and4.6,wehavethefollowingerrorbound
D (p ∥q
)≲exp(−ρT)log|X|+ϵ+D2
κT,
KL δ (cid:98)T−δ
andunderthefollowingchoiceoftheorderofparameters:
(cid:32) log(cid:0) ϵ−1log|X|(cid:1)(cid:33) (cid:32)
ϵρ
(cid:33) (cid:26)
0, γ <1,
T =O , κ=O , δ = √ (4.5)
ρ D2 log(ϵ−1log|X|) Ω(e− T), γ =1,
where the mixing time t is defined in Definition B.12, we have D (p ∥q ) ≲ ϵ with N =
mix KL δ (cid:98)T−δ
(cid:18) (cid:19)
κ−1T =O
D2ρ2log2(ϵ−1log|X|)
totalsteps.
ϵ
Thederivation(asprovidedinAppendixC.3)andconclusionsareanalogoustotheerrorboundfor
continuous diffusion models (cf. Theorem 2.1), as a summation of the truncation, approximation,
anddiscretizationerrorsasoutlinedinSection2.3. Wewouldliketopointoutthemaindifferences
betweenthecontinuousanddiscretediffusionmodels:
9
⃗
⃗
⃗
⃗
⃗ ⃗ ⃗Preprintunderreview
• Truncation Error: While the Ornstein-Uhlenbeck process converges exponentially fast in the
continuousdiffusionmodels,theexponentialconvergenceoftheforwardprocessindiscretedif-
fusionmodelsisnon-trivialforgeneralgraphsG(Q),forwhichthelowerboundonthemodified
log-Sobolevconstantρisoneofthesufficientconditions.Inpractice,theexponentialconvergence
oftheforwardprocessshouldbeverifiedforthespecificproblemathand;
• DiscretizationError: Incontinuousdiffusionmodels, theanalysisofthediscretizationerroris
basedontheItôintegralandGirsanov’stheorem,whilefordiscretediffusionmodels,thestochas-
ticintegralframework,includingthePoissonrandommeasurewithevolvingintensity(cf. Defini-
tion3.1)andchangeofmeasure(cf. Theorem3.3)thatwedevelopedabove,isemployedinstead.
Remark4.8(RemarkonEarlyStopping). AsinAssumption4.4,thetruescorefunctionmayexhibit
singularbehaviorass → T, duetopossiblevacancyinthetargetdistributionp . Tohandlethis
0
singularity,twodifferentregimesareconsideredforthetimediscretizationschemedependingonthe
continuityparameterγ ofthescorefunction(Assumption4.5). Themainintuitionisthat(a)inthe
worsecaseγ =1,earlystoppingattimes=T −δisnecessary;(b)ifthetargetdistributionp is
0
suchwell-posed(e.g. bothupperandlowerbounded)andtheratematrixQisconstructedinaway
thatthescoreexhibitscertain(local)continuityreflectedbyγ <1,onemaychooseanappropriate
shrinkageη,withwhichfinitediscretizationerrorcanbeachievedwithfinitesteps.
InTheorem4.7,thecoefficientDroughlytranslatestothedimensiondwhenthediscretediffusion
modelisappliedtoX=[S]d,whereS isthenumberofstatesalongeachdimension. Forexample,
wheneachstateisconnectedtothoseataManhattandistanceof1withunitweight,wehaveC =1,
D =2d,log|X|=dlogS holdinAssumption4.3. PluggingD =log|X|=O(d)intotheresults,
weobtainthatthetotalnumberofstepsN = O(cid:101)(d2),whereO(cid:101) denotestheorderuptologarithmic
factors.Thisrecoversthedependencydescribedin(Campbelletal.,2022,Theorem1)forτ-leaping
with a completely different set of techniques, and importantly, our results do not rely on strong
assumptions such as a uniform bound on the true score. We also reduce assumption stringency
by relating our assumption on the estimation error (Assumption 4.6) more closely to the practical
traininglossratherthanrequiringanL∞-accuratescoreestimationerror. Mostnotably,weprovide
the first convergence guarantees for τ-leaping in KL divergence, which is stronger than the total
variationdistance,fordiscretediffusionmodels.
4.3.2 UNIFORMIZATION
The error analysis of the uniformization algorithm requires the following modified assumption on
theaccuracyofthelearnedscorefunctionsθ(x ):
(cid:98)t t
Assumption4.6’(ϵ-accurateLearnedScore). Thescorefunctions (x )isestimatedbytheneural
t t
networksθ(x )withϵ-accuracy,i.e.
(cid:98)t t
(cid:34) (cid:32) (cid:33) (cid:35)
(cid:90) T−δ(cid:90) sθ(x ,y)
E K (cid:98) ss (xs−
,y)
s s(x s−,y)Q(cid:101)(x s−,y)ν(dy) ds≤ϵ.
0 X s s−
Theorem 4.9 (Error Analysis of Uniformization). Suppose the block discretization scheme
(s ) with s = 0 and s = T − δ satisfies for k ∈ [0 : N − 1], s − s ≤
b b∈[0,N] 0 N k+1 k
κ(1∨(T −s )).UnderAssumptions4.3,4.4,4.5,and4.6’,wehavethefollowingerrorbound
k+1
D (p ∥q )≲exp(−ρT)log|X|+ϵ,
KL δ T−δ
(cid:18) (cid:19)
log(ϵ−1log|X|)
where the mixing time t is defined in Definition B.12. Then with T = O
mix ρ
and the early stopping scheme δ = Ω(e−T), we have D (p ∥q ) ≲ ϵ with E[N] =
KL δ T−δ
(cid:18) (cid:19)
Dlog(ϵ−1log|X|)
O steps.
ρ
The proof of Theorem 4.9 is deferred to Appendix C.3. Following a similar argument for Theo-
rem4.7,thedimensionalitydependencyoftheuniformizationschemeisO(cid:101)(d),confirmingtheresult
for the special case X = {0,1}d in (Chen & Ying, 2024) and X = [S]d in (Zhang et al., 2024).
Theorem4.7and4.9offeradirectcomparisonoftheefficiencyoftheτ-leapinganduniformization
10
⃗
⃗
⃗
⃗
⃗ ⃗ ⃗Preprintunderreview
implementations for discrete diffusion models. Our proof reveals that the less favorable quadratic
dependency in the τ-leaping scheme arises from the truncation error, which is not present in the
uniformization scheme, illustrating a possible advantage of the latter in reducing computational
complexity.
Recallingthecurrentstate-of-the-artresultforcontinuousdiffusionmodels(Theorem2.1)isO(cid:101)(d),
weconjecturethatO(cid:101)(d)isalsotheoptimalrateinthediscretecase.Inthecontinuouscase,thelinear
dependencydoesnotdependontheaccuratesimulationoftheapproximatebackwardprocess(2.10)
(or(3.2))andisachievablewithEuler-Maruyamaschemes. Theproofwasviaanintricatestochas-
tic localization argument (Benton et al., 2023a), with which the bound on E[∥∇2logp (x )∥2] is
t t
improvedbyaO(d)-factorfromO(d2). Thecorrespondingargumentfortheτ-leapingschemeof
discretediffusionmodelswouldbeapossiblerefinementonPropositionC.2,whichwebelieveisof
independentinterestandwillbeexploredinfuturework.
5 CONCLUSION
Inthispaper,wehavedevelopedacomprehensiveframeworkfortheerroranalysisofdiscretedif-
fusionmodels. WerigorouslyintroducedthePoissonrandommeasurewithevolvingintensityand
established the Lévy-type stochastic integral alongside change of measure arguments. These ad-
vancements not only hold mathematical significance but also facilitate a clear-cut analysis of dis-
crete diffusion models. Moreover, we demonstrated that the inference process can be formulated
asastochasticintegralusingthePoissonrandommeasurewithevolvingintensity,allowingtheer-
rortobesystematicallydecomposedandoptimizedbyalgorithmicdesign,mirroringthetheoretical
framework for continuous diffusion models. Our framework unifies the error analysis of discrete
diffusionmodelsandprovidesthefirsterrorboundsfortheτ-leapingschemeinKLdivergence.
Our results lay a theoretical groundwork for the design and analysis of discrete diffusion models,
adaptabletobroadercontexts, suchastime-inhomogeneousandnon-symmetricratematrices. Fu-
tureresearchdirectionsincludeexploringthecontinuumlimitofdiscretediffusionmodelsinstate
spacesandtime(Winkleretal.,2024;Zhaoetal.,2024)andhowtoacceleratetheimplementation
viaparallelsampling(Shihetal.,2024;Chenetal.,2024a;Guptaetal.,2024). Wehopeourwork
will inspire further research on both the theoretical analysis and practical applications of discrete
diffusionmodelsinvariousfields.
ACKNOWLEDGMENTS
LexingYingacknowledgesthesupportoftheNationalScienceFoundationunderGrantNo. DMS-
2208163.
REFERENCES
AmiraAlakhdar,BarnabasPoczos,andNewellWashburn.Diffusionmodelsindenovodrugdesign.
JournalofChemicalInformationandModeling,2024.
SarahAlamdari, NityaThakkar, RiannevandenBerg, AlexXLu, NicoloFusi, AvaPAmini, and
KevinKYang. Proteingenerationwithevolutionarydiffusion:sequenceisallyouneed. BioRxiv,
pp.2023–09,2023.
Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic inter-
polants. arXivpreprintarXiv:2209.15571,2022.
MichaelSAlbergo,NicholasMBoffi,andEricVanden-Eijnden.Stochasticinterpolants:Aunifying
frameworkforflowsanddiffusions. arXivpreprintarXiv:2303.08797,2023.
David F Anderson, Arnab Ganguly, and Thomas G Kurtz. Error analysis of tau-leap simulation
methods. 2011.
DavidApplebaum. Lévyprocessesandstochasticcalculus. Cambridgeuniversitypress,2009.
11Preprintunderreview
JacobAustin,DanielDJohnson,JonathanHo,DanielTarlow,andRianneVanDenBerg.Structured
denoisingdiffusionmodelsindiscretestate-spaces. AdvancesinNeuralInformationProcessing
Systems,34:17981–17993,2021.
PavelAvdeyev,ChenlaiShi,YuhaoTan,KseniiaDudnyk,andJianZhou. Dirichletdiffusionscore
modelforbiologicalsequencegeneration. InInternationalConferenceonMachineLearning,pp.
1276–1301.PMLR,2023.
Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. From
denoisingdiffusionstodenoisingmarkovmodels. arXivpreprintarXiv:2211.03595,2022.
Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear convergence
boundsfordiffusionmodelsviastochasticlocalization. arXivpreprintarXiv:2308.03686,2023a.
JoeBenton, GeorgeDeligiannidis, andArnaudDoucet. Errorboundsforflowmatchingmethods.
arXivpreprintarXiv:2305.16860,2023b.
AdamBlock,YoussefMroueh,andAlexanderRakhlin. Generativemodelingwithdenoisingauto-
encodersandlangevinsampling. arXivpreprintarXiv:2002.00107,2020.
SergeyGBobkovandMichelLedoux. Onmodifiedlogarithmicsobolevinequalitiesforbernoulli
andpoissonmeasures. Journaloffunctionalanalysis,156(2):347–365,1998.
SergeyGBobkovandPrasadTetali. Modifiedlogarithmicsobolevinequalitiesindiscretesettings.
JournalofTheoreticalProbability,19:289–336,2006.
AndrewCampbell,JoeBenton,ValentinDeBortoli,ThomasRainforth,GeorgeDeligiannidis,and
ArnaudDoucet.Acontinuoustimeframeworkfordiscretedenoisingmodels.AdvancesinNeural
InformationProcessingSystems,35:28266–28279,2022.
AndrewCampbell, JasonYim, ReginaBarzilay, TomRainforth, andTommiJaakkola. Generative
flowsondiscretestate-spaces: Enablingmultimodalflowswithapplicationstoproteinco-design.
arXivpreprintarXiv:2402.04997,2024.
LukeCauser,GrantMRotskoff,andJuanPGarrahan. Discretegenerativediffusionmodelswithout
stochastic differential equations: a tensor network approach. arXiv preprint arXiv:2407.11133,
2024.
Stanley H Chan. Tutorial on diffusion models for imaging and vision. arXiv preprint
arXiv:2403.18103,2024.
HaoxuanChen,YinuoRen,LexingYing,andGrantMRotskoff.Acceleratingdiffusionmodelswith
parallel sampling: Inference at sub-linear time complexity. arXiv preprint arXiv:2405.15986,
2024a.
Hongrui Chen and Lexing Ying. Convergence analysis of discrete diffusion model: Exact imple-
mentationthroughuniformization. arXivpreprintarXiv:2402.08095,2024.
HongruiChen,HoldenLee,andJianfengLu.Improvedanalysisofscore-basedgenerativemodeling:
User-friendly bounds under minimal smoothness assumptions. In International Conference on
MachineLearning,pp.4735–4763.PMLR,2023a.
MinshuoChen,SongMei,JianqingFan,andMengdiWang. Anoverviewofdiffusionmodels: Ap-
plications,guidedgeneration,statisticalratesandoptimization.arXivpreprintarXiv:2404.07771,
2024b.
SitanChen,SinhoChewi,JerryLi,YuanzhiLi,AdilSalim,andAnruRZhang. Samplingisaseasy
aslearningthescore: theoryfordiffusionmodelswithminimaldataassumptions. arXivpreprint
arXiv:2209.11215,2022.
Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-degradation beyond linear diffusions:
A non-asymptotic analysis for ddim-type samplers. In International Conference on Machine
Learning,pp.4462–4484.PMLR,2023b.
12Preprintunderreview
Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability
flowodeisprovablyfast. AdvancesinNeuralInformationProcessingSystems,36,2024c.
Seunggeun Chi, Hyung-gun Chi, Hengbo Ma, Nakul Agarwal, Faizan Siddiqui, Karthik Ramani,
andKwonjoonLee. M2d2m: Multi-motiongenerationfromtextwithdiscretediffusionmodels.
arXivpreprintarXiv:2407.14502,2024.
David R Cox. Some statistical methods connected with series of events. Journal of the Royal
StatisticalSociety: SeriesB(Methodological),17(2):129–157,1955.
DoHuuDat,DoDucAnh,AnhTuanLuu,andWrayBuntine. Discretediffusionlanguagemodel
forlongtextsummarization. arXivpreprintarXiv:2407.10998,2024.
OscarDavis,SamuelKessler,MirceaPetrache,AvishekJoeyBose,etal. Fisherflowmatchingfor
generativemodelingoverdiscretedata. arXivpreprintarXiv:2405.14664,2024.
PersiDiaconisandLaurentSaloff-Coste. Logarithmicsobolevinequalitiesforfinitemarkovchains.
TheAnnalsofAppliedProbability,6(3):695–750,1996.
AndreasEberle. Markovprocesses. LectureNotesatUniversityofBonn,2009.
Andreas Eberle. Stochastic analysis. http://www.mi.unikoeln.de/stochana/
ws1617/Eberle/StochasticAnalysis2015.pdf,2015.
Patrick Emami, Aidan Perreault, Jeffrey Law, David Biagioni, and Peter St John. Plug & play
directed evolution of proteins with gradient-based discrete mcmc. Machine Learning: Science
andTechnology,4(2):025014,2023.
JiahaoFan,ZiyaoLi,EricAlcaide,GuolinKe,HuaqingHuang,andEWeinan. Accurateconforma-
tionsamplingviaproteinstructuraldiffusion. bioRxiv,pp.2024–05,2024.
GriffinFloto, ThorsteinnJonsson, MihaiNica, ScottSanner, andEricZhengyuZhu. Diffusionon
theprobabilitysimplex. arXivpreprintarXiv:2309.02530,2023.
NathanCFrey,DanielBerenberg,KarinaZadorozhny,JosephKleinhenz,JulienLafrance-Vanasse,
IsidroHotzel,YanWu,StephenRa,RichardBonneau,KyunghyunCho,etal. Proteindiscovery
withdiscretewalk-jumpsampling. arXivpreprintarXiv:2306.12360,2023.
XuefengGaoandLingjiongZhu. Convergenceanalysisforgeneralprobabilityflowodesofdiffu-
sionmodelsinwassersteindistances. arXivpreprintarXiv:2401.17958,2024.
DanielTGillespie. Approximateacceleratedstochasticsimulationofchemicallyreactingsystems.
TheJournalofchemicalphysics,115(4):1716–1733,2001.
PaulGlassermanandNicolasMerener. Convergenceofadiscretizationschemeforjump-diffusion
processes with state–dependent intensities. Proceedings of the Royal Society of London. Series
A:Mathematical,PhysicalandEngineeringSciences,460(2041):111–127,2004.
SharadGoel.Modifiedlogarithmicsobolevinequalitiesforsomemodelsofrandomwalk.Stochastic
processesandtheirapplications,114(1):51–79,2004.
ShansanGong,MukaiLi,JiangtaoFeng,ZhiyongWu,andLingpengKong. Diffuseq-v2: Bridging
discrete and continuous text spaces for accelerated seq2seq diffusion models. arXiv preprint
arXiv:2310.05793,2023.
LeonardGross. Logarithmicsobolevinequalities. AmericanJournalofMathematics,97(4):1061–
1083,1975.
NateGruver,SamuelStanton,NathanFrey,TimGJRudner,IsidroHotzel,JulienLafrance-Vanasse,
Arvind Rajpal, Kyunghyun Cho, and Andrew G Wilson. Protein design with guided discrete
diffusion. Advancesinneuralinformationprocessingsystems,36,2024.
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and
Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of
theIEEE/CVFconferenceonComputerVisionandPatternRecognition,pp.10696–10706,2022.
13Preprintunderreview
Zhiye Guo, Jian Liu, Yanli Wang, Mengrui Chen, Duolin Wang, Dong Xu, and Jianlin Cheng.
Diffusionmodelsinbioinformaticsandcomputationalbiology. Naturereviewsbioengineering,2
(2):136–154,2024.
ShivamGupta,LindaCai,andSitanChen. Fasterdiffusion-basedsamplingwithrandomizedmid-
points: Sequentialandparallel. arXivpreprintarXiv:2406.00924,2024.
Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusion-
bert: Improving generative masked language models with diffusion models. arXiv preprint
arXiv:2211.15029,2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
Emiel Hoogeboom, Alexey A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and
TimSalimans. Autoregressivediffusionmodels. arXivpreprintarXiv:2110.02037,2021a.
EmielHoogeboom,DidrikNielsen,PriyankJaini,PatrickForré,andMaxWelling. Argmaxflows
and multinomial diffusion: Learning categorical distributions. Advances in Neural Information
ProcessingSystems,34:12454–12465,2021b.
Minghui Hu, Yujie Wang, Tat-Jen Cham, Jianfei Yang, and Ponnuthurai N Suganthan. Global
contextwithdiscretediffusioninvectorquantisedmodellingforimagegeneration.InProceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11502–11511,
2022.
DanielZhengyuHuang,JiaoyangHuang,andZhengjiangLin. Convergenceanalysisofprobability
flowodeforscore-basedgenerativemodels. arXivpreprintarXiv:2404.09730,2024.
IliaIgashov,ArneSchneuing,MarwinSegler,MichaelBronstein,andBrunoCorreia. Retrobridge:
Modelingretrosynthesiswithmarkovbridges. arXivpreprintarXiv:2308.16212,2023.
Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. Layoutdm:
Discrete diffusion model for controllable layout generation. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.10167–10176,2023.
Jean Jacod and Albert Shiryaev. Limit theorems for stochastic processes, volume 288. Springer
Science&BusinessMedia,2013.
FrankPKelly. Reversibilityandstochasticnetworks. CambridgeUniversityPress,2011.
ThomasJKerbyandKevinRMoon.Training-freeguidancefordiscretediffusionmodelsformolec-
ulargeneration. arXivpreprintarXiv:2409.07359,2024.
Steven P Lalley. Continuous time markov chains. Lecture Notes, University of Chicago, pp. 34,
2012.
GünterLastandMathewPenrose.LecturesonthePoissonprocess,volume7.CambridgeUniversity
Press,2017.
Michel Ledoux. Concentration of measure and logarithmic sobolev inequalities. In Seminaire de
probabilitesXXXIII,pp.120–216.Springer,2006.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with
polynomialcomplexity. AdvancesinNeuralInformationProcessingSystems,35:22870–22882,
2022.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for
general data distributions. In International Conference on Algorithmic Learning Theory, pp.
946–985.PMLR,2023.
Tzong-YowLeeandHorng-TzerYau. Logarithmicsobolevinequalityforsomemodelsofrandom
walks. TheAnnalsofProbability,26(4):1855–1873,1998.
14Preprintunderreview
Jose Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Discrete
predictor-correctordiffusionmodelsforimagesynthesis. InTheEleventhInternationalConfer-
enceonLearningRepresentations,2022.
GenLi, YutingWei, YuxinChen, andYuejieChi. Towardsfasternon-asymptoticconvergencefor
diffusion-basedgenerativemodels. arXivpreprintarXiv:2306.09251,2023.
GenLi,YuHuang,TimofeyEfimov,YutingWei,YuejieChi,andYuxinChen. Acceleratingconver-
genceofscore-baseddiffusionmodels,provably. arXivpreprintarXiv:2403.03852,2024a.
Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards non-asymptotic convergence for
diffusion-based generative models. In The Twelfth International Conference on Learning Rep-
resentations,2024b.
Gen Li, Yuting Wei, Yuejie Chi, and Yuxin Chen. A sharp convergence theory for the probability
flowodesofdiffusionmodels. arXivpreprintarXiv:2408.02320,2024c.
Tiejun Li. Analysis of explicit tau-leaping schemes for simulating chemically reacting systems.
MultiscaleModeling&Simulation,6(2):417–436,2007.
XinerLi,YulaiZhao,ChenyuWang,GabrieleScalia,GokcenEraslan,SuragNair,TommasoBian-
calani,AvivRegev,SergeyLevine,andMasatoshiUehara.Derivative-freeguidanceincontinuous
anddiscretediffusionmodelswithsoftvalue-baseddecoding. arXivpreprintarXiv:2408.08252,
2024d.
Yang Li, Jinpei Guo, Runzhong Wang, and Junchi Yan. From distribution learning in training
to gradient search in testing for combinatorial optimization. Advances in Neural Information
ProcessingSystems,36,2024e.
Yuchen Liang, Peizhong Ju, Yingbin Liang, and Ness Shroff. Non-asymptotic convergence
of discrete-time diffusion models: New approach and improved rate. arXiv preprint
arXiv:2402.13901,2024.
YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMattLe. Flowmatching
forgenerativemodeling. arXivpreprintarXiv:2210.02747,2022.
Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and
transferdatawithrectifiedflow. arXivpreprintarXiv:2209.03003,2022.
AaronLou,ChenlinMeng,andStefanoErmon.Discretediffusionmodelingbyestimatingtheratios
ofthedatadistribution. InForty-firstInternationalConferenceonMachineLearning,2024.
YunhongLou,LinchaoZhu,YaxiongWang,XiaohanWang,andYiYang. Diversemotion: Towards
diversehumanmotiongenerationviadiscretediffusion. arXivpreprintarXiv:2309.01372,2023.
Sokhna Diarra Mbacke and Omar Rivasplata. A note on the convergence of denoising diffusion
probabilisticmodels. arXivpreprintarXiv:2312.05989,2023.
ChenlinMeng,KristyChoi,JiamingSong,andStefanoErmon. Concretescorematching: General-
izedscorematchingfordiscretedata. AdvancesinNeuralInformationProcessingSystems, 35:
34532–34545,2022.
HunterNisonoff,JunhaoXiong,StephanAllenspach,andJenniferListgarten. Unlockingguidance
fordiscretestate-spacediffusionandflowmodels. arXivpreprintarXiv:2406.01572,2024.
ChenhaoNiu,YangSong,JiamingSong,ShengjiaZhao,AdityaGrover,andStefanoErmon. Per-
mutationinvariantgraphgenerationviascore-basedgenerativemodeling. InInternationalCon-
ferenceonArtificialIntelligenceandStatistics,pp.4474–4484.PMLR,2020.
BerntØksendal. Stochasticdifferentialequations. Springer,2003.
FrancescoPedrotti,JanMaas,andMarcoMondelli. Improvedconvergenceofscore-baseddiffusion
modelsviaprediction-correction. arXivpreprintarXiv:2305.14164,2023.
15Preprintunderreview
PhilipProtter.Pointprocessdifferentialswithevolvingintensities.InNonlinearstochasticproblems,
pp.467–472.Springer,1983.
YimingQin,ClementVignac,andPascalFrossard. Sparsetrainingofdiscretediffusionmodelsfor
graphgeneration. arXivpreprintarXiv:2311.02142,2023.
PierreHRichemond, SanderDieleman, andArnaudDoucet. Categoricalsdeswithsimplexdiffu-
sion. arXivpreprintarXiv:2210.14784,2022.
Eric A Riesel, Tsach Mackey, Hamed Nilforoshan, Minkai Xu, Catherine K Badding, Alison B
Altman, Jure Leskovec, and Danna E Freedman. Crystal structure determination from powder
diffractionpatternswithgenerativemachinelearning. JournaloftheAmericanChemicalSociety,
2024.
SeveriRissanen,MarkusHeinonen,andArnoSolin. Improvingdiscretediffusionmodelsviastruc-
turedpreferentialgeneration. arXivpreprintarXiv:2405.17889,2024.
SubhamSekharSahoo,MarianneArriola,YairSchiff,AaronGokaslan,EdgarMarroquin,JustinT
Chiu,AlexanderRush,andVolodymyrKuleshov.Simpleandeffectivemaskeddiffusionlanguage
models. arXivpreprintarXiv:2406.07524,2024.
LaurentSaloff-Coste. Lecturesonfinitemarkovchains.lecturesonprobabilitytheoryandstatistics
(saint-flour,1996),301–413. LectureNotesinMath,1665,1997.
JavierESantos,ZacharyRFox,NicholasLubbers,andYenTingLin.Blackoutdiffusion:generative
diffusionmodelsindiscrete-statespaces. InInternationalConferenceonMachineLearning,pp.
9034–9059.PMLR,2023.
AriSeff,WendaZhou,FarhanDamani,AbigailDoyle,andRyanPAdams. Discreteobjectgener-
ationwithreversibleinductiveconstruction. Advancesinneuralinformationprocessingsystems,
32,2019.
Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang.
Graphaf: a flow-based autoregressive model for molecular graph generation. arXiv preprint
arXiv:2001.09382,2020.
JiaxinShi,KehangHan,ZheWang,ArnaudDoucet,andMichalisKTitsias. Simplifiedandgener-
alizedmaskeddiffusionfordiscretedata. arXivpreprintarXiv:2406.04329,2024.
AndyShih,SuneelBelkhale,StefanoErmon,DorsaSadigh,andNimaAnari. Parallelsamplingof
diffusionmodels. AdvancesinNeuralInformationProcessingSystems,36,2024.
JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsupervised
learningusingnonequilibriumthermodynamics. InInternationalConferenceonMachineLearn-
ing,pp.2256–2265.PMLR,2015.
YangSongandStefanoErmon.Generativemodelingbyestimatinggradientsofthedatadistribution.
Advancesinneuralinformationprocessingsystems,32,2019.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020.
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of
score-based diffusion models. Advances in neural information processing systems, 34:1415–
1428,2021.
Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and
Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. arXiv
preprintarXiv:2402.05841,2024.
DanielWStroock. Logarithmicsobolevinequalitiesforgibbsstates. InDirichletforms.1993.
Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time
discretediffusionmodels. arXivpreprintarXiv:2211.16750,2022.
16Preprintunderreview
BelindaTzenandMaximRaginsky.Theoreticalguaranteesforsamplingandinferenceingenerative
modelswithlatentdiffusions. InConferenceonLearningTheory,pp.3084–3114.PMLR,2019.
NicoMVanDijk.Uniformizationfornonhomogeneousmarkovchains.Operationsresearchletters,
12(5):283–291,1992.
Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pas-
cal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint
arXiv:2209.14734,2022.
PascalVincent. Aconnectionbetweenscorematchinganddenoisingautoencoders. Neuralcompu-
tation,23(7):1661–1674,2011.
Lingxiao Wang, Gert Aarts, and Kai Zhou. Generative diffusion models for lattice field theory.
arXivpreprintarXiv:2311.03578,2023.
JosephLWatson,DavidJuergens,NathanielRBennett,BrianLTrippe,JasonYim,HelenEEise-
nach,WoodyAhern,AndrewJBorst,RobertJRagotte,LukasFMilles,etal. Denovodesignof
proteinstructureandfunctionwithrfdiffusion. Nature,620(7976):1089–1100,2023.
Ludwig Winkler, Lorenz Richter, and Manfred Opper. Bridging discrete and continuous state
spaces: Exploring the ehrenfest process in time-continuous diffusion models. arXiv preprint
arXiv:2405.03549,2024.
TongWu,ZhihaoFan,XiaoLiu,Hai-TaoZheng,YeyunGong,JianJiao,JuntaoLi,JianGuo,Nan
Duan, Weizhu Chen, et al. Ar-diffusion: Auto-regressive diffusion model for text generation.
AdvancesinNeuralInformationProcessingSystems,36:39957–39974,2023.
ZhichaoWu,QiulinLi,SixingLiu,andQunYang. Dctts: Discretediffusionmodelwithcontrastive
learningfortext-to-speechgeneration. InICASSP2024-2024IEEEInternationalConferenceon
Acoustics,SpeechandSignalProcessing(ICASSP),pp.11336–11340.IEEE,2024.
Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geo-
metricdiffusionmodelformolecularconformationgeneration.arXivpreprintarXiv:2203.02923,
2022.
Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu.
Diffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on
Audio,Speech,andLanguageProcessing,31:1720–1733,2023a.
John J Yang, Jason Yim, Regina Barzilay, and Tommi Jaakkola. Fast non-autoregressive inverse
foldingwithdiscretediffusion. arXivpreprintarXiv:2312.02447,2023b.
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,
Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and
applications. ACMComputingSurveys,56(4):1–39,2023c.
KaiYi,BingxinZhou,YiqingShen,PietroLiò,andYuguangWang. Graphdenoisingdiffusionfor
inverseproteinfolding. AdvancesinNeuralInformationProcessingSystems,36,2024.
Junyi Zhang, Jiaqi Guo, Shizhao Sun, Jian-Guang Lou, and Dongmei Zhang. Layoutdiffusion:
Improvinggraphiclayoutgenerationbydiscretediffusionprobabilisticmodels. InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pp.7226–7236,2023.
Linfeng Zhang, Weinan E, and Lei Wang. Monge-ampère flow for generative modeling. arXiv
preprintarXiv:1809.10188,2018.
QinshengZhangandYongxinChen. Fastsamplingofdiffusionmodelswithexponentialintegrator.
arXivpreprintarXiv:2204.13902,2022.
Zikun Zhang, Zixiang Chen, and Quanquan Gu. Convergence of score-based discrete diffusion
models: Adiscrete-timeanalysis. arXivpreprintarXiv:2410.02321,2024.
Lingxiao Zhao, Xueying Ding, Lijun Yu, and Leman Akoglu. Improving and unifying
discrete&continuous-timediscretedenoisingdiffusion. arXivpreprintarXiv:2402.03701,2024.
17Preprintunderreview
LinZheng,JianboYuan,LeiYu,andLingpengKong. Areparameterizeddiscretediffusionmodel
fortextgeneration. arXivpreprintarXiv:2302.05737,2023.
Kun Zhou, Yifan Li, Wayne Xin Zhao, and Ji-Rong Wen. Diffusion-nat: Self-prompting discrete
diffusionfornon-autoregressivetextgeneration. arXivpreprintarXiv:2305.04044,2023.
Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, and Yan Yan. Discrete contrastive
diffusionforcross-modalmusicandimagegeneration. arXivpreprintarXiv:2206.07771,2022.
Yuchen Zhu, Tianrong Chen, Evangelos A Theodorou, Xie Chen, and Molei Tao. Quantum state
generationwithstructure-preservingdiffusionmodel. arXivpreprintarXiv:2404.06336,2024.
18Preprintunderreview
A MATHEMATICAL FRAMEWORK OF POISSON RANDOM MEASURE
In this section, we provide a mathematical framework for Poisson random measure with evolving
intensity,whichiscrucialfortheerroranalysisofdiscretediffusionmodelsinthemaintext.
A.1 PRELIMINARIES
WefirstprovidethedefinitionoftheordinaryPoissonrandommeasure.
DefinitionA.1(PoissonRandomMeasure). Let(Ω,F,P)beaprobabilityspaceand(X,B,ν)be
ameasurespacesatisfyingthat
(cid:90)
1∨|y|∨|y|2ν(dy)<∞,
X
TherandommeasureN(dt,dy)onR+×XiscalledaPoissonrandommeasurew.r.t. measureν if
itisarandomcountingmeasuresatisfyingthefollowingproperties:
(i) ForanyB ∈Band0≤s<t,N((s,t]×B)∼P(ν(B)(t−s));
(ii) Foranyt≥0andpairwisedisjointsets{B } ⊂B,{N (B ):=N((0,t]×B )} are
i i∈[n] t i i i∈[n]
independentstochasticprocesses.
Thefollowingdefinitionofpredictabilitywillbefrequentlyusedforthewell-definednessofstochas-
ticintegralsw.r.t. Poissonrandommeasure, andthustheextensionfromordinaryPoissonrandom
measuretoPoissonrandommeasurewithevolvingintensity.
DefinitionA.2(Predictability). Thepredictableσ-algebraonR+×Xisdefinedastheσ-algebra
generated by all sets of the form (s,t]×B for 0 ≤ s < t and B ∈ B. A process X is called
t
predictableifandonlyifX ispredictablew.r.t. thepredictableσ-algebraabove.
t
In the following, we will define the Poisson random measure with evolving intensity, which is a
specialcaseofrandommeasures(Jacod&Shiryaev,2013,Definition1.3).
DefinitionA.3(PoissonRandomMeasurewithEvolvingIntensity). Let(Ω,F,P)beaprobability
space and (X,B,ν) be a measure space. Suppose λ (y) is a non-negative predictable process on
t
R+×X×Ωsatisfyingthatforany0≤T <T,
(cid:90) T (cid:90)
1∨|y|∨|y|2λ (y)ν(dy)dt<∞, a.s..
t
0 X
TherandommeasureN[λ](dt,dy)onR+ ×XiscalledaPoissonrandommeasurewithevolving
intensityλ (y)w.r.t. measureν ifitisarandomcountingmeasuresatisfyingthefollowingproper-
t
ties:
(cid:16) (cid:17)
(cid:82)t(cid:82)
(i) ForanyB ∈Band0≤s<t,N[λ]((s,t]×B)∼P λ (y)ν(dy)dτ ;
s B τ
(ii) Foranyt≥0andpairwisedisjointsets{B } ⊂B,
i i∈[n]
{N [λ](B ):=N[λ]((0,t]×B )}
t i i i∈[n]
areindependentstochasticprocesses.
TheoremA.4(Well-definednessofPoissonRandomMeasurewithEvolvingIntensity). ThePoisson
randommeasureN[λ](dt,dy)withevolvingintensityλ (y)iswell-definedundertheconditionsin
t
thedefinitionabove.
Proof. Wefirstaugmentthe(X,B,ν)measurespacetoaproductspace(X×R,B×B(R),ν×m),
wheremistheLebesguemeasureonR,andB(R)istheBorelσ-algebraonR.ThePoissonrandom
measurewithevolvingintensityλ (y)canbedefinedintheaugmentedmeasurespaceas
t
(cid:90) t(cid:90) (cid:90)
N[λ]((s,t]×B):= 1 N(dτ,dy,dξ), (A.1)
0≤ξ≤λτ(y)
s B R
19Preprintunderreview
whereN(dτ,dy,dξ)isthePoissonrandommeasureonR+×X×Rw.r.t. measureν(dy)dξ.
ThenitisstraightforwardtoverifythetwoconditionsinthedefinitionofPoissonrandommeasure
withevolvingintensitybynoticingthatforpairwisedisjointsets{B } ⊂ B,{B ×R} ⊂
i i∈[n] i i∈[n]
B×B(R)arealsopairwisedisjoint.
The Poisson random process N[λ](dt,dy) with evolving intensity λ (y) is well-defined up to an
t
eventualexplosiontime
(cid:40) (cid:41)
(cid:90) T (cid:90)
T =inf λ (y)ν(dy)dt=∞, a.s. .
t
T 0 X
Wereferthereadersto(Protter,1983)foramorerigorousdetailedversionoftheproof.
Remark A.5 (Relation to the Cox process). The Poisson random measure with evolving intensity
sharesmultiplesimilaritieswiththeCoxprocess(Cox,1955;Last&Penrose,2017),includingbeing
apointprocessandwiththeintensitybeingarandommeasure. ThemaindifferenceisthattheCox
process is defined on a general measure space, while the Poisson random measure with evolving
intensity is defined on the product space (X×R,B ×B(R),ν ×m) and the intensity function is
requiredtobepredictabletoensurethewell-definednessofitsstochasticintegral.
A.2 STOCHASTICINTEGRALW.R.T. POISSONRANDOMMEASURE
Thefollowingtheoremsprovidethepropertiesofstochasticintegralsw.r.t.Poissonrandommeasure
withevolvingintensity. Theproofsarebasedontheobservationthatwiththeaugmentationofthe
measurespaceargument(A.1),thestochasticintegralw.r.t. Poissonrandommeasurewithevolving
intensityin(X,B,ν)canbereducedtothestochasticintegralw.r.t. homogeneousPoissonrandom
measurein(X×R,B×B(R),ν×m),andundercertainconditionsonthemeasurespace(X,B,ν),
tothewell-knownLévy-typestochasticintegral(Applebaum,2009). Forsimplicity, wewillwork
ontheintervalt∈[0,T]withT <T andthefollowingregularityconditionsofthePoissonrandom
measure:
0< essinf λ (y)≤ esssup λ (y)<+∞.
τ τ
τ∈[0,T],y∈X τ∈[0,T],y∈X
One can easily generalize the following results to their local versions on [0,T) by considering its
compactsubsets.
TheoremA.6(StochasticIntegralsw.r.t.PoissonRandomMeasurewithEvolvingDensity). Forany
predictableprocessK (y)onR+×X×Ω,thestochasticintegralw.r.t. Poissonrandommeasure
t
withevolvingintensityλ (y)
t
(cid:90) t(cid:90)
x =x + K (y)N[λ](dt,dy), (A.2)
t 0 t
0 X
hasauniquesolution,forwhichthefollowingpropertieshold:
(1) (Expectation)Foranyt≥0,wehave
(cid:20)(cid:90) t(cid:90) (cid:21) (cid:90) t(cid:90)
E K (y)N[λ](dt,dy) = K (y)λ (y)ν(dy)dt;
t t t
0 X 0 X
(2) (Martingale)Foranyt≥0,wehave
(cid:90) t(cid:90) (cid:90) t(cid:90) (cid:90) t(cid:90)
K t(y)N(cid:101)[λ](dt,dy):= K t(y)N[λ](dt,dy)− K t(y)λ t(y)ν(dy)dt
0 X 0 X 0 X
isalocalF -martingale;
t
(3) (ItôIsometry)Foranyt≥0,wehave
(cid:34)(cid:18)(cid:90)
t(cid:90)
(cid:19)2(cid:35)
(cid:90) t(cid:90)
E K (y)N[λ](dt,dy) = K (y)2λ (y)ν(dy)dt.
t t t
0 X 0 X
20Preprintunderreview
Proof. Wefirstwritetheintegral(A.2)intheaugmentedmeasurespace(X×R,B×B(R),ν×m)
as
(cid:90) t(cid:90) (cid:90)
x =x + K (y)1 N(dt,dy,dξ), (A.3)
t 0 t 0≤ξ≤λt(y)
0 X R
andsinceK (y)1 isapredictableprocess,thedesiredpropertiescanbederivedfromthe
t 0≤ξ≤λt(y)
correspondingpropertiesofthestochasticintegralw.r.t. Poissonrandommeasureintheaugmented
measurespace.
The subsequent proof will follow a similar argument as the proof of the stochastic integral w.r.t.
Brownianmotion(e.g. in(Øksendal,2003))bystartingfromprovingthepropertiesforelementary
processes, which in our case refer to working with the elementary predictable processes of the
followingform:
n−1 m l
(cid:88)(cid:88)(cid:88)
Z (y,ξ)(ω)= Z (ω)1 1 1 ,
t i,j,k t∈(ti,ti+1] y∈Bj ξ∈Ck
i=0 j=1k=1
where 0 = t < ··· < t = T is a partition of [0,T], B ∈ B for j ∈ [m] are a parti-
0 n j
tion of X with ν(B ) < ∞, and C ∈ B(R) for k ∈ [l] are a partition of the time interval
k k
[0,esssup λ (y)] with m(C ) < ∞, and K is bounded and F -measurable, on
τ∈[0,T],y∈X τ k i,j,k ti
whichthestochasticintegralisdefinedas
(cid:90) t(cid:90) n−1 m l
(cid:88)(cid:88)(cid:88)
Z (y,ξ)N(dt,dy,dξ)= Z N ((t ,t ]×B ×C ).
t i,j,k ti i i+1 j k
0 X i=0 j=1k=1
Then,itisstraightforwardtoverifythepropertiesofthestochasticintegralfortheelementarypre-
dictable process Z+(y,ξ), using the definition of Poisson random measure (Definition A.1). For
t
generalpredictableprocessesZ (y,ξ),wewriteZ (y,ξ) = Z+(y,ξ)−Z−(y,ξ),whereZ+(y,ξ)
t t t t t
and Z−(y,ξ) are positive and negative parts of Z (y,ξ), and apply the results to Z+(y,ξ) and
t t t
Z−(y,ξ)separately.
t
Finally,wetakeZ (y,ξ)=K (y)1 toderivethepropertiesofthestochasticintegralw.r.t.
t t 0≤ξ≤λt(y)
Poissonrandommeasurewithevolvingintensity.
We refer readers to (Eberle, 2015, Section 2.2) for detailed arguments. For the uniqueness of the
solutiontothestochasticintegral,wealsoreferto(Protter,1983,Theorem3.1).
PropositionA.7. Definethelistofjumptimes(t n) n∈Nrecursivelyas
t =0, t =inf{t>t |∆x ̸=0}, n≥0,
0 n+1 n t
thePoissonrandommeasureN[λ](dt,dy)withevolvingintensityλ (y)canbewrittenas
t
∞
(cid:88)
N[λ](dt,dy)= δ (dt)δ (dy), (A.4)
tn Yn
n=1
andthestochasticintegral(A.2)iscàdlàgandcanberewrittenasasumofjumps:
N N
(cid:88) (cid:88)
x =x + ∆x =x + K (Y ), (A.5)
t 0 tn 0 tn n
n=1 n=1
whereN isarandomvariablesatisfyingt ≤t<t ,and∆x arethejumps∆x =x −x
N N+1 tn tn tn t−
n
withx :=lim x .
t−
n
s→t−
n
s
Proof. Toseethesolutioniscàdlàg,wenoticethefollowingrightlimitattimet:
(cid:90)
lim(x −x )= K (y)N[λ](dt,dy)→0,
t+ϵ t t
ϵ→0 (t,t+ϵ]×X
andtheleftlimitattimet:
(cid:90) (cid:90)
∆x = lim(x −x )= K (y)N[λ](dt,dy)→ K (y)N[λ]({t}×dy), (A.6)
t t t−ϵ t t
ϵ→0 (t−ϵ,t]×X X
21Preprintunderreview
wherethenotationN[λ]({t}×dy)shouldbeunderstoodasN[λ]({t}×dy) = 0ift ∈/ {t n} n∈N,
orotherwiseY =N[λ]({t }×dy)isarandomvariableonX.
n n
SincethePoissonrandommeasureN[λ](dt,dy)withevolvingintensityλ (y)isarandomcounting
t
measure,itcanberepresentedasacountablesumofDiracmeasuresasin(A.4),andthuswehave
(cid:90) t(cid:90)
x =x + K (y)N[λ](dt,dy)
t 0 t
0 X
(cid:90) t(cid:90) N N
(cid:88) (cid:88)
=x + K (y) δ (dt)δ (dy)=x + K (Y ).
0 t tn Yn 0 tn n
0 X n=1 n=1
By the definition of Poisson random measure, (t ) are also the jump times of the homo-
n n∈[N]
geneous Poisson random measure N(dt,dy,dξ) in the augmented measure space (X × R,B ×
B(R),ν×m)w.r.t. measureν(dy)dξ. Therefore,withaslightabuseofnotations,wewillassume
(t ,Y ,Ξ ) arei.i.d. randomvariableswithprobabilitymeasureproportionaltodtν(dy)dξ,
n n n n∈[N]
foreachofwhichΞ ≤λ (Y )holdsbecauseotherwisethejumpwouldnotoccur.
n tn n
ThenthedistributionofY canbederivedasaconditionalprobabilityofthejumplocationY given
n n
thejumptimet andΞ ≤λ (Y ):
n n tn n
(cid:82)
ν(dy)1 dξ λ (y)ν(dy)
P(Y n =y)ν(dy)= (cid:82) R (cid:82) Ξn≤λtn(y) = (cid:82) tn , (A.7)
ν(dy)1 dξ λ (y)ν(dy)
R X Ξn≤λtn(y) X tn
andtheproofiscomplete.
ThefollowingtheoremgivesthemartingalecharacterizationofPoissonrandommeasurewithevolv-
ingintensity,whichwillbecrucialfortheproofofthechangeofmeasurearguments:
Theorem A.8 (Martingale Characterization of Poisson Random Measure with Evolving Density).
LetN[λ](dt,dy)beaF -adaptedprocessintheprobabilityspace(Ω,F,P). ThenN[λ](dt,dy)is
t
aPoissonrandommeasurewithevolvingintensityλ (y)ifandonlyifthecomplex-valuedprocess
t
(cid:18) (cid:90) t(cid:90) (cid:90) t(cid:90) (cid:16) (cid:17) (cid:19)
M [f]=exp i f (y)N[λ](dτ,dy)+ 1−eifτ(y) λ (y)ν(dy)dτ (A.8)
t τ τ
0 X 0 X
isalocalmartingaleforanypredictableprocessf (y)satisfyingthatf (y)∈L1(X,ν),a.s..
τ τ
Proof. ByPropositionA.7,werewritethestochasticintegralasasumofjumps:
(cid:90) t(cid:90) N
(cid:88)
f (y)N[λ](dτ,dy)= f (Y ),
t tn n
0 X n=1
where (t ,Y ,Ξ ) are i.i.d. random variables with probability measure proportional to
n n n n∈[N]
dtν(dy)dξ,foreachofwhichΞ ≤λ (Y )holds,followingasimilarargumentasintheproofof
n tn n
PropositionA.7.
Then,itisstraightforwardtoderivethefollowingprobabilityofthejumptimet =τ:
n
(cid:82) P(Y =y,t =τ)ν(dy) (cid:82) λ (y)ν(dy)dτ
P(t =τ)dτ = X n n = X τ ;
n (cid:82)t(cid:82) P(Y =y,t =τ)ν(dy)dτ (cid:82)t(cid:82) λ (y)ν(dy)dτ
0 X n n 0 X τ
andbythedefinitionofthePoissonrandommeasure,wehavethefollowingprobabilityofthetotal
numberofjumpsN =n:
1 (cid:18) (cid:90) t(cid:90) (cid:19)(cid:18)(cid:90) t(cid:90) (cid:19)n
P(N =n)= exp − λ (y)ν(dy)dτ λ (y)ν(dy)dτ .
n! τ τ
0 X 0 X
22Preprintunderreview
Withoutlossofgenerality,weonlyverifyE[M [f]]=1asfollows,andgeneralcasesaresimilarby
t
Markovproperty:
(cid:20) (cid:18) (cid:90) t(cid:90) (cid:19)(cid:21)
E exp i f (y)N[λ](dτ,dy)
tn
0 X
(cid:34) (cid:32) N (cid:33)(cid:12) (cid:35)
=E exp i(cid:88) f(Y n) (cid:12) (cid:12) (cid:12)Ξ n ≤λ tn(Y n), ∀n∈[N]
n=1
(cid:34) N (cid:35)
=E (cid:89) E(cid:104) eiftn(Yn)(cid:12) (cid:12)Ξ
n
≤λ tn(Y n)(cid:105)
n=1
=E(cid:34) (cid:89)N E(cid:20)(cid:90) eif (cid:82)τ(y)λ τ(y)ν(dy)(cid:12)
(cid:12)
(cid:12)t
n
=τ(cid:21)(cid:35)
n=1
X Xλ τ(y)ν(dy) (cid:12)
(cid:88)∞ 1 (cid:18)(cid:90) t(cid:90) (cid:19)n (cid:18) (cid:90) t(cid:90) (cid:19)
= eifτ(y)λ (y)ν(dy)dτ exp − λ (y)ν(dy)dτ
n! τ τ
n=1 0 X 0 X
(cid:18)(cid:90) t(cid:90) (cid:16) (cid:17) (cid:19)
=exp eifτ(y)−1 λ (y)ν(dy)dτ ,
τ
0 X
whichimmediatelyyieldsthedesiredresultE[M [f]]=1.
t
Ontheotherhand,forany0≤s<tandB ∈B,weset
Z (y)=u1 1 ,
t t∈(s,t] y∈B
whereu∈R,andbyassumption,wehave
(cid:20) (cid:18) (cid:90) t(cid:90) (cid:90) t(cid:90) (cid:16) (cid:17) (cid:19)(cid:21)
E[M [Z]]=E exp i Z (y)N[λ](dτ,dy)+ 1−eiZτ(y) λ (y)ν(dy)dτ
t τ τ
0 X 0 X
(cid:20) (cid:18) (cid:90) t(cid:90) (cid:90) t(cid:90) (cid:19)(cid:21)
=E exp iu N[λ](dτ,dy)+ (cid:0) 1−eiu(cid:1) λ (y)ν(dy)dτ
τ
s B s X
(cid:20) (cid:18) (cid:90) (cid:19)(cid:21)
=E exp iuN[λ]((s,t]×B)+(cid:0) 1−eiu(cid:1) (t−s) λ (y)ν(dy) =1,
τ
X
i.e. thefollowingholdsforanyu∈R:
(cid:90)
E[exp(iuN[λ]((s,t]×B)))]=(cid:0) eiu−1(cid:1)
(t−s) λ (y)ν(dy),
τ
X
whichbyLévy’scontinuitytheoremimpliesthat
(cid:18) (cid:90) (cid:19)
N[λ]((s,t]×B)∼P (t−s) λ (y)ν(dy) ,
τ
X
andthusN[λ](dt,dy)isaPoissonrandommeasurewithevolvingintensityλ (y)byDefinitionA.1.
t
Theorem A.9 (Itô’s Formula for Poisson Random Measure with Evolving Density). Let
N[λ](dt,dy) be a Poisson random measure with evolving intensity λ (y) in the probability space
t
(Ω,F,P)andK (y)beapredictableprocessonR+×X×Ω. Supposeaprocessx satisfiesthe
t t
stochasticintegral
(cid:90) t(cid:90)
x =x + K (y)N[λ](dt,dy), (A.9)
t 0 t
0 X
thenforanyf (y)∈C(R+×X)withprobability1,wehave
t
(cid:90) t (cid:90) t(cid:90)
f (x )=f (x )+ ∂ f (x )dτ + (f (x +K (y))−f (x ))N[λ](dτ,dy).
t t 0 0 τ τ τ τ τ− τ τ τ−
0 0 X
23Preprintunderreview
Proof. ByPropositionA.7,weagainrewritethestochasticintegralasasumofjumps:
N
(cid:88)
x =x + K (Y ),
t 0 tn n
n=1
where(t ) arethejumptimeswitht = 0andt ≤ t < t ,and(Y ) arethejump
n n∈[N] 0 N N+1 n n∈[N]
locations. Consequently,itiseasytoseethatx =x =x fort∈(t ,t ]andn∈[N].
t−
n
t tn−1 n−1 n
Thenwehavethefollowingdecomposition:
f (x )−f (x )
t t 0 0
N
(cid:88)(cid:16) (cid:17)
=f (x )−f (x )+ f (x )−f (x )+f (x )−f (x )
t t tN tN tn tn tn t−
n
tn t−
n
tn−1 tn−1
n=1
(cid:90) t (cid:88)N (cid:90) tn (cid:88)N
(cid:0) (cid:1)
= ∂ f (x )dτ + ∂ f (x )dτ + f (x +K (Y ))−f (x ) ,
τ τ tN τ τ tn−1 tn tn−1 tn n tn tn−1
tN n=1 tn−1 n=1
andforthelasttermintheaboveequation,wehave
N N
(cid:88)(cid:0) (cid:1) (cid:88)(cid:16) (cid:17)
f (x +K (Y ))−f (x ) = f (x +K (Y ))−f (x )
tn tn−1 tn n tn tn−1 tn t−
n
tn n tn t−
n
n=1 n=1
(cid:90) t(cid:90)
= (f (x +K (y))−f (x ))N[λ](dτ,dy).
τ τ− τ τ τ−
0 X
Combiningtheaboveresults,wehavethedesiredresult.
LemmaA.10. Denotethetrajectoryobtainedbysimulatingthemasterequation(2.7)oftheforward
processofthediscretediffusionmodelasx ,thenthetimeinterval∆t =t −t isdistributed
t n n+1 n
accordingtothefollowingdistribution:
(cid:18)(cid:90) τ (cid:19)
P(∆t >τ)=exp Q (x ,x )dτ′ , (A.10)
n τ′ tn tn
0
andthejumplocationx isdistributedaccordingtothefollowingdistribution:
tn+1
Q (y,x )
P(x =y)=− tn+1 tn . (A.11)
tn+1 Q (x ,x )
tn+1 tn tn
Proof. Theresultscanbefoundin(Eberle,2009,Section1.2). Whileafullyrigorousproofcanbe
conductedbydiscretizingthetime-inhomogeneouscontinuous-timeMarkovchaininto2n uniform
stepsandtakingthelimitasn → ∞, followingtheapproachin(Lalley,2012), wewillprovidea
moreintuitiveproofhereforcompleteness.
Setp =e ,wheree isthey-thunitvectorinR|X|. thenthex -thentryof(2.7)yields
tn xtn y tn
d d (cid:88)
P(x =x )= p (x )= Q (x ,y)p (y),
dt t tn dt t tn t tn t
y∈X
which,bytheassumedcontinuityoftheratematrixQ ,implies
t
P(∆t >τ)=P(x =x )=1+Q (x ,x )τ +o(τ),
n tn+τ tn tn tn tn
andthus
d logP(∆t >τ)
logP(∆t >τ)= lim n = limQ (x ,x )+o(1)=Q (x ,x ),
dτ n τ→0 τ τ→0 tn tn tn tn tn tn
integratingtheaboveequationyieldsthedesiredresult.
24Preprintunderreview
Similarly,bysettingp =e ,wehaveforally ∈X\{x }:
tn+1−τ xtn tn
P(x =y)=p (y)
tn+1 tn+1
p (y)+Q (y,x )τ +o(τ)
=lim
tn+1−τ tn+1−τ tn
(cid:80) (cid:0) (cid:1)
τ→0 y∈X\{xtn} p tn+1−τ(y)+Q tn+1−τ(y,x tn)τ +o(τ)
Q (y,x ) Q (y,x )
=
tn+1 tn
=−
tn+1 tn
,
(cid:80)
Q (y,x ) Q (x ,x )
y∈X\{xtn} tn+1 tn tn+1 tn tn
andtheresultfollows.
A.3 PROOFSOFCHANGEOFMEASURERELATEDARGUMENTS
ProofofTheorem3.3. Inthefollowing,wewilldenotetheexpectationunderthemeasurePbyE P
andtheexpectationunderthemeasureQbyE Q.
ByTheoremA.8, toverify thatthePoissonrandommeasure N[λ](dt,dy) withevolvingintensity
λ (y) is a Poisson random measure with evolving intensity λ (y)h (y) under the measure Q, it
t t t
sufficestoshowthatforanyf ∈L1(X,ν),thecomplex-valuedprocess
(cid:18) (cid:90) t(cid:90) (cid:90) t(cid:90) (cid:16) (cid:17) (cid:19)
M [f]=exp i f(y)N[λ](dτ,dy)+ 1−eif(y) λ (y)h (y)ν(dy)dτ
t τ τ
0 X 0 X
isalocalmartingaleunderthemeasureQ.
Tothisend,weperformthefollowingcalculation:
E Q[M t[f]]=E P[M t[f]Z t[h]]
(cid:20) (cid:18) (cid:90) t(cid:90) (cid:90) t(cid:90) (cid:16) (cid:17) (cid:19)
=E P exp i f(y)N[λ](dτ,dy)+ 1−eif(y) λ τ(y)h τ(y)ν(dy)dτ
0 X 0 X
(cid:18)(cid:90) t(cid:90) (cid:90) t(cid:90) (cid:19)(cid:21)
exp logh (y)N[λ](dt×dy)− (h (y)−1)λ (y)ν(dy)
t t t
0 X 0 X
(cid:20) (cid:18) (cid:90) t(cid:90) (cid:90) t(cid:90) (cid:16) (cid:17) (cid:19)(cid:21)
=E P exp i (f(y)+logh t(y))N[λ](dτ,dy)+ 1−eif(y)h t(y) λ t(y)ν(dy)
0 X 0 X
=E P[M t[f +logh]],
andbyassumption,f+logh∈L1(X,ν),a.s.,whichimpliesthatM [f+logh]isalocalmartingale
t
under the measure P again by Theorem A.8. Consequently, M [f] is a local martingale under the
t
measureQ,andtheresultfollows.
ProofofCorollary3.5. With a similar argument as in the proof of Proposition 3.2, we have the
following stochastic integral representation of the approximate backward process with the learned
neuralnetworkscorefunctionsθ:
(cid:98)t
(cid:90) s(cid:90)
y
s
=y 0+ (y−y s−)N[µ (cid:98)θ](ds,dy), withµ (cid:98)θ s(y)=s (cid:98)θ s(y s−,y)Q(cid:101)s(y s−,y)=µ (cid:98)θ s(y). (A.12)
0 X
Bythedata-processinginequalityandthechainruleofKLdivergence,wehave
D (p ∥q )≤D (p ∥q )=D (p ∥q )+E[D (p ∥q |x =y =y)].
KL T T KL 0:T 0:T KL 0 0 KL 0:T 0:T 0 0
Thennoticethatconditioningonthealignmentoftheinitialstatex = y = y foranyy ∈ X,the
0 0
secondtermintheaboveequationcanbeexpressedas
D KL(p 0:T∥q 0:T|x 0 =y 0
=y)=E(cid:20)
logd dp
q0:T(cid:12)
(cid:12) (cid:12) (cid:12)x 0 =y 0
=y(cid:21) =E(cid:20)
logZ
T−1(cid:20)
µ (cid:98)
µθ(cid:21)(cid:21)
,
0:T
where the last equality is by the change of measure in Theorem 3.3 from the stochastic integral
formulation (3.2) of the backward process (2.8) with the true score function s to the stochastic
integralformulation(A.12)oftheapproximatebackwardprocesswiththelearnedscorefunctionsθ.
(cid:98)
25
⃗
⃗ ⃗
⃗ ⃗
⃗ ⃗
⃗
⃗
⃗ ⃗
⃗Preprintunderreview
PlugintheexpressionofZ in(3.3)andnoticethat
T
µ (cid:98)θ
s =
s (cid:98)θ s(y s−,y)Q(cid:101)s(y s−,y)
=
s (cid:98)θ s(y s−,y)
,
µ s s s(y s−,y)Q(cid:101)s(y s−,y) s s(y s−,y)
wehave
(cid:20) (cid:20) µθ(cid:21)(cid:21)
E logZ−1 (cid:98)
T µ
   
(cid:90) T (cid:90) sθ(y ,y) (cid:90) T (cid:90) sθ (y ,y)
=E − log (cid:98) ss (ys− ,y)N[µ](ds×dy)+ (cid:98) ss (ys−
,y)
−1µ s(y)ν(dy)ds
0 X s s− 0 X s s−
   
(cid:90) T (cid:90) sθ (y ,y) sθ(y ,y)
=E  (cid:98) ss (ys−
,y)
−1−log (cid:98) ss (ys− ,y)s s(y s−,y)Q(cid:101)s(y s−,y)ν(dy)ds
0 X s s− s s−
(cid:34) (cid:32) (cid:33) (cid:35)
=E (cid:90) T (cid:90) s (cid:98)θ s(y s−,y)−s s(y s−,y)−s s(y s−,y)logs (cid:98) sθ s (( yy s− ,, yy )) Q(cid:101)s(y s−,y)ν(dy)ds ,
0 X s s−
rearrangingthetermsintheaboveequationyieldsthedesiredresult.
A.4 PROOFSOFSTOCHASTICINTEGRALFORMULATIONS
ProofofProposition3.2. Inthefollowing,wewilldenotethetrajectoryobtainedbysimulatingthe
masterequation(2.7)oftheforwardprocessofthediscretediffusionmodelasx andthetrajectory
(cid:101)t
obtainedbythestochasticintegral(3.1)asx ,withx =x .Wewillalsousethenotation·todenote
t 0 (cid:101)0 (cid:101)
the quantities associated with the trajectory x . The goal is to show that x and x are identically
(cid:101)t t (cid:101)t
distributedforanyt∈[0,T].
We provethis claim byinduction. We assumethat for any t ∈ [0,t ], where n ∈ N and t is the
n n
n-thjumptimewitht =0,thetwotrajectoriesx andx areidenticallydistributed. Forsimplicity,
0 t (cid:101)t
werealignthetwoprocessesx andx attimet bysettingx =x .
t (cid:101)t n tn (cid:101)tn
Wefirstconsidertheprocessx generatedbythediscretediffusionmodel(2.7).Recallthedefinition
(cid:101)t
λ t(y)=Q(cid:101)t(y,x (cid:101)t−),wehavethat
(cid:90)
(cid:88)
λ t(y)ν(dy)= Q(cid:101)t(y,x (cid:101)t−)=−Q t(x (cid:101)t−,x (cid:101)t−)=−Q t(x (cid:101)tn,x (cid:101)tn), fort∈(t n,t n+1].
X y∈X
ByLemmaA.10,thetimeinterval∆(cid:101)t
n
=(cid:101)t n+1−(cid:101)t nisdistributedaccordingto(A.10),i.e.
(cid:18)(cid:90) τ (cid:19) (cid:18) (cid:90) τ(cid:90) (cid:19)
P(∆(cid:101)t
n
>τ)=exp Q τ′(x (cid:101)tn,x (cid:101)tn)dτ′ =exp − λ τ′(y)ν(dy)dτ′ .
0 0 X
Similarly,thejumplocationx isdistributedaccordingto(A.11),i.e.
(cid:101)tn+1
Q (y,x ) λ (y)
P(x (cid:101)tn+1 =y)=− Q tn+ (1 x ,(cid:101) xtn ) = (cid:82) λ tn+ (1 y)ν(dy).
tn+1 (cid:101)tn (cid:101)tn X tn+1
Nowweturntothestochasticintegral(3.1). BydefinitionofthePoissonrandommeasure,wehave
(cid:18) (cid:90) tn+τ(cid:90) (cid:19)
P(∆t >τ)=P(N[λ]((t ,t +τ]×X)=0)=exp − λ (y)ν(dy)dτ′ ,
n n n τ′
tn X
andthejumplocationisdistributedaccordingto(A.7),i.e.
λ (y)
P(x tn+1 =y)= (cid:82)
λ
tn+ (1 y)ν(dy).
X tn+1
Comparingtheargumentsabove,weconcludethatthetwoprocessesx andx areidenticallydis-
t (cid:101)t
tributedforanyt∈[0,t ],andtheinductioniscomplete.
n+1
Theproofoftheequivalencebetweenthebackwardprocessofthediscretediffusionmodelgoverned
by (2.8) and the corresponding stochastic integral (3.2) can be conducted similarly, and the result
follows.
26
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗Preprintunderreview
A.4.1 τ-LEAPING
ProofofProposition4.1. Withoutlossofgenerality,wegivetheprooffors = s ,andthegeneral
N
casecanbeprovedsimilarly.
Thestochasticintegral(4.2)canbepartitionedbythetimediscretization(s ) intoN intervals
i i∈[0:N]
alongwhichtheevolvingintensityisconstant,i.e.
(cid:90) s(cid:90)
y =y + (y−y )N[µ ](ds,dy)
(cid:98)sN (cid:98)0 (cid:98)⌊s⌋− (cid:98)⌊·⌋
0 X
(cid:88)N (cid:90) si (cid:90)
=y + (y−y )N[µ ](ds,dy)
(cid:98)0
i=1 si−1 X
(cid:98)s−
i−1
(cid:98)si−1
N (cid:90)
(cid:88)
=y + (y−y )N[µ ]((s ,s ],dy),
(cid:98)0
X
(cid:98)s−
i−1
(cid:98)si−1 i−1 i
i=1
whichgivenXisfinite,canbefurtherdecomposedintothefollowingsumofjumps:
N (cid:90)
(cid:88)
y =y + (y−y )N[µ ]((s ,s ],dy)
(cid:98)sN (cid:98)0
X
(cid:98)s−
i−1
(cid:98)si−1 i−1 i
i=1
N
(cid:88)(cid:88)
=y + (y−y )N[µ ]((s ,s ],{y})
(cid:98)0 (cid:98)s−
i−1
(cid:98)si−1 i−1 i
i=1y∈X
N
(cid:88)(cid:88)
∼y + (y−y )P((s −s )µ (y)),
(cid:98)0 (cid:98)s−
i−1
i i−1 (cid:98)si−1
i=1y∈X
whichisexactly(4.1)intheτ-leapingalgorithm(Algorithm1).
A.4.2 UNIFORMIZATION
Theorem A.11 (Accurate Simulation by Uniformization). The uniformization algorithm (Algo-
rithm2)withitsstochasticintegralformulationin(4.4)isequivalenttotheapproximatebackward
processwithitsstochasticintegralformulationin(A.12).
ProofofProposition4.2andTheoremA.11. For simplicity, we only consider the stochastic inte-
gral(4.4)withinthetimeinterval(s ,s ].
b b+1
Werewritethestochasticintegral(4.4)asasumofjumps:
N
(cid:88)
y sb+1 =y sb + (Y n−y s− b,n)1 0≤Ξn≤(cid:82) Xµ(cid:98)θ ssb,n(y)ν(dy),
i=1
wherebyPropositionA.7,(s ) arethejumptimesand(Y ,Ξ ) arethejumplocations
b,n n∈[N] n n n∈[N]
thataredistributedaccordingto
µθ (y) µθ (y)
P(Y =y,Ξ =ξ)= (cid:98)sb,n = (cid:98)sb,n . (A.13)
n n (cid:82)
(cid:82) Xµ (cid:98)θ sb,n(y)ν(dy)(cid:82) 0λ dξ Xµ (cid:98)θ sb,n(y)ν(dy)λ
Therefore,then-thjumpisnotperformedif(cid:82) µθ (y)ν(dy)<Ξ ≤λ,whichisofprobability
X(cid:98)ssb,n n
P(Ξ
n
>µ (cid:98)θ sb,n(y
s−
b,n))=
(cid:82)
(cid:82)X
Xµ
µ(cid:98)
(cid:98)θ
s
θ
sb b, ,n
n( (y y) )ν ν( (d dy y) )λ−(cid:82) Xµ (cid:98)θ
ss
λb,n(y)ν(dy)
(cid:82) µθ (y)ν(dy)
=1−
X(cid:98)sb,n
,
λ
27Preprintunderreview
andistothestateywithprobability
(cid:18) (cid:90) (cid:19)
P Y =y,Ξ ≤ µθ (y)ν(dy)
n n X(cid:98)ssb,n
µθ (y) (cid:82) µθ (y)ν(dy) µθ (y)
=(cid:82)
(cid:98)sb,n X(cid:98)sb,n
=
(cid:98)sb,n
,
µθ (y)ν(dy) λ λ
X(cid:98)sb,n
whichcoincideswith(4.3)intheuniformizationalgorithm(Algorithm2).
By conditioning on the occurrence of each jump, i.e. 0 ≤ Ξ ≤ (cid:82) µθ (y)ν(dy), with slight
n X(cid:98)ssb,n
abuseofnotation,wehavethat
(cid:18) (cid:12) (cid:90) (cid:19) µθ (y)
P Y n =y(cid:12) (cid:12) (cid:12)0≤Ξ n ≤ Xµ (cid:98)θ ssb,n(y)ν(dy) = (cid:82) Xµ (cid:98)θ s(cid:98) bs ,nb, (n y)ν(dy),
which again by Proposition A.7 implies that y also satisfies the stochastic integral (A.12) corre-
s
spondingtotheapproximatebackwardprocess,andviceversa,andtheresultfollows.
B RESULTS FOR CONTINUOUS-TIME MARKOV CHAIN
Inthissection,wewillprovidesomeresultsforthecontinuous-timeMarkovchain(CTMC),includ-
ingthemixingtime,thespectralgap,themodifiedlog-Sobolevconstant,etc..
DefinitionB.1(GraphCorrespondingtoRateMatrix). WedenoteG(Q)asthegraphcorresponding
totheratematrixQ,i.e.
G(Q)=(X,E(G(Q)),Q), whereE(G(Q))={(x,y)∈X×X|x̸=y,Q(x,y)>0},
andtheweightofthedirectededge(x,y)∈E(G(Q))isQ(x,y).
We will assume that the continuous-time Markov chain is irreducible and reversible on the state
spaceX,andthecorrespondingstationarydistributionisπ.
B.1 SPECTRALGAP
Definition B.2 (Spectral Gap). Let L = −Q be the graph Laplacian matrix with D = diagL,
correspondingtothegraphG(Q). with
0=λ 1(L)<λ 2(L)≤...≤λ |X|(L)≤2maxD(x,x)=2D,
x∈X
thespectralgapλ(Q)oftheratematrixQisdefinedasthesecondsmallesteigenvalueofthegraph
LaplacianL,i.e. λ(Q)=λ (L).
2
Remark B.3 (Asymptotic Behavior of the score function s ). Assume Q is symmetric with the
t
followingorthogonaleigendecomposition:
Q=−UΛU⊤,
where U = (u 1,u 2,...,u |X|) is an orthogonal matrix, and the distribution p
t
has the following
decompositionw.r.t. theeigenvectorsofthegraphLaplacianL:
|X|
(cid:88)
p = α (i)u =Uα(t),
t t i
i=1
thenthesolutiontothemasterequation(2.7)isgivenby
|X|
(cid:88)
p =exp(tQ)p =Uexp(−tΛ)U⊤p =Uexp(−tΛ)α = u exp(−tλ )α (j),
t 0 0 0 j j 0
j=1
i.e. α =exp(−tΛ)α andthusforanyi∈[|X|],
t 0
|X|
(cid:88) (cid:88)
p (i)−p (i)= u (i)(−1+exp(−tλ ))α (j)=− u (i)α (j)λ O(t).
t 0 j j 0 j 0 j
j=1 j>1
28Preprintunderreview
Therefore,wehave
(cid:80)
s (x,y)=
p t(y)
=
p 0(y)− j>1u j(y)α 0(j)λ jO(t)
≲1∨(Ft)−1,
t p (x) p (x)−(cid:80) u (x)α (j)λ O(t)
t 0 j>1 j 0 j
giventhatthefollowingconditionissatisfied
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)(cid:88) (cid:12)
F =min(cid:12) u j(x)α 0(j)λ j(cid:12)>0,
x∈X(cid:12) (cid:12)
(cid:12)j>1 (cid:12)
whichonlydependsontheinitialdistributionp andtheratematrixQ.
0
Especially,thebounds (x,y) ≲ 1foranyx ∈ Xs.t. p (x) > 0ands (x,y) ≲ t−1 forthoses.t.
t 0 t
p (x)=0.
0
DefinitionB.4(Conductance). Theconductanceϕ(G)ofagraphG isdefinedas
(cid:80)
Q(x,y)
ϕ(G)=min x∈S,y∈/S .
(cid:110) (cid:111)
S⊂Xmin (cid:80) D(x,x),(cid:80) D(y,y)
x∈S y∈/S
Theorem B.5 (Cheeger’s Inequality). Denote the normalized graph Laplacian matrix by L(cid:101) =
D−1/2LD−1/2witheigenvalues
0≤λ 1(L)≤λ 2(L)≤...≤λ |X|(L)≤2,
thentheconductanceofthegraphG(Q)canbeboundedby
1 (cid:113)
λ (L)≤ϕ(G(Q))≤ 2λ (L).
2 2 2
B.2 LOG-SOBOLEVINEQUALITIES
DefinitionB.6(ModifiedLog-SobolevConstant(Bobkov&Tetali,2006)). Foranyfunctionf,g :
X→R,wedenotetheentropyfunctionalEnt (f)off as
π
Ent (f):=E [flogf]−E [f]logE [f],
π π π π
andtheDirichletformE (f,g)as
π
(cid:88) (cid:88)
E (f,g)=E [fLTg]:= f(y)(LTg)(y)π(y)= f(y)L(x,y)g(x)π(y),
π π
y∈X x,y∈X
wheretheLaplacianL=Q.Thenthemodifiedlog-SobolevconstantoftheratematrixQisdefined
as
(cid:26) (cid:12) (cid:27)
ρ(Q):=inf E π E( nf t,lo (fg )f) (cid:12) (cid:12) (cid:12)f :X→R, Ent π(f)>0 .
π
TheoremB.7(Theorem2.4,(Bobkov&Tetali,2006)). Foranyinitialdistributionp ,wehavefor
0
anyt≥0,
D (p ∥π)≤D (p ∥π)exp(−ρ(Q)t),
KL t KL 0
i.e. theKLdivergenceconvergesexponentiallyfastwithrateρ(Q).
Proof. Noticing that Ent (pt) = D (p ∥π), we differentiate D (p ∥π) with respect to t to
π π KL t KL t
obtainthat
(cid:18) (cid:19) (cid:18) (cid:19)
d
D (p
∥π)=d (cid:88) p t(x)
log
p t(x)
π(x)=
(cid:88) logp t(x)
+1
π(x)d p t(x)
dt KL t dt π(x) π(x) π(x) dt π(x)
x∈X x∈X
(cid:18) (cid:19)
=(cid:88) logp t(x)
+1
d
p (x)=−
(cid:88) logp t(x)
L(x,y)p (y)
π(x) dt t π(x) t
x∈X x,y∈X (B.1)
(cid:32) (cid:33)
=−(cid:88) p t(y) (cid:88) L(x,y)logp t(x)
π(y)
π(y) π(x)
y∈X x∈X
(cid:16)p p (cid:17)
=−E t,log t ≤−ρ(Q)D (p ∥π),
π π π KL t
andtheresultfollowsbyapplyingGrönwall’sinequalitytobothsidesabove.
29Preprintunderreview
Then,thefollowingpropositionconnectsthemodifiedlog-Sobolevconstantwiththespectralgap.
Proposition B.8 ((Bobkov & Tetali, 2006, Proposition 3.5)). The modified log-Sobolev constant
ρ(Q)oftheratematrixQisboundedbythespectralgapλ(Q),i.e. ρ(Q)≤λ(Q).
Proof. Below we provide a sketch of the informal proof of the proposition above for the sake of
completeness. Letf :X→Rbeanarbitraryfunctionandζ >0beanypositivenumber. Fromthe
definitionofthemodifiedlog-Sobolevconstant,wehave
E (eζf,ζf)≥ρ(Q)Ent (eζf). (B.2)
π π
Under the limit ζ → 0+, we may apply Taylor expansion to the two terms on the LHS and RHS,
whichimplies
E (eζf,ζf)=E (1+ζf +O(ζ2),ζf)
π π
=ζE (1,f)+ζ2E (f,f)+O(ζ3)=ζ2E (f,f)+O(ζ3)
π π π
Ent (eζf)=E [eζfζf]−E [eζf]logE [eζf]
π π π π
=E (cid:2)(cid:0) 1+ζf +O(ζ2)(cid:1) ζf(cid:3) −E [1+ζf +O(ζ2)]logE [eζf]
π π π
(B.3)
=ζE [f]+ζ2E [f2]+O(ζ3)
π π
−(cid:0) 1+ζE [f]+O(ζ2)(cid:1) log(cid:0) 1+ζE [f]+O(ζ2)(cid:1)
π π
=ζE [f]+ζ2E [f2]+O(ζ3)−(cid:0) 1+ζE [f]+O(ζ2)(cid:1)(cid:0) ζE [f]+O(ζ2)(cid:1)
π π π π
=ζ2(cid:0)E [f2]−E [f]2(cid:1) +O(ζ3)
π π
Substituting(B.3)into(B.2)andtakingthelimitζ →0+thenyieldthefollowinginequality
E (f,f)
ρ(Q)≤ π
E [f2]−E [f]2
π π
foranynon-constantfunctionf :X→R. Takinginfimumonbothsidesabovewithrespecttoallf
thenindicates
E (f,f) E (f,f)
ρ(Q)≤inf π ≤ inf π =λ (L)=λ(Q) (B.4)
f E π[f2]−E π[f]2 f:E π[f]=0 E π[f2] 2
wherethelasttwoequalitiesabovefollowsfromthedefinitionofspectralgap,asdesired.
Ingeneral,thelowerboundofthemodifiedlog-Sobolevconstantρ(Q)andthespectralgapλ(Q)
dependsontheconnectivityandotherspecificstructuresofthegraphG(Q),andtherelatedresearch
isstillanactiveareaonagraph-by-graphbasis(Bobkov&Tetali,2006).
Thepropertiesofthespectralgapλ(Q)arebetterknownintheliterature,asitiscloselyrelatedtothe
conductanceofthegraphG(Q)viaCheeger’sinequality(TheoremB.5),andthuswhenD = DI,
thespectralgapλ(Q)satisfies
(cid:114)
1 1 (cid:113) 2λ(Q)
λ(Q)= λ (L)≤ϕ(G(Q))≤ 2λ (L)= .
2D 2 2 2 D
However,asshowninPropositionB.8,thelowerboundonthemodifiedlog-Sobolevconstantρ(Q)
ishardtoobtain,astheKLdivergence,theexponentialconvergenceofwhichiscontrolledbyρ(Q),
isstrongerthanthetotalvariationdistance, theexponentialconvergenceofwhichiscontrolledby
λ(Q), via Pinsker’s inequality. The following theorem gives a rough lower bound on d-regular
graphs.
Theorem B.9 ((Bobkov & Tetali, 2006, Proposition 5.4)). Suppose G is a d-regular graph on X
with unit weights and Q is the corresponding rate matrix such that G(Q) = G, then the modified
log-Sobolevconstantρ(Q)oftheratematrixQsatisfies
λ(Q) 8dlog|X|
≤ρ(Q)≤ ,
log|X| diam(G)2
wherediam(G)isthediameterofthegraphG.
30Preprintunderreview
For some specific graphs, the modified log-Sobolev constant ρ(Q) and the spectral gap λ(Q) can
beexplicitlycalculated,suchasthefollowingexamples:
ExampleB.10(Hypercube(Gross,1975)). LetX={0,1}dandQbetheratematrixforwhichthe
graphG(Q)isahypercube,andforanytwostatesx,y ∈ X,therateQ(x,y) = 1ifxandy differ
inexactlyonecoordinate. Thenthemodifiedlog-Sobolevconstantρ(Q)andthespectralgapλ(Q)
aregivenby
ρ(Q)=λ(Q)=4,
whichisdimensionless.
ExampleB.11(AsymmetricHypercube(Diaconis&Saloff-Coste,1996)). LetX={0,1}dandQ
betheratematrixforwhichthegraphG(Q)isahypercube,andforanytwostatesx,y ∈X,therate
Q(x,y) = pifxandy differinexactlyonecoordinateandxisthestatewith0inthatcoordinate,
andQ(x,y)=q =1−pifwith1inthatcoordinate. Thenthemodifiedlog-Sobolevconstantρ(Q)
andthespectralgapλ(Q)aregivenby
2(p−q) 1
ρ(Q)= , andλ(Q)= .
pq(logp−logq) pq
Furtherresultsonlog-Sobolevinequalitiesrelatedtofinite-stateMarkovchainsarebeyondthescope
of this paper, and we refer the readers to (Stroock, 1993; Saloff-Coste, 1997; Bobkov & Ledoux,
1998;Lee&Yau,1998;Goel,2004;Ledoux,2006)formoredetail.
B.3 MIXINGTIME
DefinitionB.12(MixingTime). Wedefinethemixingtimet (ϵ)ofthecontinuous-timeMarkov
mix
chain with rate matrix Q as the smallest time t such that starting from any initial distribution p ,
0
theKLdivergenceD (p ∥π)islessthanϵ,i.e.
KL t
(cid:26) (cid:12) (cid:27)
t mix(ϵ)=inf t∈R +(cid:12) (cid:12)D KL(p t∥π)=D KL(e−tQp 0∥π)≤ϵ .
(cid:12)
Similarly, we define the mixing time t (ϵ) as the smallest time t such that starting from any
mix,TV
initialdistributionp ,thetotalvariationdistanceTV(p ,π)islessthanϵ,i.e.
0 t
(cid:26) (cid:12) (cid:27)
t mix,TV(ϵ)=inf t∈R +(cid:12) (cid:12)TV(p t,π)=TV(e−tQp 0,π)≤ϵ .
(cid:12)
With a slight abuse of notation, we will also denote the e−1-mixing time as t = t (e−1)
mix mix,KL
andt =t (e−1).
mix,TV mix,TV
PropositionB.13. Themixingtimet (ϵ)ofthecontinuous-timeMarkovchainwithratematrix
mix
Qisboundedbythemodifiedlog-Sobolevconstantρ(Q),i.e.
t
(ϵ)≲ρ(Q)−1(cid:0) logϵ−1+loglogπ−1(cid:1)
,
mix ∗
Andthemixingtimet (ϵ)isboundedbythespectralgapλ(Q),i.e.
mix,TV
t
(ϵ)≲λ(Q)−1(cid:0) logϵ−1+loglogπ−1(cid:1)
.
mix,TV ∗
Proof. Defineπ
∗
=min x∈Xπ(x),wefirstboundD KL(p 0∥π)asfollows:
D (p ∥p )≤
(cid:88)
p
(x)logp 0(x)
≤logπ−1, (B.5)
KL 0 ∞ 0 π(x) ∗
x∈X
andthusbyTheoremB.7,wehave
D (p ∥π)≤D (p ∥π)exp(−ρ(Q)t)≤logπ−1exp(−ρ(Q)t). (B.6)
KL t KL 0 ∗
Therefore,bysettingtheright-handsideof(B.6)tobeϵ,wehavethedesiredresultforthemixing
time
t (ϵ)≤
1 (cid:0) logϵ−1+loglogπ−1(cid:1)
.
mix ρ(Q) ∗
31Preprintunderreview
Forthemixingtimet (ϵ),weusethePinsker’sinequalitytoobtain:
mix,TV
(cid:113)
(cid:112)
TV(p ,π)≤2 D (p ∥π)≤2 logπ−1exp(−ρ(Q)t),
t KL t ∗
andtherefore,
t (ϵ)≲ 1 log(cid:18) ϵ−1(cid:113) logπ−1(cid:19) ≲ 1 (cid:0) logϵ−1+loglogπ−1(cid:1) .
mix,TV ρ(Q) ∗ ρ(Q) ∗
CorollaryB.14. Thee−1-mixingtimet andt ofthecontinuous-timeMarkovchainwith
mix mix,TV
ratematrixQsatisfy
t ≲ρ(Q)−1loglogπ−1, and t ≲λ(Q)−1loglogπ−1,
mix ∗ mix,TV ∗
andthust ≳t .
mix mix,TV
C PROOFS OF ERROR ANALYSIS IN SECTION 4.3
Inthissection,weprovidetheproofofthemainresultsinthemaintext.
C.1 TRUNCATIONERROR
TheoremC.1(TruncationError). Theforwardprocess(2.7)convergestotheuniformdistribution
p =1/|X|exponentiallyfastintermsoftheKLdivergence,i.e.
∞
(cid:18) (cid:13) 1 (cid:19)
D (p ∥p )=D p (cid:13) ≲e−ρtlog|X|,
KL t ∞ KL t(cid:13)|X|
where|X|isthesizeofthestatespace,andt isthemixingtimeofthecontinuous-timeMarkov
mix
chaincorrespondingtotheratematrixQdefinedinDefinitionB.12.
Proof. Since Q is symmetric, we have the stationary distribution π = 1/|X| and thus
D (p ∥p )=D (p ∥π),andπ =1/|X|.
KL t ∞ KL t ∗
ByAssumption4.3andCorollaryB.14,wehave
D (p ∥π)≤e−ρ(Q)tD (p ∥π)≤e−ρtlogπ−1 ≤e−ρtlog|X|,
KL t KL 0 ∗
wherethelastinequalityisby(B.5).
C.2 DISCRETIZATIONERROR
DenotetheshorthandnotationG(x;y) = x(logx−logy)−x;itiseasytocheckthatG′(x;y) =
logx−logy.
PropositionC.2. Foranyy ∈X,wehave
|∂ G(µ (y);µθ (y))|≲(logC+log(1∨(T −σ))+logM)µ (y)D(cid:0) 1∨(T −σ)−2(cid:1) .
σ σ (cid:98)⌊sn⌋ σ
Proof. Bythechainrule,wehave
(cid:16) (cid:17)
∂ G(µ (y);µθ (y))=G′(µ (y);µθ (y))∂ µ (y)= logµ (y)−logµθ (y) ∂ µ (y).
σ σ (cid:98)⌊sn⌋ σ (cid:98)⌊sn⌋ σ σ σ (cid:98)⌊sn⌋ σ σ
32Preprintunderreview
Wefirstcompute∂ µ (y)as
σ σ
(cid:18) (cid:19)
p (y)
∂ σµ σ(y)=Q(cid:101)(x σ−,y)∂ σs σ(x σ−,y)=Q(cid:101)(x σ−,y)∂
σ p
(σ
x )
σ σ−
(cid:18) (cid:19)
1 p (y)
=Q(cid:101)(x σ−,y)
p (x
)∂ σp σ(y)−
p
(σ
x
)2∂ σp σ(x σ−)
σ σ− σ σ−
 
p (y) (cid:88) p (y′) p (y) (cid:88) p (y′)
=Q(cid:101)(x σ−,y)−
p
(σ
x )
pσ (y)Q(y,y′)+
p
(σ
x ) p
σ
(x
)Q(x σ−,y′)
σ σ− y′∈X σ σ σ− y′∈X σ σ−
 
(cid:88) (cid:88)
=µ σ(y)− s σ(y,y′)Q(y,y′)+ s σ(x σ−,y′)Q(x σ−,y′),
y′∈X y′∈X
bywhichwehave
 
|∂ σµ σ(y)|≲µ σ(y)(cid:88) (cid:0) 1∨(T −σ)−2(cid:1) |Q(x σ−,y′)|≲µ σ(y)D(cid:0) 1∨(T −σ)−2(cid:1) ,
y′∈X
andthus
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)∂ G(µ (y);µθ (y))(cid:12)≤(cid:12)logµ (y)−logµθ (y)(cid:12)|∂ µ (y)|
(cid:12) σ σ (cid:98)⌊sn⌋ (cid:12) (cid:12) σ (cid:98)⌊sn⌋ (cid:12) σ σ
(cid:16) θ (cid:17) ≤ |logQ(cid:101)(x σ−,y)|+|logs σ(x σ−,y)|+|logQ(cid:101)(x
s−
n,y)|+|logs (cid:98)sn(x
s−
n,y)| |∂ σµ σ(y)|
≲µ (y)(cid:0) logC+log(cid:0) 1∨(T −σ)−1(cid:1) +logM(cid:1) D(cid:0) 1∨(T −σ)−2(cid:1) .
σ
PropositionC.3. Forany0<s<t≤T,wehave
(cid:90) t(cid:90)
µ (y′)ν(dy′)dσ ≲(cid:0) 1∨(T −t)−1(cid:1) D(t−s).
σ
s X
Proof.
(cid:90) t(cid:90) (cid:90) t(cid:90)
µ σ(y′)ν(dy′)dσ = s σ(x σ−,y′)Q(cid:101)(x σ−,y′)ν(dy′)dσ
s X s X
(cid:90) t(cid:90)
≲(cid:0) 1∨(T −t)−1(cid:1) Q(cid:101)(y,x σ−)ν(dy′)dσ (C.1)
s X
(cid:90) t
≲(cid:0)
1∨(T
−t)−1(cid:1)
|Q(x ,x )|dσ
≲(cid:0)
1∨(T
−t)−1(cid:1)
D(t−s).
σ− σ−
s
PropositionC.4. Foranyy ∈X,wehave
(cid:104)(cid:12) (cid:12)(cid:105)
E (cid:12)G(µ ;µθ (y))−G(µ ;µθ (y))(cid:12)
(cid:12) s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ (cid:12)
≲(cid:0) logC+log(cid:0)
1∨(T −s
)−1(cid:1) +logM(cid:1)
µ (y)D(s−s
)(cid:0)
1∨(T −s
)−1−γ(cid:1)
.
n+1 σ n n+1
Proof. ApplyingTheoremA.9tothebackwardprocess(3.2),wehave
(cid:90) s
G(µ (y);µθ (y))=G(µ (y);µθ (y))+ ∂ G(µ (y);µθ (y))dσ
s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ σ σ (cid:98)⌊sn⌋
sn
(cid:90) s(cid:90) (cid:16) (cid:17)
+ G(µ (y);µθ (y))−G(µ (y);µθ (y)) N[µ](dσ,dy′),
sn X
σ+ (cid:98)⌊sn⌋ σ (cid:98)⌊sn⌋
whereweadoptthenotationµ (y)astherightlimitofthecàglàdprocessµ (y), i.e. µ (y) =
σ+ σ σ+
s σ(x σ,y)Q(cid:101)(x σ,y).
33
⃗ ⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗ ⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗Preprintunderreview
Forthefirstterm,wehavebyPropositionC.2that
(cid:12) (cid:12) (cid:12) (cid:12)(cid:90) s ∂ σG(µ σ(y);µ (cid:98)θ ⌊sn⌋(y))dσ(cid:12) (cid:12) (cid:12) (cid:12)≲(cid:90) s(cid:12) (cid:12) (cid:12)∂ σG(µ σ(y);µ (cid:98)θ ⌊sn⌋(y))(cid:12) (cid:12) (cid:12)dσ
sn sn
(cid:90) s
≲ (cid:0) logC+log(cid:0) 1∨(T −σ)−1(cid:1) +logM(cid:1) µ (y)D(cid:0) 1∨(T −σ)−2(cid:1) dσ
σ
sn
≲(cid:0) logC+log(cid:0)
1∨(T −s
)−1(cid:1) +logM(cid:1)
µ (y)D(s−s
)(cid:0)
1∨(T −s
)−1(cid:1)
.
n+1 σ n n+1
Forthesecondterm,wehave
(cid:12) (cid:12)
(cid:12)G(µ (y);µθ (y))−G(µ (y);µθ (y))(cid:12)
(cid:12) σ+ (cid:98)⌊sn⌋ σ (cid:98)⌊sn⌋ (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
=(cid:12)G′(ξ;µθ (y))(µ (y)−µ (y))(cid:12)=(cid:12)(logξ−logµθ (y))(µ (y)−µ (y))(cid:12)
(cid:12) (cid:98)⌊sn⌋ σ+ σ (cid:12) (cid:12) (cid:98)⌊sn⌋ σ+ σ (cid:12)
≤(cid:16) |logµ σ+(y)|+|logµ σ(y)|+(cid:12) (cid:12) (cid:12)logµ (cid:98)θ ⌊sn⌋(y)(cid:12) (cid:12) (cid:12)(cid:17)(cid:12) (cid:12) (cid:12) (cid:12)µ µσ+ (( yy )) −1(cid:12) (cid:12) (cid:12) (cid:12)µ σ(y)
σ
≲(cid:0) logC+log(cid:0)
1∨(T
−σ)−1(cid:1) +logM(cid:1)(cid:0)
1∨(T
−σ)−γ(cid:1)
µ (y),
σ
wherethefirstequalityfollowsfromthemeanvaluetheoremandthelastinequalityisbyAssump-
tion4.5.
Therefore,
(cid:104)(cid:12) (cid:12)(cid:105)
E (cid:12)G(µ ;µθ (y))−G(µ ;µθ (y))(cid:12)
(cid:12) s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ (cid:12)
(cid:20)(cid:12)(cid:90) s (cid:12)(cid:21)
≤E (cid:12) (cid:12)
(cid:12)
∂ σG(µ σ(y);µ (cid:98)θ ⌊sn⌋(y))dσ(cid:12) (cid:12)
(cid:12)
sn
(cid:20)(cid:90) s(cid:90) (cid:12) (cid:12) (cid:21)
+E (cid:12)G(µ (y);µθ (y))−G(µ (y);µθ (y))(cid:12)N[µ](dσ,dy′)
sn
X(cid:12) σ+ (cid:98)⌊sn⌋ σ (cid:98)⌊sn⌋ (cid:12)
≲(cid:0) logC+log(cid:0)
1∨(T −s
)−1(cid:1) +logM(cid:1)
µ (y)D(s−s
)(cid:0)
1∨(T −s
)−1(cid:1)
n+1 σ n n+1
(cid:90) s(cid:90)
+ (cid:0) logC+log(cid:0) 1∨(T −σ)−1(cid:1) +logM(cid:1) µ (y)(cid:0) 1∨(T −σ)−γ(cid:1) µ (y′)ν(dy′)dσ
σ σ
sn X
≲(cid:0) logC+log(cid:0)
1∨(T −s
)−1(cid:1) +logM(cid:1)
µ (y)D(s−s
)(cid:0)
1∨(T −s
)−1(cid:1)
n+1 σ n n+1
+(cid:0) logC+log(cid:0)
1∨(T −s
)−1(cid:1) +logM(cid:1)
µ
(y)(cid:0)
1∨(T −s
)−1−γ(cid:1)
(s−s )D
n+1 σ n+1 n
≲(cid:0) logC+log(cid:0)
1∨(T −s
)−1(cid:1) +logM(cid:1)
µ (y)D(s−s
)(cid:0)
1∨(T −s
)−1−γ(cid:1)
,
n+1 σ n n+1
wherethesecondtolastinequalityisbyPropositionC.2.
PropositionC.5(DiscretizationError). Thefollowingboundholds
(cid:90) T−δ(cid:90) (cid:12) (cid:12)
(cid:12)G(µ (y);µθ (y))−G(µ (y);µθ (y))(cid:12)ν(dy)ds
0
X(cid:12) s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ (cid:12)
(cid:40) 2
D κT, γ <1,
≲
D2 κ(cid:0)
T
+log2δ−1(cid:1)
, γ =1,
(cid:26) κ−1T, γ <1
withN = steps,bytakingγ <η ≲1−T−1whenγ <1,andη =1
κ−1(T +logδ−1), γ =1
when γ = 1. In particular, in the former case, early stopping at time T −δ is not necessary, i.e.
δ =0.
34Preprintunderreview
Proof. WehavebyPropositionC.4that
(cid:90) sn+1(cid:90) (cid:12) (cid:12)
(cid:12)G(µ (y);µθ (y))−G(µ (y);µθ (y))(cid:12)ν(dy)ds
sn
X(cid:12) s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ (cid:12)
(cid:90) sn+1(cid:90)
≲ µ (y)ν(dy)
σ
sn X
(cid:0) logC+log(cid:0)
1∨(T −s
)−1(cid:1) +logM(cid:1)
D(s−s
)(cid:0)
1∨(T −s
)−1−γ(cid:1)
ds
n+1 n n+1
≲(cid:0) logC+log(cid:0)
1∨(T −s
)−1(cid:1) +logM(cid:1) D2
(s −s
)2(cid:0)
1∨(T −s
)−1−γ(cid:1)
.
n+1 n+1 n n+1
• Case1: γ <η ≲1−T−1
Thefollowingboundholds
(cid:90) sn+1(cid:90) (cid:12) (cid:12)
(cid:12)G(µ (y);µθ (y))−G(µ (y);µθ (y))(cid:12)ν(dy)ds
sn
X(cid:12) s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ (cid:12)
≲(cid:0) 1+log(cid:0)
1∨(T −s
)−1(cid:1)(cid:1) D2
κ(s −s
)(cid:0)
1∨(T −s
)−1−γ+η(cid:1)
,
n+1 n+1 n n+1
andthus,thefollowingerror
N (cid:88)−1(cid:90) sn+1(cid:90) (cid:12) (cid:12)
(cid:12)G(µ (y);µθ (y))−G(µ (y);µθ (y))(cid:12)ν(dy)ds
n=0 sn
X(cid:12) s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ (cid:12)
N−1
≲ (cid:88) (cid:0) 1+log(cid:0) 1∨(T −s )−1(cid:1)(cid:1) D2 κ(s −s )(cid:0) 1∨(T −s )−1−γ+η(cid:1)
n+1 n+1 n n+1
n=0
(cid:18) (cid:90) 1 (cid:19) (cid:32) (cid:90) δ−1 (cid:33)
≲D2 κ T + t−1−γ+ηlogt−1dt ≲D2 κ T + t−1−(η−γ)logtdt
δ 1
≲D2 κ(cid:0)
T
+δη−γlogδ−1(cid:1) →D2
κT, asδ →0,
isachievablewithfinitenumberofstepsN,i.e.
(cid:90) T 1 (cid:90) 1 (cid:18) 1 (cid:19)
N ≲ dt≲κ−1T +κ−1 t−ηdt≲κ−1 T + ≲κ−1T,
κ(1∧tη) 1−η
δ δ
wherewetakeη ≲1−T−1.
• Case2: γ =η =1
Wehavethefollowingbound
(cid:90) sn+1(cid:90) (cid:12) (cid:12)
(cid:12)G(µ (y);µθ (y))−G(µ (y);µθ (y))(cid:12)ν(dy)ds
sn
X(cid:12) s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ (cid:12)
≲(cid:0) 1+log(cid:0)
1∨(T −s
)−1(cid:1)(cid:1) D2
κ(s −s
)(cid:0)
1∨(T −s
)−1(cid:1)
,
n+1 n+1 n n+1
andsimilarly
N (cid:88)−1(cid:90) sn+1(cid:90) (cid:12) (cid:12)
(cid:12)G(µ (y);µθ (y))−G(µ (y);µθ (y))(cid:12)ν(dy)ds
n=0 sn
X(cid:12) s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ (cid:12)
N−1
≲ (cid:88) (cid:0) 1+log(cid:0) 1∨(T −s )−1(cid:1)(cid:1) D2 κ(s −s )(cid:0) 1∨(T −s )−1(cid:1)
n+1 n+1 n n+1
n=0
(cid:32) (cid:90) δ−1 (cid:33)
≲D2 κ T + t−1logtdt ≲D2 κ(cid:0) T +log2δ−1(cid:1) .
1
However,thenumberofstepsN isnowboundedby
(cid:90) T 1
N ≲ dt≲κ−1(T +logδ−1).
κ(1∧t)
δ
35Preprintunderreview
C.3 OVERALLERRORBOUND
TheoremC.6. Letp andq bethepathmeasuresofthebackwardprocesswiththestochastic
0:T (cid:98)0:T
integralformulation(3.2)andtheinterpolatingprocess(4.2)ofτ-leapingalgorithm(Algorithm(1)),
thenitholdsthat
(cid:34) (cid:32) (cid:33) (cid:35)
(cid:90) T (cid:90) µ (y)
D (p ∥q )=D (p ∥q )+E µ (y)log s −µ (y)+µθ (y) ν(dy)dt ,
KL 0:T (cid:98)0:T KL 0 (cid:98)0 0 X s µ (cid:98)θ ⌊s⌋(y) s (cid:98)⌊s⌋
(C.2)
wheretheexpectationistakenw.r.t. pathsgeneratedbythebackwardprocess(3.2).
Now,wearereadytopresenttheproofofTheorem4.7.
ProofofTheorem4.7. Wefirstrewritetheintegralin(C.2)as
(cid:32) (cid:33)
(cid:90) sn+1(cid:90) µ (y)
µ (y)log s −µ (y)+µθ (y) ν(dy)ds
sn X s µ (cid:98)θ ⌊s⌋(y) s (cid:98)⌊s⌋
(cid:90) sn+1(cid:90) (cid:18) µ (y)
= µ (y)log sn −µ (y)+µθ (y)
sn X sn µ (cid:98)θ ⌊sn⌋(y) sn (cid:98)⌊s⌋
(cid:19)
+G(µ (y);µθ (y))−G(µ (y);µθ (y)) ν(dy)ds.
s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋
Therefore,theoverallerrorisboundedby
D (p ∥q )
KL 0:T−δ (cid:98)0:T−δ
(cid:34) (cid:32) (cid:33) (cid:35)
(cid:90) T−δ(cid:90) µ (y)
≲D (p ∥q )+E µ (y)log sn −µ (y)+µθ (y) ν(dy)dt
KL 0 (cid:98)0 0 X sn µ (cid:98)θ ⌊sn⌋(y) sn (cid:98)⌊sn⌋
(cid:90) sn+1(cid:90) (cid:12) (cid:12)
+ (cid:12)G(µ (y);µθ (y))−G(µ (y);µθ (y))(cid:12)ν(dy)ds
sn
X(cid:12) s (cid:98)⌊sn⌋ sn (cid:98)⌊s⌋ (cid:12)
≲D (p ∥p )
KL T−δ ∞
(cid:20)N−1
(cid:88)
+E (s −s )
n+1 n
n=0
(cid:90) (cid:32) s (x ,y) (cid:33) (cid:21)
X
s sn(x
s−
n,y)log
s (cid:98)θ
ss nn (xs s− n
− n,y)
−s sn(x
s−
n,y)+s (cid:98)θ sn(x
s−
n,y) Q(cid:101)(x
s−
n,y)ν(dy)
N−1
(cid:88) 2
+ (logC+logM)D κ(s −s )
n+1 n
n=0
(cid:40) exp(−ρT)log|X|+ϵ+D2
κT, γ <1,
≲
exp(−ρT)log|X|+ϵ+D2 κ(cid:0)
T
+log2δ−1(cid:1)
, γ =1,
where in the last inequality we used results for the first term (Truncation error, cf. Theorem C.1),
thesecondterm(Approximationerror,cf. Assumption4.6)andthethirdterm(Discretizationerror,
cf. PropositionC.5).
Bytaking
(cid:32) log(cid:0) ϵ−1log|X|(cid:1)(cid:33) (cid:32)
ϵρ
(cid:33)
T =O , κ=O ,
ρ D2 log(ϵ−1log|X|)
deploying the time discretization scheme with γ < η ≲ 1−T−1 when γ < 1, and η = 1 when
γ =1,andperformingearlystoppingas
(cid:40)
0, γ <1,
δ = (cid:16) √ (cid:17)
Ω exp(− T) , γ =1,
36
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗
⃗ ⃗ ⃗ ⃗ ⃗Preprintunderreview
wehaveD (p ∥q )≤D (p ∥q )≲ϵwith
KL T−δ (cid:98)T KL 0:T−δ (cid:98)0:T−δ
(cid:32) D2 log2(cid:0) ϵ−1log|X|(cid:1)(cid:33)
N ≲κ−1T =O
ϵρ2
steps.
ProofofTheorem4.9. Due to the equivalence of the stochastic integral formulation (4.4) of the
uniformizationschemeandtheapproximatebackwardprocess(A.12)establishedinProposition4.2,
theerrorfortheuniformizationschemeisdirectlyboundedbytheerror(C.2)inCorollary3.5,i.e.
D (p ∥q )
KL 0:T−δ 0:T−δ
(cid:34) (cid:90) T−δ(cid:90) (cid:18) µ (y) (cid:19) (cid:35)
≤D (p ∥q )+E µ (y)log s −µ (y)+µθ(y) ν(dy)dt
KL 0 0 s µθ(y) s (cid:98)s
0 X (cid:98)s
≲D (p ∥p )
KL T ∞
(cid:34) (cid:32) (cid:33) (cid:35)
(cid:90) T−δ(cid:90) s (x ,y)
+E s s(x s−,y)log s s− −s s(x s−,y)+s (cid:98)θ s(x s−,y) Q(cid:101)(x s−,y)ν(dy)ds
0 X s (cid:98)θ s(x s−,y)
≲|X|exp(−ρT)+ϵ.
TheexpectationofthenumberofstepsN isboundedby
(cid:34)B−1 (cid:35) B−1
E[N]=E (cid:88) P(cid:0) λ (s −s )(cid:1) = (cid:88) λ (s −s )
sb+1 b+1 b sb+1 b+1 b
b=0 b=0
B−1
(cid:88)
≲ D(1∨(T −s ))−1(s −s )
b+1 b+1 b
b=0
(cid:18) (cid:90) 1 (cid:19)
≲D T + t−1dt =D(T +logδ−1).
δ
Bytaking
(cid:32) log(cid:0) ϵ−1log|X|(cid:1)(cid:33)
T =O , δ =Ω(exp(−T))
ρ
wehaveD (p ∥q )≤D (p ∥q )≲ϵwith
KL T−δ T−δ KL 0:T−δ 0:T−δ
(cid:32) Dlog(cid:0) ϵ−1log|X|(cid:1)(cid:33)
E[N]=O
ρ
steps.
37
⃗
⃗
⃗
⃗
⃗ ⃗
⃗
⃗⃗
⃗
⃗
⃗
⃗ ⃗ ⃗ ⃗ ⃗