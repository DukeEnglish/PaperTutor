Preprint. Underreview.
LINEAR TRANSFORMER TOPOLOGICAL MASKING
WITH GRAPH RANDOM FEATURES
IsaacReid∗1,2,AvinavaDubey2,DeepaliJain2,WillWhitney2,AmrAhmed2,
JoshuaAinslie2,AlexBewley2,MithunJacob2,AranyakMehta2,DavidRendleman2,
ConnorSchenck2,RichardE.Turner1,RenéWagner2,AdrianWeller1,3,
KrzysztofChoromanski2,4
1UniversityofCambridge,2Google,3AlanTuringInstitute,4ColumbiaUniversity
ABSTRACT
When training transformers on graph-structured data, incorporating information
abouttheunderlyingtopologyiscrucialforgoodperformance.Topologicalmask-
ing, a type of relative position encoding, achieves this by upweighting or down-
weightingattentiondependingontherelationshipbetweenthequeryandkeysin
agraph. Inthispaper,weproposetoparameterisetopologicalmasksasalearn-
ablefunctionofaweightedadjacencymatrix–anovel, flexibleapproachwhich
incorporatesastrongstructuralinductivebias. Byapproximatingthismaskwith
graphrandomfeatures(forwhichweprovethefirstknownconcentrationbounds),
weshowhowthiscanbemadefullycompatiblewithlinearattention,preserving
O(N)timeandspacecomplexitywithrespecttothenumberofinputtokens. The
fastestpreviousalternativewasO(NlogN)andonlysuitableforspecificgraphs.
Our efficient masking algorithms provide strong performance gains for tasks on
imageandpointclouddata,includingwith>30knodes.
1 INTRODUCTION
Across data modalities, transformers have emerged as a leading machine learning architecture
(Vaswani,2017;Dosovitskiyetal.,2020;Arnabetal.,2021). Theyderivetheirsuccessfrommod-
elling complex dependencies between the tokens using an attention mechanism. However, in its
vanilla instantiation the transformer is a set function, meaning it is invariant to permutation of its
inputs. Thisisoftenundesireable;e.g.wordstendtobearrangedinaparticularsequence,andgraph
nodesareconnectedtoaparticularsetofneighbours. Therefore,ithasbecomestandardtomodify
theattentionmatrixdependingonanyunderlyingstructure,buildingininductivebias. Forinstance,
causalattentionappliesatriangularbinarymasktoensurethattokensonlyattendtoprecedingel-
ementsofthesequence(Yangetal.,2019). Meanwhile,relativepositionencoding(RPE)captures
thespatialrelationshipbetweenthetokens(Shawetal.,2018). Insomecases,thequeriesandkeys
canbearrangedinagraphG andafunctionM(G)canbeusedtomodulatethetransformeratten-
tion. Thisisreferredtoastopologicalmasking(Choromanskietal.,2022). Itprovidesapowerful
avenue for incorporating structural inductive bias from G into transformers, in some cases closing
theperformancegapwiththebest-customisedgraphneuralnetworks(Yingetal.,2021).
Topologicalmaskingoflow-rankattention. Topologicalmaskingissimplewhentheattentionis
full-rank. Inthiscase,theentireattentionmatrixA ∈ RN×N (withN thenumberofinputtokens)
is materialised in memory, so its individual entries A can be pointwise multiplied by a (fixed or
ij
learnable) mask M(G) . But full-rank attention famously incurs a quadratic O(N2) space and
ij
timecomplexitycost,whichmaybecomeprohibitiveonlargeinputs. Thishasmotivatedanumber
of low-rank decompositions of A. These improve scalability by avoiding computing A explicitly,
insteaddoingsoimplicitlyinfeaturespace. Thequestionofhowtoperformtopologicalmaskingin
thelow-ranksettingischallengingandnotfullysolved. Previouslyproposedalgorithmsarelimited
toinexpressivefunctionsMandrestrictedclassesofgraphsG (e.g.treesandgrids), andareoften
O(NlogN)timecomplexityratherthanO(N)(App.B;Choromanskietal.,2022;Liutkusetal.,
2021). Addressing these shortcomings is the chief focus of this manuscript. Our central question
is: how can we implement efficient yet flexible topological (graph-based) masking of transformer
attention,inthelow-ranksettingwhereAisneverexplicitlyinstantiated?
∗Workcompletedasastudentresearcher.
1
4202
tcO
4
]GL.sc[
1v26430.0142:viXraPreprint. Underreview.
Unmasked < Masked
⊥G ∼G
N d N N d
Regular N A V N A ⊙ M(G) V
O(N2)
E
= m N d Nm(sp.) N d
Linear N Φ Q Φ K V N ΦbQ,G ΦbK,G V
O(N)
Figure 1: Schematic overview. Regular attention is O(N2), with N the number of input tokens.
Topological masking modulates A by a graph function M(G), improving predictive performance.
LinearattentionreducesthetimecomplexitytoO(N)byleveragingalow-rankdecomposition.Our
contribution(blue)isthefirstalgorithmtoachieveboth–O(N)topologicalmaskingoflow-rankat-
tention–byapproximatingM(G)withgraphrandomfeatures(GRFs).GRFsaresparsevectors(de-
noted‘sp.’)computedbysamplingrandomwalks,constructedsothatE(ΦbQ,GΦb⊤ K,G)=A⊙M(G)
withstrongconcentrationproperties(Thm.3.1).
Graph random features. To answer the question posed above, we propose to use graph random
features (GRFs) (Choromanski, 2023; Reid et al., 2024b). GRFs are sparse, randomised vectors
whosedotproductgivesanunbiasedestimateofarbitraryfunctionsofaweightedadjacencymatrix,
a class that captures the structure of G and is popular in classical graph-based machine learning
(SmolaandKondor,2003;KondorandLafferty,2002). GRFshavebeenusedforapproximateker-
nelisednodeclusteringandinscalablegraph-basedGaussianprocesses(Reidetal.,2023a;2024c),
butneverbeforeintransformers. AdvancingunderstandingoftheconvergencepropertiesofGRFs
isanotherkeygoalofthiswork.
Ourcontributions. WepresentthefirstknownO(N)algorithmfortopologicalmaskingoflinear
attentiontransformerswithgeneralG,usinggraphrandomfeatures(Fig.1).
1. We propose to modulate the transformer attention mechanism with a learnable function of a
weightedadjacencymatrix–asimple,generaltopologicalmaskingtrickwhichbringsastrong
inductivebiasandsubstantialimprovementstopredictiveperformance.
2. Forscalability,wereplacetheexact,deterministicmaskwithasparse,stochasticestimatecon-
structedusinggraphrandomfeatures(GRFs). Weuseanovelexponentialconcentrationbound
toprovethatourmethodislineartimeandspacecomplexitywithrespecttoinputsize,thefirst
knownalgorithmwiththisproperty.
3. Wedemonstratestrongaccuracygainsonimagedata,aswellasformodellingthedynamicsof
massivepointclouds(>30kparticles)inroboticsapplicationswhereefficiencyisessential.
2 PRELIMINARIES
Maskedattention. DenotebyN thenumberofinputtokensanddthedimensionalityoftheirlatent
representations. ConsidermatricesQ,K,V∈RN×dwhoserowsaregivenbythequery({q }N ),
i i=1
key({k }N )andvaluevectors({v }N )respectively.Theattentionmechanism,abasicprocessing
i i=1 i i=1
unitofthetransformer,canbewritten:
Att(Q,K,V):=D−1AV, A:=K(Q,K), D:=A1 . (1)
N
Here,thefunctionKassignsasimilarityscoretoeachquery-keypairand1 :=[1]N ∈RN. For
N i=1
regularsoftmaxattention,onesimplytakesK(Q,K) = exp(q⊤k ),optionallyalsonormalising
ij i j
byd. Toincorporatemasking,wemakethemodification
A :=M⊙K(Q,K), (2)
M
where M ∈ RN×N is a (hard or soft) masking matrix and ⊙ denotes the Hadamard (element-
wise)product. Forsoftmax,thisisequivalenttoaddingabiastermb := logM toq⊤k before
ij ij i j
exponentiating. AsremarkedinSec.1,thiscanbeusedtoenforcecausalityorasarelativeposition
2Preprint. Underreview.
encoding (RPE) (Shaw et al., 2018). Taking M(G) so that attention is modulated by a function
of the underlying graph structure is called topological masking. Topological masking is not just
a mechanism to sparsify attention for efficiency gains; its goal is to incorporate topological signal
fromtheunderlyinggraph,improvingpredictiveperformanceviaausefulinductivebias.
Improving efficiency with low-rank attention. The time-complexity of Eq. 1 is O(N2), which
famously limits the ability of regular transformers to scale to long inputs. A leading approach to
mitigate this and recover O(N) complexity is linear or low-rank attention (Katharopoulos et al.,
2020; Choromanski et al., 2020). This takes a feature mapping ϕ : Rd → Rm with m ≪ N and
constructsthematricesΦ :=[ϕ(q )]N andΦ :=[ϕ(k )]N ,whereΦ ∈RN×m. Onethen
Q i i=1 K i i=1 Q,K
definesK (Q,K):=Φ Φ⊤ andcomputes
LR Q K
Att (Q,K,V):=D−1(cid:0) Φ (cid:0) Φ⊤V(cid:1)(cid:1) , D:=Φ (cid:0) Φ⊤1 (cid:1) , (3)
LR Q K Q K N
wheretheparenthesesshowtheorderofcomputation. Exploitingassociativity,thetimecomplexity
is reduced to O(Nmd), albeit usually with some sacrifice in performance compared to full-rank
softmax attention. Different choices of feature maps ϕ exist; for example, one can take a random
featuremapthatprovidesaMonteCarloestimateofthesoftmaxkernel(Performer;Choromanski
etal.,2020;2021;Likhosherstovetal.,2024),oradeterministicnonlinearitylikeelu(x)+x(linear
transformer;Katharopoulosetal.,2020).
Efficientmaskingoflow-rankattention. AsEq.3suggests, low-rankattentionderivesitsspeed
from avoiding computing A = K (Q,K) explicitly, preferring to evaluate faster products like
LR
Φ⊤V and Φ⊤1 first and use them to weight the query features Φ . This makes the direct
K K N Q
HadamardproductimplementationofmaskinginEq.2impossible: ifthematrixAisneverexplic-
itlymaterialisedinmemory, onecannotpointwisemultiplyeachofitselements. Algorithmshave
been proposed to mask implicitly (without instantiating A or M) in very simple cases, e.g. causal
masking(Choromanskietal.,2020)orone-dimensionalRPE(Liutkusetal.,2021;Luoetal.,2021).
SubquadraticimplicitmaskingisalsopossiblewhenMisverystructured:e.g.functionsofshortest
path distance on grid graphs and trees (Choromanski et al., 2022), or of the degrees of the pair of
inputnodes(Chenetal.,2024). SeeApp.Bforareview. However,anO(N)algorithmcapableof
maskingwithflexiblefunctionsongeneralgraphshas,untilnow,remainedoutofreach.
Remainder of manuscript. In Sec. 3.1 we parameterise topological masks M(G) as learnable
functions of weighted adjacency matrices, which incorporates strong structural inductive bias into
attentionandboostsperformance. Sec.3.2showshowthiscanbeimplementedimplicitly,without
instantiatingAorMinmemory. Sec.3.3offersafasterstochasticalternativeusingGRFs,which
we prove is O(N) time and space complexity. Sec. 4 gives experimental evaluations across data
modalities,includingimagesandpointclouds. TheAppendicespresentproofsandextradetailstoo
longforthemaintext,andextendeddiscussionaboutrelatedwork.
3 TOWARDS LINEAR TOPOLOGICAL MASKING
This section details our main results. Consider an undirected graph G(N,E) where
N :={v ,...,v } is the set of nodes and E is the set of edges, with (v ,v ) ∈ E if and only if
1 N i j
thereexistsanedgebetweenv andv inG. ThesizeofthegraphN correspondstothenumberof
i j
tokens. DenotebyM(G)∈RN×N thetopologicalmaskingmatrix.
3.1 GRAPHNODEKERNELSASTOPOLOGICALMASKS
As remarked above, a common choice for M(G) is a function of the shortest path distance be-
ij
tweennodesv andv (Yingetal.,2021). Thougheffectiveatimprovingperformance,thisrequires
i j
computationforeverypairofnodessoitcannotstraightforwardlybeappliedtolow-rankattention
inascalableway–exceptforspecialstructuredG wherethemaskbecomessimple(Choromanski
etal.,2022). Shortestpathdistancesarealsoverysensitivetochangesliketheadditionorremoval
ofedges,andneedaccesstothewholegraphupfront.
A new topological mask: graph node kernels. Given that we intend M(G) to quantify some
ij
sense of topological similarity between v and v , a more principled choice may be graph node
i j
kernels: positivedefinite,symmetricfunctionsk :N ×N →Rmappingfrompairsofgraphnodes
to real numbers. These are widely used in bioinformatics (Borgwardt et al., 2005), community
detection (Kloster and Gleich, 2014) and recommender systems (Yajima, 2006), with more recent
3Preprint. Underreview.
applications in manifold learning for deep generative modelling (Zhou et al., 2020) and solving
single-andmultiple-sourceshortestpathproblems(Craneetal.,2017). Theyarealsousedtomodel
thecovariancefunctioningeometricGaussianprocesses(Zhietal.,2023;Borovitskiyetal.,2021).
Graphnodekernelsareoftenparameterisedasfollows. LetW ∈ RN×N beaweightedadjacency
matrix,sothatW := [w ] withw nonzeroifandonlyif(v ,v ) ∈ E. Acommonchoiceis
ij i,j∈N ij i j
p
w =1/ d d ,withd thedegreeofv . Thenconsiderthepowerseries
ij i j i i
∞
X
M (G):= α Wk (4)
α k
k=0
where (α )∞ ⊂ R is a set of real-valued Taylor coefficients. This is positive definite if and
k k=0
only if P∞ α λk > 0∀λ ∈ Λ(W), where Λ(W) denotes the set of eigenvalues of W. With
k=0 k i i
different choices for (α )∞ , Eq. 4 includes many of the most popular graph node kernels in the
k k=0
literature,e.g.heat,diffusion,cosineandp-steprandomwalk. Kernellearningcanbeimplemented
byoptimisingα (Reidetal.,2024b;Zhietal.,2023).
k
Eq. 4 provides a very effective parameterisation for topological attention masks, striking the right
balance between flexibility and hard-coded structural inductive bias. In Sec. 4 we will show that
itboostsperformancecomparedtounmaskedattention. However, explicitlycomputingM (G) ∈
α
RN×N and storing it in memory is incompatible with O(N) low-rank attention. The next section
willshowhowthiscanbeavoidedbymaskingimplicitly.
3.2 IMPLICITMASKINGWITHGRAPHFEATURES
Linearattentionisfastbecauseitreplacessoftmaxattentionexp(q⊤k )witha‘featurised’alterna-
i j
tiveϕ(q )⊤ϕ(k ). CanwedothesameforM (G)? Thefollowingistrue.
i j α
Remark 3.1 (Explicit N-dimensional features for graph node kernels (Reid et al., 2024b)). Let
(f )∞ ⊂ R denote the deconvolution of (α )∞ , i.e. Pk f f =α ∀ k. There exists a
k k=0 k k=0 p=0 p k−p k
featurematrixΦ ∈RN×N satisfyingM (G)=Φ Φ⊤,givenbyΦ :=P∞ f Wk.
G α G G G k=0 k
This is can be shown in a few lines of algebra; see App. A.1. We refer to the rows of the feature
matrix Φ as graph features, letting Φ =: [ϕ (v )]N with ϕ (v ) ∈ RN the graph feature of
G G G i i=1 G i
nodev . Foreverykerneltheretriviallyexistsafeaturemap(Gretton,2013),butunusuallyinthis
i
caseitisfinite-dimensionalandadmitsasimpleclosedform.
Let ⊗:Rm×RN →Rm×N denote the outer product which maps a pair of vectors to
a matrix, and vec:Rm×N →RmN denote the vectorising operation that flattens a ma-
trix into a vector. Then ϕ(q )⊗ϕ (v ) is an m × N matrix whose jkth element is
i G i
ϕ(q ) ϕ (v ) ,andvec(ϕ(q )⊗ϕ (v )) is an mN-dimensional vector whose (N(j −1)+k)th
i j G i k i G i
entryisϕ(q ) ϕ (v ) . Observethefollowing:1
i j G i k
mN
X
vec(ϕ(q )⊗ϕ (v ))⊤vec(ϕ(k )⊗ϕ (v ))= vec(ϕ(q )⊗ϕ (v )) vec(ϕ(k )⊗ϕ (v ))
i G i j G j i G i s j G j s
m N s=1
=XX ϕ(q ) ϕ (v )ϕ(k ) ϕ (v ) =(cid:0) ϕ(q )⊤ϕ(k )(cid:1)(cid:0) ϕ (v )⊤ϕ (v )(cid:1) =K M .
i k G i l j k G j l i j G i G j LRij αij
k=1 l=1
(5)
Hence, taking {vec(ϕ({q ,k })⊗ϕ (v ))} ⊂ RmN as features, we can compute masked
i i G i vi∈N
attentionimplicitly,withoutmaterialisingAandM . Concretely,wehavethat:
α
Φ Φ⊤ =K ⊙M if Φ :=[vec(ϕ({q ,k })⊗ϕ (v ))]N ∈RN×Nm. (6)
Q,G K,G LR α {Q,K},G i i G i i=1
Maskedlow-rankattentioncanthenbecomputedusingΦ asthefeaturematrices:
{Q,K},G
Att (Q,K,V,G):=D−1(cid:0) Φ (cid:0) Φ⊤ V(cid:1)(cid:1) , D:=Φ (cid:0) Φ⊤ 1 (cid:1) . (7)
LR,M Q,G K,G Q,G K,G N
Time complexity. Eqs 6 and 7 give a recipe for computing masked attention implicitly in feature
space, withoutneedingtomaterialiseAorMinmemory. However, thisapproachreliesoncom-
puting the feature matrix Φ ∈RN×Nm and evaluating products like Φ⊤ 1 , which still
{Q,K},G K,G N
incursO(N2)spaceandtimecomplexity. Foralinearmaskingalgorithm, wemustreducethisto
O(N). Thiscanbeachievedbyapproximatingthefeatures.
1Recalltheoldadage:thedotproductofouterproductsistheproductofdotproducts.
4Preprint. Underreview.
3.3 FASTERIMPLICITMASKINGWITHGRAPHRANDOMFEATURES
OurgoalisnowtoreplaceΦ
{Q,K},G
byMonteCarloestimatesΦb{Q,K},G thatsatisfy
(cid:16) (cid:17)
E ΦbQ,GΦb⊤
K,G
=Φ Q,GΦ⊤
K,G
=K LR⊙M α, Φb⊤ K,Gv ∼O(N)∀v ∈RN, (8)
meaning we apply the desired topological mask in expectation but in linear time. This can be
achievedusinggraphrandomfeatures(GRFs)(Choromanski,2023;Reidetal.,2024b).
Atahigh-level,GRFscanbeunderstoodasfollows.FromRemark3.1,thegraphfeaturefornodev
i
isgivenbyϕ (v ):=[P∞ f Wk]N . SinceWisanadjacencymatrix,Wk countsthenumber
G i k=0 k ij j=1 ij
ofgraphwalksoflengthkbetweennodesiandj,weightedbytheproductoftraversededgeweights.
Soϕ (v ) canbeinterpretedasaweightedsumoverallwalksofanylengthbetweennodesv and
G i j i
v ,withlongercontributionssuppressedsinceedgeweightsareatmost1(bynormalisationofW).
j
GRFsapplyimportancesampling,approximatingthisinfinitesumbyaMonteCarloestimateϕbG(v i)
usingrandomwalks.
U ous tin og fnG oR deF vs. ,C wo hn ec rere Ωtel :y =, t (cid:8)o (vbu )i lld th |e vG ∈R NF ,ϕb
(G
v(v
,i
v) on )e ∈sa Em ,p ll ∈es Nn (cid:9)ra isn td ho em sew ta ol fk as ll{ wω ak(i l) k} sn
k .= D1
e⊂ noΩ
te
i i i=1 i i i+1
byp(ω)theprobabilityofwalkωwhichisknownfromthesamplingmechanism. p(ω)istypically
chosensothatwalksrandomlyhalt,prioritisingsamplinglowerpowersofWthatcontributemore
tothekernel. Letω :Ω→Rbeafunctioncomputingtheproductoftraversededgeweights,
e  
len(ω)
 Y 
ω(ω):= W iflen(ω)>1, 1otherwise , (9)
e ω[i]ω[i+1]
 
i=1
wherelen(ω)isthenumberofhopsandω[i]isthenodevisitedattheithtimestep. Then:
ϕbG(v i)
q
:= n1 Xn X ω e(ω i pq () ωf len )(ωiq)I(ω
iq
prefixsubwalkofω k(i)), q =1,...,N, (10)
iq
k=1ωiq∈Ωiq
whereΩ denotesthesetofallwalksbetweennodesv andv ,ofwhichω isamember. ‘Prefix
iq i q iq
subwalk’meansthatthewalkω(i) beginningatnodev initiallyfollowsthesequenceofnodesω ,
k i iq
thenoptionallycontinues. Thiscomplicatednotationbeliesaverysimplealgorithm: onesimulates
nrandomwalksoutofnodev . Ateveryvisitednode,oneaddsacontributiontothecorresponding
i
GRFcoordinatethatdependson1)theproductoftraversededgeweights,2)theprobabilityofthe
subwalkand3)thefunctionf. Theresultisnormalisedbydividingbythenumberofwalksn.
GRFs give unbiased approximation of graph node kernels: E[ϕbG(v i)⊤ϕbG(v j)]=M αij. This fol-
lowsbecauseE[I(ω prefixsubwalkofω(i))]=p(ω ). Unbiasednessisoneofourrequiredprop-
iq k iq
erties;wealsoneedΦb{Q,K},G tosupportO(N)matrix-vectormultiplication. Wewillshowthisby
providingthefirstknownproofthatGRFsaresparse: ϕb(v i)hasO(1)nonzeroentries ∀v
i
∈N.
3.4 NOVELTHEORETICALRESULTSFORGRFS: SHARPESTIMATORSWITHSPARSE
FEATURES
It is intuitive that GRFs are sparse. The walk sampler p(ω) is chosen so that walks are short,
e.g. taking simple random walks with geometrically distributed lengths. From Eq. 10, this means
that ϕbG(v i) is only nonzero at nodes ‘close’ to v i. Most of its entries are 0. However, to prove
thatΦb{Q,K}G supportsO(N)matrix-vectormultiplicationweneedtorigorouslyrelatethistothe
qualityofapproximationofM (G). ThisrequiresconcentrationinequalitiesforGRFs,butnosuch
α
boundswerepreviouslyknown. Thefollowingresultisnovel.
Theorem3.1(GRFexponentialconcentrationbounds). ConsideragraphG withadjacencymatrix
Wandnodedegrees{d i} vi∈N. SupposeweconstructGRFs{ϕbG(v i)}
vi∈G
bysamplingnrandom
walks{ω(i)} withgeometricallydistributedlengths,terminatingwithprobabilityp
k vi∈N,k∈[[1,n]] halt
ateachtimestep. Letc := P∞ |f |(max |Wij|di)k.Supposecisfinite,whichisguaran-
teed e.g. for bounded f and sk u= it0 ablyk normav li i, sv ej d∈N W1 ,− op rha ilt f there exists some i ∈ N such that f
max i
vanishesforalli>i (seeApp.A.2). Then:
max
(cid:16) (cid:17) (cid:18) t2n3 (cid:19)
Pr |ϕbG(v i)⊤ϕbG(v j)−M αij|>t ≤2exp −
2(2n−1)2c4
. (11)
5Preprint. Underreview.
Proof sketch. Full details are in App. A.2; here, we provide an overview. Treat each pair of ran-
domwalksX :=(ω(i),ω(j))∈Ω×Ω,simulatedoutofnodesv andv respectively,asarandom
k k k i j
variable. Let k ∈ [[1,n]] so we sample n such pairs of walks in total. Consider modifying one of
the pairs of walks. If c is finite, the maximum load it is capable of depositing is finite so we can
bound the L 1-norm of the contribution made to ϕbG(v i)andϕbG(v j). This allows us to bound the
change in the estimator ϕbG(v i)⊤ϕbG(v j) if X
k
is modified. Applying McDiarmid’s inequality, the
resultfollows.
Thm3.1providesanexponentialboundontheprobabilityofatopologicalmaskestimatedeviating
fromitstruevalue. Moreover,assumingthatcremainsconstant(i.e.wefixamaximumedgeweight
and node degree as the graph grows), this probability is independent of the number of nodes N.
Therefore,ifwefixtandtheprobabilityPr,wecansolvefortheminimumnumberofwalkersnto
guaranteeagoodmaskestimatewithhighprobability,independentofgraphsize. Nowobservethe
following.
Lemma3.2(GRFsparsity). ConsideragraphrandomfeatureϕbG(v i)constructedusingmrandom
walkers each terminating with probability p at every timestep. With probability at least 1−δ,
halt
ϕbG(v i)willhavenlog(1−(1−δ)1/n)log(1−p halt)−1orfewernonzeroentries.
Proof. Sincethewalklengthsaregeometricallydistributed,theprobabilitythatasinglewalkisof
length b or shorter is 1−(1−p )b. n independent walkers are all b or shorter with probability
(cid:0)
1−(1−p
halt)b(cid:1)n
.
Letthisbeeqh ual at
lto1−δandsolveforb. AtmostbnentriesofϕbG(v i)canthen
benonzero,sothesparsityguaranteefollows.
Lemma3.2meansthat,ifwefixthenumberofwalkersn(e.g.usingtheboundinEq.11),wecan
also bound GRF sparsity with high probability. Crucially: the number of nonzero GRF entries is
independentofthegraphsizeN. Weconcludethefollowing.
Corollary3.3(GRFsimplementO(N)topologicalmasking). Supposeweconstructasetofgraph
r δa ,n td ao nm dtf hea et du ere vs ia{ tiϕb
oG
n( pv
ri
o)}
bv ai b∈ iN
lit, yc Ph ro (cid:0)o |s ϕingt (h ve )n ⊤um ϕber (o vf )w −alk Mersn |a >sd te (cid:1)s fic xri eb ded asa hb yo pv ee rw pait rh amc, ep
teha rl
st,
.
GRF i GRF j αij
ThenthenumberofnonzeroentriesinΦb{Q,K},G ∈RN×NmislinearinN,whichmeansthattopo-
logicalmaskingisimplementedwithO(N)timecomplexity.
Proof. Lemma3.2showsthatthenumberofnonzeroentriesinϕbG(v i)isindependentofthenumber
of nodes N. This must also be true of vec(ϕ(k i)⊗ϕbG(v i)), so the number of nonzero entries in
ΦbK,G :=[vec(ϕ(k i)⊗ϕbG(v i))]N
i=1
isproportionaltoN. Thesameholdsfortheequivalentmatrix
for the queries, ΦbQ,G. The time complexity of the sparse matrix-matrix multiplication Φb⊤ K,GV is
O(Nmd)andΦb⊤ K,G1
N
isO(Nm). BoththesetimecomplexitiesarelinearinN. Thisisalsothe
caseforthesubsequentcomputationsinvolvingΦbQ,G.
Further comments on theoretical results. We have provided the first concentration bounds for
GRFsandquantificationofthetradeoffbetweensparsityandkernelapproximationquality. These
results may be of interest independent from topological masking, e.g. for applications in scalable
geometricGaussianprocesses(Reidetal.,2024c;Borovitskiyetal.,2021).Thm.3.1isremarkably
general, requiringonlythemodestassumptionaboutG andf thatcisfinite. ToguaranteeO(N)
scaling we assume that c is fixed as G changes, which is guaranteed by e.g. fixed maximum node
degreeandedgeweight.
3.5 ALGORITHMANDINTUITIVEPICTURE
Alg. 1 presents our method. The results of Secs 3.1-3.4 can be intuitively understood as follows
(seeFig.2foranoverview). Graphnodekernels,whichcomputeaninfiniteweightedsumofwalks
betweenpairsofinputnodes,provideaneffectiveRPEmechanismfortransformers. Theycapture
thetopologicalrelationshipbetweenqueriesandkeys. Usinggraphfeatures,wecanimplementthis
implicitly,withoutneedingtoactuallymaterialisethemaskmatrix. Replacingthesedensefeatures
by graph random features, a sparse approximation, we can reduce the cost of implicit masking to
O(N) with low performance loss. Eq. 11 gives a remarkably tight bound on the quality of mask
estimation. Physically,maskingwithGRFsmeansthatnodesv andv onlyattendtooneanother
i j
ifmembersoftheirrespectiveensemblesofrandomwalkshit,suchthattheyvisitsomeofthesame
nodes. ThisincorporatesastrongstructuralinductivebiasfromG. Itupweightscrucialinteractions
betweennearbynodes,whilstalsosamplinglonger-rangeattentionwithlowerprobability.
6Preprint. Underreview.
vj
ϕbG(v i)= Aij=E(cid:0) ϕ(qi)⊤ϕ(kj)(cid:1)
ϕbG(v j)=
(cid:16) (cid:17)
Mαij=E ϕbG(vi)⊤ϕbG(vj))
(cid:18) h i⊤
AijMαij=E vec ϕ(qi)⊗ϕbG(vi)
vi h i(cid:17)
vec ϕ(kj)⊗ϕbG(vj)
G W Mα(G)=P∞ i=0αiWi
Figure 2: Visual overview. A graph G (left) has a weighted adjacency matrix W (centre left).
A learnable power series M (G):=P∞ α Wi is an effective topological mask or graph RPE
α i=0 i
(centre right). M (G) can be efficiently approximated using graph random features (centre top),
α
whichperformimportancesamplingofhaltingrandomwalks.ThefeatureϕbG(v i)isonlynonzeroat
entriesvisitedbytheensembleofwalksbeginningatv . InThm.3.1,weprovethatthenumberof
i
suchentriesisO(1)whilststillaccuratelyestimatingM (W) withhighprobability,soGRFsare
α ij
sparse. ThisunlocksO(N)topologicalmasking. NotethatϕbG(v i)⊤ϕbG(v j)isonlynonzeroifthe
featuresarenonzeroatsomeofthesamecoordinates,whichhappensiftheirrespectiveensemblesof
walks‘hit’.Thisincorporatesastrongstructuralinductivebias.Theequationsontherightformalise
ourmethodmathematically.
Distributedcomputation,expressivityandGATs.Alg.1issimpletodistributeformassivegraphs
that cannot be stored on a single machine; to simulate random walks one only needs a node’s im-
mediateneighbours ratherthan all of G upfront. This desirablepropertyis notsharedby masking
algorithmsthatrelyonthefastFouriertransform(Luoetal.,2021;Choromanskietal.,2022).More-
over,ourmethodcandistinguishgraphsidenticalunderthe1-dimensionalWeisfeilerLehmangraph
isomorphism heuristic (with colours replaced by node features) because their adjacency matrices
andhencemasksdiffer. Inthissense, GRFtransformersaremoreexpressivethanstandardgraph
neural networks and unmasked transformers, which notoriously fail this test (Morris et al., 2019;
Xuetal.,2019). Finally,weremarkthatouralgorithmcanbeinterpretedasastochasticextension
ofgraphattentionnetworks(GATs;Velicˇkovic´ etal.,2018),withagreaterfocusonlinearattention
and scalability. Tokens still attend to a subset of other tokens informed by the topology of G, but
importance sampling random walks permits interactions beyond nearest-neighbour. Incorporating
learnableparameters(f )imax letsusestimateaprincipledfunctionofW.
i i=0
Algorithm1O(N)topologically-maskedattentionforgeneralgraphs
Input: querymatrixQ ∈ RN×d,keymatrixK ∈ RN×d,valuematrixV ∈ RN×d,graphG with
weighted adjacency matrix W ∈ RN×N, learnable mask parameters (f )imax, number of random
i i=0
walkstosamplen∈N,query/keyfeaturemapϕ(·):Rd →Rm.
Output: maskedattentionAtt (Q,K,V,G)inO(N)time.
1:
SimulatenterminatingranL dR o,Mb
mwalks{ω k(i)}n k=1outofeverynodev i ∈N
2: Compute sparse graph random features {ϕbG(v i)} vi∈N ⊂ RN using the walks {ω k(i)}n k=1 and
learnablemaskparameters(f )imax
i i=0
3: Computethequeryandkeyfeatures{ϕ(q i)}N i=1,{ϕ(k i)}N i=1,withϕthechosenlinear-attention
nonlinearity(e.g.elu(x)+x(Katharopoulosetal.,2020)oraMonteCarloestimateofsoftmax
(Choromanskietal.,2020))
4: Combine the query/key features and graph features into a sparse topology-enhanced feature
matrix,Φb{Q,K},G :=[vec(ϕ({q i,k i})⊗ϕbG(v i))]N
i=1
∈RN×Nm
5: Computelow-rankattentionwithtopologicalmaskinginO(N)timeby
(cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)
Att LR,Mb(Q,K,V,G):=D−1 ΦbQ,G Φb⊤ K,GV , D:=ΦbQ,G Φb⊤ K,G1
N
. (12)
7Preprint. Underreview.
4 EXPERIMENTS
Inthissection,wetestouralgorithmsfortopologicalmaskingwithGRFs. Weconsiderdatamodal-
itieswithdifferentgraphtopologies: imagesandpointclouds.
Timecomplexity. Forahardware-agnosticcompar-
ison,wefirstcomputethetotalnumberofFLOPsfor
Time complexity scaling
evaluating(i)unmaskedsoftmax,(ii)unmaskedlin- 103
earand(iii)GRF-maskedlinearattentionforgraphs Softmax
of different sizes N. Fig. 3 shows the results.
101 L Li in ne ea ar
r + GRFs
Forconcretenessweuse1-dimensionalgridgraphs,
101
though the theoretical results in Sec. 3.4 are inde-
pendent of G (provided c is upper bounded). We 103
taken = 4randomwalkerspernodewithtermina-
tionprobabilityp = 0.5. Thelatentdimensionis
105
halt
d = 8andthefeaturesizeism = 8. Inthemasked
20 23 26 29 212
Number of graph nodes, N
case we show the mean number of FLOPs over 10
Figure 3: Number of FLOPs vs. number of
seedssinceGRFsparsityisitselfarandomvariable.
graphnodesforsoftmaxattention, linearat-
As anticipated, the time complexity of our mecha-
nismisO(N);thereisaconstantmultiplicativecost tention, andlinearattentionwithGRFtopo-
logicalmasking(ours).
for topological masking. We clearly improve upon
thetimecomplexityofO(N2)softmaxattention.
ViTs:gridgraphsandimagedata.Next,weuseourtopologicalmaskingalgorithmtoimprovethe
visiontransformer(ViT; Dosovitskiyetal.,2020). TheunderlyinggraphG isasquaregridofsize
num_x_patches×num_y_patches,withnodesconnectedbyanedgeiftheyareneighbours
inxory. Sincethegraphisfixed,randomwalkscanbepre-computedandfrozenforeachattention
headandlayerbeforetrainingandinference. FullarchitecturalandtrainingdetailsareinApp.C.
AllourViTmodelsalsoendowtokenswithlearnedadditiveabsolutepositionembeddings.
Table 1 shows the final test accuracies for ImageNet (Deng et al., 2009), iNaturalist2021 (Horn
et al., 2018) and Places365 (Zhou et al., 2018). For the slow (superlinear) variants, we include (i)
unmasked softmax and (ii) Toeplitz-masked linear (Choromanski et al., 2022) attention. Toeplitz
canonlybeusedforthisspecific,structuredgraphtopology. Wealsoinclude(iii)M (G)-masked
α
linear,whichtakesanexplicitpowerseriesinW(Eq.4)todirectlymodulatetheN ×N attention
matrix. This is equal to our Monte Carlo GRFs algorithm in the n → ∞ limit, so it indicates the
asymptotic performance of GRFs with very many random walkers. For the fast (linear) variants,
weinclude(iv)unmaskedlinearand(v)GRF-maskedlinearattention. Ouralgorithmgivesstrong
improvementsabovetheunmaskedbaseline,by+3.7%,+2.2%and+0.5%respectively. Remark-
ably,itiscompetitivewithmuchmoreexpensivevariants: GRFsoftenmatchorbeatToeplitz. For
ImageNetandiNaturalist2021,itevensubstantiallynarrowsthegapwithO(N2)softmax.
Variant Timecomp. ImageNet iNaturalist2021 Places365(Small)
1M 2.7M 1.8M
Unmaskedsoftmax O(N2) 0.741 0.699 0.567
Toeplitz-maskedlinear O(NlogN) 0.733 0.674 0.550
M (G)-maskedlinear O(N2) 0.741 0.692 0.551
α
Unmaskedlinear O(N) 0.693 0.667 0.543
GRF-maskedlinear O(N) 0.730 0.689 0.548
Table1: Finaltransformertestaccuraciesacrossattentionmaskingvariantsanddatasets. Ouralgo-
rithmsareshowninbold. TopologicalmaskingwithGRFsisthebestO(N)mechanism,matching
orbeatingmoreexpensivealternatives.
Ablation studies. In App. C.2, we perform ablation studies for the ViT experiments. We find
that predictive performance improves with the number of walkers n because the accuracy of the
Monte Carlo estimate Mcα(G) increases. Moreover, importance sampling walks is crucial; if we
failtoupweightimprobablewalks,thegainsvanish. Lastly,theinductivebiasfromparameterising
M (G)asafunctionofWiskey. FullylearnablegraphfeaturesofthesamedimensionalityN,an
α
impracticalandexpensivealternativetoourmethod,givemuchsmallergains.
8
WOLS
TSAF
)601/(
sPOLFPreprint. Underreview.
Groundtruth
videoframe
GRFInterlacer
prediction
Groundtruth
videoframe
GRFInterlacer
prediction
Timestep t t+∆t t+2∆t t t+∆t t+2∆t
Figure4:Renderedrollouts.NeRFrenderingsofthepredictivedynamicsofabimanualKukarobot,
conditionedontheinitialsceneandsequenceofrobotactions. Weshowfourtasks: usingadustpan
and brush, lifting a can, moving a green block, and dropping a can. GRF Interlacers model point
clouddynamicsmoreaccurately,sothepredictedframerendingsareclosertothegroundtruth.
PCTs: predictingparticledynamicsforrobotics. Asafinaltask,weuseouralgorithmtomodel
high-densityvisualparticledynamics(HD-VPD; Whitneyetal.,2023;2024),predictingthephysi-
calevolutionofscenesinvolvingabi-manualrobotusingapointcloudtransformer(PCT).Images
fromapairofcamerasareunprojectedtoasetof3Dparticles. Theirinteractions, conditionedon
specificrobotactions,aremodelledbyadynamicsnetwork. Wepredictupdatestothelatentpoint
cloud,rendertheresulttoanimagewithaconditionalNeRF(Mildenhalletal.,2021),andtrainend-
to-endwithvideopredictionloss. Suchlearneddynamicsmodelsareattractinggrowinginterestin
roboticsasamoreflexiblealternativetophysicalsimulators,buttoscaletothemassivepointclouds
neededfordetailedpredictionsefficiencyandeffectivestructuralinductivebiasesarekey.
For the dynamics network, we use an Interlacer: a PCT which alternates unmasked linear atten-
tionandmessagepassing-styleupdatesbetweenneighbouringparticles(Whitneyetal.,2024). This
architecture is twice as fast as GNNs with the same prediction quality, and can handle 4× bigger
pointclouds. TotesttheabilityofGRFstoefficientlycapturelocaltopologicalstructure,wereplace
messagepassingbyGRF-maskedlinearattention,takingquery/keyfeaturesϕ(·) = ReLU(·). Im-
plementationdetailsforHD-VPD,includingtheroboticarmandcameraconfigurations,arethesame
asreportedbyWhitneyetal.(2024);seeApp.C.3fordiscussion. Todisambiguate,wewillreferto
ourmodelastheGRFInterlacerandWhitneyetal.’sasthemessagepassing(MP)Interlacer.
In contrast to our previous experiments, for HD-VPD the
Accuracy vs. rollout timestep
underlying graph G is no longer fixed so walks cannot be 0.82
GRF Interlacer
pre-computed. Instead, we construct them on the fly, using
0.80 MP Interlacer
anapproximatek-nearestneighbourssolvertogetadjacency Unmasked PCT
listsoflengthk =3foreachnode.Weuserepellingrandom 0.78
walks for variance reduction (Reid et al., 2024a), sampling
0.76
3 hops. For easy integration with existing Interlacer code,
0.74
weuseasymmetricGRFs(App.D;Reidetal.,2024b). We
consider point clouds with N = 32768 particles, which is 0 1 2 3 4 5 6 7
Rollout timestep
toomanytoexplicitlyinstantiateAorMinmemory.
Fig.4showsexampleNeRFrenderingsoftestsetpredictive Figure 5: Accuracy comparison.
dynamics for four rollouts. Since the model is determinis- Structural similarity index measure
tic, predictionsblurwithlongtimehorizons. Incorporating (SSIM)betweengroundtruthcamera
a diffusion head (Song et al., 2020) is left to future work. frames and predictive NeRF render-
Fig.5showspredictedvideoframeaccuracy(imageSSIM) ings after 100k training steps, plot-
vs. rollout timestep for each model. GRFs outperform the ted against rollout timestep. Higher
vanilla transformer and MP Interlacer baselines, modelling isbetter.GRFsimprovetheaccuracy
theparticledynamicsmorefaithfully. Weincludeexamples of dynamics prediction compared to
ofpredictivevideosinthesupplementarymaterial. thebaselines.
9
MISS
egamIPreprint. Underreview.
5 CONCLUSION
Wehavepresentedthefirstlinearalgorithmfortransformertopologicalmaskingongeneralgraphs,
based on sampling random walks. It uses graph random features, for which we have proved the
firstknownconcentrationinequalitiesandsparsityguarantees. Ouralgorithmbringsstrongperfor-
mance gains across data modalities, including on massive point clouds with > 30k particles for
which computational efficiency is essential. Future work might include improving hardware and
libraries for high-dimensional sparse linear algebra to fully benefit from the guaranteed complex-
ityimprovements,andinvestigatingwhetherdimensionalityreductiontechniquesliketheJohnson-
Lindenstrausstransform(Dasguptaetal.,2010)canbeappliedtoGRFsforfurtherspeedups.
10Preprint. Underreview.
6 ETHICS AND REPRODUCIBILITY
Ethics statement: Our work is foundational with no immediate ethical concerns apparent to us.
However,increasesinscalabilityprovidedbyimprovementstoefficienttransformerscouldexacer-
bateexistingandincipientrisksofmachinelearning,frombadactorsorasunintendedconsequences.
Reproducibilitystatement: Wehave madeeveryeffort toensurethe work’sreproducibility. The
core algorithm is presented clearly in Alg. 1. Theoretical results are proved with accompanying
assumptionsinSec.3.4andApp. A.2. Proofsketchesarealsoincludedforclarity. Wewillmake
sourcecodeavailableonceaccepted. Apartfromtheroboticsdataset,alldatasetsarestandardand
freely available online. Exhaustive experimental details about the training and architectures are
reportedinApp.C;seeespeciallyTable2.
7 CONTRIBUTIONS AND ACKNOWLEDGEMENTS
Relative contributions. IR and KC conceptualised the project and co-designed the algorithms.
IR proved the theoretical results, implemented the GRFs code, ran the Interlacer experiments and
wrotethetext.KADwascruciallyinvolvedthroughout,runningtheViTandViViTexperimentsand
guidingtheproject’sdirection. DJandWWhelpedsetuptheInterlacerexperiments. AA,JA,AB,
MJ, AM, DR, CS, RT, RW and AW provided thoughtful feedback on the project and manuscript,
helpedsetupexperimentalinfrastructure,andmadeCambridgeandGooglegreatplacestowork.
Acknowledgementsandfunding. IRacknowledgessupportfromaTrinityCollegeExternalStu-
dentship. ThisworkwascompletedwhilstemployedasastudentresearcheratGoogle. IRsincerely
thanks Silvio Lattanzi for making this possible. RET is supported by Google, Amazon, ARM,
ImprobableandanEPSRCgrantEP/T005386/1. AWacknowledgessupportfromaTuringAIfel-
lowshipundergrantEP/V025279/1andtheLeverhulmeTrustviaCFI.
11Preprint. Underreview.
REFERENCES
AnuragArnab, MostafaDehghani, GeorgHeigold, ChenSun, MarioLucˇic´, andCordeliaSchmid.
Vivit: Avideovisiontransformer. InProceedingsoftheIEEE/CVFinternationalconferenceon
computer vision, pages 6836–6846, 2021. URL https://doi.org/10.48550/arXiv.
2103.15691.
KarstenMBorgwardt,ChengSoonOng,StefanSchönauer,SVNVishwanathan,AlexJSmola,and
Hans-PeterKriegel. Proteinfunctionpredictionviagraphkernels. Bioinformatics,21(suppl_1):
i47–i56,2005. URLhttps://doi.org/10.1093/bioinformatics/bti1007.
Viacheslav Borovitskiy, Iskander Azangulov, Alexander Terenin, Peter Mostowsky, Marc Deisen-
roth, and Nicolas Durrande. Matérn gaussian processes on graphs. In International Confer-
ence on Artificial Intelligence and Statistics, pages 2593–2601. PMLR, 2021. URL https:
//doi.org/10.48550/arXiv.2010.15538.
HuiyuanChen,ZheXu,Chin-ChiaMichaelYeh,VivianLai,YanZheng,MinghuaXu,andHang-
hangTong.Maskedgraphtransformerforlarge-scalerecommendation.InProceedingsofthe47th
International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages2502–2506,2024. URLhttps://doi.org/10.1145/3626772.3657971.
KrzysztofChoromanski,ValeriiLikhosherstov,DavidDohan,XingyouSong,AndreeaGane,Tamas
Sarlos,PeterHawkins,JaredDavis,AfrozMohiuddin,LukaszKaiser,etal. Rethinkingattention
with performers. arXiv preprint arXiv:2009.14794, 2020. URL https://doi.org/10.
48550/arXiv.2009.14794.
Krzysztof Choromanski, Haoxian Chen, Han Lin, Yuanzhe Ma, Arijit Sehanobish, Deepali Jain,
MichaelSRyoo,JakeVarley,AndyZeng,ValeriiLikhosherstov,etal. Hybridrandomfeatures.
arXiv preprint arXiv:2110.04367, 2021. URL https://doi.org/10.48550/arXiv.
2110.04367.
KrzysztofChoromanski,HanLin,HaoxianChen,TianyiZhang,ArijitSehanobish,ValeriiLikhosh-
erstov,JackParker-Holder,TamasSarlos,AdrianWeller,andThomasWeingarten. Fromblock-
toeplitzmatricestodifferentialequationsongraphs:towardsageneraltheoryforscalablemasked
transformers.InInternationalConferenceonMachineLearning,pages3962–3983.PMLR,2022.
URLhttps://doi.org/10.48550/arXiv.2107.07999.
Krzysztof Marcin Choromanski. Taming graph kernels with random features. In International
ConferenceonMachineLearning,pages5964–5977.PMLR,2023.URLhttps://doi.org/
10.48550/arXiv.2305.00156.
KeenanCrane,ClarisseWeischedel,andMaxWardetzky.Theheatmethodfordistancecomputation.
CommunicationsoftheACM,60(11):90–99,2017. URLhttps://dl.acm.org/doi/10.
1145/3131280.
AnirbanDasgupta,RaviKumar,andTamásSarlós. Asparsejohnson: Lindenstrausstransform. In
Proceedingsoftheforty-secondACMsymposiumonTheoryofcomputing,pages341–350,2010.
URLhttps://doi.org/10.1145/1806689.1806737.
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scalehi-
erarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,
pages248–255.Ieee,2009. URLhttps://doi.org/10.1109/CVPR.2009.5206848.
Joseph L Doob. Regularity properties of certain families of chance variables. Transactions of the
AmericanMathematicalSociety,47(3):455–486,1940.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929,2020. URLhttps://doi.org/10.48550/arXiv.2010.11929.
ArthurGretton. Introductiontorkhs, andsomesimplekernelalgorithms. Adv.Top.Mach.Learn.
Lecture Conducted from University College London, 16(5-3):2, 2013. URL https://www.
gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf.
12Preprint. Underreview.
Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alexander Shepard,
Hartwig Adam, Pietro Perona, and Serge J. Belongie. The inaturalist species classifi-
cation and detection dataset. In 2018 IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 8769–
8778. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.
2018.00914. URLhttp://openaccess.thecvf.com/content_cvpr_2018/html/
Van_Horn_The_INaturalist_Species_CVPR_2018_paper.html.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International conference on
machine learning, pages 5156–5165. PMLR, 2020. URL https://doi.org/10.48550/
arXiv.2006.16236.
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-
narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human ac-
tion video dataset. arXiv preprint arXiv:1705.06950, 2017. URL https://doi.org/10.
48550/arXiv.1705.06950.
KyleKlosterandDavidFGleich. Heatkernelbasedcommunitydetection. InProceedingsofthe
20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages
1386–1395,2014. URLhttps://doi.org/10.48550/arXiv.1403.3148.
Risi Imre Kondor and John Lafferty. Diffusion kernels on graphs and other discrete struc-
tures. In Proceedings of the 19th international conference on machine learning, volume 2002,
pages 315–322, 2002. URL https://www.ml.cmu.edu/research/dap-papers/
kondor-diffusion-kernels.pdf.
Valerii Likhosherstov, Krzysztof M Choromanski, Kumar Avinava Dubey, Frederick Liu, Tamas
Sarlos,andAdrianWeller. Dense-exponentialrandomfeatures: sharppositiveestimatorsofthe
gaussiankernel. AdvancesinNeuralInformationProcessingSystems,36,2024. URLhttps:
//dl.acm.org/doi/10.5555/3666122.3666168.
AntoineLiutkus, OndˇrejCıfka, Shih-LunWu, UmutSimsekli, Yi-HsuanYang, andGaelRichard.
Relative positional encoding for transformers with linear complexity. In International Confer-
enceonMachineLearning, pages7067–7079.PMLR,2021. URLhttps://doi.org/10.
48550/arXiv.2105.08399.
ILoshchilov.Decoupledweightdecayregularization.arXivpreprintarXiv:1711.05101,2017.URL
https://doi.org/10.48550/arXiv.1711.05101.
ShengjieLuo,ShandaLi,TianleCai,DiHe,DinglanPeng,ShuxinZheng,GuolinKe,LiweiWang,
andTie-YanLiu.Stable,fastandaccurate:Kernelizedattentionwithrelativepositionalencoding.
Advances in Neural Information Processing Systems, 34:22795–22807, 2021. URL https:
//doi.org/10.48550/arXiv.2106.12566.
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,and
RenNg. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. Communications
of the ACM, 65(1):99–106, 2021. URL https://doi.org/10.48550/arXiv.2003.
08934.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan,andMartinGrohe. Weisfeilerandlemangoneural: Higher-ordergraphneuralnetworks.
In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 4602–4609,
2019. URLhttps://doi.org/10.48550/arXiv.1810.02244.
IsaacReid,KrzysztofChoromanski,andAdrianWeller. Quasi-montecarlographrandomfeatures.
arXiv preprint arXiv:2305.12470, 2023a. URL https://doi.org/10.48550/arXiv.
2305.12470.
Isaac Reid, Krzysztof Marcin Choromanski, Valerii Likhosherstov, and Adrian Weller. Simplex
randomfeatures.InInternationalConferenceonMachineLearning,pages28864–28888.PMLR,
2023b. URLhttps://doi.org/10.48550/arXiv.2301.13856.
13Preprint. Underreview.
Isaac Reid, Eli Berger, Krzysztof Choromanski, and Adrian Weller. Repelling random walks. In
InternationalConferenceonLearningRepresentations,2024a.URLhttps://doi.org/10.
48550/arXiv.2310.04854.
IsaacReid,KrzysztofChoromanski,EliBerger,andAdrianWeller. Generalgraphrandomfeatures.
InInternationalConferenceonLearningRepresentations,2024b. URLhttps://doi.org/
10.48550/arXiv.2310.04859.
IsaacReid,StratisMarkou,KrzysztofChoromanski,RichardETurner,andAdrianWeller.Variance-
reducing couplings for random features: Perspectives from optimal transport. arXiv preprint
arXiv:2405.16541,2024c. URLhttps://doi.org/10.48550/arXiv.2405.16541.
OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomed-
ical image segmentation. In Medical image computing and computer-assisted intervention–
MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceed-
ings, part III 18, pages 234–241. Springer, 2015. URL https://doi.org/10.48550/
arXiv.1505.04597.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position repre-
sentations. arXivpreprintarXiv:1803.02155, 2018. URLhttps://doi.org/10.48550/
arXiv.1803.02155.
Alexander J Smola and Risi Kondor. Kernels and regularization on graphs. In Learning The-
ory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Work-
shop,COLT/Kernel2003,Washington,DC,USA,August24-27,2003.Proceedings,pages144–
158. Springer, 2003. URL https://people.cs.uchicago.edu/~risi/papers/
SmolaKondor.pdf.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020. URLhttps://doi.org/10.48550/arXiv.2011.13456.
AVaswani. Attentionisallyouneed. AdvancesinNeuralInformationProcessingSystems,2017.
URLhttps://doi.org/10.48550/arXiv.1706.03762.
Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph Attention Networks. International Conference on Learning Representations,
2018. URLhttps://doi.org/10.48550/arXiv.1710.10903.
WilliamFWhitney,TatianaLopez-Guevara,TobiasPfaff,YuliaRubanova,ThomasKipf,Kimberly
Stachenfeld,andKelseyRAllen. Learning3dparticle-basedsimulatorsfromrgb-dvideos. arXiv
preprint arXiv:2312.05359, 2023. URL https://doi.org/10.48550/arXiv.2312.
05359.
WilliamFWhitney,JacobVarley,DeepaliJain,KrzysztofChoromanski,SumeetSingh,andVikas
Sindhwani. Modelingtherealworldwithhigh-densityvisualparticledynamics. arXivpreprint
arXiv:2406.19800,2024. URLhttps://doi.org/10.48550/arXiv.2406.19800.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXivpreprintarXiv:1810.00826, 2019. URLhttps://doi.org/10.48550/
arXiv.1810.00826.
Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neu-
mann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages 5438–5448, 2022. URL https:
//doi.org/10.48550/arXiv.2201.08845.
Yasutoshi Yajima. One-class support vector machines for recommendation tasks. In Pacific-Asia
Conference on Knowledge Discovery and Data Mining, pages 230–239. Springer, 2006. URL
https://doi.org/10.1007/11731139_28.
14Preprint. Underreview.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural
information processing systems, 32, 2019. URL https://doi.org/10.48550/arXiv.
1906.08237.
Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,
and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in
Neural Information Processing Systems, 34:28877–28888, 2021. URL https://doi.org/
10.48550/arXiv.2106.05234.
Yin-CongZhi,YinChengNg,andXiaowenDong.Gaussianprocessesongraphsviaspectralkernel
learning. IEEE Transactions on Signal and Information Processing over Networks, 9:304–314,
2023. URLhttps://doi.org/10.48550/arXiv.2006.07361.
Bolei Zhou, Àgata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE Trans. Pattern Anal. Mach. Intell., 40
(6):1452–1464, 2018. doi: 10.1109/TPAMI.2017.2723009. URL https://doi.org/10.
1109/TPAMI.2017.2723009.
YufanZhou,ChangyouChen,andJinhuiXu. Learningmanifoldimplicitlyviaexplicitheat-kernel
learning.AdvancesinNeuralInformationProcessingSystems,33:477–487,2020.URLhttps:
//doi.org/10.48550/arXiv.2010.01761.
15Preprint. Underreview.
A PROOFS
A.1 PROOFOFREMARK3.1
This result was presented by Reid et al. (2024b); a short demonstration is included below for the
reader’sconvenience. Observethat
∞ N ∞ N ∞ k
M (G) :=X α Wk ( =1)XX α Wk−pWp ( =2)XXX f f Wk−pWp
α ij k ij k iv vj k−p p iv vj
k=0 v=1k=0 v=1k=0p=0
(13)
N ∞ ! ∞ !⊤
( =3)X X f Wk1 X f Wk2 ( =4) Φ Φ⊤
k1 iv k2 jv G G
v=1 k1=0 k2=0
where Φ := P∞ f Wk. Here, step (1) splits the Wk into a product of Wk−p and Wp with
G k=0 k
p∈N;step(2)replacesα bytheconvolutionα =:Pk f f ;step(3)changesthesummation
k k p=0 p k−p
indices;step(4)arrivesatthedesiredform.
A.2 PROOFOFTHM.3.1(NOVEL)
The following concentration bounds are new. Referring back to Eq. 10, recall that graph random
featuresarecomputedby
n
ϕbG(v i)
q
:= n1 X ψ(ω k(i)) q, q =1,...,N, (14)
k=1
wherewedefinetheprojectionvectorψ(·):Ω→RN suchthat
ψ(ω(i)) := X ω e(ω iq)f len(ωiq)I(ω prefixsubwalkofω(i)). (15)
k q p(ω ) iq k
iq
ωiq∈Ωiq
Ω is the set of walks between nodes v and v ; p(ω ) is the probability of a particular walk
iq i q iq
ω ,knownfromthesamplingmechanism;ω(ω )isafunctionthatreturnstheproductofweights
iq e iq
of edges traversed by ω (Eq. 9); len(ω ) is the length of ω ; and f : N → R is a sequence
iq iq iq
of a real numbers that determines the graph node kernel to be estimated. Following convention,
wewill assumethatwe sample randomwalkswith geometricallydistributedlengths, haltingwith
probability p at each timestep, though in principle different importance sampling schemes are
halt
possible.
Supposewearetoestimateasingleentryofthegraphnodekernel,M (G) . Wedosobytaking
α ij
n n
Mcαij :=ϕbG(v i)⊤ϕbG(v j)= n1
2
X X ψ(ω k(i 1))⊤ψ(ω k(j 2)) (16)
k1=1k2=1
whereE(Mcαij) = M
αij
sincetheestimateisunbiased. Wesamplenindependentrandomwalks
startingfromeverynodeofthegraph.
Letusnowconsiderasinglepairofrandomwalksfromv andv respectively. Denotethisbythe
i j
randomvariable
X :=(ω(i),ω(j))∈Ω×Ω, (17)
k k k
where n such random variables {X }n are used in total. Suppose we modify X . Under mild
k k=1 k
assumptions,wecanprovethatthechangeintheestimator|∆Mcαij|isbounded,whichwillenable
ustousederiveconcentrationinequalities.
Thiscanbeunderstoodasfollows. Tobegin,wewillboundtheL -normofψ(ω(i)). Observethat
1 k
∥ψ(ω k(i))∥ 1 ≤ ωm k(i)a ∈x
Ω
ωpX .s.ω(i)(cid:12) (cid:12) (cid:12) (cid:12)ω e(ω p) (f ωle )n(ω)(cid:12) (cid:12) (cid:12) (cid:12)≤ kX∞ =0|f k|(cid:18) vim ,vja ∈x
N
1|W −i pj| hd ali t(cid:19)k =:c, (18)
k
16Preprint. Underreview.
whichcorrespondstothecaseofaninfinitelengthwalkthathappensalmostnever. Here,ωp.s. ω(i)
k
meansthatω isa‘prefixsubwalk’ofω(i),sothefirstlen(ω)nodesofω(i) areexactlythenodesof
k k
ω.
Supposethatcisfinite. Thiscanbeguaranteedforboundedf bynormalisationofW,multiplying
byascalarsothat|max Wijdi| < 1. Morepragmatically,itcanbeachievedbystipulating
vi,vj∈N 1−p
thatthereexistssomeintegeri ∈Nsuchthatf =0∀i>i ,soweonlyconsiderwalksupto
max i max
acertainlength. Thisisanaturalchoiceinexperimentssincewewanttokeepthesetoflearnable
parameters finite. It also coincides with the intuition that very long walks should not reflect the
relationshipbetweenthenodesofagraphoffinitesize.
Wehaveseenthat0<∥ψ(ω(i))∥ ≤c.Itfollowsthat−c2 ≤ψ(ω(i))⊤ψ(ω(j))≤c2∀(k ,k ,i,j),
k 1 k1 k2 1 2
whereuponthechangeinMcαij ifwemodifyX
k
obeys
2n−1
|∆Mcαij|≤2
n2
c2. (19)
WithstraightforwardapplicationofMcDiarmid’sinequality(Doob,1940),itfollowsthat
(cid:16) (cid:17) (cid:18) t2n3 (cid:19)
Pr |ϕbG(v i)⊤ϕbG(v j)−M αij|>t ≤2exp −
2(2n−1)2c4
, (20)
completingtheproof.
As noted in the main text, this is a remarkably general and powerful result. It is the first known
concentrationinequalityforGRFs.
B EXTENDED DISCUSSION ON RELATED WORK
Inthisappendix,wegiveadetailedexplanationoftherelationshiptopreviouswork.
1. Stochasticpositionencoding(SPE)(Liutkusetal.,2021).Toourknowledge,thisistheearliest
RPE strategy compatible with O(N) transformers, incorporating information about the ‘lag’
betweentokensinthelinearattentionsettingbydrawingsamplesfromrandomprocesseswitha
prescribedcovariancestructure.ThetimecomplexityisO(N)butthemethodisonlyapplicable
tosequencedata,i.e.theveryspecialcasethatG isa1-dimensionalgrid.
2. Toeplitz masks and the fast Fourier transform (FFT) (Luo et al., 2021). Again consid-
ering sequence data, suppose that RPE between a query q and a key k is impemented by
i j
addingabiastermb tothedotproductbeforeexponentiating. Thenthecorrespondingmask
j−i
M
ij
=ebj−i isToeplitz(constantonalldiagonals). Fromnumericallinearalgebra,multiplying
a Toeplitz matrix and a vector can be achieved with O(NlogN) time complexity rather than
O(N2), using the FFT. This is subquadratic, though not linear, and again only specific G are
considered.
3. Graphormer (Ying et al., 2021). In this paper, positional encodings based on the shortest
path distances between the nodes of the graph (spatial encoding) and their respective degrees
(centrality encoding) are seen to substantially improve transformer performance, closing the
gapwithgraphneuralnetworks. Thereisnoattempttoincorporatethesetechniqueswithlinear
attention, but the paper provides strong evidence of the effectiveness of graph-based attention
modulation(topologicalmasking).
4. Block Toeplitz matrices and differential equations on graphs (Choromanski et al., 2022).
Thispaperprovesthatsubquadraticmatrix-vectormultiplicationissufficientformaskingtobe
appliedefficiently,andsuggestsanumberofalgorithmstoachievethisinspecialcases.
• BlockToeplitzmatrices. ThismethodextendstheworkofLuoetal.(2021)toshowthat
theanalogousmaskonamoregenerald-dimensionalgridisd-levelblockToeplitz.Hence,
thismaskparameterisationalsosupportsO(NlogN)masking,butnowforaslightlymore
generalclassofgraphs.
• Affine mappings on forests. If G is a forest and M = exp(τ(dist(v ,v ))), where
ij i j
dist(v ,v ) is the shortest path distance between nodes v and v and τ is an affine map-
i j i j
ping,thentopologicalmaskingcanbeimplementedinO(N).Thealgorithmusesdynamic
17Preprint. Underreview.
programmingtechniquesforrootedtrees. Notwithstandingitsspeed,theflexibilityofthis
parameterisationislimitedandonlyparticulartopologiescanbetreated.
• Heat kernels on hypercubes. Kondor and Lafferty (2002) derived a closed form for the
heatkernelonahypercube,whichalsoturnsoutgiveaToeplitzmaskthatcanbeapplied
in O(NlogN). This is a special instantiation of our power series masks M (W) for a
α
specificchoiceofG. Theauthorsdidnotempiricallytestheatkernels; wefoundthemto
performverypoorlycomparedtoourmoregeneral,fastermethod.
• Randomwalkgraphnodekernels. Here,theauthorsdefineanad-hocgraphkernelbytak-
ing the dot product between two ‘frequency vectors’, recording the nodes visited by the
random walks beginning at the two nodes of interest. This method is the closest to our
approach, but with some crucial differences: (i) we use importance sampling of random
walks,whichwefindtobecrucialforgoodperformance(seeApp.C);(ii)weuselearn-
able weights that depend of the length of random walk, permitting flexible upweighting
or downweighting of longer- and shorter-range interactions; (iii) our method provides a
Monte Carlo estimate of a learnable function of a weighted adjacency matrix, which en-
joyscertainregularisationproperties(Reidetal.,2024b). Moreimportantly,weareableto
derivestrongconcentrationboundsthatprovethatourmethodisO(N), whereasChoro-
manskietal.(2022)donotexplicitlyguaranteeanytimecomplexities.
5. Centrality degree encoding (Chen et al., 2024). This paper takes the mask M =
ij
sin(π[z(d )+z(d )]),whered isthedegreeofnodev andz :N→(0,1)isalearnable
4 i j {i,j} {i,j}
functionthereof. ThiscanbeimplementedwithlineartransformersongeneralgraphsinO(N)
time complexity, but since it is completely insensitive to the relative positions of the nodes in
thegraphitcannotbeconsideredageneraltopologicalmaskingmechanism.
6. Graphattentionnetworks(Velicˇkovic´ etal.,2018). Thisseminalworkincorporatesmasked
self-attentionintographneuralnetworks,witheachnodeattendingonlytoitsneighbours. The
focusisondevelopingaconvolution-styleneuralnetworkforgraph-structureddataratherthan
onefficientmaskingoflinearattentiontransformers.Ourmaskparameterisationismoregeneral
thanthishardnearest-neighboursexample,butsharesthefindingthatmodulatingattentionby
localtopologicalstructureboostsperformance.
C FURTHER EXPERIMENTS AND DETAILS
Here,weincludeextraexperimentsanddetailstoolongforthemaintext.
C.1 ARCHITECTURES,HYPERPARAMETERSANDTRAININGDETAILS
Table2givesdetailsfortheViTexperiments.
C.2 VITABLATIONS
Here, we run extra experiments to further evidence the findings reported in the main text. Unless
explicitlystated,architecturesandtrainingdetailsareasreportedinTable2.
Number of walkers n. We construct GRFs using {1,10,100,1000} walkers with a termination
probabilityp = 0.5, retrainingfromscratchusingtherespectivetopologicalmasks. SeeFig.6.
halt
Largernreducesthemaskvarianceandimprovespredictiveperformance, butalsomakesthefea-
tures {ϕbG(v i)}
vi∈N
less sparse so increases computational cost. Since the estimator is unbiased,
the n → ∞ limit gives dense features {ϕ (v )} whose dot product is exactly a Taylor mask
G i vi∈N
M (G).
α
Importance sampling. The crux of our approach is modulating attention by an unbiased Monte
CarloapproximationofafunctionofaweightedadjacencymatrixW. UsingGRFs, thisinvolves
samplingrandomwalkswhichwereweightby(i)theirprobabilityunderthesamplingmechanism,
(ii) the product of traversed edge weights ω(ω), and (iii) a learnable function f (see Eq. 10). To
e
investigatetheimportanceofthisprincipledsamplingmechanism,weablate(i)and(ii)andcompare
18Preprint. Underreview.
ImageNet iNaturalist2021 Places365(Small)
1M 2.7M 1.8M
Num.layers 12 12 12
Num.heads 12 12 12
Num.patches 16×16 16×16 16×16
Hiddensize 768 768 768
MLPdim. 3072 3072 3072
Optimiser Adam Adam Adam
Epochs 90 5 5
Baselearningrate 3×10−3 3×10−3 3×10−3
Finallearningrate 1×10−5 1×10−5 1×10−5
Learningrateschedule Linearwarmup(104steps),constant,cosinedecay
Batchsize 4096 64 64
ϕ(·) ReLU(·) ReLU(·) ReLU(·)
Pretrained? ✗ ✓ ✓
Num.walks 100 20 20
p 0.1 0.1 0.1
halt
Max.walklength(i ) 100 10 10
max
Table2: Architecture,hyperparametersandtrainingdetailsforViTexperiments.
withanad-hocempiricalrandomwalkkerneldefinedbythefeatures
n
ϕ (v ) := 1 X X f I(ω prefixsubwalkofω(i)), q =1,...,N. (21)
ad-hoc i q n len(ωiq) iq k
k=1ωiq∈Ωiq
Eq.21stillsimulatesrandomwalksandrecordstheirdestinations,givinganempiricalmeasureof
theoverlapbetweenrespectivenodes’neighborhoods. However,inabsenceofdependenceonω(ω)
e
and p(ω) this cannot be interpreted as a MC estimate of a function of W. This is similar to the
approachemployedbyChoromanskietal.(2022)butwithextralearnabilityviaf.
ComparingViTperformancewithn = 100randomwalksandp = 0.5,theresultsarestriking;
halt
seeTable3.Incontrasttoourmethod,themaskdefinedbyEq.21isworsethantheunmaskedbase-
line. ThisshowsthecrucialimportanceofthetheoryinSec.3.4. Intuitively,withoutupweighting
long,improbablewalksbyp(ω)−1itisdifficulttocapturelong-rangedependenciesinthetopologi-
calmask.
Fully learnable features. As a final check, we endow each graph with a fully learnable, dense
N-dimensional feature ϕ (v ) ∈ RN, relaxing the parameterisation as a function of a
dense,learned i
weighted adjacency matrix M (G). This uses many more parameters and is O(N2) rather than
α
O(N),butittechnicallyincludesGRFsasaspecialcase. Inmakingthischoicewehaveremoved
allstructuralinductivebiasfromG,allowingthemodeltoimplicitlylearnamaskingkernelwithout
anyconstraints(exceptfeaturedimension). Themodelperformsbadly,providingonlyaveryslight
improvementovertheunmaskedlinearbaseline. SeeTable3.
Figure 6: Final test accuracy vs. number of Table 3: Ablation studies. Importance sam-
walkersn.MonteCarlomaskvariancedrops plingofrandomwalksandmaskparameteri-
withthenumberofwalkssopredictionsim- sationasafunctionofWarecrucialforgood
prove. performance.
Predictive performance vs. sample size
0.74
Variant Testacc.
0.72 GRF-masked GRF-masked 0.730
Unmasked Unmasked 0.693
0.70
Mα( G) Ad-hocmasked(Eq.21) 0.689
Fullylearnablefeatures 0.696
0.68
100 101 102 103
Number of walkers, n
19
ycarucca
tset
laniFPreprint. Underreview.
C.3 HD-VPDEXPERIMENTALDETAILS
Here, we describe the high density visual particle dynamics (HD-VPD) experiment from Sec. 4
in more detail. The setup closely follows that of (Whitney et al., 2024), to which we direct the
interestedreaderforanexhaustiveinformationabouthardwareandmodellingtechniques. Wewill
provideabriefoverviewforconvenience.
Dynamics modelling as video prediction. The HD-VPD stack involves three steps: (1) encode
the input images as 3D point clouds; (2) predict updates to the point clouds, conditioned on robot
actions;(3)renderthepredictednewstateintoanimage.Wetrainend-to-endusingvideoprediction
loss. Inmoredetail:
1. Encodetheinputimages. ThemodelreceivestwoRGB-D(colourchannelsplusdepth)images
attimest−1,t−2. EachRGBimageisencodedintoper-pixelfeaturesusingaU-Net(Ron-
nebergeretal.,2015),andunprojectedusingknowncameraintrinsicsandextrinsicstoapoint
cloud(x ,f )Npixels withx∈R3andf ∈R16. Foreverytimestep,theparticlesacrossdifferent
i i i=1
camerasaremergedandsubsampleduniformlyatrandomtogetN =32768. Therobotactions
attimesteps{t−2,t−1,t},representingtherobotjointsandwheretheyplantomove,arealso
encodedaskinematicparticles.
2. Predict the point cloud dynamics. With an Interlacer (see below), use the point clouds and
kinematicparticlestopredict(∆x ,∆f )N ,andhencetheupdatedstate(xt−1+∆x ,ft−1+
i i i=1 i i i
∆f )N .
i i=1
3. Renderthenewstatetoanimage. Usearay-basedrenderersimilartoPoint-NeRF(Xuetal.,
2022)torenderanimagefortheupdatedpointcloud.
Thedynamicsmodelcanberecursivelyappliedtomakepredictionsmultiplerollouttimestepsinto
thefuture. Totrain,wesampleasubsetofraystorenderateachtimestepandcomputetheL loss
2
betweenthepredictedandobservedRGBvalues.
Interlacers as PCTs. The Interlacer is a type of point cloud transformer (PCT) designed to scale
totensofthousandsofparticles. Italternateslinear-attentiontransformerlayersthatcaptureglobal
dependenciesbetweentokenswithlocalneighbour-attenderlayersthatmodelfine-grainedgeomet-
ricstructure(seeFig.3byWhitneyetal.(2024)). Thisisreportedtooutcompetebothgraphneural
networks and vanilla linear-attention transformers. For the neighbour-attender, the authors use a
messagepassing-typealgorithmthatupdatesthefeaturesofasubsetofanchorparticlesdepending
onthek-nearestneighbours’positionandfeaturevectors,andthenupdatestheremainingparticles
depending on their closest anchor. In Sec. 4, we show that this can be replaced by GRF-masked
linearattention. Thisalsocaptureslocalstructureandobviatesexpensivemessagepassing.
Training and architecture details. This exactly follows App. E by Whitney et al. (2024), apart
from:
1. Weaverageover5seeds,discardingonewheretrainingdoesnotconverge.
2. Forouralgorithm,weuseasymmetricGRFs. (Sec.D).Weusen = 33 = 27repellingrandom
walks(Reidetal.,2023b). Insteadofterminating,wesample3hopsdeterministicallysincethe
computational bottleneck is finding the k-nearest neighbours list rather than sampling lengths
onceithasbeencomputed. Wetakeϕ(·)=ReLU(·)andcomputeGRF-maskedattentionwith
asinglehead,projectingthequeriesandkeyswithadenselayer.
3. To construct G, we find k = 3 neighbours for every node. This is done on the fly since the
neighbourscanchangeateverytimestep.
4. ForFig.5,wetrainfor100ksteps,usingalearningrateof3×10−4untilstep1000then1×10−4
untilstep100k.
Toenumeratesomeotherimportantdetails: allmodelsaretrainedwithabatchsizeof16; weuse
the AdamW optimiser (Loshchilov, 2017) with weight decay 10−3, clipping the gradient norm to
0.01; modelsaretrainedwith6steprollouts,withlossescomputedon128sampledrays; ∆x are
i
constrainedto[−0.25,0.25]byascaledtanhnonlinearitysoparticlesdonotmovemorethan25cm
pertimestep. Fortherendering, likeWhitneyetal.(2024)weusefourconcentricannularkernels
ofradii[0,0.01,0.02,0.05]withbandwidths[0.01,0.01,0.01,0.05],approximatedusing16nearest
neighbours. Thenearplaneis0.1mandthefarplaneis2m,withthebackgroundsettosolidwhite.
20Preprint. Underreview.
C.4 VIVIT:TOPOLOGICALMASKINGFORVIDEODATA
Here, we present preliminary results for incorporating Table 4: ViViT test accuracies on the
topological masking with graph random features into Kineticsbenchmark.Topologicalmask-
video vision transformers (ViViT; Arnab et al., 2021). ingwithGRFsboostsperformance.
Prompted by the original paper, we use a factorised en-
coder, where the spatial encoder follows the ViT archi- VARIANT TESTACC.
tecturefromearlierinthepaper(seeSec.4andApp.C.1)
SOFTMAX 0.754
withpatchsize16×16×2.Thefinalindexmixespairsof
SOFTMAX+GRFS 0.758
successive frames. The temporal encoder, which subse-
quentlymodelsinteractionsbetweentheframe-levelrep-
resentations, also follows this architecture but with 4 layers instead of 12. We train for 30 epochs
using the Adam optimiser, with base learning rate 10−1 and final learning rate 10−4. We take the
same schedule as in Table 2. We train and evaluate on the Kinetics 400 benchmark (Kay et al.,
2017). Table4showstheresultsfor(i)unmaskedsoftmaxattentionand(ii)softmaxattentionwith
GRFs. Topological masking boosts predictive performance by +0.4%. To our knowledge, this is
thefirstapplicationoftopologicalmaskingtothevideomodality.
D ASYMMETRIC GRAPH RANDOM FEATURES
In this Appendix, we present a variant of our core topological masking algorithm that relaxes the
assumption that the function f for the query and key ensembles of walkers are identical, instead
using asymmetric graph random features. These are believed (but not proved) to generally give
higher variance mask estimates (Reid et al., 2024b), but making a judicious choice can also bring
computational and implementation benefits. Using asymmetric GRFs for topological masking is
also a novel contribution of this paper. In Sec. 4, it is found to be a convenient choice for the
Interlacer(Whitneyetal.,2023)becauseitcanbestraightforwardlyswappedintoexistingcode.
AsymmetricGRFs. ItisstraightforwardtoseethatEq.13ofApp.A.1generalisesto
N ∞ ! ∞ !
M (G) =X X f(1)Wk1 X f(2)Wk2 (22)
α ij k1 iv k2 jv
v=1 k1=0 k2=0
ifthediscreteconvolution
k
X f(1)f(2) =α (23)
p k−p k
p=0
holds. Wedonotnecessarilyrequirethatf(1) = f(2); thisisaspecialcase. Anotherspecialcase
is f(2) = {1ifp = 0, 0otherwise}, whereupon we need f(1) = α ∀p. Sampling GRFs in this
p p p
mannerwillstillgiveanunbiasedestimateofM (G) ,butwithdifferentconcentrationproperties.
α ij
Computationalbenefits. Supposingthatf(2) = {1ifp = 0, 0otherwise},ϕ(2)(v ) = I(i = n),
p G i n
a one-hot vector only nonzero at the coordinate corresponding to v . In other words, we do not
i
needtosimulateanyrandomwalkstoobtainGRFsforthekeys;Φ(2) = I withI ∈ RN×N the
G N N
identitymatrix. Itfollowsthat
Mcα(G)=Φ( G1)Φ( G2)⊤ =Φ( G1) =:[ϕ( G1)(v i)]N i=1. (24)
But ϕ(1)(v ) is only nonzero at nodes visited by the ensemble of walkers beginning at v . This
i i
makes masked attention straightforward to efficiently compute, obviating the outer products and
vectorisation. Simply,
 N
N
(A⊙Mcα(G))V=(A⊙Φ( G1))V=X A ijϕ( G1)(v i) jv j (25)
j=1
i=1
where A = exp(q⊤k ) (softmax) or A = q⊤k (linear) and {v }N are the value vectors.
ij i j ij i j j j=1
Since ϕ(1)(v ) is sparse, the sum in Eq. 25 is computed in constant time so attention is computed
G i
21Preprint. Underreview.
in O(N). Even more explicitly, for an ensemble of walks {ω(i)} , we can rewrite the
k vi∈N,k∈[[1,n]]
aboveas
 N
Xn X A ω e(ω)f len(ω) v  (26)
 iω[−1] p(ω) ω[−1]
k=1ωp.s.ω(i)
k i=1
whereω[−1]standsforthefinalnodeofthewalkωandp.s. means‘prefixsubwalk’(seeApp.A.2).
ThenumberofFLOPstocomputeattentionfornodev dependsonthelengthofallthegeometrically
i
distributed walks beginning at node v , but we have seen that this is independent of graph size
i
(Sec. 3.4). Computing the entire matrix is only O(N). We give pseudocode for this approach in
Alg.2.
BenefitsandlimitationsofasymmetricGRFs.ThechiefbenefitofasymmetricGRFsisthatEq.26
makesefficientmaskedattentionverystraightforwardtocompute,withoutrelyingonsparsematrix
operations that may not be well-optimised in machine learning libraries. We use it for the HD-
VPDexperimentsbecauseitcanbeeasilyintegratedwiththeexistingcode: insteadoffindingeach
node’s(approximate)k-nearestneighboursandimplementingthecomplicatedmessagepassing-type
algorithmbyWhitneyetal.(2024),wefindthe3-hopneighbourhoodandstraightforwardlycompute
weightedattentionwithalearnablefunctionofwalklength.Thedrawbackisthatthevarianceofthe
maskestimatewithasymmetricGRFstendstobegreater,thoughprovingthisisanopenproblem.
Algorithm2O(N)topologicalmaskingwithasymmetricGRFs
Input: querymatrixQ ∈ RN×d,keymatrixK ∈ RN×d,valuematrixV ∈ RN×d,graphG with
weighted adjacency matrix W ∈ RN×N, learnable mask parameters {f }imax, number of random
i i=0
walkstosamplen∈N,query/keyfeaturemapϕ(·):Rd →Rm.
Output: maskedattentionAtt (Q,K,V,G)inO(N)time,withoutusingsparselinearalgebra
LR,Mb
libraries.
1: Simulatenterminatingrandomwalks{ω k(i)}n k=1outofeverynodev i ∈N
2: forv i ∈N do
3: Initialiseoutputfeature,f i ←0 d
4: Initialiseattentionnormalisation,S i ←0
5: forω(i)in{ω(i)}n do
k k=1
6: for(t,v j)inenumerate(ω(i))do
7: f i+=ϕ(q i)⊤ϕ(k j)×f t× eω p(( ωω (( ii )) [[ :: tt ]] )) ×v j
8: S i+=ϕ(q i)⊤ϕ(k j)×f t× eω p(( ωω (( ii )) [[ :: tt ]] ))
9: endfor
10: endfor
11: Normaliseoutputfeature,f i/=(S i×n)
12: endfor
13: Returnlow-rankattentionwithtopologicalmaskinginO(N)timeby
Att (Q,K,V,G)=[f ]N (27)
LR,Mb i i=1
22