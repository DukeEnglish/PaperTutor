ON THE HARDNESS OF LEARNING ONE HIDDEN LAYER NEURAL NETWORKS
SHUCHENLI§,ILIASZADIK§,MANOLISZAMPETAKIS§
Abstract. Inthiswork,weconsidertheproblemoflearningonehiddenlayerReLUneuralnet-
workswithinputsfromℝ𝑑. Weshowthatthislearningproblemishardunderstandardcrypto-
graphicassumptionsevenwhen:(1)thesizeoftheneuralnetworkispolynomialin𝑑,(2)itsinput
distributionisastandardGaussian,and(3)thenoiseisGaussianandpolynomiallysmallin𝑑.Our
hardnessresultisbasedonthehardnessoftheContinuousLearningwithErrors(CLWE)problem,
and in particular, is based on the largely believed worst-case hardness of approximately solving
theshortestvectorproblemuptoamultiplicativepolynomialfactor.
1. Introduction
In this paper, we examine the fundamental computational limitations of learning neural net-
works in a distribution-specific setting. Our focus is on the following canonical regression sce-
nario: let 𝑓 be an unknown target function that can be represented as a simple neural network,
let 𝒟 be a 𝑑-dimensional distribution from which samples 𝑥 𝑖 are drawn, i.e., 𝑥 𝑖 „ 𝒟, and
let 𝜂 𝑖 „ 𝑁p0,𝜎2 q be small Gaussian observation noise. The statistician receives 𝑚 indepen-
dent and identically distributed samples of the form p𝑥 𝑖, 𝑓p𝑥 𝑖q `𝜂 𝑖q for 𝑖 “ 1,...,𝑚 with the
ˆ
goal of constructing an estimator 𝑓 that is computable in polynomial time and achieves a small
mean squared error (MSE) on a new sample drawn from 𝒟. Specifically, we aim to minimize
” ı
𝔼
𝑥„𝒟
|𝑓ˆ p𝑥q´ 𝑓p𝑥q|2 . WeconsiderthefollowingtwoobjectivesintermsoftheMSE:
(1) Achieving Vanishing MSE: Obtaining an MSE that approaches zero as the dimension 𝑑
increases.
(2) Weak Learning: Attaining an MSE slightly better than that of the trivial mean estimator.
Formally,thismeansensuringforlargeenough 𝑑:
” ı
1
𝔼 𝑥„𝒟 |𝑓ˆ p𝑥q´ 𝑓p𝑥q|2 ď Var 𝑥„𝒟p𝑓q´ polyp𝑑q.
It is well known due to [KS09] that without further assumptions on the distribution 𝒟, e.g.,
when 𝒟 can be supported over the Boolean hypercube, learning even one-hidden layer neu-
ral networks is impossible (or “hard”1) for polynomial-time estimators under standard crypto-
graphicassumptions. Giventhesuccessofneuralnetworksinpractice,alonglineofrecentwork
has attempted to study instead the canonical continuous input distribution case where 𝒟 is the
isotropic Gaussian, i.e., 𝒟 “ 𝑁 p0,𝐼 𝑑q which is also the setting that we follow in this work. Yet,
despitealonglineofresearch,thefollowingimportantquestionremainsopen.
Isthereapoly-timealgorithmforlearning1hiddenlayerneuralnetworkswhen 𝒟 “ 𝑁 p0,𝐼 𝑑q?
§YaleUniversity.
Emails:shuchen.li@yale.edu,ilias.zadik@yale.edu,manolis.zampetakis@yale.edu.
1Followingastandardconvention,werefertoacomputationaltaskas“hard”ifitisimpossibileforpolynomial-
timemethods.
1
4202
tcO
4
]GL.sc[
1v77430.0142:viXra2 S.LI,I.ZADIK,M.ZAMPETAKIS
It is known that a single neuron, i.e., 0-hidden layer neural network, can be learned in poly-
nomial time [Zar+24], while neural networks with more that 2 hidden layers are hard to learn
[Che+22]. Nevertheless, the case of 1-hidden layer neural networks is not well understood. In
this paper we close this gap in the literature by answering the question above. We show that it
is hard to learn 1-hidden layer neural networks under Gaussian input assuming the hardness of
some standard cryptographic assumptions. Our result settles an important gap in the computa-
tional complexity of learning neural networks with simple input distributions 𝒟 as we explain
inSection1.2below.
1.1. Prior work. We now provide more details on the literature prior to this work. We first
remind the reader that, formally, polynomial-sized 1-hidden layer neural networks can be ex-
pressed using some width parameter 𝑘 “ polyp𝑑q, some weights 𝑤 𝑖 P ℝ𝑑 and some 𝑎 𝑖,𝑏 𝑖 P ℝ
asfollows
ÿ𝑘
𝑓p𝑥q “ 𝑎 𝑖px𝑥 𝑖,𝑤 𝑖y`𝑏 𝑖q `.
𝑖“1
Nowforthisclassofsinglehiddenlayerneuralnetworks,apowerfulalgorithmictoolboxhasbeen
created under the Gaussian input assumption including the works of [JSA15, Bru+17, GLM17,
Zho+17,AZLL18,Zha+19,BJW19,Dia+20,ATV21,SZB21]. Interestinglymostoftheseproposed
algorithmic constructions assume the so-called “realizable” (or noiseless) case where 𝜎 “ 0. Yet,
withtheimportantexceptionofthebrittlelattice-basedmethodsusedin[SZB21],thetechniques
usedarecustomarilyexpectedtobeatleastlymildlyrobusttonoise,andinparticulargeneralize
to the most realistic case where 𝜎 is positive but polynomially small. Another yet significantly
moreconcerningrestrictionoftheabovepositiveresultsisthattheyallrequiresomeassumptions
on the weights. For example, a common such required assumption is that the weights 𝑤 𝑖,𝑖 “
1,2,...,𝑚 are linearly independent (see e.g., [ATV21] and references therein). It is natural to
wonderwhetherrequiringanysuchassumptionisnecessaryforanypolynomial-timeestimator
to learn the class of one hidden layer neural networks, or simply an artifact of the employed
techniques.
Inthatdirection,researchershavemanagedtoestablishcertainunconditionalandconditional
lower bounds for this problem. Specifically, in terms of conditional lower bounds, [Goe+20] and
[Dia+20] proved that under a worst-case choice of weights the class of the so-called correlation
Statistical Query (cSQ) methods (containing e.g., gradient descent with respect to the squared
loss)failstolearntheclassofonehiddenlayerneuralnetworkswithsuper-constantwidtheven
in the noiseless regime. With respect to unconditional lower bounds, [SZB21] has proven that
under cryptographic assumptions (specifically the continuous Learning with Errors (CLWE) as-
sumption),ifthenoisepersample𝜂
𝑖
isallowedtobepolynomiallysmallbutadversariallychosen
(andnotGaussian)thennopolynomial-time 𝑓ˆ cansucceed2. Althoughbotharequiteinteresting
results, they unfortunately come with their drawbacks. The cSQ model is known in many set-
tings to be underperforming compared to multiple other natural polynomial-time methods (see
e.g.,[And+19,CKM22,Dam+24]),whicharguablylimitsthegeneralityofsuchanunconditional
2Formally,thelowerboundin[SZB21]isaboutthecosineactivationfunction(andnottheReLUweassumein
thiswork). Yet,standardapproximationresults[Bac17]cantransfertheresulttoonehiddenlayerneuralnetworks
atthecostofextrapolynomiallysmalladditiveapproximationerror(seealso[SZB21,AppendixE])ONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 3
lower bound. Moreover, while [SZB21] is now a lower bound against all polynomial-time algo-
rithms one could argue that the computational hardness arises exactly because of the addition
of adversarial noise and may not be inherent to learning one hidden layer neural networks. In
particular, as we mentioned in our main question above, it remains an intriguing open problem
intheaboveliteraturewhethersomepolynomial-timeestimatorcaninfactlearnthewholeclass
ofpolynomial-sizeonehiddenlayerneuralnetworksundersmallGaussiannoise.
We note that a cleaner hardness picture has been established when the neural networks have
at least two hidden layers. [DV21] has proven, via an elegant lifting technique, that crypto-
graphic assumptions (specifically the existence of a local pseudorandom generator with polyno-
mial stretch) imply that learning three hidden layers neural networks is computationally hard
even in the noiseless case where 𝜎 “ 0. Moreover, [Che+22] built on the lifting technique of
[DV21]andprovedthatunderdifferentcryptographicassumptions(specificallythelearningwith
rounding assumption) that learning two hidden layers neural networks is also computationally
hardagaininthenoiselesscase. Ontopofthat,[Che+22]alsoprovedageneralStatisticalQuery
(SQ)lowerboundinthiscaseoftwohiddenlayerneuralnetworks. Albeitthesepowerfulrecent
results,itremainselusivewhetherasimilartechniquecanprovethehardnessforthemorebasic
case of one hidden layer neural networks, something also highlighted as one of the main open
questionsin[Che+22].
1.2. Contribution. In this work, we establish that under the CLWE assumption from cryptog-
raphy [Bru+21] learning the class of one hidden layer neural network with polynomial small
Gaussian noise is indeed computationally hard. Importantly, solving CLWE in polynomial-time
implies a polynomial-time quantum algorithm that approximates within polynomial factors the
worst-case Gap shortest vector problem (GapSVP), a widely believed hard task in cryptography
and algorithmic theory of lattices [MR09]. Interestingly, our lower bound holds even under the
requirement of weakly learning the neural network. We present our findings in the following
informaltheorem.
Theorem 1.1 (Informal; see Theorem 5.4). Let ℱ 𝑘 the class of widtha𝑘 one hidden layer neural
networks and arbitrary noise variance 𝜎 “ 1{polyp𝑑q. For any 𝑘 “ 𝜔p 𝑑log𝑑q, if there exists a
polynomial-timealgorithmthatcanweaklylearn ℱ 𝑘 underGaussiannoiseofvariance 𝜎 thenthere
existsapolynomial-timequantumalgorithmthatapproximatesGapSVPwithinapolyp𝑑qfactor.
The above result settles the computational question of learning one hidden layer neural net-
works under polynomially small Gaussian noise. It is perhaps natural to wonder if we can also
obtainalowerboundagainstevensmallerlevelsofnoise.
First,wehighlightthataswealsomentionedintheIntroduction,thisisalreadyasignificantly
smallamountofnoise;mostnaturalalgorithmicschemesinlearningtheoryaremildlyrobustto
noise, and therefore they can tolerate polynomially-small levels of Gaussian noise (if not a con-
stant level). That being said, we also mention that one can prove a more general version of our
resultbycombiningthereductionsbetweenCLWEandclassicalLWE[GVV22];ifapolynomial-
a
time estimator can weakly learn ℱ 𝑘 for some 𝜔p 𝑑log𝑑 ¨logp𝑑{𝜎qq “ 𝑘 “ polyp𝑑q under
Gaussiannoiseofarbitrary variance 𝜎 “ 𝜎 𝑑 suchthatlogp1{𝜎q “ polyp𝑑q,thenthereexistsalso
a polynomial-time quantum algorithm that approximates GapSVP within a factor polyp1{𝜎,𝑑q.
Inparticular,giventhatthecurrentstate-of-the-artalgorithmforGapSVPremainssince1982the4 S.LI,I.ZADIK,M.ZAMPETAKIS
celebratedLenstra-Lenstra-Lova´sz(LLL)latticebasisreductionalgorithm[LLL82]whichhasap-
proximationfactorexppΘp𝑑qq,weprovethatanylearningalgorithmforonehiddenlayerneural
?
networkssucceedingforanyexpp´𝑜p 𝑑qq ď 𝜎wouldimmediatelyimplyamajorbreakthrough
in the algorithmic theory of lattices (see Section 6 for a lengthier discussion on this and more
detailsonthisconnection).
The only case that is left open by our results is that some very brittle algorithm can learn
in polynomial-time the class of one hidden layer neural networks (only) for exponentially small
valuesof𝜎. Infact,thatisproventobethecaseusingthebrittleLLL-basedmethodsforthecaseof
cosineneuronin[SZB21],andformultipleother“noiseless”settingsintherecentlearningtheory
literature [And+17, ZG18, GKZ21, Zad+22, DK22]. Yet, while we believe this is an interesting
and potentially highly non-trivial theoretical question, the value of any such brittle algorithmic
method in learning or statistics is unfortunately unclear since a non-negligible amount of noise
alwaysexistsinthesecases.
1.3. Organization. WebegininSection2withthetheformulationofPAC-learningneuralnet-
works and the necessary preliminaries on lattice-based cryptography that we utilize to present
our hardness result. Then in Section 3 we state formally our main result and we provide a proof
sketch. In Sections 4 and 5 we provide the proof of our result in two steps. First, we show the
hardnessoflearninganysingleperiodicneuralnetwork,andthenweshowhowthisimpliesthe
hardnessoflearning1-hiddenlayerneuralnetworks.
2. Preliminaries
2.1. Notations. Throughout the paper we use the standard asymptotic notation, 𝑜,𝜔,𝑂,Θ,Ω
for comparing the growth of two positive sequences p𝑎 𝑑q𝑑Pℕ and p𝑏 𝑑q𝑑Pℕ: we say 𝑎 𝑑 “ Θp𝑏 𝑑q
if there is an absolute constant 𝑐 ą 0 such that 1{𝑐 ď 𝑎 𝑑{𝑏 𝑑 ď 𝑐; 𝑎 𝑑 “ Ωp𝑏 𝑑q or 𝑏 𝑑 “ 𝑂p𝑎 𝑑q if
there exists an absolute constant 𝑐 ą 0 such that 𝑎 𝑑{𝑏 𝑑 ě 𝑐; and 𝑎 𝑑 “ 𝜔p𝑏 𝑑q or 𝑏 𝑑 “ 𝑜p𝑎 𝑑q if
lim 𝑑 𝑎 𝑑{𝑏 𝑑 “ 0. We say 𝑥 “ polyp𝑑q if for some 𝑟 ą 0 it holds 𝑥 “ 𝑂p𝑑𝑟 q. Let 𝑁p𝜇,𝜎2 q denote
the Gaussian distribution with mean 𝜇 and variance 𝜎2, and 𝑁p𝜇,Σq denote the multivariate
Gaussiandistributionwithmean𝜇andcovarianceΣ.
2.2. PAC-learningwithGaussianinputdistribution. Ourfocusonthisworkistheproblem
oflearningasequenceofreal-valuedfunctionclassestℱ 𝑑u𝑑Pℕ,eachoverthestandardGaussian
inputdistributiononℝ𝑑. Theinputisamultisetofi.i.d.labeledexamplesp𝑥,𝑦q P ℝ𝑑 ˆℝ,where
𝑥 „ 𝑁p0,𝐼 𝑑q, 𝑦 “ 𝑓p𝑥q`𝜉, 𝑓 P ℱ 𝑑, and 𝜉 „ 𝑁p0,𝜎2 q for some 𝜎2 ą 0. We denote by 𝐷 “ 𝐷 𝑓
theresultingdatadistribution. Thegoalofthelearneristooutputanhypothesis ℎ : ℝ𝑑 Ñ ℝthat
isclosetothetargetfunction 𝑓 inthesquaredlosssenseovertheGaussianinputdistribution.
Throughout the paper we define ℓ : ℝ ˆ ℝ Ñ ℝ ě0 the squared loss function defined by
ℓp𝑦,𝑧q “ p𝑦´𝑧q2. Foragivenhypothesis ℎ andadatadistribution 𝐷 onpairsp𝑥,𝑧q P ℝ𝑑 ˆℝ,
wedefineitspopulationloss 𝐿 𝐷pℎqoveradatadistribution 𝐷 by
𝐿 𝐷pℎq “ 𝔼 p𝑥,𝑦q„𝐷rℓpℎp𝑥q,𝑦qs . (2.1)
Wenowdefinetheimportantnotionofweaklearning.
Definition 2.1 (Weak learning). Let 𝜀 “ 𝜀p𝑑q ą 0 be a sequence of numbers, 𝛿 P p0,1q a fixed
constant, and let tℱ 𝑑u𝑑Pℕ be a sequence of function classes defined on input space ℝ𝑑. We say
that a (randomized) learning algorithm 𝒜 𝜀-weakly learns tℱ 𝑑u𝑑Pℕ over the standard GaussianONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 5
distribution if for every 𝑓 P ℱ 𝑑 the algorithm outputs a hypothesis ℎ 𝑑 such that for large values
of 𝑑 withprobabilityatleast1´ 𝛿
𝐿 𝐷 pℎ 𝑑q ď 𝐿 𝐷 p𝔼r𝑓p𝑥qsq´ 𝜀 .
𝑓 𝑓
Notethat𝔼
𝑥„𝑁p0,𝐼
𝑑qr𝑓p𝑥qsisthebestpredictoragnostictotheinputdatainthissetting. Hence,
we refer to 𝐿 𝐷p𝔼r𝑓p𝑥qsq “ Var 𝑍„𝑁p0,𝐼 𝑑qp𝑓p𝑍qq, as the trivial loss, and 𝜀 as the edge of the
learningalgorithm.
For simplicity, we refer to an hypothesis as weakly learning a function class if it can achieve
edge 𝜀 which is depending inverse polynomially in 𝑑. Moreover, we simply set from now on
𝛿 “ 1{3whenwerefertoweaklearning.
2.3. Worst-Case Lattice Problems. Some background on lattice problems is required for our
work. Westartwiththedefinitionofalattice.
Definition2.2. Givenlinearlyindependent 𝑏 1,...,𝑏 𝑑 P ℝ𝑑,let
# +
ÿ𝑑
Λ “ Λp𝑏 1,...,𝑏 𝑑q “ 𝜆 𝑖𝑏 𝑖 : 𝜆 𝑖 P ℤ,𝑖 “ 1,...,𝑑 , (2.2)
𝑖“1
whichwerefertoasthelatticegeneratedby 𝑏 ,...,𝑏 .
1 𝑑
Acoreworst-casedecisionalgorithmicproblemonlatticesisGapSVP.InGapSVP,wearegiven
an instance of the form pΛ,𝑡q, where Λ is a 𝑑-dimensional lattice and 𝑡 P ℝ, the goal is to
distinguish between the case where 𝜆 1pΛq, the ℓ 2-norm of the shortest non-zero vector in Λ,
satisfies 𝜆 1pΛq ă 𝑡 from the case where 𝜆 1pΛq ě 𝛼p𝑑q¨𝑡 for some “gap” 𝛼p𝑑q ě 1. We refer to
anysuchsuccessfulalgorithmassolvingGapSVPwithinan 𝛼p𝑑qfactor.
GapSVPisknowntobeNP-hardfor“almost”polynomialapproximationfactors,thatis,2plog𝑑q1´𝜀
foranyconstant𝜀 ą 0,assumingproblemsinNPcannotbesolvedinquasi-polynomialtime[Kho05,
HR07]. Moreover, importantly for this work, GapSVP is strongly believed to be computationally
hard (even with quantum computation), for any polynomial approximation factor 𝛼p𝑑q [MR09],
asdescribedinthefollowingconjecture.
Conjecture 2.3 ([MR09, Conjecture 1.2]). There is no polynomial-time quantum algorithm that
solvesGapSVPtowithinpolynomialfactors.
Wecommentontheversionofthisconjectureforsuper-polynomialfactorsinSection6.
2.4. ContinuousLearningwithErrors(CLWE)[Bru+21]. Ofcrucialimportancetousisthe
CLWEdecision(ordetection)problem. WedefinetheCLWEdistributionCLWE ondimension
𝛽,𝛾
𝑑 with frequency 𝛾 “ 𝛾p𝑑q ě 0, and noise rate 𝛽 “ 𝛽p𝑑q ě 0 to be the distribution of i.i.d.
samples of the form p𝑥,𝑧q P ℝ𝑑 ˆ r´1{2,1{2q where 𝑥 „ 𝑁p0,𝐼 𝑑q,𝜉 „ 𝑁p0,𝛽q, 𝑤 uniformly
chosenfromthesphere𝒮𝑑´1 and
𝑧 “ 𝛾x𝑥,𝑤y`𝜉 mod 1 . (2.3)
Notethatforthemod1operation,wetaketherepresentativesinr´1{2,1{2q. TheCLWEproblem
consists of detecting between i.i.d. samples from the CLWE distribution or the null distribution
𝑁p0,𝐼 𝑑qˆ𝑈pr´1{2,1{2qqwhichwedenoteby 𝐴 0.6 S.LI,I.ZADIK,M.ZAMPETAKIS
Given 𝛾 “ 𝛾p𝑑q and 𝛽 “ 𝛽p𝑑q, we consider a sequence of decision problems tCLWE 𝛽,𝛾u𝑑Pℕ,
indexed by the input dimension 𝑑, in which the learner receives 𝑚 samples from an unknown
distribution 𝐷 such that either 𝐷 “ 𝐴 𝛽,𝛾 or 𝐷 “ 𝐴 0. We consider the classical hypothesis
testing setting that we aim to construct a polynomial-time binary-valued testing algorithm 𝒜
which uses as input the samples and distinguishes the two distributions. Specifically, 𝒜 takes
valuesint𝐴 𝛽,𝛾,𝐴 0uandseekstooutput“𝐴 𝛽,𝛾”when 𝐷 “ 𝐴 𝛽,𝛾 and“𝐴 0”when 𝐷 “ 𝐴 0. Under
thissetup,wedefinetheadvantage of 𝒜 tobethefollowingdifference,
ˇ ˇ
ˇ ˇ
ˇP𝑥„p𝐴 𝛽,𝛾qb𝑚r𝒜p𝑥q “ 𝐴 0s´P 𝑥„𝐴b𝑚r𝒜p𝑥q “ 𝐴 0sˇ .
0
Note that the advantage simply equals to one minus the sum of the type I and type II errors in
statistical terminology. We call the advantage non-negligible if it decays at most polynomially
fasti.e.,itisΩp𝑑´𝐶 qforsome 𝐶 ą 0.
[Bru+21]providedworst-caseevidencebasedonthehardnessofGapSVP(Conjecture2.3)that
solving the CLWE decision problem with non-negligible advantage is computationally hard for
?
any 𝛽 “ 1{polyp𝑑q as long as 𝛾 ě 2 𝑑. This is an immediate corollary of the result below
combinedwithConjecture2.3.
?
Theorem 2.4 ([Bru+21, Corollary 3.2]). Let 𝛽 “ 𝛽p𝑑q “ 1{polyp𝑑q and 2 𝑑 ď 𝛾 “ 𝛾p𝑑q “
polyp𝑑q. Then,ifthereexistsapolynomial-timealgorithmforCLWE 𝛽,𝛾 withnon-negligibleadvan-
tage,thenthereisapolynomial-timequantumalgorithmforsolvesGapSVPwithinpolyp𝑑qfactors.
Forsimplicity,wesaythatsomealgorithm“solvesCLWE”torefertothefactthatthealgorithm
hasnon-negligibleadvantageforthedecisionversionofCLWE.
3. MainResult
Webeginwithdefiningtheclass ℱNN ofonehiddenlayerneuralnetworks
𝑘
$ ,
& ÿ𝑘 .
ℱ 𝑘NN “ %𝑓 𝑊,𝑏p𝑥q “ 𝑎 𝑗px𝑤 𝑗,𝑥y`𝑏 𝑗q
`
| 𝑎 P ℝ𝑘,𝑊 P ℝ𝑑ˆ𝑘,𝑏 P ℝ𝑘 -.
𝑗“1
Ourmainresultisthefollowing.
a
Theorem 3.1. Let 𝑑 P ℕ, and arbitrary 𝜎 “ polyp𝑑q´1 , 𝜀 “ polyp𝑑q´1 , and 𝑘 “ 𝜔p 𝑑log𝑑q.
Thenapolynomial-timeestimatorthat𝜀-weaklylearnsthefunctionclassℱNNoverGaussianinputs
𝑘
𝑥 i. „i.d. 𝑁p0,𝐼 𝑑qunderGaussiannoise𝜉 i. „i.d. 𝑁p0,𝜎2 qimpliesapolynomial-timequantumalgorithm
thatapproximatesGapSVPtowithinpolynomialfactors.
NoticethatdirectlyfromourTheorem3.1andthewidelybelievedConjecture2.3wecancon-
clude that no polynomial-time estimator can weakly learn the class of one hidden layer neural
networksunderarbitrarypolynomiallysmallGaussiannoise.
3.1. ProofSketchandComparisonwith[SZB21]. Our(simple)proofisanappropriatecom-
bination of two key steps. We first establish in Section 4 that solving the CLWE problem re-
ducestolearningLipschitzperiodicneuronsunderpolynomiallysmallGaussiannoise(seeTheo-
rem4.2). Thisisadirectimprovementuponthekeyresult[SZB21,Theorem3.3]thatestablishes
that CLWE reduces to learning Lipschitz periodic neurons under polynomially small adversarialONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 7
noise. Our approach is to perhaps interestingly show that one can “Gaussianize” the adversarial
noiseinthelabelsgeneratedviathereductionfollowedby[SZB21]bysimplyinjectingadditional
Gaussiannoiseofappropriatevariancetothem(seeLemma4.1).
Recall that, using standard approximation results [SZB21, Appendix E], learning 1-Lipschitz
periodic neurons under polynomially small adversarial noise is equivalent with learning one
hidden layer neural networks of appropriate polynomial width under (slightly larger) polyno-
mially small adversarial noise. Unfortunately, we cannot straightforwardly generalize this logic
toGaussianerrorsusingourfirststep,becausetheinducedapproximationerrorcaninprinciple
be too large for our “Gaussianization” lemma to work. Regardless, instead of using approxima-
tion results, in Section 5, we follow a more direct route and prove that for any arbitrary large
bounded interval r´𝑅,𝑅s one can explicitly construct an appropriate Lipschitz periodic neuron
and a polynomial-width neural network that exactly agree on r´𝑅,𝑅s, i.e., have zero “approxi-
mation”error(seeLemma5.1). ThislemmacombinedwithourfirststepTheorem4.2allowusto
reduceCLWEtolearningonehiddenlayerneuralnetworksunderGaussiannoise. Combiningthe
abovewiththereductionfromGapSVPtoCLWE(Theorem2.4)letusthenconcludeTheorem3.1.
4. CLWEreductiontoLipschitzPeriodicNeuronsunderGaussiannoise
We first recall the notion of Lipschitz periodic neurons from [SZB21]. Let 𝛾 “ 𝛾p𝑑q ą 1 be
a sequence of numbers indexed by the input dimension 𝑑 P NN, and let 𝜙 : ℝ Ñ r´1,1s be a
𝜙
1-Lipschitzand1-periodicfunction. Wedenoteby ℱ thefunctionclass
𝛾
ℱ 𝛾𝜙 “ t𝑓 : ℝ𝑑 Ñ r´1,1s | 𝑓p𝑥q “ 𝜙p𝛾x𝑤,𝑥yq,𝑤 P 𝑆𝑑´1 u (4.1)
Notethateachmemberofthefunctionclassℱ 𝛾𝜙 isfullycharacterizedbyaunitvector𝑤 P 𝑆𝑑´1.
WerefersuchfunctionclassesasLipschitzperiodicneurons.
[SZB21] has established that solving CLWE reduces to learning in polynomial-time the class
𝜙
ℱ under polynomially small adversarial noise. Their reduction is very simple; given a CLWE
𝛾
p𝑥,𝛾x𝑤,𝑥y`𝜉 mod 1qonecancreateasamplep𝑥,𝜙p𝛾x𝑤,𝑥y`𝜉qbyapplying 𝜙 since 𝜙 is1-
periodic. Butsince𝜙is1-Lipschitzand𝜉is“small”,notice𝜙p𝛾x𝑤,𝑥y`𝜉q “ 𝜙p𝛾x𝑤,𝑥yq`𝜉1for
some𝜉1 also“small”as|𝜉1| ď |𝜉|.Hence,theauthorsof[SZB21]constructfromaCLWEsample,
𝜙
a sample from the Lipschitz periodic neuron class ℱ , but under the somewhat cumbersome
𝛾
noise variable 𝜉1 which we can only control its magnitude – for this reason 𝜉1 is referred to as
smalladversarialnoisein[SZB21].
Our first step is to improve upon [SZB21] and construct instead a sample from the Lipschitz
𝜙
periodic neuron class ℱ , but under simply Gaussian noise 𝜉1. Our idea to do so is to simply
𝛾
inject additional small Gaussian noise to 𝜙p𝛾x𝑤,𝑥y`𝜉q. We prove that as long as the variance
of the added noise is of slightly larger magnitude than the magnitude of the (already polynomi-
allysmall)noise 𝜉,intotalvariationdistancethesampleapproximatelyequalsindistributionto
𝜙p𝛾x𝑤,𝑥yq`𝜉1 wherenow 𝜉1 isGaussian. Thisresultisdescribedinthefollowinglemma.
Lemma 4.1. Let 𝜙 : ℝ Ñ ℝ be an 1-Lipschitz function. For fixed 𝛾 ą 0 and 𝑤 P 𝑆𝑑´1 , and
𝑥 „ 𝑁p0,𝐼 𝑑q,𝜉 0 „ 𝑁p0,𝛽q,𝜉 „ 𝑁p0,𝜎2 q,thetotalvariationdi ?stancebetweenthedistributionsof
𝛽
p𝑥,𝜙p𝛾x𝑤,𝑥y`𝜉 0q`𝜉qandp𝑥,𝜙p𝛾x𝑤,𝑥yq`𝜉qisatmost ? .
2𝜋𝜎8 S.LI,I.ZADIK,M.ZAMPETAKIS
Proof. Since the first entries of the two pairs are the same, it suffices to upper bound the total
variancedistancebetweenthedistributionsof𝑧 1 “ 𝜙p𝛾x𝑤,𝑥y`𝜉 0q`𝜉and𝑧 2 “ 𝜙p𝛾x𝑤,𝑥yq`𝜉
conditioning on 𝑥. Note that conditioning on 𝜉
0
and 𝑥, the distribution of 𝑧
1
is 𝑁p𝜙p𝛾x𝑤,𝑥y`
𝜉 0q,𝜎2 qandthedistributionof 𝑧
2
is 𝑁p𝜙p𝛾x𝑤,𝑥yq,𝜎2 q. Thus,
«c ff
KLp𝑧 1|p𝜉 0,𝑥q}𝑧 2|p𝜉 0,𝑥qq
TVp𝑧 1|𝑥,𝑧 2|𝑥q ď 𝔼 𝜉 0„𝑁p0,𝛽qrTVp𝑧 1|p𝜉 0,𝑥q,𝑧 2|p𝜉 0,𝑥qqs ď 𝔼 𝜉 0„𝑁p0,𝛽q 2
„ ȷ „ ȷ a
|𝜙p𝛾x𝑤,𝑥y`𝜉 0q´ 𝜙p𝛾x𝑤,𝑥yq| |𝜉 0| 𝛽
“ 𝔼 𝜉 0„𝑁p0,𝛽q 2𝜎 ď 𝔼 𝜉 0„𝑁p0,𝛽q 2𝜎 “ ? 2𝜋𝜎,
where the first inequality is from the triangle inequality, the second inequality is from Pinsker’s
inequality,theequalityinthethirdlineisfromtheKLdivergencebetweentwosingledimensional
Guassians,andthelastinequalityisfromthe1-Lipschitzcontinuityof 𝜙. □
Lemma4.1allowustoestablishthefollowingkeyCLWEhardnessresultforLipschitzperiodic
neurons.
a
Theorem4.2. Let𝑑 P ℕ,𝛾 “ 𝜔p log𝑑q,𝜎 P p0,1q,𝜀 “ polyp𝑑q´1 ,𝑚 1 “ polyp𝑑q. Moreover,let
𝜙 : ℝ Ñ r´1,1sbean1-Lipschitz,1-periodicfunction. Then,apolynomial-timelearningalgorithm
using 𝑚 1 samples that 𝜀-weakly learns the function class ℱ 𝛾𝜙 over Gaussian inputs 𝑥 i. „i.d. 𝑁p0,𝐼 𝑑q
and under Gaussi!an label no)ise 𝜉 i. „i.d. 𝑁p0,𝜎2 q implies a polynomial-time algorithm for CLWE𝛽,𝛾
forany 𝛽 ď min 𝜎2 , 𝜀2 .
104𝑚2 103
1
WedefertheproofofthetheoremtoSection4.1. NoticethatadirectcorollaryofTheorem4.2is
𝜙
thataweaklearningalgorithmfortheclassℱ ,impliesaquantumalgorithmforapproximating
𝛾
GapSVPwithingpolynomialfactors.
?
Corollary4.3. Let𝑑 P ℕ,𝛾 “ polyp𝑑qwith𝛾 ě 2 𝑑,𝜎 “ polyp𝑑q´1 ,𝜀 “ polyp𝑑q´1 . Moreover,
let 𝜙 : ℝ Ñ r´1,1s be an 1-Lipschitz 1-periodic function. Then, a polynomial-time algorithm that
𝜀-weakly learns the function class ℱ 𝛾𝜙 “ t𝑓 𝛾,𝑤p𝑥q “ 𝜙p𝛾x𝑤,𝑥yq | 𝑤 P 𝑆𝑑´1 u over Gaussian
inputs 𝑥 i. „i.d. 𝑁p0,𝐼 𝑑q under Gaussian noise 𝜉 i. „i.d. 𝑁p0,𝜎2 q implies a polynomial-time quantum
algorithmthatapproximatesGapSVPtowithinpolynomialfactors.
Proof. Sincetheweaklearning !algorithm )runsinpolynomialtime,thenumberofsamplesituses
is 𝑚 1 “ polyp𝑑q. Let 𝛽 “ min 10𝜎 4𝑚2 2, 1𝜀 02 3 “ polyp𝑑q´1. By Theorem 4.2, there is a polynomial-
1 ?
time algorithm for CLWE 𝛽,𝛾. Moreover, since 𝛽 “ polyp𝑑q´1 and 2 𝑑 ď 𝛾 “ polyp𝑑q, from
Theorem 2.4, there is a polynomial-time quantum algorithm that solves GapSVP within polyp𝑑q
factors. □
4.1. ProofofTheorem4.2.
Proof. We begin with introducing some definitions and notation about CLWE and about weak
𝜙
learnersfortheclass ℱ thatwillbeusefultousduringtheproof.
𝛾
CLWE: We denote by 𝑃 0 the CLWE distribution, i.e., samples p𝑥 𝑖,𝑧 𝑖 “ 𝛾x𝑤,𝑥 𝑖y`𝜉 0,𝑖 mod 1q
where 𝑤 „ 𝑈p𝑆𝑑´1 q, 𝑥 𝑖 „ 𝑁p0,𝐼 𝑑q, and 𝜉 0,𝑖 „ 𝑁p0,𝛽q. Let 𝑄 0 denote the null distribution,
i.e., samples p𝑥 𝑖,𝑦 𝑖q where 𝑥 𝑖 „ 𝑁p0,𝐼 𝑑q, and 𝑦 𝑖 „ 𝑈r0,1s. For some appropriately largeONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 9
𝑚 2 “ polyp𝑑q that will be determined in the proof, we study whether a polynomial-time
algorithm for CLWE can distinguish between 𝑚 i.i.d. samples from 𝑃 and 𝑚 i.i.d. samples
𝛽,𝛾 0
from 𝑄 0,for 𝑚 “ 𝑚 1 `𝑚 2 “ polyp𝑑q,withnon-negligibleadvantage.
Foralabeledsamplep𝑥,𝑦q P ℝ𝑑 ˆℝ,wedefine
𝐹 𝜉p𝑥,𝑦q “ p𝑥,𝜙p𝑦q`𝜉q. (4.2)
Let 𝑃 1 and 𝑄 1 denotethedistributionsof 𝑃 0 and 𝑄 0 afterapplying 𝐹 𝜉 for 𝜉 „ 𝑁p0,𝜎2 q.
𝜙
WeakLearning ℱ 𝛾 : Let 𝑃 𝜙 denote the distribution of the samples p𝑥,𝜙p𝛾x𝑤,𝑥yq ` 𝜉q. For
𝜀 “ polyp𝑑q´1 a weak learner for 𝑃 𝜙 is a polynomial time learning algorithm 𝒜 that take as
input 𝑚 1 samples from 𝑃 𝜙 and with probability 2{3 outputs a hypothesis ℎ1 : ℝ Ñ ℝ such
˜
that 𝐿 𝑃 pℎ1q ď 𝐿 𝑃 p𝔼r𝑓 𝛾,𝑤p𝑥qsq´𝜀. Sinceweareusingthesquaredloss, ℎp𝑥q “ sgnpℎ1p𝑥qq¨
𝜙 𝜙
minp|ℎ1p𝑥q|,1q is always no worse than ℎ1p𝑥q, since 𝜙p𝑥q P r´1,1s and the noise on the label
isunbiased. Thus,wecanassumewithoutlossofgeneralitythat ℎ1p𝑥q P r´1,1s.
𝜙
Our goal in this proof is to assume access to a weak learner for the function class ℱ with
𝛾
𝑚 samples, and design an algorithm for solving the CLWE problem (as defined above) with
1
𝑚 “ 𝑚 1 ` 𝑚 2 for an appropriate number of additional samples 𝑚 2 “ polyp𝑑q. More precisely,
we want to design an efficient algorithm ℬ to distinguish between a set of samples from 𝑃 and
0
a set of samples from 𝑄 0 with 𝑚 “ 𝑚 1 ` 𝑚 2,𝑚 2 “ polyp𝑑q samples, using a weak learning
𝜙
algorithm 𝒜 for ℱ with 𝑚 samples.
𝛾 1
Definition of Algorithm ℬ. For a sufficiently large 𝑚 2 “ polyp𝑑q for the purposes of the proof
the follows, we are given 𝑚 “ 𝑚 1 `𝑚 2 “ polyp𝑑q i.i.d. samples tp𝑥 𝑖,𝑧 𝑖qu𝑚 𝑖“1 from an unknown
distribution 𝐷,whichiseither 𝑃 or 𝑄 ,algorithm ℬ followsthefollowingsteps.
0 0
(1) Sample 𝜉 𝑖 i. „i.d. 𝑁p0,𝜎2 q, 𝑖 “ 1,2,...,𝑚.
(2) Foreach𝑖 “ 1,2,...,𝑚,apply𝐹 𝜉 ,definedin(4.2),top𝑥 𝑖,𝑧 𝑖qtogetasamplep𝑥 𝑖,𝑠 𝑖qfrom
𝑖
𝐷 ,whichiseither 𝑃 or 𝑄 .
1 1 1
(3) Run 𝒜 onthefirst 𝑚 1 ofthesamplesfrom 𝐷 1,andlet ℎ : ℝ Ñ r´1,1sbethehypothesis
that 𝒜 outputs.
(4) Generate 𝑚 samplesfrom 𝑄 .
2 1
(5) Compute the empirical loss 𝐿ˆ
𝐷
pℎq of ℎ on the remaining 𝑚
2
samples from 𝐷 1, and the
1
empiricalloss 𝐿
𝑄
pℎqonthe 𝑚
2
samplesgeneratedfrom 𝑄 1.
1
(6) Testwhether 𝐿ˆ 𝐷 pℎq ď 𝐿ˆ 𝑄 pℎq´ 𝜀{5ornot.
1 1
(7) Intheend,conclude 𝐷 “ 𝑃 0 if ℎ passesthetestinstep(6)and 𝐷 “ 𝑄 0 otherwise.
Proof of Correctness of ℬ. Next we prove the correctness of this algorithm ℬ assuming the cor-
rectness of 𝒜 and using Lemma 4.1. We first show that if 𝐷 “ 𝑃 0 then ℎ will pass the test
𝐿ˆ 𝐷 pℎq ď 𝐿ˆ 𝑄 pℎq´ 𝜀{5andthenweshowthatif 𝐷 “ 𝑄 0 then ℎ willfailthistest.
1 1
CaseI:D “ P 0. Recallthat ℎ and ℎ1denotetheoutputof𝒜 given𝑚 1samplesfrom𝐷 1 “ 𝑃 1and
𝑃 respectively,employingthenotationweintroducedabove. Bythedataprocessinginequality,
𝜙 ` ˘
𝑇𝑉pℎ,ℎ1q ď TV 𝑃 1b𝑚 1,𝑃 𝜙b𝑚 1 ď 𝑚 1 ¨TVp𝑃 1,𝑃 𝜙q, where TVpℎ,ℎ1q refers to the total variation
betweenthedistributionof ℎ,andthedistributionof ℎ1. Hence,byLemma4.1,TVpℎ,ℎ1qisupper10 S.LI,I.ZADIK,M.ZAMPETAKIS
?
𝑚 𝛽
boundedby 1 . Then,
?
2𝜋𝜎
ˇ ˇ
ˇ ˇ
ˇ ˇ
ˇℙ ℎÐ𝒜p𝑃b𝑚1 qr𝐿 𝑃 𝜙pℎq ď 𝐿 𝑃 𝜙p𝔼r𝑓 𝛾,𝑤p𝑥qsq´ 𝜀s´ℙ ℎ1Ð𝒜p𝑃b𝑚1 qr𝐿 𝑃 𝜙pℎq ď 𝐿 𝑃 𝜙p𝔼r𝑓 𝛾,𝑤p𝑥qsq´ 𝜀sˇ
1 𝜙
a
𝑚 2𝛽
1
ď 2TVpℎ,ℎ1q ď ? ď 0.01,
𝜋𝜎
sincewehavechosen 𝛽 ď 𝜎2 . Thus,wehavewithprobabilityatleast2{3´0.01that
104𝑚2
1
𝐿 𝑃 pℎq ď 𝐿 𝑃 p𝔼r𝑓 𝛾,𝑤p𝑥qsq´ 𝜀. (4.3)
𝜙 𝜙
Note that 𝐿 𝑃 𝜙pℎq “ 𝔼 p𝑥,𝑧q„𝑃 𝜙pℎp𝑥q ´ 𝑧q2 “ 𝔼 𝑃 𝜙pℎp𝑥q ´ 𝑓 𝛾,𝑤p𝑥qq2 ` 𝔼𝜉2, and similarly,
𝐿 𝑃 pℎq “ 𝔼 𝑃 pℎp𝑥q´ 𝜙p𝛾x𝑤,𝑥y`𝜉 0qq2 `𝔼𝜉2. Hence,
1 1
ˇ ˇ ˇ ˇ
ˇ ˇ ˇ ˇ
ˇ𝐿 𝑃 pℎq´𝐿 𝑃 pℎqˇ “ ˇ𝔼 𝑃 pℎp𝑥q´ 𝑓 𝛾,𝑤p𝑥qq2 ´𝔼 𝑃 pℎp𝑥q´ 𝜙p𝛾x𝑤,𝑥y`𝜉 0qq2ˇ
𝜙 1 𝜙 1
ˇ ` ˘` ˘ˇ
ˇ ˇ
“ 𝔼 𝑥,𝜉 𝜙p𝛾x𝑤,𝑥yq´ 𝜙p𝛾x𝑤,𝑥y`𝜉 0q 𝜙p𝛾x𝑤,𝑥yq` 𝜙p𝛾x𝑤,𝑥y`𝜉 0q´2ℎp𝑥q
0 b
ď 4𝔼 𝑥,𝜉 |𝜙p𝛾x𝑤,𝑥yq´ 𝜙p𝛾x𝑤,𝑥y`𝜉 0q| ď 4 2𝛽{𝜋 ď 𝜀{5, (4.4)
0
where the last inequality is because we have chosen 𝛽 ď 1𝜀 02 3. Let 𝑐 “ 𝔼 𝑦„𝑈r0,1sr𝜙p𝑦qs then,
canceling𝔼𝜉2 similarly,
ˇ ˇ ˇ ˇ
ˇ ˇ ˇ ˇ
ˇ𝐿 𝑃 p𝑐q´𝐿 𝑄 p𝑐qˇ “ ˇ𝔼 𝑃 p𝑐 ´ 𝜙p𝛾x𝑤,𝑥yqq2 ´𝔼 𝑄 p𝑐 ´ 𝜙p𝑦qq2ˇ
𝜙 1 𝜙 1
ˇ ˇ
ˇ ˇ
“ ˇ𝔼 𝑦„𝑃 𝑦p𝑐 ´ 𝜙p𝑦qq2 ´𝔼 𝑦„𝑈r0,1sp𝑐 ´ 𝜙p𝑦qq2ˇ
› ›
ď
2›
p𝑐 ´
𝜙p𝑦qq2›
¨TVp𝑃 𝑦,𝑈r0,1sq
8
ď 16expp´2𝜋2𝛾2 q ď 𝑜ppolyp𝑑q´1 q ď 𝜀{5, (4.5)
where 𝑃 𝑦 denotesthedistributionofp𝛾x𝑤,𝑥y mod 1qfor 𝑥 „ 𝑁p0,𝐼 𝑑q,thesecondinequalityis
from[SZB21,ClaimI.6],andthelastinequalityisbecause 𝜀 “ polyp𝑑q´1. Combining(4.3),(4.4),
and(4.5)wegetthatwithprobabilityatleast2{3´0.01,
𝐿 𝑃 pℎq ď 𝐿 𝑃 pℎq` 𝜀{5 ď 𝐿 𝑃 p𝔼r𝑓 𝛾,𝑤p𝑥qsq´4𝜀{5
1 𝜙 𝜙
ď 𝐿 𝑃 p𝑐q´4𝜀{5 ď 𝐿 𝑄 p𝑐q´3𝜀{5 ď 𝐿 𝑄 pℎq´3𝜀{5, (4.6)
𝜙 1 1
where the third inequality is from the optimality of 𝔼r𝑓 𝛾,𝑤p𝑥qs among constant predictors for
𝑃 ,andthelastinequalityisfromtheoptimalityof 𝑐 amongallpredictorsfor 𝑄 .
𝜙 1
Usingtheremaining𝑚 2 samplesp𝑥 𝑖,𝑧 𝑖qfrom𝑃 0,andthenewlygen řerated𝑚 2`samplesp𝑥1 𝑖,𝑦 𝑖1 ˘q
from𝑄 0,𝑖 “ 𝑚 1` ř1,...,𝑚,c `omputetheem ˘piricallosses𝐿ˆ 𝑃 1pℎq “ 𝑚1
2
𝑚 𝑖“𝑚 1`1ℓ ℎp𝑥 𝑖q,𝐹 𝜉 𝑖p𝑧 𝑖q
and𝐿ˆ 𝑄 1pℎq “ 𝑚1
2
𝑚 𝑖“𝑚 1`1ℓ ℎp𝑥1 𝑖q,𝐹 𝜉 𝑖p𝑦 𝑖1q . Weknowthat|ℎp𝑥q´𝜙p𝑦q| ď 2,and𝜉isGaussian
withvariance 𝜎2 ă 1. Hence ℎp𝑥q´𝐹 𝜉p𝑦qissub-Gaussianwithsomeabsoluteconstantparam-
eter. Thereforeℓpℎp𝑥q,𝐹 𝜉p𝑦qqissub-exponentialwithsomeabsoluteconstantparameters. Then
byBernstein’sinequality,|𝐿ˆ 𝐷 pℎq´𝐿 𝑃 pℎq| ď 𝜀{5and|𝐿ˆ 𝑄 pℎq´𝐿 𝑄 pℎq| ď 𝜀{5,bothwithprob-
1 1 1 1
ability at least 1´expp´mintΩp𝑚 2𝜀2 q,Ωp𝑚 2𝜀quq which is 1´ 𝑜p1q, as long as 𝑚 2 “ 𝜔p𝜀´2 q,ONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 11
which can be polyp𝑑q. Using these concentration bounds together with (4.6) we get that with
probabilityatleast2{3´0.01´ 𝑜p1q,itholdsthat
𝐿ˆ 𝐷 pℎq ď 𝐿 𝑃 pℎq` 𝜀{5 ď 𝐿 𝑄 pℎq´2𝜀{5 ď 𝐿ˆ 𝑄 pℎq´ 𝜀{5
1 1 1 1
hence ℎ willpassthetestofthestep6ofalgorithm ℬ and ℬ willreturnthecorrectanswer.
Case II: D “ Q 0. In this case 𝐷 1 “ 𝑄 1. Using 𝑚 2 “ 𝜔p𝜀´2 q large enough and applying Bern-
stein’sinequalityagainwegetthat|𝐿ˆ 𝐷 pℎq´𝐿 𝑄 pℎq| ď 𝜀{20and|𝐿ˆ 𝑄 pℎq´𝐿 𝑄 pℎq| ď 𝜀{20,both
1 1 1 1
withprobabilityatleast1´expp´Ωp𝑚 2𝜀2 qq “ 1´𝑜p1q. Whichmeansthat|𝐿ˆ 𝐷 pℎq´𝐿ˆ 𝑄 pℎq| ď
1 1
𝜀{10 with probability at least 1´ 𝑜p1q. Hence, 𝐿ˆ 𝐷 pℎq ą 𝐿ˆ 𝑄 pℎq´ 𝜀{5 and the test in step 6 of
1 1
thealgorithm ℬ fails.
In both cases, the test correctly concludes 𝐷 “ 𝑃 0 or 𝐷 “ 𝑄 0, by using the empirical loss
𝐿ˆ
𝐷
pℎqandcomparingittothevalue 𝐿ˆ
𝑄
pℎq´ 𝜀{5. □
1 1
5. TheCryptographicHardnessofLearningOneHiddenLayerNeuralNetworks
In this section we complete the proof of Theorem 3.1. To do this we construct a family of one
hiddenlayerneuralnetworkswithpolynomialsizethatis1-Lipschitzand1-periodicoverafinite
ranger´𝑅,𝑅s. Becauseourinput 𝑥 isGaussian,andhencehas“light”tails,weshowthatthisis
enoughtoapplyourTheorem4.2fromtheprevioussectionandconcludeourhardnessresult.
Webeginbyremindingthereadertheclass ℱNN ofonehiddenlayerneuralnetworksdefined
𝑘
in(3). Letusconsiderthefollowingfunction,
#
ˇ Z ^ˇ
ˇ ˇ 3 1 ˇ ˇ 1 𝑥 ´ 𝑘, 𝑥 P r𝑘 ´1{4,𝑘 `1{4s,
𝜙p𝑥q “ ˇ𝑥 ´ ´ 𝑥 ´ ˇ´ “ 𝑘 P ℤ,
4 4 4 1{2´p𝑥 ´ 𝑘q, 𝑥 P r𝑘 `1{4,𝑘 `3{4s,
which is 1-periodic, 1-Lipschitz, and |𝜙p𝑥q| ď 1{4 for all 𝑧 P ℝ (see Figure 1). The following
lemma shows that it interestingly coincides with an one hidden layer neural network on some
interval.
Lemma5.1. For 𝑅 P ℕ,let
ˆ ˙ ˆ ˙
ÿ2𝑅
1 3
nnp𝑥q “ p𝑥 `𝑅q ´p𝑥 ´𝑅q `2 𝑥 `𝑅 ` ´ 𝑘 ´ 𝑥 `𝑅 ` ´ 𝑘 .
` ` 4 4
𝑘“1 ` `
Then,nnp𝑥q “ 𝜙p𝑥q¨1t𝑥 P r´𝑅,𝑅su.
Proof. For 𝑥 ď ´𝑅, all the ReLU functions evaluate to 0, and nnp𝑥q ř“ 0. `For 𝑥 ě 𝑅, all ˘the
R `eLU functions ˘evaluate to id, and nnp𝑥q “ p𝑥 ` 𝑅q ´ p𝑥 ´ 𝑅q ` 2 2 𝑘𝑅
“1
𝑥 `𝑅 ` 41 ´ 𝑘 ´
𝑥 `𝑅 ` 3 ´ 𝑘 “ 2𝑅 ´2𝑅 “ 0. The interesting case is of course when 𝑥 P r´𝑅,𝑅s. Observe
4
thatfor 𝑘 “ 1,2,...,2𝑅,
$
ˆ ˙ ˆ ˙ ’ &0, 𝑥 ă ´𝑅 ´3{4` 𝑘,
1 3
𝑥 `𝑅 ` ´ 𝑘 ´ 𝑥 `𝑅 ` ´ 𝑘 “ ´1{2, 𝑥 ą ´𝑅 ´1{4` 𝑘,
4 4 ’
%
` ` ´𝑥 ´𝑅 ´3{4` 𝑘, otherwise.
If𝑥 P r𝑘´1{4,𝑘`1{4sforsome 𝑘 P ℤ,thennnp𝑥q “ p𝑥`𝑅q`2p´1{2qp𝑅`𝑘q “ 𝑥´𝑘 “ 𝜙p𝑥q.
If 𝑥 P r𝑘 `1{4,𝑘 `3{4s for some 𝑘 P ℤ, then nnp𝑥q “ p𝑥 ` 𝑅q`2p´𝑥 ´ 𝑅 ´3{4` 𝑅 ` 𝑘 `
1q`2p´1{2qp𝑅 ` 𝑘q “ 1{2´p𝑥 ´ 𝑘q “ 𝜙p𝑥q. □12 S.LI,I.ZADIK,M.ZAMPETAKIS
ϕ(x) nn(x)
0.25
Out[]=
-6 -5 -4 -3 -2 -1 1 2 3 4 5 6
-0.25
Figure1. 𝜙p𝑥qandnnp𝑥qfor 𝑅 “ 3
We next consider an ainstantiation of nn from Lemma 5.1 that coincides with 𝜙 on r´𝑅,𝑅s
for arbitrary 𝑅 “ 𝜔p𝛾 log𝑑q. Observe that nn which takes a single input, one hidden layer
neural network withp4𝑅 `2q ReLU neurons. We can then define the multivariate version of nn
as NNp𝑥q “ nnp𝛾x𝑤,𝑥yq, which is also an one hidden layer neural network. This way we can
definethefollowingsubclassofonehiddenlayerneuralnetworks ℱNN givenby,
𝑘
! )
ℱNN “ 𝑓p𝑥q “ nnp𝛾x𝑤,𝑥yq | 𝑤 P ℝ𝑑,𝛾 P ℝ , (5.1)
𝑅
which contains one hidden layer neural networks with width 𝑂p𝑅q. Recall from the previous
sectionthat,for𝑥 „ 𝑁p0,𝐼 𝑑qand𝜉 „ 𝑁p0,𝜎2 q,𝑃 𝜙 denotesthedistributionofp𝑥,𝜙p𝛾x𝑤,𝑥yq`
𝜉q, which is the input for learning the periodic function 𝜙. Let 𝑃 denote the distribution of
NN
p𝑥,NNp𝑥q ` 𝜉q, which is the input for learning the one hidden layer neural network NN. We
next show that samples generated from 𝑃 are essentially the same as samples generated from
𝜙
𝑃 .
NN
Le´mma 5.2. For¯𝑅 P ℕ, the total variance distance abetween 𝑃 𝜙 and 𝑃 NN is upper bounded by
𝑂
𝛾expp´𝑅2{2𝛾2q
. When 𝛾 “ polyp𝑑q, and 𝑅 “ 𝛾 𝜔plog𝑑q`2logp1{𝜎q, the total variation
𝜎𝑅
distanceis
𝑂ppolyp𝑑qexpp´𝜔plog𝑑qqq “ 𝑑´𝜔p1q.
Proof. Notethatconditioningon𝑥,𝑧 1 “ 𝜙p𝛾x𝑤,𝑥yq`𝜉and𝑧 2 “ nnp𝛾x𝑤,𝑥yq`𝜉areGaussians
withmean 𝜙p𝛾x𝑤,𝑥yqandnnp𝛾x𝑤,𝑥yq,andvariance 𝜎2. Thus,
„ ȷ
|𝜙p𝛾x𝑤,𝑥yq´nnp𝛾x𝑤,𝑥yq|
TVp𝑃 𝜙,𝑃 NNq ď 𝔼 𝑥„𝑁p0,𝐼 𝑑qrTVp𝑧 1|𝑥,𝑧 2|𝑥qs ď 𝔼 𝑥„𝑁p0,𝐼 𝑑q 2𝜎
1 1
“ 2𝜎𝔼 𝑥„𝑁p0,1qr|𝜙p𝛾𝑥q´nnp𝛾𝑥q|s ď 8𝜎P𝑥„𝑁p0,1qr|𝑥| ě 𝑅{𝛾s
ˆ ˙
1 expp´𝑅2 {2𝛾2 q 𝛾expp´𝑅2 {2𝛾2 q
ď ? ď 𝑂 .
4𝜎 2𝜋𝑅{𝛾 𝜎𝑅
□
FromLemmas5.1and5.2wehavethatthereexistsasubclass ℱNN ofonehiddenlayerneural
𝑅
𝜙
networks that produces the same polynomially-many samples as the class ℱ for this carefully
𝛾ONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 13
picked1-Lipschitzand1-periodicfunction 𝜙. Wenextshowthatinfactalearningalgorithmfor
𝜙
ℱNN impliesalearningalgorithmfor ℱ .
𝑅 𝛾
Lemma 5.3. Let 𝑑 P ℕ, 𝛾 “ polyp𝑑q, 𝜎 “a𝜎p𝑑q P p0,1q, 𝜀 “ polyp𝑑q´1 . Moreover, let
𝜙p𝑥q “ |𝑥 ´3{4´t𝑥 ´1{4u|´1{4,and𝑅 “ 𝛾 𝜔plog ␣𝑑q`2logp1{𝜎q. Thenapolynomial-tim(e
algorithmthat𝜀-weaklylearnsthefunctionclassℱ 𝑅NN “ 𝑓 𝑤p𝑥q “ nnp𝛾x𝑤,𝑥yq | 𝑤 P ℝ𝑑,𝛾 P ℝ
over Gaussian inputs 𝑥 i. „i.d. 𝑁p0,𝐼 𝑑q under Gaussian noise 𝜉 i. „i.d. 𝑁p0,𝜎2 q implies a polynomial-
timealgorithmthat 𝜀 2-weaklylearnsthefunctionclassℱ 𝛾𝜙 “ t𝑓 𝛾,𝑤p𝑥q “ 𝜙p𝛾x𝑤,𝑥yq | 𝑤 P 𝑆𝑑´1 u
overthesameinputandnoisedistribution.
Proof. Let nn be athe function in Lemma 5.1 that coincides with 𝜙 on r´𝑅,𝑅s for arbitrary 𝑅
satisfying𝑅 “ 𝛾 𝜔plog𝑑q`2logp1{𝜎q. Then,letNNp𝑥q “ nnp𝛾x𝑤,𝑥yq,andthusNN P ℱNN.
𝑅
For 𝜀 “ polyp𝑑q´1, and 𝒜 be a polynomial-time learning algorithm that takes as input 𝑚 “
polyp𝑑q samples from 𝑃 and with probability 2{3 outputs a hypothesis ℎ1 : ℝ Ñ ℝ such that
NN
˜
𝐿 𝑃 pℎ1q ď 𝐿 𝑃 p𝔼rNNp𝑥qsq ´ 𝜀. Since we are using the squared loss, ℎp𝑥q “ sgnpℎ1p𝑥qq ¨
NN NN
maxp|ℎ1p𝑥q|,1{4q is always no worse than ℎ1p𝑥q, as NNp𝑥q P r´1{4,1{4s and the noise on the
labelisunbiased. Thus,wecanassumewithoutlossofgeneralitythat ℎ1p𝑥q P r´1{4,1{4s.
𝜙
To learn the function class ℱ 𝛾 given 𝑚 “ polyp𝑑q samples from 𝑃 𝜙, run 𝒜 directly on these
samples,whichgives ℎ,andoutput ℎ. Similarly,bythedataprocessinginequality,
TVpℎ,ℎ1q ď TVp𝑃 𝜙b𝑚,𝑃 Nb N𝑚 q ď 𝑚 ¨TVp𝑃 𝜙,𝑃 NNq.
By Lemma 5.2, this is upper bounded by 𝑚 ¨ 𝑑´𝜔p1q “ 𝑑´𝜔p1q ă 0.01. Thus, with probability
at least 2{3 ´ 0.01, we have 𝐿 𝑃 pℎq ď 𝐿 𝑃 p𝔼rNNp𝑥qsq ´ 𝜀. Since 𝔼rNNp𝑥qs is the optimal
NN NN
constantpredictorfor𝑃 NN,wehave𝐿 𝑃 NNpℎq ď 𝐿 𝑃 NNp𝔼rNNp𝑥qsq´𝜀 ď 𝐿 𝑃 NNp𝔼r𝜙p𝛾x𝑤,𝑥yqsq´𝜀.
SimilarlytotheproofofTheorem4.2,compute
ˇ ˇ
|𝐿 𝑃 NNpℎq´𝐿 𝑃 𝜙pℎq| “ ˇ 𝔼 𝑥„𝑁p0,𝐼 𝑑qpℎp𝑥q´NNp𝑥qq2 ´𝔼 𝑥„𝑁p0,𝐼 𝑑qpℎp𝑥q´ 𝜙p𝛾x𝑤,𝑥yqq2ˇ
“ |𝔼 𝑥rpNNp𝑥q´ 𝜙p𝛾x𝑤,𝑥yqqpNNp𝑥q` 𝜙p𝛾x𝑤,𝑥yq´2ℎp𝑥qqs|
ď 𝔼 𝑥|NNp𝑥q´ 𝜙p𝛾x𝑤,𝑥yq|.
We know from the proof of Lemma 5.2 that 𝔼 𝑥|NNp𝑥q ´ 𝜙p𝛾x𝑤,𝑥yq| ď 𝑑´𝜔p1q ď 𝜀{4. Thus,
|𝐿 𝑃 pℎq´𝐿 𝑃 pℎq| ď 𝜀{4. Let 𝑐 “ 𝔼r𝜙p𝛾x𝑤,𝑥yqs P r´1{4,1{4s. Thenbythesameargument,
NN 𝜙
ˇ ˇ
|𝐿 𝑃 NNp𝑐q´𝐿 𝑃 𝜙p𝑐q| “ ˇ 𝔼 𝑥„𝑁p0,𝐼 𝑑qp𝑐 ´NNp𝑥qq2 ´𝔼 𝑥„𝑁p0,𝐼 𝑑qp𝑐 ´ 𝜙p𝑥qq2ˇ
“ |𝔼 𝑥rpNNp𝑥q´ 𝜙p𝛾x𝑤,𝑥yqqpNNp𝑥q` 𝜙p𝛾x𝑤,𝑥yq´2𝑐qs|
ď 𝔼 𝑥|NNp𝑥q´ 𝜙p𝛾x𝑤,𝑥yq| ď 𝜀{4.
Therefore,withprobabilityatleast2{3´0.01,wehave𝐿 𝑃 𝜙pℎq ď 𝐿 𝑃 NNpℎq`𝜀 4 ď 𝐿 𝑃 NNr𝔼r𝜙p𝛾x𝑤,𝑥yqss´
3 4𝜀 ď 𝐿 𝑃 𝜙r𝔼r𝜙p𝛾x𝑤,𝑥yqss´ 𝜀 2. □
𝜙
Thefinalstepistocombinethiswiththehardnessoflearning ℱ fromTheorem4.2withthe
𝛾
𝜙
equivalence of learning ℱ and ℱNN to get the following result, which directly implies Theo-
𝛾 𝑅
rem3.1.14 S.LI,I.ZADIK,M.ZAMPETAKIS
a
Theorem 5.4. Let 𝑑 P ℕ, 𝜎 “ polyp𝑑q´1 , 𝜀 “ polyp𝑑q´1 , and 𝑅 “ 𝜔p 𝑑log𝑑q. Then a
polynomial-timealgorithmthat𝜀-weaklylearnsthefunctionclassℱNN,definedin(5.1)overGauss-
𝑅
ianinputs𝑥 i. „i.d. 𝑁p0,𝐼 𝑑qunderGaussiannoise𝜉 i. „i.d. 𝑁p0,𝜎2 qimpliesapolynomial-timequantum
algorithmthatapproximatesGapSVPtowithinpolynomialfactors.
?
Proof. Let 𝛾 “ 2 𝑑 and 𝜙p𝑥q “ |𝑥 ´ 3{4 ´ t𝑥 ´1{4u| ´ 1{4, which is 1-Lipschitz, 1-periodic,
and 𝜙p𝑥q P r´1{4,1{4s for all 𝑥 P ℝ. Then by Lemma 5.3, there is a polynomial-time algorithm
that 𝜀 2-weakly learns the function class ℱ 𝛾𝜙 “ t𝑓 𝛾,𝑤p𝑥q “ 𝜙p𝛾x𝑤,𝑥yq | 𝑤 P 𝑆𝑑´1 u over the
same input and noise distribution. Then by Corollary 4.3, there is a polynomial-time quantum
algorithmthatapproximatesSVPtowithinpolynomialfactors. □
6. Super-PolynomiallySmallNoise
In this section we show that our lower bound holds even if we make the noise negligible, i.e.,
smallerthananyinversepolynomialin 𝑑. Eveninthisverylownoiseregime,anyalgorithmfor
learning1-hiddenlayerneuralnetworkswithGaussianinputimpliesabreakthroughincryptog-
raphyandalgorithmictheoryoflattices. Wemakethisclaimprecisebelow.
Ifweremovetherestrictionof 𝜎 “ polyp1{𝑑qinTheorem5.4,thenusingthesameoutlineofthe
proofwecanshowthefollowinglemmathatreduceslearning1-hiddenlayerneuralnetworksto
CLWEevenwhen 𝛽 isnegligible.
a
Lemma 6.1a. Let 𝑑 P ℕ, 𝛾 “ polyp𝑑q with 𝛾 “ 𝜔p log𝑑q, 𝜎 “ 𝜎p𝑑q P p0,1q, 𝜀 “ polyp𝑑q´1 ,
and 𝑅 “ 𝛾 𝜔plog𝑑q`2logp1{𝜎q. Then a polynomial-time algorithm that 𝜀-weakly learns the
function class ℱ 𝑅NN, defined in (5.1) over Gaussian inputs 𝑥 i. „i.d. 𝑁p0,𝐼 𝑑q under Gaussian noise
𝜉 i. „i.d. 𝑁p0,𝜎2 qimpliesapolynomial-timealgorithmforCLWE 𝛽,𝛾 forany 𝛽 ď 𝜎{polyp𝑑q.
Proof. Let 𝜙p𝑥q “ |𝑥 ´ 3{4 ´ t𝑥 ´1{4u| ´ 1{4, which is 1-Lipschitz, 1-periodic, and 𝜙p𝑥q P
r´1{4,1{4s for all 𝑥 P ℝ. Then by Lemma 5.3, there is a polynomial-time algorithm that 𝜀-
2
weaklylearnsthefunctionclass ℱ 𝛾𝜙 “ t𝑓 𝛾,𝑤p𝑥q “ 𝜙p𝛾x𝑤,𝑥yq | 𝑤 P 𝑆𝑑´1 uoverthesameinput
and noise distribution. Then by Theorem 4.2, there is a polynomial-time algorithm for CLWE
𝛽,𝛾
forany 𝛽 ď 𝜎{polyp𝑑q. □
Therefore an algorithm for learning 1-hidden layer neural network implies that we can solve
CLWE
𝛽,𝛾
as long as 𝛽 is smaller than 𝜎{polyp𝑑q. The next step is to connect an algorithm for
CLWE to an algorithm for worst-case lattice problems even when 𝛽 is negligible. For the
𝛽,𝛾
case 𝛽 ě polyp𝑑q´1 we used the CLWE hardness fromxs Theorem 2.4x, which requires 𝛾{𝛽 “
polyp𝑑q. Nevertheless, we can bypass this condition by using the following recent theorem that
reduces classical LWE to CLWE from [GVV22], together with the stronger quantum reduction
fromworst-caselatticeproblemtoLWEdueto[Reg05].
Theorem6.2([GVV22,Corollary5]). Let𝑑,𝑛,𝑞 P ℕ, 𝛾,𝛽,𝜎1 ą 0. Thenforsomeconstant𝑐 ą 0,
apolynomial-timealgorithmforCLWE𝛽,𝛾 indimension 𝑑 impliesapolynomial-timealgorithmfor
LWE 𝑞,𝐷 indimension 𝑛,for
ℤ,𝜎1 ˜ ¸
?
a 𝜎1 𝑑
𝛾 “ 𝜔p 𝑑log𝑑q, 𝛽 ě 𝑐 ,
𝑞ONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 15
a
aslongaslogp𝑞q{2𝑛 “ 𝑜ppolyp𝑛q´1 q, 𝑑 ě 2𝑛log𝑞 ` 𝜔plog𝑛q,and 𝜎1 ě 𝜔p log𝑛q.
Theorem 6.3 ([Reg05, Theorem 3.1, Lemma 3.20]). Let 𝑛,𝑞 P ℕ, 𝛼 P p0,1q such that 𝛼𝑞 ą
?
2 𝑛. Then apolynomial-time algorithmforLWE 𝑞,𝐷 in dimension 𝑛 impliesa polynomial-time
ℤ,𝛼𝑞
quantumalgorithmfor 𝑂p𝑛{𝛼q-GapSVPindimension 𝑛.
If we combine these two results we get the following reduction from GapSVP to CLWE even
forsuper-polynomiallysmall 𝛽.
Corollary6.4. Let 𝑑,𝑛 P ℕ, 𝛾 ą 0,𝛽 P p0,1q. Thenapolynomial-timealgorithmforCLWE𝛽,𝛾 in
?
dimension 𝑑 aimplies a polynomial-time quantum algorithm for 𝑂p𝑛 𝑑{𝛽q-GapSVP in dimension
𝑛,if 𝛾 “ 𝜔p 𝑑log𝑑q,logp1{𝛽q ď polyp𝑛q,and3𝑛logp𝑑{𝛽q ď 𝑑 ď polyp𝑛q.
?
Proof. Fortheconstant 𝑐 ą 0fromTheorem6.2,let 𝛼 “ 𝑐´1𝛽{ 𝑑, 𝑞 “ 2𝑑{𝛽, 𝜎1 “ 𝛼𝑞.
?
Indeed, 𝛾 directlysatisfiesthesameassumption, 𝛽 satisfies 𝛽 ě 𝑐𝜎1 𝑑{𝑞, and 𝑞 satisfies
logp𝑞q “ logp2𝑑{𝛽q “ 𝑂plog𝑛 `logp1{𝛽qq ď polyp𝑛q “ 𝑜p2𝑛 ppolyp𝑛qq´1 q
and 𝑑 satisfies
𝑑 ě 3𝑛logp𝑑{𝛽q ě 2𝑛logp𝑞q` 𝜔plog𝑛q.
? a ? a ?
Finally,alsoclearly 𝜎1 “ 𝑐´1 𝑑 “ 𝜔p log𝑛qand 𝛼𝑞 “ 𝑐´1 𝑑 ě 𝑐´1 3𝑛logp𝑑{𝛽q ą 2 𝑛.
Then by Theorem 6.2, there is a polynomial-time algorithm for LWE in dimension 𝑛.
𝑞,𝐷
? ℤ,𝛼𝑞 ?
Further, since 𝛼𝑞 ą 2 𝑛, by Theorem 6.3, there is a polynomial-time algorithm for 𝑂p𝑛 𝑑{𝛽q-
GapSVPalgorithm. □
Finally,wecanuseCorollary6.4insteadofTheorem2.4intheproofofTheorem5.4togetthe
followingresult.
Theorema6.5. Let 𝑛 P ℕ, 𝜎 ě 𝑒´polyp𝑛q, Ωp𝑛logp𝑛{𝜎qq ď 𝑑 ď polyp𝑛q, 𝜀 “ polyp𝑑q´1 , and
𝑅 “ 𝜔p 𝑑log𝑑 ¨logp𝑑{𝜎qq. Thenapolynomial-timealgorithmthat 𝜀-weaklylearnsthefunction
classℱ 𝑅NN,definedin(5.1)overGaussianinputs𝑥 i. „i.d. 𝑁p0,𝐼 𝑑qunderGaussiannoise𝜉 i. „i.d. 𝑁p0,𝜎2 q
impliesapolynomial-timequantumalgorithmforppolyp𝑛q{𝜎q-GapSVPindimension 𝑛.
a
Proof. Let 𝛽 “ 𝜎{polyp𝑑q, 𝛾 “ 𝜔p 𝑑log𝑑q, then by Lemma 6.1, there is a polynomial-time
algorithm for CLWE . By Corollary 6.4, there is a polynomial-time quantum algorithm for
𝛽,𝛾
?
GapSVPwithfactor 𝑂p𝑛 𝑑{𝛽q “ polyp𝑛q{𝜎 indimension 𝑛. □
Bychoosing 𝜎 “ 2´𝑑𝜂 forconstant𝜂 P p0,1{2q,wecangetthefollowingcorollary.
a
Corollary6.6. Forconstant𝜂 P p0,1{2q,let 𝜎 “ 2´𝑑𝜂 , 𝜀 “ polyp𝑑q´1 ,and𝑅 “ 𝜔p 𝑑1`𝜂log𝑑q.
Thenfor 𝑛 P ℕwith 𝑛 “ Θp𝑑1´𝜂 q,apolynomial-timealgorithmthat 𝜀-weaklylearnsthefunction
classℱ 𝑅NN,definedin(5.1)overGaussianinputs𝑥 i. „i.d. 𝑁p0,𝐼 𝑑qunderGaussiannoise𝜉 i. „i.d. 𝑁p0,𝜎2 q
𝜂
impliesapolynomial-timequantumalgorithmfor2𝑂p𝑛1´𝜂q-GapSVPindimension 𝑛.
Proof. Since𝜂 isaconstant,indeedwehave 𝜎 ě 𝑒´polyp𝑛q,
𝑑 “ Θp𝑛1´1 𝜂q ě Ωp𝑛 ¨𝑛1´𝜂 𝜂q ě Ωp𝑛𝑑𝜂 q “ Ωp𝑛logp𝑛{𝜎qq,
a
and 𝑅 “ 𝜔p 𝑑log𝑑 ¨logp𝑑{𝜎qq. Thus, from Theorem 6.5, there is a polynomial-time quantum
𝜂
algorithmforGapSVPwithfactorpolyp𝑛q{𝜎 “ 2𝑂p𝑛1´𝜂q indimension 𝑛. □16 S.LI,I.ZADIK,M.ZAMPETAKIS
According to Corollary 6.6, Theorem 6.5 shows an interesting hardness result even when 𝜎 is
super-polynomiallysmall. Tomaketheconnectionmoreprecise,observethatforany 𝛿 P p0,1q,
if we set 𝜂 :“ 𝛿 P p0,1{2q, and 𝜎 “ 2´𝑑𝜂, then, due to Corollary 6.6, the existence of a
1`𝛿
polynomial-time algorithm that weakly learns one hidden layer neural networks with polyno-
mial width under Gaussian noise with only 2´𝑑𝜂 standard deviation, implies a polynomial-time
quantum algorithm for 2𝑂p𝑛𝛿q-GapSVP in dimension 𝑛 — a problem which is considered hard
in cryptography and algorithmic theory of lattices. We remind the reader, that the main reason
behindthisconjecturedhardnessisthatthestate-of-the-art(since1982)powerfulLLLalgorithm
forGapSVPisonlyabletoachievean2Θp𝑛q-approximation,andanyimprovementonitwouldbe
consideredamajorbreakthrough. Wealsohighlightthatanysuchalgorithmwouldbreakinfact
severalbreakthroughcryptographicconstructionssuchastherecentcelebratedresultof[JLS21].
7. Conclusions
Inthispaper,weprovedthehardnessoflearningonehiddenlayerneuralnetworkswithwidth
a
𝜔p 𝑑log𝑑qunderGaussianinputandanyinverse-polynomiallysmallGaussiannoise,assuming
the hardness of GapSVP with polynomial factors. En route, we proved the hardness of learning
Lipschitz periodic functions under Gaussian input and any inverse-polynomially small Gauss-
ian noise. This improves a similar result from [SZB21], which proved the hardness for inverse-
polynomiallysmalladversarial noise.
Moreover,ifweassumethehardnessof2𝑂p𝑑𝛿q-GapSVPfor 𝛿 P p0,1q,wealsogetthehardness
oflearningonehiddenlayerneuralnetworkswithpolynomialwidthunderGaussiannoisewith
2´𝑑𝜂 variance,where𝜂 “ 𝛿 P p0,1{2q.
1`𝛿
References
[And+17] AlexandrAndoni,DanielHsu,KevinShi,andXiaoruiSun.Correspondenceretrieval.
ConferenceonLearningTheory.PMLR.2017,pp.105–126(cit.onp.4).
[And+19] AlexandrAndoni,RishabhDudeja,DanielHsu,andKiranVodrahalli.Attribute-efficient
learningofmonomialsoverhighly-correlatedvariables.AlgorithmicLearningTheory.
PMLR.2019,pp.127–161(cit.onp.2).
[ATV21] PranjalAwasthi,AlexTang,andAravindanVijayaraghavan.Efficientalgorithmsfor
learning depth-2 neural networks with general relu activations. Advances in Neural
InformationProcessingSystems 34(2021),pp.13485–13496(cit.onp.2).
[AZLL18] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in
overparameterized neural networks, going beyond two layers. 2018. arXiv: 1811.
04918(cit.onp.2).
[Bac17] Francis Bach. Breaking the curse of dimensionality with convex neural networks. J.
Mach.Learn.Res.18:1(2017),629–681(cit.onp.2).
[BJW19] Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified
neural networks in polynomial time. Conference on Learning Theory. PMLR. 2019,
pp.195–268(cit.onp.2).
[Bru+17] AlonBrutzkus,AmirGloberson,EranMalach,andShaiShalev-Shwartz.SGDlearns
over-parameterized networks that provably generalize on linearly separable data
(2017)(cit.onp.2).REFERENCES 17
[Bru+21] Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous LWE. Proceedings
of the 53rd Annual ACM SIGACT Symposium on Theory of Computing. 2021 (cit. on
pp.3,5,6).
[Che+22] SitanChen,AravindGollakota,AdamRKlivans,andRaghuMeka.HardnessofNoise-
FreeLearningforTwo-Hidden-LayerNeuralNetworks.arXivpreprintarXiv:2202.05258
(2022)(cit.onpp.2,3).
[CKM22] SitanChen,AdamRKlivans,andRaghuMeka.Learningdeeprelunetworksisfixed-
parameter tractable. 2021 IEEE 62nd Annual Symposium on Foundations of Computer
Science(FOCS).IEEE.2022,pp.696–707(cit.onp.2).
[Dam+24] Alex Damian, Loucas Pillaud-Vivien, Jason Lee, and Joan Bruna. Computational-
Statistical Gaps in Gaussian Single-Index Models. The Thirty Seventh Annual Con-
ferenceonLearningTheory.PMLR.2024,pp.1262–1262(cit.onp.2).
[Dia+20] Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, and Nikos Zarifis. Algorithms
andsqlowerboundsforpaclearningone-hidden-layerrelunetworks.Conferenceon
LearningTheory.PMLR.2020,pp.1514–1539(cit.onp.2).
[DK22] IliasDiakonikolasandDanielKane.Non-gaussiancomponentanalysisvialatticeba-
sisreduction.ConferenceonLearningTheory.PMLR.2022,pp.4535–4547(cit.onp.4).
[DV21] Amit Daniely and Gal Vardi. From local pseudorandom generators to hardness of
learning.ConferenceonLearningTheory.PMLR.2021,pp.1358–1394(cit.onp.3).
[GKZ21] DavidGamarnik,ErenCKızıldag˘,andIliasZadik.Inferenceinhigh-dimensionallin-
ear regression via lattice basis reduction and integer relation detection. IEEE Trans-
actionsonInformationTheory 67:12(2021),pp.8109–8139(cit.onp.4).
[GLM17] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks
withlandscapedesign.2017.arXiv:1711.00501(cit.onp.2).
[Goe+20] Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam Klivans.
Superpolynomial lower bounds for learning one-layer neural networks using gra-
dient descent. International Conference on Machine Learning. PMLR. 2020, pp. 3587–
3596(cit.onp.2).
[GVV22] AparnaGupte,NeekonVafa,andVinodVaikuntanathan.Continuouslweisashardas
lwe&applicationstolearninggaussianmixtures.2022IEEE63rdAnnualSymposium
on Foundations of Computer Science (FOCS). IEEE. 2022, pp. 1162–1173 (cit. on pp. 3,
14).
[HR07] IshayHavivandOdedRegev.Tensor-BasedHardnessoftheShortestVectorProblem
to within Almost Polynomial Factors. Proceedings of the Thirty-Ninth Annual ACM
SymposiumonTheoryofComputing.2007,469–477(cit.onp.5).
[JLS21] AayushJain,HuijiaLin,andAmitSahai.Indistinguishabilityobfuscationfromwell-
founded assumptions. Proceedings of the 53rd Annual ACM SIGACT Symposium on
TheoryofComputing.2021,pp.60–73(cit.onp.16).
[JSA15] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-
convexity:Guaranteedtrainingofneuralnetworksusingtensormethods.arXivpreprint
arXiv:1506.08473 (2015)(cit.onp.2).
[Kho05] Subhash Khot. Hardness of Approximating the Shortest Vector Problem in Lattices.
J.ACM 52:5(2005),789–808(cit.onp.5).18 REFERENCES
[KS09] AdamRKlivansandAlexanderASherstov.Cryptographichardnessforlearningin-
tersectionsofhalfspaces.JournalofComputerandSystemSciences75:1(2009),pp.2–
12(cit.onp.1).
[LLL82] Arjen Klaas Lenstra, Hendrik Willem Lenstra, and La´szlo´ Lova´sz. Factoring polyno-
mials with rational coefficients. Mathematische Annalen 261: 4 (1982), pp. 515–534
(cit.onp.4).
[MR09] DanieleMicciancioandOdedRegev.Lattice-basedCryptography.Post-QuantumCryp-
tography. Berlin, Heidelberg: Springer Berlin Heidelberg, 2009, pp. 147–191 (cit. on
pp.3,5).
[Reg05] Oded Regev. On lattices, learning with errors, random linear codes, and cryptogra-
phy.STOC.2005,pp.84–93(cit.onpp.14,15).
[SZB21] MinJaeSong,IliasZadik,andJoanBruna.OntheCryptographicHardnessofLearn-
ing Single Periodic Neurons. Advances in Neural Information Processing Systems 34:
Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, De-
cember 6-14, 2021, virtual. Ed. by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.
Dauphin, Percy Liang, and Jennifer Wortman Vaughan. 2021, pp. 29602–29615 (cit.
onpp.2–4,6,7,10,16).
[Zad+22] IliasZadik,MinJaeSong,AlexanderSWein,andJoanBruna.Lattice-basedmethods
surpass sum-of-squares in clustering. Conference on Learning Theory. PMLR. 2022,
pp.1247–1248(cit.onp.4).
[Zar+24] Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, and Jelena Diakonikolas. Robustly
LearningSingle-IndexModelsviaAlignmentSharpness.Forty-firstInternationalCon-
ferenceonMachineLearning.2024(cit.onp.2).
[ZG18] Ilias Zadik and David Gamarnik. High dimensional linear regression using lattice
basis reduction. Advances in Neural Information Processing Systems 31 (2018) (cit. on
p.4).
[Zha+19] Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-
layerrelunetworksviagradientdescent.The22ndinternationalconferenceonartificial
intelligenceandstatistics.PMLR.2019,pp.1524–1534(cit.onp.2).
[Zho+17] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recov-
eryguaranteesforone-hidden-layerneuralnetworks.Internationalconferenceonma-
chinelearning.PMLR.2017,pp.4140–4149(cit.onp.2).