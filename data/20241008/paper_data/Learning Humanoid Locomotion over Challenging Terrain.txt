Learning Humanoid Locomotion
over Challenging Terrain
Ilija Radosavovic, Sarthak Kamat, Trevor Darrell, Jitendra Malik
UniversityofCalifornia,Berkeley
ProjectPage
Humanoid robots can, in principle, use their legs to go almost anywhere. De-
veloping controllers capable of traversing diverse terrains, however, remains
a considerable challenge. Classical controllers are hard to generalize broadly
while the learning-based methods have primarily focused on gentle terrains.
Here, we present a learning-based approach for blind humanoid locomotion
capableoftraversingchallengingnaturalandman-madeterrain. Ourmethod
uses a transformer model to predict the next action based on the history of
proprioceptive observations and actions. The model is first pre-trained on
a dataset of flat-ground trajectories with sequence modeling, and then fine-
tunedonuneventerrainusingreinforcementlearning. Weevaluateourmodel
on a real humanoid robot across a variety of terrains, including rough, de-
formable, and sloped surfaces. The model demonstrates robust performance,
in-context adaptation, and emergent terrain representations. In real-world
casestudies, ourhumanoidrobot successfully traversedover4 milesofhiking
trailsinBerkeleyandclimbedsomeofthesteepeststreetsinSanFrancisco.
1
4202
tcO
4
]OR.sc[
1v45630.0142:viXraFigure 1: Humanoid locomotion over challenging terrain. Our controller can successfully
traversearangeofoutdoorterrain,includingsteep,rough,rutted,rocky,wet,muddy,andsand.
2Introduction
Humanoid robots have the potential to operate in a variety of diverse environments on Earth,
andbeyond. Asignificantfractionoftheseinvolveswalkingoverchallengingterrainsoutdoors.
These include natural environments like forests, deserts, and mountains, as well as man-made
environments like city parks, cracked roads, and steep streets. Developing controllers for hu-
manoidlocomotionoverchallengingterrain,however,remainsasignificantchallenge.
Legged locomotion has been studied extensively in the context of quadrupedal (1–6) and
bipedal (7–15) robots. We now have quadrupedal robots that are able to traverse a variety of
challenging terrains. Bipedal locomotion, for robots such as Cassie, is more challenging. And
humanoidlocomotionisevenmorechallenging. Theheavylegs,upperbodywithalotofmass
andinertia,andarmscarryingpayloadsallcontributetothecomplexityofmaintainingbalance.
Theseadditionaldifficultiesarefurtherexacerbatedwhenwalkingoverchallengingterrain.
Humanoid controllers have demonstrated reliable locomotion in structured environments
using a variety of techniques (16–29). Two popular paradigms are broadly based on the zero
moment point principle (19), such as the Honda ASIMO, and model-based optimization (26),
such as the Boston Dynamics Atlas. These methods are typically extended to uneven terrains
by incorporating footstep planning with perception (30–32). However, they are often hard to
generalizeandadapttonewenvironments,ornewvariationsofseenenvironments.
We have seen exciting results in learning-based real-world humanoid locomotion (33). The
paper proposed a transformer model that takes the past history of proprioceptive observations
and actions as input, and predicts the next action. The model was trained with large-scale
reinforcement learning in simulation and deployed it to real world zero-shot. The resulting
controller can navigate various, albeit gentle, real-world terrains, including different surfaces,
likeconcreteandgrass,overgroundobstructionslikedebrisandsmallsteps,andslightslopes.
3This suggests a natural question: can we extend the same method to learning humanoid
locomotionoverchallengingterrain? Inprinciple,yes. Wecouldincreasethecomplexityofthe
trainingterrainsinsimulation,trainthemodelforlonger,anditerateonthesim-to-realtransfer.
In practice, however, extending this approach to challenging terrains is not straightforward and
involvesasetofmutuallyreinforcingchallenges: anorderofmagnitudemoretrainingsamples,
moreinvolvedenvironmentdesign,andmorecomplexsim-to-realtransfer.
Recently, we have seen an alternate approach to (33) that uses the same model architecture
but proposes a different training method (34). Instead of reinforcement learning, the model
is trained with sequence modeling on a dataset of trajectories of walking on flat. Compared to
reinforcementlearning,sequencemodelingisfarmoreefficientandstable. However,themodel
canonlybeasgoodasthedataitwastrainedon,whichinthiscasemeansgentleterrains.
Theaforementionedmethodisreminiscentof,andindeedinspiredby,generativepre-training
innaturallanguageprocessing(35–38). Modernpre-trainedlanguagemodelscanbeusedzero-
shot,promptedwithfewexamples,orfine-tunedwithadditionaldataforalignmentortoacquire
new capabilities. Our core observation is that this general principle should apply to humanoid
locomotionaswell. Intuitively,ifarobotcanwalkonflatground,itshouldbemoreefficientto
fine-tuneitonchallengingterrains,thanitwouldbetotrainarobotthatcannotwalkatall.
Here, we propose a method for learning humanoid locomotion over challenging terrain.
Our model is a transformer that predicts the next action from the history of proprioceptive
observationsandactions. Ourmethodinvolvesatwo-steptrainingprocedure. First,wepre-train
themodelwithsequencemodelingofflat-groundtrajectories. Weleverageadatasetconstructed
using a prior policy, a model-based controller, and human sequences. Second, we fine-tune
the model with reinforcement learning on uneven terrains. Pre-training enables the model to
vicariouslyabsorbtheskillsfrompriordataandprovidesagoodstartingpointforlearningnew
skillsefficiently. WecallourmodelHumanoidTransformer2,orHT-2forshort.
4WeevaluateourapproachonaDigithumanoidrobotthroughaseriesofreal-worldandsim-
ulation experiments. We find that our policies enable robust walking over a variety of different
terrains,showninFigure1. Theseincludeverysteep,rough,rutted,androckyroads,aswellas
wet, muddy, and sand surfaces. Note that some of these terrains, like loose sand, have not been
seenduringtraining. Weuseasingleneuralnetworkcontrollerforallterrains.
To evaluate our approach in real-world scenarios, we performed two case studies. First, we
took the robot on five different hikes in the Berkeley area, shown in Figure 2, and successfully
completed over 4 miles of hiking. Second, we tested our robot on very steep streets in San
Francisco, shown in Figure 3. With over 31% grade, these are among some of the steepest
streetsinthecityandtheworld. Ourmodelsuccessfullynavigatedalltestedsettings.
We perform a series of studies to understand the properties of our method. First, we look
at the latent representations of the model as the robot is walking over different terrains. We
find that the representations cluster based on the terrain (Figure 4). These representations are
emergentandhavenotbeenhand-designedorsupervisedduringtraining. Second,westudythe
abilities of our model to adapt based on context. We observe kinematic adaptation to different
terrain slope (Figure 5) and dynamic adaptation to terrain material (Figure 6). Note that the
in-contextadaptationbehaviorsareemergentandhavenotbeenpre-programmed.
We compare our approach to a state-of-the-art learning-based controller (33) and find that
our model consistently outperforms the prior work (Figure 7). To understand the key design
choices,weperformaseriesofablationstudies. Wefindthatpre-trainingleadstoconsiderable
sample efficiency gains compared to training from scratch (Figure 9), fine-tuning substantially
improvesperformance(Figure10),andbothexhibitgoodgaitpatterns(Figure11).
The presented results demonstrate that a general learning method enables humanoid loco-
motion over challenging terrain. We hope that the presented methodology will help accelerate
deploymentofhumanoidrobotsinthefullrichnessandcomplexityofthephysicalworld.
5Results
We summarize the results in Figure 1 and Movie 1. We train a single model to control a robot
towalkoveravarietyofdifferentsettings,includingsteep,rough,rocky,andsoftterrains.
Challenging terrain
Ourgoalistodevelopalearning-basedapproachthatcancontrolarealhumanoidrobottowalk
across a variety of challenging terrains. We briefly discuss the hardware platform used in the
experimentsandgiveabasicsenseofourapproachbeforepresentingtheresults.
We perform all experiments on the Digit humanoid robot developed by Agility Robotics.
Digitisapproximately1.6mtallandweightsabout45kg. Therobothas36degreesoffreedom
including the floating base. The arms have four joints each, all of which are actuated. Each leg
has eleven measurable joints, six of which are actuated. The passive joints are connected via
springsandafour-barlinkagemechanism,makingthemdifficulttosimulateaccurately.
Our controller is a transformer neural network model. It takes the history of past proprio-
ceptive observations and actions as input, and predicts the next action as output. The controller
is blind and does not use vision. It supports omni-directional walking by following the desired
velocity commands given as input. The model predicts desired joint positions and PD gains at
50Hz,whicharethenconvertedtotorquesviaaPDcontrollerrunningat2000Hz.
We deploy our controller to the real robot and evaluate it in a number of outdoor scenarios,
like shown in Figure 1. The experiments were performed over the course of about two weeks,
starting around mid May of 2024. We consider a variety of natural environments including
steep,rough,rutted,androckyterrains. Wefurtherevaluateourcontrolleronchallengingman-
made terrains, such as very steep city streets with over 31% grade. Note that many of these
terrainslikevegetation,woodchip,mud,water,sand,andotherswerenotseenduringtraining.
Allofthepresentedresultswereachievedusingasingleneuralnetworkcontroller.
6Figure 2: Berkeley hikes. Our robot has successfully completed over 4 miles of real hiking
trailsinBerkeley. (A-D)Weshowbasicstatisticsandexamplesfromfourdifferenthikes.
7Berkeley hikes
To evaluate our approach in real-world scenarios, we perform two case studies. In the first
study we take our robot to five different hikes in the Berkeley area, summarized in Figure 2.
These are normal human hikes that are collectively about 4.3 miles long. Our policy was able
tosuccessfullycompleteallofthehikes. Wedescribeeachofthehikesinmoredetailnext.
Lower Fire Trail. The first hike we completed was the Lower Fire Trail. It is 0.83 miles
long and has a 195 ft monotonically increasing elevation change. The terrain is a combination
ofroughandrockyground,withtrickyruttedareasfromdriedupwaterpaths.
PanoramicTrail. OursecondhikewasthePanoramicTrail,showninFigure2,D.Thehike
starts with a a long steady incline from the sidewalk over the dirt road, followed by a steeper
inclineoverlooseandrockyterrain,endingatawoodchipvistapointatthetop,andthengoing
back to the starting point. A particularly challenging part was a very steep incline with loose
androckyground,showninimage3fromFigure2,D.
Inspiration Trail. We then completed the Inspiration Trail hike, shown in Figure 2, A. This
was our longest hike at 1.2 miles. It contains gradual elevation changes over rough, rutted, and
grassterrain,includingruttedterraincoveredingrass,andhighgrass,likeshowninimage4.
Belgum Trail. Our fourth hike was the Belgum Trail, shown in Figure 2, B. This hike
consistsoflongsteepslopesoverroughandrockyterrain. Thetrailcontainsdeepwatermarks,
some of which were soft and muddy, like shown in image 3. The highest point of the trail is at
ahighaltitudewithstrongwindcausingnontrivialadditionaldisturbances.
Scotts Peak Trail. Our final hike was the Scotts Peak Trail, shown in Figure 2, C. This hike
is 0.8 miles long and has large elevation changes. It starts with a long and steep descent over
rough terrain with high grass (images 1–2), followed by a rocky and wet path through a forest
(image3),andloopsbacktothestart(images4–6). Partsofthetrailareindeepshadeandvery
wet,withsectionscoveredinwaterandmud,likeshowninimage3.
8Figure 3: San Francisco streets. Our robot can successfully navigate some of the steepest
streetsinSanFrancisco,whicharealsoamongsomeofthesteepeststreetsintheworld.
San Francisco streets
In addition to walking over challenging terrains in natural environments, a strong locomotion
controller should be able to handle challenging man-made terrains. In our second real-world
study,weevaluateourcontrollerinachallengingcityenvironment.
Inparticular,wetakeourrobottotheRussianHillneighborhoodofSanFrancisco,shownin
Figure 3. This neighborhood is on a hill and is characterized by a number of very steep streets.
Our controller was able to successfully navigate all of the tested streets. This included walking
upanddownsteeproads,crossingunevenroads,andeventurningonsteepslopes.
In Figure 3, we show example images with corresponding slope grades. Note that these
streets are very steep and hard even for healthy human adults. Moreover, the streets with slope
gradeof31%areamongsomeofthesteepeststreetsinSanFranciscoandintheworld.
9Figure 4: Terrain representations. We observe that the representations of our model cluster
based on the terrain the robot is walking on. These representations emerge as a byproduct of
learningtowalk. Wedidnotdesign,supervise,orimposeanyconstraintsontherepresentations.
Terrain representations
We have seen that our controller is able to traverse a variety of challenging terrains. To bet-
ter understand the properties of our controller we perform a series of studies. We begin by
analyzingthelatentrepresentationsofourneuralnetworkmodel.
We record the latent representations from the last hidden layer of our transformer model
whiletherobotiswalkingonsixdifferentterrains. Wethenprojectthelatentrepresentationsto
2Dusingt-SNE(39)andplotthemasascatterplotinFigure4. Wecolorcodeeachpointbythe
corresponding terrain type for visualization purposes only. We see that the points get grouped
intosixclearclusterscorrespondingtodifferentterraintypes. Wenotethattheserepresentations
emergedasabyproductoflearningtowalkandhavenotbeenhand-designed.
10A B
C D
contact air-time
Figure 5: Kinematic adaptation. We show the joint positions of the key hip and knee joints
when walking over terrain with different slope. We see that our controller adapts its gait based
ontheterrainslopeitiswalkingover. Thishasnotbeenpre-programmedduringtraining.
In-Context adaptation
In order for the controller to traverse challenging terrain effectively it must be able to adapt its
actionsbasedontheenvironment. Wefindthatourtransformermodelisabletodothatthrough
in-context adaptation. This behavior is emergent and has not been supervised during training.
Wediscussthreedifferentexamplescenariowherethisbehaviormanifests.
Kinematicadaptationtoterrainslope
We first study how the gait changes based on the terrain slope. In Figure 5, we show the joint
positionsofthecorehipandkneejointsoveragaitcycle. Eachcurvecorrespondstoadifferent
terrainslope,flat,uphill,anddownhill. Weobservethatthecontrolleriscommandingdifferent
joint positions and consequently using different gaits based on the terrain slope. We note that
thisbehaviorisemergentandhasnotbeenpre-programmedduringtraining. Namely,themodel
learnedhowtoadjustitsgaitbasedontheterrainslopeimplicitlyencodedinthecontext.
11Figure 6: Dynamic adaptation. (A) We study the behavior of our controller when walking on
two different types of flat terrain, trail and sand. (B) We show the joint position of the core hip
pitch joint over the gait cycle. We see that the controller achieves the same positions despite
theconsiderabledifferencesinthegroundsurface. (C)Thisisenabledbyapplyingmoretorque
whenwalkingonsandtocompensateforthedynamics. Thisshowsthatourcontrollercanadapt
atveryshorttimescalesin-context,basedonthehistoryofdesiredandachievedstates.
Dynamicadaptationtoterrainmaterial
We study adaptation in the context of different terrain materials. We consider two settings,
showninFigure6,A:walkingonahardsurfaceandwalkingonsand. Humanwalkingrequires
considerably more mechanical work and energy when walking on sand (40). In Figure 6, B,
wesee theachievedhippitch positionsoverthe gaitcycle,which arethesamein bothsettings.
However, to achieve the same positions the controller exerts more torque on sand, as shown in
Figure 6, C. Recall that our controller is blind. However, it can still sense the terrain from the
history of proprioceptive observations and actions, which encodes the discrepancy between the
desiredandachievedstatesandenablesthemodeltoadaptdynamicallybasedoncontext.
Adaptationtoexternalpushdisturbances
In addition to adapting to terrain changes, a strong controller should also be able to adapt to
external disturbances. In Movie 2, we apply strong external push while the robot is on uneven
grassterrain. Weseethatthemodelquicklyadaptsitsgaittopreventafallandmaintainbalance.
12Figure7: Comparisontobaselines. WecomparetheproposedHT-2modeltoastate-of-the-art
model HT (33). Our controller has a higher success rate at traversing downhill (A), obstacles
(B), and uphill (C) terrains. It is more efficient when walking over rough (D) and carrying
differentpayloadsuphill(E),andisbetteratfollowingvelocitycommandsonflatground(F).
Comparisons
Intheprevioussections,wehaveseenthattheproposedcontrollercansuccessfullywalkovera
range of challenging terrains. We now perform controlled quantitative comparisons to a state-
of-the-art learning-based controller from prior work in simulation (Figure 7). We perform the
comparisonsusingtheMuJoCosimulator(41),whichsupportsequalityconstraintsandenables
ustosimulatethecomplexstructureoftheDigitrobotreasonablywell. Werepeateachexperi-
mentaltrial256timesandreportthemeanandstandarddeviationforeachexperiment.
13We compare our HT-2 model trained using the proposed method, to the Humanoid Trans-
former (HT) model from prior work (33). For fair comparisons, we use the same transformer
architecture hyper-parameters for both models. The models differ only in the way that they
were trained. In the case of HT, we use the model trained by the authors. The model was
trainedwithlarge-scalereinforcementlearninginsimulationforoverabillionstepsofwalking
over smooth planes, rough planes, and gentle slope terrains. In the case of HT-2, we trained
the model via the proposed two-step approach. The model was first pre-trained with sequence
modeling of flat-ground walking trajectories and then fine-tuned with reinforcement learning
onuneventerrains,includinghills,steepslopes,roughplanes,anddiscreteobstacles.
We first compare the success rate of the models in traversing different terrain. We set up an
environment with two flat sections separated by a terrain section in between. The robot starts
on a flat section and is commanded to walk over the terrain section to the other flat section. A
trialissuccessfuliftherobotreachesthegoalintheallocatedtimeduration. Weconsiderthree
differentterraintypeswitharangeofdifficultiesforeach. WereporttheresultsinFigure7,A–
C. We see that the HT and HT-2 models are comparable downhill (A). The gap in performance
increases over discrete obstacles (B) and is the largest uphill (C). Note that the performance of
HTdecaysrapidlywithincreaseddifficultywhiletheHT-2degradesgracefully.
Increased gait variability can be predictive of human falls and is often used in clinical stud-
ies, particularly with the elderly (42). One way to quantify gait variability is to look at the
stride time variability and measure the coefficient of variation. We adopt this metric to asses
the performance of robot locomotion over rough terrain. The basic idea is that a controller that
is better at handling rough terrain will stumble less and have a smaller coefficient of variation.
We report the results in Figure 7, D. We see that the HT-2 model has a considerably smaller
stride time variability. Moreover, the HT-2 performance decreases gracefully with the increase
interraindifficulty. Incontrast,HTperformancedegradesrapidlywithincreaseddifficulty.
14In the next experiment we asses the performance of the controllers when walking up hill
with a payload. Namely, we increase the mass of the robot by up to 20 kg and command the
robottowalkuphill. Wethenmeasurethemechanicalcostoftransport. Thecontrollerthathas
amoreefficientgaitshouldhavealowermechanicalcostoftransport. InFigure7,E,wereport
theresult. WeseethatHT-2consistentlyoutperformsHTacrossallpayloads.
A good controller should be able to track desired velocity commands accurately. One way
to quantify this is to measure amount of drift while following commands. In Figure 7, F, we
report linear drift in the ground XY plane and angular drift around the vertical Z axis. We see
thattheHT-2modelhasaconsiderablylowerdriftmakingitmorecontrollable.
Discussion
Thepresentedresultsshowthatasingleneuralnetworkcanlearntocontrolahumanoidrobotto
walk over a variety of challenging terrains. These include rigid terrains seen during training as
well as terrains such as mud, water, sand, grass, and many others, which were not seen during
training. Our results were achieved using a general learning method that enables rapid acquisi-
tion of skills from existing datasets and efficient learning of new skills through interaction. We
believethattheproposedmethodologycanhavebroadapplications.
An important limitation and opportunity for future work is incorporating vision. Namely,
our current controller is blind and relies solely on proprioceptive observations. While it is
interestingthatablindcontrollercantraversetheterrainsshownhereatall,incorporatingvision
has the potential to bring two major benefits. First, when walking over terrains with discrete
irregularities, such a curb, a blind controller must either get lucky to step over them or “sense”
them by bumping into them. Having access to vision as a distant sense could enable the model
to anticipate the terrain changes and choose its steps accordingly. Second, it would make it
feasible to navigate terrains for which vision is necessary, like stairs and stepping stones. A
15clear opportunity for future work is to use the presented methodology as a starting point for
developingalocomotioncontrollerthatusesbothvisualandproprioceptiveobservations.
More broadly, the presented methodology can serve as a solid foundation for expanding
capabilities of humanoid robots beyond locomotion. Our model architecture and data-driven
training strategy make few domain-specific assumptions beyond the problem formulation. An
excitingopportunityforfutureworkistouseourmethodologyasastartingpointfordeveloping
aunifiedhumanoidmodelthatcanperformbothlocomotionandmanipulation.
Overall,thepresentedresultsdemonstratethatagenerallearningmethodenableshumanoid
locomotionoveravarietyofchallengingterrains. Wehopethatthepresentedmethodologywill
acceleratedeploymentofhumanoidrobotsinunstructuredreal-worldenvironments.
Materials and Methods
We now present the methodology used to train the neural network controller that achieved the
results from the previous sections. We describe the problem formulation, model architecture,
ourpre-trainingandfine-tuningprocedure,andpresentablationstudiesofkeydesignchoices.
Problem formulation
We formulate the problem of blind humanoid locomotion as follows. The objective of our hu-
manoidcontrolleristooutputmotortorquestomaintainbalancewhilefollowingdesiredveloc-
ity commands. The observation space consists of previous motor actions, joint positions, joint
velocities, linear velocity of the base, angular velocity of the base, and velocity commands in
theformofthedesiredlinearvelocityinthehorizontalplaneandtheangularvelocityaroundthe
up axis (Table S1). Note that the observations include the positions and velocities of all joints,
both actuated and passive. The action space is parameterized by a proportional-derivative (PD)
controller,andconsistsofposition,stiffness,anddampingvaluesforeachmotor(TableS2).
16We perform all experiments on the Digit humanoid robot developed by Agility Robotics.
Digitisapproximately1.6mtallandweightsabout45kg. Therobothas36degreesoffreedom
including the floating base. The arms have four joints each, all of which are actuated. Each leg
has eleven measurable joints, six of which are actuated. The passive joints are connected via
springsandafour-barlinkagemechanism,makingthemdifficulttosimulateaccurately.
Model architecture
We represent the controller as a neural network. Specifically, we use the Transformer (43)
architecture, which has shown excellent results in a number of domains (35–37), including
humanoidlocomotion(33,34). Thisarchitecturehasseveralappealingproperties,twoofwhich
areparticularlywell-suitedtotheproposedmethodology: (i)Scalablepre-training: itallowsfor
scalablepre-trainingonlargedatasetswithgenerallearningobjectives;(ii)In-contextlearning:
itenablesthemodeltolearntoadaptitsbehaviorattesttimebasedoncontext.
Our model has few domain-specific biases beyond the choice of the inputs and the outputs.
Weprovidethehistoryofthepreviouskproprioceptiveobservationsandactionso ,a ,...,o
t−k t−k t
asinput. Andthemodelpredictsthenextactiona asoutput. Weencodetheinputobservations
t
and actions into a sequence of k tokens. Specifically, for each timestep t, we first concatenate
theobservationsandactionsintoasinglevector. Wethenencodetheconcatenatedvectorintoa
vector of the same dimension as the hidden size of the transformer. We use a small multi-layer
perceptron (MLP) as the encoder. The weights of the MLP encoder are shared across time. To
encode the temporal order and the position of each token within the input sequence, we add
the sinusoidal position embeddings to the input tokens. We then pass the sequence of k tokens
into the Transformer model. Our architecture follows a fairly standard Transformer design and
consists of a sequence of L layers. Each layer contains of a multi-head self-attention module
and an MLP module. We encode temporal dependencies among the tokens by using causal
17attention. Inparticular,weuseacausalmaskinthemulti-headself-attentionmodulestoensure
thateachtokenonlyattendstoitselfandprevioustokens. Oncethetokensareprocessedbythe
transformer, we apply an output projection head. The output projection head is a small MLP
that projects an embedding from the hidden size of the transformer back to the original input
dimension. Notethatthepredictionscorrespondtotheconcatenatedinputs,containingboththe
observations and the actions, for each timestep. We apply the output projection head to all of
thetimestepsduringpre-training,andonlyatthelasttimestepduringfine-tuning. Similarly,the
loss is applied to the full output predictions during pre-training, and only to the action subset
duringfine-tuning. Wediscusspre-trainingandfine-tuninginthefollowingsections.
Unless noted otherwise, the transformer model used in the experiments has a context win-
dow of 16 timesteps and four transformer blocks. Each block has a hidden dimension of 192,
uses a multi-head self-attention module with four heads, and the MLP ratio of two. The input
MLP encoder has hidden sizes of [512, 512] while the output MLP head has hidden sizes of
[256,128]. Thetransformermodelhas1.4millionlearnableparametersintotal.
Pre-training with sequence modeling
Oneoptiontotrainourmodelwouldbetoperformreinforcementlearning(33). Inotherwords,
initialize the weights of the neural network randomly, from scratch, and use that as the starting
point for trial and error learning. This process is hard to perform in the real world, at present,
and can be performed in simulation instead. Namely, the model is trained on an ensemble of
randomized environments and transferred to the real world. The overall process is delicate and
time consuming, often requiring billions of samples in simulation, intricate reward design, and
iteratingthetrain-deploylooptoachievesuccessfulsim-to-realtransfer. Theseissuesarefurther
exacerbatedwiththecomplexityofthetask,makingourtaskoflearninghumanoidlocomotion
over challenging terrains particularly tricky. While some of these challenges can be partially
18alleviatedthroughtheuseoftechniqueslikedistillationandprivilegedinformation(5,6,33,44),
thefundamentalchallengesoflearningthroughinteractionfromscratchstillremain.
To overcome these challenges, our methodology uses a two step learning procedure. Our
coreobservationisthateventhoughthecapabilitiesthatwewanttolearn,inthiscasehumanoid
locomotion over challenging terrain, are not readily available in the form of data, we can still
leverage the sources of available data to learn relevant skills. Recent work (34) has shown that
wecantrainahumanoidmodeltowalkongentleterrainspurelyfromofflinedatacomingfrom
prior neural network controllers, classical model-based controllers, and humans. We show that
thismethodcanserveasaneffectivepre-trainingprocedureandenableefficientlearningofnew
skills subsequently. Intuitively, a robot that already knows how to walk on flat ground should
be better at learning how to walk up a slope, than a robot that cannot walk at all. Pre-training a
humanoid modelon existing trajectories issimilar in spiritto pre-training a languagemodel on
thetextfromtheInternet(35–37). Wedescribethepre-trainingprocedurenext.
We begin by compiling a dataset D of humanoid trajectories. Each trajectory is a sequence
of proprioceptive observations and actions over time: T = (o ,a ,o ,a ,...,o ,a ). As a ma-
1 1 2 2 T T
jor source of trajectories we use a neural network policy from prior work (33). We run the
model and record trajectories in simulation. In each trajectory, the robot walks on flat ground
for10sandfollowsarandomlysampledvelocitycommand. Asanadditionalsourceofdatawe
use trajectories that come from a model-based controller and humans. We use the model-based
controller provided by the robot manufacturer in simulation. This controller is available via an
application programming interface (API) and we do not get access to its internals. Like before,
wecollect10slongtrajectoriesofwalkingwithrandomlysampledcommands. However,unlike
inthecaseofapolicy,weonlygetaccesstoproprioceptiveobservations. Thus,eachtrajectory
is a sequence of observations without the actions: T = (o ,o ,...,o ). In the case of human
1 2 T
data,weusehumanwalkingtrajectoriesfromexistingmotioncapture(MoCap)datasets(45,46)
19Step 1: Pre-training with sequence modeling observation action
sequence
Transformer
modeling
trajectories of walking on flat ground
Step 2: Fine-tuning with reinforcement learning
reinforcement
Transformer
learning
simulation environments with uneven terrain
Figure 8: Method. Our controller is a transformer model that predicts the next action from the
history of past proprioceptive observations and actions. Our training procedure consists of a
pre-training stage (top) and a fine-tuning stage (bottom). In the pre-training stage, we pre-train
themodelwithsequencemodelingonadatasetoftrajectoriesofwalkingonflatground. Inthe
fine-tuningstage,wefine-tunethemodelwithreinforcementlearningonuneventerrains.
and human videos (47,48). We extract human poses from videos using computer vision tech-
niques (49), which can be seen as noisy MoCap. Each human walking trajectory is a sequence
of human poses. Since the morphologies of the human and the robot are different, we retarget
the human poses to the robot morphology via inverse kinematics (IK). This provides us with
approximate proprioceptive observations for the joints and the base. We set the velocity com-
mands for each trajectory approximately as well, using hindsight re-labeling. Like in the case
ofthemodel-basedtrajectories,wedonotgetaccesstoactionsforthehumantrajectories.
20We now describe our pre-training objective. Our goal is to train our transformer neural
networkarchitecturetomodelthejointtrajectorydistributionp(τ)autoregressively:
K
(cid:89)
p(τ) = p(a ,o |a ,o ,...,a ,o ) (1)
k k k−1 k−1 1 1
k=1
Todothat,wewishtominimizethenegativelog-likelihoodoverthedatasetoftrajectories:
(cid:88)
L = −logp(τ) (2)
τ∈D
We assume a Gaussian distribution with constant variance and train our model to minimize the
mean squared error between the predicted and the ground truth values. Our model takes in and
predicts the normalized observation and action values in the continuous space. We found this
approachtoworkwellanddonotperformquantizationoftheinputortheoutputvalues.
Intheexpositioninthepreviousparagraphweassumedthateachtrajectoryisasequenceof
proprioceptiveobservationsandactions. Recall,however,thatasubsetofourdatasetcontaining
trajectories that come from the model-based controller and humans does not have actions. To
overcome this challenge, we replace the missing actions with learnable mask tokens [M] to
obtain T = (o ,[M],...,o ,[M]). This enables us to tokenize and input all of the trajectories
1 T
in a unified format. During training, we ignore the loss for the predictions that correspond to
themaskedpartsoftheinputs. Theintuitionisthatsequencesofobservations,evenwithoutthe
actions,containusefulinformationabouttheworldthatthemodelcanlearnfrom. Moreover,by
jointly training with the data that has actions, the model might learn how to inpaint the actions
forthedatawithoutactions. And,consequently,modelthejointdistributionbetter.
We pre-train the model for 300 epochs on 4 A100 NVIDIA GPUs. We use a global mini-
batch size of 4096. We employ a cosine learning rate schedule with the initial learning rate of
5e-4. Thelearningrateiswarmeduplinearlyfor30epochsstartingfromonetenthoftheinitial
learning rate. We use the AdamW optimizer with weight decay of 0.01, β of 0.9, and β of
1 2
0.95. Wesummarizethepre-traininghyperparametersinTableS3.
21Fine-tuning with reinforcement learning
After pre-training on a dataset of trajectories, we obtain a model that is able to walk. However,
since the model was trained on flat-walking trajectories its capabilities are limited mostly to
gentleterrains. Forexample,themodelcannotwalkonsteepslopesorterrainsthatarefarfrom
thepre-trainingdataset. Moreover,themodelisnotveryrobusttoexternaldisturbancesorlarge
changesinterrainproperties. Nevertheless,asourexperimentsshow,thepre-trainedmodelcan
serveasaneffectivestartingpointforacquiringnewcapabilitiesthroughinteraction.
To acquire new skills, we formulate a reinforcement learning (RL) problem in simulation.
We assume access to a physics simulator with environment and robot states s , a transition
t
function S(s |s ,a ), and an observation function O(o |s ). We define a reward function
t+1 t t t t
R(s ,a ,s ), consisting of terms that specify the locomotion task and encourage desirable
t t t+1
properties,likesmoothnessandenergyminimization. Weprovidetherewardfunctiontermsand
coefficients in the Supplement. We wish to learn a policy π(a |o ,a ,...,o ) that maximizes
t t t−1 1
theexpectedsumofdiscountedrewardsthroughinteractionswiththeenvironment.
WeparameterizethepolicyasadiagonalmultivariateGaussian,withameanrepresentedby
a neural network model and a learnable diagonal covariance matrix. We initialize the weights
oftheneuralnetworkwiththepre-trainedmodel. Pre-trainingcanthusbeviewedasafavorable
initializationforreinforcementlearning. Attrainingtime,wesampleactionsfromtheGaussian
distribution to enable the policy to explore. We use the mean actions predicted by the neural
networkattesttime. Sinceourpolicystartsofffromapre-trainedmodelthatisalreadyreason-
able, we need to make sure that the initial exploration noise does not overwhelm the learning
andinstantlyundotheeffectofpre-training. Consequently,werandomlyinitializethelearnable
covariance matrix to small values; smaller than those typically used for RL from scratch (33).
The hope is that this will enable the model to start exploring slowly, around the pre-trained
model,andgraduallylearntoexpandtheexplorationfrontiertoacquirenewskills.
22We use the proximal policy optimization (PPO) algorithm (50) for fine-tuning the policy.
PPO is a policy gradient method that in addition to training the policy, or the actor, involves
training a critic network. Since the critic network is only used at training time and not required
atdeployment,wecantaketakeadvantageoftheoraclestateinformationavailableinsimulation
during training (33,44,51). In particular, the critic model receives the states rather than the
observation as input. We summarize the information contained in the critic state in Table S2.
Theactorandthecriticusethesamenetworkarchitecture,exceptforthefirstandthelastlayers
whichhavedifferentdimensionstoaccountfordifferentinputandoutputdimensions. Theydo
notshareweights. Thecriticisinitializedfromscratchandisnotpre-trained.
To acquire locomotion skills over uneven terrains, we fine-tune the model on a set of pro-
cedurally generated terrains: rough flats, smooth slopes, rough slopes, discrete obstacles, and
hills. For each of the terrain types we consider variations in difficulty, such as the slope grades
and obstacle heights. To make sure that the model retains a good flat walking gait, we include
the flat terrain, resulting in six terrain types in total. We fine-tune the model on an ensemble
of environments with different terrains jointly. Note that we fine-tune the model only on rigid
terrains,withoutsoftordeformableterrains. Nevertheless,asdemonstratedinourresults,mod-
els fine-tuned in this way can generalize to unseen real-world terrains during deployment. We
summarizetheterraintypes,difficultyranges,andproportionsinTableS5.
Weusearewardfunctionthatconsistsofmultipletermsthatcanbedividedintotwogroups.
The first group specifies the locomotion task with velocity tracking and includes the terms for
tracking linear velocity in the forward-backward and left-right directions, the angular velocity
around the up direction, and an alive bonus. In principle, or in the real world, these terms
should be sufficient to learn a good walking behavior. In practice, however, our simulation
environments and compute are limited. We are unable to simulate physical phenomena, such
as battery life and hardware damage, and the full diversity and richness of real-world terrains.
23To partially alleviate these challenges in the short term, we include additional reward terms.
Specifically, we include energy, ground contact, base movement, foot airtime, foot symmetry,
footswing,andtorsorewardterms. TherewardtermsaresummarizedintheSupplement.
We fine-tune the model on a single A10 NVIDIA GPU. We train for 2000 iterations using
2048 parallel environments and 24 steps per environment, resulting in ∼100M environment
steps in total. We perform PPO updates over 5 epochs with a minibatch size of 12288. We
set the PPO clipping parameter to 0.2, the weight decay to 0.01, and do not use the entropy
regularization. We use the initial learning rate of 1e-5 and the cosine learning rate schedule for
the actor. The learning rate is warmed up linearly for 100 iterations starting from 1e-8. The
critic uses a constant learning rate of 5e-4. We use a discount factor γ of 0.99 and generalised
advantageestimationλof0.95. Wesummarizethefine-tuninghyperparametersinTableS4.
Sim-to-real transfer
We fine-tune our model with reinforcement learning in simulation and transfer it to the real
worldzero-shot. Ourgeneralstrategyforsim-to-realtransferbroadlyfollows(33). Weprovide
additionaldetailsonsimulation,domainrandomization,andreal-worlddeploymentbelow.
WeperformexperimentsonaDigithumanoidrobot. Digitusesfour-barlinkagesthatintro-
duce loops in the kinematic tree. We use the MuJoCo simulator (41) which is able to simulate
this using equality constraints. While MuJoCo is considerably slower than GPU-based simula-
tors,likeIsaacGym(52),wecanaffordtouseaslowersimulatorthankstothesampleefficiency
of our method. In particular, our method only requires ∼100M samples for fine-tuning, which
we are able to acquire in MuJoCo in about a day in our implementation. Specifically, we use
a batched environment pool (53) to parallelize simulation on CPUs of a single machine. We
modelthekneesviaequalityconstraints. Wetreatthetoesaspassiveforconsistencywithprior
data. Weleavepotentialimprovementsfromcontrollingthetoesforfuturework.
24Each simulated environment consists of a procedurally generated terrain and a single robot
taskedwithfollowingvelocitycommands. Wefine-tuneourmodelonanensembleofdifferent
terrains. Specifically, we consider six different terrain types: flat, rough, smooth slope, rough
slope, discrete obstacles, and hills. We pre-assign a probability to each terrain type and sample
a terrain for each environment. We additionally sample terrain-specific parameters that induce
variations within terrains of the same type, such as slope steepness or obstacle heights. We
providethesummaryofterraintypes,parameters,andexampleimagesinTableS5.
The velocity commands consist of three values: forward linear velocity, lateral linear ve-
locity, and turning angular velocity. We sample each value independently from a pre-specified
range using a uniform distribution. Since moving in all three dimensions simultaneously is
rarely required, and infeasible for many triplets, we zero out each command dimension with
probabilityofone-half,fortrainingefficiency. WesummarizethecommandsinTableS6.
To enable the model to transfer from simulation to the real world, we fine-tune it on an
ensemble of randomized environments. We randomize environment physics, such as gravity,
friction, and damping ratios. To more accurately model real-world sensors, we apply noise and
delays to sensor observations in simulation. In addition, we randomize the robot properties,
including body mass and size, joint damping and stiffness, and actuation. We note that our
goal is not to learn a robust controller that can control the robot in all of the environments in
the same way. Rather, we train a transformer model that can adapt based on the context of the
environmentitisoperatingin. WesummarizetherandomizationrangesinTableS7.
After fine-tuning in simulation, we deploy our model to hardware zero-shot. The model
runsontheCPUoftheonboardIntelNUCcomputer. Weobtainjointpositions,velocities,and
IMUinformationfromthelow-levelrobotAPI.Wedonotuseanycontrolsoftwareprovidedby
the manufacturer. Our model produces actions at 50 Hz. The actions specify the joint position
targetsandthePDgainsforthePDcontrollerthatrunsat2000Hz,andoutputstorques.
25A Uneven terrain B External perturbation C Velocity tracking
Figure 9: Comparison to scratch policy. We compare a fine-tuned policy to a policy trained
from scratch, as a function of the number of environment steps used for training. We include
thescratchperformancewithupto2×moresteps. Weconsiderthreedifferentsettings: uneven
terrains,externalperturbations,andvelocitytracking. Weobservethatfine-tuningleadstoafar
bettersamplecomplexityandachievesconsiderablybetterabsoluteperformanceinallsettings.
Ablation studies
Weanalyzesomeofthekeycomponentsofourmethodology. Westudypre-trained,fine-tuned,
andscratchmodelsthroughthelensofsamplecomplexity,performance,andgaitquality.
Comparisontoscratchpolicy
We begin by studying the impact of fine-tuning a model compared to training a model from
scratch. Forfaircomparisons,weoptimizethehyperparametersofthescratchmodelseparately.
Weoptimizethefine-tunedandthescratchmodelsforupto100Mand200Menvironmentsteps,
respectively. We consider three evaluation settings: uneven terrain, external perturbations, and
velocity tracking. We report the performance as the function of training steps in Figure 9. We
observe that fine-tuning a model leads to a far better sample complexity than training from
scratch. Moreover, fine-tuning leads to considerably better absolute performance across all
settings. Thissuggeststhatfine-tuningdoesnotonlyacceleratebutalsoimproveslearning.
2697.0 95.8
84.4
45.3
78.8 25.0 5.8 87.6
24.9
74.9
93.2
95.1
Figure 10: Comparison to pre-trained policy. We compare the performance of a pre-trained
policy to a policy fine-tuned with reinforcement learning. We consider six different settings:
downhill, uphill, push, obstacles, rough, and slip. We observe that the pre-trained policy per-
forms reasonably well in easier settings, like downhill and rough. However, once we consider
hardersettings,weseethatfine-tuningbringssubstantialimprovementsinperformance;likeon
uneventerrains,suchasuphillandobstacles,overslipperyterrain,andwithpushdisturbances.
Comparisontopre-trainedpolicy
To understand the benefits fine-tuning with reinforcement learning brings over the pre-trained
model, we compare a pre-trained policy to a fine-tuned policy. In Figure 10, we report the suc-
cess rates in six different settings: downhill, uphill, push, obstacles, rough, and slip. First, we
observethatthepre-trainedpolicyperformsreasonablywellineasiersettings,suchasdownhill
and rough. Second, we observe that fine-tuning leads to substantial improvements in perfor-
mance in harder settigns. These include uneven terrains, such as uphill and discrete obstacles,
slippery terrain, and external push disturbances. These findings suggest that pre-training pro-
videsareasonablestartingpointandthattheRLfine-tuningleadstoasubstantiallybetterpolicy.
27Pre-trained Fine-tuned Scratch
L
R
2.5 5.0 7.5 2.5 5.0 7.5 2.5 5.0 7.5
Time (s) Time (s) Time (s)
Figure 11: Comparison of gait patterns. We compare the gait patterns for three models: pre-
trained, fine-tuned, and scratch. We see that both the pre-trained and fine-tuned policy have
goodandsimilargaitpatternswhilethescratchpolicyhasaconsiderablyworsegaitpattern.
Comparisonofgaitpatterns
Gait quality has a big impact on locomotion performance. A good gait enables more capable,
efficient,andvisuallypleasinglocomotion. Moreover,itislesslikelytobereliantonsimulator
imperfections and more likely to transfer to the real world. In Figure 11, we compare the gait
patterns for three models: pre-trained, fine-tuned, and scratch. We see that both the pre-trained
and the fine-tuned policy have good gaits that are fairly similar. The scratch policy, however,
hasaconsiderablyworsegait. Itishighlyasymmetricanditsleftlegisbeinglifteduplessand
is dragging. For fair comparisons, both the fine-tuned and the scratch policy are trained on the
same set of terrains and with the same set of rewards. These findings suggest that pre-training
helpsthemodeltoacquirenewcapabilitiesduringfine-tuning,whileretainingagoodgait.
Acknowledgments
This work was supported in part by DARPA Machine Common Sense program, DARPA TIA-
MAT program (HR00112490425), ONR MURI program (N00014-21-1-2801), NVIDIA, and
BAIR’s industrial alliance programs. We thank Daniel Yum, Rahul Meka, Sasha Ostojic, and
Playground Global for providing the Digit robot used in the experiments; Bike Zhang and
TorstenDarrellforhelpwiththeexperiments;KoushilSreenathforprovidinglabspace.
28References
1. M. Raibert, K. Blankespoor, G. Nelson, and R. Playter, “Bigdog, the rough-terrain
quadrupedrobot,”IFACProceedingsVolumes,2008.
2. M. Zucker, J. A. Bagnell, C. G. Atkeson, and J. Kuffner, “An optimization approach to
roughterrainlocomotion,”inICRA,2010.
3. J. Zico Kolter and A. Y. Ng, “The stanford littledog: A learning and rapid replanning
approachtoquadrupedlocomotion,”IJRR,2011.
4. J. Di Carlo, P. M. Wensing, B. Katz, G. Bledt, and S. Kim, “Dynamic locomotion in the
mitcheetah3throughconvexmodel-predictivecontrol,”inIROS,2018.
5. J.Lee,J.Hwangbo,L.Wellhausen,V.Koltun,andM.Hutter,“Learningquadrupedalloco-
motionoverchallengingterrain,”ScienceRobotics,2020.
6. A. Kumar, Z. Fu, D. Pathak, and J. Malik, “Rma: Rapid motor adaptation for legged
robots,”inRSS,2021.
7. Y.ZhaoandL.Sentis,“Athreedimensionalfootplacementplannerforlocomotioninvery
roughterrains,”inHumanoids,2012.
8. A. Hereid, E. A. Cousineau, C. M. Hubicki, and A. D. Ames, “3d dynamic walking with
underactuatedhumanoidrobots: Adirectcollocationframeworkforoptimizinghybridzero
dynamics,”inICRA,2016.
9. C.Hubicki,A.Abate,P.Clary,S.Rezazadeh,M.Jones,A.Peekema,J.VanWhy,R.Dom-
res, A. Wu, W. Martin et al., “Walking and running with passive compliance: Lessons
from engineering: A live demonstration of the atrias biped,” IEEE Robotics & Automation
Magazine,2018.
2910. T.Apgar,P.Clary,K.Green,A.Fern,andJ.W.Hurst,“Fastonlinetrajectoryoptimization
forthebipedalrobotcassie.”inRSS,2018.
11. E. R. Westervelt, J. W. Grizzle, C. Chevallereau, J. H. Choi, and B. Morris, Feedback
controlofdynamicbipedalrobotlocomotion. CRCpress,2018.
12. Y. Gong, R. Hartley, X. Da, A. Hereid, O. Harib, J.-K. Huang, and J. Grizzle, “Feedback
control of a cassie bipedal robot: Walking, standing, and riding a segway,” in 2019 Ameri-
canControlConference(ACC),2019.
13. D. Kim, S. J. Jorgensen, J. Lee, J. Ahn, J. Luo, and L. Sentis, “Dynamic locomotion for
passive-ankle biped robots and humanoids using whole-body locomotion control,” IJRR,
2020.
14. J.Siekmann,Y.Godse,A.Fern,andJ.Hurst,“Sim-to-reallearningofallcommonbipedal
gaitsviaperiodicrewardcomposition,”inICRA,2021.
15. J. Siekmann, K. Green, J. Warila, A. Fern, and J. Hurst, “Blind bipedal stair traversal via
sim-to-realreinforcementlearning,”inRSS,2021.
16. M. Vukobratovic´ and D. Juricˇic´, “Contribution to the synthesis of biped gait,” IFAC Pro-
ceedingsVolumes,1968.
17. S. Kajita, F. Kanehiro, K. Kaneko, K. Fujiwara, K. Harada, K. Yokoi, and H. Hirukawa,
“Biped walking pattern generation by using preview control of zero-moment point,” in
ICRA,2003.
18. M.HiroseandK.Ogawa,“Hondahumanoidrobotsdevelopment,”PhilosophicalTransac-
tionsoftheRoyalSocietyA:Mathematical,PhysicalandEngineeringSciences,2007.
3019. M.Vukobratovic´ andB.Borovac,“Zero-momentpoint—thirtyfiveyearsofitslife,”Inter-
nationaljournalofhumanoidrobotics,2004.
20. S. H. Collins, M. Wisse, and A. Ruina, “A three-dimensional passive-dynamic walking
robotwithtwolegsandknees,”IJRR,2001.
21. R.Tedrake,T.W.Zhang,andH.S.Seung,“Stochasticpolicygradientreinforcementlearn-
ingonasimple3dbiped,”inIROS,2004.
22. S. Iida, S. Kato, K. Kuwayama, T. Kunitachi, M. Kanoh, and H. Itoh, “Humanoid robot
controlbasedonreinforcementlearning,”inMicro-NanomechatronicsandHumanScience,
2004.
23. S.Collins,A.Ruina,R.Tedrake,andM.Wisse,“Efficientbipedalrobotsbasedonpassive-
dynamicwalkers,”Science,2005.
24. B.J.StephensandC.G.Atkeson,“Dynamicbalanceforcecontrolforcomplianthumanoid
robots,”inIROS,2010.
25. M.Johnson,B.Shrewsbury,S.Bertrand,T.Wu,D.Duran,M.Floyd,P.Abeles,D.Stephen,
N. Mertins, A. Lesman et al., “Team ihmc’s lessons learned from the darpa robotics chal-
lengetrials,”JournalofFieldRobotics,2015.
26. S.Kuindersma,R.Deits,M.Fallon,A.Valenzuela,H.Dai,F.Permenter,T.Koolen,P.Mar-
ion, and R. Tedrake, “Optimization-based locomotion planning, estimation, and control
designfortheatlashumanoidrobot,”Autonomousrobots,2016.
27. T.Koolen,S.Bertrand,G.Thomas,T.DeBoer,T.Wu,J.Smith,J.Englsberger,andJ.Pratt,
“Design of a momentum-based control framework and application to the humanoid robot
atlas,”IJHR,2016.
3128. D. Rodriguez and S. Behnke, “Deepwalk: Omnidirectional bipedal gait by deep reinforce-
mentlearning,”inICRA,2021.
29. G.A.Castillo,B.Weng,W.Zhang,andA.Hereid,“Robustfeedbackmotionpolicydesign
usingreinforcementlearningona3ddigitbipedalrobot,”inIROS,2021.
30. J.Chestnutt,J.Kuffner,K.Nishiwaki,andS.Kagami,“Planningbipednavigationstrategies
incomplexenvironments,”inHumanoids,2003.
31. R. Deits and R. Tedrake, “Footstep planning on uneven terrain with mixed-integer convex
optimization,”inHumanoids,2014.
32. R. J. Griffin, G. Wiedebach, S. McCrory, S. Bertrand, I. Lee, and J. Pratt, “Footstep plan-
ningforautonomouswalkingoverroughterrain,”inHumanoids,2019.
33. I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and K. Sreenath, “Real-world
humanoidlocomotionwithreinforcementlearning,”ScienceRobotics,2024.
34. I. Radosavovic, B. Zhang, B. Shi, J. Rajasegaran, S. Kamat, T. Darrell, K. Sreenath, and
J.Malik,“Humanoidlocomotionasnexttokenprediction,”arXiv:2402.19469,2024.
35. A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improving language under-
standingbygenerativepre-training,”2018.
36. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., “Language models
areunsupervisedmultitasklearners,”2019.
37. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P.Shyam,G.Sastry,A.Askelletal.,“Languagemodelsarefew-shotlearners,”inNeurIPS,
2020.
3238. L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,
K. Slama, A. Ray et al., “Training language models to follow instructions with human
feedback,”inNeurIPS,2022.
39. L.VanderMaatenandG.Hinton,“Visualizingdatausingt-sne.”JMLR,2008.
40. T. M. Lejeune, P. A. Willems, and N. C. Heglund, “Mechanics and energetics of human
locomotiononsand,”JournalofExperimentalBiology,1998.
41. E.Todorov,T.Erez,andY.Tassa,“Mujoco: Aphysicsengineformodel-basedcontrol,”in
IROS,2012.
42. J. M. Hausdorff, D. A. Rios, and H. K. Edelberg, “Gait variability and fall risk in
community-living older adults: a 1-year prospective study,” Archives of physical medicine
andrehabilitation,2001.
43. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I.Polosukhin,“Attentionisallyouneed,”inNIPS,2017.
44. OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron,
A.Paino,M.Plappert,G.Powell,R.Ribas,J.Schneider,N.Tezak,J.Tworek,P.Welinder,
L. Weng, Q. Yuan, W. Zaremba, and L. Zhang, “Solving rubik’s cube with a robot hand,”
arXiv:1910.07113,2019.
45. M. Plappert, C. Mandery, and T. Asfour, “The KIT motion-language dataset,” Big Data,
2016.
46. N.Mahmood,N.Ghorbani,N.F.Troje,G.Pons-Moll,andM.J.Black,“AMASS:Archive
ofmotioncaptureassurfaceshapes,”inICCV,2019.
3347. J.CarreiraandA.Zisserman,“Quovadis,actionrecognition? anewmodelandthekinetics
dataset,”inCVPR,2017.
48. M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall, and B. Schiele,
“Posetrack: Abenchmarkforhumanposeestimationandtracking,”inCVPR,2018.
49. J. Rajasegaran, G. Pavlakos, A. Kanazawa, and J. Malik, “Tracking people by predicting
3dappearance,locationandpose,”inCVPR,2022.
50. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy opti-
mizationalgorithms,”arXiv:1707.06347,2017.
51. L. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel, “Asymmetric actor
criticforimage-basedrobotlearning,”RSS,2017.
52. V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller,
N. Rudin, A. Allshire, A. Handa et al., “Isaac gym: High performance gpu-based physics
simulationforrobotlearning,”arXiv:2108.10470,2021.
53. J. Weng, M. Lin, S. Huang, B. Liu, D. Makoviichuk, V. Makoviychuk, Z. Liu, Y. Song,
T. Luo, Y. Jiang, Z. Xu, and S. Yan, “EnvPool: A highly parallel reinforcement learning
environmentexecutionengine,”inNeurIPS,2022.
54. M.H.Raibert,Leggedrobotsthatbalance. MITpress,1986.
34Supplementary Materials
Reward function
Ourrewardfunctionisasumofthefollowingterms:
• Linear velocity tracking reward (r ): This reward tracks the targets of forward and side-
lv
waywalkingvelocity.
r := exp(−||v −v∗ ||2/σ ),
lv xy xy 2 xy
where v and v∗ represent the realized and commanded base linear velocity, respec-
xy xy
tively. Wesetσ to0.2.
xy
• Angularvelocitytrackingreward(r ): Thisrewardtracksthetargetofturningvelocity,
av
r := exp(−(ω −ω∗)2/σ ),
av z z ω
where ω and ω∗ represent the realized and commanded base angular velocity along the
z z
z-axis,respectively. Wesetσ to0.2.
ω
• Alivereward(r ): rewardslongerepisodelengths.
a
r := 1
a
• Motor power reward (r ): This reward penalizes output torques to reduce energy con-
W
sumptionandpreventhardwaredamage.
r := −5·10−4 ·|⟨τ ,q˙⟩|,
W
whereτ arethemotortorquesandq˙ arethemotorvelocities.
35• Foot contact power reward (r ): This reward penalizes overly high contact force on the
fcp
robot’sfeet.
(cid:88)
r := −0.002· |⟨F(i),v(i)⟩|,
fcp
i∈foot
whereF(i)isthefootcontactforce,andv(i)isthefootvelocity.
• Base angular velocity reward (r ): This reward penalizes roll-pitch motions of the
bav
robot’sbase.
r := −0.25·||ω ||2,
bav xy 2
whereω isthebase’sroll-pitchvelocity.
xy
• Base linear velocity reward (r ): This reward penalizes vertical motion of the robot’s
blv
base.
r := −1.5·||v ||2,
blv z 2
wherev isthebaselinearvelocityonthez-axis.
z
• Baseorientationreward(r ): Thisrewardpenalizesthebase’sorientationoftherobot.
bo
r := −0.5·||g ||2,
bo xy 2
whereg isthex-and-ycomponentoftheprojectedgravityvector.
xy
• Airtimereward(r ): rewardstherobotforliftingfeetofftheground.
air
r := min([t ,t ,t ])
air air last max
where t represents the cumulative air time of the swing. t is the air time of the
air last
previousswingfoot,andt = 0.75. Summingthisrewardoverthedurationofthestep
max
producesaquantitythatisquadraticintheaverageairtime.
36• Foot lead time symmetry (r ): rewards the robot for positioning the left and right foot
sym
symmetricallyovertime.
r := −0.25·max([0,1−(t /t )])
sym lead lag
We define a lead and lag foot as follows. If foot is currently in ahead of foot in the
i j
robot’s base frame, foot is the lead foot, and foot is the lag foot. The cumulative time
i j
the lead foot has been in front of the lag foot is defined to be t . When the lead foot
lead
changes,wesett tothepreviousvalueoft .
lag lead
• Footswingtrajectorytracking(r ): Thisrewardpenalizesthedeviationofthefootswing
fs
trajectoryfromheuristictrajectories.
(cid:88)
r := 0.5· exp(−||foot traj −foot traj∗ ||2/σ )
fs i,xy i,xy 2 xy
i∈foot
(cid:88)
−50.0· (foot traj −foot traj∗ )2,
i,h i,h
i∈foot
where foot traj and foot traj are x-y and z-component of heuristic foot trajectories.
xy h
The x-y component uses Raibert heuristics (54) and the z-component uses von Mises
distributions(κ = 0.04). Wesetσ to0.2.
xy
• Selected joint position penalty (r ): This reward penalizes deviations from a “neutral”
jp
jointpositionforselectedarmjoints.
(cid:88)
r := − α (q(j)−q (j))2,
jp j 0
j∈M
where q(j) and q (j) represent the current and neutral joint positions of joint j, respec-
0
tively. The joints penalized include shoulder’s roll and yaw with a weight of α = 5, and
shoulderpitchwithaweightofα = 0.25.
37Additional details
Weprovideadditionaldetailson:
• ObservationandstatespacesinTableS1
• ActionspaceinTableS2
• Pre-traininghyperparametersinTableS3
• Fine-tuninghyperparametersinTableS4
• TerraintypesinTableS5
• CommandrangesinTableS6
• DomainrandomizationinTableS7
38Input Dimensionality Actor Critic
BaseLinearVelocity 3 ✓ ✓
BaseAngularVelocity 3 ✓ ✓
JointPositions 26 ✓ ✓
JointVelocities 26 ✓ ✓
ProjectedGravity 3 ✓ ✓
ClockInput 2 ✓ ✓
Commands 3 ✓ ✓
DenoisedProjectedGravity 3 ✓
CommandDrift 3 ✓
FootStatistics 8 ✓
GaitGuidance 10 ✓
HeightMap 121 ✓
RelativeActions 48 ✓
AbsoluteActions 48 ✓
RobotParameters 163 ✓
Table S1: Observation and state spaces. Both the pre-trained model and the actor take in
the proprioceptive observations that are accessible on hardware during deployment. Since we
do not require the critic at test time, we additionally provide the state information to the critic
duringfine-tuning. Thisstateinformationisreadilyaccessibleinsimulationandhelpslearning.
Input Dimensionality
JointPositions 16
KpGains 16
KdGains 16
Table S2: Action space. We experimentally validate our approach on a Digit robot which has
20 actuated joints in total. For consistency with the pre-training dataset, we treat the robot feet
aspassiveandcontroltheremaining16motorswithourmodel. Themodelpredictsthedesired
jointpositionsaswellasthePandDgainsforeachmotor,leadingto48dimensionalactions.
39Parameter Value
NumberofGPUs 4A100s
TrainingEpochs 300
MinibatchSize 4096
Optimizer AdamW
LearningRate 5e-4
LearningRateSchedule cosine
Warm-upEpochs 30
Warm-upSchedule linear
WeightDecay 0.01
OptimizerMomentumβ 0.9
1
OptimizerMomentumβ 0.95
2
TableS3: Pre-traininghyperparameters.
Parameter Value
NumberofGPUs 1A10
NumberofEnvironments 2048
LearningEpochs 5
StepsperEnvironment 24
MinibatchSize 12288
EpisodeLength 20seconds
DiscountFactor(γ) 0.99
GeneralisedAdvantageEstimation(λ) 0.95
InitialNoiseStandardDeviation 0.135
PPOClippingParameter 0.2
Optimizer AdamW
LearningRate(Actor) 1e-5
LearningRate(Critic) 5e-4
LearningRateSchedule(Actor) cosine
LearningRateSchedule(Critic) constant
WeightDecay 0.01
TrainingIterations 2000
TableS4: Fine-tuninghyperparameters.
40TerrainType Fraction(%) Unit Range Image
Flat 12.5 %grade [0,0]
Rough 12.5 cm [-2.5,2.5]
SmoothSlope 12.5 %grade [2,20]
RoughSlope 12.5 %grade [2,20]
Obstacles 25 cm [0.5,5]
Hills 25 scale [0.55,0.75]
Table S5: Terrains. We fine-tune the model with reinforcement learning on an ensemble of
different terrains in simulation. We use six different terrain types and resample variations of
eachterrainperiodically. Terraintypes,samplingranges,andexampleimagesareshownabove.
41Parameter Unit Range Prob. ChangeInt.
ForwardSpeed m/s [-1.0,1.0] 0.5 10sec.
LateralSpeed m/s [-0.5,0.5] 0.5 10sec.
TurningSpeed rad/s [-1.0,1.0] 0.5 10sec.
TableS6: Velocitycommands. Wesamplevelocitycommandsfromtheprovidedrangesusing
a uniform distribution. To make the commands more realistic, we set each dimension to zero
withaprobabilityofone-half. Thecommandsarere-sampledperiodically,atchangeintervals.
Parameter Unit Range Operator Distribution
JointPosition rad [0.0,0.175] additive gaussian
JointVelocity rad/s [0.0,0.15] additive gaussian
BaseLin. Vel. m/s [0.0,0.15] additive gaussian
BaseAng. Vel. rad/s [0.0,0.15] additive gaussian
GravityProjection - [0.0,0.075] additive gaussian
ObservationDelay B(p)×dt [0.0,0.2] uniform
ActionDelay B(p)×dt [0.0,0.2] uniform
MotorOffset rad [0.0,0.035] additive uniform
MotorStrength % [0.85,1.15] scaling uniform
JointDamping % [0.3,4.0] scaling loguniform
JointStiffness % [0.3,1.5] scaling loguniform
Mass % [0.5,1.5] scaling uniform
Size % [0.95,1.05] scaling uniform
KpFactor % [0.9,1.1] scaling uniform
KdFactor % [0.9,1.1] scaling uniform
Gravity m/s2 [0.0,0.67] additive uniform
Friction % [0.3,2.0] scaling uniform
TimeConstant % [0.2,5] scaling uniform
DampingRatio % [0.3,2] scaling uniform
TableS7: Domainrandomization. Ourdomainrandomizationsettingslargelyfollow(33). We
samplevaluesfromtheprovidedrangesforeachparameter. Additiveandscalingoperatorsadd
to and scale the default parameter values, respectively. The ranges specify the [lower, upper]
boundand[mean,standarddeviation]fortheuniformandGaussiandistributions,respectively.
42