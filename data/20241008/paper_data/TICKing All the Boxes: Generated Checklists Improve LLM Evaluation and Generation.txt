Preprint. UnderReview.
TICKING ALL THE BOXES: GENERATED CHECKLISTS
IMPROVE LLM EVALUATION AND GENERATION
JonathanCook∗ TimRockta¨schel
FLAIR,UniversityofOxford CentreforAI,UniversityCollegeLondon
jonathan.cook2@hertford.ox.ac.uk
JakobFoerster DennisAumiller†&AlexWang†
FLAIR,UniversityofOxford Cohere
ABSTRACT
GiventhewidespreadadoptionandusageofLargeLanguageModels(LLMs),itis
crucialtohaveflexibleandinterpretableevaluationsoftheirinstruction-following
ability. Preference judgments between model outputs have become the de facto
evaluation standard, despite distilling complex, multi-faceted preferences into a
single ranking. Furthermore, as human annotation is slow and costly, LLMs are
increasingly used to make these judgments, at the expense of reliability and in-
terpretability. Inthiswork,weproposeTICK(TargetedInstruct-evaluationwith
ChecKlists), a fully automated, interpretable evaluation protocol that structures
evaluations with LLM-generated, instruction-specific checklists. We first show
that,givenaninstruction,LLMscanreliablyproducehigh-quality,tailoredevalu-
ationcheckliststhatdecomposetheinstructionintoaseriesofYES/NOquestions.
Eachquestionaskswhetheracandidateresponsemeetsaspecificrequirementof
the instruction. We demonstrate that using TICK leads to a significant increase
(46.4% → 52.2%) in the frequency of exact agreements between LLM judge-
ments and human preferences, as compared to having an LLM directly score an
output.WethenshowthatSTICK(Self-TICK)canbeusedtoimprovegeneration
quality across multiple benchmarks via self-refinement and Best-of-N selection.
STICKself-refinementonLiveBenchreasoningtasksleadstoanabsolutegainof
+7.8%,whilstBest-of-NselectionwithSTICKattains+6.3%absoluteimprove-
mentonthereal-worldinstructiondataset,WildBench. Inlightofthis,structured,
multi-facetedself-improvementisshowntobeapromisingwaytofurtheradvance
LLMcapabilities.Finally,byprovidingLLM-generatedcheckliststohumaneval-
uatorstaskedwithdirectlyscoringLLMresponsestoWildBenchinstructions,we
notablyincreaseinter-annotatoragreement(0.194→0.256).
1 INTRODUCTION
Instruction-tuned Large Language Models (LLMs) are widely used as conversational assistants,
whereusersexpectresponsestocloselyfollowtheirintents(Weietal.,2022a;Mishraetal.,2022;
Bai et al., 2022a; Ouyang et al., 2022). The broad usage of LLMs creates a critical demand for
reliable, flexible, and transparent ways of evaluating their instruction-following abilities. How-
ever,standardevaluationmethods,suchaspreferencelabeling(Ouyangetal.,2022),directscoring
(Novikovaetal.,2018;Wangetal.,2023b),andElorating(Baietal.,2022a;Glaeseetal.,2022),
tendtoobscurethereasoningbehindevaluations. Thesemethodsalsooftenresultinsignificantdis-
agreements,bothamonghumanannotators(Hoskingetal.,2024)andbetweenmodelsandhumans
(Qinetal.,2024;Zhengetal.,2023).
To address these limitations, we introduce TICK (Targeted Instruct-evaluation with ChecKlists),
a novel approach to LLM-as-judge evaluation that uses the judge LLM to decompose instructions
∗WorkdoneduringaninternshipatCohere.
†Equaladvising.
1
4202
tcO
4
]IA.sc[
1v80630.0142:viXraPreprint. UnderReview.
Instruction
Summarize the first season of the TV Se-
ries“Mr. Robot”usingasimilartonetothe
show’smaincharacter, ElliotAlderson. Use
subheadingsandbulletpointsforformatting,
andstayunder500words.
GPT-4oGeneratedChecklist
• Istheresponseasummaryofthefirstsea-
sonoftheTVseries“Mr.Robot”?
• Istheresponsewritteninatonesimilarto
theshow’smaincharacter,ElliotAlderson?
• Doestheresponseusesubheadingsandbul-
letpointsforformatting?
• Istheresponseunder500words?
Figure1: Left: DiagramofTICKanditsdownstreamusesofaugmentinghumanevaluation, per-
formingself-refinement,andresponsefiltering.Right:Exampleofageneratedevaluationchecklist.
into checklists consisting of a series of YES/NO evaluation questions. These checklists provide
interpretable,fine-grainedassessmentsofwhetheramodelresponsesatisfiesspecificrequirements
of the instruction. Crucially, TICK eliminates manual effort in checklist creation, a substantial
cost for existing checklist-based benchmarks (Qin et al., 2024; Wen et al., 2024). We rigorously
demonstrate that current LLMs can already generate checklists matching the quality of human-
writtenones.Inexperiments,weshowthatusingTICKleadstoanabsoluteincreaseinthefrequency
ofexactagreementsbetweenanLLMjudgeandhumanpreferencesof5.8%.
Building on this, we introduce STICK (Self-TICK), an approach to in-context self-improvement
whereLLMsiterativelyrefinetheirresponsesbasedonTICKself-evaluations. Wedemonstratethat
STICK enables LLMs to achieve significant performance gains across several benchmarks with-
out the need for dataset-specific prompting or pre-existing human-written checklists. Specifically,
Command-R+showsa6.5%absoluteimprovementonInFoBench(Qinetal.,2024)anda7.1%ab-
solutegainonWildBench(Linetal.,2024),outperformingvanillaSelf-Refine(Madaanetal.,2023).
OnLiveBench(Whiteetal.,2024),STICKrefinementsenableCommand-R+toachievea3.8%im-
provementandGPT-4otogain0.8%, whereasvanillaSelf-Refineleadstosubstantialdegradation.
Theseimprovementsspantasksforwhichin-contextself-improvementhaspreviouslyprovenchal-
lenging,suchasmathematics,reasoning,andcoding(Huangetal.,2024;Kamoietal.,2024;Tyen
etal.,2024),aswellasamplifyingimprovementsontasksthathavepreviouslybeenshowntoben-
efitfromself-critiques,suchasconstrainedinstruction-following(Madaanetal.,2023). Whenused
forBest-of-Nresponseself-selection,STICKimprovesongreedydecodingby5.1%onInFoBench
and 5.3% on WildBench, and even outperforms selection by a general-purpose reward model. Fi-
nally,weexplorewhetherLLM-generatedchecklistscanassisthumanevaluatorsbyaugmentingthe
annotationprocessandfindsignificantimprovementstointer-annotatoragreement.
Insummary,wemakethefollowingcontributions:
1. We rigorously show that LLMs can generate evaluation checklists similar in quality to those
writtenbytrainedhumanannotatorsacrossmultiplediverseinstruction-followingdatasets.
2. WeintroduceTICK,achecklist-based,automaticevaluationprotocolthatyieldsstrongeragree-
mentwithhumansthanothergeneral-purposeLLM-as-judgeevaluations. BecauseTICKcanbe
fullyautomated,itischeaperandfastertorunthanexistingchecklist-basedevaluations,andcan
beappliedtoarbitraryinstruction-followingdatasets.
3. WeleverageSelf-TICK(STICK)tosubstantiallyimproveinstruction-followingabilityviaself-
refinementandBest-of-Nselectiononmultiplechallengingbenchmarks.
4. WeexploreusingLLM-generatedcheckliststoassisthumanevaluatorstaskedwithdirectlyscor-
inganLLMoutput,andfindthatthisimprovesinter-annotatoragreement.
2Preprint. UnderReview.
Command-R+ GPT-4o
Tasks
Base Self-Refine STICK Base Self-Refine STICK
Overall 32.0 23.7(↓8.3) 35.8(↑3.8) 55.4 47.1(↓8.3) 56.2(↑0.8)
Coding 18.8 9.1(↓9.7) 22.7(↑3.9) 50.4 36.4(↓14.0) 51.6(↑1.2)
DataAnalysis 25.9 5.3(↓20.6) 29.8(↑3.9) 52.4 27.2(↓25.2) 52.5(↑0.1)
Instructions 69.6 60.5(↓9.1) 75.8(↑6.2) 73.3 62.8(↓10.5) 76.2(↑2.9)
Language 24.6 13.8(↓9.8) 24.1(↓0.5) 50.9 51.4(↑0.5) 50.4(↓0.5)
Mathematics 23.7 23.6(↓0.1) 25.5(↑1.8) 52.3 51.8(↓0.5) 53.1(↑0.8)
Reasoning 29.2 30.0(↑0.8) 37.0(↑7.8) 53.3 52.7(↓0.6) 53.3(0)
Table 1: A single step of self-refinement on LiveBench with Command-R+ and GPT-4o, using
STICK to form self-critiques. Unstructured self-critiques are included as a baseline (Self-Refine),
alongwitheachLLM’sbaseperformance.
2 RELATED WORK
Instruction-FollowingEvaluation: Therehavebeenmanyeffortstoimprovetheevaluationand
benchmarkingofLLMs’instruction-followingability.Someofthesebenchmarksaggregateinstruc-
tionsetsfromadiverserangeofsourcestomeasuregeneralinstruction-followingabilityandusea
judgeLLMtoscoreoutputs(Lietal.,2023;Chiaetal.,2023;Linetal.,2024). Othersdecompose
instructionsintochecklists,madeupofYES/NOquestionsorPASS/FAILcriteriathataresponse
should meet (Zhou et al., 2023; Jiang et al., 2024; Qin et al., 2024; Wen et al., 2024). For exam-
ple, the WildBench (Lin et al., 2024) dataset pairs its instructions with checklists (generated by
two LLMs, then reviewed by humans) that are included in the evaluation prompt to get a score or
preference from a judge LLM, but not explicitly answered or used to form a metric. Approaches
to evaluation used in these works are therefore hard to make use of outside of the benchmarks
themselves, relying heavily on humans for instruction and checklist curation. We instead design
adynamicevaluationprotocolthatcanbeemployedon-the-flyandthereforeseamlesslyintegrated
intocustomevaluationworkflows,orusedtosteerhighqualitygeneration.Meanwhile,weavoidthe
costofhavinghumansintheloopandenableourevaluationqualitytoimproveasLLMsimprove.
Language Critiques: LLM critiques are an intuitive way of addressing the “black box” nature
of evaluations. These critiques are intended to point out the strengths and weaknesses of outputs
generated by the same LLM, or a different LLM. Critiques can be used to improve the quality of
overall evaluations performed by an LLM judge or reward model (Ankner et al., 2024; Bai et al.,
2022b;Wangetal.,2023a;Yeetal.,2024;Sunetal.,2024), informhumanevaluations(Saunders
et al., 2022; McAleese et al., 2024), or provide feedback that can be used to refine a response in-
context(Scheureretal.,2023;Tianetal.,2024;Madaanetal.,2023;Yuanetal.,2024). Meanwhile,
anumberofpapersprovideevidencethatnaivelypromptingLLMstoself-correctorfindreasoning
errors can lead to performance degradation (Huang et al., 2024; Tyen et al., 2024; Kamoi et al.,
2024). Byusingthetargetedandstructurednatureofchecklist-basedevaluations,weachieveself-
refinementthatoutperformsunstructuredfeedbackandworksonabroadrangetasks.
3 TICK: TARGETED INSTRUCT-EVALUATION WITH CHECKLISTS
We present an approach to automatically and robustly evaluating instruction-tuned LLMs that is
notrestrictedtoanyparticulardataset. Todoso,weuseanLLM togeneratechecklistsoftargeted
YES/NOevaluationquestionsforagiveninstruction.WethenalsouseanLLMtoevaluateresponses
withrespecttoeachchecklistquestion,exploitingthefactthatthedecomposedtaskofansweringa
single,targetedquestionismuchsimplerthancomingupwithaholisticscoreorpreferenceranking.
Individual checklist answers can then be aggregated to produce an overall score or preference. In
thissection,weprovidedetailsforeachofthesestepsandexperimentallyvalidatetheireffectiveness
byanalysingagreementbetweenLLMsandapooloftrainedhumanannotatorsateachstage. All
promptsusedareincludedinAppendixG.
3Preprint. UnderReview.
SimilaritytoH∗
ChecklistSource
BLEU ROUGE-1F1 ROUGE-2F1 ROUGE-LF1 CountMAE
GPT-4o 0.759 0.621 0.417 0.593 1.410
Command-R+ 0.709 0.570 0.357 0.534 1.416
Llama3.1-70B 0.759 0.623 0.418 0.593 1.459
H′ 0.733 0.611 0.399 0.583 2.158
Table2: Similaritybetweenchecklistsfromvarioussources,human-writtenground-truthchecklists
(H∗), and alternate human-written checklists (H’) in terms of word overlap metrics and question
count.
3.1 APPROACH
3.1.1 GENERATINGCHECKLISTS
Foragiveninstruction,weseektogenerateachecklist,i.e.alistofYES/NOquestionsthateachask
aboutadifferentrequirementoftheinstruction.AsinQinetal.(2024),weenforcethateachquestion
shouldbephrasedsuchthatananswerofYEScorrespondstocorrectlymeetingtherequirementthat
the question is targeting. To obtain these instruction-specific checklists, we prompt an LLM with
a few-shot template that specifies the instruction and the YES/NO constraint. This prompt also
mentions that checklists should cover all criteria explicitly stated in an instruction, as well as any
implicitcriteriathataregenerallyimportantforaninstruction’sproblemdomain. Figure1showsan
exampleinstructionandanLLM-generatedchecklist.
3.1.2 USINGCHECKLISTS
Oncewehavegeneratedchecklists,wepromptajudgeLLMwitheachchecklistquestiontoevaluate
thequalityofaresponse. TICKusesthesameLLMtogenerateandanswerchecklists,butdifferent
LLMsmaybeusedforeachstep. Wedenotea astheanswertothej-thquestioninthechecklist
i,j
for the i-th instruction. The quality of a response for a single instruction i is measured using the
(cid:80)
checklistPassRate(PR),definedasPR = a /n ,wheren isthelengthofthei-thchecklist
j i,j i i
anda ∈ {0,1}(i.e.,NO→0andYES→1). Theaggregateinstruction-followingqualityacross
i,j
allexamplesinadatasetofinstructionsismeasuredusingtheDecomposedRequirementsFollowing
(cid:80) (cid:80)
Ratio(DRFR; Qinetal.,2024),definedasDRFR = a / n ,i.e.,thepercentageoftotal
i,j i,j i i
checklistquestionsthatwerecorrectlyansweredfromthemodel’sresponses.
3.2 VALIDATION
3.2.1 GENERATINGCHECKLISTS
Similarity to human checklists: To verify that LLM-generated checklists are high quality, we
compare them to checklists written by trained annotators on Internal, an internal test set of 612
instructions.1 Theseinstructionshavebeenwrittenbythesamepoolofannotators,andareintended
to resemble complex, real-world use cases of instruction-tuned LLMs, ranging from open-ended
question answering to highly structured outputs. Sample instructions are available in Appendix
D. For each instruction in Internal, we collect three checklists written independently by different
annotators. Theannotatorsaregivenpreciserequirementsforwritingchecklistsaswellasasetof
highqualityexamples(seeAppendixH.1). Fromthesechecklisttriplets,weformasetofhuman-
written ground-truth checklists H∗ by manually selecting the one that best meets the annotation
requirements for each instruction in the dataset. In rare instances where none of the checklists
fully meet the specified requirements, we select the best and manually make corrections. We use
the remaining two checklists for each instruction to form a set H′ of alternative human-written
checklists.
WegeneratechecklistswithGPT-4o(OpenAI,2024),Command-R+(Cohere,2024),andLlama3.1-
70B-Instruct(Dubeyetal.,2024)(weomitthe“Instruct”forbrevity). Wecomparethesechecklists,
as well as H′, against H∗ in terms of BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and
1Wewillbeopen-sourcingthisdatasetplusthegeneratedchecklistsforusebytheresearchcommunity.
4Preprint. UnderReview.
ScoreCorrelation ChecklistEval. Question-LevelAccuracy
ChecklistGen.
Internal InFoBench GPT-4o 0.826
GPT-4o 0.772 0.853 Command-R+ 0.781
Command-R+ 0.713 0.776 Llama3.1-70B 0.778
(a)PearsoncorrelationbetweenCommand-R+ (b) Accuracy when answering individual
checklistpassrateswhenevaluatedwithLLM- checklist questions on Internal, treating a ma-
andhuman-writtenchecklists. Weuseannota- jority vote among three trained annotators as
tors and GPT-4 to answer checklist questions ground truth. Models are prompted to output
forInternalandInFoBenchrespectively. a chain-of-thought before reaching a final an-
swerforeachquestion.
Table3:(a)EvaluationofthesimilaritybetweenLLM-generatedandhuman-writtenchecklistques-
tions,and(b)similaritybetweenLLM-generatedandhuman-writtenchecklistanswers.
questioncount.2 Since H′ iscomprisedof twochecklistsperinstruction, wecompareeach toH∗
andtaketheaverageofeachmetric. Forconsistency,wealsogenerateansecondchecklistforeach
instructionfromeachLLM,andalsoaverageresultsoverthetwochecklists.
ResultsforthisexperimentareshowninTable2. WefindthatGPT-4oandLlama3.1-70Bgenerate
checkliststhatmorecloselymatchthoseinH∗ thanthealternativehuman-writtenchecklistsinH′
do. There is particularly high variation between H′ and H∗ in terms of question count, which
weobservetobebecausedifferentannotatorsassumeddifferentlevelsofgranularitywhenwriting
checklists. Command-R+ has the lowest string-level similarity, but is close in terms of question
count. These results indicate that LLMs can produce checklists that strongly resemble the best
human-writtenchecklists. ExamplesareavailableinAppendixE.
Impact on scores when replacing human checklists: We also verify the quality of LLM-
generatedchecklistsbycheckingwhethertheycanproducecomparablepassratestohuman-written
checklistswhenusedeitherbyhumanannotatorsoranLLM-as-judge. Thisismeantasanotherval-
idationofchecklistgenerationalone, anddoesnotyetconsiderhowwelltheLLMgeneratingthe
checklistcananswerthatsamechecklist. ForInternal,weusethepooloftrainedhumanannotators
toanswerchecklistquestionsusingeitherasetofmodel-writtenchecklistsorH∗. Eachevaluation
is performed independently by three annotators and the majority vote for each question is used to
compute pass rates. To considerthe impact of LLM-generated checklistswhen using an LLM-as-
judge,wealsogeneratechecklistsforpromptsfromInFoBench(Qinetal.,2024). InFoBenchisa
instruction-followingbenchmarkthatprovidesinstruction-specificevaluationchecklistswrittenby
expert human annotators. To answer InFoBench checklist questions, we follow the recommended
evaluationprotocolofusingGPT-4(OpenAI,2023)asajudgewiththebenchmark’sofficialprompt.
Table3ashowsthatthepassrateswhenusingchecklistsgeneratedbyGPT-4oorCommand-R+are
highly correlated with pass rates when using H∗, with GPT-4o checklists exhibiting the strongest
correlation. This result demonstrates that LLM-generated checklists are functionally similar to
human-writtenchecklists,furthervalidatingtheiruse.
3.2.2 USINGCHECKLISTS
Question-level agreement with humans: To verify that an LLM can reliably answer generated
checklistquestions,wefirstinvestigatehowwellthegeneratedanswersagreewiththoseoftrained
humanannotators. WeusethepreviouslygatheredsetofhumanmajorityvoteanswersforInternal
asgroundtruthandcomputetheaccuracyofchecklistanswersgeneratedbyGPT-4o,Command-R+
andLlama3.1-70B.Table3bshowsthateachoftheLLMsconsideredachievesreasonablequestion-
level accuracy, but that GPT-4o is the strongest in this regard. In Figure 2, we show how GPT-
4o’s accuracy changes under different evaluator settings with varying inference costs. Having the
evaluator output a Chain-of-Thought (CoT) (Wei et al., 2022b) prior to making a final judgement
substantiallyimprovesaccuracy. Samplingkevaluations,withCoTincluded,andtakingamajority
vote (maj@k) yields further improvement, with higher k leading to a more substantial increase.
TheseresultsdemonstratethatTICKbecomesmorereliableaswescaleinferencecompute.
2Forfurtheranalysisofthelengthsofgeneratedandhuman-writtenchecklists,seeAppendixB.1.
5Preprint. UnderReview.
Pairwiseagreementwithhumans: Next,weinvestigatehowwellTICKagreeswithhumanpair-
wisepreferences,whichisthedefactostandardforhumanevaluationofmodeloutputs. Toproduce
apreferencejudgementbetweentworesponses,wescoreeachresponseusingTICKandsaythatthe
response with the higher checklist PR is preferred. To gather human preference pairs, we provide
annotatorswithapairofresponsesfromdifferentmodelsforagiveninstructionfromInternal,then
ask them to indicate their preference on an integer sliding scale from 1, meaning “Response A is
much better than Response B”, to 5, meaning the reciprocal strong preference (further details in
AppendixH.2). Eachresponsepairistriplyannotatedandwecomputetheaveragepreferencescore
p¯acrossthesethreeannotations. Wethenbineachaveragepreferenceintoawin(1≤p¯<2.5),tie
(2.5≤p¯≤3.5),orloss(3.5<p¯≤5).
We follow Qin et al. (2024) and use the Pair-
wise Label Distance (PLD) to measure agree-
mentbetweenTICKandhumanpreferencela-
bels. PLD is a metric designed to capture the
intuition that predicting a win as a tie is not
as bad as predicting a win as a loss. It takes
a label and a prediction, and produces a value
in {0,1,2}. A PLD of 0 indicates the exact
match of a preference label (win, loss or tie),
(i.e., PLD-0 is equivalent to label accuracy in
this setup). A PLD of 1 implies a misclas-
sification in scenarios where the ground truth
label was a tie. A PLD of 2 corresponds to
an inverted preference relative to the human
preferencelabel(e.g.,predictinglosswhenthe
Figure 2: Question-level accuracy of GPT-4o
groundtruthlabeliswin). TheWeightedPair-
checklistanswersonInternal.
wiseLabelDistance(WPLD)isthendefinedas
WPLD = (cid:80)2 i (cid:80)N I[PLD = i], where N is the number of instructions. The WPLD thus
i=0 N j=0 j
rangesfrom0–2,withalowervalueindicatingstrongeragreement.
WecomparemakingpreferencejudgmentsviaTICKagainstdirectlypromptingthejudgeLLMto
expressapreference(Preference)andscoringeachresponseindividually(DirectScoring).Fordirect
scoring,wepromptthejudgeLLMtoproducea1–5scoreforeachresponse,andwesaythehigher
scoring response is preferred. We also include a hybrid of TICK and direct scoring (Check-then-
Score),wherechecklistsareincludedinthejudgeprompt,butjudgesarenotrequiredtoexplicitly
answereachchecklistquestion,similartohowcuratedchecklistsareusedinWildBenchLinetal.
(2024). We have the judge LLM use CoT in all cases, but do not use majority voting. Response
pairsareformedoutofgenerationsfromCommand-R+,GPT-4oandClaude-3-Sonnet(Anthropic,
2023). WeuseGPT-4oasthejudgeLLM.
In Table 4, we see that TICK agrees most strongly with human preferences, in terms of achieving
the lowest overall WPLD. TICK is also the only LLM-as-judge evaluation to achieve a PLD of 0
moreoftenthannot. Check-then-scorealsoagreesmorestronglywithhumansthandirectscoring,
which confirms the general utility of checklists in evaluation. However, the fact that Check-the-
ScorestilllagsbehindTICKprovidesevidencethatexplicitlyansweringandaggregatingchecklist
answers is necessary to fully utilise checklists. Despite the fact that preference judgements are
verycommon, promptingthejudgetodirectlyproduceapreferenceproduceslowagreementwith
PairwiseAgreementw/Humans
LLM-as-JudgeEval.
PLD-0 PLD-1 PLD-2 WPLD
Preference 0.293 0.497 0.210 0.917
DirectScoring 0.464 0.488 0.048 0.583
Check-then-Score 0.487 0.472 0.041 0.553
TICK 0.522 0.443 0.035 0.514
Table 4: Agreement between different LLM-as-Judge evaluations and pairwise preferences from
trainedhumanannotatorsonInternal. GPT-4oisusedasthejudgeLLM.
6Preprint. UnderReview.
Figure3: Fouriterationsofself-refinementwithCommand-R+,usingSTICKtoformself-critiques.
Unstructuredself-critiquesareincludedasabaseline(vanillaSelf-Refine).Multi-turnconversations
areexcludedfromtheWildBenchevaluation. GPT-4isusedasthejudgeLLMforeachbenchmark.
humans. Overall, these results show that LLM-as-judge evaluations benefit from a more precisely
structured and granular scoring protocol, even when that protocol is task-agnostic and generally
applicable,asinthecaseofTICK.
4 IN-CONTEXT SELF-IMPROVEMENT WITH STICK (SELF-TICK)
Having shown that TICK provides a signal of model response quality on par with trained human
annotators,weinvestigatehowitcanbeusedtoimproveresponses. WefirstexploreusingSTICK
(Self-TICK)evaluationsasfeedbackforself-refinement. OurhypothesisisthatbecauseSTICKis
targetedandinterpretable,itismoreinformativeforresponserefinementthanunstructuredfeedback
suchasvanillaSelf-Refine(Madaanetal.,2023). Secondly, weinvestigateSTICK’seffectiveness
atBest-of-Nselection. Inbothcases,weareusingthegeneratingLLMasitsowntest-timejudge.
4.1 SELF-REFINEMENTWITHCHECKLISTSASFEEDBACK
Approach: Given an instruction, we first generate an initial response from an LLM. We then
usethesameLLMtogenerateachecklistandevaluateitsoriginalresponseagainstthischecklist.
As checklist evaluations contain precise information about how a response succeeded or failed,
they can be used as targeted, actionable feedback. Whenever the previous response did not pass
all checklist evaluations, we prompt the LLM with a refinement prompt template (see Appendix
G), which contains the instruction, the previous response, and the STICK evaluations, in order to
generatearefinedresponse. Thisprocesscanbeoptionallyrepeatedforseveraliterations.
Validation: We compare self-refinement with STICK against the use of unstructured feedback
gathered by prompting the LLM to provide a detailed critique of its previous response. We refer
to this baseline as vanilla Self-Refine (Madaan et al., 2023). We first run four iterations of self-
refinementusingeachapproachonInFoBench,andWildBench(Linetal.,2024),whichisadataset
ofreal-worlduserqueries. InFoBenchevaluatesresponsesusingDRFRwithitsownexpert-written
checklists. WildBench uses WB-Score, a 1-10 LLM-as-judge rating for each response. We use
GPT-4asjudgeforeachbenchmarkandevaluatetheself-refinementofCommand-R+.
InFigure3,weshowthatSTICKsignificantlyimprovesresponsesacrossmultipleiterationsofself-
refinement,andisconsiderablymoreeffectivethanSelf-Refine. ThisimprovementholdsacrossIn-
FoBench(+6.5%),whichevaluateswithahuman-writtenchecklistnotseenduringself-refinement,
andWildBench(+7.1%),whichusesaholisticresponsescorethatisinlinewithhowLLMjudges
arecommonlyused. Bythefourthiteration,weseeresponsequalitystarttoplateauorevenregress,
highlightingthatsustainingpurelyin-contextself-improvementremainsasignificantchallenge.
WethenconsiderwhetherSTICKrefinementscanimproveresponsesinstrictlyverifiablesettings,
such as math or code generation, where response correctness is deterministically evaluated. We
7Preprint. UnderReview.
InFoBench WildBench(Single-Turn)
ScoringFunction
DRFR Precision WB-Score Precision
GreedyDecoding 0.843 N/A 64.9 N/A
RewardModel(ArmoRM) 0.863 0.306 67.5 0.323
DirectSelf-Scoring 0.848 0.191 65.7 0.258
STICK 0.894 0.611 71.2 0.528
Table5: Best-of-8selectiononInFoBenchandWildBenchusingCommand-R+withSTICK,com-
paredwithdirectself-scoringandanexternalrewardmodel(ArmoRM),aswellasgreedydecoding.
Multi-turnconversationsareexcludedfromtheWildBenchevaluation.
choosethechallengingbenchmarkLiveBench(Whiteetal.,2024)totestthis. LiveBenchcontains
frequently updated questions, spanning six task categories, and answers are scored automatically
accordingtoobjectivegroundtruthvalues. Self-refinementinthissettingisthereforeequivalentto
self-correction. WereporttheresultsforbothCommand-R+andGPT-4oinTable1,againcompar-
ingtovanillaSelf-Refine. WefindthatresponsesimprovewithasingleiterationofSTICK,butstart
todegradethereafter. Withunstructuredself-critiques,responsesimmediatelydegradeinmostcat-
egories. Command-R+,forwhichbaseperformanceisconsiderablybelowthatofGPT-4o,benefits
themostfromSTICKrefinement, makesaparticularlylargegainonreasoningtasks. BothLLMs
alsobenefitconsiderablyonexplicitinstruction-following,whichiswherechecklist-structuredfeed-
backismostpredictablyuseful,beingthesettinginwhichpriorworkonchecklist-basedevaluations
havebeenfocused(Qinetal.,2024;Wenetal.,2024).
4.2 BEST-OF-NSELECTIONWITHCHECKLIST-BASEDSCORES
Approach: AcommonapproachtomaximisingresponsequalityforagivenLLMistosampleN
responses to an instruction, and then use a scoring function, such as a reward model or LLM-as-
judge, toselectthebestresponseBest-of-Nselectionthusproduceshigherqualityresponsesatthe
costofadditionalinferencecompute(Snelletal.,2024). Focusingspecificallyonself-selection,as
thisassumesnoaccesstoanexternalrewardmodelorsuperiorLLM,weuseSTICKforBest-of-N
selectionbygeneratingN candidateresponsestoaninstructionfromanLLM,usingthesameLLM
toself-evaluateeachcandidatewithSTICK,andusingtheSTICKscoreforselection.
Validation: WecompareSTICKforBest-of-Nselectionagainstdirectself-scoring(i.e.,prompt-
ing for a single holistic score). We include using an external reward model, ArmoRM-Llama3-
8B-v0.1 (Wang et al., 2024a), as an additional baseline, despite the fact that doing so breaks the
assumption of only having access to the generating LLM for scoring responses. We evaluate on
InFoBenchandWildBench,againevaluatingbyusingeachbenchmarks’standardevaluationmetric
(defined above). For each score function, if multiple generations have the same maximum score
accordingtothatscorefunction,wekeepallofthetiedresponses. Wethencomputethetruescore
according to the task’s actual evaluation protocol, and average across all selected responses in the
caseofties. Wealsocomputetheprecisionofeachscorefunction. Whentherearenotiesunderthe
Best-of-Nselectingscorefunctionorthegroundtruthscorefunction,computingprecisionreduces
tocomputingaccuracy. Whenthereareties,precisionpenalisesselectinganyadditionalresponses
thatarenotthebest,ortyingforbest,underthegroundtruthscorefunction. However,unlikerecall,
precision does not penalise failing to select more than one response out of any that are tying for
bestunderthegroundtruthscorefunction. WeuseCommand-R+togenerateresponses,aswellas
self-evaluateresponseswithSTICKanddirectself-scoring.3
InTable5,weshowresultsforN = 8andobservethateachmethodimprovesontheperformance
of greedy decoding. STICK achieves the most significant improvement on each benchmark, with
gains of +5.1% on InFoBench and +5.3% on WildBench. STICK is the most precise scoring
function,meaningthatitmostcloselyalignswithselectionsmadeundereachbenchmarks’ground
truthevaluation.STICKismorepreciseonInFoBenchthanWildBench,whilsttheinverseistruefor
the other scores. This is likely because InFoBench scores responses against evaluation checklists,
likeSTICK,whereasWildBenchscoresresponseswithaholisticrating,likedirectself-scoring.
3WerepeatthisexperimentwithLlama3.1-70BinAppendixC.
8Preprint. UnderReview.
Eval. Protocol Inter-AnnotatorAgreement Command-R+Avg. Score
DirectScoring 0.194 3.347
Check-then-Score 0.256 3.351
Table 6: Inter-Annotator Agreement (Krippendorff’s alpha) among triply annotated labels when
providing a 1-5 score for Command-R+ responses to WildBench instructions. The average score
giventoCommand-R+isalsoreported.
5 ASSISTING HUMAN EVALUATORS WITH GENERATED CHECKLISTS
5.1 MOTIVATION
Inspiteofrecentprogressinautomaticevaluation(Ankneretal.,2024;Chiang&Lee,2023;Verga
et al., 2024; Vu et al., 2024; Wang et al., 2024a;b; Ye et al., 2024), human evaluation remains a
criticalcomponentofLLMqualityassessment. WethusinvestigatewhetherLLM-generatedeval-
uationchecklistscanhelpwitharrivingataconsistentscoreforagivenresponse. Usingchecklists
partially decomposes the annotation task to be more cognitively feasible, and helps to ensure that
specificconsiderationsrelevanttoeachinstructionarenotmissed.
5.2 CASESTUDY: SCORINGRESPONSESTOWILDBENCHINSTRUCTIONS
WeconducttworoundsofhumanevaluationonasetofCommand-R+responsestoWildBenchin-
structions. Inbothrounds,annotatorsprovideanintegerscorefrom1-5foreachresponse(further
details in Appendix H.3). In one round, annotators are asked to first answer checklist questions
generated by GPT-4o before providing each overall score (i.e., check-then-score). Annotators are
instructed to use the checklists to inform their score where appropriate, but not to limit their as-
sessment to the checklists. This holistic scoring is important for human evaluation, as we found
thatanumberofedgecasesarisethatcanleadevaluatorstoanswermostchecklistquestionswith
YES andstill justifiablyprovide alow overallscore(see AppendixH.4 foran example). Eachre-
sponse is triply annotated and we compute inter-annotator agreement using Krippendorff’s alpha
(Krippendorff,1980).
Results in Table 6 indicate that annotating the checklist before scoring yields stronger agreement
amongevaluatorsthandirectscoring. WealsofindthattheaverageevaluationscoreforCommand-
R+ across both settings stays consistent, implying that the increase in agreement corresponds to
variance reduction without having a biasing effect on the aggregate score. However, we note that
agreementisstilllow,despitebeingimproved,whichhighlightsthechallengesofgatheringannota-
tionswithstrongagreementonreal-worldinstructiondata.
6 CONCLUSION
WeintroduceTICK,afullyautomaticevaluationprotocolthatstructuresevaluationswithanLLM-
generated, instruction-specific checklist. We show that LLMs can produce high-quality checklists
that improve agreement between judge LLMs and humans. Because TICK is fully automatic, it
can easily and cheaply be applied in new settings, avoiding the need for humans to write or re-
view checklists. We next demonstrate that STICK (Self-TICK) can be used for in-context self-
improvementbyeitherself-refinementorbyBest-of-Nselection. Ourexperimentsshowthatboth
strategiesofemployingSTICKleadtosubstantialimprovementsoverbaselinesonmultiplediverse
instruction-followingdatasets,includingLiveBench,whichcoverschallengingmath,code,andrea-
soningpromptsthatbaselinesfailtoimproveon. Finally,whenweprovidehumanannotatorsLLM-
generatedchecklistsforevaluatingLLMoutputs,wefindinter-annotatoragreementimprovescon-
siderably. Overall, we show that LLMs are capable of accurately evaluating instruction-following
abilitywhenusingstructuredchecklists,anddemonstratethepotentialofthisrichfine-grainedfeed-
backtofurtherimproveLLMcapabilities.
9Preprint. UnderReview.
LIMITATIONSANDFUTUREWORK
TICKandSTICKareusefultoolsforevaluationandself-improvement,respectively. However,we
acknowledgethatchecklistsareonlyoneheuristicforstructuringevaluation;learnedordiscovered
evaluationstructuresareanexcitingdirectionforfuturework. Checklistevaluationsdonotpresent
anadvantageinallsettings,especiallygiventheadditionalinferencecostofgeneratingthechecklist.
Forexample,basicknowledgeretrievalisbestevaluatedassimplycorrectorincorrect. Relyingon
LLMsatallstepsintheevaluationprotocolmayalsopropagate,andevenexacerbate,LLMbiases.
In this work, we do not investigate self-improvement by fine-tuning on synthetic, STICK-selected
data,butdoingsoisanaturalnextstep. Trainingrewardmodelstoconditionon,orjointlyproduce,
checklistevaluationsisalsoapromisingdirectionforfuturestudy.
REFERENCES
Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, and Prithviraj Ammanabrolu.
Critique-out-LoudRewardModels. arXivpreprintarXiv:2408.11791,2024.
Anthropic. TheClaude3ModelFamily: Opus,Sonnet,Haiku,2023. URLhttps://www-cdn.
anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_
Card_Claude_3.pdfs.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain,StanislavFort,DeepGanguli,TomHenighan,NicholasJoseph,SauravKadavath,Jackson
Kernion,TomConerly,SheerElShowk,NelsonElhage,ZacHatfield-Dodds,DannyHernandez,
TristanHume,ScottJohnston,ShaunaKravec,LianeLovitt,NeelNanda,CatherineOlsson,Dario
Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared
Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human
Feedback. arXivpreprintarXiv:2204.05862,2022a.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Ols-
son, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-
Johnson,EthanPerez,JamieKerr,JaredMueller,JeffreyLadish,JoshuaLandau,KamalNdousse,
KamileLukosiute,LianeLovitt,MichaelSellitto,NelsonElhage,NicholasSchiefer,Noem´ıMer-
cado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna
Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly,TomHenighan,TristanHume,SamuelR.Bowman,ZacHatfield-Dodds,BenMann,Dario
Amodei,NicholasJoseph,SamMcCandlish,TomBrown,andJaredKaplan. ConstitutionalAI:
HarmlessnessfromAIFeedback. arXivpreprintarXiv:2212.08073,2022b.
YewKenChia,PengfeiHong,LidongBing,andSoujanyaPoria.INSTRUCTEVAL:TowardsHolis-
ticEvaluationofInstruction-TunedLargeLanguageModels. arXivpreprintarXiv:2306.04757,
2023.
Cheng-Han Chiang and Hung-yi Lee. A closer look into using large language models for auto-
maticevaluation. InHoudaBouamor,JuanPino,andKalikaBali(eds.),FindingsoftheAssoci-
ationforComputationalLinguistics: EMNLP2023,pp.8928–8942,Singapore,December2023.
Association for Computational Linguistics. URL https://aclanthology.org/2023.
findings-emnlp.599.
Cohere. Command R+ Model, 2024. URL https://docs.cohere.com/docs/
command-r-plus.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 Herd of
Models. arXivpreprintarXiv:2407.21783,2024.
Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-
bethRauh,LauraWeidinger,MartinJ.Chadwick,PhoebeThacker,LucyCampbell-Gillingham,
Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth
Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sona
10Preprint. UnderReview.
Mokra´, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William
Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey
Irving. Improvingalignmentofdialogueagentsviatargetedhumanjudgements. arXivpreprint
arXiv:2209.14375,2022.
Tom Hosking, Phil Blunsom, and Max Bartolo. Human Feedback is not Gold Standard. In The
Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria,
May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=
7W3GLNImfS.
JieHuang,XinyunChen,SwaroopMishra,HuaixiuStevenZheng,AdamsWeiYu,XinyingSong,
andDennyZhou. Largelanguagemodelscannotself-correctreasoningyet. InTheTwelfthInter-
nationalConferenceonLearningRepresentations,ICLR2024,Vienna,Austria,May7-11,2024.
OpenReview.net,2024. URLhttps://openreview.net/forum?id=IkmD3fKBPQ.
YuxinJiang,YufeiWang,XingshanZeng,WanjunZhong,LiangyouLi,FeiMi,LifengShang,Xin
Jiang,QunLiu,andWeiWang. FollowBench: AMulti-levelFine-grainedConstraintsFollowing
Benchmark for Large Language Models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar
(eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pp. 4667–4688, Bangkok, Thailand, August 2024. Association
for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.
257.
Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When Can LLMs Actually
Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs. arXiv preprint
arXiv:2406.01297,2024.
Klaus Krippendorff. Content Analysis: An Introduction to Its Methodology. SAGE Publications,
1980.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Ku¨ttler, Mike Lewis, Wen-tau Yih, Tim Rockta¨schel, Sebastian Riedel, and
DouweKiela. Retrieval-AugmentedGenerationforKnowledge-IntensiveNLPTasks. Advances
inNeuralInformationProcessingSystems,33:9459–9474,2020.
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. AlpacaEval: An Automatic Evaluator of Instruction-
followingModels,2023. URLhttps://github.com/tatsu-lab/alpaca_eval.
BillYuchenLin,YuntianDeng,KhyathiChandu,FaezeBrahman,AbhilashaRavichander,Valentina
Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. WildBench: Benchmarking LLMs with
ChallengingTasksfromRealUsersintheWild. arXivpreprintarXiv:2406.04770,2024.
Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summa-
rization Branches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational
Linguistics. URLhttps://aclanthology.org/W04-1013.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegr-
effe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bod-
hisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and
Peter Clark. Self-Refine: Iterative Refinement with Self-Feedback. In Alice Oh, Tris-
tan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Ad-
vances in Neural Information Processing Systems 36: Annual Conference on Neural Infor-
mation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html.
Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Tre-
bacz, and Jan Leike. LLM Critics Help Catch LLM Bugs. arXiv preprint arXiv:2407.00215,
2024.
11Preprint. UnderReview.
SwaroopMishra,DanielKhashabi,ChittaBaral,andHannanehHajishirzi. Cross-TaskGeneraliza-
tion via Natural Language Crowdsourcing Instructions. In Smaranda Muresan, Preslav Nakov,
and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for
ComputationalLinguistics(Volume1: LongPapers),pp.3470–3487,Dublin,Ireland,May2022.
Association for Computational Linguistics. URL https://aclanthology.org/2022.
acl-long.244.
Jekaterina Novikova, Ondˇrej Dusˇek, and Verena Rieser. RankME: Reliable Human Ratings for
Natural Language Generation. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Pro-
ceedings of the 2018 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 72–78,
NewOrleans,Louisiana,June2018.AssociationforComputationalLinguistics. URLhttps:
//aclanthology.org/N18-2012.
OpenAI. GPT-4TechnicalReport. arXivpreprintarXiv:2303.08774,2023.
OpenAI. HelloGPT-4o,2024. URLhttps://openai.com/index/hello-gpt-4o/.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
FraserKelton,LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulF.Christiano,
Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feed-
back. InSanmiKoyejo,S.Mohamed,A.Agarwal,DanielleBelgrave,K.Cho,andA.Oh(eds.),
AdvancesinNeuralInformationProcessingSystems35: AnnualConferenceonNeuralInforma-
tionProcessingSystems2022,NeurIPS2022,NewOrleans,LA,USA,November28-December
9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
b1efde53be364a73914f58805a001731-Abstract-Conference.html.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a Method for Automatic
EvaluationofMachineTranslation. InPierreIsabelle,EugeneCharniak,andDekangLin(eds.),
Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp.
311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguis-
tics. URLhttps://aclanthology.org/P02-1040.
YiweiQin,KaiqiangSong,YebowenHu,WenlinYao,SangwooCho,XiaoyangWang,Xuansheng
Wu,FeiLiu,PengfeiLiu,andDongYu. InFoBench: EvaluatingInstructionFollowingAbilityin
LargeLanguageModels. InLun-WeiKu,AndreMartins,andVivekSrikumar(eds.),Findingsof
theAssociationforComputationalLinguisticsACL2024,pp.13025–13048,Bangkok,Thailand
and virtual meeting, August 2024. Association for Computational Linguistics. URL https:
//aclanthology.org/2024.findings-acl.772.
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan
Leike. Self-critiquingmodelsforassistinghumanevaluators. arXivpreprintarXiv:2206.05802,
2022.
Je´re´myScheurer,JonAnderCampos,TomaszKorbak,JunShernChan,AngelicaChen,Kyunghyun
Cho, and Ethan Perez. Training Language Models with Language Feedback at Scale. arXiv
preprintarXiv:2303.16755,2023.
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute opti-
mallycanbemoreeffectivethanscalingmodelparameters.CoRR,abs/2408.03314,2024.doi:10.
48550/ARXIV.2408.03314. URLhttps://doi.org/10.48550/arXiv.2408.03314.
Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang.
Conifer:ImprovingComplexConstrainedInstruction-FollowingAbilityofLargeLanguageMod-
els. arXivpreprintarXiv:2404.02823,2024.
Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. To-
ward Self-Improvement of LLMs via Imagination, Searching, and Criticizing. arXiv preprint
arXiv:2404.12253,2024.
12Preprint. UnderReview.
GladysTyen,HassanMansoor,VictorCarbune,PeterChen,andTonyMak. LLMscannotfindrea-
soningerrors,butcancorrectthemgiventheerrorlocation. InLun-WeiKu,AndreMartins,and
VivekSrikumar(eds.),FindingsoftheAssociationforComputationalLinguisticsACL2024,pp.
13894–13908, Bangkok, Thailandandvirtualmeeting, August2024.AssociationforComputa-
tionalLinguistics. URLhttps://aclanthology.org/2024.findings-acl.826.
Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady
Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing Judges with Juries:
EvaluatingLLMGenerationswithaPanelofDiverseModels. arXivpreprintarXiv:2404.18796,
2024.
Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung.
FoundationalAutoraters:TamingLargeLanguageModelsforBetterAutomaticEvaluation.arXiv
preprintarXiv:2407.10817,2024.
HaoxiangWang,WeiXiong,TengyangXie,HanZhao,andTongZhang. InterpretablePreferences
viaMulti-ObjectiveRewardModelingandMixture-of-Experts.arXivpreprintarXiv:2406.12845,
2024a.
TianluWang,PingYu,XiaoqingEllenTan,SeanO’Brien,RamakanthPasunuru,JaneDwivedi-Yu,
OlgaGolovneva,LukeZettlemoyer,MaryamFazel-Zarandi,andAsliCelikyilmaz. Shepherd: A
criticforlanguagemodelgeneration. arXivpreprintarXiv:2308.04592,2023a.
Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu,
RichardYuanzhePang,MaryamFazel-Zarandi,JasonWeston,andXianLi. Self-TaughtEvalua-
tors. arXivpreprintarXiv:2408.02666,2024b.
YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahA.Smith,DanielKhashabi,and
HannanehHajishirzi.Self-Instruct:AligningLanguageModelswithSelf-GeneratedInstructions.
InAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.),Proceedingsofthe61stAnnual
MeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pp.13484–
13508,Toronto,Canada,July2023b.AssociationforComputationalLinguistics. URLhttps:
//aclanthology.org/2023.acl-long.754.
JasonWei, MaartenBosma, VincentY.Zhao, KelvinGuu, AdamsWeiYu, BrianLester, NanDu,
Andrew M. Dai, and Quoc V. Le. Finetuned Language Models are Zero-Shot Learners. In
The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022. OpenReview.net, 2022a. URL https://openreview.net/forum?
id=gEZrGCozdqR.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,
QuocV.Le,andDennyZhou.Chain-of-ThoughtPromptingElicitsReasoninginLargeLanguage
Models.InSanmiKoyejo,S.Mohamed,A.Agarwal,DanielleBelgrave,K.Cho,andA.Oh(eds.),
AdvancesinNeuralInformationProcessingSystems35: AnnualConferenceonNeuralInforma-
tionProcessingSystems2022,NeurIPS2022,NewOrleans,LA,USA,November28-December
9,2022,2022b. URLhttp://papers.nips.cc/paper_files/paper/2022/hash/
9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.
Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin
Hu,WendyGao,JiaxinXu,YimingLiu,JieTang,HongningWang,andMinlieHuang. Bench-
markingComplexInstruction-FollowingwithMultipleConstraintsComposition. arXivpreprint
arXiv:2407.03978,2024.
Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid
Shwartz-Ziv, NeelJain, KhalidSaifullah, SiddarthaNaidu, ChinmayHegde, YannLeCun, Tom
Goldstein,WillieNeiswanger,andMicahGoldblum.LiveBench:AChallenging,Contamination-
FreeLLMBenchmark. arXivpreprintarXiv:2406.19314,2024.
ZihuiwenYe,FraserGreenlee-Scott,MaxBartolo,PhilBlunsom,JonAnderCampos,andMatthias
Galle´. Improving Reward Models with Synthetic Critiques. arXiv preprint arXiv:2405.20850,
2024.
13Preprint. UnderReview.
WeizheYuan,PengfeiLiu,andMatthiasGalle´. LLMCrit: TeachingLargeLanguageModelstoUse
Criteria. arXivpreprintarXiv:2403.01069,2024.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion
Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Alice Oh, Tris-
tan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.),
Advances in Neural Information Processing Systems 36: Annual Conference on Neural In-
formation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10
- 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/
hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_
Benchmarks.html.
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny
Zhou,andLeHou. Instruction-FollowingEvaluationforLargeLanguageModels. arXivpreprint
arXiv:2311.07911,2023.
14Preprint. UnderReview.
A USING TICK FOR CATEGORICAL EVALUATIONS
WeadditionallyinvestigatewhetherTICK’schecklistevaluationscanbeusedtoprovideaggregate
feedbackonspecificmodelcapabilities,bygroupinginstructionswithsimilarrequirementstogether.
For Internal, we thus define the following instruction categories, corresponding to distinct LLM
capabilities that are relevant to the instruction set: [Classification, Concision, Data Manipulation,
DocumentGeneration,Exclusion: Keyword,Exclusion: Topic,Extraction,FileFormatting: JSON,
FileFormatting: TSV/CSV,Formatting: General,Inclusion: Keyword,Inclusion: Topic,Knowledge
Retrieval,Length,SubjectiveQA,Tone].
For100samplesfromInternal,wecollectannotationslabellingeachchecklistquestionwithappro-
priate categories. This task is performed by three annotators per checklist, and we take the inter-
sectionofchosenlabelstobegroundtruthset. WethenpromptGPT-4otogeneratelabelsforthe
samesetofchecklistquestionsbasedontheavailablecategories. Table7showstheperformanceof
GPT-4oonthiscategoryclassificationtask,bothforthesubsetofInternal,aswellasanequivalent
taskonInFoBench,wherecategorylabelsarealreadyprovidedforeachchecklistquestion.Notably,
InFoBenchonlydefinesfivecategories,makingtheclassificationmucheasier,whichisreflectedin
theresults.
Dataset Precision Recall F1
Internal 0.687 0.708 0.680
InFoBench 0.824 0.819 0.793
Table 7: GPT-4o classification performance when labelling checklist questions with a fixed set of
pre-determinedcategories.
Given the reasonable classification performance of GPT-4o on Internal (especially given the large
numberofcategorieswithwhichtolabeleachquestion)weuseGPT-4otogeneratecategorylabels
forthefulldataset. InFigure4,weshowhowCommand-R+performsacrossallchecklistquestions
foraparticularcapability,accordingtothepassrateforchecklistquestionswithinthatcategory.The
weakestcapabilitybycategoricalTICKevaluationsisLength; giventhatspecificlengthcontrolis
knowntobealimitationofcurrentLLMs,thisresultseemsconsistent.
Figure4:Command-R+passrateforchecklistquestionsonInternal,spanningdifferentperformance
categories. QuestionswerelabelledwithrelevantcategoriesbyGPT-4o.
15Preprint. UnderReview.
B FURTHER ANALYSIS OF GENERATED CHECKLISTS
B.1 COMPARISONOFHUMAN-WRITTENANDGENERATEDCHECKLISTLENGTHS
InFigure5,weshowhistogramsofthelengthsofhuman-writtenandGPT-4ogeneratedchecklists
for Internal instructions. We see that the distribution of checklist lengths is similar across human
andgeneratedchecklists, withthemaindifferencebeingthatgeneratedchecklistlengthsaremore
peakedneartothemode.
Figure 5: Histograms of checklist length of human-written (left) and GPT-4o generated (right)
checklistsforInternalinstructions.
B.2 RELATIONSHIPBETWEENCHECKLISTLENGTHANDTASKDIFFICULTY
ForbothInFoBenchandWildBench,wecomputethePearsoncorrelationbetweengeneratedcheck-
list length and the benchmark score achieved by a Command-R+ response to the corresponding
instruction. Thisgivesuscorrelationsof−0.001and0.028forInFoBenchandWildBenchrespec-
tively, thus indicating that there is no clear relationship between task difficulty and the length of
generatedevaluationchecklists.
16Preprint. UnderReview.
C ADDITONAL STICK BEST-OF-N RESULTS
In Table 8, we include a repeat of the experiment in 4.2, but with Llama-3.1-70B in place of
Command-R+.
InFoBench WildBench(Single-Turn)
ScoringFunction
DRFR Precision WB-Score Precision
GreedyDecoding 0.778 N/A 65.0 N/A
RewardModel(ArmoRM) 0.803 0.343 66.7 0.366
DirectSelf-Scoring 0.780 0.242 66.8 0.319
STICK 0.817 0.676 69.4 0.545
Table8: Best-of-8selectiononInFoBenchandWildBenchusingLlama3.1-70BwithSTICK,com-
paredwithdirectself-scoringandanexternalrewardmodel(ArmoRM),aswellasgreedydecoding.
Multi-turnconversationsareexcludedfromtheWildBenchevaluation.
17Preprint. UnderReview.
D INTERNAL INSTRUCTION SET EXAMPLES
Example1
WhohasMadonnadated? GivemeadetailedhistoryofallofMadonna’sknownromantic
relationships. Writeitasifyou’reacowboyfromawesternfilm. Keepitunder500words
andbreakitintolist-formatsoit’seasytoread
Example2
Correctthespelling/grammarmistakesinthefollowingtext, andsummarizeitinunder15
words:
Iincorectlyasumedthatahewouldgotothestorewithoutmehavingtoask. Shewontgo
unlesIask,andimtooembarasedtoaxe.
Example3
Generatealistof7movietitlesreleasedin2003alongwiththeirdirectorsandgenres.
ProvidetheminJSONformatwiththefollowingkeys: movie id,title,director,genre.
Make sure to provide a movie for each of these genres: Horror, Action, Drama, Comedy,
Fantasy, Documentary, and Animation. Arrange them in alphabetical order based on the
genre.
Example4
TellmeabouttheseriesRingofFire. Isitwellwritten? Trytokeepyourreplyunder300
words,andlimityourresponsestodirectanswerstomyquestions,ratherthananyadditional
conversationalnotes.
Example5
Fromthefollowingpassage,identifyalltheactivitiesNeildeGrasseTysonwasinvolvedin
pasttheyear2000.Answerinonesentencethatcontainsnomorethan100characters.Then,
inaseparatesentence,explainthereasoningbehindyouranswer.
NeildeGrasseTyson(bornOctober5,1958)isanAmericanastrophysicist,author,andsci-
encecommunicator. TysonstudiedatHarvardUniversity,theUniversityofTexasatAustin,
and Columbia University. From 1991 to 1994, he was a postdoctoral research associate at
PrincetonUniversity. In1994,hejoinedtheHaydenPlanetariumasastaffscientistandthe
Princetonfacultyasavisitingresearchscientistandlecturer. In1996,hebecamedirectorof
theplanetariumandoversawits$210millionreconstructionproject,whichwascompleted
in2000. Since1996,hehasbeenthedirectoroftheHaydenPlanetariumattheRoseCenter
forEarthandSpaceinNewYorkCity. ThecenterispartoftheAmericanMuseumofNat-
uralHistory,whereTysonfoundedtheDepartmentofAstrophysicsin1997andhasbeena
researchassociateinthedepartmentsince2003.
18Preprint. UnderReview.
E GENERATED CHECKLIST EXAMPLES
Here we include examples of evaluation checklists generated by GPT-4o using the prompt in Ap-
pendixG.1.
INFOBENCH
Instruction
Therearemanycreativeprojectswecanbuildathomethatarerelatedtothegiventheme,
solet’slistsomeofthem.
Theme: Circuits
InFoBenchChecklist(writtenbyhumanannotators)
• Isthegeneratedtextalist?
• Aretheitemsinthegeneratedlistrelevanttothethemeinthegiveninput?
• Cantheprojectsinthegeneratedtextbebuiltathome?
• Aretheprojectsinthegeneratedtextcreative?
GPT-4oGeneratedChecklist
• Istheresponsealistofprojects?
• Aretheprojectsdescribedcreativeinnature?
• Aretheprojectsrelatedtothethemeofcircuits?
• Aretheprojectsfeasibletobuildathome?
19Preprint. UnderReview.
WILDBENCH
Instruction
RecreatethistextasanewsarticleinthestyleofAssociatedPresswithadvancedvocabulary
andperfectdiction: {text}
WildBenchChecklist(combinedandmanuallyreviewedgenerationsfromGPT-4-Turboand
Claude-3-Opus)
• Doestherecreatedtextmaintainthefactualaccuracyoftheoriginalinformationprovided?
• Isthelanguageusedintherecreatedtextreflectiveoftheadvancedvocabularyandperfect
dictiontypicalofanAssociatedPressstylenewsarticle?
• Areallthekeydetailsfromtheoriginaltext,suchasthelocationchange,thepreviousand
new use of the property, and the statements from Mohammad Taqi Haqbein, accurately
includedintherecreatedtext?
• Doestherecreatedtextmaintainaneutralandobjectivetone,freefrombiasorsubjective
commentary?
• Isthestructureoftherecreatedtextcoherentanddoesitfollowalogicalflowthatiseasy
tofollowforreaders?
• Doestherewrittenarticlemaintainthekeyfactsandinformationfromtheoriginaltext?
• IsthearticlewritteninthedistinctstyleoftheAssociatedPress,withconciseparagraphs
andafocusonthemainnewselements?
• Doesthearticleuseadvancedvocabularyandperfectdiction,demonstratingahighlevel
oflanguageproficiency?
• Isthearticlewell-structured,withaclearleadparagraphsummarizingthemainnewsand
subsequentparagraphsprovidingadditionaldetails?
• DoesthearticleincluderelevantquotesfromtheinterviewwithMohammadTaqiHaqbein
tosupportthemainpoints?
• Isthearticlefreeofgrammaticalerrors,spellingmistakes,orawkwardphrasingthatwould
beuncharacteristicofprofessionalAssociatedPresswriting?
GPT-4oGeneratedChecklist
• Istheresponserewrittenasanewsarticle?
• DoestheresponseadheretotheAssociatedPressstyleintermsofclarityandreadability?
• Doestheresponseutilizeadvancedvocabularyappropriately?
• Isthedictionintheresponseperfectandfreefromgrammaticalerrors?
• Doestheresponsemaintainthefactualaccuracyoftheoriginaltext?
• Does the response include key details from the original text, such as the closure and re-
openinglocationsofShafaqBookHouse?
• Doestheresponseproperlyattributequotesandsources,followingjournalisticstandards?
• Doestheresponseavoidusingopinionoreditorializing, consistentwithAPstylereport-
ing?
20Preprint. UnderReview.
LIVEBENCH: CODING
Instruction
Instructions: YouareanexpertPythonprogrammer. Youwillbegivenaquestion(problem
specification)andthefirstlinesofPythonsolutiontothisproblem,andwillwriteinPython
the remaining lines of the program to produce a correct Python program that matches the
specificationandpassesalltests. YouwillNOTreturnanythingexceptforthesecondpart
oftheprogramthatyouwrote.
Question: Youaregivena0-indexedintegerarraynumsandanintegerk.
Youcanperformthefollowingoperationonthearrayatmostktimes:
Chooseanyindexifromthearrayandincreaseordecreasenums[i]by1.
Thescoreofthefinalarrayisthefrequencyofthemostfrequentelementinthearray.
Returnthemaximumscoreyoucanachieve.
Thefrequencyofanelementisthenumberofoccurrencesofthatelementinthearray.
Examples: {examples}
Constraints:
1<=nums.length<=105
1<=nums[i]<=109
0<=k <=1014
Format: You will use the following starter code to write the solution to the problem and
encloseyourcodewithindelimiters.
“‘{startercode}”’
Answer: (enclose your partial completion in backticks. Only write the missing portion of
thecode,nottheentirecode. Beverycarefultomatchtheappropriateindentation. Directly
appendingyourcodeafterthepartialcodeshouldproduceacorrectcompletionsolution.)
GPT-4oGeneratedChecklist
• Doestheresponseonlyincludethemissingportionofthecodeandnothingelse?
• Doestheresponsecorrectlycontinuefromthegivenstartingcode?
• Doestheresponsehandletheoperationscorrectlytomodifyelementsatmost‘k‘timesto
maximizethefrequencyofthemostfrequentelement?
• Does the response correctly implement logic to track and calculate the frequency of the
mostfrequentelementinthearray?
• Does the response ensure the final implementation is syntactically correct and free from
errors?
• Is the approach efficient given the constraints of the problem (‘1 <= nums.length <=
105‘,‘1<=nums[i]<=109‘,‘0<=k <=1014‘)?
21Preprint. UnderReview.
LIVEBENCH: MATH
Instruction
Differentiatethefollowingfunction:
sin(cid:0) 7x4+4(cid:1)
cos(9−x). Pleaseputyourfinalanswer
ina .
GPT-4oGeneratedChecklist
• Doestheresponsecorrectlyapplytheproductruletodifferentiatethegivenfunction?
• Does the response correctly differentiate the individual components
sin(cid:0) 7x4+4(cid:1)
and
cos(9−x)?
• Aretheintermediatestepsclearandlogicallypresented?
• Isthefinalanswercorrectlyboxedusingthe notation?
LIVEBENCH: REASONING
Instruction
Inthisquestion, assumeeachpersoneitheralwaystellsthetruthoralwayslies. Hiroshiis
at the farm. The person at the school tells the truth. The person at the train station says
the person at the airport tells the truth. [...] The person at the barbershop says the person
at the vineyard lies. Does the person at the observatory tell the truth? Does the person at
theairporttellthetruth? Doesthepersonatthefarmtellthetruth? Thinkstepbystep,and
thenputyouranswerin**bold**asalistofthreewords,yesorno(forexample,**yes,no,
yes**). Ifyoudon’tknow,guess.
GPT-4oGeneratedChecklist
• Does the response thoroughly analyze the statements and location data provided in the
instruction?
• Doestheresponseprovidestep-by-stepreasoningleadingtotheconclusions?
• Isthefinalanswerformattedcorrectlyinboldasalistofthreewords(yesorno)?
• Doesthefinalanswerreflectthetruthstatusofthepersonattheobservatory?
• Doesthefinalanswerreflectthetruthstatusofthepersonattheairport?
• Doesthefinalanswerreflectthetruthstatusofthepersonatthefarm?
• Does the response make sure to answer every question, providing an educated guess if
uncertain?
22Preprint. UnderReview.
F STICK SELF-REFINEMENT EXAMPLES
Here,weincludeexamplesofself-refinementusingSTICKwithCommand-R+.
F.1 WILDBENCH
Instruction
generatesmarteaformetatrader5thatusesbeststrategiesthatgrows10dollaraccountto
tenthousandandtheeamustknowwhentotrademargingeneratefullcode
InitialResponse(providedbyCommand-R+)
Creatinga”smart”ExpertAdvisor(EA)thatcangrowa$10accountto$10,000andmake
margintradingdecisionsrequiresasophisticatedcombinationoftradingstrategies,riskman-
agement, andmarketanalysis. [...] Pleasenotethatthiscodeprovidesabasicframework,
andyoumayneedtofurthercustomizeandoptimizeitbasedonyourspecifictradingstrat-
egyandrequirements. Additionally, it’simportanttothoroughlybacktestandforward-test
theEAonademoaccountbeforeusingitwithrealmoney.
WB-ScoreonInitialResponse(providedbyGPT-4)
Strengths:
• TheAIresponseprovidesastructuredanddetailedExpertAdvisor(EA)codefor
MetaTrader 5, which includes key trading functions such as margin trading, stop
loss,takeprofit,andtrailingstop.
• ThecodefollowsthebasicprinciplesofMQL5programming,usingappropriateli-
brariesandconstructslikeTrade.mqh,initialization,andtick-basedeventhandling.
• The response includes clear explanations of each section of the code, which can
helptheuserunderstandhowtoadaptorexpandthestrategy.
• Thedisclaimeraboutcustomization,backtesting,andfurtheroptimizationisappro-
priateandprudentforatradingscenariowhereresultsmayvary.
Weaknesses:
• Theuserspecificallyrequestedastrategythatcouldgrowa$10accountto$10,000,
which is an unrealistic goal for most trading systems. The response does not di-
rectlyaddresstheinfeasibilityofsuchahighreturnonasmallbalancewithintypi-
caltradingenvironments,whichcouldsetunrealisticexpectations.
• Thereisnoclearmentionofspecifictradingstrategiesorindicators(e.g.,moving
averages,RSI)thattheEAshoulduse. Whiletheresponseprovidesaframework,
the user requested strategies that would assist in achieving the goal, and the re-
sponselacksconcretestrategies.
• Theresponseusesahardcodedmarginmultiplierandsimplebuytrades, butdoes
notincorporateadvancedriskmanagementtechniqueslikepositionsizing,diversi-
fication,orvolatilityadjustments,whicharecriticalforlong-termaccountgrowth.
• It doesn’t directly mention that achieving such high account growth would likely
involvehighrisk,whichcanleadtocompleteaccountlossinvolatilemarkets.
• Theprovidedcodecouldbesimplified,andthereareredundantpartsliketheempty
deinit()functionthatcouldhavebeenomitted.
Score: 5
23Preprint. UnderReview.
STICKonInitialResponse(providedbyCommand-R+)
• IstheresponseacompleteEAcodeforMetaTrader5? No
• DoestheEAemploystrategiesthatarewidelyrecognizedaseffectiveorthe”best”
intrading? No
• IsthereanexplanationorevidencesuggestingthattheEAhasthepotentialtogrow
a$10accountto$10,000? No
• DoestheEAincludefunctionalitytomanagemargineffectively,knowingwhento
tradeandwhentoavoidhigh-risksituations? No
• Istheprovidedcodesyntacticallycorrectandfreeoferrors? No
• Isthecodewell-documentedtoexplainthelogicandoperationsoftheEA?Yes
RefinedResponse(providedbyCommand-R+)
Creatinga”smart”ExpertAdvisor(EA)thatcangrowa$10accountto$10,000andmake
dynamic margin trading decisions is an exciting challenge. [...] Please note that this EA
stillservesasastartingpoint,andfurthercustomizationandoptimizationarerecommended
based on your specific trading style and requirements. Backtesting, forward testing, and
ongoingmonitoringareessentialstepstoensuretheEAs´ performancealignswithyourex-
pectations.
24Preprint. UnderReview.
WB-ScoreonRefinedResponse(providedbyGPT-4)
Strengths:
• The response is quite comprehensive, covering multiple aspects of the user’s re-
questforaMetaTrader5EA(ExpertAdvisor).Itincludesafullcodewithexplana-
tionsofkeytradingstrategieslikerisk-rewardratios,margintrading,anddynamic
stop-lossadjustmentbasedonvolatility.
• The code integrates risk management techniques, which is a critical part of any
tradingEA.Featuressuchasmaximumdrawdownlimit,dynamicstop-lossadjust-
ment,andpositionsizingmaketheEAmorealignedwithpropertradingpractices.
• The response also provides guidance on customization, testing (both backtesting
andforwardtesting),andtheimportanceofoptimizingtheEAfordifferenttrading
styles,whichisimportantwhendealingwithdynamicmarketslikeforex.
• Thereiscleardocumentationwithinthecode, makingiteasierfortheusertoun-
derstandandpotentiallymodifytheEAfortheirspecificneeds.
Weaknesses:
• The claim that the EA could grow a $10 account to $10,000 is highly unrealistic
without emphasizing the level of risk and market conditions required to achieve
such results. Even though the EA has risk management features, it doesn’t ade-
quatelyaddressthehighrisksinvolvedintryingtoachievesuchdramaticreturns,
especiallyfromsuchasmallstartingbalance. Moreconservativewarningsordis-
claimersshouldbeincludedabouttherisksinvolvedinsuchaggressivegoals.
• Theresponsecouldbenefitfromamoredetailedexplanationofthetradingstrate-
giesemployedbytheEA,particularlyhowtheyalignwiththegoalofsuchahigh
levelofaccountgrowth. Whilesomeaspectsarementioned(likerisk-rewardratio
andmargintrading),theyaren’tfullyconnectedtohowtheycontributetothevery
ambitiousgrowthtarget.
• Theresponsecouldexpandonpotentiallimitationsoredgecases(e.g.,highvolatil-
ity periods, slippage, broker limitations) to give the user a more realistic under-
standingofwheretheEAmayfail.
Score: 7
STICKonRefinedResponse(providedbyCommand-R+)
• IstheresponseacompleteEAcodeforMetaTrader5? Yes
• DoestheEAemploystrategiesthatarewidelyrecognizedaseffectiveorthe”best”
intrading? Yes
• IsthereanexplanationorevidencesuggestingthattheEAhasthepotentialtogrow
a$10accountto$10,000? Yes
• DoestheEAincludefunctionalitytomanagemargineffectively,knowingwhento
tradeandwhentoavoidhigh-risksituations? Yes
• Istheprovidedcodesyntacticallycorrectandfreeoferrors? No
• Isthecodewell-documentedtoexplainthelogicandoperationsoftheEA?Yes
25Preprint. UnderReview.
F.2 LIVEBENCH
Instruction
Suppose I have a physical, solid cube. The top face is a square with vertices A, B, C, D,
andthesefourverticesaredirectlyabovetheverticesE,F,G,H,respectively. Ialsohavea
physical,solidsphere,withcenterJ.Thecubeandspherearenotoverlapping,andthethree
pointsA,D,Jarecolinear. Now, ImakeacutthroughtheplanedefinedbypointsBDHF.
Fromtheoriginalcubeandsphere,howmanypiecesaretherenowafterthecut? Thinkstep
bystep,andthenputyouranswerin**bold**asasingleinteger(forexample,**0**). If
youdon’tknow,guess.
InitialResponse(providedbyCommand-R+)
Let’sbreakdowntheobjectsandthecut:
**Cube:** The cut plane goes through the diagonal of the cube’s bottom face, BD, and
anotherdiagonalofthefrontface,FH.Thiscutwillturnthecubeinto5pieces: twosmaller
cubesandthreerectangularslabs.
**Sphere:**Thecutplaneintersectsthesphere.Sincethecenterofthesphere,J,iscolinear
withAandD,andA,Dlieonthecutplane,thecenterJmustalsobeonthecutplane. Thus,
thecutplanedividesthesphereintotwoequalhalves.
So,intotal,afterthecut,thereare**7**pieces: 5fromthecubeand2fromthesphere.
LiveBenchEvaluationonInitialResponse
Score: 0
STICKonInitialResponse(providedbyCommand-R+)
• Isthestep-by-stepreasoningprovidedintheresponselogicalandcoherent? No
• Doestheresponseidentifythecorrectnumberofpiecescreatedbythecut,consid-
eringboththecubeandthesphere? No
• Is the final answer to the number of pieces presented in bold as a single integer
(e.g.,**0**)? Yes
• Isananswergiven,beitalogicalguessorareasonedsolution? Yes
RefinedResponse(providedbyCommand-R+)
Let’sanalyzethecut’seffectonthecubeandsphere:
**Cube:**Thecutplaneintersectsthecube, creatingacomplexdivision. Theplanegoes
throughthediagonalBDofthebottomfaceandthediagonalFHofthefrontface. Thiscut
will divide the cube into multiple pieces, including smaller cubes and rectangular prisms.
However,theexactnumberofpiecesischallengingtodeterminewithoutavisualrepresen-
tationorfurtherinformation.
**Sphere:** The cut plane intersects the sphere, creating a unique situation. Since points
A,D,andJarecollinear,andAandDlieonthecutplane,thecenterJisalsoontheplane.
Thismeansthecutplanedividesthesphereintotwoequalhemispheres.
Giventhecomplexityofthecube’sdivisionandtheuncertaintyinthenumberofpieces, a
precisecountisdifficult. However,wecanconcludethatthereareatleast**3**pieces: 2
hemispheresfromthesphereandaminimumof1piecefromthecube.
LiveBenchEvaluationonRefinedResponse
Score: 1
26Preprint. UnderReview.
STICKonRefinedResponse
• Isthestep-by-stepreasoningprovidedintheresponselogicalandcoherent? Yes
• Doestheresponseidentifythecorrectnumberofpiecescreatedbythecut,consid-
eringboththecubeandthesphere? Yes
• Is the final answer to the number of pieces presented in bold as a single integer
(e.g.,**0**)? Yes
• Isananswergiven,beitalogicalguessorareasonedsolution? Yes
27Preprint. UnderReview.
G PROMPT TEMPLATES
G.1 CHECKLISTGENERATION
Please help judge an AI assistant’s response to an instruction by
providing an evaluation checklist.
To write a specific evaluation checklist, you get given the
following entity each time:
INSTRUCTION: An instruction that has been given to an AI
assistant.
## Task Details
Your task is to come up with an evaluation checklist list for a
given INSTRUCTION.
This evaluation checklist should be a list of questions that ask
whether or not specific criteria relevant to the INSTRUCTION were
met by an AI assistant’s response.
Criteria covered by your checklist could be explicitly stated in
the INSTRUCTION, or be generally sensible criteria for the
problem domain.
You should, however, try to be concise and not include
unnecessary entries in your checklist.
Checklist questions should:
- **Be answerable by ’yes’ or ’no’**, with ’yes’ meaning that the
response successfully met the corresponding requirement.
- **Be comprehensive, but concise**, meaning that all criteria
directly relevant to the INSTRUCTION should be represented by a
question, but only questions that are very clearly relevant
should be included.
- **Be precise**, meaning that checklist questions should avoid
vague wording and evaluate specific aspects of a response,
directly using the phrasing of the INSTRUCTION where appropriate.
You should always analyse the INSTRUCTION before providing an
evaluation checklist.
## Response Format
Analysis: xxx
Answer: CHECKLIST QUESTIONS (each question should appear on a new
line)
## Examples
{examples}
## Real Task
### INSTRUCTION
{message}
### Response
Please analyse the instruction and provde an answer in the
correct format.
Remember that each question should be phrased such that answering
with ’yes’ would mean that the response **successfully**
fulfilled the criteria being assessed by the question.
In most cases, your checklist should contain at least two
questions, but no more than eight.
28Preprint. UnderReview.
G.2 CHECKLISTEVALUATION
Thispromptisadaptedfrom(Qinetal.,2024).
Please act as a fair judge. Based on the provided Instruction and
Generated Text, analyse the Generated Text and answer the
Question that follows with ’YES’ or ’NO’.
Your selection should be based on your judgment as well as the
following rules:
- YES: Select ’YES’ if the generated text entirely fulfills the
condition specified in the question. However, note that even
minor inaccuracies exclude the text from receiving a ’YES’
rating. As an illustration, consider a question that asks, ‘‘Does
each sentence in the generated text use a second person?’’ If
even one sentence does not use the second person, the answer
should NOT be ‘YES’. To qualify for a ’YES’ rating, the generated
text must be entirely accurate and relevant to the question.
- NO: Opt for ’NO’ if the generated text fails to meet the
question’s requirements or provides no information that could be
utilized to answer the question. For instance, if the question
asks, ’Is the second sentence in the generated text a compound
sentence?’ and the generated text only has one sentence, it
offers no relevant information to answer the question.
Consequently, the answer should be ‘NO’.
## Output Format
Analysis: xxx
Answer: YES / NO (this should be either ’YES’ or ’NO’)
## Evaluation Information
**Instruction**
{message}
**Generated Text**
{generation}
**Question**
{question}
Please analyse and answer whether the Generated Text satisfies
the requirement of the Question.
G.3 PREFERENCE
Please act as a fair judge. Based on the provided Instruction and
Responses, analyse the Responses and provide a preference.
Your selection should be based on your judgment and correspond to
one of the following preference rankings:
1. Response A is better than Response B.
2. Response A and Response B are near-identical.
3. Response B is better than Response A.
The ’near-identical’ option (i.e., option 2) should be chosen
only if the differences between the two responses are
29Preprint. UnderReview.
semantically and syntactically insignificant, such as ’The
correct answer is New York’ and ’The right answer is New York’.
In other words, if the two responses are substantially different
in terms of their content, you must identify a preference for one
of the responses. **Responses that are different in content
but similar in quality are NOT near-identical.**
## Output Format
Analysis: xxx
Answer: PREFERENCE RANKING (this should be an integer from 1-3
and nothing else)
## Evaluation Information
**Instruction**
{message}
**Response A**
{generation_1}
**Response B**
{generation_2}
Please analyse the Responses and provide a preference ranking (1,
2, or 3). Remember to stick to the requested Output Format.
G.4 DIRECTSCORING
Please act as a fair judge. Based on the provided Instruction and
Generated Text, analyse the Generated Text and provide a 1-5
integer score.
Your selection should be based on your judgment as well as the
following guidelines for each possible score:
1. Horrible: The Generated Text is unintelligibly written
(incomplete sentences, leaps in logic, flagrant mechanical
errors) or has majorly incorrect or unverifiable information.
2. Bad: The Generated Text is occasionally difficult to
understand, dotted with minor factual or mechanical errors, or
missing crucial formatting elements.
3. Okay: The Generated Text expresses useful information, is
readable, has no factual errors, and has no more than a minor
mechanical error or two. Though it may be informative to those
unfamiliar with the subject matter, it is not overly insightful,
engaging, or likely to hold up to expert scrutiny.
4. Great: The Generated Text clearly expresses useful information
at an expert level, is readable, and has no factual or mechanical
errors. It could just use a quick adjustment with tone or length.
5. Excellent: The Generated Text clearly expresses useful
information at an expert level, is readable, has no factual or
mechanical errors, and is the perfect length and tone with regard
to the prompt.
## Output Format
Analysis: xxx
Answer: SCORE (this should be an integer from 1-5 and nothing
else)
30Preprint. UnderReview.
## Evaluation Information
**Instruction**
{message}
**Generated Text**
{generation}
Please analyse the Generated Text and provide a 1-5 integer score
according to the guidelines. Remember to stick to the requested
Output Format.
G.5 CHECK-THEN-SCORE
Please act as a fair judge. Based on the provided Instruction and
Generated Text, analyse the Generated Text and provide a 1-5
integer score.
You will also be provided with a Checklist that should help to
inform your selection.
Your selection should be based on your judgment as well as the
following guidelines for each possible score:
1. Horrible: The Generated Text is unintelligibly written
(incomplete sentences, leaps in logic, flagrant mechanical
errors) or has majorly incorrect or unverifiable information.
2. Bad: The Generated Text is occasionally difficult to
understand, dotted with minor factual or mechanical errors, or
missing crucial formatting elements.
3. Okay: The Generated Text expresses useful information, is
readable, has no factual errors, and has no more than a minor
mechanical error or two. Though it may be informative to those
unfamiliar with the subject matter, it is not overly insightful,
engaging, or likely to hold up to expert scrutiny.
4. Great: The Generated Text clearly expresses useful information
at an expert level, is readable, and has no factual or mechanical
errors. It could just use a quick adjustment with tone or length.
5. Excellent: The Generated Text clearly expresses useful
information at an expert level, is readable, has no factual or
mechanical errors, and is the perfect length and tone with regard
to the prompt.
## Output Format
Analysis: xxx
Answer: SCORE (this should be an integer from 1-5 and nothing
else)
## Evaluation Information
**Instruction**
{message}
**Generated Text**
{generation}
**Checklist**
Use this checklist to guide your evaluation, but do not limit
your assessment to the checklist.
{checklist}
31Preprint. UnderReview.
Please analyse the Generated Text and provide a 1-5 integer score
according to the guidelines. Remember to stick to the requested
Output Format.
G.6 SELF-REFINEMENT
UsingSelf-TICKasCritiques
Please use the feedback provided below to improve your previous
response to an instruction.
You will be given the following entities:
- INSTRUCTION: An instruction that has been given to an
assistant.
- RESPONSE: Your previous response.
- FEEDBACK: A list of ’yes’/’no’ questions about the response and
their answers. An answer of ’yes’ corresponds to a pass for that
question and an answer of ’no’ correspnods to a fail.
## Task Description
Your task is to improve the RESPONSE to the INSTRUCTION based on
the FEEDBACK. You should try to address any ’no’ answers in the
feedback whilst maintaining any ’yes’ answers.
If all answers in feedback are ’yes’, simply respond with your
original RESPONSE.
Provide a plan to improve the RESPONSE based on the INSTRUCTION
and FEEDBACK and then rewrite the RESPONSE with your
improvements.
## Information
**INSTRUCTION**
{message}
**RESPONSE**
{response}
**FEEDBACK**
{feedback}
## Response Format (IMPORTANT)
Plan: xxx
Answer: NEW RESPONSE
After saying ’Answer: ’ you must say nothing else besides the
improved answer.
Now please plan and write a new RESPONSE, based on the
INSTRUCTION and FEEDBACK.
GatheringUnstructuredSelf-Critiques
Please analyse a response to a particular instruction and provide
feedback on how the response can be improved.
You will be given the following entities:
- INSTRUCTION: An instruction that has been given to an
assistant.
- RESPONSE: Your previous response.
32Preprint. UnderReview.
## Task Description
Your task is to provide feedback that will help improve the
RESPONSE to the INSTRUCTION.
Please analyse the RESPONSE and provide your critical feedback,
pointing to specific actionable improvements that can be made.
## Information
**INSTRUCTION**
{message}
**RESPONSE**
{response}
Now please provide your feedback.
UsingUnstructuredSelf-Critiques
Please use the feedback provided below to improve your previous
response to an instruction.
You will be given the following entities:
- INSTRUCTION: An instruction that has been given to an
assistant.
- RESPONSE: Your previous response.
- FEEDBACK: Feedback on your previous response.
## Task Description
Your task is to improve the RESPONSE to the INSTRUCTION based on
the FEEDBACK.
Provide a plan to improve the RESPONSE based on the INSTRUCTION
and FEEDBACK and then rewrite the RESPONSE with your
improvements.
## Information
**INSTRUCTION**
{message}
**RESPONSE**
{response}
**FEEDBACK**
{feedback}
## Response Format (IMPORTANT)
Plan: xxx
Answer: NEW RESPONSE
After saying ’Answer: ’ you must say nothing else besides the
improved RESPONSE.
The new RESPONSE must exactly match the formatting of the
original.
Now please plan and write a new RESPONSE, based on the
INSTRUCTION and FEEDBACK.
33Preprint. UnderReview.
H HUMAN ANNOTATION DETAILS
The same pool of trained annotators was used in all human annotation processes. The training
undergonebyannotatorsincludesgeneral,task-agnostictrainingcoveringhighlevelguidelinesfor
annotatingtheoutputsofanAIassistant,aswellastask-specificinstructionsandexamples. Atall
stagesintheannotationprocess,weareabletointeractwithannotatorstoansweranyquestionsand
respondtorequestsforclarification.
H.1 CHECKLISTWRITING
Thefollowinginstructionsaregiventoannotators. Somesectionsareparaphrasedforbrevity.
Large language models are trained to respond to user
instructions, which can often be complex. To best evaluate
responses to a set of user instructions, we’re exploring the
viability of writing custom queries for each prompt to determine
which aspects of a complex instruction were followed correctly
and which were not.
In this project, you will write checklists of questions for given
instructions that ask whether each aspect of an instruction was
met by a model output.
## Key Concept: Facet
### Definition
Facets are distinct, individual elements of a prompt,
corresponding to the capabilities and constraints that the model
output should meet.
## Tips and Tricks
- **Checklist questions must be answerable by either** ‘yes‘
**or** ‘no‘**.** The idea is that the questions are to be asked
of responses to the given prompt; ‘yes‘ means that the response
fulfilled the facet of the instruction, and ‘no‘ means it did
not.
- **Ensure each facet you identify is represented by a single
question.** This means you can provide as many questions for a
prompt as you see fit for that prompt, but questions should
overlap as little as possible (i.e., there should not be many
questions addressing the same facet).
- **Questions should be as specific or unspecific as the facet
they correspond to.** For example:
Prompt 1: Write a short paragraph.
Q1: Is the paragraph short?
Prompt 2: Write a 3 sentence paragraph.
Q2: Is the paragraph 3 sentences long?
## Examples
{examples}
## Task Instructions
1. Carefully analyse the provided prompt.
2. Label the task:
a)Answer the two questions about the prompt being safe and
making sense.
b) Unless answering ’no’ to one of the above, write a list of
34Preprint. UnderReview.
checklist questions that could be asked of any response to
the prompt.
3. Submit the task. Great work!
H.2 PREFERENCELABELLING
Thefollowinginstructionsaregiventoannotators.Somesectionsareparaphrasedforbrevity.These
instructionsaretobeinterpretedinthecontextofamorecomprehensivemodeloutputannotation
styleguidethatisnotincludedhere.
In this project, you will be indicating your preference between
two responses to a single prompt generated by two different
LLM-powered chatbots. Evaluating responses may involve making
trade-offs between several criteria. You should do your best to
navigate these trade-offs depending on the task.
Given an instruction and two responses, please indicate which
response you prefer on the following sliding scale:
1. Response A is much better than Response B.
2. Response A is better than Response B.
3. Response A and Response B are near-identical.
4. Response B is better than Response A.
5. Response B is much better than Response A.
The ’near-identical’ option (i.e., option 2) should be chosen
only if the differences between the two responses are
semantically and syntactically insignificant, such as ’The
correct answer is New York’ and ’The right answer is New York’.
In other words, if the two responses are substantially different
in terms of their content, you must identify a preference for one
of the responses. **Responses that are different in content
but similar in quality are NOT near-identical.**
H.3 DIRECTSCORING
Thefollowinginstructionsaregiventoannotators.Somesectionsareparaphrasedforbrevity.These
instructionsaretobeinterpretedinthecontextofamorecomprehensivemodeloutputannotation
styleguidethatisnotincludedhere.
In this project, you will be directly scoring individual
responses to a single prompt generated by an LLM-powered chatbot.
Evaluating responses may involve making trade-offs between
several criteria. You should do your best to navigate these
trade-offs depending on the task.
Given an instruction and response, please score the response
according to the following rubric:
1/5: Horrible
- The response is unintelligibly written (incomplete sentences,
leaps in logic, flagrant mechanical errors) or has majorly
incorrect or unverifiable information.
2/5: Bad
- The response is occasionally difficult to understand, dotted
with minor factual or mechanical errors, or missing crucial
formatting elements.
3/5: Okay
- The response expresses useful information, is readable, has no
35Preprint. UnderReview.
factual errors, and has no more than a minor mechanical error or
two. Though it may be informative to those unfamiliar with the
subject matter, it is not overly insightful, engaging, or likely
to hold up to expert scrutiny.
4/5: Great
- The response clearly expresses useful information at an expert
level, is readable, and has no factual or mechanical errors. It
could just use a quick adjustment with tone or length.
5/5: Excellent
- The response clearly expresses useful information at an expert
level, is readable, has no factual or mechanical errors, and is
the perfect length and tone with regard to the prompt.
H.4 CHECK-THEN-SCORE
Thefollowinginstructionsaregiventoannotators.Somesectionsareparaphrasedforbrevity.These
instructionsaretobeinterpretedinthecontextofamorecomprehensivemodeloutputannotation
styleguidethatisnotincludedhere. Someofthe“tipsandtricks”includedintheseinstructionsare
takenfrom(Qinetal.,2024).
Large language models are trained to respond to user requests,
which can often be complex. To best evaluate responses to a set
of user requests, we’re exploring the viability of answering
custom checklists of specific evaluation questions for each
prompt.
You will receive a prompt and a single attempt from a model to
respond. Read the response closely and answer the labelling
questions. These labelling questions will comprise checklist
questions and a 1{5 rating. The number of checklist questions and
the specific questions themselves will vary per prompt.
Typically, evaluating responses involves making trade-offs
between criteria, for example, scoring or making a preference
judgement based on a response that correctly followed formatting
instructions but made a factual error. Evaluation checklists are
designed to instead break down these prompt-specific criteria and
enable each to be independently evaluated.
## Key Concept: Checklist Question
### Definition
Checklist questions are Yes/No questions corresponding to whether
specific criteria relevant to the user request were successfully
followed in a model response.
## Tips and Tricks
- **Answer ‘Yes‘ to a checklist question if the response entirely
fulfils the condition.** Note that even minor inaccuracies should
exclude the response from receiving a ‘Yes‘ rating.
- **Answer ‘No‘ **to a checklist question if the response fails
to meet the condition or provides no information that could be
used to answer the question.** For instance, if the question
asks, ‘‘Is the second sentence in the generated text a compound
sentence?’’ and the generated text only has one sentence, it
offers no relevant information to answer the question.
Consequently, the answer should be ‘No‘.
36Preprint. UnderReview.
- **Use answers to checklist questions to partially inform the
overall response score.** A response that mostly fails the
checklist should receive a low score. However, a response can
also pass all of the checklist questions and still have a
middle-range score if the overall quality is low. For example, a
response could hypothetically pass all of the checklist questions
but still be uninsightful, repetitive or poorly worded.
ANNOTATORFEEDBACKONCHECK-THEN-SCORE
Aftereachscoringeachresponse,weaskedannotatorstoindicatewhethergoingthroughthecheck-
listmadeprovidinganoverallscoreeasier,harder,orhadnoeffect.Thecorrespondinganswerrates
wereeasier: 78.5%, harder: 6.4%andhadnoeffect: 15%. InadditiontotheresultsinSection5,
thisprovidesfurtherevidencethatLLM-generatedchecklistssimplifythetaskofratingindividual
responsesforhumanevaluators.
DEMONSTRATINGWHYHUMANEVALUATIONSHOULDONLYUSEGENERATEDCHECKLISTSAS
APARTIALGUIDE
InthefollowingexamplefromWildBench,theuserasksfortheLLMtoreadcertainbookchapters
andwebpagesbeforeansweringspecificquestionsonthecontent. Command-R+wasusedtogen-
eratearesponse,withoutuseofRetrievalAugmentedGeneration(RAG)(Lewisetal.,2020). The
modelhallucinatedhavingvisitedthewebpagesandwroteananswerconsistentwithhavingdone
so, whichbemisleadinginpractice. Thechecklistquestions, generatedbyGPT-4o, assumethata
respondingmodelwouldhavebeenabletoaccesstherelevantmaterialandarethereforeunhelpful
inevaluatingtheCommand-R+response,asdesirablebehaviourinthisinstancewouldbetoinform
theuserthatitsrequestcannotbefulfilled. Thiswasreflectedbythelowfinalscoresgivenbyall
humanevaluators.Yet,someoftheseevaluatorsansweredmostofthechecklistquestionswithYES,
demonstrating that answers to generated checklists alone should not be used to score responses in
human evaluation, but also showing that human evaluators are robust to unhelpful or misleading
checklists. Thisexampleisexplicitlyshownbelow.
Instruction
Answer all of the following quesions in about 350 words.
1. Read the GHDI chapters and choose a (1) topic that you found
most compelling - and select 1 image that "spoke to you." Answer
this question using the Nazi Germany (1933-1945): Science section
and the image: \Albert Einstein: Authorial Fame Seems to be
Relative!" Caricature of Einstein’s Political Activity,
Kladderadatsch, No. 39 (September 1933)
2. Visit the Shoa Foundation Website and the United States
Holocaust Museum site. Look at the content, listen to some of the
testimonies that relate to our topic "racial science" and science
and culture. What did you specifically learn?
3. Revisit a company/ organization website that we had talked
about earlier this semester and review it again - with the lens
on our time period- 1939 - 1945. Is there information missing?
If so investigate the role of this company/organization during
the Nazi - period. What are your thoughts?
4. Examine the role of scientists in Nazi Germany from a
humanitarian, social and scientific perspective:
5. Design an open ended question for your fellow students.
37Preprint. UnderReview.
Command-R+Response
Sure, here are my responses to the prompts:
1. The image that spoke to me is the "Albert Einstein: Authorial
Fame Seems to be Relative!" caricature from the September 1933
issue of Kladderadatsch. This image is a powerful reminder of the
political and social climate in Nazi Germany during the 1930s.
Einstein, a Jewish scientist, was a target of propaganda and
persecution by the Nazi regime due to his theoretical physics
work and his public opposition to nationalism and racism. The
caricature, meant to ridicule Einstein, actually showcases the
fear and respect Einstein commanded, as the Nazis felt threatened
enough by his intellect and influence to try and discredit him.
2. After visiting the Shoa Foundation and the United States
Holocaust Museum sites, I learned about the personal stories and
testimonies of survivors and victims of the Holocaust,
specifically relating to "racial science." One powerful testimony
I listened to was that of a Jewish doctor, who spoke about the
"medical experiments" conducted on prisoners in concentration
camps, which were a perversion of science and an attempt to
"prove" Nazi racial theories. The museums also provided a wealth
of information on the cultural destruction inflicted by the
Nazis, including the burning of books and suppression of academic
freedom.
3. Revisiting company websites from this time period reveals
interesting omissions and complexities. For example, many
companies with a dark history during the Nazi era, such as IG
Farben or Krupp, have since been dissolved or restructured, with
their modern incarnations providing little to no information
about their role in the Third Reich. This lack of transparency
warrants further investigation, as these companies profited from
slave labor, played a key role in the war economy, and even
collaborated in the horrific human experiments conducted by the
Nazis.
4. The role of scientists in Nazi Germany was complex and had
far-reaching consequences. From a humanitarian perspective, many
scientists remained silent or complicit in the face of human
rights abuses, with some even actively participating in unethical
experiments. Socially, scientists were influenced and constrained
by the political climate, with those who opposed the regime, like
Einstein, being forced into exile. Scientifically, there was a
perversion of science to fit ideological goals, with "racial
hygiene" and eugenics gaining prominence, leading to devastating
consequences.
5. Open-ended question for fellow students: How do you think the
scientific community could have collectively resisted or
mitigated the misuse of science by the Nazi regime? Considering
the social, political, and historical context, what strategies or
actions could have been taken to uphold ethical scientific
practices during this dark period in history?
Evaluation(AnnotatorA)
9/10checklistquestionsansweredYES.Overallscore: 2/5,Bad.
38