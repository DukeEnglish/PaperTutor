WHAT MATTERS FOR MODEL MERGING AT SCALE?
PrateekYadav1∗ TuVu2,3 JonathanLai2 AlexandraChronopoulou2
ManaalFaruqui2 MohitBansal1 TsendsurenMunkhdalai2
1TheUniversityofNorthCarolinaatChapelHill 2Google 3VirginiaTech
CorrespondenceEmail: praty@cs.unc.edu
ABSTRACT
Modelmergingaimstocombinemultipleexpertmodelsintoamorecapablesingle
model,offeringbenefitssuchasreducedstorageandservingcosts,improvedgen-
eralization,andsupportfordecentralizedmodeldevelopment. Despiteitspromise,
previous studies have primarily focused on merging a few small models. This
leavesmanyunansweredquestionsabouttheeffectofscalingmodelsizeandhow
itinterplayswithotherkeyfactors—likethebasemodelqualityandnumberof
expertmodels—,toaffectthemergedmodel’sperformance. Thisworksystem-
aticallyevaluatestheutilityofmodelmergingatscale,examiningtheimpactof
thesedifferentfactors. Weexperimentwithmergingfullyfine-tunedmodelsus-
ingfourpopularmergingmethods—Averaging,TaskArithmetic,Dare-TIES,
andTIES-Merging—acrossmodelsizesrangingfrom1Bto64Bparametersand
mergingupto8differentexpertmodels. Weevaluatethemergedmodelsonboth
held-intasks,i.e.,theexpert’strainingtasks,andzero-shotgeneralizationtoun-
seenheld-outtasks. Ourwiderangeofexperimentsprovideseveralnewinsights
aboutmodelmergingatscaleandtheinterplaybetweendifferentfactors. First,
wefindthatmergingismoreeffectivewhenexpertsarecreatedfromstrongbase
models,i.e.,modelswithgoodzero-shotperformance,comparedtopre-trained
ones. Second,largermodelsfacilitateeasiermerging. Thirdmergingconsistently
improvesgeneralizationcapabilities. Notably,whenmergingeightlargeexpert
models, the merged models often generalize better compared to the multitask
trainedmodels. Fourth,wecanbettermergemoreexpertmodelswhenworking
withlargermodels.Fifth,differentmergingmethodsbehaveverysimilarlyatlarger
scales. Overall,ourfindingsshedlightonsomeinterestingpropertiesofmodel
mergingwhilealsohighlightingsomelimitations. Wehopethatthisstudywill
serveasareferencepointonlarge-scalemergingforupcomingresearch.
1 INTRODUCTION
Modelmerging(Raffel,2021)referstotheprocessofcombiningtwoormoreconstituent(expert)
models to produce a new, and potentially more powerful model. The appeal of this technique is
rootedinseveralbenefitsitcanconfer: first,itdramaticallyreducesstorageandservingcostsby
reusingasinglemodelacrosstasks;second,itenablescompositionalcombinationofcapabilities
fromexpertmodels,whichcanimprovegeneralizationtonoveltasks;andthird,mergingsupports
decentralizedandmodularmodeldevelopmentbyallowingmultiplecontributorstoindependently
buildmodelsandlatercombinethemtogether.
Thesecharacteristicshaveledtoagreatdealofrecenteffortsindevelopingcost-effectivemodel
mergingmethods(Matena&Raffel,2022b;Ilharcoetal.,2022;Jinetal.,2022;Yadavetal.,2024b;
Yangetal.,2023;Yuetal.,2024d;Shahetal.,2023;Tametal.,2023;Zhaoetal.,2024),oftenusing
simplearithmeticoperations,suchasaveragingtheparametersoftheconstituentmodels. However,
mostofthesestudiesarelimitedtosmall-scaleexperimentswithrelativelysmallmodels(typically
<7Bparameters)andmerging2or3experts(Yuetal.,2024a;c),andmainlyfocusonimproving
benchmarkperformanceonheld-intasksthattheexpertmodelsweretrainedon(Yuetal.,2024a;
Yadavetal.,2024b). Despitethepromisesthatmodelmergingholds,theresearchcommunitystill
∗WorkdoneasaninternatGoogle.
1
4202
tcO
4
]GL.sc[
1v71630.0142:viXraBase Model Average Size Base Model Average Size
PaLM-2 Multitask Task Arithmetic 1B PaLM-2 Multitask Task Arithmetic 1B
PaLM-2-IT TIES 8B PaLM-2-IT TIES 8B
Dare-TIES
64B
01 .8.0
1B
Dare-TIES 2 64 4B B Dare-TIES
64B
01 .8.0 1BDare-TIES 2 64 4B B
0.6 TIES 0.6 TIES
Task Arithmetic 0.4 Task Arithmetic 0.4
0.2 0.2
Multitask Multitask
Average Average
Average Average
Multitask Multitask
Task Arithmetic Task Arithmetic
TIES TIES
24B 8B 24B 8B
Dare-TIES Dare-TIES
Dare-TIES Dare-TIES
TIES TIES
Task Arithmetic Multitask Task Arithmetic Multitask
Average Average
(a)Merging2experts,Held-In. (b)Merging8experts,Held-In.
Figure 1: Held-In performance results from our large scale model merging experiments
conducted over keys factors like base models, model sizes, merging methods, and number of
expertsbeingmerged. Wepresentresultsfortwobasemodels,PaLM-2andaninstructiontuned
version of it, PaLM-2-IT, four different models sizes (1B,8B,24B,64B), four merging methods
(Averaging, TaskArithmetic, Dare-TIES, and TIES-Merging), when merging either 2 or 8
expertmodels. Wereporttheperformancenormalizedwiththeoracleexpert’sperformancewhich
is denoted by the bold black circle of radius 1. We also present the performance of multitask
baselinetrainontheheld-intasks. Wefindmergingexpertmodelscreatedfromtheinstructiontuned
PaLM-2-ITmodelalwaysperformsbetterthanmergingPaLM-2basedexperts. Moreover,thegap
betweenthesemodelincreasewhenwemergemoreexperts. Largerexperts(64B)mergebetterand
showthebestheld-inperformance.
lacksacomprehensivestudytoevaluateitseffectivenessaswescalethemodelsize. Moreover,itis
notclearhowscaleinterplayswithotherfactorslikenumberofexpertmodelsandbasemodelquality
toaffectthemergedmodel’sheld-inperformanceandzero-shotgeneralization. Thisisofparamount
importance,asmodelsarerapidlygrowinginsize,andmoreopen-weightmodelsanddatasetsare
becomingavailable,1drivingtheneedforpracticalandscalablemergingmethods.
Ourprimarygoalinthispaperistoprovideinsightsintothescalabilityofmodelmerging. While
a few studies have explored merging at the 13B parameter scale (Huang et al., 2024a; Yu et al.,
2024d;b), they primarily leverage increased model size and combine only 2-3 models to attain
betterperformanceonheld-intasks. Assuch,theinterplayoffactorslikemodelsize,basemodel
quality,numberofconstituentmodels—andtheireffectonbothheld-inandzero-shotgeneralization
performance(held-out)—remainslargelyunexplored. Hence,weaimtoaddressthefollowingfour
researchquestions(RQ):
RQ1: Whatistheeffectofusingpretrainedvs. instruction-tunedbasemodelsforcreatingexpert
modelsformerging?
RQ2: Doesmodelmergingbecomeeasierorharderasthemodelsizeincreases?
RQ3: Howdoesmergingaffectzero-shotgeneralizationtoheld-outtasks,andhowisthisinflu-
encedbymodelsize?
RQ4: How many expert models can be merged without performance loss, and how does this
dependonmodelsize?
Toanswerthesequestion, wesystematicallyevaluatetheeffectivenessofcurrentstate-of-the-art
mergingmethodsthroughempiricalexperiments. Specifically,weutilizethePaLM-2model(Anil
etal.,2023)anditsinstruction-tunedvariant,PaLM-2-IT,whilescalingthemodelsizesupto64B
parameters.Weexperimentwithfourpopularmergingmethods,namely,Averaging(Wortsmanetal.,
2022a;Choshenetal.,2022b),TaskArithmetic(Ilharcoetal.,2022),TIES-Merging(Yadavetal.,
2024b),andDare-TIES(Yuetal.,2024d).Weconductaseriesofsensitivityandablationexperiments
tounderstandtherelativeimportanceofseveralfactorslikemodelsize(1B,8B,24B,64Bparameters),
basemodelquality(pretrainedvs. instruction-tuned),andnumberofconstituentmodels(2,4,6,8)
1Asofwriting,thelargestopen-weightAImodelisLlama3.1405Bparameters,andHuggingFacehostsa
plethoraofcommunity-contributedresources,with1M+modelsand200K+datasets.
2PaLM-2-IT, 1B PaLM-2-IT, 24B PaLM-2-IT, 64B
1.2 1.2 1.2
Merging Method
Task Arithmetic
1.1 1.1 1.1 TIES
Multitask
1.0 1.0 1.0
PaLM-2-IT PaLM-2-IT PaLM-2-IT
2 4 6 8 2 4 6 8 2 4 6 8
# of Experts
Figure 2: Merged experts created from big and strong base models generalize better than
multitaskmodels. Wefindthatforstrongbasemodelsaswemergemoreexperts(x-axis,→),the
mergedmodel’sgeneralizationperformance(y-axis, ↑)monotonicallyincreasestoapproachand
eventuallysurpassesmultitaskbaseline. (yellowline). MoredetailsinSection4.3.
beingmerged. Additionally,weconsidertwoaxesofevaluationusingtheT0datacollection(Sanh
et al., 2021a): held-in evaluation with tasks the expert models were trained on, and held-out, for
zero-shotgeneralizationtounseentasks.
Ourexperimentresultsshedlightonthepromisesofmodelmergingandrevealinterestinginsights
intothebehaviorsofdifferentfactorsatscale. First,wefindthatthemodelinitializationplaysa
crucialroleinenhancingtheperformanceofthemergedmodel. Specifically,acrossallevaluation
settings, using strong zero-shot instruction-tuned base models to create expert models leads to
improvedperformancecomparedtousingpretrainedmodels(see§4.1). Second,largermodelsare
consistentlyeasiertomerge. Thisholdstrueregardlessofthebasemodelused(instruction-tunedor
not),numberofmodelsmerged,ormergingmethod(see§4.2). Third,ourresultsdemonstratethat
mergingsignificantlyenhanceszero-shotgeneralization,consistentlyimprovingtheabilitytoadapt
tonewtasks. Notably,whenusingstrongbasemodelsasthenumberofmergedexpertsincreases,
ourmergedmodeleithermatchesorexceedstheperformanceofastrongmulti-tasktrainingbaseline
(see§4.3). Fourth,largermodelsarebetteratmergingalargernumberofexpertmodels(see§4.4).
Finally, our numerous experiments identify specific settings where we expect model merging to
bemuchmoreuseful. Fromthisweprovidegeneralrecommendationsforpractitioners(see§4.6).
Takenasawhole,ourfindingsareapowerfultestamenttothepotentialofmodelmergingatscalefor
creatinghighlygeneralizablelanguagemodels,whichwehopewillspurmorefundamentalresearch
intothedevelopmentofpracticalandscalablemergingmethods.
2 BACKGROUND
Model merging has emerged as a cost-effective method for developing improved models. Two
commonusecasesofmergingare: (1)combiningmodelcheckpointsfromdifferentdataversions,
hyperparameters,ortrainingstagestoenhancedistributionalrobustness(Teametal.,2024;Dubey
et al., 2024), and (2) combining multiple expert models trained on different datasets to leverage
theircomplementarycapabilities. Inbothscenarios,theexpertmodelsgenerallyshareacommon
architectureandabasemodelfromwhichtheexpertmodelsarecreatedviafine-tuning.
Thisworkfocusesonmergingspecialized,fine-tunedversions(experts)ofasinglebasemodelto
enhanceitscapabilities. Eachexpertmodelistrainedondistinctdatasetscoveringdifferenttasks,
domains,and/orcapabilities. Werefertothetasks/datasetsusedfortrainingtheexpertmodelsas
“held-in”,whilethosethatarenewandunseenarecalled“held-out”. Ourgoalistocreateaunified
modelthatretainstheindividualexpertmodels’capabilitiesonheld-intaskswhileimprovingzero-
shotgeneralizationonheld-outtasks. Thismergingapproachprovidesaflexible,modularmethod
forpost-traininglargelanguagemodels,facilitatingtheadditionofnewfeaturesandcapabilitiesto
top-performingmodels.
3
ecnamrofreP
tuO-dleH2.1 MODELMERGINGMETHODS
We denote the set of N expert tasks as t ,...,t and the base model weights, representing the
1 N
common ancestor of all expert models as θ . The weights of the corresponding specialized
base
expert models, each obtained by fully fine-tuning the base model on a specific expert task, are
denoted as θ , ..., θ , respectively. We focus on “open vocabulary” models which utilize natural
1 N
languageasinputandoutputforbothclassificationandgenerationtasks,eliminatingtheneedfor
task-specificclassificationheadsmakingthemergingprocesssimpler. Giventhis,modelmerging
methods can be defined as a function M(.). This function takes as input the base model, the
set of N expert models, and potentially additional information, denoted by Φ. This additional
informationmayincludeactivationstatistics,Fishermatrices,orothermethod-specificdata. The
output of the function is the merged model, represented by its parameters θ . Formally, θ =
m m
M({θ }N , θ ,Φ),whereΦismethodspecificdata.
i i=1 base
Givenourfocusonstudyingmodelmergingwithlargemodels, weselectfourmergingmethods
based on their popularity and simplicity. We only study merging methods that can scale to tens
ofbillionsofmodelweightparametersanddonotrequireanyadditionalinformationtoperform
merging,i.e.,Φ={},asthesetechniquesareefficientforevenlargermodels. Othermorecomplex
methodsthatrequirecomputingfishermatrices(Matena&Raffel,2022a),backwardpasses(Yang
etal.,2023),oradditionalinformationlikemodelactivation(Jinetal.,2023)areskippedbecauseof
theircomputationalcomplexitiesforlargescalemodelmergingthatwefocusoninthiswork. Next,
wedescribethefourselectedmodelmergingmethodsindetail.
2.1.1 AVERAGING
Parameteraveraging(Choshenetal.,2022b;Wortsmanetal.,2022a)isawell-establishedtechnique
in federated learning (McMahan et al., 2017) and recent applications extend its utility to merge
modelsforenhancingmodelrobustnessagainstout-of-distributiondata(Wortsmanetal.,2022b;
Rame´ etal.,2022a),refinepre-trainedmodels(Yuetal.,2024a),developmultimodalmodels(Sung
etal.,2023),andcreatemultitaskmodelsbycombiningcapabilities(Yadavetal.,2024b;Ilharcoetal.,
2022). Parameteraveragingisachievedbytakingameanofalltheexpertmodelweightstogether
withoutusingthebasemodelwhichcanbeformallydescribedas,M({θ }N , θ )= 1(cid:80)N θ .
i i=1 base N i=1 i
2.1.2 TASKARITHMETIC
TaskArithmetic(Ilharcoetal.,2022)introducesanovelconceptof“taskvectors”formodelmerging.
Fortaskt ,thetaskvectorisdenotedasτ =θ −θ whichcapturestask-specificknowledgeby
i i i base
quantifyingthedifferencebetweenthefine-tunedexpertparameters(θ )andtheoriginalbasemodel
i
parameters (θ ). A scaling hyperparameter λ controls the contribution of the aggregated task-
base
specificknowledgetothefinalmodel. Themergedmodelisthenconstructedbylinearlycombining
thebasemodelparameterswithascaledsumofalltaskvectors. Formally,taskarithmeticcanbe
describedas,M({θ }N , θ ;λ)=θ +λ∗(cid:80)N (θ −θ ).
i i=1 base base i=1 i base
2.1.3 TIESMERGING
TIES-Merging(Yadavetal.,2024b)identifiestwomainchallengeswithmodelmerging: ❶during
finetuningexpertmodelsaccumulatealotofnoiseintheparameters,and❷differentexpertsmight
wanttochangethesameparameterindifferentdirectionsleadingtointerference/conflictbetween
theexpertmodels. Theydemonstratethatbothofthesefactorshurtmodelmergingandproposea
threestepsprocesstoremoveredundantparameters,followedbyresolvingsignconflicts,andfinally
aggregatingonlytheparametersthatarenotconflicting. Specifically, inTIESMergingtheyfirst
zerooutthevaluesineachtaskvectorthathavelowmagnitudestoobtainthetrimmedtaskvector
τˆ foreachtask. Next,theychosetheaggregatesign(γ )foreachparameterbasedonwhetherthe
i m
parameterhasahighertotalmagnitudeinthepositiveorthenegativedirectionacrossalltrimmed
taskvector, formally, γ =
sgn((cid:80)N
τˆ ). Finally, foreachparameterspthemodelswhosesign
m i=1 i
matchestheaggregatesignareaveragedtoobtainthemergedtaskvector. Finally,themergedmodel
isobtainedbyscalingthemergedtaskvectorusingahyperparameterλandthenaddedbacktothe
basemodelas,θp =θ +λ∗ 1 (cid:80) τˆp,whereAp ={i∈[N]|γˆp =γp}.
m base |Ap| i∈Ap i i m
42.1.4 DAREMERGING
Dare(Yuetal.,2024a)extendstheideaofTIESmergingbyproposingtouseadropout-likepruning
stagetoremovenoisebeforemerging. Specifically, aBernoullimaskM withdropprobabilityp
i
isappliedtoeachtaskvectortoobtaintheprunedtaskvectorτˆ = (1−M )⊙τ /(1−p). This
i i i
stochasticprocessrandomlyzeroesoutelementswithinthetaskvectorwhilepreservingitsexpected
value. TheseprunedtaskvectorsarethenusedalongwitheitherTIESMergingorTaskArithmetic.
DuetothepopularityoftheDarevariantthatusesTIESMerging,weusethattorepresenttheDare
methodandcallitDare-TIES.
2.2 CHALLENGES/LIMITATIONS
Model Merging has been utilized at a growing rate in practice as it has recently been applied to
buildingmodernlanguagemodelslikeLlama-3(Dubeyetal.,2024)andGemma-2(Teametal.,2024).
However,mostformalstudiesonmodelmerginghavebeenperformedwithrelativelysmallmodels.
There are a few studies that look at larger models with 7B and 13B parameters. However, those
studiesmostlyfocusonmerging2-3modelstoimprovebenchmarknumbersasopposedtobetter
understandinghowthesizeofthemodelaffectsthemodelmergingprocessandtheresultantmodel.
Tomotivateourwork,wepresentsomeofthelimitationsoftheexistingstudiesandhighlighttheir
differencewithourwork.
MostStudiesonSmallModels(<7Bparameters): Almostallexistingmodelmergingpapers
rarely use large models (> 7B). For example past works (He et al., 2024; Daheim et al., 2023;
Ortiz-Jimenezetal.,2024;Jangetal.,2024),includingpopularmethodslikeModelSoup(Wortsman
et al., 2022a), Task Arithmetic (Ilharco et al., 2023) and TIES-Merging (Yadav et al., 2024b),
RegMean(Jinetal.,2023),Fisher-Merging(Matena&Raffel,2022a)Ada-Merging(Yangetal.,
2023), MatS (Tam et al., 2024) perform experiments with model families like CLIP (Radford
etal.,2021),ViT(Dosovitskiyetal.,2021),T5(Raffeletal.,2020a),DeBERTa(Heetal.,2021),
Roberta(Liuetal.,2019), BERT(Devlinetal.,2018)withlessthan1Bparameters. Hence, itis
unclearhowwellmodelmergingworksforlargemodels,whatfactorsplayanimportantrole,the
effectofmodelsize,numberoftasksbeingmerged,anditseffectonbothheld-inperformanceand
generalizationofthemodel. Somestudieshypothesizethatbiggermodelsmightbeeasiertomerge
howevertherearenoconcretelargescalestudiestothoroughlyassesssuchclaimsatlargescale.
ModelMergingStudieswithLargeModelsareShallow: SomerecentworkslikeDARE(Yu
etal.,2024a),WIDEN(Yuetal.,2024c),Chat-Vector (Huangetal.,2024b)demonstratemerging
resultsforlargermodelswithupto13Bparameters,howeverthesestudieshaveafewlimitations: ❶
Theyprimarilyfocusonusingmodelmergingtoimprovemodelqualityandhencetheirexperiments
donotprovideconcreteinsightsonhowmodelsizeinterplayswithmerging,❷Theyonlymerge
amaximumoftwoorthreemodelsatonce, ❸Theyprimarilyfocusonheld-intasksanddonot
provideanyinsightsontheeffectofmergingonamodel’sgeneralizationabilities. Otherworkslike
RewardSoup(Rameetal.,2024),WARM(Rameetal.),WARP(Rame´etal.,2024),FuseLLM(Wan
etal.,2024a),FuseChat(Wanetal.,2024b)alsoworkwith∼7Bsizedmodelsandfocusonspecific
applicationsofmodelmergingwithoutprovidinganydeeperinsightabouthowmergingperformance
changesforlargemodels.
Varied Evaluation Setups: Most previous works rarely share their experimental setup where
both the expert datasets and the objective vary. For example, RegMean (Jin et al., 2023), Task
Arithmetic(Ilharcoetal.,2023),TIES(Yadavetal.,2024b),MaTS(Tametal.,2024)usesGLUE
tasks (Wang et al., 2018), Vision tasks, T0 held-out, and T0 held-in (Sanh et al., 2021b) tasks
respectively. Moreover,differentworksevaluatefordifferentusecaseslikeintermediatetasktraining
inFishermerging(Matena&Raffel,2022a),robustnessinmodelsoups(Wortsmanetal.,2022a),
andheld-inperformanceforDare(Yuetal.,2024a),bothheld-inandheld-outperformanceinTIES
Merging(Yadavetal.,2024b). Givenourfocusoncombiningmodelcapabilitiesintheposttraining
phase,wefocusonevaluatingonbothheld-intasksandgeneralizationtounseenheld-outtasks.
53 LARGE SCALE EVALUATION OF MODEL MERGING
Inthiswork,weaddressthelimitationsmentionedabovebysystematicallyunderstandingtheeffect
ofvariousfactorslikemodelsize,basemodelquality,mergingmethod,andthenumberofmodels
beingmergedonboththeheld-inandgeneralizationperformanceofthefinalmergedmodel. Next,
wedescribeourexperimentaldesign.
Data: Sanhetal.(2021a)foundthatexplicitmultitasktrainingofT5(Raffeletal.,2020b)ona
collectionofprompteddatasetsproducesamodelwithstrongzero-shotperformanceonunseentasks.
Thishasbecomeacommonexperimentalsettingforbenchmarkingzero-shotgeneralization(e.g.
(Longpreetal.,2023;Jangetal.,2023;Zhouetal.,2022;Chungetal.,2024;Muqeethetal.,2024).
Hence,weadopttheexperimentalsettingfromtheT0mixture(Sanhetal.,2021a)whichcontains
8held-inand4held-outtaskcategories. Foreachofthesecategoriestherearemultipledatasets
intheT0mixture(Sanhetal.,2021b)andhencetoreduceevaluationcosts, weselect2datasets
fromeachcategorybasedonthepopularityandthetraindatasetsize. Specifically,the8held-intask
categories(withatotalof16datasets)includeMultiple-choiceQA,ExtractiveQa, Closed-Book
QA, Sentiment Analysis, Topic Classification, Structure-to-text, Summarization, and Paraphrase
Identification. Similary, the 4 held-out task categories (with a total of 7 datasets) are Sentence
Completion,NaturalLanguageInference,CoreferenceResolution,andWordSenseDisambiguation.
FormoredetailsseeSectionA.
ExpertModelCreation: Recognizingthesignificanceofpost-trainingforLLMswheremodels
aretypicallyfullyfine-tuned,weperformfullfine-tuningtocreateourexpertmodelstobettermimic
thepost-trainingsetting. Moreover,inpost-trainingphasesitiscommontofirstperformInstruction
Tuning(IT)onthemodelbeforemovingontoothersteps. Hence,weexaminetheeffectofusing
stronginstruction-tunedbasemodelsontheprocessandoutcomeofmodelmerging. Giventhis,we
utilizethePaLM-2models(Aniletal.,2023)withsizes1B,8B,24B,and64Basourbasemodels
(θ ). Toobtaintheinstructiontunedbasemodel,wefurtherfine-tunedthePaLM-2modelsonthe
base
FLAN-v2dataset(Longpreetal.,2023)whileexcludingtheT0-mixturetasks(Sanhetal.,2021a).
Theseinstruction-tunedvariantsaredenotedasPaLM-2-IT. Foreachofthe2basemodeltypes
(non-ITvsIT)and4modelsizes,weperformfullfine-tuningonthe8held-intaskcategoriesresulting
64specializedexpertsmodelswhicharethenusedfurtherinourexperiments. Comprehensivedetails
regardinghyperparametersandcomputationalrequirementsareprovidedinAppendixB.
ExperimentalSetting: Givenourcollectionofexpertmodels,foreachmergingexperimentwe
selectasubsetofexpertmodelswhichwecalltheconstituentmodels. Wecreatealargemerging
experimentgridwith2basemodels(PaLM-2andPaLM-2-IT),fourmodelsizes(1B,8B,24B,64B),
fourMergingmethods(Averaging,TaskArithmetic,Dare-TIES,andTIES),thenumberofconstituent
models(2,4,6,8),and3seedstorandomlyselecttheconstituenttasksfortheexperimentresulting
inatotalof384mergingexperiments. Theseseedsaresharedacrossdifferentexperimentalsettings
toensurethesametasksareselectedacrossbasemodels,modelsizesandmergingmethodstoensure
faircomparison. Forexample,inanexperimentwemerged2expertmodels,derivedfromthe64B
PaLM-2basemodelwiththeconstituentmodelsbeingMCQandSummarizationexpertswhilethe
sameexperimentwithadifferentseedresultedinClosedBookQAandSentimentAnalysisexperts
astheconstituentmodels.
Evaluation: Foreachoftheexperimentsabove,weassessthemergedmodel’sperformanceby
evaluatingitonboththeheld-intasks–i.e.,thetrainingtasksoftheconstituentexpertmodels–and
all4held-outtaskcategories. Forexample,iftheconstituentmodelsareMCQandSummarization
experts,thenforheld-intasksweevaluateontheMCQdatasets(DREAMandCosmosQA)and
Summarizationdatasets(CNNDailyMailandXSum)resultingatotalof4held-inevaluationdatasets.
Moreover,allmergingexperimentsarealsoevaluatedonthe4held-outtaskscategoriesconsistingof
7datasetslistedinAppendixA.Thereweperformapproximately∼9000modelevaluationsacross
allofourexperiments.
Metric: Giventhatdifferentdatasetsusedifferentmetrics,wenormalizetheperformancemetricsto
makethemunitlesssothattheycanbeaggregated.Forheld-intasks,themergedmodel’sperformance
isnormalizedagainstthecorrespondingtaskexpertmodel’sperformance. However,forheld-out
62 Experts 4 Experts 6 Experts 8 Experts
011 ... 901 Expert Models 011 ... 901 Expert Models 11 .. 02 Expert Models 11 .. 02 Expert Models BBaassee P PP Pa aa a MM L LL LM MM Moo - -- -dd 2 22 2ee --IIll TTss
0.8 0.8 0.8
0.8 Merging Methods
0.7 00 .. 67 0.6 0.6 A Tav se kr a Ag re ithmetic
0.6 0.5 0.4 0.4 D TIa Er Se-TIES
0.5 1B 8B 24B 64B 0.4 1B 8B 24B 64B 1B 8B 24B 64B 0.2 1B 8B 24B 64B
Model Size
Figure 3: Instruction-tuned models facilitate easier merging. PaLM-2-IT (•) consistently
outperformsPaLM-2(•)asshownbythehugegapbetweenthegreenpoint(•)beinghigherthan
redpoints(•),acrossvariousmergingmethods, modelsizes, andnumbersofconstituentmodels,
indicatingthatstrongerinstruction-tunedbasemodelsenhancetheperformanceofmergedmodels.
Thedashedlinesdenotedtheperformanceoftheexpertstrainedontheheld-intasksasdefinedin§3.
FormoredetailsseeSection4.1.
tasks,thenormalizationwasperformedrelativetothebasemodel’sperformance. Wedenotethis
metricasnormalizedperformancethroughoutthepaper. Importantly,wewanttoemphasisethat
thismetricisrelative,withavalueof1indicatingperformancecomparabletothereferencemodel.
Hence,forheld-intasksavalueof1meansperformancesimilartothedomainexpertmodelwhile
forheld-outtasksitmeansperformanceissimilartothebasemodel. Wemarkthislineinmostof
ourfiguresandspecifythemodelsthatareusedfornormalization. Finally,togenerateaggregated
results,wecomputethemeanofnormalizedperformanceacrossalldatasetswithineachcategory,
thenacrossallcategoriesandthenoverthethreeseeds.
4 EXPERIMENTAL RESULTS
Inthissection,weexploretheinterplaybetweenmodelsizeandkeyfactorssuchasbasemodelquality,
mergingmethod,andthenumberofconstituent(expert)model,alongwiththeireffectonbothheld-in
andzero-shotgeneralization(held-out)performance. Ourfindingsare: ❶Mergingismoreeffective
whentheconstituentmodelsarederivedfrominstruction-tunedbasemodelsratherthanpretrained
ones(see§4.1);❷Largermodelsfacilitateeasiermerging(§4.2);❸Mergingsignificantlyimproves
zero-shotgeneralization,withinstruction-tunedmodelsbenefitingfromincreasedconstituentmodels,
andlargermodelsizesallowingthemergedmodeltomatchorexceedmulti-tasktraining(§4.3);❹
Wecanmergemoremodelseffectivelywhenusinglargermodels(§4.4);and❺Differentmerging
methodsperformsimilarlywhenappliedtolarge-scaleinstruction-tunedmodels. Below,weoutline
theexperimentalsetupanddiscussthesefindingsindetail.
4.1 INSTRUCTION-TUNEDMODELSFACILITATEEASIERMERGING
ExperimentalSetup: Priorresearchsuggestsaconnectionbetweenrobustzero-shotmodelsand
effective model merging. Wortsman et al. (2022a) demonstrate that averaging strong zero-shot
modelsimprovesout-of-distributionrobustness. Ortiz-Jimenezetal.(2024)indicatethateffective
pretrainingallowsforweightdisentanglement,andthusenhancingmerging. Otherstudies(Yadav
etal.,2024b;Ilharcoetal.,2023)proposethatstrongbasemodelscouldaidinmodelmerging,though
thishypothesisremainslargelyuntested.
Toassesshowbasemodelqualityaffectstheheld-inperformanceofmergedmodels,weperform
mergingexperimentswithfullyfine-tunedexpertsfromPaLM-2andPaLM-2-IT. Wevarymodel
sizesin{1B,8B,24B,64B}andthenumberofconstituentmodelsin{2,4,6,8}.Held-inperformance
is measured over three trials to minimize the impact of selected expert models and their data
distributions. A consistent seed is used across different base models, model sizes, and merging
methods to ensure fair task comparisons. We evaluate four merging methods: averaging, task
arithmetic,TIES,andDare-TIES,andalsocompareagainsttheperformanceoftask-specificexpert
models.
Findings: Ourresults,presentedinFigure3,indicatethatPaLM-2-ITmodelsdenotedbygreen
color(•),consistentlyoutperformsPaLM-2models(•)acrossvariousmergingmethods(•,▲,♦,⋆),
7
ecnamrofreP
nI-dleHPaLM-2, 2 Experts PaLM-2-IT, 2 Experts PaLM-2-IT, 8 Experts
Expert Models Expert Models Expert Models
1.0 1.0 1.0 Size
1B
0.8 0.8 0.8 8B
24B
64B
0.6 0.6 0.6
A v
era g De are-TIE S TIE S
Ta s k
Arith.
A v
era g De are-TIE S TIE S
Ta s k
Arith.
A v
era g De are-TIE S TIE S
Ta s k
Arith.
Merging Method
Figure4: Biggermodelsmergebetter. OnHeld-Inevaluations,wefindthatbiggermodelsalways
performbettercomparedtosmallermodels,barringafewoutliers. Wefindthatlargeinstruction
tunedmodelslike64BPaLM-2-ITaretheeasiesttomerge. FormoredetailsseeSection4.2.
PaLM-2, 1B PaLM-2, 24B PaLM-2, 64B
1.8 1.8 1.8
1.6 1.6 1.6 Merging Method
Task Arithmetic
1.4 1.4 1.4 TIES
Multitask
1.2 1.2 1.2
1.0 1.0 1.0
PaLM-2 PaLM-2 PaLM-2
2 4 6 8 2 4 6 8 2 4 6 8
# of Experts
Figure5: Mergedmodelsatscalegeneralizebetter. Weplottheheld-outgeneralizationofthe
mergedmodelfortwomergingmethods. Wealsoincludetheperformanceofbasemodel(dashed
line)andthemultitaskbaseline(yellowline)whichtrainedonamixtureofheld-intasks. Wefind
thatthenumberofconstituentexpertmodels(x-axis,→)hadlittleeffectonzero-shotgeneralization
asshownintheleftandcenterplots. However,increasingmodelsizesignificantlyto64Bimproved
themergedmodel’sperformanceoverthebasemodel(rightplot). FormoredetailsseeSection4.3.
modelsizes(x-axis→),andnumbersofconstituentmodels(subplots). Thissupportsourhypothesis
thatstrongerinstruction-tunedbasemodelsenhancetheperformanceofmergedmodels. Similar
tothefindingsofOrtiz-Jimenezetal.(2024),webelievethatlarge-scaleinstructiontuningfurther
disentanglesmodelweights,facilitatingeffectivemodelmergingandimprovingthebasemodel’s
zero-shotperformance.
4.2 MODELMERGINGBECOMESEASIERWITHBIGGERMODELS
ExperimentalSetup: Inthissection,weexploretheeffectofmodelsizeontheheld-inperformance
ofmergedmodels. Werunexperimentsusingdifferentmodelsizes,basemodels,mergingmethods,
andnumbersofconstituentmodels. Asinthepreviousexperiment,wereporttheaverageresultsover
threerandomseedsandcomparetheperformanceofthemergedmodelstothatofthetask-specific
expertmodels.
Findings: Figure4illustrateshowincreasingbasemodelsizeimpactsmergingeffectiveness. As
modelsizegrows(denotedbycolors,■,■,■,■),mergedmodelperformancegenerallyimproves.
Thispositivetrendisconsistentacrossallbasemodels(differentsubplots),mergingmethods(x-axis
→),andnumbersofconstituentmodels(subplots). Forlargeinstruction-tunedPaLM-2-ITmodels,
themergedmodelsperformnearlyaswellastask-specificexpertmodelsdenotedbydashedline.
Theseresultsdemonstratethatlargermodelsfacilitatemerging. Thissuggestsapromisingapproach
fordevelopingadaptive,modularpost-trainingrecipes. Iftheremainingperformancegapcanbe
furtherreduced,modelmergingcouldbecomeacost-effectivealternativetomultitasktraining. Our
fullresultsacrosssettingsareavailableintheAppendixC.
8
ecnamrofreP
nI-dleH
ecnamrofreP
tuO-dleHPaLM-2, Held-In PaLM-2-IT, Held-In PaLM-2, Held-Out PaLM-2-IT, Held-Out
Expert Model Expert Model 1.4 1.4
1.0 1.0
1.3 1.3
Size
0.8 0.8 1.2 1.2 1B
8B
1.1 1.1 24B
0.6 0.6 64B
1.0 1.0
0.4 PaLM-2 PaLM-2-IT
0.4 0.9 0.9
2 4 6 8 2 4 6 8 2 4 6 8 2 4 6 8
# of Experts
Figure6: Biggermodelsizescanmergemoreexperts. Wemergeexpertsofvarioussizescreated
fromPaLM-2andPaLM-2-ITmodelsandplottheheld-in(left)andheld-out(right)performanceof
mergedmodels. WhilePaLM-2’sheld-inperformancedegradeswithmoreexperts,PaLM-2-IT’s
performancestabilizesatamuchhigherlevel. BothPaLM-2andPaLM-2-ITmodelsconsistently
improveheld-outgeneralization,particularlyat24Band64Bscaleswithincreasingexpertcount. For
moredetailsseeSection4.4.
4.3 MERGEDMODELSATSCALEGENERALIZEBETTER
Experimental Setup: Expert models are created by fine-tuning our base model on specialized
tasks,whichcanleadtoadecreaseinitsgeneralizationcapabilities. Thisraisesthequestion: How
well,ifatall,canthemergedmodelgeneralizetoheld-outtasks? Ideally,themergedmodelshould
performatleastaswellasthebasemodelonthesetasks. Toexplorethis,weevaluatethemerged
model’sperformanceonunseentasksacrossvariousmodelsizes,mergingmethods,andnumbers
ofconstituentmodels. Additionally,wecompareourmergingapproachtoatraditionalmultitask
baseline,whereasinglemodelistrainedonamixtureofalleightheld-intaskcategories. Asdetailed
inSection3,wenormalizetheperformanceofboththemergedandmultitaskmodelagainstthebase
modeltoassessrelativegainsorlossesingeneralizationabilities.
Findings: Figure2andFigure5showthezero-shotgeneralizationperformanceofthemerged
modelusingPaLM-2-ITandPaLM-2,respectively. Overall,wefindthat: ❶Themergedmodels
outperformtheircorrespondingbasemodelsinzero-shotgeneralizationtoheld-outtasks,asindicated
by performance values greater than 1 in most cases; ❷ This improvement is consistent across
variousmodelsizes(denotedbysubplot),basemodels(differentfigures),mergingmethods(different
colors■,■),andnumbersofconstituentmodels(onx-axis→),suggestingthatmerginggenerally
improvesgeneralization;❸Forweakbasemodels(i.e.,PaLM-2)illustratedinFigure5,thenumber
of constituent expert models had little effect on zero-shot generalization (Left and Center plots).
However,increasingmodelsizesignificantlyimprovedthemergedmodel’sperformanceoverthebase
model(Rightplot);❹Incontrast,strongbasemodels(PaLM-2-IT)showadifferenttrend,zero-shot
generalizationmonotonicallyimproveswiththeadditionofmoreexpertmodelsasshowninFigure2.
Wehypothesizethispositivecorrelationarisesfromreducedmodelnoisethroughtheinclusionof
multipleexperts,resultinginbettergeneralization;and❺Notably,ourmergedmodeloutperforms
the multitask baseline when combining more than 6 large instruction-tuned expert models (over
24B). Thisindicatesthatmodelsdevelopedthroughmergingcangeneralizeevenbetterthanthose
trainedonamultitaskmixture,offeringapromisingapproachfordevelopinghighlycapablelanguage
models. OurfullresultsonothermergingmethodsandmodelsizeareavailableinAppendixC.
4.4 BIGGERMODELSIZESCANMERGEMOREEXPERTS
ExperimentalSetup: Whencreatingmultitaskmodels,datasetsfordifferenttasksordomainsare
typicallycombined. Incontrast,modelmerginginvolvesdevelopingseparateexpertmodelsforeach
taskordomainbeforecombiningthem. Previousworkhasshownthatmergingmultiplemodelscan
reducethequalityoftheresultingmodel(Yadavetal.,2024b;Ilharcoetal.,2022). Inthisstudy,we
experimentwithmergingupto8expertmodelsfromvariousbasemodels,modelsizes,andmerging
methodstoassesstheirimpactonsuccessfulmerges.
Findings: Figure 6 shows the held-in and held-out performance of the merged models using
TaskArithmeticasthenumberofconstituentmodelsincreasesshownonx-axis. Resultsforother
9
ecnamrofreP
dezilamroNmethodscanbefoundinAppendixC.Overall,weobservethat: ❶UnlikemergingwithPaLM-2,
whereheld-inperformancetypicallydeclineswithmoremodelmerges,mergingwithstrongerzero-
shotPaLM-2-ITinitiallydropsslightlyinperformancebeforestabilizingasnumberofconstituent
modelsincrease. Forexample,mergingeight8BPaLM-2modelsdecreasesperformancefrom0.66
to0.39whenincreasingthenumberofexpertsfrom2to8,whereasPaLM-2-IT’sperformanceonly
slightlydropsfrom0.91to0.86;❷Intheheld-outevaluations,themergedexpertsbasedonPaLM-2
modelsgenerallyoutperformthebasePaLM-2modelsbyasmallmargin.However,withlargermodel
sizes(64B),theperformanceimprovementincreasessignificantly,achievingabout30percentage
relativeimprovement. WeattributethissubstantialgaintothebasePaLM-2model’sweakzero-shot
performance;and❸ThemergedmodelsbasedonPaLM-2-ITshowimprovedgeneralizationover
PaLM-2-ITacrossallsettings. Additionally,forthe24Band64Bmodels,weobserveaconsistent
increaseingeneralizationcapabilitieswiththeadditionofmoreconstituentexpertmodels.
4.5 MERGINGMETHODSBECOMESIMILARATSCALE
Wefindthatallmergingmethodsexhibitsimilar
Held-In, 64B Held-Out, 64B
performance when merging large instruction-
tunedmodels. Thissuggeststhatsimplermeth- 1.10 Average Task Arithm1.e1t5ic Dare-TIES TIES
ods, such as Averaging, can be sufficient for 1.05 1.10
Expert Model
mergingpowerfullargeexpertmodels. Figure7 1.00 1.05PaLM-2-IT
showstheheld-inandheld-outperformanceof 1.00
0.95
the64BexpertsderivedfromPaLM-2-IT. All 0.95
0.90
merging methods yield comparable results on 0.90
bothheld-inandheld-outtasksforanynumber 0.85 0.85
of constituent models (shown on x-axis). We 0.80 0.80
2 4 6 8 2 4 6 8
hypothesizethatasmodelsizeincreases,expert
# of Experts
models are highly over-parameterized due to
limitedtrainingdata. Consequently,thesubtle Figure7: Differentmergingmethodsbecome
advantagesofcertainmergingtechniques–such similaratscale. Weplottheheld-inandheld-out
ashighlightinginformationviataskvectors(Il- performances of merged 64B PaLM-2-IT mod-
harcoetal.,2022),resolvinginterference(Ya- elsacrossdifferentmergingmethodsandnumbers
davetal.,2024b),orpruning(Yuetal.,2024a) ofconstituentmodels. FormoredetailsseeSec-
–whichbenefitsmallermodels,becomelessrel- tion4.5.
evant. Thisindicatesaneedformorepractical
andscalablemethodstoimprovemergingatscale.
4.6 DISCUSSIONANDTAKEAWAYS
Inthissection,wesummarizekeyinsightsfromourstudyandprovidepracticalrecommendations
for model merging practitioners. Overall, we find that: ❶ Creating expert models from the best
available base model is always beneficial. The quality of the base model can be gauged by its
zero-shotgeneralizationcapabilities. Wehypothesizethatbettergeneralizationleadstoimproved
weightdisentanglement(Ortiz-Jimenezetal.,2024)andaflatterlosslandscape,enhancinglinear
modeconnectivityandfacilitatingmodelmerging;❷Mergedmodelsoftenunderperformcompared
totask-specificexpertmodels,indicatingapotentiallossinperformance. Despitethis,specialized
expertmodelsgenerallyoutperformgeneral-purposemultitaskmodels(Liuetal.,2022;Roziereetal.,
2023;Luoetal.,2023),suggestingthattheperformancelossmaynotbesignificantwhencompared
tomultitaskmodelstrainedonspecifictasks;and❸Ourfindingsindicatethatlarge-scalemerging
canaccommodatemoremodelsandsignificantlyimprovegeneralization,outperformingmultitask
training when a powerful zero-shot base model is employed. ❹ Surprisingly, we find that when
workingwithlargeinstructiontunedmodels,differentmergingmethodperformverysimilary. This
impliesthatusingsimplemergingmethodslikeaveragingwillresultinmodelsthatarecomparable
inqualitywiththemodelsobtainedfrommoreadvancedmergingmethod. Wehopeourresearch
inspiresfurtherfundamentalstudiesondevelopingmorepracticalandscalablemergingmethods.
10
cirteM
dezilamroN5 RELATED WORK
5.1 LOSSLANDSCAPEANDWEIGHTINTERPOLATION
Whilethelossfunctionofaneuralnetworkisgenerallynon-convex, recentwork(Draxleretal.,
2018; Freeman & Bruna, 2016; Garipov et al., 2018; Jordan et al., 2023; Gueta et al., 2023) has
demonstratedthattheparametervaluesfromdifferenttrainingrunscansometimesbeinterpolated
withoutincreasingtheloss(i.e.theyaremode-connected). Manymethods(Kuditipudietal.,2019;
Tatroetal.,2020;Bentonetal.,2021)haveexploredfindingtheselow-losspathsbetweenmodels,
focusingonsimple(notnecessarilylinear)interpolations. Forexample,Frankleetal.(2020)showed
thatifapartoftheoptimizationtrajectoryissharedbetweentwoneuralnetworksthentheycanbe
interpolatedwithoutloweringaccuracy. Ontheotherhand, Neyshaburetal.(2020)showedthat
naivelyinterpolatingtwoneuralnetworkswithcompletelydisjointoptimizationtrajectoriescanresult
in a catastrophic drop in their accuracies. Entezari et al. (2021) hypothesized that if we account
forthepermutationsymmetryofneuralnetworks,thenallneuralnetworksofagivenarchitecture
trainedonthesamedatasetarelinearmodeconnected. Thisassumptionoftheexistenceofalow-loss
”basin”inparameterspaceencompassingthemodelsiscriticalformodelmerging(Ilharcoetal.,
2023). Ainsworthetal.(2022);Singh&Jaggi(2020);Wangetal.(2020);Jordanetal.(2022);Pen˜a
etal.(2023)thereforeusedtechniquesbasedonfindingpermutations(Wangetal.,2020;Ainsworth
etal.,2022)andoptimaltransport(Singh&Jaggi,2020)tobetteralignneuralnetworkstrainedfrom
scratchsothattheycanbemergedorinterpolatedwithoutincreasingtheloss.
5.2 MODELMERGING
Section2.1discussesthemergingmethodsthatweuseforourexperiments,however,thepopularityof
modelmerginghasledtoaever-growingnumberofmethodsandapplicationsofmodelmerging(He
etal.,2024;Daheimetal.,2023;Yadavetal.,2023a;b;2024b;Matena&Raffel,2022a;Jinetal.,
2023). Next, we discuss some of these methods which were omitted due to large scale practical
considerations. TangentTaskArithmetic(Ortiz-Jimenezetal.,2024)fine-tunemodelsinthetangent
spaceforbetterweightdisentanglementwhenusingTaskArithmetic. Akibaetal.(2024)explore
usingevolutionaryalgorithmstochoosewhichlayerstomerge. SLERP(Shoemake,1985)andModel
Stock(Jangetal.,2024)considerthegeometricpropertiesinweightspacewhereSLERPperforms
sphericalinterpolationofmodelweightswhileModelStockapproximatesacenter-closeweightbased
onseveralFTmodels,utilizingtheirbackboneasananchorpoint. Tangetal.(2023)trainamaskthat
learnswhichparametersareimportantforthemergedmodel.Yeetal.(2023)trainagatingnetworkto
predictaweightthatisthenusedtocomputeaweightedaverageofexamplesduringinference. Yadav
etal.(2024a)providesacomprehensivesurveyofmethodsthattrainaroutertoroutebetweenthe
differentmodelstomerge. Moreover,otherapplicationsofmodelmergingincludeintermediate-task
training(Rame´ etal.,2022b;Choshenetal.,2022a;b),continuallearning(Don-Yehiyaetal.,2022),
modelalignment(Rameetal.,2024;Rameetal.;Rame´ etal.,2024),mergingpretrainedmodelsYu
etal.(2024e),ormergingmodelsindifferentmodalities(Sungetal.,2023).
6 CONCLUSIONS
Thisstudyconductedasystematic,large-scaleempiricalinvestigationofmodelmergingwithlarge
languagemodels, addressingthelimitationsofpreviousresearchconfinedtosmall-scalemodels
and limited merging scenarios. Through extensive experiments with PaLM-2 and PaLM-2-IT
models ranging from 1B to 64B parameters, we analyzed the impact of model size, base model
quality,mergingmethod,andnumberofexpertsonbothin-domainandout-of-domaingeneralization
performance. Our findings demonstrate that model merging effectively combines diverse expert
knowledgeparticularlywithincreasingmodelsizeandwithinstruction-tunedbasemodels. Wefound
largermodelstobeconsistentlyeasiertomergeandcanmergemoremodelswithlessperformance
degradation. Importantly, model merging led to enhanced generalization capabilities, with large
mergedmodelssurpassingtheperformanceofmultitaskmodelsonheld-outtasks. Theseresults
showthatwecandevelopmodelsthatgeneralisewellinadecentralizedandmodularmanner.
11REFERENCES
DavidIfeoluwaAdelani,JadeAbbott,GrahamNeubig,DanielD’souza,JuliaKreutzer,Constan-
tine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, et al.
Masakhaner: Namedentityrecognitionforafricanlanguages. TransactionsoftheAssociationfor
ComputationalLinguistics,9:1116–1131,2021.
SamuelKAinsworth,JonathanHayase,andSiddharthaSrinivasa. Gitre-basin: Mergingmodels
modulopermutationsymmetries,2022. https://arxiv.org/abs/2209.04836.
TakuyaAkiba, MakotoShing, YujinTang, QiSun, andDavidHa. Evolutionaryoptimizationof
modelmergingrecipes. arXivpreprintarXiv:2403.13187,2024.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,etal. Palm2technicalreport. arXiv
preprintarXiv:2305.10403,2023.
GregoryBenton,WesleyMaddox,SanaeLotfi,andAndrewGordonGordonWilson. Losssurface
simplexesformodeconnectingvolumesandfastensembling. InInternationalConferenceon
MachineLearning(ICML),2021. https://arxiv.org/abs/2102.13042.
Leshem Choshen, Elad Venezian, Shachar Don-Yehia, Noam Slonim, and Yoav Katz. Where to
start? analyzingthepotentialvalueofintermediatemodels, 2022a. https://arxiv.org/
abs/2211.00107.
LeshemChoshen,EladVenezian,NoamSlonim,andYoavKatz. Fusingfinetunedmodelsforbetter
pretraining. arXivpreprintarXiv:2204.03044,2022b.
HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,YunxuanLi,
XuezhiWang,MostafaDehghani,SiddharthaBrahma,etal.Scalinginstruction-finetunedlanguage
models. JournalofMachineLearningResearch,25(70):1–53,2024.
Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment
challenge. InMachineLearningChallengesWorkshop, 2005. https://link.springer.
com/chapter/10.1007/11736790_9.
NicoDaheim,ThomasMo¨llenhoff,EdoardoMariaPonti,IrynaGurevych,andMohammadEmtiyaz
Khan. Modelmergingbyuncertainty-basedgradientmatching. arXivpreprintarXiv:2310.12808,
2023.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-
phrases. InInternationalWorkshoponParaphrasing,2005. https://aclanthology.org/
I05-5002.
ShacharDon-Yehiya,EladVenezian,ColinRaffel,NoamSlonim,YoavKatz,andLeshemChoshen.
Coldfusion: Collaborativedescentfordistributedmultitaskfinetuning,2022.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,
andNeilHoulsby.Animageisworth16x16words:Transformersforimagerecognitionatscale.In
InternationalConferenceonLearningRepresentations(ICLR),2021. https://openreview.
net/forum?id=YicbFdNTTy.
FelixDraxler,KambisVeschgini,ManfredSalmhofer,andFredHamprecht. Essentiallynobarriers
inneuralnetworkenergylandscape. InInternationalConferenceonMachineLearning(ICML),
2018. https://arxiv.org/abs/1803.00885.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
12Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation
invarianceinlinearmodeconnectivityofneuralnetworks. arXivpreprintarXiv:2110.06296,2021.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode
connectivityandthelotterytickethypothesis. InInternationalConferenceonMachineLearning
(ICML),2020. https://proceedings.mlr.press/v119/frankle20a.html.
CDanielFreemanandJoanBruna. Topologyandgeometryofhalf-rectifiednetworkoptimization.
arXivpreprintarXiv:1611.01540,2016.
TimurGaripov,PavelIzmailov,DmitriiPodoprikhin,DmitryVetrov,andAndrewGordonWilson.
Losssurfaces,modeconnectivity,andfastensemblingofdnns. InAdvancesinNeuralInformation
ProcessingSystems(NeurIPS),2018. https://arxiv.org/abs/1802.10026.
AlmogGueta,EladVenezian,ColinRaffel,NoamSlonim,YoavKatz,andLeshemChoshen. Knowl-
edgeisaregioninweightspaceforfine-tunedlanguagemodels. arXivpreprintarXiv:2302.04863,
2023.
PengchengHe,XiaodongLiu,JianfengGao,andWeizhuChen. Deberta: Decoding-enhancedbert
withdisentangledattention. InInternationalConferenceonLearningRepresentations,2021. URL
https://openreview.net/forum?id=XPZIaotutsD.
YifeiHe,YuzhengHu,YongLin,TongZhang,andHanZhao. Localize-and-stitch: Efficientmodel
mergingviasparsetaskarithmetic. arXivpreprintarXiv:2408.13656,2024.
LifuHuang,RonanLeBras,ChandraBhagavatula,andYejinChoi. Cosmosqa: Machinereading
comprehensionwithcontextualcommonsensereasoning. arXivpreprintarXiv:1909.00277,2019.
Shih-Cheng Huang, Pin-Zu Li, Yu-chi Hsu, Kuang-Ming Chen, Yu Tung Lin, Shih-Kai Hsiao,
RichardTsai,andHung-yiLee. Chatvector: AsimpleapproachtoequipLLMswithinstruction
followingandmodelalignmentinnewlanguages. InProceedingsofthe62ndAnnualMeeting
oftheAssociationforComputationalLinguistics(ACL2024), pp.10943–10959, 2024a. URL
https://aclanthology.org/2024.acl-long.590.
Shih-Cheng Huang, Pin-Zu Li, Yu-chi Hsu, Kuang-Ming Chen, Yu Tung Lin, Shih-Kai Hsiao,
RichardTsai,andHung-yiLee. Chatvector: AsimpleapproachtoequipLLMswithinstruction
followingandmodelalignmentinnewlanguages. InLun-WeiKu, AndreMartins, andVivek
Srikumar(eds.),Proceedingsofthe62ndAnnualMeetingoftheAssociationforComputational
Linguistics (Volume 1: Long Papers), pp. 10943–10959, Bangkok, Thailand, August 2024b.
AssociationforComputationalLinguistics. doi: 10.18653/v1/2024.acl-long.590. URLhttps:
//aclanthology.org/2024.acl-long.590.
GabrielIlharco,MarcoTulioRibeiro,MitchellWortsman,SuchinGururangan,LudwigSchmidt,
Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint
arXiv:2212.04089,2022.
GabrielIlharco,MarcoTulioRibeiro,MitchellWortsman,LudwigSchmidt,HannanehHajishirzi,
and Ali Farhadi. Editing models with task arithmetic. In The Eleventh International Confer-
enceonLearningRepresentations,2023. URLhttps://openreview.net/forum?id=
6t0Kwf8-jrj.
Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First quora dataset
release: Question pairs, 2017. URL https://data.quora.com/
First-Quora-Dataset-Release-Question-Pairs.
Dong-Hwan Jang, Sangdoo Yun, and Dongyoon Han. Model stock: All we need is just a few
fine-tunedmodels,2024. URLhttps://arxiv.org/abs/2403.19522.
JoelJang,SeungoneKim,SeonghyeonYe,DoyoungKim,LajanugenLogeswaran,MoontaeLee,
KyungjaeLee,andMinjoonSeo. Exploringthebenefitsoftrainingexpertlanguagemodelsover
instructiontuning. arXivpreprintarXiv:2302.03202,2023.
XisenJin,XiangRen,DanielPreotiuc-Pietro,andPengxiangCheng. Datalessknowledgefusionby
mergingweightsoflanguagemodels. arXivpreprintarXiv:2212.09849,2022.
13XisenJin,XiangRen,DanielPreotiuc-Pietro,andPengxiangCheng. Datalessknowledgefusion
bymergingweightsoflanguagemodels. InTheEleventhInternationalConferenceonLearning
Representations,2023. URLhttps://openreview.net/forum?id=FCnohuR6AnM.
KellerJordan,HanieSedghi,OlgaSaukh,RahimEntezari,andBehnamNeyshabur. Repair: Renor-
malizingpermutedactivationsforinterpolationrepair. arXivpreprintarXiv:2211.08403,2022.
Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. REPAIR:
REnormalizing permuted activations for interpolation repair. In The Eleventh International
ConferenceonLearningRepresentations,2023. URLhttps://openreview.net/forum?
id=gU5sJ6ZggcX.
RohithKuditipudi,XiangWang,HoldenLee,YiZhang,ZhiyuanLi,WeiHu,RongGe,andSanjeev
Arora. Explaininglandscapeconnectivityoflow-costsolutionsformultilayernets. Advancesin
NeuralInformationProcessingSystems(NeurIPS),2019. https://arxiv.org/abs/1906.
06247.
Re´miLebret,DavidGrangier,andMichaelAuli. Neuraltextgenerationfromstructureddatawith
applicationtothebiographydomain. arXivpreprintarXiv:1603.07771,2016.
JensLehmann, RobertIsele, MaxJakob, AnjaJentzsch, DimitrisKontokostas, PabloNMendes,
SebastianHellmann,MohamedMorsey,PatrickVanKleef,So¨renAuer,etal. Dbpedia–alarge-
scale,multilingualknowledgebaseextractedfromwikipedia. Semanticweb,6(2):167–195,2015.
Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In
Thirteenthinternationalconferenceontheprinciplesofknowledgerepresentationandreasoning,
2012a.
HectorLevesque,ErnestDavis,andLeoraMorgenstern. Thewinogradschemachallenge. Thirteenth
InternationalConferenceonthePrinciplesofKnowledgeRepresentationandReasoning,2012b.
BillYuchenLin,WangchunshuZhou,MingShen,PeiZhou,ChandraBhagavatula,YejinChoi,and
XiangRen. CommonGen: Aconstrainedtextgenerationchallengeforgenerativecommonsense
reasoning. InFindingsoftheAssociationforComputationalLinguistics: EMNLP,2020. https:
//www.aclweb.org/anthology/2020.findings-emnlp.165.
KevinLin, OyvindTafjord, PeterClark, andMattGardner. Reasoningoverparagrapheffectsin
situations. arXivpreprintarXiv:1908.05852,2019.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and
ColinARaffel. Few-shotparameter-efficientfine-tuningisbetterandcheaperthanin-context
learning. AdvancesinNeuralInformationProcessingSystems,35:1950–1965,2022.
YinhanLiu, MyleOtt, Naman Goyal, JingfeiDu, MandarJoshi, DanqiChen, OmerLevy, Mike
Lewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretraining
approach,2019. https://arxiv.org/abs/1907.11692.
ShayneLongpre,LeHou,TuVu,AlbertWebson,HyungWonChung,YiTay,DennyZhou,QuocV
Le,BarretZoph,JasonWei,etal. Theflancollection: Designingdataandmethodsforeffective
instructiontuning. arXivpreprintarXiv:2301.13688,2023.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng,
Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical
reasoningforlargelanguagemodelsviareinforcedevol-instruct. arXivpreprintarXiv:2308.09583,
2023.
AndrewMaas,RaymondEDaly,PeterTPham,DanHuang,AndrewYNg,andChristopherPotts.
Learningwordvectorsforsentimentanalysis. InProceedingsofthe49thannualmeetingofthe
associationforcomputationallinguistics: Humanlanguagetechnologies,pp.142–150,2011.
MichaelSMatenaandColinARaffel. Mergingmodelswithfisher-weightedaveraging. Advancesin
NeuralInformationProcessingSystems,35:17703–17716,2022a.
14MichaelSMatenaandColinARaffel. Mergingmodelswithfisher-weightedaveraging. Advancesin
NeuralInformationProcessingSystems,35:17703–17716,2022b.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficientlearningofdeepnetworksfromdecentralizeddata. InArtificialintelli-
genceandstatistics,pp.1273–1282.PMLR,2017.
MohammedMuqeeth,HaokunLiu,YufanLiu,andColinRaffel. Learningtorouteamongspecialized
experts for zero-shot generalization. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller,
AdrianWeller,NuriaOliver,JonathanScarlett,andFelixBerkenkamp(eds.),Proceedingsofthe
41st International Conference on Machine Learning, volume 235 of Proceedings of Machine
LearningResearch,pp.36829–36846.PMLR,21–27Jul2024. URLhttps://proceedings.
mlr.press/v235/muqeeth24a.html.
Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the sum-
mary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint
arXiv:1808.08745,2018.
BehnamNeyshabur,HanieSedghi,andChiyuanZhang.Whatisbeingtransferredintransferlearning?
Advancesinneuralinformationprocessingsystems,33:512–523,2020.
YixinNie,AdinaWilliams,EmilyDinan,MohitBansal,JasonWeston,andDouweKiela.Adversarial
nli: Anewbenchmarkfornaturallanguageunderstanding. arXivpreprintarXiv:1910.14599,2019.
GuillermoOrtiz-Jimenez,AlessandroFavero,andPascalFrossard. Taskarithmeticinthetangent
space: Improved editing of pre-trained models. Advances in Neural Information Processing
Systems,36,2024.
Fidel A Guerrero Pen˜a, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric
Granger,andMarcoPedersoli. Re-basinviaimplicitsinkhorndifferentiation. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.20237–20246,2023.
Mohammad Taher Pilehvar and Jose Camacho-Collados. Wic: the word-in-context dataset for
evaluatingcontext-sensitivemeaningrepresentations. arXivpreprintarXiv:1808.09121,2018.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
ColinRaffel. Acalltobuildmodelslikewebuildopen-sourcesoftware,2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. Journal of Machine Learning Research (JMLR), 2020a. http://jmlr.org/
papers/v21/20-074.html.
ColinRaffel,NoamM.Shazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-texttransformer. ArXiv,abs/1910.10683,2020b.
Alexandre Rame, Nino Vieillard, Leonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier
Bachem,andJohanFerret. Warm:Onthebenefitsofweightaveragedrewardmodels. InForty-first
InternationalConferenceonMachineLearning.
AlexandreRame´,KartikAhuja,JianyuZhang,MatthieuCord,Le´onBottou,andDavidLopez-Paz.
Modelratatouille: Recyclingdiversemodelsforout-of-distributiongeneralization. arXivpreprint
arXiv:2212.10445,2022a.
AlexandreRame´,KartikAhuja,JianyuZhang,MatthieuCord,Le´onBottou,andDavidLopez-Paz.
Modelratatouille: Recyclingdiversemodelsforout-of-distributiongeneralization. arXivpreprint
arXiv:2212.10445,2022b.
15Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor,
LaureSoulier,andMatthieuCord. Rewardedsoups: towardspareto-optimalalignmentbyinterpo-
latingweightsfine-tunedondiverserewards. AdvancesinNeuralInformationProcessingSystems,
36,2024.
AlexandreRame´, JohanFerret, NinoVieillard, RobertDadashi, Le´onardHussenot, Pierre-Louis
Cedoz,PierGiuseppeSessa,SertanGirgin,ArthurDouillard,andOlivierBachem. Warp: Onthe
benefitsofweightaveragedrewardedpolicies. arXivpreprintarXiv:2406.16768,2024.
MelissaRoemmele,CosminAdrianBejan,andAndrewSGordon. Choiceofplausiblealternatives:
Anevaluationofcommonsensecausalreasoning. In2011AAAIspringsymposiumseries,2011.
BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,Yossi
Adi,JingyuLiu,RomainSauvestre,TalRemez,etal. Codellama: Openfoundationmodelsfor
code. arXivpreprintarXiv:2308.12950,2023.
VictorSanh,AlbertWebson,ColinRaffel,StephenHBach,LintangSutawika,ZaidAlyafeai,Antoine
Chaffin,ArnaudStiegler,TevenLeScao,ArunRaja,etal. Multitaskpromptedtrainingenables
zero-shottaskgeneralization. arXivpreprintarXiv:2110.08207,2021a.
VictorSanh,AlbertWebson,ColinRaffel,StephenHBach,LintangSutawika,ZaidAlyafeai,Antoine
Chaffin,ArnaudStiegler,TevenLeScao,ArunRaja,etal. Multitaskpromptedtrainingenables
zero-shottaskgeneralization. InInternationalConferenceonLearningRepresentations(ICLR),
2021b. https://arxiv.org/abs/2110.08207.
Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with
pointer-generatornetworks. arXivpreprintarXiv:1704.04368,2017.
VirajShah,NatanielRuiz,ForresterCole,ErikaLu,SvetlanaLazebnik,YuanzhenLi,andVarun
Jampani. Ziplora: Any subject in any style by effectively merging loras. arXiv preprint
arXiv:2311.13600,2023.
NoamShazeerandMitchellStern. Adafactor: Adaptivelearningrateswithsublinearmemorycost.
InInternationalConferenceonMachineLearning.PMLR,2018.
Ken Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th annual
conferenceonComputergraphicsandinteractivetechniques,pp.245–254,1985.
SidakPalSinghandMartinJaggi.Modelfusionviaoptimaltransport.AdvancesinNeuralInformation
ProcessingSystems,33:22045–22055,2020.
KaiSun,DianYu,JianshuChen,DongYu,YejinChoi,andClaireCardie. Dream: Achallengedata
setandmodelsfordialogue-basedreadingcomprehension. TransactionsoftheAssociationfor
ComputationalLinguistics,7:217–231,2019.
Yi-LinSung,LinjieLi,KevinLin,ZheGan,MohitBansal,andLijuanWang. Anempiricalstudyof
multimodalmodelmerging. EmpiricalMethodsinNaturalLanguageProcessing(Findings),2023.
DerekTam,MohitBansal,andColinRaffel. Mergingbymatchingmodelsintasksubspaces. arXiv
preprintarXiv:2312.04339,2023.
Derek Tam, Mohit Bansal, and Colin Raffel. Merging by matching models in task parameter
subspaces. TransactionsonMachineLearningResearch,2024.
Anke Tang, Li Shen, Yong Luo, Liang Ding, Han Hu, Bo Du, and Dacheng Tao. Concrete
subspace learning based interference elimination for multi-task model fusion. arXiv preprint
arXiv:2312.06173,2023.
NormanTatro,Pin-YuChen,PayelDas,IgorMelnyk,PrasannaSattigeri,andRongjieLai.Optimizing
modeconnectivityvianeuronalignment. AdvancesinNeuralInformationProcessingSystems,33:
15300–15311,2020.
16Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya
Bhupatiraju, Le´onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame´, et al.
Gemma2: Improvingopenlanguagemodelsatapracticalsize. arXivpreprintarXiv:2408.00118,
2024.
FanqiWan,XintingHuang,DengCai,XiaojunQuan,WeiBi,andShumingShi. Knowledgefusion
oflargelanguagemodels. arXivpreprintarXiv:2401.10491,2024a.
FanqiWan,ZiyiYang,LongguangZhong,XiaojunQuan,XintingHuang,andWeiBi. Fusechat:
Knowledgefusionofchatmodels. arXivpreprintarXiv:2402.16107,2024b.
AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelRBowman. Glue:
Amulti-taskbenchmarkandanalysisplatformfornaturallanguageunderstanding. InInternational
ConferenceonLearningRepresentations(ICLR),2018. https://arxiv.org/abs/1804.
07461.
HongyiWang,MikhailYurochkin,YuekaiSun,DimitrisPapailiopoulos,andYasamanKhazaeni.Fed-
eratedlearningwithmatchedaveraging. InInternationalConferenceonLearningRepresentations,
2020.
Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,
AriSMorcos,HongseokNamkoong,AliFarhadi,YairCarmon,SimonKornblith,andLudwig
Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy
withoutincreasinginferencetime.InProceedingsofthe39thInternationalConferenceonMachine
Learning,volume162ofProceedingsofMachineLearningResearch,pp.23965–23998.PMLR,
2022a. URLhttps://proceedings.mlr.press/v162/wortsman22a.html.
MitchellWortsman,GabrielIlharco,JongWookKim,MikeLi,SimonKornblith,RebeccaRoelofs,
RaphaelGontijoLopes,HannanehHajishirzi,AliFarhadi,HongseokNamkoong,etal. Robust
fine-tuningofzero-shotmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pp.7959–7971,2022b.
Prateek Yadav, Leshem Choshen, Colin Raffel, and Mohit Bansal. Compeft: Compression for
communicatingparameterefficientupdatesviasparsificationandquantization,2023a.
PrateekYadav,QingSun,HantianDing,XiaopengLi,DejiaoZhang,MingTan,ParminderBhatia,
Xiaofei Ma, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal, and Bing Xiang.
Exploring continual learning for code generation models. In Proceedings of the 61st Annual
MeetingoftheAssociationforComputationalLinguistics(Volume2: ShortPapers),pp.782–792,
Toronto,Canada,July2023b.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.
acl-short.68. URLhttps://aclanthology.org/2023.acl-short.68.
PrateekYadav,ColinRaffel,MohammedMuqeeth,LucasCaccia,HaokunLiu,TianlongChen,Mohit
Bansal,LeshemChoshen,andAlessandroSordoni. Asurveyonmodelmoerging: Recyclingand
routingamongspecializedexpertsforcollaborativelearning,2024a. URLhttps://arxiv.
org/abs/2408.07057.
Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, and Mohit Bansal. Ties-merging:
Resolvinginterferencewhenmergingmodels.AdvancesinNeuralInformationProcessingSystems,
36,2024b.
EnnengYang,ZhenyiWang,LiShen,ShiweiLiu,GuibingGuo,XingweiWang,andDachengTao.
Adamerging: Adaptivemodelmergingformulti-tasklearning. arXivpreprintarXiv:2310.02575,
2023.
YiYang,Wen-tauYih,andChristopherMeek.WikiQA:Achallengedatasetforopen-domainquestion
answering. InProceedingsofthe2015ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,pp.2013–2018,Lisbon,Portugal,September2015.AssociationforComputational
Linguistics. doi: 10.18653/v1/D15-1237. URLhttps://aclanthology.org/D15-1237.
ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamW.Cohen,RuslanSalakhutdinov,
andChristopherD.Manning. HotpotQA:Adatasetfordiverse,explainablemulti-hopquestion
answering. InConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),2018.
17PengYe,ChenyuHuang,MingzhuShen,TaoChen,YongqiHuang,YuningZhang,andWanliOuyang.
Mergingvisiontransformersfromdifferenttasksanddomains. arXivpreprintarXiv:2312.16240,
2023.
Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario:
Absorbingabilitiesfromhomologousmodelsasafreelunch.InForty-firstInternationalConference
onMachineLearning,2024a.
LeYu,BowenYu,HaiyangYu,FeiHuang,andYongbinLi.Extendmodelmergingfromfine-tunedto
pre-trainedlargelanguagemodelsviaweightdisentanglement. arXivpreprintarXiv:2408.03092,
2024b.
LeYu,BowenYu,HaiyangYu,FeiHuang,andYongbinLi.Extendmodelmergingfromfine-tunedto
pre-trainedlargelanguagemodelsviaweightdisentanglement. arXivpreprintarXiv:2408.03092,
2024c.
Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario:
Absorbingabilitiesfromhomologousmodelsasafreelunch.InForty-firstInternationalConference
onMachineLearning,2024d.
LeYu,BowenYu,HaiyangYu,FeiHuang,andYongbinLi.Extendmodelmergingfromfine-tunedto
pre-trainedlargelanguagemodelsviaweightdisentanglement. arXivpreprintarXiv:2408.03092,
2024e.
RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Canamachine
reallyfinishyoursentence? arXivpreprintarXiv:1905.07830,2019.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional net-
works for text classification. In Advances in Neural Information Processing Sys-
tems (NeurIPS), 2015. https://proceedings.neurips.cc/paper/2015/file/
250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.
YiranZhao, WenxuanZhang, HuimingWang, KenjiKawaguchi, andLidongBing. Adamergex:
Cross-lingualtransferwithlargelanguagemodelsviaadaptiveadaptermerging. arXivpreprint
arXiv:2402.18913,2024.
Jing Zhou, Zongyu Lin, Yanan Zheng, Jian Li, and Zhilin Yang. Not all tasks are born equal:
Understandingzero-shotgeneralization. InTheEleventhInternationalConferenceonLearning
Representations,2022.
A DETAILED TASK DESCRIPTIONS.
WeadopttheexperimentalsettingfromtheT0mixture(Sanhetal.,2021a)whichcontains8held-in
and4held-outtaskcategoriesSpecifically,the8held-intaskcategoriesincludeMultiple-choiceQA
(withselecteddatasetsDREAM(Sunetal.,2019),CosmosQA(Huangetal.,2019)),ExtractiveQa
(AdversarialQA(Adelanietal.,2021),ROPES(Linetal.,2019)),Closed-BookQA(HotpotQA(Yang
etal.,2018),WikiQA(Yangetal.,2015)),SentimentAnalysis(AppReviews(),IMDB(Maasetal.,
2011)), Topic Classification (AG News (Zhang et al., 2015), DBPedia (Lehmann et al., 2015)),
Structure-to-text(CommonGen(Linetal.,2020),WikiBio(Lebretetal.,2016)),Summarization
(CNNDailyMail(Seeetal.,2017),XSum(Narayanetal.,2018))andParaphraseIdentification
(MRPC(Dolan&Brockett,2005),QQP(Iyeretal.,2017)). Similary,the4held-outtaskcategories
areSentenceCompletion(withselecteddatasetCOPA(Roemmeleetal.,2011),HellaSwag(Zellers
et al., 2019)), Natural Language Inference (ANLI (Nie et al., 2019), RTE (Dagan et al., 2005)),
CoreferenceResolution(WSC(Levesqueetal.,2012b),Winogrande(Levesqueetal.,2012a))and
WordSenseDisambiguation(WiC(Pilehvar&Camacho-Collados,2018)).
18B EXPERT TRAINING DETAILS
Inourresearch,weutilizedtwobasemodels,namelyPaLM-2andPaLM-2-ITtocreatespecialized
expertmodels.WetrainthePaLM-2modelforanadditional60000stepsontheFlan-v2dataset(Long-
preetal.,2023)toobtainthePaLM-2-ITmodel. WeremovedtheT0tasksfromtheflanmixturein
ordertotrainingexpertsontheminfuture. Manyofthesetrainingjobswereearlystoppeddueto
convergence. WeusedShardedAdafactor(Shazeer&Stern,2018)optimizeralongwithacosine
decayandalearningrateof1e-4for1B,24B,and64Bmodelsizesand3e-5for8Bmodel. Weusea
dropoutvalueof0.05. FollowingChungetal.(2024),weusedaninputlengthof2048andoutput
lengthof512.Tocreateexpertmodelsweperformfullfinetuningwiththefollowinghyperparameters.
Fortrainingtheexpertsmodel,forallmodelsize,wetrainbydefaultfor2000stepswithalearning
rateof3e-5anddropoutof0.05. Forsometaskweadjustthenumberofstepsdependinguponthe
convergence. Forthepurposeofevaluatingclassificationtasks(Raffeletal.,2020b),weperformrank
classification. Inthismethod,themodel’slogprobabilitiesforallpotentiallabelstringsareranked.
Themodel’spredictionisdeemedaccurateifthechoicerankedhighestalignswiththecorrectanswer.
Itshouldbenotedthatrankclassificationevaluationcanaccommodatebothclassificationtasksand
multiple-choicetasks.
C FULL RESULT TABLES
Inthissection,weprovidetheresultforthefullgridofexperimentsthatweperformed. Theresults
containinformationaboutanyoftheplotsthatarenotprovidedinthemainpaper. Table3and4
presenttheheld-inandheld-outperformanceofPaLM-2modelacrossallmodelsizes,basemodels,
mergingmethods, andthenumberofexpertsbeingmerged. Similarly, Table1and2presentthe
held-inandheld-outperformanceofPaLM-2-ITmodel.
Table1: Thetablereportstheaveragenormalizedperformancefortheheld-intaskswhenmerging
expertscreatedfromPaLM-2-ITbasemodels.
Merging Method(↓) 1B 8B 24B 64B
# of Experts(→) 2 4 6 8 2 4 6 8 2 4 6 8 2 4 6 8
Average 0.85 0.78 0.81 0.83 0.90 0.82 0.82 0.85 0.94 0.84 0.80 0.77 0.97 0.91 0.89 0.93
Task Arithmetic 0.91 0.82 0.84 0.86 0.95 0.86 0.85 0.88 0.96 0.90 0.91 0.92 1.00 0.91 0.90 0.93
Dare-TIES 0.90 0.81 0.83 0.86 0.93 0.86 0.84 0.88 0.94 0.89 0.87 0.88 0.97 0.91 0.89 0.93
TIES 0.89 0.81 0.82 0.85 0.93 0.86 0.84 0.88 0.95 0.88 0.86 0.86 0.97 0.90 0.89 0.93
Multitask 0.97 0.96 0.96 0.96 0.96 0.96 0.97 0.96 0.99 0.97 0.98 0.98 0.99 0.98 0.98 0.99
Table2: Thetablereportstheaveragenormalizedperformanceontheheld-outtaskswhenmerging
expertscreatedfromPaLM-2-ITbasemodels.
Merging Method(↓) 1B 8B 24B 64B
# of Experts(→) 2 4 6 8 2 4 6 8 2 4 6 8 2 4 6 8
Average 0.99 1.00 1.04 1.05 1.03 1.02 1.03 1.02 1.05 1.10 1.11 1.16 1.00 1.03 1.06 1.09
Task Arithmetic 1.03 1.03 1.04 1.05 1.06 1.05 1.05 1.03 1.05 1.10 1.13 1.18 1.00 1.03 1.06 1.09
Dare-TIES 1.02 1.03 1.04 1.05 1.05 1.04 1.04 1.03 1.05 1.10 1.12 1.17 1.00 1.03 1.06 1.09
TIES 1.02 1.03 1.04 1.05 1.06 1.05 1.06 1.04 1.04 1.09 1.11 1.16 1.00 1.03 1.06 1.10
Multitask 1.11 1.11 1.11 1.11 1.12 1.12 1.12 1.12 1.18 1.18 1.18 1.18 1.05 1.05 1.05 1.05
Table3: Thetablereportstheaveragenormalizedperformanceontheheld-intaskswhenmerging
expertscreatedfromPaLM-2basemodels.
Merging Method(↓) 1B 8B 24B 64B
# of Experts(→) 2 4 6 8 2 4 6 8 2 4 6 8 2 4 6 8
Average 0.63 0.44 0.36 0.26 0.66 0.53 0.50 0.32 0.70 0.48 0.51 0.27 0.80 0.74 0.69 0.67
Task Arithmetic 0.66 0.52 0.44 0.39 0.68 0.54 0.54 0.42 0.72 0.56 0.60 0.46 0.80 0.74 0.69 0.67
Dare-TIES 0.65 0.51 0.42 0.37 0.66 0.51 0.51 0.32 0.67 0.44 0.51 0.27 0.80 0.74 0.69 0.67
TIES 0.66 0.50 0.41 0.33 0.67 0.52 0.48 0.29 0.68 0.49 0.52 0.27 0.80 0.71 0.65 0.56
Multitask 0.88 0.88 0.88 0.87 1.06 1.04 1.04 1.06 1.25 1.15 1.11 1.20 0.97 0.96 0.96 0.96
19Table4: Thetablereportstheaveragenormalizedperformanceontheheld-outtaskswhenmerging
expertscreatedfromPaLM-2basemodels.
Merging Method(↓) 1B 8B 24B 64B
# of Experts(→) 2 4 6 8 2 4 6 8 2 4 6 8 2 4 6 8
Average 0.98 1.00 1.02 1.04 1.01 0.97 1.02 0.98 0.95 0.85 0.93 0.83 1.28 1.24 1.29 1.25
Task Arithmetic 1.01 1.03 1.05 1.07 1.06 1.03 1.04 1.00 1.05 1.03 1.10 1.08 1.29 1.28 1.36 1.35
Dare-TIES 0.99 1.01 1.04 1.05 1.02 1.00 1.05 1.01 0.97 0.89 0.99 0.90 1.28 1.24 1.28 1.24
TIES 1.05 1.06 1.03 1.04 1.07 1.04 1.02 0.99 1.01 0.93 0.98 0.90 1.31 1.22 1.24 1.15
Multitask 1.10 1.10 1.10 1.10 1.62 1.62 1.62 1.62 1.51 1.51 1.51 1.51 1.73 1.73 1.73 1.72
20