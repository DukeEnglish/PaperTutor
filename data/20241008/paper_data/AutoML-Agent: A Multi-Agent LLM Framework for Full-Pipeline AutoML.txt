Preprint. Underreview.
AUTOML-AGENT: A MULTI-AGENT LLM
FRAMEWORK FOR FULL-PIPELINE AUTOML
PataraTrirat1∗,WonyongJeong2,SungJuHwang1,2
1KAIST,2DeepAuto.ai
{patara.t, sungju.hwang}@kaist.ac.kr,young@deepauto.ai
ABSTRACT
Automatedmachinelearning(AutoML)acceleratesAIdevelopmentbyautomat-
ing tasks in the development pipeline, such as optimal model search and hyper-
parametertuning. ExistingAutoMLsystems oftenrequiretechnicalexpertise to
set up complex tools, which is in general time-consuming and requires a large
amountofhumaneffort.Therefore,recentworkshavestartedexploitinglargelan-
guagemodels(LLM)tolessensuchburdenandincreasetheusabilityofAutoML
frameworks via a natural language interface, allowing non-expert users to build
their data-driven solutions. These methods, however, are usually designed only
foraparticularprocessintheAIdevelopmentpipelineanddonotefficientlyuse
the inherent capacity of the LLMs. This paper proposes AutoML-Agent, a novel
multi-agentframeworktailoredforfull-pipelineAutoML,i.e.,fromdataretrieval
to model deployment. AutoML-Agent takes user’s task descriptions, facilitates
collaboration between specialized LLM agents, and delivers deployment-ready
models. Unlike existing work, instead of devising a single plan, we introduce a
retrieval-augmented planning strategy to enhance exploration to search for more
optimal plans. We also decompose each plan into sub-tasks (e.g., data prepro-
cessingandneuralnetworkdesign)eachofwhichissolvedbyaspecializedagent
we build via prompting executing in parallel, making the search process more
efficient. Moreover, we propose a multi-stage verification to verify executed re-
sults and guide the code generation LLM in implementing successful solutions.
Extensive experiments on seven downstream tasks using fourteen datasets show
thatAutoML-AgentachievesahighersuccessrateinautomatingthefullAutoML
process,yieldingsystemswithgoodperformancethroughoutthediversedomains.
1 INTRODUCTION
Automated machine learning(AutoML) has significantly reduced the need for technical expertise
and human labors in developing effective data-driven solutions by automating each process in the
AIdevelopmentpipeline(Yaoetal.,2018;Renetal.,2020;Heetal.,2021),suchasfeatureengi-
neering,modelselection,andhyperparameteroptimization(HPO).However,currentAutoMLsys-
tems(Gijsbersetal.,2024)oftennecessitateprogrammingexpertisetoconfigurecomplextoolsand
resources,potentiallycreatingbarriersforalargerpoolofuserswithlimitedskillsandknowledge.
TomakeAutoMLframeworksmoreaccessible,recentstudies(Triratetal.,2021;Viswanathanetal.,
2023;Lietal.,2023;Hollmannetal.,2023b;Liuetal.,2024a;Zhangetal.,2023;Shenetal.,2023;
Zhangetal.,2024a;Hongetal.,2024a;Guoetal.,2024a;Yangetal.,2024)havesuggestedtouse
naturallanguageinterfaceswithlargelanguagemodels(LLM)formachinelearning(ML)anddata
science(DS)tasks. Nevertheless,thesepreviousLLM-basedAutoMLframeworksonlyconsidered
a limited number of tasks due to their restricted designs, either only for a process in the pipeline
(e.g., featureengineering(Hollmannetal.,2023b;Lietal.,2024;Malbergetal.,2024), HPO(Liu
etal.,2024a;Zhangetal.,2024a),andmodelselection(Zhangetal.,2023;Shenetal.,2023))orfor
aspecificgroupofdownstreamtasks(e.g., naturallanguageprocessing(Viswanathanetal.,2023)
andcomputervision(Yangetal.,2024)). Inaddition,mostmethodsoverlooktheinherentcapability
∗WorkdoneduringaninternshipatDeepAuto.ai.
1
4202
tcO
3
]GL.sc[
1v85920.0142:viXraPreprint. Underreview.
of LLMs to search for promising models by performing actual training of the candidate models
duringthesearchprocess,makingitprohibitivelycostlyandslow.
For an AutoML framework to be truly prac- Input: User's prompt with task description, requirements, and/or constraints
tical, it should perform end-to-end AutoML, We need a solution for detecting spam messages within the SMS Spam Collection dataset.
Given the nature of our real-time messaging application, the model must be lightweight yet
considering both the data aspects (retrieval, User e isf f aic bie on vt e, 9p 5o %ss ,i b wly it ha as t lr ae ta em ncli yn e bd e lL oS wT 1M 0 0o mr G illR isU ec a or nc dh sit e pc et ru mre e. sT sh ae g ee x op ne c mte od b ic lela ds es vif ii cc ea sti .on accuracy
preprocessing, and feature engineering) and
AutoML-Agent
model aspects (selection, HPO, and deploy-
ment). Thisisbecauseaprocessinoneaspect
Agent
canaffectsubsequentprocessesintheother,po- Manager Prompt Analysis Data Handling Model Development Deployment
tentially leading to suboptimal solutions when
Output: Deployable model optimized for the given task and target platform
combining results from different frameworks. Accuracy: 98.9%
Meanwhile,theAutoMLframeworkshouldbe ...
computationally efficient, using strategies to Optimized Inference Latency: ~ 76 ms
Model
minimize the computational overhead during
search.However,therearetwomainchallenges Figure 1: AutoML-Agent receives user’s instruc-
inbuildingsuchaframework. tionsanddeliversoptimizeddeployablemodels.
HighComplexityofthePlanningTasks TheplanningoftheentireAutoMLpipelineintroduces
additionalcomplexitiescomparedtotask-orproblem-specificplanning,primarilyduetotheinter-
dependenciesamongthestepsinthepipeline.Forexample,typesofretrieveddatasetsaffectshowto
designpreprocessingstepsandneuralnetworks.Then,thedesignednetworkaffectswhichparticular
hyper-parameters need to be optimized depending on the given downstream task. Such inter-step
dependenciesresultintheenlargedsearchspacesinceitshouldconsiderallpossiblecombinations
of inter-related steps. Furthermore, enabling the framework to operate across various downstream
tasksexacerbatesthesechallenges,aseachhastask-specificrequirements.
ChallengesinAccurateImplementations Todevelopamodularandextendableframeworkthat
effectivelyhandlesdiverseMLtasks,itiscrucialtoenhancetheflexibilityoftheLLMagentinits
codegenerationability,suchasbydecouplingthetemplatecodefromthecodeforspecificdatasets.
However,usingLLMstoautonomouslygeneratecompleteMLpipelinesmayleadtohallucination
issues,includingcodeincompletion,incorrectormissingdependencies,andpotentialundiscovered
bugs(Hongetal.,2024b). Furthermore,LLMsoftenstrugglewithcodegenerationwhenprompted
withambiguoustaskdescriptions. Thus,weneedaccurateanalysisoftherequirements,andacode-
generationplatformthatcanadaptivelygeneratecodebasedondisambiguatedrequirements.
To address the above challenges, we propose a novel multi-agent framework, AutoML-Agent, for
full-pipelineAutoMLfromdataandmodelsearchtoevaluation,withstrategiestotacklethecom-
plexityoftheplanningproblemaswellasaccurateimplementationofcode. AsillustratedinFig-
ure1, AutoML-Agent acceptsa user’staskdescription andcoordinatesmultiple specializedagents
tocollaborativelyidentifyandexecuteanoptimalMLpipeline,ultimatelydeliveringadeployment-
readymodelanditsinferenceendpointastheoutput.
Specifically, to tackle the complex planning problem, we introduce a new retrieval-augmented
planningstrategyequippedwithrole-specificdecompositionandprompting-basedexecution. This
strategy produces multiple plans based on retrieved knowledge for a given task description, fa-
cilitating the exploration of promising plans. Moreover, it enables LLM agents to discern global
(pipeline-level) and local (process-level) relationships among steps through plan decomposition,
which helps them focus on their immediate sub-tasks while aligning with the user’s goal. The
retrieval-augmented component also simplifies extending LLMs to various downstream tasks us-
ing relevant knowledge. The prompting-based execution enhances search efficiency by exploiting
LLMs’ in-context learning capabilities without any further training, which could introduce addi-
tionalcost. Toenhancetheaccuracyoftheimplementation,weadoptstructure-basedpromptpars-
ingthatextractsML-relevantrequirementsfromtheuser’sdescriptionandmulti-stageverification
thatprovidesfeedbackbetweeneachstepintheframeworktoensurethequalityofinstructionswhen
guidingtheLLMforcodegeneration. Thesemodulesaimtoimprovethecorrectnessandclarityof
thetaskdescriptionforcodeimplementation. Ourmaincontributionsareasfollows.
• Weproposeanovelmulti-agentLLMframeworkforAutoML,designedtoautomatetheentireAI
developmentpipeline. Tothebestofourknowledge,thisisthefirstattempttoemployLLMsina
task-agnosticAutoMLframeworkthatspansfromdataretrievaltomodeldeployment.
2Preprint. Underreview.
Table1: ComparisonbetweenAutoML-AgentandexistingLLM-basedframeworks.
KeyFunctionality
Framework
Planning Verification FullPipeline Task-Agnostic Training-FreeSearch WithRetrieval
AutoML-GPT(Zhangetal.,2023) × × × ✓ ✓ ×
Prompt2Model(Viswanathanetal.,2023) × × ✓ × × ✓
HuggingGPT(Shenetal.,2023) ✓ × × ✓ ✓ ✓
MLCopilot(Zhangetal.,2024a) × × × ✓ ✓ ×
AgentHPO(Liuetal.,2024a) ✓ ✓ × ✓ × ×
AutoMMLab(Yangetal.,2024) × ✓ ✓ × × ×
CAAFE(Hollmannetal.,2023b) × ✓ × × × ×
DataInterpreter(Hongetal.,2024a) ✓ ✓ × ✓ × ×
DS-Agent(Guoetal.,2024a) ✓ ✓ × ✓ × ✓
AutoML-Agent(Ours) ✓ ✓ ✓ ✓ ✓ ✓
• We address the challenges due to the complexity of the planning problem in full-pipeline Au-
toML by introducing retrieval-augmented planning with role-specific plan decomposition and
prompting-basedplanexecution,enhancingtheflexibilityandefficiencyofthesearchprocess.
• Toenhancetheaccuracyofourfull-pipelineimplementation,weintegratestructure-basedprompt
parsing and multi-stage verification to ensure the quality of resulting solutions and instructions
priortoactualcodeimplementation,therebyimprovingoverallperformance.
• WedemonstratethesuperiorityoftheproposedAutoML-Agentframeworkthroughextensiveex-
perimentsonsevendownstreamtasksusingfourteendatasetsacrossfiveapplicationdomains.
2 RELATED WORK
Automatedmachinelearning(AutoML)isatransformativeapproachforoptimizingMLworkflows,
enablingbothpractitionersandresearcherstoefficientlydesignmodelsandpreprocessingpipelines
withminimalmanualintervention(Renetal.,2020;Heetal.,2021;Gijsbersetal.,2024). Despite
several advancements in AutoML(Jin et al., 2019; Feurer et al., 2022; Tang et al., 2024), most of
themaredesignedonlyforparticularelementsoftheMLpipeline.Onlyafewworks(Bisong,2019;
Mukunthu et al., 2019; Microsoft, 2021) support multiple steps of the pipeline. Also, due to the
traditionalprogramminginterfaces,thesesystemsoftenhavecomplexconfigurationproceduresand
steep learning curves that require substantial coding expertise and an understanding of the under-
lying ML concepts, limiting their accessibility to non-experts and being time-consuming even for
experiencedusers.TheselimitationshinderthewidespreadadoptionoftraditionalAutoMLsystems.
Largelanguagemodels(LLM),e.g.,GPT-4(Achiametal.,2023)andLLaMA(Touvronetal.,2023),
haverecentlyshownpromiseinaddressingtheselimitationswiththecomplexproblem-solvingskills
acrossdisciplinesviahuman-friendlylanguageinterfaces, includingAIproblems(Xietal.,2023).
Thisshifttowardsnaturallanguage-driveninterfacesdemocratizesaccessandallowsuserstoartic-
ulate their needs in a more intuitive manner. However, existing LLM-based frameworks can only
assist in a specific step of the ML pipeline, such as feature engineering(Hollmann et al., 2023b),
modelsearch(Shenetal.,2023;Hongetal.,2024a;Guoetal.,2024a), orHPO(Liuetal.,2024a;
Zhangetal.,2024a). Afewattempts(Viswanathanetal.,2023;Yangetal.,2024)supporttheentire
MLproductionpipeline, yetonlyforaspecifictypeofdownstreamtasks. Besides, thesemethods
either naively use the LLMs or overlook the inherent capabilities, making their search processes
time-consumingfortheAutoMLpipelinethatrequiressophisticatedplanningandverification.
Incontrasttotheexistingstudies,ourframeworkaimstoovercometheselimitationsbyincorporat-
inganewretrieval-augmentedplanningstrategy,coupledwithplandecompositionandprompting-
basedexecutiontechniques,alongsidestructure-basedpromptparsingandmulti-stageverification.
Through these enhancements, we can increase plan execution efficiency and support diverse ML
taskswithmoreaccuratepipelineimplementation. Table1summarizesthekeydifferencesbetween
AutoML-Agentandexistingframeworks.
3 A MULTI-AGENT LLM FRAMEWORK FOR FULL-PIPELINE AUTOML
Thissectionpresentsdetailsoftheproposedmulti-agentframework,AutoML-Agent,includingagent
specifications,apromptparsingmodule,aretrieval-augmentedplanningstrategy,aprompting-based
planexecution,andamulti-stageverification. AsdepictedinFigure2,allagentsarecoordinatedby
anAgentManagertocompletetheuser’sinstructionsbydeliveringthedeployment-readymodel.
3Preprint. Underreview.
Input:user’s prompt with task description, requirements,
and/or constraints (c) Multi-Stage Verification
Request
fail Verification fail Implementation pass
Agent Manager Verification Deployable
User pass Model
fail
Prompt Execution
Parsing Verification
External Prompt Agent
Resources pass (a)Retrieval-Augmented Planning
(APIs) Solution Summary
Training-Free Model
Pseudo Data Analysis Search and HPO Plan Implementation
Data & Data Agent Retrieval Model Agent Retrieval / Design Operation Code Generation
Model Hub Preprocessing HPO Agent Runtime Debugging
PD PD Analysis Profiling
Ranking
(b) Prompting-Based Plan Execution (parallelizable) Full-Pipeline Skeleton Code
Figure2:OverviewofourAutoML-Agentframework.(1)Initializationstageaimstoreceiveavalid
user instruction using request verification. (2) Planning stage focuses on extracting ML related
information by parsing the user instruction into a standardized form, and uses it to devise plans
accordingly. (3)Executionstageexecuteseachactiongivenbythedevisedplans. Finally,basedon
thebestexecutionresults,AutoML-Agentoutputscodescontainingdeployablemodeltotheuser.
3.1 AGENTSPECIFICATIONS
Wenowprovidebriefdescriptionsoftheagentsinourmulti-agentAutoMLframework.
AgentManager(A )actsasthecoreinterfacebetweenusersandotherLLMagentsintheframe-
mgr
work. It is responsible for interacting with the user, devising a set of global plans for subsequent
processeswithretrievedknowledge, distributingtaskstocorrespondingagents, verifyingexecuted
resultswithfeedback,andtrackingthesystemprogress.
PromptAgent(A )isanLLMspecificallyinstruction-tunedforparsingtheuser’sinstructionsinto
p
a standardized JSON object with predefined keys. The parsed information is then shared across
agentsintheframeworkduringtheplanning,searching,andverificationphases.
DataAgent(A )isanLLMpromptedfordoingtasksrelatedtodatamanipulationandanalysis.The
d
analysisresultsfromtheDataAgentareusedtoinformtheModelAgentaboutdatacharacteristics
duringthemodelsearchandHPO.
Model Agent(A ) is an LLM prompted for doing tasks related to model search, HPO, model
m
profiling, and candidate ranking. The results produced by the Model Agent are sent back to the
AgentManagerforverificationbeforeproceedingtotheOperationAgent.
OperationAgent(A )isanLLMpromptedforimplementingthesolutionfoundbytheDataand
o
Model Agents that passes the Agent Manager’s verification. The Operation Agent is responsible
forwritingeffectivecodeforactualruntimeexecutionandrecordingtheexecutionresultsforfinal
verificationbeforereturningthemodeltotheuser.
Afterwedefineallagentswiththeircorrespondingprofilesasdescribedabove(see§B.1fordetailed
prompts),theA thenassignsrelevanttaskstoeachagentaccordingtotheuser’sinput. Notethat
mgr
wecanimplementA andA withmorethanoneagentpertaskbasedonthedegreeofparallelism.
d m
3.2 FRAMEWORKOVERVIEW
We present an overview of our AutoML-Agent framework in Figure 2 and Algorithm 1. In the (1)
initialization stage, the Agent Manager (A ) receives the user instruction and checks its valid-
mgr
ity through request verification (Figure 2(c) and Line 3). In the (2) planning stage, the Prompt
Agent(A )parsestheverifieduserinstructionintoastandardizedJSONobject. Then,A gen-
p mgr
erates plans to solve the given AutoML task using retrieval-augmented planning (Figure 2(a) and
Line11). Inthe(3)executionstage,theData(A )andModel(A )Agentsdecomposetheseplans
d m
and execute them via plan decomposition(PD) and prompting-based plan execution (Figure 2(b)
4
noitazilaitinI
)1(
gninnalP
)2(
noitucexE
)3(Preprint. Underreview.
Algorithm1OverallProcedureofAutoML-Agent
Initialization: AgentManagerA ,instruction-tunedPromptAgentA ,DataAgentA ,Model
mgr p d
AgentA ,OperationAgentA ,deployment-readymodelM⋆,andsystemstateS
m o
Input: UserinstructionI
1: whileS ̸=ENDandM⋆ =∅do
2: ifS =INITthen
3: F ←A mgr(ReqVer(I)) ▷runrequestverification(§3.6)forfeedbackF
4: ifF =∅then ▷checkifI isvalid
5: R←A p(I) ▷parseuserinstructionI (§3.3)
6: S ←PLAN
7: else
8: returnF ▷returnfeedbackF totheuser.
9: endif
10: elseifS =PLANthen
11: P←A mgr(RAP(R)) ▷runretrieval-augmentedplanning(§3.4)
12: forp iinPdo
13: sd
i
←PD(R,A d,p i) ▷runplandecompositionforDataAgents(§3.5)
14: O id ←A d(sd i) ▷runpseudodataanalysis(§3.5
15: sm
i
←PD(R,A m,p i,O id) ▷runplandecompositionforModelAgents(§3.5)
16: O im ←A m(sm
i
) ▷runtraining-freemodelsearchandHPO(§3.5)
17: endfor
18: O←{(Od,Om)}P ▷aggregateexecutionoutcomesfromallagents(§3.5)
i i i=1
19: ifA mgr(ExecVer(O))ispassthen ▷runexecutionverification(§3.6)
20: I⋆ ←A mgr(O) ▷findthebestplanandcreatecorrespondinginstructions
21: M⋆ ←A o(I⋆) ▷runcodegenerationforthebestplan
22: ifA mgr(ImpVer(M⋆))ispassthen ▷runimplementationverification(§3.6)
23: S ←END ▷stoptheprocess
24: endif
25: endif
26: endif
27: endwhile
28: returnM⋆
andLine13–16),whoseresultsarethenverifiedagainsttheuser’srequirementsviaexecutionveri-
fication(Figure2(c)andLine19). Finally,A selectsthebestplanandsendsittotheOperation
mgr
Agent(A )towritecode(Line21). Aftercodegeneration,implementationverification(Figure2(c)
o
andLine22)isconductedtoensurethatthecodeisdeployment-ready.Ifanyoftheverificationsteps
fail,AutoML-Agentperformsrevisionsteps(orangelinesinFigure2)togeneratenewsolutions. In
thefollowingsubsections,weprovidethedescriptionsofeachstepmoreindetail.
3.3 INSTRUCTIONDATAGENERATIONANDPROMPTPARSING
DataGeneration ForA togenerateaccurateJSONobjects,weneedtoinstruction-tunetheLLM
p
firstbecauseitcanoutputavalidJSONobjectbutwithincorrectkeysthatareirrelevanttosubse-
quent processes. Following Xu et al. (2024), we first manually create a set of high-quality seed
instructions then automatically generate a larger instruction dataset D = {(I ,R )}N , having N
i i i=1
instruction-responsepairs. Here,I isthei-thinstructionwiththecorrespondingresponseR . We
i i
use the JSON format substantially extended from Yang et al. (2024) for response R with the fol-
i
lowingtop-levelkeystoextracttheuser’srequirementfromvariousaspectsofanAutoMLpipeline.
• User. The user key represents the user intention (e.g., build, consult, or unclear) of the given
instructionandtheirtechnicalexpertiseinAI.
• Problem. The problem key indicates the characteristics and requirements of the given task, in-
cludingarea(e.g., computervision), downstreamtask(e.g., imageclassification), applicationor
businessdomain,andotherconstraintslikeexpectedaccuracyandinferencelatency.
• Dataset. Thedatasetkeycapturesthedatacharacteristicsandproperties,includingdatamodality,
requestedpreprocessingandaugmentationtechniques,andpotentialdatasource.
5Preprint. Underreview.
• Model. The model key captures the expected model characteristics and properties, including
modelname(e.g.,ViT),family(e.g.,Transformer),andtype(e.g.,neuralnetworks).
• Knowledge. Theknowledgekeyextractsadditionalknowledgeorinsightshelpfulforsolvingthe
givenproblemdirectlyprovidedbytheuser,potentiallyassociatedwiththeexpertiselevel.
• Service. Theservicekeyisrelevanttothedownstreamimplementationanddeployment. Itpro-
videsinformationsuchasatargetdeviceandaninferenceengine.
Prompt Parsing Then, we can use the generated dataset D to train an LLM and use it as A .
p
NotethatthesestandardizedkeysareimportantforabettercontrolovertheLLMagents’behavior
withinourframeworkandnecessaryforeffectivecommunicationbetweenagents. Moreover,these
keys provide contextual information for generating a high-quality AutoML pipeline from various
perspectives. After the instruction tuning, we use the A to parse the user’s instructions (or task
p
descriptions)andreturntheparsedrequirementsR=A (I)toA ,asshownin§C.1.
p mgr
3.4 RETRIEVAL-AUGMENTEDPLANNING
Recentstudies(Guoetal.,2024b;Huangetal.,2024;Mastermanetal.,2024;Zhangetal.,2024b;
Huetal.,2024)highlightsthateffectiveplanningandtoolutilizationareessentialforsolvingcom-
plex problems with LLMs, especially in a multi-agent framework. By bridging two techniques in
asinglemodule,weproposearetrieval-augmentedplanning(RAP)strategytoeffectivelydevisea
robustandup-to-datesetofdiverseplansfortheAuotMLproblems.
LetP = {p ,...,p }beasetofplans. BasedonpastknowledgeembeddedintheLLM,knowl-
1 P
edge retrieved via external APIs (such as arXiv papers), and R, RAP generates P multiple end-
to-end plans for the entire AutoML pipeline having different scenario p . This strategy enables
i
AutoML-Agent to be aware of newer and better solutions. Specifically, we first use the parsed re-
quirementsRtoacquireasummaryoftherelevantknowledgeandinsightsviaAPIcalls,including
websearchandpapersummary. A thenusesthisinformationtodeviseP differentplans, i.e.,
mgr
P = A (RAP(R)). Note that A devises each plan independently to make the subsequent
mgr mgr
steps parallelizable. The benefit of this strategy is that it enhances exploration for better solutions
whileallowingparallelization. Examplesofgeneratedplansareprovidedin§C.2.
3.5 PROMPTING-BASEDPLANEXECUTIONANDIMPLEMENTATION
Given the generated P, we now describe how A and A execute each p using prompting tech-
d m i
niqueswithoutactuallyexecutingthecode. Examplesoftheexecutionresultsarein§C.4.
Plan Decomposition Due to the high complexity of the end-to-end plan, we first need to adap-
tivelydecomposetheoriginalplanp ∈ Pintoasmallersetofsub-taskss relevanttotheagent’s
i i
roles and expertise to increase the effectiveness of LLMs in solving and executing the given plan
(Khotetal.,2023). Theplandecomposition(PD)processinvolvesqueryingtheagentsabouttheir
understandingofthegivenplanspecifictotheirroles. Formally,sd = PD(R,A ,p ),wheresd is
i d i i
the decomposed plan for Data Agent, containing sub-tasks for the given plan p . Then, the agent
i
executesthedecomposedplantowardstheuser’srequirementsinsteadoftheoriginallengthyplan.
Wedefinethesub-taskssm ofA belowduetoitsrelianceonDataAgent’soutcomes. Examples
i m
ofdecomposedplansarein§C.3.
PseudoDataAnalysis InAutoML-Agent,A handlessub-tasksinsd,includingtheretrieval,pre-
d i
processing, augmentation, andanalysisofthespecifieddataset. Duringthedataretrievalphase, if
thedatasetisnotdirectlysuppliedbytheuser,weinitiateanAPIcalltosearchforpotentialdatasets
inrepositories, suchasHuggingFaceandKaggle, usingthedatasetnameordescription. Uponlo-
cating a dataset, we augment the prompt with metadata from the dataset’s source; if no dataset is
found, we rely on the inherent knowledge of the LLM. We then prompt A to proceed by acting
d
asif itactuallyexecutessd,accordingtothedatasetcharacteristicsanduserrequirementsR. The
i
summarizedoutcomesofthesesub-tasks,Od,arethenforwardedtotheA . Hence,Od =A (sd).
i m i d i
Training-Free Model Search and HPO Like A , A uses API calls to complete all sub-tasks
d m
sm, instead of direct code execution. However, in contrast to A , the plan decomposition for A
i d m
6Preprint. Underreview.
incorporates outcomes from the A , enabling it to recognize characteristics of the preprocessed
d
dataset,i.e.,sm = PD(R,A ,p ,Od). Here,theA ’spromptisenhancedwithinsightsgathered
i m i i m
by A about high-performing models and relevant hyperparameters for the specific ML task.
mgr
ThistechniqueallowstheA toexecutethesub-tasksinsmmoreefficiently. Usingthisaugmented
m i
prompt,theA followsasimilarproceduretoA ,undertakingmodelretrieval,runningHPO,and
m d
summarizingtheresultsofthesesub-tasks,whichincludeexpectednumericalperformancemetrics
suchasaccuracyanderror,aswellasmodelcomplexityfactorslikemodelsizeandinferencetime.
To facilitate the subsequent verification step, we also prompt the agent to return results with the
top-kmostpromisingmodels. Formally,Om =A (sm).
i m i
Plan Implementation To enhance the efficacy of A in code generation, A first verifies all
o mgr
executed results O = {(Od,Om)}P from A and A . A then selects the best outcome
i i i=1 d m mgr
O⋆ ∈ O and generates the instruction I⋆ for A to write the actual code accordingly. Formally,
o
M⋆ =A (I⋆),whereM⋆isthedeployment-readymodel.
o
3.6 MULTI-STAGEVERIFICATION
Verification,especiallywithrefinementorfeedback,isessentialformaintainingthecorrecttrajec-
toryofLLMs(Baeketal.,2024;Madaanetal.,2023;Gouetal.,2024).Ourframeworkincorporates
three verification steps to guarantee its accuracy and effectiveness: request verification, execution
verification,andimplementationverification.
RequestVerification Initially,weassesstheclarityoftheuser’sinstructionstodetermineifthey
are relevant and adequate for executing ML tasks and addressing the user’s objectives. If the in-
structionsproveinsufficientforprogressingtotheplanningstage,A willrequestadditionalin-
mgr
formation,facilitatingmulti-turncommunication. Thisrequestverification(ReqVerinAlgorithm1
Line3)step,however,oftenoverlookedinexistingstudies,placinganundueburdenonuserstofor-
mulateamoredetailedinitialprompt—achallengingtaskparticularlyforthosewhoarenon-experts
orlackexperience. PromptsforReqVerareshownin§B.4.1.
ExecutionVerification Afterexecutingtheplansin§3.5,A thenverifieswhetheranyofthe
mgr
pipelines produced by A and A (i.e., O) satisfy the user’s requirements via prompting (see
d m
§B.4.2). If the results are satisfied, the suggested solution is selected as a candidate for imple-
mentation. Thisexecutionverification(ExecVer)stepeffectivelymitigatescomputationaloverhead
inthesearchprocessbyallocatingresourcesexclusivelytothemostpromisingsolution.
ImplementationVerification Thisimplementationverification(ImpVer)phasecloselyresembles
the execution verification; however, it differs in that it involves validating outcomes derived from
the code that has been executed and compiled by A . We present the prompt for this verification
o
in§B.4.3. Iftheoutcomesmeettheuser’srequirements,A providesthemodelanddeployment
mgr
endpointtotheuser.
Notethatifanyexecutionorimplementationfailstosatisfytheuserrequirements(i.e.,doesnotpass
the verification process), these failures are systematically documented. Subsequently, the system
transitions to the plan revision stage. During this stage, A formulates a revised set of plans,
mgr
incorporatinginsightsderivedfromtheoutcomesoftheunsuccessfulplans.
4 EXPERIMENTS
Wevalidatetheeffectivenessofourfull-pipelineAutoMLframeworkbycomparingAutoML-Agent
withhandcraftedmodels,state-of-the-artAutoMLvariants,andLLM-basedframeworksacrossmul-
tipledownstreamtasksinvolvingdifferentdatamodalities.
4.1 SETUP
DownstreamTasksandDatasets AssummarizedinTable2,weselectsevendownstreamtasks
from five different data modalities, including image, text, tabular, graph, and time series. These
7Preprint. Underreview.
datasets are chosen from different sources. Also, we incorporate various evaluation metrics for
thesetasks,e.g.,accuracyforclassificationandRMSLEforregression.
Foreachtask,wepreparetwosetsofnat- Table2: Summaryofdownstreamtasksanddatasets.
ural language task descriptions to repre- DataModality DownstreamTask DatasetName EvaluationMetric
sent constraint-aware and constraint-free Image ButterflyImage
ImageClassification Accuracy
(ComputerVision) Shopee-IET
requirements (see §A) along with a full-
Text EcommerceText
pipeline skeleton script. As a result, we (NLP) TextClassification TextualEntailment Accuracy
extensivelyevaluate28generatedmodels. TabularClassification BananaQuality F1
SoftwareDefects
Notethatthissettingdiffersfromprevious Tabular CrabAge
TabularRegression RMSLE
(ClassicML) CropPrice
studies(Guo et al., 2024a; Huang et al., SmokerStatus
TabularClustering RI
StudentPerformance
2023),whichrequiredataset-specific,par-
TimeSeries Weather
tiallycompletedcodepreparation. (TimeSeriesAnalysis) Time-SeriesForecasting Electricity RMSLE
Graph Cora
NodeClassification Accuracy
(GraphMining) Citeseer
Evaluation Metrics For a comprehen-
siveevaluation,wemeasuretheagent’seffectivenessinbothcodegenerationandtask-specificper-
formanceaspectsbyusingcomprehensivescore(CS)(Hongetal.,2024a)tosimultaneouslyevaluate
both the success rate(SR) of code generation and the normalized performance score(NPS) of the
built pipelines. That is, CS = 0.5×SR+0.5×NPS. Here, NPS = 1 is a transformation of
1+s
loss-basedperformancescores,e.g.,RMSLE.Moredetailedexplanationsareincludedin§A.4.
Asdescribedabove,weevaluateallframeworksundertwodifferentsettings.TomeasureSRofeach
method, we use a grading scale ranging from 0 for total failure to 1 for perfect conformity to the
user’srequirements. Fortheconstraint-freesetting,amethodcangetascoreof0.5(passmodeling)
or1.0(passdeployment). Fortheconstraint-awaresetting,amethodcangetascoreof0.25(pass
modeling),0.5(passdeployment),0.75(partiallypasstheconstraints),or1.0(passallcases).
Baselines As we propose a framework for the novel task of full-pipeline AutoML with LLMs,
there is no direct baseline available for comparison. We thus compare AutoML-Agent against the
task-specificmanuallydesignedmodels(see§A.3): HumanModels,thevariantsofstate-of-the-art
AutoML:AutoGluon(Ericksonetal.,2020;Shchuretal.,2023;Tangetal.,2024),astate-the-of-art
LLMfordatascience: DS-Agent(Guoetal.,2024a),andgeneral-purposeLLMs: GPT-3.5(Brown
etal.,2020)andGPT-4(Achiametal.,2023)withzero-shotprompting.
Implementation Details Except for the A that is implemented with Mixtral-8x7B(Mixtral-
p
8x7B-Instruct-v0.1)(Jiangetal.,2024),weuseGPT-4(gpt-4o-2024-05-13)asthebackbonemodel
forallagentsandLLM-basedbaselinestoensureanimpartialperformanceevaluation. Toinstruc-
tion tune the A (§3.3), we automatically generate about 2.3K instruction-response pairs using
p
EvolInstruct(Xu et al., 2024). Here, we use LoRA(Hu et al., 2021) to fine-tune the model with
thegenerateddataset. ForRAP(§3.4),wesetthenumberofplansP =3andthenumberofcandi-
datemodelsk = 3. AllexperimentsareconductedonanUbuntu22.04LTSserverequippedwith
eightNVIDIAA100GPUs(CUDA12.4)andIntel(R)Xeon(R)Platinum8275CLCPU@3.00GHz.
Forrunningthegeneratedmodels, weemploythesameexecutionenvironmentasDS-Agent(Guo
etal.,2024a),withallnecessarylibrariesincludedintheskeletonscripts.
4.2 MAINRESULTS
WereporttheaveragescoresfromfiveindependentrunsforallevaluationmetricsinFigure3.
SuccessRate Figure3(left)andTable5presenttheresultsfortheSRmetric. Fortheconstraint-
free cases, which can be considered easier tasks, all methods have higher SR than ones in the
constraint-awaresetting. Notably,AutoML-Agentalsoconsistentlyoutperformsthebaselinesinthe
constraint-awaresetting,achievinganaverageSRof87.1%,whichunderscorestheeffectivenessof
the proposed framework. We conjecture that the knowledge retrieved during the planning process
helpstheagentsidentifywhichareastofocusoninordertomeetthegivenconstraints. Regarding
DS-Agent,althoughweusetheprovidedexamplecasesfortherelevanttasks,DS-Agentappearsto
fail on certain tasks due to its heavy reliance on curated case banks and the inclusion of partially
completedcode,whichisunavailableinoursetting.
8Preprint. Underreview.
Human Models AutoGluon GPT-3.5 GPT-4 DS-Agent AutoML-Agent (Ours)
SR NPS CS
Butterfly Butterfly Butterfly
Avg. 1 Shopee Avg. 1 Shopee Avg. 1 Shopee
0.8 0.8 0.8
Citeseer Ecomm Citeseer Ecomm Citeseer Ecomm
0.6 0.6 0.6
Cora 0.4 Entail Cora 0.4 Entail Cora 0.4 Entail
0.2 0.2 0.2
0 0 0
Electricity Banana Electricity Banana Electricity Banana
Weather Software Weather Software Weather Software
Student Crab Student Crab Student Crab
Smoker Crop Smoker Crop Smoker Crop
Butterfly Butterfly Butterfly
Avg. 1 Shopee Avg. 1 Shopee Avg. 1 Shopee
0.8 0.8 0.8
Citeseer Ecomm Citeseer Ecomm Citeseer Ecomm
0.6 0.6 0.6
Cora 0.4 Entail Cora 0.4 Entail Cora 0.4 Entail
0.2 0.2 0.2
0 0 0
Electricity Banana Electricity Banana Electricity Banana
Weather Software Weather Software Weather Software
Student Crab Student Crab Student Crab
Smoker Crop Smoker Crop Smoker Crop
Figure3: PerformancecomparisonacrossalldatasetsusingtheSR,NPS,andCSmetricsunder(a)
constraint-freeand(b)constraint-awaresettings. Higherscoresindicatebetterresults.
Downstream Performance We present the performance comparison between the successfully
built models in Figure 3(center) and Table 6. To ensure meaningful results and to examine how
the performance of LLM-generated models compares to state-of-the-art AutoML techniques and
manualMLpipelinescraftedbyexperiencedexperts,weselecttop-performingmodelsbyevaluat-
ingresultsreportedinPaperswithCodebenchmarksandKagglenotebooksforthesametasksand
datasets,whereapplicable,astheHumanModelsbaselines. Fromtheresults,wecanobservethat
AutoML-Agentsignificantlyoutperformsotheragents,includingHumanModels,intheNPSmetric.
In particular, AutoML-Agent achieves the best performance across all tasks under the constraint-
awaresetting. ThesefindingshighlightthesuperiorityofAutoML-Agentinadaptingtovarioussce-
narios,attributedtotheretrieval-augmentedplanning(RAP)strategy. Thisapproachenablesagents
to discover effective pipelines for given constraints. These empirical observations substantiate the
efficacyoftheproposedRAP,providingup-to-datesolutionsforvarioustasks.
Comprehensive Score Figure 3(right) and Table 7 present the weighted quality of each agent
based on the CS metric. Overall, AutoML-Agent outperforms all other baselines, especially in
morecomplicatedtasks. Interestingly,itisevidentthatgeneral-purposeLLMsstillworksrelatively
wellonclassicaltasksliketabularclassificationandregression,whilemoresophisticatedmethods,
suchasDS-AgentandourAutoML-Agent worksignificantlybetterincomplextasks. Thisfinding
alignswithpreviousresearch(Guoetal.,2024a),whichsuggeststhattabulartaskstypicallyinvolve
straightforwardfunctioncallsfromthesklearnlibrary(Pedregosaetal.,2011),andthereforedonot
demandadvancedreasoningorcodingabilitiesfromLLMagents,unlikemorecomplextasks.
4.3 RESOURCECOST
As we primarily use closed-source LLMs
Time (s) 187.71 136.34 17.88 182.6
inthispaper,weanalyzetheresourcecosts 10.781.91 1.04
in terms of time and money. Figure 4 Money ($) 0.02 0.14 0.06 0.04
0 0 0
presents the average time and monetary Time (s) 182.38 98.62 20.25 191.9
costs across different tasks and datasets 14.21 3.63 1.37
Money ($)0.01 0.11 0.15 0.06
for a single run, under the constraint-free 0 0 0
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
(upper) and constraint-aware (lower) set- Prompt Parsing Request Verification Retrieval & Planning Plan Execution Execution Verification Selection and Summarization Code Generation
tings. Onaverage,ittakesaround525sec- Figure4: Averagetimeandmonetarycostbreakdown.
onds and costs 0.30 USD (using GPT-4o)
to search for a single model that will be deployable after training. The significant amount of time
spentintheplanningstagealsosuggeststhedifficultyindevisingplansforfull-pipelineAutoML.
9
eerF-tniartsnoC
)a(
erawA-tniartsnoC
)b(
eerF-tniartsnoCerawA-tniartsnoCPreprint. Underreview.
w/ RAP w/ RAP + Decomposition AutoML-Agent
1 1 Image
0.8 Classification
0.6 0.972 0.966 T Ce lax st sification Tabular 0.4 0.95 0.955 Classification
Time-Series
0.2 Forecasting
Node
0 Classification
Image Text Tabular Time-Series Node Average 0.9 Avgerage
ClassificationClassificationClassificationForecastingClassification 1 Number3 of Plans5
(a)Resultsofablationstudy. (b)ResultsofvaryingP.
Figure5: Resultsof(a)ablationand(b)hyperparamterstudiesintheCSmetric.
4.4 ABLATIONANDHYPERPARAMETERSTUDIES
To validate the effectiveness of each component in AutoML-Agent, we conduct the following ab-
lation studies. The results are presented in Figure 5a and Table 8. First, we investigate retrieval-
augmented planning (RAP) alone, where retrieved knowledge from external APIs is directly used
without plan decomposition and multi-stage verification. As expected, this ablation leads to a de-
cline in performance, and in some cases, even fails to generate a runnable model. This outcome
highlightstheimportanceofthedecompositionandverificationmodules. Second,weevaluateRAP
withplandecomposition,wheretheplanisdecomposedforeachspecificagent. Whilethisvariant
demonstrates better downstream performance, it still fails to produce runnable models in certain
downstream tasks due to the lack of code verification. Finally, we assess the full framework with
multi-stageverification,whichprovidesfeedbacktoagents,therebyenhancingboththeirplanning
and coding capabilities. Integrating all components significantly empowers LLM agents to effec-
tivelyincorporateexternalknowledgefromvarioussourcestobuildafull-pipelineAutoMLsystem.
To further verify the effectiveness of devising multiple plans in our retrieval-augmented planning
strategy (§3.4), we conduct a hyperparameter study by varying the number of plans P in the
constraint-free setting. As shown in Figure 5b and Table 9, the number of plans does not signif-
icantly affect the success rate, likely due to GPT-4’s robust planning capabilities. However, based
ontheNPSandCSmetrics,weobservethatthenumberofplanshasanotableimpactondownstream
taskperformance. Also,theseresultsalsosuggestthataddingmoreplansdoesnotnecessarilylead
to better results, as the model may generate multiple similar plans, resulting in similar outcomes.
Consequently,weselect3asthedefaultnumberofplans.
5 CONCLUSION
This paper presents AutoML-Agent, a novel LLM-based multi-agent framework designed for Au-
toML, covering the entire pipeline from data retrieval to model deployment. AutoML-Agent tack-
lesthefull-pipelineplanningcomplexityandimplementationaccuracychallengesintheLLMsfor
task-agnosticAutoMLbyleveragingthenewlyproposedretrieval-augmentedplanningstrategyand
multi-stageverification. Inaddition, weenhancetheplanexecutionefficiencybyintegratingrole-
specificdecompositionandprompting-basedexecutiontechniquesintotheframework. Ourexperi-
mentsonsevenMLtasksdemonstratethatAutoML-Agentoutperformsexistingmethodsintermsof
successrateanddownstreamtaskperformance.
LimitationsandFutureWork Eventhoughweofferaflexiblemoduletoaccommodatevarious
MLtasksanddatamodalities,theabsenceofaskeletoncodeforcompletelynewtaskscouldincrease
the risk of code hallucination problems. Additionally, in the current version, there is still a gap
in code generation quality when using different backbones, e.g., GPT-4 vs. GPT-3.5, which is
not unique to our approach but a common challenge faced by existing LLM-based frameworks.
Developing a more robust framework that can effectively provide reasonable solutions with less
relianceontheLLMbackboneisverypromisingfuturework.
Inaddition,ourworkstillfacescodegenerationfailureswhenappliedtomachinelearningtasksthat
require significantly different development pipelines from those tested in our experiments, which
focusedongeneralsupervisedandunsupervisedsettings. Taskssuchasreinforcementlearningand
recommendation systems pose particular challenges. Consequently, extending AutoML-Agent to
these tasks will require the development of additional agents to handle specific steps in the target
pipeline,suchasactor-environmentinteractionandrewardmodelinginreinforcementlearning.
10
SC SCPreprint. Underreview.
REPRODUCIBILITY STATEMENT
Wepresentthecompletepromptsandshowcaseresultsin§Band§Dtofacilitatereproduction.More
experimentalandimplementationdetailsareprovidedin§A,alongwithdetailedresultsin§E.
ETHICS STATEMENT
We expect AutoML-Agent to offer significant advantages by promoting AI-driven innovation and
enabling individuals with limited AI expertise to effectively utilize AI capabilities. However, we
acknowledge the potential misuse of AutoML-Agent by malicious users, such as generating offen-
sive content, malicious software, or invasive surveillance tools when exposed to harmful inputs.
This vulnerability is not unique to our approach but represents a common challenge faced by ex-
isting LLMs with substantial creative and reasoning capabilities, which can occasionally produce
undesirableoutputs.
AlthoughwestrictlyinstructtheLLMtofocusongeneratingpositiveresultsformachinelearning
tasks, there is a possibility of unforeseen glitches that could introduce security issues within the
system. Therefore, we recommend running AutoML-Agent within a Docker container to ensure
isolation fromthe host’s file system. Additionally, dueto its integrationwith external services for
retrieval-augmentedgenerationandAPI-basedLLMslikeGPT-4,privacyconcernsmayarise.Users
shouldcarefullyreviewanydataincludedinAPIpromptstopreventunintendeddatadisclosures.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
JinheonBaek,SujayKumarJauhar,SilviuCucerzan,andSungJuHwang. Researchagent: Iterative
research idea generation over scientific literature with large language models. arXiv preprint
arXiv:2404.07738,2024.
Ekaba Bisong. Building machine learning and deep learning models on Google cloud platform.
Springer,2019.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. InAdvancesinneuralinformationprocessingsystems,pp.1877–1901,2020.
NickErickson,JonasMueller,AlexanderShirkov,HangZhang,PedroLarroy,MuLi,andAlexan-
der Smola. Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint
arXiv:2003.06505,2020.
MatthiasFeurer,KatharinaEggensperger,StefanFalkner,MariusLindauer,andFrankHutter.Auto-
sklearn 2.0: Hands-free automl via meta-learning. Journal of Machine Learning Research, 23
(261):1–61,2022.
MatthiasFeyandJanE.Lenssen. FastgraphrepresentationlearningwithPyTorchGeometric. In
ICLRWorkshoponRepresentationLearningonGraphsandManifolds,2019.
Pieter Gijsbers, Marcos LP Bueno, Stefan Coors, Erin LeDell, Se´bastien Poirier, Janek Thomas,
BerndBischl,andJoaquinVanschoren. Amlb:anautomlbenchmark. JournalofMachineLearn-
ingResearch,25(101):1–65,2024.
ZhibinGou, ZhihongShao, YeyunGong, yelongshen, YujiuYang, NanDuan, andWeizhuChen.
CRITIC:Largelanguagemodelscanself-correctwithtool-interactivecritiquing. InTheTwelfth
InternationalConferenceonLearningRepresentations,2024.
Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. Ds-agent: Auto-
mateddatasciencebyempoweringlargelanguagemodelswithcase-basedreasoning. InInterna-
tionalConferenceonMachineLearning,2024a.
11Preprint. Underreview.
TaichengGuo,XiuyingChen,YaqiWang,RuidiChang,ShichaoPei,NiteshVChawla,OlafWiest,
and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and
challenges. arXivpreprintarXiv:2402.01680,2024b.
XinHe, KaiyongZhao, andXiaowenChu. Automl: Asurveyofthestate-of-the-art. Knowledge-
BasedSystems,212:106622,2021.
NoahHollmann,SamuelMu¨ller,KatharinaEggensperger,andFrankHutter.TabPFN:Atransformer
thatsolvessmalltabularclassificationproblemsinasecond. InICLR,2023a.
Noah Hollmann, Samuel Mu¨ller, and Frank Hutter. Large language models for automated data
science: Introducing caafe for context-aware automated feature engineering. In Advances in
NeuralInformationProcessingSystems,volume36,pp.44753–44775,2023b.
SiruiHong, YizhangLin, BangbangLiu, BinhaoWu, DanyangLi, JiaqiChen, JiayiZhang, Jinlin
Wang, LingyaoZhang, MingchenZhuge, etal. Datainterpreter: Anllmagentfordatascience.
arXivpreprintarXiv:2402.18679,2024a.
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao
Zhang,ZiliWang,StevenKaShingYau,ZijuanLin,LiyangZhou,ChenyuRan,LingfengXiao,
ChenglinWu,andJu¨rgenSchmidhuber. MetaGPT:Metaprogrammingformulti-agentcollabo-
rativeframework. InTheTwelfthInternationalConferenceonLearningRepresentations,2024b.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685,2021.
MengkangHu,YaoMu,XinmiaoChelseyYu,MingyuDing,ShiguangWu,WenqiShao,Qiguang
Chen,BinWang,YuQiao,andPingLuo. Tree-Planner: Efficientclose-looptaskplanningwith
largelanguagemodels. InICLR,2024.
QianHuang,JianVora,PercyLiang,andJureLeskovec. Benchmarkinglargelanguagemodelsas
airesearchagents. arXivpreprintarXiv:2310.03302,2023.
Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang,
Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. arXiv
preprintarXiv:2402.02716,2024.
AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBam-
ford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.
Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
HaifengJin,QingquanSong,andXiaHu. Auto-keras: Anefficientneuralarchitecturesearchsys-
tem. InProceedingsofthe25thACMSIGKDDInternationalConferenceonKnowledgeDiscov-
ery&DataMining,pp.1946–1956,2019.
TusharKhot,HarshTrivedi,MatthewFinlayson,YaoFu,KyleRichardson,PeterClark,andAshish
Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In The
EleventhInternationalConferenceonLearningRepresentations,2023.
Dawei Li, Zhen Tan, and Huan Liu. Exploring large language models for feature selection: A
data-centricperspective. arXivpreprintarXiv:2408.12025,2024.
Haoyuan Li, Hao Jiang, Tianke Zhang, Zhelun Yu, Aoxiong Yin, Hao Cheng, Siming Fu, Yuhao
Zhang, andWangguiHe. Traineragent: Customizableandefficientmodeltrainingthroughllm-
poweredmulti-agentsystem. arXivpreprintarXiv:2311.06622,2023.
SiyiLiu, ChenGao, andYongLi. Largelanguagemodelagentforhyper-parameteroptimization.
arXivpreprintarXiv:2402.01881,2024a.
Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.
itransformer: Invertedtransformersareeffectivefortimeseriesforecasting. InICLR,2024b.
12Preprint. Underreview.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad
Majumder,KatherineHermann,SeanWelleck,AmirYazdanbakhsh,andPeterClark. Self-refine:
Iterativerefinementwithself-feedback.InThirty-seventhConferenceonNeuralInformationPro-
cessingSystems,2023.
Simon Malberg, Edoardo Mosca, and Georg Groh. FELIX: Automatic and interpretable feature
engineering using llms. In Joint European Conference on Machine Learning and Knowledge
DiscoveryinDatabases,pp.230–246.Springer,2024.
Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging
ai agent architectures for reasoning, planning, and tool calling: A survey. arXiv preprint
arXiv:2404.11584,2024.
Microsoft. NeuralNetworkIntelligence,12021. URLhttps://github.com/microsoft/
nni.
D. Mukunthu, P. Shah, and W.H. Tok. Practical Automated Machine Learning on Azure: Us-
ing Azure Machine Learning to Quickly Build AI Solutions. O’Reilly Media, 2019. ISBN
9781492055549.
FabianPedregosa,Gae¨lVaroquaux,AlexandreGramfort,VincentMichel,BertrandThirion,Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas,
Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and E´douard Duch-
esnay. Scikit-learn: Machinelearninginpython. JournalofMachineLearningResearch,12(85):
2825–2830,2011.
Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin
Wang. A comprehensive survey of neural architecture search: Challenges and solutions. arXiv
preprintarXiv:2006.02903,2020.
OleksandrShchur,AliCanerTurkmen,NickErickson,HuibinShen,AlexanderShirkov,TonyHu,
and Bernie Wang. Autogluon–timeseries: Automl for probabilistic time series forecasting. In
InternationalConferenceonAutomatedMachineLearning.PMLR,2023.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-
ginggpt: Solving ai tasks with chatgpt and its friends in hugging face. In Advances in Neural
InformationProcessingSystems,volume36,pp.38154–38180,2023.
Zhiqiang Tang, Haoyang Fang, Su Zhou, Taojiannan Yang, Zihan Zhong, Tony Hu, Katrin Kirch-
hoff,andGeorgeKarypis. Autogluon-multimodal(automm): Superchargingmultimodalautoml
withfoundationmodels. arXivpreprintarXiv:2404.16233,2024.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
PataraTrirat,YoojuShin,SejinKim,andMinseokKim. Generatingamachinelearningmodelwith
afewsentences. KoreaSoftwareCongress,pp.688–690,2021.
Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, and Graham Neubig.
Prompt2model: Generating deployable models from natural language instructions. In Proceed-
ings of the 2023 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations,pp.413–421,2023.
ZhihengXi,WenxiangChen,XinGuo,WeiHe,YiwenDing,BoyangHong,MingZhang,Junzhe
Wang,SenjieJin,EnyuZhou,etal. Theriseandpotentialoflargelanguagemodelbasedagents:
Asurvey. arXivpreprintarXiv:2309.07864,2023.
CanXu,QingfengSun,KaiZheng,XiuboGeng,PuZhao,JiazhanFeng,ChongyangTao,Qingwei
Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow
complex instructions. In The Twelfth International Conference on Learning Representations,
2024.
13Preprint. Underreview.
Chenxiao Yang, Qitian Wu, Jiahua Wang, and Junchi Yan. Graph neural networks are inherently
goodgeneralizers: InsightsbybridgingGNNsandMLPs. InICLR,2023.
Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, and Wentao Liu. Autommlab: Au-
tomaticallygeneratingdeployablemodelsfromlanguageinstructionsforcomputervisiontasks.
arXivpreprintarXiv:2402.15351,2024.
Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Yu-Feng Li, Wei-Wei Tu, Qiang
Yang,andYangYu. Takinghumanoutoflearningapplications: Asurveyonautomatedmachine
learning. arXivpreprintarXiv:1810.13306,2018.
Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, and Yuqing Yang. Mlcopilot: Unleashing the
poweroflargelanguagemodelsinsolvingmachinelearningtasks. InThe18thConferenceofthe
EuropeanChapteroftheAssociationforComputationalLinguistics,2024a.
Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mingyuan Zhou. Automl-gpt:
Automaticmachinelearningwithgpt. arXivpreprintarXiv:2305.02499,2023.
WenqiZhang,YongliangShen,LinjuanWu,QiuyingPeng,JunWang,YuetingZhuang,andWeim-
ing Lu. Self-Contrast: Better reflection through inconsistent solving perspectives. In ACL, pp.
3602–3622,2024b.
14Preprint. Underreview.
A DETAILS OF EXPERIMENTAL SETUP
This section outlines the detailed experimental setup used in this paper, including the complete
instructionpromptsforbothconstraint-free(Table3)andconstraint-aware(Table4)settings,afull-
pipelineskeletonscript(§A.1),datasetandbaselinedescriptions,aswellasevaluationmetrics.
Table3: Userinstruction(i.e.,taskdescription)forexperimentsundertheconstraint-freesetting.
Task Dataset InstructionPrompt
Image ButterflyImage IneedaveryaccuratemodeltoclassifyimagesintheButterflyImageClassification
Classification datasetintotheirrespectivecategories. Thedatasethasbeenuploadedwithitslabel
informationinthelabels.csvfile.
Shopee-IET Pleaseprovideaclassificationmodelthatcategorizesimagesintooneoffourclothing
categories. Theimagepath,alongwithitslabelinformation,canbefoundinthefiles
trainlabels.csvandtestlabels.csv.
Text EcommerceText Weneedastate-of-the-artmodelfortextclassificationbasedontheEcommerceText
Classification dataset.Themodelshouldbecapableofaccuratelyclassifyingtextintofourcategories:
Electronics,Household,Books,andClothing&Accessories. Wehaveuploadedthe
entiredatasetwithoutsplittingithere.
TextualEntailment Youaresolvingthismachinelearningtasksofclassification: Thedatasetpresented
here(theTextualEntailment)comprisesaseriesoflabeledtextpairs. Giventwotexts
(text1andtext2),yourtaskistopredicttherelationshipofthetextpairofneutral(0),
contradiction(1)orentailment(2).Theevaluationmetricisaccuracy.Buildalanguage
modeltogetagoodperformance.
Tabular BananaQuality BuildamodeltoclassifybananaqualityasGoodorBadbasedontheirnumericalin-
Classification formationaboutbananasofdifferentquality(size,weight,sweetness,softness,harvest
time,ripeness,andacidity). Wehaveuploadedtheentiredatasetforyouhereinthe
bananaquality.csvfile.
SoftwareDefects Youaresolvingthisdatasciencetasksofbinaryclassification: Thedatasetpresented
here(theSoftwareDefectsDataset)comprisesalotofnumericalfeatures.Pleasesplit
thedatasetintothreepartsoftrain,validandtest. Yourtaskistopredictthedefects
item,whichisabinarylabelwith0and1.TheevaluationmetricistheF1score.Please
trainabinaryclassificationmodeltogetagoodperformanceonthistask.
Tabular CrabAge Youaresolvingthisdatasciencetasksofregression: Thedatasetpresentedhere(the
Regression CrabAgeDataset)comprisesalotofbothcategoricalandnumericalfeatures. Pleae
splitthedatasetintothreepartsoftrain,validandtest. Yourtaskistopredicttheage
item.TheevaluationmetricistheRMSLE(rootmeansquaredlogerror).Nowtraina
regressionmodeltogetagoodperformanceonthistask.
CropPrice Ineedaregressionmodeltopredictcroppricesbasedonfeatureslikesoilcomposition,
environmentalfactors,historicalyielddata,andcropmanagementpracticesfromthe
datasetIuploadedhere.
Tabular SmokerStatus Youaresolvingthisdatasciencetasksofunsupervisedclustering:Thedatasetpresented
Clustering here(theSmokerStatusDataset)comprisesalotofnumericalfeatures.Pleaseusethe
featuresinthetest.csvfile.Yourtaskistocreatetheclustereditems,whichisabinary
labelwith0and1(twoclusters).TheevaluationmetricistheRandindexorRandscore,
canbetestedagainst’smoking’labels.Nowtrainanunsupervisedclusteringmodelto
getagoodperformanceonthistask.
HigherEducation
I want an unsupervised clustering model to group student performances into eight
StudentsPerformance
groups. The dataset named ’Higher Education Students Performance Evaluation’
(id=856)canbedownloadedviaucimlrepolibrary.Theclusteringqualitycanbecheck
againsttargetvariableOUTPUTGrade.
Time-Series Weather IwantyoutocreateamodelfornodeclassificationontheCoradatasettopredictthe
Forecasting categoryofeachpaper.YouneedtodirectlyfindtheCoradatasetfromarelevantlibrary.
Electricity IwantyoutocreateamodelfornodeclassificationontheCiteseerdatasettopredictthe
categoryofeachpaper. YouneedtodirectlyfindtheCiteseerdatasetfromarelevant
library.
Node Cora Buildamodeltoperformtime-seriesforecastingusingtheWeatherdatasetuploaded
Classification here,evaluatingitsaccuracywiththeRMSLEmetric.Notethattheinputisasequence
ofpastobservationswithfixedsize(INPUTSEQLEN=96, INPUTDIM=21). The
modelshouldpredictthenextfuturesequencewithafixedsize(PREDSEQLEN=96,
PREDDIM=21).
Citeseer Youaresolvingthismachinelearningtasksoftimeseriesforecasting:Thedatasetpre-
sentedhere(theElectricitydataset)comprisesreal-worldtimeseriesdata. Pleasesplit
thedatasetintothreepartsoftrain,validandtest.Theinputisasequenceofpastobser-
vationwithfixedsize(INPUTSEQLEN=96,INPUTDIM=321).Yourtaskistopre-
dictthenextfuturesequencewithfixedsize(PREDSEQLEN=96,PREDDIM=321).
Theevaluationmetricisrootmeansquaredlogerror(RMSLE).Nowtrainatimeseries
forecastingmodeltogetagoodperformanceonthegivenfixedsequences.
15Preprint. Underreview.
Table4: Userinstruction(i.e.,taskdescription)forexperimentsundertheconstraint-awaresetting.
Boldtextsindicateconstraintsusedforevaluation.
Task Dataset InstructionPrompt
Image ButterflyImage IneedahighlyaccuratemachinelearningmodeldevelopedtoclassifyimageswithintheButterfly
Classification ImageClassificationdatasetintotheircorrectspeciescategories. Thedatasethasbeenuploadedwith
itslabelinformationinthelabels.csvfile.Pleaseuseaconvolutionalneuralnetwork(CNN)architecture
forthistask,leveragingtransferlearningfromapre-trainedResNet-50modeltoimproveaccuracy.
Optimizethemodelusingcross-validationonthetrainingsplittofine-tunehyperparameters,andaim
foranaccuracyofatleast0.95onthetestsplit. Providethefinaltrainedmodel,adetailedreport
ofthetrainingprocess,hyperparametersettings,accuracymetrics,andaconfusionmatrixtoevaluate
performanceacrossdifferentcategories.
Shopee-IET Pleaseprovideaclassificationmodelthatcategorizesimagesintooneoffourclothingcategories.The
imagepath,alongwithitslabelinformation,canbefoundinthefilestrainlabels.csvandtestlabels.csv.
Themodelshouldachieveatleast85%accuracyonthetestsetandbeimplementedusingPyTorch.
Additionally,pleaseincludedataaugmentationtechniquesandaconfusionmatrixintheevaluation.
Text EcommerceText Werequirethedevelopmentofanadvancedneuralnetworkmodelfortextclassificationtailoredto
Classification theEcommerceTextdataset,withtheobjectiveofachievingatleast0.95classificationaccuracy.
Themodelshouldbespecificallytrainedtodistinguishtextintofourdefinedcategories:Electronics,
Household,Books,andClothing&Accessories. Tofacilitatethis,wehaveuploadedthecomplete
datasetinitsentirety,withoutanypriordivisionintotraining,validation,ortestsets.
TextualEntailment Youaresolvingthismachinelearningtaskofclassification: Thedatasetpresentedhere(theTextual
Entailment)comprisesaseriesoflabeledtextpairs. Giventwotexts,yourtaskistopredicttherela-
tionshipofthetextpairasneutral(0),contradiction(1),orentailment(2). Theevaluationmetricis
accuracy.Buildalanguagemodeltogetgoodperformance,ensuringthemodelsizedoesnotexceed
200millionparametersandtheinferencetimeislessthan200millisecondsperprediction.
Tabular BananaQuality Buildamachinelearningmodel,potentiallyXGBoostorLightGBM,toclassifybananaqualityas
Classification GoodorBadbasedontheirnumericalinformationaboutbananasofdifferentquality(size,weight,
sweetness,softness,harvesttime,ripeness,andacidity). Wehaveuploadedtheentiredatasetforyou
hereinthebananaquality.csvfile.Themodelmustachieveatleast0.98accuracy.
SoftwareDefects Youaresolvingthisdatasciencetaskofbinaryclassification:Thedatasetpresentedhere(theSoftware
DefectsDataset)comprisesalotofnumericalfeatures.Pleasesplitthedatasetintothreepartsoftrain,
valid,andtest. Yourtaskistopredictthedefectsitem,whichisabinarylabelwith0and1. The
evaluationmetricistheF1score.Pleasetrainabinaryclassificationmodeltogetagoodperformance
onthistask,ensuringthatthemodeltrainingtimedoesnotexceed30minutesandtheprediction
timeforeachinstanceisunder5milliseconds.
Tabular CrabAge Youaresolvingthisdatasciencetaskofregression:Thedatasetpresentedhere(theCrabAgeDataset)
Regression comprisesalotofbothcategoricalandnumericalfeatures. Pleasesplitthedatasetintothreepartsof
train,valid,andtest. Yourtaskistopredicttheageitem. TheevaluationmetricistheRMSLE(root
meansquaredlogerror).Nowtrainaregressionmodeltogetagoodperformanceonthistask,ensuring
thatthemodel’strainingtimedoesnotexceed30minutesandthatitcanmakepredictionsonthe
testsetwithin5seconds.
CropPrice Ineedanaccurateregressionmodeltopredictcroppricesbasedonfeatureslikesoilcomposition,
environmentalfactors,historicalyielddata,andcropmanagementpracticesfromthedatasetIuploaded
here.YoushouldoptimizethemodeltoachieveRMSLElessthan1.0
Tabular SmokerStatus Youaresolvingthisdatasciencetaskofunsupervisedclustering: Thedatasetpresentedhere(the
Clustering SmokerStatusDataset)comprisesalotofnumericalfeatures.Pleaseusethefeaturesintest.csv.Your
taskistocreatetheclustereditems,whichisabinarylabelwith0and1(twoclusters).Theevaluation
metricistheRandindexorRandscore,whichcanbetestedagainst’smoking’labels. Nowtrainan
unsupervisedclusteringmodeltogetagoodperformanceonthistask,ensuringthattheRandindexis
atleast0.75andthemodeltrainingtimedoesnotexceed10minutes.
HigherEducation
Iwantanunsupervisedclusteringmodeltogroupstudentperformancesintoeightgroups.Thedataset
StudentsPerformance
named’HigherEducationStudentsPerformanceEvaluation’(id=856)canbedownloadedviaucimlrepo
library.TheclusteringqualitycanbecheckedagainstthetargetvariableOUTPUTGrade.Themodel
shouldachieveaRandScoreofatleast0.8andcompleteclusteringwithin10minutes.
Time-Series Weather Buildastate-of-the-arttime-seriesforecastingmodelfortheWeatherdatasetuploadedhere,evaluating
Forecasting itsaccuracywiththeRMSLEmetric.Notethattheinputisasequenceofpastobservationswithfixed
size(INPUTSEQLEN=96,INPUTDIM=21). Themodelshouldpredictthenextfuturesequence
withafixedsize(PREDSEQLEN=96,PREDDIM=21).WetargetRMSLElowerthan0.05.
Electricity Youaresolvingthismachinelearningtaskoftimeseriesforecasting:Thedatasetpresentedhere(the
Electricitydataset)comprisesreal-worldtimeseriesdata.Pleasesplitthedatasetintothreepartsoftrain,
valid,andtest.Theinputisasequenceofpastobservationwithfixedsize(INPUTSEQLEN=96,IN-
PUTDIM=321).Yourtaskistopredictthenextfuturesequencewithfixedsize(PREDSEQLEN=96,
PREDDIM=321). Theevaluationmetricisrootmeansquaredlogerror(RMSLE).Nowtrainatime
seriesforecastingmodeltogetagoodperformanceonthegivenfixedsequences. Ensurethemodel
achievesanRMSLEoflessthan0.1andthatthetrainingtimedoesnotexceed1houronaGPU.
Node Cora IwantyoutodevelopanodeclassificationmodelusingtheGraphConvolutionalNetwork(GCN)
Classification algorithmtopredictthecategoryofeachpaperintheCoradataset.StartbyimportingtheCoradataset
usingthe‘Planetoid‘datasetfromthe‘torchgeometric.datasets‘moduleinPyTorchGeometric.Ensure
youpreprocessthedatatoincludenodefeaturesandlabelscorrectly.Trainthemodelusingasuitable
optimizerandlossfunction.Then,evaluateitsaccuracyonthetestset.Theaccuracyonthetestset
shouldbeover0.90.
Citeseer IwantyoutodevelopanodeclassificationmodelusingtheGraphConvolutionalNetwork(GCN)
algorithmtopredictthecategoryofeachpaperintheCiteseerdataset.StartbyimportingtheCiteseer
datasetusingthe‘Planetoid‘datasetfromthe‘torchgeometric.datasets‘moduleinPyTorchGeometric.
Ensureyoupreprocessthedatatoincludenodefeaturesandlabelscorrectly.Trainthemodelusinga
suitableoptimizerandlossfunction.Then,evaluateitsaccuracyonthetestset.Theaccuracyonthe
testsetshouldbeover0.80.
16Preprint. Underreview.
A.1 SKELETONPYTHONSCRIPT
SkeletonPythonScript(e.g.,text classification.py)
# The following code is for "text classification" task using PyTorch.
import os, random, time, json
# define GPU location
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "3"
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import gradio as gr
# TODO: import other required library here, including libraries for datasets and (pre-
trained) models like HuggingFace and Kaggle APIs. If the required module is not found,
you can directly install it by running ‘pip install your_module‘.
from torchtext import datasets, data, vocab
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, f1_score
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)
np.random.seed(SEED)
# Define device for model operations
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DATASET_PATH = "_experiments/datasets" # path for saving and loading dataset(s) (or the
user’s uploaded dataset) for preprocessing, training, hyperparamter tuning, deployment
, and evaluation
# Data preprocessing and feature engineering
def preprocess_data():
# TODO: this function is for data preprocessing and feature engineering
# Run data preprocessing
# Should return the preprocessed data
return processed_data
def train_model(model, train_loader):
# TODO: this function is for model training loop and optimization on ’train’ and ’valid’
datasets
# TODO: this function is for fine-tuning a given pretrained model (if applicable)
# Should return the well-trained or finetuned model.
return model
def evaluate_model(model, test_loader):
# In this task, we use Accuracy and F1 metrics to evaluate the text classification
performance.
# The ‘performance_scores‘ should be in dictionary format having metric names as the
dictionary keys
# TODO: the first part of this function is for evaluating a trained or fine-tuned model
on the ’test’ dataset with respect to the relevant downstream task’s performance
metrics
# Define the ‘y_true‘ for ground truth and ‘y_pred‘ for the predicted classes here.
performance_scores = {
’ACC’: accuracy_score(y_true, y_pred),
’F1’: f1_score(y_true, y_pred)
}
# TODO: the second part of this function is for measuring a trained model complexity on
a samples with respect to the relevant complexity metrics, such as inference time
and model size
# The ‘complexity_scores‘ should be in dictionary format having metric names as the
dictionary keys
# Should return model’s performance scores
return performance_scores, complexity_scores
def prepare_model_for_deployment():
17Preprint. Underreview.
# TODO: this function is for preparing an evaluated model using model compression and
conversion to deploy the model on a particular platform
# Should return the deployment-ready model
return deployable_model
def deploy_model():
# TODO: this function is for deploying an evaluated model with the Gradio Python library
# Should return the url endpoint generated by the Gradio library
return url_endpoint
# The main function to orchestrate the data loading, data preprocessing, feature
engineering, model training, model preparation, model deployment, and model evaluation
def main():
"""
Main function to execute the text classification pipeline.
"""
# TODO: Step 1. Retrieve or load a dataset from hub (if available) or user’s local
storage (if given)
dataset = None
# TODO: Step 2. Create a train-valid-test split of the data by splitting the ‘dataset‘
into train_loader, valid_loader, and test_loader.
# Here, the train_loader contains 70% of the ‘dataset‘, the valid_loader contains 20% of
the ‘dataset‘, and the test_loader contains 10% of the ‘dataset‘.
train_loader, valid_loader, test_loader = (None, None, None) # corresponding to 70%,
20%, 10% of ‘dataset‘
# TODO: Step 3. With the split dataset, run data preprocessing and feature engineering (
if applicable) using the "preprocess_data" function you defined
processed_data = preprocess_data()
# TODO: Step 4. Define required model. You may retrieve model from available hub or
library along with pretrained weights (if any).
# If pretrained or predefined model is not available, please create the model according
to the given user’s requirements below using PyTorch and relevant libraries.
model = None
# TODO: Step 5. train the retrieved/loaded model using the defined "train_model"
function
# TODO: on top of the model training, please run hyperparamter optimization based on the
suggested hyperparamters and their values before proceeding to the evaluation step
to ensure model’s optimality
model = train_model()
# TODO: evaluate the trained model using the defined "evaluate_model" function
model_performance, model_complexity = evaluate_model()
# TODO: compress and convert the trained model according to a given deployment platform
using the defined "prepare_model_for_deployment" function
deployable_model = prepare_model_for_deployment()
# TODO: deploy the model using the defined "deploy_model" function
url_endpoint = deploy_model()
return processed_data, model, deployable_model, url_endpoint, model_performance,
model_complexity
if __name__ == "__main__":
processed_data, model, deployable_model, url_endpoint, model_performance,
model_complexity = main()
print("Model Performance on Test Set:", model_performance)
print("Model Complexity:", model_complexity)
A.2 DATASETDESCRIPTIONS
AspresentedinTable2,weselectsevenrepresentativedownstreamtasks,coveringfivedatamodal-
ities. Wedescribethedatasetstheirstatisticsasfollows.
• Butterfly Image (Butterfly). This dataset includes 75 distinct classes of butterflies, featuring
over 1,000 labeled images, including validation images. Each image is assigned to a single
18Preprint. Underreview.
butterfly category. The dataset is accessible at https://www.kaggle.com/datasets/
phucthaiv02/butterfly-image-classification.
• Shopee-IET (Shopee). This dataset is designed for cloth image classification, where each
image represents a clothing item, and its corresponding label indicates the clothing cate-
gory. The available labels include BabyPants, BabyShirt, womencasualshoes, and wom-
enchiffontop. The dataset is available at https://www.kaggle.com/competitions/
demo-shopee-iet-competition/data.
• Ecommerce Text (Ecomm). This dataset is a classification-based E-commerce text
dataset comprising four categories: Electronics, Household, Books, and Clothing & Ac-
cessories, which together cover approximately 80% of any E-commerce website. It in-
cludes 50,425 instances and can be found at https://www.kaggle.com/datasets/
saurabhshahane/ecommerce-text-classification.
• TextualEntailment(Entail). Thisdatasetconsistsoflabeledpairsoftext, wherethetaskisto
predict the relationship between each pair as either neutral (0), contradiction (1), or entailment
(2). Itisdividedintoatrainingsetcontaining4,907samplesandatestingsetwith4,908samples.
WeusethedatasetprovidedbyGuoetal.(2024a).
• Banana Quality (Banana). This tabular dataset consists of numerical information on 8,000
samples of bananas, covering various quality attributes such as size, weight, sweetness, soft-
ness, harvest time, ripeness, acidity, and overall quality. The primary objective of the dataset
is to classify each banana sample as either good or bad. The dataset is available at https:
//www.kaggle.com/datasets/l3llff/banana/data.
• SoftwareDefects(Software). Thisdatasetconsistsprimarilyofnumericalfeaturesandhasbeen
divided into three parts: training, validation, and testing. The goal is to predict a binary defect
label(0or1). Thetrainingsetcontains82,428samples,thevalidationsetcontains9,158samples,
andthetestsetcontains91,587samples. WeusethedatasetprovidedbyGuoetal.(2024a).
• Crab Age (Crab). This dataset contains a mix of categorical and numerical features, and has
beendividedintothreeparts: training, validation,andtestsets. Thetaskistopredicttheageof
thecrabs. Thetrainingsetconsistsof59,981samples,thevalidationsetincludes6,664samples,
andthetestsetcontains66,646samples. WeusethedatasetprovidedbyGuoetal.(2024a).
• Crop Price (Crop). This new dataset contains 2,200 samples with key features such as nitro-
gen, phosphorus, and potassium ratios in the soil, temperature (in °C), humidity (in %), soil
pH value, and rainfall (in mm), all of which are essential for predicting crop yield values.
Crop yield prediction is crucial in modern agriculture, particularly as data-driven methods be-
comemoreprevalent. Thisdatasetisavailableathttps://www.kaggle.com/datasets/
varshitanalluri/crop-price-prediction-dataset.
• Smoker Status (Smoker). This dataset contains numerous numerical features. The goal is to
categorize smoking status of each instance into a cluster. The training set consists of 143,330
samples and the test set includes 143,331 samples. We use the dataset provided by Guo et al.
(2024a).
• Higher Education Students Performance (Student). The dataset, collected in 2019 from stu-
dents in the Faculty of Engineering and Faculty of Educational Sciences, was created to pre-
dict students’ end-of-term performances using machine learning techniques. It is a multivariate
datasetwith145instancesand31integer-typefeatures,focusingonclassificationtaskswithinthe
domain of social sciences. We adopt this dataset for unsupervised clustering instead of classifi-
cation. Thisdatasetcanbefoundathttps://archive.ics.uci.edu/dataset/856/
higher+education+students+performance+evaluation.
• Weather. The weather dataset consists of 21 meteorological factors collected every 10 min-
utes from the Weather Station at the Max Planck Biogeochemistry Institute in 2020, containing
52,603sampleswithoutanypre-splitting. Itisaccessibleathttps://github.com/thuml/
Time-Series-Library.
• Electricity. This dataset comprises hourly electricity consumption data for 321 customers col-
lected from 2012 to 2014, totaling 26,211 samples. The dataset records the electricity usage of
theseclientsonanhourlybasisandisprovidedwithoutanypre-split. Thedatasetisavailableat
https://github.com/thuml/Time-Series-Library.
• CoraandCiteseer. Thecitationnetworkdatasets, ”Cora”and”CiteSeer,”consistofnodesrep-
resenting documents and edges representing citation links between them. Both datasets provide
training,validation,andtestsplitsthroughbinarymasks. TheCoradatasetcontains2,708nodes,
10,556edges,1,433features,and7classes,whileCiteSeerconsistsof3,327nodes,9,104edges,
3,703features,and6classes. WeusetheversionprovidedbyFey&Lenssen(2019).
19Preprint. Underreview.
A.3 BASELINES
Human Models We select top-performing models based on evaluations from Papers with Code
benchmarksorKagglenotebooks,wherethesimilartasksanddatasetsareapplicable. Thechosen
modelsforrelevantdownstreamtasksaredescribedbelow.
• Image Classification. The human models for image classification tasks are obtained from a
Kagglenotebookavailableathttps://www.kaggle.com/code/mohamedhassanali/
butterfly-classify-pytorch-pretrained-model-acc-99/notebook, utiliz-
ingapretrainedResNet-18model.
• TextClassification. Fortextclassificationtasks,twomodelsareemployed. AWord2Vec-based
XGBoost model is applied to the e-commerce text dataset https://www.kaggle.com/
code/sugataghosh/e-commerce-text-classification-tf-idf-word2vec#
Word2Vec-Hyperparameter-Tuning, while the XLM-RoBERTa model is used for
the textual entailment dataset https://www.kaggle.com/code/vbookshelf/
basics-of-bert-and-xlm-roberta-pytorch.
• Tabular Classification. Due to the absence of a similar model in the repository, we use the
state-of-the-artTabPFNmodel(Hollmannetal.,2023a)designedfortabularclassificationtasks.
• Tabular Regression. For tabular regression tasks, we adopt two models specif-
ically designed for the given datasets, which are available at https://www.
kaggle.com/code/shatabdi5/crab-age-regression for the crab age
dataset and at https://www.kaggle.com/code/mahmoudmagdyelnahal/
crop-yield-prediction-99/notebookforthecropyielddataset.
• TabularClustering. Forunsupervisedclusteringtasks, weusemanuallyhyperparameter-tuned
KMeansclustering,followingtheapproachoutlinedinhttps://www.kaggle.com/code/
samuelcortinhas/tps-july-22-unsupervised-clustering,asthebaseline.
• Time-SeriesForecasting.Inthistask,weusethestate-of-the-artiTransformer(Liuetal.,2024b),
whichisdesignedforthesametaskanddatasetsasthebaselinemodel.
• NodeClassification. Fornodeclassificationtasks,wealsoemployastate-of-the-artgraphneural
network-basedmodel,PMLP(Yangetal.,2023),asthehandcraftedbaselineforbothdatasets.
AutoGluon We adopt AutoGluon as the baseline because it is a state-of-the-art AutoML frame-
work capable of handling various downstream tasks and data modalities, with the exception of
graph data. There are three variants of AutoGluon: AutoGluon-TS (Shchur et al., 2023) for time
series, AutoGluon-Tabular (Erickson et al., 2020) for tabular machine learning, and AutoGluon-
Multimodal(Tangetal.,2024)forcomputervisionandnaturallanguageprocessingtasks.
GPT-3.5andGPT-4 ForGPT-3.5andGPT-4,weusethegpt-3.5-turbo-0125andgpt-4-2024-05-
13modelsviatheOpenAIAPI.Weimplementthezero-shotbaselinesusingthepromptbelow.
Zero-ShotPromptforGPT-3.5andGPT-4Baselines
You are a helpful intelligent assistant. Now please help solve the following machine
learning task.
[Task]
{user instruction}
[{file_name}.py] ‘‘‘python
{full-pipeline skeleton script}
‘‘‘
Start the python code with "‘‘‘python". Please ensure the completeness of the code so that
it can be run without additional modifications.
DS-Agent WereproducetheDS-Agent(Guoetal.,2024a)baselineusingtheofficialsourcecode.
However, it is important to note that our framework encompasses the entire process from data re-
trieval/loadingtodeployment,whereasDS-Agentfocusessolelyonthemodelingaspect,assuming
complete data and evaluation codes are provided. In this paper, we utilize the deployment stage
of DS-Agent along with its collected case banks and Adapter prompt for the same tasks, as the
sourcecodeformanualhumaninsightscollectionduringthedevelopmentstageisunavailable.
20Preprint. Underreview.
A.4 EVALUATIONMETRICS
SuccessRate(SR) Weemploythesuccessrate(Guoetal.,2024a;Hongetal.,2024a),whicheval-
uateswhetherthemodelsbuiltbyanLLMagentareexecutableinthegivenruntimeenvironment.
Successrateisusedtoassesscodeexecution.
Fortheconstraint-freesetting,weapplyathree-levelgradingscaleasfollows.
• 0.00: Codecannotbeexecuted.
• 0.50: CodeprovidesarunnableML/DLmodel.
• 1.00: Codeprovidesarunnablemodelandanaccessibledeploymentendpoint(e.g.,Gradio).
For the constraint-aware setting, we use a five-level grading scale to evaluate whether the code
executessuccessfullyandsatisfiesthegivenconstraints. Thegradingcriteriaareasfollows.
• 0.00: Codecannotbeexecuted.
• 0.25: CodeprovidesarunnableML/DLmodel.
• 0.50: Codeprovidesarunnablemodelandanaccessibledeploymentendpoint(e.g.,Gradio).
• 0.75: Codeprovidesadeployed,runnablemodelthatpartiallymeetsconstraints(e.g.,targetper-
formance,inferencetime,andmodelsize).
• 1.00: Codeprovidesadeployed,runnablemodelthatfullymeetsconstraints.
Normalized Performance Score (NPS) In this paper, each downstream task is associated with
a specific evaluation metric, which may vary between tasks. These metrics include accuracy, F1-
score,andRMSLE.FormetricssuchasaccuracyandF1-score,wepresenttherawvaluestofacilitate
comparisonacrossidenticaldatatasks. Forperformancemetricswherelowervaluesindicatebetter
performance,suchasloss-basedmetrics,wenormalizeallperformancevaluessusingthefollowing
transformation: NPS = 1 . This transformation ensures that metrics like RMSLE are scaled
1+s
between0and1,withhigherNPSvaluesindicatingbetterperformance.
Notethatachievingdownstreamtaskperformance(NPS)requiresarunnablemodel,i.e.,SR>0. If
themodelcannotrun,theNPSiszerobydefaultasitcannotmakeanypredictions.
Comprehensive Score (CS) To evaluate both the success rate and the downstream task perfor-
mance of the generated AutoML pipelines simultaneously, we calculate CS as a weighted sum of
SRandNPS,asfollows: CS=0.5×SR+0.5×NPS.
B PROMPTS FOR AUTOML-AGENT
B.1 AGENTSPECIFICATIONS
This subsection provides the system prompt design for agent specifications in AutoML-Agent, in-
cluding Agent Manger (B.1.1), Prompt Agent (B.1.2), Data Agent (B.1.3), Model Agent (B.1.4),
andOperationAgent(B.1.5).
B.1.1 AGENTMANAGER
SystemMessageforAgentManagerSpecification
You are an experienced senior project manager of a automated machine learning project (
AutoML). You have two main responsibilities as follows.
1. Receive requirements and/or inquiries from users through a well-structured JSON object.
2. Using recent knowledge and state-of-the-art studies to devise promising high-quality
plans for data scientists, machine learning research engineers, and MLOps engineers in
your team to execute subsequent processes based on the user requirements you have
received.
21Preprint. Underreview.
B.1.2 PROMPTAGENT
SystemMessageforPromptAgentSpecification
You are an assistant project manager in the AutoML development team.
Your task is to parse the user’s requirement into a valid JSON format using the JSON
specification schema as your reference. Your response must exactly follow the given
JSON schema and be based only on the user’s instruction.
Make sure that your answer contains only the JSON response without any comment or
explanation because it can cause parsing errors.
#JSON SPECIFICATION SCHEMA#
‘‘‘json
{json_specification}
‘‘‘
Your response must begin with "‘‘‘json" or "{{" and end with "‘‘‘" or "}}", respectively.
B.1.3 DATAAGENT
SystemMessageforDataAgentSpecification
You are the world’s best data scientist of an automated machine learning project (AutoML)
that can find the most relevant datasets,run useful preprocessing, perform suitable
data augmentation, and make meaningful visulaization to comprehensively understand the
data based on the user requirements. You have the following main responsibilities to
complete.
1.Retrieve a dataset from the user or search for the dataset based on the user instruction.
2.Perform data preprocessing based on the user instruction or best practice based on the
given tasks.
3.Perform data augmentation as neccesary.
4.Extract useful information and underlying characteristics of the dataset.
B.1.4 MODELAGENT
SystemMessageforModelAgentSpecification
You are the world’s best machine learning research engineer of an automated machine
learning project (AutoML) that can find the optimal candidate machine learning models
and artificial intelligence algorithms for the given dataset(s), run hyperparameter
tuning to opimize the models, and perform metadata extraction and profiling to
comprehensively understand the candidate models or algorithms based on the user
requirements. You have the following main responsibilities to complete.
1. Retrieve a list of well-performing candidate ML models and AI algorithms for the given
dataset based on the user’s requirement and instruction.
2. Perform hyperparameter optimization for those candidate models or algorithms.
3. Extract useful information and underlying characteristics of the candidate models or
algorithms using metadata extraction and profiling techniques.
4. Select the top-k (‘k‘ will be given) well-performing models or algorithms based on the
hyperparameter optimization and profiling results.
B.1.5 OPERATIONAGENT
SystemMessageforOperationAgentSpecification
You are the world’s best MLOps engineer of an automated machine learning project (AutoML)
that can implement the optimal solution for production-level deployment, given any
datasets and models. You have the following main responsibilities to complete.
1. Write accurate Python codes to retrieve/load the given dataset from the corresponding
source.
2. Write effective Python codes to preprocess the retrieved dataset.
3. Write precise Python codes to retrieve/load the given model and optimize it with the
suggested hyperparameters.
4. Write efficient Python codes to train/finetune the retrieved model.
5. Write suitable Python codes to prepare the trained model for deployment. This step may
include model compression and conversion according to the target inference platform.
22Preprint. Underreview.
6. Write Python codes to build the web application demo using the Gradio library.
7. Run the model evaluation using the given Python functions and summarize the results for
validation againts the user’s requirements.
B.2 PROMPTSFORRETRIEVAL-AUGMENTEDPLANNING
Thissubsectionpresentspromptsforplanning-relatedprocesses(Figure2(a)),includingknowledge
retrievalandsummaryprompts(B.2.1),planningprompt(B.2.2),andplanrevisionprompt(B.2.3).
B.2.1 KNOWLEDGERETRIEVALPROMPT
PromptforKnowledgeRetrievalandSummaryforPlanning
KaggleNotebook
I searched the Kaggle Notebooks to find state-of-the-art solutions using the keywords: {
user_task} {user_domain}. Here is the result:
=====================
{context}
=====================
Please summarize the given pieces of Python notebooks into a single paragraph of useful
knowledge and insights. Do not include the source codes. Instead, extract the insights
from the source codes. We aim to use your summary to address the following user’s
requirements.
# User’s Requirements
{user_requirement_summary}
PapersWithCode
I searched the paperswithcode website to find state-of-the-art models using the keywords: {
user_area} and {user_task}. Here is the result:
=====================
{context}
=====================
Please summarize the given pieces of search content into a single paragraph of useful
knowledge and insights. We aim to use your summary to address the following user’s
requirements.
# User’s Requirements
{user_requirement_summary}
arXiv
I searched the arXiv papers using the keywords: {task_kw} and {domain_kw}. Here is the
result:
=====================
{context}
=====================
Please summarize the given pieces of arXiv papers into a single paragraph of useful
knowledge and insights. We aim to use your summary to address the following user’s
requirements.
# User’s Requirements
{user_requirement_summary}
GoogleWebSearch
I searched the web using the query: {search_query}. Here is the result:
=====================
{context}
=====================
Please summarize the given pieces of search content into a single paragraph of useful
knowledge and insights.
We aim to use your summary to address the following user’s requirements.
# User’s Requirements
{user_requirement_summary}
23Preprint. Underreview.
Summary
Please extract and summarize the following group of contents collected from different
online sources into a chunk of insightful knowledge. Please format your answer as a
list of suggestions. I will use them to address the user’s requirements in machine
learning tasks.
# Source: Google Web Search
{search_summary}
=====================
# Source: arXiv Papers
{arxiv_summary}
=====================
# Source: Kaggle Hub
{kaggle_summary}
=====================
# Source: PapersWithCode
{pwc_summary}
=====================
The user’s requirements are summarized as follows.
{user_requirement_summary}
B.2.2 PLANNINGPROMPT
PromptforRetrieval-AugmentedPlanning
Now, I want you to devise an end-to-end actionable plan according to the user’s
requirements described in the following JSON object.
‘‘‘json
{user_requirements}
‘‘‘
Here is a list of past experience cases and knowledge written by an human expert for a
relevant task:
{plan_knowledge}
When devising a plan, follow these instructions and do not forget them:
- Ensure that your plan is up-to-date with current state-of-the-art knowledge.
- Ensure that your plan is based on the requirements and objectives described in the above
JSON object.
- Ensure that your plan is designed for AI agents instead of human experts. These agents
are capable of conducting machine learning and artificial intelligence research.
- Ensure that your plan is self-contained with sufficient instructions to be executed by
the AI agents.
- Ensure that your plan includes all the key points and instructions (from handling data to
modeling) so that the AI agents can successfully implement them. Do NOT directly
write the code.
- Ensure that your plan completely include the end-to-end process of machine learning or
artificial intelligence model development pipeline in detail (i.e., from data
retrieval to model training and evaluation) when applicable based on the given
requirements.
B.2.3 PLANREVISIONPROMPT
PromptforPlanRevision
Now, you will be asked to revise and rethink {num2words(n_plans)} different end-to-end
actionable plans according to the user’s requirements described in the JSON object
below.
‘‘‘json
{user_requirements}
‘‘‘
24Preprint. Underreview.
Please use to the following findings and insights summarized from the previously failed
plans. Try as much as you can to avoid the same failure again.
{fail_rationale}
Finally, when devising a plan, follow these instructions and do not forget them:
- Ensure that your plan is up-to-date with current state-of-the-art knowledge.
- Ensure that your plan is based on the requirements and objectives described in the above
JSON object.
- Ensure that your plan is designed for AI agents instead of human experts. These agents
are capable of conducting machine learning and artificial intelligence research.
- Ensure that your plan is self-contained with sufficient instructions to be executed by
the AI agents.
- Ensure that your plan includes all the key points and instructions (from handling data to
modeling) so that the AI agents can successfully implement them. Do NOT directly
write the code.
- Ensure that your plan completely include the end-to-end process of machine learning or
artificial intelligence model development pipeline in detail (i.e., from data
retrieval to model training and evaluation) when applicable based on the given
requirements.
B.3 PROMPTSFORPROMPTING-BASEDPLANEXECUTION
This subsection presents prompts for prompting-based plan execution processes (Figure 2(b)), in-
cluding plan decomposition (Data Agent (B.3.1) and Model Agent (B.3.2)), pseudo data analysis
(B.3.3),andtraining-freemodelsearchandHPO(B.3.4).
B.3.1 PLANDECOMPOSITION: DATAAGENT
PromptforPlanDecomposition: DataAgent
As a proficient data scientist, summarize the following plan given by the senior AutoML
project manager according to the user’s requirements and your expertise in data
science.
# User’s Requirements
‘‘‘json
{user_requirements}
‘‘‘
# Project Plan
{plan}
The summary of the plan should enable you to fulfill your responsibilities as the answers
to the following questions by focusing on the data manipulation and analysis.
1. How to retrieve or collect the dataset(s)?
2. How to preprocess the retrieved dataset(s)?
3. How to efficiently augment the dataset(s)?
4. How to extract and understand the underlying characteristics of the dataset(s)?
Note that you should not perform data visualization because you cannot see it. Make sure
that another data scientist can exectly reproduce the results based on your summary.
B.3.2 PLANDECOMPOSITION: MODELAGENT
PromptforPlanDecomposition: ModelAgent
As a proficient machine learning research engineer, summarize the following plan given by
the senior AutoML project manager according to the user’s requirements, your expertise
in machine learning, and the outcomes from data scientist.
**User’s Requirements**
‘‘‘json
{user_requirements}
‘‘‘
**Project Plan**
{project_plan}
**Explanations and Results from the Data Scientist**
25Preprint. Underreview.
{data_result}
The summary of the plan should enable you to fulfill your responsibilities as the answers
to the following questions by focusing on the modeling and optimization tasks.
1. How to retrieve or find the high-performance model(s)?
2. How to optimize the hyperparamters of the retrieved models?
3. How to extract and understand the underlying characteristics of the dataset(s)?
4. How to select the top-k models or algorithms based on the given plans?
B.3.3 PSEUDODATAANALYSISBYDATAAGENT
PromptforPseudoDataAnalysis
As a proficient data scientist, your task is to explain **detailed** steps for data
manipulation and analysis parts by executing the following machine learning
development plan.
# Plan
{decomposed_data_plan}
# Potential Source of Dataset
{available_sources}
Make sure that your explanation follows these instructions:
- All of your explanation must be self-contained without using any placeholder to ensure
that other data scientists can exactly reproduce all the steps, but do not include any
code.
- Include how and where to retrieve or collect the data.
- Include how to preprocess the data and which tools or libraries are used for the
preprocessing.
- Include how to do the data augmentation with details and names.
- Include how to extract and understand the characteristics of the data.
- Include reasons why each step in your explanations is essential to effectively complete
the plan.
Note that you should not perform data visualization because you cannot see it. Make sure to
focus only on the data part as it is your expertise. Do not conduct or perform
anything regarding modeling or training.
After complete the explanations, explicitly specify the (expected) outcomes and results
both quantitative and qualitative of your explanations.
B.3.4 TRAINING-FREEMODELSEARCHANDHPOBYMODELAGENT
PromptforTraining-FreeModelSearchandHPO
As a proficient machine learning research engineer, your task is to explain **detailed**
steps for modeling and optimization parts by executing the following machine learning
development plan with the goal of finding top-{k} candidate models/algorithms.
# Suggested Plan
{decomposed_model_plan}
# Available Model Source
{available_sources}
Make sure that your explanation for finding the top-{k} high-performance models or
algorithms follows these instructions:
- All of your explanations must be self-contained without using any placeholder to ensure
that other machine learning research engineers can exactly reproduce all the steps,
but do not include any code.
- Include how and where to retrieve or find the top-{k} well-performing models/algorithms.
- Include how to optimize the hyperparamters of the candidate models or algorithms by
clearly specifying which hyperparamters are optimized in detail.
- Corresponding to each hyperparamter, explicitly include the actual numerical value that
you think it is the optimal value for the given dataset and machine learning task.
- Include how to extract and understand the characteristics of the candidate models or
algorithms, such as their computation complexity, memory usage, and inference latency.
This part is not related to visualization and interpretability.
- Include reasons why each step in your explanations is essential to effectively complete
the plan.
Make sure to focus only on the modeling part as it is your expertise. Do not conduct or
perform anything regarding data manipulation or analysis.
26Preprint. Underreview.
After complete the explanations, explicitly specify the names and (expected) quantitative
performance using relevant numerical performance and complexity metrics (e.g., number
of parameters, FLOPs, model size, training time, inference speed, and so on) of the {
num2words(k)} candidate models/algorithms potentially to be the optimal model below.
Do not use any placeholder for the quantitative performance. If you do not know the exact
values, please use the knowledge and expertise you have to estimate those performance
and complexity values.
B.4 PROMPTSFORMULTI-STAGEVERIFICATION
Thissubsectionpresentspromptsformulti-stageverification(Figure2(c)), whichensuresthecor-
rectnessofintermediateresultsbetweenstepsintheframework. Thesestagesincluderequestveri-
ficationB.4.1),executionverificationB.4.2,andimplementationverificationB.4.3.
B.4.1 REQUESTVERIFICATION
RequestVerification(Relevancy)
Is the following statement relevant to machine learning or artificial intelligence?
‘{user instruction}‘
Answer only ’Yes’ or ’No’
RequestVerification(Adequacy)
Given the following JSON object representing the user’s requirement for a potential ML or
AI project, please tell me whether we have essential information (e.g., problem and
dataset) to be used for a AutoML project?
Please note that our users are not AI experts, you must focus only on the essential
requirements, e.g., problem and brief dataset descriptions.
You do not need to check every details of the requirements. You must also answer ’yes’ even
though it lacks detailed and specific information.
‘‘‘json
{parsed user requirements}
‘‘‘
Please answer with this format: ‘a ’yes’ or ’no’ answer; your reasons for the answer‘ by
using ’;’ to separate between the answer and its reasons.
If the answer is ’no’, you must tell me the alternative solutions or examples for
completing such missing information.
B.4.2 EXECUTIONVERIFICATION
ExecutionVerification
Given the proposed solution and user’s requirements, please carefully check and verify
whether the proposed solution ’pass’ or ’fail’ the user’s requirements.
**Proposed Solution and Its Implementation**
Data Manipulation and Analysis: {data_agent_outcomes}
Modeling and Optimization: {model_agent_outcomes}
**User Requirements**
‘‘‘json
{user_requirements}
‘‘‘
Answer only ’Pass’ or ’Fail’
27Preprint. Underreview.
B.4.3 IMPLEMENTATIONVERIFICATION
ImplementationVerification
As the project manager, please carefully verify whether the given Python code and results
satisfy the user’s requirements.
- Python Code
‘‘‘python
{implementation_result[’code’]}
‘‘‘
- Code Execution Result
{implementation_result[’action_result’]}
- User’s Requirements
{user_requirements}
Answer only ’Pass’ or ’Fail’
C EXAMPLES OF INTERMEDIATE RESULTS
Thissectionpresentsexamplesofintermediateresultsfromthemainstepsofourframework,includ-
ing prompt parsing results (C.1), retrieval-augmented planning results (C.2), plan decomposition
results(C.3),andprompting-basedplanexecutionresults(C.4).
C.1 PROMPTPARSING
TaskDescriptionandParsedJSONUserRequirements
Constraint-FreeUserPrompt: BuildamodeltoclassifybananaqualityasGoodorBadbased
ontheirnumericalinformationaboutbananasofdifferentquality(size,weight,sweetness,soft-
ness,harvesttime,ripeness,andacidity). Wehaveuploadedtheentiredatasetforyouhereinthe
banana quality.csvfile.
ParsedOutput:
{
"user": {"intent": "build","expertise": "medium"},
"problem": {
"area": "tabular data analysis",
"downstream_task": "tabular classification",
"application_domain": "agriculture",
"description": "Build a model to classify banana quality as Good or Bad based on
their numerical information about bananas of different quality (size, weight,
sweetness, softness, harvest time, ripeness, and acidity).",
"performance_metrics": [],
"complexity_metrics": []
},
"dataset": [
{
"name": "banana_quality",
"modality": ["tabular"],
"target_variables": ["quality"],
"specification": null,
"description": "A dataset containing numerical information about bananas of
different quality, including size, weight, sweetness, softness, harvest time,
ripeness, and acidity.",
"preprocessing": [],
"augmentation": [],
"visualization": [],
"source": "user-upload"
}
],
"model": [
{
"name": "",
"family": "",
"type": "classical machine learning",
"specification": null,
28Preprint. Underreview.
"description": "A model to classify banana quality as Good or Bad based on their
numerical information."
}
],
...
}
Constraint-Aware User Prompt: Build a machine learning model, potentially XGBoost or
LightGBM, to classify banana quality as Good or Bad based on their numerical information
aboutbananasofdifferentquality(size,weight,sweetness,softness,harvesttime,ripeness,and
acidity). We have uploaded the entire dataset for you here in the banana quality.csv file. The
modelmustachieveatleast0.98accuracy.
ParsedOutput:
{
"user": {"intent": "build", "expertise": "medium"},
"problem": {
"area": "tabular data analysis",
"downstream_task": "tabular classification",
"application_domain": "agriculture",
"description": "Build a machine learning model, potentially XGBoost or LightGBM, to
classify banana quality as Good or Bad based on their numerical information
about bananas of different quality (size, weight, sweetness, softness, harvest
time, ripeness, and acidity). The model must achieve at least 0.98 accuracy.",
"performance_metrics": [
{"name": "accuracy", "value": 0.98}
],
"complexity_metrics": []
},
"dataset": [
{
"name": "banana_quality",
"modality": ["tabular"],
"target_variables": ["quality"],
"specification": null,
"description": "A dataset containing numerical information about bananas of
different quality, including size, weight, sweetness, softness, harvest time,
ripeness, and acidity.",
"preprocessing": [],
"augmentation": [],
"visualization": [],
"source": "user-upload"
}
],
"model": [
{
"name": "XGBoost",
"family": "ensemble models",
"type": "ensemble",
"specification": null,
"description": "A potential model to classify banana quality as Good or Bad,
potentially using XGBoost or LightGBM."
}
],
...
}
C.2 RETRIEVAL-AUGMENTEDPLANNING
ExampleResultsfromRetrieval-AugmentedPlanningwithP =3
UserInput:IneedaveryaccuratemodeltoclassifyimagesintheButterflyImageClassification
datasetintotheirrespectivecategories. Thedatasethasbeenuploadedwithitslabelinformation
inthelabels.csvfile.
Plan#1
### End-to-End Actionable Plan for Butterfly Image Classification
29Preprint. Underreview.
**Objective**: Develop a highly accurate neural network model to classify images in the
Butterfly Image Classification dataset into their respective categories.
#### 1. Data Collection and Preprocessing
1. **Dataset Loading**:
- Load the Butterfly Image Classification dataset and the associated labels from ‘labels.
csv‘.
- Verify the integrity of the dataset, ensuring all images have corresponding labels.
2. **Data Preprocessing**:
- **Image Resizing**: Resize all images to a uniform size (e.g., 224x224 pixels) to
ensure consistency.
- **Normalization**: Normalize pixel values to the range [0, 1] or standardize to have
zero mean and unit variance.
3. **Data Augmentation**:
- Apply data augmentation techniques such as cropping, horizontal and vertical flips,
rescaling, shear, zoom, and rotation to increase dataset variability.
- Utilize libraries like TensorFlow’s ‘ImageDataGenerator‘ or PyTorch’s ‘transforms‘ for
implementation.
#### 2. Model Selection and Architecture
1. **Transfer Learning**:
- Leverage pre-trained convolutional neural networks (CNNs) such as VGG16, VGG19,
MobileNet, Xception, ResNet50, and InceptionV3.
- Fine-tune these pre-trained models on the Butterfly Image Classification dataset to
utilize their learned features.
2. **Model Architecture**:
- Start with InceptionV3, known for high accuracy in similar tasks.
- Explore other models like ResNet, DenseNet, EfficientNet, and Vision Transformers.
- Consider combining CNNs with Transformer-based architectures to capture both local and
global features.
3. **Ensemble Methods**:
- Implement ensemble methods by combining predictions from multiple models to improve
overall classification performance.
- Use techniques like bagging or stacking to enhance model robustness.
#### 3. Model Training
1. **Dataset Splitting**:
- Split the dataset into training, validation, and testing sets (e.g., 80% training and
20% validation).
2. **Training Configuration**:
- Use categorical cross-entropy as the loss function.
- Select optimizers like SGD or RMSprop.
- Set up callbacks for early stopping and learning rate reduction on plateau to prevent
overfitting.
3. **Hyperparameter Tuning**:
- Perform hyperparameter tuning using libraries like PyCaret or Optuna to find the
optimal configuration.
- Utilize techniques like grid search or random search for hyperparameter optimization.
#### 4. Model Evaluation and Validation
1. **Performance Metrics**:
- Regularly monitor accuracy as the primary performance metric.
- Additionally, track precision, recall, and F1-score to ensure balanced performance
across all classes.
2. **Cross-Validation**:
- Employ k-fold cross-validation to assess the model’s robustness and generalization
ability.
3. **Handling Class Imbalance**:
- Address class imbalance through weighted loss functions or oversampling techniques to
ensure fair performance across all categories.
#### 5. Interpretability and Debugging
1. **Model Interpretability**:
- Use interpretability methods like Grad-CAM or LIME to visualize which parts of the
images influence the model’s predictions.
- Ensure the model focuses on relevant biological features of butterflies.
#### 6. Reproducibility and Documentation
1. **Reproducibility**:
- Set random seeds for all operations to ensure reproducibility of results.
30Preprint. Underreview.
- Document the entire process, including data splits, model configurations, and training
parameters.
2. **Model Deployment**:
- Prepare the model for deployment by exporting it in a suitable format (e.g., TensorFlow
SavedModel, ONNX).
- Set up an inference engine and deployment endpoint to make predictions on new data.
3. **Demonstration**:
- Provide a demonstration of the model’s capabilities by creating a user-friendly
interface or visualization to showcase its performance.
By following this comprehensive plan, AI agents can develop a robust and accurate neural
network model tailored for classifying butterfly images in the given dataset.
Plan#2
## End-to-End Actionable Plan for Butterfly Image Classification
### 1. Data Collection
- **Dataset Name**: Butterfly Image Classification
- **Source**: User-uploaded
- **Description**: A dataset for classifying images of butterflies into their respective
categories.
- **Label Information**: Available in ‘labels.csv‘ file.
### 2. Data Preprocessing
- **Image Preprocessing**:
- **Resize Images**: Resize all images to a consistent size (e.g., 224x224 pixels) to
match the input size required by pre-trained models.
- **Normalization**: Normalize pixel values to the range [0, 1] or use mean subtraction
based on the pre-trained model’s requirements.
- **Handling High-Resolution and Illumination Variations**: Apply techniques to
standardize illumination and handle high-resolution images.
### 3. Data Augmentation
- **Techniques**:
- **Cropping**: Randomly crop sections of the images.
- **Flipping**: Apply horizontal and vertical flips.
- **Rescaling**: Rescale images by a factor.
- **Shearing**: Apply shear transformations.
- **Zooming**: Apply random zoom.
- **Rotation**: Rotate images by random angles.
- **Libraries**: Use TensorFlow’s ‘ImageDataGenerator‘ or PyTorch’s ‘transforms‘.
### 4. Dataset Splitting
- **Training Set**: 80% of the dataset
- **Validation Set**: 20% of the dataset
### 5. Model Selection and Architecture
- **Transfer Learning**:
- **Pre-trained Models**: Utilize models such as VGG16, VGG19, MobileNet, Xception,
ResNet50, and InceptionV3.
- **Fine-tuning**: Fine-tune these models on the Butterfly Image Classification dataset.
- **Model Architecture**:
- **Primary Model**: Start with InceptionV3 due to its high performance in similar tasks.
- **Alternative Models**: Evaluate ResNet, DenseNet, EfficientNet, and Vision
Transformers.
- **Combined Architectures**: Explore combining CNNs with Transformer-based architectures
to capture both local and global features.
### 6. Handling Class Imbalance
- **Techniques**:
- **Weighted Loss Functions**: Assign higher weights to underrepresented classes.
- **Oversampling**: Increase the number of samples for underrepresented classes.
### 7. Model Training
- **Loss Function**: Use categorical cross-entropy.
- **Optimizers**: Start with SGD or RMSprop.
- **Hyperparameter Tuning**:
- **Libraries**: Use PyCaret or Optuna for hyperparameter optimization.
- **Parameters**: Tune learning rate, batch size, number of epochs, etc.
- **Cross-Validation**: Use k-fold cross-validation to ensure model robustness and
generalization.
### 8. Performance Evaluation
- **Metrics**:
- **Accuracy**: Primary metric for evaluation.
31Preprint. Underreview.
- **Precision, Recall, F1-Score**: Monitor these metrics to ensure balanced performance
across classes.
- **Evaluation Techniques**:
- **Confusion Matrix**: Visualize to understand misclassifications.
- **ROC Curve**: Evaluate model performance across different thresholds.
### 9. Ensemble Methods and Knowledge Distillation
- **Ensemble Methods**: Combine predictions from multiple models (e.g., patch-wise and
image-wise classifiers).
- **Knowledge Distillation**: Transfer knowledge from larger teacher models to smaller
student models to create compact yet powerful models.
### 10. Interpretability and Debugging
- **Techniques**:
- **Grad-CAM**: Visualize which parts of the images influence the model’s predictions.
- **LIME**: Provide explanations for individual predictions.
- **Purpose**: Ensure the model focuses on relevant biological features and understand
model behavior.
### 11. Reproducibility
- **Random Seeds**: Set random seeds for all processes to ensure reproducibility.
- **Documentation**: Document data splits, model configurations, and training parameters.
### 12. Deployment
- **Target Device**: Specify the target device for deployment (e.g., cloud, edge device).
- **Deployment Endpoint**: Define the endpoint for model deployment.
- **Inference Engine**: Choose an appropriate inference engine (e.g., TensorFlow Serving,
TorchServe).
- **Demonstration**: Prepare a demonstration of the model’s capabilities and performance.
By following this comprehensive plan, AI agents can effectively build, train, and deploy a
highly accurate neural network model tailored for classifying butterfly images in the
specified dataset.
Plan#3
### End-to-End Actionable Plan for Butterfly Image Classification using Neural Networks
#### 1. **Project Initialization**
**Objective**: Build a highly accurate image classification model for the Butterfly Image
Classification dataset in the biology domain.
**User Expertise**: Medium
---
#### 2. **Data Collection and Preprocessing**
**Dataset Source**: User-uploaded Butterfly Image Classification dataset with label
information in ‘labels.csv‘.
**Steps**:
1. **Load Dataset**:
- Load images and labels from the specified dataset and ‘labels.csv‘.
2. **Data Cleaning**:
- Ensure all images are correctly labeled.
- Remove any corrupted or unusable images.
3. **Data Preprocessing**:
- Resize images to a consistent size (e.g., 224x224 pixels).
- Normalize pixel values to the range [0, 1].
4. **Data Augmentation**:
- Apply augmentation techniques such as cropping, horizontal and vertical flips,
rescaling, shear, zoom, and rotation to increase dataset size and variability.
- Use libraries like TensorFlow’s ‘ImageDataGenerator‘ or PyTorch’s ‘transforms‘.
---
#### 3. **Model Development**
**Model Type**: Neural Networks
**Steps**:
32Preprint. Underreview.
1. **Leverage Transfer Learning**:
- Use pre-trained CNNs such as VGG16, VGG19, MobileNet, Xception, ResNet50, and
InceptionV3.
- Fine-tune these models on the Butterfly Image Classification dataset.
2. **Model Selection and Architecture**:
- Start with InceptionV3 due to its high accuracy in similar tasks.
- Experiment with other models like ResNet, DenseNet, EfficientNet, and Vision
Transformers.
- Consider combining CNNs with Transformer-based architectures for improved performance.
3. **Ensemble Methods and Knowledge Distillation**:
- Implement ensemble methods by combining patch-wise and image-wise classifiers.
- Use knowledge distillation to transfer knowledge from larger models to smaller, more
efficient models.
---
#### 4. **Training and Evaluation**
**Steps**:
1. **Dataset Splitting**:
- Split the dataset into training (80%), validation (10%), and testing (10%) sets.
2. **Model Training**:
- Use categorical cross-entropy as the loss function.
- Utilize optimizers such as SGD or RMSprop.
- Implement early stopping and learning rate reduction on plateau to prevent overfitting.
3. **Performance Monitoring**:
- Regularly monitor metrics such as accuracy, precision, recall, and F1-score.
- Use validation data to fine-tune hyperparameters.
4. **Handling Class Imbalance**:
- Address class imbalance using weighted loss functions or oversampling techniques.
5. **Hyperparameter Tuning and Cross-Validation**:
- Perform hyperparameter tuning with libraries like PyCaret or Optuna.
- Use cross-validation to ensure model robustness and generalization.
---
#### 5. **Model Interpretability and Debugging**
**Steps**:
1. **Interpretability**:
- Use Grad-CAM or LIME to visualize important image regions influencing model predictions
.
- Ensure the model focuses on relevant biological features.
2. **Debugging**:
- Analyze misclassified images to understand model limitations.
- Iterate on model improvements based on these insights.
---
#### 6. **Reproducibility and Documentation**
**Steps**:
1. **Reproducibility**:
- Set random seeds for all processes to ensure reproducibility.
- Document all steps, including data splits, model configurations, and training
parameters.
2. **Version Control**:
- Use version control systems (e.g., Git) to track changes and collaborate efficiently.
---
#### 7. **Deployment and Inference**
**Steps**:
1. **Model Export**:
- Export the trained model in a suitable format (e.g., TensorFlow SavedModel, ONNX).
33Preprint. Underreview.
2. **Deployment**:
- Choose a deployment platform (e.g., AWS SageMaker, Google AI Platform).
- Set up the deployment endpoint for real-time inference.
3. **Inference Engine**:
- Optimize the model for the target device (e.g., mobile, edge devices) if necessary.
- Implement efficient inference pipelines to handle incoming image data.
---
#### 8. **Demonstration and User Feedback**
**Steps**:
1. **Demonstration**:
- Create a demo application to showcase the model’s capabilities.
- Provide a user-friendly interface for uploading images and viewing classification
results.
2. **User Feedback**:
- Collect feedback from users to identify areas for improvement.
- Implement iterative updates based on user insights.
---
By following this detailed plan, AI agents can build a robust and accurate neural network
model for classifying butterfly images in the biology domain, ensuring high
performance and user satisfaction.
C.3 PLANDECOMPOSITION
ExampleResultsfromPlanDecomposition
DataAgent
### Summary of the Plan for Fashion Image Classification Using PyTorch
#### 1. Dataset Retrieval and Collection
- **Retrieve Dataset**: Load the image paths and corresponding labels from the provided ‘
train_labels.csv‘ and ‘test_labels.csv‘ files. Ensure the dataset includes images of
four clothing categories.
#### 2. Checking Dataset Types, Shapes, and Properties
- **Data Types**: Verify that the images are in the correct format (e.g., JPEG, PNG) and
the labels are correctly mapped to the images.
- **Shapes**: Check the dimensions of the images. If images are in grayscale, convert them
to RGB by replicating the single channel across three channels.
- **Properties**: Ensure that the dataset is balanced across the four clothing categories
and that there are no missing or corrupted files.
#### 3. Preprocessing the Dataset
- **Normalization**: Scale pixel values to the range [0, 1].
- **Resizing**: Resize all images to a consistent size (e.g., 224x224 pixels) suitable for
model input.
- **Splitting**: Split the training data into training and validation sets (e.g., 80%
training, 20% validation) to monitor the model’s performance and prevent overfitting.
#### 4. Efficient Data Augmentation
- **Techniques**: Apply data augmentation techniques to increase the diversity and
robustness of the training data. Techniques include:
- Random horizontal flip
- Normalization
- Random cropping
- Random erasing
- Rotating
- Shifting
- Zooming
- **Pipelines**:
- **Pipeline 1**: Random horizontal flip and normalization.
- **Pipeline 2**: Random horizontal flip, normalization, random cropping, and random
erasing.
#### 5. Extracting and Understanding Underlying Characteristics
- **Class Distribution**: Analyze the distribution of classes to ensure a balanced dataset.
- **Image Quality**: Check for variations in image quality and resolution.
34Preprint. Underreview.
- **Data Insights**: Identify any patterns or anomalies in the data that could affect model
performance.
- **Performance Metrics**: Use metrics such as accuracy and confusion matrix to evaluate
the model’s performance and understand misclassification patterns.
By following these steps, another data scientist should be able to reproduce the results
and build a robust image classification model that meets the specified accuracy
requirements and provides insightful performance analysis.
ModelAgent
### Summary of the Plan for Fashion Image Classification Using PyTorch
#### User’s Requirements
The user needs a classification model that categorizes images into one of four clothing
categories. The model should achieve at least 85% accuracy on the test set and be
implemented using PyTorch. Data augmentation techniques and a confusion matrix for
evaluation are also required.
### Project Plan Overview
#### 1. Data Preparation
- **Load Dataset**: Load image paths and labels from ‘train_labels.csv‘ and ‘test_labels.
csv‘.
- **Convert Grayscale to RGB**: Convert grayscale images to RGB.
- **Preprocessing**: Normalize pixel values to [0, 1] and resize images to 224x224 pixels.
- **Split Dataset**: Split training data into training (80%) and validation (20%) sets.
#### 2. Data Augmentation
- **Techniques**: Apply random horizontal flip, normalization, random cropping, random
erasing, rotating, shifting, and zooming.
- **Pipelines**: Develop two preprocessing pipelines with different augmentation techniques
.
#### 3. Model Architecture
- **Simple CNN**: Start with a simple CNN model.
- **Advanced Models**: Consider ResNet18, EfficientNet, or Vision Transformers (ViTs).
- **Hybrid Models**: Explore hybrid models combining CNNs and ViTs.
#### 4. Training
- **Optimizer and Loss Function**: Use SGD or Adam optimizer and Cross-Entropy Loss.
- **Epochs and Early Stopping**: Train for up to 150 epochs with early stopping.
- **Monitoring**: Monitor validation accuracy and loss curves.
#### 5. Evaluation
- **Accuracy**: Ensure at least 85% accuracy on the test set.
- **Confusion Matrix**: Generate a confusion matrix.
- **Top-1 Accuracy**: Calculate top-1 accuracy.
#### 6. Implementation
- **Framework**: Use PyTorch and torchvision.transforms.v2.
- **Resources**: Utilize resources from Kaggle and PapersWithCode.
- **Deployment**: Prepare the model for deployment if needed.
#### 7. Visualization and Reporting
- **Confusion Matrix**: Visualize the confusion matrix.
- **Performance Metrics**: Report accuracy and other metrics.
- **Documentation**: Document the entire process.
### Detailed Steps for Data Manipulation and Analysis
#### 1. Dataset Retrieval and Collection
- **Retrieve Dataset**: Load image paths and labels from ‘train_labels.csv‘ and ‘
test_labels.csv‘.
#### 2. Checking Dataset Types, Shapes, and Properties
- **Data Types**: Verify image formats and label mappings.
- **Shapes**: Check and convert image dimensions to RGB if needed.
- **Properties**: Ensure balanced dataset and data integrity.
#### 3. Preprocessing the Dataset
- **Normalization**: Scale pixel values to [0, 1].
- **Resizing**: Resize images to 224x224 pixels.
- **Splitting**: Split data into training and validation sets.
#### 4. Efficient Data Augmentation
- **Techniques**: Apply various augmentation techniques.
- **Pipelines**: Create two augmentation pipelines.
35Preprint. Underreview.
#### 5. Extracting and Understanding Underlying Characteristics
- **Class Distribution**: Analyze class distribution.
- **Image Quality**: Assess variations in image quality.
- **Data Insights**: Identify patterns or anomalies.
- **Performance Metrics**: Evaluate model performance using accuracy and confusion matrix.
### Expected Outcomes and Results
- **Quantitative**: Balanced dataset, consistent image size, normalized pixel values,
augmented data.
- **Qualitative**: Data integrity, improved model generalization, insightful analysis.
### Responsibilities Fulfillment
#### 1. How to Retrieve or Find High-Performance Model(s)?
- **Initial Models**: Start with simple CNN architectures.
- **Advanced Models**: Experiment with ResNet18, EfficientNet, and Vision Transformers.
- **Hybrid Models**: Explore combinations of CNNs and ViTs.
- **Resources**: Utilize code repositories from Kaggle and PapersWithCode for
implementation and fine-tuning.
#### 2. How to Optimize the Hyperparameters of the Retrieved Models?
- **Optimizers**: Use SGD with a learning rate of 0.015 or Adam optimizer.
- **Hyperparameter Tuning**: Experiment with different learning rates, batch sizes, and
augmentation techniques.
- **Early Stopping**: Implement early stopping based on validation loss to prevent
overfitting.
- **Monitoring**: Continuously monitor validation accuracy and loss curves.
#### 3. How to Extract and Understand the Underlying Characteristics of the Dataset(s)?
- **Class Distribution**: Use ‘pandas‘ to analyze the distribution of classes.
- **Image Quality**: Assess image quality and resolution using ‘PIL‘ or ‘OpenCV‘.
- **Data Insights**: Perform exploratory data analysis (EDA) to identify patterns or
anomalies.
- **Performance Metrics**: Use ‘sklearn.metrics‘ to evaluate model performance and generate
confusion matrices.
#### 4. How to Select the Top-k Models or Algorithms Based on the Given Plans?
- **Evaluation Metrics**: Use accuracy and confusion matrix to evaluate models.
- **Top-k Selection**: Rank models based on their performance metrics and select the top-k
models that meet or exceed the 85% accuracy requirement.
- **Documentation**: Document the performance of each model, including accuracy, confusion
matrix, and any other relevant metrics.
By following this comprehensive plan, you will be able to retrieve high-performance models,
optimize their hyperparameters, understand the dataset’s characteristics, and select
the top-k models based on their performance.
C.4 AGENT-SPECIFICPLANEXECUTIONOUTCOMES
ExampleResultsfromPrompting-BasedPlanExecutionbyEachAgent
DataAgent
### Detailed Steps for Data Manipulation and Analysis
#### 1. Dataset Retrieval and Collection
**Retrieve Dataset**:
- **Source**: The dataset is available at ‘_experiments/datasets/shopee‘.
- **Structure**: The directory structure includes ‘train‘, ‘test‘, ‘train_labels.csv‘, and
‘test_labels.csv‘.
- **Loading Data**: Load the image paths and corresponding labels from ‘train_labels.csv‘
and ‘test_labels.csv‘. This can be done using libraries like ‘pandas‘ to read the CSV
files and ‘os‘ to navigate through the directories.
**Why**: This step ensures that we have access to the correct dataset and that the paths
and labels are correctly mapped for further processing.
#### 2. Checking Dataset Types, Shapes, and Properties
**Data Types**:
- **Image Format**: Verify that images are in the correct format (JPEG, PNG). This can be
done using libraries like ‘PIL‘ (Python Imaging Library).
- **Labels**: Ensure labels are correctly mapped to images by checking the contents of the
CSV files.
36Preprint. Underreview.
**Shapes**:
- **Image Dimensions**: Check the dimensions of the images. If images are in grayscale,
convert them to RGB by replicating the single channel across three channels using
libraries like ‘PIL‘ or ‘OpenCV‘.
**Properties**:
- **Balanced Dataset**: Ensure the dataset is balanced across the four clothing categories
by analyzing the label distribution using ‘pandas‘.
- **Integrity Check**: Verify that there are no missing or corrupted files by attempting to
open each image file.
**Why**: Ensuring correct data types, shapes, and properties is crucial for accurate model
training and prevents errors during preprocessing and augmentation.
#### 3. Preprocessing the Dataset
**Normalization**:
- **Scale Pixel Values**: Normalize pixel values to the range [0, 1] using libraries like ‘
PIL‘ or ‘torchvision.transforms‘.
**Resizing**:
- **Consistent Size**: Resize all images to a consistent size (e.g., 224x224 pixels)
suitable for model input using ‘PIL‘ or ‘torchvision.transforms‘.
**Splitting**:
- **Train-Validation Split**: Split the training data into training and validation sets (e.
g., 80% training, 20% validation) using ‘sklearn.model_selection.train_test_split‘.
**Why**: Normalization ensures that the pixel values are on a common scale, which helps in
faster convergence during training. Resizing ensures that all images have the same
dimensions, which is a requirement for most deep learning models. Splitting the data
helps in monitoring the model’s performance and prevents overfitting.
#### 4. Efficient Data Augmentation
**Techniques**:
- **Random Horizontal Flip**: Randomly flip images horizontally to increase diversity.
- **Normalization**: Normalize images to have zero mean and unit variance.
- **Random Cropping**: Randomly crop images to introduce variability.
- **Random Erasing**: Randomly erase parts of images to make the model robust to occlusions
.
- **Rotating**: Rotate images randomly to make the model invariant to orientation.
- **Shifting**: Shift images randomly to simulate different viewpoints.
- **Zooming**: Zoom in and out of images to introduce scale variability.
**Pipelines**:
- **Pipeline 1**: Apply random horizontal flip and normalization.
- **Pipeline 2**: Apply random horizontal flip, normalization, random cropping, and random
erasing.
**Why**: Data augmentation increases the diversity and robustness of the training data,
which helps in improving the generalization of the model.
#### 5. Extracting and Understanding Underlying Characteristics
**Class Distribution**:
- **Analyze Distribution**: Use ‘pandas‘ to analyze the distribution of classes to ensure a
balanced dataset.
**Image Quality**:
- **Check Variations**: Assess variations in image quality and resolution using ‘PIL‘ or ‘
OpenCV‘.
**Data Insights**:
- **Identify Patterns**: Use exploratory data analysis (EDA) techniques to identify any
patterns or anomalies in the data that could affect model performance.
**Performance Metrics**:
- **Evaluate Model**: Use metrics such as accuracy and confusion matrix to evaluate the
model’s performance and understand misclassification patterns. This can be done using
‘sklearn.metrics‘.
**Why**: Understanding the underlying characteristics of the data helps in identifying
potential issues that could affect model performance. Analyzing class distribution
ensures that the dataset is balanced, which is crucial for training a fair and
unbiased model.
### Expected Outcomes and Results
37Preprint. Underreview.
**Quantitative**:
- **Balanced Dataset**: A dataset with an equal number of images for each clothing category
.
- **Consistent Image Size**: All images resized to 224x224 pixels.
- **Normalized Pixel Values**: Pixel values scaled to the range [0, 1].
- **Augmented Data**: Increased dataset diversity through augmentation techniques.
**Qualitative**:
- **Data Integrity**: No missing or corrupted files.
- **Improved Model Generalization**: Enhanced model robustness due to diverse and augmented
training data.
- **Insightful Analysis**: Clear understanding of class distribution, image quality, and
potential data patterns or anomalies.
By following these detailed steps, another data scientist should be able to reproduce the
results and build a robust image classification model that meets the specified
accuracy requirements and provides insightful performance analysis.
ModelAgent
### Detailed Steps for Modeling and Optimization
#### 1. Retrieving High-Performance Models
**Initial Models:**
- **Simple CNN**: Start with a basic Convolutional Neural Network (CNN) architecture. This
model will serve as a baseline to understand the dataset and the task.
**Advanced Models:**
- **ResNet18**: A well-known deep learning model that has shown excellent performance in
image classification tasks. It uses residual connections to ease the training of
deeper networks.
- **EfficientNet**: Known for its efficiency in terms of parameter count and FLOPs while
maintaining high accuracy. EfficientNet-B0 can be a good starting point.
- **Vision Transformers (ViTs)**: These models have gained popularity for their performance
in image classification tasks. A small variant like ViT-Small can be considered.
**Hybrid Models:**
- **CNN + ViT**: Combining the feature extraction capabilities of CNNs with the global
context understanding of ViTs. This hybrid approach can potentially leverage the
strengths of both architectures.
#### 2. Optimizing Hyperparameters
**Optimizers:**
- **SGD**: Stochastic Gradient Descent with a learning rate of 0.015.
- **Adam**: Adaptive Moment Estimation with a learning rate of 0.001.
**Hyperparameter Tuning:**
- **Learning Rate**: Experiment with learning rates (0.001, 0.005, 0.01, 0.015) to find the
optimal rate for convergence.
- **Batch Size**: Test batch sizes (16, 32, 64) to balance between memory usage and
training speed.
- **Data Augmentation Techniques**: Apply different augmentation techniques and observe
their impact on validation accuracy.
- **Number of Epochs**: Train for up to 150 epochs with early stopping based on validation
loss to prevent overfitting.
- **Weight Decay**: Regularization parameter for SGD, set to 0.0005.
**Optimal Values:**
- **Learning Rate**: 0.001 for Adam, 0.015 for SGD.
- **Batch Size**: 32.
- **Weight Decay**: 0.0005.
- **Number of Epochs**: Up to 150 with early stopping.
#### 3. Extracting and Understanding Characteristics
**Computation Complexity:**
- **Number of Parameters**: Calculate the total number of trainable parameters in each
model.
- **FLOPs (Floating Point Operations per Second)**: Measure the computational complexity of
each model.
**Memory Usage:**
- **Model Size**: Determine the size of the model in memory (MB).
- **Training Memory Usage**: Monitor GPU memory usage during training.
38Preprint. Underreview.
**Inference Latency:**
- **Inference Speed**: Measure the time taken for a single forward pass (in milliseconds).
**Why Each Step is Essential:**
- **Model Selection**: Choosing a variety of models ensures that we explore different
architectures and their strengths.
- **Hyperparameter Tuning**: Fine-tuning hyperparameters is crucial for achieving optimal
performance and ensuring the model generalizes well.
- **Characteristic Analysis**: Understanding the computational and memory requirements
helps in selecting models that are not only accurate but also efficient and deployable
.
### Candidate Models and Expected Performance
**1. ResNet18**
- **Accuracy**: Expected to achieve around 88% on the test set.
- **Number of Parameters**: Approximately 11.7 million.
- **FLOPs**: Around 1.8 billion.
- **Model Size**: Approximately 44 MB.
- **Training Time**: Approximately 2 hours on a single GPU.
- **Inference Speed**: Around 10 milliseconds per image.
**2. EfficientNet-B0**
- **Accuracy**: Expected to achieve around 87% on the test set.
- **Number of Parameters**: Approximately 5.3 million.
- **FLOPs**: Around 390 million.
- **Model Size**: Approximately 20 MB.
- **Training Time**: Approximately 3 hours on a single GPU.
- **Inference Speed**: Around 8 milliseconds per image.
**3. Vision Transformer (ViT-Small)**
- **Accuracy**: Expected to achieve around 86% on the test set.
- **Number of Parameters**: Approximately 22 million.
- **FLOPs**: Around 4.6 billion.
- **Model Size**: Approximately 85 MB.
- **Training Time**: Approximately 4 hours on a single GPU.
- **Inference Speed**: Around 15 milliseconds per image.
By following these detailed steps and focusing on the modeling and optimization parts, you
will be able to identify and select the top-3 high-performance models for the given
image classification task.
D SHOWCASE OF GENERATED PIPELINES
GeneratedPipelineunderConstraint-FreeSettingbyAutoML-Agent
User Prompt: Please provide a classification model that categorizes images into one of four
clothingcategories. Theimagepath, alongwithitslabelinformation, canbefoundinthefiles
train labels.csvandtest labels.csv.
OutputPipeline:
import os
import random
import time
import pandas as pd
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Dataset
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import gradio as gr
# Define constants
SEED = 42
DATASET_PATH = "_experiments/datasets/shopee"
TRAIN_LABELS_CSV = os.path.join(DATASET_PATH, "train_labels.csv")
TEST_LABELS_CSV = os.path.join(DATASET_PATH, "test_labels.csv")
TRAIN_DIR = os.path.join(DATASET_PATH)
39Preprint. Underreview.
TEST_DIR = os.path.join(DATASET_PATH)
MODEL_SAVE_PATH = "./agent_workspace/trained_models/resnet50_finetuned.pth"
# Set seeds for reproducibility
random.seed(SEED)
torch.manual_seed(SEED)
np.random.seed(SEED)
# Define device for model operations
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Custom dataset class
class ShopeeDataset(Dataset):
def __init__(self, csv_file, root_dir, transform=None):
self.labels_df = pd.read_csv(csv_file)
self.root_dir = root_dir
self.transform = transform
def __len__(self):
return len(self.labels_df)
def __getitem__(self, idx):
img_name = os.path.join(self.root_dir, self.labels_df.iloc[idx, 0])
image = Image.open(img_name).convert("RGB")
label = self.labels_df.iloc[idx, 1]
if self.transform:
image = self.transform(image)
return image, label
# Data preprocessing and feature engineering
def preprocess_data():
transform = transforms.Compose([
transforms.Resize((28, 28)),
transforms.ToTensor(),
transforms.Normalize((0.5,), (0.5,))
])
train_dataset = ShopeeDataset(csv_file=TRAIN_LABELS_CSV, root_dir=TRAIN_DIR, transform=
transform)
test_dataset = ShopeeDataset(csv_file=TEST_LABELS_CSV, root_dir=TEST_DIR, transform=
transform)
train_size = int(0.7 * len(train_dataset))
valid_size = int(0.2 * len(train_dataset))
test_size = len(train_dataset) - train_size - valid_size
train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(train_dataset
, [train_size, valid_size, test_size])
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
return train_loader, valid_loader, test_loader
# Model training
def train_model(model, train_loader, valid_loader):
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.00001)
model.to(device)
best_accuracy = 0.0
for epoch in range(100):
model.train()
running_loss = 0.0
for inputs, labels in train_loader:
inputs, labels = inputs.to(device), labels.to(device)
optimizer.zero_grad()
outputs = model(inputs)
loss = criterion(outputs, labels)
loss.backward()
optimizer.step()
running_loss += loss.item() * inputs.size(0)
40Preprint. Underreview.
epoch_loss = running_loss / len(train_loader.dataset)
model.eval()
correct = 0
total = 0
with torch.no_grad():
for inputs, labels in valid_loader:
inputs, labels = inputs.to(device), labels.to(device)
outputs = model(inputs)
_, predicted = torch.max(outputs, 1)
total += labels.size(0)
correct += (predicted == labels).sum().item()
accuracy = correct / total
if accuracy > best_accuracy:
best_accuracy = accuracy
torch.save(model.state_dict(), MODEL_SAVE_PATH)
print(f"Epoch {epoch+1}/{100}, Loss: {epoch_loss:.4f}, Validation Accuracy: {accuracy
:.4f}")
model.load_state_dict(torch.load(MODEL_SAVE_PATH))
return model
# Model evaluation
def evaluate_model(model, test_loader):
model.eval()
y_true = []
y_pred = []
with torch.no_grad():
for inputs, labels in test_loader:
inputs, labels = inputs.to(device), labels.to(device)
outputs = model(inputs)
_, predicted = torch.max(outputs, 1)
y_true.extend(labels.cpu().numpy())
y_pred.extend(predicted.cpu().numpy())
performance_scores = {
’Accuracy’: accuracy_score(y_true, y_pred),
’Precision’: precision_score(y_true, y_pred, average=’weighted’),
’Recall’: recall_score(y_true, y_pred, average=’weighted’),
’F1’: f1_score(y_true, y_pred, average=’weighted’)
}
model_size = os.path.getsize(MODEL_SAVE_PATH) / (1024 * 1024) # in MB
start_time = time.time()
for _ in range(100):
_ = model(inputs)
inference_time = (time.time() - start_time) / 100 # average inference time
complexity_scores = {
’Model Size (MB)’: model_size,
’Inference Time (ms)’: inference_time * 1000
}
return performance_scores, complexity_scores
# Prepare model for deployment
def prepare_model_for_deployment(model):
model.eval()
scripted_model = torch.jit.script(model)
scripted_model.save(MODEL_SAVE_PATH.replace(".pth", "_scripted.pt"))
return scripted_model
# Deploy model using Gradio
def deploy_model(model):
def predict(image):
image = transforms.Compose([
transforms.Resize((28, 28)),
transforms.ToTensor(),
transforms.Normalize((0.5,), (0.5,))
])(image).unsqueeze(0).to(device)
with torch.no_grad():
outputs = model(image)
_, predicted = torch.max(outputs, 1)
return predicted.item()
41Preprint. Underreview.
interface = gr.Interface(fn=predict, inputs=gr.Image(type="pil"), outputs="label")
url_endpoint = interface.launch(share=True)
return url_endpoint
# The main function to orchestrate the data loading, data preprocessing, feature
engineering, model training, model preparation, model deployment, and model evaluation
def main():
# Step 1: Retrieve or load dataset
train_loader, valid_loader, test_loader = preprocess_data()
# Step 2: Define model
model = models.resnet50(pretrained=True)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 4) # Assuming 4 classes
# Step 3: Train model
model = train_model(model, train_loader, valid_loader)
# Step 4: Evaluate model
model_performance, model_complexity = evaluate_model(model, test_loader)
# Step 5: Prepare model for deployment
deployable_model = prepare_model_for_deployment(model)
# Step 6: Deploy model
url_endpoint = deploy_model(deployable_model)
return train_loader, valid_loader, test_loader, model, deployable_model, url_endpoint,
model_performance, model_complexity
if __name__ == "__main__":
train_loader, valid_loader, test_loader, model, deployable_model, url_endpoint,
model_performance, model_complexity = main()
print("Model Performance on Test Set:", model_performance)
print("Model Complexity:", model_complexity)
print("Deployment URL:", url_endpoint)
GeneratedPipelineforConstraint-AwareSettingbyAutoML-Agent
User Prompt: Please provide a classification model that categorizes images into one of four
clothingcategories. Theimagepath, alongwithitslabelinformation, canbefoundinthefiles
train labels.csv and test labels.csv. The model should achieve at least 85% accuracy on the
test set and be implemented using PyTorch. Additionally, please include data augmentation
techniquesandaconfusionmatrixintheevaluation.
OutputPipeline:
import os
import random
import time
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Dataset
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import gradio as gr
# Set random seeds for reproducibility
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)
np.random.seed(SEED)
# Define device for model operations
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
42Preprint. Underreview.
DATASET_PATH = "_experiments/datasets/shopee" # path for saving and loading dataset(s)
# Custom Dataset class
class ShopeeDataset(Dataset):
def __init__(self, dataframe, transform=None):
self.dataframe = dataframe
self.transform = transform
def __len__(self):
return len(self.dataframe)
def __getitem__(self, idx):
img_path = self.dataframe.iloc[idx][’image’]
label = self.dataframe.iloc[idx][’label’]
image = Image.open(img_path).convert(’RGB’)
if self.transform:
image = self.transform(image)
return image, label
# Data preprocessing and feature engineering
def preprocess_data():
train_labels = pd.read_csv(os.path.join(DATASET_PATH, ’train_labels.csv’))
test_labels = pd.read_csv(os.path.join(DATASET_PATH, ’test_labels.csv’))
train_labels[’image’] = train_labels[’image’].apply(lambda x: os.path.join(DATASET_PATH,
x))
test_labels[’image’] = test_labels[’image’].apply(lambda x: os.path.join(DATASET_PATH, x
))
# Split the data
train_data, val_data = train_test_split(train_labels, test_size=0.2, stratify=
train_labels[’label’])
val_data, test_data = train_test_split(val_data, test_size=0.5, stratify=val_data[’label
’])
# Define transformations
train_transforms = transforms.Compose([
transforms.RandomResizedCrop(224),
transforms.RandomHorizontalFlip(),
transforms.RandomRotation(10),
transforms.ToTensor(),
transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
val_test_transforms = transforms.Compose([
transforms.Resize(256),
transforms.CenterCrop(224),
transforms.ToTensor(),
transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
# Load datasets
train_dataset = ShopeeDataset(train_data, transform=train_transforms)
val_dataset = ShopeeDataset(val_data, transform=val_test_transforms)
test_dataset = ShopeeDataset(test_data, transform=val_test_transforms)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
return train_loader, val_loader, test_loader
def train_model(model, train_loader, val_loader):
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)
num_epochs = 100
best_val_loss = float(’inf’)
early_stop_count = 0
early_stop_patience = 10
for epoch in range(num_epochs):
model.train()
running_loss = 0.0
for inputs, labels in train_loader:
inputs, labels = inputs.to(device), labels.to(device)
optimizer.zero_grad()
outputs = model(inputs)
loss = criterion(outputs, labels)
43Preprint. Underreview.
loss.backward()
optimizer.step()
running_loss += loss.item() * inputs.size(0)
epoch_loss = running_loss / len(train_loader.dataset)
# Validation
model.eval()
val_loss = 0.0
with torch.no_grad():
for inputs, labels in val_loader:
inputs, labels = inputs.to(device), labels.to(device)
outputs = model(inputs)
loss = criterion(outputs, labels)
val_loss += loss.item() * inputs.size(0)
val_loss = val_loss / len(val_loader.dataset)
print(f’Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {
val_loss:.4f}’)
# Early stopping
if val_loss < best_val_loss:
best_val_loss = val_loss
torch.save(model.state_dict(), ’./agent_workspace/trained_models/best_model.pth’)
early_stop_count = 0
else:
early_stop_count += 1
if early_stop_count >= early_stop_patience:
print("Early stopping")
break
return model
def evaluate_model(model, test_loader):
model.load_state_dict(torch.load(’./agent_workspace/trained_models/best_model.pth’))
model.eval()
all_preds = []
all_labels = []
with torch.no_grad():
for inputs, labels in test_loader:
inputs, labels = inputs.to(device), labels.to(device)
outputs = model(inputs)
_, preds = torch.max(outputs, 1)
all_preds.extend(preds.cpu().numpy())
all_labels.extend(labels.cpu().numpy())
accuracy = accuracy_score(all_labels, all_preds)
f1 = f1_score(all_labels, all_preds, average=’weighted’)
performance_scores = {
’ACC’: accuracy,
’F1’: f1
}
# Measure model complexity
start_time = time.time()
with torch.no_grad():
for inputs, _ in test_loader:
inputs = inputs.to(device)
_ = model(inputs)
inference_time = (time.time() - start_time) / len(test_loader.dataset)
model_size = os.path.getsize(’./agent_workspace/trained_models/best_model.pth’) / 1e6 #
in MB
complexity_scores = {
’Inference Time’: inference_time,
’Model Size (MB)’: model_size
}
# Generate confusion matrix
conf_matrix = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt=’d’, cmap=’Blues’)
plt.xlabel(’Predicted’)
plt.ylabel(’True’)
plt.title(’Confusion Matrix’)
44Preprint. Underreview.
plt.show()
return performance_scores, complexity_scores
def prepare_model_for_deployment(model):
# Assuming the model is already saved in the best_model.pth file
model.load_state_dict(torch.load(’./agent_workspace/trained_models/best_model.pth’))
model.eval()
return model
def deploy_model(model):
def predict(image):
transform = transforms.Compose([
transforms.Resize(256),
transforms.CenterCrop(224),
transforms.ToTensor(),
transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
image = transform(image).unsqueeze(0).to(device)
with torch.no_grad():
output = model(image)
_, pred = torch.max(output, 1)
return pred.item()
interface = gr.Interface(fn=predict, inputs=gr.Image(type="pil"), outputs="label")
url_endpoint = interface.launch(share=True)
return url_endpoint
# The main function to orchestrate the data loading, data preprocessing, feature
engineering, model training, model preparation, model deployment, and model evaluation
def main():
"""
Main function to execute the image classification pipeline.
"""
# Step 1. Retrieve or load a dataset from hub (if available) or user’s local storage (if
given)
train_loader, val_loader, test_loader = preprocess_data()
# Step 2. Define required model
model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 4)
model = model.to(device)
# Step 3. Train the retrieved/loaded model
model = train_model(model, train_loader, val_loader)
# Step 4. Evaluate the trained model
model_performance, model_complexity = evaluate_model(model, test_loader)
# Step 5. Prepare the model for deployment
deployable_model = prepare_model_for_deployment(model)
# Step 6. Deploy the model
url_endpoint = deploy_model(deployable_model)
return (
train_loader,
val_loader,
test_loader,
model,
deployable_model,
url_endpoint,
model_performance,
model_complexity,
)
if __name__ == "__main__":
train_loader, val_loader, test_loader, model, deployable_model, url_endpoint,
model_performance, model_complexity = main()
print("Model Performance on Test Set:", model_performance)
print("Model Complexity:", model_complexity)
print("Gradio URL Endpoint:", url_endpoint)
45Preprint. Underreview.
E DETAILED EXPERIMENTAL RESULTS
This section presents the detailed experimental results referenced in §4. Tables 5, 6, and 7 show
theaveragescoreswithstandarddeviations,computedfromfiveindependentruns. Tables8and9
reporttheresultsoftheablationandhyperparameterstudies,respectively. Finally,Table10reports
theaveragetimeandmoneyusedtogeneratethefinalcodeinasinglerun.
Table5: PerformancecomparisonwiththeSRmetric. Bestresultsarehighlightedinbold.
Method I Bm ua ttg ee rflC ylassi Sfi hc oa pti eo en ETe cx ot mC mlassifi Ec na tt aio iln T Ba ab nu al na arCla Sss oi ffi tc wa ati ro en Ta Cb ru al barRegr Ce rs osi pon T Sa mb ou kla errClu Ss tt ue dr ein ng t WTim eae t- hS ee rries EFo ler ce tc ra icst itin yg N Cod oe raClass Cifi ic tea st eio en r Avg.
Constraint-Free
GPT-3.5 (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.3 .20 70 4) (±0 0.1 .20 20 4) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.4 .20 20 4) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .05 57 2)
GPT-4 (±0 0.2 .40 40 7) (±0 0.6 .50 40 8) (±0 0.0 .00 00 0) (±0 0.4 .50 40 8) (±0 0.4 .50 40 8) (±0 0.4 .40 10 8) (±0 0.4 .50 40 8) (±0 0.6 .50 40 8) (±0 0.6 .40 10 8) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.4 .50 40 8) (±0 0.4 .50 40 8) (±0 0.3 .31 64 6)
DS-Agent (±0 0.4 .50 40 8) (±0 0.8 .40 40 7) (±0 0.0 .00 00 0) (±0 0.7 .40 40 7) (±0 0.8 .40 40 7) (±0 0.8 .20 70 4) (±0 0.0 .00 00 0) (±0 0.8 .40 40 7) (±0 0.9 .20 20 4) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.6 .50 40 8) (±0 0.6 .50 40 8) (±0 0.4 .25 87 1)
AutoML-Agent (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0)
Constraint-Aware
GPT-3.5 (±0 0.0 .00 00 0) (±0 0.0 .15 10 2) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .15 10 2) (±0 0.1 .15 30 7) (±0 0.1 .10 30 7) (±0 0.0 .15 10 2) (±0 0.1 .15 30 7) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .03 59 3)
GPT-4 (±0 0.1 .35 30 5) (±0 0.3 .45 80 7) (±0 0.2 .40 40 7) (±0 0.2 .40 40 7) (±0 0.1 .35 30 5) (±0 0.0 .00 00 0) (±0 0.6 .45 10 8) (±0 0.1 .20 20 4) (±0 0.4 .30 30 5) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.4 .30 30 5) (±0 0.5 .30 50 4) (±0 0.2 .22 61 6)
DS-Agent (±0 0.3 .40 10 1) (±0 0.3 .45 80 7) (±0 0.0 .00 00 0) (±0 0.2 .30 20 6) (±0 0.6 .30 30 5) (±0 0.6 .45 80 7) (±0 0.2 .40 40 7) (±0 0.2 .40 40 7) (±0 0.4 .25 70 4) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.1 .35 30 5) (±0 0.2 .30 20 6) (±0 0.4 .45 10 1) (±0 0.2 .36 08 6)
AutoML-Agent (±0 0.8 .10 10 2) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±0 0.7 .05 00 0) (±1 0.0 .00 00 0) (±1 0.0 .00 00 0) (±0 0.7 .05 00 0) (±0 0.7 .05 00 0) (±0 0.7 .05 00 0) (±0 0.9 .20 20 4) (±1 0.0 .00 00 0) (±0 0.7 .05 00 0) (±0 0.7 .05 00 0) (±0 0.8 .07 21 4)
Table6: PerformancecomparisonwiththeNPSmetric. Bestresultsarehighlightedinbold.
Method I Bm ua ttg ee rflC ylassi Sfi hc oa pti eo en ETe cx ot mC mlassifi Ec na tt aio iln T Ba ab nu al na arCla Sss oi ffi tc wa ati ro en Ta Cb ru al barRegr Ce rs osi pon T Sa mb ou kla errClu Ss tt ue dr ein ng t WTim eae t- hS ee rries EFo ler ce tc ra icst itin yg N Cod oe raClass Cifi ic tea st eio en r Avg.
Constraint-Free
HumanModels (±0 0.9 .03 01 2) (±0 0.9 .02 11 2) (±0 0.9 .03 05 0) (±0 0.6 .06 34 9) (±0 0.9 .07 06 0) (±0 0.6 .06 09 0) (±0 0.3 .02 08 0) (±0 0.4 .07 06 0) (±0 0.5 .01 03 0) (±0 0.7 .05 00 0) (±0 0.9 .07 00 0) (±0 0.9 .01 06 5) (±0 0.8 .01 01 5) (±0 0.7 .00 02 6) (±0 0.7 .05 04 5)
AutoGluon (±0 0.0 .01 04 0) (±0 0.9 .08 08 0) (±0 0.9 .08 07 0) (±0 0.8 .00 07 0) (±0 0.9 .08 00 0) (±0 0.5 .02 04 0) (±0 0.8 .07 05 0) (±0 0.4 .07 09 0) N/A N/A (±0 0.9 .09 02 0) (±0 0.9 .00 08 2) N/A N/A (±0 0.7 .05 05 0)
GPT-3.5 (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.5 .58 37 5) (±0 0.0 .29 04 9) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.4 .24 57 1) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .08 71 1)
GPT-4 (±0 0.1 .36 79 9) (±0 0.5 .44 95 9) (±0 0.0 .00 00 0) (±0 0.1 .29 96 5) (±0 0.3 .59 30 4) (±0 0.2 .28 65 1) (±0 0.3 .42 58 0) (±0 0.2 .27 40 7) (±0 0.4 .27 61 4) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.1 .38 46 3) (±0 0.1 .39 29 8) (±0 0.2 .21 57 7)
DS-Agent (±0 0.3 .40 15 9) (±0 0.7 .43 15 1) (±0 0.0 .00 00 0) (±0 0.5 .30 80 0) (±0 0.7 .46 26 8) (±0 0.5 .12 33 1) (±0 0.0 .00 00 0) (±0 0.4 .33 21 4) (±0 0.5 .00 04 1) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.4 .47 34 3) (±0 0.3 .39 63 0) (±0 0.3 .23 01 6)
AutoML-Agent (±0 0.9 .02 24 0) (±0 0.9 .04 45 3) (±0 0.9 .07 01 7) (±0 0.8 .00 03 6) (±0 0.9 .08 17 9) (±0 0.6 .16 74 4) (±0 0.8 .05 09 3) (±0 0.4 .06 25 0) (±0 0.5 .02 31 8) (±0 0.7 .06 20 1) (±0 0.9 .09 05 3) (±0 0.9 .03 97 3) (±0 0.8 .03 21 0) (±0 0.5 .09 12 5) (±0 0.8 .00 34 5)
Constraint-Aware
GPT-3.5 (±0 0.0 .00 00 0) (±0 0.1 .37 83 6) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.1 .49 36 9) (±0 0.4 .47 75 6) (±0 0.3 .45 86 8) (±0 0.0 .18 81 1) (±0 0.3 .33 08 9) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.1 .11 66 3)
GPT-4 (±0 0.1 .35 57 0) (±0 0.3 .43 65 3) (±0 0.1 .49 47 0) (±0 0.0 .16 44 4) (±0 0.1 .35 43 2) (±0 0.0 .00 00 0) (±0 0.7 .41 09 5) (±0 0.0 .29 01 4) (±0 0.4 .26 63 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.6 .33 57 6) (±0 0.5 .36 14 8) (±0 0.2 .24 31 4)
DS-Agent (±0 0.3 .43 50 1) (±0 0.3 .45 83 5) (±0 0.0 .00 00 0) (±0 0.2 .30 05 1) (±0 0.7 .47 36 4) (±0 0.3 .28 13 4) (±0 0.1 .37 83 6) (±0 0.1 .48 03 9) (±0 0.5 .00 05 1) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .29 03 9) (±0 0.3 .41 39 7) (±0 0.4 .30 63 9) (±0 0.2 .26 66 4)
AutoML-Agent (±0 0.9 .02 16 5) (±0 0.9 .07 22 2) (±0 0.9 .08 02 2) (±0 0.7 .09 26 7) (±0 0.9 .06 07 2) (±0 0.5 .17 43 2) (±0 0.8 .06 01 2) (±0 0.4 .07 23 0) (±0 0.5 .08 42 2) (±0 0.7 .06 19 0) (±0 0.9 .08 22 8) (±0 0.9 .07 08 1) (±0 0.8 .04 33 4) (±0 0.6 .03 32 7) (±0 0.8 .01 20 7)
Table7: PerformancecomparisonwiththeCSmetric. Bestresultsarehighlightedinbold.
Method I Bm ua ttg ee rflC ylassi Sfi hc oa pti eo en ETe cx ot mC mlassifi Ec na tt aio iln T Ba ab nu al na arCla Sss oi ffi tc wa ati ro en Ta Cb ru al barRegr Ce rs osi pon T Sa mb ou kla errClu Ss tt ue dr ein ng t WTim eae t- hS ee rries EFo ler ce tc ra icst itin yg N Cod oe raClass Cifi ic tea st eio en r Avg.
Constraint-Free
GPT-3.5 (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.4 .44 03 5) (±0 0.0 .29 17 6) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.4 .22 34 7) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .06 69 1)
GPT-4 (±0 0.1 .48 15 3) (±0 0.5 .57 23 3) (±0 0.0 .00 00 0) (±0 0.2 .49 18 3) (±0 0.3 .59 45 1) (±0 0.3 .34 23 9) (±0 0.3 .46 94 9) (±0 0.4 .33 95 7) (±0 0.5 .33 26 5) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.2 .49 13 7) (±0 0.2 .49 29 0) (±0 0.2 .36 06 5)
DS-Agent (±0 0.3 .45 82 3) (±0 0.7 .46 28 9) (±0 0.0 .00 00 0) (±0 0.6 .30 50 3) (±0 0.7 .48 33 8) (±0 0.6 .16 71 2) (±0 0.0 .00 00 0) (±0 0.6 .31 66 1) (±0 0.7 .10 12 1) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.5 .43 97 0) (±0 0.4 .49 56 3) (±0 0.3 .29 34 5)
AutoML-Agent (±0 0.9 .06 12 0) (±0 0.9 .07 23 1) (±0 0.9 .08 05 4) (±0 0.9 .00 01 3) (±0 0.9 .09 13 0) (±0 0.8 .03 82 7) (±0 0.9 .02 09 1) (±0 0.7 .03 12 0) (±0 0.7 .06 11 9) (±0 0.8 .08 10 0) (±0 0.9 .09 08 2) (±0 0.9 .06 49 7) (±0 0.9 .01 15 0) (±0 0.7 .09 06 7) (±0 0.9 .00 12 7)
Constraint-Aware
GPT-3.5 (±0 0.0 .00 00 0) (±0 0.1 .21 41 9) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.1 .22 73 6) (±0 0.3 .31 02 2) (±0 0.2 .32 18 2) (±0 0.0 .16 46 7) (±0 0.2 .24 24 3) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .17 07 8)
GPT-4 (±0 0.1 .35 43 3) (±0 0.3 .44 73 5) (±0 0.1 .49 48 4) (±0 0.1 .23 92 6) (±0 0.1 .35 31 9) (±0 0.0 .00 00 0) (±0 0.6 .38 95 4) (±0 0.0 .29 16 4) (±0 0.4 .23 72 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.5 .31 18 7) (±0 0.5 .33 12 9) (±0 0.2 .23 41 4)
DS-Agent (±0 0.3 .41 35 1) (±0 0.3 .45 81 5) (±0 0.0 .00 00 0) (±0 0.2 .30 13 2) (±0 0.6 .38 88 5) (±0 0.5 .31 36 2) (±0 0.1 .48 16 7) (±0 0.1 .49 21 8) (±0 0.4 .17 37 7) (±0 0.0 .00 00 0) (±0 0.0 .00 00 0) (±0 0.1 .22 72 2) (±0 0.2 .36 60 7) (±0 0.4 .32 97 0) (±0 0.2 .26 87 3)
AutoML-Agent (±0 0.8 .06 63 3) (±0 0.9 .08 16 1) (±0 0.9 .09 01 1) (±0 0.8 .09 18 3) (±0 0.8 .05 08 1) (±0 0.7 .08 76 1) (±0 0.9 .03 00 1) (±0 0.6 .01 11 0) (±0 0.6 .06 26 1) (±0 0.7 .06 00 5) (±0 0.9 .14 21 6) (±0 0.9 .08 09 1) (±0 0.7 .09 16 7) (±0 0.6 .09 11 8) (±0 0.8 .04 21 6)
46Preprint. Underreview.
Table8: Resultsofablationstudyondifferentvariations.
Plan Multi-Step Image Text Tabular Time-Series Node
RAP Average
Decomposition Verification Classification Classification Classification Forecasting Classification
SuccessRate
✓ 1.000 0.000 0.000 0.000 1.000 0.400
✓ ✓ 1.000 1.000 1.000 0.000 1.000 0.800
✓ ✓ ✓ 1.000 1.000 1.000 1.000 1.000 1.000
NormalizedPerformanceScore
✓ 0.929 0.000 0.000 0.000 0.734 0.333
✓ ✓ 0.928 0.982 0.975 0.000 0.748 0.727
✓ ✓ ✓ 0.936 0.971 1.000 0.991 0.812 0.942
ComprehensiveScore
✓ 0.965 0.000 0.000 0.000 0.867 0.366
✓ ✓ 0.964 0.991 0.988 0.000 0.874 0.763
✓ ✓ ✓ 0.968 0.986 1.000 0.996 0.906 0.971
Table9: Comparisonbetweenthedifferentnumbersofplans.
Image Text Tabular Time-Series Node
NumberofPlans Average
Classification Classification Classification Forecasting Classification
SuccessRate
1 1.000 1.000 1.000 1.000 1.000 1.000
3 1.000 1.000 1.000 1.000 1.000 1.000
5 1.000 1.000 1.000 1.000 1.000 1.000
NormalizedPerformanceScore
1 0.831 0.966 0.958 0.998 0.800 0.911
3 0.936 0.971 1.000 0.999 0.812 0.944
5 0.916 0.964 0.973 0.998 0.805 0.931
ComprehensiveScore
1 0.916 0.983 0.979 0.999 0.900 0.955
3 0.968 0.986 1.000 0.999 0.906 0.972
5 0.958 0.982 0.986 0.999 0.903 0.966
Table 10: Time and monetary costs averaged across different tasks and datasets for a single run
undertheconstraint-freeandconstraint-awaresettings.
Cost PromptParsing RequestVerification Retrieval&Planning PlanExecution ExecutionVerification SelectionandSummarization CodeGeneration Total
Constraint-Free
Money($) N/A 0.00 0.02 0.14 0.00 0.06 0.04 0.27
Time(s) 10.78 1.91 187.71 136.34 1.04 17.88 182.60 538.25
Constraint-Aware
Money($) N/A 0.00 0.00 0.11 0.00 0.15 0.06 0.32
Time(s) 14.21 3.63 182.38 98.62 1.37 20.25 191.90 512.35
47