Preprint. Underreview.
GEOMETRIC REPRESENTATION CONDITION IMPROVES
EQUIVARIANT MOLECULE GENERATION
ZianLi1,2,CaiZhou3,4,XiyuanWang1,2,XingangPeng1,2,MuhanZhang1‚àó
1InstituteforArtificialIntelligence,PekingUniversity
2SchoolofIntelligenceScienceandTechnology,PekingUniversity
3DepartmentofElectricalEngineeringandComputerScience,MassachusettsInstituteofTechnology
4DepartmentofAutomation,TsinghuaUniversity
zian@stu.pku.edu.cn, muhan@pku.edu.cn
ABSTRACT
Recentadvancementsinmoleculargenerativemodelshavedemonstratedsubstan-
tialpotentialinacceleratingscientificdiscovery,particularlyindrugdesign.How-
ever, these models often face challenges in generating high-quality molecules,
especially in conditional scenarios where specific molecular properties must be
satisfied. In this work, we introduce GeoRCG, a general framework to enhance
the performance of molecular generative models by integrating geometric repre-
sentation conditions. We decompose the molecule generation process into two
stages: first, generating an informative geometric representation; second, gener-
atingamoleculeconditionedontherepresentation. Comparedtodirectlygener-
ating a molecule, the relatively easy-to-generate representation in the first-stage
guides the second-stage generation to reach a high-quality molecule in a more
goal-oriented and much faster way. Leveraging EDM as the base generator, we
observe significant quality improvements in unconditional molecule generation
onthewidely-usedQM9andGEOM-DRUGdatasets. Morenotably,inthechal-
lengingconditionalmoleculargenerationtask,ourframeworkachievesanaverage
31%performanceimprovementoverstate-of-the-artapproaches,highlightingthe
superiority of conditioning on semantically rich geometric representations over
conditioning on individual property values as in previous approaches. Further-
more, we show that, with such representation guidance, the number of diffusion
steps can be reduced to as small as 100 while maintaining superior generation
qualitythanthatachievedwith1,000steps,therebysignificantlyacceleratingthe
generationprocess.
1 INTRODUCTION
Recent years have seen rapid development in generative modeling techniques for molecule gener-
ation (Garcia Satorras et al., 2021; Hoogeboom et al., 2022; Luo & Ji, 2022; Wu et al., 2022; Xu
etal.,2023;Morehead&Cheng,2024),whichhavedemonstratedgreatpromiseinacceleratingsci-
entificdiscoveriessuchasdrugdesign(Gravesetal.,2020). Bymodelingmoleculesaspointclouds
ofchemicalelementsembeddedinEuclideanspaceandemployingequivariantmodelsasbackbone
architectures, such as EGNN (Satorras et al., 2021), these approaches can ensure the O(3)- (or
SO(3)-)invarianceofthemodeledmoleculeprobabilityandhaveshownsignificantpromiseinboth
unconditionalandconditionalmoleculegenerations.
Despitetheadvances,preciselymodelingthemoleculardistributionq(M)stillremainsachallenge,
with current models often falling short of satisfactory results. This is especially true in the more
practicalscenarioswherethegoalistocapturetheconditionaldistributionq(M|c)forconditional
generation, with c representing a desired property such as the HOMO-LUMO gap. In such cases,
recentmodelsstillproduce molecules withpropertyerrorssignificantlylarger thanthedatalower
bound(Hoogeboometal.,2022;Xuetal.,2023).Suchchallengearisespartlybecausemoleculesare
naturallysupportedonalower-dimensionalmanifold(Mislow,2012;DeBortoli,2022;Youetal.,
‚àóCorrespondingAuthor.
1
4202
tcO
4
]GL.sc[
1v55630.0142:viXraPreprint. Underreview.
2023),yetareembeddedina3Dspacewithmuchhigherambientdimensions(N √ó(3+d),where
N isthenumberofatomsanddtheatomfeaturedimension). Consequently,directlylearningthese
distributions without additional guidance or conditioning solely on a single property can result in
substantialerrors(Songetal.,2021),oftenleadingtounstableorundesirablemolecularsamples.
a) Training (parallel) b) Sampling(sequential)
Representation Representation ùëü‚àó‚àºùëù%ùëü
P G Er ee no- ct mr oa e din etre ricd ùëü‚àà‚Ñù$! r ce op n.
d ition
Generator ùëù%ùëü Generator ùëù%ùëü core np d. i tion
Molecule Molecule
‚Ñ≥= ùë•,‚Ñé ‚àà‚Ñù!√ó#√ó‚Ñù!√ó$
Generator ùëù&‚Ñ≥‚à£ùëü Generator ùëù&‚Ñ≥‚à£ùëü ‚Ñ≥‚àó‚àºùëù&‚Ñ≥‚à£ùëü=ùëü‚àó
Figure1: TrainingandsamplingprocedureofGeoRCGinunconditionalmoleculegenerationsce-
nario. a) During training, each molecule M is mapped into an informative representation r by a
pre-trained, frozen geometric encoder E. The distribution of representations is then learned by a
lightweightrepresentationgenerator. Themoleculegeneratoristrainedinaself-conditionedman-
ner, generating amolecule M conditionedon itsownrepresentation E(M). b)During sampling,
aninformativerepresentationisfirstgenerated,whichsubsequentlyguidesthemoleculegenerator
toproducehigh-qualitymolecules.
Inthiswork,weproposeGeoRCG(Geometric-Representation-ConditionedMoleculeGeneration),
ageneralframeworkforimprovingthegenerationqualityofmoleculargenerativemodelsbylever-
aging geometric representation conditions for both unconditional and conditional generation. See
Figure1foranoverviewoftheframework.Atahighlevel,ratherthandirectlylearningtheextrinsic
moleculardistribution,weaimtofirsttransformitintoamorecompactandsemanticallymeaning-
ful representation distribution, with the help of a well-pre-trained geometric encoder E such as
Unimol(Zhouetal.,2023)andFrad(Fengetal.,2023). Thisdistributionismuchsimplerbecause
itdoesnotexhibitanygroupsymmetries,suchasO(3)/SO(3)andS(N)groupswhicharepresent
in extrinsic molecular distributions. As a result, a lightweight representation generator (Li et al.,
2023)caneffectivelycapturethisreduceddistribution. Inthesecondstage, weemployastandard
molecular generator to achieve the ultimate objective: molecular generation. Unlike conventional
approaches,ourmoleculargeneratorisdirectlyinformedbythefirst-stagegeometricrepresentation,
whichencapsulatescrucialmolecularstructureandpropertyinformation. Thisguidanceenablesthe
generationofhigh-qualitymolecularstructureswithimprovedfidelity.
Our approach is directly inspired by RCG (Li et al., 2023), which, however, focuses on image
data with fixed size and positions and does not necessitate handling Euclidean and permutation
symmetries‚Äîfactors that are markedly different in molecular data. Compared to recent work
GraphRCG (Wang et al., 2024), which applies the RCG framework to 2D graph data, we explic-
itlyhandle3DgeometrythatismorecomplexduetotheadditionalEuclideansymmetry. Moreover,
weavoidthecomplicatedstep-wisebootstrappedtrainingandsamplingprocessproposedinWang
etal. (2024)that requiresnoise alignment, sequentialtraining, and simultaneousencoder training.
Instead,weemployasimpleandintuitiveframeworkthatenablesparalleltrainingandleveragesad-
vancedpre-trainedgeometricencoderscontainingvaluableexternalknowledge(Zaidietal.,2022;
Fengetal.,2023), thusachievingcompetitiveresultswhileavoidingcomplextrainingprocedures.
Notably,whileLietal.(2023)primarilyfocusonempiricalevaluation,wealsoprovidetheoretical
characterizationsofgeneralrepresentation-conditioneddiffusionmodelsforbothunconditionaland
conditionalgeneration,whichoffersarigorousunderstandingoftheimprovedperformance.
To illustrate the effectiveness of our approach, we select one of the simplest and most classical
equivariantgenerativemodels,EDM(Hoogeboometal.,2022),asthebasemoleculargeneratorof
GeoRCG.Experimentally,ourmethodachievesthefollowingsignificantimprovements:
‚Ä¢ SubstantiallyenhancingthequalityofthegeneratedmoleculesonthewidelyusedQM9and
GEOM-DRUGdatasets. OnQM9,GeoRCGnotonlyimprovestheperformanceofEDMby
a large margin, but also significantly surpasses several recent baselines with state-of-the-art
performance(Wuetal.,2022;Xuetal.,2023;Morehead&Cheng,2024;Songetal.,2024a).
2Preprint. Underreview.
‚Ä¢ Moreremarkably,inconditionalmoleculegenerationtasks,GeoRCGyieldsanaverage31%
improvement in performance, while many contemporary models struggled to achieve even
marginalgains.
‚Ä¢ By incorporating classifier-free guidance in the molecule generator (Li et al., 2023) and
employing low-temperature sampling for representation generation (Ingraham et al., 2023),
GeoRCG demonstrate a flexible trade-off between molecular quality and diversity with-
out additional training, which is especially advantageous in molecular generation tasks that
prioritizesqualityoverdiversity.
‚Ä¢ With the assistance of the representation guidance, GeoRCG can significantly reduce the
numberofdiffusionstepsneeded,whilepreservingthemoleculargenerationquality,thereby
considerablyacceleratingthegenerationprocess.
2 RELATED WORKS
MolecularGenerativeModels Earlyworkhasprimarilyfocusedonmodelingmoleculesas2D
graphs(composedofatomtypes,connection,andedgetypes),utilizing2Dgraphgenerativemodels
to learn the graph distribution (Vignac et al., 2022; Jang et al., 2023; Jo et al., 2023; Luo et al.,
2023; Zhou et al., 2024). However, since molecules inherently exist in 3D space, where physical
laws govern their behavior and geometry provides critical information related to key properties,
recent research has increasingly focused on utilizing 3D generative models to directly learn the
geometricdistributionbymodelingmoleculesaspointcloudsofchemicalelements. Notableearly
autoregressivemodelsincludeG-SchNet(Gebaueretal.,2019)andG-SphereNet(Luo&Ji,2022).
More recently, diffusion models have demonstrated effectiveness in this domain, as evidenced by
models like EDM (Hoogeboom et al., 2022) and subsequent advancements (Xu et al., 2023; Wu
et al., 2022; Morehead & Cheng, 2024) that enhance EDM with latent space, prior information
andmorepowerfulbackbonesrespectively. Furthermore,recentadvancesinflowmethods(Lipman
etal.,2022;Liuetal.,2022b)haveinspiredthedevelopmentofgeometric,equivariantflowmethods
including EquiFM (Song et al., 2024b) and GOAT (Hong et al., 2024), which can provide much
faster molecule generation speed. Beyond these, there are also methods jointly model 2D and 3D
information (Vignac et al., 2023; You et al., 2023; Irwin et al., 2024) (also called 3D graph (You
et al., 2023)), where a representative method is MiDi (Vignac et al., 2023) which uses a diffusion
frameworktojointlydiffuseanddenoiseatomtype,bondtype,formalchargesandcoordinates.
LatentGenerativeModels Atahighlevel,ourframeworkcanalsobeviewedasalatentgener-
ative model, where data distributions are learned in a latent space (our stage 1) and decoded back
throughsomedecoder(ourstage2). Mostpriorworksinthisdomaineitherfocusonregulardata
formswithfixedpositionsandsizes(VanDenOordetal.,2017;Razavietal.,2019;Dai&Wipf,
2019; Aneja et al., 2021; Rombach et al., 2022; Li et al., 2023), or on data without Euclidean
symmetry and require explicit modeling (Wang et al., 2024). Molecular data, however, presents
uniquechallengesinbothaspects. Oneofthekeyissuesinthiscontextishowtodefinethelatent
space‚Äîdefiningitas‚Äúlatentcoordinatesandfeatures‚ÄùasinGeoLDM(Xuetal.,2023)stillresults
inageometricallystructuredandthuscomplexspace,whiledefiningitonrepresentationsaswedo
introducesthechallengeofeffectively‚Äúdecoding‚Äùaglobal,non-symmetricembeddingbackintoge-
ometricobjects.LGD(Zhouetal.,2024)trainsadiffusionmodelonaunifiedEuclideanlatentspace
obtainedbyjointlytrainingapowerfulencoderandasimpledecoder,andperformsbothgeneration
andpredictiontasksfocusingon2Dgraphs.LDM-3DG(Youetal.,2023)alsoadoptsrepresentation
latentspacebutemploysacascaded(2D+3D)auto-encoder(AE)framework,wherethedecoderis
designed(ortrained)tobedeterministic,renderingpoorperformanceonthe3Dpartasevidencedin
ourexperiments. Incontrast,wemodelthedecoderasapowerfulgenerativemodel,focusingsolely
on geometric learning while demonstrating superior effectiveness. We leave the related works of
pre-trainedgeometricencodersinAppendixA.
3 METHODS
3.1 PRELIMINARIES
Inthiswork,werepresentmoleculesaspointcloudsofchemicalelementsin3Dspace,denotedby
M=(x,h),wherex=(x ,...,x )‚ààRN√ó3 representstheatomiccoordinatesofN atoms,and
1 N
3Preprint. Underreview.
Figure2: T-SNEvisualizationsoftherepresentationsproducedbyFrad(Fengetal.,2023)forthe
QM9 dataset (left) and by Unimol (Zhou et al., 2023) for the GEOM-DRUG dataset (right). The
representationsexhibitclearclusteringbasedonnodecount.
h = (h ,...,h ) ‚àà RN√ód captures the node features of dimension d, such as atomic numbers
1 N
andcharges. ThisformulationfollowstheapproachofHoogeboometal.(2022);Xuetal.(2023);
Morehead & Cheng (2024) and is widely utilized in molecular representation learning (Thomas
etal.,2018;Lietal.,2024a;Zaidietal.,2022),facilitatingtheintegrationofpre-trainedmolecular
encoders(Zaidietal.,2022;Fengetal.,2023). Weuseqtodenotetheunderlyingdatadistribution,
such as molecule distributions q(M), and p to denote the approximated distributions modeled by
parametricmethods.
We denote the pre-trained geometric encoder as E : (cid:83)+‚àû (RN√ó3√óRN√ód) ‚Üí Rdr, which
N=1
embeds a molecule M with an arbitrary number of nodes N into a representation vector r of
fixed dimension d . The geometric encoder exhibits E(3)- (or SE(3)-) invariance, meaning that
r
E(M)=E((x,h))=E((xRT +t,h))foranyt‚ààR3andR‚ààO(3)(orSO(3)),whereO(3)is
thesetoforthogonalmatrices(andSO(3)beingthesetofspecialorthogonalmatrices).
3.2 GEORCG:GEOMETRIC-REPRESENTATION-CONDITIONEDMOLECULARGENERATION
Geometric Representation Generator To improve the quality of the generated molecules, we
propose first transforming the geometrically structured molecular distribution q(M) into a non-
geometricrepresentationdistributionq(r)usingawell-pre-trainedgeometricencoderE thatmaps
eachmoleculeMtoitsrepresentationr. Learningtherepresentationdistributionq(r)isconsider-
ably easier, since representations do not exhibit any symmetry as in explicit molecular generative
models (Xu et al., 2022; Hoogeboom et al., 2022). We thus leverage a simple yet effective MLP-
based diffusion architecture proposed in (Li et al., 2023) for the representation generator p (r),
œÜ
whichadoptstheDDIMarchitecture(Songetal.,2020)withMLPbackbonesandisoptimizedvia
thedenoisingscorematchingscheme(Vincent,2011).
Onedeviationfrompreviouspractices(Lietal.,2023;Wangetal.,2024)isthatweadditionallycon-
ditiontherepresentationgeneratoronthemolecule‚ÄôsnodenumberN bydefault1. Thisiscrucialfor
ensuringconsistencybetweenthesizeoftherepresentation‚Äôsunderlyingmoleculeandthesizeofthe
moleculeitguidestogenerate. Moreover,moleculeswithdifferentsizesoftenhavedistinctmodes
in structures and properties (Hoogeboom et al., 2022), which is reflected in their geometric repre-
sentationslearnedbymodernpre-trainedgeometricencoders(Zhouetal.,2023;Fengetal.,2023),
asshowninFigure2. Fromthefigures,itisevidentthatbyconditioningonN,thelearningprocess
fortherepresentationgeneratorbecomessimplerandmoreeffective, leadingtothefollowingloss
functionofourrepresentationgenerator:
L rep =E (r,N)‚ààDrep ,œµ‚àºN(0,I),t(cid:2) ||r‚àíf œÜ(r t,t,N)||2(cid:3) , (1)
train
whereDrep ={{(E(M),N(M))|M‚ààDmol}},withN(M)representingatomnumberofMand
train train
Dmol denoting the molecule dataset. Here, f is the MLP backbone (Li et al., 2023), and r =
‚àötrain ‚àö œÜ t
Œ± r+ 1‚àíŒ± œµisthenoisyrepresentationcomputedwiththepredefinedscheduleŒ± ‚àà(0,1].
t t t
1WeomittheconditionN inourprobabilitydecompositionsandmathematicalderivationsforstatement
simplicity,asitsinclusiondoesnotaffecttheoverallframeworkandconclusions.
4Preprint. Underreview.
Molecule Generator Since the ultimate goal of our framework is to generate molecules from
(cid:82)
q(M), we decompose the molecular distribution as q(M) = q(M|r)q(r)dr to explicitly en-
able geometric-representation conditions. Consequently, a geometric-representation-conditioned
molecular generator p (M|r) is required. In principle, we can use many modern molecule gen-
Œ∏
erators(Hoogeboometal.,2022;Xuetal.,2023;Morehead&Cheng,2024),asthesemodelscan
all take additional conditions. To illustrate the effectiveness of our approach, however, we choose
arelativelysimplemodelEDM(Hoogeboometal.,2022)asthebasegenerator. EDMisdesigned
to ensure the O(3)-invariance, i.e., for any R ‚àà O(3), p (M) = p (x,h)) = p ((xRT,h)). To
Œ∏ Œ∏ Œ∏
accomodateEDMtorepresentationconditions,weusethefollowingtrainingobjective:
L =E (cid:2) ||œµ‚àíf (M ,t,r)||2(cid:3) , (2)
mol (M,r)‚àºDmol-rep,t‚àºU(0,T),œµ‚àºNÀÜ(0,I) Œ∏ t
train
where Dmol-rep = {{(M,E(M))|M ‚àà Dmol}}, and sampling from NÀÜ(0,I) entails drawing œµ =
train train 0
[œµ(x),œµ(h)]fromN(0,I),adjustingœµ(x)bysubtractingitsgeometriccentertoobtainœµ(x),andsetting
0 0 0
œµ = [œµ(x),œµ(h)]. Thisensuresthezerocenter-of-massproperty,asthedistributionisdefinedonthis
0
subspacetoensuretranslationinvariance(Hoogeboometal.,2022). Thenoisymoleculeisgivenby
M = Œ±(M)[x,h]+œÉ(M)œµ, with time-dependent schedules Œ±(M) and œÉ(M), while the diffusion
t t t t t
backbonef ,whichisinstantiatedwithEGNN(Satorrasetal.,2021),isconditionedonr.
Œ∏
CombiningtheTwoGeneratorsTogether Therepresentationgeneratorp (r)andthemolecule
œÜ
(cid:82)
generator p (M|r) together model the molecular distribution p (M) = p (M|r)p (r)dr,
Œ∏ œÜ,Œ∏ Œ∏ œÜ
(cid:82)
which approximates the data distribution q(M) = q(M|r)q(r)dr that we aim to capture. One
notableadvantageoftheframeworkisthatthedecompositionenablesparalleltrainingofthetwo
generators. TheentiretrainingandsamplingprocedureissummarizedinAlgorithm1.
ThereareseveralkeypropertiesofGeoRCGthatfacilitatehigh-qualitymoleculegeneration. First,
GeoRCGpreservesallsymmetrypropertiesofthebasemoleculegeneratorp (M):
Œ∏
Proposition 3.1. (Symmetry Preservation) Assume the original molecular generator p (M) is
Œ∏
O(3)-orSO(3)-invariant. Then,thetwo-stagegeneratorp (M)isalsoO(3)-orSO(3)-invariant.
œÜ,Œ∏
Proof. This result follows directly from the definition. Specifically, p (M) =
œÜ,Œ∏
(cid:82) p (M|r)p (r)dr = (cid:82) p ((xRT,h)|r)p (r)dr = p ((xRT,h)) for any R ‚àà O(3) (or
Œ∏ œÜ Œ∏ œÜ œÜ,Œ∏
SO(3)). Thesecondequalityholdsduetothesymmetricpropertyofp (M),whichremainsvalid
Œ∏
whenadditionalnon-symmetricconditionsrareapplied.
Moreover,representation-conditioneddiffusionmodelscanachievenohigheroveralltotalvariation
distancethantraditionaldiffusionmodels,andcanarguablyyieldbetterresults,astherepresentation
encodes key data information that may further reduce estimation error. We present the rigorous
bound in Theorem 3.1, and provide the proof and detailed discussions in Appendix E.1. Notably,
thisisageneraltheoreticalcharacterizationthatappliestopriorexperimentalwork(Lietal.,2023).
Theorem3.1. Considertherandomvariablex ‚àà Rd ‚àº q(x),andassumethatthesecondmoment
m ofxisboundedasm2 := E [‚à•x‚àíx¬Ø‚à•2] < ‚àû, wherex¬Ø := E [x]. Further, assumethat
x x q(x) q(x)
thescore‚àálnq(x )isL -Lipschitzforallt,andthatthescoreestimationerrorinthesecond-stage
t x
diffusion is bounded by œµ such that E [‚à•s (x ,t,r)‚àí‚àálnq (x |r)‚à•2] ‚â§
œÜ,Œ∏,cond r‚àºpœÜ(r),xt‚àºqt(xt|r) Œ∏ t t t
œµ2 . Denotethestepsizeash:=T/N,whereT isthetotaldiffusiontimeandN isthenumber
œÜ,Œ∏,cond
ofdiscretizationsteps,andassumethath‚™Ø1/L . Supposewesamplex‚àºp (x|r)fromGaussian
x Œ∏
noise, where r ‚àº p (r), and denote the final distribution of x as p (x). Define
pqT|œÜ,
which
œÜ Œ∏,œÜ 0
is the endpoint of the reverse process starting from q instead of Gaussian noise. Here, q
T|œÜ T|œÜ
istheT-thstepintheforwardprocessstartingfromq := 1 (cid:82) q(x |r)p (r)dr,whereAisthe
0|œÜ A r 0 œÜ
normalizationfactor.Denotek-dimisotropicGaussiandistributionasŒ≥k.Thenthefollowingholds,
(cid:113) (cid:112) ‚àö
TV(p (x),q(x))‚™Ø KL(q ||Œ≥d+3)exp(‚àíT)+(L (d+3)h+L m h) T (3)
Œ∏,œÜ 0|œÜ x x x
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
discretizationerror
convergenceofforwardprocess
‚àö
+ œµ T + TV(q ,q ) (4)
œÜ,Œ∏,cond 0|œÜ 0
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
conditionalscoreestimationerror representationgenerationerror
5Preprint. Underreview.
Rep. Molecule
ùëê
! Generator ùëù!! ùëü | ùëê(#) Generator
ùëê(%)‚Ñ≥% ‚âàùëê%
Rep.
uncond. Generator
ùëù!"ùëü
‚Ñ≥!
ùëù ‚Ñ≥‚à£ùëü
!
Rep.
ùëê
" Generator
ùëù!# ùëü | ùëê(%)
ùëê(#)‚Ñ≥# ‚âàùëê#
Figure 3: A single molecule generator can be employed for both unconditional and conditional
moleculegenerationwithrespecttovariousproperties. Forconditionalgeneration,onlytherepre-
sentation generator is re-trained on (molecule, property) pairs, allowing it to conditionally sample
property-meaningfulrepresentationsduringthesamplingstage.
Balancing Quality and Diversity of Molecule Generation In many scientific applications, re-
searchers prioritize generating higher-quality molecules over more diverse ones. To address this
preference,weintroduceafeaturethatallowsfine-grainedcontroloverthetrade-offbetweendiver-
sityandqualityinthesamplingstage(thuswithoutretraining). Thisisachievedbyintegratingtwo
keytechniques: low-temperaturesampling(Ho&Salimans,2022)(controlledviathetemperature
T)fortherepresentationgenerator,andclassifier-freeguidance(Ho&Salimans,2022)(controlled
viathecoefficientw)forthemoleculegenerator. Weprovidemoredetailsaboutthetwotechniques
inAppendixB.Combiningthetwofeaturesenablesaflexibleandexplicitcontrol,whichwerefer
toas‚ÄúBalancingControllablility‚ÄùanddemonstrateitseffectivenessinSection4.2.
Handling Conditional Molecule Generation The framework discussed thus far focuses on un-
conditionalmoleculegeneration,wherenospecificpropertyc(e.g.,HOMOenergy)ispre-specified.
However,formoleculegeneration,amorepracticalanddesiredscenarioisconditional(alsocalled
controllable)generation,whereadditionalconditionsc,suchasHOMO-LUMOgapenergy,isintro-
duced,andourobjectiveshiftstogeneratingmoleculesfromthedistributionq(M|c). InGeoRCG,
(cid:82)
thisconditionalgenerationisnaturallydecomposedasp (M|c)= p (M|r)p (r|c)dr,mean-
Œ∏,œÜ Œ∏ œÜ
ing that we first generate a ‚Äúproperty-meaningful‚Äù molecular representation r, which is then inde-
pendentlyusedtoconditionthesecond-stagemoleculegeneration. SeeFigure3forillustration. A
key advantage of this modeling approach is that, when different properties (e.g., HOMO, LUMO,
GAP energy) need to be captured, only the representation generator requires retraining under
thenewconditions. Thisretrainingishighlyefficientduetothelightweightnatureoftherepresen-
tationgenerator. Notably,GeoRCGdemonstratesoutstandingconditionalgenerationperformance,
asshowninSection4.3. Moreover,wetheoreticallydemonstratethat,undermildassumptions,the
representationgeneratorcanprovablyestimatetheconditionaldistributionandgeneraterepresenta-
tionsthatleadtoprovablerewardimprovementstowardthetarget,whichsubsequentlybenefitsthe
second-stagegeneration. FurtherdetailsareprovidedinAppendixE.2.
4 EXPERIMENTS
4.1 EXPERIMENTSETUP
DatasetsandTasks Asamethodfor3Dmoleculegeneration,weevaluateGeoRCGonthewidely
used datasets QM9 (Ramakrishnan et al., 2014) and GEOM-DRUG (Gebauer et al., 2019; 2022).
Wefocusontwotasks:unconditionalmoleculegeneration,wherethegoalistosamplefromq(M),
and conditional (or controllable) molecule generation, where a property c is given, and we aim to
sample from q(M|c). To ensure fair comparisons, we follow the dataset split and configurations
exactly as in Anderson et al. (2019); Hoogeboom et al. (2022); Xu et al. (2023). Without further
clarification, we bold the highest scores and underline the second-highest one. Additionally, to
illustratethedirectimprovementoverourbasemodel,EDM(Hoogeboometal.,2022),wedisplay
greennumbersnexttothescoretoindicatetheaverageimprovement,andrednumberstodenotea
6Preprint. Underreview.
Table1: Qualitycomparisonofunconditionalmoleculargenerationacrossdifferentmethods. The
graycellsdenotesthebasemoleculegeneratoremployedinGeoRCG.
QM9 DRUG
Metrics
AtomSta(%)‚Üë MolSta(%)‚Üë Valid(%)‚Üë Valid&Unique(%)‚Üë AtomSta(%)‚Üë Valid(%)‚Üë
Methods
Data 99 95.2 97.7 97.7 86.5 99.9
G-Schnet 95.7 68.1 85.5 80.3 - -
GDM 97 63.2 - - 75 90.8
GDM-AUG 97.6 71.6 90.4 89.5 77.7 91.8
GraphLDM 97.2 70.5 83.6 82.7 76.2 97.2
GraphLDM-AUG 97.9 78.7 90.5 89.5 79.6 98
EDM 98.7 82 91.9 90.7 81.3 92.6
EDM-Bridge 98.8 84.6 92 90.7 82.4 92.8
GeoLDM 98.9(0.1) 89.4(0.5) 93.8(0.4) 92.7(0.5) 84.4 99.3
GCDM 98.7(0.0) 85.7(0.4) 94.8(0.2) 93.3(0.0) 89 95.5
ENF 85 4.9 40.2 39.4 - -
EquiFM 98.9(0.1) 88.3(0.3) 94.7(0.4) 93.5(0.3) 84.1 98.9
GOAT 98.4 84.1 90.9 89.99 81.8 96.0
GeoBFN(1k) 99.08(0.03) 90.87(0.1) 95.31(0.1) 92.96(0.1) 86.1 91.66
GeoRCG 99.12(0.03)0.43% 92.32(0.06)12.59% 96.52(0.2)5.03% 92.45(0.2)1.93% 84.3(0.12)3.69% 98.5(0.12)6.37%
decrease. Allresultsarecalculatedbasedon10krandomlysampledmolecules,averagedoverthree
runs,withstandarderrorsreportedinparentheses.
Instantiation of the Pre-trained Encoder We employ Frad (Feng et al., 2023), which was pre-
trained on the PCQM4Mv2 dataset (Nakata & Shimazaki, 2017) using a hybrid noise denoising
objective,asthegeometricencoderforQM9dataset. ForGEOM-DRUG,weadoptUnimol(Zhou
etal.,2023)andpre-trainitontheGEOM-DRUGdatasetitself,sinceGEOM-DRUGcontainsdis-
tinct chemical elements not present in PCQM4Mv2 or other commonly used pre-training datasets
suchasZINCorChemBL(Lietal.,2021).
Baselines A direct comparison is made with our base molecule generator, EDM (Hoogeboom
et al., 2022). Additionally, we benchmark against the non-equivariant counterparts of EDM and
GeoLDM (Xu et al., 2023), namely GDM(-AUG) (Hoogeboom et al., 2022) and GraphLDM(-
AUG) (Xu et al., 2023), as well as the autoregressive method G-SchNet (Gebauer et al., 2019).
Forfurthercomparison,weincludeadvancedequivariantdiffusionmodelslikeGeoLDM(Xuetal.,
2023), EDM-Bridge (Wu et al., 2022), and GCDM (Morehead & Cheng, 2024), along with fast
equivariantflow-basedmethodssuchasE-NF(GarciaSatorrasetal.,2021), EquiFM(Songetal.,
2024b), and GOAT (Hong et al., 2024). A recent Bayesian-based method GeoBFN (Song et al.,
2024a)isalsoconsideredforitshigh-qualitysamples. Finally,weextendthecomparisonto2Dand
3DmethodslikeMiDi(Vignacetal.,2023)andLDM-3DG(Youetal.,2023).
Additionally,weprovidefurtherexperiments,includingablationstudiesonthepre-trainedencoder,
inAppendixD.
4.2 UNCONDITIONALMOLECULEGENERATION
WefirstevaluatethequalityofunconditionallygeneratedmoleculesfromGeoRCG,withthecom-
monly adopted validity and stability metrics for assessing molecules‚Äô quality (Hoogeboom et al.,
2022). SeeAppendixCfordetaileddescriptionsofthesemetrics.
MainResults WepresentthemainresultsontheQM9andDRUGdatasetsinTable1. Below,we
highlightthekeyfindings:(i)Improvementoverthebasemodel(EDM):Byleveraginggeometric
representations,GeoRCGsignificantlyoutperformsthebasemodel,EDM,onbothQM9andDRUG
datasets. Notably, on QM9, it increases stable molecules from 82% to 93.9% and validity from
91.9%to97.4%,whilealsoimprovingmoleculeuniqueness.(ii)Superiorperformancecompared
toadvancedmethods:GeoRCGalsosurpassesincludedadvancedmodelsontheQM9dataset. On
theDRUGdataset,itoutperformsmodelssuchasEDM-BridgeandGOAT,andgetsahighscorein
validity. Although it falls short of achieving the best performance, we attribute this to the quality
of the encoder‚Äôs representations, as we pre-trained it on the GEOM-DRUG dataset, which may
lackdiversityandsufficientsize. Crucially,manystructuresinGEOM-DRUGlacktheequilibrium
conditions necessary for pre-training methods that enable effective learning of force fields (Zaidi
etal.,2022;Fengetal.,2023).Additionally,GeoRCGadoptsEDM(Hoogeboometal.,2022)asthe
7Preprint. Underreview.
basemoleculegenerator,whichcanbeweakercomparedtorecentadvancedmodels. Nevertheless,
further improvement to GeoRCG can be achievabled by incorporating more advanced molecule
generatorsorhigher-qualitypre-trainedencoders.
BalancingControllability Weproceedtoinvestigatethe‚ÄúBalancingControllablility‚Äù‚Äùfeatureof
GeoRCGintroducedinSection3.2. Tothisend,weconductedagridsearchbyvaryingbothwand
T on QM9 dataset, as depicted in Figure 4. The results indicate a clear trend: increasing w and
decreasingT improvevalidityandstabilityattheexpenseofuniqueness,allowingforfine-grained,
flexiblecontrolovermoleculegeneration. Atitsbest,thisapproachachievesamoleculestabilityof
93.9%andavalidityof97.42%,approachingthedataset‚Äôsupperbound,withatrade-offinlower
uniqueness&validityof86.82%.
Figure 4: Balance controllable (unconditional) generation on QM9 dataset. Increasing w and de-
creasingT enhancesmolecularstabilityandvalidity,withthecostofareductioninuniqueness.
Molecule Stability Validity Validity&Uniqueness
97.4 92.30 93.66 93.56 93.54 93.5 96.36 97.09 97.10 97.13 97.2 91.38 89.70 86.70 85.64 92
93.0 97.0 91
92.40 93.43 93.72 93.90 92.5 96.60 97.14 97.38 97.42 96.8 92.05 90.37 88.37 86.82 90
92.0 96.6 89
92.32 93.16 93.30 93.38 91.5 96.52 97.11 97.04 97.30 96.4 92.45 91.36 89.53 88.62 88 96.2
91.0
96.0 87
90.02 90.65 91.06 91.08 90.5 95.67 96.06 96.28 96.38 92.61 92.23 91.73 91.40
95.8 86
1 2 3 4 1 2 3 4 1 2 3 4
Inverse temperature Te1 mp Inverse temperature Te1 mp Inverse temperature Te1 mp
AdditionalComparisonwith2D&3Dmethods Wefurthercomparewithrecent2D&3Dmeth-
ods, including MiDi (Vignac et al., 2023) and LDM-3DG (You et al., 2023), which jointly learn
and generate both 2D bond information and 3D geometry. However, these models rely on exter-
naltoolslikeRDKitorOpenBabel(O‚ÄôBoyleetal.,2011)forbondcomputationattheinputstage,
whichenablesthemtoimplicitlyleveragedomainknowledgeofsuchtools. Incontrast,purely3D
methodslikeGeoRCGandthoseinourprimarycomparisonutilizeacoarselook-uptableforbond
estimation,which,whilereflecting3Dlearning,resultsinlessaccuratebondcalculation,potentially
biasing the comparison. Therefore, to provide a reference, we also report the performance of our
models combined with the same external tools (e.g., OpenBabel) for precise bond calculations in
ourgenerated3Dconformations. Furthermore,asGeoRCGessentiallycaptures3Dgeometricdis-
tributions, we place more emphasis on 3D metrics that directly evaluate 3D learning capabilities,
includingBondLengthW1andBondAngleW1proposedbyVignacetal.(2023)anddetailedinAp-
pendixC.TheresultsinTable2demonstratethatGeoRCGnotonlysignificantlyoutperformsMiDi
andLDM-3DGon3Dmetrics, highlightingtheadvantagesofusingapure3Dmodelforlearning
3Dstructures,butalsofurtherenhancesEDM‚Äôsperformance,whichhasalreadyshownconsiderable
promisein3Dlearning.
Table2: 3DgeometrystatisticsandgeneratedmoleculequalityonQM9acrossdifferentmethods.
Modelsmarkedwith‚àó indicateresultsobtainedfromourownexperiments;seeAppendixCforthe
evaluation guidelines. The stability metrics for EDM are higher than in Table 1 due to using the
MiDicodebaseforevaluation,whichpermitsmorevalencyforatoms.
Metrics Angles(‚ó¶)‚Üì BondLength(e-2AÀö)‚Üì MolSta(%)‚Üë AtomSta(%)‚Üë Validity(%)‚Üë Uniqueness(%)‚Üë
Methods
Data ‚àº0.1 ‚àº0 98.7 99.8 98.9 99.9
MiDi(uniform) 0.67(0.02) 1.6(0.7) 96.1(0.2) 99.7(0.0) 96.6(0.2) 97.6(0.1)
MiDi(adaptive) 0.62(0.02) 0.3(0.1) 97.5(0.1) 99.8(0.0) 97.9(0.1) 97.6(0.1)
LDM-3DG‚àó 3.56 0.2 94.03 99.38 94.89 97.03
EDM 0.44 0.1 90.7 99.2 91.7 98.5
EDM+OBabel 0.44 0.1 97.9 99.8 99.0 98.5
GeoRCG 0.21(0.04)52.27% 0.04(0.0)60% 95.82(0.16)5.6% 99.59(0.02)0.39% 96.54(0.27)5.28% 95.74(0.18)2.8%
GeoRCG+OBabel 0.20(0.04)54.55% 0.07(0.06)30% 98.21(0.09)0.32% 99.88(0.00)0.08% 99.0(0.04)0.0% 95.74(0.16)2.8%
8
w tneiciffeoc
ecnadiug
reifissalC
3
2
1
0
w tneiciffeoc
ecnadiug
reifissalC
3
2
1
0
w tneiciffeoc
ecnadiug
reifissalC
3
2
1
0Preprint. Underreview.
4.3 CONDITIONALMOLECULEGENERATION
We now turn to a more challenging task: generating molecules with a specific property value c
from q(M|c). We strictly follow the evaluation protocol outlined in (Hoogeboom et al., 2022).
Speicifically,QM9issplitintotwohalves,andanEGNNclassifier(Satorrasetal.,2021)istrained
onthefirsthalfforevaluatingthegeneratedmolecules‚Äôproperty, whilethegeneratoristrainedon
the second half2. We focus on six properties: polarizability (Œ±), orbital energies (Œµ , Œµ ),
HOMO LUMO
theirgap(‚àÜŒµ),dipolemoment(¬µ),andheatcapacity(C ).
v
Table3: ConditionalmoleculegenerationonQM9. ThemetricusedistheMSEbetweenthetarget
propertyvalueandtheclassifier-predictedvalue. Thegraycellsdenotesthebaselinemoleculegen-
erator employed in our proposed approach. Models marked with ‚àó indicate results obtained from
ourownexperiments;theseareprovidedonlyasacoarsereferenceduetopotentiallydifferingeval-
uationcriteria,seeAppendixCfordetails.
Properties
Methods
Œ± ‚àÜŒµ ŒµHOMO ŒµLUMO ¬µ Cv
QM9(lowerbound) 0.1 64 39 36 0.043 0.04
Random 9.01 1470 646 1457 1.616 6.857
Natoms 3.86 866 426 813 1.053 1.971
EDM 2.76 655 356 584 1.111 1.101
GeoLDM 2.37 587 340 522 1.108 1.025
GCDM 1.97 602 344 479 0.844 0.689
EquiFM 2.41 591 337 530 1.106 1.033
GOAT 2.74 605 350 534 1.01 0.883
LDM-3DG‚àó 12.29 1160 583 1093 1.42 5.74
GeoBFN 2.34 577 328 516 0.998 0.949
GeoRCG 0.89(0.005)67.75% 368.2(4.6)43.79% 220.1(1.0)38.17% 290.8(3.1)50.21% 0.831(0.008)25.2% 0.542(0.004)50.77%
TheresultsarepresentedinTable3. Thefirstthreebaselines,asintroducedbyEDM(Hoogeboom
etal.,2022),representtheclassifier‚Äôsinherentbiasasthelowerboundforperformance,therandom
evaluation result as the upper bound, and the dependency of properties on N. For more details,
pleaserefertoAppendixC.
As shown, GeoRCG nearly doubles the performance of the best existing models for most proper-
ties,withanaverage31%improvementoverthebestones. Thisisataskwheremanyrecentmodels
struggletomakeevenmodestimprovements,asevidencedinthetable. Notably,fordifferentprop-
erties, we only re-train the representation generator, as demonstrated in Section 3.2, significantly
savingtrainingtime. InFigure5,wevisualizethegeneratedsamples,whichexhibitminimalprop-
erty errors and display a clear trend as the target values increase. Additional randomly generated
moleculesareprovidedinAppendixF.2.
64.20 69.99 75.77 81.56 87.34 93.13 98.91 104.70
64.45 70.10 75.60 81.00 87.41 93.24 97.44 104.73
Figure5: ConditionallygeneratedmoleculesonpropertyŒ±usingGeoRCG.Theblacknumberin-
dicatesthespecifiedpropertyvalue(condition),thegreennumberrepresentstheevaluatedproperty
value(computedbytheclassifier)forthegeneratedmoleculeconformer.
Apotentialconcernisthatforagivenpropertyvaluec,p (r|c)mayproducearepresentationcorre-
œÜ
spondingtoamoleculefromthetrainingdataset,allowingthemoleculegeneratortosimplyrecover
itsfullconformationbasedonthatrepresentation.Thiscouldleadtosmallpropertyerrorsbutalack
ofnovelty. Toaddressthis,weconductedathoroughevaluationofthegeneratedmoleculesacross
eachproperty,findingthatthenovelty(theproportionofnewmoleculesnotpresentinthetraining
2AlthoughFigure3showsthatretrainingthemoleculegeneratorisunnecessary,forafaircomparison,we
stillretrainthemoleculegeneratoronthesecondhalfofthedataset,ratherthanusingtheoneinunconditional
moleculegenerationtrainedontheentiredataset.
9Preprint. Underreview.
dataset)remainshighatapproximately70%,comparabletoothermethods. Additionally,thecondi-
tionallygeneratedmoleculesdemonstratemuchhighermoleculestabilitythanEDM(Hoogeboom
etal.,2022). FurtherdetailscanbefoundinAppendixD.
4.4 FASTERGENERATIONWITHFEWERSTEPS
Withgeometricrepresentationcondition,itisreasonabletoexpectthatfewerdiscretizationstepsof
thereversediffusionSDE(Songetal.,2021)wouldstillyieldcompetitiveresults. Therefore,were-
ducethenumberofdiffusionstepsandevaluatethemodel‚Äôsperformance. Theresultsarepresented
in Table 4. As demonstrated, with the geometric representation condition, GeoRCG consistently
outperformsotherapproachesacrossallstepnumbers. Notably,withapproximately100steps,the
performanceofourmethodnearlyconvergestotheoptimalperformanceobservedwith1000steps,
whichalreadysurpassesallothermethodsacrossallstepnumbers. Thisdirectlyreflectsthefaster
generation speed, as the time required for the first-stage representation generation is minimal and
canbeconsiderednegligible.
Table4: QualityofgeneratedmoleculesonQM9withfewerdiffusionsteps. Thebluecellsindicate
thehighestvalueamongmethodswiththesamenumberofdiffusionsteps,whileboldfontempha-
sizesvaluesthatoutperformallothermethodsacrossalldiffusionsteps.
Metrics
#Steps AtomSta(%)‚Üë MolSta(%)‚Üë Valid(%)‚Üë Valid&Unique(%)‚Üë
Methods
Data - 99 95.2 97.7 97.7
BestbaselineinTable1 1000 99.08 90.87 95.31 92.96
EquiFM 200 98.9(0.1) 88.3(0.3) 94.7(0.4) 93.5(0.3)
GOAT 90 98.4 84.1 90.9 89.99
EDM 50 97.0(0.1) 66.4(0.2) - -
EDM-Bridge 50 97.3(0.1) 69.2(0.2) - -
GeoBFN 50 98.28(0.1) 85.11(0.5) 92.27(0.4) 90.72(0.3)
GeoRCG 50 98.75(0.05)1.80% 89.08(0.52)34.16% 95.05(0.33) 91.32(0.37)
EDM 100 97.3(0.1) 69.8(0.2) - -
EDM-Bridge 100 97.9(0.1) 72.3(0.2) - -
GeoBFN 100 98.64(0.1) 87.21(0.3) 93.03(0.3) 91.53(0.3)
GeoRCG 100 99.08(0.03)1.83% 91.85(0.34)31.59% 96.49(0.27) 92.07(0.35)
EDM 500 98.5(0.1) 81.2(0.1) - -
EDM-Bridge 500 98.7(0.1) 83.7(0.1) - -
GeoBFN 500 98.78(0.8) 88.42(0.2) 93.35(0.2) 91.78(0.2)
GeoRCG 500 99.09(0.01)0.60% 91.89(0.24)13.17% 96.57(0.12) 92.08(0.36)
EDM 1000 98.7 82 91.9 90.7
EDM-Bridge 1000 98.8 84.6 92 90.7
GeoBFN 1000 99.08(0.06) 90.87(0.2) 95.31(0.1) 92.96(0.1)
GeoRCG 1000 99.12(0.03)0.43% 92.32(0.06)12.59% 96.52(0.2)5.03% 92.45(0.2)1.93%
5 CONCLUSIONS
In this work, we present GeoRCG, an effective framework for improving the generation quality
of arbitrary molecule generators by incorporating geometric representation conditions. We use
EDM(Hoogeboometal.,2022)asthebasemoleculegeneratoranddemonstratetheeffectivenessof
ourframeworkthroughextensivemoleculargenerationexperiments. Notably,inconditionalgener-
ationtasks,GeoRCGachievesa31%performanceboostcomparedtorecentstate-of-the-artmodels.
Additionally, therepresentationguidanceenablessignificantlyfastersamplingwith10xfewerdif-
fusion steps while maintaining near-optimal performance. Beyond these empirical improvements,
we provide theoretical characterizations of general representation-conditioned generative models,
whichaddressakeygapintheexistingliterature(Lietal.,2023).
One limitation of our framework is that its generation quality could depend on the representation
quality. For instance, on the GEOM-DRUG dataset, where the encoder was less thoroughly pre-
trained, the improvements were less pronounced, and GeoRCG did not surpass SOTA methods.
Futureworkcouldfocusonimprovingtheeffectivenessofpre-trainedmodelsorexploringenhanced
representationregularizationtechniques. Furthermore,whileweemployEDM(Hoogeboometal.,
2022)asthebasemoleculegenerator,ourframeworkisgeneralandcanbeappliedtoanymolecular
10Preprint. Underreview.
generativemodel. IntegratingthisframeworkwithmoreadvancedSOTAmodelsoffersapromising
directionforfutureexplorationandperformanceenhancement.
REFERENCES
Yasin Abbasi-yadkori, Da¬¥vid Pa¬¥l, and Csaba Szepesva¬¥ri. Improved algorithms for linear stochas-
tic bandits. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger
(eds.),AdvancesinNeuralInformationProcessingSystems,volume24.CurranAssociates,Inc.,
2011. URL https://proceedings.neurips.cc/paper_files/paper/2011/
file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf.
Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural
networks. Advancesinneuralinformationprocessingsystems,32,2019.
Jyoti Aneja, Alex Schwing, Jan Kautz, and Arash Vahdat. A contrastive learning approach for
trainingvariationalautoencoderpriors. Advancesinneuralinformationprocessingsystems,34:
480‚Äì493,2021.
TomBBrown. Languagemodelsarefew-shotlearners. arXivpreprintarXiv:2005.14165,2020.
SitanChen,SinhoChewi,JerryLi,YuanzhiLi,AdilSalim,andAnruZhang. Samplingisaseasyas
learningthescore: theoryfordiffusionmodelswithminimaldataassumptions. InTheEleventh
InternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.
net/forum?id=zyLVMgsZ0U_.
XinleiChen, SainingXie, andKaiming He. Anempirical studyof trainingself-supervised vision
transformers. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pp.
9640‚Äì9649,2021.
BinDaiandDavidWipf. Diagnosingandenhancingvaemodels. arXivpreprintarXiv:1903.05789,
2019.
Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis.
arXivpreprintarXiv:2208.05314,2022.
AlexeyDosovitskiy. Animageisworth16x16words: Transformersforimagerecognitionatscale.
arXivpreprintarXiv:2010.11929,2020.
XiaominFang,LihangLiu,JieqiongLei,DonglongHe,ShanzhuoZhang,JingboZhou,FanWang,
HuaWu,andHaifengWang. Geometry-enhancedmolecularrepresentationlearningforproperty
prediction. NatureMachineIntelligence,4(2):127‚Äì134,2022.
Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, and Wei-Ying Ma. Fractional denoising for
3d molecular pre-training. In International Conference on Machine Learning, pp. 9938‚Äì9961.
PMLR,2023.
VictorGarciaSatorras,EmielHoogeboom,FabianFuchs,IngmarPosner,andMaxWelling. E(n)
equivariant normalizing flows. Advances in Neural Information Processing Systems, 34:4181‚Äì
4192,2021.
NiklasGebauer,MichaelGastegger,andKristofSchu¬®tt. Symmetry-adaptedgenerationof3dpoint
setsforthetargeteddiscoveryofmolecules. Advancesinneuralinformationprocessingsystems,
32,2019.
NiklasWAGebauer,MichaelGastegger,StefaanSPHessmann,Klaus-RobertMu¬®ller,andKristofT
Schu¬®tt. Inverse design of 3d molecular structures with conditional generative neural networks.
Naturecommunications,13(1):973,2022.
JordanGraves,JacobByerly,EduardoPriego,NarenMakkapati,SVinceParish,BrendaMedellin,
and Monica Berrondo. A review of deep learning methods for antibodies. Antibodies, 9(2):12,
2020.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598,2022.
11Preprint. Underreview.
Haokai Hong, Wanyu Lin, and Kay ChenTan. Fast3d molecule generationvia unified geometric
optimaltransport. arXivpreprintarXiv:2405.15252,2024.
EmielHoogeboom,Vƒ±ctorGarciaSatorras,Cle¬¥mentVignac,andMaxWelling. Equivariantdiffu-
sionformoleculegenerationin3d. InInternationalconferenceonmachinelearning,pp.8867‚Äì
8887.PMLR,2022.
JohnBIngraham,MaxBaranov,ZakCostello,KarlWBarber,WujieWang,AhmedIsmail,Vincent
Frappier,DanaMLord,ChristopherNg-Thow-Hing,ErikRVanVlack,etal.Illuminatingprotein
spacewithaprogrammablegenerativemodel. Nature,623(7989):1070‚Äì1078,2023.
RossIrwin,AlessandroTibo,Jon-PaulJanet,andSimonOlsson. Efficient3dmoleculargeneration
withflowmatchingandscaleoptimaltransport. arXivpreprintarXiv:2406.07266,2024.
Yunhui Jang, Dongwoo Kim, and Sungsoo Ahn. Hierarchical graph generation with k2-trees. In
ICML2023WorkshoponStructuredProbabilisticInference{\&}GenerativeModeling,2023.
Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, and Yang Liu. Energy-motivated equivariant pre-
trainingfor3dmoleculargraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume37,pp.8096‚Äì8104,2023.
Rui Jiao, Xiangzhe Kong, Ziyang Yu, Wenbing Huang, and Yang Liu. Equivariant pretrained
transformer for unified geometric learning on multi-domain 3d molecules. arXiv preprint
arXiv:2402.12714,2024.
JaehyeongJo,DongkiKim,andSungJuHwang. Graphgenerationwithdiffusionmixture. arXiv
preprintarXiv:2302.03596,2023.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. InProceedingsofnaacL-HLT,volume1,
pp. 2,2019.
Pengyong Li, Jun Wang, Yixuan Qiao, Hao Chen, Yihuan Yu, Xiaojun Yao, Peng Gao, Guotong
Xie, and Sen Song. An effective self-supervised framework for learning expressive molecular
globalrepresentationstodrugdiscovery. BriefingsinBioinformatics,22(6):bbab109,2021.
Tianhong Li, Dina Katabi, and Kaiming He. Self-conditioned image generation via generating
representations. arXivpreprintarXiv:2312.03701,2023.
ZianLi,XiyuanWang,YinanHuang,andMuhanZhang. Isdistancematrixenoughforgeometric
deeplearning? AdvancesinNeuralInformationProcessingSystems,36,2024a.
ZianLi,XiyuanWang,ShijiaKang,andMuhanZhang. Onthecompletenessofinvariantgeometric
deeplearningmodels. arXivpreprintarXiv:2402.04836,2024b.
YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMattLe. Flowmatching
forgenerativemodeling. arXivpreprintarXiv:2210.02747,2022.
ShengchaoLiu,HongyuGuo,andJianTang. Moleculargeometrypretrainingwithse(3)-invariant
denoisingdistancematching. arXivpreprintarXiv:2206.13602,2022a.
Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and
transferdatawithrectifiedflow. arXivpreprintarXiv:2209.03003,2022b.
TianzeLuo,ZhanfengMo,andSinnoJialinPan. Fastgraphgenerationviaspectraldiffusion. IEEE
TransactionsonPatternAnalysisandMachineIntelligence,2023.
YouzhiLuoandShuiwangJi. Anautoregressiveflowmodelfor3dmoleculargeometrygeneration
fromscratch. InInternationalconferenceonlearningrepresentations(ICLR),2022.
KurtMislow. Introductiontostereochemistry. CourierCorporation,2012.
Alex Morehead and Jianlin Cheng. Geometry-complete diffusion for 3d molecule generation and
optimization. CommunicationsChemistry,7(1):150,2024.
12Preprint. Underreview.
MahoNakataandTomomiShimazaki. Pubchemqcproject: alarge-scalefirst-principleselectronic
structuredatabasefordata-drivenchemistry. Journalofchemicalinformationandmodeling,57
(6):1300‚Äì1308,2017.
Yuyan Ni, Shikun Feng, Xin Hong, Yuancheng Sun, Wei-Ying Ma, Zhi-Ming Ma, Qiwei Ye, and
Yanyan Lan. Pre-training with fractional denoising to enhance molecular property prediction.
arXivpreprintarXiv:2407.11086,2024.
Noel M O‚ÄôBoyle, Michael Banck, Craig A James, Chris Morley, Tim Vandermeersch, and Geof-
freyRHutchison. Openbabel: Anopenchemicaltoolbox. Journalofcheminformatics,3:1‚Äì14,
2011.
RaghunathanRamakrishnan,PavloODral,MatthiasRupp,andOAnatoleVonLilienfeld.Quantum
chemistrystructuresandpropertiesof134kilomolecules. Scientificdata,1(1):1‚Äì7,2014.
AliRazavi,AaronVandenOord,andOriolVinyals. Generatingdiversehigh-fidelityimageswith
vq-vae-2. Advancesinneuralinformationprocessingsystems,32,2019.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684‚Äì10695,2022.
Vƒ±ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural net-
works. InInternationalconferenceonmachinelearning,pp.9323‚Äì9332.PMLR,2021.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprintarXiv:2010.02502,2020.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-based generative modeling through stochastic differential equations. In Interna-
tional Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=PxTIG12RRHS.
Yuxuan Song, Jingjing Gong, Yanru Qu, Hao Zhou, Mingyue Zheng, Jingjing Liu, and Wei-Ying
Ma. Unified generative modeling of 3d molecules via bayesian flow networks. arXiv preprint
arXiv:2403.15441,2024a.
Yuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou,
andWei-YingMa. Equivariantflowmatchingwithhybridprobabilitytransportfor3dmolecule
generation. AdvancesinNeuralInformationProcessingSystems,36,2024b.
PhilippTho¬®lkeandGianniDeFabritiis. Torchmd-net: equivarianttransformersforneuralnetwork
basedmolecularpotentials. arXivpreprintarXiv:2202.02541,2022.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXivpreprintarXiv:1802.08219,2018.
Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in
neuralinformationprocessingsystems,30,2017.
AVaswani. Attentionisallyouneed. AdvancesinNeuralInformationProcessingSystems,2017.
Clement Vignac and Pascal Frossard. Top-n: Equivariant set and graph generation without ex-
changeability. arXivpreprintarXiv:2110.02096,2021.
Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pas-
cal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint
arXiv:2209.14734,2022.
Clement Vignac, Nagham Osman, Laura Toni, and Pascal Frossard. Midi: Mixed graph and 3d
denoisingdiffusionformoleculegeneration. InJointEuropeanConferenceonMachineLearning
andKnowledgeDiscoveryinDatabases,pp.560‚Äì576.Springer,2023.
13Preprint. Underreview.
PascalVincent. Aconnectionbetweenscorematchinganddenoisingautoencoders. Neuralcompu-
tation,23(7):1661‚Äì1674,2011.
Song Wang, Zhen Tan, Xinyu Zhao, Tianlong Chen, Huan Liu, and Jundong Li. Graphrcg: Self-
conditionedgraphgenerationviabootstrappedrepresentations.arXivpreprintarXiv:2403.01071,
2024.
Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, and Qiang Liu. Diffusion-based molecule
generationwithinformativepriorbridges. AdvancesinNeuralInformationProcessingSystems,
35:36533‚Äì36545,2022.
Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geo-
metricdiffusionmodelformolecularconformationgeneration.arXivpreprintarXiv:2203.02923,
2022.
MinkaiXu,AlexanderSPowers,RonODror,StefanoErmon,andJureLeskovec. Geometriclatent
diffusionmodelsfor3dmoleculegeneration. InInternationalConferenceonMachineLearning,
pp.38592‚Äì38610.PMLR,2023.
YuningYou,RuidaZhou,JiwoongPark,HaotianXu,ChaoTian,ZhangyangWang,andYangShen.
Latent3dgraphdiffusion. InTheTwelfthInternationalConferenceonLearningRepresentations,
2023.
HuiYuan,KaixuanHuang,ChengzhuoNi,MinshuoChen,andMengdiWang.Reward-directedcon-
ditional diffusion: Provable distribution estimation and reward improvement. In Thirty-seventh
ConferenceonNeuralInformationProcessingSystems,2023.
Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro
Sanchez-Gonzalez,PeterBattaglia,RazvanPascanu,andJonathanGodwin. Pre-trainingviade-
noisingformolecularpropertyprediction. arXivpreprintarXiv:2206.00133,2022.
Cai Zhou, Xiyuan Wang, and Muhan Zhang. Latent graph diffusion: A unified framework for
generationandpredictionongraphs. arXivpreprintarXiv:2402.02518,2024.
Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng
Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework.
2023.
14Preprint. Underreview.
A MORE RELATED WORKS
Pre-trainingforMolecularEncoders Learningmeaningfulmolecularrepresentationsiscrucial
fordownstreamtaskslikemolecularpropertyprediction(Fangetal.,2022). Theparadigmofpre-
trainingonlarge-scaledatasetsfollowedbyfine-tuningonsmallerdatasetshasbeenshowntosig-
nificantlyenhancemodelperformanceinbothvisionandlanguagedomains(Kenton&Toutanova,
2019;Brown,2020;Dosovitskiy,2020),andbuildingonthissuccess,recentstudieshaveproposed
self-supervisedpre-trainingmethodsforpoint-cloud-formattedmolecules,aimingtoachievesimilar
performance gains (Zhou et al., 2023; Feng et al., 2023; Liu et al., 2022a; Fang et al., 2022; Jiao
etal.,2024;Nietal.,2024). Typicalproxytasksinvolvemaskingandrecoveringatomtypes,bond
lengths, and bond angles (Fang et al., 2022; Zhou et al., 2023). However, since molecules reside
in continuous 3D space, a more effective approach is to introduce carefully designed noise to the
coordinates and train the model to denoise it. Examples include isotropic Gaussian noise (Zaidi
et al., 2022; Zhou et al., 2023), Riemann-Gaussian noise (Jiao et al., 2023), and complex hybrid
noise(Nietal.,2024;Fengetal.,2023;Jiaoetal.,2024). Notably,Zaidietal.(2022)demonstrated
thatdenoisingequilibriumstructurescorrespondstolearningtheforcefield,therebyproducingrep-
resentationsthatarebothphysicallyandchemicallyinformative.
B ALGORITHMS
Parallel Training and Sequential Sampling We provide the high-level training and sampling
algorithmforGeoRCGinAlgorithm1.
Algorithm1ParallelTrainingandSequentialSamplingforGeoRCG
Input:MoleculedatasetDmol ‚äÇ(cid:83)+‚àû (cid:0)RN√ó3√óRN√ód(cid:1) ,pre-trainedgeometricencoderE,initial
train N=1
representation generator p (r) and molecule generator p (M|r), classifier-free guidance coeffi-
œÜ0 Œ∏0
cientw,temperatureT.
Output: Trainedrepresentationgeneratorp (r)andmoleculegeneratorp (M|r),moleculesam-
œÜ Œ∏
plesfromp (M).
œÜ,Œ∏
ParallelTraining
1: Pre-process to obtain the representation dataset Drep = {{(E(M),N(M))|M ‚àà Dmol}} and
train train
themol-repdatasetDmol-rep ={{(E(M),M)|M‚ààDmol}}.
train train
2: (Parallelly)TraintherepresentationgeneratorwithDrep withlossL definedinEquation(1);
train rep
Train the molecule generator with Dmol-rep with loss L defined in Equation (2), along with
train mol
representationperturbationintroducedinAppendixB.
SequentialSampling
1: Samplearepresentationr ‚àó ‚àºp œÜ(r)withtemperaturecontrolT.
2: SampleamoleculeM ‚àó ‚àºp Œ∏(M|r ‚àó),conditionallywithclassifier-freeguidancew.
Return: Trained representation generator p (r), molecule generator p (M|r), and generated
œÜ Œ∏
moleculesample.
Representation Perturbation Unlike typical conditional training scenarios, GeoRCG faces a
unique challenge: the representations that condition the molecule generator during training may
notalwayscoincidewiththosegeneratedbytherepresentationgeneratorduringthesamplingstage.
This issue is particularly pronounced in molecular generation than image case (Li et al., 2023),
wherepre-trainedencodersaretypicallynottrainedonthatlargedatasetswithadvancedregulariza-
tiontechniqueslikeMoCov3(Chenetal.,2021). Consequently,themoleculegeneratorissuscep-
tible to overfitting to the training representations, as evidenced by our preliminary experiments on
QM9moleculegenerationshowninTable5.
WefindthatapplyingasimplewaybyperturbingthegeometricrepresentationwithGaussiannoise
œÉ œµ (where œµ ‚àº N(0,I) and œÉ is a small variance) during the molecule generator‚Äôs training
rep rep
is particularly effective. Formally, after sampling a data point (E(M),M) from Dmol-rep, we use
train
(M,E(M) + œÉ œµ) for training. Ablation study in Appendix D show this simple method can
rep
effectivelypreventoverfittingandensurethatperformanceonnovelrepresentationsmatchesthose
fromthetrainingdataset.
15Preprint. Underreview.
Table 5: Quality of molecules generated by GeoRCG trained on the QM9 dataset without using
the representation perturbation technique, comparing different representation sources. ‚ÄúTraining
Dataset‚Äù refers to representations sampled from Drep , while ‚ÄúSampled‚Äù refers to representations
train
generatedbythetrainedrepresentationgeneratorp (r). SeeAppendixCfordetaileddescriptions
œÜ
ofthemetrics.
Metrics
MolSta(%)(‚Üë) Valid(%)(‚Üë)
Repsource
TrainingDataset 93.20(0.50) 97.07(0.32)
Sampled 86.93(0.50) 89.12(0.21)
Low-Temperature Sampling We adopt the low-temperature sampling algorithm introduced by
Chroma(Ingrahametal.,2023),however,applyingittoanMLP-baseddiffusionmodelratherthan
theequivariantdiffusionmodelthatprocessesgeometricobjectsasChroma.
Theobjectiveoflow-temperaturesamplingistoperturbthelearnedrepresentationdistributionp (r)
œÜ
byrescalingitwithaninversetemperaturefactor, 1, whereT isatunabletemperatureparameter
T
1
duringsampling,andfinallyenablessamplingfromZ Tp œÜT ,whereZ
T
isanormalizationconstant.
ThemethodproposedinChroma(Ingrahametal.,2023)scalesthescoreœµ estimatedateachdiffu-
t
siontimestepusingatime-dependentfactorŒª . Theapproachisderivedfromandhastheoretical
t
guarantees for simplified toy distributions, and its performance on complex distributions, though
lackingstrictguarantees,hasshownconsistentresultswhencombinedwithannealedLangevinsam-
pling(Songetal.,2021).Herewebrieflyintroduceitforself-containess,andrecommendthereaders
toIngrahametal.(2023)fordetailedderivationandillustration.
ConsiderthevanillareverseSDEusedinDDPMsampling(VPformulation)(Songetal.,2021):
1 (cid:112)
dr =‚àí Œ≤ r‚àíŒ≤ ‚àá logq (r)dt+ Œ≤ dw¬Ø, (5)
2 t t r t t
wherew¬Ø isareverse-timeWienerprocess,q (r)denotestheground-truthrepresentationdistribution
t
attimet,andŒ≤ representsthetime-dependentdiffusionschedule. Toincorporatelow-temperature
t
sampling,weutilizethefollowingHybridLangevinReverse-timeSDE:
(cid:18) (cid:19)
dr
=‚àí1
Œ≤ r‚àí Œª +
Œª 0œà
Œ≤ ‚àá logq
(r)dt+(cid:112)
Œ≤ (1+œà)dw¬Ø, (6)
2 t t 2 t r t t
where Œª t is a time-dependent temperature parameter defined as Œ±2 t+(1Œª ‚àí0 Œ±2 t)Œª0, with Œª 0 = T1. Œ± t
satisfies 1Œ≤ = dlogŒ±t. Theparameterœà controlstherateofLangevinequilibrationperunittime,
2 t dt
andasshowninIngrahametal.(2023),ithelpsalignmoreeffectivelywiththereweightingobjective
incomplexdistributions. Inourimplementation,weemploytheexplicitannealedLangevinprocess
(thecorrectorstepfrom(Songetal.,2021))toachievesimilarresults,withthecorrectorstepnumber
fixedto5.
Classifier-FreeGuidance Weemploytheclassifier-freeguidancealgorithm,asintroducedin(Ho
& Salimans, 2022), for our molecule generator. Specifically, we introduce a trainable ‚Äúfake‚Äù rep-
resentation, denoted as l, which serves as the unconditional signal. During the training phase, l is
initialized as learnable parameters, and with a probability of p , the true representation r is re-
fake
placed by l. This ensures that the model is capable of generating molecules unconditionally, i.e.,
p (M|l)approximatesq(M). Duringsampling,thefinalscoreestimateproducedbythemolecule
Œ∏
generatorisadjustedusingtheformula(1+w)f (M ,t,r)‚àíwf (M ,t,l),allowingflexiblecon-
Œ∏ t Œ∏ t
troloverthestrengthoftherepresentationguidance.
C EXPERIMENT DETAILS
Metrics and Baseline Descriptions We adopt the evaluation metrics, guidelines and baselines
commonlyusedinprior3Dmoleculargenerativemodelstoensureafaircomparison(Hoogeboom
etal.,2022).
16Preprint. Underreview.
‚Ä¢ Intheunconditionalsetting,weassessthegeneratedmoleculesusingseveralkeymetrics:
‚Äì AtomStability: Theproportionofatomswithcorrectvalency.
‚Äì Molecule Stability: The proportion of molecules where all atoms within the molecule
arestable.
‚Äì Validity: The proportion of molecules that can be converted into valid SMILES using
RDKit.
‚Äì Validity&Uniqueness:Theproportionofuniquemoleculesamongthevalidmolecules.
Followingpriorworkondirect3DmolecularconformationsM=(x,h)(Hoogeboometal.,
2022), weinferbondinformationbasedonatomtypesandbondlengthsusinglook-uptable
methods.
Following the approach of Hoogeboom et al. (2022); Vignac & Frossard (2021), we do not
report Novelty scores in the main text, since QM9 represents an exhaustive enumeration of
molecules satisfying a predefined set of constraints, therefore, ‚Äúnovel‚Äù molecule would nec-
essarily violate at least one of these constraints, which indicates that a model fails to fully
capture the properties of the dataset. Consistent with Hoogeboom et al. (2022), we observe
thatthenoveltyscoredecreasesasstabilityandvalidityscoresimprove,eventuallystabilizing
atapproximately60%. ThisiscomparabletothefinalnoveltyscoreofEDM,whichisaround
65%(seeTable8).
Whencomparingwith2D&3Dmodels,weevaluatetwo3DmetricsintroducedbyMiDi(Vi-
gnacetal.,2023),whichdirectlyassessthegeometrylearningability:
‚Äì BondLengthW1: Theweighted1-Wassersteindistancebetweenthebond-lengthdistri-
butionsofthegeneratedmoleculesandthetrainingdataset,withweightscorresponding
todifferentbondtypes.
‚Äì BondAngleW1: Theweighted1-Wassersteindistancebetweentheatom-centeredangle
distributionsofthegeneratedmoleculesandthetrainingdataset, withweightsbasedon
atomtypes.
Theyareformallydefinedas:
BondLengthsW1= (cid:88) qY(y)W1(DÀÜ (y),D (y)), (7)
dist dist
y‚ààbondtypes
whereqY(y)istheproportionofbondsoftypey inthetrainingset,DÀÜ (y)isthegenerated
dist
distribution of bond lengths for bond type y, and D (y) is the corresponding distribution
dist
fromthetestset. And
BondAnglesW1= (cid:88) qX(x)W1(DÀÜ (x),D (x)), (8)
angles angles
x‚ààatomtypes
whereqX(x)denotestheproportionofatomsoftypexinthetrainingset,restrictedtoatoms
withtwoormoreneighbors,andD (x)representsthedistributionofgeometricanglesof
angles
theform‚à†(r ‚àír ,r ‚àír ),whereiisanatomoftypex,andkandj areneighborsofi.
k i j i
‚Ä¢ Intheconditionalgenerationsetting,asdescribedin(Hoogeboometal.,2022),weevaluateour
approachontheQM9datasetacrosssixproperties: polarizabilityŒ±, orbitalenergiesŒµ ,
HOMO
Œµ , and their gap ‚àÜŒµ, dipole moment ¬µ, and heat capacity C . The generative model is
LUMO v
trained conditionally on the second half of the QM9 dataset, and an EGNN (Satorras et al.,
2021)classifier,trainedonthefirsthalf,isemployedtoevaluatetheMAEpropertyerrorofthe
generatedsamples.
ThreebaselinesareadoptedinTable3:
‚Äì QM9(lowerbound): ThemeanerrorofaclassifiertrainedonthefirsthalfoftheQM9
datasetandevaluatedonthesecondhalf. Thisbaselinerepresentstheinherentbias/error
of the classifier, setting a lower bound for model performance and reflecting the best
possibleperformanceamodelcanachieve.
‚Äì Random: Theclassifier‚ÄôsperformancewhenevaluatedonthesecondhalfofQM9with
randomly shuffled molecule property labels. This baseline provides an upper bound,
representingtheworstachievableperformance.
‚Äì N atoms: The performance of a classifier trained exclusively on the number of atoms
N andevaluatedusingonlyN asinput. Thisbaselinecapturestheintrinsicrelationship
betweenmolecularpropertiesandthenumberofatoms,whichagenerativemodelmust
surpasstodemonstrateeffectiveness.
17Preprint. Underreview.
ModelArchitectures,HyperparametersandTrainingDetails
‚Ä¢ RepresentationGenerator. Weusethesamearchitecturefortherepresentationgenerator
as the MLP-based diffusion model proposed in Li et al. (2023). We use 18 blocks of
residualMLPlayerswith1536hiddendimensions,1000diffusionsteps,andalinearnoise
scheduleforŒ≤ . Therepresentationgeneratoristrainedfor2000epochswithabatchsize
t
of128forboththeQM9andDRUGdatasets. TrainingonQM9takesapproximately2.5
days on a single Nvidia 4090, while training on DRUG takes around 4 days on a single
Nvidia A800. Training time can indeed be further reduced, as the model shows minimal
progressafterapproximatelyhalfofthereportedtime.
‚Ä¢ MoleculeGenerator. WeadoptEDM(Hoogeboometal.,2022)asthebasemoleculegen-
erator,usingthesameEGNN(Satorrasetal.,2021)architecture,withtheexceptionofthe
conditioningmodule. Specifically,weincorporateacross-attention(Vaswani,2017)mod-
uleforbetterconditioningonrepresentations,placingitbetweeneveryblockoftheEGNN
toenhanceregularizationandincreaseexpressiveness.FortheEGNNhyperparameters,we
use9layerswith256hiddendimensionsforQM9and4layerswith256hiddendimensions
for DRUG. The number of diffusion steps is set to 1000 (except for cases in Table 4 that
generatemoleculeswithfewersteps),andweemploythepolynomialschedulerforŒ±(M).
t
Notably, all model hyperparameters are identical to those in EDM for fair comparison.
Duringtraining,weuseabatchsizeof128and3000epochsonQM9,andabatchsizeof
64and20epochsonDRUG.TherepresentationperturbationvaluesaresettoœÉ = 0.3
rep
onQM9andœÉ = 0.5onDRUG.Trainingtakesapproximately6daysonQM9usinga
rep
singleNvidia4090,andaround10daysonDRUGusingtwoNvidiaA800GPUs.
‚Ä¢ SamplingDetails. Duringrepresentationsampling,weusethepredictor-correctormethod
from Song et al. (2021) for VP SDE to achieve better equilibrium distributions, which
facilitateslow-temperaturesampling.Formoleculesampling,theprocessremainsthesame
asinEDM,withtheadditionofclassifier-freeguidance. Fortheunconditionalgeneration
results in Table 1, we use (w,T) = (1.0,1.0) in the QM9 dataset, except for the results
in Figure 4, and (w,T) = (0.0,0.5) in the GEOM-DRUG dataset (note that w = 0.0
indicatesthatwedonotuseclassifier-freeguidance, notthattherepresentationcondition
itself is not applied). Further tuning of these two hyperparameters may lead to improved
results.
Evaluation of LDM-3DG (You et al., 2023) We evaluate the performance of LDM-3DG (You
etal.,2023),anAuto-Encoder-basedmethodthatalsoleveragesthecompactnessoftherepresenta-
tionspacetoachievegoodperformance.
‚Ä¢ FortheunconditionalresultsinTable1,weutilizethe3Dconformationsunconditionally
generated by LDM-3DG (You et al., 2023) and compute the bond information using the
look-uptablemethodfromEDM(Hoogeboometal.,2022). Notably,althoughLDM-3DG
predicts both the 2D molecular graph and the 3D conformation, we do not use the bond
informationitpredictsforthefollowingreasons:
1. For the calculation of 3D geometry statistics, we observe significant inconsistencies
betweenthegenerated2Dgraphsand3Dgeometries(e.g.,validmoleculeswithbond
lengths exceeding 100m), leading to unreliable statistics (e.g., BondLengthW1 ex-
plodingto3900).
2. For stability and validity metrics, which are fundamentally 2D and computed based
on molecular graphs (atoms and bond types), using the generated 2D graph would
ignorethecontributionofthe3Dmodule,preventinganevaluationofits3Dlearning
performance.
3. Most critically, their 2D module is explicitly designed to filter out invalid (sub-
)molecules during generation using the RDKit method. This means that if invalid
moleculesorsub-moleculesaregenerated,theyareregenerated. Thisexplicitfiltering
deviatesfromourstandardevaluationcriteriaandisunsuitableforafaircomparison.
‚Ä¢ FortheconditionalresultsinTable3,wefirstnoteapotentialissuewithLDM-3DG(You
et al., 2023): The model cannot explicitly specify the node number N during molecule
generation, as it uses an auto-regressive 2D generator that automatically stops adding
18Preprint. Underreview.
atoms/motifswhendeemedsufficient. However,theevaluationinTable3requiresspecify-
ingbothN andpropertyc,followingtheground-truthdistributionq(N,c)fromthetraining
dataset. Toensurefairevaluation, conditionsfeedingtoLDM-3DGmustalsosatisfythis
distribution.Astheauthorsclaimthemodelcanimplicitlylearnq(N)andthusq(N|c),we
firstsample10,000valuesfromq(c)andfeedthemtoLDM-3DG,expectingittoinferN
fromcimplicitlyasargued,andthusmatchingtheq(N,c)conditions.
D ADDITIONAL EXPERIMENT RESULTS
Ablation Study: Representation Encoders Geometric representations play a pivotal role in
GeoRCG.Toevaluatetheimportanceofrepresentationquality,weconductanablationstudycom-
paring the quality of molecule samples generated by GeoRCG trained under different geometric
encoderconfigurations.
Wefirstassessthebenefitsprovidedbythepre-trainingstage.Specifically,weutilizethepre-trained
encoderFrad(Fengetal.,2023), trainedonthePCQM4Mv2dataset(Nakata&Shimazaki,2017)
withahybridcoordinatesdenoisingtask(Fengetal.,2023).Thisapproachhasbeenproventoequiv-
alentlylearnforcefields(Fengetal.,2023;Zaidietal.,2022),andisthereforeexpectedtoproduce
informative representations that capture high-level molecular information. We train GeoRCG us-
ingrepresentationsfromawell-pretrainedFradandaFradwithrandomlyinitializedweights. The
molecule generation quality on QM9, as shown in Table 6, clearly underscores the critical role of
pre-training on large datasets with advanced techniques in improving representation quality, ulti-
matelyenhancingGeoRCG‚Äôsperformance.
Table 6: Quality of molecules generated by GeoRCG with different encoders trained on the QM9
dataset. ‚ÄúRandom‚Äùindicatesthattheweightswereinitializedrandomlywithoutanypre-training.
Metrics
AtomSta(%) MolSta(%) Valid(%) Valid&Unique(%)
Encoder
RandomEnc 98.55(0.01) 78.66(0.07) 94.68(0.09) 55.99(0.83)
PretrainedEnc 99.10(0.02) 92.15(0.23) 96.48(0.08) 92.45(0.21)
Next,weinvestigatetheimpactofdifferentpre-trainedencoders,whichcouldvaryinmodelstruc-
ture and proxy tasks used for pre-training. Specifically, we compare Unimol (Zhou et al., 2023),
whichemploysamessage-passingneuralnetworkframeworkincorporatingdistancefeatures(i.e.,
DisGNNin(Lietal.,2023;2024b))andprimarilyusesnaivecoordinatesdenoising,withFrad(Feng
et al., 2023), which adopts TorchMD (Tho¬®lke & De Fabritiis, 2022) as the backbone and utilizes
carefullydesignedhybrid-denoisingtasks. BothUnimol(Zhouetal.,2023)andFrad(Fengetal.,
2023)arepre-trainedontheGEOM-DRUGdatasetuntilconvergence. WevisualizetheT-SNEof
therepresentationsgeneratedforGEOM-DRUG.AsshowninFigure6, theT-SNEoftheUnimol
representations exhibits a clearer clustering pattern based on node numbers compared to the Frad
representations, which may suggest better representation learning. To further investigate, we uti-
lizebothencoderstotrainGeoRCGandsubsequentlyevaluatethequalityofmoleculegeneration.
TheFrad-basedGeoRCGachievesaValidityof96.9(0.44)andAtomStabilityof84.4(0.27),while
the Unimol-based GeoRCG achieves a Validity of 98.5(0.12) and Atom Stability of 84.3(0.12):
Although the Frad-based GeoRCG produces slightly higher atom stability, its high variance and
significantly lower validity suggest inferior performance. These findings, along with our main re-
sults,offerinsightsintothetypesofrepresentationsmoreeffectiveforguidingmoleculegeneration,
suggestingthatsensitivitytomoleculesizemaybeacriticalfactor.
AblationStudy: RepresentationPerturbation AsdiscussedinAppendixB,weinvestigatethe
effectiveness of the straightforward representation perturbation technique by introducing random
noisetoperturbtherepresentationsduringtraining.Additionally,weapplyextradropoutinthecon-
ditioningmoduleofourmoleculegeneratortomitigateoverfittingontherepresentationconditions.
Ablation experiments presented in Table 7 demonstrate the efficacy of these simple yet impactful
methods.
Quality of Conditionally Generated Molecules Detailed molecular metrics for conditionally
generatedmoleculesareprovidedinTable8. Forcomparison,wealsoincludethestabilitymetrics
19Preprint. Underreview.
Figure 6: T-SNE visualization of representations produced by the pre-trained encoders for the
GEOM-DRUGdataset,coloredbynodenumber. TheleftplotcorrespondstoUnimol(Zhouetal.,
2023),andtherightplotcorrespondstoFrad(Fengetal.,2023).
Table7: QualityofmoleculesgeneratedbyGeoRCGtrainedontheQM9dataset,withandwithout
representationperturbationandrepresentationconditiondropout.
Metrics
AtomSta(%) MolSta(%) Valid(%) Valid&Unique(%)
Hyper-parameters
repnoise‚úó cond.dropout‚úó 98.53(0.08) 86.93(0.5) 93.69(0.09) 89.12(0.21)
repnoise‚úó cond.dropout‚úì 98.62(0.08) 87.9(0.35) 94.64(0.18) 90.15(0.02)
repnoise‚úì cond.dropout‚úó 99.05(0.01) 91.69(0.08) 96.48(0.11) 92.38(0.12)
repnoise‚úì cond.dropout‚úì 99.10(0.02) 92.15(0.23) 96.48(0.08) 92.45(0.21)
ofmoleculesconditionallygeneratedbyEDM,whichhighlightanotableimprovementinstability
withGeoRCG.Specifically,EDM‚Äôsstabilityscoresare: Œ±(80.4%),‚àÜŒµ(81.73%),Œµ (82.81%),
HOMO
Œµ (83.6%),¬µ(83.3%),andC (81.03%).
LUMO v
Table8: SupplementaryevaluationofconditionallygeneratedmoleculesfromGeoRCG.Theright
sidereportsmetricsforunconditionallygeneratedmoleculesfromothermethodsforreference.Note
thatconditionalmodels(left)weretrainedonhalfoftheQM9dataset,whileunconditionalmodels
(right) were trained on the full dataset, which may account for slight decreases in stability and
validitymetrics.
Œ± ‚àÜŒµ ŒµHOMO ŒµLUMO ¬µ Cv Ours EDM GeoLDM EquiFM
AtomSta(%) 98.93(0.06) 98.84(0.02) 98.81(0.04) 98.85(0.02) 98.85(0.02) 98.8(0.03) 99.12(0.03) 98.7 98.9(0.1) 98.9(0.1)
MolSta(%) 88.89(0.51) 88.83(0.25) 88.50(0.09) 89.04(0.09) 88.66(0.15) 88.7(0.29) 92.32(0.06) 82 89.4(0.5) 88.3(0.3)
Valid(%) 94.85(0.42) 94.83(0.06) 94.84(0.15) 95.01(0.15) 94.82(0.11) 94.95(0.16) 96.52(0.2) 91.9 93.8(0.4) 94.7(0.4)
Valid&Unique(%) 90.31(0.58) 90.42(0.04) 90.44(0.28) 90.73(0.12) 90.52(0.18) 90.65(0.19) 92.45(0.24) 90.7 92.7(0.5) 93.5(0.3)
Valid&Unique&Novelty(%) 71.38(0.46) 72.23(0.58) 71.95(0.30) 71.79(0.41) 72.47(0.59) 72.53(0.72) 61.32(0.77) 65.7 58.1 57.4
E THEORETICAL ANALYSIS
In this section, we provide rigorous theoretical analysis on representation-conditioned diffusion
models. Ourtheoryisnotlimitedtomoleculegeneration, andisthefirsttheoreticalbreakthrough
fortheRCGframework(Lietal.,2023).
Our analysis is organized as follows. In Appendix E.1, we analyze the generation bound of
representation-conditioneddiffusionmodelsinunconditionalgenerationtasksbyshowing: (i)the
representationcanbewellgeneratedbythefirst-stagediffusionmodelwithmildassumptions(Ap-
pendix E.1.1); (ii) the second-stage representation-conditioned diffusion model exhibits no higher
generalizationerrorthantraditionalone-stagediffusionmodel,andcanarguablyachievelowerer-
rorleveragingtheinformativerepresentations(AppendixE.1.2). TheninAppendixE.2,weanalyze
conditionalgenerationtasksasfollows: (iii)undermildassumptionsofrepresentationsandtargets,
weprovidenovelboundforscoreestimationerror(AppendixE.2.1);(iv)generatedrepresentations
haveprovablerewardimprovementtowardsthetarget, withthesuboptimalitycomposedofoffline
regression error and diffusion distribution shift (Appendix E.2.2), thus would improve the second
stageofconditionalgeneration(AppendixE.2.3).
20Preprint. Underreview.
Notations. In this section, we use SDE and score matching formulations of diffusion models to
present our theoretical results, given their equivalence with the DDPM family (Song et al., 2021).
We consider the random variable x ‚àà Rd, and use q(¬∑) to denote the ground truth distributions,
p(¬∑) to denote the posterior distribution predicted by diffusion models. For instance, q(x) is the
groundtruthdistributionoftheunderlyingdatax,whilep (r)isthepredicteddistributionoflatent
œÜ
representations. We use T to denote the total time of diffusion models, and N to represent the
discretizationstepnumber.WeconsideraSDEwithcontinuoustime[0,T],aswellasitsdiscretized
DDPM which has N diffusion steps with step size h := T/N. The forward process is denoted
as (x ) ‚àº q , and the reverse process is denoted as (x¬Ø ) ‚àº p . If the reverse process
t t‚àà[0,T] t t t‚àà[0,T] t
is predicted by the score matching network, we use its parameters as the subscript. Please note
thattherearetwodifferentinitializationofthereverseprocess: theendofforwardprocessq and
T
standardGaussiannoiseŒ≥d. Weusesuperscriptq todifferentiatetheformerfromthelatter.
T
E.1 UNCONDITIONALGENERATION
E.1.1 PROVABLEGENERATIONOFREPRESENTATIONS
Recall the two-stage generation process of representation-conditioned generation: p(x,r) =
p (x|r)p (r). Toquantitativelyevaluatethegenerationprocess,weconsidertwostagesseparately.
Œ∏ œÜ
Inthissubsection,wefirstprovidetheoreticalanalysisontheprovablegenerationofrepresentations
p (r).
œÜ
AssumptionE.1. (Secondmomentboundofrepresentations.)
m2 :=E [||r‚àír¬Ø||2]<‚àû (9)
r q(r)
whereq(r)isthegroundtruthdistributionoftherepresentations,andr¬Ø:=E [r].
q(r)
Assumption E.2. (Lipschitz score of representations). For all t ‚â• 0, the score ‚àálnq(r ) is L -
t r
Lipschitz.
whereq(r )isthedistributionofnoisylatentr atdiffusionsteptintheforwardprocess.
t t
Finally, the quality of diffusion models obviously depends on the expressivity of score network œÜ
withpredictions(t).
œÜ
AssumptionE.3. (Scoreestimationerrorofrepresentations). Forallt‚àà[0,T],
E [||s(t)‚àí‚àálnq(r )||2]‚â§œµ2 (10)
q(rt) œÜ t œÜ,score
Thesearesimilarassumptionstotheonesin(Chenetal.,2023).
PropositionE.1. SupposeAssumptionE.1,AssumptionE.2,AssumptionE.3hold,andthestepsize
h:=T/N satisfiesh‚™Ø1/L . Thenthefollowingholds,
r
(cid:113) (cid:112) ‚àö ‚àö
TV(p œÜ(r 0),q(r))‚™Ø KL(q(r)||Œ≥dr)exp(‚àíT)+(L
r
d rh+L rm rh) T + œµ
œÜ,score
T
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
convergenceofforwardprocess discretizationerror scoreestimationerror
(11)
Thisisadirectconclusionfrom(Chenetal.,2023). IntypicalDDPMimplementation,wechoose
h = 1 and thus T = N. Remarkably, Proposition E.1 indicates the benefit of generating the
representationfirst: sinced ‚â™d,thegenerationquality(measuredbytheTVdistanceinProposi-
r
tionE.1)ofthelow-dimensionalrepresentationcaneasilyoutperformdirectlygeneratingthehigh-
dimensional data points x. The theorem also accounts for applying a lightweight MLP as the de-
noisingnetworkwhileinthestageofgeneratingtherepresentation.
E.1.2 PROVABLESECOND-STAGEGENERATION
Tractable Training Loss. Now we analyze the generation quality of the second-stage diffusion
model. Since we sample from p (x,r), we have representations as conditions even for uncondi-
Œ∏
tionalgenerationtasks. Tolearnthescorefunctionconditioningontherepresentations,considerthe
21Preprint. Underreview.
followinglossforscorematching,
(cid:90) T
L(Œ∏)= Œª(t)E [||s (x ,r,t)‚àí‚àá logq (x |r)||2]dt (12)
xt,r Œ∏ t xt t|r t
0
However,sinceq (x |r)isintractable,weusethefollowingequivalentlosses:
t|r t
L(Œ∏)=(cid:90) T Œª(t)E (cid:104) E (cid:2) ||s (x ,r,t)‚àí‚àá logq (x |x )||2|x (cid:3)(cid:105) dt+C (13)
x0,r xt|x0 Œ∏ t xt t|0 t 0 0
0
PropositionE.2. (Tractablerepresentation-conditionedscorematchingloss.)
(cid:90) T
L(Œ∏):= Œª(t)E [||s (x ,r,t)‚àí‚àá logq (x |r)||2]dt (14)
xt,r Œ∏ t xt t|r t
0
=(cid:90) T Œª(t)E (cid:104) E (cid:2) ||s (x ,r,t)‚àí‚àá logq (x |x )||2|x (cid:3)(cid:105) dt+C (15)
x0,r xt|x0 Œ∏ t xt t|0 t 0 0
0
Proof. Thekeyisthefollowingimportantpropertyholdssincethegradientistakenw.r.t. x only:
t
‚àá logq (x |r)=‚àá logq (x ,r) (16)
xt t|r t xt t,r t
The remaining of the derivation parallels to traditional DDPM. We can replace ‚àá logq (x ,r)
xt t,r t
with‚àáx logq (x ,r|x ):
t t,r|0 t 0
(cid:104) (cid:12) (cid:105)
‚àáx logq (x ,r)=E ‚àá logq (x ,r|x )(cid:12)x (17)
t t,r t x0,r|xt xt t,r|0 t 0 (cid:12) t
Thus,
E E [||s (x ,r,t)‚àí‚àá logq (x |r)||2] (18)
r xt‚àºq(xt|r) Œ∏ t xt t|r t
=E E E [||s (x ,r,t)‚àí‚àá logq (x |x ,r)||2] (19)
r x0‚àºq(x0|r) xt‚àºq(xt|r,x0) Œ∏ t xt t|r t 0
=E E E [||s (x ,r,t)‚àí‚àá logq (x |x )||2] (20)
r x0‚àºq(x0|r) xt‚àºq(xt|x0) Œ∏ t xt t|r t 0
whichisequivalenttoourtractablescorematchingloss.
RigorousErrorBoundforSecond-StageGeneration. UtilizingPropositionE.2,analysisofthe
second-stage diffusion parallels to the first stage, except that the score network takes additional
inputsr.
AssumptionE.4. (Secondmomentboundofmoleculefeatures.)
m2 :=E [||x‚àíx¬Ø||2]<‚àû (21)
x q(x)
whereq(x)isthegroundtruthdistributionofthemoleculefeatures,andx¬Ø:=E x.
q(x)
Assumption E.5. (Lipschitz score of second stage). For all t ‚â• 0, the score ‚àálnq(x ) is L -
t x
Lipschitz.
whereq(x )isthedistributionofnoisylatentx atdiffusionsteptintheforwardprocess.
t t
Finally,wemakesomeassumptionsofthescorenetworkestimationerror.
AssumptionE.6. (Scoreestimationerrorofsecond-stagediffusion). Forallt‚àà[0,T],
E [||s (x ,t,r)‚àí‚àálnq (x )||2]‚â§œµ2 (22)
r‚àºpœÜ(r),xt‚àºqt(xt) Œ∏ t t t Œ∏,score
This assumption contains the error brought by generating representations, i.e., the TV distance
showninPropositionE.1. LaterinTheoremE.1weexplicitlydealwiththeerrorbroughtbyrepre-
sentationgeneration,whichresultsinamorefine-grainederrorbound.
WenowpresentakeylemmawhichfacilitatesanalysisandtheproofofthecentralTheoremE.1.
22Preprint. Underreview.
Lemma E.1. Suppose Assumption E.4, Assumption E.5, Assumption E.6 hold, and the step size
h := T/N satisfies h ‚™Ø 1/L . Suppose we sample x ‚àº p (x|r) from Gaussian noise where
x Œ∏
r ‚àºp (r),anddenotethefinaldistributionofxasp (x). Thenthefollowingholds,
œÜ Œ∏,œÜ
(cid:113) ‚àö ‚àö
(cid:112)
TV(p (x),q(x))‚™Ø KL(q(x)||Œ≥d+3)exp(‚àíT)+(L (d+3)h+L m h) T + œµ T
Œ∏,œÜ x x x Œ∏,score
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
convergenceofforwardprocess discretizationerror scoreestimationerror
(23)
(cid:82)
Proof. Recallthenotationthatp (x) := p (x |r)p (r)dr = p predictedbydenoisingnet-
Œ∏,œÜ r 0|r 0 r 0
worksŒ∏,œÜstartingfromGaussiannoiseŒ≥d+3. ConsiderthereverseprocesspqT(x )startingfrom
0 0
q insteadofŒ≥d+3,
T
TV(p ,q(x))‚â§TV(p ,pqT)+TV(pqT,q ) (24)
0 0 0 0 0
UsingtheconvergenceoftheOUprocessinKLdivergence(see(Chenetal.,2023)),thefollowing
holdsforthefirstterm,
(cid:113)
TV(p ,pqT)‚â§TV(Œ≥d+3,q )‚â§ KL(q(x)||Œ≥d+3)exp(‚àíT) (25)
0 0 T
Thesecondtermiscausedbyscoreestimationerroranddiscretizationerror,whichcanbebounded
by
TV(pqT,q )2 ‚â§KL(q ||pqT)‚™Ø(œµ2 +L2(d+3)h+L2m2h2)T (26)
0 0 0 0 Œ∏,score x x x
WestartprovingEquation(26)byproving
N‚àí1 (cid:90) (k+1)h
(cid:88) E ||s(kh)(x ,kh,r)‚àí‚àálnq (x )||2dt‚™Ø(œµ2 +L2(d+3)h+L2m2h2)T
q0,r‚àºpœÜ Œ∏ kh t t Œ∏,score x x x
k=0 kh
(27)
Fort‚àà[kh,(k+1)h],wedecompose
E [||s(kh)(x ,kh,r)‚àí‚àálnq (x )||2] (28)
q0,r‚àºpœÜ Œ∏ kh t t
‚™ØE [||s(kh)(x ,kh,r)‚àí‚àáq (x )||2]+E [||‚àáq (x )‚àí‚àáq (x )||2] (29)
q0,r‚àºpœÜ Œ∏ kh kh kh q0 kh kh t kh
+E [||‚àáq (x )‚àí‚àáq (x )||2] (30)
q0 t kh t t
‚™Øœµ2 Œ∏,score+E q0(cid:104)(cid:12) (cid:12)(cid:12) (cid:12)‚àálnq qkh(x kh)(cid:12) (cid:12)(cid:12) (cid:12)2(cid:105) +L2E q0[||x kh‚àíx t||2] (31)
t
Notethatweomitthetermrinexpectationoflasttwotermsbecausetheyareindependentofr.
UtilizingLemma16from(Chenetal.,2023),webound
(cid:12) (cid:12)(cid:12) (cid:12)‚àálnq qkh(x kh)(cid:12) (cid:12)(cid:12) (cid:12)2 ‚™ØL2 x(d+3)h+L2 xh2||x kh||2+(L2 x+1)h2||‚àálnq t(x kh)||2 (32)
t
Forthelastterm,
||‚àálnq (x )||2 ‚™Ø||‚àálnq (x )||2+||‚àálnq (x )‚àí‚àálnq (x )||2 (33)
t kh t t t kh t t
‚™Ø||‚àálnq (x )||2+L2||x ‚àíx ||2 (34)
t t kh t
wherethesecondtermisabsorbedintothethirdtermofthedecompositionEquation(28). Thus,
E [||s(kh)(x ,kh,r)‚àí‚àálnq (x )||2] (35)
q0,r‚àºpœÜ Œ∏ kh t t
‚™Øœµ2 +L2(d+3)h+L2h2E [||x ||2]+L2h2E [||‚àálnq (x )||2]+L2E [||x ‚àíx ||2]
Œ∏,score x x q0 kh x q0 t t x q0 kh t
(36)
‚™Øœµ2 +L2(d+3)h+L2h2(d+3+m2)+L3(d+3)h2+L2(m2h2+(d+3)h) (37)
Œ∏,score x x x x x x
‚™Øœµ2 +L2(d+3)h+L2h2m2 (38)
Œ∏,score x x x
Analogousto(Chenetal.,2023),usingpropertiesofBrownianmotionsandlocalmartingales,we
can apply Girsanov‚Äôs theorem and complete the stochastic integration. Since q is the end of the
0
reverseSDE,bythelowersemicontinuityoftheKLdivergenceandthedata-processinginequality,
wetakethelimitan+dobtain
KL(q ||pqT)‚™Ø(œµ2 +L2(d+3)h+L2h2m2)T (39)
0 0 Œ∏,score x x x
WefinallyconcludewithPinsker‚Äôsinequality(TV2 ‚â§KL).
23Preprint. Underreview.
Thisresultholdsforgeneralrepresentation-conditioneddiffusionmodels,andtoourbestknowledge
we are the first to provide theories for representation-conditioned generation, which is a general
generationframeworksuitableforvariousdomainssuchasimages(Lietal.,2023)andgraphs.
Lemma E.1 quantitatively characterizes the bound on generalization error in representation-
conditioned diffusion. It directly suggests that the error of representation-conditioned diffusion
willbenohigherthanthatofitsone-stagecounterpart. Thisisbecausethefirsttwocomponentsof
thegeneralizationerror(i.e.,theconvergenceoftheforwardprocessandthediscretizationerror)of
therepresentation-conditioneddiffusionmodelalignwiththoseoftraditionalDDPM,providedthat
bothareparameterizedusingthesamediffusionprocesses.Furthermore,thethirdcomponent(score
estimationerror)canbemadeidenticalifwesimplysetallrepresentation-relevantparametersins
Œ∏
tozeroanddisregardrepresentation‚Äôsimpact. Wethereforehavethefollowingconclusion,
CorollaryE.1. Self-representation-conditioneddiffusionmodelcanhavethesameoralowergen-
erationdistributionerrorthanone-stagediffusionmodel.
Wenowgiveamorefine-grainederrorboundanalysisofrepresentation-conditioneddiffusion,given
therelationshipbetweenr andxthatenablesourfurtherqualitativeanalysisfortheargublybetter
performance.
AssumptionE.7. (representation-conditionedscoreestimationerrorofsecond-stagediffusion).For
allt‚àà[0,T],
E [||s (x ,t,r)‚àí‚àálnq (x |r)||2]‚â§œµ2 (40)
r‚àºpœÜ(r),xt‚àºqt(xt|r) Œ∏ t t t œÜ,Œ∏,cond
Thefollowingmaintheoremisnovelandprecisesinceit(i)dealswiththegenerationerroroffirst-
stage representations explicitly; (ii) takes advantages of the conditional distribution q(x|r) in the
denoisingnetwork.
TheoremE.1. (Theorem3.1inthemaintext)SupposeAssumptionE.4,AssumptionE.5,Assump-
tion E.7 hold, and the step size h := T/N satisfies h ‚™Ø 1/L . Suppose we sample x ‚àº p (x|r)
x Œ∏
from Gaussian noise where r ‚àº p (r), and denote the final distribution of x as p (x). Define
œÜ Œ∏,œÜ
pqT|œÜ,
which is the end point of the reverse process starting from q instead of Gaussian. Here
0 T|œÜ
q istheT-thstepintheforwardprocessstartingfromq := 1 (cid:82) q(x |r)p (r)drwhereAis
T|œÜ 0|œÜ A r 0 œÜ
thenormalizationfactor. Thenthefollowingholds,
(cid:113) (cid:112) ‚àö
TV(p (x),q(x))‚™Ø KL(q ||Œ≥d+3)exp(‚àíT)+(L (d+3)h+L m h) T (41)
Œ∏,œÜ 0|œÜ x x x
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
discretizationerror
convergenceofforwardprocess
‚àö
+ œµ T + TV(q ,q ) (42)
œÜ,Œ∏,cond 0|œÜ 0
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
conditionalscoreestimationerror representationgenerationerror
Proof. TheproofsketchparallelsthatofLemmaE.1,exceptthatinthefirststepwedecomposethe
TVdistanceasfollows,
TV(p ,q(x))‚â§TV(p
,pqT|œÜ)+TV(pqT|œÜ,q
)+TV(q ,q ) (43)
Œ∏,œï 0 0 0 0|œÜ 0|œÜ 0
WecompletetheproofanalogouslytotheproofofLemmaE.1.
Remarkably, when q , i.e., p fully recovers the ground truth marginal distribution of represen-
0|œÜ œÜ
tations q(r), Theorem E.1 has the same format as Lemma E.1 but with œµ < œµ . This
œÜ,Œ∏,cond Œ∏,score
is because the former is the score estimation error based on explicit relationship between x and r
while the latter learns implicitly. Thus, Theorem E.1 is a much tighter bound for representation-
conditionedgeneration. Tothebestofourknowledge, thisisthefirstrigoroustheoreticalanalysis
onRCG(Lietal.,2023). Wenowprovidesomequalitativediscussionsonwhyrepresentationscan
arguablyleadtobettergeneralizationerror.
Typically, representations are powerful (and sometimes even complete) as they encode key infor-
mationaboutxwithpotentialadditionalknowledgeviapretrainingtasks(forexample,coordinates
denoising for molecules (Zaidi et al., 2022; Feng et al., 2023)). Therefore, it is reasonable to ex-
pect that score estimation conditioned on representations can be more accurate (i.e., œµ could
Œ∏,score
besignificantlysmallerthanwhenestimatingthescorewithoutrepresentationconditioning). Ifthe
24Preprint. Underreview.
representations are complete‚Äîwhere a special case would be r = x‚Äîthis would greatly assist in
predicting thenoise. The sameapplies when r can beproperly transformedback to x by aneural
network. Moregenerally, thereareintermediatecaseswherer reflectspartialinformationaboutx
(e.g.,amultisetofatomsandbonds),whichwouldstillaidinimprovingprediction.
To conclude this subsection, we provide a detailed characterization of the generalization error of
representation-conditioned diffusion models. It is important to note that some parameters in our
assumptions, such as Lipschitz scores and estimation errors, are not constants; they are functions
of the SDE total time T and the number of diffusion steps N. Our conclusions also explain
why representation-conditioned generation methods remain competitive even when the number of
second-stagediffusionstepsN isdecreasedforfastergeneration. Thisisbecausethescoreestima-
tionerrorcanremainsmallevenwhenthenumberofdiffusionstepsisreduced,giventheguidance
fromrepresentations. Asaresult,reducingN causesaslowerincreaseinœµ (N)comparedto
œÜ,Œ∏,cond
thescoreestimationerrorwithoutrepresentationconditioning,leadingtorepresentation-conditioned
generativemodels‚Äôstrongperformancewithfewersteps.
E.2 CONDITIONALGENERATION
Inthissubsection,weaimtoprovethatconditionalgenerationusingourrepresentation-conditioned
generation have probable reward improvement. While we used c to denote conditions in the main
text, we use the notation y instead for the targets or ‚Äúreward‚Äù to keep coordinate with existing
literature. Denoteq := q(¬∑|y = a)asthegroundtruthconditionaldistribution,andpÀÜ := p(¬∑|y =
a a
a) the estimated distribution. Suppose the ground truth distribution satisfies y := f‚àó(x,r) which
canbedecomposedas
f‚àó(x,r)=g‚àó(x ,r )+h‚àó(x ,r ) (44)
‚à• ‚à• ‚ä• ‚ä•
wherewedenotex = x whenx ‚àº q(x), r = r whenr ‚àº q(r), andf‚àó(x,r) = g‚àó(x,r)when
‚à• ‚à•
x‚àºq(x),r ‚àºq(r).
To start with, we assume a linear relationship between r and y, which is reasonable thanks to the
powerfulpretrainedmodel(whichmakestherepresentationshelpfulinpredictingmoleculeproper-
ties and even complete) if some noises are allowed. In detail, the reward is f‚àó(x,r) = wÀÜ‚ä§r+Œæ
and Œæ ‚àº N(0,ŒΩ2). In some cases, we may further make Gaussian assumptions on r (WLOG,
r ‚àºN(¬µ,Œ£))butisnotnecessary.
E.2.1 PARAMETRICCONDITIONALSCOREMATCHINGERROR
First, we give a detailed form of the representation score estimation error presented in Assump-
tionE.1undertheassumptionsabove.
LemmaE.2. ForŒ¥ ‚â•0,withprobability1‚àíŒ¥,thescoreestimationerrorœµ ‚âÉœµ isbounded
r œÜ,score
by
(cid:115)
1 (cid:90) T (cid:16)1 N(S, 1)d2log1(cid:17)
E [||‚àálogq (r |y)‚àísÀÜ (r ,y,t)||2]dt‚â§œµ2 =O n r Œ¥
T ‚àít (rt,y)‚àºqt t t œÜ t 2 r t n
0 t0 0
(45)
where t is the early stopping time of the SDE, n is the number of samples, S is the parametric
0
function class of denoising network, and N(S, 1) is the log covering number of S. When S is
n
linearlyparameterized,N(S, 1)=O(d2log(drn)).
n r ŒΩ2
Proof. ThisisadirectextensionofLemmaC.1fromYuanetal.(2023). Notethatweconsiderthe
specialcasewherethelow-dimensionalsubspaceistheoriginalspace(i.e.,A = I andd = D =
dr
d in their paper), and our noised linear assumption between r and y is identical to their pseudo
r
labelingsetting(i.e.,yÀÜ=wÀÜ‚ä§r+ŒæwhereŒæ ‚àºN(0,ŒΩ2)). Weonlyprovidetheproofsketchhere.
WhenrfollowstheGaussiandesign,somealgebragives
Œ±(t) (cid:16) h(t) (cid:17) 1
‚àá logq (r,y)= B Œ±(t)r+ yw ‚àí r (46)
r t h(t) t ŒΩ2 h(t)
(cid:16) (cid:17)‚àí1
where Œ±(t) = exp(‚àít/2),h(t) = 1‚àíexp(‚àít),B(t) = Œ±2(t)I + h(t)ww‚ä§ +h(t)Œ£‚àí1 .
dr ŒΩ2
We then bound the estimation error with PAC-learning concentration argument by using Dudley‚Äôs
25Preprint. Underreview.
entropyintegraltoboundtheRademachercomplexity,andobtain
(cid:115)
(cid:16)1 N(S, 1)d2log1(cid:17)
œµ2 =O n r Œ¥ (47)
r t n
0
Further,thelogcoveringnumberofS underGaussiandesignsatisfies
1 (cid:16) d n (cid:17)
N(S, )‚â§d2log 1+ r (48)
n r t Œª ŒΩ2
0 min
where 0 < Œª < 1 is the smallest eigenvalue of Œ£, and typically the early stopping time t =
min 0
O(1). In (Yuan et al., 2023) the authors assume ŒΩ2 = 1/d which states that the variance ŒΩ2 of
r
regressionresidualsŒæreduceswhenweincreasetherepresentationdimensions,whichisreasonable.
Lemma E.2 provides a detailed score estimation error given the linear assumption between r and
y, which serves as a special case of œµ2 . Substituting it into Proposition E.1, we can obtain a
œÜ,score
quantitativeresultofrepresentationgenerationerror.
E.2.2 REWARDIMPROVEMENTVIACONDITIONALGENERATION
Next,wewanttoobtaintherewardguaranteesofthegeneratedsamplesgivetheconditiony.Define
thesuboptimalityofdistributionP as
SubOpt(P;y‚àó)=y‚àó‚àíE [f‚àó(x,r)] (49)
(x,r)‚àºP
wherey‚àó isthetargetrewardvalue(condition)andf‚àó isthegroundtruthrewardfunction. Weuse
thenotationpÀÜ :=p (r|y‚àó =a),thenwehavethefollowingresultforSubOpt(pÀÜ ;y‚àó =a),which
a œÜ a
canalsobeviewedasaformofoff-policyregret.
LemmaE.3. (Theorem4.6in(Yuanetal.,2023).)
SubOpt(pÀÜ a;y‚àó =a)‚â§E r‚àºqa(cid:104)(cid:12) (cid:12)(wÀÜ‚àíw)‚ä§r(cid:12) (cid:12)(cid:105) +(cid:12) (cid:12) (cid:12)E r‚àºqa[g‚àó(r ‚à•)]‚àíE r‚àºpÀÜa[g‚àó(r ‚à•)](cid:12) (cid:12) (cid:12)+E r‚àºpÀÜa[h‚àó(r ‚ä•)]
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
E1 E2
E3
(50)
Proof. Recallthenotationq :=q(¬∑|y =a),wehave
a
E [f‚àó(r)] (51)
r‚àºpÀÜa
‚â•E r‚àºqa[f‚àó(r)]‚àí(cid:12) (cid:12)E r‚àºpÀÜa[f‚àó(r)]‚àíE r‚àºqa[f‚àó(r)](cid:12) (cid:12) (52)
‚â•E r‚àºqa[fÀÜ(r)]‚àíE r‚àºqa(cid:2)(cid:12) (cid:12)fÀÜ(r)‚àíf‚àó(r)(cid:12) (cid:12)(cid:3) ‚àí(cid:12) (cid:12)E r‚àºpÀÜa[f‚àó(r)]‚àíE r‚àºqa[f‚àó(r)](cid:12) (cid:12) (53)
‚â•E r‚àºqa[fÀÜ(r)]‚àíE r‚àºqa(cid:2)(cid:12) (cid:12)fÀÜ(r)‚àíg‚àó(r)(cid:12) (cid:12)(cid:3) ‚àí(cid:12) (cid:12)E r‚àºqa[g‚àó(r ‚à•)]‚àíE r‚àºpÀÜa[g‚àó(r ‚à•)](cid:12) (cid:12)‚àíE r‚àºpÀÜa[h‚àó(r ‚ä•)]
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
E1 E2 E3
(54)
where wÀÜ is the estimated w by Ridge regression, E [fÀÜ(r)] = E [a] and r = r ,f ‚àó(r) =
r‚àºqa a‚àºq ‚ä•
g‚àó(r)whenr ‚àºq .
a
Here we give a brief interpretation of the decomposition. E is the prediction and generalization
1
errorcomingfromregression,whichisindependentfromthediffusionerror. BothE andE come
2 3
fromthediffusionprocess,whereE reflectsthedisparitybetweenpÀÜ andq onthesubspacesupport
2 a a
whileE measurestheoff-subspaceerrorinpÀÜ . Thefollowingresultsgiveconcreteboundsforthe
3 a
termsinLemmaE.3.
Bounding Regression Errorwith Offline Bandits. By estimating w with Ridge regression, we
have
wÀÜ =(R‚ä§R+ŒªI)‚àí1R‚ä§(Rw‚àó+Œ∑) (55)
whereR‚ä§ = (r ,...,r )andŒ∑ = (Œæ ,...,Œæ )whereŒæ ‚àº N(0,ŒΩ2). DefineV := R‚ä§R+ŒªI,
1 n 1 n i Œª
Œ£ÀÜ := 1V andŒ£ :=E rr‚ä§andtakeŒª=1,wehave
Œª n Œª qa r‚àºqa
26Preprint. Underreview.
PropositionE.3. Withhighprobability,
‚àö
(cid:113) O(||w‚àó||+ŒΩ2 d logn)
E ‚â§ Tr(Œ£ÀÜ‚àí1Œ£ )¬∑ ‚àö r (56)
1 Œª qa n
FurtherwhenrhasGaussiandesignr ‚àºN(¬µ,Œ£),
Tr(Œ£ÀÜ‚àí1Œ£ )‚â§O(cid:0)
a2
+d (cid:1) (57)
Œª qa ||w‚àó|| r
Œ£
whenn=‚Ñ¶(max{ 1 , dr }).
Œªmin ||w‚àó||2
Œ£
Proof. Firstwehave
E =E |r‚ä§(w‚àó‚àíwÀÜ)|‚â§E ||r|| ¬∑||w‚àó‚àíwÀÜ|| (58)
1 pÀÜa pÀÜa V Œª‚àí1 VŒª
where
(cid:113) (cid:113) (cid:113)
E ||r|| ‚â§ E r‚ä§V‚àí1r = Tr(V‚àí1E rr‚ä§)‚âÉ Tr(V‚àí1Œ£ ) (59)
pÀÜa V‚àí1 pÀÜa Œª Œª pÀÜa Œª qa
Œª
Henceweonlyneedtoprove
(cid:112)
||w‚àó‚àíwÀÜ|| ‚â§O(||w‚àó||+ŒΩ2 d logn) (60)
VŒª r
Usingtheclosedformexpression,wehave
wÀÜ‚àíw‚àó =V‚àí1R‚ä§Œ∑‚àíŒªV‚àí1w‚àó (61)
Œª Œª
Thus,
||w‚àó‚àíwÀÜ|| ‚â§||R‚ä§Œ∑|| +Œª||w‚àó|| (62)
VŒª V Œª‚àí1 V Œª‚àí1
‚àö
whereŒª||w‚àó|| ‚â§ Œª||w‚àó||,andaccordingto(Abbasi-yadkorietal.,2011),
V‚àí1
Œª
(cid:112)
||R‚ä§Œ∑|| =||R‚ä§Œ∑|| ‚â§O(ŒΩ2 d logn) (63)
V‚àí1 (R‚ä§R+ŒªI)‚àí1 r
Œª
withhighprobability. Wehenceconcludethefirstpartofproof.
Further,whenrhasGaussiandesignr ‚àºN(¬µ,Œ£),accordingtoLemmaC.6of(Yuanetal.,2023),
wecanprovetheresults. ThekeyhereistheconditionaldistributionfollowstheGaussianbelow,
P
(cid:0) r|fÀÜ(r)=a(cid:1) =N(cid:16) ¬µ+Œ£wÀÜ(wÀÜ‚ä§Œ£wÀÜ+ŒΩ2)(a‚àíwÀÜ‚ä§¬µ),Œ£‚àíŒ£wÀÜ(wÀÜ‚ä§Œ£wÀÜ+ŒΩ2)‚àí1wÀÜ‚ä§Œ£(cid:17)
(64)
r
Thus
(cid:18) Œ£1/2wÀÜwÀÜ‚ä§Œ£Œ£ÀÜ‚àí1Œ£1/2a2(cid:19) 1 a2
trace(Œ£ÀÜ‚àí1Œ£ )=trace Œª ‚â§(1+ ‚àö )¬∑O( +d ) (65)
Œª qa (||wÀÜ||2 +ŒΩ2)2 Œª n ||wÀÜ||2 r
Œ£ min Œ£
Noticethat||wÀÜ|| ‚â•||w‚àó|| ‚àí||wÀÜ‚àíw‚àó|| . Wehave
Œ£ Œ£ Œ£
(cid:18) ||w‚àó||+ŒΩ2(cid:112)
d
log(n)(cid:19)
||wÀÜ‚àíw‚àó|| =O ‚àö r (66)
Œ£ n
wherewecanprove||wÀÜ|| Œ£ ‚â• 21||w‚àó|| Œ£whenn=‚Ñ¶( ||wd ‚àór ||2 ).
Œ£
Remarkably,thisisamorepreciseboundimprovingtheresults(LemmaC.5andC.6)in(Yuanetal.,
2023),wherewemakelessassumptionsontherelationshipbetweenyandr,explicitlytaking||w||
andŒΩ2intoaccount.
27Preprint. Underreview.
BoundingDistributionShiftinDiffusion. Wedefinethedistributionshiftbetweentwoarbitrary
distributionsp andp restrictedunderfunctionclassLas
1 2
E [l(x)]
T(p ,p ;L):=sup x‚àºp1 (67)
1 2 E [l(x)]
l‚ààL x‚àºp2
Wehavethefollowlemma.
LemmaE.4. UndertheassumptionthatrfollowsGaussiandesign,then
(cid:115)
(cid:18) (cid:19)
T(q(r,y =a),q ;S)
TV(pÀÜ ,q )=O ry ¬∑œµ (68)
a a Œª r
min
whereœµ isdefinedinLemmaE.2. WecanboundE with:
r 2
(cid:16) (cid:112) (cid:17)
E =O (TV(pÀÜ ,q )+t ) M(a) (69)
2 a a 0
whereM(a)=O( ||wa ‚àó2 ||Œ£ +d). Byplugginginœµ2 r =OÀú( t0d ‚àö2 r n),whent 0 =(d4 r/n)1/6,E 2admitsthe
besttradeoffwithbound
(cid:115)
(cid:18) (cid:19)
T(q(r,y =a),q ;S)
E =OÀú ry ¬∑(d4/n)1/6a (70)
2 Œª r
min
Proof. TheproofdirectlyfollowsfromLemmaC.4andLemmaC.7in(Yuanetal.,2023).However,
theconclusionisslightlydifferentaswedonotassumealowdimensionalsubspaceofr.
OnecanalsoverifythatwhenrfollowsGaussiandesign,T(q(r,y =a),q ;S)=O(a2‚à®d ).
ry r
E.2.3 SECONDSTAGEOFCONDITIONALGENERATION
Now that we have proved that: (i) the first-stage diffusion model can estimate the score function
with provable error bound (Appendix E.2.1); and (ii) the generated representations have provable
reward improvement (Appendix E.2.2). We continue to show that the ultimate generated samples
alsohavedistributionshifttowardsthedesiredtargetinthefollowingcontexts.Particularly,wewant
toanswerthequestion: whyutilizingtheconditionallygeneratedrepresentationsisenoughforthe
secondstagegeneration?
First,whenweusethegeneratedrepresentationsastheonlyconditionofthesecondstagediffusion
model, the generation process is identical to the second stage of unconditional generation. There-
fore,theresultsinAppendixE.1.2canbedirectlyappliedtoanalyzethesecondstagegenerationof
conditional generation, which states that the generation conditioning on representations has small
TVdistanceerrorcomparedwithgroundtruthconditionaldistribution. Thus,whenwehavehigh-
qualityfirst-stagegeneration,thecorrespondingsecondstagegenerationwouldintroducealmostno
additionalerror, whichimpliesprovablerewardimprovementstowardsthedesiredtarget. Inaddi-
tion, thewell-pretrainedencodeensuresthatthecorrespondencebetweenrepresentationsanddata
pointsisgood,whichmakesitpossibletorigorouslyconstructthedatapointsgiventherepresenta-
tions(aspecialcasewouldberisthecompleterepresentationofx).
Wethenpartiallyanswerthisquestionfromtheinformationtheoreticperspectives. WeuseH(¬∑)to
denotetheinformationentropy,andI(¬∑;¬∑)todenotethemutualinformationbetweentwovariables.
‚Ä¢ I(x;r) ‚â• I(x;y). Ontheonehand,r containsenoughinformationtorecoverthetargets
y thankstotheresultsinAppendixE.2.2,thuswedonotexplicitlyneedy forthesecond
stage. Onetheotherhand,benefitfromthepretrainingtask,therepresentationsobviously
containsmoreinformationinadditiontoy. Thisassumptionisvalidifw‚àóinpreviousanal-
ysisissparse(therearecomponentsinrindependentofy),i.e.,H(r)>H(y). Therefore,
generatingxconditioningonrismucheasierthangeneratingconditioningony(traditional
onestagegeneration),asthescoreestimationerroroftheformeronewouldobviouslybe
muchsmallerthanthelatter.
28Preprint. Underreview.
‚Ä¢ I(x,r;y) ‚â• I(x;y). Recall Equation (44) which states the target property y depends on
bothxandr. Hence,r maycontainadditionalinformationofy obtainedfrompretrained
tasksthatishardto(orcannot)bedirectlyextractedfromx-thecomplexpretrainedmodel
assistsinextractingrelevantinformationinourtwo-stagegeneration,whileone-stagegen-
eration solely relies on the single denoising model to do so. Therefore, by leveraging
representationswithprovableerrorbounds,wecanbettershiftthedistributiontowardsthe
target.
In summary, r is an ideal middle state connecting x and y - it is easy to recover r from y (Ap-
pendixE.2.1)andtorecoverxfromy(AppendixE.1.2),andviceversa. Incomparison,itissome-
whatmoredifficulttodirectlyrecoverxfromyorpredictyfromx.Consequently,rmaybeabetter
indicatorofycomparedwithxduetotheaforementionedreasons.
Indeed,one-stagediffusionmodelsgeneratexdirectlyfromconditionsyneedtooptimizeahighly
complex score ‚àá logp(x|y) where x and y are highly non-linearly mapped. As Theorem E.4 in
x
(Yuanetal.,2023)pointsout,thenonparametricSubOptofxgeneratedbydeepneuralnetworksis
largerthanourresultsinAppendixE.2.2,whichfurthervalidatestheadvantageoffirstgeneratingr
thatcanbewellmappedfromy.
F VISUALIZATION
F.1 REPRESENTATIONVISUALIZATION
Toillustratehowwellp (r)fitsq(r),wesamplefrombothq(r)(i.e.,therepresentationsproduced
œÜ
bythepre-trainedencoderfortheQM9andGEOM-DRUGdatasets)andthetrainedrepresentation
generatorp (r). WethenvisualizetheminAppendixF.1,withcolorsindicatingwhetherthesam-
œÜ
ples are from q(r) or p (r). We compute the Silhouette Score of the clustering results, scaled by
œÜ
102 forclarity. Ascoreclosetozerosuggeststhatthetwoclustersaredifficulttodistinguish, in-
dicatingagoodfitbetweenp (r)andq(r). Similarly,weprovidethevisualizationofconditionally
œÜ
generatedrepresentationsinFigure8
Figure 7: T-SNE visualizations of representations unconditionally generated by the representation
generator(T =1.0)vs.thoseproducedbythepre-trainedencoderontheQM9andDRUGdatasets.
TheSilhouetteScoreisscaledby102forclarity.
F.2 VISUALIZATIONOFMOLECULESAMPLES
In this section, we provide additional random molecule samples to offer deeper insights into the
performanceofGeoRCG.Figure9andFigure10showunconditionalrandomsamplesgeneratedby
GeoRCGtrainedontheQM9andGEOM-DRUGdatasets,respectively. Figure11presentsrandom
samplesconditionedontheŒ±property,alongwiththeircorrespondingerrors.
29Preprint. Underreview.
Figure8: T-SNEvisualizationofrepresentationsconditionallygeneratedbytherepresentationgen-
eratorvs. thoseproducedbythepre-trainedencoderontheQM9dataset: (a)Œ±,(b)‚àÜœµ,(c)œµ ,
HOMO
(d)œµ ,(e)¬µ,and(f)C . TheSilhouetteScoreisscaledby102forclarity.
LUMO v
30Preprint. Underreview.
Figure9: UnconditionalrandomsamplesfromGeoRCGtrainedonQM9. Thenumberofnodesis
randomlysampledfromthenodedistributionq(N).
31Preprint. Underreview.
Figure 10: Unconditional random samples from GeoRCG trained on GEOM-DRUG. The number
ofnodesisrandomlysampledfromthenodedistributionq(N).
32Preprint. Underreview.
Figure 11: Conditional random samples from GeoRCG trained on QM9 dataset and Œ± property.
Black numbers indicate the specified property value condition, while green numbers represent the
evaluatedpropertyvalueofthegeneratedsamples. Thenumberofnodesandpropertyvaluecondi-
tionsarerandomlysampledfromthejointdistributionq(N,c).
33