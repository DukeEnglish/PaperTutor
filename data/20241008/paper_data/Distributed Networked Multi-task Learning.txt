1
Distributed Networked Multi-task Learning
Lingzhou Hong, Alfredo Garcia, Senior Member, IEEE
Abstract—We consider a distributed multi-task learning Federatedlearning[4],[5]isanexampleofdistributed
scheme that accounts for multiple linear model estimation learningthatutilizesacentralcomputingcentertomain-
tasks with heterogeneous and/or correlated data streams.
tain and update the global model. In this approach, each
We assume that nodes can be partitioned into groups
node is connected with the central node and indepen-
correspondingtodifferentlearningtasksandcommunicate
according to a directed network topology. Each node dently performs model training using its local data, and
estimates a linear model asynchronously and is subject only shares model updates or gradients with the central
to local (within-group) regularization and global (across server. The central server then combines the local up-
groups)regularizationtermstargetingnoisereductionand
datestoupdatetheglobalmodel.Whileaddressingsome
generalization performance improvement respectively. We
of the challenges associated with centralized or isolated provideafinite-timecharacterizationofconvergenceofthe
estimators and task relation and illustrate the scheme’s learning schemes, frequent communication between the
generalapplicabilityintwoexamples:randomfieldtemper- central node and the local nodes may lead to congestion
ature estimation and modeling student performance from bottlenecks. The process of collecting and assembling
different academic districts.
a diverse batch of data points in a central location to
Index Terms—Multi-task Learning, Distributed Opti- updateamodelmayimplysignificantlatency,especially
mization, Network-based computing systems, Multi-agent when dealing with high data payloads obtained through
systems. heterogeneous and correlated streams.
In contrast, some other peer-to-peer distributed learn-
ing schemes eliminate the need for a central computing
I. INTRODUCTION
center. This decentralization empowers the system with
INthecurrentageofbigdata,manyapplicationsoften enhanced scalability, preventing information overflow
at a central node. Additionally, it enables the system
face the challenge of processing large and complex
to efficiently handle growing data volumes, complex
datasets, which are usually not available in a single
learning tasks, and increasing network demands.
place but rather distributed across multiple locations.
In our previous work [6]–[8], we studied peer-to-
Approachesthatrequiredatatobeaggregatedinacentral
peerdistributedschemeswhereallnodessharethesame
location may be subject to significant scalability and
learning task (i.e. estimation of a linear model). This
storage challenges. In other scenarios, data are scattered
paper considers a distributed approach to multi-task
across different sites and owned by different individuals
learning (MTL) [5], where multiple learning tasks are
ororganizations.Dataprivacyandsecurityrequirements
jointly undertaken by a network of nodes. The relation
make it difficult to merge such data in an easy way.
betweenlearningtasksisnotknowninadvanceandmust
In both contexts, Distributed Learning (DL) [1]–[3] can
be inferred from the data.
provide feasible solutions by building high-performance
Many existing MTL methods rely on a central fu-
models shared among multiple nodes while maintaining
sion node that updates models in response to locally
user privacy and data confidentiality.
computed gradient updates [5], [9], [10]. In MTL, an
DLaimstobuildacollectivemachinelearningmodel
often-assumed premise is the homogeneity of data and
based on the data from multiple computing nodes that
the independence of noise, as noted in [5]. When data
can process and store data and are connected via net-
is correlated and the tasks are closely related, such
works. Nodes can utilize neighboring information to
simplification ignores one source of correlation and can
improve their own performance: rather than sharing
lead to poor model quality. Works [11]–[13] propose
raw data, they only exchange model information such
distributed MTL schemes where a central node updates
as model parameters or gradients to avoid revealing
the global model but lacks a finite-time characterization
sensitive information.
of convergence. The paper most closely related to ours
is [14], which presents an asynchronous approach for
ThisworkwassupportedinpartbytheNationalScienceFoundation
under Award ECCS-1933878 and in part by the Air Force Office of linear multitask problems. However, it does not account
ScientificResearchunderGrant15RT0767. forcommonnoiseinitsformulation,anditsconvergence
Lingzhou Hong and Alfredo Garcia are with the Department of
analysis primarily focuses on the mean square error
Industrial & Systems Engineering, Texas A&M University, College
Station,TX77843USA(e-mail:{hlz,alfredo.garcia}@tamu.edu). of estimations. In contrast, our paper offers finite-time
4202
tcO
4
]AM.sc[
1v30430.0142:viXra2
characterizations of the convergence of the estimations measure that captures the performance gap between the
and task relation precision matrix. parameter estimation and the ground truth for the inner
In this paper, we introduce a Distributed and Asyn- problem.Fortheouterproblem,weprovidedtheconver-
chronous algorithm for Multi-task Learning (DAMTL) gence analysis for the regularity measure that describes
that accounts for heterogeneous and correlated datasets. the distance of the estimated precision matrix to the
Weprovideafinite-timecharacterizationofconvergence. true task relation precision matrix. Finally, in section
In the considered architecture, we assume that nodes IV, we report the method’s performance on a synthetic
are connected via a directed graph and can be parti- temperature estimation problem and a real dataset on
tionedintogroupscorrespondingtocertaincriteria(e.g., students’ study performance.
distance, similarity). We assume that all nodes within
a group have the same learning task and that learning II. DATAANDPROCESSINGMODEL
tasksarerelatedaccordingtotheGaussianmodelwhose
A. Data Model
parameters are unknown.
We consider a set of nodes V ={1,...,N} with the
We formulate the multi-task learning as a bi-level
ability to collect and process data streams y ={y ∈
optimization problem, where the outer problem involves i i,k
Rm|k ∈N+} of the form:
estimating the covariance matrix for the Gaussian task
relationship model, while the inner problem focuses on y =X w∗+ε +Λ ξ , i∈V (1)
i,k i i i,k i k
estimating linear models. To solve this problem, we
where X ∈ Rm×p is a random data matrix with rows
analyze a two-timescale distributed algorithm consisting i
independent and identically sampled from a multivariate
of stochastic gradient descent (SGD) updates for outer
normal distribution N(ν ,Ψ ), w∗ ∈ Rp is the ground
and inner problems. Each node implements an asyn- i i i
truth coefficient vector of coefficients for node i. Here
chronousSGDwithlocallycomputedgradientsandreg-
{ε ∈Rm|k ∈N+}aretheindividualnoise,whichare
ularization updates for the inner problem. One selected i,k
independent and identically distributed (i.i.d.) random
node per group (called messenger), aside from handling
noise that only influence node i. On the other hand,
the inner problem, computes the group precision matrix
{ξ ∈ Rm|k ∈ N+} are independent realizations of a
(the inverse of the covariance matrix) updates for the k
common noise which affects node i according to the
outerproblem.Thenodescommunicatein(i)everynode
matrix Λ ∈ Rm×m, thereby introducing correlation
periodically broadcasts its model to other nodes in the i
across the noise terms from all nodes. We consider
same group, and (ii) messengers periodically exchange
Λ ’s as a diagonal matrix with diagonal entries that are
groupupdateswitheachother.Akeyfeatureisthatlocal i
possibly different.
updates (in the inner problem) take place at a higher
We assume individual noise are zero-mean E[ε ] =
frequency than global updates (in the outer problem) to i,k
0 and independent across different nodes, i.e.,
reduce communication costs and enhance the system’s m×1
E[ε ε⊺ ] = 0 for all i,j ∈ V, and j ̸= i.
robustness. Continuous and asynchronous updating dis- i,k j,k m×m (cid:13) (cid:13)
tinguishes the proposed method from existing ensemble Moreover, we assume E(cid:13) (cid:13)ε i,kε⊺ i,k(cid:13)
(cid:13)
= σ i2I m, and σ i’s
learningmethodsthatoftenrequireasynchronizedmodel may differ across nodes, making the noise term het-
aggregation step. erogeneous. The common noise vectors are i.i.d with
Estimating the precision matrix (the inverse of the E[ξ ]=0 andE∥ξ ∥2 =I .Itfollowsthecovariance
k m k m
covariance matrix) is a key component in the outer matrix of the error term in (1) for y as
i,k
problem. It is challenging due to the curse of dimen-
Ω :=E[(ε +Λ ξ )(ε +Λ ξ )⊺ ]=σ2I+Λ2.
sionality, and the estimation is sensitive to noise and i i,k i k i,k i k i i
requires a large number of observations relative to the
number of variables. There are several methods have B. Task Relationship Model
been proposed to address these challenges and provide We consider a network structure represented by a
efficient estimates [15], [16]. We consider a ridge (L ) graph G = (V,E), where an edge (i,j) ∈ E represents
2
basedestimationoftheprecisionmatrix[17],whichdoes the ability to exchange information between nodes i
not require the true (graphical) model to be (extremely) and j. This is represented by the adjacency matrix
sparse and can be easily implemented via gradient de- A ∈ RN×N with a = 1 if (i,j) ∈ E, and a = 0
i,j i,j
scent. otherwise.
The paper is structured as follows. In section II, we We further assume the graph is composed of q > 1
introduce a two-timescale stochastic algorithm for dis- connected sub-graphs (groups) G = {G ,...,G } with
1 q
tributedMTLestimationthataccountsforheterogeneous a specific structure (see Figure 1(a) below): for each
andcorrelateddatasets.InsectionIII,weprovideafinite- group there is a unique node say i∗ ∈ G (which we
i
time characterization of the convergence of regularity refer to as messenger) with edges (i∗,j)∈E to outside3
nodes j ∈ G ,k ∈ {1,...,q}\{i}. In words, nodes Σ is estimated by
k
can communicate within each subgraph (according to
p
the subgraph topology) while there is a unique node for S(W)=
1(cid:88)
ωc(ωc)⊺ .
p k k
each subgraph which we refer to as messenger that can
k=1
communicate with other messenger nodes from other
However,whenN >p,thecovariancematrixSbecomes
subgraphs. We assume the ground truth coefficients in
singular, and the precision matrix estimation cannot be
the data model in (1) are the same for members within
accuratelyestimated.Asin[17],weemployamaximum
the same subgraph, i.e.,
likelihood estimation based precision matrix estimation
w i∗ =w j∗ i,j ∈G k, k ∈{1,...,q} with a ridge (L 2) type of penalty:
b
To model the relationships between tasks, let W∗ = f(Θ,W)≜tr(S(W)Θ)+ tr((Θ−T)⊺ (Θ−T))
[w∗,...,w∗ ] ∈ Rp×N be the matrix containing the 2 (5)
1 N −log|Θ|,
ground truth coefficients for the data model in (1).
As in [18] we assume ground-truth matrix W∗ fol- where b ∈ (0,∞) is the parameter that controls the
lows a matrix-variate normal distribution, i.e. W∗ ∼ magnitude of the penalty and T is a symmetric positive
MN (M∗,I ⊗Σ), where ⊗ denotes the Kronecker definitetargetmatrix.Notethatthepenaltytermamounts
pq m
product, M∗ =[m∗,...,m∗]∈Rp×q is the mean ma- to a proper ridge penalty, since
trix with m∗ as the shared mean vector across all nodes
tr((Θ−T)⊺ (Θ−T))=∥Θ−T∥2 .
[18]. The estimating the matrix Σ will be formulated in F
terms of the precision matrix Θ≜Σ−1. Based upon definitions (2) and (5), the bi-level opti-
mization formulation of MTL is:
C. A Bi-level Formulation of MTL minf(Θ,W⋆(Θ)
Θ
(6)
The joint estimation of W and Θ can be formulated s.t. W⋆(Θ)∈argminl(Θ,W).
as a bi-level optimization problem. To elucidate this W
formulation, we begin by describing the inner-level (or We refer to min f(Θ,W⋆(Θ)) as the outer problem,
Θ
lower-level) objective in the form of task-regularized which aims to estimate the task relationship under the
least squares: assumption that the models W∗(Θ) are optimal with
respect the regularized least squares objective. Specifi-
N
(cid:88)
l(Θ,W)= [ℓ (w )+ρ (W(q(i)))], (2) cally, it seeks to estimate the task precision matrix Θ.
i i r
Theinnerproblemmin l(Θ,W)centersonestimating
i=1 W
W given a task relationship model given by Θ.
whereℓ (w )representsthelocalweightedleast-squares
i i
loss function:
1 D. A Distributed Approach to Solve (6)
ℓ (w )≜E[ (y −X w )⊺ Ω−1(y −X w )], (3)
i i 2 i i i i i i i To address the bi-level optimization problem outlined
HereW(q(i)) representstheestimationmatrixassociated above,weconsideratwo-timescaledistributedalgorithm
withsubgraphG correspondingtonodei.Thesecond thatconsistsofSGDupdatesfortheregularizedversions
q(i)
term in (2) serves as a task regularization term: of the outer and inner problems. In this scheme, each
node implements an asynchronous SGD with locally
ρ (W(q))≜tr((W(q)−M(q))Θ (W(q)−M(q))⊺ )
r l computed gradients and regularization updates for the
(4)
inner problem.
ThistermistheMahalanobisdistancebetweensubgraph
In addition to implementing updates for the inner
estimates W(q) and their mean.
problem,themessenger,alsoupdatesthesolutionforthe
To describe the outer-level (or upper-level) objective, outer problem. The communication requirements are as
let us define: follows: (i) every node periodically broadcasts its model
Wc =[w −m¯,...,w −m¯] to other nodes in the same group, and (ii) messengers
1 N
periodically exchange group updates with each other.
where m¯ = 1[(cid:80)N w ,...,(cid:80)N w ]⊺ for a given In the distributed learning process, we aim to obtain
N j=1 1j j=1 pj
estimationmatrixW.Here,theaveragesaretakenacross the estimation matrix W and the task precision matrix
thenodes(therowsofW).Letωc betheithrowofWc. Θ.Insteadoftransformingalltheinformationacrossthe
i
Accordingtothetaskrelationshipmodel,ωc ∼N(0,Σ). entire network, nodes periodically send their model es-
i
Hence, the empirical estimator of the covariance matrix timates to their neighboring nodes, while the designated4
messengers are responsible for transmitting the group where the time gaps between consecutive ticks are kept
estimations across different groups. However, this inter- very small. This design ensures that no two consecutive
group communication occurs at a lower frequency than updates or communication occur at the same tick. If at
thelocalupdateswithineachgroup.Thiscommunication the kth tick, the messenger from group l is updating the
strategy reduces the amount of information exchanged precision matrix for its group, it utilizes the following
between nodes and ensures that only necessary updates precision matrix gradient estimate:
are shared, and strikes a balance between system con-
∇ f =S +b (Θ −Th )−Θ−1−ς , (9)
sistency and computational efficiency. Θ l,k l,k k l,k k l,k l,k
In each group G , the outer problem is performed
l where {b } is a nonnegative decreasing sequence of
k
only in the group messenger, while the inner problem
penalty parameters that converge to zero, and ς ∈
l,k
is computed asynchronously by every node within the RN×N is a noise matrix generated from the gradient
group.Nowweintroducethelearningproblemsinmore
estimation process. We assume all elements of the noise
detail. matrix are independent, and each column follows ςi ∼
l,k
1) At local learner (Parameter estimation): In the N(0,ι2I) i.i.d.
parameterestimationapproach(2),eachnodei∈G ,l∈ l
l At the same time, if node i (including the messenger)
{1,...,q} solves the following “localized” convex opti-
collects a data point y at the kth tick, and assuming
i,k
mization problem,
the processing time is negligible, node i computes the
minL (Θ,W)≜(cid:8) ℓ (w )+δ ρ (w)+δ ρ (W(l(i)))(cid:9) . following estimation gradient estimate:
i i i 1 i 2 r
wi
(7) ∇ℓ =X⊺ Ω−1(X w −y )
i,k i i i,k i,k i,k (10)
In this setting, the system experiences two correlated =g −X⊺ Ω−1(ε +Λ ξ ),
i,k i,k i i,k i k
factors, one resulting from the data noise and another
fromtaskcorrelations.Weutilizetwopenaltiesandeach whereg :=∇ ℓ (w )=X⊺ Ω−1X (w −w∗)
i,k wi i i,k i,k i i,k i,k i
targetatonenoisesourcetomaintainnodecohesion.The is the “noise-free” gradient. At the tth tick, for group
consensus regularization term is defined as G , the basic two-timescale system stochastic gradient
l
update is as follows:
ρ (w)=
1(cid:88)
a ∥w −w ∥2, (8)
i 2 i,j i j Θ =Θ −β 1c ∇ f , (messenger) (11a)
j̸=i l,k+1 l,k k l,k Θ l,k
w =w −γ[1g ∇ℓ +δ 1n ∇ ρ (w )
wherea i,j indicateswhethernodeiandj areneighbors, i,k+1 i,k i,k i,k 1 i,k wi i k
and nodes can only be neighbors if they belong to the +δ 1r ∇ ρ (W(l))], i∈G (11b)
2 i,k wi r k l
same group. The consensus regularization δ ρ (w) ≥
1 i
where ∇ ρ (w ) is the in-group consensus penalty
0 serves as a measure of similarity among the models wi i k
within the same group, promoting consistency among gradient for node i, and ∇ wiρ r(W k(l)) is the task re-
models that share the same ground truth. Specially, we lationship penalty gradient for node i. Here, 1c l,k is an
have ρ (w) = 0 if and only if w = w for all j ̸= i indicatorvariablethatdetermineswhetherthereisagra-
i j i
where a =1 and i,j ∈G . dientupdatefortheouterproblemattickk.Additionally,
i,j l 1g , 1n , and 1r are the indicator random variables
The task penalty, as introduced in (4), is designed to i,k i,k i,k
to whether a gradient estimate g , a local consensus
promote cohesion among different groups (tasks). The i,k
gradient ∇ ρ (w ), and a global task penalty gradient
wpa hr ia cm he cte or ns seδ n1 su> s0 ana dnd taδ sk2 > reg0 uld ae rit ze arm tioin ne s,t rh ee spe ex ct te in vt elt yo
, ∇ wiρ r(W
k(w l)i )ai reok
btained,respectively.Theinnerprob-
impacttheoptimizationprocess. Adjustingtheseparam- lem stepsize γ remains constant throughout the process,
etersallowsformaintainingconsistencywithinthesame while the outer problem stepsize β k is a nonnegative
group and maintaining the system’s robustness. decreasing sequence.
2) At messenger (Task relationship estimation): The
messenger node from group l is responsible for col-
E. Algorithm Illustration
lecting group model updates {w ∈ G } (denoted as
i l
W )andsendsittoothermessengersinthenetwork.It WeillustratetheDAMTLnetworkinFigure1(a)with
Gl
also updates the estimation matrix W(l) when receiving 8 nodes grouped into 3 groups, and the information
updates from other groups. It further updates the corre- exchange process in Figure 1(b).
sponding precision matrix and then sends ρ (W) back In Figure 1(b), when the messenger (node 3) is ready
r
toothernodeswithinitsgroup.Notethatallmessengers to share the information (marked by an orange star), it
updatetheirgroupestimationWandΘasynchronously. sends out W to messengers 4 and 8. At the time to
G1
3) Stochastic gradient updates: In our approach, a updatetheestimationmatrix(markedbyabluetriangle),
virtual clock is employed to generate ticks (see II-F ), while W has not been received, the messenger 3 only
G35
Algorithm 1 DAMTL regular node
1: Loops
2: compute available function and penalty gradients
3: update w i,k+1 by (11b)
4: End of Loops
Algorithm 2 DAMTL messenger node i ∈ G , l ∈
l
{1,...,q}
1: Outer problem loops:
2: Inner problem loops:
3: compute available gradients
(a) 4: compute w i,k+1 by (11b)
5: End of the inner problem loops
6: update W Gl and send it to other messengers
7: updateW(l) withcollectedW Gj’s,j ∈{1,...,q}
8: update S l,k and compute Θ l
9: update ρ r(W(l)) and send it to other nodes in G l
10: update ∇ wiρ r(W(j))
11: End of the outer problem loops
let N (t)=(cid:80) 1c to count for the number of the
c,l tk≤t l,k
task precision matrix update for group l that occurred
up to time t. We assume that at kth tick the messenger
of G obtains the jth gradient update ∇f := ∇f ,
l l,(j) l,k
(b)
anddenotethistimepointast(j).Wehavethefollowing
c,l
Figure1.(a)NetworkstructureofDAMTL.Thereare8nodes relationship: t(j) = k, 1c = 1, and N (k) = j. We
assignedinto3groups,andnodes2,4,8aremessengernodes c,l l,k c,l
of groups G , G , and G respectively. (b) The information define the random time variable ∆t(j) = t(j) −t(j−1)
1 2 3 c,l c,l c,l
exchangeprocessofG 1.Theyellowshadedareaisthewithin to compute ∇f . We assume ∆t(j)’s are i.i.d. with
group information exchange timeline of G , and the red l,(j) c,l
shaded area illustrates the across group inform1 ation exchange E[∆t( cj ,l)]=∆t c,l and w.p.1,
timeline.
N (t) 1
lim c,l = :=φ ,
t→∞ t ∆t c,l l
updates W(1) with received W (during the within-
G2
where φ can be seen as the precision gradient update
group information exchange, the most recent updates l
rate.
from group members are already stored in W(1)). Sub-
sequently, the messenger 3 sends ρ (W(1)) to group Similarly, for the inner problem, we define the count-
r ing process N (t) = (cid:80) 1g to count the number
member 1 and 2. g,i tk≤t i,k
of function gradient updates ∇ℓ occurred up to time
To illustrate the process, in the following algorithm, i,k
t. Let N (t) = (cid:80) 1n count for local penalty
we make the assumption that regular nodes update
gradient
un p,i
dates ∇
ρtk≤ (wt i ),k
and N (t) = (cid:80) 1r
constantly,whereasthemessengernode,apartfromcon- wi i k r,i tk≤t i,k
ductingitsestimationupdateaftercompletingoneround count for the task penalty gradient ∇ wiρ r(W k(l)) up-
of the inner phase, will also perform group updates. dates. We assume that at the kth tick, node i receives
Specifically,Algorithm1outlinestheDAMTLprocedure the jth function gradient update ∆ℓ i,(j) := ∆ℓ i,k. We
for regular (non-messenger) nodes, while Algorithm 2 denote this time point as t(j) with t(j) = k, 1g = 1,
g,i g,i i,k
outlines the DAMTL procedure for messenger nodes. and N (k)=j. Similarly, let the time point of the jth
g,i
local penalty gradient ∇ ρ (w ) updates as t(j) =k
wi i (j) n,i
F. Continuous Time Approximation with 1n
i,k
=1 and N n,i(k)=j, and the jth task penalty
For analysis purposes, we embed the discrete-time gradient∇ wiρ r(W (( jl) ))updatesatt( rj ,i) =kwith1r
i,k
=1
process into a continuous time setting. Let the kth tick and N r,i(k) = j. We define the following random time
of the global clock happens at t , for the notation variables: let ∆t(j) = t(j)−t(j−1) be the random time
k g,i g,i g,i
simplicity, we assume t = k. For the outer problem, to compute ∇ℓ ,
k i,(j)6
For node i, we define the following random time Appendix (VI-B)):
variables: let ∆t(j) = t(j) − t(j−1) be the random
g,i g,i g,i dΘ =−φ βˆ(S +b (Θ −T )−Θ−1)dt+
time to compute ∇ℓ , ∆t(j) = t(j) −t(j−1) be the l,t l t l,s t l,s s l,s
i,(j) n,i n,i n,i βˆκ dM (12a)
random time to collect updated models from neighbors t l l,t
and compute the local penalty gradient ∇ wiρ i(w (j)), dw
i,t
=−(µ ig i,t+δ 1ϖ i∇ρ i,t+δ 2ϕ i∇ρ( ri ,t,j))dt+
and ∆t( rj ,i) =t( rj ,i)−t( rj ,i−1) be the random time to collect τ idB i,t+τ idW i,t. (12b)
t ∇he u ρp (d Wate (d l))W
.
(l) and compute the task penalty gradient where τ
i
= √ γµ i, κ
j
= ι j(cid:112) βφ j. The noises are
w r (j) approximated by Brownian terms: B is a general m
i,t
dimensional Brownian motion with covariance Υ , i.e,
i
B = C F with B being a standard m dimensional
We assume ∆t( gj ,i)’s are i.i.d. with E[∆t( gj ,i)] = ∆t g,i, Bri o,t wniani mt otionandt C
i
∈Rp×p withC iC i⊺ =Υ i.W
i,t
∆t( nj ,) i’s are i.i.d with E[∆t( nj ,) i]=∆t n,i, and ∆t( rj ,i)’s are is a general m dimensional Brownian motion with co-
i.i.d with E[∆t( rj ,i)]=∆t r,i. Then w.p.1, v mari da in mc ee nΞ sii o, ni a.e l, BW roi, wt n= iaD ni mW ot tiw onith anW dt Dbei ∈ng Ra ps ×ta pn wda ir thd
i
N (t) 1 ⊺
tl →im
∞
g, ti =
∆t g,i
:=µ i, D coi nD tai in= ingΞ ai. stH aner de arM
d
mj,t dis ima enm sa iotr ni ax lw Bi rt oh we na iac nh mco ol tu im onn
.
N (t) 1
lim n,i = :=ϖ ,
t→∞ t ∆t n,i i
III. CONVERGENCEANALYSIS
N (t) 1
lim r,i = :=ϕ .
t→∞ t ∆t r,i i To characterize convergence, we utilize measures of
Here we can see µ as the function gradient update rate, regularity and consistency. The measure of consistency
i
ϖ asthelocalpenaltygradientupdaterate,andϕ asthe for the outer problem is defined as:
i i
taskpenaltygradientupdaterate.Thus,wehavechanged q q N
thecountofindividualarrivalofupdatestoupdaterates. V
k
= 21
q
(cid:88) ∥Θ j,k−Θ ∗∥2
F
= 21
q
(cid:88)(cid:88)(cid:13) (cid:13)Θi j,k−Θi ∗(cid:13) (cid:13)2 ,
j=1 j=1i=1
(13)
We rewrite the stepsize of the outer problem β k = where Θi is the ith column of Θ and Θi is the ith
ββˆ as the product of a constant β and a decreasing j,k j,k ∗
k columnofΘ ,thegroundtruerelationprecisionmatrix.
sequence {βˆ }. We introduce the re-scale process for ∗
k For the inner problem, we define the regularity measure
both problems. For the outer problem, we define, we
as:
define Θ = Θ , and for the inner problem, we
l,t l,t/β
N
update w i,t := w i,t/γ. and by Donsker’s invariance U = 1 ∥W −W∗∥2 = 1(cid:88) ∥w −w∗∥2. (14)
principle [19], we approximate the rescaled noise terms t 2 t F 2 i,t i
i=1
as Wiener processes under the limits β →0 and γ →0,
These two metrics serve to gauge the extent to which
respectively.
the estimations deviate from the actual ground truths of
local model parameters and the task relationship.
For the inner problem, let node i generate data points
(X ,y ), k ∈ N+ instantaneously with rate µ > 0.
i,k i,k i A. Preliminary
When assuming the time required to compute gradient
estimates g locally is negligible compared to t time We use similar definitions and results as in [6] in
i,k
between model updates, it becomes equivalent to as- the convergence analysis. Consider the Laplacian matrix
suming that the gradient update rate at node i is µ . L=D−AofgraphG,whereDisthedegreematrix,and
i
Additionally,weconsidertherateofparameterexchange Aistheadjacencymatrix.Wedenotethesecondsmallest
with neighbors as ϖ , and we neglect the computation eigenvalue of L as λ . The continuous-time gradient
i 2
time for the network penalty gradient. As a result, function g defined above is a function of w , and in
i,t i,t
the update rate for the penalty gradient ∇ρ can be ouranalysis,wedenoteg (w∗)=X⊺ Ω−1X (w∗−w∗)
i,k i,t i i i
approximated as ϖ . Similarly, in the context of the and note that g (w)=0 for all i∈V and t, we write
i i,t
outer problem, we assume negligible computation time g(w∗) to simplify the notation. Similarly, we use g to
i
and a function gradient update rate of φ . Subsequently, denote g when the property holds for all t. We note
l i,t
we establish that the dynamics of Θ and w can thatthecorrespondinglossfunctionofg (thenoise-free
l,t i,t i
be modeled via the stochastic differential equations (see version of f ) is strongly convex with κ .
i i7
Letw andw betwoinputvectorstakenfromthe algebraic connectivity of the network (λ ) or the strong
i,1 i,2 2
function domain, then convexity constant κ is, the smaller the expected U .
t
(g (w )−g (w ))⊺ (w −w ) Here the function h 1 and h 2 are functions of V t that
i i,1 i i,2 i,1 i,2
(15) are associated with the bound of the Frobenius norm
≥κ ∥w −w ∥2 ≥κ∥w −w ∥2,
i i,1 i,2 i,1 i,2 of the current precision matrix estimation. The term
for κ
i
:= 2λ min(X⊺ iΩ−
i
1X i) and κ := minκ i, where (cid:82) m0t ae trc it xh 2 e( sV timt)U att iod nt osh no tw hs
e
pth ae ramin efl tu ee rn ec se timof att ih oe n.p Wre eci nsi oo tn
e
λ (·) is the smallest eigenvalue and hence g is √
min i
that the scalar u is a lower bound of U at time t,
strongly convex with κ . t t
i
we will give an alternative estimation for the bound of
We make a set of assumptions to establish bounds
h (V ) in Theorem 3 which does not depend on the
within our framework. Firstly, we assume the true rela- 2 t
selection of u .
tionship covariance matrix and the precision matrix are t
bounded. We introduce sequences {P } and {p } such
t t C. Consistency
that 0 < p ≤ λ (Σ ) ≤ λ (Σ ) ≤ P are also
t min t max t t The consistency measures {V ,t ≥ 0} captures the
bounded, ensuring these sequences remain bounded. In t
distance of the estimated task precision matrix to the
addition, also assume that the estimated task precision
ground true precision matrix for the outer problem. The
matrices Θ , l ∈ {1,...,q} are bounded, and hence
the sequencl e,t {Q t} that upper bound (cid:13) (cid:13)Θ−
t
1(cid:13) (cid:13)
F
and the f tao sll kow rei ln ag tiore nsu ol ft tp hr eov ei sd te imsa an teu sp ap te arb go ivu en nd to imnt eh .eexpected
inducedsequence{O }thatupperbound∥Θ ∥ arealso
t t F
bounded. Lastly, we make the assumption that {F } is Theorem 2. Let Θ evolve according to continuous
t l,t
a bounded sequence that upper bound [E([wc ] )4] with time dynamics (12a). Then
ij t
[w ic j] t being the (i,j)th element of Wc at time t of any (cid:90) t (cid:90) t
group. E[V ]=φ βˆ C (s)h(U )ds+φ βˆ C (s)dt
t s 1 s s 2
0 0
κ2N2 (cid:90) t
B. Regularity + j
2
βˆ t2dt
0
The regularity measure {U t,t ≥ 0} captures the (cid:113)
where the function C (t) = O + tr(Θ ) 2 and
distance of local model parameters to the ground truth 1 t ∗ p
(cid:113)
o uf ppt eh re bi on un ner dp or nob thle em e. xpT eh ce tef dol rl eo gw ui ln ag ritr yes ou flt thp ero ev si td imes ata en
s
C 2(t) = tr(Θ ∗)P t N2+NQ p2 t2 t +N are related to the
magnitude of the precision matrix estimation at t. The
at a given time.
function h(U ) is defined as a function of the regularity
t
Theorem 1. Let w
i,t
evolve according to continuous measure U t,
time dynamics (12b). Then (cid:112)
h(U )=N F
t t
(cid:80)N τ2(A +G )
E[U ]≤e−ctU +(1−e−ct) i=1 i i i + to bound the expected estimation matrix. Here {βˆ} is
t 0 c a decreasing sequence s.t. β = ββˆ with β a st mall
(cid:90) t t l,t
ecth (V )U dt constant smaller than 1, and βˆ <abs( tr(Θl,t) ) and
2 t t t 2tr(Θl,t−T)
where c =
20
µκ+2δ λ ϖ with µ = maxµ and ϖ =
βˆ
t
≤ 3N2m√k
Ft
with m
t
≤tr(Θ l,t) for all l.
1 2 i
max ϖ . The constants A and G describe the general Proof. See Appendix (VI-D).
i i i i
Brownian terms and are defined in (31) and (32). The We write the stepsize β = ββˆ as a production of
t t
function h 2(V t) is defined as a constant step β and a decreasing sequence βˆ t, and
h 2(V t)∼δ 2αϕh 1(V ut)√ P t, tr heq eu ti rr ae ce(cid:82) 0t oβ fˆ t thto e pc ro en cv ie sir og ne. mT ah te rixbo iu sn pd oso in tivβ eˆ .t Wen esu cr ae ns
t
observe that the bound on the expected V is influenced
t
where h (V ) is a function of V to bound ∥Θ ∥ (38)
1 t t j,t F by the estimation quality of the precision matrices.
for all j ∈ {1,...,q}, u is a small scalar such that
√ t Here h(U ) is a function of the regularity measure
t
0 < u ≤ U , and α is a constant describing the
t t and describes the bound of the estimation matrix. The
relationship of estimation variation. term φ(cid:82)t βˆ C (s)h(U )ds shows the influence of the
0 s 1 s
Proof. See Appendix (VI-C) . parameterestimationonthe precisionmatrixestimation.
Note that as t increases and the estimation of the
We can observe that the expected difference in es- precision stabilizes, C (t) and C (t) can be bounded
1 2
timates decreases with growing δ , which penalizes by constants, and we simplify the theorem in the next
1
disagreement with neighbors. Similarly, the larger the section.8
D. Two Time-scale Systems whichinvolvesprecisionmatrixestimation.Bymaintain-
ingfasterupdatesintheinnerproblemwhilemaintaining
In this section, we discuss the interaction of the two
a slower pace in the outer problem, the system attains
systems.InTheorem(1)and(2),wecanobtaintheupper
heightened robustness.
bound of E[U ] and E[V ] at time t. In this theorem, we
t t The following corollary follows immediately from
willdiscussthelimitingpropertyofthetwosystemsand
Theorem3andprovidesthechoicesofthestepsizesand
utilize universal bounds for the sequences {P }, {q },
t t precision update rates to achieve desired bounds on the
{Q }, and {O } and to provide simplified counterpart
t t consistency and regularity bounds.
of the previous theorems.
Corollary 1. In the long run, for any positive
Theorem 3. As t increases and the estimations of number ζ > 0, ζ > 0, define positive val-
1 2
the relation precision matrix become more stable, let ues ζ and ζ such that ζ + ζ ∼ 2ζ1 with
cOP === 2m Pm a trxa (Θx OP t. )t (cid:113), Dep N˜ fin 2= e +c N1m Q=in 2p +Nt, √ NQ F
,
a(cid:16) nO= d+ lem ttr ta ( ′x Θ sQ u∗) ct(cid:113) h, ta p2 hn (cid:17) ad t, β i t˜ n ht e′ o ou=
ut3
e ter(cid:82) r0 pt p′ ro rβˆ obt bd le
lt4
em. mA ad cs oju nφs st ta∼t nh te pζp
3
o3
r rme tic i oi ns n{io
c4
sn
′ 1t+
e1 pm
c2
sa i, zt 1 er
}ixβ˜
, at s′ cu hp βod oa s ∼te e
(cid:82)2 ∞ βˆ <ϵ fo∗ r a given ϵ>p˜ 02 , then ζ 4min(cid:110) min j(cid:2) √1 (cid:3) ,1(cid:111) , and set the inner problem
t′ t N ιjϕj
(cid:110) (cid:111)
lim E[V ]≤
φ(c 1+c 2)+κ j2N2 (cid:90) t′
βˆdt. (16)
s ct ae nps ei xz pe ea cts γ ∼ ζ 2min (cid:80)N i=1µic (′ Ai+Gi),1 , then we
t→∞ t 2 t
0
E[V ]∼ζ , and E[U ]∼ζ .
Inthelongrun,astheestimationmatrixW becomes t 1 t 2
i,t
more stable and βˆ t decreases, set √F = maxF t and Proof. See Appendix (VI-F) .
define c′ = 2µκ + 2δ λ − δ αϕO P. When δ ϕ <
1 2 2 2
2(µκ+ √δ1λ2), it follows that The preceding result provides us with choices for the
αO P inner and outer problem formulations, as well as the
(cid:80)N τ2(A +G ) precision matrix updating rate. These choices enable us
lim E[U ]≤ i=1 i i i . (17)
t→∞ t c′ to attain the predefined upper bounds established for the
expected values of V and U , which are set as ζ and
Proof. See Appendix (VI-E). t t 1
ζ , respectively.
2
In the long-term perspective, we replace the time-
IV. NUMERICALILLUSTRATION
dependent functions C (t) and C (t) with constants c
1 2 1
and c , respectively. This simplification is achieved by In this section, we apply the proposed method to
2
adopting universal bounds on the approximations of the two examples to corroborate the analytical results. First,
precision matrix. Similarly, the functions h (V ) and we apply the DAMTL algorithm to a Gaussian Markov
2 t
h(U ) are substituted with constant bounds, which are randomfield(MRF)estimationproblemusingawireless
t
included in the constants c and c . Given that (cid:82)t′ βˆdt sensor network (WSN) with synthetic data to show the
remains bounded, the upp1 er bou2 nd of E[V ] c0 ant be effectiveness of the algorithm. Next, we look at a real-
t
world problem: modeling the students’ study perfor-
controlled by the precision matrix update rate φ and
mance.
constant portion of stepsize β. These ensures that the
fluctuations of E[V ] can be bounded within desired In this section, we apply the proposed approach
t
through two illustrative examples. First, we employ
limits, supporting the stability and predictability of the
the DAMTL algorithm to a Gaussian Markov random
system.
field (MRF) estimation problem with a wireless sensor
In addition to the influence of the local consensus
network (WSN) with synthetic data. Next, we explore a
penalty parameters δ , the network connectivity λ , and
1 2 tangible real-world challenge involving the modeling of
the convexity constant κ, we can observe that the upper
students’ study performance.
bound of E[U ] is also influenced by the estimation
t
of the precision matrix and the group penalty δ . The
2
A. Temperature Estimation of A Field
condition involving δ ϕ requires the group penalty δ
2 2
to be much smaller than that of in group penalty δ , In this example, we use a similar setting as [6] 4.1.
1
and the node gradient updates are faster than that of We utilize a WSN for temperature estimation over a
precisionmatrixupdates.Thisessentiallyimpliesthatthe 10m×10m field divided into 100 equal squares. We
updates pertaining to the inner problem, which involves assumed that the temperature is uniform within each
parameter estimation, should occur at a much faster rate square,andN sensorsarearbitrarilyplacedonthefield.
compared to the updates related to the outer problem, The sensors can be divided into 4 groups according to9
geographicallocation.Weassumethefield’struetemper-
aturesaredifferentforthesensorsfromdifferentgroups,
and are saved in R100×1 vectors w∗ (note w∗ = w∗
i i j
if i,j in the same group). The sensors measure the
temperatureusingnoisylocalobservationsy ∈R100×1,
i
which are corrupted by measurement noise ε , unique
i
to sensor i, and network disturbance ξ, shared by all
sensors. Each sensor i shares only a portion of ξ, based
on a matrix Λ reflecting the sensor’s location and the
i
relativedistanceofthemeasuredsquare.Weassumew∗
i
is fixed but y i changes at each measurement, which can (a)
be expressed as follows:
y =w∗+ε +Λ ξ, (18)
i i i i
where ε ∼N (0,σ2I) and ξ ∼N (0,I). Note that if
i m i m
we set X =I, (1) and (18) have the same form.
i
Using a Gaussian MRF, we simulate the temperature
of the field and allow temperature values to range from
0◦F to 255◦F. The field has two heat sources located at
(2m,8.5m) and (8.5m,9m), and the temperature drops
from the heat source at a rate of 25◦ F/m within a
(b)
region of influence that spans 5m from the source. We
Figure 2. (a) Network structure of sensors. The dots denote
connect all nodes in a group with the group messenger
thenodesandthenodesfromthesamegroupareofthesame
and randomly connect nodes to their neighbors within color,thelinesrepresenttheedgesbetweennodes.Thereare4
2.5m. The messengers from different groups are all groups and the messenger nodes are marked by red dots. The
connected to transmit group estimations. See Figure two heat sources are located at (2m,8.5m) and (8.5m,9m)
and are marked yellow. (b) The estimation error comparison
2(a) for sensor locations and the heat map of the field.
of DAMTL, SG, and SG with a partial penalty.
Each node has the following local cost as in (3) with
ℓ (w )≜ 1(y −w )⊺Ω−1(y −w ).Thetaskrelation
i i 2 i,k i i i,k i
is estimated with (5) with T as the identity matrix.
B. Students’ Study Performance
Weaimtominimizeeachsensor’scostfunctionusing
DAMTL by selecting proper w . We use a stepsize of
i
γ = 10−4 for the inner problem and set the penalty
In this section, we consider a real data set “Junior
parameter as δ = 1500 and δ = 0.9. For the outer
1 2 School Project” from Peter Mortimer [20], which is a
problem,weutilizeadecreasingstepsizeβ = 1,where
k k longitudinal study of about 924 pupils from 50 primary
k is the iteration. We set a minimum individual noise
schools chosen at random among the 636 schools under
variance of 0.01, and allow the maximum variance to
the Inner London Education Authority (ILEA) in 1980.
vary between 1 and 5. We define the estimation error at
We build a regression model to find the relationship
time point k as
between student scores and other qualifications.
Est =
1(cid:88)4 (cid:13) (cid:13)W(l)−W∗(cid:13)
(cid:13) ,
Inthemodel,thescore
5
isconsideredastheresponse
k 4 (cid:13) k (cid:13) F variable, and the predictors are gender (student gender
l=1
changed to numeral), social (student father’s class, cat-
where W(l) is the estimated matrix at time point k egorical), raven (raven test score), english (English test
k
from group l. Figure 2(b) compares the estimation error score),math(mathtestscore).Wenormalizeallvariables
fromDAMTL(purple),SGwithonlytaskpenalty(red), and see each school as a node. Though students were
SG with noise penalty (yellow), and SG (blue). We tested on the same measure, because of the teaching
observe DAMTL has the fastest convergence, which quality difference, we assume the true model for each
shows its effectiveness. When only using one penalty, schoolisdifferentyetcorrelated.Wegroupall46schools
thealgorithmisnotasefficientasDAMTL,whichshows into 5 groups and select one school as the messenger,
theeffectivenessofthepenaltiestargetingthenoisefrom we connect all the messengers and randomly connect
both data and task relations. schools with their neighbors (See Figure 3(a)). At each10
node, the score
5
estimate is given by V. CONCLUSION
scoˆre =w +w ·gender+w ·social+w ·raven The ever-increasing dimension of data and the size of
5 0 1 2 3
+w ·englisch+w ·math, datasets have introduced new challenges to centralized
4 5
estimation, especially when data comes from streams
We define the estimation error at time point k for and the underlying data models are different. In such
school i as cases, we consider a two-timescale distributed learning
architecture of nodes that can be partitioned into several
Err (i)=∥score −X w ∥,
k 5 i i,k interconnected groups. In the proposed scheme, each
where X is the matrix containing the observations and node (or local learner) receives a data stream and asyn-
i
w is the DAMTL estimation for school i at time k. chronously implements stochastic gradient updates, and
i,k
We set the parameter stepsize of the inner problem as one selected node per group (called messenger), in a
0.0005 and a decreasing sequence β = 1 for the outer slower frequency, periodically exchange group parame-
k k
problem.Thepenaltyasδ =20andδ =2.Figure3(b) ter estimation and estimate task relationship precision
1 2
shows the estimation error of DAMTL for all groups at matrix. To ensure robust estimation, a local penalty
the final iteration of 50. targeting noise reduction and a global regularization
targeting overall performance improvement. We provide
finite-timeperformanceguaranteesontheconsistencyof
the parameter estimation and regularity of the precision
matrix estimation. We illustrate the application of the
proposedmethodfortemperatureestimationinaMarkov
Random Field with synthetic datasets and a real-world
problem of students’ study performance.
VI. APPENDIX
A. Preliminary
(a)
We use the following definition and Ito’s lemma. Let
f :X →S be a function with gradient ∇f(x).
Definition 1. Twice differentiable function f is said to
be κ-strongly convex, if
κ
(∇f(x )−∇f(x ))⊺ (x −x )≥ ∥x −x ∥2
1 2 1 2 2 1 2
for some κ > 0 and all x , x ∈ X. Or equivalently,
1 2
∇2f(x) ⪰ κI for all x ∈ X, i.e., a (∇2f(x)) ≥ κ.
min
where ∇2f(x) is the Hessian matrix, and a (·) is the
min
minimum eigenvalue.
(b)
Figure 3. (a) network of schools. Schools are denoted by Lemma 1. Multidimensional Ito Lemma [21]
dots and grouped in different colors. Neighboring schools are Let X(t)=[X (t),...,X (t)]⊺ be a p-dimensional Ito
1 p
connectedbylinesandtheselectedmessengersaredenotedby process with
reddotsandinterconnected.(b)Estimationerrorofallgroups,
and each denoted by a different color. dX(t)=udt+VdB(t),
where u is a vector of length p, V is a p×p matrix,
The results unveil a rapid convergence of estimations and B(t) = [B (t),...,B (t)]⊺ is a (standard) p-
1 p
across all schools. The collective average estimation dimensional Brownian motions. Let g(x) be a twice
errors across the five distinct groups are showed in the differentiable map from Rp into R. Then the process
vector Err = [2.0107,2.0771,1.7574,2.2998,2.5251].
Y(t)=g(X(t))
We can observe the average error magnitude for each
group is around the value of 2, and the disparities in is again an Ito process with
the error values could potentially result from variations
in school characteristics, as well as differences in the
(cid:88)p ∂g(X) 1(cid:88)p ∂2g(X)
dY(t)= dX + dX dX ,
quality of teaching provided across these schools. ∂x i i 2 ∂x i∂x j i j
i=1 i,j11
wheredX dX iscomputedusingrulesdtdt=dtdB = Consider the second to the fourth term in (19). In
i j i
dB dt=0, dB dB =1 if i=j and 0 otherwise. the re-scale process, we “squeeze” N (t/γ) function
i i j g,i
gradient updates in the interval [0,t]. We assume γ ≪
In our setting, g(x) = 1∥x∥2 with ∂g = x and
∂∂ x2g 2
i
=1, then the process
2 ∂xi i µ
µγi
i
a =nd γ∆pa tr gt ,i it .io In
t
ft oh li ls owin ste tr hv aa tl :into subintervals of length
p
dY(t)=(cid:88) X idX i+ 1 2dX·dX γNg (cid:88),i(t/γ) g
i,(l)
=γN g,i t(t/γ)Ng (cid:88),i(t/γ) g
i,(l)N
(t
t/γ)
i g,i
l=1 l=1
p p p
=(cid:88)
X i(u
idt+(cid:88)
V i,kdB k)+
1 2(cid:88)
dX idX
j
≈N
g,i(t/γ)Ng (cid:88),i(t/γ)
g
γ
i k i,j t/γ i,(l)µ
i
1 l=1
=X⊺ dX+ 1⊺ dXdX⊺ 1 (cid:90) t
2 ≈µ g ds.
i i,s
Let V denote the ith row of V, we can expand 0
i· (20)
1⊺dXdX⊺1 as
Similarly, we have the continuous approximation for
⊺ ⊺
1 dXdX 1
the local penalty
 
u dt+V dB
1 1·
=1⊺  . . .  (cid:104) u 1dt+V 1·dB,...,u pdt+V p·dB(cid:105) 1γNn (cid:88),i(t/γ) ∇ρ i,(l) =N n, ti /( γt/γ)Nn (cid:88),i(t/γ) ∇ρ i,(l)N t (t/γ)
u pdt+V p·dB l=1 l=1 n,i
(cid:88)(cid:88)(cid:88) (cid:90) t
= V i,kV j,kdt, ≈ϖ
i
∇ρ i,sds.
i j k 0
(21)
andthecontinuousapproximationforthetaskpenalty
B. Continuous Time Approximation
Nr,i(t/γ)
Inthissection,wewillderivetheformulaofdw i,tand γ (cid:88) ∇ wiρ r(W (( lj )))
Θ by writing the scheme (11a) and (11b) in the form
l,t l=1
o thf eth ne oiss eum tem rmat sion byof stap nr dev ai ro dus ms -dte ip ms e, na sn iod naa lpp Br ro ox wim nia at ne
=N
r,i(t/γ)Nn (cid:88),i(t/γ)
∇ ρ(j) t (22)
t/γ wi r,(l)N (t/γ)
motions and the rest by integrals. Then dw and r,i
i,t l=1
dΘ can be approximated by the differential form of (cid:90) t
l,t ≈ϕ ∇ρ(i,j)ds.
a stochastic Ito integral. We assume the noise terms i r,s
0
have zero-mean Gaussian distribution: ξ ∼N (0,I )
k m m
and ε
i,k
∼ N m(0,σ i2I m) for all i in the inner prob- Here, ∇ρ( ri ,s,j) denotes the task penalty gradient update
lem, and for each column of ς in the outer problem at time s of the ith node from the jth group.
ςi ∼ N(0,ι2). Let v(q) denote the component on the Nowconsidertheindividualnoise.Weassumeε ∼
j,k j i,(l)
qth dimension of v. N (0,σ2I), and all components of ε are indepen-
m i i,(l)
1) Inner Problem Approximation: dent.WeassumethateachrowofX isindependentand
i
Foreachnodei∈G andallj,werewritethescheme identically sampled from a multivariate normal distribu-
j
(11b) as: tion N(ν ,Ψ ) and Λ is diagonal. Since Ω−1 is a diag-
i i i i
onal matrix, X Ω−1 is a random matrix with rows in-
Ng (cid:88),i(t/γ) Nn (cid:88),i(t/γ) dependentandii dei
nticallyfollowN(Ω−1ν ,Ω−1ΨΩ−1).
w =w −γ g −γδ ∇ ρ (w) i i i i
i,t i,0 i,(l) 1 wi i,(l) Let ζ :=X⊺ Ω−1ε , then the qth component of ζ
l=1 l=1 i,l i,(l) i i,(l) i
can be seen as a linear combination of the components
−γδ
2Nr (cid:88),i(t/γ)
∇ wiρ r(W (( lj
)))+γNg (cid:88),i(t/γ)
X⊺ i,(l)Ω−
i
1ε
i,(l)
of ε i,(l) and the qth row of X⊺ iΩ− i 1:
l=1 l=1 d
+γNg (cid:88),i(t/γ)
X⊺ i,(l)Ω−
i
1Λ iξ
(l)
ζ i( ,q l) = k(cid:88) =1(X⊺ i,(l)Ω− i,(1 l)) q,k(ε i,(l)) k.
l=1 Note that for ∀k, the pair (X⊺ Ω−1 ) and
(19) i,(l) i,(l) q,k
(ε ) are two independent normal random variables,
where∇ℓ i,k =g i,k−X⊺ i,kΩ− i 1(ε i,k+Λ iξ k)withg i,k := thei, i( rl) pk roduct is a linear combination of two degree
X⊺ Ω−1X (w −w∗). 1 Chi-square random variables 1(cid:0) (X⊺ Ω−1 ) +
i,k i i,k i,k 4 i,(l) i,(l) q,k12
(ε ) (cid:1)2 − 1(cid:0) (X⊺ Ω−1 ) −(ε ) (cid:1)2 , which are Ξ . We can approximate the common noise term as
i,(l) k 4 i,(l) i,(l) q,k i,(l) k i
non-central and dependent. For any two j ̸= k,
( (X X⊺ i ⊺,(l)) Ωq −,j 1 a )nd (ε(X⊺ i, )(l)) q +,k (Xar ⊺e c Ωor −re 1la )ted (εand )he fn oc le - γNg (cid:88),i(t/γ) X⊺ i,(l)Ω− i 1Λ iξ (l) ≈√ γµ iW i,t =τ iW i,t.
i,(l) i,(l) q,k i,(l) k i,(l) i,(l) q,j i,(l) j l=1
lows gamma or generalized gamma for different val- (24)
ues correlation coefficient [22]. We can see ζ(q) as a
i,l Substituting (20), (21) and (22) to the corresponding
linear combination of several correlated (generalized)
termsin(11b),w approximatelysatisfiesthefollowing
i,t
Gammandistributedrandomvariables,accordingto[23],
stochastic Ito integral:
[24], it can be approximated as a new (generalized)
(cid:90) t (cid:90) t
Gamma random variable. Hence ζ can be considered
i,l w =w −µ g ds−δ ϖ ∇ρ ds−
i,t i,0 i i,s 1 i i,s
asamultivariateGammadistributedrandomvectorwith
0 0
correlation [25]. (cid:90) t (cid:90) t (cid:90) t
δ ϕ ∇ρ(i,j)ds+τ dB +τ dW .
2 i r,s i i,s i i,s
0 0 0
We assume ζ′s are i.i.d with E[ζ2 ] = Υ for all l ∈ Takingthederivativeoftheaboveequation,weget(25).
i i,l i
N+, since dw =−(µ g +δ ϖ ∇ρ +δ ϕ ∇ρ(i,j))dt+
i,t i i,t 1 i i,t 2 i r,t
E(cid:104)Ng (cid:88),i(t/γ)
ζ
(cid:105) =Ng (cid:88),i(t/γ)
E[X⊺ Ω−1]E[ε ]=0,
τ idB i,t+τ idW i,t.
i,l i,(l) i i,(l) (25)
l=1 l=1
then by multidimensional Donsker’s theorem [26], 2) Outer Problem Approximation:
√
t
Ng (cid:88),i(t/γ)
ζ
→−d
B ,
decL re et asβ ink g= seqβ uβ eˆ k nceas βˆp kr .od Inuct tho ef ra e-sc co an lesta pn rt ocβ esa s,nd wea
(cid:112) i,l i,t
N (t/γ) “squeeze” N (t/β) function gradient updates in the
g,i l=1 c,j
interval [0,t]. We assume β ≪ φ and partition this
i
where →−d denotes converge in distribution, B is a interval into subintervals of length β = γ∆t . For
general m dimensional Brownian motion withi, ct ovari- each group j, rewrite the scheme (11φ ai ) as c,j
ance Υ , i.e, B = C B with B being a standard
m
C
Cdi ⊺m
=ei
ns Υion .a Ll
eBi t,t
r τow =nia
√ni γmt
µo ,tio wn ea cn
at
d nC api p∈ roR xip m× ap tew ti hth
e
Θ
j,t
=Θ
j,0−βNc, (cid:88)j(t/β)
βˆ k(S j,k+δ 3(Θ j,k−T k)−Θ− j,k1)
i i i i i k=1
individual noise term as
Nc,j(t/β)
γNg (cid:88),i(t/γ) X⊺ Ω−1ε ≈√ γµ B =τ B . (23) +β (cid:88) βˆ kς j,k
i,(l) i i,(l) i i,t i i,t k=1
l=1 (26)
consider the second term of (26),
X⊺
iN ,(lo )w
Ω−
iw 1Λe iξc (o l)n ,sid wer
e
th ne otc eom thm ao tn fn oo rise ∀. k,Let theϑ
i,l
pa=
ir
βNc, (cid:88)j(t/β)
βˆ k(S j,k+δ 3(Θ j,k−T k)−Θ− j,k1)
(X⊺ Ω−1 Λ ) and (ξ ) are two independent nor- k=1
mali,( rl a) ndi, o( ml) vi aq r, ik
ables.
Si( ml) ilk
ar to the argument in the ≈N
c,i(t/β)Nc (cid:88),i(t/β)
βˆ (S +δ (Θ −T )−Θ−1)
β
individual noise approximation, ς i,l can be considered t/β k j,k 3 j,k k j,k φ j
k=1
as a multivariate Gamma distributed random vector
(cid:90) t
with arbitrary correlation. We assume ϑ′s are i.i.d with =φ βˆ (S +δ (Θ −T )−Θ−1)ds.
i j s j,s 3 j,s s j,s
E[ϑ2 i,l]=Ξ i for all l∈N+, it follows that 0
(27)
E(cid:2)Ng (cid:88),i(t/γ)
ξ
(cid:105)
=0,
We assume all elements are independent for the noise
l term,andeachcolumnςi ∼N(0,ι2).Weapproximate
j,k j
k=1 (cid:80)Nc,j(t/β)ςi
by a m-dimensional standard Brownian
√
t
Ng (cid:88),i(t/γ)
d
mok t= io1
nM
j,t,j i, ,k
denotingtheBrowninanapproximationof
ϑ →− W ,
(cid:112) N (t/γ) i,l i,t the ith column of the noise matrix from group j at time
g,i l=1 t.
whereW isageneralmdimensionalBrownianmotion
i,t Nc,j(t/β)
with covariance Ξ i, i.e, W i,t = D iW t with W t being a β (cid:88) ςi ≈ι (cid:112) βφ M =κ M (28)
standard m dimensional Brownian motion and D D⊺ = j,k j j j,t,i j j,t,i
i i k=113
where κ = ι (cid:112) βφ . Let M = [M ,...,M ], Then it follows that
j j j j,t j,t,1 j,t,N
then we can express (26) as N N
(cid:88) ⊺ (cid:88) ⊺
(cid:90) t dU t =− e i,tµ ig idt− e i,tδ 1ϖ i∇ρ i,tdt−
Θ =Θ −φ βˆ (S +δ (Θ −T )−Θ−1)ds
j,t j,0 j s j,s 3 j,s s j,s i=1 i=1
0 N N
+κ (cid:90) t βˆ dM . (cid:88) e⊺ i,tδ 2ϕ i∇ρ( ri ,t,j)dt+(cid:88) τ ie⊺ i,tdB i,t+ (35)
j s j,s
0 i=1 i=1
(29) N N
(cid:88) τ e⊺ dW +(cid:88) τ2(A +G )dt.
Taking derivative of (29), we can get (30) i i,t i,t i i i
i=1 i=1
dΘ j,t =−φ jβˆ t(S j,t+δ 3(Θ j,t−T t)−Θ− j,t1)dt (30) Consider the first term of (35), let µ = maxµ i and by
+κ βˆdM strong convexity assumption and the gradient at ground
j t j,t
truth g(w∗) is zero,
C. Proof of Theorem 1
N
Inthissection,weconsidertheregularitymeasurefor −(cid:88) (w −w∗)⊺ µ g
i,t i i i,t
the inner problem (14). i=1
N
U = 1 ∥W −W∗∥2 = 1(cid:88)N ∥w −w∗∥2. ≤−µ(cid:88) (w i,t−w i∗)⊺ (g i,t−g(w i∗)) (36)
t 2 t F 2 i,t i
i=1
i=1
N
L 1e (cid:80)t e ∥i, et = ∥2.w Ii n,k th−
e
w foi∗ ll, owth ie nn gsw
,
e weca fin rstrew cor nit se idU ert th=
e
≤−µκ(cid:88) ∥e i,t∥2 ≤−2µκU t.
2 i i,t i=1
term d1∥e ∥2.
2 i,t For the second term of (35), define the stacked vector
Similar to the discussion in Lemma 1, the individual
e =[eT ,...,eT ]T.Noticethattheconsensuspenalty
noise Brownian term can be seen as: t 1,t N,t
only works within a group (we assumed that only group
1⊺ dB dB⊺ 1=1⊺ d(C B )d(C B )⊺ 1 members (except messengers) are connected, and w∗ =
i,t i,t i t i t i
=1⊺ C dB dB⊺ C⊺ 1⊺ =2A dt, w j∗ when a i,j =1, otherwise, a i,j =0), then
i t t i i
N
where B t is a standard Brownian motion with −(cid:88) ϖ (cid:88) a (w −w )⊺ (w −w∗)
i i,j i,t j,t i,t i
(cid:88)(cid:88)(cid:88)
A =1/2 ci ci , i=1 j̸=i
i lk jk (31)
l j k =(cid:88)N
ϖ ∥e
∥2(cid:88)N
l
−(cid:88)N
ϖ
(cid:88)N
l
e⊺
e
where ci is the (l,k)th element of matrix C . While i i,t i,j i i,j j,t i,t
lk i i=1 j=1 i=1 j=1
for the common noise, let W be a standard Brownian
t N
motion, then ≤−ϖe⊺ Le ≤−ϖλ (cid:88) ∥e ∥2 =−ϖλ U
t t 2 i,k 2 t
1⊺ dW dW⊺ 1=1⊺ D dW dW⊺ D⊺ 1⊺ =2G dt, i=1
i,t i,t i t t i i
where ϖ = max ϖ , L = L ⊗ I with ⊗ denotes
i i m
where
the Kronecker product, and λ is the second smallest
2
(cid:88)(cid:88)(cid:88)
G =1/2 di di , eigenvalueofL.NotethatthecolumnsumsofLiszero,
i lk jk (32)
l j k hence
(cid:80)N
i=1ϖ i∥e
i,t∥2(cid:80)N
j=1l i,j = 0. Then the second
term (35) becomes
where di is the (l,k)th element of matrix D . Then the
lk i
production of the Brownian terms becomes N N
(cid:88) ⊺ (cid:88) (cid:88) ⊺
−δ β e ∇ρ =−δ β a e e
de ·de =τ2dB ·dB +τ2dW ·dW 1 i i,t i,t 1 i i,j i,t i,t
i,t i,t i i,t i,t i i,t i,t (33) i=1 i=1 j̸=i
=2τ i2(A i+G i)dt.
≤−δ λ
ϖ(cid:88)N
∥e ∥2 ≤−2δ λ ϖU .
Now we apply the Ito’s lemma to d1∥e ∥2, by (25) 1 2 i,t 1 2 t
2 i,t i=1
and (33), (37)
1 1
d∥e ∥2 =e ·de + de ·de Weconsiderthethirdtermof(35).Letρ(i,j) denotesthe
2 i,t i,t i,t 2 i,t i,t r,t
task relation penalty for node i from group G at time
=−e⊺ i,t(µ ig i,t+δ 1ϖ i∇ρ i,t+δ 2ϕ i∇ρ( ri ,t,j))dt t. Let W(j) = [w]j,t denote the estimation mj atrix for
+τ ie⊺ i,tdB i,t+τ ie⊺ i,tdW i,t+τ i2(A i+G i)dt groupjatt timet,Ml ( tk j) =[m( tj),...,m( tj)]withm( tj) =
(34)14
[mj,t,...,mj,t]⊺ is the ensemble average estimation of ϕ=maxϕ , then the third term of (35) gives
1 p i
M at t of group j, and Θ = [ϱ]j,t is task precision √
matrixestimationfromgrout p,j jofΣ−h 1q
att(usingW t(j)). δ
2(cid:88)N
ϕ ie⊺ i,tv ij
,k
≤δ
2αh 1(V Nt) P
t
(cid:88)N
ϕ ie⊺ i,t1
i=1 i=1
vj,t :=∇ tr((W(j)−M(j))Θ (W(j)−M(j))⊺ ) √
i
=∇
ww ii
tr(Θ
t,jt
(W
t(j)−t
M(
tjt ),j
)⊺
(Wt t(j)−Mt
( tj))) ≤δ
2αh 1(V Nt) P
t
(cid:88)N
ϕ i∥e i,t∥ 1
i=1
N N p √
=∇ wi(cid:88)(cid:88) ϱj h, kt(cid:88) (w lj k,t−mj l,t)(w lj h,t−mj l,t). ≤δ αϕh 1(V t) P t (cid:88)N √ N∥e ∥
h=1k=1 l=1 2 N i,t 2
√ i=1
Here vj,t =2[(cid:80)N ϱ (wj,t−mj,t),(cid:80)N ϱ (wj,t− h (V ) P
mj 2,t),.i ..,(cid:80)N k=1k ϱ= k1 i(wk pji , kt −1k mj p,t)1 ] is thk e=1 grak di ien2 tk of ≤δ 2αϕ 1 ut
t
tU t.
tr((W−M)Σ−1(W−M)⊺) from group j at time t (39)
of node i. The lth element of vj,t can be seen as the
i The last inequality is because (cid:80)N ∥e ∥ ≤
inner product of the ith column of Θ t,j and lth row of √ √ (cid:113)i=1 i,t 1
(W t(j)−M( tj)). N √(cid:80)N i=1∥e i,t∥
2
≤ N(cid:80)N
i=1
N∥e i,t∥2
2
=
asW me j,e tsti =mate 1t (cid:80)he Nlth wel je tm
.
e Fn ot ro nf ot dh ee e in ,se (m wjb ,l te −av mera j,g te
)
N rewrU itet (≤ 35)N u aU t st. By (33), (36) (37), and (39), we can
l N i=1 li 1i l √
describes the distance of the mean and individual esti-
mations, and we assume (w 1j, it−mj l,t)≤α(cid:113) [s ii]j
t
with dU t ≤−(2µκ+2δ 1λ 2ϖ−δ 2αϕh 1(V ut) P t)U tdt+
[s ]j beingthe(i,i)thelementofthecovarianceestimate N N N
froii mt groupj,andα≥3isasufficientconstantsuchthat +(cid:88) τ ie⊺ itdB i,t+(cid:88) τ ie⊺ itdW i,t+(cid:88) τ i2(A i+G i)dt
the inequality holds almost sure. We assume 0 < p ≤ i=1 i=1 i=1
t
(40)
λ (Σ ) ≤ λ (Σ ) ≤ P , where {p } and {P }
min j,t max j,t t t t
√
are two nonnegtive sequences bounding the eigenvalues Let c = 2µκ+2δ λ ϖ and h (V ) = δ αϕh1(Vt) Pt,
ofthecovariancematrixestimationsofallgroups.Since
and consider
consi1 der2
the
deriv2 ativt
e of
ec2
tU ,
ut
Σ is Hermitian, λ (Σ ) ≤ [s ] ≤ λ (Σ ) for t
t min j,t ii t max j,t
all i by Schur-Horn theorem.
d(ectU )=ectdU +cectU dt
Note the relationship between ∥Θ ∥ and the task t t t
j,t F
measure V
t
and ∥Θ ∗∥ F,
≤ecth (V )U
dt+ect(cid:88)N
τ2(A +G )dt
2 t t i i i
∥Θ ∥ ∝V −∥Θ ∥ .
j,t F t ∗ F i=1
N N
We set +ect(cid:88)
τ
e⊺
dB
+ect(cid:88)
τ
e⊺
dW
i i,t i,t i i,t i,t
N
(cid:88) i=1 i=1
h (V )=Nmax( ϱt ) (38) (41)
1 t qi
q=1
Define the summation of Ito terms,
as a function of V to bound ∥Θ ∥ , then
t j,t F N
(cid:118) (cid:118) KdB˜ =(cid:88) K dB˜ ,
(cid:117) N N (cid:117) N N t 1,i t
∥Θ j,t∥
F
=(cid:117) (cid:116)(cid:88)(cid:88) ϱ2
ij
≤(cid:117) (cid:116)((cid:88)(cid:88) ϱ ij)2 i=1
i=1j=1 i=1j=1 where K idB˜ t = τ ie⊺ i,tdB i,t + τ ie⊺ i,tdW i,t. Integrating
N N N both sides of the inequality in (41),
=(cid:88)(cid:0)(cid:88) ϱt (cid:1) ≤Nmax((cid:88) ϱt )=h (V ).
ij qi 1 t (cid:90) t
i=1 j=1 q=1 U t ≤U 0+ ecth 2(V t)U tdt
0
Then the bound of the lth element of v ij,t becomes +(1−e−ct)(cid:80)N i=1τ i2(A i+G i) +(cid:90) t
ecsKdB˜
N N c s
(cid:88) ϱt qi(w lj q,t−mj l,t)≤α(cid:88) ϱ qi(cid:112) [s ii] t 0 (42)
q=1 q=1
Since the stochastic integral is a martingale,
√
N
≤α(cid:112) P (cid:88) ϱt ≤ αh 1(V t) P t. (cid:104)(cid:90) t (cid:105)
t qi N E ecsKdB˜ =0.
s
q=1
0
√
Since U is nonnative, find u , s.t 0 < u ≤ U . Let Take expectation of (42), we can obtain an upper bound
t t t t15
of the regularity measure (11b) S = [o ] , and o = 1(cid:80)p wc wc , where
ij 1≤i,j≤N ij p l=1 l,i l,j
wc is the ith element of the lth row of Wc. Let F be
(cid:80)N τ2(A +G ) l,i √ t
E[U t]≤e−ctU 0+(1−e−ct) i=1 i
c
i i
(43)
Ea (s wca 2la )r ),t wo hb eo ru en [d wto ][E th(w
ei ej
l) e4 m] e( na tn sd oh fe Wnce
c
(aF
ppt
lybo tu on ad ls
l
(cid:90) t ij ij t t
+ ecth 2(V t)U tdt groups). Let Σ t be the covariance matrix for S j,t for all
0 groups j:
p
D. Proof of Theorem 2
E[S ]=
1(cid:88)
E[wc(wc)⊺ ]=Σ ,
In this section, we consider the task relation measure j,t p l l t
l=1
fortheouterproblem(11a).Supposethereareq groups,
Theempiricalcovariancematrixentrieshavethefollow-
the task relation measure (13) can be written as the
ing properties:
summation of norms of the columns,
1 1
V
k
= 21
q
(cid:88)q ∥Θ j,k−Θ ∗∥2
F
= 21
q
(cid:88)q (cid:88)N (cid:13) (cid:13)Θi j,k−Θi ∗(cid:13) (cid:13)2 , Var(o ij)= p 1Var(w lc ,iw lc ,j)= pE(w lc ,iw lc ,j) F2
j=1 j=1i=1 ≤ [E(wc )4]1/2[E(wc )4]1/2 ≤ t,
p l,i l,j p
where Θi is the ith column of Θ and Θi is the ith
j,k j,k ∗
column of Θ . and
∗
For each group j, let c
j,t,i
=Θi j,t−Θi ∗, we consider E(o ij)=s ij,
the derivative of the term 1(cid:13) (cid:13)Θi −Θi(cid:13) (cid:13)2 . Note that whereΣ=[s ](denotedasΣ =[s ] ).Theno isan
2 j,t ∗ ij t ij t ij
c j,t,i can be considered as the ith column of Θ j,t−Θ ∗. unbiased estimator of s ij with a variance O(1/p). Note
By (30), we have that
dc j,t,i =−φ jβˆ t(cid:104) S j,t+δ 3Θ j,t−T t)−Θ− j,t1(cid:105) idt+βˆ tκ jdM jE ,t,(io ij −s ij)2 =E(o2 ij)−2s ijEo ij +s2 ij ≤ F pt +s2 ij
where [·] i denotes the ith column of the matrix. Note For the trace of S,
that dc ·dc =βˆ2κ2Ndt, then
j,t,i j,t,i t j p p
1 1 tr(S)=
1(cid:88)
tr(wc(wc)⊺ )=
1(cid:88)
∥wc∥2
d ∥c ∥2 =c ·dc + dc ·dc p l l p l
2 j,t,i j,t,i j,t,i 2 j,t,i j,t,i l=1 l=1
(cid:104) (cid:105)
=−φ jβˆ tc⊺
j,t,i
S j,k+b t(Θ j,k−T k)−Θ− j,k1 idt
Etr(S)=tr(E(S))=
1(cid:88)p (cid:88)N
E[(wc )2]≤N(cid:112) F .
βˆ2κ2N p l,i t
+κ c⊺ dM + t j dt. l=1 i=1
j j,t,i j,t,i 2 Now we prove tr(Θ) is positive. We first discuss in the
(44)
contextofiterationsandthenextendtocontinuoustime.
The derivative of the measure (13) is the following: Let βˆ
k
< abs( 2trt (r Θ(Θ kk −) T)) and βˆ
k
≤ 3N2m√k
Ft
with m
k
≤
tr(Θ )forallj.Weprovebyinduction,firstlyassuming
q N j,k
dV
t
= 21
q
(cid:88)(cid:88) d∥c j,t,i∥2 tr(Θ 0)>0 and tr(Θ k)>0, then
j=1i=1 tr(Θ )=tr(Θ −β (cid:0) S +b (Θ −T)−Θ−1(cid:1) )
=−
1(cid:88)q (cid:88)N
φ
βˆc⊺ (cid:104)
S +b (Θ −T
)−Θ−1(cid:105)
dt
k+1 =tr(Θk k)−βk k(cid:0) tk r(S k)k +bk ktr(Θ k−T)k (cid:1)
q j=1i=1 j t j,t,i j,k t j,k k j,k i +β ktr(Θ− k1)
+
1(cid:88)q (cid:88)N
κ c⊺ dM +
1(cid:88)q (cid:88)N βˆ t2κ j2N
dt
≥tr(Θ k)−β k(cid:0) tr(S k)+b ktr(Θ k−T)(cid:1)
q j=1i=1 j j,t,i j,t,i q j=1i=1 2 ≥tr(Θ k)− 3β 2k tr(S k)
=−
βˆ
t
(cid:88)q
φ tr((Θ −Θ )⊺ (S +b (Θ −T )−Θ−1))dt >tr(Θ )−
3 2m
√k tr(S )
q j j,k ∗ j,k k j,k k j,k k 23N F k
j=1 t
tr(Θ )
+
1
q
(cid:88)q
κ
j(cid:88)N
c⊺ j,t,idM j,t,i+
βˆ t2κ 2j2N2
dt.
≥tr(Θ k)−
tr(S
kk )tr(S k)=0
j=1 i=1 We then embed the result in continuous time according
(45)
to (27), then we have tr(Θ )>0. We finish discussing
k,t
Now we discuss some matrix properties for the jth thebasicmatrixpropertieshere,andcontinuetoconsider
group at time t. For convenience, we drop the sub- the bound for (45).
scription for now. Denote the empirical covariance as16
Now we consider the first term of (45), note that Tate expectations of the equation above,
E∥S −Σ ∥2
≤(cid:88)N (cid:88)N
[s ]2+
N2F
t
tr(Θ j,t)Etr(S j,t)+N +2ˆb t∥Θ j,t−Θ ∗∥2
F
+
j,t t F ij t p (cid:115)
2N2F
i=1j=1 tr(Θ∗) t +4∥Σ ∥2 (N +λ2 (Σ−1)Q2)
N2F p t F max t t
=∥Σ ∥2 + t,
t F p ≤O N(cid:112) F +N +2ˆb ∥Θ −Θ ∥2 +
t t t j,t ∗ F
and (cid:115)
2N2F
(cid:13) (cid:13)Σ t−Θ−
t
1(cid:13) (cid:13)2
F
=(cid:13) (cid:13)Σ tΣ−
t
1(Σ t−Θ−
t
1)(cid:13) (cid:13)2
F
tr(Θ∗) p t +4∥Σ t∥2 F (N +λ2 max(Σ− t 1)Q2 t)
≤ =∥∥Σ Σt t∥ ∥2 F 2
F
(cid:13) (cid:13) (cid:13) (cid:13)Σ I− t −1 (( ΘΣ t tΣ− t)Θ −− t 1)1 (cid:13) (cid:13))(cid:13) (cid:13) 2 F2 2 . =N(cid:16) tr(Θ∗)(cid:115) 2 pF t +4∥Σ t∥2 F ( N1 + λ2 max( NΣ 2− t 1)Q2 t)
We assume (cid:13) (cid:13)Θ− t 1(cid:13) (cid:13) F ≤Q t and hence +O t(cid:112) F t++1(cid:17) +2ˆb t∥Θ j,t−Θ ∗∥2 F .
√ (47)
NQ
∥Θ ∥ ≤ t :=O . (46)
k F δ (Θ−1)2 t Note that
min t
Here {Q t} is a bounded sequence that bounds (cid:13) (cid:13)Θ− t 1(cid:13) (cid:13) F ∥Σ t∥2 F λ2 max(Σ− t 1)
and {O } can be obtained from {Q }. λ2 (Σ ) P2
t t =tr(Σ2)λ2 (Σ−1)≤N max t ≤N t .
(cid:13) (cid:13)I−(Θ tΣ t)−1(cid:13) (cid:13)2
F
≤2∥I∥2
F
+2(cid:13) (cid:13)(Θ tΣ t)−1(cid:13) (cid:13)2
F
t max t λ2 mi √n(Σ t) p2 t
≤2N +2(cid:13) (cid:13)Σ−1(cid:13) (cid:13)2(cid:13) (cid:13)Θ−1(cid:13) (cid:13)2 Let ∥W∗∥ F = B∗, we note that F t is related to the
t 2 t F regularity measure U t:
≤2N +2λ (Σ−1)2Q2
max t t 1
=2(N +λ2 max(Σ−
t
1)Q2 t) E[U t]= 2E∥W t−W∗∥2 F
Then we have the upper bound of the expected term, ≤(cid:88)(cid:88) E[w i2 j]+(cid:88)(cid:88) E[(w∗)2 ij]
i j i j
E(cid:13) (cid:13)S j,t−Θ− j,t1(cid:13) (cid:13)2
F
=E(cid:13) (cid:13)S j,t−Σ t+Σ t−Θ− j,t1(cid:13) (cid:13)2
F
∝N2(cid:112)
F t+B∗.
≤2E∥S j,t−Σ t∥2
F
+2(cid:13) (cid:13)Σ t−Θ−
t
1(cid:13) (cid:13)2
F Let h(U ) =
N√
F (h(U )2 = N2F ) and φ =
t t t 3 t
≤2N2F
t +4∥Σ ∥2 (N +λ2 (Σ−1)Q2) max(φ j), then we have
p t F max t t
(cid:16)
E[dV ]=φβˆ h(U )O +
We assume that there exist a constant b, such that t t t t
∥Θ −T∥ ≤ b∥Θ −Θ ∥ and ˆb = bb . As (cid:115)
b
→j,t
0
almF
ost surely,
tj h, et first∗ teF
rm of
(45t
)
becomt
es tr(Θ∗)
2h(U t)2 +4N2P2+4NP t2Q2
t
+1(cid:17)
+
t p t p2
t
=−− tt rr (( Θ(Θ j,j t, St− j,t)Θ +∗) t⊺ r( (S Θj ∗,t (S+ j,b tt −(Θ Θj,
−
jt ,t1− ))T +)− tr(Θ I)− j −,t1)) 1
q
(cid:88)q
κ
j(cid:88)N
c⊺ j,t,idM j,t,i+
βˆ t2κ 2j2N2
dt+2qφβˆ tˆb tV t
⊺ j=1 i=1
b tr((Θ −Θ ) (Θ −T))
t j,t ∗ j,t q N
≤tr(Θ j,t)tr(S j,t)+tr(Θ∗)(cid:13) (cid:13)S j,t−Θ− j,t1(cid:13) (cid:13)
F
+N+ ≤φβˆ t(cid:16) C 1(t)h(U t)+C 2(t)(cid:17) + 1
q
(cid:88) κ j(cid:88) c⊺ j,t,idM
j,t,i
b ∥Θ −Θ ∥ ∥Θ −T∥ j=1 i=1
≤trt (Θ j,j t, )t tr( (cid:115)S j,∗ t)F +Nj +,t 2ˆb t∥ΘF j,t−Θ ∗∥2 F + (cid:80)q j=1 2βˆ qt2κ j2N2 dt+2qφβˆ tˆb tV t
+tr(Θ∗)
2N2F
t +4∥Σ ∥2 (N +λ2 (Σ−1)Q2) (48)
p t F max t t
(cid:113)
where C (t) = O + tr(Θ ) 2 and C (t) =
1 t ∗ p 2
(cid:113)
tr(Θ ∗)P t N2+NQ p22 t +N. As t increase, qφβˆ tˆb t →0
t
and the term 2qφβˆˆb V vanishes. Note that the stochas-
t t t
tic integral is a martingale,
E(cid:104)(cid:90) t βˆ
t
(cid:88)q
κ
(cid:88)N
c⊺ dM
(cid:105)
=0
2q j j,t,i j,t,i
0 j=1 i=117
Thenwecanobtaintheexpectedboundformeasure(13): obtain
(cid:90) t φ(c′ +c )+κ2N2 (cid:90) t′
E[V ]=φ βˆ C (s)h(U )ds+ E[V ]= 1 2 j βˆdt
t s 1 s t 2 t
0 (49) 0
(cid:90) t κ2N2 (cid:90) t (cid:80)N τ2(A +G )
φ βˆ C (s)dt+ j βˆ2dt E[U ]≤ i=1 i i i .
s 2 2 t t c′
0 0
As t→∞, the exponential terms vanishes
E. Proof of Theorem 3 F. Proof of Lemma
Let φ ∼ ζ min{ 1 ,1}, and β ∼
ofIn thethi ts ws oec st yio sn te, mw se .w Fi il rl std ,is wcu ess coth ne sidli em rit ti hn eg p efr fo ep ce tsrti oes
f ζ 4min{min j(cid:2) N√1
ιjϕj3
(cid:3) ,1},
sc u′ 1+ chc2
that ζ 3 + ζ 4 ∼ 2 β˜ζ t1 ′,
changing the expression of the bounding sequences re- where β˜ =(cid:82)t′ βˆdt. Then
latedtoprecisionmatricesΘ ,thenwefurtherconsider t′ 0 t
j,t
using the bounding for the estimation matrix W to E[V ]∼ζ .
j,t t 1
deliver the long term two system expression.
When γ ∼ζ min{ c′ ,1}, then
When t increases, the estimations of the relation 2 (cid:80)N i=1µi(Ai+Gi)
precision matrix become more stable, and as the bound- (cid:80)N
µ (A +G )
ing sequences {P t}, {q t}, {Q t}, and {O t} are upper E[U t]≤γ i=1 i
c′
i i ∼ζ 2,
bounded, let P = maxP , p˜ = minp , Q = maxQ ,
t t t
andO =maxO .WecanchangethefunctionC (t)and that is, no matter whether c′ < 1 or not,
C
(t)in(49)tot
twoconstanttermsc
=O+tr(1
Θ
)(cid:113)
2 E[U ]∼ζ .
(cid:80)N i=1µi(Ai+Gi)
2 1 ∗ p t 2
(cid:113)
andc =2Ptr(Θ ) N2+NQ2+N respectively.Then
2 ∗ p˜2 REFERENCES
we can rewrite the bound of the two systems (43) and
(49) as [1] J.Verbraeken,M.Wolting,J.Katzy,J.Kloppenburg,T.Verbelen,
and J. S. Rellermeyer, “A survey on distributed machine learn-
E[V ]=φc (cid:90) t βˆh(U )dt+ φc 2+κ j2N2 (cid:90) t βˆdt ing,” Acm computing surveys (csur), vol. 53, no. 2, pp. 1–33,
t 1 t s 2 t 2020.
0 0 [2] T. Ben-Nun and T. Hoefler, “Demystifying parallel and dis-
(cid:80)N τ2(A +G ) tributeddeeplearning:Anin-depthconcurrencyanalysis,”ACM
E[U t]≤e−ctU 0+(1−e−ct) i=1 i
c
i i
ComputingSurveys(CSUR),vol.52,no.4,pp.1–43,2019.
[3] Z.Tang,S.Shi,X.Chu,W.Wang,andB.Li,“Communication-
(cid:90) t
efficient distributed deep learning: A comprehensive survey,”
+ ecth (V )U dt.
2 t t arXivpreprintarXiv:2003.06307,2020.
0 [4] L.Li,Y.Fan,M.Tse,andK.-Y.Lin,“Areviewofapplications
where h(U ) = N√ F , h (V ) = δ αϕh1(Vt)√ P, and infederatedlearning,”Computers&IndustrialEngineering,vol.
h (V
)=Nt max((cid:80)Nt ϱt2
).
t 2 ut
[5]
1 Y4 .9 Z, hp a. n1 g0 a6 n8 d54 Q, .2 Y0 a2 n0 g.
,“Asurveyonmulti-tasklearning,”IEEE
1 t q=1 qi Transactions on Knowledge and Data Engineering, vol. 34,
Inthelongrun,astheestimationmatrixW i,t becomes no.12,pp.5586–5609,2021.
more stable and βˆ decreases, the two systems also be- [6] L. Hong, A. Garcia, and C. Eksin, “Distributed networked
learningwithcorrelateddata,”Automatica,vol.137,p.110134,
come more stable. We set universal bound F =maxF ,
√ t 2022.
and note h(U t) ≤ N √F, h 1(V t) ≤ O. As U t becomes [7] A. Garcia, L. Wang, J. Huang, and L. Hong, “Distributed
small, the bound for U also bounds U , and thus networkedreal-timelearning,”IEEETransactionsonControlof
√ t t
h (V ) ≤ δ αϕO P, we change the constants c to
NetworkSystems,vol.8,no.1,pp.28–38,2020.
2 t 2 √ [8] L. Hong, A. Garcia, and C. Eksin, “Distributed networked
c′ √= (cid:16)2µκ + 2δ 1λ (cid:113)2 − (cid:17)δ 2αϕO P, and c 1 to c′ 1 = learning with correlated data,” in 2020 59th IEEE Conference
N F O + tr(Θ ∗) p2 . When δ 2ϕ < 2(µ ακ O+ √δ2 Pλ2), (
[9]
o Yn .ZD he ac ni gsi ao nn da Dnd .-YC .o Yn et uro nl g,(C “ADC re) g. ulaI rE izE aE ti, o2 n0 a2 p0 p, rp op ac. h59 to23 le– a5 r9 n2 in8 g.
δ 2 ≪ δ 1 and δ 2 ≪ µ for simplicity), we have c′ > 0. task relationships in multitask learning,” ACM Transactions on
Note that βˆ → 0, we assume that after t′, βˆ is KnowledgeDiscoveryfromData(TKDD),vol.8,no.3,pp.1–31,
t t′
2014.
negligible, The two systems can be written as
[10] M. Crawshaw, “Multi-task learning with deep neural networks:
φ(c′ +c )+κ2N2 (cid:90) ∞ Asurvey,”arXivpreprintarXiv:2009.09796,2020.
E[V ]= 1 2 j βˆdt [11] S.Liu,S.J.Pan,andQ.Ho,“Distributedmulti-taskrelationship
t 2 t learning,” in Proceedings of the 23rd ACM SIGKDD Interna-
0
(cid:80)N τ2(A +G ) tional Conference on Knowledge Discovery and Data Mining,
E[U ]≤e−c′tU +(1−e−c′t) i=1 i i i . 2017,pp.937–946.
t 0 c′ [12] V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar,
“Federatedmulti-tasklearning,”Advancesinneuralinformation
As t → ∞, the exponential terms vanish, we can processingsystems,vol.30,2017.18
[13] J.Wang,M.Kolar,andN.Srerbo,“Distributedmulti-tasklearn-
ing,”inArtificialintelligenceandstatistics. PMLR,2016,pp.
751–760.
[14] R. Nassif, C. Richard, A. Ferrari, and A. H. Sayed, “Multitask
diffusion adaptation over asynchronous networks,” IEEE Trans-
actions on Signal Processing, vol. 64, no. 11, pp. 2835–2850,
2016.
[15] J. Fan, Y. Liao, and H. Liu, “An overview of the estimation
of large covariance and precision matrices,” The Econometrics
Journal,vol.19,no.1,pp.C1–C32,2016.
[16] O. Ledoit and M. Wolf, “The power of (non-) linear shrinking:
Areviewandguidetocovariancematrixestimation,”Journalof
FinancialEconometrics,vol.20,no.1,pp.187–218,2022.
[17] W. N. Van Wieringen and C. F. Peeters, “Ridge estimation
of inverse covariance matrices from high-dimensional data,”
Computational Statistics & Data Analysis, vol. 103, pp. 284–
303,2016.
[18] Y.ZhangandD.-Y.Yeung,“Aregularizationapproachtolearning
task relationships in multitask learning,” ACM Transactions on
KnowledgeDiscoveryfromData(TKDD),vol.8,no.3,pp.1–31,
2014.
[19] M. D. Donsker, An invariance principle for certain probability
limittheorems,1951.
[20] P. Mortimore, P. Sammons, L. Stoll, D. Lewis, and R. Ecob,
“A study of effective junior schools,” International Journal of
EducationalResearch,vol.13,no.7,pp.753–768,1989.
[21] B. Oksendal, “Stochastic differential equations,” in Stochastic
differentialequations. Springer,2003,pp.65–84.
[22] A. Ferrari, “A note on sum and difference of correlated chi-
squaredvariables,”arXivpreprintarXiv:1906.09982,2019.
[23] Y. Feng, M. Wen, J. Zhang, F. Ji, and G.-x. Ning, “Sum
of arbitrarily correlated gamma random variables with unequal
parameters and its application in wireless communications,” in
2016 International Conference on Computing, Networking and
Communications(ICNC),2016,pp.1–5.
[24] L.-L. Chuang and Y.-S. Shih, “Approximated distributions of
the weighted sum of correlated chi-squared random variables,”
Journal of Statistical Planning and Inference, vol. 142, no. 2,
pp.457–472,2012.
[25] J. Zhang, M. Matthaiou, G. K. Karagiannidis, and L. Dai,
“On the multivariate gamma–gamma distribution with arbitrary
correlationandapplicationsinwirelesscommunications,”IEEE
TransactionsonVehicularTechnology,vol.65,no.5,pp.3834–
3840,2015.
[26] W.Whitt,“Anintroductiontostochastic-processlimitsandtheir
applicationtoqueues.internetsupplement,”2002.