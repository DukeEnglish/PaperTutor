Unlearnable 3D Point Clouds: Class-wise
Transformation Is All You Need
∗
XianlongWang1,2,4,5†,MinghuiLi‡,WeiLiu1,2,4,5†,HangtaoZhang4,5†,
ShengshanHu1,2,4,5†,YechaoZhang1,2,4,5†,ZiqiZhou1,2,3§,HaiJin1,2,3§
1NationalEngineeringResearchCenterforBigDataTechnologyandSystem
2ServicesComputingTechnologyandSystemLab 3ClusterandGridComputingLab
4HubeiEngineeringResearchCenteronBigDataSecurity
5HubeiKeyLaboratoryofDistributedSystemSecurity
†SchoolofCyberScienceandEngineering,HuazhongUniversityofScienceandTechnology
‡SchoolofSoftwareEngineering,HuazhongUniversityofScienceandTechnology
§SchoolofComputerScienceandTechnology,HuazhongUniversityofScienceandTechnology
{wxl99,minghuili,weiliu73,hangt_zhang,hushengshan,ycz,zhouziqi,hjin}@hust.edu.cn
Abstract
Traditionalunlearnablestrategieshavebeenproposedtopreventunauthorizedusers
fromtrainingonthe2Dimagedata. Withmore3Dpointclouddatacontaining
sensitivityinformation,unauthorizedusageofthisnewtypedatahasalsobecome
aseriousconcern. Toaddressthis,weproposethefirstintegralunlearnableframe-
workfor3Dpointcloudsincludingtwoprocesses: (i)weproposeanunlearnable
dataprotectionscheme,involvingaclass-wisesettingestablishedbyacategory-
adaptiveallocationstrategyandmulti-transformationsassignedtosamples;(ii)we
proposeadatarestorationschemethatutilizesclass-wiseinversematrixtransforma-
tion,thusenablingauthorized-onlytrainingforunlearnabledata. Thisrestoration
processisapracticalissueoverlookedinmostexistingunlearnableliterature,i.e.,
evenauthorizedusersstruggletogainknowledgefrom3Dunlearnabledata. Both
theoretical and empirical results (including 6 datasets, 16 models, and 2 tasks)
demonstratetheeffectivenessofourproposedunlearnableframework. Ourcodeis
availableathttps://github.com/CGCL-codes/UnlearnablePC
1 Introduction
Recently,3Dpointclouddeeplearninghasbeenmakingremarkablestridesinvariousdomains,e.g.,
self-driving[5]andvirtualreality[1,44]. Specifically,numerous3Dsensorsscanthesurrounding
environmentandsynthesizemassive3Dpointclouddatacontainingsensitiveinformationsuchas
pedestrianandvehicles[30]tothecloudserverfordeeplearninganalysis[10,21]. However,theraw
pointclouddatacanbeexploitedforpointcloudunauthorizeddeeplearningifadatabreachoccurs,
posingasignificantprivacythreat. Fortunately, theprivacyprotectionapproachesforpreventing
unauthorizedtraininghavebeenextensivelystudiedinthe2Dimagedomain[8,17,26,27,37,40].
Theyapplyelaborateperturbationsonimagessuchthattrainednetworksoverthemexhibitextremely
low generalization, thus failing to learn knowledge from the protected data, known as "making
dataunlearnable". Nonetheless,thestarkdisparitybetween2Dimagesand3Dpointcloudsposes
significantchallengesfordrawinglessonsfromexisting2Dsolutions.
Specifically, migrating 2D unlearnable schemes to 3D suffers from following challenges: (i) In-
compatibilitywith3Ddata. Numerousmodel-agnostic2Dimageunlearnableschemesoperate
∗MinghuiLiisthecorrespondingauthor.
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
tcO
4
]VC.sc[
1v44630.0142:viXraOriginal sample Rotation Scaling Shear Reflection Translation
Transformations None Rotation Scaling Shear Twisting Tapering Reflection Translation
∗ ∗ ∗ ∗
Illustrations of 3D
point cloud samples
Changing the Changing the Stretching or Twisting the Narrowing or Creating a Changing the
Descriptions Clean sample
angles size compressing shape shortening mirror object position
Is it reversible?
Quantity of possible
Infinite Infinite Infinite Infinite Infinite 3 Infinite
transformations
Dimension of the
multiplicative
transformation matrix
𝟑𝟑×𝟑𝟑 𝟑𝟑×𝟑𝟑 𝟑𝟑×𝟑𝟑 𝟑𝟑×𝟑𝟑 𝟑𝟑×𝟑𝟑 𝟑𝟑×𝟑𝟑 𝟒𝟒×𝟒𝟒
ℝ ℝ ℝ ℝ ℝ ℝ ℝ
Figure1:Anoverviewofexistingseventypesof3Dtransformations.“*"denotesrigidtransformationsthatdo
notaltertheshapeofthepointcloudsamples,whiletheremainingtransformationsarenon-rigidtransformations.
inthepixelspace,suchasconvolutionaloperations[37,40],makingthemfailtobedirectlytrans-
ferredtothe3Dpointspace. (ii)Poorvisualquality. Migratingmodel-dependent2Dunlearnable
methods[4,8,17,26]to3Dpointcloudsrequiresperturbingsubstantialpoints,leadingtoirregular
three-dimensionalshiftswhichmaysignificantlydegradevisualquality. Hence,thesechallengesspur
ustostartdirectlyfromthecharacteristicsofpointcloudsforproposing3Dunlearnablesolutions.
Recentworksobservethat3Dtransformationscanaltertest-timeresultsofmodels[6,15,42]. To
explorethis,weconductanin-depthinvestigationintothepropertiesofseven3Dtransformationsas
showninFig.1andrevealthemechanismsbywhichtransformationsemployedinacertainpattern
serveasunlearnableschemes(Sec.3.2). Inlightofthis,weproposethefirstunlearnableapproachin
3Dpointcloudsviamulticlass-wisetransformation(UMT),transformingsamplestovariousforms
for privacy protection. Concretely, we newly propose a category-adaptive allocation strategy by
leveraginguniformdistributionsamplingandcategoryconstraintstoestablishaclass-wisesetting,
therebymultiplyingmulti-transformationstosamplesbasedoncategories. Totheoreticallyanalyze
UMT,wedefineabinaryclassificationsetupsimilartothatusedin[18,31,37]. Meanwhile,we
employaGaussianMixtureModel(GMM)[36]tomodelthecleantrainingsetandusetheBayesian
optimaldecisionboundarytomodelthepointcloudclassifier. Theoretically,weprovethatthere
existsaUMTtrainingsetfollowsaGMMdistributionandtheclassificationaccuracyofUMTdataset
islowerthanthatofthecleandatasetinaBayesianclassifier.
Moreover, anincompatibleissueinexistingunlearnableworks[8,17,26,27,37,41]wasidenti-
fied[52],i.e.,theseapproachespreventunauthorizedlearningtoprotecteddata,buttheyalsoimpede
authorized users from effectively learning from unlearnable data. To address this, we propose a
datarestorationschemethatappliesclass-wiseinversetransformations,determinedbyalightweight
messagereceivedfromtheprotector. OurproposedunlearnableframeworkincludingUMTapproach
anddatarestorationschemeisdepictedinFig.3.
Extensiveexperimentson6benchmarkdatasets(includingsyntheticandreal-worlddatasets)using
16 point cloud models across CNN, MLP, Graph-based Network, and Transformer on two tasks
(classificationandsemanticsegmentation),verifiedtheeffectivenessofourproposedunlearnable
scheme. Wesummarizeourmaincontributionsasfollows:
• The First Integral 3D Unlearnable Framework. To the best of our knowledge, we
proposethefirstintegralunlearnable3Dpointcloudframework,utilizingclass-wisemulti-
transformation as its unlearnable mechanism (effectively safeguarding point cloud data
against unauthorized exploitation) and proposing a novel data restoration approach that
leverages class-wise reversible 3D transformation matrices (addressing an incompatible
issueinmostexistingunlearnableworks,whereevenauthorizeduserscannoteffectively
learnknowledgefromunlearnabledata).
• TheoreticalAnalysis. Wetheoreticallyindicatetheexistenceofanunlearnablesituation
thattheclassificationaccuracyoftheUMTdatasetislowerthanthatofthecleandataset
underthedecisionboundaryoftheBayesclassifierinGaussianMixtureModel.
• ExperimentalEvaluation. Extensiveexperimentson3syntheticdatasetsand3real-world
datasetsusing16widelyusedpointcloudmodelarchitecturesonclassificationandsemantic
segmentationtasksverifythesuperiorityofourproposedschemes.
22 Preliminaries
Notation. Consideringtherawpointclouddata(X,Y)∈X ×Y sampledfromacleandistribution
Dfortrainingapointcloudnetwork,theuser’sgoalistoobtainamodelF :X →Y byminimizing
thelossfunction(e.g.,cross-entropyloss)L(F(X),Y). LetT∈T bea3Dtransformationmatrix
thatdoesnotseriouslydamagethevisualqualityofpointclouds. NotethatX ∈R3×p,T ∈R3×3,
and p represents the number of points. In theoretical analysis, following [18, 31], we simplify a
trainingdatasetD toaGaussianMixtureModel(GMM)[36]N(yµ,I),wherey ∈{±1}denotes
k
theclasslabels,µ∈Rddenotesthemeanvalue,andI ∈Rd×ddenotestheidentitymatrix. Thusthe
BayesoptimaldecisionboundaryforclassifyingD isdefinedbyP(x)≡µTx=0. Theaccuracy
k
ofthedecisionboundaryP onD isequaltoϕ(||µ|| ),whereϕdenotestheCumulativeDistribution
k 2
Function(CDF)ofthestandardnormaldistribution.
DataprotectorG . G aimstoprotecttheknowledgefromthecleantrainingset(withsizeofn)
p p
D = {X ,Y }n ∼ Dbycompromisingtheunauthorizedmodelswhotrainontheunlearnable
c i i i=1
point cloud data {T (X ),Y }n , resulting in extremely poor generalization on the clean test
i i i i=1
distributionD ⊆D. Thisobjectivecanbeformalizedas:
t
(cid:88)
max E L(F(X;θ ),Y), s.t. θ =argmin L(F(T (X );θ),Y ). (1)
u u i i i
(X,Y)∼Dt θ
(Xi,Yi)∈Dc
whereG assumesthattrainingsamplesarealltransformedintounlearnableoneswhilemaintain-
p
ing normal visual effects, in line with previous unlearnable works [17, 26, 37, 40]. By the way,
solvingEq.(1)directlyisinfeasibleforneuralnetworksbecauseitnecessitatesunrollingtheentire
trainingprocedurewithintheinnerobjectiveandperformingbackpropagationthroughittoexecutea
singlestepofgradientdescentontheouterobjective[7].
AuthorizeduserG . G aimstoapplyanothertransformationT′ ∈T ontheunlearnablesample,
a a
makingtheprotecteddatalearnable. Thisisformallydefinedas:
(cid:88)
min E L(F(X;θ ),Y), s.t. θ =argmin L(F(T′(T (X ));θ),Y ). (2)
r r i i i i
(X,Y)∼Dt θ
(Xi,Yi)∈Dc
where G assumes that, without access to any clean training samples, T′ can be constructed by
a
utilizingalightweightmessageM receivedfromdataprotectors.
3 OurProposedUnlearnableSchemes
3.1 KeyIntuition
Several recent works [6, 11, 42] reveal employing 3D transformations can mislead the model’s
classificationresults. Suchaphenomenonimpliesthattheremightbesomedefectsinpointcloud
classifiers whenprocessing transformedsamples, leading usto infer that3D transformationsare
probablecandidatesfordataprotectionagainstunauthorizedtraining. Ifthetransformedpointcloud
dataareusedtotrainunauthorizedDNNs,onlysimplelinearfeaturesinherentin3Dtransformations
(atwhichtransformationsmayactasshortcuts[12])arecapturedbytheDNNs,successfullyprotecting
pointclouddataprivacy.
Ap trp al nyi sn fog r c mla as ts i- ow ni sse DNN Shortcut
“bed”
“chair”
Original Transfor- Establishing a new mapping “Gtroouinled-t”
samples mations truth label
(a) (b)
Figure2: (a)TrainingonthetransformedModelNet10dataset(employingsample-wise, dataset-wise, and
class-wisepatterns)usingPointNetclassifier.(b)Thehigh-leveloverviewoftheclass-wisesetting.
3
⋅⋅⋅
⋅⋅⋅
⋅⋅⋅
⋅⋅⋅3.2 ExploringtheMechanism
Wesummarizeexisting3DtransformationsinFig.1andformallydefinetheminAppendixA.Toseek
clarityontheapplicationandselectionoftransformations,weexplorethreeaspects: (i)execution
mechanism,(ii)exclusionmechanism,and(iii)workingmechanismasfollows.
(i)WhichexecutionmechanismsuccessfullysatisfyEq.(1)? Theextensivelyemployedexecution
patternsin2Dunlearnableapproachesaresample-wise[8,17]andclass-wise[17,37]settings. We
furthercomplementthedataset-wisesetting(usinguniversaltransformation)andimplementtheabove
executionmechanismsfortrainingaPointNetclassifier[33]onthetransformedModelNet10[48],
obtainingtestaccuracyresultsinFig.2(a). Wediscoverthatmodelachievesconsiderablylowtest
accuracy under the class-wise setting, satisfying Eq. (1). Sample-wise and dataset-wise settings
donotobviouslycompromisemodelperformance, whichcannotserveaspromisingunlearnable
routes.Moreover,wenotethatsample-wisetransformationisoftenconsideredasadataaugmentation
schemetoimprovegeneralization,whichcontradictsouraimofusingclass-wisetransformationto
lowermodelgeneralization.
(ii)Whichtransformationsneedtobeexcluded? Notalltransformationsaresuitablecandidates.
We exclude three transformations, tapering, reflection, and translation. (1) The tapering matrix
maycausepointcloudsamplestobecomeaplanarprojectionwhenηz definedinEq.(18)equals
to-1,renderingthetaperedsamplesmeaningless;(2)Thereflectionmatrixhasonlythreedistinct
transformationmatrices,renderingitincapableofassigningclass-wisetransformationswhenthe
numberofcategoriesexceedsthree;(3)Thetranslationtransformationisastraightforwardandsimple
additivetransformationthatistooeasilydefeatedbypointclouddatapre-processingapproaches.
(iii)Whydoesclass-wisetransformationwork? Weconductexperimentsusingclass-wisetrans-
formations(seeTab.6),indicatingthatthemodeltrainingontheclass-wisetransformedtraining
setachievesarelativelyhighaccuracyontheclass-wisetransformedtestset(usingthesametrans-
formation process as the training set). Besides, if we permute the class-wise transformation for
thetestset, weobtainasignificantlowaccuracyonthetestset. Therefore, weconcludethatthe
reasonwhyclass-wisetransformationworksisthatthemodellearnsthemappingbetweenclass-wise
transformationsandcorrespondingcategorylabelsasshowninFig.2(b),whichresultsinthemodel
beingunabletopredictthecorrespondinglabelsonacleantestsetlackingtransformations. This
analyticalprocessyieldsconclusionsthatareinagreementwithpriorresearch[37,42].
3.3 OurDesignforUMT
3.3.1 Category-AdaptiveAllocationStrategy
Weassigntransformationparametersbasedoncategoriestorealizeclass-wisesetting. Forrotation
transformationR∈R3×3,werefertoαandβ asslightanglesimposedonthexandyaxes,γ asthe
primaryangleforzaxis. WegeneraterandomanglesforA timesinthreedirections:
N
(cid:108)√ (cid:109)
α,β ∼U(0,r ),γ ∼U(0,r ),A = 3N (3)
s p N
whereU denotesuniformdistribution,N denotesthenumberofcategories,r isasmallrangethat
s
controlsαandβ,whiler isalargerangethatcontrolsγ. A iscomputedinsuchawaytoensure
p N
thatthenumberofcombinationsofthreeanglesisgreaterthanorequaltoN. Concretely,inthe
rotationoperation,eachofthethreedirectionshasA distinctangles,whichmeansthatthefinal
N
rotationmatrixhasA3 possiblecombinations. Tosatisfytheclass-wisesetup,A3 mustbeatleast
N (cid:108)√ (cid:109) N
N,requiringA tobenolessthan 3N . Finally,werandomlyselectN combinationsofangles
N
fortheallocation. ThescalingtransformationS ∈ R3×3 resizesthepositionofeachpointinthe
3D point cloud sample by a certain scaling factor λ, which is sampled N times from a uniform
distributionU:
λ∼U(b ,b ) (4)
l u
where b and b represent the lower bound and upper bound of the scaling factor, respectively.
l u
ForshearH ∈ R3×3 definedinEq.(13), twistingW ∈ R3×3 definedinEq.(16), theprocessof
generatingparameterswithinranges(ω ,ω )and(h ,h )isconsistenttoscaling. Therangeofthese
l u l u
parametersensuresthevisualeffectofthesample.
4Our unlearnable scheme: UMT Airplane
(i-thclass)
Category-Adaptive Allocation Strategy Data Restoration Scheme
Number of Uniform distribution U Parameter lists Secure Reverse transformation Training
categories N
𝑘𝑘,𝑟𝑟𝑠𝑠,𝑟𝑟𝑝𝑝,𝑏𝑏𝑙𝑙,𝑏𝑏𝑢𝑢…, ,𝑤𝑤𝑙𝑙,𝑤𝑤𝑢𝑢 ℒℛ,ℒ𝑆𝑆,…,ℒℋ
(Lt igra hn twsp eo igr ht t) Aut uh so er rized Training Unau ut sh eo rrized
Employing Class-wise
Data protector Transformations Z mulM tipa lt icri ax t ion Z Legally accessed Z Illegally accessed
Assigning class-wise 𝛾𝛾𝑖𝑖 𝜘𝜘𝑖𝑖
parameters O X � O � � 𝜆𝜆𝑖𝑖 X O X
…
…
Input raw 3D point
cloud data (w/o
𝛽𝛽𝑖𝑖 Y � 𝛼𝛼𝑖𝑖 Y� � � Y� �
any protection) Class-wise rotation Class-wisescaling Class-wiseshear Unlearnable
Raw point cloud data transformation transformation transformation point cloud data
𝓡𝓡 𝑺𝑺 𝑯𝑯
Figure3:Anoverviewofourproposedintegralunlearnablepipeline
Property1. SincerotationmatricesR ,R ,andR aroundthreedirectionsareallorthogonal
α β γ
matrices,Risalsoanorthogonalmatrix,whichcanbedefinedas:
∀M∈{R ,R ,R ,R}, s.t. MMT =I (5)
α β γ
wherewecandeterminetheorthogonalitybymatrixmultiplicationthroughthedefinitionsofEq.(11).
SinceR=R R R andRRT =R R R RTRTRT =I,soRisalsoanorthogonalmatrix.
α β γ α β γ γ β α
Property2. Allfourtransformationmatricesweemploy,R,S,W,andH,andthemultiplicative
combinationsofanythesematricesareallinvertiblematrices,whichcanbeformallydefinedas:
∀J ∈{f(R)f(S)f(W)f(H)|f(x)∈{x,I}}, ∃K s.t. JK=KJ =I (6)
wheretheinversematricesofR,S,W,andHaregiveninAppendixA.Thispropertyallowsthe
authorizeduserstonormallytrainontheprotecteddataduetothatmultiplyingamatrixbyitsinverse
resultsintheidentitymatrix,leadingustoproposeadatarestorationschemeinSec.3.4.
3.3.2 EmployingClass-wiseTransformations
AssumingthepointcloudtrainingsetD isdefinedas{(X ,Y ),(X ,Y ),...,(X ,Y )}N ,
c c1 i c2 i cni i i=1
wheren ,n ,...,n representthenumberofsamplesinthe1st,2nd,...,N-thcategory,respectively.
1 2 N
Weformallydefinethespectrumoftransformationsasktoindicatethenumberoftransformations
involved. ThustheultimateunlearnabletransformationmatrixT isdefinedas:
k
T
=(cid:81)k
V , ∀i̸=j, s.t. V ,V ∈{R,S,W,H},V ̸=V (7)
k i=1 i i j i j
Onceweemploytheproposedcategory-adaptiveallocationstrategytoT ,theunlearnablepoint
k
clouddatasetD isconstructedas:
u
D ={(T (X ),Y ),(T (X ),Y ),...,(T (X ),Y )}N (8)
u ki c1 i ki c2 i ki cni i i=1
OurproposedUMTschemeisdescribedinAlgorithm1. Weenumeratepossibletransformations
inEq.(7)toobtaintheunlearnabilityinTab.7andselectonetypeofclass-wisetransformationfor
eachkformorecomprehensiveexperimentsinTab.1. TofacilitatethetheoreticalstudyofUMT2,we
optforRS asthetransformationmatrixT,whichachievesthebestunlearnableeffectassuggested
inTabs.1and7. Thus, intheGMMscenario, theclass-wisetransformationmatrixisdefinedas
T =R S =λ R ∈Rd×d,whereλ ∈Risthescalingfactor.
y y y y y y
Lemma3. TheunlearnabledatasetD generatedusingUMTonD canalsoberepresentedusinga
u c
GMM,i.e.,D ∼N(yT µ,λ2I).
u y y
Proof: See Appendix D.1. Lemma 3 demonstrates that the unlearnable dataset D can also be
u
representedasaGMM,whichisderivedfromProperty1.
Lemma4. TheBayesoptimaldecisionboundaryforclassifyingD isgivenbyP (x)≡Ax⊤x+
u u
B⊤x+C =0,whereA=λ−2−λ−2,B =2(λ−2T +λ−2T )µ,andC =ln |λ2 −1I| .
−1 1 −1 −1 1 1 |λ2I|
1
Proof: SeeAppendixD.2. Lemma4revealsthattheBayesiandecisionboundaryforclassifyingD
u
isaquadraticsurfacebasedontheGMMexpressionofD .
u
2Inthesubsequenttheoreticaldescriptions,UMTreferstoUMTwithk=2usingtransformationmatrixRS.
5Lemma5. Letz ∼N(0,I),Z =z⊤z+b⊤z+c,whereb= B,c= C,and∥·∥ denote2-normof
A A 2
vectors. Foranyt≥0andγ ∈R,weemployChernoffboundtohave:
(cid:110) (cid:111)
exp t2 ||b||2−t(γ+d)
P{Z ≥E[Z]+γ}≤ 2(1−2t) 2
|(1−2t)I|21
Proof: SeeAppendixD.3. Lemma5enablesustoestablishanupperboundontheaccuracyofthe
unlearnabledecisionboundaryP appliedtothecleandatasetD ,denotedasτ (P ),aspresented
u c Dc u
inTheorem6below.
Theorem6. Foranyconstantt andt satisfying0≤t < 1 and0≤t < 1,theaccuracyofthe
1 2 1 2 2 2
unlearnabledecisionboundaryP onD canbeupper-boundedas:
u c
τ (P )≤
exp(cid:110) 2(1−t2 1 2t1)||b+2µ||2 2+t 1(µ⊤µ+b⊤µ+c)(cid:111)
Dc u 2|(1−2t 1)I|21
+
exp(cid:110) 2(1−t2 2 2t2)||b−2µ||2 2−t 2(µ⊤µ−b⊤µ+c+2d)(cid:111)
2|(1−2t 2)I|1 2
:=p +p
1 2
Furthermore,ifµ⊤µ+b⊤µ+c+d < 0and−µ⊤µ+b⊤µ−c−d < 0,wehaveτ (P ) < 1.
Dc u
Moreover,foranyµ̸=0,∃matrixT suchthatτ (P )<τ (P),whereP istheBayesoptimal
i Dc u Dc
decisionboundaryforclassifyingD .
c
Proof: SeeAppendixD.4. Theunlearnableeffecttakesplacewhenτ (P )<τ (P). Toachieve
Dc u Dc
this, we elaborately choose T , which is formalized as
µ⊤λ− −2 1T⊤ −1+λ− 12T⊤
1 µ ≪ 0. Therefore,
y λ−2−λ−2
−1 1
Theorem6theoreticallyexplainswhyUMTiseffectiveingeneratingunlearnablepointclouddata.
3.4 DataRestorationScheme
Toensurethatauthorizeduserscanachievebettergeneralizationaftertrainingonunlearnabledata,
i.e.,satisfyingEq.(2),weexploittheinversepropertiesof3Dtransformations,presentedinProperty
2,tocalculatetheinversematrixofT as:
k
T −1 =(cid:81)1 V −1, ∀i̸=j, s.t. V −1,V −1 ∈{R−1,S−1,W−1,H−1},V −1 ̸=V −1 (9)
k i=k i i j i j
In particular, we note that R−1 = RT, S−1 = 1I. Afterwards, the authorized user receives a
λ
lightweightmessageM containingclass-wiseparametersfromthedataprotectorthroughasecure
channel, therebyassigningM totheinversetransformationmatrixinEq.(9)formultiplyingthe
unlearnablesamples. OurproposedintegralunlearnableprocessisillustratedinFig.3.
4 Experiments
4.1 ExperimentalDetails
DatasetsandModels. Threesynthetic3Dpointclouddatasets,ModelNet40[48],ModelNet10[48],
ShapeNetPart[3]andthreereal-worlddatasetsincludingautonomousdrivingdatasetKITTI[30]
and indoor datasets ScanObjectNN [39], S3DIS [2] are used. We choose 16 widely used 3D
point cloud models PointNet [33], PointNet++ [34], DGCNN [43], PointCNN [23], PCT [14],
PointConv [46], CurveNet [49], SimpleView [13], 3DGCN [24], LGR-Net [57], RIConv [55],
RIConv++ [56], PointMLP [29], PointNN [54], PointTransformerV3 [47], and SegNN [60] for
evaluationofclassificationandsemanticsegmentationtasks.
Experimental setup. The training process involves Adam optimizer [20], CosineAnnealingLR
scheduler[28],initiallearningrateof0.001,weightdecayof0.0001. Weempiricallysetr ,r ,b ,b ,
s p l u
ω ,ω ,h ,andh to15◦,120◦,0.6,0.8,0◦,20◦,0,and0.4respectively. Themainresultsofdifferent
l u l u
konunlearnabilityisshowninTab.1.Specifically,k =1usesR,k =2usesRS,k =3usesRSW,
andk =4usesRSWH. Moreresultsfordifferentcombinationsofclass-wisetransformationsare
providedinTab.7. Thetablevaluescoveredby gray denotethebestunlearnableeffect.
6Datasets Schemes PointNet PointNet++ DGCNN PointCNN PCT PointConv CurveNet SimpleView RIConv++ 3DGCN PointNN PointMLP AVG
Clean 89.85±0.54 92.11±1.57 91.81±0.84 87.99±1.58 90.18±2.23 91.26±1.12 91.88±1.51 89.52±0.41 87.90±1.64 88.51±5.39 83.22±0.73 91.07±0.19 89.61±0.42
UMT(k=1) 40.64±10.69 28.73±2.00 27.41±6.71 32.82±2.70 31.58±4.27 33.64±6.63 39.78±6.80 45.31±11.85 86.72±1.46 28.01±8.72 34.47±0.29 32.85±2.35 38.50±2.01
ModelNet10 UMT(k=2) 21.18±0.93 26.36±1.71 18.84±6.14 21.97±4.04 19.72±4.52 20.84±5.96 25.04±2.15 22.75±2.43 16.53±3.63 32.79±3.23 31.94±2.11 25.41±3.96 23.61±1.06
UMT(k=3) 22.84±1.71 23.81±8.43 29.16±4.30 27.03±9.97 29.43±9.11 18.49±7.39 25.51±9.57 32.12±12.58 19.94±1.13 36.19±13.38 30.54±1.24 32.21±5.28 27.27±6.20
UMT(k=4) 18.83±2.40 20.56±14.61 15.92±1.51 20.52±6.06 20.29±2.14 21.66±2.24 23.46±12.58 24.12±6.20 26.41±2.47 34.28±17.20 25.59±0.25 26.65±11.36 23.19±5.98
Clean 86.18±0.07 90.55±0.73 89.51±0.86 81.89±5.35 87.11±1.39 88.90±0.89 87.82±0.23 85.25±0.31 84.59±1.07 86.81±1.69 74.81±0.16 87.31±0.91 85.89±0.40
UMT(k=1) 28.62±1.80 20.69±0.87 28.60±1.65 21.92±1.86 29.06±0.38 24.99±2.63 33.29±3.92 26.89±1.48 75.86±8.43 26.30±1.92 28.57±0.39 29.28±1.72 31.17±0.20
ModelNet40 UMT(k=2) 8.30±0.87 18.71±1.65 11.60±1.97 10.85±2.61 9.21±2.31 12.99±0.99 12.71±3.70 12.05±3.88 10.48±2.01 26.17±0.35 25.05±0.06 10.90±4.74 14.09±0.78
UMT(k=3) 7.48±1.66 17.62±3.10 10.41±5.48 9.43±3.09 9.91±4.19 11.28±4.93 11.23±3.07 11.21±4.09 10.92±0.48 25.31±1.29 24.53±0.10 8.94±3.43 13.19±2.41
UMT(k=4) 7.83±2.25 15.72±4.12 10.86±2.57 8.60±2.28 9.68±1.58 10.52±2.09 10.91±3.00 14.32±0.92 18.09±3.83 17.16±5.47 24.31±0.28 12.38±1.51 13.36±0.43
Clean 98.21±0.03 98.50±0.12 98.06±0.65 97.54±0.22 96.38±0.84 98.23±0.23 98.42±0.08 98.26±0.21 96.70±0.64 96.18±1.90 94.66±0.11 98.39±0.12 97.46±0.22
UMT(k=1) 63.85±12.71 50.30±21.71 64.71±13.44 51.95±14.13 54.39±3.36 29.34±0.14 71.38±9.34 56.22±6.48 96.93±0.09 37.90±9.67 58.44±0.61 47.32±8.26 56.89±3.00
ShapeNetPart UMT(k=2) 18.41±6.45 45.61±4.64 25.99±2.73 28.97±3.73 37.72±16.2625.60±8.59 26.49±8.58 38.38±5.96 5.05±3.63 32.21±13.01 49.82±1.56 34.19±4.43 30.70±1.43
UMT(k=3) 32.50±10.03 39.64±12.52 37.29±11.20 46.77±19.59 43.52±22.2431.28±4.67 37.27±10.30 41.51±2.66 6.01±6.11 47.84±7.05 50.85±0.07 47.80±21.37 38.52±6.17
UMT(k=4) 23.72±15.04 23.30±4.43 36.18±10.00 34.52±16.15 45.07±18.9929.48±11.09 29.79±4.79 40.04±9.97 32.22±1.38 33.06±28.16 51.91±0.03 40.93±21.81 35.02±11.23
Clean 98.04±2.23 99.49±0.50 99.33±0.34 99.23±0.25 98.93±0.54 98.04±1.68 99.10±1.05 99.38±0.33 99.64±0.09 99.67±0.24 99.49±0.50 95.72±5.46 98.84±0.87
UMT(k=1) 36.23±30.18 62.90±20.51 38.93±33.08 57.80±13.99 58.26±21.6639.99±6.82 33.33±10.75 56.71±30.96 98.53±1.19 68.34±10.46 70.34±0.98 47.20±20.79 55.71±5.54
KITTI UMT(k=2) 31.24±7.11 51.07±20.29 27.90±10.33 49.69±11.52 51.47±17.3625.95±11.38 20.55±4.93 51.77±8.53 99.80±0.09 70.42±27.97 47.31±10.5839.90±10.00 47.26±5.95
UMT(k=3) 19.13±6.93 53.73±12.44 31.26±21.68 56.08±17.90 54.45±11.7742.54±21.46 27.38±19.20 32.62±11.76 98.48±1.61 69.51±8.27 70.14±1.30 58.44±6.61 51.15±4.79
UMT(k=4) 26.84±16.26 61.79±6.65 29.89±15.72 54.27±11.99 64.21±10.7657.29±12.05 37.70±11.60 54.63±13.07 99.34±0.23 72.33±3.47 70.49±0.76 56.25±12.33 57.09±5.97
Clean 65.20±1.49 77.38±2.60 72.66±2.56 66.96±6.42 50.75±15.1675.42±2.06 70.92±1.06 51.93±2.89 66.34±1.21 73.84±2.75 58.17±0.30 73.32±1.39 66.91±0.72
UMT(k=1) 56.55±0.11 63.01±2.38 57.93±3.11 51.97±11.99 47.05±7.12 56.71±3.61 65.49±3.15 38.96±2.08 62.33±10.68 52.89±1.13 54.56±0.17 62.96±4.19 55.87±0.95
ScanObjectNNUMT(k=2) 14.55±1.81 49.41±3.44 20.96±3.99 13.61±4.97 15.10±2.05 20.78±4.74 21.35±2.59 20.73±9.64 10.76±1.76 52.37±5.62 48.25±0.20 16.32±4.78 25.35±0.49
UMT(k=3) 10.92±3.87 41.96±3.84 14.62±5.56 10.62±2.35 22.02±7.83 19.96±3.13 21.43±8.67 11.79±3.96 11.69±1.41 56.32±0.83 48.94±0.70 23.05±16.32 24.44±2.46
UMT(k=4) 5.77±2.10 34.65±8.31 12.16±4.84 9.87±0.95 17.86±11.6312.79±7.46 15.28±4.86 20.09±6.26 31.83±2.20 46.06±14.07 45.27±0.17 8.76±3.18 21.70±3.61
Table1:Mainresults:Theaveragetestaccuracy(%)resultswithstandarddeviationsfromthreeruns(random
seedsaresetto23,1023,and2023)ofclassificationmodelstrainedontheUMTdatasets.
Pre-processschemes PointNet PointNet++ DGCNN PointCNN CurveNet SimpleView RIConv++ 3DGCN PointNN PointMLP AVG
Cleanbaseline 86.10 91.13 89.02 75.73 87.76 85.49 85.82 84.89 75.00 87.66 84.86
SOR 9.93 19.17 9.36 10.45 11.57 14.81 6.66 25.08 25.04 15.22 14.73
SRS 9.24 22.20 10.25 7.78 12.66 14.04 11.32 24.72 26.01 12.18 15.04
Randomrotation 10.94 26.74 11.59 10.21 12.13 6.86 12.09 55.64 29.74 20.37 19.63
Randomscaling 22.33 22.45 30.15 20.22 25.61 23.25 73.62 27.48 26.50 24.39 29.60
Randomjitter 9.64 21.72 10.05 7.74 9.98 16.68 13.27 23.74 26.90 9.09 14.88
Randomrotation&scaling 63.49 34.44 44.65 37.56 72.89 51.62 78.04 64.00 36.26 78.94 56.19
Table2:Robustnessresults:Thetestaccuracy(%)resultsonUMT-ModelNet40againstpre-processschemes.
Evaluation Metrics. For classification, we report the test accuracy (in %) derived from the
classificationaccuracyoncleantestset,whichalignswiththemetricsusedinthe2Dunlearnable
schemes[17,26,37]. Forsemanticsegmentation,weuseevalaccuracyandmeanIntersectionover
Union(mIoU)asevaluationmetrics(in%),whereevalaccuracyistheratioofthenumberofpoints
classifiedcorrectlytothetotalnumberofpointsinthepointcloud,mIoUcalculatestheIoUforeach
classbetweenthegroundtruthandthepredictedsegmentation[59],andthentakestheaverageof
theseratios. Thelowerthesemetricsare,thebettertheeffectoftheunlearnablescheme.
4.2 EvaluationofProposedUnlearnableSchemes
Effectiveness. AsshowninTab.1,UMTresults
inasignificantdecreaseintestaccuracycompared 100 ModelNet10 100 ShapeNetPart
to clean baseline, indicating the unlearnable ef-
80 80
fectiveness of UMT. Moreover, all values of k
achievegoodunlearnableresults. Theaverageper- 60 C R Uel Me sa Tton dr d aa ta t ait o sa n es te dt ataset 60 C R Uel Me sa Tton dr d aa ta t ait o sa n es te dt ataset
40 40
formance of RS is the best, which is employed
20 20
asthedefaultintheremainingexperiments. We
PointNet PointNet++ DGCNN PointCNN PointNet PointNet++ DGCNN PointCNN
notethatrigidtransformationsareeasilydefeated Figure4:Thetestaccuracy(%)resultsobtainedafter
byinvariancenetworks[9,24,56]. Therefore,in trainingontheclean,UMT,andrestorationdatasets.
practicalsettings,wewillincludenon-rigidtrans-
formations,usingcombinedtransformationstoenhancetherobustnessoftheUMTscheme.
Robustness. (i)Dataaugmentations. Similarto[16,22],weemploydataaugmentationslikerandom
scaling,randomjitter,randomrotation,StatisticOutlierRemoval(SOR)[58],andSimpleRandom
Sampling(SRS)[51]againstUMT. Tab.2suggestsUMTisrobusttorandomdataaugmentations.
SORdetectsandremovesoutliersornoisypointsbutisineffectiveincounteringUMTbecauseUMT
doesnotintroduceirregularperturbationsoraddoutliers. SRSrandomlyselectsasmallsubsetfrom
theentiresetofpointswithequalprobability. UMTisrobusttoSRSasitaltersthecoordinatesof
allpoints. Evenifanarbitrarysubsetischosen,allpointswithinthesubsethavealreadyundergone
the unlearnable transformations. (ii) Adaptive attack. We assume the unauthorized user gains
knowledgeabouttheG ’suseofRS. Thusweproposerandomrotation&scalingasanadaptive
p
scheme. InTab.2,theadaptiveschemeexhibitsahigheraccuracythanotherschemes,confirmingits
effectiveness. Nonetheless,itremains28.67%lowerthancleanbaseline,revealingtherobustnessof
UMTagainstadaptiveattack. MoreresultsofadaptiveattacksareprovidedinAppendixC.3.
7
)%(
ycarucca
tseT
)%(
ycarucca
tseTEvaluationmetrics Evalaccuracy(%) mIoU(%)
Segmentationmodels PointNet++[34] PointTransformerv3[47] SegNN[60] AVG PointNet++[34] PointTransformerv3[47] SegNN[60] AVG
Cleanbaseline 74.76 74.72 79.00 76.16 40.06 40.57 50.27 43.63
UMT(k=2) 15.89 25.61 66.41 35.97 7.45 18.32 46.30 24.02
Table3:Semanticsegmentation:EvaluationofUMTonsemanticsegmentationtaskusingS3DISdataset.
Datasets Modules↓Models−→ PointNet PointNet++ DGCNN PointCNN CurveNet SimpleView RIConv LGR-Net 3DGCN PointNN PointMLP AVG
ModuleR 27.19 20.02 28.61 21.64 36.87 28.32 88.25 40.24 24.59 28.77 27.72 33.84
ModelNet40 ModuleS 9.36 48.70 15.07 7.41 13.92 20.33 32.25 12.56 88.72 65.32 18.79 30.22
UMT(k=2) 9.20 18.52 13.86 9.08 13.90 7.58 24.39 9.85 26.58 25.12 7.31 15.04
ModuleR 29.85 30.51 35.13 31.17 47.25 58.70 91.38 43.72 36.50 34.58 30.47 42.66
ModelNet10 ModuleS 34.80 57.05 42.62 33.81 30.36 43.75 42.24 32.60 89.40 73.79 41.29 47.43
UMT(k=2) 22.25 28.08 14.10 21.48 24.34 21.37 38.18 19.82 31.28 33.37 29.35 25.78
Table4:Ablationstudy: Thetestaccuracy(%)resultsderivedfromunlearnabledatawithdifferentmodules.
VisualEffect. WevisualizeUMTsamplesinFigs.7to10,indicatingthattheunlearnablepointcloud
samplesstillretaintheirnormalfeaturestructurewithvisualrationality.
EvaluationofSemanticSegmentation. WeevaluateUMTusingcommonmetricsforpointcloud
semanticsegmentationtasksinTab.3. Ascanbeseen,theperformanceofsemanticsegmentationof
dataprotectedbyUMTsignificantlydecreases. TheunderlyingreasonisthattheDNNslearnthe
featuresofclass-wisetransformationsandestablishanewmapping,whichleadstotheinabilityof
testsampleswithouttransformationstobecorrectlysegmentedbythesegmentationmodel.
EvaluationofDataRestoration. WemultiplyUMTsamplesbythetransformationmatrixinEq.(9).
Thedatabecomeslearnableaftertherestorationprocess,withtestaccuracyreachingalevelcompara-
bletothecleanbaselineasshowninFig.4. Thisstronglyvalidatestheeffectivenessoftheproposed
datarestorationscheme.
(b)
Figure5:Hyper-parametersensitivityanalysis:Theimpactofhyperparametersr ,r ,b,andb onthetest
s p l u
accuracyresults(%)ontheUMT(usingRS)ModelNet10dataset.
4.3 AblationStudyandHyper-ParameterSensitivityAnalysis
AblationonRotationModule. AsshowninTab.4,theaverageaccuracyincreasesby15.18%and
21.65%,respectively,whenonlyusingS. Thissuggeststheimportanceofclass-wiserotationmodule.
Thehightestaccuracydemonstratedby3DGCN[24]canbeattributedtoitsscalinginvariance,which
endowsitwithrobustnessagainstscalingtransformation.
AblationonScalingModule. AsalsoshowninTab.4,theaverageaccuracyincreasesby18.80%
and16.88%whenonlyusingtherotationmodule, respectively. Thehightestaccuracyachieved
onRIConv[55]andLGR-Net[57]isduetothefactthatbothnetworksarerotation-invariant,thus
providingresistanceagainstrotationtransformations. Theseablationresultsfurthermoreemphasize
theimportanceofincorporatingmorenon-rigidtransformations.
Hyper-Parameter Analysis. We analyze four hyperparameters r , r , b , and b in Fig. 5. The
s p l u
influenceofr andr ontheaccuracyremainsrelativelysmall, exhibitingtheirbestunlearnable
s p
effect when set to 15◦ and 120◦, respectively. We attribute this to the crucial role played by the
class-wisesetting,whileitisnothighlysensitivetothesizeofspecificvalues. Theunlearnableeffect
isthebestwhenb andb aresetto0.6and0.8,respectively. Similarly,thevariationsinb andb do
l u l u
notsignificantlyaltertheeffectduetotheclass-wisesetting.
4.4 InsightfulAnalysisIntoUMT
WeformalizeL(D)=E [L (F(X;θ),Y)],whereL isthecross-entropyloss,X,Yisthe
(X,Y)∼D c c
pointclouddatasampledfromdatasetD. WedefineD ,D ,andD asthetrainingset,unlearnable
tr u c
8With clean training data With clean training data
𝜽𝜽𝒕𝒕−𝟏𝟏 𝜽𝜽𝒕𝒕 𝜽𝜽𝒕𝒕+𝟏𝟏 𝜽𝜽𝒕𝒕−𝟏𝟏 𝜽𝜽𝒕𝒕 𝜽𝜽𝒕𝒕+𝟏𝟏
model w 𝜽𝜽e𝟎𝟎ight trW ainit ih
n
gU M daT
t a
Low
𝓛𝓛𝒕𝒕𝒕𝒕𝒕𝒕𝜽𝜽 𝒕𝒕𝒕𝒕𝒄𝒄∗ model
𝜽𝜽
w𝟎𝟎eight trW ainit ih
n
gU M daT
t a
∗L 𝜽𝜽o 𝒄𝒄∗w 𝓛𝓛𝒕𝒕𝒕𝒕𝒕𝒕𝒕𝒕𝒕𝒕
𝜽𝜽𝒖𝒖
∗
(a)
𝜽𝜽𝒖𝒖
(b) Low 𝓛𝓛𝒖𝒖
Figure6: UMTinweightspace. Thebluearrowrepresentsthecleantrainingtrajectoryoftheweightsθ at
i
stepi,whiletheredarrowsdenotetheUMTtrainingtrajectory.Thevaluesforplottingthisfigureareprovided
inAppendixC.4.(a)Testingoncleantestset(blueandredellipses);(b)TestingonUMTtestset(greenellipse).
testset(i.e.,testsettransformedbyUMT),andcleantestset,respectively. Thuswehavethetraining
lossL =L(D ),unlearnabletestlossL =L(D ),andcleantestlossL =L(D ).
train tr u u c c
The models trained on both clean and UMT training sets exhibit low L as shown in Fig. 6
train
(yellowellipses),indicatingthemodelsconvergewellduringtraining. Furthermore,whentestedon
D asshowninFig.6(a),thecleanmodel(trainedwithcleantrainingset)achievesalowL (blue
c c
ellipse),whiletheUMTmodel(trainedwithUMTtrainingset)exhibitsahighL (redellipse),also
c
supportingtheunlearnableeffectivenessofUMT. Fig.6(b)revealsthatthecleanmodelandUMT
modelbothexhibitlowL (greenellipse),suggestingthattheycanclassifyUMTsamples. Butthe
u
mechanismsunderlyingthetwocasesoflowL differ. Thecleanoneisduetothatthesemanticsof
u
samplescanberemainedbyUMTandthusnormallyclassified. TheUMToneisthattheUMTmodel
learnsthemappingbetweentransformationsandlabels, therebycorrectlypredictingthesamples
containingthesametransformations. Weconcludethatthecleanmodeleffectivelyclassifiesboth
clean and UMT samples, while the UMT model successfully classifies UMT samples (the UMT
processisthesameforbothtrainingandtestsamples)butfailstoclassifycleansamples.
5 RelatedWork
5.1 2DUnlearnableSchemes
Thedevelopmentof2Dunlearnableschemes[17,27,35,37,40,53]hasbeenbooming. Specifically,
model-dependentmethodsareinitiallyproposedinabundance[8,17,27]. Afterwards,numerous
model-agnosticmethodsthatsignificantlyimprovethegenerationefficiencyhavesurfaced[37,40,45].
However, duetothestructuraldisparitiesbetween3Dpointclouddataand2Dimages, applying
unlearnablemethodsdirectlyfrom2Dto3Drevealssignificantchallenges.
5.2 Protecting3DPointCloudDataPrivacy
Some works proposed an encryption scheme based on chaotic mapping [19] or optical chaotic
encryption[25],anda3Dobjectreconstructiontechniquewasintroduced[32],bothachievingprivacy
protectionforindividual3Dpointclouddata. Nevertheless,noprivacy-preservingsolutionhasbeen
proposedspecificallyforthescenarioofunauthorizedDNNlearningonabundantraw3Dpointcloud
data. Itisworthmentioningthatbothparallelworks[42,61]studiedavailabilitypoisoningattacks
against3Dpointcloudnetworks,whichlargelyreducemodelaccuracy,andbothhavethepotential
tobeappliedasunlearnableschemes. However,thefeaturecollisionerror-minimizationpoisoning
schemeproposedbyZhuetal.[61]overlookstheproblemofeffectivetrainingforauthorizedusers,
which limits its practical use in real-world applications. The rotation-based poisoning approach
proposedbyWangetal.[42]iseasilydefeatedbyrotation-invariantnetworks[55,56,57],asrevealed
inTabs.1and4.
6 Conclusion,Limitation,andBroaderImpacts
Inthisresearch,weproposethefirstintegralunlearnableframeworkin3Dpointclouds,whichutilizes
class-wisemultitransformations,preventingunauthorizeddeeplearningwhileallowingauthorized
training. Extensiveexperimentsonsyntheticandreal-worlddatasetsandtheoreticalevidenceverify
thesuperiorityofourframework. Thetransformationsincluderotation,scaling,twisting,andshear,
9whichareallcommon3Dtransformationoperations. Ifunauthorizedusersdesignanetworkthat
is invariant to all these transformations, they could potentially defeat our proposed UMT. So far,
onlynetworksinvarianttorigidtransformationslikerotationandscalinghavebeenproposed,while
networksinvarianttonon-rigidtransformationsliketwistingandshear,havenotyetbeenintroduced.
Therefore,ourresearchalsocontributestothedesignofmoretransformation-invariantnetworks.
Our research calls for the design of more robust point cloud networks, which helps improve the
robustnessandsecurityof3Dpointcloudprocessingsystems. Ontheotherhand,ifourproposed
UMTschemeismaliciouslyexploited, itmayhavenegativeimpactsonsociety, suchascausing
a sharp decline in the performance of models trained on it, affecting the security and reliability
oftechnologiesbasedon3Dpointcloudnetworks. Moretransformation-invariant3Dpointcloud
networksneedtobeproposedinthefuturetoavoidpotentialnegativeimpacts.
References
[1] EvangelosAlexiou,NanyangYang,andTouradjEbrahimi. Pointxr: Atoolboxforvisualiza-
tion and subjective evaluation of point clouds in virtual reality. In Proceedings of the 12th
InternationalConferenceonQualityofMultimediaExperience(QoMEX’20),pages1–6,2020.
1
[2] IroArmeni, OzanSener, AmirRZamir, HelenJiang, IoannisBrilakis, MartinFischer, and
SilvioSavarese. 3dsemanticparsingoflarge-scaleindoorspaces. InProceedingsofthe2016
IEEEConferenceonComputerVisionandPatternRecognition(CVPR’16),pages1534–1543,
2016. 6,18
[3] AngelX.Chang,ThomasFunkhouser,LeonidasGuibas,PatHanrahan,QixingHuang,Zimo
Li,SilvioSavarese,ManolisSavva,ShuranSong,HaoSu,etal. Shapenet: Aninformation-rich
3dmodelrepository. arXivpreprintarXiv:1512.03012,2015. 6,18,20
[4] SizheChen,GengYuan,XinwenCheng,YifanGong,MinghaiQin,YanzhiWang,andXiaolin
Huang.Self-ensembleprotection:Trainingcheckpointsaregooddataprotectors.InProceedings
ofthe11thInternationalConferenceonLearningRepresentations(ICLR’23),2023. 2
[5] XiaozhiChen,HuiminMa,JiWan,BoLi,andTianXia.Multi-view3dobjectdetectionnetwork
forautonomousdriving. InProceedingsofthe2017IEEEConferenceonComputerVisionand
PatternRecognition(CVPR’17),pages1907–1915,2017. 1
[6] Wenda Chu, Linyi Li, and Bo Li. Tpc: Transformation-specific smoothing for point cloud
models. InProceedingsofthe39thInternationalConferenceonMachineLearning(ICML’22),
pages4035–4056,2022. 2,3,15,16
[7] Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja, and Tom
Goldstein. Adversarial examples make strong poisons. In Proceedings of the 35th Neural
InformationProcessingSystems(NeurIPS’21),pages30339–30351,2021. 3
[8] Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, and Dacheng Tao. Robust unlearnable
examples: Protectingdataagainstadversariallearning. InProceedingsofthe10thInternational
ConferenceonLearningRepresentations(ICLR’22),2022. 1,2,4,9
[9] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE (3)-transformers: 3D
roto-translationequivariantattentionnetworks. InProceedingsofthe34thNeuralInformation
ProcessingSystems(NeurIPS’20),volume33,pages1970–1981,2020. 7
[10] CongGao,GengWang,WeisongShi,ZhongminWang,andYanpingChen.Autonomousdriving
security: Stateoftheartandchallenges. IEEEInternetofThingsJournal,9(10):7572–7595,
2021. 1
[11] KuofengGao,JiawangBai,BaoyuanWu,MengxiYa,andShu-TaoXia. Imperceptibleand
robustbackdoorattackin3dpointcloud. IEEETransactionsonInformationForensicsand
Security(TIFS’23),19:1267–1282,2023. 3
[12] RobertGeirhos,Jörn-HenrikJacobsen,ClaudioMichaelis,RichardZemel,WielandBrendel,
MatthiasBethge,andFelixAWichmann. Shortcutlearningindeepneuralnetworks. Nature
MachineIntelligence,pages665–673,2020. 3
[13] AnkitGoyal,HeiLaw,BoweiLiu,AlejandroNewell,andJiaDeng.Revisitingpointcloudshape
classificationwithasimpleandeffectivebaseline. InProceedingsofthe38thInternational
ConferenceonMachineLearning(ICML’21),pages3809–3820.PMLR,2021. 6
[14] Meng-HaoGuo,Jun-XiongCai,Zheng-NingLiu,Tai-JiangMu,RalphRMartin,andShi-Min
Hu. Pct: Pointcloudtransformer. ComputationalVisualMedia,7:187–199,2021. 6,17
[15] ShengshanHu,WeiLiu,MinghuiLi,YechaoZhang,XiaogengLiu,XianlongWang,LeoYu
Zhang,andJunhuiHou.Pointcrt:Detectingbackdoorin3dpointcloudviacorruptionrobustness.
10InProceedingsofthe31stACMInternationalConferenceonMultimedia(MM’23),pages666–
675,2023. 2,17
[16] ShengshanHu,JunweiZhang,WeiLiu,JunhuiHou,MinghuiLi,LeoYuZhang,HaiJin,and
LichaoSun. Pointca: Evaluatingtherobustnessof3dpointcloudcompletionmodelsagainst
adversarialexamples. InProceedingsofthe37thAAAIConferenceonArtificialIntelligence
(AAAI’23),pages872–880,2023. 7
[17] HanxunHuang,XingjunMa,SarahMonazamErfani,JamesBailey,andYisenWang. Unlearn-
ableexamples: Makingpersonaldataunexploitable. InProceedingsofthe9thInternational
ConferenceonLearningRepresentations(ICLR’21),2021. 1,2,3,4,7,9
[18] AdelJavanmardandMahdiSoltanolkotabi. Precisestatisticalanalysisofclassificationaccura-
ciesforadversarialtraining. TheAnnalsofStatistics,50(4):2127–2156,2022. 2,3
[19] XinJin,ZhaoxingWu,ChenggenSong,ChunweiZhang,andXiaodongLi. 3dpointcloud
encryptionthroughchaoticmapping. InProceedingsofthe17thPacificRimConferenceon
Multimedia(PCM’16),pages119–129,2016. 9
[20] DiederikPKingmaandJimmyBa. Adam:Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014. 6,17
[21] PeiliangLi,SiqiLiu,andShaojieShen. Multi-sensor3dobjectboxrefinementforautonomous
driving. arXivpreprintarXiv:1909.04942,2019. 1
[22] XinkeLi,ZhiruiChen,YueZhao,ZekunTong,YabangZhao,AndrewLim,andJoeyTianyi
Zhou. Pointba: Towards backdoor attacks in 3d point cloud. In Proceedings of the 18th
IEEE/CVFInternationalConferenceonComputerVision(ICCV’21),pages16492–16501,2021.
7
[23] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn:
Convolutiononx-transformedpoints.InProceedingsofthe32ndNeuralInformationProcessing
Systems(NeurIPS’18),pages828–838,2018. 6
[24] Zhi-HaoLin,Sheng-YuHuang,andYu-ChiangFrankWang.Convolutioninthecloud:Learning
deformablekernelsin3dgraphconvolutionnetworksforpointcloudanalysis. InProceedings
ofthe2020IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR’20),
pages1800–1809,2020. 6,7,8
[25] BochengLiu,YongxiangLiu,YiyuanXie,XiaoJiang,YichenYe,TingtingSong,Junxiong
Chai, MengLiu, ManyingFeng, andHaodongYuan. Privacyprotectionfor3dpointcloud
classificationbasedonanopticalchaoticencryptionscheme. OpticsExpress,31(5):8820–8843,
2023. 9
[26] ShuangLiu,YihanWang,andXiao-ShanGao. Game-theoreticunlearnableexamplegenerator.
InProceedingsoftheAAAIConferenceonArtificialIntelligence(AAAI’24),2024. 1,2,3,7
[27] YixinLiu,KaidiXu,XunChen,andLichaoSun. Stableunlearnableexample: Enhancingthe
robustnessofunlearnableexamplesviastableerror-minimizingnoise. InProceedingsofthe
AAAIConferenceonArtificialIntelligence(AAAI’24),pages3783–3791,2024. 1,2,9
[28] IlyaLoshchilovandFrankHutter. Sgdr: Stochasticgradientdescentwithwarmrestarts. arXiv
preprintarXiv:1608.03983,2016. 6,17
[29] XuMa,CanQin,HaoxuanYou,HaoxiRan,andYunFu. Rethinkingnetworkdesignandlocal
geometryinpointcloud: Asimpleresidualmlpframework. arXivpreprintarXiv:2202.07123,
2022. 6
[30] MoritzMenzeandAndreasGeiger. Objectsceneflowforautonomousvehicles. InProceedings
ofthe2015IEEEConferenceonComputerVisionandPatternRecognition(CVPR’15),pages
3061–3070,2015. 1,6
[31] YifeiMin,LinChen,andAminKarbasi. Thecuriouscaseofadversariallyrobustmodels: More
datacanhelp,doubledescend,orhurtgeneralization. InUncertaintyinArtificialIntelligence,
pages129–139.PMLR,2021. 2,3
[32] ArpitNama,AmayaDharmasiri,KanchanaThilakarathna,AlbertZomaya,andJaybieAgullo
de Guzman. User configurable 3d object regeneration for spatial privacy. arXiv preprint
arXiv:2108.08273,2021. 9
[33] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep
learningonpointsetsfor3dclassificationandsegmentation. InProceedingsofthe2017IEEE
ConferenceonComputerVisionandPatternRecognition(CVPR’17),pages652–660,2017. 4,
6
[34] CharlesRuizhongtaiQi,LiYi,HaoSu,andLeonidasJ.Guibas. Pointnet++: Deephierarchical
featurelearningonpointsetsinametricspace. InProceedingsofthe31stNeuralInformation
ProcessingSystems(NeurIPS’17),pages5099–5108,2017. 6,8
[35] Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun, and Jiliang Tang. Transferable
unlearnable examples. In Proceedings of the 11th International Conference on Learning
11Representations(ICLR’23),2023. 9
[36] DouglasAReynoldsetal.Gaussianmixturemodels.Encyclopediaofbiometrics,741(659-663),
2009. 2,3
[37] VinuSankarSadasivan,MahdiSoltanolkotabi,andSoheilFeizi. CUDA:Convolution-based
unlearnabledatasets. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR’23),pages3862–3871,2023. 1,2,3,4,7,9
[38] StubbornAtom. Distributionsofquadraticformofanormalrandomvariable. CrossValidated,
2020. (version: 2020-07-23). 23
[39] Mikaela AngelinaUy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit
Yeung. Revisiting point cloud classification: A new benchmark dataset and classification
modelonreal-worlddata. InProceedingsofthe17thIEEE/CVFInternationalConferenceon
ComputerVision(ICCV’19),2019. 6,18,20
[40] XianlongWang,ShengshanHu,MinghuiLi,ZhifeiYu,ZiqiZhou,andLeoYuZhang. Cor-
ruptingconvolution-basedunlearnabledatasetswithpixel-basedimagetransformations. arXiv
preprintarXiv:2311.18403,2023. 1,2,3,9
[41] XianlongWang,ShengshanHu,YechaoZhang,ZiqiZhou,LeoYuZhang,PengXu,WeiWan,
and Hai Jin. ECLIPSE: Expunging clean-label indiscriminate poisons via sparse diffusion
purification.InProceedingsofthe29thEuropeanSymposiumonResearchinComputerSecurity
(ESORICS’24),pages146–166,2024. 2
[42] XianlongWang,MinghuiLi,PengXu,WeiLiu,LeoYuZhang,ShengshanHu,andYanjun
Zhang. PointAPA:Towardsavailabilitypoisoningattacksin3Dpointclouds. InProceedings
of the 29th European Symposium on Research in Computer Security (ESORICS’24), pages
125–145,2024. 2,3,4,9
[43] YueWang,YongbinSun,ZiweiLiu,SanjayE.Sarma,MichaelM.Bronstein,andJustinM.
Solomon. Dynamicgraphcnnforlearningonpointclouds. ACMTransactionsOnGraphics
(TOG’19),pages1–12,2019. 6
[44] FlorianWirth,JannikQuehl,JeffreyOta,andChristophStiller. Pointatme: efficient3dpoint
cloudlabelinginvirtualreality.InProceedingsofthe2019IEEEIntelligentVehiclesSymposium
(IV’19),pages1693–1698,2019. 1
[45] ShutongWu,SizheChen,CihangXie,andXiaolinHuang. One-pixelshortcut: onthelearning
preferenceofdeepneuralnetworks. InProceedingsofthe11thInternationalConferenceon
LearningRepresentations(ICLR’23),2023. 9
[46] WenxuanWu,ZhongangQi,andLiFuxin. Pointconv:Deepconvolutionalnetworkson3dpoint
clouds. InProceedingsofthe2019IEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR’19),pages9621–9630,2019. 6
[47] XiaoyangWu,LiJiang,Peng-ShuaiWang,ZhijianLiu,XihuiLiu,YuQiao,WanliOuyang,
TongHe,andHengshuangZhao. Pointtransformerv3: Simpler,faster,stronger. arXivpreprint
arXiv:2312.10035,2023. 6,8
[48] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and
JianxiongXiao. 3dshapenets: Adeeprepresentationforvolumetricshapes. InProceedings
ofthe2015IEEEConferenceonComputerVisionandPatternRecognition(CVPR’15),pages
1912–1920,2015. 4,6,18,20,21
[49] TiangeXiang,ChaoyiZhang,YangSong,JianhuiYu,andWeidongCai. Walkinthecloud:
Learningcurvesforpointcloudsshapeanalysis. InProceedingsofthe18thIEEE/CVFInterna-
tionalConferenceonComputerVision(ICCV’21),pages915–924,2021. 6
[50] ZhenXiang,DavidJ.Miller,SihengChen,XiLi,andGeorgeKesidis.Abackdoorattackagainst
3dpointcloudclassifiers. InProceedingsofthe18thIEEE/CVFInternationalConferenceon
ComputerVision(ICCV’21),pages7597–7607,2021. 17
[51] JianchengYang,QiangZhang,RongyaoFang,BingbingNi,JinxianLiu,andQiTian. Adver-
sarialattackanddefenseonpointsets. arXivpreprintarXiv:1902.10899,2019. 7
[52] JingwenYeandXinchaoWang. Ungeneralizableexamples. InProceedingsofthe2024IEEE
ConferenceonComputerVisionandPatternRecognition(CVPR’24),2024. 2
[53] JiamingZhang,XingjunMa,QiYi,JitaoSang,YugangJiang,YaoweiWang,andChangsheng
Xu. Unlearnableclusters: Towardslabel-agnosticunlearnableexamples. InProceedingsofthe
2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR’23),2023. 9
[54] RenruiZhang,LiuhuiWang,YaliWang,PengGao,HongshengLi,andJianboShi. Parameter
isnotallyouneed: Startingfromnon-parametricnetworksfor3dpointcloudanalysis. arXiv
preprintarXiv:2303.08134,2023. 6
[55] Zhiyuan Zhang, Binh-Son Hua, David W Rosen, and Sai-Kit Yeung. Rotation invariant
convolutions for 3d point clouds deep learning. In Proceedings of the 2019 International
Conferenceon3DVision(3DV’19),pages204–213.IEEE,2019. 6,8,9
12[56] Zhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Riconv++: Effective rotation invariant
convolutions for 3d point clouds deep learning. International Journal of Computer Vision
(IJCV’22),130(5):1228–1243,2022. 6,7,9
[57] ChenZhao, JiaqiYang, XinXiong, AngfanZhu, ZhiguoCao, andXinLi. Rotationinvari-
antpointcloudclassification: Wherelocalgeometrymeetsglobaltopology. arXivpreprint
arXiv:1911.00195,2019. 6,8,9
[58] HangZhou,KejiangChen,WeimingZhang,HanFang,WenboZhou,andNenghaiYu. Dup-net:
Denoiserandupsamplernetworkfor3Dadversarialpointcloudsdefense. InProceedingsofthe
17thIEEE/CVFInternationalConferenceonComputerVision(ICCV’19),pages1961–1970,
2019. 7
[59] ZiqiZhou,YufeiSong,MinghuiLi,ShengshanHu,XianlongWang,LeoYuZhang,Dezhong
Yao,andHaiJin.Darksam:Foolingsegmentanythingmodeltosegmentnothing.arXivpreprint
arXiv:2409.17874,2024. 7
[60] XiangyangZhu,RenruiZhang,BoweiHe,ZiyuGuo,JiamingLiu,HanXiao,ChaoyouFu,Hao
Dong,andPengGao. Notimetotrain: Empoweringnon-parametricnetworksforfew-shot3d
scenesegmentation. InProceedingsofthe2024IEEEConferenceonComputerVisionand
PatternRecognition(CVPR’24),2024. 6,8
[61] YifanZhu,YiboMiao,YinpengDong,andXiao-ShanGao. Towardavailabilityattacksin3D
pointclouds. arXivpreprintarXiv:2407.11011,2024. 9
13Appendix: Unlearnable3DPointClouds: Class-wiseTransformationIsAllYou
Need
A Definitionsof3DTransformations
Existing3DtransformationsaresummarizedinFig.1andformallydefinedinthissection. The3D
transformationsaremathematicaloperationsappliedtothree-dimensionalobjectstochangetheir
position,orientation,andscaleinspace,whichareoftenrepresentedusingtransformationmatrices.
WeformallydefinethetransformedpointcloudsamplewithtransformationmatrixT∈R3×3as:
X =T·X (10)
t
whereX∈R3×pisthecleanpointcloudsample,X ∈R3×pisthetransformedpointcloudsample.
t
A.1 RotationTransformation
Therotationtransformationthatalterstheorientationandangleof3Dpointcloudsiscontrolledby
threeanglesα,β,andγ. Therotationmatricesinthreedirectionscanbeformallydefinedas:
(cid:34) 1 0 0 (cid:35) (cid:34) cosβ 0 sinβ (cid:35) (cid:34) cosγ −sinγ 0 (cid:35)
R = 0 cosα −sinα ,R = 0 1 0 ,R = sinγ cosγ 0
α β γ
0 sinα cosα −sinβ 0 cosβ 0 0 1
(11)
ThuswehaveT=R R R whileemployingrotationtransformation. Besides,wehaveR−1 =
α β γ α
RT,R−1 =RT,andR−1 =RT.
α β β γ γ
A.2 ScalingTransformation
ThescalingmatrixS canberepresentedas:
(cid:34) λ 0 0 (cid:35) (cid:34) 1 0 0 (cid:35)
S = 0 λ 0 =λ 0 1 0 (12)
0 0 λ 0 0 1
wherethescalingfactorλisusedtoperformaproportionalscalingofthecoordinatesofeachpoint
inthepointcloud.
A.3 ShearTransformation
Inthethree-dimensionalspace,shearing3isrepresentedbydifferentmatrices,andthespecificform
dependsonthetypeofshearbeingperformed. Specifically,thesheartransformationmatrixH
xy
(employed in UMT) of shifting x and y by the other coordinate z, and its corresponding inverse
matrixcanbeexpressedas:
(cid:34)1 0 s(cid:35) (cid:34)1 0 −s(cid:35)
H = 0 1 t ,H−1 = 0 1 −t (13)
xy xy
0 0 1 0 0 1
ThesheartransformationmatrixH ofshiftingxandz bytheothercoordinatey,anditscorre-
xz
spondinginversematrixcanbeexpressedas:
(cid:34)1 s 0(cid:35) (cid:34)1 −s 0(cid:35)
H = 0 1 0 ,H−1 = 0 1 0 (14)
xz xz
0 t 1 0 −t 1
And the shear transformation matrix H of shifting y and z by the other coordinate x, and its
yz
correspondinginversematrixcanbeexpressedas:
(cid:34)1 0 0(cid:35) (cid:34) 1 0 0(cid:35)
H = s 1 0 ,H−1 = −s 1 0 (15)
yz yz
t 0 1 −t 0 1
3https://www.mauriciopoppe.com/notes/computer-graphics/
transformation-matrices/shearing/
14Algorithm1OurproposedUMTscheme
Input: Clean3DpointclouddatasetD ={(X ,Y )}n ;numberofcategoriesN;slightranger ;
c i i i=1 s
primaryranger ;scalinglowerboundb ;scalingupperboundb ;twistinglowerboundw ;twisting
p l u l
upperboundw ;shearlowerboundh ;shearupperboundh ;spectrumoftransformationsk;matrix
u l u
setT ={R,S,H,W}.
s
Output: Unlearnable3DpointclouddatasetD ={(X ,Y )}n .
u ui i i=1
InitializetheslightanglelistsL =[],L =[],theprimaryanglelistL =[],therotationlistL =[],
α β γ (cid:108)√ (cid:109) R
thescalinglistL =[],thetwistinglistL =[],theshearlistL =[],andA = 3N ;
S W H N
forc=1tokdo
RandomlysampleatransformationmatrixV ∈T ;
c s
RemovetransformationmatrixV fromT ;
c s
ifV ==Rthen
c
fori=1toA do
N
forj =1toA do
N
fork =1toA do
N
L ←L ∪{[L [i],L [j],L [k]]};
R R α β γ
end
end
end
GettheL ←random.sample(L ,N);
R R
end
elseifV ==S then
c
fori=1toN do
Randomlysampleλ ∼U(b ,b );
i l u
AddtothelistL ←L ∪{λ };
S S i
end
end
elseifV ==W then
c
fori=1toN do
Randomlysampleω ∼U(w ,w );
i l u
AddtothelistL ←L ∪{ω };
W W i
end
end
elseifV ==Hthen
c
fori=1toN do
Randomlysampleh ∼U(h ,h );
i l u
AddtothelistL ←L ∪{h };
H H i
end
end
end
fori=1tondo
GetthetransformationmatrixT
=(cid:81)k
V bytheparameterlistsabove;
k i=1 i
GetthetransformeddataX =T ·X ;
ui ki i
end
Return: Unlearnable3DpointclouddatasetD .
u
A.4 TwistingTransformation
The3Dtwistingtransformation[6]involvesarotationaldeformationappliedtoanobjectinthree-
dimensionalspace,creatingatwistedorspiraledeffect. Unlikesimplerotationsaroundfixedaxes,a
twistingtransformationintroducesavariablerotationthatmaychangebasedonthespatialcoordinates
oftheobject. Forinstance,consideringatwistingtransformationalongthez-axis,wheretherotation
angleisafunctionrelatedtothez-coordinate,itcanbeexpressedas:
(cid:34)cos(θz) −sin(θz) 0(cid:35)
W (θ,z)= sin(θz) cos(θz) 0 (16)
z
0 0 1
15whereθistheparameterofthetwistingtransformation,andzisthez-coordinateoftheobject. The
inversematrixofW (θ,z)is:
z
(cid:34) cos(θz) sin(θz) 0(cid:35)
W−1(θ,z)= −sin(θz) cos(θz) 0 (17)
z
0 0 1
A.5 TaperingTransformation
Thetaperingtransformation[6]isalineartransformationusedtoaltertheshapeofanobject,causing
ittograduallybecomepointedorshortened. Inthree-dimensionalspace,taperingtransformationcan
adjustthedimensionsofanobjectalongoneormoreaxes,creatingataperingeffect. Thespecific
matrixrepresentationoftaperingtransformationdependsonthechosenaxisandthedesignofthe
transformation. Generally,taperingtransformationcanberepresentedbyamatrixthatismultiplied
bythecoordinatesoftheobjecttoachievetheshapeadjustment,whichisdefinedas:
(cid:34)1+ηz 0 0(cid:35)
A (η,z)= 0 1+ηz 0 (18)
z
0 0 1
wherezisthez-coordinateoftheobject. Consideringthatηzcouldindeedequal-1,insuchacase,
the3Dpointcloudsampleswouldbeprojectedontothez-plane,losingtheirpracticalsignificance
andthetaperingmatrixisalsoirreversible.
A.6 ReflectionTransformation
Reflectiontransformationisalineartransformationthatinvertsanobjectalongacertainplane. This
planeiscommonlyreferredtoasareflectionplaneormirror. Forreflectiontransformationsinthree-
dimensionalspace,wecanrepresentthemthroughamatrix. Regardingthereflectiontransformation
matricesofthexyplane,yzplane,andxzplane,wehave:
(cid:34)1 0 0 (cid:35) (cid:34)−1 0 0(cid:35) (cid:34)1 0 0(cid:35)
R = 0 1 0 ,R = 0 1 0 ,R = 0 −1 0 (19)
xy yz xz
0 0 −1 0 0 1 0 0 1
A.7 TranslationTransformation
The3Dtranslationtransformation4referstotheprocessofmovinganobjectinthree-dimensional
space. This transformation involves moving the object along the x, y, and z axes, respectively,
smoothlytransitioningitfromonepositiontoanother,whichisdefinedas:
1 0 0 t 
x
0 1 0 t
L= y (20)
0 0 1 t 
z
0 0 0 1
wheret , t , andt representsthetranslationalongthex, y, andz axes, respectively. This4×4
x y z
matrixisahomogeneouscoordinatematrixthatdescribesthetranslationinthree-dimensionalspace.
Additionally, thetranslationmatrixalsocanberepresentedasaadditivematrixtooriginalpoint
x ∈R3×3,whichcanbedefinedas:
i
(cid:34)t t t (cid:35)
x x x
L= t t t (21)
y y y
t t t
z z z
B SupplementaryExperimentalSettings
B.1 ExperimentalPlatform
Ourexperimentsareconductedonaserverrunninga64-bitUbuntu20.04.1systemwithanIntel(R)
Xeon(R)Silver4210RCPU@2.40GHzprocessor,125GBmemory,andfourNvidiaGeForceRTX
3090GPUs,eachwith24GBmemory. TheexperimentsareperformedusingthePythonlanguage,
version3.8.19,andPyTorchlibraryversion1.12.1.
4https://www.javatpoint.com/computer-graphics-3d-transformations
16Transformations Rotation Scaling Shear Twisting Tapering Translation AVG
w/o 89.32 89.32 89.32 89.32 89.32 89.32 89.32
Sample-wise 91.52 87.78 89.10 92.62 90.31 90.31 89.80
Dataset-wise 87.89 79.41 85.79 92.18 88.22 88.22 83.45
Class-wise 29.85 20.81 67.84 63.22 52.42 36.67 45.14
Table5: Thetestaccuracy(%)resultsondiversetypesoftransformationsonModelNet10trainingsetusing
PointNetclassifier.
Testsets↓Transformations−→ Rotation Scaling Shear Twisting Tapering Translation
Class-wisetestset 99.67 93.80 97.91 94.05 96.04 98.24
Permutedclass-wisetestset 10.68 38.22 58.37 60.68 42.51 31.94
Cleantestset 29.85 20.81 67.84 63.22 52.42 36.67
Table6:AccuracyresultsobtainedwithdifferenttestsetswhentrainingthePointNetclassifierusingclass-wise
transformedtrainingsets.
B.2 Hyper-ParameterSettings
Themodeltrainingprocessontheunlearnabledatasetandthecleandatasetremainsconsistent,using
theAdamoptimizer[20],CosineAnnealingLRscheduler[28],initiallearningrateof0.001,weight
decayof0.0001,batchsizeof16(duetoinsufficientGPUmemory,thebatchsizeissetto8when
training3DGCNonModelNet40dataset),andtrainingfor80epochs. Duetothelongertraining
processrequiredbyPCT[14],thetrainingepochsforPCTinTab.1onModelNet10,ModelNet40,
andScanObjectNNdatasetsareallsetto240.
B.3 SettingsofExploringExperiments
Weinitiallyinvestigatewhichapproachyieldsthebestunlearnableeffectamongsample-wise,dataset-
wise,andclass-wisesettingsinFig.2(a). Specificexperimentalsettingsareasfollows:
Inthedataset-wisesetting,thesameparametervaluesareappliedtotheentiredataset. Specifically,
wehaveα = β = γ = 10◦ intherotationtransformation, thescalingfactorλissetto0.8, both
shearingfactorssandtaresetto0.2. Theangleθintwistingis25◦. Thetaperingangleηissetto
25◦,andtheparametersintranslationtransformationt ,t ,andt aresetto0.15.
x y z
Inthesample-wisesetting,eachsamplehasitsindependentsetofparameters,meaningtheparameter
valuesforeachsamplearerandomlygeneratedwithinacertainrange. Intherotationtransformation,
α,β,γareuniformlysampledfromtherangeof0◦to20◦. Thescalingfactorλisuniformlysampled
from0.6to0.8, shearingfactorssandtareuniformlysampledfromtherangeof0to0.4. Both
thetwistangleθandtaperingangleηaresampledfrom0◦to50◦,andtheparametersintranslation
transformationt ,t ,andt aresampledfrom0to0.3.
x y z
Intheclass-wisesetting,theparametersfortransformationsareassociatedwiththepointcloud’s
class. Theselectionofparametersforeachclassisalsoobtainedbyrandomsamplingwithinafixed
range. Thechosenrangesaregenerallyconsistentwiththosedescribedforthesample-wisesetting
above. However,adifferenceliesintheconsiderationofslightanglerangeandprimaryanglerange
inthecaseofrotationtransformations,wheretheserangesare20◦and120◦,respectively.
Thespecificresultsoftestaccuracyunderdifferenttransformationmodes,includingsample-wise
(random),class-wise,anddataset-wise(universal),areprovidedinTab.5. Itcanbeseenthatunder
theclass-wisesetting,thefinalunlearnableeffectisthebest.
B.4 BenchmarkDatasets
Datasetintroduction. TheModelNet40datasetisapointclouddatasetcontaining40categories,
comprising9843trainingand2468testpointclouddata. ModelNet10isasubsetofModelNet40
dataset with 10 categories. ShapeNetPart that includes 16 categories is a subset of ShapeNet,
comprising12137trainingand2874testpointcloudsamples. ScanObjectNNisareal-worldpoint
clouddatasetwith15categories,comprising2309trainingsamplesand581testsamples. Similar
to[50,15],wesplitKITTIobjectcloudsintoclass“vehicle”and“human”containing1000training
17dataand662testdata. Theinputpointcloudobjectsforthemodelsencompass256pointsforthe
KITTIdatasetand1024pointsforotherdatasets. TheStanford3DIndoorSceneDataset(S3DIS)
datasetcontains6large-scaleindoorareaswith271rooms. Eachpointinthescenepointcloudis
annotatedwithoneofthe13semanticcategories.
Datasetlicenses. Thedatasetlicenseinformationislistedas:
• ModelNet10,ModelNet40[48]: AllCADmodelsaredownloadedfromtheInternetand
the original authors hold the copyright of the CAD models. The label of the data was
obtainedbyusviaAmazonMechanicalTurkserviceanditisprovidedfreely. Thisdataset
isprovidedfortheconvenienceofacademicresearchonly. Linkishttps://modelnet.
cs.princeton.edu/.
• ShapeNetPart[3]: WeusetheShapeNetdatabase(the"Database")atPrincetonUniversity
andStanfordUniversity. Linkishttps://www.shapenet.org/.
• ScanObjectNN[39]: ThelicenseisMITLicense: Copyright(c)2019Vision&Graphics
Group,HKUST.Permissionisherebygranted,freeofcharge,toanypersonobtaininga
copyofthissoftwareandassociateddocumentationfiles(the"Software"),todealinthe
Softwarewithoutrestriction,includingwithoutlimitationtherightstouse,copy,modify,
merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit
personstowhomtheSoftwareisfurnishedtodoso,subjecttothefollowingconditions:
The above copyright notice and this permission notice shall be included in all copies
orsubstantialportionsoftheSoftware. Linkishttps://hkust-vgd.github.io/
scanobjectnn/.
• S3DIS [2]: The copyright is from Stanford University, Patent Pending, copyright 2016.
Linkishttp://buildingparser.stanford.edu/dataset.html.
B.5 SettingsofRobustnessExperiments
Thescalingfactorinrandomscalingaugmentationissettoaminimumof0.8andamaximumof1.25.
Intherandomrotationoperation,thethreedirectionalrotationanglesareidenticalanduniformly
sampledfrom[0,2π). Theperturbationsintherandomjitteraresampledfromanormaldistribution
withastandarddeviationof0.05, andtheperturbationmagnitudeisconstrainedwithin0.1. The
parameterskandαinSORaresetto2and1.1,respectively. ThenumberofdroppedpointsinSRS
is500.
C SupplementaryExperimentalResults
C.1 ResultsforExecutionMechanism
WepresentthespecificexperimentalresultsofFig.2(a)inTab.5. Itcanbeobservedthatunderthe
class-wisesetting,theaveragetestaccuracyisthelowest,whiletheunlearnableconditionyieldsthe
bestperformance. Furthermore,weconductinvestigationsintotrainingthePointNetclassifierwith
class-wisetransformedtrainingsets(ModelNet10dataset),analyzingaccuracyresultsacrossdifferent
testsets,asshowninTab.6. Itisnoteworthythattheaccuracyonclass-wisetestsets(usingconsistent
transformation parameters with class-wise transformed training set) is consistently above 90%,
indicatingthatthemodelhaslearnedthemappingbetweentransformationsandlabels. Thisleadsto
correctclassificationontestsampleswiththesametransformations. However,whentheclass-wise
testsetundergoespermutation,theaccuracydropstoalevelsimilartothatofthecleantestset. This
clearlydemonstratesthatthemodelcancorrectlyclassifysamplesonlywhentheyhavecorresponding
transformations,andsampleswithouttransformationsorwithmismatchedtransformationscannotbe
correctlyclassified. Thisfurthervalidatesthatthemodellearnsaone-to-onemappingbetweenclass
transformationsandlabels.
C.2 ResultsofDiverseCombinationsofTransformations
Weinvestigatecombinationsoffourtransformations,i.e.,rotation,scaling,shear,andtwisting,and
createunlearnabledatasetsusingtheclass-wisesetting. Thetestaccuracyresultsacrossfivepoint
cloudmodelsarepresentedinTab.7. Itcanbeobservedthatthecombinationofrotationandscaling,
18Transformations PointNet PointNet++ DGCNN PointCNN PCT AVG
R 46.70 38.99 49.67 64.87 27.53 45.55
S 34.80 57.05 42.62 33.81 29.63 39.58
H 64.10 66.41 71.15 60.68 60.13 64.49
W 76.98 54.19 78.63 86.69 38.55 67.05
RS 22.25 28.08 14.10 21.48 11.89 19.56
RH 30.51 39.98 45.93 36.23 33.81 37.29
RW 34.91 44.27 29.52 46.92 32.16 37.56
SH 20.26 44.93 38.66 43.50 34.80 36.43
SW 18.28 44.27 31.39 34.25 19.93 29.62
HW 63.33 62.56 61.89 64.76 51.43 60.79
RSH 15.97 29.19 36.56 21.81 31.28 26.96
RSW 25.22 31.50 23.57 31.83 38.55 30.13
SHW 21.70 46.37 55.29 37.11 38.77 39.85
RSHW 16.52 33.37 30.51 30.73 32.49 28.72
Table7:Thetestaccuracy(%)resultsobtainedfromtrainingthepointcloudclassifiersPointNet,PointNet++,
DGCNN,PointCNN,PCTusingaModelNet10datasetgeneratedwithdiversecombinationsoftransformations
underaclass-wisesetting,whereR,S,H,andWcorrespondtorotation,scaling,shear,andtwistingrespectively.
Trainingandtestsets PointNet PointNet++ DGCNN PointCNN AVG
Cleantrainingset,cleantestset 89.32 92.95 92.73 89.54 91.14
Cleantrainingset,UMTtestset 55.51 70.93 74.78 69.71 67.73
UMTtrainingset,cleantestset 22.25 28.08 14.10 21.48 21.48
UMTtrainingset,UMTtestset 98.79 99.23 99.01 96.26 98.32
Table8:Theaccuracy(%)resultsoncleanandUMTModelNet10trainingsetandtestsetusingfourpointcloud
classifiers.Higheraccuracyvaluescorrespondtolowercross-entropylossvalues.
twotypesofrigidtransformations,achievesthemosteffectiveunlearnableeffect. Thetransformation
parametersusedinthissectionareconsistentwithAppendixB.3.
C.3 MoreResultsofUMTAgainstAdaptiveAttacks
TofurtherexploretheperformanceofUMTagainstadaptiveattacks,wesupplementtheexperimental
resultsusingfourtypesofrandomaugmentationsRSHW inTab.9,andfindthattheconclusionis
consistentwithTab.2.
ModelNet10 PointNet PointNet++ DGCNN PointCNN AVG
Cleanbaseline 89.32 92.95 92.73 89.54 91.14
UMT(k=4) 16.19 36.56 17.62 27.42 24.45
UMT(k=4)+randomRSHW 25.99 61.78 61.89 44.16 48.46
Table9:Testaccuracy(%)resultsusingUMTtrainingdataandUMTdataemployingrandomaugmentations.
C.4 ResultsofInsightfulAnalysis
Wetrainoncleantrainingsetandtestoncleantestset,trainoncleantrainingsetandtestonUMT
testset,trainonUMTtrainingsetandtestoncleantestset,andtrainonUMTtrainingsetandtest
onUMTtestsettoobtainthetestaccuracyresultsinTab.8(weensurethattheUMTparameters
fortheUMTtestsetandUMTtrainingsetareconsistent). Higheraccuracyvaluesindicatelower
cross-entropylossvalues,whileloweraccuracyvaluesrepresenthighercross-entropylossvalues.
ItcanbeobservedthatonlywhentrainedwiththeUMTtrainingset,thelossaftertestingwiththe
cleantestsetishigh(i.e.,lowaccuracy).
C.5 BoarderHyper-ParameterAnalysis
Additionally, we investigate the unlearnable effects across a wider range of hype-parameters
r ,r ,b ,b in UMT (k = 2 with RS), as shown in Tab. 10. It can be observed that our UMT
s p l u
scheme still exhibits a good unlearnable effect, and its key lies in the crucial role played by the
class-wisesetting.
19rs,rp,bl,bu PointNet DGCNN PointCNN AVG rs,rp,bl,bu PointNet DGCNN PointCNN AVG
25,120,0.6,0.8 24.78 23.35 28.19 25.44 15,120,0.75,0.8 154 38.66 40.09 31.10
25,180,0.6,0.8 26.65 25.99 20.37 234 15,120,0.6,0.7 20.59 27.53 23.68 23.93
15,90,0.6,0.8 18.39 20.59 22.03 20.34 15,120,0.6,0.8 22.25 14.10 21.48 19.28
15,240,0.6,0.8 21.26 30.51 20.70 24.16 15,120,0.6,0.9 23.90 23.90 23.35 23.72
15,120,0.6,1.2 31.50 28.63 36.78 32.30 15,120,0.6,1.0 22.58 26.54 31.61 26.91
Table 10: The test accuracy (%) results on ModelNet10 dataset with a boarder range of hype-parameters
r ,r ,b,b .
s p l u
Benchmarkdatasets Modules↓Models−→ PointNet PointNet++ DGCNN PointCNN PCT AVG
Rotationmodule 53.51 44.05 57.69 49.48 43.84 49.71
ShapeNetPart[3] Scalingmodule 28.74 77.21 37.93 44.02 51.84 47.95
UMT(k=2) 15.14 41.16 26.17 32.36 44.05 31.78
Table11:Ablationmodules: Thetestaccuracy(%)resultsachievedbytrainingonunlearnabledatacreatedby
differentmodulesontheShapeNetPartdataset.
C.6 SupplementaryAblationResults
Inthissection,weconductablationexperimentsontheUMTschemeontheShapeNetPartdataset
(keepingexperimentalparametersconsistentwiththemainexperiments),asshowninTab.11. It
canbeobservedthatwhetherusingonlytherotationmoduleoronlythescalingmodule,thefinal
unlearnableeffectisnotasgoodasUMT.Thisclearlydemonstratesthateachmodulecontributesto
theoverallUMTeffectiveness.
C.7 ResultsofMixtureofClass-wiseandSample-wiseSamples
Inthissection,weinvestigatethetestaccuracyresultswhenusingamixtureofdifferentratiosofclass-
wisesamplesandsample-wise(random)samplesinthedataset,asshowninTab.12(experimentsare
conductedontheModelNet10datasetwith10categories,where“2class-wise8sample-wise"denotes
thatsamplesfrom2categoriesundergoclass-wisetransformations,whiletheremaining8categories
undergorandomtransformations,andsoon). Wecanclearlyobservefromtheexperimentalresults
thatastheproportionofclass-wisetransformationsgraduallyincreases,thetestaccuracygradually
decreases,indicatingthattheunlearnableeffectbecomesmorepronounced. Thisstronglysuggests
thattheclass-wisesettingismoreeffectivethanthesample-wisesetting.
Clean samples
UMT
samples ° °
[𝟒𝟒.𝟕𝟕°,𝟎𝟎.𝟗𝟗,
𝟗𝟗𝟕𝟕.𝟑𝟑,𝟎𝟎.𝟕𝟕𝟕𝟕]
UMT
samples ° °
[𝟕𝟕.𝟏𝟏°,𝟑𝟑.𝟏𝟏,
𝟕𝟕𝟎𝟎.𝟕𝟕,𝟎𝟎.𝟖𝟖]
UMT
samples ° °
[𝟏𝟏𝟏𝟏.𝟕𝟕°,𝟖𝟖.𝟕𝟕,
𝟏𝟏𝟒𝟒.𝟗𝟗,𝟎𝟎.𝟕𝟕𝟕𝟕] Figure7:CleanandUMTsamplesfromModelNet10dataset.
C.8 AdditionalVisualPresentationsfor3DPointCloudSamples
ModelNet10
WevisualizecleanpointcloudsamplesandUMT(k = 2usingRS)pointcloudsamplesonfour
benchmarkdatasetsModelNet10[48],ModelNet40[48],ScanObjectNN[39],andShapeNetPart[3],
as depicted in Figs. 7 to 10, respectively. The UMT parameters consist of rotation and scaling
parameters[α,β,γ,λ]. Itcanbeobservedthattheseunlearnablesamplesexhibitsimilarfeature
informationtonormalsamples,presentinggoodvisualeffectsandmakingitdifficulttobedetected
asabnormalities.
20ModelNet10[48] PointNet PointNet++ DGCNN PointCNN AVG
100%sample-wisedata 72.69 79.52 85.79 75.66 78.42
20%class-wisedata,80%sample-wisedata 65.64 69.05 78.19 58.04 67.73
40%class-wisedata,60%sample-wisedata 58.04 59.91 60.57 58.59 59.28
60%class-wisedata,80%sample-wisedata 50.11 48.13 60.35 46.48 51.27
80%class-wisedata,20%sample-wisedata 31.83 29.19 44.09 50.55 39.02
100%class-wisedata 22.25 28.08 14.10 21.48 21.48
Table12:Mixtureresults: Thetestaccuracy(%)resultsachievedbytrainingonthemixturedataconsistingof
class-wiseUMTsamplesandsample-wiseUMTsamples.
Clean samples
UMT
samples ° °
[𝟎𝟎.𝟗𝟗°,𝟔𝟔.𝟐𝟐,
𝟗𝟗𝟗𝟗.𝟑𝟑,𝟎𝟎.𝟕𝟕𝟕𝟕]
UMT
samples ° °
[𝟗𝟗𝟎𝟎.𝟗𝟗°,𝟑𝟑.𝟕𝟕,
𝟕𝟕𝟗𝟗.𝟗𝟗,𝟎𝟎.𝟔𝟔𝟕𝟕]
UMT
samples ° °
[𝟗𝟗𝟕𝟕.𝟔𝟔°,𝟓𝟓.𝟗𝟗,
𝟑𝟑𝟔𝟔.𝟗𝟗,𝟎𝟎.𝟕𝟕𝟗𝟗] Figure8:CleanandUMTsamplesfromModelNet40dataset.
Clean samples
ModelNet40
UMT
samples ° °
[𝟒𝟒.𝟔𝟔°,𝟏𝟏𝟏𝟏.𝟕𝟕,
𝟐𝟐𝟐𝟐.𝟒𝟒,𝟏𝟏.𝟔𝟔𝟐𝟐]
UMT
samples ° °
[𝟓𝟓.𝟕𝟕°,𝟒𝟒.𝟖𝟖,
𝟑𝟑𝟔𝟔.𝟒𝟒,𝟏𝟏.𝟔𝟔𝟏𝟏]
UMT
samples ° °
[𝟖𝟖.𝟏𝟏°,𝟏𝟏𝟏𝟏.𝟖𝟖,
𝟒𝟒𝟐𝟐.𝟓𝟓,𝟏𝟏.𝟕𝟕𝟒𝟒] Figure9:CleanandUMTsamplesfromScanObjectNNdataset.
Clean samples Scan
UMT
samples ° °
[𝟓𝟓.𝟕𝟕°,𝟓𝟓.𝟏𝟏,
𝟏𝟏𝟏𝟏.𝟗𝟗,𝟎𝟎.𝟔𝟔𝟔𝟔]
UMT
samples ° °
[𝟔𝟔.𝟔𝟔°,𝟏𝟏𝟏𝟏.𝟕𝟕,
𝟕𝟕𝟔𝟔.𝟏𝟏,𝟎𝟎.𝟔𝟔𝟓𝟓]
UMT
samples ° °
[𝟕𝟕.𝟏𝟏°,𝟏𝟏𝟏𝟏.𝟏𝟏,
𝟗𝟗𝟕𝟕.𝟓𝟓,𝟎𝟎.𝟖𝟖𝟎𝟎] Figure10:CleanandUMTsamplesfromShapeNetPartdataset.
Shape
D ProofsforTheories
D.1 ProofforLemma3
Lemma3. TheunlearnabledatasetD generatedusingUMTonD canalsoberepresentedusinga
u c
GMM,i.e.,D ∼N(yT µ,λ2I).
u y y
21Proof: Assumingy =1,thenD ∼N(µ,I),andwehave
c1
E T x=T E x=T µ,
(x,y)∼Dc1 1 1 (x,y)∼Dc1 1
E (T x−T µ)(T x−T µ)⊤
(x,y)∼Dc1 1 1 1 1
=T E (x−µ)(x−µ)⊤T ⊤
1 (x,y)∼Dc1 1
=T IT ⊤ =λ 2R R ⊤I =λ 2I
1 1 1 1 1 1
Andsimilarly,assumingy =−1,wecanobtain
E T x=−T µ,
(x,y)∼Dc−1 −1 −1
E (T x−T µ)(T x−T µ)⊤ =λ 2I.
(x,y)∼Dc−1 −1 −1 −1 −1 −1
Thuswehave: D ∼N(yT µ,λ2I).
u y y
D.2 ProofforLemma4
Lemma4. TheBayesoptimaldecisionboundaryforclassifyingD isgivenbyP (x)≡Ax⊤x+
u u
B⊤x+C =0,whereA=λ−2−λ−2,B =2(λ−2T +λ−2T )µ,andC =ln |λ2 −1I| .
−1 1 −1 −1 1 1 |λ2I|
1
Proof: Attheoptimaldecisionboundarytheprobabilitiesofanypointx∈Rd belongingtoclass
y = 1andy = −1modeledbyD arethesame. Similartotheoptimaldecisionboundaryofthe
u
cleandatasetD ,wehave:
c
exp[−1(x−T µ )⊤(λ2 I)−1(x−T µ )]
2 −1 −1 −1 −1 −1
(cid:113)
(2π)d|λ2 I|
−1
exp[−1(x−T µ )⊤(λ2I)−1(x−T µ )]
= 2 1 1 1 1 1
(cid:112)
(2π)d|λ2I|
1
⇒(x−T µ )⊤λ−2(x−T µ )+ln|λ2 I|
−1 −1 −1 −1 −1 −1
=(x−T µ )⊤λ−2(x−T µ )+ln|λ2I|
1 1 1 1 1 1
⇒λ−2(x⊤x−x⊤T µ −µ⊤ T⊤ x+µ⊤ T⊤ T µ )
−1 −1 −1 −1 −1 −1 −1 −1 −1
|λ2 I|
−λ−2(x⊤x−x⊤T µ −µ⊤T⊤x+µ⊤T⊤T µ )+ln −1 =0
1 1 1 1 1 1 1 1 1 |λ2I|
1
⇒(λ−2−λ−2)x⊤x−2(λ−2µ⊤ T⊤ −λ−2µ⊤T⊤)x
−1 1 −1 −1 −1 1 1 1
|λ2 I|
+µ⊤ µ −µ⊤µ +ln −1 =0
−1 −1 1 1 |λ2I|
1
|λ2 I|
⇒P (x)≡Ax⊤x+2[(λ−2T +λ−2T )µ]⊤x+ln −1 =0
u −1 −1 1 1 |λ2I|
1
⇒P (x)≡Ax⊤x+B⊤x+C =0
u
whereA=λ−2−λ−2,B =2(λ−2T +λ−2T )µ,andC =ln |λ2 −1I| . Besides,notethatifP (x)
−1 1 −1 −1 1 1 |λ2I| u
1
islessthan0,thecategoryoftheBayesianoptimalclassificationis-1;otherwise,itis1.
D.3 ProofforLemma5
Lemma5. Letz ∼ N(0,I),Z = z⊤z+b⊤z+c,and∥·∥ denote2-normofvectors. Forany
2
t≥0andγ ∈R,weuseChernoffboundtohave:
(cid:110) (cid:111)
exp t2 ||b||2−t(γ+d)
P{Z ≥E[Z]+γ}≤ 2(1−2t) 2
|(1−2t)I|21
22Proof: SinceAisaconstant,wehave:
B C
P (x)≡x⊤x+( )⊤x+ =0
u A A
⇒P (x)≡x⊤x+b⊤x+c=0
u
whereb= B,c= C. LetZ =z⊤z+b⊤z+candz ∼N(0,I)⊂Rd. Thuswehave:
A A
1 1 1
Z =z⊤z+b⊤z+c=(z⊤+ b⊤)(z+ b)+c− b⊤b
2 2 4
1 1 1
=(z+ b)⊤(z+ b)+c− b⊤b
2 2 4
Foranyt≥0andx∼N(0,I),wewritethemomentgeneratingfunctionforaquadraticrandom
variableY =x⊤xas5:
E[exp(tY)]= 1 (cid:90) exp(cid:8) tx⊤x(cid:9) exp(cid:26) −1 (x−µ)⊤(x−µ)(cid:27) dx
(2π)d/2 2
Rd
exp(cid:8) −µ⊤µ/2(cid:9)(cid:90) (cid:26)
2t−1
(cid:27)
= exp x⊤x+µ⊤x dx
(2π)d/2 2
Rd
(cid:110) (cid:111)
exp(cid:8) −µ⊤µ/2(cid:9)(2π)d/2exp 2(1−1 2t)µ⊤µ
=
(2π)d/2 |I−2tI|1
2
(cid:110) (cid:111)
exp t µ⊤µ
1−2t
=
|I−2tI|1
2
(cid:110) (cid:111)
exp t b⊤b+t(c− 1b⊤b) exp{ t2 b⊤b+tc}
=⇒E[exp(tZ)]= 4(1−2t) 4 = 2(1−2t)
|(1−2t)I|1
2
|(1−2t)I|21
Afterthat,weemployChernoffbound,forsomeγ,wehave:
E[exp(tZ)]
P{Z ≥E[Z]+γ}≤
exp{t[γ+E(Z)]}
(cid:110) (cid:111)
exp t2 b⊤b+tc
2(1−2t)
=
exp{t(γ+E[z⊤z]+c)}|(1−2t)I|21
(cid:110) (cid:111)
exp t2 b⊤b+tc
2(1−2t)
=
exp{t(γ+Tr(I)+E(b⊤z)+c)}|(1−2t)I|21
(cid:110) (cid:111)
exp t2 b⊤b+tc
2(1−2t)
=
exp{t(γ+d+c)}|(1−2t)I|21
(cid:110) (cid:111)
exp t2 ||b||2−t(γ+d)
2(1−2t) 2
=
|(1−2t)I|21
5[38]
23D.4 ProofforTheorem6
Theorem6. Foranyconstantt andt satisfying0≤t < 1 and0≤t < 1,theaccuracyofthe
1 2 1 2 2 2
unlearnabledecisionboundaryP onthedatasetD canbeupper-boundedas:
u c
τ (P )≤
exp(cid:110) 2(1−t2 1 2t1)||b+2µ||2 2+t 1(µ⊤µ+b⊤µ+c)(cid:111)
Dc u 2|(1−2t 1)I|21
+
exp(cid:110) 2(1−t2 2 2t2)||b−2µ||2 2−t 2(µ⊤µ−b⊤µ+c+2d)(cid:111)
2|(1−2t 2)I|1 2
:=p +p
1 2
Furthermore,ifµ⊤µ+b⊤µ+c+d < 0and−µ⊤µ+b⊤µ−c−d < 0,wehaveτ (P ) < 1.
Dc u
Moreover,foranyµ̸=0∃transformationmatrixT suchthatτ (P )<τ (P).
i Dc u Dc
Proof: WenotethatifP (x)islessthan0,thecategoryoftheBayesianoptimalclassificationis-1;
u
otherwise,itis1. Here,x=yµ+zwherez ∼N(0,I)andy ∈{±1}since(x,y)∼D .
c
τ (P )=E{I(y(x⊤x+b⊤x+c)>0)}
Dc u
=P{y(µ⊤µ+z⊤z+2yµ⊤z+yb⊤µ+b⊤z+c)>0}
=P(y =1)P{y(µ⊤µ+z⊤z+2yµ⊤z+yb⊤µ+b⊤z
+c)>0|y =1}+P(y =−1)P{y(µ⊤µ+z⊤z
+2yµ⊤z+yb⊤µ+b⊤z+c)>0|y =−1}
1
= P{z⊤z+(b+2µ)⊤z+µ⊤µ+b⊤µ+c>0}
2
1
+ P{−z⊤z−(b−2µ)⊤z−µ⊤µ+b⊤µ−c>0}
2
:=p +p
1 2
Wecanseethat:
−γ :=E{z⊤z+(b+2µ)⊤z+µ⊤µ+b⊤µ+c}
1
=Tr(I)+µ⊤µ+b⊤µ+c
−γ :=E{−z⊤z−(b−2µ)⊤z−µ⊤µ+b⊤µ−c}
2
=−Tr(I)−µ⊤µ+b⊤µ−c
ApplyingLemma5,withγ =γ ,t=t forthecomputationofp ,aswellasγ =γ andt=t for
1 1 1 2 2
thecomputationofp ,wheret andt arespecificnon-negativeconstants,weobtain:
2 1 2
p =
exp(cid:110) 2(1−t2 1 2t1)||b+2µ||2 2+t 1(µ⊤µ+b⊤µ+c)(cid:111)
1 2|(1−2t 1)I|1 2
p =
exp(cid:110) 2(1−t2 2 2t2)||b−2µ||2 2−t 2(µ⊤µ−b⊤µ+c+2d)(cid:111)
2 2|(1−2t 2)I|1 2
Thisprovidesuswiththeupperboundforτ (P ). Nonetheless,toensurethatthisupperboundis
Dc u
lessthan1,additionalconditionsneedtobeaffirmed. Asγ andγ increase,thevaluesofp and
1 2 1
p diminish(p >0,p >0),andasγ increases,γ decreases(sinceγ +γ =−2b⊤µ). Welet
2 1 2 1 2 1 2
||b+ 22µ||2 2 equaltoα 1 ≥0,µ⊤µ+b⊤µ+c(alsoequalsto||µ||2 2+c− γ1+ 2γ2)equaltoβ 1,resulting
24in:
dp
1 =
1d[exp{ 1α −1 2t t2 1
1
+β 1t 1}/(1−2t 1)d 2]
dt 2 dt
1 1
=
exp{ 1α −1 2t t2 1
21
+β 1t 1}
[(1−2t 1)−d 2−1d
2α t (1−t )
+(1−2t 1)−d 2( (11 −1
2t
)21 +β 1)]
1
=
exp{ 1α −1 2t t2 1
1
+β 1t 1}
[
d
+
2α 1t 1(1−t 1)
+β ]
2(1−2t 1)d 2 1−2t 1 (1−2t 1)2 1
d 2α t (1−t )
=p ( + 1 1 1 +β )
1 1−2t (1−2t )2 1
1 1
2(2β −α )t2+2(α −2β −d)t +β +d
= 1 1 1 1 1 1 1 p
(1−2t )2 1
1
Making|(1−2t 1)I|1 2 meaningfulrequiressatisfyingthefollowingcondition:
1
1−2t >0=⇒0≤t <
1 1 2
Wenotethatp (t =0)= 1,p (t → 1)=+∞. Whenweset dp1 =0,weobtainthat:
1 1 2 1 1 2 dt1
2(2β −α )t2+2(α −2β −d)t +d+β =0
1 1 1 1 1 1 1
(cid:112)
2β −α +d± α2−2α β +d2
=⇒t = 1 1 1 1 1
1 2(2β −α )
1 1
=⇒α2−2β α +d2 ≥0
1 1 1
d2+α2 d2 α
=⇒β ≤ 1 = + 1
1 2α 2α 2
1 1
d2 α
=⇒β ≤( + 1) =d
1 2α 2 min
1
ExplanationoftheLastInequality. Assumingwedefines(α 1)= 2d α2
1
+ α 21. Theminimumvalue
ofthisfunctioncanbeobtainedusingtheArithmeticMean-GeometricMeanInequality(AM-GM
√ (cid:113)
Inequality),whichstatesthata+b ≥ 2 abfora,b ≥ 0. Thus,s(α 1) ≥ 2 2d α2
1
· α 21 = d. Since
β ≤s(α ),wecaninferthatβ ≤s(α ) ,whichmeansβ ≤d.
1 1 1 1 min 1
Theproductoftherootsoftheequationis: t t = d+β1 (weassumethatt <t ). Welet
11 12 2(2β1−α1) 11 12
f(t 1)=2(2β 1−α 1)t2 1+2(α 1−2β 1−d)t 1+d+β 1.Thuswehavef(0)=d+β 1,f(1 2)= α 21 >0.
Situation(i): β <−d. Atthistime,f(0)<0,t t >0,basedonthetrendofquadraticfunctions
1 11 12
and the distribution of roots, we can infer that 0 < t < 1, t > 1, and 2β < α . Thus we
11 2 12 2 1 1
concludethatthereexistst suchthatp (t =t )< 1.
11 1 1 11 2
Situation(ii): −d < β < 0. Atthistime,f(0) > 0,t t < 0,basedonthetrendofquadratic
1 11 12
functionsandthedistributionofroots,wealsocaninferthatt <0,t > 1,and2β <α . Thus
11 12 2 1 1
wehavethatp ≥ 1.
1 2
Situation(iii): 0<β <d. Atthistime,f(0)>0,thesignoft t simultaneouslydeterminesthe
1 11 12
directionoftheopeningofthequadraticfunctionf(t ). Whent t <0,t <0,t > 1,thuswe
1 11 12 11 12 2
havep ≥ 1;whent t >0,0<t <t < 1,theminimumpointofp isatp (t ). However,
1 2 11 12 11 12 2 1 1 12
itischallengingtocomparep (t )and 1 todeterminewhichisgreaterorsmaller.
1 12 2
Similarlyforp 2,welet ||b− 22µ||2 2 equaltoα 2 >0,−µ⊤µ+b⊤µ−c−2dequaltoβ 2,resultingin:
dp 2(2β −α )t2+2(α −2β −d)t +β +d
2 = 2 2 2 2 2 2 2 p
dt (1−2t )2 2
2 2
25Thuswehavethesimilarsituation,i.e.,β <−d. Atthistime,f(0)<0,t t >0,basedonthe
2 21 22
trendofquadraticfunctionsandthedistributionofroots,wecaninferthat0 < t < 1,t > 1,
21 2 22 2
and2β <α . Thusweconcludethatthereexistst suchthatp (t =t )< 1.
2 2 21 2 2 21 2
Takingintoaccounttheabovesituations,wehavethat: whenβ <−d,β <−d(i.e.,µ⊤µ+b⊤µ+
1 2
c+d < 0and−µ⊤µ+b⊤µ−c−d < 0),thereexistst andt respectively,makingp < 1,
√ 11 21 √ 1 2
p < 1,i.e.,p +p <1,wheret = 1 + d− α2 1−2α1β1+d2 ,t = 1 + d− α2 2−2α2β2+d2 .
2 2 1 2 11 2 2(2β1−α1) 21 2 2(2β2−α2)
Weknowthatforµ̸=0,τ (P)=ϕ(µ)> 1. Therefore,weneedtointroduceadditionalconditions
Dc 2
tofurtherensurethatp < 1,andp < 1,andconsequentlyτ (P )=p +p < 1 <τ (P).
1 4 2 4 Dc u 1 2 2 Dc
Toletp satisfyp < 1 (α >0,β <−d,and0≤t < 1),thatis,
1 1 4 1 1 1 2
exp{ α 1t2 1 +β t }< (1−2t 1)d 2
1−2t 1 1 2
1
α t2 d
⇒ 1 1 +β t < ln(1−2t )−ln2
1−2t 1 1 2 1
1
α t2 d
⇒ 1 1 +β t − ln(1−2t )<−ln2=−0.693
1−2t 1 1 2 1
1
Weassumethatg(t)= 1α −1t 22
t
+β 1t− d 2ln(1−2t). Letusassumeα
1
= 1
2
andd=3for3Dpoint
clouddata,thenβ <−3. Uponanalyzingthefunctiong(t),weobservethatasβ decreases,the
1 1
minimumvalueofg(t)alsodecreases. Weutilizevariousβ valuesandtheircorrespondingfunction
1
valuetoprovideamoreintuitiveunderstandingasshowninTab.13.
β -4 -6 -8 -10 -12 -14
1
g(0.3) 0.287 -0.313 -0.913 -1.513 -2.113 -2.713
g(0.4) 1.214 0.414 -0.386 -1.186 -1.986 -2.786
Table13: Differentβ valuesandcorrespondingg(t)witht = 0.3andt = 0.4. Theboldvaluesrepresent
1
caseswherep < 1 issatisfied.
1 4
Asforp ,sinceα ̸= α ,weneedtoreselectappropriatevaluestodemonstratetheexistenceof
2 2 1
p < 1. Similarly,toletp satisfyp < 1 (α >0,β <−d,and0≤t < 1),thatis,
2 4 2 2 4 2 2 2 2
exp{ α 2t2 2 +β t }< (1−2t 2)d 2
1−2t 2 2 2
2
α t2 d
⇒ 2 2 +β t − ln(1−2t )<−ln2=−0.693
1−2t 2 2 2 2
2
Weassumethath(t) = 1α −2t 22
t
+β 2t− d 2ln(1−2t). Letusassumeα
2
= 1
3
andd = 3, similarly,
we utilize various β values and their corresponding function value to provide a more intuitive
2
understandingasshowninTab.14.
β -4 -6 -8 -10 -12 -14
2
h(0.3) 0.249 -0.351 -0.951 -1.551 -2.151 -2.751
h(0.4) 1.081 0.281 -0.519 -1.319 -2.119 -2.919
Table14: Differentβ valuesandcorrespondingh(t)witht = 0.3andt = 0.4. Theboldvaluesrepresent
2
caseswherep < 1 issatisfied.
2 4
Therefore,weconcludethatthereexistα ,β ,t suchthatp < 1,α ,β ,t suchthatp < 1,i.e.,
1 1 1 1 4 2 2 2 2 4
p +p < 1.Atthesametime,weobservethatasmallervalueofβmakesiteasiertosatisfytheabove
1 2 2
conditions,i.e.,themorenegativeβ andβ are,themorelikelyitistosatisfytheaboveconditions.
1 2
Weformallycombineandasserttheseconditionsas,b⊤µ≪0,i.e.,µ⊤λ− −2 1T⊤ −1+λ− 12T⊤
1 µ≪0(we
λ−2−λ−2
−1 1
sufficientlysupportthisconditionintheempiricalresultsfromTabs.13and14). Thusweconclude
thatforanyµ̸=0∃transformationparametersT suchthatτ (P )<τ (P).
i Dc u Dc
26