Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge
Distillation from Multiple Large Language Models
ZhuochunLi1,YuelyuJi1,RuiMeng2,DaqingHe1,
1SchoolofComputingandInformation,UniversityofPittsburgh,Pittsburgh,USA
2SalesforceResearch
{zhl163,yuj49,dah44}@pitt.edu,memray0@gmail.com
Abstract
Largelanguagemodels(LLMs)haveexhibited
complexreasoningabilitiesbygeneratingques-
tion rationales and demonstrated exceptional
performance in natural language processing
(NLP)tasks. However, thesereasoningcapa-
bilitiesgenerallyemergeinmodelswithtens
ofbillionsofparameters, creatingsignificant
computational challenges for real-world de- Figure 1: Student LM learns from multiple teacher
ployment. Recentresearchhasconcentratedon LLMsviaPeer-Reviewdistillation.
improvingopen-sourcesmallermodelsthrough
knowledgedistillation(KD)fromcommercial
LLMs. Nevertheless,mostofthesestudiesrely Zengetal.,2022)hasshownthatsmallerlanguage
solelyontheresponsesfromonesingleLLMas
models,particularlythosewithfewerthan10bil-
thegoldrationalefortraining. Inthispaper,we
lionparameterslikeLLama2-7B,canperformsim-
introduceanovelMistake-AwarePeer-Review
ilarly to larger models in terms of following hu-
Distillation(MAPD)approach: 1)Insteadof
man instructions. However, it is challenging to
merely obtaining gold rationales from teach-
ers, ourmethodasksteacherstoidentifyand prompt smaller Language Models (LMs) to gen-
explainthestudent’smistakes,providingcus- eratereasoningstepsbyChain-of-Thought(CoT)
tomized instruction learning data. 2) We de- prompts(Wangetal.,2023). Moreover,mostexist-
signasimulatedpeer-reviewprocessbetween ingreasoningdatasetslackhigh-qualityrationale,
teacher LLMs, which selects only the gener-
whichisdefinedasjustifyingamodel’soutputby
atedrationalesabovetheacceptancethreshold.
providing a natural language explanation for the
Thisreducesthechanceofteachersguessing
finalcorrectanswer(Gurrapuetal.,2023),dueto
correctlywithflawedrationale,improvingin-
thehighcostofmanualannotations.
structionaldataquality. Comprehensiveexperi-
mentsandanalysisonmathematical,common- Toaddressthesechallenges,distillingthecapa-
sense,andlogicalreasoningtasksdemonstrate bilitiesofLLMsemergesasaresource-friendlyand
theeffectivenessofourmethod. effective strategy. Through collecting rationales
generatedbyLLMsforinstructiontuning,previous
1 Introduction
studieshavebeenabletodistilltheprivateLLMs’
LargeLanguageModels(LLMs)haveproventobe reasoningabilitiesintosmallermodels(Wangetal.,
highlyeffectiveinaddressingawiderangeofcom- 2022; Ho et al., 2023; Magister et al., 2022; Fu
plextasks(Nietal.,2024;FanandTao,2024),in- et al., 2023). However, most of these efforts fall
cludingmathematicalreasoning(Lewkowyczetal., withinthescopeofLabelingKnowledgeDistilla-
2022; Imani et al., 2023), commonsense reason- tion(Xuetal.,2024b),whereLLMsareprimarily
ing (Zhao et al., 2024; Achiam et al., 2023), and usedtoannotatedatafortrainingsmallermodels,
logicalreasoning(Liuetal.,2023;Xuetal.,2023b). without utilizing smaller model’s output as feed-
However,theseemergentreasoningabilitiestend back to generate customized instruction data to
tomanifestonlyinLLMswithmorethan100bil- improve the LM in return. As a result, LLMs re-
lionparameters,whilesmallermodelsstruggleto mainunawareofthelimitationsofsmallermodels,
exhibit such capabilities (Wei et al., 2022a). De- whichhamperstheirabilitytoprovidetargetedanal-
spite this, recent research (Touvron et al., 2023; ysisandfeedback,reducingtheeffectivenessofthe
4202
tcO
4
]LC.sc[
1v36630.0142:viXrareasoningdistillationprocess. generalreasoningabilities.
Furthermore, prior research typically employs
2. We design a simulated Peer-Review mech-
onlyoneLLMastheteacher,whichcanintroduce
anism between teacher LLMs to filter out
morebiasedtrainingdatacomparedtousingmul-
flawedrationalesandimprovetheconfidence
tipleteacherLLMsduringdistillation. Therefore,
ofinstructiontuningdata.
we propose using multiple LLMs from different
organizations as teachers to provide more impar-
3. Our work provides a comprehensive bench-
tial and diverse training data for the student LM.
markonthemathematical,commonsense,and
Additionally,wedesignedasimulatedpeer-review
logicalreasoningtasks. Experimentsandcom-
processbetweentheteacherLLMs,wherethera-
parisonswithotherconcurrentworksdemon-
tionalegeneratedbyoneLLMisreviewedbyother
stratetheeffectivenessofourmethodindis-
LLMs. Only the rationales that pass this peer-
tillingthereasoningabilityofteacherLLMs.
reviewprocessareincludedinthetrainingdataset.
This method reduces the likelihood of flawed ra- 2 RelatedWork
tionales,evenwhenacorrectanswerisprovided,
LLMReasoning Recentstudieshavefocusedon
therebyimprovingtheoverallqualityofthetrain-
provokingthethoughtprocessesofLLMs,validat-
ingdatausedforinstructiontuning.
ingtheireffectivenessinreasoningtasks(Weietal.,
To this end, we propose a Mistake-Aware
2022b; Imani et al., 2023; Fu et al., 2023), such
Peer-Review Distillation (MAPD) method via
as GSM8K (Cobbe et al., 2021), SVAMP (Patel
Peer-Review knowledge distillation from multi-
etal.,2021), andStrategyQA(Gevaetal.,2021).
pleLLMs,asbrieflyshowninFigure1. Inspired
Various techniques have been developed to en-
by the natural human learning process (Konold
hance LLM reasoning abilities (Chu et al., 2023;
et al., 2004), we argue that students should not
Xuetal.,2024a). Forinstance,Chain-of-Thought
onlyknowwhatisthecorrectanswerbutalsolearn
(CoT) (Wei et al., 2022b) improves reasoning by
why they made mistakes. Therefore, in addition
promptingLLMstogenerateintermediatenatural
toprovidingthecorrectrationalegeneratedbythe
languagethoughtprocesses. Huangetal.(2022)
teacherLLMs,wealsopresentthestudentmodel’s
provesthatLLMscanself-improvereasoningcapa-
mistakestotheteacherLLMsandreturnmistake-
bilitythroughself-trainingbycollectingdatausing
specific feedback to the student model. This en-
majority voting techniques. Chung et al. (2024)
ablesthestudenttolearnboththe"what"andthe
demonstrated that smaller LMs can partially ac-
"why,"enhancingitsreasoningabilitiesandequip-
quireCoTskillsbytrainingondatawithrationales.
pingittosolvesimilarproblems,evenifithasnot
Inthispaper,wefurthershowthattheCoTperfor-
encounteredthembefore. Furthermore,inspiredby
mance of smaller LMs can be improved through
themulti-agentevaluationframeworkofNanetal.
integratedinstructionlearningusingCoTdatase-
(2023),weemploymultipleLLMsasteachersand
lectedbymajorityvotingfromLLMs.
askthemthesamequestion. EachteacherLLM’s
KnowledgeDistillationfromLLMs Distilling
answerisreviewedbytheotherteachers,andonly
knowledgefromLLMsbyfine-tuningsmallerlan-
theresponsesthatpassthispeer-reviewprocessare
guage models to follow instructions using high-
includedintheinstructiontrainingdataset. Webe-
quality data collected from LLMs has become a
lievethispeer-reviewmechanismbetweenteacher
prominentresearchdirection(Xuetal.,2023a;Fu
LLMs can significantly reduce biased or flawed
etal.,2023;Lietal.,2024). Thisapproachserves
rationales,leadingtoimproveddistillationperfor-
as an effective method for transferring the emer-
mance. Insummary,thecontributionsofourwork
gentabilitiesofblack-boxLLMstosmalleropen-
areasfollows:
sourcemodels. However,whilerecentworks(Ho
1. TheMistake-AwarePeer-ReviewDistillation etal.,2023;Shridharetal.,2022;Guoetal.,2024)
(MAPD)approachisintroducedtohelpstu- useLLM-generatedreasoningrationalesassuper-
dentLMlearnnotonlyfromthegold-standard visorysignalstotrainsmallertask-specificmodels,
rationalebutalsofromfeedbackontheirown theyoftenoverlookprovidingstudentmodelswith
mistakes provided by teacher LLMs, which feedbackontheirmistakeswhentheiranswersare
builds a comprehensive instruction tuning incorrect. Toaddressthis,wecollectboththecor-
methodaimedatenhancingthestudentLM’s rect rationale and mistake-specific feedback forstudentmodels’wronganswersfromLLMs,inte-
gratingthemintoinstructiontuningtoenhancethe
overallreasoningcapabilitiesofthestudentmod-
els. Moreover, unlike most studies that rely on a
singleteacherLLM(Wangetal.,2023;Chenglin
et al., 2023; Zhu et al., 2024), we employ multi-
ple LLMs as teachers to increase the diversity of
generateddata. Finally,comparedtostudiesofthe
peer-review in LLMs for evaluation (Ning et al.,
2024;Chuetal.,2024),wedesignasimulatedpeer-
review process to ensure high-quality instruction
training data, thereby improving the distillation
performance.
Figure 2: The prompt template P and P for asking
r f
teacherLLMstogeneraterationaleandmistakefeed-
3 Method back. Thepartcoloredinyellowistheteacher’soutput.
AsillustratedinFigure3,weintroduceaMistake-
AwarePeer-ReviewDistillation(MAPD)knowl- theconcatenationofthestudentmodel’srationale
edgedistillationmethodthatempowersthestudent
r′ andanswery′,withtheanswertypicallyatthe
model to improve by learning from its own mis- endoftheoutput. Sincethecorrectrationaler is
takesandthecorrectanswersgeneratedbymulti- oftennotprovidedinD train,wefollowWangetal.
ple teacher models. Specifically, our instruction (2023)’sworkbyconsideringr′ asthewrongratio-
learningprocedureinvolvesfourmajorsteps: (1) naleify′ ̸= y. Finally,themistakesetD mistake is
ThestudentLMtakesan“exam“onthetrainingset collectedasfollows:
toidentifymistakesthatareincorrectlygenerated
D ={(x,r′,y′)|y′ ̸=y∩(x,y)∈D } (1)
rationales. (2)Wethencraftvariouspromptsthat mistake train
incorporate the question and the student’s wrong The collected mistake set D highlights
mistake
rationaletoprompttheteacherLLMstogenerate the student’s reasoning weaknesses and will be
thegoldanswersandprovidefeedbackonthestu- utilizedforthefollowingpurposes:
dent’serrorsrespectively. (3)Subsequently,asim- 1) Providingtheincorrectlyansweredquestions
ulatedpeer-reviewprocessisconductedamongthe fortheteacherLLMstogeneratecorrectratio-
teacherLLMstoproducehighlyconfidentinstruc- nales.
tional data. (4) Finally, the student model learns
2) Using the student’s incorrect rationales to
toreasonthroughinstructionlearningbasedonthe
prompt the teacher LLMs to identify errors
peer-reviewedcorrectanswersandtailoredcorrec-
andcreatecustomizedmistakesfeedback.
tionsonitsmistakesprovidedbytheteacherLLMs.
3.2 InquiryTeacherLLMswiththeMistakes
We expect the teacher LLM to function as a rea-
3.1 ExamonStudentModel
soning instructor who can identify student’s mis-
We aim to gather samples from reasoning bench- takes and provide tailored feedback, rather than
markswherethestudentmodelincorrectlyanswers merely an answer provider. Therefore, we query
questions. Thesesampleswillbeusedtocreatecus- the teacher LLMs with the student’s incorrectly
tomizedinstructionaldatafromtheteachermodels. answered questions, aiming for them to generate
To achieve this, the student model undergoes an thecorrectrationaleandidentifyspecificerrorsin
“exam”onthetrainingsetD toassessitsrea- thestudent’smistakes. Webelievethatcustomized
train
soningabilityandcollectthemistakesetD , trainingdata,whichincludesboth"what"thecor-
mistake
which are the samples containing incorrect ratio- rectanswerisand"why"themistakesweremade,
nales and answers. Specifically, given a dataset can effectively address the student’s weaknesses.
D = {x,y}, where x is the question and y is WeusethesamepromptP employedtoconduct
r
thegoldanswer,weproposetoinputthequestion exams on the student model to obtain rationales
x into the student model to generate the output from the teachers. For prompt P to gather feed-
f
f(x) = [r′,y′]. Here, the square brackets denote backonthestudent’smistakes,wefollowZelikmanFigure 3: Overview of the proposed Mistake-Aware Peer-Review Distillation (MAPD) method. The specific
structureofthepeer-reviewprocessisexplainedintheleft-bottomsub-figure.
et al. (2022) by adding a hint that explicitly pro- LLMs. Since most relevant datasets do not pro-
videsthecorrectanswertothequestion,ensuring videgoldrationales,weassumethateachLLM’s
moreaccurateresponses. Thedetailedprompttem- rationale should be reviewed and scored by peer
plates are shown in Figure 2. In detail, for each LLMs,whichisinspiredbythemulti-agentevalu-
sample (x,r′,y′) ∈ D , we request each ationframeworkofNanetal.(2023). Onlythose
mistake
teacherT(x)togeneraterationaler andfeedback rationalesthatpassthispeer-reviewprocesswith
t
f whichwillbecollectedasthefeedbacksetD : highconfidencewillbeincludedinthefinalinstruc-
t f
tional tuning dataset. Figure 3 has explained the
r t = T(P r(x)) peer-reviewprocess. Specifically,assumewehave
f = T(P (x,r′,y)) (2) threedifferentteacherLLMsT ,T ,T . Weobtain
t f 1 2 3
D = {(x,r′,f )|x ∈ D } their rationales based on Equation 2, resulting in
f t train
r ,r ,r . For each rationale, we incorporate it
t1 t2 t3
3.3 SimulatedPeer-ReviewBetweenTeacher into the designed peer-review prompt P shown
pr
Models in Figure 4 and request the other LLMs to score
During our experiments, we observedthat the ra- therationaleasS t(r t). Onlytherationalewithan
tionalesprovidedbyteacherLLMsarenotalways averagescoreexceedingtheacceptancethreshold
accurate,evenwhenthefinalanswermatchesthe Th will be included in the rationale set D r. The
goldanswer. Thisdiscrepancyisrareincommon peer-reviewprocesscanbedescribedasfollows:
mathematical tasks, where there is often a strict
S (r )=T (P (x,r ,y)), S (r )=T (P (x,r ,y))
t2 t1 2 pr t1 t3 t1 3 pr t1
correlationbetweenthecorrectnessoftherationale
S (r )=T (P (x,r ,y)), S (r )=T (P (x,r ,y))
t1 t2 1 pr t2 t3 t2 3 pr t2
andthefinalanswernumberduetotheinherentna-
S (r )=T (P (x,r ,y)), S (r )=T (P (x,r ,y))
t1 t3 1 pr t3 t2 t3 2 pr t3
tureofmathematics. However,formultiple-choice
D ={(x,r )|if (S (r )+S (r ))/2≥Th}
r t1 t2 t1 t3 t1
questions,suchasthoseinthecommonsenseStrat-
∪{(x,r )|if (S (r )+S (r ))/2≥Th}
t2 t1 t2 t3 t2
egyQA(Gevaetal.,2021)(TrueorFalse)andlogic
∪{(x,r )|if (S (r )+S (r ))/2≥Th}
t3 t1 t3 t2 t3
LogiQA(Liuetal.,2020)(A,B,C,D)benchmarks, (3)
thereareinstanceswhereacorrectrationalemay
3.4 InstructionTuningonStudentModel
lead to an incorrect final choice, or a wrong ra-
tionalemightresultinacorrectfinalchoice. See The reasoning ability of the student LM can be
AppendixCformorepeer-reviewexamplesondif- enhanced through instruction tuning (Wei et al.,
ferentbenchmarks. 2021),whichincorporatesbothgoldanswersand
To address this issue and avoid having teacher customized mistake corrections provided by the
LLMs "guess" the correct answer, we propose teacher models. See Appendix B for explicit in-
a simulated peer-review process among teacher structiontuningtemplatesondifferentbenchmarks.whereCEdenotestheCross-Entropyfunction,and
thesquarebracketsrepresentthestringconcatena-
tion. f([x,r′])representsthestudentLM’soutput
whengiven[x,r′]astheinput.
Joint Learning The final optimization process
integrateslearningfrombothgoldanswersandthe
teachers’customizedmistakefeedback. Therefore,
theinstructionlearninglossesfromEquation4and
Equation5arecombinedasfollows:
L = α∗L +(1−α)∗L (6)
mf gr
whereαcontrolstheimpactoflearningfrommis-
takes,balancingthetwolearningobjectives.
Figure4: TheprompttemplateP foraskingteacher
pr
LLMstoperformpeer-reviewprocess. Thepartcolored
inyellowistheteacher’soutput. 4 Experiments
4.1 Datasets
LearnfromTeacher’sRationales Therationales
MathematicalReasoning Wefocusedonthetwo
generatedbytheteacherLLMsarespecificallytai-
most popular math problem datasets to evaluate
lored to address the student’s weaknesses, identi-
mathematical reasoning ability. GSM8K (Cobbe
fiedthroughthestudent’spreviousexam. Accord-
et al., 2021) is a dataset of 8.5K high-quality,
ing to Equation 3, these collected rationales are
linguistically diverse grade school math word
combined into the set D as the gold rationales,
r
problems created by human problem writers.
which are then used to fine-tune the student LM.
SVAMP (Patel et al., 2021) is a challenge set for
Fortheinstructiontuningprocess,weaimforthe
elementary-level Math Word Problems (MWP),
studentmodel,whengiventhequestionxasthein-
consistingofshortnaturallanguagenarrativesthat
struction,toproduceananswerthatcloselyaligns
describe a state of the world and pose questions
with the corresponding rationale r in D . This
t r
aboutunknownquantities.
traininglossisthereforedescribedasfollows:
Commonsense Reasoning StrategyQA (Geva
L = CE(f(x), r |x) (4) et al., 2021) is a question answering benchmark
gr t
where the required reasoning steps are implicit
whereCEdenotestheCross-Entropyfunction,and withinthequestionandshouldbeinferredusinga
f(x) represents the student LM’s output when commonsensestrategy.
givenxastheinput. LogicalReasoning LogiQA(Liuetal.,2020)isa
Learn from Student’s Mistakes In addition to dataset constructed from logical comprehension
learning from correct rationales, we propose that problems sourced from publicly available ques-
thestudentmodelshouldalsolearnfromitsown tionsoftheNationalCivilServantsExaminationof
mistakes, simulating the typical human learning China. Thesequestionsaredesignedtoassesscivil
process. Thisapproachhelpsthestudentnotonly servantcandidates’criticalthinkingandproblem-
graspthecorrectanswersbutalsounderstandthe solving abilities. For our experiments, we used
reasonsbehindtheerrors. Tofacilitatethis,wecon- onlytheEnglishversionofthedataset.
structedthefeedbacksetD ,basedonEquation2,
f
whichprovidesfeedbackonthestudent’smistakes. 4.2 Models&Baselines
Throughthisprocess,weexpectthestudentLMto
Models For teacher LLMs, considering the ex-
learntheteacher’sreasoningcapabilitiesandgen-
pense and accessibility of our diverse LLMs, we
erateoutputsthatcloselyalignwiththeteacher’s
selectGPT-3.5-turbo1,Gemini-1.0-pro(Teametal.,
feedbackf whengiveninstructionstoidentifyits
t 2023),andMixtral-8x7B-Instruct-v0.1(Jiangetal.,
ownmistakes. Finally,thelossfunctionforlearn-
2024)asteachermodels. ThesethreeLLMswere
ingfrommistakefeedbackisdefinedasfollows:
1https://platform.openai.com/docs/models/
L mf = CE(f([x,r′]), f t |[x,r′]) (5) gpt-3-5-turboMethod #Params DistillationTeachers Mathematical Commonsense Logical
GSM8K SVAMP StrategyQA LogiQA
TeacherLLM
GPT-3.5-turbo 175B - 78.01* 82.30* 70.92* 40.55*
Gemini-1.0-pro - - 76.42* 81.10* 67.03* 39.94
Mixtral-8x7B-Instruct-v0.1 46.7B - 74.40* 81.60* 72.83* 34.19*
StudentLM
Llama2-7B-chat(Touvronetal.,2023) 7B - 16.55 44.71 48.53 16.50
CodeT5-Large+PaD(Zhuetal.,2024) 770M GPT-3.5-turbo 44.90* 51.00* - -
GPT2-Large+Soc(Shridharetal.,2022) 774M GPT-3 21.08* - 66.40* -
GPT-J+Self-Reflection(Wangetal.,2023) 6B ChatGPT 33.10* 55.00* 65.90* -
Llama-7B+NCE(Lietal.,2024) 7B GPT-3.5-turbo,GPT-4 41.93* 51.50* - -
Llama2-7B+ReversalMath(Guoetal.,2024) 7B GPT-4 52.10* 59.20* - -
ORCA2-7B(Mitraetal.,2023) 7B ChatGPT,GPT-4 47.23* - - 35.02*
T5-XXL+Specialized(Fuetal.,2023) 11B GPT-3.5 27.10* 35.60* - -
T5-XXL+CoT(Magisteretal.,2022) 11B PaLM,GPT-3 21.99* - 63.77* -
Peer-ReviewedDistillation(Ours)
Llama2-7B+Teacher-Mistral 7B Mixtral-8x7B-Instruct 22.67 47.33 65.65 32.12
Llama2-7B+Teacher-Gemini 7B Gemini-1.0-pro 26.84 49.33 57.93 32.73
Llama2-7B+Teacher-GPT 7B GPT-3.5-turbo 30.71 51.67 60.12 31.04
Llama2-7B+Teacher-Multiple 7B Multiple 36.24 59.50 67.67 36.27
Table1: Accuracy(%)acrossvariousreasoningtaskswithdifferentdistillationmethods. *denotestheresultsare
fromtheoriginalpaperorofficialdocument. "Teacher-x"indicatesthespecificteacherLLMusedinthedistillation
experiment. ThebestperformancesamongstudentLMsaremarkedinbold.
builtbydifferentorganizationsbutallprovedpow- 4.3 ExperimentalSetup
erful NLP abilities. Furthermore, Mixtral-8x7B-
All three teacher LLMs were configured with
Instruct-v0.1isanopen-sourcemodelthatisdiffer-
a Temperature of 0.8 and Max_tokens set to
entthantheothertwoprivateones,weassumeall
512. Thestudentmodelwasinstruction-tunedus-
thesevariationswillmakeourmultiple-teacherdis-
ing a learning rate of 1e-5 over 10 epochs with
tillationmoreimpartial. Inaddition,wechoosethe
AdamW(Loshchilov,2017)astheoptimizerinits
open-sourceLMLlama2-7B-chat(Touvronetal.,
defaultsettings. TheparameterαinEquation6was
2023)foritsleadingperformanceamongsimilar-
set to 0.5 to balance the impact of learning from
sizemodelsandactivecommunitytocompareour
mistakes. ThedatasetsweredownloadedfromHug-
work.
gingface,utilizingthestandardtrain/testsetsplit.
Baselines Todemonstratetheeffectivenessofour
All evaluation results are based on the zero-shot
method,wecompareitagainstthefollowingbase-
setting. Primary experiments were conducted on
lines: (1)TheteacherLLMsandstudentLMwith-
fourNvidiaA100-80GBGPUs. Additionalimple-
outfine-tuning,tohighlighttheimpactofdistilling
mentationdetailscanbefoundinAppendixA.
reasoning abilities from the teachers. (2) Sophis-
ticateddistillationmethodsappliedtoGPT-series 5 ResultsandAnalysis
and T5 open-source models with fewer parame-
TheevaluationresultsaredemonstratedinTable1.
ters(Shridharetal.,2022;Wangetal.,2023;Zhu
etal.,2024). (3)Threerelevantworksthatutilize
5.1 AdvantageofDistillation
LLMstoenhancereasoningcapabilitiesofLlama-
7B (Li et al., 2024) and Llama2-7B (Guo et al., TheinferenceresultsofthestudentLMshowedsig-
2024; Mitra et al., 2023), with a particular focus nificantimprovementafterapplyingknowledgedis-
on mathematical reasoning. (4) Two distillation tillation. Thetestaccuracyafterinstructiontuning
approaches that use T5-XXL as the student LM, onLlama2-7B-chathasimprovedfrom16.55%to
whichhasalargerparametersize(Fuetal.,2023; 36.24%onGSM8K,44.71%to59.50%onSVAMP,
Magister et al., 2022). (5) Finally, our methods 48.53%to67.67%onStrategyQA,and16.50%to
withindividual-LLMandmultiple-LLMs,demon- 36.27% on LogiQA. While there remains a no-
stratetheadvantageofmultiple-teacherdistillation ticeablegapbetweenthestudentLMandteacher
inimprovingthereasoningabilitycomparedtothe LLMs in mathematical reasoning, the fine-tuned
singleteacher. Llama2-7B-chatdemonstratedcomparableperfor-
manceinotherreasoningtasks. Consideringthatweusedonlyasubsetofthetrainingdatathatdidn’t 5.3 EffectivenessofDistillationviaMultiple
passtheexam,itisnotablethattheresultsstillout- TeacherLLMs
performedtheweakestLLMsincommonsenseand
logicalreasoningtasks,despitethestudentmodels As shown in Table 1, our multiple-teacher distil-
beingsignificantlysmallerinsize. Theexamresult lationwithpeer-reviewmethodimprovesaverage
on the original Llama2-7B-chat can be found in accuracyby6.16%acrossallfourbenchmarkscom-
Table2. paredtosingleteacherdistillationmethodsusing
Mistral, Gemini, and GPT respectively. This im-
Dataset #Wrong/#Train Accuracy provementindicatesthattherationaleflawspresent
GSM8K 6236/7473 16.55% in a single teacher’s output can be mitigated by
SVAMP 387/700 44.71% peer-review between multiple teachers. Conse-
StrategyQA 825/1603 48.53% quently,ourdistillationenablesthestudentmodel
LogiQA 6159/7376 16.50% tolearnfrommoreconvincingandsolidrationales,
leadingtoenhancedreasoningabilities.
Table2: ExamresultonoriginalLlama2-7B-chat.
Additionally, our findings reveal that different
teacherLLMspossessvaryinglevelsofrationale
capability,resultinginperformancedifferenceson
5.2 ComparisonofConcurrentMethods thesamebenchmark. Forinstance,GPT-3.5-turbo
demonstratessuperior78.01%accuracyonmath-
Table1presentstheresultsofourapproachalong-
ematicalreasoningcomparedtoMistral(74.40%)
side other relevant distillation methods. When
andGemini(76.42%),whileMistralexcelsincom-
comparedtomodelswithfewerparameters,such
monsensereasoningwith72.83%andGeminiper-
asGPT2-LargeandGPT-Jwithadvanceddistilla-
formsbetterinlogicalreasoningtaskswith40.55%.
tion techniques, our method consistently outper-
DetailedcomparisonsofthestudentLM’soutput
formsthem. However,thisimprovementcouldbe
before and after distillation are provided in Ap-
attributed to the enhanced reasoning capabilities
pendixD.
providedbythelargerparametersize. Toaddress
this, we also compared our approach with differ-
entdistillationmethodsbasedonthesamestudent 5.4 AnalysisaboutNecessityofPeer-Review
LM.FortheGSM8Kbenchmark,ourperformance
Toassesstheimportanceofthepeer-reviewprocess
(36.24%) lags behind Llama-7B+NCE (41.93%)
further, we compare the evaluation results with
andReversalMath(52.10%),likelybecausethese
andwithoutpeer-review,keepingallothersettings
modelswereexclusivelyfine-tunedonmathemat-
constant,asshowninTable3. Whenpeer-review
icaltasks,withGSM8Kbeingakeyanddifficult
isabsent,thetestaccuracyacrossallbenchmarks
benchmarkinthisdomain. Theothertrainedmath-
willdecreaseby7.84%onaverage. Itstrengthens
ematical datasets improved student LM’s overall
that answers generated by multiple teachers may
mathematicalreasoningcapability. Inaddition,we
contain varying rationales, potentially confusing
utilizedonlyasubsetofthetrainingdatathatdid
thestudentmodelduringinstructiontuning.
not pass the exam, which is significantly smaller
comparedtothetrainingdatausedinotherstudies. Inaddition,theexperimentswithoutpeer-review
Nevertheless,ourapproachstillyieldsbetterperfor- even fall behind the best single teacher-GPT dis-
manceonanothereasierandsmallermathematical tillationoutcomesonGSM8K(29.65%<30.71%).
benchmark,SVAMP(59.50%). Additionally,our Thispatternisparticularlypronouncedincommon-
superiorresultsonLogiQA(36.27%)comparedto sense and logical reasoning tasks, where the ab-
ORCA2-7B(35.02%)highlighttheeffectivenessof sence of peer-review leads to the poorest perfor-
ourpeer-revieweddistillationmethodinenhancing mance: 56.52% and 29.63% respectively. These
logicalreasoning. Finally,toassesstheimportance findingsalignwithourassumptionthatpeer-review
ofmodelsizeanddistillationmethod,wecompared may have a smaller impact on mathematical rea-
our approach with a larger model, T5-XXL. De- soning tasks, where the rationale and final result
spitehavingfewerparameters,ourwell-designed are highly correlated, but significantly improves
reasoningdistillationmethodenablesustoachieve thequalityofinstructiondataincommonsenseand
betterresultsthanthosebasedonT5-XXL. logicalreasoningtasks.TeacherLLM(s) Mathematical Commonsense Logical
GSM8K SVAMP StrategyQA LogiQA
Mistral 22.67 47.33 65.65 32.12
Gemini 26.84 49.33 57.93 32.73
GPT 30.71 51.67 60.12 31.04
Multiple(wP-R) 36.24 59.50 67.67 36.27
Multiple(w/oP-R) 29.65↓6.59 52.52↓6.98 56.52↓11.15 29.63↓6.64
Table3: Accuracy(%)acrossvariousreasoningtasks
withandwithoutpeer-reviewmethod. "P-R"standsfor
peer-review. Thebestperformancesaremarkedinbold.
5.5 AbalationStudyofLearningfrom
Mistakes
AsakeycomponentofourMAPDmethod,weini-
tiallysettheproportionoflearningfrommistakes
to 0.5 in previous experiments for simplicity. To
Figure5: TheeffectofαinEquation6onthetuning
explore the influence of balancing learning from
performance of student LM. α=0.00 indicates the ab-
goldrationalesandlearningfrommistakes,wead-
senceoflearningfrommistakes.
justedthevalueofαinEquation6acrossdifferent
extents of the two learning approaches. Specifi- teacherLLMs. Comprehensiveresultsfrommathe-
cally,αwasvariedfrom[0,0.25,0.5,0.75,1],and matical,commonsense,andlogicalreasoningtasks
experimentswereconductedonallbenchmarksfor highlightthesuccessoftheMAPDmethodinun-
5epochs,whilekeepingotherparametersconstant. locking the reasoning potential of smaller LMs.
Figure5visualizeshowlearningfrommistakesaf- We hope that our findings will encourage further
fectsinstruction-tuning. Ourfindingssupportthe investigationsintodistillationLLMs’reasoningca-
hypothesisthatlearningfrommistakespositively pabilities.
impactsinstructiontuning. However,therelation-
Limitations
ship is not uniformly positive across all α values
onthefourbenchmarks.
Although our method has demonstrated effec-
ForGSM8KandLogiQA,thebenefitsoflearn-
tiveness in the reasoning ability distillation from
ing from mistakes increase when α < 0.25, but
teachermodelstothestudentmodel,thistechnique
starttodecreasewhenαexceeds0.25. Conversely,
hasseverallimitations. First,ourexperimentspri-
for StrategyQA and SVAMP, the advantages of
marilyrelyonGPT-3.5-turbo,Gemini-1.0-pro,and
learningfrommistakesconsistentlygrowandreach
Mixtral-8x7B-Instruct-v0.1asteacherLLMsdue
theirpeakwhenα = 0.75. Theseresultssuggest
to considerations of availability and cost. Future
thatplacingtoomuchemphasisonlearningfrom
researchcouldbenefitfromusingmorepowerful
mistakes(i.e.,ahigherαvalue)canleadtodimin-
modelslikeGPT-4,OpenAIo1,andClaude-3Opus.
ishedperformanceandincreasedinstability. Con-
Additionally,weselectedLlama2-7Basthestudent
sequently,itisimportanttoevaluateandoptimize
LMforitstrainingavailabilityandtherobustopen-
theαvaluefordifferenttaskstoeffectivelybalance
sourcecommunitythatallowsustobenchmarkour
thelearningof"what"(correctanswers)and"why"
resultsagainstrelatedwork. Futurestudiesmight
(understandingmistakes)duringtraining.
exploremoreadvancedmodelslikeLlama3tofur-
thervalidatetheapproach. Secondly,duetotime
6 Conclusion
andcostconstraints, ourmethoddoesnotcollect
Inthispaper,weintroduceanoveldistillationap- the student LM’s incorrect rationales and update
proachcalledtheMistake-AwarePeer-ReviewDis- the instruction dataset after each epoch. The po-
tillation(MAPD)method. First, weimplementa tentialbenefitsofcontinuouslyincorporatingfresh
simulated peer-review process between multiple datathroughouttrainingremainunexplored. Lastly,
teacher LLMs to gather highly reliable and less we employed the default cross-entropy loss func-
biasedoutputs,whichrefinesthequalityofinstruc- tionforinstructiontuning. Itwouldbeworthwhile
tiontuningdataset. Additionally,wedevelopedan to explore more sophisticated methods, such as
integrated instruction tuning method that allows Reinforcement Learning with Human Feedback
the student LM to learn from both the gold ratio- (RLHF),andtointegrateadditionaltechniquesinto
naleandfeedbackonitsmistakesprovidedbythe thejointlearningframework.References Sai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini
Lourentzou,andFerasABatarseh.2023. Rational-
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama ization for explainable nlp: a survey. Frontiers in
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, ArtificialIntelligence,6:1225093.
DiogoAlmeida,JankoAltenschmidt,SamAltman,
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
NamgyuHo,LauraSchmid,andSe-YoungYun.2023.
arXivpreprintarXiv:2303.08774.
Large language models are reasoning teachers. In
Proceedings of the 61st Annual Meeting of the As-
LiChenglin,ChenQianglong,WangCaiyu,andZhang sociationforComputationalLinguistics(Volume1:
Yin. 2023. Mixed distillation helps smaller lan- LongPapers),pages14852–14882.
guage model better reasoning. arXiv preprint
arXiv:2312.10730.
JiaxinHuang,ShixiangShaneGu,LeHou,YuexinWu,
XuezhiWang,HongkunYu,andJiaweiHan.2022.
ZhengChu,JingchangChen,QianglongChen,Weijiang Large language models can self-improve. arXiv
Yu,TaoHe,HaotianWang,WeihuaPeng,MingLiu, preprintarXiv:2210.11610.
BingQin,andTingLiu.2023. Asurveyofchainof
thought reasoning: Advances, frontiers and future. ShimaImani,LiangDu,andHarshShrivastava.2023.
arXivpreprintarXiv:2309.15402. Mathprompter: Mathematicalreasoningusinglarge
languagemodels. arXivpreprintarXiv:2303.05398.
Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li,
and Yiqun Liu. 2024. Pre: A peer review based Albert Q Jiang, Alexandre Sablayrolles, Antoine
large language model evaluator. arXiv preprint Roux,ArthurMensch,BlancheSavary,ChrisBam-
arXiv:2401.15641. ford,DevendraSinghChaplot,DiegodelasCasas,
Emma Bou Hanna, Florian Bressand, et al. 2024.
HyungWonChung,LeHou,ShayneLongpre,Barret Mixtralofexperts. arXivpreprintarXiv:2401.04088.
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
Wang,MostafaDehghani,SiddharthaBrahma,etal. KathrynEKonold,SusanPMiller,andKyleBKonold.
2024. Scalinginstruction-finetunedlanguagemodels. 2004. Using teacher feedback to enhance student
JournalofMachineLearningResearch,25(70):1–53. learning. TeachingExceptionalChildren,36(6):64–
69.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
MarkChen,HeewooJun,LukaszKaiser,Matthias AitorLewkowycz,AndersAndreassen,DavidDohan,
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro EthanDyer,HenrykMichalewski,VinayRamasesh,
Nakano,etal.2021. Trainingverifierstosolvemath Ambrose Slone, Cem Anil, Imanol Schlag, Theo
wordproblems. arXivpreprintarXiv:2110.14168. Gutman-Solo,etal.2022. Solvingquantitativerea-
soningproblemswithlanguagemodels. Advances
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and inNeuralInformationProcessingSystems,35:3843–
Christopher Ré. 2022. Flashattention: Fast and 3857.
memory-efficientexactattentionwithio-awareness.
AdvancesinNeuralInformationProcessingSystems, YiweiLi,PeiwenYuan,ShaoxiongFeng,BoyuanPan,
35:16344–16359. Bin Sun, Xinglin Wang, Heda Wang, and Kan Li.
2024. Turningdustintogold: Distillingcomplexrea-
Xiaojing Fan and Chunliang Tao. 2024. Towards re- soningcapabilitiesfromllmsbyleveragingnegative
silient and efficient llms: A comparative study of data. InProceedingsoftheAAAIConferenceonAr-
efficiency,performance,andadversarialrobustness. tificialIntelligence,volume38,pages18591–18599.
arXivpreprintarXiv:2408.04585.
HanmengLiu,RuoxiNing,ZhiyangTeng,JianLiu,Qiji
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Zhou,andYueZhang.2023. Evaluatingthelogical
Tushar Khot. 2023. Specializing smaller language reasoningabilityofchatgptandgpt-4. arXivpreprint
models towards multi-step reasoning. In Inter- arXiv:2304.03439.
national Conference on Machine Learning, pages
10421–10430.PMLR. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang,
YileWang,andYueZhang.2020. Logiqa: Achal-
MorGeva,DanielKhashabi,EladSegal,TusharKhot, lenge dataset for machine reading comprehension
DanRoth,andJonathanBerant.2021. Didaristotle withlogicalreasoning.
usealaptop? aquestionansweringbenchmarkwith
implicitreasoningstrategies. ILoshchilov.2017. Decoupledweightdecayregulariza-
tion. arXivpreprintarXiv:1711.05101.
PeiGuo,WangjieYou,JuntaoLi,YanBowen,andMin
Zhang.2024. Exploringreversalmathematicalrea- Lucie Charlotte Magister, Jonathan Mallinson, Jakub
soningabilityforlargelanguagemodels. InFindings Adamek, Eric Malmi, and Aliaksei Severyn. 2022.
oftheAssociationforComputationalLinguisticsACL Teaching small language models to reason. arXiv
2024,pages13671–13685. preprintarXiv:2212.08410.ArindamMitra, LucianoDelCorro, ShwetiMahajan, Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
AndresCodas,ClarisseSimoes,SahajAgarwal,Xuxi Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
Chen, Anastasia Razdaibiedina, Erik Jones, Kriti drewMDai,andQuocVLe.2021. Finetunedlan-
Aggarwal, et al. 2023. Orca 2: Teaching small guagemodelsarezero-shotlearners. arXivpreprint
language models how to reason. arXiv preprint arXiv:2109.01652.
arXiv:2311.11045.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Linyong Nan, Ellen Zhang, Weijin Zou, Yilun Zhao, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Wenfei Zhou, and Arman Cohan. 2023. On eval- MaartenBosma,DennyZhou,DonaldMetzler,etal.
uating the integration of reasoning and action in 2022a. Emergentabilitiesoflargelanguagemodels.
llmagentswithdatabasequestionanswering. arXiv arXivpreprintarXiv:2206.07682.
preprintarXiv:2311.09721.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
Haowei Ni, Shuchen Meng, Xupeng Chen, Ziqing
etal.2022b. Chain-of-thoughtpromptingelicitsrea-
Zhao, Andi Chen, Panfeng Li, Shiyao Zhang,
soninginlargelanguagemodels. Advancesinneural
Qifu Yin, Yuanqing Wang, and Yuxi Chan. 2024.
informationprocessingsystems,35:24824–24837.
Harnessing earnings reports for stock predictions:
A qlora-enhanced llm approach. arXiv preprint
CanwenXu,DayaGuo,NanDuan,andJulianMcAuley.
arXiv:2408.06634.
2023a. Baize: An open-source chat model with
parameter-efficient tuning on self-chat data. arXiv
Kun-PengNing,ShuoYang,Yu-YangLiu,Jia-YuYao,
preprintarXiv:2304.01196.
Zhen-HuiLiu, YuWang, MingPang, andLiYuan.
2024. Peer-review-in-llms: Automatic evaluation
FangzhiXu,QikaLin,JiaweiHan,TianzheZhao,Jun
methodforllmsinopen-environment. arXivpreprint
Liu,andErikCambria.2023b. Arelargelanguage
arXiv:2402.01830.
modelsreallygoodlogicalreasoners? acomprehen-
siveevaluationfromdeductive,inductiveandabduc-
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. tiveviews. arXivpreprintarXiv:2306.09841.
2021. Are nlp models really able to solve
simple math word problems? arXiv preprint HanXu, JingyangYe, YutongLi, andHaipengChen.
arXiv:2103.07191. 2024a. Can speculative sampling accelerate react
without compromising reasoning quality? In The
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya SecondTinyPapersTrackatICLR2024.
Sachan. 2022. Distilling reasoning capabilities
into smaller language models. arXiv preprint Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen,
arXiv:2212.00193. ReynoldCheng,JinyangLi,CanXu,DachengTao,
and Tianyi Zhou. 2024b. A survey on knowledge
distillationoflargelanguagemodels. arXivpreprint
Gemini Team, Rohan Anil, Sebastian Borgeaud,
arXiv:2402.13116.
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
EricZelikman,YuhuaiWu,JesseMu,andNoahGood-
Anja Hauth, et al. 2023. Gemini: a family of
man.2022. STar: Bootstrappingreasoningwithrea-
highlycapablemultimodalmodels. arXivpreprint
soning. InAdvancesinNeuralInformationProcess-
arXiv:2312.11805.
ingSystems.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
Bhosale, et al. 2023. Llama 2: Open founda-
Anopenbilingualpre-trainedmodel. arXivpreprint
tion and fine-tuned chat models. arXiv preprint
arXiv:2210.02414.
arXiv:2307.09288.
ZiruiZhao,WeeSunLee,andDavidHsu.2024. Large
YizhongWang, YeganehKordi, SwaroopMishra, Al- language models as commonsense knowledge for
isaLiu,NoahASmith,DanielKhashabi,andHan- large-scaletaskplanning. AdvancesinNeuralInfor-
nanehHajishirzi.2022. Self-instruct: Aligninglan- mationProcessingSystems,36.
guagemodelswithself-generatedinstructions. arXiv
preprintarXiv:2212.10560. XuekaiZhu,BiqingQi,KaiyanZhang,XinweiLong,
ZhouhanLin,andBowenZhou.2024. Pad:Program-
Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jia- aideddistillationcanteachsmallmodelsreasoning
hai Wang, Minghui Song, Zihan Zhang, Haizhen betterthanchain-of-thoughtfine-tuning. InProceed-
Huang, Furu Wei, Weiwei Deng, Feng Sun, et al. ingsofthe2024ConferenceoftheNorthAmerican
2023. Democratizing reasoning ability: Tailored Chapter of the Association for Computational Lin-
learningfromlargelanguagemodel. arXivpreprint guistics: HumanLanguageTechnologies(Volume1:
arXiv:2310.13332. LongPapers),pages2571–2597.A ExperimentalSetupDetails Hyperparameter Value
Epoch 10
A.1 DatasetsStatistics
Barchsize 8
WedownloadeddatasetsGSM8K,SVAMP,Strat- Learningrate 1e-5
egyQA, and LogiQA from Huggingface. All Warmupratio 0.03
datasets are split according to the official origi- Maxseqlength 512
nalsplitratio. Thedatasetstatisticsareshownin Optimizer AdamW
Table4. Gradientaccumulationsteps 2
Maxgradnorm 0.3
Dataset Type #Train #Test
GSM8K Mathematical 7473 1319 Table7: StudentLMtraininghyperparametersettings.
SVAMP Mathematical 700 300
StrategyQA Commonsense 1603 687
B InstructionTuningTemplates
LogiQA Logical 7376 651
• Instructiontuningtemplatesforlearningfrom
Table4: Datasetstatistics.
mistakes.
– Forallbenchmarks:
A.2 TeacherLLMsParameters "### Instruction: Imagine you are a
teacher, I will give you one student’s
Table 5 shows the unified parameters setting for
incorrect answer to a question. You
GPT-3.5-turbo,Gemini-1.0-pro,andMixtral-8x7B-
should point out the mistakes in the
Instruct-v0.1 LLMs to generate answers for the
student’sanswer.
studentLM.AllinferencesfromteacherLLMsare
###Input: {}
acquiredbyAPIs.
###Response: {}"
Parameter Value
Temperature 0.8
• Instructiontuningtemplatesforlearningfrom
Maxtokens 512
goldrationale.
Topp 1
Presencepenalty 0 – ForbenchmarksGSM8KandSVAMP:
Frequencypenalty 0 "###Instruction: Answerthefollowing
question. Let’sthinkstepbystep.
Table5: TeacherLLMsparametersettings.
###Input: {}
###Response: {}"
A.3 StudentLMParameters
– ForbenchmarkstrategyQA:
ExperimentsareperformedwiththeHuggingface
"###Instruction: Answerthefollowing
TrainerframeworkandFlashAttention(Daoetal.,
question. Let’sthinkstepbystep. First,
2022). WeusefourNvidiaA100-80GBGPUswith
you should answer "true" or "false".
FP16 for training and evaluation. The inference
Then,youshouldexplainhowyoudraw
parameter settings across all datasets are shown
thisconclusion.
in Table 6. The adopted training hyperparameter
###Input: {}
settingsacrossalldatasetsareshowninTable7.
###Response: {}"
Parameter Value
Temperature 0.3 – ForbenchmarklogiQA:
Maxnewtokens 512 "###Instruction: Answerthefollowing
Topp 0.9 question based on the given context,
Topk 50 query, and options. Let’s think step by
Dosample True step.
###Input: {}
Table6: StudentLMinferenceparametersettings. ###Response: {}"C Peer-ReviewExamples
Table 8 provides detailed examples of the peer-
review process on GSM8K and StrategyQA. It
highlights instances where the causality between
the teacher LLM’s rationale and the final answer
maybeinsufficient,anddemonstrateshowourpeer-
reviewmechanismeffectivelyidentifiesthemost
confidentrationales.
D CaseStudyofDistillationImpacton
StudentLM’sOutput
Table9providesthecomparisonsofstudentLM’s
behaviordifferencebeforeandaftertheinstruction
tuningacrossfourbenchmarks.GSM8KExample
Question "Joshissavingupforaboxofcookies.Toraisethemoney,heisgoingtomakebraceletsandsellthem.
Itcosts$1forsuppliesforeachbraceletandhesellseachonefor$1.5. Ifhemakes12braceletsand
afterbuyingthecookiesstillhas$3,howmuchdidtheboxofcookiescost?"
Goldanswer "Hemakes$.5profitoneachbraceletbecause1.5-1=.5Heearns$6because12x.5=6Thecookies
cost$3because6-3=3####3"
GPT’sanswer "Let’sfirstcalculateJosh’sprofitperbracelet:Profitperbracelet=Sellingprice-CostpriceProfitper
bracelet=$1.5-$1Profitperbracelet=$0.5Now,let’scalculateJosh’stotalprofitfrommakingand
selling12bracelets:Totalprofit=Profitperbracelet*NumberofbraceletsTotalprofit=$0.5*12Total
profit=$6Weknowthatafterbuyingthecookies,Joshstillhas$3left.Thismeanshespent$6-$3=
$3ontheboxofcookies.Therefore,theboxofcookiescost$3."
Gemini’sscore:5,Mistral’sscore:5
Gemini’sanswer "15"
GPT’sscore:1,Mistral’sscore:2
Mistral’sanswer "Theboxofcookiescost$13.Here’sthereasoning:1.Joshspends$1onsuppliesforeachbracelet,and
hemakes12bracelets.So,thetotalcostofsuppliesis12×$1=$12.2.Hesellseachbraceletfor$1.5,
andhemakes12bracelets.So,thetotalrevenuefromsellingthebraceletsis12×$1.5=$18.3.After
buyingthecookies,hestillhas$3left. Thismeansthathistotalrevenue($18)minusthecostofthe
cookies($X)equalstheamountofmoneyhehasleft($3).Inotherwords,$18-$X=$3.4.Solvingfor
thecostofthecookies($X),weget$X=$18-$3=$15.However,theproblemstatesthatafterbuying
thecookiesandstillhaving$3,thecostofthecookiesshouldbe$13,not$15.Thismightbeamistake
inthequestionitself."
GPT’sscore:2,Gemini’sscore:2
Peer-reviewedanswers GPT’sanswer
StrategyQAExample
Question "Would2020ToyotaSupralagbehindataNascarrally?"
Goldanswer "True. The2020ToyotaSuprahasatopspeedof155MPH.Nascarstockcarsroutinelyexceed200
MPH."
GPT’sanswer "False.Explanation:1.The2020ToyotaSupraisnotacartypicallyusedinNASCARracing.NASCAR
racesprimarilyfeaturespeciallydesignedstockcarsthatmeetspecificperformanceandsafetyregulations.
2.The2020ToyotaSupraisaproductionsportscardesignedforroaduse,notforNASCARtrackracing.
Whileitmayhaveimpressiveperformancecapabilitiesontheroad,itisnotoptimizedforNASCAR
ovaltrackracingconditions. 3. NASCARracesinvolvehighspeeds,closecompetition,andspecific
handlingcharacteristicsthataredifferentfromwhatthe2020ToyotaSupraisdesignedfor.TheSupra
wouldlikelystruggletokeepupwithdedicatedNASCARracecarsinaNASCARrallyduetothese
differencesindesignandperformancecharacteristics."
Gemini’sscore:4,Mistral’sscore:2
Gemini’sanswer "True.The2020ToyotaSupraisasportscar,whileNASCARvehiclesarepurpose-builtracecars.The
SuprawouldnotbecompetitiveinaNASCARracebecauseitisnotdesignedforthattypeofracing.
NASCARvehiclesaremuchmorepowerfulandhaveamuchbettersuspensionsystemthantheSupra.
TheSuprawouldalsobeatadisadvantagebecauseitisnotasaerodynamicasaNASCARvehicle."
GPT’sscore:4,Mistral’sscore:5
Mistral’sanswer "True. NASCARracecarsaredesignedforhigh-speedovaltrackracing,withpowerfulV8engines,
aerodynamicbodies,andspecializedsuspensionsystems.TheSupra,ontheotherhand,isastreet-legal
sportscar,withaturbochargedinline-sixengineandamoreversatilesuspensionsetup. IftheSupra
rebuildstheengineandimproveothersystems,itislikelysuitedfortheuniquedemandsofNASCAR
racingandkeepupwithNASCARvehicles."
GPT’sscore:2,Gemini’sscore:3
Peer-reviewedanswers Gemini’sanswer
Table8: Detailedexamplesofpeer-reviewprocessondifferentbenchmarksGSM8KExample
Question Wengearns$12anhourforbabysitting.Yesterday,shejustdid50minutesofbabysitting.How
muchdidsheearn?Goldanswer:$10
Student’soutputbeforetuning Answer:$18.Explanation:Wengearns$12perhourforbabysitting.Sinceshedid50minutesof
babysitting,sheearned$12x0.75=$9per50minutes.So,sheearned$9x2=$18.
Teachers’correctrationale Wengearned$10for50minutesofbabysitting.Here’sthelogicbehindthis:1.Weknowthat
Wengearns$12perhour. 2. Tofindouthowmuchsheearnsin50minutes,wefirstneedto
convertthetimeintohours.3.Weknowthat1hourequals60minutes,so50minutesisequalto
50/60hours,whichsimplifiesto0.833hours.4.Now,wecancalculatehowmuchWengearns
for0.833hoursbymultiplyingherhourlyratebythenumberofhours:$12x0.833=$10.So,
Wengearned$10for50minutesofbabysitting.
Teachers’mistakesfeedback Thestudent’sanswercontainsamistakeincalculatinghowmuchWengearnedfor50minutes
ofbabysitting.Theerrorisinmultiplyingthehourlyrateby0.75togettheamountearnedin50
minutes.Instead,thecorrectapproachwouldbetocalculatetherateperminutebydividingthe
hourlyrateby60,andthenmultiplyingthatratebythenumberofminutesworked.Thecorrect
answeris$10,not$6.
Student’soutputaftertuning Wengearns$12perhour,sosheearns$12/60=$0.20perminute.Sinceshedid50minutesof
babysitting,sheearned$0.20x50=$10.
SVAMPExample
Question Paulgotaboxof110crayonsforhisbirthday.Duringtheschoolyearhegave90crayonstohis
friendswhilehelost412crayons.Howmanymorecrayonsdidhelosethanthosehegavetohis
friends?Goldanswer:322
Student’soutputbeforetuning Answer:392.Explanation:Paulgave90crayonstohisfriends,sohehad110-90=20crayons
left.Sohelost412-20=392.
Teachers’correctrationale Paullost412crayonsandgaveaway90crayons.Tofindouthowmanymorecrayonshelost
thanthosehegavetohisfriends,wecansubtractthenumberofcrayonshegaveawayfromthe
numberofcrayonshelost:412-90=322Therefore,Paullost322morecrayonsthanthosehe
gavetohisfriends.
Teachers’mistakesfeedback The mistakes in the student’s answer are as follows: The student calculated the number of
crayonsPaullostbysubtractingtheremainingcrayons(20)fromthetotalnumberofcrayonshe
lost(412),whichisincorrect.Thecorrectcalculationshouldbe412-90=322.Therefore,the
correctanswershouldbe412-90=322,indicatingthatPaullost322morecrayonsthanhegave
tohisfriends.
Student’soutputaftertuning Paullost412crayons-90crayons=322crayonsmorethanthosehegavetohisfriends.
StrategyQAExample
Question WouldsomeoneinMumbairefertoSolanummelongenaasaneggplant?Goldanswer:False
Student’soutputbeforetuning True:InMumbai,Solanummelongenaiscommonlyreferredtoasaneggplant.
Teachers’correctrationale False.InMumbai,peoplewouldmorecommonlyreferto"Solanummelongena"as"brinjal"or
"baingan"ratherthan"eggplant."While"eggplant"iswidelyusedinAmericanEnglish,inIndia,
especiallyinregionslikeMumbai,theterm"brinjal"ismorecommon. "Baingan"isaHindi
wordusedacrossmanypartsofIndia.
Teachers’mistakesfeedback Thestudent’sanswercontainsamistakeintheexplanationprovided.Thecorrectanswertothe
questionisFalse,butthestudentmistakenlyprovidedahintthatsupportsthecorrectanswer.
ThecorrectinformationshouldbethatinMumbai,Solanummelongenaiscommonlyreferredto
asabrinjal,notaneggplant.
Student’soutputaftertuning Answer: False. Solanummelongenaisnotreferredtoas"eggplant"inMumbai. Theterm
"eggplant" is specifically used in American English and Solanum melongena is known as
"brinjal"inMumbai.So,theanswerisfalse.
LogiQAExample
Question SomeCantonesedon’tlikechili,sosomesouthernersdon’tlikechili.Whichofthefollowing
canguaranteetheaboveargument?Options:["SomeCantoneselovechili.","Somepeoplewho
likepeppersaresoutherners.","AllCantonesearesoutherners.","SomeCantoneselikeneither
peppersnorsweets."].Goldanswer:"AllCantonesearesoutherners."
Student’soutputbeforetuning Answer:["SomeCantoneselovechili."]Rationale:Theargumentstatesthat"SomeCantonese
don’t like chili," which implies that not all Cantonese people dislike chili. Therefore, the
statement"SomeCantoneselovechili"istheonlyoptionthatcanguaranteetheargument.
Teachers’correctrationale Thecorrectoptionis:"AllCantonesearesoutherners."Explanation:Theargumentstatesthat
someCantonesedon’tlikechili,andtoconcludefromthisthatsomesouthernersdon’tlikechili,
weneedtoensurethatCantoneseareincludedwithinthebroadercategoryofsoutherners.Ifall
Cantonesearesoutherners,thenthestatementabout"someCantonesenotlikingchili"canbe
appliedtosouthernersaswell,makingtheargumentvalid.
Teachers’mistakesfeedback Thestudent’sanswerisincorrect.Thestatement"SomeCantoneselovechili"doesnotguarantee
theargumentabout"southerners"providedinthequestion.Thecorrectanswershouldbe"All
Cantonese are southerners." This option directly connects the dislike for chili among some
Cantonesetothefactthatsomesouthernersdon’tlikechili,makingitthemostrelevantchoiceto
supporttheargument.
Student’soutputaftertuning Answer:["AllCantonesearesoutherners."]Explanation:Theargumentstates:"SomeCantonese
don’tlikechili,sosomesouthernersdon’tlikechili."Toguaranteethisargument,weneedto
establishthatsomesoutherners(specificallyCantonese)don’tlikechili.
Table9: CasestudyofdistillationimpactonstudentLM’soutput