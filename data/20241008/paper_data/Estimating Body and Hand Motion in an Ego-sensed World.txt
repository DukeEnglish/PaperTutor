Estimating Body and Hand Motion in an Ego-sensed World
BrentYi1 VickieYe1 MayaZheng1 LeaMu¨ller1
GeorgiosPavlakos2 YiMa1 JitendraMalik1 AngjooKanazawa1
1UCBerkeley 2UTAustin
Egocentric Inputs Body Pose, Height, and Hands Example Results
Figure1. EgoAllo.Wepresentasystemthatestimateshumanbodypose,height,andhandparametersfromegocentricSLAMposesandimages.
Outputscapturethewearer’sactionsintheallocentricreferenceframeofthescene,whichwevisualizeherewith3Dreconstructions.
Abstract andassistivetechnologies.Forresearchinmachineperception,
these devices also present exciting opportunities in the form
WepresentEgoAllo,asystemforhumanmotionestimation ofegocentricsensing.Sensorsfromwearabledevicesprovide
from a head-mounted device. Using only egocentric SLAM abundant observations of 3D environments, while capturing
posesandimages,EgoAlloguidessamplingfromaconditional the embodied perspective of human agents as they observe,
diffusionmodeltoestimate3Dbodypose, height, andhand navigate,andinteractwiththeworldaroundthem.
parametersthatcaptureadevicewearer’sactionsintheallo- Whatcanweunderstandfromthesedevicesastheiradop-
centriccoordinateframeofthescene.Toachievethis,ourkey tionwidens?Asastartingpoint,parallax-inducingegomotion
insightisinrepresentation: weproposespatialandtemporal providesexcellentconditionsforadvancesin3Dreconstruction
invariance criteria for improving model performance, from andsceneunderstanding[18,66,108].Limitingperceptionto
whichwederiveaheadmotionconditioningparameterization onlythesurroundingworld,however,wouldneglectacrucial
that improves estimation by up to 18%. We also show how pieceoftheego-sensorypuzzle:theindividualwhosedecisions
the bodies estimated by our system can improve the hands: shapetheinputs.Capturingthewearer’sactionsandmotionin
theresultingkinematicandtemporalconstraintscanreduce additiontothescenepromisestounlockapplicationsacrossaug-
errors in noisy monocular estimates by 40%. Project page: mentedandvirtualreality,robotics,andgeneralhumanbehavior
https://egoallo.github.io analysis,whileunveilingaction-tiedsemanticsinthesceneitself.
We therefore introduce EgoAllo, a system that uses ego-
centricinputstoestimatethewearer’sactionsintheallocentric
1.Introduction world. This is a difficult task: while body parts like hands
occasionallyappearinegocentricimageframes,mostbodypa-
Head-mounteddevicesarepermeatingthemainstreammarket, rametersareneverdirectlyobservedbythesedevices.Accurate
withapplicationsinareaslikevirtualreality,contentcreation, estimates in the global frame also require harmony between
4202
tcO
4
]VC.sc[
1v56630.0142:viXraboth pose and height parameters. These must be consistent of-Gaussiansto3Dkeypointtrajectories[25], whilemodern
withboththeegomotionandscenescale,aligningthefeetto approachesincludetrainingvariationalautoencoders[37,73]
thegroundandtheheadtothesensedheightofthecamera. tomodeleitherautoregressivetransitions[15,50,72]orfull
We cast estimation as guided sampling from a diffusion- spatiotemporal sequences [22]. After training, these priors
basedmotionmodel.Weconsidertwoegocentricinputs:apro- canbeappliedtoestimationproblemsiniterativeoptimization
prioceptiveoneintheformofheadpose,whichweuseforcondi- frameworks[40,72,99].EgoAlloisbuiltonthesameintuition
tioning,andimages,whichweuseforvisualhandobservations as these methods, but follows previous work in ego-sensed
thatareincorporatedviaguidance.Wethenestimatebodypose, motionestimationandusesatask-specificconditionalprior.
height,andhandparameters,unlikemostpriorworksinegocen-
Denoisingdiffusionforhumanmotion.ThecoreofEgoAllo
trichumanmotionestimation[6,47]thatfocusonbodypose.
is a denoising diffusion model [23, 67, 82] from which we
We achieve this by training a head motion-conditioned
can sample 4D human body motion. While diffusion mod-
diffusion model as a motion prior, and guiding its sampling
elsareprimarilyknownfortheirsuccessintext-conditioned
to recover a hand-body sequence that aligns with image
image generation [76, 78], they have also enabled advances
observations.Ourresultsareenabledbyakeyinsight:thehead
in human motion synthesis conditioned on modalities like
observationconditioningrepresentationiscriticalforaccurate
text[35,36,109],music[1,91],poses[35,47],andobjectgeom-
ego-sensedposeestimation.Westudytheroleofchoicesforthis
etry[42,46,48].EgoAlloadoptsasimilarconditionaldiffusion
representationby(1)identifyingdesirablespatialandtemporal
approach,whilespecificallystudyingthedesignofcondition-
invariancepropertiesthatarenotfulfilledbyexistingsystems,
ingparametersusedforego-sensedhumanmotionestimation.
(2)usingthesepropertiestoderiveimprovedparameterizations
Theiterativenatureofdenoisingdiffusionalsoenablesguid-
forourmotionprior,and(3)presentingasystematicevaluation
ance[11,13,30,35,85,111],wheredenoisingstepsaresteered
that shows between 4.9% and 17.9% error difference when
tosatisfyadesiredobjective. Weuseguidancetoincorporate
comparedagainstpriorwork.Furthermore,weshowhowthe
observationslikevisualhandposeobservationsduringtest-time.
resultingsystemcanimprovehandestimation,reducingerrors
byover40%comparedtonoisymonocularestimates. Human motion from egocentric observations. EgoAllo
builds on intuition from several prior works in egocentric
2.RelatedWork sensing for human motion estimation. Many rely on fish-
eye cameras that place the wearer’s body into the field of
3Dhumanrecoveryfromexternalvisualinputs.Alargebody view[27,74,89,89,90,93,94,96]. Otherapproachesrely
ofworkhasaddressedestimatingtheparametersofhumanbody on body-mounted cameras [81], simulation-based physical
modelslikeSCAPE[2]orSMPLanditsvariants [51,61,77] plausibility [53, 104, 105], body- and hand-mounted inertial
from third-person visual inputs, where human subjects are sensors [45, 102, 103], handheld controllers [6, 28, 29], and
observedfromtheviewofoutsidecameras. Themajorityof interaction cues from other humans [57]. Concurrent works
theseworksfocusonextracting3Drepresentationsfromsingle havealsousedtheNymeria[54]datasetforegocentricmotion
images, for example by lifting 2D keypoint observations to with language description outputs [24], as well for online
3D[56],viaend-to-endregression[20,31,33,39,58,60,75], settingswithscenegeometryandCLIP[68]featureinputs[21].
via optimization [19, 43, 61], or by exploiting synergies Mostrelevantly,EgoEgo[47]demonstrateshowhumanbody
between regression and optimization [41]. When multiple poses can be estimated offline without body observability
framesareavailableintheformofavideo,temporalcontextand assumptions.Theauthorsaccomplishthisbycarefullyintegrat-
trackingcanalsobeincorporated[16,34,38,62,63,69,106]. ingseveralcomponents: amonocularSLAMsystem[87], a
Theinputs(images)andoutputs(humanmeshes)ofmanyof pose-conditionedgravityvectorregressionnetwork,anoptical
thesesystemsaresuperficiallysimilartotheegocentricsetting flowfeature-conditionedheadorientationandscaleregression
addressedbyEgoAllo,butegocentricdevicespresentunique network,andaheadpose-conditionedbodydiffusionmodel.
challengesbecausethebodybeingestimatedistypicallybehind Forestimatesthatcanbemoreeasilygroundedintheallocentric
theoutwards-facingcamerasusedasinput. coordinate frame, EgoAllo differs in both inputs—we study
conditioning parameters computed from the metric SLAM
Priors for human motion. The primary challenge of ego-
poses provided by devices like Project Aria [83]—and
sensed human motion estimation is limited observability; a
outputs—weconsiderbodyheightvariationandhandposes.
prior is required to resolve ambiguities. For human motion,
thesepriorsaretypicallyframedasunconditionaldistributions Conditioning for ego-sensed poses. Prior works vary in
over plausible human motions. These distributions can be how head pose information is parameterized and used as
represented either by modeling the physical constraints of neuralnetworkinput. AvatarPoser[28]andBoDiffusion[6]
ourworld[5,49,65,71]orbylearninggenerativemodelsof parameterize head pose as four components: world-frame
humanmotiondirectlyfromdata. Forlearningunconditional orientation, orientation deltas, world-frame position, and
priors,classicaldata-drivenapproachesincludefittingmixtures- world-frame position deltas. These works are focused onSLAM poses
Global alignment
Invariant
 Local body

conditioning sampling
Estimated motion
Denoising
Egocentric video
floor height
Hand

estimator
HaMeR
Constraints
Figure2.OverviewofcomponentsinEgoAllo.Werestrictthediffusionmodeltolocalbodyparameters{Θt,βt},{ψt} (Section3.1.1).An
t j t,j
invariantparameterization(Section3.1.2)ofSLAMposesisusedtoconditionadiffusionmodel.Thesecanbeplacedintotheglobalcoordinate
frameviaglobalalignment(Section3.2.1)toinputposes.Whenavailable,egocentricvideoisusedforhanddetectionviaHaMeR[64],which
canbeincorporatedintosamplesviaguidance(Section3.2.2).
settingswithVRcontrollerinput,andparameterizecontroller 3.1.Ego-conditionedmotiondiffusion
pose inputs the same way. EgoEgo [47]’s diffusion model
Notation:weuseT =(R ,p )todenoteanSE(3)trans-
usesonlyabsoluteheadpositionsandorientations,but,similar A,B A,B A,B
formtoframeAfromframeB,composedofrotation(R )
to HuMoR [72], in implementation defines a per-sequence A,B
andposition(p )terms. Temporalstepstaresuperscripted
canonicalcoordinateframetoensurethatallinputtrajectories A,B
anddiffusionnoisestepsnaresubscripted. ⃗xt thusrefersto
passedtothemodelarealignedwiththesameinitialxyposition 0
thet-thtimestepofaclean(n=0)humanmotionsequence.
and forward direction. In our work, we will refer to this as
Given an observation window of T timesteps, EgoAllo’s
sequence canonicalization. Finally, EgoPoser [29] proposes
motion prior is a diffusion model that aims to capture the
a similar scheme that aligns initial positions for both head
distributionofhumanmotions⃗x ={⃗x1,...,⃗xT}conditioned
poseandcontrollerposeinputs. Weproposeanalternativeto 0 0 0
onheadposeencodings⃗c={⃗c1,...,⃗cT}. Foreachtimestept,
theseparameterizationsthatismotivatedbytherobustnessand
werepresenthumanmotionintheformofSMPL-H[51,77]
generalizationbenefitsofinvariance,asobservedinpriorwork
model parameters {Tt , Θt, β}: root transforms
fordesigningbothrepresentations[9,52,79,95,101,112]and world,root
Tt ∈SE(3),wheretheperson’srootframeislocatedat
neuralnetworkarchitectures[7,8,12,14,32,44,88,98,107]. world,root
Specifically,weintroduceinSection3.1.2aparameterization theirpelvis, axis-anglelocaljointrotationsΘt∈R51×3, and
thatisinvarianttobothspatialandtemporalshifts.
time-invariantshapeβ∈R16.
Dependencies between local joint rotations, body size
variation, and global motion make this learning task a
challenging one. Our key insight is that this difficulty can
3.Method be reduced by designing parameterizations with desirable
invarianceproperties. Spatialandtemporalinvariancesallow
themodeltofocusontheessentialstructureofmotion,without
Westudytheproblemofusingsensorsfromanegocentricdevice
beingaffectedbyirrelevantshiftsinpositionortime.
toestimatetheactionsofawearerinanallocentriccoordinate
frame.Weassumeaflatfloorandtwoegocentricinputs—poses
fromthedevice’sSLAMsystemandcameraimages. 3.1.1 Diffusionoutputrepresentation
Our system uses head pose information to condition a Asoutput,wesamplejointrotations,bodyshapes,andbinary
diffusion-basedprioroverbodyposeandheight,andincorpo- contact predictions ⃗xt = {Θt,βt,ψt }, where body
0 j=1...21
ratesvisualhandobservationsduringsampling. Thisallows shape βt is supervised to be equal for all timesteps and ψt
j
ittobenefitfromboth3Dhumanmotioncapturedatasets[55], is a per-joint contact indicator. Notably, these parameters
whichareusedforthemotionprior,andfromlarge-scaleimage arealllocal—wediscusshowoutputscanbeplacedintothe
datasets[64],whichareusedforhandestimates. allocentriccoordinateframeinSection3.2.1.Wechoosethisoutputsetforthreemainreasons.(1)Body CPF
shape encodes the wearer’s height, which is critical for
groundinginthemetric-scalegeometryofthescene. Thisis
rarely considered by prior work: with the exception of [29],
which is focused on tracking with controller input, existing
Projection
methods[6,28,47]otherwiseproduceoutputsusingafixed
“mean” human shape. (2) Contact predictions enable losses
forcommonproblemslikefootskating,whicharediscussed
inSection3.2.2. (3)Finally,localbodiesareinvarianttothe
globalcoordinateframe.Aswediscussnext,theconditioning
Canonical
parameterizationforthemodelcanthereforealsobeinvariant
toarbitrarytransformationsalongthefloorplane. World
3.1.2 Invariantconditioning
Figure3. Locallycanonicalizedcoordinateframes.Wecompute
The goal of our conditioning representation is to map raw
our invariant conditioning parameterization (Equation 4) using
SLAM poses (head motion) to a parameterization that is
transformationscomputedfromthreecoordinateframes. Following
amenabletolearningforthediffusionmodel.
[83],theCPFhasthez-axisforward. FollowingHuMoR[72],the
Raw inputs. To capture the head motion at each time step, worldandcanonicalz-axespointup.Canonicalframesarecomputed
we assume as input poses of a central pupil frame (CPF), byprojectingtheCPFframeorigintothegroundplane,thenaligning
thecanonicaly-axistotheCPFforwarddirection.
which the SLAM systems of devices like Project Aria can
providewithmillimeter-levelaccuracy[83].Fortime1...T,we
Noparameterizationusedbyexistingworksatisifiesbothof
reparameterizetheseposesforconditioningusingafunctiong:
these properties. The sequence canonicalization approach
Tt =(Rt ,pt )∈SE(3), (1) of EgoEgo [47] achieves spatial invariance (Invariance 1),
world,cpf world,cpf world,cpf
butinsertsasequence-widedependencyonthefirsttimestep
{⃗c1,...,⃗cT}=g({T1 ,...,TT }). (2)
world,cpf world,cpf of each window that results in a violation of Invariance 2.
The absolute poses and pose deltas used by [6, 28] satisfy
TheCPFframediffersfrompriorworksthatconditionona
Invariance2,butnotInvariance1.Finally,therelativepositions
coordinateframeattachedtotheSMPLhumanmodel’s“head
consideredby[29]areneitherspatiallynortemporallyinvariant.
joint”[6,28,29,47]. Theoffsetbetweenthisheadjointand
thedeviceposedependsontheheadshapecapturedbyβt,and
Invariantconditioning.Weproposeaformulationforgthat
isthusdifficulttoprecomputeinoursetting. achievesbothinvariancepropertiesbylocallycanonicalizing
To encode absolute height, we assume that the world headmotionwithrespecttothefloorateachtimestep.Webuild
frame’s+z-axisfacesupwards,andthatthegroundislocatedat ontherelativemotionoftheCPFframeateachtimet,which
z=0.Groundparametersaredirectlyavailableinthetraining respectsbothInvariance1and2:
data [55]; at test time, we can also extract these parameters
fromsparseSLAMpointsviaRANSAC(AppendixA.1). ∆Tt c− pf1,t=(Tt w− or1 ld,cpf)−1Tt world,cpf. (3)
Invariancegoals.AsdiscussedinSection2,priorworkvaries Importantly,thetranslationcomponentofthistransformation
in how the function g is implemented. To understand how isinthelocalframe.Thisisdistinctfromworld-frameposition
theyimpactthelearningproblem,weproposetwoinvariance deltas[6,28,29],whichstillviolateInvariance1.
properties for head motion representations. Each reduces Relativetransformsalone,however,donotencodeinforma-
representationalredundancy,whicheasesthelearningproblem. tionrelativetothesceneorfloor:fulltrajectoriescanevenbe
flippedupsidedownwithoutimpacting∆Tt−1,t.Wetherefore
Invariance1(Spatial) Global transformations along the cpf
propose to ground relative motion to the floor plane with a
floorplaneshouldnotaffectaperson’slocalmotion. Given
transformation between the CPF frame and a per-timestep
T ∈ SE(3) restricted to the XY plane, g should fulfill
xy
canonical frame, which is computed by projecting the CPF
g({T Tt } )=g({Tt } )∀T .
xy world,cpf t world,cpf t xy frametothefloor.Thisencodesheadheightandorientation:
Invariance2(Temporal) Headmotionrepresentationsfora
(cid:110) (cid:111)
givenbodymotionshouldbeindependentoflocationwithin ⃗ct= ∆Tt−1,t, (Tt )−1Tt . (4)
cpf world,canonical world,cpf
atemporalwindow. Thiscanbeexpressedastemporalshift (cid:124) (cid:123)(cid:122) (cid:125)
equivariance.Let⃗ctbeasdefinedinEquation2.Ifweshiftin- Invariantimplementationofg(·)
putsbyδsuchthat{⃗c1 ,...,⃗cT }=g({T1+δ ,...,TT+δ }), We visualize an example of a canonical frame in Figure 3.
shift shift world,cpf world,cpf
thengshouldsatisfy⃗ct =⃗ct+δforoverlappingtimesteps. CanonicalframesarepositionedbyprojectingtheCPForigin
shifttothefloorplane;givenstandardbasese ,wecompute: 3.2.2 Guidancelosses
{x,y,z}
Our diffusion model learns a distribution of human motion
pt =(cid:2) e e ⃗0(cid:3)⊤ pt . (5)
world,canonical x y world,cpf conditionedonthecentralpupilframemotion.Attesttime,we
incorporate constraints from physical priors and visual hand
For orientation, we align the canonical frame’s local z-axis
observationsviaguidance[11,30,111]. Similarto[35,46],
paralleltotheworldz-axisanditslocaly-axistowardthe“for-
we accomplish this by applying losses to the joint rotations
ward”direction⃗vtoftheCPFframe.WithR (·):R→SO(3)
z Θ={Θ1,...,ΘT}predictedbyµ (⃗x ,n,⃗c). Wetreatthebody
constructing a z-axis rotation and e again as standard θ n
{x,y,z} shape βt and contacts ψt as fixed and optimize over
bases,wecomputethisas: j=1...21
bodyandfingerposetominimizehandobservation,skating,
⃗vt=Rt e , (6) andpriorcostswithaLevenberg-Marquardtoptimizer:
world,cpf z
Rt =R (cid:0) −arctan2(cid:0) e⊤⃗vt,e⊤⃗vt(cid:1)(cid:1) Rt . (7) E(Θ) =E(Θ) +E(Θ)+E(Θ). (11)
world,canonical z x y world,cpf guidance hands skate prior
We begin by running HaMeR on the egocentric image
Thiscanonicalframedefinitionisanimportantdeparturefrom
correspondingtoeachtimestept.Whendetected,thisproduces
priorwork. WhileEgoEgo[47]andHuMoR[72]usesimilar
3DhandestimatesintheformofMANO[77]jointparameters
canonicalframes,theyonlycomputeonepersequence.Instead,
andcamera-centric3Dhandkeypointsˆtt forhandjoint
wecomputeEquations5and7ateverytimestep.Thisenables camera,j
setj∈H.Witheachsubcriptedλindicatingascalarweighting
floorplanegroundingwithoutsacrificingInvariance2.
term,wehave:
3.2.Estimationviasampling E(Θ) =λ E(Θ) +λ E(Θ) . (12)
hands hands3D hands3D reproj reproj
Weuseourlocalbodyrepresentationandinvariantconditioning
The 3D objective E(Θ) minimizes the distance between
strategies to train a motion prior in the form a denoising hands3D
thedetectedMANOhandparametersandthecorresponding
diffusion model [23]. Given diffusion step n = N...1, we
SMPL-H hand parameters, in terms of wrist pose and local
follow[70]andapproximatethedenoisingprocessas:
jointrotations. WithΠ asprojectionwithcameraintrinsics
K
p (⃗x |⃗x ,⃗c)=N(µ (⃗x ,n,⃗c),σ2I), (8) K,p(Θt) ∈R3astheworldpositionforjointjattimet,and
θ n−1 n θ n n world,j
T fromthedevicecalibration,thereprojectionlossis:
camera,cpf
whereatransformer[92]µ istrainedtopredicttheposterior
mean from ⃗x and conditθ ioning ⃗c. With noise-dependent E(Θ) = (cid:88) ||Π (p(Θt) )−Π (pˆt )||2, (13)
n reproj K camera,j K camera,j 2
weighttermw n,thelosscanbewrittenas: t,j∈H
minE E (cid:2) w ∥µ (⃗x ,n,⃗c)−⃗x ∥2(cid:3) . (9) p( cΘ amt e) ra,j=T camera,cpf(Tt world,cpf)−1p( wΘ ort l) d,j. (14)
xˆ0
⃗x0∼q0 n∼U,⃗xn∼qn|0 n θ n 0
Toreducefootskating,weusecontactpredictionstoapplya
After training, we estimate human motions by following skatingloss[72,99]foreachtimetandjointj:
D saD mI pM ling[8 p4 r] ocf eo dr uc reon ind cit li uo dn ea sl ss ea vm erp al lin adg d. itioT nh ae lcf oin mal poE ng eo nA tsl :lo a E s( kΘ at) e=(cid:88) λ skate||1 2(ψ jt+ψ jt−1)(pt world,j−pt w− o1 rld,j)||2 2. (15)
globalalignmentphase,guidancelossesforphysicalconstraints t,j
andvisualhandobservations,andapathfusion[3]approach Finally,weminimizeapriortermE(Θ). Thistermpenalizes
forlongersequencelengths.Wedescribethesebelow. prior
deviationsbetweeneachconstrainedrotationmatrixΘtandthe
originaloutputrotationsΘˆtfromourdenoiserµ (⃗x ,n,⃗c),in
θ n
3.2.1 Globalalignment termsofbothjointrotationandrotationalvelocity.
Toplacesampledbodiesintotheallocentriccoordinatesystem,
wecomputetheabsoluteposeoftheSMPL-Hrootas: 3.2.3 Sequencelengthextrapolation
For longer sequences at test time, we draw on existing
Tt =Tt T(Θt,βt), (10)
world,root world,cpf cpf,root methodsincompositionalgenerationforbothimage[3,110]
and human motion [4, 80] diffusion models. We train our
whereT(Θt,βt)computesthetransformbetweentherootofthe
motion prior using subsequences of up to length 128; when
cpf,root
humanandtheirCPFframeforagivensetoflocalposeand input observations exceed this length at test time, we split
shapeparameters.Similarprocessesareappliedin[6,28,29]. intowindowswitha32-timestepoverlapbetweenneighbors.
Incontrasttodirectlyoutputtingabsolutebodytransformations We then run our model µ (⃗x ,⃗c,n) on windows in parallel.
θ n
fromthediffusionmodel[47],thisguaranteesexactalignment Diffusion paths for overlapping regions are fused following
betweenestimatesandtheinputSLAMsequences. MultiDiffusion[3]aftereachdenoisingstep.Conditioning Seqlen Invariance1/2 MPJPE↓ %Diff PA-MPJPE↓ %Diff GND↑
EgoAllo (Eq.4) 32 ✔ / ✔ 129.8±1.1 — 109.8±1.1 — 0.98±0.00
Absolute+Local Relative 32 P / ✔ 133.0±1.1 2.4% 113.6±1.2 3.4% 0.95±0.00
Absolute+Global Deltas [6,28] 32 ✗ / ✔ 136.2±1.1 4.9% 118.3±1.2 7.7% 0.93±0.01
Sequence Canonicalization [47] 32 ✔ / ✗ 153.1±1.5 17.9% 128.7±1.5 17.1% 0.76±0.01
Absolute 32 ✗ / ✔ 159.9±1.2 23.2% 141.0±1.3 28.4% 0.89±0.01
EgoAllo (Eq.4) 128 ✔ / ✔ 119.7±1.3 — 101.1±1.3 — 1.00±0.00
Absolute+Local Relative 128 P / ✔ 124.5±1.3 4.0% 104.9±1.4 3.8% 1.00±0.00
Absolute+Global Deltas [6,28] 128 ✗ / ✔ 127.4±1.3 6.4% 109.8±1.4 8.6% 0.99±0.00
Sequence Canonicalization [47] 128 ✔ / ✗ 134.0±1.8 11.9% 112.1±1.6 10.9% 0.88±0.02
Absolute 128 ✗ / ✔ 148.3±1.5 23.9% 131.2±1.6 29.8% 0.96±0.01
Table1.Motionpriorconditioningcomparison.Wetrainandevaluateotherwiseidenticalmodelsusingfourconditioningparameterizations
onAMASS[55]testsetsequences,usingsequencesoflength32and128.Parameterizationsvaryintheirspatial(1)andtemporal(2)invariance
properties,whichwelooselyclassifyasfollowingcompletely(✔),partially(P),ornotatall(✗). Theconditioningparameterizationused
byEgoAlloreduceserrorsbyalmost18%comparedtothesequencecanonicalizationapproachusedbythemostrelevantrelatedwork[47].
4.Experiments 4.1.Bodyestimation
We conduct a series of experiments to evaluate EgoAllo’s In our first set of experiments, we evaluate body estimation
conditioningparameterization,bodyestimationaccuracy,and from only device SLAM poses, without considering images
handestimationperformance. orhands.Thissettingallowsustoisolatetheadvantagesofour
bodymotionprior,whilefacilitatingdirectcomparisonagainst
Training.TotrainEgoAllomodelsusedinourexperiments, methodsthatdonotconsiderhands[47].
we need sequences containing human body and hand pose
parameters, body shapes, and device SLAM poses Tt .
world,cpf
Similar to prior work [6, 28, 47], we train EgoAllo using 4.1.1 Invariantconditioningevaluation
AMASS[55]withsynthesizeddeviceposes.Weannotatetrain
split sequences by anchoring a central pupil frame between We begin by evaluating the importance of the spatial and
verticescorrespondingtotheleftandrightpupilsintheblend temporalinvariancecriteriadiscussedinSection3.1.2.Wedo
skinnedmesh,andattraintimesamplesequencesuniformly this by comparing five implementations of the conditioning
betweenlength32and128. g:(1) EgoAllo isthefinalinvariantrepresentationthatwe
proposeinEquation4.(2) Absolute+Local Relative
Evaluation. We evaluate with four datasets. We use appends absolute poses with the relative pose deltas written
inEquation3.(3) Absolute+Global Deltas appends
AMASS[55],RICH[26],andAriaDigitalTwins(ADT)[59]
absolute poses with relative orientation and the world-frame
forbodyestimationevaluation,andEgoExo4D[17]forhand
positiondeltasusedby[6,28].(4) Sequence Canonical-
estimation evaluation. AMASS and RICH do not include
ization usesthealignmentapproachimplementedby[47],
egocentricdata;weannotatethesewithsyntheticdeviceposes
whichviolatestemporalinvariance.(5) Absolute naively
using the same procedure we use for training. ADT and
conditionsonabsoluteposes,whichviolatespatialinvariance.
EgoExo4DbothincludeegocentricimagesandSLAMposes
capturedusingProjectAriaglasses[83],whichweusedirectly. We train conditional diffusion models with otherwise
identical architecture using each parameterization, and then
Metrics. To quantify performance, we report four metrics: evaluateontheAMASS[55]testset.Metrics,includingpercent
(1) MPJPE is a world-frame mean per-joint position error differencecomparedtoEgoAllo,arereportedinTable1.
(millimeters).(2)PA-MPJPEistheProcrustes-alignedmean Overall, we find that the choice of conditioning parame-
per-joint position error in millimeters, where joint positions terization makes a dramatic impact on estimation accuracy.
arealignedonaper-timestepbasisbeforeerrorarecomputed. We observe accuracy improve consistently as invariance
(3) GND is a grounding metric, designed in response to a propertiesareincorporatedintotherepresentation.Comparedto
phenomenawhereego-sensedhumans“float”abovetheground. EgoAllo, Absolute conditioningwouldincreaseMPJPE
Givenahumanbodytrajectory,thismetriccontainsasimple byover23%forbothshorter(length32)andlonger(length
binaryindicatorofwhetherthefeetofthehumanevertouch 128)sequences.Comparedto EgoAllo, SeqCanonical
the ground plane. (4) T is the average SMPL head joint conditioningwouldincreaseMPJPEbynearly18%forlength
head
positionerrorinmillimeters. 32sequencesand12%forlength128sequences.AMASS[55]
Method Seq MPJPE↓ PA-MPJPE↓ GND↑ T ↓
head
EgoAllo 32 129.8±1.1 109.8±1.1 0.98±0.00 6.4±0.1
NoShape 32 138.1±1.1 118.8±1.1 0.94±0.01 44.7±0.4
EgoEgo 32 184.0±1.5 158.6±1.6 0.81±0.01 45.2±1.0
VAE+Opt 32 199.5±1.3 191.4±1.4 0.49±0.01 78.0±1.5
EgoAllo 128 119.7±1.3 101.1±1.3 1.0±0.00 6.2±0.1
NoShape 128 128.1±1.3 110.3±1.4 0.98±0.01 44.6±0.7
EgoEgo 128 167.4±2.1 145.8±2.0 0.92±0.01 54.9±1.9
(a)Ground-truth
VAE+Opt 128 205.3±2.6 192.3±2.8 0.75±0.02 67.8±3.1
RICH[26]
Method Seq MPJPE↓ PA-MPJPE↓ GND↑ T ↓
head
EgoAllo 32 193.7±3.4 174.8±3.6 0.95±0.01 8.8±0.2
NoShape 32 200.9±3.3 183.3±3.6 0.73±0.02 44.9±0.9
EgoEgo 32 215.4±3.9 192.9±4.0 0.73±0.02 56.2±2.9
VAE+Opt 32 352.0±6.7 354.8±6.5 0.59±0.02 319.3±11.6
EgoAllo 128 176.2±5.6 160.1±5.9 0.96±0.02 8.9±0.3
NoShape 128 185.7±5.5 169.9±5.8 0.82±0.03 45.8±1.6
EgoEgo 128 207.8±6.9 187.8±6.8 0.88±0.03 66.5±5.4
VAE+Opt 128 319.8±10.1 323.8±10.5 0.75±0.04 274.4±17.6 (b)EgoAllo
AriaDigitalTwins[59]
Method Seq MPJPE↓ PA-MPJPE↓ GND↑ T ↓
head
EgoAllo 32 173.5±1.1 146.1±1.1 0.88±0.01 -
NoShape 32 178.5±1.1 153.0±1.1 0.89±0.01 -
EgoEgo 32 212.5±1.4 181.3±1.6 0.64±0.01 -
VAE+Opt 32 284.9±1.6 283.9±1.9 0.63±0.01 -
EgoAllo 128 155.1±1.6 129.3±1.6 0.94±0.01 -
NoShape 128 163.7±1.6 140.0±1.6 0.96±0.01 -
EgoEgo 128 182.6±2.3 153.9±2.6 0.73±0.02 -
VAE+Opt 128 290.8±3.8 282.5±4.4 0.7±0.02 - (c)EgoEgo[47]
Table2.Bodyestimationperformance,comparedagainstabase-
linewithoutshapeprediction,EgoEgo[47],andVAE+Opt[72,99].
WeexcludetheT metricforADTbecausetheBiomech57head
head
jointsusedbyADTarenotdirectlycomparabletotheSMPL-Hhead
jointsusedbyourmodel.
4.1.2 Comparisonsagainstbaselines
(d)VAE+Opt
TofurtherstudyEgoAllo’sbodyestimationquality,wecompare
Figure 4. Egocentric human motion estimation for a running
against three baselines. (1) NoShape. First, NoShape refers
sequence.Weshowtheground-truth,anoutputfromEgoAllo,and
toavariationofEgoAllothatturnsoffshapeestimation,and
outputsfromtwobaselines.TheglassesCADmodelisplacedatthe
thus cannot estimate the wearer’s height. (2) EgoEgo. We
conditioningtransformationT .
world,cpf
alsocompareagainstthehumanmotiondiffusionmodelfrom
EgoEgo[47].ThisissimilartoEgoAllo,butconsidersonlythe
SMPL“mean”bodyshapeandusescanonicalizedcoordinates
CPFposealignmentcost.
forconditioningandasmodeloutput.(3)VAE+Opt.Finally,
wecompareagainstanapproachbasedontheSLAHMR[99] Duetodifferencesinproblemformulation,manyexisting
frameworkforhumanmotionestimationfromexocentricvideo. methodsforegocentrichumanmotionestimationaredifficult
AkeyadvantageofSLAHMRisthatitusesanunconditional todirectlycompare. Thisisparticularlytruewhentheyhave
motionprior[72]inanoptimizationframework.Itcantherefore different inputs, such as fisheye cameras [89, 93, 94], wrist-
beadaptedtonewsettingswithoutre-training—wekeepthe mountedsensors[45],orhandheldcontrollerposes[6,28,29].
samebodyposeandshapevariablesastheoriginalpipeline, Additionally,priorworkslikeEgoEgo[47]donotincorporate
butreplacetheexocentrickeypoint[69]costwithanegocentric visioninputsforhandestimation. Forfairness,werestrictallset,butperformancedeterioratesdramaticallywhenevaluating
onRICHorAriaDigitalTwins.VAE+OptoutputsinFigure4
alsolookoverlysmoothed, withoutthesameexpressiveness
astheconditionalpredictionsofEgoAlloorEgoEgo[47].This
highlightstheadvantageofusingaconditionaldiffusionmodel
problemforthisestimationproblem.
(a)Ground-truth Shapeestimationevaluation.Tobetterunderstandtheshape
estimation characteristics of EgoAllo, we compare against
againstthe“mean”shapeusedbyEgoEgoandtheNoShape
ablation. OntheAMASStestset,wefind: EgoAlloslightly
improvesoverallshape(19mm→18mmmeanvertex-to-vertex
error)andproducesmuchbetterheight(52mm→32mmmean
height error), but is not able to generalize in terms of body
weight (5kg→8kg mean weight error). The body shape is
(b)EgoAllo
inferredfromthewearer’sheadpose,whichintuitivelyprovides
strong height constraints but is less corelated with weight.
Accurateheightiskeyforpropersceneplacement,asreflected
byboththeMPJPEandGNDmetrics.
4.2.Handestimation
To evaluate evaluate hands estimated by EgoAllo, we run
(c)EgoEgo[47] HaMeRonthesegmentoftheEgoExo4D[17]validationset
thatislabeledwith3Dhandposekeypoints.Wequantitatively
compare four hand estimation methods in Table 4.2. In
(1)HaMeR[64],weuseHaMeRout-of-the-boxonundistorted
egocentricRGBimages.Wedonotassumeboundingboxesas
input;instead,wefollowtheHaMeRdemocodeandcompute
cropsusingViTPose[97].Whenmultiplehandsaredetectedfor
asingleside,wenaivelytakethefirstone.(2)EgoAllo-Mono
(d)VAE+Opt
refersourfullsystem,whichusesthesamemonocularHaMeR
Figure5. Egocentrichumanmotionestimationforasquatting handestimatesforguidance.In(3)EgoAllo-Wrist3D,weaug-
sequence.ThecontentsofthisfiguremirrorFigure4,butusespatial mentoursystemwithwristposeestimatesfromProjectAria’s
shiftstovisualizedifferenttimestepswithinasequence. MachinePerceptionServices[83]—unlikeHaMeR,whichas-
sumesmonocularinput,thisusesapairofSLAMcamerasthat
areuniquetoProjectAria.Finally,(4)EgoAllo-NoReprojre-
methodsinthissectiontoonlyCPForheadposeasinput.
movesthereprojectionterm(Equation14)fromEgoAllo-Mono.
Instead,handguidanceisdonedirectlyusingthe3Dwristposes
EgoAlloimprovesbodymotionestimates.Wereportmetrics
predictedbyHaMeR.Forfairnessacrosssettings,wecompute
inTable2andvisualizeexampleoutputsinFigures4and5.We
metricsonlyontimestepswhereHaMeRestimatesareavailable.
findthatEgoAlloenablessignificantestimationimprovements
Quantitative results are provided in Table 3. While
acrossalldatasets,including20∼30%accuracyimprovements
HaMeR’slocalposes(PA-MPJPE)areslightlybetter,EgoAllo’s
overEgoEgoforbothshorterandlongerevaluationsequences.
hand-bodyestimationsignificantlyimproveshowwellhands
Wefoundshapeestimationcriticalforproducingmetric-scale,
are estimated in the world coordinate system. Compared to
grounded estimates of human body motion, with the head
HaMeR,EgoAllodropsMPJPEfrom237.90mm→131.45mm.
aligned to input SLAM poses and the feet planted on the
Incorporating more accurate wrist pose estimates (EgoAllo-
observedgroundplane. Thisisevidentinqualitativeresults,
Wrist3D)offersapracticalsolutionforfurtherimprovements:
improved grounding metrics, and in the 6∼7% MPJPE gap
131.45mm→60.08mm. Reprojection-basedguidanceisalso
betweenEgoAlloandtheNoShapeablation.
morerobust:EgoAllo-NoReprojoutputsaretheworstinboth
VAE optimization converges poorly. Optimization-based MPJPEandPA-MPJPE.
estimation approaches have been effective for settings with Qualitatively,weobservedthathighhandestimationerrors
keypointcosts[72,99],butwefoundconvergencedifficultin innaivemonocularestimationwithHaMeRareexplainedby
ourlessconstrainedsetting.InTable2,weobservepoorgener- acombinationofdetectionfailuresandmonocularambiguities.
alization:VAE+OptperformscompetitivelyontheAMASStest Even when detections succeed, the scale and distance ofancestep. Inthefuture,itmaybepossibletobootstrapusing
outputsfromourmodeltotrainafeedforwardmodelthatavoids
thisstep. Successforhandguidancealsostilldependsonrea-
sonablemonocularhandestimates.Estimationcanthereforefail
asaresultoferrorslikeleft/rightflippingorspuriousdetections,
whichwefoundcauseshigherrorsinourTable3handestima-
tionmetrics.Finally,wealsotrainonlyonAMASS[55],which
includesfloorplanesbutnodetailedscenegeometry.Asaresult,
ourmethodwillfailifwecannotdetectanapproximatefloor
plane;thisismostlikelyinsettingslikehillsorstaircases.Inthe
future,wehopetoextendourinsightstodatawithmoredetailed
sceneinformation,whichconcurrentworkhashighlightedthe
usefulnessofininforminghumanbodyestimation[21].
Conclusion.WepresentedEgoAllo,asystemforestimating
humanmotionusingsensorsfromhead-mounteddevices.Our
method takes advantage of motion capture data [55] and an
Figure6.Bodyestimationimproveshandestimation.Weshowraw off-the-shelf visual hand estimator [64] to jointly estimate
outputsfromHaMeR[64]inblueandhand-bodyestimationsfrom human body pose, height, and hand parameters. EgoAllo
EgoAlloinpurple.Top:improvedsceneinteractionduringtouchscreen
highlights the importance of spatial and temporal invariance
operationwithEgoAllo-Mono.Weknowapriorithatthefingersare
in conditioning for this problem, while demonstrating how
contactingthescreeninthissequence.Bottom:qualitativeexamples
estimatedbodiescanbeusedtoimprovehandestimation.
from EgoExo [17] evaluation, showing the differences between
monocularhandsandEgoAllo-Wrist3Destimates. Acknowledgments. This project was funded in part
by NSF:CNS-2235013 and IARPA DOI/IBC No.
Method MPJPE↓ PA-MPJPE↓ 140D0423C0035. YM acknowledges support from the
jointSimonsFoundation-NSFDMSgrant#2031899,theONR
HaMeR 237.90±1.89 13.04±1.89
grantN00014-22-1-2102,theNSFgrant#2402951,andpartial
EgoAllo-Mono 131.45±0.39 14.71±0.39 supportfromTBSI,InnoHK,andtheUniversityofHongKong.
EgoAllo-Wrist3D 60.08±0.26 14.38±0.26 JM was supported by ONR MURI N00014-21-1-2801. BY
EgoAllo-NoReproj 143.20±0.42 14.75±0.42 is supported by the National Science Foundation Graduate
ResearchFellowshipProgramunderGrantDGE2146752.The
authorswouldalsoliketothankHongsukChoi,MichaelTaylor,
Table3.Handestimationerrorsinmillimeters.EgoAllo’shand-body TylerBonnen,SongweiGe,ChungMinKim,andJustinKerr
estimationcanconstrainandresolveambiguitiesinnoisyoutputsfrom
forinsightfultechnicaldiscussionandsuggestions,aswellas
HaMeR,whichweobservecanreduceMPJPEforhandsbyover40%.
JiamanLiforhelpfulanswerstoquestionsaboutEgoEgo.
References
monocularHaMeRestimatesareoftenincorrectorflickerin
betweenframes.Incorporatingthesehandsviaguidancewith [1] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and
ourdiffusionmotionpriorencouragesfinaloutputsthatobey Gustav Eje Henter. Listen, denoise, action! audio-driven
thekinematicandsmoothnessconstraintsimposedbyplausible motionsynthesiswithdiffusionmodels. ACMTransactions
body motion—we provide examples of HaMeR estimates onGraphics(TOG),42(4):1–20,2023.2
renderedjointlywithEgoAllooutputsinFigure6,wherewe [2] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller,
noterealisticelbowandshoulderposes. SebastianThrun,JimRodgers,andJamesDavis.Scape:shape
completionandanimationofpeople.InACMSIGGRAPH2005
5.Dicussion Papers,pages408–416.ACMNewYork,NY,USA,2005.2
[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.
Multidiffusion: Fusing diffusion paths for controlled image
Limitationsandfuturework.Whilethecorecontributionsof
generation. arXivpreprintarXiv:2302.08113,2023.5,14
EgoAlloaregeneral,thecurrentimplementationofoursystem
[4] GermanBarquero,SergioEscalera,andCristinaPalmero.Seam-
hasafewlimitationsthatwehopetoexploreinfuturework.
lesshumanmotioncompositionwithblendedpositionalencod-
First,diffusionmodelguidanceisatest-timeoptimizationpro- ings.InProceedingsoftheIEEE/CVFConferenceonComputer
cessthatrequireshyperparametersandincursaruntimecost:our VisionandPatternRecognition,pages457–469,2024.5
optimizationusesaCPU-basedsparselinearsolver[10],which [5] Marcus A Brubaker, David J Fleet, and Aaron Hertzmann.
forlength-128sequencestakesseveralsecondsforeachguid- Physics-based person tracking using the anthropomorphicwalker. International journal of computer vision, 87(1-2): segmentation for egocentric perception. arXiv preprint
140–155,2010.2 arXiv:2403.18118,2024.1
[6] AngelaCastillo,MariaEscobar,GuillaumeJeanneret,Albert [19] Peng Guan, Alexander Weiss, Alexandru O Balan, and
Pumarola,PabloArbela´ez,AliThabet,andArtsiomSanakoyeu. MichaelJBlack. Estimatinghumanshapeandposefroma
Bodiffusion:Diffusingsparseobservationsforfull-bodyhuman singleimage. InInt.Conf.Comput.Vis., pages1381–1388.
motionsynthesis. InProceedingsoftheIEEE/CVFInterna- IEEE,2009.2
tionalConferenceonComputerVision,pages4221–4231,2023. [20] RizaAlpGulerandIasonasKokkinos.Holopose:Holistic3d
2,4,5,6,7 humanreconstructionin-the-wild.InIEEEConf.Comput.Vis.
[7] RQiCharles,HaoSu,MoKaichun,andLeonidasJGuibas. PatternRecog.,pages10884–10894,2019.2
Pointnet:Deeplearningonpointsetsfor3dclassificationand [21] Vladimir Guzov, Yifeng Jiang, Fangzhou Hong, Gerard
segmentation. In2017IEEEconferenceoncomputervision Pons-Moll,RichardNewcombe,CKarenLiu,YutingYe,and
andpatternrecognition(CVPR),pages77–85.IEEE,2017.3 Lingni Ma. Hmd2: Environment-aware motion generation
[8] HaiweiChen,ShichenLiu,WeikaiChen,HaoLi,andRandall fromsingleegocentrichead-mounteddevice. arXivpreprint
Hill.Equivariantpointnetworkfor3dpointcloudanalysis.In arXiv:2409.13426,2024.2,9
ProceedingsoftheIEEE/CVFconferenceoncomputervision [22] ChenganHe,JunSaito,JamesZachary,HollyRushmeier,and
andpatternrecognition,pages14514–14523,2021.3 YiZhou.Nemf:Neuralmotionfieldsforkinematicanimation.
InNeurIPS,2022.2
[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and
[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
GeoffreyHinton.Asimpleframeworkforcontrastivelearning
of visual representations. In International conference on diffusionprobabilisticmodels. Advancesinneuralinformation
machinelearning,pages1597–1607.PMLR,2020.3
processingsystems,33:6840–6851,2020.2,5
[24] FangzhouHong, VladimirGuzov, HyoJinKim, YutingYe,
[10] Yanqing Chen, Timothy A Davis, William W Hager, and
Richard Newcombe, Ziwei Liu, and Lingni Ma. Egolm:
Sivasankaran Rajamanickam. Algorithm 887: Cholmod,
Multi-modal language model of egocentric motions. arXiv
supernodalsparsecholeskyfactorizationandupdate/downdate.
preprintarXiv:2409.18127,2024.2
ACMTransactionsonMathematicalSoftware(TOMS),35(3):
[25] Nicholas Howe, Michael Leventon, and William Freeman.
1–14,2008.9
Bayesian reconstruction of 3d human motion from single-
[11] Hai Ci, Mingdong Wu, Wentao Zhu, Xiaoxuan Ma, Hao
camera video. Advances in neural information processing
Dong,FangweiZhong,andYizhouWang. Gfpose:Learning
systems,12,1999.2
3dhumanposepriorwithgradientfields. InProceedingsof
[26] Chun-HaoP.Huang,HongweiYi,MarkusHo¨schle,Matvey
the IEEE/CVF conference on computer vision and pattern
Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel
recognition,pages4800–4810,2023.2,5
Scharstein,andMichaelJ.Black.Capturingandinferringdense
[12] TacoCohenandMaxWelling.Groupequivariantconvolutional
full-bodyhuman-scenecontact. InIEEEConf.Comput.Vis.
networks. InInternationalconferenceonmachinelearning,
PatternRecog.,pages13274–13285,2022.6,7,14
pages2990–2999.PMLR,2016.3
[27] Hao Jiang and Vamsi Krishna Ithapu. Egocentric pose
[13] PrafullaDhariwalandAlexanderNichol. Diffusionmodels
estimation from human vision span. In 2021 IEEE/CVF
beatgansonimagesynthesis. Advancesinneuralinformation
InternationalConferenceonComputerVision(ICCV),pages
processingsystems,34:8780–8794,2021.2
10986–10994.IEEE,2021.2
[14] Haiwen Feng, Peter Kulits, Shichen Liu, Michael J Black,
[28] JiaxiJiang,PaulStreli,HuajianQiu,AndreasFender,Larissa
andVictoriaFernandezAbrevaya.Generalizingneuralhuman
Laich, Patrick Snape, and Christian Holz. Avatarposer:
fittingtounseenposeswitharticulatedse(3)equivariance.In
Articulatedfull-bodyposetrackingfromsparsemotionsensing.
Proceedings of the IEEE/CVF International Conference on
InEuropeanconferenceoncomputervision,pages443–460.
ComputerVision,pages7977–7988,2023.3
Springer,2022.2,4,5,6,7
[15] Saeed Ghorbani, Calden Wloka, Ali Etemad, Marcus A [29] JiaxiJiang,PaulStreli,ManuelMeier,AndreasFender,and
Brubaker,andNikolausFTroje.Probabilisticcharactermotion ChristianHolz. Egoposer: Robustreal-timeego-bodypose
synthesisusingahierarchicaldeeplatentvariablemodel. In estimationinlargescenes. arXivpreprintarXiv:2308.06493,
Computer Graphics Forum, pages 225–239. Wiley Online 2023.2,3,4,5,7
Library,2020.2 [30] ZhongyuJiang,ZhuoranZhou,LeiLi,WenhaoChai,Cheng-
[16] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Yen Yang, and Jenq-Neng Hwang. Back to optimization:
Angjoo Kanazawa*, and Jitendra Malik*. Humans in 4D: Diffusion-basedzero-shot3dhumanposeestimation. InPro-
Reconstructingandtrackinghumanswithtransformers.InInt. ceedingsoftheIEEE/CVFWinterConferenceonApplications
Conf.Comput.Vis.,2023.2 ofComputerVision,pages6142–6152,2024.2,5
[17] KristenGrauman,AndrewWestbury,LorenzoTorresani,Kris [31] HanbyulJoo,NataliaNeverova,andAndreaVedaldi.Exemplar
Kitani,JitendraMalik,TriantafyllosAfouras,KumarAshutosh, fine-tuningfor3dhumanmodelfittingtowardsin-the-wild3d
VijayBaiyya,SiddhantBansal,BikramBoote,etal.Ego-exo4d: humanposeestimation. In2021InternationalConferenceon
Understandingskilledhumanactivityfromfirst-andthird-person 3DVision(3DV),pages42–52.IEEE,2021.2
perspectives. arXivpreprintarXiv:2311.18259,2023.6,8,9,14 [32] Angjoo Kanazawa, Abhishek Sharma, and David Jacobs.
[18] QiaoGu,ZhaoyangLv,DuncanFrost,SimonGreen,Julian Locallyscale-invariantconvolutionalneuralnetworks. arXiv
Straub, and Chris Sweeney. Egolifter: Open-world 3d preprintarXiv:1412.5104,2014.3[33] Angjoo Kanazawa, Michael J Black, David W Jacobs, and [49] Zongmian Li, Jiri Sedlar, Justin Carpentier, Ivan Laptev,
JitendraMalik.End-to-endrecoveryofhumanshapeandpose. NicolasMansard,andJosefSivic. Estimating3dmotionand
InIEEEConf.Comput.Vis.PatternRecog.,pages7122–7131, forcesofperson-objectinteractionsfrommonocularvideo.In
2018.2 ProceedingsoftheIEEE/CVFConferenceonComputerVision
[34] AngjooKanazawa,JasonYZhang,PannaFelsen,andJitendra andPatternRecognition,pages8640–8649,2019.2
Malik. Learning3dhumandynamicsfromvideo. InIEEE [50] HungYuLing,FabioZinno,GeorgeCheng,andMichielVan
Conf.Comput.Vis.PatternRecog.,pages5614–5623,2019.2 De Panne. Character controllers using motion vaes. ACM
[35] Korrawe Karunratanakul, Konpat Preechakul, Supasorn TransactionsonGraphics(TOG),39(4):40–1,2020.2
Suwajanakorn,andSiyuTang. Guidedmotiondiffusionfor
[51] MatthewLoper,NaureenMahmood,JavierRomero,Gerard
controllablehumanmotionsynthesis. InProceedingsofthe
Pons-Moll,andMichaelJBlack.Smpl:Askinnedmulti-person
IEEE/CVFInternationalConferenceonComputerVision,pages
linear model. In Seminal Graphics Papers: Pushing the
2151–2162,2023.2,5
Boundaries,Volume2,pages851–866,2023.2,3
[36] JihoonKim,JiseobKim,andSungjoonChoi.Flame:Free-form
[52] DavidGLowe.Distinctiveimagefeaturesfromscale-invariant
language-basedmotionsynthesis&editing. InProceedings
keypoints. Internationaljournalofcomputervision,60:91–110,
of the AAAI Conference on Artificial Intelligence, pages
2004.3
8255–8263,2023.2
[53] Zhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani.
[37] Diederik P Kingma and Max Welling. Auto-encoding
Dynamics-regulated kinematic policy for egocentric pose
variationalbayes. arXivpreprintarXiv:1312.6114,2013.2
estimation. Advances in Neural Information Processing
[38] Muhammed Kocabas, Nikos Athanasiou, and Michael J
Systems,34:25019–25032,2021.2
Black.Vibe:Videoinferenceforhumanbodyposeandshape
estimation.InIEEEConf.Comput.Vis.PatternRecog.,pages [54] Lingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov,
5253–5263,2020.2 Yifeng Jiang, Rowan Postyeni, Luis Pesqueira, Alexander
[39] MuhammedKocabas,Chun-HaoPHuang,OtmarHilliges,and Gamino,VijayBaiyya,HyoJinKim,etal.Nymeria:Amassive
MichaelJBlack. Pare:Partattentionregressorfor3dhuman collectionofmultimodalegocentricdailymotioninthewild.
bodyestimation.InProceedingsoftheIEEE/CVFInternational arXivpreprintarXiv:2406.09905,2024.2
ConferenceonComputerVision,pages11127–11137,2021.2 [55] NaureenMahmood,NimaGhorbani,NikolausFTroje,Gerard
[40] MuhammedKocabas, YeYuan, PavloMolchanov, Yunrong Pons-Moll,andMichaelJBlack. Amass: Archiveofmotion
Guo,MichaelJ.Black,OtmarHilliges,JanKautz,andUmar captureassurfaceshapes.InIEEEConf.Comput.Vis.Pattern
Iqbal. Pace: Humanandmotionestimationfromin-the-wild Recog.,pages5442–5451,2019.3,4,6,7,9,14
videos.In3DV,2024.2 [56] JulietaMartinez,RayatHossain,JavierRomero,andJamesJ
[41] NikosKolotouros,GeorgiosPavlakos,MichaelJBlack,and Little. Asimpleyeteffectivebaselinefor3dhumanposeesti-
KostasDaniilidis.Learningtoreconstruct3dhumanposeand mation.InInt.Conf.Comput.Vis.,pages2640–2649,2017.2
shapeviamodel-fittingintheloop.InInt.Conf.Comput.Vis.,
[57] EvonneNg,DonglaiXiang,HanbyulJoo,andKristenGrauman.
pages2252–2261,2019.2
You2me:Inferringbodyposeinegocentricvideoviafirstand
[42] NileshKulkarni,DavisRempe,KyleGenova,AbhijitKundu,
secondpersoninteractions. InProceedingsoftheIEEE/CVF
JustinJohnson,DavidFouhey,andLeonidasGuibas. Nifty:
ConferenceonComputerVisionandPatternRecognition,pages
Neural object interaction fields for guided human motion
9890–9900,2020.2
synthesis. arXivpreprintarXiv:2307.07511,2023.2
[58] MohamedOmran,ChristophLassner,GerardPons-Moll,Peter
[43] Christoph Lassner, Javier Romero, Martin Kiefel, Federica
Gehler,andBerntSchiele.Neuralbodyfitting:Unifyingdeep
Bogo,MichaelJBlack,andPeterVGehler.Unitethepeople:
learningandmodelbasedhumanposeandshapeestimation.
Closingtheloopbetween3dand2dhumanrepresentations.
In2018internationalconferenceon3Dvision(3DV),pages
InIEEEConf.Comput.Vis.PatternRecog.,pages6050–6059,
484–494.IEEE,2018.2
2017.2
[59] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott
[44] YannLeCun,YoshuaBengio,etal. Convolutionalnetworks
Peters,ThomasWhelan,ChenKong,OmkarParkhi,Richard
forimages,speech,andtimeseries. Thehandbookofbrain
Newcombe,andYuhengCarlRen. Ariadigitaltwin: Anew
theoryandneuralnetworks,3361(10):1995,1995.3
benchmarkdatasetforegocentric3dmachineperception. In
[45] Jiye Lee and Hanbyul Joo. Mocap everyone everywhere:
Proceedings of the IEEE/CVF International Conference on
Lightweight motion capture with smartwatches and a head-
ComputerVision,pages20133–20143,2023.6,7,14
mountedcamera. arXivpreprintarXiv:2401.00847,2024.2,7
[46] JiamanLi,AlexanderClegg,RoozbehMottaghi,JiajunWu, [60] GeorgiosPavlakos,LuyangZhu,XiaoweiZhou,andKostas
XavierPuig,andCKarenLiu.Controllablehuman-objectinter- Daniilidis. Learning to estimate 3d human pose and shape
actionsynthesis. arXivpreprintarXiv:2312.03913,2023.2,5 fromasinglecolorimage.InIEEEConf.Comput.Vis.Pattern
[47] JiamanLi,KarenLiu,andJiajunWu.Ego-bodyposeestimation Recog.,pages459–468,2018.2
via ego-head pose estimation. In IEEE Conf. Comput. Vis. [61] GeorgiosPavlakos,VasileiosChoutas,NimaGhorbani,Timo
PatternRecog.,pages17142–17151,2023.2,3,4,5,6,7,8,14 Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and
[48] JiamanLi,JiajunWu,andCKarenLiu.Objectmotionguided MichaelJ.Black. Expressivebodycapture: 3Dhands,face,
human motion synthesis. ACM Transactions on Graphics andbodyfromasingleimage. InIEEEConf.Comput.Vis.
(TOG),42(6):1–11,2023.2 PatternRecog.,pages10975–10985,2019.2[62] Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. [76] RobinRombach,AndreasBlattmann,DominikLorenz,Patrick
Human mesh recovery from multiple shots. In IEEE Conf. Esser, and Bjo¨rn Ommer. High-resolution image synthesis
Comput.Vis.PatternRecog.,pages1485–1495,2022.2 withlatentdiffusionmodels.InProceedingsoftheIEEE/CVF
[63] GeorgiosPavlakos,EthanWeber,MatthewTancik,andAngjoo conferenceoncomputervisionandpatternrecognition,pages
Kanazawa.Theonewheretheyreconstructed3dhumansand 10684–10695,2022.2
environmentsintvshows. InEur.Conf.Comput.Vis.,pages [77] Javier Romero, Dimitrios Tzionas, and Michael J Black.
732–749.Springer,2022.2 Embodiedhands: Modelingandcapturinghandsandbodies
[64] GeorgiosPavlakos,DandanShan,IlijaRadosavovic,Angjoo together. arXivpreprintarXiv:2201.02610,2022.2,3,5
Kanazawa,DavidFouhey,andJitendraMalik.Reconstructing [78] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
handsin3Dwithtransformers.Inarxiv,2023.3,8,9 JayWhang,EmilyLDenton,KamyarGhasemipour,Raphael
[65] Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.
Abbeel,andSergeyLevine. Sfv: Reinforcementlearningof Photorealistic text-to-image diffusion models with deep
physicalskillsfromvideos. ACMTransactionsOnGraphics language understanding. Advances in Neural Information
(TOG),37(6):1–14,2018.2 ProcessingSystems,35:36479–36494,2022.2
[66] Chiara Plizzari, Shubham Goel, Toby Perrett, Jacob Chalk, [79] SilvioSavareseandLiFei-Fei.3dgenericobjectcategorization,
AngjooKanazawa,andDimaDamen. Spatialcognitionfrom localizationandposeestimation. In2007IEEE11thinterna-
egocentricvideo:Outofsight,notoutofmind. arXivpreprint tionalconferenceoncomputervision,pages1–8.IEEE,2007.3
arXiv:2404.05072,2024.1
[80] YonatanShafir,GuyTevet,RoyKapon,andAmitHBermano.
[67] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Humanmotiondiffusionasagenerativeprior. arXivpreprint
JonathanTBarron,AmitHBermano,EricRyanChan,Tali arXiv:2303.01418,2023.5
Dekel,AleksanderHolynski,AngjooKanazawa,etal. State
[81] TakaakiShiratori,HyunSooPark,LeonidSigal,YaserSheikh,
of the art on diffusion models for visual computing. arXiv
andJessicaKHodgins. Motioncapturefrombody-mounted
preprintarXiv:2310.07204,2023.2
cameras.InACMSIGGRAPH2011papers,pages1–10.ACM
[68] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,
NewYork,NY,USA,2011.2
GabrielGoh,SandhiniAgarwal,GirishSastry,AmandaAskell,
[82] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
PamelaMishkin,JackClark,etal.Learningtransferablevisual
and Surya Ganguli. Deep unsupervised learning using
modelsfromnaturallanguagesupervision.InInternationalcon-
nonequilibriumthermodynamics.InInternationalconference
ferenceonmachinelearning,pages8748–8763.PMLR,2021.2
onmachinelearning,pages2256–2265.PMLR,2015.2
[69] JathushanRajasegaran,GeorgiosPavlakos,AngjooKanazawa,
[83] KiranSomasundaram,JingDong,HuixuanTang,JulianStraub,
and Jitendra Malik. Tracking people by predicting 3d
Mingfei Yan, Michael Goesele, Jakob Julian Engel, Renzo
appearance, locationandpose. InIEEEConf.Comput.Vis.
De Nardi, and Richard Newcombe. Project aria: A new
PatternRecog.,pages2740–2749,2022.2,7
tool for egocentric multi-modal ai research. arXiv preprint
[70] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
arXiv:2308.13561,2023.2,4,6,8
andMarkChen.Hierarchicaltext-conditionalimagegeneration
[84] JiamingSong,ChenlinMeng,andStefanoErmon.Denoising
withcliplatents,2022.5
diffusionimplicitmodels. arXivpreprintarXiv:2010.02502,
[71] DavisRempe,LeonidasJGuibas,AaronHertzmann,Bryan
2020.5
Russell,RubenVillegas,andJimeiYang.Contactandhuman
[85] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma,
dynamicsfrommonocularvideo. InComputerVision–ECCV
AbhishekKumar,StefanoErmon,andBenPoole.Score-based
2020:16thEuropeanConference,Glasgow,UK,August23–28,
generativemodelingthroughstochasticdifferentialequations.
2020,Proceedings,PartV16,pages71–87.Springer,2020.2
arXivpreprintarXiv:2011.13456,2020.2
[72] DavisRempe, TolgaBirdal, AaronHertzmann, JimeiYang,
[86] JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,WenBo,
SrinathSridhar,andLeonidasJGuibas. Humor: 3dhuman
andYunfengLiu.Roformer:Enhancedtransformerwithrotary
motionmodelforrobustposeestimation.InProceedingsofthe
positionembedding. Neurocomputing,568:127063,2024.14
IEEE/CVFinternationalconferenceoncomputervision,pages
11488–11499,2021.2,3,4,5,7,8 [87] ZacharyTeedandJiaDeng. Droid-slam: Deepvisualslam
[73] DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. formonocular,stereo,andrgb-dcameras. Advancesinneural
Stochasticbackpropagationandapproximateinferenceindeep informationprocessingsystems,34:16558–16569,2021.2
generativemodels. InInternationalconferenceonmachine [88] NathanielThomas,TessSmidt,StevenKearnes,LusannYang,
learning,pages1278–1286.PMLR,2014.2 LiLi,KaiKohlhoff,andPatrickRiley. Tensorfieldnetworks:
[74] HelgeRhodin,ChristianRichardt,DanCasas,EldarInsafut- Rotation-and translation-equivariant neural networks for 3d
dinov,MohammadShafiei,Hans-PeterSeidel,BerntSchiele, pointclouds. arXivpreprintarXiv:1802.08219,2018.3
andChristianTheobalt.Egocap:egocentricmarker-lessmotion [89] Denis Tome, Patrick Peluse, Lourdes Agapito, and Hernan
capture with two fisheye cameras. ACM Transactions on Badino. xr-egopose: Egocentric 3d human pose from an
Graphics(TOG),35(6):1–11,2016.2 hmdcamera. InProceedingsoftheIEEE/CVFInternational
[75] GregoryRogez,PhilippeWeinzaepfel,andCordeliaSchmid. ConferenceonComputerVision,pages7728–7738,2019.2,7
Lcr-net:Localization-classification-regressionforhumanpose. [90] Denis Tome, Thiemo Alldieck, Patrick Peluse, Gerard
InIEEEConf.Comput.Vis.PatternRecog.,pages3433–3441, Pons-Moll,LourdesAgapito,HernanBadino,andFernando
2017.2 De la Torre. Selfpose: 3d egocentric pose estimation froma headset mounted camera. IEEE Transactions on Pattern [105] YeYuanandKrisKitani.Ego-poseestimationandforecasting
AnalysisandMachineIntelligence,2020.2 asreal-timepdcontrol.InProceedingsoftheIEEE/CVFInter-
[91] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: nationalConferenceonComputerVision,pages10082–10092,
Editable dance generation from music. In Proceedings of 2019.2
theIEEE/CVFConferenceonComputerVisionandPattern [106] YeYuan,UmarIqbal,PavloMolchanov,KrisKitani,andJan
Recognition,pages448–458,2023.2 Kautz.Glamr:Globalocclusion-awarehumanmeshrecovery
[92] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit, withdynamiccameras. InIEEEConf.Comput.Vis.Pattern
Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Recog.,2022.2
Polosukhin. Attentionisallyouneed. Advancesinneural [107] ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,Barnabas
informationprocessingsystems,30,2017.5,14 Poczos,RussRSalakhutdinov,andAlexanderJSmola.Deep
[93] JianWang,LingjieLiu,WeipengXu,KripasindhuSarkar,and sets. Advancesinneuralinformationprocessingsystems,30,
ChristianTheobalt. Estimatingegocentric3dhumanposein 2017.3
globalspace. InProceedingsoftheIEEE/CVFInternational [108] DaiweiZhang,GengyanLi,JiajieLi,Mickae¨lBressieux,Otmar
ConferenceonComputerVision,pages11500–11509,2021.2,7 Hilliges,MarcPollefeys,LucVanGool,andXiWang.Egogaus-
[94] JianWang,ZheCao,DiogoLuvizon,LingjieLiu,Kripasindhu sian:Dynamicsceneunderstandingfromegocentricvideowith
Sarkar,DanhangTang,ThaboBeeler,andChristianTheobalt. 3dgaussiansplatting. arXivpreprintarXiv:2406.19811,2024.1
Egocentric whole-body motion capture with fisheyevit and [109] MingyuanZhang,ZhongangCai,LiangPan,FangzhouHong,
diffusion-based motion refinement. In Proceedings of the Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse:
IEEE/CVF Conference on Computer Vision and Pattern Text-driven human motion generation with diffusion model.
Recognition,pages777–787,2024.2,7 arXivpreprintarXiv:2208.15001,2022.2
[95] Laurenz Wiskott and Terrence J Sejnowski. Slow feature
[110] QinshengZhang,JiamingSong,XunHuang,YongxinChen,
analysis: Unsupervised learning of invariances. Neural
andMing-YuLiu. Diffcollage: Parallelgenerationoflarge
computation,14(4):715–770,2002.3
contentwithdiffusionmodels.In2023IEEE/CVFConference
[96] WeipengXu,AvishekChatterjee,MichaelZollhoefer,Helge onComputerVisionandPatternRecognition(CVPR),pages
Rhodin,PascalFua,Hans-PeterSeidel,andChristianTheobalt.
10188–10198.IEEE,2023.5
Mo2cap2: Real-timemobile3dmotioncapturewithacap-
[111] Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian,
mountedfisheyecamera. IEEEtransactionsonvisualization
Darren Cosker, and Siyu Tang. Probabilistic human mesh
andcomputergraphics,25(5):2093–2101,2019.2
recoveryin3dscenesfromegocentricviews. InProceedings
[97] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.
oftheIEEE/CVFInternationalConferenceonComputerVision,
Vitpose+:Visiontransformerfoundationmodelforgenericbody
pages7989–8000,2023.2,5
poseestimation. arXivpreprintarXiv:2212.04246,2022.8
[112] ZhengdongZhang,ArvindGanesh,XiaoLiang,andYiMa.
[98] Jingyun Yang, Congyue Deng, Jimmy Wu, Rika Antonova,
Tilt: Transform invariant low-rank textures. International
Leonidas Guibas, and Jeannette Bohg. Equivact: Sim(3)-
journalofcomputervision,99:1–24,2012.3
equivariantvisuomotorpoliciesbeyondrigidobjectmanipu-
lation,2023.3
[99] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo
Kanazawa.Decouplinghumanandcameramotionfromvideos
inthewild.InIEEEConf.Comput.Vis.PatternRecog.,pages
21222–21232,2023.2,5,7,8
[100] BrentYi,MichelleLee,AlinaKloss,RobertoMart´ın-Mart´ın,
andJeannetteBohg. Differentiablefactorgraphoptimization
for learning smoothers. In 2021 IEEE/RSJ International
ConferenceonIntelligentRobotsandSystems(IROS),2021.14
[101] BrentYi,WeijiaZeng,SamBuchanan,andYiMa.Canonical
factors for hybrid neural fields. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pages
3414–3426,2023.3
[102] XinyuYi,YuxiaoZhou,andFengXu.Transpose:Real-time3d
humantranslationandposeestimationwithsixinertialsensors.
ACMTransactionsonGraphics(TOG),40(4):1–13,2021.2
[103] XinyuYi,YuxiaoZhou,MarcHabermann,VladislavGolyanik,
ShaohuaPan, ChristianTheobalt, andFengXu. Egolocate:
Real-timemotioncapture,localization,andmappingwithsparse
body-mountedsensors. arXivpreprintarXiv:2305.01599,2023.
2
[104] YeYuanandKrisKitani.3dego-poseestimationviaimitation
learning. In Proceedings of the European Conference on
ComputerVision(ECCV),pages735–750,2018.2A.Appendix
A.1.Floorheightestimation
AkeyrequirementofouralgorithmisSLAMposesthatcanbe
situatedrelativetothefloor. Whilefloorheightsareprovided
inourtrainingdata,theyarenotdirectlyavailableonreal-world
data.WefoundthatasimpleRANSAC-basedalgorithmworks
wellonreal-worlddatafromProjectAria[59].WefilterSLAM
pointsbyconfidence,thenuseRANSACtofindaz-valuewith
thatbestfitsaplane.Examplefloorplaneoutputs,usingscenes
fromtheEgoExo4D[17]dataset,areshowninFigure7.
A.2.Sequencelengthevaluation
At test time, EgoAllo follows MultiDiffusion [3] for extrap-
olatingtosequencesthatarelongerthanthe128thatweuse
for training. To evaluate that this works, we filter out test
sequences shorter than 256 frames and then evaluate both
EgoAlloandEgoEgo[47]withsubsequencesofsize32,128,
and 256. We report metrics on these sequences in Table 4.
BothEgoAlloandEgoEgoincludewindowingstrategiesfor
handlinglongersequences;unlikepriorwork,however,wefind
thattheEgoAllosystemimprovesevenaftertestsetsequence
lengthssurpassthetrainingsetsequencelength.
Seqlen 32 128 256
EgoAllo 149.3 130.3 127.9
EgoEgo 187.7 173.8 184.3
Table4.HowdoesMPJPEchangewithsequencelength?
A.2.1 Biomech57evaluationdetails
The majority of our evaluation data (AMASS [55] and
Figure7. Floorheightexamples.Pointcloud-derivedfloorheight
RICH [26]) is provided directly using SMPL conventions. examplesontheEgoExo4Ddataset.
BecauseourmodeloutputsSMPL-Hparameters,thismakes
computationofjointerrormetricsstraightforward.
The one exception is the Aria Digital Twins dataset [59], A.3.Implementationdetails
whichweuseforquantitativebodymetrics.Eachdevicewearer
Weuseatransformer[92]architecturewithrotarypositional
intheAriaDigitalTwinsdatasetisrecordedviaanOptitrack
embeddings[86]forEgoAllo’sdenoisingmodelµ (⃗x ,⃗c,n).
θ n
motioncapturesystem, whichrecords57jointlocations(30
Latent representations ⃗z for conditioning sequences ⃗c are
c
hand joints, 27 body joints) following the Biomech57 joint
computed using six transformer blocks, each containing a
template. TheBiomech57jointstandarddoesnotmatchthe
self-attentionlayerfollowedbya2-layerMLP.Skipconnections
SMPL-Hjointsweuseinourmethod.Toevaluateourmethod
are used throughout. The denoised output
⃗xˆ
is produced
0
on ADT, we match and compare the common major joints
by using six additional transformer blocks that take ⃗x as
n
betweenthetwotemplates.
input,whileconditioningon⃗z viacross-attention.Allhidden
c
Wemanuallycorrespondedeachofthe57jointsbetween
dimensionsaresetto512.Forguidance,weuseinfrastructure
Biomech57andthestandardSMPL-Hjointconventions.While
forsparsenonlinearleastsquaresimplementedin[100]. For
themajorityofthesehave1:1correspondences—feet,knees,
additionaldetails,werefertoourcoderelease.
hips,shoulders,elbows,wrist,andfingerjoints,forexample,
areconsistentlydefined—othersliketheheadandcollarbone
jointsaremisaligned.WhencomputingMPJPEvaluesforthe
ADTdataset,wemaskoutthesejoints.