System 2 reasoning capabilities are nigh
Scott C. Lowe
Vector Institute
Toronto, Canada
scott.lowe@vectorinstitute.ai
October 7, 2024
Abstract
Inrecentyears,machinelearningmodelshavemadestridestowardshuman-likereasoningcapabil-
ities from several directions. In this work, we review the current state of the literature and describe
theremaining stepstoachieveaneuralmodel whichcan perform System2reasoning analogous toa
human. Wearguethatif currentmodelsareinsufficient tobeclassed asperforming reasoning, there
remains verylittle additional progress needed to attain that goal.
1 Introduction
The dual process theory of thought processes is long standing within psychology (Wason & Evans, 1974;
Evans, 2008; Stanovich & West, 2000) and was popularized more broadly by Kahneman (2012). In this
framework, human thinking capabilities are conceptualized as two distinct modes of thought. System 1
is fast, automatic, instinctive, and more emotional; System 2 is slower, effortful, deliberate, and more
logical. System1 is, in essence,unconscious thought, and System 2 is consciousthought; though there is
notyetconsensus onwhether “ah-ha”moments whichcome followinganincubation periodare triggered
by unconscious work of System 1 or 2 (Christensen, 2005; Gilhooly, 2016). Additionally, due to its
instinctive and reactive nature, System 1 is more prone to bias than System 2, though System 2 is not
without bias.
Incomparisonto thesetwocognitivesystemsofthought,feed-forwardneuralnetworksaresometimes
described as being analogous to System 1. Their outputs are immediate and automatic, yielded immedi-
ately without what might call “deliberation”. Like with System 1, the computational system producing
the output does not and can not provide an explicit explanation for why it produced a certain response,
making interpretability challenging, even when attempting to induce it to provide an a posteriori justifi-
cation for its response; Jung et al., 2022). Such systems are effectively performing pattern matching for
their current stimulus against the body of data imbibed during training.
In comparison, symbolic rule-based algorithms (classical “artificial intelligence”), whether they are
manually or programatically created, can provide an explanation for their reasoning. However their
performance is limited because the space of the real-world is too large to be handled with a narrow set
of rules that are coded in the stimulus domain.
In this work, we review the existing literature in the space of reasoning from the perspective of
philosophy and machine learning, and we speculate on what form a neural network would need to take
for it to be able to perform reasoning in the style of System 2. We argue the majority of hurdles needed
to achieve this task have already been cleared, and there are a small number of pieces of the puzzle
remaining. Thus complex agents, trained through deep learning, that can reason logically about the
real-worldwill be available in the near-term, if they are not here already.
1
4202
tcO
4
]IA.sc[
1v26630.0142:viXra2 Background
2.1 Modalities of human thought
HistorictextsindicatethatancientphilosopherssuchasPlatousedtothinkthatthinkingwassynonymous
with an inner monologue. However, whilst an internal monologue (inner speech) is common, it is not
ubiquitousandmostpeopleoverestimatehowoftentheirthoughtsareexpressedverbally(Hurlburt et al.,
2013). There is a wide variety of inner experiences across humans (Hurlburt & Heavy, 2006), and most
people are surprised when they first discover that other people’s internal experiences differ greatly from
their own.
The modalities of human inner thought are (Hurlburt & Schwitzgebel, 2011; Hurlburt et al., 2013):
• Inner speaking/inner monologue — thoughts expressed verbally, e.g. talking to yourself, hearing
your/a voice while recalling.
• Inner seeing/visual imagery — thoughts expressed visually, e.g. picturing a memory or imagining
a hypothetical scene.
• Feelings — a conscious experience of emotional processes, e.g. sadness when grieving.
• Unsymbolized thinking — thoughts expressed without words or images, e.g. drinking a glass of
water, without internal discussion or commentary.
• Sensory awareness— attending to a sensory aspect of the environmentfor an unimportant reason,
e.g. hearing someone talk but seeing the light reflecting off their glasses.
Mostpeople experienceinner speechandinner imagerysomeofthe time butnotallofthe time, with
themajorityoftheirthoughtprocessesunsymbolized(Hurlburt et al.,2013). Howeverthereareoutliersin
eachdirection,withsomepeoplehavingnoinnerspeech(anauralia),constantinnerspeech,nomind’seye
(aphantasia),or extremely vividmentalimageryasdetailed assensorystimuli(hyperphantasia). Day-to-
dayobservationsofpeopleacrosssocietydemonstrate,andacademicstudiesconfirm,thatpeopleareable
tocompletetasksirrespectiveofwhethertheirinternalthoughtsarerepresentedthroughspeech,imagery,
or neither (Keogh et al., 2021; Hinwar & Lambert, 2021); though the lack of inner sight does impair the
ability to recall visual characteristics (Monzel et al., 2022; Bainbridge et al., 2021). Additionally, note
that those possessing an inner monologue who speak multiple languages can have their inner monologue
flip between languages depending on recent context. These observations lead us to hypothesise that
conscious thoughts (i.e. System 2 thinking) are fundamentally abstract in nature, but can be projected
to language and visual modalities internally.
2.2 What is System 2 reasoning?
As a thought exercise, consider the task of solving this illustrative example from Kahneman (2012):
A bat and a ball together cost $1.10. The bat costs $1 more than the ball. How much does the
ball cost?
System 1 is responsible for the automatic response which immediately comes to mind on reading the
problem: ten cents. This answer is yielded in an involuntary manner, seemingly to all who hear the
question for the first time. However, by engaging System 2 we can verify whether the intuitive solution
is correct, and reason about it. By reflecting on the intuitive answer, one can observe that if this were
the price of the ball, the total would be $1.20, hence the answer is incorrect. Since as the total price is
the difference in price between the two objects ($1) plus twice the price of the ball, the answer is in fact
5 cents.
2If we analyse it, it appears that the instinctive response stems from pattern matching—the problem
looks at first glance like other problems comparing the quantity of two items, which we have solved in
the past using the “subtraction” method, hence we instinctively try to apply it here.
Onewaytoconceptualizereasoningisasaseriesofhypothesisgenerationandverificationsteps. Ifthe
initial hypothesis fails the verification, we then come up with a new hypothesis conditioned on the new
informationgeneratedduring the verificationprocess. This processisrepeateduntila revisedhypothesis
satisfies the verification step. Note that such a framework is similar to the Actor-Critic reinforcement
learning algorithm (Konda & Tsitsiklis, 1999), with the actor analogousto the hypothesis generatorand
the critic as the verifier.
Alternatively, reasoning can be conceptualized as a train of thought in a continuous stream of con-
sciousness(Potter et al.,2014;James,2012). Thisframeworkiscomparabletothechain-of-thoughtLLM
prompting technique (Wei et al., 2022).
2.3 Existing neural reasoning agents
Previous work has found that by prompting LLMs with an in-context example of chain-of-thought rea-
soning, and asking it to think step-by-step for its own answer, models can be coerced into “thinking”
step-by-step (Wei et al., 2022). Providing such a prompt changes this distribution of most likely next
token to be a longer trajectory with smaller steps toward the solution to a question outputted first. By
having the model attend to its own steps as it progresses, it builds on its previous steps such that its
final result is more likely to be accurate (Wei et al., 2022; Li et al., 2024). However, recent work has
demonstrated the majority of the gains seen when using chain-of-thought prompting can be matched
by prompting with a long series of task-independent filler tokens instead, suggesting the length of the
sequenceandthesizeofthecomputegraphismoreimportantthanthetextualoutput(Pfau et al.,2024).
This implies the transformer can process data through unseen computations within the hidden layers of
thenetwork,unwitnessedinthechain-of-thoughttokensthatitoutputs. Suchfindingsmaybeanalogous
toSystem2reasoninginhumans,whichwenotedin§2.1areprimarilynon-symbolicbutcanbeprojected
toconsciouslyobservedlanguagestreams(Hurlburt et al.,2013),thoughsuchahypothesisischallenging
to investigate due to the difficulties of interpreting deep transformer representations (Rai et al., 2024).
In the domain of closed-world games, tremendous gains were seen by applying deep reinforcement
learningmodels that learnthroughself-play to optimize a value function (Silver et al.,2016,2017, 2018).
Inthiscase,thevaluenetworkcanbefittopredictthelikelihoodofeachplayerwinningthegamefroma
givenposition. Theresultofthegamecanbeobjectivelydeterminedbycontinuingtoplaythematchand
seeing who wins, providing a strong training signalto the value network. Since the value network is able
to fit this task, it is able to steer the actormodel effectively. Results are further improvedby performing
a Monte Carlo Markov chain tree search at inference time, steered by the model’s predictions to prune
the tree to a narrowrangeof feasible moves,to evaluate future game states and choose an optimalmove.
SuchsearchesaresimilartotheTree-of-thoughtsapproachtoimprovechain-of-thoughtsreasoning(Long,
2023; Yao et al., 2023).
When deploying LLMs on mathematics problems, step-levelverificationof chain-of-thoughthas been
shown to be effective training technique (Cobbe et al., 2021; Uesato et al., 2022; Lightman et al., 2024).
3 Future steps and potential pitfalls
3.1 Learning to reason
Given the existing neural reasoning techniques, and their analogous relationship to human reasoning
processes, we posit that networks already can learn to reason.
MultipleworkshaveshowntrainingLLMstoreasonstep-by-stepisbestachievedbystep-levelfeedback
(Zelikman et al.,2022;Pfau et al.,2024;Lightman et al.,2024). Oneissuefortrainingareasoningmodel
at scale is thus that there is a lack of large-scale reasoning datasets to train on in which humans have
written out their train of thoughts explicitly, though some do exist (Yang et al., 2018). However, such
3data can be acquired at modest scale, and (by explicitly labelling which steps are valid and which are
not) suchdata canbe used to traina verifier that predicts whether individuallogicalreasoningsteps are
sound. This verifier (similar to the rationalisation evaluator used by Zelikman et al. (2022)) can serve a
similarroletothestep-wisemathsproblemsolverofLightman et al.(2024). Usingthis,wecanbootstrap
more chain-of-thought data by tasking a pretrained LLM with chain-of-thought prompting to generate
more reasoning data, and discarding outputs which contain steps which do not pass the verifier, similar
to that used in Lightman et al. (2024).
Note that the verifier is an essential part of this pipeline, and it must be accurate in order for the
iterative self-distillation to be effective. But in any scenario where verification is easier than generation,
the verifier (even if learnt and imperfect) can be deployed to iteratively refine and distill the generative
model (Christiano et al., 2018). An alternative bootstrapformulationwould be to generate a large body
of chain-of-thoughts data using chain-of-thought prompting applied on a large corpus of problems with
known solutions. We then train a verifier to, given a particular point in the chain-of-thought, classify
whether the model will get the right answer. This verifier model will serve a similar role to the value
functionin self-playRL systems (Silver et al., 2018), andwe canfine-tune ourmodel to generateits step
of thoughts whilst trying to maximize the verifier’s probability the problem will be solved. Since such a
systembears similarity to Q-learningand STaR bootstrapreasoning(Zelikman et al., 2022), it might be
aptlygiventhename“Q*”. Wenotethatotherrecentworkhassuccessfullyappliedreinforcementlearning
fine-tuningtopretrainedLLMs,suchasreinforcementwithhumanfeedback(RLHF)(Ziegler et al.,2020)
or with harmlessness feedback (Bai et al., 2022); and these methods can be improved by modifying
the method to provide direct feedback (Rafailov et al., 2024; Lee et al., 2024). The implementation we
propose would be a similar reinforcement learning fine-tuning stage, but with a objective focused on
reasoning accuracy.
All the components for this solution seemingly already exist in the literature, and it is even possible
such a model has already been trained recently (OpenAI, 2024).
3.2 Applicability
LLMs trained only on textual data are unlikely to master reasoning about the real-world, since their
observations of it are highly indirect. When humans communicate with each other, they do so with a
large body of common experiences merely from being creatures raised and living in the real world. This
meansthat manythings thatare takenfor grantedremainunstatedas they areassumedto be knownby
all parties in the discourse.
In order for foundation models to be able to reason efficiently about the world, we speculate they
will need a world model that is built on sensory observations, not just text descriptions. More recent
foundation models have made progress in this direction (Zhang et al., 2022) by being multi-modal, pro-
cessing both language and visual stimuli. However, we posit that further gains will be made when using
data which captures the richness of the real world through video data and (less abundantly) embodied
sensorimotor data. Video data has rich features about the world, enabling the network to construct its
own intuitive physics, infer cause and effect (Bardes et al., 2024).
3.3 Scaling
Willscalinglawscontinuetoholdforchain-of-thoughtreasoning,orwillsuchmodelshitscalingproblems?
The “bitter lesson” of machine learning has been that gains from methods that can exploit generic
compute scaling (e.g. larger and more flexible models, trained increasingly large datasets), in the long-
run outperform gains from human-knowledge adjustments due to Moore’s law (Sutton, 2019). Thus
we postulate that reasoning models will naturally also benefit from utilizing general methods rather
than hand-tuned routines. This is evidenced by recent work deploying LLMs on mathematical problems
(Snell et al.,2024),whichfoundthatevaluationperformanceincreasesastheamountofinferencecompute
increases.
4However, one possible obstacle is the quadratic scaling of transformers with respect to their input
sequence length due to their all-to-all attention. Inefficient chain-of-thought reasoning will create exces-
sivelyverbosethought-histories,greatlyincreasingthe amountofcompute requiredto reachthe endof a
chain-of-thought. This poses a challenge to efficiently utilize compute when the model’s inference steps
are scaled up. There have been various attempts to modify transformers to scale better (Child et al.,
2019; Choromanski et al., 2021; Dao et al., 2022; Dao, 2024). Recently there have also been orthogonal
effortstowardsSOTALLMsthatarebuiltusingStateSpaceModel(SSM)architectures(Gu et al.,2022;
Poli et al., 2023; Gu & Dao, 2023; Dao & Gu, 2024).
More critically, as the number of entities to reasonabout grows,the number of potential interactions
betweentheentitiesgrowsexponentially. Thishasthe potentialtoout-scalethe computationalresources
availabletotrainanddeployreasoningmodels. However,wenotethathumanworkingmemoryislimited
to 7±2 objects or chunks acrossa variety of tasks, where the number and size of chunks depends on the
individual’s familiarity with the items being held in memory (Miller, 1956). This implies that reasoning
does notrequire all-to-allattentionoverobjects in the thoughthistory,rather it only requiresa constant
memory space. The remaining challenges are (1) items being held in memory must be appropriately
compact; (2) when only a limited number of items are retained in memory, the model must learn which
memories to keep and which to drop.
With regards to compactness, this is a challenge for token-based models as typically the embedding
space has the same granularity as the stimulus space. Yet recent hierarchical models from the vision
literature offer insights into how a hierarchical token-based model may look, in which the embedding
space is more spatially compact than the stimulus representations (Liu et al., 2021; Fan et al., 2021;
Li et al., 2022; Ryali et al., 2023).
Withregardstoselectingmemoriestoretain,recentworkonmemory-augmentedtransformers(Bulatov et al.,
2024) and on SSMs that can select and retain memories in their state-space (Dao & Gu, 2024) each pro-
videresearchdirectionstowardsthisgoal,thoughthereisstillworktobedone. Evenifmemoryselection
remains challenging, less efficient reasoning models will be possible in the meantime.
3.4 Safety concerns
As new capabilities are introduced to AI models, it is important to monitor these frontier models for
potentialsafetyrisks(Phuong et al.,2024). FromanAIcontrolperspective,MLagentswhichcanreason
and strategically plan present a much larger risk than passive models which merely predict things. Like
any ML model in deployment, there is a societal risk that the model’s learnt biases from its training
distribution will result in its behaviour diverging from human aspirations.
But more importantly, such a model raises the existential risk from AI models. Models which can
reason can use their abilities to plan and strategize, potentially over the long-term. If allowed to act
autonomously to achieve a goal, they may erroneously or surreptitiously plan with subgoals that involve
takingcontrolofresourcestheyshouldnothaveaccesstoo,etc. Tomitigatetheseconcerns,itisimportant
that training data be screened to ensure it does not contain instructions we would not wish an agent to
take when deployed in the wild.
Anotherconcernregardsthescrutibilityofreasoningagents. CurrentLLMsmustalwaysprojecttheir
chain-of-thought reasoning steps to English, though there are concerns that their internal computation
may not be fully reflected in their outputs (Pfau et al., 2024; Lyu et al., 2023; Lanham et al., 2023).
From a gainof function perspective, it may be advantageousto train models that can reasonin abstract
conceptsthatdonotdirectlycorrespondtotokensinthetrainingcorpus. However,weareofthe opinion
thatsteps must alwaysbe takento ensurethat modelreasoningis projectedinto a frame (be it language
or imagery) in which it can be explicitly and as completely as possible communicated to humans.
4 Conclusions
We have discussedthe literature surrounding the philosophy of human inner thought and reasoning,and
thecurrentneuralnetworkapproachesto reasoningmodels. The currentnetworkshavestronganalogues
5to processes ascribed to human reasoning. We thus argue they already achieve reasoning, though to
limited degrees due to either their limited domains or lack of explicit training.
Fromthis,weproposeapipelinewhichcombinesseveralexistingtechniquesfromthemachinelearning
literature together as a candidate for how a reasoning agent could be explicitly trained to reason. By
expandingthebreadthoftrainingdatatoincludericher,raw,temporalstimulisuchasvideo,weanticipate
the model can achieve a more capable world model. Thus we conclude that neural reasoning models are
either already here, or if not they will be soon.
Acknowledgements
Manythanksto DavidEmerson,MichaelZhang,andIulia Eyriayfor insightfuldiscussionsandfeedback,
and to Philip from AI Explained for providing the initial inspiration (AI Explained, 2024).
References
AI Explained. o1 - what is going on? why o1 is a 3rd paradigm of model + 10 things you might not
know, 2024. URL https://www.youtube.com/watch?v=KKF7kL0pGc4.
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini,
A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D.,
Tran-Johnson, E., Perez, E., Kerr, J., Mueller, J., Ladish, J., Landau, J., Ndousse, K., Lukosuite, K.,
Lovitt, L., Sellitto, M., Elhage, N., Schiefer, N., Mercado, N., DasSarma, N., Lasenby, R., Larson, R.,
Ringer, S., Johnston, S., Kravec, S., Showk, S. E., Fort, S., Lanham, T., Telleen-Lawton, T., Conerly,
T., Henighan, T., Hume, T., Bowman, S. R., Hatfield-Dodds, Z., Mann, B., Amodei, D., Joseph, N.,
McCandlish, S., Brown, T., and Kaplan, J. Constitutional AI: Harmlessness from AI feedback, 2022.
URL https://arxiv.org/abs/2212.08073.
Bainbridge,W.A.,Pounder,Z.,Eardley,A.F.,andBaker,C.I.Quantifyingaphantasiathroughdrawing:
Those without visual imagery show deficits in object but not spatial memory. Cortex, 135:159–172,
2021. ISSN 0010-9452. doi:10.1016/j.cortex.2020.11.014.
Bardes,A.,Garrido,Q.,Ponce,J.,Rabbat,M.,LeCun,Y.,Assran,M.,andBallas,N. Revisitingfeature
prediction for learning visual representations from video. arXiv:2404.08471, 2024.
Bulatov, A., Kuratov, Y., Kapushev, Y., and Burtsev, M. S. Scaling transformer to 1M tokens and
beyond with RMT, 2024. URL https://arxiv.org/abs/2304.11062.
Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers,
2019. URL https://arxiv.org/abs/1904.10509.
Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P.,
Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethink-
ing attention with performers. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=Ua6zuk0WRH.
Christensen,B.Problematicassumptionsinincubationeffectstudiesandwhattodoaboutthem.Creative
Cognition: Analogy And Incubation, 2005.
Christiano, P., Shlegeris, B., and Amodei, D. Supervising strong learners by amplifying weak experts,
2018. URL https://arxiv.org/abs/1810.08575.
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton,
J.,Nakano,R.,Hesse,C.,andSchulman,J. Trainingverifierstosolvemathwordproblems,2021. URL
https://arxiv.org/abs/2110.14168.
6Dao,T.FlashAttention-2: Fasterattentionwithbetterparallelismandworkpartitioning.InInternational
Conference on Learning Representations (ICLR), 2024.
Dao, T. and Gu, A. Transformers are SSMs: Generalized models and efficient algorithms through
structured state space duality. arXiv preprint arXiv:2405.21060, 2024.
Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. FlashAttention: Fast and memory-efficient exact
attentionwith IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS),2022.
Evans, J. S. B. T. Dual-processing accounts of reasoning, judgment, and social cognition. Annu. Rev.
Psychol., 59(1):255–278,2008.
Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., and Feichtenhofer, C. Multiscale vision
transformers. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 6804–
6815,2021. doi:10.1109/ICCV48922.2021.00675.
Gilhooly, K. J. Incubation and intuition in creative problem solving. Frontiers in Psychology, 7, 2016.
ISSN 1664-1078. doi:10.3389/fpsyg.2016.01076.
Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint
arXiv:2312.00752, 2023.
Gu, A., Goel, K., and Ré, C. Efficiently modeling long sequences with structured state spaces. In The
International Conference on Learning Representations (ICLR), 2022.
Hinwar, R. P. and Lambert, A. J. Anauralia: The silent mind and its association with aphantasia.
Frontiers in Psychology, 12, 2021. ISSN 1664-1078. doi:10.3389/fpsyg.2021.744213.
Hurlburt, R. and Heavy, C. Exploring Inner Experience: The Descriptive Experience Sampling Method.
Advances in consciousness research. John Benjamins Pub., 2006. ISBN 9789027252005.
Hurlburt, R. and Schwitzgebel, E. Describing Inner Experience?: Proponent Meets Skeptic. Life and
Mind: Philosophical Issues in Biology and Psychology.MIT Press, 2011. ISBN 9780262516495.
Hurlburt,R.T.,Heavey,C.L.,andKelsey,J.M. Towardaphenomenologyofinnerspeaking. Conscious-
ness and Cognition, 22(4):1477–1494,2013. ISSN 1053-8100. doi:10.1016/j.concog.2013.10.003.
James, W. The Principles of Psychology, Vol. 1. Number v. 1. Dover Publications, 2012. ISBN
9780486123493.
Jung, J., Qin, L., Welleck, S., Brahman, F., Bhagavatula, C., Le Bras, R., and Choi, Y. Maieutic
prompting: Logically consistent reasoning with recursive explanations. In Goldberg, Y., Kozareva,
Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 1266–1279,Abu Dhabi, United Arab Emirates, December 2022. Association for
Computational Linguistics. doi:10.18653/v1/2022.emnlp-main.82.
Kahneman, D. Thinking, fast and slow. Penguin, London, 2012. ISBN 97801410335700141033576.
Keogh, R., Wicken, M., and Pearson, J. Visual working memory in aphantasia: Retained ac-
curacy and capacity with a different strategy. Cortex, 143:237–253, 2021. ISSN 0010-9452.
doi:10.1016/j.cortex.2021.07.012.
Konda, V. and Tsitsiklis, J. Actor-critic algorithms. In Solla, S., Leen, T., and Müller, K.
(eds.), Advances in Neural Information Processing Systems, volume 12. MIT Press, 1999. URL
https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pd
7Lanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Denison, C., Hernandez, D., Li, D., Durmus, E.,
Hubinger, E., Kernion,J., Lukošiu¯te˙, K., Nguyen, K., Cheng, N., Joseph, N., Schiefer, N., Rausch, O.,
Larson, R., McCandlish, S., Kundu, S., Kadavath, S., Yang, S., Henighan, T., Maxwell, T., Telleen-
Lawton,T.,Hume,T.,Hatfield-Dodds,Z.,Kaplan,J.,Brauner,J.,Bowman,S.R.,andPerez,E. Mea-
suring faithfulness in chain-of-thought reasoning, 2023. URL https://arxiv.org/abs/2307.13702.
Lee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J., Lu, K., Bishop, C., Hall, E., Carbune, V.,
Rastogi, A., and Prakash, S. RLAIF vs. RLHF: Scaling reinforcement learning from human feedback
with AI feedback, 2024. URL https://arxiv.org/abs/2309.00267.
Li, Y., Wu, C.-Y., Fan, H., Mangalam, K., Xiong, B., Malik, J., and Feichtenhofer, C.
MViTv2: Improved multiscale vision transformers for classification and detection. In 2022
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4794–4804, 2022.
doi:10.1109/CVPR52688.2022.00476.
Li, Z., Liu, H., Zhou, D., and Ma, T. Chain of thought empowers transformers to solve inherently
serial problems. In The Twelfth International Conference on Learning Representations, 2024. URL
https://openreview.net/forum?id=3EWTEy9MTM.
Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J.,
Sutskever, I., and Cobbe, K. Let’s verify step by step. In The Twelfth International Conference
on Learning Representations, 2024. URL https://openreview.net/forum?id=v8L0pN6EOi.
Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,S.,andGuo,B. Swintransformer: Hierarchical
vision transformer using shifted windows. In 2021 IEEE/CVF International Conference on Computer
Vision (ICCV), pp. 9992–10002,2021. doi:10.1109/ICCV48922.2021.00986.
Long,J.Largelanguagemodelguidedtree-of-thought,2023.URLhttps://arxiv.org/abs/2305.08291.
Lyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong, E., Apidianaki, M., and Callison-Burch,
C. Faithful chain-of-thought reasoning. In Park, J. C., Arase, Y., Hu, B., Lu, W., Wijaya, D.,
Purwarianti, A., and Krisnadhi, A. A. (eds.), Proceedings of the 13th International Joint Conference
on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association
for Computational Linguistics (Volume1: Long Papers),pp.305–329,NusaDua,Bali,November2023.
Association for Computational Linguistics. doi:10.18653/v1/2023.ijcnlp-main.20.
Miller, G. A. The magical number seven plus or minus two: some limits on our capacity for processing
information. Psychol. Rev., 63(2):81–97,March 1956.
Monzel, M., Vetterlein, A., and Reuter, M. Memory deficits in aphantasics are not restricted to autobi-
ographical memory – perspectives from the dual coding approach. Journal of Neuropsychology, 16(2):
444–461,2022. doi:10.1111/jnp.12265.
OpenAI.LearningtoreasonwithLLMs,2024.URLhttps://openai.com/index/learning-to-reason-with-llms/.
Pfau, J., Merrill, W., and Bowman, S. R. Let’s think dot by dot: Hidden computation
in transformer language models. In First Conference on Language Modeling, 2024. URL
https://openreview.net/forum?id=NikbrdtYvG.
Phuong, M., Aitchison, M., Catt, E., Cogan, S., Kaskasoli, A., Krakovna, V., Lindner, D., Rahtz, M.,
Assael, Y., Hodkinson, S., Howard, H., Lieberum, T., Kumar, R., Raad, M. A., Webson, A., Ho, L.,
Lin, S., Farquhar, S., Hutter, M., Deletang, G., Ruoss, A., El-Sayed, S., Brown, S., Dragan, A., Shah,
R., Dafoe, A., and Shevlane, T. Evaluating frontier models for dangerous capabilities, 2024. URL
https://arxiv.org/abs/2403.13793.
8Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and Ré,
C. Hyena hierarchy: Towards larger convolutional language models. In International Conference on
Machine Learning, pp. 28043–28078.PMLR, 2023.
Potter, M. C., Wyble, B., Hagmann, C. E., and McCourt, E. S. Detecting meaning in rsvp at 13
ms per picture. Attention, Perception, & Psychophysics, 76(2):270–279, Feb 2014. ISSN 1943-393X.
doi:10.3758/s13414-013-0605-z.
Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference opti-
mization: Yourlanguagemodelissecretlyarewardmodel. Advances in NeuralInformation Processing
Systems, 36, 2024.
Rai, D., Zhou, Y., Feng, S., Saparov, A., and Yao, Z. A practical review of mechanistic interpretability
for transformer-basedlanguage models, 2024. URL https://arxiv.org/abs/2407.02646.
Ryali,C.,Hu,Y.-T.,Bolya,D.,Wei,C.,Fan,H.,Huang,P.-Y.,Aggarwal,V.,Chowdhury,A.,Poursaeed,
O.,Hoffman,J.,Malik,J.,Li,Y.,andFeichtenhofer,C.Hiera: ahierarchicalvisiontransformerwithout
the bells-and-whistles. In Proceedings of the 40th International Conference on Machine Learning,
ICML’23. JMLR.org, 2023.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J.,
Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner,
N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering
thegameofgowithdeepneuralnetworksandtreesearch. Nature,529(7587):484–489,Jan2016. ISSN
1476-4687. doi:10.1038/nature16961.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L.,
Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., and
Hassabis, D. Mastering the game of Go without human knowledge. Nature, 550(7676):354–359, Oct
2017. ISSN 1476-4687. doi:10.1038/nature24270.
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Ku-
maran,D.,Graepel,T.,Lillicrap,T.,Simonyan,K.,andHassabis,D. Ageneralreinforcementlearning
algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.
doi:10.1126/science.aar6404.
Snell,C.,Lee,J.,Xu,K.,andKumar,A. ScalingLLMtest-timecomputeoptimallycanbemoreeffective
than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314.
Stanovich, K. E. and West, R. F. Advancing the rationality debate. Behavioral and Brain Sciences, 23
(5):701–717,2000. doi:10.1017/S0140525X00623439.
Sutton, R. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019.
Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and
Higgins, I. Solving math word problems with process- and outcome-based feedback, 2022. URL
https://arxiv.org/abs/2211.14275.
Wason, P. and Evans, J. Dual processes in reasoning? Cognition, 3(2):141–154, 1974. ISSN 0010-0277.
doi:10.1016/0010-0277(74)90017-1.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q. V., and
Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. In Koyejo, S.,
Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural In-
formation Processing Systems, volume 35, pp. 24824–24837. Curran Associates, Inc., 2022. URL
https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Con
9Yang,Z.,Qi,P.,Zhang,S.,Bengio,Y.,Cohen,W.W.,Salakhutdinov,R.,andManning,C.D.HotpotQA:
A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods
in Natural Language Processing (EMNLP), 2018.
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K.
Tree of thoughts: Deliberate problem solving with large language models, 2023. URL
https://arxiv.org/abs/2305.10601.
Zelikman, E., Wu, Y., Mu, J., and Goodman, N. STaR: Bootstrapping reasoning with reasoning. In
Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural
Information Processing Systems, volume 35, pp. 15476–15488. Curran Associates, Inc., 2022. URL
https://proceedings.neurips.cc/paper_files/paper/2022/file/639a9a172c044fbb64175b5fad42e9a5-Paper-Con
Zhang, C., Van Durme, B., Li, Z., and Stengel-Eskin, E. Visual commonsense in pretrained unimodal
andmultimodal models. In Carpuat, M., de Marneffe,M.-C., and Meza Ruiz, I. V. (eds.), Proceedings
of the 2022 Conference of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pp. 5321–5335,Seattle, United States, July 2022.Association for
Computational Linguistics. doi:10.18653/v1/2022.naacl-main.390.
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano,
P., and Irving, G. Fine-tuning language models from human preferences, 2020. URL
https://arxiv.org/abs/1909.08593.
10