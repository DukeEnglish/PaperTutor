GenSim2:
Scaling Robot Data Generation with
Multi-modal and Reasoning LLMs
PuHua1∗MinghuanLiu2,3∗AnnabellaMacaluso2∗YunfengLin3
WeinanZhang3HuazheXu1LiruiWang4‡
InstituteforInterdisciplinaryInformationSciences,TsinghuaUniversity1,
UCSD2,ShanghaiJiaoTongUniversity3,MITCSAIL4
https://gensim2.github.io/
Abstract: Robotic simulation today remains challenging to scale up due to the
humaneffortsrequiredtocreatediversesimulationtasksandscenes. Simulation-
trained policies also face scalability issues as many sim-to-real methods focus
on a single task. To address these challenges, this work proposes GenSim2, a
scalableframeworkthatleveragescodingLLMswithmulti-modalandreasoning
capabilities for complex and realistic simulation task creation, including long-
horizon tasks with articulated objects. To automatically generate demonstration
dataforthesetasksatscale,weproposeplanningandRLsolversthatgeneralize
withinobjectcategories. Thepipelinecangeneratedataforupto100articulated
tasks with 200 objects and reduce the required human efforts. To utilize such
data,weproposeaneffectivemulti-tasklanguage-conditionedpolicyarchitecture,
dubbedproprioceptivepoint-cloudtransformer(PPT),thatlearnsfromthegener-
ateddemonstrationsandexhibitsstrongsim-to-realzero-shottransfer.Combining
theproposedpipelineandthepolicyarchitecture,weshowapromisingusageof
GenSim2 that the generated data can be used for zero-shot transfer or co-train
with real-world collected data, which enhances the policy performance by 20%
comparedwithtrainingexclusivelyonlimitedrealdata.
Keywords: CodingLargeLanguageModels, VisionLanguageModels, Robotic
Simulation,Multi-taskSim-to-RealTransfer,ArticulatedObjectManipulation
1 Introduction
Robot learning requires large amounts of interaction data and evaluation, which are expensive to
acquire at scale in the real world. Robot simulation holds the promise of providing such data and
verification in high diversity and efficiency across objects, tasks, and scenes. While the ability to
simulate has led to many successes in AI across Gaming, Go, and Mathematical Proofs [2, 3, 4],
therearetworequirementsforsuchapathtobesuccessfulinrobotics: Thedataneedstoscalein
complexity without significant human efforts and the data needs to be realistic enough to transfer
totherealworld. Previousworks[5,6,7,8,9,10,11]havemadesignificantprogressinscalable
simulationbenchmarksinroboticsandtrainingpoliciesonthesimulationdata.
Foundation models [12], particularly generative models pre-trained on internet-scale data [13, 14,
15],havedemonstratedimpressivecapabilitiesrequiredforgeneratingrobotsimulationtasks,such
ascoding[16],spatialreasoning[17],tasksemantics[9],planning[18,19],videoprediction[20,21],
andcostandrewardunderstanding[22,23]. Whilefoundationmodelshaveshownimpressivecapa-
bilitiestooutputactionstosolverobotictasksdirectlyintherealworld[24],simulationprovidesa
low-costandscalableplatformtolearnrobustend-to-endpolicies. Inadditiontogeneratingnumer-
ous tasks, automatically solving them to produce demonstration data and transferring the learned
∗equalcontribution.‡projectlead.
4202
tcO
4
]OR.sc[
1v54630.0142:viXra(A) Large-scale Task and Data Generation
(B) Multi-Task Training in Simulation
Scene:
Random Pose Object:
Random Instance Pointcloud:
Random Cropping
(C)Sim-to-Real Transfer
Figure 1. GenSim2 introduces a scalable task and data generation framework in SAPIEN [1] for
articulation objects with multi-modal and reasoning LLMs. The framework comprises three main
stages: 1) (Top) We first generate large-scale robotics tasks and collect massive data with LLMs;
2) (Bottom-left) Then, we train a multi-task point cloud-based policy in simulation with imitation
learning;3)(Bottom-right)Finally,wezero-shottransferthepolicytotherealworld.
skillstotherealworldpresentsignificantchallenges. Toaddressthis,thedatagenerationandpolicy
learningpipelinesmustscaleefficientlywhileminimizingthesim-to-realgaps.
In this work, we propose GenSim2 (Fig. 1), a scalable framework that amplifies robotics data by
generating diverse articulated and long-horizon simulation tasks and demonstrations. In addition
tothesemanticknowledgeandcompositionalcapabilitiesinlanguagemodels, thesetasksalsore-
quireextended6-dofmotionswithcontacts. Togeneratedemonstrationsforsuchtasks,wedevelop
varioussolvers,includingakeypoint-basedmotionplannerthatgeneralizesacrossobjectcategories
andinitialconditions. Moreover,weusethevisualinformationinamulti-modalLLM(MLLM)to
iterativelygenerateandverifynoveltasks,totheextentof100taskstotrainamulti-taskpolicy.
GenSim2tacklesmorechallengingarticulatedandlong-horizontasksbeyondtop-downpick-and-
place[9]. ThesecomplextasksrequiregroundingandreasoningcapabilitiesbeyondpreviousLLM
techniques.Therefore,GenSim2isanagentpipelinefeaturingmulti-modalfoundationmodels(e.g.
GPT-4V)andreasoningmodels(e.g. OpenAIo1)aswellasgeneralizedkeypointplannersforsuch
complex tasks. To fully utilize the generated data, we further propose a sim-to-real native policy
architecture, dubbed proprioceptive point-cloud transformer (PPT), with point-cloud observations
andlanguageconditioning. Thiseffectivearchitectureisdesignedforsim-to-realtransferwiththe
large-scaledatageneratedinsimulation.
Inourexperiments,wehavegeneratedover100articulatedobjectmanipulationtaskswith35objects
(over 200 instances), including 50 long-horizon tasks. Our task generation pipeline achieves 25%
bettersuccessratesthanpreviousworks. Afterusinganautomatedpipelinetosolvethesetasksand
generate task demonstrations, our multi-task policy can jointly solve 24 tasks with random scene
configurationsandonlysuffera3%performancedropwhentestedonunseenobjectinstances.More
importantly,byintegratingtheGenSim2pipelinewiththedesignedpolicyarchitecture,weachieve
2promisingresultsacross8real-worldarticulatedobjecttasks,throughsuchaszero-shottransferand
co-trainingwithreal-worlddata. Thisapproachleadstoa20%improvementinpolicyperformance
comparedtotrainingexclusivelyonlimitedteleoperationdata.
Insummary,weshowapromisingwaytoreducedatacollectioneffortsandsolvereal-worldprob-
lemsthroughmassivesimulateddatagenerationwithMLLMs. Ourcontributionsareasfollows:(1)
Wedesignaroboticsimulationtaskgenerationpipelinethatexplorestheusageofinternet-scalevi-
sualandlanguageknowledgeembeddedincodingmulti-modalLLMs(e.g.GPT-4V),togenerateup
to100articulatedobjectmanipulationtasks. (2)Weproposeasimulateddemonstrationgeneration
pipeline that can generate high-quality data with minimal labeling efforts. The solution integrates
smoothmotionsfrommodel-basedplannersthathavespatialandobject-levelgeneralization.(3)We
developanovelpolicyarchitectureforseamlesssim-to-realtransferthatdistillsmulti-taskdemon-
strationefficientlyandwithstrongsim-to-realperformance.
2 RelatedWork
TaskGenerationinRoboticSimulation. Trainingandevaluationforroboticshavegreatlybene-
fitedfromdevelopmentsinroboticsimulationsuchasMujoco, NVIDIAIsaac, Drake, andSapien
[25,26,27,1]. Withthesetools,researchersandengineershavebeenabletodevelopbenchmarks
withtenstohundredsofunique,hand-designedrobotictasks[28,29,30,31,32,33,28]. However,
the requirements of high-quality assets, scenes, and task design along with required verifications
fortasksolvabilityandreal-worldtransferdemandasignificantamountofhumanskillandeffort.
Recent works have explored methods such as domain randomizations [34, 7, 35, 36], procedural
assetgeneration[8,37]andtext-to-3Dsynthesis[38,39,40,41]tomitigatesomeoftheseefforts.
Researchers also investigated how generative models can generate suites of robotic or agent tasks
[23,42,43,44,45,9,10,11]orgenerateinteractivesimulationsdirectlyfromvideos[20,21]. In
particular, GenSim[9]developsanovelpipelinethatgeneratesover100simulationtasksutilizing
LLMs,verifiesthesetasks,usesthemtogeneratedata,andtrainsmulti-taskpoliciesontopofthis
data. Similarly,RoboGen[10]usesRLtosolvecomplicatedlocomotiontasksanddeformablema-
nipulation and RoboCasa [11] generates massive high-fidelity tasks for multiple embodiments in
mobilemanipulationtasks. Inourwork,weleveragemulti-modalLLMstogenerate6-DOFrobotic
manipulationandlong-horizontasksatscale. Furthermore,wefocusoncomplexyetrealistictasks
thatcansolvedwithdifferentsolversandtransferredtotherealworld.
Multi-Task Policy Learning with 3D Information. With the surge of robotic data, recent works
have explored multi-task policy learning, usually conditioned on language inputs. In particular,
policy learning often benefits from access to explicit 3D information when doing generalized 6-
DOFtasks[46,47]. [48]usesRGB-Dinputsandoutputpixel-levelaffordancemap. [46]usesvoxel
aspolicyinputinformationandoutputkeyframeactions. [47,49]usespointcloudasinputstothe
policies. [50]useskeypointsatthecornerstomanipulateboxeswithdexteroushands. Inourwork,
wefocusonpoint-cloudtransformersasmulti-taskpoliciesforarticulatedobjectmanipulation.
Sim-to-RealTransfer. Whilesimulationprovidesscalabletrainingdataandevaluationruns, sim-
to-realtransfer[51,52]isachallengeandanactiveresearcharea.RL-basedcontrollers[53]leverage
simulationenvironmentsforlarge-scaleinteractionstolearnrobustbehaviors. Previousworkshave
also explored techniques such as domain randomizations [54, 34], domain adaptations [55], and
policy composition [56] to improve sim-to-real transfer. Our work can be viewed as a distillation
processfromtheknowledgeinfoundationmodelssuchasMLLMs[14,13],intopolicies,whichcan
haveasmallersim-to-real“semantic”gapcomparedtorule-basedsimulationdata.
3 GeneratingTasksandTrainingPoliciesatScale
The proposed GenSim2 framework executes a series of processes including task proposal (3.1),
demonstrationgeneration(3.2),andpolicylearningandtransfer(3.3). Weillustratethepipelinein
Fig.2. OurworkenhancesGenSim[9]attheleveloftaskcomplexitybeyondtop-downpick-and-
3Task Proposal Data Generation Multi-Task Training Evaluation&Transfer
Asset&TaskLibrary kPAM Motion Planner DemonstrationCollection Spatial & Object Level
Generalization
MLLM
Constraints:
LLM (p1, p2),
(v1, v2, align), Simulation
…
task description assets env code
MLLM
“ p u t a b a l l
in a box” , , “open box”
Pre-actuation Post-actuation
LLM motions motions Multi-Task Policy Training Sim2Real Transfer
Sub-task 1 Sub-task 2 Sub-task 3 RL Policy Learner
“...” “...” MLLM
“...”
Task Decomposition Reward TrP aiP nO ing
Figure2. OverviewofGenSim2framework. Thepipelineconsistsof(1)taskproposal,(2)solver
creation,(3)multi-tasktraining,and(4)generalizationevaluationandsim-to-realtransfer.
place,whichnecessitatesgroundedmulti-modaltaskdesigns,generic6-dofmotionplannersandRL
solvers,andscalablepolicyarchitectures.
3.1 TaskProposal
Primitive Task. Our pipeline begins with proposing primitive tasks, namely tasks with a single
simple motion (e.g. opening a box), by prompting a LLM to generate a novel task, defined by its
taskdescription(ashortphraseorsentence),usedassetsandtaskcode. Westartfromafixedasset
libraryandasmalltasklibraryinitializedwithhand-designedexampletasks.Weparsethemintothe
promptsforin-contextlearning. WequerytheLLMtodiscoverapplicableassetsforcreatinganew
task. The LLM then takes in as input the generated task definition and outputs the corresponding
taskcode. ThiscodeisfinallycompiledandreadyfordemonstrationgenerationinSection3.2.
Long-horizonTask. Moreover,weextendourpipelinetogeneratelong-horizontasksthatconsist
ofmultiplestepsofprimitivetasksandinvolvemanipulatingarticulatedandrigid-bodyobjects. Ex-
amples include opening a box, placing a ball inside, and then closing the box. When generating
along-horizontask,weexecuteataskdecompositionprocessbetweentaskproposalandcodeim-
plementation, todecomposethelong-horizontaskintoseveralsub-tasks(primitivetasks). Weuse
twodistinctmethodsforgeneratinglong-horizontasks: (1)Top-down: Wedirectlygenerateatask
in a curriculum, then decompose the task into several sub-tasks, each with a dedicated solver. (2)
Bottom-up: We first generate primitive tasks and build up a task library. Then the LLM will be
promptedtoselecttasksfromthepre-builttasklibrarytocomposeanewtask. Wehaveobserved
reasoningLLMtoimprovetaskproposalperformanceinthisstage. Afterwehavefinishedthetask
planning,wecontinuetogeneratesolversanddemonstrationsforthesubtasksinsuccession. Fig.9
demonstratesvisualizationsofsomegeneratedlong-horizontasks.
3.2 DemonstrationGeneration
In this section, our goal is to create a solver to generate demonstrations given a task (or sub-task)
code. The objective of our generation pipeline is to collect large-scale, high-quality data for gen-
eral 6-DOF manipulation task learning in the real-world setting, our task solver should meet the
following requirements: 1) Universal for 6-DOF tasks without task-specific designs; 2) Robust to
differentscene configurations; 3)Fasttoexecute; 4) Deterministicwithhigh success rates. While
suchasolveriseasytodesignfortop-downpick-and-place,inwhichweonlyneedtospecifysome
2Dpositiononthetableaswaypointsandexecuteprimitive“pick”and“place”actions,itbecomes
challengingformorecomplextaskssuchasgeneralarticulatedobjectmanipulation.
4Scene Image Constraints: Actuation
LLM ( (, , , ,c po ain rc ai ld lee l)n ,t), Pose
C
a
a nn
o
vy eo lu
t
ag se kn erate ( , , …or …thogonal)
“open-the-box” Descrip. MLLM
asset library & Code keypoint info Simulation
Solver Creation
Task Proposal 1: Constraints Rejection Sampling
Solver Creation 2: Actuation Motions
ActuationMotions Now generate the
actuation motions for
Trajectory Data P (Tr re a- na sc lat tu ea _t xio , n -0: .05), the solver config
(Translate_z, -0.15)
Post-actuation: Actuation Pose
(Rotate, 30)
𝑧 MLLM
Simulation 𝑥
PMroet-iaocntuation PMoostti-oanctuation
Rejection Sampling
Figure 3. Multi-modal Task Generation Pipeline. GenSim2 first is prompted to generate the
task code, given few-shot examples and available assets. It then renders the scene image and the
keypoint information of the task assets is fed into the MLLM model to generate a planner config
fortheactuationpose. Theactuationposeisthenextendedtoactuationmotions,whicharefedinto
GPT-4Vforinspections. Inthisexample,thepipelineproducesataskmotionforopeningthebox.
To address this challenge, we propose and investigate the planner config generation with a multi-
modalLLM[14]. Wealsoproposetwotypesoftasksolvers: akPAM-basedmotionplanneranda
complementaryRL-basedpolicylearner(seeinAppendixA.3),tosolvethegeneratedtasks.
MotionPlanner. kPAM[57]isakeypoint-basedplannerproposedforcategory-levelmanipulation
tasks. It defines a robotic task by an actuation pose, namely the homogeneous transformation re-
quired to manipulate the target object, and it addresses an optimization problem based on several
keypoint-based constraints to get this pose. Following Wang et al. [33], we improve and extend
kPAM to produce an object-centric trajectory of end-effector poses, termed actuation motions, to
solve a generated task. We parse the constraints and actuation motions into parameterized config-
urationsthatareeasilyinterpretableandcodable. WeprovideanillustrationinFig.7toshowhow
thekPAMplannersolvesatask.
Multi-modalTaskSolverGeneration.Comparedtopreviousworksthatprimarilyutilizelanguage
prompts, our approach incorporates visual information and key points [58, 59, 60], serving as an
explicitrepresentationofscenedetails(e.g.,objectstructures,affordanceandspatialinformation).
The pipeline of solver generation is depicted in Fig. 7. First, we execute the generated code and
capturesceneimages. Thisvisualdata,togetherwithobjectkeypoints,arethenusedtopromptan
MLLM to generate the planner constraints. Subsequently, we visualize the actuation pose defined
by these constraints. Finally, we query the MLLM to generate the actuation motions. Optionally,
wealsointroducerejectsamplingtoguidetheMLLMinrefiningitspreviousoutputs.
3.3 PolicyLearningandTranfer
Multi-TaskTraining. Wedesignamulti-taskpolicystructure,denotedProprioceptionPointcloud
Transformer(PPT),asillustratedinFig.4.Specifically,wehandlethreekindsofobservations:point
cloud, proprioceptive states, and language task description, all of which can be obtained from the
real world. Each observation is separately tokenized via respective encoders and cross-attention,
fusedtogetherinthesharedlatentspacethroughtransformerblocks,andpost-processedintoglobal
conditiontokens[61]. Giventheglobalconditiontokens,thepolicypredictsasequenceofactions
throughthepolicyhead. Inourimplementation,wetestagainstvariouspolicyheadssuchasMLPs,
thetransformerdecoder[62],andthediffusionmodel[63]. SeeAppendixB.2formoredetails.
5Figure 4. The proposed Proprioception Point-cloud Transformer (PPT) policy architecture maps
language,pointcloud,andproprioceptioninputsinasharedlatentspaceforactionprediction. The
policyactionheadsupportsvariousarchitecturesfromdiffusion[63]totransformerdecoder[62].
0.8 0.78 M Reu alt si- om no ind ga l L L LL MM 0.8 1 3- -s sh ho ot t 0.7 0.8 0.78 G w/e on CS oim T2
LLM 5-shot
0.6 0.6
0.6
0.5
0.44
0.4 0.36 0.4 0.4
0.2 0.18 0.2 0.2
0.1
0 0 0
Figure5. Ablationstudyoncomponentsofourgenerationpipeline. Allresultsarebasedonno
less than 10 generations. Left) We use various types of LLMs for solver generation and find that
multi-modalLLM(GPT-4V)outperformstheothers. Middle)Wefindthattheperformanceofour
methodwillincrease,asweincreasethemaximumiterationforrejectsampling. Right)Weobserve
thatsplittingsolvergenerationintoapromptchainsurpassesgeneratingthewholesolverconfig.
Sim-to-RealTransfer. Totransferbetweensimulationandrealenvironments,weignorethecolor
informationofthepointcloud, andaugmentthepointcloudobservationsofsimulateddemonstra-
tionsduringtraining. Thisaugmentationinvolvescropping,addingGaussiannoiseasperturbations,
and randomly dropping points. In the real-world setting, we process the actual point cloud using
uniform sampling, farthest point sampling, and outlier removal. This processed clean point cloud
servesasinputtothemulti-taskpolicyduringinference.
4 Experiment: TaskandDataGeneration
Inthissection,weaimtoverifythefeasibilityofthetaskgenerationframeworkandinvestigatethe
effectivenessofthetaskanddatagenerationpipeline.
EvaluationCriteria. Weevaluate thetaskgeneration proceduremainly ontwotypes ofsuccess
rates: 1)executionrate,thesuccessrateofcompletingthewholepipelinewithoutanyerrorsuchas
syntaxorruntimeerror;2)solutionrate,thesuccessrateofsolvingthegeneratedtask.
Ablation Study. In this section, we conduct a careful ablation study on each component of
GenSim2. Theresults,depictedinFig.13,aresummarizedasfollows:
Types of LLMs: We test multi-modal LLM, reasoning LLM, and vanilla LLM on solution rate in
Fig. 5 left. Without visual data as input, the performance of the pipeline deteriorates with signifi-
cantlylowersuccessratesintaskgenerationunderbothcriteria.Theperformancedropsbecausethe
constraintsforactuationposecannotbecorrectlygeneratedasitrequiresdetailsforobjectstructure.
6
etaR
sseccuS
etaR
sseccuS
etaR
sseccuSTable 1. Comparison with RoboGen on task generation success rates. GenSim2-B represents the
bottom-up long-horizon task proposal method, while GenSim2-T and GenSim2-T (o1) represent
top-downmethodsusingGPT-4andOpenAI-o1respectively.
Type Primitive Long-horizon
Method GenSim2 RoboGen GenSim2-B GenSim2-T GenSim2-T(o1) RoboGen
Execution 0.94 0.94 1.00 0.83 0.87 0.76
Solution 0.78 0.58 0.68 0.54 0.60 0.43
Meanwhile,theactuationmotionsalsosufferfromlargeerrorsbecausethevanillaLLMhasnoac-
cesstospatialrelationshipsbetweenobjects.Thishighlightsthecrucialroleofvisualinformationin
thesuccessofGenSim2,asitprovidesessentialdetailsaboutthescenesuchasspatialrelationships
and the appearance of objects. Moreover, reasoning capability also improves vanilla LLM’s score
onsolvergeneration.Wehypothesizethatthereasoningtraceof“think”beforecoding,basedonthe
taskandconfigstructurecanreducehallucinationsingeneratingkPAMconstraints. SeeAppendix
E.2andE.3formoredetails.
Multi-shotRejectSampling: InFig.5middle,wehavevisualizedthesolutionratesofourpipeline
underdifferentmaximumiterationsofmulti-shotrejectionsampling. Theresultsindicatethatour
pipelinewithmulti-shotrejectionsamplingsurpassestheversionwithoutit,highlightingtheimpor-
tance of self-reflection in LLMs. Additionally, allowing LLMs more opportunities to refine their
answers through iterations leads to improved performance. We also explore the use of rejection
samplingfromGPT-4V,whichtakessolverconfigvisualizationsasinputanddeterminestheireffec-
tiveness,inourpipeline.Duetothedomaingapsofvisualperceptionandunderstandingcapabilities
ofMLLMs,particularlyinprocessing3Droboticscenes([64,65]),theMLLMcangenerateirrele-
vantresponses. Morepowerfulmodelsandadvancedpromptingandfinetuningmethodsarelikely
toimprovetheseinfuturework.
Chain-of-thought Prompt Design: We observe that dividing solver generation into a prompt chain
(firstgeneratingconstraintsandthenactuationmotions)yieldsbetterresults(over30%)thandirectly
outputtingthecompletesolveratonceinFig.5right. Thisapproachallowsforfocused,sequential
processingofcomponents,leveragingoutputsfromearlierstagesforsubsequentones.
ComparisonwithBaseline. WecomparethepipelineofGenSim2usingakPAMmotionplanner
with another open-sourced framework RoboGen[10], which encompasses a large number of long-
horizonarticulatedtasks. Forprimitivetasks,weconsiderthesub-taskswithinalong-horizontask
in RoboGen for comparison. For long-horizon tasks, we compare our top-down methods (labeled
asGenSim2-TandGenSim2-T(o1),utilizingGPT-4andOpenAI-o1astaskdecomposers,respec-
tively)andbottom-upmethod(GenSim2-B)withRoboGen’slong-horizontasks.
Theresults,presentedinTab.1,indicatethatourmethodexceedsthebaselineinbothprimitiveand
long-horizontasksettings. Theunderlyingreasonisourapproach’simplementationoffine-grained
motionplannergeneration,whereasRoboGendirectlygeneratesatask-specificrewardfunctionand
reliesonreinforcementlearningfortasktraining,whichcanbeinherentlymorefragile. Addition-
ally,thebottom-upapproachsurpassesthetop-downapproachinbothmetrics.Thisadvantagestems
fromtheformer’srelianceoncomposingexistingtasks,wheremostfailuresoccurduringthechain-
ingofsub-tasks,whereasthelatternecessitatesgeneratingeachsub-taskfromscratch.Furthermore,
the enhanced reasoning capabilities of OpenAI o1 contribute to more logical task decompositions
comparedtovanillaGPT-4,resultinginsuperiorperformanceacrossbothevaluationcriteria.
5 Experiment: Multi-TaskPolicyTrainingandTransfer
Inthissection,weshowhowthegenerateddatabyGenSim2canbeusedbymulti-taskimitation
learningandsim-to-realtransfer.
70.8 0.8 0.6
0.49
0.66 0.46
0.6 0.6 0.55
0.4
0.46
0.43 0.29
0.4 0.4
0.19
0.2
0.2 0.2
0 0 0
4 Tasks 10 Tasks15 Tasks20 Tasks24 Tasks PPT No lang. No p.c.d No proprio. PPT-Train PPT-Test RGB-TrainRGB-Test
Figure 6. Multi-task training results. All results are evaluated with 20 episodes per task. Left)
Asthenumberoftrainingtasksincreases,thepolicyperformancedecreasesfirstandthenincreases.
Middle)Werespectivelyablatetheinputmodalitiesleveragedinourpolicyarchitectureon4tasks.
Right)Wefindthatpoliciesexhibitstrongobject-levelgeneralization. Foreacharchitecture,theleft
andrightbarsillustratethesuccessratesduringthetrainingandtestingphases,respectively.
5.1 Multi-taskTraininginSimulation
Training.Withthetasksanddatageneratedbyourpipeline,wetrainamulti-taskpolicy(with382M
parameters)acrossdifferentnumbersoftasksandtestitsgeneralizationtonewscenarios. InFig.6
left, wejointlytrainadifferentnumberofLLM-generatedtasksandteston4originaltasksunder
thelowdataregimesuchas10demospertask. Interestingly,addingmoretaskswillfirstdropthe
performanceandthenincreaseitbyvirtueofscaling. PleaserefertoAppendixBformoredetails.
Generalization. After training, we test the generalization ability of our policy to unseen object
instances,comparedwithapolicywithRGBinputs. Wesplittheinstancesintotrain/testsets. Eval-
uatedresultsareshowninFig.6right,wherewefindthatthesuccessratesofthePPTarchitectures
only drop by less than 3% on unseen instances, whereas the RGB policies reduce by quite a bit.
Theseresultsindicatethatbygeneratingdatawithobject-levelandspatial-levelvarianceasdomain
randomization, along with the pointcloud-based policy architecture design, the trained policy can
acquiregeneralizationinbothaspects,whichlaysthefoundationforfurthersim-to-realtransfer.
5.2 Real-WorldExperiments
To evaluate the quality of the data collected in simulation and how GenSim2 helps in real-robot
tasks,wetestsim-to-realandco-trainingonthegenerateddataoverseveraltasks. Asforthesetup,
weusetheFrankaResearch3robotarmwithamodified,deformableTPUparallelgripperforeasier
grasping. TherobotworkcellisequippedwiththreeRealSenseD435cameras: onewrist-mounted
andtwoexternallyfacingthescene. EachcapturesanRGB-Dobservationwhichiscombinedand
processedintoapointcloudtobeusedasobservations. MoredetailsarelistedinAppendixC.
Weperformexperimentson8real-worldtasks,collecting100demonstrationsforeachtaskinsim-
ulation, along with an additional 10 real-world demonstrations via teleoperation. We assess the
qualityandutilityofthegenerateddataacrossthreedistincttrainingsetups: (1)usingonlysimula-
tiondata,(2)usingonlyreal-worlddata,and(3)usingacombinationofboth. Theresultsofthese
evaluationsarepresentedinTab.2. Ourfindingscanbesummarizedasfollows: (a)datagenerated
byGenSim2enableseffectivezero-shotsim-to-realtransfer,withtheresultingpolicyoutperform-
ingonetrainedsolelyonlimitedreal-worlddata;and(b)whenco-trainedwithreal-worlddata,the
datageneratedbyGenSim2significantlyenhancesthepolicyperformanceevenby20%inabsolute
valuesand50%inrelativescale. Theseresultsunderscorethepotentialoflarge-scale,high-quality
datageneration,likeGenSim2,toreducetheburdenofextensivereal-worlddatacollectionwhile
improvingpolicyeffectivenessforreal-worldtasks.
8
etaR
sseccuS
etaR
sseccuS
etaR
sseccuSFigure7. RealExperiments: Weobservethatthemulti-taskpolicy,trainedonsimulationdata,can
performrobustlyonvariousunseenreal-worldobjectsthroughsim-to-realtransfer.
Table 2. Evaluation on a set of real-world tasks using one multi-task policy. We report the
per-tasksuccessrateof10episodeevaluationswith8real-worldtasks. Sim-onlyrepresentspolicy
trainedpurelywith100simulationdatageneratedbyGenSim2.Real-onlyrepresentspolicytrained
with 10 real-world data collected by teleoperation. Combined represents policy co-trained with
combinedsimulationandreal-worlddata.
TrainingData OpenLaptop CloseLaptop OpenSafe CloseSafe CloseDrawer SwingBucket OpenBox CloseBox Average
Real-only 0.5 0.0 0.2 0.4 1.0 0.5 0.2 0.1 0.363
Sim-only 0.7 0.5 0.1 0.3 0.8 0.5 0.5 0.0 0.425
Combined 0.8 0.7 0.3 0.6 1.0 0.8 0.0 0.4 0.575
6 LimitationsandDiscussions
Ourproposedmethodhasseverallimitations.Duetothelackof“roboticcentric”knowledge,suchas
3Dspatialunderstanding,foundationmodelslikeGPT-4Vstillfacehallucinationissuesincreating
meaningfultasksandsuccessfullycodingthem. Additionally,humaninvolvement,thoughminimal,
is still required to generate these complex manipulation tasks. Finally, we have only considered
6-doftasksinzero-shotsim-to-realtransferwithlimitedpointcloudobservations.
7 ConclusionandFutureWork
In this work, we propose GenSim2, a task and demonstration generation framework that utilizes
multi-modalfoundationmodelstogenerateupto100roboticsimulationtasksatscale,suchaslong-
horizon manipulation with articulated objects. We ablate on different task solvers and generation
components, aswellasproposeamulti-taskpointcloud-basedpolicyarchitecturethatdistillsgen-
erateddemonstrationsfromsimulationtransfertotherealworld. Futureworksincludeexpanding
taskcomplexityanddiversitythroughadvancedmulti-modalagentsand3Dassetgeneration.Future
workscanexploreadvancedsim-to-realmethodsforcomplextaskswithmultipleembodiments.
Acknowledgments
WewanttothankProfessorXiaolongWangforhiskindsupportanddiscussionofthisproject. We
thankYuzheQinandFanboXiangfortheirgeneroushelpinSapiendevelopment.WethankMazeyu
Jiforhishelpwithreal-worldexperiments. ThisworkispartlysupportedbytheAmazonGreater
BostonTechInitiativeandAmazonPONo. 2D-06310236.
9References
[1] F.Xiang, Y.Qin, K.Mo, Y.Xia, H.Zhu, F.Liu, M.Liu, H.Jiang, Y.Yuan, H.Wang, L.Yi,
A.X.Chang, L.J.Guibas, andH.Su. SAPIEN:Asimulatedpart-basedinteractiveenviron-
ment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2020.
[2] C.Berner,G.Brockman,B.Chan,V.Cheung,P.Dkebiak,C.Dennison,D.Farhi,Q.Fischer,
S.Hashme,C.Hesse,etal.Dota2withlargescaledeepreinforcementlearning.arXivpreprint
arXiv:1912.06680,2019.
[3] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert,
L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge.
nature,550(7676):354–359,2017.
[4] T.H.Trinh,Y.Wu,Q.V.Le,H.He,andT.Luong.Solvingolympiadgeometrywithouthuman
demonstrations. Nature,625(7995):476–482,2024.
[5] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino,
M.Plappert,G.Powell,R.Ribas,etal. Solvingrubik’scubewitharobothand. arXivpreprint
arXiv:1910.07113,2019.
[6] E. Kaufmann, L. Bauersfeld, A. Loquercio, M. Mu¨ller, V. Koltun, and D. Scaramuzza.
Champion-leveldroneracingusingdeepreinforcementlearning. Nature,620(7976):982–987,
2023.
[7] K. Fang, T. Migimatsu, A. Mandlekar, L. Fei-Fei, and J. Bohg. Active task randomization:
Learningvisuomotorskillsforsequentialmanipulationbyproposingfeasibleandnoveltasks.
arXivpreprintarXiv:2211.06134,2022.
[8] M. Deitke, E. VanderBilt, A. Herrasti, L. Weihs, K. Ehsani, J. Salvador, W. Han, E. Kolve,
A.Kembhavi,andR.Mottaghi. Procthor: Large-scaleembodiedaiusingproceduralgenera-
tion. AdvancesinNeuralInformationProcessingSystems,35:5982–5994,2022.
[9] L. Wang, Y. Ling, Z. Yuan, M. Shridhar, C. Bao, Y. Qin, B. Wang, H. Xu, and X. Wang.
Gensim: Generatingroboticsimulationtasksvialargelanguagemodels. InTheTwelfthInter-
nationalConferenceonLearningRepresentations,2023.
[10] Y.Wang,Z.Xian,F.Chen,T.-H.Wang,Y.Wang,K.Fragkiadaki,Z.Erickson,D.Held,and
C.Gan.Robogen:Towardsunleashinginfinitedataforautomatedrobotlearningviagenerative
simulation. arXivpreprintarXiv:2311.01455,2023.
[11] S.Nasiriany,A.Maddukuri,L.Zhang,A.Parikh,A.Lo,A.Joshi,A.Mandlekar,andY.Zhu.
Robocasa:Large-scalesimulationofeverydaytasksforgeneralistrobots.InRobotics:Science
andSystems(RSS),2024.
[12] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,
J.Bohg,A.Bosselut,E.Brunskill,etal. Ontheopportunitiesandrisksoffoundationmodels.
arXivpreprintarXiv:2108.07258,2021.
[13] G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.
Dai,A.Hauth,etal. Gemini: afamilyofhighlycapablemultimodalmodels. arXivpreprint
arXiv:2312.11805,2023.
[14] OpenAI. Gpt-4technicalreport,2023.
[15] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Tay-
lor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. Video genera-
tion models as world simulators. 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators.
10[16] B. Rozie`re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Re-
mez, J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint
arXiv:2308.12950,2023.
[17] W.Huang,C.Wang,R.Zhang,Y.Li,J.Wu,andL.Fei-Fei. Voxposer: Composable3dvalue
mapsforroboticmanipulationwithlanguagemodels. arXivpreprintarXiv:2307.05973,2023.
[18] K.Lin,C.Agia,T.Migimatsu,M.Pavone,andJ.Bohg. Text2motion: Fromnaturallanguage
instructionstofeasibleplans. arXivpreprintarXiv:2303.12153,2023.
[19] W.Huang,F.Xia,T.Xiao,H.Chan,J.Liang,P.Florence,A.Zeng,J.Tompson,I.Mordatch,
Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language
models. arXivpreprintarXiv:2207.05608,2022.
[20] D. Valevski, Y. Leviathan, M. Arar, and S. Fruchter. Diffusion models are real-time game
engines,2024. URLhttps://arxiv.org/abs/2408.14837.
[21] J. Bruce, M. D. Dennis, A. Edwards, J. Parker-Holder, Y. Shi, E. Hughes, M. Lai,
A.Mavalankar,R.Steigerwald,C.Apps,etal. Genie: Generativeinteractiveenvironments. In
Forty-firstInternationalConferenceonMachineLearning,2024.
[22] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu, L. Fan, and
A.Anandkumar.Eureka:Human-levelrewarddesignviacodinglargelanguagemodels.arXiv
preprintarXiv:2310.12931,2023.
[23] H.Ha,P.Florence,andS.Song. Scalingupanddistillingdown: Language-guidedrobotskill
acquisition. arXivpreprintarXiv:2307.14535,2023.
[24] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code
as policies: Language model programs for embodied control. In 2023 IEEE International
ConferenceonRoboticsandAutomation(ICRA),pages9493–9500.IEEE,2023.
[25] E.Todorov,T.Erez,andY.Tassa. Mujoco:Aphysicsengineformodel-basedcontrol. In2012
IEEE/RSJinternationalconferenceonintelligentrobotsandsystems,pages5026–5033.IEEE,
2012.
[26] R.TedrakeandtheDrakeDevelopmentTeam. Drake:Model-baseddesignandverificationfor
robotics,2019. URLhttps://drake.mit.edu.
[27] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Macklin,D.Hoeller,N.Rudin,
A.Allshire,A.Handa,etal. Isaacgym: Highperformancegpu-basedphysicssimulationfor
robotlearning. arXivpreprintarXiv:2108.10470,2021.
[28] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese,
Y. Zhu, and R. Mart´ın-Mart´ın. What matters in learning from offline human demonstrations
forrobotmanipulation. InConferenceonRobotLearning(CoRL),2021.
[29] A.Zeng, P.Florence, J.Tompson, S.Welker, J.Chien, M.Attarian, T.Armstrong, I.Krasin,
D.Duong,V.Sindhwani,etal. Transporternetworks:Rearrangingthevisualworldforrobotic
manipulation. InConferenceonRobotLearning,pages726–747.PMLR,2021.
[30] B.Liu,Y.Zhu,C.Gao,Y.Feng,Q.Liu,Y.Zhu,andP.Stone. Libero: Benchmarkingknowl-
edgetransferforlifelongrobotlearning. AdvancesinNeuralInformationProcessingSystems,
36,2024.
[31] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A
benchmarkandevaluationformulti-taskandmetareinforcementlearning. InConferenceon
robotlearning,pages1094–1100.PMLR,2020.
11[32] S.James,Z.Ma,D.R.Arrojo,andA.J.Davison. Rlbench: Therobotlearningbenchmark&
learningenvironment. IEEERoboticsandAutomationLetters,5(2):3019–3026,2020.
[33] L.Wang,K.Zhang,A.Zhou,M.Simchowitz,andR.Tedrake. Robotfleetlearningviapolicy
merging. arXivpreprintarXiv:2310.01362,2023.
[34] J.Tobin, R.Fong, A.Ray, J.Schneider, W.Zaremba, andP.Abbeel. Domainrandomization
for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ
internationalconferenceonintelligentrobotsandsystems(IROS),pages23–30.IEEE,2017.
[35] Z.Chen,S.Kiami,A.Gupta,andV.Kumar. Genaug: Retargetingbehaviorstounseensitua-
tionsviagenerativeaugmentation. arXivpreprintarXiv:2302.06671,2023.
[36] F.Ramos,R.C.Possas,andD.Fox.Bayessim:adaptivedomainrandomizationviaprobabilis-
ticinferenceforroboticssimulators. arXivpreprintarXiv:1906.01728,2019.
[37] L.Makatura,M.Foshey,B.Wang,F.Ha¨hnLein,P.Ma,B.Deng,M.Tjandrasuwita,A.Spiel-
berg,C.E.Owens,P.Y.Chen,etal. Howcanlargelanguagemodelshelphumansindesign
andmanufacturing? arXivpreprintarXiv:2307.14377,2023.
[38] H. Jun and A. Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463,2023.
[39] A.Nichol,H.Jun,P.Dhariwal,P.Mishkin,andM.Chen. Point-e: Asystemforgenerating3d
pointcloudsfromcomplexprompts. arXivpreprintarXiv:2212.08751,2022.
[40] B.Poole,A.Jain,J.T.Barron,andB.Mildenhall.Dreamfusion:Text-to-3dusing2ddiffusion.
arXivpreprintarXiv:2209.14988,2022.
[41] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh, C. Tan, J. Peralta,
B.Ichter,etal. Scalingrobotlearningwithsemanticallyimaginedexperience. arXivpreprint
arXiv:2302.11550,2023.
[42] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandku-
mar. Voyager: An open-ended embodied agent with large language models. arXiv preprint
arXiv:2305.16291,2023.
[43] N.Shinn,F.Cassano,A.Gopinath,K.Narasimhan,andS.Yao. Reflexion: Languageagents
withverbalreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,36,
2024.
[44] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and
A. Garg. Progprompt: Generating situated robot task plans using large language models.
In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523–
11530.IEEE,2023.
[45] J.Duan,W.Yuan,W.Pumacay,Y.R.Wang,K.Ehsani,D.Fox,andR.Krishna. Manipulate-
anything: Automating real-world robots using vision-language models. arXiv preprint
arXiv:2406.18915,2024.
[46] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic
manipulation. InConferenceonRobotLearning,pages785–799.PMLR,2023.
[47] L. Wang, Y. Xiang, W. Yang, A. Mousavian, and D. Fox. Goal-auxiliary actor-critic for 6d
robotic grasping with point clouds. In Conference on Robot Learning, pages 70–80. PMLR,
2022.
[48] M.Shridhar,L.Manuelli,andD.Fox. Cliport: Whatandwherepathwaysforroboticmanipu-
lation. InConferenceonRobotLearning,pages894–906.PMLR,2022.
12[49] Y.Ze,G.Zhang,K.Zhang,C.Hu,M.Wang,andH.Xu. 3ddiffusionpolicy. arXivpreprint
arXiv:2403.03954,2024.
[50] A. Handa, A. Allshire, V. Makoviychuk, A. Petrenko, R. Singh, J. Liu, D. Makoviichuk,
K. Van Wyk, A. Zhurkevich, B. Sundaralingam, et al. Dextreme: Transfer of agile in-hand
manipulationfromsimulationtoreality. In2023IEEEInternationalConferenceonRobotics
andAutomation(ICRA),pages5977–5984.IEEE,2023.
[51] W.Zhao,J.P.Queralta,andT.Westerlund.Sim-to-realtransferindeepreinforcementlearning
forrobotics: asurvey. In2020IEEEsymposiumseriesoncomputationalintelligence(SSCI),
pages737–744.IEEE,2020.
[52] Y.Chebotar,A.Handa,V.Makoviychuk,M.Macklin,J.Issac,N.Ratliff,andD.Fox. Closing
thesim-to-realloop: Adaptingsimulationrandomizationwithrealworldexperience. In2019
InternationalConferenceonRoboticsandAutomation(ICRA),pages8973–8979.IEEE,2019.
[53] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki,
A. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation.
TheInternationalJournalofRoboticsResearch,39(1):3–20,2020.
[54] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic
controlwithdynamicsrandomization. In2018IEEEinternationalconferenceonroboticsand
automation(ICRA),pages3803–3810.IEEE,2018.
[55] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine,
R.Hadsell,andK.Bousmalis. Sim-to-realviasim-to-sim: Data-efficientroboticgraspingvia
randomized-to-canonicaladaptationnetworks.InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages12627–12637,2019.
[56] L.Wang,J.Zhao,Y.Du,E.H.Adelson,andR.Tedrake. Poco: Policycompositionfromand
forheterogeneousrobotlearning. arXivpreprintarXiv:2402.02511,2024.
[57] L.Manuelli,W.Gao,P.Florence,andR.Tedrake. kpam: Keypointaffordancesforcategory-
levelroboticmanipulation. InTheInternationalSymposiumofRoboticsResearch,pages132–
157.Springer,2019.
[58] F. Liu, K. Fang, P. Abbeel, and S. Levine. Moka: Open-vocabulary robotic manipulation
throughmark-basedvisualprompting. arXivpreprintarXiv:2403.03174,2024.
[59] N. Di Palo and E. Johns. Keypoint action tokens enable in-context imitation learning in
robotics. arXivpreprintarXiv:2403.19578,2024.
[60] W. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei. Rekep: Spatio-temporal reasoning of
relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652,
2024.
[61] L.Wang,X.Chen,andK.H.JialiangZhao. Scalingproprioceptive-visuallearningwithhet-
erogeneouspre-trainedtransformers. InNeurips,2024.
[62] T.Z.Zhao, V.Kumar, S.Levine, andC.Finn. Learningfine-grainedbimanualmanipulation
withlow-costhardware. arXivpreprintarXiv:2304.13705,2023.
[63] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:
Visuomotorpolicylearningviaactiondiffusion. arXivpreprintarXiv:2303.04137,2023.
[64] X.Fu, Y.Hu, B.Li, Y.Feng, H.Wang, X.Lin, D.Roth, N.A.Smith, W.-C.Ma, andR.Kr-
ishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint
arXiv:2404.12390,2024.
13[65] D.Liu,X.Dong,R.Zhang,X.Luo,P.Gao,X.Huang,Y.Gong,andZ.Wang.3daxiesprompts:
Unleashingthe3dspatialtaskcapabilitiesofgpt-4v. arXivpreprintarXiv:2312.09738,2023.
[66] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov.Proximalpolicyoptimization
algorithms. arXivpreprintarXiv:1707.06347,2017.
[67] G.Ilharco,M.Wortsman,R.Wightman,C.Gordon,N.Carlini,R.Taori,A.Dave,V.Shankar,
H.Namkoong,J.Miller,H.Hajishirzi,A.Farhadi,andL.Schmidt. Openclip,July2021. URL
https://doi.org/10.5281/zenodo.5143773. Ifyouusethissoftware,pleasecite
itasbelow.
[68] G. Qian, Y. Li, H. Peng, J. Mai, H. Hammoud, M. Elhoseiny, and B. Ghanem. Pointnext:
Revisiting pointnet++ with improved training and scaling strategies. In Advances in Neural
InformationProcessingSystems(NeurIPS),2022.
[69] G.Yan,Y.-H.Wu,andX.Wang. Dnact: Diffusionguidedmulti-task3dpolicylearning. arXiv
preprintarXiv:2403.04115,2024.
[70] Y. Ze, G. Yan, Y.-H. Wu, A. Macaluso, Y. Ge, J. Ye, N. Hansen, L. E. Li, and X. Wang.
Gnfactor:Multi-taskrealrobotlearningwithgeneralizableneuralfeaturefields.InConference
onRobotLearning,pages284–301.PMLR,2023.
[71] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:
Representingscenesasneuralradiancefieldsforviewsynthesis.CommunicationsoftheACM,
65(1):99–106,2021.
14Appendix
A TaskGenerationDetails
A.1 AssetStatistics
WesummarizetheinformationofthearticulatedassetlibraryweusedinthisprojectinTable4. To
easetheworkloadoftheLLMinunderstandingthemanipulatablepartsofanArticulation,weonly
reserve one unfixed (prismatic or revolute) joint in each object and fix the others. For rigid body
objects, webuildadatasetadaptedfromtheYCBobjectdatasetinManiskill2. Wehavemodified
thescaleandjointlimitsoftheobjectstomakethesceneproper.
A.2 TaskGenerationStatistics
Inthissection,weinvestigatethestatisticsofthetasksinthegenerationprocess. Wehavegenerated
100tasksbasedontheassetlibrarydescribedinAppendixA.1,including50primitivetasksand50
long-horizontasks. Noticethatweconsidertaskswiththesameobjectclassbutdifferentinstances
asthesametasks. Despitethetaskswehavegenerated,thesetasksdonotincludeallassetsinthe
assetlibraryandtheassetlibrarycanalsobeexpandedtocontainmoreobjectclassesandinstances.
A.3 TaskSolverDetails
kPAM Planner. kPAM[57] is a keypoint-based planner proposed for category-level manipulation
tasks,andwefollowtheroadmapproposedinWangetal.[33]toimprovekPAMandapplyitinour
articulation tasks. It generally defines a robotic task by the homogeneous transformation required
to manipulate the target object. kPAM discovers such transformation by solving an optimization
problembuiltonseveralkeypoint-basedconstraints. AfterkPAMimplicitlydefinestheend-effector
posetogetcontactwiththeobject,whichwecalltheactuationpose,weneedtobuildthetrajectory
toapproachtheactuationposeandsubsequentlycompletethetask. Tothisend,pre-actuationand
post-actuationmotionsareintroduced,bothofwhichincludeaseriesofwaypointstoguidetheend-
effectormotions(suchasmakingaturnorpushingbymovingforward).Togetherwiththeactuation
pose,anobject-centrictrajectoryofkeyposesisdesignedtosolveanarticulationmanipulationtask.
WeprovideanillustrationofleveragingkPAMplannertosolveataskinFig.8.
RLLearner. Inadditiontoplanners,wealsoleverageareinforcementlearning(RL)learner[10]as
analternativesolverforthegeneratedmanipulationtasks,inwhichwepromptanLLMtoproduce
a reward function for a target task. After the task proposal stage, we’re provided a task definition
andtaskcodegeneratedbytheLLMwhichisusedascontextforaLLMtogenerateanexecutable
rewardfunction. TheLLMissupplementedwithasmalllibraryofcuratedrewardexamplesaswell
asanAPIconsistingofrewardfunctionscorrespondingtovariouspredefinedcomponentsthatitcan
referto,suchasthedistancebetweentheend-effectorandtheobject,orthejointpositionsofcertain
articulated objects. If successful generation is achieved, we append the reward into the task code
andexecutethefinallearningcodeusingProximalPolicyOptimization(PPO)[66]andfixadefault
setofhyperparametersforalltasks.
Insummary,weproposethekPAMmotionplannerandRLpolicylearnerascomplementarymeth-
ods for solving generated tasks. For kPAM, since it is defined by a well-formulated optimization
problem regarding fine-grained constraints and costs, the output motions are much smoother and
morenaturalthanmotionsfromotherlearning-basedmethods. Moreover,kPAMisinherentlygen-
eralizabletodifferentsceneconfigurationsandobjectinstancesbecausetheconstraintsandactuation
motionsareall“object-centric”elements,whichwillbefurtherexploitedfordomainrandomization
inSection3.3. Furthermore,kPAMisfasttoexecute(∼2secondsplanningtimegivenaconfigura-
tion)androbusttomultipleruns. DespitethestrengthofkPAM,RLprovidesadvantagesincreative
solution proposals for tasks that are too complex or ambiguous for kPAM to define proper con-
straints,especiallyforthoseinvolvingthinobjectsorcontact-richmanipulation. Inthisproject,we
giveprioritytothekPAMplannerinmostcasesbecauseitproducesmorerealisticandgeneralizable
motions,andleavesthosethatcannotbesolvedfortheRLlearner.
15Constraints Actuation Actuation
Pose Motions
( , , coincident),
Pre-actuation:
( , , parallel), (Translate_x, -0.05), (Translate_z, -0.15)
( , , orthogonal)
…… Post-actuation:
(Rotate, 30)
𝑧
Pre-actuation Post-actuation
Motion Motion
𝑥
Figure8. IllustrationofkPAMPlanner. WeshowhowtoleveragethekPAMplannertosolvethe
taskOpenBox. First,constraintsaredefinedtomakethegrippergetcontactwiththeboxlid. Then
based on the this actuation pose, some actuation motions are assigned to complete the motion of
openingabox.
Secure gold in safe
Initial scene “Open thesafe” “Grasp thegold” “Place thegold “Closethesafe”
intothesafe”
Prepare breakfast
“Open theMicrowave” “Putthesteak “Putthebowlinto “Closethe microwave” “Pressthetoaster”
inthebowl” themicrowave”
Figure9. IllustrationofGeneratedLong-horizonTasks. Weshowtwoexampletasksgenerated
byourmethod,includingsecuringgoldinasafe(top)andpreparingbreakfast(bottom). Theformer
contains4sub-tasksandissolvedbychainingmotionplannersofallsub-tasks. Thelattercontains
7sub-tasksandisteleoperatedduetoitscomplexity.
A.4 HumanVerification
We measurethe humanefforts required in GenSim2, includinglabeling keypointsfor the motion
plannerandmulti-shotrejectionsamplingintasksolvergeneration.
KeypointLabeling. Wesetupaprotocolforkeypointlabelingandannotated1-3keypointsforeach
asset. Wemeasuredthetimespenton10objectsandtheaveragetimeis8.2spertask.
Rejection Sampling. While task solver generation inevitably involves some degree of human in-
tervention, we have made significant efforts to minimize this effort. To evaluate the efficiency of
ourapproach,weconductedauserstudymeasuringthetimerequiredforahumanusertogenerate
a novel task and corresponding solver using GenSim2, and compared it with the time needed for
manual task design. The results are presented in Tab. 3. We consider the performance of one of
the project’s authors (User 1) as expert proficiency and additionally involve two other users who
are unfamiliar with our pipeline. The findings indicate that our method significantly reduces task
creation time by 50%, and over 90% of the required time involves waiting for LLM to response.
Furthermore,thereducedperformancegapbetweenexpertsandbeginnerssuggeststhatourpipeline
requireslesstechnicalexpertise,makingitaccessibletouserswithvaryinglevelsoffamiliarity.
16Table3. Taskcreationtimemeasurement. Theresultsareallbasedon5tasks. Theaveragetask
creationtimeisaround4minutesforGenSim2.
User1(Expert) User2 User3
Method GenSim2 Manual GenSim2 Manual GenSim2 Manual
Average 159.4s 463.8s 241.0s 778.6 292.0s 727.4s
A.5 DemonstrationGenerationStatistics
In this section, we assess the efficiency and quality of demonstration generation using our kPAM-
based motion planner. Recalling our goal to use our generation pipeline as a data amplifier to
enhancethegeneralizationandsim-to-realtransferabilityofasimulationpolicy,thegenerateddata
shouldbediverseandofhighqualityfordataaugmentationordomainrandomization.
To this end, we evaluate the robustness of our kPAM solver to random scene configurations and
objectinstances.Forprimitivetasks,weadduniformnoisein[−0.1m,0.1m]tothexandypositions
oftheobject,aswellasarandomrotationin[−30◦,30◦]totheyawaxis.Forsomehardlong-horizon
tasks,inwhichthearticulationissosmallthatitmaybestrugglingtoplacesomethinginside,wefail
togeneratesuccessfuldemonstrationswithnormalrandomization, sowereducethescaleofnoise
byhalf. Forarticulations, werandomlyselectaninstanceadaptedfromPartNet-Mobilityandadd
noisetoitsdefaultjointposition.
Wetestthesuccessratesofgeneratingdemonstrationsfor24single-stepand15long-horizontasks,
andreportthemtaskbytaskinTab.5.Foreachtask,werunthedemocollectionprocess5times,and
eachtimethekPAMsolverisexecutedfor50episodeswithbothspatialandobjectrandomization.
Wecomputethemeanandstandarddeviationofthe5runsonthesuccessrate.
17Table4. StatisticsofassetsusedinGenSim2
Object Num JointType AssetDescription
laptop rotate 44 revolute Alaptopfixedonthetable, withalidconnectedbya
revolutejointthatcanbeopenedandclosed
stapler move 3 prismatic Astaplerthatcanbemovedonthetable
window push 2 prismatic Awindowwithaframeconnectedbyaprismaticjoint
thatcanbepushedleftandright
suitcase move 3 prismatic Asuitcasethatcanbemovedonthetable
oven 4 revolute Anovenfixedonthetable, withadoorconnectedby
arevolutejointthatcanbeopenedandclosed
drawer 20 prismatic A drawer fixed on the table, connected with its body
byaprismaticjointthatcanbeopenedandclosed
box move 1 prismatic Aboxthatcanbemovedonthetable
kitchen pot 3 prismatic Akitchenbotfixedonthetable,connectingitslidand
bodywithaprismaticjoint
bucket lift 5 prismatic Abucketthatcanbeliftedfromthetable
trash can 3 revolute Atrashcanfixedonthetable,withalidconnectedby
arevolutejointthatcanbeopenedandclosed
stapler press 3 revolute Astaplerfixedonthetable,withitshandleconnected
toitsbodybyarevolutejointthatcanbepressed
bottle 2 revolute Abottlefixedonthetable,withatopconnectedtoits
bodybyaprismaticjoint
bag move 2 prismatic Abagthatcanbemovedonthetable
dishwasher 3 revolute Adishwasherwithadoorconnectedbyarevolutejoint
thatcanbeopenedandclosed
suitcase rotate 4 revolute Asuitcasefixedonthetable,withalidconnectedbya
revolutejointthatcanbeopenedandclosed
toaster move 10 prismatic Atoasterthatcanbemovedonthetable
bag swing 2 revolute A bag fixed on the table, with a strap connected by a
revolutejoint
toilet 2 revolute Atoiletfixedonthetable,lidconnectedbyarevolute
joint
door 2 revolute A door with its frame connected by a revolute joint
thatcanbeopenedandclosed
coffee machine 2 prismatic Acoffeemachinefixedonthetable,withabuttoncon-
nectedbyaprismaticjointtobepressed
faucet 13 revolute Afaucetfixedonthetable,withahandleconnectedby
arevolutejointthatcanbeturnedonandoff
washing machine 2 revolute Awashingmachinefixedonthetable,withadoorcon-
nectedbyarevolutejoint
switch 2 revolute Aswitchwithaframeconnectedbyarevolutejoint
bucket swing 26 revolute A bucket fixed on the table, with a handle connected
byarevolutejoint
safe move 2 prismatic Asafethatcanbemovedonthetable
toaster press 10 prismatic A toaster fixed on the table, with a button connected
byaprismaticjoint
window rotate 2 revolute Awindowwithaframeconnectedbyarevolutejoint
refrigerator 2 revolute Arefrigeratorfixedonthetable,withadoorconnected
byarevolutejointthatcanbeopenedandclosed
laptop move 3 prismatic Alaptopthatcanbemovedonthetable
safe rotate 15 revolute A safe fixed on the table, with a door connected by a
revolutejointthatcanbeopenedandclosed
bag lift 2 prismatic Abagthatcanbeliftedfromthetable
microwave 10 revolute Amicrowavefixedonthetable,withadoorconnected
byarevolutejointthatcanbeopenedandclosed
box rotate 13 revolute A box fixed on the table, with a lid connected by a
revolutejointthatcanbeopenedandclosed
bucket move 2 prismatic Abucketthatcanbemovedonthetable
18Table5. SuccessRatesofdemonstrationgenerationondifferenttasks
TaskName TaskType SuccessRate
OpenBox Primitive 0.84±0.07
CloseBox Primitive 0.94±0.03
OpenLaptop Primitive 0.76±0.03
CloseLaptop Primitive 0.95±0.01
TurnOnFaucet Primitive 0.67±0.05
TurnOffFaucet Primitive 0.72±0.03
OpenDrawer Primitive 0.80±0.02
PushDrawerClose Primitive 0.87±0.06
SwingBucketHandle Primitive 0.89±0.03
PressToasterLever Primitive 0.96±0.03
RotateMicrowaveDoor Primitive 0.92±0.01
CloseSafe Primitive 0.80±0.04
OpenSafe Primitive 0.62±0.03
PushToasterForward Primitive 0.99±0.01
CloseSuitcaseLid Primitive 0.82±0.05
SwingSuitcaseLidOpen Primitive 0.81±0.06
RelocateSuitcase Primitive 0.80±0.04
LiftBucketUpright Primitive 0.76±0.05
MoveBagForward Primitive 0.72±0.05
CloseMicrowave Primitive 0.53±0.04
SwingDoorOpen Primitive 0.79±0.03
ToggleDoorClose Primitive 0.80±0.05
CloseRefrigeratorDoor Primitive 0.82±0.03
OpenRefrigeratorDoor Primitive 0.75±0.06
PlaceGolfBallIntoDrawer Long-horizon 0.56±0.09
PlaceCrackerBoxIntoDrawer Long-horizon 0.53±0.08
PlaceLemonIntoDrawer Long-horizon 0.53±0.06
PlaceSoftBallIntoDrawer Long-horizon 0.58±0.06
DropAppleIntoDrawer Long-horizon 0.58±0.05
StachCupInBox Long-horizon 0.40±0.09
PlaceGolfBallIntoBox Long-horizon 0.58±0.06
PutCrackerBoxInBox Long-horizon 0.48±0.08
StoreBlockInBox Long-horizon 0.62±0.03
DropAppleIntoBox Long-horizon 0.64±0.04
StoreLemonInRefrigerator Long-horizon 0.48±0.07
PlaceCrackerBoxIntoRefrigerator Long-horizon 0.26±0.09
SecureGoldInSafe Long-horizon 0.21±0.05
FillMugWithWater Long-horizon 0.13±0.04
19B Multi-TaskTrainingDetails
B.1 TaskSettings
Wegraduallyaddthenumberoftrainingtasksfrom4to24inSection5.1andtestthescalingability
of our framework on the original 4 tasks. We provide a detailed task list here to better clarify our
tasksettings:
• 4tasks: OpenBox,CloseBox,OpenLaptop,CloseLaptop;
• 10 tasks: 4 tasks + OpenDrawer, PushDrawerClose, SwingBucketHandle, LiftBucketUpright,
PressToasterLever,PushToasterForward;
• 15tasks: 10tasks+MoveBagForward, OpenSafe, CloseSafe, RotateMicrowaveDoor, CloseMi-
crowave;
• 20tasks:15tasks+CloseSuitcaseLid,SwingSuitcaseLidOpen,RelocateSuitcase,TurnOnFaucet,
TurnOffFaucet;
• 24tasks: 20tasks+SwingDoorOpen,ToggleDoorClose,CloseRefrigeratorDoor,OpenRefriger-
atorDoor.
B.2 ArchitectureImplementation
Weimplementedamulti-modalpolicyarchitecture,asillustratedinFig.4,whichincludesatrans-
formerpolicystem, anactionhead, anddifferentencodersformodelingvarioustypesofobserva-
tionsastokens.
Specifically,wetaketheCLIP[67]tokenizertoencodethetaskinstruction,whichisfrozenduring
training;apre-trainedPointNext[68]toencodethepointcloud,whichisfine-tunedduringtraining;
an MLP for encoding the proprioception states, which is trained from scratch. The transformer
conducts self-attention over all tokens, and then we post-process the tokens for different action
heads. For example, we compute the mean pooling of all the tokens as the global condition of
thediffusionandMLPhead;forthetransformer,wecomputecross-attentionbetweenthemodeled
token and a set of position embeddings to get the final action sequence. Regarding the PointNext
encoder, we utilize the pre-trained model on ScanObjectNN Classification1, as it does not include
colorinformation,makingiteasierfordownstreamsim-to-realtransfer. Notethatthediffusionhead
and the transformer head both model and predict action sequences, but the MLP head only works
forsingle-stepaction.
B.3 PolicyArchitectureComparisonagainstBaselines.
We first evaluate the Proprioception Pointcloud Transformer (PPT) in solving multiple tasks. We
train one policy with demonstrations of 10 tasks on the commonly used manipulation benchmark,
RLBench[32]. Inparticular,wetrainPPTfor250epochsondatacollectedusingthesamesetupas
inYanetal.[69]. ResultsshowninTab.6performancebestperformancesover6/10tasksimproved
performanceoverrecentcompetitivebaselinemethodssuchasPerAct[46]andGNFactor[70]with
asignificantreductioninrequiredcameraviewsandpre-trainedfeaturerepresentations. Notethat
GNFactor [70] requires rather hassles to set up for real-world experiments, as it requires getting
features from the neural radiance field [71]. On the contrary, our proposed PPT architecture only
requirespointcloud,languages,androbotsensorstates,whicharemucheasiertoobtainwithreal-
worldrobots.
B.4 AdditionalExperimentsforObject-LevelGeneralization
Aftermulti-tasktraining,wetestthegeneralizationabilityofourpolicyonunseenobjectinstances.
Wechosetasksusingassetswithmorethan20instancesand10instancestorespectivelyconstruct
1https://drive.google.com/drive/folders/1A584C9x5uAqppbjNNiVqlA_
7uOOOlEII?usp=sharing
20Table 6. Multi-Task Performance on RLBench. We evaluate 25 episodes for each checkpoint
on 10 tasks across 3 seeds and report the success rates (%) of the final checkpoints. Our method
outperformsthemostcompetitivebaselinePerAct[46]andGNFactor[70]over6/10tasks.
turn drag open putin sweepto meatoff phoneon place slide putin
Methods tap stick fridge drawer dustpan grill base wine block safe
PerAct 57.3±6.1 14.7±6.1 4.0±6.9 16.0±13.9 4.0±6.9 77.3±14.0 98.7±2.3 6.7±6.1 29.3±22.0 48.0±26.2
GNFactor 56.0±14.4 68.0±38.6 2.7±4.6 12.0±6.9 61.3±6.1 77.3±9.2 96.0±6.9 8.0±6.9 21.3±6.1 30.7±6.1
PPT(ours) 68.0±0.10 69.3±0.06 10.1±0.10 22.7±0.06 50.1±0.06 57.3±0.02 44.0±0.04 29.3±0.06 21.3±0.06 52.0±0.00
Generalization to Unseen Objects
0.8
0.65
0.59
0.6
0.49
0.46
0.4
0.32 0.3 0.32 0.31
0.23 0.21
0.2 0.18 0.17
0
5 Tasks Train 5 Tasks Test 15 Tasks Train 15 Tasks Test
Diffusion Transformer MLP
Figure10. Resultsforobject-levelgeneralizationtest. Weshowthatourpolicieswithvarioushead
architecturescangeneralizetounseenobjects.
two training sets of 5 tasks and 15 tasks. During training, we only leverage data generated from
90%oftheinstancesandleavetheremaining10%fortesting. Theinitialposesoftheobjectsare
randomlyinitializedinbothproceduresasmentioned.
We find that the success rates only drop by around 3% on unseen instances, as shown in Fig. 10.
Theseresultsindicatethatbygeneratingdatawithobject-levelandspatial-levelvarianceasdomain
randomization,ourpolicycanacquiregeneralizationinbothaspects,whichlaysthefoundationfor
furthersim-to-realtransfer.
C Real-WorldDetails
Real World Setup
Weselected8real-worldtaskstoevaluatethemulti-task
policy trained on generated data from our pipeline. The Wrist Camera Right Camera
Wrist Camera
OpenLaptop task requires the robot to fully open a par- Multi-Task Policy
Right Camera
tiallyclosedlaptop,similarlyCloseLaptopclosesthelap-
Left Camera
top lid. The OpenSafe and Close MS ula ti-Tf ae sk
performed similar Franka Research 3
Policy Left Camera
openingandclosingmotionsonanarticulatedRosbaot fSetatedoor.
The CloseDrawer task required the robot to reach and
close an open drawer. The generated SwingBucket task
required the robot to push a Lhanagunagde lDeescpripetiornpendicular to its Point Cloud

Point Cloud Observation Robot State Observation
currentposition. TheOpenBoxtaskrequiredtherobotto
Real World Setup
open a box lid from a partially closed position and the Figure11. Real-worldSetup.
CloseBoxtaskrequiredtherobottofullycloseaboxlid.
As we didn’t utilize a discrete action space such as keyframes for real-world experiments, real-
time control was important. We achieved fast inference speeds with our multi-task policy with
0.1s inference latency on an NVIDIA 3080 GPU. We parallelized and synchronized point cloud
processingfrom3IntelRealsenseD435camerasinordertopreventadditionallatency. Pleaseview
Fig.11toseetherobotworkcelldescribed.Processingthepointcloudsinvolvedauniformsampling
21
Objects Used
etaR
sseccuSTable 7. Real-world Generalizability Experiments. We transfer a multitask policy from sim to
realandevaluate15episodesforeachtaskandobjectandreportthesuccessratesbelow.
Task OpenBox OpenLaptop CloseDrawer
Asset Small Medium Long Large Toy Real Short Tall Average
SuccessRate 46.7% 60.0% 40.0% 40.0% 80.0% 73.3% 80.0% 53.3% 61.7%
Figure12. RealWorld3DObservation: Pointcloudsofdifferentobjectsintherealworld. Cat-
egoryofobjectsfromlefttoright: laptop, safe, box, andbucket. Colorisshownforeasierunder-
standing,however,colorisnotusedasinputtothemulti-taskpolicy.
stepfortimeefficiency,farthestpointsampling,andoutlierremovaltocleanandsimplifythepoint
cloudforsuccessfulinferenceduringSim-to-Realtransfer.
C.1 GeneralizationonUnseenObjects
In addition, we also evaluate how well the multi-task policy could generalize to different objects
within each task. We run an additional set of experiments for the OpenBox, CloseDrawer, and
OpenLaptoptasks,onvaryingobjects. Thankstothetrainingscaleinsimulationandthedesignof
ourPPTpolicyarchitecture,ourmethodcanachieveover60%successrateonmultipletaskswith
variousunseenobjects,asshowninTab.7. Inaddition,theperformancesondifferentobjectswithin
the sametask are comparable, demonstratingthe robust real-worldgeneralization achieved by our
policy.
C.2 Co-TrainingExperimentDetails
To conduct additional experiments on differ-
entmulti-taskpoliciesincorporatingreal-world
data we incorporated a teleoperation setup us-
ing an HTC Vive controller and base station
to track 6-DOF hand movements which were
mappedtocontroltherobotarm’spose. Toin-
terface with the HTC Vive controller we used
the (triad-openvr package) and SteamVR. We
implemented a position-velocity controller for Figure13. Comparisonofsimulation(left)vsreal
the robot to track given poses from the con- (right)pointcloudofalaptop.
troller. Usingthisteleoperationsystem,wecol-
lected 10 demonstrations for each generated task, collecting state and observation data for each
trajectory. Weco-trainedamulti-taskpolicyusing100episodesofsimulationdatapertaskandthe
10collecteddemonstrationsofrealdatapertask.
D ComparisonwithPreviousWorks
Different from previous efforts including GenSim (top-down manipulation tasks), RoboGen (dy-
namictasks),andRobocasa(mobilemanipulationtaskswithmultipleembodiments),GenSim2fo-
cuseson6-DOFrealistictasksthatallowtaskandhigh-qualitydatagenerationatscale,whichreally
22Table8. Comparisontoseveralreleasedgenerationframeworksinroboticssimulation.
Methods GenSim2(ours) GenSim[9] RoboGen[10] RoboCasa[11]
TaskType Articulation Top-down Articulation Articulation
DataGeneration Fromscratch Fromscratch Fromscratch Teleoperated
Efficiency Fast Fast Slow(RL) Slow(teleoperation)
DataTransferability High High Low(RL) Medium
Sim-to-Real Zero-shot/co-train Zero-shot/finetune None Co-train
benefits solving real-world tasks. However, in contrast to GenSim2, GenSim is different from the
tasklevel,whichprimarilydealswithsimplertop-downpick-placetasks;RoboGenisdifferentfrom
thedemonstrationgenerationmethod,whichleveragesRLformanipulationtasksandthusproduces
unrealistic and untransferable data for real-world tasks; RoboCasa is different from the pipeline,
whichrequireslargeamountsofhumanteleoperationtocollectsourcedata. Moreover,ourexperi-
mentsontakingadvantageofgeneratedsimulationdataforreal-worldtransfershowthatweachieve
asuccessrateof42.5%forzero-shotsim2realtransferand57.5%fortransferviaco-trainingwith
both simulation and real data, while RoboCasa, which conducts similar sim2real experiments as
ours,onlyachievesscorelowerthan25%byco-training. Wesummarizefeaturesofgenerateddata
oftheaforementionedpreviousworksinTab.8.
Furthermore, our task and data generation method features multimodal LLMs as well as a gener-
alizable planner to solve such complex tasks. In our experiment, we verify that the multi-modal
tasksolvergenerationinthissystemcangreatlyimprovethetaskgenerationsuccessratesby60%,
outperformingpreviousworkby20%,andtheplannerusedisalsofriendlyandconvenientfordata
generation.
23E PromptExamples
E.1 PromptTemplates
Inthissection,wedemonstratesomeexamplesoftheprompttemplateswehaveusedtoqueryLLM.
Theprompttemplatesareshownasfollows:
Prompt: TaskProposal
Youareanexpertincreatingroboticsimulationenvironmentsandtasks. Youaregivensomeartic-
ulatedassetsforexample. Pleasecomeupwithacreativeuseofthegrippertomanipulateasingle
Articulation. Notethatthesimulationenginedoesnotsupportdeformableobjectsoraccuratecolli-
sionmodelsforcontacts. Moreover,therobotcanonlyexecuteasimpletrajectoryofonemotion.
===========================
Herearealltheassets. Pleasetrytocomeupwithtasksusingonlytheseassets.
...
===========================
Herearesomeexamplesofgoodtasks. Trytolearnfromthesestructuresbutavoidoverlapping
withthem.
...
===========================
Herearesomebadexampletaskinstanceswithreasons. Trytoavoidgeneratingsuchtasks.
...
reasons: ...
===========================
PleasedescribeaNEWtaskinnaturallanguagesandexplainitsnoveltyandchallenges.
Note:
-Donotuseassetsthatarenotinthelistabove.
-Donotrepeatthetaskssimilartothegoodexamplesorthealreadygeneratedtasks.
-Thetaskneedstoobeyphysicsandremainfeasible.
-Donotcreatesimilartaskswiththesame“assets-used”set.
-Alltheassetsareonthetablewheninitialized.
-Donotplaceobjectsonsmallobjects.
-OnlyoneArticulationcanbeloaded.
-Thetaskcontainsasimpletrajectoryofonemotion.
-Thetaskshouldhaveacleargoal,e.g. use“open/close”insteadof“adjustposition”.
Beforethenextstep,pleasecheckifthegeneratedtaskisabadtaskshownintheaboveexamples
andmeetsallthecriteriaasstatedabove. Specifically,ifthetask**only**containsasimpletra-
jectoryofonemotion,andshouldhavea**clear**goal. Explainindetail,andgetaconclusion.
Ifthetaskisabadtask,regenerateanewone.
Then,formattheanswerinaPythondictionarywithkeys“task-name”andvaluetypestring,“task-
description”(oneshortphrase),andvaluetypestringwithlower-caseandseparatedbyhyphens,
“assets-used”andvaluetypelistofstrings,and“success-criteria”(choosefrom“articulated open”,
“articulated closed”,“distance articulated rigidbody”,“distance gripper rigidbody”,and“dis-
tance gripper articulated”)andvaluetypelistofstrings. Trytobeascreativeaspossible.
PleaseremembernottoaddanyextracommentstothePythondictionary.
Let’sthinkstepbystep.
Prompt: TaskDecomposition
Youareanexpertincreatingroboticsimulationenvironmentsandtasks. Arobotarmwitha2-
fingergripperisusedinalltheroboticsimulationenvironmentsandtasks. Ineachtask,thereis
exactlyoneArticulationandonerigidbodyobjectthatyoucanmanipulate. Youwillbegiven
long-horizontaskswitheachtaskincludingatleast2sub-tasks. Eachsub-taskcanonlyinclude
onesimplemotionsuchasmovingthegrippertosomeobject,openingorclosingthegripperfin-
gers,orinteractingwithcertainArticulationsbyitsprismatic/revolutejoints.
Pleasecomeupwithadecompositionofthegivenlong-horizontasktogetseveralsub-tasks.
Somerulesofsuchdecompositionarelistedhere:
241. Eachlong-horizontaskcannotincludeover5sub-tasks,andusually3-4areenough.
2. Eachsub-taskshouldonlyincludeonesimplemotionasmentioned.
3. Eachsub-task(except“grasp”and“ungrasp”)shouldbepresentedintheformatofaPython
dictionarywithkeys“task-name”andvaluetypestringwithlower-caseandseparatedbyhy-
phens,“task-description”(onespecificsentence)andvaluetypestring,“assets-used”andvalue
typelistwithnecessaryasset(s)inthecurrentsub-task,and“success-criteria”(choosefrom“ar-
ticulated open”,“articulated closed”,“distance articulated rigidbody”,“distance gripper rigidbody”,
and“distance gripper articulated”)andvaluetypelistofstrings.
4. Eachsub-taskshouldhaveonlyoneassetusedinthetask.
5. Ifthemotionofopeningorclosingthegripperfingersisincludedinthewholetask,itshould
belistedasaseparatesub-task,whose“task-name”shouldstrictlybe“grasp”or“ungrasp”respec-
tivelyandshouldbetheonlykeyinthedictionary.
===========================
Hereisanexampleofthedecompositionofthefollowinglong-horizontask“...”:
...
#Sub-task1
...
#Sub-task2
...
#Sub-task3
...
#Sub-task4
...
===========================
Nowpleasestarttogenerateasub-taskdecompositionofthefollowingnewtask:
...
Prompt: CodeGeneration
NowIwillprovideyouwithsomereferencecodeandyoucanwritethecodeforthetask
“TASK NAME TEMPLATE”.
...
===========================
Thegeneratedcodeshouldfollowthesamestructureasthereferenceandcallsimilarfunctions.
Donotuselibraries,extrafunctions,properties,arguments,orassetsthatyoudon’tknow.
Remembertoimportusedfunctionsfromthecorrespondingfilesastheexampletaskcodes.
FortheArticulation,use“self.articulator”torefertoit,whichshouldbethesameasthe“assets-
used”ofthetask.
Fortheobjectsused,youonlyhavetopassthecorrespondingparameter(e.g,articulator)andits
name(stringformat,e.g.,‘box’)inthe‘ init ’functionasshowninabovecodes,andthebase
classwillautomaticallyloadthem.
Pleasecommentonthecodetoexplainwhateachpiecedoesandwhyit’swrittenthatway.
Nowwritethecodeforthetask“TASK NAME TEMPLATE”inthePythoncodeblock.
Prompt: kPAMSolverStage1
Youareanexpertinsolvingrobotictasksbycodingtasksolutionconfigs. Nowpleasesolvethe
newlygeneratedtaskbygeneratingthetasksolutionconfig.
Thetasksolutionconfigcontainsthenecessarypositions,parameters,andkeypointsforanexisting
trajectoryoptimizationalgorithmtosolveafeasiblesolution. Itmainlycontainstwoparts,con-
straintsandpre/post-actuationmotions:
(1)Theconstraintsareusedtoensurethegripperisincontactwiththeobjectandtoimplicitly
defineacertainactuationpose.
(2)Thepre-actuationmotionsareusedtomovethegrippertotheactuationpose,whilethepost-
actuationmotionsareusedtocompletethetaskaftertheactuationpose.
===========================
Hereisthetaskdescription.
...
===========================
25Herearealltheavailablekeypointnamesfortheusedmanipulatorandassetandtheirdescriptions.
...
===========================
Herearesomeexamplesoftheconstraintpartofsomeconfigs.
...
===========================
Notethat,intheconstraintlist,youneedtodefinedifferentitemsofconstrainttodefineanactua-
tionposeforthetask. Therearesomepre-definedtypesofconstraintsyoucanuse:
(1)point2point constraint: Thisconstraintisusedtoensuretwokeypoints(“keypoint name”and
“target keypoint name”,respectivelyonthetoolandobject)areincontact.
(2)frame axis parallel: Thisconstraintisusedtoensuretwoaxes(respectivelyonthetoolandob-
ject)areparallel. Theaxisonthetoolisdefinedbyaunitvectorfrom“axis from keypoint name”
to“axis to keypoint name”,whiletheaxisontheobjectisdefinedby“target axis”([1,0,0]or
[0,1,0]or[0,0,1])whichisinthecoordinateframeof“target axis frame”(worldorobject).
(3)frame axis orthogonal: Thisconstraintisusedtoensuretwoaxes(respectivelyon
thetoolandobject)areorthogonal. Theaxisonthetoolisdefinedbyaunitvectorfrom
“axis from keypoint name”to“axis to keypoint name”,whiletheaxisontheobjectisde-
finedby“target axis”([1,0,0]or[0,1,0]or[0,0,1])whichisinthecoordinateframeof“tar-
get axis frame”(worldorobject).
(4)keypoint axis parallel: Thisconstraintisusedtoensuretwoaxes(respectivelyon
thetoolandobject)areparallel. Theaxisonthetoolisdefinedbyaunitvectorfrom
“axis from keypoint name”to“axis to keypoint name”,whiletheaxisontheobjectisdefinedby
anotherunitvectorfrom“target axis from keypoint name”to“target axis to keypoint name”.
(5)keypoint axis orthogonal: Thisconstraintisusedtoensuretwoaxes(respectivelyon
thetoolandobject)areorthogonal. Theaxisonthetoolisdefinedbyaunitvectorfrom
“axis from keypoint name”to“axis to keypoint name”,whiletheaxisontheobjectisdefinedby
anotherunitvectorfrom“target axis from keypoint name”to“target axis to keypoint name”.
Thetoleranceisusedtodefinethetoleranceoftheconstraint.
Thetarget inner productisusedtodefinetheinnerproductbetweenthetwoaxes. Forexam-
ple,ifyouwanttoensuretwoaxesareparallelandinthesamedirection,youcansetthetar-
get inner productto1.0. Ifyouwanttoensuretwoaxesareparallelandofoppositedirections,
youcansetthetarget inner productto-1.0. Ifyouwanttoensuretwoaxestobeorthogonal,you
cansetthetarget inner productto0.0.
Usually,youneedtodefineonepoint2point constrainttoensurecontactandseveralaxiscon-
straintstoadjusttheactuationpose.
===========================
Nowpleasefirstgeneratetheconstraintpartfortask“TASK NAME TEMPLATE”inthesame
configformatastheabove.
Donotusetermsthatyouhavenotseenbefore.
TheoutputshouldbeintheYAMLformatwithnoextratext.
Prompt: kPAMSolverStage2
Youareanexpertinsolvingrobotictasksbyprovidingsomemotionplansandcodingtasksolu-
tionconfigsfora2-fingerrobotarm. Nowpleasesolvethenewlygeneratedtaskbygeneratingthe
tasksolutionconfig.
Thetasksolutionconfigcontainsthenecessarypositions,parameters,andkeypointsforanexisting
trajectoryoptimizationalgorithmtosolveafeasiblesolution. Itmainlycontainstwoparts,con-
straintsandpre/post-actuationmotions:
(1)Theconstraintsareusedtoensurethegripperisincontactwiththeobjectandtoimplicitly
defineacertainactuationpose. Anactuationposemeansthekeyframethattherobotarmmanipu-
latestheobject,usuallyrepresentingthemomentwhenthegrippergetscontactwiththeobject.
(2)Thepre-actuationmotionsareusedtomovethegrippertotheactuationpose,whilethepost-
actuationmotionsareusedtocompletethetaskaftertheactuationpose.
===========================
Hereisthetaskdescription.
...
===========================
Herearealltheavailablekeypointnamesfortheusedmanipulatorandassetandtheirdescriptions.
...
===========================
26Herearesomeexamplesofthepre/post-actuationpartofsometasksolutionconfigs.
...
===========================
NowpleasegenerateSOLVER TRIALSdifferentpre/post-actuationmotionsfortask
“TASK NAME TEMPLATE”followingthesameconfigformatshownabovebasedonthecon-
straintpartthatisgeneratedpreviously. Thepre/post-actuationmotionsofdifferentsolutionscan
bediverse,buttheirtasknamesshouldbethesame.
Donotusetermsthatyouhavenotseenbefore.
TheoutputshouldbeintheYAMLformatwithnoextratext.
Thediversityofthemotionscanbeachievedbyusingdifferentaxesfortranslation.
Noticethatthepre-actuationandpost-actuationmotionsarerelativetotheactuationposeandthe
translationmotionsarerepresentedincoordinatesrelativetothemanipulatorbase.
Let’sthinkstepbystep,andtryyourbesttounderstandthejob.
27E.2 ExampleforMulti-modalLLMResponse
We provide an example conversation when generating a solver config for the task “close-drawer”
withamulti-modalLLM,andcompareitwiththeresponsegeneratedbyavanillaLLM.Someof
thepromptdetailsareomitted,andpleaserefertoAppendixEforthecompleteprompts.
ExampleforMulti-modalLLMResponse
Input:
Youareanexpertinsolvingrobotictasksbycodingtasksolutionconfigs.Nowpleasesolvethenewly
generatedtaskbygeneratingthetasksolutionconfig.
......
===========================
Hereisthetaskdescription.
{
“task-name”:“close-drawer”,
“task-description”:“pushadrawertoaclosedposition”,
“assets-used”:[“drawer”],
“success-criteria”:[“articulated closed”]
}
===========================
......
Theinitialsceneofthistaskisvisualizedinthefirstuploadedimage. Theframeaxesofthegripper
and the object are respectively visulized in the second and third images, where red, green and blue
axesrepresentX,YandZaxesrespectively.
......
Nowpleasefirstgeneratetheconstraintpartfortask“close-drawer”inthesameconfigformatasthe
above.
Output:
......
task name:CloseDrawer
category name:Articulated
tool keypoint name list:[tool head,tool tail,tool side]
object keypoint name list:[articulated object head]
constraint list:
-keypoint name:tool head
target keypoint name:articulated object head
tolerance:0.0001
type:point2point constraint
-axis from keypoint name:tool head
axis to keypoint name:tool side
target axis:[1.0,0,0]
28target axis frame:object
tolerance:0.01
target inner product:1
type:frame axis parallel
-axis from keypoint name:tool head
axis to keypoint name:tool tail
target axis:[0,0,1.0]
target axis frame:object
tolerance:0.01
target inner product:0
type:frame axis orthogonal
Input:
......
Theinitialsceneofthistaskisvisualizedinthefirstuploadedimage.Theactuationposeofthistaskis
visualizedintheseconduploadedimage.Theframeaxesofthegripperandtheobjectarerespectively
visualizedinthethirdandfourthimages, wherered, green, andblueaxesrepresenttheX,Y,andZ
axesrespectively.
......
Now please generate 3 different pre/post-actuation motions for task “close-drawer” following the
same config format shown above based on the constraint part that was generated previously. The
pre/post-actuation motions of different solutions can be diverse, but their task names should be the
same.
......
Output:
......
task name:CloseDrawer
category name:Articulated
#Pre-actuationmotionsforthemanipulator.
pre actuation motions:
-[”translate x”,-0.1]
-[”translate z”,-0.15]
#Post-actuationmotionsforthemanipulator.
post actuation motions:
-[”translate x”,0.1]
Success!
29ExampleforVanillaLLMResponse
Input:
Youareanexpertinsolvingrobotictasksbycodingtasksolutionconfigs.Nowpleasesolvethenewly
generatedtaskbygeneratingthetasksolutionconfig.
......
===========================
Hereisthetaskdescription.
{
“task-name”:“close-drawer”,
“task-description”:“pushadrawertoaclosedposition”,
“assets-used”:[“drawer”],
“success-criteria”:[“articulated closed”]
}
===========================
......
Nowpleasefirstgeneratetheconstraintpartfortask“close-drawer”inthesameconfigformatasthe
above.
Output:
......
task name:CloseDrawer
category name:Articulated
tool keypoint name list:[tool head,tool tail,tool side]
object keypoint name list:[articulated object head]
constraint list:
-keypoint name:tool head
target keypoint name:articulated object head
tolerance:0.0001
type:point2point constraint
-axis from keypoint name:tool side
axis to keypoint name:tool tail
target axis:[0,1.0,0] Visualizationof Actuation Pose
target axis frame:world
tolerance:0.01
target inner product:1
type:frame axis parallel
-axis from keypoint name:tool head
axis to keypoint name:tool tail
target axis:[0,0,1.0]
target axis frame:world
tolerance:0.01
target inner product:0
type:frame axis orthogonal
Failtogeneratecorrectconstraints!
30E.3 ExampleforReasoningLLMResponse
Weprovideanexampleconversationwhengeneratingasolverconfigforthetask“close-safe”witha
reasoningLLM,andcompareitwiththeresponsegeneratedbyavanillaLLM.Someoftheprompt
details are omitted, and please refer to Appendix E for the complete prompts. We highlight some
limitationsofapplyingvisionlanguagefoundationmodelstoroboticsimulationtoday,butweexpect
furtherprogress,suchasfinetuningAPIs,wouldbeabletocorrectsomeofthesefailuremodes.
ExampleforReasoningLLMResponse
Input:
Youareanexpertinsolvingrobotictasksbycodingtasksolutionconfigs.Nowpleasesolvethenewly
generatedtaskbygeneratingthetasksolutionconfig.
......
===========================
Hereisthetaskdescription.
{
“task-name”:“close-safe”,
“task-description”:“Closethesafedoor”,
“assets-used”:[“safe rotate”],
“success-criteria”:[“articulated closed”]
}
===========================
......
Nowpleasefirstgeneratetheconstraintpartfortask“close-safe”inthesameconfigformatastheabove.
Output:(Reasoningpartstrackedwithblue)
Step1:UnderstandtheTaskandKeypoints
-Task:Closethesafedoor.
-ManipulatorKeypoints:
-tool head:Middlepointbetweenthetwofingertips.
-tool tail:Middlepointbetweenthetwofingerbases.
-tool side:Tipoftherightfinger.
-ArticulatedObjectKeypoints:
-articulated object head:Middlepointofthesafehandle.
-articulated object inside base:Pointinsidethesafetodropobjects.
Step2:PlanFeasibleConstraints
Weneedtodefineconstraintsthatcanbesatisfiedandaredifferentfromthepreviousattempts.
Constraintstypicallyinclude:
1. Point-to-Point Constraint: Ensure contact between a keypoint on the tool and a keypoint on the
object.
2.AxisConstraints:Orientthetoolrelativetotheobjecttofacilitatetheaction.
Config1
Constraint1:Point-to-PointConstraint
-Purpose:Ensuretherightfingertipcontactsthesafehandle.
-Implementation:
-keypoint name:tool side
target keypoint name:articulated object head
tolerance:0.0001
type:point2point constraint
Constraint2:FrameAxisParallelConstraint
-Purpose:Alignthegripper’sZ-axiswiththedoor’sclosingdirection.
31-Assumptions:
-Thedoorclosesalongtheobject’sX-axis([1,0,0]).
-Thegripper’sZ-axisisfromtool tailtotool head.
-Implementation:
-axis from keypoint name:tool tail
axis to keypoint name:tool head
target axis:[1.0,0,0]
target axis frame:object
tolerance:0.01
target inner product:1.0
type:frame axis parallel
Constraint3:FrameAxisParallelConstraint
-Purpose:Alignthegripper’sX-axiswiththedoor’shingeaxis.
-Assumptions:
-Thedoor’shingeaxisisalongtheobject’sZ-axis([0,0,1]).
-Thegripper’sX-axisisfromtool headtotool side.
-Implementation:
-axis from keypoint name:tool head
axis to keypoint name:tool side
target axis:[0,0,1.0]
target axis frame:object
tolerance:0.01
target inner product:1.0
type:frame axis parallel
Config1
task name:CloseSafe
category name:Articulated
tool keypoint name list:[tool head,tool tail,tool side]
object keypoint name list:[articulated object head,articulated object inside base]
constraint list:
#Ensuretherightfingertipcontactsthesafehandle
-keypoint name:tool side
target keypoint name:articulated object head
tolerance:0.0001
type:point2point constraint
#Aligngripper’sZ-axiswiththedoor’sclosingdirection
-axis from keypoint name:tool tail
axis to keypoint name:tool head
target axis:[1.0,0,0]
target axis frame:object
tolerance:0.01
target inner product:1.0
type:frame axis parallel
#Aligngripper’sX-axiswiththedoor’shingeaxis
-axis from keypoint name:tool head
axis to keypoint name:tool side
target axis:[0,0,1.0]
target axis frame:object
tolerance:0.01
target inner product:1.0
type:frame axis parallel
Summary
Right finger tip contacts the safe handle, with the gripper’s Z-axis aligned with the door’s closing
direction,andX-axisalignedwiththedoor’shingeaxis.
32These configurations are different from the previous ones and aim to provide feasible solutions for
closingthesafedoor.
Input:
......
Nowpleasegenerate3differentpre-actuationmotionsfortask“close-safe”followingthesameconfig
format shown above based on the constraint part that is generated previously. The pre-actuation
motionsofdifferentsolutionscanbediverse,buttheirtasknamesshouldbethesame.
......
Output:
......
task name:CloseSafe
category name:Articulated
#Pre-actuationmotionsforthemanipulator.
#Eachmotionisrepresentedinthe[mode,value]format.
#Mode:translate x,translate y,translate z,orrotate.
#Value:distanceinmetersorangleinradians.
pre actuation motions:
-[”translate z”,-0.1] #[liftgripperabovehandletoapproachfromabove]
Input:
......
Nowpleasegenerate3differentpost-actuationmotionsfortask“close-safe”followingthesameconfig
format shown above based on the constraint part that is generated previously. The post-actuation
motionsofdifferentsolutionscanbediverse,buttheirtasknamesshouldbethesame.
......
Output:(Reasoningpartstrackedwithblue)
Step1:UnderstandtheTask
We need to generate three different post-actuation motion plans for the task“close-safe”. The post-
actuationmotionsaretheactionsthemanipulatorperformsafterreachingtheactuationposetocom-
pletethetaskofclosingthesafedoor.
Step2:ConsiderPossibleMotions
Sincethegoalistoclosethesafedoor,weneedtodecidehowthemanipulatorcanmovetoachieve
this,usingonlytheallowedmotionset:
-Option1:Pushthedoorforwardtocloseit.
Step3:DefinetheMotionsAccordingtotheRobot’sCoordinateSystem
-ForwardDirection:Alongtheredaxis;usefulforpushingactions.
-BackwardDirection:Oppositetotheredaxis;usefulforpullingactions.
-LeftDirection:Alongthegreenaxis;canbeusedtoswingthedoorclosedifithingesthatway.
-RightDirection:Oppositetothegreenaxis.
- Up/Down Directions: Along the blue axis; may not be as relevant for closing a door but can be
includedforcompleteness.
Step4:GenerateMotionPlans
Config1:Pushthedoorclosedbymovingforward.
-Post-actuationmotions:[”move-forward”]
33Step5:EnsureCompliancewithConstraints
-Eachmotionplanusesonlytheallowedmotions.
-Motionsarewithinthe3-steplimit.
-Directionsaredefinedaccordingtotherobot’scoordinatesystem.
-Plansavoidcollisionwiththetable(e.g.,nodownwardmovementsthatmightcauseacollision).
Step6:OutputtheConfigsinYAMLFormat
task name:CloseSafe
category name:Articulated
#Post-actuationmotionsforthemanipulator.
#Post-actuationmotionsshouldbeintheformatofapythonlist. Eachelementinthelistshouldbea
stringwithlower-caseandseparatedbyhyphens.
#post-actuationposelist
post actuation motions:[”move-forward”] #[pushdoorclosed]
Success!
34ExampleforVanillaLLMResponse
Input:
Youareanexpertinsolvingrobotictasksbycodingtasksolutionconfigs.Nowpleasesolvethenewly
generatedtaskbygeneratingthetasksolutionconfig.
......
===========================
Hereisthetaskdescription.
{
“task-name”:“close-safe”,
“task-description”:“Closethesafedoor”,
“assets-used”:[“safe rotate”],
“success-criteria”:[“articulated closed”]
}
===========================
......
Nowpleasefirstgeneratetheconstraintpartfortask“close-drawer”inthesameconfigformatasthe
above.
Output:
......
task name:CloseSafe
category name:Articulated
tool keypoint name list:[tool head,tool tail,tool side]
object keypoint name list:[articulated object head]
constraint list:
#ensurethegripper’stool headisincontactwiththesafehandle-keypoint name:tool head
target keypoint name:articulated object head
tolerance:0.0001
type:point2point constraint
# ensure the gripper’s top side is aligned with
thesafehandle’sdirection(assumingthehandle
movesintheworld’sYdirection) Visualizationof Actuation Pose
-axis from keypoint name:tool head
axis to keypoint name:tool tail
target axis:[0,1.0,0]
target axis frame:world
tolerance:0.01
target inner product:1
type:frame axis parallel
#ensurethegripper’sfrontsideisperpendicular
totheworld’sYdirection
-axis from keypoint name:tool tail
axis to keypoint name:tool side
target axis:[0,1.0,0]
target axis frame:world
tolerance:0.01
target inner product:0
type:frame axis orthogonal
Failtogeneratecorrectconstraints!
35E.4 ExampleFailureCases
Weprovidesomefailurecasesofdifferentstagesinthissection.
FailureCaseforTaskProposal
Input:
You are an expert in creating robotic simulation environments and tasks. You are given some
articulatedassetsforexample. Pleasecomeupwithcreativeuseofthegrippertomanipulateasingle
articulated object. Note that the simulation engine does not support deformable objects or accurate
collisionmodelsforcontacts.Moreover,therobotcanonlyexecuteasimpletrajectoryofonemotion.
......
Format the answer in a Python dictionary with keys ”task-name” and value type string, ”task-
description” (one short phrase), and value type string with lower-case and separated by hyphens,
”assets-used” and value type list of strings, and ”success-criteria” (choose from ”articulated open”,
”articulated closed”, ”distance articulated rigidbody”, ”distance gripper rigidbody”, and ”dis-
tance gripper articulated”)andvaluetypelistofstrings.Trytobeascreativeaspossible.
......
Output:
......
{
“task-name”:“unlock-safe”,
“task-description”:“rotatethesafe’shandletoahorizontalpositiontosimulateunlocking”,
“assets-used”:[“safe rotate”],
“success-criteria”:[“articulated open”]
}
Failtogenerateacorrecttask!
Failurereason:Thehandleisnotrotatable.
FailureCaseforTaskDecomposition
Input:
You are an expert in creating robotic simulation environments and tasks. You will be given long-
horizontaskswitheachtaskincludingatleast2sub-tasks. Eachsub-taskcanonlyincludeonesimple
motionsuchasmovingthegrippertosomeobject,openingorclosingthegripperfingers,orinteracting
withcertainarticulatedobjectsbyitsprismatic/revolutejoints.
......
Nowpleasestarttogenerateasub-taskdecompositionofthefollowingnewtask:
{
“task-name”:“tidy-up-foam-brick”,
“task-description”: “Pickupthefoambrick,openthemicrowavedoor,placethebrickinside,and
closethedoor.”,
“assets-used”:[“microwave”,“foam brick”]
}
......
36Output:
#Sub-task1
{
“task-name”:“reach-foam-brick”,
“task-description”:“reachthefoambrickandmakethefoambrickbetweengripperfingers”,
“assets-used”:[“foam brick”],
“success-criteria”:[“distance gripper rigidbody”]
}
#Sub-task2
{
“task-name”:“grasp”
}
#Sub-task3
{
“task-name”:“open-microwave-door”,
“task-description”:“openthemicrowavedoorbyinteractingwithitsrevolutejoint”,
“assets-used”:[“microwave”],
“success-criteria”:[“articulated open”]
}
#Sub-task4
{
“task-name”:“place-inside-microwave”,
“task-description”:“movethegripperwiththefoambrickinsidethemicrowave”,
“assets-used”:[“microwave”],
“success-criteria”:[“distance articulated rigidbody”]
}
#Sub-task5
{
“task-name”:“ungrasp”
}
#Sub-task6
{
“task-name”:“close-microwave-door”,
“task-description”:“closethemicrowavedoorbyinteractingwithitsrevolutejoint”,
“assets-used”:[“microwave”],
“success-criteria”:[“articulated closed”]
}
Failtogenerateacorrecttaskdecomposition!
Failurereason:Doorshouldbeopenbeforetheobjectisgrasped.
FailureCaseforSolverCreation
Input:
Youareanexpertinsolvingrobotictasksbycodingtasksolutionconfigs.Nowpleasesolvethenewly
generatedtaskbygeneratingthetasksolutionconfig.
......
===========================
Hereisthetaskdescription.
{
“task-name”:“open-microwave”,
“task-description”:“openthemicrowavedoor”,
37“assets-used”:[“microwave”],
“success-criteria”:[“articulated open”]
}
===========================
......
Theinitialsceneofthistaskisvisualizedinthefirstuploadedimage. Theframeaxesofthegripper
and the object are respectively visulized in the second and third images, where red, green and blue
axesrepresentX,YandZaxesrespectively.
......
Nowpleasefirstgeneratetheconstraintpartfortask“close-drawer”inthesameconfigformatasthe
above.
Output:
......
task name:OpenMicrowave
category name:Articulated
tool keypoint name list:[tool head,tool tail,tool side]
object keypoint name list:[articulated object head]
constraint list:
-keypoint name:tool head
target keypoint name:articulated object head
tolerance:0.0001
type:point2point constraint
-axis from keypoint name:tool head
axis to keypoint name:tool tail
target axis:[0,0,1.0] Visualizationof Actuation Pose
target axis frame:world
tolerance:0.01
target inner product:0
type:frame axis orthogonal
-axis from keypoint name:tool head
axis to keypoint name:tool side
target axis:[0,1.0,0]
target axis frame:object
tolerance:0.01
target inner product:1
type:frame axis parallel
Failtogenerateacorrectsolverconfig!
Failurereason:Thecorrespondingactuationposecannotcompletethetask.
38