Nonstationary Sparse Spectral Permanental Process
ZichengSun1§,YixuanZhang2§,ZenanLing3,XuhuiFan4FengZhou1,5∗
1CenterforAppliedStatisticsandSchoolofStatistics,RenminUniversityofChina
2China-AustriaBeltandRoadJointLaboratoryonAIandAM,HangzhouDianziUniversity
3SchoolofEIC,HuazhongUniversityofScienceandTechnology
4SchoolofComputing,MacquarieUniversity
5BeijingAdvancedInnovationCenterforFutureBlockchainandPrivacyComputing
{sunzicheng2020, feng.zhou}@ruc.edu.cn, yixuan.zhang@hdu.edu.cn
Abstract
Existingpermanentalprocessesoftenimposeconstraintsonkerneltypesorsta-
tionarity, limiting the model’s expressiveness. To overcome these limitations,
weproposeanovelapproachutilizingthesparsespectralrepresentationofnon-
stationary kernels. This technique relaxes the constraints on kernel types and
stationarity, allowingformoreflexiblemodelingwhilereducingcomputational
complexitytothelinearlevel. Additionally,weintroduceadeepkernelvariant
byhierarchicallystackingmultiplespectralfeaturemappings,furtherenhancing
the model’s expressiveness to capture complex patterns in data. Experimental
results on both synthetic and real-world datasets demonstrate the effectiveness
ofourapproach,particularlyinscenarioswithpronounceddatanonstationarity.
Additionally, ablationstudiesareconductedtoprovideinsightsintotheimpact
ofvarioushyperparametersonmodelperformance. Codeispubliclyavailableat
https://github.com/SZC20/DNSSPP.
1 Introduction
Manyapplicationdomainsinvolvepointprocessdata,whichrecordsthetimesorlocationsofevents
occurring within a region. Point process models are used to analyze these event data, aiming to
uncover patterns of event occurrences. The Poisson process [15], as an important model in the
fieldofpointprocesses,playsasignificantroleinneuroscience[4],finance[11],criminology[31],
epidemiology[5],andseismology[8]. TraditionalPoissonprocessesassumeparameterizedintensity
functions,whichseverelyrestrictstheflexibilityofthemodel. Toaddressthisissue,aviablesolution
istoemployBayesiannonparametricmethods,byimposinganonparametricGaussianprocess(GP)
priorontheintensityfunction,resultingintheGaussianCoxprocess[3]. Thisgreatlyenhancesthe
flexibilityofthemodel,whilealsoendowingitwithuncertaintyquantificationcapabilities.
InGaussianCoxprocesses,posteriorinferenceoftheintensityischallengingbecausetheGPprioris
notconjugatetothePoissonprocesslikelihoodthatincludesanintensityintegral. Furthermore,to
ensurethenon-negativityoftheintensity,weneedtousealinkfunctiontotransformtheGPprior.
Commonlyusedlinkfunctionsincludeexponential[20;12],sigmoid[1;9;6],square[18;7;29],
ReLU[16],andsoftplus[24]. Amongthese,usingthesquarelinkfunction,i.e.,thepermanental
process[19],allowsfortheanalyticalcomputationoftheintensityintegral[18;33;29],thusreceiving
widespreadattentioninrecentyears. Forthisreason,thisworkfocusesonthepermanentalprocess.
Currently, the permanental process faces three issues: (1) The permanental process inherits the
notoriouscubiccomputationalcomplexityofGPs,makingitimpracticalforusewithalargeamount
∗Correspondingauthor.
§Equalcontributions.
Preprint.Underreview.
4202
tcO
4
]LM.tats[
1v18530.0142:viXraofdata[18;14]. (2)Existingworksonpermanentalprocesseseitherrequirecertainstandardtypes
ofkernels,suchasthesquaredexponentialkernel,etc.,toensurethattheintensityintegralhasan
analyticalsolution[18;7;33];ortheyrequirethekernelstobestationary[14;29]. Theseconstraints
limittheexpressivepowerofthemodel. (3)Furthermore,existingworkshavepredominantlyutilized
simpleshallowkernels,whichrestrictstheflexibilityofthemodel. Althoughsomeshallowkernels
[34;28]areflexibleandtheoreticallycapableofapproximatinganyboundedkernel,theyarelimited
inrepresentingcomplexkernelsduetocomputationalconstraintsinpracticalusage.
Inthisstudy,weutilizethesparsespectralrepresentationofnonstationarykernelstoaddressthese
limitations in modeling the permanental process. The sparse spectral representation provides a
low-rankapproximationofthekernel,effectivelyreducingthecomputationalcomplexityfromcubic
to linear level. The nonstationary sparse spectral kernel overcomes the limitation of stationary
assumption. Bytreatingthefrequenciesaskernelparametersforoptimization,wecandirectlylearn
thenonstationarykernelfromthedatainaflexiblemannerwithoutrestrictingthekernel’sform. We
furtherextendtheshallownonstationarysparsespectralkernelbyhierarchicallystackingmultiple
spectralfeaturemappingstoconstructadeepkernel,whichexhibitssignificantlyenhancedexpressive
powercomparedtoshallowones. WetermtheconstructedmodelasNonstationarySparseSpectral
PermanentalProcess(NSSPP)andthecorrespondingdeepkernelvariantasDNSSPP.
Weconductexperimentsonbothsyntheticandreal-worlddatasets. Theresultsindicatethatwhenthe
datais(approximately)stationary,(D)NSSPPachievessimilarperformancetostationarybaselines.
However,whenthenonstationarityinthedataispronounced,(D)NSSPPcanoutperformbaseline
models,demonstratingthesuperiorityof(D)NSSPP.Additionally,weperformablationstudiesto
assesstheimpactofvariousmodelhyperparameters.
2 RelatedWork
Inthissection,wepresentsomerelatedworksonhowtoreducethecomputationalcomplexityofthe
stationarypermanentalprocess,aswellassomeresearchonnonstationarykernels.
EfficientPermanentalProcess DuetotheadoptionoftheGPpriorinthepermanentalprocess,
posteriorinferenceinvolvescomputingtheinverseofthekernelmatrix.Thisresultsinacomputational
complexityofO(N3),whereN isthenumberofdatapoints. Toreducecomputationalcomplexity,
several works have introduced low-rank approximation methods from the GP domain into the
permanental process, such as inducing points [18], Nyström approximation [7; 33], and spectral
representation[14;29]. Thesemethodsessentiallyinvolvelow-rankapproximationsofthekernel
matrix,therebyreducingthecomputationalcomplexityfromO(N3)toO(NR2),whereR≪N is
thenumberofrank,suchasthenumberofinducingpointsorfrequencies. Inthiswork,weutilizethe
spectralrepresentationmethod,reducingthecomputationalcomplexityfromcubictolinearlevel.
Anotherreasonforadoptingthespectralrepresentationisthatitfacilitatestheconstructionofthe
subsequentdeepnonstationarykernel.
NonstationaryKernel Stationarykernelsassumethatthesimilaritybetweendifferentlocations
dependsonlyontheirrelativedistance. However,previousstudieshaveindicatedthatthestationary
assumptionisnotsuitablefordynamiccomplextasks,asthesimilarityisnotconsistentacrossthe
inputspace[23]. Toaddressthisissue,someworksproposednonstationarykernelsbytransforming
theinputspace[30]orusinginput-dependentparameters[10]. Additionally,otherworksleveraged
thegeneralizedFouriertransformofkernels[37]toproposenonstationaryspectralkernels[26;32].
Althoughtheaforementionedshallownonstationarykernelsareflexibleandtheoreticallycapableof
approximatinganyboundedkernels,theyarelimitedinrepresentingcomplexkernelsinpractical
usage. In recent years, many deep architectures have been introduced into kernels, significantly
enhancingtheirexpressivepower[35;36]. Despitethedevelopmentof(deep)nonstationarykernels
inGP,interestingly,tothebestofourknowledge,therehasbeennopriorworkapplyingthemin
GaussianCoxprocesses. Thisgapiswhatthisworkseekstoaddress.
3 Preliminaries
Inthissection,weprovidesomefundamentalknowledgeaboutthepermanentalprocessandsparse
spectralkernel.
23.1 PermanentalProcess
ForaPoissonprocess,ifx∈X ⊂RD,theintensityfunctionλ(x)=lim E[N([x,x+δ ])]/δ
δx→0 x x
canbeusedtoquantifytherateofeventsoccurringatlocationx. ACoxprocesscanbeviewedas
aPoissonprocesswhoseintensityfunctionitselfisarandomprocess. TheGaussianCoxprocess
employs the GP to model the intensity function: λ(x) = l◦f(x) where f is a GP function and
l:R→R+isalinkfunctiontoensurethenon-negativityoftheintensityfunction.
ThenonparametricnatureofGPenablestheGaussianCoxprocesstobeflexibleinmodelingevents
inspaceortime. However,theposteriorinferenceofGaussianCoxprocessischallenging. According
tothelikelihoodfunctionofaPoissonprocess:
N (cid:18) (cid:90) (cid:19)
p({x }N |λ(x)=l◦f(x))=(cid:89) λ(x )exp − λ(x)dx , (1)
i i=1 i
i=1 X
andtheBayesianframework,theposterioroflatentfunctionf canbewrittenas:
(cid:81)N
λ(x
)exp(−(cid:82)
λ(x)dx)GP(f|0,k)
p(f|{x }N )= i=1 i X . (2)
i i=1 (cid:82) (cid:81)N
λ(x
)exp(−(cid:82)
λ(x)dx)GP(f|0,k)df
i=1 i X
(cid:82)
InEq.(2),therearetwointegrals. Thefirstone λ(x)dxcannotbesolvedduetothestochastic
X
natureoff.Additionally,theintegraloverf inthedenominatorisafunctionalintegralinthefunction
space,whichisalsoinfeasible. Thisissueisthewell-knowndoubly-intractableproblem.
The link function l : R → R+ has many choices, such as exponential, sigmoid, square, ReLU,
andsoftplus. Thisworkfocusesontheutilizationofthesquarelinkfunction, alsoknownasthe
permanentalprocess. Furthermore,ifwesetthekernelintheGPpriortobestationary: k(x ,x )=
1 2
k(x −x ),thenthemodelisreferredtoasastationarypermanentalprocess.
1 2
3.2 SparseSpectralKernel
Thesparsespectralrepresentationisacommonmethodforthelow-rankapproximationofkernels.
ThisapproachisbasedonBochner’stheorem.
Theorem 3.1. [2] A stationary kernel function k(x ,x ) = k(x −x ) : RD → R is bounded,
1 2 1 2
continuous,andpositivedefiniteifandonlyifitcanberepresentedas:
(cid:90)
k(x −x )= exp(iω⊤(x −x ))dµ(ω),
1 2 1 2
RD
whereµ(ω)isaboundednon-negativemeasureassociatedtothespectraldensityp(ω)= µ(ω) .
µ(RD)
Definingϕ(x)=exp(iω⊤x),wehavek(x −x )=σ2E [ϕ(x )ϕ(x )∗]where∗denotesthe
1 2 p(ω) 1 2
complexconjugateandσ2 =µ(RD).IfweutilizetheMonteCarlotosample{ω }R independently
r r=1
fromp(ω),thenthekernelcanbeapproximatedby
σ2 (cid:88)R
k(x −x )≈ ϕ (x )ϕ (x )∗ =Φ(R)(x )⊤Φ(R)(x )∗, (3)
1 2 R r 1 r 2 1 2
r=1
whereϕ r(x)=exp(iω r⊤x),Φ(R)(x)= √σ R[ϕ 1(x),··· ,ϕ R(x)]⊤,whichcorrespondstotherandom
Fourierfeatures(RFF)proposedby[25]. ItcanbeprovedthatEq.(3)isequivalenttoa2R-sized
trigonometricrepresentation(proofisprovidedinAppendixA):
σ
Φ(R)(x)= √ [cos(ω⊤x),··· ,cos(ω⊤x),sin(ω⊤x),··· ,sin(ω⊤x)]⊤. (4)
1 R 1 R
R
Whenwespecifythefunctionalformofthekernel(spectralmeasureµ),theRFFmethodcanlearn
theparametersofthekernel(spectralmeasureµ)fromthedata. However,whenthefunctionalform
ofthekernel(spectralmeasureµ)isunknown,RFFcannotflexiblylearnthekernel(spectralmeasure
µ)fromthedata. Anotherapproachistotreatthefrequencies{ω }R askernelparameters. By
r r=1
optimizingthesefrequencies,wecandirectlylearnthekernelfromthedatainaflexiblemanner,
whichcorrespondstothesparsespectrumkernelmethodproposedby[17].
34 OurModel
ThepermanentalprocessinheritsthecubiccomputationalcomplexityofGPs,renderingitimpractical
forlarge-scaledatasets. Toefficientlyconductposteriorinference,manyworksemployedvarious
low-rankapproximationmethods[18;7;33]. Thesemethodsrequirecertainstandardtypesofkernels,
suchasthesquaredexponentialkernel,polynomialkernel,etc.,toensurethattheintensityintegral
hasanalyticalsolutions. Thislimitationseverelyaffectstheflexibilityofthemodel. Toaddressthis
issue,sparsespectralpermanentalprocessesareproposedinrecentyears[14;29]. Theadvantage
ofthismethodliesinitsabilitytoanalyticallycomputetheintensityintegral,whilenotrequiring
restrictionsonthekernel’sfunctionalform. However,thedrawbackisthatitisonlyapplicableto
stationarykernels. Therefore,anaturalquestionarises: canwefurthergeneralizethesparsespectral
permanentalprocessestonotonlyremoverestrictionsonthekernel’sfunctionalformbutalsomake
themapplicabletononstationarykernels?
4.1 NonstationarySparseSpectralPermanentalProcess
ExistingsparsespectralpermanentalprocessesrelyontheprerequisiteofBochner’stheorem,which
limitstheextensionofthesparsespectralmethodtononstationarykernels. Fortunately,formore
generalnonstationarykernels,thespectralrepresentationsimilartoBochner’stheoremalsoexists.
Theorem 4.1. [37] A nonstationary kernel function k(x ,x ) : RD × RD → R is bounded,
1 2
continuous,andpositivedefiniteifandonlyifitcanberepresentedas:
(cid:90)
k(x ,x )= exp(cid:0) i(ω⊤x −ω⊤x )(cid:1) dµ(ω ,ω ),
1 2 1 1 2 2 1 2
RD×RD
whereµ(ω ,ω )isaLebesgue-Stieltjesmeasureassociatedtosomeboundedpositivesemi-definite
1 2
spectraldensityp(ω ,ω )= µ(ω1,ω2).
1 2 µ(RD,RD)
Similarly,wecanusetheMonteCarlomethodtoapproximateit. However,itisworthnotingthat,to
ensurethatthespectraldensityp(ω ,ω )ispositivesemi-definite,wemustfirstsymmetrizeit,i.e.,
1 2
p(ω ,ω )=p(ω ,ω );andsecondlyintroducediagonalcomponentsp(ω ,ω )andp(ω ,ω ). We
1 2 2 1 1 1 2 2
cantakeageneraldensityfunctiongontheproductspaceandthenparameterizep(ω ,ω )inthe
1 2
followingformtoensureitspositivesemi-definiteproperty[26;32]:
1
p(ω ,ω )= (g(ω ,ω )+g(ω ,ω )+g(ω ,ω )+g(ω ,ω )). (5)
1 2 4 1 2 2 1 1 1 2 2
Then,wecanobtainthesparsespectralfeaturerepresentationofanynonstationarykernel:
k(x ,x )=
σ2
E (cid:2) exp(cid:0) i(ω⊤x −ω⊤x )(cid:1) +exp(cid:0) i(ω⊤x −ω⊤x )(cid:1)
1 2 4 g(ω1,ω2) 1 1 2 2 2 1 1 2
+exp(cid:0) i(ω⊤x −ω⊤x )(cid:1) +exp(cid:0) i(ω⊤x −ω⊤x )(cid:1)(cid:3)
1 1 1 2 2 1 2 2
≈
σ2 (cid:88)R
(cid:2) exp(cid:0) i(ω⊤x −ω⊤x )(cid:1) +...+exp(cid:0) i(ω⊤x −ω⊤x )(cid:1)(cid:3) (6)
4R 1r 1 2r 2 2r 1 2r 2
r=1
=Φ(R)(x )⊤Φ(R)(x )+Φ(R)(x )⊤Φ(R)(x )+Φ(R)(x )⊤Φ(R)(x )+Φ(R)(x )⊤Φ(R)(x )
1 1 2 2 2 1 1 2 1 1 1 2 2 1 2 2
=φ(R)(x )⊤φ(R)(x ),
1 2
whereσ2 =µ(RD,RD),{ω ,ω }R areindependentsamplesfromg(ω ,ω ),and
1r 2r r=1 1 2
σ
Φ(R)(x)= √ [cos(ω⊤x),··· ,cos(ω⊤ x),sin(ω⊤x),··· ,sin(ω⊤ x)]⊤,
1 11 1R 11 1R
2 R
σ
Φ(R)(x)= √ [cos(ω⊤x),··· ,cos(ω⊤ x),sin(ω⊤x),··· ,sin(ω⊤ x)]⊤, (7)
2 21 2R 21 2R
2 R
φ(R)(x)=Φ(R)(x)+Φ(R)(x).
1 2
IftheGP’skernelinthepermanentalprocess(Eq.(2))adoptstheabovesparsespectralnonstationary
kernel,thenwerefertothismodelasNSSPP.
44.2 DeepNonstationarySparseSpectralPermanentalProcess
Wecanfurtherenhancetheexpressivepowerofthenonstationarykernelbystackingmultiplespectral
featuremappingshierarchicallytoconstructadeepkernel. Thespectralfeaturemappingφ(R)(x)
hasanotherequivalentform:
σ
φ(R)(x)= √ [cos(ω⊤x+b )+cos(ω⊤x+b ),...,cos(ω⊤ x+b )+cos(ω⊤ x+b )]⊤,
11 11 21 21 1R 1R 2R 2R
2R
(8)
where{b ,b }R areuniformlysampledfrom[0,2π]. ThederivationofEq.(8)isprovidedin
1r 2r r=1
AppendixB.
ThespectralfeaturemappinginEq.(8)canbeviewedasamorecomplexsingle-layerneuralnetwork:
itemploystwosetsofweights(ω)andbiases(b)tolinearlytransformtheinput,followedbythe
applicationofcosineactivationfunctions,andthenaddstheresultstoobtainasingleoutputdimension.
Bystackingmultiplelayersontopofeachother,wenaturallyconstructadeepnonstationarykernel:
Ψ(R)(x)=φ(R)◦φ(R) ◦···◦φ(R)(x), k(x ,x )≈Ψ(R)(x )⊤Ψ(R)(x ), (9)
L L−1 1 1 2 1 2
whereΨ(R)(x)isthespectralfeaturemappingwithLlayersandφ(R)denotesthel-thlayer§. This
l
actuallyrecoversthedeepspectralkernelproposedby[36]. IftheGP’skernelinthepermanental
process(Eq.(2))adoptstheabovedeepsparsespectralnonstationarykernel,thenwerefertothis
modelasDNSSPP.
DNSSPP exhibits enhanced expressiveness compared to NSSPP due to its deep architecture. In
subsequentsections,weconsistentlyuseΨ(R)asthespectralfeaturemapping. Whenithasasingle
layer,themodelcorrespondstoNSSPP;whenmultiplelayers,themodelcorrespondstoDNSSPP.
5 Inference
Fortheposteriorinferenceof(D)NSSPP,thisworkemploysafastLaplaceapproximationexploit-
ingthesparsespectralrepresentationofnonstationarykernels. Otherinferencemethods,suchas
variationalinference,arealsofeasible. Specifically,weextendthemethodproposedin[29]from
stationarykernelstononstationarykernels.
5.1 JointDistribution
WhenweapproximateakernelusingEq.(6)orEq.(9),theGPpriorcanalsobeapproximatedas:
f(x)≈β⊤Ψ(R)(x), β ∼N(0,I). (10)
(cid:0) (cid:1)2
Additionally,wesettheintensityfunctionasλ(x)= f(x)+α . Following[14],weintroducean
offsetαtomitigatethenodallineproblemcausedbythenon-injectivenessofλ = f2. Thisissue
resultsintheintensitybetweenpositiveandnegativevaluesoff beingforcedtozero,despitethe
underlyingintensitybeingpositive.
CombingthelikelihoodinEq.(1)andtheequivalentGPpriorinEq.(10),weobtainthejointdensity:
logp({x }N ,β|Θ)=logp({x }N |β,Θ)+logp(β)
i i=1 i i=1
(cid:88)N (cid:90) 1 (11)
= log((β⊤Ψ(R)(x )+α)2)− λ(x)dx− β⊤β+C,
i 2
i=1 X
whereC isaconstantandΘdenotesthemodelhyperparameters. Theintensityintegraltermcanbe
furtherexpressedas:
(cid:90)
λ(x)dx=β⊤Mβ+2αβ⊤m+α2|X|, (12)
X
§Forsimplicity,hereweassumealllayershavethesamewidthR,butitcanalsovary.
5where|X|isthewindowsize,MisaR×Rmatrix,andmisaR-sizedvectorwitheachentry:
(cid:90) (cid:90)
M = Ψ(R)(x)Ψ(R)(x)dx, m = Ψ(R)(x)dx, i,j ∈1,...,R. (13)
i,j i j i i
X X
Itisworthnotingthattheintensityintegralispotentiallyanalyticallysolvable. Forasingle-layer
spectralfeaturemapping(NSSPP),analyticalsolutionsforMandmexist(seeAppendixC).However,
formultiple-layermapping(DNSSPP),analyticalsolutionsarenotpossible. Inthefollowing,we
analyticallysolveMandmforNSSPP,whileusingnumericalintegrationforDNSSPP.
5.2 LaplaceApproximation
WeusetheLaplace’smethodtoapproximatetheposteriorp(β|{x }N ,Θ). TheLaplace’smethod
i i=1
approximatestheposteriorwithaGaussiandistributionbyperformingasecond-orderTaylorexpan-
sionaroundthemaximumofthelogposterior. Followingthecommonpractice:
1 1 D
logp(β|{x },Θ)≈logN(β|βˆ,Q)=− (β−βˆ)⊤Q−1(β−βˆ)− log|Q|− log2π, (14)
i 2 2 2
whereβˆisthemodeofthetrueposteriorandQisthenegativeinverseHessianofthetrueposterior
evaluatedatthemode:
∇ logp({x }N ,β|Θ)| =0, Q−1 =−∇2 logp({x }N ,β|Θ)| . (15)
β i i=1 β=βˆ β i i=1 β=βˆ
ThemodeβˆistypicallynotdirectlysolvablebasedonEq.(15),soweemployoptimizationmethods
tofindit. Then,theprecisionmatrixQ−1canbecomputedanalytically(seeAppendixD).
5.3 HyperparameterandComplexity
ThemodelhyperparameterΘconsistsofthekernelparametersσ,ω,b(iftherearemultiplelayers,
thesehyperparametersincludeparametersfromalllayers)andthebiasoftheintensityfunctionα.
Wecanlearnthesehyperparametersfromthedatabymaximizingthemarginallikelihood:
logp({x }N |Θ)
i i=1
=logp({x }N ,β|Θ)−logp(β|{x }N ,Θ)≈logp({x }N ,β|Θ)−logN(β|βˆ,Q). (16)
i i=1 i i=1 i i=1
SubstitutingEqs.(11)and(14)intoEq.(16),wecanobtainamarginallikelihoodapproximation.
OptimizingthisyieldstheoptimalhyperparameterΘ.
Similartocommonlow-rankapproximationmethodsforkernels,thecomputationalcomplexityof
theproposedinferencemethodisreducedfromO(N3)toO(NR2),whereR ≪ N,i.e.,linearly
withN. AcompletealgorithmpseudocodeisprovidedinAppendixE.
5.4 PredictiveDistribution
After obtaining the posterior β|{x }N ,Θ ∼ N(β|βˆ,Q), we can compute the posterior of the
i i=1
intensityfunction. Foranyx∗ ∈X,thepredictivedistributionoff(x∗)+αis
f(x∗)+α|{x }N ,Θ∼N(µ(x∗),σ2(x∗)),
i i=1
where
µ(x∗)=βˆ⊤Ψ(R)(x∗)+α, σ2(x∗)=Ψ(R)(x∗)⊤QΨ(R)(x∗).
Then,theintensityλ(x∗)=(cid:0) f(x∗)+α(cid:1)2
canbeproventohavethefollowingmeanandvariance:
E[λ(x∗)]=µ2(x∗)+σ2(x∗), Var[λ(x∗)]=2σ4(x∗)+4µ2(x∗)σ2(x∗). (17)
6 Experiments
Inthissection,wemainlyanalyzethesuperiorityinperformanceof(D)NSSPPoverbaselinemodels
onbothsyntheticandreal-worlddatasets, aswellastheimpactofvarioushyperparameters. We
performallexperimentsusingtheserverwithtwoGPUs(NVIDIATITANVwith12GBmemory),
twoCPUs(eachwith8cores,Intel(R)Xeon(R)CPUE5-2620v4@2.10GHz),and251GBmemory.
611 02 20 G V L
S
GB SBr So P PP Su P PP Pn Pd Truth N N D EvS S N eM S S nP SP tP PP sP 167.2169.4170.7170.1170.2 11 77 46 173.0175.3175.6175.1175.2 111 777 456
8 15 168.6172.7175.3175.1173.8 172 171.8174.8175.6175.0175.4 173
6 10 170 168.9171.5174.6175.7176.0 172
24 G V L S GB SBr So P PP Su P PP Pn Pd Truth N N D EvS S N eM S S nP SP tP PP sP 5 1 16 76 1. .4 11 17 72 5. .2 61 17 76 0. .1 51 17 63 3. .6 51 16 68 3. .8
4
11 66 68 1 16 68 8. .4 31 16 69 7. .9 81 17 73 0. .9 11 17 74 1. .7 41 17 75 2. .4
9
111 677 901
0 0 164 168
0 2 4 6 8 10 0 2 4 6 8 10 10 20 50 100 200 1 5 20 50 100
x x width epoch
(a)StationaryData (b)NonstationaryData (c)Widthv.s.depth (d)Epochv.s.learningrate
Figure1: (a)Thefittingresultsoftheintensityfunctionsforallmodelsonthestationarysynthetic
data;(b)thoseonthenonstationarysyntheticdata. Theimpactof(c)networkwidthanddepth,(d)
thenumberofepochsandlearningrateontheL ofDNSSPPonthenonstationarydata.
test
6.1 Baselines
Weemployseveralpreviousworksasbaselinesforcomparison.ThesebaselinesincludeVBPPwhich
utilizestheinducingpointsmethod[18],LBPPemployingtheNyströmapproximation[33],aswellas
SSPPandGSSPPutilizingtheFourierrepresentation[29]. WeuseGSSPPbasedonGaussiankernel,
Matérnkernelwithparameter1/2and5/2respectively,andwedenotethemasGSSPP,GSSPP-M12
andGSSPP-M52. Besides,weofferabaselinethatemploysstackedmappingsofstationarykernels
whichwenameitDeepSparseSpectralPermanentalProcess(DSSPP).Allofthesemethodsutilize
stationarykernels. Asfarasweknow,thereiscurrentlynoworkemployingnonstationarykernels
inpermanentalprocesses. Tocomparewithnonstationarybaselines,weimplementanonstationary
permanentalprocessusingthenonstationaryspectralmixturekernel[26],referredtoasNSMPP.
6.2 Metrics
We employ two metrics to evaluate the performance of various baselines. One is the expected
log-likelihoodonthetestdata,denotedasL ,withdetailsprovidedinAppendixF.Additionally,
test
whentheground-truthintensityisavailable,e.g.,forthesyntheticdata,therootmeansquareerror
(RMSE)betweentheexpectedposteriorintensityandthegroundtruthservesasanothermeasure.
6.3 SyntheticData
Datasets Tobettercomparetheperformanceof(D)NSSPPwithbaselinemodelsonpointprocess
datainbothstationaryandnonstationaryscenarios,wesimulatedastationaryandanonstationary
permanentalprocessontheinterval[0,10],respectively. Specifically,weassumetwokernels:
1 x⊤x 1
k (x ,x )=exp(− ∥x −x ∥2), k (x ,x )=( 1 2 +1)3exp(− ∥x −x ∥2),
1 1 2 2 1 2 2 1 2 100 2 1 2
wherek isastationaryGaussiankernelandk isanonstationarykernelobtainedbymultiplyinga
1 2
Gaussiankernelwithapolynomialkernel. WeusethesetwokernelstoconstructastationaryGPand
anonstationaryGP,respectively,andrandomlysampletwocorrespondinglatentfunctions. Based
onλ(x) = (f(x)+2)2,weconstructtwocorrespondingintensityfunctions. Finally,weusethe
thinningalgorithm[22]tosimulatetwosetsofsyntheticdata. Foreachintensity,wesimulateten
datasetsanduseeachdatasetalternatelyasthetrainingsetandtheremainingonesasthetestsets.
Setup For NSSPP, the number of frequencies (network width) is set to 50. For DNSSPP, we
experimentedwiththreedifferentconfigurations:DNSSPP-[50,30],DNSSPP-[100,50],andDNSSPP-
[30,50,30]. Each number represents the width of the corresponding network layer. Therefore,
DNSSPP-[50,30]impliesamodelwithtwolayers,wherethefirstlayerhasawidthof50andthe
secondlayerhasawidthof30,andsoon. Forbaselinemodels,weneedtosetthenumberofrankfor
kernellow-rankapproximation. Wesetthenumberofinducingpointsto50forVBPP,thenumberof
eigenvaluesto50forLBPP,andthenumberoffrequenciesto50forSSPP,GSSPP,andNSMPP.For
DSSPP,tofacilitatecomparisonwithDNSSPP,weadoptthesamelayerconfigurations.
Results ThefittingresultsofintensityfunctionsforallmodelsaredepictedinFig.1. Toquanti-
tativelycomparethefittingperformanceofdifferentmethodsinbothstationaryandnonstationary
7
)x(
detcepxE
)x(
detcepxE htped
1
2
3
4
etar
gninrael
1.0
50.0
10.0
500.0
100.0Table 1: The performance of L , RMSE and runtime for DNSSPP, DSSPP, NSSPP and other
test
baselinesontwosyntheticdatasets. ForL ,thehigherthebetter;forRMSEandruntime,thelower
test
thebetter. Theupperhalfcorrespondstononstationarymodels,whilethelowerhalftostationary
models. Onthestationarydataset,DSSPPperformscomparablytoDNSSPP.Onthenonstationary
dataset,DSSPPdoesnotoutperformDNSSPPduetotheseverenonstationarityinthedata.
StationarySyntheticData NonstationarySyntheticData
L test RMSE Runtime(s) L test RMSE Runtime(s)
DNSSPP-[100,50] 156.43(±37.52) 0.061(±0.020) 9.27 175.70(±26.89) 0.076(±0.015) 9.40
DNSSPP-[50,30] 156.55(±37.44) 0.061(±0.017) 9.00 173.68(±26.70) 0.094(±0.012) 9.11
DNSSPP-[30,50,30] 156.55(±37.66) 0.068(±0.017) 10.32 174.47(±26.57) 0.086(±0.013) 10.07
NSSPP 153.56(±36.82) 0.070(±0.009) 8.39 172.87(±26.86) 0.10(±0.012) 8.96
NSMPP 153.05(±36.96) 0.065(±0.018) 3.19 172.17(±26.42) 0.079(±0.019) 4.63
DSSPP-[100,50] 155.62(±37.31) 0.060(±0.015) 7.64 173.91(±10.27) 0.090(±0.013) 8.27
DSSPP-[50,30] 155.51(±36.98) 0.065(±0.012) 7.56 172.54(±26.74) 0.103(±0.015) 7.54
DSSPP-[30,50,30] 153.75(±37.34) 0.073(±0.016) 8.95 174.35(±26.70) 0.086(±0.013) 9.74
SSPP 153.91(±36.70) 0.071(±0.016) 1.95 165.94(±30.00) 0.140(±0.009) 2.48
GSSPP 154.12(±37.09) 0.073(±0.014) 7.72 170.13(±26.43) 0.101(±0.022) 14.19
GSSPP-M12 150.65(±36.84) 0.071(±0.018) 9.30 168.49(±26.67) 0.083(±0.018) 15.33
GSSPP-M52 154.46(±37.24) 0.075(±0.022) 9.63 169.33(±26.61) 0.107(±0.026) 14.49
LBPP 150.80(±36.13) 0.082(±0.008) 0.31 168.97(±26.70) 0.126(±0.006) 0.37
VBPP 155.29(±36.52) 0.072(±0.021) 1.68 172.95(±30.00) 0.087(±0.016) 1.83
70 70 2000 2000
60 60 1750 1750
50 50 1500 1500
40 40 1250 1250
1000 1000
30 30
750 750
20 20
500 500
10 10 250 250
0 0 0 0
LBPPforRedwoods DNSSPPforRedwoods LBPPforTaxi DNSSPPforTaxi
Figure2: ThefittingresultsoftheintensityfunctionsfromLBPPandDNSSPPontheRedwoodsand
Taxidatasets. AdditionalresultsforvariousbaselinesonthreedatasetsareprovidedinAppendixG.
scenarios,weemployL andRMSEasmetrics. ThecomparisonresultsarepresentedinTable1.
test
DNSSPPwiththreedifferentconfigurationsexhibitssignificantadvantagesonbothstationaryand
nonstationarydatasets. NSSPPandNSMPPshowbetterperformanceinthenonstationaryscenario
comparedtomoststationarybaselines. Thisisreasonable,asstationarybaselinesfacechallengesin
accuratelymodelingnonstationarydata. Becausestationarykernelsareasubsetofnonstationaryker-
nels,nonstationarymodelstypicallyperformwellonstationarydataaswell. Comparedtorelatively
shallowmodels,DNSSPPachievesbetterperformancebutalsoincursalongerrunningtime.
6.4 RealData
Datasets Inthissection,weusethreesetsofcommonreal-worlddatasetstoevaluatetheperfor-
manceofourmethodandthebaselines. CoalMiningDisaster[13]: Thisisa1-dimensionaldataset
containing191incidentsthatoccurredbetweenMarch15,1875,andMarch22,1962. Eachevent
representsaminingaccidentthatkilledmorethan10people.Redwoods[27]:Thisisa2-dimensional
datasetthatdescribesthedistributionofredwoodsinanarea,consistingof195events. PortoTaxi
[21]: This is a large 2-dimensional dataset containing the tracks of 7,000 taxis in Porto during
2013/2014. Wefocusontheareawithcoordinatesbetween(41.147,−8.58)and(41.18,−8.65),and
extract3,000pick-uppointsasourdataset. Foreachdataset,werandomlypartitionthedatainto
trainingsetandtestsetofapproximatelyequalsize.
Setup Forrealdatasets, becausewedonotknowtheground-truthintensity, weuseL asthe
test
evaluationmetric. SinceNSSPPgenerallyperformsworsethanDNSSPP,wereportonlytheresults
ofDNSSPPinthissection. ForDNSSPP,weconsistentlyusethesamethreeconfigurationsasin
thesyntheticdata. Forbaselinemodels,wechoosedifferentnumbersofrankforkernellow-rank
approximationfor1-dimensionaland2-dimensionaldatasets. Specifically,forthe1-dimensional
dataset(Coal),wesetthenumberofrank,i.e.,thenumberofinducingpointsforVBPP,thenumber
8Table2: TheperformanceofL andruntimeforDNSSPPandotherbaselinesonthreerealdatasets.
test
Theupperhalfcorrespondstononstationarymodels,whilethelowerhalftostationarymodels.
Coal Redwoods Taxi
L Runtime(s) L Runtime(s) L Runtime(s)
test test test
DNSSPP-[100,50] 225.73(±2.96) 10.54 79.56(±0.014) 11.09 7171(±73) 73.3
DNSSPP-[50,30] 225.85(±2.61) 11.38 79.50(±0.020) 10.45 6682(±37) 70.1
DNSSPP-[30,50,30] 225.79(±4.49) 11.76 79.55(±0.025) 12.72 7246(±37) 81.0
NSMPP 223.28(±3.60) 2.88 77.64(±6.21) 4.43 6492(±62) 94.6
SSPP 221.42(±1.87) 1.72 78.57(±2.83) 2.45 6245(±45) 58.4
GSSPP 221.08(±6.32) 5.05 76.97(±5.80) 5.94 6445(±97) 110.64
GSSPP-M12 223.11(±5.02) 5.61 72.96(±11.89) 5.90 6599(±76) 104.75
GSSPP-M52 221.89(±3.06) 5.17 77.98(±6.05) 5.23 6526(±113) 112.54
LBPP 218.30(±4.12) 0.33 80.40(±0.72) 0.34 6096(±25) 3.16
VBPP 219.15(±4.54) 1.69 77.06(±0.88) 2.14 6156(±34) 9.10
ofeigenvaluesforLBPP,andthenumberoffrequenciesforSSPP,GSSPP,andNSMPP,to10. For
the2-dimensionaldatasets(RedwoodsandTaxi),wesetthenumberofrankto50. Wediscussthe
reasonablenessoftheabovesetupinAppendixH.
Results Fig. 2 shows the fitting results of the intensity functions from LBPP and DNSSPP on
theRedwoodsandTaxidatasets. ThequantitativecomparisonresultsareshowninTable2. Since
real-worlddataismoreorlessnonstationarytovaryingdegrees, nonstationarymodelsgenerally
performbetteronreal-worlddata. DNSSPPsecuresbothfirstandsecondplacesontheCoalandTaxi
datasetsandachievessecondplaceontheRedwoodsdataset. Forsimplerdatasets,suchasCoaland
Redwoods,whichhaverelativelysimpledatapatterns,theperformanceofDNSSPPiscomparableto
thatofthesimplerbaselines. However,forthemorecomplexTaxidataset,DNSSPPdemonstrates
significantadvantagesoverthesimplerbaselines. ThisindicatesthatDNSSPPhasaclearadvantage
inhandlingdatawithmorecomplexpatterns,suchaslarge-scaleorhigh-dimensionaldatasets.
6.5 AblationStudies
Inthissection,weanalyzetheimpactoffourconfigurationhyperparametersontheperformanceof
DNSSPP:thenetworkwidthanddepth,aswellasthenumberofepochsandlearningrateduringthe
updateofhyperparameterΘ.
6.5.1 WidthandDepthofNetwork
WeinvestigateDNSSPP’sperformancewithvaryingnetworksizesbyadjustingwidthanddepthon
nonstationarysyntheticdata. Inourexperiments,wemaintainconsistentwidthacrosslayers. Results
areillustratedinFig.1c. Asthenumberoflayersandthewidthofthenetworkincrease,DNSSPP’s
performanceinitiallyimprovesandthendeclines,reflectingthephenomenonofoverfittingasthe
networksizeincreases. Regardingthenumberoflayers,DNSSPPgenerallyexhibitsunderfitting
whenthereisonlyasinglelayer,whileoverfittingtendstooccurwithfourlayers. Intermsofnetwork
width,duetotherelativelysmallsizeofthesyntheticdata,overfittingbecomesapparentwhenthe
widthislarge. However,forlargerdatasets,amorecomplexnetworkstructureshouldbechosen.
6.5.2 EpochandLearningRateofΘ
Duringtheinferenceprocess,weneedtoperformnumericaloptimizationonthehyperparameterΘ,
implyingtheneedtodeterminetheepochandlearningrateforoptimization. Weconductexperiments
onthenonstationarysyntheticdatatoinvestigatethecoordinationofepochandlearningrate. Results
areillustratedinFig.1d. Whenthelearningrateistooloworthenumberofepochsisinsufficient,
theupdateofΘissluggish,leadingtoinferiorperformance. However,whenthelearningrateistoo
highorthenumberofepochsisexcessive,modelperformancegenerallydeclinesaswell. Thisis
becauseourmodeltrainingisabi-leveloptimization: theinnerlevelistheLaplaceapproximation,
andtheouterlevelistoupdatethehyperparameterΘ. Anexcessivelyhighlearningrateortoomany
epochscancauseΘtoconvergetooquickly,makingtheoverallmodeltrainingmorepronetogetting
stuckinlocaloptima,thusresultinginsuboptimalperformance.
97 Limitations
NSSPPnotonlyremovesrestrictionsonthekernel’sfunctionalformbutalsomakesitapplicableto
nonstationarykernels. DNSSPPfurtherenhancesitsexpressivepowerthroughadeeparchitecture.
However,thedeeparchitectureofDNSSPPpreventsanalyticalcomputationoftheintensityintegral,
necessitatingnumericalintegration. Thisrelianceonnumericalintegrationimposesalimitationon
themodel’scomputationalspeed.
8 Conclusions
Inthisstudy,weintroducedNSSPPanditsdeepkernelvariant,DNSSPP,toaddresslimitationsin
modelingthepermanentalprocess. Thisapproachovercomesthestationaryassumption,allowing
forflexiblelearningofnonstationarykernelsdirectlyfromdatawithoutrestrictingtheirform. By
leveraging the sparse spectral representation of nonstationary kernels, we achieved a low-rank
approximation,effectivelyreducingcomputationalcomplexity. Ourexperimentsonsyntheticand
real-worlddatasetsdemonstratedthat(D)NSSPPachievedcompetitiveperformanceonstationary
datawhileexcellingonnonstationarydatasets. Thisstudyunderscorestheimportanceofconsidering
nonstationarityinthepermanentalprocessanddemonstratestheefficacyof(D)NSSPPinthiscontext.
AcknowledgmentsandDisclosureofFunding
ThisworkwassupportedbyNSFCProjects(Nos. 62106121,62406119),theMOEProjectofKey
ResearchInstituteofHumanitiesandSocialSciences(22JJD110001),thefundamentalresearchfunds
forthecentraluniversities,andtheresearchfundsofRenminUniversityofChina(24XNKJ13),the
NaturalScienceFoundationofHubeiProvince(2024AFB074),andtheGuangdongProvincialKey
LaboratoryofMathematicalFoundationsforArtificialIntelligence(2023B1212010001).
References
[1] Adams, R. P., Murray, I., and MacKay, D. J. Tractable nonparametric Bayesian inference
in Poisson processes with Gaussian process intensities. In Proceedings of the 26th Annual
InternationalConferenceonMachineLearning,pp.9–16.ACM,2009.
[2] Bochner,S. VorlesungenuberFourierscheIntegrale. AkademischeVerlagsgesellschaft,1932.
[3] Cox, D.R. Somestatisticalmethodsconnectedwithseriesofevents. JournaloftheRoyal
StatisticalSociety.SeriesB(Methodological),pp.129–164,1955.
[4] Dezfouli, A., Nock, R., Arabzadeh, E., and Dayan, P. Neural network Poisson models for
behaviouralandneuralspiketraindata. bioRxiv,2020.
[5] Diggle,P.,Rowlingson,B.,andSu,T.-l. Pointprocessmethodologyforon-linespatio-temporal
diseasesurveillance. Environmetrics: TheofficialjournaloftheInternationalEnvironmetrics
Society,16(5):423–434,2005.
[6] Donner,C.andOpper,M. EfficientBayesianinferenceofsigmoidalGaussianCoxprocesses.
JournalofMachineLearningResearch,19(1):2710–2743,2018.
[7] Flaxman,S.,Teh,Y.W.,Sejdinovic,D.,etal. Poissonintensityestimationwithreproducing
kernels. ElectronicJournalofStatistics,11(2):5081–5104,2017.
[8] Geng,J.,Shi,W.,andHu,G. BayesiannonparametricnonhomogeneousPoissonprocesswith
applicationstoUSGSearthquakedata. SpatialStatistics,41:100495,2021.
[9] Gunter,T.,Lloyd,C.,Osborne,M.A.,andRoberts,S.J. EfficientBayesiannonparametric
modellingofstructuredpointprocesses. InConferenceonUncertaintyinArtificialIntelligence,
pp.310–319,2014.
[10] Heinonen, M., Mannerström, H., Rousu, J., Kaski, S., and Lähdesmäki, H. Non-stationary
Gaussian process regression with Hamiltonian Monte Carlo. In Artificial Intelligence and
Statistics,pp.732–740.PMLR,2016.
10[11] Ilalan,D. APoissonprocesswithrandomintensityformodelingfinancialstability. TheSpanish
ReviewofFinancialEconomics,14(2):43–50,2016.
[12] Illian, J.B., Sørbye, S.H., andRue, H. Atoolboxforfittingcomplexspatialpointprocess
modelsusingintegratednestedLaplaceapproximation(INLA). 2012.
[13] Jarrett,R.G. Anoteontheintervalsbetweencoal-miningdisasters. Biometrika,66(1):191–193,
1979.
[14] John,S.andHensman,J. Large-scaleCoxprocessinferenceusingvariationalFourierfeatures.
InInternationalConferenceonMachineLearning,pp.2362–2370.PMLR,2018.
[15] Kingman,J.F.C. Poissonprocesses,volume3. ClarendonPress,1992.
[16] Ko,Y.-J.andSeeger,M.W. ExpectationpropagationforrectifiedlinearPoissonregression. In
AsianConferenceonMachineLearning,pp.253–268.PMLR,2016.
[17] Lázaro-Gredilla, M., Quinonero-Candela, J., Rasmussen, C. E., and Figueiras-Vidal, A. R.
SparsespectrumGaussianprocessregression. TheJournalofMachineLearningResearch,11:
1865–1881,2010.
[18] Lloyd,C.,Gunter,T.,Osborne,M.,andRoberts,S. VariationalinferenceforGaussianprocess
modulatedPoissonprocesses. InInternationalConferenceonMachineLearning,pp.1814–
1822,2015.
[19] McCullagh,P.andMøller,J. Thepermanentalprocess. Advancesinappliedprobability,38(4):
873–888,2006.
[20] Møller,J.,Syversveen,A.R.,andWaagepetersen,R.P. LogGaussianCoxprocesses. Scandi-
navianjournalofstatistics,25(3):451–482,1998.
[21] Moreira-Matias, L., Gama, J., Ferreira, M., Mendes-Moreira, J., andDamas, L. Predicting
taxi–passengerdemandusingstreamingdata. IEEETransactionsonIntelligentTransportation
Systems,14(3):1393–1402,2013.
[22] Ogata,Y. Space-timepoint-processmodelsforearthquakeoccurrences. AnnalsoftheInstitute
ofStatisticalMathematics,50(2):379–402,1998.
[23] Paciorek, C. J. and Schervish, M. J. Spatial modelling using a new class of nonstationary
covariancefunctions. Environmetrics: TheofficialjournaloftheInternationalEnvironmetrics
Society,17(5):483–506,2006.
[24] Park,M.,Weller,J.P.,Horwitz,G.D.,andPillow,J.W. Bayesianactivelearningofneuralfiring
ratemapswithtransformedGaussianprocesspriors. Neuralcomputation,26(8):1519–1541,
2014.
[25] Rahimi,A.andRecht,B. Randomfeaturesforlarge-scalekernelmachines. Advancesinneural
informationprocessingsystems,20,2007.
[26] Remes,S.,Heinonen,M.,andKaski,S. Non-stationaryspectralkernels. Advancesinneural
informationprocessingsystems,30,2017.
[27] Ripley, B.D. Modellingspatialpatterns. JournaloftheRoyalStatisticalSociety: SeriesB
(Methodological),39(2):172–192,1977.
[28] Samo,Y.-L.K.andRoberts,S. Generalizedspectralkernels. arXivpreprintarXiv:1506.02236,
2015.
[29] Sellier,J.andDellaportas,P. SparsespectralBayesianpermanentalprocesswithgeneralized
kernel. InInternationalConferenceonArtificialIntelligenceandStatistics,pp.2769–2791.
PMLR,2023.
[30] Snoek,J.,Swersky,K.,Zemel,R.,andAdams,R. InputwarpingforBayesianoptimization
ofnon-stationaryfunctions. InInternationalconferenceonmachinelearning,pp.1674–1682.
PMLR,2014.
11[31] Taddy,M.A. AutoregressivemixturemodelsfordynamicspatialPoissonprocesses:application
totrackingintensityofviolentcrime. JournaloftheAmericanStatisticalAssociation,105(492):
1403–1417,2010.
[32] Ton,J.-F.,Flaxman,S.,Sejdinovic,D.,andBhatt,S. SpatialmappingwithGaussianprocesses
andnonstationaryFourierfeatures. Spatialstatistics,28:59–78,2018.
[33] Walder,C.J.andBishop,A.N. FastBayesianintensityestimationforthepermanentalprocess.
InInternationalConferenceonMachineLearning,pp.3579–3588.PMLR,2017.
[34] Wilson,A.andAdams,R. Gaussianprocesskernelsforpatterndiscoveryandextrapolation. In
Internationalconferenceonmachinelearning,pp.1067–1075.PMLR,2013.
[35] Wilson,A.G.,Hu,Z.,Salakhutdinov,R.,andXing,E.P. Deepkernellearning. InArtificial
intelligenceandstatistics,pp.370–378.PMLR,2016.
[36] Xue, H., Wu, Z.-F., and Sun, W.-X. Deep spectral kernel learning. In International Joint
ConferenceonArtificialIntelligence,pp.4019–4025,2019.
[37] Yaglom,A.M. CorrelationTheoryofStationaryandRelatedRandomFunctions,VolumeI:
BasicResults,volume131. Springer,1987.
12A ProofofEquation(4)
ThederivationofEq.(4)isprovidedbelow:
σ2 (cid:88)R
k(x −x )≈ ϕ (x )ϕ (x )∗
1 2 R i 1 i 2
i=1
σ2 (cid:88)R
= exp(iω⊤x )exp(−iω⊤x )
R i 1 i 2
i=1
σ2 (cid:88)R
= (cos(ω⊤x )+isin(ω⊤x ))(cos(ω⊤x )−isin(ω⊤x ))
R i 1 i 1 i 2 i 2
i=1
σ2 (cid:88)R
= (cos(ω⊤x )cos(ω⊤x )+sin(ω⊤x )sin(ω⊤x ))
R i 1 i 2 i 1 i 2
i=1
=Φ(R)(x )⊤Φ(R)(x ). (18)
1 2
Inthederivation,duetothesymmetryofthespectrumdistribution,itcanbeassumedthat2Rpoints
aresymmetricallysampled,whichallowstheeliminationoftheimaginarypartintheequationwithout
alteringtheexpectation.
B ProofofEquation(8)
ThederivationofEq.(8)isprovidedbelow:
k(x ,x )≈
σ2 (cid:88)R
(cid:2) exp(cid:0) i(ω⊤x −ω⊤x )(cid:1) +...+exp(cid:0) i(ω⊤x −ω⊤x )(cid:1)(cid:3)
1 2 4R 1r 1 2r 2 2r 1 2r 2
r=1
=
σ2 (cid:88)R
(cid:2) cos(ω⊤x −ω⊤x )+...+cos(ω⊤x −ω⊤x )(cid:3) (19)
4R 1r 1 2r 2 2r 1 2r 2
r=1
σ2 (cid:88)R (cid:88)
= [cos(ω⊤x −ω⊤x )],
4R ir 1 jr 2
r=1i,j=1,2
wherethesecondlineisbecausethekernelisreal-valued,sowecansafelyeliminatetheimaginary
part. Additionally,consideringthelawoftotalexpectation,wehave
E [cos(ω⊤x +ω⊤x +2b)]=E [E [cos(ω⊤x +ω⊤x +2b)|ω ,ω ]], (20)
ωi,ωj i 1 j 2 ωi,ωj b i 1 j 2 i j
whereb∼Uniform(0,2π). Andconsidertheperiodicityofcosinefunction,wehave
E [cos(ω⊤x +ω⊤x +2b)|ω ,ω ]=0, (21)
b i 1 j 2 i j
soEq.(20)isfinallyequalto0. Therefore, wecanfurtherobtainanotherequivalentformofthe
spectralfeaturemapping,throughthefollowingprocedure:
σ2 (cid:88)R (cid:88)
k(x ,x )≈ [cos(ω⊤x −ω⊤x )+cos(ω⊤x +ω⊤x +b +b )]
1 2 4R ir 1 jr 2 ir 1 jr 2 ir jr
r=1(i,j)={1,2}2
σ2 (cid:88)R (cid:88)
= 2cos(ω⊤x +b )cos(ω⊤x +b ) (22)
4R ir 1 ir jr 1 jr
r=1(i,j)={1,2}2
=φ(R)(x )⊤φ(R)(x ),
1 2
where{b ,b }R areuniformlysampledfrom[0,2π].
1r 2r r=1
13C AnalyticalComputationofIntensityIntegral
Theintensityfunctionofthepermanentalprocesshasthefollowingexpressionλ(x)=(β⊤Ψ(R)(x)+
α)2. Foreaseofderivation,weadopttheformofEq.(7)forthespectralfeaturemappingΨ(R)(x)in
subsequentsteps,i.e.,a2R-sizedvector. WhenΨ(R)(x)takestheformofEq.(8),i.e.,anR-sized
vector,thecalculationresultoftheintensityintegralremainsthesame. TheintensityintegralonX is:
(cid:90) (cid:90)
λ(x)dx= (β⊤Ψ(R)(x)+α)2dx
X X
(cid:90) (cid:90) (cid:90)
=β⊤ Ψ(R)(x)Ψ(R)(x)⊤dxβ+2αβ⊤ Ψ(R)(x)dx+ α2dx
X X X
=β⊤Mβ+2αβ⊤m+α2|X|,
where
(cid:90) (cid:90)
M = Ψ(R)(x)Ψ(R)(x)dx, m = Ψ(R)(x)dx, i,j ∈1,...,2R. (23)
i,j i j i i
X X
ItisworthnotingthatMandmareanalyticallysolvable. Theanalyticalsolutionforthestation-
ary case is provided in Appendix C.2 in [29], and we provide the analytical expression for the
nonstationarycasehere.
ForM,theproductoftwospectralfeaturemappingsis:
 (cid:80) cos(ω⊤x)cos(ω⊤x), ifi,j =1,...,R,
σ2
(a,b)=
(cid:80){1,2}2
sin(ω
a⊤a ii
x)sin(ω
b⊤b jj
x), ifi,j =R+1,...,2R,
Ψ(R)(x)Ψ(R)(x)= (a,b)={1,2}2
i j 4R (cid:80) cos(ω⊤x)sin(ω⊤x), ifi=1,...,R, j =R+1,...,2R,
(a,b)=
(cid:80){1,2}2
sin(ω
a⊤a ii x)cos(ωb b⊤j
jx), ifi=R+1,...,2R, j =1,...,R.
(a,b)={1,2}2
(24)
Wecanfurthertransformtheproducttermoftrigonometricfunctions:
cos(ω⊤x)cos(ω⊤x)= 1 [cos(cid:0) (ω −ω )⊤x(cid:1) +cos(cid:0) (ω +ω )⊤x(cid:1) ],
ai bj 2 ai bj ai bj
sin(ω⊤x)sin(ω⊤x)= 1 [cos(cid:0) (ω −ω )⊤x(cid:1) −cos(cid:0) (ω +ω )⊤x(cid:1) ], (25)
ai bj 2 ai bj ai bj
cos(ω⊤x)sin(ω⊤x)= 1 [sin(cid:0) (ω −ω )⊤x(cid:1) +sin(cid:0) (ω +ω )⊤x(cid:1) ].
ai bj 2 ai bj ai bj
Withoutlossofgenerality,weconsidertheintegraldomainX as[−d,d]D. Thenwecancompute
the integral of the above expression analytically. Since we only consider the D = 1 and D = 2
pointprocessdatainthispaper,wespecificallydemonstratetheanalyticalintegralexpressionsinthe
one-dimensionalandtwo-dimensionalcaseshere.
One-dimension:
(cid:40)
(cid:90) (cid:0) (cid:1) 2 sin(ηd) ifη ̸=0
cos ηx dx= η (26)
2d ifη =0,
[−d,d]
(cid:90)
(cid:0) (cid:1)
sin ηx dx=0. (27)
[−d,d]
Two-dimension:
(cid:90) cos(cid:0) η⊤x(cid:1) dx=(cid:40) η12 η2(cid:0) cos((η 1−η 2)d)−cos((η 1+η 2)d)(cid:1) ifη 1,η 2 ̸=0
(28)
4d2 ifη =η =0,
[−d,d]2 1 2
(cid:90)
sin(cid:0) η⊤x(cid:1)
dx=0, (29)
[−d,d]2
14whereη denotesthefirstordinateofη,andη denotesthesecondordinateofη. Itshouldbenoted
1 2
thattheoretically,theintegralinEq.(28)onlyhastwopossibleresultsasexplainedintherightside.
TheanalyticalsolutionoftheintegralofEq.(25)canbeobtainedbyreplacingη withω −ω
ai bj
orω +ω . Notethatthecasea = bandi = j correspondstothesecondcaseinEq.(26)and
ai bj
Eq.(28),whichmeansthecomputationofthediagonalentriesofMisdifferentfromthatelsewhere.
Next,wecancomputem:
(cid:40)(cid:82) (cid:0) cos(ω⊤x)+cos(ω⊤x)(cid:1)
dx, ifi=1,...,R,
m i = (cid:82)[−d,d]D (cid:0) sin(ω⊤1i x)+sin(ω⊤2i x)(cid:1) dx, ifi=R+1,...,2R. (30)
[−d,d]D 1i 2i
ItiseasytoseethatifD =1,mcanbeanalyticallycomputedusingEq.(26)andEq.(27),while
ifD = 2,mcanbeanalyticallycomputedusingEq.(28)andEq.(29). Itisworthnotingthatthe
analyticalexpressionoftheintensityintegralaboveappliesonlytoNSSPP(single-layernetwork).
ForDNSSPP(multi-layernetwork),thenestedstructureoftrigonometricfunctionsleadstoMand
mlackinganalyticalsolutions,andweneedresorttonumericalintegration.
D DerivationofLaplaceApproximation
ThegradientandtheHessianmatrixoftheposteriorofβare:
∇ logp({x }N
,β|Θ)=−(2M+I)β−2αm+2(cid:88)N Ψ(R)(x i)
, (31)
β i i=1 β⊤Ψ(R)(x )+α
i
i=1
∇2 logp({x }N
,β|Θ)=−(2M+I)−2(cid:88)N Ψ(R)(x i)Ψ(R)(x i)⊤
. (32)
β i i=1 (β⊤Ψ(R)(x )+α)2
i
i=1
In theory, setting Eq. (31) to 0 can solve for the mode of the posterior, denoted as βˆ. However,
in practice, we cannot analytically solve it, so we use numerical optimization to obtain βˆ. Then
accordingtoEq.(32),wecanobtaintheprecisionmatrixQ−1 =−∇2 logp({x }N ,β|Θ)| .
β i i=1 β=βˆ
E Pseudocode
ThepseudocodeofourinferencealgorithmisprovidedinAlgorithm1.
Algorithm1:Inferencefor(D)NSSPP
DefinethenetworkdepthLandwidthR,andinitializethehyperparameterΘ;
forIterationdo
CalculateMandmbyEq.(13);
UpdatethemodeβˆbymaximizingEq.(14);
UpdatethecovariancematrixQbyEq.(15);
UpdatethehyperparametersΘbymaximizingEq.(16);
end
Foranyx∗,outputthepredictedmeanandvarianceofλ(x∗)byEq.(17).
F ComputationofExpectedTestLog-likelihood
The computation of expected test log-likelihood is provided in Appendix E.2 in [29]. For the
completenessofthearticle,werestatethespecificderivationprocesshere. Consideringtheposterior
oftheintensityfunction,wehavetheapproximationoftheexpectedtestlog-likelihood:
E [logp({x∗}N∗ |{x }N )]
β i i=1 i i=1
(cid:90) N∗
(cid:88)
≈−E [ (β⊤Ψ(R)(x)+α)2dx]+ E [log(β⊤Ψ(R)(x∗)+α)2], (33)
β β i
X i=1
15derivedfromEq.(1).
First,wecomputethefirstterminEq.(33):
(cid:90) (cid:90) (cid:90)
E [ (β⊤Ψ(R)(x)+α)2dx]= E [β⊤Ψ(R)(x)+α]2dx+ Var[β⊤Ψ(R)(x)]dx
β β
X X X
(cid:90) (cid:90) (cid:90)
= (Ψ(R)(x)⊤ββ⊤Ψ(R)(x))dx+2α (β⊤Ψ(R)(x))dx+α2|X|+ (Ψ(R)(x)⊤QΨ(R)(x))dx
X X X
=β⊤Mβ+tr(QM)+2αβ⊤m+α2|X|, (34)
whereMandmaredefinedasEq.(13).
Then,weconsiderthesecondterminEq.(33). Applyingthecomputationalmethodusedby[18],we
canobtainthefollowingexpression:
N∗
(cid:88)
E [log(β⊤Ψ(R)(x∗)+α)2] (35)
β i
i=1
β⊤Ψ(R)(x)+α 1
=−G˜(− )+log( Ψ(R)(x)⊤QΨ(R)(x))−Const, (36)
2Ψ(R)(x)⊤QΨ(R)(x) 2
whereConst≈0.57721566istheEuler-Mascheroniconstant. G˜ isdefinedas
G˜(z)=2z(cid:88)∞ j!zj
,
(2) (1/2)
j j
j=0
where(·) denotestherisingPochhammerseries:
j
(a) =1,(a) =a(a+1)(a+2)...(a+k+1).
0 k
Inpracticalcomputation,wefirstpreparealookuptablefortheG˜ functionandthenobtainnumerical
approximationsthroughlinearinterpolation.
G AdditionalExperimentalResults
Inthissection,weprovideadditionalexperimentalresultsfortherealdata. Specifically,wepresent
thefittingresultsoftheintensityfunctionsfromallmodelsontheCoaldatasetinFig.3;thefitting
resultsoftheintensityfunctionsfromVBPP,SSPP,GSSPP,andNSMPPontheRedwoodsdatasetin
Fig.4;andthefittingresultsoftheintensityfunctionsfromVBPP,SSPP,GSSPP,andNSMPPonthe
TaxidatasetinFig.5.
VBPP NSMPP
0.008
LBPP DNSSPP
SSPP Events
0.006 GSSPP
0.004
0.002
0.000
1860 1880 1900 1920 1940 1960
x
Figure3: ThefittingresultsoftheintensityfunctionsfromallmodelsontheCoaldataset.
H SomeDiscussionaboutExperimentalSetupforRealData
Fortherealdata,wechosedifferentnumbersofranksforthe1-dimensionaldataset(10forCoal)and
the2-dimensionaldatasets(50forRedwoodsandTaxi)forallbaselinemethods,exceptforDNSSPP.
ThereasonisthattheCoaldatasetisverysmall(only191events)andhasasimpledatapattern. A
16
)x(
detcepxE70 70 70 70
60 60 60 60
50 50 50 50
40 40 40 40
30 30 30 30
20 20 20 20
10 10 10 10
0 0 0 0
VBPPforRedwoods SSPPforRedwoods GSSPPforRedwoods NSMPPforRedwoods
Figure4: ThefittingresultsoftheintensityfunctionsfromVBPP,SSPP,GSSPPandNSMPPonthe
Redwoodsdataset.
2000 2000 2000 2000
1750 1750 1750 1750
1500 1500 1500 1500
1250 1250 1250 1250
1000 1000 1000 1000
750 750 750 750
500 500 500 500
250 250 250 250
0 0 0 0
VBPPforTaxi SSPPforTaxi GSSPPforTaxi NSMPPforTaxi
Figure5: ThefittingresultsoftheintensityfunctionsfromVBPP,SSPP,GSSPPandNSMPPonthe
Taxidataset.
highernumberofrankswouldnotfurtherimproveperformancebutwouldsignificantlyincreasethe
algorithm’srunningtime. Therefore,wesetthenumberofranksto10forCoal,but50forRedwoods
andTaxibecausethelattertwodatasetsarelargerandmorecomplex,requiringahighernumberof
rankstoimproveperformance.
Todemonstratethis,wefurtherincreasedthenumberofranksforallbaselinemodelsto50onthe
Coaldatasetandcomparedtheresulttorank=10. TheresultisshowninTable3. Ascanbeseenfrom
theresults,whenweincreasedthenumberofranksfrom10to50,theperformanceofallbaseline
modelsdidnotimprovesignificantly,butthealgorithm’srunningtimeincreasedsubstantially.
Insummary,forthesamedataset,thenumberofranksforallbaselinemethodsiskeptconsistent,
exceptforDNSSPPbecauseitisnotasingle-layerarchitecture. Thechoiceofthenumberofranks
forthebaselinemethodswasmadeconsideringthetrade-offbetweenperformanceandrunningtime.
Further increasing the number of ranks would not significantly improve model performance but
wouldresultinasubstantialincreaseincomputationtime.
Table3: TheperformanceofL andruntimeforallbaselinesontheCoaldataset,withthenumber
test
ofranksetto10and50. Whenweincreasethenumberofranksfrom10to50,theperformance
ofallbaselinemodelsdoesnotimprovesignificantly, butthealgorithm’srunningtimeincreases
substantially.
Coal(rank=10) Coal(rank=50)
L Runtime(s) L Runtime(s)
test test
NSMPP 223.28(±3.60) 2.88 222.92(±3.63) 4.74
SSPP 221.42(±1.87) 1.72 221.77(±2.92) 3.16
GSSPP 221.08(±6.32) 5.05 221.67(±4.28) 19.08
GSSPP-M12 223.11(±5.02) 5.61 219.33(±4.25) 18.88
GSSPP-M52 221.89(±3.06) 5.17 217.80(±5.25) 16.94
LBPP 218.30(±4.12) 0.33 219.88(±2.68) 2.68
VBPP 219.15(±4.54) 1.69 219.25(±4.51) 6.26
17