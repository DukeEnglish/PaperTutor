Minimax-optimal trust-aware multi-armed bandits
Changxiao Cai∗ Jiacheng Zhang†
October 7, 2024
Abstract
Multi-armedbandit(MAB)algorithmshaveachievedsignificantsuccessinsequentialdecision-making
applications, under the premise that humans perfectly implement the recommended policy. However,
existing methods often overlook thecrucial factor of human trust in learning algorithms. When trust is
lacking, humans may deviate from the recommended policy, leading to undesired learning performance.
Motivatedbythisgap,westudythetrust-awareMABproblembyintegratingadynamictrustmodelinto
thestandardMABframework. Specifically,itassumesthattherecommendedandactuallyimplemented
policydiffersdependingonhumantrust,whichinturnevolveswiththequalityoftherecommendedpolicy.
We establish the minimax regret in the presence of the trust issue and demonstrate the suboptimality
of vanilla MAB algorithms such as the upper confidence bound (UCB) algorithm. To overcome this
limitation, we introduce a novel two-stage trust-aware procedure that provably attains near-optimal
statisticalguarantees. Asimulationstudyisconductedtoillustratethebenefitsofourproposedalgorithm
when dealing with thetrust issue.
Keywords: multi-armed bandit, trust-aware decision making, regret bound, minimax optimality
Contents
1 Introduction 2
1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Problem formulation 4
3 Main results 5
3.1 Sub-optimality of UCB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Trust-aware UCB algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Minimax optimal regret . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Analysis 8
4.1 Regret lower bound for the UCB algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 Regret upper bound for our proposed algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5 Numerical experiments 10
6 Discussion 11
A Proof of Theorem 1 14
B Proof of Theorem 2 21
∗Department ofIndustrial andOperations Engineering, UniversityofMichigan,cxcai@umich.edu.
†Department ofStatistics,TheChineseUniversityofHongKong,jiachengzhang@cuhk.edu.hk.
1
4202
tcO
4
]LM.tats[
1v15630.0142:viXraC Proof of Theorem 3 29
1 Introduction
Themulti-armedbandit(MAB)algorithmshaveachievedremarkablesuccessacrossdiversedomainsrelated
to sequential decision-making, including healthcare intervention (Tewari and Murphy, 2017; Rabbi et al.,
2018),recommendationsystems(Li et al.,2010;Kallus and Udell,2020),anddynamicpricing(Kleinberg and Leighton,
2003; Wang et al., 2021), to name a few. While a variety of prior work has been dedicated to this problem,
existing results often emphasize learning uncertain environments and overlook humans’ willingness to trust
the output of learning algorithms, assuming humans implement the recommended policy with perfect preci-
sion. However,innumerousreal-worldapplications,theremayexistadiscrepancybetweentherecommended
and actually executed policy, which heavily relies on humans’ trust in the recommendations. The humans
would embrace the suggested policy only if they have a high level of trust; otherwise, they tend to adhere
to their ownpolicy. Meanwhile,high-qualityrecommendations,inturn, canfoster growingtrustthroughout
the decision-making process. One concrete example can be found in human-robot interaction (HRI), where
autonomous systems such as robots are employed to assist humans in performing tasks (Chen et al., 2020).
Although the robot is fully capable of completing tasks, a novice user may not trust the robot and refuse
its suggestion, leading to inefficient collaboration. Additionally, trust-aware decision-making finds crucial
applicationsinemergencyevacuations(Robinette and Howard,2012). Myopicindividualsmightfollowtheir
own policy, such as the shortest path strategy, which is not necessarily optimal due to traffic congestion. If
the evacuationplandesignerfailstoaccountforthe uncertaintyinactionexecutioncausedbytrustissues,a
skeptical individual is unlikely to follow recommendations, potentially resulting in disastrous consequences.
This motivates research on trust-aware sequential decision-making, where deviations in decision imple-
mentation arising from trust issues need to be taken into account. Human trust should be monitored and
influenced during the decision-making process in order to achieve optimal performance. Despite the foun-
dational importance, studies of trust-aware sequential decision-making remain largely unexplored. Only
recently have researchersbegun to empirically incorporate trust into algorithm design (Moyano et al., 2014;
Azevedo-Sa et al., 2020; Xu and Dudek, 2016; Chen et al., 2018; Akash et al., 2019). However,the theoreti-
cal understanding of trust-aware sequential decision-making still remains an open area of research.
In this paper, we take an initial step by formulating the problemof trust-awareMAB composedof three
key components: (i) uncertain environments and unknown human trust that can be learned and influenced,
(ii) a policy-maker that designs MAB algorithms for decision recommendation, and (iii) an implementer
who may or may not execute the recommended action based on trust, which is unobservable to the policy-
maker. The sequential decision-making process can be described as follows. At each time h, (1) the policy-
maker selects an arm apm according to some policy πpm and recommends it to the implementer that takes
h h
actions in reality; (2) the implementer pulls an arm aac according to some policy πac, which depends on
h h
the recommended arm apm, her own policy πown, and her trust t in the policy-maker; (3) the implementer
h h h
receivesa randomrewardR (aac) associatedwith the pulled arm aac. The goalis to designa policy πpm for
h h h
the policy-maker to maximize the expected cumulative rewards received, that is, E[ H R (aac)].
h=1 h h
One critical issue surrounding the trust-aware MAB lies in decision implementation deviation. The
P
policy-maker loses full control of exploration and exploitation due to the trust issue, which significantly
hurts the balanced trade-off attained by the standard MAB methods. When the implementer’s trust level
is low, she will not explore the uncertain environment effectively as desired by the recommended policy
πpm, while at the same time failing to fully exploit the optimal arm even after it has been identified. In
particular, when her own policy πown outperforms most arms, ignoring deviations in decision execution and
blindlyapplyingvanillaMABalgorithms,suchastheupperconfidencebound(UCB)algorithm,canseverely
decrease performance. Indeed, when πown performs relatively well, the suboptimal arms are rarely selected
bytheimplementer. TheUCB-typealgorithm,however,attempts tokeeppullingthosesuboptimalarmsfor
explorationpurposes,causingapersistentdeclineintrust. Thisfurtherexacerbatesdecisionimplementation
deviation and hinders effective exploration, resulting in a large regret.
In light of such challenges, a fundamental question we seek to address in this paper is:
Can we design a trust-aware MAB algorithm that achieves (near-)minimax optimal statistical guarantees in
the presence of trust issues?
21.1 Contributions
Encouragingly,we deliver an affirmative answer to the question above. We integrate a dynamic trust model
inthestandardMABframeworkthatcharacterizessequentialdecision-makingunderuncertainenvironments
with unknown human trust behavior. We establish that the minimax lower bound on the H-period regret
scales as Ω(√KH), where K is the size of the arm set. Furthermore, we show that the standard UCB
algorithm (Lai et al., 1985; Auer et al., 2002a) can incur a near-linear regret Ω(H) when the trust issue is
present. Here and throughout,we use the asymptotic notations O() (resp. O()) to representupper bounds
· ·
onthegrowthrateuptoaconstant(resp.logarithmic)factor,andΩ()(resp.Ω(e))forlowerboundssimilarly.
· ·
Toaddressthisgap,wedesignanoveltrust-awareUCBalgorithmconsisteingoftwostages. Theapproach
first identifies the arms that ensure the implementer’s trust in recommendaetions grows. It then conducts
trust-aware exploration and exploitation, simultaneously optimizing decisions and building trust. Notably,
our procedure operates in an adaptive manner without requiring any knowledge of the implementer’s trust
level, trust set, or own policy. We characterize the regret of our algorithm as O(√KH), which provably
achieves the minimax regret up to a logarithmic factor for a wide range of horizon H.
To the best of our knowledge, this is the first theory to develop a trust-awaree algorithm that provably
achieves near-minimax optimal statistical efficiency. The major technical contribution lies in the delicate
characterizationof the interplay between the trust dynamics and decision-making.
1.2 Related work
Multi-armed bandits. Since the seminal work (Robbins, 1952), the MAB problem has been extensively
studied, which is well-understood to a large extent. We refer readers to Bubeck et al. (2012); Slivkins et al.
(2019);Lattimore and Szepesvári(2020)foracomprehensiveoverview. Numerousalgorithms—includingthe
UCB(Lai et al.,1985;Auer et al.,2002a),successiveelimination(SE)(Even-Dar et al.,2006;Auer and Ortner,
2010), and Thompson sampling (TS) (Thompson, 1933; Agrawal and Goyal, 2012)—have been developed
that provably attain the minimax regret (Auer et al., 2002b; Gerchinovitz and Lattimore, 2016). Our trust-
aware MAB formulation shares the same spirit as numerous variants of the MAB problem that are mo-
tivated by diverse practical constraints in real-world applications. A non-comprehensive list of exam-
ples includes MAB with safety constraints where actions must satisfy uncertain and cumulative or round-
wise constraints (Amani et al., 2019; Pacchiano et al., 2021; Badanidiyuru et al., 2018; Liu et al., 2021),
transfer/meta-learning for MAB where datasets collected from similar bandits are leveraged to improve
learning performances (Cai et al., 2024; Lazaric et al., 2013; Cella et al., 2020; Kveton et al., 2021), risk-
averse MAB where the objective is to minimize risk measures such as Conditional Value-at-Risk (CVaR)
and Value-at-Risk (VaR) (Sani et al., 2012; Cassel et al., 2018; Wang et al., 2023), etc. It is worth noting
that deviations in decision implementations have been explored in the context of MABs. For instance, the
incentive-compatibleMAB(Frazier et al.,2014;Mansour et al.,2015)studiesthescenarioswheredeviations
arisefromthenatureofrecommendations—exploitativearmsaremorelikelytobefollowedthanexploratory
ones. In contrast, our framework focuses on deviations driven by trust issues.
Trust-aware decision-making. Trust-aware decision-making is largely driven by human-robot interac-
tion, where trust between humans and autonomous systems presents as a fundamental factor in realiz-
ing their full potential (Azevedo-Sa et al., 2020; Xu and Dudek, 2016; Chen et al., 2018; Akash et al., 2019;
Bhat et al., 2022). Researchontrust-awaredecision-makingcan be broadlydivided into reactiveand proac-
tive approaches. The reactive approach employs a predetermined scheme that specifies the policy-maker’s
behaviors when the human’s trust falls outside an optimal range (Azevedo-Sa et al., 2020; Xu and Dudek,
2016). The formulationconsidered in the currentpaper belongs to the proactive approach,which integrates
humantrustintothedesignofdecision-makingalgorithms. InadditiontoMABs,Markovdecisionprocesses
(MDPs) and Partially Observable Markov decision processes (POMDPs) are also widely used in modeling
thetrust-awaredecision-makingprocess(Chen et al.,2018,2020;Bhat et al.,2022;Akash et al.,2019). This
proactive approach allows the learner to adapt its recommendations in response to human trust. However,
these studies are predominantly empirical and lack a theoretical foundation.
31.3 Notation
For any a,b R, we define a b := max a,b and a b := min a,b . For any finite set , we let ∆( ) be
the
probabili∈
ty simplex over
∨
. For
any{ pos}
itive
int∧
eger K,
de{ note}
by [K] := 1,2,
,A
K . We
useA1
A { ··· } {·}
to representthe indicator function. For any distributions P,Q, KL(P Q) stands for the KL-divergence. For
anya R, denote by a (resp. a )the largest(resp.smallset)integek rthatis nolarger(resp.smaller)than
∈ ⌊ ⌋ ⌈ ⌉
a. For any two functions f(n),g(n) > 0, the notation f(n) . g(n) (resp. f(n) & g(n)) means that there
exists a constant C > 0 such that f(n) Cg(n) (resp. f(n) Cg(n)). The notation f(n) g(n) means
≤ ≥ ≍
that C f(n) g(n) C f(n) holds for some constants C ,C >0. In addition, f(n)=o(g(n)) means that
0 1 0 1
≤ ≤
limsup f(n)/g(n) = 0, f(n) g(n) means that f(n) c g(n) for some small constant c > 0, and
n→∞ ≪ ≤ 0 0
f(n) g(n) means that f(n) c g(n) for some large constant c >0.
1 1
≫ ≥
1.4 Organization
The rest of the paper is organized as follows. Section 2 formulates the problem and introduces definitions
and assumptions. Section 3 presents our theoretical findings and the analysis of main theories is presented
in Section 4. The detailed proofs and technical lemmas are deferred to the appendix. Section 5 presents the
numerical performance of the proposed algorithms. We conclude with a discussion of future directions in
Section 6.
2 Problem formulation
Trust-aware multi-armed bandit. We study a (stochastic) K-armed bandit, described by a sequence
of i.i.d. random variables R (1), ,R (K) , where R (k) denotes the random reward generated by
h ··· h h≥1 h
armk at time h. The rewardsare assumed to be bounded in [0,1] for any k [K] and h 1. The expected
reward associated with arm(cid:0) k is denoted by r(cid:1) (k) := E[R (k)] for any k [K∈ ]. We denote≥ r⋆ the maximum
1
∈
expected reward, i.e., r⋆ := max r(k). A policy π = π is a collection of distributions over the
k∈[K] h h≥1
{ }
arm set, where π ∆([K]) specifies the (possibly randomized) arm selection at time h.
h
The MAB gam∈ e operates as follows: at each time step h, the policy-maker recommends an arm apm to
h
the implementer. Based on apm, the implementer pulls arm aac according to some policy πac (which may
h h h
differ from apm) and receives a random reward R (aac) yielded by the arm aac.
h h h h
Decisionimplementationdeviation. Tocharacterizedeviationsindecisionimplementation,wefocuson
the disuse trust behaviormodel, whichis widely recognizedby empiricalstudies in human-robotinteraction
scenarios (Chen et al., 2020; Bhat et al., 2024).
Specifically, the implementer is assumed to have an own policy πown, which is unknown to the policy-
maker. Given the recommended action apm, the implementer chooses to either follow the instruction apm
h h
or take action according to her own policy πown, based on her trust level t at time h. More concretely, we
h h
assume that
apm, if χ =1;
aac = h h (1)
h (ao hwn, if χ
h
=0.
Here, χ
ind.
Bern(t ),h 1 is a sequence of independent Bernoulli random variables, where t represents
h h h
∼ ≥
the implementer’s trust level in the policy-maker at time h. Without loss of generality, the trust level t is
h
assumedtobeboundedin[0,1]. Whenχ =1,theimplementeradoptstherecommendationapm. Otherwise,
h h
shechoosesanarmaown accordingtoherownpolicyπown. Inotherwords,attimeh,theimplementeradopts
h h
the recommendation apm with probability t , while reverting to her own policy πown with probability 1 t .
h h h − h
Intuitively, a higher trust level indicates a greater tendency for the implementer to follow the policy-maker.
Trust update mechanism. We incorporate a dynamic trust model to capture how recommendations
influence the implementer’s evolving trust. The trust level t at time h is assumed to satisfy
h
α
h
t = , (2)
h
α +β
h h
4where α =β =1, and
1 1
α =α +1 apm , and β =β +1 apm / , h 2. (3)
h h−1 { h−1 ∈T} h h−1 { h−1 ∈T} ∀ ≥
Here, the set [K] consists of arms that, when recommended, increase the implementer’s trust in the
T ⊂
policy-maker. Roughly speaking, the implementer’s trust level is the frequency at which the recommended
arm belongs to her trust set. The trust increases whenever an arm from the trust set is recommended,
T
resulting in a higher likelihood of following it in the future.
Remark 1. The trust update rule is motivated by the pattern observed in real-world human-subject ex-
periments (Guo and Yang, 2021) and aligns with the well-known Laplace’s rule of succession in probability
theory. We note that the initial values α = β = 1 are chosen for simplicity of presentation. Our results
1 1
naturally extend to general cases with arbitrary constants α ,β >0.
1 1
We emphasize that the trust set is chosen by the implementer and is inaccessible to the policy-maker.
T
This informationasymmetryis an inherentfactor driving the challengesin such a hierarchicalstructure. To
ensure effective learning when the policy-maker and implementer share aligned interests—both seeking to
maximize rewards—we introduce a consistency assumption regarding the trust set , as described below.
T
Assumption 1. The trust set includes at least an optimal arm, that is, k such that r(k)=r⋆.
T ∃ ∈T
Assumption 1 requires that when a unique optimal arm exists, it belongs to the trust set. In cases of
multiple optimal arms yielding the highest expected rewards, the trust set contains at least one such arm.
All in all, it guarantees that the implementer’s trust increases when an optimal arm is recommended, and
hence building the implementer’s trust aligns with learning the optimal arm.
Remark 2. The trust set can be viewed as representing the implementer’s a priori beliefs or preferences
T
regardingthe arms. For instance,it may consistofarms that the implementer initially considerspotentially
optimal based on her limited knowledge. When the policy-maker suggests pulling an arm outside , it
T
challengestheimplementer’spreconceptionsandundermineshertrustinthecredibilityofrecommendations.
We denote by Π(K, ,πown) the class of trust-aware MABs that satisfy the conditions above.
T
Goal. Our objective is to develop a policy πpm for the policy-maker that minimizes the expected regret
accumulated over H time steps by the implementer policy πac, namely,
H
E[reg (πpm)]:=E r⋆ r(aac) aac πac .
H − h h ∼ h
" #
h X=1
(cid:0)
(cid:1)(cid:12)
(cid:12)
The expectation is taken with respect to the randomness of the ex(cid:12)ecuted policy πac, which depends on the
recommendedpolicy πpm andtrustlevel t . We emphasize thatthe policy πpm attime h depends only
{ h }h≥1 h
onobservationsanteriortoh,specifically aac,R (aac),χ . Moreover,theownpolicyπown andtrust
i i i i 1≤i<h
t is unobservable to the policy-maker.
{ i }1≤i<h (cid:8)(cid:0) (cid:1)(cid:9)
3 Main results
3.1 Sub-optimality of UCB
OnemaynaturallywonderwhetherwecanresorttotheclassicalMABalgorithmssuchastheUCBalgorithm
(Lai et al., 1985; Auer et al., 2002a) to solve the trust-aware MAB problem. Unfortunately, the answer is
negative,whichisformalizedbythefollowingregretlowerboundfortheUCBalgorithminTheorem1below.
The proof can be found in Appendix A. For completeness of presentation,we present the UCB algorithmin
Algorithm 2 in the appendix.
Theorem 1. There exists a multi-armed bandit, a trust set, and an own policy such that the expected regret
of the policy πUCB generated by the UCB algorithm obeys
cH
E[reg (πUCB)] , (4)
H ≥ log(H)
where c is some constant independent of H. p
5Remark 3. This theorem constructs a hard MAB instance with a fixed number of arms, emphasizing the
suboptimality of horizondependency H. It is straightforwardto generalizeit to encompass a broaderrange
of cases.
Theorem1revealsthesuboptimality ofthe UCBalgorithminthe presenceofthe trustissue. Todevelop
some intuition about its failure, note that the arm prescribed at time h is only selected with probability t
h
due to deviations in decision implementation and that recommending an arm not in the trust set reduces
the implementer’s trust. As shall be clear from the analysis in Section 4, without accounting for the trust
issue, the UCB algorithmadopts a relatively “aggressive” explorationstrategy,causing the suboptimal arms
to dominate the recommendations in the initial stage. As a consequence, the trust level t delays rapidly
h
to nearly zero within o(H) time steps. This implies that the UCB algorithm effectively loses control of the
decision-making in practice, with the implementer adhering to her own policy πown for the remainder of the
game. Therefore, the UCB algorithm incurs a near-linear regret Ω(H) as long as πown is not the optimal
policy.
e
3.2 Trust-aware UCB algorithm
Revisiting the failure of the UCB approach highlights an important lesson for trust-aware policy design:
maintainingahighleveloftrustisessentialwhenexploringthesuboptimalarms. Inlightofthisobservation,
we proposedatwo-stagetrust-awareUCB paradigmthat leveragesthe informationcontainedinthe pattern
of decision implementation deviations. The first phase involves uniform arm selection by the policy-maker,
aiming to identify the implementer’s trust set and eliminate the arms outside this set. This procedure
guaranteesthattherecommendedpolicywillbefollowedsubsequently. Oncethiseliminationstepiscomplete,
the algorithmtransitions to the second stage, where the policy-maker conducts trust-awareexplorationand
exploitation to identify the optimal arm. All in all, our strategy maintains the implementer’s trust while
effectively distinguishing the best arm.
We summarize our trust-aware method in Algorithm 1 and elaborate on its two stages.
Stage 1: trust set identification. As a preliminary stage for trust-aware exploration and exploitation,
we aim to identify arms that do not belong to the implementer’s trust set. The rationale behind this step
is grounded in the trust update mechanism described in (2) and (3). Eliminating these arms allows the
policy-maker to explore safely without losing the implementer’s trust in Stage 2.
Since the trust level is not observable, we estimate it by counting the frequency with which each recom-
mended arm is followed. Specifically, this phase runs for K rounds and maintains an estimated trust set
T
at the end of eachround, where the round length m K3log(H) is chosento ensure accurateidentification
≍
while controlling cumulative regret. In round k, Algorithm 1 selects the k-th arm 2m times and recordbs
whether the recommendations are followed. For each k [K], we define
∈
m 2m
1 1
Y(1) := χ and Y(2) := χ . (7)
k m 2m(k−1)+i k m 2m(k−1)+i
i=1 i=m+1
X X
Moreover,we define the comparisonthreshold at round k for each k >1 as
1/(5k), if =0;
|T|
λ ( )= 1/(5k), if =k 1; (8)
k 
T − |T| −
0, othb erwise.
b b
For k = 1, we compare Y(1) +Y(2) with
1/2
to determine if arm 1 belongs to the implementer’s trust set
k k
, as the sum approaches 1 if 1 and 0 otherwise with high probability. For k > 1, we use Y(2) Y(1)
T ∈ T k − k
to determine whether the k-th arm belongs to the trust set . This difference represents the discrepancy
T
between the implementer’s policy compliance frequency in the first and subsequent m trials. It is not hard
to see that the expectation of the difference Y(2) Y(1) exceeds λ when k and is smaller otherwise.
k − k k ∈ T
Moreover,ourchoiceofroundlength mensuresthatthe observeddifferencealignswithitsexpectationwith
high probability.
6Algorithm 1 Trust-aware UCB
1: Input: arm set [K], time horizon H.
2: Initialize arm set ∅, m 30K3log(H), and H 0 2mK.
T ← ← ←
3: for h=1,...,H 0 do
4: Choose apm kb, where k h/(2m) .
h ← ←⌈ ⌉
5: Observe aa hc, R h(aa hc), and χ h.
6: if h 0 mod 2m then
7: Co≡ mpute Y(1) and Y(2) as in (7).
k k
8: if h=2m then
9: if Y(1)+Y(2) 1/2 then
1 1 ≥
10: Update 1 .
T ←T ∪{ }
11: else
12: if Y k(2) −Yb k(1) ≥bλ k( T) (cf. (8)) then
13: Update k .
T ←T ∪{ }
14: Set N Hac 0+1(a) ←1 and UCBb H0+1(a) ←1 for any a ∈[K].
15: for h=H 0+1,..b.,H bdo
16: Choose ap hm ←argmax a∈TbUCB h(a) with ties broken uniformly at random.
17: Observe aa hc, R h(aa hc), and χ h.
18: Update Nac (aac), where
h+1 h
s−1
Nac(a):=1 1 aac =a , a [K],s>H +1. (5)
s ∨ { i } ∀ ∈ 0
i= XH0+1
19: Update UCB h+1(aa hc), where
s−1
1 log(H)
UCB (a):=1 R (a)1 aac =a +2 , a [K],s>H +1. (6)
s ∧(N sac(a)
i= XH0+1
i { i } sN sac(a) ) ∀ ∈ 0
20: Output: policy apm .
h h≥1
(cid:8) (cid:9)
Stage 2: trust-aware exploration-exploitation. Equipped with our estimate of the trust set , the
T
remainderof ouralgorithmbuilds on the optimistic principle to distinguishthe optimalarm. Thanksto the
elimination procedure in Stage 1, the trust level t is guaranteed to keep increasing in the second stbage at
h
a rate of 1 t =O(1/h). This gradual increase ensures the implementer’s trust remains high, allowing for
h
−
effective exploration and exploitation, as will be demonstrated in the analysis in Section 4.
3.3 Minimax optimal regret
WeproceedtopresentthetheoreticalguaranteesofAlgorithm1inTheorem2below. Theproofispostponed
to Appendix B.
Theorem 2. For any K 2, the expected regret of the policy π generated by Algorithm 1 satisfies
≥
sup E[reg (π)] C KHlog(H)+C K4log2(H), (9)
H ≤ 1 2
Π(K,T,πown)
p
for some positive constants C and C independent of H and K.
1 2
The regretupper bound of our proposed algorithm contains two terms. The first term O( KHlog(H))
represents the regret accumulated in the trust-aware exploration and exploitation phase, while the second
p
term O(K4log2(H)) accounts for the trust set identification stage.
Inaddition,Theorem3belowestablishestheminimaxlowerboundontheregretofthetrust-awareMAB
problem. The proof can be found in Appendix C.
7Theorem 3. For any K 2, one has
≥
inf sup E[reg (π)] c√KH, (10)
π Π(K,T,πown) H ≥
for some positive constant c that is independent of H and K.
Here,the infimumistakenoverthe classofadmissiblepoliciesobeyingthepolicyattime hdepends only
on observations prior to time h, that is, aac,R (aac),χ .
i i i i 1≤i<h
We provide several important implications as follows.
(cid:0) (cid:1)
(1) Near-minimax optimal regret. When the time horizon H is sufficiently large (H & K7log3(H)), the
secondtermbecomes negligible. Asaresult,the regretupperbound(9)inTheorem2matchesthe minimax
lower bound (10) in Theorem 3 up to a logarithmic term. This illustrates that near-optimal statistical
efficiency can be attained despite the presence of trust issues. Also, in the classical setting where the
recommended policy is executed exactly, the minimax lower bound on regret scales Ω(√KH) (Auer et al.,
2002b;Gerchinovitz and Lattimore,2016). Therefore,thetrustissuedoesnotincreasethecomplexityofthe
problem.
(2) Adaptivity to unknown trust and own policy. Our algorithm does not require any prior knowledge of
theimplementer’strustlevelt,trustset ,orownpolicyπown. Thisadaptivityensuresitsbroadapplicability
T
across diverse practical scenarios.
(3) Cost of decision implementation deviation. The term O(K4) in the regret upper bound (9) can be
seen as a burn-in cost incurred by deviations in arm selection. Before the policy-maker establishes enough
trust, such deviations lead to extra regret if we do not imposeeany assumption on the own policy πown.
Remark 4. While optimizing the dependency of burn-in cost on K seems plausible, it remains unclear
whether this term is an inherent consequence of the trust issue or an artifact of the algorithm and proof.
Therefore,we did notprioritize further optimizationatthis stage andfocus onachieving the optimaldepen-
dence on H. We leave the task of addressing this burn-in cost and achieving the optimal dependence on K
to future work.
Remark 5. We remark that the logarithmic term in O( KHlog(H)) can be removed by modifying the
bonus term. However, given that this work’s primary focus is to demonstrate how to overcome trust issues,
p
we chose not to devote significant effort to optimizing this dependency.
4 Analysis
A criticaldifference in analyzingthe trust-awareMAB comparedto the standardMAB lies in capturing the
trust behavior t . For any policy πpm, the expected regret can be decomposed as
h h≥1
{ }
H H
E[reg (πpm)]=E t r⋆ r(apm) +E (1 t )(r⋆ rown , (11)
H " h − h # " − h − h #
h=1 h=1
X (cid:0) (cid:1) X (cid:1)
=:regp Hm =:rego Hwn
wherer hown :=E a∼πhown[R h(a)]. Control|lingregp Hm{zismoreo}rlesssta|ndardwh{ezreonecan}applystandardUCB-
type arguments (see e.g. Lattimore and Szepesvári (2020)). On the other hand, controlling regown requires
H
more delicate effort due to the trust factor. In the classical MAB problem without the trust issue, t = 1
h
for any h 1 and thus regown = 0. However, when the trust factor is incorporated, the trust level and
≥ H
armselectionareintertwined: the trustlevelinfluences the likelihoodofthe recommendedarmbeing pulled,
which in turn affects the trust level in the subsequent time step. Therefore,our main technical contribution
towards developing regret bounds lies in pinning down the interaction among the trust dynamics t ,
h h≥1
the recommended arms apm , and the selected arms aac . { }
{ h }h≥1 { h}h≥1
4.1 Regret lower bound for the UCB algorithm
We present the proof outline of Theorem 1 in this section. For the MAB instance, we set K = 260. The
expected rewards are chosen to be r(k) = 1/6 log(H)+1 if k < K and r(k) = 1/3 log(H)+1 if k = K.
p p
8The implementation’s trust set is set as = K and the own policy πown is chosen to be a uniform
T { }
distribution over K 1,K , i.e., πown(K 1)=πown(K)=1/2 for all h 1. For convenience of notation,
{ − } h − h ≥
we define ∆own :=r⋆ rown =1/(12 log(H)+1).
Recall thh e decomp− osh ition in (11). Since r⋆ r(apm) for any h 1, we can lower bound the regret as
p ≥ h ≥
follows:
H H
1
reg regown = ∆own(1 t )= (1 t ). (12)
H ≥ H h − h 12 log(H)+1 − h
h=1 h=1
X X
p
The key idea of the proof is to show that with high probability, the sum of the trust levels up to the final
time step H t isboundedby O log2(H) . Intuitively, the time horizoncanbe dividedinto threestages.
h=1 h
1. DuriPng the first stage 1 h (cid:0) 256log((cid:1)H) , the trust level is simply upper bound by 1 and hence the
≤ ≤⌊ ⌋
sum is bounded by h.
2. In the second stage 256log(H) < h 1024log(H) , as the suboptimal arms are insufficiently
⌊ ⌋ ≤ ⌊ ⌋
explored, their UCB estimates can be as high as that of the optimal arm. Consequently, the UCB
algorithmrecommends these suboptimal arms with at least a constant probability, causing a constant
upper bound for the trust level (1/3 in our analysis). Therefore, the sum of trust levels in this stage
scales O(h/3).
3. In the third stage 1024log(H) < h H, due to the frequent recommendations of suboptimal arms
⌊ ⌋ ≤
in the previous two stages, the trust level keeps decaying at a rate of O(1/h). Hence, the sum of trust
levelsinthis stepscalesO(log(h)). Combiningthe trustaccumulatedinthese threestagesleadsto the
expression of s in the analysis.
h
e
More specifically, let us define s as follows:
h
h, if 1 h 256log(H) ;
≤ ≤⌊ ⌋
s :=1h+ 2 256log(H) , if 256log(H) <h 1024log(H) ; (13)
h 3 3⌊ ⌋ ⌊ ⌋ ≤⌊ ⌋
1
1024log(H) 1+log h , if 1024log(H) <h H.
2⌊ ⌋ ⌊1024log(H)⌋ ⌊ ⌋ ≤
n (cid:16) (cid:17)o
Weshallshowthatwithhighprobability,
h t s holdssimultaneouslyforallh [H]. Asanimmediate
i=1 i ≤ h ∈
consequence, we have E H t =o(H). Substituting this into (12) leads to the claimed conclusion.
h=1 h P
(cid:2)P (cid:3)
4.2 Regret upper bound for our proposed algorithm
We proceedto provide the proofoutline ofTheorem 2 in this section. We shallbound the regretincurredin
the two stages separately:
H0 H
reg = r⋆ r(aac) + (r⋆ r(aac) . (14)
H − h − h
h X=1
(cid:0) (cid:1)
h= XH0+1
(cid:1)
=:regt Hs =:regt Ha
| {z } | {z }
Stage 1: trust set identification. In this stage, it suffices to upper bound the round length m required
(1)
to determine whether each arm k belongs to the trust set . Towards this, recall the definitions of Y
T k
and Y(2) in (7). For k = 1, it is easy to see that E Y(1) +Y(2) equals 1 O(1/m) (resp. O(1/m)) when
k k k −
1 (resp. 1 / ). Moreover, the variance satisfies Var(Y(1) +Y(2)) = O(1/m2). Hence, applying the
Be∈ rnT stein inequ∈ aliT ty shows that with high probabil(cid:2) ity, Y(1) +k Y(cid:3) (2) k exceeds 1e /2 if 1 , ae nd falls below
k k ∈ T
1/2 otherwise. As for k > 1, straightforward calculations yields E Y(2) Ye(1) = Ω 1/(k2m) S /k2
k − k ∨ k−1
and Var(Y(2) Y(1)) = O (k S )/(k2m2) S /(mk2) , where S := 1 i . Given
k − k − k−1 ∨ k−1 (cid:2) k−1 (cid:3) 1≤i≤(cid:0)k−1 { ∈ T} (cid:1)
our choice of round length m K3log(H), we can invoke the Bernstein inequality to show that with high
(cid:0) ≍ (cid:1) P
9probability, Y(2) Y(1) exceeds the comparison threshold λ in (8) when k . Similarly, one can also
k − k k ∈ T
apply the same argumentto show that the difference is smaller than λ in the case k / . Combining these
k
∈T
two observations allows us to reliably test whether k for each k [K].
∈T ∈
Therefore, the regret incurred in Stage 1 can be bounded by
H0
E regts =E r⋆ r(aac) H =2mK .K4log(H). (15)
H " − h #≤ 0
h=1
(cid:2) (cid:3) X(cid:0) (cid:1)
Stage 2: trust-aware exploration-exploitation. Inviewof (11),wedecomposetheregretinthisstage
into the following two parts:
H H
regta = t r⋆ r(apm) + (1 t )(r⋆ rown). (16)
H h − h − h − h
h= XH0+1
(cid:0) (cid:1)
h= XH0+1
regt Ha-pm regt Ha-own
| {z } | {z }
To control the second term
regta-own,
we note the key observation that output by Stage 1 accurately
H T
estimate the trust set with high probability. Consequently, one can use induction to show that the trust
T
level t obeys b
h
1 H K4log(H)
1 t = 1 (1 t ). 0 = . (17)
h h−1
− − 1+H +h − h h
(cid:18) 0 (cid:19)
As a direct result,
regta-own
can be controlled by
H
H H
E regta-own =E ∆own(1 t ) E (1 t ) .K4log2(H). (18)
H " h − h #≤ " − h #
(cid:2) (cid:3)
h= XH0+1 h= XH0+1
Turning to
regta-pm,
recall that χ = 1 means that the implementation implements the recommended
H h
policy. One can express
H H
regta-pm = t r⋆ r(apm) = ∆(a) t 1 apm =a . (19)
H h − h h { h }
h= XH0+1
(cid:0) (cid:1)
aX∈Tb h= XH0+1
By invoking a concentration argument, we know that with high probability,
H H
χ 1 apm =a t 1 apm =a O K2log(H) .
h { h }≥ h { h }−
h= XH0+1 h= XH0+1
(cid:0) (cid:1)
Meanwhile, one can apply a standard UCB argument (see e.g., Lattimore and Szepesvári (2020)) to show
that with high probability, H χ 1 apm =a . log(H). Putting these two observationstogether leads
h=H0+1 h { h } ∆2(a)
to H t 1 apm =a .O log(H) +K2log(H) H. Combined with (19), this leads to
h=H0+1 h { h } P ∆2(a) ∧
P (cid:0) E regta-pm . KH(cid:1) log(H)+K3log(H). (20)
H
Combining (15), (18), and (20)(cid:2)complet(cid:3)es thp e proof of Theorem 2.
5 Numerical experiments
The numerical effectiveness of our algorithm is shown in Figure 1. For the sake of comparison, we plot the
regrets of the UCB algorithmboth in the presence and absence of the trust issue. We set K =10. For each
arm k, the expected reward is set as r(k) = k/(2K) and the random reward follows R (k) (r(k),0.1).
h
∼ N
104000
UCBw/trustissue
3500
UCBw/otrustissue 0.8
3000 Trust-awareUCB
2500 0.6
Trust-awareUCB
2000
1500 0.4 UCBw/trustissue
1000
0.2
500
0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
H:timehorizon ×105 H:timehorizon ×105
(a) (b)
Figure 1: Comparisonof the trust-aware and trust-blind algorithms: (a) regret; (b) trust level.
The implementer’s trust set is set to be = 9,10 and the own policy πown is chosen to be Unif( 9,10 )
T { } h { }
for all h 1.
≥
Figure 1 (a) plots the regret vs. the horizon length H, with the regret averaged over 100 independent
Monte Carlo trials; (b) depicts the trust level vs. the horizon length H in a typical Monte Carlo trial. As
canbeseen,ourtrust-awarealgorithmoutperformsthe(trust-blind)UCBalgorithminthepresenceoftrust
issues, whose regret grows linearly as predicted by our theorem. When recommendations are fully trusted
and followed, our algorithm achieves comparable performances to the UCB algorithm. In addition, our
algorithmmaintainsahighleveloftrustaftertheinitialstage,whereasthe trustleveloftheUCBalgorithm
decays rapidly to near zero, which is consistent with our theory.
6 Discussion
We have studied the trust-aware MAB problem, where decision-making needs to account for deviations
in decision implementation due to human trust issues. We established the suboptimality of vanilla MAB
algorithms when faced with the trust issue and proposed a two-stage trust-aware algorithm, which achieves
provable (near-)minimax optimal statistical guarantees.
Moving forward, several extensions are worth pursuing. To begin with, the proposed algorithm achieves
the minimax regret when the time horizon is not too small. Investigating whether it is possible to achieve
minimax optimality across the entire H range would be valuable. Also, it is important to develop a general
frameworkthataccommodatesawidervarietyoftrustmodels,makingtheapproachmorerobustandbroadly
applicable. Finally, extending the MAB framework to study trust-aware reinforcement learning within the
MDP framework would be of great interest.
References
Agrawal, S. and Goyal, N. (2012). Analysis of thompson sampling for the multi-armed bandit problem. In
Conference on learning theory, pages 39–1.JMLR Workshop and Conference Proceedings.
Akash, K., Reid, T., and Jain, N. (2019). Improving human-machine collaboration through transparency-
based feedback–part ii: Control design and synthesis. IFAC-PapersOnLine, 51(34):322–328.
Amani, S., Alizadeh, M., and Thrampoulidis, C. (2019). Linear stochastic bandits under safety constraints.
Advances in Neural Information Processing Systems, 32.
Auer, P., Cesa-Bianchi,N., and Fischer, P. (2002a). Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47:235–256.
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. (2002b). The nonstochastic multiarmed bandit
problem. SIAM journal on computing, 32(1):48–77.
Auer,P.andOrtner,R.(2010). Ucbrevisited: Improvedregretboundsforthestochasticmulti-armedbandit
problem. Periodica Mathematica Hungarica, 61(1-2):55–65.
11
tergeR tsurTAzevedo-Sa,H., Jayaraman,S. K., Yang, X. J., Robert, L. P., and Tilbury, D. M. (2020). Context-adaptive
management of drivers’ trust in automated vehicles. IEEE Robotics and Automation Letters, 5(4):6908–
6915.
Badanidiyuru, A., Kleinberg, R., and Slivkins, A. (2018). Bandits with knapsacks. Journal of the ACM
(JACM), 65(3):1–55.
Bhat,S.,Lyons,J.B.,Shi,C.,andYang,X.J.(2022).Clusteringtrustdynamicsinahuman-robotsequential
decision-making task. IEEE Robotics and Automation Letters, 7(4):8815–8822.
Bhat,S.,Lyons,J.B.,Shi,C.,andYang,X.J.(2024).Evaluatingtheimpactofpersonalizedvaluealignment
in human-robot interaction: Insights into trust and team performance outcomes. In Proceedings of the
2024 ACM/IEEE International Conference on Human-Robot Interaction, pages 32–41.
Bubeck, S., Cesa-Bianchi, N., et al. (2012). Regret analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends® in Machine Learning, 5(1):1–122.
Cai, C., Cai, T. T., and Li, H. (2024). Transfer learning for contextualmulti-armed bandits. The Annals of
Statistics, 52(1):207–232.
Cassel,A., Mannor,S., andZeevi,A.(2018). Ageneralapproachto multi-armedbandits under riskcriteria.
In Conference on learning theory, pages 1295–1306.PMLR.
Cella, L., Lazaric, A., and Pontil, M. (2020). Meta-learning with stochastic linear bandits. In International
Conference on Machine Learning, pages 1360–1370.PMLR.
Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, learning, and games. Cambridge university press.
Chen, M., Nikolaidis, S., Soh, H., Hsu, D., and Srinivasa, S. (2018). Planning with trust for human-robot
collaboration.InProceedings ofthe2018 ACM/IEEEinternationalconferenceonhuman-robotinteraction,
pages 307–315.
Chen, M., Nikolaidis, S., Soh, H., Hsu, D., and Srinivasa, S. (2020). Trust-aware decision making for
human-robotcollaboration: Modellearningandplanning.ACMTransactionsonHuman-RobotInteraction
(THRI), 9(2):1–23.
Cover, T. M. (1999). Elements of information theory. John Wiley & Sons.
Even-Dar, E., Mannor, S., Mansour, Y., and Mahadevan, S. (2006). Action elimination and stopping
conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning
research, 7(6).
Frazier,P., Kempe, D., Kleinberg,J.,andKleinberg,R.(2014). Incentivizing exploration. InProceedings of
the fifteenth ACM conference on Economics and computation, pages 5–22.
Freedman, D. A. (1975). On tail probabilities for martingales. the Annals of Probability, pages 100–118.
Gerchinovitz,S.andLattimore,T.(2016). Refinedlowerboundsforadversarialbandits. Advances in Neural
Information Processing Systems, 29.
Guo, Y. and Yang, X. J. (2021). Modeling and predicting trust dynamics in human–robot teaming: A
bayesian inference approach. International Journal of Social Robotics, 13(8):1899–1909.
Kallus, N. and Udell, M. (2020). Dynamic assortment personalization in high dimensions. Operations
Research, 68(4):1020–1037.
Kleinberg, R. and Leighton, T. (2003). The value of knowing a demand curve: Bounds on regret for online
posted-price auctions. In 44th Annual IEEE Symposium on Foundations of Computer Science, 2003.
Proceedings., pages 594–605.IEEE.
12Kveton, B., Konobeev, M., Zaheer, M., Hsu, C.-w., Mladenov, M., Boutilier, C., and Szepesvari, C. (2021).
Meta-thompson sampling. In International Conference on Machine Learning, pages 5884–5893.PMLR.
Lai,T.L., Robbins,H., etal.(1985). Asymptoticallyefficientadaptiveallocationrules. Advances in applied
mathematics, 6(1):4–22.
Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press.
Lazaric, A., Brunskill, E., et al. (2013). Sequential transfer in multi-armed bandit with finite set of models.
Advances in Neural Information Processing Systems, 26.
Li, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th international conference on World wide web,
pages 661–670.
Liu,X.,Li,B.,Shi,P.,andYing,L.(2021). Anefficientpessimistic-optimisticalgorithmforstochasticlinear
bandits with general constraints. Advances in Neural Information Processing Systems, 34:24075–24086.
Mansour, Y., Slivkins, A., and Syrgkanis, V. (2015). Bayesian incentive-compatible bandit exploration. In
Proceedings of the Sixteenth ACM Conference on Economics and Computation, pages 565–582.
Moyano, F., Beckers, K., and Fernandez-Gago, C. (2014). Trust-aware decision-making methodology for
cloud sourcing. In International Conference on Advanced Information Systems Engineering, pages 136–
149. Springer.
Pacchiano, A., Ghavamzadeh, M., Bartlett, P., and Jiang, H. (2021). Stochastic bandits with linear con-
straints. In International conference on artificial intelligence and statistics, pages 2827–2835.PMLR.
Rabbi,M.,Aung,M.S.,Gay,G.,Reid,M.C.,andChoudhury,T.(2018). Feasibilityandacceptabilityofmo-
bile phone–based auto-personalized physical activity recommendations for chronic pain self-management:
Pilot study on adults. Journal of medical Internet research, 20(10):e10147.
Robbins, H. (1952). Some aspects of the sequential design of experiments. Bulletin of the American Mathe-
matical Society, 58(5):527–535.
Robinette,P.andHoward,A.M.(2012). Trustinemergencyevacuationrobots. In2012 IEEE international
symposium on safety, security, and rescue robotics (SSRR), pages 1–6. IEEE.
Sani, A., Lazaric, A., and Munos, R. (2012). Risk-aversion in multi-armed bandits. Advances in neural
information processing systems, 25.
Slivkins, A. et al. (2019). Introduction to multi-armed bandits. Foundations and Trends® in Machine
Learning, 12(1-2):1–286.
Tewari, A. and Murphy, S. A. (2017). From ads to interventions: Contextual bandits in mobile health. In
Mobile Health, pages 495–517.Springer.
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3-4):285–294.
Tsybakov,A. B. (2008). Introduction to Nonparametric Estimation. Springer.
Wang,K.,Kallus,N., andSun, W.(2023). Near-minimax-optimalrisk-sensitivereinforcementlearningwith
cvar. In International Conference on Machine Learning, pages 35864–35907.PMLR.
Wang, Y., Chen, B., and Simchi-Levi, D. (2021). Multimodal dynamic pricing. Management Science,
67(10):6136–6152.
Xu, A. and Dudek, G. (2016). Maintaining efficient collaboration with trust-seeking robots. In 2016
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3312–3319.IEEE.
13Algorithm 2 (Trust-blind) UCB
1: Input: arm set [K], time horizon H.
2: Initialize arm set [K]. Set Ntb(a) 1 and UCBtb(a) 1 for any a [K].
A← 1 ← 1 ← ∈
3: for h=1,...,H do
4: Choose apm argmax UCBtb(a) with ties broken uniformly at random.
h ← a∈A h
5: Observe aa hc, R h(aa hc), and χ h.
6: Update Ntb (aac), where
h+1 h
s−1
Ntb(a):=1 1 aac =a , a [K],s>1. (21)
s ∨ { i } ∀ ∈
i=1
X
7: Update UCBtb (aac), where
h+1 h
s−1
1 log(H)
UCBtb(a):= R (a)1 aac =a +2 , a [K],s>1. (22)
s N stb(a)
i=1
i { i } sN stb(a) ∀ ∈
X
8: Output: policy {ap hm }h≥1.
A Proof of Theorem 1
Recall the definition of random variables χ , where χ = 1 indicates the implementer follows the
i i≥1 i
recommended policy πpm at time i, whereas χ{ =} 0 means the implementer takes her own policy πown. Let
i i
us define a sequence of random variables γ , where γ := 1 aown = K indicates whether or not the
{ i }i≥1 i { i }
implementer’s own policy chooses the K-th arm at time i. In addition, we define the filtration generated
i
F
by the information by time i, that is
=σ apm,aac,R(aac),χ .
Fi j j j j j∈[i]
(cid:16)(cid:0) (cid:1) (cid:17)
By Algorithm 1 and the trust update rule in Section 2, t and UCB (k) are predictable
{ i }i≥1 i k∈[K] i≥1
with respect to the filtration , that is, t and UCB (k) for all k [K]. Moreover,
{Fi }i≥1 i+1 ∈ Fi i+1(cid:8)(cid:0) ∈ Fi (cid:1) (cid:9) ∈
according to the assumption on the policy implementation deviation in Section 2, the distribution of χ
i
conditional on the filtration is given by
i−1
F
χ Bern(t ).
i i−1 i
|F ∼
Now let us begin the proof. As a reminder, the expected rewards of the MAB instance are set as
1
, if k <K;
6 log(H)+1
r(k)=

p
1
, if k =K.
3 log(H)+1
The implementer’s ownpolicy is chosen
tobe
p πown πown =Unif( K 1,K )for all h 1. As a result, we
h ≡ { − } ≥
have ∆own :=r⋆ rown =1/(12 log(H)+1).
As
rh
⋆
r(apm−
)
fh
or any h 1, we use the decomposition in (11) to lower bound the regret
≥ h ≥p
H H
1
reg regown =∆ownE (1 t ) = H E t . (23)
H ≥ H " − h # 12 log(H)+1 − " h #!
h=1 h=1
X X
p
This implies it boils down to controlling the cumulative sum of the trust levels.
14Towards this, recall the definition of s in (13) where
h
h, if 1 h 256log(H) ;
≤ ≤⌊ ⌋
1 2
 h+ 256log(H) , if 256log(H) <h 1024log(H) ;
s h := 13 3⌊ ⌋
h
⌊ ⌋ ≤⌊ ⌋
1024log(H) 1+log , if 1024log(H) <h H.
2⌊ ⌋ 1024log(H) ⌊ ⌋ ≤
Let us define
the
event
n (cid:16)⌊ ⌋(cid:17)o
h
:= t >s , (24)
h i h
H ( )
i=1
X
for each h 1. We shall show that
≥
H
P 4H−1/3. (25)
i
H ≤
(cid:18)i=1 (cid:19)
[
As an immediate consequence, one obtains
H H H
E t E 1 c t +E 1 t
" h #≤ " {HH} h # " {HH } h #
h=1 h=1 h=1
X X X
s +P( )H
H H
≤ H
512 log(H)+1 +4H2/3,
≤
where the last step applies (13), (25) and th(cid:0)e fact that (cid:1) t 1 for any h 1. Substituting this into (23)
h
≤ ≥
yields that for H sufficiently large,
1 H
reg H 512 log(H)+1 4H2/3 c ,
H ≥ 12 log(H)+1 − − ≥ log(H)
(cid:16) (cid:0) (cid:1) (cid:17)
where c>0 is some universapl constant. p
Therefore,itsufficestoestablish(25). Tothisend,weneedthefollowingFreedman’sinequality(Cesa-Bianchi and Lugosi,
2006, Lemma A.8), which is a generalization of the Bernstein inequality for a sum of bounded martingale
difference sequences. (See also Freedman (1975)).
Lemma 1 (Freedman’sinequality). LetX ,...,X bea boundedmartingale difference sequencewith respect
1 n
to a filtration ( ) and with X R. Let S := i X be the associated martingale. Denote the
Fi 0≤i≤n | i | ≤ i j=1 j
sum of the conditional variances by
P
n
Σ2 := E X2 . (26)
n i |Fi−1
i=1
X (cid:2) (cid:3)
Then for any τ,σ2 > 0, one has
τ2
P maxS >τ and Σ2 σ2 exp . (27)
i∈[n] i n ≤ ≤ −2(σ2+Rτ/3)
(cid:26) (cid:27) (cid:18) (cid:19)
As a consequence, for any 0<δ <1, one further has
P maxS >3Rlog(1/δ) 3σ2log(1/δ) and Σ2 σ2 δ. (28)
i∈[n] i ∨ n ≤ ≤
(cid:26) (cid:27)
p
With Lemma 1 in place, we proceed to prove (25).
15• We start with the case 1 h 256log(H) . As the trust level t is bounded in [0,1] for any i 1, it
i
≤ ≤⌊ ⌋ ≥
is straightforwardto bound
h
t h=s ,
i h
≤
i=1
X
where we recall the definition of s in (13) for h 256log(H) . Therefore, we have
h
≤⌊ ⌋
P( )=0, (29)
h
H
for all 1 h 256log(H) .
≤ ≤⌊ ⌋
• Next, let us fix an arbitrary 256log(H) <h 1024log(H) . We decompose the sum
⌊ ⌋ ≤⌊ ⌋
h ⌊256log(H)⌋ h
t = t + t . (30)
i i i
Xi=1 Xi=1 i=⌊256Xlog(H)⌋+1
We shall show that the trust level t is upper bounded by 1/3 for all i > 256log(H) with high
i
⌊ ⌋
probability. Byconstruction,anyarminthecomplementofthesupportofπown (namely, 1,2,...,K
{ −
2 )getspulledonlywhentheimplementerfollowstherecommendedpolicy,i.e.,whenχ =1. Therefore,
i
}
the following bound holds for any 1 i 1024log(H) ,
≤ ≤⌊ ⌋
K−2 i
Ntb(k) χ i 1024log(H). (31)
i ≤ j ≤ ≤
k=1 j=1
X X
As a result, this indicates that there exist at least three arms k ,k ,k [K] satisfying
1 2 3
∈
maxNtb(k ) 4log(H).
j∈[3] i j ≤
This further implies that
log(H)
UCBtb(k )= µ (k )+2 1=1.
i j i j sN itb(k j)!∧
b
Therefore, we find that k ,k ,k argmax UCB (k) for any 1 i 1024log(H) . On the
{ 1 2 3 } ⊂ k∈[K] i ≤ ≤ ⌊ ⌋
other hand, note that the trust increases only if apm = K by the choice of the trust set. Combining
i
this observation with the uniformly random tie-breaking in Algorithm 1, we know that
1
P apm =K , (32)
{ i |Fi−1 }≤ 4
for all 1 i 1024log(H) . Let us define X := 1 apm = K P apm = K for each
≤ ≤ ⌊ ⌋ i { i }− { i | Fi−1 }
1 i 1024log(H) . Straightforwardcalculation reveals that X 1,
i
≤ ≤⌊ ⌋ | |≤
E X =0,
i i−1
|F
and (cid:2) (cid:3)
1
E X2 =P apm =K 1 P apm =K .
i |Fi−1 { i |Fi−1 } − { i |Fi−1 } ≤ 4
(cid:2) (cid:3) (cid:0) (cid:1)
ApplyingLemma1withδ =H−4/3,R=1,andσ2 =i/4,weobtainthatforany1 i 1024log(H) ,
≤ ≤⌊ ⌋
i i
P 1 apm =K > P apm =K +4log(H) ilog(H) H−4/3. (33)
( { j } j |Fi−1 ∨ )≤
j=1 j=1
X X (cid:8) (cid:9) p
16Note that when i 256log(H) 225log(H), we have
≥⌊ ⌋≥
4 1 1
4log(H) ilog(H) i = i. (34)
∨ ≤ 225 ∨ 225 15
(cid:18) r (cid:19)
p
Therefore, let us define the event
i
19
:= 1 apm =K > i , (35)
Ji ( { j } 60 )
j=1
X
for each 256log(H) i 1024log(H) . Combining (32), (33), and (34), we have P( i) H−4 3 for
⌊ ⌋≤ ≤⌊ ⌋ J ≤
all 256log(H) i 1024log(H) . It follows from the union bound that
⌊ ⌋≤ ≤⌊ ⌋
⌊1024log(H)⌋
P
i
H−31 , (36)
J !≤
i=⌊256[log(H)⌋
In what follows, we shall work on the event ⌊1024log(H)⌋ c. Recall the trust update rule in (2)–(3).
i=⌊256log(H)⌋Ji
The trust level t at time i admits the following expression:
i T
i−1
1 1
t = + 1 apm =K . (37)
i 1+i 1+i { j }
j=1
X
This allows us to derive that for any 256log(H) <i 1024log(H) +1,
⌊ ⌋ ≤⌊ ⌋
i−1
1 1 1 19 1
t + 1 apm =K + , (38)
i ≤ 1+i i 1 { j }≤ 1+i 60 ≤ 3
− j=1
X
for sufficiently large H (for instance, 256log(H) 591). As a result, this demonstrates that
⌊ ⌋≥
h ⌊256log(H)⌋ h
1
t = t + t 256log(H) + h 256log(H) =s ,
i i i h
≤ 3 −
Xi=1 Xi=1 i=⌊256Xlog(H)⌋+1
(cid:4) (cid:5) (cid:0) (cid:4) (cid:5)(cid:1)
where the last step follows from the definition of s in (13). Consequently, this shows that
h
⌊1024log(H)⌋
c c .
Ji ⊂HH
i=⌊256\log(H)⌋
Recognizing that this holds for an arbitrary 256log(H) <h 1024log(H) , we find that
⌊ ⌋ ≤⌊ ⌋
⌊1024log(H)⌋ ⌊1024log(H)⌋
c c.
Ji ⊂ Hi
i=⌊256\log(H)⌋ i=⌊256l\og(H)⌋+1
Combining this with (36), we conclude that
⌊1024log(H)⌋ ⌊1024log(H)⌋
P
i
P
i
H−31 . (39)
H !≤ J !≤
i=⌊256l[og(H)⌋+1 i=⌊256[log(H)⌋
• As for any h> 1024log(H) , we shall prove
⌊ ⌋
⌊1024log(H)⌋ h
P
i i
H−31 +2hH−4 3. (40)
(cid:18)i=⌊256[log(H)⌋J
(cid:19)
[(cid:18)i=⌊1024[log(H)⌋+1H (cid:19)!≤
17Then taking h=H leads to
H
P i H−1 3 +2H H−34 =3H−1/3.
H !≤ ·
i=⌊1024[log(H)⌋+1
Towardsthis, we would like to establish (40) by induction. To begin with, (40) holds for the base case
h= 1024log(H) as shown in (39).
⌊ ⌋
Next, let us fix an arbitrary h > 1024log(H) and assume that (40) holds for h. We wish to prove
⌊ ⌋
the claim for h+1. By the trust update rule in (2) and (3), we can also express the trust level t at
i
time i as
1 1
t = 1 t + 1 apm =K . (41)
i − 1+i i−1 1+i { i−1 }
(cid:18) (cid:19)
For each i> 1024log(H) , let us define the event
⌊ ⌋
:= apm =K . (42)
Ji i
On the event h c, we can use ((cid:8) 41) to der(cid:9) ive
i=⌊1024log(H)⌋+1Ji
T 1 1 1 2+ 1024log(H)
t = 1 t = 1 1 t = ⌊ ⌋t ,
i − 1+i i−1 − 1+i − i i−2 1+i ⌊1024log(H)⌋+1
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)
for all 1024log(H) <i h+1. Moreover, recall from (38) that on the event ⌊1024log(H)⌋ c, the
⌊ ⌋ ≤ i=⌊256log(H)⌋Ji
trust at time i satisfies t 1 for any 256log(H) <i 1024log(H) +1. This reveals that on the
i ≤ 3 ⌊ ⌋ ≤⌊ ⌋ T
event h c, the trust level at time i obeys
i=⌊256log(H)⌋Ji
T 2+ 1024log(H) 2+ 1024log(H) 1024log(H)
t = ⌊ ⌋t ⌊ ⌋ ⌊ ⌋, (43)
i 1+i ⌊1024log(H)⌋+1 ≤ 3(1+i) ≤ 2i
for all 1024log(H) < i h+1 provided H is sufficiently large. As a consequence, on the event
⌊ ⌋ ≤
h c, the following holds for all 1024log(H) <i h+1:
i=⌊256log(H)⌋Ji ⌊ ⌋ ≤
T
i ⌊256log(H)⌋ ⌊1024log(H)⌋ i
t = t + t + t
j j j j
Xj=1 Xj=1 j=⌊256Xlog(H)⌋+1 j=⌊1024Xlog(H)⌋+1
i
(i) 1 1024log(H)
256log(H) + 1024log(H) 256log(H) + ⌊ ⌋
≤ ⌊ ⌋ 3 ⌊ ⌋−⌊ ⌋ 2j
(cid:0) (cid:1)
j=⌊1024Xlog(H)⌋+1
(ii) 1 i 1024log(H)
1024log(H) + ⌊ ⌋dx
≤ 2⌊ ⌋ 2x
Z⌊1024log(H)⌋
1 1 i (iii)
= 1024log(H) + 1024log(H) log = s .
i
2⌊ ⌋ 2⌊ ⌋ 1024log(H)
(cid:18)⌊ ⌋(cid:19)
Here,(i)arisesfromt [0,1]foranyi 1,(38),and(43);(ii)istrueas4 x 4x foranyx [0,1];
i
∈ ≥ ⌊ ⌋≤⌊ ⌋ ∈
(iii) uses the definition of s in (13). This demonstrates that
h
h h+1
c c.
Ji ⊂ Hi
i=⌊256\log(H)⌋ i=⌊1024\log(H)⌋+1
It follows that
⌊1024log(H)⌋ h+1
i i
J H
(cid:18)i=⌊256[log(H)⌋ (cid:19) [(cid:18)i=⌊1024[log(H)⌋+1 (cid:19)
18⌊1024log(H)⌋ h
i i
⊂ J J
(cid:18)i=⌊256[log(H)⌋ (cid:19) [(cid:18)i=⌊256[log(H)⌋ (cid:19)
⌊1024log(H)⌋ h h
= ( ) ( c)
Ji Ji ∩Hi Ji ∩Hi
(cid:18)i=⌊256[log(H)⌋ (cid:19) [(cid:18)i=⌊1024[log(H)⌋+1 (cid:19) [(cid:18)i=⌊1024[log(H)⌋+1 (cid:19)
⌊1024log(H)⌋ h h
c ,
⊂ Ji Hi Ji ∩Hi
(cid:18)i=⌊256[log(H)⌋ (cid:19) [(cid:18)i=⌊1024[log(H)⌋+1 (cid:19) [(cid:18)i=⌊1024[log(H)⌋+1
(cid:0)
(cid:1)(cid:19)
which leads to
⌊1024log(H)⌋ h+1
P
i i
(cid:18)i=⌊256[log(H)⌋J
(cid:19)
[(cid:18)i=⌊1024[log(H)⌋+1H (cid:19)!
⌊1024log(H)⌋ h h
P + P c .
≤ (cid:18)i=⌊256l[og(H)⌋+1Ji
(cid:19)
[(cid:18)i=⌊1024[log(H)⌋+1Hi (cid:19)!
i=⌊1024Xlog(H)⌋+1
(cid:0)Ji ∩Hi
(cid:1)
We claim that for any 1024log(H) <i h, one has
⌊ ⌋ ≤
P
Ji
∩Hic ≤2H−7 3. (44)
Assuming the validity of (44), we arrive a(cid:0)t (cid:1)
⌊1024log(H)⌋ h+1
P i i H−1 3 +2hH−4 3 +2hH−37 H−1 3 +2(h+1)H−4 3,
(cid:18)i=⌊256[log(H)⌋J
(cid:19)
[(cid:18)i=⌊1024[log(H)⌋+1H (cid:19)!≤ ≤
leading to the claim in (40) for h+1. This completes the proof of the claim in (40) by standard
induction arguments.
Therefore, it remains to prove (44). Before proceeding, we summarize several bounds for s (cf. (13))
h
that will be useful for the proof. First of all, for any i> 1024log(H) , one has
⌊ ⌋
1 i 1 i i
s = 1024log(H) 1+log 1024log(H) = , (45)
i
2⌊ ⌋ (cid:18)⌊1024log(H) ⌋(cid:19)!≤ 2⌊ ⌋ ⌊1024log(H)
⌋
2
where the inequality holds since 1+log(x) x for any x > 0. Next, it is straightforward to check
≤
that (45) also holds for i = 1024log(H) , namely, s = 1 1024log(H) + 2 256log(H)
⌊ ⌋ ⌊1024log(H)⌋ 3⌊ ⌋ 3⌊ ⌋ ≤
1 1024log(H) . In addition, for any i 1024log(H) , we have
2⌊ ⌋ ≥⌊ ⌋
1
s 1024log(H) 511log(H), (46)
i
≥ 2⌊ ⌋≥
for sufficiently large H, and
1
s < 1024log(H) 1+log(H) 512log(H) 1+log(H) . (47)
i
2⌊ ⌋ ≤
(cid:0) (cid:1) (cid:0) (cid:1)
Withtheseresultsinplace,letusbeginproving(44). Foreachi 1,letusdefinetherandomvariables
≥
1
X :=χ t and Y :=γ (1 χ ) (1 t ),
i i i i i i i
− − − 2 −
where we recall γ :=1 aown =K . In addition, we define the events
i { i }
i i
:= χ > t +7log(H) 7log(H)s ,
i j j i
K ( ∨ )
j=1 j=1
X X p
19i i
1
:= γ (1 χ )< (1 t ) 7log(H) 7log(H)i ,
i j j j
L ( − 2 − − ∨ )
j=1 j=1
X X p
for each 1024log(H) <i h. It is straightforwardto check that E[X ]=E[Y ]=0,
i i−1 i i−1
⌊ ⌋ ≤ |F |F
E X2 =t (1 t ) t ,
i |Fi−1 i − i ≤ i
E (cid:2)Y i2 |Fi−1(cid:3) ≤1,
for any i 1. Invoking Lemma 1 w(cid:2) ith δ = (cid:3) H−37 , R = 1, and σ2 = s i, we know that for each
≥
1024log(H) <i h,
⌊ ⌋ ≤
P c H−7/3, (48)
{Ki ∩Hi}≤
and
P c H−7/3. (49)
{Li ∩Hi}≤
Recall the definition of in (24) and πown = Unif( K 1,K ) for all h 1. On the event c c,
Hi h { − } ≥ Ki Hi
we have
T
K−2 i i
Ntb(k) χ t +7log(H) 7log(H)s
i ≤ j ≤ j ∨ i
k=1 j=1 j=1
X X X p
7 7 3
1+ s s .
i i
≤ 511 ∨ 511 ≤ 2
(cid:18) r (cid:19)
where the last line uses the fact that i t s on the event c and the lower bound of s in (46).
j=1 j ≤ i Hi i
In particular, this allows us to obtain
P
K−2
1 1 3 1
min Ntb(k) Ntb(k) s = s . (50)
1≤k≤K−2 i ≤ K 2 i ≤ 2582 i 172 i
− k=1
X
Meanwhile, on the event c c, we can lower bound
Li ∩Hi
i i
1
Ntb(K) γ (1 χ ) (1 t ) 7log(H) 7log(H)i
i ≥ j − j ≥ 2 − j − ∨
j=1 j=1
X X p
1 7 7 1 1
i s s ,
i i
≥ 2 − 511 ∨ 511 − 2 ≥ 6
(cid:18) r (cid:19)
where the penultimate inequality holds because i t s on the event c and log(H) 1 s
j=1 j ≤ i Hi ≤ 511 i ≤
1 i due to (45) and (46); the last inequality holds because of (45). This implies that
1022 P
log(H) 96log(H)
UCB (K) r(K)+4 1 r(K)+ 1.
i ≤ (cid:18) sN itb(K) (cid:19)∧ ≤ (cid:18) s s i (cid:19)∧
On the other hand, (50) allows us to derive
log(H) 2752log(H)
max UCB (k) 4 1 1. (51)
1≤k≤K−2 i ≥ smin 1≤k≤K−2N itb(K) ∧ ≥s s i ∧
By our construction of the instance, the reward of the optimal arm satisfies
1 1 512log(H)
r(K)= < ,
3s1+log(H) s 9s i
20where the last step follows from (47). Taken collectively with the fact that 512/9+√96 < √2752,
this leads to
p
UCB (K)< max UCB (k).
i i
1≤k≤K−2
BythearmselectionprocedureoftheUCBalgorithm,weknowthatr(apm)=K. Recallthedefinition
i 6
of for i> 1024log(H) in (42). This implies that
i
J ⌊ ⌋
c c c c c,
Ki ∩Li ∩Hi ⊂Ji ∩Hi
which further yields
c c c .
Ji ∩Hi ⊂ Ki ∩Hi ∪ Li ∩Hi
As a result, combining (48), (49) with the un(cid:0)ion boun(cid:1)d p(cid:0)roves (44(cid:1)).
• Finally,putting (29),(39),and(40)collectivelywiththe unionboundestablishes(25). This concludes
the proof of Theorem 1.
B Proof of Theorem 2
Stage 1: trust set identification. Recall the definitions of Y(1) and Y(2) in (7):
k k
m 2m
1 1
Y(1) := χ and Y(2) := χ .
k m 2m(k−1)+i k m 2m(k−1)+i
i=1 i=m+1
X X
In addition, we denote
δ :=Y(2) Y(1), k >1.
k k − k ∀
As a reminder, for any k > 1, the value of δ will be used to test whether the k-th arm belongs to the
k
implementer’s trust set or, equivalently, leads to increasing trust.
To this end, denoteT S := k 1 l for each k [K] and we set S := 0 by default. It is
k ℓ=1 { ∈ T} ∈ 0
straightforwardto see that for any k [K] and i [2m],
P ∈ ∈
1+2mS +i 1
k−1
− , if k ;
2+2m(k 1)+i 1 ∈T
t =E χ = − −
2m(k−1)+i 2m(k−1)+i 1+2mS
 k−1
, if k / .
(cid:2) (cid:3) 2+2m(k 1)+i 1 ∈T
− −
• We begin with the case k =1. Let us
denoteY
:=Y(1)+Y(2).
1 1 1
– If 1 / , the expectation of Y can be upper bounded by
1
∈T
1 2m 1 1 2m 1 1
E[Y ]= dx log(1+2m).
1
2m 1+i ≤ 2m 1+x ≤ 2m
i=1 Z0
X
As for the variance, we can compute
2m 2m 2m
i
V := Var(χ )= t (1 t )= .
1 i i − i (1+i)2
i=1 i=1 i=1
X X X
As x x/(a+x)2 is increasing in [0,a] and decreasing in [a, ) for any a>0, we can bound
7→ ∞
2m i 1 2m i 1 2m x
V = = + + dx
1 (1+i)2 4 (1+i)2 ≤ 4 (1+x)2
i=1 i=2 Z1
X X
211 1 1
= +log m+ log(m),
1+2m − 4 2 ≤
(cid:18) (cid:19)
where the last step holds as long as m 4. Therefore, applying the Bernstein inequality shows
≥
that with probability at least 1 H−2,
−
4log(H) 2
Y E[Y ]+ + V log(H)
1 1 1
≤ 3 m m
log(1+2m) 4log(Hp) 2 1
+ + log(m)log(H) ,
≤ 2m 3m m ≤ 2
p
where the last step holds provided m 3log(H) and H is sufficiently large.
≥
– On the other hand, if 1 , we can compute
∈T
2m
1 i 1
E[Y ]= 1 log(1+2m).
1
2m 1+i ≥ − 2m
i=1
X
and
2m 2m
i
V := Var(χ )= log(m).
1 i (1+i)2 ≤
i=1 i=1
X X
Invoking the Bernstein inequality yields that with probability exceeding 1 H−2,
−
4log(H) 2
Y E[Y ] V log(H)
1 1 1
≥ − 3 m − m
log(1+2m) 4lopg(H) 2 1
1 log(m)log(H) ,
≥ − 2m − 3m − m ≥ 2
p
where the last step is true as long as m 3log(H) and H is sufficiently large.
≥
– As a result, our procedure that adding arm 1 to the estimated trust set if Y 1 correctly
1 ≥ 2
identifies 1 with probability at least 1 H−2.
∈T −
• We proceed to consider the case k >1, where δ is used to identify the trust set.
k
– Let us first consider the case k / .
∈T
To begin with, we can apply the trust update mechanism to control the expectation of δ by
k
m−1 m−1
1 1+2S m 1 1+2S m
E[δ ]= k−1 k−1
k
m 2+2(k 1)m+m+i − m 2+2(k 1)m+i
i=0 − i=0 −
X X
m−1
1+2S m
k−1
=
− 2+2(k 1)m+i 2+2(k 1)m+m+i
i=0 − −
X
m (cid:0) 1+(cid:1)(cid:0)2S k−1m (cid:1)
dx
≤− 2+2(k 1)m+x 2+2(k 1)m+m+x
Z0
− −
1 m2
(cid:0) (cid:1)(cid:0) (cid:1)
= (1+2S m)log 1+ .
k−1
−m 4 1+(k 1)m (1+km)
(cid:18) − (cid:19)
As log(1+x) x/2 for any x [0,1], one can furth(cid:0)er upper boun(cid:1)d
≥ ∈
m(1+2S m) 1+2S m
E[δ ] k−1 k−1 , (52)
k
≤−8 1+(k 1)m (1+km) ≤−9(k 1)km
− −
where the last step is true provid(cid:0)ed m 1. (cid:1)
≫
22As for the variance of δ , it is not hard to see that
k
2m 2m
V := Var χ = t (1 t )
k 2(k−1)m+i 2(k−1)m+i 2(k−1)m+i
−
i=1 i=1
X (cid:0) (cid:1) X
2m−1
1+2(k 1 S )m+i
k−1
=t 1 t +(1+2S m) − − .
2(k−1)m+1 − 2(k−1)m+1 k−1 2+2(k 1)m+i 2
(cid:0) (cid:1) Xi=1 −
Straightforwardcalculation yields (cid:0) (cid:1)
2m−1
1+2(k 1 S )m+i
k−1
− −
2
2+2(k 1)m+i
Xi=1 −
(cid:0) 2m−1 1+2(k 1(cid:1) S k−1)m 2m x
− − dx+ dx
≤ Z0 2+2(k 1)m+x 2 Z1 2+2(k 1)m+x 2
− −
2m 1+(cid:0)2(k 1 S k−1)m(cid:1)+x (cid:0) (cid:1)
− − dx
≤ Z0 2+2(k −1)m+x 2
1+2S m m m
(cid:0)k−1 (cid:1)
= +log 1+
− 2+2km 1+(k 1)m 1+(k 1)m
− (cid:18) − (cid:19)
1+2S m m m
k−1
+
≤− 2+2km 1+(k 1)m 1+(k 1)m
− −
m 1+2(k S )m
k−1
= − ,
1+(k 1)m 2(1+km)
−
where we use that fact that x x/(a+x)2 is increasing in [0,a] and decreasing in [a, ) for any
7→ ∞
a >0 and 2+2(k 1)m>2m for k >1; the last inequality follows from log(1+x) x for any
− ≤
x 0. Therefore, we find that
≥
m(1+2S m) 1+2(k S )m
k−1 k−1
V t 1 t + −
k 2(k−1)m+1 2(k−1)m+1
≤ − 2 1+(k 1)m (1+km)
−(cid:0) (cid:1)
(cid:0) (cid:1)
1 2(1+2S m)(k S )
+ k−1 − k−1 , (cid:0) (cid:1) (53)
≤ 4 (k 1)k
−
where the last line holds because t (1 t ) 1/4 for any h 1, k S 1, and m 1.
h h k−1
− ≤ ≥ − ≥ ≫
Putting (52) and (53) together, we now invoke the Bernstein inequality to find that with proba-
bility at least 1 H−2,
−
4log(H) 2
δ E[δ ]+ + V log(H)
k k k
≤ 3 m m
p
1+2S m 4log(H) 2 log(H) 2(1+2S m)(k S )log(H)
k−1 k−1 k−1
+ + + −
≤−9(k 1)km 3 m ms 4 (k 1)k
− −
1+2S m 7log(H) 2√2 (k S )log(H) S (k S )log(H)
k−1 k−1 k−1 k−1
+ + − +4 − (54)
≤−9(k 1)km 3 m m s (k 1)k s (k 1)km
− − −
1+2S m 6log(H) S (k S )log(H)
k−1 k−1 k−1
+ +4 − . (55)
≤−9(k 1)km m s (k 1)km
− −
where we use √a+b √a+√b for any a,b 0.
≤ ≥
If S =0, (55) implies that
k−1
1 6log(H) 1
δ + . (56)
k
≤−9(k 1)km m ≤ 5k
−
23where the final step is true as long as m 30Klog(H).
≥
If S =k 1, one has
k−1
−
2 6log(H) 1
δ + , (57)
k
≤−9k m ≤−5k
provided m Klog(H).
≫
Otherwise, we obtain
2 S 6log(H) S (k S )log(H) 1 S
k−1 k−1 k−1 k−1
δ + +4 − <0, (58)
k
≤−9(k 1)k m s (k 1)km ≤−9(k 1)k
− − −
where the last step holds as long as m K3log(H) log(H)k(k 1)(k S )/S .
k−1 k−1
≫ ≥ − −
– Let us proceed to consider the case k .
∈T
First, the expected difference E[δ ] can be bounded by
k
m−1 m−1
1 1+2S m+m+i 1 1+2S m+i
E[δ ]= k−1 k−1
k
m 2+2(k 1)m+m+i − m 2+2(k 1)m+i
i=0 − i=0 −
X X
m−1
1+2(k 1 S )m
k−1
= − −
2+2(k 1)m+i 2+2(k 1)m+m+i
i=0 − −
X
m (cid:0) 1+2(k(cid:1)(cid:0) 1 S k−1)m (cid:1)
− − dx
≥ 2+2(k 1)m+x 2+2(k 1)m+m+x
Z0
− −
1 m2
(cid:0) (cid:1)(cid:0) (cid:1)
= 1+2(k 1 S )m log 1+
k−1
m − − 4 1+(k 1)m (1+km)
(cid:18) − (cid:19)
(cid:0) (cid:1)
1+2(k 1 S )m
− − k−1 , (cid:0) (cid:1) (59)
≥ 9(k 1)km
−
where the last step holds as m 1 and log(1+x) x/2 for any x [0,1].
≫ ≥ ∈
Next, it is straightforwardto compute the variance
2m m
V := Var χ = t (1 t )
k 2(k−1)m+i 2(k−1)m+i 2(k−1)m+i
−
i=1 i=1
X (cid:0) (cid:1) X
2m−1
1+2S m+i
k−1
=t 1 t + 1+2(k 1 S )m .
2(k−1)m+1 − 2(k−1)m+1 − − k−1 2+2(k 1)m+i 2
(cid:0) (cid:1) (cid:0) (cid:1) Xi=1 −
Applying the same argument for (53), we can bound (cid:0) (cid:1)
1 2m 1+2S m+x
k−1
V + 1+2(k 1 S )m dx
k ≤ 4
(cid:0)
− − k−1 (cid:1)Z0 2+2(k −1)m+x 2
1 m 1+2(k 1 S k−1)m 1+(cid:0)2(S k−1+1)m (cid:1)
+ − −
≤ 4 2 1+(k 1)m (1+km)
(cid:0) − (cid:1)(cid:0) (cid:1)
1 1+2(k 1(cid:0) S k−1)m (S(cid:1)k−1+1)
+ − − . (60)
≤ 4 (k 1)k
(cid:0) − (cid:1)
Combining (59) and (60) with the Bernstein inequality yields that with probability at least 1
−
H−2,
4log(H) 2
δ E[δ ] V log(H)
k k k
≥ − 3 m − m
p
241+2(k −1 −S k−1)m 4log(H) 2 log(H)
+
1+2(k −1 −S k−1)m (S k−1+1)log(H)
≥ 9(k 1)km − 3 m − ms 4 (k 1)k
− (cid:0) − (cid:1)
1+2(k 1 S )m 7log(H) 2√2 (S +1)log(H) (k 1 S )(S +1)log(H)
k−1 k−1 k−1 k−1
− − +4 − −
≥ 9(k 1)km − 3 m − m s (k 1)k s (k 1)km
− − −
1+2(k 1 S )m 6log(H) (k 1 S )(S +1)log(H)
k−1 k−1 k−1
− − 4 − − . (61)
≥ 9(k 1)km − m − s (k 1)km
− −
where we use √a+b √a+√b for any a,b 0.
≤ ≥
If S =0, we know from (55) that
k−1
2 6log(H) log(H) 1
δ 4 > , (62)
k
≥ 9k − m − km 5k
r
where the last step follows from m Klog(H).
≫
If S =k 1, one knows that
k−1
−
1 6log(H) 1
δ . (63)
k
≥ 9(k 1)km − m ≥−5k
−
where the final inequality is true as long as m 30Klog(H).
≥
Otherwise, we obtain
2k 1 S 6log(H) (k 1 S )(S +1)log(H)
k−1 k−1 k−1
δ − − 4 − −
k
≥ 9 (k 1)k − m − s (k 1)km
− −
1k 1 S
k−1
− − >0, (64)
≥ 9 (k 1)k
−
where the last step holds as long as m K3log(H) k(k 1)(k S )/S log(H).
k−1 k−1
≫ ≥ − −
– When S =0, combining (56) and (62) shows that adding arm k to if δ >1/(5k) correctly
k−1 (cid:0) k (cid:1)
T
identifies whether k with probability at least 1 H−2.
∈T −
WhenS =k 1,putting(57)and(63)togetherrevealsthataddingarmkto ifδ > 1/(5k)
k−1 k
− T −
correctly tests whether k with probability exceeding 1 H−2.
∈T −
Otherwise, collecting (58) and (64) together demonstrates that adding arm k to if δ > 0
k
T
correctly determines whether k with probability at least 1 H−2.
∈T −
Finally, we can use an induction argument to conclude with probability exceeding 1 KH−2, all arms
−
outside the trust set have been eliminated after the first stage. In other words, by defining the event
:= = , (65)
ts
E T T
we have (cid:8) b (cid:9)
P c 2KH−2. (66)
{Ets}≤
In what follows, we shall work on the event .
ts
E
Stage 2: trust-aware exploration-exploitation. By Stage 1 of Algorithm 1, the trust level at time
H +1 satisfies t =S /K, where we recall S := K 1 k counts the number of the arms that
0 H0+1 K K k=1 { ∈T}
belong to the trust set . Similar to (41), we can express
T P
1 1 1
1 t = 1 (1 t )+ 1 apm / = 1 (1 t ), h>H +1.
− h − 1+h − h−1 1+h h−1 ∈T − 1+h − h−1 ∀ 0
(cid:18) (cid:19) (cid:18) (cid:19)
(cid:8) (cid:9)
25Here, the last line is true under the event . Therefore, one can then use induction to obtain that for each
ts
E
h>H ,
0
H +2 H +2K S
0 0 K
1 t = (1 t )= − . (67)
−
h
h+1 −
H0+1
h+1 K
In particular, t is an increasing function in h when h>H .
h 0
With this in place, we are ready to control the regret. By (14) and (16), one can derive
H0 H H
reg = r⋆ r(aac) + t r⋆ r(apm) + (1 t )(r⋆ rown).
H − h h − h − h − h
h X=1
(cid:0) (cid:1)
h= XH0+1
(cid:0) (cid:1)
h= XH0+1
regt Hs regt Ha-pm regt Ha-own
In what follows, we
sh|
all
cont{ rz
ol
regts}
,
re| gta-pm and{ rz egta-own sep} ara|
tely.
{z }
H H H
• Let us start with regts. By our choice of the round length m, it is easy to bound
H
regts H =2mK =2K4log(H). (68)
H ≤ 0
• Toboundregpm,letusfirstrecallthenotation∆(a):=r⋆ r(a)foranya [K]. Inaddition,wedefine
H − ∈
s−1
Npm(a):=1 1 apm =a , (69)
s ∨ { i }
i= XH0+1
for any a [K] and s>H . With these notations in hand, it is straightforwardto derive
0
∈
H
regta-pm = ∆(a) t 1 apm =a . (70)
H h { h }
aX∈Tb h= XH0+1
Thissuggestsweneedto control H t 1 apm =a . Towardsthis,let usfix ana [K] suchthat
h=H0+1 h { h } ∈
r(a)<r⋆. Recall the definition of χ , which obeys
h
P
χ Bern(t ).
h h−1 h
|F ∼
Let us define a sequence of randomvariables X :=χ 1 apm =a t 1 apm =a , h 1. Straightfor-
h h { h }− h { h } ≥
ward calculation yields that
E X ]=0.
h h−1
|F
The sum of the conditional variances can be(cid:2) controlled by
H H H
E X2 ]= 1 apm =a t (1 t ) (1 t )
h |Fh−1 { h } h − h ≤ − h
h= XH0+1
(cid:2)
h= XH0+1 h= XH0+1
H
H +2K S
0 K
= −
h+1 K
h= XH0+1
H K S 1
K
(H +2) − dx
0
≤ K x+1
ZH0
K S H +1
K
(H +2) − log
0
≤ K H +1
(cid:18) 0 (cid:19)
2H log(H) c K4log2(H),
0 1
≤ ≤
26where we use (67) under the event in the second line, and c > 0 is some universal constant.
ts 1
Applying Lemma 1 by taking δ = H−E 1, R = 1, and σ = c K4log2(H), we find that with probability
1
1 H−1,
−
H H
χ 1 apm =a t 1 apm =a √3c K2log(H). (71)
h { h }≥ h { h }− 1
h= XH0+1 h= XH0+1
Moreover,let us define the event
h
Eta-pm :=
((cid:12) (cid:12)i= XH0+1
(cid:0)R i(a) −r(a) (cid:1)1 {aa ic =a
} (cid:12) (cid:12)≤
qN hac +1(a)log(H), ∀a ∈[K],h>H 0 ). (72)
(cid:12) (cid:12)
We claim that und(cid:12)er the event ta-pm, the following(cid:12)holds for all a [K] such that r(a)<r⋆:
E ∈
H
16log(H)
χ 1 apm =a . (73)
h { h }≤ ∆2(a)
h= XH0+1 (cid:24) (cid:25)
To see this, suppose that H χ 1 apm =a > 16log(H) for some a [K] satisfying r(a) <r⋆.
h=H0+1 h { h } ∆2(a) ∈
Then there exists an H′ H such that l m
≤P
H′
16log(H)
χ 1 apm =a = .
h { h } ∆2(a)
h= XH0+1 (cid:24) (cid:25)
Now, for any h H′, one knows from the definition of Nac(a) in (5) that
≥ h
H′
N hac +1(a) ≥N Hac ′+1(a)= χ h1 {aa hc =a
}
h= XH0+1
H′ H′
16log(H)
= χ 1 apm =a + (1 χ )1 aown =a , (74)
h { h } − h { h }≥ ∆2(a)
h= XH0+1 h= XH0+1 (cid:24) (cid:25)
where the second equality follows from the assumption on decision implementation deviations in (1).
On the other hand, note that the UCB estimator (6) is constructed basedon Nac(a). Under the event
h
ta-pm, one can derive
E
h
1
UCB (a)= R (a)1 aac =a
h Nac (a) i { i }
h+1 i= XH0+1
(i) log(H) (ii) 1
r(a)+ r(a)+ ∆(a)
≤ sN hac +1(a) ≤ 4
1 (iii) log(H)
<r(a⋆) ∆(a) r(a⋆)
− 4 ≤ −sN hac +1(a⋆)
h
(iv) 1
R (a⋆)1 aac =a⋆ =UCB (a⋆).
≤ Nac (a⋆) i { i } h
h+1 i= XH0+1
where (i) holds under the event (72); (ii) and (iii) are due to (74); (iv) arises from (72). By the arm
selection criterion of the UCB algorithm, this implies that apm =a for all h > H′. This further leads
h 6
to
H H′ H
16log(H)
χ 1 apm =a = χ 1 apm =a + χ 1 apm =a = ,
h { h } h { h } h { h } ∆2(a)
h= XH0+1 h= XH0+1 h X=H′ (cid:24) (cid:25)
27which contradicts the assumption. Therefore, this proves the claim (73).
Inaddition, recognizethat R (a) r(a) 1 aac =a is a sequence ofmartingaledifferences with
i − { i } i≥1
respect to the filtration ( ) . It is straightforwardto see that
Fi(cid:8)i(cid:0)≥0 (cid:1) (cid:9)
E R (a) r(a) 1 aac =a =0;
i − { i }|Fi−1
h h
(cid:2)(cid:0) (cid:1) (cid:3)
E R (a) r(a) 21 aac =a E 1 aac =a =Nac (a).
i − { i }|Fi−1 ≤ { i }|Fi−1 h+1
i=1 i=1
X (cid:2)(cid:0) (cid:1) (cid:3) X (cid:2) (cid:3)
We can then invoke the Azuma-Hoeffding inequality to obtain that for any fixed h H′, with proba-
≥
bility at least 1 2H−2,
−
h
R (a) r(a) 1 aac =a Nac (a)log(H).
i − { i } ≤ h+1
(cid:12) (cid:12)i= XH0+1 (cid:0) (cid:1) (cid:12) (cid:12) q
(cid:12) (cid:12)
Combined with the uni(cid:12)on bound, we find that (cid:12)
P c 2KH−1. (75)
Eta-pm
≤
(cid:8) (cid:9)
As a result, combining (71), (73), and (75) revealsthat with probability at least 1 O(KH−1), for all
−
a [K] such that r(a)<r⋆:
∈
H
log(H)
t 1 apm =a c +K2log(H) H, (76)
h { h }≤ 2 ∆2(a) ∧
h= XH0+1 (cid:18) (cid:19)
where c >0 is some numerical number. Plugging (76) back into (70) and taking ∆= Klog(H)/H,
2
we obtain with probability exceeding 1 O(KH−1),
− p
H H
regta-pm = ∆(a) t 1 apm =a + ∆(a) t 1 apm =a
H h { h } h { h }
a∈Tb :X∆(a)≤∆ h= XH0+1 a∈Tb :X∆(a)>∆ h= XH0+1
H
∆H + ∆(a) t 1 apm =a
≤ h { h }
a:∆X(a)>∆ h= XH0+1
log(H)
∆H +c ∆(a) +K2log(H)
≤ 2 ∆2(a)
a:∆X(a)>∆ (cid:18) (cid:19)
K
∆H +c log(H)+c K3log(H)
2 2
≤ ∆
2 c KHlog(H)+c K3log(H). (77)
2 2
≤
p
• It remains to control
regta-own.
By the trust bound in (67) under the event , we can show that
H Ets
H H
(i)
regta-own = (1 t )∆own (1 t )
H − h h ≤ − h
h= XH0+1 h= XH0+1
H
(ii) H +2K S
0 K
−
≤ h+1 K
h= XH0+1
H K S 1
K
(H +2) − dx
0
≤ K x+1
ZH0
K S H +1
K
(H +2) − log
0
≤ K H +1
(cid:18) 0 (cid:19)
282H log(H) c K4log2(H). (78)
0 1
≤ ≤
Here, (i) follows from ∆own 1 for all h 1; (ii) arises from (67); the last step follows from the choice
h ≤ ≥
of the round length m.
• Combining (68), (77), and (78) yields that with probability at least 1 O(KH−1),
−
reg .K4log(H)+ KHlog(H)+K3log(H)+K4log2(H)
H
KHlog(H)p+K4log2(H).
≍
p
Combining Stage 1 and Stage 2. Finally, we can bound
E[reg ]. KHlog(H)+K4log2(H)+KH−1H
H
pC KHlog(H)+C K4log2(H)
1 2
≤
for some constants C ,C independent of p H and K. This concludes the proof.
1 2
C Proof of Theorem 3
Fix an arbitrary admissible policy πpm. We will construct two trust-aware K-armed bandit instances and
use the superscripts (1) and (2) to distinguish the quantities associated with the first and second instances,
respectively.
Letusdenoteby := apm,aac,aown,R (aac),χ and := apm,aac,R (aac),χ . LetP(1)and
D h h h h h h i∈[H] D h h h h h i∈[H]
P(1) denotetheprobability(cid:0)distributionoftherando(cid:1)mvariablesin (cid:0)and inthefirstin(cid:1)stance,respectively.
The probability distributions P(2) and P(2) are defined similarle y foD r the sD econd instance. Also, we use E(1)
aend E(2) to denote the associated expectations. e
For the MABs, the random rewareds are chosen to be Bernoulli random variables where R (k)
h
∼
Bern r(k) for all k [K] and h 1. For the first instance, we let r(1)(1) = 1/2+∆ and r(1)(k) = 1/2
∈ ≥
for any k = 1, where 0 < ∆ 1/8 will be specified later. Moreover, let k = 1 be some arm such that
0
E(1)[(cid:0)
Nac
(cid:1)6
(k)] H/(K 1)
u≤
nder the policy πpm. As for the second
instanc6
e, we let r(2)(1) = 1/2+∆,
H+1 ≤ −
r(2)(k ) = 1/2+2∆, and r(2)(k) = 1/2 otherwise. Finally, the trust set and own policy πown are chosen
0
T
to be the same in the two instances.
With these definitions in hand, we can express the probability density p(1) of P(1) as
H
p(1)( )= p(1)(apm )p(1)(aown )p(1)(χ )p(1)(aac apm,aown,χ )p(1) R (aac) aac .
D h |Fi−1 h |Fi−1 h |Fi−1 h | h h h h h | h
i=1
Y (cid:0) (cid:1)
Since πpm is fixed and πown and are the same in the two instances, we have
T
dP(1) H p(1) R (aac) aac
log ( )= log h h | h .
dP(2) D p(2) R (aac) aac
i=1 (cid:0) h h | h(cid:1)
X
Taking the expectation with respect to P(1) yields (cid:0) (cid:1)
H p(1) R (aac) aac
KL P(1) P(2) =E(1) log h h | h
k "
i=1
p(2) (cid:0)R h(aa hc) |aa hc (cid:1)#
(cid:0) (cid:1) X
K (cid:0) (cid:1)
( =i) E(1)[Nac (k)]KL Bern r(1)(k) Bern r(2)(k)
H+1 k
k X=1 (cid:16) (cid:0) (cid:1) (cid:0) (cid:1)(cid:17)
( =ii)E(1)[Nac (k )]KL Bern r(1)(k ) Bern r(2)(k )
H+1 0 0 k 0
(iii) H (cid:16) (cid:0) (cid:1) (cid:0) (cid:1)(cid:17)
KL Bern(1/2) Bern(1/2+2∆)
≤ K 1 k
−
(cid:0) (cid:1)
29(iv) H
. ∆2.
K 1
−
Here, (i) follows from the divergence decomposition in Lattimore and Szepesvári (2020, Lemma 15.1); (ii)
and (iii) are due to the construction of the instances; (iv) follows from Lemma 2 below and ∆ 1/8.
≤
Lemma 2. For any a,b [0,1], let Bern(a) and Bern(b) denote two Bernoulli distributions with parameters
∈
a and b, respectively. Then one has
(a b)2
KL(Bern(a) Bern(b)) − . (79)
k ≤ b(1 b)
−
In addition, if b 1/2 1/4, then one further has
| − |≤
KL(Bern(a) Bern(b)) 8(a b)2. (80)
k ≤ −
Consequently, by choosing ∆ K/H, we obtain
≍
p H∆2
KL P(1) P(2) .1. (81)
k ≤ K 1
−
(cid:0) (cid:1)
Moreover,from the data processing inequality (Cover, 1999), we can further control
KL P(1) P(2) KL P(1) P(2) .1. (82)
k ≤ k
Therefore, applying the standard redu(cid:0)cetion scehem(cid:1)e (see(cid:0)e.g., Tsyba(cid:1)kov (2008)), we conclude that
inf sup E[reg (π)]&H∆exp KL P(1) P(2) &√KH.
π Π(K,T,πown) H − k
(cid:16) (cid:0) (cid:1)(cid:17)
e e
30