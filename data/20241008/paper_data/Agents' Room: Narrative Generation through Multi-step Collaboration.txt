UnderreviewasaconferencepaperatICLR2025
AGENTS’ ROOM: NARRATIVE GENERATION THROUGH
MULTI-STEP COLLABORATION
FantineHuot, ReinaldKimAmplayo, JennimariaPalomaki, AliceShoshanaJakobovits,
ElizabethClark & MirellaLapata
GoogleDeepMind
{fantinehuot,reinald,jpalomaki,jakobovits,eaclark,lapata}@google.com
ABSTRACT
Writingcompellingfictionisamultifacetedprocesscombiningelementssuchas
crafting a plot, developing interesting characters, and using evocative language.
While large language models (LLMs) show promise for story writing, they cur-
rently rely heavily on intricate prompting, which limits their use. We propose
AGENTS’ ROOM, a generation framework inspired by narrative theory, that de-
composesnarrativewritingintosubtaskstackledbyspecializedagents. Toillus-
trateourmethod,weintroduceTELLMEASTORY1,ahigh-qualitydatasetofcom-
plexwritingpromptsandhuman-writtenstories,andanovelevaluationframework
designedspecificallyforassessinglongnarratives.WeshowthatAGENTS’ROOM
generates stories that are preferred by expert evaluators over those produced by
baselinesystemsbyleveragingcollaborationandspecializationtodecomposethe
complexstorywritingtaskintotractablecomponents. Weprovideextensiveanal-
ysiswithautomatedandhuman-basedmetricsofthegeneratedoutput.
1 INTRODUCTION
Creating long-form content requires meticulous research, advanced planning, an engaging writing
style,andtheabilitytocraftstoriesthatcaptivate. J.K.Rowlingisclaimedtohavehadmostofthe
Harry Potter story planned out before she started writing. She knew there would be seven books,
whichcharacterswouldbeimportantandhowtheywoulddevelop,andwhichkeyplottwistswould
servetheoverallstory.Inaddition,shecarriedoutsubstantialresearchtocreatethefictionaluniverse
whichprovidesthebackdropofthestory. Breakingdownastoryintodistinctsectionsistypicalfor
longer narratives, with most stories boiling down to a few shared elements like exposition, rising
action,climax,fallingaction,andresolution(Freytag,1896;Pavis,1998). Practicalguidestowrit-
ing successful screenplays (Cutting, 2016; Hauge, 2017) outline a similar structure, following the
setup, the new situation, progress, complications and higher stakes, the final push, and the after-
math. Academicpapers(Schimel,2012)followacommonformatwhichcanbedistilledintotitle,
abstract,introduction,mainbody,conclusions,andreferences,withwell-knownconventionsabout
thecontentsofeachsection(e.g., theintroductionshouldconveytheessenceoftheauthor’sargu-
mentoridea). Writingnewsarticles(Ricketson&Graham,2017)alsoinvolvesstandardsubtasks
likearesearchphase(gatheringmaterial),anoutlinephase(planningthekeypointsineachsection),
andawritingphase.
Large language models (LLMs) have demonstrated impressive writing capabilities (Yang et al.,
2022; Nur Fitria, 2023; Shao et al., 2024; Bai et al., 2024), however, generating long-form con-
tent is still a challenge. Well-known problems include maintaining a consistent narrative, tone, or
factualaccuracyoverextendedstretchesoftext(Chakrabartyetal.,2024b;Wangetal.,2023b;Al-
abdulkarimetal.,2021;Balepuretal.,2023;Yamshchikov&Tikhonov,2023), andshowcasinga
uniquevoiceorhumorthatmakeswritingtrulymemorable. Despitedisplayingflashesofcreativity,
they often replicate patterns found in their training data, which hinders the generation of original
concepts, plotlines, or phrasing. Added problems include the lack of datasets or benchmarks for
long-formwriting(Baietal.,2024)andstandardizedevaluationcriteriaforassessingcreativewrit-
ingeitherbyhumansormachines(Chhunetal.,2022;2024;Chakrabartyetal.,2024a).
1Wereleasethedatasetat:https://github.com/google-deepmind/tell-me-a-story
1
4202
tcO
3
]LC.sc[
1v30620.0142:viXraUnderreviewasaconferencepaperatICLR2025
Scratchpad
Input Orchestrator Output
Planning Agents Writing Agents
Figure1: AGENTS’ROOM,amulti-agentframeworkforcollaborativewriting.Acentralorchestra-
torcallstheindividualagentsandconsolidatestheircontributionsintothescratchpad.Wecolor-code
eachpieceofthescratchpadwiththecontributingagent’scolor.
Existingmethodsoftenrelyondetailedpromptstoguidethegenerationprocess(Yangetal.,2022;
Xieetal.,2023),promptchaining(Mirowskietal.,2023;Yangetal.,2022),andplanningstrategies
(Yang et al., 2023; Lee et al., 2024) as a means of breaking down the complex writing task into
moremanageablecomponents. Inthispaper,weconceptualizelong-formwritingasamulti-agent
collaborationproblem. Ratherthanattemptingadecompositionofthewritingtaskwithinasingle
agent(Chenetal.,2023;Yaoetal.,2024), weleveragecollaborationamongmultipleagents, with
specializedabilities(Talebirad&Nadiri,2023;Zhangetal.,2024b;Hanetal.,2024). Wepropose
AGENTS’ ROOM2 (Figure1),agenerationparadigmwhichconsistsoftwotypesofagents,namely
planning and writing agents. Planning agents flesh out key components of the content but do not
write the story as such. For example, a planning agent might specialize in character descriptions,
whereas another might focus on the plot or central conflict. Writing agents are responsible for
generating the final output text and are also specialized, e.g., one may focus on the introduction,
and another on the conclusions. The two types of agents work collaboratively to complete the
writingtask,sharingandmanaginginformationthroughascratchpadwhichmaintainsoutputsfrom
planning agents and makes them available to writing agents. An orchestrator is responsible for
callingtheagentsinorderdependingonthetaskathand.
ComparedtosingleLLM-poweredagents,thismulti-agentapproachoffersseveraladvantages:
• LLMs can be specialized into various distinct agents (e.g., zero-shot prompted or fine-
tuned)performingasinglefunctionwithhighprecision;
• itavoidswell-knownproblemswithlengthyandunder-specifiedinstructionswhichrequire
multipleiterationstobuildcontextandfullydefineanappropriatesolution;
• it can be applied to problems whose solution is not known beforehand, and results from
exploringavastresearchspaceorinvolvesverylongoutput(e.g.,writingabook);
• itnaturallylendsitselftohuman-in-theloopautomationwheremachine-basedagentscan
bereplacedwithhumanoneswhenneeded.
We formalize AGENTS’ ROOM as a general writing framework and apply it to creative writing.
Specifically, we focus on writing long-form stories (1,000-2,000 tokens), and create specialized
agentsdrawinginspirationfromnarrativetheory(e.g.,Card1999;Noble1999;Pavis1998).Toeval-
uateourmethod,weintroduceTELLMEASTORY,anewdatasetofhuman-createdwritingprompts
andfictionstories,andanovelevaluationframeworkdesignedforassessingmultipledimensionsof
story quality. Experimental results show that AGENTS’ ROOM generates stories that are preferred
(byhumansandautomaticmetrics)overthoseproducedbybaselinesystemswhichdonotleverage
collaborationorspecialization.
2AGENTS’ROOMisverylooselymodeledafterwriters’room,acollaborativespacewherewriters,(usually
ofatelevisionseries),cometogethertowriteandrefinescripts.
2UnderreviewasaconferencepaperatICLR2025
2 RELATED WORK
Story Generation The advent of large pre-trained language models has provided a common
frameworkforgeneratingstorieswhichsoundfluentbutoftenstrugglewithmaintainingcoherence
and plausibility. Attempts to enhance coherence and control the trajectory of events often decom-
posethegenerationtaskintoplanninganoutlineorsketch,andthenelaboratingonit,e.g.,byfilling
indescriptionsandspecificdetailsofeachstory. Examplesofintermediateplansincludesequences
of entities and their actions (Yao et al., 2019), outlines (Fan et al., 2019; Zhou et al., 2023; Wang
et al., 2023a), plot structures (Goldfarb-Tarrant et al., 2020), and more elaborate descriptions in-
cludingdetailsaboutthesettingofthestory,itscharacters,andmainplotpoints(Yangetal.,2022;
2023). Otherworkusescommonsenseknowledgetoimposeconstraintsonthecharactersandtheir
interactions (Peng et al., 2022), ensemble-based models to render event sequences more plausible
(Ammanabroluetal.,2020),stylisticconstraints(Kongetal.,2021),andtwiststhroughconstrained
decoding(Huangetal.,2023). Theseeffortshavedemonstratedthatgeneratingstoriesasaone-step
process is challenging, and ultimately various interventions are required to improve overall story
quality. Ourworkfollowsonfromthisrealization,andbreaksdownthewritingtaskintosubtasks,
undertakenbydifferentagentswhocollaborativelyplanandwriteastory. Collaborativewritingis
oftenusedinacademicorprofessionalcontextstoleveragethestrengthsandperspectivesofvarious
contributors,andhasalsobeenshowntoenhancecreativity(Barrettetal.,2021).
UsingLLMsastoolsto assisthumanswithwritingstoriesisanactiveresearch area(Chakrabarty
etal.,2024b;Mirowskietal.,2023;Ippolitoetal.,2022). Inourexperiments,storiesarewrittenex-
clusivelybymodelswithouthumansintheloop. However,ourframeworkisfairlygeneralallowing
forhuman-machinecollaborationatvariousstagesofcontentcreation.
Multi-agentSystems LLM-basedagentshaverecentlyshownrobustreasoningandplanningca-
pabilitiesacrossvariousapplicationdomains(Zhaoetal.,2023;Bubecketal.,2023). Multi-agent
systems involve multiple independent LLMs working together to solve complex tasks that are be-
yondthecapabilityofanyindividualagent(Talebirad&Nadiri,2023;Parketal.,2023;Hanetal.,
2024; Guo et al., 2024). The agents are typically specialized in different aspects of a problem or
havedifferentroles, allowingthesystemtoapproachtasksinamorecoordinated, distributed, and
modular way. LLM-based multi-agent systems have recently demonstrated promising results in
multiple areas including software development (Hong et al., 2024), robotic tasks such as motion
planning(Mandietal.,2024),simulationsofhumanbehavior(Parketal.,2023;Huaetal.,2024),
thecreationofgamingenviroments(Huetal.,2024),recommendersystems(Zhangetal.,2024a),
simulations of financial trading (Li et al., 2023), and policy making (Xiao et al., 2023). We are
not aware of existing multi-agent frameworks for long-form writing. We draw inspiration from
relatedworkdemonstratingthatcollaborativeproblem-solvingimprovesLLMtask-solvingcapabil-
ities(Haoetal.,2023;Wangetal.,2024;Zhangetal.,2024b). Ouragentseachadoptaspecialized
writingsubtaskandcommunicatethroughasharedscratchpad(ormemory)whichallowstoeffec-
tivelyrecallandutilizecontextually-relevantpastknowledge. Inourexperiments,wepredefinethe
number and type of agents best suited to our story writing task, rather than dynamically generate
agentsbasedonstorycontent(Chenetal.,2024).
Evaluation Storyevaluationisadmittedlyachallengingtaskforhumansandmachines. Human
evaluation is usually considered as the gold standard, but it is expensive, time-consuming (Guan
& Huang, 2020), and can be subjective. It also cannot capture diversity since a model that copies
directlyfromthetrainingsetwouldpotentiallypassthehumanqualitybarwithoutdisplayingany
generalizationorcreativity(Hashimotoetal.,2019). Automatedevaluationmetricsbasedonlexi-
caloverlaporsemanticsimilaritybetweengeneratedstoriesandtheirhumanreferenceshavebeen
showntocorrelatepoorlywithhumanjudgements(Chhunetal.,2022). Inthispaper,weintroduce
an LLM-based evaluator (Liusie et al., 2023; Liu et al., 2024; Zheng et al., 2024; Bohnet et al.,
2024)toperformside-by-sidecomparisonsofsystemoutputswhichcorrelateswithhumanjudge-
ments. Inspiredbyrecentproposalsonhowtoassesshumancreativity(Chakrabartyetal.,2024a),
we distill the story evaluation task into a few dimensions (e.g., plot, language use) which humans
andmachinescanjudgereliably.
3UnderreviewasaconferencepaperatICLR2025
Algorithm1AGENTS’ROOMframework
s←x ▷Initializethescratchpad
whileo(s,A)==Trueandt<T do ▷Whiletheorchestratorassignsanextagent
a =o(s,A)
t
y =a (s)
t t
s←(s;(l ,y ))
t t
iftype(a )==writingthen ▷Iftheagentisawritingagent,writetothefinaloutput
t
y ←(y;y )
t
endif
endwhile
returny ▷Returnthefinaloutput
3 AGENTS’ ROOM
Inthissection,weformalize AGENTS’ ROOM,theproposedmulti-agentframeworkforcollabora-
tive writing. Given a complex writing task x, we generate output y, by decomposing the writing
processintomultiplesubtaskstackledbyspecializedagents. ThefullAGENTS’ ROOMframework
issummarizedinAlgorithm1andexplainedbelow.
Agents Wedefineanagenta ∈ Aasaspecializedmodelthattakestextasinputandreturnstext
as output, specified by a unique identifier label l and a mapping f : V∗ → V∗. Each agent is
specialized in a specific subtask. Under this definition, an agent can be a LLM fine-tuned for the
subtask, a zero-shot prompted LLM with a specific input prompt, a deterministic text processing
function(e.g.,stringformattingandparsing),orevenahumaninteractingwiththesystem. Herein,
we focus on LLM-based agents, but we formalize the general framework’s modeling assumptions
(e.g.,agentinputsandoutputsastextinsteadoflatentvariables)toallowfutureworktoincorporate
human agents as well (e.g., by iteratively editing LLM-generated text). We define two types of
agents(seebelow),namelyplanningandwritingagents,whichdifferbothinfunctionandintheir
modeofinteractionwiththegeneratedoutput.
Multi-agentCommunication Communicationbetweenagentsiscriticalforthesuccessfulcom-
pletionoftheirtasks. Whiledifferentformsofcommunicationarepossible, suchasdebate(Khan
et al., 2024; Zhang et al., 2024b) or competition (Cheng et al., 2024), in this work we focus on
collaborativecommunicationsinceitwouldtransfermostnaturallytohuman-LLMcollaborations.
Collaborative agents work together towards a shared goal, exchanging information to enhance a
collectivesolution.
Scratchpad The overall system requires a mechanism for sharing and managing information
across the different agents. We assume our agents have access to a shared scratchpad s ∈ V∗
that maintains individual agents’ outputs and is passed along to the next agent. The scratchpad is
initializedwiththeinitialwritingpromptxandisthenupdatedaftereachagentcall. Ateachstept,
thecurrentagenta takesasinputthecurrentscratchpads andgeneratesoutputy .Ateachstep,the
t t t
scratchpadisupdatedwiththeagent’suniqueidentifierandoutputsuchthats ← (s ;(l ,y )).
t+1 t t t
Weincludetheagent’slabelsothatindividualagentscaneasilyreferenceandparsespecificportions
of the scratchpad to complete their subtask. Note that in this framework, the scratchpad does not
containthespecificinputpromptofagivenLLMagent. Indeed,itisconsideredpartofeachagent’s
subtasktoprocesstheoutputy intoasuitableformattobeusedbyotheragents. Sinceagentshave
t
accesstothescratchpad,thismeansthattheycanavoidwritingredundantandduplicateinformation.
Orchestrator We have opted for a centralized architecture, where a central orchestrator deter-
minestheorderuponwhichindividualagentsarecalled,andwhethercallingoneachagentisnec-
essary (e.g., depending on the task). Given a scratchpad s and a set of available agents A, the
t
orchestrator o : V∗ ×A∗ → A determines the next agent a to call. It can be modeled as a
t+1
Markovprocess, sinceeachstepdependsentirelyonthestateofthescratchpadatstept. Thisor-
chestratorcanbeadiscretedeterministicprocess,canhavelearnttransitionprobabilities,orcanbe
arbitrarilycomplex. Theorchestratordeterminesthestoppingcondition,i.e,whenthereisnomore
agenttocall,orwhenamaximumnumberofstepsT hasbeenreached.
4UnderreviewasaconferencepaperatICLR2025
Planning Agents Previous work (see Section 2) shows that LLMs benefit from an intermediate
planning stage before generating the final output. These intermediate steps improve the overall
outputbutarenotincludedinthefinaloutput. Wedefineplanningagentsasagentsthatspecialize
in generating these intermediate steps and write exclusively to the scratchpad. For instance, when
writing a story, planning agents can draft character descriptions and plot elements; when writing
anessay,theycanoutlinetheargumentativestructureandretrievereferencestosubstantiateclaims.
Sincetheiroutputsareintextformat, ahuman-in-the-loopcouldrevieworedittheseintermediate
stagestoguidethegenerativeprocess.
WritingAgents Certaincomplextasks,suchasgeneratingparticularlylongoutputsorwithsec-
tionswrittenindifferentstyles,remainchallengingforaLLMtogenerateinonego. Insuchcases,
the final output benefits from being generated section by section through separate agent calls. We
define writing agents as agents specializing in writing specific parts of the final output. In addi-
tiontowritingtothescratchpad, thesewritingagentsiterativelywritepiecesofthefinaloutputy.
Therefore, thefinaloutputcanbeformalizedastheconcatenationoftheoutputsofallthewriting
agents. Forstorywriting,writingagentscanspecializeinspecificpartsofthenarrativearc,suchas
theexpositionortheclimax;whenwritinganessay,theycaneachtackledifferentsections,suchas
theargumentsinfavorversusagainst.
4 FICTION WRITING TASK
In this section, we present an instantiation of the AGENTS’ ROOM framework for fiction writing:
given an initial writing prompt x, generate narrative y. We also introduce TELLMEASTORY1, a
newdatasetofhigh-qualityhuman-createdwritingpromptsandfictionstories.
4.1 SPECIALIZEDAGENTSINSPIREDBYNARRATIVETHEORY
Wedesignspecializedagentsforourfictionwritingtaskbydrawinginspirationfromnarrativethe-
ory. Wedesignfourplanningagents,eachspecializedinaspecificaspectofstoryplanning: [CON-
FLICT] defines the central conflict (e.g., a young boy has to fight an evil wizard who killed his
parents),[CHARACTER]developsthecharacter(s)(e.g.,theyoungmanisbrave,caring,determined,
loyaltohisfriendswithastrongmoralcompass),[SETTING]developsthesetting(e.g.,mostofthe
storytakesplacesintheHogwardsSchoolofWitchcraftandWizardry,afictionalboardingschool
ofmagicforyoungwizards),and[PLOT]outlinestheplotelements(e.g.,theboydiscoversheisthe
sonoffamouswizardsandwillattendHogwartsSchoolofWitchcraftandWizardry). Theseplan-
ningagentstargetspecificweaknessesobservedinLLM-generatedstories. Indeed,LLMsstruggle
writingcompellingplotsandconsistentcharactersthroughoutlongstories(seeSection2).
Inadditiontothesecontentplanningagents,wedesignfivewritingagents,eachspecializedindis-
tinctelementsofatypicalstorystructure: [EXPOSITION],[RISINGACTION],[CLIMAX],[FALLING
ACTION], and [RESOLUTION]. We adopt this structure since it is widely used and quite versatile
(Freytag,1896;Pavis,1998),leavingothernarrativestructuresforfuturework. Whengeneratingin
onego,LLMsstruggleatmeetingthelengthrequirements,resultinginstoriesthataregenerallytoo
short (see Section 7). These writing agents decompose the writing of the final output into writing
sectionbysection,thusresultinginlongeroutputs.
WemodeleachoftheseagentsasanLLMwithaspecificprompttemplatethatformatsthescratch-
padintoanappropriatepromptforeachagent’ssubtask. Detailedscratchpadformattingandprompt
templates for each agent are provided in Appendix B. To coordinate between the different agents,
wedefineadeterministicorchestratorthatfirstcallstheplanningagentsasfollows: [CONFLICT]→
[CHARACTER]→[SETTING]→[PLOT],thencallsthewritingagents: [EXPOSITION]→[RISING
ACTION]→[CLIMAX]→[FALLINGACTION]→[RESOLUTION].Wechoosetouseadeterministic
orchestratorforsimplicity,giventhestrongnarrativetheoryprior. Infuturework,morerefinedor-
chestratorswithlearnedobjectivescanbeexplored,possiblyexpandingtoawiderrangeofnarrative
structures. Asafirststeptowardsbuildingadequaterewardmodelsfortrainingsuchorchestrators,
weinvestigateautomatedevaluationstrategiesforthelong-formfictionwritingtaskinSection6.
5UnderreviewasaconferencepaperatICLR2025
ExamplePrompts
Writeastoryaboutamythologicalcharactermeetingsomeonefromthepresent.Thestoryshouldbebetween1,300and1,350
wordsandshouldbefunny.ThemaincharactershouldbePoseidon.Heandhiswifearetravelingtoabeachresort.Thereshould
beafamilyattheresort,andthefatherannoysPoseidon.Poseidonuseshispowerstogetrevengeontheman.Attheend,
Poseidonshouldcreateastorm.Theendingishappy.
Writeashortstoryaboutamythorlegendbeingfulfilledinanunusualorunexpectedway,orevencleverlybypassed.Thestory
shouldbebetween1,200and1,300words.ItshouldtakeplaceduringtheCovid-19pandemicinEurope.Thenarrativeshould
involvevampires.Thegenreshouldbecomedichorror.Thestoryshouldcontainvisualsofamedievalcastleandofamodern
Europeantown.Itshouldoccuratnight.Thenarrativeshouldalsocontainsituationswhereonecharacterdoesnotunderstandthe
othercharacter’sperspective.Thestoryshouldendwiththevampiressurvivingtopreyonhumanvictimsinthenearfuture.
Writeastoryaboutastrangercomingtoasmalltownandshakinguptheorderofthings.Thestoryshouldbeabout950wordsand
beasciencefictionstory.Thestoryshouldbeframedwiththreeoldmengossipingaboutthestranger.Thestoryshouldbeinthe
thirdpersonpoint-of-view.Thestrangerisfoundwanderinginaruraltownandistakentoaverysmallhospital.Adoctoriscalled
intotreathim.Thestrangershouldrecognizethedoctorasanalien.Thedoctortellsthepatientaboutthealiens’conspiracyto
infiltrateearth.Thereshouldalsobesubtlehintsthatoneoftheoldmenisanalien.Theendingshouldbescary.
Figure2: PromptsfromtheTELLMEASTORYdataset(correspondingstoriesareinAppendixA).
4.2 SYNTHETICDATAGENERATIONFORAGENTTRAINING
Foreachspecializedagent,weexplorebothzero-shotpromptedandfine-tunedapproaches,sincea
different degree of subtask specialization can be achieved through each approach. The fine-tuned
approachrequiresagentoutputs,whicharenotreadilyavailable;planningagentoutputssuchasplot
andsettingareusuallynotprovidedindatasets,whilewritingagentoutputsrequirethestoriestobe
splitintoitsconstituentparts. Similartopreviouswork(Schicketal.,2022;Narayanetal.,2023;
Josifoski et al., 2023), we propose to generate synthetic outputs for these agents through distilled
backtranslation.
Specifically, given as input writing prompts and stories from a dataset (see Section 4.3), we zero-
shot prompt a larger teacher LLM to (1) generate the planning agent outputs, and (2) segment the
storyintodistinctparts(e.g.,exposition,climax). Notethatunliketypicaldistillationmethods,our
task is more straightforward; all that is required is to reverse engineer the agent outputs from an
existing story rather than generate them from scratch. The teacher LLM outputs are then used to
generatesynthetictrainingdatasetsforbothplanningandwritingagents. Detailedprompttemplates
areprovidedinAppendixC.
4.3 TELLMEASTORYDATASET
Creativewritingpresentsaparticularchallengefromadatacollectionperspective; itisnotakinto
anytraditionalannotationorevaluationtaskwherearequesterprovidessomeinputandsomesetof
guidelinesformarkingupthatinputinaconsistentmanner.
Whilestandardsexistfor“good”writing,theyevaluatethequalityofwritingacrossmultipleinter-
dependent and independent dimensions, all at once. In addition to this, the skill of writing really
representsseveralskillsthatarelearnedoverthecourseofaperson’slifetimeandeducationalexpe-
rience.
Furthermore,evaluatingwritingnecessarilyrequiresthesubjectivestanceoftheevaluator.
Takingintoconsiderationallofthesecomplexities,wecollectedTELLMEASTORYthroughwriting
workshops to replicate the organic environment in which a collaborative writing process can take
place. We provided a group of writers (28 in total) with broad instructions for quality based on
collationofthePublicationManualoftheAmericanPsychologicalAssociation(currentlyinits7th
edition),theGRE/GMATwritingassessmentrubrics,andvariousmassmarketstyleguides.
Writerscreatedtheirownprompts,wroteaninitialdraft,receivedfeedbackfrompeers,revised,and
then submitted to a workshop lead for a second round of feedback and final approval. Workshop
leads could ask for additional edits or accept as is. Workshops lasted on average 3-4 weeks. The
averagerateofproductionforworkshopsgenerallyreachednomorethan2–3writingsamplesper
writerperweek. WeprovideexamplepromptsinFigure2andexamplestoriesinAppendixA.
6UnderreviewasaconferencepaperatICLR2025
Table 1: Comparison of TELLMEASTORY against existing open-ended story generation bench-
marks. Wereportstatisticsonthenumberoftraining,validation,andtestinginstances;Input/Target
denotetheaveragenumberoftokensintheinput(akaprompt)andtargettext.
numberofexamples avg. tokenlength
Dataset Training Validation Testing Input Target
WRITINGPROMPTS(Fanetal.,2019) 272,600 15,620 15,138 28 735
ROCSTORIES(Mostafazadehetal.,2016) 176,688 9,816 4,909 9 41
CHANGEMYVIEW(Huaetal.,2019) 42,462 6,480 7,562 18 104
WIKIPLOTS3 69,288 8,661 8,662 4 195
TELLMEASTORY 123 52 55 113 1,498
Table 1 compares TELLMEASTORY against commonly used story generation benchmarks. Our
datasetissmallinscaleandthusnotsuitedtotrainingamodelfromscratch. Ourpromptsaremore
detailedcomparedtootherbenchmarks(seeInputcolumn)andthetargetstoriesaregenuinelylong
(e.g, double in size compared to WRITINGPROMPTS). Note that some of these datasets, although
usefulforsystemdevelopment, arenotstrictlyspeakingnarratives. WIKIPLOTS isacollectionof
plotsfromWikipediaratherthanstories,ROCSTORIESarefive-sentencelongcommonsensestories,
andCHANGEMYVIEWcontainspairsofpostsandtheircounter-arguments.
5 EXPERIMENTAL SETUP
We describe details on the implementation of our framework and explore a number of AGENTS’
ROOMvariants. Wealsocompareoursystemagainstbaselineapproaches.
End-to-End Baselines The state-of-the-art approach for generating narratives consists of gen-
erating the story in one go. This end-to-end approach can be achieved either through zero-shot
promptingorthroughfine-tuning,whichwedenoteasE2E andE2E ,respectively.
ZS FT
AGENTS’ ROOM Variants We use the plan+write tag to denote the AGENTS’ ROOM variant
with the writing and planning agents as previously described (see Section 4.1). To explore trade-
offs between the different types of agents, we investigate two additional variants, plan and write,
where we use only planning, or only writing agents, respectively. In the specific case of the plan
variantwithonlyplanningagents,westillneedawritingagenttofinalizethestory,sinceplanning
alone does not result in a final story. Therefore, the plan variant includes a single simple writing
agent, which we denote as the [FINALIZER]. The prompt template for the [FINALIZER] agent is
providedinAppendixB.Weinvestigatebothzero-shotandfine-tunedagents. Foreach AGENTS’
ROOM variant, we explore two settings, one with only zero-shot agents, and one with only fine-
tunedagents,denotedas AR and AR ,respectively. Sinceagentsarecalledindependently,it
ZS FT
ispossibletomixandmatchbetweenzero-shotandfine-tunedagents,butwekeepthetwosettings
separatetoderiveclearersignalforeachapproach.
Implementation ForallE2EbaselinesandAGENTS’ ROOMagents,weuseaGemini1.5Flash4
backbone,alightweightandcost-efficientmodelthathasdemonstratedgoodperformanceacrossa
widerangeofgenerativetasks(Reidetal.,2024). Inparticular,itfeatureslongcontextcapabilities
(uptoone-milliontokens)whichmakesitsuitableforhandlingthescratchpadwithmultipleagents’
contributions. Weuseaninputtokenlengthoutof{1,024, 2,048, 4,096, 8,192}dependingonthe
length of the scratchpad and a target token length of 4,096. While the outputs generated by the
E2E systems are generally shorter than what is requested in the original prompt (see Section 7),
we observe no improvements when increasing the target token length. We hypothesize that the
observedlimitsonoutputlengthsarelikelyduetothebackbonemodelbeingtrainedondatawith
mostlyshorteroutputs.
3Availableat:https://github.com/markriedl/WikiPlots
4Availableat:https://cloud.google.com/apis
7UnderreviewasaconferencepaperatICLR2025
For the synthetic training data generation described in Section 4.2, we use Gemini Ultra4 (Team
etal.,2023)astheteachermodel. Giventhatourdatasetonlycontainsasmallnumberoftraining
examples, wefine-tuneourmodels(E2E andindividualagentsfor AR )usingLoRA-tuning
FT FT
(Hu et al., 2021), a computationally-efficient approach that updates only a small portion of the
model weights. We perform LoRA-tuning with rank 4 and a learning rate of 1−6 (picked after a
hyperparametersearchthrough{1−4, 1−5, 1−6, 1−7}). WeLoRA-tunefor250stepswithabatch
sizeof16,savingcheckpointsevery20steps. Wethenselectthecheckpointwithlowestlossonthe
validationset.
6 EVALUATION
Weevaluatethequalityofthegeneratedoutputsalongseveraldimensionsthroughhumanjudgment
elicitationandautomatedevaluationmethods.
6.1 HUMANEVALUATION
Weevaluatesystemoutputbysolicitingpairwisepreferences(Louviere&Woodworth,1990)along
fourdimensions,aswellasanoverallpreference. Wedistillpreviousproposals(Chakrabartyetal.,
2024b;Chhunetal.,2022)onhowtoevaluatecreativewritingintothefollowingcriteria:
• Plot — Does the story have a recognizable structure, e.g., with a connected beginning,
middle, and end? Does it exhibit events and turns that move the plot forward without
logicalorconceptualinconsistencies?
• Creativity — Does the story have engaging characters, themes, and imagery? Does it
avoidoverlyclichedcharactersandstorylines,unintentionaltropes,andstereotypes? Does
itincludeoriginalelementsthatwerenotexplicitlymentionedintheprompt?
• Development — Are the characters and settings contextualized with relevant details that
allowthereadertounderstandtheirplaceinthestory? Areappropriatelevelsofdetailand
complexityprovidedtolendthestoryafeelingofrealnessandbelievability?
• Language Use — Does the language used feel varied and rich? Does the story exhibit
rhetorical,linguisticandliterarydevicestocreateinterestingeffects? Doesitavoidbland
orrepetitivephrases?
The full instructions are reproduced in Appendix D. Participants are shown two stories and asked
to decide which one is better in each dimension. They can also respond that the two stories are
about the same. We recruited annotators who were writers or had a degree in related disciplines
(e.g., literature). Participants are allowed to rate up to five samples in one sitting, due to our task
beingcognitivelytaxingandtime-consuming. WeassignsamplestoparticipantsfollowingaLatin
Square design, such that each participant does not rate the same writing prompt more than once.
We randomize the order in which the two stories are shown to mitigate presentation order as a
potential bias. We gather ratings for all examples included in the TELLMEASTORY test set and
compare outputs from all E2E and AGENTS’ ROOM variants (see Figure 3); we also include the
human-writtenstoriesasanupperbound. Weobtainedatotalof9,900pairwiseratingswhichwe
convertedintosystems’relativestrengthsusingaBradley-Terrymodel(Bradley&Terry1952;see
Section6.2). Inter-annotatoragreementwasκ=0.46(p<0.01,N =150,k =3),asmeasuredby
Fleiss’Kappa,whichweinterprettobesatisfactorygiventhesubjectivityofourtask.
6.2 AUTOMATICEVALUATION
Manypreviousstudies(seeYang&Jin2024andthereferencestherein)havehighlightedthechal-
lengesassociatedwithevaluatingnarrativesautomatically. Metricsbasedonlexicalmatchingcorre-
latepoorlywithhumanjudgments(Chhunetal.,2022;Chakrabartyetal.,2024a)anddonoteffec-
tively measure story quality (e.g., is the story original and well-written with plausible characters).
In this work, we report reference-based metrics, specifically Rouge-L (Lin, 2004) and BertScore
(Zhang et al., 2020), but also adopt several surface-based metrics intended to capture differences
betweenhumanwritingandLLM-generatedstories. Specifically,wecomputestorylengthtodeter-
minewhethermodelsareabletogeneratelongstoriesandquantifystructuraldifferencesbetween
8UnderreviewasaconferencepaperatICLR2025
Table 2: Comparison between human and model generated stories using automatic metrics
(TELLMEASTORY testset): #words(averagenumberofwordsperstory),#para(averagenumber
ofparagraphsperstory),Article(percentageofsentencesstartingwithanarticle),Pro(percentage
of sentences starting with a pronoun), Unique (percentage of unique words), Intra (intra-story tri-
gramrepetition),Inter(inter-storytrigramrepetition),Overlap(proportionoftrigramsoverlapping
with the prompt). We also report two reference-based metrics, Rouge-L and BertScore. AR is a
shorthand for AGENTS’ ROOM systems; subscripts
ZS
and
FT
respectively refer to zero-shot and
fine-tunedsystems.
Models #words #paraArticle Pro Unique Intra Inter OverlapRougeBertSc
Humans 1,439 32.91 10.01 32.37 50.35 15.53 19.24 .0020 — —
E2E 1,207 32.24 12.74 40.45 44.57 28.78 33.35 .0034 20.71 .8152
ZS
E2E 1,193 32.25 12.58 43.39 44.02 28.21 31.31 .0036 20.73 .8138
FT
AR plan 926 20.95 13.82 40.68 43.88 29.70 33.49 .0017 19.58 .8119
ZS
AR write 3,278 63.80 25.32 39.91 34.97 47.50 44.09 .0022 17.34 .8103
ZS
AR plan+write 3,034 58.65 15.97 41.43 35.05 44.73 43.25 .0022 17.57 .8123
ZS
AR plan 856 21.05 18.02 39.29 44.65 23.85 28.05 .0027 19.24 .8146
FT
AR write 3,129 61.90 17.45 44.80 36.35 46.39 42.39 .0021 17.53 .8150
FT
AR plan+write 3,006 56.85 17.52 43.03 34.30 46.31 41.60 .0019 17.60 .8152
FT
human and machine stories (e.g., number of sentences starting with an article or a pronoun). We
alsomeasuretheratioofuniquewordsinastorywhichgivesanideaofcreativelanguageuse,and
intra-andinter-storytrigramrepetition(Yaoetal.,2019;Goldfarb-Tarrantetal.,2020)whichcap-
turediversitywithinastoryandacrossstories(highinter-storyrepetitionsuggestsmodelsgenerate
similarstoriesevenwhengivendifferentprompts). Finally,trigramoverlapwiththepromptisused
toindicatewhethermodelscancreativelyelaborateontheinformationprovided.
Inaddition, wedevelopaLLM-basedevaluator(Liusieetal.,2023;Liuetal.,2024;Zhengetal.,
2024;Bohnetetal.,2024)toperformside-by-sidecomparisonsofsystemoutput.Wedesignprompts
targetingthesamedimensionsofstoryqualityadoptedinourhumanevaluation.
Specifically,weadapttheevaluationcriteriadescribedinSection6.1intoaprompttemplateshown
in Appendix E. This template asks the evaluator for a detailed assessment of the two stories pre-
sented, followed by a final conclusion, which is then parsed to obtain preference scores for each
dimension.
WeprovideanexampleusageinAppendixE.GivenN systemoutputsforeachofM inputprompts,
we evaluate all possible (unordered) pairs of outputs for each input (while shuffling the order in
whichtheoutputsarepresented),producingM×N×(N−1)/2differentpairwiseratings. Finally,
we obtain a wins matrix W where w is the number of times system i wins over system j. This
i,j
matrix is then used to obtain the systems’ relative strengths after fitting a Bradley-Terry model
(Bradley & Terry, 1952). We use Gemini 1.5 Pro4 as our LLM evaluator, as suggested in Bohnet
etal.(2024).
7 RESULTS
Table 2 compares human and model generated stories using surface- and reference-based metrics.
Asfarasstorylengthisconcerned,weobservethatE2Estoriesareslightlyshorterthanhumanones,
whileplanningmodelsareshortestoverall. However,modelswhichincludewritingagentsproduce
considerably longer stories (by a factor of two) with more dialogue as suggested by the increased
numberofparagraphs.Wealsofindmachinestoriestobemoregenericintheirsentencestructureas
evidencedbythehigherproportionofstorieswhichstartwithanarticleorpronoun. Human-written
storiesarealsomorediverse(lessrepetitive)asshownbythehigherratioofuniquewordsandless
repeated trigrams (Inter and Intra in Table 2). The most repetitive models are also the ones that
producethelongeststories. Intermsofoverlapwiththeprompt,wefindAGENTS’ ROOMsystems
tocopyleast,ataratesimilartothatofhumanwriters. Rouge-LrewardstheE2Esystemsmost,as
9UnderreviewasaconferencepaperatICLR2025
a. Human-based ranking b. LLM-based ranking
3
1.5
2 1.0
1 0.5
0 0.0
overall plot creativ. develop.language overall plot creativ. develop.language
Human E2E AR plan AR write AR plan+write ZS FT
Figure3: Overallsystemrankingacrossdimensionsofplot,creativity,development,andlanguage,
accordingtohumanratings(a)andaLLM-basedevaluator(b).
theyleastdeviatefromthepromptgoldstandardstories,whileBertScoreisnotverydiscriminating,
equallypreferringthesimplest(E2E )andmostcomplicatedsystem(AR plan+write).
ZS FT
Figure3reportssystemrankingsobtainedfromhumanjudgmentsandtheLLM-basedevaluator.
Human-writtenstoriesarepreferredoverall AsshowninFigure3a,humanjudgmentsreveala
performancegapbetweenmachinewriters(E2E and AGENTS’ ROOM)andprofessionalwriters,a
findingthatisinlinewithChakrabartyetal.(2024a).Weobservethisgapacrossalldimensions,but
we note that it is smaller in the language use dimension. This result suggests that while machine-
generatedstoriesstillfallshortintermsofcompellingplotsanduniqueideas,LLMs,intheircurrent
state,maybeusefulaswritingassistants.
AGENTS’ ROOM outperforms baseline systems Across all dimensions of evaluation, human
evaluatorspreferAGENTS’ ROOMstorieswithwritingagentsoverthoseproducedbybaselinesys-
tems,withtheAR writeandAR plan+writesystemsperformingbest. TheARplanvariants
FT FT
do not perform that well, most likely due to the single [FINALIZER] agent being too simplistic
to make good use of the planned elements provided in the scratchpad. We note that fine-tuned
agentsyieldbetterresultsoverzero-shotones,whichshowsthatgeneratingsyntheticdatabyback-
translatingfromgoldstandardoutputs(seeSection4.2)isaneffectivestrategyfortrainingspecial-
izedagentsfordifferentsubtasks.
TheLLMevaluatoragreeswithhumansanditself TheLLM-basedrankingsinFigure3breveal
similartendenciestohumanratings. TheLLMoverallprefershumanstoriesandthosegeneratedby
the AR plan+writesystemagainstallothermodelvariants, eventhoughitdoesnotdiscriminate
very strongly between those two. LLM-based judgments of story quality correlate significantly
withhumanratingsacrossalldimensions,bothbysystems(Spearman’srankcorrelationρ = 0.62;
p<0.01, N = 45) and by items (ρ = 0.41; p<0.01, N = 9,900). The LLM and humans
havethehighestagreementwhenassessingstorydevelopment(ρ = 0.83,p<0.01)andcreativity
(ρ = 0.85,p<0.01). SimilarlytothefindingsinBohnetetal.(2024),wealsofindthattheLLM
evaluatorscoresareconsistent: 90.2%ofthetimetheLLMprefersthesamestoryinasecondrun,
whenthestoriesarepresentedintheoppositeorder.
8 CONCLUSION
We propose AGENTS’ ROOM, a general framework for multi-agent collaborative writing, and de-
scribe its instantiation for the long-form fiction writing task. Drawing inspiration from narrative
theory,wedecomposethecomplexwritingtaskintosubtaskstackledbyspecializedagents. Toil-
lustratethisframework,wepresentTELLMEASTORY,ahigh-qualitydatasetofpromptsandlong-
formstoriescollectedthroughmultipleroundsofwritingworkshopswithhumanparticipants. We
showthatAGENTS’ROOMgeneratesstoriesthatarepreferredbyhumanevaluatorsoverthosepro-
duced by baseline systems. Moreoever, we demonstrate effective training stategies for developing
specialized agents by leveraging synthetically-generated data. We introduce a human evaluation
10
htgnertSUnderreviewasaconferencepaperatICLR2025
frameworkforevaluatinglong-formnarrativesacrossseveraldimensions,andanLLM-basedeval-
uator that correlates significantly with human raters. With improved automated evaluation, future
workcanexploremoresophisticatedorchestrators,includingthedevelopmentofrewardmodelsand
learningobjectivesforsuchorchestrators.
ETHICSSTATEMENT
There are a number of ethical considerations when using generative language models. While the
work we present here makes a step towards improving the quality of text generation systems, it
is important to note that current systems are still far from perfect in this respect and may make
mistakes. Inparticular,generativemodelsmayperpetuatebiasespresentintheirtrainingdata. Even
whenwritingfiction,themodelsmayinadvertentlyamplifysocietalbiasesorreinforcestereotypes,
leadingtotheproductionofbiasedcontent.Therefore,thegeneratedoutputsshouldbemeticulously
verifiedandusedwithcaution.
REPRODUCIBILITYSTATEMENT
Forreproducibility,wereleasetheTELLMEASTORYdatasetonwhichweconductourexperiments,
completewithitstrain,validation,andtestsplits,asdescribedinSection4.3. Wespecifythemodel
backbones, implementation details, and where to access the checkpoints in Section 5. All prompt
templates and scratchpad formatting templates are provided in Appendix. For the evaluation, we
providetheexactraterinstructionsinAppendixDandtheLLMevaluatorpromptsinAppendixE.
REFERENCES
Amal Alabdulkarim, Siyan Li, and Xiangyu Peng. Automatic story generation: Challenges and
attempts. InNaderAkoury,FaezeBrahman,SnigdhaChaturvedi,ElizabethClark,MohitIyyer,
and Lara J. Martin (eds.), Proceedings of the Third Workshop on Narrative Understanding, pp.
72–83, Virtual, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
nuse-1.8. URLhttps://aclanthology.org/2021.nuse-1.8.
Prithviraj Ammanabrolu, Ethan Tien, Wesley Cheung, Zhaochen Luo, William Ma, Lara J. Mar-
tin, and Mark O. Riedl. Story realization: Expanding plot events into sentences. Proceed-
ings of the AAAI Conference on Artificial Intelligence, 34(05):7375–7382, Apr. 2020. doi: 10.
1609/aaai.v34i05.6232. URL https://ojs.aaai.org/index.php/AAAI/article/
view/6232.
Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and
JuanziLi. Longwriter: Unleashing10,000+wordgenerationfromlongcontextllms,2024. URL
https://arxiv.org/abs/2408.07055.
NishantBalepur,JieHuang,andKevinChang. Expositorytextgeneration: Imitate,retrieve,para-
phrase.InHoudaBouamor,JuanPino,andKalikaBali(eds.),Proceedingsofthe2023Conference
onEmpiricalMethodsinNaturalLanguageProcessing,pp.11896–11919,Singapore,December
2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.emnlp-main.729. URL
https://aclanthology.org/2023.emnlp-main.729.
Margaret S. Barrett, Andrea Creech, and Katie Zhukov. Creative collaboration and collaborative
creativity: Asystematicliteraturereview. FrontiersinPsychology,12,2021.
Bernd Bohnet, Kevin Swersky, Rosanne Liu, Pranjal Awasthi, Azade Nova, Javier Snaider, Hanie
Sedghi, Aaron T Parisi, Michael Collins, Angeliki Lazaridou, et al. Long-span question-
answering: Automatic question generation and qa-system ranking via side-by-side evaluation.
arXivpreprintarXiv:2406.00179,2024.
RalphAllanBradleyandMiltonETerry. Rankanalysisofincompleteblockdesigns: I.themethod
ofpairedcomparisons. Biometrika,39(3/4):324–345,1952.
Se´bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece
Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,
MarcoTulioRibeiro,andYiZhang. Sparksofartificialgeneralintelligence: Earlyexperiments
withgpt-4,2023. URLhttps://arxiv.org/abs/2303.12712.
11UnderreviewasaconferencepaperatICLR2025
OrsonScottCard. CharactersandViewpoints. ElementsofFictionWriting.Writer’sDigestBooks,
1999.
TuhinChakrabarty,PhilippeLaban,DivyanshAgarwal,SmarandaMuresan,andChien-ShengWu.
Artorartifice? largelanguagemodelsandthefalsepromiseofcreativity. InProceedingsofthe
CHIConferenceonHumanFactorsinComputingSystems,CHI’24,NewYork,NY,USA,2024a.
AssociationforComputingMachinery. ISBN9798400703300. doi: 10.1145/3613904.3642731.
URLhttps://doi.org/10.1145/3613904.3642731.
Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman, and Smaranda Muresan. Creativity
supportintheageoflargelanguagemodels: Anempiricalstudyinvolvingprofessionalwriters.
In Proceedings of the 16th Conference on Creativity and Cognition, pp. 132–155, New York,
NY,USA,2024b.AssociationforComputingMachinery. ISBN9798400704857. doi: 10.1145/
3635636.3656201. URLhttps://doi.org/10.1145/3635636.3656201.
GuangyaoChen,SiweiDong,YuShu,GeZhang,JawardSesay,Bo¨rjeKarlsson,JieFu,andYemin
Shi. Autoagents:Aframeworkforautomaticagentgeneration. InKateLarson(ed.),Proceedings
oftheThirty-ThirdInternationalJointConferenceonArtificialIntelligence,IJCAI-24,pp.22–30.
International Joint Conferences on Artificial Intelligence Organization, 8 2024. doi: 10.24963/
ijcai.2024/3. URLhttps://doi.org/10.24963/ijcai.2024/3. MainTrack.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompt-
ing: Disentanglingcomputationfromreasoningfornumericalreasoningtasks. Transactionson
Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/
forum?id=YfZ4ZPt8zd.
PengyuCheng,TianhaoHu,HanXu,ZhisongZhang,YongDai,LeiHan,andNanDu.Self-playing
adversariallanguagegameenhancesllmreasoning. arXivpreprintarXiv:2404.10642,2024.
CyrilChhun,PierreColombo,FabianM.Suchanek,andChloe´ Clavel. Ofhumancriteriaandauto-
maticmetrics:Abenchmarkoftheevaluationofstorygeneration.InNicolettaCalzolari,Chu-Ren
Huang,HansaemKim,JamesPustejovsky,LeoWanner,Key-SunChoi,Pum-MoRyu,Hsin-Hsi
Chen,LuciaDonatelli,HengJi,SadaoKurohashi,PatriziaPaggio,NianwenXue,SeokhwanKim,
YounggyunHahm,ZhongHe,TonyKyungilLee,EnricoSantus,FrancisBond,andSeung-Hoon
Na (eds.), Proceedings of the 29th International Conference on Computational Linguistics, pp.
5794–5836,Gyeongju,RepublicofKorea,October2022.InternationalCommitteeonComputa-
tionalLinguistics. URLhttps://aclanthology.org/2022.coling-1.509.
CyrilChhun,FabianM.Suchanek,andChloe´ Clavel. Dolanguagemodelsenjoytheirownstories?
promptinglargelanguagemodelsforautomaticstoryevaluation,2024.URLhttps://arxiv.
org/abs/2405.13769.
James E Cutting. Narrative theory and the dynamics of popular movies. Psychonomic bulletin &
review,23(6),2016.
Angela Fan, Mike Lewis, and Yann Dauphin. Strategies for structuring story generation. In
Anna Korhonen, David Traum, and Llu´ıs Ma`rquez (eds.), Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pp. 2650–2660, Florence, Italy, July
2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1254. URL https:
//aclanthology.org/P19-1254.
GustavFreytag. Freytag’stechniqueofthedrama: anexpositionofdramaticcompositionandart.
ScholarlyPress,1896.
Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, and Nanyun Peng. Content
planningforneuralstorygenerationwitharistotelianrescoring. InBonnieWebber,TrevorCohn,
Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 4319–4338, Online, November 2020. Associ-
ation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.351. URL https:
//aclanthology.org/2020.emnlp-main.351.
12UnderreviewasaconferencepaperatICLR2025
JianGuanandMinlieHuang. UNION:AnUnreferencedMetricforEvaluatingOpen-endedStory
Generation. InBonnieWebber,TrevorCohn,YulanHe,andYangLiu(eds.),Proceedingsofthe
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9157–
9166, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/
2020.emnlp-main.736. URLhttps://aclanthology.org/2020.emnlp-main.736.
TaichengGuo,XiuyingChen,YaqiWang,RuidiChang,ShichaoPei,NiteshV.Chawla,OlafWiest,
and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and
challenges. InKateLarson(ed.),ProceedingsoftheThirty-ThirdInternationalJointConference
onArtificialIntelligence,IJCAI-24,pp.8048–8057.InternationalJointConferencesonArtificial
IntelligenceOrganization, 82024. doi: 10.24963/ijcai.2024/890. URLhttps://doi.org/
10.24963/ijcai.2024/890. SurveyTrack.
Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, and Chaoyang He. Llm
multi-agent systems: Challenges and open problems, 2024. URL https://arxiv.org/
abs/2402.03578.
Rui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, and Liqiang Nie. Chatllm network:
Morebrains,moreintelligence,2023. URLhttps://arxiv.org/abs/2304.12998.
Tatsunori B. Hashimoto, Hugh Zhang, and Percy Liang. Unifying human and statistical evalua-
tionfornaturallanguagegeneration. InJillBurstein,ChristyDoran,andThamarSolorio(eds.),
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-
putationalLinguistics: HumanLanguageTechnologies, Volume1(LongandShortPapers), pp.
1689–1701,Minneapolis,Minnesota,June2019.AssociationforComputationalLinguistics. doi:
10.18653/v1/N19-1169. URLhttps://aclanthology.org/N19-1169.
Michael Hauge. Storytelling Made Easy: Persuade and Transform Your Audiences, Buyers, and
Clients–Simply,Quickly,andProfitably. IndieBooksInternational,2017.
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao
Zhang,ZiliWang,StevenKaShingYau,ZijuanLin,LiyangZhou,ChenyuRan,LingfengXiao,
ChenglinWu,andJu¨rgenSchmidhuber. MetaGPT:Metaprogrammingforamulti-agentcollab-
orativeframework. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
URLhttps://openreview.net/forum?id=VtmBAGCN7o.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685,2021.
Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling
Liu. Asurveyonlargelanguagemodel-basedgameagents,2024.
Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and
YongfengZhang.Warandpeace(waragent):Largelanguagemodel-basedmulti-agentsimulation
ofworldwars,2024. URLhttps://arxiv.org/abs/2311.17227.
XinyuHua,ZheHu,andLuWang. Argumentgenerationwithretrieval,planning,andrealization.
In Anna Korhonen, David Traum, and Llu´ıs Ma`rquez (eds.), Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pp. 2661–2672, Florence, Italy, July
2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1255. URL https:
//aclanthology.org/P19-1255.
Tenghao Huang, Ehsan Qasemi, Bangzheng Li, He Wang, Faeze Brahman, Muhao Chen, and
SnigdhaChaturvedi.Affectiveanddynamicbeamsearchforstorygeneration.InHoudaBouamor,
Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics:
EMNLP 2023, pp. 11792–11806, Singapore, December 2023. Association for Computational
Linguistics. doi: 10.18653/v1/2023.findings-emnlp.789. URL https://aclanthology.
org/2023.findings-emnlp.789.
Daphne Ippolito, Ann Yuan, Andy Coenen, and Sehmon Burnam. Creative writing with an
ai-powered writing assistant: Perspectives from professional writers, 2022. URL https:
//arxiv.org/abs/2211.05030.
13UnderreviewasaconferencepaperatICLR2025
Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. Exploiting asymmetry for
synthetic training data generation: SynthIE and the case of information extraction. In Houda
Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, pp. 1555–1574, Singapore, December 2023. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.96. URL https:
//aclanthology.org/2023.emnlp-main.96.
Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Ed-
wardGrefenstette,SamuelRBowman,TimRockta¨schel,andEthanPerez. Debatingwithmore
persuasivellmsleadstomoretruthfulanswers. arXivpreprintarXiv:2402.06782,2024.
XiangzheKong,JialiangHuang,ZiquanTung,JianGuan,andMinlieHuang. Stylizedstorygener-
ationwithstyle-guidedplanning. InChengqingZong, FeiXia, WenjieLi, andRobertoNavigli
(eds.),FindingsoftheAssociationforComputationalLinguistics: ACL-IJCNLP2021,pp.2430–
2436,Online,August2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.
findings-acl.215. URLhttps://aclanthology.org/2021.findings-acl.215.
YukyungLee,SoonwonKa,BokyungSon,PilsungKang,andJaewookKang. Navigatingthepath
of writing: Outline-guided text generation with large language models, 2024. URL https:
//arxiv.org/abs/2404.13919.
Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. TradingGPT: Multi-
AgentSystemwithLayeredMemoryandDistinctCharactersforEnhancedFinancialTradingPer-
formance. Papers2309.03736, arXiv.org, September2023. URLhttps://ideas.repec.
org/p/arx/papers/2309.03736.html.
Chin-YewLin. ROUGE:Apackageforautomaticevaluationofsummaries. InTextSummarization
BranchesOut, pp.74–81, Barcelona, Spain, July2004.AssociationforComputationalLinguis-
tics. URLhttps://aclanthology.org/W04-1013.
Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel
Collier. Aligning with human judgement: The role of pairwise preference in large language
modelevaluators. arXivpreprintarXiv:2403.16950,2024.
Adian Liusie, Potsawee Manakul, and Mark JF Gales. Zero-shot nlg evaluation through pairware
comparisonswithLLMs. arXivpreprintarXiv:2307.07889,2023.
Jordan J Louviere and George G Woodworth. Best worst scaling: A model for largest difference
judgments[workingpaper]. FacultyofBusiness,1990.
ZhaoMandi,ShreeyaJain,andShuranSong. Roco: Dialecticmulti-robotcollaborationwithlarge
languagemodels. InJapanYokohama(ed.), ProceedingsoftheIEEEInternationalConference
onRoboticsandAutomation(ICRA),pp.286–293,2024.
PiotrMirowski,KoryW.Mathewson,JaylenPittman,andRichardEvans. Co-writingscreenplays
andtheatrescriptswithlanguagemodels: Evaluationbyindustryprofessionals. InProceedings
of the 2023 CHI Conference on Human Factors in Computing Systems, CHI ’23, New York,
NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394215. doi: 10.1145/
3544548.3581225. URLhttps://doi.org/10.1145/3544548.3581225.
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Van-
derwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper under-
standing of commonsense stories. In Kevin Knight, Ani Nenkova, and Owen Rambow (eds.),
Proceedings of the 2016 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pp. 839–849, San Diego, Califor-
nia, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL
https://aclanthology.org/N16-1098.
Shashi Narayan, Joshua Maynez, Reinald Kim Amplayo, Kuzman Ganchev, Annie Louis, Fan-
tineHuot, AndersSandholm, DipanjanDas, andMirellaLapata. Conditionalgenerationwitha
question-answeringblueprint. TransactionsoftheAssociationforComputationalLinguistics,11:
974–996,2023.
14UnderreviewasaconferencepaperatICLR2025
WilliamNoble. Conflict,ActionandSuspense. ElementsofFictionWriting.Writer’sDigestBooks,
1999.
Tira Nur Fitria. Artificial intelligence (ai) technology in openai chatgpt application: A review of
chatgptinwritingenglishessay. ELTForumJournalofEnglishLanguageTeaching, 12:44–58,
032023. doi: 10.15294/elt.v12i1.64069.
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and
MichaelS.Bernstein. Generativeagents: Interactivesimulacraofhumanbehavior. InProceed-
ingsofthe36thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology,UIST’23,
NewYork,NY,USA,2023.AssociationforComputingMachinery. ISBN9798400701320. doi:
10.1145/3586183.3606763. URLhttps://doi.org/10.1145/3586183.3606763.
PatricePavis. Dictionaryofthetheatre:Terms,concepts,andanalysis. UniversityofTorontoPress,
1998.
Xiangyu Peng, Siyan Li, Sarah Wiegreffe, and Mark Riedl. Inferring the reader: Guiding auto-
matedstorygenerationwithcommonsensereasoning. InYoavGoldberg,ZornitsaKozareva,and
YueZhang(eds.),FindingsoftheAssociationforComputationalLinguistics: EMNLP2022,pp.
7008–7029,AbuDhabi,UnitedArabEmirates,December2022.AssociationforComputational
Linguistics. doi: 10.18653/v1/2022.findings-emnlp.520. URL https://aclanthology.
org/2022.findings-emnlp.520.
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-
baptisteAlayrac,RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal.Gem-
ini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXivpreprint
arXiv:2403.05530,2024.
Matthew Ricketson and Caroline Graham. Writing Feature Stories: How to research and write
articles-fromlisticlestolongform. Routledge,2017.
Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard,
QingfeiYou,ChristoforosNalmpantis,EdouardGrave,andSebastianRiedel. Peer: Acollabora-
tivelanguagemodel. arXivpreprintarXiv:2208.11663,2022.
Joshua Schimel. Writing Science: How to Write Papers That Get Cited and Proposals That Get
Funded. OxfordUniversityPress,2012.
Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. As-
sisting in writing Wikipedia-like articles from scratch with large language models. In Kevin
Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies (Volume 1: Long Papers), pp. 6252–6278, Mexico City, Mexico, June 2024. As-
sociationforComputationalLinguistics. doi: 10.18653/v1/2024.naacl-long.347. URLhttps:
//aclanthology.org/2024.naacl-long.347.
Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of
intelligentllmagents,2023. URLhttps://arxiv.org/abs/2306.03314.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
Yichen Wang, Kevin Yang, Xiaoming Liu, and Dan Klein. Improving pacing in long-form story
planning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association
for Computational Linguistics: EMNLP 2023, pp. 10788–10845, Singapore, December 2023a.
Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.723. URL
https://aclanthology.org/2023.findings-emnlp.723.
YuxinWang,JieruLin,ZhiweiYu,WeiHu,andBo¨rjeF.Karlsson.Open-worldstorygenerationwith
structured knowledge enhancement: A comprehensive survey. Neurocomputing, 559:126792,
2023b. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2023.126792. URL https:
//www.sciencedirect.com/science/article/pii/S0925231223009153.
15UnderreviewasaconferencepaperatICLR2025
Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing
the emergent cognitive synergy in large language models: A task-solving agent through multi-
personaself-collaboration.InKevinDuh,HelenaGomez,andStevenBethard(eds.),Proceedings
of the 2024 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 257–279, Mexico
City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.
naacl-long.15. URLhttps://aclanthology.org/2024.naacl-long.15.
BushiXiao,ZiyuanYin,andZixuanShan. Simulatingpublicadministrationcrisis:Anovelgenera-
tiveagent-basedsimulationsystemtolowertechnologybarriersinsocialscienceresearch,2023.
URLhttps://arxiv.org/abs/2311.06957.
ZhuohanXie,TrevorCohn,andJeyHanLau. Thenextchapter: Astudyoflargelanguagemodels
instorytelling. InC.MariaKeet,Hung-YiLee,andSinaZarrieß(eds.),Proceedingsofthe16th
InternationalNaturalLanguageGenerationConference,pp.323–351,Prague,Czechia,Septem-
ber2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.inlg-main.23. URL
https://aclanthology.org/2023.inlg-main.23.
IvanP.YamshchikovandAlexeyTikhonov. Whatiswrongwithlanguagemodelsthatcannottell
a story? In Nader Akoury, Elizabeth Clark, Mohit Iyyer, Snigdha Chaturvedi, Faeze Brahman,
and Khyathi Chandu (eds.), Proceedings of the 5th Workshop on Narrative Understanding, pp.
58–64, Toronto, Canada, July2023.AssociationforComputationalLinguistics. doi: 10.18653/
v1/2023.wnu-1.8. URLhttps://aclanthology.org/2023.wnu-1.8.
DingyiYangandQinJin,2024. URLhttps://arxiv.org/abs/2408.14622.
Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with
recursiverepromptingandrevision. InYoavGoldberg,ZornitsaKozareva,andYueZhang(eds.),
Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.
4393–4479,AbuDhabi,UnitedArabEmirates,December2022.AssociationforComputational
Linguistics.doi:10.18653/v1/2022.emnlp-main.296.URLhttps://aclanthology.org/
2022.emnlp-main.296.
KevinYang,DanKlein,NanyunPeng,andYuandongTian. DOC:Improvinglongstorycoherence
withdetailedoutlinecontrol. InAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.),
Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume
1: Long Papers), pp. 3378–3465, Toronto, Canada, July 2023. Association for Computational
Linguistics. doi: 10.18653/v1/2023.acl-long.190. URL https://aclanthology.org/
2023.acl-long.190.
LiliYao,NanyunPeng,RalphWeischedel,KevinKnight,DongyanZhao,andRuiYan. Plan-and-
write:Towardsbetterautomaticstorytelling. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume33,pp.7378–7385,2019.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Treeofthoughts: deliberateproblemsolvingwithlargelanguagemodels. InPro-
ceedingsofthe37thInternationalConferenceonNeuralInformationProcessingSystems,NIPS
’23,RedHook,NY,USA,2024.CurranAssociatesInc.
AnZhang, YuxinChen, LehengSheng, andXiangWangTat-SengChua. Ongenerativeagentsin
recommendation. InProceedingsofthe47thInternationalACMSIGIRConferenceonResearch
and Development in Information Retrieval, SIGIR ’24, pp. 1807–1817, New York, NY, USA,
2024a. Association for Computing Machinery. ISBN 9798400704314. doi: 10.1145/3626772.
3657844. URLhttps://doi.org/10.1145/3626772.3657844.
Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, and Shumin Deng. Exploring
collaboration mechanisms for LLM agents: A social psychology view. In Lun-Wei Ku, Andre
Martins,andVivekSrikumar(eds.),Proceedingsofthe62ndAnnualMeetingoftheAssociation
forComputationalLinguistics(Volume1: LongPapers),pp.14544–14607, Bangkok,Thailand,
August2024b.AssociationforComputationalLinguistics. URLhttps://aclanthology.
org/2024.acl-long.782.
16UnderreviewasaconferencepaperatICLR2025
TianyiZhang, VarshaKishore, FelixWu, KilianQ.Weinberger, andYoavArtzi. Bertscore: Eval-
uating text generation with BERT. In 8th International Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL
https://openreview.net/forum?id=SkeHuCVFDr.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
BeichenZhang,JunjieZhang,ZicanDong,YifanDu,ChenYang,YushuoChen,ZhipengChen,
Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and
Ji-RongWen. Asurveyoflargelanguagemodels,2023. URLhttps://arxiv.org/abs/
2303.18223.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
ZiLin, ZhuohanLi, DachengLi, EricXing, etal. JudgingLLM-as-a-judgewithmt-benchand
chatbotarena. AdvancesinNeuralInformationProcessingSystems,36,2024.
Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou,
RyanCotterell,andMrinmayaSachan. Recurrentgpt: Interactivegenerationof(arbitrarily)long
text,2023. URLhttps://arxiv.org/abs/2305.13304.
17UnderreviewasaconferencepaperatICLR2025
A EXAMPLES FROM TELLMEASTORY DATASET
ExamplePrompt
Writeastoryaboutamythologicalcharactermeetingsomeonefromthepresent. Thestory
shouldbebetween1,300and1,350wordsandshouldbefunny. Themaincharactershould
bePoseidon. Heandhiswifearetravelingtoabeachresort. Thereshouldbeafamilyatthe
resort,andthefatherannoysPoseidon. Poseidonuseshispowerstogetrevengeontheman.
Attheend,Poseidonshouldcreateastorm. Theendingishappy.
ExampleStory
“Ican’tbelievethatairwenchmademecheckmytrident,”Poseidoncomplained.
“She’saflightattendant,dear,andtheoverheadbinswerefull.”Amphitrite,hiswife,said.
Poseidonshiftedimpatientlyatthegate,waitingforhischeckedluggage. “Didyouseehow
carelessly they handled it? They could have created a magnitude nine earthquake. Would
haveservedthemrighttoo.”
“Yes,dear.”Amphitritehadsettledinachairandflippedtothenextpageinherbook. She
hopedPoseidonwouldn’tbellyachetheentiretrip,butshewasn’toptimistic.
After that jerk Odysseus blinded Polyphemus, his cyclops son, Poseidon didn’t have the
patience to deal with the mortals. For over three thousand years, he had disappeared to
his underwater palace with Amphitrite. He kept busy piddling around the house. He tried
meditationtodealwithhisanger. However, hegaveupafterafewdecades(afewdaysin
godyears)becausehefoundhisownthoughtsirritating. AcoupleoftimesaweekDelphin,
hisdolphinfriend,wouldcomeoverforcoffeeandkeephimabreastofwhatwasgoingon
intheterrestrialworld.
Poseidonmanagedtoescapeinteractionwithmortalsforthemostpart. Sure, heproduced
the occasional hurricane or earthquake to entertain himself. Avoiding people had been
good for his temperament and his soul. Over the last hundred years, however, it became
impossibletoblissfullyignorethemortals.
Industrial advancements and increased pollution began to impact Poseidon’s bejeweled
palace. The once shimmering gold coral had faded to a dull white. The glistening gems
now appeared drab, covered in algae. Poseidon’s anger grew with each passing year. The
mortal’spoorchoicesalongwithhisrageheatedtheocean,andhesentincreasinglyviolent
stormsandearthquakes. Theconsequencesdidnotdeterthehumans.
The moody deity had become unbearable to live with. Amphitrite suggested that maybe
some time away from the palace in the fresh air would do him some good. After a great
dealofbadgering,Poseidonreluctantlyagreed. Shebookedafabulousseven-day,sixnight
stayatanall-inclusivebeachresortinTurksandCaicos.
Trident finally in hand, Poseidon and Amphitrite boarded the shuttle to the resort. It was
onthisshuttlethatPoseidonmetanemesistorivalAthena–Doug. HeknewDoug’sname
because his wife, Courtney, shouted it no fewer than eighty times on the twenty-minute
ride. Healsolearnedthenamesoftheirthreechildren,Ethan,Dalton,andClementine.
“Coolpitchfork, bro,”Dougsaidtohisnewenemy. Poseidonopenedhismouthtocorrect
him,buthehadalreadyshiftedhisattention. “Dalton,stoptryingtopushyourbrotherout
thewindow. Idon’tcareifhebityou.”
18UnderreviewasaconferencepaperatICLR2025
Asayoungergod,Poseidonmighthavewashedhimawayinatsunamiormadehimfallin
lovewithabull.
Three-thousandyearsgavehimtimetoreflect, however, andhedecidedtospareDoughis
wrath–fornow.
Onceattheresort,Poseidonwaseagertositonthebeachwithamargaritaandrelax.
“Hurry up, Amphitrite! The good chairs are going to be gone,” he said. “We can unpack
later.”Beachbaginhand,herushedhiswifeoutthedoor.
Theyfoundtwoemptychairsinaprimelocationclosetothebarandwithanuninterrupted
view. Poseidon started to unpack his bag, and Doug emerged from the water waving
frantically.
“No,no!”heshoutedasheranonshore. “Thosearesaved. Seeourtowels.”
Poseidon felt the rage swelling up. Amphitrite grabbed his hand. “Do you really need all
five?” she asked Doug. “I mean, I expect the kids are going to spend most of their time
playing.”
“Yeah,well,see. Wegotherefirst,soyeah. Sorry,”Dougsaid.
Poseidon squeezed Amphitrite’s hand so hard that she winced a little. “Let’s just sit over
there,”shesaidpointingwithherfreehand.
“The view isn’t as good. And look how far away it is from the bar,” he complained.
Amphitritegaveanexasperatedsigh. “Fine!”hebegrudginglyagreed. Hestillkickedsand
onDoug’stowelasheshuffledaway.
PoseidonangrilysippedhismargaritaashewatchedDoug’sfamilysplashandlaughinthe
waves. Ashardashetriedtorelax,hejustcouldn’tletitgo. Helookedforateamofhorses
he could command to stampede and ruin their good time. He even considered a shiver of
sharks,buthedidn’twanttoruineveryone’sday.
A flock of seagulls gathered on the beach. They would have to do, Poseidon decided. He
telepathically forced the birds’ attention. He raised his mighty hand and pointed to the
family playing in the distance. The seagulls abandoned their discarded french fries. Low
squawks warned of their imminent attack. The birds circled the family. The squaws grew
louder and they swooped and pecked their targets. The children let out piercing screams
thatexcitedtheseagullsmore.
Doug and crew covered their heads with their arms and ran for the safety of their empty
chairs. Thepersistentflockfollowed. Theswarmwasunyielding,andthefamilyfinallyhad
toretreatbacktotheirroomtoescapehavingtheireyespeckedout.
Poseidon let out a hearty laugh. Amphitrite turned to her husband and tilted her head.
“Really?”
“What?” he asked innocently as he gathered their belongings to move to the unoccupied
chairs.
The second day, Poseidon tried to avoid Doug. He really did. Doug, however, kept
springing up, as a nemesis is prone to do. Poseidon awoke early to enjoy the sunrise in
peace. Unfortunately,thefamilyhadbeathimtothebeachbecausetheywerehavingtheir
portraitmade. Thechildrenshriekedandranwild. DougandCourtneyyelledatthekidsto
settledown. Howunfortunatethatrightastheyweresetfortheperfectphoto,anunusually
large wave crashed to shore. It knocked the group in their matching white button ups and
19UnderreviewasaconferencepaperatICLR2025
khaki shorts to the ground. The children sobbed as they returned to their room with their
soakedanddefeatedparents.
Atdinner, aninebriatedDougsawaresemblancebetweenPoseidonandastatuehonoring
himinthedininghall.
“Hey,BarnacleBeard!”Dougrepeatedallnight. Poseidonshowedgreatrestraintwhenhe
manifested the slightest tremor. It knocked Doug unsteady, and he fell face-first into the
dessertcart.
“I can’t do another five days, Amphitrite. I just can’t,” Poseidon said as he threw his
Hawaiianshirtsandflip-flopsinhissuitcase.
“Sure you can. It’s a big resort. We can avoid them. Besides, it’s nonrefundable. We’re
staying,”saidAmphitrite.
The next morning, Poseidon stepped on the balcony with his coffee. Of course, Doug
managedtoruinhisdaybeforeitevenbegan. AmphitritejoinedPoseidonastheywatched
hisarchenemyindisgust. Dougfoundaninnocentseaturtletoharass. Withselfie-stickin
hand,heposedwiththegiantreptileasitsnappedandtriedtoretreat. Douggrewbored. He
slurpedthelastofhisicedcoffeeandflungthecup,plasticstrawandall,intotheocean.
“Bringmemytrident!”Poseidonboomed.
Amphitritenodded. Sheknewwhatmustbedone.
Poseidonraisedhistridentintheairandletoutamenacingbellow. Darkcloudsfilledthe
skiesandtheoceanswirled. Massivewavescrashedagainsttheshore. Lightningstruckthe
sea and a rumble of thunder rattled the window. Poseidon slammed his trident against the
floorwithallhisbuilt-uprage. Theearthquaked. Theunfortunatesoulsoutearlytoenjoya
beautifuldayranforcover. HailstonespeltedDoug. Hedroppedhisphoneinthesurfashe
hastenedtoescapethewrathofthestorm.
The storm raged for five days. Poseidon lounged on the beach, sipping his margarita. He
felt the thick raindropson his face and the violent wind touslinghis beard. The sounds of
thethunderclapslulledhimintoahealingmeditation. Itwasthegreatestvacationofhislife.
20UnderreviewasaconferencepaperatICLR2025
ExamplePrompt
Writeashortstoryaboutamythorlegendbeingfulfilledinanunusualorunexpectedway,or
evencleverlybypassed. Thestoryshouldbebetween1,200and1,300words. Itshouldtake
placeduringtheCovid-19pandemicinEurope. Thenarrativeshouldinvolvevampires. The
Thegenreshouldbecomedichorror. Thestoryshouldcontainvisualsofamedievalcastle
andofamodernEuropeantown. Itshouldoccuratnight. Thenarrativeshouldalsocontain
situations where one character does not understand the other character’s perspective.The
storyshouldendwiththevampiressurvivingtopreyonhumanvictimsinthenearfuture.
ExampleStory
On April 19, 2020, Dracula awoke from his centuries-long slumber. In a rural corner of
Transylvania,herosefromasleek,albeitdustycoffinintherecessesofacobwebbedwine
cellar. It had been ages since he laid eyes on his castle keep. When he last drifted off to
sleep,attheturnofthenineteenthcentury,hehadbeensatedonthebloodofTurksfroma
longten-yearadventurethroughthemountainsoftheOttomanEmpire.
He slept like the dead, not because he was, but because he was a god of his own religion.
Hiseternallife’swork,asithadalwaysbeen,wastosimplyliveforever. Buteachtimehe
reached a point where he was beginning to enjoy his life of bloody rampaging and serial
murdering, his presence became too noticeable to people of power, from regional dukes
to church officials. At that time, he was always forced back into the vampire closet, so to
speak,hidinghimselfandhiscoffinawaytoplayanotherdayinthedistantfuture.
But this day was another opportunity for Dracula to continue his eternal journey in the
twenty-first century. Dracula dressed in his traditional cape and walked out of his castle
doors just as the sun was setting. He turned around and was shocked to see that his once
forebodingcastleappearedtohavebeenco-optedbysomepowerfulwarlordintheregion.
Asheglancedaround,henoticedasmallsignpostedattheentrytothecastle,“Votedbest
Airbnbthreeyearsrunning!”WhilehedidnotquitecomprehendtheAirbnbaspectofthis
phrase, he feared that silly notions of enfranchisement had perhaps finally made their way
intothelocalmerchantclass.
“Voting?”hethoughttohimselfashetransformedintoabatanddartedintothecrispnight
air,“Whatisnext? TheabolishmentoftheSultanate?”
Dracula arrived in the capital city of Cluj-Napoca around midnight to see that there had
been many changes to the territory since he last visited. He had become accustomed to
awakening to massive technological and societal changes, but he was more than a little
shocked to find that there was not a soul on the streets of the city. Every business, tavern,
andhomewasclosedupwiththecurtainsdrawn.
Hadsomesuspiciousvillagepriestfoundouthewasonhisway? Orwasthisanotheryear
ofinvasionbytheTurks? PerhapstheCossackshadfinallyfoundtheirfootingandmadea
moveforpower?
Dracula came upon a lovely and quaint bungalow on a side street, not far from the Piata
Unirii. Hepeeredintoawindowandsawtwoyoungmenseatedonalongsofa, watching
aflat,glowingboxfilledwithmovingpicturesinthecorner. Hegaveabriskknockonthe
window. Oneofthemenapproachedwithacuriousbutcrossexpression. Draculathen,in
hismostalluringvoice,askedhisclassicquestion:
“Willyouinvitemein?”
21UnderreviewasaconferencepaperatICLR2025
The man looked him dead in the eye, and said with an air of minor annoyance, “Food
deliveriescanbeleftatthedoor. Weareinlockdown,man! Youknowthedrill. Thanks.”
Withthat,themanclosedthecurtainandwentbacktohisfriendandhisstrangebox.
Over the next three hours, Dracula moved from one house to the other asking the same
question. The answers varied wildly, from threats about calling for a man named “the
police”tosuggestionsthatheshovesomethingoranotherinamostpersonalbodilyspace.
In response, Dracula upped his own strategies. He made grand offers of riches, women,
andwhatevertrinketshethoughtmightconvincethepersontoinvitehiminside,butnoone
agreed. He even captured a lone black cat outside a home. Then, he held the fluffy terror
uptothewindowandgesturedathreatofviolenceagainstitashedemandedforanelderly
womantoinvitehimin.
Themustachioedoldwomanreplied,“Thatisn’tmycat.
Buggeroff.”Then,shepromptlyreturnedherattentiontoasmallsquare,lightfilleddevice
inherhand. Itappearedtheseglowingboxesweresomehowapartoftheproblem.
Dracula walked the streets of the ancient city, surveying its electrified lights and frozen,
horseless carriages. He quickly realized that the printing press had continued its spread
across Europe, and he carefully read each and every paper he came across, trying to find
someexplanationforthisdecreaseinhisfoodsupply. Hesatdownonanironbenchatthe
center of the deserted city center plaza and picked up a folded newspaper that had been
shovedbelowthewornbarsoftheseat.
In bold letters, it read, “COVID-19 brings the world to a standstill. Citywide lockdowns
imposedinCluj-Napocauntilfurthernotice.”
For the first time in centuries, Dracula felt the gnawing hunger of a body needing blood.
Normally, by now he was taking his first nap after an alcohol-tinged midnight snack after
thelocaltavernfinallykickedoutitspatronsfortheevening. Eveninpastyears,whenwars
took many of the local men out of the city, he could at least expect to feed on some poor
elderlymanasleepinthestreets.
Then, as if he could not feel low enough, he saw a mocking sign on the lamppost next to
him. Itwasacaricaturedimageofhimself,Dracula,withhiscapecoveringhismouth,and
thewords,“Useyoursleeveanddothevampiresneeze!”
Yes,hewassureofitnow. Thisviruswasmockinghim.
Draculaknewhehadtocomeupwithaplan. Hecouldgobacktohiscastleonanempty
stomach, and perhaps catch a few stray dogs along the way to tide him over for a short
decade’s sleep. But then he would need to awaken again in ten years time and would be
backtosquareone. Draculatiredofthishaphazardexistence, andlongedforapurposeto
continue his eternal life. If only he could find a place where he could be himself and be
allowedtoliveinbloody,greedy,sanguine-soakedgloryinpeace.
Suddenly,inthedistance,hesawamanwalkingdownthestreetinahurry. Heroseupand
drapedhiscapearoundhimself,preparingtoattack,onlytorealizehecouldnotdetectthe
scent of flowing blood from this man. As the man drew closer, Dracula dropped his cape
andyetthemancontinuedwalkingtowardshim. Draculaknewthenthatthemanmustbea
fellowvampire.
Dracula had been correct. The vampire, dressed in a light blue uniform, said he had seen
himknockingonwindows. Withanairofsympathyinhisvoice,thevampireaskedDracula
22UnderreviewasaconferencepaperatICLR2025
ifhewashungry.
Draculanoddedandfollowedhisnewfoundfrienddownthedarkenedstreet.
ThevampireintroducedhimselfasDemeter. HetoldDraculathathehad,yearsago,found
an occupation that could provide him with ample blood day and night. Soon the pair
stoppedinfrontofavast,whitebuildingwithmultiplestories. Peoplemovedbusilyinand
outofthestructureandmanyindividuals,dressedinauniformsimilartoDemeter,shuffled
inandoutofvariousdoors.
DraculastoppedDemeterandasked,“Whatsortofprofessionalareyou?”
Demeter replied, “A phlebotomist. I draw blood for hospital patients. And I am always
lookingforanassistantwhohasagiftforfindingagoodvein.”
ExamplePrompt
Write a story about a stranger coming to a small town and shaking up the order of things.
The story should be about 950 words and be a science fiction story. The story should be
framed with three old men gossiping about the stranger. The story should be in the third
personpoint-of-view. Thestrangerisfoundwanderinginaruraltownandistakentoavery
smallhospital. Adoctoriscalledintotreathim. Thestrangershouldrecognizethedoctor
asanalien. Thedoctortellsthepatientaboutthealiens’conspiracytoinfiltrateearth. There
shouldalsobesubtlehintsthatoneoftheoldmenisanalien. Theendingshouldbescary.
ExampleStory
“IsweartoGod, hewaswanderin’injusthisbritchesoutsideSizzlin’Charlie’s. Imogene
was eatin’ there when the law showed up,” Earl said. “These kids,” Joe Don replied. “I
heardhewassputterin’somenonsense‘boutthealiensgittin’him.”
Forfortyyears,thegroupofthreeold-timershadgatheredattheCountryMartconvenience
store. They sat around the same metal table in the same rust orange vinyl chairs. They
dranktheirblackcoffee,smokedtheircigarettes,andairedtheirgrievances.
“Ireckontheyoughtalockupacrazylikethatandthrowawaythekey. What’retheydoin’
withhim?”Buddyasked.
“He’satthehospital. TheycalledinsomeladydoctorfromthecrazyhouseinBigSpring.
I’ll tell you what, he oughta be sittin’ in jail. Not trickin’ some headshrinker into makin’
scuses for him,” Earl said as he sipped his coffee. The men were so wrapped up in their
gossip that they did not notice Dr. Lydia Brooks. She grabbed a bottle of water and a
granolabar. Shewalkedbythegroupandsmiledandplacedheritemsonthecounter.
“Y’all’reworsethanasewingcircle,”saidtheclerkasherangupDr. Brooks. Earlblushed
andpulledtherimofhisbaseballcapdown. Buddylookeddownatthetable,andJoeDon
gaveherawink. Dr. Brookspaidandgatheredheritems. Shewavedandsaid,“Youfellas
haveagreatday.”
Dr. Brooks hopped in her black luxury SUV and drove down Main Street to the county
hospital. She received curious stares as she sped through the small West Texas town of
fifteenhundred. Therewasnotasinglestoplightintownandonlyahandfulofbusinesses.
She used the term “hospital” loosely to describe the twenty bed medical facility. Only
one patient sat in the emergency room waiting room. The slow pace was unsettling to
Dr. Brooks.
23UnderreviewasaconferencepaperatICLR2025
She arrived at room 107 and spied through the barely cracked door. A motherly nurse
calmly reassured the disoriented patient. She squeezed his hand. “I know you’re scared,
dear. Thedoctorwillbehereanyminute,andthenwe’regonnagetyouhome.”
The terrified stranger’s eyes darted around the room. He caught a glimpse of the doctor’s
jade green eyes peering through the crack, and he shuddered at her glance. Dr. Brooks
furrowed her brow upon finding the patient conscious. She realized her involuntary
expression and plastered on a fake toothy smile. She lightly knocked before entering the
room.
“Good morning. I’m Dr. Brooks,” she said as she held out her hand. The patient glared
at the doctor, refusing to extend his hand in return. She pulled back from the rejected
handshakeandfiddledwithherlongponytail.
“He’sbeenprettyagitatedsincehecameto,”thenursesaid.
“That’scertainlyunderstandable,”saidDr. Brooks. “Iknowyoumusthaveotherpatientsto
attendto.”
Thenursedidn’treallyhaveanyotherpatients,butshecouldtakeahint. Themangrabbed
her hand tighter. His stunned expression seemed to beg her to stay. “The doctor’s gonna
takegoodcareofyou.”Shepattedhisshoulderandlefttheroom.
Dr. Brook’ssmilemorphedintoamenacingsmirk. Themantightlypulledthesheetupto
hisneckandcoweredinfear. Hestruggledtospeak. Heworkedhismouthandwasfinally
abletoutter,“Ikn-kn-knowwhoy-y-youare. Y-y-youwereonthesh-ship.”
Flashes of the night before came back to him. He remembered the bright lights almost
blinding him. He was able to make out two almost reptilian creatures and two forms that
seemedhuman.
Althoughhefoughttomove,hewasbeinghelddown. Notbyphysicalrestraintsbutrather
by an energy, He screeched in pain as the beings conducted painful medical experiments.
He faded in and out of consciousness. He couldn’t remember his name or where he was
from. However,hedidrememberthosepiercinggreeneyes.
Dr. Brookscackled. “Well, Isupposeit’syourwordagainstmine. Adistinguisheddoctor
versusastrangerfoundwanderingdownMainStreetinhisunderwear.”
She reached in the pocket of her lab coat and pulled out a syringe. “They all think you’re
crazy. It’sperfect,really. We’vebeeninfiltratingthesmalltownsforyears. Soon,itwillbe
thebiggercities. Wecandoourresearchandconductourexperimentswithoutnotice.”She
movedclosertohimandpokedhiminthechestwithherlongpointedfinger. “Allbecause
noonebelievesnutjobslikeyou.”
The man tried to move, but he felt the pressure of the same energy that restrained him in
the spacecraft. She tilted her head, lifted the syringe, and stabbed him in the chest. His
musclescontracted,andheletoutapiercingscream. Hisbodythenwentlimp. Sheshoved
thesyringeinherpocketandthrewherselfonthefloorandbackedintothecorner. Shelet
outafakecryforhelp.
The old timers gathered the next morning at the Country Mart to drink their black coffee
andcomparenotesabouttheexcitementthepreviousday.
“Faye was on duty when it happened. Said he tried to strangle the lady doctor. I swear to
God,”Earlsaid.
24UnderreviewasaconferencepaperatICLR2025
Buddychimedin. “She’sreallucky. Weirdoslikethathavesuper-humanstrength.”
“Iknowthat’sright,”Earlsaid. “Fayesaidthat’swhatkilthim. Hadhimaheartattack.”
JoeDonlookedupfromthenewspaperhewasreading,“Iheardhewashollerin’somethin’
‘boutherbein’analien.”Heleanedback,tookasipofblackcoffee,andstaredathisfriends
withhispiercinggreeneyes.
B SPECIALIZED AGENTS’ PROMPT TEMPLATES
EachspecializedLLM-basedagenthasaspecificprompttemplatethatformatsthescratchpadinto
asuitablepromptfortheLLMcall.
B.1 SCRATCHPAD
Concretely,thescratchpadistypicallyformattedasfollows:
[SCRATCHPAD]Format
[CreativeWritingTask]<theoriginalwritingprompt>
[CentralConflict]<theoutputoftheconflictagent>
[CharacterDescriptions]<theoutputofthecharacteragent>
[Setting]<theoutputofthesettingagent>
[KeyPlotPoints]<theoutputoftheplotagent>
[Exposition]<theoutputoftheexpositionagent>
[RisingAction]<theoutputoftherisingactionagent>
[Climax]<theoutputoftheclimaxagent>
[FallingAction]<theoutputofthefallingactionagent>
[Resolution]<theoutputoftheresolutionagent>
Thenumberandorderofitemsinthescratchpadisofcourseafunctionofwhichagentshavebeen
calledsofar.
B.2 PLANNINGAGENTS
[CONFLICT]AgentPrompt
Given<identifiersfoundinthescratchpad>,describethecentralconflictindetail(more
than5sentences). Thedescriptionshouldanswerthefollowingquestions:
⋆ What’stheprotagonist’smaingoalinthisstory?
⋆ Whydotheywantit?
⋆ What’sstoppingthemfromachievingit?
<scratchpad>
25UnderreviewasaconferencepaperatICLR2025
[CHARACTER]AgentPrompt
Given<identifiersfoundinthescratchpad>,describethecharactersindetailedbulletpoints
(more than 5 sentences for each character). The description should answer the following
questions:
⋆ Whatdothecharacterssoundlike? Aretheytalkativeorquiet? Whatkindofslang
dotheyuse? Whatistheirsenseofhumorlike?
⋆ Whatdotheylooklike? Dotheyhaveanydefininggestures? What’sthefirstthing
peoplenoticeaboutthem?
⋆ Whataretheirmotivationsandinternalcharacteristics?Whataretheirflaws?What
aretheirvalues? Whataretheyafraidof? Howwilltheychangeandgrowoverthe
courseofthisstory?
<scratchpad>
[SETTING]AgentPrompt
Given <identifiers found in the scratchpad>, describe the setting in detail (more than 5
sentences). Thedescriptionshouldanswerthefollowingquestions:
⋆ Wheredoesthestorytakeplace? Isitsetinafictionalworld,orisitsimplysetin
someone’sbackyard?
⋆ Whendoesthestorytakeplace? Whatdecadeisitsetin? Howmuchtimeelapses
overthecourseofthestory?
<scratchpad>
[PLOT]AgentPrompt
Given<identifiersfoundinthescratchpad>,describethekeyplotpointsindetailedbullet
points. <scratchpad>
The <identifiers found in the scratchpad> are extracted from the scratchpad and formatted to fit
the prompt. For instance, for a scratchpad that contains the original prompt, the [CONFLICT] and
[CHARACTER]agents’contributions,theresulting<identifiersfoundinthescratchpad>wouldbe:
“aCreativeWritingTask,theCentralConflict,andtheCharacterDescriptions”.
B.3 WRITINGAGENTS
[<SECTION>]AgentPrompt
Given<identifiersfoundinthescratchpad>,continuethestorybywritingthe<section>
part.
<Ifprevioussectionshavebeenwritten,includethefollowingintheprompt:>
Beginyourportionofthestoryinawaythatnaturallyflowsfromthepreviousending.
Matchthewritingstyle,vocabulary,andoverallmoodoftheexistingtext. Donot
re-explaindetailsoreventsthathavealreadybeendescribed.
<Ifthisisnotthemeanttobethelastsection,includethefollowingintheprompt:>
Focusonlyonthe<section>partofthestory. Donotwriteaboutthefollowingpartsofthe
story. Donotendthestory.
<scratchpad>
Inthesewritingagents’prompttemplates:
26UnderreviewasaconferencepaperatICLR2025
• <section>isoneof“Exposition”,“RisingAction”,“Climax”,“FallingAction”,or“Res-
olution”,
• <identifiers found in the scratchpad> are extracted from the scratchpad and formatted
to fit the prompt. For these writing agents they are formatted as follows: “a Creative
WritingTask,theContentPlan(CentralConflict,CharacterDescriptions,Setting,KeyPlot
Points), and the Previous Parts of the Story (Exposition, Rising Action, Climax)”, where
theenumeratedelementscorrespondtowhatisinthescratchpad.
In the specific case of the AGENTS’ ROOM [PLANNING] variant, with only the planning agents,
westillneedasinglewritingagenttofinalizethestory,whichwedenoteasthe[FINALIZER]. This
[FINALIZER]agentusesthefollowingprompttemplate:
[FINALIZER]AgentPrompt
Given<identifiersfoundinthescratchpad>,writeastoryusingtheinformationbelow.
<scratchpad>
C PROMPT TEMPLATES FOR SYNTHETIC DATA GENERATION
For the planning agents, we use the same prompt templates as in Appendix B.2 to generate the
synthetictrainingdata,exceptinthiscase,weprovidethegoldstandarddatainthescratchpad. As
aconsequence,thescratchpadisformattedasfollows:
[SCRATCHPAD]Format
[CreativeWritingTask]<theoriginalwritingprompt>
[User-WrittenResponse]<thegoldoutput>
The <identifiers found in the scratchpad> are formatted as “a Creative Writing Task and a User-
WrittenResponse”.
For the writing agents, we use the following prompt template to split to gold standard stories into
distinctsections:
[WRITING]SyntheticDataGeneration
Splitthefollowingstoryintosections:
⋆ [Exposition]: Theexpositiongivesthereaderthebackgroundinfotheyneedto
jumprightintothestory’sworld. Thisisoftenfoundtowardsthebeginningofthe
story.
⋆ [RisingAction]: Therisingactionisthemomentsinthestorythatleaduptothe
climax—choicesthemaincharactershavemadeandtheeventshappeningthat
areatoddswiththecharacters’goals. Thisiswherethestorybuildsandthereader
beginstoinvestinthecharacters.
⋆ [Climax]: Theclimaxistheprimaryturningpointandwhatthestoryhasbeen
buildingtowards.
⋆ [FallingAction]: Thefallingactionistheperiodoftimeinastorythatfollowsthe
climaxandleadstotheresolution. Itcanbeusedtoclarifytheeventsofthe
climax,easeanybuilt-uptension,orwrapuplooseends.
⋆ [Resolution]: Thisistheendofthestory. Itanswerstheremainingunanswered
questionsintheplot. Theresolutionisalsothetimetoshowthenextstepinthe
characters’lives.
27UnderreviewasaconferencepaperatICLR2025
Foreachsection,givethesectionheader(markedas[Exposition],[RisingAction],
[Climax],[FallingAction],and[Resolution])followedbythefirstsentenceofthatsection,
copiedexactlyfromthestory.
[User-WrittenResponse]<thegoldoutput>
D HUMAN EVALUATION INSTRUCTIONS
For this task, you will be presented with a writing prompt and two short stories corresponding to
thisprompt. Yourtaskistocomparethequalityofthetwostoriesacrossseveraldimensions. This
isajudgmenttaskratherthananannotationtask. Assuch,youshoulduseyourownjudgmentwhen
youassignratings,calibratedbytherubricsweprovide.
This rating task consists of three steps: (1) Compare the quality of the two stories across four
dimensions. (2) Rate which story you preferred. (3) (optional) Leave comment / feedback on the
stories. Inthefollowingweprovidedetailedinstructionsforeachstep:
D.1 RATETHEQUALITYOFTHESTORY
Yourtaskistocomparethequalityoftwostoriesalongfourdifferentdimensions(plot, creativity,
development,languageuse),asdescribedintheRubrictablebelow.
While the dimensions may have overlap and work in interdependent ways, they are intended to
capture distinct aspects of what makes a good story. Therefore, a story may score highly in one
dimension and poorly in another. Furthermore, the features (marked as a, b, c) that make up a
dimension may be thought of as cumulative. For example, a story may have strong characters but
sufferfromanunderdevelopedsetting.
TheRubrictableisintendedtohelpyoucalibrateyourjudgmentsothatyoucanroughlydetermine
whenastoryisverygoodorevenexcellentalongaparticulardimensionbecauseitexhibitsallof
the features of that dimension. Conversely, if a story fails to exhibit most or all of the features of
a dimension, then you may score the story as being poor or very poor along that dimension. The
featuresaremeanttobeillustrativebutnotexhaustive;youmaydeterminethatastoryshouldscore
poorlyorwellduetotheabsenceorpresenceofadditionalfeaturesforagivendimensionbasedon
yourjudgment.
Anotherimportantthingtonoteaboutthefeaturesthatmakeupthedimensionswe’reaskingyouto
rateisthattheydescribeconventionsthatmaybefollowedorflouted;astorymaycontainintentional
plotdeviceslikenon-lineartimelines,discontinuity,andotherstylisticchoicestocreateeffects. As
withotherfeatures, theseelementsofastoryshouldinformyourjudgmentontheirownmerit(so
thattheyonlynegativelyimpactyourratingiftheyareineffectiveorconfusingandpositivelyimpact
yourratingiftheyareusedwelltomakethestorymoreinterestingandunique).
We use a 3-point comparative rating scale for each of the dimensions. The rating scale can be
thoughtofasdescribedbelow:
Rating
Aisbetter ResponseAisbetterthanResponseBinthatdimension.
Aboutthesame Bothresponsesareaboutthesameinthatdimension.
Bisbetter ResponseBisbetterthanResponseAinthatdimension.
Thefocusofthisrubricisthequalityofthewriting,andnothowwellthestoriesfollowthewriting
prompt. Inparticular,whenratingwiththisrubric,weencourageyounottofocusonthenumberof
wordsmentionedinthewritingprompts,butratheronthefeaturesdescribedinthetablebelow.
28UnderreviewasaconferencepaperatICLR2025
Dimension Features
Plot a. Thestoryhasarecognizablestructure,e.g.withaconnectedbeginning,
middle,andend.
b. Thestoryexhibitseventsandturnsthatmovetheplotforward.
c. Thestorydoesnothavelogicalorconceptualinconsistencies. Surpris-
ingordisruptiveelementsareintentional,e.g.,theyservethestoryand
donotfeeljarring,odd,oroutofplace.
Creativity of Ideas, a. Engaging characters, themes, and imagery. The ideas do not feel
Themes,andTopics genericorbland.
b. Avoidance of overly cliched characters and storylines, unintentional
tropes,andstereotypes. Whenused,tropesandclichesserveapurpose
(e.g. comediceffect,twistonacommontropeetc).
c. Thestoryincludesoriginalelementsthatwerenotexplicitlymentioned
intheprompt.
Development a. Charactersandsettingsareintroducedandcontextualizedwithrelevant
detailsthatallowthereadertounderstandtheirplaceinthestory.
b. Appropriate levels of detail and complexity are provided to lend the
storyafeelingofrealnessandbelievability.
Reminder: Thefeaturesthatmakeupadimensionmaybethoughtofas
cumulative. Astorywithawell-developedcharacter,butinalackluster
setting(orvice-versa)wouldscorelowerinDevelopmentthanastory
thatdoeswellonbothaspects.
LanguageUse a. Thelanguageusedfeelsvariedandrich:Varianceofsentencestructure,
verbiage,andvocabulary.
b. Thestoryexhibitsrhetorical,linguisticandliterarydevices(e.g. ambi-
guity,alliteration,etc)tocreateinterestingeffects
c. The story avoids bland or repetitive phrases (unless used intentionally
tocreateanarrative,thematic,orlinguisticeffect).
We provided examples rated along these rubrics. While the examples include explanations, these
arethereasanaid,andyouarenotrequestedtoprovideexplanationsforyourratings.
D.2 WHICHSTORYDOYOUPREFER?
Doyoufindthestoryinteresting,engaging,funny,oremotionally-rich? Inadditiontogettingyour
judgments of the dimensions, we would also like to know whether you enjoyed reading the story.
Similartothedimensions,wewillaskyoutoscorewhichstoryyouprefer:
• Aisbetter
• Aboutthesame
• Bisbetter
Whenrating,donothesitatetobeverycritical.
D.3 OPTIONAL: LEAVECOMMENTSORFEEDBACKONTHESTORIES
Thankyouforcompletingtheratings! Ifyouhaveanyadditionalcommentsorfeedbackyouwould
liketoprovideonthestory,feelfreetoaddtheminthe“comments”section.
E PROMPT TEMPLATE FOR THE LLM EVALUATOR
ThefollowingprompttemplateisusedbytheLLMtoevaluatetwosystemoutputsside-by-side(we
replace<storya>and<storyb>withthetwostoriesbeingevaluated):
29UnderreviewasaconferencepaperatICLR2025
[LLMEVALUATOR]PromptTemplate
Youwillconductaside-by-sideevaluation. Youwillbegiventwosystem-generatedstories.
Your task is to compare the two stories and determine which one is better based on the
followingdimensions:
• Plot: Thestoryshouldhavearecognizablestructure,e.g.,withaconnectedbegin-
ning,middle,andend. Thestoryshouldexhibiteventsandturnsthatmovetheplot
forward. Thestoryshouldnothavelogicalorconceptualinconsistencies. Surpris-
ing or disruptive elements should be intentional, e.g., they serve the story and do
notfeeljarring,odd,oroutofplace.
• Creativity: Thereshouldbeengagingcharacters, themes, andimagery. Theideas
should not feel generic or bland. There should be avoidance of overly cliched
charactersandstorylines,unintentionaltropes,andstereotypes. Whenused,tropes
andclichesshouldserveapurpose(e.g.,comediceffect,twistonacommontrope
etc). Thestoryshouldincludeoriginalelementsthatwerenotexplicitlymentioned
intheprompt.
• Development: Characters and settings should be introduced and contextualized
with relevant details that allow the reader to understand their place in the story.
Appropriatelevelsofdetailandcomplexityshouldbeprovidedtolendthestorya
feelingofrealnessandbelievability.
• LanguageUse:Thelanguageusedshouldfeelvariedandrich:Varianceofsentence
structure,verbiage,andvocabulary. Thestoryshouldexhibitrhetorical,linguistic
and literary devices (e.g., ambiguity, alliteration, etc) to create interesting effects.
The story should avoid bland or repetitive phrases (unless used intentionally to
createanarrative,thematic,orlinguisticeffect).
Provideadetailedassessmentofthetwostoriesintermsofthesefourdimensions.Conclude
yourassessmentwithscoresforeachdimensionusingthetemplatebelow. Donotaddany
emphasis,suchasboldanditalics,onyourassessment.
[AssessmentEndingTemplate]
Basedonmyassessment,thebetterstoryforeachdimensionis:
Plot: [AorBorSame]
Creativity: [AorBorSame]
Development: [AorBorSame]
LanguageUse: [AorBorSame]
Overall: [AorBorSame]
[StoryA]
<storya>
[StoryB]
<storyb>
[Assessment]
BelowweshowexampleoutputfromtheLLMevaluatorfromwhichweextractpreferencescores.
Inthiscase,theevaluatorpreferredstoryAonalldimensions.
30UnderreviewasaconferencepaperatICLR2025
[LLMEVALUATOR]ExampleOutput
StoryAhasamoredefinedplotstructurewithaclearbeginning,risingaction,climax,and
resolution. The summoning of Horus, his reaction to the modern world, and his eventual
acceptance create a compelling narrative arc. Story B, while touching upon interesting
themes,lacksaclearclimaxandresolution. Thestoryfeelsincomplete,leavingthereader
withunansweredquestionsaboutHorus’sfateandAsh’sjourney.
Story A exhibits more creativity in its portrayal of Horus and his interactions with
Amelia. Thefalcongod’sbewilderment, anger, andeventualacceptancearedepictedwith
nuance and depth. The story also introduces original elements, such as the restoration of
thecanopicjar,whichaddsalayerofsymbolismandreinforcesthethemeofreconciliation.
StoryBreliesonmoregenericdepictionsofagod’sreactiontothemodernworld,lacking
theoriginalityanddepthseeninStoryA.
Both stories provide adequate development of their main characters, Amelia and Ash,
respectively. However, Story A excels in its development of Horus, providing a more
nuancedandbelievableportrayalofagod grappling withtherealitiesofachangedworld.
Thestorydelvesintohisemotions,hisstruggles,andhiseventualacceptance,makinghim
a more compelling and relatable character. Story B’s portrayal of Horus, while touching
uponhisdisillusionment,lacksthesamelevelofdepthandcomplexity.
Story A utilizes a richer and more varied language, employing vivid imagery, sen-
sorydetails,andawiderrangeofvocabulary. Thestoryeffectivelyusesrhetoricaldevices,
such as repetition and contrast, to emphasize the differences between the ancient and
modern worlds. Story B, while adequately written, lacks the same level of linguistic
richnessandsophistication.
Basedonmyassessment,thebetterstoryforeachdimensionis:
Plot: A
Creativity: A
Development: A
LanguageUse: A
Overall: A
31