[
    {
        "title": "Estimating Body and Hand Motion in an Ego-sensed World",
        "authors": "Brent YiVickie YeMaya ZhengLea MüllerGeorgios PavlakosYi MaJitendra MalikAngjoo Kanazawa",
        "links": "http://arxiv.org/abs/2410.03665v1",
        "entry_id": "http://arxiv.org/abs/2410.03665v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03665v1",
        "summary": "We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture the wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve the hands: the resulting kinematic and temporal\nconstraints result in over 40% lower hand estimation errors compared to noisy\nmonocular estimates. Project page: https://egoallo.github.io/",
        "updated": "2024-10-04 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03665v1"
    },
    {
        "title": "Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models",
        "authors": "Zhuochun LiYuelyu JiRui MengDaqing He",
        "links": "http://arxiv.org/abs/2410.03663v1",
        "entry_id": "http://arxiv.org/abs/2410.03663v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03663v1",
        "summary": "Large language models (LLMs) have exhibited complex reasoning abilities by\ngenerating question rationales and demonstrated exceptional performance in\nnatural language processing (NLP) tasks. However, these reasoning capabilities\ngenerally emerge in models with tens of billions of parameters, creating\nsignificant computational challenges for real-world deployment. Recent research\nhas concentrated on improving open-source smaller models through knowledge\ndistillation (KD) from commercial LLMs. Nevertheless, most of these studies\nrely solely on the responses from one single LLM as the gold rationale for\ntraining. In this paper, we introduce a novel Mistake-Aware Peer-Review\nDistillation (MAPD) approach: 1) Instead of merely obtaining gold rationales\nfrom teachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data. 2) We design a\nsimulated peer-review process between teacher LLMs, which selects only the\ngenerated rationales above the acceptance threshold. This reduces the chance of\nteachers guessing correctly with flawed rationale, improving instructional data\nquality. Comprehensive experiments and analysis on mathematical, commonsense,\nand logical reasoning tasks demonstrate the effectiveness of our method.",
        "updated": "2024-10-04 17:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03663v1"
    },
    {
        "title": "System 2 reasoning capabilities are nigh",
        "authors": "Scott C. Lowe",
        "links": "http://arxiv.org/abs/2410.03662v1",
        "entry_id": "http://arxiv.org/abs/2410.03662v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03662v1",
        "summary": "In recent years, machine learning models have made strides towards human-like\nreasoning capabilities from several directions. In this work, we review the\ncurrent state of the literature and describe the remaining steps to achieve a\nneural model which can perform System 2 reasoning analogous to a human. We\nargue that if current models are insufficient to be classed as performing\nreasoning, there remains very little additional progress needed to attain that\ngoal.",
        "updated": "2024-10-04 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03662v1"
    },
    {
        "title": "Geometric Representation Condition Improves Equivariant Molecule Generation",
        "authors": "Zian LiCai ZhouXiyuan WangXingang PengMuhan Zhang",
        "links": "http://arxiv.org/abs/2410.03655v1",
        "entry_id": "http://arxiv.org/abs/2410.03655v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03655v1",
        "summary": "Recent advancements in molecular generative models have demonstrated\nsubstantial potential in accelerating scientific discovery, particularly in\ndrug design. However, these models often face challenges in generating\nhigh-quality molecules, especially in conditional scenarios where specific\nmolecular properties must be satisfied. In this work, we introduce GeoRCG, a\ngeneral framework to enhance the performance of molecular generative models by\nintegrating geometric representation conditions. We decompose the molecule\ngeneration process into two stages: first, generating an informative geometric\nrepresentation; second, generating a molecule conditioned on the\nrepresentation. Compared to directly generating a molecule, the relatively\neasy-to-generate representation in the first-stage guides the second-stage\ngeneration to reach a high-quality molecule in a more goal-oriented and much\nfaster way. Leveraging EDM as the base generator, we observe significant\nquality improvements in unconditional molecule generation on the widely-used\nQM9 and GEOM-DRUG datasets. More notably, in the challenging conditional\nmolecular generation task, our framework achieves an average 31\\% performance\nimprovement over state-of-the-art approaches, highlighting the superiority of\nconditioning on semantically rich geometric representations over conditioning\non individual property values as in previous approaches. Furthermore, we show\nthat, with such representation guidance, the number of diffusion steps can be\nreduced to as small as 100 while maintaining superior generation quality than\nthat achieved with 1,000 steps, thereby significantly accelerating the\ngeneration process.",
        "updated": "2024-10-04 17:57:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03655v1"
    },
    {
        "title": "GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs",
        "authors": "Pu HuaMinghuan LiuAnnabella MacalusoYunfeng LinWeinan ZhangHuazhe XuLirui Wang",
        "links": "http://arxiv.org/abs/2410.03645v1",
        "entry_id": "http://arxiv.org/abs/2410.03645v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03645v1",
        "summary": "Robotic simulation today remains challenging to scale up due to the human\nefforts required to create diverse simulation tasks and scenes.\nSimulation-trained policies also face scalability issues as many sim-to-real\nmethods focus on a single task. To address these challenges, this work proposes\nGenSim2, a scalable framework that leverages coding LLMs with multi-modal and\nreasoning capabilities for complex and realistic simulation task creation,\nincluding long-horizon tasks with articulated objects. To automatically\ngenerate demonstration data for these tasks at scale, we propose planning and\nRL solvers that generalize within object categories. The pipeline can generate\ndata for up to 100 articulated tasks with 200 objects and reduce the required\nhuman efforts. To utilize such data, we propose an effective multi-task\nlanguage-conditioned policy architecture, dubbed proprioceptive point-cloud\ntransformer (PPT), that learns from the generated demonstrations and exhibits\nstrong sim-to-real zero-shot transfer. Combining the proposed pipeline and the\npolicy architecture, we show a promising usage of GenSim2 that the generated\ndata can be used for zero-shot transfer or co-train with real-world collected\ndata, which enhances the policy performance by 20% compared with training\nexclusively on limited real data.",
        "updated": "2024-10-04 17:51:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03645v1"
    }
]