[
    {
        "title": "Minimax-optimal trust-aware multi-armed bandits",
        "authors": "Changxiao CaiJiacheng Zhang",
        "links": "http://arxiv.org/abs/2410.03651v1",
        "entry_id": "http://arxiv.org/abs/2410.03651v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03651v1",
        "summary": "Multi-armed bandit (MAB) algorithms have achieved significant success in\nsequential decision-making applications, under the premise that humans\nperfectly implement the recommended policy. However, existing methods often\noverlook the crucial factor of human trust in learning algorithms. When trust\nis lacking, humans may deviate from the recommended policy, leading to\nundesired learning performance. Motivated by this gap, we study the trust-aware\nMAB problem by integrating a dynamic trust model into the standard MAB\nframework. Specifically, it assumes that the recommended and actually\nimplemented policy differs depending on human trust, which in turn evolves with\nthe quality of the recommended policy. We establish the minimax regret in the\npresence of the trust issue and demonstrate the suboptimality of vanilla MAB\nalgorithms such as the upper confidence bound (UCB) algorithm. To overcome this\nlimitation, we introduce a novel two-stage trust-aware procedure that provably\nattains near-optimal statistical guarantees. A simulation study is conducted to\nillustrate the benefits of our proposed algorithm when dealing with the trust\nissue.",
        "updated": "2024-10-04 17:55:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03651v1"
    },
    {
        "title": "How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework",
        "authors": "Yinuo RenHaoxuan ChenGrant M. RotskoffLexing Ying",
        "links": "http://arxiv.org/abs/2410.03601v1",
        "entry_id": "http://arxiv.org/abs/2410.03601v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03601v1",
        "summary": "Discrete diffusion models have gained increasing attention for their ability\nto model complex distributions with tractable sampling and inference. However,\nthe error analysis for discrete diffusion models remains less well-understood.\nIn this work, we propose a comprehensive framework for the error analysis of\ndiscrete diffusion models based on L\\'evy-type stochastic integrals. By\ngeneralizing the Poisson random measure to that with a time-independent and\nstate-dependent intensity, we rigorously establish a stochastic integral\nformulation of discrete diffusion models and provide the corresponding change\nof measure theorems that are intriguingly analogous to It\\^o integrals and\nGirsanov's theorem for their continuous counterparts. Our framework unifies and\nstrengthens the current theoretical results on discrete diffusion models and\nobtains the first error bound for the $\\tau$-leaping scheme in KL divergence.\nWith error sources clearly identified, our analysis gives new insight into the\nmathematical properties of discrete diffusion models and offers guidance for\nthe design of efficient and accurate algorithms for real-world discrete\ndiffusion model applications.",
        "updated": "2024-10-04 16:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03601v1"
    },
    {
        "title": "Nonstationary Sparse Spectral Permanental Process",
        "authors": "Zicheng SunYixuan ZhangZenan LingXuhui FanFeng Zhou",
        "links": "http://arxiv.org/abs/2410.03581v1",
        "entry_id": "http://arxiv.org/abs/2410.03581v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03581v1",
        "summary": "Existing permanental processes often impose constraints on kernel types or\nstationarity, limiting the model's expressiveness. To overcome these\nlimitations, we propose a novel approach utilizing the sparse spectral\nrepresentation of nonstationary kernels. This technique relaxes the constraints\non kernel types and stationarity, allowing for more flexible modeling while\nreducing computational complexity to the linear level. Additionally, we\nintroduce a deep kernel variant by hierarchically stacking multiple spectral\nfeature mappings, further enhancing the model's expressiveness to capture\ncomplex patterns in data. Experimental results on both synthetic and real-world\ndatasets demonstrate the effectiveness of our approach, particularly in\nscenarios with pronounced data nonstationarity. Additionally, ablation studies\nare conducted to provide insights into the impact of various hyperparameters on\nmodel performance.",
        "updated": "2024-10-04 16:40:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03581v1"
    },
    {
        "title": "On the Hardness of Learning One Hidden Layer Neural Networks",
        "authors": "Shuchen LiIlias ZadikManolis Zampetakis",
        "links": "http://arxiv.org/abs/2410.03477v1",
        "entry_id": "http://arxiv.org/abs/2410.03477v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03477v1",
        "summary": "In this work, we consider the problem of learning one hidden layer ReLU\nneural networks with inputs from $\\mathbb{R}^d$. We show that this learning\nproblem is hard under standard cryptographic assumptions even when: (1) the\nsize of the neural network is polynomial in $d$, (2) its input distribution is\na standard Gaussian, and (3) the noise is Gaussian and polynomially small in\n$d$. Our hardness result is based on the hardness of the Continuous Learning\nwith Errors (CLWE) problem, and in particular, is based on the largely believed\nworst-case hardness of approximately solving the shortest vector problem up to\na multiplicative polynomial factor.",
        "updated": "2024-10-04 14:48:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03477v1"
    },
    {
        "title": "Linear Transformer Topological Masking with Graph Random Features",
        "authors": "Isaac ReidKumar Avinava DubeyDeepali JainWill WhitneyAmr AhmedJoshua AinslieAlex BewleyMithun JacobAranyak MehtaDavid RendlemanConnor SchenckRichard E. TurnerRené WagnerAdrian WellerKrzysztof Choromanski",
        "links": "http://arxiv.org/abs/2410.03462v1",
        "entry_id": "http://arxiv.org/abs/2410.03462v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03462v1",
        "summary": "When training transformers on graph-structured data, incorporating\ninformation about the underlying topology is crucial for good performance.\nTopological masking, a type of relative position encoding, achieves this by\nupweighting or downweighting attention depending on the relationship between\nthe query and keys in a graph. In this paper, we propose to parameterise\ntopological masks as a learnable function of a weighted adjacency matrix -- a\nnovel, flexible approach which incorporates a strong structural inductive bias.\nBy approximating this mask with graph random features (for which we prove the\nfirst known concentration bounds), we show how this can be made fully\ncompatible with linear attention, preserving $\\mathcal{O}(N)$ time and space\ncomplexity with respect to the number of input tokens. The fastest previous\nalternative was $\\mathcal{O}(N \\log N)$ and only suitable for specific graphs.\nOur efficient masking algorithms provide strong performance gains for tasks on\nimage and point cloud data, including with $>30$k nodes.",
        "updated": "2024-10-04 14:24:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03462v1"
    }
]