[
    {
        "title": "Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models",
        "authors": "Zhuochun LiYuelyu JiRui MengDaqing He",
        "links": "http://arxiv.org/abs/2410.03663v1",
        "entry_id": "http://arxiv.org/abs/2410.03663v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03663v1",
        "summary": "Large language models (LLMs) have exhibited complex reasoning abilities by\ngenerating question rationales and demonstrated exceptional performance in\nnatural language processing (NLP) tasks. However, these reasoning capabilities\ngenerally emerge in models with tens of billions of parameters, creating\nsignificant computational challenges for real-world deployment. Recent research\nhas concentrated on improving open-source smaller models through knowledge\ndistillation (KD) from commercial LLMs. Nevertheless, most of these studies\nrely solely on the responses from one single LLM as the gold rationale for\ntraining. In this paper, we introduce a novel Mistake-Aware Peer-Review\nDistillation (MAPD) approach: 1) Instead of merely obtaining gold rationales\nfrom teachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data. 2) We design a\nsimulated peer-review process between teacher LLMs, which selects only the\ngenerated rationales above the acceptance threshold. This reduces the chance of\nteachers guessing correctly with flawed rationale, improving instructional data\nquality. Comprehensive experiments and analysis on mathematical, commonsense,\nand logical reasoning tasks demonstrate the effectiveness of our method.",
        "updated": "2024-10-04 17:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03663v1"
    },
    {
        "title": "Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models",
        "authors": "Tinghui ZhuQin LiuFei WangZhengzhong TuMuhao Chen",
        "links": "http://arxiv.org/abs/2410.03659v1",
        "entry_id": "http://arxiv.org/abs/2410.03659v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03659v1",
        "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%.",
        "updated": "2024-10-04 17:59:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03659v1"
    },
    {
        "title": "RAFT: Realistic Attacks to Fool Text Detectors",
        "authors": "James WangRan LiJunfeng YangChengzhi Mao",
        "links": "http://arxiv.org/abs/2410.03658v1",
        "entry_id": "http://arxiv.org/abs/2410.03658v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03658v1",
        "summary": "Large language models (LLMs) have exhibited remarkable fluency across various\ntasks. However, their unethical applications, such as disseminating\ndisinformation, have become a growing concern. Although recent works have\nproposed a number of LLM detection methods, their robustness and reliability\nremain unclear. In this paper, we present RAFT: a grammar error-free black-box\nattack against existing LLM detectors. In contrast to previous attacks for\nlanguage models, our method exploits the transferability of LLM embeddings at\nthe word-level while preserving the original text quality. We leverage an\nauxiliary embedding to greedily select candidate words to perturb against the\ntarget detector. Experiments reveal that our attack effectively compromises all\ndetectors in the study across various domains by up to 99%, and are\ntransferable across source models. Manual human evaluation studies show our\nattacks are realistic and indistinguishable from original human-written text.\nWe also show that examples generated by RAFT can be used to train adversarially\nrobust detectors. Our work shows that current LLM detectors are not\nadversarially robust, underscoring the urgent need for more resilient detection\nmechanisms.",
        "updated": "2024-10-04 17:59:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03658v1"
    },
    {
        "title": "Aligning LLMs with Individual Preferences via Interaction",
        "authors": "Shujin WuMay FungCheng QianJeonghwan KimDilek Hakkani-TurHeng Ji",
        "links": "http://arxiv.org/abs/2410.03642v1",
        "entry_id": "http://arxiv.org/abs/2410.03642v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03642v1",
        "summary": "As large language models (LLMs) demonstrate increasingly advanced\ncapabilities, aligning their behaviors with human values and preferences\nbecomes crucial for their wide adoption. While previous research focuses on\ngeneral alignment to principles such as helpfulness, harmlessness, and honesty,\nthe need to account for individual and diverse preferences has been largely\noverlooked, potentially undermining customized human experiences. To address\nthis gap, we train LLMs that can ''interact to align'', essentially cultivating\nthe meta-skill of LLMs to implicitly infer the unspoken personalized\npreferences of the current user through multi-turn conversations, and then\ndynamically align their following behaviors and responses to these inferred\npreferences. Our approach involves establishing a diverse pool of 3,310\ndistinct user personas by initially creating seed examples, which are then\nexpanded through iterative self-generation and filtering. Guided by distinct\nuser personas, we leverage multi-LLM collaboration to develop a multi-turn\npreference dataset containing 3K+ multi-turn conversations in tree structures.\nFinally, we apply supervised fine-tuning and reinforcement learning to enhance\nLLMs using this dataset. For evaluation, we establish the ALOE (ALign With\nCustOmized PrEferences) benchmark, consisting of 100 carefully selected\nexamples and well-designed metrics to measure the customized alignment\nperformance during conversations. Experimental results demonstrate the\neffectiveness of our method in enabling dynamic, personalized alignment via\ninteraction.",
        "updated": "2024-10-04 17:48:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03642v1"
    },
    {
        "title": "What Matters for Model Merging at Scale?",
        "authors": "Prateek YadavTu VuJonathan LaiAlexandra ChronopoulouManaal FaruquiMohit BansalTsendsuren Munkhdalai",
        "links": "http://arxiv.org/abs/2410.03617v1",
        "entry_id": "http://arxiv.org/abs/2410.03617v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03617v1",
        "summary": "Model merging aims to combine multiple expert models into a more capable\nsingle model, offering benefits such as reduced storage and serving costs,\nimproved generalization, and support for decentralized model development.\nDespite its promise, previous studies have primarily focused on merging a few\nsmall models. This leaves many unanswered questions about the effect of scaling\nmodel size and how it interplays with other key factors -- like the base model\nquality and number of expert models -- , to affect the merged model's\nperformance. This work systematically evaluates the utility of model merging at\nscale, examining the impact of these different factors. We experiment with\nmerging fully fine-tuned models using 4 popular merging methods -- Averaging,\nTask~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B\nparameters and merging up to 8 different expert models. We evaluate the merged\nmodels on both held-in tasks, i.e., the expert's training tasks, and zero-shot\ngeneralization to unseen held-out tasks. Our experiments provide several new\ninsights about model merging at scale and the interplay between different\nfactors. First, we find that merging is more effective when experts are created\nfrom strong base models, i.e., models with good zero-shot performance. Second,\nlarger models facilitate easier merging. Third merging consistently improves\ngeneralization capabilities. Notably, when merging 8 large expert models, the\nmerged models often generalize better compared to the multitask trained models.\nFourth, we can better merge more expert models when working with larger models.\nFifth, different merging methods behave very similarly at larger scales.\nOverall, our findings shed light on some interesting properties of model\nmerging while also highlighting some limitations. We hope that this study will\nserve as a reference point on large-scale merging for upcoming research.",
        "updated": "2024-10-04 17:17:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03617v1"
    }
]