[
    {
        "title": "Estimating Body and Hand Motion in an Ego-sensed World",
        "authors": "Brent YiVickie YeMaya ZhengLea MüllerGeorgios PavlakosYi MaJitendra MalikAngjoo Kanazawa",
        "links": "http://arxiv.org/abs/2410.03665v1",
        "entry_id": "http://arxiv.org/abs/2410.03665v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03665v1",
        "summary": "We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture the wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve the hands: the resulting kinematic and temporal\nconstraints result in over 40% lower hand estimation errors compared to noisy\nmonocular estimates. Project page: https://egoallo.github.io/",
        "updated": "2024-10-04 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03665v1"
    },
    {
        "title": "Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models",
        "authors": "Tinghui ZhuQin LiuFei WangZhengzhong TuMuhao Chen",
        "links": "http://arxiv.org/abs/2410.03659v1",
        "entry_id": "http://arxiv.org/abs/2410.03659v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03659v1",
        "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%.",
        "updated": "2024-10-04 17:59:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03659v1"
    },
    {
        "title": "GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs",
        "authors": "Pu HuaMinghuan LiuAnnabella MacalusoYunfeng LinWeinan ZhangHuazhe XuLirui Wang",
        "links": "http://arxiv.org/abs/2410.03645v1",
        "entry_id": "http://arxiv.org/abs/2410.03645v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03645v1",
        "summary": "Robotic simulation today remains challenging to scale up due to the human\nefforts required to create diverse simulation tasks and scenes.\nSimulation-trained policies also face scalability issues as many sim-to-real\nmethods focus on a single task. To address these challenges, this work proposes\nGenSim2, a scalable framework that leverages coding LLMs with multi-modal and\nreasoning capabilities for complex and realistic simulation task creation,\nincluding long-horizon tasks with articulated objects. To automatically\ngenerate demonstration data for these tasks at scale, we propose planning and\nRL solvers that generalize within object categories. The pipeline can generate\ndata for up to 100 articulated tasks with 200 objects and reduce the required\nhuman efforts. To utilize such data, we propose an effective multi-task\nlanguage-conditioned policy architecture, dubbed proprioceptive point-cloud\ntransformer (PPT), that learns from the generated demonstrations and exhibits\nstrong sim-to-real zero-shot transfer. Combining the proposed pipeline and the\npolicy architecture, we show a promising usage of GenSim2 that the generated\ndata can be used for zero-shot transfer or co-train with real-world collected\ndata, which enhances the policy performance by 20% compared with training\nexclusively on limited real data.",
        "updated": "2024-10-04 17:51:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03645v1"
    },
    {
        "title": "Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need",
        "authors": "Xianlong WangMinghui LiWei LiuHangtao ZhangShengshan HuYechao ZhangZiqi ZhouHai Jin",
        "links": "http://arxiv.org/abs/2410.03644v1",
        "entry_id": "http://arxiv.org/abs/2410.03644v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03644v1",
        "summary": "Traditional unlearnable strategies have been proposed to prevent unauthorized\nusers from training on the 2D image data. With more 3D point cloud data\ncontaining sensitivity information, unauthorized usage of this new type data\nhas also become a serious concern. To address this, we propose the first\nintegral unlearnable framework for 3D point clouds including two processes: (i)\nwe propose an unlearnable data protection scheme, involving a class-wise\nsetting established by a category-adaptive allocation strategy and\nmulti-transformations assigned to samples; (ii) we propose a data restoration\nscheme that utilizes class-wise inverse matrix transformation, thus enabling\nauthorized-only training for unlearnable data. This restoration process is a\npractical issue overlooked in most existing unlearnable literature, \\ie, even\nauthorized users struggle to gain knowledge from 3D unlearnable data. Both\ntheoretical and empirical results (including 6 datasets, 16 models, and 2\ntasks) demonstrate the effectiveness of our proposed unlearnable framework. Our\ncode is available at \\url{https://github.com/CGCL-codes/UnlearnablePC}",
        "updated": "2024-10-04 17:49:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03644v1"
    },
    {
        "title": "HyperCMR: Enhanced Multi-Contrast CMR Reconstruction with Eagle Loss",
        "authors": "Ruru XuCaner ÖzerIlkay Oksuz",
        "links": "http://arxiv.org/abs/2410.03624v1",
        "entry_id": "http://arxiv.org/abs/2410.03624v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03624v1",
        "summary": "Accelerating image acquisition for cardiac magnetic resonance imaging (CMRI)\nis a critical task. CMRxRecon2024 challenge aims to set the state of the art\nfor multi-contrast CMR reconstruction. This paper presents HyperCMR, a novel\nframework designed to accelerate the reconstruction of multi-contrast cardiac\nmagnetic resonance (CMR) images. HyperCMR enhances the existing PromptMR model\nby incorporating advanced loss functions, notably the innovative Eagle Loss,\nwhich is specifically designed to recover missing high-frequency information in\nundersampled k-space. Extensive experiments conducted on the CMRxRecon2024\nchallenge dataset demonstrate that HyperCMR consistently outperforms the\nbaseline across multiple evaluation metrics, achieving superior SSIM and PSNR\nscores.",
        "updated": "2024-10-04 17:29:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03624v1"
    }
]