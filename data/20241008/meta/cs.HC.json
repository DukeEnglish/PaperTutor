[
    {
        "title": "Aligning LLMs with Individual Preferences via Interaction",
        "authors": "Shujin WuMay FungCheng QianJeonghwan KimDilek Hakkani-TurHeng Ji",
        "links": "http://arxiv.org/abs/2410.03642v1",
        "entry_id": "http://arxiv.org/abs/2410.03642v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03642v1",
        "summary": "As large language models (LLMs) demonstrate increasingly advanced\ncapabilities, aligning their behaviors with human values and preferences\nbecomes crucial for their wide adoption. While previous research focuses on\ngeneral alignment to principles such as helpfulness, harmlessness, and honesty,\nthe need to account for individual and diverse preferences has been largely\noverlooked, potentially undermining customized human experiences. To address\nthis gap, we train LLMs that can ''interact to align'', essentially cultivating\nthe meta-skill of LLMs to implicitly infer the unspoken personalized\npreferences of the current user through multi-turn conversations, and then\ndynamically align their following behaviors and responses to these inferred\npreferences. Our approach involves establishing a diverse pool of 3,310\ndistinct user personas by initially creating seed examples, which are then\nexpanded through iterative self-generation and filtering. Guided by distinct\nuser personas, we leverage multi-LLM collaboration to develop a multi-turn\npreference dataset containing 3K+ multi-turn conversations in tree structures.\nFinally, we apply supervised fine-tuning and reinforcement learning to enhance\nLLMs using this dataset. For evaluation, we establish the ALOE (ALign With\nCustOmized PrEferences) benchmark, consisting of 100 carefully selected\nexamples and well-designed metrics to measure the customized alignment\nperformance during conversations. Experimental results demonstrate the\neffectiveness of our method in enabling dynamic, personalized alignment via\ninteraction.",
        "updated": "2024-10-04 17:48:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03642v1"
    },
    {
        "title": "TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation",
        "authors": "Jonathan CookTim RocktäschelJakob FoersterDennis AumillerAlex Wang",
        "links": "http://arxiv.org/abs/2410.03608v1",
        "entry_id": "http://arxiv.org/abs/2410.03608v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03608v1",
        "summary": "Given the widespread adoption and usage of Large Language Models (LLMs), it\nis crucial to have flexible and interpretable evaluations of their\ninstruction-following ability. Preference judgments between model outputs have\nbecome the de facto evaluation standard, despite distilling complex,\nmulti-faceted preferences into a single ranking. Furthermore, as human\nannotation is slow and costly, LLMs are increasingly used to make these\njudgments, at the expense of reliability and interpretability. In this work, we\npropose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,\ninterpretable evaluation protocol that structures evaluations with\nLLM-generated, instruction-specific checklists. We first show that, given an\ninstruction, LLMs can reliably produce high-quality, tailored evaluation\nchecklists that decompose the instruction into a series of YES/NO questions.\nEach question asks whether a candidate response meets a specific requirement of\nthe instruction. We demonstrate that using TICK leads to a significant increase\n(46.4% $\\to$ 52.2%) in the frequency of exact agreements between LLM judgements\nand human preferences, as compared to having an LLM directly score an output.\nWe then show that STICK (Self-TICK) can be used to improve generation quality\nacross multiple benchmarks via self-refinement and Best-of-N selection. STICK\nself-refinement on LiveBench reasoning tasks leads to an absolute gain of\n$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute\nimprovement on the real-world instruction dataset, WildBench. In light of this,\nstructured, multi-faceted self-improvement is shown to be a promising way to\nfurther advance LLM capabilities. Finally, by providing LLM-generated\nchecklists to human evaluators tasked with directly scoring LLM responses to\nWildBench instructions, we notably increase inter-annotator agreement (0.194\n$\\to$ 0.256).",
        "updated": "2024-10-04 17:09:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03608v1"
    },
    {
        "title": "Generative AI in the Software Engineering Domain: Tensions of Occupational Identity and Patterns of Identity Protection",
        "authors": "Anuschka SchmittKrzysztof Z. GajosOsnat Mokryn",
        "links": "http://arxiv.org/abs/2410.03571v1",
        "entry_id": "http://arxiv.org/abs/2410.03571v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03571v1",
        "summary": "The adoption of generative Artificial Intelligence (GAI) in organizational\nsettings calls into question workers' roles, and relatedly, the implications\nfor their long-term skill development and domain expertise. In our qualitative\nstudy in the software engineering domain, we build on the theoretical lenses of\noccupational identity and self-determination theory to understand how and why\nsoftware engineers make sense of GAI for their work. We find that engineers'\nsense-making is contingent on domain expertise, as juniors and seniors felt\ntheir needs for competence, autonomy, and relatedness to be differently\nimpacted by GAI. We shed light on the importance of the individual's role in\npreserving tacit domain knowledge as engineers engaged in sense-making that\nprotected their occupational identity. We illustrate how organizations play an\nactive role in shaping workers' sense-making process and propose design\nguidelines on how organizations and system designers can facilitate the impact\nof technological change on workers' occupational identity.",
        "updated": "2024-10-04 16:20:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03571v1"
    },
    {
        "title": "Artificial Human Lecturers: Initial Findings From Asia's First AI Lecturers in Class to Promote Innovation in Education",
        "authors": "Ching Christie PangYawei ZhaoZhizhuo YinJia SunReza Hadi MogaviPan Hui",
        "links": "http://arxiv.org/abs/2410.03525v1",
        "entry_id": "http://arxiv.org/abs/2410.03525v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03525v1",
        "summary": "In recent years, artificial intelligence (AI) has become increasingly\nintegrated into education, reshaping traditional learning environments. Despite\nthis, there has been limited investigation into fully operational artificial\nhuman lecturers. To the best of our knowledge, our paper presents the world's\nfirst study examining their deployment in a real-world educational setting.\nSpecifically, we investigate the use of \"digital teachers,\" AI-powered virtual\nlecturers, in a postgraduate course at the Hong Kong University of Science and\nTechnology (HKUST). Our study explores how features such as appearance,\nnon-verbal cues, voice, and verbal expression impact students' learning\nexperiences. Findings suggest that students highly value naturalness,\nauthenticity, and interactivity in digital teachers, highlighting areas for\nimprovement, such as increased responsiveness, personalized avatars, and\nintegration with larger learning platforms. We conclude that digital teachers\nhave significant potential to enhance education by providing a more flexible,\nengaging, personalized, and accessible learning experience for students.",
        "updated": "2024-10-04 15:45:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03525v1"
    },
    {
        "title": "How Toxicity Classifiers and Large Language Models Respond to Ableism",
        "authors": "Mahika PhutaneAnanya SeelamAditya Vashistha",
        "links": "http://arxiv.org/abs/2410.03448v1",
        "entry_id": "http://arxiv.org/abs/2410.03448v1",
        "pdf_url": "http://arxiv.org/pdf/2410.03448v1",
        "summary": "People with disabilities (PwD) regularly encounter ableist hate and\nmicroaggressions online. While online platforms use machine learning models to\nmoderate online harm, there is little research investigating how these models\ninteract with ableism. In this paper, we curated a dataset of 100 social media\ncomments targeted towards PwD, and recruited 160 participants to rate and\nexplain how toxic and ableist these comments were. We then prompted\nstate-of-the art toxicity classifiers (TCs) and large language models (LLMs) to\nrate and explain the harm. Our analysis revealed that TCs and LLMs rated\ntoxicity significantly lower than PwD, but LLMs rated ableism generally on par\nwith PwD. However, ableism explanations by LLMs overlooked emotional harm, and\nlacked specificity and acknowledgement of context, important facets of PwD\nexplanations. Going forward, we discuss challenges in designing\ndisability-aware toxicity classifiers, and advocate for the shift from ableism\ndetection to ableism interpretation and explanation.",
        "updated": "2024-10-04 14:09:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.03448v1"
    }
]