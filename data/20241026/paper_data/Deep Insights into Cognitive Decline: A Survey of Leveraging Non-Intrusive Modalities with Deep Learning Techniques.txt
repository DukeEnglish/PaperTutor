Deep Insights into Cognitive Decline: A Survey of
Leveraging Non-Intrusive Modalities with Deep
Learning Techniques
1st David Ortiz-Perez 2nd Manuel Benavent-Lledo
Dept. of Computer Science and Technology Dept. of Computer Science and Technology
University of Alicante University of Alicante
Alicante, Spain Alicante, Spain
dortiz@dtic.ua.es mbenavent@dtic.ua.es
3rd Jose Garcia-Rodriguez 4th David Toma´s 5th M. Flores Vizcaya-Moreno
Dept. of Computer Science and Technology Dept. of Software Unit of Clinical Nursing Research
University of Alicante and Computing Systems Faculty of Health Sciences
Alicante, Spain University of Alicante University of Alicante
jgarcia@dtic.ua.es Alicante, Spain Alicante, Spain
dtomas@dlsi.ua.es flores.vizcaya@ua.es
Abstract—Cognitive decline is a natural part of aging, often natural aspect of aging, some individuals may experience a
resulting in reduced cognitive abilities. In some cases, however, moreacceleratedorpronounceddeteriorationintheseabilities
this decline is more pronounced, typically due to disorders such
due to cognitive disorders, such as dementia [3]. The severity
as Alzheimer’s disease. Early detection of anomalous cognitive
of this deterioration spans a broad spectrum, with Mild Cog-
decline is crucial, as it can facilitate timely professional in-
tervention. While medical data can help in this detection, it nitive Impairment (MCI) representing an intermediate stage
often involves invasive procedures. An alternative approach is to between typical age-related decline and more severe forms,
employ non-intrusive techniques such as speech or handwriting including different types of dementia [4], [5]. The decline
analysis, which do not necessarily affect daily activities. This
in cognitive abilities due to aging represents a significant
survey reviews the most relevant methodologies that use deep
challenge for our society, particularly in light of the trend
learning techniques to automate the cognitive decline estimation
task, including audio, text, and visual processing. We discuss toward an aging population [6]. Nonetheless, not all disorders
the key features and advantages of each modality and method- affecting cognitive abilities are associated with age. One such
ology, including state-of-the-art approaches like Transformer disorder is aphasia, which is usually caused by strokes [7].
architecture and foundation models. In addition, we present
Recent technological advances, particularly deep learning,
works that integrate different modalities to develop multimodal
have the potential to revolutionize numerous fields, including
models. We also highlight the most significant datasets and the
quantitativeresultsfromstudiesusingtheseresources.Fromthis healthcare. The integration of automation into medical prac-
review, several conclusions emerge. In most cases, the textual tice offers promising opportunities to increase the efficiency
modality achieves the best results and is the most relevant and productivity of healthcare professionals [8], [9]. In the
for detecting cognitive decline. Moreover, combining various
context of cognitive decline estimation, these technologies
approaches from individual modalities into a multimodal model
havedemonstratedpotentialinfacilitatingearlydetection[10],
consistently enhances performance across nearly all scenarios.
IndexTerms—CognitiveDecline,DeepLearning,DataModal- [11], allowing timely intervention in the early stages of the
ity, Non-Intrusive Techniques disorder. Early detection is crucial, as it enables patients to
receive professional treatment from the onset of the disorder.
I. INTRODUCTION
This timely intervention is particularly important, as previous
COGNITIVE abilities play a crucial role in daily life, studies have shown that support from doctors, psychiatrists,
constituting a fundamental aspect of our interactions or psychologists can significantly improve patients’ quality of
withothers.Theseabilitiesincludereasoning,variousmemory life and help mitigate cognitive decline over time, ultimately
functions,attention,andlanguagecomprehensionandcommu- benefiting their cognitive and behavioral functioning [12].
nication [1]. As individuals age, cognitive function typically Deeplearningmethodsareknowntobedata-hungryandcan
undergoes a natural decline, with certain abilities, such as benefit from multiple data sources, each of which presents its
processing speed, reasoning, and memory, showing more pro- own trade-off between contribution to detection, cost, privacy,
nounced signs of deterioration, while others remain relatively and degree of intrusiveness to the patient. Many studies rely
unaffected. [2]. Despite cognitive decline is a common and onmedicaldata,includingMagneticResonanceImages(MRI)
4202
tcO
42
]GL.sc[
1v27981.0142:viXraof patients’ brains, to diagnose these conditions [13]–[19]. decline disorders and assessments; Section III details the
Despite providing relevant information on the disorders and methodology used to select the works included in this study;
their effects on patient’s brains, medical data collection is no- Section IV presents the most relevant datasets in this area;
tably invasive. Additionally, these approaches face limitations Section V explores unimodal approaches, while Section VI
in terms of feasibility in different contexts. As a result, this discussesthecombinationoftheseapproachesintomultimodal
studyfocusesonnon-invasivetechniquesthatavoiddisrupting frameworks; Section VII provides a brief discussion of the
patients’ daily routines. These alternative approaches rely on methodologies examined; finally, Section VIII presents the
videorecordings,integratingvisual,audio,andtextualdata.In conclusions drawn from this research.
thiscontext,patientprivacy,easeofdeployment,andensuring
minimal interference with daily activities are key factors.
II. BACKGROUNDCONCEPTS
Although various reviews and surveys have already ad- This section defines background concepts essential for un-
dressed cognitive decline estimation, most rely solely on derstanding how various disorders impact cognitive function
medical imaging, such as MRI or EHR (Electronic Health in patients. It introduces the most common disorders in this
Records) [20]–[22]. Imaging methods have been also studied context, along with the primary assessment methods used to
specifically for certain conditions, including Alzheimer’s or evaluate the cognitive state of individuals.
MCI [23]–[27]. Similarly, Alzheimer’s or Parkinson’s have
A. Cognitive disorders
also been investigated using non-intrusive modalities [28]–
[30]. However, we find a notable deficit in the literature Although cognitive function deteriorates over time, severe
regarding comprehensive studies on overall cognitive decline deterioration can occur, often due to cognitive disorders. This
estimation that leverage non-intrusive data modalities. This subsection briefly introduces the main disorders included in
work aims to fill this gap in the literature. this survey that can affect normal cognitive behavior.
1) Mild Cognitive Impairment (MCI): This syndrome is
In this study, we propose a survey focused on the early
characterized by a more significant cognitive decline than is
prediction of cognitive decline using deep learning methods
typicalforanindividualofasimilarageandeducationallevel.
and non-intrusive data. To achieve this, we systematically
However, this decline is less severe than that experienced in
review existing methodologies in this field and introduce
dementia and does not significantly interfere with activities of
the most relevant datasets for training and evaluating the
daily living [31], [32]. MCI often leads to dementia and typi-
performance of models designed to predict cognitive decline.
callyinvolvesdifficultieswithoneormorecognitivefunctions,
Additionally, we discuss the effectiveness of each modality
suchasmemory,learning,reasoning,attention,languageskills,
and identify the most suitable approaches for utilizing them.
or loss of interest or motivation [33].
Furthermore,weexplorevariousstrategiesforcombiningthese
modalities into multimodal models to leverage their unique 2) Dementia: IncontrasttoMCI,dementiaischaracterized
capabilities and features. Employing these modalities enables by a more pronounced decline in cognitive abilities, leading
a more comprehensive understanding of the issue, ultimately to significant challenges for patients in their daily lives [34].
leading to improved performance on a variety of tasks. In Approximately 70% of dementia cases are associated with
summary, our main contributions are as follows: Alzheimer’s disease, known for its characteristic memory
loss[35].However,Alzheimer’sdiseasealsoaffectsothercog-
• To the best of our knowledge, this is the first survey fo- nitive functions, including attention and language skills. As a
cusedonthegeneralestimationofcognitivedeclineusing
result,patientsoftenexperiencedifficultiesinarticulatingtheir
non-intrusivemodalitieswithdeeplearningmethods.The
thoughts effectively, having trouble finding the appropriate
modalities included in this survey are audio, text, video,
words [36].
and image.
3) Aphasia: This condition is caused by damage to the
• We systematically review the most relevant works in this brain regions, usually resulting from a stroke [7]. Aphasia
scope, categorizing them by modality and approach. Ad-
primarilyimpairsanindividual’sabilitytocommunicate,lead-
ditionally,wepresentthecombinationofthesemodalities
ing to difficulties in both speaking and understanding others’
into various multimodal models.
speech. Different types of aphasia affect speech in different
• We discuss the effectiveness and characteristics of each ways depending on the affected brain regions, resulting in
methodology, both for individual modalities and multi-
either simplified speech or overly complex sentences and the
modal fusions. This analysis allows us to identify the
use of invented words [37], [38].
mostpromisingchallengesandfutureresearchdirections.
4) Parkinson: Parkinson’s disease is a neurodegenerative
Furthermore, we highlight the key issues and difficulties
disease that affects the motor function of people who suffer
related to cognitive decline estimation tasks.
from it [39]. It causes unintended or uncontrollable move-
• We provide a comparative analysis of different ap- ments such as shaking, stiffness, and difficulty in balance and
proaches and their performance on standard datasets and
coordination. This disease is closely related to dementia, as
benchmarks, accompanied by a summary of the findings.
manypeoplewithParkinson’sdiseasealsosufferfromit[40].
The remainder of the paper is structured as follows: Sec- AlthoughParkinson’saffectsthenervoussystemprimarily,itis
tion II introduces background concepts related to cognitive alsoassociatedwithaseveredeclineincognitivefunction[41].5) Apathy: Although not a disease, apathy is present in a
Number of Studies per Year
variety of disorders, including dementia [42]. It refers to a
lack of interest or emotion and is associated with cognitive 600
decline. Apathy negatively impacts global cognition, verbal
fluency, and visual and verbal memory [43], [44].
400
B. Cognitive tests
Some tools allow us to accurately measure the cognitive 200
decline over time and identify significant decreases. It is
crucial to closely monitor the cognitive status of patients with
0
cognitive disorders to track progression. The most relevant 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024
measuresincludetheMini-MentalStateExamination(MMSE) Year
and the Montreal Cognitive Assessment (MoCA) [45].
TheMMSEconsistsof11questionscoveringfivedomains: Fig.1. Numberofstudiesidentifiedduringthescreeningprocessperyear.
orientation, registration, attention/calculation, recall, and lan-
guage[46].AperfectscoreontheMMSEis30,andascoreof
25 or higher usually indicates normal cognitive function [47].
The MMSE is primarily used to assess the risk of dementia.
Incontrast, theMoCA isdesigned todetect MildCognitive Records identified through database search
(n = 1938)
Impairmentandearlystagesofdementia[48].TheMoCAtest
includesseven domains:executive/visuospatial function,nam-
ing, attention, language, abstraction, recall, and orientation.
Records after removing duplicates
Similar to the MMSE, the MoCA also has a maximum score (n = 1208)
of 30, but it is composed of 30 questions [49].
Records screened Records excluded
(n = 1208) (n = 1013)
III. REVIEWMETHODOLOGY
A systematic review of existing methodologies in the auto-
matic cognitive decline estimation task using Deep Learning
Full-text articles
Full-text articles assessed for eligibility
techniques was conducted to identify and analyze the most excluded
(n = 195)
(n = 152)
relevant works. This review involved searching for articles in
various databases: PubMed1, Scopus2, Web of Science3, and
IEEE Xplore4. The search was limited to English-language
articles published from 2018 to the present. These dates were
Articles included in the final collection
chosenduetotheintroductionofthewell-knownTransformer (n = 43)
model in 2017 [50], which outperforms many earlier meth-
ods. Additionally, there has been a significant increase in
publications in our area of interest since that year compared
to previous years, as shown in Figure 1. The search was Fig.2. PRISMAflowoftheselectionprocess.
performed on October 1, 2024, and was based on titles,
abstracts, and metadata. We followed the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) OR “aphasia” OR “alzheimer” OR “dementia” OR “MCI”
approach [51]. An overview of the selection process is de- OR “cognitive impairment”) AND ( “deep learning” OR
picted in Figure 2. “CNN”OR“LSTM”OR“transformer”)AND(“multimodal”
The search strategy included several keywords related to OR “speech” OR “video” OR “RGB” OR “linguistic” OR
cognitive decline, such as “cognitive decline”, “aphasia”, “audio”)”.
“alzheimer”,“dementia”,“MCI”,and“cognitiveimpairment”. After screening the different works, the first step was to
Thesekeywordshavebeencombinedwithdeeplearningterms, remove duplicates from the different databases. This was
suchas“deeplearning”,“CNN”,“LSTM”,and“transformer”. followed by a manual review to assess whether the remain-
Finally, also with the non-intrusive modalities we are tar- ing papers met the inclusion criteria. Specifically, included
geting, such as “multimodal”, “speech”, “video”, “RGB”, works were required to propose, train, and test an artificial
“linguistic”, and “audio”. The final query used over the intelligence model, excluding reviews or surveys that do not
previously mentioned databases was: “(“cognitive decline” train models. Additionally, the selected papers must use the
selected non-invasive modalities, excluding any other. The
1https://pubmed.ncbi.nlm.nih.gov/
final criterion focused on the detection of cognitive decline,
2http://www.scopus.com/
3http://www.isiknowledge.com/ thereby excluding studies that used the specified modalities
4http://ieeexplore.ieee.org/ for assistive purposes rather than for detection. The resulting
seiduts
fo
rebmuN
noitacifitnedI
gnineercS
ytilibigilE
noitacifitnedIpapers were fully read and scored based on the following subjects from various regions and languages, including
quality assessment questions: English,German,Mandarin,Spanish,andTaiwanese.The
• Does the work provide implementation details? number of samples varies across the different corpora, as
• Does the work provide detailed information about the do the modalities, which may include video, audio, and
models used? text. This dataset consists of subjects with Alzheimer’s
• Does the work propose different approaches or modali- disease,MCI,andcontrolgroups,withover700samples.
ties? • DementiaBank Pitt Corpus. The Pitt Corpus [53] is
• Does the work provide the source code? the most widely used corpus within the DementiaBank
• Does the work use standard performance metrics (accu- project.Itconsistsof309Englishlanguagesamplesfrom
racy, F1-Score, etc.)? individuals with Alzheimer’s disease, and 243 samples
• Is the dataset publicly available? from cognitively healthy individuals. During the record-
ings, participants are asked to perform various verbal
By applying these quality assessment questions, we can
fluency tasks and to describe a picture, specifically The
ensure that the final works included in our study are of high
Cookie Theft Picture [54]. These tasks are commonly
quality and suitable for further analysis.
used in psychological assessments to evaluate cognitive
Finally,theselectedstudiesandtheirresultshavebeencate-
status. Available modalities include both audio and tran-
gorizedbasedonthemodalitiesandmethodologiesemployed.
scribed text.
We propose a taxonomy that first categorizes methods by the
• ADReSS. The Alzheimer’s Dementia Recognition
modality used and subsequently by the methodology applied.
through Spontaneous Speech (ADReSS) dataset [55] is
The modality groups include audio, text, vision (image and
a balanced and higher-quality subset of the Pitt Corpus
video), and multimodal combinations. For the audio modality,
Cookie Theft Picture description. It is also a challenge
several methodologies are employed, including the use of ex-
datasetusedattheInterspeech2020conference.Thissub-
tracted audio features, Transformers, Mel Frequency Cepstral
sethasundergonenoiseremoval,voiceactivitydetection,
Coefficients (MFCCs), and 2D spectrograms. These method-
and volume normalization. It includes 78 Alzheimer’s
ologies are combined with deep learning techniques such as
patients and 78 control subjects, with data provided in
Convolutional Neural Networks (CNNs), Long Short-Term
both audio and transcribed text formats.
Memory(LSTM)networks,SupportVectorMachines(SVMs),
• ADReSSo. The Alzheimer’s Dementia Recognition
or Transformer architectures. In the case of the text modality,
through Spontaneous Speech only (ADReSSo)
two primary approaches are distinguished: Transformer-based
dataset[56]isachallengedatasetusedattheInterspeech
modelsandRecurrentNeuralNetworks(RNNs).Thesecanbe
2021 conference, obtained through fluency and image
furtherintegratedwithtechniquessuchasCNNs,GatedRecur-
description tasks. In this case, it is not derived from the
rent Units (GRUs), LSTMs, Large Language Models (LLMs),
Pitt Corpus. The dataset contains audio recordings from
or textual feature extraction. For vision modalities, CNNs are
87 patients diagnosed with Alzheimer’s disease and 79
primarily used for image processing, while video analysis
control subjects, with no accompanying transcriptions.
often incorporates methods such as video feature extraction,
• ADReSS-M. The Multilingual Alzheimer’s Dementia
pose estimation, Transformers, or CNNs. Finally, multimodal
Recognition through Spontaneous Speech (ADReSS-M)
approaches include the use of multimodal Transformers or
dataset[57]isachallengedatasetusedattheInternational
fusion strategies, which encompass early fusion, cross-modal
Conference on Acoustics, Speech and Signal Processing
attention, late fusion, and joint fusion strategies. Figure 3
(ICASSP)2023conference.Thisdatasetconsistsofaudio
presents a taxonomy of the reviewed works, illustrating the
recordings from Greek and English speakers describing
different approaches.
pictures and is not derived from the Pitt Corpus. It
includes 148 Alzheimer’s patients and 143 controls, with
IV. DATASETS
audio being the only modality provided, as it focuses on
The quality and quantity of data utilized in training deep acoustic features.
learning models represents a critical factor. Generally, larger • Taukadial. The Taukadial dataset [58] is a challenge
and more diverse datasets result in better model performance dataset used at the Interspeech 2024 conference and con-
by improving the model’s ability to generalize across a wider tainsaudiorecordingsfromChineseandEnglishspeakers
range of scenarios. However, in the domain of cognitive im- performing three different picture description tasks. It
pairmentdetection,theavailabilityofsuitabledatasetsremains comprises 222 samples from individuals with MCI and
asignificantchallenge.Duetothesensitivenatureofdatafrom 165 control subjects, with only audio data available.
individuals with cognitive disorders, access to such datasets • AphasiaBank. The AphasiaBank dataset [59] comprises
is often restricted. This section aims to highlight the most various corpora featuring subjects from different regions
relevant and accessible datasets employed in this field. A and languages, including English, Croatian, French, Ital-
summary of these datasets can be found in Table I: ian, Mandarin, Romanian, and Spanish. Subjects engage
• DementiaBank. The DementiaBank dataset [52] is the in various conversational tasks with clinicians, with data
largest available dataset, comprising 15 corpora with available in video, audio, and text formats. The datasetAudio Modality Multimodal Vision Modalities
Audio Features MFCC Multimodal models
Video Modality
2D CNN Multimodal
MLP 2D CNN 2D CNN
+ LSTM Transformer Video Features
SVM 2 +D L SC TN MN 2D Spectrogram Fusion Strategies GRU CNN
2D CNN Cross-modal
2D CNN Early Late Joint
Transformer LSTM + LSTM attention Pose
Visual GRU
Transformer
Transformer Text Modality
Transformers RNN Transformer CNN
Transformer LLM LSTM GRU
Image Modality
LSTM
CNN LSTM TF + LSTM
+ GRU
CNN
Textual CNN &
GRU
features LSTM
Fig.3. Proposedtaxonomyofthereviewedmethods,groupedbymodalityandmethodology.Yellowboxesindicateaudiomethods,bluefortext,greenfor
vision(imageandvideo),andpurpleformultimodalapproaches.
size varies across the corpora and includes a total of 180 different fluency and speaking tasks. This dataset is
aphasic patients and 140 control subjects. composed of Spanish people, and comprises 50 people
• NCMMSC2021. The National Conference on Man- with Parkinson’s and 50 control people. The available
Machine Speech Communication (NCMMSC2021) modality is audio.
dataset5 consists of audio recordings of subjects • DemCare. The DemCare dataset [66] contains video
performing different tasks, including picture descriptions recordings of elderly individuals performing daily life
and fluency tasks. The dataset includes 79 subjects with activities. This dataset includes both video and audio
Alzheimer’s, 93 with MCI, and 108 control subjects. modalities.However,sincetherearenospecificspeaking
Data is provided in audio and text modalities, and the tasks,languageisnotarelevantfactor.Thesubjectsinthe
dataset is divided into two versions: one containing dataset include both Greek individuals with Alzheimer’s
short conversations and the other containing longer and control participants, including a total of 89 subjects.
conversations. • PRAXIS Gesture. The PRAXIS Gesture dataset [67]
• CCC.TheCarolinasConversationCollection(CCC)[60] consists of upper-body video recordings of participants
is a dataset composed of audio conversations. It com- performing various gestures. Subjects are asked to repeat
prises 400 samples of people with Alzheimer’s and 200 each gesture until they complete it correctly. The dataset
samples for control. Besides the audio modality, textual includes 60 subjects in total, categorized as follows: 29
transcriptions are provided. elderlyindividualswithnormalcognitivefunction,2with
• B-SHARP. The Brain, Stress, Hypertension, and Aging amnestic MCI, 7 with unspecified MCI, 2 with vascular
Research Program (B-SHARP) [61] comprises a series dementia, 10 with mixed dementia, 6 with Alzheimer’s
of conversations and tasks, including the description of disease, 1 with posterior cortical atrophy, 1 with corti-
images conducted by a group of subjects. The group is cobasaldegeneration,and2withseverecognitiveimpair-
formed by 141 MCI patients and 185 controls. The data ment (SCI).
modalities are audio and its transcriptions in text.
• I-CONECT.TheInternet-BasedConversationalEngage-
ment Clinical Trial (I-CONECT) dataset [62]–[64] con-
In this study, we have included basic metrics such as
sists of different video recordings from subjects having a
accuracy and F1-Score for the classification of cognitive
conversation. This dataset compromises 100 MCI and 86
impairment in the aforementioned datasets. Additionally, we
control subjects. The data is provided in video and audio
have reported the primary metric used for a regression task.
modalities.
This task involves predicting a subject’s cognitive score based
• PC-GITA. The PC-GITA dataset [65] is composed of
on the MMSE, and is measured by the Root Mean Square
audio recordings, where people are asked to perform
Error(RMSE).Thesemetricsareappliedintheselectedworks
5https://github.com/lzl32947/NCMMSC2021 AD Competition to ensure consistency in performance evaluation.TABLEI
COGNITIVEDISORDERSESTIMATIONDATASETS.FORSIMPLICITY,“AD”DENOTESALZHEIMER’SDISEASE,“PAD”DENOTESPROBABLEALZHEIMER’S
DISEASE,AND“DTP”DENOTESDIFFERENTTYPESOFDEMENTIA.
Dataset Modalities Language Distribution Task
DementiaBankGeneral[52] Video,Audio&Text Multiple AD,MCI&Control:>700 Imagedescription
DementiaBankPittCorpus[53] Audio&Text English AD:309,Control:243 Fluencytasks&Imagedescription
ADReSS[55] Audio&Text English AD:78,Control:78 Imagedescription
ADReSSo[56] Audio English AD:87,Control:79 Fluencytasks&Imagedescription
ADReSS-M[57] Audio English&Greek PAD:148,Control:143 Imagedescription
Taukadial[58] Audio&Text English&Chinese MCI:222,Control:165 Imagedescription
AphasiaBank[59] Video,Audio&Text Multiple Aphasia:180,Control:140 Conversation&Imagedescription
NCMMSC2021 Audio&Text Mandarin AD:79,MCI:93,Control:108 Imagedescription
CCC[60] Audio&Text English AD:400,Control:200 Conversation
B-SHARP[61] Audio&Text English MCI:141,Control:185 Conversation&Imagedescription
I-CONECT[62]–[64] Video&Audio English MCI:100,Control:86 Conversation
PC-GITA[65] Audio Spanish Parkinson:50,Control:50 Speakingtasks
DemCare[66] Video&Audio - AD,MCI&Control:89 Dailylifeactivities
PRAXISGesture[67] Video - DTP:22,MCI:9,Control:29 Gestures
V. SINGLEMODALITY This section presents an overview of the different methods
employed for cognitive estimation using the audio modality,
In the context of cognitive estimation, different modalities
summarized in Table II. This table is organized by first
provide varied insights into the status of the individual. This
distinguishing the disorder being treated, followed by the
section introduces unimodal analysis of audio, text, image,
dataset used for training and testing. Additionally, it provides
and video data using a variety of approaches depending on
the publication year, the methodology that achieved the best
the context. It is important to note that many of the presented
performance in each study, and the key metrics previously
works focusing on a single modality also propose approaches
mentioned: accuracy, F1-Score, and RMSE. The best results
for other modalities or suggest combining multiple single-
for each dataset are highlighted in bold. The main techniques
modality approaches into a multimodal framework, as will be
employed are detailed in the following subsections, including
showninSectionVI.Whileaparticularworkmaydemonstrate
2D Spectrograms, MFCCs, Audio Features, and Transformers
robustperformanceinagivenmodality,itmaynotnecessarily
methods.
exhibit the same level of proficiency in other modalities.
1) 2D Spectorgrams Methods: The audio recordings are
A. Audio Modality typically represented by waveforms. These waveforms show
the amplitude of the signal as a function of time. The audio
The audio modality represents how humans perceive and
modalityisconsideredaone-dimensionaldatatype,consisting
process auditory information from the environment, and is
of a sequence of samples taken at specific and discrete
typically captured by microphones. This modality is easy
time intervals. However, there are also multi-channel audio
to obtain and, in the case of recording speech, can provide
representations, such as stereo, which combine two mono
valuable insights into how we speak, as well as facilitate
(single-channel) signals, thereby creating a two-dimensional
the extraction of other crucial modalities, such as text by a
data structure.
transcriptionprocess.Besidestexttranscription,thereisalsoa
wide range of features that can be representative, particularly Oneofthemostsignificantpropertiesofaudioisitssample
in how we express our thoughts. The way we express our rate, which indicates the number of samples taken per second
ideascanbeverysignificantincognitiveestimation,aspeople ofaudio.Theemploymentofone-dimensionalrepresentations
with more severe decline tend to struggle to find the correct in the field of deep learning can bring many benefits. One
and appropriate words, leading to long pauses, hesitation, and such benefit is the simplicity of usage, as the raw data is
the use of incorrect words among other issues [68], [69]. utilizedwithoutfurtherpreprocessing.Thiseliminatestheneed
Furthermore, speech is frequently employed to identify an in- for additional processing, resulting in low computational cost
dividual’semotionalstate,aprocessknownasSpeechEmotion and training time. Nevertheless, despite the existence of some
Recognition(SER)[70].SERtypicallyusesarangeofspeech works that employ raw one-dimensional representations for
features, including prosody, pitch, and rhythm to accurately audio classification, alongside one-dimensional CNNs [99],
predict emotions such as happiness or fear [71], [72]. Some [100], this is not the prevailing approach [101]. Instead,
studies have shown that cognitive disorders negatively affect they are often converted into two-dimensional data, such as
emotional mood [73], [74], making it another relevant source spectrograms [102].
of information for this task. Aspectrogramisavisualrepresentationofanaudiorecord-TABLEII
PERFORMANCECOMPARISONOFAUDIO-BASEDMETHODSOVERDIFFERENTCOGNITIVEDISORDERSANDDATASETS.FORSIMPLICITY,“MFCC”
DENOTESMEL-FREQUENCYCEPSTRALCOEFFICIENTS.
Score
Disorder Dataset Work Year Method
Acc F1 RMSE
[75] 2023 2DSpectrograms 78.90 - -
[76] 2023 Transformers - 80.5 -
[77] 2021 AudioFeatures 75.30 76.00 -
ADReSSo [78] 2023 Transformers 71.20 73.10 -
[79] 2024 Transformers 69.01 70.39 -
[80] 2021 AudioFeatures 68.00 - 6.03
[81] 2023 2DSpectrograms 62.10 60.7 -
[82] 2022 2DSpectrograms 74.00 - -
[83] 2020 AudioFeatures 72.92 - 5.07
[84] 2020 AudioFeatures 72.73 72.73 -
Alzheimer
[85] 2020 2DSpectrograms 70.80 70.80 -
ADReSS
[86] 2021 AudioFeatures 68.75 - -
[87] 2021 2DSpectrograms 66.67 - -
[88] 2023 2DSpectrograms 65.00 69.76 -
[89] 2021 MFCC 64.58 - 6.24
[90] 2021 AudioFeatures 82.59 82.94 -
DementiaBank
[91] 2023 2DSpectrograms 73.49 - -
PittCorpus
[92] 2022 2DSpectrograms 73.00 73.00 -
Owndataset [93] 2023 Transformers 88.50 88.30 -
Taukadial [94] 2024 AudioFeatures 70.75 76.22 3.11
MCI
I-CONECT [95] 2024 Transformers 50.42 54.02 -
NCMMSC2021L [78] 2023 Transformers 85.70 85.70 -
Alzheimer
NCMMSC2021S [78] 2023 Transformers 81.70 81.00 -
&MCI
Privatedataset [96] 2022 AudioFeatures 82.18 - -
Parkinson PC-GITAspeech [97] 2021 AudioFeatures 68.56 - -
Mandarin
Aphasia [98] 2022 2DSpectrograms - - 3.53
AphasiaBank
ing that illustrates the frequency content of the recording over formations. One crucial step involves the application of the
time. This combined time-frequency representation enables Fourier transform to convert the audio signal into a frequency
deep learning models to capture temporal patterns while also magnitude representation. The magnitude of each frequency
analyzingthefrequencyspectrum.Consequently,thesemodels componentindicatesitsinfluenceontheoverallaudiosample.
can extract more detailed information from audio recordings However,thisinitialtransformationlackstemporaldata,which
thanfromrawwaveforms[103],[104].Byshowingfrequency is essential for audio processing as it contains significant
changes over time, spectrograms make it easier to distinguish information. To ensure the retention of temporal informa-
sounds with similar time-domain characteristics but different tion throughout the audio sample, a windowing technique is
frequency-domain characteristics. Additionally, these repre- employed. This technique segments the audio into overlap-
sentations are closer to how humans perceive sounds based ping windows over time. Each windowed segment undergoes
on frequency and temporal changes, which are effectively Fast Fourier Transform (FFT) to convert the signal from the
captured in spectrograms [105], [106]. time domain to the frequency domain. The FFT computes
The use of spectrogram representations offers numerous the Discrete Fourier Transform (DFT) of a sequence. This
advantages in deep learning, particularly for general tasks. process, applied to each segment, is known as Short-Time
However, when it comes to human speech, other transforma- Fourier Transform (STFT). Combining these processed trans-
tions can be even more beneficial. One such transformation formations across all segments yields the final spectrogram,
is the Mel Spectrogram [107], a type of spectrogram mapped providing a two-dimensional representation of the audio that
to the Mel Scale. The Mel Scale was designed to represent includes both time and frequency information. To obtain a
frequencies in a way that better reflects how humans perceive Mel Spectrogram, another transformation must be performed
sound. Humans do not perceive all frequencies equally; we to scale the frequencies to the previously mentioned Mel
are more sensitive to differences in lower frequencies than in scale. Figure 4 summarizes the transformations to obtain the
higherones.Forinstance,it’smucheasierforustodistinguish spectrogramandMelSpectrogramfromarawaudiorecording.
between sounds at 100 Hz and 200 Hz than between 10,000
Hz and 10,100 Hz. Given this perceptual difference, Mel Once all the features are extracted into a two-dimensional
Spectrograms are especially valuable for processing human image representation of the audio signal, image-based models
speech, as they more accurately capture the nuances of how can be used for audio classification tasks. Vision models can
we hear sounds [108], [109]. extract the most relevant features of an image [110], [111]. In
In order to generate spectrograms and Mel Spectrograms the case of the widely used classical CNNs, a set of different
from raw waveforms, it is necessary to apply specific trans- kernels and pooling layers are applied to an image to obtainaccuracy and F1-Score in the Alzheimer recognition task
using the ADReSS dataset. In this study, the authors first
preprocess the audio signal by applying a Mel Spectrogram
Raw audio
recording process with a window size of 25 ms, retaining only seg-
Audio ments of eight or sixteen seconds from the full recording.
digitalization
A distinctive feature of this method is the employment of a
Audio waveform Siamese network [124], inspired by various studies utilizing
signal
thismodelforhealthapplications[125],speechemotionrecog-
nition [126], and speech impairment detection [127]. These
STFT
Siamese networks learn through contrastive learning applied
totwoseparatemodels,which,inthisstudy,wereCNNs.This
Spectrogram contrastive learning approach pulls together segments of the
same class (Alzheimer’s or Control) and separates segments
of different classes. Subsequently, the embeddings obtained
Apply
Mel Scale from each model serve as inputs to another and final CNN
for classification. The downside of this method is that the
Mel-Spectrogram application of contrastive learning restricts the experiment to
classification and does not provide information for regression
tasks. Additionally, the authors experimented with feeding a
CNN the raw audio signal, but the use of Mel Spectrograms
andSiamesenetworksresultedina4.1%improvement.Inthis
Fig. 4. Overview of the process for converting audio signals into a Mel work, the authors trained their own CNN and did not use any
Spectrogram.
pre-trained models.
Another notable work using Mel Spectrograms is presented
more complex features and classify the image [112], [113]. in [91], [128] and [92], both focusing on the classification of
These CNNs have been the traditional state-of-the-art method Alzheimer’s using the DementiaBank Pitt Corpus. In the case
for image processing [114]. However, the advent of well- of [91], Mel Spectrograms were combined with pre-trained
known Transformer-based architectures is replacing them. CNNs DenseNet [129], MobileNet [130], and ResNet [131],
Transformers [50], which were originally designed for text with the best results achieved using pre-trained DenseNet
processing, have migrated to other modalities due to their (accuracy of 73.49%). In the case of [92], the proposed
exceptional performance in the text modality field [115], pre-trained CNNs included ResNet18 [131], ResNet34 [131],
[116]. Among the different modalities where Transformers ResNet50 [131], SqueezeNet [132], and VGG16 [133], with
have migrated, we can distinguish video [117], audio [118], thebestresultsobtainedusingVGG16,achievinganaccuracy
and image [119]. The combination of both extracting spectro- of 66% and an F1-Score of 62%. When combined with
grams, specifically Mel Spectrograms, and using them to feed demographic data from patients, the accuracy and F1-Score
aCNNhasbeenemployedinmanyfields,forexample,inthe both improved to 73%.
analysisofmusictimbre[120].Additionally,theuseofvision In addition to the use of CNNs to extract features from
Transformermodelswiththecombinationofspectrogramshas images, there are also studies where the embeddings obtained
been applied in other fields, such as general audio classifi- from these CNNs are fed into a LSTM model. The LSTM
cation [121] and even Transformers specialized for spectro- model is a specific type of recurrent neural network that
grams [122]. In addition, the Whisper model [123], which enhances the capabilities of previous recurrent models by
is the state-of-the-art model for audio transcription, relies on integrating long-term dependencies for sequential data [134].
Transformer architecture. This model also uses spectrograms The work of [75] employs this method to predict Alzheimer’s
for processing audio, incorporating convolutional layers to using the ADReSSo dataset, achieving an accuracy of 78.9%.
create an encoding for the subsequent Transformer layers. In this study, the employed model is the pre-trained VGG16
The cognitive estimation task is not an exception regarding model. The same method has been used in the work of [98]
these types of methods. The best results have been obtained to predict the severity of Aphasia using the Mandarin Aphasi-
through the use of Mel Spectrograms. Despite requiring a aBank dataset. In this work the pre-trained ResNet model has
preprocessing step to convert the audio signal into a spec- been used as the CNN. This application to Aphasia achieved
trogram, which slightly increases computational time, this anRMSEscoreof3.53onthecognitivescoreofthesubjects.
process often yields improved performance. For instance, in This study also proposes the accuracy in determining each
the work of[93], using spectrograms and 2DCNNs improved level of severity of Aphasia, but not for the differentiation
Alzheimer’s disease detection accuracy from 72.6% with raw between aphasic and normal cognitive subjects.
waveforms and 1D CNNs to 84.4% using Mel Spectrograms In the work proposed by S. Siddhant et al. [81], they
and 2D CNNs, resulting in a relative improvement of 16.3% proposethevisionTransformerMVITV2[135]toanalyzethe
In addition, [85] achieved a performance of 70.8% in both generated Mel Spectrogram. They also provide an ablationstudy comparing the results with other previously mentioned The primary benefit of using this methodology with Mel
CNNs, such as VGG19, ResNet-101, and DenseNet-161. The Spectrograms is the more compact information and lower
use of this Transformer-based model outperformed the best dimensionality of data. Additionally, the main idea of this
result obtained from CNNs by 3%, achieving an accuracy of method is to keep the most relevant data regarding human
62.1 and an F1-Score of 60.7, demonstrating the capabilities speech tasks. However, the main disadvantage is the need for
of this type of network for this task. additional processing steps and the potential loss of informa-
There are also similar works over the ADReSS dataset. In tion during the discretization of the representation.
another study by I. Loukas et al. [88], the performance of the In the domain of cognitive estimation tasks, the study that
vision Transformer ViT [119] is compared with various pre- achieved the most favorable results when employing MFCCs
trained CNNs, including GoogLeNet [136], ResNet50 [131], was conducted by A. Meghanani et al. [89]. In this research,
WideResNet-50, [137] AlexNet [138], SqueezeNet [132], the effectiveness of both Log Mel Spectrograms and MFCCs
DenseNet-201 [129], MobileNetV2 [139], MnasNet1 [140], was investigated using a combination of CNNs and LSTMs
ResNeXt-50 [131], VGG16 [133], and EfficientNet-B2 [141]. for feature classification. The experiments revealed that com-
The performance of this vision transformer outperformed bining MFCCs with CNNs and LSTMs produced superior
everysingleCNNmodelbyatleast2%inaccuracy,achieving accuracy results, outperforming the Log Mel Spectrogram
up to 65% in accuracy and a 69.76% F1-Score. In addition, model by 6%, achieving up to 64.58% accuracy. For the Log
the work in [87] includes an ablation study comparing the Mel Spectrograms, employing a pre-trained ResNet model
performance of CNNs and vision Transformers. In this study, improved accuracy to 62.50%, but it is not sufficient to
the CNNs tested were MobileNet and YamNet, while the improve the results obtained using MFCCs. Nevertheless, in
vision Transformer model used was Speechbert [142]. Those the regression task, the Log Mel Spectrograms demonstrated
CNNs had as input MFCCs, a method which will be detailed the most optimal performance, achieving an RMSE of 5.9.
inthefollowingsubsection.TheSpeechbertmodelusingMFS 3) Audio Features Methods: When listening to speech,
outperformed the best CNN, MobileNet, by more than 7% in it is possible to derive several statistical acoustic features,
accuracy, achieving up to 66.67% in accuracy. including frequency, jitter, and shimmer. These values can
be obtained using a variety of toolkits that process audio
In the work [82], authors propose the use of the Au-
signalsandproducedifferentsetsoffeatures.Amongthemost
dio Spectrogram Transformer model [122], fed with a Mel
relevant toolkits for this extraction are OpenSMILE [148],
Spectrogram, which achieved an accuracy of 74% over the
[149],COVAREP[150],Kaldi[151],andOpenDBM6.These
ADReSS dataset.
toolkitsnotonlyprovidethepreviouslymentionedfeaturesbut
2) MFCCs Methods: Further preprocessing of audio sam-
also other relevant ones, such as MFCCs, offering valuable
ples can be performed to obtain MFCCs. This represents a
data for analysis. These features have been applied in various
more compact representation of the audio sample, attempting
speech-related fields, including emotion recognition [152].
to convey the overall shape of a spectral envelope [143]. In
For instance, the CMU-MOSEI dataset [153] provides audio
addition to mapping frequencies to the Mel scale, MFCCs
modality in vectors processed by the COVAREP software.
transform these features into the cepstral domain, which is
Additionally, these methods have been utilized for tasks such
usefulforavarietyofaudioandspeechprocessingtasks[144],
as bird sound classification [154] and dysarthric speech clas-
[145]. This methodology has also been applied in the health
sification [155]. The employment of this methodology brings
fieldfortaskssuchasheartsoundclassification[146]andauto-
manybenefits,asitextractsnumerousfeatures,includingthose
maticdepressiondetection[147].However,thisrepresentation
previously mentioned. However, it also has disadvantages,
requires additional processing steps in comparison to Mel
such as the significant time required for preprocessing the
Spectrograms. This representation is computed by applying a
audio to obtain these features. Some of the set of features
logarithmfunctiontoaMelSpectrogram.Afterthislogarithm
available from these toolkits include:
procedure, a Discrete Cosine Transform (DCT) is applied to
decorrelate these features and reduce their dimensionality. An • Emobase [148]: The representation includes MFCCs,
fundamental frequency (F0), F0 envelope, line spectral
example of this representation can be seen in Figure 5.
pairs (LSP), and intensity features. It contains up to 988
features. This feature set can be obtained though the use
of OpenSMILE toolkit.
• IS10 linguistic [156]: This is an expanded and more
recent iteration of the previous Emobase feature set.
The feature set includes 16 different types of Low-Level
Descriptors(LLDs),PulseCodeModulation(PCM)loud-
ness, eight log Mel frequency bands, eight line spectral
0 1.5 3 4.5 6 7.5 9 10 12 14 pairs, MFCCs, frequency, F0 envelope, voicing proba-
Time
bility, jitter, shimmer, and other features. LLDs include
Fig. 5. Example of an MFCC representation, obtained from the Mel
SpectrograminFigure4. 6https://github.com/AiCure/open dbm
tneiciffeoC
CCFMlogarithmic harmonicto-noise ratio, voice quality fea- yielded further performance improvements. The final results
tures,ViterbismoothingforF0,spectralharmonicity,and demonstrated an accuracy of up to 68.75% on the ADReSS
psycho-acoustic spectral sharpness. This representation dataset.Similarly,thosefeatureswerecombinedwithanMLP
yields 1,582 features. This feature set can be obtained intheworkproposedbyD.Ortiz-Perezetal.[94].Inthiscase,
though the use of OpenSMILE toolkit. thefeatureswiththebestoutcomescomefromtheOpenDBM
• eGeMAPS [157]: This feature set reduces other feature toolkit, outperforming the ones obtained from OpenSMILE.
setstoabasicsetof88features.Thefeaturesetcomprises This approach yielded an accuracy of up to 70.75%, an F1-
sevenLLDs,includingspectralparametersofMFCCand Scoreof76.22%,andanRMSEscoreof3.11ontheTaukadial
spectral flux, as well as frequency-related parameters dataset over chinese and english speakers.
of formant bandwidth. This feature set can be obtained These features are also employed in combination with
though the use of OpenSMILE toolkit. LSTMs, as seen in [80], where COVAREP features were used
• ComParE 2016 [158]: This set comprises 6,373 fea- on the ADReSSo dataset, achieving up to 68% accuracy and
tures and may be regarded as an extended version of an RMSE score of 6.03. Additionally, as discussed in the
the IS10 linguistic feature set. It compromises energy, Spectrogram section, features can be processed with CNNs to
spectral, MFCC, voicing related LLDs, and statistical extract features and an LSTM to establish temporal relations,
functionals. This feature set can be obtained though the as demonstrated in [83]. In this study, the performance of
use of OpenSMILE toolkit. eGeMAPS, ComParE 2016, and VGGish features was evalu-
• VGGish [159]: This feature set has been obtained from ated,withVGGishfeaturesyieldingsignificantlybetterresults.
trainingaCNNwithanAudioSet[160]foraudioclassi- The results achieved were 72.92% accuracy and an RMSE
fication.Thefeaturesetcomprises128dimensions,which score of 5.07 on the ADReSS test set. The study proposed
are the embeddings obtained from the pre-trained CNN, in [97] used their own extracted audio features combined
with each dimension extracted from audio segments of with a CNN for the recognition of Parkinson’s disease on the
960 milliseconds in length. Parkinson PC-GITA speech dataset, obtaining an accuracy of
• I-Vector [161]: These are statistical speaker representa- 68.56%.
tion vectors. These representations have been used in There are different works, such as those proposed by L.
the health domain, including for Parkinson’s disease and Zhaoci et al. [90] and S. Zhengyan et al. [96], where both use
dysarthric speech recognition [162], [163]. This feature pre-trained models for Automatic Speech Recognition (ASR)
set are obtained from statistical modeling techniques, andtheirintermediatehiddenstatesincombinationwithCNNs
compromising 400 features and can be obtained though and LSTMs to predict Alzheimer’s. In [90], this methodology
the use of Kaldi toolkit. achievedanaccuracyof82.59%andanF1-Scoreof82.94%on
• X-Vector [164]: These are discriminative deep neural the DementiaBank Pitt Corpus dataset. In [96], the best result
network-based speaker embeddings that have demon- was an accuracy of 82.18% on their own dataset, predicting
strated superior performance in speaker and language both Alzheimer’s and MCI.
recognition tasks, outperforming I-Vector [165], [166]. There are other studies, such as the one proposed by N.
This feature set are obtained from the embeddins of Wang et al. [77], where the best performance was achieved
pretrained models compromising 512 features can be using X-Vector, slightly improving upon the second-best re-
obtained though the use of Kaldi toolkit. sults and notably improving over features such as Emobase
• COVAREP: This set of 79 features encompasses a range andIS10 linguistic.Inthiswork,X-Vectorfeatureswereused
ofcharacteristics,includingprosodicfeatures,voicequal- in combination with a Transformer encoder, achieving up to
ityfeatures,andspectralfeatures(MFCCs,harmonicand 75.3% accuracy and 76% F1-Score on the ADReSSo dataset.
phase distortion means and deviations). This feature set Similarly, in [84], the best results were obtained using X-
can be obtained though the use of COVAREP toolkit. Vector, which outperformed I-Vector, achieving up to 72.73%
• OpenDBM:Thefeaturesprovidedincludevoiceintensity, in both accuracy and F1-Score on the ADReSS dataset.
MFCCs, voice prevalence, shimmer, jitter, pause charac- 4) Transformer Methods: As previously stated, Trans-
teristics,differentnoises,andfrequencyinformation.The formermodelsareemployedindifferentmodalities,including
feature set compromises 43 features and can be obtained audio. This architecture typically comprises two principal
though the use of OpenDBM toolkit. components:anencoderandadecoder.Theencoderprocesses
the given input, resulting in embeddings, which serve as a
Regarding the cognitive estimation task, numerous studies representation of data. After being processed, embeddings
have achieved promising results through the utilization of capturethemostrelevantinformation.Withthisrepresentation,
this methodology. For instance, in [86] these features were the decoder part of the Transformer generates new data based
employed to train an MLP and a GRU. In this work, the ontheserepresentations[167],[168].Intheproblempresented
performanceofEmobase,ComParE2016,andeGeMAPSwas in this study, we are addressing a classification task for
evaluated, with Emobase features demonstrating the most fa- identifying a disorder and a regression task for estimating
vorable outcomes. Furthermore, the combination of these fea- cognitive state. Therefore, we will focus exclusively on the
tures with the means of the values across the entire recording encoder component, which is responsible for obtaining thedata representation and not for generating new data. This and an F1-Score of 70.39%. In this case, no other methods
representation allows for the prediction of either a disorder or Transformer models were evaluated for performance com-
or the cognitive state. parison. However, it included an ablation study to determine
ThemainadvantageofTransformersoverpreviousmethods whether it was best using Whisper or Wav2Vec2 for the ASR
lies in the attention mechanism, which is a key component of task, with the Whisper model outperforming Wav2Vec2. This
this architecture. This mechanism assigns weights to different model is also employed in the study proposed by D. Escobar-
partsofasequencebasedontheirimportancerelativetoother Grisales et al. [93] but uses its own recorded dataset. In this
words, enabling the processing of sequences without relying case, it is evaluated in comparison with the use of spec-
on a strict sequential order [50]. As a result, the model can trograms and CNNs. The Transformer-based model slightly
focus on the most relevant segments of the input. Attention outperformstheuseofspectrogramsand2-dimensionalCNNs,
has been shown to yield excellent results and facilitates obtaining up to an 88.5% accuracy and 88.3% F1-Score.
parallelcomputation,improvingprocessingtimescomparedto On the other hand, raw waveforms in combination with 1-
traditional sequential data processing methods. This attention dimensional CNNs yielded poor results.
mechanism is implemented through the Multi-Head Attention AnotherstudythatemployedWav2Vec2istheoneproposed
layer, which identifies the most relevant relationships among byY.Yingetal.[78],whichworkswiththreedifferentdatasets
theinputelements[50],[169].InordertofeedtheTransformer for Alzheimer’s and MCI recognition. In this study, the
architecture,theinputmustbeinanembeddingrepresentation. Transformer-based model outperforms the extraction of var-
There are different ways of obtaining those embeddings. ious audio features, including eGeMAPS and IS10 lingusitc.
In text modality, for example, sentences are encoded into This model achieves an accuracy of up to 71.2% and an F1-
numericalIDsrepresentingwordsorpartsofwords,whichare Score of 73.1% on the ADReSSo dataset for Alzheimer’s
then expanded into embeddings for further processing [115]. classification. It also attains an accuracy and F1-Score of
In the case of audio, there are two main options: using 85.7% on the NCMMSC2021 long speech dataset and 81.7%
raw waveforms or spectrograms. In both cases, CNNs are and 81%, respectively, on the NCMMSC2021 short speech
employed to extract features and embeddings that feed into dataset, both for the classification of Alzheimer’s, MCI, and
the Transformer encoder layers. One example of using raw healthy subjects.
waveforms is the Wav2Vec model [170], while an example Similarly, the work in [95] utilizes the Wav2Vec2 model
of using spectrograms for an audio Transformer is the Audio in combination with a dense layer to predict MCI on the
Spectrogram Transformer [122]. This architecture has been I-CONECT dataset, achieving up to 50.42% accuracy and
employedindifferentaudio-relatedtasks,suchasmusicgenre 54.02% F1-Score. The poor performance of this methodology
classification [171], emotion recognition [172], or speaker isbecausethemostrelevantfeaturesinthisdatasetcomefrom
recognition [173]. the video modality.
Regarding the cognitive estimation task, several works uti- In general, we can appreciate that regarding the Transform-
lizetheTransformerarchitecturetoprocessaudiosignals.One ersforaudio,themostemployedoneandtheonethatachieves
ofthemostnotableisproposedbyZ.Cuietal.[76],whichnot the best results is the Wav2Vec2 model.
only predicts Alzheimer’s disease using the ADReSSo dataset
B. Text Modality
but also examines the impact of depression on this disease.
Some studies link emotional mood, particularly depression, The text modality represents how humans structure and
with cognitive impairment disorders. A significant percentage express their thoughts to others using natural language in
of people suffering from Alzheimer’s exhibit neuropsychiatric written form, as opposed to spoken language. Textual data
symptoms, and half of those with Alzheimer’s also suffer is highly relevant for cognitive decline estimation because, by
from depression [174]. Additionally, studies show that indi- analyzing text, we can understand how people structure and
viduals with a history of depression have a higher risk of express their thoughts. Additionally, it can provide valuable
developing Alzheimer’s [175], [176]. Therefore, this study insights into different word choices, as cognitive function
also incorporates a depression dataset, specifically the DAIC- can affect expressive tasks. For instance, individuals may
WOZ dataset [177]. For this study, various Transformer- struggle with selecting the correct word, leading to incorrect
based architectures were employed, including WavLM [178], or undesired choices.
HuBert[179],andWav2Vec2[118].Amongthese,theWavLM Text is typically transcribed from audio modality. There
model performed best, achieving an F1-Score of up to 80.5, are different approaches for this problem, for example using
slightly surpassing the others. In addition to these proposed professional transcribers who manually transcribe audio into
models, a fine-tuned version of WavLM using the depression text. This method ensures a proper result, but its main disad-
dataset was also tested, but this approach did not improve vantages are its time-consuming nature and the necessity of
upon the basic WavLM model. Another proposed method professionalsperformingthetask.Forthispurpose,therehave
involved using a hidden state from the transcription model been many recent advances in the ASR task, which aims to
Whisper [123], but this did not yield improvements. automatically transcribe audio into text [180].
In [79] the authors employed the Wav2Vec2 model on the Recent works have achieved results similar to those of
ADReSSo dataset, achieving an accuracy of up to 69.01% professional transcribers [123]. However, when applied toTABLEIII
PERFORMANCECOMPARISONOFTEXT-BASEDMETHODSOVERDIFFERENTCOGNITIVEDISORDERSANDDATASETS.
Score
Disorder Dataset Work Year Method
Acc F1 RMSE
[75] 2023 Transformer 88.70 - -
[76] 2023 Transformer - 88.00 -
[79] 2024 Transformer 83.10 83.10 -
[184] 2021 Transformer 83.10 83.02 4.45
ADReSSo
[80] 2021 RNN 81.00 - 4.43
[78] 2023 Transformer 78.90 79.00 -
[81] 2023 Transformer 77.50 77.40 -
[77] 2021 Transformer 73.50 73.50 -
[189] 2020 Transformer 89.60 - -
[190] 2022 Transformer 87.50 93.33 -
[82] 2022 Transformer 85.00 - -
[191] 2020 Transformer 83.30 - 4.56
[85] 2020 RNN 81.30 81.20 4.66
[83] 2020 Transformer 81.25 - 4.01
ADReSS
Alzheimer [87] 2021 Transformer 82.08 - -
[192] 2022 Transformer 81.00 79.00 -
[84] 2020 Transformer 72.92 - -
[91] 2023 Transformer 90.36 - -
[193] 2023 RNN 89.50 89.30 -
[194] 2021 Transformer 88.08 87.23 -
DementiaBank [195] 2024 RNN 83.60 - -
PittCorpus [92] 2022 Transformer 82.00 81.00 -
[196] 2022 RNN 81.54 85.19 -
[197] 2019 RNN - 74.37 -
Privatedataset [93] 2023 Transformer 77.90 76.90 -
B-SHARP [61] 2020 Transformer 74.10 - -
MCI
I-CONECT [95] 2024 Transformer 60.75 59.67 -
Taukadial [94] 2024 Transformer 74.82 79.43 3.14
Alzheimer NCMMSC2021L [78] 2023 Transformer 70.70 70.00 -
&MCI NCMMSC2021S [78] 2023 Transformer 54.30 52.70 -
the domain of health, there are additional challenges that tectures, are detailed in the following subsections.
must be addressed. For instance, in the context of cognitive 1) Recurrent Neural Networks: Before the arrival of
decline, uncertainty or indecision may impede the accurate
the Transformer architecture, which revolutionized the entire
transcriptionofaudio.Nevertheless,therearesomeworksthat
Natural Language Processing field, the standard was to use
address this issue [181]–[183]. In this context, the work [184]
RNNs [198]. These networks process sequential and temporal
uses the aforementioned Transformer-based Wav2Vec model
data, making them suitable for the text modality. However,
to accurately transcribe the corresponding audio, incorporat-
RNNs have some limitations, such as struggling with long
ing pauses for further prediction of dementia using textual
sentences and losing information from the beginning of the
information.
sequence. For this reason, different approaches have been
This modality can also be obtained using other approaches, proposed to improve these limitations. For instance, LSTM
such as text generative models for data augmentation, where models were designed to maintain long-term dependencies
we find many recent advantages [185], [186]. However, as better than the original RNNs [199], [200]. Similarly, GRUs
we are dealing with cognitively impaired subjects, the results wereintroducedtohandlelong-termdependenciesmoreeffec-
may not be the most appropriate since the generation may not tively, offering a more lightweight and faster architecture than
includethedifferentgrammaticalerrorsorwaysofexpressing LSTMs while achieving similar performance [201], [202].
themselves. To address this issue, some researchers have The application of this methodology offers promising ben-
proposedfine-tuningaGPT-2model[187]togeneratetextthat efits, including the ability to capture the dependencies among
is more relevant, and similar to that produced by cognitively different words and their relationships. Additionally, these
impaired individuals [188]. typesofnetworksaresimpleandeasytotrain.However,since
This section presents an overview of the various methods they process data sequentially, they cannot be parallelized,
employed for the cognitive estimation task using the text whichrepresentsasignificantdisadvantage.Thismethodology
modality, all presented in Table III. This table is organized by has been widely employed in many tasks related to the
disorderandfurthercategorizedbydataset,presentingtheyear NaturalLanguageProcessing(NLP)field,suchasgeneraltext
of publication, methodology used, and performance metrics classification or sentiment analysis [203]–[205].
such as accuracy, F1-Score, and RMSE. The best results for In the context of cognitive estimation, in [80] the authors
each dataset are highlighted in bold. The primary methods proposeusingLSTMsalongwiththeBERTTransformer[115]
employed, including the use of RNNs and Transformer archi- for Alzheimer’s detection over the ADReSSo dataset. In thisstudy, the Transformer architecture outperformed the LSTM
model. However, using the LSTM in combination with text
features, such as disfluencies and pauses obtained from the
recordings, achieved the best results with an accuracy of up
to 81% and an RMSE score of 4.43. The work in [85], also
exploitingtheADReSSdataset,proposesusingabidirectional
LSTM in combination with an attention module achieving up
to 81.3% and 81.2% accuracy and F1-Score, respectively, and
an RMSE score of 4.66.
In the work in [196] over the DementiaBank Pitt Corpus,
the text undergoes an enrichment preprocess, including the
addition of POS-Tagging features. This, in combination with
LSTMs, achieves up to 81.54% accuracy and 85.19% F1-
Score, surpassing other proposed approaches, such as the use
of Transformer encoder blocks. In the work in [193] over the Fig.6. TheCookieTheftPicture.Theimageusedforthepicturedescription
same dataset, the authors combine a Text CNN [206] with task,from[209].
an LSTM. The application of CNNs to sentence classification
has also been demonstrated in several studies [203], [207].
This combination of TextCNN and LSTM achieved an ac- Regarding the cognitive estimation tasks, a notable number
curacy of 89.5% and an F1-Score of 89.3%, outperforming of works propose the employment of this Transformer archi-
otherproposedmethods,includingbothapproachesseparately tecture.Oneexampleistheworkin[81],inwhichtheauthors
and Transformers such as DistilBERT [116], BERT [115], propose the use of different pre-trained Transformer models,
GPT2 [187], and RoBERTa [208]. including DistilBERT [116], BERT [115], RoBERTa [208],
As previously stated, the ADDReSS dataset consists of DistilRoBERTa [116], and ALBERT [216]. Experiments were
audio recordings in which subjects describe The Cookie Theft carried out over the ADReSSo dataset, obtaining the most
Picture (see Figure 6). The work in [195] utilizes this image promising results with the use of the DistilBERT model,
in their approach, where the picture is segmented into distinct with 77.5% accuracy and 77.4% F1-Score. Similarly, in the
areas.Forinstance,themotherandthechildrepresentseparate work [75], authors compare the use over the same dataset
areas.Additionally,aclusterofsynonymouswordsisproposed of different Transformer models, such as RoBERTa, BERT,
foreachregion(e.g.,synonymsfor“mother”include“lady”or DistilBERT, and XLNET [217]. In this study, the most sig-
“woman”). These sets of synonyms, combined with descrip- nificant findings were obtained using the RoBERTa model,
tions of the image, are used to construct a matrix that counts with an accuracy of 88.7%. Within the same dataset, in [184]
co-occurrences in the descriptions. This matrix serves as the the authors experiment with the use of BERT in combination
input embedding for a subsequent LSTM model, achieving an with the addition of encoded pauses during the speech to the
accuracy of up to 83.6%. The work proposed by Y. Pan et al. transcriptions.Theadditionofsuchpausesresultsinanotable
[197] combines various methodologies, including LSTMs, an enhancement in performance, with an approximately 10%
attention layer, and GRU. This method, which surpasses the increase in accuracy and F1-Score, and a slight improvement
performanceofasingleLSTMusedforpredictingAlzheimer’s in the RMSE score. The final scores with these pauses have
disease, achieves an F1-Score of 74.37 on the DementiaBank shown results of up to 83.1 and 83.02 in accuracy and F1-
Pitt Corpus. Score and 4.45 in RMSE. Another work using Transformers
2) Transformers: As previously stated, the Transformer overthisdatasetistheoneproposedin[76],wheretheauthors
architecturehasbecomestate-of-the-artinmanyNLPtasks.A employ a BERT model obtaining up to 88% accuracy. BERT
pertinent advantage of this approach is that it eliminates the has shown to be the most used Transformer model in this
need for sequential training, allowing for parallelized learn- task,andin[78]thismodelistestednotonlyontheADReSSo
ing.Additionally,largepre-trainedTransformer-basedmodels, datasetbutalsoontheNCMMSC2021dataset,withbothlong
which have been trained on extensive datasets, bring signif- and short sentences. In this study, BERT obtains 78.9% and
icant benefits by incorporating vast textual knowledge [210], 74%accuracyandF1-ScoreovertheADReSSodataset,70.7%
[211]. This extensive preexisting knowledge has proven to and70%intheNCMMSC2021longsentences,and54.3%and
attain impressive results in various related tasks. 52.7% over the short sentences of this dataset.
However, these large models also have drawbacks, such as Regarding the works that use the textual modality on the
higher demands on computational power and memory, and ADReSS dataset, all but one employ Transformer architec-
longer training times compared to earlier methods like RNNs. tures. The work in [87] compares the performance of Long-
ThismethodologyisemployedextensivelyinthefieldofNLP, Former[218]andthebaseandlargeversionsofBERT.Inthis
including applications such as text classification [115], text case, the best performance was achieved by the LongFormer,
summarization [212], text generation [185], [213], and text withanaccuracyofupto82.02%onthedataset.Moreover,the
translation [214], [215]. employment of the large version of BERT also resulted in animprovement over the base version. In the work proposed by sentences for majority voting, Transformer embeddings, and
I.Loukasetal.[190],theauthorsexperimentwithseveralpre- classifiers. The best results were obtained using the Large
trained Transformers, including BERT, BioBERT [219], Bio- versionofBERT,outperformingthebaseversionandtheother
ClinicalBERT [220], RoBERTa, ConvBERT [221], ALBERT, Transformers. In addition, this approach also outperformed
and XLNet. The most relevant results were obtained using the combination of the Transformer with an LSTM, achieving
BERT, achieving up to 87.5% accuracy and a 93.33% F1- accuracy and F1-Score of 88.08 and 87.23, respectively.
Score. This work proposed a multiclass model to predict the In [61], three different Transformers, RoBERTa, BERT,
severity of dementia based on the MMSE Score, but it did and AlBERT, were tested on the B-SHARP dataset for the
not improve the classification approach. The work proposed detection of MCI. The RoBERTa model yielded the best
in [82] employs a BERT model in combination with a SVM, individual results. However, combining the embeddings from
obtaining an accuracy of 85% on ADReSS dataset. all three Transformers resulted in even better performance,
The work in [191] evaluates the BERT model against achieving an accuracy of up to 74.1%. The work proposed
classicalmachinelearningalgorithmsandtextualfeatures.The in [94] employs the combination of different Transformer
tested machine learning algorithms include Support Vector embeddingsforthetextmodality,specificallyforthedetection
Machines (SVM), Random Forest, and Naive Bayes. The of MCI in the Taukadial challenge. In this study, the em-
textual features included, which are very relevant to dementia beddings obtained from a BERT model were integrated with
detection [222], [223], were speech-graph-based [224], lex- similarity embeddings derived from a comparison between
ical [225], syntactic [226], and proportions of various POS- the subjects’ descriptions and the actual descriptions of the
Taggingfeaturesbasedonthepicture-describedfeatures[227]. images in question. This approach achieves an accuracy of
Theexperimentationshowsthatthebestresultswereobtained 74.82, an F1-Score of 79.43, and an RMSE score of 3.14.
with BERT, achieving 83.3% accuracy and a 4.56 RMSE Thestudyby[95]employsBERTembeddingscombinedwith
score. a dense layer to predict MCI using the I-CONECT dataset,
Inanotherwork[83],theauthorsproposedifferentmethods, achieving an accuracy of 60.75% and an F1-Score of 59.67%.
suchasGLOVEembeddings[228]withattentionmodulesand This approach demonstrates an improvement over the results
Transformers, such as RoBERTa, Transformer-XL [229], and obtained using the audio modality.
GPT [230]. In addition to these models, extensive research In [93], three different approaches were proposed for the
on the addition of different textual features has also been detectionofAlzheimer’sdiseaseusingtheresearchers’private
carriedout,inparticular,theadditionofhand-craftedfeatures, dataset. These approaches, which have been combined with
suchaspsycholinguistic,repetitiveness,andlexicalcomplexity CNNs, included the use of Word2Vec embeddings [238],
features. These psycholinguistic features have proven to be BERT, and BETO [239] Transformers. The BETO model, a
relevant in Alzheimer’s recognition [231]. Additionally, POS- Spanish version of BERT, achieved the best results with ac-
Tagging features have been added and compared to the tran- curacy and F1-Score reaching 77.9% and 76.9%, respectively.
scriptions, as they have also proven to be relevant [232]. The The main reason for the superior performance of the BETO
results showed that the best outcomes were obtained using model is that their dataset contains recordings in Spanish.
Transformer-XL and the addition of these textual features In the work in [79], Transformers and the capabilities of
resulted in a slight improvement over simple Transformers. largelanguagemodels(LLMs)areutilized.Thesemodelshave
The final results were 81.25% accuracy and 4.01 RMSE. demonstrated exceptional performance in various language
Similarly, the work in [189] compares two Transformer tasks and can be applied across many fields [240]–[242].
models, BERT and ERNIE [233], in combination with the ThisstudyemploysembeddingsobtainedfromaBERTmodel
enhancement of textual features by the addition of different alongsidetheChatGPTmodel[243].ChatGPThasbeentasked
tokens representing pauses, depending on how long those with evaluating the fluency of the speech. This evaluation,
pauses are in the conversation. This study showed better combinedwiththeBERTembeddingsyieldedthebestresults,
performancewiththeuseofERNIEandalsothattheinclusion achievingupto83.1%inbothaccuracyandF1-Scoreoverthe
ofthesepausesreflectsanimprovement,obtaininganaccuracy ADReSSo dataset.
of up to 89.6%. The work in [77] combines Transformer In addition, Transformer architectures can be integrated
architecturewithvariouslinguisticfeaturesextractedusingthe with more simple RNNs. An example is the work in [192]
NLTK toolkit [234] tested on the ADReSSo dataset. Those on the ADReSS dataset. In this study, various embeddings,
linguistic features included POS-Tagging, as well as lexical including Glove, Word2Vec, Doc2Vec [244], BERT, AlBERT,
diversity. Studies have shown that this type of feature can and RoBERTa were combined with Bi-directional LSTMs.
be relevant for Alzheimer’s detection [235]–[237], and results The best results were achieved using BERT embeddings,
achieved 73.5% in both accuracy and F1-Score. with an accuracy of up to 81% and an F1-Score of 79%.
The work [194] proposes experimenting with different It is noteworthy that most of the Transformer embeddings
Transformers, such as BERT, XLNet, and XLM on the outperformed the other approaches. Similarly, in [84] BERT
DementiaBank Pitt Corpus. The authors present a pipeline embeddings were employed along with LSTMs to achieve an
composed of a data augmenter, which includes synonym and accuracy of up to 72.92% on the same dataset. The work
contextual substitution, splitting the description into different proposed in [91] employs a similar approach by combiningTABLEIV
PERFORMANCECOMPARISONOFVISION-BASEDMETHODSOVERDIFFERENTCOGNITIVEDISORDERSANDDATASETS.
Score
Modality Disorder Dataset Work Year Method
Acc F1
Image Alzheimer Privatedataset [248] 2021 CNN 81.03 -
[249] 2024 Transformer 90.63 93.03
I-CONECT
MCI [95] 2024 Transformer 69.75 66.45
Privatedataset [250] 2020 VideoFeatures 87.88 -
Video
Parkinson Privatedataset [251] 2019 Pose 82.50 -
Apathy Privatedataset [252] 2022 VideoFeatures 95.34 94.5
Alzheimer&MCI Privatedataset [96] 2022 CNN 72.21 -
BERT embeddings with Bi-directional LSTMs, but applied movements. For instance, the study proposed by J. Sun et
to the Dementia Bank Pitt Corpus dataset, achieving an al. [249] employs this modality to predict cognitive decline,
accuracyofupto90.36%.Theworkproposedin[92]explores specificallyMCI,usingtheI-CONNECTdataset.Inthiswork,
various approaches, including Sentence-BERT [245], GloVe, the authors analyzed the facial expressions and movements of
FastText [246], and FLAIR [247]. The best performance was subjectswhileinteractingwithothersonvideorecordings.Ad-
achievedusingtheSentence-BERTmodelincombinationwith ditionally,featuressuchaslipandeyemovements,whichplay
GRU, attaining an accuracy of up to 82% and an F1-Score of a critical role in both verbal and non-verbal communication,
81% on the DementiaBank Pitt Corpus dataset. are essential for detecting cognitive impairment [257]–[260].
The videos were processed using a video Transformer called
C. Vision modalities
MC-ViVit, based on the ViVit transformer for video [117].
While most research on non-intrusive modalities focuses With this implementation, the authors achieved an accuracy
on audio and text, other relevant modalities can also offer of 90.63% and an F1-Score of 93.03%. In the same dataset,
valuable insights for detecting these disorders. Among these the approach proposed in [95] utilizes the video modality,
are image and video modalities, which together form our specifically raw frames, which are processed using a video
proposed visual modalities. The most relevant works within Transformer, namely the Video Swin Transformer [261]. This
thesemodalitiesareshowninTableIV.Thistableisstructured architecture extracts spatio-temporal information from the
by modality and further categorized by disease and dataset. It frames throughout the video. The extracted features are then
presents the year of publication, the methodologies used, and combinedwithadenselayertoperformthefinalclassification
performance metrics such as accuracy and F1-Score. In this ofthesubject’scognitivestate,achievinganaccuracyofupto
case,theRMSEisnotreported,asnoneoftheincludedworks 69.75% and an F1-Score of 66.45%. This modality proves to
provide this information. The best results for each dataset are be the most relevant, as it delivers the best performance when
highlighted in bold. The methodologies are detailed in the compared to other modalities.
followingsubsectionsaccordingtothemodalityapplied.These
In the work by L. Xing et al. [250], authors propose using
modalities are image and video.
video features with their dataset, which contains recordings
1) Image: In the image modality, we observe that the
of individuals using sign language, both those with MCI and
proposed work by Nicole D. Cilia et al. [248] uses im-
healthy individuals. The videos were preprocessed to extract
ages of handwriting from healthy subjects and people with
various video features, including facial landmarks and poses.
Alzheimer’srecordedintheirdataset.Researchhasshownthat
This information was used to train different CNN models,
patients with cognitive disorders exhibit alterations in spatial
specificallyVGG16andResNet50.Experimentsdemonstrated
organization and poor control of movements [253]. Hand-
that the VGG16 model achieved the best performance, with
writing, which results from a complex network of cognitive,
anaccuracyof87.88%.Similarly,intheworkproposedbyD.
kinesthetic, and perceptive motor skills, can be significantly
Abhijit et al. [252] various video features were obtained, in-
altered. Additionally, handwriting features are relevant in
cludingfacialactionunits,gaze,pose,andemotionalfeatures.
medicineandpsychologyfordiagnosticpurposes[254],[255].
Thisstudyfocusedontheirdatasettopredictapathy.Although
In this work, the authors ask subjects to perform handwriting
apathy is not a disease, it is common in other conditions
tasks on a tablet, later processing these images. In addition
and typically affects people’s cognitive abilities. Using the
to RGB images, the dataset includes features such as writing
extractedinformationandarecurrentGRUmodel,theauthors
speed and straightness error. This data was used to fine-tune
achieved an accuracy of 95.34% and an F1-Score of 94.5%.
several pre-trained CNNs, including VGG19, ResNet, and
InceptionV3[256].Themostrelevantresultscamefromusing Parkinson’sdiseasehasvarioussymptoms,includingFreez-
the ResNet model, yielding up to 81.03% accuracy. ing of Gait (FoG). FoG is a symptom where patients feel
2) Video: Another relevant modality for these cognitive stuck while walking and experience a cessation of movement,
estimationtasksisvideo.Videosprovidevaluableinformation even when they intend to keep walking [262], [263]. FoG
from subjects, such as their facial expressions and body becomes more frequent over time as the disease progresses,
poses during different actions, which offer insights into their significantlyaffectingdailylifeandqualityoflife[264],[265].Therefore, detecting this symptom is crucial for identifying methodologies for combining modalities rely on the fusion of
Parkinson’sdiseaseandcanbeachievedthroughdeeplearning unimodal models.
video analysis [266], [267]. In the work in [251], the authors Regarding the pre-existing multimodal models, there are
suggestusingposeinformationfromvideosofpeoplewalking many examples relying on the Transformer architecture, in-
topredictsymptomsandthediseaseintheirdataset.Thispose cluding VideoBERT [282] or VisualBERT [283], among oth-
information is used to train a recurrent GRU model, which ers.Theworkproposedin[81]experimentedwithmultimodal
achieves an accuracy of 82.5%. CLIP [279] and ViLBERT [284]. Additionally, experiments
Eye movement tracking is a technique used to monitor have been carried out on the fusion of ResNet and BERT
where individuals focus their gaze, providing valuable in- features obtained from unimodal approaches. These experi-
sights into their visual attention. This data can be obtained ments were performed on the ADReSSo dataset to predict
through video analysis, similar to how textual data can be Alzheimer’s disease. ViLBERT achieved the best results, with
derived from the transcription of spoken conversations. In an accuracy and F1-Score of up to 83.7%, outperforming the
the study proposed in [96], this method has been applied to fusion of single modality approaches.
their dataset to predict Alzheimer’s and MCI. Research has The other methodology to combine different modalities is
demonstrated a connection between impaired eye movements theuseoffusionstrategies.Thesefusionscanbeimplemented
andcognitivedisorders,astheseconditionscanaffectsubjects’ in diverse ways and for various purposes, primarily including
attention [268]–[271]. In this study, subjects were asked to early,cross-modal,late,andjointfusionstrategies.Thegoalof
describe an image that had been segmented into different these fusions is to integrate different modalities into a single
regions of interest. By analyzing eye movement data, the representation of the data.
researchers could determine which areas the subjects focused The early fusion strategy involves combining different
onwhiledescribingtheimage.Thisinformationhasbeenused sources of information before they are processed by the
to train a CNN, specifically the VGG13 model, achieving an model [285], [286]. These fusion strategies are also applied
accuracy of 72.21%. in pre-existing multimodal Transformers. One example is the
previously mentioned VideoBERT, a Transformer model that
VI. MULTIMODALITY utilizes early concatenation to encode the full multimodal
context [287]. In [84], an early fusion strategy was used to
Humans engage with the environment through multiple combine audio and text representations, achieving 81.25%
sensory channels to gather diverse information for decision accuracy on the ADReSS dataset and outperforming single-
making. We use our senses to listen, observe, feel, taste, modality approaches.
and interpret facial expressions, among other cues [272]. The Cross-modal attention allows interactions across different
combination of different sources of information is known as modalities within a single model [288], [289]. For instance, a
a multimodal approach, and it can be very beneficial because cross-modal Transformer encoder combines the key and value
it provides a better global understanding of our environment, vectorsofonemodalitywiththequeryofanother[273],[290].
enabling us to make better decisions. Thismethod,knownascross-attention[291],wasfirstusedin
As mentioned in the previous section, different sources the VilBERT model [284]. In [96], the authors introduce a
of information offer additional insights into the problem we cross-modal attention Transformer that combines eye move-
are addressing [273]–[275]. Combining these sources can be ment from video and audio features. This approach achieved
highly beneficial, as each modality complements the others. an accuracy of 84.26% on their dataset, outperforming single
For instance, text alone does not convey how people speak, modalitiy approaches.
suchastheirtoneofhesitationorspeechjitter.However,com- In the study [276], authors propose the employment of
bining text (how people structure and choose thoughts) with Mel Spectrograms in combination with Visual Transformers,
audio (how they utter them) provides a better understanding specifically the ViT model, for the audio modality. While this
thanusingeithermodalityalone.TheworkspresentedinTable approach is integrated into their multimodal framework, no
V illustrate this point. This table is organized by disorder results are presented for the unimodal method. This work
and further categorized by dataset. It presents the year of introduces a cross-modal attention Transformer to integrate
publication, the methodology used for combining modalities, audio and text representations. An additional fusion strategy
the modalities themselves, whether the multimodal approach is also considered, which involves the joint fusion of these
outperformsunimodalmethods,andperformancemetricssuch representationsthroughconcatenation.Nevertheless,thecross-
as accuracy, F1-Score, and RMSE. The best results for each modal attention Transformer demonstrates superior perfor-
dataset are highlighted in bold. mance, attaining up to 90% accuracy and F1-Score on the
Recent advances in technology have focused on integrating ADReSSo dataset. The work proposed by W. Ning et al. [77]
multiple modalities into a single model capable of processing presents an attention layer to fuse cross-modal attention in
them simultaneously [279]–[281]. Various approaches exist theaudioandtextmodalities,achievingupto77.2%accuracy
for combining modalities, such as employing pre-existing and 77.6% F1-Score on the same dataset. The study in [88]
multimodal models or using fusion strategies, which can be proposes integrating BERT for the text modality as part of
implemented in several ways. In this survey, the majority of their multimodal framework. However, no results are reportedTABLEV
PERFORMANCECOMPARISONOFMULTIMODALMETHODSOVERDIFFERENTCOGNITIVEDISORDERSANDDATASETS.FORSIMPLICITY,“CMA”DENOTES
CROSS-MODALATTENTION,“IMP.”DENOTESIMPROVEMENT,“A”DENOTESAUDIOMODALITY,“T”DENOTESTEXTMODALITY,“V”DENOTESVIDEO
MODALITY,AND“?”DENOTESWHETHERTHEPERFORMANCEOFUNIMODALMODELSHASNOTBEENREPORTED,ANDWECANNOTCONFIRMIFTHE
MULTIMODALAPPROACHACHIEVEDBETTERORWORSERESULTS.
Score
Disorder Dataset Work Year Method Modalities Imp.
Acc F1 RMSE
[276] 2024 CMA A&T ? 90.00 90.00 -
[76] 2023 Joint A&T ✓ - 92.80 -
[79] 2024 Joint A&T ✓ 87.32 87.25 -
ADReSSo [80] 2021 Joint A&T ✓ 84.00 - 4.26
[78] 2023 Joint A&T ✓ 83.70 83.80 -
[81] 2023 Transformers A&T ✓ 83.70 83.70 -
[77] 2021 CMA A&T ✓ 77.2 77.6 -
[277] 2022 Joint A&T ? 90.00 89.94 -
[88] 2023 CMA A&T ✓ 88.33 88.69 -
Alzheimer [278] 2022 Joint A&T ? 86.25 85.48 -
[82] 2022 Late A&T ✓ 86.00 - -
ADReSS [85] 2020 Late A&T ✓ 85.20 85.40 4.65
[87] 2021 Joint A&T ✓ 82.92 - -
[83] 2020 Joint A&T ✓ 81.25 - 3.77
[84] 2020 Early A&T ✓ 81.25 - -
[86] 2021 Joint A&T ✓ 72.92 - -
DementiaBank [91] 2023 Joint A&T ✗ 86.65 - -
PittCorpus [92] 2022 Joint A&T ✗ 81.00 80.00 -
Owndataset [93] 2023 Joint A&T ✗ 77.20 76.30 -
Taukadial [94] 2024 Joint A&T ✓ 75.09 78.49 2.93
MCI I-CONECT [95] 2024 CMA T&V ✓ 81.57 76.80 -
Owndataset [96] 2022 CMA A&V ✓ 84.26 - -
Alzheimer NCMMSC2021L [78] 2023 Joint A&T ✓ 89.10 88.60 -
&MCI NCMMSC2021S [78] 2023 Joint A&T ✓ 84.00 83.50 -
for this unimodal approach. In this study, authors explores outperform the proposed combinations.
different fusion strategies, including cross-modal attention, The late fusion strategy involves combining the results of
joint fusion, and a Gated Multimodal Unit [292]. In their each unimodal model after processing them separately [293].
experimentsontheADReSSdataset,thebestperformancewas In [85], the authors propose using majority voting for clas-
achievedusingthecross-modalattentionmechanism,resulting sification tasks and a weighted average of the results for
in 88.33% accuracy and an 88.69% F1-Score, outperforming regression tasks on the ADReSS dataset. This methodology
unimodal approaches. achievedanaccuracyof85.2%,85.4%F1-Score,andaRMSE
score of 4.65. Similarly, the work proposed in [82] also
The work in [95] proposes a cross-modal attention module
employsmajorityvotingfortheirunimodalapproaches.Inthis
to fuse different modalities into bimodal approaches, compar-
study, this fusion strategy yielded 86% accuracy on the same
ing the outcomes obtained from each combination of modali-
dataset, outperforming the unimodal approaches.
ties.Furthermore,theperformanceofunimodalmodelsisalso
presented, offering valuable insights into the most promising A joint or hybrid fusion strategy involves combining fea-
information that can be extracted from this dataset. Among tures obtained from each modality for later use in a final
the unimodal models, the best performance is obtained from model, but only after being processed individually by each
the video modality, followed by the text modality, and then modality model [294]. This combination is the most common
the audio modality. This dataset consists in video recordings approach in the selected works for this study.
of elderly individuals having conversations, having the visual The work proposed in [76] employs a concatenation of
features relevant information. Consistent with this, the best the features obtained from each modality model, followed
performance is achieved by combining video and text modal- by training a Transformer encoder. This approach achieved
ities, outperforming all other bimodal and unimodal models, a maximum F1-Score of 92.8% on the ADReSSo dataset.
withupto81.57%accuracyand76.80%F1-Score.Thesecond Similarly,theworkproposedbyB.Jeong-Uketal.[79]andM.
mosteffectivecombinationcomesfromfusingaudioandvideo Rohania [80] uses a concatenation of features for subsequent
modalities,improvingtheremainingunimodalmodels.Finally, training of a dense layer. The former yielded an accuracy of
thecombinationofaudioandtextalsoimprovestheunimodal 87.32% and F1-Score of 87.25% on the ADReSSo dataset,
models but ranks as the weakest bimodal combination. This while the latter resulted in an accuracy of 84% and RMSE
study demonstrates the relevance of combining different data scoreof4.26onthesamedataset.In[78],afterconcatenation,
sources to achieve better understanding and performance. the trained model was a SVM, which achieved an accuracy of
However, the authors do not present a combination of all 83.7% and an F1-Score of 83.8% on the ADReSSo dataset.
threemodalitiesintoatrimodalmodel,whichcouldpotentially This methodology was also tested on the NCMMSC2021dataset, achieving up to 89.1% accuracy and F1-Score of VII. DISCUSSION
88.6% for large sentences. In the case of short sentences, the
In previous sections, the most promising publicly available
results yielded 84% accuracy and F1-Score of 83.5%.
datasetsandmethodologiesforcognitivestateestimationusing
In [86], the authors introduce a LSTM model for the text non-invasive modalities have been reviewed. This section
modality,whichisimplementedintheirmultimodalapproach. presents the main findings of our analysis, together with
However,noresultsareprovidedusingonlythismodality.For suggestions for future research directions. These findings are
the combination of features, authors propose concatenating relatedtocognitivedisorders,datasets,andbothunimodaland
the features followed by a dense layer for final multimodal multimodal approaches.
classification. This approach, applied to the ADReSS dataset, Cognitive disorders. In addition to the natural decline
yielded 72.92% accuracy. Similarly, the work in [87] employs associated with aging, several disorders can affect cognitive
this methodology and compares it with a late fusion strategy. abilities. Depending on the disorder, the impact on cognitive
In this study, the joint approach outperforms the late one, functions can vary, and even in some cases, it can lead to
achieving 82.92% accuracy on the same dataset. In the case dependency. Early detection is crucial to start professional
of [278], instead of concatenation, joint fusion is performed treatment that can significantly improve a patient’s quality
by taking the product of the features, followed by a dense of life. These cognitive disorders include dementia, MCI,
layer for final multimodal classification. This method yielded aphasia,Parkinson’sdisease,andapathy.Asmentionedearlier,
accuracy of 86.25% and 85.48% F1-Score on the ADReSS our society is experiencing an aging trend, leading to an
dataset. However, it is uncertain whether this approach could increasingly older population. This demographic shift results
improve on unimodal approaches, as unimodal results are not in a growing prevalence of age-related disorders, such as
reported. dementia. Most research in this field focuses on dementia,
primarilyduetotheavailabilityofrelevantdatasets.However,
In the work in [93], the authors propose implementing
recent studies have also begun addressing other conditions,
a joint concatenation followed by a dense layer, comparing
such as MCI, and in some cases, exploring the combination
the results with early and late fusion strategies. The best
of these two disorders.
performance was achieved by the joint function, with an
Datasets. Given the sensitivity of this topic, large datasets
accuracy of 77.2% and F1-Score of 77.6% on their dataset.
for training deep learning models are not readily available,
However, this multimodal approach did not outperform the
posing a significant limitation in the field. Existing datasets
audio-proposed modality.
vary in format and encompass diverse types of information.
For instance, some include multiple cognitive tasks, such as
In [83], the authors experiment with joint concatenation
image description or fluency tasks. While the majority of
followed by a MLP on the ADReSS dataset. The results
datasets focus on Alzheimer’s disease, there are also datasets
achieved were 81.25% accuracy and an RMSE score of 3.77.
targeting other conditions, such as MCI.
Similarly, the work in [91] employs concatenation followed
Most of these datasets are based on audio and text modal-
by an MLP, yielding 86.65% accuracy on the DementiaBank
ities, largely due to privacy concerns associated with video
Pitt Corpus, though it does not improve on the text modality
data, particularly when faces are recorded. However, the
results. In the work in [92], the authors propose a joint
datasets are relatively small for deep learning purposes, typi-
concatenationfollowedbyanMLP,contrastingtheresultswith
callycontainingbetween250and500samples,withthelargest
earlyandlatefusionstrategies.Thebestresultswereachieved
containing only around 600 samples (excluding Dementia-
by the joint fusion, with 81% accuracy and an F1-Score of
Bank, where an exact sample count is difficult to ascertain
80%. The authors in [94] also propose a joint concatenation
duetothevariouscorporaitcomprises).Intermsoflanguage,
followed by an MLP, achieving up to 75.09% accuracy, an
English is the most commonly represented, present in all but
F1-Score of 78.49%, and an RMSE score of 2.93 on the
twoof thedatasets, andin manycases,it isthe onlylanguage
Taukadial dataset. Finally, the work in [277] proposes a joint
available.
concatenation followed by an attention module and a dense
layer, outperforming other proposed fusion strategies, such as In order to address the issue of data scarcity, various data
cross-attention.Thismethodologyachieved90%accuracyand augmentation techniques can be applied, depending on the
an F1-Score of 89.94% on the ADReSS dataset. specific type of data. In this study, several works that utilize
such techniques have been highlighted, showing significant
The application of multimodal approaches, which use all performanceimprovementscomparedtonotusingthem.These
available information for a task, has been shown to enhance approaches hold promise for future research directions. An
the outcomes of unimodal models in most situations, with alternativeapproachtomitigatingthisissueinvolvestheuseof
few exceptions (see [91]–[93]). Therefore, employing these largepre-trainedmodels,particularlyinfew-shotandzero-shot
methodologies is crucial for improving not only the perfor- learning scenarios. These architectures reduce the reliance on
mance of the models but also their robustness. Moreover, large, densely labeled datasets for training downstream tasks.
the joint fusion approach is the most prominent and usually A notable example is the use of Vision Language Models
achieves the best results. (VLMs) for video understanding [295]–[297].Unimodal Approaches. There are several methods for existing multimodal models or by fusing modalities. The best
estimating cognitive states using various information sources. results are typically obtained through joint fusion strategies,
In this study, the focus has been on non-intrusive approaches, which generally outperform other fusion techniques in com-
including audio, text, images, and video. The most common parative studies. However, the use of pre-existing multimodal
and relevant modalities are audio and text. Audio data can be models is limited, with only one instance identified in the
analyzed through various deep learning techniques, providing literature. This indicates a promising avenue for research
valuable insights into speech articulation and pronunciation. in this area, particularly concerning multimodal models that
However, one limitation of this modality is the extensive incorporate extensive pretraining and rich knowledge bases,
preprocessing required to handle the data effectively. Among which may exhibit substantial performance in this task.
these methodologies, Transformer models stand out, with the Future directions. One crucial aspect that must be con-
Wav2Vec2 model, published in 2020, showing the most no- sidered, though often overlooked in related works, is the
table results in this field. While Wav2Vec2 has proven highly explainability of the models. When dealing with sensitive
effective, more recent Transformer architectures may offer medical data, it is essential to understand why and how
even better performance for audio-based tasks. Investigating predictions are made in order to extract meaningful insights.
howthesenewermodelscanbeappliedtoaudiodatapresents Additionally, this information can bring useful insights about
an exciting direction for future research. the processed data. While recent research has increasingly
For textual data, Transformer architectures are the most emphasized explainability, further efforts are needed in this
widely used approach. When combined with linguistic fea- area rather than simply deploying black-box models [298]–
tures, they achieve remarkable results. Many studies demon- [300].
strate that this architecture outperforms audio-based ap- An additional potential source of valuable information
proaches across different datasets, with few exceptions. This comes from the use of Human Digital Twins (HDTs). These
suggests that transcribed text often provides more relevant digital representations of individuals can include comprehen-
information, offering deeper insights into how individuals sive medical records, offering crucial insights into a patient’s
structure their thoughts in sentences, as opposed to how they health status. Furthermore, HDTs can simulate and analyze
vocalize them. Conversely, modalities such as image or video daily human activities, generating synthetic data that could
are not present in the selected studies due to the scarcity be beneficial for training models aimed at cognitive decline
of available datasets. Nonetheless, these modalities can be estimation [301], [302].
highlybeneficialforestimatingcognitivedecline,asevidenced
by studies analyzing facial expressions during speech or gait
VIII. CONCLUSIONS
patterns in diseases such as Parkinson’s. Cognitive abilities are essential in our daily lives, influ-
Multimodal Approaches. The integration of different encing how we interact with our environment and others. A
modalities has demonstrated significant potential and fre- declineintheseabilitiesovertime,asweage,isanormaland
quently results in enhanced performance. By integrating vari- natural process. However, in some cases, this deterioration is
ous data sources, a more comprehensive understanding of the more severe in some individuals. This severity is often due to
problem can be achieved. The advantages of a multimodal differentdisorders.Earlydetectionofsignificantcognitivede-
approach are evident: it facilitates improved insights and fos- clinecanbehighlybeneficial,allowingfortimelyprofessional
tersthedevelopmentofmorerobustsolutions.Thisrobustness intervention and improving quality of life.
arises from the model’s capacity to utilize diverse sources of In this work, we have systematically reviewed the most
information, reducing its reliance on a single data type for relevant methodologies for analyzing and predicting this de-
makingpredictions.Suchanapproachisparticularlybeneficial cline,focusingonnon-intrusivemethods.Theseapproachesdo
in scenarios where certain modalities may be missing or not disrupt daily activities, preserving quality of life without
irrelevant. relying on invasive techniques like medical imaging.
However, challenges persist. Processing different types of To the best of our knowledge, this study represents the
information necessitates a more complex architecture for ef- first survey of estimation of general cognitive decline based
fectivelycombiningthemodalities.Moreover,thetrainingand on non-invasive data. The methodologies covered include
inferenceproceduresbecomemoretime-consumingduetothe audio, textual, and visual analyses, leveraging deep learning
requirement for integrating diverse data sources. The appli- techniques. We also assess the key features and benefits of
cation of multimodal strategies, which leverage all available each approach, their integration into multimodal frameworks,
information for a given task, has been shown to enhance andtheirperformanceonstandarddatasets.Finally,wepresent
the outcomes of unimodal models in 75% of the approaches the most significant datasets relevant to this task, providing
studied. Conversely, the combination of modalities resulted detailed information about their content, including the activ-
in inferior performance in 12.5% of the cases, while another ities performed by subjects, the disorders studied, and the
12.5% of studies did not report the performance of unimodal modalities provided.
models,makingitimpossibletoconcludethatthecombination We conclude that there is a significant lack of sufficient
consistently yields superior performance. datasets in this area, with limited options that are not optimal
This integration can be achieved through the use of pre- in terms of sample size. Existing datasets typically focus onaudio and text modalities, primarily due to the sensitivity of [11] S. Liu, S. Liu, W. Cai, S. Pujol, R. Kikinis, and D. Feng, “Early
thedatainvolved.Forexample,videodatacanraiseadditional diagnosis of alzheimer’s disease with deep learning,” in 2014 IEEE
11th international symposium on biomedical imaging (ISBI). IEEE,
concerns regarding privacy and ethical considerations, which
2014,pp.1015–1018.
further limits its availability and use. Consequently, most [12] M. Di Luca, D. Nutt, W. Oertel, P. Boyer, J. Jaarsma, F. Destre-
proposed studies rely on audio and text modalities, while becq,G.Esposito,andV.Quoidbach,“Towardsearlierdiagnosisand
treatment of disorders of the brain,” Bulletin of the World Health
othernon-intrusivemodalities,includingvideo,haveveryfew
Organization,vol.96,no.5,p.298,2018.
representations and often use their own recorded datasets. [13] D.Wen,Z.Wei,Y.Zhou,G.Li,X.Zhang,andW.Han,“Deeplearning
From our discussion comparing these studies, we also methodstoprocessfmridataandtheirapplicationinthediagnosisof
cognitiveimpairment:abriefoverviewandouropinion,”Frontiersin
determine that the text modality is the most relevant feature,
neuroinformatics,vol.12,p.23,2018.
yielding the most significant results across the datasets. Fur- [14] H. Taheri Gorji and N. Kaabouch, “A deep learning approach for
thermore, the combination of these modalities generally leads diagnosis of mild cognitive impairment based on mri images,” Brain
sciences,vol.9,no.9,p.217,2019.
to improved performance, surpassing any unimodal approach.
[15] L. Kang, J. Jiang, J. Huang, and T. Zhang, “Identifying early mild
This is because different data types complement one another, cognitive impairment by multi-modality mri-based deep learning,”
providing a more comprehensive understanding of the prob- Frontiersinagingneuroscience,vol.12,p.206,2020.
[16] X.Feng,Z.C.Lipton,J.Yang,S.A.Small,F.A.Provenzano,A.D.N.
lem.
Initiative,F.L.D.N.Initiativeetal.,“Estimatingbrainagebasedona
uniformhealthypopulationwithdeeplearningandstructuralmagnetic
ACKNOWLEDGMENTS resonanceimaging,”Neurobiologyofaging,vol.91,pp.15–25,2020.
[17] S.Chauhan,L.Vig,M.DeFilippoDeGrazia,M.Corbetta,S.Ahmad,
This work has been funded by the Valencian regional and M. Zorzi, “A comparison of shallow and deep learning methods
government CIAICO/2022/132 Consolidated group project forpredictingcognitiveperformanceofstrokepatientsfrommrilesion
images,”Frontiersinneuroinformatics,vol.13,p.53,2019.
AI4Health, by Valencian government and International Cen-
[18] G.Zhu,B.Jiang,L.Tong,Y.Xie,G.Zaharchuk,andM.Wintermark,
ter for Aging Research ICAR funded project “IASISTEM”. “Applicationsofdeeplearningtoneuro-imagingtechniques,”Frontiers
This work has also been supported by a Spanish national inneurology,vol.10,p.869,2019.
and a regional grant for PhD studies, FPU21/00414, and [19] T.Jo,K.Nho,andA.J.Saykin,“Deeplearninginalzheimer’sdisease:
diagnosticclassificationandprognosticpredictionusingneuroimaging
CIACIF/2022/175.
data,”Frontiersinagingneuroscience,vol.11,p.220,2019.
[20] E. Pellegrini, L. Ballerini, M. d. C. V. Hernandez, F. M. Chappell,
REFERENCES V. Gonza´lez-Castro, D. Anblagan, S. Danso, S. Mun˜oz-Maniega,
D. Job, C. Pernet et al., “Machine learning of neuroimaging for
[1] D. Kiely, “Cognitive function in encyclopaedia of quality of life and assisteddiagnosisofcognitiveimpairmentanddementia:asystematic
well-beingresearch(ed.michalos,ac)974–978,”2014. review,” Alzheimer’s & Dementia: Diagnosis, Assessment & Disease
[2] I.J.Deary,J.Corley,A.J.Gow,S.E.Harris,L.M.Houlihan,R.E. Monitoring,vol.10,pp.519–535,2018.
Marioni, L. Penke, S. B. Rafnsson, and J. M. Starr, “Age-associated [21] S.A.Graham,E.E.Lee,D.V.Jeste,R.VanPatten,E.W.Twamley,
cognitive decline,” British Medical Bulletin, vol. 92, no. 1, pp. 135– C. Nebeker, Y. Yamada, H.-C. Kim, and C. A. Depp, “Artificial
152,092009.[Online].Available:https://doi.org/10.1093/bmb/ldp033 intelligence approaches to predicting and detecting cognitive decline
[3] S.Duong,T.Patel,andF.Chang,“Dementia:Whatpharmacistsneed inolderadults:Aconceptualreview,”Psychiatryresearch,vol.284,p.
to know,” Canadian Pharmacists Journal / Revue des Pharmaciens 112732,2020.
du Canada, vol. 150, no. 2, pp. 118–129, 2017. [Online]. Available: [22] H. Choi, K. H. Jin, A. D. N. Initiative et al., “Predicting cognitive
https://doi.org/10.1177/1715163517690745 declinewithdeeplearningofbrainmetabolismandamyloidimaging,”
[4] F. Portet, P. J. Ousset, P. J. Visser, G. B. Frisoni, F. Nobili, Behaviouralbrainresearch,vol.344,pp.103–109,2018.
P. Scheltens, B. Vellas, J. Touchon, and the MCI Working Group [23] S. Grueso and R. Viejo-Sobera, “Machine learning methods for pre-
of the European Consortium on Alzheimer’s Disease (EADC), “Mild dictingprogressionfrommildcognitiveimpairmenttoalzheimer’sdis-
cognitive impairment (mci) in medical practice: a critical review of easedementia:asystematicreview,”Alzheimer’sresearch&therapy,
theconceptandnewdiagnosticprocedure.reportofthemciworking vol.13,pp.1–29,2021.
groupoftheeuropeanconsortiumonalzheimer’sdisease,”Journalof [24] S. L. Warren and A. A. Moustafa, “Functional magnetic resonance
Neurology, Neurosurgery & Psychiatry, vol. 77, no. 6, pp. 714–718, imaging,deeplearning,andalzheimer’sdisease:Asystematicreview,”
2006.[Online].Available:https://jnnp.bmj.com/content/77/6/714 JournalofNeuroimaging,vol.33,no.1,pp.5–18,2023.
[5] P. Celsis, “Age-related cognitive decline, mild cognitive impairment [25] M.Ansart,S.Epelbaum,G.Bassignana,A.Boˆne,S.Bottani,T.Cattai,
or preclinical alzheimer’s disease?” Annals of Medicine, vol. 32, R. Couronne´, J. Faouzi, I. Koval, M. Louis et al., “Predicting the
no. 1, pp. 6–14, 2000, pMID: 10711572. [Online]. Available: progression of mild cognitive impairment using machine learning: a
https://doi.org/10.3109/07853890008995904 systematic, quantitative and critical review,” Medical Image Analysis,
[6] World Health Organization, “Mental health action plan 2013-2020,” vol.67,p.101848,2021.
WHO Library Cataloguing-in-Publication DataLibrary Cataloguing- [26] T.Jo,K.Nho,andA.J.Saykin,“Deeplearninginalzheimer’sdisease:
in-PublicationData,pp.1–44,2023. diagnosticclassificationandprognosticpredictionusingneuroimaging
[7] A. R. Damasio, “Aphasia,” New England Journal of Medicine, vol. data,”Frontiersinagingneuroscience,vol.11,p.220,2019.
326,no.8,pp.531–539,1992. [27] A. Alberdi, A. Aztiria, and A. Basarab, “On the early diagnosis
[8] C. Cabello-Collado, J. Rodriguez-Juan, D. Ortiz-Perez, J. Garcia- of alzheimer’s disease from multimodal signals: A survey,” Artificial
Rodriguez, D. Toma´s, and M. F. Vizcaya-Moreno, “Automated IntelligenceinMedicine,vol.71,pp.1–29,2016.[Online].Available:
generation of clinical reports using sensing technologies with deep https://www.sciencedirect.com/science/article/pii/S0933365716300732
learningtechniques,”Sensors,vol.24,no.9,2024.[Online].Available: [28] V. Skaramagkas, A. Pentari, Z. Kefalopoulou, and M. Tsiknakis,
https://www.mdpi.com/1424-8220/24/9/2751 “Multi-modal deep learning diagnosis of parkinson’s disease—a sys-
[9] R. Zhao, R. Yan, Z. Chen, K. Mao, P. Wang, and R. X. Gao, tematic review,” IEEE Transactions on Neural Systems and Rehabili-
“Deep learning and its applications to machine health monitoring,” tationEngineering,vol.31,pp.2399–2423,2023.
Mechanical Systems and Signal Processing, vol. 115, pp. 213–237, [29] Q. Yang, X. Li, X. Ding, F. Xu, and Z. Ling, “Deep learning-based
2019. speechanalysisforalzheimer’sdiseasedetection:aliteraturereview,”
[10] J. Venugopalan, L. Tong, H. R. Hassanzadeh, and M. D. Wang, Alzheimer’sResearch&Therapy,vol.14,no.1,p.186,2022.
“Multimodal deep learning models for early detection of alzheimer’s [30] X. Qi, Q. Zhou, J. Dong, and W. Bao, “Noninvasive
diseasestage,”Scientificreports,vol.11,no.1,p.3254,2021. automatic detection of alzheimer’s disease from spontaneousspeech: a review,” Frontiers in Aging Neuroscience, vol. 15, the prisma statement,” International journal of surgery, vol. 8, no. 5,
2023. [Online]. Available: https://www.frontiersin.org/journals/ pp.336–341,2010.
aging-neuroscience/articles/10.3389/fnagi.2023.1224723 [52] A. M. Lanzi, A. K. Saylor, D. Fromm, H. Liu, B. MacWhinney, and
[31] S. Gauthier, B. Reisberg, M. Zaudig, R. C. Petersen, K. Ritchie, M. L. Cohen, “Dementiabank: Theoretical rationale, protocol, and il-
K. Broich, S. Belleville, H. Brodaty, D. Bennett, H. Chertkow et al., lustrativeanalyses,”AmericanJournalofSpeech-LanguagePathology,
“Mildcognitiveimpairment,”Thelancet,vol.367,no.9518,pp.1262– vol.32,no.2,pp.426–438,2023.
1270,2006. [53] J.T.Becker,F.Boiler,O.L.Lopez,J.Saxton,andK.L.McGonigle,
[32] R. C. Petersen, “Mild cognitive impairment,” CONTINUUM: lifelong “Thenaturalhistoryofalzheimer’sdisease:descriptionofstudycohort
LearninginNeurology,vol.22,no.2,pp.404–418,2016. andaccuracyofdiagnosis,”Archivesofneurology,vol.51,no.6,pp.
[33] P.B.Rosenberg,M.M.Mielke,B.Appleby,E.Oh,J.-M.Leoutsakos, 585–594,1994.
andC.G.Lyketsos,“Neuropsychiatricsymptomsinmcisubtypes:the [54] K. P. Elaine Giles and J. R. Hodges, “Performance on the
importanceofexecutivedysfunction,”Internationaljournalofgeriatric boston cookie theft picture description task in patients with
psychiatry,vol.26,no.4,pp.364–372,2011. early dementia of the alzheimer’s type: Missing information,”
[34] Z. Arvanitakis, R. C. Shah, and D. A. Bennett, “Diagnosis and Aphasiology, vol. 10, no. 4, pp. 395–408, 1996. [Online]. Available:
management of dementia,” Jama, vol. 322, no. 16, pp. 1589–1599, https://doi.org/10.1080/02687039608248419
2019. [55] S. Luz, F. Haider, S. de la Fuente, D. Fromm, and B. MacWhinney,
[35] D.S.GeldmacherandP.J.Whitehouse,“Evaluationofdementia,”New “Alzheimer’s dementia recognition through spontaneous speech: The
EnglandJournalofMedicine,vol.335,no.5,pp.330–336,1996. adresschallenge,”2020.
[36] S. H. Ferris and M. Farlow, “Language impairment in alzheimer’s [56] ——, “Detecting cognitive decline using speech only: The adresso
diseaseandbenefitsofacetylcholinesteraseinhibitors,”Clinicalinter- challenge,”2021.
ventionsinaging,pp.1007–1014,2013. [57] S. Luz, F. Haider, D. Fromm, I. Lazarou, I. Kompatsiaris, and
[37] H. S. Kirshner and S. M. Wilson, “Aphasia and aphasic syndromes,” B. MacWhinney, “Multilingual alzheimer’s dementia recognition
Bradley’sNeurologyinClinicalPracticeE-Book,vol.133,2021. through spontaneous speech: a signal processing grand challenge,”
2023.
[38] M. P. Alexander and A. E. Hillis, “Aphasia,” Handbook of clinical
neurology,vol.88,pp.287–309,2008. [58] S. D. L. F. Garcia, F. Haider, D. Fromm, B. MacWhinney, A. Lanzi,
Y.-N. Chang, C.-J. Chou, Y.-C. Liu et al., “Connected speech-
[39] L. V. Kalia and A. E. Lang, “Parkinson’s disease,” The Lancet, vol.
based cognitive assessment in chinese and english,” arXiv preprint
386,no.9996,pp.896–912,2015.
arXiv:2406.10272,2024.
[40] D. Aarsland, K. Andersen, J. P. Larsen, R. Perry, T. Wentzel-Larsen,
[59] B.MacWhinney,D.Fromm,M.Forbes,andA.Holland,“Aphasiabank:
A. Lolk, and P. Kragh-Sørensen, “The rate of cognitive decline in
Methods for studying discourse,” Aphasiology, vol. 25, no. 11, pp.
parkinson disease,” Archives of neurology, vol. 61, no. 12, pp. 1906–
1286–1307,2011.
1911,2004.
[60] C. Pope and B. H. Davis, “Finding a balance: The carolinas
[41] D. Aarsland, B. Creese, M. Politis, K. R. Chaudhuri, D. H. Ffytche,
conversation collection,” Corpus Linguistics and Linguistic Theory,
D.Weintraub,andC.Ballard,“Cognitivedeclineinparkinsondisease,”
vol. 7, no. 1, pp. 143–161, 2011. [Online]. Available: https:
NatureReviewsNeurology,vol.13,no.4,pp.217–231,2017.
//doi.org/10.1515/cllt.2011.007
[42] R. Van Reekum, D. T. Stuss, and L. Ostrander, “Apathy: why care?”
[61] R. A. Li, I. Hajjar, F. Goldstein, and J. D. Choi, “Analysis
The Journal of neuropsychiatry and clinical neurosciences, vol. 17,
of hierarchical multi-content text classification model on B-
no.1,pp.7–19,2005.
SHARP dataset for early detection of Alzheimer’s disease,” in
[43] G.Montoya-Murillo,N.Ibarretxe-Bilbao,J.Pen˜a,andN.Ojeda,“The
Proceedings of the 1st Conference of the Asia-Pacific Chapter
impactofapathyoncognitiveperformanceintheelderly,”International
of the Association for Computational Linguistics and the 10th
JournalofGeriatricPsychiatry,vol.34,no.5,pp.657–665,2019.
InternationalJointConferenceonNaturalLanguageProcessing,K.-F.
[44] G. Konstantakopoulos, D. Ploumpidis, P. Oulis, P. Patrikelis, Wong, K. Knight, and H. Wu, Eds. Suzhou, China: Association
A.Soumani,G.N.Papadimitriou,andA.M.Politis,“Apathy,cognitive for Computational Linguistics, Dec. 2020, pp. 358–365. [Online].
deficits and functional impairment in schizophrenia,” Schizophrenia Available:https://aclanthology.org/2020.aacl-main.38
research,vol.133,no.1-3,pp.193–198,2011.
[62] D.Carr,“Howtosuccessfullynavigatearevise-and-resubmitdecision
[45] S.Gluhm,J.Goldstein,K.Loc,A.Colt,C.VanLiew,andJ.Corey- and handle rejections,” Innovation in Aging, vol. 3, no. Suppl 1, p.
Bloom,“Cognitiveperformanceonthemini-mentalstateexamination S224,2019.
and the montreal cognitive assessment across the healthy adult lifes- [63] K. Yu, K. Wild, K. Potempa, B. M. Hampstead, P. A. Lichtenberg,
pan,” Cognitive and Behavioral Neurology, vol. 26, no. 1, pp. 1–5, L. M. Struble, P. Pruitt, E. L. Alfaro, J. Lindsley, M. MacDonald,
2013. J. A. Kaye, L. C. Silbert, and H. H. Dodge, “The internet-based
[46] I. Arevalo-Rodriguez, N. Smailagic, M. R. i Figuls, A. Ciapponi, conversational engagement clinical trial (i-conect) in socially isolated
E. Sanchez-Perez, A. Giannakou, O. L. Pedraza, X. B. Cosp, adults 75+ years old: Randomized controlled trial protocol and
and S. Cullum, “Mini-mental state examination (mmse) for the covid-19 related study modifications,” Frontiers in Digital Health,
detection of alzheimer’s disease and other dementias in people vol. 3, 2021. [Online]. Available: https://www.scopus.com/inward/
with mild cognitive impairment (mci),” Cochrane Database of record.uri?eid=2-s2.0-85121367382&doi=10.3389%2ffdgth.2021.
Systematic Reviews, vol. 3, 2015. [Online]. Available: https: 714813&partnerID=40&md5=dad167760b9a0f0c30c4a9e08937c379
//doi.org//10.1002/14651858.CD010783.pub2 [64] C.-Y.Wu,N.Mattek,K.Wild,L.M.Miller,J.A.Kaye,L.C.Silbert,
[47] J. S. Shiroky, H. M. Schipper, H. Bergman, and H. Chertkow, “Can andH.H.Dodge,“Canchangesinsocialcontact(frequencyandmode)
youhavedementiawithanmmsescoreof30?”AmericanJournalof mitigate low mood before and during the covid-19 pandemic? the i-
Alzheimer’sDisease&OtherDementias®,vol.22,no.5,pp.406–415, conect project,” Journal of the American Geriatrics Society, vol. 70,
2007. no.3,pp.669–676,2022.
[48] P. T. Trzepacz, H. Hochstetler, S. Wang, B. Walker, A. J. Saykin, [65] J. R. Orozco-Arroyave, J. D. Arias-London˜o, J. F. Vargas-Bonilla,
andA.D.N.Initiative,“Relationshipbetweenthemontrealcognitive M. C. Gonza´lez-Ra´tiva, and E. No¨th, “New Spanish speech corpus
assessmentandmini-mentalstateexaminationforassessmentofmild database for the analysis of people suffering from Parkinson’s
cognitiveimpairmentinolderadults,”BMCgeriatrics,vol.15,pp.1–9, disease,” in Proceedings of the Ninth International Conference
2015. on Language Resources and Evaluation (LREC’14), N. Calzolari,
[49] S. Hoops, S. Nazem, A. Siderowf, J. Duda, S. Xie, M. Stern, and K. Choukri, T. Declerck, H. Loftsson, B. Maegaard, J. Mariani,
D.Weintraub,“Validityofthemocaandmmseinthedetectionofmci A. Moreno, J. Odijk, and S. Piperidis, Eds. Reykjavik, Iceland:
and dementia in parkinson disease,” Neurology, vol. 73, no. 21, pp. European Language Resources Association (ELRA), May 2014, pp.
1738–1745,2009. 342–347. [Online]. Available: http://www.lrec-conf.org/proceedings/
[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. lrec2014/pdf/7 Paper.pdf
Gomez,L.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”2023. [66] A. Karakostas, A. Briassouli, K. Avgerinakis, I. Kompatsiaris, and
[51] D. Moher, A. Liberati, J. Tetzlaff, D. G. Altman, P. Group et al., M. Tsolaki, “The dem@care experiments and datasets: a technical
“Preferred reporting items for systematic reviews and meta-analyses: report,”2016.[Online].Available:https://arxiv.org/abs/1701.01142[67] F.Negin,P.Rodriguez,M.Koperski,A.Kerboua,J.Gonza`lez,J.Bour- [87] Y. Zhu, X. Liang, J. A. Batsis, and R. M. Roth, “Exploring
geois, E. Chapoulie, P. Robert, and F. Bremond, “Praxis: Towards deeptransferlearningtechniquesforalzheimer’sdementiadetection,”
automatic cognitive assessment using gesture recognition,” Expert Frontiers in Computer Science, vol. 3, 2021. [Online]. Available:
SystemswithApplications,2018. https://www.frontiersin.org/articles/10.3389/fcomp.2021.624683
[68] J. V. Egas-Lo´pez, R. Balogh, N. Imre, I. Hoffmann, M. K. Szabo´, [88] L. Ilias, D. Askounis, and J. Psarras, “Detecting dementia from
L.To´th,M.Pa´ka´ski,J.Ka´lma´n,andG.Gosztolya,“Automaticscreen- speech and transcripts using transformers,” Computer Speech &
ingofmildcognitiveimpairmentandalzheimer’sdiseasebymeansof Language, vol. 79, p. 101485, 2023. [Online]. Available: https:
posterior-thresholding hesitation representation,” Computer Speech & //www.sciencedirect.com/science/article/pii/S0885230823000049
Language,vol.75,p.101377,2022. [89] A. Meghanani, C. Anoop, and A. Ramakrishnan, “An exploration
[69] C.Themistocleous,M.Eckerstro¨m,andD.Kokkinakis,“Voicequality of log-mel spectrogram and mfcc features for alzheimer’s dementia
andspeechfluencydistinguishindividualswithmildcognitiveimpair- recognitionfromspontaneousspeech,”in2021IEEEspokenlanguage
ment from healthy controls,” Plos one, vol. 15, no. 7, p. e0236009, technologyworkshop(SLT). IEEE,2021,pp.670–677.
2020. [90] Z. Liu, Z. Guo, Z. Ling, and Y. Li, “Detecting alzheimer’s disease
[70] B.J.Abbaschian,D.Sierra-Sosa,andA.Elmaghraby,“Deeplearning from speech using neural networks with bottleneck features and data
techniquesforspeechemotionrecognition,fromdatabasestomodels,” augmentation,”inICASSP2021-2021IEEEInternationalConference
Sensors,vol.21,no.4,p.1249,2021. onAcoustics,SpeechandSignalProcessing(ICASSP),2021,pp.7323–
[71] S.G.KoolagudiandK.S.Rao,“Emotionrecognitionfromspeech:a 7327.
review,” International journal of speech technology, vol. 15, pp. 99– [91] D. Ortiz-Perez, P. Ruiz-Ponce, D. Toma´s, J. Garcia-Rodriguez, M. F.
117,2012. Vizcaya-Moreno, and M. Leo, “A deep learning-based multimodal
[72] T.M.Wani,T.S.Gunawan,S.A.A.Qadri,M.Kartiwi,andE.Am- architecture to predict signs of dementia,” Neurocomputing, vol. 548,
bikairajah, “A comprehensive review of speech emotion recognition p. 126413, 2023. [Online]. Available: https://www.sciencedirect.com/
systems,”IEEEaccess,vol.9,pp.47795–47814,2021. science/article/pii/S0925231223005362
[73] D.Ortiz-Perez,P.Ruiz-Ponce,J.Rodr´ıguez-Juan,D.Toma´s,J.Garcia- [92] I.Krstev,M.Pavikjevikj,M.Toshevska,andS.Gievska,“Multimodal
Rodriguez,andG.J.Nalepa,“Deeplearning-basedemotiondetection datafusionforautomaticdetectionofalzheimer’sdisease,”inInterna-
in aphasia patients,” in International Conference on Soft Computing tionalConferenceonHuman-ComputerInteraction. Springer,2022,
ModelsinIndustrialandEnvironmentalApplications. Springer,2023, pp.79–94.
pp.195–204. [93] D. Escobar-Grisales, C. D. R´ıos-Urrego, and J. R. Orozco-Arroyave,
[74] C. Code, G. Hemsley, and M. Herrmann, “The emotional impact of “Deeplearningandartificialintelligenceappliedtomodelspeechand
aphasia,” in Seminars in speech and language, vol. 20. © 1999 by languageinparkinson’sdisease,”Diagnostics,vol.13,no.13,p.2163,
ThiemeMedicalPublishers,Inc.,1999,pp.19–31. 2023.
[75] P. Priyadarshinee, C. J. Clarke, J. Melechovsky, C. M. Y. Lin, [94] D.Ortiz-Perez,J.Garcia-Rodriguez,andD.Toma´s,“Cognitiveinsights
B. B. T., and J.-M. Chen, “Alzheimer’s dementia speech (audio acrosslanguages:Enhancingmultimodalinterviewanalysis,”inInter-
vs. text): Multi-modal machine learning at high vs. low resolution,” speech2024,2024,pp.952–956.
Applied Sciences, vol. 13, no. 7, 2023. [Online]. Available: [95] F. F. Poor, H. H. Dodge, and M. H. Mahoor, “A multimodal
https://www.mdpi.com/2076-3417/13/7/4244 cross-transformer-based model to predict mild cognitive impairment
[76] Z. Cui, W. Wu, W.-Q. Zhang, J. Wu, and C. Zhang, “Transferring using speech, language and vision,” Computers in Biology and
speech-generic and depression-specific knowledge for alzheimer’s Medicine, vol. 182, p. 109199, 2024. [Online]. Available: https:
disease detection,” in 2023 IEEE Automatic Speech Recognition //www.sciencedirect.com/science/article/pii/S0010482524012848
and Understanding Workshop (ASRU). IEEE, Dec. 2023. [Online]. [96] Z. Sheng, Z. Guo, X. Li, Y. Li, and Z. Ling, “Dementia detection
Available:http://dx.doi.org/10.1109/ASRU57964.2023.10389785 by fusing speech and eye-tracking representation,” in ICASSP 2022 -
[77] N. Wang, Y. Cao, S. Hao, Z. Shao, and K. Subbalakshmi, “Modular 2022IEEEInternationalConferenceonAcoustics,SpeechandSignal
multi-modalattentionnetworkforalzheimer’sdiseasedetectionusing Processing(ICASSP),2022,pp.6457–6461.
patientaudioandlanguagedata.”inInterspeech,2021,pp.3835–3839. [97] N. Narendra, B. Schuller, and P. Alku, “The detection of parkinson’s
[78] Y. Ying, T. Yang, and H. Zhou, “Multimodal fusion for alzheimer’s disease from speech using voice source information,” IEEE/ACM
diseaserecognition,”AppliedIntelligence,vol.53,no.12,pp.16029– TransactionsonAudio,Speech,andLanguageProcessing,vol.29,pp.
16040,2023. 1925–1936,2021.
[79] J.-U. Bang, S.-H. Han, and B.-O. Kang, “Alzheimer’s disease recog- [98] Y.Ge,T.Wang,J.Cao,andS.Xu,“Anovelmulti-tasklearningbased
nition from spontaneous speech using large language models,” ETRI automatic speech impairment assessment algorithm,” in 2022 China
Journal,2024. AutomationCongress(CAC),2022,pp.887–892.
[80] M. Rohanian, J. Hough, and M. Purver, “Alzheimer’s dementia [99] S. Allamy and A. L. Koerich, “1d cnn architectures for music genre
recognition using acoustic, lexical, disfluency and speech pause classification,” in 2021 IEEE Symposium Series on Computational
features robust to noisy inputs,” CoRR, vol. abs/2106.15684, 2021. Intelligence(SSCI),2021,pp.01–07.
[Online].Available:https://arxiv.org/abs/2106.15684 [100] S. Abdoli, P. Cardinal, and A. Lameiras Koerich, “End-to-end
[81] S. B. Shah, A. Bhandari, and P. G. Shambharkar, “Leveraging mul- environmental sound classification using a 1d convolutional neural
timodal information in speech data for the non-invasive detection network,” Expert Systems with Applications, vol. 136, pp. 252–
of alzheimer’s disease,” in 2023 14th International Conference on 263,2019.[Online].Available:https://www.sciencedirect.com/science/
Computing Communication and Networking Technologies (ICCCNT), article/pii/S0957417419304403
2023,pp.1–6. [101] K.Zaman,M.Sah,C.Direkoglu,andM.Unoki,“Asurveyofaudio
[82] A.Hle´dikova´,D.Woszczyk,A.Akman,S.Demetriou,andB.Schuller, classificationusingdeeplearning,”IEEEAccess,2023.
“Dataaugmentationfordementiadetectioninspokenlanguage,”2022. [102] Y.M.Costa,L.S.Oliveira,andC.N.SillaJr,“Anevaluationofconvo-
[83] J.Koo,J.Lee,J.Pyo,Y.Jo,andK.Lee,“Exploitingmulti-modalfea- lutionalneuralnetworksformusicclassificationusingspectrograms,”
turesfrompre-trainednetworksforalzheimer’sdementiarecognition,” Appliedsoftcomputing,vol.52,pp.28–38,2017.
inInterspeech,102020,pp.2217–2221. [103] A.Satt,S.Rozenberg,R.Hooryetal.,“Efficientemotionrecognition
[84] A.Pompili,T.Rolland,andA.Abad,“Theinesc-idmulti-modalsystem from speech using deep learning on spectrograms.” in Interspeech,
fortheadress2020challenge,”arXivpreprintarXiv:2005.14646,2020. 2017,pp.1089–1093.
[85] N.Cummins,Y.Pan,Z.Ren,J.Fritsch,V.S.Nallanthighal,H.Chris- [104] Y.Zeng,H.Mao,D.Peng,andZ.Yi,“Spectrogrambasedmulti-task
tensen, D. Blackburn, B. W. Schuller, M. Magimai-Doss, H. Strik audio classification,” Multimedia Tools and Applications, vol. 78, pp.
et al., “A comparison of acoustic and linguistics methodologies for 3705–3722,2019.
alzheimer’s dementia recognition,” in Interspeech 2020. ISCA- [105] D.HowardandJ.Angus,Acousticsandpsychoacoustics. Routledge,
International Speech Communication Association, 2020, pp. 2182– 2013.
2186. [106] B. C. Moore, An introduction to the psychology of hearing. Brill,
[86] P. Mahajan and V. Baths, “Acoustic and language based deep learn- 2012.
ing approaches for alzheimer’s dementia detection from spontaneous [107] S. S. Stevens, J. Volkmann, and E. B. Newman, “A Scale for the
speech,”FrontiersinAgingNeuroscience,vol.13,p.623607,2021. Measurement of the Psychological Magnitude Pitch,” The Journal ofthe Acoustical Society of America, vol. 8, no. 3, pp. 185–190, 01 [128] D. Ortiz-Perez, P. Ruiz-Ponce, D. Toma´s, and J. Garcia-Rodriguez,
1937.[Online].Available:https://doi.org/10.1121/1.1915893 “Deep learning-based dementia prediction using multimodal data,” in
[108] T. Arias-Vergara, P. Klumpp, J. C. Vasquez-Correa, E. No¨th, J. R. 17thInternationalConferenceonSoftComputingModelsinIndustrial
Orozco-Arroyave, and M. Schuster, “Multi-channel spectrograms for and Environmental Applications (SOCO 2022), P. Garc´ıa Bringas,
speechprocessingapplicationsusingdeeplearningmethods,”Pattern H.Pe´rezGarc´ıa,F.J.Martinez-dePison,J.R.VillarFlecha,A.Tron-
AnalysisandApplications,vol.24,pp.423–431,2021. cosoLora,E.A.delaCal,A´.Herrero,F.Mart´ınezA´lvarez,G.Psaila,
[109] H.Purwins,B.Li,T.Virtanen,J.Schlu¨ter,S.-Y.Chang,andT.Sainath, H. Quintia´n, and E. S. Corchado Rodriguez, Eds. Cham: Springer
“Deeplearningforaudiosignalprocessing,”IEEEJournalofSelected NatureSwitzerland,2023,pp.260–269.
TopicsinSignalProcessing,vol.13,no.2,pp.206–219,2019. [129] G. Huang, Z. Liu, and K. Q. Weinberger, “Densely connected
[110] L.JiaoandJ.Zhao,“Asurveyonthenewgenerationofdeeplearning convolutional networks,” CoRR, vol. abs/1608.06993, 2016. [Online].
inimageprocessing,”IeeeAccess,vol.7,pp.172231–172263,2019. Available:http://arxiv.org/abs/1608.06993
[111] S.Minaee,Y.Boykov,F.Porikli,A.Plaza,N.Kehtarnavaz,andD.Ter- [130] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
zopoulos,“Imagesegmentationusingdeeplearning:Asurvey,”IEEE T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient
transactions on pattern analysis and machine intelligence, vol. 44, convolutionalneuralnetworksformobilevisionapplications,”CoRR,
no.7,pp.3523–3542,2021. vol. abs/1704.04861, 2017. [Online]. Available: http://arxiv.org/abs/
[112] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, 1704.04861
X. Wang, G. Wang, J. Cai et al., “Recent advances in convolutional [131] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
neuralnetworks,”Patternrecognition,vol.77,pp.354–377,2018. for image recognition,” CoRR, vol. abs/1512.03385, 2015. [Online].
[113] K. O’shea and R. Nash, “An introduction to convolutional neural Available:http://arxiv.org/abs/1512.03385
networks,”arXivpreprintarXiv:1511.08458,2015. [132] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
[114] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of convo- and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
lutional neural networks: analysis, applications, and prospects,” IEEE parametersand¡0.5mbmodelsize,”arXivpreprintarXiv:1602.07360,
transactionsonneuralnetworksandlearningsystems,vol.33,no.12, 2016.
pp.6999–7019,2021. [133] K. Simonyan and A. Zisserman, “Very deep convolutional networks
[115] J.Devlin,M.Chang,K.Lee,andK.Toutanova,“BERT:pre-training forlarge-scaleimagerecognition,”2015.
ofdeepbidirectionaltransformersforlanguageunderstanding,”CoRR, [134] A. Sherstinsky, “Fundamentals of recurrent neural network (rnn)
vol. abs/1810.04805, 2018. [Online]. Available: http://arxiv.org/abs/ and long short-term memory (lstm) network,” Physica D: Nonlinear
1810.04805 Phenomena, vol. 404, p. 132306, 2020. [Online]. Available:
[116] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled https://www.sciencedirect.com/science/article/pii/S0167278919305974
version of BERT: smaller, faster, cheaper and lighter,” CoRR, vol. [135] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and
abs/1910.01108, 2019. [Online]. Available: http://arxiv.org/abs/1910. C. Feichtenhofer, “Mvitv2: Improved multiscale vision transformers
01108 forclassificationanddetection,”2022.
[117] A.Arnab,M.Dehghani,G.Heigold,C.Sun,M.Lucic,andC.Schmid, [136] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
“Vivit:Avideovisiontransformer,”CoRR,vol.abs/2103.15691,2021. D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
[Online].Available:https://arxiv.org/abs/2103.15691 convolutions,”2014.
[118] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: [137] S.ZagoruykoandN.Komodakis,“Wideresidualnetworks,”2017.
A framework for self-supervised learning of speech representations,” [138] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassification
CoRR, vol. abs/2006.11477, 2020. [Online]. Available: https://arxiv. with deep convolutional neural networks,” in Advances in Neural
org/abs/2006.11477 Information Processing Systems, F. Pereira, C. Burges, L. Bottou,
[119] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, and K. Weinberger, Eds., vol. 25. Curran Associates, Inc., 2012.
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, [Online]. Available: https://proceedings.neurips.cc/paper files/paper/
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
words: Transformers for image recognition at scale,” CoRR, vol. [139] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen,
abs/2010.11929, 2020. [Online]. Available: https://arxiv.org/abs/2010. “Inverted residuals and linear bottlenecks: Mobile networks for
11929 classification,detectionandsegmentation,”CoRR,vol.abs/1801.04381,
[120] J. Pons, O. Slizovskaia, R. Gong, E. Go´mez, and X. Serra, “Timbre 2018.[Online].Available:http://arxiv.org/abs/1801.04381
analysisofmusicaudiosignalswithconvolutionalneuralnetworks,”in [140] M. Tan, B. Chen, R. Pang, V. Vasudevan, and Q. V. Le, “Mnasnet:
201725thEuropeanSignalProcessingConference(EUSIPCO). IEEE, Platform-aware neural architecture search for mobile,” CoRR, vol.
2017,pp.2744–2748. abs/1807.11626, 2018. [Online]. Available: http://arxiv.org/abs/1807.
[121] Y. Zhang, B. Li, H. Fang, and Q. Meng, “Spectrogram transformers 11626
for audio classification,” in 2022 IEEE International Conference on [141] M. Tan and Q. V. Le, “Efficientnet: Rethinking model scaling for
ImagingSystemsandTechniques(IST). IEEE,2022,pp.1–6. convolutional neural networks,” CoRR, vol. abs/1905.11946, 2019.
[122] Y. Gong, Y.-A. Chung, and J. Glass, “Ast: Audio spectrogram trans- [Online].Available:http://arxiv.org/abs/1905.11946
former,”arXivpreprintarXiv:2104.01778,2021. [142] Y.Chuang,C.Liu,andH.Lee,“Speechbert:Cross-modalpre-trained
[123] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and language model for end-to-end spoken question answering,” CoRR,
I.Sutskever,“Robustspeechrecognitionvialarge-scaleweaksupervi- vol. abs/1910.11559, 2019. [Online]. Available: http://arxiv.org/abs/
sion,”2022. 1910.11559
[124] L.Bertinetto,J.Valmadre,J.F.Henriques,A.Vedaldi,andP.H.Torr, [143] S.Gupta,J.Jaafar,W.W.Ahmad,andA.Bansal,“Featureextraction
“Fully-convolutional siamese networks for object tracking,” in Com- using mfcc,” Signal & Image Processing: An International Journal,
puter Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, vol.4,no.4,pp.101–108,2013.
October 8-10 and 15-16, 2016, Proceedings, Part II 14. Springer, [144] L.Muda,M.Begam,andI.Elamvazuthi,“Voicerecognitionalgorithms
2016,pp.850–865. using mel frequency cepstral coefficient (mfcc) and dynamic time
[125] S. Boelders, V. S. Nallanthighal, V. Menkovski, and A. Ha¨rma¨, “De- warping(dtw)techniques,”arXivpreprintarXiv:1003.4083,2010.
tectionofmilddyspneafrompairsofspeechrecordings,”inICASSP [145] V. Tiwari, “Mfcc and its applications in speaker recognition,” Inter-
2020-2020 IEEE International Conference on Acoustics, Speech and national journal on emerging technologies, vol. 1, no. 1, pp. 19–22,
SignalProcessing(ICASSP). IEEE,2020,pp.4102–4106. 2010.
[126] Z. Lian, Y. Li, J. Tao, and J. Huang, “Speech emotion recognition [146] M. Deng, T. Meng, J. Cao, S. Wang, J. Zhang, and H. Fan, “Heart
via contrastive loss under siamese networks,” in Proceedings of the soundclassificationbasedonimprovedmfccfeaturesandconvolutional
Joint Workshop of the 4th Workshop on Affective Social Multimedia recurrent neural networks,” Neural Networks, vol. 130, pp. 22–32,
ComputingandFirstMulti-ModalAffectiveComputingofLarge-Scale 2020.
MultimediaData,2018,pp.21–26. [147] E. Rejaibi, A. Komaty, F. Meriaudeau, S. Agrebi, and A. Othmani,
[127] J.Wang,Y.Qin,Z.Peng,andT.Lee,“Childspeechdisorderdetection “Mfcc-basedrecurrentneuralnetworkforautomaticclinicaldepression
with siamese recurrent network using speech attribute features.” in recognitionandassessmentfromspeech,”BiomedicalSignalProcess-
INTERSPEECH,vol.2,2019,pp.3885–3889. ingandControl,vol.71,p.103107,2022.[148] F. Eyben, M. Wo¨llmer, and B. Schuller, “Opensmile: the munich [166] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur,
versatileandfastopen-sourceaudiofeatureextractor,”inProceedings “X-vectors:Robustdnnembeddingsforspeakerrecognition,”in2018
of the 18th ACM International Conference on Multimedia, ser. MM IEEE international conference on acoustics, speech and signal pro-
’10. New York, NY, USA: Association for Computing Machinery, cessing(ICASSP). IEEE,2018,pp.5329–5333.
2010, p. 1459–1462. [Online]. Available: https://doi.org/10.1145/ [167] T. Lin, Y. Wang, X. Liu, and X. Qiu, “A survey of transformers,” AI
1873951.1874246 open,vol.3,pp.111–132,2022.
[149] F.Eyben,F.Weninger,F.Gross,andB.Schuller,“Recentdevelopments [168] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang,
inopensmile,themunichopen-sourcemultimediafeatureextractor,”in A.Xiao,C.Xu,Y.Xuetal.,“Asurveyonvisiontransformer,”IEEE
Proceedingsofthe21stACMinternationalconferenceonMultimedia, transactions on pattern analysis and machine intelligence, vol. 45,
2013,pp.835–838. no.1,pp.87–110,2022.
[150] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer, “Co- [169] S.Khan,M.Naseer,M.Hayat,S.W.Zamir,F.S.Khan,andM.Shah,
varep—a collaborative voice analysis repository for speech technolo- “Transformersinvision:Asurvey,”ACMcomputingsurveys(CSUR),
gies,”in2014ieeeinternationalconferenceonacoustics,speechand vol.54,no.10s,pp.1–41,2022.
signalprocessing(icassp). IEEE,2014,pp.960–964. [170] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec:
[151] D.Povey,A.Ghoshal,G.Boulianne,L.Burget,O.Glembek,N.Goel, Unsupervised pre-training for speech recognition,” CoRR, vol.
M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The kaldi abs/1904.05862, 2019. [Online]. Available: http://arxiv.org/abs/1904.
speech recognition toolkit,” in IEEE 2011 workshop on automatic 05862
speech recognition and understanding. IEEE Signal Processing [171] Y. Zhuang, Y. Chen, and J. Zheng, “Music genre classification with
Society,2011. transformer classifier,” in Proceedings of the 2020 4th international
[152] P.NimitsurachatandP.Washington,“Audio-basedemotionrecognition conferenceondigitalsignalprocessing,2020,pp.155–159.
using self-supervised learning on an engineered feature space,” AI, [172] F. Andayani, L. B. Theng, M. T. Tsun, and C. Chua, “Hybrid lstm-
vol.5,no.1,pp.195–207,2024. transformer model for emotion recognition from speech audio files,”
[153] A. B. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L.-P. Morency, IEEEAccess,vol.10,pp.36018–36027,2022.
“Multimodal language analysis in the wild: Cmu-mosei dataset and [173] N.VaessenandD.A.VanLeeuwen,“Fine-tuningwav2vec2forspeaker
interpretabledynamicfusiongraph,”inProceedingsofthe56thAnnual recognition,”inICASSP2022-2022IEEEInternationalConferenceon
Meeting of the Association for Computational Linguistics (Volume 1: Acoustics,SpeechandSignalProcessing(ICASSP). IEEE,2022,pp.
LongPapers),2018,pp.2236–2246. 7967–7971.
[154] K.Qian,Z.Zhang,F.Ringeval,andB.Schuller,“Birdsoundsclassifi- [174] C.G.LyketsosandH.B.Lee,“Diagnosisandtreatmentofdepression
cationbylargescaleacousticfeaturesandextremelearningmachine,” inalzheimer’sdisease:apracticalupdatefortheclinician,”Dementia
in2015IEEEGlobalConferenceonSignalandInformationProcessing andgeriatriccognitivedisorders,vol.17,no.1-2,pp.55–64,2003.
(GlobalSIP). IEEE,2015,pp.1317–1321. [175] R. C. Green, L. A. Cupples, A. Kurz, S. Auerbach, R. Go,
[155] N. Narendra and P. Alku, “Dysarthric speech classification using D. Sadovnick, R. Duara, W. A. Kukull, H. Chui, T. Edeki et al.,
glottal features computed from non-words, words and sentences.” in “Depressionasariskfactorforalzheimerdisease:themiragestudy,”
Interspeech,2018,pp.3403–3407. Archivesofneurology,vol.60,no.5,pp.753–759,2003.
[156] B.Schuller,S.Steidl,A.Batliner,F.Burkhardt,L.Devillers,C.Mu¨ller, [176] R. L. Ownby, E. Crocco, A. Acevedo, V. John, and D. Loewenstein,
andS.Narayanan,“Theinterspeech2010paralinguisticchallenge,”in “Depression and risk for alzheimer disease: systematic review, meta-
Proc.INTERSPEECH2010,Makuhari,Japan,2010,pp.2794–2797. analysis,andmetaregressionanalysis,”Archivesofgeneralpsychiatry,
[157] F. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andre´, vol.63,no.5,pp.530–538,2006.
C.Busso,L.Y.Devillers,J.Epps,P.Laukka,S.S.Narayanan,andK.P. [177] J.Gratch,R.Artstein,G.M.Lucas,G.Stratou,S.Scherer,A.Nazarian,
Truong, “The geneva minimalistic acoustic parameter set (gemaps) R. Wood, J. Boberg, D. DeVault, S. Marsella et al., “The distress
for voice research and affective computing,” IEEE Transactions on analysis interview corpus of human and computer interviews.” in
AffectiveComputing,vol.7,no.2,pp.190–202,2016. LREC. Reykjavik,2014,pp.3123–3128.
[158] B. Schuller, S. Steidl, A. Batliner, J. Hirschberg, J. K. Burgoon, [178] S.Chen,C.Wang,Z.Chen,Y.Wu,S.Liu,Z.Chen,J.Li,N.Kanda,
A. Baird, A. Elkins, Y. Zhang, E. Coutinho, and K. Evanini, “The T.Yoshioka,X.Xiao,J.Wu,L.Zhou,S.Ren,Y.Qian,Y.Qian,J.Wu,
INTERSPEECH 2016 Computational Paralinguistics Challenge: De- M.Zeng,andF.Wei,“Wavlm:Large-scaleself-supervisedpre-training
ception, Sincerity & Native Language,” in Proc. Interspeech 2016, for full stack speech processing,” CoRR, vol. abs/2110.13900, 2021.
2016,pp.2001–2005. [Online].Available:https://arxiv.org/abs/2110.13900
[159] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen, [179] W. Hsu, B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov, and
R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, A.Mohamed,“Hubert:Self-supervisedspeechrepresentationlearning
M. Slaney, R. J. Weiss, and K. W. Wilson, “CNN architectures for by masked prediction of hidden units,” CoRR, vol. abs/2106.07447,
large-scale audio classification,” CoRR, vol. abs/1609.09430, 2016. 2021.[Online].Available:https://arxiv.org/abs/2106.07447
[Online].Available:http://arxiv.org/abs/1609.09430 [180] H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, and F. Bensaali,
[160] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, “Deep transfer learning for automatic speech recognition: Towards
R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and better generalization,” Knowledge-Based Systems, vol. 277, p.
human-labeled dataset for audio events,” in 2017 IEEE international 110851, 2023. [Online]. Available: https://www.sciencedirect.com/
conference on acoustics, speech and signal processing (ICASSP). science/article/pii/S0950705123006019
IEEE,2017,pp.776–780. [181] N.Jamal,S.Shanta,F.Mahmud,andM.Sha’abani,“Automaticspeech
[161] N.Dehak,P.J.Kenny,R.Dehak,P.Dumouchel,andP.Ouellet,“Front- recognition(asr)basedapproachforspeechtherapyofaphasicpatients:
end factor analysis for speaker verification,” IEEE Transactions on Areview,”inAIPConferenceProceedings,vol.1883. AIPPublishing,
Audio,Speech,andLanguageProcessing,vol.19,no.4,pp.788–798, 2017.
2011. [182] I. G. Torre, M. Romero, and A. A´lvarez, “Improving aphasic speech
[162] Y. Hauptman, R. Aloni-Lavi, I. Lapidot, T. Gurevich, Y. Manor, recognition by using novel semi-supervised learning methods on
S. Naor, N. Diamant, and I. Opher, “Identifying distinctive acoustic aphasiabankforenglishandspanish,”AppliedSciences,vol.11,no.19,
andspectralfeaturesinparkinson’sdisease.”inInterspeech,2019,pp. p.8872,2021.
2498–2502. [183] J. Weiner, M. Engelbart, and T. Schultz, “Manual and automatic
[163] I. Laaridh, W. B. Kheder, C. Fredouille, and C. Meunier, “Automatic transcriptionsindementiadetectionfromspeech.”inInterspeech,2017,
prediction of speech evaluation metrics for dysarthric speech,” in pp.3117–3121.
Interspeech,2017. [184] Y.Zhu,A.Obyat,X.Liang,J.A.Batsis,andR.M.Roth,“WavBERT:
[164] D. Snyder, D. Garcia-Romero, D. Povey, and S. Khudanpur, “Deep Exploiting Semantic and Non-Semantic Speech Using Wav2vec and
neuralnetworkembeddingsfortext-independentspeakerverification.” BERTforDementiaDetection,”inProc.Interspeech2021,2021,pp.
inInterspeech,vol.2017,2017,pp.999–1003. 3790–3794.
[165] D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, D. Povey, and [185] H.Zhang,H.Song,S.Li,M.Zhou,andD.Song,“Asurveyofcon-
S. Khudanpur, “Spoken language recognition using x-vectors.” in trollabletextgenerationusingtransformer-basedpre-trainedlanguage
Odyssey,vol.2018,2018,pp.105–111. models,”ACMComputingSurveys,vol.56,no.3,pp.1–37,2023.[186] J. Li, T. Tang, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Pre-trained [207] Y.ZhangandB.Wallace,“Asensitivityanalysisof(andpractitioners’
language models for text generation: A survey,” ACM Computing guide to) convolutional neural networks for sentence classification,”
Surveys,vol.56,no.9,pp.1–39,2024. arXivpreprintarXiv:1510.03820,2015.
[187] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskeveretal., [208] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,
“Languagemodelsareunsupervisedmultitasklearners,”OpenAIblog, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly
vol.1,no.8,p.9,2019. optimized bert pretraining approach,” 2019. [Online]. Available:
[188] C. Li, D. Knopman, W. Xu, T. Cohen, and S. Pakhomov, “GPT- https://arxiv.org/abs/1907.11692
D: Inducing dementia-related linguistic anomalies by deliberate [209] J. Fritsch, S. Wankerl, and E. No¨th, “Automatic diagnosis of
degradation of artificial neural language models,” in Proceedings alzheimer’sdiseaseusingneuralnetworklanguagemodels,”inICASSP
of the 60th Annual Meeting of the Association for Computational 2019-2019 IEEE International Conference on Acoustics, Speech and
Linguistics (Volume 1: Long Papers), S. Muresan, P. Nakov, SignalProcessing(ICASSP). IEEE,2019,pp.5841–5845.
and A. Villavicencio, Eds. Dublin, Ireland: Association for [210] T.B.Brown,B.Mann,N.Ryder,M.Subbiah,J.Kaplan,P.Dhariwal,
Computational Linguistics, May 2022, pp. 1866–1877. [Online]. A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,
Available:https://aclanthology.org/2022.acl-long.131 A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
[189] J. Yuan, Y. Bian, X. Cai, J. Huang, Z. Ye, and K. Church, “Disflu- D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
encies and fine-tuning pre-trained language models for detection of M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,
alzheimer’sdisease.”inInterspeech,vol.2020,2020,pp.2162–6. A.Radford,I.Sutskever,andD.Amodei,“Languagemodelsarefew-
[190] L.IliasandD.Askounis,“Explainableidentificationofdementiafrom shotlearners,”CoRR,vol.abs/2005.14165,2020.[Online].Available:
transcripts using transformer networks,” IEEE Journal of Biomedical https://arxiv.org/abs/2005.14165
andHealthInformatics,vol.26,no.8,pp.4153–4164,2022. [211] J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman,
[191] A. Balagopalan, B. Eyre, F. Rudzicz, and J. Novikova, “To bert D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4
or not to bert: Comparing speech and language-based approaches technicalreport,”arXivpreprintarXiv:2303.08774,2023.
for alzheimer’s disease detection,” in Interspeech, 2020. [Online]. [212] Y.LiuandM.Lapata,“Textsummarizationwithpretrainedencoders,”
Available:https://api.semanticscholar.org/CorpusID:220961484 arXivpreprintarXiv:1908.08345,2019.
[192] A. S. Nambiar, K. Likhita, K. S. Pujya, D. Gupta, S. Vekkot, and
[213] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,
S. Lalitha, “Comparative study of deep classifiers for early dementia
O. Levy, V. Stoyanov, and L. Zettlemoyer, “BART: denoising
detectionusingspeechtranscripts,”in2022IEEE19thIndiaCouncil
sequence-to-sequence pre-training for natural language generation,
InternationalConference(INDICON). IEEE,2022,pp.1–6.
translation, and comprehension,” CoRR, vol. abs/1910.13461, 2019.
[193] N. Liu and L. Wang, “An approach for assisting diagnosis of
[Online].Available:http://arxiv.org/abs/1910.13461
alzheimer’s disease based on natural language processing,” Frontiers
[214] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
inAgingNeuroscience,vol.15,2023.
Y.Zhou,W.Li,andP.J.Liu,“Exploringthelimitsoftransferlearning
[194] A. Roshanzamir, H. Aghajan, and M. Soleymani Baghshah,
with a unified text-to-text transformer,” CoRR, vol. abs/1910.10683,
“Transformer-based deep neural network language models for
2019.[Online].Available:http://arxiv.org/abs/1910.10683
alzheimer’sdiseaseriskassessmentfromtargetedspeech,”BMCMed-
[215] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,
icalInformaticsandDecisionMaking,vol.21,pp.1–14,2021.
A. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained
[195] M. Bouazizi, C. Zheng, S. Yang, and T. Ohtsuki, “Dementia
text-to-text transformer,” CoRR, vol. abs/2010.11934, 2020. [Online].
detection from speech: What if language models are not the
Available:https://arxiv.org/abs/2010.11934
answer?” Information, vol. 15, no. 1, 2024. [Online]. Available:
[216] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and
https://www.mdpi.com/2078-2489/15/1/2
R. Soricut, “ALBERT: A lite BERT for self-supervised learning of
[196] C.Zheng,M.Bouazizi,andT.Ohtsuki,“Anevaluationoninformation
languagerepresentations,”CoRR,vol.abs/1909.11942,2019.[Online].
composition in dementia detection based on speech,” IEEE Access,
Available:http://arxiv.org/abs/1909.11942
vol.10,pp.92294–92306,2022.
[217] Z. Yang, Z. Dai, Y. Yang, J. G. Carbonell, R. Salakhutdinov, and
[197] Y. Pan, B. Mirheidari, M. Reuber, A. Venneri, D. Blackburn, and
Q.V.Le,“Xlnet:Generalizedautoregressivepretrainingforlanguage
H. Christensen, “Automatic hierarchical attention neural network for
understanding,”CoRR,vol.abs/1906.08237,2019.[Online].Available:
detecting ad,” in Proceedings of Interspeech 2019. International
http://arxiv.org/abs/1906.08237
SpeechCommunicationAssociation(ISCA),2019,pp.4105–4109.
[218] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-
[198] D.E.Rumelhart,G.E.Hinton,andR.J.Williams,“Learninginternal
document transformer,” CoRR, vol. abs/2004.05150, 2020. [Online].
representations by error propagation, parallel distributed processing,
Available:https://arxiv.org/abs/2004.05150
explorationsinthemicrostructureofcognition,ed.derumelhartandj.
mcclelland.vol.1.1986,”Biometrika,vol.71,no.599-607,p.6,1986. [219] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,
[199] S.HochreiterandJ.Schmidhuber,“Longshort-termmemory,”Neural “Biobert: a pre-trained biomedical language representation model for
computation,vol.9,no.8,pp.1735–1780,1997. biomedical text mining,” CoRR, vol. abs/1901.08746, 2019. [Online].
[200] G. Van Houdt, C. Mosquera, and G. Na´poles, “A review on the long Available:http://arxiv.org/abs/1901.08746
short-term memory model,” Artificial Intelligence Review, vol. 53, [220] E.Alsentzer,J.R.Murphy,W.Boag,W.Weng,D.Jin,T.Naumann,
no.8,pp.5929–5955,2020. and M. B. A. McDermott, “Publicly available clinical BERT
[201] K.Cho,B.VanMerrie¨nboer,C.Gulcehre,D.Bahdanau,F.Bougares, embeddings,” CoRR, vol. abs/1904.03323, 2019. [Online]. Available:
H. Schwenk, and Y. Bengio, “Learning phrase representations using http://arxiv.org/abs/1904.03323
rnnencoder-decoderforstatisticalmachinetranslation,”arXivpreprint [221] Z. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, and S. Yan, “Convbert:
arXiv:1406.1078,2014. Improving BERT with span-based dynamic convolution,” CoRR, vol.
[202] J.Chung,C.Gulcehre,K.Cho,andY.Bengio,“Empiricalevaluationof abs/2008.02496, 2020. [Online]. Available: https://arxiv.org/abs/2008.
gatedrecurrentneuralnetworksonsequencemodeling,”arXivpreprint 02496
arXiv:1412.3555,2014. [222] M.Yancheva,K.C.Fraser,andF.Rudzicz,“Usinglinguisticfeatures
[203] G.LiuandJ.Guo,“Bidirectionallstmwithattentionmechanismand longitudinally to predict clinical scores for alzheimer’s disease and
convolutionallayerfortextclassification,”Neurocomputing,vol.337, relateddementias,”inSLPAT@Interspeech,2015.[Online].Available:
pp.325–338,2019. https://api.semanticscholar.org/CorpusID:891184
[204] C.Zhou,C.Sun,Z.Liu,andF.Lau,“Ac-lstmneuralnetworkfortext [223] Z.Zhu,J.Novikova,andF.Rudzicz,“Detectingcognitiveimpairments
classification,”arXivpreprintarXiv:1511.08630,2015. by agreeing on interpretations of linguistic features,” ArXiv, vol.
[205] M. E. Basiri, S. Nemati, M. Abdar, E. Cambria, and U. R. Acharya, abs/1808.06570,2018.[Online].Available:https://api.semanticscholar.
“Abcdm: An attention-based bidirectional cnn-rnn deep model for org/CorpusID:51938927
sentiment analysis,” Future Generation Computer Systems, vol. 115, [224] N. B. Mota, N. Vasconcelos, N. Lemos, A. C. de Souza Pieretti,
pp.279–294,2021. O. Kinouchi, G. A. Cecchi, M. Copelli, and S. Ribeiro, “Speech
[206] Y. Kim, “Convolutional neural networks for sentence classification,” graphs provide a quantitative measure of thought disorder in
CoRR, vol. abs/1408.5882, 2014. [Online]. Available: http://arxiv.org/ psychosis,” PLoS ONE, vol. 7, 2012. [Online]. Available: https:
abs/1408.5882 //api.semanticscholar.org/CorpusID:9506186[225] A. B. Warriner, V. Kuperman, and M. Brysbaert, “Norms of valence, [244] Q. V. Le and T. Mikolov, “Distributed representations of sentences
arousal, and dominance for 13,915 english lemmas,” Behavior anddocuments,”CoRR,vol.abs/1405.4053,2014.[Online].Available:
Research Methods, vol. 45, pp. 1191 – 1207, 2013. [Online]. http://arxiv.org/abs/1405.4053
Available:https://api.semanticscholar.org/CorpusID:16918336 [245] N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings
[226] H. Ai and X. Lu, “A web-based system for automatic measurement usingsiamesebert-networks,”arXivpreprintarXiv:1908.10084,2019.
of lexical complexity,” in 27th Annual Symposium of the Computer- [246] A.Joulin,E.Grave,P.Bojanowski,andT.Mikolov,“Bagoftricksfor
Assisted Language Consortium (CALICO-10). Amherst, MA. June, efficienttextclassification,”arXivpreprintarXiv:1607.01759,2016.
2010,pp.8–12. [247] A.Akbik,D.Blythe,andR.Vollgraf,“Contextualstringembeddings
[227] B. Croisile, B. Ska, M.-J. Brabant, A. Ducheˆne, Y. Lepage, for sequence labeling,” in Proceedings of the 27th international con-
G. Aimard, and M. Trillet, “Comparative study of oral and ferenceoncomputationallinguistics,2018,pp.1638–1649.
written picture description in patients with alzheimer’s disease,” [248] N. D. Cilia, T. D’Alessandro, C. De Stefano, F. Fontanella, and
Brain and Language, vol. 53, pp. 1–19, 1996. [Online]. Available: M. Molinara, “From online handwriting to synthetic images for
https://api.semanticscholar.org/CorpusID:36544389 alzheimer’sdiseasedetectionusingadeeptransferlearningapproach,”
[228] J. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors IEEE Journal of Biomedical and Health Informatics, vol. 25, no. 12,
for word representation,” in Proceedings of the 2014 Conference pp.4243–4254,2021.
on Empirical Methods in Natural Language Processing (EMNLP), [249] J. Sun, H. H. Dodge, and M. H. Mahoor, “Mc-vivit: Multi-branch
A. Moschitti, B. Pang, and W. Daelemans, Eds. Doha, Qatar: classifier-vivit to detect mild cognitive impairment in older adults
AssociationforComputationalLinguistics,Oct.2014,pp.1532–1543. using facial videos,” Expert Systems with Applications, vol. 238,
[Online].Available:https://aclanthology.org/D14-1162 p. 121929, 2024. [Online]. Available: https://www.sciencedirect.com/
[229] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and science/article/pii/S0957417423024314
R.Salakhutdinov,“Transformer-xl:Attentivelanguagemodelsbeyond [250] X. Liang, A. Angelopoulou, E. Kapetanios, B. Woll, R. Al Batat,
a fixed-length context,” CoRR, vol. abs/1901.02860, 2019. [Online]. andT.Woolfe,“Amulti-modalmachinelearningapproachandtoolkit
Available:http://arxiv.org/abs/1901.02860 to automate recognition of early stages of dementia among british
[230] A. Radford and K. Narasimhan, “Improving language understanding sign language users,” in Computer Vision – ECCV 2020 Workshops,
by generative pre-training,” 2018. [Online]. Available: https://api. A. Bartoli and A. Fusiello, Eds. Cham: Springer International
semanticscholar.org/CorpusID:49313245 Publishing,2020,pp.278–293.
[231] K. C. Fraser, J. A. Meltzer, and F. Rudzicz, “Linguistic features [251] K.Hu,Z.Wang,W.Wang,K.A.E.Martens,L.Wang,T.Tan,S.J.
identifyalzheimer’sdiseaseinnarrativespeech,”JournalofAlzheimer’s Lewis,andD.D.Feng,“Graphsequencerecurrentneuralnetworkfor
Disease,vol.49,no.2,pp.407–422,2016. vision-basedfreezingofgaitdetection,”IEEETransactionsonImage
Processing,vol.29,pp.1890–1901,2019.
[232] F. Di Palo and N. Parde, “Enriching neural models with targeted
features for dementia detection,” in Proceedings of the 57th Annual [252] A. Das, X. Niu, A. Dantcheva, S. L. Happy, H. Han, R. Zeghari,
Meeting of the Association for Computational Linguistics: Student P. Robert, S. Shan, F. Bremond, and X. Chen, “A spatio-temporal
ResearchWorkshop,F.Alva-Manchego,E.Choi,andD.Khashabi,Eds.
approachforapathyclassification,”IEEETransactionsonCircuitsand
SystemsforVideoTechnology,vol.32,no.5,pp.2561–2573,2022.
Florence, Italy: Association for Computational Linguistics, Jul. 2019,
pp.302–308.[Online].Available:https://aclanthology.org/P19-2042 [253] G. Vessio, “Dynamic handwriting analysis for neurodegenerative dis-
easeassessment:aliteraryreview,”AppliedSciences,vol.9,no.21,p.
[233] Y. Sun, S. Wang, Y. Li, S. Feng, H. Tian, H. Wu, and
4666,2019.
H. Wang, “ERNIE 2.0: A continual pre-training framework for
languageunderstanding,”CoRR,vol.abs/1907.12412,2019.[Online]. [254] E.Onofri,M.Mercuri,M.Salesi,S.Ferrara,G.M.Troili,C.Simeone,
M. R. Ricciardi, S. Ricci, and T. Archer, “Dysgraphia in relation to
Available:http://arxiv.org/abs/1907.12412
cognitive performance in patients with alzheimer’s disease,” Journal
[234] S. Bird, E. Klein, and E. Loper, Natural language processing with
of Intellectual Disability-Diagnosis and Treatment, vol. 1, no. 2, pp.
Python:analyzingtextwiththenaturallanguagetoolkit. ”O’Reilly
113–124,2013.
Media,Inc.”,2009.
[255] S.Mu¨ller,O.Preische,P.Heymann,U.Elbing,andC.Laske,“Diagnos-
[235] P.Garrard,L.M.Maloney,J.R.Hodges,andK.Patterson,“Theeffects
ticvalueofatablet-baseddrawingtaskfordiscriminationofpatients
ofveryearlyalzheimer’sdiseaseonthecharacteristicsofwritingbya
in the early course of alzheimer’s disease from healthy individuals,”
renownedauthor,”Brain,vol.128,no.2,pp.250–260,2005.
JournalofAlzheimer’sDisease,vol.55,no.4,pp.1463–1469,2017.
[236] V. Berisha, S. Wang, A. LaCross, and J. Liss, “Tracking discourse
[256] C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna,“Rethink-
complexity preceding alzheimer’s disease diagnosis: A case study
ingtheinceptionarchitectureforcomputervision,”inProceedingsof
comparingthepressconferencesofpresidentsronaldreaganandgeorge
theIEEEconferenceoncomputervisionandpatternrecognition,2016,
herbert walker bush,” Journal of Alzheimer’s Disease, vol. 45, no. 3,
pp.2818–2826.
pp.959–963,2015.
[257] U. Nam, K. Lee, H. Ko, J.-Y. Lee, and E. C. Lee, “Analyzing facial
[237] R. S. Bucks, S. Singh, J. M. Cuerden, and G. K. Wilcock, “Analysis
andeyemovementstoscreenforalzheimer’sdisease,”Sensors,vol.20,
ofspontaneous,conversationalspeechindementiaofalzheimertype:
no.18,p.5349,2020.
Evaluation of an objective technique for analysing lexical perfor-
[258] M. C. N. Dourado, B. Torres Mendonc¸a de Melo Fa´del, J. P.
mance,”Aphasiology,vol.14,no.1,pp.71–91,2000.
Simo˜es Neto, G. Alves, and C. Alves, “Facial expression recogni-
[238] T.Mikolov,K.Chen,G.Corrado,andJ.Dean,“Efficientestimationof tion patterns in mild and moderate alzheimer’s disease,” Journal of
wordrepresentationsinvectorspace,”arXivpreprintarXiv:1301.3781, Alzheimer’sDisease,vol.69,no.2,pp.539–549,2019.
2013.
[259] B. Jin, Y. Qu, L. Zhang, and Z. Gao, “Diagnosing parkinson disease
[239] J.Can˜ete,G.Chaperon,R.Fuentes,J.-H.Ho,H.Kang,andJ.Pe´rez, through facial expression recognition: video analysis,” Journal of
“Spanishpre-trainedbertmodelandevaluationdata,”2023.[Online]. medicalInternetresearch,vol.22,no.7,p.e18697,2020.
Available:https://arxiv.org/abs/2308.02976 [260] H.Tanaka,H.Adachi,H.Kazui,M.Ikeda,T.Kudo,andS.Nakamura,
[240] A.J.Thirunavukarasu,D.S.J.Ting,K.Elangovan,L.Gutierrez,T.F. “Detectingdementiafromfaceinhuman-agentinteraction,”inAdjunct
Tan,andD.S.W.Ting,“Largelanguagemodelsinmedicine,”Nature ofthe2019internationalconferenceonmultimodalinteraction,2019,
medicine,vol.29,no.8,pp.1930–1940,2023. pp.1–6.
[241] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, [261] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu,
B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language “Videoswintransformer,”CoRR,vol.abs/2106.13230,2021.[Online].
models,”arXivpreprintarXiv:2303.18223,2023. Available:https://arxiv.org/abs/2106.13230
[242] Y.Chang,X.Wang,J.Wang,Y.Wu,L.Yang,K.Zhu,H.Chen,X.Yi, [262] M.A.Hely,W.G.Reid,M.A.Adena,G.M.Halliday,andJ.G.Morris,
C. Wang, Y. Wang et al., “A survey on evaluation of large language “Thesydneymulticenterstudyofparkinson’sdisease:theinevitability
models,” ACM Transactions on Intelligent Systems and Technology, ofdementiaat20years,”Movementdisorders,vol.23,no.6,pp.837–
vol.15,no.3,pp.1–45,2024. 844,2008.
[243] OpenAI, “Chatgpt: Openai’s gpt-4 language model,” 2024. [Online]. [263] M.Macht,Y.Kaussner,J.C.Mo¨ller,K.Stiasny-Kolster,K.M.Eggert,
Available:https://www.openai.com/chatgpt H.-P. Kru¨ger, and H. Ellgring, “Predictors of freezing in parkinson’sdisease:asurveyof6,620patients,”Movementdisorders,vol.22,no.7, tasks,” CoRR, vol. abs/1908.02265, 2019. [Online]. Available:
pp.953–956,2007. http://arxiv.org/abs/1908.02265
[264] B.R.Bloem,J.M.Hausdorff,J.E.Visser,andN.Giladi,“Fallsand [285] R. Zheng, J. Chen, M. Ma, and L. Huang, “Fused acoustic and text
freezingofgaitinparkinson’sdisease:areviewoftwointerconnected, encodingformultimodalbilingualpretrainingandspeechtranslation,”
episodic phenomena,” Movement disorders: official journal of the inInternationalConferenceonMachineLearning. PMLR,2021,pp.
MovementDisorderSociety,vol.19,no.8,pp.871–884,2004. 12736–12746.
[265] S.J.LewisandR.A.Barker,“Apathophysiologicalmodeloffreezing [286] B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, “Learning audio-
of gait in parkinson’s disease,” Parkinsonism & related disorders, visualspeechrepresentationbymaskedmultimodalclusterprediction,”
vol.15,no.5,pp.333–338,2009. arXivpreprintarXiv:2201.02184,2022.
[266] T.Khan,J.Westin,andM.Dougherty,“Motioncueanalysisforparkin- [287] J. Lin, A. Yang, Y. Zhang, J. Liu, J. Zhou, and H. Yang, “Interbert:
sonian gait recognition,” The open biomedical engineering journal, Vision-and-language interaction for multi-modal pretraining,” 2021.
vol.7,p.1,2013. [Online].Available:https://arxiv.org/abs/2003.13198
[267] M. Nieto-Hidalgo, F. J. Ferra´ndez-Pastor, R. J. Valdivieso-Sarabia, [288] Y.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency,
J.Mora-Pascual,andJ.M.Garc´ıa-Chamizo,“Avisionbasedproposal and R. Salakhutdinov, “Multimodal transformer for unaligned
for classification of normal and abnormal gait using rgb camera,” multimodal language sequences,” 2019. [Online]. Available: https:
Journalofbiomedicalinformatics,vol.63,pp.82–89,2016. //arxiv.org/abs/1906.00295
[268] R.J.Molitor,P.C.Ko,andB.A.Ally,“Eyemovementsinalzheimer’s [289] X. Xu, T. Wang, Y. Yang, L. Zuo, F. Shen, and H. T. Shen, “Cross-
disease,”JournalofAlzheimer’sdisease,vol.44,no.1,pp.1–12,2015. modal attention with semantic consistence for image–text matching,”
IEEE transactions on neural networks and learning systems, vol. 31,
[269] A. Oyama, S. Takeda, Y. Ito, T. Nakajima, Y. Takami, Y. Takeya,
no.12,pp.5412–5425,2020.
K.Yamamoto,K.Sugimoto,H.Shimizu,M.Shimamuraetal.,“Novel
[290] A.Khare,S.Parthasarathy,andS.Sundaram,“Self-supervisedlearning
method for rapid assessment of cognitive impairment using high-
withcross-modaltransformersforemotionrecognition,”in2021IEEE
performanceeye-trackingtechnology,”Scientificreports,vol.9,no.1,
SpokenLanguageTechnologyWorkshop(SLT),2021,pp.381–388.
p.12932,2019.
[291] V.Murahari,D.Batra,D.Parikh,andA.Das,“Large-scalepretraining
[270] D.Lagun,C.Manzanares,S.M.Zola,E.A.Buffalo,andE.Agichtein,
for visual dialog: A simple state-of-the-art baseline,” 2020. [Online].
“Detecting cognitive impairment by eye movement analysis using
Available:https://arxiv.org/abs/1912.02379
automaticclassificationalgorithms,”Journalofneurosciencemethods,
[292] J.Arevalo,T.Solorio,M.Montes-yGomez,andF.A.Gonza´lez,“Gated
vol.201,no.1,pp.196–203,2011.
multimodal networks,” Neural Computing and Applications, vol. 32,
[271] K. Mengoudi, D. Ravi, K. X. Yong, S. Primativo, I. M. Pavisic,
pp.10209–10228,2020.
E.Brotherhood,K.Lu,J.M.Schott,S.J.Crutch,andD.C.Alexander,
[293] Y.Zhang,D.Sidibe´,O.Morel,andF.Me´riaudeau,“Deepmultimodal
“Augmentingdementiacognitiveassessmentwithinstruction-lesseye-
fusion for semantic image segmentation: A survey,” Image and
tracking tests,” IEEE journal of biomedical and health informatics,
Vision Computing, vol. 105, p. 104042, 2021. [Online]. Available:
vol.24,no.11,pp.3066–3075,2020.
https://www.sciencedirect.com/science/article/pii/S0262885620301748
[272] A.A.Lazarusetal.,“Multimodalbehaviortherapy:I.”1976.
[294] S. R. Stahlschmidt, B. Ulfenborg, and J. Synnergren, “Multimodal
[273] P.Xu,X.Zhu,andD.A.Clifton,“Multimodallearningwithtransform-
deep learning for biomedical data fusion: a review,” Briefings in
ers: A survey,” IEEE Transactions on Pattern Analysis and Machine
Bioinformatics, vol. 23, no. 2, p. bbab569, 01 2022. [Online].
Intelligence,vol.45,no.10,pp.12113–12132,2023.
Available:https://doi.org/10.1093/bib/bbab569
[274] W. Guo, J. Wang, and S. Wang, “Deep multimodal representation [295] M. Wang, J. Xing, and Y. Liu, “Actionclip: A new paradigm
learning:Asurvey,”IeeeAccess,vol.7,pp.63373–63394,2019. for video action recognition,” 2021. [Online]. Available: https:
[275] T. Baltrusˇaitis, C. Ahuja, and L.-P. Morency, “Multimodal machine //arxiv.org/abs/2109.08472
learning: A survey and taxonomy,” IEEE transactions on pattern [296] W. Wu, X. Wang, H. Luo, J. Wang, Y. Yang, and W. Ouyang,
analysisandmachineintelligence,vol.41,no.2,pp.423–443,2018. “Bidirectional cross-modal knowledge exploration for video
[276] D. Altinok, “Explainable multimodal fusion for dementia detection recognitionwithpre-trainedvision-languagemodels,”2023.[Online].
from text and speech,” in International Conference on Text, Speech, Available:https://arxiv.org/abs/2301.00182
andDialogue. Springer,2024,pp.236–251. [297] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,
[277] L. Ilias and D. Askounis, “Multimodal deep learning models S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger,
for detecting dementia from speech and transcripts,” Frontiers and I. Sutskever, “Learning transferable visual models from
in Aging Neuroscience, vol. 14, 2022. [Online]. Available: https: natural language supervision,” 2021. [Online]. Available: https:
//www.frontiersin.org/articles/10.3389/fnagi.2022.830943 //arxiv.org/abs/2103.00020
[278] L. Ilias, D. Askounis, and J. Psarras, “A multimodal approach for [298] J. Go´rriz et al., “Computational approaches to explainable artificial
dementiadetectionfromspontaneousspeechwithtensorfusionlayer,” intelligence:Advancesintheory,applicationsandtrends,”Information
in 2022 IEEE-EMBS International Conference on Biomedical and Fusion, vol. 100, p. 101945, 2023. [Online]. Available: https:
HealthInformatics(BHI),2022,pp.1–5. //www.sciencedirect.com/science/article/pii/S1566253523002610
[279] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal, [299] A.Holzinger,G.Langs,H.Denk,K.Zatloukal,andH.Mu¨ller,“Caus-
G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and ability and explainability of artificial intelligence in medicine,” Wiley
I. Sutskever, “Learning transferable visual models from natural Interdisciplinary Reviews: Data Mining and Knowledge Discovery,
language supervision,” CoRR, vol. abs/2103.00020, 2021. [Online]. vol.9,no.4,p.e1312,2019.
Available:https://arxiv.org/abs/2103.00020 [300] G. Vilone and L. Longo, “Notions of explainability and evaluation
[280] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, approachesforexplainableartificialintelligence,”InformationFusion,
T.Lacroix,B.Rozie`re,N.Goyal,E.Hambro,F.Azhar,A.Rodriguez, vol.76,pp.89–106,2021.
A. Joulin, E. Grave, and G. Lample, “Llama: Open and efficient [301] F. Tao, H. Zhang, A. Liu, and A. Y. Nee, “Digital twin in industry:
foundation language models,” 2023. [Online]. Available: https: State-of-the-art,”IEEETransactionsonindustrialinformatics,vol.15,
//arxiv.org/abs/2302.13971 no.4,pp.2405–2415,2018.
[281] J.Li,D.Li,S.Savarese,andS.Hoi,“Blip-2:Bootstrappinglanguage- [302] K. Bruynseels, F. Santoni de Sio, and J. Van den Hoven, “Digital
image pre-training with frozen image encoders and large language twins in health care: ethical implications of an emerging engineering
models,”2023.[Online].Available:https://arxiv.org/abs/2301.12597 paradigm,”Frontiersingenetics,vol.9,p.31,2018.
[282] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid,
“Videobert: A joint model for video and language representation
learning,”2019.[Online].Available:https://arxiv.org/abs/1904.01766
[283] L.H.Li,M.Yatskar,D.Yin,C.-J.Hsieh,andK.-W.Chang,“Visual-
bert:Asimpleandperformantbaselineforvisionandlanguage,”arXiv
preprintarXiv:1908.03557,2019.
[284] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining
task-agnostic visiolinguistic representations for vision-and-language