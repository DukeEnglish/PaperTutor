CAMEL-Bench: A Comprehensive Arabic LMM Benchmark
SaraGhaboura1* AhmedHeakl1* OmkarThawakar1 AliAlharthi1
InesRiahi2 AbduljalilSaif2 JormaLaaksonen2 FahadShahbazKhan1,3
SalmanKhan1,4 RaoMuhammadAnwer1,2
1MohamedbinZayedUniversityofAI,2AaltoUniversity,3LinköpingUniversity,4AustralianNationalUniversity
https://mbzuai-oryx.github.io/Camel-Bench/
Abstract
R Li v h e c en vva ea Me g aa nrc si lMl te l o l uu ran e iu aart d ceg ts t .y ie ve t oe ov a IL na m i nltsr uMh bu us a tel eha Mh t t ni i il ia m o sn csv r hnto we e or md oa n b ow a d as e rdi l o u rn kt i kmn n c ,cfte fi h fo wi ens oo md rs g e rn eee an tld da o hs rt en kfa e( t vd sL m a As eM si au lu rg k or alnMn st e p bi .di pfi s ipe aH c) lc r rea c o ls e c aLan t d owa npt M o men gai mvn d pM ub et i i r ale n nr ee g,br g a he eo e m n es f nt tt ntoa ocp li ss ys hen i rtk vr m eEd esf e po a. xe n rrv i Lr g esm ke T Ml stsl iii eho snn Mt nhip gog s t-- F PIr d lu aei nt ns ttP si& fl i a c
T
Vn a yt e ts piog eil ngl snG ieee so ss spa UtiL naa dl n A edG I rgs m seT ro tr s ia a acpn a g ns utp ia e dlo l tr ut iUa nt n rio gd an e lr stS aeG ne no dsp s ia R nt iia n gl e gC mo ns otru tc etion Visual understanding/
reasoning
Object Mhalluc uin lati ton
ievaluati mon
UoMat ndh & dl ao lgi ec rrea ss S ton ci in ag e &n n tifi dic R r n eea g as V so in si on u ng a il nQ gu Ies n Ut fio or Dn O GA nrns a C ow h C doe pr ii ccn mg es p Rl uV re R Q x e smA alV & ti -s w u aM eoa rl ul l nd nt iP O- te S i dr b p jc imae t ei ap ncat li g t o -e gu l n n N eud vne er d es lt e P a r wn s Pt od si e Sa lrn ng c wed c Hi e tn ep t etg Lri a DBe o n irn n nP o oe do e ot ci w sen k ut rx si mt tS ili n ed g ne ts
s
Ca p d u
u
p
sti mno
l
Aav al dsa ne
e
Mne er r
n
dtd rg s
Et
ise de nub Lt ia gd ne sp -no Bdn eo tdm
ae
oc ep i rh
s
nnu a
es
em cgil
t
vsna ha, aa ,s nt c clri
ad
uo a ok
oni
an n, m
mn
d
tdn gp eo pa r,3 lf
re
bem
v
i8o mx srie
d
o
ev s v od
e
sue ai tor b s
de
aC u-
u
r4 dA ssa on0 ceo l uM d0 enm p
ne
nsE e dm a
r
i
ar
s
nL i
r
2ci tn g-l
a
ie 9sl B o-ni p ,bo i 0e dt gn an in
3i
ec o
n
sc 6l nns eguhp e,
d
q,d, re h umic ala n a eao
e
lk g sn inm de tz, d
d
ii
ar omp cws
ba
nr u. u r ii
l
ssi ls lT t
e
iite tt tmi hh e y-s ui n a.e ame
n
tg
Odip da ig ao nr eg uh ro c
rg
ee rt- --
,
Medical Un Dd i Pe a P
C
Br hg ls iu aan st nb ir ila o cim
c
n cs
a
Mld ai es Hi c dn M iey cg ea al ldt i Sch i cin Cee en lc ee bratM U iee nI sd di m ec raa sl g t e a Fn ood din Cug ltural Spe Cc oui ntf riUi
es
&c n derstan Ld ai ndmn arksg LandU
m
an rksderst Ca oun ntrid esV i &nid geo U On cd caeD sCr iousiC na ltt suagh ranra la -d Sr mt pi n e&
SG
cg
i
fce icen
neT era
a
slb VD le idsi eC a ogh ra ar mts s
filtered from a larger pool of samples, where the quality
is manually verified by native speakers to ensure reliable
modelassessment. Weconductevaluationsofbothclosed-
Figure1.TheproposedCAMEL-Benchcoverseightdiverseand
source, including GPT-4 series, and open-source LMMs.
challengingdomains: multimodalunderstandingandreasoning,
Ouranalysisrevealstheneedforsubstantialimprovement,
OCRanddocuments,chartsanddiagrams,videos,cultural-specific
especiallyamongthebestopen-sourcemodels,witheventhe content,medicalimages,agriculturalimages,andremotesensing
closed-source GPT-4o achieving an overall score of 62%. understandinginArabic.CAMEL-Benchcovers38sub-domains
Ourbenchmarkandevaluationscriptsareopen-sourced.1 withover29KquestionscarefullycuratedbynativeArabicspeakers
torigorouslyevaluateessentialskillsdesiredinArabicLMMs.
1.Introduction
tionanswering(VQA)[24,25],andcomplexvisualreason-
Largemultimodalmodels(LMMs)haverecentlyachieved
ing[12]. Theserecentdevelopmentshaveledtotheintro-
significantadvancementsacrossabroadspectrumoftasks,
ductionofdifferentbenchmarkstoevaluatetheperformance
includingvisualreasoning,perception,andmultimodalun-
ofopenandclosed-sourceLMMs. Despitetheseadvances,
derstanding. Closed-source models such as GPT-4V and
mostexistingLMMbenchmarksareEnglish-centric,limit-
open-sourceLMMs,suchasLLaVA[28]havedemonstrated
ingtheirapplicabilitytootherlanguages[44].
effectivenessintaskslikeimagecaptioning[43],visualques-
Withover400millionspeakers, Arabicisthe5th most
1*EqualContributions widelyspokenlanguagesglobally. Inthecontextoflarge
4202
tcO
42
]VC.sc[
1v67981.0142:viXraFigure2.CAMEL-Benchexamplesspanningeightdiversedomains,encompassingawiderangeofvisualdatatypesandtasks.
language models (LLMs), there exist various attempts in Domain/Characteristics Exams-V* CVQA* Henna KHATT CAMEL-Bench
(ours)
developingArabicLLMs[19,48]whichhasalsoledtothe MultimodalUnd.&Reasoning ✓ ✗ ✓ ✗ ✓
introductionofArabicLLMbenchmarks[21].Inthecontext OCR&DocsUnd. ✗ ✗ ✗ ✓ ✓
Charts&DiagramsUnd. ✓ ✗ ✗ ✗ ✓
ofLMMs,fewrecentworksexploreArabic-centricevalua- VideoUnd. ✗ ✗ ✗ ✗ ✓
MedicalImageUnd. ✗ ✗ ✗ ✗ ✓
tionsincertainareassuchas,scientificexams[13],cultural AgriculturalImageUnd. ✗ ✗ ✗ ✗ ✓
aspects [4, 46], Arabic question answers and documents Remote-SensingUnd. ✗ ✗ ✗ ✗ ✓
Cultural-SpecificUnd. ✗ ✓ ✓ ✗ ✓
[1,34]. However,thereisstillalackofcomprehensiveand OpenSource ✓ ✓ ✗ ✓ ✓
diverseArabicLMMevaluationbenchmark(seeTab.1)to QuestionNumbers 823 200 1.1K 5K 29K
rigorouslyevaluateandstudyLMMsforArabic.
Table1.ComparisonofourCAMEL-BenchwithexistingArabic
To address the aforementioned issue, we introduce the
LMMbenchmarks: Exams-V[13], CVQA[46], Henna[4], and
first comprehensive Arabic LMM evaluation benchmark, KHATT[34].Here*denotesthatonlyArabicpartofbenchmark
namedCAMEL-Bench. CAMEL-Benchisdesignedtoen- iscounted.
compass a wide range of tasks and focus on the Arabic-
speakingpopulation. Itspanseightdiversedomainsand38 tification,plantillness,andgeospatialimagerysubdomains
sub-domains (see Fig. 1). The eight domains are: Multi- (land,transportationandconstruction).
modal understanding and reasoning, OCR and document Our CAMEL-Bench comprises 29,036 questions (see
understanding,chartanddiagramunderstanding,videoun- Fig. 2)andfollowsanextensivemanualverificationprocess
derstanding,cultural-specificunderstanding,medicalimage bynative-speakerstoensuretheresultingbenchmarkisof
understanding, Agriculturalimageunderstanding,andre- high-quality. Weconductextensiveexperimentsusingopen
motesensingunderstanding. Further,the38sub-domains andclosed-sourceLMMs. Ourresultsrevealtheneedfor
(see Fig. 1) covered by our CAMEL-Bench are: visual substantialimprovementinhandlingofArabicmultimodal
understanding and reasoning, object hallucination evalua- data, shedding light on the areas requiring further Arabic
tion,mathandlogicreasoning,scientificreasoning,VQA, LMMimprovements.
infographicsVQA,complexvisualperception,real-world
spatial understanding, multi-image understanding, object- 2.CAMEL-Bench
levelperception,newsletter,powerpointslides,scenetext,
2.1.DataCollection
handwriting,lines,books,documents,charts,diagrams,ta-
bles,generalvideoscenes,cultural-specificoccasions,coun- Ourdatasetencompasseseightdiversedomainstoensurea
triesandlandmarksinvideos,countriesandlandmarksin versatilemulti-taskArabicLMMbenchmarkfordifferent
images,food,celebrities,culturalVQA,basicmedicalsci- real-world scenarios. Each domain is further sub-divided
ence,clinicalmedicine,publichealth,pharmacy,diagnosis, intodifferentsub-domains,eachfocusingonadistinctas-
medicalunderstanding,planttypes,fruitandveggiesiden- pect. Duringthedatacollectionprocess, weeitherutilizeDomains Sub-Domains Source NumberofQuestions
VisualUnderstanding/Reasoning MME,MMBench,MMT-Bench-MI,SEED,MMMU 3,971
ObjectHallucinationEvaluation CountBench,MMT-Bench-MI,POPE 997
MathandLogicReasoning MathVista 531
ScientificReasoning ScienceQA-IMG,Exams-V 1,624
VisualQuestionAnswering GQA,VizWiz,VQAv2 3,840
MultimodalUnderstandingandReasoning
InforGrahpicsVQA AI-Generated(GPT-4o),Pinterest 120
ComplexVisualPerception BLINK 1,422
Real-worldSpatialUnderstanding RealWorldQA 624
Multi-imageUnderstanding MMT-Bench-MI,MuirBench 1,062
Object-levelPerception COCO,ImageNet,Mocheg,Snli-Ve 60
ScannedDocuments(OCR) ArabicDatasetOCR 480
ScannedDocuments(VQA) MTVQA 703
SceneText(OCR) EvArEST 1,217
Books(OCR) HistoricalArabicHandwrittenTextRecognitionDataset 40
OCRandDocumentUnderstanding PowerPointSlides(OCR) ISI-PPT-Dataset 2,354
PowerPointSlides(VQA) ISI-PPT-Dataset 711
Handwriting(OCR) KHATTLine 1,400
Newsletters(OCR) PATD 506
Lines(OCR) PATS-01 520
Charts ChartQA 745
ChartandDiagramUnderstanding DiagramsUnderstanding MMMU(diagrams),ICON-QA,AI-Generated,Pinterest,BCE-Arabic 1,994
Tables BCE-Arabic,Excel 81
Countries/Landmarks Pexel 87
VideoUnderstanding Cultural-SpecificOccasions Pexel 24
GeneralVideoScenes Video-MME 654
Celebrities arab-celeb-dataset 444
CulturalSpecificUnderstanding Food arabic-food-101,Pexel 347
Countries/Landmarks Pexel 494
BasicMedicalScience MMMU,MMMUPro 89
ClinicalMedicine MMMU,MMMUPro 83
PublicHealth MMMU,MMMUPro 87
Pharmacy MMMU,MMMUPro 82
MedicalImagingUnderstanding
Diagnosis MMMU,MMMUPro 87
MedicalUnderstanding MMT-MI-Bench 78
AgriculturalImageUnderstanding AgricultureImageUnderstanding AgroGPT 769
RemoteSensingUnderstanding RemoteSensingUnderstanding GeoChat 709
Total 29,036
Table2.Differentdatasourcesusedfor38sub-domainscorrespondingtoeightdomains,witharound29kquestionsintotal.Thedifferent
datasourcesinclude:MME[15],MMBench[30],MMT-Bench-MI[56],SEED[23],MMMU[58],MMMU-Pro[60],CountBench[39],
POPE[26],MathVista[33],Exams-V(Arabicportion)[13],ScienceQA-IMG[32],GQA[20],VizWiz[10],VQAv2[17],BLINK[16],
MuirBench[50],COCO[27],Imagenet[14],Mocheg[55],Snli-Ve[54],Pinterest[42],RealWorldQA[53],PATS-01[3],KHATT[34],
PATD[40],HistoricalArabicHandwrittenTextRecognitionDataset[37],ISI-PPT-Dataset[52],EvArEST[18],MTVQA[49],ChartQA
[35],IconQA[31],BEC-Arabic[47],Claude-3.5[5],arab-celeb-dataset[36],arabic-food-101[6],Countriesandlandmarks[41,51,57],
Pexel[41],AgroGPT[7],GeoChat[22].Thesedatasourcesarecarefullytranslatedandverifiedtoensurequalityandrelevance.
availableArabicmultimodaldatasamplesoremploysam- ual curation (e.g., for countries and landmarks in videos),
plesfromexistingEnglish-centricLMMbenchmarks. These andAI-generatedcontentbasedonamanuallyprovidedcon-
EnglishsamplesarethentranslatedtoArabicviaGPT-4o text(e.g.,fordiagramsandinfographics). Next,wegenerate
andverified.Alternatively,wemanuallycollectandgenerate multiple-choicequestions(MCQs)foreachsampleusingthe
theArabicsamplesforremainingsub-domainsfrominter- GPT-4omodel.Thepromptismeticulouslycraftedtoadhere
net. Tab.2presentsthedetailsofdifferentdatasourcesused tokeycriteria: eachsamplegeneratesthreemultiple-choice
fordatacollectionforthe38sub-domainscorrespondingto questions(MCQs),withfourdistinct,non-synonymousop-
eightdomains,witharound29kquestionsintotal. tionsperquestion,onlyoneofwhichiscorrect. Theques-
tionscontainnoembeddedhints,ensuringthatanswersare
2.2.Question-AnswersPairsGeneration
derivedexclusivelyfromtheimage,withoutrequiringprior
WenotethatamajorpartofouroriginalArabicdataisnot knowledge. Additionally,theimagemustprovideenoughin-
derivedfromready-madeVQAdatasets. Somesub-domains, formationtofullysupportthecorrectanswer,eliminatingthe
such as celebrities and food, consist of image-only data, needforguesswork. Intotal,thisprocessproducesacorpus
whileothers,likePexel’scountriesandlandmarks,contain of4.4Kgeneratedquestionswith17.7Kanswers,enablinga
image-captionpairs. TocreatearichanddiverseVQAcor- comprehensivesetofquestionsforevaluation.
pus,wefirstensurethateachimageisaccompaniedbyde-
tailedcontextualinformation. Thiscontextissourcedfroma
combinationofWikipedia(e.g.,forfood-relateddata),man-Original Arabic Data Filtering and Verification Process
Major/ Minor
Manual Verification All Data
Changes Yes
Yes (20% random samples) >= 40%? Manual Verification
No
Sub-Category Original
All Collected
Level Arabic
Non-Refined Verification Data?
VQA pairs Translated Arabic Data Filtering and Verification Process CAMEL-
Bench
Manual Handling (ours)
No Model fails → Manually translate
EN
Refine & Verify
AR
Model translates with issues
Qwen7b Data to Non-Translated Review GPT-4o
inspect
Fuzzy evaluate translation Good translation but non-Arab words
EngO lir sig hi n Da al ta Gu Ps Tin -4g o AT rr aa bn is cl a Dt ae td a Y Ne os -- -- >> A Rp ep jer co tv ee dd (E asn t "r Rie es j em ca terk de ")d
Figure3.TheCAMEL-BenchFilteringandVerificationPipelineconsistsoftwopaths:OriginalArabicandtranslatedArabic.Fororiginal
Arabic(toprow),a20%randomsampleundergoesmanualverification;iferrorsarebelow40%,thedatapasses;otherwise,theentire
sub-categoryisreviewed.ForTranslatedArabic(bottomrow),WeemployQwen7Bmodel[8]toassesssemanticsimilaritybetweenthe
originalandtranslatedquestion-answerpairsonfuzzy-basisevaluation.Pairspassingtheevaluationproceed,whilethosethatfailundergo
manualreview. Basedonthis,datamayrequireManualHandlingformanualre-translation,Refine&Verifyforrefinementthroughthe
model,orNon-TranslatedReviewwherethedataisre-sentfortranslationduetotheabsenceofanArabicversion.
2.3.DataFilteringandVerification 3.CAMEL-BenchBenchmarkEvaluation
EvaluationMetrics: Ourevaluationframeworkisdesigned
withthreespecializedmetrics,eachcarefullyalignedtodif-
Thedatacollectionandquestion-answerpairgenerationpro-
ferenttypesofdatasetsandtasks. ForMCQdatasetslike
cessleadtoover41kquestionsintotalwhichthenundergoes
MMT[56]andMMMU[58],weutilizeexactmatchaccu-
to filtering and verification process. The CAMEL-Bench
racytoensurepreciseevaluation.Foropticalcharacterrecog-
filteringandverificationprocess(seeFig.3)iscarefullycon-
nition(OCR)datasets,suchasPATS[3]andEvarest[18],
ductedbasedonwhether theQAtextis originallyArabic
whereaccuratetextextractioniscritical,weadopteditdis-
or translated into Arabic from English language. For all
tance[45]asthekeymetric. Formoreflexibledatasetslike
sub-domainsderivedfromoriginalArabiccontext,wetake
VQAv2[17],MathVista[33],andGeoChat[22],wheremul-
a20%randomlysampledsubsetformanualverification. In
tiple synonymous answers can be considered correct. we
case if the error remains less below a 40% threshold, the
implementafuzzyevaluationmethodforallsuchdatasets.
sub-categoryisacceptedintoCAMEL-Bench. Alternatively,
ThisapproachusesGPT-4otocomparethepredictedanswer
theentiresub-categoryundergoesmanualreview.
withthegroundtruth,whileaccountingforthecontextof
the question. By incorporating these diverse metrics, our
evaluationprovidesarobustandcomprehensiveassessment
In case of the translated Arabic data from English, the
thatadaptstotheuniquedemandsandresponseformatsof
originalEnglishcontextisalsoincorporatedintothefilter-
eachdataset.
ingandverificationprocess. Here,Qwen7B[8]isusedto
comparethesemanticsimilaritybetweentheEnglishandthe Tab.3presentsacomparativeevaluationoffivedifferent
English-translateddataattheQA-pairlevelusingfuzzyeval- modelsonarangeofmultimodal(MM)understandingtasks,
uation. Toensurethemodelunderstandssemanticsimilarity eachassessingthecapabilitiesofthemodelsindistinctdo-
inArabic,weprovided5few-shotsprompting.Subsequently, mains. ThemodelsincludeGPT-4o,GPT-4o-mini,Gemini-
QA-pairsrejectedbyQwen7B[8]aremanuallyreviewed, 1.5-Pro,Gemini-1.5-Flash,andQwen2-VL-2B,evaluatedon
resultinginoneofthreeoutcomes. ManualHandlingimply- keytaskssuchasmultimodalreasoning,OCR&document
ingthatdatarequiresfullre-translation. RefineandVerify understanding,chart&diagraminterpretation,videoanal-
referringthatthetranslationcanberefinedusingthemodel. ysis,andseveraldomain-specifictaskslikeculturalunder-
Non-TranslatedReviewimplyingthatthenon-translateddata standing,medicalimaging,agricultural(agro)understanding,
is re-sent to the model for translation. Consequently, we andremotesensing. GPT-4oexcelsacrosstasks,leadingin
obtain29,036high-qualityquestions. MMreasoning(57.90),chart/diagramunderstanding(73.57),MMUnderstanding OCR&Document Charts&Diagram Video CulturalSpecific Medical Agro RemoteSensing
Method
&Reasoning Understanding Understanding Understanding Understanding Imaging Specific Understanding
GPT-4o 57.90 59.11 73.57 74.27 80.86 49.90 80.75 22.85
GPT-4o-mini 48.82 42.89 64.98 68.11 65.92 47.37 79.58 16.93
Gemini-1.5-Pro 46.67 36.59 47.06 42.94 56.24 33.77 72.12 17.07
Gemini-1.5-Flash 45.58 33.59 48.25 53.31 46.54 42.86 76.06 14.95
Pangea-7B 40.09 26.47 38.87 49.01 20.34 31.99 74.51 6.67
Qwen2-VL-2B 40.59 25.68 27.83 38.90 34.27 29.12 52.02 12.56
InternVL2-8B 30.41 15.91 30.27 51.42 20.88 29.48 44.47 5.36
LLaVa-NeXt-7B 26.33 19.12 27.56 44.90 28.30 22.54 42.00 8.33
Table3.Performancecomparisonofdifferentclosed-andopen-sourceLMMsonCAMEL-Bench.Wepresentper-domainresultsof
sevenLMMs: GPT-4o[38],GPT-4o-mini[38],Gemini-1.5-Pro[2],Gemini-1.5-Flash[2],Pangea-7B[59],Qwen2-VL[9],InternVL2-
8B[11],andLLaVaNeXt-7B[29]. GPT-4oexcelsinmostdomains,whileGPT-4o-minioffersanimpressivebalanceofperformance
andmodelsize. Allmodelsstrugglewithremotesensing,medicalimaging,OCR&documentunderstanding,andgeneralmultimodal
understandingandreasoningdomains.Open-sourcemodelslikeInternVL2-8BandLLaVaNeXt-7Bshowadeclineinperformanceacross
domains,withtheirbestresultsinvideounderstanding.
Figure4. Qualitativeexamplehighlightingdifferentscenarioswheredifferentclosed-weightmodelsstruggleonCAMEL-Bench. The
correctresponseisshowningreen,andtheincorrectoneintheredbox.
Figure5.Qualitativeexamplehighlightingdifferentscenarioswheredifferentopen-weightmodelsstruggleonCAMEL-Bench.Thecorrect
responseisshowningreen,andtheincorrectoneintheredbox.
videoanalysis(74.27),cultural(80.86)andagro-specificun- ing). Remotesensingunderstandingalsoremainsdifficult,
derstanding (80.75). Models perform well on MCQs and withscoreslike22.85(GPT-4o)and16.93(Qwen2-VL-2B),
binary-optiontasksduetoguessingprobabilityandcontext. highlighting the complexities of interpreting satellite im-
Infographics,designedforeasyinterpretation,alsoseehigh agery.
accuracyacrossallmodels. Incontrast,ArabicOCRtasks, Amongtheopen-sourcemodelsevaluatedonourArabic
particularlyindatasetslikeKhatt,historicaldocumentsprove multimodal benchmark, Pangea-7B stands out by outper-
exceptionally challenging. This difficulty stems from the formingInternVL2-8BandLLaVaNeXt-7Binkeyareas. It
complexnatureofArabicscript,whichusesligaturesand achieveshigherscoresinmultimodalunderstandingandrea-
diacritics(smallmarkingsthatalterpronunciationandmean- soning(40.09),OCRanddocumentunderstanding(26.47),and charts and diagram understanding (38.87). This sug- [2] GoogleAI. Gemini:Afamilyofhighlycapablemultimodal
geststhatPangea-7B’smultilingualandculturallydiverse models,2023. 5
training data enhance its ability to handle complex tasks [3] Husni A Al-Muhtaseb. Arabic text recognition of printed
acrossdifferentlanguagesandcultures. However,similarto manuscripts.Efficientrecognitionofoff-lineprintedArabic
otheropen-sourcemodels,Pangea-7Bstrugglesinremote textusingHiddenMarkovModels,BigramStatisticalLan-
sensingunderstanding,scoring6.67,highlightingchallenges guageModel,andpost-processing. PhDthesis,Universityof
Bradford,2010. 3,4
withspecializedtasks. Overall,Pangea-7B’sperformance
[4] FakhraddinAlwajih,ElMoatezBillahNagoudi,GaganBha-
underscoresthebenefitsofincorporatingdiverselinguistic
tia,AbdelrahmanMohamed,andMuhammadAbdul-Mageed.
andculturaldataintrainingmultilingualmultimodalLLMs
Peacock:Afamilyofarabicmultimodallargelanguagemod-
whileindicatingareasforimprovement.
elsandbenchmarks. arXivpreprintarXiv:2403.01031,2024.
The Fig. 4 and Fig. 5 highlight a critical challenge in
2
Arabicmultimodalunderstanding,whereallmodelsfailto
[5] Anthropic. Claude,2024. AIassistant. 3
accurately interpret the linguistic context in the provided
[6] ArarTawil. Arabicfood101. https://www.kaggle.com/
CAMEL-Benchsamples. Thisunderscoresthecomplexity
datasets/araraltawil/arabic-food-101,2023. 3
of Arabic linguistics, especially in multimodal tasks, and
[7] Muhammad Awais, Ali Husain Salem Abdulla Alharthi,
the need for more robust language models that can effec-
AmandeepKumar,HishamCholakkal,andRaoM.Anwer.
tivelyintegratebothvisualandtextualinformationinArabic Agrogpt: Efficientagriculturalvision-languagemodelwith
contexts. experttuning. arXiv,2024. 3
[8] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,Xi-
4.Conclusion,LimitationsandSocietalImpact
aodongDeng, YangFan, WenbinGe, YuHan, FeiHuang,
BinyuanHui,LuoJi,MeiLi,JunyangLin,RunjiLin,Dayi-
Wepresentacomprehensiveanddiversebenchmark,named
hengLiu,GaoLiu,ChengqiangLu,KemingLu,JianxinMa,
CAMEL-Bench,forArabicLMMevaluation. Tothebestof
Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,
ourknowledge,CAMEL-Benchisthefirstcomprehensive SinanTan,JianhongTu,PengWang,ShijieWang,WeiWang,
ArabicLMMbenchmarkcomprisingeightdiversedomains ShengguangWu,BenfengXu,JinXu,AnYang,HaoYang,
and38sub-domainswitharound29kquestionsthatarefil- JianYang,ShushengYang,YangYao,BowenYu,Hongyi
teredfromalargerpoolof41ksampleswiththequalityver- Yuan,ZhengYuan,JianweiZhang,XingxuanZhang,Yichang
ifiedbynativespeakers. Weconductextensiveevaluations Zhang,ZhenruZhang,ChangZhou,JingrenZhou,Xiaohuan
ofopen-andclosed-sourceLMMs,highlightingtheneedfor Zhou, and Tianhang Zhu. Qwen technical report. arXiv
preprintarXiv:2309.16609,2023. 4
substantialimprovementsindifferentareasforfutureArabic
[9] JinzeBai, ShuaiBai, ShushengYang, ShijieWang, Sinan
LMMdevelopment. AlthoughourCAMEL-Benchstrives
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
tosignificantlycontributetowardsdevelopingsophisticated
Zhou. Qwen-vl:Afrontierlargevision-languagemodelwith
ArabicLMMs,wenotethatitmainlycoversmodernstan-
versatileabilities. arXivpreprintarXiv:2308.12966,2023. 5
dardArabicanddoesnotfullyexploreotherArabicdialects.
[10] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Lit-
As the data samples are either based on existing datasets
tle,AndrewMiller,RobertCMiller,RobinMiller,Aubrey
ornewdatathatiscrawledfromtheinternet,itispossible
Tatarowicz, BrandynWhite, SamualWhite, etal. Vizwiz:
thatCAMEL-Benchexhibitsbiasesalreadyexistinginthe
nearly real-time answers to visual questions. In Proceed-
benchmarks. Nevertheless,webelieveCAMEL-Benchisa ingsofthe23ndannualACMsymposiumonUserinterface
steptowardstheinclusionofArabiclanguageandArabic- softwareandtechnology,pages333–342,2010. 3
speakingpopulationsinaccessingthebenefitsofLMMs. [11] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,
Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu,
Acknowledgements LeweiLu,BinLi,PingLuo,TongLu,YuQiao,andJifeng
Dai. Internvl: Scaling up vision foundation models and
WesincerelythankFarahHusainSalemAbdullahAlharthfor
aligningforgenericvisual-linguistictasks. arXivpreprint
hervaluablecontributionstothemanualdataverificationpro-
arXiv:2312.14238,2023. 5
cess.
[12] JaeminCho,JieLei,HaoTan,andMohitBansal. Unifying
vision-and-languagetasksviatextgeneration.InICML,2021.
References
1
[1] AbdelrahmanAbdallah, MahmoudKasem, MahmoudAb- [13] RocktimJyotiDas,SimeonEmilovHristov,HaonanLi,Dim-
dalla,MohamedMahmoud,MohamedElkasaby,YasserEl- itar Iliyanov Dimitrov, Ivan Koychev, and Preslav Nakov.
bendary, andAdamJatowt. Arabicaqa: Acomprehensive Exams-v:Amulti-disciplinemultilingualmultimodalexam
dataset for arabic question answering. In Proceedings of benchmark for evaluating vision language models. arXiv
the47thInternationalACMSIGIRConferenceonResearch preprintarXiv:2403.10378,2024. 2,3
andDevelopmentinInformationRetrieval,pages2049–2059, [14] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi
2024. 2 Fei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase.InProceedingsoftheIEEE/CVFConferenceonComputer [26] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXin
VisionandPatternRecognition(CVPR),pages248–255.Ieee, Zhao, and Ji-Rong Wen. Evaluating object hallucina-
2009. 3 tion in large vision-language models. arXiv preprint
[15] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,Meng- arXiv:2305.10355,2023. 3
dan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, [27] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
XingSun,etal. Mme: Acomprehensiveevaluationbench- PietroPerona,DevaRamanan,PiotrDollár,andCLawrence
markformultimodallargelanguagemodels. arXivpreprint Zitnick. Microsoft coco: Common objects in context. In
arXiv:2306.13394,2023. 3 European Conference on Computer Vision (ECCV), pages
[16] XingyuFu,YushiHu,BangzhengLi,YuFeng,HaoyuWang, 740–755.Springer,2014. 3
XudongLin,DanRoth,NoahASmith,Wei-ChiuMa,and [28] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
RanjayKrishna. Blink:Multimodallargelanguagemodels Visualinstructiontuning. InNeurIPS,2023. 1
canseebutnotperceive. arXivpreprintarXiv:2404.12390, [29] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,
2024. 3 ShengShen,andYongJaeLee.Llava-next:Improvedreason-
[17] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBa- ing,ocr,andworldknowledge,2024. 5
tra,andDeviParikh. Makingthevinvqamatter:Elevating
[30] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang
theroleofimageunderstandinginvisualquestionanswering.
Zhang,WangboZhao,YikeYuan,JiaqiWang,ConghuiHe,
InProceedingsoftheIEEE/CVFConferenceonComputer
ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelan
VisionandPatternRecognition(CVPR),pages6904–6913,
all-aroundplayer? InEuropeanConferenceonComputer
2017. 3,4
Vision,pages216–233.Springer,2025. 3
[18] HebaHassan,AhmedEl-Mahdy,andMohamedEHussein.
[31] PanLu,LiangQiu,JiaqiChen,TonyXia,YizhouZhao,Wei
Arabicscenetextrecognitioninthedeeplearningera:Analy-
Zhang,ZhouYu,XiaodanLiang,andSong-ChunZhu.Iconqa:
sisonanoveldataset. IEEEAccess,2021. 3,4
Anewbenchmarkforabstractdiagramunderstandingand
[19] Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao
visuallanguagereasoning. InThe35thConferenceonNeural
Cheng,DingjieSong,ZhihongChen,AbdulmohsenAlharthi,
InformationProcessingSystems(NeurIPS)TrackonDatasets
BangAn,ZicheLiu,ZhiyiZhang,JunyingChen,JianquanLi,
andBenchmarks,2021. 3
BenyouWang,LianZhang,RuoyuSun,XiangWan,Haizhou
[32] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei
Li,andJinchaoXu.Acegpt,localizinglargelanguagemodels
Chang,Song-ChunZhu,OyvindTafjord,PeterClark,and
inarabic,2023. 2
Ashwin Kalyan. Learn to explain: Multimodal reasoning
[20] DrewAHudsonandChristopherDManning. Gqa: Anew
viathoughtchainsforsciencequestionanswering. InThe
dataset for real-world visual reasoning and compositional
36thConferenceonNeuralInformationProcessingSystems
questionanswering. InProceedingsoftheIEEE/CVFConfer-
(NeurIPS),2022. 3
enceonComputerVisionandPatternRecognition(CVPR),
[33] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,
pages6700–6709,2019. 3
HannanehHajishirzi,HaoCheng,Kai-WeiChang,Michel
[21] FajriKoto,HaonanLi,SaraShatnawi,JadDoughman,Abdel-
Galley,andJianfengGao. Mathvista:Evaluatingmathemati-
rahmanBodaSadallah,AishaAlraeesi,KhalidAlmubarak,
calreasoningoffoundationmodelsinvisualcontexts. arXiv
ZaidAlyafeai,NehaSengupta,ShadyShehata,etal. Ara-
preprintarXiv:2310.02255,2023. 3,4
bicmmlu:Assessingmassivemultitasklanguageunderstand-
[34] SabriAMahmoud,IrfanAhmad,WasfiGAl-Khatib,Moham-
inginarabic. arXivpreprintarXiv:2402.12840,2024. 2
madAlshayeb,MohammadTanvirParvez,VolkerMärgner,
[22] KartikKuckreja,MuhammadS.Danish,MuzammalNaseer,
andGernotAFink.Khatt:Anopenarabicofflinehandwritten
Abhijit Das, Salman Khan, and Fahad S. Khan. Geochat:
textdatabase. PatternRecognition,47(3):1096–1112,2014.
Groundedlargevision-languagemodelforremotesensing.
2,3
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
[35] AhmedMasry,DoLong,JiaQingTan,ShafiqJoty,andEna-
sionandPatternRecognition(CVPR),2024. 3,4
mulHoque. ChartQA:Abenchmarkforquestionanswering
[23] BohaoLi,YuyingGe,YixiaoGe,GuangzhiWang,RuiWang,
aboutchartswithvisualandlogicalreasoning. InFindings
RuimaoZhang, andYingShan. Seed-bench: Benchmark-
oftheAssociationforComputationalLinguistics:ACL2022,
ingmultimodallargelanguagemodels. InProceedingsof
pages 2263–2279, Dublin, Ireland, 2022. Association for
theIEEE/CVFConferenceonComputerVisionandPattern
ComputationalLinguistics. 3
Recognition(CVPR),2024. 3
[36] Mohammad-Alfaifi. Github-mohammad-alfaifi/arab-celeb-
[24] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:
dataset. https://github.com/mohammad-alfaifi/arab-
Bootstrappinglanguage-imagepre-trainingforunifiedvision-
languageunderstandingandgeneration. InProceedingsof celeb-dataset,n.d. Accessed:2024-10-15. 3
theInternationalConferenceonMachineLearning(ICML), [37] Rayyan Najam and Safiullah Faizullah. Historical arabic
pages12888–12900.PMLR,2022. 1 handwrittentextrecognitiondataset,2024. 3
[25] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip- [38] OpenAI. Gpt-4omodel. https://openai.com,2024. Ac-
2: Bootstrappinglanguage-imagepre-trainingwithfrozen cessed:2024-10-14. 5
imageencodersandlargelanguagemodels.InProceedingsof [39] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar
theInternationalConferenceonMachineLearning(ICML), Mosseri,MichalIrani,andTaliDekel. Teachingcliptocount
pages19730–19742.PMLR,2023. 1 toten. InProceedingsoftheIEEE/CVFInternationalCon-ferenceonComputerVision(ICCV),pages3170–3180,2023. [53] xAI. xai.grok-1.5visionpreview. https://x.ai/blog/
3 grok-1.5v,2024. 3
[40] PATD. Printedarabictextdatabaseforrecognitionsystems. [54] NingXie,FarleyLai,DerekDoran,andAsimKadav. Visual
http://www.inf.u-szeged.hu/patd/. 3 entailment:Anoveltaskforfine-grainedimageunderstand-
[41] Pexel. Pexel:Thebestfreestockphotos,royalty-freeimages ing. arXivpreprintarXiv:1901.06706,2019. 3
andvideossharedbycreators. https://www.pexels.com/. [55] BarryMenglongYao,AdityaShah,LichaoSun,Jin-HeeCho,
3 andLifuHuang. End-to-endmultimodalfact-checkingand
[42] Pinterest. Pinterest platform. https://www.pinterest. explanationgeneration:Achallengingdatasetandmodels. In
com/. 3 Proceedingsofthe46thInternationalACMSIGIRConference
[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya onResearchandDevelopmentinInformationRetrieval,pages
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, 2733–2743,2023. 3
AmandaAskell,PamelaMishkin,JackClark,etal. Learning [56] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han
transferablevisualmodelsfromnaturallanguagesupervision. Lin,YueYang,HaoZhang,WenboZhang,YuqiLin,Shuo
InProceedingsoftheInternationalConferenceonMachine Liu,jiayilei,QuanfengLu,PengGao,RunjianChen,Peng
Learning(ICML),pages8748–8763.PMLR,2021. 1 Xu,RenruiZhang,HaozheZhang,YaliWang,YuQiao,Ping
[44] HanoonaRasheed,MuhammadMaaz,AbdelrahmanShaker, Luo, Kaipeng Zhang, and Wenqi Shao. MMT-bench: A
SalmanKhan,HishamCholakal,RaoM.Anwer,TimBald- comprehensivemultimodalbenchmarkforevaluatinglarge
win,MichaelFelsberg,andFahadS.Khan. Palo: Alarge vision-languagemodelstowardsmultitaskAGI. InProceed-
multilingualmultimodallanguagemodel. InProceedingsof ingsoftheInternationalConferenceonMachineLearning
theIEEE/CVFWinterConferenceonApplicationsofCom- (ICML),2024. 3,4
puterVision(WACV),2025. 1 [57] YouTube. https://www.youtube.com/,2024. Accessed:
[45] EricSvenRistadandPeterNYianilos. Learningstring-edit 2024-10-01. 3
distance.IEEETransactionsonPatternAnalysisandMachine
[58] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,Ruoqi
Intelligence,20(5):522–532,1998. 4 Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming
[46] DavidRomero,ChenyangLyu,HaryoAkbariantoWibowo, Ren,YuxuanSun,CongWei,BotaoYu,RuibinYuan,Ren-
Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik liangSun,MingYin,BoyuanZheng,ZhenzhuYang,Yibo
Mandal,AlinaDragonetti,ArtemAbzaliev,AtnafuLambebo Liu, WenhaoHuang, HuanSun, YuSu, andWenhuChen.
Tonja,etal.Cvqa:Culturally-diversemultilingualvisualques- Mmmu:Amassivemulti-disciplinemultimodalunderstand-
tionansweringbenchmark. arXivpreprintarXiv:2406.05967, ingandreasoningbenchmarkforexpertagi. InProceedings
2024. 2 oftheIEEE/CVFConferenceonComputerVisionandPattern
[47] RanaSMSaad,RandaIElanwar,NSAbdelKader,Samia Recognition(CVPR),2024. 3,4
Mashali,andMargritBetke. Bce-arabic-v1dataset:Towards
[59] XiangYue, YueqiSong, AkariAsai, SeungoneKim, Jean
interpretingarabicdocumentimagesforpeoplewithvisual
deDieuNyandwi,SimranKhanuja,AnjaliKantharuban,Lin-
impairments. InProceedingsofthe9thACMInternational
tangSutawika,SathyanarayananRamamoorthy,andGraham
ConferenceonPervasiveTechnologiesRelatedtoAssistive
Neubig. Pangea:Afullyopenmultilingualmultimodalllm
Environments,pages1–8,2016. 3
for39languages. arXivpreprintarXiv:2410.16153,2024. 5
[48] Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh
[60] XiangYue,TianyuZheng,YuanshengNi,YuboWang,Kai
Katipomu,HaonanLi,FajriKoto,WilliamMarshall,Gurpreet
Zhang,ShengbangTong,YuxuanSun,BotaoYu,GeZhang,
Gosal,CynthiaLiu,ZhimingChen,etal. Jaisandjais-chat:
HuanSun,YuSu,WenhuChen,andGrahamNeubig.Mmmu-
Arabic-centricfoundationandinstruction-tunedopengenera-
pro:Amorerobustmulti-disciplinemultimodalunderstand-
tivelargelanguagemodels.arXivpreprintarXiv:2308.16149,
ingbenchmark. arXivpreprintarXiv:2409.02813,2024. 3
2023. 2
[49] JingqunTang,QiLiu,YongjieYe,JinghuiLu,ShuWei,Chun-
huiLin,WanqingLi,MohamadFitriFaizBinMahmood,Hao
Feng,ZhenZhao,YanjieWang,YuliangLiu,HaoLiu,Xiang
Bai, andCanHuang. Mtvqa: Benchmarkingmultilingual
text-centricvisualquestionanswering,2024. 3
[50] FeiWang,XingyuFu,JamesYHuang,ZekunLi,QinLiu,
XiaogengLiu,MingyuDerekMa,NanXu,WenxuanZhou,
Kai Zhang, et al. Muirbench: A comprehensive bench-
markforrobustmulti-imageunderstanding. arXivpreprint
arXiv:2406.09411,2024. 3
[51] Wikipedia. Wikipediathefreeencyclopedia. https://www.
wikipedia.org/. 3
[52] YueWuandPremNatarajan. Self-organizedtextdetection
withminimalpost-processingviaborderlearning.InProceed-
ingsoftheIEEE/CVFInternationalConferenceonComputer
Vision(ICCV),2017. 3A.Appendix MMMU,andBCE-Arabic. Theevaluationfocusesontasks
suchasunderstandingdiagrammaticreasoningandtabular
B.MoreonDatasetCuration
data with 1,994 questions from diagram datasets and 745
questionsinvolvingcharts,providingarobustexamination
Thedatasetutilizedinthisworkwascarefullycuratedwith
ofthemodel’sabilitytointerpretvisualdataefficiently.
a rigorous focus on data quality, relevance, and diversity.
Our curation process involved selecting multimodal data
C.4.VideoUnderstanding
fromvariousdomains,includingimages,text,videos,and
specializedfieldssuchasmedicalimaging,agriculture,and This category assesses the model’s ability to process and
remotesensing. Toensuretheintegrityandaccuracyofthe comprehendvideodata,focusingontaskslikerecognizing
dataset, we employed multiple stages of data verification. countries,landmarks,andoccasions.Video-MMEisapromi-
This process involved cross-validation, thorough verifica- nent dataset, contributing 654 questions to the evaluation.
tion procedures for Arabic content, and the integration of Theinclusionofdiversesub-domains,suchasrecognizing
standardizeddatasourceswhereapplicable. cultural aspects through video, highlights the importance
oftemporalandvisualinformationsynthesisinmultimodal
C.DatasetOverviewandTaskSplits reasoning.
This section provides a comprehensive breakdown of the C.5.CulturalSpecificUnderstanding
datasetsusedacrosseightdistinctcategories,illustratingthe
Theculturalunderstandingdomainteststhemodel’scapacity
diversityanddepthofourevaluationframework. Eachcate-
tohandletasksspecifictocertaincultures,includingfood,
goryisfurtherdividedintosub-domains,ensuringthatthe
landmarks, and celebrities. Datasets like arabic-food-101
multimodal models are rigorously tested on a wide range
andPexelchallengethemodeltorecognizeculturallysignifi-
oftasksanddatasets. Thisstructureguaranteescomprehen-
cantitems,with444questionsfocusedoncelebritiesand494
sivecoverageandintroducesvariedchallengestothoroughly
oncountries/landmarks. Thesetaskshighlightthemodel’s
assess model performance. Refer to Tab. 2 for a detailed
abilitytoadaptandgeneralizeacrossdifferentculturalcon-
breakdownofthedatacategorieswiththeirstatistics.
texts.
C.1.MultimodalUnderstandingandReasoning
C.6.MedicalImaging
This category encompasses various sub-domains such as
visualunderstanding, objecthallucinationevaluation, and Coveringarangeofsub-domainsinthemedicalfield,this
complex visual perception. Key datasets include MME, categoryincludestasksrelatedtobasicmedicalscience,clin-
MMBench, ScienceQA-IMG, and VQA2. These datasets icalmedicine,andpublichealth,usingdatasetslikeMMMU
testthemodel’sabilitytohandleintricatereasoningtasks and MMT-MI-Bench. These datasets assess the model’s
acrossbothvisualandtextualinputs,withatotalof3,971 potentialinspecializedmedicalcontexts, withover1,200
questionsunderthevisualunderstandingsub-domain,and questionsspanningacrossdiagnosis,medicalunderstanding,
significant representation from other tasks like scientific andpharmacy,ensuringarigorousevaluationofthemodel’s
reasoning(1,624questions)andobject-levelperception(60 performanceinhandlingcriticalmedicalinformation.
questions).
C.7.AgriculturalImageUnderstanding
C.2.OCRandDocumentUnderstanding
Theagriculturaldomainisrepresentedthroughdatasetslike
Documentunderstandingcoversscanneddocuments,scene AgroGPT, with 769 questions focused on agricultural un-
understanding,textextraction,andmore. Thiscategoryem- derstanding tasks. These tasks test the model’s capacity
phasizespreciseOCRandtextualrecognitionfromimages to process and interpret images related to agricultural set-
andscannedmaterials. DatasetslikeArabicDatasetOCRand tings,reinforcingthemodel’sabilitytoworkwithreal-world
ISI-PPT-Dataset challenge the model to process a diverse scenariosinagricultureandenvironment-basedchallenges.
rangeofdocumenttypes. Asubstantialnumberofquestions
C.8.RemoteSensingUnderstanding
comefromHandwrittenTextdatasets(1,400questions)and
PPTOCR(2,354questions),ensuringthemodelisevaluated This category evaluates the model’s ability to handle re-
acrossbothstructuredandunstructureddocumenttypes. mote sensing data, specifically focusing on geographical
datainterpretationthroughdatasetslikeGeoDataVQAand
C.3.ChartandDiagramUnderstanding
GeoChat. With709questionsinthisdomain,themodelis
In chart and diagram interpretation, models are tested on testedonitsspatialreasoningandunderstandingofcomplex
understandingvisualrepresentationsofdata,suchascharts, remote-sensing imagery, crucial for applications in fields
diagrams,andtables. ThisincludesdatasetslikeChartQA, likeenvironmentalmonitoringandgeography.Intotal,thedatasetincludes29,036questionsacrossall
categories, providingacomprehensiveanddiversebench-
mark for evaluating the multimodal model’s performance
acrossawidespectrumoftasks. Thisbalanceddistribution
ensuresthat the model is tested thoroughly, with eachdo-
mainofferinguniquechallengesandinsightsintothemodel’s
strengthsandareasforimprovement.
D.CAMEL-BenchDataSamples
Fig.2showcasesCAMEL-Bench’sversatilityacrosseight
distinct domains, covering tasks like Multimodal Reason-
ing, OCR&DocumentUnderstanding, Chart&Diagram
Interpretation,VideoSceneAnalysis,andmorespecialized
areas like Remote Sensing, Agricultural Image Analysis,
MedicalImageInterpretation,andCultural-SpecificKnowl-
edge. Eachdomainpresentsuniquechallenges,fromlogical
reasoning and handwritten text recognition to medical di-
agnostics and cultural symbol identification. This variety
emphasizesCAMEL-Bench’sstrengthinsupportingthede-
velopmentofAIsystemscapableofaddressingreal-world
applicationsinhealthcare,agriculture,geospatialanalysis,
andcross-culturalcontexts.